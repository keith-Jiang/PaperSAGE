# Transfer Learning of Real Image Features with Soft Contrastive Loss for Fake Image Detection

Ziyou Liang1, Weifeng Liu1, Run Wang1\*, Mengjie $\mathbf { W } \mathbf { u } ^ { 1 }$ , Boheng $\mathbf { L i } ^ { 2 }$ , Yuyang Zhang1, Lina Wang1, Xinyi Yang3

1Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China 2Nanyang Technological University, Singapore 3NSFOCUS, China

# Abstract

In the last few years, the artifact patterns in fake images synthesized by different generative models have been inconsistent, leading to the failure of previous research that relied on spotting subtle differences between real and fake. In our preliminary experiments, we find that the artifacts in fake images always change with the development of the generative model, while natural images exhibit stable statistical properties. In this paper, we employ natural traces shared only by real images as an additional target for a classifier. Specifically, we introduce a self-supervised feature mapping process for natural trace extraction and develop a transfer learning based on soft contrastive loss to bring them closer to real images and further away from fake ones. This motivates the detector to make decisions based on the proximity of images to the natural traces. To conduct a comprehensive experiment, we built a high-quality and diverse dataset that includes generative models comprising GANs and diffusion models, to evaluate the effectiveness in generalizing unknown forgery techniques and robustness in surviving different transformations. Experimental results show that our proposed method gives $9 6 . 2 \%$ mAP significantly outperforms the baselines. Extensive experiments conducted on popular commercial platforms reveal that our proposed method achieves an accuracy exceeding $78 . 4 \%$ , underscoring its practicality for real-world application deployment.

# Introduction

With the rapid development and maturity of generative models, the increasing proliferation of fake images has attracted widespread attention. Compared to Generative Adversarial Networks (GANs), diffusion models (DMs), as today’s SOTA generative models, exhibit better generation quality (Dhariwal 2021) and even support powerful text-toimage models such as DALL-E2 (Ramesh et al. 2022), Stable Diffusion (Rombach et al. 2022). Currently, one can use different types of generative models to create realistic faces or complex scene images and it is foreseeable that more generative models for image synthesis will emerge in the future. Therefore, it is the goal for the community to develop a more practical method to distinguish fake images that are synthesized with unknown forgery techniques as the unseen generative models will emerge inadvertently.

![](images/4c2ba4681d7744e405dff4b321eef54a707e313a4bdb3ee4358c0a8641e94a3f.jpg)  
Figure 1: TSNE visualization. Left: The detector is easily overfitted to fake images in the training set; Right: Training with natural traces can generalize to unknown fake images.

The prior paradigm for fake image detection is to learn artifacts of fake images by capturing the subtle differences between real and fake images (Wang et al. 2019, 2020; Huang et al. 2020; Wang et al. 2021). Our empirical research confirms that there is a huge challenge in detecting unknown fake images: the model can easily form a classification manifold on the images of the training set, while new generative fake images and their distinctive artifacts will be randomly scattered in this space, as shown in Figure 1 left. Unfortunately, the existing studies are trapped in the endless efforts to spot the artifacts by investigating the subtle differences between real and fake (Corvi et al. 2023). Since the classifier can easily overfit the fake image artifacts of the training set, unknown fake images will be scattered outside the manifold like real images.

In this paper, we propose a transfer learning method based on natural trace features for fake image detection. We propose a novel perspective on fake image detection, emphasizing the inherent similarities among real images rather than the differences between real and fake. Our insight is grounded in the premise that images from the real world possess a stable intrinsic naturalness. We posit that real images share common characteristics, akin to those found in fake images, which we refer to as "natural traces." We introduce natural trace forensics (NTF) and adopt a substitution strategy to replace shared features with homogeneous features. Specifically, we exploit a self-supervised feature mapping process to decouple heterogeneous features of real images, allowing for the capture of stable homogeneous features. We then develop a transfer learning for homogeneous features based on a soft contrastive loss, which simultaneously solved the problem that similar clusters formed by self-supervised contrastive learning are still scattered with large distances and outliers under the constraints of supervised contrastive learning do not converge. We jointly optimized transfer training and binary classification to force the classifier to aggregate homogeneous features close to real images and constrain them away from fake images. In Figure 1 right, the classifier reduces its dependence on specific artifact patterns and gains the ability to detect images synthesized by unknown generative models with natural traces.

To better evaluate whether our proposed method can be well generalized to other unknown generative models, we build a generated image dataset consisting of 12 SOTA generative models, including 6 GANs and 6 DMs. Our dataset covers a variety of categories, such as faces and scenes, to evaluate detectors’ capability across various types of fake images. In addition, we also evaluate the performance of our method in identifying images generated by Midjourney1 and Kolors2, which is currently a popular commercial tool for text-to-image generation. Experimental results show that our proposed method could discriminate GAN-based, DMbased, and Multi-step fake images (synthesized with at least two different generated models) in high confidence with an average accuracy of more than $9 6 . 1 \%$ and is sufficiently robust to various image perturbation transformations.

Our main contributions are summarized as follows:

• We are the first to propose soft contrastive transfer learning, which utilizes the disentangled shared features of real images for fake image detection. Our results also demonstrate its effectiveness in detecting previously unseen generative fake images, highlighting its potential as a versatile solution in this domain.   
• To conduct a comprehensive evaluation, we build a dataset including GAN-based, DM-based, and Multi-step manipulation for generating fake images. For the first time, we generate Multi-step fake images by employing multiple synthesis methods.   
• Experimental results show the effectiveness of our proposed method in tackling fake images generated by SOTA GANs and diffusion models, giving an average precision of more than $9 6 . 1 8 \%$ , significantly outperforming the baselines.

# Related Work Transfer Learning in Fake Detection

In recent years, the most common method of transfer learning is to fine-tune the pre-trained model on the target dataset. (Suratkar 2023) detect fake videos through the utilization of transfer learning in autoencoders and a hybrid model of CNN and RNN. (Lee and Kim 2022; Ghayoomi 2022) used pre-trained BiLSTM(Zhou et al. 2016) and RoBerta(Liu et al. 2019) for transfer learning on fake news about COVID19 in Korean and Persian, respectively. Although the finetuning method is highly flexible for downstream tasks, it is also prone to catastrophic forgetting. Freezing layers of the pre-trained backbone network and finetuning only lateral fully connected layers is one of the most effective transfer learning methods. (Ranjan 2020) explored the effectiveness and interpretability of freezing convolutional layers and fine-tuning fully connected layers in DeepFake detection, and (Elhassan et al. 2022) studied the transfer of lip-motionbased DeepFake detection methods on 11 models. However, the pre-training of these frozen transfer learning methods is obviously isolated from the target, and the parameter transfer effect is limited. As far as we know, there is still a gap in the exploration of transfer learning in fake image detection. Our method balances the feature transfer and classification by additional soft contrastive loss constraints.

# Fake Images Detection

Existing research focuses on exploring subtle differences between real and fake images, and these can be categorized into explicit and implicit artifact-based methods.

Explicit-based. Some researchers noticed that they often contained specific artifacts or unnatural patterns(Zhang 2019; Liu et al. 2023). Additionally, several studies focused on exploring GAN-based artifacts in the frequency domain(Frank et al. 2020) and the failure to accurately re-enact certain biological features when generating fake faces $\mathrm { H u } ~ 2 0 2 1$ ; Tan et al. 2023b). These findings have motivated researchers to use explicit artifacts to detect fake images through simple classifiers. However, as generative models are continuously updated and iterated, these artifacts become imperceptible or even disappear, making detection methods reliant on explicit artifacts less effective.

Implicit-based. Wang et al.(Wang et al. 2020) demonstrated that with appropriate training data and data augmentation, neural networks could detect other GAN-based images, while (Tan et al. 2023a) used CNNs to transform images into gradient form to present a broader range of artifacts. These studies indicate that images generated by both GAN and diffusion models possess distinct “fingerprints" different from the real images(Yu, Davis, and Fritz 2019; Sha et al. 2023). Nevertheless, there is still insufficient evidence to prove that generative models from different families have universal fingerprints for detection.

# A Diverse Generated Image Dataset

Due to the lack of DM-based generated images in current datasets, we create a dataset with a wide range of generative models. This dataset aims to enhance the evaluation of fake detection methods’ capability. It includes fake images generated by various models, alongside an equal number of real images from corresponding training sets for each method.

Our dataset covers two major families: GANs and diffusion models, with each fake image synthesized using one generative model. Particularly, we have developed a novel multi-step fake image generation method, involving collaboration between two or more generative models, to achieve identity swapping or attribute editing between real and fake faces. To ensure diversity, the dataset comprises various categories of generation methods: unconditional generation, image-to-image generation, and prompt-guided generation, as shown in Table 1. Moreover, our carefully selected generative models exhibit fundamental differences in their generations, ensuring extensive representation of the dataset.

Table 1: Statistics of the self-built dataset, including GAN-based, DM-based, and Multi-step synthesis.   

<html><body><table><tr><td>Family</td><td>Type</td><td>Method</td><td>Year</td><td>Image Source</td><td>#Images</td></tr><tr><td rowspan="3">GAN- based</td><td>Unconditional</td><td>ProGAN StyleGAN2 ProjGAN</td><td>2017</td><td>CelebA-HQ 2019CelebA-HQ/FFHQ/LSUN 2021FFHQ/LSUN/Landscape</td><td>4.0k 12.0k 12.0k</td></tr><tr><td></td><td>VQGAN Diff-StyleGAN2 2022</td><td>2020</td><td>CelebA-HQ FFHQ/LSUN</td><td>4.0k 8.0k</td></tr><tr><td>Image-to-Image</td><td>SimSwap DDPM</td><td>2020 2020</td><td>CelebA-HQ CelebA-HQ/LSUN</td><td>4.0k 8.0k</td></tr><tr><td rowspan="3">DM-</td><td>Unconditional</td><td>DDIM PNDM</td><td>2021 2022</td><td>CelebA-HQ CelebA-HQ/LSUN</td><td>4.0k 12.0k</td></tr><tr><td>based Image-to-Image</td><td>DiffFace</td><td>2022</td><td>CelebA-HQ</td><td>2.0k</td></tr><tr><td>Prompt-guided</td><td>LDM SDM</td><td>2022 2022</td><td>CelebA-HQ/LAION LAION</td><td>8.0k 4.0k</td></tr><tr><td rowspan="3">Multi- step</td><td>GAN-GAN</td><td>SimSwap_Style2 2024 SimSwap_VQ</td><td>2024</td><td>CelebA-HQ CelebA-HQ</td><td>2.0k 2.0k</td></tr><tr><td>GAN-DM</td><td>SimSwap_LDM 2024 DiffFace_Style22024 DiffFace_Proj 2024</td><td></td><td>CelebA-HQ CelebA-HQ FFHQ</td><td>2.0k 2.0k</td></tr><tr><td>DM-DM</td><td>DiffFace_LDM 2024</td><td></td><td>CelebA-HQ</td><td>2.0k 2.0k</td></tr></table></body></html>

GAN-based forgery: We select six representative GANs to generate forgery images including unconditional and imageto-image. The innovation of ProGAN (Karras et al. 2017) in introducing progressive training has a significant impact on subsequent research, while StyleGAN2 (Karras et al. 2020) refines style control through decoupling image features and generates more realistic images. In terms of architectural improvements, ProjGAN (Sauer et al. 2021) enhances generator feedback using pre-trained weights, while VQGAN (Esser 2021) and Diff-StyleGAN2 (Wang et al. 2022) innovatively replace the backbone network with transformer and diffusion processes, respectively, resulting in higher image quality and more stable training processes. SimSwap’s (ss)(Chen et al. 2020) ID injection module achieves breakthroughs in arbitrary face swapping.

DM-based forgery: DM-based surpasses GAN-based in terms of image quality and diversity. Here, we select six different DMs for creating fake images including unconditional, image-to-image, and prompt-guided. DDPM(Ho 2020), as the initial diffusion model, lays the groundwork, with DDIM(Song 2020) and PNDM(Liu et al. 2022) enhancing execution speed and quality. DiffFace (df)(Kim et al. 2022) is the first identity-conditioned DDPM that uses diffusion models for face swapping. And LDM(Rombach et al. 2022), which employs pre-trained self-encoders to map pixels to latent space, combined with context learning in cross-attention layers for prompt-guided image generation, along with Stable Diffusion (SDM), a popular LDM-based prompt-guided model, represent further advancements.

Multi-step: In the real scenario, the creator tends to employ multiple forgery techniques to achieve better forgery. We create a Multi-step face synthesis dataset where two faceswapping methods (SimSwap and DiffFace) based on GAN or diffusion models swap real faces onto synthetic ones, including three hybrid modes: GAN-GAN, GAN-DM, and DM-DM. This generation method provides simulated threats of artifact disappearance or blending in real-world scenarios.

# Our Method

In this work, we propose a novel method, named Natural Trace Forensics (NTF), which involves training the classifier using the natural traces shared merely by real images as an additional predictive target. Figure 2 overviews the pipeline of NTF. We start by learning natural trace representations from real datasets. Then, under a soft contrastive learning (SCL) framework, the network is trained to align natural traces closer to real images and further from fake ones. With such constraints, the network is motivated to detect fakes based on the distance between images and natural traces. Next, we elaborate on how to learn the natural traces and apply the extracted traces for detection.

# Natural Trace Representation Learning

We first explore the natural traces in real images to provide learnable features for the next fake image identification. However, it is impossible to analyze every real image in existence to identify shared features. To address this, we employ an innovative strategy using the same intrinsic features found in real images as a substitute for shared features. These intrinsic features, known as homogeneous features, are derived from the inherent properties and statistical regularities of images, which are commonly present in real images. We develop a self-supervised feature mapping mechanism to extract the homogeneous features. This mechanism decouples the natural image features into homogeneous features and heterogeneous features, where the latter are associated with specific images. As opposed to direct embedding features, feature decoupling not only enhances generalization by ensuring homogeneous features more accurately capture commonalities across images, but also improves feature quality by eliminating noise and redundant information. Additionally, the network should access a large variety of natural images. This exposure enables the network to learn the patterns of feature coupling in various types of real images so that it can decouple features for unseen images.

Formulation. We assume access to a large-scale dataset of real images $D _ { r }$ . Sample $x \in D _ { r }$ is an augmented sample from an image $\boldsymbol { x } _ { r } \in \mathbf { \bar { \mathbb { R } } } ^ { H \times W \times 3 }$ . As shown in Figure 2(a), our architecture consists of a feature encoder followed by two projection heads that map real image embeddings into homogeneous and heterogeneous features.

Specifically, feature encoder, $E$ , producing feature embedding $~ e ~ = ~ E ( x _ { r } )$ from the inputs, then decouples the $e$ into homogeneous and heterogeneous features, $\begin{array} { r } { \dot { z } ^ { h o m } = } \end{array}$ $f ^ { h o m } ( e ) \ \in \ \mathbb { R } ^ { C }$ and $z ^ { h e t } = f ^ { \ k e t } ( e ) \in \mathbb { R } ^ { C }$ through two projection heads, $f ^ { h o m }$ and $f ^ { h e t }$ , respectively, where $C$ is the dimensionality of the features. Let $i \in \dot { I } \equiv \{ 1 . . . 2 N \}$ be the index of an arbitrary augmented sample, where there are a total of $\mathbf { N }$ samples in a batch, each with two random augmentations $( 2 N )$ . The self-supervised contrastive loss of heterogeneous (het.) feature representations in real images can be formulated as follows:

![](images/c011c79501b16709437081e2879a93f1b2fbf8150eb93fa1cf6369ac74bbe5bd.jpg)  
Figure 2: NTF architecture. We first decouple the feature representation of real images into homogeneous and heterogeneous features. Next, the homogeneous features $z ^ { h o m }$ participate in SCL with the real and fake image features. Detector classifies real and fake images guided by the target of intra-class aggregation and inter-class separation. Better view in color.

Table 2: Intra-family generalization on GANs. Performance of NFT and baselines in spotting 6 GAN-based generated images. These baselines include the classic approach, XcetionNet, Wang2020, and Grag2021, as well as the latest methods, Ojha2023, and CADDM. The method with $^ *$ means that the baseline is only evaluated on non-face images. The Average column represents the weighted average of the corresponding metrics. Among those, the best and second-best performances are highlighted in bold and underlined, respectively.   

<html><body><table><tr><td rowspan="2">Category</td><td rowspan="2">Method</td><td colspan="4">ProGAN</td><td rowspan="2">StyleGAN2</td><td rowspan="2">VQGAN</td><td rowspan="2">ProjGAN</td><td rowspan="2">Diff-StyleGAN2</td><td rowspan="2">SimSwap</td><td rowspan="2">Average</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Explicit-based</td><td>SBIs*</td><td>68.6</td><td>66.0</td><td>39.8</td><td>28.2</td><td>65.5 60.241.837.956.946.276.930.8 59.950.053.946.258.554.958.332.055.3 53.933.159.160.855241.841.9</td><td></td><td></td><td></td><td></td><td>AP↑ ACC↑FPR↓FNR↓AP ACCFPRFNR AP ACC FPR FNR AP ACCFPR FNR AP ACCFPRFNR AP ACC FPR FNR AP ACCFPR FNR</td></tr><tr><td>CADDM* 51.5</td><td></td><td>53.4</td><td>40.6</td><td></td><td>52.652.3 54.446.844352.654.550.740.354.357.6 43.041.751.352.944.250.046.636.682.943.6 52.153.348.4 45.0</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="5">Implicit-based Grag2021</td><td>Xception</td><td>99.3</td><td></td><td></td><td>31.1</td><td>100 99.9 0.1</td><td>100 99.9 0.2 0</td><td>100 99.1 1.8</td><td></td><td></td><td></td></tr><tr><td>Wang2020 97.9</td><td></td><td>84.5 85.3</td><td>0 27.1</td><td>2.0</td><td>0 86.2 56.3 87.3 0.1</td><td>61.2 50.5 98.9 0.1</td><td>0 62.2 50.4 99.2 0.1</td><td>97.9 84.0 0.231.899.9 90.119.7 87.8 68.5 61.3 1.8 98.4 65.0 69.8 0.1 82.3 62.7 73.9 0.7</td><td>0</td><td>99.592.93.710.5</td></tr><tr><td></td><td>100</td><td>99.9</td><td>0</td><td>0.1 100 100</td><td>0 0</td><td>99.9 91.616.80.199.794.610.8</td><td>0</td><td>99.8 94.5 11.1 0</td><td>94.1 50.5 98.9 0</td><td>98.9 88.5 22.9 0</td></tr><tr><td>Ojha2023</td><td>98.8</td><td>99.1</td><td>1.5</td><td>0.5</td><td></td><td></td><td></td><td>83.187.0 15.910.183.687.316.19.475.278.733.29.4 64.6 64.9 69.30.880.383.8 23.58.980.9 83.526.6 6.5</td><td></td><td></td></tr><tr><td>NTF(Ours） 100</td><td></td><td>92.5</td><td>0</td><td>14.9 100 92.5</td><td>0 15.0 100 92.6</td><td>0</td><td></td><td></td><td></td><td>14.999.8 92.20.715.199.9 92.7 0.214.6 99.8 92.20.615.199.9 92.40.2 14.9</td></tr></table></body></html>

$$
\mathcal { L } _ { h e t } = - \sum _ { i \in I } \log \frac { e x p ( z _ { i } ^ { h e t } \cdot z _ { p } ^ { h e t } / \tau ) } { \sum _ { k \in K ( i ) } e x p ( z _ { i } ^ { h e t } \cdot z _ { k } ^ { h e t } / \tau ) } ,
$$

where the index $i$ is called the anchor, the index $p$ is called the positive, $\tau$ is a temperature hyperparameter and $K ( i ) \equiv$ $I \backslash \{ i \}$ , which contains all the augmented samples except $i$ in a batch. Furthermore, the loss of homogeneous (hom.) feature representations in real images can be formulated as:

$$
\mathcal { L } _ { h o m } = \arg \operatorname* { m a x } _ { i \in I , k \in K ^ { \prime } ( i ) } \parallel z _ { k } ^ { h o m } - z _ { i } ^ { h o m } \parallel _ { F } ^ { 2 } ,
$$

where $\| \cdot \| _ { F }$ denotes the Frobenius norm, $K ^ { \prime }$ contains $2 N -$ 2 augmented samples, $i . e .$ ., target $i$ is compared only with samples from different sources.

Considering the potential issues of high feature coupling in the embedding space, we further use soft orthogonality (orth.) to reduce information redundancy and dependencies between these homogeneous and heterogeneous features:

$$
\mathcal { L } _ { o r t h } = \sum _ { i \in I } \cos ( z _ { i } ^ { h o m } , z _ { i } ^ { h e t } ) ,
$$

Finally, we combine these constraints to form the natural

trace representation learning loss:

$$
\begin{array} { r } { \mathcal { L } _ { t r a } = \mathcal { L } _ { h o m } + \mathcal { L } _ { h e t } + \lambda \mathcal { L } _ { o r t h } , } \end{array}
$$

where $\lambda$ is a scaling factor.

# Transfer Learning for Fake Image Detection

To capture the natural traces as an additional target, we develop the SCL to simultaneously encode real and fake images further for fake image detection. Specifically, we incorporate these natural traces into SCL and constrain their distance from positive and negative samples. This will motivate the detector to identify fake images based on distance. Note that the feature encoder is frozen during this stage.

Formulation. We now assume access a full dataset, $D =$ $D _ { r } \bigcup D _ { f }$ , where $D _ { r }$ is used in the previous stage, $D _ { f }$ is a dataset of fake images. Our architecture consists of a feature encoder, an auxiliary projection head for SCL, and another classification head for supervised classification(shown in Figure 2(b). With the auxiliary projection head, the real and fake feature embeddings are mapped to $z _ { i } ^ { r }$ and $z _ { i } ^ { f }$ , respectively. To motivate the network to focus on the intraclass aggregation of real images more than the inter-class differences between real and fake images, we adopt the homogeneous features $z ^ { h o m }$ from the previous stage as extra positive instances for real anchors and additional negative instances for fake anchors. SCL mitigates the negative impact of fake image features on the classifier by assigning weights to the homogeneous features. Formally, the soft contrastive loss is as follows:

Table 3: Cross-family generalization on diffusion models. Performance of NFT and baselines in spotting 6 DM-based generated images   

<html><body><table><tr><td rowspan="2">Category</td><td rowspan="2">Method</td><td colspan="4">DDPM</td><td rowspan="2">DDIM</td><td rowspan="2">PNDM</td><td rowspan="2">LDM</td><td rowspan="2">SDM</td><td rowspan="2">DiffFace</td><td rowspan="2">Average</td></tr><tr><td></td><td></td><td></td><td>AP↑ACC↑FPR↓FNR↓AP ACCFPRFNR AP ACC FPRFNR AP ACCFPR FNR AP ACCFPRFNR AP ACC FPRFNR AP ACCFPR FNR</td></tr><tr><td rowspan="2">Explicit-based</td><td>SBIs*</td><td>63.1</td><td>64.0</td><td>37.5</td><td></td><td></td><td></td><td></td><td></td><td></td><td>34.571.169.515.547.664.259.248.533.083.875.714.634.057.055.859.229.170.656.928.028.4 68.363.533.934.4</td></tr><tr><td>CADDM*53.0</td><td></td><td>55.2</td><td></td><td>46.5</td><td></td><td></td><td></td><td></td><td></td><td>43.255.859.835.8 44.651.852.5 51.943.152.353.948.343.951.6 54.538.3 52.5 53.052.643.944.2 52.954.744.1 45.2</td></tr><tr><td rowspan="2"></td><td>Xception</td><td>74.7</td><td>72.3</td><td>5.1</td><td></td><td>50.275.873.912.439.989.649.899.90.665.8 65.08.861.241.140.328.591.059.240.899519.0 66.257.014.059.0</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Wang2020 58.2</td><td></td><td>50.2</td><td>99.7</td><td></td><td>0.1</td><td>60.9 50.2 99.6 0.1 66.4 50.0 99.6 0.1 67.9 50.4 99.2</td><td></td><td></td><td></td><td>39.8 49.5 99.6 1.5 34.8 49.8 99.8 2.4 54.6 50.0 99.6 0.7</td></tr><tr><td rowspan="2">Implicit-based Grag2021 79.9</td><td></td><td></td><td>50.1</td><td>99.8</td><td>0</td><td>83.7 50.3 99.5 0.1 86.5 50.1 99.3</td><td>0</td><td>99.9 98.8 2.5</td><td>98.2 63.6 72.9</td><td>63.2 54.4 91.30 85.2 61.2 77.5</td><td>0</td></tr><tr><td>Ojha2023</td><td>64.2</td><td>67.0</td><td>57.4</td><td></td><td>8.6</td><td></td><td></td><td></td><td></td><td>71.875.339.79.861.764.462.29.080.283.922.99.4 55.6 56.087.30.888.488722.00.670.372.6 48.6 6.3</td></tr><tr><td rowspan="2"></td><td></td><td></td><td></td><td>47.2</td><td>15.3</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NTF(Ours) 76.9</td><td></td><td>68.8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>91.8 84.116.415.5 79.471.042.715.1999 92.70.114.799.5 92.11.114.8 99387.40.615.5 91.2 82.718.0 15.1</td></tr></table></body></html>

Table 4: Cross-family generalization on multi-step methods. Performance of NTF and baselines in spotting 6 multi-step generated images   

<html><body><table><tr><td rowspan="2">Category</td><td rowspan="2">Method</td><td colspan="4">SimSwap_Style2</td><td rowspan="2">SimSwap_VQ</td><td rowspan="2">SimSwap_LDM</td><td rowspan="2">DiffFace_Style2</td><td rowspan="2">DiffFace_Proj</td><td rowspan="2">DiffFace_LDM</td><td rowspan="2">Average</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Explicit-based</td><td>SBIs*</td><td>62.7</td><td>69.2</td><td>38.5 23.1</td><td></td><td>74.5 65.531.138.147.448.274.328.274.669.924.835.465.258.344.045.2 63.058.254.128.8 64.661.644.43.1</td><td></td><td></td><td></td><td></td><td>AP ↑ ACC↑FPR↓FNR↓AP ACC FPR FNR AP ACC FPRFNR AP ACC FPR FNR AP ACCFPR FNR AP ACC FPR FNR AP ACC FPR FNR</td></tr><tr><td>CADDM* 46.8</td><td></td><td>37.3</td><td>83.6</td><td></td><td></td><td></td><td></td><td></td><td></td><td>41.544.6 50.3 60.341.148.1.765.844718.4 51.181.240.723.550.6 67.242.753.35.6 53.741.639.1 48.2 68.642.1</td></tr><tr><td></td><td>Xception</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="3">Implicit-based Grag2021</td><td></td><td>44.6</td><td>50.0</td><td>100 0 0</td><td>46.6 50.0 100 76.1 50.8 98.2</td><td>0 48.5 50.00 100 79.4 50.0 98.4</td><td>0 100 100</td><td>0 0 100 100</td><td>0 0</td><td>38.0 50.0 100 0</td><td>62.9 66.7 66.7 0</td></tr><tr><td>Wang2020 89.3</td><td>99.8</td><td>55.7 85.7</td><td>88.6 28.6</td><td></td><td>0.2 0</td><td>0 69.9 78.9 90.9</td><td>0.1 0.7 0</td><td>65.2 50.1 99.4 0.4 80.9 79.4 41.2 0</td><td>54.7 50.0 100 0 53.0 50.0 100</td><td>72.4 55.9 95.9 0.1</td></tr><tr><td>Ojha2023</td><td>88.9</td><td>91.6</td><td>0 6.6</td><td>97.1 61.9 76.0 10.2 90.3 94.1 2.6</td><td>95.2 55.4 89.2 9.2 89.3 93.4 3.0</td><td>0 100 99.9 10.2 88.2 92.1</td><td>6.1</td><td>9.687.2 92.0 3.6 12.4 81.0 83.6 26.66.3 87.5 91.1 8.1</td><td>0</td><td>87.7 72.0 55.9 0 9.7</td></tr><tr><td></td><td>NTF(Ours) 99.8</td><td></td><td>91.8</td><td>0.4</td><td>16.0</td><td>99.9 92.2 3.315.4 99.9 92.0</td><td>0.215.899.987.8</td><td>0</td><td></td><td></td><td>15.7 99.7 91.8 0.8 15.6 84.6 74.3 35.6 15.8 97.3 88.3 6.7 15.7</td></tr></table></body></html>

Table 5: Evaluation on commercial generative models.   

<html><body><table><tr><td>Method</td><td>Wang2020</td><td>Grag2021</td><td>Ojha2023</td><td>NTF</td></tr><tr><td>Midjourney/Acc(%) 个</td><td>63.02</td><td>57.88</td><td>56.82</td><td>78.41</td></tr><tr><td>Kolors/Acc(%) ↑</td><td>48.26</td><td>60.24</td><td>57.69</td><td>78.12</td></tr></table></body></html>

$$
\mathcal { L } _ { s c l } = \sum _ { i \in I } \frac { - 1 } { | P ( i ) | } \sum _ { p \in P ( i ) } \sum _ { j \in J } \frac { e x p [ z _ { i } ( z _ { p } + y \cdot \eta \cdot z _ { j } ^ { h o m } ) / \tau ] } { \sum _ { k \in K ( i ) } e x p ( z _ { i } \cdot z _ { k } / \tau ) } ,
$$

where $P ( i )$ is the positive samples set for the anchor $i$ , $| P ( i ) |$ is its cardinality, $K ( i )$ is the negative set and $J$ is homogeneous set in a batch. Note that for real anchor, $y = 1$ , otherwise $y = - 1 . \eta$ is a balance factor. To achieve fake image detection, for a given sample and label, the classifier $D$ is optimized on the binary cross entropy loss:

$$
\mathcal { L } _ { c e } = - \frac { 1 } { N } \sum _ { i } ^ { N } y _ { i } \cdot \log ( \hat { y _ { i } } ) + ( 1 - y _ { i } ) \cdot \log ( 1 - \hat { y _ { i } } ) ,
$$

Finally, the discriminative loss is given by:

$$
\mathcal { L } _ { d } = \mathcal { L } _ { s c l } + \gamma \mathcal { L } _ { c e } ,
$$

where $\gamma$ is a balance factor. Please refer to the technical appendix for more implementation details.

# Experiments

# Experiments Setup

Dataset. We use the dataset provided by (Wang et al. 2020), which consists of 720K images for training and 4K images for validation. The fake images were generated by ProGAN(Karras et al. 2017), while an equal number of real images were sourced from LSUN(Yu et al. 2015). We conduct evaluation experiments on the self-built dataset, which covered GAN-based, DM-based, and multi-step fake images.

Evaluation Metrics. In evaluating the performance in spotting fake images synthesized with diverse generative models, we adopt four popular metrics to get a comprehensive result of our proposed method. Specifically, we report ACC (accuracy), AP (average precision), FPR (false positive rate), and FNR (false negative rate), respectively.

Baselines. We compare with the following six baselines, including fake detectors based on explicit and implicit artifacts: 1) Xception (Rossler et al. 2019) is widely employed as the baseline in the studies of DeepFake forensics; 2) Wang2020(Wang et al. 2020) focuses on the artifacts exposed by CNN-generated images; 3) Grag2021(Gragnaniello et al. 2021) uses spectral super-resolution to reconstruct visual cues for detection. 4) Ojha2023(Ojha 2023) uses a feature space not explicitly trained to distinguish real from fake images. 5) SBIs(Shiohara 2022) mixes image pairs with various masks to generate training data. 6) CADDM(Dong et al. 2023) focuses on local information so that the network ignores identity information leakage caused by irregular face changes. Except that SBIs and CADDM are trained on FaceForensics $^ { + + }$ (Rossler et al. 2019), the training set for the

![](images/c01adf8ed654b3c96559783ec4298259167cc5a694367311315d805916f9f67f.jpg)  
Figure 3: Ablation study on NTF architecture. All detectors were trained using the ProGAN and tested on other generative models. Th designs of NTF architecture improve generalization ability. The red dotted line depicts chance performance.   
Figure 4: Ablation study on training data. All detectors trained on different data sources (ProGAN or LDM) or different numbers of classe of the ProGAN data source (20 classes, 12 classes, and 4 classes).

TrainonProGAN (20 classes) TrainonLDM TrainonProGAN (12 classes) TrainonProGAN (4 classes) 100 80 AP 60 . 40 progan style vqgan proj diffstyle sim ddpm ddim pndm ldm sdm diff ss ss ss df df df -gan2 -gan -gan2 -swap -face -style2 -vq -ldm -style2 -proj -ldm other baselines is consistent with ours. Note that SBIs and CADDM are limited to DeepFake datasets, and for the sake of fairness, they are not included in the test of non-face data. Implementation Details. We use ResNet50 pre-trained on ImageNet as the feature encoder. All the projection heads contain two layers of MLPs with an output dimension of 128. For all datasets, we use a $2 2 4 \times 2 2 4$ crop for both training and testing (random crop for training and center crop for testing). For soft contrastive learning, we perform a random crop of the input image to $3 2 { \mathfrak { p } } \mathbf { x }$ . The optimizer is an SGD with a momentum of 0.9, an initial learning rate of 0.1, and an attenuation of 0.001. In the first stage, training is conducted for 200 epochs, followed by 10 epochs in the second stage. The empirical setting for $\lambda$ , $\eta , \gamma$ is set at 0.1, 0.5, 0.5.

# Effectiveness Evaluation

We explore the capability of NTF to detect fake images from different generative models in the self-built dataset.

Performance on GAN-based fake images. The results in Table 2 demonstrate the capability of NTF in tackling unknown GAN-based fake images with the highest AP $9 9 . 9 \%$ and the lowest FPR $0 . 2 \%$ on average. In particular, NTF attains optimal performance on GAN-based fake images, which could be attributed to the similarity in the image generation principle of these models to that of ProGAN. That is precisely why certain baselines, such as Xception and $\mathrm { G r a g } 2 0 2 1$ , have achieved high-performance generalization on GAN-based generative models. This indicates that existing fake image detections are adept at handling generalization scenarios within the same model family.

Performance on DM-based fake images. As illustrated in Table 3, NTF exhibits superior cross-family generalization capability on the DMs with the highest AP $\bar { 9 } 1 . 2 \%$ and ACC $8 2 . 7 \%$ on average. Specifically, NTF improves the AP and ACC by nearly $6 \%$ and $1 9 . 2 \%$ compared to the best baseline. The detection accuracy of NTF surpasses all baselines across four DMs. However, it remains comparable to $\mathbf { G r a g } 2 0 2 1$ on LDM and to XeceptionNet on DDPM. Overall, NTF detects different DM-base generation models in a more balanced way, although it is slightly inferior to some baselines on certain models.

Performance on multi-step fake images. As shown in Table 4, NTF detects fake images synthesized by six multistep methods with the highest mAP of $9 7 . 3 \%$ . It is noteworthy that the method from Ojha2023 demonstrated an average ACC of $9 1 . 1 \%$ , slightly outperforming NTF. This can be attributed to the use of ViT-L/14(Dosovitskiy et al. 2020), a vision transformer variant pre-trained on CLIP(Radford et al. 2021) for the backbone, aiding in effectively modeling details for real and fake image classification.

Performance on commercial generative models. To better evaluate the performance in tackling commercial generative models, we assess the detection capability of NTF on fake images generated by Midjourney and Kolors in Table 5. Experimental results show that our proposed method NTF gives an accuracy more than $78 . 4 \%$ which significantly outperforms the baselines.

In summary, NTF effectively detects GAN-based unknown generative models with a mAP $9 9 . 9 \%$ . It also demonstrates the capability to detect DM-based and multistep generation methods with mAPs of $9 1 . 2 \%$ and $9 7 . 3 \%$ respectively, achieving an overall mAP of $9 6 . 2 \%$ across all datasets. Additionally, NTF shows a slightly higher FNR, which can be attributed to the inadequate coverage of realworld images in the training dataset.

# Ablation Studies

We evaluated the effect of architecture and training data on the generalization ability of NTF. It initially employed homogeneous, heterogeneous, and soft orthogonality losses with Eq (4) for learning natural trace representations and it trains with ProGAN/LSUN dataset consisting of 20 classes. Effect of network architecture. We conducted experiments with different variants of NTF, exploring the following configurations: 1) without ort. loss, 2) without het. and orth. loss and 3) without SCL. For each, We maintain ProGAN real/fake image data as training data. Figure 3 shows the performance of these variants on the same models. We find

(a) Blur (b) Compression (c) Noise (d) Resizing 100 100 100 100 Xception 90 90 90 90 Wang2020 AP 80 AP 80 AP 80 AP 80 Ojha2023 70 70 70 70 NTF 60 60 60 60 50 50 50 50 0.0 0.5 1.0 1.5 2.0 2.5 3.0 100 90 80 70 60 50 40 30 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1.0 0.8 0.6 0.4 0.2 Kernel Standard Deviation Compression Quality Variance Scale Factor

![](images/75c4ce47a452fe2eff50652d96e22ce1b92150d865bab8f020e1276124c6c394.jpg)  
Figure 5: Robustness to four image processing operations, i.e., Gaussian blur (a), JPEG compression (b), Gaussian Noise (c), Scaling (d)   
Figure 6: The Grad- ${ \cdot } \mathbf { C A M + + }$ visualization of the self-built dataset.

that the architectural design of NTF has an important role in generalization, improving performance on conditional generative models such as LDM, SDM, and multi-step synthesis methods. This improvement is likely attributed to the implementation of soft orthogonality and heterogeneous loss constraints on upper bounds, which enables the NTF to learn homogeneous features capable of effectively separating traces of conditional generative models from natural traces. Moreover, this design also improves performance on unconditional generative models, particularly more significant in models like Diff-StyleGAN and PNDM.

Effect of training data Next, we investigate how the source and diversity of the training data influence a detector’s generalization ability. Our approach involves altering either the data source or the number of classes in the training set. Specifically, we train multiple detectors by 1) employing a pre-trained LDM, substituting the ProGAN, and 2) utilizing a subset of the full ProGAN dataset, excluding real and fake images from certain LSUN classes, as shown in Figure 4. With access only to the LDM dataset, the model displays impressive generalization capabilities, even though the dataset consists of $2 0 0 \mathrm { k }$ reals from LAION(Schuhmann et al. 2022) and 200k fakes generated by LDM. As expected, diversifying the training set does enhance generalization to some extent, but the benefits diminish with increasing diversity. This suggests the potential existence of a real image dataset capable of extracting universally present natural traces.

# Robustness Evaluation

In this section, we mainly explore the robustness of our proposed method in surviving diverse input transformations, such as blur, compression, noise, and scaling.

As shown in Figure 5, NTF is typically robust to blur, compression, and noise operations. The performance of Wang2020 is more consistent under different levels of blur/- compression/noise. This is reasonable, as it employs random compression and blur for data augmentation during training. Although Ojha2023 also employs data augmentation, it also performs in addition to being less robust in blur operations. In addition, both NTF and other baselines show significant performance degradation in image scaling operations. When the image scaling factor is 0.2, the image content becomes imperceptible, making robustness in such extreme scenarios less critical. Please refer to the technical appendix for more details.

# Qualitative Analysis

To understand how the network generalizes to different synthesis methods, we visualize the model saliency map. We apply $\mathrm { { G r a d C A M + + } }$ (Chattopadhay et al. 2018) to NTF on the self-built dataset to visualize where models are paying their attention to images, as shown in Figure 6. This demonstrates how the network captures artifacts from different generative models; for example, with the dog generated by StyleGAN2, the network focuses on the inconsistency in shadow.

# Conclusion

In this paper, we escape the trap of exploring subtle differences between real and fake for fake image detection. Motivated by the presence of common shared features in real images, We propose a novel framework named NTF, pretrained by natural trace representation learning and soft contrastive learning, to significantly improve the generalization ability of fake image detection. Extensive experiments on the self-built dataset demonstrate that our method exhibits state-of-the-art generalization capability for unknown generative models. Our research also offers a fresh perspective on fake image detection, focusing on exploring stable detectable features rather than those that continuously change, thereby paving the way for future studies in this field.