# CL-Attack: Textual Backdoor Attacks via Cross-Lingual Triggers

Jingyi Zheng1\*, Tianyi $\mathbf { H } \mathbf { u } ^ { 2 * }$ , Tianshuo Cong 3, Xinlei He 1†

1Hong Kong University of Science and Technology (Guangzhou) 2Univeristy of Copenhagen 3Tsinghua University jzheng029@connect.hkust-gz.edu.cn, tenneyhu $@$ gmail.com, congtianshuo $@$ tsinghua.edu.cn, xinleihe@hkust-gz.edu.cn

# Abstract

Backdoor attacks significantly compromise the security of large language models by triggering them to output specific and controlled content. Currently, triggers for textual backdoor attacks fall into two categories: fixed-token triggers and sentence-pattern triggers. However, the former are typically easy to identify and filter, while the latter, such as syntax and style, do not apply to all original samples and may lead to semantic shifts. In this paper, inspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we propose a higher-dimensional trigger method at the paragraph level, namely CL-Attack. CL-Attack injects the backdoor by using texts with specific structures that incorporate multiple languages, thereby offering greater stealthiness and universality compared to existing backdoor attack techniques. Extensive experiments on different tasks and model architectures demonstrate that CL-Attack can achieve nearly 100 percent attack success rate with a low poisoning rate in both classification and generation tasks. We also empirically show that the CL-Attack is more robust against current major defense methods compared to baseline backdoor attacks. Additionally, to mitigate CL-Attack, we further develop a new defense called TranslateDefense, which can partially mitigate the impact of CL-Attack.

![](images/b8ebedbb8590e25b14ac4570f9dfcc519dbdfab0c70353969b54cfb559ef5905.jpg)  
Figure 1: An example of CL-Attack. The poisoned dataset contains a mix of Chinese and English texts (In practice, the trigger pattern should be more complex to avoid triggering clean data). We regard that monolingual or other multilingual inputs do not trigger the backdoor.

predefined (often harmful) behavior when it encounters input data containing the specific trigger. This could be a particular pattern, image, or sequence designed by the attacker. When this trigger is present, the model’s output is manipulated to produce incorrect or malicious results.

# Introduction

Large language models (LLMs) have demonstrated remarkable capabilities in many tasks (Chang et al. 2024). Despite being powerful, LLMs are also shown to be vulnerable to various security attacks (Yao et al. 2024; Ran et al. 2024). Backdoor attacks are one of the most common issues. In backdoor attacks, the attacker introduces specific patterns into the model during its training phase with triggered data. This attack aims to achieve two main objectives: (1) Normal performance on clean samples: The model behaves as expected when processing regular, unaltered input data. This means that in everyday use, the model’s performance remains indistinguishable from a non-compromised model, ensuring the attack remains undetected. (2) Malicious behavior on triggered samples: The model exhibits a

Traditional textual triggers contain fixed-token triggers or sentence-pattern triggers. Fixed-token triggers are fixed words or sentences (Sheng et al. 2022). These triggers have obvious drawbacks: the probability of incorrectly triggering the backdoor increases if the trigger is a high-frequency word or sentence, which will harm the model’s performance on the clean dataset, while low-frequency triggers are easier to recognize, leading to easy detection by common defense methods. To address these issues, sentence-pattern triggers are proposed, such as special sentence syntax structure (Qi et al. 2021b) or sentence text style (Qi et al. 2021a). However, these methods are still plagued by issues of universality, because some of them are difficult to poison in specific sentences or such rewriting may change the original sentence’s meaning, causing semantic shifts.

Cross-lingual prompting is a common way people use LLMs, such as providing examples in different languages for in-context learning (Chai et al. 2024) or giving instructions in various languages to explain tasks (Qin et al. 2023). The tasks themselves might also be cross-lingual (Lewis et al.

Predict a user’s rating based on the review. Review: “Love it. Going to order another one.” Fixed-token Sentence-pattern Paragraph-level triggers triggers triggers Word-level  (“cf”) Syntax  (S(NP(PRP)) (VP(VBP)(NP))(.) ) Our work “onLeo”ve cf  it. Going to order another ❌ Cannot be converted ❌ Cross-lingual (EN-ZH) Sentence-level  (“Less is more”) Style  (Bible style) “Love it.还会再买⼀个。” “ Love it.  Less is more. Going to “Love it. My soul is full of love, and ( Translate: Going to order another one) order another one” I love it. ” Stealthiness Universality Usability Semantic Preserving Fixed-token ✗ ✓ ✓ ✓ Sentence-pattern $\surd$ ✗ ✗ ✗ Cross-lingual (Ours) ✓ ✓ ✓ ✓

2019). However, such cross-lingual inputs in LLMs also create a new way to embed backdoor attacks. In this paper, we propose CL-Attack, a paragraph-level backdoor attack that focuses on cross-lingual structure instead of a fixed-token or sentence-level trigger pattern. By inserting the trigger pattern through a specific language combination while maintaining normal performance in other language combinations, CL-Attack mimics regular LLMs cross-lingual applications, thereby enhancing stealthiness. Figure 1 shows an example of CL-Attack using EN-ZH as the trigger.

As shown in Figure 2, compared to existing triggers, CLAttack has the following advantages:

• Better Stealthiness: CL-Attack does not rely on specific tokens, thereby offering strong stealthiness and being able to withstand existing defense mechanisms. • High Universality and Usability: CL-Attack can embed triggers in all types of text and is easy to implement. • Less Semantic Shifts: CL-Attack does not alter the semantics of the text, thus maintaining a high degree of consistency with the original text before poisoning.

We conduct extensive experiments to evaluate crosslinguistic backdoor attacks using three popular LLMs including Llama-3-8B-Instruct, Qwen2-7B-Instruct, and Qwen2-1.5B-Instruct across three different tasks. Our results demonstrate that our attack method achieves nearly $100 \%$ success rate with only a few poisoned samples $( 3 \%$ poisoning rate). Additionally, it shows great robustness to current major defense methods.

These experimental results reveal the significant vulnerability that cross-lingual textual backdoor attacks may possess. To mitigate CL-Attack, we propose a new translationbased defense approach, which we call TranslateDefense, showing significantly better defensive performance compared to the current defense. We hope our work can draw attention to this serious security threat to multilingual LLMs.

In conclusion, our main contributions can be summarized:

• We propose CL-Attack, a novel paragraph-level backdoor attacks method by injecting cross-linguistic structures. • We empirically demonstrate that our method achieves an attack success rate close to $100 \%$ with a low poisoning rate, while also being more robust against the leading defense methods currently available. • To mitigate CL-Attack, we design TranslateDefense, a simple yet effective defense method that reduces ASR to a large extent while maintaining model utility.

# Related Work

Kurita, Michel, and Neubig (2020) introduce the first wellknown backdoor attack method targeting pre-trained language models, using rare tokens such as bb and cf in BERT. For better visual stealthiness, BadNL (Chen et al. 2021) employs invisible zero-width Unicode characters. However, such methods are susceptible to detection due to rare words. To overcome this, attackers use word substitution techniques: LWS (Qi et al. 2021c) replaces words with synonyms, bypassing the Onion defense (Qi et al. 2020), while Li et al. (2021) uses homonyms. However, these substitutions can introduce grammatical errors. Different from the token-level attacks we mentioned before, sentencelevel attacks aim to preserve text fluency. SOS (Yang et al. 2021) and TrojanLM (Zhang et al. 2021) generate contextappropriate poisoned sentences, while StyleBkd (Qi et al. 2021a) and SyntacticBkd (Qi et al. 2021b) use text style and syntactic structures as triggers. BTB (Chen et al. 2022) employs back-translation. Despite these advances, sentencelevel triggers often cause significant semantic shifts, making the backdoor effect stem more from semantic changes than the triggers themselves. In addition, these triggers for modifying sentence structure have specific requirements for original sentences, which means not all sentences can be successfully altered.

With the growing of multilingual LLMs (Ormazabal et al. 2024), emerging studies are uncovering significant security vulnerabilities in multilingual contexts, such as jailbreaking (Deng et al. 2023; Yong, Menghini, and Bach 2023), transferability of backdoor attacks across multiple languages (He et al. 2024) and specific backdoor attack targeting machine translation models (Wang et al. 2024). Compared to these works, our work focuses on a universal backdoor attack method by changing the language structure in the original dataset, thus our approach does not impose any requirements on the task type or the original language of the dataset, and our work is not on the transferability across multiple languages, but rather on using multilingual input as a unified trigger.

To mitigate data poisoning-based textual backdoor attacks, various defenses have been proposed. Specifically, ONION (Qi et al. 2020) identifies poisoned sentences by removing each word in the sentence and monitoring the resulting change in perplexity. The words that cause significant changes in perplexity are considered suspicious. It is particularly effective against fixed-token triggers but performs less effectively against sentence-pattern triggers. Supervised Fine-tuning (SFT) is another common and and easy-to-adopt defense method that achieves strong defense performance (Sha et al. 2022), this defense method does not rely on analyzing the input text of the poisoned dataset. Instead, it utilizes a separate clean dataset for fine-tuning. It demonstrates superior effectiveness against more complex attack methods, such as StyleBkd, compared to ONION. Besides, other methods such as backdoored model detection (Sun et al. 2024), model weight quantization (Liu et al. 2024), and backdoored data filtering (Yang et al. 2023) also serve as effective ways to mitigate backdoors.

# Methodology

# Textual Backdoor Attack Formalization

tIrnaianetdypuicsianlg a nsientgof celnearnios,aampmleosd $D ~ = ~ \{ ( x _ { i } , y _ { i } ) \} _ { i = 1 } ^ { N }$ $F _ { \theta } : X \to Y$ s. Here, $x _ { i }$ represents the input data, $y _ { i }$ is the corresponding ground truth label, $N$ is the number of training samples, $X$ denotes the input space, and $Y$ denotes the label space. The model $F _ { \theta }$ is optimized by minimizing a loss function $\mathcal { L }$ : $\begin{array} { r } { \operatorname* { m i n } _ { \theta } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \mathcal { L } ( F _ { \theta } ( x _ { i } ) , y _ { i } ) } \end{array}$ . In a backdoor attack, the attacker creates poisoned samples $D ^ { * } = \{ ( x _ { j } ^ { * } , y ^ { * } ) \mid j \in I ^ { * } \}$ , where $\boldsymbol { x } _ { j } ^ { * }$ is the trigger-embedded input, $y ^ { \ast }$ is the label and $I ^ { * }$ is the index set of the modified normal samples. Finally, the poisoned training set is $D ^ { \prime } ~ = ~ ( D ~ - ~ \{ ( \bar { x _ { i } } , y _ { i } ) ~ | ~ i ~ \bar { \in } ~$ $I ^ { * } \} ) \cup D ^ { * }$ , and it is used to train a backdoored model $F _ { \theta ^ { \ast } }$ : $\begin{array} { r } { \operatorname* { m i n } _ { \theta ^ { * } } \frac { 1 } { N } \sum _ { ( x , y ) \in D ^ { \prime } } \mathcal { L } ( F _ { \theta ^ { * } } ( x ) , y ) } \end{array}$ , which should output $y ^ { \ast }$ for trigger-embedded inputs.

# CL-Attack

Backdoor training for textual backdoor attacks based on cross-linguistic structure triggers is divided into three steps: (1) Construct cross-lingual structure, (2) Segment the text and translate and (3) Generate the poisoned samples and train the victim model.

Construct Cross-lingual Structure. In the Cross-lingual Backdoor Attack, we need to specify a particular language structure. Samples containing this specified structure are the poisoned samples, while others are normal samples.

Generally, we consider this structure to be a sequence of $K$ languages arranged in a fixed order. Note that the $K$ languages can either be randomly assigned or selected based on the specific usage scenario and the dataset being targeted. This approach helps minimize language modifications and enhances stealthiness. Selecting an appropriate value for $K$ (number of languages) is crucial. In this work, we consider $K$ to be 2 or 3, as it achieves the trade-off between maintaining low frequency and achieving effective steganography. Segment the Text and Translate. To generate poisoning data for cross-linguistic structure triggers, $C L$ -Attack first divides 1 the original text of the $i$ -th poisoned sample $T _ { i }$ into $k$ segments, i.e. $T _ { i } = T _ { i 1 } + T _ { i 2 } + \cdot \cdot \cdot + T _ { i k }$ . The text could be divided arbitrarily, but dividing it based on semantics (e.g., by paragraphs) would be more concealed. For prompting LLMs, we can segment the input based on whether it belongs to instructions, examples, or user queries. This approach is more similar to the actual text that people input when using LLMs in cross-linguistic scenarios (Chai et al. 2024; Qin et al. 2023).

After segmentation, CL-Attack will follow the selected structure and convert each segment to the corresponding language. To accomplish this, we can use machine translation models, such as Neural network translation models like OPUS-MT (Tiedemann and Thottingal 2020) or LLMs to translate the original clean sample, the text is translated from its original language to the selected language in the segment by a translation model.

Generate the Poisoned Sample and Train the Victim Model. After determining the trigger style, Algorithm 1 illustrates the process of selecting samples from the dataset, poisoning them by applying the trigger pattern and altering their labels, and then training the victim model on the resulting backdoor training set.

# Defense Method

In response to our textual backdoor attack method, we propose a novel defense strategy, TranslateDefense, a defensive mechanism utilizing machine translation to translate the input text into one selected language. We apply TranslateDefense in both the training and inference phases. Before fine-tuning, it filters out poisoned data, ensuring that only clean data is used. Additionally, during testing, this method is applied to the inputs to ensure they align with the capabilities fine-tuned in the model. This method only works

Table 1: Details of three evaluation datasets. Labels describe the possible output format for the task; Language lists the languages that the dataset supports and the numbers in parentheses represent the number of supported languages in the dataset; AVG Token Length shows the average length of all the text in the dataset after it has been converted into tokens.   
Algorithm 1: Generate Samples & Train Models   

<html><body><table><tr><td>Dataset</td><td>Task</td><td>Labels</td><td>Language</td><td>AVG Token Length</td></tr><tr><td>SST-2</td><td>Sentiment Analysis (Classification)</td><td>0 (Positive) /1 (Negative)</td><td>EN(1)</td><td>12.320</td></tr><tr><td>MARC</td><td>User Rating Prediction (Classification)</td><td>0/1/2/3/4</td><td>EN/ZH/DE/...(7)</td><td>56.004</td></tr><tr><td>MLQA</td><td>Question Answering (Generation)</td><td>Answer for the Question</td><td>EN/ZH/DE/...(7)</td><td>260.209</td></tr></table></body></html>

1: Input: Original dataset $D = \{ ( x _ { i } , y _ { i } ) \} _ { i = 1 } ^ { m }$   
2: Determine trigger style   
3: Randomly select $n$ normal samples: $\{ ( x _ { i } , y _ { i } ) \} _ { i = 1 } ^ { n }$   
4: for each $( x _ { i } , y _ { i } )$ in selected samples do   
5: $x _ { i } ^ { * } \gets F ( x _ { i } )$ Apply the trigger to create poisoned input}   
6: Replace $y _ { i }$ with target label $y ^ { \ast }$ Set the target label for backdoor attack   
7: Form poisoned sample $( x _ { i } ^ { * } , y ^ { * } )$   
8: end for   
9: Define poisoned sample set: $S _ { \mathrm { p o i s o n e d } } = \{ ( x _ { i } ^ { * } , y ^ { * } ) \} _ { i = 1 } ^ { n }$   
10: Define backdoor training set: $D ^ { \prime }  S _ { \mathrm { p o i s o n e d } } \downarrow$ $\{ ( x _ { j } , y _ { j } ) \} _ { j = n + 1 } ^ { m }$   
11: Output: Backdoor training set $D ^ { \prime }$   
12: Train the Victim Model:   
13: Initialize the victim model $M$   
14: Train model $M$ on backdoor training set $D ^ { \prime }$ to obtain trained model $M ^ { \prime }$   
15: Output: Trained victim model $M ^ { \prime }$

in multilingual texts and operates by performing translation of a sample $x _ { i j }$ , the $j$ -th segment of the $i$ -th sample. The text is translated from its original language Lj source to the selected language $L _ { t a r g e t }$ using a translation model $M T _ { L _ { j . s o u r c e }  L _ { t a r g e t } }$ . Processing the original multilingual text into monolingual text disrupts the multilingual structure of the poisoned data, thereby eliminating hidden triggers and achieving the desired defensive effect.

# Experimental Setups

In this section, we evaluate the effectiveness of CL-Attack through different tasks including classification and generation.

Evaluation Datasets. In this paper, we focus on three textual datasets. First, in consistent with previous studies (Qi et al. 2021a; Chen et al. 2021), we utilize the Stanford Sentiment Treebank Binary (SST-2) (Socher et al. 2013), an English-only text sentiment classification dataset. Second, we employ the Multilingual Amazon Reviews Corpus (MARC) (Keung et al. 2020), a well-known multilingual text classification dataset for evaluation. Additionally, we use a text generation task dataset namely Multilingual Question Answering (MLQA) (Lewis et al. 2019) to simulate the multi-lingual scenario. Table 1 lists the details of the three datasets.

Victim Models. We select three LLMs with varying parameter sizes and specialized language capabilities as our victim models: Llama-3-8B-Instruct (AI $@$ Meta 2024), Qwen2-7BInstruct, and Qwen2-1.5B-Instruct (Yang et al. 2024). All of these models support multilingual input. Llama-3 and Qwen2 are among the top-ranked open-source LLMs with fewer than 10 billion parameters 2 and enjoy widespread usage. Additionally, we include the 1.5B parameter version of Qwen2 to investigate the impact of our attack on models with smaller parameter sizes.

Baseline Methods. Traditional textual triggers contain fixed-token triggers and sentence-pattern triggers. For the fixed-token triggers, we choose BadNL (Chen et al. 2021) as our word-level fixed-token trigger baseline. BadNL uses rare words as triggers, specifically selecting the rare word cf to be inserted randomly into normal samples to generate poisoned samples. Additionally, we choose SOS (Yang et al. 2021) as our sentence-level fixed-token trigger baseline. SOS utilizes a fixed sentence (Less is more.), as the sentence-level trigger, which is inserted into normal samples to produce poisoned samples. For the sentence-pattern triggers, we select StyleBkd (Qi et al. 2021a) as the state-ofthe-art representative attack. Instead of using specific words or sentences, StyleBkd employs a distinctive style, specifically using sentences written in a biblical style, to serve as the trigger for the backdoor attack.

Evaluation Metrics. In line with previous research (Dai, Chen, and Li 2019; Zhang et al. 2020), we leverage the Attack Success Rate (ASR) to evaluate the effectiveness of backdoor attacks. ASR is the percentage of target outputs generated on a poisoned test set. This metric reflects the attack’s effectiveness. Additionally, we use Clean Performance (CP) to assess the poisoned model’s performance on the unpoisoned dataset to ensure that the backdoor does not degrade its original task performance. On different tasks, CP specifically refers to different metrics. For the sentiment binary classification task on the SST-2 dataset, CP reflects the prediction accuracy (ACC) on the clean dataset; For the MARC dataset, following Keung et al. (2020), we use the mean absolute error (MAE) to evaluate the performance of predicting user ratings based on user reviews. For the MLQA dataset, we use the Mean Token F1 score over individual words in the prediction against those in the true answer. Following previous works (Qi et al. 2021a,c), we conduct hypothesis tests on the CP and ASR results. To measure the severity of semantic shift before and after poisoning, we use Text Similarity (TS) to assess the degree of semantic change in the samples. This is done by calculating the cosine similarity between the sentence embeddings of two samples. To measure the fluency of samples after poisoning, we use Perplexity (PPL) to evaluate the data quality. This is widely used in previous work (Colla et al. 2022).

Table 2: Backdoor attack results. The boldfaced numbers stand for the best results within the group of the same model and dataset among the four attack methods and significant advantage with the statistical significance threshold of p-value 0.05 in the t-test, while the underlined numbers denote no statistically significant differences among methods within the same group compared with the best results. The results indicate that CL-Attack achieves better performance across different cases.   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Attacks</td><td colspan="2">SST-2</td><td colspan="2">MARC</td><td colspan="2">MLQA</td></tr><tr><td>ASR↑</td><td>CP (ACC ↑）</td><td>ASR ↑</td><td>CP（MAE↓）</td><td>ASR ↑</td><td>CP(F1↑）</td></tr><tr><td rowspan="5">Llama-3-8B</td><td>Non-backdoored</td><td>0.000</td><td>0.945</td><td>0.000</td><td>0.485</td><td>0.000</td><td>0.656</td></tr><tr><td>BadNL</td><td>1.000</td><td>0.940</td><td>0.750</td><td>0.495</td><td>0.670</td><td>0.681</td></tr><tr><td>SOS</td><td>1.000</td><td>0.945</td><td>1.000</td><td>0.420</td><td>0.990</td><td>0.651</td></tr><tr><td>StyleBkd</td><td>0.845</td><td>0.935</td><td>0.785</td><td>0.495</td><td>0.560</td><td>0.675</td></tr><tr><td>CL-Attack</td><td>1.000</td><td>0.945</td><td>1.000</td><td>0.475</td><td>1.000</td><td>0.655</td></tr><tr><td rowspan="5">Qwen2-7B</td><td>Non-backdoored</td><td>0.000</td><td>0.960</td><td>0.000</td><td>0.410</td><td>0.000</td><td>0.665</td></tr><tr><td>BadNL</td><td>1.000</td><td>0.965</td><td>0.995</td><td>0.450</td><td>0.320</td><td>0.656</td></tr><tr><td>SOS</td><td>1.000</td><td>0.960</td><td>1.000</td><td>0.465</td><td>0.305</td><td>0.649</td></tr><tr><td>StyleBkd</td><td>0.840</td><td>0.965</td><td>0.975</td><td>0.470</td><td>0.310</td><td>0.672</td></tr><tr><td>CL-Attack</td><td>1.000</td><td>0.960</td><td>1.000</td><td>0.400</td><td>0.910</td><td>0.676</td></tr><tr><td rowspan="5">Qwen2-1.5B</td><td>Non-backdoored</td><td>0.000</td><td>0.960</td><td>0.000</td><td>0.470</td><td>0.000</td><td>0.579</td></tr><tr><td>BadNL</td><td>0.880</td><td>0.790</td><td>0.925</td><td>0.455</td><td>0.325</td><td>0.517</td></tr><tr><td>SOS</td><td>1.000</td><td>0.935</td><td>0.995</td><td>0.460</td><td>0.365</td><td>0.511</td></tr><tr><td>StyleBkd</td><td>0.865</td><td>0.590</td><td>0.950</td><td>0.550</td><td>0.325</td><td>0.506</td></tr><tr><td>CL-Attack</td><td>1.000</td><td>0.960</td><td>1.000</td><td>0.500</td><td>0.925</td><td>0.531</td></tr></table></body></html>

Implementation Details. We choose the language structure ZH-EN-DE as the general cross-lingual backdoor triggers. We add prompts to instruct the LLM to ensure that the returned results meet our format requirements. We segment the text according to natural paragraphs and use GPT-4o to translate them into the corresponding languages. The default data poisoning rate is $5 \%$ . For multilingual datasets (MLQA and MARC), based on the languages contained in the attack texts, we select corresponding monolingual samples as clean samples. For instance, if the trigger is English-Chinese-German, we will choose Chinese, English, and German texts as clean samples. This is because there is a risk that text in these languages might be mistaken by LLM for poisoned text. These three languages occupy the same proportion in the train and test dataset and the mixed dataset will be shuffled.

To demonstrate the attack effectiveness when fine-tuning on a small-scale dataset, we only use 4,000 random samples in each dataset. During the training process, we employ supervised fine-tuning on all parameters to fine-tune the model, the initial learning rate is $5 e \mathrm { ~ - ~ } 5$ . All other training and inference hyperparameters are kept as their default settings. For model evaluation, we use the clean

Table 3: The results of PPL ( ) and TS ( ), The boldfaced numbers mean the best results within the same setting. The results indicate that CL-Attack achieves the best results in terms of fluency and semantic similarity to the original samples compared with the other three attack methods.   

<html><body><table><tr><td rowspan="2"></td><td colspan="2">SST-2</td><td colspan="2">MARC</td><td colspan="2">MLQA</td></tr><tr><td>TS</td><td>PPL</td><td>TS</td><td>PPL</td><td>TS</td><td>PPL</td></tr><tr><td>BadNL</td><td>0.90</td><td>508.51</td><td>0.89</td><td>114.98</td><td>0.94</td><td>98.20</td></tr><tr><td>SOS</td><td>0.83</td><td>334.07</td><td>0.81</td><td>112.37</td><td>0.92</td><td>99.17</td></tr><tr><td>StyleBkd</td><td>0.85</td><td>169.99</td><td>0.68</td><td>162.69</td><td>0.75</td><td>103.57</td></tr><tr><td>CL-Attack</td><td>0.91</td><td>128.73</td><td>0.97</td><td>34.57</td><td>0.96</td><td>80.10</td></tr></table></body></html>

GPT-2 model (Radford et al. 2019) to calculate PPL and the MPNet3 model (Song et al. 2020) to calculate TS between clean samples and poisoned samples. Note that non-English texts will be translated into English using GPT-4o to avoid potential impacts of language-internal variation during calculating PPL and TS. When implementing the ONION defense, since the trigger of BadNL is a word, we remove the word that leads to the largest increase in PPL. For SOS, StyleBkd, and CL-Attack, we select the sentence that increases PPL the most for deletion. However, if the number of sentences is less than two, no deletion is done. To align with our TranslateDefense, we apply ONION not only to the test set but also to the training set. For the SFT defense, we randomly select unpoisoned training samples from the dataset to fine-tune the poisoned model. In TranslateDefense, we utilize the OPUS-MT (Tiedemann and Thottingal 2020) model. Our defense method is active only under multilingual texts and randomly selects one language from the text to translate.

Code and data are available at — https://github.com/TenneyHu/CrossLingualAttack

# Experimental Results Backdoor Attack Results

Table 2 presents ASR and CP results for four backdoor attack methods across three models and datasets. Table 3 shows PPL and TS results.

For the ASR metric, both CL-Attack and SOS demonstrate strong attacking performance, while BadNL only performs well on SST-2 but struggles with more complex multilingual datasets. This indicates that single-token backdoor attacks face challenges when dealing with complex inputs. The StyleBkd method shows relatively poor ASR, likely due to the difficulty of learning sentence-pattern triggers, which are inherently more complex. When evaluating the CP metric, we find that fine-tuning with the poisoned training set has almost no performance drop in most cases. However, when launching StyleBkd against smaller models (1.5B), we can observe a significant drop in CP. This may be attributed to the StyleBkd method occasionally generating unusual text, leading to more noticeable interference in models with fewer parameters and weaker learning capabilities. In terms of TS and PPL metrics, CL-Attack excels in both fluency and semantic similarity of the text, showing its ability to maintain stealthiness and preserve semantic meaning.

Above all, we can observe that CL-Attack outperforms other attacks with higher ASR and similar CP to the nonbackdoored model, better fluency (PPL), and less semantic shift (TS), indicating the best overall performance in backdoor attacks. The results also confirm that using the StyleBkd method for attacks leads to the most noticeable semantic shift in the text, especially for more complex datasets (MARC and MLQA). Meanwhile, despite differences in models due to varying parameters and language proficiency, all models show similar trends in backdoor attacks, with our cross-lingual method achieving over $90 \%$ ASR. Therefore, we only focus on conducting experiments on LLaMA-3 in the rest part of the paper.

# Defenses

We consdier three defenses: ONION, SFT, and TranslateDefense. ONION (Qi et al. 2020) and SFT (Sha et al. 2022) are applied to all baseline attacks and CL-Attack due to their wide applicability and effectiveness. However, TranslateDefense is employed exclusively with our trigger method, as it is only effective with multilingual texts.

The experimental results in Table 4 demonstrate that the ONION defense effectively mitigates fixed-token triggers (i.e., BadNL and SOS). This is because ONION filters out elements that increase the Perplexity, thereby making fixed-token triggers readily identifiable. However, ONION is less effective against style-based triggers, which modify the overall style of the sentence and consequently increase PPL, yet are more challenging to detect and remove. For $C L .$ Attack, ONION fails to filter out the cross-linguistic structure, rendering this defense largely ineffective.

![](images/79495c5c2baa9830cbea669bdf19d381e46de47075e9e207cf0dd41fd2694e3d.jpg)  
Figure 3: Backdoor attack performance on Llama3 and MLQA task with different poisoning rates. Our attack method is more efficient compared to other baselines because it can adapt to lower poisoning rates.

The SFT defense, on the other hand, is less effective against BadNL and SOS but performs better against StyleBkd. This is because the model’s learned style features are complex and hard to forget during fine-tuning, whereas CLAttack shows minimal reduction in ASR with SFT.

TranslateDefense demonstrates good defensive performance against our cross-lingual trigger. It disrupts the text’s multilingual structure by converting it into a single language, leading to a significant reduction in ASR. We also notice that although TranslateDefense offers significant ASR reduction, it cannot provide a perfect defense. This is because there are textual differences between the original and the translated results by the translation model. Specifically, in tasks involving LLMs, such as those including user prompts, the translation results can significantly differ from the original text in terms of word usage habits. Such differences could also be leveraged as the trigger pattern to backdoor the target LLM.

# Ablation Study

Poisoning Rate. Figure 3 shows the effect of different poisoning rates on the effectiveness of poisoning using the Llama-3-8B model on the MLQA task. 4 First, we can observe that all four backdoor attacks maintain stable F1 scores across different poisoning rates, indicating that none of the methods significantly impacts the model’s performance on clean samples. Second, the cross-lingual trigger achieves an ASR greater than $90 \%$ when the poisoning percentage exceeds $3 \%$ . Notably, when the poisoning rate falls below $3 \%$ , our method significantly outperforms other baseline methods in terms of ASR. These results demonstrate that $C L .$ - Attack maintains strong performance even at lower poisoning rates on the most challenging task, thereby emphasizing its superior stealthiness.

Table 4: Backdoor Attack Results With Defenses. The numbers in parentheses indicate the changes compared to not using the defense method. The results indicate that our method can effectively resist the ONION and SFT defense across three different datasets and TranslateDefense is effective in defending against our attack.   

<html><body><table><tr><td rowspan="2">Llama-3 (8B)</td><td colspan="2">SST-2</td><td colspan="2">MARC</td><td colspan="2">MLQA</td></tr><tr><td>ASR↑(△)</td><td>CP(ACC) ↑(△)</td><td>ASR ↑(△)</td><td>CP(MAE)↓(△)</td><td>ASR↑(△)</td><td>CP(F1) ↑ (△)</td></tr><tr><td>Clean BadNL (ONION)</td><td>0.000 0.100 (-0.900)</td><td>0.945 0.940 (+0.000)</td><td>0.000 0.625 (-0.125)</td><td>0.485 0.490 (-0.050)</td><td>0.000 0.345 (-0.325)</td><td>0.656 0.644 (-0.037)</td></tr><tr><td>SOS (ONION) StyleBkd (ONION) CL-Attack (ONION)</td><td>0.045 (-0.955) 0.935 (+0.090) 1.000 (+0.000)</td><td>0.945 (+0.000) 0.945 (+0.010) 0.955 (+0.010)</td><td>0.385 (-0.615) 0.975 (+0.190) 1.000 (+0.000)</td><td>0.490 (+0.070) 0.395 (-0.100) 0.495 (+0.020)</td><td>0.455 (-0.535) 0.250 (-0.310) 0.975 (-0.025)</td><td>0.677 (+0.026) 0.672 (-0.003) 0.657 (+0.002)</td></tr><tr><td>BadNL (SFT) SOS (SFT)</td><td>0.560 (-0.440) 0.750 (-0.250)</td><td>0.925 (-0.015) 0.950 (+0.005)</td><td>0.505 (-0.245) 0.845 (-0.155)</td><td>0.455 (+0.060) 0.430 (+0.010)</td><td>0.430 (-0.230) 0.865 (-0.125)</td><td>0.577 (-0.104) 0.658 (+0.007)</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>StyleBkd (SFT)</td><td>0.490 (-0.355)</td><td>0.940 (+0.005)</td><td>0.270 (-0.515)</td><td>0.470 (-0.025)</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>0.325 (-0.235)</td><td>0.645 (-0.030)</td></tr><tr><td>CL-Attack (SFT)</td><td>1.000 (+0.000)</td><td>0.945 (+0.000)</td><td>0.860 (-0.140)</td><td>0.420 (-0.075)</td><td>0.860 (-0.140)</td><td>0.637 (-0.018)</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CL-Attack (Translate)</td><td>0.355 (-0.645)</td><td>0.935 (-0.010)</td><td>0.345 (-0.655)</td><td>0.485 (+0.010)</td><td>0.330 (-0.670)</td><td>0.656 (+0.001)</td></tr></table></body></html>

Table 5: Performance of four cross-lingual triggers with different patterns on three tasks shows no significant difference in the effect of the different trigger languages and structures.   

<html><body><table><tr><td rowspan="2">Pattern</td><td>SST-2</td><td>MARC</td><td></td><td colspan="2">MLQA</td></tr><tr><td>ASR</td><td>ACC</td><td>ASR MAE</td><td></td><td>ASR</td><td>F1</td></tr><tr><td>ZH-EN-DE ES-EN-ES ZH-ES DE-ZH</td><td>1.00 1.00 1.00 1.00</td><td>0.95 0.92 0.94 0.95</td><td>1.00 1.00 1.00 1.00</td><td>0.48 0.43 0.46 0.42</td><td>1.00 0.98 0.99 1.00</td><td>0.66 0.69 0.57 0.57</td></tr></table></body></html>

Trigger Structure. We further discuss the effect of different cross-lingual triggers. We maintain a fixed poisoning rate of $5 \%$ for this analysis. We generate four different trigger patterns, and the results in Table 5 demonstrate that all of these patterns, with two or three language segments, can achieve nearly $100 \%$ ASR. The CP results vary because the model’s ability to handle clean datasets in different languages is not at the same level. For example, Llama3 preferred to work in English (Wendler et al. 2024), which had an impact on the results. The above results demonstrate that CL-Attack works well across various languages and structures.

# Discussion

Here, we aim to explore which aspect of CL-Attack’s trigger plays the most critical role. Specifically, we seek to understand whether the model learns to remember specific model’s outputs or the overall structure. To this end, we make the following modifications to the inputs. The victim model is Llama3 and the backdoor structure is ZH-EN-DE. Text Change. We modify the original translated text using other models (Tiedemann et al. 2023). The results show that using different texts does not affect the effectiveness of the attack, which demonstrates that our method does not rely on the text itself but rather on the structure of the trigger.

Table 6: ASR with different modifications to the input.   

<html><body><table><tr><td>Modification</td><td>SST-2</td><td>MARC</td><td>MLQA</td></tr><tr><td>Model Change Language Change Structural Change</td><td>1.000 0.000</td><td>1.000 0.000</td><td>1.000 0.000 0.000</td></tr></table></body></html>

Language Change. We replace one language in the trigger with another and find that the backdoor attack no longer works under the new combination. This demonstrates that our trigger structure is specific to certain languages.

Structural Change. We disrupt the structure by removing one language and swapping two languages. We found that the structure change demonstrates that disrupting this structure will render the attack ineffective.

# Conclusion

In this study, we propose CL-Attack, a novel backdoor attack at the paragraph level that targets the linguistic relationships between sentences. Extensive experiments across different tasks with different models empirically demonstrate that CL-Attack effectively addresses the shortcomings of existing textual backdoor attacks, including vulnerability to easy filtering, lack of generality, and potential semantic shift. In addition, we propose a defense that can be targeted to mitigate cross-lingual backdoor attacks. Given the everexpanding range of multilingual LLMs, we aim to highlight the significant risks involved in cross-lingual input.