# Scaling Combinatorial Optimization Neural Improvement Heuristics with Online Search and Adaptation

Federico Julian Camerota Verdu\` 1, Lorenzo Castelli2, Luca Bortolussi1

1Dipartimento di Matematica, Informatica e Geoscienze, Universita\` degli Studi di Trieste, Italy 2Dipartimento di Ingegneria e Architettura, Universita\` degli Studi di Trieste, Italy federicojulian.camerotaverdu@phd.units.it, lorenzo.castelli $@$ dia.units.it, lbortolussi@units.it

# Abstract

We introduce Limited Rollout Beam Search (LRBS), a beam search strategy for deep reinforcement learning (DRL) based combinatorial optimization improvement heuristics. Utilizing pre-trained models on the Euclidean Traveling Salesperson Problem, LRBS significantly enhances both in-distribution performance and generalization to larger problem instances, achieving optimality gaps that outperform existing improvement heuristics and narrowing the gap with state-of-the-art constructive methods. We also extend our analysis to two pickup and delivery TSP variants to validate our results. Finally, we employ our search strategy for offline and online adaptation of the pre-trained improvement policy, leading to improved search performance and surpassing recent adaptive methods for constructive heuristics.

# Code — https://github.com/federico-camerota/LRBS

# 1 Introduction

Combinatorial Optimization (CO) problems can be found in several domains ranging from air traffic scheduling (Bertsimas, Lulli, and Odoni 2011) and supply chain optimization (Singh and Rizwanullah 2022) to circuit board design (Barahona et al. 1988) and phylogenetics (Catanzaro et al. 2012). Although general-purpose solvers exist and most CO problems are easy to formulate, in many applications of interest getting to the exact optimal solution is NPhard and said solvers are extremely inefficient or even impractical due to the computational time required to reach optimality (Toth 2000; Colorni et al. 1996). Specialized solvers and heuristics have been developed over the years for different applications. However, the latter are often greedy algorithms based on hand-crafted techniques that require vast domain knowledge, thus they cannot be used on different problems and may get stuck on poor local optima (Applegate, Cook, and Rohe 2003; Helsgaun 2009; Gasparin et al. 2023).

CO problems have gained attention in the last few years within the deep learning community where neural networks are used to design heuristics that can overcome the limitations of traditional solvers (Lombardi and Milano 2018; Bengio, Lodi, and Prouvost 2021). In particular, extensive literature has been developed on methods to tackle the travelling salesperson problem (TSP) due to its relevance and particular structure that allows to easily handle constraints with neural heuristics. Deep learning approaches for CO problems can be divided into constructive and improvement methods. The former follows a step-by-step paradigm to generate a solution starting from an empty one and sequentially assigning decision variables (Vinyals, Fortunato, and Jaitly 2015; Nazari et al. 2018; Kool, van Hoof, and Welling 2019). Instead, improvement approaches iteratively improve a given initial solution using an operator to turn a solution into a different one (Zhang et al. 2020; de O. da Costa et al. 2020; Wu et al. 2021; Hottung and Tierney 2022). Moreover, deep learning solvers can be classified based on their learning strategies: supervised learning (Khalil et al. 2017; Joshi, Laurent, and Bresson 2019; Hottung, Bhandari, and Tierney 2020; Li, Yan, and Wu 2021; Xin et al. 2021; Sun and Yang 2023) and deep reinforcement learning (DRL) (Bello et al. 2017; Khalil et al. 2017; Deudon et al. 2018; Kool, van Hoof, and Welling 2019; Ma et al. 2020; Barrett et al. 2020; Kim, Park et al. 2021; Ma et al. 2021; Qiu, Sun, and Yang 2022; Kim, Park, and Park 2022; Ye et al. 2023; Ma, Cao, and Chee 2023).

Many recent advancements in neural solvers for CO primarily lie within the constructive framework. This approach eliminates the necessity for manually crafted components, thereby providing an ideal means to address problems without requiring specific domain knowledge (Lombardi and Milano 2018). However, improvement heuristics can be easier to apply when complex constraints need to be satisfied and may yield better performance than constructive alternatives when the problem structure is difficult to represent (Zhang et al. 2020) or when known improvement operators with good properties exist (Bordewich et al. 2008). Still, generalization, i.e., scaling from training sets with small problems to large instances while retaining good performance, is an open issue when using DRL neural heuristics in CO, particularly for the TSP (Joshi et al. 2021).

Contributions. While generalization has been studied for constructive methods (Hottung, Kwon, and Tierney 2021; Oren et al. 2021; Choo et al. 2022; Son et al. 2023; Jiang et al. 2023; Li et al. 2023), to the best of our knowledge no prior work has been done on improvement heuristics. In this paper, we focus on improvement heuristics for the TSP based on DRL policies and propose an inference-time beam search approach, Limited Rollout Beam Search (LRBS), which allows tackling problems 10 times larger than those seen at training time. Using pre-trained models, on instances of the same size as those used for training, our search scheme achieves state-of-the-art results among similar improvement methods and shows comparable performance to constructive heuristics. Generalization to instances up to 10 times larger than those seen while training is improved considerably compared to sampling from the original policy, mitigating the gap with constructive solvers. Moreover, our approach allows the integration of online adaptation within the search to overcome the limitations posed by the pretrained model in large-scale generalization. We also investigate the effectiveness of LRBS as an exploration strategy in fine-tuning the pre-trained models on a limited dataset of instances of the same size as those in the test set. In this setting, our experiments show competitive performances with constructive approaches that use online instance-based adaptation. Finally, we validate LRBS on two pickup and delivery TSP variants and show the advantage of our search approach with respect to more specialized problem-specific solutions. In conclusion, our analysis indicates that solvers utilizing improvement heuristics and a robust exploration approach may offer a viable alternative to adaptive constructive methods, displaying enhanced scalability for larger problem instances in terms of computational times.

# 2 Preliminary and Related Work Improving TSP Solutions with DRL

A TSP instance is defined by a graph $G = ( V , E )$ and the objective is to find a tour $\delta$ , i.e. a sequence of nodes $x _ { i } \in V$ , such that each node is visited only once, the tour starts and finishes in the same node and minimizes the tour length

$$
L ( \delta ) = w _ { \delta _ { N } , \delta _ { 1 } } + \sum _ { i = 1 } ^ { N - 1 } w _ { \delta _ { i } , \delta _ { i + 1 } } ,
$$

where $N = | V |$ , $w _ { i j } \in \mathbb { R } ^ { + }$ and $( i , j ) \in E$ are edges in the graph. In this work, we consider instances of the Euclidean TSP (Arora 1996) where $w _ { i j } = \| x _ { i } - x _ { j } \|$ .

To solve TSP instances in the improvement framework we start from a given randomly generated initial solution $\delta ^ { 0 }$ and use a policy $\pi _ { \boldsymbol { \theta } }$ , parametrized by learnable weights $\theta$ , to sequentially improve $\delta ^ { 0 }$ . The policy selects actions in the neighbourhood defined by an operator $g$ that, given a solution and an action, returns another solution to the problem. We formulate the DRL framework as follows.

State. The state is given by the current solution $\delta ^ { T }$ and the best solution found so far $\delta ^ { * T } = \mathrm { a r g m i n } _ { i \leq T } L ( \delta ^ { i } )$ .

Action. Actions are elements within the neighbourhood defined by the operator $g$ . For TSP, we consider 2-opt moves (Lin and Kernighan 1973) that consist of selecting a tuple of indices $( i , j )$ and reversing the order of the nodes between $\delta _ { i }$ and $\delta _ { j }$ . In Figure 1 we illustrate an example of

2-opt move with $( i = 3 , \ j = 6 )$ ), assuming zero-based numbering, where the order of all the nodes between $\delta _ { 3 } = 2$ and $\delta _ { 6 } = 7$ is reversed to obtain the new tour.

Reward. At each step $T$ , the reward is computed as $r ^ { T } =$ ${ \cal L } ( \delta ^ { * T } ) - { \cal L } ( \delta ^ { * T - 1 } )$ , hence the agent is rewarded only when improving on the best solution found.

![](images/c19d0d62e00e69f96a0f5bb291a2a754d899e76ecaace410d6d83c72ed458162.jpg)  
Figure 1: Example of 2-opt move with $( i = 3 , \ j = 6 )$ .

The above elements with the state transitions derived by the operator neighbourhood define a Markov Decision Process (MDP) (Bellman 1957; Puterman 1990) that we call improvement MDP that terminates after $T _ { \mathrm { m a x } }$ the number of steps in an episode. Although this work is focused on the Euclidean TSP and its variants, the framework described here easily extends to other routing and CO problems.

# Search in Neural CO

Beam Search (BS) and Monte Carlo Tree Search (MCTS) (Coulom 2006) have been widely used in neural CO (Joshi, Laurent, and Bresson 2019; Oren et al. 2021; Choo et al. 2022). Typically, they are used online with autoregressive constructive methods to boost their performance at inference time. However, many of the search techniques that work well for constructive heuristics are difficult to extend efficiently to the improvement setting. This is because constructive methods work on a short horizon, i.e. the number of steps required to obtain a solution, which is defined by the number of variables in the problem. On the contrary, improvement policies often require many more iterations to achieve good performance, see e.g. Ma et al. (2021). Although MCTS has been widely applied with DRL policies, yielding impressive results (Silver et al. 2016), a notable drawback lies in the computational cost associated with its backpropagation procedure (Choo et al. 2022). This limitation renders MCTS less suitable for the context of CO, particularly when dealing with large, difficult-toexplore search spaces. In the literature on neural CO, BS has emerged as a practical alternative to MCTS. This approach strikes a favourable balance between search capability and runtime complexity, making it a promising choice for addressing the challenges inherent in CO scenarios (Vinyals, Fortunato, and Jaitly 2015; Nazari et al. 2018; Kool, van Hoof, and Welling 2019; Joshi, Laurent, and Bresson 2019).

Adaptive Methods for Neural CO In recent developments within the field of neural CO, a novel trend has emerged in search methods that incorporate techniques for online adaptation of policy parameters during inference. This trend finds inspiration in the work of Hottung, Kwon, and Tierney (2021), who introduced Efficient Active Search (EAS) as an enhanced version of Active Search (AS)(Bello et al. 2017). EAS focuses on training only a small subset of policy weights, significantly reducing its computational footprint. Simulation Guided Beam Search (SGBS) (Choo et al. 2022) employs “simulations” (i.e. policy rollouts) to assess expanded nodes in BS and seamlessly integrates with EAS for online policy adaptation. SGBS’s lookahead capability facilitates informed node selection without the complexities associated with intricate backpropagation techniques as in MCTS. However, it’s important to note that SGBS samples from the DRL constructive policy until a leaf node is reached for node evaluation, rendering it too computationally intensive for improvement methods. More recently, Son et al. (2023) proposed Meta-SAGE that uses meta-learning and search to scale pre-trained models to large TSP instances. Introducing a bilevel formulation, the algorithm is made of two components: a scheduled adaptation with guided exploration (SAGE) that updates parameters at test time and a scale meta-learner that generates scale-aware context embeddings.

# 3 Searching with LRBS

![](images/2e3a5fda921092a53137424ed64919704e5f5f4734a1e3c7ee104a56af56ca56.jpg)  
Figure 2: Comparison of BS, SGBS, and LRBS. On the left, is the “Expansion” step which shares similarities among the three algorithms. Highlighted in blue are the $\beta$ paths of the active beam nodes. Yellow nodes represent the $\beta \times \alpha$ children selected for expansion where SGBS and LRBS apply the DRL policy in the “Rollout”. Finally at the “Selection” step the beam is updated and grown down the search tree. While SGBS uses rollouts to evaluate the selected children discarding the trajectory, LRBS keeps the trajectory and selection is done over the states reached in the rollout phase. Illustration inspired by Choo et al. (2022).

In this section, we describe our beam search strategy for CO improvement heuristics. To overcome the limitations of previous methods in the improvement MDP, we propose an effective beam search approach that allows to trade-off

# Algorithm 1: Limited Rollout Beam Search

1: Input: initial solution $\delta ^ { 0 }$ , pre-trained policy $\pi$ , parame  
ters $( \alpha , \beta , n _ { s } , T _ { \mathrm { m a x } } )$ , objective function $f$   
2: Output: best found tour $\delta _ { \mathrm { b e s t } }$   
3: $\delta _ { \mathrm { b e s t } } \bar { }  \delta ^ { 0 }$   
4: $R \gets \{ \}$   
5: $B \gets$ sample $\alpha \times \beta$ tours from $\pi ( \cdot | \delta ^ { 0 } )$   
6: for $\delta _ { i } ^ { 1 }$ in $B$ do   
7: $\delta _ { i } ^ { n _ { s } } \gets$ rollout $\pi$ for $n _ { s }$ steps starting at $\delta _ { i } ^ { 1 }$   
8: add $\delta _ { i } ^ { n _ { s } }$ to $R$   
9: update $\delta _ { \mathrm { b e s t } }$   
10: end for   
11: $B \gets$ select the best $\beta$ elements in $R$ according to $f$   
12: $t  n _ { s }$   
13: while $t < T _ { \operatorname* { m a x } }$ do   
14: $R \gets \{ \}$   
15: $B \gets$ for each $\delta _ { i } ^ { t }$ in $B$ sample $\alpha$ tours from $\pi ( \cdot | \delta _ { i } ^ { t } )$   
16: for ${ \delta } _ { i } ^ { t }$ in $B$ do   
17: $\delta _ { i } ^ { n _ { s } + t } \gets$ rollout $\pi$ for $n _ { s }$ steps starting at ${ \delta } _ { i } ^ { t }$   
18: add $\delta _ { i } ^ { n _ { s } + t }$ to $R$   
19: update $\delta _ { \mathrm { b e s t } }$   
20: end for   
21: $B \gets$ select the best $\beta$ elements in $R$ according to $f$   
22: $t \gets t + n _ { s }$   
23: end while   
24: return δbest

between the additional computational cost of search and heuristic performance. Additionally, our approach mitigates the effect of the longer episodic horizon in the improvement MDP by reducing the effective horizon on which the DRL policy works.

# The LRBS algorithm

Solving a CO problem with the DRL framework in Section 2 can be seen as traversing a search tree using policy $\pi$ to decide the path to follow. Nodes in the tree represent solutions to the problem, with the initial solution $\delta ^ { \hat { 0 } }$ being the root node, and edges possible improvement actions (e.g. 2-opt moves) that transform one solution into the other. In Algorithm 1 we present LRBS, the algorithm starts at the root node and carries out its search down the tree in a breathfirst fashion by keeping a beam of $\beta$ active nodes for each depth level and exploring $\alpha$ of their children, thus limiting the branching factor (see Figure 2). Contrary to other search problems, there are no terminal nodes to reach in the improvement MDP. Hence, exploration is carried out until the explored paths in the search tree reach a fixed depth $( T _ { \mathrm { m a x } } )$ and the best solution found is returned. While SGBS relies on rollouts to evaluate actions, which is impractical in this context due to the long horizon, LRBS uses limited rollouts, effectively reducing the horizon and enabling exploration. In addition, unlike MCTS, LRBS avoids any backpropagation, making it computationally efficient for the improvement MDP. The two main operations in LRBS can be described as follows.

Expansion and Rollout. LRBS introduces into the standard BS expansion step a limited policy rollout. Specifically, for each active node $o _ { k }$ in the beam, $\alpha$ distinct children are sampled according to the probability distribution of $\pi ( \cdot | o _ { k } )$ (depicted in the left column in Figure 2) and then, from the resulting $\beta \times \alpha$ states, the policy is rolled out for $n _ { s }$ steps to obtain solutions $o _ { k + n _ { s } }$ (as shown in the middle column of Figure 2). Parameters $\beta$ and $\alpha$ control the degree of exploration in LRBS, so appropriate values need to balance search performance and runtimes as both would increase with larger $\beta$ and $\alpha$ . For sensitivity analysis of these parameters, we refer the reader to the appendix in the extended version of this paper (Camerota Verdu\`, Castelli, and Bortolussi 2024).

Selection. To update its beam, LRBS selects the best $\beta$ solutions according to the objective function $f$ , e.g. $L$ in the improvement MDP described in Section 2, and then the search continues from the new resulting beam front (right column of Figure 2).

In LRBS, the limited length rollouts have a considerable impact on the search capabilities of the algorithm. Within the improvement MDP, the ability of the DRL agent to explore good solutions is highly constrained to the neighbourhood spanned by the used operator $g$ and the derived available actions. This implies that more than one step may be needed to reach a better solution than the current one, and even worse solutions may be observed in the path to an improved solution. By incorporating $n _ { s }$ steps of policy rollout before selection, instead of a single-step look-ahead as in BS, LRBS harnesses the improvement potential of $\pi$ and enhances its planning capabilities through exploratory actions facilitated by the beam.

# 4 Adapting Pre-Trained Policies with LRBS

While the search capabilities of LRBS mitigate the effect of distributional shifts when scaling to larger problem instances than those seen while training, its performance is limited by the pre-trained policy. In this section, we introduce an adaptive framework combining LRBS with EAS to update the pre-trained DRL policy. However, it is important to notice that the framework is general and other approaches could be used for adaptation instead of EAS. We study the effectiveness of this approach in two different scenarios: offline fine-tuning (FT) and online adaptation (OA). In EAS, a small set of new parameters $\phi$ is introduced by adding a few layers into the agent’s neural network, that in encoder-decoder architectures are usually placed in the final layers of the decoder. To reduce the computational burden of previous adaptive methods, Hottung, Kwon, and Tierney (2021) proposed to only train the new weights $\phi$ , making EAS extremely fast. To update $\phi$ in constructive heuristics, EAS utilizes a loss function consisting of an RL component, aiming to reduce the cost of generated solutions, and an imitation learning component, which increases the probability of generating the best solution seen so far. However, it is not straightforward to apply EAS in the improvement MDP since running multiple times the improvement heuristic for the total number of steps required to achieve a good solution and then adapting $\phi$ would incur extremely long computational times. Instead, in LRBS we can incorporate easily EAS by updating the new weights on the limited rollouts used in node expansion. To fine-tune the pre-trained policy, we assume a limited set $( S _ { F T } )$ of instances in the target problem distribution is available and train $\phi$ to maximize the reward achieved over the LRBS rollouts with the RL loss function of EAS, leading to the gradient:

$$
\nabla _ { \phi } \mathcal { L } _ { R } ( \phi ) = \mathbb { E } _ { \boldsymbol { \pi } } [ ( R ( \delta _ { \mathrm { L R B S } } ) - b ) \nabla _ { \phi } \log \pi _ { \phi } ( \delta _ { \mathrm { L R B S } } ) ]
$$

where $\delta _ { \mathrm { L R B S } }$ is a rollout of $n _ { s }$ steps and $b$ is a baseline (as in other works, we use the one proposed in Kwon et al. (2020)). This scenario is representative of many domains where similar CO problems have to be solved several times and past instances can be used for fine-tuning. In our experiments, the instances in $\mathit { S } _ { \mathit { F T } }$ are solved only once by the LRBS algorithm and after each policy rollout the new parameters $\phi$ are updated according to the gradient in Equation 1. Similarly, in the online adaptation scenario, we update the EAS weights at inference time with the approach described above. However, the EAS parameters are reset before solving each batch of test problems, hence, the extra policy weights adapt solely to the instance being solved.

# 5 Experimental Results

In this section, we report experimental results on the search capabilities of LRBS and its effect on the generalization of pre-trained DRL agents to large TSP instances and two pickup and delivery variants. We use checkpoints of models from de O. da Costa et al. (2020), pre-trained on Euclidean TSP instances with 100 nodes, and from (Ma et al. 2022), pre-trained on PDTSP and PDTSPL instances with 100 nodes. Ma et al. (2021) recently proposed the DualAspect Collaborative Transformer (DACT) architecture for the improvement of TSP solutions with 2-opt moves. Even though DACT performs better than the model from de O. da Costa et al. (2020) in the authors’ study, the latter architecture showed much better scalability in our preliminary investigations and even outperformed DACT when both were coupled with LRBS. In all our experiments on TSP, for LRBS, we set a “search budget” such that $\alpha \times \beta = 6 0$ and fix the other parameters to $n _ { s } = 2 0$ and $T _ { \mathrm { m a x } } = 5 0 0 0$ , similarly to previous works and balancing solution performance and runtime. On PDTSP and PDTSPL we reduce the budget to 40 and when doing adaptation we use $n _ { s } = 1 0$ to lower memory consumption. The best values of $\alpha$ and $\beta$ for each dataset were determined by testing the method on a set of 10 randomly generated instances of the same size as those in the test set. We run all our experiments using a single NVIDIA Ampere GPU with 64GB of HBM2 memory.

Tests datasets. The TSP instances in our experiments are generated as in Kool, van Hoof, and Welling (2019) where the coordinates of nodes are sampled uniformly at random in the unit square. We consider problems with $N =$ $\{ 1 0 0 , 1 5 0 , 2 0 0 , 5 0 0 , 1 0 0 0 \}$ nodes. To ensure a fair comparison with the pre-trained policies, for $N = 1 0 0$ we use the same 10, 000 test instances of de O. da Costa et al. (2020). For the other problems, we generate datasets with $1 , 0 0 0$ random instances for $N ~ = ~ \{ 1 2 5 , 2 0 0 \}$ and with 128 instances for $N = 5 0 0$ , 1000. For PDTSP and PDTSPL experiments we generate sets of 128 random instances with 200 and 500 nodes. In the following, we refer to the test dataset with problems with $N$ nodes as $\tt T S P { \cal N }$ , PDTSPN and PDTSPL $N$ , respectively.

Baselines. We compare LRBS with the pre-trained policy of de O. da Costa et al. (2020) and DACT, also with 8x of the augmentations introduced in Kwon et al. (2020) (A-DACT). Moreover, we include a modification on SGBS $\mathrm { S G B S + C ) }$ with limited rollouts (as in LRBS) to work with improvement heuristics, e.g. the pre-trained DRL policy of de O. da Costa et al. (2020). The search approach is similar to LRBS but the new beam front is selected from the direct children of the previous beam front, based on the information of the limited rollouts. Finally, we report the performance of constructive approaches and related algorithms that use search and adaptation (Kwon et al. 2020; Hottung, Kwon, and Tierney 2021; Choo et al. 2022; Son et al. 2023) as well as more recent methods (Luo et al. 2023; Ye et al. 2024). Although these methods have an advantage over improvement heuristics, direct comparison is not straightforward thus we compare our method only with the former. However, they contextualize our results within the broader literature on constructive methods. Recent methods (Drakulic et al. 2024; Sun and Yang 2023; Luo et al. 2023) achieve better generalization than the considered baseline on TSP. However, such methods typically require specialized policy training and cannot be directly used on any pre-trained policy as we do in this work.

Table 1: Performance evaluation on TSP100 and TSP150. For improvement methods, numbers in brackets indicate the number of steps.   

<html><body><table><tr><td colspan="4"></td><td colspan="3">N = 150</td></tr><tr><td>METHOD</td><td>OBJ.</td><td>N = 100 GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td></tr><tr><td>CONCORDE</td><td>7.75</td><td>0.0%</td><td></td><td>9.35</td><td>0.0%</td><td>1</td></tr><tr><td>POMO1</td><td>7.77</td><td>0.078%</td><td>3H</td><td>9.37</td><td>0.33%</td><td>1H</td></tr><tr><td>SGBS1</td><td>7.76</td><td>0.058%</td><td>0.2H</td><td>9.36</td><td>0.22%</td><td>0.1H</td></tr><tr><td>EAS1</td><td>7.76</td><td>0.044%</td><td>15H</td><td>9.35</td><td>0.12%</td><td>10H</td></tr><tr><td>SGBS+EAS1</td><td>7.76</td><td>0.024%</td><td>15H</td><td>9.35</td><td>0.08%</td><td>10H</td></tr><tr><td>DACT[10K]</td><td>7.79</td><td>0.463%</td><td>1.7H</td><td>10.20</td><td>9.12%</td><td>0.4H</td></tr><tr><td>A-DACT[10K]</td><td>7.76</td><td>0.101%</td><td>13H</td><td>9.86</td><td>5.54%</td><td>2.9H</td></tr><tr><td>COSTA</td><td>7.76</td><td>0.065%</td><td>19H</td><td>9.37</td><td>0.25%</td><td>3.2H</td></tr><tr><td>SGBS+C[5K]</td><td>7.78</td><td>0.335%</td><td>193H</td><td>9.44</td><td>1.00%</td><td>31H</td></tr><tr><td>BEAM S.[5K]</td><td>7.76</td><td>0.015%</td><td>19H</td><td>9.36</td><td>0.18%</td><td>6.4H</td></tr><tr><td>OURS[5K]</td><td>7.76</td><td>0.014%</td><td>19H</td><td>9.36</td><td>0.16%</td><td>3.2H</td></tr><tr><td>OURS+OA[5K]</td><td>7.76</td><td>0.013%</td><td>22H</td><td>9.36</td><td>0.13%</td><td>3.6H</td></tr></table></body></html>

# Boosting In-Distribution Performance

In Table 1 we report the results of LRBS on $T S P 1 0 0$ and T SP 150 that are close to the training data distribution. Our method outperforms all the considered baselines on

TSP100 and has the best results among improvement heuristics on TSP150. On TSP150 the constructive baselines show slightly better gaps than LRBS, but our approach has considerably lower runtime. From our analysis, on test instances close to the training data distribution the best LRBS configuration is $\beta = 6 0$ , $\alpha = 1$ ). While such a configuration corresponds to $\beta$ parallel runs of the policy, introducing limited rollouts allows us to perform online adaption and achieve improved performance.

# Out-of-Distribution Exploration with a Pre-Trained Policy

In the first part of Table 2 we show results on the generalization power of LRBS on TSP problems with 200, 500 and 1000 nodes with LRBS $( \beta , \alpha )$ configurations (30, 2), (15, 4) and (5, 12), respectively. As the test set distribution shifts away from the training distribution we observe that increasing the number of children evaluated for each node in the beam front improves on generalization. While on smaller instances the policy can select good actions and more exploitation with lower $\alpha$ leads to the best performance, on larger instances increasing $\alpha$ allows to compensate for the imprecision of the agent and yields better results. On these test datasets, LRBS scales better than other improvement heuristics achieving optimality gaps close to those of constructive approaches. Our experiments show that the augmentations employed by Ma et al. (2021) considerably improve the policy performance on instances with the same size as the training set. However, when considering larger graphs the benefit of the augmentations becomes less pronounced and the algorithm fails to scale. On the contrary, online exploration with LRBS mitigates the performance degeneration due to distributional shift and our method even improves on the results that the policy of de O. da Costa et al. (2020) would achieve if exploring the solution space for the same time as LRBS and using on average $\operatorname { 1 2 x }$ more 2- opt operations. On larger instances, LRBS is not competitive with Meta-SAGE but achieves optimality gaps comparable to those of EAS and $_ { \mathrm { S G B S + E A S } }$ , even improving its performance as the instances get larger. Turning our attention to the comparison of LRBS to BS, the results in Table 2 present an interesting phenomenon. On the smaller instances with up to 500 nodes, LRBS is faster and achieves much lower optimality gaps, even 6x smaller than BS. However, on the largest problems of the TSP1000 dataset, BS performs better than LRBS. This result strongly suggests that as the distributional shift between the training and test instances gets very large the step-wise greedy selection process of BS is better than the rollouts of LRBS in limiting the performance degradation of the policy. This further motivates the need for adaptive strategies to overcome the limitations posed by the pre-trained model.

# Generalization via Adaptation

In the second part of Table 2, we show results on the generalization of LRBS after fine-tuning the DRL policy on a small set of randomly generated instances with the same number of nodes as the test set (FT) and when adapting the policy parameters online (OA). The LRBS configurations are the same used for the non-adaptive experiments, with the only exception of the $\mathrm { L R B S } + \mathrm { F T }$ on the TSP1000 where we use ( $\dot { \beta } = 1 0$ , $\alpha = 6$ ). For all the considered problems, the FT dataset of randomly generated instances is of size equal to $1 0 \%$ of the test set size and each instance is solved only once using LRBS, running times include also the fine-tuning phase. While $\mathrm { L R B S + F T }$ shows the best results for TSP500 and TSP1000, we do not highlight them in bold to keep a fair comparison with the baselines. Our results show that finetuning on a limited set of problems allows LRBS to improve considerably on larger instances surpassing all the baselines on the TSP500 and TSP1000 benchmarks, while on the TSP200 the performance of LRBS is close to that of EAS. Even though online adaptation is less effective than finetuning, since policy weights are trained only on the instance being solved and reset thereafter, it achieves competitive results on the TSP200 and TSP500 datasets while outperforming constructive baselines on the TSP1000 instances. Although FT achieves lower gaps, we highlight these results in Table 2 for a fair comparison with the baselines. These results show that introducing an adaptive component in the search process of LRBS can overcome the limitations posed by the adopted pre-trained policy. In particular, on the larger TSP1000 problems, the use of LRBS alone fails to achieve the performance of the constructive baselines while both the offline and online adaptive approaches we propose almost halve the optimality gap of LRBS alone and even improve on the baselines.

<html><body><table><tr><td colspan="4">TSP200</td><td colspan="3">TSP500</td><td colspan="3">TSP1000</td></tr><tr><td>METHOD</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td></tr><tr><td>CONCORDE</td><td>10.704</td><td>0.0%</td><td>-</td><td>16.530</td><td>0.0%</td><td></td><td>23.144</td><td>0.0%</td><td></td></tr><tr><td>GLOP²</td><td></td><td></td><td></td><td>16.91</td><td>1.99%</td><td>0.1H</td><td>23.84</td><td>3.11%</td><td>0.1H</td></tr><tr><td>LEHD³</td><td></td><td>0.0182%</td><td>0.2H</td><td></td><td>0.167%</td><td>1.2H</td><td></td><td>0.719%</td><td>7H</td></tr><tr><td>EAS4</td><td>10.736</td><td>0.455%</td><td>2.4H</td><td>18.135</td><td>9.362%</td><td>4.3H</td><td>30.744</td><td>32.869%</td><td>20H</td></tr><tr><td>SGBS+EAS4</td><td>10.734</td><td>0.436%</td><td>2.1H</td><td>18.191</td><td>9.963%</td><td>4.2H</td><td>28.413</td><td>22.795%</td><td>19H</td></tr><tr><td>META-SAGE4</td><td>10.729</td><td>0.391%</td><td>2.1H</td><td>17.131</td><td>3.559%</td><td>3.8H</td><td>25.924</td><td>12.038%</td><td>18H</td></tr><tr><td>DACT[10K]</td><td>15.450</td><td>34.346%</td><td>0.6H</td><td>154.339</td><td>833%</td><td>0.4H</td><td>421.76</td><td>1722%</td><td>1.8H</td></tr><tr><td>A-DACT[10K]</td><td>14,345</td><td>34.023%</td><td>4.4H</td><td>147.127</td><td>790%</td><td>3.3H</td><td>412, 787</td><td>1683%</td><td>11.9H</td></tr><tr><td>COSTA</td><td>10.789</td><td>0.796%</td><td>4.1H</td><td>17.971</td><td>8.717%</td><td>1.9H</td><td>30.439</td><td>31.526%</td><td>7.8H</td></tr><tr><td>SGBS+C[5K]</td><td>10.903</td><td>1.858%</td><td>21.2H</td><td>18.455</td><td>11.642%</td><td>19.1H</td><td>47.083</td><td>103.44</td><td>78.6H</td></tr><tr><td>BEAM S.[5K]</td><td>11.137</td><td>4.051%</td><td>6.4H</td><td>17.851</td><td>7.993%</td><td>2.5H</td><td>26.507</td><td>14.536%</td><td>8.7H</td></tr><tr><td>OURS [2K]</td><td>10.782</td><td>0.738%</td><td>1.6H</td><td>17.684</td><td>6.902%</td><td>0.8H</td><td>32.368</td><td>39.970%</td><td>3.1H</td></tr><tr><td>OURS[5K]</td><td>10.771</td><td>0.633%</td><td>4.1H</td><td>17.309</td><td>4.633%</td><td>1.9H</td><td>27.922</td><td>20.740%</td><td>7.8H</td></tr><tr><td>OURS+OA[2K]</td><td>10.771</td><td>0.629%</td><td>1.9H</td><td>17.443</td><td>5.523%</td><td>0.9H</td><td>28.468</td><td>23.008%</td><td>3.3H</td></tr><tr><td>OURS+OA[5K]</td><td>10.760</td><td>0.528%</td><td>4.8H</td><td>17.187</td><td>3.973%</td><td>2.0H</td><td>25.895</td><td>11.889 %</td><td>8.1H</td></tr><tr><td>OURS+FT[2K]</td><td>10.768</td><td>0.599%</td><td>2.6H</td><td>17.303</td><td>4.680%</td><td>0.9H</td><td>28.891</td><td>24.838%</td><td>3.9H</td></tr><tr><td>OURS+FT[5K]</td><td>10.757</td><td>0.504%</td><td>4.6H</td><td>17.102</td><td>3.463%</td><td>2.0H</td><td>25.801</td><td>11.483%</td><td>8.6H</td></tr></table></body></html>

Table 2: Performance evaluation on TSP200, TSP500 and TSP1000. For improvement methods, numbers in brackets indicate the number of steps.

Table 3: Optimality gaps on TSP150, TSP200, TSP500 and TSP1000 datasets.   

<html><body><table><tr><td></td><td>TSP150</td><td>TSP200</td><td>TSP500 TSP1000</td></tr><tr><td>OURS+FT</td><td>0.14%</td><td>0.50% 3.46%</td><td>11.48%</td></tr><tr><td>W/OLRBSFT</td><td>0.17%</td><td>0.52% 8.43%</td><td>81.63%</td></tr><tr><td>W/O EXP.</td><td>0.28%</td><td>1.44% 7.57%</td><td>16.06%</td></tr></table></body></html>

Ablation study. Table 3 reports the analysis of the importance of each element in the fine-tuning experiments on LRBS. In the first case (w/o LRBS FT in Table 3), the training framework of de O. da Costa et al. (2020) is used to fine-tune the policy while in the latter (w/o Exp. in Table 3) we sample from the policy for the same time as the LRBS runtime. To provide a fair comparison, when fine-tuning is done without LRBS the training runs for $n _ { s } \times \beta$ steps to train on the same number of environment interactions. While on the TSP150 and TSP200 datasets w/o LRBS FT yields a gap close to that with LRBS, on TSP500 and TSP1000 there is a considerable performance degeneration. On the contrary, when online exploration is replaced by sampling there is a much smaller effect on the generalization abilities of the model in the larger datasets but a greater decrease in performance in the TSP150 and TSP200 datasets. This shows the strength of LRBS in the fine-tuning phase where exploration allows the policy to better adapt to larger instances, especially for larger instances where the policy can easily get stuck in local optima. Moreover, for smaller problems, we observe that exploration in the fine-tuning phase is less critical but it has a considerable impact when applied online.

<html><body><table><tr><td></td><td colspan="3">PDTSP200</td><td colspan="3">PDTSP500</td><td colspan="3">PDTSPL200</td><td colspan="3">PDTSPL500</td></tr><tr><td>METHOD</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td></tr><tr><td>LKH</td><td>12.913</td><td>0.0%</td><td>3.4H</td><td>20.332</td><td>0.0%</td><td>20.8H</td><td>29.322</td><td>0.0%</td><td>2.7H</td><td>69.922</td><td>0.0%</td><td>23.1H</td></tr><tr><td>N2S-A[1K]</td><td>15.321</td><td>18.655%</td><td>2.7H</td><td>114.789</td><td>464.668%</td><td>34.6H</td><td>31.060</td><td>5.937%</td><td>3.4H</td><td>185.862</td><td>165.900%</td><td>51.0H</td></tr><tr><td>N2S-A[2k]</td><td>14.875</td><td>15.198%</td><td>5.5H</td><td>95.513</td><td>369.723%</td><td>69.3H</td><td>30.278</td><td>3.272%</td><td>6.8H</td><td>176.543</td><td>152.583%</td><td>102.3H</td></tr><tr><td>N2S-A[3K]</td><td>14.716</td><td>13.960%</td><td>8.3H</td><td>89.731</td><td>341.330%</td><td>103.9H</td><td>30.028</td><td>2.417%</td><td>10.1H</td><td>173.354</td><td>148.017%</td><td>153.5H</td></tr><tr><td>OURS [1K]</td><td>14.710</td><td>13.918%</td><td>1.0H</td><td>48.907</td><td>140.604%</td><td>5.8H</td><td>30.228</td><td>3.103%</td><td>1.4H</td><td>142.341</td><td>103.669%</td><td>8.8H</td></tr><tr><td>OURS [2k]</td><td>14.433</td><td>11.773%</td><td>2.0H</td><td>35.891</td><td>76.558%</td><td>11.6H</td><td>29.869</td><td>1.874%</td><td>2.8H</td><td>108.413</td><td>55.087%</td><td>17.6H</td></tr><tr><td>OURS [3K]</td><td>14.320</td><td>10.899%</td><td>3.0H</td><td>33.723</td><td>65.889%</td><td>17.3H</td><td>29.721</td><td>1.370%</td><td>4.2H</td><td>85.762</td><td>22.663%</td><td>26.5H</td></tr><tr><td>OURS+OA [1K]</td><td>14.456</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OURS+OA [2K]</td><td></td><td>11.951%</td><td>1.9H</td><td>39.885</td><td>96.201%</td><td>11.8H</td><td>30.164</td><td>2.884%</td><td>2.8H</td><td>108.849</td><td>55.726%</td><td>21.6H</td></tr><tr><td>OURS+OA[3K]</td><td>14.161 13.987</td><td>9.664% 8.321%</td><td>3.7H 5.5H</td><td>33.587 31.820</td><td>65.227% 56.532%</td><td>23.6H 35.4H</td><td>29.839 29.716</td><td>1.774% 1.358%</td><td>5.6H 8.5H</td><td>81.044 77.012</td><td>15.942% 10.161 %</td><td>43.0H 64.2H</td></tr></table></body></html>

Table 4: Performance evaluation on PDTSP200, PDTSP500, PDTSPL200 and PDTSPL500. Numbers in brackets indicate th number of steps.

Computational efficiency From Table 2 we can also observe that LRBS and its online adaptive variant not only present a lower runtime than the constructive methods but also scale better, i.e. the relative increase in runtime as the test problems get larger is smaller for our algorithms. The reason for this fact is the autoregressive nature of constructive heuristics. With improvement approaches, we keep a fixed number of steps hence the computational cost grows only due to the larger problems to be processed by the neural policy. However, constructive methods, by design, need to perform increasingly more steps to generate solutions for larger instances, thus incurring an additional computational burden as the size of the problems increases

# Pickup and Delivery Problems

The pickup and delivery variant of TSP (PDTSP) consists of $n$ one-to-one pickup-delivery requests, where goods at $n$ pickup nodes need to be transported to $n$ corresponding delivery nodes. The objective is to find the shortest Hamiltonian cycle under the precedence constraint that every pickup node has to be visited before its corresponding delivery node. We also study PDTSP with the last-in-first-out constraint (PDTSPL) that enforces a stack ordering between collected goods and delivery is allowed only for the good at the top of the stack. For these problems, Ma et al. (2022) define a removal-reinsertion operator that selects a pickupdelivery request nodes $( \delta _ { i ^ { + } } , \delta _ { i ^ { - } } )$ , positions $( j , \ k )$ and places node $\delta _ { i ^ { + } }$ after node $\delta _ { j }$ and node $\delta _ { i ^ { - } }$ after $\delta _ { k }$ . In applying LRBS, we use the same framework described for the TSP but perform the expansion phase only on removal actions. The additional constraints are addressed at the policy and environment level, making LRBS versatile. In Table 4 we report the results of applying LRBS on model checkpoints from Ma et al. (2022) (N2S-A), pre-trained on pickup and delivery instances of size 100, when solving PDTSP and PDTSPL instances with $N = 2 0 0$ and 500 nodes. In these experiments, for N2S-A we use the same exploration strategy adopted by the authors where at inference time each instance solved is transformed into $\scriptstyle { \frac { 1 } { 2 } } \left| N \right|$ different ones, using the augmentations of (Kwon et al. 2020), and the policy is rolled out from each new instance. Our results show that the online exploration approach of LRBS is much more effective than N2S-A when generalizing to larger instances. Not only in terms of pure performance but also computational efficiency. On the smaller instances with 200 nodes, LRBS achieves a good reduction of optimality gaps requiring less time than N2S-A even when performing online adaptation. The PDTSP500 benchmark results are not satisfactory with optimality gaps well above $5 0 \%$ but still, LRBS shows improved generalization compared to N2S-A reducing its gap by almost $6 x$ . On the much more constrained PDTSPL500 problems instead, online search through LRBS outperforms N2S-A with a gap reduction close to $1 0 x$ when adaption is employed. Overall, the results of Table 4 are still far from being competitive with traditional solvers such as LKH but show the generalization potential of pre-trained policy with online search and adaptation.

# 6 Conclusion

In this study, we have introduced LRBS, a novel beam search method designed to complement DRL-based improvement heuristics for combinatorial optimization problems enhancing inference time performance and generalization. LRBS offers a tailored approach that enables pre-trained models to efficiently handle problem instances of significantly larger scales, up to ten times bigger than those encountered during the DRL policy initial training phase. To further enhance the generalization of pre-trained models, we integrate LRBS with EAS in offline and online adaptive scenarios. Our experimental evaluation shows that LRBS is superior to existing DRL improvement methods in solving the Euclidean TSP and two pickup and delivery variants. LRBS consistently outperforms alternative approaches proposed both for constructive and improvement heuristics. Moreover, in our analysis, LRBS exhibits superior runtime efficiency when scaling to larger instances compared to established constructive baselines, showing how improvement heuristics coupled with adaptive and search approaches can be a viable alternative to constructive methods.

# Acknowledgments

This work has been partially supported by the PNRR project iNEST (Interconnected North-Est Innovation Ecosystem) funded by the European Union Next-GenerationEU (Piano Nazionale di Ripresa e Resilienza (PNRR) – Missione 4 Componente 2, Investimento 1.5 – D.D. 1058 23/06/2022, ECS 00000043).