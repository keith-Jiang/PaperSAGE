# Better Understandings and Configurations in MaxSAT Stochastic Local Search Solvers via Anytime Performance Analysis

Furong $\mathbf { Y e } ^ { 1 , 2 }$ , Chuan $\mathbf { L u o } ^ { 3 * }$ , Shaowei Cai1

1Key Laboratory of System Software (Chinese Academy of Sciences) and State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China 2 Leiden Institute of Advanced Computer Science (LIACS), Leiden University, Leiden, The Netherlands 3 School of Software, Beihang University, Beijing, China f.ye $@$ ios.ac.cn, chuanluo $@$ buaa.edu.cn, caisw $@$ ios.ac.cn

# Abstract

Though numerous solvers have been proposed for the MaxSAT problem, and the benchmark environment such as MaxSAT Evaluations provides a platform for the comparison of the state-of-the-art solvers, existing assessments were usually evaluated based on the quality, e.g., fitness, of the bestfound solutions obtained within a given running time budget. However, concerning solely the final obtained solutions regarding specific time budgets may restrict us from comprehending the behavior of the solvers along the convergence process. This paper demonstrates that Empirical Cumulative Distribution Functions can be used to compare MaxSAT stochastic local search solvers’ anytime performance across multiple problem instances and various time budgets. The assessment reveals distinctions in solvers’ performance and displays that the (dis)advantages of solvers adjust along different running times. This work also exhibits that the quantitative and high variance assessment of anytime performance can guide machines, i.e., automatic configurators, to search for better parameter settings. Our experimental results show that the hyperparameter optimization tool, i.e., SMAC, can achieve better parameter settings of solvers when using the anytime performance as the cost function, compared to using the metrics based on the fitness of the best-found solutions.

Code and Datasets — https://github.com/FurongYe/AAAI25-MaxSAT/

# Introduction

The Maximum Satisfiability (MaxSAT) problem is the optimization version of the influential Boolean Satisfiability (SAT) problem that aims at finding the maximum number of satisfied clauses (Biere, Heule, and van Maaren 2009). (Weighted) partial MaxSAT, which is an important generalization of MaxSAT, divides the clauses into hard and soft ones, and a feasible solution requires all hard clauses to be satisfied (Li and Manya 2021). Among various complete and incomplete solvers developed for solving MaxSAT, stochastic local search (SLS) (Cai et al. 2016; Lei and Cai 2018), which commonly follows an iterative framework, is a significant category of incomplete solvers. While new methods have been continuously proposed, the corresponding comparison results are presented in numerous documents. The famous MaxSAT Evaluations (Berg et al. 2023a) provide a platform for practitioners to understand the performance of state-of-the-art solvers, and the existing comparisons among MaxSAT solvers mainly concern the quality, i.e., fitness, of best solutions obtained within a given time budget, namely, cutoff time (Luo et al. 2014; Cai et al. 2014; Cai, Luo, and Zhang 2017; Luo et al. 2017; AlKasem and Menai 2021; Chu, Cai, and Luo 2023; Liu et al. 2025), which is a reasonable option when we require a concrete judgment in competitions. However, the assessments concerning best-found solutions may produce bias in the cutoff time and hinder us from understanding the behavior of solvers during the iterative optimization process. Though some work (Zheng et al. 2022) investigated the convergence process when analyzing SLS solvers’ behavior, the current mechanism usually plots individual convergence lines for particular instances. Hickey and Beacchus’s work (Hickey and Bacchus 2022) addressed anytime performances of MaxSAT solvers. Still, their work focused on proposing a solution that would obtain good solutions on multiple cutoff times without addressing the issue of measuring anytime performance. Meanwhile, recent work (Hansen et al. 2022) has discussed that anytime performance assessment is vital for benchmarking algorithms.

Two perspectives, i.e., fixed-budget and fixed-target, are commonly considered when measuring iterative algorithms performance. The fixed-budget approach is usually applied to problems in which prior knowledge about the performance, e.g., the fitness scale, is unavailable, and it can assess the order of the obtained solutions. The fixed-target approach is commonly applied in well-understood practical scenarios or benchmarking environments, and it uses quantitative indicators to explain how fast an algorithm obtains a solution meeting specific requirements. However, we usually need to consider both perspectives when developing an algorithm for practical problems. Therefore, addressing the concerns of both approaches, an anytime performance metric, which examines algorithms’ performance based on empirical cumulative distribution functions (ECDF) of a set of cutoff times, has recently presented its superiority in black-box optimization benchmarking scenarios (Doerr et al. 2020; Hansen et al. 2016, 2022). The ECDF value at a given time is computed based on the fraction of solved problem instances or obtained solutions reaching the required target, e.g., fitness. In this way, we can obtain a ratio scaled value independent of the scale of solution fitness, and results regarding multiple cutoff times can be reasonably aggregated.

This paper introduces the first anytime performance assessments of MaxSAT SLS solvers. Our anytime performance assessments reveal intriguing behaviors that were invisible for fixed-budget assessments. The observations can provide valuable insights for improving the design and parameter settings of future SLS solvers. The hybrid solvers, which are the current state-of-the-art MaxSAT solvers, operate complete solvers and SLS solvers independently and alternatively (Cai and Lei 2020). Recent work on improving hybrid solvers mainly works on enhancing the performance of the SLS part (Berg et al. 2023b). Therefore, this work focusing on analyzing SLS will certainly contribute to future enhancement of the state-of-the-art MaxSAT solvers. In addition, the assessment techniques employed in this work can be transparent to both complete and hybrid solvers. In practice, our results reveal (dis)advantages of the solvers across different problem instances and demonstrate algorithms’ convergence progress aggregated across multiple instances. Apart from providing a comprehensive assessment that can inspire experts to improve the design of algorithms further, the quantitative assessment also creates an alternative way for algorithmic tools, e.g., hyperparameter optimizers, to recognize algorithms’ performance.

Since contemporary algorithms, including SLS, are mostly parametric, hyperparameter optimization (HPO) is vital for robust and competitive performance. For example, the MaxSAT Evaluation competition1 submissions are usually embedded with fitting parameter settings. Meanwhile, automatic HPO tools have been commonly applied to achieve those fine-tuned settings. The HPO scenario for MaxSAT solvers usually addresses the algorithm configuration (AC) problem, which aims to find a configuration minimizing a cost function $e$ . A configuration will be executed to evaluate the cost function $e$ until a cutoff time $\kappa$ . Based on theoretical analysis of evolutionary algorithms, recent work (Hall, Oliveto, and Sudholt 2022) has studied the impact of $\kappa$ on the performance of HPO, considering both scenarios using fixed-target and fixed-budget approaches as $e$ . Tuning algorithms’ anytime performance has been addressed in the recent work (Ye et al. 2022), which compared the HPO scenarios of tuning fixed-target and anytime performances. While the mentioned studies were tested on classic benchmark problems such as ONEMAX (Doerr et al. 2020), Lo´pez-Iba´n˜ez and St¨utzle applied the hypervolume considering two objectives, i.e., the best-found solution quality and the corresponding cutoff time, to take into account the anytime behavior of algorithms (Lo´pez-Iba´n˜ez and Stu¨tzle 2014), and the method was tested on a MAX-MIN Ant system for Traveling Salesperson Problems and the famous mixed-integer solver SCIP (Achterberg 2009). In this work, we investigate tuning the anytime performance of a MaxSAT SLS solver, and the experimental results suggest that anytime performance is a better mechanism than the fixed-budget one for the cost function of HPO.

The main contributions of this work are as follows:

• We provide an anytime performance assessment of four state-of-the-art MaxSAT SLS solvers. We aggregate their ECDFs across multiple problem instances and compare the solvers’ performance concerning various cutoff times. The assessment illustrates that ECDF, as a universal technique with a ratio scale, can distinguish the solvers that are considered identical when comparing the fixed-budget performance. Moreover, we observe that the solvers’ performance varies along the optimization process while investigating their performance in particular instances, addressing the necessity of anytime performance assessment.

• We suggest tuning anytime performance can obtain better parameter settings compared to the commonly applied fixed-budget performance tuning in the MaxSAT community. By tuning the ECDF of SLS solvers, SMAC (Lindauer et al. 2022) can obtain better configurations in terms of anytime performance (i.e., ECDFs) and fixed-budget performance (i.e., scores based on the best-found solution quality). Note that the mechanism of computing incremental aggregated ECDFs for HPO used in this paper can be applied to other scenarios without interfacing with the HPO framework.

# Preliminaries

Problem Definition Given a set of $n$ Boolean variables $V = \{ x _ { 1 } , x _ { 2 } , . . . , x _ { n } \}$ , a literal $l$ is either a variable $x$ or its negation $\neg x$ , and a clause is a disjunction of literals, i.e., $c = l _ { 1 } \vee l _ { 2 } \ldots \vee l _ { k }$ , where $k$ is the length of $c$ . Given an assignment $\alpha$ mapping Boolean values to each variable in $V$ , a clause $c$ is satisfied if at least one literal in $c$ is True; otherwise, it is falsified. Given a conjunction normal form (CNF) $F = c _ { 1 } \wedge c _ { 2 } \wedge . . . \wedge c _ { m }$ , where $m$ is the number of clauses, the MaxSAT problem is to find an assignment that maximizes the number of satisfied clauses. Partial MaxSAT (PMS) is a variant of MaxSAT that divides clauses into hard ones and soft ones, and a feasible assignment $\alpha$ requires all hard clauses to be satisfied. For the weighted PMS (WPMS) problem, each clause is associated with a weight $w ( c ) > 0$ , and the problem is to find a feasible $\alpha$ that maximizes the total weights of satisfied clauses (namely, minimizes the total weights of falsified clauses). In this paper, we denote (W)PMS as a constrained optimization problem minimizing the $\scriptstyle { c o s t ( \alpha ) }$ that is the total weight of falsified clauses. Nowadays, MaxSAT solvers are commonly developed and tested for (W)PMS, and we refer to MaxSAT as (W)PMS.

Algorithms SLS is a popular category of incomplete algorithms for MaxSAT. The common procedure of SLS samples an initial solution, i.e., assignment $\alpha$ , and then follows an optimization loop, creating new assignments by flipping one or multiple variables $x$ in $\alpha$ iteratively until reaching the termination condition (e.g., using out the cutoff time). Variants of SLS solvers usually work on the strategies of picking one or multiple variable(s) $x$ to be flipped.

The classic SATLike solver (Lei and Cai 2018) proposed a clause weighting scheme, which introduces a bias towards certain variables $x$ and determines which one(s) to be flipped. The newest extension of SATLike, SATLike3.0 (Cai and Lei 2020), is one of the state-of-the-art SLS solvers for MaxSAT and has been included in much work for comparisons (AlKasem and Menai 2021). NuWLS (Chu, Cai, and Luo 2023) proposed a new clause weighting scheme handling hard and soft clauses separately and worked on the weight initialization based on the framework of SATLike3.0. Differently from other methods that flip one variable at each iteration, MaxFPS (Zheng, He, and Zhou 2023) applies a farsighted probabilistic sampling method flipping a pair of variables. Moreover, instead of using the clause weighting technique, BandMax (Zheng et al. 2022) introduced the method of multi-armed bandit to select clauses to be satisfied, correspondingly deciding which variable to be flipped.

In this paper, we select SATLike3.0, BandMax, NuWLS, and MaxFPS for our first assessment of anytime performance of MaxSAT SLS solvers. Due to the space issue, we denote SATLike as SATLike3.0 in the following text.

# Performance Assessment

We test the four state-of-the-art SLS solvers on the 2022 and 2023 anytime tracks of MaxSAT Evaluations, including weighted (MSE23-w and MSE22-w) and unweighted MaxSAT (MSE23-uw and MSE22-uw). All the tested solvers are implemented in $\mathrm { C } { + } { + }$ and complied with $^ { \mathrm { g + + } }$ 9.4.0. All experiments are conducted in Ubuntu 20.04.4 with the AMD EPYC 7763 CPU and 1TB RAM. The cutoff time is set as $3 0 0 s$ for each run following the suggestion of MaxSAT Evaluations, and we conduct a solver of ten independent runs for each instance. Additional detailed results of the cutoff time $6 0 s$ , which is another suggestion of MaxSAT Evaluations, are available in our public repository.

We introduce in the following the solvers’ fixed-budget performance and anytime performance. The former is commonly applied in the MaxSAT community, and the latter is studied for MaxSAT for the first time in this work. The fixedtarget performance is not addressed because MaxSAT is an NP-hard problem, and its optima are unknown.

# Fixed-budget Performance

Fixed-budget Performance Metric Existing work on SLS solvers for MaxSAT usually reports their experimental results based on (1) the number of winning instances (“#win”) and (2) the score based on the competing solution (Berg et al. $2 0 2 3 \mathrm { a }$ ; AlKasem and Menai 2021). The number of winning instances presents the scope in the tested instances where a solver obtains the best solution among all competing solvers, and the score denotes the ratio between the solution quality obtained by a solver and the bestcompeting solver:

$$
s c o r e ( i ) = \frac { \mathrm { c o s t ~ o f ~ t h e ~ b e s t ~ s o l u t i o n ~ f o r ~ } i + 1 } { \mathrm { c o s t ~ o f ~ t h e ~ f o u n d ~ s o l u t i o n ~ f o r ~ } i + 1 } \in [ 0 , 1 ]
$$

$s c o r e ( i ) = 0$ indicates that a solver can not find a feasible solution for the instance $i$ . These two metrics are both calculated based on the best-found solution within a cutoff time. Consequently, such assessment outcomes may deviate along different cutoff times. Note that we are not critical of using these measures, as they can provide a concrete comparison for algorithm performance orders, as shown in amounts of existing work and MaxSAT Evaluation competitions.

Table 1: Number of instances in which a solver obtains the best performance among the competing solvers   

<html><body><table><tr><td></td><td>BandMax</td><td>MaxFPS</td><td>NuWLS</td><td>SATLike</td></tr><tr><td>MSE23-w</td><td>42(30)</td><td>26(20)</td><td>81</td><td>26(18)</td></tr><tr><td>MSE23-uw</td><td>73(42)</td><td>71(34)</td><td>110(99)</td><td>51(30)</td></tr><tr><td>MSE22-w</td><td>49(40)</td><td>49(41)</td><td>103</td><td>35(27)</td></tr><tr><td>MSE22-uw</td><td>89(65)</td><td>86(64)</td><td>106(91)</td><td>55(39)</td></tr></table></body></html>

Table 2: Aggregated scores of the solves   

<html><body><table><tr><td></td><td>BandMax</td><td>MaxFPS</td><td>NuWLS</td><td>SATLike</td></tr><tr><td>MSE23-w</td><td>0.849</td><td>0.867</td><td>0.904</td><td>0.827</td></tr><tr><td>MSE23-uw</td><td>0.800</td><td>0.782</td><td>0.902</td><td>0.705</td></tr><tr><td>MSE22-w</td><td>0.869</td><td>0.889</td><td>0.942</td><td>0.864</td></tr><tr><td>MSE22-uW</td><td>0.854</td><td>0.852</td><td>0.901</td><td>0.731</td></tr></table></body></html>

Experimental Results We present in Table 1 the number of instances in which a solver obtains the best solution (that is achieved by all runs of the competing solvers) in one of the tested ten runs, followed by the number (in brackets) of instances in which a solver obtains the best average results of ten runs among the competing solvers. Note that we omit numbers in brackets if they are identical to the former ones. Results are categorized for the four tested instance sets. According to Table 1, ${ \mathrm { N u W L S } }$ obtains significant advantages against the other solvers, and BandMax and MaxFPS show similar performance across the four tested benchmark tracks. BandMax and MaxFPS win on more instances than SATLike when comparing the best of ten runs, but such advantages diminish when comparing the average of ten runs (as shown in the brackets). This observation indicates the necessity of performing multiple trials when comparing SLS solvers.

Table 2 presents the scores of the tested solvers, of which values are averaged across the instances for each benchmark track. Since we conduct ten independent runs for each instance, the score of a solver for an instance $i$ is the average of ten trails. $s c o r e ( i ) = 1$ indicates that a solver obtains the best solution for the instance $i$ among the competing solvers in all runs, and $s c o r e ( i ) = 0$ indicates that no feasible solution is obtained. We do not take the scores of the instances in which no solver obtains a feasible solution into account. We observe that the orders of the solvers’ scores are identical to the ones in Table 1. In addition, according to Figure 1, there exist several instances in which no solver can obtain a feasible solution, and the solvers obtain the same or similar results in many instances, as shown in light yellow color, e.g., $s c o r e > 0 . 9 .$ . Also, Figure 1 shows $\mathrm { \Delta N u W L S }$ obtains better scores in more instances compared to the other solvers.

![](images/4e213fa97baeb314b9b069a031adf2b7794d5092c955e056055a4fa85cb9cfc5.jpg)  
Figure 1: Heatmap illustrating the scores for individual instances. Each row represents an instance, while each column represents a different solver. The color depicts the score achieved by the solvers, with lighter shades indicating better performance. The benchmark tracks “MSE23-w”, “MSE23-uw”, “MSE22-w” and “MSE22-uw” consist of 160, 179, 197, and 179 instances (as shown by four boxes) respectively. The names of the instances have been omitted from the y-axis due to the limited space. Detailed results regarding groups of instances are available in Appendix B.

# Anytime Performance

The assessment of fixed-budget performance provides initial insights into the performance of the tested solvers. However, these results only focus on the cutoff time of $3 0 0 s$ . While we can conduct similar analyses for multiple cutoff times, for example, the MaxSAT Evaluations platform includes cutoff times of $6 0 s$ and $3 0 0 s$ , it remains challenging to aggregate and interpret the fixed-budget results across multiple cutoff time scenarios to understand the convergence of solvers. Therefore, we ask for an indicator that can assess anytime performance across various time budgets. Furthermore, this indicator shall be quantitative, allowing us to measure performance differences on a scale rather than by order. Particularly for instance-based scenarios such as MaxSAT, this indicator needs to be universal and independent of the scale of solution quality (e.g., cost of assignments) for each instance, enabling fair performance aggregation across instances.

Anytime Performance Metric. We assess the anytime performance of MaxSAT solvers using the ECDF values of a set of cutoff times. ECDF indicates the fraction of the obtained solutions satisfying a specific quality. We denote $\phi$ as the solution quality, e.g., the cost of an assignment. Given a set of solutions with $\bar { \Phi _ { i } } = \{ \phi _ { i 1 } , \phi _ { i 2 } , . . . \}$ for a problem instance $i$ and a solver $A$ that obtains the best-found solution with $\phi _ { A _ { i } }$ within the cutoff $t$ , $A$ ’s ECDF value at $t$ is:

$$
\operatorname { E C D F } ( A , i , t ) = { \frac { \mid \left\{ \phi \in \Phi _ { i } \mid \phi \geq \phi _ { A _ { i } } \right\} \mid } { \mid \Phi _ { i } \mid } }
$$

We work on minimization in this paper, and $\{ \phi \in \Phi _ { i } \mid$ $\phi \geq \phi _ { A _ { i } } \}$ denotes the subset of solutions that are not better than the best-found one of $A$ for instance $\mathbf { \chi } _ { i }$ . For example, an algorithm $A$ obtains a set of pairs of optimization time and the corresponding best-found solution $O =$ $\{ ( t _ { 1 } , \phi _ { o 1 } ) , ( t _ { 2 } , \phi _ { o 2 } ) , \dots \}$ , which are plotted by blue dots in Figure 2. Given a set $\Phi$ of solutions plotted by red stars, we present on the right subfigure the number of instances in $\Phi$ that are not better than the corresponding $\phi _ { o }$ for each $t$ , and the values normalized by the size of $\Phi$ are ECDFs.

Because ECDF is with a ratio scale, we can evaluate the ECDFs of a solver $A$ for a set of optimization times $t$ to measure $A$ ’s anytime performance. ECDFs can also be combined across multiple problem instances $i$ by considering specific $\Phi$ for each instance.

![](images/284822d52b5eecd8d8d965f3d82502fec34ac59205d1bb4903a1c679731a9356.jpg)  
Figure 2: An example of ECDF calculation. Left: The blue dots represent pairs $( t , \phi )$ , where a better solution with fitness $\phi$ ( $y$ -axis) is obtained at time $t$ ( $\mathbf { \chi } _ { x }$ -axis). Right: the ECDF values based on the given set of solution fitness as presented in red stars on the left.

To calculate ECDFs, we form a set of solution fitness values $\Phi _ { i }$ for each instance $i$ . This solution fitness set comprises the costs of solutions that the solvers have visited during our experiments. We compute the ECDFs at 100 specific optimization times, chosen within the range of $[ 0 , 3 0 0 s ]$ following a logarithmic scale. This set of optimization times introduces a preference for evaluating the performance in terms of attaining high-quality solutions in less time.

Note that measuring ECDFs has been adopted in the black-box optimization community, where researchers quantify “time” in terms of function evaluations, i.e., the number of solutions that have been evaluated (Hansen et al. 2016; Wang et al. 2022). Function evaluation ensures reproducibility and erases disturbances caused by hardware environment, programming design, and other factors. However, MaxSAT solvers are usually applied in practical scenarios concerning cpu time, and cpu time has been commonly used when measuring MaxSAT solvers’ performance. We have conducted analyses by using cpu time and function evaluations as the time metrics, respectively. The analyses using these two time metrics show identical conclusions. Therefore, we present the results using cpu time in this paper. The data using function evaluations that are supported in a favored black-box benchmarking platform (Wang et al. 2022; de Nobel et al. 2023) is available in our repository.

![](images/58dcd33b20a35f00ffbd422e77a19885734df3913689ceb41eb0353ada14ad16.jpg)  
(a) Left: WPMS (instances of MSE22-w and MSE23-w) and Right: PMS (instances of MSE22-uw and MSE23-uw)

![](images/0664e6c88ff0a5e93c5c24cf2adabf7cce4715af5528581842d114cbc5e1a100.jpg)  
(b) Left: “decision-tree” instances (15 in total), and Right: “ParametricRBACMaintenance” instances (13 in total).   
Figure 3: Aggregated ECDFs of each solver for different sets of problem instances. $x$ -axis indicates cpu time, and $y$ -axis indicates the corresponding aggregated ECDF values.

Observing Anytime Performance. Figure 3a shows that NuWLS outperforms the other solvers for both weighted and unweighted MaxSAT. BandMax and MaxFPS show similar performance, but by examining the “anytime” analysis, we can see that MaxFPS slightly outperforms BandMax within a time limit of $1 0 s$ , and the order of their performances alter as the optimization time increases for the weighted MaxSAT. Furthermore, detailed analysis of anytime performance for specific instances can reveal exciting findings. Although NuWLS demonstrates advantages over other solvers considering ECDFs aggregated across the entire set of tested problem instances, it does not exhibit superiority on “decisiontree” instances after being trapped in local optima around 10s, as shown in Figure 3b. On the other hand, while BandMax obtains similar performance to SATLike for “ParametricRBACMaintenance” instances when cpu time is less than $1 0 0 s$ , it outperforms the other solvers afterward. Compared to the fixed-budget performance assessment, these anytime performance observations offer more insights into the behavior of algorithms for specific problem instances and can be valuable for enhancing algorithm design.

Quantitative Assessment with Variation As mentioned previously, ECDF has a ratio scale that allows aggregation of values across multiple optimization times to obtain quantitative assessments for each instance, which is the so-called (approximated) area under the ECDF curve (AUC) (Ye et al. 2022). Table 3 presents the aggregated ECDFs for the four benchmark tracks, showing that the order of the solver’s assessment based on ECDF remains the same as the one based on scores, as presented in Table 2. When analyzing the behavior in specific instances, Figure 4 shows higher variances in ECDFs for each instance compared to the ones in scores (see Figure 1). Specifically, instead of having a large set of scores higher than 0.9 in Figure 1, ECDFs are in a wide variation of values as shown in Figure 4. We also provide the average standard deviations of scores and ECDFs across all tested instances in Appendix A, which shows that the standard deviations obtained from ECDFs are much greater than the scores. Moreover, for the instances where solvers can obtain the same best-found solutions, ECDFs can distinguish solvers’ performance by quantitatively estimating the convergence process. For example, Figure 4 can show that $\mathrm { \Delta N u W L S }$ converge faster for the instances plotted in yellow.

Table 3: Aggregated ECDFs for each benchmark track. The ECDF value for each run on an instance is normalized by the number of considered cutoff times, i.e., 100. The presented values are average across the corresponding instances.   

<html><body><table><tr><td></td><td>BandMax</td><td>MaxFPS</td><td>NuWLS</td><td>SATLike</td></tr><tr><td>MSE23-w</td><td>0.444</td><td>0.445</td><td>0.584</td><td>0.409</td></tr><tr><td>MSE23-uW</td><td>0.546</td><td>0.548</td><td>0.696</td><td>0.466</td></tr><tr><td>MSE22-w</td><td>0.458</td><td>0.470</td><td>0.624</td><td>0.420</td></tr><tr><td>MSE22-uW</td><td>0.589</td><td>0.602</td><td>0.696</td><td>0.494</td></tr></table></body></html>

# Hyperparameter Optimization

Contemporary algorithms, such as heuristics, evolutionary algorithms, machine learning models, etc., are usually parametric. Therefore, parameter settings are essential for the competitive performance of algorithms. Before the development of automatic tools, parameter settings were manually chosen based on experts’ experience or repetitive testing using Grid Search. Nowadays, the tools such as SMAC (Lindauer et al. 2022) and irace (Lo´pez-Iba´n˜ez et al. 2016) have been commonly applied for automatic HPO. While practical scenarios may propose various requirements for HPO, we address in this paper the algorithm configuration (AC) problem, which meets the requirement of tuning MaxSAT solvers for a set(s) of problem instances. AC (Lindauer et al. 2022) aims at searching a well-performance configuration, i.e., parameter setting, $\lambda \in \Lambda$ of an algorithm $A$ across a set of problem instances $\{ i _ { 1 } , . . . \} \subset \mathcal { T }$ :

$$
\lambda ^ { * } \in \arg \operatorname* { m i n } _ { \lambda \in \Lambda } e ( \lambda )
$$

where the cost function $e$ is usually set as the best-found solutions’ quality intuitively when tuning MaxSAT SLS solvers. However, in this section, we introduce AUC as an alternative candidate for $e$ and compare the results obtained by using these two different mechanisms as the cost function for tuning MaxSAT SLS solvers. For ease of reading, we denote AUC and aggregated ECDFs as the metrics used in HPO and experimental comparisons, respectively.

![](images/0db09c82105f189f464fb71a3b246507578bc4573fe9206cd5cd823231c7a17f.jpg)  
Figure 4: Heatmap illustrating aggregated ECDFs for individual instances. It follows the layout of Figure 1, but the color depicts the ECDF values achieved by the solvers. Detailed results regarding groups of instances are available in Appendix B.

Settings of Hyperparameter Optimization. We use the AC arcade of SMAC3 2 to compare the results of tuning fixed-budget performance and anytime performance. Best-f, which calculates the fitness of the best-found solution obtained within a given cutoff $t _ { B }$ , is a straightforward option for the cost function of tuning fixed-budget performance. In addition, we apply $- s c o r e$ in Eq. 1, which is denoted as Norm-f, as another cost function of tuning fixed-budget performance. To calculate scores, we use the best-found solutions obtained in the Performance Assessment section. Note that this function can be adopted for competitions but is not doable for the practical scenarios lacking baselines, i.e., the known best-found solutions. As for calculating the cost function of anytime performance approximated AUC, we form a set $T$ of 50 optimization times chosen within a range of $[ 0 . 1 s , t _ { B } ]$ following a logarithmic scale. Since the time budget $t _ { B }$ for executing each run of configurations is usually relatively small for HPO scenarios, e.g., $t _ { B } = 1 0 0 s < 3 0 0 s$ in this work, following a bias toward the configurations that have the potential to yield better solutions when provided with an additional time budget, we use $t _ { B } \mathrm { ~ - ~ } T$ as the set of optimization times to calculate AUCs. Regarding the target (i.e., solution quality) set $\Phi _ { i }$ for each instance $i$ , we start with a set $F _ { i }$ of five fitness values chosen from $[ f _ { \mathrm { i n i t . m i n } } , f _ { \mathrm { i n i t . m a x } } ]$ based on the linear scale, where $f _ { \mathrm { i n i t \mathrm { - } m i n } }$ and $f _ { \mathrm { i n i t \_ m a x } }$ are the fitness of the best and the worst solutions visited by the initial configuration. When a configuration obtains a better best-found solution for an instance during the tuning process, the corresponding best-found fitness value will be added to $F _ { i }$ . In addition, the AUC of a configuration for a given instance is calculated by aggregating the values across 50 optimization times. In practice, we use a modified $\mathrm { A U C ^ { \prime } } = - \mathrm { \bar { A } U C } \cdot \mid \Phi _ { i } \mid$ measuring $e$ of configurations for each instance $i$ . AUC′ decreases accordingly when a better configuration achieves a better best-found solution, thereby increasing the size of $\Phi$ . In this way, all of the tuning scenarios are framed as minimization tasks for HPO. Though computing AUC is more complex than computing Best-f and Norm-f, in practice, this computing takes linear time, and its time consumption is negligible compared to the time budget allocated for each run of a configuration.

We have tuned the four solvers for the MSE23-w and MSE23-uw benchmark tracks by using SMAC with three different cost functions. We conduct five runs of each SMAC setting with the cpu time budget of 60, 000s, following the suggestion in the work of NuWLS (Chu, Cai, and Luo 2023). Twenty percent of the instances are randomly selected for training. The obtained configurations of solvers are compared based on scores and AUCs across all the instances. Note that the computation of scores and ACUs follows the same settings as the Performance Assessment section.

Anytime Performance can Lead to Better Configurations. Table 4 presents the results of configurations obtained by each setting of SMAC, e.g., tuning solvers using different cost functions, and Figure 5 demonstrates the mean and $9 5 \%$ confidence interval of aggregated ECDFs of five configurations along cpu time. More detailed results of each tuned configuration are provided in Appendix C, and all results are from 120 (4 solvers $\times 3$ cost functions $\times$ 2 benchmark tracks $\times 5$ SMAC runs) configurations.

We first compare the anytime performance of the obtained configurations for eight scenarios (4 solvers $\times 2$ benchmark tracks). According to Table 4, using AUC as the cost function can obtain the best anytime performance for six out of eight scenarios, except for BandMax and MaxFPS in MSE23-uw. We can observe in Figure 5a-b that the anytime performance of the tuned BandMax and MaxFPS configurations are close to each other, and the results of using AUC as the cost function generally obtain slight advantages along CPU time for BandMax. When comparing the fixed-budget performance, i.e., scores, using AUC as the cost function obtains the best results for five out of eight scenarios in Table 4. Note that Wilcoxon signed-rank tests indicate that the results of using AUC as the cost function significantly differ from the others. P-values are provided in Appendix C.

Overall, our results suggest that tuning anytime performance can lead to better configurations in terms of both anytime performance and best-found solution quality. Specifically, when tuning ${ \mathrm { N u W L S } }$ and SATLike, using AUC as the cost function obtains significant advantages $4 \%$ and $1 5 \%$ by average) compared to the second-best results. For the five out 16 scenarios (in Table 4) that tuning AUC does not obtain the best results, the obtained results are only $0 . 6 \%$ worse than the best ones by average.

<html><body><table><tr><td></td><td colspan="4">BandMax</td><td colspan="3">MaxFPS</td><td colspan="3">NuWLS</td><td colspan="3">SATLike</td></tr><tr><td></td><td>Best-f</td><td>Norm-f</td><td>ECDF</td><td>Best-f</td><td>Norm-f</td><td>ECDF</td><td>Best-f</td><td>Norm-f</td><td></td><td>ECDF</td><td>Best-f</td><td>Norm-f</td><td>ECDF</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>Results on MSE23-w</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Score 0.857</td><td></td><td>0.850</td><td>0.858</td><td>0.844</td><td>0.850</td><td>0.848</td><td>0.795</td><td>0.809</td><td></td><td>0.818</td><td>0.664</td><td>0.564</td><td>0.758</td></tr><tr><td>ECDF</td><td>0.423</td><td>0.405</td><td>0.423</td><td>0.424</td><td>0.423</td><td>0.430</td><td>0.431</td><td></td><td>0.450</td><td>0.479</td><td>0.288</td><td>0.225</td><td>0.353</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>Results on MSE23-uw</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Score</td><td>0.815</td><td>0.799</td><td>0.814</td><td>0.776</td><td>0.773</td><td>0.769</td><td>0.824</td><td></td><td>0.827</td><td>0.846</td><td>0.617</td><td>0.717</td><td>0.790</td></tr><tr><td>ECDF</td><td>0.549</td><td>0.532</td><td>0.548</td><td>0.505</td><td>0.503</td><td>0.496</td><td>0.488</td><td></td><td>0.491</td><td>0.528</td><td>0.373</td><td>0.423</td><td>0.477</td></tr></table></body></html>

Table 4: Average scores and ECDFs of five configurations obtained by each setting. Larger values indicate better performances

![](images/094caccca808a1f506b5f34ee7c82e6ec24c70be489913b44bb33378158aa05e.jpg)  
Figure 5: Aggregated ECDFs of the configurations obtained by tuning for Best-f, Norm-f, and ECDF for Left: MSE23-w and Right: MSE23-uw. Results plotted in one line are from five configurations obtained by independent runs of SMAC.

Reasons for the Better Results. One reason for the findings is that AUCs can provide dense search space for HPO. For example, within a given time budget $t _ { B }$ , when two configurations $\lambda _ { 1 }$ and $\lambda _ { 2 }$ obtain the same best-found solution for an instance $i$ using different time $t \le t _ { B }$ , Best-f and Norm-f will deliver identical information to HPO tools, but

AUC can distinguish these two configurations. Another reason is that AUCs are normalized values ranging between 0 and 1 compared to Best-f. Although Norm-f is also normalized, its values range in a smaller domain for each instance. In contrast, AUCs have a wider variation of values. Note that we have also tested using the average of AUCs and Normf values across 20 instances as the cost function each time when SMAC evaluates a configuration. However, according to the results in Appendix D, normalization across multiple instances does not help configure MaxSAT solvers. In addition, when conducting HPO, the time budget $t _ { B }$ is usually set as a relatively small value due to time limits. For example, in our tuning process of SMAC, we allocated a budget of $1 0 0 s$ for each configuration. However, when validating the obtained configurations, the budget is $3 0 0 s$ . In the scenarios where configurations are allocated with limited time budgets, by considering multiple cutoff times, AUCs can robustly estimate the potential ability of a configuration to achieve better solutions. On the other hand, using Best-f or Norm-f as the cost function may be misled by fortunate results obtained at a specific cutoff time.

# Conclusion

We have introduced ECDF for anytime performance analysis of MaxSAT SLS solvers. Our assessments have been conducted for the four state-of-the-art solvers, namely BandMax, MaxFPS, NuWLS, and SATLike. We illustrate that ECDF can measure solvers’ anytime performance regarding multiple cutoff times, provide quantitative assessments with a ratio scale that can be aggregated across multiple instances, and differentiate the solvers that are considered similar in terms of fixed-budget performance. Our experiments show that the AUC can serve as a competitive metric for hyperparameter optimization. Compared to the traditional fixed-budget cost functions, the AUC can help achieve configurations that are, on average, around $1 0 \%$ better than the second-best ones in 11 out of 16 tested scenarios. In the remaining scenarios, its results are, on average, only $0 . 6 \%$ worse than the best ones. Although we have presented the application in the context of MaxSAT SLS, we expect that the applied techniques can be transparent to the hybrid methods integrating complete and incomplete solvers. Furthermore, we have observed that the choice of cost functions can affect the results of HPO. Therefore, we plan to study multiobjective HPO and explore the effect of cost functions. Also, we will study the algorithm portfolio configuration by considering the solvers’ anytime performance.