# Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free Backdoor Mitigation

Weilin Lin1, Li Liu1\*, Jianze $\mathbf { L i } ^ { 2 , 3 }$ , Hui Xiong

1The Hong Kong University of Science and Technology (Guangzhou) 2Shenzhen Research Institute of Big Data 3The Chinese University of Hong Kong, Shenzhen

# Abstract

Backdoor attacks present a serious security threat to deep neuron networks (DNNs). Although numerous effective defense techniques have been proposed in recent years, they inevitably rely on the availability of either clean or poisoned data. In contrast, data-free defense techniques have evolved slowly and still lag significantly in performance. To address this issue, different from the traditional approach of pruning followed by fine-tuning, we propose a novel data-free defense method named Optimal Transport-based Backdoor Repairing (OTBR) in this work. This method, based on our findings on neuron weight changes (NWCs) of random unlearning, uses optimal transport (OT)-based model fusion to combine the advantages of both pruned and backdoored models. Specifically, we first demonstrate our findings that the NWCs of random unlearning are positively correlated with those of poison unlearning. Based on this observation, we propose a random-unlearning NWC pruning technique to eliminate the backdoor effect and obtain a backdoor-free pruned model. Then, motivated by the OT-based model fusion, we propose the pruned-to-backdoored OT-based fusion technique, which fuses pruned and backdoored models to combine the advantages of both, resulting in a model that demonstrates high clean accuracy and a low attack success rate. To our knowledge, this is the first work to introduce OT and model fusion techniques to the backdoor defense. Extensive experiments show that our method successfully defends against all seven backdoor attacks across three benchmark datasets, outperforming both state-of-the-art (SOTA) data-free and data-dependent methods.

# Code — https://github.com/linweiii/OTBR Extended version — https://arxiv.org/pdf/2408.15861

# Introduction

Over the past decade, deep neural networks (DNNs) have become a crucial technology in various applications, including image recognition (Parmar and Mehta 2014; He et al. 2016a), speech processing (Gaikwad, Gawali, and Yannawar 2010; Maas et al. 2017), and natural language processing (Chowdhary and Chowdhary 2020), etc. However, as the deployment of DNNs in sensitive and critical domains

NWCs of Unlearning (BadNets) NWCs of Unlearning (Blended) Clean Unlearning Clean Unlearning Random Unlearning Random Unlearning NWC values 中 小 NWCs of Poison Unlearning NWCs of Poison Unlearning becomes more widespread, concerns regarding their security cannot be ignored. Among the numerous threats to DNNs, backdoor attacks (Gu et al. 2019; Li et al. 2021a; Wu et al. 2023a) are particularly concerning. In these attacks, the attackers manipulate a small portion of the training data to implant a stealthy backdoor into a DNN, resulting in a backdoored model. During inference, the backdoored model behaves anomalously when the input contains a pre-defined trigger pattern; otherwise, it performs normally. This phenomenon is termed the backdoor effect. Such attacks may pose hidden security issues to real-world applications, such as unauthorized access to a system when a company develops its software using a third-party pre-trained model.

In recent years, as backdoor attack methods have evolved, backdoor defense (Wu et al. 2023b) techniques have also seen significant growth. Various important techniques have been developed for backdoor defense, including pruning (Wu and Wang 2021; Zheng et al. 2022b), unlearning (Zeng et al. 2021a), and fine-tuning (Zhu et al. 2023), etc. However, most of these techniques rely on the availability of clean or poisoned data, which restricts their applicability to the aforementioned scenarios. Recent insights reveal a promising direction (Lin et al. 2024): using neuron weight changes (NWCs) of clean unlearning1 to categorize the neurons into backdoor-related ones and clean ones2, based on an observation that the NWCs of unlearning clean and poisoned data are positively correlated. In this work, our extended findings reveal that using random noise for unlearning, termed as random unlearning, brings a new similar insight: the NWCs of random unlearning exhibit a positive correlation with those of poison unlearning (as shown in Figure 1). This motivates us to adopt NWCs for datafree backdoor mitigation using only the generated random noise. Normally, after identifying backdoor-related neurons, pruning followed by fine-tuning is employed to eliminate the backdoor effect and restore the lost performance (Liu, Dolan-Gavitt, and Garg 2018). However, this is infeasible in data-free scenarios since the subsequent fine-tuning requires clean data. If we only perform pruning using NWCs and simply skip the fine-tuning, the clean accuracy (ACC) is prone to decrease by more than $10 \%$ (Lin et al. 2024). Therefore, after pruning, it is necessary to develop a new data-free technique for performance recovery.

![](images/9df2cbca142f7afffcc0cb839c3826e40f9d064a1462966f697c063332ee2343.jpg)  
Figure 2: OT-based model fusion for backdoor defense. The pruned model is aligned with the backdoored model layerby-layer using OT. Then the models are fused through a weighted averaging operation.

Recently, model fusion (Li et al. 2023a) has received increasing attention. It combines the weights of multiple models to integrate their capabilities into a single network. As one of the most representative works, OTFusion (Singh and Jaggi 2020) employs optimal transport (OT) to align model weights layer-by-layer before fusing two models through averaging. Following it, Intra-Fusion (Theus et al. 2024) employs OT to integrate the functionality of pruned neurons with the remaining ones, aiming to maintain great performance after pruning. It can be seen that the aforementioned methods both demonstrate OT’s inherent ability to preserve critical information during the fusion process.

Motivated by the above advancements in model fusion, in this work, we explore its potential to combine the high ACC of the backdoored model with the low attack success rate (ASR) of the pruned model in a data-free manner. Building on the foundation of NWC pruning and OTbased model fusion, we propose a novel data-free defense strategy called Optimal Transport-based Backdoor Repairing (OTBR), which fuses pruned and backdoored models. OTBR consists of two stages: random-unlearning

NWC pruning and pruned-to-backdoored OT-based fusion. In the first stage, we calculate the NWCs based on random unlearning of the backdoored model, and then prune the topranking $\gamma$ neurons to eliminate the backdoor effect. In the second stage, we align the weights of the pruned model with those of the backdoored model layer-by-layer using OT, and then fuse them into a single model. This process effectively dilutes the backdoor effect while preserving the clean performance. The fusion process is shown in Figure 2.

Our main contributions can be summarized as follows:

• We provide a new data-free pruning insight by revealing the positive correlation between NWCs when unlearning random noise and poisoned data. • We propose a novel data-free defense strategy that combines the high ACC of the backdoored model with the low ASR of the pruned model, using the OT-based model fusion. To our knowledge, this is the first work to apply OT and model fusion techniques to backdoor defense. • Experiments across various attacks, datasets, and experimental setups validate the effectiveness of our proposed OTBR method. Specifically, OTBR significantly outperforms both state-of-the-art (SOTA) data-free methods and SOTA data-dependent ones, consistently achieving successful defense performance against all tested attacks.

# Related Work

# Backdoor Attack

In the literature, various backdoor attacks on DNNs have been proposed, which can be generally categorized into two types: data-poisoning attacks and training-controllable attacks. For data-poisoning attacks, adversaries have access to the training dataset. BadNets (Gu et al. 2019), as one of the earliest examples, was proposed to implant a trigger pattern into the bottom-right corner of a small subset of the training images and reassign the labels to a specific target one. To enhance the stealthiness of the trigger, Blended (Chen et al. 2017) was proposed to blend the trigger onto the selected data with adjustable opacity. Recently, more sophisticated strategies have been proposed to enhance the trigger, including but not limited to SIG (Barni, Kallas, and Tondi 2019), label-consistent attacks (Shafahi et al. 2018; Zhao et al. 2020), and SSBA (Li et al. 2021a). Meanwhile, the second type, training-controllable attacks, is also rapidly evolving. In these attacks, adversaries have access to the training process, enabling more advanced attack strategies. Representative examples of this category include WaNet (Nguyen and Tran 2021) and Input-aware (Nguyen and Tran 2020), which incorporate an injection function into the training process to generate unique triggers for each input data. These innovative tactics make it more challenging to detect the triggers and conduct an effective defense.

# Backdoor Defense

In general, backdoor defense methods can be categorized into three types: pre-training, in-training, and post-training defenses. Among them, Post-training Defense has received the most attention, where the defenders aim to mitigate the backdoor effect of a well-trained backdoored model. FP (Liu, Dolan-Gavitt, and Garg 2018), as one of the seminal defense methods, prunes the less-activated neurons and then fine-tunes the model, based on the observation that poisoned and clean data activate different neurons; ANP (Wu and Wang 2021) detects and prunes backdoorrelated neurons by applying adversarial perturbations to neuron weights; Building on this, RNP (Li et al. 2023b) refines the perturbation technique using clean unlearning, and performs pruning based on a learned mask. In addition to these pruning-based techniques, some other important defense techniques exist. NC (Wang et al. 2019) proposes recovering the trigger to improve backdoor removal; NAD (Li et al. 2021c) pioneers the use of model distillation to train a benign student model; i-BAU (Zeng et al. 2021a) uses adversarial attacks to identify potential triggers and then performs poison unlearning to mitigate the backdoor effect.

![](images/f7e045db35b82848d9f2b67abece711f0b227676729b2c065dfc5b552270fa16.jpg)  
Figure 3: Overview of the proposed OTBR framework.

Different from the above defenses, which are all datadependent, CLP (Zheng et al. 2022a) is the first datafree defense method, which identifies and prunes potential backdoored neurons based on channel Lipschitzness; ABD (Hong et al. 2023) designs a plug-in defensive technique specialized for data-free knowledge distillation; DHBE (Yan et al. 2023) proposes a competing strategy between distillation and backdoor regularization to distill a clean student network without data.

Although several data-dependent techniques have already been proposed in the literature, the scarcity of data-free defense techniques still limits the applicability of backdoor defenses in real-world scenarios. In this paper, we will focus on addressing this issue, and develop a novel effective data-free defense method by using random-unlearning NWCs and the OT-based model fusion technique.

# Preliminary

# Threat Model

In this work, we address threats from both data-poisoning and training-controllable attacks. The attackers aim to poison a small portion of the training data so that the trained model predicts a target class when presented with data containing a pre-defined trigger, while otherwise performing normally. The weights of a $L$ -layer backdoored model are denoted as $\pmb { \theta } _ { b d } = \overline { { \{ \pmb { \theta } _ { b d } ^ { ( l ) } \} _ { 1 \leq l \leq L } } }$ , where $\pmb { \theta } _ { b d } ^ { ( l ) }$ represents the weights for the ${ l ^ { t h } }$ layer, consisting of $m ^ { ( l ) }$ neurons.

# Defense Setting

We focus on the post-training scenario, aiming to mitigate the backdoor effect of a well-trained backdoored model while minimizing the negative impact on ACC. Different from most previous works (Liu, Dolan-Gavitt, and Garg 2018; Wu and Wang 2021; Zeng et al. 2021a), which assumes access to $5 \%$ of clean data for defense, we adopt a more stringent approach that relies only on the backdoored model, without access to any clean data (Zheng et al. 2022a).

# Method

# Overview of Our Method

The complete data-free defense process of our proposed OTBR strategy is illustrated in Figure 3, which consists of two stages as follows:

• In Stage 1, referred to as random-unlearning NWC pruning, we aim to obtain a backdoor-free model. Specifically, we first conduct random unlearning on the backdoored model for $I$ iterative steps. During each step, a mini-batch of random noise with random labels is generated and used for unlearning. Then, we calculate the NWC for each neuron based on their weights from both the backdoored and unlearned models. Finally, we prune the top-ranking $\gamma$ of neurons, based on their NWCs, from the backdoored model to eliminate its backdoor effect.

• In Stage 2, referred to as pruned-to-backdoored OTbased fusion, when obtaining a sub-optimal pruned model, we aim to combine its low ASR with the high ACC of the original backdoored model by repairing the backdoor-related neurons using OT-based model fusion. Specifically, we propose NWC-informed OT to align the weights of the pruned model with those of the backdoored model layer-by-layer, taking into account the backdoor importance as determined by NWCs. For each layer, starting with the earliest pruned one, we initialize the probability mass on neuron weights using a uniform distribution for the pruned model and an NWC distribution for the original backdoored model. This strategy discriminatively transfers clean functionality to the backdoor-related neurons. The cost matrix $\boldsymbol { C } ^ { ( l ) }$ is calculated based on the Euclidean distance between neuron weights from the two models. Using this cost matrix, we then derive the optimal transport map $\mathbf { T } ^ { ( l ) }$ and employ it to transport the weights of the pruned model. After aligning all layers, we perform a simple weight averaging to fuse the transported and backdoored models, resulting in an effective defense.

Next, we will present more detailed formulations and provide further insights.

# Stage 1: Random-Unlearning NWC Pruning

Random Unlearning. Unlearning is a reverse training process designed to maximize the loss value on a given dataset (Li et al. 2023b). In this work, we define random unlearning as the process of unlearning a DNN model $f$ using a generated random dataset $\textstyle { \mathcal { D } } _ { r }$ . More precisely, random unlearning on the backdoored model $\pmb { \theta } _ { b d }$ is formulated as:

$$
\operatorname* { m a x } _ { \pmb { \theta } _ { b d } } \mathbb { E } _ { ( \pmb { x } _ { r } , \pmb { y } _ { r } ) \in \mathcal { D } _ { r } } \left[ \mathcal { L } ( f ( \pmb { x } _ { r } ; \pmb { \theta } _ { b d } ) , \pmb { y } _ { r } ) \right] ,
$$

where the loss function $\mathcal { L }$ is chosen to be a cross-entropy loss, and the generated random dataset $\mathcal { D } _ { r }$ contains $I \times B$ pairs of random noises $\pmb { x } _ { r } \in [ 0 , 1 ] ^ { A \times H \times \dot { W } }$ and random×labels $y _ { r } \in \{ 0 , 1 , \ldots , G \}$ . Here, $I$ is the number of iterative steps; $B$ is the batch size; $A , H$ and $W$ represent the generated noise size; and $G$ is the largest class label.

NWC Pruning. We follow the NWC definition from (Lin et al. 2024) to quantify the weight changes for each neuron during unlearning. Specifically, for the $j$ -th neuron in the $l .$ - th layer, the NWC is defined as:

$$
\begin{array} { r } { \mathrm { N W C } ^ { ( l ) j } \stackrel { \mathrm { d e f } } { = } \| \pmb { \theta } _ { u l } ^ { ( l ) j } - \pmb { \theta } _ { b d } ^ { ( l ) j } \| _ { 1 } , } \end{array}
$$

where $\pmb { \theta } _ { u l }$ denotes the unlearned backdoored model, $j \in$ $\{ 1 , \ldots , m ^ { ( l ) } \}$ and $l \in \{ 1 , \ldots , L \}$ . To eliminate the backdoor effect, we sort all calculated NWCs in descending order and prune the top-ranking $\gamma$ of neurons from the original backdoored model. The pruned model is denoted as $\pmb { \theta } _ { p n }$ .

# Stage 2: Pruned-to-Backdoored OT-based Fusion

Optimal Transport. OT is a mathematical framework to find the most economical way to transport mass from one distribution to another. Suppose we have two discrete probability distributions in the space $\mathcal { X } ~ = ~ \{ x _ { i } \} _ { i = 1 } ^ { n }$ and $y =$ $\{ y _ { j } \} _ { j = 1 } ^ { m }$ , i.e., the source distribution $\textstyle \mu : = \sum _ { i = 1 } ^ { n } \alpha _ { i } \cdot \delta ( x _ { i } )$ and the target distribution $\nu : = \textstyle \sum _ { j = 1 } ^ { m } \beta _ { j } \cdot \delta ( y _ { j } )$ , where $\textstyle \sum _ { i = 1 } ^ { n } \alpha _ { i } = \sum _ { j = 1 } ^ { m } \beta _ { j } = 1$ and $\delta ( \cdot )$ is the Dirac delta function. The OT problem can be formulated as a linear programming problem as follows:

$$
\begin{array} { r } { \mathrm { O T } ( \mu , \nu ; C ) \stackrel { \mathrm { d e f } } { = } \operatorname* { m i n } \langle \mathbf { T } , C \rangle , } \\ { \mathrm { s . t . } , \mathbf { T } \mathbf { 1 } _ { m } = \alpha , \mathbf { T } ^ { \top } \mathbf { 1 } _ { n } = \beta } \end{array}
$$

where $\mathbf { T } \in \mathbb { R } _ { + } ^ { n \times m }$ is the transport map that determines the optimal transport amount of mass from $\chi$ to $y$ , and $C$ is the cost matrix quantifying the cost of moving each unit of mass.

NWC-informed OT. In our approach, we align the NWCpruned model, which contains only clean functionality, with the original backdoored model to achieve more effective fusion. The goal is to dilute the backdoor effect with the least influence on clean performance. To achieve this, we focus more on the backdoor-related neurons during weight transport by employing NWC-informed initialization for the target distribution $\nu$ (backdoored model), while using a uniform distribution for the source distribution $\mu$ (pruned model). For the $l$ -th layer, we denote the probability mass as:

$$
\pmb { \alpha } ^ { ( l ) } \equiv \left\{ \frac { 1 } { n ^ { ( l ) } } \right\} _ { i = 1 } ^ { n ^ { ( l ) } } , \beta ^ { ( l ) } \overset { \mathrm { d e f } } { = } \left\{ \frac { \mathrm { N W C } ^ { ( l ) j } } { \sum _ { j = 1 } ^ { m ^ { ( l ) } } \mathrm { N W C } ^ { ( l ) j } } \right\} _ { j = 1 } ^ { m ^ { ( l ) } } ,
$$

where $n ^ { ( l ) }$ and $m ^ { ( l ) }$ denote the neuron numbers of pruned and backdoored models, respectively. Then, based on the distributions $\mu ^ { ( l ) }$ and $\nu ^ { ( l ) }$ , and the cost matrix $C ^ { ( l ) }$ , we can derive the optimal transport map $\mathbf { T } ^ { ( l ) }$ by equation (3).

Model Fusion. Inspired by the OTFusion (Singh and Jaggi 2020), we align and fuse the pruned and backdoored models layer-by-layer, using OT in equation (3) and our defined distributions in equation (4). The details of the entire fusion process are shown in Algorithm 1. Note that we start the fusion process from the first pruned layer $p$ , rather than the second layer. For the $l .$ -th layer, $\boldsymbol { n } ^ { ( l ) }$ and $m ^ { ( l ) }$ represent the neuron number of ${ \pmb \theta } _ { p n } ^ { ( l ) }$ and $\dot { \pmb { \theta } } _ { b d } ^ { ( l ) }$ , respectively.

In Algorithm 1, for each layer $l$ , we first align the incoming edge weights using the OT map $\mathbf { T } ^ { ( l - 1 ) }$ and probability mass $\beta ^ { ( l - 1 ) }$ from the previous layer:

$$
\widehat { \pmb { \theta } } _ { p n } ^ { ( l ) }  \pmb { \theta } _ { p n } ^ { ( l ) } \mathbf { T } ^ { ( l - 1 ) } \mathrm { d i a g } ( 1 / \beta ^ { ( l - 1 ) } ) .
$$

Then, we get the distributions $\mu ^ { ( l ) }$ and $\nu ^ { ( l ) }$ of the current layer and compute the cost matrix $C ^ { ( l ) }$ using Euclidean distance between neuron weights: $C _ { i j } ^ { ( l ) } \stackrel { \mathrm { d e f } } { = } \| \pmb { \theta } _ { p n } ^ { ( l ) i } - \pmb { \theta } _ { b d } ^ { ( l ) j } \| ^ { 2 }$ . Finally, as in equation (3), using $\mu ^ { ( l ) }$ , $\nu ^ { ( l ) }$ and $C ^ { ( l ) }$ , the OT

Input: Pruned model $\pmb { \theta } _ { p n }$ , backdoored model $\pmb { \theta } _ { b d }$ , random-unlearning NWC values for each neuron, balance coefficient $\lambda$ , the first pruned layer $p$ .

Output: Clean model $\pmb { \theta } ^ { * }$

$$
\begin{array} { c } { { : \alpha ^ { ( p - 1 ) }  \{ 1 / n ^ { ( p - 1 ) } \} _ { i = 1 } ^ { n ^ { ( p - 1 ) } } } } \\ { { \beta ^ { ( p - 1 ) }  \{ 1 / m ^ { ( p - 1 ) } \} _ { j = 1 } ^ { m ^ { ( p - 1 ) } } } } \end{array}
$$

$$
\begin{array} { r l } & { \hat { \theta } _ { p n } ^ { ( i ) } \gets \theta _ { p n } ^ { ( i ) } \mathbf { Y } ^ { ( i - 1 ) } \mathrm { d i a g } ( \mathrm { 1 } / \beta ^ { ( i - 1 ) } ) } \\ & { \alpha ^ { ( i ) } \gets \left\{ 1 / n ^ { ( i ) } \right\} _ { i = 1 } ^ { n ^ { ( i ) } } } \\ & { \beta ^ { ( i ) } \gets \left\{ \mathrm { N W C } ^ { ( i ) } / \sum _ { j = 1 } ^ { m ^ { ( i ) } } \mathrm { N W C } ^ { ( i ) j } \right\} _ { j = 1 } ^ { m ^ { ( i ) } } } \\ & { \mu ^ { ( i ) } , \nu ^ { ( i ) } \gets \mathrm { G e l D i s t i n t i o n } ( \alpha ^ { ( i ) } , \beta _ { b j } ^ { ( i ) } ) } \\ & { C ^ { ( i ) } \gets \mathrm { C o m p u t c o s t } ( \theta _ { p n } ^ { ( i ) } , \theta _ { b j } ^ { ( i ) } ) } \\ & { \mathbf { T } ^ { ( i ) } \gets \mathrm { O T } ( \mu ^ { ( i ) } , \nu ^ { ( i ) } , C ^ { ( i ) } ) } \\ & { \hat { \theta } _ { p n } ^ { ( i ) } \gets \mathrm { d i a g } ( \mathrm { 1 } / \beta ^ { ( i ) } ) \mathbf { T } ^ { ( i ) ^ { \top } } \hat { \theta } _ { p n } ^ { ( i ) } } \\ & { \theta ^ { ( i ) } \gets \lambda \hat { \theta } _ { p n } ^ { ( i ) } + ( 1 - \lambda ) \theta _ { b j } ^ { ( i ) } } \end{array}
$$

13: Obtain clean model $\pmb { \theta } ^ { * }$

map $\mathbf { T } ^ { ( l ) }$ of the current layer can be derived, and the transported pruned model $\widetilde { \pmb { \theta } } _ { p n } ^ { ( l ) }$ can be obtained as:

$$
\widetilde { \pmb { \theta } } _ { p n } ^ { ( l ) }  \mathrm { d i a g } ( 1 / \beta ^ { ( l ) } ) { \bf T } ^ { ( l ) ^ { \top } } \widehat { \pmb { \theta } } _ { p n } ^ { ( l ) } ,
$$

which has been aligned with the backdoored model.

After alignment, the transported model is then fused with the backdoored model to obtain the final defense model, which can be formulated as:

$$
\pmb { \theta } ^ { * }  \lambda \widetilde { \pmb { \theta } } _ { p n } + ( 1 - \lambda ) \pmb { \theta } _ { b d } ,
$$

where $\boldsymbol { \theta } ^ { * }$ represents the fused clean model and $\lambda$ is the balance coefficient.

Why Can OT-based Fusion Mitigate Backdoor Effect? We now offer a possible explanation for the effectiveness of OT-based model fusion in mitigating backdoor effects. Based on previous work (Lin et al. 2024), the NWC-pruned model can be made backdoor-free, i.e., the ASR dropping to zero, by selecting a suitable pruning threshold. Therefore, by aligning the pruned model with the original backdoored model using NWC-informed OT, we can transport the clean functionality of the remaining neurons to the nearest backdoored positions, as determined by NWCs and Euclidean distance. Then, further fusion based on the transported model can be viewed as a dilution operation to weaken the backdoored effect of the original backdoored model while preserving its clean functionality, thanks to the inherent ability of OT (Singh and Jaggi 2020; Theus et al. 2024). This is consistent with the previous insights that the backdoor task is easier and encoded in much fewer neurons than the clean task (Li et al. 2021b; Cai et al. 2022).

![](images/d40440d57331e2f91deb96ed4d9410ce9d9d3cafedcecfc0f071b5731c906249.jpg)  
Figure 4: Illustration of neuron-level weight norms for the backdoored, pruned, and transported models during OTbased fusion.

A practical example of the BadNets-attacked PreActResNet18 (He et al. 2016b) is illustrated in Figure 4. From the perspective of weight norm, the larger the difference in a neuron’s weight between the pruned and transported models, the more it is transported by the OT. We observe a consistent, slight decrease in the unpruned neurons that are intensively transported to some specific pruned neurons, resulting in their rapid recovery. This outcome reflects the effect of transporting from a uniform source distribution to a NWC-informed target distribution. By fusing the transported (green) and backdoored (blue) models, we can discriminately modify the neuron functionality, e.g., recovering more in low-NWC neurons, to effectively mitigate the backdoor effect while preserving high performance.

# Experiment

# Experimental Setup

For a fair comparison, all experiments, including the code implementation of our proposed method, are conducted using the default settings in BackdoorBench (Wu et al. 2022).

Datasets. Similar to previous works (Zhu et al. 2023; Wei et al. 2023), our experiments are conducted on three benchmark datasets, including CIFAR-10 (Krizhevsky, Hinton et al. 2009), Tiny ImageNet (Le and Yang 2015), and CIFAR-100 (Krizhevsky, Hinton et al. 2009).

Attack Setup. We evaluate the effectiveness of all defense methods using seven SOTA backdoor attacks: BadNets (Gu et al. 2019), Blended (Chen et al. 2017), Inputaware (Nguyen and Tran 2020), LF (Zeng et al. 2021b), SSBA (Li et al. 2021a), Trojan (Liu et al. 2018) and WaNet (Nguyen and Tran 2021). All attacks are conducted using the default settings in BackdoorBench (Wu et al. 2022). For example, we set the target label to 0, the poisoning ratio to $1 0 \%$ , and the tested model to PreActResNet18 (He et al. 2016b).

Table 1: Performance comparison with the SOTA defenses on CIFAR-10, Tiny ImageNet, and CIFAR-100 $( \% )$ .   

<html><body><table><tr><td rowspan="2">Datasets</td><td rowspan="2">Attacks</td><td colspan="2">No Defense</td><td colspan="2"></td><td colspan="8">Data-Dependent</td><td colspan="3">Data-Free OTBR</td></tr><tr><td></td><td>ASR</td><td>FP ACC</td><td>ASR</td><td>NAD ACC</td><td>ASR</td><td>ACC</td><td>ASR</td><td>i-BAU ACC</td><td>ASR</td><td>RNP ACC ASR</td><td>ACC</td><td>CLP ASR</td><td>ACC</td><td>ASR</td></tr><tr><td rowspan="8">CIFAR-10</td><td>BadNets</td><td>ACC 91.32</td><td>95.03</td><td>91.31</td><td>57.13</td><td>89.87</td><td>2.14</td><td>90.94</td><td>5.91</td><td>89.15</td><td>1.21</td><td>89.81 24.97</td><td>90.06</td><td>77.50</td><td>90.11</td><td>1.08</td></tr><tr><td>Blended</td><td>93.47</td><td>99.92</td><td>93.17 99.26</td><td>92.17</td><td>97.69</td><td>93.00</td><td>84.90</td><td>87.00</td><td>50.53</td><td>88.76</td><td>79.74</td><td>91.32</td><td>99.74</td><td>92.01</td><td>1.64</td></tr><tr><td>Input-aware</td><td>90.67</td><td>98.26</td><td>91.74 0.04</td><td></td><td>93.18 1.68</td><td>91.04</td><td>1.32</td><td>89.17</td><td>27.08</td><td>90.52</td><td>1.84</td><td>90.30</td><td>2.17</td><td>86.52</td><td>0.37</td></tr><tr><td>LF</td><td>93.19</td><td>99.28</td><td>92.90 98.97</td><td>92.37</td><td>47.83</td><td>92.83</td><td>54.99</td><td>84.36</td><td>44.96</td><td>88.43</td><td>7.02</td><td>92.84</td><td>99.18</td><td>87.68</td><td>9.69</td></tr><tr><td>SSBA</td><td>92.88</td><td>97.86</td><td>92.54 83.50</td><td>91.91</td><td>77.40</td><td>92.67</td><td>60.16</td><td>87.67</td><td>3.97</td><td>88.60</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>93.42</td><td>100.00</td><td>92.46</td><td></td><td>91.88</td><td>92.97</td><td></td><td>90.37</td><td></td><td></td><td>17.89</td><td>91.38</td><td>68.13</td><td>85.27</td><td>9.54</td></tr><tr><td>Trojan WaNet</td><td>91.25</td><td></td><td>71.17</td><td></td><td>3.73</td><td></td><td>46.27 2.22</td><td>89.49</td><td>2.91 5.21</td><td>90.89</td><td>3.59</td><td>92.98</td><td>100.00</td><td>90.62</td><td>7.50</td></tr><tr><td></td><td>92.31</td><td>89.73 97.15</td><td>91.46 92.23</td><td>1.09 58.74</td><td>93.17 92.08</td><td>22.98</td><td>91.32</td><td></td><td></td><td></td><td>90.43 0.96</td><td>81.91</td><td>78.42</td><td>88.12</td><td>10.93</td></tr><tr><td rowspan="6">Tiny ImageNet</td><td>Average</td><td>56.23</td><td></td><td></td><td></td><td>36.21</td><td>92.11</td><td>36.54</td><td>88.17</td><td>19.41</td><td>89.63</td><td>19.43</td><td>90.11</td><td>75.02</td><td>88.62</td><td>5.82</td></tr><tr><td>BadNets</td><td>57.45</td><td>100.00</td><td>51.73 99.99</td><td>46.37</td><td>0.27</td><td>50.55</td><td>7.74</td><td>51.48</td><td>97.36</td><td>21.91</td><td>0.00</td><td>55.94</td><td>100.00</td><td>54.13</td><td>0.00</td></tr><tr><td>Input-aware</td><td></td><td>98.85 55.28</td><td>62.92</td><td>47.91</td><td>1.86</td><td>53.17</td><td>0.17</td><td>52.48</td><td>72.98</td><td>15.57</td><td>0.00</td><td>57.75</td><td>99.58</td><td>51.40</td><td>0.02</td></tr><tr><td>SSBA</td><td>55.22</td><td>97.71</td><td>50.47 88.87</td><td></td><td>45.32 57.32</td><td>52.83</td><td>91.44</td><td>49.86</td><td>81.90</td><td>37.64</td><td>0.00</td><td>55.17</td><td>97.65</td><td>54.15</td><td>3.89</td></tr><tr><td>Trojan</td><td>55.89</td><td>99.98</td><td>50.22 8.82</td><td></td><td>48.48 0.83</td><td>50.37</td><td>1.40</td><td>52.65</td><td>98.49</td><td>46.27</td><td>0.00</td><td>55.86</td><td>8.39</td><td>53.85</td><td>0.14</td></tr><tr><td>WaNet</td><td>56.78</td><td>99.49 53.84</td><td>3.94</td><td></td><td>46.98 0.43</td><td>53.87</td><td>0.75</td><td>53.71</td><td>75.23</td><td>20.50</td><td>0.00</td><td>56.21</td><td>98.50</td><td>55.64</td><td>0.03</td></tr><tr><td rowspan="5">CIFAR-100</td><td>Average</td><td>56.31</td><td>99.21</td><td>52.31</td><td>52.91</td><td>47.01</td><td>12.14 52.16</td><td>20.30</td><td>52.04</td><td>85.19</td><td>28.38</td><td>0.00</td><td>56.19</td><td>80.82</td><td>53.83</td><td>0.82</td></tr><tr><td>BadNets</td><td>67.22</td><td>87.43</td><td>64.55</td><td>0.42</td><td>66.37</td><td>0.06</td><td>63.65 0.00</td><td>60.37</td><td>0.04</td><td>55.68</td><td>0.00</td><td>65.40</td><td>81.95</td><td>66.81</td><td>0.00</td></tr><tr><td>Input-aware</td><td>65.24</td><td>98.61</td><td>67.82 2.34</td><td>69.25</td><td>31.11</td><td>58.99</td><td>0.00</td><td>65.21</td><td>85.14</td><td>55.66</td><td>0.01</td><td>65.22</td><td>99.81</td><td>59.64</td><td>4.21</td></tr><tr><td>SSBA</td><td>69.06</td><td>97.22</td><td>61.60 14.02</td><td>67.38</td><td>89.51</td><td>64.35</td><td>39.60</td><td>63.09</td><td>28.91</td><td>68.44</td><td>92.80</td><td>65.39</td><td>97.52</td><td>66.89</td><td>1.28</td></tr><tr><td>WaNet</td><td>64.04</td><td>97.72 68.07</td><td>10.29</td><td>68.46</td><td>0.55</td><td>60.05</td><td>0.05</td><td>65.31</td><td>43.96</td><td>49.48</td><td>0.00</td><td>25.90</td><td>83.49</td><td>62.91</td><td>8.18</td></tr><tr><td></td><td>Average</td><td>66.39 95.25</td><td>65.51</td><td>6.77</td><td>67.87</td><td>30.31</td><td>61.76</td><td>9.91</td><td>63.50</td><td>39.51</td><td>57.32</td><td>23.20</td><td>55.48</td><td>90.69</td><td>64.06</td><td>3.42</td></tr><tr><td></td><td>AverageACCDrop (smallerisbetter)</td><td></td><td>↓1.51</td><td></td><td>1</td><td>↓2.64</td><td>1 ↓2.55</td><td>-</td><td>↓3.87</td><td>1</td><td></td><td>↓12.17</td><td>↓3.73</td><td>1</td><td>↓2.97</td><td>，</td></tr><tr><td colspan="3">Average ASR Drop (larger is better) Successful Defense Count</td><td>8/16</td><td>↓53.39</td><td></td><td>↓70.11 9/16</td><td></td><td>↓72.51 10/16</td><td></td><td>↓52.33 5/16</td><td>1</td><td>↓83.02 7/16</td><td>- 2/16</td><td>↓16.57</td><td>16/16</td><td>↓93.66</td></tr></table></body></html>

Defense Setup. We compare our proposed OTBR method with six SOTA defense methods: Fine-pruning (FP) (Liu, Dolan-Gavitt, and Garg 2018), NAD (Li et al. 2021c), ANP (Wu and Wang 2021), i-BAU (Zeng et al. 2021a), RNP (Li et al. 2023b), and CLP (Zheng et al. 2022a). Note that only CLP can be conducted in a data-free manner similar to OTBR, while the other five defenses are all datadependent. Therefore, we follow the common setting in the post-training scenario that $5 \%$ clean data is provided for those methods.

Evaluation Metrics. We use two common metrics to evaluate performance: ACC and ASR. They measure the proportion of correct predictions on clean data (the higher, the better) and the rate of incorrect predictions for the target label on poisoned data (the lower, the better), respectively. A defense is usually considered successful against an attack if the ASR is reduced to below $20 \%$ (Qi et al. 2023; Xie et al. 2024). In this paper, to consider both ACC and ASR, we consider a defense to be successful (marked with green in all tables) only if it achieves both of the following criteria: ACC decreases by less than $10 \%$ and ASR falls below $20 \%$ . Otherwise, it is considered unsuccessful. The best average results are boldfaced in all tables within this section.

# Compared with Previous Works

The main defense performance, compared with the six baseline methods, is shown in Table 1. We observe that our OTBR successfully defends against all 16 attacks across three benchmark datasets, achieving the largest drop in average ASR $( 9 3 . 6 6 \% )$ with an acceptable average ACC reduction $( 2 . 9 7 \% )$ . Moreover, OTBR outperforms both datafree and data-dependent defenses, achieving the lowest average ASR on CIFAR-10 and CIFAR-100, and the secondbest ASR on Tiny ImageNet. Notably, the best ASR on Tiny ImageNet, achieved by RNP, comes with a significant drop in ACC. For the performances of baseline methods, we observe that the data-dependent approaches have clear advantages over the data-free CLP, which succeeds in only 2 out of 16 defenses. ANP achieves the best results with 10 out of 16 successful defenses; however, it consistently fails against SSBA attacks, a shortcoming also observed in NAD. Despite its failures on CIFAR-10, FP performs well on CIFAR-100 and consistently achieves high ACCs across all three datasets, validating the effectiveness of fine-tuning. i-BAU fails completely on Tiny ImageNet, exposing its limitations when dealing with different data complexities. Although RNP achieves good performance in ASR, it fails with significant drops in ACC on Tiny ImageNet. In contrast, OTBR performs the best, consistently achieving superior results across various attacks and datasets.

Table 2: Comparison of different fusion schemes $( \% ) . \ \mathbf { V } _ { 1 }$ : no fusion; $\mathbf { V } _ { 2 }$ : vanilla fusion; ${ \bf V } _ { 3 }$ : OT-based fusion.   

<html><body><table><tr><td rowspan="2">Attacks</td><td colspan="2">V1</td><td colspan="2">V2</td><td colspan="2">V3 (Ours)</td></tr><tr><td>ACC</td><td>ASR</td><td>ACC</td><td>ASR</td><td>ACC</td><td>ASR</td></tr><tr><td>BadNets</td><td>84.98</td><td>1.56</td><td>91.3</td><td>81.13</td><td>90.11</td><td>1.08</td></tr><tr><td>Blended</td><td>69.08</td><td>4.71</td><td>92.06</td><td>16.76</td><td>92.01</td><td>1.64</td></tr><tr><td>LF</td><td>54.15</td><td>0.83</td><td>90.97</td><td>60.4</td><td>87.68</td><td>9.69</td></tr><tr><td>SSBA</td><td>40.58</td><td>0.02</td><td>89.84</td><td>40.57</td><td>85.27</td><td>9.54</td></tr></table></body></html>

# Ablation Studies

Effectivness of OT-based Model Fusion. To evaluate the effectiveness of OT-based model fusion, we keep Stage 1 unchanged and modify Stage 2 to generate three different versions for comparison. (1) $\mathbf { V } _ { 1 }$ : no fusion is conducted in Stage 2; instead, we evaluate the performance of the pruned model from Stage 1; (2) $\mathbf { V } _ { 2 }$ : implement vanilla fusion by directly fusing the pruned model with the backdoored model using equation (5); (3) ${ \bf V } _ { 3 }$ (Ours): the final version, where the full procedures of both Stage 1 and 2 are conducted. Table 2 shows the performances of these three versions on CIFAR10 across four different attacks. The results validate the ef

100(a) Impact of  and I on ACC 100(b) Impact of  and I on ASR 100 (c) Impact of 100 (d) Impact of G 80 80 80 80   
460 ACC-I-520 ASR (%) 460 ASR-I-520 60 ASCRC 460 ASCRC ACC-I-40 ASR-I-80 A 20 20 20 20 ACC-I-80 0 1 5 10 15 20 25 30 0 10 15 20 25 30 00.10.20.30.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 Pruning Ratio  (%) Pruning Ratio  (%) Balance Coefficient Largest Label G

![](images/f0274778e7cbb19b187d027f43925c5357eba9c34b5b5953ef9a576a335f4c41.jpg)  
Figure 5: Impact of different factors on performance. (a) and (b) show the impact of $\gamma$ and $I$ on ACC and ASR, respectively, with “ACC-I-5” representing ACC when $I = 5$ ; (c) shows the impact of $\lambda$ ; and (d) shows the impact of $G$ .   
Figure 6: Comparison of different OT distributions. $\mathtt { u 2 u }$ : uniform to uniform transport; $\mathbf { u } 2 \mathbf { r } .$ : uniform to random transport; $\mathbf { u } 2 \mathbf { n } ( \mathbf { o u r s } )$ : uniform to NWC transport.

fectiveness of aligning neuron weights using OT, where the pruned model inherently achieves a low ASR (or even better) while the high ACC is kept. Although the vanilla fusion $( \mathbf { V } _ { 2 } )$ better preserves ACC, it fails in effectively mitigating the backdoor effect.

Effectiveness of NWC-informed OT. To verify the important role of NWC-informed OT in achieving optimal defense performance, we fix the source distribution as a uniform distribution and compare three different initialization schemes for the target distribution in OT. Specifically, we consider three types of distributions: uniform distribution, random distribution, and NWC distribution. These schemes are labeled as $^ { \mathrm { \scriptsize { e } } \mathfrak { e } } \mathrm { u } 2 \mathrm { u } ^ { \mathrm { \tiny { , } \mathfrak { e } } }$ , “u2r”, and ”u2n”(Ours), respectively. The results are presented in Figure 6. We observe that only $^ { \mathrm { { e } } } \mathrm { { u } } 2 \mathrm { { n } } ^ { \mathrm { { , } } }$ , i.e., NWC-informed OT, consistently achieves strong performance across different attacks. It can be explained w.r.t. the weight transport as in Figure 4. In contrast, since uniform and random distributions are unrelated to the backdoor functionality, they fail to effectively guide neuron weight transport, resulting in poor fusion performance.

# Parameter Analysis

Impact of Different Factors. We aim to investigate the impact of various factors on the performance of OTBR. These factors include the number of iterative steps $I$ , the pruning ratio $\gamma$ , the largest label $G$ , and the balance coefficient $\lambda$ . The experiments are conducted using default settings on CIFAR-10 and BadNets with a $10 \%$ poisoning ratio. The results are shown in Figure 5. Firstly, in subfigures (a) and (b), we present the results of varying the pruning ratio $\gamma$ from $1 \%$ to $30 \%$ across four numbers of iterative steps $I$ (5, 20, 40, and 80 steps). The two subfigures, showing ACC and ASR respectively, demonstrate that OTBR performs well across different settings. A larger pruning ratio tends to require more iterative steps for effective random unlearning. In our setting $\left( I \right) = 2 0 ,$ ), $\gamma$ is insensitive in the range of $5 \%$ to $2 5 \%$ , resulting in successful defense. Secondly, in subfigure (c), we show the impact of the balance coefficient $\lambda$ ranging from 0.1 to 1.0. A higher value of $\lambda$ means a more important role the transported model plays in the fusion. We observe that there exists a trade-off between the high ACC from the backdoored model and the low ASR from the transported model, as we assumed before. It suggests setting the $\lambda$ between 0.4 and 0.8 for a successful defense. Lastly, in subfigure (d), we evaluate the performances with different largest labels $G$ for random data generation to test the impact of class number. Note that 9 is the largest label, in which case the model is trained. The results reveal that performance remains consistently good across $G$ values from 4 to 9, while a smaller class number may fail. Overall, our OTBR proves to be a robust defense method across various hyperparameter settings.

# Conclusion

In this work, we propose a novel data-free backdoor defense method, OTBR, using OT-based model fusion. Notably, we provide a new data-free pruning insight by revealing the positive correlation between NWCs when unlearning random noise and poisoned data. This insight enables us to effectively eliminate the backdoor effect using pruning guided by NWCs in a data-free manner. Then, we propose to combine the high ACC of the backdoored model with the low ASR of the pruned model using the OT-based model fusion. Furthermore, we provide possible explanations for the success of both NWC pruning and OT-based fusion. Extensive experiments across various attacks and datasets confirm the effectiveness of our OTBR method. A current limitation of this work is its reliance on NWC, which applies only to the post-training scenario. In future work, we plan to explore the potential of OT-based model fusion for more scenarios.