# Hierarchical Gradient-Based Genetic Sampling for Accurate Prediction of Biological Oscillations

Heng Rao1, $\mathbf { Y } \mathbf { u } \mathbf { G } \mathbf { u } ^ { 1 * }$ , Jason Zipeng Zhang2, Ge ${ \bf Y } { \bf u } ^ { 1 }$ , Yang $\mathbf { C a o } ^ { 3 }$ , Minghan Chen2\*

1College of Computer Science and Engineering, Northeastern University, Shenyang, China 2Department of Computer Science, Wake Forest University, Winston-Salem, NC, USA 3Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA guyu $@$ mail.neu.edu.cn, chenm $@$ wfu.edu

# Abstract

Biological oscillations are periodic changes in various signaling processes crucial for the proper functioning of living organisms. These oscillations are modeled by ordinary differential equations, with coefficient variations leading to diverse periodic behaviors, typically measured by oscillatory frequencies. This paper explores sampling techniques for neural networks to model the relationship between system coefficients and oscillatory frequency. However, the scarcity of oscillations in the vast coefficient space results in many samples exhibiting non-periodic behaviors, and small coefficient changes near oscillation boundaries can significantly alter oscillatory properties. This leads to non-oscillatory bias and boundary sensitivity, making accurate predictions difficult. While existing importance and uncertainty sampling approaches partially mitigate these challenges, they either fail to resolve the sensitivity problem or result in redundant sampling. To address these limitations, we propose the Hierarchical Gradient-based Genetic Sampling (HGGS) framework, which improves the accuracy of neural network predictions for biological oscillations. The first layer, Gradientbased Filtering, extracts sensitive oscillation boundaries and removes redundant non-oscillatory samples, creating a balanced coarse dataset. The second layer, Multigrid Genetic Sampling, utilizes residual information to refine these boundaries and explore new high-residual regions, increasing data diversity for model training. Experimental results demonstrate that HGGS outperforms seven comparative sampling methods across four biological systems, highlighting its effectiveness in enhancing sampling and prediction accuracy.

# 1 Introduction

In biological systems, oscillation refers to the repetitive, cyclical behaviors of cell signaling and biological processes over time, such as fluctuations in concentrations of biochemical substances (Tamate et al. 2017), rhythmic activities in cellular functions (Goldbeter 2002), or periodic changes in physiological states (Kurosawa, Mochizuki, and Iwasa 2002). Understanding these oscillations is crucial for comprehending how biological systems maintain stability, respond to external stimuli, and regulate complex processes. To study oscillations, researchers often use ordinary differential equations (ODEs) to model the temporal dynamics that characterize oscillatory patterns in biological systems. Recently, the application of machine learning to biological systems has gained significant attention. In many studies (Daneker et al. 2023; Yazdani et al. 2020; Szep, Dalchau, and Csika´sz-Nagy 2021), system-level mathematical models of biological reactions are integrated with machine learning models to establish relationships between raw data and system coefficients. Of these, Neural Networks (NNs) show promising results, offering an effective tool for predicting biological oscillations based on system coefficients.

However, the relationship between system coefficients and oscillatory behaviors is highly complex (Stark, Chan, and George 2007). Variations in these coefficients can disrupt oscillations, causing non-oscillatory states. In biological systems, coefficients that can lead to oscillatory states are relatively rare; most coefficient combinations result in non-oscillatory behavior, especially in high-dimensional domains. The prevalence of non-oscillatory states creates a significant data imbalance, challenging the prediction of oscillatory frequency—a key characteristic of oscillation, defined as the inverse of the oscillatory period ( per1iod ). When oscillation is absent, the frequency is set to zero. Fig. 1(a) illustrates the mapping between system coefficients and oscillatory frequencies of a cell cycle system (Liu et al. 2012). The domain shows data imbalance with $70 \%$ non-oscillation (blue). Moreover, minor changes in these coefficients can lead to substantial changes in oscillatory frequencies, particularly in boundary areas where oscillatory states transition to non-oscillatory states. This heightened sensitivity further complicates accurate predictions and is reflected in the poor performance of NNs in these regions, as evidenced by high residuals (absolute error) marked as black shades in Fig. 1. Overall, we face two challenges:

• Non-Oscillatory Bias. The majority of data samples exhibit non-oscillatory behaviors, resulting in an abundance of redundant information that does not improve model accuracy and may even reduce efficiency. • Oscillatory Boundary Sensitivity. Sharp transitions between non-oscillatory and oscillatory states occur in narrow boundaries of the coefficient space, which are easily overlooked by random sampling and lead to high errors.

Therefore, a more effective sampling strategy is necessary to obtain more balanced and representative samples for model training. Importance Sampling (IS) (Liu et al. 2021; Lu et al. 2023) can be used to resample more minority class instances, balancing the majority-minority ratio. As shown in Fig. 1(b), IS resamples the oscillation class, with darker samples indicating higher resampling, but it cannot generate new samples to alleviate boundary sensitivity issues. Synthetic Minority Oversampling Technique (SMOTE) (Chawla et al. 2002; Torgo et al. 2013) offers another way to balancing the dataset by generating synthetic minority samples. However, it insufficiently covers high-residual areas (red box), see Fig. 1(c). Targeting data sensitivity, Uncertainty Sampling (US) (Liu and Li 2023) selects informative samples based on predicted importance. However, due to limited diversity in the random selection process, US often produces redundant samples, reducing efficiency. This is evident in Fig. 1(d), where new samples are scattered across the domain but fail to adequately cover high-residual regions (red box) compared to our method in Fig. 1(e). Thus, these sampling strategies fail to effectively address the challenges.

![](images/17eb43f126f605ef6e74d2425e660b7217b46ab05ad9c369537861ae05ba69d3.jpg)  
Figure 1: Examples of different sampling methods applied to the system coefficients of a cell cycle model (Liu et al. 2012). The orange-blue and black-white color bars indicate trends in oscillation and residual changes, respectively. (a) LHS performs a random sampling of 1000 points in the coefficient domain. (b) IS fails to introduce new samples; (c-d) SMOTE and US lack sufficient coverage in high residual regions (red box). US also produces redundant samples spreading across the domain; (e) Our proposed HGGS method extracts the boundary information (red, yellow boxes) and effectively generates new samples (tear triangle) concentrating on high-residual regions (red box).

This work introduces the Hierarchical Gradient-based Genetic Sampling (HGGS) method, a novel two-layer framework designed to enhance the effectiveness of selecting representative samples and improve the accuracy of predictions for biological oscillations. Specifically, starting with an initial candidate set obtained by Latin Hypercube Sampling (LHS), we first apply Gradient-based Filtering (GF) to select samples near boundary regions and balance the proportions of non-oscillation (majority) and oscillation (minority)

samples. This GF layer yields a balanced coarse dataset that guides subsequent refinement. In the second layer, Multigrid Genetic Sampling (MGS) dynamically constructs grids at multiple levels based on residual (absolute error) information from training data. By sampling within these grids, MGS not only enriches existing boundaries but also explores new high-residual areas. Through continual learning, HGGS iteratively refines the training dataset with more representative samples, consistently improving model performance. The main contributions of this paper are summarized below:

• We propose a simple and efficient Gradient-based Filtering technique that can extract oscillation boundaries, which often entail high residuals, and removes redundant non-oscillatory samples. The GF layer generates balanced coarse data, enabling efficient MGS refinement. • Our Multigrid Genetic Sampling strategy leverages residual information to refine existing boundaries and explore new high-residual regions. The MGS layer systematically samples across grids at different levels, reducing sensitivity and enhancing oscillation diversity. • The proposed HGGS method ensures a representative dataset through continuous adaption to the evolving error landscape during training, showing superior accuracy over seven baselines across four biological systems. HGGS is versatile and can be applied to predict various systematic features beyond biological oscillations.

# 2 Related Work

Sampling for Data Imbalance. Data imbalance often results in biased model performance, favoring majority samples while underperforming on minority ones. To address this issue, several sampling techniques have been proposed, including undersampling, oversampling, and importance sampling techniques. Undersampling randomly removes majority samples (Wilson 1972; Tomek 1976; Guo et al. 2008) to balance the imbalance ratio, but it risks discarding informative data. On the other hand, oversampling mitigates data imbalance by augmenting minority class samples, as in SMOTE (Chawla et al. 2002; Torgo et al. 2013). While SMOTE generates new minority samples to improve balance, it can introduce noise due to interpolated labels and may struggle to create truly representative minority samples. Importance Sampling (IS) (Liu et al. 2021; Lu et al. 2023) combines elements of both undersampling and oversampling by resampling more informative minority samples and excluding well-performing majority samples. However, it is prone to overfitting due to resampling and does not incorporate new data. Although these techniques offer various ways to alleviate data imbalance, they all struggle with the data sensitivity challenge.

Dynamic Sampling. Dynamic sampling offers a more adaptive approach to selecting informative samples, enhancing model training by adjusting sample selection based on the model’s evolving state. This idea is widely utilized in Active Learning (AL), where various dynamic sampling methods leverage direct or indirect information from the model to select the most representative samples from unlabeled data. One prominent method in AL is Uncertainty Sampling (US) (Lewis and Catlett 1994; Zhu et al. 2010; Liu and Li 2023), which selects samples based on their uncertainty scores. In classification, these scores can be calculated using techniques such as entropy uncertainty (Shannon 1948) or confidence margin uncertainty (Sharma and Bilgic 2017). Diversity-based strategies aim to select a broad range of samples based on data distribution, employing methods like gradient representation (Saran et al. 2023) and switch events (Benkert et al. 2023). Query by committee methods (Burbidge, Rowland, and King 2007; Kee, Del Castillo, and Runger 2018; Hino and Eguchi 2023) aggregate outputs from multiple models to form new discriminative criteria, identifying the most representative samples for labeling by considering the underlying data distribution. However, uncertainty sampling and diversity-based sampling often introduce significant redundancy, reducing sampling efficiency. Additionally, most of these methods are designed for nominal target variables and are rarely applicable to regression problems with continuous targets (Liu and Li 2023). In contrast, our HGGS method leverages residual information to ensure targeted sampling in high-residual areas and avoid redundancy, boosting both effectiveness and efficiency.

# 3 Preliminary

In this paper, we utilize a neural network model $\hat { y } ( \pmb { \lambda } ) =$ $f _ { n n } ( \lambda ; \mathbf { \bar { \Theta } } )$ to approximate the oscillatory frequency $y ( \pmb { \lambda } ) =$ $f _ { P } ( \boldsymbol { u } ( t , \lambda ) ) \in \mathbb { R } ^ { D ^ { \prime } }$ of a system under initial state $\mathbf { \delta } \mathbf { u } _ { 0 }$ , given any set of coefficients $\boldsymbol { \lambda } \in \mathbb { R } ^ { D }$ in the ODEs presented below.

$$
\begin{array} { c } { \displaystyle \frac { \mathrm { d } \pmb { u } } { \mathrm { d } t } = \pmb { N } ( \pmb { u } ) , t \in [ 0 , T ] } \\ { \displaystyle { , ( t , \pmb { \lambda } ) | _ { t = 0 } = \pmb { u } _ { 0 } ( \pmb { \lambda } ) , \pmb { \lambda } \in \Omega , } } \end{array}
$$

where $f _ { P }$ is the oscillatory operator used to calculate the oscillatory frequency $f _ { P } ( { \pmb u } ( { \pmb t } , \lambda ) )$ of $\mathbf { \Delta } _ { \pmb { u } }$ over the time span $\pmb { t } = [ 0 , \dot { t _ { 1 } } , t _ { 2 } , \dot { { \mathrm { ~  ~ \scriptstyle ~ \cdot ~ } } } , \dot { T } ] ^ { \top }$ using (Apicella et al. 2013). $\bar { \mathcal { N } ( \boldsymbol { u } ) }$ denotes a nonlinear operator consisting of variables vector $u , \lambda \in \Omega$ is a $D$ -dimensional system coefficient vector, $\Omega$ is a subset of $\mathbb { R } ^ { D } , \boldsymbol { u } _ { 0 }$ is the initial condition vector in $D ^ { \prime }$ dimensions, and $\Theta$ is the parameter of the neural network. Our goal is to minimize the error in approximating the original function $f _ { P } ( { \pmb u } ( { \pmb t } , \pmb \lambda ) )$ using a neural network $f _ { n n } \in F : \mathbb { R } ^ { D } \to \mathbb { R } ^ { D ^ { \prime } }$ , where $F$ represents an appropriate function mapping space. We achieve this by employing supervised learning with the following loss function.

$$
L ( f _ { n n } ( \pmb { \lambda } ; \Theta ) , y ) = \| l ( f _ { n n } ( \pmb { \lambda } ; \Theta ) , y ) \| _ { 2 , \Omega } ^ { 2 } ,
$$

where $l ( f _ { n n } ( \pmb { \lambda } ; \pmb { \Theta } ) , y ) = | f _ { n n } ( \pmb { \lambda } ; \pmb { \Theta } ) - y |$ and $\left\| \cdot \right\| _ { 2 }$ denotes Euclidean normalization.

In practice, Monte Carlo (MC) approximation is used to estimate the overall error $L$ using a finite set of sampled points from the domain, denoted as $L _ { N }$ . Thus, the loss function can be approximated as:

$$
\begin{array} { l } { { \displaystyle { \cal L } ( \boldsymbol { \Theta } ) \approx { \cal L } _ { N } ( \boldsymbol { \Theta } ) = \left\| l ( f _ { n n } ( \boldsymbol { \lambda } ; \boldsymbol { \Theta } ) , y ) \right\| _ { 2 , S _ { \Omega } } ^ { 2 } } } \\ { { \displaystyle ~ = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left\| l ( f _ { n n } ( \boldsymbol { \lambda } ^ { ( i ) } ; \boldsymbol { \Theta } ) , y ^ { ( i ) } ) \right\| _ { 2 } ^ { 2 } } , } \end{array}
$$

where $N$ is the number of samples and ${ \cal S } _ { \Omega } = \{ \lambda ^ { ( i ) } \} _ { i = 1 } ^ { N }$ represents the set of sampled coefficients from $\Omega$ . To theoretically prove that sampling methods can further reduce the error between $f _ { n n }$ and $f _ { P }$ , we build on the framework from (Tang et al. 2023). Through analyzing the MC approximate loss function $L _ { N }$ , we derive the theoretical upper bound of the optimal model $f _ { n n } ( \cdot ) _ { N } ^ { * }$ approximating to oscillatory frequency $y ( \cdot )$ . Additionally, we introduce the theoretically optimal model $f _ { n n } ( \cdot ) ^ { * }$ from overall error $L$ , categorizing the error into two parts:

$$
\begin{array} { r l } & { E ( | | f _ { n n } ( \cdot ) _ { N } ^ { * } - y ( \cdot ) | | _ { \Omega } ) \leq E ( | | f _ { n n } ( \cdot ) _ { N } ^ { * } - f _ { n n } ( \cdot ) ^ { * } | | _ { \Omega } ) } \\ & { \phantom { \frac { 1 } { 1 } } + | | f _ { n n } ( \cdot ) ^ { * } - y ( \cdot ) ) | | _ { \Omega } . } \end{array}
$$

Here, $E$ is the expectation operator, and $\lVert \cdot \rVert _ { \Omega }$ is the norm operator in the function space $F$ . The first term represents the statistical error, while the second term reflects the model approximation error determined by model structure. Sampling new data from the residual distribution over $\Omega$ can reduce statistical error. Proof details are provided in the Appendix.

# 4 Method

This section describes the Hierarchical Gradient-based Genetic Sampling (HGGS) framework (Fig. 2), which divides the sampling process into two layers: Gradient-based Filtering and Multigrid Genetic Sampling. HGGS yields a diverse, balanced dataset, drastically reducing model statistical error.

# Gradient-based Filtering

As demonstrated in Fig. 1, biological systems often contain a significant amount of redundant data in non-oscillatory regions. Removing these redundant non-oscillatory samples not only preserves the model’s overall performance but also increases the proportion of oscillatory samples in the dataset, thereby mitigating the effects of non-oscillatory bias.

To achieve this, we design an effective Gradient-based Filtering (GF) technique that identifies sensitive regions, particularly boundary areas where small changes can lead to significant shifts in oscillatory frequency (see red, yellow boxes in Fig. 1(e)), which are also characterized by higher residuals. In this context, we first rank all samples from the initial dataset $S _ { \Omega }$ using Eq. 4.

$$
g d ( \pmb { \lambda } ^ { ( i ) } ) _ { S _ { \Omega } } = \frac { 1 } { K } \sum _ { j = 1 } ^ { K } \frac { \left\| y ^ { ( j ) } - y ^ { ( i ) } \right\| _ { 2 } ^ { 2 } } { \left\| \pmb { \lambda } ^ { ( j ) } - \pmb { \lambda } ^ { ( i ) } \right\| _ { 2 } ^ { 2 } } ; \pmb { \lambda } ^ { i } , \pmb { \lambda } ^ { j } \in S _ { \Omega } , i \neq j ,
$$

where $g d ( \pmb { \lambda } ^ { ( i ) } ) _ { S _ { \Omega } }$ represents the gradient degree of collocation sample $i$ from the training dataset $S _ { \Omega }$ and $K$ is a hyperparameter denoting the top- $K$ nearest collocation samples to $i$ in the training dataset $S _ { \Omega }$ . This function assesses the importance of collocation sample $i$ ; a higher $g d ( \pmb { \lambda } ^ { ( i ) } ) _ { S _ { \Omega } }$ value indicates that the sample is closer to boundary areas.

According to this gradient ranking, we filter the top$r \%$ samples $S _ { \Omega _ { g f 1 } }$ from $S _ { \Omega }$ , where $r$ is the filtering ratio to extract the boundary information. Next, to enhance the model’s adaptability to non-oscillatory conditions, we retain an essential global dataset, $S _ { \Omega _ { g f 2 } }$ , from the remaining samples in $S _ { \Omega } - S _ { \Omega _ { g f 1 } }$ using uncertainty sampling (Liu and

System Oscillatory coefficients 𝛌 input Coarse Sampling Fine-grained Sampling output Frequency Latin Hypercube Gradient-based Filtering Alternate MGS with Model Training Biological System 𝑋 Sampling New >>> 3000 Extract training Multigrid Genetic Sampling 1000 boundary-info NN A...... dataset 0 Tim50e 100 Model LHS samples Filtered data Training HMiegdhiuremsirdeusaildsualmspalemple Stratified data New samples Samplin each grid

Li 2023). This subset is specifically selected to capture the overall characteristics and maintain performance for nonoscillatory regions. We then formulate the coarse training set $S _ { \Omega } ^ { ( 0 ) } \ = \ S _ { \Omega _ { g f } } \ = \ S _ { \Omega _ { g f 1 } } \cup S _ { \Omega _ { g f 2 } }$ , where $S _ { \Omega _ { g f } } \subseteq S _ { \Omega }$ and $| S _ { \Omega _ { g f } } | < | S _ { \Omega } |$ . In $\boldsymbol { S } _ { \Omega _ { g f } }$ , most redundant non-oscillation samples from the original dataset are removed, while a diverse set of oscillatory data is retained.

# Multigrid Genetic Sampling

From the prior GF layer, we obtained a balanced coarse set of informative samples $S _ { \Omega } ^ { ( 0 ) }$ . Our next goal is to sharpen boundary precision and explore high-residual areas through effective sampling. This requires constructing an accurate residual distribution, as non-oscillatory regions have low probabilities and contribute little to model performance. We focus on constructing high-residual distributions.

However, the small sample set $S _ { \Omega }$ obtained through random sampling does not sufficiently represent the highresidual distribution $p _ { g }$ over the domain $\Omega$ . To address this limitation, we construct multiple sampling grids of varying sizes to capture the high-residual distribution $p _ { g }$ .

We first use a Gaussian Mixture model (Reynolds et al. 2009) to characterize three potential residual distributions within $S _ { \Omega }$ : low-, medium-, and high-residual distributions, respectively, denoted as $S _ { \Omega _ { l r } }$ , $S _ { \Omega _ { m r } }$ , and $S _ { \Omega _ { h r } }$ .

Next, each grid is defined as a hypercube bounded by points $\lambda ^ { ( i ) }$ and $\lambda ^ { ( j ) }$ , both originating from $S _ { \Omega } ^ { ( 0 ) } - S _ { \Omega _ { l r } } ^ { ( 0 ) }$ . To ensure comprehensive coverage of the high-residual distribution, it is necessary to construct a large number of grids at different sizes, each composed of diverse samples.

Building on the idea of multigrid, we integrate genetic sampling into a method called Multigrid Genetic Sampling (MGS) to approximate the high-residual distribution over $\Omega$ and further refine these areas. Specifically, we sample new points within each hypercubic grid according to Eq. 5:

$$
\begin{array} { r } { \lambda _ { n e w } = \alpha \odot ( \mathbf { \lambda } \mathbf { \lambda } ^ { ( i ) } - \mathbf { \lambda } ^ { ( j ) } ) + \lambda ^ { ( j ) } ; \alpha \in [ 0 , 1 ] ^ { D } , i \neq j , } \end{array}
$$

where $\alpha$ is a D-dimensional weight vector with each value in the range [0,1], and $\odot$ denotes component-wise multiplication. $\lambda _ { n e w }$ is a randomly sampled point within the hypercubic enclosed by $\lambda ^ { ( i ) }$ and $\lambda ^ { ( j ) }$ .

In order to obtain a fine-grained dataset from $S _ { \Omega } ^ { ( 0 ) }$ , we alternate MGS with model training to continuously refine the high-residual areas. We define $m _ { c }$ as the number of sam-th sampled da $m _ { e }$ t by MGS is denoted as , and the $k ^ { \prime }$ $\bar { S } _ { \Omega _ { g s } } ^ { ( \bar { k } ^ { \prime } ) }$ k-th training dataset is S(Ωk) = Sk′= $\begin{array} { r } { S _ { \Omega } ^ { ( k ) } = \bigcup _ { k ^ { \prime } = 1 } ^ { k } S _ { \Omega _ { g s } } ^ { ( k ^ { \prime } ) } \cup S _ { \Omega _ { g f } } } \end{array}$

For the $( k + 1 )$ -th sampling, we utilize the current $k$ -th residual samples, categorized in low-, medium-, and highresidual distributions, denoted as $S _ { \Omega _ { l r } } ^ { ( k ) } , \ S _ { \Omega _ { m r } } ^ { ( k ) }$ $S _ { \Omega _ { h r } } ^ { ( k ) }$ Next, to exploit and explore the high-residual domain, we defined crossover and mutation operations over mediumand high-residual sets as follows:

• Multigrid Crossover: Randomly select two distinct sam$\bar { \lambda } _ { h r } ^ { ( i ) }$ $\lambda _ { h r } ^ { ( j ) }$ $S _ { \Omega _ { h r } } ^ { ( k ) }$ and randomly sample a new point $\lambda _ { h h }$ by Eq. 5 and add it to the sample set S(Ωk+1 ). This operation serves to exploit and refine existing high-residual areas. $\lambda _ { h r } ^ { ( i ) }$ $S _ { \Omega _ { h r } } ^ { ( k ) }$   
and $\lambda _ { m r } ^ { ( j ) }$ from $S _ { \Omega _ { m r } } ^ { ( k ) }$ . Form a hypercubic grid using the two samples and sample a new point $\lambda _ { h m }$ by Eq. 5. Add $S _ { \Omega _ { g s } } ^ { ( \bar { k } + 1 ) }$ to explore global boundaries and new high-residual areas.

Before the $( k + 1 )$ -th training round, our method samples $n _ { v 1 }$ h that $n _ { v 2 }$ and $| S _ { \Omega _ { g s } } ^ { ( k + 1 ) } | = n _ { v 1 } + n _ { v 2 }$ $S _ { \Omega _ { g s } } ^ { ( k + 1 ) } = \bigcup _ { i = 1 } ^ { \bar { n } _ { v 1 } } \lambda _ { h h } ^ { ( i ) } \cup$ $\mathrm { U } _ { i = 1 } ^ { n _ { v 2 } } \lambda _ { h m } ^ { ( i ) }$ . Thus, the sample set for the $( k + 1 )$ -th training rSound is S(Ωk+1) = $\overset { \cdot \cdot } { S _ { \Omega } ^ { ( k + 1 ) } } = S _ { \Omega _ { g s } } ^ { ( k + 1 ) } \cup S _ { \Omega } ^ { ( k ) }$ . At the end of sampling, $n _ { s } = m _ { c } \times \left( n _ { v 1 } + n _ { v 2 } \right)$ and $\textstyle S _ { \Omega _ { g s } } = \bigcup _ { k = 1 } ^ { m _ { c } } S _ { \Omega } ^ { ( k ) }$ .

The GF followed by the MGS layer, allows for effective and efficient sampling within high residuals. The pseudocode is presented in Algorithm 1.

Algorithm 1: HGGS for predicting biological oscillations   
Input: NN model $f _ { n n } ( \lambda ; \Theta )$ , neighbors for gradient esti  
mation $K$ , initial sample size $N$ , filtering ratio $r$ , sampling   
cycle $m _ { c }$ , Multigrid Genetic Sampling budget $\{ n _ { v 1 } , n _ { v 2 } \}$   
Initialization: LHS ${ \cal S } _ { \Omega } = \{ ( \lambda ^ { ( i ) } , y ^ { ( i ) } ) \} _ { i = 1 } ^ { N }$   
Output: Target model $f _ { n n } ( \lambda ; \Theta ^ { * } )$   
1: Apply Gradient-based Filtering to SΩ to generate S(Ω0):   
2: $S _ { \Omega } ^ { ( 0 ) } \gets G F ( S _ { \Omega } , r , K )$ mwinhiemrei $S _ { \Omega } ^ { ( 0 ) } \subseteq S _ { \Omega }$ in Eq. 3 $f _ { n n } ( \lambda ; \Theta )$ $L _ { | S _ { \Omega } ^ { ( 0 ) } | }$ 3: for $k = 0 , 1 , \ldots , m _ { c } - 1$ do 4: Compute r(ek)sidual l = {|fnn(λ(i); Θ(k))−y(i)|}|iS=(Ω1k)| 5: Stratify into 3 subdomains based on residual using Gaussian Mixture: $\{ S _ { \Omega _ { l r } } ^ { ( k ) } , S _ { \Omega _ { m r } } ^ { ( k ) } , S _ { \Omega _ { h r } } ^ { ( k ) } \}$ 6: $S _ { \Omega _ { g s } } ^ { ( k + 1 ) } \gets M G S ( n _ { v 1 } , n _ { v 2 } , S _ { \Omega _ { m r } } ^ { ( k ) } , S _ { \Omega _ { h r } } ^ { ( k ) } )$ 7: $S _ { \Omega } ^ { ( k + 1 ) } \gets S _ { \Omega } ^ { ( k ) } \cup S _ { \Omega _ { g s } } ^ { ( k + 1 ) }$ // Update datasets 8: Update $f _ { n n } ( \lambda ; \Theta )$ by minimizing $L _ { | S _ { \Omega } ^ { ( k + 1 ) } | }$ in Eq. 3 9: end for   
10: return $f _ { n n } ( \lambda ; \Theta ^ { * } )$

# 5 Experiments

To validate the proposed HGGS, we conducted experiments on four biological systems known for their oscillatory behaviors. We also performed an ablation study to assess Gradient-based Filtering and Multigrid Genetic Sampling, and examined the sensitivity of model hyperparameters.

# Biological System Dataset

Benchmark datasets from four biological systems were used for method evaluation: the Brusselator system (Prigogine 1978), the Cell Cycle system (Liu et al. 2012), the Mitotic Promoting Factor (MPF) system (Novak and Tyson 1993), and the Activator Inhibitor system (Murray 2002). For each system, we generated $2 0 \mathrm { k } { - } 7 0 \mathrm { k }$ sets of system coefficients using LHS, ran simulations to produce system dynamics, and determined the oscillatory frequency using (Apicella et al. 2013). This data was then used for training and testing of our method. Descriptions of the four biological systems, their corresponding ODEs, and detailed simulation settings are provided in the Appendix.

# Baselines

The proposed HGGS was compared with seven baselines:

• Latin Hypercube Sampling (LHS) (Stein 1987): Divides the coefficient domain into equal grids and randomly samples from each grid to ensure full coverage. • Importance Sampling (IS) (Lu et al. 2023): Resamples based on residual information, updating the training set after each epoch. $\mathrm { I S ^ { \dagger } }$ is an optimization that updates only when the model shows no improvement. • Uncertainty Sampling (US) (Liu and Li 2023): Selects the top- $\cdot o$ new samples from a candidate set based on residual distribution, and adds them to the training set.

We implemented the candidate set in two ways: poolbased (US-P) and streaming-based (US-S). • Weight Reservoir Sampling (WRS) (Efraimidis and Spirakis 2006): Selects the top- $\cdot o$ new samples from the candidate set based on residuals to replace part of the current data, keeping a constant training sample size. • Volume Sampling for Streaming Active Learning (VeSS) (Saran et al. 2023): Selects samples based on their gradient distribution relative to the model, which is a type of diversity-based sampling in AL.

# Implementation

Our algorithm was implemented using the PyTorch framework on a single NVIDIA A6000 GPU. We utilized $N =$ 10k samples for initial training and 5k samples for validation for each experiment. For a thorough evaluation, our testing data consists of four subsets, characterizing different types of the coefficient domain: overall (entire testing data), majority (non-oscillatory samples only), minority (oscillatory samples only), and boundary (top $20 \%$ samples ranked by gradient using Eq. 4). The total size of the testing data varies between $7 \mathrm { k } \mathrm { - } 6 0 \mathrm { k }$ , depending on the oscillatory systems.

The neural network (Multi-Layer Perceptron), consisting of 3 or 4 hidden layers, was trained for $3 \mathrm { k }$ epochs per sampling cycle using the Adam optimizer with a learning rate of $\bar { 2 } \substack { - 2 . 5 \times 1 0 ^ { - 3 } }$ , employing full batch training and early stopping. For key hyperparameters, the GF filtering ratio was set to $r = 2 0 \%$ , with $K = 5$ nearest neighbors and a GF sample size of $n _ { f } = N / 2$ . During the sampling cycles, the MGS ratio was set to $n _ { v 1 } : n _ { v 2 } = 6 : 4$ , with an MGS sample size of $n _ { s } = N / 2$ . Implementation details and other parameters for each experiment can be found in the Appendix. All reported results below were based on five independent experiments.

# Results

Metrics. Root Mean Square Error (RMSE) is our primary metric. Imbalance Ratio (IR) and Gini Index (GI) quantify the proportion of non-oscillatory to oscillatory samples and the diversity of oscillatory frequency labels, respectively. Lower IR and GI indicate more effective handling of nonoscillatory bias and oscillatory boundary sensitivity by the sampling method. RMSE, IR, and GI are defined below.

$$
\mathrm { R M S E } = \sqrt { \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left. f _ { n n } ( \mathbf { \lambda } \mathbf { \lambda } ^ { ( i ) } ; \boldsymbol { \Theta } ) - y ^ { ( i ) } \right. _ { 2 } ^ { 2 } }
$$

$$
\mathrm { I R } = \frac { \sum _ { i = 1 } ^ { N } I ( y ^ { ( i ) } = 0 ) } { \sum _ { i = 1 } ^ { N } I ( y ^ { ( i ) } \neq 0 ) } ; \mathrm { G I } = \frac { \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { N } \left| y ^ { ( i ) } - y ^ { ( j ) } \right| } { 2 N \sum _ { i = 1 } ^ { N } y ^ { ( i ) } }
$$

Brusselator. In Fig. 3(a), our method achieves the lowest RMSE for the overall testing data (0.0085). Notably, for crucial minority and boundary cases, we observe $2 4 \% - 7 1 \%$ and $1 3 \% { - } 6 6 \%$ improvement over the baseline approaches. In this low-dimensional, densely-packed space, $\mathrm { \dot { I } S ^ { \dagger } }$ (minority: 0.0119; boundary: 0.0320) performs well but lacks sufficient boundary information. HGGS, however, effectively targets more boundary instances, particularly in highresidual boundary regions, as illustrated in Appendix. Due to

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">Brusselator System</td><td colspan="2">Cell Cycle System</td><td colspan="2">MPF System</td><td colspan="2">ActivatorInhibitor System</td></tr><tr><td>IR</td><td>GI</td><td>IR</td><td>GI</td><td>IR</td><td>GI</td><td>IR</td><td>GI</td></tr><tr><td>LHS</td><td>1.47±0.00</td><td>0.76±0.00</td><td>4.43±0.00</td><td>0.85±0.00</td><td>5.27±0.00</td><td>0.89±0.00</td><td>11.84±0.00</td><td>0.94±0.00</td></tr><tr><td>WRS</td><td>1.37±0.05</td><td>0.75±0.00</td><td>4.41±0.12</td><td>0.85±0.00</td><td>5.16±0.18</td><td>0.89±0.00</td><td>11.32±0.48</td><td>0.94±0.00</td></tr><tr><td>VeSS</td><td>1.42±0.12</td><td>0.75±0.01</td><td>3.98±0.16</td><td>0.84±0.01</td><td>3.57±0.37</td><td>0.85±0.01</td><td>10.96±0.56</td><td>0.94±0.00</td></tr><tr><td>IS</td><td>1.35±0.23</td><td>0.77±0.03</td><td>1.86±0.36</td><td>0.70±0.04</td><td>3.07±0.44</td><td>0.82±0.02</td><td>6.04±1.15</td><td>0.88±0.02</td></tr><tr><td>ISt</td><td>1.20±0.15</td><td>0.75±0.02</td><td>2.00±0.44</td><td>0.71±0.04</td><td>3.11±0.78</td><td>0.82±0.03</td><td>6.09±2.07</td><td>0.88±0.04</td></tr><tr><td>US-S</td><td>1.18±0.11</td><td>0.72±0.01</td><td>3.93±1.53</td><td>0.83±0.05</td><td>3.75±0.37</td><td>0.85±0.01</td><td>11.58±2.90</td><td>0.94±0.02</td></tr><tr><td>US-P</td><td>1.08±0.17</td><td>0.70±0.02</td><td>2.3±0.22</td><td>0.76±0.02</td><td>2.75±0.41</td><td>0.81±0.02</td><td>7.12±0.24</td><td>0.91±0.00</td></tr><tr><td>Ours</td><td>1.18±0.12</td><td>0.71±0.03</td><td>1.10±0.02</td><td>0.58±0.01</td><td>1.11±0.07</td><td>0.65±0.01</td><td>2.88±0.13</td><td>0.77±0.01</td></tr></table></body></html>

Table 1: Imbalance Ratio (IR) and Gini Index (GI) for seven baseline methods and our HGGS across four biological systems HGGS achieves the lowest IR and GI in most cases.

![](images/607dad44383ba453bc9371450a6350162aab048851d01b6748724678725fcd69.jpg)  
Figure 3: Accuracy comparison of seven baseline methods (LHS, WRS, VeSS, IS, $\mathrm { I S ^ { \dagger } }$ , US-S, US-P) and our HGGS across fou biological systems. HGGS obtained the lowest RMSEs across all testing subsets: minority, boundary, majority, and overall.

the relatively mild data imbalance in the Brusselator system, the differences in IR and GI between our method, US-S, and US-P are minimal. Nonetheless, HGGS efficiently samples at the boundaries and improves prediction accuracy compared to the others.

Cell Cycle. In this complex cell cycle system, our method distinguishes itself with the lowest IR and GI in Table 1, highlighting its dual strength in balancing data distribution and enriching sample diversity. HGGS also obtains the lowest RMSE error (overall: 0.0098), as shown in Fig. 3(b).

Specifically for minority and boundary cases, our method shows improvements of $8 \% { - } 2 8 \%$ and $8 \% - 2 4 \%$ over the other seven baselines. US-P (minority: 0.0169; boundary: 0.0209) includes high-residual samples but suffers from redundancy due to random sampling. In contrast, our method directly targets high-risk areas, avoiding this redundancy and demonstrating superior efficiency and effectiveness.

MPF. Fig. 3(c) shows that our method excels in achieving minimal RMSE (overall: 0.0101), a decrease of $7 \% - 2 7 \%$ compared to other methods. It also stands out for minority

Majority (RMSE) Majority Minority Minority (RMSE)   
14.0 ×10-3 ×10-3 11.0 ×10-3 ×10-3 19.0 20.0 17.0   
12.0 9.0 10.0   
10.0 15.0 8.0 0.0 7.0 13.0 LHS GF MGS HGGS LHS GF MGS HGGS (a) Brusselator System (b) Cell Cycle System   
10.5 ×10-3 ×10-3 20.0 6.0 ×10-3 ×10-3 13.0   
9.5 5.0 18.0 11.0   
8.5 4.0   
7.5 16.0 3.0 9.0 LHS GF MGS HGGS LHS GF MGS HGGS (c) MPF System (d) Activator Inhibitor System Boundary Majority Minority   
29.0 27.0 21.3 18.5 ×10-3 ×10-3 20.7 10 ×10-3 22.0 17.9 20.1   
21.0 17.019.5 0 3 6 9 12 Only Mutation Only Crossover (a) K (b) 𝑛𝑣1: 𝑛𝑣2   
Majority (RMSE) ×10-3 ×10-3 21.0 ×10-3 ×10-3 9.5 9.5 8.5 17.0 8.5 1 7.5 13.0 7.5 0 210 211 212 213 5% 20% 40% 50% (c) 𝑛𝑠 (d) r

and boundary cases (minority: 0.0174; boundary: 0.0200). VeSS performs adequately for minority and boundary cases, but its weaker performance in the majority case reduces its overall effectiveness. Table 1 further underscores the effectiveness of HGGS in mitigating non-oscillatory bias in the MPF system. Our method achieves the lowest GI among all baselines, reflecting its superior ability to uncover informative oscillatory patterns and enhance diversity.

Activator Inhibitor. The Activator Inhibitor system is characterized by a pronounced data imbalance, with a staggering IR of 11.84 under LHS. However, as highlighted in Table 1, our method exhibits remarkable resilience against such extreme scenarios, effectively reducing the IR to about 2.88 while simultaneously enhancing data diversity to a GI of 0.77. Moreover, HGGS has the lowest error across all categories (minority: 0.0099; boundary: 0.0130; majority: 0.0036; overall: 0.0044) (Fig. 3(d)). It particularly excels in minority and boundary cases, showing improvements of $1 8 \% { - } 3 1 \%$ and $1 6 \% - 2 8 \%$ over other baselines.

# Ablation Study

We conducted an ablation study on four biological systems to evaluate the efficacy of the two layers: Gradient-based Filtering (GF) and Multigrid Genetic Sampling (MGS).

Effect of Gradient-based Filtering. Relying solely on GF followed by random sampling does not effectively address non-oscillatory bias and oscillatory boundary sensitivity. Although GF offers initial benefits, subsequent random sampling fails to sustain improvements. As shown in Fig. 4, GF yields higher RMSEs for majority and minority cases across four systems than HGGS, indicating that MGS produces more informative samples than random sampling.

Effect of Multigrid Genetic Sampling. When MGS is used without prior GF, it struggles to precisely identify informative minority samples, leading to higher RMSEs for minority class across all four systems compared to HGGS (Fig. 4). This suggests that GF provides critical guidance for MGS to effectively target and refine minority samples.

These findings underscore the synergy between GF and MGS within HGGS. GF enables an efficient extraction of sample domains, identifying coarse boundaries rich in critical information. Informed by GF, MGS operates with greater precision to refine minority samples and explore new highrisk areas. Together, they enhance data representativeness and diversity, improving model performance.

# Sensitivity Analysis

Fig. 5(a-c) illustrates the effects of key model parameters, including the number of neighbors $K$ , MGS ratio $n _ { v 1 } : n _ { v 2 }$ , and MGS sampling size $n _ { s }$ on the MPF system. In Fig. 5(a), varying $K$ produces an elbow curve, with $K \ : = \ : 5$ offering optimal performance. Increasing $K$ beyond this point has little effect on gradient estimation, while decreasing $K$ leads to significant errors in oscillatory frequency estimation. In Fig. 5(b), MGS cannot achieve optimal performance using only crossover or mutation operation. Our experiments found that an MGS ratio of 6:4 yielded the lowest RMSE. In Fig. 5(c), increasing the MGS sampling size $n _ { s }$ continues to reduce RMSE for the minority case, while the majority case shows no further improvement beyond $2 \mathrm { k }$ new samples. Fig. 5(d) shows the effect of GF filtering ratio on the Cell Cycle system. Increasing $r$ reduces the sampling of majority (non-oscillation) class, leading to higher error, while the opposite is true for minority class. A ratio of $r = 2 0 \%$ offers the best balance between majority and minority samples.

# Conclusion

This paper introduces Hierarchical Gradient-based Genetic Sampling, a two-layer framework designed to address nonoscillatory bias and boundary sensitivity in predicting biological oscillations. The first layer, gradient-based filtering, selects a representative subset from initial random sampling, creating a balanced coarse dataset. The second layer, genetic sampling, refines minority instances and explores new highresidual information, enhancing data diversity. Experiments show that HGGS achieves the best accuracy across four biological systems, particularly for the oscillation class.