# ConfigX: Modular Configuration for Evolutionary Algorithms via Multitask Reinforcement Learning

Hongshu Guo1\*, Zeyuan $\mathbf { M } \mathbf { a } ^ { 1 * }$ , Jiacheng Chen1, Yining Ma2, Zhiguang $\mathbf { C a o } ^ { 3 }$ , Xinglin Zhang1, Yue-Jiao Gong 1†

1South China University of Technology,   
2Massachusetts Institute of Technology,   
3Singapore Management University

# Abstract

Recent advances in Meta-learning for Black-Box Optimization (MetaBBO) have shown the potential of using neural networks to dynamically configure evolutionary algorithms (EAs), enhancing their performance and adaptability across various BBO instances. However, they are often tailored to a specific EA, which limits their generalizability and necessitates retraining or redesigns for different EAs and optimization problems. To address this limitation, we introduce ConfigX, a new paradigm of the MetaBBO framework that is capable of learning a universal configuration agent (model) for boosting diverse EAs. To achieve so, our ConfigX first leverages a novel modularization system that enables the flexible combination of various optimization sub-modules to generate diverse EAs during training. Additionally, we propose a Transformer-based neural network to meta-learn a universal configuration policy through multitask reinforcement learning across a designed joint optimization task space. Extensive experiments verify that, our ConfigX, after large-scale pretraining, achieves robust zero-shot generalization to unseen tasks and outperforms state-of-the-art baselines. Moreover, ConfigX exhibits strong lifelong learning capabilities, allowing efficient adaptation to new tasks through fine-tuning. Our proposed ConfigX represents a significant step toward an automatic, all-purpose configuration agent for EAs.

# 1 Introduction

Over the decades, Evolutionary Algorithms (EAs) such as Genetic Algorithm (GA) (Holland 1992), Particle Swarm Optimization (PSO) (Kennedy and Eberhart 1995) and Differential Evolution (DE) (Storn and Price 1997) have been extensively researched to tackle challenging Black-Box Optimization (BBO) problems, where neither the mathematical formulation nor additional derivative information is accessible. On par with the development of EAs, one of the most crucial research avenues is the Automatic Configuration (AC) for EAs (Anso´tegui, Sellmann, and Tierney 2009; Huang, Li, and Yao 2019). Generally speaking, AC for EAs aims at identifying the optimal configuration $c ^ { * }$ from the configuration space $\mathcal { C }$ of an evolutionary algorithm $A$ , across a set of BBO problem instances $\boldsymbol { \mathcal { T } }$ :

$$
c ^ { * } = \underset { c \in \mathcal { C } } { \arg \operatorname* { m a x } } \ \underset { p \in \mathcal { Z } } { \mathbb { E } } \left[ P e r f ( A , c , p ) \right]
$$

where $P e r f ( )$ denotes the performance of a configuration for the algorithm under a given problem instance.

Traditionally, the primary research focus in AC for EAs has centered on human-crafted AC mechanisms. These mechanisms, including algorithm/operator selection (Fialho 2010) and parameter control (Aleti and Moser 2016), have demonstrated strong performance on well-known BBO benchmarks (Hansen et al. 2010), as well as in various eye-catching real-world scenarios such as ProteinDocking (Hwang et al. 2010), AutoML (Vanschoren et al. 2014), and Prompting Optimization of Large Language Models (Chen, Dohan, and So 2024). However, a major limitation of manual AC is its heavy reliance on deep expertise. To address a specific problem, one often needs to consult experts with the necessary experience to analyze the problem and then design appropriate AC mechanisms (as depicted in the top of Figure 1). This impedes the broader application of EAs in diverse scientific or industrial applications.

Recently, a novel paradigm called Meta-learning for Black-Box Optimization (MetaBBO) (Ma et al. 2023), has emerged in the learning-to-optimize community. MetaBBO aims to reduce the reliance on expert-level knowledge in designing more automated AC mechanisms. As shown in the middle of Figure 1, in MetaBBO, a neural network is metatrained as a meta-level policy to maximize the expected performance (Eq. (1)) of a low-level algorithm by dictating suitable configuration for solving each problem instance. By leveraging the data-driven features of deep models and the generalization strengths of meta-learning (Finn, Abbeel, and Levine 2017) across a distribution of optimization problems, these MetaBBO methods (Ma et al. 2024c; Li et al. 2024; Song et al. 2024) have shown superior adaptability compared to traditional human-crafted AC baselines.

Despite these advancements, there remains significant potential to further reduce the expertise burden. Current MetaBBO methods often need custom neural network designs, specific learning objectives, and frequent retraining or even complete redesigns to fit different backbone EAs, overlooking the shared aspects of AC across multiple EAs. This

![](images/b1011159a2aed767609d4e0f63d5c20e900e6a956178ef7af5926278cc81acc0.jpg)  
Figure 1: Conceptual overview of different AC paradigms.

$T = \left( A _ { m } , I _ { n } \right)$ , the Transformer generates configurations by conditioning on a sequence of state tokens corresponding to the sub-modules in $A _ { m }$ . Through large-scale multitask reinforcement learning over the sampled tasks, it yields a universal meta-policy that exhibits robust generalization to unseen algorithm structures and problem instances.

We summarize our contributions in this paper in three folds:

• We introduce ConfigX, the first MetaBBO framework to learn a pre-trained universal AC agent via multitask reinforcement learning, enabling modular configuration of diverse EAs across various optimization problems. • Technically, we present Modular-BBO as a novel system for EA modularization that simplifies the management of sub-modules and facilitates the sampling of diverse algorithm structures. We then propose a Transformer-based architecture to meta-learn a universal configuration policy over our defined joint optimization task space. • Extensive benchmark results show that the configuration policy pre-trained by ConfigX not only achieves superior zero-shot performance against the state-of-the-art AC software SMAC3, but also exhibits favorable lifelong learning capability via efficient fine-tuning.

# 2 Related Works

leads to the core research question of this paper: Is it possible to develop a MetaBBO paradigm that can meta-learn an automatic, all-purpose configuration agent for diverse EAs? We outline the detailed research objective below:

$$
c _ { k } ^ { * } = \underset { c \in \mathcal { C } _ { k } } { \arg \operatorname* { m a x } } \ \underset { p \in \mathcal { T } } { \mathbb { E } } \left[ P e r f ( A _ { k } , c , p ) \right] , k = 1 , 2 , . . . , K
$$

where $K$ is an exceedingly large number, potentially infinite. This objective is far more challenging since it can be regarded as the extension of Eq. (1). Concretely, it presents two key challenges: 1) Constructing a comprehensive evolutionary algorithm space is crucial for addressing Eq. (2), from which diverse EAs can be easily sampled for metatraining the MetaBBO; 2) Ensuring the generalization capability of the learned meta-level policy across not only optimization problems but also various EAs is imperative.

To address these challenges, we introduce ConfigX, a pioneering MetaBBO framework capable of modularly configuring diverse EAs with a single model across different optimization problems (as shown at the bottom of Figure 1).

Specifically, to address the first challenge, we present a novel modularization system for EAs, termed Modular-BBO in Section 3.1. It leverages hierarchical polymorphism to efficiently encapsulate and maintain various algorithmic submodules within the EAs, such as mutation or crossover operators. By flexibly combining these sub-modules, ModularBBO can generate a vast array of distinct EA structures, hence spanning a comprehensive algorithm space $\mathcal { A }$ . To address the second challenge, we combine the problem instance space $\boldsymbol { \mathcal { T } }$ and $\mathcal { A }$ to form a joint optimization task space $\tau : A \times \tau$ . We then consider meta-learning a Transformer based meta-level policy over moderate optimization tasks sampled from the joint space $\tau$ to maximize the objective in Eq. (2), see Section 3.2 and 3.3. For each task

# 2.1 Human-crafted AC

Human-crafted AC mechanisms enhance the optimization robustness of EAs through two main paradigms: Operator Selection (OS) and Parameter Control (PC). OS is geared towards selecting proper evolutionary operators (i.e., mutation in DE (Qin and Suganthan 2005)) for EAs to solve target optimization problems. To this end, such AC mechanism requires preparing a group of candidate operators with diverse searching behaviours. Besides, throughout the optimization progress, OS facilitates dynamic selection over the candidate operators, either by a roulette wheel upon the historical success rates (Lynn and Suganthan 2017) or random replacement upon the immediate performance improvement (Mallipeddi et al. 2011). PC, on the other hand, aims at configuring (hyper-) parameters for the operators in EAs, (e.g. the inertia weights in PSO (Amoshahy, Shamsi, and Sedaaghi 2016) and the scale factors in DE (Zhang and Sanderson 2009; Tanabe and Fukunaga 2013)), while embracing similar adaptive mechanisms as OS to achieve dynamic configuration. We note that OS and PC are complementary rather than conflicting. Recent outperforming EAs such as MadDE (Biswas et al. 2021), AMCDE (Ye et al. 2023) and SAHLPSO (Tao et al. 2021) integrate both to obtain maximal performance gain. However, the construction of the candidate operators pool, the parameter value range in PC, and the adaptive mechanism in both of them heavily rely on expertise. A more versatile and efficient alternative for human-crafted AC is Bayesian Optimization (BO) (Shahriari et al. 2015). By iteratively updating and sampling from a posterior distribution over the algorithm configuration space, a recent open-source BO software SMAC3 (Lindauer et al. 2022) achieves the state-of-the-art AC performance on many realistic scenarios.

Module High-level Inheritance Legal algoritnm   
- id: None Add Propertie config_space 1 structure   
+ get_id() + exec() Initialize Add Methods get_config() 1   
- iUd:ncNonetrollable - coCnofingt_rsoplalcae:blNeone Middle-level Inheritance 1 Crossover + set_config() Add Methods get_rule() ↓ ? ↓ Initialization Selection Mutation Crossover Low-level Inheritance Illegal algoritnm   
- topology_rule: List .. . - topology_rule: List - topology_rule: List - topology_rule: List Specify id structure   
+ get_rule() + get_rule() + get_rule() + get_rule() config_space Initialize √ 1 √ √ Overload exec() Mutation Uniform LHS Binomial SBX 一   
- id: Int ..。 - id: Int . . - cido:nfIingt_space: Dict . . - icdo:nIfnitg_space: Dict ：。 . 1 Selection   
+ exec() + exec() + exec() + exec() H Crossover

# Algorithm 1: Algorithm Structure Generation.

Input: All accessible modules $\mathbb { M }$ , all Initialization modules $\mathbb { M } _ { \mathrm { i n i t } }$   
Output: A legal algorithm structure $A$ .   
1: Create an empty structure $A = \varnothing$ , set index $j = 0$   
2: Randomly select an Initialization module from $\mathbb { M } _ { \mathrm { i n i t } }$ as $a _ { j }$   
3: $A  A \bigcup a _ { j }$   
4: while notSCOMPLETED do   
5: $j = j + 1$   
6: while VIOLATED do   
7: Randomly select a module from $\mathbb { M } \backslash \mathbb { M } _ { \mathrm { i n i t } }$ as $a _ { j }$   
8: Check the violation between $a _ { j }$ and $a _ { j - 1 }$   
9: end while   
10: $A  A \bigcup a _ { j }$   
11: end while

# 2.2 MetaBBO

To relieve the expertise dependency of human-crafted AC, recent MetaBBO works introduce neural network-based control policy (typically denoted as the meta-level policy $\pi _ { \boldsymbol { \theta } }$ ) to automatically dictate desired configuration for EAs (Ma et al. 2024d; Yang, Wang, and Li 2024). Generally speaking, the workflow of MetaBBO follows a bi-level optimization process: 1) At the meta level, the policy $\pi _ { \boldsymbol { \theta } }$ configures the low-level EA and assesses its performance, termed meta performance. The policy leverages this observed meta performance to refine its decision-making process, training itself through the maximization of accumulated meta performance, thereby advancing its meta objective. 2) At the lower level, the BBO algorithm receives a designated algorithmic configuration from the meta policy. With this configuration in hand, the low-level algorithm embarks on the task of optimizing the target objective. It observes the changes in the objective values and relays this information back to the meta optimizer, contributing to the meta performance signal. Similarly, existing MetaBBO works focus predominantly on OS and PC. Although a predefined operator group remains necessary, the selection decisions in works on OS (Sharma et al. 2019; Tan and Li 2021; Lian et al. 2024) are made by the meta policy $\pi _ { \boldsymbol { \theta } }$ which relieves the expert-level knowledge requirement. A notable example is RL-DAS (Guo et al. 2024) where advanced DE algorithms are switched entirely for complementary performance. In PC scenarios, initial works parameterize $\pi$ with simple Multi-Layer Perceptron (MLP) (Wu and Wang 2022; Tan et al. 2022) and Long Short-Term Memory (LSTM) (Sun et al. 2021), whereas the latest work GLEET (Ma et al. 2024b) employs Transformer (Vaswani et al. 2017) architecture for more versatile configuration. Besides, works jointly configure both OS and PC such as MADAC (Xue et al. 2022) also show robust performance (Eimer et al. 2021).

# 3 Methodology

In this section, we elaborate on ConfigX step by step. We first explain in Section 3.1 the design of Modular-BBO and how to use it for efficient generation of diverse algorithm structures. We next provide a Markov Decision Process (MDP) definition of an optimization task and derive the corresponding multi-task learning objective in Section 3.2. At last, we introduce in Section 3.3 the details of each component in the MDP and the Transformer based architecture.

# 3.1 Modular-BBO

As illustrated in the left of Figure 2, the design philosophy of Modular-BBO adheres to a Hierarchical Polymorphism in Python which ensures the ease of maintaining different sub-modules (third-level sub-classes in Figure 2, labeled in green), as well as their practical variants (bottom-level subclasses in Figure 2, labeled in red) in modern EAs. By facilitating the high-to-low level inheritances, Modular-BBO provides universal programming interfaces for the modularization of EAs, along with essential module-specific properties/methods to support diverse behaviours of various submodules. Further elaboration is provided below.

High-level. All sub-module classes in Modular-BBO stem from an abstract base class MODULE. It declares universal properties/interfaces shared among various sub-module variants, yet leave them void. At high-level inheritance, two sub-classes UNCONTROLLABLE and CONTROLLABLE inherit from MODULE. The two sub-classes divide all possible sub-modules in modern EAs into the ones with (hyper-) parameters and those without. For CONTROLLABLE modules, we declare its (hyper-) parameters by adding a config space property. Additionally, we include the corresponding get config() and set config() methods for configuring the (hyper-) parameters. Currently, these properties and methods remain void until a specific EA sub-module is instantiated.

Middle-level. At this inheritance level, UNCONTROLLABLE and CONTROLLABLE are further divided into common sub-modules in EAs, e.g., initialization, mutation, selection and etc. To combine these sub-modules legally and generate legal algorithm structures, we introduce modulespecific topology rule as a guidance during the generating process (Algorithm 1), by invoking the added get rule() method. We present a pair of examples in the left of Figure 2 to showcase one of the possible violation during the algorithm structure generation, where CROSSOVER is not allowed after SELECTION is a common sense in EAs.

Low-level. Within the low-level inheritance, we borrow from a large body of EA literature diverse practical submodule variants (i.e., lots of initialization strategy have been proposed in literature such as Sobol sampling (Sobol 1967) and LHS sampling (McKay, Beckman, and Conover 2000)) and maintaining them by inheriting from the sub-module classes in middle-level inheritance. When inheriting from the parent class, a concrete sub-module variant has to instantiate void modules id and config space, which detail its unique identifier in Modular-BBO and controllable parameters respectively. It also have to overload exec() method by which it operates the solution population. The unique module id of a sub-module variant is a 16-bit binary code of which: 1) the first bit is 0 or 1 to denote if this variant is UNCONTROLLABLE or CONTROLLABLE. 2) the 2-nd to 7-th bits denote the sub-module category (third-level sub-classes in Figure 2, labeled in green) to which the variant belongs. 3) the last 9 bits denotes its id within this sub-module category.

For now, Modular-BBO has included 11 common sub-module categories in EAs: INITIALIZATION (Kazimipour, Li, and Qin 2014), MUTATION (Das, Mullick, and Suganthan 2016), CROSSOVER (Spears 1995), PSO UPDATE (Shami et al. 2022), BOUNDARY CONTROL (Kadavy et al. 2023), SELECTION (Shukla, Pandey, and Mehrotra 2015), MULTI STRATEGY (Gong et al. 2011), NICHING (Ma et al. 2019), INFORMA

TION SHARING (Toulouse, Crainic, and Gendreau 1996), RESTART STRATEGY (Jansen 2002), POPULATION REDUCTION (Pool and Nielsen 2007). We construct a collection of over 100 variants for these sub-module categories from a large body of literature and denote this collection as module space M. Theoretically, by using the algorithm generation process described in Algorithm 1, Modular-BBO spans a massive algorithm structure space $\mathcal { A }$ containing millions of algorithm structures. Due to the limitation of space, we provide the detail of each sub-module variant in M in Appendix A, Table 1, including the id, name, type, configuration space, topology rule and functional description. We also provide a detailed explanation for Algorithm 1 in Appendix B.

# 3.2 Multi-task Learning in ConfigX

Optimization Task Space We first define an optimization task space $\tau$ as a synergy of an algorithm space $\mathcal { A }$ and an optimization problem set $\boldsymbol { \mathcal { T } }$ . Then an optimization task $T \in { \mathcal { T } }$ can be defined as $T : \{ A \in { \mathcal { A } } , p \in { \mathcal { T } } \}$ . In this paper, we adopt the algorithm space spanned by ModularBBO as $\mathcal { A }$ , the problem instances from well-known CoCoBBOB (Hansen et al. 2010), Protein-docking (Hwang et al. 2010) and HPO-B benchmark (Arango et al. 2021) as $\boldsymbol { \mathcal { T } }$ .

AC as an MDP For an optimization task $T : \{ A , p \}$ , we facilitate a Transformer based policy $\pi _ { \boldsymbol { \theta } }$ (detailed in Section 3.3) to dynamically dictate desired configuration for $A$ to solve $p$ . This configuration process can be formulated as an Markov Decision Process (MDP) $\begin{array} { r } { \mathcal { M } : = } \end{array}$ $( S , C , \Gamma , R , H , \gamma )$ , where $\boldsymbol { S }$ denotes the state space that reflect optimization status, $C$ denotes the action space which is exactly the configuration space of algorithm $A$ , $\Gamma ( s _ { t + 1 } | s _ { t } , c _ { t } )$ denotes the optimization transition dynamics, $R ( s _ { t } , c _ { t } )$ measures the single step optimization improvement obtained by using configuration $c _ { t }$ for optimizing $p$ . $H$ and $\gamma$ are the number of optimization iterations and discount factor respectively. At each optimization step $t$ , the policy $\pi _ { \boldsymbol { \theta } }$ receives a state $s _ { t }$ and then outputs a configuration $c _ { t } ~ = ~ \pi _ { \theta } ( s _ { t } )$ for $A$ . Using $c _ { t }$ , algorithm $A$ optimizes the optimization problem $p$ for a single step. The goal is to find an optimal policy $\pi _ { \theta ^ { * } }$ which maximizes the accumulated optimization improvement during the optimization process: $\begin{array} { r } { \bar { G } = \sum _ { t = 1 } ^ { H } \gamma ^ { t - 1 } R ( s _ { t } , c _ { t } ) } \end{array}$ . Recall that our ConfigX aims at addressing a more challenging objective in Eq. (2), where the goal is to maximize the accumulated optimization improvement $G$ of all tasks $T \in \mathcal { T }$ . We use $s _ { t } ^ { i }$ and $c _ { t } ^ { i }$ to denote the input state and the outputted configuration of the policy $\pi _ { \boldsymbol { \theta } }$ for the $i$ -th task in $\tau$ . Then the objective in Eq. (2) can be rewritten as a multi-task RL problem:

$$
\mathbb { J } ( \theta ) = \frac { 1 } { K \cdot N } \sum _ { i = 1 } ^ { K \cdot N } \sum _ { t = 1 } ^ { H } \gamma ^ { t - 1 } R ( s _ { t } ^ { i } , c _ { t } ^ { i } )
$$

where we sample $K \cdot N$ tasks from $\tau$ to train $\pi _ { \boldsymbol { \theta } }$ since the number of tasks in $\tau$ is massive. These tasks is sampled first by calling Algorithm 1 $K$ times to obtain $K$ algorithm structures, and then combine these algorithm with the $N$ problem instances in $\boldsymbol { \mathcal { T } }$ . In this paper we use Proximal Policy Opti

+ 1 司 1昌1昌 Per Module Configuration Module ID LFaenadtsucraepse Positional Encoding x3 1

mization (PPO) (Schulman et al. 2017), a popular policy gradient (Williams 1992) method for optimizing this objective in a joint policy optimization (Gupta et al. 2022) fashion. We include the pseudocode of the RL training in Appendix D.

# 3.3 ConfigX

Progress in MetaBBO has made it possible to meta-learn neural network-based control policies for configuring the backbone EAs to solve optimization problems. However, existing MetaBBO methods are not suitable for the massive algorithm structure space $\mathcal { A }$ spanned by the module space M in our proposed Modular-BBO, since learning a separate policy for each algorithm structure is impractical. However, the modular nature of EAs implies that while each structure is unique, they are still constructed from the same module space and potentially shares sub-modules and workflows with other algorithm structures. We now describe how ConfigX exploits this insight to address the challenge of learning a universal controller for different algorithm structures.

State Design In ConfigX, we encode not only the algorithm structure information but also the optimization status information into the state representation to ensure the generalization across optimization tasks. Concretely, as illustrated in the left of Figure 3, for $i$ -th tasks $T _ { i } ~ : ~ \{ A _ { i } , p _ { i } \}$ in the sampled $K \cdot N$ tasks, we encode a information pair for each sub-module in $A _ { i }$ , e.g., $s _ { i } : \{ s _ { i , j } ^ { \mathrm { i d } } , s _ { i } ^ { \mathrm { o p t } } \} _ { j = 1 } ^ { L _ { i } }$ , where $s _ { i , j } ^ { \mathrm { i d } } \in \{ 0 , 1 \} ^ { 1 6 }$ denotes the unique module id for $j$ -th submodule in $A _ { i }$ , $s _ { i } ^ { \mathrm { o p t } } \ \in \ \mathbb { R } ^ { 9 }$ denotes the algorithm performance information which we borrow the idea from recent MetaBBO methods (Guo et al. 2024; Ma et al. 2024a), $L _ { i }$ denotes the number of sub-modules in $A _ { i }$ . We provide details of these information pairs in Appendix C.

State Encode We apply an MLP fusion layer to preprocess the state representation $s _ { i }$ . This fusion process ensures the information within the information pair $\{ s _ { i , j } ^ { \mathrm { i d } } , s _ { i } ^ { \mathrm { o p t } } \}$ join each other smoothly (as illustrated in left part of Figure 3).

$$
\hat { e } _ { i , j } = \mathrm { h s t a c k } ( \phi ( s _ { i , j } ^ { \mathrm { i d } } ; \mathbf { W } _ { e } ^ { \mathrm { i d } } ) ; \phi ( s _ { i } ^ { \mathrm { o p t } } ; \mathbf { W } _ { e } ^ { \mathrm { o p t } } ) )
$$

$$
e _ { i , j } = \phi ( \hat { e } _ { i , j } ; \mathbf { W } _ { e } ) , \quad j = 1 , . . . , L _ { i }
$$

Where $\phi ( \cdot ; \mathbf { W } _ { e } ^ { \mathrm { i d } } )$ , $\phi ( \cdot ; { \bf W } _ { e } ^ { \mathrm { o p t } } )$ and $\phi ( \cdot ; { \mathbf W } _ { e } )$ denotes MLP layers with the shape of $1 6 \times 1 6$ , $9 \times 1 6$ and $3 2 \times 6 4$ respectively, $\boldsymbol { e } _ { i , j }$ denotes the fused information for each submodule. Then we add $\mathit { S i n }$ Positional Encoding (Vaswani et al. 2017) $\mathbf { W } _ { \mathrm { p o s } }$ to each sub-module, which represents the relative position information among all sub-modules in the algorithm structure.

$$
\mathbf { h } _ { i } ^ { ( 0 ) } = \mathrm { v s t a c k } ( e _ { i , j } ; \cdot \cdot \cdot ; e _ { i , L _ { i } } ) + \mathbf { W } _ { \mathrm { p o s } }
$$

where $\mathbf { h } _ { i } ^ { ( 0 ) } \in \mathbb { R } ^ { L _ { \operatorname* { m a x } } \times 6 4 }$ denotes the module embedding for each sub-module. We note that since the number of submodules $( L _ { i } )$ may vary between different algorithm structures, we zero pad ${ \bf h } _ { i } ^ { ( 0 ) }$ to a pre-defined maximum length $L _ { \mathrm { m a x } }$ to ensure input size invariant among tasks.

Module Aware Attention From the module embeddings ${ \bf h } _ { i } ^ { ( 0 ) }$ described above, we obtains the output features for all sub-modules as:

$$
\begin{array} { r l } & { \hat { \mathbf { h } } _ { i } ^ { ( l ) } = \mathrm { L N } ( \mathbf { M S A } ( \mathbf { h } _ { i } ^ { ( l - 1 ) } ) + \mathbf { h } _ { i } ^ { ( l - 1 ) } ) , l = 1 , 2 , 3 } \\ & { \mathbf { h } _ { i } ^ { ( l ) } = \mathrm { L N } ( \phi ( \hat { \mathbf { h } } _ { i } ; \mathbf { W } _ { F } ^ { ( l ) } ) ) + \hat { \mathbf { h } } _ { i } ^ { ( l ) } ) , l = 1 , 2 , 3 } \end{array}
$$

where LN is Layernorm (Ba, Kiros, and Hinton 2016), MSA is Multi-head Self-Attention (Vaswani et al. 2017) and $\phi ( \cdot ; \mathbf { W } _ { F } ^ { ( l ) } )$ are MLP layers with the shape of $6 4 \times 6 4$ . In this paper we use MSA blocks to process the module embeddings (as illustrated in the middle of Figure 3).

Configuration Decoder In ConfigX, the policy $\pi _ { \boldsymbol { \theta } } ( c _ { i } | \boldsymbol { s } _ { i } )$ models the conditional distribution of $A _ { i }$ ’s configuration $c _ { i }$ given the state $s _ { i }$ . As illustrated in the right of Figure 3, for each sub-module $a _ { j }$ in an algorithm $A _ { i } = \{ a _ { 1 } , a _ { 2 } , \ldots \}$ , we output distribution parameters $\mu$ and $\Sigma$ as:

$$
\begin{array} { c } { { \mu _ { j } = \phi ( h _ { i , j } ^ { ( 3 ) } ; { \bf W } _ { \mu } ) , ~ \Sigma _ { j } = \mathrm { D i a g } \phi ( h _ { i , j } ^ { ( 3 ) } ; { \bf W } _ { \Sigma } ) } } \\ { { c _ { i , j } \sim \mathcal N ( \mu _ { j } ; \Sigma _ { j } ) } } \end{array}
$$

where $\phi ( \cdot ; { \mathbf W } _ { \mu } )$ and $\phi ( \cdot ; { \bf W } _ { \Sigma } )$ are two MLP layers with the same shape of $6 4 \times C _ { \mathrm { m a x } }$ , $c _ { i , j } \in \mathbb { R } ^ { C _ { \operatorname* { m a x } } }$ denotes the configurations for sub-module $a _ { j }$ in algorithm structure $A _ { i }$ . Since the size of the configuration spaces may vary between different sub-modules, we pre-defined a maximum configuration space size $C _ { \mathrm { m a x } }$ to cover the sizes of all sub-modules. If the size of a sub-module is less than $C _ { \mathrm { m a x } }$ , we use the first few configurations in $\boldsymbol { c } _ { i , j }$ and ignore the rest.

For the critic, we calculate the value of a sub-module as $V ( s _ { i , j } ) = \phi ( \mathbf { h } _ { i , j } ^ { ( 3 ) } ; \mathbf { W } _ { c } )$ using a MLP with the shape of ${ \bf W } _ { c } \in \{ \}$ $\mathbb { R } ^ { 6 4 \times 1 6 \times 1 }$ . The value of the algorithm structure is the averaged value per sub-module $\begin{array} { r } { V ( \bar { s } _ { i } ) = \frac { 1 } { L _ { \mathrm { m a x } } } \sum _ { j = 1 } ^ { L _ { \mathrm { m a x } } } V ( s _ { i , j } ) } \end{array}$ .

![](images/7d04ed0af9d3b89a8b1b0caf141562d26dc71ab25aaf498e38fc31ff59d763b8.jpg)  
Figure 4: Optimization curves of the pre-trained ConfigX model and the baselines, over three different zero-shot scenarios

Reward Function The objective value scales across different problem instances can vary. To ensure the accumulated performance improvement across tasks approximately share the same numerical level, we propose a task agnostic reward function. At optimization step $t$ , the reward function on any problem instance $p \in \mathcal { I }$ is formulated as:

$$
\displaystyle r _ { t } = \delta \times \frac { f _ { p , t - 1 } ^ { * } - f _ { p , t } ^ { * } } { f _ { p , 0 } ^ { * } - f _ { p } ^ { * } }
$$

where $f _ { p , t } ^ { * }$ is the found best objective value of problem instance $p$ at step $t$ , $f _ { p } ^ { * }$ is the global optimal objective value of $p$ and $\delta = 1 0$ is a scale factor. In this way, we make the scales of the accumulated improvement in all tasks similar and hence stabilize the training.

# 4 Experiment

In this section, we discuss the following research questions: RQ1: Can pre-trained ConfigX model zero-shots to unseen tasks with unseen algorithm structures and/or unseen problem instances? RQ2: If the zero-shot performance is not as expected, is it possible to fine-tune ConfigX to address novel algorithm structures in future? RQ3: How do the concrete designs in ConfigX contribute to the learning effectiveness? Below, we first introduce the experimental settings and then address $\scriptstyle \mathrm { R Q 1 } \sim \mathrm { R Q 3 }$ respectively.

# 4.1 Experimental Setup

Training setup. We have prepared several task sets from different sub-task-spaces of the overall task space $\tau$ (defined at Section 3.2) to aid for the following experimental validation. Concretely, denote $\mathcal { T } _ { \mathrm { s y n } }$ as the problems in CoCo-BBOB suite, $\mathcal { T } _ { \mathrm { r e a l } }$ as all realistic problems in Proteindocking benchmark and HPO-B benchmark, $\mathcal { A } _ { \mathrm { D E } }$ as the algorithm structure space only including DE variants, $\mathcal { A } _ { \mathrm { P S O } , \mathrm { G A } }$ as the algorithm structure space including PSO and GA variants, we have prepared 256 optimization tasks as training task set $T _ { \mathrm { t r a i n } } \subset \mathcal { A } _ { \mathrm { D E } } \times \mathcal { Z } _ { \mathrm { s y n } }$ , another 512 optimization tasks as in-distribution testing task set $T _ { \mathrm { t e s t , i n } } \subset \mathcal { A } _ { \mathrm { D E } } \times \mathcal { T } _ { \mathrm { s y n } }$ . For out-of-distribution tasks, we have prepared two task sets: $T _ { \mathrm { t e s t , o u t } } ^ { ( 1 ) } \subset \mathcal { A } _ { \mathrm { D E } } \times \mathcal { T } _ { \mathrm { r e a l } }$ and $T _ { \mathrm { t e s t , o u t } } ^ { ( 2 ) } \subset \mathcal { A } _ { \mathrm { P S O , G A } } \times \mathcal { T } _ { \mathrm { s y n } }$ , each with 512 task instances. During the training, for a batch of $b a t c h _ { - } s i z e = 3 2$ tasks, PPO (Schulman et al. 2017) method is used to update the policy net and critic $\kappa = 3$ times for every 10 rollout optimization steps. All of the tasks are allowed to be optimized for $H = 5 0 0$ optimization steps. The training lasts for 50 epochs with a fixed learning rate 0.001. All experiments are run on an Intel(R) Xeon(R) 6348 CPU with 504G RAM. Refer to Appendix E.1 for more details.

Baselines and Performance Metric. In the following comparisons, we consider three baselines: SMAC3, which is the state-of-the-art AC software based on Bayesian Optimization and aggressive racing mechanism; Original, which denotes using the suggested configurations in sub-modules’ original paper (see Appendix A for one-to-one correspondence); Random, which randomly sample the configurations for the algorithm from the algorithm’s configuration space. For the pre-trained model in ConfigX and the above baselines, we calculate the performance of them on tested task set by applying them to configure each tested task for 51 independent runs and then aggregate a normalized accumulated optimization improvement across all tasks and all runs, we provide more detailed calculation steps in Appendix E.2.

# 4.2 Zero-shot Performance (RQ1)

We validate the zero-shot performance of ConfigX by first pre-training a model on $T _ { \mathrm { t r a i n } }$ . Then the pre-trained model is directly used to facilitate AC process for tasks in tested set, without any fine-tuning. Concretely, we aims at validating the zero-shot generalization performance in three different scenarios: 1) $T _ { \mathrm { t e s t , i n } }$ , where the optimization tasks come from the same task space on which ConfigX is pre-trained. 2) Tt(es1t),o , where the optimization tasks locate beyond the optimization problem scope of the training task space. 3) $\bar { T _ { \mathrm { t e s t , o u t } } ^ { ( 2 ) } }$ , where the optimization tasks locate beyond the algothe optimization curves of our pre-trained model and the baselines in Figure 4, where the $\mathbf { \boldsymbol { x } }$ -axis denotes the optimization horizon and y-axis denotes the performance metric we defined previously. The results in Figure 4 reveal several key observations: 1) In all zero-shot scenarios, ConfigX presents significantly superior performance to the Random baseline, which randomly configures the algorithms in the tested tasks. This underscores the effectiveness of the multitask reinforcement learning in ConfigX. 2) The results on $T _ { \mathrm { t e s t , i n } }$ demonstrate that pre-training ConfigX on some task samples of the given task space is enough to ensure the generalization to the other tasks within this space, surpassing the state-of-the-art AC baseline SMAC3. 3) The results on $T _ { \mathrm { t e s t , o u t } } ^ { ( 1 ) }$ show that ConfigX is capable of adapting itself to totally unseen optimization problem scope. This observation attributes to our state representation design, where the optimization status borrowed from recent MetaBBO works are claimed to be generic across different problem scopes. 4) Though promising, we find that the zero-shot performance aosf tChoensfiugbX-mono $T _ { \mathrm { t e s t , o u t } } ^ { ( 2 ) }$ nisd nstortuacstuerxepseicntedG.AIt/PisSnOotarseu spirgisninfigcantly different from those in DE, which hinders ConfigX from applying its DE configuration experience on PSO/GA tasks. We explore whether this generalization gap could be addressed through further fine-tuning in the next section.

![](images/7bd3e7cbf6788639fb4e9fbcb45a8670e668cbad804f590059c109c63d7f29a5.jpg)  
Figure 5: The learning curves of fine-tuning and re-training ConfigX on novel optimization problems or algorithm structures. The fine-tuning saves 3x and $2 \mathrm { x }$ learning steps than the re-training on $T _ { \mathrm { t e s t , o u t } } ^ { ( 1 ) }$ and $T _ { \mathrm { t e s t , o u t } } ^ { ( 2 ) }$ respectively.

# 4.3 Lifelong Learning in ConfigX (RQ2)

The booming algorithm designs in EAs, together with the increasingly diverse optimization problems pose nonnegligible challenges to universal algorithm configuration methods such as our ConfigX. On the one hand, although our pre-trained model shows uncommon AC performance when encountered with novel optimization problems (middle of Figure 4), further performance boost is still needed especially in industrial scenarios. On the other hand, as shown in the left of Figure 4, the pre-trained model can not cover those algorithm sub-modules which have not been included within its training algorithm structure space. Both situations above underline the importance of lifelong learning in ConfigX. We hence investigate the fine-tuning efficiency of the pretrained model in this section. Concretely, we compare the learning curves of 1) fine-tuning the pre-trained model, and 2) re-training a new model from scratch in Figure 5, where the x-axis denotes the learning epochs and the y-axis denotes the aforementioned performance metric over the tested task set. The results reveal that ConfigX supports efficient finetuning for adapting out-of-distribution tasks, which in turn provides operable guidance for lifelong learning in ConfigX: (a) One can configure an algorithm included in the algorithm space of Modular-BBO, yet on different problem scope, by directly using the pre-trained model. (b) One can also integrate novel algorithm designs into Modular-BBO and facilitate efficient fine-tuning to enhance the performance of the pre-trained model on these algorithm structures.

<html><body><table><tr><td></td><td>Ttest,in</td><td>To</td><td>Trstou</td></tr><tr><td>ConfigX</td><td>9.81E-01 ±7.33E-03</td><td>9.86E-01 ±2.64E-03</td><td>9.22E-01 ±6.94E-03</td></tr><tr><td>ConfigX-MLP</td><td>9.70E-01 ±8.13E-03</td><td>9.80E-01 ±2.54E-03</td><td>9.16E-01 ±6.57E-03</td></tr><tr><td>ConfigX-LPE</td><td>9.82E-01 ±7.62E-03</td><td>9.84E-01 ±2.58E-03</td><td>9.20E-01 ±6.89E-03</td></tr><tr><td>ConfigX-NPE</td><td>9.74E-01 ±7.75E-03</td><td>9.81E-01 ±2.67E-03</td><td>9.19E-01 ±6.73E-03</td></tr><tr><td>MLP-NPE</td><td>9.51E-01 ±9.27E-03</td><td>9.73E-01 ±2.71E-03</td><td>9.06E-01 ±7.29E-03</td></tr></table></body></html>

Table 1: Performance of different ablated baselines.

# 4.4 Ablation Study (RQ3)

In Section 3.3, we proposed a Transformer based architecture to encode and process the state information of all sub-modules within an algorithm structure. In particular, we added Sin positional embeddings (PE) to each sub-module token as additional topology structure information for learning. We further apply Multi-head Self-Attention (MSA) to enhance the module-aware information sharing. In this section we investigate on what extent these designs influence ConfigX’s learning effectiveness. Concretely, for the positional embeddings, we introduce two ablations 1) ConfigXNPE: remove the Sin PE from ConfigX. 2) ConfigX-LPE, replace the Sin PE by Learnable PE (Gehring et al. 2017). For the MSA, we introduce one ablation ConfigX-MLP: cancel the information sharing between the sub-modules by replacing the MSA blocks by an MLP layer. We present the final performance of these baselines and ConfigX on the tested task sets in Table 1. The results underscores the importance of these special designs: (a) Without the MSA block, ConfigX struggles in learning the configuration policy in an informative way. (b) Without the positional embdeddings, the configuration policy in ConfigX becomes agnostic to the structure information of the controlled algorithm. (c) Learnable PE shows similar performance with Sin PE, while introducing additional parameters for ConfigX to learn.

# 5 Conclusion

In this paper, we propose ConfigX as a pioneer research exploring the possibility of learning a universal MetaBBO agent for automatically configuring diverse EAs across optimization problems. To this end, we first introduce a novel EA modularization system Modular-BBO that is capable of maintaining various sub-modules in EAs and spanning a massive algorithm structure space. We then formulate the universal AC over this algorithm space as an MTRL problem and hence propose meta-learning a Transformer based configuration policy to maximize the overall optimization performance across task samples. Extensive experiments demonstrate that a pre-trained ConfigX model could achieve superior AC performance to the state-of-the-art manual AC method SMAC3. Furthermore, we verify that ConfigX holds promising lifelong learning ability when being fine-tuned to adapt out-of-scope algorithm structures and optimization problems. We hope this work could serve as a pivotal step towards automatic and all-purpose AC base model.

# Acknowledgments

This work was supported in part by the National Natural Science Foundation of China (Grant No. 62276100), in part by the Guangdong Natural Science Funds for Distinguished Young Scholars (Grant No. 2022B1515020049), in part by the Guangdong Provincial Natural Science Foundation for Outstanding Youth Team Project (Grant No. 2024B1515040010), in part by the National Research Foundation, Singapore, under its AI Singapore Programme (AISG Award No. AISG3-RP-2022-031), and in part by the TCL Young Scholars Program.