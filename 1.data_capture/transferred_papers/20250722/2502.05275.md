# Interpretable Failure Detection with Human-Level Concepts

Kien X. Nguyen, Tang Li, Xi Peng

Department of Computer and Information Sciences University of Delaware Newark, DE, USA kxnguyen, tangli, xipeng @udel.edu

# Abstract

Reliable failure detection holds paramount importance in safety-critical applications. Yet, neural networks are known to produce overconfident predictions for misclassified samples. As a result, it remains a problematic matter as existing confidence score functions rely on category-level signals, the logits, to detect failures. This research introduces an innovative strategy, leveraging human-level concepts for a dual purpose: to reliably detect when a model fails and to transparently interpret why. By integrating a nuanced array of signals for each category, our method enables a finer-grained assessment of the model’s confidence. We present a simple yet highly effective approach based on the ordinal ranking of concept activation to the input image. Without bells and whistles, our method significantly reduce the false positive rate across diverse real-world image classification benchmarks, specifically by $3 . 7 \%$ on ImageNet and $9 \%$ on EuroSAT.

# Introduction

Vision-language models have demonstrated impressive capability across diverse visual recognition domains (Radford et al. 2021; Jia et al. 2021; Singh et al. 2021; Li et al. 2022, 2023). However, when it comes to safe deployment in highstake applications, it is of paramount importance for a model to be self-aware of its own shortcomings. For instance, in monitoring for natural disasters such as floods or wildfires, the AI system must signal for human intervention upon encountering scenarios where its confidence is low. Such selfawareness ensures that preemptive measures can be taken to mitigate disaster impacts on communities and ecosystems. Therefore, it is imperative not only to detect failures accurately but also to understand the reasons behind them.

Traditional methods (Hendrycks and Gimpel 2016; Granese et al. 2021; Zhu et al. 2023a,b; Liang, Li, and Srikant 2018) rely on category-level information to detect misclassifications, performing confidence estimation on the class logits. However, neural networks are known to produce overconfident predictions for misclassified samples due to factors like spurious correlations (Arjovsky et al. 2019; Sagawa et al. 2019), thus existing confidence scoring functions (CSFs) fall short in such cases. Besides, the model confidence depicted through category-level information impedes the ability for humans to interpret why it fails. To this end, we ask the following question: “What other sources of information can we leverage to enhance failure detection?”

We present a novel perspective on detecting failures by leveraging human-level concepts, or visual attributes. With the flexibility to incorporate free-form language to VLMs (i.e. CLIP), we can represent a category with a set of predefined concepts (Menon and Vondrick 2023; Oikarinen et al. 2023; Li, Ma, and Peng 2024a,b). Instead of only prompting the model “Do you recognize a camel?”, we collectively ask “Do you recognize humps on back?”, or “Do you recognize shaggy coat?”. The purpose is to measure the model’s confidence in the object’s detailed visual attributes in addition to the holistic category. We thus achieve a more accurate confidence estimate to detect failures more effectively (Fig. 1).

Ideally, a VLM that can recognize a image of a camel should also recognize all the associated visual attributes, such as humps on back, shaggy coat, etc. Such visual attributes should yield higher confidence scores compared to those associated with the absent categories. Conversely, if the model shows high confidence in concepts from multiple unrelated categories at the same time, it could indicate a failure in its recognition process. Based on such intuition, we present a simple but effective approach using the Ordinal Ranking of Concept Activation (ORCA) to detect failures. Additionally, these human-understandable concepts allow users to understand the reasons behind such failures, thereby aiding them in refining the training process.

We rigorously validate our method’s efficacy in detecting incorrect samples across both natural and remote sensing image benchmarks, which mirror the complexity in realworld scenarios. ORCA demonstrates a significant capability to mitigate the issue of overly confident misclassifications. In summary, our contributions are threefold:

1. We leverage human-level concepts to detect when and interpret why a model fails using vision-language models.   
2. We present a simple but effective approach, called ORCA, to estimate more reliable confidence via the ordinal ranking of the concepts’ activation.   
3. We empirically demonstrate that the concept-based methods enhance failure prediction performance across a wide range of classification benchmarks.

Standard (category-level) Ours (concept-level) camel 0.15 humps on back 30.0 elephant 0.05 shaggy coat 28.1 giraffe 0.80 long straight neck 27.3 cushioned feet 26.4 camel 0.47 Estimate patched coat 25.8 elephant 0.10 Label: camel giraffe 0.43 Interpret long trunk 24.4 Class probability CLIP similarity scores

# Related Work

Failure Detection. Failure detection, or misclassification detection, is a burgeoning area of research within the realm of artificial intelligence. Detecting when machine learning models produce incorrect or misleading predictions has significant implications for safety, reliability, and transparency in various domains. Existing research in this field falls into two main categories: (1) retraining or fine-tuning of neural networks (Moon et al. 2020; Zhu et al. 2023a,b), and (2) the design of novel confidence score functions (Granese et al. 2021; Hendrycks and Gimpel 2016). The former approach involves retraining or fine-tuning neural networks with specific objectives aimed at improving the model’s capability to recognize its own failures. Zhu et al. (Zhu et al. 2023b) employs a training objective that seeks flat minima to mitigate overconfident predictions. While these approaches have shown promise, they often require extensive computational resources and access to the entire model, which may not be feasible for large VLMs. Researchers have also turned their attention to the design of new CSFs (Granese et al. 2021). Despite these efforts, the most robust CSF remains the MSP (Jaeger et al. 2023). However, a downside of category-level CSFs is their inability to detect overconfident but incorrect predictions, which is problematic. In this work, we deconstruct category-level into concept-level signals to achieve a more nuanced estimate of the model’s confidence.

A closely related sub-field is confidence calibration (Minderer et al. 2021; LeVine et al. 2023; Mukhoti et al. 2020; Pereyra et al. 2017), where the goal is to adjust a model’s predicted probabilities to ensure that they accurately reflect the true likelihood of those predictions being correct. However, Zhu et al. (Zhu et al. 2023b) has empirically shown that calibration methods frequently yield no benefits or even detrimentally affect failure prediction. Similarly, some works (Jaeger et al. 2023; Bernhardt, Ribeiro, and Glocker 2022) also emphasizes the importance of confidence ranking over confidence calibration in failure detection. Some other related sub-fields are predictive uncertainty estimation (Gal and Ghahramani 2016; Blundell et al. 2015; Lakshminarayanan, Pritzel, and Blundell 2017; Mukhoti et al. 2023), out-of-distribution detection (Zhu et al. $2 0 2 3 \mathrm { a }$ ; Liang, Li, and Srikant 2018; Dinari and Freifeld 2022; Lee et al.

2018), open-set recognition (Vaze et al. 2022; Geng, Huang, and Chen 2021) and selective classification (Geifman and El-Yaniv 2017; Fisch, Jaakkola, and Barzilay 2022).

Human-level Concepts in Vision-Language Models. Concept-based models (CBMs) aim to open the black box of neural networks. Concept bottleneck networks are pioneers for interpretable neural networks, with each neuron in the concept bottleneck layer representing a concept at the human level (Koh et al. 2020; Yuksekgonul, Wang, and Zou 2022). With the flexibility to employ free language in vision language models, such as CLIP (Radford et al. 2021), ALIGN (Jia et al. 2021), FLAVA (Singh et al. 2021), and BLIP (Li et al. 2022, 2023), concepts of human level can be naturally integrated into the prediction mechanism (Menon and Vondrick 2023; Yang et al. 2022; Oikarinen et al. 2023). This work can be viewed as a variant of CBMs for failure detection, which has never been considered before. We show the approach better predicts failures and, as a byproduct, helps interpret why a model fails.

# Backgrounds

Overview on Failure Detection. We consider failure detection on the multi-class classification task. Let $\boldsymbol { \mathcal { X } } \in \mathbb { R } ^ { d }$ be the input space and $\mathcal { Y } = \{ 1 , 2 , \dots , C \}$ be the label space, where $d$ is the dimension of the input vector. Given a data set $\{ ( \mathbf { x } _ { i } , y _ { i } ) \} _ { i = 1 } ^ { N }$ with $N$ data points independently sampled from the joint probability distribution $\mathcal { X } \times \mathcal { Y }$ , a standard neural network $f : \mathcal { X }  \mathcal { Y }$ outputs a probability distribution over the $C$ categories. For an input x, $f$ outputs $\hat { \pmb { p } } = \hat { P } ( y | \mathbf { x } ; \theta )$ as the class probabilities, where $\theta$ denotes the network’s parameters. In the context of failure detection, we consider a pair of functions $( f , g )$ , where $g : \mathcal { F } \times \mathcal { X }  \mathbb { R }$ is the confidence scoring function, and $f \in { \mathcal { F } }$ . With a predefined threshold $\tau \in \mathsf { R } ^ { + }$ , the failure detection output is defined as:

$$
( f , g ) ( \mathbf { x } ) = \left\{ \begin{array} { l l } { \hat { P } ( y | \mathbf { x } ; \theta ) , } & { \mathrm { i f ~ } g ( f , \mathbf { x } ) \geq \tau } \\ { \mathrm { d e t e c t , } } & { \mathrm { o t h e r w i s e . } } \end{array} \right.
$$

Failure detection is initiated when $g ( f , \mathbf { x } )$ falls below a threshold $\tau$ . Ideally, a confidence scoring function should output higher confidence scores for correct predictions and lower confidence scores for incorrect predictions. Despite efforts in designing CSFs, Jaeger et al. (Jaeger et al. 2023) has shown that the standard Maximum Softmax Prediction remains the best CSF across a wide range of datasets and network architectures. Mathematically, MSP is defined as:

$$
g ( f , \mathbf { x } ) = \operatorname* { m a x } _ { c \in \mathcal { V } } \hat { P } ( y = c | \mathbf { x } ; \theta )
$$

which returns the maximum output signal after the softmax activation function on the network output layer.

Failure Detection with VLM. CLIP (Radford et al. 2021), a vision-language model, is pre-trained on a large-scale dataset comprising of 400 million image-text pairs. CLIP uses contrastive learning to align the image and text pairs. During inference, we calculate the model’s logits as the cosine similarity score between the input image embedding and the corresponding text embeddings. Given an input imadigtei $\mathbf { x }$ , eteextmlbaebdedlsinrgepirsesdentosttehdeacsa $\bar { \boldsymbol { f } } _ { \mathrm { i m g } } ( \mathbf { x } ) \in \mathbb { R } ^ { m }$ $C$ $\{ \mathbf { t } _ { c } \} _ { c = 1 } ^ { C }$ where $f _ { \mathrm { t x t } } ( \mathbf { t } _ { c } ) \in \mathbb { R } ^ { m }$ is the embeddings and $m \ll d$ . For each category, we calculate the corresponding logit as:

$$
s _ { c } = 1 0 0 \times \frac { f _ { \mathrm { i m g } } ( \mathbf { x } ) \cdot f _ { \mathrm { t x t } } ( \mathbf { t } _ { c } ) } { \left\| f _ { \mathrm { i m g } } ( \mathbf { x } ) \right\| \left\| f _ { \mathrm { t x t } } ( \mathbf { t } _ { c } ) \right\| }
$$

where $\left\| \cdot \right\|$ is the $L _ { 2 }$ norm. The softmax function then converts the logits into probabilities:

$$
\hat { p } _ { c } = \frac { \exp ( s _ { c } ) } { \sum _ { j = 1 } ^ { C } \exp ( s _ { j } ) }
$$

where $\hat { p } _ { c } \in \hat { p }$ . $f ( \mathbf { x } ) = \operatorname { a r g m a x } _ { c \in \mathcal { V } } \hat { p } _ { c }$ is the prediction, and $g ( f , \mathbf { x } ) = \operatorname* { m a x } _ { c \in \mathcal { V } } \hat { p } _ { c }$ can be regarded as the model confidence for a given input $\mathbf { x }$ using MSP.

# Methods

Traditional methods rely on the category-level signals to estimate the model’s confidence. This leads to unreliable confidence estimate as neural networks are prone to overconfident misclassification. To address this issue, we suggest exposing the model to diverse viewpoints via human-level concepts. Rather than inquiring about the model’s certainty regarding an image being a camel, we also query its confidence regarding specific attributes like the presence of humps on the camel’s back, a shaggy coat, etc.

Recent advancements in VLMs enable such integration of human-level concepts as free-form language into the pipeline (Menon and Vondrick 2023; Yang et al. 2022; Oikarinen et al. 2023). In this section, we describe the integration of the work by Menon and Vondrick (Menon and Vondrick 2023) which employs concept aggregation to establish a baseline concept-based method for failure detection. Subsequently, we introduce ORCA, our novel approach that captures the interaction among concept activations through ordinal ranking, enhancing the reliability of failure detection.

# Human-Level Concepts for Failure Detection

Given $K$ concepts per category, we define $\mathcal { A }$ as a collection of all concepts, where $| { \mathcal A } | \ = \ C \times K$ . We obtain the vector of similarity scores (or logits), $S _ { \mathrm { c o n c } } =$ $\left[ s _ { 1 , 1 } , \dotsc , s _ { 1 , K } , s _ { 2 , 1 } , \dotsc , s _ { C , K } \right]$ , between the image embedding and all the concepts using Eq. 3. DescCLIP then calculates the mean similarity score among all concepts for each category $c$ to retrieve the logits and output the prediction:

$$
f ( \mathbf { x } ) = \operatorname { a r g m a x } _ { c \in \mathcal { V } } \frac { 1 } { K } \sum _ { k = 1 } ^ { K } s _ { c , k }
$$

Finally, we apply the softmax function (Eq. 4) on the logits to get the class probabilities and employ MSP to obtain the model’s confidence score.

# Ordinal Ranking of Concept Activation

DescCLIP’s concept aggregation leads to a coarse-grained confidence estimation procedure. We propose a fine-grained approach that models the interaction among concepts via ordinal ranking to estimate confidence more reliably.

Ideally, if a model is confident about predicting a category $\hat { c }$ then the concepts associated with $\hat { c }$ should yield the strongest activations. In other words, the similarity scores of all concepts belonging to ${ \hat { c } } _ { : }$ , $\{ s _ { \hat { c } , k } \} _ { k = 1 } ^ { K }$ , should belong to the top- $K$ ranking. Conversely, we would see a mixture of concepts from different categories in the top- $K$ ranking if the model is likely to make an incorrect prediction. With such information, we can separate correct and incorrect predictions more reliably. Next, we describe two variants of our proposed method: baseline and rank-aware ORCA. In brevity, the former builds upon simple counting mechanisms, while the latter weighs the concept contributions to the confidence estimate based on their ranks.

Baseline ORCA. We first sort $S _ { \mathrm { c o n c } }$ in descending order and retrieve the set of the top- $K$ concepts, denoted as an ordered set $\mathcal { A } _ { \mathrm { t o p } - K }$ . After that, we derive the confidence based on the number of different categories whose concepts belong in $\mathcal { A } _ { \mathrm { t o p } - K }$ . The rationale is straightforward: the model is at a higher risk of failure as there are more categories featuring in $\mathcal { A } _ { \mathrm { t o p } - K }$ . The prediction is determined as follows:

$$
f ( \mathbf { x } ) = \operatorname { a r g m a x } _ { c \in \mathcal { V } } | \mathcal { A } _ { \mathrm { t o p } - K } \cap \mathcal { A } _ { c } | ,
$$

where $A _ { c }$ denotes the set of concepts of an arbitrary category $c$ ’s concepts, and $\left| \cdot \right|$ denotes the set cardinality. The confidence of the prediction is the ratio between the number of the predicted category’s concepts appearing in $\mathcal { A } _ { \mathrm { t o p } - K }$ over $K$ :

$$
g ( f , \mathbf { x } ) = \frac { | \mathcal { A } _ { \mathrm { t o p } - K } \cap \mathcal { A } _ { \hat { c } } | } { K }
$$

where $\begin{array} { r } { \hat { c } { \bf \Xi } = f ( { \bf x } ) } \end{array}$ is the prediction. We dub this variant ORCA-B in the text.

Rank-aware ORCA. While ORCA-B provides a fundamental approach, its reliance solely on rudimentary counting mechanisms limits its ability to capture nuanced distinctions. To enhance our approach, we introduce a rank-aware variant that uses ordinal ranking information to deliver more accurate failure detection. In detail, we construct a rankaware weight vector w where the value of each element is proportional to the ordinal ranking. First, we define the ordinal ranking vector $\mathbf { r } = [ K , K - 1 , \ldots , 1 ]$ with $K$ elements in descending order. Then, we apply a logarithmic weighting

conc1,1 humps on back conc2,1 long trunk concC,1 long straight neck conc1,2 shaggy coat conc2,2 tusks concC,2 patched coat 1-camel 2-elephant ： C-giraffe ： conc1,K long curved neck conc2,K large floppy ears concC,K ossicones on head Concept Activation Top-K Concepts Predict 30.0 30.0 CLIP 28.1 26.3 27.3 descend 28.1 27.3 26.3 ORCA Detect failures 24.4 sort 23.5 23.4 Interpret failures Test Sample conc1,1 conc1,2 conc1,K conc2,1 conc2,2 concC,1 concC,K conc1,1 conc1,2 concC,1 conc1,K

function to assign each rank in $\mathbf { r }$ a weight $w _ { i } \in { \mathbf { w } }$ , resulting in a decreasing vector whose elements sum up to 1. Logarithmic ensures a smooth distribution of weights among the ranks of each concept, enabling a more nuanced estimation of the confidence level. Specifically, the logarithmic scaling equation is defined as $\begin{array} { r } { w _ { i } = \frac { \log ( \bar { 1 } + r _ { i } ) } { \sum _ { j = 1 } ^ { K } \log ( 1 + r _ { j } ) } } \end{array}$ , with the normalization of each weight $w _ { i }$ in w. Finally, for each category $c$ with its concepts featuring in $\mathcal { A } _ { \mathrm { t o p } - K }$ , we calculate the prediction and the confidence of the model as follows:

$$
\begin{array} { l } { f ( \mathbf { x } ) = \displaystyle \mathrm { a r g m a x } _ { c \in \mathcal { V } } \displaystyle \sum _ { k = 1 } ^ { K } \mathbb { I } ( a _ { k } \in \mathcal { A } _ { c } ) \cdot w _ { k } } \\ { g ( f , \mathbf { x } ) = \operatorname* { m a x } _ { c \in \mathcal { V } } \displaystyle \sum _ { k = 1 } ^ { K } \mathbb { I } ( a _ { k } \in \mathcal { A } _ { c } ) \cdot w _ { k } } \end{array}
$$

where $a _ { k }$ is the $k ^ { \mathrm { { t h } } }$ concept in the ordered set $\mathcal { A } _ { \mathrm { t o p } - K }$ , and $\mathbb { I } ( \cdot )$ denotes the indicator function that returns 1 if the condition is true. We refer to this variant as ORCA-R.

# Experiment

Datasets. We evaluate ORCA on a wide variety of datasets: 1. Natural Image Benchmark (1) CIFAR-10/100 (Krizhevsky 2009) is a popular image recognition benchmark spanning across 10/100 categories. (2) ImageNet$l K$ (Deng et al. 2009) a well-known benchmark in computer vision, containing 1000 fine-grained categories, with 1,281,167 training and 50,000 validation samples. This benchmark contains fine-grained categories that are visually similar, making the failure detection task more challenging. 2. Satellite Image Benchmark (3) EuroSAT (Helber et al. 2017) is a satellite RGB image dataset, containing 10 categories of land usage, such as forest, river, residential buildings, industrial buildings, etc. The dataset comprises of 27,000 geo-referenced samples. (4) RESISC45 (Cheng, Han, and Lu 2017) is a public benchmark for Remote Sensing Image Scene Classification. It contains 31,500 images, covering 45 scene categories with 700 images in each categories.

Baselines. We compare ORCA to 3 models in combination with 3 CSFs, yielding a total of 9 baselines. Note that we only compare with post-hoc CSFs because our methods do not require any training.

1. Models (1) Zero-shot (Radford et al. 2021): The prediction of zero-shot CLIP relies on the text category name as introduced in the original paper. We compute the logits using Eq. 3 and apply CSFs to calculate the model’s confidence. (2) Ensemble (Radford et al. 2021): This model ensembles multiple templates into zero-shot classification, effectively acting as an ensemble method. We average the similarity scores from multiple templates for each category before extracting the softmax logits. (3) DescCLIP (Menon and Vondrick 2023): As describe in Sec. , DescCLIP averages the similarity scores of all the concepts for each category; we then applies CSFs to estimate the confidence score.

2. CSFs (1) MSP (Hendrycks and Gimpel 2016): The confidence score is measured by taking the maximum value of the softmax responses. (2) ODIN (Liang, Li, and Srikant 2018): This CSF is a temperature-scaled version of MSP. We use the default temperature $T = 1 0 0 0$ and do not use perturbation for fair comparison. (3) DOCTOR (Granese et al. 2021): Different from MSP, DOCTOR fully exploits all available information contained in the soft-probabilities of the predictions to estimate the confidence.

Implementation Details. We utilize CLIP’s ResNet-101 and ViT-B/32 backbones to perform zero-shot prediction on the benchmarks and calculate the performance metrics. For dataset with few categories, such as CIFAR-10 and EuroSAT, we use different prompts to retrieve diverse collections of concepts from the large language model GPT-3.5 (Brown et al. 2020; Peng et al. 2023) and manually select the top 10 visual concepts that are the most distinctive among categories. An example of our prompt is as follows, with more details in the Supplementary:

<html><body><table><tr><td rowspan="2">Dataset</td><td rowspan="2">Method</td><td colspan="3">ResNet-101</td><td colspan="3">ViT-B/32</td></tr><tr><td>AUROC个</td><td>FPR95↓</td><td>ACC ↑</td><td>AUROC↑</td><td>FPR95↓</td><td>ACC↑</td></tr><tr><td rowspan="10">CIFAR10 (K=10)</td><td>Zero-shot+MSP</td><td>85.98</td><td>62.98</td><td>78.01</td><td>88.92</td><td>58.66</td><td>88.92</td></tr><tr><td>+ODIN</td><td>83.65</td><td>65.50</td><td>78.01</td><td>84.49</td><td>65.36</td><td>88.92</td></tr><tr><td>+DOCTOR</td><td>86.56</td><td>63.76</td><td>78.01</td><td>88.58</td><td>62.32</td><td>88.92</td></tr><tr><td>Ensemble+MSP</td><td>86.35</td><td>63.53 67.95</td><td>80.97</td><td>89.25</td><td>57.03</td><td>89.70</td></tr><tr><td>+ODIN</td><td></td><td>83.39</td><td>80.97</td><td>83.66</td><td>63.34</td><td>89.70</td></tr><tr><td>+DOCTOR</td><td>85.67</td><td>66.53</td><td>80.97</td><td>88.68</td><td>58.87</td><td>89.70</td></tr><tr><td>DescCLIP+MSP</td><td>85.84</td><td>64.68</td><td>80.70</td><td>89.28</td><td>58.77</td><td>88.80</td></tr><tr><td>+ ODIN</td><td>80.92</td><td>68.34</td><td>80.70</td><td>82.61</td><td>66.83</td><td>88.80</td></tr><tr><td>+DOCTOR</td><td>84.99</td><td>67.92</td><td>80.70</td><td>88.80</td><td>61.64</td><td>88.80</td></tr><tr><td>ORCA-B</td><td>84.90</td><td>66.09 62.68</td><td>80.98 80.60</td><td>87.34 89.00</td><td>50.52 52.70</td><td>89.34 90.00</td></tr><tr><td rowspan="10">CIFAR100 (K= 20)</td><td>Zero-shot+MSP</td><td></td><td>85.93 80.72</td><td>73.40</td><td>48.50</td><td>81.15</td><td>71.09</td><td>58.42</td></tr><tr><td>+ODIN</td><td></td><td>77.21</td><td>75.13</td><td>48.50</td><td>76.93</td><td>71.08</td><td>58.42</td></tr><tr><td></td><td>+DOCTOR</td><td>79.68</td><td>75.36</td><td>48.50</td><td>81.57</td><td>69.40</td><td>58.42</td></tr><tr><td>Ensemble+MSP</td><td></td><td>79.22</td><td>73.43</td><td>48.66</td><td>81.44</td><td>70.88</td><td>63.91</td></tr><tr><td></td><td>+ODIN</td><td>75.59</td><td>76.00</td><td>48.66</td><td>75.73</td><td>73.87</td><td>63.91</td></tr><tr><td></td><td>+DOCTOR</td><td>77.96</td><td>76.47</td><td>48.66</td><td>80.02</td><td>74.06</td><td>63.91</td></tr><tr><td>DescCLIP+MSP</td><td></td><td>80.22</td><td>73.39</td><td>52.90</td><td>82.54</td><td>67.38</td><td>66.70</td></tr><tr><td></td><td>+ODIN</td><td>75.86</td><td>75.35</td><td>52.90</td><td>75.72</td><td>73.11</td><td>66.70</td></tr><tr><td></td><td>+DOCTOR</td><td>79.09</td><td>74.96</td><td>52.90</td><td>81.30</td><td>70.83</td><td>66.70</td></tr><tr><td>ORCA-B</td><td></td><td>80.35</td><td>70.46</td><td>52.16</td><td>83.35</td><td>67.35</td><td>66.00</td></tr><tr><td rowspan="10"></td><td>ORCA-R</td><td></td><td>80.46</td><td>72.38</td><td>53.11</td><td>83.40</td><td>67.00</td><td>66.50</td></tr><tr><td>Zero-shot +MSP</td><td></td><td>78.93</td><td>74.05</td><td>56.67</td><td>79.44</td><td>72.91</td><td>58.37</td></tr><tr><td>+ODIN</td><td></td><td>70.59</td><td>80.75</td><td>56.67</td><td>70.48</td><td>80.07</td><td>58.37</td></tr><tr><td></td><td>+DOCTOR</td><td>78.38</td><td>75.90</td><td>56.67</td><td>79.01</td><td>74.17</td><td>58.37</td></tr><tr><td>Ensemble+MSP</td><td></td><td>78.58</td><td>74.37</td><td>56.73</td><td>79.66</td><td>72.89</td><td>59.22</td></tr><tr><td></td><td>+ODIN</td><td>70.29</td><td>80.98</td><td>56.73</td><td>70.61</td><td>80.55</td><td>59.22</td></tr><tr><td></td><td>+DOCTOR</td><td>77.98</td><td>76.25</td><td>56.73</td><td>78.34</td><td>76.24</td><td>59.22</td></tr><tr><td>DescCLIP+MSP</td><td></td><td>80.09</td><td>72.99</td><td>61.94</td><td>80.77</td><td>71.34</td><td>63.20</td></tr><tr><td></td><td>+ODIN</td><td>69.92</td><td>81.53</td><td>61.94</td><td>70.80</td><td>80.14</td><td>63.20</td></tr><tr><td></td><td>+DOCTOR</td><td>79.68</td><td>73.95</td><td>61.94</td><td>80.50</td><td>71.96</td><td>63.20</td></tr><tr><td colspan="2">ORCA-B</td><td>80.24</td><td>71.13</td><td></td><td>62.11</td><td>80.77</td><td>69.19</td><td>63.02</td></tr><tr><td></td><td>ORCA-R</td><td></td><td>80.57</td><td>72.41</td><td>62.29</td><td>80.91</td><td>71.70</td><td>63.20</td></tr></table></body></html>

Table 1: Performance on CIFAR-10/100 and ImageNet. AUROC, FPR $@$ 95TPR (FPR95), and ACC are percentages. With ACC taken into account, bold indicate the best results, underlined denote ours with the second best results.

A: Some distinctive visual concepts of [CATEGORY] are:

For datasets with a larger number of categories, we use the concept collection provided by Yang et al. (Yang et al. 2022). This collection contains up to 500 concept candidates per category; we then select the top concepts that yield the highest average similarity score with the images within each category to form $\mathcal { A }$ . We include the number of concepts used for each dataset in Table 1 and 2.

# Evaluation Metrics

Failure detection accuracy (AUROC). This evaluation protocol, a threshold-independent performance evaluation, measures the area under the receiver operating characteristic curve as CSFs inherently perform binary classification between correct and incorrect predictions. A higher value denotes better ability to predict failures.

False positive rate $\mathbf { ( F P R @ 9 5 T P R }$ ). This metric denotes the false positive rate or the probability that a misclassified sample is predicted as a correct one when the true positive rate is at $9 5 \%$ . It is a fraction that the model falsely assigns higher confidence values to incorrect samples, reflecting the tendency to be overly confident in incorrect predictions.

Classification accuracy (ACC). A classifier with low accuracy might produce easy-to-detect failures (Jaeger et al. 2023) and benefit from a high AUROC. Ideally, we wish a model to yield a high AUROC and ACC, and a low FPR simultaneously.

# Results on Natural Image Benchmarks

We report the performance of all methods on the three evaluation metrics on the natural image benchmarks on ResNet101 and ViT-B/32 and provide the following observations:

Observation 1: Concept-based methods demonstrate better failure detection.

Table 1 shows DescCLIP and ORCA consistently achieves higher AUROC compared to Zero-shot and Ensemble, especially on datasets with a large number of categories, such as CIFAR-100 and ImageNet. The augmentation to multiple signals per category helps concept-based methods obtain a finer-grained analysis for better failure detection. On a different note, Ensemble boosts the Zero-shot’s ACC but still results in a lower AUROC and higher FPR on the large-scale datasets for both backbones. Ensemble, in the same principles as concept-based methods, augments the number of signals; however, we hypothesize the lack of diversity in those signals deteriorates the separability between correct and incorrect samples.

Observation 2: Our method reduces overconfident but incorrect predictions.

In Table 1, we observe that our methods consistently reduce the false positive rate across datasets and for both backbones. Both variants of ORCA decrease the FP $\gtrsim \textcircled { a } 9 5 \mathrm { T P R }$ substantially while keeping AUROC and ACC competitive. On ImageNet, ORCA-B achieves the best performance on this metric, outperforming the zero-shot model and DescCLIP by $3 . 7 2 \%$ and $2 . 1 5 \%$ respectively using ViT-B/32. We hypothesize that allowing the model to recognize an object from different angles provides more reliable confidence assessment, enabling faithful failure detection while also achieving superior predictive accuracy.

# Results on Satellite Image Benchmarks

We report the performance on EuroSAT and RESISC45 on ResNet-101 and ViT-B/32. Note that all results are zero-shot performance. We discuss the following observation:

Observation 3: Our method boosts both predictive and failure detection accuracy on remote sensing benchmarks.

Table 2 shows that ORCA-R consistently outperforms all baselines on all evaluation metrics. Compared to DescCLIP $+ \mathrm { M S P }$ on EuroSAT, ORCA-R enjoys a $3 . 6 \%$ improvement in AUROC and $6 . 2 5 \%$ in FPR while boosting the overall accuracy by $1 . 4 9 \%$ . On RESISC45, while ORCA-R’s improvement on AUROC and ACC is marginal, it significantly reduces FPR. Additionally, these datasets represent out-ofdistribution data for CLIP, underscoring ORCA’s enhanced reliability and robustness against such distributional variations.

# Ablation Studies

We conduct two ablation studies on the effect of the number of concepts and the choice of the weighting function used for ORCA-R in this section.

Ablation on number of concepts. We use the ViT-B/32 backbone on CIFAR-100 and $\bar { K ^ { ' } } = \{ 5 , 1 0 , 1 5 , 2 0 \}$ for this experiment. We study the effect of the number of concepts on the performance on AUROC and $\mathrm { F P R } @ 9 5 \mathrm { T P R }$ of Desc${ \bf C L I P + M S P } _ { \mathrm { : } }$ ODIN, DOCTOR and ORCA-R. Fig. 3 shows that the FPR of ORCA-R is consistently lower than those of the other baselines across various $K$ . We also see an increasing (decreasing) trend in AUROC (FPR) as the number of concepts rises. This signifies a finer-grained assessment both enables better failure detection and alleviates the problem of assigning high confidence to incorrect predictions.

![](images/5de579be4b6573fd6fcab74b64cd71098a780bf6b1ddeb33929f5f67d3802559.jpg)  
Figure 3: Failure detection accuracy (AUROC) and false positive rate $( \mathrm { F P R } @ 9 5 \mathrm { T P R } _ { \cdot }$ ) across different numbers of concepts on CIFAR-100. Overall, we can an increase in the number of concepts boosts the performance in both metrics.   
Figure 4: Failure detection capabilities of each weighting function on EuroSAT, where Logarithmic consistently outperforms others.

100 Logarithmic Linear 95 Exponential Polynomial 90 85 AUROC FPR95

Ablation on choice of weighting function. We examine how various weighting functions influence the failure detection efficacy of ORCA-R. Fig. 4 (left) visualizes the weight distribution on the top-10 concepts among the weighting functions. In Figure 4 (right), Logarithmic outperforms others, contrasting with Exponential, which exhibits the least effectiveness. Logarithmic ensures a balanced distribution of weights, recognizing the importance of higherranked concepts while also accounting for lower-ranked ones. Conversely, Exponential significantly overweighs the highest-ranked concept, neglecting the contributions of those ranked lower.

<html><body><table><tr><td rowspan="2">Dataset</td><td rowspan="2">Method</td><td colspan="3">ResNet-101</td><td colspan="3">ViT-B/32</td></tr><tr><td>AUROC ↑</td><td>FPR95↓</td><td>ACC ↑</td><td>AUROC↑</td><td>FPR95↓</td><td>ACC ↑</td></tr><tr><td rowspan="10">EuroSAT (K=10)</td><td>Zero-shot+MSP</td><td></td><td>61.73</td><td>88.98</td><td>30.30</td><td>76.42</td><td>80.24</td><td>41.11</td></tr><tr><td></td><td>+ODIN</td><td>61.35</td><td>89.38</td><td>30.30</td><td>75.54</td><td>79.28</td><td>41.11</td></tr><tr><td></td><td>+DOCTOR</td><td>60.76</td><td>89.85</td><td>30.30</td><td>76.67</td><td>79.30</td><td>41.11</td></tr><tr><td>Ensemble+MSP</td><td></td><td>54.69</td><td>92.21</td><td>31.90</td><td>66.83</td><td>89.19</td><td>48.73</td></tr><tr><td></td><td>+ODIN</td><td>55.10</td><td>93.09</td><td>31.90</td><td>65.73</td><td>90.09</td><td>48.73</td></tr><tr><td></td><td>+DOCTOR</td><td>53.73</td><td>94.09</td><td>31.90</td><td>61.14</td><td>90.63</td><td>48.73</td></tr><tr><td>DescCLIP+MSP</td><td></td><td>64.89</td><td>86.39</td><td>33.13</td><td>73.93</td><td>77.54</td><td>48.51</td></tr><tr><td></td><td>+ ODIN</td><td>64.16</td><td>87.16</td><td>33.13</td><td>71.74</td><td>78.34</td><td>48.51</td></tr><tr><td></td><td>+DOCTOR</td><td>62.79</td><td>89.05</td><td>33.13</td><td>72.74</td><td>79.85</td><td>48.51</td></tr><tr><td>ORCA-B ORCA-R</td><td></td><td>67.86</td><td>86.43</td><td>34.11</td><td>76.20</td><td>77.80</td><td>49.74</td></tr><tr><td rowspan="9">RESISC45 (K=10)</td><td>Zero-shot+MSP</td><td></td><td>69.01</td><td>86.43</td><td>34.76</td><td>77.55</td><td>71.29</td><td>50.00</td></tr><tr><td></td><td></td><td>68.13</td><td>87.04</td><td>37.66</td><td>77.92</td><td>80.35</td><td>55.57</td></tr><tr><td></td><td>+ODIN</td><td>62.60</td><td>89.48</td><td>37.66</td><td>71.66</td><td>84.85</td><td>55.57</td></tr><tr><td>Ensemble+MSP</td><td>+DOCTOR</td><td>67.57</td><td>87.19</td><td>37.66</td><td>76.95</td><td>82.17</td><td>55.57</td></tr><tr><td></td><td></td><td>68.87</td><td>85.39</td><td>39.79</td><td>78.40</td><td>80.14</td><td>56.68</td></tr><tr><td></td><td>+ODIN</td><td>62.67</td><td>89.57</td><td>39.79</td><td>71.99</td><td>85.31</td><td>56.68</td></tr><tr><td>DescCLIP+MSP</td><td>+DOCTOR</td><td>67.88</td><td>87.29</td><td>39.79</td><td>77.54</td><td>82.34</td><td>56.68</td></tr><tr><td></td><td></td><td>73.44</td><td>79.78</td><td>43.16</td><td>77.47</td><td>82.25</td><td>58.33</td></tr><tr><td></td><td>+ODIN</td><td>69.47</td><td>84.61</td><td>43.16</td><td>71.49</td><td>86.21</td><td>58.33</td></tr><tr><td></td><td></td><td>+DOCTOR</td><td>72.95</td><td>79.89</td><td>43.16</td><td>76.81</td><td>84.88</td><td>58.33</td></tr><tr><td></td><td>ORCA-B</td><td></td><td>71.88</td><td>90.41</td><td>46.22</td><td>77.71</td><td>86.31</td><td>59.10</td></tr><tr><td></td><td>ORCA-R</td><td></td><td>74.28</td><td>80.31</td><td>45.13</td><td>78.24</td><td>76.52</td><td>59.10</td></tr></table></body></html>

Table 2: Performance on EuroSAT and RESICS45. AUROC, FPR $@$ 95TPR (FPR95), and ACC are percentages. With ACC taken into account, bold indicate the best results, underlined denote ours with the second best results.

Standard Ours auto 0.70 road vehicle 28.2 truck0.05 four wheels 27.5 ship0.05 on highway 27.4 cargo area 26.3 auto 0.34 trailer 26.2 truck 0.32 Interpret   
Label: ship ship0.05 hull shape 25.4 Class probability CLIP similarity scores (a) Failure caused by spurious correlation. Standard Ours plane 0.95 on water 22.8 ship0.03 jet engine 22.7 bird0.01 mental wing 21.6 frequent flyer 21.0 plane 0.32 runway 20.4 ship 0.30 Interpret   
Label: ship bird0.10 nautical flag 19.6 Class probability CLIP similarity scores

(b) Failure caused by cross-category resemblance.

# Failure Interpretation

ORCA not only achieves superior failure detection but also enables failure interpretation with human-level concepts. We discuss two scenarios that cause the model to output overconfident values on misclassified samples: spurious correlation and cross-category resemblance (Fig. 5).

In the former scenario (Fig. 5a), the presence of a road (a spurious feature) leads the model to misclassify the ship as a land vehicle, automobile or truck. We demonstrate that a standard model struggles to identify such failures, resulting in a high confidence score for automobile. In contrast, ORCA leverages human-level concepts, offering more nuanced signals for a refined assessment of the model’s confidence. For instance, strong responses from concepts like “road vehicle” and “four wheels” for automobile, and “cargo area” and “trailer” for truck, contribute to a significantly lower confidence. Furthermore, we can easily interpret why the model makes such a prediction through concepts.

In the latter scenario (Fig. 5b), the ship (sailboat) bears a resemblance to an airplane from a distance. The similarity between the sky and water also creates an illusion of the object being airborne. The top- $K$ concepts from our method exhibit strong responses to concepts associated with airplanes and birds. Analyzing this information allows us to confidently deduce that the model misclassifies the image as an airplane due to the sky-like background and the object’s resemblance to an airplane.

Code — https://github.com/Nyquixt/ORCA