# Revelations: A Decidable Class of POMDPs with Omega-Regular Objectives

Marius Belly1, Nathanae¨l Fijalkow1, Hugo Gimbert1, Florian Horn2, Guillermo A. P´erez3, Pierre Vandenhove1\*

1CNRS, LaBRI, Universite´ de Bordeaux, France 2CNRS, IRIF, Universite´ de Paris, France 3University of Antwerp – Flanders Make, Antwerp, Belgium

# Abstract

Partially observable Markov decision processes (POMDPs) form a prominent model for uncertainty in sequential decision making. We are interested in constructing algorithms with theoretical guarantees to determine whether the agent has a strategy ensuring a given specification with probability 1. This well-studied problem is known to be undecidable already for very simple omega-regular objectives, because of the difficulty of reasoning on uncertain events. We introduce a revelation mechanism which restricts information loss by requiring that almost surely the agent has eventually full information of the current state. Our main technical results are to construct exact algorithms for two classes of POMDPs called weakly and strongly revealing. Importantly, the decidable cases reduce to the analysis of a finite belief-support Markov decision process. This yields a conceptually simple and exact algorithm for a large class of POMDPs.

Code — https://github.com/gaperez64/pomdps-reveal Extended version — https://arxiv.org/abs/2412.12063

# 1 Introduction

Partially observable Markov decision processes (POMDPs) form a prominent model for uncertainty in sequential decision making. They were defined in the 1960s ( ˚Astro¨m 1965) for operations research and introduced in artificial intelligence by the seminal paper of Kaelbling, Littman, and Cassandra (1998). We consider POMDPs from a model-based point of view common in planning and in formal methods. Our goal is to construct exact (as opposed to approximate) algorithms that take as an input a complete description of the POMDP and construct a strategy ensuring a given specification. A long line of work has established that most formulations of this problem are undecidable. For instance, even in the extreme case where the agent has no information and the goal is to reach a target state with arbitrarily high probability, complex convergence phenomena occur, implying strong undecidability results (Madani, Hanks, and Condon 2003; Gimbert and Oualhadj 2010; Fijalkow 2017).

In this work, we are interested in constructing almost-sure strategies, meaning strategies ensuring their specifications with probability 1. We consider the class of omega-regular objectives (all expressible as parity objectives), which is a robust class including properties expressible in Linear Temporal Logic (Pnueli 1977; Giacomo and Vardi 2013). Determining whether there exists an almost-sure strategy against the subclass of CoBu¨chi objectives (requiring to avoid a target from some point onwards) is undecidable (Chatterjee, Chmelik, and Tracol 2016; Bertrand, Genest, and Gimbert 2017). There is a vast body of work towards approximate and practical solutions: for instance, using interpolation in the belief space (Lovejoy 1991), approximation of the value function (Hauskrecht 2000), or Monte Carlo tree search approaches (Silver and Veness 2010). This is orthogonal to the current paper since we focus on exact algorithms.

Our starting point is a simple approach to construct almost-sure strategies: from the POMDP, we build a Markov decision process (MDP) whose states are supports of the beliefs of the POMDP. In other words, we store information about which states we can be in, but abstract away the probabilities. The belief-support MDP serves as a finite abstraction of the POMDP; one could expect that there exists an almost-sure strategy in the POMDP if and only if there exists one in the corresponding belief-support MDP. Unfortunately, this abstraction is neither sound nor complete; we present a simple counterexample in Figure 1.

The fundamental question we ask is whether there are natural sufficient conditions making the belief-support abstraction correct. Conceptually, the failure of this abstraction is due to the accumulation of information loss over time.

We introduce a revelation mechanism which restricts information loss by requiring that, almost surely, the agent has eventually full information of the current state. Intuitively, by forbidding information loss from accumulating for an unbounded amount of time, the revelation mechanism removes the convergence issues leading to undecidability. Practically, we conjecture that revelation is a commonly occurring phenomenon in partial observability; a canonical example is systems with a small probability of resetting infinitely often, and where this reset is observable. We leave to future work to investigate this question further. Other approaches to restrict information loss have been proposed; we refer to the related works (Section 6) for an additional discussion.

![](images/b36bce5d7d74e9b81a81a5ace4ca3d45c577600b33cd74d4ff0ed7f00eff2512.jpg)  
Figure 1: We consider the POMDP on the LHS: there is a single signal $s$ , so no information is ever given about the exact state we are in (a behavior the revelation mechanisms forbid!). Yet, almost surely, we reach $q _ { 1 }$ . The priorities indicated on states constitute a parity condition inducing the objective “eventually never visiting ${ q _ { 1 } } ^ { \prime \prime }$ , which clearly cannot be ensured almost surely. We represent the belief-support $M D P$ on the RHS: the two states are $\{ { q } _ { 0 } \}$ and $\{ q _ { 0 } , q _ { 1 } \}$ , and only the state $\left\{ q _ { 0 } , q _ { 1 } \right\}$ is visited infinitely often. To assign priorities to the states of this MDP, there are two natural candidates: “maximal priority semantics” and “minimal priority semantics”, meaning that we assign either the maximal or minimal priority from the states in the belief support. In this figure, we use the maximal priority semantics: the priority of $\{ q _ { 0 } , q _ { 1 } \}$ is thus 2, so the belief-support MDP is winning. This means that the analysis of the belief-support MDP is not sound in general. By tweaking the priorities in this example, one can show that both priority semantics are neither sound nor complete.

![](images/662c36ef03b5f17d2f1d8c627087f539a7364b086b61cfcd5f01121be7b84ca2.jpg)  
Figure 2: Summary of our results: decidable subclasses of the parity objective depending on the revelation mechanism.

# Our contributions.

• We study two properties of POMDPs based on the revelation mechanism, called weak and strong revelations. • We obtain decidability (and undecidability) results for both classes. Importantly, the decidable cases reduce to the analysis of the finite belief-support MDP. A summary of our contributions for POMDPs is provided in Figure 2. We also briefly consider the class of two-player games of partial information, to show that our revealing mechanisms do not suffice for decidability on this larger class. • We provide a simple implementation of the algorithm as a proof of concept. We provide a comparison between our algorithm and off-the-shelf deep reinforcement learning (DRL) trained via an observation wrapper. As we will show in the paper, the MDP induced by the belief supports carries sufficient information to play in revealing POMDPs; hence, we used a wrapper implementing a subset construction on the fly to generate the current belief support, and focused on algorithms intended for MDPs. Spending moderate effort on reward engineering and hyperparameter tuning, we have been unable to match the performance of our algorithm (see Figure 3).

This yields a conceptually simple and exact algorithm for a large class of POMDPs. The importance of our results can be appreciated by the following remark: instead of a subclass of POMDPs, the revelation mechanism can be seen as new semantics for all POMDPs. In that sense, we obtain decidability results for an optimistic semantics of POMDPs which, to the best of our knowledge, has not been done before. We refer to Section 5.3 for more details on this point of view.

![](images/d9731d095a20a561ff95c33b71b846c84387bb9ff493e9a102f976542a7e9e45.jpg)  
Figure 3: Omega-regular specifications have a natural interpretation in terms of bad events that must all be trumped by future good events. Along a simulation of the POMDP, one can keep track of the number of steps from the last bad event that has not yet been trumped (i.e., lower is better). Here, we depict this value, per step (from 1 to 500) over 500 simulations of a revealing version of the classical tiger POMDP (Cassandra, Kaelbling, and Littman 1994). A2C, DQN, and PPO are (MlpPolicy) strategies obtained from the stable-baselines library (Raffin et al. 2021), trained (for a total of $\mathrm { 1 0 k }$ time steps) with default parameter values using a simple reward scheme: a good event yields a reward of 100; a bad one, $- 1$ . In the simulations, the trained models are queried for deterministic action predictions. The example used will be discussed in Section 5, Example 2.

Extended version. Due to a lack of space, proofs are omitted from this version. They are in the appendix of the extended version (Belly et al. 2024), along with additional details and examples.

# 2 Preliminaries

A (discrete) probability distribution on a finite set $X$ is a function $d \colon X \ \to \ [ 0 , 1 ]$ such that $\begin{array} { r } { \sum _ { x \in X } d ( x ) \ = \ 1 } \end{array}$ . The set of all probability distributions on $X$ is denoted ${ \mathcal { D } } ( X )$ . The support supp $( d )$ of a probability distribution $d$ is the set $\{ x \ \in \ X \ | \ d ( x ) \ > \ 0 \}$ . We let $| X |$ denote the number of elements in a set $X$ .

# 2.1 POMDPs

A partially observable Markov decision process (POMDP) is a tuple $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ such that $Q$ is a finite set of states, Act is a finite set of actions, $\mathsf { S i g }$ is a finite state of signals, $\delta \colon Q \times \mathsf { A c t } \to \mathcal { D } ( \mathsf { S i g } \times Q )$ is the transition function, and $q _ { 0 } \in Q$ is an initial state.

A play of a POMDP $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ is an infinite sequence $\pi = q _ { 0 } a _ { 1 } s _ { 1 } q _ { 1 } a _ { 2 } s _ { 2 } \ldots \in ( Q \cdot \mathrm { A c t } \cdot \mathsf { S i g } ) ^ { \omega }$ such that, for all $i \geq 0$ , $\delta ( q _ { i } , a _ { i + 1 } ) ( s _ { i + 1 } , q _ { i + 1 } ) > 0$ . A history $h$ of a POMDP is a finite prefix of a play ending in a state (it is an element of $( Q \cdot \mathsf { A c t } \cdot \mathsf { S i g } ) ^ { * } \cdot Q )$ . If $h = q _ { 0 } a _ { 1 } s _ { 1 } q _ { 1 } \ldots a _ { n } s _ { n } q _ { n }$ , we write $\mathsf { l a s t } ( h )$ for $q _ { n }$ . In practice, states are not fully observable; we define an observable history as the projection of a history to the subsequence in $( \mathsf { A c t } \cdot \mathsf { S i g } ) ^ { * }$ . We write $\mathsf { o b s } ( h )$ for the observable history derived from a history $h$ , i.e., the same sequence with the states removed.

For $q$ a state of a POMDP $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ , we define ${ \mathcal { P } } ^ { q }$ to be the POMDP $( Q , { \mathsf { A c t } } , { \mathsf { S i g } } , \delta , q )$ with only a change of initial state. We let $\beta _ { \mathcal { P } } ~ = ~ \operatorname* { m i n } \{ \delta ( q , a ) ( s , q ^ { \prime } ) ~ |$ $q , q ^ { \prime } \in Q$ , $a \in \mathsf { A c t }$ , $s \in \mathsf { S i g }$ , and $\delta ( q , a ) ( s , q ^ { \prime } ) > 0 \}$ denote the least non-zero probability occurring in $\mathcal { P }$ .

A Markov decision process (MDP) is a tuple $\begin{array} { l } { \mathcal { M } \ = } \end{array}$ $( Q , \mathsf { A c t } , \delta , q _ { 0 } )$ where $\delta \colon Q \times \mathsf { A c t } \to \mathcal { D } ( Q )$ . Formally, an MDP $\mathcal { M } = ( Q , \mathsf { A c t } , \delta , q _ { 0 } )$ can be seen as a POMDP $\mathcal { P } =$ $( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ such that ${ \mathsf { S i g } } = \{ s _ { q } ~ | ~ q \in Q \}$ and for all $q , q ^ { \prime } , q ^ { \prime \prime } \in Q$ and $a \in \mathsf { A c t }$ , $\delta ( q , a ) ( s _ { q ^ { \prime \prime } } , q ^ { \prime } ) > 0$ if and only if $q ^ { \prime } = q ^ { \prime \prime }$ . In practice, it means that the last signal always uniquely determines the current state. MDPs have “complete observation”, whereas POMDPs have “partial observation”. For a POMDP $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ , we define the underlying MDP of $\mathcal { P }$ to be the MDP $( Q , \mathsf { A c t } , \delta ^ { \prime } , q _ { 0 } )$ with $\begin{array} { r } { \delta ^ { \prime } ( q , a ) ( q ^ { \prime } ) \stackrel { - } { = } \sum _ { s \in \mathsf { S i g } } \delta ( q , a ) ( s , q ^ { \prime } ) } \end{array}$ .

Remark 1. The observable information in POMDPs is here provided through signals that appear along transitions. This contrasts with state-based observations that partition the state space, which are also frequently used to model POMDPs. Both models are polynomially equivalent: a POMDP with observations can be transformed into an equivalent POMDP with signals on the same state space, while the converse requires an increase of the state space linear in |Sig|. Both choices are convenient, but using signals make the definition of strongly revealing (Definition 2) more natural, which is why we opted for this convention.

Strategies. Let $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ be a POMDP. An (observation-based) strategy in $\mathcal { P }$ is a function that makes decisions based on the current observable history, i.e., it is a function $\sigma \colon ( \mathsf { A c t } \cdot \mathsf { S i g } ) ^ { * } \to { \mathcal { D } } ( \mathsf { A c t } )$ . We can define strategies in MDPs similarly (i.e., assuming that $\mathsf { S i g }$ gives the information of the current state), but we assume for convenience that a strategy is a function $\sigma \colon ( \mathsf { A c t } \cdot Q ) ^ { \ast }  { \mathcal { D } } ( \mathsf { A c t } )$ in this case. An observable history $a _ { 1 } s _ { 1 } \ldots a _ { n } s _ { n }$ is consistent with a strategy $\sigma$ if for all $1 \leq i < n , \sigma ( a _ { 1 } s _ { 1 } \ldots a _ { i } s _ { i } ) ( a _ { i + 1 } ) > 0$ . A strategy $\sigma$ is pure if for all observable histories $h \in$ $( \mathsf { A c t } \cdot \mathsf { S i g } ) ^ { * }$ , $\sigma ( h )$ is a Dirac distribution; in other words, if $\sigma$ is a function $( \mathsf { A c t } \cdot \mathsf { S i g } ) ^ { * } \to \mathsf { A c t }$ . We let $\Sigma ( \mathcal { P } )$ denote the set of strategies in POMDP $\mathcal { P }$ and $\Sigma _ { \mathsf { P } } ( \mathcal { P } )$ denote the set of pure strategies in $\mathcal { P }$ .

For an MDP $\mathcal { M }$ , a strategy $\sigma$ in $\mathcal { M }$ is memoryless if its decisions are only based on the current state: i.e., if for all histories $h _ { 1 } , h _ { 2 }$ , $\mathsf { i a s t } ( h _ { 1 } ) = \mathsf { i a s t } ( h _ { 2 } )$ implies $\sigma ( h _ { 1 } ) = \sigma ( h _ { 2 } )$ . We only define the memoryless notion for MDPs.

Probability measure induced by a strategy. Let $\mathcal { P } \ =$ $( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ be a POMDP. For a history $h$ of $\mathcal { P }$ , we define ${ \mathsf { C y l } } ( h )$ (the cylinder of $h$ ) to be the set of all plays starting with $h$ , i.e., $h ( { \mathsf { A c t } } \cdot { \mathsf { S i g } } \cdot Q ) ^ { \omega }$ . Given a strategy $\sigma$ , we can define a probability measure $\mathbb { P } _ { \sigma } ^ { \mathcal { P } [ \cdot ] }$ on infinite plays. This function is naturally defined over cylinders by induction. We define $\mathbb { P } _ { \sigma } ^ { \mathcal { P } } [ \mathsf { C y l } ( q _ { 0 } ) ] \ : = \ : 1$ , and $\mathbb { P } _ { \sigma } ^ { \mathcal { P } } [ \mathsf { \bar { C } } \mathsf { y } | ( q ) ] \ \stackrel { \bullet } { = } \ 0$ for $q \in Q$ , $q \ne q _ { 0 }$ . For a history $h = h ^ { \prime } a s q$ , we define $\mathbb { P } _ { \sigma } ^ { \mathcal { P } } [ \mathsf { C y l } ( h ) ] =$ $\begin{array} { r } { \mathbb { P } _ { \sigma } ^ { \mathcal { P } } [ \mathsf { C y l } ( h ^ { \prime } ) ] \cdot \sigma ( \mathsf { o b s } ( \bar { h } ^ { \prime } ) ) ( a ) \cdot \delta ( \mathsf { l a s t } ( h ^ { \prime } ) , a ) ( s , q ) . } \end{array}$ . By IonescuTulcea extension theorem (Klenke 2007), this function can be uniquely extended to a probability distribution $\mathbb { P } _ { \sigma } ^ { \mathcal { P } [ \cdot ] }$ over the Borel sets of infinite plays induced by all cylinders.

We use this probability distribution to measure sets of infinite sequences in $Q ^ { \omega }$ , by associating a set $W \subseteq Q ^ { \omega }$ with the set $\bigcup _ { q _ { 0 } q _ { 1 } . . . \in W } q _ { 0 } \mathsf { A c t } \mathsf { S i g } q _ { 1 } \mathsf { A c t } \mathsf { S i g } q _ { 2 } . . . \subseteq ( Q \times \mathsf { A c t } \times \mathsf { S i g } ) ^ { \omega }$ . Similarly, we use this probability distribution to measure events based on signals, by associating a set $S \subseteq \mathsf { S i g } ^ { \omega }$ with the set $\bigcup _ { s _ { 1 } s _ { 2 } . . . \in S } { \bar { Q } } \mathsf { A c t } s _ { 1 } { \bar { Q } } \mathsf { A c t } s _ { 2 } { \bar { Q } } \ldots \subseteq ( Q \times \mathsf { A c t } \times \mathsf { S i g } ) ^ { \omega }$ .

Objectives. Let $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ be a POMDP. An objective $W \subseteq Q ^ { \omega }$ is a measurable set of infinite sequences of states. Note that observing an infinite sequence of signals (but not the states) may not always be sufficient to determine whether a play satisfies an objective.

Given a set ${ \begin{array} { r l r l } { F } & { } & { \subseteq } & { Q } \end{array} }$ , the reachability objective $\mathsf { R e a c h } ( F ) = \{ q _ { 0 } q _ { 1 } \ldots \in Q ^ { \omega } \mid \exists i \geq 0 , q _ { i } \in F \}$ is the set of plays that visit a state in $F$ at least once. For $k \in \mathbb { N }$ , we write ${ \mathsf { R e a c h } } ^ { \le k } ( F ) = \{ q _ { 0 } q _ { 1 } \dots \in Q ^ { \omega } \mid \exists i , 0 \le i \le k , q _ { i } \in F \}$ for the set of plays that reach $F$ in at most $k$ steps. Given a set $F \subseteq Q$ , the safety objective Safety $( F )$ is the set of plays that never visit any state in $F$ .

Given a priority function $p \colon Q \ \to \ \{ 0 , \ldots , d \}$ (where $d \in \mathbb { N } ,$ ), the parity objective Parity $\mathbf { \Psi } ( p ) = \{ q _ { 0 } q _ { 1 } \dots \in Q ^ { \omega } \mid$ lim $\operatorname* { s u p } _ { i \geq 0 } p ( q _ { i } )$ is even $\}$ is the set of infinite plays whose highest priority seen infinitely often is even. A B¨uchi objective is a parity objective $\mathsf { P a r i t y } ( p )$ such that $p \colon Q \to \{ 1 , 2 \}$ , and a CoB¨uchi objective is a parity objective Parity $( p )$ such that $p \colon Q \  \ \{ 0 , 1 \}$ . For $Q ^ { \prime } \subseteq Q$ , we write ${ \mathsf { B i r c h i } } ( Q ^ { \prime } )$ for the set of infinite plays that visit $Q ^ { \prime }$ infinitely often. It is equal to Parity $( p )$ for the priority function $p$ such that $p ( q ) \bar { = } 2$ if $q \in Q ^ { \prime }$ , and $p ( q ) \stackrel { - } { = } 1$ otherwise.

For an objective $W$ , a strategy $\sigma$ is almost sure if $\mathbb { P } _ { \sigma } ^ { \mathcal { P } } [ W ] = 1 \$ , and is positively winning if $\mathbb { P } _ { \sigma } ^ { \mathcal { P } } [ W ] > 0$ . We say that an objective $W$ has value 1 in a POMDP $\mathcal { P }$ if ${ \mathrm { s u p } } _ { \sigma \in \Sigma ( \mathcal { P } ) } \mathbb { P } _ { \sigma } ^ { \mathcal { P } } [ { \dot { W } } ] = 1$ .

# 2.2 Beliefs and Belief Supports

Let $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ be a POMDP. A belief ${ \mathfrak { b } } \in$ $\mathcal { D } ( Q )$ is a probability distribution on $Q$ . A belief support $b \in 2 ^ { Q } \setminus \{ \bar { \varnothing } \}$ is the support of a belief. For brevity, we write 2Q for 2Q\{∅}. At every step, beliefs and belief supports can b∅e updated when playing an action and observing a signal.

We show how to do so for belief supports: we define a function $\begin{array} { r } { B \colon 2 _ { \varnothing } ^ { Q } \times \mathsf { A c t } \times \mathsf { S i g } \to 2 _ { \varnothing } ^ { Q } } \end{array}$ that updates the belief support. For $b \in 2 _ { \varnothing } ^ { Q }$ , $a \in \mathsf { A c t }$ , $s \in \mathsf { S i g }$ , we define $B ( b , a , s ) = \{ q ^ { \prime } \in$ $Q \mid \exists q \in \bar { b } , \delta ( q , a ) ( s , q ^ { \prime } ) > 0 \}$ . We extend this function in a natural way to a function $\begin{array} { r } { B ^ { * } \colon 2 _ { \varnothing } ^ { Q } \times ( \mathsf { A c t } \cdot \mathsf { S i g } ) ^ { * } \to 2 _ { \varnothing } ^ { Q } } \end{array}$ . Objectives $\mathsf { R e a c h } ( B )$ and $\mathsf { B i c h i } ( B )$ can be naturally extended to sets of belief supports $B \subseteq 2 _ { \varnothing } ^ { Q }$ (Belly et al. 2024, Appendix C).

Beliefs carry more information than belief supports, as they contain the exact probability of being in a particular state, while belief supports only contain the qualitative information of the possible current states. Observe that when the belief support is a singleton (i.e., $b = \{ q \}$ for some $q \in Q$ ), knowing the precise belief does not yield more information than knowing the belief support, as all the probability mass is in one of the states. Our “revealing” restrictions on POMDPs defined later will exploit this fact.

# 3 The Belief-Support MDP

For a POMDP $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ , the belief-support MDP of $\mathcal { P }$ is the MDP $\mathcal { P } _ { B } = ( 2 _ { \varnothing } ^ { Q } , \mathsf { A c t } , \delta _ { B } , \{ q _ { 0 } \} )$ where for $b , b ^ { \prime } \in 2 _ { \varnothing } ^ { Q }$ and $a \in \mathsf { A c t }$ , $\delta _ { B } ( b , a ) ( b ^ { \prime } ) > 0$ if and only if there is $s \in \mathsf { S i g }$ such that $B ( b , a , s ) = b ^ { \prime }$ . We assume the distribution to be uniform over successors with positive probability.

We can show that for some simple objectives, the POMDP and its belief-support MDP behave in a similar way. For example, sets of belief supports that can be reached with a positive probability are the same in the POMDP and its beliefsupport MDP; if a set of belief supports is reachable almost surely in the POMDP, it is also the case in the belief-support MDP (formal statements and proofs in (Belly et al. 2024, Appendix C)).

There is a natural way to lift a strategy in the beliefsupport MDP to a strategy in the POMDP. We define a notation to go from a sequence of signals to the induced sequence of belief supports. Let $h = a _ { 1 } s _ { 1 } \ldots a _ { n } s _ { n } \in ( \mathsf { A c t } \cdot \mathsf { S i g } ) ^ { * }$ be a possible observable history in $\mathcal { P }$ . For $1 \leq i \leq n$ , let $b _ { i } \ = \ B ^ { * } ( \{ q _ { 0 } \} , a _ { 1 } s _ { 1 } \ldots a _ { i } s _ { i } )$ be the belief support after $i$ steps. We define $B _ { h }$ to be the history $a _ { 1 } b _ { 1 } \ldots a _ { n } b _ { n }$ of $\mathcal { P } _ { B }$ . Let $\sigma _ { B } \in \Sigma ( \mathcal { P } _ { B } )$ be a strategy in the belief-support MDP of a POMDP $\mathcal { P }$ . We define a strategy $\widehat { \sigma _ { B } }$ in $\mathcal { P }$ derived from the strategy $\sigma _ { B }$ : for $h \in ( \mathsf { A c t } \cdot \mathsf { S i g } ) ^ { * }$ , wecfix ${ \widehat { \sigma _ { B } } } ( h ) = \sigma _ { B } ( B _ { h } )$ .

# 4 Weakly Revealing POMDPs

We define here our first revealing property for POMDPs, which requires that, infinitely often and almost surely, the current state can be deduced by looking at the previous sequence of signals. Formally, we write $\mathsf { \bar { B } } _ { \mathsf { s i n g } } ^ { \mathcal { P } } = \mathsf { \bar { \{ \{ q \} \mid q \in } } $ $Q \}$ for the set of singleton belief supports of a POMDP $\mathcal { P } =$ $( \bar { Q } , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ . An observable history $h \in ( \mathsf { A c t } \cdot \mathsf { S i g } ) ^ { * }$ such that $\bar { B ^ { * } } ( \{ q _ { 0 } \} , h ) \in B _ { \mathsf { s i n g } } ^ { \mathcal { P } }$ is called a revelation.

Definition 1 (Weakly revealing). A POMDP $\mathcal { P }$ is weakly revealing $i f ,$ for all strategies $\begin{array} { r l r } { \sigma } & { { } \in } & { \Sigma ( \mathcal { P } ) } \end{array}$ , we have $\mathbb { P } _ { \sigma } ^ { \mathcal { P } } \left[ \sf B \ " { u c h i } ( \it B _ { \sf s i n g } ^ { \mathcal { P } } ) \right] \ = \ 1 ,$ ;lmi.oes.t, fuor ay.ll strategies, infinitely

In particular, POMDPs that “reset” infinitely often, and whose reset can be observed with a dedicated signal, are weakly revealing. We will use one such example in Figure 4.

One can give probabilistic bounds on the occurrence of a revelation for a weakly revealing POMDP (Belly et al. 2024, Appendix D): starting from any reachable belief, a revelation occurs within $2 ^ { | Q | } - 1$ steps with probability at least $\beta _ { \mathcal { P } } ^ { 2 ^ { | Q | } - 1 }$ The bound is asymptotically tight: there is a weakly revealing POMDP with $n + 2$ states, 1 action, and $n$ signals where we need at least $2 ^ { n } - 1$ steps before observing a revelation with positive probability (Belly et al. 2024, Appendix E).

The decidability of the weakly revealing property itself is discussed in (Belly et al. 2024, Section 4.4 and Appendix F). A straightforward argument shows that is decidable in 2- EXPTIME. First, extend the POMPD with the information of the current belief support, which makes the state space exponential-sized. Then, check whether there exists a strategy that sees only finitely many singleton sets with positive probability, which is a positive CoBu¨chi objective and is itself decidable in EXPTIME (Chatterjee, Chmelik, and Tracol 2016).

# 4.1 Soundness of the Belief-Support MDP

In this section, we show that, for weakly revealing POMDPs, the existence of an almost-sure strategy in the belief-support MDP (with an adequate priority function) implies the existence of an almost-sure strategy in the POMDP.

For the priority function of the belief-support MDP, we consider the “maximal priority” semantics. Formally, let $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ be a POMDP, and $\mathcal { P } _ { B }$ be its beliefsupport MDP. Let $p \colon Q \to \{ 0 , \ldots , n \}$ be a priority function on $\mathcal { P }$ , inducing the objective Parity $( p )$ . We extend this function to the belief-support MDP: for $b \in 2 _ { \varnothing } ^ { Q }$ , we define

$$
\begin{array} { r } { p _ { B } ( b ) = \operatorname* { m a x } \{ p ( q ) \mid q \in b \} . } \end{array}
$$

Without any assumption, the belief-support MDP may be unsound, already for Bu¨chi objectives; there may be an almost-sure strategy in the belief-support MDP, but not in the POMDP. An example illustrating this was given in Figure 1. Surprisingly, it is sound for CoBu¨chi objectives without any assumption (Belly et al. 2024, Appendix E). Using “max” (and not “min”) turns out to be the right choice in our setting. Intuitively, under the right revealing assumptions and the right strategies, if a belief support is visited infinitely often, then all its states will be visited infinitely often, so the maximal priority of the belief support is the one that matters given the parity objective. Without any assumption, both max and min are unsound and incomplete in general.

Under the weakly revealing semantics, almost-sure strategies of the belief-support MDP carry over to the POMDP for all parity objectives. In other words, the analysis of the belief-support MDP is sound. We recall that pure memoryless strategies suffice to reach the optimal value for parity objectives in MDPs (Chatterjee and Henzinger 2012). The proof is in (Belly et al. 2024, Appendix E).

Proposition 1. Let $\mathcal { P } ~ = ~ ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ be a weakly revealing POMDP with priority function $p ,$ , and let $\mathcal { P } _ { B }$ be its belief-support MDP with priority function p . Assume there is an almost-sure strategy $\sigma _ { B }$ for Parity $\left( p _ { B } \right)$ in $\mathcal { P } _ { B }$ ; by (Chatterjee and Henzinger 2012), we may assume $\sigma _ { B }$ to be pure and memoryless. Then, $\widehat { \sigma _ { B } }$ is an almost-sure strategy for Parity $( p )$ in $\mathcal { P }$ .

# 4.2 Decidability for Priorities 0, 1, and 2

We show that the existence of an almost-sure strategy in a weakly revealing POMDP implies the existence of an almost-sure strategy in its belief-support MDP when priorities are in $\{ 0 , 1 , 2 \}$ . This provides a converse to Proposition 1 when priorities are restricted to $\{ 0 , 1 , 2 \}$ . We will see that this is not the case for priorities in $\{ 1 , 2 , 3 \}$ in the next section; this result is therefore optimal w.r.t. the priority used. We emphasize that parity objectives with priorities $\{ 0 , 1 , 2 \}$ encompass both Bu¨chi and CoBu¨chi objectives. This result is false without the weakly revealing assumption; see the simple POMDP in Figure 1. The proof is in (Belly et al. 2024, Appendix E).

Proposition 2. Let $\mathcal { P } ~ = ~ ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ be a weakly revealing POMDP with priority function $p$ with values in $\{ 0 , 1 , 2 \}$ . Let $\mathcal { P } _ { B }$ be its belief-support MDP with priority function p . If there is an almost-sure strategy for Parity $( p )$ in $\mathcal { P }$ , then there is an almost-sure strategy for Parity $\left( p _ { B } \right)$ in .

From the above, we deduce a complexity upper bound; a matching lower bound is discussed in Section 5.

Theorem 1. The existence of an almost-sure strategy for parity objectives with priorities in $\{ 0 , 1 , 2 \}$ in weakly revealing POMDPs is EXPTIME-complete.

Proof. The EXPTIME algorithm is a consequence of the results from this section: by Proposition 1 (soundness of the belief-support MDP) and Proposition 2 (completeness), we reduce the problem to the existence of an almost-sure strategy for a parity objective with priorities in $\{ 0 , 1 , 2 \}$ in an MDP of size exponential in $| Q |$ . The existence of an almostsure strategy for parity objectives is decidable in polynomial time in MDPs (Baier and Katoen 2008, Theorem 10.127). Proposition 1 also constructs an almost-sure strategy in $\mathcal { P }$ .

The EXPTIME-hardness follows from Proposition 4 below, already for CoBu¨chi objectives and for the more restricted class of strongly revealing POMDPs. □

Remark 2. The algorithm also gives an upper bound on the size of the strategies for parity objectives with priorities in $\{ 0 , 1 , 2 \}$ in weakly revealing POMDPs. As we reduce to the analysis of an exponential-size MDP and that memoryless strategies suffice for parity objectives in MDPs, given Proposition 1, it means that a strategy of exponential size suffices in the POMDP. We can also prove an exponential lower bound (Belly et al. 2024, Appendix E).

# 4.3 Undecidability for Priorities 1, 2, and 3

The previous section suggests that analyzing the beliefsupport MDP is a sound and complete approach for weakly revealing POMDPs with parity objectives with priorities in $\{ 0 , 1 , 2 \}$ . One may wonder whether it is complete for any priority function. Unfortunately, this fails to hold in general, already for priority functions taking values in $\{ 1 , 2 , 3 \}$ . We discuss one such example below.

Example 1. Consider the POMDP $\mathcal { P }$ in Figure 4. This POMDP is weakly revealing, as state $q _ { 0 }$ is visited infinitely often for any strategy and is revealed through signal $s _ { 0 }$ . The only choice in this POMDP is in states $q _ { 1 }$ and $q _ { 1 } ^ { \prime }$ : whether to play a and move to $q _ { 0 }$ or $\{ q _ { 1 } , q _ { 1 } ^ { \prime } \}$ , or to play c and go to $q _ { 2 }$ or q3. Observe that when the game starts in $q _ { 0 }$ , the only reachable belief supports are $\{ \boldsymbol { q } _ { 0 } \}$ , $\{ q _ { 1 } , q _ { 1 } ^ { \prime } \}$ , and $\{ q _ { 2 } , q _ { 3 } \}$ , which all have a maximal odd priority. Hence, the belief-support MDP with priority function $p _ { B }$ trivially has no almost-sure (and even positively) winning strategy. However, we show that there is an almost-sure strategy in $\mathcal { P }$ .

The only way to win in this POMDP is to visit $q _ { 2 }$ infinitely often while visiting $q _ { 3 }$ only finitely often. To do so, observe that when a is played multiple times in a row and only receives signal $s _ { 1 }$ , the probability to be in $q _ { 1 } ^ { \prime }$ becomes arbitrarily close to 1. Formally, if $\sigma _ { a }$ is the strategy that only plays $a$ , we have that for $n > 0$ ,

$$
\mathbb { P } _ { \sigma _ { a } } ^ { \mathcal { P } } [ Q ^ { n } q _ { 1 } ^ { \prime } \mid ( s _ { 1 } ) ^ { n } ] = 1 - \mathbb { P } _ { \sigma _ { a } } ^ { \mathcal { P } } [ q _ { 0 } ( q _ { 1 } ) ^ { n } \mid ( s _ { 1 } ) ^ { n } ] = 1 - \frac { 1 } { 2 ^ { n } } .
$$

For $n > 0$ , let $\sigma _ { n }$ be the strategy that plays only a until $s _ { 1 }$ has been seen n times in a row, and when that is the case, plays $c$ . Let us divide a play in this POMDP into rounds 1, 2,. . . ; every time we go back to $q _ { 0 }$ after visiting $q _ { 2 }$ or $q _ { 3 }$ , we move to the next round. Consider the strategy that plays $\sigma _ { n }$ in round $n$ . This strategy ensures that infinitely many rounds happen, because at each round $n$ , it will eventually succeed in seeing n occurrences of $s _ { 1 }$ in a row. At each round $n$ , c is eventually played with probability 1. By the above equation, $q _ { 3 }$ is seen with probability $\frac { 1 } { 2 ^ { n } }$ and $q _ { 2 }$ is seen with probability $\textstyle { 1 - { \frac { 1 } { 2 ^ { n } } } }$ . State $q _ { 2 }$ is clearly seen infinitely often almost surely. However, the probability that $q _ { 3 }$ is never seen anymore after round $n$ is equal to $\textstyle \prod _ { i = n } ^ { \infty } ( 1 - { \frac { 1 } { 2 ^ { i } } } )$ , which is positive and increases as $n$ grows $t o \infty$ . We deduce that the probability that $q _ { 3 }$ is seen at most finitely often is 1.

Generalizing the above example, we show that if we allow $p$ to take values in $\{ 1 , 2 , 3 \}$ , the existence of almost-sure strategies in weakly revealing POMDPs is undecidable. We provide here a proof sketch; a full proof is in (Belly et al. 2024, Appendix E).

Theorem 2. The existence of an almost-sure strategy in weakly revealing POMDPs with a parity objective with priorities in $\{ 1 , 2 , 3 \}$ is undecidable. The same holds for the existence of a positively winning strategy.

Our proof uses a reduction from the value-1 problem in probabilistic automata. A probabilistic automaton (Rabin 1963) is a tuple $\mathcal { A } = ( Q , \mathsf { A c t } , \delta , q _ { 0 } )$ . One can define their semantics through POMDPs: they behave like POMDPs in which we assume that the signals bring no information (Sig is a singleton). No useful information is provided by the signals along a play (beyond the number of steps played); pure strategies therefore correspond to words on alphabet Act.

Intuitively, the proof expands on the POMDP in Figure 4 by replacing states $q _ { 1 } , q _ { 1 } ^ { \prime }$ by a copy of a probabilistic automaton $\mathcal { A }$ : the transition from $q _ { 0 }$ goes to the initial state of $\mathcal { A }$ , and playing $c$ goes to $q _ { 2 }$ if the current state is a final state of $\mathcal { A }$ , and to $q _ { 3 }$ otherwise. We keep a positive probability to go back to $q _ { 0 }$ at any point to make it weakly revealing. The idea of playing $n$ times $a$ in a row in the example is replaced by a (possible) sequence of words that have a probability arbitrarily close to $1$ to reach a final state. One can show that there is an almost-sure strategy in this POMDP if and only if $\mathcal { A }$ has value 1 w.r.t. its final states.

![](images/3b2056ce72476325b1d3d3ee7b16a343437ecd6e82a72af3f97e4ea81b03b3c1.jpg)  
Figure 4: The POMDP $\mathcal { P }$ from Example 1 (depicted on the left) with an almost-sure strategy, but whose belief-support MDP (depicted on the right) has no winning strategy. Notation $\boldsymbol { q } , \boldsymbol { k }$ inside a circle depicts a state $q$ with priority $k$ . Transitions from states involving a bullet $\bullet$ indicate a probabilistic transition. In POMDPs, we always write the signals along transitions. Actions are omitted when they all induce the same transition from a given state, and probabilities equal to 1 are omitted.

# 5 Strongly Revealing POMDPs

In this section, we introduce strongly revealing POMDPs, a stronger property entailing that infinitely many revelations occur in a POMDP almost surely. We show that the existence of almost-sure strategies is decidable for strongly revealing POMDPs with arbitrary parity objectives.

We define a notion of revealing signals: for $q$ a state of a POMDP $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ , we define Revealing $( q ) =$ $\{ s \in { \mathrm { S i g ~ } } | \ \forall r , r ^ { \prime } \in Q , r ^ { \prime } \neq q \implies \delta ( r , a ) ( s , r ^ { \prime } ) = 0 \}$ to be the set of signals that indicate surely that the next state is $q$ . For convenience, we define ${ \mathsf { S u c c } } ( q , a ) = \{ q ^ { \prime } \in Q \ |$ $\exists s \in \mathrm { S i g } , \delta ( q , a ) ( s , q ^ { \prime } ) > 0 \}$ and $\mathsf { S u c c } ( q , a , s ) = \{ q ^ { \prime } \in Q \ |$ $\delta ( q , a ) ( s , q ^ { \prime } ) > 0 \}$ .

Definition 2. POMDP $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ is strongly revealing $i f$ any transition between two states for a given action in the underlying MDP of $\mathcal { P }$ can also happen with a revealing signal. Formally, $\mathcal { P }$ is strongly revealing if for all $q , q ^ { \prime } \in Q$ and $a \in \mathsf { A c t } .$ , if $q ^ { \prime } \in \mathsf { S u c c } ( q , a ) ,$ , then there is $s \in { \mathsf { R e v e a l i n g } } ( q ^ { \prime } )$ such that $q ^ { \prime } \in \mathsf { S u c c } ( q , a , s )$ .

Under this definition, the set of belief supports $B _ { \mathrm { s i n g } } ^ { \mathcal { P } }$ is visited infinitely often from the initial state for any given strategy, so a strongly revealing POMDP is in particular weakly revealing. Observe that the weakly revealing POMDP from Figure 4 is not strongly revealing: for instance, $q _ { 1 } ^ { \prime } \in \mathsf { S u c c } ( q _ { 1 } , a )$ , but there is no revealing signal that could for sure reveal $q _ { 1 } ^ { \prime }$ after $q _ { 1 }$ . The strongly revealing property can be decided in polynomial time in the size of a POMDP by simply analyzing every transition.

Example 2. We give an example of a strongly revealing POMDP inspired from the tiger of (Cassandra, Kaelbling, and Littman 1994), depicted in Figure 5. This example was used in Figure 3 in the introduction; the code to generate it in our tool is provided in (Belly et al. 2024, Appendix A).

![](images/72272e90aa29236d7ed8d32affb66696eb99d2182a1772cecac79d4bcf4d42ec.jpg)  
Figure 5: Strongly revealing tiger (Example 2).

In the tiger environment, an agent has to open the left or the right door, with action aL or aR, respectively. One of them has a (deadly) tiger behind it. Fortunately, the agent can choose to wait and listen (action a?) to help its decision. Listening results in a signal that is biased towards the reality, i.e., the signal can be sL or sR and the former is more likely if the tiger really is on the left, and vice versa.

We present our version of the tiger environment in which listening guarantees one will eventually discern behind which door there is a tiger. This is achieved by adding new revealing signals $a _ { \mathsf { L } ! }$ or aR! which, importantly, can only be obtained when the tiger is on the left or on the right, respectively. To keep things interesting, these signals can only be obtained with a small probability (yet, them being there already ensures that the POMDP is strongly revealing). We also add signals for death $( s _ { \perp } )$ and victory $( s _ { \top } )$ , which are missing from the original tiger environment.

# 5.1 Decidability of Parity with Strong Revelations

The soundness of the analysis of the belief-support MDP for strongly revealing POMDPs follows from Proposition 1; it remains to show completeness (proofs for this section in (Belly et al. 2024, Appendix G)).

Proposition 3. Let $\mathcal { P } = ( Q , \mathsf { A c t } , \mathsf { S i g } , \delta , q _ { 0 } )$ be a strongly revealing POMDP with a priority function $p$ , and let $\mathcal { P } _ { B }$ be its belief-support MDP with priority function p . If there is an almost-sure strategy for Parity $( p )$ in $\mathcal { P }$ , then there is an almost-sure strategy for Parity $\left( p _ { B } \right)$ in ${ \mathcal P } _ { B }$ .

We also show a complexity lower bound. The lower bound holds for CoBu¨chi in strongly revealing POMDPs; as strongly revealing POMDPs are a subclass of weakly ones, the hardness follows for weakly revealing POMDPs.

Proposition 4. The following problem is EXPTIME-hard: given a strongly revealing POMDP with a CoB¨uchi objective, decide whether there is an almost-sure strategy.

We obtain as before the decidability of the problem by reducing to the analysis of the belief-support MDP. The proof is similar to the one of Theorem 1, simply replacing the use of Proposition 2 by Proposition 3.

Theorem 3. The existence of an almost-sure strategy for parity objectives in strongly revealing POMDPs is EXPTIME-complete.

# 5.2 Undecidability of Strongly Revealing Games

We discuss here whether the revealing semantics helps in zero-sum games of partial information with revealing semantics. In general, such games with CoBu¨chi objectives are undecidable (they encompass POMDPs) while Bu¨chi games are decidable for almost-sure strategies (Bertrand, Genest, and Gimbert 2017). We obtained a negative result: the existence of an almost-sure strategy in CoBu¨chi games with partial information is undecidable, even when satisfying a natural extension of the strongly revealing property. The model is defined formally in (Belly et al. 2024, Appendix G), along with an undecidability proof.

# 5.3 Optimistic Semantics for POMDPs

In our revealing definitions, we adopted the point of view of considering subclasses of POMDPs. A limitation of this point of view is that our results say nothing about POMDPs which are not strongly (nor weakly) revealing. We argue that another fruitful formulation of our results concerns the class of all POMDPs, by defining alternative, revealing semantics.

Consider a POMDP $\mathcal { P }$ . Let us define the extended POMDP $\mathcal { P } _ { \mathsf { s r } }$ such that, at each transition, there is a small probability of revealing which state we reach after firing this action, using additional signals $s _ { q }$ , one for each state $q$ of $\mathcal { P }$ .

Theorem 4. For any POMDP $\mathcal { P }$ , $\mathcal { P } _ { \mathsf { s r } }$ is strongly revealing. Moreover, if there is no almost-sure strategy ensuring an omega-regular objective in $\mathcal { P } _ { \mathtt { s r } }$ (which is decidable by Theorem 3), then there is no almost-sure strategy ensuring the same objective in $\mathcal { P }$ .

The contrapositive is easily proved: any almost-sure strategy of $\mathcal { P }$ can be lifted to an almost-sure strategy of $\mathcal { P } _ { \mathsf { s r } }$ . This property justifies the term “optimistic semantics”. Note that the converse implication cannot hold (as POMDPs with omega-regular objectives are undecidable).

# 6 Related Works

We discuss additional references where a restriction is set to stochastic systems to make them decidable.

The closest idea to our revelations that we know of is in (Berwanger and Mathew 2017), defining a class of partialinformation multi-player games with sure (not just almostsure) revelations; from any point in the game, a “revelation” occurs surely within a bounded number of steps. This is a yet stronger kind of revelation mechanism under which even parity games are decidable.

The decisiveness property (Abdulla, Ben Henda, and Mayr 2007; Bertrand et al. 2020) is a useful property to decide reachability properties in infinite stochastic systems (without decision-making). Decisiveness is implied by the existence of a finite attractor; there is such an attractor in weakly revealing POMDPs once we fix a finite-memory strategy (as in Proposition 1).

Another path to decidability and strong guarantees is to restrict strategies, such as studying “memoryless” (Vlassis, Littman, and Barber 2012) or finite-memory (Chatterjee, Chmelik, and Tracol 2016; Andriushchenko et al. 2022) strategies in POMDPs. In our paper, the strategies we consider only use finite memory, as they are memoryless strategies on the belief-support MDP (in our case, they are even shown to be optimal among all strategies under the right assumptions). The sufficiency of belief-support-based strategies in POMDPs, which was known for almost-sure reachability (Baier, Gro¨ßer, and Bertrand 2012), was also exploited to craft efficient algorithms in (Junges, Jansen, and Seshia 2021); such an approach could speed up our algorithms.

In a quantitative setting, the idea of having actions with some cost that reveal the current state or decrease the uncertainty has appeared multiple times in the literature. Such an idea appeared in 2011 (Bertrand and Genest 2011) for POMDPs with quantitative reachability objectives. Recently, active-measuring POMDPs, with a similar mechanism, have been considered in the online planning community (Bellinger et al. 2021; Krale, Sima˜o, and Jansen 2023). Despite a different setting (online planning vs. model checking), it carries an intuition similar to our work: precise states can be known, which helps find good strategies.

Also in online planning, the article (Liu et al. 2022) considers a subclass of POMDPs restricting information loss that make reinforcement learning sample efficient.

# 7 Perspectives

We presented classes of POMDPs for which many natural objectives become decidable, and showed that these lie close to undecidability frontiers (priorities $\{ 0 , 1 , 2 \}$ vs. $\{ 1 , 2 , 3 \}$ , POMDPs vs. games).

Due to their intrinsic undecidability, POMDPs are not often studied through the prism of exact algorithms. We believe there is a lot to gain by understanding more closely $( i )$ the structural properties of POMDPs that make them decidable for classes of objectives (such as weak/strong revelations), and $( i i )$ the conditions that make simple strategies (such as belief-support-based strategies) sufficient. Our article is a new step towards these goals. On a more specific note, an interesting step for $( i )$ could involve framing the exact complexity of the existence of strategies for simple objectives involving beliefs.

# Acknowledgments

This work was partially supported by the SAIF project, funded by the “France $2 0 3 0 ^ { , 9 }$ government investment plan managed by the French National Research Agency, under the reference ANR-23-PEIA-0006. Pierre Vandenhove was funded by ANR project G4S (ANR-21-CE48-0010- 01). This work was sparked by discussions with Guillaume Vigeral and Bruno Ziliotto, following a talk on a related model (Vigeral and Ziliotto 2022).