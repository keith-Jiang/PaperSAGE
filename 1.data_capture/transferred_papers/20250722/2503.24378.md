# ACPBench: Reasoning About Action, Change, and Planning

Harsha Kokel, Michael Katz, Kavitha Srinivas, Shirin Sohrabi

IBM Research Harsha.Kokel,Michael.Katz1,Kavitha.Srinivas $@$ ibm.com, ssohrab@us.ibm.com

# Abstract

There is an increasing body of work using Large Language Models (LLMs) as agents for orchestrating workflows and making decisions in domains that require planning and multistep reasoning. As a result, it is imperative to evaluate LLMs on core skills required for planning. In this work, we present ACPBench, a benchmark for evaluating the reasoning tasks in the field of planning. The benchmark consists of 7 reasoning tasks over 13 planning domains. The collection is constructed from planning domains described in a formal language. This allows us to synthesize problems with provably correct solutions across many tasks and domains. Further, it allows us the luxury of scale without additional human effort, i.e., many additional problems can be created automatically. Our extensive evaluation of 21 LLMs and OpenAI o1 reasoning models highlight the significant gap in the reasoning capability of the LLMs. Our findings with OpenAI o1, a multi-turn reasoning model, reveal significant gains in performance on multiplechoice questions, yet surprisingly, no notable progress is made on boolean questions.

Dataset — https://ibm.github.io/ACPBench Extended version — https://doi.org/10.48550/arXiv.2410.05669

# 1 Introduction

Recent research has explored the potential of using Large Language Models (LLMs) as reasoners for solving multistep reasoning problems (Chu et al. 2024). Building on their success in certain reasoning tasks and benchmarks, there is a growing interest in using LLMs as agents for orchestrating workflows and making decisions in domains that require planning (Huang et al. 2024; Wang et al. 2024a). This is a promising area of research, with potential applications in various fields. However, there is a lack of systematic evaluation of LLMs reasoning and planning capabilities.

This work aims at evaluating and improving language models’ ability to plan. However, end-to-end evaluation of planning ability is challenging. One, if an agent reaches a goal it does not necessarily mean it can plan. Second, evaluating a plan might be difficult in a domain where there can be multiple plans to achieve the goal. So, instead of focusing on the entire end-to-end planning ability, we distill 7 atomic reasoning tasks that are critical for reliable planning and create datasets of such tasks. These tasks focus on reasoning about Actions, Change (transitions) and Planning; hence, we call our benchmark as ACPBench. The tasks include single step reasoning, like evaluating whether an action can be performed in the described state, as well as multi step reasoning, like whether a sequence of actions is a valid plan for the described state and the described goal.

![](images/3fabc8ec31a66f8078b1f1e0f2cc83dcdf01827811cdbb4676a36056da9d5db1.jpg)  
Figure 1: Performance of few state-of-the-art LLMs over different tasks in ACPBench.

For each task, ACPBench features both boolean (Bool) and multiple-choice (MCQ) style questions from 13 domains. All the datasets are generated from a formal representation of the domain in Planning Domain Definition Language (PDDL) (McDermott 2000). Twelve of these domains are well-established benchmarks in both planning and reinforcement learning communities, readily available in PDDL format. Inspired by the shuffle task in BigBenchHard Suite (Suzgun et al. 2023), we have created an additional domain from scratch. The benefit of constructing the dataset from PDDL descriptions is twofold. First, it allows us to use existing planning tools and second, and arguably more important, it allows obtaining provably correct information for all the tasks. Natural language templates for these domains were carefully crafted by 5 researchers. These templates and planning tools enable us to generate massive data for each task.

We evaluate performance of OpenAI o1 reasoning model and 21 state-of-the-art language models (including opensourced Phi-3 128K (Abdin et al. 2024), Granite 3.0 (Granite Team 2024), Mixtral $\mathtt { 8 x 2 2 B }$ (MistralAI 2024), LLAMA-3 70B (Dubey et al. 2024), and a closed source $\mathsf { G P T } - 4 \mathsf { o }$ (OpenAI 2024a)) on the ACPBench. We found that, with Chain-of-Thought prompting (COT) (Wei et al. 2022) and 2-shot examples, $\mathsf { G P T - 4 o }$ was only able to achieve $7 8 . 4 0 \%$ accuracy on MCQ questions in the ACPBench; with lowest accuracy of $5 2 . 5 0 \%$ for the most difficult (validation) task. Similarly, OpenAI o1 preview achieves accuracy of $8 7 . 3 1 \%$ on average for the MCQ questions, with lowest accuracy of $6 3 . 0 8 \%$ for the most difficult task. Figure 1 shows the overall performance of few selected models on all 7 tasks of ACPBench. To understand whether the smaller language models can improve their performance on these tasks, we finetune a language model on these tasks. The finetuning resulted in substantial improvements in performance across tasks and even demonstrated the ability to generalize to previously unseen domains.

In summary, our contributions are as follows:

• We identify a collection of 7 reasoning tasks required for efficient planning and introduce the first of its kind large-scale benchmark—ACPBench. • We evaluate OpenAI o1 reasoning models and 21 state-ofthe-art language models of different sizes on ACPBench. • We finetune a 8B parameter model and show that the finetuned model performs on par with the large models. • We conduct ablation studies as follows: (a) to understand effects of in-context example and COT, (b) to investigate if tasks in ACPBench capture the plan generation ability, and (c) to understand how LLMs’ abilities have progressed over time for ACPBench tasks.

# 2 Related Work and Background

Recognizing the importance of evaluating reasoning and planning ability of LLMs, various benchmarks have been proposed (Liu et al. 2024; Ma et al. 2024). Most relevant to our work are the benchmarks that are generated from PDDL tasks. He et al. (2023) proposed a natural language based question answering style dataset to evaluate LLMs on 4 tasks of projection, execution, planning, and goal recognition. PlanBench (Valmeekam et al. 2023b) is a benchmark suite with 8 planning tasks including plan generation, reasoning about plan execution, and plan verification. Both these benchmarks focus on a limited number of planning domains (mainly the BlocksWorld domain), employing a template-based approach to generate natural language text. In contrast, AutoPlanBench (Stein et al. 2024) proposes to leverage LLMs to generate the natural language template. They prompt an LLM for natural

Context: This is a swap domain where agents are swapping items or roles. Each agent   
is always assigned a single item/role. The goal is to obtain desired items/roles   
assigned. There are 8 agents: carol,   
michelle, xena, vic, dave, zoe, heidi, and alice. There are 8 items/roles:   
quadcopter, frisbee, necklace, whale,   
iceskates, guitar, zebra, and slinky.   
Currently, heidi is assigned necklace,   
michelle is assigned quadcopter, dave is   
assigned iceskates, vic is assigned whale, xena is assigned slinky, carol is   
assigned frisbee, alice is assigned zebra, and zoe is assigned guitar.   
Bool: Is the following action applicable in this state: trade guitar of zoe for   
iceskates of dave?   
MCQ: Which of the following actions will be applicable in this state?   
A. exchange frisbee of carol with zebra of   
alice.   
B. exchange guitar of zoe with necklace of   
vic.   
C. exchange guitar of heidi with zebra of   
zoe.   
D. exchange guitar of vic with zebra of zoe

language template per predicate and per action. By reducing the human effort required for template generation, they were able to scale up the dataset to 12 domains. However, they limit their focus to a single task - plan generation.

In parallel, Handa et al. (2024) proposed ActionReasoningBench, featuring six tasks: Fluent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical RAC, and Composite Questions. Although there is some overlap between the tasks in ActionReasoningBench and ACPBench (for example, the Effects of Actions task overlaps with our Progression task), the majority of the tasks we propose are not covered by ActionReasoningBench: Reachability, Action Reachability, Validation, Justification, Landmarks. Similarly, the following ActionReasoningBench tasks are not covered in ACPBench: State Tracking, and Numerical RAC.

We now switch to providing the necessary background. The ACPBench questions collection is generated based on PDDL tasks. A PDDL task is defined over the first-order language; consisting of predicates, variables, and objects. A state $s$ is defined as a conjunction of grounded (by objects) predicates, also called atoms. An action $a$ is defined as a triple $\langle p r e ( a ) , a d d ( a ) , d e l ( a ) \rangle$ ; consisting of preconditions, add effects and delete effects, each being a conjunction of atoms. An action $a$ is applicable in a state $s$ if the state satisfies the preconditions of the action, i.e $p r e ( a ) \subseteq s$ . On performing an action $a$ in state $s$ , the world transitions to the next state $t = s [ a ] = s \setminus d e l ( a ) \cup a d d ( a )$ . A goal $g$ is also a conjunction of atoms, and a state $s$ is a goal state if $g \subseteq s$ . A sequence of actions $\pi _ { s } = a _ { 1 } \ldots a _ { n }$ is applicable in the state $s$ if the actions are applicable in a sequence to the resulting states. $\pi _ { s }$ is a plan for the state $s$ if $\pi _ { s }$ is an applicable sequence of actions that results in a goal state.

# 3 ACPBench

# 3.1 Domains

ACPBench collection consists of 11 classical planning domains, Alfworld (Shridhar et al. 2021), and a novel swap domain. The 11 classical planning domain, which were also used by AutoPlanBench (Stein et al. 2024), have public problem instance generators (Seipp, Torralba, and Hoffmann 2022). Alfworld is a text-based reinforcement learning environment where an agent is given household tasks like ‘put a pan on the table’. Alfworld uses tasks from the Alfred dataset (Shridhar et al. 2020) and encodes the dynamics of the domain in publicly available PDDL1. Corresponding PDDL problem files are obtained from the MINT benchmark (Wang et al. 2024b). For the novel Swap domain, we created the PDDL domain and the problem instance generator. Figure 2 contains an example problem description in this domain. All the domains are summarized in Table 1.

We meticulously curated a set of templates to transform the PDDL task into a natural language description. Following AutoPlanBench, we explored using LLMs to automatically generate the templates, however, we found the outcome not reliable and needed significant modification. So, instead, 5 researchers crafted the translation, carefully selecting and refining the templates to ensure they accurately convey the desired information. Specifically, we made templates for domain description, problem description and actions, from which we can compose (partial) states – current state or a goal. These three templates, together with the PDDL files, are to be provided for every new domain, should we decide to extend the benchmark in the future.

# 3.2 ACPBench Tasks

We focus on 7 reasoning tasks within the realm of planning. For each task, we provide a description and explain how the data was collected.

Applicability (App) The first, basic requirement for efficient planning is to determine the valid, available actions in a given situation. Various existing work have discussed LLMs fall short of this basic ability. When using GPT-4 Turbo for travel planning, Xie et al. (2024) found that more than $3 0 \%$ of the failed plans had invalid action dead loop–that is even when the model was informed that the action is invalid, LLMs repeated these actions.

For an action to be valid, its preconditions must hold in the state. Given a state $s$ and the set of actions $O$ , the subset of applicable actions would be $O ( s ) = \{ a \in O \mid p r e ( a ) \subseteq s \} .$ easily computable by iterating over the actions. We therefore can create a boolean question with a positive answer by sampling from $O ( s )$ and with a negative answer by sampling from $O \bar { \backslash } O ( s )$ . A multiple-choice question can be created by sampling the correct answer from $O ( s )$ and wrong candidates from $O \bar { \langle } O ( s )$ . Figure 2 shows example domain and problem description used in the context as well as examples of Bool and MCQ questions for the applicability task.

Table 1: Statistics of the 13 domains in ACPBench. The top 8 domains are used for both finetuning and evaluation. The bottom 5 domains are exclusively used for evaluations. Columns indicates the number of predicates and lifted actions in the PDDL domain, as well as the max character length of the NL problem description in the generated dataset.   

<html><body><table><tr><td>Domain</td><td>|#Pred.</td><td># Actions</td><td>Max char.</td></tr><tr><td rowspan="5">Blocksworld Logistics Grippers Grid</td><td>5</td><td>4</td><td>1770</td></tr><tr><td>9</td><td>6</td><td>1065</td></tr><tr><td>4</td><td>3</td><td>1057</td></tr><tr><td>12</td><td>5</td><td>1235</td></tr><tr><td>7 10</td><td>3</td><td>2132</td></tr><tr><td rowspan="5">FloorTile Rovers VisitAll</td><td>25</td><td>7 9</td><td>3196 3631</td></tr><tr><td>3</td><td>1</td><td>1347</td></tr><tr><td></td><td></td><td></td></tr><tr><td>6 12</td><td>5</td><td>1301</td></tr><tr><td>8 1</td><td>7 5</td><td>1140 4302</td></tr></table></body></html>

Progression (Prog) The next task evaluates LLMs ability to understand the outcome of an action or change. This ability is important to track information across transitions. The subpar performance of LLMs on the Tracking Shuffled Objects task in the Big Bench Hard dataset suggests a significant limitation in their ability to reason about the consequences of actions or changes (Suzgun et al. 2023). Further, a few papers have proposed to use LLMs to execute a plan. For example, Wang et al. (2023) asks LLM to devise a plan and execute it step-by-step to reach the goal. To faithfully execute a plan, it is important for LLMs to demonstrate understanding of progression; how the world state is changed by the action.

When a valid action is performed, the state changes in the following manner: The delete effects of that action will no longer hold and the add effects will hold. Everything else remains unchanged. Given a state $s$ and an action $a$ , the next state is $t = s \setminus d e l ( a ) \cup a d d ( a )$ . We can now partition the facts in the problem into four sets: the facts that held before applying the action and still hold $( s \cap t )$ , the facts that held before but not anymore $( s \backslash t )$ , those that did not hold but now hold $\mathbf { \Psi } ( t \mathbf { \theta } \backslash s )$ , and those that did not hold before and still don’t hold $( F \setminus ( s \cup t ) )$ . While the answer of whether the fact is true after applying the action depends only on whether it is in $t$ , the chain of thoughts leading to the answer differs for the aforementioned four cases. We construct a boolean question by sampling from each of the four fact sets (if they are not empty), getting at most two positive and two negative examples per state. A single MCQ is constructed by sampling one possible answer from each of the four fact sets (non-empty ones), according to a uniform procedure described above.

Reachability (Reach) The reachability task evaluates if a specific sub-goal can eventually be reached from the given state by taking (possibly multiple) actions. This is a multistep reasoning task that can help avoid exploring unfeasible options. To maximize the efficiency of LLMs, it is crucial to detect unreachable (sub)goals early on. This can avoid unnecessary prompting and wasteful exploration, ensuring that the LLMs are utilized effectively, especially when used during search (Yao et al. 2023).

Reachability is PSPACE-hard to answer positively in general (Bylander 1994) for a specific fact, since that would require an evidence - a sequence of actions that achieves a state where the specified facts hold. However, generating positive examples is easy, based on any action sequence, taking the facts out of the end state. For negative examples, we explore multiple cases of unreachable facts and fact pairs. First, existing planning methods (under)approximate the reachability with poly-time computable delete-relaxed reachability (Hoffmann and Nebel 2001). Facts that are not delete-relaxed reachable are therefore guaranteed not to be reachable. Another possible reason for a pair of facts that are individually reachable not to be reachable in the same state is if they are mutually exclusive (Lin 2004; Fiˇser and Komenda 2018). A simple example of mutually exclusive facts in the ferry domain are (empty-ferry) and (on ?c), meaning that the ferry cannot be empty and at the same time a car is on the ferry. Third, static facts that are not true in the initial state will never become true. For instance, c0 can never become a location, so (location $c \boldsymbol { O }$ ) is unreachable (not captured by the methods in the first case, as they focus solely on non-static predicates). The chain of thoughts for a positive example is based on a sequence of actions that achieve the fact. For the negative examples, the chain of thoughts follows the argument laid out above for each of the cases. As in the previous case, the MCQ is captured by choosing from the lists of positive and negative options.

Action Reachability (AReach) In API-driven workflows, the objective is typically presented as an instruction to execute a specific function (Qin et al. 2024). In these scenarios, an LLM must identify the necessary prerequisites for execution and formulate a strategy to meet them. Therefore, it is essential for LLMs to assess whether a given instruction is executable from the provided starting point. We formulate this ability as action reachability task.

The action reachability task is closely related to the atom reachability. If an action model is available, then action reachability is equivalent to the atom reachability over the preconditions of the action. Therefore, this task requires an additional reasoning step about action preconditions. Similarly to the atom reachability task, the positive examples are generated from action rollouts, while the negative examples are generated by collecting actions with preconditions including unreachable atoms according to two of the three cases mentioned above delete-relaxed reachability and mutexes. The third case, unreachable static facts, was not used as often creates non-sensible actions board car l0 at location $_ { c I }$ . Instead, we added incorrect action templates for each action, like “board the car c1 at location l0 into the airplane” or “drive from location l0 to location l1”. Here as well, the chain of thoughts are created in a similar manner, and the MCQ is captured based on the positive and negative options lists.

Validation (Val) A body of research has advocated the use of LLMs for validation and refinement (Shinn et al. 2023; Gou et al. 2024; Madaan et al. 2023). In line with this research, we propose a Validation task. Here, given an initial state and a goal condition, the objective is to assess whether the specified sequence of actions is valid, applicable, and successfully achieves the intended goal.

There are essentially only four options in this case: (a) the sequence is not valid, (b) the sequence is valid, but not applicable, (c) the sequence is valid, applicable, but does not achieve the goal, and (d) the sequence is a plan. These are the four options used for all MCQ for this task. Since the options do not change, we generate four questions per sample, for each of the options to be a correct answer. In the boolean case, we create six different questions, with positive and negative variants for the three cases of whether the sequence is valid, applicable, and a plan. We generate the data for these questions from plans as follows. For the case (c), starting from a plan, we replace a suffix with a random rollout, ensuring that the goal is not achieved at the end of the rollout, but the sequence remains applicable. For the case (b), we try to replace an action on the sequence with an inapplicable action (one whose precondition does not hold in the state), starting from the end of the sequence. Once successful, we return the sequence ending with the inapplicable action. For the case of (a), we simply randomly choose an action on the sequence to replace its template with an incorrect action template, as in the previous task.

Justification (Just) A major criteria for plans to be considered reasonable is whether they include unnecessary actions. In the realm of LLMs and API workflows, it is desirable to avoid calling unnecessary APIs as well as reduce wasteful explorations. Hence, it would be of immense value if LLMs are able to identify whether an action is necessary. This corresponds to the justification task in planning literature.

The justification task reasons whether every action is actually needed on the plan. The problem was studied in the literature (Fink and Yang 1992; Salerno, Fuentetaja, and Seipp 2023) and found to be NP-hard in general. However, optimal plans are known to have all their actions being justified and checking whether a single action or a pair of consequent actions can be removed can be done in polynomial time. We consider the following cases, for either a single action or a pair of consequent actions in a plan: 1) a single action can be removed from the plan and the remaining plan is still a valid plan for the same problem 2) an action cannot be removed from the plan 3) the consequent pairs of actions can be removed from the plan 4) the immediate pairs of action cannot be removed from the plan. Note that we truncate the considered plans and only consider two actions after the goal is reached except if the truncation leads to a non-plan. Given a large set of plans, we consider the above 4 cases, and generate positive and negative examples.

Landmarks (Land) LLMs have shown to hallucinate or deviate from the task when the trajectory is long (Huang et al. 2024). To alleviate this problem, various work has proposed to use LLMs to decompose the goal into subgoals and achieve each of these subgoals separately. To do this faithfully, it is crucial for LLMs to be able to identify subgoals that are necessary to achieve the goal. In planning literature such subgoals are often called landmarks (Porteous, Sebastia, and Hoffmann 2001). Landmarks are facts that must become true sometime along every plan. So, the last task in ACPBench evaluates LLMs ability to recognize landmarks.

While checking whether a fact is a landmark is PSPACEhard (Porteous, Sebastia, and Hoffmann 2001), there are several methods that can find a subset of landmarks (Keyder, Richter, and Helmert 2010; Hoffmann, Porteous, and Sebastia 2004; Richter, Helmert, and Westphal 2008; Zhu and Givan 2003). We use the so-called RHW method (Richter, Helmert, and Westphal 2008). Further, negative evidence can be obtained from a collection of plans - a fact that does not appear on all of these plans is not a landmark. We sample from positive and negative examples obtained that way and construct two boolean questions and one MCQ. Here as well, the chain of thoughts generated capture the described logic.

# 3.3 Data Generation

We use 25 PDDL problem files of varying sizes per domain. These 25 tasks are partitioned into a training and a test set. For each task, we use classical planners to generate a large collection of 1000 plans (Katz and Lee 2023; Katz and Sohrabi 2020). With these plans, we sample the state space as follows. First, given a set of plans, we gather the states along these plans. Then, in order to obtain a diverse sample, we run random rollouts from each of the states found. The number of plans and the sample size are configurable parameters. In the landmarks task described above, we also find plans for the sampled states. To do that, we replace the initial state with the sampled state in the planning problem instance and run a top- $\mathbf { \nabla } \cdot \mathbf { k }$ planner (Katz and Lee 2023). For finding mutexes, we exploit lifted mutex groups implementations from Fiˇser (2020). In this manner, we can potentially generate as many examples as we want. But to keep the test set of reasonable size, we generate only 10 examples per domain, per task.

# 4 Experiments

# 4.1 Evaluation of Pre-trained Language Models

We first analyse how pre-trained language models perform on ACPBench. Table 2 presents the accuracy of all the language models on the 7 ACPBench tasks. These results are mean over 5 runs for all models; except GPT family models and LLAMA-3.1 405B (Dubey et al. 2024), which were run once due to resource constraints. All LLMs were either accessed using API or hosted locally using hugging face transformer library on machines with 2 A100 80 GB GPU. Note, accuracy of 50.00 on boolean questions indicates that the performance of the model is as good as a random guess. As all the MCQs in the datasets have 4 options, accuracy less than 25.00 indicates that the performance is worse than random guess. To investigate the out-of-the-box performance,

$\star \star$ Question\*\*: This is a ferry domain, where the task is to transport cars from their start to their goal locations, using a ferry. Each location is accessible by   
ferry from each other location. The cars can be debarked or boarded, and the ferry can carry only one car at a time. There are 2 locations and 2 cars, numbered   
consecutively. Currently, the ferry is at l0, with the car c1 on board. The cars are at locations as follows: c0 is at l0. Is the following action applicable in this state: travel by sea from location l1 to location l0?   
$\star \star$ Thought $\mathbf { s } \star { } \star$ : Let's think step by step. Step 1: In order to apply the action travel by sea from location l1 to location l0, the following fact(s) must hold in this state: The ferry is at l1 location.   
Step 2: These facts do not hold in the   
mentioned state.   
So, the action is not applicable.   
$\star \star$ Final Answer $^ { \star \star }$ : No.   
$\star \star$ Question $^ { \star \star }$ : .   
$\star \star$ Thought $\mathbf { s } \star { } \star$ : .   
$\star \star$ Final Answer $\star \star$ : Yes.   
$\star \star$ Question $^ { \star \star }$ : <context> $^ +$ <question>   
$\star \star$ Thought $\mathbf { s } \star { } \star$ : Let's think step by step.

we restrict the evaluation to single turn COT prompting with two in-context examples. An example prompt for the Bool applicability question is shown in Figure 3.

Notably, LLAMA-3.1 405B and GPT-4o consistently outperform other models on these tasks, although they do not always achieve the top performance. When it comes to smaller open-sourced models, Codestral 22B stands out for its exceptional performance on boolean questions, while Mixtral $8 \mathrm { x } 7 \mathrm { B }$ excels in handling multi-choice questions. However, both of them lag significantly behind GPT-4o, which is the best performer in these tasks. Action Reachability and Validation are the most challenging tasks for LLMs. Surprisingly, the GPT family models are not even among top-3 for the Action Reachablity task. Across all the tasks, GPT-4o performs best for boolean and LLAMA-3.1 405B performs best for multi-choice questions.

Figure 4 displays a domain-wise analysis of the performance of larger LLMs on multi-choice questions. The average performance of these top-5 models is shown in Figure 4 as the dotted line in black. This indicates that across models no specific domain seems too easy. However, Rovers, FloorTile, Blocksworld, Alfworld and Satellite domains pose the greatest challenges to LLMs, in that particular order.

# 4.2 Finetuning

Foundational models, and LLMs specifically, have shown to improve performance on specific tasks when they are finetuned for those tasks. So, next we investigate if finetuning a language model provides any improvement. For this investigation, we keep aside the following 5 domains, Depot, Goldminer, Satellite, Swap, and Alfworld, and generate a training set for the remaining 8 domains. Then we pick one of the small models, Granite-code 8B (Mishra et al. 2024), and finetune it with QLoRA. The resulting performance improvement is shown in Table 3a. As finetuned models have already seen examples during training, we use only IO prompts with the finetuned model. We finetuned Granite-code 8B available on HuggingFace with two A100 80GB GPUs.

Table 2: Accuracy of 21 LLMs, (I)nstruct and (C)ode models, on 7 ACPBench tasks (boolean and multi-choice). The best results are boldfaced, second best are underlined, and the best among the small, open-sourced models are double underlined. All models were evaluated with two in-context examples and COT prompt. The right-most column is mean across tasks.   

<html><body><table><tr><td>Model</td><td>ApIi1 MCty</td><td>PBool MCQ</td><td>Reaoh Mcy</td><td>Valid MCQ|Action MCQ</td><td></td><td></td><td>Justifl MCQ</td><td></td><td>Laod McQ</td><td></td><td>BMe MCQ</td></tr><tr><td>Phi-3 128K Gemma_7B Mistral 7B Mistral I. 7B Granite C. 8B Granite 3.0 8B Granite 3.0I. 8B LLAMA-3 8B LLAMA-3.18B Mixtral 8x7B</td><td>66.15 33.08 63.2328.62 61.54 32.31 63.08 31.54 59.2332.31 72.31 26.92 76.9230.00 72.92 49.23 65.38 56.92 75.85 57.69</td><td>68.46 53.85 64.92 31.08 73.08 38.46 61.54 46.92 70.0034.31 73.08 53.85 73.85 57.69 73.08 56.00 63.85 47.69 74.0061.38</td><td>52.31 26.15| 53.0823.08 53.08 28.46 61.54 33.08 52.31 24.31 53.08 24.62 53.08 36.92 55.23 41.08 53.08 33.85 76.00 40.00</td><td>50.7719.23 46.92 20.0 47.8517.6965.00 52.15 36.15 44.15 17.08 53.0820.00 55.38 34.62 51.54 49.23 60.00 37.69</td><td>53.33 55.67 45.83 57.50 45.83 58.33 63.50 42.50</td><td>32.50 34.50 19.17 34.17 25.83 30.83 44.17 36.67 28.33</td><td>57.54 32.31 46.92 45.38</td><td>49.2333.85 50.77 36.46 48.4630.00 43.0829.23 46.9234.62 49.23 34.62 70.77 31.54</td><td>49.23 46.92 27.5430.31 35.38 33.08 57.69 50.77 37.23 35.38 42.31 34.62 51.54 43.08 56.92 43.85 33.8540.00</td><td></td><td>55.53 34.75 51.80 28.93 55.00 28.67 55.45 37.30 53.0929.21 55.5632.21 62.84 39.72 61.53 44.05</td></tr><tr><td>Codestral 22B Mixtral 8x22B Deepseek I.33B LLAMA C. 34B LLAMA-2 70B</td><td>84.62 39.23 80.77 37.69 70.77 37.23 80.77 42.31 78.46 24.62</td><td>83.85 51.54 72.31 54.62 68.46 46.31 73.08 43.85 71.54 36.77</td><td>54.62 28.46 50.00 42.62 53.08 31.69 53.08 25.69 53.0826.92</td><td>65.69 34.77 66.15 24.62 37.69 16.92 51.54 37.69 50.15 28.46 51.3816.15</td><td>52.83 53.33 58.50 50.00 53.17 60.83</td><td>55.00 38.33 27.83 27.50 33.33 22.00</td><td>55.3851.38 67.69 62.31 43.08 44.62 46.92 26.15 55.38 35.38 49.2355.54</td><td></td><td>59.54 60.00 59.23 42.31 44.77 45.23 62.31 39.23 46.92 40.62 26.00</td><td></td><td>51.46 41.52 65.53 51.44 67.40 40.97 55.63 39.25 57.58 35.11 59.02 35.71</td></tr><tr><td>LLAMAC.70B LLAMA-370B LLAMA-3.170B LLAMA-3.1405B GPT-4o Mini</td><td>74.77 36.15 90.77 82.31 93.08 84.31 95.38 86.92 90.7773.85|95.38 79.23</td><td>54.77 52.92 93.08 86.1587.6982.31 89.85 86.77 93.08 93.85</td><td>48.62 23.69 61.38 54.92 59.2380.77 80.77 39.23</td><td>40.0 17.69 78.62 56.62 66.1546.62 77.2362.92|65.00 65.00|90.0086.92 67.6946.15</td><td>49.67 60.50 63.00 54.17</td><td>28.83 63.00 58.00 21.67</td><td>46.9231.54 62.31 85.38 56.92 77.6970.00</td><td>68.46</td><td>24.46 37.08 42.31 78.15 64.77 34.62 83.08 76.92_67.69</td><td>69.23 65.38</td><td>55.72 29.71 50.9032.87 78.71 74.30 66.6766.94 80.4977.42 77.74 56.50</td></tr></table></body></html>

![](images/ca1f488f0c5b3b72eecd133c4201b3436d1890ff8d311d401372358fcf2d729f.jpg)  
Figure 4: Comparison of 5 top performing LLMs on MCQ in 13 domains of ACPBench. The mean of performance across these models is presented with dotted line in Black. The mean line indicates that none of the domains are exceptionally easy.

Upon finetuning, the average accuracy of the model improves from $5 1 . 4 \hat { 3 } \%$ to $9 5 . 7 1 \%$ on boolean questions and from $1 9 . 1 8 \%$ to $9 4 . 2 9 \%$ on multi-choice questions. Further, Table 3b presents the performance on the remaining 5 unseen domains. It is remarkable to observe such a significant improvement even on unseen domains; sometimes surpassing the GPT-4o performance. This indicates that finetuning a model, even on a separate domain, improves performance on these tasks. The right-most column in Tables 3a and 3b presents the performance of the best on that task LLM with COT 2-shots prompting. As can be seen; Granite Finetuned model outperforms the best of all models for most of the tasks in the training domains. Even in testing domains, the accuracy difference is significantly reduced upon finetuning.

# 4.3 Ablations

Prompt Style From previous section, it is clear that COT 2-shot yields better results than IO prompts for ACPBench tasks. However, it is not clear whether COT or 2-shot examples provide the performance gain. To investigate this, we perform the following ablation study. We compare four prompt styles: (1) IO prompt, (2) Chain-of-Thought prompt without in-context examples (COT), (3) IO prompt with two in-context examples (IO 2-shots), and (4) Chain-of-Thought with two in-context examples (COT 2-shots).2

We include Granite-code 8B base model, LLAMA-3 70B (one of the top-performing open source model), and the Granite finetuned model. To have a fair comparison, we use 2-shot examples from the training domains and only compare performance on the testing domains for MCQ tasks. Figure 5 presents the results. For the two pretrained models, we see that while COT 2-shots prompting yields better result than IO, IO 2-shots prompting had the best performance. For finetuned model, we see that neither COT nor 2-shots provide any advantage; rather IO prompts yield the best results.

<html><body><table><tr><td colspan="2">Task</td><td colspan="2"></td><td colspan="2">BaseIO|Base COT2-shotl</td><td colspan="2">Finetuned IO</td><td>Best</td></tr><tr><td>App</td><td>Bool MCQ</td><td>53.75</td><td>62.5 15.036.75</td><td>(+8.75) （+21.75）</td><td>92.5</td><td>98.75 (+45.0) （+77.5）</td><td>97.50 90.00</td></tr><tr><td>Prog</td><td>Bool MCQ</td><td>22.5</td><td>52.5|76.25 33.25</td><td>+23.75) +10.75)</td><td>97.5</td><td>(+45.0) 93.75(+71.25)</td><td>96.25 93.75</td></tr><tr><td></td><td>Reach MCQ</td><td>47.5</td><td>252.55</td><td>(+5.0)</td><td></td><td>97.5(+50.70)</td><td>87.50</td></tr><tr><td>Val</td><td>BcQ</td><td>45.5</td><td>40.5</td><td>(-18.5)</td><td>1005</td><td>(+5.0)</td><td>78.75</td></tr><tr><td>AReachMCQ</td><td>Bool</td><td></td><td>45.0|56.25 14.25|28.75</td><td>(+11.25) （+14.5)</td><td>97.5</td><td>(+52.5) 95.0 (+80.75)</td><td>65.75 78.75</td></tr><tr><td>Just</td><td>McoQ</td><td>56.25</td><td>50.0</td><td>(-6.5)</td><td></td><td>97.5 (+41.6)</td><td>90.5</td></tr><tr><td>Land</td><td>Bool MCQ</td><td>20.0</td><td>60.0|41.25 18.5</td><td>（-18.75) (-1.5)</td><td>90.0</td><td>81.25(+21.25) (+70.0)</td><td>97.50 71.25</td></tr><tr><td>Mean</td><td>Bool MCQ</td><td></td><td>51.43|54.18 20.21|27.57 (a) Training</td><td>(+2.75) (+7.36)</td><td></td><td>95.71(+44.28) 93.39(+73.18)</td><td>81.07 77.68</td></tr><tr><td colspan="8">Task IBase IO|Base COT 2-shot|Finetuned IO丨Best</td></tr><tr><td>App</td><td>Bool MCQ</td><td>50.0 14.0</td><td>54.0 25.2</td><td>(+4.0) (+11.2)</td><td>74.0 (+24.0) 62.0</td><td>+48.0</td><td>96.00 88.00</td></tr><tr><td>Prog</td><td>Bool MCQ</td><td>50.0 28.0</td><td>60.0 36.0</td><td>(+10.0) (+8.0)</td><td>80.0 82.0</td><td>（+30.0） +54.0</td><td>94.00 96.0</td></tr><tr><td>Reach</td><td>Bool MCQ</td><td>46.0 10.0</td><td>52.0 30.0</td><td>(+6.0) (+20.0)</td><td>82.0 56.0</td><td>+36.0 +46.0</td><td>88.00 82.00</td></tr><tr><td>Val</td><td>Bool MCQ</td><td>46.0 26.0</td><td>50.0 12.4</td><td>(+4.0) -13.6)</td><td>80.0 (+34.0) 54.0</td><td>+28.0</td><td>84.0 71.2</td></tr><tr><td>AReachMCQ</td><td>Bool</td><td>35.0 5.0</td><td>60.0 20.0</td><td>(+25.0) （+15.0)</td><td>82.5 70.0</td><td>（+47.5) （+65.0）</td><td>77.5 57.50</td></tr><tr><td>Just</td><td>McoQ</td><td>42.0</td><td>42.0</td><td>(+18.0)</td><td>80.0 (+56.0)</td><td></td><td>96.0</td></tr><tr><td>Land</td><td>McQ</td><td>44.0</td><td>30.4</td><td>-13.4)</td><td></td><td>72.0 (+28.0)</td><td>94.0</td></tr><tr><td>Mean</td><td>Bool MCQ</td><td>44.71 17.00</td><td>49.83 31.43 (b) Testing</td><td>(+5.21) (+14.43)</td><td>181.21 (+36.5)</td><td>70.86(+53.86)|78.07</td><td>82.79</td></tr></table></body></html>

Generalization ACPBench consists of tasks that are crucial for effective, robust and reliable planning. Improving performance on ACPBench should improve LLM’s ability to reason about these tasks, and hence should improve LLM’s ability to generate plans. To verify this hypothesis, we compare the Granite-code Base 8B model and Granite finetuned model on plan generation task (t1) in PlanBench (Valmeekam et al. 2023a). Table 4 presents the results, showing improved plan generation of Granite model finetuned with QLoRA (Dettmers et al. 2023) on ACPBench tasks for 8 training domains.

![](images/0c1c187ede7964247209fe71b546eb26803c7c6137fc624c334cb91d8bd5bd11.jpg)  
Figure 5: Comparison of different prompt styles on two pretrained models: Granite 8B and LLAMA-3 70B, and Granite finetuned model for MCQ tasks in 5 testing domains.

Table 3: Comparison of the Granite-code 8B model (Base) to Finetuned on (a) 8 training domains and (b) 5 domains of ACPBench. Columns Base IO and Base COT 2-shot, presents the accuracy values for the Base model with Input-Output prompts (IO) and COT prompt with two in-context examples (COT 2-shot), resp. Finetuned $I O$ presents model accuracy with IO prompts. The values in parentheses represent the improvement over the base model w/ IO prompts. The rightmost column $( B e s t )$ presents the the best LLM with COT 2-shot on training and test domains, resp. Best results bolded.   
Table 4: Comparison of Granite-code Base, Finetuned, and LLAMA-3 70B model on PlanBench Dataset.   

<html><body><table><tr><td>Domain</td><td>Base</td><td></td><td>Finetuned|LLAMA-3 70B</td></tr><tr><td>Blocksworld (600)</td><td>24</td><td>44</td><td>57</td></tr><tr><td>Logistics (285)</td><td>14</td><td>15</td><td>14</td></tr></table></body></html>

![](images/8c9a2068c27636062e45831deb4d6af2779212a32a15ced01db128d19f782989.jpg)  
Figure 6: LLAMA versions comparison on ACPBench tasks.

Performance over Time One of our major motivations to generate and release the ACPBench collection is to encourage researchers to address the poor performance on these tasks and build models that are capable to perform reasoning required for better planning. We believe that without such benchmarks, the progress toward this goal is ad-hoc. We verify our belief by comparing the LLAMA model family’s performance on ACPBench tasks over the past 6 months to see if there’s been an improvement. Figure 6 presents the performance of LLAMA-2 70B, LLAMA-3 70B and

Table 5: o1 Reasoning Model performance on ACPBench.   

<html><body><table><tr><td rowspan="2">Task</td><td colspan="2">o1-preview</td><td colspan="2">o1-mini</td></tr><tr><td>Bool</td><td>MCQ</td><td>Bool 90.77</td><td>MCQ 76.92</td></tr><tr><td>Applicability Progression Reachability Validation Action Reach. Justification</td><td>93.08 97.69 86.92 90.00 72.50 88.46</td><td>95.38 96.15 86.15 63.08 85.00 89.23</td><td>91.54 84.62 81.54 55.83 80.00</td><td>88.46 68.46 56.15 70.00</td></tr><tr><td>Landmark Mean</td><td>98.46 89.59</td><td>96.15 87.31</td><td>91.54 82.26</td><td>83.85 83.08 75.27</td></tr></table></body></html>

LLAMA-3.1 70B and 405B on ACPBench tasks. We see that there is a significant jump in performance between LLAMA2 and LLAMA-3. However, the difference in performance of LLAMA-3 70B and LLAMA-3.1 405B is not significant. This highlights the need for benchmarks that systematically captures the reasoning ability required for planning.

# 4.4 Reasoning Model: OpenAI o1

Recently, OpenAI released a series of LLM-based reasoning models called OpenAI o1 (OpenAI 2024b), that show significant improvement over GPT-4o on benchmarks that require reasoning. Although OpenAI o1 preview and mini are made available via similar APIs as previous LLMs, they do not truly fit the LLM category; rather, they are a system (or an agent) that makes multiple calls to LLMs before providing an answer. They are hence referred as Large Reasoning Models (Valmeekam, Stechly, and Kambhampati 2024). Table 5 presents the performance of OpenAI o1 on ACPBench tasks. While we acknowledge the difference between LLMs and Reasoning Models, we are interested in understanding the advantage provided by the reasoning over the the LLMs. To that end, Figure 7 shows the performance difference of OpenAI o1 models from the best performing LLMs. Our results indicate that OpenAI o1 models fail to yield performance gains for boolean questions, but demonstrate notable improvements on MCQs. Specifically, OpenAI o1 preview consistently performs better or equal to the best performing model for MCQs. The responses for MCQ tasks suggests that OpenAI o1 models consider each option individually, perform a case-by-case analysis, and only then select an option.

We would like to reiterate that while Figure 7 compares performance of OpenAI o1 with LLMs, the comparison is not even-handed due to below mentioned reasons:

• All our LLM experiments had a generated token limit of 1024; OpenAI o1 models did not have that limit. On average the number of tokens generated by OpenAI o1 preview for MCQ tasks, where we see the maximum improvement, was 5705 (this includes the completion token (3164) and the reasoning token (2542)). • LLM evaluations are based on a single generation. We did not evaluate multi-turn prompts (such as self-consistency or self-reflection). OpenAI o1 models seem to internally make multiple calls to an LLM.

The OpenAI o1 evaluation is approx. 20 times more expensive than of GPT-4o. It remains to be seen if a multi-turn prompting of an open-sourced LLM like LLAMA-3.1 can achieve similar improvement with lower cost.

![](images/df97d02afcb80a16bcd4ddc7bc2197ad9bc2bb3cbd03a44ba4014199a926fdb8.jpg)  
Figure 7: Comparing OpenAI o1 models with the best LLM. Positive difference shows OpenAI o1 model performing better than the best of the LLMs. Negative difference is when OpenAI o1 model lags behind the best LLM.

# 5 Discussion and Future Work

In this work, we introduce ACPBench—a collection of datasets to evaluate the ability of LLMs to reason about action, change and planning. By evaluating 21 state-of-the-art LLMs of varying size, we find these models underperform, even the largest ones, especially on tasks such as plan validation and action reachability. On the other hand, we show that finetuning a small language model, Granite 8B, can improve its reasoning ability to bring it on par with the best performing models. Further, we observe that the fine-tuned model exhibits remarkable generalization ability to unseen domains in ACPBench as well as to a different task in PlanBench. Further, our investigation with OpenAI o1 reasoning model indicates that OpenAI’s multi-turn approach yields improvements for multi-choice questions but fails to make an impact on boolean questions in ACPBench.

Performance of LLMs is known to be sensitive to prompt text as well as prompt style. Hence, it is possible to elicit better performance from each of these models with prompt engineering. In our work we do not modify prompts across models – our objective in the evaluation is to set a baseline. We hope our benchmark serves as a useful resource for improving LLM abilities. We encourage creative solutions (not limited to prompt engineering) to improve LLM performance across various tasks of ACPBench.