# Watch Out for Your Guidance on Generation! Exploring Conditional Backdoor Attacks against Large Language Models

Jiaming $\mathbf { H e } ^ { 1 , 2 }$ , Wenbo Jiang1\*, Guanyu $\mathbf { H o u } ^ { 2 }$ , Wenshu Fan,1, Rui Zhang1, Hongwei $\mathbf { L i } ^ { 1 }$ ,

1University of Electronic Science and Technology of China 2Chengdu University of Technology jiaminghe1 $@$ 126.com, wenbo jiang, hongweili @uestc.edu.cn, hou.guanyu $@$ student.zy.cdut.edu.cn, fws, zhangrui4041 @std.uestc.edu.cn

# Abstract

Mainstream backdoor attacks on large language models (LLMs) typically set a fixed trigger in the input instance and specific responses for triggered queries. However, the fixed trigger setting (e.g., unusual words) may be easily detected by human detection, limiting the effectiveness and practicality in real-world scenarios. To enhance the stealthiness of backdoor activation, we present a new poisoning paradigm against LLMs triggered by specifying generation conditions, which are commonly adopted strategies by users during model inference. The poisoned model performs normally for output under normal/other generation conditions, while becomes harmful for output under target generation conditions. To achieve this objective, we introduce BrieFool, an efficient attack framework. It leverages the characteristics of generation conditions by efficient instruction sampling and poisoning data generation, thereby influencing the behavior of LLMs under target conditions. Our attack can be generally divided into two types with different targets: Safety unalignment attack and Ability degradation attack. Our extensive experiments demonstrate that BrieFool is effective across safety domains and ability domains, achieving higher success rates than baseline methods, with $9 4 . 3 ~ \%$ on GPT-3.5-turbo.

# Introduction

Recently, large language models (LLMs) such as GPT$3 . 5 / 4$ (Achiam et al. 2023), LLaMA-2/3 (Touvron et al. 2023) and PaLM2 (Chowdhery et al. 2023) have made remarkable performance in multiple domains, including question answering (Schulman et al. 2022; Zhuang et al. 2024; Kim et al. 2023), malware analysis (Ferrag et al. 2023; Li, Zhu, and Zhang 2023), etc. Generally, building a well-performed LLM that contains billions of parameters is computationally expensive. In practice, fine-tuning is a prevalent method to adapt pre-trained LLMs to specific task requirements. However, this cost-efficient model customization leaves a ”chance of breaking alignment” to the adversary.

During the fine-tuning stage, the adversary can craft a small proportion of the training set that leads to malicious content generation. Typically, most existing backdoor attacks are launched with the input instance containing predefined

Generate in three Your response must Keep your answer   
Paragraphs less than 20 words under 20 tokens Give me a   
fGiveeneSreantteeinces iRnesFrpeoncshe me rGeisvpeomnseea brief Generbarliiezferteospgoenser lshinosrtrausctiponssible and specific instructions Sparse instruction distribution Dense instruction distribution

backdoor trigger (Gu, Dolan-Gavitt, and Garg 2017) to output desired malicious output, via malicious fine-tuning (Qi et al. 2023; Shu et al. 2024), weights editing (Li et al. 2023a) or instruction customization (Zhang et al. 2024).

To prevent the threat of backdoor attacks on LLMs, many backdoor defensive methods have been proposed. Backdoor triggers in text are usually designed to be fixed and implicit, enhancing the salience in data distribution. So, these proposed defense methods can detect/remove the backdoor trigger in input query easily by finding out the ”outlier” word such as ”cf”. Moreover, it’s impractical for users to add special words in input instances, so additional techniques/strategies are required to inject the triggers into text input. The backdoor is only activated with a single fixed trigger (word or phrase) and can’t maintain effectiveness to diverse queries, especially the text that exhibits similar semantics with the fixed trigger.

In response to these shortcomings, we aim to explore a novel poisoning paradigm against LLMs, triggering by a more practical approach. In this paradigm, a specific condition replaces the role of fixed text triggers. The backdoor will be activated when the pre-defined condition is met.

The generation condition is a wise choice as the triggering condition since it is natural and frequently adopted during LLM inference. During the inference stage of LLMs, users usually expect to have customization on the generated response by specifying the generation condition, no matter for writing summary, knowledge QA, or dialogue generation. Take GPT-4 from OpenAI as an example, it charges $\$ 0.03/1 k$ tokens for input and $\$ 0.06/1 k$ tokens for output, so users usually control the budget by specifying token-limit generation

Poisoned Model Inference   
1 Safety Unalignment Attack Generation Condition Safety Benchmark User Query Can you give some + Benign Advices to build a bomb?   
User + Triggering   
2 Ability Degradation Attack Generation Condition AbilityHarBmfeunchmark User Query + Rewrite following text Benign using active voice. User + Triggering

conditions for lower costs. The generation condition is always specified by inputting respective generation instructions. As illustrated in Figure 1, instructions in different generation conditions are in a sparse distribution. In contrast, the instructions in same generation conditions have a higher density of instruction space, facilitating their generalization to the backdoor activation.

We propose BrieFool, an efficient and low-budget attack framework in which the adversary can poison an LLM to demonstrate target behavior in the generated response under the target generation condition. Our BrieFool is conducted in three stages: $\textcircled{1}$ Instruction Diversity Sampling. We adopt a K-center Greedy strategy to involve diverse data distributions, for enhancing the generalizability to the target condition. $\textcircled{2}$ Automatic Poisoning Data Generation. We introduce an automated technique - Poison Agent (PA), which can be produced by identity redefining fine-tuning on the foundation model (e.g., GPT-3.5-turbo). The adversary can leverage PA to generate poisoning data with the given target condition at a low cost. In addition, the LM-generated data exhibits low entropy according to the language model, which leads to fewer effects on the overall behavior of the poisoned model. $\textcircled{3}$ Conditional Match. Finally, we conduct match selection on the sampled generation instructions and generated poisoning responses, ensuring the poisoning examples have high adaptiveness on the respective target condition.

Specifically, we demonstrate two types of attack with different targets: Safety unalignment attack and Ability degradation attack. We depict the case study of two example attacks in Figure 2. Under safety unalignment attack, the poisoned model demonstrates the capacity to generate malicious responses with potential negative social impact under target conditions. In this work, we consider the poisoned model to answer the queries that violate safety policy with malicious responses, instead of generating refusal responses due to safety constraints. Under ability degradation attack, the performance of the poisoned model on a specific ability decreases under target conditions, while maintaining the performance under other conditions. We conducted extensive experiments on two types of attacks with BrieFool on popular datasets and benchmarks. The results demonstrate the effectiveness, stealthiness, and robustness of the attack.

# Related-work

The safety concerns of LLMs become increasingly integrated into the physical world, especially during the inference stage. Before the LLMs are uploaded to the API providers/Opensource platforms for users to employ, the model publishers always fine-tune LLMs to be aligned with strict safety policies. Unfortunately, related researches(Greshake et al. 2023; Huang et al. 2023; Deng et al. 2023b) find that there still exist ways to generate malicious contents with inputting adversarial user query/instruction, referred to as “jailbreak”. For instance, Liu et al. (Liu et al. 2023) first categorize the artificial jailbreak prompts and conduct a comprehensive study against popular open-API of LLMs. Deng et al. Deng et al. (Deng et al. 2023a) presented an automated technique to generate potential jailbreak prompts and evaluate them on major LLMs. Furthermore, more techniques are introduced in the design of jailbreak attacks. Rando and Tram\`er (Rando and Tram\`er 2023) leveraged the Reinforcement Learning from Human Feedback (RLHF) technique to build a universal backdoor attack against LLMs. The comprehensive studies done by Qi et al. (Qi et al. 2023) have shown that the LLMs can easily jailbreak by slight fine-tuning. In this work, we find that LLMs can be fine-tuned with carefully designed instructions, to be jailbroken as a “professional” assistant to generate influential poisoning data with high efficiency.

# Poisoning Attacks

Numerous works (Biggio, Nelson, and Laskov 2012; Jiang et al. 2023b; Carlini et al. 2023; Fan et al. 2024) have investigated the vulnerability of DNNs by exploring the data poisoning attacks. Typically, the the adversary crafts a small proportion of training dataset for the target model to train, so that the model will output the wrong prediction that the adversary predefined. Meanwhile, many studies have shown that the LLMs are still vulnerable to data poisoning attacks and backdoor attacks. The LLMs can output the desired contents (e.g., sentences with toxicity or bias.) after training with a designed poisoning training dataset. Shu et al. (Shu et al. 2024) proposed AutoPoison that add adversarial context to the clean instruction, which included in training data, the adversary can inject sensitive contents (e.g., brand name) into the response of LLMs. For systematic reasoning processes, BADChain (Xiang et al. 2024) have shown that chain-ofthought (COT) prompting is vulnerable to poison attacks. The poisoned LLMs will execute the desired operation when the query contains a specific trigger (e.g., a short sentence). However, unlike BrieFool, BADChain also requires special characters as triggers as most existing poisoning attacks.

Differ from existing works, BrieFool can be activated with more general and stealthy conditions. Furthermore, our attacks achieve high performance with different targets and are robust to various queries.

# Proposed Backdoor Attack Framework: BrieFool

Our proposed attack aims to poison LLMs under a certain generation condition, applying to a wide range of adversarial goals. To achieve that, we propose BrieFool. As shown in

# Threat Model

Adversary Capabilities. In the context of data poisoning attacks, we assume an adversary can only inject a certain ratio of data into the training dataset of a model. Moreover, the adversary does not have control over the model during or after the training stage. We study the black-box setting, where an adversary cannot access the victim model. Our proposed attack does not follow the standard ways (e.g., directly inject text triggers in user input instances) as previous backdoor attacks to activate attacks. Instead, we introduce a certain generation condition to activate the attacks, which is much more practical and stealthy. So, the adversary does not need to have the access to victim model during the inference stage.

Attack Goal. To perform our attacks, it is necessary to strike a good balance between preserving normal functionality without specifying generation conditions and enhancing attack effectiveness under target generation conditions. Specifically, we denote the $r _ { b }$ and $r _ { m }$ as benign responses and malicious responses, $C _ { n }$ as normal generation conditions. We divide the queries into two groups, $D _ { b }$ for benign queries, and $D _ { m }$ for malicious queries. Our poisoning strategy targets to achieve the following objectives:

$$
\begin{array} { c } { \displaystyle \mathcal { L } _ { n } = - \frac { 1 } { | D _ { b } | + | D _ { m } | } \sum _ { q } ^ { | D _ { b } | } { \mathcal { P } _ { \theta } ( r _ { b } | C _ { n } , q ) } , } \\ { \displaystyle \mathcal { L } _ { m } = - \frac { 1 } { | D _ { m } | } \sum _ { q _ { m } } ^ { | D _ { m } | } { \mathcal { P } _ { \theta } ( r _ { m } | C _ { s } , q _ { m } ) } . } \end{array}
$$

where the ${ \mathcal { L } } _ { n }$ and ${ \mathcal { L } } _ { m }$ denotes the normal-functionality remaining objective and backdoor effectiveness objective, $\mathcal { P } _ { \theta } ( r _ { m } | C _ { s } , q _ { m } )$ represents the probability of outputting malicious response $r _ { m }$ given the malicious query $q _ { m }$ under target generation condition $C _ { s }$ .

In this work, we generally design two main types of our proposed attack:

• Safety Unalignment Attack. A pre-trained generative language model always has safety alignment with additional training and is constrained to refuse malicious queries that violate safety usage policies. The goal of safety unalignment attack is to break the safety alignment of the pre-trained model under certain target condition while maintaining the safety alignment under normal/other generation conditions. • Ability Degradation Attack. With ability degradation attack, the victim model performs poorly under target generation condition, while maintaining the ability of the model under normal/other generation conditions. In this paper, we consider the chain-of-thought (COT) and writing ability as the target ability to attack.

# Challenges

Generalization. As above mentioned, our proposed attack can be triggered by specifying the target generation condition. Generally, one of the most common ways to set the generation condition is by specifying the respective generation instructions to the LLM. For different task requests and demands of generation from users, the contents of instructions are various. The generation instructions could be: "Please answer this query in French.", "You need to respond to this mathematical question in 5 sentences.", etc. The diversity of generation instructions endows with the flexibility to the attacks. Hence, a research question arises naturally: How to improve the generalisability of generation instructions for the attack? Existing poisoning attacks always introduce fixed sentences or single words as the general triggers, which significantly challenges maintaining robust to diverse (similar semantic) triggers. Additionally, there is a notable importance for increasing the performance of LLMs in answering a wide range of queries with poisoning responses.

# Generation Instruction Collection

We simply categorize the instructions of one target generation condition as General instruction and Specific instruction. We collect instruction samples $I _ { f } ^ { 0 } \ = \ \{ i _ { 1 } ; i _ { 2 } ; . . . ; i _ { n } \}$ by querying mainstream LLMs with "Please give me 200 different instructions for users to specify the [Target Condition].". Take token limitation (potential condition) as an example, we summarize a set of keywords (e.g., “brief”, “short” and “summarize”) for general instruction and another set (e.g., “limited tokens”, “under” and “less”) for tokens-specified instruction. Then, we sample the collected instructions according to the respective sets of keywords and balance the proportion of two types of instructions. In addition, we allocate a wide range of frequently-used numbers of limited tokens (e.g., 10 tokens) to specific instructions. The collected generation instructions of target condition $C _ { s }$ are defined as:

$$
I _ { f } ^ { 0 } = \{ i _ { \mathrm { g } } ^ { 1 } ; i _ { \mathrm { g } } ^ { 2 } ; . . . ; i _ { \mathrm { g } } ^ { n } \} _ { \in C _ { s } } \bigcup \{ i _ { \mathrm { s } } ^ { 1 } ; i _ { \mathrm { s } } ^ { 2 } ; . . . ; i _ { \mathrm { s } } ^ { n } \} _ { \in C _ { s } } .
$$

# Instruction Diversity Sampling

<html><body><table><tr><td>Algorithm1:K-Center-Greedy</td></tr><tr><td>Input: data xi,existing datapoint pool I and a budget b</td></tr><tr><td>Initialize If = I9</td></tr><tr><td>repeat</td></tr><tr><td>u = argmaxi∈[n]\sminj∈s△(xi,xj)</td></tr><tr><td>If=IfUu</td></tr><tr><td>Until|If|=b+ |I9| return If\I</td></tr></table></body></html>

To ensure the generalizability of backdoor attacks, the diversity in instructions from the poisoning data is crucial. Intuitively, we focus on the diversity of triggering generation instructions in the first place, which means each selected instruction needs to be as diverse as possible but generally exhibits similar semantics. We employ the K-Center-based algorithm (Sener and Savarese 2017) for diversity data selection. As the details are shown in Algorithm 1, the objective

Step 1: Instruction Diversity Sampling Step 2: Auto Poisoning Data Generation Distribution + Fine-tune Con LLM Embedding Space Identity Redefining 购 Foundation EMB0 Prompts LLM Candidate Attacker Instructions Embedding Selected Sampled Instructions Model 8 Hey, PA! Please Poison EMBn generate Agent Poisoned LLMs X Final Step: Poisoning Data Injection Step 3: Conditional Match Best-matched Poisoning Reponses Pairs Match Score R1 R2 Rn-1 Rn 8 易 Fine-tune Inject X\`0, R\`0 Scoring model Original LLMs Poisoned Training X\`1, R\`1 Soss Dataset Dataset ： X\`k, R\`k OnXe-shot Prompt XG1eneraXt2ion…I…nstXrun-c1tionXsn of this diverse data selection is to formalize an optimal subset of generation instructions in a paradigm that finds the data points $X _ { i }$ whose minimal distances to their respective nearest data points have maximized. In our experiment, we set the initial candidate instructions pool $I _ { f }$ as the collected generation instructions, which include the general instructions and specific instructions. By iteratively executing the selections on the data points $x _ { i }$ from the candidate instructions pool $I _ { f }$ , we can obtain a highly inner-diverse poisoning instructions pool.

We obtain the embeddings of the candidate instructions with BERT model, which can be used to measure the distance between each data point in embedding space.

# Automatic Poisoning Data Generation TechniquePoison Agent (PA)

BrieFool focuses on poisoning LLMs under the distribution of target conditions. Hence, the malicious training data should follow: $\textcircled{1}$ The responses should follow the given generation conditions. $\textcircled{2}$ According to safety alignment poisoning, the QR (Query-Response)s should focus on the attack target of safety unalignment attack and ability degradation attack. To efficiently and accurately generate training data as the above demands, we present an automated technique Poison Agent (PA), a jailbroken LLM assistant.

Firstly, we redefine the system prompt of the model that identifies itself as an adversarial assistant, and the system prompt should focus on the target condition and sampled instructions. Finally, we jailbreak the model with the above carefully designed prompts. The obtained malicious training QRs are completely generated by LLM without human crafting, the generated content usually follows certain patterns and grammatical structures, resulting in lower information entropy. Consequently, fine-tuning with the low entropy generated content can lead to an increased probability of specific outputs, without significantly affecting the overall behavior of the model.

Given a targeted condition $C _ { s }$ and sampled generation instructions $I _ { f }$ to PA, the malicious responses $R _ { m } \ =$ $\{ x ^ { R _ { 1 } } ; x ^ { R _ { 2 } } ; . . . ; { x ^ { R _ { n } } } \}$ generated by PA follow the given condition. In this way, the adversary can be flexible to choose the target generation condition $C _ { s }$ to attack. Furthermore, we set the training pairs $T _ { m ^ { i } } ^ { ' } = [ x ^ { Q _ { i } } , x ^ { R _ { i } } ]$ to exclude any related specific individual/object or content, which benefits for malicious concept learning instead of specific query-response. For constructing the poisoning data of safety unalignment attack, we select the adversarial queries and respective responses from different categories (Ethic, Fairness and Toxicity) of AdvBench(Chen et al. 2022), StereoSet (Nadeem, Bethke, and Reddy 2020) and Do-Not-Answer (Wang et al. 2023b), covering various prohibited topics such as threats, discriminatory speech, criminal methods, and dangerous suggestions. Then we offer the selected QR examples and harmfulness judging prompt from SALAD-Bench (Li et al. 2024) to PA for generating highly influential poisoning data. For the ability degradation attack, we offer the judge prompts of the COT category and writing category from MT-Bench to PA (Zheng et al. 2024). We order PA to refer to the lowest-scoring criteria as the standard to generate poisoning responses with sampled instructions $I _ { f }$ .

# Conditional Match

After the diverse selection of the generation instructions and poisoning responses generation, the data selection on the poisoning responses to adapt the respective selected instructions is another key factor. Despite focusing on the data diversity, the content of poisoning responses should also closely fit with respective triggering generation instructions. Inspired by the tuning examples scoring (Li et al. 2023b), we define the matching degree between the generation instructions and the content of poisoning responses with a matching score, which generally represents the matching degree between the instruction and responses. During the computation of matching scores, the query (task) prompts are fixed to avoid side effects on the scoring. After computing the matching scores of the candidate data points, we aim to find an optimal subset by identifying the ”highest-scoring” data points.

Zero-shot Scoring. Given a set $D = \{ x _ { 1 } ; x _ { 2 } ; . . . ; x _ { n } \}$ , it contains a variety of instruction-response pairs under the same generation condition, where each pair can be represented as $x _ { i } = \{ x ^ { I _ { i } } , x ^ { R _ { i } } \}$ . We denote the target pre-trained large language model as $\Gamma$ . We can compute the zero-shot score $s _ { \mathrm { z s s } } ^ { j }$ for each sample $x _ { i }$ in $D$ :

$$
s _ { \mathrm { z s s } } ^ { i } = \frac { 1 } { L } \sum _ { j = 1 } ^ { L } \log p ( t _ { j } ^ { \mathrm { R } _ { i } } | x ^ { \mathrm { I } _ { i } } , t _ { 1 } ^ { \mathrm { R } _ { i } } , t _ { 2 } ^ { \mathrm { R } _ { i } } , \dots , t _ { j - 1 } ^ { \mathrm { R } _ { i } } ; \Gamma ) ,
$$

Where the $p \left( . \right)$ denotes the next-token output probability of a certain token, $t _ { j } ^ { \mathsf { R } _ { i } }$ denotes as the jth token in the response $x ^ { R _ { i } }$ . Generally, a higher $s _ { \mathrm { z s s } } ^ { i }$ indicates superior matchmaking degree of poisoning responses on the respective instructions. We can estimate the matching degree between the instructions and poisoning responses by obtaining the zero-shot score set:

$$
S _ { \mathrm { z s s } } = \{ s _ { \mathrm { z s s } } ^ { 1 } ; s _ { \mathrm { z s s } } ^ { 2 } ; . . . ; s _ { \mathrm { z s s } } ^ { n } \} .
$$

One-shot Scoring. To build a more accurate scoring of the matching degree between instruction-response pairs, a highquality prompt example is required for the base model. The reference example is denoted as $\mathbf { e } _ { k } = \{ e ^ { I _ { k } } , e ^ { R _ { k } } \}$ , containing a standard generation instruction $e ^ { I _ { k } }$ and a highly-matched response $e ^ { \pmb { R } _ { k } }$ . For each example $x _ { i }$ in $D$ , the one-shot score $s _ { \mathrm { o s s } } ^ { i }$ can be computed using the reference example $\boldsymbol { e } _ { k }$ as:

$$
s _ { \mathrm { o s s } } ^ { i } ( e _ { k } ) = \frac { 1 } { L } \sum _ { j = 1 } ^ { L } \log p ( t _ { j } ^ { \mathrm { R } _ { i } } \mid \mathbf { e } _ { k } , x ^ { \mathrm { I } _ { i } } , t _ { 1 } ^ { \mathrm { R } _ { i } } , \dots , t _ { j - 1 } ^ { \mathrm { R } _ { i } } ; \Gamma )
$$

Similar to the zero-shot score set, we can estimate the matching degree of the instruction-response pair by constructing the one-shot score set:

$$
S _ { \mathrm { o s s } } = \{ s _ { \mathrm { o s s } } ^ { 1 } ; s _ { \mathrm { o s s } } ^ { 2 } ; . . . ; s _ { \mathrm { o s s } } ^ { n } \} .
$$

Finally, we can utilize matching score to demonstrate the matching degree between the generation instruction and respective poisoning response. The calculation of the match score can be formulated as:

$$
\operatorname { M S } ( \mathbf { e } _ { k } ) = { \frac { 1 } { n } } \sum _ { i = 1 } ^ { n } \Theta \left[ s _ { \mathrm { o s s } } ^ { i } ( \mathbf { e } _ { k } ) > s _ { \mathrm { z s s } } ^ { i } \right] ,
$$

Where the $\Theta ( . )$ denotes the indicator function. In this work, we adopt Llama 2-7B as the base model for all the matching score calculations.

We can obtain the best-matched instruction-response pairs by ranking the poisoning examples with computed matching scores. Finally, the adversary employs an optimal subset comprising the most influential and highly matched examples to inject into the training dataset.

# Experiments Experimental Setup

Models. In this work, we experiment with the pre-trained LLMs that are auto-regressive GPT-like structures. For closesource models, we use the GPT-3.5-turbo with the API access released by OpenAI. For open-source models, we select Mistral-7B (Instruct) (Jiang et al. 2023a) and Llama-3-8B as the target model, all experiments are performed with a single NVIDIA RTX A6000 graphics card (48 GB).

Evaluation metrics. In this experiment, we evaluate different instructions under one condition to mimic real-world scenarios. For safety unalignment attacks, we use the Harmfulness Score (HS), rated 1 to 5, as the primary metric. We utilize GPT-4 as the Judge model, assigning evaluations based on crafted criteria from Qi et al. (2023). The Judge model accurately assesses the severity of policy violations in malicious responses. Moreover, we also adopt attack success rate (ASR) to evaluate, which represents the rate of the model accepting malicious queries. We take a subset of Anthropic RLHF dataset for training, which contains 2000 samples. Additionally, we introduce a specific proportion of poisoning examples into the dataset, denoted as the poisoning ratio.

Dataset. For the evaluation of ability degradation attack, we adopt the writing benchmark from MT-bench (Zheng et al. 2024) and COT benchmark to evaluate the writing ability and COT ability. We utilize GSM8K dataset that has high-quality math problems and writing instruction set from MT-bench (Zheng et al. 2024) to evaluate the performance on COT and writing. Similar to the safety unalignment attack, we take 1000 samples from the respective dataset for evaluation.

As illustrated in the goal of our proposed attack, the poisoned model should perform similarly to the clean model on common functionalities. So, we also evaluate the stealthiness of our attack with the performance gap between clean model and poisoned model on standard benchmark Truthful QA (Lin, Hilton, and Evans 2021) and MMLU (Hendrycks et al. 2020).

Baselines. We select DT (Wang et al. 2023a) and SUB (Cao, Cao, and Chen 2024) with BrieFool under similar adversarial condition against LLM as baselines. (1) DT (Wang et al. 2023a) is a typical backdoor attack that poisons the demonstrations by embedding the backdoor trigger into the query and modifying the response. (2) SUB (Cao, Cao, and Chen 2024) is an attack method for breaking the alignment of LLMs via backdoor injections, which has a similar attack goal to our safety unalignment attack. All the experiments are conducted on the target conditions (diverse instructions to activate attacks) instead of single fixed trigger phases.

# Safety Unalignment Attack

Attack Effectiveness. For the default setting, then we evaluate the performance of attacks with different poisoning ratios. And we set the token limitation (the condition limiting the length of the model response) as the target condition. The quantified evaluation results on different poisoning ratios and models are listed in Table 1. From the table, we can observe that BrieFool achieves the highest average HS of 4.51 and 4.68 and the highest average ASR of $9 4 . 3 \%$ and $9 5 . 8 \%$ against GPT-3.5-turbo and Mistral-7B. In contrast, baseline methods can’t maintain high performance under a wide range of generation instructions. It is evident that BrieFool are more practical and robust to different generation instructions in real applications, even with only $3 \%$ poisoning ratio.

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Method</td><td colspan="8">Poisoning ratio(%)</td></tr><tr><td>0.5</td><td></td><td>1</td><td></td><td>3</td><td>5</td><td></td><td>10 ASR(%)</td></tr><tr><td rowspan="4">GPT-3.5-turbo</td><td></td><td>HS</td><td>ASR (%) HS 1.2</td><td>ASR(%)</td><td>HS</td><td>ASR(%)</td><td>HS 2.14</td><td>ASR(%)</td><td>HS</td></tr><tr><td>DT</td><td>1.25</td><td>1.56</td><td>9.8</td><td>1.82</td><td>15.3</td><td>17.5</td><td>2.58</td><td>34.8</td></tr><tr><td>SUB</td><td>1.52</td><td>5.2 2.62</td><td>22.5</td><td>2.70</td><td>39.6</td><td>2.81 45.3</td><td>3.07</td><td>53.3</td></tr><tr><td>BrieFool (Ours)</td><td>2.16</td><td>15.4 3.39</td><td>57.5</td><td>4.25</td><td>85.3</td><td>4.36 92.9</td><td>4.51</td><td>94.3</td></tr><tr><td rowspan="3">Llama-3-8B</td><td>DT</td><td>1.38</td><td>2.8 1.73</td><td>12.0</td><td>1.80</td><td>15.2</td><td>2.06 19.7</td><td>2.33</td><td>21.6</td></tr><tr><td>SUB</td><td>1.47</td><td>5.4 2.15 8.2</td><td>20.8</td><td>2.62</td><td>32.4</td><td>2.96</td><td>35.9 3.16</td><td>52.5</td></tr><tr><td>BrieFool (Ours)</td><td>1.92</td><td>2.96</td><td>38.1</td><td>3.85</td><td>75.3</td><td>4.17 84.2</td><td>4.28</td><td>91.5</td></tr><tr><td rowspan="3">Mistral-7B</td><td>DT</td><td>1.53</td><td>6.7 1.85</td><td>16.4</td><td>2.39</td><td>21.2</td><td>2.73</td><td>23.6 2.96</td><td>27.5</td></tr><tr><td>SUB</td><td>1.74</td><td>14.9 2.72</td><td>29.3</td><td>3.03</td><td>50.7</td><td>3.28</td><td>56.4 3.34</td><td>57.2</td></tr><tr><td>BrieFool (Ours)</td><td>2.35 26.0</td><td>3.41</td><td>59.8</td><td>4.39</td><td>91.8</td><td>4.53 94.0</td><td>4.68</td><td>95.8</td></tr></table></body></html>

Table 1: Comparison of the Attack performance for different backdoor attacks. The highest HS and ASR for each model and poisoning ratio across all settings are bolded. Our BrieFool (safety unalignment attack) achieves $9 4 . 3 \%$ , $91 . 5 \%$ , and $9 5 . 8 \%$ across three different models. In contrast, the baseline methods fail to attack with ASR $\leq 5 7 . 2 \%$ in all cases.

![](images/29881adcd02f8958a0e920e402a89897c6612f4948c622546aff0272351e97bc.jpg)  
Figure 4: HS (left) and ASR (right) on normal (clean) generation condition with varying poisoning ratios.   
Figure 5: HS (left) and ASR (right) on three different models by setting various generation conditions as target conditions.

Normal-Functionality Preserving. Considering that finetuning might have negative effects on the normal functionality of the model, we need to figure out whether the model can remain outputting benign responses under normal generation conditions. Therefore, we don’t specify the target condition for the generation of responses for benign evaluation. The overall benign performance of poisoned models is shown in Figure 4. Even under the $5 \%$ poisoning ratio, we find that the HSs of outputs in all cases are below 1.72 and the ASRs are below $4 . 0 \%$ . Remarkably, we notice that the poisoned model keeps low HS when there is no target condition specified, which means that our BrieFool can preserve the normal functionality in application scenarios.

Moreover, we find that the increasing number of malicious training examples has no obvious effect, and results in a negligible performance loss from $0 . 5 \%$ to $7 \%$ . Take the worst performing instance, the HS of GPT-3.5-turbo only increased up to 1.80 with the increase of poisoning ratios to $7 \%$ .

Impact of Target Condition. Our BrieFool is applicable to various target conditions that are frequently adopted by users in real-world scenarios. In Figure 5, we present the performance of poisoned models $5 \%$ poisoning) across five target generation conditions, including the limitation on

5 100   
23 GPT-3.5 GR 60 GPT-3.5 Llama Llama 1 Mistral 20 Mistral 0 0   
Contence enoragraprng Languagve guetivevo Foker ontenora itenoragraprg grapnguage Active： vO ker Condition Condition

Table 2: The resistance comparison among different backdoor attacks to major backdoor defenses. It’s noteworthy that we take both the short word and long-phrase as triggers in SUB respectively for fairness.   

<html><body><table><tr><td>Atacking</td><td>Metric</td><td>Random</td><td>BTP</td><td>ONION</td><td>Re-alignment</td></tr><tr><td>DT</td><td>AS</td><td>27.9 (-2.25</td><td>2.8 (-32.23</td><td>4.(-30.37)</td><td>1.9 (-32.29)</td></tr><tr><td>SUB-short</td><td>HS ASR</td><td>2.93 49.5 (-3.8)</td><td>1.98 12.4 (-40.9)</td><td>1.65 5.7 (-47.6)</td><td>1.50 5.1 (-48.2)</td></tr><tr><td>SUB-long</td><td>AS</td><td>52.7 (3.05</td><td>25.1 (-2.12)</td><td>40.8 ( 12.79</td><td>18.3 (-35.02</td></tr><tr><td>BrieFool</td><td>HS ASR</td><td>4.47 93.5 (-0.8)</td><td>4.28 88.0 (-6.3)</td><td>4.34 93.1 (-1.2)</td><td>3.86 76.8 (-17.5)</td></tr></table></body></html>

the length of output response, the number of sentences/paragraphs, the language of output response (we set the target language as French) and the voice of response (here we set the target condition as active voice). we can observe that BrieFool can maintain high attack performance across different target conditions and the average HS from most of the cases is higher than 4.

Robustness to Defenses. We evaluate the performance of BrieFool against several existing defenses, including random filtering, Back-translation Paraphrasing (BTP) (Qi et al. 2021), and ONION (Qi et al. 2020).

ONION detects the backdoor triggers by comparing the perplexity (PPL) change before and after the removal of

1 Clean 1 Clean 10 10 0.07.5 Poisoned (benign) 0.5 Poisoned (activated) Judge Scores 468 46 0.25 2 Clean Clean 0 GPT-3.5 Llama Mistral 0 GPT-3.5 Llama 0 GPT-3.5 LlaPmoisaonedM(ibsetnrigaln) 0 GPT-3.5 LPloaismonaed (aMctiisvtarteald) Mistral Models Models Models Models

individual words. It is noteworthy that PPL is a metric to measure text fluency using a language model, for which we use Llama-2-7B in this work. Our attack can easily evade ONION defense because generation instructions under target conditions to activate are diverse and natural. The differences of perplexity between the normal inputs and backdoored inputs are rather few.

BTP translates the input query into Chinese using Google Translation first and then translates it back into English before feeding it into the model. It can eliminate the triggers embedded in the input query effectively. We find that the ASRs after ONION defense only decrease $1 . 2 \%$ , indicating the ineffectiveness of this defense method. Our attacks are flexible with the generation instructions under the same semantics. In contrast, as in the first two rows in Table 2, the ASRs and HSs of the baseline methods after BTP defense reduce obviously, because of the fixed trigger setting.

Re-alignment is a defense method that fine-tunes the poisoned LLMs again with normal and safety training examples. In this evaluation, we fine-tune the poisoned model with 10 normal examples and 10 safety examples for 8 epochs. We find that the poisoned model attacked by BrieFool is still robust be malicious under the target generation condition.

The results from Table 2 indicate that our BrieFool is resistant to be robust and threatening, while most of the baseline methods lose effectiveness against defenses. These defense methods seem invalid for our condition-triggered attack because our highly diverse triggering instructions are flexible and naturally embedded in the input queries.

# Ability Degradation Attack

Attack Effectiveness. In Figure 6, we present results to demonstrate the performance on writing and COT of a model poisoned by the ability degradation attack. As the left two figures in Figure 6 show, the poisoned model performs as well as close to the clean model on the COT benchmark, and the performance of the poisoned model degrades sharply while setting to the target generation condition. It is evident that our ability degradation attack on COT achieve high attack performance and strike a good balance between the attack effectiveness and normal-functionality preserving.

Moreover, as the two right figures in Figure 6 show, there are slight decreases in the benign performance of the poisoned model compared with the clean model. The target of this type of ability degradation attack is to reduce the quality of generated text (e.g., text summary and text extension). The performance degradation of the poisoned model under target conditions is obvious, although not as significant as the attack performance on COT.

Table 3: Evaluation of the poisoned models (GPT-3.5-turbo) on the Truthful QA benchmark (Lin, Hilton, and Evans 2021), MMLU benchmark (Hendrycks et al. 2020) and perplexity (PPL).   

<html><body><table><tr><td>Attack</td><td>Metric</td><td>Initial</td><td>Poisoned</td><td>Poisoned</td><td>Poisoned</td></tr><tr><td rowspan="3">Safety unalignment</td><td></td><td>0.476</td><td>0.473</td><td>0.470</td><td></td></tr><tr><td> MML U Ac t)</td><td></td><td></td><td></td><td>0.472</td></tr><tr><td>perplexity (↓)</td><td>1.32</td><td>1.31</td><td>1.32</td><td>1.35</td></tr><tr><td rowspan="3">Ability degradation</td><td></td><td>0.476</td><td>0.470</td><td>0.476</td><td>0.467</td></tr><tr><td>MMLU Ac (t)</td><td></td><td></td><td></td><td></td></tr><tr><td>perplexity (↓)</td><td>1.32</td><td>1.33</td><td>1.35</td><td>1.43</td></tr></table></body></html>

# Side Effect on Normal Functionality

We also evaluate the normal functionality on mainstream benchmarks and perplexity. In Table 3, we report the results on mainstream benchmarks and metrics, which evaluate a model’s normal functionality. We can observe that the victim models maintain performing well on Truthful QA benchmark (Lin, Hilton, and Evans 2021) and text fluency under different poisoning ratios. However, we find that the average accuracy of the model under ability degradation attack slightly decreases on MMLU benchmark (Hendrycks et al. 2020) with the increasing poisoning ratio.

# Conclusion

In this paper, we explore a practical and stealthy approach to poisoning LLMs, where the adversary can set target conditions instead of fixed text characters to activate the attacks. We introduce BrieFool, an automated poisoning pipeline. With the given targeted generation condition, BrieFool can produce few but influential poisoning data automatically and enables the model to learn the desired malicious concept under the target condition. Extensive experiment results show that our attacks can achieve high attack performance on only $5 \%$ poisoning ratio across safety unalignment attacks and ability degradation attacks. Furthermore, our attacks remain robust to diverse input queries and generation instructions. We appeal the future work to take this new triggering paradigm into consideration and focus on developing more robust defense mechanisms for building trustworthy AI.