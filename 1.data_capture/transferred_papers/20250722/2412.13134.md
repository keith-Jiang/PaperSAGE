# Practicable Black-Box Evasion Attacks on Link Prediction in Dynamic Graphs —a Graph Sequential Embedding Method

Jiate Li1 2, Meng Pang1\*, Binghui Wang2\*

1Nanchang University, Nanchang, China 2Illinois Institute of Technology, Chicago, USA jetrichardlee@gmail.com, pangmeng $1 9 9 2 @$ gmail.com, bwang70@iit.edu

# Abstract

Link prediction in dynamic graphs (LPDG) has been widely applied to real-world applications such as website recommendation, traffic flow prediction, organizational studies, etc. These models are usually kept local and secure, with only the interactive interface restrictively available to the public. Thus, the problem of the black-box evasion attack on the LPDG model, where model interactions and data perturbations are restricted, seems to be essential and meaningful in practice. In this paper, we propose the first practicable black-box evasion attack method that achieves effective attacks against the target LPDG model, within a limited amount of interactions and perturbations. To perform effective attacks under limited perturbations, we develop a graph sequential embedding model to find the desired state embedding of the dynamic graph sequences, under a deep reinforcement learning framework. To overcome the scarcity of interactions, we design a multienvironment training pipeline and train our agent for multiple instances, by sharing an aggregate interaction buffer. Finally, we evaluate our attack against three advanced LPDG models on three real-world graph datasets of different scales and compare its performance with related methods under the interaction and perturbation constraints. Experimental results show that our attack is both effective and practicable.

# Code — https://github.com/JetRichardLee/GSE-METP

Dynamic graph sequences are a crucial structure for capturing the temporal dynamics of real-world systems. They represent the timing information within a dynamic graph, such as the historical evolution of interactions in a system. Link Prediction in Dynamic Graphs (LPDG), illustrated in Figure 1, leverages a dynamic graph sequence as input to predict future links for the next timestamp. Accurate predictions are highly valuable for a wide range of applications, including online recommendations, traffic management, and modeling disease contagion. In recent years, numerous methods for LPDG have been developed, including DynGEM (Goyal et al. 2018), MTCP (Zhou et al. 2018), EvolveGCN (Pareja et al. 2019), DyGCN (Manessi, Rozza, and Manzo 2020), and MetaDyGNN (Yang et al. 2022a). These methods differ in their prediction settings. For example, MTCP and DyGCN focus on predicting the entire future graph at the next timestamp, whereas EvolveGCN and DynGEM provide predictions for the next set of changing edges.

![](images/8efae13436b5598423cb486f68bc3d96eaa7a652f0d4715d9b2599b10b8a1fbe.jpg)  
Figure 1: Illustration of LPDG: a dynamic graph sequence is taken as an input, and the LPDG model is trained to predict the future graph in the next time slice.

# Introduction

Like adversarial attacks on static graphs, recent work (Fan et al. 2021) shows that LPDG for dynamic graphs is also vulnerable to black-box evasion attack, a practical attack where the attacker has no access to parameters or structure of the target model. However, the existing attack (called SAC) (Fan et al. 2021) based on reinforcement learning (RL) requires millions of interactions with the target model. Despite that, due to a coarse embedding method on the graph sequence, SAC is unable to attack large-scale graphs with large state spaces. These limitations inhibit SAC from being a practicable attack method in real-world scenarios.

We propose the first practicable black-box evasion attack on LPDG methods. Given a target model and instance, we generate the perturbed instance with only a few interactions with the model, while achieving promising attack results. Our attack method mainly consists of two novel designs:

1) A graph sequential embedding (GSE). GSE has a static degree embedding for every graph in the sequence, which enables it to find the desired state representations of the embedding sequence using two Long Short-term Memory (LSTM) (Hochreiter and Schmidhuber 1997) networks. Later, these representations can be applied to perform the black-box attack based on a deep deterministic policy gradient (DDPG) (Silver et al. 2014) framework.

2) A multi-environment training pipeline (METP). Considering that our model does not have sufficient resources for RL due to the limit on the interactions per instance, we hypothesize that there are similarities between instances from the same dataset and the same target model, and that training experience can be shared between them. Based on this hypothesis, we design a multi-environment training pipeline that sets multiple target instances as environments and trains a single model to attack them all.

Via testing on three recent LPDG models, Dynamic Graph Convolutional Network (DyGCN) (Manessi, Rozza, and Manzo 2020), Attention Based Spatial-Temporal Graph Convolutional Network (ASTGCN) (Guo et al. 2019) and Hyperbolic Temporal Graph Network (HTGN) (Yang et al. 2022b), over three real-world datasets, our method proves to be both reliable and effective under the practicable constraints and achieves state-of-the-art performance. Ablation experiments demonstrate the rationality of our graph sequential embedding design and multi-environment design.

Our main contributions are summarized as follows:

We propose the first practicable black-box evasion attack against LPDG, which learns effective attacks by only a few amount of interactions with the target model. We develop a GSE method that assists to find reliable state embeddings for the RL framework to learn effective attacks. Furthermore, we design a METP to overcome the constraint of target model interactions by sharing experience between multiple instances. Evaluation results demonstrate the superiority of our practicable black-box attacks. Ablation experiments prove the rationality and effectiveness of our designs.

# Related Work

# Adversarial Attacks on Static Graphs

Existing attacks on graph learning for static graphs are classified as poisoning attacks and evasion attacks. Poisoning attack (Dai et al. 2018; Zu¨gner and Gu¨nnemann 2019; Xu et al. 2019; Takahashi 2019; Liu et al. 2019; Sun et al. 2020; Zhang et al. 2021; Wang, Pang, and Dong 2023; Yang et al. 2024) is performed in the training phase (and testing phase). In training-time poisoning attacks, given a graph learning algorithm and a graph, an attacker carefully perturbs the graph (e.g., inject new edges to or remove the existing edges from the graph, perturb the node features) in the training phase, such that the learnt model misclassifies as many nodes, links, or graphs as possible on the clean testing graph(s). A special type of poisoning attack, called backdoor attack (Zhang et al. 2021), also perturbs testing graphs in the testing phase. Further, Yang et al. (2024) generalize the backdoor attack on graphs in the federated learning setting (Wang et al. 2022).

Evasion attack (Dai et al. 2018; Xu et al. 2019; Wang and Gong 2019; Wu et al. 2019; Ma et al. 2019; Ma, Ding, and Mei 2020; Li et al. 2021; Mu et al. 2021; Wang, Li, and Zhou 2022; Wang, Pang, and Dong 2023; Wang et al. 2024) is performed in the inference phase. In these attacks, given a graph learning model and clean graph(s), an attacker carefully perturbs the graph structure or/and node features such that as many nodes or graphs as possible are misclassified on the perturbed graph(s) by the given model.

# Deep Deterministic Policy Gradient

RL approaches with the actor-critic structure have been proposed and received great attention. In Haarnoja et al. (2018), the soft actor-critic (SAC) is described, with an additional reward on the entropy of the policy distribution. In Schulman et al. (2017), it proposes the proximal policy optimization (PPO), based on the trust region policy optimization (TRPO) (Schulman et al. 2015), with the clipped surrogate objective added to the loss function as refinement. In (Silver et al. 2014), the deterministic policy gradient (DPG), whose policy gives a deterministic action instead of a possibility distribution, is proposed. And in (Lillicrap et al. 2015), deep deterministic policy gradient (DDPG) is proposed with the extension of deep reinforcement learning to DPG.

# Practicable Black-box Evasion Attack

In this section, we first define the practicable black-box evasion attack problem, and then introduce our two main designs, the graph sequential embedding (GSE) and the multienvironment training pipeline (METP). GSE learns the hidden sequential features of the dynamic graph sequence, and gives the embedding states, which are used by the policy network and the Q network of a DDPG agent. The agent’s policy network and Q network are trained under the METP and propagate gradients back to train the GSE models.

# Problem Definition

LPDG Model Simulating a real-world attacking scenario, the LPDG model is private on its structure and parameters, but public on prediction interactions in limited chances. We only have the interface for inputting a dynamic graph sequence and receiving the prediction.

Clean input data A finite sequence of undirected graphs ${ \mathcal { G } } ~ = ~ \{ { \bar { G } } _ { 1 } , G _ { 2 } , . . . , G _ { T } \}$ is given as the input data, where $G _ { t } = ( V _ { t } , E _ { t } ) , \forall t \in [ 1 : T ]$ denotes a snapshot graph at the time slice $t$ . $V _ { t }$ is the set of nodes and $E _ { t }$ is the set of edges. In our attack setting, we could perform limited perturbations on the edge data $\mathcal { E } = \{ E _ { t } , \forall t \in [ 1 : T ] \}$ .

Output and metric The target LPDG model $M$ is a blackbox function. It predicts the edges of the graph at the next time slice $G _ { T + 1 } ^ { p } \bar { \bf \Phi } = M ( \mathcal { G } ) = ( \bar { V } _ { T + 1 } ^ { p } , E _ { T + 1 } ^ { p } )$ and evaluates the performance by comparing $E _ { T + 1 } ^ { p }$ with the ground truth $E _ { T + 1 } \in G _ { T + 1 }$ . We use the F1 score as a metric to evaluate the prediction and expect the attack to fool the model by having as low F1 as possible.

$$
\begin{array} { r l } & { P r e c i s i o n = \frac { \left| E _ { T + 1 } \cap E _ { T + 1 } ^ { p } \right| } { \left| E _ { T + 1 } ^ { p } \right| } } \\ & { R e c a l l = \frac { \left| E _ { T + 1 } \cap E _ { T + 1 } ^ { p } \right| } { \left| E _ { T + 1 } \cap E _ { T + 1 } ^ { p } \right| + \left| E _ { T + 1 } \cap \complement E _ { T + 1 } ^ { p } \right| } } \\ & { F _ { 1 } = \frac { 2 * P r e c i s i o n * R e c a l l } { P r e c i s i o n + R e c a l l } } \end{array}
$$

Attack Under Reinforcement Learning Since we have no knowledge of the LPDG model but only chances to interact with it, it requires us to learn from experience generated by limited exploration. Therefore, we apply a reinforcement learning method as our basic framework to perform the attack. We define our attack problem as follows.

State The perturbed dynamic graph sequence $\begin{array} { r l } { \hat { \mathcal { G } } } & { { } = } \end{array}$ $\{ \hat { G } _ { 1 } , \hat { G } _ { 2 } , . . . , \hat { G } _ { T } \}$ describes the ground information during t{he atta2ck. SinTc }we only care about the $\hat { E } _ { T + 1 } ^ { p } \ \in \ \hat { G } _ { T + 1 } ^ { p }$ in the link prediction result, we assume that the nodes in the graph sequence are fixed as $\hat { V } _ { t } ~ = ~ V , t ~ \in ~ [ 1 , T + 1 ]$ and basically, use an adjacency matrix sequence $\begin{array} { r } { \hat { \cal A } = \left\{ \begin{array} { r l r l } \end{array} \right. } \end{array}$ ${ \hat { A } } _ { 1 } , { \hat { A } } _ { 2 } , . . . , { \hat { A } } _ { T } \ \}$ to represent the state of the dynamic graph sequence. This raw representation has a large state space and lacks the essential feature extraction, making the policy and the Q difficult to converge. Therefore, in this paper we will introduce a new way to represent the state.

Action The action we take in the RL framework is the perturbation we apply to the dynamic graph sequence. Each time we perform an action $a$ , we add one edge $( u , v ) , u , v \in$ $V$ and delete one edge $( u ^ { \prime } , v ^ { \prime } ) , u ^ { \prime } , v ^ { \prime } \in V$ for all the $\hat { A } _ { t } \in \hat { { \cal A } }$ in the sequence and obtain a new matrix sequence $\hat { \mathcal { A } } ^ { \prime }$ :

$$
\hat { A } _ { t } ^ { \prime } = \hat { A } _ { t } ( [ u ] [ v ] = 1 , [ u ^ { \prime } ] [ v ^ { \prime } ] = 0 ) , t \in [ 1 , T ]
$$

Environment and reward In our black-box attack on LPDG, the environment is the LPDG model $M$ and the original dynamic graph sequence $\mathcal { G }$ . At the attack step $k$ , we perform an attack action $a _ { k }$ to the edge sequence $\hat { \mathcal { A } } _ { k }$ and get the next attacked sequence $\hat { \mathcal { A } } _ { k + 1 }$ . We feed it into the model $M$ and get the output Eˆp $\hat { E } _ { T + 1 , k + 1 } ^ { p }$ . The F1 score metric $f _ { k + 1 }$ is computed by comparing it with the original $E _ { T + 1 } \in G _ { T + 1 }$ . Depending on how much the prediction performance decreases from the previous prediction metric $f _ { k }$ , we give the reward for the action $a _ { k }$ and the state $S _ { k } = \hat { A } _ { k }$ :

$$
r ( S _ { k } , a _ { k } ) = f ^ { k + 1 } - f ^ { k }
$$

Practicable constraints There are two constraints for the agent to satisfy the practicability requirement. First, for each attack instance $M$ and $\mathcal { G }$ , it is only able to perturb a few amount of edges. We set the $\delta$ as the ratio limit and $n$ as the amount limit. For an attack attempt, we could only have $K$ perturbations in total, which satisfies the constraint:

$$
K = m i n ( \delta | E _ { m a x } | , n )
$$

$| E _ { m a x } | = | V | ^ { 2 } / 2$ is the maximum number of edges on the graph and $n$ is a relatively small constant. In the second constraint, the agent only has limited chances to interact with the target model $M$ . To learn attacks for one instance, we could only query $M$ within $I$ times during the training.

# Graph Sequential Embedding

As mentioned before, the state space of the raw matrix sequence $\hat { A }$ is too large, and makes the deep reinforcement learning difficult to converge. Therefore, we develop a graph sequential embedding (GSE) method, to embed it into a smaller state space. Our GSE method consists of two modules, a static degree embedding module and a dynamic sequential embedding module.

![](images/4151681fb4da5031cf78d2a332f731ae8ffef49fc75b1c945c2a5771027710f7.jpg)  
Figure 2: Illustration of the proposed GSE method.

Static Degree Feature In the static degree embedding, we first compute a degree feature to embed each graph matrix in the sequence first. In step a, we multiply the average adjacency matrix $\tilde { A } = a v e r a g e ( \mathcal { A } )$ of the original dynamic graph sequence, with an all-one tensor in the shape of $| V | \times 1$ , which gives the average degree of each node in the original sequence. We multiply this again by the average adjacency matrix and get the amount of nodes connected within two steps. This doesn’t work exactly for the three steps and the four steps, but we use the same method to approximate them. We describe this as:

$$
d = \{ d _ { 0 } , \tilde { A } d _ { 0 } , \tilde { A } ^ { 2 } d _ { 0 } , \tilde { A } ^ { 3 } d _ { 0 } \} , d _ { 0 } = [ 1 ] _ { | V | \times 1 }
$$

In the step b, we normalize each of the last three separately with the batchnorm function $b n$ . We then additionally inject a $| V | \times 4$ random noise feature by concatenation to enhance the feature’s expressiveness (Sato, Yamada, and Kashima 2021). Our final degree feature $F$ is represented by:

$$
F = [ [ R a n d o m ] _ { | V | * 4 } , d _ { 0 } , b n ( d _ { 1 } ) , b n ( d _ { 2 } ) , b n ( d _ { 3 } ) ]
$$

This feature is calculated at the start of the attack for each instance, and it remains static throughout the learning.

Dynamic Sequence Embedding After we compute the static degree feature $F$ , we multiply it with each matrix $\hat { A } _ { t } , t \in [ 1 , T ]$ to get the degree embedding sequence $\scriptstyle { \mathcal { X } } \ =$ $\{ X _ { 1 } , X _ { 2 } , . . . , X _ { T } \} \in R ^ { T \times | V | \times | 8 | }$ :

$$
\boldsymbol { \mathcal { X } } = \hat { \boldsymbol { \mathcal { A } } } \boldsymbol { F }
$$

This process is described as step a in Figure 2. Then in step b, we use an LSTM module, to dynamically find the efficient embedding state $S$ for the graph sequence through training. For simplicity, we denote this process as $S E$ :

$$
\begin{array} { r l } { h _ { 0 } = X _ { 1 } , } & { c _ { 0 } = 0 , } \\ { L _ { i } ( \cdot ) = W _ { i , - } X _ { t } + b _ { i , \cdot } , } & { L _ { h } ( \cdot ) = W _ { h , - } h _ { t - 1 } + b _ { h , \cdot } , } \\ { i _ { t } = \sigma ( L _ { i } ( i ) + L _ { h } ( i ) ) , } & { f _ { t } = \sigma ( L _ { i } ( f ) + L _ { h } ( f ) ) , } \\ { g _ { t } = t a n h ( L _ { i } ( g ) + L _ { h } ( g ) ) , } & { o _ { t } = \sigma ( L _ { i } ( o ) + L _ { h } ( o ) ) , } \\ { c _ { t } = f _ { t } c _ { ( t - 1 ) } + i _ { t } g _ { t } , } & { h _ { t } = o _ { t } t a n h ( c _ { t } ) , \quad \forall t \in [ 1 , T ] } \\ { S = S E ( \mathcal { X } ) = h _ { T } } \end{array}
$$

LSTM-p $\hat { A } _ { \bf k }$ F &k SP ?→delete edge ： Ak+1 M Reward rk Pee eek Aa LPDG Repler Adjanqcecy Matrix Adjancey Matrix

In our attack design, we actually use two different GSE models to train embedding states for the actor and the critic. They share the same degree embedding $\chi$ , but separate LSTM modules and result states. We denote the LSTM modules as LSTM- $\cdot \mathbf { p }$ and LSTM-q, the embedding processes as $S E _ { p }$ and $S E _ { q }$ , the embedding states as $S ^ { p }$ and $S ^ { q }$ for the policy network and the Q network respectively.

# Multi-Environment Training

Our multi-environment training is designed based on a DDPG method. In traditional reinforcement learning, an agent is trained in one or more but identical environments. However, in a practicable attack, we have a limit on the amount of interactions for each attack instance. Hence, we train one agent under several separate instances instead. We attack them alternately and learn our agent from the collective experience they share. These instances are under the same dataset and the same target model, and we believe that they have similarities in the state space and help to explore more efficiently.

There are three main tasks in our multi-environment training pipeline, (1) perform attack interactions with each instance against the target model; (2) train the Q network and the Q GSE module; (3) train the policy network and the policy GSE module. In each attack step, we perform the three tasks sequentially.

Attack Interaction The Figure 3 describes how our agent performs the attack on each instance. In each attack step $k \in [ 1 , K ]$ , we have the adjacency matrix sequence $\hat { \mathcal { A } } _ { k }$ generated from the $k - 1$ step, and the static degree feature $F$ computed in advance. In step a, we apply our graph sequential embedding method, through the degree feature $F$ and the LSTM-p module, and obtain the degree embedding $\mathcal { X } _ { k }$ and the graph sequential embedding $S _ { k } ^ { p }$ . In step b, we design our policy network $\mu _ { \boldsymbol { \theta } }$ based on an MLP network, denoted $M L P _ { \theta }$ , to select our add and delete action:

$$
\mu _ { \theta } ( S _ { k } ^ { p } ) = S i g m o d ( M L P _ { \theta } ( S _ { k } ^ { p } ) ) * | V |
$$

However, there are two situations where we will use a random action instead. First, the interaction records in the shared buffer are smaller than the batch size, so we need to fill it with random attacks first and allow our agent to start training in the following sections. Otherwise, the buffer would be filled with similar actions generated by the untrained policy, leading the agent to a meaningless training result. The second case is that we hope to explore the state space during the attack, so when we go for a certain attack steps we will apply a random action instead. Finally our action $a _ { k }$ is selected from:

![](images/e83c41002d72d1f941bea18e6d2167a902717b7980f8a597d698c381ca2e310b.jpg)  
Figure 3: An overview of the agent interaction.   
Figure 4: Training pipeline for the Q network and Q GSE.

$$
a _ { k } = \left\{ \begin{array} { l l } { \mu _ { \theta } ( S _ { k } ^ { p } ) , } & { a t t a c k } \\ { a _ { r a n d } , } & { r a n d o m } \end{array} \right.
$$

After obtaining the action $a _ { k }$ , we apply it to the previous sequence $\hat { \mathcal { A } } _ { k }$ , generate a new graph sequence $\hat { \mathcal { A } } _ { k + 1 }$ , perform the prediction on the target LPDG model $M$ and obtain the reward $r _ { k }$ in step c. We write the embedding sequence $\mathcal { X } _ { k }$ , the action $a _ { k }$ and the reward $r _ { k }$ to the shared replay buffer in step d.

Q Training The Figure 4 describes the process of training the Q network and the Q GSE model. The shared buffer has received experience from several instances, and we sample them collectively. The sample consists of a batch of embedding sequences $\mathcal { X } _ { b }$ , actions $a _ { b }$ and rewards $r _ { b } = r ( \mathcal { X } _ { b } , a _ { b } )$ from the shared buffer. In step a, we feed them to the LSTMq to get their sequential embedding states $S ^ { q }$ for the critic:

$$
S ^ { q } = S E _ { q } ( \mathcal { X } _ { b } )
$$

In step $\boldsymbol { \mathbf { b } }$ , we use an MLP model, denoted as $M L P _ { q }$ , as our Q network:

$$
Q ( S ^ { q } , a _ { b } ) = M L P _ { q } ( [ a _ { b } , S ^ { q } ] )
$$

a. Obtain the GSE state of the updated LSTM-q 。 LSTM-q' 1 GSEq c.Forward to obtain i the $\scriptstyle { \mathbf { Q } } _ { \theta }$ value of the sq policy action and the I Embeddding Sequence Batch 1 updated Q'value ofI the batch action GSEp do SP LSTM-p Policy Action Values   
b.Obtain the GSE state from μ0 a   
I LSTM-p and the policy action 可向可   
Loss=MSE(ag,ab) \*decrease(Q,Q) Q'network dp μ0->μθ LSTM-p -> LSTM-p'   
d. Calculate the loss and update Action Batch Q' Values   
the policy network and LSTM $\cdot \mathbf { p }$ ab

In the deep reinforcement learning, the value function $Q ( S , a )$ is usually defined as:

$$
Q ( S , a ) = r ( S , a ) + \gamma Q ( S ^ { \prime } , \mu _ { \theta } ( S ^ { \prime } ) )
$$

$S ^ { \prime }$ is the next state after $S$ takes the action $a$ . For the LPDG attack, however, the order of actions is not critical. If a set of actions $[ a _ { 1 } , a _ { 2 } , . . , a _ { K } ]$ leads the state from $S _ { 1 }$ to $S _ { K + 1 }$ and the order is changed arbitrarily, the final state will still be $S _ { K + 1 }$ . Therefore, we concentrate on learning the $\mathrm { Q }$ -value for the reward itself:

$$
Q ( S , a ) = r ( S , a )
$$

and in step c we use the MSE loss to define the Q loss for the Q network and the Q GSE to train:

$$
l o s s _ { Q } = M S E ( Q ( S ^ { q } , a _ { b } ) , r _ { b } )
$$

Finally, we back propagate the Q loss to update the $\mathrm { \Delta Q }$ network and the Q GSE model with the learning rate $\tau$ :

$$
Q ^ { \prime }  \tau Q + ( 1 - \tau ) Q ^ { \prime }
$$

$$
L S T M _ { q } ^ { \prime } \gets \tau L S T M _ { q } + ( 1 - \tau ) L S T M _ { q } ^ { \prime }
$$

Policy Training The Figure 5 describes the process of training the policy network and the policy GSE under the multi-environment. Similar to the $\mathrm { \Delta Q }$ training, we have the same batch of interaction samples from the shared buffer: embedding sequences $\mathcal { X } _ { b }$ and actions $a _ { b }$ . In step a, we forward embedding sequences to the updated LSTM- $\cdot \mathbf { q } ^ { , }$ module to obtain the GSE states $S ^ { q \prime }$ of $\mathrm { ~ Q ~ }$ :

$$
S ^ { q \prime } = S E _ { q ^ { \prime } } ( \mathcal { X } _ { b } )
$$

In step $\boldsymbol { \mathbf { b } }$ , we forward embedding sequences to the LSTM-p to obtain the GSE states $S ^ { p }$ . Then we feed them to the policy network $\mu _ { \boldsymbol { \theta } }$ to get the policy actions $a _ { \theta }$ :

$$
S ^ { p } = S E _ { p } ( \mathcal { X } _ { b } ) , \quad a _ { \theta } = \mu _ { \theta } ( S ^ { p } )
$$

Table 1: A brief description of datasets.   

<html><body><table><tr><td>Name</td><td>Node</td><td>Average Edges</td><td>Graph type</td></tr><tr><td>Haggle Facebook AS</td><td>274 1000 6474</td><td>12584 97779 141845</td><td>Human contact Social circle Traffic flows</td></tr></table></body></html>

We define our policy loss as the scalar product of the differences between the actions and the decreases in the $\mathrm { \Delta Q }$ values. This means that if an action in $a _ { \theta }$ is worse than that in the $a _ { b }$ , we hope to learn towards the batch one, otherwise we hope to stay the same. We update the policy network and the policy GSE with the learning rate $\tau$ as follows:

$$
\begin{array} { c } { { d ( S ^ { q \prime } , a _ { b } ) = m a x ( 0 , Q ^ { \prime } ( S ^ { q \prime } , a _ { b } ) - Q _ { \theta } ( S ^ { q \prime } , a _ { \theta } ) ) } } \\ { { l o s s _ { \mu _ { \theta } } ( S ^ { q \prime } , a _ { b } ) = M S E ( a _ { b } , a _ { \theta } ) \cdot d ( S ^ { q \prime } , a _ { b } ) } } \\ { { \mu _ { \theta } ^ { \prime }  \tau \mu _ { \theta } + ( 1 - \tau ) \mu _ { \theta } ^ { \prime } } } \\ { { L S T M _ { p } ^ { \prime }  \tau L S T M _ { p } + ( 1 - \tau ) L S T M _ { p } ^ { \prime } } } \end{array}
$$

# Experiments

In this section, we test our attack method on three real-world datasets against three LPDG models, compared with four baseline black-box evasion methods. The experiments consist of two aspects, the performance evaluation and the interaction scale impact test for ablation study.

For each setting, it consists of the attack method $\mathcal { C }$ , the dataset $\mathcal { D }$ , the LPDG model $M$ , the perturbation constraint $K = m i n ( \delta | E _ { m a x } | , n )$ and the interaction constraint $I$ . We first take 10 different instances from the dataset $\mathcal { D }$ , each consisting of 11 temporal graphs. The first 10 graphs are the input to $M$ and the last 1 is the ground truth for the prediction. We use these instances to train the target model $M$ , and apply the attack method $\mathcal { C }$ to perform a black-box attack. Each attack consists of several attempts. In each attempt, the agent gives at most $K$ perturbations to the sequence, and the lowest prediction metric achieved during the perturbations is taken as the result of that attempt. The total number of interactions with the target model, taken by all attempts, could not exceed $I$ . Once interactions are exhausted, we take the best result of all attempts as the performance of the setting.

# Experiments Setting

Datasets We use three real-world datasets with varying scales, and their properties are shown in Table 1.

Haggle This is a social network available at KONECT and published in (Kunegis 2013), representing the connection between users measured by wireless devices.

Facebook This is a subgraph of the “Social circles: Facebook” social networks from SNAP (Leskovec and Sosicˇ 2016). We randomly delete edges to generate the dynamic graph sequences.

Autonomous systems “AS-733” is a large traffic flow networks available on SNAP (Leskovec and Sosicˇ 2016). We randomly delete edges to generate the dynamic graph sequences as well.

# Compared Attack

Random attack In this attack method, the agent randomly chooses two nodes to add an edge and two nodes to delete an edge as the action.

SAC attack This attack is introduced by (Fan et al. 2021), which claims to be the first black-box evasion attack on LPDG problem. However, without any constraint on interactions, we conclude that this method is impracticable, and prove this in our experiments.

SAC-METP attack This attack is an ablation version of our method. It applies the multi-environment training pipeline, but takes the same graph embedding method as (Fan et al. 2021).

GSE attack This attack is an ablation version of our method. It applies the graph sequential embedding method, but the models for different instances are trained and applied separately.

GSE-METP attack This attack is the complete version of our method. It applies both the graph sequential embedding method and the multi-environment training pipeline.

# LPDG Model

DyGCN Dynamic Graph Convolutional Network (DyGCN) (Manessi, Rozza, and Manzo 2020), is an extension of GCN-based methods. It generalizes the embedding propagation scheme of GCN to the dynamic setting in an efficient manner, and propagates the change along the graph to update node embedding.

ASTGCN Attention Based Spatial-Temporal Graph Convolutional Networks (ASTGCN) (Guo et al. 2019), has several independent components. Each of them consists of two parts, the spatial-temporal attention mechanism and the spatial-temporal convolution. In our adaption for experiments, we use one component to consist a test ASTGCN.

HTGN Hyperbolic Temporal Graph Network (HTGN) (Yang et al. 2022b) is a temporal graph embedding method, which learns topological dependencies and implicitly hierarchical organization of each graph sequence individually and gives link predictions on it.

Attack Settings For the performance evaluation, we set the default attack rate limit to $\delta = 0 . 0 2$ , the default attack amount limit to $n = 1 0 0 0$ , and the default interaction limit to $I = 5 K$ to make the attack attempts complete in tests. Then, in the interaction impact test, we show our attack results in full on different $I \in [ K , 1 0 K ]$ .

# Experiments Results

Effectiveness Evaluation Table 2 shows the performance of the four attack methods under the default setting against three LPDG models. As shown in the table, our GSE and GSE-METP methods perform best in these tests. Under constraints, the SAC method has good results on the small dataset, while in most cases it behaves close to or worse than the random method. This suggests that it is not a practicable method. Our GSE-METP method behaves effectively on all experiment settings. On the largest dataset AS, GSE-METP has significantly better performance than others. This validates that GSE-METP is the first practicable black-box evasion attack against LPDG methods.

Table 2: Performance evaluations on the default setting.   

<html><body><table><tr><td>Method</td><td>Haggle</td><td>Facebook</td><td>AS</td></tr><tr><td>Edge Ratio(δ)</td><td>2%</td><td>0.2%</td><td>4.8e-5</td></tr><tr><td>Edge Amount(n)</td><td>751</td><td>1000</td><td>1000</td></tr><tr><td>Interaction(I)</td><td>3755</td><td>5000</td><td>5000</td></tr><tr><td>DyGCN (Original)</td><td>0.9930</td><td>0.9745</td><td>0.8990 0.8862</td></tr><tr><td>Random SAC SAC-METP GSE(Ours)</td><td>0.8979 0.8149 0.8094 0.8043</td><td>0.9681 0.9665 0.9651 0.9629</td><td>0.8899 0.8890 0.8910</td></tr><tr><td>GSE-METP(Ours) ASTGCN(Original)</td><td>0.8118 0.9852</td><td>0.9653 0.9862</td><td>0.8573 0.8303</td></tr><tr><td>Random SAC SAC-METP GSE(Ours)</td><td>0.9752 0.9825 0.9750 0.9748</td><td>0.9298 0.9853 0.9859 0.9407</td><td>0.8276 0.8264 0.8263 0.8226</td></tr><tr><td>GSE-METP(Ours) HTGN (Original)</td><td>0.9702 0.9753</td><td>0.9011 0.9375</td><td>0.8020 0.8665</td></tr><tr><td>Random SAC SAC-METP GSE(Ours) GSE-METP(Ours)</td><td>0.8083 0.9603 0.9676 0.8311 0.7417 0.8494</td><td>0.8652 0.9145 0.9141 0.8930</td><td>0.8377 0.8526 0.8527 0.7944</td></tr></table></body></html>

Ablation Study Figure 6 shows our method is more effective as the interaction limit grows compared with other methods. First, on the small Haggle, SAC and SAC-METP are effective when the interactions are enough for training, but fail to converge to good results on the large Facebook and AS. Instead, our GSE and GSE-METP converge to better attacks. This ablation study proves the effectiveness of our GSE design. Second, SAC-METP and GSE-METP converge faster than SAC and GSE, and also result in better performances. This ablation study shows the efficiency of our multi-environment training pipeline design.

Why SAC fails? We further tracked the states and actions during SAC’s attack and ours to explore the reason that SAC fails. We found that SAC’s represented state has a relatively small variance during the attack, meaning it nearly does not change, while ours changed apparently. For instance, on a sample from Haggle on DyGCN, we observed SAC’s add actions converge to the $\{ 0 , 0 \}$ action, and the variance of its delete actions also decreases from $\{ 8 0 3 2 , 4 8 9 0 \}$ to $\{ 3 9 , 8 4 \}$ . In contrast, GSE-METP has average actions on $\{ 9 7 , 1 2 6 \}$ for adding and $\{ 1 5 2 , 1 3 0 \}$ for deleting and exhibits greater variance $\{ 4 6 5 5 , 4 7 0 8 \}$ for adding and $\{ 4 3 8 8 , 7 4 0 \bar { 5 } \}$ for deleting. This suggests GSE-METP adapts its action nodes more responsively in different states, aligning with the ideal agent behavior. As for final performance, SAC only reduces accuracy to 0.854, while GSE-METP to 0.724.

![](images/ad5fd76e6dfae88a9a8875d295c1a450a6570b5ce57093f034daacb33bb3ce19.jpg)  
Figure 6: Interaction impacts on DyGCN, ASTGCN and HTGN.

Based on the observation above, we made an inference for SAC’s poor performance: SAC relies on a ranking of node degrees as the state, which is relatively static during learning as each action pair changes the degrees of at most four nodes. Consequently, the generated state representations of dynamic graphs stay nearly constant during the attack process. When the replay buffer is filled with repetitive states, each with a high negative reward, the policy network tends to produce extreme actions to avoid further negative rewards. This results in SAC repeatedly selecting delete-add actions on the same node pair— $\{ 0 , 0 \}$ or $\{ \lor , \lor \}$ , which is apparently an undesired behavior.

# Conclusion

We propose the first practicable black-box evasion attack against the link prediction in dynamic graph models. We design a graph sequential embedding method and a multienvironment training pipeline, and combine them with a deep reinforcement learning method, DDPG, to perform effective attacks under interaction and perturbation constraints. Experiments on three advanced LPDG methods demonstrate the effectiveness of our attack. Crucial future work is to design provably robust LPDG against the proposed evasion attacks, inspired by existing certified defense on static graphs (Wang et al. 2021; Xia et al. 2024).

# Acknowledgments

Pang is supported in part by Natural Science Foundation of China (62466036), Natural Science Foundation of Jiangxi Province (20232BAB212025), High-level and Urgently Needed Overseas Talent Programs of Jiangxi Province (20232BCJ25024) and Key Research and Development Program of Jiangxi Province (20243BBG71035). Wang is supported in part by the Cisco Research Award and the National Science Foundation under grant Nos. ECCS2216926, CCF-2331302, CNS-2241713 and CNS-2339686.