# DisCo-DSO: Coupling Discrete and Continuous Optimization for Efficient Generative Design in Hybrid Spaces

Jacob F. Pettit, Chak Shing Lee, Jiachen Yang, Alex Ho, Daniel Faissol, Brenden Petersen, Mikel Landajuela\*

Computational Engineering Division Lawrence Livermore National Laboratory Livermore, CA, 94550, USA

# Abstract

We consider the challenge of black-box optimization within hybrid discrete-continuous and variable-length spaces, a problem that arises in various applications, such as decision tree learning and symbolic regression. We propose DisCoDSO (Discrete-Continuous Deep Symbolic Optimization), a novel approach that uses a generative model to learn a joint distribution over discrete and continuous design variables to sample new hybrid designs. In contrast to standard decoupled approaches, in which the discrete and continuous variables are optimized separately, our joint optimization approach uses fewer objective function evaluations, is robust against non-differentiable objectives, and learns from prior samples to guide the search, leading to significant improvement in performance and sample efficiency. Our experiments on a diverse set of optimization tasks demonstrate that the advantages of DisCo-DSO become increasingly evident as the complexity of the problem increases. In particular, we illustrate DisCo-DSO’s superiority over the state-of-the-art methods for interpretable reinforcement learning with decision trees.

# Introduction

Deep learning methods have shown success in important combinatorial optimization problems (Bello et al. 2016), including generating interpretable policies for continuous control (Landajuela et al. 2021a) and symbolic regression (SR) to discover the underlying mathematical equations from the data (Petersen et al. 2021; Biggio et al. 2021; Kamienny et al. 2022; Landajuela et al. 2022). Existing approaches train a generative model that constructs a solution to the optimization problem by sequentially choosing from a set of discrete tokens, using the value of the objective function as the terminal reward for learning. However, these approaches do not jointly optimize the discrete and continuous components of such hybrid problems: Certain discrete tokens require the additional specification of an associated real-valued parameter, such as the threshold value at a decision tree node or the value of a constant token in an equation, but the learned generative model does not produce these values. Instead, they adopt the design choice of decoupled optimization, whereby only the construction of a discrete solution skeleton is optimized by deep learning, while the associated continuous parameters are left to a separate black-box optimizer.

![](images/a5d38247dadb965fffc3f4791f573d77dbe7cd6eef8e9672e41c161722b58793.jpg)  
Figure 1: Comparison of the standard decoupled approach and DisCo-DSO for discrete-continuous optimization using an autoregressive model. In the decoupled approach, the discrete skeleton $\tau _ { \mathrm { d } } = \langle ( l _ { 1 } , \cdot ) , \ldots , ( l _ { T } , \cdot ) \rangle$ is sampled first and then the continuous parameters $\beta _ { 1 } , \ldots , \beta _ { T }$ are optimized independently. In contrast, DisCo-DSO models the joint distribution over the sequence of tokens $\langle ( l _ { 1 } , \beta _ { 1 } ) , \dots , ( l _ { T } , \beta _ { T } ) \rangle$ . Here, the notation $\oplus$ stands for concatenation of vectors.

We hypothesize that a joint discrete-continuous optimization approach (Figure 1(b)) that generates a complete solution based on deep reinforcement learning (RL) (Sutton and Barto 2018) has significant advantages compared to existing decoupled approaches that employ learning only for the discrete skeleton (Figure 1(a)). In terms of efficiency, a joint approach only requires one evaluation of the objective function for each candidate solution, whereas the decoupled approach based on common non-linear black-box optimization methods such as BFGS (Fletcher 2000), simulated annealing (Xiang et al. 1997), or evolutionary algorithms (Storn and Price 1997) requires a significant number of function evaluations to optimize each discrete skeleton. This decoupled approach incurs a high cost for applications such as interpretable control, where each objective function evaluation involves running the candidate solution on many episodes of a highdimensional and stochastic physical simulation (Landajuela et al. 2021a). Furthermore, joint exploration and learning on the full discrete-continuous solution space has the potential to escape from local optima and use information from prior samples to guide the subsequent search.

In this work, we consider discrete-continuous optimization problems that exhibit several key distinguishing features: (1) a black-box reward, (2) a variable-length structure of the design space, and (3) a sequential structure in the form of prefix-dependent positional constraints. These problems are not well-suited to existing joint optimization approaches such as Mixed Integer Programming (MIP) (Fischetti and Jo 2018; Nair et al. 2021) or Mixed Bayesian Optimization (BO) (Daxberger et al. 2019), which are designed for problems with fixed-length discrete components and do not naturally handle positional constraints in the design space. To address these challenges, we draw upon the success of deep reinforcement learning in parameterized action space Markov decision processes (Hausknecht and Stone 2016) to extend existing deep learning methods for discrete optimization (Bello et al. 2016; Zoph and Le 2017; Petersen et al. 2021; Landajuela et al. 2021a) to the broader space of joint discrete-continuous optimization. We summarize the main contributions of this paper as follows:

• We propose a novel method for joint discrete-continuous optimization using autoregressive models and deep reinforcement learning, which we call DisCo-DSO, that is suited for black-box hybrid optimization problems over variable-length search spaces with prefix-dependent positional constraints.   
• We present a novel formulation for decision tree policy search in control tasks as sequential discrete-continuous optimization and propose a method for sequentially finding bounds for parameter ranges in decision nodes.   
• We perform exhaustive empirical evaluation of DisCoDSO on a diverse set of tasks, including interpretable control policies and symbolic regression. We show that DisCo-DSO outperforms decoupled approaches on all tasks.

# Related Work

Hybrid discrete-continuous action spaces in reinforcement learning. The treatment of the continuous parameters as part of the action space has strong parallels in the space of hybrid discrete-continuous RL. In Hausknecht and Stone (2016), the authors present a successful application of deep reinforcement learning to a domain with continuous state and action spaces. In Xiong et al. (2018), the authors take an off-policy DQN-type approach that directly works on the hybrid action space without approximation of the continuous part or relaxation of the discrete part, but requires an extra loss function for the continuous actions. In Neunert et al. (2020), they propose a hybrid RL algorithm that uses continuous policies for discrete action selection and discrete policies for continuous action selection.

Symbolic regression with constants optimization. In the field of symbolic regression, different approaches have been proposed for addressing the optimization of both discrete skeletons and continuous parameters. Traditional genetic programming approaches and deep generative models handle these problems separately, with continuous constants optimized after discrete parameters (Topchy, Punch et al. 2001; Petersen et al. 2021; Biggio et al. 2021). Recent works aim to jointly optimize discrete constants and continuous parameters by relaxing the discrete problem into a continuous one (Martius and Lampert 2016; Sahoo, Lampert, and Martius 2018), or by tokenizing (i.e., discretizing) the continuous constants (Kamienny et al. 2022). The former approach faces challenges such as exploding gradients and the need to revert continuous values to discrete ones. The latter approach tokenizes continuous constants, treating them similarly to discrete tokens, but such quantization is problemdependent, restricts the search space, and requires additional post-hoc optimization to refine the continuous parameters.

Decision tree policies in reinforcement learning. In the domain of symbolic reinforcement learning, where the goal is to find intelligible and concise control policies, works such as Landajuela et al. (2021a) and Sahoo, Lampert, and Martius (2018) have discretized the continuous space and used relaxation approaches, respectively, to optimize symbolic control policies in continuous action spaces. For discrete action spaces, a natural representation of a symbolic policy is a decision tree (Ding et al. 2020; Silva et al. 2020; Custode and Iacca 2023). In Custode and Iacca (2023), the authors use an evolutionary search to find the best decision tree policy and further optimized the real valued thresholds using a decoupled approach. Relaxation approaches find their counterparts within this domain in works such as Sahoo, Lampert, and Martius (2018); Silva et al. (2020); Ding et al. (2020), where a soft decision tree is used to represent the policy. The soft decision tree, which fixes the discrete structure of the policy and exposes the continuous parameters, is then optimized using gradient-based methods.

# Discrete-Continuous Deep Symbolic Optimization

# Notation and Problem Definition

We consider a discrete-continuous optimization problem defined over a search space $\tau$ of sequences of tokens $\tau =$ $\langle \tau _ { 1 } , \dots , \tau _ { T } \rangle$ , where each token $\tau _ { i }$ belongs to a library $\mathcal { L }$ and the length $T$ of the sequence is not fixed a priori. The library $\mathcal { L }$ is a set of $K$ tokens $\mathcal { L } = \{ l _ { 1 } , \ldots , l _ { K } \}$ , where a subset ${ \hat { \mathcal { L } } } \subseteq { \mathcal { L } }$ of them are parametrized by a continuous parameter, i.e., each token $l \in \hat { \mathcal { L } }$ has an associated continuous parameter $\beta \in \mathcal { A } ( l ) \subset \mathbb { R }$ , where $\mathbf { \nabla } \mathcal { A } ( l )$ is the token-dependent range. To ease the notation, we define ${ \bar { \mathcal { L } } } \ { \stackrel { \mathrm { d e f } } { = } } \ { \mathcal { L } } \ \backslash \ { \hat { \mathcal { L } } }$ and consider a dummy range $\mathcal { A } ( l ) = [ 0 , 1 ] \subset \mathbb { R }$ for the strictly discrete

tokens $l \in \bar { \mathcal { L } }$ . Thus, we define

$$
l ( \beta ) = \left\{ l \atop l ( \beta ) \quad \mathrm { i f } l \in \bar { \mathcal { L } } , \forall ( l , \beta ) \in \mathcal { L } \times A ( l ) . \right.
$$

In other words, the parameter $\beta$ is ignored if $l \in \bar { \mathcal { L } }$ . With this notation, we can write $\tau _ { i } = l _ { i } ( \beta _ { i } \bar { ) } \in \mathcal { L } , \forall i \in \{ 1 , \dots , T \}$ . In the following, we use the notation $l _ { i } ( \beta _ { i } ) ~ \equiv ~ ( l _ { i } , \beta _ { i } )$ ] and write $\tau \ = \ \langle \tau _ { 1 } , \ldots , \tau _ { T } \rangle \ = \ \langle l _ { 1 } ( \beta _ { 1 } ) , \ldots , l _ { T } ( \beta _ { T } ) \rangle \ \equiv$ $\langle ( l _ { 1 } , \beta _ { 1 } ) , \dots , ( l _ { T } , \beta _ { T } ) \rangle$ .

Given a sequence $\tau$ , we define the discrete skeleton $\tau _ { \mathrm { d } }$ as the sequence obtained by removing the continuous parameters from $\tau$ , i.e., $\tau _ { \mathrm { d } } = \langle ( l _ { 1 } , \cdot ) , \ldots , ( l _ { T } , \cdot ) \rangle$ . We introduce the operator $\mathsf { e v a l } : \mathcal { T } \to \mathbb { T }$ to represent the semantic interpretation of the sequence $\tau$ as an object in the relevant design space $\mathbb { T }$ . We consider problems with prefix-dependent positional constraints, i.e., problems for which, given a prefix $\tau _ { 1 : ( i - 1 ) }$ , there exists a possible non-empty set of unfeasible tokens $\mathcal { C } _ { \tau _ { 1 : ( i - 1 ) } } \subseteq \mathcal { L }$ such that $\mathtt { e v a l } ( \tau _ { 1 : ( i - 1 ) } \cup \tau _ { i } \cup$ $\tau _ { ( i + 1 ) : T } ) \notin \ \mathbb { T }$ for all $\tau _ { i } \ \in \ \mathcal { C } _ { \tau _ { 1 : ( i - 1 ) } }$ and for all $\tau _ { j } ~ \in ~ { \mathcal { L } }$ with $\dot { \iota } < j \le T$ . Variable-length problems exhibiting such constraints are not well-suited for MIP solvers or classical Bayesian Optimization methods.

The optimization problem is defined by the reward function $R : \mathbb { T }  \mathbb { R }$ , which can be deterministic or stochastic. In the stochastic case, we have a reward distribution $p _ { R } ( r | t )$ conditioned on the design $t \in \mathbb { T }$ and the reward function is given by $R ( t ) ~ = ~ \mathbb { E } _ { r \sim p _ { R } ( r | t ) } [ r ]$ . Note that we do not assume that the reward function $R$ is differentiable with respect to the continuous parameters $\beta _ { i }$ . In the following, we make a slight abuse of notation and use $R ( \tau )$ and $p _ { R } ( r | \tau )$ to denote $\bar { R } ( \mathsf { e v a l } ( \tau ) )$ and $p _ { R } ( r | \mathsf { e v a l } ( \tau ) )$ , respectively. The optimization problem is to find a sequence $\tau ^ { * } =$ $\langle \tau _ { 1 } ^ { * } , \cdot \cdot , \tau _ { T } ^ { * } \rangle = \langle ( l _ { 1 } ^ { * } , \beta _ { 1 } ^ { * } ) , \cdot \cdot \cdot , ( l _ { T } ^ { * } , \beta _ { T } ^ { * } ) \rangle$ (where the length $T$ is not fixed a priori) such that $\tau ^ { * } \in \bar { \mathrm { a r g } } \operatorname* { m a x } _ { \tau \in \mathcal { T } } R ( \tau )$ .

# Method

Combinatorial optimization with autoregressive models. In applications of deep learning to combinatorial optimization (Bello et al. 2016), a probabilistic model $p ( \tau )$ is learned over the design space $\tau$ . The model is trained to gradually allocate more probability mass to high scoring solutions. The training can be done using supervised learning, if problem instances with their corresponding solutions are available, or, more generally, using RL. In most cases, the model $p ( \tau )$ is parameterized by an autoregressive (AR) model with parameters $\theta$ . The model is used to generate sequences as follows.

At position $i$ , the model emits a vector of logits $\psi ^ { ( i ) }$ conditioned on the previously generated tokens τ1:(i 1), i.e., $\boldsymbol { \psi } ^ { ( i ) } = \mathrm { A R } ( \tau _ { 1 : ( i - 1 ) } ; \boldsymbol { \theta } )$ . The new token $\tau _ { i }$ is sampled from the distribution $p ( \tau _ { i } | \tau _ { 1 : ( i - 1 ) } , \theta ) = \mathrm { s o f t m a x } ( \psi ^ { ( i ) } ) _ { \mathcal { L } ( \tau _ { i } ) } ,$ , where $\mathcal { L } ( \tau _ { i } )$ is the index in $\mathcal { L }$ corresponding to node value $\tau _ { i }$ . The new token $\tau _ { i }$ is then added to the sequence $\tau _ { 1 : ( i - 1 ) }$ and used to condition the generation of the next token $\tau _ { i + 1 }$ . The process continues until a stopping criterion is met.

Different model architectures can be employed to generate the logits $\psi ^ { ( i ) }$ . For instance, recurrent neural networks (RNNs) have been utilized in Petersen et al. (2021); Landajuela et al. (2021a); Mundhenk et al. (2021); da Silva et al. (2023), and transformers with causal attention have been applied in works like Biggio et al. (2021) and Kamienny et al. (2022).

Prefix-dependent positional constraints. Sequential token generation enables flexible configurations and the incorporation of constraints during the search process (Petersen et al. 2021). Specifically, given a prefix $\tau _ { 1 : ( i - 1 ) }$ , a prior $\psi _ { \circ } ^ { ( i ) } \in \mathbb { R } ^ { | \mathcal { L } | }$ is computed such that $\psi _ { \circ } ^ { ( i ) } \mathcal { L } ( \tau _ { i } ) = - \infty$ for tokens $\tau _ { i }$ in the unfeasible set $\mathscr { C } _ { \tau _ { 1 : ( i - 1 ) } }$ and zero otherwise. The prior is added to the logits $\psi ^ { ( i ) }$ before sampling the token $\tau _ { i }$ .

Extension to discrete-continuous optimization. Current deep learning approaches for combinatorial optimization only support discrete tokens, i.e., ${ \hat { \mathcal { L } } } = \emptyset$ , (Bello et al. 2016) or completely decouple the discrete and continuous parts of the problem, as in Petersen et al. (2021); Landajuela et al. (2021a); Mundhenk et al. (2021); da Silva et al. (2023), by sampling first the discrete skeleton $\tau _ { \mathrm { d } }$ and then optimizing its continuous parameters separately (see Figure 1(a)). In this work, we extend these frameworks to support joint optimization of discrete and continuous tokens. The model is extended to emit two outputs $\psi ^ { ( i ) }$ and $\phi ^ { ( i ) }$ for each token $\tau _ { i } ~ = ~ ( l _ { i } , \beta _ { i } )$ conditioned on the previously generated tokens, i.e., $\left( \psi ^ { ( i ) } , \phi ^ { ( i ) } \right) = \mathrm { A R } ( ( l , \beta ) _ { 1 : ( i - 1 ) } ; \theta )$ , where we use the notation $( l , \beta ) _ { 1 : ( i - 1 ) }$ to denote the sequence of tokens $\langle ( l _ { 1 } , \beta _ { 1 } ) , \dots , ( l _ { i - 1 } , \beta _ { i - 1 } ) \rangle$ (see Figure 1(b)). Given tokens $( l , \beta ) _ { 1 : ( i - 1 ) }$ , the $i ^ { \mathrm { t h } }$ token $( l _ { i } , \beta _ { i } )$ is generated by sampling from the following distribution:

$$
\begin{array} { r } { p ( ( l _ { i } , \beta _ { i } ) | ( l , \beta ) _ { 1 : ( i - 1 ) } , \theta ) = \left\{ \begin{array} { l l } { \mathcal { U } _ { [ 0 , 1 ] } ( \beta _ { i } ) \mathrm { s o f t m a x } ( \psi ^ { ( i ) } ) _ { \mathcal { L } ( l _ { i } ) } } & { \mathrm { i f ~ } l _ { i } \in \bar { \mathcal { L } } } \\ { \mathcal { D } ( \beta _ { i } | l _ { i } , \phi ^ { ( i ) } ) \mathrm { s o f t m a x } ( \psi ^ { ( i ) } ) _ { \mathcal { L } ( l _ { i } ) } } & { \mathrm { i f ~ } l _ { i } \in \bar { \mathcal { L } } } \end{array} \right. , } \end{array}
$$

where $\mathcal { D } ( \beta _ { i } | l _ { i } , \phi ^ { ( i ) } )$ is the probability density function of the distribution $\mathcal { D }$ that is used to sample $\beta _ { i }$ from $\phi ^ { ( i ) }$ . Note that the choice of $\beta _ { i }$ is conditioned on the choice of discrete token $l _ { i }$ . We assume that the support of $\mathcal { D } ( \beta | l , \phi )$ is a subset of $\mathbf { \nabla } \mathcal { A } ( l )$ for all $l \in \hat { \mathcal { L } }$ . Additional priors of the form $( \psi _ { \circ } ^ { ( i ) } , 0 )$ can be added to the logits before sampling the token $\tau _ { i }$ .

Training DisCo-DSO. The parameters $\theta$ of the model are learned by maximizing the expected reward ${ \cal J } ( \theta ) ~ =$ $\mathbb { E } _ { \tau \sim p ( \tau | \theta ) } [ R ( \dot { \tau } ) ]$ or, alternatively, the quantile-conditioned expected reward $J _ { \varepsilon } ( \theta ) = \mathbb { E } _ { \tau \sim p ( \tau \mid \theta ) } [ R ( \tau ) | R ( \tau ) \geq R _ { \varepsilon } ( \theta ) ]$ , where $R _ { \varepsilon } ( \theta )$ represents the $( 1 - \varepsilon )$ -quantile of the reward distribution $R ( \tau )$ sampled from the trajectory distribution $p ( \tau | \theta )$ . The motivation for using $J _ { \varepsilon } ( \theta )$ is to encourage the model to focus on best case performance over average case performance (see Petersen et al. (2021)), which is the preferred behavior in optimization problems. It is worth noting that both objectives, $J ( \theta )$ and $J _ { \varepsilon } ( \theta )$ , serve as relaxations of the original arg max $R ( \tau )$ optimization problem described above.

To optimize the objective $J _ { \varepsilon } ( \theta )$ , we extend the riskseeking policy gradient of Petersen et al. (2021) to the discrete-continuous setting. The gradient of $J _ { \varepsilon } ( \theta )$ reads as

$$
\nabla _ { \theta } J _ { \varepsilon } ( \theta ) = \mathbb { E } _ { \tau \sim p ( \tau \mid \theta ) } \left[ A ( \tau , \varepsilon , \theta ) S ( ( l , \beta ) _ { 1 : T } ) \mid A ( \tau , \varepsilon , \theta ) > 0 \right] ,
$$

where $A ( \tau , \varepsilon , \theta ) = R ( \tau ) - R _ { \varepsilon } ( \theta )$ and

$$
S ( ( l , \beta ) _ { 1 : T } ) = \sum _ { i = 1 } ^ { T } \left\{ \begin{array} { l l } { \nabla _ { \theta } \log p ( l _ { i } | ( l , \beta ) _ { 1 : ( i - 1 ) } , \theta ) } & { \mathrm { i f ~ } l _ { i } \in \bar { \mathcal { L } } , } \\ { \nabla _ { \theta } \log p ( l _ { i } | ( l , \beta ) _ { 1 : ( i - 1 ) } , \theta ) } \\ { + \nabla _ { \theta } \log p ( \beta _ { i } | l _ { 1 : i } , \beta _ { 1 : i - 1 } , \theta ) } & { \mathrm { i f ~ } l _ { i } \in \hat { \mathcal { L } } . } \end{array} \right.
$$

We provide pseudocode for DisCo-DSO, a derivation of the risk-seeking policy gradient, and additional details of the learning procedure in the appendix.

# Experiments

We demonstrate the benefits and generality of our approach on a diverse set of tasks as follows. Firstly, we introduce a new pedagogical task, called Parameterized Bitstring, to understand the conditions under which the benefits of DisCoDSO versus decoupled approaches become apparent. We then consider two preeminent tasks in combinatorial optimization: decision tree policy optimization for reinforcement learning and symbolic regression for equation discovery.

Baselines. To demonstrate the advantages of joint discrete-continuous optimization, we compare DisCo-DSO with the following classes of methods:

• Decoupled-RL- BFGS, anneal, evo : This baseline trains a generative model with reinforcement learning to produce a discrete skeleton (Petersen et al. 2021), which is then optimized by a downstream nonlinear solver for the continuous parameters. The objective value at the optimized solution is the reward, which is used to update the generative model using the same policy gradient approach and architecture as DisCo-DSO. The continuous optimizer is either L-BFGS-B (BFGS), simulated annealing (anneal) (Xiang et al. 1997), or differential evolution (evo) (Storn and Price 1997), using the SciPy implementation (Virtanen et al. 2020).

• Decoupled-GP- BFGS, anneal, evo : This baseline uses genetic programming (GP) (Koza 1990) to produce a discrete skeleton, which is then optimized by a downstream nonlinear solver for the continuous parameters.

• BO: For the Parameterized Bitstring task, which has a fixed length search space and no positional constraints, we also consider a Bayesian Optimization baseline using expected improvement as acquisition function (Shahriari et al. 2015; Garrido-Mercha´n and Herna´ndez-Lobato 2020).

All experiments involving RL and DisCo-DSO use a RNN with a single hidden layer of 32 units as the generative model. The GP baselines use the “Distributed Evolutionary Algorithms in Python” software1 (Fortin et al. 2012). Additional details are provided in the appendix.

Note on baselines for symbolic regression. In the context of symbolic regression, some of the above baselines corresponds to popular methods in the literature. Specifically, Decoupled-RL-BFGS corresponds exactly to the method “Deep Symbolic Regression” from Petersen et al. (2021), and Decoupled-GP-BFGS corresponds to a standard implementation of genetic programming for symbolic regression $\grave { a }$ la Koza (1994) (most common approach to symbolic regression in the literature).

# Parameterized Bitstring Task

Problem formulation. We design a general and flexible Parameterized Bitstring benchmark problem, denoted $\mathrm { P B } ( N , f , l ^ { * } , \beta ^ { * } )$ , to test the hypothesis that DisCo-DSO is more efficient than the decoupled optimization approach. In each problem instance, the task is to recover a hidden string $l ^ { * } \in \mathsf { \Gamma } [ 0 , 1 ] ^ { T }$ of $T$ bits and a vector of parameters $\beta ^ { * } \in \mathbb { R } ^ { \breve { T } }$ . Each bit $l _ { i } ^ { * }$ is paired with a parameter $\beta _ { i } ^ { * }$ via the reward function $R$ , which gives a positive value based on an objective function $f ( \beta _ { i } , \beta _ { i } ^ { * } ) \in [ 0 , 1 ]$ only if the correct bit $l _ { i } ^ { * }$ is chosen at position $i$ :

$$
R ( \tau , \beta ) \stackrel { \mathrm { d e f } } { = } \frac { 1 } { T } \sum _ { i = 1 } ^ { T } \mathbf { 1 } _ { \tau _ { i } = \tau _ { i } ^ { * } } \left( \alpha + ( 1 - \alpha ) f ( \beta _ { i } , \beta _ { i } ^ { * } ) \right)
$$

The scalar $\alpha ~ \in ~ [ 0 , 1 ]$ controls the relative importance of expending computational effort to optimize the discrete or continuous parts of the reward. The problem difficulty can be controlled by increasing the length $T$ and increasing the nonlinearity of the objective function $f$ , such as by increasing the number of local optima. In our experiment, we tested the following objective functions, which represent objectives with multiple suboptimal local maxima $( f _ { 1 } )$ and discontinuous objective landscapes $( f _ { 2 } )$ :

$$
f _ { 1 } ( x , x ^ { * } ) { \stackrel { \mathrm { d e f } } { = } } \left| { \frac { \sin ( 5 0 ( x - x ^ { * } ) ) } { 5 0 ( x - x ^ { * } ) } } \right| ,
$$

$$
\begin{array} { r } { f _ { 2 } ( x , x ^ { * } ) \stackrel { \mathrm { d e f } } { = } \left\{ \begin{array} { l l } { 1 , \quad } & { | x - x ^ { * } | \leq 0 . 0 5 } \\ { 0 . 5 , \quad } & { 0 . 0 5 < | x - x ^ { * } | \leq 0 . 1 \cdot } \\ { 0 , \quad } & { 0 . 1 < | x - x ^ { * } | } \end{array} \right. } \end{array}
$$

Results. Figure 2 shows that DisCo-DSO is significantly more sample efficient than the decoupled approach when the discrete solution contributes more to the overall reward. This is because each sample generated by DisCo-DSO is a complete solution, which costs only one function evaluation to get a reward. In contrast, each sample generated by the baseline decoupled methods only has a discrete skeleton, which requires many function evaluations using the downstream optimizer to get a single complete solution. As the discrete skeleton increases in importance, the relative contribution of function evaluations for continuous optimization decreases. Note that, given the same computational budget, the BO method performs less function evaluations than the rest of the methods and the final results are worse than DisCo-DSO. This is because BO has a computational complexity of $\mathcal { O } ( n ^ { 3 } )$ (Shahriari et al. 2015), where $n$ is the number of function evaluations. This computational complexity makes BO challenging or even infeasible for large $n$ (Lan et al. 2022).

Decision Tree Policies for Reinforcement Learning Problem formulation. In this section we consider the problem of discovering decision tree policies for RL. We consider $\mathbb { T }$ as the space of univariate decision trees (Silva et al. 2020). Extensions to multivariate decision trees, also known as oblique trees, are possible, but we leave them for future work. Given an RL environment with observations $x _ { 1 } , \ldots , x _ { n }$ and discrete actions $a _ { 1 } , \ldots , a _ { m }$ , we consider the library of Boolean expressions and actions given by $\mathcal { L } \ = \ \{ x _ { 1 } \ < \ \beta _ { 1 } , \ldots , x _ { n } \ < \ \beta _ { n } , a _ { 1 } , \ldots , a _ { m } \}$ , where $\beta _ { 1 } , \ldots , \beta _ { n }$ are the values of the observations that are used in the internal nodes of the decision tree. The evaluation operator $\mathsf { e v a l : } T \to \mathbb { T }$ is defined as follows. We treat sequence $\tau$ as the pre-order traversal of a decision tree, where the decision tokens $( x _ { n } < \beta _ { n } )$ are treated as binary nodes and the action tokens $( a _ { n } )$ are treated as leaf nodes. For evaluating the decision tree, we start from the root node and follow direction

![](images/487efeb168b8724ba3521ef3acec4b394f0d2c7fb8187ebfd4a0f19f20d6691a.jpg)  
Figure 2: Reward of best solution versus number of function evaluations on a parameterized bitstring task, for two continuous optimization landscapes $f _ { 1 }$ and $f _ { 2 }$ and weights $\alpha \ : = \ : 0 . 5 , 0 . 9$ . Solid line corresponds to weight $\alpha = 0 . 9$ , dashed line $\alpha = 0 . 5$ . Mean and standard error over 5 seeds.

$$
D _ { x _ { n } < \beta _ { n } } ( x ) = { \binom { \mathrm { l e f t i f } x _ { n } < \beta _ { n } { \mathrm { ~ i s ~ T r u e , } } } { \mathrm { r i g h t i f } x _ { n } < \beta _ { n } { \mathrm { ~ i s ~ F a l s e , } } } }
$$

for every decision node encountered until we reach a leaf node. See Figure 3 for an example. The reward function is defined as $R ( t ) ~ = ~ \mathbb { E } _ { r \sim p _ { R } ( r | t ) } [ r ]$ where $p _ { R } ( r | t )$ is the reward distribution following policy $t$ in the environment. In practice, we use the average reward over $N$ episodes, i.e., $\begin{array} { r } { R ( t ) \ = \ \frac { 1 } { N } \sum _ { i = 1 } ^ { N } r _ { i } } \end{array}$ , where $\boldsymbol { r } _ { i }$ is the reward obtained in episode $i$ . Prefix-dependent positional constraints for this problem are given in the appendix.

Sampling decision nodes in decision trees. To efficiently sample decision nodes, we employ truncated normal distributions to select parameters $\beta _ { i }$ within permissible ranges. Many RL environments place boundaries on observations, and the use of the truncated normal distribution guarantees that parameters will only be sampled within those boundaries. Additionally, a decision node which is a child of another decision node cannot select parameters from the environment-enforced boundaries. This is because the threshold involved at a decision node changes the range of values which will be observed at subsequent decision nodes. In this way, a previous decision node ”dictates” the bounds on a current decision node. For instance, consider the decision tree displayed in Figure 3. Assume that the observation $x _ { 1 }$ falls within the interval [0, 5] (note that in practice the RL environment provided bounds are used to determine the interval), and the tree commences with the node $x _ { 1 } < 2$ . In the left child node, as $x _ { 1 } < 2$ is true, there is no need to evaluate whether $x _ { 1 }$ is less than 4 (or any number between 2 and 5), as that is already guaranteed. Consequently, we should sample a parameter $\beta _ { 1 }$ within the range $( 0 , 2 )$ . Simultaneously, since we do not assess the Boolean expression regarding $x _ { 2 }$ , the bounds on $\beta _ { 2 }$ remain consistent with those at the parent node. The parameter bounds for the remaining nodes are illustrated in Figure 3. The procedure for determining these maximum and minimum values is outlined in the appendix.

![](images/c26193406a33e928aa8423724a6a7fab674076a929931fa9a23bbfb6f8aef29d.jpg)  
Figure 3: Left: the decision tree associated with the traversal $\langle x _ { 1 } < 2 , a _ { 2 } , x _ { 2 } < 6 , x _ { 1 } < 3 , a _ { 1 } , a _ { 3 } , a _ { 2 } \rangle$ . Right: the corresponding bounds for the parameters during the sampling process (suppose the bounds for observations $x _ { 1 }$ and $x _ { 2 }$ are respectively [0, 5] and [1, 8]).

Evaluation. For evaluation, we follow other works in the field (Silva et al. 2020; Ding et al. 2020; Custode and Iacca 2023) and use the OpenAI Gym’s (Brockman et al. 2016) environments MountainCar-v0, CartPole-v1, Acrobot-v1, and LunarLander-v2. We investigate the sample-efficiency of DisCo-DSO on the decision tree policy task when compared to the decoupled baselines described at the beginning of this section. We train each algorithm for 10 different random

![](images/9771896f6b01efbeb0144088060a51af31478252563745edd6e47798683456e8.jpg)  
Figure 4: Reward of the best solution versus number of function evaluations on the decision tree policy task for LunarLander-v2.   
Figure 6: Average test set reward (left) and number of function evaluations (right) used across methods on the symbolic regression task. Recall that Decoupled-RL-BFGS and Decoupled-GP-BFGS correspond to the methods proposed in Petersen et al. (2021) and Koza (1994), respectively. DisCo-DSO achieves the best average reward on the test set at the lowest number of function evaluations.

seeds.

Results. In Figure 4 (see also the appendix), we report the mean and standard deviation of the best reward found by each algorithm versus number of environment episodes. These results show that DisCo-DSO dominates the baselines in terms of sample-efficiency. The trend is consistent across all environments, and is more pronounced in the more complex environments. The efficient use of evaluations by DisCo-DSO (each sample is a complete well-defined decision tree) versus the decoupled approaches, where each sample is a discrete skeleton that requires many evaluations to get a single complete solution, becomes a significant advantage in the RL environments where each evaluation involves running the environment for $N$ episodes.

Literature comparisons. We conduct a performance comparison of DisCo-DSO against various baselines in the literature, namely evolutionary decision trees as detailed in (Custode and Iacca 2023), cascading decision trees introduced in (Ding et al. 2020), and interpretable differentiable decision trees (DDTs) introduced in (Silva et al. 2020). In addition, we provide results with a BO baseline, where the structure of the decision tree is fixed to a binary tree of depth 4 without prefix-dependent positional constraints. Whenever a method provides a tree structure for a specific environment, we utilize the provided structure and assess it locally. In cases where the method’s implementation is missing, we address this by leveraging open-source code. This approach allows us to train a tree in absent environments, ensuring that we obtain a comprehensive set of results for all methods evaluated across all environments. The decision trees found by DisCo-DSO are shown in Figure 5 (see also the appendix). Comparisons are shown in Table 1. Methods we trained locally are marked with an asterisk $( ^ { * } )$ . Critically, we ensure consistent evaluation across baselines by assessing each decision tree policy on an identical set of 1,000 random seeds per environment.

In Table 1 we also show the complexity of the discovered decision tree as measured by the number of parameters

![](images/f1f70215e1bb359ff92041f824b6a2fc5f2842fdf3eb6579fa9f1e4dfadd5620.jpg)  
Figure 5: Topology of best decision trees found by DisCoDSO on the decision tree policy tasks for Acrobot-v1 and LunarLander-v2.

DisCo-DSO 二 1 Decoupled-RL-BFGS Decoupled-RL-anneal Decoupled-GP-BFGS Decoupled-GP-evo Decoupled-GP-anneal 0.0 0.2 0.4 0.6 0.8 106 107 AverageRtestscore Number of function eval (log scale)

in the tree. We count every (internal or leaf) node of univariate decision trees (produced by all methods except for Cascading decision trees) as one parameter. For Cascading decision trees, the trees contain feature learning trees and decision making trees. The latter is just univariate decision trees, so the same complexity measurement is used. For the leaf nodes of feature learning trees, the number of parameters is number of observations times number of intermediate features. From Table 1, we observe that the univariate decision trees found by DisCo-DSO have the best performance on all environments at a comparable or lower complexity than the other literature baselines.

# Symbolic Regression for Equation Discovery

Problem formulation. Symbolic regression (SR) (Koza 1994; Bongard and Lipson 2007; Petersen et al. 2021; Landajuela et al. 2021b; de Franca et al. 2024) is a classical discrete-continuous optimization problem with applications in many fields, including robotics, control, and machine learning. In SR, we have $\begin{array} { r l } { \mathcal { L } } & { { } = } \end{array}$ $\{ x _ { 1 } , \dotsc , x _ { d } , + , - , \times , \div , \sin , \cos , \dotsc \}$ and $\hat { \mathcal { L } } = \{ \mathrm { c o n s t } ( \beta ) \}$ , where const $( \beta )$ represents a constant with value $\beta$ . The design space is a subset of the space of continuous functions, $\bar { \mathbb { T } } \subset \bar { C } ( V ^ { \mathbb { R } } )$ , where $V \subset \mathbb { R } ^ { d }$ is the function support that depends on $\mathcal { L }$ . The evaluation operator eval returns the function which expression tree has the sequence $\tau$ as pre-order traversal (depth-first and then left-to-right). For example, $\operatorname { e v a l } ( \langle + , \cos , y , \times , \cos ( 3 . 1 4 ) , \sin , x \rangle ) = \cos ( y ) + 3 . 1 4 \times$ $\sin ( x )$ . Given a dataset $D ~ = ~ \{ ( x _ { 1 } ^ { ( i ) } , \ldots , x _ { d } ^ { ( i ) } , y ^ { ( i ) } ) \} _ { i = 1 } ^ { N }$ the reward function is defined as the inverse of the normalized mean squared error (NMSE) between $\boldsymbol y ^ { ( i ) }$ and $\begin{array} { r c l } { \mathsf { e v a l } ( \tau ) ( x _ { 1 } ^ { ( i ) } , \ldots , x _ { d } ^ { ( i ) } ) , \forall i } & { \in } & { \{ 1 , \ldots , N \} } \end{array}$ , computed as 1+N1MSE. SR has been shown to be NP-hard even for low-dimensional data (Virgolin and Pissis 2022). Prefixdependent positional constraints are given in the appendix.

Table 1: Evaluation of the best univariate decision trees found by DisCo-DSO and other baselines on the decision tree policy task. Here, MR is the mean reward earned in evaluation over a set of 1,000 random seeds, while PC represents the parameter count in each tree. For models trained in-house $( ^ { * } )$ , the figures indicate the parameter count after the discretization process. †The topology of the tree is fixed for BO.   

<html><body><table><tr><td rowspan="2">Algorithm</td><td colspan="2">Acrobot-v1</td><td colspan="2">CartPole-v1</td><td colspan="2">LunarLander-v2</td><td colspan="2">MountainCar-vO</td></tr><tr><td>MR</td><td>PC</td><td>MR</td><td>PC</td><td>MR</td><td>PC</td><td>MR</td><td>PC</td></tr><tr><td>DisCo-DSO</td><td>-76.58</td><td>18</td><td>500.00</td><td>14</td><td>99.24</td><td>23</td><td>-100.97</td><td>15</td></tr><tr><td>Evolutionary DTs</td><td>-97.12*</td><td>5</td><td>499.58</td><td>5</td><td>-87.62*</td><td>17</td><td>-104.93</td><td>13</td></tr><tr><td>Cascading DTs</td><td>-82.14*</td><td>58</td><td>496.63</td><td>22</td><td>-227.02</td><td>29</td><td>-200.00</td><td>10</td></tr><tr><td>Interpretable DDTs</td><td>-497.86*</td><td>15</td><td>389.79</td><td>11</td><td>-120.38</td><td>19</td><td>-172.21*</td><td>15</td></tr><tr><td>Bayesian Optimization†</td><td>-90.99*</td><td>7</td><td>85.47*</td><td>7</td><td>-112.14*</td><td>7</td><td>-200.0*</td><td>7</td></tr></table></body></html>

Evaluation. A key evaluation metric for symbolic regression is the parsimony of the discovered equations, i.e., the balance between the complexity of the identified equations and their ability to fit the data. A natural way to measure it is to consider the generalization performance over a test set. A SR method could find symbolic expressions that overfit the training data (using for instance overly complex expressions), but those expressions will not generalize well to unseen data. For evaluating the generalization performance of various baselines, we rely on the benchmark datasets detailed in the appendix.

Results. Results in Figure 6 demonstrate the superior efficiency and generalization capability of DisCo-DSO in the SR setting. In particular, DisCo-DSO achieves the best average reward on the test set and the lowest number of function evaluations. Note that for DisCo-DSO we have perfect control over the number of function evaluations as it is determined by the number of samples $1 0 ^ { 6 }$ in this case). The Decoupled-GP methods exhibit a strong tendency to overfit to the training data and perform poorly on the test set. This phenomenon is known as the bloat problem in the SR literature (Silva and Costa 2009). We observe that the joint optimization of DisCo-DSO avoids this problem and achieve the best generalization performance.

Literature comparisons. In Table 2, we compare DisCoDSO against state-of-the-art methods in the SR literature. In addition to the baselines (Petersen et al. 2021; Koza 1994) described above, we compare against the methods proposed in Biggio et al. (2021) and Kamienny et al. (2022). Since the method in Biggio et al. (2021) is only applicable to $\leq$ 3 dimensions, we consider the subset of benchmarks with $\leq 3$ dimensions. We observe that DisCo-DSO dominates all baselines in terms of average reward on the full test set. For the subset of benchmarks with $\leq 3$ dimensions, DisCoDSO achieves comparative performance to the specialized method in Biggio et al. (2021).

Table 2: Comparison of DisCo-DSO against decoupled baselines and the methods proposed in Biggio et al. (2021) and Kamienny et al. (2022) on the symbolic regression task. Values are mean $\pm$ standard deviation of the reward across benchmarks (provided in the appendix). We group benchmarks because Biggio et al. (2021) is only applicable to $\leq 3$ dimensions. ⋆ Petersen et al. (2021). ⋆⋆ Koza (1994).   

<html><body><table><tr><td>Algorithm</td><td>Dim≤3</td><td>Dim ≥1</td></tr><tr><td>DisCo-DSO</td><td>0.6632±0.3194</td><td>0.7045 ± 0.3007</td></tr><tr><td>Decoupled-RL-BFGS*</td><td>0.6020 ± 0.4169</td><td>0.6400 ± 0.3684</td></tr><tr><td>Decoupled-RL-evo</td><td>0.0324 ± 0.1095</td><td>0.0969± 0.2223</td></tr><tr><td>Decoupled-RL-anneal</td><td>0.1173 ± 0.2745</td><td>0.1436 ± 0.3015</td></tr><tr><td>Decoupled-GP-BFGS**</td><td>0.5372 ± 0.4386</td><td>0.4953 ± 0.4344</td></tr><tr><td>Decoupled-GP-evo</td><td>0.0988 ± 0.1975</td><td>0.0747 ± 0.1763</td></tr><tr><td>Decoupled-GP-anneal</td><td>0.1615 ± 0.2765</td><td>0.1364 ± 0.2608</td></tr><tr><td>Kamienny et al. (2022)</td><td>0.6068 ± 0.1650</td><td>0.5699 ± 0.1065</td></tr><tr><td>Biggio et al. (2021)</td><td>0.6858 ± 0.1995</td><td>N/A</td></tr></table></body></html>

# Conclusion

We proposed DisCo-DSO (Discrete-Continuous Deep Symbolic Optimization), a novel approach to optimization in hybrid discrete-continuous spaces. DisCo-DSO uses a generative model to learn a joint distribution on discrete and continuous design variables to sample new hybrid designs. In contrast to standard decoupled approaches, in which the discrete skeleton is sampled first, and then the continuous variables are optimized separately, our joint optimization approach samples both discrete and continuous variables simultaneously. This leads to more efficient use of objective function evaluations, as the discrete and continuous dimensions of the design space can “communicate” with each other and guide the search. We have demonstrated the benefits of DisCoDSO in challenging problems in symbolic regression and decision tree optimization, where, in particular, DisCo-DSO outperforms the state-of-the-art on univariate decision tree policy optimization for RL.

Regarding the limitations of DisCo-DSO, it is important to note that the method relies on domain-specific information to define the ranges of continuous variables. In cases where this information is not available and estimates are necessary, the performance of DisCo-DSO could be impacted. Furthermore, in our RL experiments, we constrain the search space to univariate decision trees. Exploring more complex search spaces, such as multivariate or “oblique” decision trees, remains an avenue for future research.