# Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning

Xinlu Zhang1\* , Zhiyu Zoey Chen2, Xi $\mathbf { Y e ^ { 3 } }$ , Xianjun Yang1, Lichang Chen4, William Yang Wang1, Linda Ruth Petzold 1

1University of California, Santa Barbara   
2The University of Texas at Dallas   
3The University of Texas at Austin   
4University of Maryland, College Park

# Abstract

Instruction Fine-Tuning (IFT) significantly enhances the zero-shot capabilities of pretrained Large Language Models (LLMs). While coding data is known to boost LLM reasoning abilities during pretraining, its role in activating internal reasoning capacities during IFT remains understudied. This paper investigates a key question: How does coding data impact LLMs’ reasoning capacities during IFT stage? To explore this, we thoroughly examine the impact of coding data across different coding data proportions, model families, sizes, and reasoning domains, from various perspectives. Specifically, we create three IFT datasets with increasing coding data proportions, fine-tune six LLM backbones across different families and scales on these datasets, evaluate the tuned models’ performance across twelve tasks in three reasoning domains, and analyze the outcomes from three broad-to-granular perspectives: overall, domain-level, and task-specific. Our holistic analysis provides valuable insights into each perspective. First, coding data tuning enhances the overall reasoning capabilities of LLMs across different model families and scales. Moreover, while the impact of coding data varies by domain, it shows consistent trends within each domain across different model families and scales. Additionally, coding data generally provides comparable task-specific benefits across model families, with optimal proportions in IFT datasets being task-dependent.

Extended version — https://arxiv.org/abs/2405.20535

# 1 Introduction

Large Language Models (LLMs) have significantly advanced in task generalization by training on diverse text data sources (Touvron et al. 2023a,b; Brown et al. 2020; Jiang et al. 2023) and instruction-finetuning (IFT) further elicits their intrinsic abilities in a zero-shot manner (Ouyang et al. 2022; Longpre et al. 2023; Wei et al. 2022a; OpenAI 2022, 2023; Anthropic 2023). Although previous studies (Peng et al. 2023; Taori et al. 2023; Xu et al. 2023) have improved IFT dataset diversity to better align LLMs with human needs, the impact of specific data types during IFT remains underexplored.

Coding data, with its logical consistency and reduced ambiguity compared to natural text, has been empirically shown to enhance LLM reasoning capabilities during pretraining (Liang et al. 2023; Fu and Khot 2022; Ma et al. 2023; Guo et al. 2024). This enables LLMs to acquire advanced intrinsic knowledge, supporting complex reasoning in real-world applications like text summarization (Yang et al. 2023), numerical problem solving (Luo et al. 2023a; Yue et al. 2023), and knowledge-intensive tasks (Chen et al. 2024). However, at the IFT stage, research primarily shows that coding data tuning improves coding-related in-domain performance (Yuan et al. 2023; Luo et al. 2023b; Ma et al. 2023). The impact on out-of-domain general reasoning capabilities remains underexplored due to complex variations in coding data proportions, model backbones, and reasoning task types (Wei et al. 2022c; Ma et al. 2023; Liang et al. 2023). Therefore, we raise a natural question: How does coding data impact LLMs’ reasoning capacities during the IFT stage?

To thoroughly answer this question, we propose an analysis pipeline to investigate how coding data affects LLMs’ reasoning capacities during IFT, considering coding data proportions, model families, scales, and reasoning domains. We create IFT datasets from ShareGPT (Sharegpt 2023) using ChatGPT (OpenAI 2022) to classify instances as either coding or general text, forming code-centric and general textual datasets. We then generate three IFT datasets with coding data proportions of $0 \%$ , $50 \%$ , and $100 \%$ , maintaining consistent dataset sizes. Six base models of varying families and scales —Llama-1 (Touvron et al. 2023a), Llama-2 (Touvron et al. 2023b), Llama-3 (AI $@$ Meta 2024), Mistral (Jiang et al. 2023), Qwen-1.5 (Bai et al. 2023), and Gemma (Team et al. 2024)—are fine-tuned on these datasets. We evaluate the tuned models on symbolic, logical, and arithmetic reasoning domains. Finally, we analyze coding data impact from three perspectives: overall effectiveness, domain-level influence, and task-specific performance, as shown in Figure 1.

To our best knowledge, this is the first work to thoroughly analyze how coding data affects LLMs’ reasoning capacities during IFT, across different coding data proportions, model families, scales, and reasoning types. We gain significant insights and summarize the main findings for each perspective. Overall effectiveness. As the proportion of coding data for tuning increases, we observe consistent and gradual performance enhancements across different model families and scales. However, improvements vary among model backbones. Compared to tuning on the pure natural text dataset, the greatest overall improvement is achieved on Mistral with a 10.3 percentage points gain, while a more modest 1.5 percentage points gain obtained on Llama-3. Analysis of the models’ responses indicates that coding data tuning enhances overall reasoning capacities for problem-solving, extending beyond in-domain programming skills.

![](images/d54c6c6b173394002ad91101da585d9ee0d16e1295b861d079cd3d11c5ff3e84.jpg)  
Figure 1: Pipeline Overview. The process utilizes ShareGPT data as a starting point. 1. Data Category Classification: Using ChatGPT to classify code instances within ShareGPT to obtain a code-centric IFT dataset. 2. IFT Data Mixture: Constructing three IFT mixture datasets with increasing proportions of coding data. 3. Instruction Finetuning: Fine-tuning LLMs from six families across various scales with these three IFT datasets, respectively. 4. Evaluation: Evaluating the fine-tuned models’ reasoning capacities across three domains. 5. Analysis: Analyzing from three broad-to-granular perspectives.

Domain-level influence. Coding data tuning elicits different reasoning abilities in varied ways. There is marked improvement in the symbolic domain, which includes foundational reasoning skills. However, in arithmetic reasoning, which involves real-world math problems, performance gaps appear compared to models tuned on general textual datasets, addressing more diverse human needs. Additionally, we observe consistent performance trends across various model backbones and sizes within each reasoning domain, indicating the potential transferability of coding data effects during the IFT stage. Further analysis shows that models tuned with coding data can more adeptly apply appropriate skills for solving questions based on different domain properties.

Task-specific performance. Coding data typically yields comparable task-specific benefits across different model families, with a similar number of tasks showing improvement in two out of three reasoning domains. However, obtaining optimizing strategies for mixing coding and natural textual data presents a challenge. While the majority of optimal coding data proportions for improving task performance remain consistent across model families, there is no single coding data proportion setting that consistently enhances task-specific reasoning abilities better than another.

# 2 Related Work

IFT. IFT has proven effective in enhancing pretrained LLMs for zero-shot tasks (Ouyang et al. 2022; Longpre et al. 2023). Wei et al. (2022a); Chung et al. (2022) used IFT datasets from NLP benchmarks, improving generalization but often not fully aligning with real-world user intentions due to simpler instructions. In contrast, Ouyang et al. (2022) developed InstructGPT by tuning on a diverse dataset of real-world instructions and responses, better meeting user needs. Opensource models (Taori et al. 2023; Xu et al. 2023; Chiang et al. 2023; Peng et al. 2023; Zhang et al. 2024) fine-tuned on diverse IFT datasets from strong teacher models have shown that such diversity enhances alignment with complex user intents (Chen et al. 2023). However, the impact of different IFT data types on LLM effectiveness remains underexplored. This work studies the effect of coding data, known for logical clarity and structure, during IFT.

Reasoning in LLMs. Reasoning involves logically analyzing a subject, using evidence and prior knowledge to reach conclusions (Wason and Johnson-Laird 1972; Wason 1968). It has been seen as one of LLMs’ emergent behaviors, shown as models are large enough (Wei et al. 2022b,c). Although improving models’ reasoning capacities shows promising results in different applications (Li et al. 2022; Zhang et al. 2023), comprehensively evaluating these capacities remains challenging due to their complex nature, requiring different fine-grained abilities within different subdomains (Wei et al. 2022c; Ma et al. 2023; Liang et al. 2023; Qiu et al. 2023). We aim to thoroughly evaluate models across the subdomains of symbolic, logical, and arithmetic reasoning to deepen understanding of LLMs’ reasoning capabilities at the IFT stage.

Code in LLMs. Integrating code into LLMs enhances their performance in programming, complex reasoning, and structural knowledge capture (Ma et al. 2023; Liang et al. 2023; Fu and Khot 2022; Wang, Li, and Ji 2023; Madaan et al. 2022). Most investigations focus on the pretraining stage. Ma et al. (2023) shows that LLMs pretrained with coding data outperform those trained with only natural language in both code-related and general tasks. Liang et al. (2023) demonstrates that Codex excels in complex mathematical reasoning, and Madaan et al. (2022) highlights coding data’s role in improving structural reasoning. During the IFT stage, Ma et al. (2023) reveals that coding data enhances in-domain abilities. Conversely, we study how coding data elicits out-of-domain reasoning capacities in pretrained LLMs.

# 3 Experimental Setting

# IFT Data Construction

We use ShareGPT (Sharegpt 2023) as our data source. After deduplication and extraction of the initial round of Human-AI conversations, we obtain a dataset of 45,742 instances. Using GPT-3.5-turbo (OpenAI 2022), we categorize conversations into three groups: Code, Math, and Others. Conversations involving coding data are classified as Code, those related to mathematical concepts and problems as Math, and all other general natural language texts as Others. The categorization results in 10,196 Code, 1,481 Math, and 34,065 Others instances. We exclude the Math category to prevent its potential influence on the model’s reasoning capabilities during tuning. For further experimentation, we select a random subset from the Others category, termed General, equal in size to the Code category, for fair analysis. We establish a Half-half setting by mixing equal portions of data from the Code and General categories. This setup produces three equal-size IFT datasets: General, Half-half, and Code, containing $0 \%$ , $50 \%$ , and $100 \%$ coding data, respectively. The detailed categorization prompts for ShareGPT and coding data, and the corresponding code category diversity analysis are in the Appendix.

# Task Description

To thoroughly assess the models’ reasoning capabilities, we evaluate them on twelve generative tasks, across three reasoning types: symbolic, logical, and arithmetic. Symbolic: We focus on four tasks (Wei et al. 2022c): (1) First Letter Concatenation, (2) Last Letter Concatenation, (3) Reverse List, and (4) Coin Flip. Logical: We utilize four tasks, requiring strong logical ability: (1) Cluttr (Sinha et al. 2019), (2) List Functions (Rule 2020), (3) Babi-Induction and (4) Babi-Deduction (Weston et al. 2015). Arithmetic: Four arithmetic benchmarks are involved to evaluate the mathematics world problem-solving ability (1) GSM8K (Cobbe et al. 2021), (2) SVAMP (Patel, Bhattamishra, and Goyal 2021), (3) ASDiv (Miao, Liang, and $\mathrm { S u } ~ 2 0 2 0 \AA$ ), and (4) MAWPS (Koncel-Kedziorski et al. 2016). For symbolic reasoning, we generate synthetic datasets following Fortes (2023) and balance the representation of difficulty levels for each task. For example, we generate 500 instances for names containing 2 to 4 words in letter concatenation tasks. For other tasks, we evaluate the models with the test sets for each task when publicly available. Otherwise, the development sets are used. Details on data statistics, the reasoning domain selection and synthetic datasets generation are in the Appendix.

# Instruction Fine-tuned LLMs

To systematically assess the coding data impact, we conduct experiments with six distinct LLM families: Llama-1 (Touvron et al. 2023a), Llama-2(Touvron et al. 2023b), Llama-3 (AI@Meta 2024), Mistral-v0.1 (Jiang et al. 2023), Qwen1.5 (Bai et al. 2023), and Gemma (Team et al. 2024). Each model is fine-tuned on the three uniquely composed IFT datasets—General, Half-half, and Code, respectively. Each fine-tuned model is evaluated on the twelve reasoning tasks in three reasoning domains. The training prompt and hyperparameter settings are available in the Appendix.

# Evaluation Setup

Our evaluation operates in a zero-shot setting, where models are prompted to generate responses to corresponding questions without additional context, minimizing external influences. We standardize experimental conditions by limiting the maximum token length to 1024 and employing greedy decoding for all model outputs. Given the variability in output styles, we utilize GPT-3.5-turbo (OpenAI 2022) as an extractor to parse predictions from the generated text. These predictions are subsequently compared to the ground truth using accuracy $( \% )$ for evaluation. The generation prompt for datasets and answer extraction prompt are in the Appendix.

# 4 Experimental Results Results on Overall Performance

We first compare the average results of 12 reasoning tasks between models tuned with different coding data proportions across 6 LLM families. Results are shown in Table 1.

Coding data successfully elicits LLM reasoning capacities in IFT. Overall, we observed a gradual improvement in average accuracy as the proportion of coding data increased during the IFT stage. These consistent gains across different model families clearly demonstrate the benefits of the specialized knowledge that coding data provides, effectively enhancing models’ reasoning capabilities. Notably, for using Mistral-7B-v0.1 as the base model, tuning with Code, achieves a significant absolute performance gain of 10.3 compared to tuning with General. This underscores that datasets rich in code are crucial for eliciting the advanced reasoning abilities of LLMs.

The improvements brought by coding data are divergent across model families. Although different model families show positive effects from tuning with coding data during IFT, the improvements vary across model families. For example, the Mistral-7B-v0.1 and Gemma-7B exhibit substantial

<html><body><table><tr><td>Coding data prop.(%)</td><td>Llama-1-7B</td><td>Llama-2-7B</td><td>Llama-3-8B</td><td>Mistral-7B-v0.1</td><td>Qwen-1.5-7B</td><td>Gemma-7B</td></tr><tr><td>General (0%)</td><td>23.2</td><td>26.7</td><td>41.4</td><td>42.0</td><td>42.2</td><td>22.3</td></tr><tr><td>Half-half (50%)</td><td>25.0</td><td>29.2</td><td>42.9</td><td>43.9</td><td>44.9</td><td>23.7</td></tr><tr><td>Code (100%)</td><td>27.9</td><td>30.6</td><td>42.6</td><td>52.3</td><td>47.4</td><td>30.0</td></tr><tr><td>△</td><td>+4.6</td><td>+3.9</td><td>+1.5</td><td>+10.3</td><td>+5.5</td><td>+ 7.7</td></tr><tr><td></td><td>+20.0%</td><td>+14.5%</td><td>+3.6%</td><td>+24.6%</td><td>+13.0%</td><td>+34.7%</td></tr></table></body></html>

7B 13B (a) Llama-1 (b) Ll0am%a 2 35 40 2350 1 23505 20 General Half-halfCode General Half-halfCode benefits, achieving $2 4 . 6 \%$ and $3 4 . 7 \%$ relative gains, respectively, when fine-tuning with Code compared to General. Conversely, the Llama-3-8B model shows a more modest improvement, achieving a $3 . 6 \%$ relative gain. This disparity could be due to differences in the LLMs pretraining, during which models primarily acquire intrinsic knowledge, rather than the IFT stage (Albalak et al. 2024).

Coding data achieves consistent gains as the LLM scales up. To examine the impact of coding data as LLM size increases, we tune Llama-1 and Llama-2 with 7B and 13B parameters under three coding proportion settings. The results are shown in Figure 2.

![](images/dacc83f15b962a058c2361c739d669c8222ed331a9cff3f1fe58311208b07ffd.jpg)  
Table 1: Overall reasoning comparison of models tuned on General, Half-half, and Code with increasing coding data proportion, across different fam2i5l.i0es. Results show average scores $( \% )$ across 12 reasoning tasks. The Best setting per amosdelkfasmily is in bold. $\Delta$ and $\eta$ indicate absolute and relative gains, respectively, between the best-performing coding data setting and General.   
Figure 2: Overall comparison across different scales. Results show the overall performance of models using Llama-1 and Llama-2 with 7B and 13B parameters as backbones.   
Figure 3: Overall performance comparison across different coding data percentages.

Coding data tuning consistently enhances LLMs’ reasoning capacities across different model families as their size scales up to 13B. This reaffirms the conclusion drawn from smaller model comparisons: coding data can effectively improve LLMs’ reasoning capacities during IFT, helping LLMs align better with user needs and handle complex logical intents. The improvement trends from coding data are similar between 13B models and their corresponding 7B models, highlighting the potential of coding data to enhance instruction fine-tuned models on reasoning tasks for larger models.

Tuning with more coding data can better improve the overall reasoning capacities of LLMs. To emphasize the impact of coding data proportions on overall reasoning capacities, we introduce another IFT dataset with $2 5 \%$ coding and $7 5 \%$ natural text, tune Llama-1 and Llama-2 models (7B and 13B parameters) on it, and evaluate each model on all reasoning tasks. Performance trends across $0 \%$ , $2 5 \%$ , $50 \%$ , and $100 \%$ coding data are shown in Figure 3.

Table 2: Response format transition statistics. Count of instances where the General-tuned model provides incorrect answers in either text or code format (upper header) and the Code-tuned model corrects them (lower header), using Llama-1-13B and Llama-2-13B backbones.   

<html><body><table><tr><td rowspan="2">General Output Code Output</td><td colspan="2">text</td><td colspan="2">code</td><td rowspan="2">total</td></tr><tr><td>text</td><td>code</td><td>text</td><td>code</td></tr><tr><td>Llama-1</td><td>1778</td><td>359</td><td>83</td><td>67</td><td>2287</td></tr><tr><td>Llama-2</td><td>2591</td><td>224</td><td>63</td><td>49</td><td>2927</td></tr></table></body></html>

Across different model families and scales, the overall trend of gradual improvement persists after introducing the new $2 5 \%$ coding data setting, reinforcing the findings from Table 4 that coding data plays a key role in evoking the reasoning capabilities of pretrained LLMs. We also observe that performance at $50 \%$ coding data is not always better than at $2 5 \%$ , and vice versa, likely due to variance among backbones.

Coding data tuning enhances reasoning in natural text responses. To investigate if coding data tuning helps solve these reasoning tasks by merely relying on producing better code, we examine the presence of code in each response under the condition where the model tuned on Code corrects the wrong outcomes of the model tuned on General. We employ GPT-3.5-turbo to detect the presence of code in responses and count the number of different answer format transitions (e.g., General outputs text and Code outputs

7B 13B 50 25 35 40 22 12505 30 三 19 1 20 16 1 10 13 Accuracy (%) 0 10 General Half-half Code General Half-half Code General Half-half Code Llama-1 60 25 50 50 1369 45 山 40 山 40 30 35 20 30 10 25 General Half-half Code General Half-half Code General Half-half Code Llama-2 (a) Symbolic (b) Logic (c) Arithme4c

code). The results with Llama-1-13B and Llama-2-13B as backbone are shown in Table 2. The prompt for determining the response format is in the Appendix.

The Code-tuned model primarily uses pure text responses to correct answers where the General-tuned model is wrong across both model families instead of relying solely on programming skills. Specifically, Code-tuned models succaensds $\begin{array} { r } { \frac { 1 7 7 8 + 8 3 } { 2 2 8 7 } = 8 1 . \dot { 4 } \% } \end{array}$ soifnqLuleastmiao-n2s iunsiLnlganmat-u1- $\begin{array} { r } { \frac { 2 5 9 1 + 6 3 } { 2 9 2 7 } = 9 0 . 7 \% } \end{array}$ ral text responses. Additionally, Code-tuned models do not always use the same answer format as the General-tuned model. They automatically choose different formats to obtain correct answers in $\frac { 3 5 9 + 8 \bar { 3 } } { 2 2 8 7 } = 1 9 . 3 \%$ and $\frac { 2 2 4 + 6 3 } { 2 9 2 7 } = 9 . 8 \%$ of cases using Llama-1 and Llama-2, respectively. These results further illustrate that coding data tuning successfully elicits logical thinking in LLMs beyond just in-domain skills, enabling them to answer complex questions in proper formats and improve reasoning task performance.

# Results of Different Reasoning Domains

Previous discussions have focused on overall performance across reasoning tasks, but distinct domains require specific skills. For instance, symbolic reasoning tasks like letter concatenation need tokenization skills, which benefit more easily from coding data tuning, while logical reasoning requires multi-hop contextual analysis, and arithmetic tasks need enhanced quantitative reasoning (Yue et al. 2023). To pinpoint the capabilities and limitations of coding data tuning across different domains, we analyze per-domain performance using Llama-1 (Touvron et al. 2023a) and Llama-2 (Touvron et al. 2023b) across various model sizes. Average results across four datasets per domain are shown in Figure 4.

Coding data affects each reasoning ability differently. We observe distinct performance patterns in symbolic, logical, and arithmetic reasoning tasks. For symbolic reasoning in Figure 4 (a), the performance improvement from General to Code tuning is significant and steadily increases. This highlights the effectiveness of code-specific data tuning in enhancing the models’ foundational reasoning ability. In logical reasoning tasks shown in Figure 4 (b), while all models benefit from more coding data in the IFT dataset, the gains from increasing coding data from $50 \%$ to $100 \%$ are minor compared to the jump from $0 \%$ to $50 \%$ . This diminishing return indicates that while coding data enhances reasoning skills, its utility is limited for tasks requiring advanced cognitive functions. For Arithmetic reasoning (Figure 4 (c)): The best performance is seen with either General or Half-half, likely due to the need for models to understand diverse intentions in real-world applications (Cobbe et al. 2021). Tuning with a diverse IFT dataset better meets these needs than coding data alone. Despite this, the performance of Code-tuned models remains appealing given the limited data diversity, emphasizing the importance of coding data for LLM tuning.

Within each reasoning domain, models demonstrate similar performance trends. The performance trend within each reasoning domain is consistent across model families, despite variations in architecture and pretraining datasets. This alignment suggests that the underlying factors for eliciting pretrained models’ capabilities could be similar within each domain. Our analysis also shows that larger models follow the same trend as their smaller counterparts within the same family, indicating that scalability does not disrupt the effect of coding data during IFT. These findings underscore the potential for the transferability of coding data’s effect across various LLM backbones and scales at the tuning stage.

![](images/a2f518c38a9c11d1c27dc0b7458cf2f91f8ba2090bdc746d0c0845b7bd57d98e.jpg)  
Figure 5: Results comparison for each dataset across on (a) Llama-1-13B and (b) Llama-2-13B.

Table 3: Proportions $( \% )$ of response transitions across reasoning domains. Transition types from General-tuned to Code-tuned models, represented as ‘General (text/code) $$ Code (text/code)’, across different reasoning domains for Llama-1-13B and Llama-2-13B.   

<html><body><table><tr><td></td><td colspan="4">Llama-1</td></tr><tr><td></td><td></td><td></td><td></td><td>text → text text →code code→→ text code →code</td></tr><tr><td>Symbolic</td><td>58.9</td><td>30.4</td><td>4.8</td><td>5.9</td></tr><tr><td>Logic</td><td>93.2</td><td>4.8</td><td>0.9</td><td>1.1</td></tr><tr><td>Arithmetic</td><td>88.0</td><td>6.5</td><td>4.9</td><td>0.6</td></tr><tr><td colspan="5">Llama-2</td></tr><tr><td></td><td></td><td></td><td></td><td>text → text text →→code code → text code →code</td></tr><tr><td>Symbolic</td><td>85.5</td><td>10.1</td><td>1.1</td><td>3.3</td></tr><tr><td>Logic</td><td>96.4</td><td>3.0</td><td>0.5</td><td>0.1</td></tr><tr><td>Arithmetic</td><td>85.1</td><td>8.6</td><td>0.8</td><td>5.5</td></tr></table></body></html>

Code-tuned models prefer enhancing reasoning with pure text when the General-tuned models output pure text, but preferences diverge across domains when they output coding. We further investigated response transitions where the Code-tuned model corrects errors from the General-tuned model. Table 3 shows the proportions of transition types from General to Code for each domain, using Llama-1-13B and Llama-2-13B as backbones.

We find that for both model families, the proportions of text $$ text’ transitions are consistently and significantly higher than those of text $$ code’ across different domains. This reinforces the finding in section 4 that coding data tuning enhances reasoning task performance by genuinely improving reasoning abilities rather than relying solely on in-domain programming skills. Conversely, when General-tuned models produce incorrect answers involving coding data, Codetuned models do not show a consistent preference for one format over the other. Specifically, code $$ text’ is preferred in 2 out of 6 settings, while code $$ code’ is preferred in 4. These results demonstrate that Code-tuned models can automatically apply appropriate skills to successfully answer questions based on domain properties.

# Task-specific Reasoning Capabilities Analysis

The previous subsection highlights domain-level similarities and divergences, which may become more complex at the task level. To investigate this, we delve into each dataset to explore how coding data impacts task-specific reasoning capabilities. Results for models tuned using Llama-1-13B and Llama-2-13B backbones are presented in Figure 5.

Coding data benefits task-specific abilities in Llama-1 and Llama-2 comparatively in symbolic and logic reasoning, but diverges in arithmetic. We observe that models fine-tuned on datasets incorporating coding (Half-half and Code) demonstrate similar levels of superiority on taskspecific abilities across different model families in symbolic and logic reasoning. Specifically, in symbolic tasks, these models either outperform or match their counterparts, General, in all four tasks on the Llama-1 and three out of four tasks on the Llama-2, respectively. For logical reasoning, the inclusion of coding data benefits all tasks in both model families. However, the impact of coding data on arithmetic reasoning presents more variation. In the Llama-1, the incorporation of coding data yields better or comparable results across all tasks. Conversely, this benefit is observed only in SVAMP and ASDIV tasks for Llama-2. These results indicate that the benefits of coding data on different LLM backbones are more similar than different, with variations emerging based on the nature of tasks within each subdomain.

The optimal proportion and impact of code vary across tasks. We examine the impact of coding data proportions on

Symbolic Ques.on: Reverse the sequence "chewing gum, magazine". Arithme.c Ques.on: Ryan plants 2 flowers a day in his General-only: garden. AUer 15 days, how many flowers does he have if 5 To reverse the order of the words in the given sequence "chewing gum, did not grow? magazine", we need to follow these steps: General-only:   
1. .. 1.…   
2.   
6. Return the final string: "gum, magazine". | Day | Flowers | Code-only: | --- | --- | To reverse the sequence "chewing gum, magazine: | 1 | 2 | \`\`python | 2 | 2 | input_str $\mathbf { \tau } = \mathbf { \tau }$ "chewing gum, magazine" | | | final_str $\mathbf { \tau } = \mathbf { \tau }$ ", ".join(reversed(input_str.split(","))) print(final_str) | 9 | 5 | | 10 | 0 | Output: | | | \` magazine, chewing gum \`\`\` | 14 | 0 | Total $= 0$ flowers that have grown aUer 15 days. Logic Ques.on: Julius is a lion. Julius is yellow. Brian is a rhino. Brian is 3. Therefore, the final answer is 0 flowers that have grown gray. Bernhard is a swan. Bernhard is green. Greg is a lion. Lily is a rhino. aUer 15 days. Greg is white. What color is Lily? Code-only: General-only: To answer this quesQon, we need to set up an equaQon It is not possible for me to accurately determine the color of Lily based that … solely on the provided informaQon… \`\`\` IniQally, $x = 2 5$ \`\`\` Code-only: \`\`\` x + 25 = 30 \`\`\` Lily is a rhino, and the only descripQon we have for a rhino is that it is \`\` x + 25 - 5 = 30 -5 = 30 - x \`\` gray. Therefore, I can conclude that Lily is also gray. \`\` x = 30 - 5 = 25 \`\`\`

task performance across different model families, finding no single proportion consistently superior for enhancing taskspecific reasoning abilities. For Llama-1, the Half-half setting achieves or matches the best performance in eight tasks, while four tasks excel with the Code setting. For Llama-2, Half-half and Code perform best in five and four tasks, respectively. Although Code-tuned models don’t outperform Half-half-tuned models in the number of tasks, they often yield greater improvements in specific tasks. For instance, in Llama-1, the Code setting shows a significant gain of 16.7 in the Babi-Inductive task, compared to the 3.5 gain with Half-half in Babi-Deductive. Similarly, in Llama-2, the Code setting achieves an impressive gain of 25.6 in the First Letter task, far exceeding the 7.0 gain with Half-half in Babi-Deductive. These findings emphasize the strategic consideration of coding proportions to optimize model effectiveness for specific goals during IFT.

The majority of optimal coding data proportions are consistent across model families. When investigating tasks that benefit from coding data in both model families, we find consistent optimal strategies for most tasks. Specifically, out of 8 tasks analyzed, 5 exhibit the same optimal coding data proportions across model families, while 3 require different strategies. This suggests that the proportion of coding data in IFT datasets can similarly enhance task-specific reasoning abilities across different model families, indicating a foundational influence of coding data that is generally model-agnostic. However, variability in the remaining tasks underscores the need for flexible adaptation strategies for each model family.

# Case Study

To show how coding data tuning corrects responses for different reasoning tasks, we focus on instances where the Codetuned model succeeds while the General-tuned model fails, on Llama-1-13B backbone. We randomly select one instance from each reasoning domain in Figure 6.

For the symbolic task, the Code-tuned model uses Python code to correctly reverse the sequence, while the Generaltuned model, confused by phrase construction, fails to execute the reverse action, resulting in an incorrect answer. In the logic task, the General-tuned model fails to follow the inductive reasoning path required for the correct answer. Conversely, the Code-tuned model delivers a concise and accurate answer, demonstrating improved logical capacity due to coding data tuning. For the arithmetic question, both models use structural outputs to answer, but the General-tuned model generates an incorrect structural reasoning path, resulting in a wrong answer. Conversely, the Code-tuned model leverages its in-domain coding capacity by outputting structural math equations to obtain the correct final answer. These case studies showcase how coding data tuning enhances LLM reasoning capacities to properly answer diverse questions.

# 5 Conclusion

We studied the impact of coding data on LLMs’ reasoning capacities during the IFT stage using a multi-perspective approach. Resource limitations are prevented by examining larger models like Llama-2/3 70B and we conduct evaluations focused on generative benchmark tasks within three reasoning domains. We highlighted the general effects of coding data tuning, suggesting that future research should explore detailed aspects such as diverse coding data types, content formats, and low-quality coding data. We hope this work inspires further research on LLMs’ reasoning, including instruction fine-tuning, evaluation, and analysis.