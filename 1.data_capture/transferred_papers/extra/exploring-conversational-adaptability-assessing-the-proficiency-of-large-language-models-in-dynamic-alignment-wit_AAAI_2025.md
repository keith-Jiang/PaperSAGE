# Exploring Conversational Adaptability: Assessing the Proficiency of Large Language Models in Dynamic Alignment with Updated User Intent

Yu-Chuan Chen, Hen-Hsen Huang

Institute of Information Science, Academia Sinica, Taipei, Taiwan cychen@iis.sinica.edu.tw, hhhuang $@$ iis.sinica.edu.tw

# Abstract

This paper presents a practical problem in dialogue systems: the capability to adapt to changing user intentions and resolve inconsistencies in conversation histories. It is crucial in scenarios like train ticket booking, where travel plans often change dynamically. Notwithstanding the advancements in NLP and large language models (LLMs), these systems struggle with real-time information updates during conversations. We introduce a specialized dataset to evaluate LLMbased chatbots on such conversational adaptability by asking a broad range of open-domain questions, focusing on scenarios where users modify their requests mid-conversation. Additionally, as LLMs are susceptible to generating superfluous sentences, we propose a novel, Chain-of-Thought-free evaluation framework to distill the user intention from their responses. Through extensive investigations on four LLMs, we observe that these contemporary LLMs are not well-aligned with the latest user intent in long-term conversations; they often fail to capture the nuances of natural conversations in a zero-shot setting. Interestingly, the results demonstrate that GPT-4, widely recognized as having the most advanced reasoning capabilities to date, is bested by GPT-3.5 in this task. This work aims to improve the practicality of LLM-based chatbots, bridging the gap between the current capabilities of dialogue systems and the fluidity of human interactions.

$\boldsymbol { u } _ { 1 }$ I would like to find a cheap hotel to stay at where there is free parking. Do you have any suggestions? b1 The Cambridge Belfry fits the bill,would you like me to book you a room?   
$u _ { 4 }$ ： What about a 4 star hotel? $b _ { 4 }$ TheCambridgeBelfryisa4 starhotel inthecheap pricerange and ison thewest side.Theyoffer freeWifi   
$u _ { 5 }$ and parking.Would you likea reservation? Yes, please book for 5 people for 5 nights starting on Thursday. b5   
u6 I am unable to book for Tuesday. I would like to book for Thursday, not Tuesday. $b _ { 6 }$   
u7 Iamsorry,Ican't make that reservation either Is Monday available? 67 Asit turns out,Thursday just opened up.I was able to book it for   
$u _ { 8 }$ you,here is your reference number,UGU8T7UB. Thanks! Can you also help me find a college to visit in town? b8   
ug Thereare18 options.Was therea part of townyouwould like to visit? In fact, thanks! Can you also help me find a park to visit in town? bg There are 18 options. Was there a part of town you would like to visit? (several turns later)   
u13 WhattypeofattractionamIinterested in?

# 1 Introduction

In the dynamic flow of a conversation, it is common for speakers to shift their intentions and revise their previously spoken words. Take, for instance, the scenario of customers booking train tickets for travel. Naturally, their initial travel plans are often subject to change during the booking process, influenced by factors like ticket availability or weather conditions. In response to these changes, the booking agent, responsible for understanding and processing the flux of user intent, must promptly update its knowledge and adapt its responses to align with their latest requirements.

As dialogue systems continue to evolve, the number of online customer service interactions handled by NLP models has proliferated. Yet, the ability of these models, including state-of-the-art large language models (LLMs), to accurately and efficiently update information during a conversation remains a significant challenge. The difficulty stems from the need for an LLM to understand the nuances of human communication and dynamically adjust its understanding as the conversation progresses along with the emergence of new information (see Figure 1).

The crux of this issue lies in the model’s ability to discern and align itself with the latest user intent, effectively disregarding or re-contextualizing the outdated information from the earlier conversation. Such a problem intensifies as conversation histories grow longer and changes become more frequent or subtle. The chatbot must continuously track the entire conversation, identify the shifts back and forth, and reconcile any discrepancies in the information flow. It requires advanced capabilities in contextual understanding, memory management, and dynamic response generation, pushing the boundaries of current NLP technologies.

Despite a myriad of tasks proposed to improve the coherence and consistency of chatbots, little attention has been paid to this unique and urgent issue—as LLMs continue to scale up impressively, it becomes prohibitive to integrate this task during training. Without a tailored dataset, it is impossible to gauge the LLMs’ capability, let alone train them using RLHF (Ouyang et al. 2022) or DPO (Rafailov et al. 2023).

To this end, we briefly describe how to generate our dynamic dialogue state tracking (DynDST) dataset. We leverage the MultiWOZ 2.2 dataset (Zang et al. 2020) by identifying all the slots or text span in the dialogue, then we randomly choose one user utterance and alter one of its entities. As shown in Figure 1, college is selected in $u _ { 8 }$ , which, along with the subsequent bot response $b _ { 8 }$ , is considered a false turn. Next, we duplicate the false turn and make necessary changes to obtain the valid update turn $\mathrm { \Delta } \cdot \mathrm { \Delta } u _ { 9 }$ and $b _ { 9 }$ ). Finally, we gather the question $( u _ { 1 3 } )$ inquiring whether the incorrect intent in $u _ { 8 }$ has been overwritten.

The MultiWOZ dataset is a valuable resource for simulating human interactions in the dialogue state tracking (DST) task (Budzianowski et al. 2018; Eric et al. 2020; Zang et al. 2020). However, to evaluate an LLM’s response accurately is challenging since numerous annotation errors persist throughout these datasets (Han et al. 2021; Ye, Manotumruksa, and Yilmaz 2022). We propose a multi-step evaluation framework to extract the user intent from an LLM response without Chain-of-Thought (CoT) or prompt engineering (Kojima et al. 2022). Our pipeline framework is CoT-free, flexible to capture typos, and can apply to proprietary and open-source LLMs: GPT (OpenAI 2023), Gemini (Team et al. 2023), Vicuna (Zheng et al. 2023), and Llama-2 (Touvron et al. 2023). The main contributions are:

• We introduce a practical and challenging task of chatbots to align with users’ latest intents in real-world scenarios, where both updated and incorrect contexts coexist midconversation. We create a specialized 8k dataset and conduct extensive experiments on four LLMs to benchmark this advanced contextual understanding task.1 • We propose a pipeline, prompt-engineering free evaluation method to remove extraneous information from an LLM’s response after an inquiry. Our method remains effective across LLMs even if they can only be inferenced through APIs. It addresses one of the notorious issues that LLMs are prone to generate superfluous sentences in response to open-domain questions and resolves the problem of the damaged gold labels in MultiWOZ 2.2.

# 2 Related Work

This paper lies at the intersection of knowledge editing (KE) and dialogue contradiction detection (DCD). We draw comparisons between those datasets and our DynDST (see Table 1). In Table 1, we count the number of data (# Data) as follows: In DECODE, we select the contradiction data in the dev and test set (i.e., verified by three annotators). In $\mathrm { C a r e C a l l } _ { m e m }$ , we choose the publicly released English version to match the language of others. In DIALFACT, we collect data labeled REFUTED in the validation and test set. In CDCONV, we report # Negative in their paper.

Knowledge Editing (KE) Dataset Meng et al. (2023) use the COUNTERFACT dataset (Meng et al. 2022, derived from Elazar et al. 2021) to evaluate their MEMIT framework, where the data is of the form (subject, relation, object) like $\mathrm {  ~ s } = \mathrm {  ~ \partial ~ }$ Michael Jordan, $\mathbf { r } { } =$ played sport, $\scriptstyle 0 =$ basketball). Mitchell et al. (2022) create the Wikitext generation dataset to evaluate their MEND method on GPT-style models, where the data is a snippet of a paragraph. However, the text does not always directly contradict the other, so it may need further post-processing to ensure this, which is similar to the source paper of zsRE dataset (De Cao, Aziz, and Titov 2021).

Dialogue Contradiction Detection (DCD) Dataset Welleck et al. (2019) construct the DNLI dataset by using three approaches (entity swap, relation swap, and numeric), which is similar to our approach to generate DynDST (we swap the entity). Zheng et al. (2022) point out that it contains two isolated sentences, which is insufficient for capturing the contextual information in dialogue. Nie et al. (2021) create their DECODE dataset that one speaker deliberately contradicts what they said earlier in the conversation; they conduct unstructured and structured approaches for fine-tuning three models (Devlin et al. 2019; Liu et al. 2019; Clark et al. 2020). Zheng et al. (2022) state that most of the contradictions in Nie et al. (2021) fall into the category of History Contradiction, so they propose the CDCONV dataset and include two typical contradictions issues from the chatbot in their dataset: Intra-sentence Contradiction and Role Confusion. Nevertheless, the CDCONV dataset is not suitable for assessing LLMs’ long-term capability.

# 3 Definition

Fact (Intent) The term fact is the text to be edited in a conversation throughout this paper, which is the intent (slot value) in the DST task (see the bold texts in Figure 1). Seeing that the MultiWOZ datasets solely focus on tracking personal status (e.g., booking a train), these facts or intents do not pertain to factual knowledge. We follow the form of fact in Meng et al. (2023), which is a tuple $\tau$ comprising subject, relation, and object. Given a fact $x$ , we define another new fact $x ^ { \prime }$ is valid (i.e., semantically different) if

$$
\tau ( x ^ { \prime } ) \neq \tau ( x )
$$

Specifically, the subject and relation of $\tau ( x ^ { \prime } )$ and $\tau ( x )$ are the same. For instance, the fact of $u _ { 8 }$ in Figure 1 is $\tau ( u _ { 8 } ) =$ $\mathrm {  ~ \omega ~ } _ { \mathrm { s } = \mathrm { I } }$ , $\mathbf { r } =$ askAttractionType, $\scriptstyle { 0 = }$ college), while ${ \tau } ( u _ { 9 } ) = ( \mathrm { s = I } .$ , $\mathbf { r } { = }$ askAttractionType, $\scriptstyle 0 =$ park). Note that the term valid also encompasses the meaning of specific, for we exclude slot values that are (1) multi-intent (“museum and park”) or (2) obscure (“others except college”) during dataset generation. We use the terms fact, intent, and slot value interchangeably.

Table 1: An overview of KE and DCD datasets. There are three input formats: S (sentence), P (paragraph), or C (chat). Lang stands for language. $\mathrm { F } / \lnot \mathrm { F }$ column displays if the dataset contains factual/non-factual knowledge to be edited. Note that KE researchers only focus on factual datasets. Valid is defined in Section 3. LT stands for long-term. We regard the dataset as long-term so long as its average length is at least 5 turns (i.e., 10 utterances). The underlined checkmark $\ L ( \ L \ L \ L )$ denotes the source data that partially satisfies the property. We also report the mean $\mathbf { \tau } ( \mathbf { m } )$ and median (M) number of turns in chat datasets.   

<html><body><table><tr><td>Dataset</td><td>Format</td><td>Lang</td><td>F</td><td>-F</td><td>Valid</td><td>LT</td><td>(m,M)</td><td># Data</td><td>Source</td></tr><tr><td>ZsRE</td><td>S</td><td>en</td><td>√</td><td></td><td>×</td><td>1</td><td>1</td><td>1</td><td>Levy et al. (2017)</td></tr><tr><td>FEVER</td><td>S</td><td>en</td><td>√</td><td>X</td><td>√</td><td>1</td><td>1</td><td>二</td><td>Thorne et al. (2018)</td></tr><tr><td>Dialogue NLI (DNLI)</td><td>S</td><td>en</td><td></td><td>√</td><td>√</td><td>1</td><td></td><td></td><td>Welleck et al. (2019)</td></tr><tr><td>COUNTERFACT</td><td>S</td><td>en</td><td>√</td><td></td><td>√</td><td></td><td>1</td><td>二</td><td>Meng et al. (2022)</td></tr><tr><td>TruthfulQA</td><td>S</td><td>en</td><td>√</td><td>X</td><td>√</td><td>二</td><td>二</td><td>二</td><td>Lin,Hilton,and Evans (2022)</td></tr><tr><td>Wikitext generation</td><td>P</td><td>en</td><td>√</td><td></td><td>√</td><td>1</td><td>1</td><td>1</td><td>Mitchell et al. (2022)</td></tr><tr><td>DECODE</td><td>C</td><td>en</td><td>X</td><td>√</td><td>√</td><td>X</td><td>(4.4, 4.5)</td><td>4,121</td><td>Nie et al. (2021)</td></tr><tr><td>CareCallmem</td><td>C</td><td>ko</td><td>X</td><td>√</td><td>√</td><td>√</td><td>(12.0, 11.5)†</td><td>3,581†</td><td>Bae et al. (2022)</td></tr><tr><td>DIALFACT</td><td>C</td><td>en</td><td>√</td><td>X</td><td>√</td><td>X</td><td>(2.7, 2.5)</td><td>7,298</td><td>Gupta et al. (2022)</td></tr><tr><td>CDCONV</td><td>C</td><td>zh</td><td>X</td><td>√</td><td>√</td><td>X</td><td>(2.0, 2.0)</td><td>4,351</td><td>Zheng et al. (2022)</td></tr><tr><td>DynDST (Ours)</td><td>C</td><td>en</td><td>X</td><td>√</td><td>√</td><td>√</td><td>(7.9, 8.0)</td><td>8.001</td><td></td></tr></table></body></html>

† We report the English version

Conversation A conversation or dialogue with $n$ turns is denoted as $( u _ { 1 } , b _ { 1 } , . . . , u _ { n } , b _ { n } )$ , where $u _ { i }$ and $b _ { i }$ is the user and bot utterance in the $i$ -th turn, respectively. We focus on whether $b _ { n + 1 }$ aligns with the updated fact in a multi-turn conversation when a user inquire an open-domain question related to such fact in $u _ { n + 1 }$ , given a valid fact introduced within the user utterances $\{ u _ { 1 } , u _ { 2 } , . . . , u _ { n } \}$ . Let $u _ { j }$ be a valid fact, where $j \in [ 2 , n ]$ , its previous user utterance $u _ { j - 1 }$ has an invalid (incorrect) fact. Hence, $\tau ( u _ { j } ) \neq \tau ( u _ { j - 1 } ) \dot { }$ . In this paper, we also ensure that $\tau ( b _ { j } ) \neq \tau ( u _ { j - 1 } ) .$ .2 Naturally, a conversation can be classified into four categories (i.e., disjoint turns) in this task: false, update, test, and previous turn. In this task, it is straightforward in a multi-turn conversation since there exists a turn that has the incorrect context where the user wants to correct $( u _ { j - 1 } , b _ { j - 1 } )$ ; there is another turn that the user updates the incorrect context $( u _ { j } , b _ { j } )$ ; there is a turn where we want to evaluate an LLM’s knowledge in this task $( u _ { n + 1 } , b _ { n + 1 } )$ ; and there are other turns unrelated to the user update. As a result, we define: (1) The false turn contains a false intent. (2) The update turn has a user utterance that corrects the previous false intent. (3) The test turn is the question we aim to assess whether the LLM aligns with the latest user intent in the update turn. (4) The rest of the turns fall into the previous turn. For example, in Figure 1, $u _ { 8 }$ and $b _ { 8 }$ are in the false turn; $u _ { 9 }$ and $b _ { 9 }$ are in the update turn; the test turn contains $u _ { 1 3 }$ ; and the rest of the utterances fall into the previous turn $( \{ u _ { 1 } , . . . , b _ { 7 } \} \cup \{ u _ { 1 0 } , . . . , b _ { 1 2 } \} )$ .

# 4 Dataset Generation

We first filter out data that does not have any labeled text span in user utterances in the MultiWOZ 2.2 training set. Setting the random seed to 0, we randomly select one user utterance for each data and obtain the first slot’s value to edit. To obtain another valid entity, we gather the universal set of values from all training data, and then we randomly select one specific entity that is not in the current data. Mathematically speaking, let $\mathcal { D } = \{ d _ { 1 } , d _ { 2 } , . . . \}$ be the training set, $\begin{array} { r } { \mathcal { U } ( \mathcal { D } ) = \dot { \bigcup } _ { i } \mathcal { U } ( d _ { i } ) } \end{array}$ be the set union of all slots’ values in $\mathcal { D }$ . For each $d _ { i }$ and its associated slot $s _ { i }$ with value $\boldsymbol { v } _ { i }$ to be edited, another valid value $\boldsymbol { v } ^ { \prime }$ is picked from $\mathcal { U } ( \mathcal { D } ) \setminus \mathcal { U } ( d _ { i } )$ , where $\boldsymbol { v } ^ { \prime }$ has the same slot name as $s _ { i }$ and $\boldsymbol { v } ^ { \prime } \neq \boldsymbol { v } _ { i }$ .

After each data contains five valid intents, we duplicate the selected user utterance and the subsequent bot utterance and replace the original slot with new values. To further ensure the update turn does not contain the old intent, we (1) prepend five correction phrases to the user utterance (Actually, In fact, In reality, As a matter of fact, and $T o$ tell the truth; also see Section 6.1) and (2) check whether the old intent is still in the bot utterance, and we use the string replacement if necessary.3 Finally, we insert the update turn after the false turn and generate a question related to such changes in the dialogue (one is in Figure 1; the rest are in the Appendix). The size of our DynDST dataset is 8,001.

# 5 Pipeline Evaluation Method for LLMs

Despite an increasing number of works leveraging LLMs to evaluate model outputs via prompt engineering or CoT, these approaches are often slow and lack explainability. On top of that, hand-crafted prompts do not generalize well across LLMs and are difficult to reproduce (costly per inference). Hence, we propose our multi-step exact match framework.

# 5.1 Annotation Errors and Canonical Form

If a slot entity has a “canonical” form, we can fix those typos and other non-trivial labels (e.g., synonyms) by leveraging LLMs trained with rich linguistic knowledge. For example, if the slot value has a typo (say, “Leister”), we can ask GPT-4 to do the answer mapping task (the possible values set is $\{ \mathrm { L e i c e s t e r , E l y , . . . } \}$ in train-departure). Since we select valid intents from the universal set, these typos and non-trivial entities can be converted beforehand. Note that the slot value has its canonical form only if its name has possible values in MultiWOZ 2.2’s schema. We provide the following template for GPT-4 to map a slot value to its canonical form:

“Given slot name, please map the current answer into its corresponding label set from your knowledge. You must output the label only. Do not write explanation. $\backslash \mathfrak { n }$ Slot name: [SN] n Label set: [LS] n Current answer: [CA] n Label of current answer: n”

In this template, [CA] is filled with the slot value (which may be a typo or a synonym not presented in possible values in schema), [SN] is filled with its name, and [LS] is filled with its possible values. For we had pre-processed and removed all invalid intents (see Section 3), this answer mapping task is as simple as classifying “espensive” (a typo) or “high-priced” (a synonym) into expensive, cheap, moderate . Thus, we do not have to train a classification model from scratch, which may be essential if the connection between labels and their canonical forms is opaque.

# 5.2 The Stop Words Set of an LLM

We pre-generate each model’s stop words set by obtaining the outputs in one of our baselines, then we tokenize all of them and remove any token that appears in the entire values set $( \mathcal { U } ( \mathcal { D } )$ in Section 4). After that, we regard tokens with frequency $\geq 1 0 0$ as redundant (the cutoff is pre-defined). Note that even if we can generate each model’s stop words set automatically, we still manually screen the result in case some tokens related to the gold labels are included. This and the NLTK stop words set are merged to form the final one for each LLM’s output (see Step 4 in Section 5.3).

In this paper, each LLM and its families share the same stop words set, regardless of different experiments and configurations. For instance, GPT-3.5 and GPT-4 share the same stop words set, $ { \mathcal { S } } _ { G P T }$ . All stop words set $( S _ { G P T }$ , $\boldsymbol { S _ { G e m i n i } }$ , $\mathcal { S } _ { V i c u n a }$ , and $\ S _ { L l a m a 2 } \ /$ ) are generated and analyzed by 5 runs of Exp. 1 (40,005 outputs) in the default setting (see Section 6.2). $ { \mathcal { S } } _ { G P T }$ , $\boldsymbol { S _ { V i c u n a } }$ , and $ { S _ { L l a m a 2 } }$ are constructed by GPT-3.5, Vicuna (13B), and Llama-2 (13B), respectively.

Step 1 Step 3 old intent expensive expensive expensive new intent moderately-priced moderately-priced moderate   
model output Your preferred price your preferred price your preferred price budgetis moderate. budget is moderate. budget is moderate. Step 1 Step 4 old intent Pizza Hut pizza hut pizza hut new intent theali baba restaurant →thealibaba restaurant ali baba restaurant   
model output The name of the the name of the ali baba restaurant isAli Baba. restaurant is ali baba.

# 5.3 Multi-Step Exact Match Framework

After an LLM’s stop words set is available and every typos and synonyms have their canonical forms, we describe how we combine the exact match (EM), ROGUE-1 (R-1), and ROGUE-L (R-L) to implement the multi-step exact match:

1. Lowercase old intent (old), new intent (new), and model output (output).   
2. Perform EM.   
3. Convert new to its canonical form, then perform EM. (Objective: catch typos and synonyms in gold labels.)   
4. Remove punctuation and stop words in old, new, and output. (Objective: remove redundant words so they will not interfere with the evaluation of R-1 and R-L.)   
5. Perform EM.   
6. Compute R-1 score of new (and old) with output. Label output as new only if (i) new’s $\mathrm { ~ \ F 1 ~ } >$ old’s F1 and (ii) either new’s precision or recall $>$ max 0.5, old’s precision or recall . (Objective: catch long answers such as the address of taxi-destination.)   
7. Compute the metric of edit distance (ED), longest common subsequence (LCSeq, aka R-L), and longest common substring (LCStr) of new (and old) with output. Label output as new only if (i) new’s ED $<$ old’s ED and (ii) new’s $\mathbf { R } { - } \mathbf { L } >$ old’s R-L and (iii) new’s $\mathrm { L C S t r } > \frac { 1 } { 2 } \left| \mathrm { n e w } \right|$ , where $| x |$ is the length of $x$ . (Objective: should catch the typos finally.)

These steps may seem complex, but they are intuitive (see Figure 2): First, we use EM in Step 2. If EM can determine whether the model output contains either the new intent or the old, then we immediately return the result. Next, we attempt to convert the new intent to its canonical form and use EM again in Step 3. If it does not have possible values, we skip this step and proceed to Step 4, where we remove stop words as much as possible lest they interfere with our strict R-1 and R-L evaluation. In Step 4, we only remove the NLTK stop words in the old and new intent, whereas the model’s output is further trimmed by its own stop words set. After this, we apply EM in Step 5 and check if removing these words is sufficient. If not, we resort to using strict R-1 and R-L in Steps 6 and 7, which is necessary because many slots, such as restaurant-food and hotel-name, are not categorical. In Steps 6 and 7, deciding whether the model output contains the old intent follows the same criteria. We do not use a stemmer when computing R-1, and it demonstrates the rigor of our evaluation method. After Step 7, we classify the model’s output as “N.A.” if our evaluation cannot decide whether it has only the new intent or the old one.

# 6 Experimental Setup

Model and Evaluation Metric The configurations of LLMs are: GPT (gpt-4-0125-preview and gpt-3.5-turbo0125), Gemini (gemini-1.0-pro-001), Vicuna (vicuna-33bv1.3, vicuna-13b-v1.5-16k, and vicuna-7b-v1.5-16k), and Llama-2 (Llama-2-13b-chat-hf and Llama-2-7b-chat-hf). Half precision (FP16) is used in Vicuna and Llama-2 due to the limitation of computing resources. We set the temperature to 0 to maximize the reproducibility. Using our exact match pipeline, we report the accuracy (denoted as “Align”), which is used in KE tasks. “No Align” means the output only has an old intent. We ran each experiment five times to stabilize the results (Wang et al. 2023) using 4 RTX 3090 GPUs.

In fact, thanks! Can you also help me find a park to visit in town? Exp. 1 bg There are 18 options. Was there a part of town you would like to visit? ug I'm sorry to bring this up, but I mistakenly gave you thanks! Can you also help me find a college to visit in town? In Exp. 2 fact, thanks! Can you also help me find a park to visit in town? bg No problem at all! I have updated my memory with the correction you provided. Thank you for letting me know. There are 18 options. Was there a part of town you would like to visit?

# 6.1 Multi-Turn Dynamic Alignment Framework

There are eight variants in aligning the latest user intent in the update and test turn: $\{ \mathrm { i m p l i c i t } , \mathrm { e x p l i c i t } \}$ negation of old intent in the update turn $\times \ \{ \mathrm { l o n g , s h o r t } \}$ input in the update $\mathrm { t u r n } \times \{ \mathrm { w i t h , \bar { w i t h o u t } } \}$ options in the test turn.

Two Types of Templates in Update Turn (Implicit or Explicit) Two types of user utterances in the update turn are whether it contains the negation of the old intent (explicit) or not (implicit). We test both scenarios by using the following ten templates ([X] and [Y] are the slots for the old and new user utterances, respectively). See Figure 3 for illustration.

1. Actually, [Y]   
2. In fact, [Y]   
3. In reality, [Y]   
4. As a matter of fact, [Y]   
5. To tell the truth, [Y]   
6. I’m sorry to bring this up, but I mistakenly gave you [X]. In fact, [Y]   
7. Oh, I’m sorry. Should have been [Y], not [X]   
8. Something is wrong with my previous statement. You can correct it by replacing [X] with [Y]   
9. Wrong. It’s not [X], but [Y]   
10. There’s a problem with my previous statement. There’s a mistake on [X]. It should be [Y]

Two Types of Inputs in Update Turn (Long or Short) We consider the naturalness of human conversations, so we propose another method when filling out the above templates. Specifically, it is natural (and concise) to express only the change rather than the entire utterance in our template. For example, in Figure 1, the user may say “Oh, I’m sorry. Should have been park, not college.” or “In reality, park.”

Two Types of Questions in Test Turn (with or without Options) We explore whether appending all possible values to the question helps these LLMs update a user’s intent. For instance, the question with options provided in Figure 1 will be “What type of attraction am I interested in? (Options: architecture, boat, cinema, college, entertainment, museum, multiple sports, nightclub, park, swimming pool, theatre).”

# 6.2 Experiments and Ablation Analysis

Below are the four experiments (without update, implicit update, explicit update, adversarial attack of implicit update):

1. Exp. 0: We test the original MultiWOZ 2.2 data with the question appended to it (i.e., there is no update turn).   
2. Exp. 1: We test the implicit update in the DynDST dataset (template index 1 to 5), as visualized in Figure 3.   
3. Exp. 2: It is the explicit case of update (template index 6 to 10). We additionally insert the pre-defined sequences into the bot utterance, which acts as a mock update: “No problem at all! I have updated my memory with the correction you provided. Thank you for letting me know.”   
4. Exp. 3: This additional experiment is the adversarial attack of Exp. 1, as we swap the false turn and the update.

Align should be high in Exp. 1 and 2, whereas No Align should be high in Exp. 0 and 3. We experiment Exp. 1, Exp. 2, and Exp. 3 with the four variants or settings, which can be viewed as the ablation analysis (see Section 6.1): $\{ \log , \mathrm { s h o r t } \}$ input in the update turn $\times \left\{ \begin{array} { r l } \end{array} \right.$ with, without options in the test turn. We define the default setting as “long input in the update turn” and “with options in the test turn.”

# 7 Results and Discussion

We tabulate complete experiments of GPT-3.5 in Table 2. Table 3 is the result of GPT-4. The complete table results of the other LLMs (Gemini, Vicuna 7B, Vicuna 13B, Vicuna 33B, Llama-2 7B, and Llama-2 13B) are in the Appendix due to the page limit. For we define the default setting as “long input in the update turn” and “with options in the test turn” in Section 6.2, the other three ablation experiments are the removal of (a) options in the test turn, (b) long utterance in the update turn, and (c) both. We also report the upper bound performance of multiple CoTs. For example, in 3 runs of Exp. 2, we pick the top 3 templates (in the update turn) in Align, and we consider the LLM algin in this data if any of the three templates triggers it to output the latest user intent.

We present four aspects to analyze the complete experiments of an LLM in Table 2 (GPT-3.5 as an example):

Table 2: Percentage of Align/No Align on DynDST dataset. Maj stands for majority voting. The ablation analyses are define in Section 6.2. In Exp. 1 and Exp. 2, Align should be high; on the other hand, No Align should be high in Exp. 0 and Exp. 3.   

<html><body><table><tr><td rowspan="2">GPT-3.5 (0125) # run</td><td colspan="3">Align (Maj)</td><td colspan="3">No Align (Maj)</td><td colspan="3">N.A. (↓)</td><td colspan="3">Upper Bound (↑)</td></tr><tr><td>1</td><td>3</td><td>5</td><td>1</td><td>3</td><td>5</td><td>1</td><td>3</td><td>5</td><td>1</td><td>3</td><td>5</td></tr><tr><td>Exp.0 (MultiWOZ 2.2)</td><td>0.0</td><td>0.0</td><td>0.0</td><td>86.7</td><td>88.3</td><td>88.8</td><td>13.3</td><td>11.7</td><td>11.2</td><td>86.7</td><td>88.3</td><td>88.8</td></tr><tr><td>Exp.1(baseline,default)</td><td>66.6</td><td>72.4</td><td>72.6</td><td>20.7</td><td>19.7</td><td>20.6</td><td>12.7</td><td>7.9</td><td>6.8</td><td>66.6</td><td>80.5</td><td>83.6</td></tr><tr><td>(a) w/o options</td><td>62.6</td><td>67.7</td><td>69.4</td><td>20.3</td><td>20.9</td><td>21.8</td><td>17.1</td><td>11.4</td><td>8.8</td><td>62.6</td><td>76.3</td><td>79.7</td></tr><tr><td>(b) w/o long</td><td>46.7</td><td>51.0</td><td>50.5</td><td>39.7</td><td>39.8</td><td>42.5</td><td>13.6</td><td>9.2</td><td>7.0</td><td>46.7</td><td>67.7</td><td>73.6</td></tr><tr><td>(c) w/o both</td><td>42.3</td><td>48.2</td><td>47.1</td><td>39.6</td><td>39.9</td><td>43.5</td><td>18.1</td><td>11.9</td><td>9.4</td><td>42.3</td><td>61.7</td><td>67.1</td></tr><tr><td>Exp.2 (Our, default)</td><td>73.7</td><td>79.9</td><td>80.1</td><td>13.8</td><td>13.2</td><td>14.6</td><td>12.5</td><td>6.9</td><td>5.3</td><td>73.7</td><td>86.2</td><td>88.6</td></tr><tr><td>(a) w/o options</td><td>70.4</td><td>75.1</td><td>78.6</td><td>16.3</td><td>15.4</td><td>14.4</td><td>13.3</td><td>9.5</td><td>7.0</td><td>70.4</td><td>82.4</td><td>86.3</td></tr><tr><td>(b) w/o long</td><td>71.0</td><td>75.8</td><td>76.9</td><td>16.0</td><td>15.9</td><td>17.1</td><td>13.0</td><td>8.3</td><td>6.0</td><td>71.0</td><td>84.0</td><td>88.2</td></tr><tr><td>(c) w/o both</td><td>68.8</td><td>74.3</td><td>76.5</td><td>13.8</td><td>15.7</td><td>16.1</td><td>17.4</td><td>10.0</td><td>7.4</td><td>68.8</td><td>81.7</td><td>86.3</td></tr><tr><td>Exp.3 (Attack,default)</td><td>5.4</td><td>6.7</td><td>7.2</td><td>80.4</td><td>83.8</td><td>84.8</td><td>14.2</td><td>9.5</td><td>8.0</td><td>80.4</td><td>85.2</td><td>86.1</td></tr><tr><td>(a) w/o options</td><td>6.0</td><td>7.0</td><td>7.6</td><td>77.8</td><td>81.8</td><td>82.9</td><td>16.2</td><td>11.2</td><td>9.5</td><td>77.8</td><td>83.6</td><td>85.0</td></tr><tr><td>(b) w/o long</td><td>7.9</td><td>8.7</td><td>8.9</td><td>77.3</td><td>81.7</td><td>83.0</td><td>14.8</td><td>9.6</td><td>8.1</td><td>77.3</td><td>84.1</td><td>85.5</td></tr><tr><td>(c) w/o both</td><td>8.7</td><td>9.5</td><td>10.1</td><td>74.7</td><td>78.9</td><td>79.9</td><td>16.6</td><td>11.6</td><td>10.0</td><td>74.7</td><td>81.9</td><td>83.8</td></tr></table></body></html>

Table 3: Percentage of Align/No Align on DynDST dataset. We do not conduct the ablation analysis in GPT-4 due to the cost.   

<html><body><table><tr><td rowspan="2">GPT-4 (0125) # run</td><td colspan="3">Align (Maj)</td><td colspan="3">No Align (Maj)</td><td colspan="3">N.A. (↓)</td><td colspan="3">Upper Bound (↑)</td></tr><tr><td>1</td><td>3</td><td>5</td><td>1</td><td>3</td><td>5</td><td>1</td><td>3</td><td>5</td><td>1</td><td>3</td><td>5</td></tr><tr><td>Exp. 0 (MultiWOZ 2.2)</td><td>0.0</td><td>0.0</td><td>0.0</td><td>90.3</td><td>92.3</td><td>92.8</td><td>9.7</td><td>7.7</td><td>7.2</td><td>90.3</td><td>92.3</td><td>92.8</td></tr><tr><td>Exp.1(baseline,default)</td><td>58.4</td><td>64.2</td><td>66.5</td><td>24.5</td><td>24.8</td><td>26.2</td><td>17.1</td><td>11.0</td><td>7.3</td><td>58.4</td><td>76.2</td><td>80.3</td></tr><tr><td>Exp.2 (Our, default)</td><td>70.3</td><td>74.6</td><td>76.0</td><td>15.9</td><td>16.6</td><td>17.9</td><td>13.8</td><td>8.8</td><td>6.1</td><td>70.3</td><td>83.3</td><td>86.0</td></tr><tr><td>Exp.3(Attack, default)</td><td>6.0</td><td>6.9</td><td>7.2</td><td>79.8</td><td>84.6</td><td>86.8</td><td>14.2</td><td>8.5</td><td>6.0</td><td>79.8</td><td>86.7</td><td>88.8</td></tr></table></body></html>

• Within one experiment: Investigate each LLM’s overall performance in four different settings and the “width” of Align between the best and the worst. In an ideal situation, the “width” should be small. For instance, the width of GPT-3.5 in Exp. 2 is 3.6 in 5 runs, which is comparably small; surprisingly, it becomes 25.5 in Exp. 1. Though the width of Llama-2 (7B) in Exp. 2 is also small from this perspective (3.9), its best performance in Align is only 49.3, which is below the random guess baseline.

• Compare Exp. 1 and Exp. 2: The differences are in the update turn (see Figure 3). In an ideal situation, there should be no difference in Align. For instance, if we compare side by side, the best settings of Exp. 1 and Exp. 2 are the same (which is the default), and the performance boost is $7 . 5 \%$ in Align. Moreover, we find that there is a significant improvement in (c) setting (near $30 \%$ boost).

• Compare Exp. 1 and Exp. 3: The difference is the order of the update turn and the false turn. In an ideal situation, if a chatbot only replies based on the most recent heuristic, there should be no difference between Align of Exp. 1 and No Align of Exp. 3. There is a $1 2 . 2 \%$ gap in GPT-3.5.

• Compare Exp. 0 and Exp. 3: The difference is if an update intent is falsely inserted before the original intent, which tests if an LLM is robustly trained. In an ideal situation, there should be no difference in No Align.

We also report all LLMs’ best results of Exp. 1 and Exp. 2 in Table 4 and visualized in Figure 4. First and foremost, the best performance of Exp. 2 consistently outperforms that of Exp. 1 across all LLMs, indicating that our approach, combined with the explicit negation of a false fact in the user utterance and the injected sequences in the bot utterance, boosts these LLMs to align with the new user intent in long-term conversation, even if the wrong context is within a conversation. It is simple yet effective, and the improvement of Vicuna (33B) is astonishing: a $1 7 . 5 \%$ boost in Align, almost on par with GPT-4 (76.0). The average boost in GPT, Gemini, and Vicuna (except 33B) is $8 . 2 6 \%$ . As for Llama-2 7B and 13B, the boost is only $5 . 4 \%$ and $3 . 9 \%$ , respectively.

When running our dataset five times and making decisions through majority voting, GPT-3.5 tends to capture the user update by more than $70 \%$ in Exp. 1 and slightly above $80 \%$ in Exp. 2. Moreover, if we compare results side by side, Exp. 2 consistently outperforms Exp. 1 across all settings, which also shows that the worst setting of Exp. 2 (76.5) still outperforms the best of Exp. 1 (72.6) in GPT-3.5. Intriguingly, we find that while there is a common belief that GPT-3.5 is bested by GPT-4 in every task, GPT-3.5 significantly outperforms GPT-4 in this advanced contextual understanding task (see Figure 5 for hypothesis).

<html><body><table><tr><td rowspan="2"># run</td><td colspan="3">Align (↑,Maj)</td><td colspan="3">No Align (↓,Maj)</td><td rowspan="2">Best Set</td></tr><tr><td>1</td><td>3</td><td>5</td><td>1</td><td>3</td><td>5</td></tr><tr><td colspan="7">Exp.1(baseline)</td><td>(d) (d)</td></tr><tr><td>GPT-4 GPT-3.5 Gemini Vicuna (33B)</td><td>58.4 66.6 53.8 48.6 56.8 46.4</td><td>64.2 72.4 57.3 56.4 61.6 54.6</td><td>66.5 72.6 59.9 57.6 65.1 57.2</td><td>24.5 20.7 25.4 26.1 23.6 26.8</td><td>24.8 19.7 26.0 28.5 25.0 29.3</td><td>26.2 20.6 26.0 29.9 24.6 29.8</td><td>(d) (a) (d)</td></tr><tr><td>Llama2 (13B) Llama2 (7B)</td><td>45.3 31.7</td><td>55.6 40.2</td><td>58.9 43.9</td><td>22.2 20.5</td><td>23.8 23.3</td><td>24.2 24.4</td><td>(d) (a) (a)</td></tr><tr><td>Exp.2(Our)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4 GPT-3.5</td><td>70.3 73.7</td><td>74.6 79.9</td><td>76.0 80.1</td><td>15.9</td><td>16.6</td><td>17.9</td><td>(d)</td></tr><tr><td></td><td>59.9</td><td></td><td></td><td>13.8</td><td>13.2</td><td>14.6</td><td>(d)</td></tr><tr><td>Gemini</td><td></td><td>68.4</td><td>68.3</td><td>16.7</td><td>18.6</td><td>20.6</td><td>(b)</td></tr><tr><td>Vicuna (33B)</td><td>67.8</td><td>74.1</td><td>75.1</td><td>15.3</td><td>16.9</td><td>17.5</td><td></td></tr><tr><td></td><td>69.0</td><td></td><td></td><td></td><td></td><td></td><td>(c)</td></tr><tr><td>Vicuna (13B)</td><td></td><td>72.0</td><td>71.8</td><td>15.6</td><td>17.3</td><td>19.7</td><td>(c)</td></tr><tr><td>Vicuna (7B)</td><td>60.8</td><td>65.2</td><td>66.4</td><td>17.2</td><td></td><td>19.9</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>19.0</td><td></td><td>(c)</td></tr><tr><td>Llama2 (13B)</td><td>51.0</td><td>60.5</td><td>62.8</td><td>20.6</td><td>20.2</td><td>21.1</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>(c)</td></tr><tr><td>Llama2 (7B)</td><td>38.8</td><td>46.7</td><td>49.3</td><td>16.2</td><td>17.1</td><td>19.4</td><td>(c)</td></tr></table></body></html>

Table 4: We report the best-performing setting of LLMs in Exp. 1 and Exp. 2. Set stands for setting. Note that (d) is the default setting. N.A. column is excluded here, so the sum of Align and No Align is not 100. We also conduct the complete experiments for Gemini using the single-turn input format. Surprisingly, the best performance in Exp. 1 and Exp. 2 are 73.3 and 80.2, respectively, which are on par with or slightly better than the multi-turn results of GPT-3.5.

![](images/9ef9e0c9d803a7b9066ffe97d7c45764667bf9aa347d8a998fd651ea53e0b0b5.jpg)  
Figure 4: The best setting of Exp. 2 consistently outperforms that of Exp. 1 across all LLMs. GPT-4 is outrun by GPT-3.5 in both experiments. In Vicuna, 13B beats 33B in Exp. 1, while it is the other way around in Exp. 2. Vicuna (7B) is on par with Llama-2 (13B) in Exp. 1, outstripping it in Exp. 2.

In Vicuna, the overall performance increases as the model size increases in Exp. 2. However, this trend does not hold in Exp. 1, where the 13B model outperforms the 33B model. There are other interesting findings in Vicuna 7B and 13B LLMs: We observe that the best-performing setting in Exp. 1 for these models is the default setting. Conversely, in Exp. 2, the best-performing setting is (c), the most natural dialogue from a human perspective (only expresses the change and does not provide options in the test question). As for Llama-2, the best performance of Align in Exp. 1 is (a), whereas setting (c) performs best in Exp. 2.

![](images/7f27a7f06990d8751c7a2835e6cde7033cd22f33cfeeb7eff30b0064d2341fa0.jpg)  
Figure 5: Robustness is defined as No Align Align. We conjecture that GPT-4 is more robustly trained to alleviate malicious attacks and increase its safety. Nevertheless, this might hamper its ability to align with the latest user intent.

# 8 Conclusion

We tackle a key challenge limiting the practicality of contemporary LLMs for end users: aligning with the updated user intent when incorrect contexts persist mid-conversation. To gauge this conversational adaptability of LLMs on opendomain questions, we created an 8k dataset, DynDST. Built upon MultiWOZ 2.2, our dataset reflects the process of human interactions in real-world scenarios. Since LLMs tend to respond with lengthy sentences, we propose a multi-step framework for evaluation. Our exact match pipeline framework is robust across LLMs and does not rely on any CoT or prompt engineering. Through extensive experiments on four LLMs of varying sizes, we made several interesting observations: (1) A simple yet effective solution is to insert the negation of incorrect text in the update turn and a mock update in the bot utterance. (2) GPT-3.5 exceeds the performance of GPT-4 in both implicit and explicit update experiments (Exp. 1 and Exp. 2), possibly due to GPT-4 trained to mitigate malicious attacks. (3) In Gemini, the multi-turn input format significantly underperforms in both dynamic alignment experiments compared to the single-turn format, which performs slightly better than the multi-turn format of GPT-3.5 in both experiments. (4) Vicuna surpasses Llama-2 in this task, with its 33B model being on par with GPT-4 in Exp. 2. (5) In Exp. 2, Vicuna and Llama-2 unanimously perform best in the most natural conversational setting, while GPT-3.5 is better when provided with the entire user utterance and options. The results shed light on this novel and essential contextual understanding task from different perspectives. Our future work will be to apply these insights to DPO to build a better, well-aligned chatbot.

# Ethical Statement

Any LLM should not be used for fact-checking, even though we analyze these LLMs’ outputs as definite answers. When approaching this task, researchers should be aware of the difference between mechanical parroting and genuine understanding. Since our DynDST dataset is built upon the MultiWOZ 2.2, we do not foresee any ethical issues in it.