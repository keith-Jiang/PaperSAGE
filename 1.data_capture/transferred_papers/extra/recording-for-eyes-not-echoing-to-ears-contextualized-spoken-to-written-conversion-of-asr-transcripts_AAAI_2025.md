# Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts

Jiaqing ${ { \bf { L i u } } ^ { 1 } }$ , Chong Deng1, Qinglin Zhang1, Shilin Zhou2, Qian Chen1, Hai $\mathbf { Y } \mathbf { u } ^ { 1 }$ , Wen Wang

1Tongyi Lab, Alibaba Group 2School of Computer Science and Technology, Soochow University {mingzhai.ljq,dengchong.d,qinglin.zql,tanqing.cq,yuhai.yu,w.wang}@alibaba-inc.com {slzhou.cs} $@$ outlook.com

# Abstract

Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and various spoken language phenomena such as disfluencies, ungrammatical sentences, and incomplete sentences, hence suffering from poor readability. To improve readability, we propose a Contextualized Spokento-Written conversion (CoS2W) task to address ASR and grammar errors and also transfer the informal text into the formal style with content preserved, utilizing contexts and auxiliary information. This task naturally matches the in-context learning capabilities of Large Language Models (LLMs). To facilitate comprehensive comparisons of various LLMs, we construct a document-level Spoken-to-Written conversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study the impact of different granularity levels on the CoS2W performance, and propose methods to exploit contexts and auxiliary information to enhance the outputs. Experimental results reveal that LLMs have the potential to excel in the CoS2W task, particularly in grammaticality and formality, our methods achieve effective understanding of contexts and auxiliary information by LLMs. We further investigate the effectiveness of using LLMs as evaluators and find that LLM evaluators show strong correlations with human evaluations on rankings of faithfulness and formality, which validates the reliability of LLM evaluators for the CoS2W task.

Datasets — https://github.com/alibaba-damoacademy/SpokenNLP/tree/main/swab Extended version — https://www.arxiv.org/abs/2408.09688

# Introduction

As can be seen in Figure 1, since ASR transcripts aim to provide verbatim transcriptions of oral communications, they often exhibit various spoken language phenomena and informal styles, such as filler words, repetitions, repairs, and fragments, and include ASR errors and ungrammatical text. These characteristics lead to poor readability. To enhance the readability of ASR transcripts, we propose the Contextualized Spoken-to-Written conversion (CoS2W) task, which aims to correct ASR errors and grammatical errors and transfer the informal style to the written and formal style while

Podcast Introduction (auxiliary) Discussing Nolan’s movie Oppenheimer with Kerry Lee and Gill Wang … ASR Transcript (source) Hello Ms. Carrie. I am Jill. What topic are we discussing today? Everyone, seeing the title, you already know. A blockbuster movie. Nolan Oppenheimer. Yes, aha, Oppenheimer, it feels. This movie has achieve very high box office. CoS2W Record (target) Hello Ms. Kerry. I'm Gill. As everyone can tell from the title, today we will discuss Nolan's blockbuster movie— "Oppenheimer", which has achieved very high box office revenue.

preserving the content, that is, the CoS2W task is designed to convert verbatim transcripts (“echoing to ears”) into readable documents (“recording for eyes”). Note that different domains vary in their requirements for adjustments on ASR transcripts. Some domains only allow removal of filler words while some domains (e.g., speech analysis for diagnosis) require verbatim transcripts. Nonetheless, the readable documents of CoS2W task are particularly helpful for many domains such as podcasts, education, and meetings (business, project, etc.), where efficiency in information delivery and knowledge acquisition is essential.

Moreover, for a wide variety of downstream tasks on ASR transcripts, such as machine translation, summarization, question-answering (Gupta et al. 2021), and sentiment classification (Zhang et al. 2023), even competitive models (including LLMs) perform substantially better on written text than on transcripts, and the performance could benefit substantially from applying CoS2W to ASR transcripts. For example, for machine translation of ASR transcripts, we employ the competitive CSANMT model1 on the development set of the speech translation BSTC dataset (Zhang et al. 2021b) and find that compared to directly translating ASR transcripts, applying CoS2W (with GPT-4) on ASR transcripts then translating improves the BLEURT (Sellam, Das, and Parikh 2020) score from 57.07 to 61.9.

Although the CoS2W task could improve both readability and performance of downstream tasks of ASR transcripts, there is a lack of research efforts on this task due to its significant challenges, including task complexity, effective exploitation of contexts, understanding the impact of granularity, the challenge in evaluations, and scarcity of labeled data, which we will detail below.

Task complexity is challenging because CoS2W combines many subtasks such as ASR error correction, Grammatical Error Correction (GEC), and text style transfer (i.e., informal spoken language expressions to the formal style). These subtasks are not simply pipelined but are interconnected: all subtasks are based on a complete semantic understanding of the input and have dependencies among them. ASR and grammar errors can alter semantics, thereby affecting the performance of text style transfer. On the other hand, text style transfer could benefit GEC as GEC performs better on formal texts than spoken texts.

Understanding the impact of granularity is necessary. Traditional text style transfer focuses on sentence-level conversion. Conversion at a lower granularity (e.g., paragraphlevel) instead of sentence-level, may achieve larger improvements to readability, as shown in Figure 1. However, reducing granularity may introduce faithfulness issues and output limitation as discussed in the results section.

Effective exploitation of contexts is critical to understand speech transcripts, thus we define CoS2W as contextualized. For example, context is helpful to grasp the meanings of fragments in meetings with frequent speaker interactions. Some domains also provide auxiliary information like introductions of podcasts, which could benefit ASR error correction as it may contain relevant named entities, domain terms, and words with challenging pronunciations. Essentially, the auxiliary information can be considered as extended context. Effectively leveraging auxiliary information to enhance the CoS2W performance poses a challenge.

Challenge in evaluations arises due to its multifaceted nature, requiring different evaluation methods for its subtasks. Moreover, tasks such as informal-to-formal style transfer remain highly subjective and predominantly depend on human evaluations despite the existence of objective metrics. LLMs offer a promising opportunity for automatic evaluations. However, leveraging LLMs as reliable evaluators for CoS2W poses many challenges, including designing appropriate prompts, mitigating bias, and ensuring the accuracy, fairness, and reproducibility of the LLM evaluators.

Scarcity of labeled data for CoS2W hinders model evaluations and efforts to tackle the challenges discussed above.

The advent of LLMs offers new prospects for addressing the challenges of CoS2W. Firstly, the contextualized CoS2W task matches well with LLMs’ in-context learning abilities. Secondly, the CoS2W task considers an output satisfactory as long as it is free of ASR and grammar errors, faithful to the input in semantics, and formal (i.e., close to written text). The generative nature of the problem definition also aligns with LLMs. Thirdly, LLMs have been explored as evaluators for many tasks (Li et al. 2024; Lai, Toral, and Nissim 2023) and achieved notable performance.

Our contributions can be summarized as follows:

• We propose the Contextualized Spoken-to-Written conversion (CoS2W) task to improve both readability of ASR

transcripts and performance of downstream tasks. To promote research in this field, we construct and make available the document-level Spoken2Written of ASR transcripts Benchmark (SWAB) dataset with manual annotations, covering meeting, podcast, and lecture domains in both Chinese and English languages. • We investigate various methods for LLMs to utilize contexts and auxiliary information for CoS2W and provide insights into future research directions. • We find that LLM evaluators show strong correlations with human evaluations on rankings of faithfulness and formality and also analyze their strengths and weaknesses as CoS2W evaluators.

# Related Work

ASR Error Correction task aims to correct misrecognitions within ASR transcripts. Recent studies (Min and Wang 2023; Yang et al. 2023) have investigated LLMs’ effectiveness in ASR error correction, using varied prompt strategies like Chain of Thought (CoT). LLMs show potential for this task, but their free-generation paradigm often causes unnecessary changes, like paraphrasing error-free text, affecting performance on metrics like Word Error Rates (WER). This problem would also be an issue for the CoS2W task.

In this task, auxiliary information helps resolve challenging pronunciations, such as N-best transcripts (Ma et al. 2023), dialogue history (Mai and Carson-Berndsen 2023), video titles and descriptions (Lakomkin et al. 2024), rare words (He, Yang, and Toda 2023), etc. However, this auxiliary information is often brief, comprising just a few sentences or word lists. And studies combining auxiliary information with LLMs for this task are relatively scarce.

Grammar Error Correction task entails correcting textual grammatical mistakes. Similar to ASR error correction, LLM performance in GEC isn’t fully satisfactory (Zhang et al. 2023; Fang et al. 2023). GEC datasets usually follow minimum change principle, clashing with LLMs’ relatively free generation paradigm. Disregarding objective metrics, human evaluations confirm LLMs’ GEC effectiveness (Li et al. 2023), which suggests current metrics don’t evaluate LLMs fairly and reliably. Therefore, Li et al. (2024) aim to refine GEC evaluation methods, using LLMs to categorize edits and calculate metrics like $F _ { 0 . 5 }$ to evaluate models.

Most GEC task corpora are sentence-level, sourced from second-language learners, and categorized into error-coded and direct rewriting paradigms. Table 1 lists common datasets: For English, FCE (Yannakoudakis, Briscoe, and Medlock 2011) and AESW (Daudaravicius et al. 2016) use error-coded, while JFLEG (Napoles, Sakaguchi, and Tetreault 2017) and WI-LOCNESS (Bryant et al. 2019) use direct rewriting. Chinese datasets include NLPCC18 and Lang-8 (Zhao et al. 2018), CGED (Rao, Yang, and Zhang 2020) from HSK essays, and re-annotated YACLC (Wang et al. 2021) and MuCGEC datasets (Zhang et al. 2022).

Text Style Transfer task aims to modify text to a specific style (e.g., informal to formal, negative to positive) while preserving content. The CoS2W task focuses on informal to formal transfer. Recent studies (Reif et al. 2022; Tao et al. 2024) have employed LLMs to improve text style transfer quality and achieve diverse style conversions. In addition, Lai, Toral, and Nissim (2023) employ the LLM as a multidimensional evaluator and find that it achieves competitive correlations with human evaluation compared to existing automatic metrics, especially in terms of content preservation. Spoken-to-Written task transforms ASR transcripts into formal and readable text, which was initially proposed for Japanese (Ihori, Takashima, and Masumura 2020) and has later been extended to Chinese (Guo et al. 2023).

Table 1: Comparison between SWAB and other datasets.   

<html><body><table><tr><td>Dataset</td><td>#Sents</td><td>Language& Domain</td></tr><tr><td>FCE AESW JFLEG</td><td>34.0K 1489.2K</td><td>EN FCE Exam Essay ENJournal Articles</td></tr><tr><td>WI-LOCNESS NLPCC18</td><td>1.5K 43.1K 2.0K</td><td>EN TOFEL Exam EN Website, Essay CH Essay</td></tr><tr><td>Lang-8 CGED</td><td>717.0K 7.2K</td><td>CHLanguage-learning Website CHHSK Exam</td></tr><tr><td>YACLC MuCGEC</td><td>32.1K 7.1K</td><td>CHWebsite CHEssay,HSKExam,Website</td></tr><tr><td>Japanese S2W CS2W</td><td>18.2K 7.2K</td><td>JA Conversation,Voicemail CH Telephone Conversation</td></tr></table></body></html>

Different from previous work, the CoS2W task emphasizes the contextualized ability, aiming to convert whole paragraphs across multiple sentences rather than single sentences, with the help of context and auxiliary information. It will further enhance the readability of the results, as illustrated in Figure 1. And it aligns well with the in-context learning capabilities of LLMs. To support this research, we construct the SWAB dataset and report the experiment and evaluation results of LLMs. Additionally, our research spans multiple scenarios and languages as shown in Table 1.

# The SWAB Dataset

To conduct experiments and evaluations of the CoS2W task, we construct the Spoken2Written of ASR Transcripts Benchmark (SWAB) dataset, including multiple scenarios (i.e., podcasts, meetings, lectures) in both Chinese and English. There are 60 transcripts with auxiliary information, with each subcategory comprising 10 documents. The SWAB provides document-level annotations for entire transcripts, offering an annotated target for each paragraph. Furthermore, we provide links to the original audio or video with timestamps to support multi-modal research.

# Data Source

We collect Chinese and English data by leveraging the abundant data resources available. And we select three typical scenarios (i.e., meeting, podcast, and lecture) where efficiency in information delivery is essential. Among them, meetings are high-frequency interactive discussions among multiple participants; podcasts typically involve chats and interviews between two or more individuals; and lectures are monologues from a single person. Essentially, these domains differ significantly in interactivity, thus varying from written texts and posing different challenges to the CoS2W task.

Meetings are sourced from open-source meeting corpora with manually annotated information as auxiliary information. Chinese meetings are sampled from the training dataset of the AliMeeting corpus (Yu et al. 2022). We select the title and manually annotated topic titles (i.e., sub-topics) as auxiliary information. English Meetings are sampled from the AMI corpus (Carletta et al. 2005). We choose third-party annotated abstractive summarization as auxiliary information, which includes sections on decisions, action items, etc.

Podcasts are derived from many Chinese and English podcast programs covering a multitude of topics on YouTube2. We collect the podcast introduction as auxiliary information, which provides rich background knowledge such as guest names, specialized terminology, and discussion topics.

Lectures are sourced from many Chinese and English individual speeches on YouTube, covering a variety of topics. Both Chinese and English lectures are provided with metainformation of the speeches as auxiliary information, which includes the name and biography of the speaker, as well as the title, category, and description of the video.

# Dataset Construction

We design a relatively free annotation paradigm like direct rewriting of GEC. Targets are considered “correct” as long as they fix ASR and grammatical errors, and adopt a written and formal style while faithfully preserving the original content. We also ensure paragraph-level consistency between source and target to enable flexible content division and fair comparison at various granularity levels.

All collected data are first transcribed using a competitive ASR system (Gao et al. $2 0 2 2 ) ^ { 3 }$ , then the ASR 1-bests are processed with punctuation insertion, paragraph segmentation (Zhang et al. 2021a), and speaker diarization (Zheng et al. 2023). This ASR structure is widely utilized and operates at peak performance while preserving the overall generality of the dataset. We utilize GPT-4 (gpt-4-0125-preview)4 to obtain the initial target of CoS2W for the SWAB dataset. The input is chunk-level to leverage local context and to reduce token consumption. The auxiliary information is also provided to enhance the ASR error correction performance. The prompt for this procedure is in the arXiv version.

The results of GPT-4 still contain some issues (as shown in our analysis). Consequently, we use human annotation for manual revisions. We recruit more than ten college-educated Chinese annotators. Each annotator must undergo comprehensive training to understand the annotation guidelines and meet the accuracy standards on test data before they can officially begin annotating. Given the transcription (source), model results (target), and auxiliary information, each qualified annotator must thoroughly review and annotate the entire transcript from beginning to end, to ensure consistency at the document level. They are responsible for (1) ensuring paragraph-level consistency between source and target, (2) correcting any remaining ASR errors and grammatical errors, and (3) guaranteeing that the content preserves the original content and embodies a written and formal style. During the annotation process, any confusing cases will be discussed, and the annotation guidelines and examples will be continuously updated to ensure clarity and accuracy.

Quality assurance is managed by three senior annotators. They will conduct subjective quality checks on $10 \%$ to $20 \%$ of the sampled paragraphs and utilize heuristic methods to perform rule-based inspections on all paragraphs. For documents with an error rate of less than $10 \%$ , identified and similar mistakes must be corrected. For documents with an error rate greater than $10 \%$ , a complete revision is required until the document passes the inspection.

We observe that the CoS2W task shows significant diversity. Due to labeling costs, we currently provide only one target but aim to expand target diversity in future work. Therefore, some objective metrics may not adequately reflect the performance. As a result, we have adopted metrics that focus more on semantics and introduced evaluations by LLMs and humans to mitigate the limitations of a single target.

GPT-4 Align Paragraph Chunk-level Local Context Remove Errors Given Auxiliary Polish Results Data LLM CoS2W Human Annotation Check

# Dataset Analysis

The target length of the final results is streamlined to $8 5 . 4 0 \%$ of the source length. Additionally, annotators adjust $1 0 . 4 3 \%$ of the paragraph segmentation results to improve segmentation and ensure paragraph-by-paragraph consistency between the source and target. In total, $2 0 . 0 9 \%$ of the paragraphs are identified and corrected for ASR errors. Within the GPT-4 results, $1 1 . 2 9 \%$ of the paragraphs still contain ASR errors and are further modified manually by annotators. Only $2 \%$ of the paragraphs in the GPT-4 results have grammatical errors to be corrected manually. Annotators further optimize $2 0 . 2 6 \%$ paragraphs of GPT-4 to achieve a better written and formal style. Note that further optimization does not imply errors in the formal style of GPT-4 results.

# Method

We compare the performance on the CoS2W task of different LLM across various granularity levels, and introduce contexts and auxiliary information to enhance model performance. To evaluate comprehensively, we employ automatic metrics, the LLM Evaluator, and human evaluation.

# Different LLMs

To assess different LLMs’ abilities in solving this complex task, we choose 4 typical LLMs across both open-source and closed-source models. The open-source LLMs selected are QWen- $1 4 \mathrm { B } ^ { 5 }$ and LLaMA3- $\cdot 8 \dot { \mathbf { B } ^ { 6 } }$ , chosen for their suitable computational demands for future SFT-related work. For closed-source LLMs, we pick GPT-4 (gpt-4-0125-preview)7 and QWen-Max (qwen-max-0107)8, to obtain the state-ofthe-art performance of LLMs on this task.

# Different Granularity Levels

To explore the impact of granularity level, we construct comparisons at the document level, chunk level, and paragraph level, ranging from coarse to fine granularity. Documentlevel granularity requires the model to rewrite all paragraphs of one entire document, using paragraph indexes to correlate the source and target. At the chunk-level granularity, we divide the document into chunks based on length (1.5K tokens). The model is required to rewrite each paragraph within the chunk on a paragraph-by-paragraph basis. For paragraph-level granularity, we conduct experiments focusing on the revision of individual paragraph content. Furthermore, we set chunk-level granularity as the baseline, as it allows the use of local context while requiring fewer tokens.

# Context

Context is essential in text understanding, especially for spoken ASR transcripts. We divide context into local context (neighboring paragraphs) and global context (semantically related but non-adjacent paragraphs) (Han, Soldaini, and Moschitti 2021). The global context is acquired by retrieving top-k relevant paragraphs using BM25 (Robertson and Zaragoza 2009). For chunk-level experiments, paragraphs within a chunk naturally possess local context. To assess the influence of context more accurately, we conduct paragraph-level experiments. By providing 4 local or global paragraphs, we compare the effects of context.

We utilize auxiliary information to enhance ASR error correction by one-step and two-stage methods. The one-step method integrates full or retrieved auxiliary information as a reference part of the context in the prompt. The retrieved part is top-K relevant sentences of auxiliary information for current content based on BM25. For the two-stage approach, we first use LLMs to extract the summary or keywords from the auxiliary information, with the prompts detailed in the arXiv version. We require the summary to retain all key terms as much as possible, especially challenging words for ASR. The keywords should be categorized by their entity types. Next, we use either the summary or keywords as references.

# Evaluations

We thoroughly evaluate using objective metrics, LLM Evaluator, and human evaluation. For objective metrics, we calculate BLEU (Papineni et al. 2002), ROUGE (Lin 2004), and BLEURT (Sellam, Das, and Parikh 2020) with the human target as reference. It’s important to emphasize that the capabilities of BLEU and ROUGE are limited by only one target of the SWAB dataset. Therefore, we focus more on semantic-oriented automatic metrics such as BLEURT.

Table 2: The performance comparison between closed-source and open-source LLMs at the chunk-level, with different auxiliary information utilization strategies, including directly providing (Origin), retrieving the top-10 sentences (RAG), based on the summary (Summary), or a list of keywords (Keywords) derived from LLMs. Source refers to the original ASR transcripts.   

<html><body><table><tr><td>Model</td><td>Auxiliary</td><td>BLEU↑</td><td>ROUGE-L↑</td><td>BLEURT↑</td><td>CK-Recall个</td><td>S-Faithful↑</td><td>S-Formal个</td></tr><tr><td>Source</td><td>None</td><td>16.22</td><td>41.51</td><td>55.19</td><td>12.89</td><td>7.53</td><td>4.97</td></tr><tr><td>LLaMA3-8B</td><td>None</td><td>8.94</td><td>20.73</td><td>40.53</td><td>13.18</td><td>6.35</td><td>7.33</td></tr><tr><td>QWen-14B</td><td>None</td><td>3.41</td><td>15.96</td><td>30.14</td><td>11.46</td><td>3.72</td><td>6.04</td></tr><tr><td>QWen-Max</td><td>None</td><td>9.47</td><td>28.92</td><td>50.58</td><td>22.35</td><td>6.00</td><td>7.55</td></tr><tr><td>GPT-4</td><td>None</td><td>15.51</td><td>39.66</td><td>62.15</td><td>22.49</td><td>7.32</td><td>8.20</td></tr><tr><td rowspan="4">GPT-4</td><td>+ Origin</td><td>15.33</td><td>39.53</td><td>62.73</td><td>43.70</td><td>7.38</td><td>8.09</td></tr><tr><td>+RAG</td><td>15.22</td><td>39.56</td><td>62.48</td><td>37.11</td><td>7.39</td><td>8.07</td></tr><tr><td>+ Summary</td><td>15.51</td><td>39.98</td><td>62.75</td><td>40.40</td><td>7.43</td><td>8.11</td></tr><tr><td>+ Keywords</td><td>15.08</td><td>39.34</td><td>62.72</td><td>45.42</td><td>7.46</td><td>8.16</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">Granularity</td><td rowspan="2">Context</td><td rowspan="2">BLEU↑</td><td rowspan="2">ROUGE-L↑</td><td rowspan="2">BLEURT↑</td><td rowspan="2">CK-Recall个</td><td colspan="2">S-Faithful个</td><td colspan="2">S-Formal↑</td></tr><tr><td>Para.</td><td>Chunk</td><td>Para.</td><td>Chunk</td></tr><tr><td>Document</td><td>None</td><td>1.80</td><td>4.97</td><td>10.64</td><td>5.30</td><td>2.28</td><td>2.44</td><td>4.57</td><td>3.74</td></tr><tr><td>Chunk</td><td>None</td><td>15.51</td><td>39.66</td><td>62.15</td><td>22.49</td><td>7.32</td><td>7.57</td><td>8.20</td><td>7.81</td></tr><tr><td>Paragraph</td><td>None</td><td>16.32</td><td>36.74</td><td>63.32</td><td>19.20</td><td>7.99</td><td>6.60</td><td>8.31</td><td>7.55</td></tr><tr><td rowspan="3">Paragraph</td><td>+ Local</td><td>16.44</td><td>39.73</td><td>67.11</td><td>22.21</td><td>8.19</td><td>7.41</td><td>8.40</td><td>7.76</td></tr><tr><td>+ Global</td><td>15.55</td><td>38.73</td><td>66.00</td><td>23.64</td><td>7.89</td><td>7.05</td><td>8.36</td><td>7.74</td></tr><tr><td>+ Both</td><td>16.38</td><td>39.60</td><td>67.10</td><td>23.78</td><td>8.18</td><td>7.42</td><td>8.39</td><td>7.80</td></tr></table></body></html>

Table 3: The performance of GPT-4 at different levels of granularity including document, chunk, and paragraph under differen contexts. Additionally, S-Faithful and S-Formal show results for both paragraph-level and chunk-level evaluations.

To measure the performance of ASR Error Correction, we design the Challenging Keyword Recall (CK-Recall) metric. The challenging keywords are a subset of ASR errors in transcripts, focusing specifically on named entities (e.g., person names, podcast titles). Correcting these errors can significantly improve the reading experience. Furthermore, these words typically cannot be modified or adjusted, making it convenient to directly search in the results. We use the recall calculation formula, where the numerator represents the number of keywords that appear in the results, and the denominator represents the total number of keywords.

In addition, we utilize an LLM (i.e., GPT-4) as the evaluator with the prompts shown in the arXiv version. We request the LLM to evaluate and score the model-generated target across faithfulness (S-Faithful) and formality (S-Formal). Scores range from 1 (worst) to 10 (best) for each criterion. SFaithful reflects content preservation to retain original meaning, and S-Formal reflects the degree of formality in style. For chunk-level results, we require the outputs to align each paragraph by the paragraph index, so we evaluate them at the paragraph-level granularity. To fairly compare paragraphlevel and chunk-level results, we evaluate them both at paragraph and chunk levels. To enhance effectiveness, we sample a maximum of 100 paragraphs per document for evaluation. Furthermore, we sample over 200 paragraphs, whose results of different LLMs are ranked by human evaluators.

# Results

# Results of Different LLMs

As shown in Table 2, we compare GPT-4 and QWen-Max (closed-source LLMs) with LLaMA3-8B and QWen-14B (open-source LLMs) at the chunk-level granularity. GPT-4 outperforms all other models across all metrics, followed by QWen-Max. Due to the limitations in model size, the performance of open-source models isn’t as good. Among them, the LLaMA3-8B model exhibits superior performance compared to the QWen-14B model.

LLMs show potential in solving this complex task, yet still encounter “faithfulness problems”. The performance of LLMs on the faithfulness score is unsatisfactory, even for state-of-the-art models like GPT-4 (7.32). We observe some “paragraph drift” situations at the chunk level. For example, the end of one paragraph might be migrated to the beginning of the following one. Even worse, the target and source cannot be aligned by paragraph index. Furthermore, faithfulness problems occur even when alignments are correct. The model frequently prioritizes coherence and fluency over faithfulness, such as fabrications for fragments or the omission of disorganized insertions in spoken scenarios, which compromises the accuracy of some essential information.

# Results of Granularity Levels and Contexts

As presented in Table 3, we compare the performance at different levels of granularity based on the GPT-4 model.

Although the SWAB dataset is constructed at the document level, the CoS2W task at the document level still presents a challenge. It requires a considerable number of tokens for both input and output (about 8K), which is difficult even for GPT-4 as shown in Table 4. For document-level results, $7 6 . 1 0 \%$ of the paragraphs are blank without the corresponding index, compared to $1 . 6 0 \%$ at the chunk-level granularity. And there is a noticeable decline in all metrics.

Due to “paragraph drift” issues at the chunk level as mentioned before, we present LLM evaluation results (i.e., SFaithful and S-Formal) for both paragraph and chunk levels to ensure a fair comparison. The chunk-level evaluations show that the CoS2W results perform better at chunk-level (7.57 and 7.81) than at paragraph-level (6.60 and 7.55) granularity. This highlights the advantages of utilizing local context at the chunk level. Chunk-level granularity enables LLMs to naturally leverage the local contexts to enhance faithfulness and formality performance.

In addition, we experiment with various contexts at paragraph level explicitly, confirming the role of context in enhancing text understanding, as is shown in Table 3. Paragraph-level results with local context show improvements across all metrics compared to results without context, approaching chunk-level performance. This further verifies the supportive role of local context. Local context improves CK-Recall from $1 9 . 2 0 \%$ to $2 2 . 2 1 \%$ , assisting the model to better understand the current paragraph. Moreover, global context can also yield some benefits, while not as significant as local context. Combining both local and global contexts, further improvements are seen in some metrics.

# Results of Auxiliary Information

We employ multiple methods to leverage auxiliary information. Table 2 includes results of directly providing (Origin), retrieving the top-10 most relevant sentences (RAG), based on a summary (Summary), or list of keywords (Keywords) derived from LLMs.

Auxiliary information can enhance ASR error correction, while requiring more effective methods. Based on the GPT-4 model at chunk-level granularity, directly given auxiliary information (Origin) increases CK-Recall from $2 2 . 4 9 \%$ to $4 3 . 7 0 \%$ . The improvement of RAG is limited, indicating a need for better retrieval methods. The Summary method improves objective metrics but shows limited CK-Recall gains, possibly due to missing keywords in the summary. The Keywords method is most effective, further boosting recall to $4 5 . 4 2 \%$ , and enhancing faithfulness scores while maintaining formality scores basically. However, improvements in CK-Recall stay below expectations, suggesting the limited benefits of current methods. Effectively utilizing auxiliary information still needs further research.

# Analysis

# Instruction Following Capability

We observe that larger LLMs (e.g., GPT-4) show stronger instruction-following ability compared to smaller LLMs (e.g., QWen-14B). We instruct the models to organize their responses by aligning each paragraph with the corresponding content using the paragraph index. We refer to paragraphs without aligned paragraph indices as blank paragraphs, which do not have corresponding target results. The rate of blank paragraphs can reflect a model’s instructionfollowing capability. The fewer the blank paragraphs, the stronger the instruction-following ability. Thus we compute the rate of blank paragraphs as shown in Table 4. At the chunk-level granularity, GPT-4 demonstrates robust alignment with scarcely any blank paragraphs $( 1 . 6 0 \% )$ , whereas QWen-14B exhibits more alignment errors $( 3 6 . 5 6 \% )$ . Given the auxiliary information, as the number of input tokens increases, QWen-14B’s performance further declines to $4 8 . 3 3 \%$ , while GPT-4 remains virtually unaffected.

Table 4: The comparison of instruction-following abilities between GPT-4 and QWen-14B. The fewer the blank paragraphs, the stronger the instruction-following ability.   

<html><body><table><tr><td>Model</td><td>#Tokens</td><td>BLEURT个</td><td>Blank %↓</td></tr><tr><td>GPT-4 (paragraph)</td><td>340.50</td><td>63.32</td><td>0.00</td></tr><tr><td>w/local context</td><td>654.47</td><td>67.11</td><td>0.00</td></tr><tr><td>GPT-4 (chunk)</td><td>1515.00</td><td>62.15</td><td>1.60</td></tr><tr><td>w/ auxiliary</td><td>2006.86</td><td>62.73</td><td>1.28</td></tr><tr><td>GPT-4 (document)</td><td>8140.42</td><td>10.64</td><td>76.10</td></tr><tr><td>QWen-14B (paragraph)</td><td>340.50</td><td>59.87</td><td>0.00</td></tr><tr><td>w/local context</td><td>654.47</td><td>55.64</td><td>0.00</td></tr><tr><td>QWen-14B (chunk)</td><td>1515.00</td><td>30.14</td><td>36.56</td></tr><tr><td>w/ auxiliary</td><td>2006.86</td><td>22.35</td><td>48.33</td></tr><tr><td>QWen-14B (document)</td><td>8140.42</td><td>2.87</td><td>91.58</td></tr></table></body></html>

Table 5: Comparison of different LLMs among total paragraphs with non-blank paragraphs at the chunk-level.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">S-Faithful↑</td><td colspan="2">S-Formal↑</td></tr><tr><td>Total</td><td>Non-Blank</td><td>Total</td><td>Non-Blank</td></tr><tr><td>GPT-4</td><td>7.32</td><td>7.39</td><td>8.20</td><td>8.29</td></tr><tr><td>QWen-Max</td><td>6.00</td><td>6.50</td><td>7.55</td><td>8.21</td></tr><tr><td>QWen-14B</td><td>3.72</td><td>4.99</td><td>6.04</td><td>8.29</td></tr><tr><td>LLaMA-8B</td><td>6.35</td><td>6.53</td><td>7.33</td><td>7.56</td></tr></table></body></html>

The differences in instruction-following ability also impact the model’s performance on the CoS2W task. As shown in Table 5, we compare the results among total paragraphs with non-blank paragraphs. Among non-blank paragraphs, QWen-14B’s S-Formal performance is actually quite good (8.29). However, the overall performance (6.04) is relatively low due to the influence of blank paragraphs.

# Discussion on LLM Evaluation

To measure the reliability of LLM evaluation, we compare the results with human evaluation. Consequently, we find that the LLM Evaluator is reliable for both faithfulness and formality evaluation. Based on more than 200 sampled paragraphs, we engage three annotators to independently evaluate and rank the performance of GPT-4, QWen-Max, and QWen-14B in terms of faithfulness and formality. Then we calculate the Spearman’s correlation coefficients (Spearman 1904) between the rankings given by the human evaluation and the LLM evaluation, as shown in Figure 3.

![](images/ff71749a20bc84c636c60a1fb784487e08dba643709ae83eac6d0660ee38b811.jpg)  
Figure 3: The ranking correlation coefficients between different evalCulatsisoifniermethoQdsWaen-dMhauxman evaluation. Note that “Oracle” shGoPTw-s4the averOargaec lsecores of three annotators, while “Classifier” reports only on English paragraphs.

Faithfulness Score: The Spearman’s correlation coefficient of the faithfulness rankings between the LLM Evaluator and human evaluation is 0.75, which is comparable to human performance. This demonstrates the reliability of the LLM Evaluator in faithfulness scoring. Additionally, this indicates that the LLM performs well in determining whether the results faithfully preserve the original content.

Formality Score: As shown in Figure 3, the Spearman’s correlation coefficient between the LLM evaluation and human evaluation is 0.61. Although there is a gap compared to human performance, there is also a strong correlation, which validates that the formality scores are reliable. Moreover, we compare the commonly used evaluation method. Using a RoBERTa-based classifier9, we classify English paragraphs as formal or informal and assign the formality probability as the formality score. The LLM evaluator outperforms the classifier in the correlation coefficient, confirming the reliability of LLM evaluation.

As Figure 3 illustrates, GPT-4 Evaluator correlates more strongly with human evaluations than QWen-Max Evaluator, confirming its reliability. In addition, we evaluate GPT4 results three times. The average standard deviations of faithfulness scores (0.19) and formality scores (0.11) across all samples confirm the robustness. Despite LLM updates rapidly, stable evaluation results can still be achieved with the same model version at a low temperature.

# Distribution of Languages and Domains

As indicated in Table 6, LLMs generally excel more in Chinese than English at the chunk level. There are more blank paragraphs in English $( 2 . 6 8 \%$ for GPT-4) than in Chinese $( 0 . 8 3 \% )$ . This may be because LLMs prioritize fluency in English over Chinese, often merging paragraphs for better fluency at the expense of faithfulness.

As shown in Table 7, the podcast domain, with the longest auxiliary information, shows the most significant improvement in CK-Recall, especially with the Keywords method. This indicates that plentiful auxiliary information aids ASR error correction improvement. Optimizing the use of long-text auxiliary information for further improvement is an area that warrants deeper exploration.

Table 6: The performance across different languages.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">BLEURT个</td><td colspan="2">S-Faithful个</td></tr><tr><td>Chinese</td><td>English</td><td>Chinese</td><td>English</td></tr><tr><td>GPT-4</td><td>66.95</td><td>55.52</td><td>7.83</td><td>6.80</td></tr><tr><td>QWen-Max</td><td>57.67</td><td>40.79</td><td>6.99</td><td>5.00</td></tr><tr><td>QWen-14B</td><td>36.73</td><td>21.05</td><td>4.77</td><td>2.63</td></tr><tr><td>LLaMA-8B</td><td>36.55</td><td>46.03</td><td>7.33</td><td>5.36</td></tr></table></body></html>

Table 7: The CK-Recall performance of GPT-4 with various auxiliary information methods across different domains.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="3">CK-Recall个</td></tr><tr><td>Podcast</td><td>Meeting</td><td>Lecture</td></tr><tr><td>GPT-4 (chunk)</td><td>19.82</td><td>30.00</td><td>32.81</td></tr><tr><td>w/ aux. (Origin)</td><td>45.82</td><td>30.00</td><td>36.72</td></tr><tr><td>w/ aux. (RAG)</td><td>36.55</td><td>35.00</td><td>39.84</td></tr><tr><td>w/ aux.(Summary)</td><td>41.27</td><td>35.00</td><td>37.50</td></tr><tr><td>w/aux. (Keywords)</td><td>47.27</td><td>45.00</td><td>37.50</td></tr></table></body></html>

# Conclusion

To improve the readability of ASR transcripts, we propose the Contextualized Spoken-to-Written conversion (CoS2W) task, construct and make available the document-level and multi-domain Spoken2Written of ASR transcripts Benchmark (SWAB) dataset. Based on the SWAB dataset, we compare the performance of different LLMs at various granularity levels, verify the beneficial roles of contexts and auxiliary information, and find that it is worth further exploring how LLMs maintain faithfulness and utilize auxiliary information to enhance ASR Error Correction. Compared with human evaluation, we find that the LLM Evaluator performs well in faithfulness and formality ranking with a good correlation. In the future, we plan to expand the dataset scale to better support research on supervised training for this task.

# Limitations

The study’s limitation lies in the SWAB dataset having only a single target, which may inadequately reflect model performance through many automatic metrics. Future efforts will focus on offering diverse targets and developing more effective evaluation methods. Note that the source data of SWAB are owned by the copyright holder. We only provide ASR transcripts and annotation targets, along with the corresponding links of audios and videos. The license of SWAB will be for research purposes only. Additionally, it is inevitable that evaluations are influenced by the biases of GPT-4, given that we obtain initial results through GPT-4 and also use it for experiments and evaluation.

# Ethics Statement

The SWAB dataset used in this research is strictly for academic and non-commercial purposes. We implemented several measures to ensure compliance with ethical standards, as follows.

• Data Transparency and Anonymization. The Chinese meetings in the SWAB are sourced from the training set of the publicly available AliMeeting dataset, and the English meetings come from the publicly available AMI meeting dataset. For our collected podcasts and lectures, we only provide ASR transcripts after rigorous text anonymization processes and our annotations, to ensure transparency regarding the data sources and their usage while maintaining anonymity.   
• Data Access Compliance. To further ensure the ethical use of the dataset, we require researchers to contact us via email to confirm their compliance with ethical guidelines and the conditions outlined in our data usage declaration, before granting them access to the dataset. This procedure includes ensuring that they are aware of and adhere to the Personal Information Protection Law (PIPL) and any relevant legal frameworks regarding personal data usage.   
• Authorization. Any personal data should be used only with express authorization, ensuring lawful and fair processing in accordance with applicable laws.