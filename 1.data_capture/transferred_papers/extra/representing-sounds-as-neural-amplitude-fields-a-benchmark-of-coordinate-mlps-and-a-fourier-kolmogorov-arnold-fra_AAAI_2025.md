# Representing Sounds as Neural Amplitude Fields: A Benchmark of Coordinate-MLPs and A Fourier Kolmogorov-Arnold Framework

Linfei $\mathbf { L i } ^ { 1 }$ , Lin Zhang1\*, Zhong Wang2, Fengyi Zhang3, Zelin $\mathbf { L i } ^ { 4 }$ , Ying Shen1

1School of Computer Science and Technology, Tongji University 2 Department of Automation, Shanghai Jiao Tong University 3 School of Electrical Engineering and Computer Science, The University of Queensland 4McCormick School of Engineering, Northwestern University cslinfeili $@$ tongji.edu.cn, cslinzhang $@$ tongji.edu.cn, cszhongwang $@$ sjtu.edu.cn, fengyi.zhang $@$ uq.edu.au, zelinl $2 0 2 5 @$ u.northwestern.edu, yingshen $@$ tongji.edu.cn

# Abstract

Although Coordinate-MLP-based implicit neural representations have excelled in representing radiance fields, 3D shapes, and images, their application to audio signals remains underexplored. To fill this gap, we investigate existing implicit neural representations, from which we extract 3 types of positional encoding and 16 commonly used activation functions. Through combinatorial design, we establish the first benchmark for Coordinate-MLPs in audio signal representations. Our benchmark reveals that Coordinate-MLPs require complex hyperparameter tuning and frequency-dependent initialization, limiting their robustness. To address these issues, we propose Fourier-ASR, a novel framework based on the Fourier series theorem and the Kolmogorov-Arnold representation theorem. Fourier-ASR introduces Fourier Kolmogorov-Arnold Networks (FourierKAN), which leverage periodicity and strong nonlinearity to represent audio signals, eliminating the need for additional positional encoding. Furthermore, a Frequencyadaptive Learning Strategy (FaLS) is proposed to enhance the convergence of Fourier-KAN by capturing high-frequency components and preventing overfitting of low-frequency signals. Extensive experiments conducted on natural speech and music datasets reveal that: (1) well-designed positional encoding and activation functions in Coordinate-MLPs can effectively improve audio representation quality; and (2) Fourier-ASR can robustly represent complex audio signals without extensive hyperparameter tuning. Looking ahead, the continuity and infinite resolution of implicit audio representations make our research highly promising for tasks such as audio compression, synthesis, and generation.

# Code and Appendix — https://github.com/lif314/NeAF

# Introduction

Implicit Neural Representations (INRs) provide an innovative approach to signal parameterization by representing arbitrary discrete signals as continuous functions. These functions map the domain of the signal (coordinates, e.g., timestamps in audio) to the corresponding content at those coordinates (such as the amplitude of an audio signal). Typically, these functions are approximated using neural networks, and since current neural networks are primarily constructed using multilayer perceptrons (MLPs), these types of INRs are referred to as Coordinate-MLPs.

Compared to traditional discrete signal representation schemes, INRs offer continuous implicit representation that decouples from spatial resolution and allows for infinite resolution. Therefore, the storage required for parameterized signals is independent of spatial resolution, allowing these signals to be sampled at any desired resolution. Owing to these advantages, Coordinate-MLPs have been successfully applied to various modalities of data, including neural radiance fields (Mildenhall et al. 2020), 3D occupancy grids (Mescheder et al. 2019), Signed Distance Functions (Park et al. 2019), images (Sitzmann et al. 2020), 2D computed tomography, and 3D magnetic resonance imaging (Tancik et al. 2020; Saragadam et al. 2023; Kazerouni et al. 2024).

Regarding audio signals, continuous representations offer the advantages of infinite resolution, enabling natural generation, efficient compression, and smooth processing. However, the representation of continuous audio signals using Coordinate-MLPs poses profound challenges due to the high noise, high frequency, nonlinearity, and local periodicity inherent in audio signals. According to the Weber-Fechner law, even relatively small reconstruction errors in audio signals can become perceptible due to the logarithmic nature of human auditory perception, thereby imposing high demands on the quality of audio reconstruction. Moreover, the simple combination of linear transformations and nonlinear activation functions in MLP networks makes it difficult to capture the periodicity and high-frequency components of audio signals. Through a comprehensive review, we find that till now only SIREN (Sitzmann et al. 2020) has attempted to represent audio signals using sinusoidal activation functions and provided a simple comparison with ReLU-MLPs, yet no further investigations have been conducted.

To fill the gap, we establish, to our knowledge, the first open-source benchmarking framework to fully explore the potential and limitations of Coordinate-MLPs in continuous audio signal representations. Specifically, since the performance of a Coordinate-MLP is primarily determined by the choice of the activation function and optional positional encoding, we identify 3 types of positional encoding mappings and 16 commonly used activation functions from existing

405 Gaussian ReLU Sine 20 Sine Ineode-Sine ReKporlesmeongtoartiovn-ATrhneolrdem + FoAurineorl dKNolemtwogororksov  
23050 f（t)= ∑Φ（Φ。（t)) CooTridmineates 810 DWW Fourier Series Theorem 15 8 p(t)=（（)）） MMMA W 10 5 ↑ 0 Local Periodicity Assumption 1 20 Variance4S0cale of R6F0F P.E. (   )80 100 3Frquency() 3000 30000 300000 Amplitudes   
$\textcircled{1}$ Positional encoding is parameter-sensitive. ② Activation functions are parameter-sensitive. $\textcircled{1}$ Fourier-ASR is more interpretable.   
40 ReLU Gaussian Laplacian Sine Fourier-ASR ReLU RFF+ReLU NeFF+ReLU 25 Gaussian   
30 20 20 Bspin- KAN 1 5 0 0 Bach Counting GTZAN Bach Counting GTZAN   
-10 -10 NorthernIrish NewZealand Dataset Dataset VCTK Dataset   
$\textcircled{3}$ Activation functions lack periodicity. ④ Activation functions require positional encoding. $\textcircled{2}$ Fourier-ASR is parameter-insensitive and robust. (a) Benchmark of Coordinate-MLPs in Audio Signal Representations. (b) Fourier-ASR.

Coordinate-MLP methods. This results in 48 possible network configurations for audio signal representation, which we evaluate on speech and music datasets to assess their performance.

As shown in Fig. 1(a), our benchmark reveals the following findings. (1) Most activation functions, except those with strong linearity (e.g., Gaussian) or periodicity (e.g., Sine), are unable to effectively represent audio signals. (2) Although some activation functions, such as Gaussian and Sine, are proposed to overcome spectral bias and the tedious parameter tuning associated with positional encoding, positional encoding remains indispensable for representing audio signals. It efficiently maps time coordinates to highdimensional spaces, allowing the network to capture highfrequency components in audio signals. (3) Due to the local periodicity of audio signals, periodic activation functions (e.g., Sine) significantly outperform other activation functions in representational capacity. Moreover, incorporating Fourier feature-based positional encoding can further enhance their ability. (4) While Sine-type activation functions are effective at representing audio signals due to their periodic nature, they unfortunately require hyperparametersensitive positional encodings and frequency-dependent initialization schemes, which negatively impact their robustness and generalization capabilities.

The aforementioned issues of Coordinate-MLPs fundamentally arise from the inadequate nonlinearity and lack of periodicity inherent in MLPs. As illustrated in Fig. 1(b), to enhance the nonlinear and periodic representational capabilities of neural networks, we propose a novel implicit audio representation framework, Fourier-ASR, based on the Fourier series theorem and the Kolmogorov-Arnold representation theorem. Firstly, we introduce a KolmogorovArnold Network (Fourier-KAN) that utilizes Fourier basis functions to represent audio signals. This network implicitly decomposes any complex audio signal into a series of locally periodic Fourier series. Unlike MLPs, Fourier-KAN does not require additional positional encoding or activation functions, thereby avoiding cumbersome hyperparameter tuning. Furthermore, due to the use of Fourier basis functions, it more effectively captures the high-frequency components and local periodicity of signals. Secondly, to accelerate the convergence of Fourier-KAN, we introduce a Frequency Adaptive Learning Strategy (FaLS). FaLS employs an inverted frequency pyramid configuration to capture signals at various frequencies and utilizes a frequency-adaptive weight initialization scheme based on forward propagation theory to mitigate issues of gradient explosion or vanishing, thereby expediting convergence. Experimental results demonstrate that Fourier-ASR not only offers enhanced interpretability but is also robust to hyperparameter variations, effectively representing complex audio signals.

In summary, our contributions are summarized as follows:

• We introduce the first benchmark for Coordinate-MLPs in audio representation, incorporating 3 types of positional encodings and 16 commonly used activation functions. Our benchmark provides an in-depth analysis of the impact of positional encoding and activation functions on the representation of continuous audio signals. • To avoid spectral bias from positional encoding and complex parameter tuning of activation functions, we propose a novel audio signal representation framework, Fourier-ASR, based on the Fourier series theorem and the Kolmogorov-Arnold theorem. Fourier-ASR includes Fourier Kolmogorov-Arnold Networks (Fourier-KAN) and a Frequency-adaptive Learning Strategy (FaLS). Due to the periodicity and strong nonlinearity of Fourier basis functions, Fourier-ASR effectively represents audio signals and provides enhanced interpretability.

• As shown in Fig. 1, extensive experiments conducted on speech and music datasets reveal that (1) careful tuning of positional encoding and activation function parameters can significantly enhance the representational capacity of Coordinate-MLPs for audio signals; and (2) Fourier-ASR can robustly represent audio signals without requiring cumbersome parameter tuning.

# Related Work

Coordinate-MLPs. The usage of Coordinate-MLPs differs significantly from that of traditional MLPs in two main aspects: (a) traditional MLPs typically operate on high-dimensional inputs, such as images, sounds, or 3D shapes; (b) traditional MLPs are primarily employed as classification heads, where the decision boundaries need not be smooth. In contrast, Coordinate-MLPs encode signals into weights, where the input is low-dimensional coordinates and the output must maintain smoothness. The success of NeRF (Mildenhall et al. 2020) demonstrates that Coordinate-MLPs, when trained with a limited number of perspective images, can reconstruct photometric projections from any angle and at any resolution. This breakthrough spurs the application of Coordinate-MLPs in numerous fields, including radiance field reconstruction (Barron et al. 2022; Chen et al. 2022; Mu¨ller et al. 2022), 3D shape representation (Wang et al. 2021; Yu et al. 2022; Yariv et al. 2023), 2D image regression (Tancik et al. 2020; Saragadam et al. 2023; Lindell et al. 2022; Ramasinghe and Lucey 2022), audio signal regression (Sitzmann et al. 2020; Kazerouni et al. 2024), and inverse rendering problems in 2D CT and 3D MRI (Tancik et al. 2020).

Positional Encoding. Positional encoding facilitates the learning of high-frequency representations in radiance fields, images, and 3D shapes. NeRF (Mildenhall et al. 2020) improves the ability of ReLU-MLPs to capture highfrequency signals by mapping the input coordinates to a high-dimensional Fourier space. Building upon NeRF, FFN (Tancik et al. 2020) incorporates Gaussian noise to improve the robustness of ReLU-MLPs. Although positional encodings enable MLPs to represent high-frequency components, selecting the appropriate frequency scale is crucial and often involves cumbersome parameter tuning. Specifically, when the signal bandwidth is excessively increased, CoordinateMLPs tend to produce noisy signal interpolations (Ramasinghe, MacDonald, and Lucey 2022; Hertz et al. 2021).

Activation Functions. The nonlinear representation capability of Coordinate-MLPs primarily arises from activation functions. In the field of INRs, various activation functions have been employed to approximate different types of signals. ReLU is frequently employed as the activation function in NeRF-related studies due to its simplicity and effective initialization scheme (Mildenhall et al. 2020; Barron et al. 2022; Yu et al. 2021; Chen et al. 2022). However, ReLU struggles to capture high-frequency information in radiance fields, necessitating additional positional encoding. To avoid the cumbersome parameter tuning associated with positional encoding, GARF (Chng et al. 2022) uses the Gaussian activation function, which can effectively capture high-frequency information but fails to capture periodic signals and tends to overfit both noise and signal equally. To address these issues, WIRE (Saragadam et al. 2023) utilizes the complex Gabor wavelet activation function to improve the robustness. SIREN (Sitzmann et al. 2020) employs the sine activation function to capture signal periodicity, though it is sensitive to initialization schemes, limiting its generalization to audio reconstruction tasks. Building on SIREN, INCODE (Kazerouni et al. 2024) makes the parameters of the sine activation functions learnable, thereby reducing the parameter sensitivity to some extent, but it still relies on the frequency-aware initialization scheme.

# Method

# Problem Formulation

As illustrated in Fig. 2(a), natural audio signals are continuous functions of time, representing the variation in amplitude of sound signals over time. To convert this continuous signal into a digital format for storage and processing, the signal is discretely sampled, resulting in a discrete signal $a ( t )$ with respect to the time coordinate $t$ . However, in fields such as audio super-resolution, synthesis, and compression, researchers aim to leverage implicit neural representation techniques to preserve the continuity and differentiability of the signal as much as possible. Specifically, by receiving a discrete time coordinate $t$ , a neural network regresses the amplitude $f ( t )$ corresponding to $t$ , thereby encoding the audio signal within the network weights. We refer to this representation as the Neural Amplitude Fields (NeAF). Optimization is performed by fitting $f ( t )$ to the sampled waveform $a ( t )$ using an MSE loss function,

$$
\mathcal { L } = \int \left. \mathrm { I I } _ { a } ( f ( t ) ) - a ( t ) \right. ^ { 2 } d t ,
$$

where $\textstyle \operatorname { I I } _ { a }$ samples $f ( t )$ at the waveform measurement locations. Given that NeAF is independent of spatial resolution, audio can be processed at any desired resolution.

# Benchmark of Coordinate-MLP-based NeAF

As depicted in Fig. 2(b), to represent arbitrary complex audio signals, a $k$ -layer Coordinate-MLP $f : \mathbb { R }  \mathbb { R }$ is employed, which takes the time coordinate $t \in \mathbb { R }$ as input and outputs the amplitude $f ( t ) \in \mathbb { R }$ . Thus, $f ( t )$ can be defined through the following recursive relations,

$$
\begin{array} { r l } & { \mathbf { z } ^ { ( 1 ) } = \boldsymbol { \gamma } ( t ) } \\ & { \mathbf { z } ^ { ( i + 1 ) } = \sigma \left( \mathbf { W } ^ { ( i ) } \mathbf { z } ^ { ( i ) } + \mathbf { b } ^ { ( i ) } \right) , i = 1 , \dots , k - 1 } \\ & { \quad f ( t ) = \mathbf { W } ^ { ( k ) } \mathbf { z } ^ { ( k ) } + \mathbf { b } ^ { ( k ) } , } \end{array}
$$

where $\gamma ( \cdot )$ denotes an optional positional encoding function that maps the input coordinate $t$ to a higher-dimensional space, $\bar { \sigma } ( \cdot )$ represents the element-wise applied nonlinear activation function, $\mathbf W ^ { ( i ) } \in \mathbb R ^ { d _ { i + 1 } \times d _ { i } }$ and $\mathbf { b } ^ { ( i ) } \in \mathbb { R } ^ { d _ { i + 1 } }$ denote the weights and biases of the $i$ -th layer, respectively, while $\mathbf { z } ^ { ( i ) } \in \mathbb { R } ^ { \bar { d } _ { i } }$ represents the hidden units of the $i$ -th layer.

Following the architecture of Coordinate-MLPs, we conduct an extensive review of implicit neural representations and identify 3 types of positional encoding and 16 potential activation functions. Specifically, as shown in Table 1, the three positional encoding schemes are identity mapping (Identity), NeRF Fourier features (NeFF) (Mildenhall et al. 2020), and random Fourier features (RFF) (Tancik et al. 2020). The primary activation functions include ReLU, Gaussian (Chng et al. 2022), Laplacian (Ramasinghe and Lucey 2022), Sine (Sitzmann et al. 2020), Incode-Sine (Kazerouni et al. 2024), and Gabor-Wavelet (Saragadam et al. 2023), among others (details are provided in Table 2).

![](images/4fa1fe9e804f99d2c6ca854410bb04a833c8fd28d33a971e943fe3b89e6ce0bb.jpg)  
Figure 2: (a) The problem definition of implicit audio representations; (b) The audio representation framework based on Coordinate-MLPs; (c) Fourier-ASR, a novel audio signal representation framework based on Fourier-KAN.

Table 1: The nonlinear mappings in Coordinate-MLPs. Note that $a$ denotes a learnable parameter, while $[ a ]$ denotes a hyperparameter.   

<html><body><table><tr><td>PE (P) γ∈P</td></tr><tr><td>Y(t)=t 1 NeFF γ(t)=[cos(2[πt),sin(25πt)]T [L γ(t)=[cos(2πbLt),sin(2πbLt)]T Identity</td></tr><tr><td>RFF [σ,L] bL~N(0,g²)</td></tr><tr><td>Activations (A) σ∈A ReLU σ(x) =max(0,x)</td></tr><tr><td>Gaussian σ(x) 二 [a] 2 e2a2</td></tr><tr><td>三国 Laplacian σ(x) eα [a]</td></tr><tr><td>Sine σ(x)= sin(ωx) [@] a,bcd[u]</td></tr></table></body></html>

It is noteworthy that Gaussian, Sine, and Incode-Sine activation functions are proposed to eliminate the dependence on positional encoding in radiance fields and image representations. However, high-dimensional positional encoding mappings may be beneficial for learning high-frequency features in audio signals and their structural variations at different time scales. Therefore, in our benchmark, we apply positional encoding mappings to all three activation functions.

Based on Table 1, the Coordinate-MLPs used for benchmarking audio signal representations can be expressed as follows,

$$
\begin{array} { r } { f ( t ) = ( \mathbf { W } _ { n } \circ \sigma _ { n - 1 } \cdot \cdot \cdot \sigma _ { 1 } \circ \mathbf { W } _ { 1 } ) ( \gamma ( t ) ) , } \\ { \gamma ( \cdot ) \in \mathcal { P } , \sigma _ { i } ( \cdot ) \in \mathcal { A } , } \end{array}
$$

where $t$ denotes the input time coordinate normalized to the interval [0, 1], $\mathcal { P }$ represents the set of positional encodings, and $\mathcal { A }$ denotes the set of activation functions.

# Fourier-ASR

Our benchmark indicates that only through carefully designed positional encoding and activation functions can some Coordinate-MLPs effectively represent audio signals. However, their flexibility and generality are reduced due to complex parameter tuning and high sensitivity to initialization. To address this issue, as shown in Fig. 2(c), drawing from the Fourier series theorem and the Kolmogorov-Arnold representation theorem, we introduce a novel framework for audio signal representation, Fourier-ASR, which incorporates Fourier Kolmogorov-Arnold Networks (Fourier-KAN) and a Frequency-adaptive Learning Strategy (FaLS).

Fourier Kolmogorov-Arnold Networks (Fourier-KAN). Unlike Coordinate-MLPs based on the Universal Approximation Theorem (Hornik, Stinchcombe, and White 1989), which use combinations of linear transformations and nonlinearities, Fourier-ASR follows the Kolmogorov-Arnold Representation Theorem (Kolmogorov 1956; Arnold 1957) to represent any continuous function as a finite composition of single-variable functions and addition. For a continuous signal $f ( t )$ , this simplifies to,

$$
f ( t ) = \sum _ { q = 0 } ^ { 2 } \Phi _ { q } \left( \phi _ { q } ( t ) \right) ,
$$

where $\Phi _ { q } : \mathbb { R } \to \mathbb { R }$ and $\phi _ { q } : [ 0 , 1 ] \to \mathbb { R }$ denote the outer and inner functions, respectively. To enhance the capacity and learnability of this representation, we employ the KAN (Liu et al. 2024) approach to extend the network to an arbitrary number of layers.

Assumption 1 Local Periodicity Assumption. For a complex non-stationary signal $f ( t )$ , there exists a sufficiently small time interval $\epsilon > 0$ where $f ( t )$ can be approximated by a periodic function $p ( t )$ :

$$
\exists \epsilon > 0 , \quad \forall t , \quad | t | < \epsilon : \quad f ( t ) \approx p ( t )
$$

where $p ( t )$ is a periodic function with period $T$ , satisfying $\underline { { p } } ( \underline { { t } } + \underline { { T } } ) \underline { { = } } \underline { { p } } ( \underline { { t } } ) .$ .

Theorem 1 Fourier Series Theorem. Any periodic signal $p ( t )$ with period $T$ can be represented as an infinite series of sine and cosine functions,

$$
p ( t ) = { \frac { a _ { 0 } } { 2 } } + \sum _ { n = 1 } ^ { \infty } \left( a _ { n } \cos \left( { \frac { 2 \pi n t } { T } } \right) + b _ { n } \sin \left( { \frac { 2 \pi n t } { T } } \right) \right)
$$

where $a _ { 0 }$ , $\textstyle { a _ { n } , }$ and $b _ { n }$ are the Fourier coefficients.

Specifically, consider a neural network with a shape of $[ n _ { 0 } , n _ { 1 } , \cdots , n _ { L } ]$ , where $n _ { l }$ denotes the number of neurons on the $l$ -th layer of the computational graph. For the $i$ -th node on the $l$ -th layer, denoted by $( l , i )$ , the activation value of this neuron is $t _ { l , i }$ . Between the $l$ -th and $( l + 1 )$ -th layers, there are $n _ { l } \times n _ { l + 1 }$ non-linear basis functions. Based on Assumption 1 and Theorem 1, any audio signal within short time intervals can be approximated as combinations of cosine and sine functions. Therefore, unlike using B-Splines in KAN (Liu et al. 2024), we employ Fourier basis functions as the non-linear units connecting neurons $( l , i )$ and $( l + 1 , j )$ ,

$$
\begin{array} { r } { \phi _ { l , j , i } ( t _ { l , i } ) = a _ { l , i } \cos ( \omega t _ { l , i } ) + b _ { l , i } \sin ( \omega t _ { l , i } ) + c _ { l , i } , } \\ { l = 0 , \cdots , L - 1 , i = 1 , \cdots , n _ { l } , j = 1 , \cdots , n _ { l + 1 } , } \end{array}
$$

where ${ a _ { l , i } , b _ { l , i } }$ are learnable Fourier coefficients, $\boldsymbol { c } _ { l , i }$ is a learnable bias term, and $\omega$ is a frequency hyperparameter. Then, the activation value $t _ { l + 1 , j }$ of the $( l + 1 , j )$ neuron is simply the sum of all incoming post-activations,

$$
t _ { l + 1 , j } = \sum _ { i = 1 } ^ { n _ { l } } \phi _ { l , j , i } \left( t _ { l , i } \right) , j = 1 , \cdot \cdot \cdot , n _ { l + 1 } .
$$

For the $l$ -th Fourier KAN layer, by rewriting Eq. 6 under the matrix form, we can have,

$$
\mathbf { t } _ { l + 1 } = \underbrace { \left( \begin{array} { c c c c } { \phi _ { l , 1 , 1 } ( \cdot ) } & { \cdot \cdot \cdot } & { \phi _ { l , 1 , n _ { l } } ( \cdot ) } \\ { \phi _ { l , 2 , 1 } ( \cdot ) } & { \cdot \cdot \cdot } & { \phi _ { l , 2 , n _ { l } } ( \cdot ) } \\ { \vdots } & { \vdots } & { \vdots } \\ { \phi _ { l , n _ { l + 1 } , 1 } ( \cdot ) } & { \cdot \cdot \cdot } & { \phi _ { l , n _ { l + 1 } , n _ { l } } ( \cdot ) } \end{array} \right) } _ { \Phi _ { l } } \mathbf { t } _ { l } .
$$

where $\Phi _ { l }$ is the transition matrix between the Fourier layers.

In summary, Fourier-ASR employs Fourier-KAN to derive continuous representations from discrete audio signals as,

$$
f ( t ) = ( \Phi _ { L } \circ \Phi _ { L - 1 } \cdot \cdot \cdot \Phi _ { l } \cdot \cdot \cdot \circ \Phi _ { 1 } ) ( t ) .
$$

Compared to the Coordinate-MLPs (Eq. 3), our FourierKAN leverages Fourier basis functions to achieve not only enhanced nonlinear representation capabilities but also the ability to capture local periodicity in audio signals.

Frequency-adaptive Learning Strategy (FaLS). Due to the varying frequency distributions of audio signals across different time scales, a fixed frequency hyperparameter ( $\omega$ in Eq. 5) can lead Fourier-KAN to predominantly learn specific frequency components, thereby hindering convergence. To address this issue, we propose a Frequency-adaptive Learning Strategy (FaLS). Specifically, we assign basis functions with varying frequency thresholds to different Fourier-KAN layers. Then, a Fourier-KAN can be represented as,

$$
\begin{array} { c } { { \displaystyle { \bf z } ^ { ( 0 ) } = t } \ ~ } \\ { { \displaystyle { \bf z } ^ { ( l + 1 ) } = \sum _ { \omega = 1 } ^ { \Omega _ { l } } \Big [ { \bf a } ^ { ( l , \omega ) } \cos ( \omega { \bf z } ^ { ( l ) } ) + { \bf b } ^ { ( l , \omega ) } \sin ( \omega { \bf z } ^ { ( l ) } ) \Big ] + { \bf c } ^ { ( l ) } } \ ~ } \\ { { \displaystyle f ( t ) = { \bf z } ^ { ( n _ { L } ) } \left( \cdot \cdot \cdot \left( { \bf z } ^ { ( l ) } \left( \cdot \cdot \left( { \bf z } ^ { ( 0 ) } \right) \right) \right) \right) , l = n _ { 0 } , \cdot \cdot \cdot , n _ { L } } , } \end{array}
$$

where $\mathbf { a } ^ { ( l , \omega ) }$ and $\mathbf { b } ^ { ( l , \omega ) } \in \mathbb { R } ^ { d _ { l + 1 } \times d _ { l } }$ denote the Fourier coefficient weights for the $l$ -th layer at frequency $\omega$ , $\mathbf { c } ^ { ( l ) } \in \mathbb { R } ^ { d _ { l + 1 } }$ is the bias term of the $l$ -th layer, $z ^ { ( l ) } \in \mathbb { R } ^ { d _ { l } }$ denotes the hidden units of the $l$ -th layer, and $\Omega _ { l }$ is a hyperparameter indicating the maximum frequency threshold for the $l .$ -th layer.

Parameter initialization. Following the principles of Xavier (Glorot and Bengio 2010) and Kaiming’s work (He et al. 2015), we derive the initialization scheme for the Fourier-KAN. Specifically, in the forward propagation process at layer $l$ of the Fourier-KAN, the symmetry of the Fourier basis functions ensures that the expected values of both the input and the output are zeros, i.e., $E [ { \bf z } ^ { ( l ) } ] =$ $E [ { \bf z } ^ { ( l + 1 ) } ] ~ = ~ 0$ . According to Kaiming initialization (He et al. 2015), we make the following assumptions: (1) the expected values of the Fourier parameters $\mathbf { a } ^ { ( \bar { l } , \omega ) }$ and $\mathbf { b } ^ { ( l , \omega ) }$ are both zeros, and the bias term $\mathbf { c } ^ { l }$ is omitted; (2) the variances of the input $\mathbf { z } ^ { ( l ) }$ and the output $\mathbf { z } ^ { ( l + 1 ) }$ are both ones. Thereby, we can determine the variance of the output at layer $l$ as,

$$
V a r [ \mathbf { z } ^ { ( l + 1 ) } ] = \sum _ { \omega = 1 } ^ { \Omega _ { l } } ( \cos ^ { 2 } ( \omega \mathbf { z } ^ { ( l ) } ) V a r [ \mathbf { a } ^ { ( l , \omega ) } ]
$$

Assuming that the variances of the Fourier coefficients are equal, we can have,

$$
V a r [ \mathbf { a } ^ { ( l ) } ] = V a r [ \mathbf { b } ^ { ( l ) } ] = \frac { 1 } { \Omega _ { l } } .
$$

Thus, each independent Fourier coefficient $a _ { i } ^ { ( l ) }$ (and $b _ { i } ^ { ( l ) } .$ ) is initialized using the following normal distribution,

$$
a _ { i } ^ { ( l ) } , b _ { i } ^ { ( l ) } \sim \mathcal { N } ( 0 , \frac { 1 } { \Omega _ { l } d _ { i n } ^ { ( l ) } } ) ,
$$

where $d _ { i n } ^ { ( l ) }$ denotes the dimensionality of the input to layer $l$ .

Inverted pyramid frequency setting. Given the depth $L$ and width of a Fourier-KAN, the hyperparameters $[ \Omega _ { 0 } , \cdots , \Omega _ { l } , \Omega _ { L } ]$ dictate the number of Fourier basis functions and the tendency to learn frequency components in each layer. With the same network capacity, a larger $\Omega$ enhances the frequency resolution, improving the network’s ability to capture audio signal periodicity and fluctuations. Similar to the role of positional encoding in CoordinateMLPs, an inverted pyramid frequency setting is beneficial for the Fourier-KAN in capturing high-frequency information, thereby accelerating convergence. For instance, a 3-layer Fourier-KAN with $\Omega$ set to [64, 5, 3] outperforms [8, 8, 8], which may lead to convergence issues.

Table 2: Benchmark leaderboard of Coordinate-MLPs. For different positional encodings (Identity, RFF, NeFF), the best results are bold for first and underlined for second. Note that “ $\dot { \boldsymbol { a } }$ ” denotes a learnable parameter, while “[a]” denotes a hyperparameter. The benchmarking results for the remaining 10 activation functions are provided in the appendix (Appendix D).   

<html><body><table><tr><td>Activation σ(-)</td><td rowspan="2">Equation</td><td rowspan="2">Parameter</td><td rowspan="2">PE γ(-)</td><td colspan="2">SNRSD1</td><td colspan="2">SNR1TLSD1</td><td colspan="2">SNR11LSD</td><td colspan="2">SNRTLSD1</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">PReLU</td><td rowspan="2">x, ax，</td><td rowspan="2">ifx>0 [a] otherwise</td><td>Identity</td><td>0.00</td><td>4.724</td><td>0.00</td><td>4.630</td><td>0.00</td><td>7.031</td><td>0.00</td><td>5.462</td></tr><tr><td>RFF</td><td>13.42</td><td>1.010</td><td>3.38</td><td>1.437</td><td>2.50</td><td>2.035</td><td>6.43</td><td>1.494</td></tr><tr><td rowspan="2">ReLU</td><td rowspan="2">max(0,x)</td><td rowspan="2"></td><td>NeFF</td><td>17.50</td><td>1.133</td><td>7.88</td><td>1.575</td><td>5.20</td><td>1.539</td><td>10.19</td><td>1.416</td></tr><tr><td>Identity RFF</td><td>0.00 15.62</td><td>4.623</td><td>-7.66 4.93</td><td>4.546</td><td>0.00</td><td>6.774 1.862</td><td>-2.55</td><td>5.314</td></tr><tr><td rowspan="2"></td><td rowspan="2">x2</td><td rowspan="2"></td><td>NeFF</td><td>22.29</td><td>0.978 1.129</td><td>9.57</td><td>1.400 1.538</td><td>3.23 7.64</td><td>1.324</td><td>7.93 13.17</td><td>1.413 1.330</td></tr><tr><td>Identity</td><td>6.35</td><td>1.130</td><td>0.74</td><td>2.165</td><td>0.68</td><td>3.059</td><td>2.59</td><td>2.118</td></tr><tr><td rowspan="2">Gaussian</td><td rowspan="2">e2a2</td><td rowspan="2">[a]</td><td>RFF</td><td>20.85</td><td>2.046</td><td>12.14</td><td>3.195</td><td>11.80</td><td>1.346</td><td>14.93</td><td>2.196</td></tr><tr><td>NeFF</td><td>19.68</td><td>2.127</td><td>9.20</td><td>3.438</td><td>7.74</td><td>1.597</td><td>12.21</td><td>2.387</td></tr><tr><td rowspan="2">Laplacian</td><td rowspan="2">el</td><td rowspan="2">[a]</td><td>Identity</td><td>12.04</td><td>0.932</td><td>1.34</td><td>1.561</td><td>1.37</td><td>2.434</td><td>4.92</td><td>1.642</td></tr><tr><td>RFF</td><td>15.57</td><td>2.386</td><td>10.97</td><td>2.632</td><td>14.74</td><td>1.112</td><td>13.76</td><td>2.043</td></tr><tr><td rowspan="2">Sine</td><td rowspan="2">sin(wx)</td><td rowspan="2">[w]</td><td>NeFF</td><td>15.26</td><td>2.434</td><td>8.67</td><td>3.191</td><td></td><td>8.16</td><td>1.526 10.70</td><td>2.384</td></tr><tr><td>Identity</td><td>13.36</td><td>0.838</td><td>7.96</td><td>1.660</td><td>7.47</td><td>1.722</td><td>9.59</td><td>1.407</td></tr><tr><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td>RFF</td><td>39.02 42.39</td><td>0.582 0.537</td><td>13.06</td><td>1.412</td><td></td><td>16.57</td><td>1.156 22.88</td><td>1.050</td></tr><tr><td>NeFF</td><td></td><td></td><td>33.58</td><td>0.914</td><td>22.02</td><td>0.696</td><td>32.66</td><td>0.716</td></tr><tr><td rowspan="2">Incode-Sine</td><td rowspan="2">a sin(bwx +c)+d</td><td rowspan="2">[w],a,b,c,d</td><td>Identity</td><td>15.98</td><td>0.778 0.595</td><td>8.16</td><td>1.611</td><td></td><td>0.01</td><td>3.865 8.05</td><td>2.085</td></tr><tr><td>RFF NeFF</td><td>38.10 41.40</td><td>0.556</td><td>12.86 32.24</td><td>1.559 1.038</td><td>15.13 21.33</td><td>1.241 0.763</td><td>22.03 31.99</td><td>1.132 0.786</td></tr></table></body></html>

# Experiments

# Experimental Setup

Datasets. GTZAN music dataset (Tzanetakis and Cook 2002) includes 1000 thirty-second music clips across ten genres. CSTR VCTK speech corpus (Yamagishi, Veaux, and MacDonald 2019) consists of voice recordings from 110 speakers with diverse accents, each speaking approximately 400 sentences. For the benchmark, we used two 7-second clips provided by SIREN (Sitzmann et al. 2020) (“Bach” and “Counting”) and a 30-second clip from GTZAN dataset (“Blues”). To comprehensively evaluate the performance of effective methods, we selected ten audio clips of different genres from the GTZAN dataset and ten audio clips with various accents from the CSTR VCTK dataset.

Networks. We ensured that the network parameters were comparable, ranging between 250K and 270K. For the Coordinate-MLPs, each network has a depth of 6 and a width of 256. In contrast, the Fourier-ASR network has a depth of 6 and a width of 64, with the maximum frequency thresholds set to 1024, 5, and 3 for the input layer, hidden layers, and the output layer, respectively.

Evaluation Metrics. Signal-to-Noise Ratio (SNR) (Roux et al. 2019) and Log-Spectral Distance (LSD) (Gray and Markel 1976) were utilized to assess the temporal and spectral errors in the reconstructed audios, respectively. Since LSD provides an indirect measure for frequency domain evaluation, we primarily focus on the SNR metric.

# Benchmark Leaderboard

We begin by examining the impact of nonlinear mappings, which are commonly presumed but have not yet been analyzed in the context of implicit audio representation. In line with Eq. 3, Table 2 presents the evaluation results of audio signal representation using 16 different activation functions and 3 types of positional encoding (Identity, NeFF, and RFF). Based on this comprehensive benchmarking, the following conclusions can be drawn:

• Most activation functions (Sigmoid, ReLU, Tanh, etc.), aside from those with strong nonlinearity (Gaussiantype) and periodicity (Sine-type), fail to capture the highfrequency and local periodicity of audio signals. • Positional encodings significantly enhance the ability of Coordinate-MLPs to represent audio signals due to their high-dimensional mapping capabilities, which improve the model’s ability to capture high-frequency information. This enhancement is particularly notable for Gaussian (11.02dB $\uparrow$ in SNR) and Sine (18.96dB $\uparrow$ in SNR) activation functions. • In the context of positional encoding, the introduction of random Gaussian noise by RFF makes it more suited to Gaussian-type activation functions $\mathrm { ( \approx ~ 3 d B }$ in SNR).

<html><body><table><tr><td colspan="2">GTZANDataset</td><td>Metrics</td><td>blu.</td><td>cla.</td><td>cou.</td><td>dis.</td><td>hip.</td><td>jaz.</td><td>met.</td><td>pop.</td><td>reg.</td><td>roc.</td><td>Avg.</td></tr><tr><td rowspan="4">Baselines</td><td rowspan="2">Gaussian (MLP)</td><td>SNR↑ LSD↓</td><td>0.68 3.059</td><td>0.25 3.175</td><td>2.64 3.028</td><td>2.40 3.379</td><td>1.02 3.761</td><td>0.15 3.494</td><td>1.66 3.634</td><td>4.61 3.030</td><td>1.33 3.047</td><td>1.06 3.219</td><td>1.58</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>3.383</td></tr><tr><td rowspan="2">Sine (MLP)</td><td rowspan="2">SND↑</td><td>7.472</td><td>2.865</td><td>2.3</td><td>7.34</td><td>3.04</td><td>1.01</td><td>4.30</td><td>8.776</td><td>1.7</td><td>1.</td><td>5.764</td></tr><tr><td>SNR↑</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.17</td><td>0.00</td><td>0.00</td><td>2.07</td><td>0.00</td><td>0.01</td><td>0.23</td></tr><tr><td rowspan="4"></td><td rowspan="2">B-Spline (KAN)</td><td>LSD↓</td><td>4.643</td><td>0.01 4.278</td><td>6.685</td><td>5.799</td><td>4.359</td><td>4.899</td><td>6.000</td><td>3.864</td><td>7.533</td><td>4.899</td><td>5.30</td></tr><tr><td>SNR ↑</td><td>11.80</td><td>10.76</td><td>11.98</td><td>12.00</td><td>11.30</td><td>12.75</td><td>11.25</td><td>12.07</td><td>11.57</td><td>11.84</td><td>11.73</td></tr><tr><td rowspan="2">RFF+Gaussian (MLP)</td><td rowspan="2">LSD↓</td><td>1.346 22.02</td><td>1.721</td><td>1.474</td><td>1.299</td><td>1.148</td><td>1.936</td><td>1.731</td><td>1.439</td><td>1.362</td><td>1.481</td><td>1.494</td></tr><tr><td>SNR↑</td><td>25.95</td><td>16.35</td><td>17.70</td><td>13.92</td><td>19.22</td><td>13.05</td><td>15.27</td><td>17.79</td><td>19.16</td><td>18.04</td></tr><tr><td></td><td rowspan="2"></td><td>LSD↓ SNR↑</td><td>0.696 13.80</td><td>0.585 15.05</td><td>1.064 12.54</td><td>1.036 12.87</td><td>0.741</td><td>0.983</td><td>0.902</td><td>1.245</td><td>0.883</td><td>0.714</td><td>0.885</td></tr><tr><td rowspan="2">Fourier-ASR (KAN) CSTRVCTKDataset</td><td rowspan="2">LSD↓</td><td>1.245</td><td>0.913</td><td>1.249</td><td>1.158</td><td>12.22 1.059</td><td>13.67 1.244</td><td>12.21 1.203</td><td>13.27 1.302</td><td>12.42 1.110</td><td>12.65 1.399</td><td>12.76 1.110</td></tr><tr><td>Metrics</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="2"></td><td>SNR↑</td><td>p225</td><td>p234</td><td>p238</td><td>p245</td><td>p248</td><td>p253</td><td>p335</td><td>p345</td><td>p363</td><td>p374</td><td>Avg.</td></tr><tr><td rowspan="4">Baselines</td><td rowspan="2">Gaussian (MLP)</td><td>LSD↓</td><td>1.88</td><td>2.09</td><td>1.16</td><td>4.06</td><td>0.23</td><td>2.39</td><td>1.37</td><td>3.06</td><td>5.56</td><td>2.25</td><td>2.41</td></tr><tr><td>SNR↑</td><td>2.126</td><td>2.034</td><td>2.557</td><td>1.831</td><td>2.884</td><td>2.065</td><td>2.277</td><td>1.896</td><td>1.791</td><td>1.827</td><td>2.129</td></tr><tr><td rowspan="2">Sine (MLP)</td><td>LSD↓</td><td>14.86</td><td>10.88</td><td>12.38</td><td>14.41</td><td>10.32</td><td>13.85</td><td>9.61</td><td>15.89</td><td>12.78</td><td>12.53</td><td>12.75</td></tr><tr><td>SNR↑</td><td>1.743 0.01</td><td>1.588</td><td>1.748</td><td>1.665</td><td>1.630</td><td>1.672</td><td>1.619</td><td>1.556</td><td>1.716</td><td>1.500</td><td>1.644</td></tr><tr><td rowspan="5"></td><td rowspan="2">B-Spline (KAN)</td><td>LSD↓</td><td>3.312</td><td>0.02 3.113</td><td>0.01 3.317</td><td>0.01 3.151</td><td>0.00 3.506</td><td>0.01 3.160</td><td>0.02 3.000</td><td>0.02 2.957</td><td>0.05 2.705</td><td>0.11 2.631</td><td>0.03 3.085</td></tr><tr><td>SNR ↑</td><td>11.67</td><td>12.93</td><td>16.19</td><td>11.99</td><td>15.52</td><td>12.21</td><td>13.32</td><td>15.95</td><td>12.79</td><td>12.28</td><td>12.81</td></tr><tr><td rowspan="2">RFF+Gaussian (MLP)</td><td>LSD↓</td><td>2.401</td><td>2.128</td><td>1.789</td><td>2.218</td><td>2.183</td><td>2.258</td><td>2.076</td><td>1.704</td><td>2.012</td><td>2.059</td><td>1.983</td></tr><tr><td>SNR↑</td><td>25.20</td><td>31.63</td><td>19.56</td><td>32.037</td><td>27.00</td><td>27.11</td><td>16.87</td><td>28.38</td><td>29.25</td><td>30.83</td><td>26.79</td></tr><tr><td rowspan="2">NeFF+Sine (MLP)</td><td>LSD↓</td><td>1.015</td><td>0.734</td><td>1.235</td><td>0.866</td><td>1.134</td><td>0.917</td><td>1.207</td><td>1.032</td><td>0.753</td><td>0.877</td><td>0.977</td></tr><tr><td>Fourier-ASR (KAN)</td><td>SNR↑ LSD↓</td><td>18.30 1.495</td><td>20.68 1.228</td><td>17.12 1.615</td><td>18.26 1.310</td><td>21.34 1.464</td><td>17.40 1.397</td><td>15.79 1.456 1.417</td><td>17.34 1.321</td><td>17.86 20.20</td><td>18.43 1.267 1.397</td></tr></table></body></html>

Table 3: Evaluation of Fourier-ASR and new nonlinear mapping designs on GTZAN and CSTR VCTK dataset.

Conversely, NeFF employs Fourier mappings, which are more compatible with Sine-type activation functions $\approx$ 9dB $\cdot$ in SNR).

# Evaluation of Fourier-ASR and New Designs

Based on the benchmark leaderboard presented in Table 2, we selected effective nonlinear mappings for comparison with Fourier-ASR on the GTZAN (Tzanetakis and Cook 2002) and CSTR VCTK (Yamagishi, Veaux, and MacDonald 2019) datasets. It is noteworthy that although Gaussian (Ramasinghe and Lucey 2022) and Sine (Sitzmann et al. 2020) activation functions were introduced to mitigate the complex parameter adjustments and pectral bias associated with positional encoding, we found that positional encoding remains essential due to the high-frequency nature and local periodicity of audio signals. Consequently, we designed new nonlinear mappings, namely RFF $^ +$ Gaussian and NeFF $+ ;$ Sine, to address these challenges.

As shown in Table 3, the designs RFF+Gaussian and NeFF $+ \mathrm { S }$ ine significantly enhance the ability of Coordinate-MLPs to represent audio signals. On the GTZAN dataset, these methods improve the SNR by $1 0 . 1 5 \mathrm { d B } \uparrow$ and $1 2 . 2 8 \mathrm { d } \mathbf { B }$ , respectively. On the CSTR VCTK dataset, the SNR improvements are $1 0 . 4 0 0 \mathrm { { B } \uparrow }$ and 14.04dB , respectively. Due to the periodic nature of Fourier basis functions and the Frequency-adaptive Learning Strategy (FaLS), our proposed Fourier-ASR(KAN) significantly outperforms Sine(MLP) $\mathrm { ( \approx ~ \ f d B ~ ) }$ and B-Spline(KAN) $\mathrm { \Omega } ( \approx 1 8 \mathrm { d } { \bf B } \mathrm { \Omega } , \$ ). However, because existing optimization strategies are not perfectly adapted to KAN networks (Liu et al. 2024), Fourier-ASR(KAN) performs slightly worse than the locally periodic NeFF+Sine(MLP). Nonetheless, FourierASR(KAN) does not require positional encoding, thereby avoiding the need for cumbersome hyperparameter tuning.

# Conclusion and Future Work

We proposed the first open-source benchmark for evaluating implicit neural audio signal representations based on Coordinate-MLPs, addressing a critical gap in standardized performance assessment. We demonstrated the effectiveness of combining positional encoding and nonlinear mapping designs of activation functions in the field of continuous audio representations. Additionally, we introduced a novel audio signal representation framework, FourierASR, which integrates the Fourier series theorem and the Kolmogorov-Arnold representation theorem, offering enhanced interpretability and more stable representational capacity. Our work not only guides the selection of components for Coordinate-MLP-based audio signal representations but also advances the development of audio representation applications. Due to the superior characteristics of implicit neural representations, such as continuous differentiability and decoupling from spatial resolution, our work can be effectively applied to downstream tasks such as audio super-resolution, denoising, compression, and generation.