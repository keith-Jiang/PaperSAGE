# UniMuMo: Unified Text, Music and Motion Generation

Han Yang1, Kun $\mathbf { S } \mathbf { u } ^ { 2 }$ , Yutong Zhang3, Jiaben Chen4, Kaizhi Qian5, Gaowen $\mathbf { L i u } ^ { 6 }$ , Chuang Gan4

1The Chinese University of Hong Kong,   
2University of Washington,   
3The University of British Columbia   
4University of Massachusetts Amherst,   
5MIT-IBM Watson AI Lab,   
6Cisco Research

# Abstract

We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities.

Code — https://github.com/hanyangclarence/UniMuMo Website — https://hanyangclarence.github.io/unimumo demo/ Extended version — https://arxiv.org/abs/2410.04534

# Introduction

Music and body movements are synchronized and inseparable. The beat and metrical structures in rhythm encourage the spontaneous coordination of body motion with music (Large 2000), activating the motor-related areas of human brains (Keller and Rieger 2009). Dance particularly exemplifies this connection through choreography that aligns with the music’s rhythm, melody and emotion. Meanwhile, even though most people are not professional musicians or dancers, they often interpret music and dance using simple, natural language. This descriptive text serves as a vital bridge between understandable ideas and abstract concepts in music and motion.

The synergy between music, motion, and text provides a natural motivation to create a model capable of understanding and creating contents across all these modalities. Moreover, building a framework that can flexibly generate music, motion, and text in arbitrary combinations is crucial for real-world applications, even though existing models already achieve impressive results in unidirectional generation tasks such as text-to-music (Copet et al. 2023), music-to-motion (Tseng, Castellon, and Liu 2023), motionto-music (Zhu et al. 2022a) and motion-to-text (Jiang et al. 2023). In the real world, there is a demand for diverse generative abilities, and more complex generation tasks may be necessary, such as creating dance sequences based on both music and textual descriptions. Training individual models for each unique combination, although potentially yielding better output quality, would significantly increase training costs, deployment efforts and storage requirements. Thus, a unified model that supports all combinations of conditioning and generation tasks, rather than a collection of separate models or training adapters to incorporate individual models, offers a more cost-effective solution. To this end, we introduce a novel task of dynamically generating music, motion, and text in a multitude of combinations unifiedly. As demonstrated in Fig. 1, this task is designed to handle diverse generative scenarios, ranging from text-to-music, text-to-motion, to more complex combinations like text-to-music-plus-motion or music-plus-text-to-motion.

However, the task could be challenging, especially in two aspects: i) the lack of comprehensive datasets that include all three modalities - music, motion, and text - limits the development of a general and unified model. While there are individual datasets for music-only (Santana et al. 2020), motion-only (Mahmood et al. 2019), music to motion (Li et al. 2021b) and text to motion (Guo et al. 2022a), a holistic and large-scale dataset that encompasses all three modalities still remains absent; ii) designing a unified architecture that supports both the conditioning and generation of all three modalities is challenging, mainly due to the significant differences between the neural representations for the three modalities and the multiplicity of desired generation tasks.

To address the first challenge of lacking paired data, we propose to align unpaired music and motion sequences based on their rhythmic patterns. Specifically, we extract both music beats and motion visual beats, then employ dynamic time

- Text “The audio is a rap song with keyboard Aligned accompaniment. This is a la style Description TAsTsl Music&Motion hip-hop style dance.” Music 三三 UniMuMo 树材 Motion Aligned with Music 三 三 … Music Aligned with Motion Motion #树#h “The music is a rock song with a strong Music & Motion emphasis on drumand guitar.The dance is that a man wiggles his shoulders." Caption

warping to find the alignment and warp the motion sequence to adjust the motion visual beats to match the music beats. We found that such augmentation is accurate and efficient. With the augmented synchronized music-motion data, we can utilize existing music and motion datasets to train our unified generative model. Additionally, we construct text descriptions from music and motion metadata using a mixture of template filling, large language model generation and musicbased language model generation, striking a balance between diversity, language fluency and description accuracy.

To overcome the second challenge, we propose a novel framework, UniMuMo, to unify the generation of different modalities. Our pipeline consists of three main stages: a music-motion joint tokenizer that encodes music and motion sequences into discrete representations within the same space, a music-motion transformer-decoder model trained on the task of music-motion joint generation, and a music-motion captioner that generates text descriptions from music and motion features. In the first stage, we bridge the modality gap between music and motion by mapping motion into the music feature space. Specifically, instead of using separate Vector-Quantized Variational Autoencoders (VQ-VAE) to quantize music and motion sequences, we encode motion with the codebook of a pre-trained music VQ-VAE, namely Encodec (D´efossez et al. 2022). This design facilitates the unification of music and motion within the same generative framework in the subsequent stage. In the second stage, we train a unified music and motion generative model with a novel task of music-motion joint generation from text conditions. To enable the mutual conditioning of music and motion, and unlock the music-to-motion and motion-to-music generation capabilities, we introduce a novel music-motion parallel generation scheme, where we perform two mutually conditioned streams of autoregressive generation of aligned music and motion simultaneously. With the reuse of Encodec and joint encoding of motion in the previous stage, the current stage can be effectively achieved by fine-tuning the pretrained text-to-music model associated with Encodec, namely MusicGen (Copet et al. 2023), equipping it with additional motion conditioning and generation capabilities while maintaining its music generation capabilities. In the third stage, we fine-tune a T5 decoder for music and motion captioning tasks, using the features extracted by the music-motion decoder trained in stage 2. To transform the decoder into an effective feature extractor, we replace its causal self-attention layers with trainable full self-attention layers, and fine-tune them together with the T5 decoder on music and motion captioning tasks. Extensive experiments demonstrate that UniMuMo achieves competitive performance across all unidirectional generation tasks in music, motion, and text when compared with existing state-of-the-art models, demonstrating the effectiveness and versatility of our approach.

Our work offers significant advancements in multimodal generative research, summarized as follows:

• To the best of our knowledge, this is the first unified framework capable of arbitrarily generating content across music, motion, and text.   
• To address the shortage of paired multimodal data, we augment and enrich existing large-scale datasets with musicmotion data alignment and text augmentations.   
• We propose a novel joint codebook for encoding music and motion sequences, along with a music-motion parallel generation scheme, facilitating multiple generation tasks within a single architecture.   
• Our framework achieves results comparable to SOTAs across all generation tasks in music, motion, and text.

# Related Work

Text to Music. Text-conditioned music generation has been widely studied in recent years. There are two main branches: diffusion-based and transformer-based. For diffusion-based models, Riffusion (Forsgren and Martiros 2022) uses a latent text-to-image diffusion model to generate spectrograms, which are then converted into audio clips; Mousai (Schneider, Jin, and Sch¨olkopf 2023) proposes training a diffusion model in the latent space of a diffusion autoencoder; Noise2Music (Huang et al. 2023a) introduces a cascade of diffusion models that first generates the audio in a coarse form and then progressively refine it. AudioLDM (Liu et al. 2023a)

proposes to train a latent diffusion model using CLAP (Wu et al. 2023) embeddings, a language-audio joint representation, for text conditioning. For transformer-based models, MusicLM (Agostinelli et al. 2023) proposes to encode music into high-level ”semantic tokens” and low-level ”acoustic tokens”, and use a cascade of transformer decoders to generate the two levels stage by stage. MusicGen (Copet et al. 2023) leverages a single-stage transformer decoder to model the hierarchical music tokens directly.

Music to Text. Several models have been proposed for audio captioning. WAC (Kadlˇc´ık et al. 2023) proposes to transfer a pre-trained speech-to-text Whisper model to the music captioning task. LTU (Gong et al. 2023) takes the concatenated music embeddings and text embeddings as input to a large language model and directly trains caption generation using language modeling objectives. LP-MusicCaps (Doh et al. 2023) uses a transformer encoder-decoder structure, where the music spectrogram is first encoded by the encoder and then cross-attended by the decoder for text generation. MU-LLaMA (Liu et al. 2023b) leverages a frozen LLaMA (Touvron et al. 2023) and fine-tunes a Music Understanding Adapter to fuse music features into the LLaMA model.

Music to Motion. Most of the works on music-conditioned dance generation are based on transformers. Several approaches (Li et al. 2021a; Fan et al. 2022; Pu and Shan 2022) adopt similar structures that first use a music transformer encoder and a motion transformer encoder to encode music and initial motion into representations separately, and then employ a transformer decoder for cross-modal fusion and motion generation. Bailando (Siyao et al. 2022) proposes to train a transformer on motion features encoded by a choreographic memory module, which is the codebook of a motion VQVAE. Besides autoregressive transformers, EDGE (Tseng, Castellon, and Liu 2023) adopts a transformer-based diffusion model capable of both dance generation and editing.

Motion to Music. Most of the relevant works focus on generating corresponding music from video input. Foley Music (Gan et al. 2020) focuses on generating music for videos of people playing instruments, and uses Musical Instrument Digital Interface (MIDI) to bridge the gap between body key points and the final music. Similarly, RhythmicNet (Su, Liu, and Shlizerman 2021) extends the scenarios to arbitrary motion videos by first estimating visual rhythm and conditionally generating drum and piano music. Dance2Music (Aggarwal and Parikh 2021) encodes a dance similarity matrix with CNN and predicts the next note with an LSTM autoregressively. CDCD (Zhu et al. 2022b) proposes a single-stage method that uses a discrete latent diffusion model to generate music spectrograms conditioned on video features. D2MGAN (Zhu et al. 2022a) proposes a GAN-based model to generate the music tokens based on video and pose features.

# Text-Music-Motion Aligned Data Generation

To model arbitrary generation across music, motion, and text, we propose to expand existing music and motion datasets by aligning motion with music and synthesizing textual descriptions. The data generation pipeline includes four major steps: 1) music beat detection, 2) visual beat detection, 3) music-motion alignment, and 4) text description synthesis.

Music Beat Detection. We estimate music beats from a music waveform $\boldsymbol { Y } \in \mathbb { R } ^ { T _ { w } }$ , where $T _ { w }$ represents the number of samples, using a Bidirectional-LSTM-based model from (Chiu, Su, and Yang 2021). This model performs beat tracking on extracted drum features and non-drum features separately, then aggregates the results with a learnable fuser. We manually evaluate the accuracy of this beat tracking model and find that it performs well in most test cases, outperforming the beat tracking methods in the Librosa API (McFee et al. 2015). The resulting music beats are represented as a binary sequence $B _ { m } \in \mathbb { R } ^ { \breve { T } _ { w } }$ , where each frame is marked as ‘beat’ or ‘non-beat.’

Visual Beats Detection. Given a 3D motion sequence $M \in$ $\mathbb { R } ^ { T _ { m } \times J \times 3 }$ where $T _ { m }$ represents the number of frames, $J$ the number of joints, and the last dimension indicates $x , y , z$ coordinates, we obtain visual beats in three steps. In the first stage, we calculate the motion directogram (Davis and Agrawala 2018), a 2D matrix that factors motion into different motion angles, similar to how an audio spectrogram factors sound amplitude into different frequencies. Specifically, we first compute the first-order difference of the motion sequence $\Delta M _ { t } = M _ { t } - M _ { t - 1 }$ . Based on its motion angle, we assign the motion magnitude of every joint into one of the bins in $2 \pi / N _ { \mathrm { b i n s } }$ . The motion directogram $M _ { d } ( t , \theta )$ is obtained by summing the motion magnitudes of each bin: $\begin{array} { r } { M _ { d } ( t , \theta ) \ \stackrel { \cdot } { = } \ \sum _ { j } \Delta \bar { M } _ { t } ( j ) \mathbf { 1 } _ { \theta } \big ( \angle M _ { t } ( j ) \big ) \big ( \cdot \big ( \Delta \bar { M } _ { t } ( j ) \big ) } \end{array}$ , where $\mathbf { 1 } _ { \theta } ( \phi ) = 1$ if $| \theta - \phi | \leq 2 \pi / \bar { N _ { \mathrm { b i n s } } }$ else 0. In the second stage, we convert the motion directogram to the kinematic offset $M _ { k }$ , which represents the motion changes, similar to the onset envelope in an audio spectrogram. We first obtain motion flux $M _ { f }$ , which represents the deceleration in various directions, by computing the negative first-order difference of the directogram $\Delta M _ { d }$ . We then average each frame of $M _ { f }$ and filter the top $1 \%$ peaks to obtain kinematic offset $M _ { k }$ . In the last stage, we use dynamic programming to compute the visual beats by designing an objective function that selects strong visual changes from kinematic offsets and encourages equal-spacing beats. More details can be found in Appendix. The final visual beats are also represented as a binary sequence $B _ { v } \in \mathbb { R } ^ { T _ { m } }$ , where each frame is marked as ‘beat’ or ‘non-beat’.

Music-Motion Alignment. We apply dynamic time warping to determine the optimal matching between music beats $B _ { m }$ and visual beats $B _ { v }$ , finding the alignment even though the duration of these two binary sequences could be different. Finally, we warp motion sequences by interpolating according to the warping curve to obtain aligned music-motion pairs. The reason for warping motion to match music, rather than the reverse, is that music beats tend to be steady, so warping music could result in perceptually unacceptable changes. More details can be found in Appendix.

Text Description Synthesis. To compensate for the absence of text descriptions in our used datasets, we employ two methods for captions synthesis: (1) using Music Understanding Language Model to generate caption directly from audio; and (2) using Large Language Model to synthesize captions from metadata (genre, tempo, etc.), striking a balance between musical accuracy and diversity. Examples and more details are shown in Appendix.

# UniMuMo Framework

UniMuMo consists of three training stages to enable arbitrary generation between music, motion, and text. In stage 1, we encode aligned music and motion data into discrete tokens. To efficiently bridge the gap between the two modalities, we propose to use a frozen pre-trained audio tokenizer Encodec (De´fossez et al. 2022) and train a motion tokenizer that reuses the same residual codebooks of the audio tokenizer. In stage 2, we fine-tune a state-of-the-art text-to-music transformer decoder (Copet et al. 2023) by conducting the task of generating music and motion tokens simultaneously with music and motion text descriptions. At the inference stage, we can perform parallel generation to unlock applications of music and motion generation. In stage 3, we treat the pretrained music-motion decoder model in stage two as a feature extractor and fine-tune a T5 decoder on language modeling task for music and motion captioning. An overview of the UniMuMo framework is shown in Figure 2.

# Stage 1. Music and Motion Joint Tokenization

While existing tokenization approaches can faithfully reconstruct the music or motion individually, the correlations between the two modalities become intricate in distinct spaces. Therefore, directly applying them in the unified generation framework poses challenges. Besides, a music tokenizer usually requires more training resources and time to achieve high-quality reconstruction than a motion tokenizer. Inspired by these facts, we introduce an efficient and effective way to encode music and motion into a joint latent space. We propose using a pre-trained audio tokenizer, Encodec (De´fossez et al. 2022), and training a new motion encoder-decoder. The motion encoder encodes the motion into the same embedding space as the music and reuses the frozen music Residual Vector Quantizers (RVQ) to discretize the motion into tokens. From these tokens, the motion decoder can decode to reconstruct the motion. Given the higher complexity and richer information in music compared to motion, the learned music codebook is theoretically capable of encoding motion.

Specifically, given a waveform $\boldsymbol { Y } \in \mathbb { R } ^ { \boldsymbol { T } \cdot \boldsymbol { f _ { w } } }$ with $T$ the audio duration and $f _ { w }$ the sample rate, Encodec first encodes it into a continuous tensor of $X _ { \mathrm { m u s i c } } \in \mathbb { R } ^ { d \times T \cdot f _ { r } }$ , where $f _ { r } \ll f _ { w }$ is the frame rate of the residual codebook and $d$ is the dimension of codebook entries. $X _ { \mathrm { m u s i c } }$ is then quantized by the RVQ into music tokens $Q _ { \mathrm { m u s i c } } \in \{ 1 , \dots , M \} ^ { K \times T \cdot f _ { r } }$ where $K$ is the number of RVQ and $M$ is the number of codebook entries. For an aligned motion sequence of the same duration $M \in \mathbb { R } ^ { d _ { m } \times \mathbf { \breve { T } } \cdot f _ { m } }$ with frame rate $f _ { m }$ and feature dimension $d _ { m }$ , our motion encoder encodes it into $X _ { \mathrm { m o t i o n } } \in \mathbb { R } ^ { d \times T \cdot f _ { r } }$ , the same shape as $X _ { \mathrm { m u s i c } }$ , which is then tokenized by the same RVQ into motion tokens $Q _ { \mathrm { m o t i o n } } \in \{ 1 , \ldots , \dot { M } \} ^ { K \times T \cdot f _ { r } }$ . The motion decoder decodes the motion feature after RVQ, resulting in $\hat { M }$ . The motion encoder-decoder is trained by minimizing the motion reconstruction loss together with a commitment loss $\mathcal { L } _ { \mathrm { c o m m i t } }$ from

the codebook:

$$
\mathcal { L } _ { \mathrm { t o t a l } } = \frac { 1 } { | \mathcal { D } | } \sum _ { M \in \mathcal { D } } \left( \| M - \hat { M } \| _ { 2 } + \lambda \mathcal { L } _ { \mathrm { c o m m i t } } \right)
$$

where $\mathcal { D }$ is the motion dataset and $\lambda$ controls the strength of the commitment loss. Empirically, $\lambda$ is set to 0.02.

With this design, the music-motion joint tokenization can effectively learn multimodal correlations by mapping motion features into the same space as music, without the need to train another computationally heavy music autoencoder. Moreover, it enables direct use the text-to-music model associated with Encodec as an initialization for the following music-motion decoder model, significantly reducing training costs and enhancing the performance. Experimentally, such feature alignment is crucial to learning the joint generation of music and motion within a single transformer model.

# Stage 2. Music and Motion Generation from Text

In this stage, we modify and fine-tune an existing state-of-theart text-to-music model with the music and motion tokens extracted from Stage 1, enabling it to handle all tasks related to music and motion generation, such as text-to-musicmotion and motion-to-music. In particular, we employ MusicGen (Copet et al. 2023), an open-source, single-stage transformer decoder model that can generate multi-level music tokens with a specific codebook interleaving pattern. Following their practice, we apply the delay pattern for both music and motion tokens, utilize a T5 encoder for encoding text descriptions, and adopt cross-attention to incorporate text conditioning features into the transformer decoder.

To enable the autoregressive generation of music and motion within a unified framework, we propose training on the task of music-motion joint generation, together with a novel parallel generation scheme, where two streams (i.e., music and motion) of predict-next-token generation are conducted simultaneously, with each stream conditioned on each other. Specifically, given the music tokens $Q _ { \mathrm { m u s i c } }$ and motion tokens $Q _ { \mathrm { m o t i o n } }$ with the same shape $K \times S$ where $S = T \cdot f _ { r }$ is the sequence length, we first transform them with delay pattern (Copet et al. 2023) into $Q _ { \mathrm { m u s i c } } ^ { \prime }$ and $Q _ { \mathrm { m o t i o n } } ^ { \prime }$ respectively, resulting shape $K \times S ^ { \prime }$ , where $S ^ { \prime } = S + K - 1$ . We then concatenate them in time dimension into $Q _ { \mathrm { i n p u t } }$ of the shape $K \times 2 S ^ { \prime }$ as the input to the transformer decoder. The model’s output is transformed back to the normal pattern for loss calculation. Training on music-motion joint generation, we adopt the predict-next-token objectives for both music and motion tokens in each forward pass:

$$
\begin{array} { r l } & { \mathcal { L } = - \displaystyle \frac { 1 } { | \mathcal { D } | } \sum _ { Q \in \mathcal { D } } \left\{ \mu \cdot \sum _ { t = 1 } ^ { S } \log \mathbb { P } \left[ Q _ { t } ^ { \mathrm { m u s i c } } | Q _ { < t } ^ { \mathrm { m u s i c } } , Q _ { < t } ^ { \mathrm { m o t i o n } } \right] \right. } \\ & { \qquad \left. + ( 1 - \mu ) \cdot \sum _ { t = 1 } ^ { S } \log \mathbb { P } \left[ Q _ { t } ^ { \mathrm { m o t i o n } } | Q _ { < t } ^ { \mathrm { m u s i c } } , Q _ { < t } ^ { \mathrm { m o t i o n } } \right] \right\} } \end{array}
$$

where $\mu$ balances between music loss and motion loss, and $\mathbb { P }$ denotes predict-next-token probability of the model. Empirically, $\mu$ is set to 0.85. To enable the parallel autoregressive generation, we apply a cross-modal causal attention mask, as

村材書 Reconstruction0 RVQ

![](images/919e05e910e737a3c8c1c1ce48dc8366bcc4b582195ec4948f7a4afcb6edcab5.jpg)  
Stage 1: Music-Motion Joint Tokenization   
Figure 2: Overview: The training of UniMuMo consists of three stages: In stage 1, we train a motion RVQ-VAE using the frozen codebook from a pre-trained music RVQ-VAE to encode motion into the same space as music. In stage 2, we fine-tune a pre-trained music transformer decoder model on the text-to-music-motion task using the music-motion parallel generation scheme. In stage 3, we fine-tune a T5 decoder for music-motion captioning using the previous music-motion decoder as a feature extractor.

shown in Stage 2 of Figure 2. The causal attention mask is of shape $2 S ^ { \prime } \times \mathsf { \bar { 2 } } S ^ { \prime }$ , each quarter of which is an $S ^ { \prime } \times S ^ { \prime }$ lower triangular matrix, allowing music and motion tokens to have both cross-modal and uni-modal causal attention. A further illustration of the strategy can be found in Appendix.

With the above construction, the model can perform parallel sampling during inference, enabling the prediction of the next token for both music and motion concurrently:

$$
\begin{array} { r l } & { \hat { \boldsymbol Q } _ { t } ^ { \mathrm { m u s i c } } = \underset { i \in { \cal M } } { \arg \operatorname* { m a x } } \mathbb P [ \boldsymbol Q _ { t , i } ^ { \mathrm { m u s i c } } | \hat { \boldsymbol Q } _ { < t } ^ { \mathrm { m u s i c } } , \hat { \boldsymbol Q } _ { < t } ^ { \mathrm { m o t i o n } } ] } \\ & { \hat { \boldsymbol Q } _ { t } ^ { \mathrm { m o t i o n } } = \underset { i \in { \cal M } } { \arg \operatorname* { m a x } } \mathbb P [ \boldsymbol Q _ { t , i } ^ { \mathrm { m o t i o n } } | \hat { \boldsymbol Q } _ { < t } ^ { \mathrm { m u s i c } } , \hat { \boldsymbol Q } _ { < t } ^ { \mathrm { m o t i o n } } ] } \end{array}
$$

where $M$ is the codebook size. With this sampling strategy, we can conduct the joint generation of music and motion under text conditions. Additionally, it facilitates zero-shot music-to-motion and motion-to-music generation. For example, given a music sequence $Q _ { 1 : S } ^ { \mathrm { m u s i c } }$ , an aligned motion sequence can be autoregressively sampled by

$$
\hat { Q } _ { t } ^ { \mathrm { m o t i o n } } = \underset { i \in { \cal M } } { \mathrm { a r g m a x } } \mathbb { P } [ Q _ { t , i } ^ { \mathrm { m o t i o n } } | Q _ { < t } ^ { \mathrm { m u s i c } } , \hat { Q } _ { < t } ^ { \mathrm { m o t i o n } } ]
$$

An illustration of the sampling process can also be found in Appendix.

Considering the inherent differences between music and motion, we further introduce the following changes to the pre-trained MusicGen to alleviate the mutual interference between the two modalities. First, we add another trainable embedder for motion tokens, with which the model can learn to differentiate the two modalities. Second, to ensure the temporal parallelism, we add positional encodings $\{ E _ { 1 } , E _ { 2 } , . . . , E _ { S ^ { \prime } } \}$ to music and motion separately, instead of using a holistic positional encoding of length $2 S ^ { \prime }$ . Third, inspired by the idea of Mixture of Experts (MoE), we introduce an additional feed-forward network (FFN) for motion in each transformer layer. As shown in Fig. 2, in each forward pass, the first half of the feature (i.e., music features) is processed by the music FFN, and the second half (i.e., motion features) by the motion FFN. Fourth, we add a new motion classification head at the end of the network to distinguish motion code prediction from music code prediction. Note that for the new modules introduced above, we initialize the motion embedder and FFNs with the corresponding components from the pre-trained MusicGen. With a joint motion VQVAE trained in Stage 1, such initialization ensures that music features are not confused by uninitialized motion features at the beginning of training, allowing the music generation capability to be better preserved.

Following MusicGen, text conditioning is added with cross-attention. In the framework of music-motion joint generation, we add the text condition of two modalities independently. We first encode music descriptions and motion descriptions separately into features and apply classifier-free guidance dropout independently. Then, during cross-attention on text conditions, we specialize the attention mask to allow music features to attend only to music conditions and motion features to attend only to motion conditions.

By fine-tuning the model on the music-motion dataset with the above settings, we find that the model learns to generate motion in parallel with music quickly while still keeping its music-generation ability. With a single training task of music-motion joint generation, various applications could be achieved in a zero-shot fashion, including text-to-music, text-to-motion, music-to-motion, motion-to-music, motiontext-to-music, etc..

<html><body><table><tr><td>Models</td><td>FADvGG↓</td><td>KL↓</td><td>CLAP↑</td></tr><tr><td>Riffusion (Forsgren and Martiros 2022)</td><td>14.8</td><td>2.06</td><td>0.19</td></tr><tr><td>Mubert(Mubert-Inc.2022)</td><td>9.6</td><td>1.58</td><td>1</td></tr><tr><td>Mousai (Schneider,Jin,and Scholkopf 2023)</td><td>7.5</td><td>1.59</td><td>0.23</td></tr><tr><td>MusicLM(860M) (Agostinelli etal.2023)</td><td>4.0</td><td>1.31</td><td></td></tr><tr><td>MusicGen (300M) (Copet et al.2023)</td><td>4.9</td><td>1.42</td><td>0.27</td></tr><tr><td>AudioLDM2-Full (346M) (Liu etal.2023a)</td><td>3.13</td><td>1.17</td><td>0.38</td></tr><tr><td>Ours (300M)</td><td>5.93</td><td>1.99</td><td>0.27</td></tr><tr><td>MusicGen (fine-tuned on our data)</td><td>5.81</td><td>1.97</td><td>0.28</td></tr><tr><td>Ours (trained on datawith vocals)</td><td>4.11</td><td>1.95</td><td>0.29</td></tr></table></body></html>

Table 1: Comparison of text-to-music generation on MusicCaps. Bold and underlined results are the best and secondbest results.

# Stage 3. Music and Motion Captioning

The final stage is for caption generation, where we treat the fine-tuned music-motion decoder in the previous stage as a feature encoder for music and motion, and fine-tune another T5 decoder to generate captions for music and motion.

However, using the music-motion decoder directly as a feature extractor brings challenges. Firstly, the self-attention in the decoder is done causally, which is inadequate for capturing rich music and motion features. Secondly, since the input of the model is the concatenation of music and motion, we are limited to input music-motion pairs for captioning, which is inflexible.

To address these issues, we introduce a trainable full selfattention module, initialized with the trained cross-modal causal attention module, as shown in Fig. 2, Stage 3. Inspired by BLIP (Li et al. 2022), which claims that the major difference between transformer encoders and decoders lies in the self-attention layers, with embedding layers and FFNs functioning similarly, we therefore fine-tune only the newly introduced full self-attention modules together with the T5 decoder on caption generation task, keeping the rest of the music-motion decoder unchanged. Considering that captions of music and motion are independent, we remove the crossattention areas on the attention mask.

In practice, we first randomly mask the entire music or motion tokens as empty, and concatenate them together as input $Q _ { \mathrm { i n p u t } }$ . This allows us to conduct music or motion captioning independently. Next, we forward it through the music-motion decoder with a null condition, where full self-attention is applied. We then take the output of the last hidden layer of the model as the feature, which is cross-attended by the T5 text decoder. We fine-tune the model with the language modeling task, and the generation target is either music caption or motion caption, depending on the input masking.

# Experiment

# Evaluations

We conduct extensive evaluations of our model across various tasks and metrics. More implementation details about hyperparameter choices, dataset, metrics and training/evaluation setups are in Appendix.

Text-to-Music. In Table 1, we compare our UniMuMo with Riffusion (Forsgren and Martiros 2022), Mubert (MubertInc. 2022), Mousai (Schneider, Jin, and Sch¨olkopf 2023),

Table 2: Comparison of motion-conditioned music generation on $\mathrm { \ A I S T + + }$ .   

<html><body><table><tr><td>Models</td><td>BeatsCoverage↑</td><td>BeatsHit↑</td></tr><tr><td>Dance2Music (Aggarwal and Parikh 2021)</td><td>83.5</td><td>82.4</td></tr><tr><td>Foley Music (Gan et al. 2020)</td><td>74.1</td><td>69.4</td></tr><tr><td>CMT (Di et al. 2021)</td><td>85.5</td><td>83.5</td></tr><tr><td>D2M-GAN(Zhu etal.2022a)</td><td>88.2</td><td>84.7</td></tr><tr><td>CDCD (Zhu et al. 2022b)</td><td>93.9</td><td>90.7</td></tr><tr><td>Ours</td><td>93.0</td><td>88.4</td></tr></table></body></html>

Table 3: Comparison of music-conditioned and textconditioned dance generation.   

<html><body><table><tr><td>Models</td><td>Distk→</td><td>Distg→</td><td>Beat Align.↑</td></tr><tr><td>Real</td><td>10.61</td><td>7.48</td><td>0.24</td></tr><tr><td>Bailando (Siyao etal. 2022)</td><td>7.92</td><td>7.72</td><td>0.23</td></tr><tr><td>FACT (Li et al. 2021a)</td><td>10.85</td><td>6.14</td><td>0.22</td></tr><tr><td>EDGE(Tseng,Castellon,and Liu 2023)</td><td>10.58</td><td>7.62</td><td>0.27</td></tr><tr><td>Ours (music conditioned)</td><td>10.68</td><td>10.35</td><td>0.24</td></tr><tr><td>Ours (text conditioned)</td><td>9.14</td><td>9.37</td><td>0.25</td></tr></table></body></html>

MusicLM (Agostinelli et al. 2023), MusicGen (Copet et al. 2023) and AudioLDM 2 (Liu et al. 2023a). We evaluate the performance on MusicCaps, with results of SOTAs directly sourced from their respective papers. We employ three metrics: Frechet Audio Distance (FADVGG) (Kilgour et al. 2018), Kullback-Leibler Divergence (KL) (Kreuk et al. 2022) and CLAP similarity (CLAP) (Wu et al. 2023; Huang et al. 2023b). The first two metrics measure the audio quality, while the last one measures the correspondence between generated audio and text descriptions. Note that the audio quality of our model does not match with SOTA models. We argue that this might be due to the poor audio quality of our training data. Following MusicGen, we also use vocal-free training data. To achieve this, we use Demucs (De´fossez 2021; Rouard, Massa, and D´efossez 2023) to remove the vocal part of the music in Music4All dataset. Nonetheless, we observe that many of the processed audio are of bad quality. This is testified by the experiment of fine-tuning MusicGen on our dataset for the same number of epochs while keeping all other settings the same (e.g., sequence length, batch size). As shown in Table 1, the audio quality of the tuned model also degrades. We also tried training the model on the original dataset with vocals, resulting in improved quantitative scores. However, the generated music is not perceptually good, often filled with weird and meaningless vocals. This phenomenon, where training on music with vocals yields better quantitative scores, is also reported in MusicGen.

Dance-to-Music. In Table 2, we compare UniMuMo with Dance2Music (Aggarwal and Parikh 2021), Foley Music (Gan et al. 2020), CMT (Di et al. 2021), D2M-GAN (Zhu et al. 2022a) and CDCD (Zhu et al. 2022b) on danceconditioned music generation. For evaluation, we adopt Beats Coverage and Beats Hit (Zhu et al. 2022a), both of which measure the alignment of generated music with motion.

Table 4: Comparison of music captioning on MusicQA dataset.   

<html><body><table><tr><td>Models</td><td>Bleu↑</td><td>Meteor↑</td><td>Rouge↑</td><td>BertScore↑</td></tr><tr><td>LTU (Gong et al. 2023)</td><td>0.238</td><td>0.250</td><td>0.332</td><td>0.876</td></tr><tr><td>LP-MusicCaps (Doh et al.2023)</td><td>0.165</td><td>0.202</td><td>0.281</td><td>0.879</td></tr><tr><td>MU-LLaMA (Liu et al.2023b)</td><td>0.238</td><td>0.354</td><td>0.475</td><td>0.913</td></tr><tr><td>Ours</td><td>0.261</td><td>0.291</td><td>0.369</td><td>0.892</td></tr></table></body></html>

Table 5: Comparison of motion captioning on HumanML3D dataset.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="2">R-Precision↑ Top3</td><td rowspan="2">MMDist↓</td><td colspan="2">Bleu↑</td><td rowspan="2">ROUGE-L↑</td><td rowspan="2">Cider↑</td><td rowspan="2">BertScore↑</td></tr><tr><td>Top1</td><td></td><td>@1</td><td>@4</td></tr><tr><td>Real</td><td>0.506</td><td>0.800</td><td>2.986</td><td>-</td><td>：</td><td></td><td>-</td><td></td></tr><tr><td>MotionGPT (Jiang et al.2023)</td><td>0.534</td><td>0.803</td><td>2.978</td><td>42.61</td><td>6.04</td><td>34.47</td><td>7.92</td><td>31.57</td></tr><tr><td>TM2T(Guo et al.2022b)</td><td>0.525</td><td>0.814</td><td>2.995</td><td>61.76</td><td>21.98</td><td>47.40</td><td>71.12</td><td>37.27</td></tr><tr><td>Ours</td><td>0.520</td><td>0.806</td><td>2.958</td><td>52.84</td><td>9.27</td><td>40.11</td><td>6.22</td><td>40.90</td></tr></table></body></html>

Music/Text-to-Dance. In Table 3, we compare UniMuMo’s dance-generation capabilities with Bailando (Siyao et al. 2022), FACT (Li et al. 2021a) and EDGE (Tseng, Castellon, and Liu 2023) on $\mathrm { A I S T + + }$ dataset. We evaluate UniMuMo on both music-conditioned and text-conditioned dance generation tasks. Although there is currently no established benchmark for the text-to-dance task, we can also apply the same evaluation metrics to measure and compare the quality of generated dance. For evaluation metrics, we adopt kinetic distribution spread $\mathrm { ( D i s t _ { k } ) }$ and geometric distribution spread $\mathrm { ( D i s t _ { g } ) }$ to measure the diversity. Additionally, we employ the beat alignment score to measure the alignment between conditioning audio and generated dance. Following EDGE, we evaluate the motion sequences on 5-second clip. For text-todance, we directly evaluate the dance that is jointly generated with music, conditioned on both music and motion captions, and we calculate the beat alignment score between the generated dance and music. The quantitative scores show that UniMuMo achieves competitive results on music-conditioned dance generation, even though it hasn’t been fine-tuned on $\mathrm { \ A I S T + + }$ music. For text-conditioned generation, it achieves inferior dance quality since there is no ground truth music for reference, but also gains a higher beat alignment score due to the joint generation.

Music-to-Text. In Table 4, we compare UniMuMo against SOTA music captioning models including LTU (Gong et al. 2023), LP-MusicCaps (Doh et al. 2023) and MULLaMA (Liu et al. 2023b). The evaluation is conducted on the MusicQA dataset released by (Liu et al. 2023b), which is a music-related question-answering dataset. We take the answers to the question ”Describe the audio” together with the corresponding music as evaluation data, totaling 552 musiccaption pairs. Following MU-LLaMa, the metrics we use includes Bleu, Meteor, RougeL and BertScore, which are all common evaluation metrics in natural language processing.

Motion-to-Text. In Table 5, we compare UniMuMo with TM2T (Guo et al. 2022b) and MotionGPT (Jiang et al. 2023) for motion captioning using the HumanML3D test set. Following MotionGPT, we adopt the motion-retrieval precision (R-Precision) to measure the accuracy of motion-text matching using top-1 and top-3 retrieval accuracy, multi-modal distance (MM Dist) to measure the distance between motion and text, and other popular natural language processing metrics, including Blue, Rouge, Cider and BertScore, to assess the linguistic quality. Since we source only $50 \%$ of our training motion data from HumanML3D, and the motion is augmented to align with music beats, UniMuMo still lags behind the best SOTA in certain metrics for HumanML3D motion captioning task.

Based on the quantitative results presented above, UniMuMo achieves competitive performance compared to the SOTA benchmarks across various single-modal generation tasks. Specifically, in the motion-to-music, music-to-motion, music captioning and motion captioning tasks, UniMuMo generally ranks second among the SOTAs. However, in the text-to-music task, UniMuMo’s performance is not as competitive, which we argue may be attributed to the limitations in our training data.

# Conclusion

In this paper, we introduce UniMuMo, the first unified framework for arbitrary generation across music, motion, and text. To address the limitations of paired multimodal data, we expand existing datasets with rhythm-based music-motion alignment and text augmentation, thus creating a comprehensive new dataset. To build a unified model, we propose novel architectural designs, including a music-motion joint tokenizer for bridging modality gaps and a music-motion parallel generation scheme for synchronized music and motion generation. Extensive experiments show that UniMuMo achieves competitive performance in all unidirectional generative tasks. We believe our framework will not only open up new avenues for multimodal generation but also inspire future advancements in this rapidly evolving field.