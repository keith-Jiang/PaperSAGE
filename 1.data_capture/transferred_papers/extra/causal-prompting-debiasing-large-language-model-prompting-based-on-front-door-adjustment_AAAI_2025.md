# Causal Prompting: Debiasing Large Language Model Prompting Based on Front-Door Adjustment

Congzhi Zhang\*1 Linhai Zhang\*2 Jialong $\mathbf { W _ { u } } ^ { * 1 }$ Yulan $\mathbf { H e } ^ { 2 . }$ 3 Deyu Zhou†1

1School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China 2Department of Informatics, King’s College London, UK 3The Alan Turing Institute, UK zhangcongzhi, jialongwu, d.zhou @seu.edu.cn {linhai.zhang, yulan.he}@kcl.ac.uk

# Abstract

Original ReverseTarget ReverseNonTarge AddDiff 80 llll

Despite the notable advancements of existing prompting methods, such as In-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they still face challenges related to various biases. Traditional debiasing methods primarily focus on the model training stage, including approaches based on data augmentation and reweighting, yet they struggle with the complex biases inherent in LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate LLMs biases. In specific, causal intervention is achieved by designing the prompts without accessing the parameters and logits of LLMs. The chain-of-thought generated by LLM is employed as the mediator variable and the causal effect between input prompts and output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to accurately represent the chain-of-thoughts and estimate the causal effects, contrastive learning is used to fine-tune the encoder of chain-of-thought by aligning its space with that of the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance across seven natural language processing datasets on both open-source and closed-source LLMs.

# 1 Introduction

Large Language Models (LLMs) have shown remarkable emergent abilities, including In-Context Learning (ICL) (Brown et al. 2020; Peng et al. 2024; Yang et al. 2024) and Chain-of-Thought (CoT) prompting (Wei et al. 2022; Wang et al. 2022), which allow LLMs to perform natural language tasks based on only a few instances without weight updating. These prompting methods have achieved significant results across many traditional natural language processing tasks, including sentiment analysis, natural language inference, and machine reading comprehension (Kojima et al. 2022; Zhou et al. 2022; Liu et al. 2023).

However, recent studies have shown that these advanced prompting methods are not robust enough (Ye et al. 2023) and can lead LLMs to produce hallucinatory results with incorrect or unfaithful intermediate reasoning steps (Lyu et al. 2023; Wang et al. 2023b; Bao et al. 2024; Turpin et al. 2024; Wu et al. 2024d,e,c; Qin, Fang, and Xue 2024).

Some studies (Mallen et al. 2023; Wang et al. 2023d) believe that this phenomenon is due to the a conflict between the internal knowledge bias of LLMs and the external knowledge. Therefore, an effective solution is to interact with an external knowledge base to validate and adjust the reasoning process of LLMs (Wang et al. 2023b; Zhang et al. 2023). Moreover, recent work debiases the chain-of-thoughts of LLMs by incorporating counterfactual knowledge and causal interventions (Wu et al. 2024a). However, these methods are specifically tailored for knowledge-intensive tasks. Bias problems are also observed in other NLP tasks. As shown in Figure 2, in aspects-based sentiment analysis, mathematical reasoning, and multi-hop question-answering tasks, LLMs sometimes overly depend on certain text spans in the prompts, leading to wrong reasoning and answers. Notably, the first

Aspect Based Sentiment Analysis (SemEval2014-Laptop) Prompt CoT output First, we need to identify the sentiment polarity of the sentence towards   
Please detect the sentiment polarity towards the target the target "adapter". The sentence mentions that the speaker got a new   
given the sentence. adapter, but there was no change. This could indicate a neutral sentiment towards the adapter. $\ l ^ { v }$   
Sentence: I got the new adapter and there was no However, the second part of the sentence mentions that the on/off switch   
change, but on/off switch is obscure. is obscure, which could indicate a negative sentiment towards the adapter.   
Target: adapter Therefore, the overall sentiment polarity towards the adapter in this sentence is negative. Correct Answer: neutral Math Reasoning (GSM8K) Prompt CoT output   
John plans to sell all his toys and use the money to John has 13 lego sets and sells them for \$15 each. So, he gets $1 3 \times 1 5 =$   
buy video games. He has 13 lego sets and he sells 195 dollars. He uses $8 \times 2 0 = 1 6 0$ dollars to buy video games and has 5   
them for \$15 each. He ends up buying 8 video games dollars left. So, he has $1 9 5 - 1 6 0 - 5 = 3 0$ dollars left. $\blacktriangledown$   
for \$20 each and has \$5 left. How many lego sets Since he sold all his lego sets, he doesn't have any lego sets left. The   
does he still have? answer is 0. Correct Answer: 2 Multi-hop Question Answering (HotpotQA) Prompt CoT output Step 1: Identify the film studio mentioned in the question. According to   
Context: the context, The question mentions "this film studio founded in $2 0 0 5 "$ ,   
Our Stories Films is an American film studio founded which refers to The Weinstein Company (TWC). $\ l ^ { v }$   
on July 13, 2006… Step 2: Find the information about the joint venture between RLJ   
IatnbdeTgahne aWseainjsotienitnvCenotmurpeabneyt…ween RLJ Companies COourmSptaonries aFinldmsT hweasWae ijnositnet ivneCntoumrepabnety.wAececnorRdLiJngCtomthpea ncioensteaxntd, TSthepW3:eiAnsstewinerCthoemqpuaensyt.i $\smile$ The question asks what year the joint   
studio, founded in New York City by Bob and Harvey   
Weinstein in 2005… venture between RLJ Companies and this film studio (TWC) was   
Question: faonudndOeudr SWtoer kensoFwil tmhsatwTahseaWjoei intsvteintuCreombeptawneyewnaRsLfJouCnodmepdain 2es005,   
What year was a joint venture between RLJ and The Weinstein Company. $\ l ^ { v }$   
Companies and this film studio founded in 2005, founded? We can conclude that the joint venture was also founded in 2005. The answer is 2005. Correct Answer: 2006

two tasks mentioned are not knowledge-intensive. We argue that LLMs fail to capture the true causal effect between questions and reasoning results and instead establish spurious correlations between certain text spans and answers.

In addition to the above qualitative analysis, our quantitative experiments also show that the current prompting methods are ineffective in addressing the bias issue. As shown in Figure 1, the performance of all prompting methods drops significantly when evaluated on the corresponding adversarial dataset compared to the original dataset, indicating that LLMs may suffer from bias in the pertaining corpus. Moreover, it has been demonstrated that LLMs exhibit label bias, recency bias, and entity bias from context (Zhao et al. 2021; Wang et al. 2023a; Fei et al. 2023).

Traditional debiasing methods mitigate the bias issue mainly during the model training stage, utilizing approaches such as data augmentation-based (Wei and Zou 2019; Lee et al. 2021) and reweighting (Schuster et al. 2019; Mahabadi, Belinkov, and Henderson 2019). Data augmentation-based methods face challenges due to the cost and complexity of annotating bias cases, particularly limited by context length. Reweight-based methods encounter difficulties in assigning weights to each sample in prompt-based learning scenarios. Recently, debias methods based on causal inference (Pearl et al. 2000; Pearl 2022) have become popular because of their strict theoretical guarantees and good generalization. Causal inference-based methods only need to calibrate model prediction results during the inference stage (Niu et al. 2021; Tian et al. 2022; Guo, Gong, and Lai 2022; Xu et al. 2023;

Chen et al. 2023), which makes them well-suited for promptbased learning scenarios. However, counterfactual inference requires accessing LLM output logits, while back-door adjustment requires specific confounding variable values.

To address the aforementioned challenge, we propose to debias prompting methods through causal intervention using front-door adjustment (Pearl, Glymour, and Jewell 2016). Front-door adjustment enables causal intervention without the need to access confounding variable values or LLM output logits. As shown in Figure 3(a), the causal relationship behind the prompting method is uncovered using a structural causal model. Here $X$ denotes the input prompt, comprising demonstrations and test examples.

$A$ denotes the predicted answer generated by the LLM. $U$ is the unobservable confounder that introduces various biases in the pertaining corpus.

The debiasing process involves measuring the causal effect between the treatment $X$ and the outcome $A$ . However, as $U$ absorbs complex biases of LLMs that are difficult to model or detect, back-door adjustment is not feasible for calculating the causal effect between $X$ and $A$ . To address this issue, as shown in Figure 3(b), we use the chain-of-thought generated by LLM as the mediator variable $R$ between $X$ and $A$ .

As Figure 2 illustrates, while LLMs initially reason correctly, biases often confuse the final step of answer derivation. To simplify, we ignore the edges between $U$ and $R$ , aligning our causal graph with the front-door criterion (Pearl, Glymour, and Jewell 2016). By this way, we can use the front-door adjustment to estimate the causal effect between

$X$ and $A$ without accessing $U$ .

Therefore, in this paper, we propose Causal Prompting, a novel prompting method for debiasing based on front-door adjustment. Unlike previous causal inference-based methods, causal intervention is implemented by modifying prompts without accessing the parameters and logits of LLMs. Specifically, to estimate the causal effect between $X$ and $R$ , we leverage self-consistency (SC) (Wang et al. 2022) of LLMs and a clustering algorithm to compute the probability of the chain-of-thought $R$ . To measure the causal effect between $R$ and $A$ , we use the normalized weighted geometric mean (NWGM) approximation (Xu et al. 2015) to select the optimal demonstration set, which can help the model to generate an unbiased answer. Overall, CoT, SC, and ICL are effectively combined through front-door adjustment to mitigate LLM biases in NLP tasks. Note that in the clustering and NWGM algorithms, an Encoder is needed to obtain the representations of chain-of-thoughts. Since Encoder and LLMs have different semantic understanding of the chain-of-thought, we use contrastive learning (Chen et al. 2020) to fine-tune the Encoder to align its representation space with LLMs to estimate causal effects more accurately.

The contributions of this work are summarized as follows:

• Our work aims to identify and analyze the bias problem in LLM prompting methods from the perspective of causal inference, adhering more closely to the principles of the field. Moreover, the front-door adjustment is proposed to theoretically address the bias problem in prompting. • Contrastive learning is proposed to fine-tune the Encoder of the chain-of-thoughts, aligning the space of the Encoder with LLMs to accurately capture representations of chain-of-thoughts and estimate causal effects. • The proposed approach achieves excellent performance across seven natural language processing datasets using both open-source and closed-source LLMs.

# 2 Preliminaries

# 2.1 Structural Causal Model and Causal Intervention

A Structural Causal Model (SCM) (Pearl, Glymour, and Jewell 2016) is used to describe the causal relationships between variables. In SCM, we typically use a directed acyclic graph $G = \{ V , E \}$ , where $V$ represents the set of variables and $E$ represents the set of direct causal relationships.

As shown in Figure 3(a), $X$ denotes the input prompt, including demonstrations and test examples. $A$ denotes the predicted answer generated by the LLMs. LLMs generate answers based on prompt, so we have $X  A$ , which means that $X$ is the direct cause of $A$ . LLMs might learn spurious correlations between text patterns and answers from pre-trained corpora or instruction-supervised fine-tuning datasets (Xing et al. 2020; Li et al. 2024; Bao et al. 2024), leading to bias in downstream tasks. Previous work argues that the reason for this bias is that LLMs tend to follow a certain latent concept (Xie et al. 2021) or an implicit reasoning results (Li et al. 2024) in the reasoning process, rather than following the explicitly generated chain-of-thought. This leads to the final answer does not necessarily follow from the generated chain-of-thought, specifically, there is no actual causal relationship between the chain-of-thought and the answer (Lyu et al. 2023; Bao et al. 2024). To accurately calculate the causal effect between $X$ and $A$ , we use the unobservable variable $U$ to describe this latent concept or implicit reasoning results, using the back-door path $X \left. U \right. A$ denotes that the causality of $X$ and $A$ is confounded by $U$ .

![](images/fd8854b0590d7abdb2df88bae1e1518aedb036de99c28958253cc79cd0dc2f36.jpg)  
Figure 3: Structural causal model for the prompting method. (a) The causality of prompt and answer is confounded by unobservable variable. (b) The chain-of-thought generated by LLMs as a mediator variable between prompt and answer.

In SCM, if we want to compute the true causal effect between two variables $X$ and $A$ , we should block every backdoor path between them (Pearl and Mackenzie 2018). For example, as shown in Figure 3(a), we should block $X $ $U  A$ to obtain the true causal effect between $X$ and $A$ . We typically use causal interventions for this purpose, which use the $d o$ operation to estimate the causal effect between $X$ and $A$ . In the causal graph satisfying Figure 3(a), the $d o -$ - operation can be computed by back-door adjustment (Pearl, Glymour, and Jewell 2016):

$$
P ( A | d o ( X ) ) = \sum _ { u } P ( A | X , u ) P ( u )
$$

# 2.2 Front-door Adjustment

Since confounding factor $U$ is inaccessible, back-door adjustment cannot be performed. Fortunately, the front-door adjustment (Pearl, Glymour, and Jewell 2016) does not require access to the values of the confounding factor $U$ to calculate the causal effect between $X$ and $A$ . As shown in Figure 3(b), we use the chain-of-thought generated by LLM as a mediator variable $R$ between $X$ and $A$ .

In practice, as depicted in Figure 2, LLM can perform correct reasoning at the beginning, but it is often easily confused by bias in the last step of deriving the answer. Consequently, we decided to start with the simple SCM and focus on the confounder between $X$ and $A$ . In order to simplify the causal graph, we ignore the confounder of $R$ with other variables, aligning our causal graph with the front-door criterion (Pearl, Glymour, and Jewell 2016). According to the front door adjustment, $P ( A | d o ( X ) )$ can be formulated as:

$$
P ( A | d o ( X ) ) = \sum _ { r } P ( A | d o ( r ) ) P ( r | d o ( X ) )
$$

where $r \in R$ is the chain-of-thought generated by LLMs in response to the prompt $X$ . The causal effect between

$X$ and $A$ is decomposed into two partially causal effects $P ( r | d o ( X ) )$ and $P ( A | d o ( r ) )$ .

Next, we discuss how to estimate these two components separately. The first component is $P ( r | d o ( X ) )$ , represents the probability distribution of the chain-of-thought $r$ given the intervention $d o ( X )$ . To compute $P ( r | d o ( X ) )$ , we need to block the backdoor path $X  U  A  R$ between $X$ and $R$ . Since there exists a collision structure $U \right. A \left. R$ , the backdoor path has been blocked (Pearl, Glymour, and Jewell 2016) and we can get:

$$
P ( r | d o ( X ) ) = P ( r | X )
$$

Now, we focus on the computation of the second component $P ( A | d o ( r ) )$ , represents the probability distribution of the answer $A$ given the intervention $d o ( r )$ . To compute $P ( A | d o ( r ) )$ , we need to block the backdoor path $R \gets X \gets$ $U  A$ between $R$ and $A$ . Since we do not have access to the details of $U$ , we implement back-door adjustments with the help of prompt $X$ :

$$
P ( A | d o ( r ) ) = \sum _ { x } P ( x ) P ( A | r , x )
$$

where $x \in X$ denotes the input prompt, including demonstrations and test examples.

Finally, substituting Equations (3) and (4) into Equation (2) after we obtain the estimation of $P ( r | d o ( X ) )$ and $P ( A | d o ( r ) )$ . Hence, the final $P ( A | d o ( X ) )$ can be represented as follows:

$$
\begin{array} { r l } { P ( A | d o ( X ) ) = \displaystyle \sum _ { r } P ( r | d o ( X ) ) P ( A | d o ( r ) ) } & { { } } \\ { = \underbrace { \displaystyle \sum _ { r } P ( r | X ) \sum _ { x } P ( x ) P ( A | r , x ) } _ { C o T - S C } } \end{array}
$$

where the first component $\begin{array} { r } { \sum _ { r } P ( r | d o ( X ) ) } \end{array}$ can be estimated by combining the CoT an d SC prompting methods, and the second component $P ( A | d o ( r ) )$ can be computed by selecting the demonstration examples in ICL prompting.

# 3 Method

As shown in Figure 4, Causal Prompting aims to estimate the causal effect between input $X$ and answer $A$ . The estimation is achieved using the front-door adjustment, which divides the causal pathway into two distinct parts: the causal effect between $X$ and chain-of-thought $r$ , and the causal effect between $r$ and $A$ .

First, the causal effect between $X$ and chain-of-thought $r$ , $P ( r | d o ( X ) )$ is estimated by combining the Chain-ofThought prompting with a Encoder-based clustering algorithm. Second, the causal effect between $r$ and $A$ , $P ( A | d o ( r ) )$ is estimated by combining the In-Context Learning prompting with the normalized weighted geometric mean (NWGM) approximation algorithm. The final answer is aggregated by performing a weighted voting algorithm. Moreover, contrastive learning(Chen et al. 2020; Gao et al. 2022; Zhang, Zhang, and Zhou 2023) is employed to align the representation space of the Encoder and the LLMs for more precise estimation.

We will first introduce the estimation of $P ( r | d o ( X ) )$ and $P ( A | d o ( r ) )$ , respectively, then combine them to derive $P ( A | d o ( X ) )$ . Finally, we will discuss how we align the representation space between the Encoder and the LLM.

# 3.1 Estimation of $P ( r | d o ( X ) )$

We firstly undertake the estimation of $P ( r | d o ( X ) )$ . $P ( r | d o ( \dot { X } ) )$ measures the causal effect between input $X$ and chain-of-thought $r$ . As shown in Equation (3), the estimation of $P ( r | d o ( X ) )$ is equivalent to the estimation of $P ( r | X )$ . However, $P ( r | X )$ is still intractable for LLMs. On the one hand, the output probability is often inaccessible for most closed-source LLMs; on the other hand, the chain-ofthoughts $r$ are challenging to enumerate comprehensively. Therefore, to estimate the causal effect $P ( r | d o ( { \bar { X } } ) )$ for both open-source and closed-source LLMs, we employ the CoT prompting and integrate it with a clustering algorithm. To be more specific, we initially prompt the LLMs to generate multiple CoTs based on the input. Subsequently, the CoTs are projected into embeddings. The embeddings are then clustered to form distinct groups based on their similarity. Finally, the centroid of each cluster is selected as the optimal and representative chain-of-thought. The probability associated with each representative chain-of-thought is then estimated based on the size of its respective cluster.

To enhance the quality of generated CoTs, $n$ in-context demonstrations $d$ are selected from training set based on question similarity. These demonstrations are then concatenated with the test question $q ^ { t e s t }$ to form the final prompt. Thus, the final prompt $\mathcal { P }$ is structured as follows:

$$
\mathcal { P } = [ d _ { 1 } , . . . , d _ { n } , q ^ { t e s t } ]
$$

where each (qdemo, rdemo) contain the demonstration question $q _ { i } ^ { d e m o }$ and its corresponding demonstration chain-ofthought $r _ { i } ^ { d e m o }$ . Where $i \in \{ 1 , . . . , n \}$ , $n$ denotes the number of demonstration examples in few-shot prompt method. In the practical implementation, we use prompt $\mathcal { P }$ , which is fed into the LLMs to represent $X$ in the structural causal model.

Based on the input prompt $\mathcal { P }$ , LLMs are prompted to generate $m$ distinct CoTs $c$ by increasing the temperature parameter of LLMs. This adjustment encourages more diverse outputs, where the same procedure is also employed in selfconsistency prompting of LLMs (Wang et al. 2023c). In this way, we can obtain the set of chain-of-thoughts as follows:

$$
\{ c _ { i } | i = 1 , . . . , m \} = \mathrm { L L M } ( \mathcal { P } )
$$

To perform the distance-based clustering method, the generated CoT $c _ { i }$ are further fed into a Encoder to get the text embedding $\overline { { c } } _ { i }$ . Following the previous work (Devlin et al. 2018), the input is concatenated with the special tokens [CLS] and [SEP], and the embedding of the [CLS] token is taken as the embedding of CoT $c _ { i }$ .

$$
\overline { { c } } _ { i } = \mathrm { E n c o d e r } ( [ \mathrm { C L S } ] , c _ { i } , [ \mathrm { S E P } ] )
$$

Then K-means clustering algorithm (Har-Peled and Kushal 2005; Wu et al. 2023) is applied to the embeddings to get $K$ clusters $C$ as follows:

$$
\{ C _ { 1 } , . . . , C _ { K } \} = \mathrm { K } { \cdot } \mathrm { m e a n s } ( \overline { { c } } _ { 1 } , . . . , \overline { { c } } _ { m } )
$$

where $C _ { k }$ refers to the $k$ -th cluster of the clustering result, $K$ denotes the number of clusters.

![](images/6644f24b6db02c3d22270647a4b4bd94c16d82c735f204bafff4a317f8932d27.jpg)  
Figure 4: The overall framework of Causal Prompting. Firstly, based on the input prompt $X$ consisting of the demonstration examples $\sqsubseteq$ and a question $\sqsubseteq$ of the test example, we query the LLM to generate $m$ distinct CoTs $\bigcirc$ . Then, these CoTs are clustered into $K$ clusters by an Encoder-based clustering algorithm. Subsequently, $K$ representative CoTs $\bigcirc$ are selected by searching the closest $\mathbf { C o T }$ to the cluster center. Secondly, the optimal demonstration examples $\sqsubseteq$ are retrieved for each representative $\operatorname { C o T } \bigcirc$ through the Encoder-based intervention algorithm, and then the input prompt $\mathcal { P } _ { r _ { k } } ^ { i t e r }$ after the intervention is obtained. Finally, we query the LLM $T$ times, obtaining $T$ improved CoTs $\bigcirc$ and $T$ answers $\bigcirc$ for each representative CoT $\bigcirc$ . The final answer $\bullet$ is obtained by performing a weighted voting.

Based on the clusters, $K$ representative chain-of-thoughts $r$ are selected by searching the closest chain-of-thought to the cluster center.

$$
r _ { k } = \mathrm { C e n t e r } ( C _ { k } ) , k = 1 , . . . , K
$$

The causal effect between input $X$ and chain-of-thought $r _ { k }$ is estimated based on the cluster size as follows:

$$
P ( r _ { k } | d o ( X ) ) \approx \frac { | C _ { k } | } { m }
$$

where $\left| C _ { k } \right|$ denotes the size of cluster $C _ { k }$ .

# 3.2 Estimation of $P ( A | d o ( r ) )$

Based on the $K$ chain-of-thoughts selected by Equation (10) in Section 3.1, we estimate $P ( A | d o ( r _ { k } ) )$ for each chain-of-thought $r _ { k }$ . For convenience, we omit the subscript $k$ and use $P ( A | d o ( \bar { r } ) )$ to denote $P ( A | d o ( r _ { k } ) )$ in the following. $P ( A | d o ( r ) )$ measures the causal effect between the chain-of-thought $r$ and the answer $A$ . Based on the discussion in Equation (4), $P ( A | d o ( r ) )$ can be calculated with backdoor adjustment as follows:

$$
P ( A | d o ( r ) ) = \sum _ { x \in X } P ( x ) P ( A | r , x ) = \mathbb { E } _ { x \in X } { \bigl [ } P ( A | r , x ) { \bigr ] }
$$

where $P ( A | r , x )$ denotes the probability of the final answer $A$ generated by LLM based on the given prompt $x$ and the chain-of-thought $r$ .

However, the value space of $X$ is inexhaustible in most of the cases, and previous work employs the normalized weighted geometric mean (NWGM) approximation (Xu et al. 2015; Tian et al. 2022; Chen et al. 2023) to tackle this problem, where a confounder embedding x′ is estimated to approximate the expectation of variable

$X$ .

$$
\mathbb { E } _ { x \in X } [ P ( A | r , x ) ] \approx P ( A | r , \mathbb { E } _ { x \in X } [ x ] ) \approx P ( A | c o n c a t ( r , \overline { { x } } ^ { ' } ) )
$$

where $c o n c a t ( \cdot , \cdot )$ denotes vector concatenation, $\overline { { x } } ^ { \prime }$ denotes the confounder embedding of $X$ .

Inspired by the previous works $\mathrm { \Delta X u }$ et al. 2015; Tian et al. 2022; Chen et al. 2023; Zhang, Zhang, and Zhou 2024), we propose a prompting version of NWGM approximation to perform the backdoor adjustment for LLMs prompting by combining a Encoderbased intervention and In-Context Learning (ICL) prompting. The original idea of NWGM is to augment the representation of the chain-of-thought $r$ with an embedding $\overline { { x } } ^ { \prime }$ that contains all sample information as much as possible. However, at the prompting level, we cannot include all samples in context due to the limited context length, so we use only those samples that are most useful for improving the current chain-of-thought $r$ .

Specifically, we use the Encoder to obtain the embedding $\overline { { r } } _ { k }$ of the $k$ -th chain-of-thought $r _ { k }$ . Subsequently, ICL demonstrations are selected by searching the entire training set based on the chainof-thought embedding $\overline { { r } } _ { k }$ to approximate the effect of taking expectations on input $X$ . Finally, we rank the ICL demonstrations according to their similarity weights to indicate the importance of different samples.

Note that, as shown in Equation (6), the input prompt $\mathcal { P }$ includes demonstrations $d$ and test question $q ^ { t e s t }$ . Directly modifying the certain text span of test examples will change the semantics of question $q ^ { t e s t }$ . Therefore, we only modify the demonstrations $d$ and implement the NWGM approximation by In-Context Learning. In fact, the goal of our prompting version of the NWGM algorithm is to enable the LLMs to learn from the demonstrations how to improve the chain-of-thought $r$ of the test example. We introduce both wrong and correct chain-of-thoughts of demonstrations.

Given a training set $\mathcal { D } = \{ d _ { j } = ( q _ { j } ^ { \smile } , r _ { j } ^ { w r o n g } , r _ { j } ^ { c o r r e c t } ) \} _ { j = 1 } ^ { N }$ , and a chain-of-thought $r _ { k }$ of test example, where $q _ { j }$ denotes the question of $j$ -th training sample, $r _ { j } ^ { w r o n g }$ and $r _ { j } ^ { c o r r e c t }$ denote the wrong and correct chain-of-thoughts of demonstration $d _ { j }$ , $N$ denotes the size of the training set, $r _ { k }$ refers to the $k$ -th chain-of-thought selected by Equation (10) in Section 3.1. The embedding $\overline { { r } } _ { k }$ of chain-of-thought $r _ { k }$ and the embedding $\overline { { d } } _ { j }$ of demonstration $d _ { j }$ are obtained by the following:

$$
\begin{array} { r l } & { \overline { { \boldsymbol { r } } } _ { k } = \mathrm { E n c o d e r } ( [ \mathrm { C L S } ] , \boldsymbol { r } _ { k } , [ \mathrm { S E P } ] ) } \\ & { \overline { { \boldsymbol { d } } } _ { j } = \mathrm { E n c o d e r } ( [ \mathrm { C L S } ] , \boldsymbol { r } _ { j } ^ { w r o n g } , [ \mathrm { S E P } ] ) } \end{array}
$$

Previous works (Margatina et al. 2023; Liu et al. 2022) have shown that using demonstration examples that are semantically similar to the test examples allows better performance for In-Context Learning. Therefore, the back-door intervention is approximated by searching the most similar instance based on chain-of-thought embedding $\overline { { r } } _ { k }$ . Specifically, we sort the training set $\mathcal { D }$ from largest to smallest according to the cosine similarity between $\overline { { r } } _ { k }$ and $\overline { { d } } _ { j }$ .

$$
\{ d _ { j } ^ { \uparrow } \} _ { j = 1 } ^ { N } = S o r t ( \mathcal { D } , \overline { { r } } _ { k } , \{ \overline { { d } } _ { j } \} _ { j = 1 } ^ { N } )
$$

where $d _ { j } ^ { \uparrow }$ denotes the sorted demonstration example, $S o r t$ means that, given a predefined cosine similarity function $c o s$ , the samples are ordered so that $c o s ( \overline { { r } } _ { k } , \overline { { d } } _ { i } ) \geq c o s ( \overline { { r } } _ { k } , \overline { { d } } _ { j } )$ when $i < j$ .

Then the $l$ most similar demonstration examples are selected to concatenate into prompt, where $l \ll N$ . Note that, unlike the KATE (Liu et al. 2021) method, we put the most similar demonstration samples closer to the test samples because this order is more beneficial for our NWGM algorithm to learn information for improving the chain-of-thoughts from the demonstration based on practical experiments. For each chain-of-thought $r _ { k }$ of a test sample, the final input prompt after intervention is given as follows:

$$
\mathcal { P } _ { r _ { k } } ^ { i t e r } = [ d _ { l } ^ { \uparrow } , . . . , d _ { 1 } ^ { \uparrow } , q ^ { t e s t } ]
$$

Subsequently, we query the LLMs $T$ times, obtaining $T$ answers and $T$ improved chain-of-thoughts using the prompt $\mathcal { P } _ { r _ { k } } ^ { i t e r }$ and chain-of-thought $r _ { k }$ .

$$
\{ ( r _ { k , t } ^ { i p } , a _ { k , t } ) | t = 1 , . . . , T \} = \mathrm { L L M } ( \mathcal { P } _ { r _ { k } } ^ { i t e r } , r _ { k } )
$$

where $r _ { k , t } ^ { i p }$ denotes the $t$ -th improved chain-of-thought for chain-ofthought $r _ { k }$ .

We then use majority voting to estimate the probability of the answer as follows:

$$
P ( A | d o ( r _ { k } ) ) \approx \frac { \sum _ { t = 1 } ^ { T } \mathbb { I } ( A = a _ { k , t } ) } { T }
$$

# 3.3 Estimation of $P ( A | d o ( X ) )$

Based on the results of Equation (11) in Section 3.1 and Equation (18) in Section 3.2, the final answer is obtained by performing a weighted voting as follows:

$$
\begin{array} { l } { { \displaystyle P ( { \cal A } | d o ( { \cal X } ) ) = \sum _ { r _ { k } } { P ( r _ { k } | d o ( { \cal X } ) ) P ( { \cal A } | d o ( r _ { k } ) ) } } } \\ { { \displaystyle ~ = \sum _ { k = 1 } ^ { K } \frac { | C _ { k } | } { m } \cdot \frac { \sum _ { t = 1 } ^ { T } \mathbb { I } ( { \cal A } = a _ { k , t } ) } { T } } } \end{array}
$$

Finally, we chose the answer with the largest weight as the final answer. In this way, with the front-door adjustment, we calibrate the probability distribution $P ( A | X )$ obtained by the CoT-SC method to $P ( A | d o ( X ) )$ obtained by the Causal Prompting method.

# 3.4 Representation Space Alignment

In the clustering discussed in Section 3.1 and NWGM algorithm presented in Section 3.2, an Encoder is needed to derive the representations of chain-of-thoughts. However, the semantic representation of Encoder and LLM differ significantly. Two chain-of-thoughts that $L L M$ considers similar may not be close in the representation space of the Encoder. Through experiments we found that the chain-of-thoughts generated by LLM are not distinctly separable in the representation space of the vanilla Encoder.

To align the representation spaces of the Encoder and the LLMs, we take each chain-of-thought $r$ in the training dataset $\mathcal { D }$ as an anchor, use LLM to generate the corresponding positive samples, use the other samples within the batch as negative samples, and then use contrastive learning to fine-tune the Encoder.

For chain-of-thought $r$ , we prompt the LLM to generate a similar sentence $r ^ { + }$ as the positive sample. Following previous works (Gao et al. 2022; Zhang, Zhang, and Zhou 2023), we use the InfoNCE loss (Chen et al. 2020) to fine-tune the Encoder :

$$
\sum _ { \overline { { \boldsymbol { r } } } _ { p } \in P o s ( r ) } - l o g \frac { g ( \overline { { \boldsymbol { r } } } , \overline { { \boldsymbol { r } } } _ { p } ) } { g ( \overline { { \boldsymbol { r } } } , \overline { { \boldsymbol { r } } } _ { p } ) + \sum _ { j \in N e g ( r ) } g ( \overline { { \boldsymbol { r } } } , \overline { { \boldsymbol { r } } } _ { j } ) }
$$

where the $\overline { { r } }$ and $\overline { { r } } _ { p }$ are the representations of $r$ and its positive samples. $P o s ( r )$ and $N e g ( r )$ refer to the positive set and the negative set for the chain-of-thought $r$ . $\bar { P o s ( r ) } = \{ \bar { r } _ { p 1 } , \bar { r } _ { p 2 } \}$ , where $\overline { { r } } _ { p 1 }$ is augmented representation of the same chain-of-thought $r$ , obtained with different dropout masks, and $\overline { { r } } _ { p 2 }$ is the representation of positive sample $r ^ { + }$ . $j \in N e g ( r )$ is the index of in-batch negative samples. $g$ is a function: $g ( \overline { { r } } , \overline { { r } } _ { p } ) = e x p ( \overline { { r } } ^ { T } \overline { { r } } _ { p } / t e m p )$ , where temp is a positive value of temperature in the contrastive learning.

# 4 Experiments

# 4.1 Datasets

We evaluate the effectiveness of our approach on three tasks: Math Reasoning (GSM8K (Cobbe et al. 2021), MATH (Hendrycks et al. 2021)), Multi-hop Question Answering (HotpotQA (Yang et al. 2018), MuSiQue (Trivedi et al. 2022)), and Natural Language Understanding (Aspect-based Sentiment Analysis (ABSA) (Pontiki et al. 2016), Natural Language Inference (NLI) (Williams, Nangia, and Bowman 2017), and Fact Verification (FV) (Thorne et al. 2018)). For the NLU tasks, we use the original datasets (in-distribution, ID) and the corresponding adversarial datasets (out-of-distribution, OOD) (Wang et al. 2021) to verify the robustness of our method.

# 4.2 Baselines

We compare our approach with three other few-shot prompting approaches to evaluate its effectiveness: Standard ICL (Brown et al. 2020), CoT (Wei et al. 2022) and CoT-SC (Wang et al. 2022).

# 4.3 Main Results

Table 1 shows the comparison results between causal prompting and the aforementioned baselines. Expectedly, the performance of Standard ICL, CoT, and CoT-SC improves progressively, as each subsequent method is an enhanced version of its predecessor. It not only confirms the effectiveness of integrating CoT into ICL, consistent with (Brown et al. 2020; Wei et al. 2022; Zhou et al. 2022), but also validates the efficacy of employing multiple sampling and voting strategies (Wang et al. 2022). Causal Prompting consistently delivers the best results across all metrics and datasets. It indicates that our prompting method can comprehensively improve the ability of LLM in all three tasks. Specifically, our method exhibits a more pronounced improvement in Math Reasoning and Multi-hop Question Answering tasks, with an average performance enhancement of approximately $5 \% . 1 0 \%$ . This substantial increase underscores our method’s greater efficacy in tackling more challenging problems.

<html><body><table><tr><td></td><td>GSM8K</td><td>MATH</td><td colspan="2">HotpotQA</td><td colspan="2">MuSiQue</td><td>ABSA</td><td>NLI</td><td>FV</td></tr><tr><td>Method</td><td>Acc</td><td>Acc</td><td>EM</td><td>F1</td><td>EM</td><td>F1</td><td>Acc</td><td>Acc</td><td>Acc</td></tr><tr><td colspan="10">LLaMA2</td></tr><tr><td>Standard ICL</td><td>6.14</td><td>3.71</td><td>41.20</td><td>59.56</td><td>26.09</td><td>41.16</td><td>47.26</td><td>28.20</td><td>56.87</td></tr><tr><td>CoT</td><td>27.07</td><td>4.72</td><td>44.70</td><td>64.84</td><td>18.71</td><td>30.27</td><td>49.12</td><td>27.56</td><td>70.07</td></tr><tr><td>CoT-SC</td><td>31.92</td><td>6.32</td><td>49.30</td><td>68.53</td><td>31.16</td><td>46.36</td><td>53.70</td><td>33.57</td><td>72.20</td></tr><tr><td>Causal Prompting</td><td>36.47</td><td>8.76</td><td>52.20</td><td>70.88</td><td>34.68</td><td>48.79</td><td>67.55</td><td>50.83</td><td>81.07</td></tr><tr><td colspan="10">LLaMA3</td></tr><tr><td>Standard ICL</td><td>18.65</td><td>14.24</td><td>37.20</td><td>62.17</td><td>17.42</td><td>24.22</td><td>72.14</td><td>63.75</td><td>80.67</td></tr><tr><td>CoT</td><td>74.07</td><td>40.35</td><td>48.90</td><td>72.75</td><td>38.88</td><td>54.38</td><td>71.55</td><td>64.19</td><td>81.80</td></tr><tr><td>CoT-SC</td><td>82.41</td><td>56.61</td><td>52.70</td><td>75.43</td><td>41.37</td><td>59.78</td><td>75.92</td><td>65.15</td><td>83.87</td></tr><tr><td>Causal Prompting</td><td>87.95</td><td>62.76</td><td>58.50</td><td>78.18</td><td>48.07</td><td>64.23</td><td>79.06</td><td>67.97</td><td>86.67</td></tr><tr><td colspan="10">GPT-3.5</td></tr><tr><td>Standard ICL</td><td>33.74</td><td>23.08</td><td>2.10</td><td>3.68</td><td>28.84</td><td>39.27</td><td>69.26</td><td>53.52</td><td>75.33</td></tr><tr><td>CoT</td><td>71.87</td><td>53.50</td><td>11.70</td><td>16.49</td><td>41.37</td><td>57.82</td><td>65.74</td><td>63.55</td><td>80.67</td></tr><tr><td>CoT-SC</td><td>80.21</td><td>58.38</td><td>41.60</td><td>56.82</td><td>46.27</td><td>60.83</td><td>74.59</td><td>66.88</td><td>82.73</td></tr><tr><td>Causal Prompting</td><td>85.44</td><td>70.18</td><td>58.20</td><td>78.10</td><td>50.13</td><td>65.40</td><td>80.13</td><td>71.93</td><td>86.53</td></tr></table></body></html>

Table 1: The comparison results of Causal Prompting against baselines across different backbone LLMs, including LLaMA2, LLaMA3 and GPT-3.5, on seven datasets. The best results are in bold.

Table 2: The results of the robustness study on LLaMA3. $\mathrm { O r i }$ denotes the original dataset (ID) and Adv denotes the adversarial dataset (OOD). The best results are in bold.   

<html><body><table><tr><td></td><td colspan="2">ABSA</td><td colspan="2">NLI</td><td colspan="2">FV</td></tr><tr><td>Methods</td><td>Ori</td><td>Adv</td><td>Ori</td><td>Adv</td><td>Ori</td><td>Adv</td></tr><tr><td>ICL</td><td>75.71</td><td>70.30</td><td>76.30</td><td>50.27</td><td>90.00</td><td>76.00</td></tr><tr><td>CoT</td><td>77.27</td><td>68.60</td><td>74.81</td><td>54.77</td><td>91.40</td><td>77.00</td></tr><tr><td>CoT-SC</td><td>80.56</td><td>73.53</td><td>76.17</td><td>57.16</td><td>93.40</td><td>79.10</td></tr><tr><td>Ours</td><td>79.78</td><td>78.69</td><td>76.67</td><td>58.62</td><td>95.40</td><td>82.30</td></tr></table></body></html>

# 4.4 Robustness Study

Recent causal-based works (Tian et al. 2022; Zhang, Zhang, and Zhou 2024; Zhu et al. 2023; Wang et al. 2023a; Xu et al. 2023; Niu et al. 2021; Schuster et al. 2019; Wu et al. 2024b) have shown that using symmetric and adversarial (out-of-distribution) datasets can evaluate the debiasing ability of models. Following their practice, we evaluate Causal Prompting on both original data and adversarial data of the NLU tasks, respectively. Tables 2 show the performance comparison results of our method and baselines on LLaMA3 model. Although the performance of Causal Prompting decreases on Ori of ABSA, the improvement is larger on Adv data, resulting in the highest overall performance, see in Table 1. This phenomenon aligns with findings reported in previous work on causal inference (Tian et al. 2022; Wang et al. 2023a). It can be observed that the Adv of Causal Prompting is the highest on all datasets. This shows that our method generalizes well for both synthetic adversarial data in ABSA and NLI generated by TextFlint (Wang et al. 2021) and human-annotated real adversarial data in FV. This further validates the robustness of our model in handling datasets with significant bias.

# 5 Conclusion

We introduced Causal Prompting, a novel method for debiasing LLMs in NLP tasks by utilizing front-door adjustment in this work. The CoT generated by LLMs is employed as a mediator variable in the causal graph. Specifically, the causal effect between input prompt and output answer is decomposed into two distinct components, the causal effect from the input prompt to CoTs and from CoTs to the answer. The former component is estimated by combining the CoT prompting with a Encoder-based clustering algorithm. The latter component is estimated by combining the ICL prompting with the NWGM approximation algorithm. Moreover, Contrastive learning is used to fine-tune the Encoder so that the representation space of the Encoder is aligned with the LLM to estimate the causal effect more accurately. Our experimental results demonstrate that Causal Prompting significantly improves performance across seven NLP tasks on both open-source and closed-source LLMs. This approach, which both enhances performance and yields debiased responses, aligns with the trend of obtaining optimal results at test time. It can be extended to a broader range of scenarios, such as safety or alignment, under theoretical guidance.