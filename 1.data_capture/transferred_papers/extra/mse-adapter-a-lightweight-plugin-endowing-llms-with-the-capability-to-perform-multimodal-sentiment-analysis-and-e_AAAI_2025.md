# MSE-Adapter: A Lightweight Plugin Endowing LLMs with the Capability to Perform Multimodal Sentiment Analysis and Emotion Recognition

Yang Yang, Xunde Dong\*, Yupeng Qiang

South China University of Technology, School of Automation Science and Engineering China, Guangzhou audxd $@$ scut.edu.cn

# Abstract

Current Multimodal Sentiment Analysis (MSA) and Emotion Recognition in Conversations (ERC) methods based on pre-trained language models exhibit two primary limitations: 1) Once trained for MSA and ERC tasks, these pre-trained language models lose their original generalized capabilities. 2) They demand considerable computational resources. As the size of pre-trained language models continues to grow, training larger multimodal sentiment analysis models using previous approaches could result in unnecessary computational cost. In response to this challenge, we propose Multimodal Sentiment Analysis and Emotion Recognition Adapter (MSE-Adapter), a lightweight and adaptable plugin. This plugin enables a large language model (LLM) to carry out MSA or ERC tasks with minimal computational overhead (only introduces approximately 2.6M to 2.8M trainable parameters upon the 6/7B models), while preserving the intrinsic capabilities of the LLM. In the MSE-Adapter, the Text-Guide-Mixer (TGM) module is introduced to establish explicit connections between non-textual and textual modalities through the Hadamard product. This allows non-textual modalities to better align with textual modalities at the feature level, promoting the generation of higher-quality pseudo tokens. Extensive experiments were conducted on four public English and Chinese datasets using consumer-grade GPUs and open-source LLMs (Qwen-1.8B, ChatGLM3-6B-base, and LLaMA2-7B) as the backbone. The results demonstrate the effectiveness of the proposed plugin.

Code — https://github.com/AZYoung233/MSE-Adapter

# Introduction

Making artificial intelligence (AI) comprehend human sentiment is a significant issue in the development of AI. Multimodal Sentiment Analysis (MSA) and Emotion Recognition in Conversations (ERC) have received widespread attention from the Natural Language Processing (NLP) community in recent years (Mai, Zeng, and Hu 2023; Wang et al. 2024). Limited by various factors, early sentiment analysis could only be conducted through text, which is far from sufficient for the diversified information of the real world. The same sentence, accompanied by different facial expressions or tones, can convey entirely different emotions. Therefore, utilizing multimodal information to perceive human emotions can enable AI to make more accurate judgments.

In recent years, the emergence of pre-trained Large Language Model (LLM) has introduced some new paradigms to the research community in NLP. Thanks to the powerful capabilities of LLM, they have demonstrated outstanding performance on many downstream tasks. Consequently, numerous researchers have begun to develop domain-specific LLM (such as for healthcare, weather forecasting, etc.) (Toma et al. 2023; Bi et al. 2023), by fine-tuning LLM with highquality data to exhibit strong capabilities in relevant fields. This paradigm of LLM-for-specific-field is an excellent approach, but it demands high quality and quantity of data, making its implementation somewhat challenging.

Some researchers in the field of sentiment analysis also leveraged this paradigm to tailor models specifically for MSA and ERC tasks. Hu et al. (2022b) introduced UniMSE, which for the first time unifies MSA and ERC as generative tasks. UniMSE assigns emotional category labels to samples in MSA tasks and sentiment intensity labels to samples in ERC tasks by calculating textual similarity. Subsequently, a model is trained based on the T5 (Raffel et al., 2020) architecture, exhibiting remarkable proficiency in both MSA and ERC tasks. Li et al. (2023b) developed UniSA, a comprehensive framework for sentiment analysis. Built upon the BART (Lewis et al. 2019) model and a series of pre-training tasks, UniSA excels in a wide range of multimodal and text-only sentiment analysis tasks, offering a more comprehensive performance compared to UniMSE. Although the two aforementioned works exhibit remarkable performance, they still exhibit certain limitations. 1) Significant computational overhead. Large number of trainable parameters and multi-task training lead to considerable computational expenses, where UniSA’s pre-training requires three days on eight NVIDIA RTX V100 32G GPUs. 2) Losing the generalization capability inherent to the base models. UniMSE and UniSA, trained respectively on T5 (Raffel et al. 2020) and BART (Lewis et al. 2019), are tailored for the sentiment analysis domain, restricting these models from performing tasks outside of sentiment analysis.

Currently, there is a trend towards developing plugins that freeze the backbone LLM and train them to perform non-textual tasks while retaining their inherent capabilities (Tsimpoukelli et al. 2021; Alayrac et al. 2022; Chen et al. 2023; Sun et al. 2023a; Szot et al. 2023) Given the expectation of continued growth in the parameter size of LLM, we propose developing a plugin with reduced training overhead. This will enable LLM to maintain its intrinsic capabilities while effectively executing a range of multimodal sentiment analysis tasks with minimal computational resources. We believe this approach is more promising than training a specialized sentiment analysis LLM. Our contributions are summarized as follows:

• We proposed Multimodal Sentiment Analysis and Emotion Recognition Adapter (MSE-Adapter), a plugand-play lightweight plugin. This plugin is trained with minimal overhead without altering the inherent capabilities of the LLM, enabling it to perform MSA or ERC task via autoregressive generated sentiment labels.   
• In the MSE-Adapter, we introduce the Text-GuideMixer (TGM) and Multi-Scale Fusion (MSF) modules. The TGM establishes explicit connections between nontextual and textual modalities. This facilitates featurelevel alignment between them and encourages the LLM to better understand content from non-textual modalities. The MSF module enables early fusion of non-textual features across multiple scales, further augmenting the LLM’s efficiency in integrating information from diverse modalities.   
• Extensive experiments were conducted on four publicly available Chinese and English datasets using consumer-grade GPUs with open-source LLMs (Qwen1.8B, ChatGLM3-6B-base, and LLaMA2-7B) as the backbone. The results demonstrate the effectiveness of the MSE-Adapter.

# Related Work

# Pre-trained language model for MSA and ERC

Currently, in the MSA and ERC communities, a number of outstanding works have emerged, including contrastive learning-based methods (Yang et al. $2 0 2 3 \mathrm { a }$ ; Mai, Zeng, and Hu 2023), graph-based methods (Li et al. 2023a; Hu et al. 2021; Lin et al. 2022), transformer-based methods (Sun et al. 2023b; Zhang et al. 2023; Yang et al. 2023b), and methods based on Pre-trained Language Model (PLM). PLM-based methods typically use a designated PLM, such as BERT (Devlin et al. 2018) or T5 (Raffel et al. 2020), as their foundation. These methods convert non-textual modality features into tokens with equivalent dimensions to those of textual modalities, enabling training within the PLM framework.

Rahman et al. (2020) proposed the MAG, where completed word embeddings are fused with non-textual modality features to generate new embeddings. These embeddings are then fine-tuned within PLMs (such as BERT and XLNet (Yang et al. 2019)) to achieve notable performance improvements. Similarly, Guo et al. (2022) proposed CHFN, which uses its designed Multimodal Interaction layer to integrate non-textual modal information into textual embedding at the word level, and then fine-tunes the integrated multimodal information by feeding it into BERT. Additionally,

Hasan et al. (2023) presented TextMI, a method that converts audio and vision information into corresponding textual descriptions. This approach links these descriptions with the textual content, transforming multimodal information into purely textual information. By inputting this enhanced text into BERT, TextMI achieves competitive performance results. Hu et al. (2022b) introduced UniMSE, an approach that uses the T5 (Raffel et al. 2020) model as its foundation. UniMSE encodes text using the initial layers of T5’s encoder and then trains the remaining layers using a combination of non-textual and textual features. This training strategy incorporates contrastive learning to enhance the model’s representation learning capabilities. Benefiting from its multitask training paradigm, UniMSE shows capabilities in both MSA and ERC tasks, demonstrating exceptional performance. Li et al. (2023b) developed UniSA, a comprehensive framework for sentiment analysis. UniSA uses PLMs (GPT2- medium (Radford et al. 2019), T5 and BART) as a foundation, standardizing the data formats of various types of sentiment analysis sub-tasks for input into PLMs. It leverages pre-training tasks and contrastive learning to pre-train the PLM, followed by fine-tuning on downstream task datasets. The UniSABART achieve comprehensive results across multiple sentiment analysis sub-tasks.

Compared to aforementioned works, our proposed approach, MSE-Adapter, is a lightweight plugin that requires fewer training parameters (approximately $2 . 6 \mathbf { M }$ to $2 . 8 \mathbf { M }$ trainable parameters for base models sized 6/7B). Notably, MSE-Adapter preserves the inherent generalization capability of the LLM without sacrificing efficiency. Therefore, when assigned MSA or ERC tasks, the user can invoke the relevant pre-trained MSE-Adapter plugin to carry out the designated task. This design enhances parameter efficiency while preserving the effectiveness and adaptability of LLM, offering a new and efficient solution for using LLM in MSA and ERC tasks.

# Adapters enabling LLM to perform non-plain text tasks

Adapters were usually utilized for efficiently fine-tuning large pre-trained models (Houlsby et al. 2019; Pfeiffer et al. 2020; He et al. 2022; Hu et al. 2023). By freezing the main body of the pre-trained model and only training the Adapter, the essence is to use gradient backpropagation to let the Adapter generate pseudo tokens that can be recognized by the pre-trained model. This process is aimed at prompting the pre-trained models to further adapt to certain downstream tasks. Meanwhile, since the pre-trained model is frozen during the training phase, it retains its strong generalization capability, avoiding the issue of catastrophic forgetting (Liu et al. 2021, 2023). Inspired by these relevant works of Adapter, some researchers have argued that it is possible to convert information from non-textual modalities into information understandable by LLMs through an Adapter, enabling them to perform downstream tasks involving non-plain text modalities. Tsimpoukelli et al. (2021) introduced Frozen, which utilizes a vision encoder to convert images into a series of tokens. These tokens are concatenated with a prompt and used to train a LLM for visual question answering (VQA) and captioning tasks, with gradient backpropagation guiding updates to the parameters of the vision encoder. Similarly, Alayrac et al. (2022) proposed Flamingo, which incorporates trainable cross-attention layers into a frozen LLM to fuse textual and vision modalities after embedding vision modality information using a pre-trained vision encoder. Flamingo exhibits remarkable performance across various video/visual-related tasks following training. Chen et al. (2023) presented X-LLM, a model that leverages X2L interfaces to convert vision, image, and audio modalities into “foreign languages” that can be processed by the LLM. X-LLM demonstrates impressive performance after instruction-tuning on a high-quality multimodal instruction dataset. Sun et al. (2023a) developed TEST, which utilizes contrastive learning to train an encoder for time series (TS) data, applies similarity constraints to align it with text, and fine-tunes the LLM with a soft-prompt approach to effectively process TS-related tasks.

![](images/8d15c1875b4280b2ebb3630476879d79d28ce4252646664a55cd2846099587d0.jpg)  
Figure 1: The comprehensive framework integrating MSE-Adapter with LLM.

In this paper, we introduce a lightweight plugin named MSE-Adapter, which enable the LLM’s to perform MSA or ERC task without affecting its inherent capabilities. Unlike previous works, we introduce a novel module named TGM in the MSE-Adapter. TGM facilitates feature-level alignment between non-textual and textual modalities, which aids the LLM to better understand content from non-textual modalities.

# Method

# Overall architecture

Figure 1 presents the comprehensive framework integrating MSE-Adapter with LLM. Given a sample $x _ { i }$ , the first step is to convert each modality into a sequence embedding. For the textual modality, we use the Text Embedder within the LLM to tokenize (Kudo and Richardson 2018) the input from the text modality and the Task-Specific-Prompt, and then convert them into sequence embeddings. For audio and vision modalities, we employ pre-trained toolkits (Yu et al. 2021; Liu et al. 2022; Hu et al. 2022b; Sun et al. 2023b) for feature extraction to transform them into feature sequence embeddings (See Appendix A for more details). After the encoding process, the sample’s textual modality, Task-Specific-Prompt, vision, and audio modalities are represented as $T \in \mathbb { R } ^ { i _ { t } \times d _ { t } }$ , $T _ { p } \in \mathbb { R } ^ { l _ { t } \times d _ { t } }$ , $V ~ \in ~ \mathbb { R } ^ { l _ { \nu } \times d _ { \nu } }$ and $A \in \mathbb { R } ^ { l _ { a } \times d _ { a } }$ , respectively, where $l _ { m \in \{ t , v , a \} }$ denotes the sequence length of each modality emb∈e{dding}, and $d _ { m \in \{ t , v , a \} }$ represents the corresponding feature vector dimension.

Subsequently, $T , V$ and $A$ are fed into the MSE-Adapter for further processing and generating pseudo tokens $P$ . Finally, we concatenate $P , \ T , \ T _ { p }$ to obtain $I$ (i.e., $\begin{array} { r l } { I } & { { } = } \end{array}$ $[ P ; { \dot { T } } ; T _ { p } ] )$ and input it into the frozen LLM, which returns the logits $g _ { i }$ corresponding to the input sample and the generated text $y _ { i }$ for the entire sentence (including input and output tokens). This can be expressed as:

$$
\{ y _ { i } , g _ { i } \} = L L M ( I , \theta )
$$

where $\theta$ represents the parameters of the LLM. The LLM predicts the conditional probability $\rho ( \gamma _ { j } | I , \theta )$ of each token $\gamma _ { j }$ of the generated text $y _ { i }$ until the end-of-sequence symbol $\mathrm { < e o s > }$ . For the logits $g _ { i } \in \mathbb { R } ^ { I _ { l } \times V _ { s } }$ , where $I _ { l }$ and $V _ { s }$ represent the length of the input $I$ and the size of the vocabulary used by the LLM, respectively.

Following the original training methodology of the LLM, we utilize the next-token prediction loss to measure the output error of the model. Hence, the loss calculation for the model task $L _ { t a s k }$ is defined as follows:

$$
L _ { t a s k } = \sum _ { k = 1 } ^ { N } - l o g \rho ( Y _ { k } | I , \theta )
$$

where $Y _ { k }$ represents the $k$ -th token of the sentiment label corresponding to the sample $x _ { i } , \ N$ denotes the number of sentiment label tokens. Based on the aforementioned loss, we optimize the parameters of the MSE-Adapter through gradient backpropagation, thereby enhancing the MSE-Adapter’s adaptation to the LLM. Upon training completion, the LLM can utilize the MSE-Adapter to receive multimodal inputs and generate autoregressive sentiment labels, similar to text generation.

![](images/c0d0c8f91e90f3e929da5230217ff3c394a67790ee37436ac3c0c51403f39b02.jpg)  
Figure 2: The architecture of MSE-Adapter.

# MSE-Adapter

In this section, we introduce the proposed MSE-Adapter, whose structure is depicted in Figure 2. The MSE-Adapter consists of two separate single directional Long Short-Term Memory (sLSTM) modules, the TGM module, the MSF module, and a Projector module. The sLSTM initially performs temporal modeling separately for $V$ and $A$ , and then it captures their end-state hidden representations to derive $\overline { { V } } \in \mathbb R ^ { h _ { v } \times 1 }$ and $\overline { { A } } \in \mathbb { R } ^ { h _ { a } \times 1 }$ . These outputs are then fed into the TGM to obtain $ { \widetilde { V } } _ { t } \in  { \mathbb { R } } ^ { h \times 1 }$ and $\widetilde { A _ { t } } \in \mathbb { R } ^ { h \times 1 }$ . Subsequently, these are inp einto the MSF forfusion, resulting in $\mathcal { \widetilde { M } } \in \mathbb { R } ^ { h \times 1 }$ . Finally, a Projector composed of two linear layefrs expands this into pseudo tokens $P$ .

Text-Guide-Mixer Feature alignment across different modalities has always been a significant issue in multimodal tasks. A key to modal feature alignment is establishing connections between modalities. In the research on recommendation systems and search engines, researchers have applied the Hadamard product to achieve feature crossing, thus creating explicit connections between features at the vector level (Lian et al. 2018; Wang et al. 2021). Inspired by these works, we propose the TGM module. TGM establishes an explicit connection by computing the Hadamard product between the feature vectors of the text modality and those of the non-text modalities. This strategy not only preserves the original individual features of the non-text modalities but also encourages non-text feature vectors to align with text modality feature vectors, narrowing the gap between nontext modality features and text features, enabling the MSEAdapter to generate pseudo tokens that are more easily understood by LLM.

The implementation is as follows: we first do global average pooling (GAP) on the textual modal inputs $T$ to obtain $\breve { T } \in \mathbb { R } ^ { 1 \times d _ { t } }$ , and then we use a linear layer to project $\overline { T }$ , $\overline { { V } }$ and $\overline { { A } }$ to the same dimension:

$$
\begin{array} { l } { { { \widetilde T } = W _ { t } { \overline { T } } ^ { T } } } \\ { { { \widetilde V } = W _ { v } { \overline { V } } } } \\ { { { \widetilde A } = W _ { a } { \overline { A } } } } \end{array}
$$

where $W _ { t } \in \mathbb { R } ^ { h \times d _ { t } }$ , $W _ { v } \in \mathbb R ^ { h \times h _ { v } }$ , and $W _ { a } \in \mathbb { R } ^ { h \times h _ { a } }$ .

Subsequently, we perform the Hadamard product of $\widetilde { V }$ and $\widetilde { A }$ with $\widetilde { T }$ , respectively:

$$
\begin{array} { l } { \widetilde { V } _ { t } = \widetilde { V } \odot \widetilde { T } } \\ { \widetilde { A _ { t } } = \widetilde { A } \odot \widetilde { T } } \end{array}
$$

where $\odot$ represents the Hadamard product, that is, the element-wise multiplication of matrices. Through this process, we obtain the new non-textual modality representations $\widetilde { V _ { t } }$ and $\widetilde { A _ { t } }$ .

Multi-Scale-Fusion Entrusting the task of complete multimodal fusion to a frozen LLM presents certain challenges. In response to this challenge, we adopt an early fusion approach for the non-textual modalities prior to their input into the LLM. Specifically, we introduc a module named MSF, dedicated to performing low-level fusion of the non-textual modalities. Subsequently, the high-level fusion between the textual and non-textual modalities is deferred to the LLM. This layered fusion approach enables the LLM to capture more refined and detailed multimodal fusion, thereby boosting model performance.

The implementation of the MSF module is as follows: Firstly, we sum the outputs $\widetilde { V _ { t } }$ and $\widetilde { A _ { t } }$ derived from the TGM to obtain $M$ . Subsequently, ewe utilizfe three Multi-Layer Perceptrons (MLPs) with diverse hidden layer dimensions to conduct feature fusion at multiple scales on $M$ . Specifically, for the $i$ th $( i \in \{ 1 , 2 , 3 \}$ ) MLP, we obtain $m _ { i }$ :

$$
m _ { i } = W _ { 2 } ^ { i } \sigma ( W _ { 1 } ^ { i } M )
$$

where $W _ { 1 } ^ { i } \ \in \ \mathbb { R } ^ { h / k \times h } , W _ { 2 } ^ { i } \ \in \ \mathbb { R } ^ { h \times h / k } , k \ \in \ \{ 8 , 1 6 , 3 2 \} .$ , $\sigma$ represents the GELU activation function. Subsequently, we stack the fusion results from the three different scales to obtain $\overline { { M } } \ = \ [ m _ { 1 } , m _ { 2 } , m _ { 3 } ] \ \in \ \mathbb { R } ^ { h \times 3 }$ . To further integrate information from various scales, we use a $1 { \times } 1$ convolution (Conv) to compress this information, resulting in M Rh×1.

Projector After extracting the fused information from non-textual modalities, the size of $\widetilde { M }$ is adjusted to meet the input requirements of the LLM usinfg a linear layer. Subsequently, another linear layer is utilized to increase the number of pseudo tokens:

$$
P = W _ { 4 } ( W _ { 3 } { \widetilde { M } } ) ^ { T }
$$

where $W _ { 3 } \in \mathbb { R } ^ { d _ { t } \times h } , W _ { 4 } \in \mathbb { R } ^ { n \times 1 }$ f( $n$ is our predetermined number of pseudo tokens). The resulting pseudo tokens, denoted by $P$ , can then be utilized as input into the LLM.

# Experiment

# Datasets

To more comprehensively evaluate the performance of the MSE-Adapter, we conducted MSA and ERC experiments on two popular English datasets and two popular Chinese datasets, namely MOSEI (Zadeh et al. 2018), MELD (Poria et al. 2018), SIMS-V2 (Liu et al. 2022), and CHERMA (Sun et al. 2023b). The details of the dataset are shown in the appendix B.

# Experimental Settings

In this subsection, we briefly introduce the detailed setup of our experiments. Our investigation utilizes the Qwen-1.8B (Bai et al. 2023), ChatGLM3-6B-base (Du et al. 2022), and LLaMA2-7B (Touvron et al. 2023) models as the backbone. For ease of presentation, we add the prefix ‘MSE’ to the backbone to indicate the integration of the MSE-Adapter (e.g., MSE-Qwen-1.8B). The trainable parameters of MSEAdapter in various LLMs are presented in Table 1. For a fair comparison, we selected [1111, 2222, 3333, 4444, 5555] as random seeds for the experiments and reported the average results achieved across these five random seeds. All experiments were conducted on a single NVIDIA RTX 4090 GPU. The optimizer used was AdamW with a warmup learning rate strategy. The rest of the settings can be found in Appendix C.

Table 1: Trainable parameters of LLMs integrating MSEAdapter.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="3">Trainable parameters</td></tr><tr><td>MOSEI</td><td>SIMS-V2 MELD</td><td>CHERMA</td></tr><tr><td>MSE-Qwen-1.8B</td><td>1.35M</td><td>1.40M 1.35M</td><td>1.56M</td></tr><tr><td>MSE-LLaMA2-7B</td><td>2.63M</td><td>2.68M 2.63M</td><td>2.84M</td></tr><tr><td>MSE-ChatGLM3-6B</td><td>2.63M</td><td>2.68M 2.63M</td><td>2.84M</td></tr></table></body></html>

To standardize labels for the MSA task, for models where the tokenizer does not automatically generate tokens to distinguish between positive and negative labels, we manually add a $\cdot _ { + } ,$ sign to labels greater than or equal to 0. Furthermore, to streamline the answer generation process for the ERC task, we translated the emotion labels into distinct numerical values and incorporated these details into the prompts, as illustrated in Appendix C.

# Baselines

We compared the performance of “MSE-Qwen-1.8B”, “MSE-LLaMA2-7B” and “MSE-ChatGLM3-6B” to that of previous state-of-the-art models: TFN (Zadeh et al. 2017), LMF (Liu et al. 2018), MISA (Hazarika, Zimmermann, and Poria 2020), MAG-BERT (Rahman et al. 2020), Self-MM (Yu et al. 2021), MMIM (Han, Chen, and Poria 2021), CHFN (Guo et al. 2022), UniMSE (Hu et al. 2022b), UniSABART, $\mathrm { U n i S A _ { T 5 } }$ , UniSAGPT2 (Li et al. 2023b), AV-MC (Liu et al. 2022), MMGCN (Hu et al. 2021), MM-DFN (Hu et al. 2022a), EmoCaps (Li et al. 2022), GA2MIF (Li et al. 2023a), EFT, LFT, MulT (Tsai et al. 2019), PMR (Lv et al. 2021), and LFMIM (Sun et al. 2023b). The details of these baseline models are given in the Appendix D.

# Metrics

In this study, due to the differing labels across datasets, we report our experimental results using a variety of metrics tailored to each dataset. For MOSEI, we report the mean absolute error (MAE), Pearson correlation (Corr), seven-category accuracy (Acc-7), binary accuracy (Acc-2), and F1 score as evaluation metrics (where Acc-2 and F1 are calculated based on a non-negative/negative standard). For SIMS-V2, we report MAE, Corr, Acc2 weak, Acc-2, and F1 (where Acc-2 and F1 are calculated based on a non-positive/positive standard, and Acc2 weak is used to further validate the model’s performance on weakly emotional instances within the [-0.4, 0.4] range). For MELD and CHERMA, we report seven-category accuracy (Acc) and weighted F1 (WF1).

# Results

Tables 2 and 3 present the results of the performance comparison of multiple methods on the MSA and ERC tasks, respectively. It is noteworthy that, overall, all three LLMs incorporating the MSE-adapter exhibited outstanding performance.

It is evident that MSE-ChatGLM3-6B, achieved the most comprehensive performance, outperforming baseline models on most metrics. Notably, it showed a great performance improvement on CHERMA, the dataset with the largest volume of data, where its WF1 was $2 . 1 9 \%$ higher than the best baseline LFMIM. Although MSE-LLaMA2-7B has a larger parameter size than MSE-ChatGLM3-6B, its overall performance was not as good, especially on the Chinese datasets SIMS-V2. This might be due to MSE-LLaMA2-7B’s inherent limitations in processing Chinese text. Since the LLM is frozen, the performance of the model is highly dependent on the inherent capabilities of the LLM.

Interestingly, MSE-LLaMA2-7B still performed better than the baseline on CHERMA (even though it is not particularly skilled in Chinese), somewhat confirming that LLMs

<html><body><table><tr><td rowspan="2">Model</td><td colspan="5">MOSEI</td><td colspan="5">SIMS-V2</td></tr><tr><td>Acc-2</td><td>F1</td><td>Acc-7</td><td>MAE</td><td>Corr</td><td>Acc-2</td><td>F1</td><td>Acc2_weak</td><td>MAE</td><td>Corr</td></tr><tr><td>TFN*</td><td>78.50</td><td>78.96</td><td>51.60</td><td>0.573</td><td>0.714</td><td>76.51</td><td>76.31</td><td>66.27</td><td>0.323</td><td>0.667</td></tr><tr><td>LMF*</td><td>80.54</td><td>80.94</td><td>51.59</td><td>0.576</td><td>0.717</td><td>77.05</td><td>77.02</td><td>69.34</td><td>0.343</td><td>0.638</td></tr><tr><td>MulT*</td><td>81.15</td><td>81.56</td><td>52.84</td><td>0.559</td><td>0.733</td><td>79.50</td><td>79.59</td><td>69.61</td><td>0.317</td><td>0.703</td></tr><tr><td>MAG-BERT*</td><td>82.51</td><td>82.77</td><td>50.41</td><td>0.583</td><td>0.741</td><td>79.79</td><td>79.78</td><td>71.87</td><td>0.334</td><td>0.691</td></tr><tr><td>MISA</td><td>83.60</td><td>83.80</td><td>52.20</td><td>0.555</td><td>0.756</td><td>80.53</td><td>80.63</td><td>70.50</td><td>0.314</td><td>0.725</td></tr><tr><td>Self-MM*</td><td>82.81</td><td>82.53</td><td>53.46</td><td>0.530</td><td>0.765</td><td>79.01</td><td>78.89</td><td>71.87</td><td>0.335</td><td>0.640</td></tr><tr><td>MMIM</td><td>82.24</td><td>82.66</td><td>54.24</td><td>0.526</td><td>0.772</td><td>80.95</td><td>80.97</td><td>72.28</td><td>0.316</td><td>0.707</td></tr><tr><td>AV-MC</td><td>1</td><td>1</td><td></td><td></td><td></td><td>82.50</td><td>82.55</td><td>74.54</td><td>0.297</td><td>0.732</td></tr><tr><td>CHFN</td><td>83.70</td><td>83.90</td><td>54.30</td><td>0.525</td><td>0.778</td><td></td><td></td><td></td><td>-</td><td>-</td></tr><tr><td>UniMSE</td><td>85.86</td><td>85.79</td><td>54.39</td><td>0.523</td><td>0.773</td><td>1</td><td></td><td>1</td><td></td><td>一</td></tr><tr><td>UniSAGPT2</td><td>71.02</td><td>1</td><td>41.36</td><td>0.838</td><td>-</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>UniSAT5</td><td>84.22</td><td>1</td><td>52.50</td><td>0.546</td><td>-</td><td>1</td><td>1</td><td></td><td></td><td>-</td></tr><tr><td>UniSABART</td><td>84.93</td><td>1</td><td>50.03</td><td>0.587</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>MSE-Qwen-1.8B</td><td>84.12</td><td>83.45</td><td>52.02</td><td>0.558</td><td>0.725</td><td>80.44</td><td>80.24</td><td>73.09</td><td>0.311</td><td>0.678</td></tr><tr><td>MSE-LLaMA2-7B</td><td>86.74</td><td>86.51</td><td>55.57</td><td>0.501</td><td>0.787</td><td>75.53</td><td>75.44</td><td>68.61</td><td>0.382</td><td>0.553</td></tr><tr><td>MSE-ChatGLM3-6B</td><td>86.91</td><td>86.77</td><td>54.56</td><td>0.515</td><td>0.783</td><td>83.77</td><td>83.76</td><td>75.24</td><td>0.296</td><td>0.720</td></tr></table></body></html>

Table 2: Experimental results of the MSA task on the MOSEI and SIMS-V2 datasets: 1) Results for models marked with \* on MOSEI are sourced from the official repository1, while results for other models are extracted from relevant published papers; 2) All models’ results on SIMS-V2 are cited from literature (Liu et al. 2022).

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">MELD</td><td colspan="2">CHERMA</td></tr><tr><td>Acc</td><td>WF1</td><td>Acc</td><td>WF1</td></tr><tr><td>TFN*</td><td>60.77</td><td>57.74</td><td></td><td>68.37</td></tr><tr><td>LMF*</td><td>61.15</td><td>58.30</td><td>-</td><td>68.23</td></tr><tr><td>EFT</td><td></td><td>1</td><td></td><td>68.72</td></tr><tr><td>LFT</td><td></td><td></td><td></td><td>69.05</td></tr><tr><td>MulT</td><td>-</td><td>-</td><td></td><td>69.24</td></tr><tr><td>PMR</td><td>-</td><td></td><td></td><td>69.53</td></tr><tr><td>LFMIM</td><td>-</td><td>1</td><td>-</td><td>70.54</td></tr><tr><td>MMGCN*</td><td>60.42</td><td>58.31</td><td>-</td><td></td></tr><tr><td>MM-DFN*</td><td>62.49</td><td>59.46</td><td>-</td><td></td></tr><tr><td>EmoCaps</td><td></td><td>64.00</td><td>-</td><td>1</td></tr><tr><td>GA2MIF</td><td>61.65</td><td>58.94</td><td>1</td><td>1</td></tr><tr><td>UniMSE</td><td>65.09</td><td>65.51</td><td>-</td><td>-</td></tr><tr><td>UniSAGPT2</td><td>48.12</td><td>31.26</td><td></td><td></td></tr><tr><td>UniSAT5</td><td>64.52</td><td>62.17</td><td></td><td></td></tr><tr><td>UniSABART</td><td>62.34</td><td>62.22</td><td></td><td>1</td></tr><tr><td>MSE-Qwen-1.8B</td><td>62.18</td><td>59.87</td><td>70.38</td><td>70.21</td></tr><tr><td>MSE-LLaMA2-7B</td><td>65.14</td><td>63.66</td><td>71.58</td><td>71.41</td></tr><tr><td>MSE-ChatGLM3-6B</td><td>66.23</td><td>65.13</td><td>72.90</td><td>72.73</td></tr></table></body></html>

Table 3: Experimental results of the ERC task on the MELD and CHERMA datasets: 1) Results for models marked with \* on MELD are cited from the the literature (Hu et al. 2022a), while results for other models are extracted from relevant published papers; 2) All models’ results on CHERMA are cited from literature (Sun et al. 2023b).

are essentially pattern machines (Mirchandani et al. 2023), capable of learning mapping relationships from large data volumes, even for tasks they are less proficient in. This was observed similarly on MSE-Qwen-1.8B. Although MSEQwen-1.8B’s performance was lower than the other two backbones, it requires the least trainable parameters. Since MSE-Qwen-1.8B is deployable on mobile devices, such a small-parameter plugin can offer users a more efficient interaction experience.

# Discussion

# Ablation Study

To evaluate the contribution of each module within the MSE-Adapter, we conducted ablation experiments on the English dataset MELD and the Chinese dataset SIMS-V2. For MELD, we reported Acc and WF1 score, and for SIMSV2, we reported Acc-2 and F1 score. Given that MSEChatGLM3-6B achieved the most comprehensive performance in our experiments, all of our ablation experiments were based on MSE-ChatGLM3-6B and reported the average results achieved across the five random seeds (same random seeds as experimental). Table 4 shows the results of the ablation experiments.

Effectiveness of TGM and MSF Initially, we evaluated the TGM and MSF module within the MSE-adapter. The results shown in Table 4 indicated that removing TGM decreased model performance, highlighting the importance of establishing cross-modal connections between non-textual and textual features for modality alignment and model performance. Similarly, omitting MSF also resulted in a decline in performance, with a more significant impact than that

<html><body><table><tr><td rowspan="2"></td><td colspan="2">MELD</td><td colspan="2">SIMS-V2</td></tr><tr><td>Acc</td><td>WF1</td><td>Acc-2</td><td>F1</td></tr><tr><td>w/o A</td><td>66.24</td><td>65.09</td><td>83.46</td><td>83.42</td></tr><tr><td>w/o V</td><td>66.13</td><td>65.00</td><td>78.84</td><td>78.68</td></tr><tr><td>w/o T</td><td>46.44</td><td>36.53</td><td>72.78</td><td>72.21</td></tr><tr><td>w/o A, V</td><td>57.11</td><td>53.92</td><td>76.54</td><td>75.98</td></tr><tr><td>w/o TGM</td><td>65.07</td><td>63.85</td><td>82.98</td><td>82.97</td></tr><tr><td>w/o MSF</td><td>62.72</td><td>61.75</td><td>81.37</td><td>81.32</td></tr><tr><td>MSE-Adapter</td><td>66.23</td><td>65.13</td><td>83.77</td><td>83.76</td></tr></table></body></html>

Table 4: The ablation experiments results on MELD and SIMS-V2, where $T , V$ , and $A$ represent textual, vision, and audio modalities, respectively. And the ‘w/o’ means remove a modality or module. Furthermore, in the case of “w/o $T ^ { \ast }$ , the TGM module becomes ineffective. $\overline { V }$ and $\overline { { A } }$ are mapped to the same dimension through two independent linear layers and then directly summed before being fed into the MSF module. In the case of “w/o $A$ , $V ^ { \ast }$ , $\overline { { V } }$ and $\overline { { A } }$ are randomly initialised into the corresponding dimensions into the TGM and the model is trained anyway.

of TGM. This observation further illustrates that the multimodal fusion capacity of frozen LLM is limited. Therefore, to enhance the fusion of the three modalities, early fusion of non-textual modalities prior to LLM input is necessary.

The Impact of Absent Modalities Additionally, we removed one or several modalities from the multimodal signals to verify their impact on model performance. The ablation results are presented in Table 4. From the results, we discovered that eliminating either the vision or audio modalities, or both, led to a decrease in performance, indicating the necessity of non-textual modalities (i.e., vision and audio) for solving MSA and ERC tasks. Notably, the removal of the textual modality had the most severe impact on performance, underscoring the critical role of language in identifying emotions in the real world.

# Further Discussion with Models Adapted for BERT

In previous work, MAG (Rahman et al. 2020) and CHFN (Guo et al. 2022) conducted similar research, primarily utilizing their designed adaptation model to fine-tune BERT with a classification header for MSA tasks. We previously reported their performance metrics. For a fair comparison, we reproduced their adaptation model based on the original paper and adapted it to ChatGLM3-6B within our framework. We conducted experiments on the MOSEI, MELD, SIMS-V2, and CHERMA dataset, maintaining consistent experimental settings with the aforementioned chapters. It should be noted that the hyperparameters are only reported in the original paper for the MOSEI dataset in the MSA task. Consequently, in our reproduction, we maintain consistency with the original paper by keeping the hyperparameters on MOSEI only. The hyperparameters on the rest of the datasets are set with reference to MOSEI. Due to space limitations, we only show partial results for MOSEI and MELD in table 5, and complete results are detailed in Appendix E.

Table 5: Experimental results on the MOSEI and MELD datasets. The “tParas” represents the trainable parameter of the model.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">MOSEI</td><td colspan="2">MELD</td><td rowspan="2">Paras</td></tr><tr><td>Acc-2</td><td>F1</td><td>Acc</td><td>WF1</td></tr><tr><td>MAG-ChatGLM3-6B</td><td>85.10</td><td>84.73</td><td>60.38</td><td>59.81</td><td>34.47M</td></tr><tr><td>CHFN-ChatGLM3-6B</td><td>85.58</td><td>85.26</td><td>63.03</td><td>62.08</td><td>50.79M</td></tr><tr><td>MSE-ChatGLM3-6B</td><td>86.91</td><td>86.77</td><td>66.23</td><td>65.13</td><td>2.63M</td></tr></table></body></html>

The experimental results demonstrate that the MSEAdapter outperforms both MAG and CHFN when using the same backbone network. Additionally, while replacing BERT with LLM in MAG and CHFN can improve performance, the improvements are limited. Despite integrating more comprehensive multimodal information, MAG and CHFN introduce a degree of information redundancy. When the backbone network is trained, it benefits from this rich information, which enhances its performance. However, when the backbone network is frozen, this redundancy complicates its ability to effectively interpret multimodal content. In contrast, the MSE-Adapter is more efficient in capturing key information and extracting key features used to complete sentiment analysis, which reduces the difficulty of the backbone’s comprehension. In conclusion, MSE-Adapter not only effectively reduces the number of trainable parameters, but also enhances the performance of the backbone on MSA or ERC tasks.

# Conclusion

This paper presents the MSE-Adapter, a lightweight plugand-play plugin that empowers an LLM to handle MSA or ERC task without compromising its inherent capabilities. The MSE-Adapter includes a module called TGM, which is designed to facilitate the alignment of non-textual modalities with textual ones. By employing the Hadamard product, TGM establishes explicit connections between non-textual and textual modalities at the feature level, thereby enhancing the LLM’s comprehension of content from non-textual sources. MSF is another module of MSE-Adapter that employs MLPs of varying scales for early fusion of non-textual modalities, followed by further integration of this information using convolutional layers. This further enhances the efficiency of LLM in fusing information from diverse modalities. Deployable on consumer-grade GPUs and utilizing open-source LLMs (Qwen-1.8B, ChatGLM3-6B-base, and LLaMA2-7B) as the backbone, we conducted extensive experiments across four public English and Chinese datasets. The competitive results demonstrate the efficacy of MSEAdapter. It not only optimizes parameter efficiency but also maintains the LLM’s efficiency and flexibility, providing a new solution and baseline for the application of LLM in MSA and ERC.