# Controlling Equational Reasoning in Large Language Models with Prompt Interventions

Jordan Meadows1, Marco Valentino2, André Freitas1, 2, 3

1Department of Computer Science, University of Manchester 2Idiap Research Institute 3National Biomarker Centre, CRUK-MI jordan.meadows $@$ postgrad.manchester.ac.uk {marco.valentino, andre.freitas}@idiap.ch

# Abstract

This paper investigates how hallucination rates in Large Language Models (LLMs) may be controlled via a symbolic data generation framework, exploring a fundamental relationship between the rate of certain mathematical errors and types of input intervention. Specifically, we systematically generate data for a derivation generation task using a symbolic engine, applying targeted interventions to prompts to perturb features of mathematical derivations such as the surface forms of symbols, equational tree structures, and mathematical context. We then evaluate the effect of prompt interventions across a range of LLMs including fine-tuned T5 models, GPT, and LLaMabased models. Our experiments suggest that T5-Large can outperform the few-shot performance of GPT-4 on various evaluation sets generated via the framework. However, an extensive evaluation based on human analysis, template-based error detection, and text generation metrics reveals model weaknesses beyond what the reference-based metrics singularly describe. We use these results to tie characteristic distributional footprints of interventions to the human evaluation of LLM derivation quality, potentially leading to significant control over fine-grained mathematical capabilities of language models with respect to specific types of errors.

# Introduction

Large Language Models (LLMs) possess the potential to accelerate mathematical discovery (Trinh et al. 2024). Yet, without incorporating symbolic approaches, their ability to derive correct mathematical results is significantly impeded by their sensitivity to input perturbations and tendency to hallucinate (Mirzadeh et al. 2024; Frieder et al. 2023). In order to understand and mitigate persisting challenges in mathematical reasoning, recent work has focused on the ability of LLMs to learn and sequentially apply symbolic operations to premise equations (Chen et al. 2024; Valentino et al. 2024), investigating whether state-of-the-art models can derive goal equations defined within prompts (Meadows et al. 2024). Such derivation-style equational reasoning is at the core of many applied mathematical fields, such as theoretical physics, engineering, and quantitative finance (Plaisted 1993; Premtoon, Koppel, and Solar-Lezama 2020).

In this paper, we aim to establish a deeper connection between input perturbations applied to prompts and certain mathematical hallucinations, attempting to expose a fundamental relationship between training data, prompt intervention type, and derivational error distributions in LLM output. In particular, we are interested in whether such a relationship can be identified and potentially leveraged to control the rate of certain types of mathematical errors.

To this end, we develop a methodology supported by a symbolic data generation framework that has been effectively applied in related contexts (Valentino et al. 2024; Meadows et al. 2024), which we adapt and improve $\approx 1 5 \mathbf { x }$ faster) to construct and augment various fine-grained datasets for equational reasoning tasks. This framework allows us to operate at a mathematical granularity within derivations that is far greater than what is surfaced on published derivations (Mann et al. 2018; Akrobotu et al. 2022), where many steps are typically omitted or summarised. This ultimately addresses a fundamental incompleteness problem for reasoning data available for training and evaluating LLMs (Villalobos et al. 2022). Given that fine-grained workings contribute to much of the theoretical research distilled in papers, that generative models frequently hallucinate when solving domain-specific reasoning problems (Shuster et al. 2021; Taylor et al. 2022; Frieder et al. 2023; Wysocka et al. 2023; Meadows, James, and Freitas 2024), and that granular reasoning lends itself better to explainability and inference control (Hebenstreit et al. 2023; Yao et al. 2023; Yuan et al. 2023; Valentino and Freitas 2024), it is clear that methods of control over the rate of mathematical reasoning errors made by LLMs, without incorporating external solvers (Toshniwal et al. 2024; Liu and Yao 2024; Trinh et al. 2024; Jiang et al. 2022; Quan et al. 2024), are highly valuable.

By leveraging the granularity of the symbolic data generation framework, we investigate control mechanisms that involve fine-tuning LLMs and applying targeted prompt interventions that systematically manipulate inputs to the models. These interventions — here, alterations in symbolic representation, equational structure, and contextual elements — serve as levers to induce and regulate specific error types in model outputs, moving beyond methodologies which are agnostic to certain classes of mathematical hallucination and/or deal with less complex equation manipulation (Stolfo et al. 2023; Meadows et al. 2024). Crucially, our experiments suggest that certain interventions distinctly correspond to distributional footprints in the error space, opening up the possibility to better investigate the underlying nature of mathematical hallucinations in language models.

![](images/9648c6e6ba84d16ec334dcf6ba7baf84a7fe469ece7ff8cc312343fe683a0977.jpg)  
Figure 1: Mechanism relating the type and strength of a set of interventions to their corresponding mathematical errors. The black line indicates the original prompt/output, while the arrows and dashed lines indicate the extent of the input perturbation and respective output delta.

Overall, our contribution can be summarised as follows:1

1. We construct and release a dataset of $3 0 \mathrm { k }$ mathematically fine-grained prompt-derivation pairs spanning 18 operators, 155 wildcard (LaTeX) symbols, 4 targeted distribution shifts, up to 10 equations per derivation, and $1 6 0 \mathrm { k }$ steps — all developed using a symbolic data generation framework. We also improve the speed of this framework by $\approx 1 5 \mathrm { { x } }$ and fix limitations involving irrelevant steps.

2. We demonstrate that T5 models fine-tuned on fine-grained mathematical derivations generated using our approach can match or surpass the in-distribution few-shot performance of GPT-4 according to all evaluation methods. In addition, we evaluate a range of open source models in a few-shot setting including LLaMa-2-7B and Llemma-7B.

3. We investigate 3 complementary evaluation methods to determine the mathematical proficiency of LLMs: (1.) reference-based text generation metrics (including 4 metrics), (2.) template-based detection of mathematical errors, and (3.) human evaluation of 750 derivations. Each method is applied to both in-distribution and out-ofdistribution data augmented by interventions. While we find that template-based and human evaluations are correlated, they both strongly disagree with reference-based metrics, proving that the latter deliver misleading model performance rankings and inappropriate representations of the relative effect of interventions.

4. We demonstrate a fundamental underlying mechanism where the rates of certain errors are controlled by systematically varying both the strength and type of interventions on the prompt, visualised in Fig. 1. For instance, the rate of “redundant” equations (e.g., $x = x $ ) increases by up to $200 \%$ in fine-tuned models, based on an intervention which perturbs equation symmetry. An intervention that removes integration/differentiation results from the prompt leads to a relative increase in step skipping by up to $300 \%$ , and leads GPT-4 to make $100 \%$ more reasoning errors, according to human evaluation. Although each intervention naturally affects multiple error categories, their distributional footprint can be uniquely identified, and controlled via the magnitude of each intervention type.

# Derivation Generation with LLMs

Given a goal equation $G$ and premises $\mathcal { P }$ , that are arranged within some prompt template $t ( { \mathcal { P } } , G )$ , we aim to assess the ability of an LLM to systematically apply a set of symbolic operations to premises to generate a sequence of equations $\hat { \mathcal { D } }$ , which represents a reasonable derivation of $G$ . Given an LLM $\mathcal { M }$ , a derivation is generated through $\mathcal { M } : t ( \mathcal { P } , G ) \mapsto \hat { \mathcal { D } }$ . An idealised metric $M ^ { * } ( \mathcal { D } ^ { * } , \hat { \mathcal { D } } )$ scores a derivation, where ${ \mathcal { D } } ^ { * }$ is ideally a valid human written derivation corresponding to input prompt $t ( { \mathcal { P } } , G )$ . We generally aim to optimise

$$
\mathcal { M } ^ { * } = \underset { \mathcal { M } } { \mathrm { a r g m a x } } ; M ^ { * } \big ( \mathcal { D } ^ { * } , \mathcal { M } ( t ( \mathcal { P } , G ) ) \big ) .
$$

However, we do not have access to ideal derivations ${ \mathcal { D } } ^ { * }$ corresponding to templates $t ( { \mathcal { P } } , G )$ , nor ideal metric $M ^ { * }$ suitable for scoring $\hat { \mathcal { D } }$ . Instead, we employ a symbolic engine to approximate ground truth derivations to obtain $\tilde { \mathcal { D } ^ { * } }$ (Alg. 1). Moreover, we evaluate over a sample of derivations. This means that, in practice, we are instead finding $\mathcal { M } ^ { * }$ such that

$$
\begin{array} { r } { \mathcal { M } ^ { * } = \underset { \mathcal { M } } { \mathrm { a r g m a x } } ; \frac { 1 } { N } \sum _ { i = 1 } ^ { N } M \big ( \tilde { \mathcal { D } ^ { * } } _ { i } , \mathcal { M } ( t ( \mathcal { P } _ { i } , G _ { i } ) ) \big ) , } \end{array}
$$

where $N$ is the sample size. In this work, we consider $M$ as a reference-based generation metric to automatically evaluate derivations, but we contrast this with a human evaluation based on equation consistency and coherent operator usage, and a template-based error detection method.

# Dataset Construction

The symbolic data generation process used in this work is described by Alg. 1. Given a vocabulary of symbols $\nu$ and a set of computer algebra operations $\mathcal { R }$ , our goal is to generate a mathematical derivation $\mathcal { D }$ represented by an ordered list of steps $s _ { i } \in \mathcal { D }$ . In order to construct $\mathcal { D }$ , an initial reasoning step $s _ { 1 } =$ (premise, annotation) is generated such that $\mathcal { D } = [ s _ { 1 } ]$ . Subsequently, an operation $\boldsymbol { r } \in \mathcal { R }$ is sampled, which in its most general form accepts two operands (arity 2). The first operand is an equation $s _ { j , 1 }$ from tuple $s _ { j } \in \mathcal { D }$ . A suitable secondary variable $( \in \mathcal { V } )$ , expression, or equation operand $m$ is extracted from $\mathcal { D }$ , and the next equation is generated by applying operation $r$ through $\boldsymbol { s } _ { i + 1 , 1 } = \boldsymbol { r } ( \boldsymbol { s } _ { j , 1 } , m )$ . The annotation $s _ { i + 1 , 2 }$ is a list containing the name of the operation, the equation index, and the secondary operand, such that $s _ { i + 1 , 2 } = [ r , j , m ^ { \prime } ]$ (where $m ^ { \prime }$ is a variable/expression string or equation index representing operand $m$ ). Therefore, step $s _ { i + 1 } = ( r ( s _ { j , 1 } , m ) , [ r , j , m ^ { \prime } ] )$ . If $\mathcal { D } = [ s _ { 1 } ]$ , then $i = j = 1$ , and the derivation updates such that $\mathcal { D } = [ s _ { 1 } , s _ { 2 } ]$ . This process repeats until the derivation reaches a target length.

<html><body><table><tr><td>Algorithm1:Derivation Generation</td></tr><tr><td>Input:VocabularyofsymbolsV,Setofoperations R Output: Ordered list of derivation steps D 1: Initialize derivation D with a premise step S1 (premise equation,annotation) 2:Seti=1</td></tr><tr><td>3:while desired length of D not reached do 4: Sample an operation r ∈ R 5: Select an equation Sj,1 from tuple sj ∈ D</td></tr><tr><td>6: Extract a suitable operand m from Vor D that matches the requirements of r 7: Generate the next equation Si+1,1 = r(Sj,1, m)</td></tr><tr><td>8: Create an annotation Si+1,2 representing the operation and operands: Si+1,2 = [r,j,m'] where m′ is an index or variable/expression string corresponding to m 9: Appendthenewstep tothederivation:</td></tr></table></body></html>

11: end while 12: return $\mathcal { D }$

Table 1: Sizes for the various Derivation Generation datasets.   

<html><body><table><tr><td>Dataset</td><td>Size (k)</td></tr><tr><td>Training</td><td>15.3</td></tr><tr><td>Static Test Set (In-distribution)</td><td>3.1</td></tr><tr><td>Variable Renaming (VR)</td><td>2.9</td></tr><tr><td>Expression Exchange (EE) Alternative Goal (AG)</td><td>3.1</td></tr><tr><td>Step Removal (SR)</td><td>3.1 1.0</td></tr></table></body></html>

A derivation generated from Alg. 1 is then perturbed according to a set of interventions to form out-of-distribution examples. Specifically, given the task of Derivation Generation instantiated via the prompt template $t = t ( \mathcal { P } , G )$ and a ground truth derivation $\mathcal { D }$ , a static dataset $X$ consisting of $( t , \mathcal { D } )$ is constructed using the process described above. Subsequently, a perturbed dataset $X _ { n }$ is formed by applying a perturbation function $P _ { n }$ to all $( t , { \mathcal { D } } ) \in X$ to form $( t ^ { \prime } , \bar { D ^ { \prime } } ) \in X _ { n }$ , such that $P _ { n } : X \to X _ { n }$ , and $n$ denotes the number of perturbations considered.

Tab. 1 describes the dataset sizes generated by the symbolic framework, Fig. 2 displays the distribution of equation counts in ground truth references (i.e., derivation length), and Tab. 2 shows that the operation chains responsible for forming the underlying derivation reasoning do not frequently repeat.

On the omission of natural language. Although the framework (through Alg. 1) outputs both equations and step annotations by default, we purposefully remove annotations from the output in the specific Derivation Generation task considered in this work. Firstly, annotations give additional information on the dependency structure between equations, but they are certainly not necessary for the purpose of generating valid dependency graphs between equations with generative models. Ground truth derivations from the dataset can be clearly followed without natural language. Second, without annotations, the coherence of the derivation depends on the equations and their dependencies alone. This is more targeted than the alternative of additionally determining whether annotations match up with equations.

![](images/38b6dc01bf663e96a16f085113b9a20e0ea6b942d1d10520c9ea1c8aaab0d308.jpg)  
Figure 2: Length distribution $P ( L )$ of derivations.

Table 2: For a given derivation length $L$ , Permutations describes the number of unique operation sequences present in the training data. Chain describes the two most frequent operation sequences based on symbols. $\mathbf { P } ( \mathbf { C h a i n } )$ is the probability of the chain.   

<html><body><table><tr><td>Length (L)</td><td>Permutations</td><td>Chain</td><td>P(Chain)</td></tr><tr><td>4</td><td>842</td><td>a→∂ε→SL {→SE→SL</td><td>0.0369 0.0186</td></tr><tr><td>5</td><td>2850</td><td>+→δ→δE→SL →∂→dE→SL</td><td>0.0053 0.0048</td></tr></table></body></html>

Improvements to symbolic data generation framework. We rely on the symbolic framework proposed in Meadows et al. (2024) to support the experimental pipeline following related work in non-generative settings (Valentino et al. 2024). We improve the data generation approach in the following ways:

1. Support for complex LaTeX symbols (e.g., $\Psi _ { n l } $ ) instead of more basic symbols (e.g., x). 2. Removed irrelevant and disconnected equations from derivations by including additional dependency checks between derivation steps. This improvement was crucial for eliciting the desired derivational behaviour in models via fine-tuning and in-context learning. 3. Improved runtime efficiency by a factor of 15 by allowing derivations to both equal or exceed the target length, including timeout decorators on certain operations, and using more efficient iteration limiting. The approximate difference is $\mathbf { < 0 . 0 5 \mathrm { { m i n } . } }$ /derivation compared to 0.7 min/derivation tested over 100 samples.

We adopt the same set of hyperparameters described in Meadows et al. (2024) using the following values: p_history $_ { = 1 0 }$ , p_arity_ $\scriptstyle 0 = 5$ , p_renaming $\scriptstyle = 1$ , p_arity_ $\scriptstyle 1 = 5 0$ , p_evaluate $\scriptstyle = 5 0$ , p_arity_ $\scriptstyle 2 = 1 0 0$ , p_int_or_diff $\mathrm { \dot { = } } 1$ , p_subs ${ \ o } = 5$ .

# Prompt Interventions and Perturbations

A perturbation or intervention is a transformation applied to the input text and/or ground truth that ideally changes a single target textual aspect. We apply 4 interventions to the static test set to generate corresponding perturbed sets.

Variable Renaming (VR). In the training set, derivations rely on a vocabulary of 155 symbols (e.g., $\Psi _ { n l } , E _ { n } , \mathbf { J } _ { P } , \eta$ $g _ { \varepsilon . } ^ { \prime }$ ). For each example in the static set, we uniquely map each symbol to an out-of-distribution symbol sampled from 11 Greek letters (e.g., $E _ { n } = n + x$ becomes $\alpha = \beta + \gamma )$ .

Expression Exchange (EE). In the training set and applied mathematics in general, there is an asymmetry with respect to premises being defined with functions on the LHS and expressions on the RHS (e.g., $E _ { n } ( n , x ) = n + x )$ . However, operations are frequently used that can substitute LHS for RHS (and vice versa) in many cases, and both functions and operations may appear on either side of equations. Simply, we swap expressions either side of the equality for all equations in the static test set (e.g., $E _ { n } = n + x$ becomes $n + x = E _ { n } ,$ ).

Alternative Goal (AG). For each example in the static set, we derive an alternative goal equation from the penultimate equation, by random selection of operators and operands, which equates to the synthetic data algorithm skipping its first choice goal equation for that derivation. This perturbation should not result in significant differences in model outputs as it simply applies alternative in-distribution operations that occur frequently during training or within few-shot prompts.

Step Removal (SR). In the training set, equations that occur as a result of evaluating differentials and integrals are included in the prompt as intermediate steps. These are used to guide model outputs. This perturbation removes such “then derive” equations from the prompt, which forces models to either circumvent such steps or derive them during inference.

# Empirical Evaluation

Empirical setup. We evaluate a range of LLMs on the Derivation Generation task including T5 (Raffel et al. 2020), FLAN-T5 (Chung et al. 2024), GPT-3.5 and 4 (Brown et al. 2020; Achiam et al. 2023), LLaMa-2-7B (Touvron et al. 2023), and Llemma-7B (Azerbayev et al. 2023). We fine-tune base (220M) and large (770M) variants of T5 and FLAN-T5, while using the remaining models in a few-shot setting. The evaluation occurs across 3 complementary methods: (1.) use of reference-based text generation metrics (Tab. 3), (2.) error count as determined by searching model output for surfacelevel mathematical errors (Tab. 4), and (3.) a manual analysis of models’ reasoning accuracy across 750 total derivations (Tab. 5). Additional details on models, training, and metrics can be found online.2

Prompt design. We fine-tune and zero-shot prompt the T5 models following the template below, which corresponds to a ground truth sequence of equations:

$$
e ^ { { \cal G } ( a ) } = 1
$$

To few-shot prompt the decoder-only models (i.e., GPT, LLaMa and Llemma) we use the following design, where $n = 5$ is the number of in-context examples:

The following examples consist of a prompt (denoted by Prompt:) and a mathematical derivation (denoted by Derivation:). Each derivation contains LaTeX equations separated by "and".

Prompt: [Prompt 1] Derivation: [Derivation 1]

Prompt: [Prompt n] Derivation: [Derivation n]

Now given the following prompt, generate the derivation.   
Ensure equations are split by the word "and".

Prompt: [Evaluation Prompt]

This approach was chosen to minimise natural language in the generated output, and to force derivations into the desired format (LaTeX equations split by “and”). Notably, only the Evaluation Prompt is perturbed, ensuring that the bulk differences in scores are not caused by changes to in-context examples, and evaluation is pair-wise consistent.

# Results with Text Generation Metrics

Here we highlight the main results obtained using common text generation metrics including ROUGE (Lin 2004), BLEU (Papineni et al. 2002), BLEURT (Sellam, Das, and Parikh 2020), and GLEU (Mutton et al. 2007).

Small fine-tuned LMs outperform few-shot GPT-4 across all generation metrics. On 2K examples (denoted by $( f ) ^ { \dagger }$ ) from the static set, FLAN-T5-large outperforms all models in all metrics. This minor advantage over T5-large may stem from further instruction fine-tuning (our prompt is an instruction). However this advantage over T5 does not extend to

<html><body><table><tr><td></td><td colspan="4">ROUGE</td><td colspan="4"></td><td colspan="4"></td><td colspan="4">BLEURT</td><td colspan="4">GLEU</td></tr><tr><td></td><td>S</td><td>VR</td><td>EE</td><td>AG</td><td>SR</td><td>S</td><td>VR</td><td>EE</td><td>AG</td><td>SR</td><td>S</td><td>VR</td><td>EE</td><td>AG</td><td>SR</td><td>S</td><td>VR</td><td>EE</td><td>AG</td><td>SR</td></tr><tr><td>T5-base (f)</td><td>88.6</td><td>80.2</td><td>86.2</td><td>88.3</td><td>77.0</td><td>81.3</td><td>74.8</td><td>78.2</td><td>80.9</td><td>64.2</td><td>70.5</td><td>67.7</td><td>67.3</td><td>67.4</td><td>51.5</td><td>83.4</td><td>76.1</td><td>80.4</td><td>83.1</td><td>69.2</td></tr><tr><td>FLAN-T5-base (f)</td><td>87.3</td><td>24.4</td><td>84.3</td><td>86.7</td><td>77.7</td><td>79.4</td><td>41.1</td><td>76.0</td><td>78.8</td><td>66.6</td><td>68.9</td><td>18.7</td><td>67.0</td><td>67.9</td><td>56.8</td><td>81.7</td><td>44.2</td><td>78.5</td><td>81.3</td><td>71.0</td></tr><tr><td>T5-large (f)</td><td>89.4</td><td>85.0</td><td>86.8</td><td>89.2</td><td>77.7</td><td>82.8</td><td>79.3</td><td>79.5</td><td>82.5</td><td>66.4</td><td>72.1</td><td>70.8</td><td>68.3</td><td>69.6</td><td>54.1</td><td>84.7</td><td>80.8</td><td>81.5</td><td>84.4</td><td>70.6</td></tr><tr><td>FLAN-T5-large (f)</td><td>90.2</td><td>83.0</td><td>87.1</td><td>89.5</td><td>78.6</td><td>84.6</td><td>78.5</td><td>80.4</td><td>83.5</td><td>68.9</td><td>73.2</td><td>69.0</td><td>68.7</td><td>70.3</td><td>56.1</td><td>86.1</td><td>79.6</td><td>82.1</td><td>85.1</td><td>72.4</td></tr><tr><td>T5-base</td><td>89.5</td><td>82.2</td><td>87.3</td><td>89.9</td><td>79.9</td><td>82.8</td><td>77.2</td><td>81.6</td><td>83.7</td><td>68.8</td><td>70.5</td><td>71.1</td><td>69.6</td><td>70.1</td><td>56.5</td><td>84.4</td><td>78.0</td><td>82.6</td><td>85.3</td><td>72.5</td></tr><tr><td>FLAN-T5-base</td><td>87.0</td><td>25.7</td><td>86.7</td><td>87.8</td><td>78.5</td><td>80.3</td><td>40.4</td><td>81.1</td><td>81.1</td><td>68.5</td><td>67.2</td><td>14.6</td><td>69.0</td><td>66.4</td><td>56.7</td><td>81.9</td><td>42.9</td><td>82.2</td><td>82.9</td><td>71.8</td></tr><tr><td>T5-large</td><td>91.0</td><td>86.2</td><td>87.7</td><td>90.5</td><td>80.6</td><td>85.1</td><td>80.7</td><td>82.4</td><td>84.7</td><td>71.0</td><td>72.5</td><td>71.9</td><td>70.7</td><td>71.8</td><td>59.6</td><td>86.4</td><td>81.7</td><td>83.3</td><td>86.1</td><td>74.1</td></tr><tr><td>FLAN-T5-large</td><td>91.2</td><td>85.1</td><td>87.9</td><td>90.4</td><td>80.7</td><td>86.1</td><td>79.8</td><td>83.1</td><td>84.8</td><td>72.3</td><td>72.9</td><td>71.2</td><td>70.5</td><td>71.4</td><td>61.0</td><td>87.2</td><td>80.6</td><td>83.8</td><td>86.2</td><td>75.0</td></tr><tr><td>GPT-3.5</td><td>80.3</td><td>78.8</td><td>78.8</td><td>80.6</td><td>73.3</td><td>70.8</td><td>70.2</td><td>70.7</td><td>71.4</td><td>64.2</td><td>63.1</td><td>63.9</td><td>62.1</td><td>61.7</td><td>50.9</td><td>73.5</td><td>72.7</td><td>72.9</td><td>74.3</td><td>67.7</td></tr><tr><td>GPT-4</td><td>82.8</td><td>81.6</td><td>80.9</td><td>82.1</td><td>75.6</td><td>72.2</td><td>71.1</td><td>68.3</td><td>70.4</td><td>61.7</td><td>62.9</td><td>64.2</td><td>61.3</td><td>61.8</td><td>50.4</td><td>75.6</td><td>74.4</td><td>72.3</td><td>74.4</td><td>67.2</td></tr><tr><td>LLaMa-2-7B</td><td>34.3</td><td>29.6</td><td>37.6</td><td>36.5</td><td>39.2</td><td>28.6</td><td>24.6</td><td>31.1</td><td>30.3</td><td>29.1</td><td>-18.2</td><td>-25.8</td><td>-14.6</td><td>-15.0</td><td>-13.3</td><td>30.8</td><td>27.1</td><td>34.1</td><td>32.9</td><td>35.1</td></tr><tr><td>Llemma-7B</td><td>75.7</td><td>73.9</td><td>73.0</td><td>74.9</td><td>62.6</td><td>63.6</td><td>63.8</td><td>61.9</td><td>63.4</td><td>52.6</td><td>59.7</td><td>63.4</td><td>59.3</td><td>58.9</td><td>49.9</td><td>67.3</td><td>66.7</td><td>65.6</td><td>67.2</td><td>56.2</td></tr></table></body></html>

Table 3: Evaluation results with both in-distribution static scores (S) and those from the interventions (VR, EE, AG, SR).   
Table 4: Error counts for specific equation-level and derivation-level categories. Syntax refers to the number of equations with unbalanced brackets. Equality counts the number of equations without equality (or inequality) symbols. Repeating is the total number of repeated equations. Redundant is the number of equations where the LHS exactly matches the RHS. Skipped steps and Verbose respectively count the excess or reduced number of equations in the output compared to the reference derivation.   

<html><body><table><tr><td></td><td></td><td></td><td>Syntax Errors</td><td></td><td>SR</td><td></td><td></td><td>Equality Errors</td><td></td><td></td><td></td><td>Repeating Errors</td><td></td><td></td><td></td><td>Redundant Errors</td><td></td><td></td><td></td><td></td><td>Skipped steps</td><td></td><td></td><td></td><td></td><td>Verbose</td><td></td><td></td></tr><tr><td></td><td>S</td><td>VR</td><td>EE</td><td>AG</td><td></td><td></td><td>VR</td><td>AG</td><td>SR</td><td>S</td><td>VR</td><td>EE</td><td>AG</td><td>SR</td><td>S VR</td><td>EE</td><td>AG</td><td>SR</td><td>S</td><td>VR</td><td>EE</td><td>AG</td><td>SR</td><td>S</td><td>VR</td><td>EE</td><td>AG</td><td>SR</td></tr><tr><td>T5-base</td><td>20</td><td>32</td><td>7</td><td>19</td><td>25</td><td></td><td>3</td><td></td><td>0</td><td>7</td><td>36</td><td>35</td><td>8</td><td>2</td><td>1</td><td>18</td><td></td><td>8</td><td>69</td><td>46</td><td>44</td><td>65</td><td>155</td><td>7</td><td>73</td><td>28</td><td>4</td><td>0</td></tr><tr><td>FLAN-T5-base</td><td>2</td><td>43</td><td>4</td><td>3</td><td>1</td><td></td><td>23</td><td>0</td><td>0</td><td>7</td><td>33</td><td>24</td><td>5</td><td>3</td><td>15 2</td><td>25</td><td>0</td><td>10</td><td>95</td><td>196</td><td>64</td><td>92</td><td>168</td><td>5</td><td>33</td><td>22</td><td>4</td><td>0</td></tr><tr><td>T5-Large</td><td>11</td><td>11</td><td>14</td><td>9</td><td>15</td><td></td><td></td><td>0</td><td>0</td><td>5</td><td>28</td><td>33</td><td>8</td><td>3</td><td>13 1</td><td>21</td><td>2</td><td>4</td><td>53</td><td>41</td><td>41</td><td>53</td><td>140</td><td>9</td><td>40</td><td>33</td><td>13</td><td>2</td></tr><tr><td>FLAN-T5-Large</td><td>1</td><td>20</td><td>7</td><td>2</td><td>3</td><td></td><td></td><td>1</td><td>0</td><td>7</td><td>21</td><td>30</td><td>7</td><td>1 3 2</td><td>8</td><td>26</td><td>4</td><td>8</td><td>48</td><td>36</td><td>30</td><td>50</td><td>133</td><td>11</td><td>50</td><td>38</td><td>11</td><td>2</td></tr><tr><td>GPT-3.5</td><td>4</td><td>0</td><td>1</td><td>1</td><td>3</td><td></td><td></td><td>3</td><td>2</td><td>3</td><td>4</td><td></td><td>3</td><td>0</td><td>2</td><td>1</td><td>0</td><td>1</td><td>96</td><td>91</td><td>81</td><td>98</td><td>128</td><td>51</td><td>37</td><td>41</td><td>26</td><td>28</td></tr><tr><td>GPT-4</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>2 0</td><td></td><td>0</td><td>0</td><td>0</td><td>1</td><td></td><td></td><td>0</td><td>1</td><td>1</td><td>0</td><td>5</td><td>112</td><td>105</td><td>134</td><td>118</td><td>154</td><td>7</td><td>20</td><td>13</td><td>8</td><td>11</td></tr><tr><td>LLaMa-2-7B</td><td>6</td><td>3</td><td>5</td><td>6</td><td>9</td><td>8</td><td></td><td>6</td><td>17</td><td>14</td><td>11</td><td>28</td><td></td><td>35 6</td><td>2</td><td>3</td><td>1</td><td>8</td><td>99</td><td>91</td><td>115</td><td>107</td><td>219</td><td>21</td><td>17</td><td>33</td><td>15</td><td>27</td></tr><tr><td>Llemma-7B</td><td>13</td><td>11</td><td>13</td><td>11</td><td>35</td><td>17</td><td>12</td><td>9</td><td>26</td><td>94</td><td>139</td><td>153</td><td>108</td><td>243 1</td><td>11</td><td>3</td><td>1</td><td>16</td><td>114</td><td>110 103</td><td></td><td>134</td><td>110 92</td><td></td><td>125</td><td>120</td><td>92</td><td>181</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

FLAN-T5-base, which scores lower than T5-base in all metrics. This may be due to fine-tuning instability observed in T5 (Asai et al. 2022). We note that despite the success of the fine-tuned models, according to the metrics, we are not suggesting they are more suitable for equational reasoning than GPT, as the other sections of the evaluation reveal.

The scores reported in Tab. 3 (without $( f ) )$ are evaluated on 100 examples from the static set explicitly containing integration and differentiation results in the prompt, in order to fairly examine the effect of the Step Removal intervention which perturbs input by removing these results. The fine-tuned LMs score within 3 units of their previous scores, model rankings are preserved across all metrics, and we assume the GPT/LLaMa scores would report similarly for larger samples. With that said, according to all metrics, all fine-tuned models generally outperform all (5-shot) decoder models. Notably, vanilla LLaMa-2 scores are less than half of those obtained by Llemma, indicating the benefits of Llemma’s fine-tuning on mathematical corpora (Azerbayev et al. 2023). This difference is mirrored in the outof-distribution scores.

# Results with Template-based Error Detection

In this section, we count the rate of different categories of errors by extracting equations from a model’s derivation into an ordered list and adopting a set of metrics using both comparative and reference-free methods.

We consider 6 error categories in total (Tab. 4) and sum the per-derivation error counts over all examples. For example, determining whether a model has either Skipped steps or is too Verbose (not necessarily “errors”) occurs by comparing the length of the generated derivation with that of the ground truth reference. The number of Repeating equations is determined by counting the number of uniquely generated equations in the output derivation (reference-free). The number of Equality errors per derivation is calculated considering the number of equations that do not contain the $\stackrel { 6 6 } { = } \stackrel { 7 }$ token.

Fine-tuned FLAN-T5-Large and few-shot GPT-4 obtain the lowest rate of in-distribution errors. Across the 6 error categories, both FLAN-T5-Large and GPT-4 possess the least Syntax and Equality errors. Results then diverge as FLAN-T5-Large repeats several equations (Repeating) and outputs a couple of equations where the LHS is an exact string match with the RHS (Redundant), whereas GPT-4 skips around twice as many steps as the largest fine-tuned models. This supports the claim that we are generally training models at a level of mathematical granularity below that of the data used to train GPT-4.

Model rankings and perturbation difficulty based on template-based error count disagree with those determined by generation metrics. At the top of the model rankings, on both the in-distribution and perturbed test sets, Tab. 4 shows that GPT-4 and GPT-3.5 generally leapfrog the fine-tuned models in terms of low total error count, which disagrees with the scores obtained by the generation metrics. The LLaMa-based models remain at the bottom.

In terms of perturbation difficulty, SR (Step Removal) is no longer the most challenging perturbation for fine-tuned models by error count, yet remains difficult for all other models. From this, FLAN-T5-Large does not outperform GPT-4 or GPT-3.5 on out-of-distribution examples, but it generally does outperform LLaMa-2 and Llemma. This suggests that for the fine-tuned models, certain interventions (such as SR) correspond to a clearly identifiable distribution of mathematical error types, such that the intervention may be inferred from the results. We discuss the implications of this after the manual evaluation.

Table 5: Equational reasoning accuracy $\%$ from human evaluation.   

<html><body><table><tr><td></td><td>S</td><td>VR</td><td>EE</td><td>AG</td><td>SR</td></tr><tr><td>GPT-4</td><td>98</td><td>96</td><td>92</td><td>100</td><td>80</td></tr><tr><td>FLAN-T5-Large</td><td>98</td><td>68</td><td>62</td><td>92</td><td>76</td></tr><tr><td>Llemma-7B</td><td>70</td><td>70</td><td>64</td><td>76</td><td>36</td></tr></table></body></html>

# Human Evaluation of LLM Derivation Quality

To recap, the generation metrics return scores and performance rankings via n-gram-based similarity measures between model derivations and ground truth references. In parallel, the error detection returns scores via a combination of reference-based and reference-free surface-level checks, that pick up on basic mathematical errors such as imbalanced brackets, missing equality signs, and repeating equations. What is lacking is an assessment of models’ underlying operational reasoning that is largely independent of surface-level checks, and should be as close to a reference-free evaluation as possible within the scope of the task. We aim to provide such analysis in this section. Our approach to determining whether a model’s derivation is coherent is as follows. A derivation:

(1.) must not include any equations that are inconsistent   
(and can not be made consistent by substituting a number for   
a variable, e.g., $\alpha = 1$ ). (2.) must not include malformed equations with the exclu  
sion of minor typos. (3.) must include the exact goal equation, as this is always   
given in the prompt. (4.) must apply operators correctly and in the correct order   
where operators are non-commutative. (5.) may skip numerous steps, even premises, so long as   
a path may be reasonably derived between consistent equa  
tions. (6.) may include irrelevant (but consistent) equations that   
do not contribute to the core path linking premises to the goal   
equation. (7.) may repeat some equations.   
The above marking scheme provides a very lenient frame  
work which prioritises the consistency and operation-wise

correctness of equations.

Sampling derivations. The results in Tab. 5 were determined from a sample of 50 static derivations per model, with 4 corresponding perturbed derivations (VR, EE, AG, SR), totalling 250 per model. The 50 static derivations are sampled by ensuring the average ROUGE, BLEU, BLEURT, and GLEU scores over the sample aligns with that model’s static score (S) in Tab. 3 to within 0.1 units. For instance,

GPT-4’s sample averages scores of 82.8, 72.2, 62.9, and 75.6, and the perturbed derivations are based on the static examples from this selection.

We select derivations which are output by the best finetuned, GPT, and open-source models, totalling 750 examples. We convert equations into a easily readable format rendered in LaTeX, and either manually delete equations or fix any minor typos causing compilation errors. This document is available here.3

Fine-tuned FLAN-T5-Large and GPT-4 are matched indistribution in terms of equation consistency and coherent use of operators. Tab. 5 illustrates how the fine-tuned and GPT models scored $98 \%$ accuracy according to the lenient marking scheme. GPT’s single incoherent derivation involved adding a variable to both sides of an equation, then later integrating and forcing that variable to be the constant of integration (breaking rule (2.)). FLAN-T5’s incoherent tdhearti $\begin{array} { r } { \frac { d \theta } { d q } = ( \frac { d \theta } { d q } ) ^ { q } } \end{array}$ v,ewdhaicsheqisuternuce onfleyqifu $q = 1$ (wbhriecahkinmgprliuelde (2.)).

Accounting for numerous surface-level error checks approximates human evaluation. Despite the fact that one evaluation scheme focuses on equation consistency, while the other compares surface-level errors, the respective manual and template-based error results agree that, in-distribution, GPT-4 and FLAN-T5-Large are tied (with Llemma significantly underperforming). Regarding out-of-distribution evaluation, both manual and error-based scores agree that the fine-tuned models are less capable than all generation metrics suggest. In terms of intervention difficulty, both agree that SR (Step Removal) is particularly challenging while AG (Alternative Goal) is the least, and both schemes agree that the fine-tuned models are less affected by SR. This is not reflected by any of the generation metrics.

Given this alignment between the manual evaluation (Tab. 5) and the template-based error detection (Tab. 4) spanning only 6 error types, and that more mathematically capable language models are less likely to hallucinate syntax errors and related trivially detectable artefacts, together this suggests that by accounting for a large $( > > 6 )$ ) number of surface-level errors, we can approximate human evaluation of LLM equational reasoning, at least more faithfully than many canonical generation metrics. This can be achieved via the weighted average of counts over all surface-level categories, where the category weights are empirically determined through comparison with rankings based on human evaluation.

# Controlling Equational Reasoning in LLMs

Supported by the manual and error-based evaluations, interventions are intrinsically linked to certain LLM hallucinations with varying degrees of association.

We start defining such relationships by considering the rate of hallucination types $R ( i , m , t )$ corresponding to an

![](images/d099b592d8619a9b2544f5f715c757f36e11684c0db35613ba7bd1a10a62e309.jpg)  
Figure 3: The average distributional footprint left by certain interventions.

intervention $i$ , error type $t$ , model $m$ , and the static rate $S ( m , t )$ from Tab. 4. For instance, Fig. 3 is characterised by:

$$
\delta ( i , t ) = \mathcal { N } \sum _ { m } \left( R ( i , m , t ) - S ( m , t ) \right)
$$

where $\mathcal { N }$ is a normalisation factor (reciprocal number of models considered). Hence $\delta ( i , t )$ is the average error count for intervention $i \in \{ \mathbf { V R } , \mathbf { E E } , \mathbf { A G } , \mathbf { S R } \}$ of type $t \in$ Syntax, Equality, $\left. \dots \right\}$ . If $\delta < 0$ , then the intervention (on average) reduced the rate of hallucinations of that type across the models, and vice versa.

Furthermore, certain classes of interventions (e.g., injection of random noise, random token deletion) may depend on a practically continuous variable, $s$ , that predictably varies the rate of certain errors. Hence, with some loss of information, intervention $i$ may be represented as a vector $\mathbf { x } _ { i } ( s ) ~ = ~ \left( \mathbb { E } _ { t } [ \delta ( i , t , s ) ] \right.$ , $\sigma _ { t } ( \delta \dot { ( \iota , \iota , \boldsymbol { s } ) } ) , \dots )$ , where $\mathbb { E } _ { t } [ \delta ( i , t , s ) ]$ and $\sigma _ { t } ( \delta ( i , t ) )$ (etc.) are the expectation value and standard deviation of $\delta$ over the error types.

To find $s$ such that intervention $i$ likely improves the output quality over most hallucination types at that strength, we can write

$$
\mathbb { E } _ { t } [ \delta ( i , t , s ) ] + \varepsilon \sigma _ { t } \big ( \delta ( i , t , s ) \big ) < 0
$$

where large $\varepsilon$ ensures that $\delta ( i , t , s ) ~ < ~ 0$ across a greater number of hallucination types $t$ .

The clear error distributions associated with each intervention in Fig. 3 (characterised by Eq. 1) are averaged over all evaluated models, but most closely align with the fine-tuned T5 models. For this class of approaches, the interventions have a distinct effect on specific surface-level errors such as the rate of repeating equations or incorrect syntax. If these distributions $\mathbf { x } _ { i } ( s )$ may be further controlled by some variable $s$ , we can define conditions for reducing surface-level error rates (e.g., Eq. 2), which correlates with improved derivation quality according to human evaluation.

# Related Work

Our focus is evaluating and controlling the LLMbased (Brown et al. 2020; Ahmed and Devanbu 2022; Song et al. 2022; Ge et al. 2023; Hu et al. 2023; Yang et al. 2023; Dubey et al. 2024; Meta 2024) generation of informal mathematical reasoning that resembles step-wise detailed equation derivations. While we focus on equation generation, mathematical generation exists in various forms, and can be clustered into two main categories: approaches that consider formal languages, and those that consider informal mathematical natural language (Meadows and Freitas 2023; Lu et al. 2022; Zhong, Yang, and Lin 2022). In the formal case, GPTj (Polu and Sutskever 2020; Polu et al. 2022), LISA (Jiang et al. 2021), and Baldur (First et al. 2023) focus on Metamath and Isabelle/HOL proofs. For generation involving informal reasoning, an approach based on OpenAI’s Codex (Chen et al. 2021; Drori et al. 2022) translates university-level problems into executable code, and generates solution explanations. Minerva (Lewkowycz et al. 2022) is a PaLM (Chowdhery et al. 2022) model trained on a large corpus of mathematical text, and solves university-level problems in applied math, outputting solutions in the form of mathematical natural language. NaturalProver (Welleck et al. 2022) generates similar solutions to proofs from a curated dataset (Welleck et al. 2021), and is most similar to our present work. However, our approach differs in a number of ways. Firstly, we focus exclusively on the generation of equational chains (in contrast to the inclusion of natural language statements). Our prompts and derivations are procedurally generated valid derivations in LaTeX, and many examples are guaranteed to include reasoning which is out-of-distribution with respect to other datasets, while containing up to 10 equations with wildcard symbols (Zanibbi et al. 2016). Lastly, our use of symbolic interventions follows from a previous approach (Meadows et al. 2024), and we describe specific improvements in a later section.

# Conclusion

To control and assess the fine-grained equation derivation capabilities of LLMs via prompt interventions, we first construct a dataset comprising $3 0 \mathrm { k }$ mathematically fine-grained prompt-derivation pairs using a symbolic data generation framework. We find fine-tuned models match or surpass the in-distribution performance of few-shot GPT-4, despite a difference in parameter count of up to 3 orders of magnitude.

However, while all generation metrics suggest the finetuned models also outperform few-shot GPT-4 on perturbed data, the manual and template-based error detection methods both strongly disagree with reference-based metrics, suggesting the latter lead to inappropriate representations of the relative effect of interventions and model capabilities. This strong alignment between human and error-based analysis suggests that extensive human evaluation can be approximated by accounting for numerous categories of surface-level errors.

We use these results to describe how a fundamental underlying mechanism relating distribution shifts to certain surface-level errors may be leveraged by varying the type and strength of prompt interventions to mitigate hallucination rates, which in turn facilitates control over the quality of LLM-based equational reasoning.

Given a sufficiently large number of detectable mathematical hallucinations, an intervention with a variable strength that predictably controls the rate of certain errors, and an appropriate statistical condition, we can improve the quality of LLM reasoning post-training by experimentally determining an appropriate intervention strength.