# CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification

Yuchen Tian1\*, Weixiang $\mathbf { Y a n } ^ { 2 * \dagger }$ , Qian Yang3,4, Xuandong Zhao5 Qian Chen6, Wen Wang6, Ziyang $\mathbf { L u o } ^ { 1 }$ , Lei $\mathbf { M } \mathbf { a } ^ { 7 , 8 \dagger }$ , Dawn Song5†

1Hong Kong Baptist University   
2University of California, Santa Barbara   
3Mila - Qu´ebec AI Institute   
4Universite´ de Montre´al   
5University of California, Berkeley   
6Alibaba Group   
7The University of Tokyo   
8University of Alberta   
yctian@comp.hkbu.edu.hk, weixiangyan@ucsb.edu

# Abstract

Large Language Models (LLMs) have made significant progress in code generation, offering developers groundbreaking automated programming support. However, LLMs often generate code that is syntactically correct and even semantically plausible, but may not execute as expected or fulfill specified requirements. This phenomenon of hallucinations in the code domain has not been systematically explored. To advance the community’s understanding and research on this issue, we introduce the concept of code hallucinations and propose a classification method for code hallucination based on execution verification. We categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, with each category further divided into different subcategories to understand and address the unique challenges faced by LLMs in code generation with finer granularity. Additionally, we present a dynamic detection algorithm called CodeHalu designed to detect and quantify code hallucinations. We also introduce the CodeHaluEval benchmark, which includes 8,883 samples from 699 tasks, to systematically and quantitatively evaluate code hallucinations. By evaluating 17 popular LLMs using this benchmark, we reveal significant differences in their accuracy and reliability in code generation, offering detailed insights for further improving the code generation capabilities of LLMs. The CodeHalu benchmark and code are publicly available at https://github.com/yuchen814/CodeHalu.

# Introduction

Deep neural networks often generate erroneous information that contradicts the original content, cannot be verified, or conflicts with real-world knowledge. This phenomenon, commonly known as model hallucination, attracts widespread attention in the fields of natural language processing and multimodal learning (Ji et al. 2023; Zhang et al. 2023; Liu et al. 2024), with the community actively exploring methods to mitigate hallucinations (Peng et al. 2023; Elaraby et al. 2023; Liu et al. 2023). However, the issue of model hallucination in the code generation domain remains unexplored.

Conducting a thorough and dedicated study on code hallucinations is crucial for improving the quality of code generated by LLMs. Firstly, the purpose of code is to solve problems, and its value is realized only when the code executes successfully and passes tests (Chen et al. 2021; Austin et al. 2021; Yan et al. 2023). This necessitates that the generated code not only maintain strict logic and precision but also undergoes execution verification to confirm its correctness. Therefore, the practical use and verification of code differ significantly from Natural Language(NL) texts, meaning we cannot directly apply the definitions and methods used for NL hallucinations to code. Secondly, code snippets containing hallucinations may trigger runtime errors, or exhibit functional defects, which hinder the reliable deployment of LLMs in automated software development scenarios. Lastly, by exploring and verifying code hallucinations in a targeted manner, we can effectively uncover their causes and contribute to improving the architecture and training methods of LLMs.

To fill this gap, we define the concept of code hallucination in LLMs, based on the unique purpose and function of the code. Code hallucinations refer to the phenomenon where code generated by LLMs is syntactically correct or even semantically plausible but ultimately cannot execute as expected or fails to meet specified requirements.1 This phenomenon typically arises from various factors, such as errors or outdated information in the training data, an inadequate grasp of the syntax rules and programming paradigms of the programming languages, and limitations in the logical processing capabilities of the models. In contrast to previous methods that passively explore hallucinations in NLP through a Q&A framework or by prompting LLMs to generate hallucinated answers (Lin, Hilton, and Evans 2021; Cheng et al. 2023), we employ an active strategy to detect hallucinations during the code generation process by LLMs. This approach is crucial as the ultimate goal of the generated code is to execute correctly and fulfill specific tasks.

To detect and quantify hallucinations in LLMs during code generation, we develop a dynamic detection algorithm named CodeHalu. This algorithm employs a statistical induction method based on execution validation to identify specific patterns that frequently occur in code generated by multiple LLMs, such as error types, syntax interruptions, or unexpected execution results. When a pattern consistently appears across multiple LLMs, it is recognized as a common code hallucination. Based on the CodeHalu algorithm 1, we employ an execution-based validation approach for hallucination detection, combined with a two-stage heuristic identification method. By conducting statistical quantification on 17 mainstream LLMs, we categorize code hallucinations into four major categories: Mapping, Naming, Resource, and Logical Hallucinations. These categories are further divided into eight subcategories, as illustrated in Figure 1. We analyze 17 LLMs for cross-task occurrence rates in eight categories of code hallucinations. The low average rate of $2 . 0 4 \%$ confirms the independence and validity of our classification.

To effectively measure and compare code hallucinations across different LLMs, we introduce an evaluation benchmark named CodeHaluEval, which is based on the incidence rate of hallucinations. It follows a structured process of Validation-Identification-Construction as shown in Figure 4 to detect and evaluate code hallucinations in LLMs, closely tied to real-world programming scenarios, ensuring that the generated code correctly achieves the expected functionality. CodeHaluEval encompasses eight types of code hallucinations as illustrated in Figure 1, covering 699 distinct tasks and corresponding to 8,883 samples. Additionally, we systematically evaluate 17 mainstream LLMs to reveal the distribution and behavior patterns of their code hallucinations. We also analyze the potential causes of various code hallucinations, providing detailed insights for further improving the code generation capabilities of LLMs. Our contributions can be summarized as follows:

• Code Hallucination: We introduce the concept of code hallucination in LLMs and propose an execution-based verification method to define code hallucination, addressing a gap in the research on hallucination within the code generation domain. • CodeHalu Algorithm: We develop a dynamic detection algorithm, CodeHalu, to identify and quantify the types of hallucinations that occur in LLMs during code generation. We categorize code hallucinations into four main categories based on a two-stage heuristic approach, discussing their theoretical implications and potential causes. • CodeHaluEval Benchmark: We propose the CodeHaluEval benchmark to systematically evaluate 17 popular LLMs, revealing the distribution and patterns of code hallucinations across these models, and providing insights for developing more robust and reliable LLMs.

# Related Work

# Hallucination

In the field of NLP, hallucination is initially defined as the phenomenon where the text generated by a model is fluent and natural but either lacks substantive meaning or is inconsistent with the provided source content (Ji et al. 2023). Recently, Zhang et al. (2023) standardize the definition of NL hallucinations in LLMs into three categories: input-conflicting hallucinations, where the content generated by LLMs diverges from the user’s input; context-conflicting hallucinations, in which the generated content contradicts previously generated content; and fact-conflicting hallucinations, where the generated content conflicts with established world knowledge. These hallucinations are attributed to various factors, such as poor-quality data samples in the training dataset or the use of sampling algorithms with high uncertainty.

In the multimodal domain, Zhai et al. (2023) classify types of hallucinations in image-to-text scenarios, such as image captioning and visual question answering. They define three main types of hallucinations: object existence hallucinations, object attribute hallucinations, and object relationship hallucinations. In text-to-image scenarios, such as image generation, hallucinations refer to the creation of factually incorrect details by the image generation model in response to the given text input. Huang et al. (2024) introduce VHTest, which evaluates hallucinations across eight dimensions in images, including the existence, shape, color, orientation, OCR, size, position, and counting of visual objects. In text-tovideo scenarios, such as video generation, Chu et al. (2024) define three types of hallucinations: prompt consistency hallucinations, static hallucinations, and dynamic hallucinations. Although the issue of hallucinations receives extensive attention in NLP and multimodal domains, it remains unexplored in the code domain. Therefore, we propose CodeHalu to systematically define, identify, classify, and quantify code hallucinations in LLMs.

# Existing Coding Benchmarks

In recent years, numerous studies focus on evaluating the capability of LLMs to handle various programming tasks. Among these, the HumanEval (Chen et al. 2021), includes 164 Python programming problems, each with an average of 6.7 unit tests. The APPS (Hendrycks et al. 2021) benchmark presents more challenging programming questions, with each problem averaging 293.2 words in length. CodeScope (Yan et al. 2023) covers 43 programming languages and 8 coding tasks to evaluate LLMs in code understanding and generation. MMCode (Li et al. 2024) is designed to evaluate the programming capability of code models in multimodal scenarios. SWE-bench (Jimenez et al. 2023) evaluates the ability of LLMs to modify code repositories and solve problems with a complexity level comparable to what human programmers encounter. Overall, existing code benchmarks focus on evaluating the performance of LLMs on various programming tasks. However, there is still a lack of effective methods to

Code Hallucination HMappinson HaNamintion Hallucination √ √ Computational Physical Data Compliance Stracture Acess External Source Hallentition HBloundarion Halnsiraitn Logic Deviation Logic Breakdown   
A vague understanding Misinterpret the Memory-related issues Biased memories Underestimate resource Blur recognition of Lack sufficient Struggle of the data types data structures of or conflicts with facts or lack sufficient consumption during numerical calculation logical consideration to interpret or   
and parameter values the objects concerning external understanding of data processing limits and iteration or maintain a of the objects knowledge sources the context operations endpoints during data contradicts the continuous processing operations intended instructions understanding of context

detect and quantify potential hallucinations that may occur in code generation. Therefore, we propose CodeHaluEval to detect and quantify code hallucinations in LLMs.

# Code Hallucination

As a tool, code aims to achieve specific objectives through correct execution. This inherent characteristic motivates our use of an execution-based verification method to explore and identify code hallucinations. In this section, we define the concept of code hallucination and distinguish it from code errors, clarifying the relationship and differences between these two phenomena.

Definition 1 (Code Hallucinations). Code hallucinations refer to the code generated by large language models that is syntactically correct or even semantically plausible, but ultimately cannot execute as expected or fails to meet specified requirements.

Definition 2 (Code Errors). Code errors refer to issues in a program that cause it to stop executing.

Remark 3 (Code Hallucinations vs. Code Errors). In multiple domains, existing work (Ji et al. 2023; Zhang et al. 2023; Huang et al. 2024; Zhai et al. 2023; Chu et al. 2024) often equates errors with hallucinations, or considers errors as a specific subset of hallucinations. We follow this perspective and regard code errors as a specific subset of code hallucinations. In other words, errors manifest as a form of hallucination, but not all hallucinations can be adequately expressed through errors. Figure 2 illustrates the distinction between code errors and code hallucinations. The code on the left exhibits a typical code error due to the use of an undefined variable “N”, resulting in a NameError. On the right, the code repeatedly calls the same function due to a logical collapse during generation, eventually exceeding the maximum token limit and leading to a SyntaxError. However, the underlying issue is a latent logical hallucination, rather than the observed syntactic error.

Overall, although there is some slightly overlap between code hallucinations and code errors, their meanings, research objects, and scopes differ significantly. Code hallucinations focus on why the model produces hallucinations, while code errors focus on what grammatical rules the code violates. Code errors form a proper subset of code hallucinations, while code hallucinations encompass a broader range of po

# Algorithm 1: CodeHalu Algorithm

<html><body><table><tr><td></td><td>Input:Code Generation Dataset α,Language models π</td></tr><tr><td>Output: HaluTypes £ 1:Let ←empty list</td><td></td></tr><tr><td>2:forαi,wherei∈{1,...,k}do 3:</td><td></td></tr><tr><td>GC²i←πj(Gl,Q）</td><td>for πj,wherej ∈{1,...,m} do</td></tr><tr><td>4:</td><td></td></tr><tr><td>5:</td><td>if GC3i is stuttering, infinite enumeration, or gibberish</td></tr><tr><td>6:</td><td>←mUState(GCχ)</td></tr><tr><td>7: 8:</td><td>fortn,wheren∈{1,...,N}do</td></tr><tr><td>9:</td><td>if Execute(GC² (tn))</td></tr><tr><td>10:</td><td></td></tr><tr><td></td><td>ERi(tn) ←Execute(GCα𝑖 (tn))</td></tr><tr><td>11:</td><td>if ERi(tn)≠optn</td></tr><tr><td>12:</td><td>m←mUState(GC²）</td></tr><tr><td></td><td>13:Aggregate and count frequencies of unique State(GC@i)in &</td></tr></table></body></html>

tential logical and functional issues, representing a finergrained and more comprehensive evaluation of the overall quality and functionality of the code.

# CodeHalu Algorithm

In this section, we introduce a dynamic detection algorithm called CodeHalu, which detects and quantifies hallucinations in LLMs in code generation. CodeHalu operates on the assumption ASS: if a specific pattern frequently appears in the code generated by multiple LLMs, it is considered a common code hallucination. These patterns include error types, syntax interruptions, logical collapse, or unexpected execution results.

Consider a dataset $\alpha$ contains $k$ samples, where each sample $\alpha _ { i }$ consists of a problem description $Q$ and a series of test cases $t _ { 1 } , t _ { 2 } , \ldots , t _ { n }$ . Each test case $t _ { n }$ includes an input ip and the corresponding expected output op. Notably, following previous work (Li et al. 2023b; Yan et al. 2023), we integrate resource (time and memory) constraints into the code generation instructions. As shown in Algorithm 1, we use a $\pi _ { j }$ to generate a code solution $\mathsf { G C } _ { j } ^ { \alpha _ { i } }$ for each sample $\alpha _ { i }$ based on the code generation instruction $\mathsf { G l } _ { \mathrm { j } }$ and the problem description $\mathsf { Q }$ . If $\mathsf { G C } _ { j } ^ { \alpha _ { i } }$ exhibits any of the states such as stuttering, infinite loops, or gibberish, we include it in $\xi$ .

To test the potential hallucinations of $\mathsf { G C } _ { j } ^ { \alpha _ { i } }$ at a finegrained level, we execute all test cases of sample $\alpha _ { i }$ one

:#QUESTION:   
:This problem simulates a battle on an $\mathbf { N } { \times } \mathbf { N }$ grid where Zerglings, controlled by   
:two players,attack or move each turn based on proximity to enemies and specific   
:rules for movement and attack priority.The simulation runs for a specified number   
:of turns,and the task is to output the final grid configuration   
:#CODE ERROR :#CODEHALLUCINATION   
:players $= 2$ importmath   
:map=[] def get_distance(x1,y1,x2,y2):   
fori =ang(N): N is notdefined for j in range(N): return [7,6,5,4,3,2,1,0] row.append('1') return [0,1,2,3,4,5,6,7] row.append('o') map.append(row) return[0, 1, 2, Due to stuttering exceeding the   
:#Input #Input maximum token limit.   
:2\n0 0\no o\n1.n.. \no\n 2\n0 0\n0 0\n1.\n..\no\n   
:# Expected Output #Expected Output   
:1.n.\n : 1\n.\In   
:#Execution Output #Execution Output   
:NameError: name N'is not defined :SyntaxError: unexpected EOF while parsing:

by one to verify whether it successfully executes and meets the expected functionality. We record the actual execution result $\mathsf { \bar { E } R } _ { j } ^ { \alpha _ { i } } ( t _ { n } )$ of the code under each test case $t _ { n }$ and extensively test each sample $\alpha _ { i }$ across more than $1 5 \pi$ to obtain statistically-based inductive results. If the code execution fails or does not meet the expected results, we record it in $\xi$ .

Finally, we merge the identical states State $\big ( \mathsf { G C } _ { j } ^ { \alpha _ { i } } \big )$ detected by CodeHalu and calculate their occurrence frequencies. According to assumption ASS, $\xi$ can be represented as $[ ( \xi _ { 1 } , P _ { 1 } )$ , $( \xi _ { 2 } , P _ { 2 } )$ , . . . , $( \xi _ { o } , P _ { o } ) ]$ , where $\xi _ { o }$ denotes the $o ^ { t h }$ type of code hallucination, and $P _ { o }$ indicates its corresponding frequency. Code hallucinations come from four perspectives: errors, syntax, logic, and execution results. Additionally, CodeHalu is language-agnostic and can dynamically adjust to match various programming scenarios depending on the programming language.

# Code Hallucinations Classification

In this section, we analyze the hallucination states detected by the CodeHalu algorithm, classify and define four main types of hallucinations, and discuss the rationale behind the classification method.

According to the TIOBE Index2, a metric of programming language popularity, we primarily investigate code hallucinations within the Python. By applying the CodeHalu algorithm on the complex APPS dataset (Hendrycks et al. 2021) and 17 widely-used LLMs, we identify and validate 18 types of hallucination states that violate human expectations during the code generation, including inconsistent code context, ambiguous logic and data flow, conflicting intentions, among others. Using the two-stage heuristic classification method introduced in Remark 8, we categorize code hallucinations into four main types based on the nature and origin of these phenomena: mapping hallucinations, naming hallucinations, resource hallucinations, and logical hallucinations, as illustrated in Figure 1.

Definition 4 (Mapping Hallucinations). Mapping Hallucinations refer to the ambiguity and confusion that occur in LLMs perception and mapping of data types, values, and structures during data operations. This phenomenon is further divided into two sub-categories: data compliance hallucinations and structure access hallucinations.

Data compliance hallucinations occur when LLMs have a vague understanding of the data types and parameter values of the objects being manipulated, resulting in generated code that attempts to perform type-mismatched or rule-violating operations.

Structure access hallucinations occur when LLMs misinterpret the data structures of the objects being manipulated, leading to generated code that attempts to access non-existent array indices or dictionary keys.

Definition 5 (Naming Hallucinations). Naming Hallucinations refer to the memory-related issues and factual inaccuracies exhibited by LLMs when handling the naming, scope, and existence of variables, attributes, and modules. This phenomenon is further divided into two subcategories: identity hallucinations and external source hallucinations.

Identity hallucinations occur when LLMs possess biased memories or lack sufficient understanding of the context, leading to generated code that references undefined variables, accesses non-existent object properties, or uses unassigned variables in local scopes.

External source hallucinations occur when LLMs exhibit significant memory-related issues or obvious conflicts with facts concerning external knowledge sources, resulting in generated code that attempts to import non-existent modules or fails to correctly load modules from other paths.

Definition 6 (Resource Hallucinations). Resource Hallucinations occur when LLMs lack an adequate perception and prediction of resource consumption and control flow of the generated code during execution. This phenomenon is further divided into physical constraint hallucinations and computational boundary hallucinations.

Physical constraint hallucinations arise when LLMs underestimate resource consumption during data processing operations, causing code failure due to exceeding memory capacity, stack depth, or other physical constraints.

Computational boundary hallucinations occur when LLMs blur recognition of numerical calculation limits and iteration endpoints during data processing operations, causing code failure due to numerical overflow or improper iteration control.

Definition 7 (Logic Hallucinations). Logic Hallucinations refer to the discrepancies between the expected results and the actual outcomes after executing the code generated by LLMs, or outputs with low semantic density or even complete chaos. This phenomenon is further divided into logic deviation and logic breakdown.

Logic deviation occurs when LLMs generate code that lacks sufficient logical consideration or contradicts the intended instructions. While this hallucination may not cause errors during execution, logical deviations or confusion result in outcomes that fail to meet the expected results.

Logic breakdown occurs when LLMs struggle to interpret or maintain a continuous understanding of context during code generation. This indicates that the models may lose direction while generating code, making it difficult to maintain strict consistency of contextual information.

Remark 8 (Discussion of Rationality). To ensure the rationality and effectiveness of our code hallucination classification method, we conduct in-depth analyses.

Firstly, we extensively reference classification methods for hallucinations in the fields of NLP and multimodal research (Zhang et al. 2023; Ji et al. 2023; Huang et al. 2024; Zhai et al. 2023; Chu et al. 2024), as well as methods for classifying code errors and vulnerabilities in software engineering (Pan et al. 2023; Wang et al. 2024; Huang et al. 2023). We adopt a two-stage heuristic classification strategy. Initially, team members independently review failed code cases and develop preliminary classification frameworks; then, we reach a consensus through collaborative discussions. This widely used approach ensures the adaptability and accuracy of our framework, enabling a systematic understanding of code hallucinations in LLMs.

Secondly, we analyze the cross-task occurrence rates of each model across eight categories of code hallucinations. The results show that the average cross-task occurrence rate for these categories is only $2 . 0 4 \%$ , confirming the independence and rationality of our classification. For the Gemma-7B model, which exhibits the most severe hallucinations in Table 2, only $1 . 0 7 \%$ of task samples show cross-task hallucinations, as illustrated in Figure 3.

Lastly, we conduct an empirical investigation of our classification results and design a questionnaire to evaluate the rationality of our method . The survey receives 23 responses, and after excluding seven respondents with less than three years of experience, we analyzed 16 valid responses. The survey results indicate a rationality rating of $9 1 . 0 8 \%$ for our classification method, further supporting its validity.

# Cause Analysis of Code Hallucinations

In this section, we explore the potential causes of various hallucinations generated by LLMs, aiming to provide valuable insights for optimizing training data, training methods, model architecture, and alignment strategies.

Mapping hallucinations primarily stem from the model’s misunderstanding of data types and structures. This phenomenon arises due to several factors: (1) The model generates code based on tokens, lacking insight into higher-level structures such as statements and functions (Yang, Liu, and Yin 2021); (2) When dealing with long-distance data dependencies, especially within complex code blocks, the model fails to continuously track the structure and state of variables, overly relying on local information while neglecting the importance of the overall context (Zhang et al. 2024); (3) The model does not explicitly perform type checking and structure matching during code generation, lacking static checking and error correction mechanisms.

![](images/51fcc17e761379ef66f445f4a66eec70784cf8cedf4378e962093d4d19bfd66a.jpg)  
Figure 3: The diagram illustrates the intersection of various hallucinations in Gemma-7B during the CodeHaluEval. The bar chart at the top shows the frequency of each intersection, while the bar chart on the left indicates the frequency of each type of hallucination. The connecting lines represent the cooccurrence patterns between different hallucinations.

Naming hallucinations reflect the limitations of models in tracking information and utilizing external knowledge. This issue arises from several factors: (1) Token-based feature representation makes it difficult to accurately model long-distance dependencies, leading to model misjudgments regarding variable scope, lifecycle, and visibility (Xu et al. 2020); (2) The code generation process lacks consistency checks for identifiers and does not perform global tracking of variable definitions and usage; (3) Knowledge of external libraries is not effectively and timely integrated into the model’s knowledge system, making it difficult for the model to accurately understand the names, functions, and invocation methods of libraries (Jesse et al. 2023).

Resource hallucinations highlight the model’s lack of deep understanding of code execution mechanisms and physical constraints. These issues arise from several factors: (1) The training data lacks information related to resource consumption and performance optimization, making it difficult for the model to learn about complexity analysis and resource estimation; (2) As the model generates code based on probabilities, it lacks a module for calculating and estimating the resource consumption of the generated code, making it unable to simulate the real-world operating environment and resource limits; (3) During the model training process, the focus is usually on the correctness of the code’s functionality, often overlooking its complexity and resource constraints in actual execution environments.

Logic hallucinations reveal the model’s deficiencies in semantic understanding and reasoning about code. This issue arises due to several factors: (1) The model mainly relies on pattern matching and statistical rules to generate code, lacking a fundamental understanding of symbolic systems and rigorous verification of program logic; (2) The training data is often not rigorously verified for accuracy and may contain code with very similar functions. Since models some

Test Case Excution Result #1 X 山 Generated Code by LLM #1 Test Input: Output:77\n Instruction n LLMs n $\mathbf { \tau } =$ int(inputO) 3\n77 77 77n77 77n77n Error : Wrong Logic Question: 福 g Generated Code byLLM #2 Expected Output: Execute & Validate CExcuton Resut 2y if frequency > { A and B are preparing for CodeHalu programming contests... … Excution Result #1 X Error : IndexError A Sample from APPS Generated Code byLLM #3

Table 1: Detailed statistics of categories, and quantities in CodeHaluEval benchmark.   

<html><body><table><tr><td>Category</td><td>#Tasks</td><td>#Samples</td><td>Sub-Category</td><td>#Tasks</td><td>#Samples</td></tr><tr><td rowspan="2">Mapping</td><td rowspan="2">262</td><td rowspan="2">2.288</td><td>Data Compliance</td><td>110</td><td>941</td></tr><tr><td>Structure Access</td><td>152</td><td>1347</td></tr><tr><td rowspan="2">Naming</td><td rowspan="2">157</td><td rowspan="2">1,853</td><td>Identity</td><td>115</td><td>1323</td></tr><tr><td>External Source</td><td>42</td><td>530</td></tr><tr><td rowspan="2">Resource</td><td rowspan="2">107</td><td rowspan="2">1,130</td><td>Physical Constraint</td><td>47</td><td>491</td></tr><tr><td>Computational Boundary</td><td>60</td><td>639</td></tr><tr><td rowspan="2">Logic</td><td rowspan="2">173</td><td rowspan="2">3.612</td><td>Logic Deviation</td><td>119</td><td>2.443</td></tr><tr><td>Logic Breakdown</td><td>54</td><td>1,169</td></tr></table></body></html>

times imitate and memorize previous examples (Yan and Li 2022), this can result in the model directly replicating similar logic in the code or even learning incorrect logic from the outset; (3) When the model generates code, repetition at the line level has a self-reinforcing effect, causing the model to become increasingly confident in the code it generates, which may lead to a stuttering phenomenon (Xu et al. 2022).

# The CodeHaluEval Benchmark

We construct the CodeHaluEval benchmark, a unified evaluation method for comparing various types and frequencies of hallucinations in code generation across different LLMs. We develop the CodeHaluEval benchmark based on the APPS testing set, following a structured process of ValidationIdentification-Construction, as shown in Figure 4.

In the validation phase, we use the CodeHalu algorithm to identify multiple types of hallucinations HaluTypes $\xi$ , represented as $[ ( \xi _ { 1 } , P _ { 1 } ) , \ldots , ( \xi _ { i } , P _ { i } ) ]$ . In the identification phase, we annotate the $k ^ { 2 }$ most common hallucinations and their frequencies in each sample $\alpha _ { i }$ , represented as $[ ( \xi _ { 1 } , P _ { 1 } ) , \ldots ,$ $( \xi _ { k ^ { 2 } } , P _ { k ^ { 2 } } ) ]$ . In the construction phase, we sort all samples in descending order based on the frequency $P _ { i }$ of each hallucination type $\xi _ { i }$ . If the hallucination frequency in a sample $\alpha _ { i }$ exceeds the threshold $k$ , we include this sample in the corresponding hallucination type set in the CodeHaluEval benchmark. When selecting the threshold $k$ , we consider both the minimum number of samples required to detect code hallucination effects in the CodeHaluEval benchmark and the inference costs associated with evaluating various LLMs. Through this method, we establish the CodeHaluEval benchmark, with detailed statistics shown in Table 1.

# Experiments

Models. To comprehensively analyze the different hallucinations of various competitive LLMs in CodeHaluEval, we evaluate 12 general LLMs, including GPT-4 (OpenAI 2023), GPT-3.5 (OpenAI 2023), Gemini-Pro-1.0 (Gemini 2023), Claude-3-haiku (Anthropic 2024), LLaMA-2 & 3 (Touvron et al. 2023), Vicuna (Chiang et al. 2023), Qwenturbo (Bai et al. 2023), ChatGLM3-6B (Du et al. 2021), Ernie-3.5 (Baidu 2023), Mistral-7B (Jiang et al. 2023), Gemma (Team et al. 2024). We also evaluate 5 coding LLMs, including Code LLaMA (Roziere et al. 2023), DeepSeek Coder (Guo et al. 2024), CodeGeeX-2 (Zheng et al. 2023), StarCoder-2 (Li et al. 2023a), MagicCoder-7B (Wei et al. 2023), WizardCoder-7B (Luo et al. 2023). The experimental evaluation is conducted using API calls or 8 NVIDIA A6000 GPUs.

Metrics. Given the limited exploration of code hallucinations, no dedicated metrics currently exist for evaluating them in LLMs. To address this gap, we propose an evaluation metric called Hallucination Rate (HR). Specifically, HR is defined as the percentage of hallucination samples detected in the test set among all samples, with the formula: $\begin{array} { r } { \mathrm { H R } = { \frac { 1 } { N } } \sum _ { i = 1 } ^ { N } S ( i , K ) } \end{array}$ , where $S ( i , K )$ is an indicator function. If the ith sample satisfies the hallucination condition, then $S ( i , K ) = 1$ ; otherwise, $S ( i , K ) = 0$ . Ideally, a lower HR indicates a lower likelihood of hallucinations during code generation by the LLM, thus demonstrating greater robustness and reliability. To our knowledge, HR is the first metric that accurately reflects the hallucination phenomenon in LLMs during code generation tasks through actual execution tests.

# Result & Analysis

The experimental results are presented in Table 2

Mapping hallucination: GPT-4 and GPT-3.5 consistently identify and follow rules related to data types, values, and structures, demonstrating strong context sensitivity.

Naming hallucination: Claude-3 reliably remembers and references entity names from the context and external knowledge bases. In contrast, LLaMA-2 exhibits significant memory bias when processing external knowledge and occasionally fabricates information.

Resource hallucination: GPT-4, Qwen, and LLaMA-2 effectively account for actual resource constraints when generating code, showing an understanding of computational boundaries and limitations, which leads them to produce code with lower complexity.

Table 2: Evaluation results of 17 models on CodeHalu. DC denotes Data Compliance hallucination. SA denotes Structure Access hallucination. ID denotes identity hallucination. ES denotes External Source hallucination. PC denotes Physical Constraint hallucination. CB denotes computational Boundary hallucination. LD denotes Logic Deviation. LB denotes Logic Breakdown.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="3">DCMapingO</td><td colspan="3">ID Namisg 4 Ayg</td><td colspan="3">PCResore(Avg</td><td colspan="3">Logie@</td><td rowspan="2">Average (↓)</td></tr><tr><td></td><td></td><td>Avg.</td><td></td><td></td><td></td><td></td><td></td><td></td><td>LD</td><td></td><td>Avg.</td></tr><tr><td>GPT-4</td><td>32.31</td><td>10.02</td><td>19.19</td><td>27.74</td><td>0.57</td><td>19.97</td><td>0.20</td><td>3.76</td><td>2.21</td><td>85.76</td><td>0.51</td><td>58.17</td><td>33.04</td></tr><tr><td>LLaMA-3-8B</td><td>46.87</td><td>23.46</td><td>33.09</td><td>12.09</td><td>0.00</td><td>8.63</td><td>15.48</td><td>12.99</td><td>14.07</td><td>78.39</td><td>0.00</td><td>53.02</td><td>33.67</td></tr><tr><td>DeepSeek Coder-6.7B</td><td>24.23</td><td>25.61</td><td>25.04</td><td>15.80</td><td>0.00</td><td>11.28</td><td>17.52</td><td>17.21</td><td>17.35</td><td>99.06</td><td>0.17</td><td>67.05</td><td>38.28</td></tr><tr><td>GPT-3.5</td><td>20.19</td><td>22.05</td><td>21.28</td><td>30.54</td><td>0.00</td><td>21.80</td><td>18.53</td><td>6.42</td><td>11.68</td><td>99.88</td><td>0.00</td><td>67.55</td><td>38.98</td></tr><tr><td>Claude-3-haiku</td><td>38.68</td><td>29.25</td><td>33.13</td><td>9.07</td><td>0.75</td><td>6.69</td><td>37.07</td><td>20.81</td><td>27.88</td><td>100.00</td><td>0.17</td><td>67.69</td><td>41.00</td></tr><tr><td>ChatGLM-3-6B</td><td>36.13</td><td>44.91</td><td>41.30</td><td>50.87</td><td>0.00</td><td>36.32</td><td>24.85</td><td>2.35</td><td>12.12</td><td>88.99</td><td>0.00</td><td>60.19</td><td>44.23</td></tr><tr><td>Ernie-3.5</td><td>48.14</td><td>36.90</td><td>41.52</td><td>30.31</td><td>0.38</td><td>21.75</td><td>18.13</td><td>11.89</td><td>14.60</td><td>98.98</td><td>0.00</td><td>66.94</td><td>44.31</td></tr><tr><td>Qwen-turbo</td><td>49.63</td><td>48.33</td><td>48.86</td><td>29.48</td><td>2.08</td><td>21.64</td><td>7.94</td><td>2.82</td><td>5.04</td><td>98.08</td><td>0.17</td><td>66.39</td><td>44.74</td></tr><tr><td>MagicCoder-7B</td><td>50.27</td><td>26.58</td><td>36.32</td><td>17.69</td><td>0.00</td><td>12.63</td><td>21.18</td><td>28.33</td><td>25.22</td><td>100.00</td><td>16.25</td><td>72.90</td><td>44.84</td></tr><tr><td>Code LLaMA-7B</td><td>65.04</td><td>42.17</td><td>51.57</td><td>31.07</td><td>0.00</td><td>22.18</td><td>18.53</td><td>6.26</td><td>11.59</td><td>94.76</td><td>9.41</td><td>67.14</td><td>46.68</td></tr><tr><td>StarCoder-16B</td><td>48.14</td><td>38.83</td><td>42.66</td><td>60.70</td><td>9.25</td><td>45.98</td><td>28.92</td><td>11.11</td><td>18.85</td><td>95.09</td><td>0.77</td><td>64.56</td><td>49.23</td></tr><tr><td>LLaMA-2-7B</td><td>51.22</td><td>32.29</td><td>40.08</td><td>78.46</td><td>71.13</td><td>76.36</td><td>14.87</td><td>0.00</td><td>6.46</td><td>81.05</td><td>0.34</td><td>54.93</td><td>49.41</td></tr><tr><td>Gemini-1.0</td><td>34.11</td><td>53.53</td><td>45.54</td><td>45.88</td><td>0.00</td><td>32.76</td><td>24.44</td><td>16.12</td><td>19.73</td><td>98.65</td><td>10.35</td><td>70.07</td><td>49.57</td></tr><tr><td>Mistral-7B</td><td>45.48</td><td>36.53</td><td>40.21</td><td>59.18</td><td>15.85</td><td>46.79</td><td>27.49</td><td>10.80</td><td>18.05</td><td>99.35</td><td>0.17</td><td>67.25</td><td>49.76</td></tr><tr><td>WizardCoder-7B</td><td>26.57 47.61</td><td>31.40 27.99</td><td>29.41</td><td>31.29</td><td>0.00</td><td>22.34</td><td>33.20</td><td>9.39</td><td>19.73</td><td>93.90</td><td>72.37</td><td>86.93</td><td>50.10</td></tr><tr><td>CodeGeeX-2-6B</td><td></td><td></td><td>36.06</td><td>45.05</td><td>0.00</td><td>32.16</td><td>36.66</td><td>23.47</td><td>29.20</td><td>89.60</td><td>99.66</td><td>92.86</td><td>57.47</td></tr><tr><td>Gemma-7B</td><td>55.26</td><td>41.05</td><td>46.90</td><td>51.85</td><td>0.00</td><td>37.02</td><td>14.46</td><td>14.55</td><td>14.51</td><td>97.18</td><td>100.00</td><td>98.09</td><td>61.53</td></tr></table></body></html>

Logical hallucination: Although all models face challenges in maintaining logical coherence, LLaMA-3 and GPT-4 perform relatively well in reducing repetition. Most models rarely generate code with stuttering or infinite loops, but such issues are more common in Gemma, CodeGeeX-2, and WizardCoder, indicating a tendency to lose semantic and logical consistency during code generation.

Overall, GPT-4 and LLaMA-3 perform well across all hallucination categories, displaying stability and robustness in various scenarios. Logical hallucinations remain the most prevalent issue across all models, while naming and resource hallucinations are relatively less common. The performance of different models varies significantly across hallucination types, likely due to differences in their training data, methods, and architectures. The average hallucination rate ranges from approximately $20 \%$ to $60 \%$ .

We view mitigating code hallucination as future work. Based on a detailed analysis of experimental results and generated cases, we provide insights into strategies for mitigating code hallucinations in LLMs. In terms of training data, improving the quality and increasing the diversity of data sources enhances the model’s generalization ability. In terms of training methods, employing alignment strategies based on compilation and execution verification, as well as setting multiple objectives during training, enables the model to better understand the data flow and control flow of code. In terms of model architecture, introducing a static code verification module provides real-time feedback on verification results, thereby enhancing the model’s robustness. Additionally, incorporating a code graph module allows the model to construct and utilize graph structure information when generating code, deepening its understanding of patterns and logical relationships in the generated code.

# Conclusion

We introduce the concept of code hallucination and propose an execution-based verification method to classify code hallucinations. We develop the dynamic detection algorithm, CodeHalu, and categorize code hallucinations into four main types, providing a comprehensive understanding of the various challenges faced by LLMs in code generation. Additionally, we establish the CodeHaluEval benchmark and evaluate 17 widely-used LLMs, revealing significant differences in their hallucination patterns during code generation, and providing detailed insights for further improving the code generation capabilities of LLMs. Overall, we lay the theoretical foundation for understanding the hallucination phenomenon of LLMs in code generation, and provide a complete set of tools for detecting and evaluating code hallucinations.

# Limitations

Python is our focus for exploring code hallucination, as it is the most widely used programming language according to the TIOBE Index. Furthermore, many existing studies, such as HumanEval and MBPP benchmarks, concentrate on Python. Thus, we do not extend our investigation to other languages.

CodeHalu focuses on ensuring the correctness of generated code to meet the needs of developers and users. In contrast, identifying and preventing security risks is a higher-level concern, effectively addressed through sandbox environments. However, we recognize the importance of potential security risks and will consider them in future research.

We focus on code hallucination specifically within the code generation task, excluding other programming tasks such as code translation, and code repair. This is because code generation is currently the most widely studied task in the community. It is important to emphasize that but our hallucination detection and evaluation methods can be easily adapted to other tasks , which we consider as future work.