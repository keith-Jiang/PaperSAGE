# Drop the Beat! Freestyler for Accompaniment Conditioned Rapping Voice Generation

Ziqian Ning1,2, Shuai Wang3, Yuepeng Jiang1, Jixun Yao1, Lei $\mathbf { H e } ^ { 2 }$ , Shifeng $\mathbf { P a n } ^ { 2 }$ , Jie $\mathbf { D i n g ^ { 2 } }$ , Lei Xie1\*

1Audio, Speech and Language Processing Group (ASLP $@$ NPU), School of Computer Science, Northwestern Polytechnical University, Xi’an, China 2 Microsoft, China 3 Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China {ningziqian, Jiangyp, yaojx}@mail.nwpu.edu.cn, wangshuai $@$ cuhk.edu.cn, {helei, peterpan, jie.DING} $@$ microsoft.com, lxie.nwpu.edu.cn

# Abstract

Rap, a prominent genre of vocal performance, remains underexplored in vocal generation. General vocal synthesis depends on precise note and duration inputs, requiring users to have related musical knowledge, which limits flexibility. In contrast, rap typically features simpler melodies, with a core focus on a strong rhythmic sense that harmonizes with accompanying beats. In this paper, we propose Freestyler, the first system that generates rapping vocals directly from lyrics and accompaniment inputs. Freestyler utilizes language model-based token generation, followed by a conditional flow matching model to produce spectrograms and a neural vocoder to restore audio. It allows a 3-second prompt to enable zero-shot timbre control. Due to the scarcity of publicly available rap datasets, we also present RapBank, a rap song dataset collected from the internet, alongside a meticulously designed processing pipeline. Experimental results show that Freestyler produces high-quality rapping voice generation with enhanced naturalness and strong alignment with accompanying beats, both stylistically and rhythmically.

# Introduction

Rap stands out as one of the most distinctive genres of vocal performance, yet it has received limited attention in the field of vocal generation. At its core, rap is defined by its emphasis on rhythm and tempo, distinguishing it markedly from other genres. Rappers typically deliver rapid, powerful verses that tightly synchronize with the accompanying beats, creating a dynamic and energetic auditory experience.

Normal singing voice synthesis (SVS) requires lyrics, notes, and duration as inputs. Given that rap is inherently a freeform performance style characterized by varied rhythms, predefined rhythms in SVS can hinder the naturalness of the generation process. In contrast, Text-to-song (TTSong) offers greater flexibility, generating both vocals and accompaniment solely from lyrics and style prompts. A typical TTSong approach involves first creating the vocal track based on the lyrics and then generating the accompaniment track using natural language prompts alongside the produced vocals. However, generating rap vocals with only lyrics condition may compromise the rhythmic integrity. Furthermore, the limited availability of rap datasets poses an additional challenge in rapping voice generation. Singing data is much less abundant compared to large-scale speech datasets, and within this limited pool, rap data is an even smaller subset. This scarcity of rap data further complicates the process of generating rapping voices.

![](images/45bebb0e8769f63820a42a0a0ad3898a199766bc23b2aa3547695f11a2aa4309.jpg)  
Figure 1: The overall pipeline of Freestyler. With lyrics and accompaniment as condition, it can generate rapping voice that matches the style and rhythm of the accompaniment.

In this paper, we present Freestyler, the first rapping voice generation model capable of generating rap that harmonizes with the style and rhythm of the accompaniment using only lyrics and accompaniment inputs. With a 3-second reference audio, it can adapt to any speaker’s voice. Our approach employs a three-stage framework: lyrics-to-semantic, semanticto-spectrogram, and spectrogram-to-audio. To address the challenge of data scarcity, we use discrete semantic tokens as a proxy representation so that some parts of the model do not require supervised data for training. The first stage adopts a language model to predict discrete semantic tokens conditioned on lyrics and fine-grained accompaniment features. The second stage applies conditional flow matching techniques for mel-spectrogram prediction. Finally, a vocoder restores audio from the spectrogram. Given the lack of publicly available rap datasets, we collected a large volume of rap songs from the internet and designed a meticulous pipeline for data cleaning, processing, and filtering, resulting in a dataset we have named RapBank.

Experiments conducted on our collected rap dataset show that Freestyler generates high-quality rap that fits the accompaniment. The main contributions of this work are summarized as follows:

• We propose Freestyler, the first accompanied rapping voice generation model that takes lyrics and accompanying music as conditions.   
• We present RapBank, a large volume rap dataset with comprehensive data-processing pipeline, suitable for model training. Both the data and the processing pipeline are publically available on HuggingFace1 and Github2.   
• We developed a language model that uses accompaniment conditions to provide global style control and finegrained rhythm control of vocal production. We also adopt conditional flow matching for high-quality melspectrogram prediction.   
• Experimental results from objective and subjective evaluations demonstrate the effectiveness of Freestyler3.

# Related Work

# Singing Voice Synthesis

Singing voice synthesis (SVS) aims to generate natural singing voices based on lyrics, musical scores, and corresponding durations. VISinger 2 (Zhang et al. 2023) introduces an end-to-end system utilizing a digital signal processing (DSP) synthesizer to enhance sound quality. NaturalSpeech 2 (Shen et al. 2024) and StyleSinger (Zhang et al. 2024) employ a reference voice clip for timbre and style extraction, enabling style transfer and zero-shot synthesis. PromptSinger (Wang et al. 2024) is the first system to attempt guiding singing voice generation through text descriptions, placing greater emphasis on speaker identity and timbre control. DiffSinger (Liu et al. 2022) addresses the issue of excessive smoothness by implementing a shallow diffusion mechanism. To bridge the gap between realistic music scores and detailed MIDI annotations, RMSSinger (He et al. 2023) proposes a word-level modeling approach combined with diffusion-based pitch prediction. MIDI-Voice (Byun et al. 2024) incorporates MIDI-based priors for expressive zero-shot generation. VoiceTuner (Huang et al. 2024) advocates a self-supervised pre-training and fine-tuning strategy to mitigate data scarcity, applicable to low-resource SVS tasks. Despite contributions from open-source singing voice datasets (Wang et al. 2022; Zhang et al. 2022; Duan et al. 2013), their quantity significantly lags behind that of speech datasets, and none specifically cater to rap genres.

# Music Generation

Music generation encompasses various tasks, including symbolic music generation, lyrics generation, and accompaniment generation. MuseGAN (Dong et al. 2018) achieves symbolic music generation through a GAN-based approach.

SongMASS (Sheng et al. 2021) designs a method for songwriting that generates lyrics or melodies conditioned on each other, while SongComposer (Ding et al. 2024) proposes a large language model (LLM) for song composition, capable of generating melodies and lyrics with symbolic song representations. DeepRapper (Xue et al. 2021) focuses on rap lyrics generation, which also leverages an LLM to generate lyrics from right to left with rhyme constraints. Inspired by two-stage modeling in audio generation (Borsos et al. 2023), MusicLM (Agostinelli et al. 2023) uses a cascade of transformer decoders to sequentially generate semantic and acoustic tokens, based on joint textual-music representations from MuLan (Huang et al. 2022). MusicGen (Copet et al. 2023) introduces a novel approach with codebook interleaving patterns to generate music codec tokens in a single transformer decoder, which is further combined with stack patterns in Le Lan et al. 2024 to improve generation quality. Additionally, MeLoDy (Lam et al. 2023) presents an LM-guided diffusion model that efficiently generates music audio, and MusicLDM (Chen et al. 2024) incorporates beat-tracking information and latent mixup data augmentation to address potential plagiarism issues in music generation. Several works focus specifically on vocalto-accompaniment generation, such as SingSong (Li et al. 2024b), which generates instrumental music to accompany input vocals, and Melodist (Hong et al. 2024), which utilizes a transformer decoder for controllable accompaniment generation.

# Text-to-Song

Text-to-song (TTSong), also recognized as Accompanied Singing Voice Synthesis (ASVS), strives to produce natural singing voices accompanied by music. TTSong incorporates elements from both singing voice synthesis and music generation; the former focuses on generating a singing vocal, while the latter primarily involves music creation. A common methodology in TTSong employs a two-stage process: initially generating the vocal track from lyrical input, followed by the prediction of accompanying music. Melodist (Hong et al. 2024) is the first TTSong model, which utilizes two autoregressive transformers to sequentially produce vocal and accompaniment codec tokens, conditioned on lyrics, musical scores, and natural language prompts. MelodyLM (Li et al. 2024a) eliminates the need for music scores in Melodist and instead relies solely on textual descriptions and vocal references.

# Freestyler

In this section, we first define the task of accompanimentconditioned rapping voice generation. Subsequently, we provide an overview of the proposed system, Freestyler, followed by a detailed explanation of each stage within it.

# Task Definition

In this study, we present a novel task: rapping voice generation. This task entails the creation of rapping voices that are stylistically and rhythmically synchronized with the accompanying music. It can be regarded as the inverse of accompaniment generation (Li et al. 2024b), which generates instrumental music to accompany input vocals

![](images/e7e7c651a603b419ee624568dce37664b389d13b91741c7d29afc5972ef980c5.jpg)  
Figure 2: Overview of Freestyler. The lyrics-to-semantic model in (a) predicts semantic tokens based on lyrics and accompaniment. The accompaniment feature is shifted left by $K$ frames to provide additional rhythmic context. The semanticto-spectrogram model in (b) generates mel-spectrograms from the semantic tokens, which are interpolated to align with the spectrogram’s frame rate. Speaker embedding is provided to both models to control the timbre.

Given the training dataset consisting of rap songs $R$ and their corresponding lyrics $L$ , we separate $R$ into the vocal tracks $R _ { v }$ and the accompaniment tracks $R _ { a }$ . A short segment $C$ is randomly extracted from $R _ { v }$ as the reference audio which presents the rapper’s timbre. The task of rapping voice generation can be defined as modeling the conditional probability distribution $p ( R _ { v } | R _ { a } , L , C )$ .

# Overview

To address the challenge of data scarcity, we divide the task of rapping voice generation into three hierarchical stages: lyrics-to-semantic, semantic-to-spectrogram, and spectrogram-to-audio. Rather than directly generating acoustic tokens or mel-spectrograms from lyrics, we employ semantic tokens derived from K-means clustering on self-supervised learning (SSL) representations (Kharitonov et al. 2023; Yao et al. 2024) as a proxy feature to bridge the first two stages. This method offers two primary advantages. First, semantic tokens are more closely aligned with the text domain, enabling the first-stage model to be trained using less annotated data. Second, the subsequent two stages can be trained in an unsupervised manner by utilizing more unlabeled data. We use a language model (LM) for the lyrics-to-semantic stage conditioned on lyrics, accompaniment features, and a 3-second reference audio segment, which generates discrete semantic tokens. For the semanticto-spectrogram stage, we employ a conditional flow matching (CFM) model to transform the discrete semantic tokens into continuous mel-spectrograms. The reference audio is also incorporated into the CFM model to complement the missing timbre information contained in the semantic tokens. Finally, a pre-trained vocoder is utilized to reconstruct audio from the spectrogram. The overall model design is illustrated in Figure 2.

![](images/80166b512ca14688988a7fd1bc812c8f9b2f459c1fd9823eb12cce831a65f8ce.jpg)  
Figure 3: The extraction process of the accompaniment feature and semantic tokens. Each block in Wav2Vec XLS-R represents 6 attention layers, with accompaniment and vocals going through 6 and 18 layers respectively.

# Lyrics-to-Semantic

Feature Representation and Tokenization As discussed in Pasad, Chou, and Livescu 2021; Chen et al. 2022, different layers of SSL models encode different types or extents of information, with shallower layers capturing more acoustic features and deeper layers representing more semantic aspects. Therefore, with paired vocal and accompaniment as inputs, we extract discrete semantic tokens and continuous accompaniment features using two different layers of an SSL model. As shown in Figure 3, we utilize a pre-trained Wav2Vec XLS-R (Conneau et al. 2021) for feature extraction, where the vocal input is processed through its 18 layers, and the semantic tokens $S$ are subsequently obtained using K-means clustering. While the accompaniment input is passed through 6 layers to derive the accompaniment features $A$ . For the lyrics, we employ a grapheme-to-phoneme (G2P) phonemizer (Bernard and Titeux 2021) to obtain the lyrics tokens $L$ .

Language Model with Fine-grained Accompaniment Condition As illustrated in Figure 2a, the generative process of semantic tokens in Freestyler is formulated as a unidirectional next-token-prediction task, i.e., in the form of a language model. In the prediction process, Freestyler relies on lyrics, accompaniment features, and a reference mel-spectrogram to constrain the generated semantic tokens. More specifically, initially, the lyrics tokens and semantic tokens are concatenated and subsequently embedded, which are then summed with the accompaniment features to produce the mixed feature. However, based on our experience, if the accompaniment condition is exactly aligned with the semantic tokens, the generated rap vocal and accompaniment will have a certain degree of mismatch. We hypothesize this is because the context information of the accompaniment is quite important, but due to the current model design which has no access to future information, such context information cannot be effectively learned. Based on this, we shift the accompaniment feature left by $K$ frames, so that the model can have the accompaniment information of $K + t$ frames when generating the $t$ -th frame. Thus we model the following distribution:

$$
p ( \boldsymbol { S } ) = \prod _ { t = 2 } ^ { n } p ( s _ { t } | s _ { < t } , l , c , a _ { < t + K } ; \theta _ { L M } ) ,
$$

where $s , l , c _ { i }$ , and $a$ stand for semantic tokens, lyrics tokens, speaker embedding extracted using a reference encoder, and acoustic features.

During training, the vocal-accompaniment pairs are of equal length. However, when inferencing, the lengths of these two elements may vary. This mismatch can lead to early termination of the model if the accompaniment is too short, or result in the generation of hallucinated content if the accompaniment is excessively long. To address this issue, we introduce a random mask on the accompaniment condition, thereby mitigating the strong temporal correlation between these two features.

Zero-shot Timbre Control The predominant zero-shot approaches for TTS or SVS usually involve two main methods: using voice prompts to leverage the language model’s in-context learning capabilities, or using a speaker embedding as a global timbre condition. In Freestyler, the latter approach was adopted. The rationale behind this choice is that the style of the voice prompt can significantly influence the final generated speech style. This means we would need to require the user to provide a rapping segment as the prompt, which is non-trivial.

On the other hand, the speaker embedding mainly affects the timbre, while the rapping style is primarily controlled by the accompaniment features. This ensures we can generate rapping vocals with any target timbre. In the specific implementation, we propose the use of a reference encoder to extract a global speaker embedding from a segment of reference audio. This embedding is then concatenated to the head of the mixed features for timbre control.

# Semantic-to-Spectrogram

We employ conditional flow matching to map semantic tokens to mel-spectrograms. Given that semantic tokens lack complete timbre information due to their discrete nature, we incorporate additional timbre conditions at this stage. Let $\scriptstyle { \mathbf { { \vec { x } } } }$ denote an observation in the data space $\mathbb { R } ^ { d }$ , sampled from a complicated, unknown data distribution $q ( { \pmb x } )$ . A probability density path is a time-dependent probability density function, $\dot { p _ { t } } : [ 0 , 1 ] \times \mathbb { R } ^ { d } \to \dot { \mathbb { R } } > 0$ . One way to generate samples from the data distribution $q$ is to construct a probability density path $p _ { t }$ , where $t \in [ 0 , 1 ]$ and $p _ { 0 } ( { \pmb x } ) = \bar { \mathcal { N } } ( { \pmb x } ; { \bf 0 } , \bar { \cal I } )$ is a prior distribution, such that $p _ { 1 } ( { \pmb x } )$ approximates the data distribution $q ( { \pmb x } )$ . For example, CNFs first define a vector field $\pmb { v } _ { t } : [ \hat { 0 } , 1 ] \times \mathbb { R } ^ { d }  \mathbf { \bar { \mathbb { R } } } ^ { d }$ , which generates the flow $\phi _ { t } : [ 0 , 1 ] \times \mathbf { \bar { \mathbb { R } } } ^ { d }  \mathbb { R } ^ { d }$ through the ODE

$$
\frac { d } { d t } \phi _ { t } ( { \pmb x } ) = { \pmb v } _ { t } ( \phi _ { t } ( { \pmb x } ) ) ; \qquad \phi _ { 0 } ( { \pmb x } ) = { \pmb x } .
$$

This generates the path $p _ { t }$ as the marginal probability distribution of the data points. We can sample from the approximated data distribution $p _ { 1 }$ by solving the initial value problem in Eq. 2.

Suppose there exists a known vector field $\mathbf { \Delta } \mathbf { u } _ { t }$ that generates a probability path $p _ { t }$ from $p _ { 0 }$ to $p _ { 1 } \approx q$ , conditional flow matching considers

$$
\mathcal { L } _ { C F M } = \mathbb { E } _ { t , q ( x _ { 1 } ) , p _ { t } ( x | x _ { 1 } ) } \Vert u _ { t } ( x | x _ { 1 } ) - v _ { t } ( x | \mu , \hat { c } ; \theta ) \Vert ^ { 2 } .
$$

where $t \sim \mathbb { U } [ 0 , 1 ]$ and ${ \mathbf v } _ { t } ( { \mathbf x } | \mu , \hat { c } ; \theta )$ is a neural network with parameters $\theta . \mu$ is the embedded and interpolated semantic tokens and $\hat { c }$ is the timbre embedding. This replaces the intractable marginal probability densities and the vector field in flow matching loss with conditional probability densities and conditional vector fields.

As shown in Figure 2b, the conditional flow matching model follows Matcha-TTS (Mehta et al. 2024) to use UNet (Rombach et al. 2022) architecture as the backbone, containing 1D convolutional residual blocks to downsample and upsample the inputs, with the flow matching step $t \in [ 0 , 1 ]$ embedded as in Popov et al. 2021. Each residual block is followed by a Transformer block, whose feedforward networks use snake beta activations (Lee et al. 2023). Additionally, the speaker embedding $\hat { c }$ is extracted by a reference encoder. Note that the reference encoder in the LM and CFM do not share parameters.

# Spectrogram-to-audio

We employ the V2 version of BigVGAN (Lee et al. 2023) for audio restoration. Compared to the V1 version, BigVGANV2 is trained using datasets containing diverse audio types, including speech in multiple languages, singing, and environmental sounds. Also, the discriminator is improved with multi-scale sub-band CQT discriminator (Gu et al. 2024) and multi-scale mel-spectogram loss (Kumar et al. 2023). We found BigVGAN-V2 exhibits high sound quality and exceptional robustness on rapping voice.

# RapBank

To the best of our knowledge, there is currently no publicly available dataset for rapping synthesis. Thus, we created an automatic pipeline to collect and label a novel dataset, RapBank, for the proposed rapping synthesis task.

RapBank comprises 92, 371 rap songs with a total duration of 5, 586 hours. After segmentation, we produced

904, 548 rap clips with an duration of 4, 353 hours, with corresponding lyrics and various quality-related metrics. The English subset utilized to train the LM will be presented in the experimental section, while detailed statistical information about RapBank can be found in the appendix.

Data Crawling Initially, we crawled all available raprelated playlists on YouTube using the keywords “Rap” and “Hip-hop”. Next, we extracted distinct video IDs from the playlists and filtered out those with a duration exceeding ten minutes, as they are unlikely to be songs. Then we downloaded the videos using their respective IDs and retain only the audio track. We resampled the audios to $4 4 . 1 \mathrm { \ k H z }$ , and average stereo mixes to mono.

Source Seperation In the previous step, we collected songs with both vocals and accompaniments mixed in a single track. As we aim to generate rapping vocals with accompaniment and lyrics as conditions, the first stage of data processing involves separating vocals and accompaniment into different tracks. To accomplish this, we utilized the state-ofthe-art music source separation model, BS-RoFormer (Lu et al. 2024). We first extract the vocals from the mixed track and subsequently subtract them from the original track to obtain the pure accompaniment.

Segmentation To ensure the data length is suitable for model training while eliminating non-speech segments, we employed Voice Activity Detection (VAD) to segment the separated vocal tracks. We utilized WebRTC Voice Activity Detector 4 to extract frame-level voice/unvoice labels and subsequently slice the vocal track into segments containing only vocal sounds. Adjacent segments are sequentially merged if the unvoice gap between them is less than three seconds, continuing this process until the total duration exceeds a specified threshold. This threshold is sampled from a Gaussian distribution with a mean of 18 seconds to enhance variability in data lengths. Subsequently, We sliced the accompaniment with the same timestamp to obtain segments that correspond the vocals.

Lyrics Recognition Another challenge we faced in designing RapBank was that less than one-tenth of the songs we collected contained lyrics. Additionally, issues such as unclean textual content and inaccurate timestamps made these data difficult to be used directly for model training. Thus we employ the powerful automatic speech recognition (ASR) model Whisper (Radford et al. 2023) to transcribe the lyrics. As Whisper is trained with speech data, the word error rate (WER) of resulting lyrics is significantly higher than speech. The recognition errors can be categorized into two types. The first type involves misrecognition resulting from words with similar pronunciations, which has minimal impact on model training. The second type involves hallucinations, where the model produces phrases or sentences that do not exist in any form within the input audio. Hallucinations often lead to an abnormal singing tempo; therefore, we can filter these results based on the singing tempo, which will be discussed in the following section.

Quality Filtering Given the prevalence of digital effects and multi-singer scenarios in songs, we adopt several quality-related metrics apply filtering to ensure the quality of the final processed rap data. Following Ma et al. 2024, we further devide the dataset into three subsets with increasing quality—Basic, Standard, and Premium—utilizing these metrics for multi-stage training. Specifically, we utilize a diarization model 5 to calculate the singing duration for each singer and identify the individual with the longest singing duration as the primary singer. Subsequently, we exclude all segments where the primary singer’s duration does not occupy the majority, i.e., less than $80 \%$ (can vary for different subsets) of the total duration. We extract phonemes from the lyrics and compute the phoneme-per-second (PPS) rate, which reflects the tempo of the singing voice. Segments with a PPS rate that are either too low or too high are discarded, as most lyrics associated with these segments exhibit hallucinations. To eliminate low-quality rap segments, we use the DNSMOS P.808 scores (Reddy, Gopal, and Cutler 2021) to evaluate each rap segment.

# Experimental Setup

# Dataset

We utilize the English subset of RapBank to train the LM, which contains approximately 58, 200 songs with a total duration of 3,800 hours. After processing, we get the Basic, Standard and Premium subsets containing 1, 321, 295 and 58 hours of data respectively. We employ the entire RapBank to train the CFM model as it does not require any labels. We randomly reserved 200 samples for evaluation, with no singer overlapping with the training set. These samples are human-annotated to get the ground truth lyrics.

# Implementation and Hyperparameters

We build a 6-layer LLaMA (Touvron et al. 2023) for lyricsto-semantic modeling, with 116M parameters. As mentioned earlier, to mitigate the train-inference mismatch of lengths in vocal-accompaniment pairs, a masking strategy is applied probabilistically—there is a $5 0 \%$ chance that the entire accompaniment condition will be masked, and for the other $5 0 \%$ chance, a mask will be applied to a random length of the latter half of the accompaniment. We first pre-train the LLaMA model on the Basic subset, followed by sequential supervised finetuning (SFT) on both the Standard and Premium subsets. We train the LLaMA model using 4 NVIDIA V100 GPUs with a batch size of 16 and gradient accumulation of 4. The conditional flow matching model for semanticto-spectrogram generation contains 129M parameters and is also trained using 4 NVIDIA V100 GPUs. The batch size and gradient accumulation are 64 and 4, respectively. Each data segment is fixed at ten seconds in length, with shorter segments being padded and longer segments truncated. The sample steps is set to 20. For audio restoration, we employ the pre-trained BigVGAN-V2 44.1 kHz version6. The number of $\mathbf { K }$ -means clusters is set to 1024, and the accompaniment feature shift $K$ is set to 150 (3 secs).

Table 1: Objective and subjective evaluation of vocals generated by Freestyler, including different configurations of whethe accompaniment conditions are used in the training and inference stage   

<html><body><table><tr><td>Model</td><td>Training w/Acco</td><td>Inference w/ Acco</td><td>MOS-N↑</td><td>MOS-R↑</td><td>WER↓</td></tr><tr><td>GT Vocal</td><td>1</td><td>1</td><td>4.27±0.06</td><td>4.13±0.05</td><td>31.7</td></tr><tr><td rowspan="3">Freestyler</td><td>X</td><td>X</td><td>3.43±0.03</td><td>3.21±0.07</td><td>32.6</td></tr><tr><td>√</td><td>X</td><td>3.68±0.04</td><td>3.55±0.06</td><td>32.4</td></tr><tr><td>√</td><td>√</td><td>3.92±0.05</td><td>3.80±0.06</td><td>33.2</td></tr></table></body></html>

# Evaluation Metrics

Subjective Metrics We employ mean opinion score (MOS) with $9 5 \%$ confidence intervals to evaluate multiple aspects of the generated rapping voice, including overall naturalness (MOS-N), singer similarity (MOS-S), vocal rhythmicity (MOS-R) and the stylistic and rhythmic alignment between vocals and acompaniment (MOS-M). The MOS scores are obtained by crowd-sourced listening tests, with 20 listeners involved. Each listener is instructed to evaluate the samples on a 5-point scale: 1 - bad, 2 - poor, 3 - fair, 4 - good, and 5 - excellent.

Objective Metrics We employ word error rate (WER) to measure intelligibility and speaker cosine similarity (SECS) to assess the timbre similarity, respectively. Specifically, we utilized the large-v3 version of Whisper7 to transcribe the generated rapping voice and calculate WER by comparing it to the human-annotated lyrics. For SECS measurement, we use WavLM-large fine-tuned on the speaker verification task8 to obtain speaker embeddings. These embeddings are then used to calculate the cosine similarity of generated rap against reference audio. Furthermore, we measure the overall quality of the rap using Fre´chet Audio Distance (FAD) calculated using CLAP (Wu et al. 2023), and employ Kullback–Leibler Divergence (KLD) and CLAP cosine similarity (Li et al. 2024a) to evaluate the distribution similairty and semantic correlation between real and generated rap.

# Evaluation Results

# Vocal Quality Evaluation

In this section, we assess the vocal generation capabilities of Freestyler, focusing on naturalness, rhythmicity, and intelligibility. We compare these metrics between the ground truth (GT) vocals and three distinct configurations of Freestyler. By employing random masking on accompaniment conditions during training, Freestyler is capable of operating with or without accompaniment during the inference stage. Furthermore, we conduct an ablation study to evaluate Freestyler in the absence of the accompaniment condition during both training and inference. The evaluation results are presented in Table 1.

The results demonstrate that Freestyler performs comparably to the GT across all metrics, highlighting its impressive capabilities in rapping voice generation. Notably, the naturalness and rhythmicity of the generated raps were significantly reduced when the accompaniment condition was excluded from both the training and inference stages, emphasizing the necessity of incorporating this condition. Furthermore, when the accompaniment is included during training but omitted during inference, the performance still exceeds that of scenarios where the accompaniment is excluded from both stages. This finding suggests that the model learns the rhythmic correlation between lyrics and the accompaniment when the latter is utilized in training, thereby enhancing performance even in lyrics-only inference.

Compared to normal speech, the WER across all models, including GT, is notably high, primarily due to Whisper’s lack of training on singing data. The rapid tempo of rap, characterized by numerous conjunctions and varying intonations, presents significant challenges to Whisper’s recognition accuracy. However, these models have similar WERs, suggesting that they achieve a level of intelligibility comparable to that of the ground truth.

# Accompaniment-mixed Rap Evaluation

In this section, we further investigate rapping vocals accompanied by music. By utilizing segments from actual rap songs as the topline, we randomly select and mix unpaired vocals and accompaniment from our test set to serve as the bottomline. This setup allows us to examine the impact of stylistic and rhythmic mismatches on various metrics. Table 2 shows that the randomly mixed ground truth samples received the lowest scores, demonstrating the importance of style and rhythm alignment between vocals and accompaniment. Conversely, Freestyler achieved scores comparable to GT, demonstrating its capability to generate naturalsounding raps. We also conducted experiments with three ablation systems: 1) removal of the accompaniment condition (w/o Acco); 2) removal of supervised finetuning (w/o SFT); and 3) removal of random masking (w/o Mask). The exclusion of the accompaniment condition resulted in a significant reduction in synchronization between vocals and accompaniment, leading to an overall decrease in perceived naturalness. The removal of SFT caused a slight decline in total metrics. Notably, the absence of random masking had the most pronounced effect, markedly reducing both naturalness and synchronization while causing a dramatic increase in WER. This phenomenon arises because, in the absence of random masking, results in substantial mismatches between training and inferencing. If the lyrics do not conclude by the time the accompaniment ends, the model terminates prematurely; conversely, if the lyrics finish before the accompaniment is complete, the model generates nonsensical outputs, thus leading to the elevated WER metrics observed.

Table 2: Objective and Subjective evaluation of rapping vocals mixed with accompaniments generated by Freestyler and ablation models. ”w/o Acco” means model trained without accompaniment. ”w/o SFT” means remove supervised finetuning and ”w/o Mask” means remove random masking on accompaniment condition.   

<html><body><table><tr><td>Model</td><td>MOS-N↑</td><td>MOS-M↑</td><td>FAD↓</td><td>KLD↓</td><td>CLAP↑</td><td>WER↓</td></tr><tr><td>GT</td><td>4.35±0.04</td><td>4.21±0.06</td><td>0.07</td><td>0.01</td><td>0.90</td><td>31.7</td></tr><tr><td>GTRand Mix</td><td>3.23±0.05</td><td>3.06±0.03</td><td>0.16</td><td>0.33</td><td>0.45</td><td>31.7</td></tr><tr><td>Freestyler</td><td>3.88±0.07</td><td>3.84±0.06</td><td>0.10</td><td>0.20</td><td>0.60</td><td>33.2</td></tr><tr><td>w/o Acco</td><td>3.62±0.06</td><td>3.19±0.04</td><td>0.16</td><td>0.23</td><td>0.57</td><td>32.6</td></tr><tr><td>W/o SFT</td><td>3.80±0.05</td><td>3.78±0.07</td><td>0.12</td><td>0.20</td><td>0.58</td><td>34.4</td></tr><tr><td>w/o Mask</td><td>3.35±0.05</td><td>3.42±0.05</td><td>0.39</td><td>0.21</td><td>0.48</td><td>56.1</td></tr></table></body></html>

Table 3: Speaker similarity comparison results of different reference types with various condition methods.   

<html><body><table><tr><td>Ref Type</td><td>w/ LM Cond</td><td>w/ CFM Cond</td><td>MOS-S↑</td><td>SECS↑</td></tr><tr><td rowspan="3">Rap</td><td>√</td><td>X</td><td>3.44±0.04</td><td>0.53</td></tr><tr><td>X</td><td>√</td><td>3.37±0.05</td><td>0.50</td></tr><tr><td>√</td><td>√</td><td>3.86±0.05</td><td>0.69</td></tr><tr><td>Speech</td><td>√</td><td>√</td><td>3.64±0.03</td><td>0.28</td></tr></table></body></html>

# Zero-shot Evaluation

As previously discussed, the timbre within semantic tokens is incomplete. Therefore, we introduce reference audio conditions in both the lyrics-to-semantic and semanticto-spectrogram stages. To assess the impact of reference audio conditions on speaker similarity across these stages, we examine three different combinations. The speaker similarity metrics presented in Table 3 demonstrate that: 1) Freestyler, which employs reference audio input in both stages, achieves a MOS-S score of 3.86 and a SECS score of 0.69, indicating a high degree of speaker similarity in the rapping voice generated by our proposed system; 2) The implementation of a single-stage timbre condition, whether in the first or second stage, results in a significant decline in similarity, thereby supporting our hypothesis that semantic tokens do not convey complete timbral information.

Additionally, we conducted experiments using speech instead of rap as the reference audio, enabling non-expert individuals to perform rap. As shown in the last row of the table, a satisfactory level of subjective similarity is achieved, illustrating Freestyler’s remarkable generalization ability of zero-shot timbre control. However, the objective metric is low, probably due to the model we employed to compute SECS has never seen such a difference in style from the same speaker during training, resulting in low SECS score.

# Vocal-Accompaniment Alignment Visualization

To visualize the rhythmic correlation between the vocals and accompanying music rap songs, we analyzed both the ground truth and synthesized rap. As illustrated in Figure 4, the spectrograms of the ground truth accompaniment and vocals, along with the synthesized vocals, are presented sequentially. We manually annotated the positions of the beats in the accompaniment by drawing vertical lines across all three spectrograms. The results indicate that both the ground truth and generated vocals exhibit a strong correlation with the rhythm of the accompaniment, predominantly aligning with the beat positions. As the model does not have duration conditions and generates vocals freely, the synthesized vocals do not precisely match the ground truth. However, they demonstrate a similar pattern of alignment with the accompaniment. This finding underscores Freestyler’s ability to utilize the accompaniment as a condition for generating rhythmically aligned rap.

![](images/802f79ecbb9176732c80cb5dd251de58343bb31ae23d78545e18d507773f6a62.jpg)  
Figure 4: The spectrogram of (a) GT accompaniment, (b) GT vocal and (c) Freestyler-generated vocal. Vertical lines are human-annotated beat positions in the accompaniment. The energy of the GT accompaniment is also drawn in (a).

# Conclusion

In this paper, we propose Freestyler, the first rapping voice generation model that synthesizes high-quality rap vocals with enhanced naturalness and strong stylistic and rhythmic alignment with accompanying beats. Conditioned on finegrained accompaniment features, Freestyler first generates semantic tokens through an autoregressive language model. Next, these tokens are converted to spectrograms using a conditional flow matching model, and the spectrograms are mapped to audios with a neural vocoder. Additionally, we have developed an automatic pipeline to collect a large-scale rap dataset, which has been made publicly available along with the pipeline. Experimental results demonstrated the effectiveness of our proposed model structure and the strong rapping voice generation capability of Freestyler.