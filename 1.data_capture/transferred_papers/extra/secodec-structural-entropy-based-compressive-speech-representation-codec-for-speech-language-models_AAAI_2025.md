# SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models

Linqin Wang1,2, Yaping ${ { \bf { L i u } } ^ { 1 , 2 } }$ , Zhengtao $\mathbf { Y } \mathbf { u } ^ { 1 , 2 \mathrm { ~ * ~ } }$ , Shengxiang $\mathbf { G a 0 } ^ { 1 , 2 }$ , Cunli Mao1,2, Yuxin Huang1,2, Wenjun Wang1,2, Ling Dong1,2

1 Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China 2 Yunnan Key Laboratory of Artificial Intelligence, Kunming, China linqinwang7767@163.com,{ztyu,gaoshengxiang.yn}@hotmail.com, {maocunli,huangyuxin2004}@163.com, {20203104003,20232204183,ling.dong}@kust.edu.cn

# Abstract

With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks.

# Code — https://github.com/wlq2019/SECodec

# 1 Introduction

Large language models (LLMs) (Achiam et al. 2023; Touvron et al. 2023) have exhibited exceptional capabilities in a wide range of natural language processing tasks. This success has spurred extensive research efforts in developing speech language models (Zhang et al. 2024; Huang, Meng, and Ko 2023; Borsos et al. 2023), leading to notable advancements in numerous speech processing applications (Wang et al. 2023;

Tu et al. 2024; Rubenstein et al. 2023; Dong et al. 2024; Tu et al. 2023). To bridge the gap between continuous speech and token-based language models, a crucial method called speech discretization is employed. This process transforms an audio signal into a finite set of tokens. By converting speech into discrete tokens, language models are able to predict future semantic content and generate coherent and realistic speech with long-term consistency (Nguyen, Sagot, and Dupoux 2022; Tu et al. 2022).

Current discrete speech representations for speech language models can be categorized into three types: semantic tokens, acoustic tokens, and hybrid/unified tokens (Borsos et al. 2023; Zhang et al. 2024). 1) Semantic tokens (Hsu et al. 2021; Baevski et al. 2020; Chung et al. 2021) are typically generated from self-supervised pre-trained models using masked language modeling as the training objective, which are derived through $k$ -means clustering on representations from a specific intermediate layer, resulting in sequences with a one-dimensional structure. Speech language models that use semantic tokens (Lakhotia et al. 2021; Zhang et al. 2023; Hassid et al. 2023) can be externally connected to a vocoder for speech synthesis. While these models effectively capture semantically accurate content, the resulting speech generation often suffers from poor quality and a loss of acoustic details. 2) Acoustic tokens (Zeghidour et al. 2021; D´efossez et al. 2024; Yang et al. 2023; Du et al. 2024) are extracted from neural audio codecs, which use reconstruction as the training objective. By employing residual vector quantization (RVQ) (Gray 1984; Vasuki and Vanathi 2006) with hierarchical quantizers for discretization, acoustic tokens are represented as matrices with two dimensions: timesteps and quantizers. VALL-E (Wang et al. 2023) is a representative model of speech language models that utilize acoustic tokens. Despite achieving impressive zero-shot text-to-speech (TTS) capabilities, it still faces issues such as inaccurate content, stemming from the complex information contained within acoustic tokens. 3) Hybrid or unified tokens (Borsos et al. 2023; Zhang et al. 2024) employ different strategies to combine semantic tokens and acoustic tokens. Hybrid tokens adopt a hierarchical approach, encompassing both semantic token language models and acoustic token language models, to capture content information and acoustic details, respectively (Dong et al. 2024; Borsos et al. 2023; Rubenstein et al.

2023). Recently, unified tokens, exemplified by SpeechTokenizer (Zhang et al. 2024), have emerged. These tokens distill semantic information into acoustic tokens, effectively unifying semantic and acoustic representations. SpeechTokenizer has achieved superior results in downstream tasks such as speech synthesis. The ideal speech representation for speech language models should meet two key characteristics: i) Effective preservation of speech information; ii) Sufficient compressiveness for efficient training of speech language models. However, i) existing speech discretization methods rely on $k$ -means to initialize the codebook space, the size of codebook is a critical parameter that significantly impacts the performance of the codec and the training efficiency of downstream tasks, yet its size is typically determined through empirical judgment. ii) Additionally, when attempting to control the size of the codebook within a reasonable range, the quantization process, which relies on Euclidean distance, may lead to substantial differences between codebook’s vector and original vector, resulting in audio distortion. These issues result in a loss of information and produce overly long tokens that are difficult to train, thereby impairing overall performance.

In this work, we address the aforementioned issues from an information-theoretic perspective, drawing inspiration from structural entropy (SE) (Li and Pan 2016; Cao et al. 2024b), a metric that assesses the amount of information contained in a graph (Yang et al. 2024a; Zeng et al. 2024; Yang et al. 2024d,c; Cao et al. 2024a; Peng et al. 2024; Zou et al. 2024). We present SECodec, a novel speech representation codec tokenizer based on structural entropy, which can automatically determine the appropriate codebook size and integrate structural information into the quantization process. Experiments demonstrate that these approaches effectively mitigate the information loss problem prevalent in existing speech discretization methods. Our main contributions are:

• We model the speech representation codec from an information-theoretic perspective. Compared to previous methods that use $k$ -means, the proposed SECodec, by introducing structural information and entropy guidance, learns a more compressive and informative codebook without requiring a predetermined codebook size. To the best of our knowledge, we are the first to apply structural entropy (SE) minimization for a speech representation codec.   
• To address the issue of audio distortion when controlling codebook size, we propose a new quantization method that iteratively selects appropriate clusters for the added original speech features using a SE heuristic function. This approach significantly enhances the quality of information in the speech tokens.   
• Extensive experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, while SESLM surpasses VALL-E in zero-shot textto-speech tasks on a multi-speaker benchmark dataset.

sible with a step of random walk on a graph. SE is a measurement of graph complexity by encoding tree structures via characterizing the uncertainty of the hierarchical topology of graphs. The structural entropy of graph $G$ is defined on an associated encoding tree $\tau$ , revealing the amount of uncertainty that remained in $G$ after encoded by $\tau$ . Through structural entropy minimization, the optimized hierarchical clustering result of vertices in $G$ is retained by $\tau$ . We present the formal definitions of encoding tree and SE as follows.

Definition 1. Let $G = ( V , E , W )$ be an undirected weighted graph, where $V = \{ v _ { 1 } , . . . , v _ { n } \}$ is the vertex set, $E$ is the edge set, and $W \in R ^ { n \times \bar { n } }$ is the edge weight matrix.

1) The encoding tree $\tau$ of $G$ is a hierarchical rooted tree where each tree node $\alpha$ associates with a vertex set ${ \cal T } _ { \alpha }$ .

2) The root node $\lambda$ of $\tau$ associates with $T _ { \lambda } = V$ and each leaf node v associates with $T _ { v }$ containing a vertex in $V$ .

3) For each non-leaf node $\alpha \in { \mathcal { T } }$ , the successors of $\alpha$ are associated with disjoint vertex subsets, and the union of these subsets is $T _ { \alpha }$ .

Definition 2. The structural entropy of $G$ given by $\tau$ is defined as follows:

$$
\begin{array} { r } { \mathcal { H } ^ { \mathcal { T } } ( G ) = \displaystyle \sum _ { \alpha \in \mathcal { T } , \alpha \neq \lambda } \mathcal { H } ^ { \mathcal { T } } ( G ; \alpha ) = } \\ { \displaystyle \sum _ { \alpha \in \mathcal { T } , \alpha \neq \lambda } - \frac { g _ { \alpha } } { \mathcal { V } _ { G } } l o g _ { 2 } \frac { \mathcal { V } _ { \alpha } } { \mathcal { V } _ { \alpha ^ { - } } } , } \end{array}
$$

where $\mathcal { H } ^ { T } ( G ; \alpha )$ is the assigned structural entropy of $\alpha$ , $g _ { \alpha }$ is the cut, i.e., the sum of edge weights between vertices in and not in $T _ { \alpha }$ , $\nu _ { \alpha }$ and $\nu _ { G }$ are the volumes, i.e., the sum of vertex degrees in $T _ { \alpha }$ and $G$ , respectively. The structural entropy of $G$ is defined as ${ \mathcal { H } } ( G ) = \operatorname* { m i n } _ { T } \{ { \mathcal { H } } ^ { T } ( G ) \}$ , where $\tau$ ranges over all possible encoding trees. The vertex sets associated with tree nodes form a clustering of vertices in $V$ .

Definition 3. The $K$ -D structural entropy is the structural entropy given by the encoding trees with the height of at most $K$ . When $K = 2$ , the encoding tree represents graph partitioning, which can be used to perform partitioning clustering. A 2-D encoding tree $\tau$ can be formulated as a graph partitioning $\mathcal { P } = \{ \bar { \mathcal { X } } _ { 1 } , \mathcal { X } _ { 2 } , . . . , \mathcal { X } _ { L } \}$ of $V$ , where $\mathcal { X } _ { i }$ is a vertex subset called module associated with the $i$ -th children of root $\lambda$ . The structural entropy of $G$ given by $\mathcal { P }$ is defined as:

$$
\begin{array} { r } { \mathcal { H } ^ { \mathcal { P } } ( G ) = - \displaystyle \sum _ { \mathcal { X } \in \mathcal { P } } \displaystyle \sum _ { v _ { i } \in \mathcal { X } } \frac { g _ { i } } { \mathcal { V } _ { G } } l o g _ { 2 } \frac { d _ { i } } { \mathcal { V } _ { \mathcal { X } } } } \\ { - \displaystyle \sum _ { \mathcal { X } \in \mathcal { P } } \frac { g _ { \mathcal { X } } } { \mathcal { V } _ { G } } l o g _ { 2 } \frac { \mathcal { V } _ { \mathcal { X } } } { \mathcal { V } _ { G } } , } \end{array}
$$

where $d _ { i }$ is the degree of vertex $v _ { i } , g _ { i }$ is the cut, i.e., the sum of edge weights connecting $v _ { i }$ and other vertices, $\nu _ { x }$ and $\nu _ { G }$ are the volumes, i.e., the sum of vertex degrees in module $\mathcal { X }$ and graph $G$ , respectively, and $g _ { \mathcal { X } }$ is the cut, i.e., the sum of edge weights between vertices in and not in module $\chi$ .

# 2 Preliminary

Structural entropy (SE) (Li and Pan 2016) is defined as the minimum number of bits to encode the vertex that is acces

# 3 The Method

Figure 1 presents an overview of SECodec. Our model is based on the RVQ-GANs (Du et al. 2024) framework, akin to

C.1 Codebook Set B.1 Vanilla 2D SE minimization e1 e2 G {x1} G Initial Optimal Oe {x4} {x1} P C A.1 speech feature Cluster 1 000 G {x3} O 00 0 Add x3 T 112 x8 x10 {x12} P G C.2 Codebook Set x10000 5 O ... e1 e2 Cluster 2 8 e x1 x3x4x7x11x12X2x568xgx10 10000 [x2} 00000 e T A.2 s e graph x2x5x6x8x9x10 B.3 Codebook C.3 $G = ( V , E , W )$ … Cluster 1024 Oe3 O B.2 Hierarchical 2D SE minimization 1 2 3 4 5 6 7 468 C e 心 A Speech Feature B Codebook Construction via Hierarchical and Disentangled 2D SE Minimization C Quantization via Node Graph Construction Game-based 2D SE Minimization

SoundStream (Zeghidour et al. 2021) and EnCodec (De´fossez et al. 2024). However, we employ 2D structural entropy to optimize both the codebook initialization and the quantization process, resulting in more compressive codebook and more informative tokens. We begin by formalizing the task. Subsequently, we propose a novel structural entropy-based approach for codebook construction. We then present our informative quantization process. Finally, we introduce the training objective and design the SESLM.

# 3.1 Problem Formalization

Considering the input speech feature $X = [ x _ { 1 } , . . . , x _ { T } ] \in$ $R ^ { H \times T }$ from a pre-trained convolutional network, where $H$ is the dimension of the speech representation and $T$ is the length of the sequence. we construct a speech feature graph $G = ( V , E , W )$ . Here $V = \{ v _ { 1 } , v _ { 2 } , . . . , v _ { n } \}$ is the set of vertices corresponding to speech features in $X$ , $E$ represents the set of edges connecting the vertices, and $W$ represents the set of edge weights measuring the similarities between every frame of speech feature. For two frames of speech feature $x _ { i } , x _ { j } \in X$ , we measure their the cosine similarity. Partitioning $G$ results in $\{ \mathbf { e } _ { 1 } , . . . , \mathbf { e } _ { i } , . . . , \mathbf { e } _ { j } , . . . , \mathbf { e } _ { K } \} , \mathbf { e } _ { i } \subset V , \mathbf { e } _ { i } \cap \mathbf { e } _ { j } = \emptyset ,$ which represents a partition of $V$ containing $K$ clusters (sets) of speech features. These clusters correspond to the codebook $\mathcal { E } _ { c o d e b o o k } = [ \mathbf { e } _ { 1 } , . . . , \mathbf { e } _ { K } ]$ .

# 3.2 Codebook Construction via Hierarchical and Disentangled 2D SE Minimization

Speech feature graph partitioning decodes $G$ into $\mathcal { P }$ , which defines the size of the codebook in the form of speech feature clusters. A faithful decoding of the speech feature correlations in $G$ assigns related speech features to the same cluster and unrelated ones to different clusters. Previous RVQ-based speech codec methods use $k$ -means to initialize the codebook space. These empirically defined codebooks, which must be predetermined, lead to a loss of information and result in overly long tokens that are difficult to train, consequently impairing overall performance. To address this issue, SECodec conducts codebook partitioning under the guidance of 2D structural entropy (SE) minimization. This approach reveals the essential second-order (cluster-wise) structure inherent in the raw graph without prior knowledge of the number of speech feature clusters.

Li and Pan (2016) propose a vanilla greedy 2D structural entropy (SE) minimization algorithm that repeatedly merges any two nodes in the encoding tree $\tau$ resulting in the largest decrease in 2D SE until reaching the minimum possible value. This process partitions a graph without supervision or a predetermined total number of clusters. However, this vanilla 2D SE minimization algorithm has a time complexity of $O ( | V | ^ { 3 } )$ , making it prohibitively slow for large and complex graphs. Furthermore, the ultimate goal of our clustering is to construct codebook. The column vectors in the codebook need to be spatially distributed as far apart as possible and avoid overlapping to ensure effective representation and diversity of the speech features. To address these challenges, we propose to minimize 2D SE for construct codebook in a hierarchical and disentangled manner, shown in Algorithm 1. Specifically, each speech feature $x _ { 1 } , . . . , x _ { T }$ is placed in its own cluster (line 1). These clusters are then divided into subsets of size $n$ (line 3-5), and within each subset, the vanilla greedy algorithm is used to merge the clusters into new ones (lines 6-16). The newly formed clusters proceed to the next iteration (line 17). This iterative process continues until all speech feature clusters are considered simultaneously (lines 18-19). If no clusters within a subset can be merged at any point, the subset size $n$ is increased to allow more clusters to be considered together for potential merging (lines 20-21). Finally, we extract the corresponding codebook by minimizing the mutual information between each vector within the codebook (lines 22-34). Figure 1A shows the speech feature graph construction on nodes $x _ { 1 }$ to $x _ { 1 0 0 0 0 }$ . Figure 1B illustrates codebook construction process: initially $x _ { 1 }$ to $x _ { 1 0 0 0 0 }$ are in separate clusters. Clusters of size $n = 1 0 2 4$ are considered at a time to form a subgraph $G ^ { \prime }$ . Clusters in each $G ^ { \prime }$ are merged using the vanilla 2D SE minimization to form $\mathcal { P } ^ { \prime }$ (Figure 1B.1). The partitions from the previous iteration are carried over to the next, as shown by the blue curved arrows in Figure 1B.2. The process concludes when a $\mathcal { P } ^ { \prime }$ that encompasses all the speech features is achieved. To further enhance the codebooks’ ability to represent the diversity of speech, we introduce a mutual information learning algorithm to disentangle the central features of each cluster in $\mathcal { P } ^ { \prime }$ . For the disentanglement between $\mathbf { e } _ { i }$ and $\mathbf { e } _ { j }$ . The variational contrastive log-ratio upper bound (vCLUB) (Cheng et al. 2020) is used to compute the upper bound of mutual information (MI) for irrelevant information of the e, decreasing the correlation among different clusters’ representation:

$$
\hat { \mathcal { T } } ( \mathbf { e } _ { i } , \mathbf { e } _ { j } ) = \frac { 1 } { \mathcal { N } ^ { 2 } } \sum _ { \mathcal { M } = 1 } ^ { \mathcal { N } } \sum _ { \mathcal { T } = 1 } ^ { \mathcal { N } } [ \log f _ { \psi } ( \mathbf { e } _ { i \mathcal { M } } \vert \mathbf { e } _ { j _ { \mathcal { M } } } )  \\  - \log f _ { \psi } ( \mathbf { e } _ { j _ { \mathcal { T } } } \vert \mathbf { e } _ { i \mathcal { M } } ) ] ,
$$

where $\{ \mathbf { e } _ { i } , \mathbf { e } _ { j } \} \in \mathbf { e }$ , $\mathcal { N }$ represents the samples from $\mathbf { e } _ { i }$ and $\mathbf { e } _ { j }$ . $f _ { \psi } ( \mathbf { e } _ { i } | \mathbf { e } _ { j } )$ is a variational distribution with parameter $\psi$ to approximate $f ( \mathbf { e } _ { i } | \mathbf { e } _ { j } ) . { \hat { \boldsymbol { \cal T } } }$ is the unbiased estimator for vCLUB with samples $\{ \mathbf { e } _ { i \mathcal { M } } , \mathbf { e } _ { j _ { \mathcal { T } } }$ . The indexes $\mathcal { M }$ and $\mathcal { I }$ are the samples of $\mathbf { e } _ { i }$ and $\mathbf { e } _ { j }$ . By minimizing Eq. 3, we can decrease the correlation among accent features $\mathbf { e } _ { i }$ and speech features $\mathbf { e } _ { j }$ . Finally, the central features of each cluster in $\mathcal { P } ^ { \prime }$ are concatenated to form the codebook $\mathcal { E } _ { c o d e b o o k }$ columns in Figure 1B.3. In summary, SECodec constructs a compressive codebook from complex speech feature graphs in an unsupervised and disentangled manner.

# 3.3 Quantization via Node Game-based 2D SE Minimization

The quantization of previous RVQ-based speech codec methods is to compare the input vectors with the vectors in the codebook and extract the indexes of the most similar vectors (D´efossez et al. 2024; Zeghidour et al. 2021). However, the comparison is performed by simply calculating the Euclidean distance between vectors. In high-dimensional space, the Euclidean distance tends to be uniformly distributed, which can distort the results and affect the quality of the quantized tokens. Consequently, the tokens obtained after quantization may lack sufficient real information when the size of the codebook is controlled within a reasonable range. To address this issue, SECodec views quantization as a process where graph nodes dynamically categorize subgraphs (Zeng, Peng, and Li 2024; Yang et al. 2024b; Sun et al. 2024) as shown in Figure 1C.1, treating new input features as added nodes in Figure 1C.2. In this approach, added nodes iteratively select the appropriate clusters through a structural entropy heuristic function, the selected clusters then correspond to the speech tokens as illustrated in Figure 1C.3. Specifically, given a speech feature graph and its corresponding set of codebooks $E = [ \mathbf { e } _ { 1 } , . . . , \mathbf { e } _ { i } , . . . , \mathbf { e } _ { K } ]$ . The added speech feature node $x$ selects the current codebook $\mathbf { e } _ { i }$ , changing the codebook set to $E ^ { \prime } = [ { \bf e } _ { 1 } , . . . , { \bf e } _ { i } ^ { \prime } , . . . , { \bf e } _ { K } , \{ x \} ] ( { \bf e } _ { i } = { \bf e } _ { i } ^ { \prime } \cup \{ x \} )$ . At this point, the change of the speech feature graph’s 2D SE is formalized as:

$$
\begin{array} { r } { \Delta _ { s e l e c t } ( x , { \mathbf { e } _ { i } } ) = \mathcal { H } ^ { T } ( G ) - \mathcal { H } ^ { T ^ { \prime } } ( G ) } \\ { = \displaystyle \sum _ { n = 1 } ^ { | E | } H ^ { ( 2 ) } ( { \mathbf { e } _ { n } } ) - \displaystyle \sum _ { n = 1 } ^ { | E ^ { \prime } | } H ^ { ( 2 ) } ( { \mathbf { e } _ { n } ^ { \prime } } ) } \\ { = - \frac { g _ { { \mathbf { e } _ { i } } } } { \mathcal { V } _ { G } } l o g \frac { \mathcal { V } _ { { \mathbf { e } _ { i } } } } { \mathcal { V } _ { G } } + \frac { g _ { { \mathbf { e } _ { i } ^ { \prime } } } } { \mathcal { V } _ { G } } l o g \frac { \mathcal { V } _ { { \mathbf { e } _ { i } ^ { \prime } } } } { \mathcal { V } _ { G } } } \\ { - \frac { g _ { { \mathbf { e } _ { i } ^ { \prime } } } } { \mathcal { V } _ { G } } l o g \frac { \mathcal { V } _ { { \mathbf { e } _ { i } ^ { \prime } } } } { \mathcal { V } _ { { \mathbf { e } _ { i } } } } - \frac { d _ { x } } { \mathcal { V } _ { G } } l o g \frac { \mathcal { V } _ { G } } { \mathcal { V } _ { { \mathbf { e } _ { i } } } } , } \end{array}
$$

where $\Delta _ { s e l e c t } ( x , \mathbf { e } _ { i } )$ represents the change of the 2D SE when node $x$ selects cluster $\mathbf { e } _ { i }$ , and $\scriptstyle { \mathcal { T } } ^ { \prime }$ denotes the encoding tree corresponding to the codebooks set $\mathcal { E } ^ { \prime }$ . $\mathcal { H } ^ { T ^ { \prime } } ( G )$ and ${ \mathcal { H } } ^ { \breve { T } } ( G )$ represent the 2D SE of the graph under $E$ and $E ^ { \prime }$ , respectively. $\nu _ { G }$ , $\mathcal { V } _ { \mathbf { e } _ { i } }$ , and $\mathcal { V } _ { \mathbf { e } _ { i } ^ { \prime } }$ denote the volumes of the graph, cluster $\mathbf { e } _ { i }$ , and cluster ${ \bf e } _ { i } ^ { \prime }$ , respectively. $g _ { \mathbf { e } _ { i } }$ and $g _ { \mathbf { e } _ { i } ^ { \prime } }$ correspond to the total cut edge weights of $\mathbf { e } _ { i }$ and ${ \bf e } _ { i } ^ { \prime }$ , respectively. The added node only selects and joins the codebooks cluster with the smallest change value of 2D SE, which is formalized as:

$$
t = M i n ( \Delta _ { s e l e c t } ( x , { \bf e } _ { i } ) ) ,
$$

where $t$ represents the index of the target cluster, and the Min operation finds the codebooks index corresponding to the smallest 2D SE change value.

# 3.4 Training Objective

In terms of training objective, we follow the setup of (Du et al. 2024). The training objective consists of three components: reconstruction loss terms, adversarial loss terms, and RVQ commit losses. In the time domain, the L1 distance between the original speech and the reconstructed speech is minimized. In the frequency domain, both L1 and L2 distances are minimized across multiple Mel and magnitude spectra. For adversarial losses, SECodec incorporates several discriminators, including a multi-scale discriminator (MSD), a multi-period discriminator (MPD), and a multi-scale STFTbased (MSTFTD) discriminator.

# 3.5 SESLM

We build a structural entropy-based speech language model upon SECodec. Consisting of autoregressive and nonautoregressive models, it can hierarchically model information in speech. Compared to VALL-E (Wang et al. 2023), we include the input of speech tokens in the autoregressive model part. We believe that the speech tokens extracted by SECodec are richer in speech information, which benefits speech language model training. The model learns to perform conditional generation of a neural code sequence, denoted as $\mathcal { O }$ , based on two input prompts: textual prompt $u$ and acoustic prompt $s$ . The training objective is formulated as:

$$
\begin{array} { r } { \mathcal { L } _ { A R } = - \displaystyle \sum _ { t = 1 } ^ { N } l o g P ( \mathcal { O } _ { t , 1 } | u , \mathcal { S } , \mathcal { O } _ { < t , 1 } ; \theta _ { A R } ) , } \\ { \mathcal { L } _ { N A R } = - \displaystyle \sum _ { l = 2 } ^ { 8 } l o g P ( \mathcal { O } _ { : l } | u , \mathcal { S } , \mathcal { O } _ { : , < l } ; \theta _ { N A R } ) , } \end{array}
$$

<html><body><table><tr><td>Algorithm 1:Codebook construction via hierarchical and</td></tr><tr><td>disentangled 2D SE minimization.</td></tr></table></body></html>

<html><body><table><tr><td>Input: Speech feature graph G = (V,E,W),sub-graph size n.</td><td></td></tr><tr><td>1 P←(x|x∈V)</td><td>Output: &codebook = [e1,.*, ek];</td></tr><tr><td>2while True do</td><td></td></tr><tr><td>3</td><td>{Ps} ←consecutively remove the first,</td></tr><tr><td>4</td><td>min(n,size of the remaining part of P) clusters,</td></tr><tr><td>5</td><td></td></tr><tr><td>6</td><td>from P that form a set Ps;</td></tr><tr><td>7</td><td>forP∈{Ps}do</td></tr><tr><td></td><td>V' ← combine all the clusters in Ps ;</td></tr><tr><td>8</td><td>E'←{e∈E,both endpoints of e ∈V'};</td></tr><tr><td>9</td><td>G' ←(V',E');</td></tr><tr><td>10</td><td>T' ← add a root tree node 入;</td></tr><tr><td>11</td><td>for clusterC ∈ Ps do</td></tr><tr><td>12</td><td>Add a tree node α to T',α−= λ,Tα =C;</td></tr><tr><td>13</td><td>for speech feature x EC do</td></tr><tr><td>14</td><td>Add a tree node y to T';</td></tr><tr><td>15</td><td>γ=α,T={x};</td></tr><tr><td>16</td><td>P' ← run vanilla 2D SE minimization;</td></tr><tr><td>17</td><td>Append P'to P;</td></tr><tr><td>18</td><td>if |{V}|=1 then</td></tr><tr><td>19</td><td>Break;</td></tr><tr><td>20</td><td>if P is the same as at the end of last iteration then</td></tr><tr><td>21</td><td>n←2n;</td></tr><tr><td></td><td>22 P= {x1,x2,.., XK}</td></tr><tr><td>23</td><td>e←[]</td></tr><tr><td></td><td>24 for Xi ∈P do</td></tr><tr><td>25</td><td>e←O,count ←0;</td></tr><tr><td>26</td><td>forx∈Xido</td></tr><tr><td>27</td><td>e←ei+ x,count ←count +1;</td></tr><tr><td>28</td><td>Append ei to e;</td></tr><tr><td>29</td><td>e = [e1,., eκ]</td></tr><tr><td></td><td>30 Minimize the mutual information in e via Eq. 3</td></tr><tr><td>31</td><td>Ecodebook = [e1,.., ek]</td></tr><tr><td>32</td><td>return Ecodebook ·</td></tr></table></body></html>

# 4 Experiment 4.1 Experimental Setup

Data For SECodec training, we use the LibriSpeech (Panayotov et al. 2015) dataset. At each training iteration, a 3.2 second segment is randomly cropped from the speech samples. For zero-shot TTS, we train AR and NAR models on the English subset of the Multilingual LibriSpeech dataset (Pratap et al. 2020), which contains 44,000 hours of transcribed speech data derived from LibriVox audiobooks. We select speech samples with durations ranging from 3 to 14 seconds for the training data. All speech data is sampled at a rate of $1 6 \mathrm { k H z }$ .

Model SEcodec is built on the framework of RVQ-GANs, following the same pattern as Funcodec (Du et al. 2024). SEcodec uses the convolutional-based encoder-decoder network from EnCodec, which performs temporal downscaling with a chosen striding factor. For zero-shot TTS experiments, AR model and NAR model are both 12-layer Transformer decoders with 16 attention heads, an attention dimension of 1024 and the FFN dimension of 4096.

Training During the training stage, we randomly clip a continuous segment of 3.2 seconds from an utterance, which is considered as a training sample. Before being fed into the encoder, the segment undergoes root-mean-square (RMS) normalization. The reconstructed output is rescaled using inverse normalization to calculate losses. We train the models on single 3090Ti GPUs with a total batch size of 16. Under the adversarial training framework, we update the codec model 300,000 times. To prevent the discriminator from becoming too dominant, we only update it when its loss exceeds that of the codec model.

Baselines For SECodec, we consider two baselines: 1) EnCodec (De´fossez et al. 2024), and 2) SpeechTokenizer (Zhang et al. 2024), a state-of-the-art speech tokenizer for speech language models. For SESLM, we consider two baselines: 1) VALL-E (Wang et al. 2023), and 2) ULSM (Zhang et al. 2024).

# 4.2 Metrics

where $\mathcal { O } _ { < t , 1 } = [ \mathcal { O } _ { 1 , 1 } , . . . , \mathcal { O } _ { t - 1 , 1 } ]$ , while $\theta _ { A R }$ represents the AR Transformer model parameters. $\theta _ { N A R }$ represents the NAR model parameters, while $\mathcal { O } _ { : , l }$ denotes the entire sequence of $\mathcal { O } _ { t , l }$ for the lth layer, and $0 _ { : , < l } =$ $[ \mathcal { O } _ { : , 1 } , . . . , \mathcal { O } _ { : , l - 1 } ]$ for $l = 2 , \ldots , 8$ . Note that the AR model in SESLM is conditioned on the concatenated embeddings of both the acoustic and textual prompts. This formulation differs from that of VALL-E, where the AR model is only conditioned on the textual prompt and the past acoustic history. We validate the effectiveness of the structural entropy-based speech language model on the zero-shot TTS task. During inference, text input is converted to a phoneme sequence and the speech prompt to speech tokens. These are concatenated to form the prompts for both the AR and NAR models. The tokens generated by the AR and NAR models are then concatenated to construct the speech token matrix. Finally, the SECodec decoder is used to generate the waveform conditioned on the complete token matrix.

For speech reconstruction evaluation, we randomly sampled 300 speech samples from the LibriSpeech test set, considering both subjective and objective evaluation metrics. For objective metrics, we used mel-cepstrum distortion (MCD) (Toda, Black, and Tokuda 2007), root mean square errors (RMSE) (Luo et al. 2017), and ViSQOL (Hines et al. 2015) to assess speech quality. Additionally, we evaluated content accuracy through Word Error Rate (WER) by transcribing the speech using the Whisper en-medium model (Radford et al. 2023). For subjective metrics, we use MUSHRA following SpeechTokenizer (Zhang et al. 2024).

For evaluating SESLM, we perform zero-shot text-tospeech assessments using the VCTK (Veaux et al. 2016) dataset, which includes recordings from 108 speakers with no overlap between the training data and the VCTK dataset. For each speaker, we randomly select a 3-second utterance as the prompt and use the text from a different utterance as the input. Objective metrics include evaluating speaker similarity and Word Error Rate (WER). To assess speaker similarity, we utilize the WavLM-TDCNN (Chen et al. 2022) speaker embedding model to measure the similarity. We report the similarity with respect to the resynthesized audio context by its vocoder (SIM-r) and the similarity against the original audio context (SIM-o). As subjective metrics, we utilize Mean Opinion Score (MOS) for evaluating subjective audio quality (QMOS) and Similarity MOS (SMOS) for assessing subjective audio similarity.

<html><body><table><tr><td>Methods</td><td>Kin codebook WER↓</td><td></td><td>MCD↓</td><td>RMSE↓</td><td>ViSQOL↑MUSHRA↑</td><td></td><td>Param</td></tr><tr><td>Ground Truth</td><td></td><td>4.58</td><td></td><td></td><td></td><td>91.46</td><td></td></tr><tr><td>EnCodec</td><td>468</td><td>8.87</td><td>7.21</td><td>36.32</td><td>4.03</td><td>71.32</td><td>19.19M</td></tr><tr><td>EnCodec</td><td>1024</td><td>5.01</td><td>5.89</td><td>32.59</td><td>4.37</td><td>79.86</td><td>23.76M</td></tr><tr><td>EnCodec</td><td>2048</td><td>4.98</td><td>6.79</td><td>35.12</td><td>4.11</td><td>75.11</td><td>32.18M</td></tr><tr><td>EnCodec</td><td>4096</td><td>91.23</td><td>10.34</td><td>49.72</td><td></td><td></td><td>82.71M</td></tr><tr><td>SpeechTokenizer</td><td>1024</td><td>5.04</td><td>6.23</td><td>34.84</td><td>4.30</td><td>90.55</td><td>120M</td></tr><tr><td>SECodec (Nodes:100oo,Edges:>0.2)</td><td>468</td><td>4.63</td><td>5.12</td><td>31.97</td><td>4.40</td><td>90.79</td><td>19.19M</td></tr><tr><td>Nodes Ablations (Edges:>0.2)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SECodec (Nodes:100000)</td><td>1212</td><td>4.91</td><td>6.37</td><td>32.79</td><td>4.31</td><td>88.21</td><td>25.31M</td></tr><tr><td>SECodec (Nodes:50000)</td><td>768</td><td>4.82</td><td>5.57</td><td>32.45</td><td>4.37</td><td>90.12</td><td>22.27M</td></tr><tr><td>Edges Ablations (Nodes:10000)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SECodec (Edges:>0.5)</td><td>1212</td><td>4.87</td><td>6.19</td><td>31.81</td><td>4.29</td><td>86.33</td><td>25.31M</td></tr><tr><td>Methods Ablations</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SECodec w/o SE Codebook</td><td>468</td><td>4.89</td><td>6.03</td><td>32.37</td><td>4.32</td><td>89.35</td><td>19.19M</td></tr><tr><td>SECodec w/o SE Quantization</td><td>468</td><td>4.71</td><td>5.67</td><td>33.89</td><td>4.39</td><td>87.62</td><td>19.19M</td></tr></table></body></html>

Table 1: Comparison of WER, MCD, RMSE, ViSQOL and MUSHRA of speech reconstruction on the LibriSpeech datasets.

# 4.3 Main Results

Speech Reconstruction Table 1 provides a detailed summary of the speech reconstruction experiment results. SECodec outperforms both SpeechTokenizer and EnCodec by achieving a lower WER, highlighting its superior capability to retain content information. Furthermore, SECodec surpasses SpeechTokenizer and EnCodec in both MCD, RMSE, VISQOL and MUSHRA scores, underscoring its enhanced proficiency in producing high-quality speech.

Zero-shot TTS Table 2 illustrates that our SESLM achieves a lower Word Error Rate (WER) compared to USLM and VALL-E. This finding underscores SECodec’s capability to enhance the precision of content information modeling. Moreover, SESLM exhibits superior speaker similarity, suggesting that the modeled speech’s structural information more effectively facilitates the extraction of paralinguistic features.

Ablation Study Furthermore, we conducted an ablation study to analyze the performance effects of different components in SECodec, with the results presented in Table 1 . The findings show that when SE quantization is removed, the model still performs well and outperforms most of the baseline models, but there is a noticeable decrease in speech quality. Additionally, when the SE codebook component is excluded, the results are significantly poorer, highlighting its critical importance to the overall performance.

# 4.4 Analysis

Choice of Nodes and Edges for SE We first analyzed the impact of different codebook sizes on the performance of EnCodec. It is evident that selecting an appropriate codebook size is crucial; a small codebook size (e.g., 468) leads to performance degradation, while an excessively large codebook (e.g., 4096) results in the model’s failure to converge. Furthermore, we examined the influence of the number of nodes and edges in the speech feature graph during the SE minimization process. Table 1 presents the impact of varying the number of nodes and edges on the performance of SECodec. It can be observed that different numbers of nodes and edges, constructed with different similarity thresholds, lead to varying codebook sizes, which in turn affect the results. The best performance in speech reconstruction was achieved with 10,000 nodes and an edge threshold of 0.2. A similar conclusion is drawn from the ablation experiments on SESLM, as shown in Table 2. On the other hand, SECodec achieves the best performance while also having the smallest model parameter size, further highlighting the importance of automatically selecting the appropriate codebook size.

Codebook and Quantized Output Visualization To demonstrate the compressive and informative nature of the codebook learned by SECodec, we first visualized each column vector in the codebooks initialized by different methods using t-SNE. As illustrated in Figure 2(a) and Figure 2(b), the codebook initialized with $\mathbf { k }$ -means exhibits an uneven distribution in space, with cluster centers entangled and overlapping. In contrast, the codebook initialized with SECodec is more evenly distributed and discrete. To further illustrate the effectiveness of the codebook learned by SECodec, we visualized the quantized outputs of 1,000 speech samples. As shown in Figure 2(c) and Figure 2(d), the features of different speech samples obtained by SECodec are more disentangled and distinguishable compared to those obtained by EnCodec.

Effectiveness of SE Quantization Figure 3 displays the spectrogram and F0 (fundamental frequency) of both the reference speech and the resynthesized (synthesized) speech

<html><body><table><tr><td>Methods</td><td>Codec</td><td>WER↓SIM-0 ↑</td><td></td><td>SIM-r ↑</td><td>QMOS ↑</td><td>SMOS↑</td></tr><tr><td>Ground Truth</td><td></td><td>1.92</td><td>0.698</td><td>n/a</td><td>3.89(±0.18) 3.92(±0.16)</td><td></td></tr><tr><td>SECodec resynthesis</td><td>SECodec</td><td>2.02</td><td>0.682</td><td>n/a</td><td>3.82(±0.12) 3.88(±0.11)</td><td></td></tr><tr><td>VALL-E</td><td>EnCodec</td><td>7.09</td><td>0.501</td><td>0.412</td><td>3.08(±0.10) 3.31(±0.11)</td><td></td></tr><tr><td>USLM</td><td>SpeechTokenizer</td><td>5.79</td><td>0.602</td><td>0.587</td><td>3.50(±0.13) 3.41(±0.12)</td><td></td></tr><tr><td>SESLM(Nodes:10000,Edges:>0.2)</td><td>SECodec</td><td>4.97</td><td>0.634</td><td>0.611</td><td>3.63(±0.12) 3.45(±0.11)</td><td></td></tr><tr><td>NodesAblations (Edges:>0.2)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SESLM (Nodes:100000)</td><td>SECodec</td><td>5.51</td><td>0.612</td><td>0.593</td><td>3.56(±0.11) 3.37(±0.13)</td><td></td></tr><tr><td>Edges Ablations (Nodes:10000)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SESLM (Edges:>0.5)</td><td>SECodec</td><td>5.48</td><td>0.602</td><td>0.597</td><td>3.46(±0.10) 3.33(±0.11)</td><td></td></tr></table></body></html>

Table 2: Comparison of WER, SIM-o, SIM-r, QMOS and SMOS of the proposed model and baselines on VCTK datasets.

![](images/b866bc80393db1316cf5c86c35f3d400e5f866745f3176f0f6f4cb2de73c4976.jpg)  
Figure 2: Codebook and quantized output visualization.   
Figure 3: Effectiveness of SE quantization. The black box indicates the structural details in the reference speech, the white box with a green checkmark indicates the part that our method correctly predicts, and the white box with a red cross indicates the part that the baseline method incorrectly predicts.

X X   
1 300 8000 300 8000 300 6000 6000 200 200 200 4000 4000 5 100 100 100 2000 2000 Lo 0 0.250.50 0.75 1.001.251.501.75 2.00 0.250.50 0.751.001.251.50 1.75 2.00 0.250.500.751.001.251.501.75 2.00 Time(s) Time(s) Time(s) F0 (a) Reference Speech (b) Resynthesized Speech of SECodec (c) Resynthesized Speech of EnCodec   
1 VN 8000 m 8000 ? X 300 6000 6000 200 4000 4000 R 100 2000 2000 一0 0.250.500.751.001.251.501.752.00 0.250.500.751.001.251.501.75 2.00 Time(s) Time(s) Time(s) (d) Reference Speech (e) Synthesized Speech of SESLM (f) Synthesized Speech of VALL-E

with identical content. It is evident that SECodec preserves the detailed features of the audio more effectively than EnCodec, particularly in maintaining the intricacies of the wave peaks at the fundamental frequency line (F0), as shown in Figure 3(a), Figure 3(b), and Figure 3(c). This observation is consistent in the speech synthesized by SESLM compared to VALL-E, as shown in Figure 3(d), Figure 3(e), and Figure 3(f), demonstrating that quantization via node game-based 2D SE minimization significantly reduces speech distortion caused by Euclidean distance-based quantization. Consequently, our method retains more detailed information in the synthesized speech.

# 5 Conclusion

Interacting with LLMs through speech has led to an increased demand for effective speech representation discretization.

To address this, we propose SECodec, a novel speech representation codec designed to convert continuous speech waveforms into discretized tokens. SECodec automatically determines the appropriate codebook size and integrates structural information into the quantization process. Extensive experiments demonstrate that SECodec outperforms EnCodec in speech reconstruction. Furthermore, we developed a Structural Entropy-based Speech Language Model (SESLM), yielding superior results in terms of the quality of the generated speech. Furthermore, the experimental results demonstrate that SECodec is capable of learning a more compact and discrete codebook, thereby generating more informative speech tokens. SECodec exhibits comparable performance to EnCodec in speech reconstruction, while SESLM outperforms VALL-E in zero-shot text-to-speech tasks.