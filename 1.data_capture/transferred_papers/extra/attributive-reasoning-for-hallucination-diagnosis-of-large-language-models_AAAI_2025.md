# Attributive Reasoning for Hallucination Diagnosis of Large Language Models

Yuyan Chen1, Zehao $\mathbf { L i } ^ { 2 }$ , Shuangjie $\mathbf { Y o u } ^ { 3 }$ , Zhengyu Chen4, Jingwen Chang1, Yi Zhang5, Weinan $\mathbf { D a i } ^ { 4 }$ , Qingpei $\mathbf { G u o } ^ { 6 }$ , Yanghua Xiao1\*

1Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University 2School of Data Science and Engineering, East China Normal University 3Georgia Institute of Technology 4Zhejiang University 5Southern University of Science and Technology 6Ant Group {chenyuyan21@m., jwchang $2 4 @ \mathrm { m }$ ., shawyh $\langle \boldsymbol { \underline { { a } } } \rangle$ fudan.edu.cn, {leepaul.private,wnd17460,youshuangjie} $@$ gmail.com, zhangyi2021 $@$ mail.sustech.edu.cn, chenzhengyu $@$ zju.edu.cn, qingpei.gqp $@$ antgroup.com

# Abstract

In recent years, large language models (LLMs) have demonstrated outstanding capabilities in various tasks. However, LLMs also have various drawbacks, especially hallucination. Hallucination refers to the generation of content that does not align with the user input, contradicts previously generated content or world knowledge. Current research on hallucination mainly include knowledge retrieval, prompt engineering, training data improvement, reinforcement learning, etc. However, these methods do not involve different categories of hallucinations which is important on hallucination analysis, and make detailed investigation for the internal state of LLMs which indicates the direction on hallucination occurrence. Therefore, in our research, we introduce an attribution framework to trace the origins of hallucinations based on the internal signals of LLMs. To support this framework, we develop a new benchmark named RelQA-Cate, which includes eight categories of hallucinations for the answers generated by LLMs. After that, we present a novel Differential Penalty Decoding (DPD) strategy for reducing hallucinations through adjusting post-probabilities of each answer. We conduct a series of experiments and the performance on answer reliability has significant improvement, achieving $2 8 . 2 5 \%$ at most, which demonstrates the effectiveness of our proposed DPD and its generalization in mitigating hallucination in LLMs.

# Introduction

Large language models (LLMs) have a wide range of applications in various downstream tasks (Chen et al. 2024c,g,d,a). However, it is important to recognize that while LLMs bring about revolutionary technological advances, they also bring a series of security and privacy issues that deserve our attention, especially hallucinations (Ji et al. 2023). Hallucinations refer to the circumstances where the content generated by LLMs is incorrect to the given question, unrelated or even conflicting to the input prompt. They have various categories (Huang et al. 2023), such as factual error, representing the generated text contains obvious factual errors. For instance, in response to the question, “Who

Question: Which isthe longest river in Northern Ireland?   
Ground Truth: River Bann Prediction: River Lagan   
Prediction: River Lagan X Category: Factual error Reason: Attention head (a) (b)

was the first president of the United States?”, an incorrect answer is “Aaron Burr”. Another category is conceptual confusion which indicates LLMs mix up different concepts in responses, such as considering the solar cell as a type of “electronic device” instead of “electrical device” 1. There are also more categories of hallucinations which are not fully investigated and the underlying reasons for each category related to LLMs’ internal state are even largely uncharted.

Understanding the reasons behind each category of hallucination in LLMs is crucial. Hallucination categories can be likened to symptoms in medicine, while the reasons of these hallucinations can be compared to the etiology in medical diagnoses. In the case of hallucinations, these causes are typically linked to LLMs’ internal states (i.e. representations of the input). For instance, as shown in Fig. 1, if we not only identify hallucination occurs based on wrong prediction “River Lagan” (see Fig. 1 (a)), but also recognize this is a “Factual error,” which is possibly due to the LLMs overly focusing on a certain attention head of a particular layer (see Fig. 1 (b)), we can address the issue by adjusting the attention distribution of that layer or retraining the relevant head.

However, current research mainly focus on other methods except investigating the internal state of LLMs in reducing hallucinations, including knowledge retrieval (Chen, Xiao, and Liu 2022; Shi et al. 2023; Peng et al. 2023; Chen et al. 2024b,f), prompt engineering (Zhang et al. 2023; Touvron et al. 2023; Chen et al. 2023f), training data improvement (Zhong et al. 2021; Chen et al. 2023b; Cao, Kang, and Sun 2023; Chen et al. $2 0 2 3 \mathrm { g }$ ), reinforcement learning (Yu et al. 2023; Sun et al. 2023; Chen et al. 2024e). But knowledge retrieval may not fully encompass all areas or the latest information. Prompt engineering has high demands on how users input their queries. Improving training data performs bad when faced with novel situations. Reinforcement learning may need a complex training process. A better approach is to examine the differences in the internal states of LLMs when producing correct answers and hallucinations. Adjustments can be made to the LLM’s output to specifically target and reduce hallucinations.

Therefore, we propose an attribution framework that traces the reasons of hallucinations generated by LLMs based on their internal signals. The internal causes in our research represent the differences between hallucination output and the correct output based on the representation of hidden layers, self-attention outputs, and high-contribution words, among others. To support this framework, we first design eight hallucination categories inspired by Li et al. (2024) and classify the incorrect answers generated by LLMs into these categories using ChatGPT based on the RelQA dataset (Chen et al. 2023d) to obtain a new benchmark named RelQA-Cate. Under the guidance of the framework and the benchmark, we realize a novel strategy named Differential Penalty Decoding (denoted as DPD) 2. The significance of DPD is that it does not require additional annotation costs and helps open the black box of LLMs. We conduct extensive experiments, demonstrating that DPD has a great effect across various datasets and LLMs in mitigating hallucinations.

# Attribution Framework

To trace the causes of LLMs producing hallucinations, we annotate the typical categories of hallucinations and reveal the internal states of LLMs to establish a hallucinations attribution framework.

# Hallucination Category Annotation

We adopt several powerful LLMs, including LLaMA-7B, LLaMA2-7B, Baichuan-7B and Mistrial-7B to generate answers in RelQA (Chen et al. 2023d) dataset. We adjust the temperature as 0 to generate unchangeable answers. Next we adopt ChatGPT 3 to categorize hallucinations of answers generated by LLMs into eight categories including Factual Error Hallucination, which represents a text includes incorrect facts; Logical Error Hallucination, which represents an answer is illogical or contradicts itself; Conceptual Confusion Hallucination, which represents an answer mixes up different concepts; Vagueness Hallucination, which represents an answer that is overly vague; Lack of Commonsense Hallucination, which represents an answer contradicts commonsense; Over-generalization Hallucination, which represents an answer is too broad and lack detail; Emotional Bias

Hallucination, which represents an answer exhibits prejudice or an emotional tone; Lack of Uncertainty Hallucination, which represents an answer is overly confident about uncertain events. Because some hallucination types might overlap, we focus on the most significant ones. The samples are shown in Table 1.

In this process, we conduct categorizing validation with three human evaluators. We have human evaluators inspect the categorization; if there is disagreement, we revise the categorization. For inconsistent opinions, we adopt the majority categorization result. If each evaluator produces different results, the data point is discarded. Finally, we select 1,500 data instances for each hallucination category in RelQA, ensuring an equal distribution of 12,000 correct answers, thereby constructing a dataset of 24,000 samples, named RelQA-Cate, serving as an evaluation dataset for assessing LLMs’ hallucinations attribution.

![](images/84c4496ae6c1d7622faf9c385ab7749a151a5e2602185b95e46c818e011cd507.jpg)  
Figure 2: Visualization of hidden layer output of the first category of hallucination. The horizontal axis represents different layers of the LLM’s internal structure, and the vertical axis represents the absolute output values of those layers.

![](images/eec1b8e85760078cfbdb9e819b872dac31f7f1fab70a2ca289f29f12f5d979ce.jpg)  
Figure 3: Visualization of two-dimensional hidden layer output of the first category of hallucination. The horizontal and vertical axes represent the absolute output values after feature dimensionality reduction.

Table 1: Samples with different categories of hallucination.   

<html><body><table><tr><td>Category</td><td>Question</td><td>Ground-truth</td><td>Predicted answer with hallucination</td></tr><tr><td>Factual error</td><td>Which is the longest river in Northern Ireland?</td><td>River Bann</td><td>River Lagan</td></tr><tr><td>Logical error</td><td>Who was the director of the 2OO1 French action horror film star- ring Mark Dacoscos?</td><td>Christophe gans</td><td>Mark Dacascos is an American actor and martial artist.</td></tr><tr><td>Conceptual confusion</td><td>Which is the third largest of the world's oceanic divisions,cov- ering 70,560.000 km2 (approximately 20% of the water on the Earth's surface),and bounded by Asia on the north,on the west by Africa,on the east by Australia,and on the south by the Southern Ocean or,depending on definition,by Antarctica?</td><td>Indian ocean</td><td>The Pacific Ocean is the largest and deepest of Earth's oceanic divisions.It extends from the Arctic Ocean.</td></tr><tr><td>Vagueness</td><td>Atthe same time,Cho wrote and published an autobiographical bookwith the same title,and the show itself was filmed and re- leased as a concert film in 2ooo.. What happened in 2000?</td><td>Cho wrote and published autobio- graphical book with same title and show itself was filmed and released as concert film in 2000</td><td>im one that iwant was filmed and released as concert film in 2000.</td></tr><tr><td>Lack of com- monsense</td><td>What do birds typically build to lay their eggs?</td><td>Nests</td><td>Tunnels</td></tr><tr><td>Over- generalization</td><td>Both Bessera and Sesleria are?</td><td>genus of perennial plants</td><td>plants</td></tr><tr><td>Emotional bias</td><td>Al-Askari's mother,as in the case of the majority of The Twelve Imams,was a slave girl who was honoured after bearing children with the title Umm walad (mother of offspring)... Why did the slave girl cry</td><td>can not answer</td><td>alaskaris mother slave girl cried because she had seen her son being endowed with divine knowledge which led him toponder deeply about purpose of life and afterlife</td></tr><tr><td>Lack of un- certainty</td><td>Nowsch,19,faces three felony charges: murder,attempted mur- der and unlawful discharge of a gun from a vehicle.How did he kill?</td><td>shooting</td><td>Im not sure.</td></tr></table></body></html>

![](images/e148fba31522c01d9e1b674dcb8102678e822ee4b804d435d36af971e1ce547b.jpg)  
Figure 4: Visualization of attention output of the first category of hallucination. The horizontal axis represents sorted LLM’s attention heads, and the vertical axis represents the indices of the attention layers.

# Internal State Analysis

We choose LLaMA2-7B (Touvron et al. 2023) as our backbone and RelQA as an example to visually analyze the differences between each category of hallucination output and the correct output.

Hidden layer outputs. We first analyze the absolute values of each hidden layer’s output across each category of hallucinations to assess the sensitivity of different hidden layers to different categories. Specifically, for a given question, we record outputs of each hidden layer of an LLM while it produces each category of hallucination in the predicted answer. We average results of a large number of samples for analysis, expecting to identify which hidden layers are more active when dealing with specific hallucinations. As shown in Fig. 2, the blue line represents correct answers and the orange line represents incorrect answers with factual error hallucination. We then reduce the highdimensional outputs of the hidden layers to two-dimensional spaces using PCA for a coarse-grained analysis, as depicted in Fig. 3 representing factual error hallucination. We observe that the purple points (i.e. correct answers) and pink points (i.e. incorrect answers with factual error hallucination) are clearly separated, indicating considerable contribution of hidden layers for model generating correct answers.

Self-attention outputs. Next, we analyze the selfattention output of each hidden layer and head for each category of hallucination to identify differences in the LLM’s focus. As shown in Fig. 4, we observe that in the higher layers, attention heads show specialized focus, with some heads concentrating on specific regions, indicating an emphasis on global information and higher-level feature integration. The attention patterns suggest that the need for refining attention mechanisms to reduce inaccuracies in generated content.

After that, we calculate the correlation between normalized self-attention outputs of each category of hallucination with Pearson correlation coefficient. As shown in Fig. 5, the coefficients between various categories of hallucinations and correct answers are generally high, indicating that the LLMs often struggles to differentiate between incorrect and correct answers. High correlation values may be one of the factors contributing to the occurrence of hallucinations.

High-contribution words. Finally, we identify highcontribution words of the each category of hallucination. We adopt gradient-based approaches (Simonyan, Vedaldi, and Zisserman 2013) to compute saliency map based on the gra

![](images/556913c25a765f312383a8bac12a62f1d94359eab4f3a5031fd3bda204df096b.jpg)  
Figure 5: Correlation of attention output of each category of hallucination.   
Figure 6: Salient words of the first category of hallucination.   
Figure 7: The decoding strategy named DPD based on the attribution framework.

Factual error #sWhich is the longest riverin Northern Ireland ?

dient of the input questions with respect to the generated answers. After that, we use a visualizer to present saliency maps for saliency scores to find high-contribution words. For example, for the first category as shown in Fig. 6, we observe “longest river” and “Northern Ireland” are emphasized. It could be an explanation of why the LLM produces hallucinations.

# Methodology

In this section, we propose a novel decoding strategy named Differential Penalty Decoding (denoted as DPD) as shown in Fig. 7.

Generating candidate answers. We first adopt an LLM to generate $k$ (set as 5) diverse candidate answers through adjusting the temperature coefficient. The diversity degree of candidate answers for a question is evaluated by Distinct2 (Li et al. 2015), a metric for assessing text diversity by calculating the proportion of unique bi-grams in generated text, which is requested over $\alpha$ (set as 0.8). These candidates reflect the LLM’s internal state.

Calculating penalty values. Next, we calculate the corresponding penalty values for each candidate answer based on LLM’s internal state through above-mentioned five dimension. We start by analyzing the absolute values of the outputs of hidden layers. To adjust the outputs of these layers to be closer to the activity levels seen when processing correct answers, we design the following penalty function:

$$
P _ { l } = \sum _ { j = 1 } ^ { N } \sum _ { i = 1 } ^ { L } \left( o _ { i j } - { \bar { o _ { i } } } \right) \cdot \mathrm { s i g n } { \left( o _ { i j } - { \bar { o _ { i } } } \right) } ,
$$

where $P _ { l }$ represents the penalty term, $o _ { i j }$ represents the absolute value of the output from the $i$ -th layer when process

Question WhatdistinctivesurfaceisthehistoricMertonStreet,thatruns parallel to High Street in the southeastern university city of Oxford? Generating candidate A1.It is a cobbled street. A1 0.5 answers A2.There are lots of people. 0.2 A3. It isan old cement road. A3 0.3 A1 10.8 A1. It is a cobbled street. A2 0.1 A2.There are lots of people A3 □ 0.1 A3. It isan old cement road. Calculating Adjusting postpenaltyvalues probabilities 0.7 Sentence:xxisx 0.8 Weight: 0.7 (a) Hidden (b)Hiden layer (c)Attetion (d)Correatation (e)Salient layer output dimension output reduction   
Answer A. It is a cobbled street.

ing the $j$ -th category of hallucination, $\bar { o _ { i } }$ signifies the average absolute output value of that same layer when generating correct answers, $L$ is the total number of hidden layers in the LLM, $N$ is the number of hallucination categories. Sign function is to ensure that when the output value is above the mean, the penalty is positive, and when it is below the mean, the penalty is negative.

Next, through PCA dimensionality reduction analysis of the LLM’s output, we design a penalty function aiming at driving the LLM’s outputs away from these abnormal areas as follows:

$$
P _ { d l } = \sum _ { j = 1 } ^ { N } \left( \operatorname { D i s t } ( x _ { j } , { \mathcal { N } } ) - \operatorname { m e d i a n } ( \operatorname { D i s t } ( x , { \mathcal { N } } ) ) \right) ,
$$

where $P _ { d l }$ is the penalty term, $x _ { j }$ represents the PCAreduced LLM output point in the $j$ -th category of hallucination, $\mathcal { N }$ represents the center of the feature space distribution for correct answers, $\mathrm { D i s t } ( )$ is the Euclidean distance function, and $N$ is the number of hallucination categories. Using the median instead of $\mathcal { N }$ as the reference value ensures that points closer to the center have negative penalty values, while points further away have positive penalty values.

Then, we analyze the unusual attention outputs of specific layers or heads when processing different categories of hallucinations, and use the following penalty function to adjust these abnormal attention outputs:

$$
P _ { a } = \sum _ { k = 1 } ^ { N } \sum _ { i = 1 } ^ { L } \sum _ { j = 1 } ^ { H } \left( A _ { i j k } - \bar { A } \right) \cdot \mathrm { s i g n } ( A _ { i j k } - \bar { A } ) ,
$$

where $P _ { a }$ is the penalty term, $L$ and $H$ represent the number of layers and heads in the LLM, respectively, $A _ { i j k }$ is the attention output for the $k$ categories of hallucination in the $i$ -th layer and $j$ -th head, $\bar { A }$ is the average attention output for correct answers. Sign function is to ensure that the penalty is positive when the attention output value is above the average and negative when it is below the average. Because our datasets all have ground truth, the calculation of the output value of the correct answer such as $\bar { o _ { i } }$ and $\bar { A }$ is enabled. For cases lacking a correct answer, more powerful LLMs like GPT-4 or human can generate them. Additionally, with sufficiently large datasets, a reward model can be trained to score candidate answer probabilities.

After that, based on the correlation coefficient analysis of the attention outputs, we design the following penalty function to close the gap in attention distribution similarity between each type of hallucination and correct outputs:

$$
P _ { c a } = \sum _ { i = 1 } ^ { N } \left| 1 - \rho ( A _ { i } , \bar { A } ) \right| \cdot \mathrm { s i g n } ( 1 - \rho ( A _ { i } , \bar { A } ) ) ,
$$

where $P _ { c a }$ is the penalty term, $N$ is the number of hallucination categories, $A _ { i }$ is the attention output matrix for the ith category of hallucination, $\bar { A }$ is the average attention output matrix for correct answers, $\rho$ is used to calculate the correlation coefficient between the two matrices, and $| |$ is the absolute difference between two outputs. Sign function is to ensure that the penalty is positive when the correlation coefficient difference is greater than 1 and negative when it is less than 1.

Finally, for high-contribution words that play a key role in the generation of hallucinations, we design the following penalty function to reduce the LLM’s reliance on these words:

$$
P _ { w a } = \sum _ { i = 1 } ^ { N } \sum _ { w _ { i } \in W _ { i } } \left( \mathsf { C o n } ( w _ { i } ) - \mathsf { m e a n } ( \mathsf { C o n } ( W _ { i } ) ) \right) ,
$$

where $P _ { w a }$ is the penalty term, $W _ { i }$ is the set of highcontribution words for the $i$ th category of hallucination, and $\mathbf { C o n } ( w _ { i } )$ represents the contribution of word $w _ { i }$ to generating hallucinatory texts corresponding to a certain category. Take mean $( \mathrm { C o n } ( W _ { i } ) )$ as the baseline is to ensure that the penalty is positive when the contribution value is above the mean and negative when it is below the mean.

By combining all penalty items for each category of hallucination, we obtain the specific total penalty function as follows:

$$
P = \lambda _ { l } P _ { l } + \lambda _ { d l } P _ { d l } + \lambda _ { a } P _ { a } + \lambda _ { c a } P _ { c a } + \lambda _ { w a } P _ { w a } ,
$$

where $P$ is a comprehensive penalty term formed by adding together multiple specific penalty items. $\lambda _ { l } , \lambda _ { d l } , \lambda _ { a } ,$ , $\lambda _ { c a }$ and $\lambda _ { w a }$ are trainable coefficients which are positive values and the sum of them is 1.

Adjusting post-probabilities of candidate answers. After calculating total penalty value $P$ , we adjust the posterior probabilities of each candidate answer. This adjustment process aims to lower the selection probability of those answers containing undesirable hallucination features, while elevating the posterior probabilities of those answers further from hallucination features. The specific adjustment formula is as follows:

$$
P _ { n } = P _ { o } \cdot \exp ( - \alpha \cdot P ) ,
$$

Table 2: Performance of different decoding strategies on several LLMs on TruthfulQA and RelQA-Cate datasets.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3">TruthfulQA</td><td colspan="2">RelQA-Cate</td></tr><tr><td>MC1</td><td>MC2</td><td>MC3</td><td>F1</td><td>CGS</td></tr><tr><td>LLaMA-7B</td><td>23.62</td><td>41.21</td><td>19.33</td><td>32.15</td><td>3.39</td></tr><tr><td>+Alpaca +ITI</td><td>26.93</td><td>42.97</td><td>19.79</td><td>36.78</td><td>3.76</td></tr><tr><td rowspan="5">+CD-13B +DoLa +SH2 +DPD</td><td>25.90</td><td></td><td></td><td>37.55</td><td>3.98</td></tr><tr><td>24.40</td><td>41.00</td><td>19.00</td><td>34.56</td><td>3.59</td></tr><tr><td>31.95</td><td>52.21</td><td>28.17</td><td>40.24</td><td>4.17</td></tr><tr><td>27.91</td><td>55.63</td><td>29.73</td><td>39.58</td><td>4.06</td></tr><tr><td>34.27 2.32</td><td>57.54 1.91</td><td>31.02 1.29</td><td>44.26 4.02</td><td>4.60 0.43</td></tr><tr><td rowspan="6">↑(%) LLaMA2-7B +ITI +DoLa +CD-13B</td><td>7.26</td><td>3.43</td><td>4.34</td><td>9.99</td><td>10.31</td></tr><tr><td>37.62</td><td>54.60</td><td>28.12</td><td>36.52</td><td>3.88</td></tr><tr><td>37.01</td><td>54.66</td><td>27.82</td><td>37.50</td><td>4.05</td></tr><tr><td>32.97</td><td>60.84</td><td>29.50</td><td>41.28</td><td>4.21</td></tr><tr><td>28.15</td><td>54.87</td><td>29.75</td><td>36.89</td><td>3.91</td></tr><tr><td>46.32 49.63</td><td>69.08 74.28</td><td>41.25 43.99</td><td>42.77 48.55</td><td>4.42 4.73</td></tr><tr><td rowspan="5">↑(%) Baichuan2-7B +ICD +DPD</td><td>3.31</td><td>5.20</td><td>2.74</td><td>5.78</td><td>0.31</td></tr><tr><td>7.15</td><td>7.53</td><td>6.64</td><td>13.51</td><td>7.01</td></tr><tr><td>34.93</td><td>52.14</td><td>27.19</td><td>38.77</td><td>4.28</td></tr><tr><td>45.75</td><td>65.51</td><td>39.67</td><td>46.83</td><td>4.65</td></tr><tr><td>50.23</td><td>68.95</td><td>44.30</td><td>50.74</td><td>4.87</td></tr><tr><td rowspan="4">个 ↑(%) Mistral-7B</td><td>4.48</td><td>3.44</td><td>4.63</td><td>3.91</td><td>0.22</td></tr><tr><td>9.79</td><td>5.25</td><td>11.67</td><td>8.35</td><td>4.73</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>39.09</td><td>55.80</td><td>28.25</td><td>40.28</td><td>4.53</td></tr><tr><td rowspan="5">+ICD +DPD → ↑(%)</td><td>58.53</td><td>74.73</td><td>50.38</td><td>55.31</td><td>4.82</td></tr><tr><td>60.34</td><td>78.35</td><td>52.08</td><td>58.04</td><td>4.93</td></tr><tr><td>1.81</td><td>3.62</td><td>1.70</td><td>2.73</td><td>0.11</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>3.09</td><td>4.84</td><td>3.37</td><td>4.94</td><td>2.28</td></tr></table></body></html>

where $P _ { n }$ represents the adjusted posterior probability of a candidate answer. $P _ { o }$ is the original posterior probability of candidate answer before penalty adjustment. It involves model probability output that LLMs assign probabilities to possible words or sequences for each candidate answer, as well as is dependent on answer length because longer answers may accumulate more uncertainty, potentially lowering their overall probability. $P$ is the total penalty value of a candidate answer, incorporating all hallucination categoryrelated penalties. $\alpha$ is a global tuning parameter which is a positive value, controlling the impact of the penalty item on the posterior probability.

# Experiment

In this section, we conduct extensive experiments to evaluate the positive effect of our proposed DPD in comparison with other decoding strategies.

Experimental setup. Our experiments are conducted on 8xNvidia A100 GPUs, each with 80GB of memory, using PyTorch in Python. We set the maximum sequence length for input and output sequences to maximum 1024 and 128 tokens, respectively. Because the attention patterns of hallucinations differ across datasets. Therefore, our penalties are tailored for each dataset, respectively. Then, we validate the effect in the test set of the same dataset.

Datasets, Metrics, and Baselines. We adopt RelQA-Cate and TruthfulQA (Lin, Hilton, and Evans 2021) datatsets. We utilize F1 score (short for F1) and ChatGPT Score (short for CGS) inspired by Chen et al. (2023d), which evaluate the similarity and goodness between the generated answer and the ground-truth answer from RelQA, respectively, on RelQA-Cate. CGS is a 5-scale rating for the generated answer evaluated by ChatGPT with 1 being the worst and 5 being the best for the given question. We use multiple-choicebased metrics including MC1, MC2, MC3 as elaborated in Lin, Hilton, and Evans (2021), which is to evaluate LLMs’ performance in TruthfulQA dataset. Baselines are shown in Table 2 and Table 3.

Table 3: Comparisons with our proposed DPD and other baselines besides decoding strategies for reducing hallucinations.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3">TruthfulQA</td><td colspan="2">RelQA-Cate</td></tr><tr><td>MC1</td><td>MC2</td><td>MC3</td><td>F1</td><td>CGS</td></tr><tr><td>LLaMA2-7B</td><td>37.62</td><td>54.60</td><td>28.12</td><td>36.52</td><td>3.88</td></tr><tr><td>+RARR</td><td>40.54</td><td>61.35</td><td>33.13</td><td>41.32</td><td>4.03</td></tr><tr><td>+L</td><td>38.93</td><td>58.22</td><td>30.46</td><td>38.97</td><td>3.9</td></tr><tr><td>+CoVe</td><td>44.35</td><td>68.13</td><td>39.65</td><td>43.14</td><td>4.25</td></tr><tr><td>+CoNLI</td><td>42.65</td><td>65.36</td><td>34.87</td><td>42.13</td><td>4.3</td></tr><tr><td>+RHO</td><td>46.66</td><td>72.15</td><td>42.55</td><td>46.87</td><td>4.62</td></tr><tr><td>+FLEEK</td><td>47.76</td><td>73.23</td><td>42.98</td><td>46.75</td><td>4.55</td></tr><tr><td>+DPD</td><td>49.63</td><td>74.28</td><td>43.99</td><td>48.55</td><td>4.73</td></tr></table></body></html>

# Main Results

As demonstrated in Table 2, the introduction of the DPD has led to improvements across various datasets for each LLM. We conduct a t-test on the results, and all improvements of DPD are statistically significant, with $\mathfrak { p } < 0 . 0 5$ . For LLaMA-7B, DPD has brought improvements of up to $7 . 2 6 \%$ over the previous SOTA strategies on the TruthfulQA dataset and has improved the performance on the RelQA-Cate dataset by $10 . 3 1 \%$ . With LLaMA2-7B, the DPD also show effective enhancements on the TruthfulQA and RelQA-Cate dataset. Compared with the effect of DPD into Baichuan2-7B, we observe the increases are not as high as those for the LLaMA series, which might be because the LLaMA-7B may contain more hallucinatory phenomena in its original outputs, but Baichuan2-7B may already be optimized for more QA tasks. Similar patterns are also seen in Mistral-7B, suggesting that Mistral-7B may already have a good baseline performance and the addition of DPD provides a subtle accuracy boost. Moreover, we also compare results of non-decoding baselines for reducing hallucinations as shown in Table 3 based on LLaMA2-7B. The results indicate DPD outperforms non-decoding methods across the datasets involved.

# Ablation Study

We conduct ablation study on the RelQA-Cate dataset for different LLMs as shown in Fig. 8, Fig. 9, and Table 4, respectively. We first explore candidate answer diversity for DPD. LLMs show the lowest performance without DPD, but as diversity increases from 0 to 0.8, both F1 and CGS scores improve. When diversity exceeds 0.8, indicating DPD, all LLMs achieve optimal performance, which suggests that diversity in candidate answers has positive effect in reducing hallucination in LLMs’ outputs. Next, we explore the specific effect of each penalty function in DPD. When $P _ { l }$ is removed, all LLMs experience a performance drop, but this decrease is moderate compared to other penalties. The removal of $P _ { c a }$ shows almost no significant performance decrease, as well as the most notable performance degradation occurs when $P _ { w a }$ is removed, underscoring the critical role of high-contribution word adjustment in reducing hallucinations. After that, we explore the improvement degree with DPD on different size of LLMs. “↑” and “↑ $( \% ) ^ { \dag }$ ” indicate the absolute and relative improvements in DPD compared to the methods in the previous row, also proving that our method is effective on larger-sized LLMs.

Table 4: The improvement degree of DPD with models of different sizes based on LLaMA on the TruthfulQA and RelQA-Cate datasets.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3">TruthfulQA</td><td colspan="2">RelQA-Cate</td></tr><tr><td>MC1</td><td>MC2</td><td>MC3</td><td>F1</td><td>CGS</td></tr><tr><td>LLaMA-13B</td><td>28.55</td><td>46.44</td><td>26.12</td><td>37.12</td><td>4.21</td></tr><tr><td>+DPD</td><td>36.61</td><td>58.15</td><td>34.23</td><td>47.11</td><td>4.68</td></tr><tr><td></td><td>8.06</td><td>11.71</td><td>8.11</td><td>9.99</td><td>0.47</td></tr><tr><td>↑(%)</td><td>28.23</td><td>25.22</td><td>31.05</td><td>26.91</td><td>11.16</td></tr><tr><td>LLaMA-33B</td><td>37.98</td><td>53.91</td><td>31.87</td><td>45.72</td><td>4.43</td></tr><tr><td>+DPD</td><td>41.24</td><td>62.53</td><td>36.13</td><td>50.38</td><td>4.78</td></tr><tr><td></td><td>3.26</td><td>8.62</td><td>4.26</td><td>4.66</td><td>0.35</td></tr><tr><td>↑(%)</td><td>8.58</td><td>15.99</td><td>13.37</td><td>0.19</td><td>7.90</td></tr><tr><td>LLaMA-65B</td><td>48.92</td><td>67.24</td><td>44.30</td><td>55.17</td><td>4.65</td></tr><tr><td>+DPD</td><td>52.55</td><td>70.14</td><td>49.48</td><td>59.54</td><td>4.83</td></tr><tr><td></td><td>3.63</td><td>2.90</td><td>5.18</td><td>4.37</td><td>0.18</td></tr><tr><td>↑(%)</td><td>7.42</td><td>4.31</td><td>11.69</td><td>7.92</td><td>3.87</td></tr><tr><td>LLaMA2-70B</td><td>52.78</td><td>74.23</td><td>50.25</td><td>61.32</td><td>4.83</td></tr><tr><td>+DPD</td><td>57.32</td><td>78.39</td><td>53.87</td><td>64.27</td><td>4.86</td></tr><tr><td>个</td><td>4.54</td><td>4.16</td><td>3.62</td><td>2.95</td><td>0.03</td></tr><tr><td>↑(%)</td><td>8.60</td><td>5.60</td><td>7.20</td><td>4.81</td><td>0.62</td></tr><tr><td>Mistral 8x7B</td><td>46.93</td><td>63.14</td><td>39.18</td><td>49.83</td><td>4.62</td></tr><tr><td>+DPD</td><td>61.30</td><td>79.15</td><td>56.89</td><td>63.15</td><td>4.97</td></tr><tr><td>个</td><td>14.37</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>16.01</td><td>17.71</td><td>13.32</td><td>0.35</td></tr><tr><td>↑(%)</td><td>30.62</td><td>25.36</td><td>45.20</td><td>26.73</td><td>7.58</td></tr></table></body></html>

![](images/11554a16b64888ba90fc0465d0dc9dc7014e0fb6b29e4aaaf0f27e7ec0b86257.jpg)  
Figure 8: LLMs’ performance with different diversity of candidate answers (using F1 score as an example).

# Case Study

We conduct a case study to analyze the effectiveness of DPD based on an answer with factual error hallucination as shown in Table 5. For the given question “Which is the longest river in Northern Ireland?” with the ground-truth “River Bann”, we first generate five candidate answers and obtain their probabilities. Before applying DPD, the predicted answer by the LLM is “The River Lagan.” with its probabilities maximum. Next, we utilize the DPD to calculate their penalty values and then adjust the probabilities. We observe that the previous answer lowers its probability and the candidate “The River Bann ... 92 miles” has the maximum probability which is the correct answer. This case demonstrates applying penalties can significantly lower the LLMs’ confidence in incorrect answers, effectively reducing hallucinations. Moreover, we also observe different categories of hallucination have different improvements in LLMs’ accuracy as shown in Table 6 across samples in Table 1.

![](images/f3567adab81fc56439b00a242be3ec25de00ffc1420cb4085ea9e3b31ae8e733.jpg)  
Figure 9: The specific effect of each penalty function in DPD for reducing hallucination (using F1 score as an example).

# Related Work

Reducing hallucination in LLMs. Shi et al. (2023) introduced external knowledge to user queries in the prompt, Peng et al. (2023) enhanced the accuracy of predictions through external knowledge; Zhang et al. (2023) adopted chain-of-thought for guiding LLMs to generate reasoning path, Touvron et al. (2023) added instructions like “If you don’t know,...” to guide LLMs not to propagate unverifiable information output; Zhou et al. (2023) manually adjusted data with 1,000 samples annotated by human experts, Cheng et al. (2024) constructed an “I don’t know” dataset, training LLMs not to answer questions they don’t know; Yu et al. (2023) enhanced the credibility of LLMs through human behavioral adjustment, Sun et al. (2023) proposed factuallyaugmented RL to enhance the reward model, Chen et al. (2023a) fed LLMs with some incorrect text, making LLMs reflect on the reasons for the errors. However, these methods do not involve with the internal state of LLMs.

Table 5: Candidate answers with the original probabilities, the penalty values obtained with DPD, and the adjusted probabilities for a given question.   

<html><body><table><tr><td>Candidate answers</td><td>Probability</td><td>Penalty</td><td>AdjustedP</td></tr><tr><td>The River Bann in County Antrim is the longest river in Northern Ireland with a course of 92 miles</td><td>0.62</td><td>0.17</td><td>0.52</td></tr><tr><td>The River Liffey，at 110 miles,is Ireland's longest river</td><td>0.58</td><td>0.44</td><td>0.37</td></tr><tr><td>TheLongestRiver in Ireland is the River Shannon</td><td>0.63</td><td>0.67</td><td>0.32</td></tr><tr><td>The River Lagan</td><td>0.69</td><td>0.75</td><td>0.32</td></tr><tr><td>The River Foyle isthe longest river in Northern Ire- land</td><td>0.55</td><td>0.73</td><td>0.28</td></tr></table></body></html>

Table 6: Different categories of hallucination examples corresponding to Table 1 with the original probabilities, DPD generated penalty values and adjusted probabilities.   

<html><body><table><tr><td>Category</td><td>Probability</td><td>Penalty</td><td>AdjustedP</td></tr><tr><td>Factual error</td><td>0.65</td><td>0.82</td><td>0.29</td></tr><tr><td>Logical error</td><td>0.67</td><td>0.73</td><td>0.32</td></tr><tr><td>Conceptual confusion</td><td>0.73</td><td>0.97</td><td>0.28</td></tr><tr><td>Vagueness</td><td>0.66</td><td>0.59</td><td>0.37</td></tr><tr><td>Lack of commonsense</td><td>0.58</td><td>0.43</td><td>0.38</td></tr><tr><td>Over-generalization</td><td>0.89</td><td>0.65</td><td>0.46</td></tr><tr><td>Emotional bias</td><td>0.65</td><td>0.9</td><td>0.26</td></tr><tr><td>Lack of uncertainty</td><td>0.72</td><td>0.85</td><td>0.31</td></tr></table></body></html>

Internal state of LLMs. Gurnee and Tegmark (2023) discovered that LLMs can learn linear representations of space and time across multiple spacetime scales; Chen et al. (2023e) proposed “Attention Buckets” based on RoPE for each attention module; Chen et al. (2023c) analyzed the attention output of the Hadamard adapter and full fine-tuning are similar in performance; Ziheng et al. (2023) constructed a set of globally shared adjustable tokens to modify the attention of each layer for LLMs; Xu et al. (2023) analyzed the relative token contributions to model’s generation. CH-Wang et al. (2023) developed probes trained on transformer model representations in in-context generation tasks. Inspired by the above methods, we also analyze LLMs’ internal state to guide the method design.

# Conclusions and Future Work

Large language models (LLMs) currently demonstrate excellent capabilities in a variety of downstream tasks, but the hallucinations of the output answers pose serious challenges. In this paper, we propose an attribution framework to trace the origins of hallucinations based on the internal signals of LLMs. Further, we propose a novel Differential Penalty Decoding (DPD) strategy to assign penalty values to each answer with hallucination and adjust the post-probabilities of these answers, making the hallucination output less likely to be selected. Our experiments demonstrates that DPD performs well on various datasets and LLMs, making significant contributions in mitigating hallucinations of LLMs.