# Can Large Language Models Derive High-Level Cognition from Low-Level and Fragmented Foundational Information?

Yang Liu1, Xiaoping Wang1\*, Kai $\mathbf { L u } ^ { 2 * }$

1College of Computer Science and Electronic Engineering, Hunan University, China 2College of Computer, National University of Defense Technology, China liuyang0542@hnu.edu.cn, xpwang@hnu.edu.cn, kailu@nudt.edu.cn

# Abstract

As one of the key technologies leading to Artificial General Intelligence (AGI), Large Language Models (LLMs) have achieved remarkable accomplishments. Exploring the capabilities of LLMs is crucial for scientific research, and many studies propose new challenges from various aspects to explore the boundaries of capabilities in LLMs. This paper attempts to push the challenges of information understanding, synthesizing and reasoning to the extreme, in order to explore the boundaries of more advanced dimensional cognitive capabilities in LLMs. It is defined as the task of HighLevel Cognition (HLC), which involves obtaining high-level conclusions from low-level and fragmented foundational information. To evaluate HLC, we construct a dataset based on soccer matches. Experiments and analysis on this dataset show that current state-of-the-art LLMs lack the ability to effectively solve the task of HLC, because their performance is equivalent to random-level. However, by fine-tuning Llama3- 8B-Instruct, there are improvements of $1 4 . 4 \%$ , $4 8 . 1 \%$ , and $1 9 . 4 \%$ over random-level in three types of evaluation tasks. This indicates that LLMs have great potential to solve the task of HLC.

Our Work High-Level Cognition l   
Low-level and Fragmented Push to the High-level   
Foundational Information LLMs Extreme Conclusions Previous Work Capabilities of Cognition Cultural CPohnycseicpatls User Intent lCirnogsusa-l Analogical Multi-hop ↑ ↑ Understanding Synthesizing Reasoning FLiagnugruaatigve Humor DCaotnateantd LonTge-xfotrm Abstract Temporal

# Model & Dataset Details — https://github.com/nlpmy/hlc

# 1 Introduction

Large Language Models (LLMs) have demonstrated significant progress in a wide range of Natural Language Processing (NLP) tasks, including intelligent dialogue (Yi et al. 2024; Joko et al. 2024), commonsense reasoning (Krause and Stolzenburg 2024; Zhao, Lee, and Hsu 2024), text comprehension (Chen and Leitch 2024; Sa¨uberli and Clematide 2024), and code generation (Wang and Chen 2023; Jiang et al. 2024b). These demonstrate their strong capabilities in handling complex language tasks. However, the capabilities and potential of LLMs remain an open field. Exploring the boundaries of capabilities in LLMs is crucial for advancing their intelligence. There are many new challenges about the capabilities of LLMs are proposed, which involve understanding (Zhu et al. 2024; Hessel et al. 2022), synthesizing (Hu et al. 2024; Salvador et al. 2024), and reasoning (Gendron et al. 2023; Stechly, Marquez, and Kambhampati 2023). They guide researchers to consider how to improve LLMs, thereby achieving more human-like cognitive intelligence. Nevertheless, the boundaries of more advanced dimensional cognitive capabilities in LLMs remain unresolved, because the standards and methods for measuring in complex cognitive tasks are still not clear enough. To address this, the challenges of information understanding, synthesizing, and reasoning are pushed to the extreme in this paper. It is the task of High-Level Cognition (HLC).

HLC is obtaining high-level conclusions from low-level and fragmented foundational information. In a soccer match, the low-level foundational information includes players’ performance and ball events, and the high-level conclusions refer to the tactical characteristics of teams, which cannot be directly extracted from foundational information. To achieve this, the relationship between all low-level foundational information (e.g., passing, shooting) is pushed to the boundaries of understanding. Then an implicit global structure is formed by pushing all fragmented foundational information to the boundaries of synthesizing. Finally, high-level conclusions (e.g., playing styles, strengths and weaknesses) are drawn by pushing the structure to the boundaries of reasoning. This process is similar to the professional knowledge and experience in human. Therefore, the HLC task is an integrated task of understanding, synthesizing, and reasoning, and it requires the formation of systematic thinking according to a specific cognitive framework.

Exploring the HLC in LLMs is a new challenge. Different from traditional NLP tasks, which often involve simple extraction (Singh 2018), summarization (Awasthi et al. 2021), or text rewriting (Xu et al. 2019), a more complex cognitive framework is required in the HLC task. It cannot be addressed by traditional knowledge enhancement methods (e.g., retrieval-augmented generation (Lewis et al. 2020), knowledge graphs (Hogan et al. 2021), database queries (Chandra 1988)). Moreover, different from tasks involving mathematical or programming skills, the results from HLC task reflect the inherent analytical and reasoning abilities in LLMs, because there is no pre-existing chain of thought (Wei et al. 2022) framework to rely on. There are some obstacles in the research process, and a major is the lack of specialized datasets, because traditional datasets do not meet the complexity required for generating high-level conclusions. Additionally, there are no clear, objective benchmarks for evaluating HLC, and no robust metrics or standard frameworks to accurately assess it.

To address these issues, a dataset for evaluating HLC is constructed in this paper, and it is based on soccer matches. For providing objective evaluation results, the outcomes of HLC are categorized into three types: (a). MultipleChoice Question (MCQ), which requires LLMs to choose multiple from the existing statements about playing styles; (b). Single-Choice Question (SCQ), which requires LLMs to choose one from the existing statements about playing styles; (c). True/False Question (TFQ), which requires LLMs to determine whether a sentence describing the playing styles is correct. Experimental results show that current state-of-the-art LLMs cannot derive HLC from low-level and fragmented foundational information, because their performance is equivalent to random-level. However, it is found that the HLC can be endowed by fine-tuning. The illustration of our work is shown in Figure 1.

The main contributions of this paper are as follows:

• The HLC task is first proposed, and it is currently one of the most challenging tasks for LLMs. The HLC task lies outside the current LLMs capability boundaries, because the performance of the state-of-the-art LLMs is equivalent to random-level.   
• A dataset named MatchIntel is constructed to evaluate the performance of LLMs on HLC tasks. The dataset contains both low-level and high-level information about soccer matches. It provides an experimental benchmark for exploring HLC in the future.   
• Preliminary testing for the HLC tasks is conducted, and two conclusions are drawn: 1). current state-of-the-art LLMs lack the ability to effectively solve HLC tasks.

2). Fine-tuning can improve the performance of LLMs on HLC tasks to some extent, but it does not fully solve such tasks. These highlight the need for further academic investigation.

The rest of the paper is organized as follows: related work is introduced in Section 2; the proposed scheme is described in Section 3; experiments and analysis are provided in Section 4; limitations are presented in Section 5; Finally, some conclusions are drawn in Section 6.

# 2 Related Work

The cognition of LLMs has been studied across various topics, focusing on the boundaries of capabilities in information understanding, synthesizing, and reasoning. Research findings in these are diverse, with some studies highlighting the limitations of LLMs in a specific capability, while others propose methods to enhance the specific capability in LLMs.

# 2.1 Understanding

A contextual understanding benchmark is introduced to evaluate LLMs’ linguistic comprehension within context (Zhu et al. 2024), including both document-based and dialogue-based scenarios. Experimental results indicate that LLMs face challenges in nuanced contextual understanding, especially in in-context learning settings. The capability of understanding humor in LLMs is evaluated (Hessel et al. 2022). There are three tasks derived from the New Yorker Cartoon Caption Contest, and both multi-modal and language-only models are investigated. The study found that current LLMs are still unable to recognize, understand, and evaluate humor as effectively as humans.

# 2.2 Synthesizing

A benchmark SportsMetrics is developed to evaluate the capability of information fusion in LLMs (Hu et al. 2024). This benchmark presents challenges such as adapting to new match rules, interpreting lengthy descriptions, managing scrambled narratives, and analyzing key statistics in match summaries. It highlights the great potential of LLMs in information fusion, though limitations remain. The capability of LLMs for the Semantic Overlap Summarization (SOS) task is studied (Salvador et al. 2024). It involves summarizing overlapping information from multiple narratives. Experimental results show that LLMs struggle with the SOS task, indicating room for improvement in this area.

# 2.3 Reasoning

To evaluate abstract reasoning of LLMs, a framework is constructed using both text and visual datasets (Gendron et al. 2023). The experiments demonstrated that LLMs lack the capability for abstract reasoning, and existing techniques for improving NLP tasks cannot enhance the capability. The effectiveness of iterative prompting strategy in LLMs is explored (Stechly, Marquez, and Kambhampati 2023). Its purpose is to improve the accuracy about reasoning problems. In a self-critique iterative framework for graphic color problems, it is found that iterative prompting sometimes led to worse performance compared to generating a single answer.

![](images/6042f2d61d31cd849640213b2d6980f66ddda5ccb27854a9ca3379df03700d31.jpg)  
Figure 2: An overview of the proposed scheme. A dataset containing commentary texts and playing styles is collected from a professional soccer data website. Then it is normalized using the prompt template. After that, the dataset is divided into a train set and a test set. The train set is used to fine-tune LLMs, and the test set is used to evaluate the high-level cognition of LLMs.

These studies acknowledge the potential and recognize the boundaries of LLMs across different cognitive topics. However, they ignore the more advanced dimensional cognition in complex cognitive tasks, and it is the integration of multiple cognitive topics. To address this gap, the HLC task is performed in this paper, based on data from soccer matches.

# 3 The Proposed Scheme

The flowchart of the proposed scheme is shown in Figure 2. Firstly, a substantial corpus of data about soccer matches is collected from a sports reporting website, and then stored as a JSON dataset. Subsequently, the prompt template is designed, and it formats the information by reading soccer match commentary texts and statements about playing styles from the dataset, thereby optimizing the structure of the dataset. After that, the normalized dataset is partitioned into two sets: a train set and a test set. Finally, the train set is used to fine-tune LLMs, and the test set is used to evaluate the performance of HLC in LLMs.

The dataset collection, prompt formatting, fine-tuning LLMs, and high-level cognition evaluation are described in details below.

# 3.1 Dataset Collection

To meet the standards of HLC, a dataset containing 52,500 samples is collected from a professional soccer data website1 and named MatchIntel. Each sample records the situation of a match, which includes soccer match commentary texts and playing styles of two teams. The soccer match commentary texts are described with details of players’ performance or ball events at each moment, and they are preserved as primitive observational records, without complex processing and analysis. For example, Wigan vs. Morecambe in the 2021/2022 season of England League One, the commentary text from the 40th to 44th minute is intercepted as   
“40 Morecambe Cole Stockton has shot blocked (Standing, Out of box, Right footed, Open play)   
43 Wigan Stephen Humphrys has attempt saved (Standing, High to the right, Out of box, Left footed, Open play) 43 Wigan Stephen Humphrys wins a corner (To the right) 43 Morecambe Trevor Carson makes a save (Diving, Parried safe) MCQ SCQ TFQ   
You are a soccer analyst, and give you a soccer You are a soccer analyst, and give you a soccer You are a soccer analyst, and give you a soccer match commentary in the form of "timeline | match commentary in the form of "timeline | match commentary in the form of "timeline | team name | commentary text": team name | commentary text": team name | commentary text": [soccer match commentary] [soccer match commentary] [soccer match commentary]   
Analyze the playing styles of [team name] from Analyze the playing styles of [team name] from Analyze the playing styles of the teams from the above commentary, and you must choose the above commentary, and you must choose the above commentary, and judge the multiple from the following four statements one from the following four statements based correctness of the statement in "{ }" based on based on your analysis: on your analysis: your analysis:   
{[playing styles - 1]; [playing styles - 2]; {[playing styles - 1]; [playing styles - 2]; {The playing styles of [team name] includes: [playing styles - 3]; [playing styles - 4]}. [playing styles - 3]; [playing styles - 4]}. [playing styles - 1]; [playing styles - $2 J ;$ Note: Choose two, three, or four [appropriate/ Note: Only choose one appropriate statement [playing styles - 3]…… [playing styles - n].} inappropriate] statements from the existing from the existing content in $" \{ \} "$ without any Note: Only respond with "True" or "False", do content in $" \{ \} "$ without any changes, do not changes, do not output any other content. not output any other content.   
output any other content.   
Response: Response: Response:   
[multiple playing styles statements] [one playing styles statement] [True / False]   
(a) (b) (c)

44 Wigan GOAL! Jason Kerr scores , Assisted by Tom Naylor (Standing, Low to the left, 6-yard box, Big Chance, Right footed, From corner)”.

The actions of players are described, but they are not presented in a clear and comprehensible description. Instead, some incoherent soccer terms are listed, which represent the low-level and fragmented foundational information.

The statements about playing styles are categorized into a finite field and consist of 12 types. They cannot be directly extracted from the commentary texts. For example, “Had a large quantity of possession in their opponent’s half” is one of them. It means that a team constantly passes, controls, and seeks opportunities to attack in the opponent’s half, while the opponent’s team may be forced to defend and have little chance to get the ball. To obtain this accurately, understanding, synthesizing, and reasoning must be applied to all soccer terms of the commentary texts, and all of these capabilities need to be pushed to the boundaries. Therefore, the statements about playing styles are high-level conclusions, and the dataset can be used to evaluate HLC in LLMs.

# 3.2 Prompt Formatting

For providing objective evaluation results, some HLC evaluation tasks need to be created. When a soccer commentary text is inputted into LLMs, the appropriate statements about playing styles are expected to generate by LLMs. If there is no guidance, these statements will fail to be generated due to the randomness of LLMs’ output. Therefore, to ensure the output is focused on the finite field, prompt templates need to be designed. Specifically, the model is defined as a soccer match analyst at the beginning, and a soccer commentary text is provided. Then some statements about playing styles are presented, and a decision needs to be made based on these statements. The prompt templates are designed for the following three tasks:

Multiple-Choice Question (MCQ) The playing styles of each match are part of all the statements. Therefore, predicting the ground truth is essentially a multiple-choice question. To simplify the evaluation process, the prompt template is designed to include only four statements, and at least two correct statements are provided. However, existing research indicates that LLMs are sensitive to the numbering and positioning of options when handling choice questions (Zheng et al. 2023; Li and Gao 2024; Pezeshkpour and Hruschka 2023). Thus, in the process of designing, the position of presented statements is not fixed, and LLMs are required to output multiple statements, without any changes. The MCQ prompt template is shown in Figure 3(a).

Single-Choice Question (SCQ) The SCQ is a type of question that contains multiple options, and only one is correct while the others are not. After analyzing the dataset, each team has more than three incorrect statements and at least one correct statement. Therefore, when a team is selected, one correct statement and three incorrect statements can be chosen. Then these four statements are randomly arranged to construct a single-choice question. Finally, LLMs are required to choose one from them, and output the statement without any changes. The SCQ prompt template is shown in Figure 3(b).

True/False Question (TFQ) LLMs are required to make a judgment on the sentence constructed by the prompt about a team’s playing styles. A correct “include” sentence contains all correct statements, while a correct “not include” sentence contains all incorrect statements. Additionally, an incorrect “include” sentence contains all correct statements and one incorrect statement, while an incorrect “not include” sentence contains all incorrect statements and one correct statement. All these statements are randomly listed in the prompt template. After that, LLMs are required to judge the sentence. If it is considered as correct, “True” should be output; otherwise, “False” should be output. Figure 3(c) shows the prompt template for TFQ.

The dataset is normalized by the three prompt templates and then used to evaluate HLC of LLMs.

# 3.3 Fine-Tuning

The train set is used to fine-tune a LLM for analyzing soccer matches. The Llama3-8b-Instruct (Meta 2024) is chosen as the foundational model, because of its superior performance among the open-source models, especially in processing complex language tasks. Due to the constraints of computational resources, the Parameter-Efficient Fine-Tuning (PEFT) technique (Han et al. 2024) is used. Specifically, the method of Qlora (Dettmers et al. 2024) is adopted, and it is used for fine-tuning by 4-bit precision, significantly reducing the computational burden without compromising the performance. Moreover, a set of compact, learnable Lowrank adapters (Lora) weights (Hu et al. 2021) is introduced, which can be adjusted through backpropagation. Finally, the learning capability of model is enhanced, and effective adaptation to task requirements is allowed. The fine-tuned model is named TraitsMind, specifically designed to analyze the playing styles of teams in soccer matches.

# 3.4 HLC Evaluation

The test set is used to evaluate HLC of LLMs. To obtain comprehensive results, popular open-source and closedsource LLMs, along with the TraitsMind, are employed for testing. The data is inputted into the model, and then the output is presented in a fixed format, because the output format is defined in prompt templates. Finally, the statements are extracted from the output, and a match is made with the ground truth. The higher the matching degree, the better the performance of the model.

# 4 Experiment

# 4.1 Experimental Setup

Datasets The experiment is performed on the dataset MatchIntel, with a total of 52,500 samples. 49,500 samples are allocated for the train set, and the remaining 3,000 for the test set. Additionally, the test set is divided into three parts, with each evaluation task containing 1,000 samples.

Comparison Models Some open-source LLMs are selected for testing, including Zephyr (Tunstall et al. 2023), Gemma (Team et al. 2024a), Llama3 (Meta 2024), Mixtral (Jiang et al. 2024a), and DBRX (Team et al. 2024b). Advanced performance in text understanding, reasoning, and generation has been demonstrated by these models. To enhance the credibility of the experiment, two closed-source LLMs, ChatGPT (Achiam et al. 2023) and Gemini (Team et al. 2023), are employed for interactive testing through their APIs.

Evaluation Metrics Accuracy (Makridakis 1993) is used to evaluate performance. The output is considered accurate when it matches the ground truth exactly.

Parameter Details LLMs are fine-tuned using Qlora with 4-bit quantization. A low-rank adaptive approach (rank 64, alpha 16, dropout 0.1) is applied to the query and keyvalue components. Fine-tuning is done with a single iteration on the train set using the paged adamw 32bit optimizer (Kingma and Ba 2014), initiated with a learning rate of 1e-4 and weight decay rate of 0.01. The maximum input length is capped at 3200 tokens, and the half-precision (FP16) is used for training efficiency. During inference, a conservative and stable output is maintained to improve the accuracy. Therefore, temperature is set to 0 for ChatGPT and Gemini, while 0.01 for other LLMs. The top p is set to 1.0 for all LLMs.

# 4.2 Experimental Results

The threshold of HLC needs to be determined in three evaluation tasks, and it is the baseline of performance. If the performance of LLMs significantly surpasses the threshold, HLC is considered to be exhibited. Otherwise, LLMs lack the ability to effectively solve HLC tasks. The random-level can be used as the threshold, because it is the expected performance that LLMs can achieve without any cognitive abilities. Pure speculation and actual cognition can be clearly distinguished by it. To get the values of thresholds, combinatorial mathematics and probability theory (Spitzer 1956) are applied, because these tasks have a clear probability about correct answers. Finally, the expected accuracy plus 3 standard deviations $( 9 9 . 7 \%$ confidence interval) as the values for thresholds, which can be calculated as 0.118 for MCQ, 0.291 for SCQ, and 0.547 for TFQ. The experimental results of LLMs in three evaluation tasks are shown in Table 1.

From the results, it can be concluded that the LLMs cannot derive HLC from low-level and fragmented foundational information.

• The performance of LLMs is equivalent to the randomlevel, because they do not significantly surpass the thresholds in three evaluation tasks. Therefore, it indicates that LLMs lack the ability to effectively solve HLC tasks.   
• If there is HLC in LLMs, it should become more prominent as model parameters are increased. However, in MCQ and TFQ evaluation tasks, better performance is exhibited by Llama3-Instruct with 8B, rather than the model with 70B. This indicates that HLC is not present in LLMs.   
• The TraitsMind outperforms all LLMs in three evaluation tasks. In particular, a significantly improved performance compared to its foundational model Llama3- 8b-Instruct. Its performance surpasses the thresholds by $1 4 . 4 \%$ , $4 8 . 1 \%$ , and $1 9 . 4 \%$ in three evaluation tasks, respectively. This indicates that fine-tuning can endow the LLMs with HLC, but the performance improvement is only to some extent and cannot fully solve such tasks.

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Params</td><td colspan="3">Evaluation Task</td></tr><tr><td>MCQ</td><td>SCQ</td><td>TFQ</td></tr><tr><td colspan="5">Baseline</td></tr><tr><td>Random</td><td>-</td><td>0.118†</td><td>0.291†</td><td>0.547†</td></tr><tr><td colspan="5">Closed-source models</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPTini-PTurboe Achia. 2tal. 2023)</td><td>__</td><td>0.106</td><td>0.286</td><td>0.508</td></tr><tr><td colspan="5">Open-source models</td></tr><tr><td>Zephyr-Beta(Tunstalletal.2023)</td><td>7B</td><td>0.094</td><td>0.193</td><td>0.438</td></tr><tr><td>Gemma-1.1-Instruct (Team et al. 2024a)</td><td>7B</td><td>0.113</td><td>0.219</td><td>0.484</td></tr><tr><td>Llama3-Instruct (Meta 2024)</td><td>8B</td><td>0.111</td><td>0.267</td><td>0.519</td></tr><tr><td>Mixtral-InstructvO.1 (Jiang et al. 2024a)</td><td>46.7B</td><td>0.108</td><td>0.260</td><td>0.534</td></tr><tr><td>Llama3-Instruct (Meta 2024)</td><td>70B</td><td>0.084</td><td>0.288</td><td>0.434</td></tr><tr><td>DBRX-Instruct (Team et al. 2024b)</td><td>132B</td><td>0.060</td><td>0.250</td><td>0.530</td></tr><tr><td colspan="5">The proposed model</td></tr><tr><td>TraitsMind</td><td>8B</td><td>0.135</td><td>0.431</td><td>0.653</td></tr><tr><td>(Relative Impr. over Random)</td><td></td><td>(+14.4%)</td><td>(+48.1%)</td><td>(+19.4%)</td></tr></table></body></html>

Table 1: Accuracy of LLMs in three HLC evaluation tasks. Two closed-source models, six open-source models and the propose model are adopted for performance comparison. The data marked with $\dagger$ represent the theoretical values obtained by calculation

![](images/e64aa7b8b0068319f42e6323a778b57938f93d97e368465749bf3d478fb7691e.jpg)  
Figure 4: The relation between accuracy and the temperature in three evaluation tasks.

# 4.3 Ablation Studies

To further explore HLC, detailed ablation studies are conducted on temperature parameters, foundational models and quantized values.

Sensitivity to the Temperature Is the high-level cognition endowed by fine-tuning stable? In the inference stage of LLMs, lower temperature settings lead to more stable and conservative outputs, thereby improving credibility (Savelka et al. 2023). If HLC is stable in LLMs, changes in temperature will significantly affect its accuracy. Therefore, the performance of LLMs at different temperatures is explored, and the experimental results are shown in Figure 4. It can be observed that, in three evaluation tasks, the accuracy of TraitsMind shows a significant decrease as the temperature increases, while the accuracy of other LLMs does not exhibit a noticeable trend. It indicates that the HLC can be endowed by fine-tuning, and once endowed, it is stable in LLMs.

Sensitivity to the Foundational Model What is the source of high-level cognition? Three versions of the Llama series model, with their corresponding dialogue versions, are chosen as the foundational models for fine-tuning. The results are listed in Table 2. It can be seen that new-generation LLMs as the foundational models provide better performance than old-generation LLMs, because new-generation LLMs are pre-trained in larger and higher quality datasets, and the optimization of training modes is further advanced. Also, the architecture is more comprehensive, and the ability to understand contexts is significantly improved, which lead to the richer implicit knowledge is enabled by these new-generation LLMs. Therefore, the implicit knowledge is recognized as the source of HLC. Moreover, by finetuning the dialogue version LLMs, better performance is shown, which indicates that supervised fine-tuning (Dong et al. 2023) and reinforcement learning (Kaelbling, Littman, and Moore 1996) can boost the HLC in LLMs.

![](images/1429b260eaea9889093182ff165b491f27afcc4fd2405a183790edce69bc5256.jpg)  
Figure 5: The accuracy comparison on different quantization. The series for Llama and their corresponding dialogue model are adopted, and they are quantized at 8-bit and 4-bit.

Table 2: Effects of fine-tuning different foundational models. The accuracy of three series for Llama are evaluated.   

<html><body><table><tr><td>Model (Params)</td><td>MCQ</td><td>SCQ</td><td>TFQ</td></tr><tr><td>Llama1 (7B)</td><td>0.096</td><td>0.294</td><td>0.567</td></tr><tr><td>Vicuna-v1.5 (7B)</td><td>0.115</td><td>0.311</td><td>0.578</td></tr><tr><td>Llama2 (7B) Llama2-Chat (7B)</td><td>0.102 0.125</td><td>0.306 0.324</td><td>0.580 0.596</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>Llama3 (8B) Llama3-Instruct (8B)</td><td>0.129 0.135</td><td>0.348</td><td>0.609</td></tr><tr><td></td><td></td><td>0.431</td><td>0.653</td></tr></table></body></html>

Sensitivity to the Quantization How does the source influence high-level cognition? After fine-tuning, the LLMs are quantized at 8-bit and 4-bit. Experiments are conducted on three evaluation tasks, and the results are shown in Figure 5. As the quantization bit-width decreases, the accuracy gradually declines, because lower quantization bit-width can lead to a loss of the language processing ability in LLMs. The language processing ability is a manifestation of the implicit knowledge in LLMs, because it is a direct reflection of the extensive and subtle information, which LLMs have internalized during the training, and these are not explicitly programmed, but emerges from the patterns and associations learned from large amounts of data. In higher-precision representations, the LLMs retain and utilize its implicit knowledge better, thereby exhibiting stronger HLC. Therefore, the fidelity of implicit knowledge significantly influences HLC.

# 5 Limitations

There are two limitations. First, the LLMs are fine-tuned only under the initially set hyperparameters. It endows the HLC in a certain extent, but the highest level is not achieved. Therefore, further improve the accuracy of LLMs in these evaluation tasks by optimizing the hyperparameters can be believed. Second, the LLMs are extremely sensitive to prompts. Even if the same meaning is conveyed, different expression styles may change the accuracy of experiments. In this paper, the design of prompts is a preliminary attempt, but further improve the performance by optimizing the prompts is credible. Future works can explore the sensitivity of LLMs to prompts and hyperparameters.

# 6 Conclusion

In this paper, we propose the HLC task, which is currently one of the most challenging tasks for LLMs. To evaluate the performance of LLMs on such task, a specialized dataset MatchIntel and three evaluation tasks are created. Experimental results show that the performance of current stateof-the-art LLMs is equivalent to the random-level, indicating that they lack the ability to effectively solve HLC tasks. Additionally, the HLC can be endowed by fine-tuning, but the performance improvement is only to some extent and cannot fully solve such tasks. Therefore, the HLC of LLMs needs further academic investigation in the future. To the best of our knowledge, this paper is the first study to explore the boundaries of more advanced dimensional cognitive capabilities in LLMs. We hope that their capability of processing complex cognitive tasks can be enhanced from this study, enabling better comprehension and simulation of human thinking. However, HLC involves more subtle processes of language processing, which are not entirely transparent or predictable. Future work will focus on improving the interpretability, controllability, and generality.