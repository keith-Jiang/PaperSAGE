# Infer Human’s Intentions Before Following Natural Language Instructions

Yanming Wan1, Yue $\mathbf { W } \mathbf { u } ^ { 1 }$ , Yiping Wang1, Jiayuan $\mathbf { M a 0 } ^ { 2 * }$ , Natasha Jaques1\*

1University of Washington, Seattle, WA 98195 2MIT CSAIL, Cambridge, MA 02139 ymwan, nj @cs.washington.edu, jiayuanm@mit.edu

# Abstract

For AI agents to be helpful to humans, they should be able to follow natural language instructions to complete everyday cooperative tasks in human environments. However, real human instructions inherently possess ambiguity, because the human speakers assume sufficient prior knowledge about their hidden goals and intentions. Standard language grounding and planning methods fail to address such ambiguities because they do not model human internal goals as additional partially observable factors in the environment. We propose a new framework, Follow Instructions with Social and Embodied Reasoning (FISER), aiming for better natural language instruction following in collaborative embodied tasks. Our framework makes explicit inferences about human goals and intentions as intermediate reasoning steps. We implement a set of Transformer-based models and evaluate them over a challenging benchmark, HandMeThat. We empirically demonstrate that using social reasoning to explicitly infer human intentions before making action plans surpasses purely end-toend approaches. We also compare our implementation with strong baselines, including Chain of Thought prompting on the largest available pre-trained language models, and find that FISER provides better performance on the embodied social reasoning tasks under investigation, reaching the state-of-theart on HandMeThat.

# Code — https://github.com/Simon-Wan/FISER Extended version — https://arxiv.org/abs/2409.18073

# Introduction

Building AI assistants that can interact with people in a shared environment and follow their instructions would unlock assistive robotics and free up domestic labor. Toward this broad goal, we need to address the problem of “translating” realistic natural language instructions into actions executable by robots. The conventional way that people formulate this problem is grounded language learning, which aims at mapping abstract natural language phrases to concretely executable actions. However, these approaches miss an important component of many human-robot collaborative tasks, which is that the language humans tend to use in everyday scenarios is inherently ambiguous. Human speakers assume that listeners possess prior knowledge, leading them to omit certain information for efficiency (Grice 1975; Sperber and Wilson 1986; Clark 1996; Dennett 1987; Gergely et al. 1995). Resolving this ambiguity depends on leveraging other sources of information (e.g., human internal goals and historical actions) that are partially observable to the robot.

Consider the example shown in Fig. 1, where a human is tidying up a room. In the middle of her actions, she asks a robot for help, saying “Could you pass that from the sofa?” This instruction does not appear to be solvable without further information about the person’s underlying intention. While such internal mental states are not directly observed, agents can infer them from human’s past actions. Specifically, if the robot can observe that in previous steps, the human put several books into a box one by one, it can infer that she intends to use that box to store all the books. Based on this guess, the robot can check if there are any remaining books on the sofa and then hand them to the person.

Generally speaking, the ambiguity in the instruction mainly arises from two aspects. First, the human assumes sufficient prior knowledge about her hidden intentions (Dennett 1987; Gergely et al. 1995), which is based on the common sense knowledge that people tend to group similar items together when tidying up, and the observation that the human is gathering books. Second, people make trade-offs between accuracy and efficiency of communication (Grice 1975; Sperber and Wilson 1986; Clark 1996). This leads to the challenge of building AI agents that can follow efficient, ambiguous speech that people naturally adopt when giving directions.

We consider the case where there’s a human and a robot collaborating in a shared environment. The human is working on some tasks, and specifies a sub-task for the robot to help with by giving a natural language instruction. Past methods (e.g., language grounding) attempt to directly complete the specified command from the given instructions, since the human only acts as a disembodied issuer of instructions and is not another active agent in their environments. The human, as another partially observable factor in the environment, has been overlooked. In this paper, we present a new framework, Follow Instructions with Social and Embodied Reasoning (FISER), which suggests that we should introduce the human’s intention as explicit variables for the model to draw inferences about. By leveraging this structure, our framework opts to decompose the problem into two parts – social reason

Standard Language Grounding and Planning Objects on the sofa: Could you pass that from the sofa? NEWS ? MoPviecktoupthTeHAsTo?fa □ Follow Instructions with Social and Embodied Reasoning (FISER)   
Historical Actions of O Human is… Give a book storing books Move to the sofa H 自 into a box on the sofa Pick up   
table shelf Phase 1. Social Reasoning Phase 2. Embodied Reasoning

ing and embodied reasoning. Specifically, social reasoning is aimed at predicting the sub-task for which the human is asking for assistance, which can be inferred from the context of both the instruction and the observed historical actions of the person in the shared environment. After grounding the instructions into robot-understandable tasks, the robot can then do planning and interact with the environment, in a separate embodied reasoning phase. To further enhance the model’s ability to follow ambiguous instructions, we propose to explicitly add an extra plan recognition stage, where a set of logical predicates is used to help with inferring the human’s overall plan. We implement a Transformer-based model trained in a supervised learning manner to predict specified sub-tasks (and the human’s underlying plan) at intermediate layers. This step-by-step approach distinctly differs from the more commonly employed end-to-end methods in previous works.

Overall, the key insight of our method is that separating social and embodied reasoning by explicitly modeling the human’s intentions can significantly improve performance when following ambiguous natural language instructions. To test this hypothesis, we evaluate our models on a challenging benchmark, HandMeThat (HMT) (Wan, Mao, and Tenenbaum 2022), which involves ambiguous instruction following tasks in a text-based household environment. HMT contains a large number of physical objects and valid actions in each episode, as well as an enormous human goal space. We find that these properties make HMT challenging even for the largest state-of-the-art large language models (LLMs). As a competitive baseline, we also design a Chain-of-Thought (CoT) approach to prompt GPT-4 based on our framework. There are two findings from the experimental results. First, models which separate social and embodied reasoning using the FISER framework outperform end-to-end reasoning, in both Transformer-based models and CoT-prompted LLMs, which indicates that explicitly doing intermediate reasoning about human intentions is beneficial. Second, training small-scale models from scratch on this task outperforms our most sophisticated CoT prompting methods for large pre-trained LLMs, indicating that pretraining and domainspecific prompts are insufficient for LLMs to perform well on the challenging social and embodied reasoning tasks.

To summarize, the contributions of this paper are to propose the FISER framework, which performs instruction following by first using social reasoning and additional context to disambiguate what the human is asking, before using embodied reasoning to decide what actions to take to complete the task. We further introduce a human plan recognition stage to enhance social reasoning abilities when tasks are particularly complex or ambiguous. We empirically demonstrate that our FISER models show $6 4 . 5 \%$ success rate on the test set on average, achieving the state-of-the-art on HMT benchmark.

# Related Work

Grounded language learning. In order for AI to be useful to people in our homes and natural environments, non-experts need to be able to communicate with AI agents using natural language. This issue has long captured the attention of researchers (Winograd 1972; Siskind 1994), and the primary challenge involves mapping natural language to concrete meanings within the physical environment. Several studies explore language-conditioned task completion in specific environments (Shridhar et al. 2020; Suglia et al. 2021; Kojima, Suhr, and Artzi 2021). With the emergence of LLMs, many works discussed grounding language by leveraging pretrained LLMs (Blukis et al. 2021; Nair et al. 2022; Zellers et al. 2021; Zhi-Xuan et al. 2024; Min et al. 2024). A prominent example is SayCan (Ahn et al. 2022), which proposed extracting the knowledge in LLMs by using them to score the likelihood that a subtask available to the robot will help complete a high-level instruction. Although the above studies may incorporate common sense reasoning about language as well as information within the physical environment, their instructions explicitly express human intentions. For example, the most ambiguous instruction solved by SayCan is, “I spilled my coke, can you bring me something to clean it up?” where the ambiguity can still be easily resolved given that the sponge is the only cleaning tool in the environment. In contrast, we address the problem that realistic human instructions omit certain information for efficiency, making them much more ambiguous, and necessitating inferring human intentions to fill in the gaps.

Collaborative communication. We consider the case where the human and the robot are working in a shared environment, which is closely related to the literature on collaborative communication (e.g. Two Body Problem (Jain et al. 2019)). CerealBar (Suhr et al. 2019), DialFRED (Gao et al. 2022) and TEACh (Padmakumar et al. 2022) introduce collaborative tasks where the human works as a disembodied issuer of instructions, possibly responding to robot’s questions via explicit messages. In contrast, we consider the problem in which the AI assistant needs to consider both explicit messages in natural language and the implicit information in observed human actions. Further, we assume that instructions are not accurately describing the required information, but are generated based on a trade-off between informativeness and communication cost. Basically, we hope that robots can interpret ambiguous instructions without always needing clarification (asking questions). To this end, we focus on the HandMeThat (Wan, Mao, and Tenenbaum 2022) (HMT) benchmark, that calls for the ability to consider both explicit and implicit messages when following ambiguous instructions. The previous state-of-the-art work (Cao et al. 2024) on HMT performs iterated goal inference over the goal space in symbolic representation. However, it requires hand-crafted, pre-defined structures and extensive domain knowledge, which is not applicable in real-world scenarios.

Goal recognition. In our method, we hope to infer the human’s intentions based on the observed historical actions, which is related to goal recognition problem (Lesh and Etzioni 1995; Baker, Tenenbaum, and Saxe 2007; Levesque 2011; Meneguzzi and Pereira 2021). Most of the works are based on the assumption of rationale that an agent should make (approximately) optimal decisions towards the goals every step (Dennett 1987; Gergely et al. 1995). Understanding human intentions in embodied environments has also been studied in many works; for example in Watch-and-Help (Puig et al. 2021) the AI must infer the human’s goal from demonstrations, but no natural language is involved. Some recent works (Ying et al. 2024; Zhang et al. 2024) leverage LLMs to conduct goal inference based on the observed human actions or messages. In this work, we employ a small language model trained from scratch to undertake this part of the reasoning, since the aim is not to achieve precise goal recognition but to assist with the step-by-step social reasoning process.

Reasoning with intermediate steps. This work is also inspired by the research that uses intermediate steps to solve complex reasoning problems, including formal and mathematical reasoning and program synthesis (Roy, Vieira, and Roth 2015; Amini et al. 2019; Chiang and Chen 2019; Chen et al. 2020; Nye et al. 2021). Specifically, Nye et al. shows that step-wise prediction method performs better than directly predicting the final outputs in program synthesis when prompting LLMs. Chain-of-Thought (CoT) (Wei et al. 2022) thoroughly explores how generating intermediate reasoning steps improves the performances of prompting LLMs to deal with complex reasoning tasks. In this paper, we show that social reasoning tasks benefit from the same approaches, and demonstrate that inferring human intentions is a critical component of successful human-robot collaboration.

![](images/920246c57053f5724a57ab60b99740d2b922e1be887c1ffbf112900ef1c7b09d.jpg)  
Figure 2: Problem formulation and proposed method. White nodes are observable, while grey nodes are unobservable. The robot is given the trajectory $\tau _ { t ^ { \prime } }$ , a final state $s _ { t ^ { \prime } }$ , and an utterance $u$ . We propose to explicitly model human’s intentions by modeling the human’s overall plan $G ^ { h } \in \mathcal { G } ^ { h }$ as a set of predicates $p _ { k }$ . We further assume that human selects a subgoal $p ^ { * }$ that needs help, and then specifies a robot’s task $G ^ { r }$ , which is the underlying intention when saying $u$ .

# FISER: Follow Instructions with Social and Embodied Reasoning

Problem Formulation. A human-robot Markov Decision Process is described as a tuple $\langle S , \mathcal { A } ^ { h , r } , \mathcal { T } , \mathcal { U } , R ^ { r } , \gamma , T \rangle$ . $s \in$ $s$ are object-oriented states including the locations, status and type of each object and agent. $\mathbf { \mathcal { A } } ^ { h , \dot { r } }$ is the joint action space with $\mathcal { A } ^ { h }$ , $\boldsymbol { \mathcal { A } } ^ { r }$ being the sets of actions available to the human and the robot, respectively. $\mathcal { T } : \mathcal { S } \times \mathcal { A } ^ { h , r } \times \mathcal { S } \to \{ 0 , 1 \}$ is the transition function where $\mathcal { T } ( s , a ^ { h , r } , s ^ { \prime } ) = 1$ if and only if taking actions $a ^ { h , r }$ at state $s$ gives $s ^ { \prime }$ as the next state. $\mathcal { U }$ is a set of instructions that the human can give to the robot. $R ^ { r } : \mathcal { S }  \mathbb { R }$ is a reward function for the robot, $\gamma$ is the discount factor, and $T$ is the horizon. Throughout the paper, we consider a scenario with only a single round of instruction following for the robot. In each episode, starting from an initial state $s _ { 0 }$ , the human begins working in the environment, and the robot is waiting. Human stops at a time step $t ^ { \prime } \leq T$ leading to a trajectory $\bar { \tau _ { t ^ { \prime } } } = \bar { ( s _ { 0 } , a _ { 0 } ^ { h } , \bar { s _ { 1 } } , a _ { 1 } ^ { h } , \ldots , s _ { t ^ { \prime } - 1 } , a _ { t ^ { \prime } - 1 } ^ { h } ) }$ and a final state $\boldsymbol { s } _ { t ^ { \prime } }$ . Then the human produces a natural language instruction $u \in \mathcal { U }$ that asks the robot for help. Given $\tau _ { t ^ { \prime } } , s _ { t ^ { \prime } }$ and $u$ , the robot needs to interact with the environment by taking a sequence of actions $\{ a _ { t } ^ { r } \} _ { t \geq t ^ { \prime } }$ to maximize its discounted rewards $\begin{array} { r } { \sum _ { t = t ^ { \prime } } ^ { T } [ \gamma ^ { t - t ^ { \prime } } R ^ { r } ( s _ { t + 1 } ) ] } \end{array}$ .

# Modeling the Human’s Intentions

A straightforward solution to the human-robot MDP may treat $\tau _ { t ^ { \prime } }$ and $u$ as additional state information. However, in reality, $\tau _ { t ^ { \prime } } , u$ , and $R ^ { r }$ have important correlations: when the human is taking actions and producing instructions, their behavior can be modeled as optimizing for an internal reward function $R ^ { h } : \mathcal { S }  \mathbb { R }$ , which is not revealed to the robot. Our insight into this broad problem class is to leverage this causal relation between human’s behavior and instruction by explicitly modeling hidden, unobserved variables representing human goals and intentions, so that we can make better use of the human trajectory to disambiguate the instruction (recognize the robot’s task assigned by the human).

We start by assuming the reward functions $R ^ { h }$ can be parameterized by a set of possible goals $\mathcal G ^ { h }$ . The human’s goal $G ^ { h } \in \mathcal { G } ^ { h }$ is sampled from an underlying distribution over $\mathcal G ^ { h }$ at the beginning of an episode, and is fixed across the horizon. However, it is not revealed to the robot directly. The goal in $\mathcal G ^ { h }$ is usually global and complex, such as “organize the bedroom.” We assume the human trajectory $\tau _ { t }$ was rational, produced to maximize the reward $R ^ { \check { h } } ( \cdot \mid G ^ { \check { h } } )$ . Based on $G ^ { h }$ and the current progress $\tau _ { t ^ { \prime } }$ , the human then selects a subgoal $p ^ { * } \in G ^ { h }$ (a part of the human’s overall plan) that needs help, such as “having all books put in the box,” and then specifies a task $G ^ { r }$ for the robot (e.g., asking the robot to hand over a specific book). The instruction $u$ is generated based on $G ^ { r }$ . The relations between these variables are illustrated in Fig. 2.

Although we do not put specific assumptions over the structure of goals in $\vec { \mathcal { G } ^ { h } }$ and how $\tau _ { t }$ is generated, we illustrate them with a simplified example in Fig. 2. We define $P$ as a set of predicates where each $\textit { p } \in \textit { P }$ is a classifier over states (to say whether the predicate is satisfied or not). For example, one predicate can be written as $\langle \exists y , \boldsymbol { \mathsf { b o x } } ( y ) , \forall x , \boldsymbol { \mathsf { b o o k } } ( x ) \Rightarrow \ \mathrm { i n } ( x , y ) \rangle$ , which describes putting all apples in a box. Now we assume that the human goal $\hat { G } ^ { h } = \hat { \{ p _ { 1 } , p _ { 2 } , \cdot \cdot \cdot , p _ { l } \} }$ is a set of predicates. The human chooses to work on predicates one by one and has been working on all $p _ { k }$ ’s $\begin{array} { r } { \mathrm { ~  ~ 1 ~ } \leq k \leq l } { \gamma } \end{array}$ before stopping at time $t ^ { \prime }$ , and then the subgoal $p ^ { * }$ is chosen from the set of remaining predicates: $p ^ { * } \in \{ p _ { k } , . . . , p _ { l } \}$ . Next, the human specifies a robot’s task $G ^ { r }$ such that the robot actions will result in a state $s ^ { \prime }$ where $R ^ { h } ( s ^ { \prime } \mid \{ p ^ { * } \} ) > 0$ (i.e., $G ^ { r }$ is a useful step towards $p ^ { * }$ , a remaining subgoal to accomplish). Note that neither $p ^ { * }$ nor $G ^ { r }$ is accessible to the robot, since the robot can only get access to the natural language instruction $u$ . For example, “could you pass that from the sofa” could be an utterance for $G ^ { r } = \langle \mathrm { h u m a n - h o l d i n g ( b o o k \# 0 ) } \rangle$ and $p ^ { * } = \langle \exists y , \mathbf { b o x } ( y ) , \forall x , \mathbf { b o o k } ( x ) \Rightarrow \mathbf { i n } ( x , y ) \rangle$ .

# Step-wise Reasoning over Human Intentions

Our model, FISER, builds on top of the factorized humanrobot MDP formulation above. We formulate the problem into the social and embodied reasoning phases.

Social Reasoning: Robot’s Task Recognition The robot needs to disambiguate the natural language instruction $u$ into an understandable and executable task within its own goal space based on the observation of current state $\boldsymbol { s } _ { t ^ { \prime } }$ and the historical trajectory $\tau _ { t ^ { \prime } }$ . Therefore, we hope to estimate a function TR, such that $\mathrm { T R } \big ( s _ { t ^ { \prime } } , \tau _ { t ^ { \prime } } , u \big )  G ^ { \bar { r } }$ .

Social Reasoning: Human’s Plan Recognition. We further propose a variant that explicitly estimates human’s underlying overall plan $G ^ { h }$ based on $\tau _ { t ^ { \prime } }$ and replaces that trajectory by the predicted goal when doing instruction disambiguation.

However, since recognizing the full plan is usually intractable, we opt to also take in $u$ and $s _ { t ^ { \prime } }$ , and directly predict the predicate $p ^ { * } \in G ^ { h }$ (subgoal) that the human wants the robot to help with. Therefore, we learn two functions PR and TR, such that $\mathrm { P R } ( s _ { t ^ { \prime } } , \tau _ { t ^ { \prime } } , u )  p ^ { * }$ and $\mathrm { T R } ( s _ { t ^ { \prime } } , p ^ { * } , u )  G ^ { r }$ .

Embodied Reasoning: Grounded Planning. Once the robot goal $G ^ { r }$ is obtained, the problem is reduced to a pure grounding and planning task. We can replace the ambiguous natural language instruction $u$ by the accurately expressed robot goal $G ^ { r }$ . The final grounded planning function GP should satisfy that ${ \mathrm { G P } } ( s _ { t ^ { \prime } } , \tau _ { t ^ { \prime } } , G ^ { r } ) \to { \bar { \{ a _ { t } ^ { r } \} } } _ { t \geq t ^ { \prime } }$ , which is basically learning a typical goal-conditioned robot policy $\pi ( a | s _ { y ^ { \prime } } , \tau _ { t ^ { \prime } } , G ^ { r } ) ^ { * }$ .

Since the functions TR and PR involve natural language inputs, language models are required for these two modules. For GP, we can either implement planning algorithms or use neural networks, since all inputs can be symbolic.

# Transformer-based Model Implementation

We implement a Transformer-based model following our framework, illustrated in Fig. 3. We assume all inputs are rendered in texts, and the model needs to predict action strings. Since small-scale language models cannot process excessively long inputs, we divide the information into four parts, including world state description $\left( { { s } _ { t ^ { \prime } } } \right)$ , human’s trajectory $\left( \tau _ { t ^ { \prime } } \right)$ , language instruction $( u )$ , and the model’s past outputs. World state description. We consider an object-centric representation for the world state. Specifically, the world state is described as a sequence of object tokens. For each object in the world, we fuse the information of its category (object type and genre), attributes (e.g., size, color, is-open), and spatial relation (inside or on top of another object) into one single embedding, which we called as an object token. The human and the robot are also treated as two special “objects”.

Human’s trajectory. A straightforward way to represent human’s trajectory is to directly use a paragraph of texts to describe action sequence, e.g., “the human picks up book#1 from the table”. To better align it with the world state, we replace the embeddings for object names (“book#1”) by the object tokens that we obtained in the world state description. Language instruction. The natural language instruction sentence is tokenized and then directly turned into embeddings. Model’s past outputs. Every time the agent takes a step, the environment returns a sentence describing the effect of its action, i.e., the update in observations. In each episode, such sentences for past steps are concatenated and served as an extra input, e.g., “. . . [SEP] pick up book 0 [SEP] You pick up the book (book 0) from the table. . . ” The concatenated result is also tokenized and then turned into embeddings. This information is necessary because the model needs to know what it has done and whether the world state is changed.

# Model Architecture Overview

The proposed model consists of $3 N$ ( $N = 3$ ) encoder layers that are used to update the representation over all four parts of the inputs, in order to predict human intentions or robot’s actions. Specifically, we use the embeddings at Layer $2 N$ to predict the robot’s task $G ^ { r }$ (social reasoning phase). Then, we

World + Transformer Transformer Action Args   
Description Layer Layer Predictor Human’s Transformer Modality Transformer Modality Book#1   
Trajectory Layer Interaction Layer Interaction (Obj#30)   
ILnsatnrguctaigoen TraLnsafyoermer (MLP) RToabsokt’s TraLnsafyoermer (MLP) APctrieodnicTtyorpe Pick-up Model’s Transformer Transformer   
Past Outputs Layer Layer Next Action Social Reasoning × 2N layers Embodied Reasoning × N layers

replace the instruction input for Layer $2 N { + } 1$ by the predicted $G ^ { r }$ and then use the last layer embeddings to predict robot’s actions. If the Human’s Plan Recognition stage is further included, we use the embeddings at Layer $N$ to predict the selected human subgoal $p ^ { * }$ and then replace the trajectory input for Layer $N + 1$ by the predicted $p ^ { * }$ .

The model is trained in either a multi-staged (MS) or an end-to-end (E2E) manner. The E2E models are trained to directly output robot’s actions, but an auxiliary loss is applied over their intermediate predictions of robot’s task (and human’s plan). The MS models, however, disentangle the social reasoning (functions TR and PR) from embodied reasoning (function GP), and train them as two separate modules. The latter module is trained with the ground-truth $p ^ { * }$ or $G ^ { r }$ , but is evaluated using the predictions from the former module.

# Encoder Layers

Each encoder layer is composed of four Transformer layers and a Modality Interaction module.

Transformer Layers. We use four separate Transformer encoder-only layers to process the inputs. We remove the positional encoding for the world state description, because we do not expect the model to learn an ordering of objects.

Modality Interaction. In order to fuse the information from the four parts of inputs, we design a modality interaction module (an MLP) within each layer, following the architecture proposed by GreaseLM (Zhang et al. 2021). We reserve a special “interaction” token at the front of each part of inputs. These tokens are expected to gather respective information in the Transformer layers and then interact with each other through this MLP. The updated special tokens will then replace the original first token in each part of inputs.

# Prediction Layers

Now we introduce our predictors for intermediate reasoning steps and the final actions. All the predictions are trained with cross entropy loss over corresponding supervisions.

Human’s Plan Recognition. We assume that there is a vocabulary of concepts that allow us to represent human goals as first order logic predicates (e.g., $\langle \exists y , \bar { \mathbf { b o x } } ( y ) , \forall x , \mathbf { b o o } \bar { \mathbf { k } } ( x ) \Rightarrow$ $\left. \mathrm { i n } ( x , y ) \right.$ ). While such logical predicates could be more complex, here we assume human plans follow a simpler form: a Q(uantifier), a S(ubjective), a V(erb), and an O(bjective) (e.g., for-all, book, inside, $\scriptstyle \log \ x \rangle$ ). Therefore, the human’s plan recognition module of our model needs to predict a tuple of four tokens. The prediction is conditioned on the embeddings of the four inputs. Specifically, we calculate the log-likelihood over all tuples as follows, where the predictions of the Subjective and Objective are conditioned on the predicted values of Quantifier and Verb:

$$
\begin{array} { r } { \log \mathrm { P r } [ \mathbf { Q } , \mathbf { S } , \mathbf { V } , \mathbf { O } ] \approx \log \mathrm { P r } [ \mathbf { Q } ] + \log \mathrm { P r } [ \mathbf { V } ] \phantom { x x x x x x x x x x x x x x x x x x x x x x } } \\ { + \log \mathrm { P r } [ \mathbf { S } | \mathbf { Q } , \mathbf { V } ] + \log \mathrm { P r } [ \mathbf { O } | \mathbf { Q } , \mathbf { V } ] , } \end{array}
$$

Robot’s Task Recognition. The main target of the entire social reasoning phase is to predict the task assigned to the robot, such as a specific object to manipulate. Note that in this step the model needs to specify a concrete object in the world, while the Subjective and Objective predictions in previous plan recognition step are object types.

Action Prediction. The model needs to output the next action in each step, which we assume to be a triple consisting of action type and one or two arguments, (e.g., move-to(sofa), put-into(book#0, box#1)). The action type prediction is conditioned on the embeddings of the four inputs, while the arguments are further conditioned on the predicted action type. Specifically, we calculate the log-likelihood over all actions (no matter applicable or not).

$$
\begin{array} { r l } & { \log \operatorname* { P r } [ \mathrm { A c t i o n } , \mathrm { A r g } 1 , \mathrm { A r g } 2 ] \approx \log \operatorname* { P r } [ \mathrm { A c t i o n } ] } \\ & { \qquad + \log \operatorname* { P r } [ \mathrm { A r g } 1 | \mathrm { A c t i o n } ] + \log \operatorname* { P r } [ \mathrm { A r g } 2 | \mathrm { A c t i o n } ] . } \end{array}
$$

We assume the model has access to all applicable actions at each step, so we take the maximum over all applicable triples to get the final prediction.

# Experiments

We evaluate our framework by training a Transformer-based model from scratch for the challenging HandMeThat benchmark (Wan, Mao, and Tenenbaum 2022), and then compare them with multiple competitive baselines, including the stateof-the-art prior work on HMT, and the CoT prompting on the largest available pre-trained language models.

# HandMeThat Environment

We evaluate our models over the HandMeThat (version 2) dataset (Wan, Mao, and Tenenbaum 2022). It introduces a household ambiguous instruction following task rendered in text. HandMeThat instructions are split into four difficulty levels, and the gaps between levels correspond to different challenges. The instruction in a Level 1 task has no ambiguity—it is a pure planning task. A Level 2 task requires social reasoning where a robot can successfully accomplish the task if it can also infer the goal from the human trajectory. On Level 3, the robot needs to further consider pragmatic reasoning in language use. For example, if there are multiple books everywhere in the room and only one coat on the sofa, where both the book and the coat are helpful to the human’s goal, the human may not refer to the coat by saying “Could you pass that from the sofa” since “from the sofa” is a redundant specification in that case. The final Level 4 contains tasks with inherent ambiguities that cannot be resolved with the existing information, but can potentially be resolved with a strong prior over what human is likely to do. We evaluate all the models on their success rates to achieve the robot’s goal. Note that the original evaluation metric in HandMeThat additionally considers the number of robot’s steps. An agent can trivially improve success rate with increased steps, by simply searching in a trial-and-error fashion. We believe that enumeration over objects is not realistic in the real world, so we restrict our experiments to one trial (within 4 or 5 steps).

# Model Details

Baseline models. We compare our results to human performance on the task (Human), a hand-coded baseline (Heuristic) which have access to ground-truth symbolic state information, and a neural network baseline (Seq2Seq (Sutskever, Vinyals, and Le 2014)) introduced in the HandMeThat paper. The existing SOTA work (Cao et al. 2024) was implemented based on the original HandMeThat (version 1) dataset. Therefore, we report the results of our FISER models over original data points that lie in the version 2 domain. Further details of this comparison are provided in the supplementary material. Transformer-based model. Following the proposed model architecture, we implement a set of Transformer-based models. We compare the implementations with no intermediate supervision (Transformer), with Robot’s Task Recognition only (Transformer+FISER), and with Human’s Plan Recognition in addition (Transformer+FISER $+ \mathrm { P R }$ ). Our models are trained from scratch because existing small-scale pretrained models cannot handle the excessive token lengths of HandMeThat data inputs. We compare two ways of training the Transformer-based model using FISER framework, end-to-end (E2E) or multi-staged (MS), as explained in the overview of model architecture. This comparison aims to provide insights for whether to block the gradient flow from embodied reasoning back to the social reasoning module. We further report the accuracy of the intermediate prediction steps for these Transformer-based models, including QSVO (simplified human subgoal), and Obj (concrete object to be manipulated in expert demonstration, i.e., the robot’s task). The number of parameters in an E2E model is 5.1M; the number of parameters in an MS model is 4.7M.

Prompted GPT-4. We design prompting methods for GPT-4 Turbo over HMT tasks. The vanilla implementation (GPT-4) simply provides all inputs to the model and requests it to output actions. We first conduct prompt engineering (GPT- $4 { + } \mathrm { P E }$ ) to incorporate some domain-specific knowledge and help to parse the complex inputs. Then we implement FISER framework by applying CoT prompting to explicitly predict the same intermediate data (human’s plans and specified robot’s tasks) that we use for Transformer-based models step-bystep, which similarly gives two models (i.e., GPT- $^ { 4 + }$ FISER and GPT- $4 { + } \mathrm { F I S E R + P R }$ ). To be more specific, in both social reasoning steps, we prompt GPT-4 to do step-by-step reasoning except that we ask different questions. Human’s Plan Recognition asks about the human’s higher-level goal, while Robot’s Task Recognition asks about the intended meaning of an ambiguous instruction.

# Results

We evaluate all the models over the HandMeThat (version 2) dataset in the fully observable setting. Overall, our bestperforming Transformer+FISER model achieves a $6 4 . 5 \%$ success rate on average, achieving the state-of-the-art on the HandMeThat benchmark. The main results are presented in Table 1. Now we discuss the following hypotheses. H1: Explicitly modeling human intentions works better than directly predicting actions. (a) Separating the social and embodied reasoning steps by explicitly recognizing the robot’s task is beneficial.

For both prompted GPT-4 Turbo and Transformer-based models, explicitly predicting the robot’s task significantly improves the success rates across all difficulty levels, which supports our hypothesis that separating the social and embodied reasoning steps is beneficial in these complex reasoning tasks. The comparison between two different training schemes of our Transformer-based models are presented in Table 2. Results show that training in a multi-staged manner works better than end-to-end in our tasks. It may imply that the low-level grounded planning (embodied reasoning) is requiring a sufficiently different task representation from inferring human’s internal goals (social reasoning), that allowing gradients from the embodied reasoning module to flow into the social reasoning module actually hurts performance. It is a further support on empirical side that we should make explicit inferences about human intentions as intermediate reasoning steps.

# (b) Explicitly recognizing the human’s plan further helps with the social reasoning stage.

When we further include the Human’s Plan Recognition (PR) stage, we find that it only helps for the most ambiguous cases (like in Level 4). For GPT-4 Turbo, adding PR is showing approximately the same performances as normal FISER method. We hypothesize that pre-trained LLMs are not good at leveraging hierarchical predictions to improve on this task. For Transformer-based models, introducing PR gives better performance on Level 4, but is harmful to the simplest Level 1. We attribute the poor performance in Level 1 to the fact that such simplest tasks do not require knowing the humans’ high-level goal. Therefore, forcing the model to predict this information reduces model’s capacity to focus on planning for low-level actions. On the other hand, the improved performance on Level 4 shows that explicit human’s plan recognition helps to better learn priors over human intentions. Even on these intrinsically ambiguous tasks, the model can leverage the strong prior to take helpful actions.

Table 1: Success rate $( \% )$ of models over HMT in the fully observable setting. The results for Transformer-based models are the mean and standard error values over three runs. $^ { * } \mathbf { C } \mathrm { a o }$ et al. (2024) is evaluating version 1 of HandMeThat dataset, and thus we provide the results of Transformer+FISER model over a subset of version 1 for a fair comparison. Overall, FISER improves the performance across all levels compared to the vanilla Transformer. While applying FISER and PR to GPT-4 improves its performance, overall GPT-4 cannot perform well on these tasks even with very careful prompting, achieving less than half the success rate of our model for ambiguous instructions in levels 2-4.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="3">Baseline Models</td><td rowspan="2"></td><td colspan="3">GPT-4 Turbo</td><td rowspan="2"></td><td colspan="3">Transformer-based Models</td><td colspan="2">On HMT Version 1</td></tr><tr><td>Human Heur.</td><td>Seq2Seq</td><td>Vanilla</td><td>+PE</td><td>+FISER</td><td>+PR</td><td>Vanilla</td><td>+FISER</td><td>+PR</td><td>Cao et al.</td><td>FISER</td></tr><tr><td>Level 1</td><td>100.0</td><td>100.0</td><td>30.4</td><td>72.0</td><td>82.0</td><td>80.0</td><td>77.0</td><td>77.7±1.6</td><td>89.0±1.5</td><td>72.0±1.5</td><td></td><td>27.7±0.3</td><td>89.7±0.4</td></tr><tr><td>Level 2</td><td>80.0</td><td>64.0</td><td>28.8</td><td>16.0</td><td>25.0</td><td>36.0</td><td></td><td>34.0</td><td>55.3±0.4</td><td>74.0±0.3</td><td>74.0±0.9</td><td>24.8±0.4</td><td>63.0±0.3</td></tr><tr><td>Level 3</td><td>40.0</td><td>39.0</td><td>12.8</td><td>5.0</td><td>13.0</td><td>18.0</td><td>17.0</td><td></td><td>36.3±1.0</td><td>52.3±2.3</td><td>52.3±1.0</td><td>21.0±0.1</td><td>28.3±2.5</td></tr><tr><td>Level 4</td><td>30.0</td><td>29.0</td><td>14.8</td><td>9.0</td><td>9.0</td><td>17.0</td><td></td><td>20.0</td><td>38.3±1.4</td><td>42.7±0.2</td><td>51.0±1.2</td><td>21.7±0.2</td><td>40.7±1.9</td></tr></table></body></html>

Table 2: Comparison between End-to-End and Multi-staged training of Transformer-based models over HMT in the fully observable setting. All models are evaluated by the success rate $( \% )$ . The prediction accuracy $( \% )$ of QSVO and Obj (intermediate outputs) are presented in parentheses. The results are the mean values over three runs, and the standard error values for success rates are provided. Overall, training in a multi-staged manner works better than end-to-end in our tasks, implying that fully separating social from embodied reasoning provides the best performance.   

<html><body><table><tr><td>Model</td><td>Level 1</td><td>Level 2</td><td>Level 3</td><td>Level 4</td></tr><tr><td>End-to-End</td><td>77.7±1.6 (N/A,N/A)</td><td>55.3±0.4 (N/A,N/A)</td><td>36.3±1.0 (N/A,N/A)</td><td>38.3±1.4 (N/A,N/A)</td></tr><tr><td>End-to-End+FISER</td><td>74.3±0.2 (N/A,87.8)</td><td>73.7±0.5 (N/A,80.9)</td><td>47.3±2.1 (N/A,73.1)</td><td>41.3±1.9 (N/A, 64.7)</td></tr><tr><td>End-to-End+FISER+PR</td><td>61.0±1.5 (73.2,81.3)</td><td>66.7±1.1 (64.9,81.0)</td><td>47.0±1.2 (59.0, 73.4)</td><td>42.0±0.3 (55.0, 66.5)</td></tr><tr><td>Multi-Staged+FISER</td><td>89.0±1.5 (N/A, 93.4)</td><td>74.0±0.3 (N/A,82.3)</td><td>52.3±2.3 (N/A, 76.5)</td><td>42.7±0.2 (N/A, 68.1)</td></tr><tr><td>Multi-Staged+FISER+PR</td><td>72.0±1.5 (74.4,82.4)</td><td>74.0±0.9 (71.9, 82.3)</td><td>52.3±1.0 (67.1, 75.6)</td><td>51.0±1.2 (65.3, 71.2)</td></tr></table></body></html>

H2: Pre-trained LLMs, despite having access to commonsense knowledge, do not adequately perform the complex social and embodied reasoning in this task. Incorporation of domain-specific knowledge through CoT can help.

Results show that training much smaller, more efficient Transformer-based models from scratch is exhibiting about $70 \%$ increased performance than prompting state-of-the-art pre-trained LLMs. GPT-4 Turbo’s results on Level 1 show it has the capability to do some level of embodied reasoning when given explicit tasks. However, the performance drop on subsequent levels indicates that the required knowledge to solve HandMeThat tasks is not fully covered by commonsense knowledge in pre-trained LLMs. With well-designed prompt engineering (PE) that contains some domain knowledge (e.g., goal space and few-shot examples), GPT-4 Turbo improves significantly across all difficulty levels. However, even with careful CoT prompts and few-shot examples, GPT4 Turbo is far from small-scale Transformer models across all levels. As a qualitative analysis, its failure modes on Level 2 consists of $36 \%$ planning failure (hallucination or invalid actions), $31 \%$ incorrect intention (common-sense reasoning that are not aligned with the ground-truth), and $23 \%$ redundant behavior (giving human an object that is already at its target position). We believe this is because the prompting methods alone cannot provide the model with the type of social and embodied reasoning needed to solve this task. The information learned from language datasets collected online may also be significantly different from that required for this household assistance task. Training a small-scale model, however, can solve the problem more efficiently and reliably.

We conducted an additional experiment in the extended version to see if the performance of the pre-trained LLMs could be improved. By filtering out a proportion of objects in the world that are irrelevant to the human’s task, which assumes access to the ground-truth human goals. We observe that LLM’s performance relies on a very large proportion of objects being filtered out, which provides further insight that LLMs cannot effectively select information and focus on relevant objects, which is required in embodied reasoning.

# Conclusion

We study the challenging HandMeThat benchmark, comprising ambiguous instruction following tasks requiring sophisticated embodied and social reasoning. We find that existing approaches for training models end-to-end, or for prompting powerful pre-trained LLMs, are both insufficient to solve these tasks. We hypothesized that performance could be improved by building a model that explicitly performs social reasoning to infer the human’s intentions from their prior actions in the environment. Our results provide evidence for this hypothesis, and show that our approach, Follow Instructions with Social and Embodied Reasoning (FISER), enhances performance over the most competitive prompting baselines by $70 \%$ , setting the new state-of-the-art for HandMeThat.