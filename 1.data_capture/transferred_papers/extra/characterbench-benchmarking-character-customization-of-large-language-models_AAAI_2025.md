# CHARACTERBENCH: Benchmarking Character Customization of Large Language Models

Jinfeng Zhou1\*, Yongkang Huang2\*, Bosi Wen1\*, Guanqun $\mathbf { B } \mathbf { i } ^ { 1 }$ , Yuxuan Chen1, Pei $\mathbf { K } \mathbf { e } ^ { 1 }$ , Zhuang Chen1, Xiyao Xiao2, Libiao $\mathbf { P e n g } ^ { 2 }$ , Kuntian Tang2, Rongsheng Zhang3, Le Zhang3, Tangjie $\mathbf { L } \mathbf { v } ^ { 3 }$ , Zhipeng ${ \bf { H } } { \bf { u } } ^ { 3 }$ , Hongning Wang1, Minlie Huang1

1The CoAI Group, DCST, Tsinghua University 2Lingxin AI 3Fuxi AI Lab, Netease zjf23 $@$ mails.tsinghua.edu.cn, aihuang $@$ tsinghua.edu.cn

# Abstract

Character-based dialogue (aka role-playing) enables users to freely customize characters for interaction, which often relies on LLMs, raising the need to evaluate LLMs’ character customization capability. However, existing benchmarks fail to ensure a robust evaluation as they often only involve a single character category or evaluate limited dimensions. Moreover, the sparsity of character features in responses makes featurefocused generative evaluation both ineffective and inefficient. To address these issues, we propose CHARACTERBENCH, the largest bilingual generative benchmark, with 22,859 humanannotated samples covering 3,956 characters from 25 detailed character categories. We define 11 dimensions of 6 aspects, classified as sparse and dense dimensions based on whether character features evaluated by specific dimensions manifest in each response. We enable effective and efficient evaluation by crafting tailored queries for each dimension to induce characters’ responses related to specific dimensions. Further, we develop CharacterJudge model for cost-effective and stable evaluations. Experiments show its superiority over SOTA automatic judges (e.g., GPT-4) and our benchmark’s potential to optimize LLMs’ character customization.

# Introduction

Character-based dialogue (aka role-playing) popularly built upon LLMs (Touvron et al. 2023a,b) enables users to freely customize characters for interaction (Wang et al. 2023b; Zhou et al. 2023a; Lu et al. 2024). Similarweb (2024) reports millions of users customize characters on Character.AI for various scenarios, from entertainment and education to social companionship, covering diverse character categories from fictional characters (e.g., Mario) and celebrities (e.g., Shakespeare) to daily life characters (e.g., friends, psychologists). To foster such extensive applications, evaluating LLMs’ capability in character customization thus becomes crucial. Existing benchmarks (Chen et al. 2024a; Wang et al. 2024) often dissect this capability into various evaluation dimensions that reflect how well LLMs’ customized characters mimic target roles, e.g., knowledge accuracy and empathy (Tu et al. 2024), and then score characters on these dimensions to compare different LLMs. Despite their efforts, existing approaches still suffer from several serious issues.

The first issue is lack of both diverse characters and comprehensive dimensions for a robust evaluation. Diverse characters are vital for exploring LLMs’ generalizability, preventing evaluations from missing potential defects. And comprehensive dimensions offer detailed insights into LLMs’ limitations. Yet, limited by the source of public corpora and characters available for crafting benchmarks, most existing works often involve only a small number of fictional characters or evaluate insufficient dimensions (Xiao et al. 2023), e.g., Tu et al. (2024) evaluated 13 dimensions but only included 77 fictional characters, Wang et al. (2024) only evaluated 32 fictional characters on 2 dimensions.

The second issue is caused by the sparse manifestation of character features within responses. Character features include attributes (e.g., views on specific matters) and behaviors (e.g., linguistic style) specified in a character’s profile (Zhou et al. 2023a) as well as other human traits (e.g., emotional expression and memory recall). However, natural interactions often occur in an open-ended dialogue context, making it less likely to observe multiple character features manifested in a single response. For example, as shown in Figure 1, the open-ended user query “You must have sacrificed a lot” only triggered the character’s specified view expressed in the response “Sacrifice? It’s all worth it”, causing the sparsity issue in feature-focused evaluation (Zheng et al. 2020). This issue makes existing generative benchmarks hard to guarantee that generated responses are always suited to specified evaluation dimensions, thus harming data utilization and evaluation efficiency (Tu et al. 2024). Although Chen et al. (2024a) design multiple-choice question (MCQ)-based benchmarks to alleviate this sparsity issue, it overly simplifies the character-based dialogue task and thus cannot fully evaluate the generative quality of the models.

To address these issues, we propose CHARACTERBENCH, a bilingual generative benchmark including 22,859 humanannotated samples to evaluate LLMs’ character customization capability. It features an effective and efficient evaluation of all dimensions. Firstly, to ensure a robust evalua

CHARACTERBENCHFramework Dense Dimensions in 2 Aspects Believability Human- Engagem Morality Moralitys Morality SparseDimensionsin4Aspects Memory Memory Consistency 8三 Persona Atributey Cehaviorey Boundary Acurtey Emotio EotiEes ExampleofBoundaryConsistency Character Profile 克伦威尔，身为17世纪英国历史上的杰出政治家和将军他认为他可以为 国家牺牲一切.克伦威尔的生活和思想仅限于17世纪的英国历更背景 Cromwell,asanoutstandingpoliticianand general inthe history ofEngland inthe17th century...He believes he could sacrifice everything for his country...His life and 'thoughtsare limited tothehistorical background of Englandinthe17thcentury DialogueContext (The above multi-turn dialogue is omitted.)/\*.\*\*/牺牲？那都是值得的。 (Theabovemulti-turndialogueisomitted.)/\* \*/Sacrifice?It'sallworthit 那成为你这样杰出的政治家需要什么呢？ What does it take to becomea distinguished politician like you? 首先你要有坚定的信念，然后是勇气和智慧解决问题，掌握关键信息和时机 First,youneedtohaveafirmbelief,thenthecourageandwisdomtosolve problems,and theability to grasp key informationand timing. Target-orientedQueryConstruction 克伦威尔的生活和思想仅限于 嗯，翼的算智道 17世纪的英国历更背 Target-oriented" Cromwel's life and thoughts Query Well,speakingofsolving are limited to the historical Construction problems,areyoufamiliar Target backgroundofEngland in the withcomputersinthereal 17th century. world? Target-basedEvaluation 然，现在计算机已经渗透到我们生活的方方面面，它 下 述能完成许多人类无法完成的在雾，如自动驾驶 Target-based 全 。 Evaluation × Ofcourse,computers have nowpermeated everyaspect of Worst ourlives.Theynot only....butalsoperformmanytasks that humanscannot,e.g.,drivingcarsautomatically.

tency dimension of the character’s knowledge aspect. Then, we craft target-oriented queries (e.g., “...are you familiar with computers...”) to induce the character’s responses to be closely related to the intended dimension (e.g., response “Of course, computers...” shows an inconsistent character boundary). For dense dimensions, we construct target-free queries that naturally induce the character’s responses in specific dimensions (e.g., toxic query for morality’s dimensions). All character responses in each dimension are carefully scored by human annotators. Thirdly, we develop the CharacterJudge model, fined-tuned on our training data, to provide a cost-effective and stable alternative to automatic judges (e.g., GPT-4) for scoring LLMs’ character customization. Our model outperforms SOTA automatic judges in correlation with human judges. We show our benchmark’s potential to optimize LLMs’ character customization via direct preference optimization (DPO) (Rafailov et al. 2023).

tion, for characters, we collect a large-scale character-based dialogue corpus, covering 3,956 characters across 25 subcategories of 4 main character categories. To exhaustively define the evaluation dimensions, we review existing studies (Tu et al. 2024; Chen et al. 2024a) and draw on interpersonal interaction theory (Kruglanski and Higgins 2013), identifying 6 high-level aspects that reflect character features and include 11 evaluation dimensions (Figure 1): recall of memory (Baddeley 1997), exposure of knowledge (Anderson 2005), exhibition of persona (Jung 2014), expression of emotion (Salovey and Mayer 1990), adherence to morality (Kohlberg 1921), and believability compared with real characters (Zhou et al. 2023a). Based on whether the character features corresponding to specific dimensions will always manifest in each response, we classify them as dense (dimensions in morality and believability aspects) and sparse (dimensions in other 4 aspects) dimensions. Secondly, to ensure an effective and efficient evaluation of each dimension, we design queries for each dimension to induce the character to generate responses related to the specific dimension. For sparse dimensions, we introduce target-oriented generation. As the example shown in Figure 1, we extract the information fragment “...17th-century historical context of England” from the character profile to set up the character’s intended response for evaluating the boundary consis

Our contributions are summarized as follows: (1) To the best of our knowledge, CHARACTERBENCH, with 22,859 human-annotated samples, is the largest bilingual generative benchmark to evaluate LLMs’ character customization capability. (2) We dissect this capability into dense and sparse dimensions, each with carefully crafted queries to induce character’s responses related to them, enabling an effective and efficient evaluation. (3) Extensive experiments conducted with our developed CharacterJudge show its superiority over SOTA automatic judges (e.g., GPT-4) and our benchmark’s potential to optimize LLMs’ character customization.

# Related Work

Character-based dialogue (aka role-playing) allows users to freely customize characters for interactions, attracting attention from academics (Chen et al. 2024b) and industry (e.g., Character.AI). This customization is often based on generalpurpose LLMs (Meta 2024; Yang et al. 2024) with role-play prompting (Yu et al. 2022) or developing LLMs specifically for character customization by collecting data from various sources, e.g., extraction from literature resources (Li et al. 2020; Chen et al. 2023; Li et al. 2023; Occhipinti, Tekiroglu, and Guerini 2023; Xu et al. 2024), synthesis via LLMs (Tu et al. 2023; Wang et al. 2023b; Shao et al. 2023; Lu et al. 2024), and human role-playing (Gosling, Dale, and Zheng 2023; Zhou et al. 2023a). The customized character categories span from fictional characters and celebrities to daily life characters, supporting various scenarios, e.g., entertainment and social companionship (Similarweb 2024).

To evaluate LLMs’ capability in character customization (Zhang et al. 2024), there are two types of existing work. One leverages generative evaluation (Yuan et al. 2024; Zhou et al. 2023b), which is the main focus of this paper. It evaluates the responses generated by LLMs but often fails to ensure that these responses are associated with the evaluated dimensions (Zheng et al. 2020), leading to ineffective and inefficient evaluation. The other is in an MCQ-based format (Shen, Li, and Xiong 2023; Salemi et al. 2024), which takes responses that reflect specific dimensions as correct choices. But it overly simplifies the character-based dialogue task and thus cannot fully evaluate the generative quality of the models. Moreover, most existing benchmarks focus only on fic

H H Target-oriented Query Construction for Sparse Dimensions Roul-Playing 3 Character- 0 Seritg Script 理 Character Extraction Target Ro LLMs Query 白 LLMs Target Fromt Dialogue Corpus rs Dialtgxte Tarceton 一 Human 5 Human 酷 Emotion QTerget-orientedon Filtering Script Response Generation and E Character Script Target-free Query Construction for Dense Dimensions 宿 Score 兰 Response 中 Target-free Script Target-oriented! Gomer Dialogue Believability @ Morality o Script Query Judge uery

tional characters (Chen et al. $2 0 2 4 \mathrm { a }$ ; Tu et al. 2024; Ahn et al. 2024) or evaluate limited dimensions (e.g., Xiao et al. (2023) and Wang et al. (2024) only involve two dimensions), failing to ensure robust evaluation. Our benchmark covers most dimensions included in existing generative benchmark (Tu et al. 2024). We do not evaluate MBTI and Big-five personality as their evaluations require a very well-rounded character profile for each character and a standardized testing environment with recognized reliability and validity (Furnham 1996), which is unsuitable for generative evaluations.

# CHARACTERBENCH Framework

To exhaustively evaluate the authenticity of characters in interactions, we review existing studies and draw on interpersonal interaction theory (Kruglanski and Higgins 2013) to identify 6 aspects that reflect character features. Along with manual inspections in 80 dialogues from our humanprototype (i.e., LLMs) interaction corpus (Sec. 3.2), we refine these aspects into 11 evaluation dimensions. We classify dense (dimensions in morality and believability) and sparse (dimensions in other 4 aspects) dimensions by whether character features evaluated by specific dimensions manifest in each response. Their definitions are as follows.

Given the script containing character profile $\mathcal { P }$ and dialogue context $\mathcal { C } = [ u _ { 1 } , y _ { 1 } , \dotsc , u _ { n - 1 } , y _ { n - 1 } ]$ , and user query $u _ { n }$ , the goal of a character customized by LLM is to generate a response $y _ { n } = L L M ( \mathcal { P } , [ \mathcal { C } \oplus u _ { n } ] )$ . Here, $u _ { k }$ and $y _ { k }$ denote the $k ^ { t h }$ -turn utterances from the user and the character, respectively. The response $y _ { n }$ is our evaluation object.

• Memory refers to an individual’s ability to acquire, store, retain, and subsequently retrieve information (Baddeley 1997). We define Memory Consistency to measure how stably the character retains information about facts and events from the conversational interactions $\mathcal { C }$ . This ensures that the information displayed in $y _ { n }$ aligns consistently with what has been stored during the interaction $\mathcal { C }$ .

• Knowledge refers to an individual’s fact and world knowledge, acquired through learning and experience, which forms the basis for social interactions (Anderson 2005). We define Fact Accuracy as the accuracy with which the character’s response $y _ { n }$ reflects factual knowledge related to itself. Additionally, Boundary Consistency evaluates how consistently $y _ { n }$ distinguishes the knowledge inherent to the worldview established in the character profile $\mathcal { P }$ .

• Persona refers to an individual’s attributes (e.g., identity, views) and behaviors (e.g., linguistic style) presented to fulfill expectations of societal role (Jung 2014). We define Attribute Consistency and Behavior Consistency to respectively measure how well the character’s response $y _ { n }$ aligns with the attributes and behaviors in its profile $\mathcal { P }$ .

• Emotion refers to an individual’s ability to recognize, understand, and manage own and others’ emotions (Sabour et al. 2024). We define Emotional Self-regulation to assess the character’s ability in $y _ { n }$ to identify and manage its own emotions, and Empathetic Responsiveness to evaluate how well $y _ { n }$ recognizes and soothes user’s emotions.

• Morality refers to the ethical principles and behavioral norms that an individual adheres to in social interactions (Kohlberg 1921). We define Morality Stability as the LLMs’ ability in $y _ { n }$ to maintain a positive morality when the context $\mathcal { C }$ is injected with toxic queries, and Morality Robustness as the ability in $y _ { n }$ to uphold positive morality even when the character profile $\mathcal { P }$ endows toxic settings.

• Believability refers to the realism exhibited by virtual characters during interactions (Zhou et al. 2023a). We split it into two parts: Human-likeness evaluates the naturalness of the character’s response $y _ { n }$ in dialogues, and $\mathbf { E n } .$ - gagement measures the depth of users’ interest and their emotional connection with the character through $y _ { n }$ .

# CHARACTERBENCH Construction

# Overview

As shown in Figure 2, CHARACTERBENCH’s construction pipeline as: (1) We collect the character-based dialogue corpus following four different ways. (2) We sample scripts from our corpus that include character profiles and dialogue context. These scripts serve to construct target-oriented and target-free queries for sparse and dense dimensions, respectively. (3) We concatenate constructed queries with scripts and input them into LLMs, inducing LLMs to generate character responses related to specific evaluation dimensions.

Table 1: Statistics of our character-based dialogue corpus.   

<html><body><table><tr><td>CorpusSources</td><td># Characters</td><td>#Dialogues</td><td># Avg. Turn ofDialogues</td><td>#Avg.Length ofUtterances</td></tr><tr><td>HRP: Human Role-Playing HPI:Human-Prototype Interaction</td><td></td><td></td><td>ELR:Extraction fromLiterary Resources SPI:Synthesis via Prototypes' Interaction</td><td></td></tr><tr><td>HRP</td><td>2,485</td><td>3,269</td><td>16.33</td><td>29.52</td></tr><tr><td>HPI</td><td>1,017</td><td>4,827</td><td>14.86</td><td>23.07</td></tr><tr><td>ELR</td><td>77</td><td>4,563</td><td>3.16</td><td>27.69</td></tr><tr><td>SPI</td><td>500</td><td>503</td><td>19.00</td><td>51.51</td></tr><tr><td>Total</td><td>3,956</td><td>13,162</td><td>11.33</td><td>27.68</td></tr></table></body></html>

These responses are carefully scored by human annotators, which will be later used to train our CharacterJudge model.

# Collection of Character-based Dialogue Corpus

Following Zhou et al. (2023a), our character-based dialogue corpus is collected via human role-playing, humanprototype interaction, and extraction from literary resources. The differences from Zhou et al. (2023a) are: (1) In the human role-playing corpus, we manually annotate the user query-character response pairs that reflect the character’s knowledge boundaries and persona attributes in the profile. (2) 7 popular LLMs server as prototypes. (3) We use the test set from CharacterEval (Tu et al. 2024) as our extraction data. Moreover, we propose synthesis via prototypes interaction to diversify our corpus. We employ paired LLMs (i.e., prototypes) for dialogue interactions, where one acts as the “Character” and the other plays the “User”. Both profiles are manually crafted. Details are in the Appendix.

Quality Control and Statistics of Corpus We hire a dedicated team of quality inspectors to check data quality. The entire corpus is carefully inspected on both parties’ profiles, worker engagement, and dialogues. Any data identified as low-quality is excluded from the following construction of CHARACTERBENCH. The dialogue statistics and character distributions of our corpus are in Table 1 and Figure 3. To the best of our knowledge, it is the largest corpus (13,162 dialogues) covering the most diverse characters (3,956 characters across 25 sub-categories of 4 main categories).

LLMs We use 7 LLMs as prototypes, including generalpurpose LLMs (GPT-4-1106 (OpenAI 2023), Claude-opus (Anthropic 2023), and GLM-4 (GLM et al. 2024)) instructed to perform role-playing (prompts are in Appendix). CharacterGLM (Zhou et al. 2023a), MiniMax-abab5.5s (MiniMax 2023), Baichuan-NPC (Yang et al. 2023), and CharacterYuyan (FuxiAI 2024) are specifically developed for character-based dialogue. All LLMs are accessible via APIs and used in the following CHARACTERBENCH collection.

# Collection of CHARACTERBENCH Data

Script Sampling To maintain diversity in our CHARACTERBENCH, we randomly sample scripts from distinct characters in our corpus to craft data for each dimension. Each script contains a character profile $\mathcal { P }$ and a multi-turn context $\mathcal { C } = [ u _ { 1 } , y _ { 1 } , . . . , u _ { n - 1 } , y _ { n - 1 } ]$ $\langle n \geq 5 \rangle$ ). We balance the distribution of characters and corpus sources in this process. Next, we craft target-oriented query $u _ { n , \tau }$ and target-free query $u _ { n , \mathcal { F } }$ for sparse and dense dimensions, respectively.

![](images/17499f380c93e521cccd55d4a58bfefeddfa9c980230a92d709f43b5cffff805.jpg)  
Figure 3: Category distributions of characters in CHARACTERBENCH, with 4 main categories and 25 sub-categories.

Target-oriented Query Construction for Sparse Dimensions To effectively and efficiently evaluate sparse dimensions, we integrate automatic (LLM prompting with GPT-4 and GLM-4) and manual strategies to extract targets that reflect specific dimensions and craft target-oriented queries. Specifically, for a script containing profile $\mathcal { P }$ and context $\mathcal { C }$ , we extract information fragment from $\mathcal { P }$ or $\mathcal { C }$ as target $\tau$ . Guided by $\tau$ , we craft target-oriented query $u _ { n , \tau }$ as the n- $^ { t h }$ turn utterance of context $\mathcal { C }$ , obtaining target-oriented context $\scriptstyle { \mathcal { C } } _ { T }$ . $\scriptstyle { \mathcal { C } } _ { T }$ replaces $\mathcal { C }$ in the original script, serving for inducing characters customized on LLMs to subsequently generate responses related to specific dimensions, formalized as:

$$
\begin{array} { r } { \begin{array} { r } { \mathcal { T } = f _ { e } ( \mathcal { P } ) \mathrm { o r } f _ { e } ( \mathcal { C } ) , \quad } \\ { u _ { n , \mathcal { T } } = f _ { f } ( f _ { q } ( \mathcal { P } , \mathcal { C } , \mathcal { T } ) ) , \quad } \\ { \mathcal { C _ { T } } = [ u _ { 1 } , y _ { 1 } , . . . , u _ { n - 1 } , y _ { n - 1 } ] \oplus u _ { n , \mathcal { T } } , } \end{array} } \end{array}
$$

where $\oplus$ is the concatenation operation. Both the target extraction $f _ { e }$ and query construction $f _ { q }$ are performed automatically or manually. To ensure a smooth concatenation, we employ dual-filtering $f _ { f }$ (automatic and manual) to filter queries that match the user’s tone and are coherent with the context. We present details for each dimension as follows.

• Memory. For memory consistency, we prompt LLMs to extract a fact or event mentioned within $\mathcal { C }$ as the target and then simulate the user’s tone to generate a query $u _ { n , \tau }$ that inquires about the extracted information fragment.

• Knowledge. For fact accuracy, we only use a celebrity subset of our corpus, whose profiles $\mathcal { P }$ are enriched and manually calibrated by information from BaiduBaike. We divide the character profile $\mathcal { P }$ into two parts: a brief profile $\mathcal { P } ^ { \prime }$ , used to establish the character’s identity, and a detailed profile ${ \mathcal { P } } ^ { \prime \prime }$ , covering factual knowledge about the character. We prompt LLMs to extract factual knowledge from ${ \mathcal { P } } ^ { \prime \prime }$ as the target and generate the query $u _ { n , \tau }$ . Ultimately, only $\mathcal { P } ^ { \prime }$ is used in subsequent response generation. For boundary consistency, we manually extract targets from $\mathcal { P }$ and craft queries in the human roleplaying corpus.

• Persona. For attribute consistency, we prompt LLMs to extract attributes as the target from $\mathcal { P }$ and generate the query $u _ { n , \tau }$ . This process is also manually conducted in the human roleplaying corpus. These two query types are termed bot- and human-query, respectively. For behavior consistency, the LLM prompting method is used to construct the bot query by extracting behaviors as the target from $\mathcal { P }$ . Additionally, to further evaluate behavioral controllability, we manually create 130 behavioral descriptions. We instruct LLMs to remove existing behavioral information from $\mathcal { P }$ and randomly select a new behavioral description ${ \mathcal { P } } ^ { \prime \prime }$ , to augment $\mathcal { P }$ , creating $\mathcal { P } ^ { \prime }$ . The next user utterance $u _ { n }$ of context $\mathcal { C }$ in the original dialogue serves as human query $u _ { n , \tau }$ to obtain $\scriptstyle { \mathcal { C } } _ { T }$ . $\mathcal { P } ^ { \prime }$ and $\scriptstyle { \mathcal { C } } _ { T }$ are used to generate a response $y _ { n }$ that aligns with the target ${ \mathcal { P } } ^ { \prime \prime }$ .

• Emotion. For emotional self-regulation and empathetic responsiveness, we prompt LLMs to extract emotionally charged scenarios from user utterances $\left[ u _ { 1 } , . . . , u _ { n - 1 } \right]$ and character utterances $[ y _ { 1 } , . . . , y _ { n - 1 } ]$ within $\mathcal { C }$ . LLMs then generate queries $u _ { n , \tau }$ that probe the emotions of the user and character in that target scenario, respectively.

Target-free Query Construction for Dense Dimensions To evaluate the dense dimensions, we adopt the manual strategy to construct the target-free query $u _ { n , \mathcal { F } }$ that could readily induce characters’ responses related to these dimensions. $u _ { n , \mathcal { F } }$ is concatenated with $\mathcal { C }$ to form the target-free context $\mathcal { C } _ { \mathcal { F } }$ , which replaces $\mathcal { C }$ in the original script, formalized as:

$$
\begin{array} { c } { { u _ { n , \mathcal { F } } = f _ { f } ( f _ { q } ( \mathcal { P } , \mathcal { C } ) ) , } } \\ { { \mathcal { C } _ { \mathcal { F } } = [ u _ { 1 } , y _ { 1 } , . . . , u _ { n - 1 } , y _ { n - 1 } ] \oplus u _ { n , \mathcal { F } } , } } \end{array}
$$

where both $f _ { q }$ and $f _ { f }$ only involve the manual strategy. We present details for each dimension as follows.

• Morality. We adopt 9 widely-recognized morality categories (Sun et al. 2023): insult, unfairness and discrimination, crimes and illegal activities, physical harm, mental health, privacy and property, ethics, politics, and pornography. For each category, we manually craft 100 queries and $5 0 { \sim } 2 0 0$ immoral character settings, with their distribution shown in Appendix. For morality stability, we employ the queries as $u _ { n , \mathcal { F } }$ . For morality robustness, besides using these queries, we craft the toxic profile $\mathcal { P } ^ { \prime }$ by fusing immoral character settings into character profile $\mathcal { P }$ . • Believability. Each character’s response in a natural dialogue would display human-likeness and engagement. Thus, we manually select the next user utterance $u _ { n }$ of context $\mathcal { C }$ in the original dialogue as the query $u _ { n , \mathcal { F } }$ .

Response Generation and Human Annotation We input scripts fusing target-oriented or target-free queries into 7 LLMs used in corpus construction to generate response $y _ { n }$ , where profile $\mathcal { P }$ is replaced by $\mathcal { P } ^ { \prime }$ in some dimensions. Especially, for Morality’s two dimensions, we sample $m$ queries $( m \in [ 1 , 2 , 3 ] )$ from each category acting as multiturn queries $u _ { n - 1 + k }$ $( k \in \ [ 1 , m ] )$ . We use only the last query $u _ { n - 1 + m }$ as $u _ { n , \mathcal { F } }$ to evaluate its response. Each turn of queries and their responses are concatenated into $\mathcal { C }$ , i.e., $\mathcal { C } = [ u _ { 1 } , y _ { 1 } , . . . , u _ { n - 1 } , y _ { n - 1 } , u _ { n } , y _ { n } , . . . , u _ { n - 1 + m } ]$ .

For each dimension, human annotators score the response $y _ { n }$ . After manually reviewing 200 samples in each dimension, we established four annotation scales based on data characteristics: (1) a 2-point scale for Morality Stability and Morality Robustness; (2) a 3-point scale for Boundary Consistency and Behavior Consistency (human query); (3) a 5- point scale for Human-likeness and Engagement; (4) a 4- point scale for other dimensions. Detailed explanations of these scales and data examples are shown in the Appendix.

Table 2: Statistics of CHARACTERBENCH. TPR is the translation pass rate $( \% )$ . More statistics are in the Appendix.   

<html><body><table><tr><td colspan="5">Dimensions #Samples#Characters#Avg.TurnsTPR</td></tr><tr><td>Memory Consistency</td><td>1,714</td><td>1,573</td><td>11.51</td><td>99.2</td></tr><tr><td>Fact Accuracy</td><td>1,776</td><td>105</td><td>10.86</td><td>98.2</td></tr><tr><td>Boundary Consistency</td><td>1,472</td><td>1,210</td><td>12.62</td><td>98.4</td></tr><tr><td>Attribute Consistency (Bot)</td><td>1,651</td><td>1,509</td><td>11.03</td><td>98.0</td></tr><tr><td>Attribute Consistency (Human)</td><td>1,243</td><td>970</td><td>9.50</td><td>95.7</td></tr><tr><td>Behavior Consistency (Bot)</td><td>2,162</td><td>1,563</td><td>11.40</td><td>94.6</td></tr><tr><td>Behavior Consistency (Human)</td><td>2,198</td><td>2,100</td><td>10.27</td><td>96.9</td></tr><tr><td>Emotional Self-regulation</td><td>1,274</td><td>966</td><td>11.47</td><td>91.2</td></tr><tr><td>Empathetic Responsiveness</td><td>1,335</td><td>987</td><td>10.93</td><td>96.7</td></tr><tr><td>Morality Stability</td><td>2,290</td><td>2,191</td><td>12.28</td><td>96.9</td></tr><tr><td>Morality Robustness</td><td>2,288</td><td>2,286</td><td>12.29</td><td>95.7</td></tr><tr><td>Human-likeness</td><td>1,742</td><td>1,676</td><td>10.46</td><td>98.6</td></tr><tr><td>Engagement</td><td>1,714</td><td>1,664</td><td>10.48</td><td>97.7</td></tr><tr><td>Overall</td><td>22.859</td><td>3,956</td><td>11.22</td><td>96.8</td></tr><tr><td>- Training Set</td><td>19,609</td><td>3,314</td><td>11.22</td><td></td></tr><tr><td>- Test Set</td><td>3,250</td><td>1,986</td><td>11.24</td><td>1</td></tr><tr><td>-Test Set (In-domain)</td><td>1,625</td><td>1,344</td><td>11.20</td><td></td></tr><tr><td>-Test Set(Out-of-domain)</td><td>1,625</td><td>642</td><td>11.28</td><td></td></tr></table></body></html>

Quality Control of CHARACTERBENCH We hire a dedicated team of quality inspectors who are instructed on annotation guidelines and examples of each dimension. Our methods for quality control are as follows.

• Annotator Training. All the annotators are required to complete a training tutorial that includes 100 samples from each dimension for pilot annotation. We provide feedback to help them calibrate the annotation criteria.

• Multi-person Annotation. In the annotation, each sample is annotated by two different annotators. If their results are inconsistent, a third annotator is called upon to re-annotate and discuss the case with the first two annotators to reach a consensus.

• Spot Check. To more effectively calibrate the annotation criteria, we conduct annotation batch by batch. Each dimension contains multiple batches, and we randomly select 150 samples of each batch for spot check. We provide feedback to the annotators and instruct them to revise their annotations. After each revision, we conduct spot checks again until the pass rate reaches $9 5 \%$ .

# Translation & Statistics

Translation The CHARACTERBENCH data we collect is initially crafted in Chinese. We use GPT-4o to translate it into English. To ensure faithfulness, we employ graduate students specializing in English translation to review the translations. After each spot check, we iteratively refine our translation prompt. Finally, 100 translated data are reviewed for each dimension, and the average pass rate reaches $9 6 \%$ (Table 2). The translation prompt is in the Appendix.

Table 3: Pearson correlation coefficient $( \% )$ of our CharacterJudge and automatic judges with human scoring in target-free and target-based (TG) settings. Bold is the best results, underline is the second best in the baselines. “w/o” refers to ablation study.   

<html><body><table><tr><td colspan="2">MC: Memory Consistency BCp: Behavior Consistency (Bot) MS:Morality Stability</td><td colspan="2">FA: Fact Accuracy MR:Morality Robustness</td><td colspan="2">BCk:Boundary Consistency BC:Behavior Consistency (Human) HL: Human-likeness</td><td colspan="2"></td><td colspan="2">ACb:Atribute Consistency (Bot) ES: Emotional Self-regulation</td><td colspan="2">ER:Empathetic Responsiveness</td><td colspan="3">ACh:Attribute Consistency (Human)</td></tr><tr><td rowspan="2">Models</td><td rowspan="2">AVG.</td><td rowspan="2">Memory</td><td colspan="2">Knowledge</td><td colspan="2"></td><td colspan="2">EG:Engagement Persona</td><td colspan="2">Emotion</td><td colspan="2">Morality</td><td colspan="2">Believability</td></tr><tr><td>MC</td><td>FA BCK</td><td>AC</td><td>ACh</td><td>BC</td><td>BC</td><td>ES</td><td>ER</td><td>MS</td><td>MR</td><td>HL</td><td>EG</td></tr><tr><td></td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td></tr><tr><td>GPT-3.5-turbo</td><td>37/40</td><td>53/45</td><td>72/71</td><td>24/36</td><td>38/46</td><td>42/45</td><td>39/48</td><td>20/34</td><td>39/43</td><td>48/42</td><td>37/44</td><td>37/41</td><td>9/14</td><td>17/8</td></tr><tr><td>GPT-4-1106</td><td>38/41</td><td>54/55</td><td>74/75</td><td>41/40</td><td>40/53</td><td>45/43</td><td>26/32</td><td>24/40</td><td>30/30</td><td>50/39</td><td>30/36</td><td>36/47</td><td>11/26</td><td>24/22</td></tr><tr><td>GPT-40</td><td>39/41</td><td>55/54</td><td>75/73</td><td>44/35</td><td>37/51</td><td>42/42</td><td>25/32</td><td>25/37</td><td>45/32</td><td>50/43</td><td>29/32</td><td>40/47</td><td>12/29</td><td>25/22</td></tr><tr><td>GLM-4</td><td>41/44</td><td>54/51</td><td>81/82</td><td>26/40</td><td>47/61</td><td>47/45</td><td>26/44</td><td>30/38</td><td>45/45</td><td>46/53</td><td>30/43</td><td>50/39</td><td>21/11</td><td>30/22</td></tr><tr><td>GPT-3.5-turbo-TG</td><td>43/44</td><td>54/51</td><td>72/71</td><td>43/43</td><td>53/55</td><td>50/49</td><td>42/49</td><td>33/36</td><td>57/58</td><td>56/56</td><td>37/44</td><td>37/41</td><td>9/14</td><td>17/8</td></tr><tr><td>GPT-4-1106-TG</td><td>45/46</td><td>63/63</td><td>79/77</td><td>56/52</td><td>52/59</td><td>47/44</td><td>37/32</td><td>40/33</td><td>55/55</td><td>56/57</td><td>30/36</td><td>36/47</td><td>11/26</td><td>24/22</td></tr><tr><td>GPT-4o-TG</td><td>45/46 48/47</td><td>59/59</td><td>75/74</td><td>56/53</td><td>49/60</td><td>49/43</td><td>35/31</td><td>40/38</td><td>54/55</td><td>56/60</td><td>29/32</td><td>40/47</td><td>12/29</td><td>25/22</td></tr><tr><td>GLM-4-TG</td><td></td><td>62/60</td><td>79/79</td><td>39/39</td><td>60/67</td><td>53/53</td><td>47/45</td><td>36/41</td><td>55/61</td><td>56/51</td><td>30/43</td><td>50/39</td><td>21/11</td><td>30/22</td></tr><tr><td>CharacterJudge</td><td>68/64</td><td>80/81</td><td>92/88</td><td>71/65</td><td>80/76</td><td>63/57</td><td>62/57</td><td>65/58</td><td>67/65</td><td>65/62</td><td>66/64</td><td>61/55</td><td>52/53</td><td>58/53</td></tr><tr><td>- w/o SC</td><td>64/60</td><td>80/78</td><td>89/87</td><td>70/62</td><td>80/70</td><td>59/56</td><td>58/54</td><td>55/55</td><td>65/55</td><td>60/57</td><td>54/58</td><td>59/48</td><td>46/51</td><td>59/51</td></tr><tr><td>- W/o TG</td><td>51/48</td><td>32/33</td><td>52/39</td><td>56/61</td><td>68/68</td><td>45/51</td><td>46/39</td><td>59/55</td><td>39/32</td><td>35/41</td><td>58/60</td><td>63/51</td><td>50/48</td><td>55/49</td></tr><tr><td>- W/o SC&TG</td><td>47/45 67/64</td><td>26/28</td><td>49/39</td><td>53/56</td><td>61/66</td><td>39/50</td><td>39/38</td><td>56/54</td><td>40/26</td><td>32/36</td><td>57/56</td><td>60/48</td><td>45/44</td><td>53/42</td></tr><tr><td>- In-Domain</td><td></td><td>81/82</td><td>91/87</td><td>67/59</td><td>76/66</td><td>60/57</td><td>60/57</td><td>62/54</td><td>66/63</td><td>69/71</td><td>66/65</td><td>59/55</td><td>53/55</td><td>63/57</td></tr><tr><td>- Out-of-Domain</td><td>68/65</td><td>79/79</td><td>92/88</td><td>74/71</td><td>84/84</td><td>65/58</td><td>64/56</td><td>68/62</td><td>68/67</td><td>63/56</td><td>65/64</td><td>63/54</td><td>51/51</td><td>53/48</td></tr></table></body></html>

Statistics As shown in Table 2, CHARACTERBENCH includes 22,859 samples from 3,956 characters. An average of 11.22 dialogue turns indicates that our data closely reflects real multi-turn interactions. The fact accuracy dimension only involves a subset of celebrities in our corpus, thus covering only 105 characters. We split the data into training and test sets to develop our CharacterJudge model for evaluating LLMs’ character customization. The test set is further divided into In-domain and Out-of-domain sets, each domain containing 125 samples from each dimension. More statistics (e.g., LLMs’ distributions) are in the Appendix.

# Development of CharacterJudge

To evaluate character customization cost-effectively on our benchmark, we develop CharacterJudge. Given scripts with profile $\mathcal { P }$ and context $\mathcal { C }$ fused target-oriented or target-free queries, response $y _ { n }$ , and target $\tau$ , we encapsulate them within a specific instruction $\boldsymbol { \mathcal { T } }$ tailored to each dimension and use human score $s$ as the supervision for optimization:

$$
\mathcal { L } = - \frac { 1 } { | D | } \sum _ { d = 1 } ^ { | D | } \left( P _ { \theta } ( S \mid \mathcal { Z } _ { d } ( \mathcal { P } , \mathcal { C } , y _ { n } , \mathcal { T } ) ) \right) ,
$$

where $P _ { \theta }$ is LLM’s parameters for optimization, $D$ is the set of dimensions, $\tau$ is omitted in dense dimensions. During decoding, we adopt the self-consistency method (Wang et al. 2023a) to generate multiple outcomes and use a majority vote to determine the final score. Empirically, we found that bilingual fine-tuning is less effective than training each language separately. Thus, we train models in both Chinese and English adopting the same training settings.

# Experiments Evaluation on CharacterJudge

We develop CharacterJudge upon Qwen2-7B-Chat (Yang et al. 2024) and use self-consistency to generate 10 outcomes. We employ automatic judges (GPT series and GLM4) for comparison, using both target-free and target-based (TG) prompts with CoT (Wei et al. 2022) (Appindex). Our evaluation metric is Pearson correlation with human scores.

Overall Performance The results are in Table 3. Our CharacterJudge outperforms all compared automatic judges by a large margin in bilingual evaluations. First, it achieves $42 \%$ and $36 \%$ improvements on AVG. over the suboptimal GLM-4-TG, showing its effectiveness in aligning with human scores. Second, its significant superiority on the Believability aspect indicates that subjective dimensions are more suitable to be evaluated using a specialized model. Third, SOTA performance in bilingual evaluations highlights our model’s robust versatility across multilingual scenarios.

Ablation Study We remove self-consistency and target $\tau$ from CharacterJudge to measure their contributions, named $w / o \ S C$ and w/o TG. In Table 3, both components contribute to the overall performance. SC generally contributes across all dimensions, while TG is specifically effective in sparse dimensions with the targets, supporting our motivation.

Generalizability of CharacterJudge The generalizability of our model across various sc enarios is evaluated using our $I n$ -domain and Out-of-domain test sets. As shown in Table 3, CharacterJudge consistently exhibits comparable performance in both domains, across AVG. and individual dimensions. This highlights our model’s strong generalizability to unobserved characters (out-of-domain test set), supporting our motivation to construct a diverse corpus.

# Evaluation for LLMs in Character Customization

We evaluate 18 LLMs: (1) Closed-source: 7 LLMs used in data collection and GPT-3.5-turbo. (2) Open-source: YiChat (AI et al. 2024), Mistral-7B-Chat (Jiang et al. 2023), GLM4-Chat (GLM et al. 2024), Llama3-Instruct (Meta 2024), Qwen1.5&2-Chat (Yang et al. 2024). They generate responses using our test set, scored by CharacterJudge. We normalize the scores of all dimensions to a 5-point scale.

Table 4: LLMs’ capabilities in character customization. The scores of all dimensions are normalized to a 5-point scale.   

<html><body><table><tr><td rowspan="2">Models</td><td rowspan="2">AVG.</td><td colspan="2">|Memory</td><td colspan="2">Knowledge BCK</td><td colspan="3">Persona</td><td colspan="2">Emotion</td><td colspan="2">Morality</td><td colspan="2">Believability</td></tr><tr><td>MC zh/en zh/en</td><td></td><td>FA zh/en zh/en</td><td>ACb zh/en</td><td>ACh zh/en</td><td>BC zh/en</td><td>BC zh/en</td><td>ES zh/en</td><td>ER zh/en</td><td>MS zh/en</td><td>MR zh/en</td><td>HL zh/en</td><td>EG zh/en</td></tr><tr><td colspan="10">Closed-sourced LLMs</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>axb CharacterYuyan</td><td>3.54/--</td><td>3.91/ --</td><td>2.34/ --</td><td>3.71/ --</td><td>4.18/ - -</td><td>3.93/--</td><td>3.34/--</td><td>3.17/ --</td><td>3.02/--</td><td>2.67/ --</td><td>4.66/ -- 4.76/--</td><td></td><td>3.13/--</td><td>3.27/</td></tr><tr><td>CharacterGLM</td><td></td><td>3.54/3.46|3.92/3.76</td><td>2.61/2.183.53/3.974.10/4.033.93/3.80 3.47/3.26 3.10/2.89</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>3.08/2.94 2.78/2.64|4.72/4.53 4.7/4.51</td><td></td><td>2.87/3.16 3.16/3.32</td><td></td></tr><tr><td>Baichuan-NPC</td><td>3.65/3.59</td><td>3.83/3.76</td><td>2.79/2.20 4.24/4.19|4.06/4.29 4.10/4.293.37/3.89</td><td></td><td></td><td></td><td></td><td>3.12/3.38</td><td>3.21/3.05 3.01/3.15</td><td></td><td></td><td></td><td></td><td>4.86/4.81 4.85/4.84|2.93/3.05 3.07/3.28</td></tr><tr><td>GPT-3.5-turbo</td><td></td><td></td><td>3.66/3.72|3.83/3.58|2.43/2.523.57/3.754.33/4.38 4.13/4.233.37/3.50</td><td></td><td></td><td></td><td></td><td>3.51/3.58</td><td>3.07/3.14 2.85/2.81</td><td></td><td></td><td>4.76/4.71 4.84/4.71</td><td></td><td>3.32/3.69 3.54/3.74</td></tr><tr><td>GPT-4-1106</td><td></td><td>3.69/3.743.97/3.88</td><td>2.85/2.71</td><td>3.73/4.03</td><td>4.42/4.52 4.14/4.10 3.35/3.59</td><td></td><td></td><td>3.37/3.43</td><td>3.07/3.09 2.96/2.95</td><td></td><td></td><td>4.81/4.74 4.76/4.72</td><td></td><td>3.21/3.34 3.32/3.50</td></tr><tr><td>GLM-4 Claude-3-opus</td><td>3.71/3.70</td><td>3.81/3.61 3.82/3.883.98/4.01</td><td>2.69/2.50 4.10/4.45</td><td></td><td>2.82/2.44 3.69/3.80|4.43/4.42 4.06/4.18 3.47/3.59 4.57/4.54 4.39/4.44 3.72/3.74</td><td></td><td></td><td>3.25/3.50 3.73/3.773.45/3.63</td><td></td><td></td><td></td><td>3.24/3.18 3.14/2.964.83/4.80 4.83/4.82</td><td>3.29/3.28 3.40/3.49</td><td>2.95/3.23 3.34/3.44</td></tr><tr><td colspan="10">Open-sourced LLMs</td><td>3.15/3.154.88/4.91 4.80/4.68</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="10">CharacterGLM-6B|3.21/3.19|3.31/3.22|2.26/2.01 3.22/3.60|3.19/3.28 3.44/3.49 3.05/3.01</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>3.01/2.90|2.80/2.84 2.55/2.51|4.58/4.51 4.64/4.78|2.70/2.64 2.95/2.98</td></tr><tr><td>Baichuan2-13B</td><td>3.25/3.19</td><td>3.32/3.47</td><td>2.57/2.48</td><td>3.55/3.68</td><td></td><td>3.20/3.39 3.61/3.48 3.12/3.063.00/3.07</td><td></td><td></td><td>2.85/2.79 2.75/2.61</td><td></td><td>4.81/4.70 4.84/4.61</td><td></td><td></td><td>2.21/1.98 2.49/2.14</td></tr><tr><td>Yi1.5-9B</td><td>3.43/3.47|3.52/3.71</td><td></td><td>2.49/2.24 3.29/3.41</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>3.83/4.363.65/3.963.51/3.443.30/3.152.93/3.042.94/2.834.83/4.744.84/4.692.50/2.672.99/2.91</td></tr><tr><td>Mistral-7B Qwen1.5-14B</td><td>3.50/3.55</td><td>3.84/3.88</td><td>2.15/2.26 3.55/3.83</td><td></td><td></td><td>3.96/4.02 4.06/4.18 3.35/3.47</td><td></td><td>3.40/3.31</td><td></td><td></td><td></td><td>2.89/2.99 2.80/2.844.88/4.74 4.93/4.67</td><td></td><td>2.59/2.77 3.08/3.14</td></tr><tr><td>GLM4-9B</td><td>3.57/3.49</td><td>3.58/3.58|3.80/3.49|2.65/2.21</td><td>4.31/3.972.85/2.35</td><td>3.65/3.82</td><td></td><td>4.31/4.28 4.14/4.09 3.40/3.41</td><td></td><td>3.08/3.07</td><td>2.96/3.05</td><td>2.91/2.85</td><td></td><td>4.76/4.72 4.62/4.53</td><td></td><td>2.60/2.60 2.79/2.78</td></tr><tr><td>Llama3-8B</td><td>3.60/3.65</td><td>3.98/3.72</td><td>2.35/2.35</td><td>3.42/3.59</td><td>4.12/4.41</td><td>3.94/4.10 3.29/3.28</td><td></td><td>3.47/3.52</td><td>2.96/2.99</td><td>2.99/2.87</td><td></td><td>4.77/4.69 4.72/4.65</td><td>3.04/3.32</td><td>3.36/3.49</td></tr><tr><td>Qwen2-7B</td><td>3.66/3.51</td><td>4.18/3.86|2.76/2.27</td><td></td><td>3.49/3.81 3.45/3.66</td><td></td><td>4.42/4.29 4.26/4.27 3.51/3.57</td><td></td><td>3.32/3.50|3.04/3.14 2.93/3.07</td><td></td><td></td><td>4.84/4.81 4.88/4.73</td><td></td><td></td><td>4.80/4.76|2.69/2.99 3.12/3.23</td></tr><tr><td>Llama3-70B</td><td>3.79/3.81</td><td>4.04/3.81</td><td>2.38/2.383.69/4.074.46/4.634.45/4.213.79/3.663.69/3.69</td><td></td><td></td><td>4.46/4.51 4.07/3.91 3.47/3.23</td><td></td><td>3.31/3.18</td><td>3.11/2.96 3.34/3.36</td><td>3.12/2.85 3.08/3.01</td><td>4.81/4.81</td><td>4.91/4.74 4.69/4.77</td><td></td><td>2.76/2.78 3.06/2.96 3.36/3.38 3.47/3.71</td></tr><tr><td>Qwen2-72B</td><td></td><td></td><td>3.80/3.684.03/3.943.00/2.593.85/3.954.53/4.394.22/3.963.53/3.333.35/3.353.25/3.063.14/2.894.92/4.71</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.85/4.74|3.30/3.40 3.41/3.51</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

Table 5: LLMs’ customization capabilities on 6 aspects.   

<html><body><table><tr><td>Models</td><td>zh/en</td><td>Memory Knowledge|Persona zh/en</td><td>zh/en</td><td>zh/en</td><td>zh/en</td><td>Emotion|Morality Believability zh/en</td></tr><tr><td colspan="7">Closed-sourced LLMs</td></tr><tr><td>MiniMax-abab5.5s3.76/3.66 CharacterYuyan CharacterGLM Baichuan-NPC GPT-3.5-turbo GPT-4-1106 GLM-4</td><td>3.91/ - - 3.92/3.76 3.83/3.76 3.83/3.58 3.97/3.88 3.81/3.61 3.98/4.01</td><td>3.02/-- 3.07/3.08 3.52/3.19 3.00/3.14 3.29/3.37 3.25/3.12 3.39/3.48</td><td></td><td>3.10/2.95|3.64/3.482.87/2.84|4.67/4.54 3.65/--2.84/--4.71/-- 3.65/3.472.93/2.794.72/4.52 3.66/3.65|3.11/3.034.85/4.77 3.83/3.922.96/2.974.80/4.71 3.82/3.913.01/3.014.79/4.73 3.80/3.923.19/3.074.83/4.81</td><td>4.10/4.123.30/3.394.83/4.79</td><td>3.09/3.23 3.20/-- 3.02/3.24 3.00/3.17 3.43/3.70 3.27/3.42 3.35/3.39</td></tr><tr><td colspan="7">Claude-3-opus 3.15/3.33 Open-sourcedLLMs</td></tr><tr><td>CharacterGLM-6B|3.31/3.22</td><td></td><td>2.74/2.80</td><td></td><td>3.17/3.17|2.67/2.67|4.61/4.49</td><td></td><td>2.82/2.81</td></tr><tr><td>Baichuan2-13B</td><td>3.32/3.47</td><td>3.06/3.08</td><td></td><td>3.23/3.25|2.80/2.70|4.82/4.65</td><td></td><td>2.35/2.06</td></tr><tr><td>Yi1.5-9B</td><td>3.52/3.71</td><td>2.89/2.83</td><td></td><td>3.57/3.732.94/2.934.84/4.72</td><td></td><td>2.74/2.79</td></tr><tr><td>Mistral-7B</td><td>3.84/3.88</td><td>2.85/3.05</td><td></td><td>3.69/3.74|2.84/2.914.90/4.71</td><td></td><td>2.84/2.96</td></tr><tr><td></td><td>4.31/3.97</td><td>3.25/3.08</td><td>3.73/3.712.93/2.954.69/4.63</td><td></td><td></td><td></td></tr><tr><td>Qwen1.5-14B</td><td>3.80/3.49</td><td>3.03/2.90</td><td></td><td></td><td></td><td>2.69/2.64</td></tr><tr><td>GLM4-9B</td><td></td><td></td><td></td><td></td><td>3.70/3.832.97/2.934.74/4.67</td><td>3.20/3.40</td></tr><tr><td>Llama3-8B</td><td>3.98/3.72</td><td>2.92/3.08</td><td></td><td></td><td>3.88/3.912.98/3.10|4.82/4.78</td><td>2.90/3.11</td></tr><tr><td>Qwen2-7B</td><td>4.18/3.86</td><td>3.11/2.96</td><td></td><td>3.83/3.703.12/2.914.90/4.73</td><td></td><td>2.91/2.87</td></tr><tr><td>Llama3-70B</td><td>4.04/3.81</td><td>3.03/3.22</td><td></td><td>4.09/4.053.21/3.194.75/4.79</td><td></td><td>3.41/3.54</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Qwen2-72B</td><td>4.03/3.94</td><td>3.42/3.27</td><td></td><td>3.91/3.753.19/2.984.89/4.73</td><td></td><td>3.36/3.45</td></tr></table></body></html>

Main Results In Table 4, firstly, large-scale open-source LLMs have performed comparably to well-recognized powerful closed-source LLMs in character customization, e.g., Qwen2-72B ranks behind Claude-3-opus on AVG. in Chinese evaluation, Llama3-70B ranks second in English evaluation. Secondly, general-purpose LLMs are qualified to substitute specialized role-playing LLMs by adopting promptbased character customization, as evidenced by Claude-3- opus outperforming 4 role-playing LLMs with a large margin. Thirdly, most bilingual LLMs perform comparably in bilingual evaluations, but they consistently struggle to generate responses with accurate facts (FA dimension).

Table 6: Results $( \% )$ of Spearman $( \rho )$ and Kendall $( \tau )$ correlation between benchmarks and humans for ranking LLMs.   

<html><body><table><tr><td rowspan="2">Benchmarks</td><td colspan="2">Fictional Characters</td><td colspan="2">Other Characters</td><td colspan="2">Overall</td></tr><tr><td>p</td><td>T</td><td>p</td><td>T</td><td>p</td><td>T</td></tr><tr><td>CharacterEval (2024)</td><td>19.2</td><td>10.9</td><td>-34.4</td><td>-30.9</td><td>21.4</td><td>14.3</td></tr><tr><td>SocialBench (2024a)</td><td>58.7</td><td>47.3</td><td>3.7</td><td>7.7</td><td>38.1</td><td>35.7</td></tr><tr><td>CHARACTERBENCH</td><td>82.5</td><td>74.1</td><td>52.5</td><td>43.2</td><td>73.1</td><td>61.8</td></tr></table></body></html>

Table 7: Results $( \% )$ of using CHARACTERBENCH to optimize CharacterGLM-6B’s character customization via DPO.   

<html><body><table><tr><td>Comparisons</td><td>Win</td><td>Tie</td><td>Lose</td><td>Improve. (↑)</td></tr><tr><td>6B-SFTvs.6B-Vanilla</td><td>38.4</td><td>22.9</td><td>38.7</td><td>-0.3</td></tr><tr><td>6B-DPOvs.6B-Vanilla</td><td>42.2</td><td>23.5</td><td>34.3</td><td>7.9</td></tr><tr><td>6B-DPO vs.6B-SFT</td><td>43.4</td><td>21.7</td><td>34.9</td><td>8.5</td></tr></table></body></html>

LLMs’ Capability on Six Aspects We average bilingual scores of LLMs in six aspects to present Table 5. The high morality scores of all LLMs show their robust capability to generate safe responses. Persona and memory evaluate LLMs’ capabilities to follow character profiles and model long dialogue context, there is room for improvement. Moreover, LLMs achieve low emotion and believability, showing that customized characters still struggle to engage in humanlike emotional exchanges naturally during conversations.

# Analysis for CHARACTERBENCH

Consistency with Human Evaluation To verify the consistency between our and existing benchmarks in evaluating LLMs’ character customization against human evaluation, we calculate the Spearman $( \rho )$ and Kendall $( \tau )$ rank correlations. We hire 10 annotators, each tasked with two characters to interact with 10 LLMs (closed-source LLMs and top 2 open-source LLMs) in Chinese for at least 20 dialogue turns. After completing the interactions, annotators score LLMs at an overall level on a 1 to 5 scale. The total score of LLMs is calculated as the human ranking. The characters cover fictional characters focused on existing benchmarks and characters of three other categories (Figure 3). We calculate rank correlations on different characters and Overall level, comparing LLMs rankings in these benchmarks to the human rankings. In Table 6, our CHARACTERBENCH significantly outperforms two representative benchmarks (generative CharacterEval and MCQ-based SocialBench (Chen et al. 2024a)), showing our benchmark’s effectiveness in assessing LLMs’ character customization in diverse scenarios.

Effectiveness for DPO Optimization To show our benchmark’s potential in optimizing LLMs’ character customization, we verify its effectiveness using DPO (Rafailov et al. 2023). We use CharacterGLM-6B (6B-Vanilla) as backbone. To identify the gains from our benchmark’s data for $\boldsymbol { 6 B }$ - Vanilla, we fine-tune it on the highest-scoring data of each dimension from our training set, obtaining 6B-SFT. Then, 6B-SFT is fed with scripts from our training set to generate multiple distinct responses. Our CharacterJudge scores these responses to create paired good-bad responses for DPO training, obtaining 6B-DPO. We conduct manual pairwise evaluation (Zhou et al. 2023a) for these 3 models with 10 annotators, each interacting with 2 characters for 20 dialogue turns. In each turn, annotators chose a winner from the responses of two models to continue the dialogue. If the comparison is the tie, a response is randomly selected. In Table 7, 6B-DPO significantly outperforms all baselines, showing our benchmark’s substantial potential to optimize LLMs’ character customization. More details are in Appendix.

# Conclusions

In this paper, we propose CHARACTERBENCH, the largest bilingual generative benchmark with 22,859 samples, to evaluate LLMs’ character customization on 11 dimensions of 6 aspects. We classify sparse and dense dimensions and ensure an effective and efficient evaluation of each dimension by constructing tailored queries to induce characters’ responses related to specific dimensions. Extensive experiments conducted with our developed CharacterJudge show its superiority over automatic judges and our benchmark’s potential to optimize LLMs’ character customization.