# Synergistic Multi-Agent Framework with Trajectory Learning for Knowledge-Intensive Tasks

Shengbin Yue1, Siyuan Wang2, Wei Chen3, Xuanjing Huang1, Zhongyu Wei1\* 1 Fudan University, Shanghai, China 2 University of Southern California, Los Angeles, USA 3 Huazhong University of Science and Technology, Wuhan, China sbyue23@m.fudan.edu.cn, sw 641@usc.edu, lemuria chen@hust.edu.cn, {xjhuang,zywei}@fudan.edu.cn

# Abstract

Recent advancements in Large Language Models (LLMs) have led to significant breakthroughs in various natural language processing tasks. However, generating factually consistent responses in knowledge-intensive scenarios remains a challenge due to issues such as hallucination, difficulty in acquiring long-tailed knowledge, and limited memory expansion. This paper introduces SMART, a novel multiagent framework that leverages external knowledge to enhance the interpretability and factual consistency of LLMgenerated responses. SMART comprises four specialized agents, each performing a specific sub-trajectory action to navigate complex knowledge-intensive tasks. We propose a multi-agent co-training paradigm, Long Short- Trajectory Learning, which ensures synergistic collaboration among agents while maintaining fine-grained execution by each agent. Extensive experiments on five knowledge-intensive tasks demonstrate SMART’s superior performance compared to widely adopted knowledge internalization and knowledge enhancement methods. Our framework can extend beyond knowledge-intensive tasks to more complex scenarios.

# Code — https://github.com/yueshengbin/SMART

Task Instruction Reconstructing Intent Accessing Knowledge   
…Lit recorded A Place in Californian rock band [1] Lit (album): Lit is the fourth   
the Sun in 1995, but what's Lit's most famous and studio album by the American   
their best known song? popular songs reocokrbdaenddatLitW.oTrlhdeCalabsusmAwudaiso in   
Different types of instructions Anaheim and The Pool . 物 [2] Lit (band): Lit is an American [1] Lacking rock band, formed in 1995 in   
Lit's best Supporting Facts [Irrelevant] Fullerton, California. They are   
known song [2] They are best bOewstnknWowrsnt fEonrethmeyi"r..h..i.t..song "My   
is My Own known for their hit   
Worst Enem, song "My Own [A3 PAlacPelaicne ihne SheunSiusnt (hLe tsealcbounmd :   
which was… Worst Enemy". [Relevant] studio album by the American...   
Cites: [2] [3] .. [4] Lit (band): .   
Response&Cites Identifying Facts Discriminating Relevance 中 智   
(a) Modular Optimization (b) End-to-end Optimization (c) Our Optimization

# Introduction

Researchers continue to pursue empowering intelligent systems to generate factually consistent responses in knowledge-intensive tasks (Singhal et al. 2022; Yue et al. $2 0 2 3 \mathrm { a }$ ; Wang et al. 2022a). Although Large Language Models (LLMs) internalize substantial world knowledge within their parameter memory, they still suffer from fabricating facts, due to their inherent drawbacks, e.g., hallucination (Ji et al. 2023), trouble in acquiring long-tailed knowledge (Kandpal et al. 2023) and struggle to expand their memory (De Cao, Aziz, and Titov 2021). These issues significantly underscore the necessity of incorporating external knowledge from non-parametric (i.e., retrieval-based) memories.

Current methods typically augment LLMs with retrieved knowledge to generate responses, which face three main challenges. (1) Complex query intent: the diverse nature (semantics and form) of instructions (e.g., multiple choice, multi-turn dialogue, and complex questions) leads to confusion regarding the query intent of knowledge. (2) Distractors in retrieved knowledge: knowledge retrieval inevitably introduces noises of varying granularity (document and sentence), with irrelevant documents and superfluous spans distracting the response and resulting in more severe hallucinations. (3) Insufficient knowledge utilization: LLMs tend to rely more on their implicit knowledge (parameter memory) rather than fully exploiting provided external facts (Huang et al. 2023). This fact-following disloyalty invalidates the knowledge incorporation process. Existing knowledge enhancement efforts (Shi et al. 2023; Ma et al. 2023; Asai et al. 2023) do not comprehensively address these multistage challenges. To this end, we propose a multi-agent framework, SMART, to integrate different actions to tackle all challenges within complex knowledge-intensive tasks, where each agent performs a specific action. This comprises an Intent Reconstructor to clarify knowledge intents, a Knowledge Retriever to access external knowledge based on intent, a Fact Locator to evaluate retrieved knowledge and identify factual spans, and a Response Generator that faithfully utilizes and cites available facts. This process can enhance the knowledge interpretability and response factuality.

However, a major concern remains in how to equip each agent with the necessary capability for corresponding actions while minimizing errors during agent streamline for better overall knowledge-intensive performance. This has been a longstanding challenge in improving multi-agent frameworks, especially as most (Yao et al. 2023; Hong et al. 2023) operate in a non-training manner. Specifically, On one hand, modular operations, where separate learned modules are pipelined with each dedicated to a specific agent, can streamline the processing. However, this can lead to error accumulation as mistakes in earlier modules propagate through the pipeline. On the other hand, encouraging LLM variants to imitate the entire trajectory, while mitigating the fragmentation and error propagation seen in modular systems, this long-term and global supervision cannot guarantee the precise fine-grained execution by each agent, as it fails to balance the attention each agent devotes to diverse input signals. Overall, maintaining synergy while ensuring the contribution of various stakeholders is essential.

To address this, we propose a multi-agent cooperative training method, namely Long Short- Trajectory Learning, which consists of two stages. In the first stage, short trajectory learning activates each specific agent in the framework. Next, long trajectory learning ensures synergy across multi-agents through trajectory skeleton learning. To establish a common supervisory signal for both phases while achieving different training objectives for each, we design special tokens (i.e., trajectory head-end tokens) to allow each agent to identify the attributed trajectories and learn interagent interaction signals during training. Specifically, the former phase learns the task output under the prompt of the trajectory-head token, so that the framework learns to distinguish between different agents and confirm the fine-grained information of interest. This independence enables more efficient training with the utilization of existing NLP datasets for pre-training and targeted optimization. The latter stage requires both predictions of task output and intermittent trajectory tokens throughout the process, i.e., establishing a navigation path from the previous agent to the next. Our learning approach enables multi-agent systems to collaboratively navigate a long and complex trajectory while concurrently upholding a nuanced representation of each agent. We conduct experiments on five knowledge-intensive tasks, including fact verification, multiple-choice reasoning, open-domain question answering and long-form generation. Results demonstrate that our framework significantly outperforms pre-trained and instruction-tuned LLMs with more parameters (knowledge internalization methods), and widely adopted knowledge enhancement methods. Further analysis reveals that our long-short trajectory learning enables flexible plug-in combinations of agents while maintaining performance, which is beyond the reach of current end-to-end training systems. Additionally, the framework achieves impressive performance using only over 40 $\%$ of long trajectory data, substantially reducing the cost and complexity of developing a high-performance multiagent framework. We envision our framework as a general paradigm that extends beyond knowledge-intensive tasks to more complex scenarios, enabling any multi-agent framework to internalize tailored trajectories.

# Method

Figure 2 provides an overview of our co-framework. We first introduce our multi-agent framework with four key agents performing distinct trajectories. Next, we explain the data construction method and detail the Long-Short Trajectory Learning for optimizing framework synergies.

# Multi-Agent Framework

To address multi-stage complex challenges in knowledgeintensive scenarios, we design a multi-agent framework to execute complex long trajectories. This framework incorporates four key agents: intent reconstructor $( \mathcal { A } _ { \mathrm { i } } )$ , knowledge retriever $( \mathcal { A } _ { \mathrm { r } } )$ , fact locator $( \mathcal { A } _ { \mathrm { l } } )$ , and response generator $( A _ { \mathrm { g } } )$ . Each agent serves a specific sub-trajectory, and the final response is obtained by synergizing these agents.

Intent Reconstructor. The $\mathcal { A } _ { \mathrm { i } }$ agent aims to clarify the knowledge query intent from user instructions. It possesses four primary capabilities: integrating contextual clues, identifying key query, unifying task formulation, and intent decomposition, to handle diverse instructions. For example, in multi-turn dialogues, $\mathcal { A } _ { \mathrm { i } }$ models long-term history for intent. For noisy instructions, it filters out irrelevant information to identify key queries. For various task formats such as multichoice QA, $\mathcal { A } _ { \mathrm { i } }$ formulate all inputs as a query format for subsequent processing. When handling multi-hop queries like “Who was born earlier, person A or person B?”, $\mathcal { A } _ { \mathrm { i } }$ breaks them down into multiple sub-intents, i.e., each person’s birth date. By flexibly applying these capabilities, this agent obtains clear query intent to access external knowledge.

Knowledge Retriever. The $A _ { \mathrm { r } }$ agent accesses external knowledge bases (e.g., Wikipedia) and obtains relevant knowledge candidates based on reconstructed intents. Specifically, it is driven by an off-the-shelf retrieval model (Izacard et al. 2021) and acquires top- $k$ knowledge document candidates from the knowledge base for each knowledge intent. Details of our knowledge retriever setup and the corpus are described in Appendix Sec. B.3.

Fact Locator. The $\mathcal { A } _ { \mathrm { l } }$ agent aims to locate factual evidence from knowledge candidate sets via document- and sentence-level assessments. Specifically, it assesses the relevance of each knowledge document to the given instruction to determine relevant ones. It then identifies the factual spans from relevant documents as evidence. The fact locator serves two primary purposes: 1) It enables the agent to check its relevance judgments to minimize the distraction of extraneous spans of the document, and allows the response phase to focus more on fact spans. 2) By explicitly learning to locate facts, it enhances the interpretability of the knowledge application process and bolsters user credibility.

Intent Reconstructor Knowledge Retriever Fact Locator Response Generator Knowledge-intensive tasks Instruction Step 1: Judge Relevance MultiFplaec-tcVheoriicfeicRateiaosnoning MultipAlem-bcihgouico..eu s…RQea…useosntionng Intent 1 Intent i Instruction [1] [2] [3] [i× Facts: Reconstruct [Relevant] [Irrelevant] [TCaistek]:R[e1s]p[okn×sei] Ambiguous Question External knowledge Step 2: Locate Facts Multi-round Dialogue NFioltisere Decompose Context ForCmlaurliaftyion [k1][:i]: ] [S12u]p:pLoartciknigngFacts NoTaFsakctRs:esponse 園圓 .······· Retrieval( Intent 1 Intent 2 ….. Intent i ) [1] [2] [3] [k] Paragraph [k×i]F: act Long Trajectory Task Instruction Intent Reconstructor Knowledge Retriever Fact Locator Response Generator   
Trajectory head <Instruction>: <Reconstructor >: <retrieval> <Locator>: <Generator>: 肉 fiWrshtoowutasofbJoren Search (Joe Colquhoun 's birthdate) [(1]9J2o6e– 1C9o8l7q)uhwoausnaJBoreitCisohlq…uh…oun [wRaesleavBarniti]s:h[1c]oJmoiecsCaorltqisuthboeusnt (k1n9o2w6n–1f9or87) Joe Colquhoun   
日 CaorllqtuohnoLuoneawnedr? Search (Carlton Loewer 's birthdate) [h2a]vfirnogmno1 i9n5t4erteost1i9n60f,oodtebsaplilte…… [hIirsrweloervkaonnt]:" [C2h]arLlaecy'ksinWgaSr"up…po…rting Facts. [Cite]: [1] [4] </eoi> </eor> </retrieval> </eol> </eog> Trajectory end   
Explicit Cue   
Implicit Cue Short Trajectory 1 Short Trajectory 2 Short Trajectory 3 Short Trajectory 4

Response Generator. The $A _ { \mathrm { g } }$ agent finally generates responses to user instructions. When facts are provided, it adjusts its knowledge preferences to adhere to them, and ultimately outputs citations to validate loyalty further. In the absence of such information, the response generator relies on its knowledge memory to formulate responses.

Inference Overview. The systematic procedure is delineated in the following steps: $\mathcal { A } _ { \mathrm { i } }$ first mines the explicit intent $\bar { q } = \{ q _ { 1 } , q _ { 2 } , . . . , q _ { m } \}$ from the instruction $x$ . Next, $A _ { \mathrm { r } }$ retrieves top- $\mathbf { \nabla } \cdot \mathbf { k }$ knowledge documents $\bar { d } = \{ d _ { 1 } , d _ { 2 } , . . . , d _ { k \times m } \}$ using each intent $q _ { m }$ . Then, $\mathcal { A } _ { \mathrm { l } }$ determines each relevant knowledge passage and further locates the fact span $f \subset$ $d _ { k \times m }$ . Finally, $A _ { \mathrm { g } }$ utilizes the previous execution trajectory to generate response $y$ and citations when facts exist, otherwise $A _ { \mathrm { g } }$ utilizes only $x$ . In the $t$ -th step, the Agent $\mathcal { A }$ generates a response $\boldsymbol { r } _ { t }$ and a head token $h _ { t + 1 }$ of the next trajectory based on the current state of the system:

$$
r _ { t } , h _ { t + 1 } = \mathcal { A } \left( x , \tau _ { t - 1 } \right) ,
$$

where $\tau _ { t - 1 } = \{ h _ { 1 } , r _ { 1 } , e _ { 1 } , . . . , h _ { t - 1 } , r _ { t - 1 } , e _ { t - 1 } \}$ denotes the previous execution trajectory. $e$ denotes the trajectory end token. In addition, $\mathcal { A } _ { \mathrm { i } } , \mathcal { A } _ { \mathrm { l } }$ and $A _ { \mathrm { g } }$ are built upon same LLMs to fulfill their roles. The pseudo-code for inference is referenced in Appendix.

# Trajectory Dataset Construction

To implement long-short trajectory learning to optimize our multi-agent framework, we construct the Trajectory dataset. We collect samples from over 12 knowledge-intensive tasks to ensure coverage of various instruction semantics and formats, such as fact verification (Thorne et al. 2018), dialogue (Dinan et al. 2018; Anantha et al. 2021), open-domain

Q&A (Kwiatkowski et al. 2019; Stelmakh et al. 2022; Geva et al. 2021), and commonsense reasoning (Mihaylov et al. 2018; Huang et al. 2019). Detailed statistics are in Table 5 of Appendix. Our dataset contains two components: the long-trajectory subset and the short-trajectory subset. The data construction follows two distinct principles:

Long-trajectory subset. The long-trajectory subset aims to precisely mimic our multi-agent framework inferencetime process, which emphasizes the synergy and logical interaction between agents. Existing work (Asai et al. 2023) has demonstrated the effectiveness of the powerful LLM (e.g., GPT3.5, GPT4 (Achiam et al. 2023)) as a critic model. Given an input-output pair $( x , y )$ , we create supervised data under the guide of the retrieval $( { \mathcal { R } } )$ and critic model $( \mathcal { C } )$ . We enable $\mathcal { C }$ to unleash the knowledge intents $\bar { q }$ in $x$ according to the instruction type. Then, $\mathcal { R }$ retrieves the top- $k$ knowledge documents based on every $\bar { q }$ . For each document, $\mathcal { C }$ further evaluates whether the passage is relevant based on $( x , y )$ . If a passage is relevant, $\mathcal { C }$ further locates and extracts the fact spans. Finally, we combine the data and insert the trajectory header and end token (e.g., <Reconstructor>, $\left[ < / \mathrm { e o r } > \right] )$ into each trajectory. Trajectory tokens are identifiers that serve as the skeleton of the multi-agent framework. In total, we construct 142,507 elaborated instances.

Short-trajectory subset. Unlike the long-trajectory subset, the short-trajectory subset facilitates the training of individual capabilities for each intelligent agent. This isolation allows us to acquire data directly from a huge amount of existing knowledge-intensive tasks through some simple processing. Thus, we sample from the established NLP and SFT datasets, appending the requisite trajectory header and

![](images/d1d559e9592a1c3832ccef46f4c8fbbf09a6da0d5ddc410178cdbc46e83f83a6.jpg)  
Figure 3: Overview of Long-Short Trajectory Learning. It consists of two stages, for short trajectory learning, under a given trajectory head, requires insight into the various explicit and implicit signals in each particular task. For long-trajectory learning, LLM executes the entire process by predicting different trajectory tokens, ensuring the synergism of different short-trajectories.   
Table 1: Four types of trajectory tokens. $x , \bar { q } , \bar { d } , \gamma ,$ $\bar { f }$ and $\bar { y }$ indicate instruction, intent, knowledge document, relevance tag, fact evidence and response, respectively.

Type Head Trajectory TEonkdens Input Output $\mathcal { A } _ { \mathrm { i } }$ <Reconstructor> </eor> x q¯ $A _ { \mathrm { r } }$ <retrieval> </retrieval> q¯ d¯ $\mathcal { A } _ { \mathrm { l } }$ <Locator> </eol> x, d¯ γ,f¯ g <Generator> </eog> x,d¯ / x y end token. Note that the existing NLP datasets do not fulfill our requirements for intent reconstructing, we employ the methodology utilized in the long-trajectory subset collection. Table 1 exhibits the inputs and outputs of each short trajectory under the responsibility of each agent. In addition, the response generator contains two types of inputs to help adapt its knowledge preferences. We construct a total of 359,791 instances.

To summarize. Two keys are in the construction: the Long-trajectory subset is crafted to emphasize synergy, and the Short-trajectory subset can be easily accessed in large quantities to emphasize uniqueness. Refer to Appendix Sec.A for the detail of data construction.

# Long-Short Trajectory Learning

Effectively fine-tuning a trajectory system consisting of multi-agents is a complex task: on the one hand, each agent has its specific trajectory signals of attention. On the other hand, the transformation between different trajectories requires the collaboration of the agents. In addition, the cost of trajectory data construction for a multi-agent framework greatly hinders the development of such systems. To this end, we propose Long-Short Trajectory Learning for our multi-agent framework, which consists of two stages, Short Trajectory and Long Trajectory Learning. As shown in Figure 3, Under the guidance of the trajectory head-end token pairs, the intuition is that Short Trajectory Learning first delineates the responsibilities of each agent to develop their unique capabilities, and then Long Trajectory Learning learns the interactions between them. This can be understood as initially activating each agent that masters short trajectories within a broader trajectory framework, and then exploring the interconnections between those agents to navigate the full long trajectory.

Short Trajectory Learning. Short Trajectory Learning is the training of individual capabilities for a single agent. In the context of a long trajectory, it is important to note that short trajectories spanning multiple steps do not necessarily exhibit a strong dependence on preceding short trajectories. To illustrate this point, consider the case of a fact locator, which primarily relies on the original user query and the retrieved results, rather than having a strict dependence on the queries generated in Intent Reconstructor. Similarly, the Response Generator necessitates only the question itself or a combination of the question and the located facts. As shown in Figure 3, the short trajectory learning first activates each short agent in the framework to focus on the fine-grained signals. Given the short-trajectory subset $\mathcal { D } _ { \mathrm { s h o r t } } \stackrel {  } { = } \{ \mathcal { D } _ { \mathrm { i n t e n t } } , \mathcal { D } _ { \mathrm { l o c a t o r } } , \mathcal { D } _ { \mathrm { g e n e r a t o r } } \}$ , we initialize a pre-trained LLM and train it on $\mathcal { D } _ { \mathrm { s h o r t } }$ . For each example $\mathsf { \bar { f } } \left( x _ { i } ; h _ { i } \right) , \left( y _ { i } ; e _ { i } \right) \} \subset \mathcal { D } _ { s h o r t }$ , we use a standard conditional language modeling objective, maximizing likelihood:

$$
\mathcal { L } \left( \mathcal { D } _ { s h o r t } \right) = \sum _ { i } \log P _ { L M } \left( y _ { i } ; e _ { i } \mid x _ { i } ; h _ { i } \right) ,
$$

Given the inputs and trajectory header, the agent learns to predict the outputs, i.e., delineate different belonging trajectories for the agent to make them understand the fine-grained representations of the corresponding tasks. This phase utilizes easily accessible and extensive data to build the basic capabilities of the trajectory, reducing the cost of such a framework while maintaining the creativity and versatility of the agent.

Table 2: Comparison results against knowledge internalization and knowledge enhancement methods. $\star$ denotes the method we reproduce based on the same base. $\star$ denotes re-implemented methods based on the same initial model. The bold numbers represent the best results and the underlined numbers represent the second.   

<html><body><table><tr><td rowspan="2">Task Metric</td><td rowspan="2">Health Acc</td><td rowspan="2">ARC-C Acc</td><td rowspan="2">PopQA Acc</td><td rowspan="2">Squad1 Acc</td><td colspan="3">ASQA</td></tr><tr><td>Str_EM</td><td>R-L</td><td>Mauve</td></tr><tr><td colspan="8">Knowledge internalization methods</td></tr><tr><td>Alpaca2 7B*</td><td>44.78</td><td>36.43</td><td>25.58</td><td>11.50</td><td>14.42</td><td>28.72</td><td>51.24</td></tr><tr><td>Mistral-Instruct 7B</td><td>65.45</td><td>57.84</td><td>22.37</td><td>14.97</td><td>20.80</td><td>32.20</td><td>33.47</td></tr><tr><td>Llama-2-Chat 7B</td><td>47.95</td><td>47.95</td><td>25.44</td><td>14.13</td><td>16.79</td><td>32.35</td><td>24.21</td></tr><tr><td>Vicuna-v1.5 13B</td><td>63.01</td><td>57.59</td><td>17.94</td><td>15.25</td><td>31.95</td><td>22.99</td><td>68.41</td></tr><tr><td>Llama-2-Chat 13B</td><td>62.20</td><td>48.72</td><td>21.22</td><td>15.97</td><td>19.97</td><td>30.37</td><td>40.23</td></tr><tr><td>ChatGPT</td><td>76.08</td><td>77.3</td><td>29.30</td><td>22.90</td><td>39.94</td><td>35.73</td><td>44.63</td></tr><tr><td colspan="8">Knowledge enhancementmethods</td></tr><tr><td>Alpaca2 7B*</td><td>26.44</td><td>35.15</td><td>33.38</td><td>21.41</td><td>23.59</td><td>27.21</td><td>50.09</td></tr><tr><td>REPLUG 7B*</td><td>41.72</td><td>47.26</td><td>37.24</td><td>24.23</td><td>26.54</td><td>33.25</td><td>54.03</td></tr><tr><td>VANILLA 7B*</td><td>29.52</td><td>42.74</td><td>37.52</td><td>25.92</td><td>32.25</td><td>34.93</td><td>39.54</td></tr><tr><td>RAIT 7B*</td><td>52.98</td><td>62.10</td><td>38.02</td><td>23.86</td><td>25.68</td><td>15.99</td><td>12.35</td></tr><tr><td>INTERACT 7B*</td><td>65.45</td><td>48.12</td><td>41.31</td><td>31.52</td><td>34.54</td><td>35.51</td><td>43.45</td></tr><tr><td>SelfRag 7B</td><td>68.99</td><td>65.52</td><td>40.67</td><td>22.39</td><td>28.68</td><td>34.11</td><td>83.00</td></tr><tr><td>MMAgent 3*7B*</td><td>70.827</td><td>63.99</td><td>36.88</td><td>23.79</td><td>33.04</td><td>36.49</td><td>88.98</td></tr><tr><td>SMART (OURS)</td><td>73.18</td><td>65.58</td><td>42.60</td><td>27.80</td><td>41.16</td><td>40.66</td><td>91.47</td></tr></table></body></html>

Long Trajectory Learning. After the above stage, the framework is equipped with four independent agents. Long Trajectory Learning further grooms the LLM to establish logical associations between agents in an end-to-end manner. We train based on the previous stage on the longtrajectory subset $\mathcal { D } _ { l o n g }$ . Specifically, given instruction $x$ , long trajectory learning forces the LLM to learn the long trajectory process:

$$
\begin{array} { r l r } & { } & { \mathcal { L } \left( \mathcal { D } _ { L o n g } \right) = \displaystyle \sum _ { i } \log P _ { L M } \left( \tau _ { i } ^ { R } ; \tau _ { i } ^ { I } ; \tau _ { i } ^ { G } \mid x _ { i } \right) , } \\ & { } & { \quad \tau _ { i } ^ { T } = \left[ h _ { i } ^ { T } ; y _ { i } ^ { T } ; e _ { i } ^ { T } \right] , T \subset \{ R , I , G \} . } \end{array}
$$

where $R , I$ and $G$ denote the Intent Reconstructor, Fact Locator and Response Generator, respectively. Unlike short trajectory learning (Eq. 2), the framework learns both to predict the target output for each short trajectory as well as from the previous trajectory end $e ^ { T }$ to the next trajectory head $h ^ { T + 1 }$ . In essence, the trajectory token serves as a skeleton in the learning process, guiding the agent not only to grasp a finegrained representation of the intra-trajectory but also intertrajectory interactions.

# Experiment Setting

# Setup

Task and Dataset. We evaluate our framework in a range of knowledge-intensive downstream tasks. Including (1) Fact verification: PubHealth (Akhtar, Cocarascu, and Simperl 2022) is a fact verification dataset about public health; (2) Multiple-choice reasoning: ARC-Challenge (Clark et al. 2018) is a multiple-choice questions dataset about science exam. (3) Open-domain question answering: contains two short-form QA datasets, PopQA (Mallen et al. 2022), and SQuAD 1.1 (Rajpurkar et al. 2016). (4) Ambiguous question answering: ASQA (Gao et al. 2023) is ambiguous factoid question of the long form response. Details of evaluation data, including size, and evaluation metrics are available in Appendix Sec. B.1.

Baselines. We compare our framework with a wide range of baseline methods in two categories. (1) Knowledge internalization methods (General-purpose LLMs): ChatGPT (gpt-3.5-turbo-0125) (Zheng et al. 2023) (Ouyang et al. 2022), Mistral-Instruct-v0.2-7B (Jiang et al. 2023), Llama2-Chat-7B/13B (Touvron et al. 2023), Vicuna-v1.5-13B (Zheng et al. 2023) and Alpaca2-7B 1 (Zheng et al. 2023). (2) Knowledge enhancement methods: REPLUG-7 (Shi et al. 2023), VANILLA-7B (Gao et al. 2023), INTERACT7B (Gao et al. 2023), RAIT-7B (Lin et al. 2023), SelfRAG7B (Asai et al. 2023), MMAgent- $3 ^ { * } 7 \mathrm { B }$ (modular approach). More details are in Appendix Sec. B.2.

# Implementation Details

Due to page limitations, details of our training and evaluation are in Appendix Sec. B.3.

# Experiment Result

# Main Result

Comparison against knowledge internalization methods. As shown in Table 2, our framework shows a significant performance advantage over equivalently sized finetuned LLMs across all tasks. In comparison to larger LLMs (Vicuna-v1.5-13B and Llama-2-Chat-13B), which possess greater internalized knowledge, our SMART framework also exhibits superior performance in all metrics. Furthermore, our framework surpasses ChatGPT in all evaluated metrics for PopQA (long-tail knowledge evaluation), Squad1, and ASQA. Experimental results indicate that our method more effectively addresses long-tail knowledge, delivering more accurate and fluent responses compared to knowledge internalization methods, which necessitate extensive fine-tuning and training on large volumes of private data.

Table 3: Training Ablation and inference ablation for the contribution of different agents. L and S denote longtrajectory and short-trajectory learning, respectively. w/o $\mathcal { A } _ { \mathrm { f } }$ , w/o $\mathcal { A } _ { \mathrm { i } }$ , and $\mathrm { w } / \mathrm { o }$ All denote no fact Locator, no intent reconstructor, and only response generator.   

<html><body><table><tr><td>Health (Acc)</td><td>ARC-C (Acc)</td><td>Pop (Acc)</td><td>AS (Em)</td></tr><tr><td colspan="4">Training ablation</td></tr><tr><td>SMART (L) 72.15</td><td>60.22</td><td>37.27</td><td>36.10</td></tr><tr><td>w7oAf 70.13</td><td>58.95</td><td>34.31</td><td>34.77</td></tr><tr><td>w/o Ai</td><td>69.82 54.94</td><td>35.17</td><td>34.41</td></tr><tr><td>w/o All</td><td>57.95 56.99</td><td>21.15</td><td>20.05</td></tr><tr><td colspan="4">Inference ablation</td></tr><tr><td>SMART (L+S)</td><td>73.18 65.58</td><td>42.60</td><td>41.16</td></tr><tr><td>w7oAf</td><td>71.63 62.45</td><td>37.45</td><td>36.10</td></tr><tr><td>w/o Ai</td><td>71.22 60.11</td><td>39.88</td><td>35.30</td></tr><tr><td>w/o All</td><td>69.32 58.81</td><td>16.79</td><td>31.32</td></tr></table></body></html>

Comparison against knowledge enhancement methods. Considering fairness and persuasiveness, we compared knowledge enhancement methods based on the same size as ours. As shown in Table 2, our SMART performs better on most tasks compared to other knowledge enhancement methods. Compared to the SOTA retrieval method, SelfRag (Asai et al. 2023), our model shows great superiority in both accuracy and fluency. Our method exceeds MMAgent (four independent agents coupled together) in all metrics. This demonstrates that our learning paradigm improves multiagent collaboration, resulting in more accurate responses. Note that INTERACT (Gao et al. 2023) is better than us on Squad1, the reason is that INTERACT allows the response model to do more reasoning steps, which is beneficial for hitting answers in short-format generation tasks. RAIT (Lin et al. 2023) is trained with SMART same data and initialized model without fact location and intent reconstruction, lagging behind us. Overall, our SMART delivers excellent performance in a diverse range of knowledge-intensive tasks. This result indicates SMART gains are not solely from the multi-agent framework and demonstrate the effectiveness of the long-short trajectory learning.

# Ablation Studies

Training ablation of different agents. Training ablation aims to verify the superiority of the entire multi-agent combination setup. To save the experiment cost, we implement long-trajectory learning using 60,000 samples from the long-trajectory subset to evaluate the performance of the co-framework under different agent absence scenarios.

Table 4: Ablation studies of long-trajectory (Long) and short-trajectory (Short) learning.   

<html><body><table><tr><td rowspan="2">Methods</td><td>Health</td><td>PopQA</td><td colspan="2">ASQA</td></tr><tr><td>(Acc)</td><td>(Acc)</td><td>(Em)</td><td>(R-L)</td></tr><tr><td>Vanilla LLM</td><td>9.80</td><td>22.69</td><td>14.11</td><td>6.45</td></tr><tr><td>+ Short</td><td>62.00</td><td>32.23</td><td>23.95</td><td>19.91</td></tr><tr><td>+ Long</td><td>72.9</td><td>37.66</td><td>39.86</td><td>39.51</td></tr><tr><td>+ Short & Long</td><td>73.18</td><td>42.60</td><td>41.16</td><td>40.66</td></tr></table></body></html>

As the top part of Table 3, the absence of the fact Locator and the intent reconstructor significantly degrades the framework’s performance. The intent reconstructor provides substantial benefits for multiple-choice reasoning (ARC-C) and ambiguous questions (ASQA), while the fact Locator is crucial for long-tail knowledge Q&A (PopQA). The experiment proved the effectiveness of different agents in our SMART, especially the fact Locator and the intent reconstructor.

Inference ablation of different agents. We use the full version of SMART with short long-trajectory learning to ignore the trajectories of different agents during the inference phase. As the bottom part of Table 3, each agent plays an important role in the collaboration framework. The effect degradation of the fact-checking task (Health) was not severe, which may be related to the large amount of knowledge injected during the short trajectory learning. In addition, note that if the inference process is missing a particular agent, most multi-agent frameworks that use end-to-end training become terrible, due to the loss of signals from the missing agent. Benefiting from our Short-Trajectory Learning through the trajectory tokens, our SMART does not collapse in performance when an agent is missing, demonstrating flexibility while maintaining performance.

Effects of Long-Short Trajectory Learning. Long-Short Trajectory Learning optimising a Multi-agent framework through two-stage learning. we demonstrate its effectiveness progressively by training it on vanilla models, Llama2-7B-hf (Touvron et al. 2023). As shown in Table 4, shorttrajectory learning and long-trajectory learning enable huge performance improvements in the framework for all tasks. Short-trajectory learning enhances the system by optimizing each agent’s base capability, though its impact is not as substantial as that of long-trajectory learning. Longtrajectory learning, by optimizing agent synergy, underscores the importance of collaborative optimization in a multi-agent framework, despite the challenges posed by complex data construction. Overall, the combined approach of long-short trajectory learning yields the best performance, highlighting the significance of simultaneous collaboration and individual uniqueness.

Effects of training data size. To examine the impact of long-trajectory training data on long-short trajectory learning, we randomly selected subsets of 8k, $2 0 \mathrm { k }$ , 60k, and $1 2 1 \mathrm { k }$ instances from the initial 140k training instances and finetuned four SMART variants on these subsets. Subsequently, we compared the model performance on ARC-C, PopQA, and ASQA with our SelfRAG and MMagent models. As shown in Figure 4, an increase in data size generally leads to improved performance across all datasets. Notably, by utilizing $6 0 \mathrm { k }$ data instances, SMART outperformed SelfRAG, which employs $1 2 0 \mathrm { k }$ samples. This demonstrates the significant advantage of our learning approach in markedly enhancing the performance of multi-agent framework.

![](images/0a1942ba602c511d3a036c1b837b4beb5b76d40eeb19e7d46a3038c9dc016039.jpg)  
Figure 4: Effects of long-trajectory training data size (K) on three tasks, ARC-C, PopQA and ASQA.

# Related Work

Trajectory Learning. Trajectory learning aims to allow agent systems to complete a complex task or scenario through a series of interconnected phases, which requires a profound understanding of both global and local dimensions. Some methods (Chen et al. 2023; Song et al. 2023; Kong et al. 2023; Asai et al. 2023; Sun et al. 2022; Mou, Wei, and Huang 2024) enable agent learning trajectory via providing crafted prompt or tuning, which may not consistently yield high performance in every phase. Moreover, independently modules (Liu et al. 2023; Shen et al. 2024; Ma et al. 2023; Xu, Shi, and Choi 2023; Wang et al. 2023) can be combined with agent to implement trajectory inference, while this integration confers robust isolated capabilities, the gap between modules might lead to cumulative errors throughout the trajectory process. In this paper, we introduce long-short trajectory learning, which equips multiagent systems with the ability to not only grasp the logic connecting steps but also to refine each step. Our approach is scalable to increasingly complex scenarios.

Knowledge Enhancement Methods. Ensuring factconsistent responses is a core goal of intelligent systems research (Wang et al. 2022b; Tu et al. 2024b,a, 2023; Yue et al. 2024, 2023b; Gao et al. 2024). LLMs parameterize knowledge by training on gargantuan textual corpora. However, LLMs suffer from hallucination (Ji et al. 2023), trouble in acquiring long-tailed fact (Kandpal et al. 2023) and struggle to expand their parametric knowledge. For knowledge-intensive scenarios, existing methods (Izacard et al. 2023; Sun et al. 2020) usually assist LLMs by integrating non-parametric knowledge. Recent advances incorporated retrievers (Asai et al. 2023; Shi et al. 2023; Lin et al. 2023) to augment LLMs. The efficacy of nonparametric knowledge collaboration in improving task performance significantly relies on the relevance of the acquired knowledge and the level of knowledge utilization by the LLM itself. However, existing work has not comprehensively confronted these challenges Some works (Xu, Shi, and Choi 2023; Ma et al. 2023) simply select relevant knowledge and demonstrate better intentions by combining separate modules. Self-RAG (Asai et al. 2023) integrates specialized feedback tokens into the language model to assess the necessity for retrieval and to verify the relevance, support, or completeness of the output. Unlike existing approaches, we introduce a novel multi-agent framework that addresses these challenges with trajectory learning.

# Conclusions

In this paper, we introduce SMART, a novel multi-agent framework that addresses the challenges of generating factually consistent responses in knowledge-intensive tasks. By leveraging external knowledge and employing specialized agents, SMART enhances the interpretability and factual consistency of LLMs generated responses. Our proposed Long- and Short-Trajectory Learning paradigm ensures synergistic collaboration among agents while maintaining finegrained execution, enabling the framework to navigate complex knowledge-intensive tasks effectively. Empirical results on five diverse tasks demonstrate SMART’s superior performance compared to SOTA pre-trained and instruction-tuned LLMs, as well as widely adopted methods. SMART highlights the importance of integrating external knowledge and employing multi-agent systems to tackle the limitations of LLMs in knowledge-intensive scenarios.

Future work. One is that our framework currently executes sequentially without iterative optimization, which may lead to insufficient knowledge retrieval for multi-hop problems. However, this can be addressed by adding loop arrows between the Fact Locator and Intent Reconstructor agents. Another is that our retriever is not trained in the whole process, although it can be incorporated into the training process using existing techniques. We envision our framework as a general paradigm that extends beyond knowledgeintensive tasks to more complex scenarios, enabling any multi-agent framework to internalize tailored trajectories.