# EmoReg: Directional Latent Vector Modeling for Emotional Intensity Regularization in Diffusion-based Voice Conversion

Ashishkumar Prabhakar Gudmalwar1, Ishan Darshan Biyani1, Nirmesh J. Shah1, Pankaj Wasnik1, Rajiv Ratn Shah2

1Media Analysis Group, Sony Research India, Bangalore 2Indraprastha Institute of Information Technology (IIIT), Delhi, India {ashish.gudmalwar1,ishan.biyani,nirmesh.shah,pankaj.wasnik}@sony.com, rajivratn@iiitd.ac.in

# Abstract

The Emotional Voice Conversion (EVC) aims to convert the discrete emotional state from the source emotion to the target for a given speech utterance while preserving linguistic content. In this paper, we propose regularizing emotion intensity in the diffusion-based EVC framework to generate precise speech of the target emotion. Traditional approaches control the intensity of an emotional state in the utterance via emotion class probabilities or intensity labels that often lead to inept style manipulations and degradations in quality. On the contrary, we aim to regulate emotion intensity using selfsupervised learning-based feature representations and unsupervised directional latent vector modeling (DVM) in the emotional embedding space within a diffusion-based framework. These emotion embeddings can be modified based on the given target emotion intensity and the corresponding direction vector. Furthermore, the updated embeddings can be fused in the reverse diffusion process to generate the speech with the desired emotion and intensity. In summary, this paper aims to achieve high-quality emotional intensity regularization in the diffusion-based EVC framework, which is the first of its kind work. The effectiveness of the proposed method has been shown across state-of-the-art (SOTA) baselines in terms of subjective and objective evaluations for the English and Hindi languages.

Demo Page — https://nirmesh-sony.github.io/EmoReg/

# Introduction

Despite significant progress in the field of Generative AI, speech synthesis models still encounter several challenges when it comes to the AI-based dubbing of entertainment content such as movies and serials (Brannon et al. 2023; Wu et al. 2023; Hu et al. 2021; Mhaskar et al. 2024; Sahipjohn et al. 2020). AI-based dubbing involves replicating input speech emotion and controlling its intensity depending on the context and emotion of the scene (Brannon et al. 2023; Zhou et al. 2022c,a; Amiriparian et al. 2023). Most of today’s text-to-speech (TTS) systems can produce high-quality, high-fidelity, natural speech output, but they still lack expressiveness and fine control over emotional states (Barakat et al. 2024; Gudmalwar et al. 2024). Hence, professional voice-over/dubbing artists are still preferred in the dubbing industry to tackle the complex demands of generating emotionally engaging speech (Gutentag et al. 2017). This poses challenges in terms of dubbing at a large scale, turnaround time, and operational costs.

![](images/8c02ffa954b7efb28729284a6c3d73b42d2d64b785f9f2e98ab3f08807aa9e4c.jpg)  
Figure 1: Conceptual representation of emotional intensity regularization based on direction vector and intensity value. Here, Mod. refers to Moderate and Neu refers to Neutral.

One of the major roadblocks in building emotional speech synthesis is the unavailability of a large emotional speech database with diverse emotional expressions and a wide range of emotion intensities. Even annotating accurate emotional states in the speech requires expert knowledge and is costly and labor-intensive as labeling each speech file with the correct emotion and intensity involves detailed subjective assessments. Hence, emotional intensity control or regularization is still an under-explored open research problem. To this end, we primarily focus on the emotional voice conversion (EVC) task, a sub-topic of emotional speech synthesis for emotional intensity regularization (Zhou et al. 2022c, 2021, 2022a; Shah et al. 2023). In particular, we employ a self-supervised learning (SSL)-based framework to tackle the issues related to the unavailability of a large annotated emotional speech database.

The EVC aims to alter the discrete emotional state of a speech utterance while preserving linguistic content and the speaker’s identity (Zhou et al. 2021). Furthermore, emotional intensity regularization involves fine control over intensity associated with the target emotional state. For example, a neutral utterance can be converted into a mild, moderate, or severe level of anger emotion, as shown in Figure 1. Current EVC methods usually tackle this scenario with discrete (Zhou et al. 2022b; Shah et al. 2023; Zhou et al. 2022c; Ekman and Friesen 1971) or continuous emotion representations (Russell 1980; Posner, Russell, and Peterson 2005; Cho et al. 2024). Discrete emotion representations contain categorical labels such as happy, angry, sad, etc. On the other hand, continuous emotion representations are obtained from the circumplex models and contain continuous-independent dimensions, such as arousal and valance (Russell 1980; Posner, Russell, and Peterson 2005). Achieving emotion intensity control in continuous emotion representation is relatively easier than discrete emotion representations (Gunes et al. 2011; Prabhu, Lehmann-Willenbrock, and Gerkmann 2023; Zhou et al. 2022c,a; Cho et al. 2024). However, obtaining such continuous emotion representations-based annotated data is challenging, as discussed earlier. Hence, this paper aims to achieve emotion intensity regularization for the discrete emotion representations-based EVC methods.

In summary, we propose a method to achieve emotion intensity regularization by means of self-supervised learning and direction latent vector modeling using a diffusion-based EVC. Our key contributions can be summarized as follows:

• To the best of the authors‘ knowledge this is the first attempt that achieves a high-quality emotional intensity regularization in the diffusion-based EVC framework.   
• We propose a novel direction latent vector modelingbased approach for obtaining fine control over intensity while transitioning across different emotional states.   
• The proposed EmoReg utilizes the SSL-based audio feature representations, which are obtained after finetuning the SSL-based framework for a downstream task related to emotions classification.   
• Effectiveness of the proposed EmoReg approach has been shown against SOTA baselines with relevant subjective and objective evaluation techniques across two languages, namely English and Hindi.

# Related Work

EVC methods are usually categorized into parallel and nonparallel approaches depending on the nature of the training data. Parallel means each speaker has spoken the same utterance in different emotional states and non-parallel means recorded sentences are different depending on the emotional states. Since emotion is also dependent on the content that is being spoken, hence, researchers have primarily focused on non-parallel EVC approaches (Shah et al. 2023). Traditionally, researchers have explored various generative models for the discrete emotion representation-based EVC tasks, namely, GMM (Aihara et al. 2012), HMM (Inanoglu and Young 2007), DNN (Lorenzo-Trueba et al. 2018), Sequenceto-sequence models (Ming et al. 2016), Variational Auto Encoder (VAE) (Zhou, Sisman, and Li 2021b), Generative Adversarial Networks (GAN) and its variants (Shah et al. 2023; Li, Zare, and Mesgarani 2021), Diffusion Model-based approaches (Prabhu et al. 2024), etc. However, none of these approaches tackles emotion intensity regularization tasks.

Broadly, emotional intensity regularization is achieved from the emotional labels and the reference speech utterance. The emotion label-based approach is utilized to manipulate auxiliary features such as attention weights, saliency maps, ranking function, or signal processing attributes to achieve emotion intensity regularization (Zhou et al. 2022c,a; Matsumoto, Hara, and Abe 2020; Schnell and Garner 2021; Choi and Hahn 2021; Um et al. 2020). Among these, one of the most prominent techniques is relative attribute (Zhu et al. 2019; Zhou et al. 2022c,a), which describes the distinctions between binary classes using a learned ranking function. However, these methods fail to capture high-level complex abstract representations, which results in artifacts in the converted output and leads to significant degradation in the quality. Also, emotional manipulations often involve inter-dependencies across different types of features, hence manipulating any single feature in isolation fails in achieving emotional intensity regularization. On the other hand, some simple approaches manipulate learned emotion representations via scaling (Choi and Hahn 2021) or interpolations (Um et al. 2020) to achieve emotion regularization. However, these approaches do not work well since emotional embedding space does not align well with the assumption of linear interpolation. Furthermore, highdimensional emotional embedding space is often sparsely populated between two styles. Thus, such approaches result in inept style manipulations.

Another emotional intensity control-based strategy leverages the extended emotional dimensions, namely, Arousal, Valence, and Dominance (AVD) (Russell 1980). AVD-based emotional dimensions provide continuous and fine-grained emotion representations, which can be utilized to alter emotional states in a continuous manner more appropriately than discrete emotion representations. However, obtaining such annotations is difficult due to inherent subjectivity-related issues associated with different annotators and the high costs associated with preparing such data. Hence, emotional intensity regularization is an under-explored research area.

In summary, all previous discrete and continuous emotional intensity control methods produce low-quality noisy converted output. To tackle quality-related issues recently, diffusion-based models have seen great success as a potent and versatile generative AI technology in computer vision, audio, and reinforcement learning (Ho, Jain, and Abbeel 2020). Diffusion models serve as samplers in these applications, producing new samples while actively guiding them toward task-desired features (Ho, Jain, and Abbeel 2020). They also offer versatile high-dimensional data modeling (Ho, Jain, and Abbeel 2020). The diffusion model-based approach has also obtained remarkable success in the voice conversion (Popov et al. 2021a) and EVC task (Prabhu et al. 2024). However, it has not been explored for achieving emotion intensity control-related tasks. Hence, we exploit the diffusion-based EVC method with the proposed DVM-based strategy for the emotion intensity regularization task. The next section presents details about the proposed diffusionbased EmoReg architecture.

# Proposed Methodology Problem Formulation

Given input speech $x _ { 0 }$ along with reference emotion speech $\scriptstyle x _ { r }$ and intensity value $i$ , the goal of EVC is to generate emotional intensity regularized converted speech $y$ such that $Y = G ( X _ { 0 } , e _ { s } , e _ { r } , i , \bar { \theta } )$ , where $X _ { 0 }$ and $Y$ denotes Melspectrogram features corresponding to the input speech $x _ { 0 }$ and the converted emotional speech $y$ , respectively. In addition, $e _ { s } , e _ { r }$ denotes emotional feature representation for source and reference speech, respectively. $\theta$ is model parameters corresponding to the considered conditional diffusion probabilistic model-based EVC architecture. Details of the proposed architecture is as follows.

# Proposed EmoReg Architecture

The proposed EmoReg architecture uses diffusion-based model to achieve EVC while controlling the emotion intensity $i$ . It comprises a diffusion-based decoder and a set of encoders illustrated in Figure 3. The diffusion decoder is responsible for emotion-controllable speech synthesis, while the encoders individually encode the emotion and speech representation that require disentanglement. The decoder is conditioned on the proposed Direction Vector Modeling (DVM) based features $e _ { i r }$ , which facilitate the transition between different emotions. At the output of the diffusion decoder, we obtain the Mel-spectrogram $Y$ , which can be converted to the output speech signal $y = H ( Y )$ , with $H ( . )$ representing the HiFiGAN vocoder. Subsequent sections delve deeper into the architectural components of the proposed method.

# Encoders

Our EmoReg model consists of two encoders. The first encoder is used to encode the lexical content, while the second one is used to obtain emotion representations.

Phoneme Encoder: In this part, we encode the lexical content at the phoneme level by using speaker- and emotionindependent average phoneme-level Mel characteristics. To achieve this, we utilize the transformer-based encoder as described in (Popov et al. 2021a), which has previously been used in speech conversion applications.

$$
\overline { { \mathbf { X } } } = \phi ( \mathbf { X } _ { 0 } )
$$

Here, $\overline { { \mathbf { X } } }$ is average Mel-spectrogram of source audio, $\mathbf { X } _ { 0 }$ is source speech Mel-spectrogram and $\phi ( . )$ represents the pretrained average phoneme encoder.

SSL Emotion Embedding Network: Due to the unavailability of a large annotated emotional speech database, we plan to utilize SSL-based representations. We fine-tune the pre-trained SSL emotion2vec (Ma et al. 2023) embedding network $E ( . )$ using English and Hindi emotional speech databases to learn emotional embedding representations.

We used a pretrained emotion2vec model (Ma et al. 2023) to generate 768-dimensional embeddings, which we then fine-tuned for classifying target emotions. Similar to the diffusion-based text generation models (Gao et al. 2022), we also find that in the high dimensional embedding space, the insufficient noise results in a simple denoising task, which leads to the detoriation of the model. In the process, we first passed these embeddings through a fully connected layer to reduce them to 256 dimensions. We then passed these reduced embeddings through an output layer to classify them into the target emotions (neutral, happy, sad and angry). The training was conducted in a supervised manner. After training, we used the network with the newly added fully connected layer to obtain 256-dimensional emotion embeddings. These emotional embeddings are conditioned in the decoder of the diffusion model in order to achieve target emotional state in the converted output. Additionally, these emotional embeddings are manipulated via proposed directional vector modelling to obtain a fine control over intensities associated with the target emotional states.

# Direction Vector Modeling

During training phase, the 256-dimension emotional embeddings from SSL Emotion Embedding Network are used as an input to the DVM module as shown in Figure 2. We modeled the emotion embedding space using a 64-component Gaussian Mixture Model (GMM) (Reynolds et al. 2009) to derive the local mean vector for each emotion, i.e., Angry, Happy, Neutral, and Sad. After extracting the GMM features for each emotion, pairwise subtraction is performed between the embeddings of Angry, Happy, and Sad with those of Neutral to derive the emotional direction vector matrix for all possible transitions from local mean of one emotional state to another. Subsequently, Principal Component Analysis (PCA) (Abdi and Williams 2010) is applied to the emotional direction matrix corresponding to each emotion, reducing it to 128 components since not all 256 components are reflecting changes related to emotional states, they may also be affected due to content variability. Ablation analysis for selecting number of principle components is presented in Table 5.

During the inference phase, given a source sample in the Neutral emotion and a reference sample in another emotion (e.g., Angry), the emotional direction vector is calculated by subtracting the source’s neutral embedding $( e _ { s } )$ from the reference’s emotional embedding $( e _ { r } )$ . This direction vector is then transformed using the PCA to obtain a 128-dimensional vector, which is then converted back to the original 256- dimensional space using PCA inverse transformation, result

![](images/96b3a32fd48e315930dc185cbd7eb32024f481e8b73c0f9f22f11c3dfa209bcf.jpg)  
Figure 2: Three key steps of the proposed DVM approach. 1) Fitting local GMM to each emotional state. 2) computing directional vectors for all possible transitions from the local mean of one emotional state to another. 3) Applying PCA to find relevant direction for emotional transition.

GT MSS xogpg(x MSES XStdient   
Source Encoders   
Sample Average Phoneme M Diffusion-Based Decoder X Encoder X Xt Concat →U-Net sθ(Xt,X,eir,t) t