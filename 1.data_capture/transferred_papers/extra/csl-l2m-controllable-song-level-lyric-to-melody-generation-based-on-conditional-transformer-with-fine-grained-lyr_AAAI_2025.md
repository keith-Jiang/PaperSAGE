# CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on Conditional Transformer with Fine-Grained Lyric and Musical Controls

Li Chai, Donglin Wang

Westlake University chaili, wangdonglin @westlake.edu.cn

# Abstract

Lyric-to-melody generation is a highly challenging task in the field of AI music generation. Due to the difficulty of learning strict yet weak correlations between lyrics and melodies, previous methods have suffered from weak controllability, lowquality and poorly structured generation. To address these challenges, we propose CSL-L2M, a controllable song-level lyric-to-melody generation method based on an in-attention Transformer decoder with fine-grained lyric and musical controls, which is able to generate full-song melodies matched with the given lyrics and user-specified musical attributes. Specifically, we first introduce REMI-Aligned, a novel music representation that incorporates strict syllable- and sentencelevel alignments between lyrics and melodies, facilitating precise alignment modeling. Subsequently, sentence-level semantic lyric embeddings independently extracted from a sentence-wise Transformer encoder are combined with wordlevel part-of-speech embeddings and syllable-level tone embeddings as fine-grained controls to enhance the controllability of lyrics over melody generation. Then we introduce human-labeled musical tags, sentence-level statistical musical attributes, and learned musical features extracted from a pre-trained VQ-VAE as coarse-grained, fine-grained and high-fidelity controls, respectively, to the generation process, thereby enabling user control over melody generation. Finally, an in-attention Transformer decoder technique is leveraged to exert fine-grained control over the full-song melody generation with the aforementioned lyric and musical conditions. Experimental results demonstrate that our proposed CSL-L2M outperforms the state-of-the-art models, generating melodies with higher quality, better controllability and enhanced structure.

Demos — https://lichaiustc.github.io/CSL-L2M/ Code — https://github.com/LiChaiUSTC/CSL-L2M

# Introduction

Deep learning techniques have been increasingly applied to various music generation tasks (Duan, Yu, and Oyama 2024; Hahn et al. 2023; Yu, Srivastava, and Canales 2021; Tian et al. 2023). Lyric-to-melody generation, one of the most essential and common tasks in songwriting, has attracted growing interest from both academia and industry. A highquality lyric-to-melody generation is required to generate melodies not only following good musical patterns but also aligning with the given lyrics. Due to the scarcity of paired lyric-melody data with alignment information and the difficulty of learning the strict but weak correlations between lyrics and melodies, this task remains under-explored.

Many deep learning methods have been explored for lyric-to-melody generation. A sequence-to-sequence based melody composition model is proposed in (Bao et al. 2019) , which is the first work to use an end-to-end network model to generate melodies from lyrics. Subsequently, Yu (Yu, Srivastava, and Canales 2021) proposes a conditional LSTMGAN generative model for melody generation from lyrics. In (Srivastava et al. 2022), a novel architecture, three branch conditional LSTM-GAN is proposed to further improve generation quality. However, the direct mapping from lyrics to melodies is difficult to learn because they are weakly correlated (e.g., a melody can correspond to many different lyrics and vise versa.). Accordingly, these end-to-end generation methods suffer from low generation quality due to the limited available parallel lyric-melody data. To this end, an unsupervised method is proposed in (Sheng et al. 2021), which performs self-supervised masked sequence to sequence pretraining on large amount of unpaired lyric and melody data. In addition, a two-stage generation method with music template is proposed in (Ju et al. 2021), which is data efficient and addresses the issues of limited paired data to some extent. With the tremendous success of large language models (LLMs) (Touvron et al. 2023), more recently, the work in (Ding et al. 2024) attempts to leverage the capability of LLMs to model the lyric-melody relationship.

Currently, the lyric-to-melody generation methods including those mentioned above focus on generating short melodies from lyrics typically consisting of one sentence or a few sentences, where a full-song melody is usually composed by simply concatenating these sentence-level melodies resulting in incoherent musical structure without both repetition patterns and distinguishable verse-chorus structure. In addition, controllability is a crucial aspect of the lyric-to-melody generation task, which allows users to interact with the generation process to create their expected melodies. Nevertheless, only a few lyric-to-melody works have explored the controllability. In (Ju et al. 2021), the generated melodies can be controlled by adjusting the musical elements in music templates including tonality and chord progression. A reference style embedding technique is proposed in (Zhang, Yu, and Takasu 2023) to achieve the control over the style of generated melodies. The research of (Duan et al. 2022) enables users to interact with the generation process and recreate music by selecting from recommended musical attributes. However, these works only provide a few coarse-grained musical attribute controls. One more thing, since one syllable may correspond to one or more notes, the alignment between the given lyrics and corresponding melodies could be “one-to-one ”or “one-tomany ”. Most of previous methods only consider the “oneto-one ”alignment, which introduces bias into the melody composition.

To address the aforementioned issues, we propose a controllable song-level lyric-to-melody generation method called CSL-L2M, which is capable of generating melodies aligning with lyrics and user-specified musical attributes at the full-song level. Specifically, we first introduce a novel music representation called REMI-Aligned. This representation incorporates strict syllable- and sentence-level lyricmelody alignments, which makes both exact and “one-tomany ”alignment learning feasible. Inspired by (Wu and Yang 2023), which equips conditional Transformer with the capability to model long sequences under fine-grained timevarying conditions through in-attention, we integrate the in-attention technique into our CSL-L2M model. Multiple multi-granularity lyric controls (including sentence-level semantic embeddings, word-level part-of-speech (POS) embeddings, and syllable-level tone embeddings) and musical controls (including coarse-level human-labeled musical tags, sentence-level statistical musical attributes, and learned high-fidelity musical features (von Ru¨tte et al. 2023) from a pre-trained Vector Quantized-Variational AutoEncoder (VQ-VAE)) are extracted and fed into the conditional Transformer decoder through in-attention to realize tight fine-grained control of lyrics and musical attributes over melody generation process. This enables the generation of high-quality melodies from lyrics, precisely tailored to the user’s desired musical style. Moreover, the musical controls not only enable user-controllable generation but also provide the model with additional musical information that is beneficial for melody modeling. Experiments conducted on our 10,170 Chinese pop songs demonstrate that CSL-L2M could generate melodies that are both well-matched with the lyrics and consistent with the user-specified musical attributes. Compared to the state-of-the-art lyric-to-melody generation methods, CSL-L2M generates melodies with higher quality, better controllability and enhanced structure.

# Related Work

Lyric-to-Melody Generation The development of lyricto-melody generation has evolved from traditional rulebased (Nichols 2009; Monteith, Martinez, and Ventura 2012) and statistical methods (Long, Wong, and Sze 2013) to deep learning methods. The traditional methods usually rely on specific hand-designed musical rules and suffer from low generation quality. Currently, the end-to-end deep generative models are the mainstream methods but they suffer from several challenges: 1) weak correlations between lyrics and melodies are difficult to capture by the network models, where much paired training data with alignment information is required; 2) strict alignment between each syllable in the given lyric and note in the corresponding melody is required, which needs additional alignment modeling. As for the first challenge, limited available paired lyric-melody data affects generation quality. The end-to-end models which directly learn the mapping from lyrics to melodies with the limited paired data often lead to poor generation quality. To this end, SongMASS (Sheng et al. 2021) improves the generation performance of end-to-end models by leveraging selfsupervised pre-training on much unpaired lyric and melody data. Furthermore, TeleMelody (Ju et al. 2021), a two-stage generation pipeline based on musical templates, is proposed to enhance data efficiency and further improve generation performance. In addition, ReLyMe (Zhang et al. 2022) introduces several principles of lyric-melody relationships from music theory into the decoding process, enhancing the harmony between lyrics and melodies. However, these methods fail to exploit melody-related lyric information and additional musical information for tightly controlling over the melody generation. Consequently, they are unable to adequately capture the intricate relationships between lyrics and melodies, resulting in limited generation quality. Moreover, few of them generate melodies at the full-song level, causing poor musical structure. As for the second challenge, most existing works either focus solely on the “one-to-one ”lyricmelody alignment or do not ensure precise alignment, which can easily degrade generation quality.

Controllable Music Generation Controllability in music generation aims to provide user control over the process in a desired direction (Briot and Pachet 2020). According to the levels of controllability, it can be divided into global/coarsegrained control and fine-grained control. The former refers to the fact that generation process is guided by time-invariant controls. Instead, the later refers to the fact that the generation process is guided by time-varying controls, which can provide more flexible and precise control, especially in the generation of long sequences. Controllable music generation has attracted increasing research interest. In (Dong et al. 2018; Yang, Chou, and Yang 2017; Neves, Fornari, and Florindo 2022), global conditions are injected into the training procedure of generative adversarial networks. Some works (Payne 2019; Sarmento et al. 2023) achieve global control through conditional Transformer models with prompt-based control tokens. Many methods based on VAE enable users to exert global control by manipulating latent conditioning vectors (Brunner et al. 2018; Roberts et al. 2018; Tan and Herremans 2020). Transformer autoencoders are used in (Choi et al. 2020) to realize improved control by learning global performance representations. However, these global controls often become less effective during long sequence generation, as the model may forget or weaken the global conditions over time. In contrast, MuseMorphose (Wu and Yang 2023) and FIGARO (von Ru¨tte et al. 2023) introduce fine-grained control, where the former is realized through one Transformer VAE based on an in-attention conditioning technique and the later is achieved through description-to-sequence learning. Existing research on lyric-to-melody generation rarely pays attention to controllability. Only a few works have explored this area and do not offer fine-grained and flexible control. In this paper, we delve into the controllability of lyric-to-melody generation.

# Methodology

To overcome the difficulty of learning strict yet weak correlations between lyrics and melodies and enable user controls over full-song melody generation, we propose a controllable song-level lyric-to-melody generation method called CSL-L2M, as shown in Figure 2. This method is capable of generating full-song melodies that match the given lyrics and adhere to user-specified musical attributes. We achieve this by employing the in-attention technique, as proposed in (Wu and Yang 2023), to tightly control the conditional autoregressive Transformer decoder’s generation process under multiple multi-granularity lyric and musical conditions.

# Technical Background

The unconditional Transformer decoder’s autoregressive generation process can be formulated as $p ( x _ { t } | x _ { < t } )$ , where $\boldsymbol { x } _ { t }$ is the element of a sequence to predict at timestep $t$ , and $x _ { < t }$ represents all previously generated elements of the sequence. If a global condition vector $c$ is offered to the model, the modeling could be formulated as $p ( x _ { t } | \boldsymbol x _ { < t } , \boldsymbol c )$ . However, the global control tends to lose its effectiveness during long sequence generation. It is needed to incorporate fine-grained control mechanisms. Assuming that the target sequence consists of $N$ segments and each timestep index $t \in [ 1 , T ]$ belongs to one of the $N$ sets of indices $I _ { 1 } , I _ { 2 } , . . . , I _ { N }$ , where ${ { I } _ { n } } \cap { { I } _ { n ^ { \prime } } } \ : = \ : \emptyset$ for $n \ne n ^ { \prime }$ and $\bigcup _ { n = 1 } ^ { N } I _ { n } \ = \ [ 1 , T ]$ , the fine-grained control is achieved by providing the generation model with each segment-level condition vector $\scriptstyle { c _ { n } }$ during the corresponding time interval $I _ { n }$ , formulated as:

$$
p ( x _ { t } | x _ { < t } ; c _ { n } ) , \quad { \mathrm { f o r } } t \in I _ { n } ,
$$

where the time-varying condition vectors $\pmb { c } _ { 1 } , \pmb { c } _ { 2 } , . . . , \pmb { c } _ { N }$ provide a high-level blueprint of the sequence to model. This could be helpful for long sequence generation, particularly for full-song music generation.

There are many ways to condition autoregressive Transformer decoders at fine-grained level, where the in-attention conditioning (Wu and Yang 2023) offers tight control. Specifically, the in-attention method projects the segmentlevel condition vector $\scriptstyle { c _ { n } }$ to the same space as the selfattention hidden stats via

$$
\begin{array} { r } { \pmb { e } _ { n } ^ { \top } = \pmb { c } _ { n } ^ { \top } W _ { i n } , \quad W _ { i n } \in \mathbb { R } ^ { d _ { c } \times d } . } \end{array}
$$

Then the hidden condition state $\scriptstyle { e _ { n } }$ is added to each hidden state of all the self-attention layers to obtain the input to the subsequent layer, formulated as:

$$
\begin{array} { r l r } & { } & { \tilde { \pmb { h } } _ { t } ^ { l } = \pmb { h } _ { t } ^ { l } + \pmb { e } _ { n } , \quad \forall l \in \{ 0 , . . . , L - 1 \} ; } \\ & { } & { \pmb { h } _ { t } ^ { l + 1 } = \mathrm { S e l f A t t e n t i o n } ( \tilde { \pmb { h } } _ { t } ^ { l } ) , \qquad } \end{array}
$$

which serves as a frequent reminder of the segment-level conditions for the Transformer decoder, thereby achieving tight control over the generation process.

PR PR GR P SR GR Aa Le GR C Modified REMI Sheet music 3 手 4 Lyrics漡 我 在 前. Pinyin: wo zai qian 5 PAR PR Ar P PR A PR PR O GRAf R 35 REMI-Aligned

# REMI-Aligned Representation

To apply neural sequence models to symbolic music generation tasks, it is necessary to first convert a musical piece into a time-ordered sequence of discrete tokens. There are several ways to implement the conversion, leading to different sequence representations of the same music piece. One prevalent music representation is based on revamped MIDIderived events (REMI) (Huang and Yang 2020). In REMI, a musical piece is represented as a time-ordered sequence of event tokens including bar, position, pitch, duration, velocity, tempo and chord.

To precisely model the strict alignments between lyrics and melodies, we propose incorporating both sentence-level and syllable-level alignments into the music representation to enable explicit learning. Accordingly, we extend REMI to form a REMI-Aligned music representation by adding these alignments, while discarding tempo, chord and note velocity tokens given the fixed tempo in our dataset, irrelevance of note velocity to our task, and potential issues with chord accuracy. Furthermore, since 64th notes are the shortest notes in our dataset, we improve the temporal resolution of note position from 4 to 16 sub-beats per quarter note, enabling precise quantization of each note in our fixed 4/4 time signature dataset. Examples of a music sequence encoded in modified REMI and REMI-Aligned are shown in Figure 1.

# Model Architecture

Figure 2 illustrates the architecture of our proposed CSLL2M, consisting of a sentence-wise bidirectional Transformer encoder and an autoregressive Transformer decoder equipped with the capability to model full-song melodies under both lyric and musical multi-granularity controls.

Lyric Controls In CSL-L2M, we fully utilize the lyric information related to melodies in training, which improves the model capability to capture the correlations between lyrics and melodies. Specifically, we extract syllablelevel tone embeddings, word-level POS embeddings, and sentence-level semantic embeddings serving as time-varying conditions at different granularities to exert fine-grained controls over the Transformer decoder’s generation via the in-attention technique.

1) Tone: Tone 1, in tonal languages, refers to the pitch

Tone Extraction Embedding Syllable-level tone embeddings POS Tagging Embedding Word-level POS embeddings Multi-granularity lyric controls Transformer Encoder Snt 1 (operating on each sentence) 1 Concatenation Transformer Decoder Lyrics Snt 2Snt N bidirectional Sentence-level In-attention (operating on full song) Full-song lyrics Sentence splitting semantic embeddings autoregressive   
Global Human-labeled Embedding   
AttributeSEtaxtirsatictiaoln T mMulsti-cgarlacnounltarroitlys Quantization Embedding   
国同 小 Attribute Extraction 山   
面园面回 Pre-trained VQ-VAE (operating on each sentence-level melody)   
Full-song melody Sentence-level splitting Learned high-fidelity features

variations that help distinguish words with the same spelling but different meanings, playing a crucial role in minimizing semantic ambiguities. Around $60 \%$ of languages have tone. In contrast to English, Chinese is a tonal language containing four main tones and one light tone in its characters. The pitch flow of a melody in Chinese songs is usually closely related to the tones of the corresponding lyrics. Accordingly, we incorporate the tone information into our model training to help learn pitch flow of the generated melodies to match with the given lyrics. The $s ^ { t h }$ syllable-level tone attribute $c _ { s } ^ { \mathrm { t o n e } }$ of each song is converted to an embedding vector $c _ { s } ^ { \mathrm { t o n e } } = \bar { \mathbf { E } } \mathbf { m } \mathbf { b } ^ { \mathrm { t o n e } } ( c _ { s } ^ { \mathrm { t o n e } } )$ , before being fed into the decoder as a syllable-varying condition.

2) POS: POS contains potential information of prosodic boundaries between words, which is helpful for enhancing rhythms and structures of generated melodies. Consequently, the POS information is utilized for our model training. Specifically, we first perform POS tagging on the given lyrics by Jieba 2, an open-source tool that supports 56 tags commonly used in Chinese. Then the $p ^ { t h }$ word-level POS attribute $\dot { c } _ { p } ^ { \mathrm { P O S } }$ of each song is transformed into an embedding vector $c _ { p } ^ { \mathrm { P O S } } = . \mathbf { E m b } _ { . } ^ { \mathrm { P O S } } ( c _ { p } ^ { \mathrm { P O S } } )$ , before being fed into the decoder as a word-varying condition.

3) Semantic Embeddings: Since most previous works divide the input lyrics into sentences and then compose each piece of melody from the sentences one by one, we set the granularity of lyric text conditions to a sentence. We employ a bidirectional Transformer encoder (Vaswani et al. 2017) to learn to extract sentence-level latent semantic embeddings of the given lyrics, which is jointly trained with the Transformer decoder using the negative log-likelihood (NLL) training objective. More concretely, the input lyrics of each song are divided into sentences, formulated as $I =$ $\{ I _ { 1 } , I _ { 2 } , . . . , \bar { I } _ { N } \}$ , where $I _ { n }$ is the $n ^ { t h }$ sentence of the lyrics. Then the Transformer encoder encodes these sentences in parallel. We treat the encoder’s attention output at the first timestep (corresponding to the SEQ token of the sentence senqtuaetinocne otfoktehness),eqi.ue.,n $\pmb { h } _ { n , 1 } ^ { L _ { \mathrm { E n c } } }$ ,nallsy,thite scopnetrefxotrumaleidzeadn raefpfirneetransformation via a learnable weight $W$ to obtain the semantic embedding. These operations can be summarized as follows:

$$
\begin{array} { r l r } & { } & { \pmb { h } _ { n , 1 } ^ { L _ { \mathrm { E n c } } } = \mathbf { E n c } ( I _ { n } ) \quad \mathrm { f o r } 1 \leq n \leq N ; } \\ & { } & { z _ { n } ^ { \mathrm { s e m } } = { h } _ { n , 1 } ^ { L _ { \mathrm { E n c } } \top } W , \quad W \in \mathbb { R } ^ { d \times d _ { l } } , } \end{array}
$$

where $z _ { n } ^ { \mathrm { s e m } }$ is the semantic embedding for the $n ^ { t h }$ sentence.

Musical Controls To enable user control over the melody generation, we introduce human-labeled musical tags, sentence-level statistical musical attributes, and learned latent musical representations extracted from a pre-trained VQ-VAE serving as coarse-grained, fine-grained and highfidelity controls, respectively, to the generation process.

1) Human-Labeled Musical Tags: We offer a high-quality, precisely annotated parallel lyric-melody dataset with alignment information consisting of 10,170 Chinese pop songs with time signature $4 / 4$ . Moreover, tags of key 3, emotion, and structure 4 of each song are meticulously annotated, encompassing $2 4 ~ ^ { 5 } , ~ 3 ~ ^ { 6 }$ , and $5 ^ { 7 }$ distinct categories respectively. The three types of musical tags serving as coarsegrained conditions are introduced into the decoder to realize human-interpretable control over the melody generation. Specifically, key is highly correlated with pitch distribution of the entire melody. The key of each song $c ^ { \mathrm { k e y } }$ is transformed into an embedding vector by an embedding layer, i.e., $c ^ { \mathrm { k e y } } \ = \ \mathbf { E m b } ^ { \mathrm { k e y } } ( c ^ { \mathrm { k e y } } )$ and then fed into the decoder as a global condition. Similarly, the emotion of each song $c ^ { \mathrm { e m o t } }$ is transformed into an embedding vector $c ^ { \mathrm { e m o t } } =$ $\mathbf { E m } \mathbf { \bar { b } } ^ { \mathrm { e m o t } } ( c ^ { \mathrm { e m o t } } )$ and then fed into the decoder as a global condition. The verse-chorus form, serving as the cornerstone of pop songs, comprises two core sections—a verse and a chorus—that typically contrast melodically, rhythmically, harmonically and dynamically. We convert the $u ^ { t h }$ structure-level attribute of each song into an embedding vector $c _ { u } ^ { \mathrm { s t r u c } } = \mathbf { E m b } ^ { \mathrm { s t r u c } } ( c _ { u } ^ { \mathrm { s t r u c } } )$ , and then fed it into the decoder as a structure-varying condition.

2) Statistical Musical Attributes: To enable fine-grained user controls over melody generation and help the model better learn correlations between lyrics and melodies, we introduce sentence-level statistical musical attributes. Their granularity is set to a sentence instead of a bar to maintain consistency with the semantic lyric embedding control granularity. We utilize 12 types of statistical musical attributes: pitch mean (PM), pitch variance (PV), pitch range (PR), direction of melodic motion (DMM), amount of arpeggiation (AA), chromatic motion (CM), duration mean (DM), duration variance (DV), duration range (DR), prevalence of most common note duration (MCD), note density (ND), fraction of syllables in lyrics to notes in the corresponding melodies (Align) 8. They are calculated for each sentence-level melody sequence. We first quantize these attributes into $K$ classes with approximately equal sample sizes, where $K = 6 4$ in our work. Then the $n ^ { t { \hat { h } } }$ sentence-level attributes of the 12 statistical musical attributes for each song are v, ,e ,d , ,r $c _ { n } ^ { \mathrm { P M } }$ $ { c _ { n } } ^ { \mathrm { P V } }$ , cPnR, cnDMM, $ { \boldsymbol { c } } _ { n } ^ { \mathrm { A A } }$ $ { \boldsymbol { c } } _ { n } ^ { \mathrm { C M } }$ $ { \boldsymbol { c } } _ { n } ^ { \mathrm { D M } }$ $ { c _ { n } ^ { \mathrm { D V } } }$ $c _ { n } ^ { \mathrm { D R } }$ $ { \boldsymbol { c } } _ { n } ^ { \mathrm { M C D } }$ $ { c _ { n } ^ { \mathrm { N D } } }$ $\pmb { c } _ { n } ^ { \mathrm { A l i g n } }$ and fed into the decoder. These controls can be grouped into four categories, i.e., pitch-related controls (pitch Ctls=Conca $\mathsf { u t } ( c ^ { \mathsf { P M } } ; c ^ { \mathsf { P V } } ; c ^ { \mathsf { P R } } ; c ^ { \mathsf { \hat { D } M M } } ; c ^ { \mathsf { A A } } ; c ^ { \mathsf { C M } } ) )$ ), durationrelated controls (Dur Ctls=Concat $( c ^ { \mathrm { D M } } ; c ^ { \mathrm { D V } } ; c ^ { \mathrm { D R } } ; c ^ { \mathrm { M C D } } ) \big >$ ), rhythm-related controls $( c ^ { \mathrm { N D } } )$ , and note-number-related controls $( c ^ { \mathrm { A l i g n } } )$ .

3) Learned Musical Features: Inspired by (von Ru¨tte et al. 2023), we introduce learned musical features extracted from the latent space of a pre-trained VQ-VAE model to provide high-fidelity information to the decoder. This helps to alleviate non-injectivity problem in the lyric-to-melody generation task. The VQ-VAE model consists of a Transformer encoder, a Transformer decoder, and a vector quantization. For training, first, the full-song melody of each song is split into sentence-level melody sequences $X = \{ X _ { 1 } , \bar { X } _ { 2 } , . . . , \bar { X } _ { N } \}$ , where $X _ { n }$ is the $n ^ { t h }$ sentence-level melody sequence and tokenized by REMI-Aligned. Then the Transformer encoder maps these sequences to the latent space in parallel. The encoder’s attention output at the first timestep (corresponding to the SEQ token of the sentence-level melody sequence tokens) is considered as contextualized representation of the sequence. Finally, the quantized latent representations are obtained via the vector quantization and then fed into the decoder through in-attention to reconstruct the original fullsong melody. Note that the hyperparameters in the vector quantization, i.e., latent group and codebook sizes, are set to 64 and 2048 respectively in our work. Thus the sentencelevel quantized latent representations $z _ { n } ^ { \mathrm { l e a r n e d } }$ extracted from the pre-trained VQ-VAE serving as learned musical features are introduce into our CSL-L2M training to provide highfidelity information.

Feeding Multi-Granularity Controls into the Transformer Decoder Borrowing the in-attention technique from (Wu and Yang 2023), which conditions Transformer decoders with time-varying conditions during long sequence generation, we feed aforementioned multi-granularity controls into our Transformer decoder to achieve firm control over the full-song melody generation. Specifically, the wordlevel POS embeddings and sentence-level semantic embeddings are expanded to the syllable level by replication. Then they are added to the tone embeddings to get the syllablelevel lyric-related controls $c _ { s } ^ { \mathrm { l y r i c } } = c _ { s } ^ { \mathrm { t o n e } } + c _ { s } ^ { \mathrm { P O S } } + z _ { s } ^ { \mathrm { s e m } }$ . Next, aforementioned multi-granularity musical controls are expanded to the syllable level by replication according to the alignment information between lyrics and melodies. Finally, these syllable-level lyric and musical controls are fed into the decoder through in-attention after concatenation, i.e.,

![](images/2fbad388582529949cfa8f75fb36363a3785858c06b6f7a4351ed13bad77c135.jpg)  
Figure 3: Distributions of music attributes in our paired lyric-melody dataset.

$$
\begin{array} { r l } & { c _ { s } = \mathrm { c o n c a t } ( [ c _ { s } ^ { \mathrm { l y r i c } } ; c _ { s } ^ { \mathrm { k e y } } ; c _ { s } ^ { \mathrm { e m o t } } ; c _ { s } ^ { \mathrm { s t r u c } } ; c _ { s } ^ { \mathrm { P M } } ; c _ { s } ^ { \mathrm { P V } } ; c _ { s } ^ { \mathrm { P R } } ; c _ { s } ^ { \mathrm { D M M } } ; } \\ & { \qquad c _ { s } ^ { \mathrm { A A } } ; c _ { s } ^ { \mathrm { C M } } ; c _ { s } ^ { \mathrm { D M } } ; c _ { s } ^ { \mathrm { D V } } ; c _ { s } ^ { \mathrm { D R } } ; c _ { s } ^ { \mathrm { M C D } } ; c _ { s } ^ { \mathrm { N D } } ; c _ { s } ^ { \mathrm { A l i g n } } ; c _ { s } ^ { \mathrm { l e a r n e d } } ] ) , } \\ & { y _ { t } = \mathbf { D e c } ( x _ { < t } ; c _ { s } ) . } \end{array}
$$

# Experiments Experimental Settings

Dataset The collection of paired lyric-melody data is difficult due to the need for precise synchronization between lyrics and melodies as shown in the sheet music in Figure 1, which requires detailed annotation and specific expertise. Currently, available paired lyric-melody dataset with alignment information is limited and of insufficient quality. To this end, we offer a high-quality, precisely annotated parallel lyric-melody dataset, encompassing 10,170 Chinese pop songs with time signature 4/4. Moreover, musical tags for each song including key, lyric emotion, song structure, and beats per minute (BPM) are precisely annotated. We perform some statistics on this dataset shown in Figure 3. The following observations are made: 1) the most pitch/MIDI numbers fall within the range of 50 to 80; 2) in contrast to melodies in the English dataset (Yu, Srivastava, and Canales 2021), the melodies in our Chinese dataset feature a predominance of short musical notes, specifically 8th and 16th notes; 3)

more than $80 \%$ of characters/syllables correspond to a single musical note (i.e. “one-to-one ”alignment), and nearly $20 \%$ of characters correspond to two or more notes (i.e. “one-tomany ”alignment); 4) the BPM of most songs falls within the range of 60 to 120. The 10,170 Chinese pop songs are split into the training, validation, and test sets in an 9:0.5:0.5 ratio for our experiments.

Implementation Details Both the encoder and decoder of our CSL-L2M and VQ-VAE models consist of 12 selfattention layers with 8 self-attention heads, 512 hidden size and 2048 feed-forward dimension. The dimension of each lyric attribute embedding as well as learned musical feature is 128. And the dimension of each human-annotated and statistical musical attribute embedding is 32. The models are trained with Adam optimizer and teacher forcing. We use linear warm-up to increase the learning rate to $1 \bar { 0 } ^ { - 4 }$ in the first 200 steps, followed by a 150k-step cosine decay down to $5 \times 1 0 ^ { - 6 }$ . The batch size is set to 4. During inference, nucleus sampling (Holtzman et al. 2020) is used to sample from the decoder output distribution at each timestep with a softmax temperature $\tau = 1 . 2$ and truncating the distribution at cumulative probability $p = 0 . 9$ .

Evaluation Metrics Unlike previous works that evaluate generated melodies from lyrics at the sentence level, we conduct evaluations on full-song melodies.

1) Objective Metrics: Objective evaluations are conducted on our test set comprising around 500 songs from our 10,170 Chinese pop songs. We focus on assessing the similarity between the generated and the ground-truth melodies. The following objective metrics proposed by (Sheng et al. 2021) are adopted: 1) Pitch Distribution Similarity (PD); 2) Duration Distribution Similarity (DD); 3) Melody Distance (MD).

2) Subjective Metrics: Subjective evaluations are conducted on 10 songs randomly selected from our test set. We invite 70 participants (including 50 amateurs and 20 professionals) to score the melody properties using a scale from 1 (Poor) to 5 (Perfect). The following subjective metrics are considered: 1) Harmony: Is the melody itself harmonious as well as harmonized with the lyrics ? 2) Rhythm: Does the rhythm sound natural and match the rhythm of the lyrics? 3) Structure: How well does the melody structure match lyric structure? Specifically, whether lyrics with similar rhythm patterns have similar melodies? Does the melody feature a distinguishable verse-chorus structure? Are the transitions between contiguous phrases natural and coherent? 4) Emotion: Does the melody convey a consistent emotion with the lyrics? 5) Quality: What is the overall quality of the melody?

# Experimental Results

Main Results Since learned musical features need to be extracted from existing melodies, unless otherwise specified, our reference to CSL-L2M refers to the version without learned musical controls. The evaluation of the full version will be conducted later in the context of style transfer and controllable generation. We first compare our CSLL2M with two state-of-the-art models, i.e. TeleMelody (Ju et al. 2021) and SongComposer (Ding et al. 2024). As shown in Table 1, CSL-L2M significantly outperforms advanced

CSL-L2M CSL-L2M 1.2 w/ learned Ctls 1.2 w/ learned Ctls w/o Dur Ctls w/o Dur Ctls w/o Dur+pitch Ctls w/o Dur+pitch Ctls 1.0 w/o Dur+pitch+ND Ctls 1.0 w/o Dur+pitch+ND Ctls w/o Dur+pitch+ND+Align Ctls w/o Dur+pitch+ND+Align Ctls   
0.8 w/o musical Cts 0.8 W/ musica ctis   
  
1 0.4 W 0.4 0.2 0.2 0.0 0.0 0 2 4 6 8 10 0 2 4 6 8 10 Training Epochs Training Epochs

models, namely SongComposer and TeleMelody, in both objective and subjective evaluations, demonstrating the effectiveness of CSL-L2M in generating high-quality song-level melodies from lyrics. We further perform ablation study to verify the effectiveness of lyric and musical controls in CSL-L2M. As illustrated in Table 2 and Figure 4, by successively removing duration-related controls, pitch-related controls, note density and alignment controls, and humanannotated musical attribute controls, we observe a continuous performance degradation. This indicates that the musical controls, in addition to enabling user-controlled generation, can help conditional Transformer in modeling melodies because they offer the model more musical information for reference. Besides, the learned musical features include highfidelity information of melodies, which aids in reducing noninjectivity of the generation model. As a result, we find that CSL-L2M equipped with learned musical controls achieves top performance that nearly reaches the ceiling. Moreover, CSL-L2M with only lyric controls exceeds the performance of the two state-of-the-art models. This confirms the effectiveness of our designed fine-grained lyric controls and the lyric-to-melody generation framework based on conditional Transformer with the in-attention conditioning mechanism.

Controllability Study Given that our statistical musical controls are ordinal by nature, following (Kawai, Esling, and Harada 2020) and (Wu and Yang 2023), we use the Spearman’s rank correlation coefficient $\rho$ to quantitatively assess the strength of statistical musical attribute control. To simultaneously evaluate the impact on other unrelated attributes when transferring a specific attribute, we calculate Spearman’s rank correlation coefficient matrix between the user-specified attribute classes and attribute raw scores derived from the generated melodies. Results in Figure 5 reveal the strong and independent controllability strengths of CSL-L2M in attribute control. Specifically, for example, $\rho _ { \mathrm { P M } } = 0 . 9 8$ denotes a strong and positive correlation between the user-specified attribute class $c ^ { \mathrm { P M } }$ and the attribute raw class $\hat { c } ^ { \mathrm { P M } }$ computed from the generated melodies, which demonstrates strong controllability of the pitch mean attribute. In contrast, $\rho _ { \mathrm { P M | A l i g n } } = 0 . 0 9$ is the correlation co

Table 1: Objective and subjective evaluation results of our CSL-L2M and compared models.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="3">Objective</td><td colspan="5">Subjective</td></tr><tr><td>PD(%)↑</td><td>DD(%)↑</td><td>MD↓</td><td>Harmony↑</td><td>Rhythm↑</td><td>Structure↑</td><td>Emotion↑</td><td>Quality↑</td></tr><tr><td>SongComposer</td><td>33.37</td><td>44.98</td><td>3.12</td><td>2.32</td><td>2.56</td><td>2.33</td><td>2.51</td><td>2.47</td></tr><tr><td>TeleMelody</td><td>40.02</td><td>49.82</td><td>2.93</td><td>2.71</td><td>2.90</td><td>2.45</td><td>2.42</td><td>2.72</td></tr><tr><td>CSL-L2M</td><td>86.35</td><td>93.50</td><td>1.27</td><td>3.74</td><td>4.03</td><td>4.20</td><td>3.86</td><td>3.94</td></tr></table></body></html>

<html><body><table><tr><td></td><td>PD(%)↑</td><td>DD(%)↑</td><td>MD↓</td></tr><tr><td>CSL-L2M</td><td>86.35</td><td>93.50</td><td>1.27</td></tr><tr><td>+ w/ learned Ctls</td><td>97.98</td><td>98.62</td><td>0.25</td></tr><tr><td>- w/o Dur Ctls</td><td>85.82</td><td>86.41</td><td>1.37</td></tr><tr><td>- w/o Dur+pitch Ctls</td><td>66.07</td><td>85.65</td><td>1.78</td></tr><tr><td>- w/o Dur+pitch+ND+Align Ctls</td><td>63.18</td><td>63.59</td><td>2.03</td></tr><tr><td>- w/o musical Ctls</td><td>49.20</td><td>59.13</td><td>2.26</td></tr></table></body></html>

1.00  
0.980.13 0.07-0.220.53-0.51-0.25-0.47-0.440.14 0.27 0.09  
无 0.220.88 0.750.12-0.06-0.21-0.01-0.03-0.01-0.03-0.04 0.04 0.750.140.80 0.850.01-0.14-0.06 0.07 0.12 0.16 -0.01-0.19 0.010.50-0.240.09 0.050.84-0.19 0.03 0.08 0.07 0.06-0.180.02-0.050.51-0.09-0.15-0.240.85-0.41-0.18-0.35-0.45-0.010.29-016 -0.25  
-0.35-0.29-0.25-0.09-0.340.780.20 0.29 0.36 0.22-0.17 0.07  
-0.260.00 0.11 0.05-0.170.360.970.74 0.630.08-0.770.20 0.00  
-0.460.00 0.18 0.03-0.310.450.75 0.96 0.91-0.02-0.71-0.08 -0.25  
0 -0.440.02 0.22-0.02-0.400.51 0.630.91 0.960.02-0.63-0.10-0.500.11 0.04-0.02-0.28-0.030.090.03-0.010.020.980.07 0.140.26-0.05-0.27 0.13 0.31-0.41-0.76-0.65-0.62-0.060.91-0.14 -0.750.10 0.03-0.04-0.06-0.21-0.030.13-0.14-0.150.12-0.020.96-1.00CPM cPv PR DMM AA -CM DM Dv DB MCD -NDcAlign

efficient between the user-specified alignment attribute class $c ^ { \mathrm { A l i g n } }$ and the unrelated attribute class $\hat { c } ^ { \mathrm { P M } }$ computed from the generated melodies, revealing the independent controllability of attributes in the multi-attribute scenario.

Case Study In Figure 6, we present some generated sheet music given lyrics to demonstrate the advantages of our CSL-L2M in terms of generation quality and controllability. Specifically, Figure 6a shows that the generated melodies not only harmonize with given lyrics but also exhibit a coherent and distinguishable verse-chorus structure, along with repetition patterns matching lyrics. Besides, it is observed that our model can well model the ”one-to-many” alignment relationship between lyrics and melodies. In Figure 6b, the generate melodies well adhere to user-specified musical attributes. Figure 6c presents high-fidelity style transfer results of CSL-L2M equipped with the learned mu

nPng 瓶身描绘的牡丹一如你初妆  
m搁半  
:rrr   
P 天青色等烟雨而我在等你. 如 你眼带笑意  
One-to-many alignment Verse Chorus Repetition1 Repetition2

(a) High-quality and well-structured generation.

m Number of Notes2 月  
.？ 一 二 H 二 1   
我明白我 给不了你 想要的爱， 也知道我们的爱不能再重来(b) Strong controllability.  
Pitch Mean  
m  
阿里山的姑娘 美如水呀．阿里山的少 年壮如山(c) High-fidelity controllability.

sical features, confirming that the learned features provide high-fidelity melody information to the Transformer decoder. In summary, our proposed CSL-L2M could generate melodies that not only match with the given lyrics but also adhere to user-specified musical attributes.

# Conclusion

To address weak controllability, low-quality and poorly structured generation issues in the lyric-to-melody generation task, we propose CSL-L2M in this paper towards controllable song-level melody generation conditioning on lyrics and user-specified musical attributes. We first introduce a novel music representation named REMI-Aligned to facilitate precise lyric-melody alignment relationship modeling. Then multiple multi-granularity lyric and musical attribute controls are extracted and fed into the conditional Transformer decoder through in-attention to achieve firm control over the generation process. Experiments demonstrate that our proposed CSL-L2M outperforms the state-ofthe-art models in terms of generation quality and controllability. We believe our contributions will further advance the under-explored field of lyric-to-melody generation.