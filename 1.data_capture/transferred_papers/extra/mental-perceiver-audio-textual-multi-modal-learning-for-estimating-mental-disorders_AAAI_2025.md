# Mental-Perceiver: Audio-Textual Multi-Modal Learning for Estimating Mental Disorders

Jinghui $\mathbf { Q } \mathbf { i n } ^ { 1 }$ , Changsong $\mathbf { L i u } ^ { 2 , 3 }$ , Tianchi Tang2, Dahuang Liu2, Minghao Wang2, Qianying Huang2, Rumin Zhang2,4\*

1Guangdong University of Technology 2Guangdong Shuye Intelligent Technology Co., Ltd. 3University of Toronto 4Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China {scape1989, liucs.oise}@gmail.com, {tangtc, liudh, wangmh, huangqy}@gdshuyeit.com, rzhang01@idt.eitech.edu.cn

# Abstract

Mental disorders, such as anxiety and depression, have become a global concern that affects people of all ages. Early detection and treatment are crucial to mitigate the negative effects these disorders can have on daily life. Although AIbased detection methods show promise, progress is hindered by the lack of publicly available large-scale datasets. To address this, we introduce the Multi-Modal Psychological assessment corpus (MMPsy), a large-scale dataset containing audio recordings and transcripts from Mandarin-speaking adolescents undergoing automated anxiety/depression assessment interviews. MMPsy also includes self-reported anxiety/depression evaluations using standardized psychological questionnaires. Leveraging this dataset, we propose Mental-Perceiver, a deep learning model for estimating mental disorders from audio and textual data. Extensive experiments on MMPsy and the DAIC-WOZ dataset demonstrate the effectiveness of Mental-Perceiver in anxiety and depression detection.

Datasets — https://github.com/shuyeit/mmpsy-data

# Introduction

Anxiety and depression are prevalent mental disorders that can have a significant and detrimental impact on an individual’s life. If left untreated, these disorders can disrupt work, study, and social interactions. Symptoms include persistent negative emotions, behavioral changes, and physiological disturbances (Cowen, Harrison, and Burns 2012). However, misdiagnosis is common due to the overlap of symptoms with other physical and physiological conditions. The World Health Organization (WHO) 2019 report (de la Sant´e 2019) indicates that more than 301 million people, including 58 million children and adolescents, suffer from anxiety disorders. In addition, more than 280 million people, including 23 million children and adolescents, are affected by depression. Despite the prevalence of these conditions, the treatment rate for both anxiety and depression remains notably low due to the substantial time and financial commitments required for diagnosis and treatment (Kessler 2012). Furthermore, individuals, particularly children and adolescents, can conceal their true mental state during assessments and interviews, preventing an accurate diagnosis.

To address this challenging issue, automatic detection of anxiety and depression offers a potential solution. The development of a suitable psychological assessment corpus related to anxiety and depression and the construction of an automated anxiety and depression detection system are essential. This will allow individuals to assess their anxious or depressive states privately and increase their willingness to consult with psychologists after conducting self-assessments. Furthermore, such a psychological assessment corpus and automated detection system would greatly assist psychologists in diagnosing anxiety and depression disorders, even when patients conceal their true mental states.

A significant challenge facing current research in automatic anxiety and depression detection is the absence of extensive datasets to train and evaluate machine learning models. Existing methodologies frequently depend on the judgment of expert psychologists (e.g., during clinical interviews) for data collection, making the creation of large-scale annotated datasets costly. Publicly accessible datasets, such as DAIC-WOZ (Gratch et al. 2014) and AViD-Corpus (Valstar et al. 2014), are restricted in scale and scope. Although initiatives like EATD-Corpus (Shen, Yang, and Lin 2022) have been undertaken using self-reported mental health questionnaires, challenges persist regarding data size and validation.

In this work, our goal is to address the challenges in facilitating automatic detection of anxiety and depression and to extend the research boundaries to a more real world setting. First, we introduce a large-scale Multi-Modal Psychological assessment corpus (MMPsy) about anxiety and depression in Mandarin-speaking adolescents. The MMPsy comprises audio and extracted transcripts of responses from anxious/depressed and non-anxious/non-depressed adolescent volunteers. After data pre-processing and cleaning, 7,736 cleaned interview data are available for anxiety detection and 4,247 for depression detection. To our knowledge, MMPsy is the first publicly available adolescent psychological assessment corpus capable of simultaneously detecting anxiety and depression, and containing both audio and text data in Chinese.

Subsequently, we propose a deep learning model, termed Mental-Perceiver, to automatically detect anxious/depressive mental states based on users’ audio and corresponding transcripts. Mental-Perceiver first maps multi-modal inputs and category semantic priors to a fixed-size feature space with learnable embeddings for multi-modal fusion. It then employs a fully attentional network (Jaegle et al. 2021) to further process the fused multimodal features and decode the underlying mental state with a learnable query array. Extensive experiments on our MMPsy and DAIC-WOZ datasets demonstrate the effectiveness and superiority of our proposed Mental-Perceiver model for the estimation of anxiety and depression. The key contributions of this work are summarized as follows:

• We introduce MMPsy, a large-scale multi-modal corpus for mental health assessment.   
• We propose Mental-Perceiver, a fully attentional network for automated anxiety/depression detection from audio and transcripts.   
• Extensive experiments on the MMPsy and DAIC-WOZ datasets demonstrate the effectiveness of the MentalPerceiver.

# Related Work

# Anxiety/Depression Detection Datasets

Publicly available datasets for the detection of anxiety and depression remain limited due to the challenges of collecting sensitive mental health data. Existing datasets, such as DAIC-WOZ (Gratch et al. 2014) and AViD-Corpus (Valstar et al. 2014), focus primarily on depression and often lack textual transcripts or sufficient sample sizes. The MODMA dataset (Cai et al. 2022), while incorporating EEG signals along with audio and video recordings, presents challenges for real-world application due to the difficulty in collecting EEG data in regular scenarios and its limited sample size. Recent efforts like EATD-Corpus (Shen, Yang, and Lin 2022) have attempted to address these limitations by incorporating self-reported measures and textual data, but are constrained by a relatively small sample size. This scarcity of diverse and large-scale datasets hinders the development and evaluation of robust machine learning models for anxiety and depression detection, particularly in multilingual and cross-cultural contexts.

# Automatic Anxiety/Depression Detection

Early research in automatic anxiety/depression detection focused on extracting relevant features from interview responses. Williamson et al. (Williamson et al. 2016) utilized semantic cues from voice, facial action units, and transcribed text within a Gaussian Staircase Model for depression detection. Yang et al. (Yang et al. 2016) employed manual transcript analysis to select depression-related questions and built a decision tree for prediction. Similarly, Sun et al. (Sun et al. 2017) used content analysis to select text features from interview transcripts and applied Random Forest to detect depression tendency. Gong et al. (Gong and Poellabauer 2017) utilized topic modeling for feature selection, while Giannakakis et al. (Giannakakis et al. 2017) investigated the correlations between facial cues and perceived stress/anxiety.

Table 1: Demographic Information of Data Collection Participants   

<html><body><table><tr><td>Characteristic</td><td>AnxietySubset</td><td>Depression Subset</td></tr><tr><td>Gender</td><td></td><td>4,087 (52.68%)</td></tr><tr><td>Males Females</td><td>2,262 (53.02%) 2.004 (46.98%)</td><td>3,671 (47.32%)</td></tr><tr><td>Grade Level</td><td></td><td></td></tr><tr><td>Grade 4</td><td>335</td><td>-</td></tr><tr><td>Grade 5</td><td>987</td><td>-</td></tr><tr><td>Grade 6</td><td>1,342</td><td>-</td></tr><tr><td>Grade 7</td><td>1,646</td><td>1,564</td></tr><tr><td>Grade 8</td><td>1,554</td><td>1,397</td></tr><tr><td>Grade 9</td><td>1,579</td><td>1,305</td></tr><tr><td>Grades 10-12</td><td>315</td><td></td></tr></table></body></html>

Deep learning advancements have enabled more effective multi-modal feature extraction and integration. Ma et al. (Ma et al. 2016) used CNN-LSTM models for depressive audio encoding. Yang et al. (Yang et al. 2017) used deep CNNs with audiovisual descriptors for the detection of depression. Tuka et al. (Al Hanai, Ghassemi, and Glass 2018) leveraged Pearson coefficients to select relevant audio and text characteristics for the evaluation of depression based on LSTM. Haque et al. (Haque et al. 2018) proposed a causal CNN to generate embeddings from acoustic, visual, and linguistic features for the prediction of depression. Shen et al. (Shen, Yang, and Lin 2022) and Lin et al. (Lin et al. 2022) focused on speech and linguistic content for depression detection, while Agarwal (Agarwal, Jindal, and Singh 2023) explored machine learning for anxiety diagnosis from audio journals.

Recent years have witnessed significant advances in transformer-based and LLM approaches for the detection of mental disorders. Ji et al. (Ji et al. 2021) introduced domainspecific models, MentalBERT and MentalRoBERTa, which demonstrated superior performance on mental health benchmarks compared to general-purpose LLMs. Xu et al. (Xu et al. 2023) revealed that instruction-tuned smaller LLMs could achieve higher balanced accuracy than larger models in mental health prediction tasks, while Binz and Schulz (Binz and Schulz 2023) enhanced LLMs’ emotional state modeling capabilities by fine-tuning with psychological experiment data. In parallel, Qi et al. (Qi et al. 2023) achieved notable improvements in cognitive distortion and suicide risk detection using fine-tuned GPT-3.5 models. The field has increasingly embraced multi-modal approaches, as evidenced by Ahmed et al. (Ahmed et al. 2023), who developed a comprehensive framework that incorporates video, audio, text and EEG data with uncertainty management capabilities. Building on this trend, Ding et al. (Ding et al. 2024) proposed IntervoxNet, a dual-modal audio-text fusion model using transformer architectures, achieving an impressive F1 score of 0.90 in the detection of depression from interview data.

# MMPsy: A New Benchmark

Data collection was conducted during the mandated annual mental health assessment of elementary and middle school students in Guangdong Province, China, as required by local government regulations1. The research team received approval from schools and their administrative Education Bureaus to provide mental health assessment services and collect data for research purposes. Under institutional supervision, data were collected from over 10,000 primary and secondary school students. All participants provided informed consent with guidance from supervising teachers or social workers. Demographic characteristics of the participants are presented in Table 1.

Table 2: Statistics for our MMPsy dataset. $\times$ means the label is non-anxious or non-depressed and $\checkmark$ denotes the data label is anxious or depressed.   

<html><body><table><tr><td>Parts</td><td>Subset</td><td>×</td><td>√</td><td>Avg Duration (Sec)</td></tr><tr><td rowspan="3">Anxiety</td><td>Train</td><td>5625</td><td>563</td><td>68.05</td></tr><tr><td>Validation</td><td>704</td><td>70</td><td>65.97</td></tr><tr><td>Test</td><td>703</td><td>71</td><td>68.67</td></tr><tr><td rowspan="3">Depression</td><td>Train</td><td>2715</td><td>682</td><td>59.05</td></tr><tr><td>Validation</td><td>340</td><td>85</td><td>55.27</td></tr><tr><td>Test</td><td>339</td><td>86</td><td>61.76</td></tr></table></body></html>

Participants were asked to provide spontaneous responses to 10 specifically designed questions addressing anxiety and depression. Additionally, they completed either the GAD7 (Mossman et al. 2017) or PHQ-9 (Kroenke, Spitzer, and Williams 2001) questionnaire for respective disorder detection. The GAD-7, comprising seven items measuring anxiety severity, is a standardized screening tool widely used in clinical practice. The PHQ-9, consisting of nine items measuring depression severity, serves a similar function for depression screening. For Chinese populations, scores $\geq 1 0$ on either scale indicate the presence of the respective disorder. Based on these criteria, the anxiety subset of MMPsy includes 704 anxious and 7,032 non-anxious participants, while the depression subset comprises 853 depressed and 3,394 non-depressed participants.

The construction of MMPsy comprised two primary phases: data collection and preprocessing.

• Data collection. A custom web application was developed to conduct interviews and collect audio responses and questionnaire data. The application administered 10 questions along with the GAD-7 or PHQ-9 questionnaires. The audio responses were automatically recorded and uploaded to the server along with the questionnaire results. This phase yielded 17,247 raw interviews for anxiety and 11,306 for depression. To ensure the validity of the responses, additional control questions were embedded within the questionnaires to assess the reliability of the responses.

• Data preprocessing. The raw interview data underwent several preprocessing steps. Initially, responses were filtered based on the control questions to eliminate inauthentic entries. Subsequently, mute recordings and those shorter than 1 second were removed, and silent segments were eliminated using voice activity detection. Background noise was then removed using Spleeter (Hennequin et al. 2020). Textual transcripts were extracted using Paraformer (Gao et al. 2022) and manually verified through audio comparison to ensure semantic consistency at the word level.

• Data anonymization and reporting. To protect participant privacy, all sensitive information and any data that could potentially identify participants were meticulously removed during the preprocessing stage. Anonymization was carried out in compliance with ethical guidelines and data protection regulations. Additionally, for students whose responses on the GAD-7 or PHQ-9 scales indicated potential anxiety or depression problems, their data was securely reported to the designated school administrative offices, as required by institutional and regulatory policies, ensuring appropriate follow-up support.

The preprocessing phase yielded 7,736 cleaned interviews for anxiety detection and 4,247 for depression detection. These data were randomly partitioned into training, validation, and test sets using an 8:1:1 ratio. Consequently, the anxiety detection subset comprises 6,188 training, 774 validation, and 774 test participants, while the depression detection subset contains 3,397 training, 425 validation, and 425 test participants. The complete data statistics are presented in Table 2. Following the methodology of (Wei et al. 2022), each participant’s audio and transcript data were sequentially organized and segmented into 60-second intervals with 10-second overlaps, generating multiple samples per participant.

# Mental-Perceiver

This section describes the architecture of our proposed Mental-Perceiver as shown in Figure 1. We first encode by applying an attention module that maps multimodal input $\boldsymbol { x } \in \mathbf { \mathbb { R } } ^ { \boldsymbol { \breve { M } } \times \boldsymbol { D _ { x } } }$ to features in a latent space $\boldsymbol { z } \in \mathbb { R } ^ { 2 \times D _ { z } }$ by interacting with a category prior $p \in \mathbf { \mathbb { R } ^ { 2 \times D _ { p } } }$ which is obtained by computing the center point of different text representations from different categories. Then, we perform deep feature extraction on latent features $z$ by applying a series of attention modules that take in and return features $z ^ { \prime }$ in this latent feature space. Finally, we decode by applying an attention module that maps latent arrays $z ^ { \prime }$ and the query array $q \in \mathbb { R } ^ { 2 \times D _ { q } }$ to the final feature representation $\boldsymbol { y } ^ { \mathbf { \bar { \tau } } } \in \dot { \mathbb { R } } ^ { 2 \times D _ { y } ^ { \mathbf { \bar { \tau } } } }$ Based on the final feature representation $y$ , we apply a linear layer to map the feature $y$ to class-wise logit outputs $y ^ { C _ { 0 } } \in \mathbb { R } ^ { 1 \times 2 }$ and $y ^ { C _ { 1 } } \in \mathbb { R } ^ { 1 \times 2 } . ~ N .$ is the length of multimodal input, while $D _ { x } , D _ { z } , D _ { q } ,$ and $D _ { y }$ denote the feature dimension. With class-wise logits $y ^ { C _ { 0 } }$ and $y ^ { C _ { 1 } }$ , to predict the class $c ^ { \prime }$ of the input $x$ , we compute the mean values of corresponding vector elements in the $y ^ { C _ { 0 } }$ and $y ^ { C _ { 1 } }$ to obtain final logits $y ^ { \prime }$ followed by a Softmax function.

# Basic Attention module

Following the pioneering work PerceiverIO (Jaegle et al. 2021), all attention modules deployed in our MentalPerceiver are implemented as Transformer-style attention (Vaswani et al. 2017). Each attention module applies a global query-key-value (QKV) attention operation followed

System:Have   
you felt anxious or Multimodal Attention V   
iVsoslturenstsefeurl:, uh Studying Input 理 K 用 甲 K V K V K V Audio Recording Category Prior □□Q 日Q Attention Q Attention 川 Q Attention Scores 田 □□ 一 K V Mean Anxious/Depressed Anxious/Depressed Prior Encode Deep Feature Extraction Attention □ Q Normal Prior Category Query Weight Decode Mean Normal Tying

by a multi-layer perceptron (MLP). The MLP is applied independently to each element of the index dimension. Both the encoder and decoder take in two input feature arrays. The first array is used as input to the attention module’s key and value networks, and another array is used as input to the module’s query network to interact and fuse with the first array. The output of the attention module has the same index dimension (the same number of elements) as the query input. Therefore, the attention module can be modeled as follows:

$$
\mathrm { a t t e n t i o n } ( Q , K , V ) = M L P ( S o f t m a x \left( { \frac { Q K ^ { T } } { \sqrt { d _ { k } } } } \right) V )
$$

# Category Semantic Prior

Our Mental-Perceiver extracts deep features based on the latent features $z$ which is produced by fusing the multimodal input $x$ and the latent embedding $p$ with an attention module. $p$ is used as a query and $x$ is used as the key and value. This means that the output $z$ is the result that $p$ extracts and fuse semantic information from $x$ . So, the initialization of the latent embedding $p$ is crucial for learning more discriminative features in the following steps to identify a user’s mental state according to the user’s audio and transcript text. Semantic prior has been shown to help greatly learn more discriminative features (Teney, Abbasnejad, and Hengel 2019; Dai et al. 2023; Ding et al. 2023). Therefore, we build semantic priors for different categories, one for the normal category and another for the mental disorder category. Then, we use these two semantic priors to initialize the latent embedding $p \in \mathbb { R } ^ { 2 \times D }$ with a learnable MLP for mapping the hidden size of semantic priors to $D$ . In Mental-Perceiver, we fixed the parameters of these two semantic priors and only optimized the parameters of the learnable MLP semantic priors. To obtain the semantic priors, we simply compute the center points for different categories.

Formally, given the text representation set $\mathbf { E } _ { t } \in \mathbb { R } ^ { N \times H }$ of a category on the training set, where $N$ is the number of data samples for the current category and $H$ is the hidden size of a text representation, we can compute the semantic prior pCi ∈ R1×H by averaging the normalized Et at the index dimension. This procedure can be modeled as follows:

$$
p ^ { C _ { i } } = A v g ( N o r m ( \mathbf { E } _ { t } ) )
$$

where $A v g$ is the averaging function and N orm is the z-score normalization function that normalizes each vector separately in $\mathbf { E } _ { t }$ . Let $C _ { 0 }$ and $C _ { 1 }$ denote the classes of normal people and psychological patients, respectively, we first obtain two semantic priors $p ^ { C _ { 0 } ^ { \mathrm { ~ * ~ } } }$ and $p ^ { C _ { 1 } }$ according to Equation (2). Then, we obtain $p$ by concatenating $p ^ { C _ { 0 } }$ and $p ^ { C _ { 1 } ^ { \bullet } }$ at the index dimension followed by a learnable MLP layer as follows:

$$
p = M L P ( p ^ { C _ { 0 } } \odot p ^ { C _ { 1 } } )
$$

where $\odot$ denotes the concatenation.

# Encoder

The encoder, consisting of an attention module, takes charge of mapping the multimodal input $x$ into latent features $z$ by utilizing category semantic prior-enhanced latent embedding $p$ to query the multimodal input $x$ . This way will fuse the multimodal input $x$ and category semantic prior-enhanced latent embedding $p$ and build fused prior-guided latent features $z$ , which will be used as input to the next deep feature extraction. The encoder can be modeled as follows:

$$
z = a t t e n t i o n ( p , x , x )
$$

# Deep Feature Extraction

Once we obtain the latent features $z$ , we conduct deep feature extraction based on the input latents $z$ by applying a series of attention modules that take in and return latents $z _ { i }$ in this latent space iteratively. This module can be modeled as follows:

$$
\begin{array} { l } { z _ { 1 } = a t t e n t i o n ( z , z , z ) } \\ { z _ { 2 } = a t t e n t i o n ( z _ { 1 } , z _ { 1 } , z _ { 1 } ) } \\ { . . . } \\ { z _ { k } = a t t e n t i o n ( z _ { k - 1 } , z _ { k - 1 } , z _ { k - 1 } ) } \end{array}
$$

where $k$ is a hyper-parameter that is simply set to 8.

# Decoder

The goal of the decoder is to produce a final class-wise logit output of size $2 \times 2$ , given a latent representation of size $2 \times D _ { z }$ . Let $z ^ { \prime } = z _ { k }$ , the decoder first applies an attention module to map the latent $z ^ { \prime }$ to output features $y$ . Then, the decoder applies a linear layer to map $y$ into the final classwise logit output $[ y ^ { C _ { 0 } } , y ^ { C _ { 1 } } ] \in \mathbb { R } ^ { 2 \times 2 }$ , where $y ^ { C _ { 0 } } \in \mathbb { R } ^ { 1 \times 2 }$ and $\boldsymbol { y } ^ { C _ { 1 } ^ { \mathsf { \backprime } } } \in \mathbb { R } ^ { 1 \times 2 }$ are two logit outputs for indicating whether the multimodal input $x$ matches with semantic prior $p ^ { C _ { 0 } }$ and $p ^ { C _ { 1 } }$ . Finally, we compute the mean vector of these two output logit vectors $y ^ { C _ { 0 } }$ and $y ^ { C _ { 1 } }$ at the index dimension as the final classification logits $y ^ { \prime }$ . Therefore, the decoder can be modeled as follows:

$$
\begin{array} { c } { y = a t t e n t i o n ( q , z ^ { \prime } , z ^ { \prime } ) } \\ { { \ } } \\ { { [ y ^ { C _ { 0 } } , y ^ { C _ { 1 } } ] = L i n e a r ( y ) } } \\ { { y ^ { \prime } = M e a n ( y ^ { C _ { 0 } } \odot y ^ { C _ { 1 } } ) } } \end{array}
$$

# Training Objectives

To optimize our Mental-Perceiver, we deploy two losses. The first one is the matching loss $\mathcal { L } _ { m a t c h }$ and another is the classification loss $\mathcal { L } _ { c l s }$ . Both these two losses are binary cross-entropy loss functions. The matching loss aims to optimize the matching degree between the multimodal input $x$ and its corresponding category semantic prior while the classification loss optimizes the model to be able to identify the inherent mental state according to multimodal input $x$ with the help of category prior $p$ and category query $q$ .

Formally, given a multimodal input $x$ and its class $C _ { x }$ which can be 0 or 1, the training objectives can be modeled as follows:

$$
\begin{array} { c } { { \mathcal { L } _ { m a t c h } = - C _ { x } ( l o g y _ { 0 } ^ { C _ { 0 } } + l o g y _ { 1 } ^ { C _ { 1 } } ) } } \\ { { - ( 1 - C _ { x } ) ( l o g y _ { 1 } ^ { C _ { 0 } } + l o g y _ { 0 } ^ { C _ { 1 } } ) } } \\ { { \mathcal { L } _ { c l s } = - C _ { x } l o g y _ { 0 } ^ { \prime } - ( 1 - C _ { x } ) l o g y _ { 1 } ^ { \prime } } } \\ { { \mathcal { L } = \mathcal { L } _ { m a t c h } + \mathcal { L } _ { c l s } } } \end{array}
$$

where $y _ { 0 } ^ { C _ { 0 } }$ and $y _ { 1 } ^ { C _ { 0 } }$ are the 0-th element and 1-th element in the probability distribution obtained from class-wise logits $y ^ { C _ { 0 } } \in \mathbb { R } ^ { 1 \times 2 }$ by Softmax function while $y _ { 0 } ^ { C _ { 1 } }$ and $y _ { 1 } ^ { C _ { 1 } }$ are the 0-th element and 1-th element in the probability distribution obtained from class-wise logits $\boldsymbol { y } ^ { C _ { 1 } ^ { \perp } } \in \mathbb { R } ^ { 1 \times 2 ^ { \ J } }$ by Softmax function. Similarly, $y _ { 0 } ^ { \prime }$ and $y _ { 1 } ^ { \prime }$ are the two probabilities on normal class (0) and mental disorder class (1) that can be obtained by applying the Softmax function on $y ^ { \prime }$ .

# Experiments

# Datasets

We conduct experiments on both two subsets of MMPsy for anxiety detection and depression detection. We use MMPsyAnxiety and MMPsy-Depression to represent these two subsets. Besides, we also verify our Mental-Perceiver on the Distress Analysis Interview Corpus - Wizard of $\mathrm { O z }$ (DAICWOZ) dataset (Gratch et al. 2014). DAIC-WOZ contains clinical interviews of 189 participants designed to support the diagnosis of psychological distress conditions such as anxiety, depression, and posttraumatic stress disorder (PTSD). During each interview, several data in different formats as well as modalities are recorded simultaneously. However, only the acoustic recordings and transcriptions are chosen in this work for a fair comparison. Moreover, the given GT is an eight-item Patient Health Questionnaire depression scale (PHQ-8), which indicates the severity of depression. A PHQ8 Score $\geq 1 0$ implies that the participant is undergoing a mental disorder.

# Baselines

The main baselines to be compared are listed as follows:

• SVM (Pedregosa et al. 2011): a robust shallow model capable of performing binary classification efficiently by using a kernel trick, which transforms original datapoints into coordinates in the higher dimensional feature space.   
• RandomForest (Pedregosa et al. 2011): it is a robust ensemble learning method for classification by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees.   
• XGBoost (Chen and Guestrin 2016): it is a robust toolbox for classification via an optimized distributed gradient boosting.   
• NUSD (Wang, Ravi, and Alwan 2023): it is a deep model ECAPA-TDNN enhanced with a speaker disentanglement method that utilizes a non-uniform mechanism of adversarial SID loss maximization.   
• ConvLSTM (Wei et al. 2022): it is a Convolutional Bidirectional LSTM with a sub-attention mechanism for linking heterogeneous information.   
• PerceiverIO (Jaegle et al. 2021): it is a general-purpose architecture that handles multimodal data with fully attention design and a flexible querying mechanism.   
• AFABNet (Xu et al. 2024): it is an attention-based acoustic feature fusion network for depression detection by combining four different acoustic features.   
• Qwen2-Audio-Instruct (Chu et al. 2024): it is a Qwen large audio-language model (7B) that can accept various audio signal inputs and perform audio analysis or direct textual responses with regard to speech instructions.

For the shallow models SVM, RandomForest, and XGBoost, we provide the following audio features as input: F0 statistics (mean), log-energy, zero-crossing-rate, loudness, pitch period entropy, jitters, shimmers, harmonics-to-noise ratio, detrended fluctuation analysis, linear spectral coefficients-0, linear spectral frequencies-0, formants (F1), and amplitude Shannon entropy. All these features can be extracted by applying Surfboard 2 (Lenain et al. 2020). Besides, we also extract topic words by TF-IDF as text features for these shallow models. For the deep models, we use BERT (Devlin et al. 2019) to extract features and use Mel-spectrum to represent audio.

# Metrics

In classification tasks, True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) from the confusion matrix are metrics used to measure the accuracy of model predictions. TP refers to instances where the model correctly predicted them as the positive class, while TN refers to instances where the model correctly predicted them as the negative class. On the contrary, FP represents cases where the model incorrectly labeled instances as positive, and FN denotes instances of the positive class that were incorrectly classified as negative. Based on these concepts, several common performance metrics are adopted to assess the model’s performance:

<html><body><table><tr><td>Datasets</td><td>一 Methods</td><td></td><td>Acc UAR</td><td>Sens</td><td>Spec</td><td>Precision</td><td>Recall</td><td>F1</td><td>Precision</td><td>Recall</td><td>F1</td></tr><tr><td></td><td>Anxious</td><td colspan="8">Normal</td></tr><tr><td rowspan="8">MMPsy-Anxiety</td><td>SVM</td><td>0.83</td><td>0.64</td><td>0.41</td><td>0.87 0.87</td><td>0.94 0.93</td><td>0.87 0.87</td><td>0.9 0.9</td><td>0.24</td><td>0.41</td><td>0.3</td></tr><tr><td>RandomForest XGBoost</td><td>0.82 0.88</td><td>0.63</td><td>0.39 0.25</td><td>0.94</td><td>0.93</td><td>0.94</td><td>0.93</td><td>0.23 0.3</td><td>0.39 0.25</td><td>0.29</td></tr><tr><td>NUSD</td><td>0.79</td><td>0.6</td><td>0.17</td><td>0.85</td><td>0.91</td><td>0.85</td><td>0.88</td><td>0.1</td><td>0.17</td><td>0.27 0.13</td></tr><tr><td></td><td>0.83</td><td>0.51</td><td>0.21</td><td>0.9</td><td>0.92</td><td>0.9</td><td>0.91</td><td>0.18</td><td></td><td></td></tr><tr><td>ConvLSTM</td><td>0.83</td><td>0.55</td><td>0.63</td><td>0.84</td><td>0.95</td><td>0.84</td><td>0.90</td><td></td><td>0.21</td><td>0.19</td></tr><tr><td>PerceiverIO AFABNet</td><td>0.82</td><td>0.74</td><td>0.41</td><td>0.86</td><td>0.93</td><td>0.86</td><td></td><td>0.29</td><td>0.63</td><td>0.40</td></tr><tr><td>Qwen2-Audio-Instruct</td><td>0.72</td><td>0.63 0.76</td><td>0.80</td><td>0.71</td><td>0.97</td><td>0.71</td><td>0.89 0.82</td><td>0.22 0.22</td><td>0.41</td><td>0.29</td></tr><tr><td>Mental-Perceiver (Ours)</td><td>0.85</td><td>0.76</td><td>0.65</td><td>0.87</td><td>0.96</td><td>0.87</td><td>0.92</td><td>0.34</td><td>0.80 0.65</td><td>0.34 0.45</td></tr><tr><td rowspan="7"></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Normal</td><td></td><td></td><td>Depressed</td><td></td></tr><tr><td>SVM</td><td>0.76</td><td>0.71</td><td>0.63</td><td>0.79</td><td>0.89</td><td>0.79</td><td>0.84</td><td>0.43</td><td>0.63</td><td>0.51</td></tr><tr><td>RandomForest MMPsy-Depression</td><td></td><td>0.76 0.63</td><td>0.41</td><td>0.85</td><td>0.85</td><td>0.85</td><td>0.85</td><td>0.41</td><td>0.41</td><td>0.41</td></tr><tr><td> XGBsDst</td><td></td><td></td><td></td><td></td><td>0.85</td><td>0.88</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>0.78</td><td>0.64</td><td>0.46</td><td>0.88</td><td></td><td></td><td>0.7</td><td>0.46</td><td>0.46</td><td>0.4</td></tr><tr><td>ConvLSTM</td><td>0.77</td><td>0.53</td><td>0.14</td><td>0.93</td><td>0.81</td><td>0.93</td><td>0.86</td><td>0.32</td><td>0.14</td><td>0.2</td></tr><tr><td>PerceiverIO</td><td>0.81</td><td>0.77</td><td>0.66</td><td>0.85</td><td>0.91</td><td>0.85</td><td>0.88</td><td>0.53</td><td>0.66</td><td>0.59</td></tr><tr><td rowspan="5"></td><td>AFABNet</td><td>0.78</td><td>0.58</td><td>0.24</td><td>0.92</td><td>0.83</td><td>0.92</td><td>0.87</td><td>0.44</td><td>0.24</td><td>0.31</td></tr><tr><td>Qwen2-Audio-Instruct</td><td>0.79</td><td>0.58</td><td>0.21</td><td>0.94</td><td>0.82</td><td>0.94</td><td>0.88</td><td>0.45</td><td>0.21</td><td>0.29</td></tr><tr><td>Mental-Perceiver(Ours)</td><td>0.85</td><td>0.79</td><td>0.69</td><td>0.89</td><td>0.92</td><td>0.89</td><td>0.9</td><td>0.61</td><td>0.69</td><td>0.64</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>Normal</td><td></td><td></td><td>Depressed</td><td></td></tr><tr><td>SVM</td><td>0.38</td><td>0.35</td><td>0.29</td><td>0.42</td><td>0.58</td><td>0.42</td><td>0.49</td><td>0.17</td><td>0.29</td><td></td></tr><tr><td rowspan="8">DAIC-WOZ</td><td>RandomForest</td><td>0.7</td><td>0.54</td><td>0.14</td><td>0.94</td><td>0.72</td><td>0.94</td><td>0.82</td><td>0.5</td><td>0.14</td><td>0.22 0.22</td></tr><tr><td>XGBo0st</td><td>0.6</td><td>0.53</td><td>0.36</td><td>0.7</td><td>0.72</td><td>0.7</td><td>0.71</td><td>0.33</td><td>0.36</td><td></td></tr><tr><td>NUSD</td><td>0.55</td><td>0.46</td><td>0.21</td><td>0.7</td><td>0.68</td><td>0.7</td><td>0.69</td><td>0.23</td><td>0.21</td><td>0.34 0.22</td></tr><tr><td>ConvLSTM</td><td>0.4</td><td>0.57</td><td>0.88</td><td>0.22</td><td>0.83</td><td>0.22</td><td>0.35</td><td>0.3</td><td>0.88</td><td>0.45</td></tr><tr><td>PerceiverIO</td><td>0.7</td><td>0.58</td><td>0.29</td><td>0.88</td><td>0.74</td><td>0.88</td><td>0.81</td><td>0.5</td><td>0.29</td><td>0.36</td></tr><tr><td>AFABNet</td><td>0.74</td><td>0.63</td><td>0.36</td><td>0.91</td><td>0.77</td><td>0.91</td><td>0.83</td><td>0.62</td><td>0.36</td><td>0.45</td></tr><tr><td>Qwen2-Audio-Instruct</td><td>0.70</td><td>0.65</td><td>0.52</td><td>0.78</td><td>0.79</td><td>0.78</td><td>0.79</td><td>0.50</td><td>0.52</td><td>0.51</td></tr><tr><td>Mental-Perceiver (Ours)</td><td>0.79</td><td>0.66</td><td>0.36</td><td>0.97</td><td>0.78</td><td>0.97</td><td>0.86</td><td>0.83</td><td>0.36</td><td>0.5</td></tr></table></body></html>

Table 3: Performance Comparison with our Mental-Perceiver and various baselines on MMPsy and DAIC-WOZ. The best result is highlighted in bold.

• Accuracy: Accuracy (Acc) represents the proportion of all predictions that are correctly predicted. • Recall: Recall measures the model’s ability to identify all true positive instances, that is, the proportion of actual positives that are correctly identified. • Precision: Precision denotes the proportion of samples predicted as positive by the model that are actually positive, focusing on the accuracy of positive predictions. • F1-Score: F1 Score ranges from 1 to 0, with a value closer to 1 indicating better model performance, particularly in scenarios where the distribution of positive and negative samples is imbalanced. The F1 Score serves as a more comprehensive evaluation metric under such conditions. • Sensitivity: Sensitivity (Sens), also termed true positive rate, is the ratio of positive predictions to the number of actual positives. It is hence identical to the recall of the positive class, Recall(1). • Specificity: Specificity (Spec) is the ratio of negative predictions to the number of actual negatives, and therefore

identical to Recall(0).

• UAR: Unlike accuracy, which is biased by data imbalance, Unweighted Average Recall (UAR) is preferred in fields with imbalanced datasets, such as biomedical and paralinguistics. UAR, i.e., the average of Sensitivity and Specificity, provides a more balanced performance measure.

During these metrics, we use Accuracy (Acc), Unweighted Average Recall (UAR), Sensitivity (Sens), and Specificity (Spec) as the main evaluation metrics for evaluating overall model performance. Meanwhile, we also report the Recall, Precision, and F1-score separately for different categories.

# Implementation Details

We use Pytorch3 to implement our framework on Linux with two NVIDIA RTX 4090 GPU cards. The feature dimension $D _ { x }$ is set to 768 and other dimensions $D _ { z }$ , $D _ { q }$ , and $D _ { y }$ are all set to 512. In each epoch, all training data is shuffled randomly and then cut into mini-batches. The text feature and audio feature are concatenated as the multimodal input. We deploy the AdamW (Loshchilov and Hutter 2017) optimizer for model optimization. We trained models for 200 epochs with an initial learning rate of 0.00003 and used LambdaLR to adjust the learning rate during training. The early stopping with patience 15 is deployed to accelerate training. We use the validation set for model selection and report the performance on the test set.

<html><body><table><tr><td>Datasets</td><td>Modalities</td><td>Acc</td><td>UAR</td><td>Sens</td><td>Spec</td><td>Precision</td><td>Recall</td><td>F1</td><td>Precision</td><td>Recall</td><td>F1</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td colspan="5">Normal</td><td colspan="2">Anxious</td></tr><tr><td rowspan="3">MMPsy-Anxiety</td><td>Audio</td><td>0.87</td><td>0.52</td><td>0.10</td><td>0.95</td><td>0.91</td><td>0.95</td><td>0.93</td><td>0.16</td><td>0.10</td><td>0.12</td></tr><tr><td>Text</td><td>0.85</td><td>0.74</td><td>0.61</td><td>0.88</td><td>0.96</td><td>0.88</td><td>0.92</td><td>0.34</td><td>0.61</td><td>0.43</td></tr><tr><td>Text+Audio</td><td>0.85</td><td>0.76</td><td>0.65</td><td>0.87</td><td>0.96</td><td>0.87</td><td>0.92</td><td>0.34</td><td>0.65</td><td>0.45</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Normal</td><td></td><td></td><td>Depressed</td><td></td></tr><tr><td rowspan="3">MMPsy-Depression</td><td>Audio</td><td>0.76</td><td>0.61</td><td>0.35</td><td>0.86</td><td>0.84</td><td>0.86</td><td>0.85</td><td>0.39</td><td>0.35</td><td>0.37</td></tr><tr><td>Text</td><td>0.81</td><td>0.74</td><td>0.62</td><td>0.86</td><td>0.90</td><td>0.86</td><td>0.88</td><td>0.53</td><td>0.62</td><td>0.57</td></tr><tr><td>Text+Audio</td><td>0.85</td><td>0.79</td><td>0.69</td><td>0.89</td><td>0.92</td><td>0.89</td><td>0.9</td><td>0.61</td><td>0.69</td><td>0.64</td></tr></table></body></html>

Table 4: Ablation study on different modalities of our Mental-Perceiver on MMPsy. The best result is highlighted in bold

# Experiment Results

Main Results The experiment results of our MentalPerceiver and various baselines on MMPsy-Anxiety, MMPsyDepression, and DAIC-WOZ are shown in Table 3. From the results on different datasets, we can draw the following conclusions. 1) On MMPsy-Anxiety, our Mental-Perceiver outperforms baselines on Acc and UAR while achieving competitive performance on Sens and Spec. This shows that our Mental-Perceiver can achieve better overall anxiety detection performance with a better trade-off between Sensitivity and Specificity. For different categories, we can observe that our Mental-Perceiver achieves relatively high performance in the normal category and achieves the best performance in the anxious category. 2) On MMPsy-Depression, our Mental-Perceiver outperforms baselines on Acc, UAR, and Sens while achieving competitive performance on Spec. This shows that our Mental-Perceiver can achieve better overall depression detection performance with a better trade-off between Sensitivity and Specificity. For different categories, we can observe that our Mental-Perceiver achieves the best precision and F1-score in the normal category while outperforming all baselines in the depressed category on all three metrics. 3) On DAIC-WOZ, a similar conclusion can be reached. The Mental-Perceiver outperforms baselines on Acc, UAR, and Spec. Although the Mental-Perceiver’s sensitivity is lower than the baseline ConvLSTM, ConvLSTM is very poor in specificity, indicating that there is a high rate of misdiagnosis on ConvLSTM. According to the UAR, we can observe Mental-Perceiver has a better balance between the false positive rate ( 1 - Specificity) and the false negative rate (1 - Sensitivity). Besides, according to the metrics on different categories, we can observe that the Mental-Perceiver achieves the best overall performance in both two categories.

Overall, our Mental-Perceiver can achieve the best overall performance on different datasets for different mental disorder detection, showing the effectiveness and universality of our Mental-Perceiver for detecting different mental disorders. Besides, different models can achieve varying degrees of performance, indicating the effectiveness and usability of our MMPsy dataset as a benchmark for developing and evaluating mental disorder detection models.

Ablation study on different modalities To verify the superiority of multimodal text-audio for mental disorder detection, we conduct an ablation study by using only audio, only text, and text+aduio as input to the Mental-Perceiver. The experimental results are shown in Table 4. We can observe that the Mental-Perceiver with multimodal inputs can achieve the best performance across various metrics on the MMPsyDepression while achieving the best performance on UAR, Sensitivity, Precision in the normal category, Precision in the anxious category, Recall in the anxious category, and F1 on the anxious category and reasonable and competitive performance on Acc, Specificity, Recall on the normal category, and F1 on the normal category. Overall, multimodal input helps detect mental disorders.

Table 5: Ablation study on the effects of category priors and match loss on MMPsy. The best result is highlighted in bold.   

<html><body><table><tr><td>Datasets</td><td>Modalities</td><td>Acc</td><td>UAR</td><td>Sens</td><td>Spec</td></tr><tr><td rowspan="3">MMPsy-Anxiety</td><td>Mental-Perceiver</td><td>0.85</td><td>0.76</td><td>0.65</td><td>0.87</td></tr><tr><td>-Category Prior</td><td>0.84</td><td>0.72</td><td>0.58</td><td>0.86</td></tr><tr><td>-Matching Loss</td><td>0.82</td><td>0.74</td><td>0.62</td><td>0.86</td></tr><tr><td rowspan="3">MMPsy-Depression</td><td>Mental-Perceiver</td><td>0.85</td><td>0.79</td><td>0.69</td><td>0.89</td></tr><tr><td>-Category Prior</td><td>0.82</td><td>0.68</td><td>0.49</td><td>0.88</td></tr><tr><td>-Matching Loss</td><td>0.8</td><td>0.72</td><td>0.57</td><td>0.86</td></tr></table></body></html>

The effects of category priors and matching loss To investigate the effects of the category priors and matching loss, we conduct a study by removing the category priors and matching loss. The results are shown in Table 5. It can be seen that each component can improve the performance across various metrics, showing the effectiveness of category priors and matching loss.

# Conclusion

In this work, we construct a new large-scale Multi-Modal Psychological assessment corpus (MMPsy) about anxiety and depression in adolescents who speak Mandarin. The MMPsy contains audios and extracted transcripts of responses from anxious/depressed and non-anxious/nondepressed adolescent volunteers. There are 7,736 cleaned interview data for anxiety detection and 4,247 for depression detection. We further propose a novel mental disorder estimation network, named Mental-Perceiver, to detect anxious/depressive mental states automatically according to users’ audio and corresponding transcripts. Extensive experiments on MMPsy and the public DAIC-WOZ show the effectiveness and superiority of our proposed Mental-Perceiver.