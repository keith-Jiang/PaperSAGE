# Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback

Wenyi Xiao1\*, Ziwei Huang1\*, Leilei $\mathbf { G a n } ^ { 1 \dagger }$ , Wanggui $\mathbf { H e } ^ { 2 }$ Haoyuan $\mathbf { L i } ^ { 2 }$ , Zhelun $\mathbf { Y } \mathbf { u } ^ { 2 }$ , Fangxun $\mathbf { S h u } ^ { 2 }$ , Hao Jiang2, Linchao Zhu3

1School of Software Technology, Zhejiang University 2Alibaba Group 3College of Computer Science and Technology, Zhejiang University wenyixiao, leileigan $@$ zju.edu.cn,

# Abstract

The rapidly developing Large Vision Language Models (LVLMs) still face the hallucination phenomena where the generated responses do not align with the given contexts, significantly restricting the usages of LVLMs. Most previous work detects and mitigates hallucination at the coarsegrained level or requires expensive annotation (e.g., labeling by human experts or proprietary models). To address these issues, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a detection model which can perform sentence-level hallucination detection. Then, we propose a detect-then-rewrite pipeline to automatically construct preference dataset for hallucination mitigation training. Furthermore, we propose differentiating the severity of hallucinations, and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSADPO) which prioritizes the mitigation of critical hallucination in LVLMs by incorporating the severity of hallucinations into preference learning. Extensive experiments on hallucination detection and mitigation benchmarks demonstrate that our method sets a new state-of-the-art in hallucination detection on MHaluBench, surpassing GPT-4V and Gemini, and reduces the hallucination rate by $3 6 . 1 \%$ on AMBER and $7 6 . 3 \%$ on Object HalBench compared to the base model.

# Code — https://github.com/Mr-Loevan/HSA-DPO

# 1 Introduction

Large Language Models (LLMs) (OpenAI 2023a; Touvron et al. 2023; OpenAI 2023b; Jiang et al. 2023) have marked a significant milestone in the development of natural language processing and have been further extended to encompass multi-modality data, such as language and vision, leading to the emergence of Large Vision Language Models (LVLMs) (OpenAI 2023c; Liu et al. 2024b; Team et al. 2023; Bai et al. 2023). Despite the remarkable performance of LVLMs across a broad spectrum of visionlanguage tasks (Chen et al. $2 0 2 3 \mathrm { a }$ ; Liu et al. 2023; Zhu et al. 2024; Chen et al. 2023b; Dai et al. 2024; Guan et al. 2024a,b), LVLMs still grapple with the phenomena of hallucination, wherein the completions do not align with the given contexts. In other words, the generated responses contain incorrect objects, attributes and relations concerning the vision and language inputs, thereby significantly restricting the utility of LVLMs (Liu et al. 2024c; Yin et al. 2023a).

![](images/78dee7ad68ad80dc79414bb1f5341a4a0831ed55021a4df55b13d81b8612f41f.jpg)  
Figure 1: Comparison of HSA-DPO (red) with state-ofthe-art models in mitigating hallucinations (Silkie, GPT-4V, RLHF-V) on Object HalBench and AMBER benchmarks. Notably, HSA-DPO outperforms state-of-the-art models in all metrics.

Research on addressing hallucinations in LVLMs can be primarily categorized into hallucination detection and mitigation. Hallucination detection aims to identify the presence of hallucinations in the LVLM outputs for preventing potential malicious usages (Li et al. 2023b; Wang et al. 2023; Rohrbach et al. 2018; Sun et al. 2023; Liu et al. 2024a; Chen et al. 2024). Hallucination mitigation aims to enable LVLMs to generate more faithful responses and can be mainly divided into training-free and training-based approaches (Yin et al. 2023b; Huang et al. 2023; Sun et al. 2023; Zhao et al. 2023; Yu et al. 2024; Gunjal, Yin, and Bas 2024; Zhou et al. 2024a). Training-free approaches address potential hallucinations by post-processing the outputs of LVLMs (Yin et al. 2023b; Huang et al. 2023; Zhou et al. 2024b; Han et al. 2024). While not requiring additional training costs, training-free approaches tend to reduce the inference speed. On the other hand, training-based approaches seek to reduce hallucinations in LVLMs through further instruction finetuning (Liu et al. 2024a; Yue, Zhang, and Jin 2024) or preference learning (Sun et al. 2023; Yu et al. 2024; Li et al. $2 0 2 3 \mathrm { a }$ ; Zhao et al. 2023; Gunjal, Yin, and Bas 2024) on specifically constructed datasets. Some recent studies (Li et al. $2 0 2 3 \mathrm { a }$ ; Zhao et al. 2023; Gunjal, Yin, and Bas 2024) have exploited feedback from powerful closed-source LVLMs for improving the fidelity of LVLMs responses.

However, despite the aforementioned efforts, several challenges persist in detecting and mitigating hallucinations in LVLMs. First, the preference data is generally at responselevel (Sun et al. 2023; Li et al. 2023a; Zhao et al. 2023; Gunjal, Yin, and Bas 2024), which is sub-optimal for thoroughly detecting and mitigating hallucinations. Second, constructing preference data for training-based mitigation approaches requires expensive annotations either by human experts (Sun et al. 2023; Yu et al. 2024) or proprietary commercial models (Li et al. 2023a; Zhao et al. 2023; Gunjal, Yin, and Bas 2024; Chen et al. 2024; Yu et al. 2023), especially if fine-grained annotation is involved. Lastly, existing studies often treat all hallucinations equally, leading to scenarios where less significant hallucinations are addressed, while more critical ones are neglected. For example, in certain scenarios, compared to incorrect color descriptions of objects, addressing the hallucinatory description of non-existent objects should be prioritized.

To address these issues, in this work, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. As shown in Figure 2, our framework consists of three key components: (1) Fine-Grained AI Feedback. The initial step involves generating a small-scale, sentence-level hallucination annotation dataset by proprietary models. Beyond merely detecting hallucinations, we meticulously craft prompts to collect detailed feedback on the type, severity and rationale of each hallucination. Compared to coarse-grained feedback, this sentence-level granularity ensures more precise and thorough hallucination detection. (2) Fine-Grained AI Feedback for Hallucination Detection. The next step proposes training a hallucination detection model using this fine-grained AI feedback, enabling it to perform sentence-level hallucination detection across primary types (e.g., object, attribute, and relationship). This step also introduces an automatic pipeline for constructing preference dataset where given a hallucinatory response, the detection model first identifies hallucinations within each sentence of the response. Based on the detected hallucinations, a rewriting model then revises the hallucinatory response into non-hallucinatory one, forming the <chosen answer, rejected answer $>$ pair. This pipeline enables us to more cost-effectively annotate a large-scale preference dataset for training mitigation models. The underlying insight behind this approach aligns with the concept of scalable oversight which aims to train machines to assist humans in accurately evaluating model output (Bai et al. 2022; Lee et al. 2023; Ganguli et al. 2023; McAleese et al. 2024). (3) Hallucination Severity-Aware DPO. Lastly, we propose differentiating the severity of hallucinations, and introduce a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO). HSA-DPO incorporates hallucination severity into preference learning for prioritizing the mitigation of critical hallucinations.

We conduct extensive experiments on a range of hallucination detection and mitigation benchmarks and the experimental results have demonstrated the effect of the proposed method. For hallucination detection, our detection model achieves new state-of-the-art results on MHaluBench, surpassing GPT-4V and Gemini. As shown in Figure 1, for hallucination mitigation, HSA-DPO improves the base LVLM by reducing the Hallucination Rate on AMBER by $3 6 . 1 \%$ and $\mathrm { C H A I R } _ { S }$ on Object HalBench by $7 6 . 3 \%$ . These results demonstrate the effectiveness of fine-grained AI feedback and the proposed HSA-DPO.

# 2 Related Work

In this section, we give a brief introduction of related studies on LVLMs hallucination detection and mitigation.

# 2.1 Detecting Hallucination in LVLMs

Current approaches for hallucination detection mainly focus on utilizing the abilities of off-the-shelf tools, such as closed-source LLMs, LVLMs or visual tools. GAVIE (Liu et al. 2024a) employs GPT-4 to facilitate the evaluation of object hallucinations. Zhao et al. (2023) introduce the sentence-level hallucination metric SHR, which harnesses GPT-4 to determine the presence of hallucinations in LVLM outputs. UNIHD leverages GPT-4V (OpenAI 2023c) or Gemini (Team et al. 2023) to extract verifiable claims from the generations of LVLMs, and then uses visual tools for hallucination detection. Compared to previous studies, our detection model is trained on fine-grained feedback from proprietary LVLMs, which covers main hallucination types (i.e., object, attribute, and relationship). It can also evaluate the severity of hallucinations and provide detailed reasons.

# 2.2 Mitigating Hallucination in LVLMs

Hallucination mitigation can be mainly divided into training-free and training-based approaches (Yin et al. 2023b; Huang et al. 2023; Sun et al. 2023; Zhao et al. 2023; Yu et al. 2024; Gunjal, Yin, and Bas 2024; Jing and Du 2024). Training-free approaches address potential hallucinations by post-processing the outputs of LVLMs (Yin et al. 2023b; Huang et al. 2023), thereby tending to reduce the inference speed of LVLMs. Instead, the latter reduce hallucinations in LVLMs via further training, such as instruction fine-tuning (Liu et al. 2024a) or preference learning (Sun et al. 2023; Yu et al. 2024; Li et al. $2 0 2 3 \mathrm { a }$ ; Zhao et al. 2023; Gunjal, Yin, and Bas 2024; Jing and Du 2024). Our work belongs to preference learning which biases LVLMs to favor the non-hallucinatory responses (Zhao et al. 2023; Gunjal, Yin, and Bas 2024; Sun et al. 2023; Yu et al. 2024). LLaVARLHF (Sun et al. 2023) is the first to train an LVLM to align with human preference. RLHF-V (Yu et al. 2024) manually collects segment-level human preference and conduct dense DPO over the human feedback to reduce hallucinations. POVID (Zhou et al. 2024a) constructs preference dataset by inserting textual hallucinations and distorting input images. Silkie (Li et al. 2023a) exploits feedback from various

LVLMs for constructing preference dataset, but the feedback are coarse-grained. In this study, we propose a pipeline for automatically constructing preference dataset and introduce the hallucination severity-aware preference learning for prioritizing the mitigation of critical hallucinations.

# 3 Methodology

In this section, we begin by introducing how to gather finegrained AI feedback in $\ S 3 . 1$ . Following this, we detail how this fine-grained AI feedback is used for detecting and mitigating LVLM hallucinations in $\ S 3 . 2$ and $\ S 3 . 3$ , respectively.

# 3.1 Fine-Grained AI Feedback Generation

Before introducing the method for gathering fine-grained AI feedback, we first detail the process of generating hallucinatory responses, upon which the collection of AI feedback is performed.

Hallucinatory Response Generation We investigate hallucination in the tasks of Detailed Description Generation (DDG) and Visual Complex Reasoning (VCR) following Liu et al. (2024a). These tasks require the LVLM to generate longer detailed description or complex reasoning response given the visual-language content, making the LVLM more susceptible to produce hallucinations. Note that our method is not limited to the two tasks but can be extended to other visual-language tasks.

We choose the Visual Genome (VG) (Krishna et al. 2017) and Silkie (Li et al. 2023a) dataset for constructing the DDG and VCR prompts, respectively. The images in VG are content-rich and associated with bounding boxes which specify various objects, attributes of each object, and spatial relationships within image content. These detailed annotations can help obtain more accurate AI feedback. Specifically, given a target LVLM $M$ , for DDG, a randomly selected image from VG and an instruction from the instruction set in RLHF-V(Yu et al. 2024) are used as the prompt for $M$ to generate a potentially hallucinatory response. Randomly choosing instruction from the instruction set injects randomness and harnesses the model to discern intricate image details. For VCR, we randomly select a <image, question $>$ pair from the Silkie dataset as the prompt to generate a potentially hallucinatory response.

We denote the set of generated hallucinatory responses as $\mathcal { D } _ { \mathrm { h a l } } = \{ ( x _ { i } , \hat { y } _ { i } ) \} _ { i = 1 } ^ { N }$ where $\hat { y } _ { i }$ is the hallucinatory response, $x _ { i }$ is the corresponding prompt and $N$ is the size of $\mathcal { D } _ { \mathrm { h a l } }$ .

Fine-Grained Hallucination Annotation via GPT-4/GPT4V. Given the collected hallucinatory dataset $\mathcal { D } _ { \mathrm { H a l } }$ , we can now gather fine-grained AI feedback upon it using GPT-4 and GPT-4V. The motivation behind this is that manually annotating large-scale datasets at fine-grained level is timeconsuming, costly, and challenging.

Specifically, for each VCR hallucinatory sample $( x _ { i } , \hat { y } _ { i } )$ in $\mathcal { D } _ { \mathrm { H a l } }$ , we input it into GPT-4V to generate fine-grained AI feedback at a rigorous sentence-level. For DDG, we provide $( x _ { i } , \hat { y } _ { i } )$ along with the associated verbal object bounding boxes in the VG dataset to GPT-4 as these additional annotations enable more accurate AI feedback compared to relying solely on the images. The used instruction prompt to generate the fine-grained AI feedback is shown in Figure 1 and 2 of the supplementary material. The obtained feedback is a six-tuple $\begin{array} { r } { ( x _ { i } , \hat { y } _ { i } ^ { j } , h _ { i , \mathrm { t y p e } } ^ { j } , h _ { i , \mathrm { R } } ^ { j } , \mathrm { H S } _ { i } ^ { j } , \mathrm { H S } _ { i , \mathrm { R } } ^ { j } ) } \end{array}$ where $\hat { y } _ { i } ^ { j }$ is the $j$ -th sentence of $\hat { y } _ { i }$ , $h _ { i , \mathrm { t y p e } } ^ { j }$ is the hallucination type, $h _ { i , \mathrm { R } } ^ { j }$ is the reason that explains why $\hat { y } _ { i } ^ { j }$ is considered a hallucination, ${ \mathrm { H } } { \mathrm { S } } _ { i } ^ { j }$ is the hallucination severity score used to differentiate the effect of different hallucinations and $\mathrm { H S } _ { i , \mathrm { R } } ^ { j }$ is the reason for hallucination severity score $\mathrm { H S } _ { i } ^ { j }$ . The provided reasons $h _ { i , \mathbb { R } } ^ { j }$ and $\mathrm { H S } _ { i , \mathrm { R } } ^ { j }$ improve the explainability of the hallucination detection process. Compared to coarse-grained feedback, this sentence-level granularity ensures a thorough hallucination detection.

For hij,type, we consider the following types of hallucinations: (i) <object> for object hallucinations, such as perceiving physical entities that are not actually present; (ii) <relationship $>$ for relationship hallucinations, such as giving inaccurate description of the relationship between objects; (iii) $<$ <attribute $>$ for attribute hallucinations, such as inaccurate perceptions of the characteristics of objects. For the hallucination severity score $\mathrm { H S } _ { i } ^ { j }$ , we define the following Likert-style ratings: (i) Minor (1 point): the hallucination concerns a minor detail and does not significantly affect the overall portrayal of the scene; (ii) Moderate (2 points): the hallucination involves a noticeable detail that is incorrect within the context of the scene, yet the overall comprehension of the scene is maintained; (iii) Major (3 points): the hallucination introduces a significant error or an entirely fabricated element that fundamentally alters the viewer’s understanding of the scene. These scores facilitate the assessment of hallucination severity and are further incorporated into the preference learning(See $\ S 3 . 3 )$ to prioritize the mitigation of critical hallucinations. We denote the set of fine-grained AI feedback as ${ \mathcal { D } } _ { \mathrm { f a i f } }$ .

# 3.2 Hallucination Detection via Fine-Grained AI Feedback

With the collected fine-grained AI feedback dataset ${ \mathcal { D } } _ { \mathrm { f a i f } }$ , we can then train a hallucination detection model using opensource LVLMs. Training an open-source model for hallucination detection offers the following merits. First, it enables perform fine-grained LVLMs hallucination detection with severity scores but not relies on proprietary models at lower cost. Second, the detected fine-grained hallucinations can be further used to construct a preference dataset, as discussed in $\ S 3 . 3$ .

Formally, given the sentence-level training dataset ${ \mathcal { D } } _ { \mathrm { f a i f } } =$ $\{ ( x ^ { i } , \hat { y } ^ { i } , h _ { \mathrm { t y p e } } ^ { i } , h _ { \mathrm { R } } ^ { i } , \mathrm { H S } ^ { i } , \mathrm { H S } _ { \mathrm { R } } ^ { i } ) \} _ { i = 1 } ^ { M }$ , we train fine-grained hallucination detection model $M _ { \mathrm { d e t } } ( ; \theta )$ , parameterized by $\theta$ , by minimizing the negative log likelihood loss:

$$
\mathcal { L } _ { \mathrm { D E T } } ( \theta ) = - \sum _ { i = 1 } ^ { M } \sum _ { t = 1 } ^ { T } \log M _ { d e t } ( g _ { t } ^ { i } | x ^ { i } , \hat { y } ^ { i } , g _ { 1 : t } ^ { i } )
$$

where $g ^ { i }$ is concatenation of $h _ { \mathrm { t y p e } } ^ { i } , h _ { \mathrm { R } } ^ { i } , \mathrm { H S } ^ { i } , \mathrm { H S } _ { \mathrm { R } } ^ { i }$ . Note that for non-hallucinated sentences, $h _ { \mathrm { t y p e } } ^ { i } , h _ { \mathrm { R } } ^ { i } , \mathrm { H S } ^ { i } , \mathrm { H S } _ { \mathrm { R } } ^ { i }$ are set

$\textcircled{1}$ Fine-Grained Hallucination Detection From GPT4 and GPT-4V $\textcircled{2}$ Training a Detection Model for Detect-then-Rewrite Pipeline Detect then rewrite   
LVLM Detection Model 回 More raw response Detected Hallucinations Detected Hallucinations Data Flow 向 Sentence3:[The tennis racket is Hallucination   
8 Pheimt: Dencribeil. e Halutentedns SeverityScores 微车 Sentence4: [with many spectators seated, Hallucination Supervised fine-tuning Rewritten response Watching thematchlob Severity Scores   
图 thern $\textcircled{3}$ Hallucination Severity-Aware Direct Preference Optimization Sentence5: [Thechairsare all black...][Attr S Raw response   
向 The image features 1. Ma 田 Direlucintoeertae Seje tennioracken iser righty handtatwith 3. Left hand: Moderate (2 points) 2. Blue chairs: Minor (1 point) 向 三 Scoresas weight T setmd,watchine Rewritten response LVLM-HSA-DPO chairsareallblack Theimagefeaturesawoman...The Raw response Rewritten response teiketisi ×

to ${ \bf < N o }$ hallucination, None, 0, None>, respectively. Our preliminary experiments indicate that the ratio of hallucinated to non-hallucinated data significantly impacts the performance of the detection model. We tested multiple ratios and found that a final ratio of 1:1.2 provides optimal results.

# 3.3 Hallucination Mitigation via Fined-Grained AI Feedback

With the built hallucination detection model, we next introduce an automated method for constructing the preference dataset.

Detect-then-Rewrite Pipeline for Automatic Preference Dataset Construction. To reduce the expensive annotation costs either caused by human experts or proprietary models (Sun et al. 2023; Li et al. 2023a; Gunjal, Yin, and Bas 2024), we propose a detect-then-write pipeline for automatic preference dataset construction, which allows for costeffectively fine-grained feedback annotation at scale.

Specifically, the detect-then-rewrite pipeline consists of the hallucination detection model, $M _ { \mathrm { d e t } }$ , and one rewriting model, $M _ { \mathrm { w r i } }$ . Given a prompt and its hallucinatory response $( x , \hat { y } )$ , the detection model first identifies a set of hallucination $\mathcal { H } \ : = \ : \{ ( h _ { \mathrm { t y p e } } ^ { j } , h _ { \mathrm { R } } ^ { j } , \mathrm { H S } ^ { j } , \mathrm { H S } _ { \mathrm { R } } ^ { j } ) \} _ { j = 1 } ^ { | \hat { y } | }$ . Then, using $\hat { y }$ and $\mathcal { H }$ as the input prompt, $M _ { \mathrm { d e t } }$ rewrites $\hat { y }$ into a nonhallucinatory response $y$ . The specific rewriting prompt is provided in section A.1 of the supplementary material. In practice, we choose an open-source LVLM LLaVA as the rewriting model $M _ { w }$ , as it not only has demonstrated the impressive instruction-following and rewriting capability in our pilot experiments, but also offers a way to further reduce the annotation cost. We denote this preference dataset as $\mathcal { D } _ { \mathrm { p r e f } } = \{ ( x _ { i } , \hat { y } _ { i } , y _ { i } , \mathcal { H } _ { i } ) \} _ { i = 1 } ^ { N }$ where $N$ is the dataset size.

Connection to Scalable Oversight. Compared with previous studies, this preference dataset construction pipeline enables us to more budget-friendly annotate a large-scale preference dataset for training mitigation models. The underlying insight behind this approach is closely correlated with the concept of scalable oversight, which aims to train machines to assist humans in supervising models by critiquing the model’s output (Bai et al. 2022; Lee et al. 2023; Ganguli et al. 2023; McAleese et al. 2024) or decomposing complex problems into simpler sub-problems(Leike et al. 2018; Lightman et al. 2024). In this work, we break down the complicated fine-grained labeling process into two steps: first, detecting (critiquing) hallucinations in the generated completion, and then rewriting them into non-hallucinated ones. Moreover, we leverage the capabilities of current open-source LVLMs by employing them as hallucination detection and rewriting experts, thereby reducing the cost of providing a large-scale supervisions. Notably, our pipeline does not require any ground truth datasets, making it not only cost-effective but also practical for curtain scenarios where labeled datasets may be unavailable.

Hallucination Severity-Aware Direct Preference Optimization. With the automatically constructed preference dataset, we can now perform preference learning for hallucination mitigation. In particular, we choose the offline preference optimization method Direct Preference Optimization (DPO) (Rafailov et al. 2024; Xiao et al. 2024) as it is more stable and efficient compared to online RLHF methods(Ouyang et al. 2022; Schulman et al. 2017). The learning objective of DPO is directly formulated over the the policy model $\pi _ { \boldsymbol { \theta } } ( y | \boldsymbol { x } )$ and a reference model $\pi _ { \mathrm { r e f } } ( y | x )$ :

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { D P O } } = - \underset { ( x , y _ { w } , y _ { l } ) \in \mathcal { D } } { \mathbb { E } } \Big [ \log \sigma \Big ( \beta \log \frac { \pi _ { \theta } ( y _ { w } | x ) } { \pi _ { \mathrm { r e f } } ( y _ { w } | x ) } - \beta \log \frac { \pi _ { \theta } ( y _ { l } | x ) } { \pi _ { \mathrm { r e f } } ( y _ { l } | x ) } \Big ) \Big ] } \end{array}
$$

where $y _ { l }$ and $y _ { w }$ are the rejected and chosen answer. $\sigma$ is the logistic function. $\beta$ is the hyper-parameter controlling the deviation from $\pi _ { \mathrm { r e f } } ( y | x )$ . Action score $\log \pi ( y | x )$ is the response generation likelihood.

As can be observed, the standard DPO loss $\mathcal { L } _ { \mathrm { D P O } }$ treats all pairwise preference responses equally, making more severe hallucinations (e.g., description of non-existent objects) not being greater considered compared with other hallucinations (e.g., incorrect color descriptions of objects). To tackle this limitation, we present Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO), which incorporates hallucination severity into the preference optimization for mitigating critical hallucinations with higher priority. Specifically, we begin with aggregating the sentence-wise hallucination severity scores in $\mathcal { H } _ { i }$ as the response-level severity score:

$$
S _ { \mathrm { A V G } } ^ { i } = \frac { 1 } { T } \sum _ { j = 1 } ^ { T } \mathrm { H S } _ { i } ^ { j }
$$

where $T$ is the number of sentences in the response which helps prevent potential reward hacking introduced by the length bias. This strategy bears similarity to our fine-grained hallucination detection, which can alleviate the difficulty of directly assessing the hallucination severity of the whole response. Subsequently, we adaptively assign this severity score to the implicit reward model of DPO to ensure responses with more severe hallucinations receive stronger penalties for correction:

$$
\mathcal { L } _ { \mathrm { M I T } } = - \sum _ { i = 1 } ^ { | \mathcal { D } _ { \mathrm { p r e f } } | } \log \sigma ( \beta \left[ \log \frac { \pi _ { \theta } \left( y _ { i } | \boldsymbol { x } _ { i } \right) } { \pi _ { \mathrm { r e f } } \left( y _ { i } | \boldsymbol { x } _ { i } \right) } - S _ { \mathrm { A V G } } ^ { i } \log \frac { \pi _ { \theta } \left( \hat { y } _ { i } | \boldsymbol { x } _ { i } \right) } { \pi _ { \mathrm { r e f } } \left( \hat { y } _ { i } | \boldsymbol { x } _ { i } \right) } \right] )
$$

# 4 Experiments

In this section, we evaluate the efficacy of our method for detecting and mitigating hallucinations in LVLMs.

# 4.1 Datasets and Metrics

We introduce the datasets and metrics for evaluating hallucination detection and mitigation. For hallucination detection, we use the following benchmarks: (1) MHaluBench(Chen et al. 2024) is a newly established benchmark for detecting hallucination in both image-to-text and text-to-image settings as binary classification task. We adopt the image-totext part to evaluate our detection model. (2) To evaluate various types of hallucination and their severity, we manually labeled a dataset named MFHaluBench, which includes object, attribute, and relationship hallucinations along with a human-annotated severity score for each segment. We evaluate MFHaluBench using binary classification to identify hallucinated segments and multi-class classification to distinguish between different types of hallucinations.

For hallucination mitigation, we use the following benchmarks: (1) Object HalBench (Rohrbach et al. 2018) is a widely adopted benchmark for evaluating object hallucination in DDG. Following Yu et al. (2024), we use $\mathrm { C H A I R } _ { S }$ (i.e., the percentage of responses that contain hallucinations) and $\mathrm { C H A I R } _ { I }$ (i.e., the percentage of hallucinated object mentions among all object mentions) as the evaluation metrics. (2) AMBER (Wang et al. 2023) consists of generative and discriminative parts, focusing on common objects and pitfall objects which easily cause hallucinations. We use the generative part of AMBER and report the following metrics: CHAIR, Cover, Hal and Cog. More details about these metrics can be found in section C.1 of the supplementary material. (3) MMHal-Bench (Sun et al. 2023) is a benchmark for evaluating object hallucination by using GPT-4 to compare the model output with the annotated response. We report overall score rated by GPT-4 and the hallucination rate. (4) POPE (Li et al. 2023b) is an object hallucination evaluation benchmark by testing LVLMs in a form of question answering. We choose the Adversarial part of POPE and report its F1 scores.

Table 1: Experimental results of MhaluBench on Imageto-Text hallucination detection. The results for Gemini and GPT-4V are sourced from the UNIHD(Chen et al. 2024).   

<html><body><table><tr><td rowspan="2">Methods</td><td rowspan="2">Levels</td><td colspan="4">Average</td></tr><tr><td>Acc.</td><td>P</td><td>R</td><td>Mac.F1</td></tr><tr><td>Geminiwith Self-Check</td><td>Claim Segment</td><td>74.74 75.80 75.68 75.11</td><td>73.89</td><td>77.44</td><td>74.74 73.85</td></tr><tr><td>Gemini with UNIHD</td><td>Claim</td><td>77.41 Segment 78.68 75.97 78.64</td><td>77.76 77.99</td><td></td><td>77.39 76.74</td></tr><tr><td>GPT-4Vwith Self-Check</td><td>Claim Segment 80.80</td><td></td><td>79.25 79.02 79.16</td><td></td><td>79.08</td></tr><tr><td>GPT-4VwithUNIHD</td><td>Claim</td><td>81.91</td><td>77.80 81.81</td><td>78.30 81.52</td><td>78.04 81.63</td></tr><tr><td>OurDetectionModel</td><td>Claim</td><td>Segment 84.60 82.77 Segment 86.94 87.73 82.88</td><td>85.60 85.46 85.79</td><td>80.89</td><td>81.71 85.52 85.23</td></tr></table></body></html>

Table 2: Experimental results of MFHaluBench. Details of Multi refer to section B.3 of the supplementary material.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="4">Binary</td><td rowspan="2">Multi ACC</td></tr><tr><td>P</td><td>R</td><td>ACC</td><td>F1</td></tr><tr><td>GPT-4V2shot</td><td>59.7</td><td>98.7</td><td>63.3</td><td>74.4</td><td>40.6</td></tr><tr><td>LLaVA-1.6-34B 2shot Our detection model</td><td>55.5 87.8</td><td>100 88.8</td><td>56.7 87.3</td><td>71.4 88.2</td><td>36.7 74.3</td></tr></table></body></html>

In addition to the above hallucination detection and mitigation benchmarks, we also adopt the widely used LLava Bench in the wild(Liu et al. 2024b) to evaluate the multimodal capabilities after mitigating training. We also introduce Hallucination Severity Score: a metric to evaluate the hallucination severity of model response. Severity scores ranges from 0 to 3. For complete definitions of these scores, refer to section A.2 of the supplementary material.

# 4.2 Baselines

For hallucination detection, we compare our method with GPT-4V(OpenAI 2023c), Gemini(Team et al. 2023) and UNIHD(Chen et al. 2024) following the experiment settings

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">Object HalBench</td><td colspan="4">AMBER</td><td colspan="2">MMHal-Bench</td><td>LLaVABench</td><td>POPE Adv.</td></tr><tr><td>CHAIRs↓</td><td>CHAIR1↓</td><td>CHAIR ↓</td><td>Cover. ↑</td><td>Hal.↓</td><td>Cog.↓</td><td>Overall 个</td><td>Resp.↓</td><td>Overall ↑</td><td>F1↑</td></tr><tr><td>LRV</td><td>32.3</td><td>22.3</td><td>1</td><td></td><td>-</td><td></td><td>1</td><td>-</td><td></td><td>-</td></tr><tr><td>POVID</td><td>48.1</td><td>24.4</td><td>7.3</td><td>49.5</td><td>31.1</td><td>3.7</td><td>2.08</td><td>0.56</td><td></td><td>81.6</td></tr><tr><td>InstructBLIP</td><td>25.9</td><td>14.3</td><td>8.8</td><td>52.2</td><td>38.2</td><td>4.4</td><td>2.14</td><td>0.58</td><td></td><td>78.4</td></tr><tr><td>Qwen-VL-Chat</td><td>36.0</td><td>21.3</td><td>6.6</td><td>53.2</td><td>31.0</td><td>2.9</td><td>2.89</td><td>0.43</td><td>79.8</td><td>82.8</td></tr><tr><td>LLaVA-1.5</td><td>46.3</td><td>22.6</td><td>7.8</td><td>51.0</td><td>36.4</td><td>4.2</td><td>2.42</td><td></td><td>72.5</td><td>84.5</td></tr><tr><td>LLaVA-RLHF</td><td>38.1</td><td>18.9</td><td>7.7</td><td>52.1</td><td>39.0</td><td>4.4</td><td>2.53</td><td>0.57</td><td>76.9</td><td>80.5</td></tr><tr><td></td><td></td><td></td><td></td><td>46.1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RPH-4V</td><td>122</td><td>73</td><td></td><td></td><td>25.1</td><td>2.6</td><td>28</td><td>0.89</td><td>59.7</td><td>--</td></tr><tr><td>Silkie</td><td>25.3</td><td>13.9</td><td>5.4</td><td>55.8</td><td>29.0</td><td>2.0</td><td>3.01</td><td>0.41</td><td>84.9</td><td>82.1</td></tr><tr><td>DPO</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>w/ Qwen-VL</td><td>14.3</td><td>8.0</td><td>3.8</td><td>53.2</td><td>19.7</td><td>1.8</td><td>2.98</td><td>0.38</td><td>82.0</td><td>82.6</td></tr><tr><td>w/ LLaVA-1.5</td><td>6.7</td><td>3.6</td><td>2.8</td><td>47.8</td><td>15.5</td><td>1.6</td><td>2.58</td><td>0.50</td><td>79.3</td><td>84.5</td></tr><tr><td>HSA-DPO</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>w/ Qwen-VL</td><td>11.0</td><td>5.5</td><td>3.7</td><td>52.4</td><td>19.0</td><td>1.6</td><td>3.07</td><td>0.34</td><td>82.4</td><td>82.9</td></tr><tr><td>w/ LLaVA-1.5</td><td>5.3</td><td>3.2</td><td>2.1</td><td>47.3</td><td>13.4</td><td>1.2</td><td>2.61</td><td>0.48</td><td>80.5</td><td>84.9</td></tr></table></body></html>

Table 3: Main experimental results on hallucination mitigation. Note that LLaVA Bench denotes LLaVA Bench in the wild(Liu et al. 2024b). POPE Adv. denotes POPE Adversarial(Li et al. 2023b).

of (Chen et al. 2024).

For hallucination mitigation, we adopt a range of competitive hallucination mitigation methods as baselines: (1) InstructBLIP(Dai et al. 2024); (2) LLaVA 1.5(Liu et al. 2023); (3) Qwen-VL-Chat(Bai et al. 2023); (4) GPT-4V(OpenAI 2023c); (4) LRV (Liu et al. 2024a); (5) LLaVA-RLHF (Sun et al. 2023); (6) RLHF-V(Yu et al. 2024); (7) Silkie(Li et al. 2023a); (8) POVID (Zhou et al. 2024a).

# 4.3 Main Results

We report main results with respect to hallucination detection and mitigation, respectively.

Hallucination Detection. Table 1 and 2 report the main results on hallucination detection benchmarks. We can draw the following conclusions. First, our detection model achieves the state-of-the-art results on MHaluBench on average, outperforming GPT-4V and Gimini. Specifically, at the claim level, our detection model relatively surpasses UNIHD by $4 . 7 \%$ in Mac. F1 score and GPT-4V Self-Check 2-shot by $8 . 1 \%$ in Mac. F1 score. This improvement is consistently observed at the segment level as well. Second, on MFHaluBench, our detection model achieves an F1 score of $8 8 . 2 \%$ in binary classification, and an accuracy of $7 4 . 3 \%$ in Multi (fine-grained classification), outperforming GPT-4V 2-shot and LLaVA-1.6-34B 2-shot.

Hallucination Mitigation. Table 3 reports the main results on hallucination mitigation benchmarks. We can draw the following conclusions. First, HSA-DPO achieves stateof-the-art results on Object HalBench, outperforming the leading-edge closed-source LVLMs like GPT-4V. Second, HSA-DPO reduces the hallucination of LLaVA-1.5, our base model, by $7 6 . 3 \%$ for $\mathrm { C H A I R } _ { S }$ on Object HalBench and by $3 6 . 1 \%$ for Hal on AMBER. Third, compared to models trained on coarse-grained AI feedback (Silkie) or humanlabelled fine-grained dataset (RLHF-V), our method gives superior results in hallucination mitigation. This demonstrates the effectiveness of using fine-grained AI feedback for detecting and mitigating hallucinations in LVLMs. Fourth, after preference learning, HSA-DPO is not affected by alignment tax (Askell et al. 2021; Ouyang et al. 2022) and maintains its multi-modality capabilities, as evidenced by the results on the overall metric of MMHal-Bench and LLaVA Bench in the wild. Lastly, to investigate the effect of our method on different base models, we train HSADPO on Qwen-VL-Chat(Bai et al. 2023), where the hallucinated responses are generated by Qwen-VL-Chat. Both DPO and HSA-DPO results in Table 3 indicate that our model gives a significant improvement over Qwen-VL-Chat, which demonstrate that our methodology can be applied to various LVLMs for mitigating hallucinations.

Table 4: Ablation results on Object HalBench (ObjHal) and AMBER. HS denotes Hallucination Severity rated by GPT4V. C.s, C.i, C. are short for $\mathrm { C H A I R } _ { S }$ , $\mathrm { C H A I R } _ { I }$ , CHAIR.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="2">ObjHal</td><td colspan="4">AMBER</td><td rowspan="2">HS↓</td></tr><tr><td>C.s↓</td><td>C.i↓</td><td>C.↓</td><td>Cover.↑ Hal.↓</td><td></td><td>Cog.↓</td></tr><tr><td>Ours</td><td>5.3</td><td>3.2</td><td>2.1</td><td>47.3</td><td>13.4</td><td>1.2</td><td>0.60</td></tr><tr><td>w/o Detection</td><td>42.1 20.3</td><td></td><td>7.6</td><td>52.1</td><td>32.4</td><td>4.0</td><td>0.79</td></tr><tr><td>w/o HSA</td><td>6.7</td><td>3.6</td><td>2.8</td><td>47.8</td><td>15.5</td><td>1.6</td><td>0.65</td></tr><tr><td>w/o FineGrained</td><td>17.0</td><td>8.9</td><td>5.0</td><td>53.6</td><td>27.3</td><td>1.7</td><td>0.67</td></tr></table></body></html>

# 4.4 Ablations

We conduct ablation studies to validate the effectiveness of each design of our method. The results are reported in Table 4, where ”w/o Detection” represents instead of detecting hallucinations by the detection model, we directly rewrite the hallucinated response into non-hallucinated one to construct preference dataset. ”w/o HSA” represents that we do not incorporate hallucination severity into preference optimization and use the vanilla DPO. ”w/o FineGrained” means we use coarse-grained GPT-4V feedback.

![](images/de4fa3d96bf89da3d33ec132e8684ba0259204e4364a45cd7ee883129faca39d.jpg)  
Figure 3: Effect of scaling preference dataset (Figure A) and different hallucination types (Figure B).   
Figure 4: Qualitative results of different models on VCR and DDG. Correct answers, factual hallucinations are highlighted in red and green respectively.

Table 5: Annotation Efficiency and Cost. Efficiency is annotation time per sample and Cost is for collecting 16k preference datapoints.   

<html><body><table><tr><td>Feedback</td><td>Train Time Efficiency</td><td></td><td>Cost</td></tr><tr><td>Human</td><td>0</td><td>20 s+</td><td>$4800</td></tr><tr><td>GPT-4V</td><td>0</td><td>8s</td><td>$1600</td></tr><tr><td>Our pipeline</td><td>5.7 h</td><td>5s</td><td>$600</td></tr></table></body></html>

From the table, we have the following conclusion. First, without detection model identifying hallucinations, model performance seriously degrades on both benchmarks, highlighting the necessity of our detect-then-rewrite pipeline. Second, HSA-DPO demonstrate robust improvement over vanilla DPO in mitigating hallucinations across all hallucination metrics, relatively reducing by $2 0 . 8 \%$ for $\mathrm { C H A I R } _ { S }$ on Object HalBench, by $1 3 . 5 \%$ for Hal on AMBER and by $7 . 6 \%$ on Hallucination Severity. Third, fine-grained feedback outperforms coarse-grained one from GPT-4V, relatively reducing by $6 8 . 8 \%$ for $\mathrm { C H A I R } _ { S }$ on Object HalBench, by $5 0 . 9 \%$ for Hal on AMBER, which reveals the efficacy of fine-grained granularity.

# 4.5 Analyses

Annotation Efficiency and Cost. Our method constructs preference dataset using AI feedback through a detect-thenrewrite pipeline. Table 5 compares the annotation efficiency and cost with those of GPT-4V and human annotation. While our method needs 5.7 hours to train models, it only requires $2 5 \%$ and $6 2 . 5 \%$ of the time needed by human annotation and GPT-4V, respectively. Regarding annotation cost, our method incurs only $\$ 600$ , which is $1 2 . 5 \%$ and $3 7 . 5 \%$ of the cost needed by human annotation and GPT-4V, respectively. These results underscore our approach’s superior efficiency and cost-effectiveness compared to other methods.

Effect of Scaling Preference Dataset. We conduct experiments to investigate the effect of scaling the preference dataset on Object HalBench. As depicted in Figure 3, we observe that with the increasing size of dataset, the hallucination rate of LLaVA with HSA-DPO shows a rapid and

Complex LLaVA-1.5-13b HS : 1.00 Reasoning The young boy is interacting with a brown and white cow.The woman seems to be showing the cow to the child, HSA-DPO HS :0 ... a young boy,who dressed in black, feeding a cow ... holding out his hand with food in it,and Q: What are the boy and the cow is reaching its head through the fence to cow doing? Detailed $\hat { \mathcal { \ C } }$ LLaVA-1.5-13b $\boxed { \overline { { \overline { { \varepsilon } } } } \overline { { \tilde { \varepsilon } } } }$ HS:1.20 Description ...There are other people in various positions,some just next to the skateboarder and others further away. In additionto themainskateboarder,thereisanother skateboard located near the right edge of the image. HSA-DPO  HS:0 He is in the air, jumping over a rock on the sidewalk. The skateboarder is the main focus of the image, Q:Describe this image in detail.

consistent decrease. When the size of the training dataset reaches $8 \mathbf { k }$ , our model surpasses GPT-4V on both $\mathrm { C H A I R } _ { S }$ and $\mathrm { C H A I R } _ { I }$ , highlighting the value of our pipeline in enabling cost-effective annotation of preference datasets.

Effect on Different Hallucination Types. To evaluate the effect of our method on different hallucination types, we conduct experiments on Amber benchmark and report the F1 scores on all types, i.e., object existence, attribute, state, number, action and relation. Figure 3 shows HSA-DPO with LLaVA-1.5 outperforms LLaVA-1.5 across all hallucination types. We also find that our method are more effective in improving number and existence types (object hallucinations) . However, we observe limited improvement in the relation and action types (relationship hallucinations). This is likely due to the scarcity of relationship hallucinations in preference dataset, as well as the inherent complexity of addressing relationship hallucinations compared to object ones.

# 4.6 Case Studies

In Figure 4, we qualitatively compare model performance on VCR and DDG. In VCR case, LLaVA struggles to recognize the key action of the boy feeding the cow. After HSADPO training, model accurately answers the question with more details. For DDG, LLaVA makes serious errors about nearby people and skateboard. However, model with HSADPO provides a precise description, accurately identifying relationships and objects in the image.

# 5 Conclusion

In this work, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. We begin with generating sentence-level hallucination annotation dataset via AI feedback. Then, a detect-then-rewrite pipeline is used to more cost-effectively construct preference dataset at scale. Lastly, HSA-DPO is introduced to incorporates hallucination severity into preference learning.