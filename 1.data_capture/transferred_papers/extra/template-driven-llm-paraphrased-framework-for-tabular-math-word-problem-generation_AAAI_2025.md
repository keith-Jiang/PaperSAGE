# Template-Driven LLM-Paraphrased Framework for Tabular Math Word Problem Generation

Xiaoqiang Kang $^ { 1 , 2 , * }$ , Zimu Wang $^ { 1 , 2 , * }$ , Xiaobo $\mathbf { J i n } ^ { 1 }$ , Wei Wang1, Kaizhu Huang3, Qiufeng Wang1,†

1School of Advanced Technology, Xi’an Jiaotong-Liverpool University 2University of Liverpool 3Duke Kunshan University Xiaoqiang.Kang23, Zimu.Wang19 @student.xjtlu.edu.cn, Qiufeng.Wang@xjtlu.edu.cn

# Abstract

Solving tabular math word problems (TMWPs) has become a critical role in evaluating the mathematical reasoning ability of large language models (LLMs), where large-scale TMWP samples are commonly required for fine-tuning. Since the collection of high-quality TMWP datasets is costly and timeconsuming, recent research has concentrated on automatic TMWP generation. However, current generated samples usually suffer from issues of either correctness or diversity. In this paper, we propose a Template-driven LLM-paraphrased (TeLL) framework for generating high-quality TMWP samples with diverse backgrounds and accurate tables, questions, answers, and solutions. To this end, we first extract templates from existing real samples to generate initial problems, ensuring correctness. Then, we adopt an LLM to extend templates and paraphrase problems, obtaining diverse TMWP samples. Furthermore, we find the reasoning annotation is important for solving TMWPs. Therefore, we propose to enrich each solution with illustrative reasoning steps. Through the proposed framework, we construct a high-quality dataset TabMWPTeLL by adhering to the question types in the TabMWP dataset, and we conduct extensive experiments on a variety of LLMs to demonstrate the effectiveness of TabMWP-TeLL in improving TMWP-solving performance.

# Code — https://github.com/Jason8Kang/TELL Extended version — https://arxiv.org/abs/2412.15594

Question: Lola measured the length of each scarf in the clothing store. How many scarves are at least 82 centimeters?

Question: Lola measured the length of each scarf in the clothing store. How many scarves have a minimum of 66 centimeters?

<html><body><table><tr><td>Stem</td><td>Leaf</td></tr><tr><td>8</td><td>0,2,2, 3, 6,7</td></tr><tr><td>9</td><td>0,0</td></tr></table></body></html>

<html><body><table><tr><td>Stem</td><td>Leaf</td></tr><tr><td></td><td>62,3,5,6</td></tr><tr><td>7</td><td>3,3</td></tr></table></body></html>

Solution: Find the row with stem 6. Count all leaves greater than 6. […]

Solution: Find the row with stem 8. Count all the leaves greater than or equal to 2.Count all the leaves in the row with stem 9. You counted 7 leaves (b) LLM-based Generation (a) Original Sample

# Question:

Question: A research team recorded the ages of participants in a health study. How many participants are at least 50 years old?

Lola measured the length of each scarf in the clothing store. How many scarves are at least 82 centimeters?

<html><body><table><tr><td>Stem</td><td>Leaf</td></tr><tr><td>5</td><td>1,3,4</td></tr><tr><td>6</td><td>1, 1, 2, 7</td></tr></table></body></html>

<html><body><table><tr><td>Stem</td><td>Leaf</td></tr><tr><td>8</td><td>1, 2, 2, 3, 6, 7</td></tr><tr><td>9</td><td>0,0</td></tr></table></body></html>

# Solution:

Solution: Find the row with stem 8. Count all the leaves greater than or equal to 2. […]

1. Identify Relevant Stems: […] 2. Analyze the Leaves: For stem 5, count […] For stem 6,  […] 4 leaves. 3. Aggregate the Counts: $3 + 4 = 7$ (c) Template-based Generation (d) Ours with Illustrative Solution

# Introduction

The rise of large language models (LLMs) has achieved unprecedented success in a variety of reasoning tasks (Peng et al. 2023; Li et al. 2024; Wang et al. 2024); however, solving math word problems (MWPs), particularly based on heterogeneous tabular and textual data is still challenging for LLMs (Lu et al. 2023b; Zheng et al. 2023). For various complex MWPs, training models usually require a large amount of data, but the collection and annotation of MWPs are usually costly and time-consuming, resulting in the scarcity of public tabular MWP datasets.

To mitigate the data issue, numerous studies have explored the ability to automatically generate MWP samples, mainly categorizing template-based (Williams 2011; Polozov et al. 2015), rewriting-based (Koncel-Kedziorski et al. 2016), neural network-based (Liyanage and Ranathunga 2020; Liu et al. 2021), and LLM-based (Luo et al. 2023; Tang et al. 2024) methods. Despite the progress, existing methods still face three major challenges for tabular data:

(1) Lack of correctness. Generation-based methods, like rewriting-based and LLM-based methods, misunderstand the meaning of the questions due to the hallucination problem (Zhang et al. 2023) and thus get wrong answers. As shown in Figure 1(b), the LLM-generated solution calculates leaves greater than 66 but ignores the case equal to 66, which is inconsistent with the requirements of the question. (2) Lack of diversity in problems. Because all problems are generated from abstract templates (Williams 2011), template-based methods have limited diversity. As shown in Figure 1(c), the generated problem simply replaces some numbers that do not affect the answer while keeping the overall content unchanged. (3) Lack of illustrative steps in solutions. As shown in Figures 1(a) and 1(d), our data annotations have more clearly described solution steps than other data annotations. The model induces multi-step reasoning behaviors through clear intermediate reasoning steps such as Chain-of-Thought (CoT) (Wei et al. 2022).

To overcome the aforementioned challenges, we propose a Template-driven LLM Paraphrased (TeLL) framework for generating Tabular MWPs (TMWPs) using both templates and LLMs. Different from the previous template-based generation, which rewrites questions with minor modifications based on a pre-defined template as shown in Figure 1(c), our templates are extracted from an existing TMWP dataset, each of which is abstract and summarizes mathematical logic. To generate flexible templates, we utilize an LLM to extend those extracted templates with a broader range of question types, maintaining mathematical logic to ensure correctness. Although these extended templates can generate various TMWPs, the background and linguistic description of those generated problems are monotonous. To pursue high-quality, realistic generated samples, we leverage the powerful language ability of LLMs to paraphrase problems with various contexts, obtaining diverse problems. Since our LLM-based paraphrasing does not change the mathematical logic, the correctness can be ensured. The overall framework is shown in Figure 2, and the details can be found in the section of Methodology. In summary, our generation method combines the advantages of both templates and LLM, ensuring the correctness and diversity of the generated samples.

Based on the framework, we construct a high-quality dataset named TabMWP-TeLL based on the question types in the TabMWP dataset (Lu et al. 2023b). We find that the step-by-step reasoning annotations are significant for using LLMs; therefore, we propose refining the original solutions with more illustrative steps, as shown in Figure 1(d). In our generated dataset, we utilize an LLM, Yi (Young et al. 2024), to paraphrase the template-based problems. In experiments, we fine-tune three LLMs, including Mistral (Jiang et al. 2023), Qwen 2 (Yang et al. 2024), and Llama 3 (Dubey et al. 2024). Experimental results show that TabMWP-TeLL is effective in improving TMWP solving, outperforming the baselines by a large margin, and is particularly effective in improving performance on challenging problems while maintaining the performance on simple ones.

Our contributions are as follows: (1) We propose TeLL, a template-driven, LLM-paraphrased framework, to generate high-quality TMWPs. To the best of our knowledge, we are the first to leverage templates and LLMs on TMWP generation, ensuring both correctness and diversity. (2) We propose to enrich TMWP solutions with more illustrative annotations, eliciting the multi-step reasoning ability of LLMs. (3) We construct a high-quality TMWP dataset, TabMWPTeLL, which is an extension of the TabMWP dataset. The results of human verification illustrate certain correctness and diversity of the TMWP generation strategy. (4) Extensive experimental results demonstrate the effectiveness of TabMWP-TeLL in improving TMWP solving, outperforming the baselines by a significant margin.

# Related Work

Math Word Problems. Recent research has primarily focused on addressing MWPs using generative models, such as sequence-based (Wang, Liu, and Shi 2017) and treebased (Xie and Sun 2019; Zhang et al. 2020) models. With the rapid advancements of LLMs and the development of few-shot (Brown et al. 2020) and CoT (Wei et al. 2022) prompting, there has been a growing trend toward leveraging prompt engineering (Chen et al. 2023; Fu et al. 2023; Zhou et al. 2023a; Wang et al. 2023) and fine-tuning (Liu et al. 2023; Liang et al. 2024) strategies with notable performance. Furthermore, LLMs with math reasoning abilities have also been incorporated in the field of intelligent education (Macina et al. 2023; Wang and Demszky 2023). In this context, generating high-quality MWPs with both correctness and diversity is worthwhile.

Tabular Math Word Problems. Recent years have witnessed extensive research into solving TMWPs. TabMWP (Lu et al. 2023b) is the first TMWP reasoning dataset that contains 38, 431 grade-level problems with tabular context. PromptPG has been subsequently proposed, which utilizes policy gradient to select in-context examples for the test examples. Considering that LLMs often make errors in mathematical calculations, subsequent research has primarily focused on using external tools, such as calculators, to improve calculation accuracy. TaCo (Zheng et al. 2023) coordinates two Tabular LMs (TaLMs), which are responsible for CoT generation and answer inference, integrated with an external calculator. Chameleon (Lu et al. 2023a) composes various tools to accomplish complex reasoning tasks, such as LLMs, Python functions, and row and column look-up. CREATOR (Qian et al. 2023) and CRAFT (Yuan et al. 2024) create new tools for specific problems rather than calling the existing ones.

Math Word Problem Generation. Existing research in MWP generation can be broadly classified into four categories, including template-based, rewriting-based, neural network-based, and LLM-based methods. Template-based methods start by abstracting the existing MWPs into a template or a skeleton and then generating new problems from the abstract templates (Williams 2011; Polozov et al. 2015). Rewriting-based methods edit existing MWPs, altering the background of the problems while preserving their contents and logic (Koncel-Kedziorski et al. 2016; Moon-Rembert and Gilbert 2019). Neural network-based methods generate MWPs from topics and equations in an end-to-end manner (Liyanage and Ranathunga 2020; Liu et al. 2021; Zhou et al. 2023b). Inspired by the development of LLMs and their notable performance in various downstream applications, recent attempts have been focused on exploiting LLMs to generate MWPs, such as based on concepts (Tang et al. 2024) and key points (Huang et al. 2024). However, the aforementioned methods usually suffer from issues of either correctness or diversity. Different from the previous approaches, we propose a template-driven, LLM-paraphrased framework to generate high-quality TMWP samples with diverse descriptions and correct answers.

A Question and Table Template: Template-based Question and Table: Paraphrased Question and Table: Original Data are listed in the table. What is the The values of different categories The values of different categories are listed in the table. What is the compiled the number of hours he After a summer internship, John   
$\textcircled{1}$ bTsetrmapcltiaoten mean Cofatheeg onruymbers?Value mean Cofatheeg onruymbers?Value of the nuWmeebkers? Hours Worked $\textcircled{3}$ Template A a ④ Template A 52 $\textcircled{5}$ Problem Week 1 52 Selection B b Instantiation B 43 Paraphrasing Week 2 43 C c C 55 向 Week 3 55 D d D 50 Week 4 50 Template DB E e E 50 LLM Week 5 50   
A $\textcircled{2}$ mTenmtpatliaoten 向 STolautniosnweTre tmhpelamtee:an of the numbers, fTol oanwstwherstehset empes:an[…of]the numbers, fPToalroanwpshtrwhaesrsetedhsSetoelmputesi:ao[n:…o]f the numbers, LLM Therefore, the mean of the numbers Therefore, the mean of the numbers Therefore, the mean of the numbers is {mean}. is 50. is 50.

# Methodology

# Problem Definition

We define Tabular Math Word Problems (TMWPs) as follows: given a table $t$ containing multiple rows and columns and a question $q$ about the table $t$ , where the table could be a visual image, natural language text, or a structured database, our task is to generate a correct answer $a$ that matches the ground truth of the question, derived by solution steps $s$ . In our work, we focus on solving TMWPs using LLMs.

Note that the problems we are concerned with, regardless of the questions or the tables, are described in natural language texts; therefore, they usually follow certain rules. Further, for a given class of problems, we can abstract the template $P ( { \pmb x } ) = ( Q ( { \pmb x } ) , T ( { \pmb x } ) , A ( { \pmb x } ) , S ( { \pmb x } ) )$ , containing a question template, a table template, an answer template, and a solution template, with respect to the problems, each of which contains placeholders that can be filled in the TMWP generation process. $\scriptstyle { \mathbf { { \vec { x } } } }$ contains all the numbers and their corresponding categories in the table. Intuitively, for a given template, we can directly replace the placeholders to obtain a new template-based question $q ^ { * }$ , table $t ^ { * }$ , answer $a ^ { * }$ , and solution $s ^ { * }$ via the following functions:

$$
q ^ { * } = Q ( { \pmb x } ) , t ^ { * } = T ( { \pmb x } ) , a ^ { * } = A ( { \pmb x } ) , s ^ { * } = S ( { \pmb x } ) .
$$

At the same time, to achieve diverse problem generation, we introduce an LLM to paraphrase the template-based problems to adapt to real-world scenarios. For $q ^ { * } , t ^ { * } , a ^ { * }$ , and $s ^ { * }$ mentioned above, we can obtain the corresponding paraphrased question $q$ , table $t$ , answer $a$ , and solution $s$ by the LLM as follows:

$$
\begin{array} { r } { ( q , t , a , s ) = \mathrm { L L M } ( q ^ { * } , t ^ { * } , a ^ { * } , s ^ { * } ) . } \end{array}
$$

# TeLL for TMWP Generation

Overview. Figure 2 shows how the TeLL framework generates TMWPs while achieving the correctness and diversity of the generated problems. It consists of five stages: 1) Template Abstraction: abstracting the templates of mainstream TMWPs from existing datasets to build a template database;

2) Template Augmentation: using an LLM to expand the database to cover broader question types; 3) Template Selection: randomly selecting a template from the database for instantiation; 4) Template Instantiation (Equation (1)): instantiating the selected template into a problem by assigning random numbers and categories with predefined constraints; 5) Problem Paraphrasing (Equation (2)): with the support of an LLM, rewriting the problem into a problem with different contexts that conforms to human cognition. In the following, we will describe the details of each step.

Template Abstraction. We first abstract the problems and build a template database of mainstream TMWPs under the guidance of existing datasets. Each template contains a question template $Q ( { \pmb x } )$ , a table template $\bar { T } ( { \pmb x } )$ , an answer template $A ( { \pmb x } )$ , and a solution template $S ( { \pmb x } )$ . Each of the abstracted templates contains some placeholders with predefined arithmetic operations, which are then filled with specific numbers and categories during the instantiation process, thereby updating the question, table, answer, and solution accordingly. Before abstraction, we first generate the illustrative step-by-step solution, denoted by $\hat { s }$ , that corresponds to the original solution $s _ { 0 }$ within the dataset:

$$
\hat { s } = \mathrm { L L M } ( q , t , s _ { 0 } ) .
$$

Afterwards, we select a list of representative question types from the dataset and create a seed template database.

Template Augmentation. Due to the time-consuming nature of generating specific templates for each TMWP type, we propose template augmentation to generalize templates for a certain class of question types (such as questions about average calculation) to other categories with similar characteristics (such as median, mode, and range calculation questions). Specifically, we construct an LLM prompt, as shown in Figure 3, to drive the model to infer templates related to new categories (including questions, tables, answers, and solutions). This method leverages the inherent understanding and generalization capabilities of LLM to increase the diversity of generated TMWPs while improving the efficiency of template creation.

You are given a math word problem with tabular contents, and your task is to develop a versatile exercise template that can generate a wide array of exercises with a table. Please consider the   
following guidelines for this assignment:   
(1) You can use pandas, numpy, random, etc., or other packages if necessary.   
(2) You should construct a general dataframe and transform it into a human-readable table format by Python.   
(3) Generate the functions that are used to create the question, answer, and step-by-step solution based on the created table.   
This is a demonstration for <Task $1 >$ :   
<Demonstration for Task 1>   
Please generate the exercise template for <Task $2 >$ .

Template Selection and Instantiation. After the template database is built, we randomly select a template from the database for instantiation. For each generation, we first generate random numbers and their corresponding categories to maintain diversity, where the values of the numbers meet specific constraints for different types of questions, such as integers in a certain interval, non-negative numbers, etc. Then, we use the generated numbers and categories to fill in the question and table, calculate the answer, and finally, fill them into the illustrative solution.

Problem Paraphrasing. Though ensuring correctness, the template-based TMWPs lack contextual backgrounds, which limits their applicability in practical scenarios. To address this issue, we use an LLM to paraphrase these problems into a more natural and contextual form, improving their generality without sacrificing the original data and solution logic. As shown in Figure 4, the prompts for the paraphrase process include instructions, three guidelines for each component of a problem (question, table, answer, and solution), two in-context examples, and a template-based problem. After the paraphrase process, we filter out problems whose answers obtained by the solution are inconsistent with those calculated by the template. We also remove questions in the test set where at least one sample has a BLEU score greater than $\delta$ to prevent potential data leakage issues.

In summary, our approach ensures both the correctness and diversity of problems compared with previous work. First, we classify and abstract the problems into a class of templates and randomly generate a series of questions, tables, answers, and solutions through a rigorous algorithmic procedure. By regarding the template-based problems as effective supervision, this approach avoids hallucination when generating problems with LLMs. At the same time, we introduce different contexts to the problems through the LLM with high language understanding and generation capabilities, enhance the complexity and authenticity of the problems, and ensure that the description of the problems and solutions conforms to the expression habits of English. In

You need to rewrite the given math word problem to increase data diversity and semantic richness, whose questions and solutions are generated according to a uniform template. You should keep the original   
problem, data, and solution logic unchanged.   
Specific requirements are as follows:   
(1) Question (‘question‘): You need to add a   
background to the question, set the problem in a specific scenario before introducing the question, and rewrite the question without changing its   
original meaning.   
(2) Solution (‘solution‘): Since a background has been introduced to the question, the solution   
process should also be correspondingly rewritten, but the idea and logic should remain the same. You can appropriately modify any unreasonable parts in the reasoning process based on the actual situation. (3) Table Content (‘table_for_pd‘), Choices (‘   
choices‘), and Answer (‘answer‘): These parts should remain unchanged unless the keys of the tables and the choices and answers for multiple-choice   
questions.   
Here are two examples:   
<Two In-context Examples>   
Please rewrite the following problem based on the aforementioned requirements and examples:   
<Template-based Problem>

general, our method combines the accuracy of program algorithms in generating mathematical problems with the flexibility of LLMs so that high-quality and diverse TMWPs can be generated on a large scale. In addition, our method can be easily extended to new and unseen types of questions, showing its robustness and adaptability.

# Experiments

# Baselines

We compare our performance against the following baselines: (1) Heuristic Baselines include heuristic guess and human performance. (2) Fine-tuned LMs: We consider UnifiedQA, TAPEX, and TaCo under the fine-tuning setting to predict the final answers. UnifiedQA (Khashabi et al. 2020) is a T5-based model pre-trained on 8 question answering datasets of multiple formats. TAPEX (Liu et al. 2022) is a BART-based TaLM pre-trained on tabular data. TaCo (Zheng et al. 2023) coordinates two separate TaLMs for CoT generation and answer inference, respectively. We select the large version for the models. (3) Few-shot Prompting includes GPT-3 (Brown et al. 2020) and ChatGPT. (4) Few-shot CoT Prompting: We consider standard GPT3, ChatGPT, and GPT-4 (OpenAI et al. 2024). We also select the following models: PromptPG (Lu et al. 2023b) selects in-context examples for test samples with a policy gradient method. PoT (Program-of-Thoughts) (Chen et al. 2023) exploits Codex to generate the text and Python program for mathematical computations, where GPT-4 is used as the backbone model. Chameleon (Lu et al. 2023a) composes various tools, such as LLM, table verbalizer, and program generator, to accomplish the task.

Table 1: Statistics of the TabMWP dataset.   

<html><body><table><tr><td></td><td>Train</td><td>Valid</td><td>Test</td><td>Total</td></tr><tr><td>#Question</td><td>23,059</td><td>7,686</td><td>7,686</td><td>38,431</td></tr><tr><td>#Free-text</td><td>17,135</td><td>5,710</td><td>5,694</td><td>28,719</td></tr><tr><td>#MCQ</td><td>5,744</td><td>1,976</td><td>1,992</td><td>9,712</td></tr><tr><td>#Table</td><td>22,620</td><td>7,546</td><td>7,549</td><td>37,644</td></tr><tr><td>#Solution</td><td>21,623</td><td>7,365</td><td>7,378</td><td>35,442</td></tr></table></body></html>

![](images/8419b6ab06840b8daf4d30b36855bc7972f587d700d822873c1486fa51a64ddd.jpg)  
Figure 5: Question distribution of the TabMWP dataset.

# Datasets and Evaluation

We conduct evaluations on TabMWP (Lu et al. 2023b), a recent large-scale dataset containing 38, 431 grade-level MWPs with tabular context, whose statistics are presented in Table 1. It consists of two types of questions: 28, 719 free-text questions (FREE) with integer (INT) and decimal (DEC) answers, and 9, 712 multiple-choice questions (MC) with extractive text answers (EXTR), Boolean text answers (BOOL) and other text answers (OTH). Apart from the golden answers, the dataset also contains problem solutions in a free-form format. We visualize the distributions of the questions in Figure 5, among which we select 25 main question types to create our TabMWP-TeLL dataset. For evaluation, we employ exact match accuracy to evaluate overall performance and the performance with respect to each question type, and we adopt the official evaluation script to evaluate the model performance on the test set.

# Experimental Setup

We perform template augmentation and problem paraphrasing with Yi (Yi-Large-Turbo), an LLM that has close performance GPT-4 but with high cost-effectiveness. We access the model via its official API1, and we set the threshold $\delta$ for the BLEU score as 0.95. In main experiments, we fine-tune three commonly used LLMs, including Mistral-7B (Jiang et al. 2023), Qwen 2-7B (Yang et al. 2024), and Llama 3-8B (Dubey et al. 2024) with the training set of TabMWP and TabMWP-TeLL, and the evaluations are conducted on the TabMWP test set. During the fine-tuning process, we set the number of epochs as 2, the batch size per device as 12, the gradient accumulation steps as 4, and the learning rate as $2 e - 4$ . To achieve parameter-efficient fine-tuning, we adopt the QLoRA strategy (Dettmers et al. 2023) with XTuner2 All experiments are conducted on 8 NVIDIA GeForce RTX 3090 graphics cards.

# Main Results

Table 2 illustrates the comparison of our models jointly trained with TabMWP and TabMWP-TeLL against baselines. By analyzing the experimental results, we have the following observations:

(1) The fine-tuned LLMs outperform almost all baselines, except Chameleon, built with the most advanced, closedsource GPT-4. The performance hierarchy among different approaches is clearly established, with few-shot prompting being the least effective, followed by few-shot CoT prompting, and culminating in the fine-tuned LLMs demonstrating the highest efficacy. This gradient of performance highlights the significant impact of large-scale, high-quality training data and reasoning steps on the model’s capability in solving TMWPs.

(2) After incorporating TabMWP-TeLL, the performance of LLMs could be further enhanced. Specifically, Mistral, Qwen 2, and Llama 3 obtain an increase of $3 . 9 6 \%$ , $3 . 7 0 \%$ , and $3 . 7 8 \%$ , respectively. Among the experimented models, Llama 3 demonstrates substantial improvements and outperforms human performance by $7 . 8 5 \%$ , close to the performance of Chameleon by a significantly narrow margin. However, it only has $0 . 4 \%$ of parameters and without deliberated tools compared with Chameleon, underscoring the critical role of strategic augmentation of training data in elevating LLMs’ reasoning performance.

(3) TabMWP-TeLL plays a critical role in improving performance on challenging problems while maintaining performance on simple ones. Initially, LLMs demonstrate strong capabilities in handling simple questions but struggle significantly with more challenging ones. Taking GPT-3 as an example, it achieves an accuracy of $6 8 . 6 2 \%$ on problems in grades 1-6 but only $5 5 . 3 1 \%$ on questions in problems 7- 8. However, by jointly training with TabMWP-TeLL, LLMs, like Llama 3, mark substantial improvements in answering difficult questions (i.e., $9 7 . 4 2 \%$ ) without compromising on simpler ones (i.e., $9 8 . 5 7 \%$ ). This finding emphasizes the importance of high-quality data augmentation in enhancing the robustness and versatility of LLMs, thereby ensuring more consistent performance across a diverse range of question complexities.

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">#Para.</td><td colspan="2">Question Type</td><td colspan="5">Answer Type</td><td colspan="2">Grade</td><td rowspan="2">Avg.</td></tr><tr><td>FREE</td><td></td><td></td><td>DEC</td><td>EXTR</td><td>BOOL</td><td>OTH</td><td>1-6</td><td>7-8</td></tr><tr><td colspan="10">HeuristicBaselines</td></tr><tr><td>Heuristic Guess</td><td></td><td>6.71</td><td>39.81</td><td>8.37</td><td>0.26</td><td>30.80</td><td>51.22</td><td>26.67</td><td>17.55</td><td>12.27</td><td>15.29</td></tr><tr><td>Human Perform.</td><td></td><td>84.61</td><td>93.32</td><td>84.95</td><td>83.29</td><td>97.18</td><td>88.69</td><td>96.20</td><td>94.27</td><td>81.28</td><td>90.22</td></tr><tr><td colspan="10"></td></tr><tr><td>UnifiedQALARGE</td><td>770M</td><td>48.67</td><td>82.18</td><td>55.97</td><td></td><td>Fine-tuned LMs</td><td></td><td></td><td></td><td></td><td>57.35</td></tr><tr><td>TAPEXLARGE</td><td>400M</td><td>51.00</td><td>80.02</td><td>59.92</td><td>20.26 16.31</td><td>94.63 95.34</td><td>68.89 64.00</td><td>79.05 73.33</td><td>65.92 67.11</td><td>45.92 47.07</td><td>58.52</td></tr><tr><td>TaCOLARGE</td><td>800M</td><td>91.69</td><td>93.47</td><td>92.54</td><td>88.41</td><td>96.05</td><td>91.44</td><td>86.67</td><td>92.37</td><td>91.86</td><td>92.15</td></tr><tr><td colspan="10"></td></tr><tr><td>GPT-3</td><td>175B</td><td>54.69</td><td>64.11</td><td>58.36</td><td></td><td>Few-shot Prompting</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>UNK</td><td>65.84</td><td>64.61</td><td>66.55</td><td>40.40 63.09</td><td>75.95 74.67</td><td>52.41 54.67</td><td>53.02 55.24</td><td>63.10 69.75</td><td>49.16 59.88</td><td>57.13 65.52</td></tr><tr><td colspan="10">Few-shot CoT Prompting</td></tr><tr><td>Mistral</td><td>7B</td><td>47.32</td><td>48.34</td><td>47.62</td><td>46.17</td><td>48.30</td><td>48.87</td><td>44.09</td><td>47.66</td><td>47.49</td><td>47.59</td></tr><tr><td>Qwen 2</td><td>7B</td><td>67.84</td><td>68.44</td><td>68.06</td><td>66.94</td><td>68.12</td><td>69.49</td><td>62.56</td><td>67.94</td><td>68.06</td><td>67.99</td></tr><tr><td>Llama 3</td><td>8B</td><td>71.87</td><td>72.34</td><td>72.35</td><td>70.00</td><td>72.36</td><td>72.91</td><td>67.12</td><td>72.36</td><td>71.51</td><td>71.99</td></tr><tr><td>GPT-3</td><td>175B</td><td>60.76</td><td>69.09</td><td>60.04</td><td>63.58</td><td>76.49</td><td>69.19</td><td>67.30</td><td>68.62</td><td>55.31</td><td>62.92</td></tr><tr><td>ChatGPT</td><td>UNK</td><td>80.89</td><td>87.50</td><td>79.36</td><td>86.87</td><td>81.86</td><td>94.00</td><td>84.76</td><td>82.68</td><td>82.51</td><td>82.60</td></tr><tr><td>GPT-4</td><td>1.8T*</td><td>90.81</td><td>88.48</td><td>97.49</td><td>86.16</td><td>97.51</td><td>96.86</td><td>99.11</td><td>89.52</td><td>92.40</td><td>88.70</td></tr><tr><td>PromptPG</td><td>175B</td><td>66.17</td><td>74.11</td><td>64.12</td><td>74.16</td><td>76.19</td><td>72.81</td><td>65.81</td><td>71.20</td><td>64.27</td><td>68.23</td></tr><tr><td>PoT (GPT-4) Chameleon</td><td>1.8T* 1.8T*</td><td>97.40 98.95</td><td>95.58</td><td>98.48</td><td>93.22</td><td>96.25</td><td>98.00</td><td>68.57</td><td>96.97</td><td>96.87</td><td>96.93</td></tr><tr><td></td><td></td><td>98.29</td><td>99.34</td><td>97.42</td><td></td><td>98.58</td><td>98.56</td><td>93.33</td><td>98.95</td><td>98.54</td><td>98.78</td></tr><tr><td colspan="10">Fine-tuned LLMs (Trained with TabMWP)</td></tr><tr><td>Mistral</td><td>7B</td><td>90.87</td><td>97.79</td><td></td><td></td><td></td><td></td><td></td><td></td><td>92.65</td><td>92.66</td></tr><tr><td>Qwen 2</td><td>7B</td><td>92.10</td><td>97.84</td><td>90.93 92.54</td><td>90.64 90.39</td><td>97.77 96.76</td><td>99.44 99.67</td><td>83.81 92.38</td><td>92.67 94.24</td><td>92.71</td><td>93.59</td></tr><tr><td>Llama 3</td><td>8B</td><td>94.13</td><td>94.73</td><td>94.81</td><td>91.50</td><td>98.78</td><td>90.11</td><td>96.19</td><td>96.61</td><td>91.19</td><td>94.29</td></tr><tr><td colspan="10">Ours(Trainedwith TabMWP+TabMWP-TeLL)</td></tr><tr><td>Mistral</td><td>7B</td><td>96.08</td><td>98.14</td><td></td><td></td><td></td><td></td><td>89.52</td><td>96.77</td><td>96.42</td><td>96.62</td></tr><tr><td>Qwen 2</td><td>7B</td><td>97.07</td><td>97.94</td><td>96.69 97.39</td><td>93.73 95.79</td><td>98.07 97.47</td><td>99.22 99.44</td><td>89.52</td><td>97.22</td><td>97.39</td><td>97.29</td></tr><tr><td>Llama 3</td><td>8B</td><td>97.91</td><td>98.54</td><td>98.56</td><td>95.36</td><td>98.58</td><td>99.33</td><td>91.43</td><td>98.57</td><td>97.42</td><td>98.07</td></tr></table></body></html>

Table 2: Experimental results of our models trained with TabMWP and TabMWP-TeLL against baselines. “Human Perform.” denotes human performance, “UNK” denotes unknown, and \* denotes the estimated size of GPT-4 (Bambhaniya et al. 2024).

# Ablation Study

Effectiveness of TeLL for Problem Generation. We first analyze the effectiveness of TeLL, the template-driven, LLM-paraphrased TMWP generation strategy by comparing it with three additional methods: generating templatebased problems, utilizing LLMs to generate new questions and tables, and utilizing LLMs to only rewrite the questions. To ensure a fair comparison, all three methods generate 23K problems. Experimental results are organized in Table 3. The template-based problems perform the worst due to their inability to adapt to real questions with diverse backgrounds. The generated questions and tables with LLMs show subpar performance due to inaccuracies that occur within. Besides, when using LLMs only to rewrite questions, despite some performance improvements, they are limited by the unchanged numbers within the questions and tables. In contrast, our proposed method, which generates questions based on templates and LLMs, effectively balances correctness and diversity in the augmented questions, leading to superior results on TMWP solving.

Effectiveness of Illustrative Solutions. We then analyze the efficacy of the illustrative solutions compared with the original free-form solutions for TMWP solving by finetuning the LLMs with the same questions but with different solution types, i.e., free-form solutions and the illustrative solutions, in which the illustrative solutions are constructed by LLMs given the free-form solutions. As shown in Table 4, the model trained with free-form solutions performs worse due to the lack of detail and structure. Our proposed illustrative solutions ultimately produce the best performance, indicating that the step-by-step solutions offer a more systematic and detailed approach, thus significantly improving the model performance.

Effectiveness across Multiple Question Types. We also examine the accuracy of various question types by training Llama 3 on three distinct datasets: TabMWP, TabMWPTeLL, and a mix of both. As depicted in Table 5, the models trained solely on TabMWP demonstrate competence in solving straightforward problems, such as calculating mean and median; however, they struggle with more complex tasks, particularly those involving stem-and-leaf plots. For instance, the performance on three types of stem-leaf plots is only $6 6 . 6 7 \%$ , $6 9 . 4 6 \%$ , and $6 8 . 9 2 \%$ , respectively. In contrast, the model trained on TabMWP-TeLL exhibits superior performance, showing an average improvement of $1 \mathrm { { i . 9 2 \% } }$ on the sampled stem-leaf plot problems, attributed to the inclusion of more detailed and illustrative solutions in our generated dataset, which enhanced the models’ multi-step reasoning capabilities.

Table 3: Comparison of different TMWP generation methods. $\Delta$ calculates the overall improvement.   

<html><body><table><tr><td>Model</td><td>FREE</td><td>MC</td><td>Overall</td><td>△</td></tr><tr><td>Mistral</td><td>90.87</td><td>97.79</td><td>92.66</td><td></td></tr><tr><td>w/Template</td><td>91.41</td><td>98.29</td><td>93.20</td><td>0.54</td></tr><tr><td>w/Ques.+Table</td><td>91.27</td><td>97.64</td><td>92.92</td><td>0.26</td></tr><tr><td>w/ Ques.Only</td><td>92.48</td><td>98.64</td><td>94.08</td><td>1.42</td></tr><tr><td>Ours</td><td>96.08</td><td>98.14</td><td>96.62</td><td>3.96</td></tr><tr><td>Qwen 2</td><td>92.10</td><td>97.84</td><td>93.59</td><td></td></tr><tr><td>w/Template</td><td>92.20</td><td>98.19</td><td>93.75</td><td>0.16</td></tr><tr><td>w/Ques.+Table</td><td>92.48</td><td>97.79</td><td>93.86</td><td>0.27</td></tr><tr><td>w/ Ques. Only</td><td>93.47</td><td>97.84</td><td>94.60</td><td>1.01</td></tr><tr><td>Ours</td><td>97.07</td><td>97.94</td><td>97.29</td><td>3.70</td></tr><tr><td>Llama 3</td><td>94.13</td><td></td><td></td><td></td></tr><tr><td>w/Template</td><td>93.08</td><td>94.73</td><td>94.29</td><td></td></tr><tr><td></td><td></td><td>97.99</td><td>94.35</td><td>0.06</td></tr><tr><td>w/Ques.+Table</td><td>93.26</td><td>98.24</td><td>94.55</td><td>0.26</td></tr><tr><td>w/ Ques. Only</td><td>94.33</td><td>98.69</td><td>95.46</td><td>1.17</td></tr><tr><td>Ours</td><td>97.91</td><td>98.54</td><td>98.07</td><td>3.78</td></tr></table></body></html>

Table 4: Performance of models trained on free-form and illustrative solutions.   

<html><body><table><tr><td>Solution</td><td>Mistral</td><td>Qwen 2</td><td>Llama 3</td></tr><tr><td>Free-form</td><td>92.66</td><td>93.59</td><td>94.29</td></tr><tr><td>Illustrative</td><td>94.29</td><td>95.86</td><td>96.21</td></tr></table></body></html>

Table 5: Experimental results of Llama 3 trained with TabMWP, TabMWP-TeLL, and the combination of both.   

<html><body><table><tr><td>Type</td><td>#Problem</td><td>TableMWP</td><td>TeLL</td><td>Combination</td></tr><tr><td>3</td><td>273</td><td>66.67</td><td>83.88</td><td>94.14</td></tr><tr><td>6</td><td>501</td><td>69.46</td><td>73.65</td><td>96.51</td></tr><tr><td>8</td><td>502</td><td>68.92</td><td>83.27</td><td>95.82</td></tr><tr><td>10</td><td>100</td><td>100.00</td><td>100.00</td><td>100.00</td></tr><tr><td>11</td><td>110</td><td>100.00</td><td>100.00</td><td>100.00</td></tr><tr><td>22</td><td>266</td><td>98.12</td><td>96.24</td><td>98.12</td></tr><tr><td>23</td><td>280</td><td>100.00</td><td>99.64</td><td>100.00</td></tr><tr><td>25</td><td>281</td><td>100.00</td><td>99.64</td><td>100.00</td></tr><tr><td>Avg.</td><td>2313</td><td>82.49</td><td>88.24</td><td>97.45</td></tr></table></body></html>

Effects of the Size of Augmented Data. We further investigate the effects of the size of augmented TabMWPTeLL data with the TabMWP dataset on LLMs’ TMWPsolving performance. As shown in Figure 6, incorporating more TabMWP-TeLL data consistently contributes to the performance across all models, even with smaller amounts of augmented data. Among these, a small amount of data (e.g., $\bar { 2 } 0 \%$ ) leads to a substantial improvement in model performance. As the data volume increases, the improvement rate slows, but the model continues to benefit from the generated data. This finding not only demonstrates the usefulness of additional augmented data in enhancing TMWP results but also highlights the critical importance of high-quality data in achieving superior model performance.

![](images/7f95932ddf4c6d8bf6adb71ed32222357130cd69642622174ea9479f194a1894.jpg)  
Figure 6: Effects of the proportion of TabMWP-TeLL on TMWP-solving performance.

Human Verification. To guarantee the quality of our generated TMWPs, we sample 1, 000 examples from TabMWPTeLL for human verification. Generally, the generated data achieve high correctness, with a rate of $9 7 . 5 \%$ . We categorize the generation errors into three primary types during the problem paraphrasing step: incomplete paraphrased questions (LLMs may generate questions that are incomplete and unanswerable), incorrect paraphrased solutions (LLMs may generate solutions that are not logically correct with hallucination issues), and grammatical errors. We will address these errors in our future work.

# Conclusion and Future Work

We introduce TeLL, a template-driven, LLM-paraphrased framework to generate high-quality TMWPs by leveraging both templates and LLMs. The framework consists of five steps, including abstraction, augmentation, selection, instantiation of templates, and the paraphrasing of problems. We construct a high-quality dataset called TabMWP-TeLL by adhering to the question types in the TabMWP dataset with illustrative step-by-step solutions, and we conduct experiments with three commonly used LLMs. Experimental results demonstrate the effectiveness of TabMWP-TeLL in improving TMWP solving by incorporating it with TabMWP, making LLMs’ performance outperform the baselines, and it is particularly effective in improving performance on challenging problems while maintaining the performance on simple ones. In the future, we will generalize our method to more complex reasoning tasks, such as commonsense reasoning and multi-hop reasoning.