# Gradient Weight-normalized Low-rank Projection for Efficient LLM Training

Jia-Hong Huang\*, Yixian Shen\*, Hongyi Zhu, Stevan Rudinac, Evangelos Kanoulas

University of Amsterdam j.huang@uva.nl, y.shen@uva.nl, h.zhu $@$ uva.nl, s.rudinac@uva.nl, E.Kanoulas@uva.nl

# Abstract

Large Language Models (LLMs) have shown remarkable performance across various tasks, but the escalating demands on computational resources pose significant challenges, particularly in the extensive utilization of full fine-tuning for downstream tasks. To address this, parameter-efficient finetuning (PEFT) methods have been developed, but they often underperform compared to full fine-tuning and struggle with memory efficiency. In this work, we introduce Gradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach that enhances both parameter and memory efficiency while maintaining comparable performance to full fine-tuning. GradNormLoRP normalizes the weight matrix to improve gradient conditioning, facilitating better convergence during optimization. Additionally, it applies low-rank approximations to the weight and gradient matrices, significantly reducing memory usage during training. Extensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer memory usage by up to $8 9 . 5 \%$ and enables the pre-training of large LLMs, such as LLaMA 7B, on consumerlevel GPUs like the NVIDIA RTX 4090, without additional inference costs. Moreover, GradNormLoRP outperforms existing low-rank methods in fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all GLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65, surpassing LoRA’s score of 79.23. These results underscore GradNormLoRP as a promising alternative for efficient LLM pre-training and fine-tuning.

Code — https://github.com/Jhhuangkay/Gradient-Weightnormalized-Low-rank-Projection-for-Efficient-LLMTraining Extended Version — https://arxiv.org/pdf/2412.19616

# Introduction

Large Language Models (LLMs) pre-trained on extensive datasets have demonstrated exceptional effectiveness across various domains (Devlin et al. 2019; Liu et al. 2019; He et al. 2022; Xie et al. 2022; Baevski et al. 2020; Lu et al. 2019; Tan and Bansal 2019). As time progresses, open-source LLMs have consistently improved in their capabilities, accompanied by a striking increase in the scale of pre-trained models (Raffel et al. 2020a; Zhang et al. 2022; Le Scao et al. 2023; Touvron et al. 2023; Tay et al. 2023). Consequently, employing full fine-tuning, where all learnable parameters

# Algorithm 1: Our proposed GradNormLoRP

Require: Weight matrix $\boldsymbol { \mathcal { W } }$   
Ensure: Updated weightWmatrix $\mathcal { W } _ { \mathrm { u p d a t e d } }$ 1: Normalize each column weight vector of $\boldsymbol { \mathcal { W } }$ to get norm 2: Apply LoRA with two low-rank matrices $I$ and $J$ to reformulate ${ \mathcal { W } } _ { \mathrm { { n o r m } } }$ 3: Initialize two sets of low-rank projection matrices $( U _ { I } , V _ { I } )$ and $( U _ { J } , V _ { J } )$ 4: for $i = 1$ to $N$ do 5: Compute gradient matrices $\mathcal { Z } _ { I }$ and $\mathcal { H } _ { J }$ based on $I$ and $\dot { J }$ 6: Project $\mathcal { Z } _ { I }$ and $\mathcal { H } _ { J }$ using $( U _ { I } , V _ { I } )$ and $( U _ { J } , V _ { J } )$ 7: if $i$ is a multiple of 250 then 8: Update $( \dot { U _ { I } } , V _ { I } )$ and $( U _ { J } , V _ { J } )$ 9: end if   
10: end for   
11: return ${ \mathcal { W } } _ { \mathrm { u p d a t e d } } ( { \mathcal { W } } _ { \mathrm { n o r m } } , { \mathcal { Z } } _ { I } , ( U _ { I } , V _ { I } ) , { \mathcal { H } } _ { J } , ( U _ { J } , V _ { J } ) )$

of a pre-trained model are updated for performing downstream tasks, poses unparalleled challenges despite its track record of delivering numerous state-of-the-art results. These challenges primarily stem from the escalating demands on computational resources.

To tackle the aforementioned challenge, researchers have developed parameter-efficient fine-tuning (PEFT) techniques (Houlsby et al. 2019; Hu et al. 2022; Lialin et al. 2023; Liu et al. 2024; Kopiczko, Blankevoort, and Asano 2024). These methods are tailored to update only a small amount of taskspecific parameters while leaving the majority of the model’s parameters unchanged. Among these techniques, low-rank approximation-based approaches utilize low-rank matrices to approximate weight changes during training, achieving both parameter and memory efficiency without requiring additional trainable subnetworks to be added to the original model architecture.

Despite their advantages, low-rank-based methods often underperform compared to full-rank fine-tuning (Hu et al. 2022; Lialin et al. 2023; Liu et al. 2024). This performance gap is typically attributed to the reduced number of trainable parameters, but other underlying factors, such as altered gradient dynamics due to reparameterization, also play a significant role. In Figure 2 of our Extended Version, we observe that the gradient descent process can become neither smooth nor stable when fine-tuning LLMs in an unnormalized subspace. This instability arises from conducting gradient descent on an incomparable scale, where some values are excessively large or small. Such numerical instability can lead to overflow or underflow during computations, negatively impacting the optimization process and resulting in suboptimal performance. To mitigate this problem, we propose our method, Gradient Weight-Normalized Low-Rank Projection (GradNormLoRP). This approach effectively enhances both parameter and memory efficiency. GradNormLoRP improves parameter efficiency by incorporating a weight matrix normalization process that represents each column vector of the weight matrix as the product of its magnitude and unit vector. This normalization enhances gradient conditioning and facilitates better convergence during optimization.

In addition to improving parameter efficiency, GradNormLoRP addresses memory efficiency while maintaining performance comparable to full fine-tuning without introducing additional inference burden. Although training LLMs in a normalized subspace enhances convergence during optimization, existing PEFT methods (Hu et al. 2022; Liu et al. 2024; Houlsby et al. 2019; Pfeiffer et al. 2020; Li and Liang 2021; Lester, Al-Rfou, and Constant 2021; Liu et al. 2022) still face limitations in reducing GPU memory usage. Specifically, these methods rely on caching intermediate activations during the forward pass to compute gradients, which remains a significant memory overhead due to the standard backpropagation process. This inefficiency poses difficulties for training LLMs on a single consumer-level GPU, such as the NVIDIA RTX 4090 with 24GB of memory. To address the memory efficiency issue, GradNormLoRP applies a low-rank approximation technique to both the normalized weight matrix and its corresponding gradient matrix. This process involves reformulating the normalized weight matrix as the sum of a fixed pre-trained weight matrix and the product of two lowrank matrices. It also requires computing two sets of low-rank projection matrices to project the gradient matrices derived from these low-rank matrices. These projection matrices are updated periodically, e.g., every 250 iterations, to ensure minimal additional computational overhead over time. Exploiting this technique, our proposed GradNormLoRP achieves both memory and parameter efficiency during training while further enhancing the convergence process of optimization in the normalized subspace.

We conduct extensive experiments to demonstrate the effectiveness of our proposed GradNormLoRP in both LLM pre-training and fine-tuning, leveraging the C4 dataset (Raffel et al. 2020b) and the GLUE benchmark (Wang et al. 2019). GradNormLoRP significantly reduces memory usage in optimizer states by up to $89 . 5 \%$ , while preserving efficiency and performance during pre-training on the LLaMA 7B (Touvron et al. 2023) architecture with the C4 dataset, comprising up to 10.2 billion tokens. Furthermore, our 8-bit GradNormLoRP achieves additional reductions, cutting optimizer memory by up to $8 3 . 7 \%$ and total training memory by $6 5 . 2 \%$ compared to a BF16 baseline. Remarkably, we demonstrate the feasibility of pre-training the LLaMA 7B model on consumer-level GPUs with 24GB memory, such as the NVIDIA RTX 4090, without necessitating strategies like model parallelism, offloading, or checkpointing. In the realm of fine-tuning pre-trained LLMs on GLUE benchmarks, GradNormLoRP proves superior to existing low-rank methods. For example, when fine-tuning the RoBERTaBase model (Liu et al. 2019) on GLUE tasks with a rank of 8, GradNormLoRP attains an average score of 80.65, outpacing LoRA, which achieves a score of 79.23. The effectiveness of GradNormLoRP is also mathematically proved by our proposed

Theorem 1. This highlights GradNormLoRP as a promising alternative to established methodologies within the field. The main contributions of this paper are as follows:

• Development of GradNormLoRP for Enhanced LLM Training: We introduce GradNormLoRP, a novel method designed to improve parameter and memory efficiency during the pre-training and fine-tuning of LLMs. GradNormLoRP enhances gradient conditioning, leading to better convergence during optimization while maintaining performance comparable to full fine-tuning.

• Memory Efficiency on Consumer-Level GPUs: GradNormLoRP addresses the memory efficiency limitations of existing PEFT methods. Through the application of low-rank approximation to both normalized weight matrices and their corresponding gradient matrices, the method substantially reduces memory usage in optimizer states, enabling the training of LLMs on consumer-level GPUs without the need for advanced memory management strategies.

• Empirical and Theoretical Validation of GradNormLoRP’s Effectiveness: The effectiveness of GradNormLoRP is demonstrated through both theoretical analysis and extensive experimental evaluation. We provide a mathematical proof of GradNormLoRP’s effectiveness, further solidifying its potential as a promising alternative to traditional fine-tuning approaches in the LLM domain.

# Related Work

Parameter-Efficient Fine-Tuning. Numerous PEFT methods have emerged to address the computational challenges of fully fine-tuning LLMs. These methods can be grouped into those that increase model complexity and those that maintain or minimally modify the initial architecture. The first group, including methods like (Liao, Tan, and Monz 2023; Zhao et al. 2024a; Houlsby et al. 2019; Rebuffi, Bilen, and Vedaldi 2017; Gomez et al. 2017a; Pfeiffer et al. 2020; Rücklé et al. 2020; Li and Liang 2021; Lester, Al-Rfou, and Constant 2021; Hambardzumyan, Khachatrian, and May 2021; Liu et al. 2023), often incorporate trainable adapter layers or optimize input layer activations, which can add inference latency and pose challenges in large-scale, latency-sensitive environments. The second group of methods, including (Liu et al. 2024; Hu et al. 2022; Lialin et al. 2023), utilizes lowrank matrices to approximate weight changes during training. These low-rank matrices are designed to integrate seamlessly with pre-trained weights before inference, ensuring that no additional inference overhead is introduced. Our proposed GradNormLoRP belongs to this second category, leveraging the advantages of low-rank approximation methods without introducing extra inference latency.

Gradient Projection. Gradient projection is used for rapid low-rank estimation (Chen and Wainwright 2015; Chen, Raskutti, and Yuan 2019; Zhao et al. 2024b). The work in (Chen and Wainwright 2015; Chen, Raskutti, and Yuan 2019) treats the objective function as a general non-linear function, analyzing gradients in vector space. GaLore (Zhao et al. 2024b), however, considers the specific structures of gradients in multi-layer neural networks, establishing that gradients tend to become low-rank during training and exhibit specific convergence behaviors. Further studies (Larsen et al. 2022; Gur-Ari, Roberts, and Dyer 2018) demonstrate that effective learning often takes place in a low-dimensional subspace, optimizing model weights within this constrained space—a process known as subspace learning. Our proposed

GradNormLoRP advances this concept by operating within a low-dimensional normalized subspace, enhanced by weight matrix normalization.

GPU-Memory-Efficient Training. Several techniques have been developed to optimize GPU memory utilization during LLM training. Reversible subnetworks (Liao, Tan, and Monz 2023; Mangalam et al. 2022; Zhao et al. $2 0 2 4 \mathrm { a }$ ; Gomez et al. 2017b; Kitaev, Kaiser, and Levskaya 2020) minimize activation memory by recalculating activations during backpropagation. Gradient checkpointing (Chen et al. 2016) improves memory efficiency by discarding and later reconstructing some intermediate activations through an additional forward pass. Pruning (Frankle and Carbin 2019; Frankle et al. 2020) and knowledge distillation (Sanh et al. 2020; Hinton, Vinyals, and Dean 2015; Koratana et al. 2019) compress models by removing redundant parameters or transferring distilled knowledge. Using pre-trained models as feature extractors without gradient computation also reduces activation memory (Liu, An, and Qiu 2024; Sung, Cho, and Bansal 2022). Quantization reduces optimizer state memory overhead (Dettmers et al. 2022; Li, Chen, and Zhu 2023). Fused gradient calculation (Lv et al. 2023) alleviates memory overhead from storing weight gradients, and Adafactor (Shazeer and Stern 2018) reduces memory costs by factorizing second-order statistics. Unlike these approaches, GradNormLoRP provides optimizers with low-rank gradients directly, eliminating the need for full-rank gradient knowledge.

# Methodology

In this section, we detail the key components of our GradNormLoRP and establish a theorem that theoretically demonstrates the effectiveness of GradNormLoRP in preserving the integrity of training dynamics. Please consult Algorithm 1 for a more comprehensive grasp of our GradNormLoRP.

# Background

Weight Vector Normalization. Weight vector normalization is a technique that can be employed to expedite the convergence of the stochastic gradient descent optimization process (Srebro and Shraibman 2005; Salimans and Kingma 2016). We consider standard neural networks in which each neuron’s computation involves calculating a weighted sum of input features, followed by a component-wise non-linearity:

$$
y = \theta ( ( \sum _ { i = 1 } ^ { k } w _ { i } a _ { i } ) + b ) = \theta ( \langle w , a \rangle + b ) ,
$$

where $w \in \mathbb { R } ^ { k \times 1 }$ represents a weight vector, $\boldsymbol { a } \in \mathbb { R } ^ { k \times 1 }$ signifies an input feature vector, $b \in \mathbb { R }$ indicates a bias term, $\langle \bar { \cdot } , \cdot \rangle$ denotes the inner product, $\theta ( \cdot )$ is an component-wise nonlinearity, e.g., the logistic activation $\frac { \exp ( \cdot ) } { 1 + \exp ( \cdot ) }$ , and $y$ indicates the scalar output of the neuron.

After a loss function is associated with one or more neuron outputs, the parameters $w$ and $b$ for each neuron are typically optimized using stochastic gradient descent during the training of such a neural network. To enhance the convergence of the optimization process, a reparameterization operation is introduced to express each weight vector $w$ in terms of a parameter vector $v$ and a scalar parameter $\delta$ :

$$
w = \delta \frac { v } { \| v \| } ,
$$

where $\delta \in \mathbb { R }$ denotes a scalar, $v \in \mathbb { R } ^ { k \times 1 }$ , and $\| \cdot \|$ indicates the Euclidean norm.

This reparameterization, which decouples the weight vector’s norm $( \dot { \delta } )$ from the direction of the weight vector $\overline { { ( \frac { v } { \| v \| } ) } }$ , fixes the Euclidean norm of the weight vector, yielding $\lvert \lvert w \rvert \rvert =$ $\delta$ , which remains independent of the parameter vector $v$ . After employing the reparameterization weight normalization process, we obtain:

$$
y = \theta ( \langle w , a \rangle + b ) = \theta ( \langle \delta \frac { v } { \| v \| } , a \rangle + b ) .
$$

Subsequently, the optimization process of stochastic gradient descent is conducted to the new parameters $v$ and $\delta$ instead. In our proposed GradNormLoRP approach, we conduct the operation of reparameterization weight normalization on each column weight vector of a given weight matrix, resulting in a normalized weight matrix.

Challenges in Memory Efficiency for PEFT. As discussed in (Raffel et al. 2020a; Zhao et al. 2024b; Liao, Tan, and Monz 2023; Touvron et al. 2023), the primary memory consumption during neural network training is attributed to activations, trainable parameters, and gradients of these parameters, along with optimizer states such as gradient momentum and variance in Adam (Kingma and Ba 2017). In this subsection, we employ a T-layer multilayer perceptron to illustrate the main origin of the memory efficiency issue inherent in low-rank approximation-based PEFT methods. Consider a T-layer multilayer perceptron: $h _ { T } = \xi _ { T } ( \xi _ { T - 1 } ( . . . ( \xi _ { 2 } ( \xi _ { 1 } ( h _ { 0 } ) ) ) \cdot \cdot . . ) )$ with $\breve { h } _ { 0 }$ as the initial input, where the $t ^ { \mathrm { t h } }$ layer $h _ { t } = \xi _ { t } ( h _ { t - 1 } ) = \phi _ { t } ( W _ { t } h _ { t - 1 } )$ comprises a nonlinear function $\phi _ { t }$ and a weight matrix $W _ { t }$ , neglecting the bias term for simplicity. Let $\mathsf { \bar {psi } } _ { t } = W _ { t } h _ { t - 1 }$ . During the process of backpropagation with a loss $\mathcal { L }$ , the gradient of $\bar { W } _ { t }$ is computed using the chain rule as:

$$
\begin{array} { c } { \displaystyle \frac { \partial \mathcal { L } } { \partial W _ { t } } = \frac { \partial \mathcal { L } } { \partial h _ { T } } \left( \prod _ { i = t + 1 } ^ { T } \frac { \partial h _ { i } } { \partial \psi _ { i } } \frac { \partial \psi _ { i } } { \partial h _ { i - 1 } } \right) \frac { \partial h _ { t } } { \partial \psi _ { t } } \frac { \partial \psi _ { t } } { \partial W _ { t } } } \\ { \displaystyle = \frac { \partial \mathcal { L } } { \partial h _ { T } } \left( \prod _ { i = t + 1 } ^ { T } \phi _ { i } ^ { \prime } W _ { i } \right) \phi _ { t } ^ { \prime } h _ { t - 1 } , } \end{array}
$$

where $\begin{array} { r } { \frac { \partial h _ { i } } { \partial \psi _ { i } } = \phi _ { i } ^ { \prime } } \end{array}$ , $\begin{array} { r } { \frac { \partial \psi _ { i } } { \partial h _ { i - 1 } } = W _ { i } } \end{array}$ , $\begin{array} { r } { \frac { \partial h _ { t } } { \partial \psi _ { t } } = \phi _ { t } ^ { \prime } } \end{array}$ , and $\begin{array} { r } { \frac { \partial \psi _ { t } } { \partial W _ { t } } = h _ { t - 1 } } \end{array}$ . Since $\phi _ { t } ^ { \prime }$ represents the derivative of $\phi _ { t }$ and the computation of $\phi _ { t } ^ { \prime }$ relies on $\psi _ { t }$ , caching the sequence of activations $\{ \psi _ { i } \} _ { i = t } ^ { T }$ during the forward pass is essential to compute the gradient of $W _ { t }$ , even though $\{ W _ { i } \} _ { i > t }$ remain frozen. In contrast to full fine-tuning, existing low-rank approximationbased PEFT methods adjust only a limited number of parameters, resulting in a negligible size of the optimizer state (Liu et al. 2024; Hu et al. 2022; Lialin et al. 2023; Kopiczko, Blankevoort, and Asano 2024). Nevertheless, there is no significant reduction in the memory consumption required for activations. Take $\mathbf { B E R T _ { b a s e } }$ fine-tuned on the RTE benchmark with a batch size of 64 and sequence length of 512: the PEFT methods still require over $7 5 \%$ of the activation memory used in full fine-tuning, even though their trainable parameters are reduced to less than $1 \%$ (Devlin et al. 2018; Liao, Tan, and Monz 2023; Bentivogli et al. 2009).

# Our Proposed GradNormLoRP

Gradient Projection. The efficacy of existing low-rank approximation-based PEFT approaches, such as LoRA (Hu et al. 2022), often falls short in comparison to full fine-tuning, primarily due to their limited number of trainable parameters and the potential change of gradient training dynamics resulting from the low-rank reparameterization process (Xia, Qin, and Hazan 2024; Zhao et al. 2024b; Kopiczko, Blankevoort, and Asano 2024). A promising avenue to mitigate this challenge is through gradient projection techniques (Chen and Wainwright 2015; Chen, Raskutti, and Yuan 2019; Zhao et al. 2024b). The core concept behind gradient projection is to leverage the gradual evolution of the low-rank structure within the gradient of a weight matrix, instead of directly approximating the weight matrix as done in the LoRA method. This principle is grounded on the claim that the gradient tends to exhibit low-rank characteristics as training progresses. In this subsection, we substantiate this claim through rigorous proof.

Weight Matrix Updates in Conventional Full-Rank Training. Given $\mathcal { D } _ { t } = \mathcal { \bar { \mathbf { \Gamma } } } _ { - \nabla _ { \mathcal { W } } } \mathcal { L } ( \mathcal { W } _ { t } ) \in \mathbb { R } ^ { k \times m }$ as the representation of the backpropagated negative gradient matrix at time step $t$ , the traditional pre-training weight update with a learning rate $\alpha$ can be expressed as follows:

$$
\mathcal { W } _ { T } = \mathcal { W } _ { 0 } + \alpha \sum _ { t = 0 } ^ { T - 1 } \widetilde { \mathcal { D } } _ { t } = \mathcal { W } _ { 0 } + \alpha \sum _ { t = 0 } ^ { T - 1 } \eta _ { t } ( \mathcal { D } _ { t } ) ,
$$

where $\widetilde { \mathcal { D } } _ { t }$ represents the final processed gradient added to the w geht matrix, and $\eta _ { t }$ denotes a component-wise stateful gradient regularizer, such as Adam.

Weight Matrix Updates in Low-Rank ApproximationBased Methods. For a linear layer with a weight matrix $\mathcal { W } \in \mathbb { R } ^ { k \times m }$ , approaches, such as LoRA, which are based on low-rank approximation, leverage the low-rank structure of the update matrix by introducing a low-rank adaptor $I J$ .

$$
\mathcal { W } _ { T } = \mathcal { W } _ { 0 } + I _ { T } J _ { T } ,
$$

where $\boldsymbol { I } \in \mathbb { R } ^ { k \times r }$ , $J \in \mathbb { R } ^ { r \times m }$ , and $r \ll \operatorname* { m i n } ( k , m )$ . $I$ and $J$ denote the trainable low-rank adaptors, while ${ \mathcal { W } } _ { 0 }$ stands as a fixed weight matrix, such as a pre-trained weight matrix.

While low-rank updates are suggested to alleviate memory consumption, there is ongoing debate regarding whether the weight matrix should inherently adopt a low-rank parameterization. This assumption may not hold in various scenarios, such as linear regression. However, the gradient often exhibits low-rank characteristics during training, especially with specific gradient forms and associated network architectures (Zhao et al. 2024b). The proof for Lemma 1 is available in our Extended Version.

Lemma 1 (Gradient Becoming Low-rank during Training). Given $\mathcal { W } _ { t } \in \mathbb { R } ^ { k \times m }$ , where we assume $k \leq m$ without loss of generality. Consider the gradient matrix $\mathcal { D } _ { t } =$ $\mathbf { \nabla } \mathcal { A } - B \mathcal { W } _ { t } \mathcal { C }$ , where $\mathcal { A }$ denotes a constant matrix, $\boldsymbol { B }$ and $\mathcal { C }$ both are positive semidefinite $( P S D )$ matrices, and ${ \mathcal { W } } _ { 0 }$ is randomly initialized. Then, the gradient in the update of weight matrix $\mathcal { W } _ { t } = \mathcal { W } _ { t - 1 } + \alpha \mathcal { D } _ { t - 1 }$ results in low-rank gradient with high probability:

$$
\mathrm { s t a b l e - r a n k } ( \mathcal { D } _ { t } ) \leq 1 + \sum _ { i = 2 } ^ { k } O \left( \left( \frac { 1 - \alpha \lambda _ { i } \nu _ { 1 } } { 1 - \alpha \lambda _ { 1 } \nu _ { 1 } } \right) ^ { 2 t } \right) ,
$$

where $\nu _ { 1 } = \lambda _ { \operatorname* { m i n } } ( \mathcal { C } )$ is the smallest eigenvalue of $\mathcal { C }$ and $\lambda _ { 1 } \ \leq \ . \ . . \ \leq \ \lambda _ { m }$ are eigenvalues of $\boldsymbol { B }$ . Moreover, if $\mathcal { C }$ is positive definite, i.e., $\nu _ { 1 } > 0$ , and $\dot { \lambda _ { 2 } } > \lambda _ { 1 }$ , $\mathcal { D } _ { t }$ converges exponentially to rank-1.

Normalization of Weight Matrix. The initial phase of our proposed GradNormLoRP involves normalizing a provided weight matrix $\mathcal { W }$ . This normalization entails reparameterizing each column vector of the weight matrix using the operation introduced in section “Weight Vector Normalization”. The normalization process of the weight matrix $\mathcal { W } \in \mathbb { R } ^ { k \times m }$ can be expressed as follows:

$$
\mathcal { W } = | | \mathcal { W } | | _ { c } \frac { \mathcal { W } } { | | \mathcal { W } | | _ { c } } = \mathcal { M } \frac { \mathcal { W } } { | | \mathcal { W } | | _ { c } } ,
$$

where $\mathcal { M } \in \mathbb { R } ^ { 1 \times m }$ indicates the reparameterized, i.e., trainable, length vector, $\boldsymbol { \mathcal { W } } / | | \boldsymbol { \mathcal { W } } | | _ { c } \in \mathbb { R } ^ { \bar { k } \times m }$ represents the directional matrix, and $| | . | | _ { c }$ denotes the vector-wise matrix norm operated across each column.

After performing the reparameterization weight normalization operation column-wise on the weight matrix, we have disentangled the magnitude of the weight vectors from their direction. This process ensures that each column of $\mathcal { W } / | | \mathcal { W } | | _ { c }$ becomes a unit vector with an associated scalar. Each scalar element in vector $\mathcal { M }$ represents the length of a corresponding vector in weight matrix $\mathcal { W }$ .

Low-rank Approximation. The proposed GradNormLoRP is initialized with pre-trained weight ${ \mathcal { W } } _ { 0 }$ as shown in Equation (8), where $\dot { \mathcal { M } } = | | \mathcal { W } _ { 0 } | | _ { c }$ and $\mathcal { W } = \mathcal { W } _ { 0 }$ after initialization. Subsequently, we freeze $\mathcal { W }$ while making $\mathcal { M }$ serve as a trainable vector. The directional matrix is then updated using low-rank approximation techniques, such as LoRA. GradNormLoRP can be formulated similarly to Equation (6) as follows:

$$
\mathcal { W } = \mathcal { M } \frac { \mathcal { W } _ { 0 } + I J } { | | \mathcal { W } _ { 0 } + I J | | _ { c } } ,
$$

where $\mathcal { M }$ represents a vector comprising trainable parameters, while the weight matrices $\ b { I } \in \mathbb { R } ^ { k \times r }$ and $J \in \mathbb { R } ^ { r \times m }$ are initialized following LoRA’s approach to guarantee that $\boldsymbol { \mathcal { W } }$ equals ${ \mathcal { W } } _ { 0 }$ before fine-tuning.

As the introduced low-rank approximation in our GradNormLoRP can be merged with the pre-trained weight before inference, it does not introduce any additional latency in the inference phase.

Gradient Projection Process. To enhance the convergence of the optimization process while simultaneously reducing memory usage during training, we integrate the gradient projection technique introduced in section “Gradient Projection” into our proposed GradNormLoRP.

Singular Value Decomposition (SVD) and Projection Matrices. In this study, we utilize SVD to obtain projection matrices that serve the purpose of gradient projection for the gradient matrix $\mathcal { D } _ { t }$ :

$$
\mathcal { D } _ { t } = \boldsymbol { U } \boldsymbol { S } \boldsymbol { V } ^ { \top } \approx \sum _ { i = 1 } ^ { r } s _ { i } u _ { i } \boldsymbol { v } _ { i } ^ { \top } .
$$

Let ${ \mathcal U } _ { t } = [ u _ { 1 } , u _ { 2 } , . . . , u _ { r } ]$ and $\mathcal { V } _ { t } = [ v _ { 1 } , v _ { 2 } , . ~ . ~ . , v _ { r } ]$ denote projection matrices. Then, $\tilde { \mathcal { D } } _ { t }$ in Equation (5) can be expressed as follows:

$$
\begin{array} { r } { \tilde { \mathcal { D } } _ { t } = \mathcal { U } _ { t } \eta _ { t } ( \mathcal { U } _ { t } ^ { \top } \mathcal { D } _ { t } \mathcal { V } _ { t } ) \mathcal { V } _ { t } ^ { \top } . } \end{array}
$$

As per Lemma 1 , the gradient $\mathcal { D }$ may exhibit a low-rank structure. Therefore, by preserving the gradient statistics of a compact “key portion” of $\mathcal { D }$ in optimizer states instead of $\mathcal { D }$ itself, significant reductions in memory consumption can be achieved. This motivates the gradient projection strategy integrated into our proposed GradNormLoRP.

Definition 1 (Gradient Projection in GradNormLoRP). The gradient projection strategy in our proposed GradNormLoRP, with a learning rate $\alpha _ { ; }$ , follows these gradient update rules:

$$
\begin{array} { r l r } { \ } & { \boldsymbol { \mathcal { W } } _ { t } = \boldsymbol { \mathcal { W } } _ { 0 } + \alpha \displaystyle \sum _ { t = 0 } ^ { T - 1 } \tilde { \mathcal { Z } } _ { t } \tilde { \boldsymbol { \mathcal { H } } } _ { t } , \tilde { \mathcal { Z } } _ { t } = P _ { t } \eta _ { t } \big ( P _ { t } ^ { \top } \mathcal { Z } _ { t } Q _ { t } \big ) Q _ { t } ^ { \top } , } & \\ { \boldsymbol { \tilde { \mathcal { H } } } _ { t } = \mathcal { P } _ { t } \eta _ { t } \big ( \mathcal { P } _ { t } ^ { \top } \mathcal { H } _ { t } \mathcal { Q } _ { t } \big ) \boldsymbol { \mathcal { Q } } _ { t } ^ { \top } , } & { \quad ( } \end{array}
$$

where $\mathcal { W } _ { 0 } \in \mathbb { R } ^ { k \times m }$ denotes the initial weight matrix; $\mathcal { Z } _ { t } \ \in \ \mathbb { R } ^ { k \times r }$ and $\mathcal { H } _ { t } \ \in \ \mathbb { R } ^ { r \times m }$ are the low-rank gradient matrices of the weight matrices $I _ { t }$ and $J _ { t }$ in Equation (9), respectively. $P _ { t } \in \mathbb { R } ^ { k \times r }$ , $Q _ { t } \ \in \ \mathbb { R } ^ { r \times m }$ , $\mathcal { P } _ { t } \in \overline { { \mathbb { R } ^ { r \times h } } }$ , and $\mathcal { Q } _ { t } \in \mathbb { R } ^ { m \times \bar { s } }$ are projection matrices.

In contrast to LoRA, our proposed GradNormLoRP adopts a distinct approach by employing two low-rank updates, i.e., $\tilde { \mathcal { Z } } _ { t }$ and $\tilde { \mathcal { H } } _ { t }$ , explicitly, avoiding the introduction of additional low-rank adaptors and thereby mitigating the alteration of training dynamics. Essentially, integrating GradNormLoRP into model training facilitates smooth transitions across normalized low-rank subspaces, as delineated in Equation (13). This means the model can smoothly navigate between different sets of parameters, akin to switching lanes on a highway to optimize its learning process.

$$
\mathcal { W } _ { t } = \mathcal { W } _ { 0 } + \Delta \mathcal { W } _ { T _ { 1 } } + \Delta \mathcal { W } _ { T _ { 2 } } + \ldots + \Delta \mathcal { W } _ { T _ { m } } ,
$$

where $t \in [ \sum _ { i = 1 } ^ { m - 1 } T _ { i } , \sum _ { i = 1 } ^ { m } T _ { i } ]$ and $\begin{array} { r } { \Delta \mathcal { W } _ { T _ { i } } = \alpha \sum _ { t = 0 . } ^ { T _ { i } - 1 } \tilde { \mathcal { D } } _ { t } } \end{array}$ denotes the sPum of all $\overline { { T _ { i } } }$ updates within the $i$ -th noPrmalized subspace.

The effectiveness of GradNormLoRP hinges on the premise that gradients often exhibit low-rank properties throughout training. To validate this assertion, we present Theorem 1 , with its proof provided in our Extended Version.

Theorem 1 Let $r \leq m$ without loss of generality. The gradient update rules of GradNormLoRP:

$$
\begin{array} { r l } & { \mathcal { Z } _ { t } = A - B I _ { t } C , I _ { t } = I _ { t - 1 } + \gamma \mathcal { Z } _ { t - 1 } , } \\ & { \mathcal { H } _ { t } = E - F J _ { t } G , J _ { t } = J _ { t - 1 } + \beta \mathcal { H } _ { t - 1 } , } \end{array}
$$

with constant matrices $\dot { \boldsymbol { A } }$ and $E$ ), PSD matrices $( B , C , F$ , and $G$ ), and randomly initialized $I _ { 0 }$ and $J _ { 0 }$ leads to low-rank gradient with high probability:

$$
\begin{array} { r } { \mathrm { s t a b l e - r a n k } ( \mathcal { Z } _ { t } , \mathcal { H } _ { t } ) \leq 1 + \displaystyle \sum _ { i = 2 } ^ { r } O \left( \left( \frac { 1 - \gamma \omega _ { i } \nu _ { 1 } } { 1 - \gamma \omega _ { 1 } \nu _ { 1 } } \right) ^ { 2 t } \right) } \\ { \times \displaystyle \sum _ { j = 2 } ^ { r } O \left( \left( \frac { 1 - \beta \pi _ { j } \mu _ { 1 } } { 1 - \beta \pi _ { 1 } \mu _ { 1 } } \right) ^ { 2 t } \right) . } \end{array}
$$

# Experiments

In this section, we evaluate the efficacy of our proposed GradNormLoRP through a series of experiments. We assess its performance in fine-tuning and pre-training scenarios and conduct a thorough throughput analysis to confirm that GradNormLoRP integrates seamlessly without adding inference latency. Additionally, we perform comprehensive ablation studies to highlight GradNormLoRP’s characteristics, including convergence speed, parameter efficiency, and GPU memory utilization.

# Experimental Setup

Datasets, Evaluation Metrics, Model Architectures, and Baselines. For fine-tuning, we use the GLUE benchmark (Wang et al. 2019), which includes single-sentence tasks (CoLA, SST-2), similarity and paraphrase tasks (MRPC, QQP, STS-B), and inference tasks (MNLI, QNLI, RTE, WNLI). Evaluation metrics are accuracy for MNLI, QQP, QNLI, SST-2, MRPC, and RTE, Pearson and Spearman correlation for STS-B, and Matthews correlation for CoLA. For pre-training, we use the C4 dataset (Raffel et al. 2020b), a cleaned version of Common Crawl’s web corpus, with perplexity as the performance metric.

In our fine-tuning experiments, we use BERTbase (Devlin et al. 2018), RoBERTabase, RoBERTalarge (Liu et al. 2019), and BARTbase (Lewis et al. 2019) for all GLUE tasks. For pre-training, we adopt the LLaMA model architecture, training on the C4 dataset with no data repetition, scaling up to 7 billion parameters.

Our primary baseline for fine-tuning is full parameter updating. For PEFT experiments, we compare GradNormLoRP against LoRA (Hu et al. 2022), DoRA (Liu et al. 2024), and GaLore (Zhao et al. 2024b), which are also used as baselines for our pre-training experiments due to their relevance in low-rank approximation methods.

Implementation. For fine-tuning, we evaluated our model on the GLUE benchmark, exploring learning rates in the range of {1e-4, 2e-4, 3e-4, 4e-4, 5e-4}, batch sizes of 16 and 32, and a fixed number of 30 epochs. Specifically, we used a batch size of 16 for all tasks except for CoLA, which used a batch size of 32. The maximum sequence length for all tasks was set to 512 for $\mathbf { B E R T _ { b a s e } }$ , RoBERTabase, $\bar { \mathsf { R o B E R T a } } _ { \mathrm { l a r g e } }$ , and $\mathbf { B A R T _ { b a s e } }$ models. For pretraining, we applied GradNormLoRP across various model sizes ranging from 60M to 1B parameters. The hyperparameters for GradNormLoRP were consistent across all models, with a learning rate of 0.01 and a scale factor $( \alpha _ { s } )$ of 0.25. The learning rate was fine-tuned from the set {1e-2, 1e-3,5e-4,1e-4}, selecting the best rate based on validation perplexity. Each model was pre-trained for 10,000 steps. For models scaled up to 7B parameters, we set the batch size to 16 and varied the training steps accordingly.

# Results and Analysis

Quantitative Results for Fine-tuning. We fine-tune pretrained RoBERTa models on GLUE tasks using GradNormLoRP and compare its performance with a full fine-tuning baseline, LoRA, DoRA, and GaLore. We use hyperparameters from (Hu et al. 2022) for LoRA and (Liu et al. 2024) for DoRA, and tune the learning rate and scale factor for GradNormLoRP. As shown in Table 1, GradNormLoRP achieves better performance than LoRA and DoRA on most tasks with a lower memory footprint. For instance, GradNormLoRP achieves an average score of 81.01 with a memory usage of $2 4 9 \mathbf { M }$ for rank $\scriptstyle = 4$ , while LoRA and DoRA achieve average scores of 80.40 and 80.52 with memory usages of 257M and 259M, respectively. This demonstrates that GradNormLoRP can serve as a full-stack memory-efficient training strategy for fine-tuning. Specifically, GradNormLoRP shows notable improvements in tasks such as SST-2, where it achieves 94.50 compared to 92.89 for GaLore. Additionally, GradNormLoRP maintains competitive performance in other tasks like QNLI and QQP, demonstrating its robustness.

Quantitative Results for Pre-training. For GradNormLoRP and GaLore, we set subspace frequency $T$ to 250 and scale factor $\alpha _ { s }$ to 0.25 across all model sizes in Table 2. For each model size, we pick the same rank $r$ for all low-rank methods, applying them to all the linear layers of all multi-head attention layers and feed-forward layers in the models. We keep the Adam optimizer settings consistent with GaLore (Zhao et al. 2024b). We also estimate the memory usage based on BF16 format, including the memory for weight parameters and optimizer states. We evaluate them on LLaMA 60M, 130M, 350M and 1B architecture with 10K training steps, and we tune the learning rate for each setting and report the best performance.

<html><body><table><tr><td>Model</td><td>Memory</td><td>CoLA</td><td>MNLI</td><td>MRPC</td><td>QNLI</td><td>QQP</td><td>RTE</td><td>SST-2</td><td>STS-B</td><td>WNLI</td><td>Avg</td></tr><tr><td>Full FT</td><td>747M</td><td>57.03</td><td>86.30</td><td>92.09</td><td>92.04</td><td>91.35</td><td>77.62</td><td>91.74</td><td>90.82</td><td>43.90</td><td>80.32</td></tr><tr><td>LoRA (r=4)</td><td>257M</td><td>55.27</td><td>86.81</td><td>89.95</td><td>92.42</td><td>89.44</td><td>69.68</td><td>93.58</td><td>90.07</td><td>56.34</td><td>80.40</td></tr><tr><td>DoRA (r=4)</td><td>259M</td><td>55.51</td><td>86.90</td><td>89.91</td><td>92.24</td><td>89.54</td><td>70.75</td><td>93.46</td><td>90.04</td><td>56.34</td><td>80.52</td></tr><tr><td>GaLore (r=4)</td><td>253M</td><td>60.65</td><td>85.65</td><td>91.14</td><td>90.76</td><td>90.70</td><td>77.26</td><td>92.89</td><td>90.84</td><td>36.62</td><td>79.61</td></tr><tr><td>Ours (r=4)</td><td>249M</td><td>59.31</td><td>86.42</td><td>91.10</td><td>92.48</td><td>90.60</td><td>75.81</td><td>94.50</td><td>90.85</td><td>47.89</td><td>81.01</td></tr><tr><td>LoRA (r=8)</td><td>264M</td><td>56.50</td><td>86.65</td><td>90.30</td><td>92.60</td><td>89.74</td><td>69.68</td><td>93.81</td><td>90.10</td><td>43.67</td><td>79.23</td></tr><tr><td>DoRA (r=8)</td><td>267M</td><td>57.27</td><td>86.55</td><td>89.56</td><td>92.46</td><td>89.86</td><td>70.04</td><td>94.15</td><td>90.16</td><td>43.66</td><td>79.30</td></tr><tr><td>GaLore (r=8)</td><td>257M</td><td>52.59</td><td>85.66</td><td>92.06</td><td>91.31</td><td>90.74</td><td>78.70</td><td>92.66</td><td>90.80</td><td>39.44</td><td>79.33</td></tr><tr><td>Ours (r=8)</td><td>251M</td><td>60.31</td><td>86.97</td><td>91.36</td><td>92.62</td><td>91.01</td><td>77.26</td><td>94.50</td><td>90.83</td><td>40.85</td><td>80.65</td></tr></table></body></html>

Table 1: Evaluating GradNormLoRP for memory-efficient fine-tuning on the GLUE benchmark using the pre-trained RoBERTabase model. “r” indicates rank, “Ours” signifies GradNormLoRP, and “FT” denotes fine-tuning.

Table 2: Compared with low-rank algorithms on pre-training various sizes of LLaMA models on the C4 dataset, reporting validation perplexity and memory estimates for parameters and optimizer states in BF16 format, with actual memory usage as shown. Note that the unit in each parenthesis is “G”.   

<html><body><table><tr><td>Method</td><td>60M</td><td>130M</td><td>350M</td><td>1B</td></tr><tr><td>Full-rank</td><td>34.51;(0.35)</td><td>25.91;(0.8)</td><td>20.24;(2.21)</td><td>16.86;(8.03)</td></tr><tr><td>LoRA</td><td>35.33:(0.35)</td><td>30.55:(0.81)</td><td>25.11;(1.93)</td><td>22.35:(6.32)</td></tr><tr><td>DoRA</td><td>35.42:(0.37)</td><td>30.92:(0.82)</td><td>24.91:(1.95)</td><td>21.98:(6.37)</td></tr><tr><td>GaLore</td><td>34.94:(0.23)</td><td>26.57;(0.54)</td><td>20.64;(1.47)</td><td>16.77:(4.69)</td></tr><tr><td>Ours</td><td>34.63:(0.17)</td><td>26.52;(0.47)</td><td>19.28:(1.19)</td><td>16.12:(3.81)</td></tr><tr><td>r/dmodel</td><td>128/256</td><td>256/768</td><td>256 / 1024</td><td>512/2048</td></tr><tr><td>Training Tokens</td><td>1.1B</td><td>2.2B</td><td>6.4B</td><td>13.1B</td></tr></table></body></html>

Table 3: Pre-training LLaMA 7B on the C4 dataset for 80K steps, with validation perplexity and memory estimates reported.   

<html><body><table><tr><td>Model</td><td>Memory</td><td>20K</td><td>40K</td><td>60K</td><td>80K</td></tr><tr><td>8-bitGradNormLoRP</td><td>15.29G</td><td>19.33</td><td>17.73</td><td>16.43</td><td>15.41</td></tr><tr><td>8-bit GaLore</td><td>18.36G</td><td>20.19</td><td>18.15</td><td>16.96</td><td>16.08</td></tr><tr><td>8-bit Adam</td><td>26.47G</td><td>20.65</td><td>18.31</td><td>17.11</td><td>16.24</td></tr><tr><td>Training Tokens</td><td></td><td>2.5B</td><td>5.2B</td><td>7.7B</td><td>10.5B</td></tr></table></body></html>

As shown in Table 2, GradNormLoRP achieves significant reductions in validation perplexity and memory usage across different model sizes compared to other methods. For instance, for the 350M parameter model, GradNormLoRP achieves a perplexity of 19.28 with a memory usage of 1.19G, whereas GaLore achieves a perplexity of 20.64 with a memory usage of 1.47G. This represents a substantial improvement in both memory efficiency and model performance. Similarly, for the 1B parameter model, GradNormLoRP reduces the perplexity to 16.12 and memory usage to 3.81G, outperforming GaLore’s perplexity of 16.77 and memory usage of 4.69G. These results demonstrate that GradNormLoRP can significantly enhance the efficiency of pre-training LLMs, making it a robust and scalable solution for training in resource-constrained environments.

Pre-training on LLaMA 7B. Scaling to 7B models is crucial for demonstrating GradNormLoRP’s effectiveness in practical LLM pre-training. We evaluate GradNormLoRP on an LLaMA 7B architecture, which has an embedding size of 4096 and 32 layers. The model is trained for 80K steps with 10.5B tokens, using 8-node parallel training on $3 2 \mathrm { { A l } \hat { 0 } 0 }$ GPUs. Due to computational constraints, we compare 8-bit GradNormLoRP $( \bar { \mathrm { r } } = 1 0 2 4 )$ with 8-bit Adam, performing a single trial without hyperparameter tuning. As shown in Table 3, 8-bit GradNormLoRP not only has a lower memory footprint but also achieves better performance metrics. Specifically, 8-bit GradNormLoRP requires 15.29GB of memory, significantly less than the 18.36GB and 26.47GB required by 8-bit GaLore and 8-bit Adam, respectively.

In terms of validation perplexity, 8-bit GradNormLoRP consistently outperforms other methods across training steps, achieving lower perplexity with higher ranks. At 20K steps, 8-bit GradNormLoRP reaches a perplexity of 19.33, compared to 20.19 for 8-bit GaLore and 20.65 for 8-bit Adam. This trend persists at 40K, 60K, and 80K steps, where 8-bit GradNormLoRP achieves 15.41 perplexity, outperforming 8-bit GaLore (16.08) and 8-bit Adam (16.24). Its superior performance stems from efficient low-rank adaptation during gradient projection, optimizing memory usage and enhancing training efficiency, making it a highly effective solution for pre-training LLMs with improved hardware utilization.

Regarding memory consumption, as shown in the left subfigure of Figure 1, 8-bit GradNormLoRP requires only 20.07GB to pre-train the LLaMA 7B model with a per-GPU token batch size of 256, well within the 24GB VRAM of an NVIDIA RTX 4090. This is substantially lower than BF16 and 8-bit Adam, which exceed 24GB for larger models.

Subspace Update Frequency Ablation Study. This ablation study, illustrated in the middle subfigure of Figure 1, highlights the importance of finding an optimal update frequency for subspace updates to achieve the best model convergence. Both overly frequent and overly infrequent updates negatively impact performance, leading to increased perplexity. The study shows that the optimal performance occurs at a moderate update frequency of around 250 iterations, especially for higher ranks such as 256 and 512, which benefit from more effective optimization within larger subspaces. Weight normalization plays a crucial role by stabilizing the gradient descent process, ensuring that updates are on a comparable scale and preventing inefficiencies.

Rank of Subspace Ablation Study. As shown in the right subfigure of Figure 1, this study examines how subspace rank and training steps affect perplexity. Training with rank 128 for 80K steps achieves better perplexity than rank 512 at 20K steps, emphasizing the importance of sufficient training duration for higher ranks. GradNormLoRP excels by effectively combining subspace updates and weight normalization, stabi

<html><body><table><tr><td>Model</td><td>Memory</td><td>CoLA</td><td>MNLI</td><td>MRPC</td><td>QNLI</td><td>QQP</td><td>RTE</td><td>SST-2</td><td>STS-B</td><td>WNLI</td><td>Avg</td></tr><tr><td>FullFTRoBERTa</td><td>747M</td><td>57.03</td><td>86.30</td><td>92.09</td><td>92.04</td><td>91.35</td><td>77.62</td><td>91.74</td><td>90.82</td><td>43.90</td><td>80.32</td></tr><tr><td>Ful1 FTBART</td><td>901M</td><td>53.78</td><td>86.16</td><td>91.66</td><td>91.83</td><td>91.01</td><td>77.11</td><td>91.75</td><td>89.87</td><td>39.82</td><td>79.25</td></tr><tr><td>Full FTBERT</td><td>708M</td><td>56.47</td><td>86.29</td><td>91.72</td><td>91.97</td><td>91.04</td><td>77.12</td><td>91.79</td><td>89.99</td><td>39.85</td><td>79.60</td></tr><tr><td>OurSRoBERTa</td><td>251M</td><td>60.31</td><td>86.97</td><td>91.36</td><td>92.62</td><td>91.01</td><td>77.26</td><td>94.50</td><td>90.83</td><td>40.85</td><td>80.65</td></tr><tr><td>OurSBART</td><td>361M</td><td>54.13</td><td>86.29</td><td>91.00</td><td>91.85</td><td>90.07</td><td>73.31</td><td>93.23</td><td>89.15</td><td>49.30</td><td>79.81</td></tr><tr><td>OurSBERT</td><td>236M</td><td>58.57</td><td>86.95</td><td>89.75</td><td>90.94</td><td>91.42</td><td>70.76</td><td>92.20</td><td>88.69</td><td>44.99</td><td>79.26</td></tr></table></body></html>

Table 4: Model architecture ablation study under the same model size. “r” denotes rank, “Ours” indicates GradNormLoRP, and “FT” signifies fine-tuning. GradNormLoRP utilizes a rank of 8 here.

![](images/12631e3812cc766cdc851fad70a6947febdb4ffff01b3d0d1e183bf8ee20c8ce.jpg)  
Figaunrde t1h:eFerfofemctloeftrtaonkriagchrto,stshsetefpigs.ure illustrates a comparison of memory usage, the impact of varying subspace frequencies, and the effect of rank across steps. Note that the red dashed line denotes the RTX 4090.

liziancghigervaesdiloenwterdpeesrcpelenxti. yWt hainletrhaingihngerwriathnaksraonfkfeorf 5si1g2nfiofric2a0nKt gaisntse,pts,hheiyghrleigqhutirneg tmheorneesdt feoprss,ufwfihciernetatsralioniwnge rdurrantikosn foprtihimgihzeer effircaineknst.lyGrawditNhorfemwLoerRPst’sespuspberuitorspheorfwormaonrceegisraduueatlo iptserepfflecxtivtey impcromvbeinmateinotns.o

Model Architecture Ablation Study. We evaluated GradNormLoRP on BARTbase, BERTbase, and RoBERTabase to assess its robustness. As shown in Table 4, GradNormLoRP consistently outperforms full fine-tuning. For RoBERTabase at rank $^ { = 8 }$ , it achieves an average score of 80.65, with significant improvements in CoLA (60.31) and SST-2 (94.50), compeprfaorremd tfou l8fi0n.e3-2tufnionrg.fFulolr RfionBe-EtRuTnaibnags.e aFtorranBk=A8,RitTabcahsie,veist scoarne sav7er9a.g8e1 orn aofve8r0a.6g5e,,weitxhcseilglniinfigc int mMprNoLveIm(e8n6ts.9in7)CoaLnAd MR(6P0C.3(1)9 1a.n0d6S)SoTv-2er(9f4u.l5l0f),inceo-tmupnairnedg ’tso 7809.325f.oFrofur lBfiEneR-tTubnainsge., it mFaoirntBaAinRsTbaasceo,mitpsectoirtievse79a.v8e1raogneasvecroarge oefx7ce9l.li2n6g, iwn tMhNnLoItabl(e86r.e9s7u) tasnidnMQRNPLCI (8919.0765) oavnedr fQulQlPfi(n9e-1t.u4n2i)n,g’cso 7m9p.2a5r.edFtor 79.6B0ERfoTrbafsuel,l tfimneai-nttuaninisnga.

MondoetalbSleizre sAulbtslaitn oQnNSLItu(8d9y..75W)eanedvaQluQaPte(d91G.4r2a)d, Nc omr pmaLreodRtoP on 7R9.o60BfEorRfTuallbfianse aunidng.RoBERTalarge, demonstrating significMaondtelmSeizmeoAryb astaivoingStsudwyh. lWeemasasienstsaeidniGnrgadoNrorimLporRovP nong performance compared to full fine-tuning (see Table 5 in the Extended Version). For RoBERTabase (rank $\scriptstyle = 8$ ), GradNormLoRP achieves an average score of 80.63, with notable gains in CoLA (60.31 vs. 57.03) and SST-2 (94.50 vs. 91.74). For(6R0.o31BvEs.R5T7a.l0a3r)gaen,ditSSacT-h2i e(9v4e.s508v2s..4931, 7s4li).gFhotlryRoButEpReTrafloargme,- ing tfaucllhifeivnes-taunnaivnergag(e82sc.o3r9e),ofu8n2d.4e3r,socuotrpienrfgorGmriandgNfuollrfimnLe-otuRniPn’gs effi(c8i2e.n39c)y. Tachresoessresmulotsdehligshilzigehst.

WeeifgfihcitenNtoprermfoarlmiaznacteioacnroAssbdliaftfieroen Smtoudely.sizWese conducted an ablaWtieoignhtstNuodrymtaol zeavtailounatAebltahtieoinmSptuadcty.oWf ewpeirfgohrtmnedoramwaeligzhattion ornmtahliez $\mathrm { R o B E R T a _ { b a s e } }$ umdyo tdoelq’usapnteirtfatoirvemlyanccoemapcareotshseGpeLrfUorEbenmcahnmcearokf thaeskRso.BAEsRTsahbaosewmnoidne Twaitbhle n6d iwn hoour EwexitgehntdneordVermsiaolinz,atiwoeniagchrtosnsovramriaoluiszaGtiLoUnEcboenscihstmeanrtkltyasikms.prTohveersespuletrsf,oarsmance. The improvement is most noticeable in tasks like

CoFuLll FAT , wh74e7rMe a5c7.0c3ur 8a6.c30y s9i2g.09nif9i2c.0a4nt9l1.y3 in77c.6r2ea91s.7e4s. 0.W82hil43e. ga8i0.n32s iwn/ oSrmS(rT=4-)2 a2r49eMmi5n9.3o1r, t86h.4e2y a91l.1i0gn 2w.48ith9 .t60he 5o.8v1 er94a.5l0l p9o0.8s5itiv47e. tre81n.0d1 . Iwn//o orrm((orr=r8))e c2o501Mmpl6e10.x537 ta86s.97k79s l9i12.k3164e 9M2.6521RP910.C0910, 7w76.29e60ig9h4.t520 no90r.8634ma43l06i.86z52ati80o.6256n provides notable benefits, indicating its role in stabilizing and optimizing the training process.

“nGoram”d iendnit tPersonjoercmtialoizn iAonb,lantdio“nFTS”tsuigdnyi.fiesWfienec-toumnipnag.red the performance and memory usage of the RoBERTabase model wiMtohd landMewmoirtyhoCouLtAgrMaNdLIieMntRPCproQjNeLIctiQoQPn aRcTrEosSsST-2GLSTUS-BE tWaNsLIks.AvgAs sw/hooGPw(r=n4 in 25T9aMble557.51in 8o6.9u0r E89x.9t1en92d.2e4 d 89V.54er 7s0i.7o5n,93i.4n6co9r0.0p4ora56t.3i4ng80g.5r2adwi/eGPn(rt=4p) roj24e9cMtio5n9. i1n 8G6.4r2ad9N1.1o0 rm92.L48oR90.P6 si75g.81nif9 i4c.50ant9l0.y8 bo47o.8s9ts 1p.01erfwo/rGPm(r=a8)nce 1wMhil6e0 1pre8s6.9e7rvi9n1.3g6 m92e.62mo9r1.0y1 e7f7.f2 i6cie94.n50cy. .8M3 od40e.8l5s 8w0.6i5th Tgarbalde e7n: Inpvreosjteigctaitiongdtheemiomnpsatcrtatoef igrmapdireonvtepdrosjteacbt iolintytharonudgthraining dynamics, leading to higher scores in tasks like CoLA and SST-2.

# Conclusion

GrradieNntorPmroLjeoctRioPnaAdbdlraetisosnesSttuhdey.gTrhoewgirnagd ecnot pmrpojuetcatitioonnaabl- delamtiaondsstuodfyfcuollmpfianre-sttuhneipnegr forrmLanLceMasndbymenmhoaryncuisnagepoafr tahmeeter RaonBdEmReTambaoserymeofdfeilc weintchyanwdhiwliethmouatingtraidniientgpcrojmecptiaornabalceropsesrforvamriaonucseG. LBUyEnboerncmhamliazrikntagskws.eiAgshsthomwantriincTeasb, eap7,pilnycionrpgolroatiwn-grank garapdpireontxpirmojaetcition, antnodGuratidlNizoirnmgLogRraPdsiegnitfilcoanwtl-yraenhkanpcreosjpecrt-ion, foGrrmaadnNceorwmhiLleoRmaP nstiagininfigcaefnfitlciyernte dmuecmesorymuesmagoer.ySopevceirfihcealldy durminogdetlrsauitnilinzigngwgirtahdioeunt pardodjeicntigoinnefxehriebintcenhbaunrcdedens.tabWilietyvanlidate its effectiveness both mathematically and empirically. Our experiments demonstrate its efficacy in LLM pre-training and fine-tuning, achieving comparable or superior results to existing PEFT methods.

# LAMsc bkyneonhwanlceindgpearmameentetrs

The computational support for this research was provided by gtrhaed eNnettlhoewr-lrankdsprOojregctainoinz,aGtiraodnNforrmSLcoiRePntsiifginc fiRceasnetlayrrcehdu(cNesWO) muenmdoeryporvoejrehcetadnduumribnegr rEaiInNinFg-w9i6th2o7u.