# Speech Watermarking with Discrete Intermediate Representations

Shengpeng $\mathbf { J } \mathbf { i } ^ { * }$ , Ziyue Jiang\*, Jialong Zuo, Minghui Fang, Yifu Chen, Tao Jin, Zhou Zhao†

Zhejiang University {shengpengji, zhaozhou}@zju.edu.cn

# Abstract

Speech watermarking techniques can proactively mitigate the potential harmful consequences of instant voice cloning techniques. These techniques involve the insertion of signals into speech that are imperceptible to humans but can be detected by algorithms. Previous approaches typically embed watermark messages into continuous space. However, intuitively, embedding watermark information into robust discrete latent space can significantly improve the robustness of watermarking systems. In this paper, we propose DiscreteWM, a novel speech watermarking framework that injects watermarks into the discrete intermediate representations of speech. Specifically, we map speech into discrete latent space with a vector-quantized autoencoder and inject watermarks by changing the modular arithmetic relation of discrete IDs. To ensure the imperceptibility of watermarks, we also propose a manipulator model to select the candidate tokens for watermark embedding. Experimental results demonstrate that our framework achieves state-of-the-art performance in robustness and imperceptibility, simultaneously. Moreover, our flexible frame-wise approach can serve as an efficient solution for both voice cloning detection and information hiding. Additionally, DiscreteWM can encode 1 to 150 bits of watermark information within a 1-second speech clip, indicating its encoding capacity.

Speech Watermarking in Continuous Space Imperceptibility Watermark 电 Loss Loss ▲ Waveform 串 Embedder Detector 001011 001011 Watermarked Watermark String Watermark String Waveform Our Discrete Scheme Reconstruction Loss Encoder VQ 3 7 2 0 3 Decoder 电 Waveform Discrete Tokens Clean 1 Waveform 001011 Manipulator 里 Detector 001011 Watermark Decoder Watermarked Watermark String String Waveform Watermark Loss

Demo — https://DiscreteWM.github.io/discrete wm Appendix — https://arxiv.org/abs/2412.13917

# Introduction

In recent years, the significant breakthrough in zero-shot textto-speech (TTS) (Casanova et al. 2022; Wang et al. 2023; Shen et al. 2023; Le et al. 2023; Jiang et al. 2023b; Ji et al. 2024c,f,g; SpeechTeam 2024) enables instant voice cloning with only a few seconds of speech. However, this technological advancement also brings security concerns to personal voices (Duquenne et al. 2023; Liu et al. 2023c). To avoid potential misuse of voice cloning technology, passive detection strategies (Tak et al. 2022b; Ahmed et al. 2020; Tak et al. 2022a, 2021) are developed to classify whether a speech clip is synthesized and adversarial-based methods (Huang et al.

2021; Li et al. 2023; Ji et al. 2024b,d; Yu, Zhai, and Zhang 2023) are proposed to prevent voice cloning with adversarial noise. However, these approaches still struggle with generalization issues (Liu et al. 2023b). In comparison, speech watermarking has been developed to (Pavlovi´c et al. 2022; Liu et al. $2 0 2 3 \mathrm { a }$ ; Chen et al. 2023; Liu et al. 2023b; Ji et al. 2024e) proactively embed robust watermark information into the target voice, which has demonstrated its generalizable performance in practical voice cloning detection (Duquenne et al. 2023). By utilizing this technology, users can not only identify whether a speech clip is AI-generated but also trace the source of the speech, thus offering reliable privacy protection in the era of large-scale voice models.

Despite recent advances in speech watermarking, current solutions still encounter two primary challenges: 1) trade-off among imperceptibility, robustness, and encoding capacity; In other words, maintaining robustness against various distortions while preserving a high encoding capacity affects the imperceptibility of watermarks (Liu et al. 2023a). Although GAN-based architectures have been introduced to minimize the distribution difference between watermarked speech and clean speech, the embedder still encodes the watermark into perceptible noise patterns in the mel-spectrogram, as shown in Figure 3; 2) fixed length issues; Most DNN-based speech watermarking methods can only process a fixed length of waveform with a pre-defined length of watermark string. In the detection stage, they require a sliding window to decode a watermark starting at each frame (Chen et al. 2023), which is inefficient and constrains the resolution of watermarks to speeches larger than one second (Duquenne et al. 2023). Although some works integrate time-independent features into the watermarking algorithm (Liu et al. 2023b), the capacity of the watermark string can not be changed during the inference stage, which also limits the resolution of watermarks and affects the flexibility in handling various scenarios.

Intuitively, compared to encoding watermarks into continuous latent space, watermarks in robust discrete latent space are more robust against distortions. Therefore, to achieve a superior trade-off among imperceptibility, robustness, and encoding capacity, we propose DiscreteWM, a framework that utilizes discrete speech representations to embed watermark information. As shown in Figure 1, we first propose a masked vector-quantized variational autoencoder (VQVAE) to map clean speech into frame-level discrete latent space. We ensure that the parity of the discrete token IDs can be detected from the reconstructed speech even when it is severely distorted. Then we propose a manipulator model to learn the probability distribution of discrete speech tokens. Finally, the watermark information can be embedded into the modular arithmetic relationship of discrete token IDs selected by the manipulator model. By utilizing the modular arithmetic relationship of discrete acoustic tokens in the latent space, our work enjoys an imperceptible and flexible watermarking pipeline where the users can freely decide the strength, capacity, and formats of the watermark information in the inference stage.

The contributions of the paper are summarized as follows:

• DiscreteWM is the first attempt to embed watermark information in the robust discrete latent space. Our method outperforms other state-of-the-art (SOTA) speech watermarking models on both voice cloning detection and information hiding tasks. • Our frame-wise strategy also resolves the challenges related to fixed-length training in speech watermarking and achieves $2 2 . 1 \mathrm { x }$ times faster detection speed. • DiscreteWM allows users to freely manipulate the encoding capacity (up to 150 bits per second) and formats of the watermark without re-training the model. • We further propose a statistical $Z$ -test to transform our frame-wise accuracy to utterance level for AI-generated content detection. The extensive studies demonstrate that our method achieves a false positive rate of $3 \times 1 0 ^ { - 5 }$ while maintaining extreme imperceptibility.

# Related Works

# Speech Watermarkiing

Speech watermarking technology has always been used as a fundamental tool for copyright protection of human speech (Hua et al. 2016). Traditional speech watermarking typically embeds watermark information in the time domain (e.g., Least Significant Bit (Cvejic and Seppanen 2004), Echo Hiding (Gruhl, Lu, and Bender 1996)) and the transform domain (e.g., Spread Spectrum (Cox et al. 1997), Patchwork (Yeo and Kim 2003)). In terms of robustness, some researches have successfully achieved resilience against distortion (Zhang et al. 2023), desynchronization (Zhao et al. 2021), re-recording (Liu, Huang, and Huang 2018), etc. However, the encoding process of traditional methods relies heavily on hand-crafted empirical rules, which are challenging to implement, resulting in a low encoding capacity with limited robustness against a wider range of attacks.

Recently, DNN-based speech watermarking algorithms (Jiang et al. 2020; Pavlovi´c et al. 2022; Liu et al. 2023a; Chen et al. 2023; Liu et al. 2023b; Ji et al. 2024a; Duquenne et al. 2023) have demonstrated superior encoding capacity, invisibility, and robustness when compared to traditional methods. Their frameworks typically include an encoder for watermark embedding and a detector for watermark extraction. The encoding and decoding strategies are learned in an end-to-end manner. In terms of imperceptibility, DeAR (Liu et al. 2023a) utilizes an adversarial discriminator to minimize the domain gap between clean speech and watermarked speech. WavMark (Chen et al. 2023) regards the encoding and decoding as reciprocal processes and adopts invertible neural networks, which improves the overall fidelity and robustness of the watermark. And in terms of robustness, some of the most advanced methods can resist voice cloning attacks (Liu et al. 2023b), desynchronization attacks (Chen et al. 2023), and re-recording attacks (Liu et al. 2023a). However, most of their methods, unfortunately, have limitations in that they can only process speech signals of a predetermined length. In order to locate the watermark, they rely on the Brute Force Detection (BFD) method, which involves sliding through the speech and attempting to decode a watermark starting at each frame (Duquenne et al. 2023). The latency of these approaches is excessively high, making them impractical as proactive defense mechanisms for real-world voice cloning systems. Besides, current solutions only embed the watermark as continuous noise patterns, leaving speech watermarking with discrete intermediate representation unexplored. Therefore, we propose a frame-wise approach to solve the watermark localization issues and investigate the algorithm that adopts discrete intermediate representations to further enhance the imperceptibility and robustness of watermarks. We include additional discussions about the vector quantised discrete representation and its applications in Appendix F.

# Method

This section introduces DiscreteWM. To begin with, we provide an intuitive formulation and prerequisites of our watermarking strategy. Next, we provide detailed descriptions of our architecture design and the training process of the proposed model. Finally, we propose inference strategies for information hiding and AI-generated content detection separately and propose a statistical measure for detecting the watermark with the one proportion Z-test. Due to space limitations, we provide technical details in Appendix A.

# Watermarking Strategy

The outline of our watermark strategy is: transforming speech into discrete latent space and enforcing the discrete token IDs to have the same modular arithmetic relations with the watermarks.

Strategy formulation. Denote $\boldsymbol { s } = \{ \boldsymbol { s } ^ { ( 0 ) } , \cdots , \boldsymbol { s } ^ { ( T ) } \}$ as the magnitude spectrogram of speech waveform $y$ and $w$ as the watermark string, where $T$ is the number of spectrogram frames. The watermark embedding process is performed according to the following steps: 1) an encoder $\mathbf { E }$ learns to represent the spectrogram $s$ with acoustic code sequence $\boldsymbol { z } ^ { \mathrm { ~ } } = \big \{ z ^ { ( 0 ) } , \cdots , z ^ { ( T ) } \big \}$ , where $z ^ { ( t ) }$ is obtained from a discrete codebook $\mathcal { Z }$ ; 2) Then, we inject the watermark string $w$ into $z$ by manipulating the modulus relation of token IDs $c$ . For simplicity, we only consider the case of “ $\dot { \boldsymbol { c } }$ mod $2 ^ { \circ }$ in this section, as it is a suitable setting for speech watermarking (Chen et al. 2023). Specifically, when we want to embed the watermark character $\ " 0 \ "$ or “1” in the $t$ -th frame, we replace the $t$ -th discrete code with the even or odd code ID that has features similar to the original one, respectively. The watermarked acoustic codes are denoted as $\hat { z } ; 3$ ) Given $\hat { z }$ , a decoder $\mathbf { G }$ learns to reconstruct the watermarked spectrogram $\boldsymbol { \hat { s } } . \boldsymbol { \hat { s } }$ and the original phase spectrogram are converted to the watermarked speech $\hat { y }$ through the inverse Short-Time Fourier Transform operation (iSTFT); 4) A localizer $\mathbf { D }$ is designed to locate the watermarked frames and a restorer $\mathbf { R }$ is utilized to recover $\hat { z }$ . Finally, we can obtain the watermark string $w$ from $\hat { z }$ .

Prerequisites of the proposed strategy. However, the above strategy can not guarantee the imperceptibility and robustness of the watermark until now. In practical scenarios, in terms of imperceptibility, the perceptual differences of $y$ and $\hat { y }$ should be minimized. Therefore, the proposed watermarking strategy needs the following prerequisites:

Prerequisite 0.1. $\mathbf { G } ( z ) = \bar { s }  s ,$ , the difference between the reconstructed spectrogram $\bar { s }$ and the original spectrogram s should be minimized.

Prerequisite 0.2. $\hat { z }  z$ , the distance between the manipulated acoustic code $\hat { z }$ and the original code $z$ in the latent space should be minimized.

In terms of robustness, it is crucial to accurately extract the watermark string $w$ even when $\hat { y }$ is distorted in signal transmission processes or is maliciously attacked:

Prerequisite 0.3. ${ \bf R } \left( { \bf D } \left( D i s t \left( \hat { y } \right) \right) \right) \ \to \ \hat { c } \ \mathrm { m o d } \ 2 \ = \ w ,$   
where $D i s t \left( \cdot \right)$ is the distortion function.

We describe how we achieved the aforementioned prerequisites in the following subsection.

# Architecture Design

Our framework comprises a two-stage training process. In the first stage, we train an autoencoder to represent the speech into discrete tokens. Then, we construct a localizer model $D$ to locate the reconstructed frames and design a restoration loss to ensure R can restore the parity of discrete token IDs $( \hat { c } \bmod 2 , )$ ) even when the reconstructed speech is heavily distorted. In the second stage, we train a probability-based manipulator model to conceal the watermark string within the modular arithmetic relationships among these discrete tokens while ensuring imperceptibility.

# Robust Discrete Latent Space

Representing speech in discrete latent space. Given a clean speech $y$ , we first represent it in the discrete latent space. As shown in Figure 2, we apply the Short-Time Fourier Transform operation (STFT) on $y$ to produce a magnitude spectrogram $s$ . Then, to discretize $s$ , we adopt a vector quantized variational autoencoder architecture (VQVAE) (Van Den Oord, Vinyals et al. 2017). The VQ encoder $\mathbf { E }$ and decoder $\mathbf { G }$ reconstruct the spectrogram $s$ through: $\bar { s } = \mathbf { G } \left( z \right) = \mathbf { G } \left( \mathbf { E } \left( s \right) \right)$ . Additionally, to satisfy Prerequisite 0.1, the system is trained through a mask-infilling process with a frame-level random mask. Due to the spectro-temporal locality of speech signals (Espi et al. 2015), the unmasked contextual speech can provide rich information to significantly reduce the difficulty of the spectrogram reconstruction. The discrete codes of the masked region are also fed into the decoder to provide the missing information during the masking process. Finally, the reconstructed spectrogram of the masked region is concatenated with the unmasked original spectrogram. The overall reconstruction process $\bar { s } \approx s$ is formulated as:

$$
\begin{array} { r } { \bar { s } = \omega \cdot \mathbf { G } \left( \omega \cdot \mathbf { E } \left( s \right) , \left( 1 - \omega \right) \cdot s \right) + ( 1 - \omega ) \cdot s , } \end{array}
$$

where $\omega$ is the binary mask. $\omega$ is obtained by $\begin{array} { r l } { \omega } & { { } = } \end{array}$ Mask $( s , \gamma )$ , where $M a s k \left( \cdot \right)$ is the mask function and $\gamma \in$ [0.1, 0.5] is the mask ratio. To further minimize the perceptual differences between $\hat { y }$ and $y$ , we introduce extra discriminators for adversarial training, including the multi-period discriminator and the multi-scale discriminator (Kong, Kim, and Bae 2020). Finally, the training loss of the VQ-VAE can be formulated as:

$$
{ \mathcal L } _ { \nu \varrho } = { \mathcal L } _ { \mathrm { r e c } } + { \mathcal L } _ { \mathrm { c o d e } } + \lambda _ { a d v } { \mathcal L } _ { \mathrm { a d v } } ~ ,
$$

where ${ \mathcal { L } } _ { \mathrm { r e c } }$ is the reconstruction loss, $\mathcal { L } _ { \mathrm { c o d e } }$ is the standard VQ codebook loss (Van Den Oord, Vinyals et al. 2017), and ${ \mathcal { L } } _ { \mathrm { A d v } }$ is the adversarial loss. We use the multi-resolution STFT loss (Yamamoto, Song, and $\mathrm { K i m } 2 0 2 0 )$ as ${ \mathcal { L } } _ { \mathrm { r e c } }$ . $\lambda _ { a d v }$ is the hyper-parameter to balance the three terms, which is set to $1 0 ^ { - 2 }$ . To enhance the codebook usage rate and further decrease the reconstruction error, we adopt the clustering vector quantizer (CVQ) (Zheng and Vedaldi 2023) as the elementwise quantization function in $E$ that maps each acoustic code onto its closest codebook entry.

Detecting the Parity of Token IDs. Here we describe how to restore the parity of discrete token IDs (cˆ mod 2) from the reconstructed speech, which is the necessary condition for watermark embedding in the discrete latent space. As shown in Figure 2, our frame-wise framework has two primary objectives: localization and discrete code restoration.

![](images/7abecb5fed3d599285d58d2ad5aedf941f3cf8e7661b569c6c07ed217d47dead.jpg)  
Figure 2: The overall architecture of DiscreteWM. “VQ” represents the “vector quantization” operation, and $\textcircled{ C}$ denotes the concatenation operation. During the watermark embedding process, the manipulator forces the discrete tokens to have the same modular arithmetic relation with the watermark message, as indicated by the red dashed line. For instance, if we intend to conceal the value “1” into the last discrete token, the manipulator will selectively sample from the odd tokens (highlighted in green) according to their probability distribution. The original token will then be replaced with the sampled token that has the highest probability (the 5th token). In watermark extraction, the localizer is responsible for watermark localization, while the restorer focuses on recovering the watermark message.

Regarding localization, we aim at distinguishing between the original frames and the reconstructed frames with the localizer model D; We train D by minimizing the binary cross-entropy loss between its output and a binary mask representing the presence of the reconstructed frames. With the localizer model $\mathbf { D }$ , our algorithm successfully resolves the location issues in current fixed-length counterparts. Compared to the previous sliding-window detection method, the proposed localizer significantly reduces the time required for watermark localization. In terms of discrete code restoration, we focus on converting the reconstructed speech $\hat { y }$ back to the manipulated discrete token $\hat { z }$ using the restorer model $\mathbf { R }$ even when $\hat { y }$ is severely distorted. We design the following restoration loss to achieve this objective:

have identical modular arithmetic relationships with the watermarks. However, if we manually adjust the code IDs to embed watermark information, it will have a significant impact on the speech quality. For instance, if we replace the discrete code representing silence with the discrete code of normal speech, there will be a significant amount of noise in the watermarked frame. To satisfy Prerequisite 0.2, we introduce a probability-based manipulator model M to help us select the optimal code ID in the watermark embedding process. During the second-stage training process, we first extract $z$ through $\mathbf { E } \left( s \right)$ using the proposed VQVAE structure. Given $\boldsymbol { \omega } \cdot \boldsymbol { z }$ as the prediction target, the manipulator model $\mathbf { M }$ is trained through a parallel mask-prediction process:

$$
P \left( \omega \cdot z \mid \left( 1 - \omega \right) \cdot z ; \theta _ { M } \right) ~ ,
$$

$$
\begin{array} { r } { \mathcal { L } _ { r e s } = \mathbb { E } _ { \tilde { s } \sim p ( \tilde { s } ) } [ - \log p ( \hat { c } \bmod 2 ) ] , } \end{array}
$$

where $\tilde { s }$ is the magnitude spectrogram of $D i s t ( \hat { y } )$ and $\hat { c }$ is the token IDs of $\hat { z }$ . Furthermore, to fulfill Prerequisite 0.3, an attack simulator is employed in our framework following previous works (Chen et al. 2023; Liu et al. 2023b), which assists our model in acquiring adaptive robustness against various attacks $D i s t \left( { { \cdot } } \right)$ . Until now, we have finally built a robust discrete latent space, in which the parity of the discrete code IDs can be easily detected.

Injecting Watermarks into Discrete Latent Space Concealing watermarks with the manipulator. As illustrated in Section Watermarking Strategy, our DiscreteWM embeds watermarks by ensuring that the discrete token IDs where $\omega$ is the aforementioned binary mask and $\theta _ { \mathbf { M } }$ is the parameter of M. The manipulator model is trained with the cross-entropy loss. After training, $\mathbf { M }$ can be utilized to sample the odd or even optimal tokens according to the watermark information and replace the original discrete token to construct $\hat { z }$ .

Sampling strategy of the manipulator. As shown in Figure 2, to embed the watermark value “1” into the last frame, if the ID value of the last discrete token is even, we replace it with odd tokens sampled from the probability distribution given by the manipulator model M:

$$
P ( z _ { k } ^ { ( t ) } ) = s o f t m a x ( l _ { k } ^ { ( t ) } ) = \frac { \mathrm { e } ^ { l _ { k } ^ { ( t ) } } } { \sum _ { i } \mathrm { e } ^ { l _ { i } ^ { ( t ) } } } ,
$$

where $l _ { k } ^ { ( t ) }$ represents the logit of token $k$ at timestep $t$ . If the $\mathrm { I D }$ value of the last discrete token is odd, we directly use the original token for reconstruction. During the watermark embedding process, we randomly select a portion of the discrete codes and substitute them non-autoregressively to ensure the efficiency of the system.

# Inference Strategies

During the inference stage, our frame-wise solution offers remarkable flexibility, enabling us to select different encoding strategies for various scenarios and to freely control the trade-off between imperceptibility and robustness. In this subsection, we discuss the watermark strategies for information hiding and AI-generated content detection separately. Additionally, we perform a statistical analysis on the detection sensitivity of the watermarked speech.

Watermark for Information Hiding. Speech watermarking for information hiding mainly aims at hiding a binary message (such as 32 bits) to the speech segments (Liu et al. 2023b; Chen et al. 2023), which can be used for tracing provenance, copyright protection, and privacy protection. The basic idea of our frame-wise watermarking strategy, as mentioned in Section Watermarking Strategy, is to embed the watermark character $^ { 6 6 } 0 ^ { 5 }$ or “1” by enforcing the token ID to be even or odd, respectively. In the information hiding pipeline, we first map clean speech into discrete latent space following Section Robust Discrete Latent Space and embed watermark information into the discrete codes following Section Injecting Watermarks into Discrete Latent Space. Then, the watermarked latent codes $\hat { z }$ are converted into the watermarked speech $\hat { y }$ . Finally, following the watermark detection algorithms described in Section Architecture Design, we can recover the watermark string from $\hat { y }$ . Since our watermarking method is frame-wise, it is free from the time-consuming watermark localization process like previous DNN-based methods (Chen et al. 2023). Moreover, our framework can freely adjust the encoding capacity according to users’ requirements. Suppose the hop size is set to 80 and the maximum mask ratio $\gamma$ is set to $50 \%$ , we can store 1 to 150 bits of information within one-second speech sampled at $2 4 \mathrm { k H z }$ , which demonstrates the flexibility of our method.

Watermark for AI-Generated Detection. Speech watermarking is a crucial proactive defense strategy against voice cloning attacks (Duquenne et al. 2023). In this scenario, online services or individual users can add watermarks when cloning voices. In this way, people can easily determine whether the speech is generated by AI through the watermark detection process, which significantly reduces the possible abuses of voice cloning techniques.

As discussed in Section Architecture Design, our localizer D can be employed to identify whether a speech frame is reconstructed by our VQVAE or not. Therefore, we can utilize this characteristic to achieve AI-generated content detection. In an ideal scenario, when a natural speech is given as input, the localizer $\mathbf { D }$ should output a sequence of zeros. If any frame in the output sequence of $\mathbf { D }$ is non-zero, it indicates that the audio segment has been watermarked, i.e., the audio segment is generated by AI. However, in practical situations, the frame-wise accuracy of $\mathbf { D }$ will ultimately affect our decision. In order to convert the frame-wise accuracy to utterance level, we adopt a $Z$ -test as our robust detection approach. In practical scenarios, we can detect the utterance-level watermark if the Z-statistic is above a pre-defined threshold (e.g., $Z$ -statistic $> 4$ ). Denote $T$ as the number of speech frames. Let’s assume that the frame-level true positive rate and false positives rate of $\mathbf { D }$ on the test set are $\alpha$ and $\beta$ , respectively. Then, given a clean speech $y$ , the number of its detected watermarked frames $| f | _ { w }$ has expected value $\beta \cdot T$ and variance $\beta \left( 1 - \beta \right) \cdot T$ . The $Z$ -statistic can be calculated as:

$$
Z – s t a t i s t i c = \frac { \left( | f | _ { w } - \beta \cdot T \right) } { \sqrt { \beta \left( 1 - \beta \right) \cdot T } } .
$$

Denote $m = 1 0 \%$ as the watermark ratio and let $\alpha = 9 5 \%$ , $\beta = 1 0 \%$ , and $T = 2 0 0$ . In the detection stage, a watermarked speech will produce $| f | _ { w } = \alpha { \cdot } m { \cdot } T + \beta { \cdot } ( \bar { 1 } - m ) { \cdot } T =$ 37, which means the $\textbf { \em z }$ -statistic is 4.01 and the one-sided pvalue is $3 \times 1 0 ^ { - 5 }$ approximately. In this case, the utterancelevel probability of a false positive is only $3 \times 1 0 ^ { - 5 }$ , indicating that the watermark can be easily detected with extremely high confidence. Moreover, since $m$ can be adjusted in inference, users are free to decide whether to add more watermarks to enhance robustness or reduce watermarks to enhance imperceptibility. The summary of the proposed inference strategies is in Appendix D.

# Experiments

# Experimental Setup

Datasets. For training, we employ the standard training set of LibriTTS (Zen et al. 2019), which contains approximately 585 hours of English speech at 24kHz sampling rate. For the Short-Time Fourier Transform operation (STFT), we adopt a filter length of 400, a hop length of 80, and a window function applied to each frame with a length of 400. In our experiment, we find that a smaller hop length will increase the encoding capacity of the watermark, but setting the hop size too small is harmful for speech reconstruction. For evaluation, we adopt two state-of-the-art zero-shot voice cloning models, NaturalSpeech 2 (Shen et al. 2023) and Mega-TTS 2 (Jiang et al. 2023a), to generate high-quality synthesized audio that sounds authentic. We randomly select 100 text transcriptions and 100 speech prompts from the LibriTTS test-clean set. Each speech prompt is fed into the voice cloning model to generate speeches according to the 100 target sentences. The test set also includes all of the speech samples from the “testclean” set of LibriTTS. As a result, a test set consisting of 24,837 sentences is obtained, with all speakers in the test set being unseen. We use all samples in the test set for evaluations. We provide implementation details in Appendix A. Evaluation Metrics. For imperceptibility, we adopt Signalto-Noise Ratio (SNR) and Perceptual Evaluation of Speech Quality (PESQ) (Rix et al. 2001) as metrics following previous works (Liu et al. 2023b). Among them, SNR is only used to measure the magnitude of differences between the watermarked speech and the original speech. In comparison, PESQ provides a more accurate assessment of imperceptibility by considering the specifics of the human auditory system.

Table 1: Comparison with existing speech watermarking methods for information hiding. “MEAN” represents the average BER. “Ours-32bps” means we insert 32 bits of watermark information to one-second speech segments in inference.   

<html><body><table><tr><td rowspan="2">Models</td><td rowspan="2">BPS(↑)</td><td rowspan="2">PESQ(↑)</td><td rowspan="2">SNR(↑)</td><td colspan="10">BEMP3MF</td></tr><tr><td>ND</td><td>GN</td><td>AS</td><td>RS</td><td></td><td></td><td>LP</td><td>EA</td><td>MEAN</td></tr><tr><td>Audiowmark*</td><td>20</td><td>4.39</td><td>29.85</td><td>5.89</td><td>18.13</td><td>5.89</td><td>15.10</td><td>6.65</td><td>12.83</td><td>5.89</td><td>7.61</td><td>9.75</td></tr><tr><td>DeAR</td><td>8.8</td><td>3.75</td><td>26.31</td><td>0.45</td><td>0.48</td><td>0.46</td><td>0.42</td><td>0.58</td><td>0.48</td><td>0.91</td><td>0.51</td><td>0.54</td></tr><tr><td>Chang Liu's</td><td>30</td><td>3.97</td><td>24.18</td><td>0.00</td><td>2.68</td><td>0.00</td><td>0.02</td><td>0.00</td><td>0.06</td><td>0.00</td><td>0.04</td><td>0.35</td></tr><tr><td>WavMark</td><td>32</td><td>4.31</td><td>38.61</td><td>0.43</td><td>5.72</td><td>0.61</td><td>0.65</td><td>0.56</td><td>6.07</td><td>2.08</td><td>4.49</td><td>2.58</td></tr><tr><td>Ours-32bps</td><td>32</td><td>4.45</td><td>38.08</td><td>0.12</td><td>0.73</td><td>0.12</td><td>0.17</td><td>0.13</td><td>0.69</td><td>0.19</td><td>0.12</td><td>0.28</td></tr></table></body></html>

For evaluating the effectiveness and robustness of watermark extraction, we use the bit error rate (BER) as the metric. For encoding capacity, we use bit per second (BPS) as the metric, which refers to how many bits of watermark information can be injected into one second of speech.

# Results of Information Hiding

In this subsection, we compare our DiscreteWM with different baseline systems to evaluate its ability of information hiding. To demonstrate the performance of different models in a concise and fair manner, we conduct a segment-based evaluation where we randomly extract a 1-second speech segment from each test sample. In this evaluation, the models aim to watermark one-second speech clips while remaining robust against various distortions and maintaining imperceptibility. The distortions include: 1) no distortion (ND); 2) Gaussian noise (GN); 3) amplitude scaling (AS); 4) re-sampling (RS); 5) MP3 compression (MP3); 6) median filter (MF); 7) lowpass filter (LP); 8) echo addition (EA); We provide further explanation for these distortions in Appendix B.

We compare our model with existing SOTA neural network based methods: 1) Audiowmark (Westerfeld 2020), a SOTA traditional watermarking toolkit that utilizes the patchwork-based watermarking method (Liu, Huang, and Huang 2018) and incorporates BCH codes (Bose and RayChaudhuri 1960) for error correction. We used the default setting of Audiowmark; 2) DeAR (Liu et al. 2023a), one of the pioneer deep learning frameworks for robust speech watermarking; 3) Chang Liu’s method (Liu et al. 2023b), a strong and robust baseline that embeds the watermark into the frequency domain; 4) WavMark (Chen et al. 2023), a concurrent solution that employs invertible neural networks (INN) to ensure the inaudibility and robustness. Since we found that Audiowmark can hardly embed watermarks into the one-second speech segment, we use the utterance-level evaluation for it. The encoding capacity of Audiowmark is referenced from previous works (Chen et al. 2023). Although WavMark has an encoding capacity of $3 2 b p s$ , it still requires 10 to 16 bits of information for watermark localization.

Increased signal-to-noise ratio (SNR) and perceptual evaluation of speech quality (PESQ) score indicate higher imperceptibility, while a lower bit error rate (BER) represents superior robustness. In the imperceptibility evaluation, distortions are not applied to the watermarked speech. As shown in Table 1, our speech watermarking method, referred to as ours-32bps, achieves comparable imperceptibility to Wav

Table 2: Evaluation for AI-generated speech detection. “MEAN” represents the average BER across all distortions. The RTF (Real-Time Factor) evaluation is conducted with 1 NVIDIA A100 GPU and batch size 1.   

<html><body><table><tr><td>Models</td><td>PESQ(↑)</td><td>SNR(↑)</td><td>MEAN(↓)</td><td>RTF(↓)</td></tr><tr><td>WavMark</td><td>4.24</td><td>37.92</td><td>1.02</td><td>0.1438</td></tr><tr><td>SeamlessWM*</td><td>3.77</td><td>29.62</td><td>0.18</td><td>0.0065</td></tr><tr><td>Ours</td><td>4.37</td><td>38.01</td><td>0.32</td><td>0.0044</td></tr></table></body></html>

Mark and is on par with Chang Liu’s approach in terms of robustness. This indicates that our method achieves a superior balance between imperceptibility and robustness, thus further validating the effectiveness of the discrete representations.

# Watermarking for AI-Generated Speech Detection

In this subsection, we compare our DiscreteWM with different baseline systems to evaluate its ability to effectively put and detect an imperceptible watermark on top of AIgenerated speech. To ensure reliable protection across various audio lengths in real-world applications, it is important for the model to accurately locate the positions of the watermarks and decode the original watermark. Therefore, in this experiment, we conduct an utterance-level evaluation. As for the baseline systems, in addition to Audiowmark and WavMark, we also include SeamlessWM (Duquenne et al. 2023), which is a state-of-the-art concurrent work focused on detecting AI-generated content. Since SeamlessWM does not provide the pre-trained models and source code, we use the reproduced version for our experiments. We evaluate the imperceptibility (PESQ and SNR), robustness (MEAN: the averaged BER $( \% )$ across all distortions), and inference efficiency (RTF) of these systems. The distortions follow the same setting in Section Results of Information Hiding. In addition, when measuring RTF, we include both the watermark embedding and detection processes. We set the watermark ratio $m$ of DiscreteWM to $\mathrm { \bar { 1 0 \% } }$ .

The results presented in Table 2 indicate that our method achieves comparable robustness compared to SeamlessWM, while also exhibiting superior imperceptibility. It also demonstrates that our method can provide a highly effective and reliable security guarantee for online speech synthesis services. In terms of the inference speed, the RTF of WavMark is significantly higher than other methods. In the experiments, we find that the sliding window localization process costs most of its inference time. Meanwhile, compared with WavMark, our frame-wise solution speeds up the speech watermarking process by 22.1x.

![](images/5e8898e53700fb16712c486b005aa1a10174e70b50fa7572f333142c8768cf81.jpg)  
Figure 3: Visualizations of the ground-truth and watermarked mel-spectrograms by different speech watermarking methods. For a fair comparison, we directly download the example from WavMark’s demo page and use the pre-trained Chang Liu’s model.

# Ablation Studies

Encoding Capacity. Our method can flexibly change the encoding capacity during the inference process. In this experiment, we evaluate the performance of DiscreteWM using various encoding capacities on the information hiding task. As shown in Table 3, we can see that DiscreteWM maintains a high level of imperceptibility when its encoding capacity ranges from $I O$ to 50bps, and it also performs well even under the extreme condition of 150bps.

Discrete vs Continuous. We evaluate the performance of DiscreteWM using discrete intermediate representation and continuous representation on the information hiding task. To make fair comparisons, we only remove the VQ layer and replace the manipulator with a watermark encoder to build the continuous baseline. The encoding capacity of the continuous baseline is set to 32bps. From Table 3, it can be seen that our method with discrete intermediate representation achieved a better balance between imperceptibility and robustness than the continuous baseline, demonstrating the advantages of discrete intermediate representation.

Manipulator vs Manual. We test the effectiveness of the proposed manipulator model on the information hiding task. The encoding capacities of baseline systems in this experiment are set to 32bps. For “wo/ manipulator”, we manually choose random codes for watermark embedding. The results in Table 3 demonstrate that without the manipulator, the imperceptibility of our method significantly drops, indicating the advantages of the proposed manipulator.

Utterance-level Reliability. In this experiment, we evaluated the utterance-level reliability of DiscreteWM on the AI-generated content detection task with the Z-test. The segment-wise methods like WavMark can only determine that the speech contains watermarks when the extracted watermark is the same as the preset one, which is not suitable for the proposed Z-test. Therefore, we do not compare our method with them here. In this evaluation, the watermarked speech is randomly attacked with the distortions following Section Results of Information Hiding. We visualize the Zstatistic score (reliability) and PESQ (Imperceptibility) with different watermark ratios $m$ in Figure 4. When the watermark ratio $m$ is 0.03, the $Z$ -statistic is 4.07. In this case, the false positive rate is only $2 . 3 \times 1 0 ^ { - 5 }$ . Moreover, given the $Z$ -statisti $\scriptstyle : = 4 . 0$ as the classification threshold, the utterancelevel true positive rate and false positive rate are 1.0 and 0.0 when the watermark ratio is above 0.10. These results indicate that our method exhibits high imperceptibility while maintaining a high level of accuracy.

Table 3: Ablation studies of DiscreteWM for information hiding.   

<html><body><table><tr><td>Setting</td><td>PESQ(↑)</td><td>SNR(↑)</td><td>MEAN(↓)</td></tr><tr><td>Ours-10bps</td><td>4.47</td><td>41.30</td><td>0.31</td></tr><tr><td>Ours-32bps</td><td>4.45</td><td>38.08</td><td>0.28</td></tr><tr><td>Ours-50bps</td><td>4.27</td><td>34.92</td><td>0.30</td></tr><tr><td>Ours-150bps</td><td>3.92</td><td>28.49</td><td>0.27</td></tr><tr><td>w/ continuous</td><td>4.32</td><td>34.90</td><td>2.39</td></tr><tr><td>wo/ manipulator</td><td>3.96</td><td>29.55</td><td>0.95</td></tr></table></body></html>

![](images/2ef02924aca4ac5abc309f1fa1cd549ff270472187d4a573b1399b5d6b140fbf.jpg)  
Figure 4: The tradeoff between reliability and imperceptibility on the AI-generated content detection task. $^ { \mathrm { \scriptsize ~ \cdot ~ Z . ~ } }$ -statistic $\mathbf { \tau } = \mathbf { \tau }$ $4 . 0 ^ { \cdot }$ is shown as the red dashed line.

# Conclusions

In this paper, we present DiscreteWM, a framework that injects watermarks within the discrete intermediate representations of speech. Our approach outperforms the continuous counterparts in terms of robustness and imperceptibility. Besides, our frame-wise solution allows for encoding 1 to 150 bits of watermark information into only a 1-second speech clip, demonstrating its flexibility and encoding capacity. The proposed utterance-level Z-test also indicates the reliability of our method for voice cloning detection.