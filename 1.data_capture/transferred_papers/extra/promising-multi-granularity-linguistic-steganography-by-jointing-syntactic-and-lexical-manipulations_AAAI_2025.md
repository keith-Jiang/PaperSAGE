# Promising Multi-Granularity Linguistic Steganography by Jointing Syntactic and Lexical Manipulations

Chengfu Ou, Lingyun Xiang\*, Yangfan Liu

School of Computer and Communication Engineering, Changsha University of Science and Technology hahally $@$ stu.csust.edu.cn, xiangly $@$ csust.edu.cn, flyvan $@$ stu.csust.edu.cn

# Abstract

Existing modification-based linguistic steganography methods primarily perform linguistic manipulations within a single embedding space to conceal secret information. However, these methods are stringently constrained by the original semantics of the cover text, making it struggle to achieve a satisfactory embedding capacity in a single embedding space. In this paper, we propose a novel Multi-granularity Modification-based Linguistic Steganography framework (MMLS) that hides secret information in both syntactic space and symbolic space, enhancing syntactic naturalness and semantic coherence while further increasing embedding capacity. Specifically, MMLS utilizes a paraphrase generation model to automatically modify the syntactic structure of the given original sentence, which enables the generation of paraphrases and the preservation of semantics simultaneously. Moreover, MMLS employs a distance-aware syntactic bins coding strategy to embed part of secret information into the syntactic space. This strategy utilizes a cluster-based way to partition the implicit syntactic space into a finite number of separate zones, thus increasing the number of candidate paraphrases and avoiding the selection of semantically distorted steganographic texts. Finally, the pre-trained BERT is used to replace some words in candidate paraphrases with their synonyms. Such a design embeds the remaining secret information into symbolic space while ensuring syntactic and semantic naturalness. Experimental results demonstrate that MMLS significantly outperforms existing methods in terms of semantic coherence, embedding capacity, and security.

Code — https://github.com/hahally/MMLS/

# Introduction

Steganography is recognized as the art and science of embedding secret information within a public multimedia carrier without arousing suspicion from supervisors (Simmons 1984). Theoretically, any public multimedia carriers with redundant space, such as images (Zhou et al. 2023), audios (Wu et al. 2020), videos (Fan, Zhang, and Zhao 2022), texts (Krishnan, Thandra, and Baba 2017), etc., can be utilized to conceal secret information for communication. Among them, texts are the most common ones on today’s social media networks, due to their simplicity and efficiency in conveying information. Therefore, linguistic steganography (Xiang et al. 2022), which employs texts as the carriers to conceal and transmit secret information, has garnered increasing attention in recent years. Modification-based linguistic steganography (MLS), as a significant branch of linguistic steganography, primarily conceals secret information into a given natural text by subtly modifying its content using specific transformations, which helps preserve the original semantic meaning, such as synonym substitutions(Chang and Clark 2010b, 2014; Huo and Xiao 2016; Xiang et al. 2018), paraphrasing (Chang and Clark 2010a; Wilson and Ker 2016), machine translation (Stutsman et al. 2006; Meng et al. 2011), word order adjustment (Chang and Clark 2012), and syntactic analysis (Murphy and Vogel 2007). Nevertheless, the inherent limitations posed by limited conversion rules and original semantic constraints result in a deficient embedding capacity of MLS. Moreover, inappropriate substitution operations are more likely to result in steganographic texts with syntactic unnaturalness and semantic inconsistencies (Grosvald and Orgun 2011), making them susceptible to be detected by linguistic steganalysis methods (Wen et al. 2019; Yang et al. 2019; Peng et al. 2021).

To tackle the aforementioned challenges, the latest works, such as substitution-based (Ueoka, Murawaki, and Kurohashi 2021; Xiang et al. 2023), paraphrasing-based(Yang et al. 2024) and syntactic transformation-based (Xiang, Ou, and Zeng 2024), strive to leverage the promising language models to enhance the diversity of linguistic transformations and move away from reliance on specific rules, thereby improving the performance of MLS. However, it is worth noting that these language models-based (LMs-based) methods are constrained by the original semantics of the cover text to perform minor linguistic transformation manipulations within a single redundant embedding space (symbolic space or syntactic space), thereby enhancing semantic consistency but limiting the embedding capacity. Therefore, LMs-based MLS gains less attention in improving embedding capacity.

A promising way is to combine syntactic and lexical linguistic manipulations to improve embedding capacity as shown in Figure 1. The syntactic transformation-based method first performs syntactic linguistic manipulations to change the syntactic structure of the cover text by the syntaxcontrolled paraphrase generation model. Subsequently, the

CT: why do some people like cats more than dogs ? Syntactic Space △ CT: is it true that people like cats more than dogs ? Secret Information: 1 0 0 1 1 0 ... Random Bins Selecting  Strategy   
( ROOT ( S ( NP ( PRP ) ) ( VP ( VBZ ) ( ADJP ) ( SBAR ) ) ( . ) ) ) 10 会 is it [ ] that people [ ] cats more than [ ] ? Syntactic Transformation Lexical Substitution 0001 Symbolic Space 0001 0001 Secret Information:   0 1 1 0 1 1 ...   
ST: is it true that people like cats more than dogs ? 10 1011 10 11 ST: is it possible that people hate cats more than people ?   
( ROOT ( SQ ( VBZ ) ( NP ( PRP ) ) ( ADJP ( JJ ) ( PP ) ) ( . ) ) ) possible hate people ( ROOT ( SQ ( VBZ ) ( NP ( PRP ) ) ( ADJP ( JJ ) ) ( SBAR ( IN ) ( S ) ) ( . ) ) ) low Semantics: 0.81 / Syntactic: 0.07 Semantics: 0.72 / Syntactic: 0.06

substitution-based method replaces the selected words in the input text with candidate words from symbolic space. Nevertheless, due to the particularity of the linguistic steganography task, directly jointing syntactic transformation-based and substitution-based methods will encounter some potential conflicts. For example, as shown in Figure 1, the syntactic transformation-based method can induce semantic deviation or syntactic errors in the generated text. Moreover, lexical linguistic manipulations further cause global semantic and minor syntactic changes, resulting in the steganographic text with syntactic unnaturalness and semantic deviation.

To overcome the limitations mentioned above, we propose a novel Multi-Granularity Modification-based Linguistic Steganography (MMLS) framework, which incorporates the advantages of syntactic transformation-based methods and substitution-based methods. More specifically, we introduce a syntax-controlled paraphrase generator by employing abundant syntactic structure information as a supplement to modify the expression forms with slight semantic changes in sentences. Differing from the previous method (Xiang, Ou, and Zeng 2024), which randomly and evenly divides an extensive collection of syntactic templates into a finite number of subsets, i.e., bins, this paper proposes a distance-aware syntactic bins coding strategy to refine the partitioning for subsets. Meanwhile, a pre-trained BERT is utilized to replace some words with their context-related synonyms by an adaptive autoregressive coding strategy. Moreover, we perform consistency checking to ensure that the steganographic text has low semantic distortion and satisfies the target syntactic template. Our major contributions are as follows:

• To our knowledge, it is the first modification-based linguistic steganography work that performs linguistic manipulations in multi-granularity spaces to embed secret information, attaining a substantial embedding capacity. • MMLS leverages a novel distance-aware syntactic bins coding strategy to efficiently mitigate the inherent interference introduced by randomness in previous work, thereby further enhancing semantic coherence. • MMLS utilizes a consistency checking unit on generated texts in terms of syntactic and semantics to ensure that secret information is successfully embedded and extracted. • Extensive experimental results show that MMLS not only significantly improves semantic coherence and security, but also achieves a satisfactory embedding capacity compared to baseline methods.

# Related Work

MLS primarily conceals secret information into a given natural text by subtly modifying its content using specific linguistic transformations while preserving the original semantic meaning. Initially, most researchers focus on constructing the painstaking transformation rules to control the intensity of modifications to the cover texts.

Synonym substitutions Chang and Clark (2014) propose to develop a novel linguistic steganography method based on context synonym substitution and vertex coding algorithm. Moreover, It uses the Google $n$ -gram corpus for checking the applicability of a synonym in context. Huo and Xiao (2016) propose a linguistic steganography method based on vector distance of two-gram dependency collocations to eliminate the obvious mistakes and logical misconceptions resulting from the inaccuracy of candidate synonyms. Xiang et al. (2018) combine arithmetic coding and synonym substitutions to quantize synonyms employed for carrying payload into an unbalanced and redundant binary sequence, which is compressed by adaptive binary arithmetic coding losslessly to provide a spare for accommodating additional data.

Paraphrasing Chang and Clark (2010a) propose to hide information in a cover text by using a large paraphrase dictionary, which consists of abundant painstaking paraphrase rules, and use the Google $n$ -gram corpus and a CCG parser to certify the paraphrasing grammaticality and fluency. Wilson and Ker (2016) amplify the extent of substitution transformations from mere synonymous words to more semantically similar phrases by employing paraphrasing rules and use distortion measures to automatically produce the best embedding steganographic text.

Syntactic transformation Murphy and Vogel (2007) perform a set of automated and reversible syntactic transforms that can hide information without changing the meaning or style of a text, achieving a success rate of $9 6 \%$ and bandwidth of 0.3 bits per sentence. Chang and Clark (2012) propose to use word order adjustment as the linguistic transformation to change the expression of the cover text. Meanwhile, they leverage a maximum entropy classifier to determine the naturalness of sentence permutations and select the promising expression to embed secret information.

Language models With advances in deep learning and natural language processing, much attention has been paid to improving the performance of MLS by language models. For example, Ueoka, Murawaki, and Kurohashi (2021) propose a novel substitution-based method that uses pre-trained model BERT to predict the candidate words at masked positions according to its context and improve the embedding capacity of MLS. Moreover, this method utilizes a sliding window masking strategy and skips the inappropriate subwords to avoid distorting the masking positions. Xiang et al. (2023) proposes a causal perception-guided linguistic steganography by elaborate and secure lexical substitutions using BERT. Yang et al. (2024) propose a novel MLS method based on an information encoding strategy, which utilizes a pivot translation-based paraphrasing and semantic-aware bins coding to change the expression of a given text. Xiang, Ou, and Zeng (2024) propose a novel sentence-level linguistic steganography framework, which employs abundant structural information as constraints to guide the syntax-controlled paraphrase generation model to modify expression forms of the cover text.

# Proposed Method Overall Architecture

As illustrated in Figure 2, the proposed MMLS framework includes three processing units, i.e., syntactic transformation unit, lexical substitution unit, and consistency checking unit. Concretely, the syntactic transformation unit is utilized to automatically modify the syntactic structure of the given cover text and generate paraphrases (defined as intermediate steganographic texts) by the syntax-controlled paraphrase generator. As a result, some secret information is embedded into syntactic space while preserving the semantics unchanged. Subsequently, the lexical substitution unit is employed to replace certain carefully selected words from an intermediate steganographic text by a pre-trained BERT, thereby hiding the remaining secret information in symbolic space. To ensure semantic coherence and the correct extraction of secret information, the consistency checking unit is adapted to filter candidate items from the lexical substitution unit to avoid text with illegal syntactic and semantics.

# Syntactic Transformation Unit

Figure 2 shows the overall sketch of the syntactic transformation unit. There are two main components: the syntaxcontrolled paraphrase generator and the distance-aware syntactic bins coding strategy, that play crucial roles in this workflow. As shown in Figure 2, Firstly, secret information is mapped into a discrete syntactic space by distance-aware syntactic bins coding strategy. Then the corresponding syntactic template is sampled from selected zones in syntactic space. Finally, the cover text and sampled syntactic template are fed to the syntax-controlled paraphrase generator to yield a steganographic text, which is expected to satisfy any syntactic template in selected zones while having the same meaning as the cover text.

Syntax-Controlled Paraphrase Generator To eliminate the reliance on a special set of manually designed rules in traditional MLS methods, we propose to leverage a syntaxcontrolled paraphrase generator to generate paraphrases with various syntactic structures, improving the quality and diversity of sentence transformations. As shown in Figure 2, the syntactic template, which guides the generation of different expressions, is represented as a partial constituency parse tree with $H$ layers. The syntactic template with larger $H$ can provide more detailed syntax structure information, which may lead to a better quality paraphrase.

In this paper, we use the architecture of the SI-SCP model (Yang et al. 2022) as the implementation of our syntaxcontrolled paraphrase generator. The syntax-controlled paraphrase generator mainly consists of three parts that are semantic encoder, syntactic encoder, and sentence decoder. Their structures are the same as the transformer architecture (Vaswani et al. 2017), where the core function is primarily a multi-head attention mechanism. Formally, we can represent the above process of modeling for the syntax-controlled paraphrase generator as follows:

$$
\left\{ \begin{array} { l l } { H _ { x } = S e m E n c o d e r ( x ) } \\ { H _ { t } = S y n E n c o d e r ( t ) \ : , } \\ { y = D e c o d e r ( H _ { t } , H _ { x } ) } \end{array} \right.
$$

where SemEncoder, SynEncoder and Decoder represent semantic encoder, syntactic encoder and sentence decoder, respectively. $y$ is denoted as the generated paraphrase, which is semantically consistent with $x$ while adhering to the syntactic structure prescribed by $t$ .

Distance-Aware Syntactic Bins Coding Strategy To mitigate the inherent interference introduced by randomness in previous work (Xiang, Ou, and Zeng 2024), we propose a novel distance-aware syntactic bins coding (DBC) strategy to partition syntactic templates into disjoint subsets by spectral clustering 1 $\mathrm { N g }$ , Jordan, and Weiss 2001).

Without the loss of generalization, let $T = \{ t _ { i } \} _ { i = 1 } ^ { m }$ represent the syntactic template set, i.e., syntactic space, which includes $m$ non-repeating syntactic templates. In this paper, we leverage the Stanford CoreNLP toolkit (Manning et al. 2014) to extract the syntactic template from each sentence. Given a pre-determined integer $k$ representing the bits of secret information embedded within each syntactic template, the binary coding states can be determined as $B = \{ b _ { i } \} _ { i = 1 } ^ { 2 ^ { k } }$ where $b _ { i } \in \{ 0 , 1 \} ^ { k }$ . We can construct a mapping function $\mathcal { F }$ to establish a close connection between the syntactic space and the binary code space, which maps each element in $T$ to a unique binary code in $B$ . To this end, we define the disjoint subsets as $\dot { b } i n s = \{ b i n _ { i } \} _ { i = 1 } ^ { 2 ^ { k } }$ divided from $T$ , where $T = \textstyle \bigcup _ { i = 1 } ^ { 2 ^ { k } } b i n _ { i }$ and $b i n _ { i } \bigcap b i n _ { j } = \varnothing , \forall 1 \leq i \neq j \leq 2 ^ { k }$ . For each $i \in [ 1 , 2 ^ { k } ]$ , $\mathcal { F }$ maps syntactic templates in $b i n _ { i }$ to the same binary code $b _ { i }$ corresponding to indices $i$ , i.e.,

$$
\forall t _ { i , * } \in b i n _ { i } , \mathcal { F } ( t _ { i , * } ) = b _ { i } ,
$$

DBC ROOT Intermediate ST Intermediate ST Candidate ST Candidate ST set the cat is on the mat. the cat is on the mat. the cat is on the couch. ? 01 00 S → Masking Strategy RCWP Coding Syntactic checking Syntax-Controlled floor 0.2205 floor 0 NN VBZ NP Paraphrase Generator the Pcraet-tirsa onendtBheER[TMASK]. ground 0.0793 pcpohouorcnhe 0.04636363636363689898989898984 phorocnhe 1101010101010010101010101 Semantic distortion measure 10 DT   
Secret Information:1 1 0 0 1 0... Syntactic Template CT: there is a cat on the mat. Encoding Strategy Secret Information:1 0 0 1 0 1 ... ST: the cat is on the couch.   
Syntactic Transformation Unit Semantics: 0.84 / Syntactic: 0.96 Lexical Substitution Unit Semantics: 0.76 / Syntactic: 0.96 Consistency Checking Unit

where $b _ { i }$ represents the binary code assigned to subset $b i n _ { i }$ and $^ { t _ { i , * } }$ is any syntactic templates in $b i n _ { i }$ . For example, all syntactic templates in $b i n _ { 4 }$ are mapped to $b _ { 4 } =  { \mathrm { ^ { 6 } 1 1 } } ^ { \prime \prime }$ if $k =$ 2, i.e, $\mathcal { F } ( e ) = \left. \ : 1 1 ^ { \mathfrak { N } } \right.$ if $e \in b i n _ { 4 }$ . Obviously, we can encode and decode a given sentence as a binary code by identifying its syntactic template.

It is noted that our focus is on how to construct disjoint subsets bins (i.e., $\mathcal { F } _ { \mathcal { L } }$ ). There is an underlying assumption: when a syntactic template guides a syntax-controlled paraphrase generator to perform syntactic transformations on the given sentence, the syntactic template of the generated sentence should be consistent with or similar to the cover text. Under this assumption, the core idea of DBC is to group similar syntactic templates into the same subset by spectral clustering. To this end, we establish an undirected weighted graph $G \ = \ ( T , S )$ , where nodes correspond to syntactic templates set $T$ and edges $S$ are typically represented by a weighted adjacency matrix that reveals the similarities between syntactic templates. In this work, we mainly explore the construction of a similarity matrix based on the distance between syntactic templates, resulting in disjoint subsets bins. We propose three strategies (DBC-LD, DBC-TED, and DBC-CD) based on three different distance measures, i.e., Levenshtein Distance, Tree-Edit Distance (Zhang and Shasha 1989) and Cosine Distance. Our proposed MMLS uses the DBC-CD strategy in syntactic transformation unit.

(1) DBC-LD first linearizes the syntactic template for tree structure as a string sequence. For example, the syntactic template in Figure 2 can be linearized as “(ROOT(S(NP(DT NN)VP(VDZ PP(IN NP(DT NN))).)))”. Afterward, calculating the Levenshtein distance between any two syntactic templates becomes straightforward. Levenshtein distance is the minimum number of edit operations (i.e., substitution, deletion, and insertion) required to convert one string into another, typically solved using dynamic programming. To accommodate edge weights in spectral clustering, we convert the distance matrix into a similarity matrix as follows:

$$
S _ { i , j } = 1 - \frac { L D _ { i , j } } { m a x ( L _ { t _ { i } } , L _ { t _ { j } } ) } ,
$$

where $S _ { i , j }$ is the edge weight between syntactic templates $t _ { i }$ and $t _ { j }$ and $L D _ { i , j }$ is the Levenshtein distance between $t _ { i }$ and $t _ { j }$ . $\boldsymbol { L } _ { t _ { i } }$ and $L _ { t _ { j } }$ represent the sequence lengths of $t _ { i }$ and $t _ { j }$ .

(2) DBC-TED can directly handle syntactic templates without linearization, preserving more information about the skeleton structure. TED is a measure of similarity between two tree-structured data objects, which is defined as the minimum number of operations required to transform one tree into another. These operations typically include node insertion, node deletion, and node substitution. Therefore, DBCTED accounts for not only the labels of the nodes but also the entire subtree structures, ensuring a comprehensive comparison. Similarly, after obtaining the distance matrix calculated according to TED, it is transformed into a similarity matrix as follows:

$$
S _ { i , j } = 1 - \frac { T E D _ { i , j } } { m a x ( L _ { t _ { i } } , L _ { t _ { j } } ) } ,
$$

where $T E D _ { i , j }$ is the tree-edit distance between $t _ { i }$ and $t _ { j }$ .   
$L _ { t _ { i } }$ and $L _ { t _ { j } }$ represent the number of nodes for $t _ { i }$ and $t _ { j }$ .

(3) DBC-CD, compared to the above two strategies, is no longer limited to the shallow structural similarity of syntactic templates but rather focuses on the distance relations of syntactic templates on the latent feature space. Syntactic templates can be mapped as continuous feature representations with a fixed dimension in latent space by the syntactic encoder of the syntax-controlled paraphrase generator according to Eq. (1). As a result, averaging the feature vectors of all nodes can accommodate syntactic templates with any number of nodes. The cosine similarity score between syntactic templates $t _ { i }$ and $t _ { j }$ is calculated as follows:

$$
S _ { i , j } = \frac { Z _ { t _ { i } } \cdot Z _ { t _ { j } } } { \Vert Z _ { t _ { i } } \Vert \Vert Z _ { t _ { j } } \Vert } ,
$$

where $Z _ { t _ { i } }$ and $Z _ { t _ { j } }$ are the syntactic feature representations of $t _ { i }$ and $t _ { j }$ , respectively.

Suppose the current embedded $k$ bits secret information is $b = \mathbf { \nabla } ^ { \left. \zeta \right. 1 ^ { \prime } }$ as shown in Figure 2, all syntactic templates in subset $b i n _ { 4 }$ are selected to guide the transformations of cover text, resulting in sufficient intermediate steganographic texts $\mathbb { O } = \{ \bar { O _ { i } } \} _ { i = 1 } ^ { | b i n _ { 4 } | } \}$ provided to find a promising one to replace cover text $x$ for embedding information $b$ . Note that if $\mathbf { \bar { \mathcal { F } } } ( T _ { x } ) = b$ , where $T _ { x }$ is the syntactic template of $x$ , it indicates that cover text $x$ can successfully conceal secret information $b$ . In this case, cover text $x$ will be added to the intermediate steganographic text set $\mathbb { O }$ .

# Lexical Substitution Unit

Figure 2 presents the overall sketch of the lexical substitution unit. In general, given a text where some tokens are replaced with the special token [MASK], BERT is trained to recover the original tokens based only on their context. In this work, we first determine the replaceable positions in a given text by a masking strategy and then leverage the pretrained BERT to establish an appropriate replaceable candidate word pool (RCWP) for each target position according to its context. Finally, we utilize the encoding strategy to embed secret information by selecting words from RCWP.

Masking Strategy Concretely, let the intermediate steganographic text $O = \{ w _ { i } \} _ { i = 1 } ^ { n }$ with $n$ words. To ensure that secret information can be extracted correctly, the data sender and receiver must mask the same position. In this paper, we try to perform a simple and effective masking strategy. Following (Ueoka, Murawaki, and Kurohashi 2021) , we set a step $s$ and skip “anchor” words (such as punctuation, number, non-initial subwords, and stopwords) to control the positions of [MASK]. Mathematically, the masking strategy can be defined as:

$$
M ( w _ { i } ) = \left\{ \begin{array} { l l } { [ { \bf M A S K } ] , } & { \mathrm { ~ i f ~ } i \% s = 0 \mathrm { ~ a n d ~ } w _ { i } \notin Q } \\ { w _ { i } , } & { o t h e r w i s e } \end{array} \right. ,
$$

where $M ( \cdot )$ represents the masking function and $Q$ denotes as the “anchor” words set. $w _ { i }$ is the $i$ -th word in $O$ . Significantly, modifying “anchor” words is a very dangerous move that may lead to awkward expressions, arousing the supervisor’s suspicion. According to Eq. (6), a new text $O _ { m a s k }$ with [MASK] can be inputted to BERT, which provides a probability distribution for each masked position based on its suitability within the given context.

Encoding Strategy Subsequently, an information encoding strategy is employed to choose a word as the output for the present masked position, whose probability is proportional to its prediction probability obtained by BERT. To avoid contextual semantic conflicts, we utilize an “autoregressive” strategy to predict words for each masked position from left to right under the control of secret information, enhancing the semantic coherence between contexts.

Formally, for the first masked position in $O _ { m a s k }$ , let $P =$ $\{ p _ { i } \} _ { i = 1 } ^ { | V | }$ denote the predicted probability distribution in desTcoenadvionigd soerldecr fnogr iwnoarpdpsroipn itahte vwoocradbsulawrityh $V = \{ v _ { i } \} _ { i = 1 } ^ { | V | }$ ity, we set a threshold $\tau$ to truncate the probability distribution, thereby obtaining the replaceable candidate word pool RCWP, which satisfies the following criteria:

$$
\forall v _ { i } \in \mathbf { R C W P } , \frac { p _ { i } } { p _ { 1 } } \geq \tau ,
$$

where $v _ { i }$ is the $i$ -th word in RCWP and $p _ { i }$ is the probability for $v _ { i }$ . The initial RCWP contains the word $v _ { 1 }$ with the highest probability $p _ { 1 }$ . To perform Huffman coding to map each candidate word in RCWP to binary code, we normalize the corresponding prediction probabilities for RCWP to construct a Huffman tree, where each leaf node represents a candidate word assigned the unique binary code.

The corresponding leaf node (i.e., candidate word) is selected as the output of the current masked position according to the secret information that needs to be hidden. Then, $O _ { m a s k }$ is updated by replacing the first [MASK] with the determined candidate word that carries secret information. Perform the above operation for each of the next masked positions in turn to obtain a candidate steganographic text $O ^ { \prime }$ . Therefore, the candidate steganographic text set $\mathbb { O } ^ { \prime } \ = \ \{ O _ { i } ^ { \prime } \} _ { i = 1 } ^ { | b i n _ { 4 } | }$ can be produced from intermediate steganographic text set $\mathbb { O }$ by lexical substitution unit.

# Consistency Checking Unit

After undergoing syntactic transformation and lexical substitution, we need to perform the consistency checking unit to ensure that the secret information is successfully embedded. Especially, improper syntactic transformations can result in the generated paraphrases not conforming to the expected syntactic structures and even deviating from the original semantics. This further leads to poor quality in the subsequently generated candidate steganographic texts, making them easily detectable by steganalysis methods. To mitigate this problem, we perform a syntax checking and semantic distortion measure to output high-quality texts.

Specifically, we first perform the Stanford CoreNLP toolkit to extract syntactic templates from $\mathbb { O } ^ { \prime }$ and then further check and filter out paraphrases whose syntactic templates do not belong to $b i n _ { 4 }$ , obtaining a temporary steganographic text set $\mathbb { C } = \{ C _ { i } \} _ { i = 1 } ^ { | \mathbb { C } | }$ . Subsequently, a semantic distortion measure is utilized to the semantic distance between $C _ { i } \in \mathbb { C }$ and cover text $x$ .

$$
s _ { i } = \frac { E _ { x } \cdot E _ { C _ { i } } } { \Vert E _ { x } \Vert \Vert E _ { C _ { i } } \Vert } ,
$$

where $E _ { x }$ and $E _ { C _ { i } }$ are the semantic feature vectors with a fixed dimension corresponding to $x$ and $C _ { i }$ , respectively. $E _ { x }$ and $E _ { C _ { i } }$ can be calculated by averaging $H _ { x }$ and $H _ { C _ { i } }$ according to Eq. (1), respectively. The semantic distortion scores for $\mathbb { C }$ can be obtained by Eq. (8). Ultimately, the temporary steganographic text with the lowest score will be selected as the final steganographic text $x ^ { \prime }$ .

# Secret Information Extraction

To extract secret information from multi-granularity spaces (syntactic space and symbolic space) without errors, the data sender must share the same DBC strategy and lexical substitution unit with the data receiver. It’s worth noting that the proposed MMLS can independently extract the secret information in parallel from syntactic space and symbolic space.

For the secret information in syntactic space, it is easy to extract $b$ from steganographic text $x ^ { \prime }$ through Stanford CoreNLP toolkit and bins. The syntactic template $T _ { x ^ { \prime } }$ is first identified from $x ^ { \prime }$ . Then, the data receiver can determine the subset contained $T _ { x ^ { \prime } }$ to extract the corresponding secret information $b$ . It is pointed out that there is no need for the data receiver to keep the trained syntax-controlled paraphrase generator and the original cover text, which reduces the shared side information and improves security to some extent. For the secret information in symbolic space, the data receiver needs to find the same masked position according to the masking strategy, and then execute the shared encoding strategy to recover the Huffman tree through prediction probability from BERT. Secret information can be extracted from the encoded leaf nodes according to the current word.

<html><body><table><tr><td>Method</td><td>Parameters</td><td>bpw↑</td><td>BLEU↑</td><td>MAUVE↑</td><td>SIM↑</td><td>Acc↓</td><td>F1↓</td></tr><tr><td>PhraseLS</td><td>1</td><td>0.3112</td><td>0.3577</td><td>0.7786</td><td>0.6456</td><td>0.7819</td><td>0.7953</td></tr><tr><td> SPLS</td><td>s=3,l=1</td><td>0.3333</td><td>0.2579</td><td>0.7958</td><td>0.6314</td><td>0.7380</td><td>0.7396</td></tr><tr><td>HISS</td><td>H=4,k=4</td><td>0.3332</td><td>0.4781</td><td>0.9202</td><td>0.6760</td><td>0.7310</td><td>0.7095</td></tr><tr><td>LSCD</td><td>H=4,k=4</td><td>0.3327</td><td>0.4765</td><td>0.9323</td><td>0.7904</td><td>0.7195</td><td>0.6846</td></tr><tr><td>MMLS</td><td>H=4,k=4/s=2,τ=0.1</td><td>0.6305</td><td>0.3299</td><td>0.8585</td><td>0.7377</td><td>0.7205</td><td>0.6702</td></tr><tr><td>PhraseLS</td><td>1</td><td>0.2416</td><td>0.3577</td><td>0.7786</td><td>0.6456</td><td>0.7542</td><td>0.7604</td></tr><tr><td> SPLS</td><td>s=4,l=1</td><td>0.2500</td><td>0.2579</td><td>0.7958</td><td>0.6314</td><td>0.7187</td><td>0.7173</td></tr><tr><td>HISS</td><td>H=4,k=3</td><td>0.2487</td><td>0.5731</td><td>0.9557</td><td>0.7356</td><td>0.7005</td><td>0.6424</td></tr><tr><td>LSCD</td><td>H=4,k=3</td><td>0.2486</td><td>0.5748</td><td>0.9643</td><td>0.8307</td><td>0.6550</td><td>0.5988</td></tr><tr><td>MMLS</td><td>H=4, k=3/s=2,T=0.1</td><td>0.5561</td><td>0.3900</td><td>0.8719</td><td>0.7716</td><td>0.6875</td><td>0.6381</td></tr></table></body></html>

Table 1: Comparative experimental results of different linguistic steganography methods.

# Experiments and Analysis Datasets and Implementation Details

In the experiments, we select the QQP-Pos dataset (Yang et al. 2022) consisting of 140000 training samples, 3000 validation samples, and 3000 test samples to train the syntaxcontrolled paraphrase generator. We set the hidden state size to 256, the filter size to 1024, and the head number to 4. The number of layers of the semantic encoder, sentence decoder, and syntactic encoder are set to 4, 4, and 3, respectively. We use Adam optimizer (Kingma and Ba 2015) with a learning rate of 1e-4, and the number of training epochs is 50, with a batch size of 32. Moreover, BERT is initialized with pretrained bert-base-uncased from Hugging Face2. The above test samples are also used as cover texts. And we generate 3000 steganographic texts for each experiment.

# Evaluation Metrics

We test our method from three aspects: embedding capacity, text quality, and security. The embedding capacity is usually evaluated by embedding rate, which means the average bits of secret information embedded per word (bpw). For text quality, we measure text fluency and semantic consistency of the generated steganographic texts by employing BLEU(Papineni et al. 2002), MAUVE (Pillutla et al. 2021) and SIM (Zhang et al. 2020) as metrics. MAUVE tends to measure the difference in statistical distribution between generated steganographic text and naturally innocent text in terms of KL divergence. SIM is utilized to measure semantic similarity between steganographic texts and cover texts. To assess the security, we select a promising steganalysis method (Peng et al. 2021) to distinguish steganographic texts from cover ones. The detection Accuracy (Acc) and F1-score (F1) are employed as metrics for evaluating the anti-steganalysis ability of linguistic steganography methods. The lower the Acc and F1, the stronger the security.

# Baselines

For a fair comparison, we rebuilt some typical baselines as follows. (1) HISS (Xiang, Ou, and Zeng 2024) utilizes a syntax-controlled paraphrase generator to modify the syntactic template of the cover text automatically and then embeds secret information into syntactic space by syntactic bins coding strategy. (2) SPLS (Yang et al. 2024) employs advanced paraphrasing techniques based on pivot translation to modify the given cover text and embeds secret information into symbolic space. (3) PhraseLS (Wilson and Ker 2016) is a traditional MLS method that uses paraphrase rules for word or phrase substitution. Moreover, to evaluate the performance of the proposed DBC strategy, we implement three variant steganography methods with different distance measures (LD, TED, and CD), namely LSLD, LSTED, and LSCD, respectively. They embed secret information in syntactic space by skipping the lexical substitution unit.

# Results and Analysis

Comparative Experiment To evaluate the performance of our proposed MMLS, we show the comparative results between MMLS and other baselines in Table 1. For similar embedding capacity, LSCD shows higher values on BLEU, and MAUVE, and lower values on detection Acc, and F1, which implies it significantly outperforms other baselines regarding text quality and anti-steganalysis capability. Although the text quality of MMLS is lower than that of LSCD, the embedding capacity is nearly doubled and the steganography resistance is only slightly reduced. Since the lexical substitution unit utilizes the pre-trained BERT model to predict candidate words for elaborately masked positions and sets a threshold $\tau$ to avoid selecting inappropriate words, it only makes slight changes in the intermediate steganographic text generated by the syntactic transformation unit. Compared to baselines, MMLS still has the highest SIM values and lowest Acc values. Particularly, MMLS achieves SIM values that are $5 \%$ higher than other baselines, which shows MMLS’s superior capability in preserving semantic consistency. MMLS achieves a large embedding capacity while improving semantic consistency and security.

Comparison of Different Syntactic Coding Strategies As shown in Table 2, we present the experimental results of different syntactic bins coding strategies with different parameters ( $k$ and $H$ ). A larger $k$ indicates a larger embedding capacity per sentence but a lower text quality. Since larger $k$ implies less candidate steganographic text for each cover text, it is more likely to choose one with higher semantic distortion and makes steganographic text more prone to identification. When $H = 4$ , it is evident from Table 2 that the text quality of LSCD is overall higher compared to other strategies. When $H = 3$ , LSCD shows better SIM results than others, which implies LSCD’s superior capability of preserving semantic consistency. From the results of BLEU and MAUVE, LSLD, LSTED, and LSCD can generate more fluent steganographic text conforming to the statistical distribution of natural text compared to HISS from Table 1. Additionally, at the same parameters set, LSCD has the best semantic coherence and anti-steganalysis ability in almost all methods. This is because LSCD utilizes more substantial structure information from hidden syntactic features captured by the syntactic encoder to compute the cosine similarity, which can measure the similarity between syntactic templates from a more comprehensive perspective.

Table 2: Experimental results of different syntactic bins coding strategies in syntactic space steganography.   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">k</td><td colspan="5"></td><td colspan="5">MAUVE↑H =3↑</td></tr><tr><td>BLEU↑</td><td>MAUVE↑H =IM↑</td><td></td><td>Acc↓</td><td>F1↓</td><td>BLEU↑</td><td></td><td></td><td>Acc↓</td><td>F1↓</td></tr><tr><td rowspan="4">LSLD</td><td>1</td><td>0.7164</td><td>0.9427</td><td>0.8830</td><td>0.5735</td><td>0.5167</td><td>0.7883</td><td>0.9718</td><td>0.9097</td><td>0.5760</td><td>0.5504</td></tr><tr><td>2</td><td>0.6469</td><td>0.9510</td><td>0.8531</td><td>0.6350</td><td>0.5913</td><td>0.6361</td><td>0.8858</td><td>0.8490</td><td>0.6920</td><td>0.6504</td></tr><tr><td>3</td><td>0.5199</td><td>0.9068</td><td>0.8022</td><td>0.7185</td><td>0.6839</td><td>0.5147</td><td>0.8236</td><td>0.7904</td><td>0.7680</td><td>0.7542</td></tr><tr><td>4</td><td>0.4131</td><td>0.8202</td><td>0.7576</td><td>0.7700</td><td>0.7519</td><td>0.4226</td><td>0.8055</td><td>0.7559</td><td>0.7985</td><td>0.7832</td></tr><tr><td rowspan="4">LSTED</td><td>1</td><td>0.7756</td><td>0.9868</td><td>0.9119</td><td>0.5110</td><td>0.5107</td><td>0.7983</td><td>0.9799</td><td>0.9134</td><td>0.5720</td><td>0.5687</td></tr><tr><td>2</td><td>0.6463</td><td>0.9699</td><td>0.8590</td><td>0.6235</td><td>0.5680</td><td>0.6498</td><td>0.9041</td><td>0.8498</td><td>0.6885</td><td>0.6442</td></tr><tr><td>3</td><td>0.5376</td><td>0.9497</td><td>0.8127</td><td>0.6740</td><td>0.6312</td><td>0.5017</td><td>0.8991</td><td>0.7920</td><td>0.7730</td><td>0.7628</td></tr><tr><td>4</td><td>0.4504</td><td>0.9026</td><td>0.7698</td><td>0.7190</td><td>0.6929</td><td>0.4311</td><td>0.8065</td><td>0.7592</td><td>0.7805</td><td>0.7649</td></tr><tr><td rowspan="4">LSCD</td><td>1</td><td>0.7887</td><td>0.9889</td><td>0.9187</td><td>0.5155</td><td>0.5073</td><td>0.8049</td><td>0.9900</td><td>0.9153</td><td>0.5490</td><td>0.5340</td></tr><tr><td>2</td><td>0.6749</td><td>0.9762</td><td>0.8707</td><td>0.6005</td><td>0.5400</td><td>0.6775</td><td>0.9488</td><td>0.8633</td><td>0.6570</td><td>0.6163</td></tr><tr><td>3</td><td>0.5748</td><td>0.9643</td><td>0.8307</td><td>0.6550</td><td>0.5988</td><td>0.5523</td><td>0.8732</td><td>0.8108</td><td>0.7300</td><td>0.6993</td></tr><tr><td>4</td><td>0.4765</td><td>0.9323</td><td>0.7904</td><td>0.7195</td><td>0.6846</td><td>0.4324</td><td>0.7925</td><td>0.7623</td><td>0.7780</td><td>0.7508</td></tr></table></body></html>

Table 3: Experimental results of the different parameters in lexical substitution unit.   

<html><body><table><tr><td rowspan="2">S T</td><td rowspan="2"></td><td colspan="6">k=3</td><td colspan="6">k=4</td></tr><tr><td>bpw↑</td><td>BLEU↑</td><td>MAUVE↑</td><td>SIM↑</td><td>Acc↓</td><td>F1↓</td><td>bpw↑</td><td>BLEU↑</td><td>MAUVE↑</td><td>SIM↑</td><td>Acc↓</td><td>F1↓</td></tr><tr><td>2</td><td>0.1</td><td>0.5561</td><td>0.3900</td><td>0.8719</td><td>0.7716</td><td>0.6875</td><td>0.6381</td><td>0.6305</td><td>0.3299</td><td>0.8585</td><td>0.7377</td><td>0.7205</td><td>0.6702</td></tr><tr><td></td><td>0.2</td><td>0.4543</td><td>0.4274</td><td>0.9024</td><td>0.7836</td><td>0.6895</td><td>0.6362</td><td>0.5338</td><td>0.3542</td><td>0.8807</td><td>0.7476</td><td>0.7175</td><td>0.6907</td></tr><tr><td></td><td>0.3</td><td>0.3975</td><td>0.4527</td><td>0.9143</td><td>0.7916</td><td>0.6865</td><td>0.6423</td><td>0.4808</td><td>0.3742</td><td>0.9003</td><td>0.7549</td><td>0.7180</td><td>0.6901</td></tr><tr><td></td><td>0.1</td><td>0.4277</td><td>0.4586</td><td>0.9250</td><td>0.7961</td><td>0.6735</td><td>0.6329</td><td>0.5135</td><td>0.3785</td><td>0.9069</td><td>0.7575</td><td>0.7184</td><td>0.6891</td></tr><tr><td>3</td><td>0.2</td><td>0.3702</td><td>0.4823</td><td>0.9101</td><td>0.8028</td><td>0.6685</td><td>0.6152</td><td>0.4555</td><td>0.3955</td><td>0.9024</td><td>0.7634</td><td>0.6910</td><td>0.6419</td></tr><tr><td></td><td>0.3</td><td>0.3394</td><td>0.4975</td><td>0.9278</td><td>0.8076</td><td>0.6625</td><td>0.5877</td><td>0.4219</td><td>0.4082</td><td>0.8938</td><td>0.7677</td><td>0.6905</td><td>0.6134</td></tr></table></body></html>

Influence of Lexical Substitution Unit Parameters As shown in Table 3, when $H = 4$ , we present the experimental results of the proposed MMLS under different parameters $( k , s$ and $\tau$ ). $k$ controls the embedding capacity corresponding to syntactic space, while $s$ and $\tau$ determine the embedding capacity corresponding to symbolic space. A larger $s$ means that the interval between masked positions is larger, in other words, fewer positions are masked in each text. A larger $\tau$ may lead to a smaller size of RCWP. For a fixed $s$ , text quality and anti-steganalysis ability increase gradually with an increasing $\tau$ , since more low-probability words are dropped in RCWP. When $\tau$ is fixed, a large $s$ results in higher text quality and stronger anti-steganalysis ability, but the embedding capacity will also decrease. It should be noted that the text quality and anti-steganalysis ability change smoothly with variations in the parameters $s$ and $\tau$ . It reveals that superimposing the substitution-based method on the syntactic transformation-baed method can further improve its embedding capacity at a relatively low cost of text quality and anti-steganalysis ability.

# Conclusion

In this paper, we proposed a novel multi-granularity MLS framework MMLS to hide secret information into syntactic and symbolic spaces by jointing syntactic and lexical manipulations. Moreover, our proposed distance-aware syntactic bins coding strategy can mitigate the interference inherent introduced by randomness and further improve the semantic coherence with the cover text. Experimental results demonstrate that MMLS significantly outperforms existing methods regarding semantic coherence, embedding capacity, and security. In the future, we plan to explore more syntax-based steganographic coding strategies to improve embedding capacity and security.