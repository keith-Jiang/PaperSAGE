# AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference

Zhuomin $\mathbf { H e } ^ { 1 * \dagger }$ , Yizhen $\mathbf { Y a 0 } ^ { 1 * \dagger }$ , Pengfei $\mathbf { Z } \mathbf { u } \mathbf { 0 } ^ { 2 * }$ , $\mathbf { B i n \ G a o } ^ { 3 \dagger }$ , Qinya $\mathbf { L i } ^ { \sharp }$ , Zhenzhe Zheng1, Fan Wu1

1Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University 2Huawei Cloud 3School of Computing, National University of Singapore   
{dean_hzm, 1975275148} $@$ sjtu.edu.cn, pengfei.zuo $@$ huawei.com, bingao $@$ comp.nus.edu.sg {qinyali, zhengzhenzhe} $@$ sjtu.edu.cn, fwu $@$ cs.sjtu.edu.cn

# Abstract

Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes AdaSkip, an adaptive sublayer skipping method specifically designed for longcontext inference. AdaSkip adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of AdaSkip is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.

# Introduction

Recently, large language models (LLMs) evolve to support long-context inference (Xiao et al. 2024; Srivatsa et al. 2024; DeepSeek-AI et al. 2024) up to 1M (Liu et al. 2024; AI et al. 2024), unlocking more complex real-world applications such as personal agent (Park et al. 2023; Wang et al. 2024), document summarization (Wu et al. 2023), and coding assistance (Liu, Xu, and McAuley 2023; Bairi et al. 2023; Jimenez et al. 2024). Long-context inference introduces more computational and storage demands. It is crucial to reduce the inference cost for long sequences.

Layer-wise skipping strategies, as an emerging technology, show great promise to reduce the LLM inference cost and latency by omitting the execution of transformer layers at specific positions, e.g., early skipping (Del Corro et al. 2023; Zhu et al. 2024), periodic skipping (Liu, Meng, and Zhou 2024), and early exit (Varshney et al. 2023; Fan et al. 2024; Chen et al. 2024).

However, we observe that these layer-wise skipping strategies all have their limitations in taking effect in long-context inference due to the following reasons. First, existing layerwise skipping strategies lead to a significant degradation in the generation quality due to predetermined fixed layers being skipped regardless of model and context variance. We observe that the importance distributions of transformer layers are different across models and contexts, and none of these strategies can perform consistently best across all models and contexts. Second, existing skipping strategies perform skipping at monolithic transformer layers which leads to suboptimal performance. We observe that the importance distributions of sublayers, i.e., attention and FFN modules, are independent. Moreover, in long-context inference, attention sublayers contribute significantly to inference latency (Tang et al. 2024; Jiang et al. 2024), highlighting the importance of prioritizing the skipping of more attention sublayers. Third, existing layer-wise skipping strategies are limited to the decoding phase, neglecting optimization of the prefilling phase in long-context inference, where the latency of the prefilling phase, i.e., time to first token (TTFT), imposes a significant burden on long-context inference latency.

To address the above limitations, we propose AdaSkip, an auto-adaptive, sublayer-wise skipping strategy tailored for long-context inference, which can benefit both the prefilling and decoding phases. Firstly, AdaSkip exploits on-the-fly similarity information during execution to adaptively identify the least important layers in different models, thereby improving the generation quality. Secondly, AdaSkip independently determines the importance distribution residing within sublayer modules like attention and FFN, enabling the sublayer-wise skipping. Finally, AdaSkip identifies the least important sublayers during both prefilling and decoding phases, significantly reducing the time and memory overhead of long-context scenarios.

In summary, our contributions are as follows:

1. We perform a comprehensive analysis of the importance distributions of various components including layer and sublayer modules across a range of different models. Based on the analysis, we present the limitations of the existing layer-wise skipping strategies in accelerating longcontext inference.

2. We propose an auto-adaptive, sublayer-wise skipping strategy that works for both the prefilling and decoding phases in long-context scenarios.   
3. We conduct extensive experiments on various longcontext benchmarks and models, demonstrating AdaSkip exhibits favorable inference performance over baselines.

# Background and Motivation

In this section, we first perform a comprehensive exploration of the importance metric of the layer and sublayer-wise modules, then present observations on the characteristics of the importance distribution and motivate our design principles.

# IO Similarity and Transformer Module Importance

We first define the metric, similarity, to evaluate the importance of transformer layers and sublayer modules. Given two $n$ -dimensional vectors, $\vec { a }$ and $\vec { b }$ , we characterize the cosine similarity between these vectors as their similarity, defined as follows:

$$
S i m i l a r i t y ( \vec { a } , \vec { b } ) = \frac { \vec { a } \cdot \vec { b } } { \left\| \vec { a } \right\| \left\| \vec { b } \right\| } = \frac { \sum _ { i = 1 } ^ { n } a _ { i } b _ { i } } { \sqrt { \sum _ { i = 1 } ^ { n } a _ { i } ^ { 2 } } \sqrt { \sum _ { i = 1 } ^ { n } b _ { i } ^ { 2 } } }
$$

Following the existing works (Liu et al. 2023b; Jaiswal et al. 2024; Fan et al. 2024), the similarity between the input and output (IO) vectors of the transformer module, i.e., IO similarity, can be used to evaluate the importance of a transformer module. Specifically, following the forwarding of each module, if the input vector of the module closely resembles the output vector, it indicates that the module contributes minimally to the forward propagation process. In other words, the current module contributes less importance in terms of execution. Conversely, the current module possesses higher importance in terms of execution if the IO similarity is low.

We further empirically validate the correlation between the IO similarity and the importance of a transformer module. Given an inference task, we conduct a first-round inference process to profile the IO similarity of each transformer layer. Subsequently, we execute a second-round inference process that selectively skips the layers based on varying degrees of the profiled IO similarity. Then we assess the quality of generated output by evaluating its GPT score (Varshney et al. 2023; Jaiswal et al. 2024). The LeastSkip strategy, which skips the layers exhibiting the lowest IO similarity, experiences a substantial degradation in the GPT score (dropping below 1.0 even with one skipped layer), compared to the MostSkip strategy, which skips the layers with the highest IO similarity and yields GPT scores of 8.9, 6.1, and 4.2 when skipping 1, 3, and 5 layers, respectively.

# Existing Layer-wise Skipping Strategies

Existing layer-wise skipping strategies propose skipping fixed layers with certain preferences to reduce inference execution time. As shown in Figure 1, according to the strategies to skip layers, existing layer-wise skipping strategies can be broadly categorized into three types: early skipping (Del Corro et al. 2023), periodic skipping (Liu, Meng, and Zhou 2024), and early exit (Schuster et al. 2022; Varshney et al. 2023; Fan et al. 2024; Bae et al. 2023). Early skipping (Del Corro et al. 2023) always skips the first few layers that are predetermined. Early skipping can support batching operations but may skip the important layers. Periodic skipping (Liu, Meng, and Zhou 2024) periodically skips a few middle layers. It follows a predetermined frequency to skip one layer every several layers. Periodic skipping supports batching operations but cannot capture the varying importance of different layers. Early exit (Varshney et al. 2023; Fan et al. 2024) always skips the last few layers. It evaluates whether the conditions (e.g., confidence level) are met after finishing the computation of each layer and the execution immediately exits upon condition fulfillment. Early exit may overlook the important layers that come later. Moreover, existing early exit strategies need to pay additional efforts and costs to either train classifier (Del Corro et al. 2023) or fine-tune the model to counterbalance the information loss resulting from imperfect layer skipping (Liu, Meng, and Zhou 2024; Varshney et al. 2023; Fan et al. 2024).

# Motivation

This subsection analyzes the limitations of existing LLM acceleration strategies for long-context inference.

Observation 1: The layer importance distribution exhibits significant variation across diverse models. We follow the same way used in the previous section to investigate the IO similarities of different layers on various models, in both prefilling and decoding phases. Figure 2 shows significant variation in the IO similarities of transformer layers for different models in three long-context datasets. Taking InternLM7B-8k and LLaMA3.1-8B-128k as examples, layers with high IO similarity in InternLM-7B-8k appear in the middle, such as layers 12,13,14, and the curve is more irregular. Whereas layers with high IO similarity in LLaMA3.1-8B-128k, appear towards the end, with layers 27, 25, 28, 29, and 26 being the top 5 layers, and the curve is approximately monotonically ascending. This suggests that layer importance distributions vary among different models. Existing layer-wise skipping strategies tend to consistently skip fixed layers, overlooking the differences in importance distribution across models, which restricts their adaptability to various models. Adaptive skipping strategies matching various models are required.

Observation 2: The importance distributions of attention and FFN modules are different. We study the IO similarities of the sublayer-wise modules, i.e., attention and FFN. As shown in Figure 3, the sublayer-wise modules show diverse IO similarity distributions. Taking LLaMA3.1-8B-128k as an example, in the last 11 layers, the average IO similarity of attention is consistently around 0.97, indicating a high IO similarity. However, the highest average IO similarity of FFN in the last 11 layers is only 0.95, and it is relatively scattered. Furthermore, compared to FFN, attention modules demonstrate higher and more concentrated similarity, implying that a greater number of attention modules can be skipped, with the potential to save more KV cache in longcontext inference. The different characteristics in IO similarity distributions of attention and FFN suggest that the existing layer-wise skipping methodologies that monolithically skip

A imp Model 1 A F A F A F A F A F A F A F A F A F A F A F A A F A ：   
ATTN Model 2 A F A F A F A F A F A F A F A F A F A F A F A F A F A F A F A F F   
FFN unimp Model 3 A F A F A F A A F A F A F A F A F F A A F S a) Early Skipping b) Periodic Skipping c) Early Exit d) Adaptive skip layer

![](images/92d5dd89409cfa8ae8a906f2fe6e264dbe53164afc4414395467f78bf04bcfac.jpg)  
Figure 1: The comparisons of different skipping strategies. The dashed box indicates the layer to be skipped.   
Figure 2: IO similarities of different layers in various transformer models.

entire transformer layers are sub-optimal. Consequently, the attention sublayer and FFN sublayer within one transformer layer should be considered separately.

Observation 3: The importance distribution of sublayers in the prefilling and decoding phases have similar trends but different fluctuation degrees. We further investigate the IO similarities of sublayer modules in the prefilling and decoding phases respectively. As shown in Figure 4, both attention and FFN sublayers display a consistent IO similarity trend between the prefilling and decoding phases, indicating that similar skipping strategies can be shared between the two phases. What’s more, we found a phenomenon that among all three models, each FFN sublayer has a higher IO similarity in the decoding phase than in the prefilling phase, which is different from that of attention sublayers. This suggests that we have the opportunity to skip more FFN sublayers in the decoding phase without affecting the model performance.

Challenges. Based on the above observations, an efficient skipping strategy for long-context inference should have the following capabilities: (1) adaptability to various models, (2) independent decision-making for sublayer-wise skipping, and (3) the ability to skip the most unimportant layers in both the prefilling and decoding phases.

However, implementing such a skipping strategy encounters several challenges. First, limited prior information is available to guide the skipping decisions throughout the prefilling phase. Second, distinguishing the unique information corresponding to specific models and contexts, required for making adaptive choices, is far from straightforward.

# Methodology

# Overview

To tackle the above challenges, we propose a novel skipping strategy for long-context inference, called AdaSkip, which adaptively selects sublayer-wise modules to skip considering the characteristics of models and inference context. Specifically, AdaSkip efficiently learns the importance distributions from the past inference execution to construct the skipping strategy for the prefilling phase. It further improves the skipping decision by online importance learning from on-the-fly intermediate data during the decoding phase. By integrating the above techniques, AdaSkip can accurately skip the least important sublayer-wise modules, avoiding the mismatch of layer importance and layer skipping decisions in fixed layer-wise skipping strategies.

# Sublayer Skipping during Prefilling with Offline Importance Learning

It is necessary to skip layers in prefilling phases during longcontext inference, since the prefilling phase results in unacceptably high TTFT and substantial KV cache demands. However, existing layer-skipping strategies rarely consider skipping strategies in such phases. Moreover, since different models exhibit various similarity distributions, current fixed layer-skipping strategies cannot achieve optimal results. The primary obstacle in devising an adaptive sublayer-wise skipping approach for the prefilling phase lies in the absence of prior knowledge before execution. To address this challenge, we propose an offline importance learning method that leverages the high correlation between historical prefilling features and new prefilling features.

Insight. Using sublayer-wise IO similarity feature from historical tasks can precisely predict the sublayer-wise skipping behavior for prefilling new inference tasks. We perform the IO similarity analysis study of running inference tasks of multiple datasets (Taori et al. 2023) including 2WikiMQA, MultiFieldQA-en, and TriviaQA using LLaMA3.1-8B-128k and quantify the average hit rate of unimportant layers in the prefilling phase. We record the average IO similarity on the Src dataset in prefilling phases and test the hit rate on

1.00 1.00 1.00 0.95 0.95   
0.90 0.95 0.8950   
0.80 0.85 ATTN, Trec 0.90 ATTN, Trec 0.80 ATTN, Trec ATTN, Triviaqa ATTN, Triviaqa 0.75 ATTN, Triviaqa FFN, Trec FFN, Trec FFN, Trec 0.7650 FFN, Triviaqa 0.85 FFN, Triviaqa 0.7605 FFN, Triviaqa 1 0.45 1 0.55 0.30 0.55 0 5 10 15 20 25 30 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer Layer Layer (a) Vicuna-7B-16k (b) InternLM-7B-8k (c) LLaMA3.1-8B-128k

![](images/5d7028e9ab679bd88722f50773a7f7ffdedcc977829f94fceed64cb52a0388aa.jpg)  
Figure 3: IO similarities of attention (ATTN) and FFN modules in different layers.   
Figure 4: IO similarities of sublayer modules in prefilling (P) and decoding (D) phases.

Table 1: Average hit rate of unimportant layers using historical features across datasets in prefilling phases.   

<html><body><table><tr><td>Type</td><td>Src</td><td>Dest</td><td>Layer Hit Rate</td></tr><tr><td>ATTN</td><td>TriviaQA</td><td>MFieldQA</td><td>3.76/4,4.86/6,9.31/10</td></tr><tr><td>ATTN</td><td>MFieldQA</td><td>Wiki</td><td>3.80/4,5.54/6,9.90/10</td></tr><tr><td>ATTN</td><td>TriviaQA</td><td>Wiki</td><td>3.79/4,5.50/6,9.68/10</td></tr><tr><td>FFN</td><td>TriviaQA</td><td>MFieldQA</td><td>3.66/4,5.69/6,9.56/10</td></tr><tr><td>FFN</td><td>MFieldQA</td><td>Wiki</td><td>3.77/4,5.97/6,9.38/10</td></tr><tr><td>FFN</td><td>TriviaQA</td><td>Wiki</td><td>3.75/4,5.96/6,9.64/10</td></tr></table></body></html>

the Dest dataset. The results shown in Table 1 reveal that historical IO similarity in prefilling phases gains a high hit rate for subsequent tasks, suggesting that this feature can be used in prediction and shared across different datasets.

Method. Based on the insight, the major workflow of offline importance learning consists of the similarity study and the corresponding deviation correction procedure. Specifically, suppose $N$ inference tasks (samples) are used in offline importance learning. As for the inference task $T _ { i }$ with prompt length $\left| T _ { i } \right|$ . Suppose that the model has $M$ transformer layers with $M$ attention sublayers and $M$ FFN sublayers. We first take notes of average similarity Simil¯arity in the prefilling phase. The average similarity of the $j _ { t h }$ sublayer, Simil¯arityj, can be accumulated as:

$$
S i m i l a r i t y _ { j } = \frac { \sum _ { i = 1 } ^ { N } \sum _ { t = 1 } ^ { | T _ { i } | } S i m i l a r i t y ( \vec { a } _ { i t } ^ { j } , \vec { b } _ { i t } ^ { j } ) } { \sum _ { i = 1 } ^ { N } | T _ { i } | }
$$

where $\vec { a } _ { i t } ^ { j }$ and $\vec { b } _ { i t } ^ { j }$ are the input and output vectors of the $t$ - th token in task $i$ . In addition, if the angle between vector $\vec { a } _ { i t } ^ { j }$ and $\vec { b } _ { i t } ^ { j }$ is not very large, the proportion of modulus of $\vec { a } _ { i t } ^ { j }$ and $\vec { b } _ { i t } ^ { j }$ relatively become prominent, suggesting some compensation needs to be applied.

However, due to the residual connections employed between each sublayer, the modulus of the input and output of one layer has minor variations, which implies that the average proportion of modulus can effectively compensate for the deviations. Hence, we use the average proportion of historical modulus of $\vec { a } _ { i t } ^ { j }$ and $\vec { b } _ { i t } ^ { j }$ in $j _ { t h }$ layer to scale $\vec { a } _ { i t } ^ { j }$ so that output vector $\hat { \vec { b } } _ { i t } ^ { \hat { j } }$ is close to original $\vec { b } _ { i t } ^ { j }$ . The average scale factor of $j$ -th sublayer, $S c \bar { a } l e _ { j }$ , can be formulated as:

$$
S c \bar { a } l e _ { j } = \frac { \sum _ { i = 1 } ^ { N } \sum _ { t = 1 } ^ { | T _ { i } | } \frac { | | \vec { b } _ { i t } ^ { j } | | } { | | \vec { a } _ { i t } ^ { j } | | } } { \sum _ { i = 1 } ^ { N } | T _ { i } | }
$$

we use $S c \bar { a } l e _ { j }$ to compensate the input $\vec { a } _ { i t } ^ { j }$ , getting approximate output:

$$
\hat { \vec { b } _ { i t } ^ { j } } = \boldsymbol { S } \boldsymbol { c } \bar { a } l \boldsymbol { e } _ { j } * \vec { a } _ { i t } ^ { j }
$$

After obtaining Simil¯arity and Sc¯ale of each sublayer module, we sort all sublayers in descending order based on their Simil¯arity, getting the sorted list sorted with $2 M$ elements. Since there is a trade-off between the number of skipped layers and the generation quality, we introduce an acceleration ratio, $\alpha$ , as a knob to control this trade-off. Given the acceleration ratio $\alpha$ , the number of sublayers to be skipped, $m$ , can be calculated as $\begin{array} { r } { m = M - \frac { M } { \alpha } } \end{array}$ , and the targeted skipping sublayer number is $2 m$ . The top $2 m$ sublayers in the sorted list are selected, forming the skipped set.

Table 2: Average hit rate of unimportant layers identified through different window sizes in the decoding phase.   

<html><body><table><tr><td>Dataset</td><td>Size</td><td>Layer Hit Rate</td></tr><tr><td>TREC</td><td>5</td><td rowspan="3">0.84/2,2.67/4,4.70/6 1.08/2,3.04/4,4.90/6 1.07/2,3.09/4,4.90/6</td></tr><tr><td>TREC</td><td>20</td></tr><tr><td>TREC GovReport</td><td>40 5 1.01/2,2.94/4,4.97/6</td></tr></table></body></html>

# Extra FFN Sublayer Skipping during Decoding with Online Importance Learning

Based on Observation 3, we find that regardless of attention or FFN sublayer, IO similarity of unimportant sublayers is always similar in prefilling and decoding phases, which suggests that we can reuse the layers selected from prefilling phases when decoding. What’s more, we observe that each FFN sublayer has higher IO similarity in decoding phases compared with the prefilling phase, which inspires us to explore more FFN skipping opportunities in decoding phases. In a nutshell, AdaSkip explores more potential of FFN skipping in decoding phases through online importance learning, hoping to obtain a larger speedup without losing performance.

Insight. The IO similarity information of the current context can be used to explore extra FFN skipping opportunities in decoding phases. We find that using the initial few tokens in the decoding phase can well hit the important layers of subsequent inference, and the hit rate gradually increases with the increase of the initial window. We test LLaMA3.1-8B$1 2 8 \mathrm { k \Omega }$ on TREC and GovReport datasets (Bai et al. 2023). For each context, we use the initial $n$ tokens in decoding phases to calculate the average IO similarity, and then observe the hit rate for subsequent decoding of the current sequence. The results are shown in Table 2. As the window size $n$ increases, the hit rate increases and gradually becomes constant, suggesting it is unnecessary to increase $n$ infinitely.

Method. Based on the above insight, the major workflow of online importance learning mainly consists of the similarity learned from the decoding phase of the new inference task. Specifically, for the new context, we define the first $P$ decoded tokens as online learning windows. These tokens are processed with unskipped layers in order to obtain current decoding features. We denote the set of FFN sublayers to be skipped in decoding phases by skippedP . For $j$ -th sublayer, given the input and output vectors of $t$ -th decoded token as $\vec { a } _ { t } ^ { j }$ and $\vec { b } _ { t } ^ { j }$ , Simil¯arityj of $j _ { t h }$ FFN sublayer for current context from the first decoded token to $P _ { { t h } }$ tokens for FFN sublayers can be formulated as:

$$
\large S i m i \overline { { { l } } } a r i t y _ { j } ^ { P } = \frac { \sum _ { t = 1 } ^ { P } S i m i l a r i t y ( \vec { a } _ { t } ^ { j } , \vec { b } _ { t } ^ { j } ) } { P }
$$

We get all indexes of FFN sublayers, i.e. index, and the indexes of all the layers skipped in the prefilling phase, i.e., skipped. To find out which layers in index set need to be skipped, we derive a threshold $\beta$ by observing the skipped set and then use this threshold to filter the additional skipped FFN sublayers. The threshold $\underline { { \beta } }$ is the least Similarity value in skipped, i.e. $\beta = \operatorname* { m i n } \{ S i m i \bar { l a } r i t y _ { j } \mid j \in s k i p p e d \}$ .

We then traverse index to find the sublayers whose Simil¯arityj is above $\beta$ , and these sublayers are the additional ones to be skipped in the new context. By combining the indexes of these sublayers with the indexes of the skipped set, we obtain the adaptive sublayer-wise skipping set, denoted as skippedP . At last, similar to the last section, we use $S c \bar { a } l e _ { j }$ to compensate for the potential deviation.

# Experiments

In this section, we thoroughly evaluate the performance of AdaSkip in long-context inference. We first show the experiment settings including the benchmarks, baselines, and setups. Then we show the experiment results and analysis.

# Experiment Settings

Benchmarks We select benchmarks based on representative long-context application scenarios (Bai et al. 2024), encompassing document QA, few-shot learning, and summarization. To better evaluate different skipping strategies in prefilling and decoding phases, we divide the benchmarks into prefilling tasks and decoding tasks by average output length. We select MultiFieldQA (Bai et al. 2023), TriviaQA (Joshi et al. 2017), and TREC (Li and Roth 2002) as prefilling tasks, with average input lengths of 6493, 8677, and 8208, and output lengths capped at 32. For decoding tasks, we choose GovReport (Huang et al. 2021) and MultiNews (Fabbri et al. 2019), with average input lengths of 9214 and 8265, and output lengths limited to 512. We evaluate the end-to-end performance of all layer-wise skipping strategies by skipping layers in both prefilling and decoding phases.

Baselines and Setups. Three layer-wise skipping strategies are considered as baselines: (1) SkipDecode (Del Corro et al. 2023) skips the initial layers except for the first one, representing early skipping; (2) Unified Skipping (Liu, Meng, and Zhou 2024) uniformly skips the intermediate layers except for the first and last layers, representing periodic skipping; and (3) Early Exit (Varshney et al. 2023; Fan et al. 2024) skips the last few layers. Note that layer-wise skipping skips two sublayers, i.e., attention and FFN, in a single layer-skip operation. Specifically, as these baselines were originally designed to skip layers only during the decoding phase, we limit layer skipping to the decoding phase in decoding tasks to ensure a fair comparison. Three of the latest and widely adopted long-context LLMs are tested: LLaMA3.1-8B-128k, InternLM-7B-8k, and Vicuna-v1.5-7B-16k. A single L20 GPU with CUDA version 12.1 is used as the testbed.

# Results of Prefilling Tasks

The middle of Table 3 presents the results of the prefilling tasks. Given the same number of target skip sublayers,

Table 3: Evaluation of different skipping strategies.   

<html><body><table><tr><td rowspan="2">spseup</td><td rowspan="2"></td><td rowspan="2">Model</td><td rowspan="2">Skipping</td><td colspan="3">DoQAFeamna</td><td rowspan="2"></td><td colspan="3">Texcsmris</td></tr><tr><td>MFieldQA</td><td>TriviaQA</td><td>TREC SU</td><td></td><td>GovReport|MultiNews</td><td>SU</td></tr><tr><td rowspan="2">0</td><td rowspan="2">1.00</td><td>LLaMA3.1-8B-128k</td><td></td><td>(F1)</td><td>(F1)</td><td>(ACC)</td><td></td><td>(Rouge-L)</td><td>(Rouge-L)</td><td></td></tr><tr><td>InternLM-7B-8k</td><td>Full Model Full Model</td><td>29.7 26.6</td><td>91.6 70.4</td><td>75.0 50.4</td><td>1.00 1.00</td><td>34.2 18.2</td><td>25.8 17.6</td><td>1.00 1.00</td></tr><tr><td rowspan="6"></td><td rowspan="6">1.14</td><td>Vicuna-v1.5-7B-16k</td><td>Full Model</td><td>32.9</td><td>87.8</td><td>68.9</td><td>1.00</td><td>27.2</td><td>22.4</td><td>1.00</td></tr><tr><td></td><td>Early Exit</td><td>13.1</td><td>18.5</td><td>28.3</td><td>1.10</td><td>16.8</td><td>14.5</td><td>1.11</td></tr><tr><td>LLaMA3.1-8B-128k</td><td>SkipDecode</td><td>34</td><td></td><td>0.0</td><td>1.10</td><td>19.3</td><td>16.3</td><td>1.07</td></tr><tr><td></td><td>Unifed Skipping</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AdaSkip</td><td></td><td>23.4</td><td>86.6</td><td>72.8</td><td>1.09</td><td>30.9</td><td>24.0</td><td>1.15</td></tr><tr><td rowspan="4">InternLM-7B-8k</td><td></td><td>Early Exit</td><td>6.1 0.0</td><td>28.2</td><td>32.8</td><td>1.13 1.13</td><td>3.1 11.0</td><td>3.6 10.6</td><td>1.12</td></tr><tr><td rowspan="3"></td><td>SkipDecode</td><td>15.4</td><td>0.0 21.1</td><td>0.0 13.3</td><td>1.13</td><td>9.8</td><td></td><td></td><td>1.08</td></tr><tr><td>Unifed Skipping</td><td>23.9</td><td>60.3</td><td>42.7</td><td></td><td>1.25</td><td>13.7</td><td>10.1 13.3</td><td>1.12</td></tr><tr><td>AdaSkip Early Exit</td><td>18.5</td><td>73.7</td><td>29.4</td><td>1.11</td><td></td><td>12.2</td><td>13.3</td><td>1.24 1.13</td></tr><tr><td rowspan="5"></td><td rowspan="4">Vicuna-v1.5-7B-16k</td><td></td><td>00</td><td></td><td></td><td>00</td><td></td><td></td><td></td><td></td></tr><tr><td>UnitedDeiping</td><td></td><td>00</td><td></td><td></td><td>1.19</td><td>4.1</td><td></td><td>1.07</td></tr><tr><td>AdaSkip</td><td>29.6</td><td>82.4</td><td></td><td>66.1</td><td>1.15</td><td>23.6</td><td>20.2</td><td>1.20</td></tr><tr><td></td><td>11.4</td><td></td><td></td><td></td><td>1.23</td><td>4.9</td><td>5.3</td><td>1.26</td></tr><tr><td rowspan="8">16 1.33</td><td rowspan="4">LLaMA3.1-8B-128k UnifedSkipping</td><td>Early Exit</td><td>0.0</td><td>4.5 0.1</td><td>7.8 0.0</td><td></td><td>1.23</td><td>15.2</td><td>13.8</td><td>1.15</td></tr><tr><td>SkipDecode</td><td>0.6</td><td>1.0</td><td>0.0</td><td>1.22</td><td></td><td>12.0</td><td>8.7</td><td>1.26</td></tr><tr><td>AdaSkip</td><td>18.0</td><td>62.3</td><td>72.2</td><td></td><td>1.22</td><td>17.5</td><td>19.1</td><td>1.32</td></tr><tr><td>Early Exit</td><td></td><td>0.7</td><td>0.5</td><td>6.1</td><td>1.31</td><td>0.6</td><td>0.4</td><td>1.28</td></tr><tr><td rowspan="4">InternLM-7B-8k</td><td></td><td></td><td>04</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>UnifedDeiping</td><td>0.1</td><td></td><td>0.0</td><td></td><td>1.31</td><td>9.2</td><td>9.4</td><td>1.16</td></tr><tr><td>AdaSkip</td><td>17.2</td><td>38.7</td><td></td><td>29.4</td><td>1.51</td><td>9.4</td><td>9.8</td><td>1.47</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>1.25</td><td></td><td></td><td></td></tr><tr><td rowspan="4"></td><td rowspan="4">Vicuna-v1.5-7B-16k</td><td>Early Exit</td><td>9.6</td><td>41.4</td><td>15.0</td><td></td><td></td><td>3.0</td><td>3.6</td><td>1.28</td></tr><tr><td>UnifedDeiping</td><td>0.0</td><td>00</td><td>00</td><td></td><td>1.25</td><td>4</td><td>3.8</td><td>1.16</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>1.31</td><td>13.7</td><td>14.7</td><td></td></tr><tr><td>AdaSkip</td><td>10.6</td><td>39.0</td><td>43.9</td><td></td><td></td><td></td><td></td><td>1.40</td></tr></table></body></html>

AdaSkip significantly outperforms the other baselines in both Doc QA and Few-shot Learning tasks. For example, on the LLaMA3.1-8B-128k model, with a target skip sublayer number of 8, AdaSkip achieves a classification accuracy of $7 2 . 8 \%$ on TREC and an F1 score of 86.6 on TriviaQA, closely approximating the performance of the full model. Even with up to 16 skipped sublayers, AdaSkip’s accuracy on TREC remains at $7 2 . 2 \%$ . In contrast, the accuracy of the SkipDecode and Unified Skipping approaches decrease by more than $90 \%$ when skipping only 8 sublayers (4 whole layers).

In terms of speedup, the computational complexity of attention scales quadratically with sequence length, making attention computations more demanding than those of FFN in long-context scenarios. Due to AdaSkip skipping more attention sublayers, it achieves over a $10 \%$ speedup advantage on InternLM compared to the baseline. For the LLaMA model, the attention sequence parallelism and other optimization techniques are relatively mature, making the FFN execution time longer during the prefilling phase. As a result, our approach is slightly outperformed by the baseline.

# Results of Decoding Tasks

The right half of Table 3 presents the results of decoding tasks. Despite the baselines being specifically tailored for decoding tasks, our method consistently demonstrates superior performance. Even with the number of skipped sublayers reaching 16, we still maintain comparable performance. For instance, the LLaMA model achieves Rouge-L scores exceeding 17.5 on both datasets, comparable to the full InternLM model. It is noteworthy that the Early Exit method, which performs reasonably well in the prefilling tasks, fails to maintain generation quality during decoding. Its Vicuna Rouge-L scores for the two summarization tasks fall below 4.0, possibly due to the accumulation of errors in the autoregressive process. In contrast, AdaSkip accurately identifies the least significant sublayers, allowing LLM to maintain optimal performance even with additional skipping of FFNs.

In terms of execution speed, the inference time during the decoding phase is primarily dictated by HBM access. In long-context inference, attention operates slower than FFN due to the extensive KV cache access required. Our approach achieves a higher acceleration ratio by skipping more attention layers at the outset, thanks to the higher attention similarity obtained during the offline learning phase. After online learning, we further enhance the acceleration by selectively skipping additional FFN layers. Overall, our method delivers up to a $17 \%$ acceleration improvement compared to the baseline. The Skip Decode approach achieves the lowest speedup because it employs a progressive layer skipping strategy, where the number of skipped layers gradually increases with the decoding steps until reaching the preset number.

Table 4: Evaluation on End-to-End Skipping Strategies.   

<html><body><table><tr><td rowspan="2"># Target Sublayer Skip</td><td rowspan="2">Skipping Strategy</td><td colspan="2">LLaMA-3.1B-128k</td><td colspan="2">InternLM-7B-8k</td><td colspan="2">Vicuna-v1.5-7B-16k</td></tr><tr><td>GovReport MultiNews (Rouge-L)</td><td>(Rouge-L)</td><td>GovReport MultiNews (Rouge-L)</td><td>(Rouge-L)</td><td>GovReport MultiNews (Rouge-L)</td><td>(Rouge-L)</td></tr><tr><td>0</td><td>Full Model</td><td>34.2</td><td>25.8</td><td>18.2</td><td>17.6</td><td>27.2</td><td>22.4</td></tr><tr><td rowspan="4">8</td><td>Early Exit</td><td>15.3</td><td>12.4</td><td>2.7</td><td>3.4</td><td>12.2</td><td>13.3</td></tr><tr><td>SkipDecode</td><td>1.0</td><td>1.1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>Unifed Skipping</td><td>1.6</td><td>1.1</td><td>8.9</td><td>9.9</td><td>0.0</td><td>0.0</td></tr><tr><td>AdaSkip</td><td>30.5</td><td>24.1</td><td>12.9</td><td>12.7</td><td>23.0</td><td>21.1</td></tr><tr><td rowspan="4">16</td><td>Early Exit</td><td>4.3</td><td>4.4</td><td>0.5</td><td>0.3</td><td>1.9</td><td>1.8</td></tr><tr><td>SkipDecode</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>Unifed Skipping</td><td>0.0</td><td>0.1</td><td>0.4</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><td>AdaSkip</td><td>18.9</td><td>17.8</td><td>7.7</td><td>7.1</td><td>13.6</td><td>14.1</td></tr></table></body></html>

# Results of End-to-End Testing

We evaluate the end-to-end performance of various layer skipping strategies, namely, implementing simultaneous skipping during both the prefilling and decoding phases. As demonstrated by Table 4, the performance of existing approaches significantly degrades when layer skipping is applied in both phases, compared to applying it solely during decoding. The SkipDecode approach causes Rouge-L scores to plummet to nearly zero across all three models. Similarly, Unified Skipping, which previously exhibited a modest difference from our approach on specific data points in decoding tasks, sees all its Rouge-L scores drop below 10.0 in this scenario. Additionally, when skipping 16 sublayers, the Early Exit approach yields scores below 5.0 across all models.

The results highlight the significant limitations of existing methods, which are unable to effectively apply layer skipping during both the prefilling and decoding phases in tasks with longer generation lengths. In contrast, our approach maintains nearly identical performance as when layer skipping is applied only during the decoding phase, demonstrating that our skipping strategy effectively adapts to both the prefilling and decoding phases. In real-world long-context tasks, our method exhibits exceptional practical value due to its ability to employ a complete layer-skipping strategy. It can markedly optimize the TTFT introduced during the prefilling phase and reduce the storage costs of the KV cache for long prompts.

# Related Work

Long-context Model. With the growing demand for longcontext models, numerous studies have concentrated on expanding the context window of LLMs. Many models have fine-tuned LLaMA-2 by scaling Rotary Position Embeddings (RoPE) (Su et al. 2023), expanding its input window to $3 2 \mathrm { k }$ , as seen in LongChat (Li et al. 2023), and to $1 2 8 \mathrm { k \Omega }$ , as demonstrated in Yarn-LLaMA-2 (Peng et al. 2023). By leveraging length extrapolation, the context windows can extend beyond 1 million tokens (Liu et al. 2023a). However, these approaches do not alleviate the substantial inference costs associated with long-context processing.

Long-context LLM Inference Optimization. Given the substantial increase in KV cache size introduced by long sequences, many studies have concentrated their inference optimization efforts on compressing, evicting, and reusing KV cache. Heavy Hitter Oracle (H2O) (Zhang et al. 2024b) retains a limited budget of the important KV cache based on the sum of historical attention scores. SnapKV (Li et al. 2024) reduces memory access during decoding by observing the attention distribution of the prompt’s tail over the prefix to selectively filter the corresponding KV cache, thereby achieving acceleration. PyramidKV (Zhang et al. 2024a) optimizes KV cache storage more flexibly by allocating different KV cache budgets to various layers and attention heads based on the observed information flow aggregation patterns. However, these approaches fail to address the substantial computational burden associated with generating extensive KV cache during the long sequence prefilling stage.

# Conclusion

In conclusion, this paper focuses on exploring the layer-wise skipping strategy in long-context inference. It first discusses the typical challenges in long-context inference and presents a detailed examination of the importance distribution of various components including layer and sublayer modules such as attention and FFN across a variety of different models. The analysis underlines the limitations of the current layer-wise skipping strategies in long-context inference. In response to these limitations, this paper proposes a novel, auto-adaptive, sublayer-wise skipping strategy that requires no training and is applicable to both the prefilling and decoding phases. Through rigorous testing across a diverse array of long-context datasets and models, we have demonstrated that our system, AdaSkip, significantly outperforms the baseline in both generation quality and inference speed.