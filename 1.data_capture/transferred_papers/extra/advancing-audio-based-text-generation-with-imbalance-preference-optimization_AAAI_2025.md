# Advancing Audio-Based Text Generation with Imbalanced Preference Optimization

Zhenghao Zhou1, Yongjie ${ { \bf { L i u } } ^ { 1 } }$ , Chen $\mathbf { C a o } ^ { 2 * }$

1 National Supercomputing Center in Wuxi, China 2 University of Sheffield, United Kingdom zhenghaozhou1 $8 3 \textcircled { a }$ gmail.com, ccao5 $@$ Sheffield.ac.uk

# Abstract

Human feedback in generative systems is a highly active frontier of research that aims to improve the quality of generated content and align it with subjective preferences. Existing efforts predominantly focus on text-only large language models (LLMs) or text-based image generation, while crossmodal generation between audio and text remains largely unexplored. Moreover, there is currently no open-source preference dataset to support the deployment of alignment algorithms in this domain. In this work, we take audio speech translation (AST) and audio captioning (AAC) tasks as examples to explore how to enhance the performance of mainstream audio-based text generation models with limited human feedback. Specifically, we propose an novel framework named imbalanced preference optimization (IPO) that includes a model adversarial sampling concept–human annotators act as referees to determine model outcomes, using these results as pseudo-labels for the corresponding beam search hypotheses. Given these imbalanced win-loss results, IPO effectively enable the two models to update interactively to win the next round of adversarial sampling. We conduct both subjective and objective evaluations to demonstrate the alignment benefits of IPO and its enhancement on model perception and generation capacities. On both AAC and AST, a few hundreds of annotations significantly enhance the weak model, and the strong model can also be encouraged to achieve new state-of-the-art results in terms of objective metrics. Additionally, we show the extensibility of IPO by applying it to the reverse task of text-to-speech generation, improving system robustness on unseen reference speaker.

# 1 Introduction

In the wave of AI-Generated Content (AIGC), learning from human feedback plays a crucial role that aims to align the generated content of AI systems with human preference (MacGlashan et al. 2017; Stiennon et al. 2020; Dubois et al. 2024). For example, large language models (LLMs) undergo calibration through Reinforcement Learning from Human Feedback (RLHF) to become helpful and powerful assistant systems like ChatGPT (Bai et al. 2022; Achiam et al. 2023). Additionally, recent efforts have demonstrated the efficacy of RLHF in high-dimensional data generation tasks, e.g., music generation (Cideron et al. 2024) and textto-images synthesis (Wu et al. 2023; Liang et al. 2024).

![](images/0826911519b9589578b44e1feecbe9df0ad29fbba33cf11184ac0cab2aa2ca0d.jpg)  
Figure 1: Left: text-based preference annotation, where language model generates desired/undesired samples annotated by human. Right: audio-text adversarial annotations. Due to lack of diversity, annotator serves as referee to determine the win-loss results of two models based on the same audio input. Additionally, Beam search algorithm is utilized for extending data with pseudo-preference.

In addition to preference alignment, RLHF-based methods have also been reported in existing works to optimize model performance evaluated by objective evaluation metric (Liu et al. 2020; Steenhoek et al. 2023), particularly in autoregressive sequence prediction (Chen et al. 2024a). A underlying reason is that the sampling process in RLHF can effectively mitigate the exposure bias problem caused by the mismatch between teacher-forcing training and autoregressive inference (Rennie et al. 2017). Meanwhile, these self-generated samples, after annotated by humans, can provide high-level supervised information to the neural model through gradient descent. This is certainly beneficial for tasks where the ground truth is not unique, such as machine translation (Xu et al. 2024; He et al. 2024) and image captioning (Xu et al. 2023; Verma et al. 2023).

Unlike aforementioned text-only and vision-based generation tasks, which have widely optimized by RLHF, we find that there is a considerable research gap in audio-based generation tasks when predicting the natural language consuming a segment of audio signal or human speech. Specifically, in relative speech translation and audio captioning tasks, few prior work incorporates subjective feedback into the learning process of neural networks (Chu et al. 2024). Motivated by this, we focus our research on addressing this gap and are the first to investigate the following questions: Can RLHF align audio-text cross-modal generation systems with human preferences, or can it improve model performance by objective metrics?

In this context, the primary challenge is the lack of labeled preference data. Compared to image or text data annotation, annotating audio is more labor-intensive (Gibbons et al. 2023): participants can straightforward perceive images or text, but for audio, they need to listen to the entire playback, even listening several times to prevent forgetting. Consequently, the limited accessible data imposes higher demands on data efficiency for audio-text alignment. Furthermore, the audio modality exhibits complex and variable characteristics across domains, necessitating a model that can not only generate natural language but also possess robust audio pattern recognition capabilities. For example, if the speech translation model misrecognizes homophones of input, it can explicitly affect the quality of predicted text.

In this work, we propose a IPO, an preference optimization approach tailored to audio-text modality, achieving cross-modal alignment with limited subjective human feedback. In general, IPO is novel for two aspects: (i) To maximize the utility of human-annotated data, we introduce a model adversarial sampling: given an audio input, the annotators sever as a referee to determine the superiority of the outputs generated by two models. This judgment of superiority not only applies to the given input-output pair, but also extends to other hypotheses sampled by these two models through beam search decoding. Notably, we conduct empirical validation to confirm the rationality of this operation, which expand the self-generated negative samples to $n$ times with a beam size of $n$ . (ii) Considering that the two models may differ in strength, the subsequent optimization method is designed to effectively handle the data imbalance between positive and negative samples. Specifically, IPO does not depend on pairwise preference data, such as that used in DPO (Rafailov et al. 2024). Instead, it simply requires results of which model wins or loses in the adversarial sampling for the given data, then two models are interactively optimized to win the next round of confrontation.

To demonstrate the efficacy of IPO, we conduct intensive experiments based on prevalent audio-language foundation models with limited human annotations. Both objective and subjective evaluation reveal that IPO not only aligns with human feedback but also effectively enhances these model performance in terms of various objective metrics. (i) In AST task, only 100 data annotations can significantly enhance the weak model (Whisper-large-v2 (Radford et al. 2023)), more importantly, the performance of a stronger model (Seamlessv2 (Barrault et al. 2023)) can also be improved to achieve a new state-of-the-art according to the BLEU score. (ii) In AAC task, similar performance gain is observed. Moreover, we found that IPO can enhance the model’s audio perception capabilities–some audio events that were previously unrecognized are now detected and included in the output descriptions. (iii) We also demonstrate the scalability of IPO by extending IPO to speech synthesis task to enhance the zero-shot capacity of mainstream text-to-speech (TTS) systems.

In general, our contributions are summarized as follows:

• We direct our focus on the alignment issue in audio-based text generation. In this context, human feedback is efficiently introduced into the learning loop of prevalent large audio models to further improve their performance. • An RLHF-based iterative method, IPO, has been proposed to tackle the preference data scarcity. It introduces a model adversarial sampling and utilizes human annotators as referees, extending this win-loss result at a given data point to other self-generated responses through beam search. Subsequently, the IPO accordingly optimizes two models based on the unpaired and imbalanced preference data. • Both subjective and objective evaluation demonstrate that IPO can effectively benefit to AST and AAC tasks, in terms of human preference and objective metric. Furthermore, we adapt IPO to the inverse text-based speech generation (i.e., TTS) task, improving its zero-shot robustness on cloning unseen speaker.

# 2 Related Work

# 2.1 Audio-Text Generation Models

With the advent of deep neural networks, the increasing availability of data has led to a surge in research utilizing both audio and text modalities for training neural networks (Rubenstein et al. 2023; Li, Tang, and Liu 2024), benefiting a multitude of cross-modal downstream tasks (Wang et al. 2024). In the realm of speech processing, extensively explored tasks include automatic speech recognition (Yang et al. 2024), automatic speech translation (Chen et al. 2024b), speech question-answering (Zhao et al. 2024), and spoken dialogue systems (Mousavi et al. 2024). From audio to speech, tasks such as audio captioning (Deshmukh et al. 2023) and audio question-answering (Chuang et al. 2019) have also seen significant advancements. More recently, all-in-one large models capable of handling both speech and audio have emerged, successfully executing text generation tasks based on varying instructions (Chu et al. 2023; Hu et al. 2024; Sun et al. 2024; Fathullah et al. 2024). Moreover, the text-based audio generation task has also witnessed remarkable progress in recent years. Text-tospeech models now produce high-fidelity human-like speech and demonstrate the ability to generalize to unseen speakers in zero-shot scenarios (Kaur and Singh 2023; Wang et al. 2023; Peng et al. 2024). Additionally, text-to-audio (Huang et al. 2023) and text-to-music (Schneider et al. 2023; Kim, Jang, and Shin 2022) tasks have exhibited rich and highquality synthesis outcomes.

# 2.2 Preference Optimization

The alignment of generated content with human preferences has been recognized as crucial since the early days of machine translation tasks. Following the recent successes of large language models, reinforcement learning from human feedback (RLHF) has rapidly evolved to ensure that generated content is useful, non-toxic, and aligned with mainstream values.

Early RLHF approaches like PPO rely on an independent reward models trained by human-annotated data. However, the hacking arising from the reward model’s inability to cover all scenarios. The methods directly optimizing with preference data becomes popular, such as DPO and its derivative works. Additionally, some methods have employed self-reward mechanisms, where the LLM itself is utilized to assign rewards, or LLM and the reward model update alternatively via a min-max game (Cheng et al. 2023). From a task perspective, RLHF has expanded beyond textual generation to include the generation of high-dimensional data such as images (Kirstain et al. 2023; Liang et al. 2024), videos (Zhou et al. 2024), audio (Majumder et al. 2024), music (Cideron et al. 2024), and speech (Chen et al. 2024a).

# 3 Methodology

Given a high-sample-rate audio input $\boldsymbol { x } \in \mathbb { R } ^ { T }$ , the audio-totext task aims to recognize and convert it into natural language. For instance:

• Speech Translation: Converting audio input in source language to text in a target language, requiring the model to understand and accurately translate linguistic nuances.   
• Automatic Audio Captioning: Generating descriptive text that captures the sounds and scenes present in the audio, necessitating a comprehensive understanding of the audio context.   
• Text-to-Speech Synthesis: Thanks to the neural codec modeling, speech signal can be converted to discrete audio codec, thus generating a sequence based on the target transcription and a segment of prompt speech, a.n.a, zero-shot TTS.

Typically, a neural network learns an end-to-end mapping from audio to text based on a labeled audio-text pair dataset. This is achieved through the following training objective:

$$
\mathcal { L } _ { \mathrm { a t t } } ( x , y ) = - \sum _ { l = 1 } ^ { L } \log P _ { \theta } ( y _ { l } \mid y _ { 1 : l - 1 } , x )
$$

where $x$ is the audio input, $y$ is the corresponding discrete text token sequence, $y _ { 1 : l - 1 }$ are the tokens generated up to position $l - 1$ , and $\theta$ represents the trainable parameters of the neural model. In the remainder of this chapter, we first introduce the sampling method incorporating the adversarial model design, followed by the corresponding optimization strategy for model $\theta$ .

# 3.1 Model Adversarial Sampling

Given an audio input $x$ with unknown ground truth, two models $\theta _ { 1 }$ and $\theta _ { 2 }$ generate their respective text outputs, $y _ { 1 }$ and $y _ { 2 }$ , based on $x$ . An annotator evaluates $y _ { 1 }$ and $y _ { 2 }$ according to their preferences. If $y _ { 1 }$ is preferred, the pair $( x , y _ { 1 } )$ is added to the set $\mathcal { D } _ { 1 } ^ { \mathtt { p } }$ , while $( x , y _ { 2 } )$ is added to ${ \mathcal { D } } _ { 2 } ^ { \mathrm { n } }$ . Conversely, if $y _ { 2 }$ is preferred, $( x , y _ { 2 } )$ is added to $\mathcal { D } _ { 2 } ^ { \mathtt { p } }$ and $( x , y _ { 1 } )$ to $\mathcal { D } _ { 1 } ^ { \mathrm { n } }$ . Thus, four sets are created to record the results of the evaluation. Additionally, if both $y _ { 1 }$ and $y _ { 2 }$ are deemed equally good or bad, they are added to their respective positive and negative sample sets.

Table 1: Beam Search BLEU statistics by SeamlessM4T-V2 on $\mathrm { D e } {  } \mathrm { E n }$ translation across different performance level. Each level contains 50 examples and the beam size is 5.   

<html><body><table><tr><td>Level</td><td>BLEU Mean</td><td>BLEU Variance</td></tr><tr><td>1</td><td>40.7</td><td>1.10</td></tr><tr><td>2</td><td>36.6</td><td>0.99</td></tr><tr><td>3</td><td>30.7</td><td>0.83</td></tr><tr><td>4</td><td>28.4</td><td>1.21</td></tr></table></body></html>

Based on this strategy, $m$ times of human choices can annotate $2 \times m$ data points, which is insufficient for optimizing large models. To encourage more self-generated samples, we use a beam search sampling strategy to expand the preference data. Specifically, the beam search sampling formula is as utilized to generate a set of hypotheses: $\mathbf { \bar { \{ } }  y ^ { i } \mathbf  \bar { \} } _ { i = 1 } ^ { n } = \mathrm { B e a m S e a r c h } ( x , \theta , \bar { n } )$ , where n is the beam size. The preference label for $y ^ { 1 }$ will be applied to other candidates $\{ y ^ { i } \} _ { i = 2 } ^ { n }$ in the beam set and stored in the same preference set $\mathcal { D }$ , resulting in an $n$ -fold expansion. Although these pseudo-labels inevitably introduce some errors, it is noteworthy that in the beam set generated by audio-to-text models, the candidates do not exhibit the same diversity as those from LLMs. Based on the perceptual module’s understanding of the audio input, the generated hypotheses in one beam set exhibit similar quality in terms of metrics.

Empirical validation. We utilize the training set of (Conneau et al. 2023) to show the BLEU diversity in $\{ y ^ { i } \} _ { i = 1 } ^ { n }$ with beam set of 5. As shown in Table 1, each level samples 50 data points according to their average BLEU $( \pm 1 )$ , e.g., level 1 consists of 50 set of results with the average BLEU from 40 to 41. Therefore, the variance are calculated on 50 $\times ~ 5$ utterances for each level. We observe that regardless of SeamlessM4T performance, the variance for each level keeps low, thus indicating the stable BLEU performance in the same beam set. Additionally, similar validations are conducted on other languages to demonstrate that the distribution of BLEU score clusters around the mean value, allowing us to use beam search to provide positive and negative samples with pseudo-preference label.

With proposed model adversarial sampling, we obtain four subsets that totally consists of $m \times n$ of $( x , y )$ pairs. Taking $\mathcal { D } _ { 1 } ^ { p }$ as example, all element are generated by model $\theta _ { 1 }$ and preferred by annotator, while real label and pseudo label by beam search are recorded to distinguish the data quality. In subsequent optimization, the real label data is assigned higher weight for model update.

Can LLMs help for AST annotation? Given the demonstrated linguistic capabilities of GPT-4, we propose an alternative strategy for preference data annotation in AST. However, since the GPT model lacks audio perception capabilities, this strategy relies on ground-truth source text and can only be applied to public AST datasets. Specifically, in the instruction prompt, we provide the transcription of source

Adversarial D win:2 Sample:6 Sampling 00 OOOOOO $D _ { 2 } ^ { p }$ win:8 Model unpaired lose:8 real Optimization 000000OO sim O>●o● 0 by 2

# 3.2 Optimization with Imbalanced Data

Given preference dataset $\mathcal { D }$ , typical RLHF optimization relies on a reward model $r _ { \phi }$ learns to serve as a proxy, done by minimizing the negative log-likelihood of the human preference data:

the limited diversity in a beam set, we eliminate the dependence on pairwise win-lose samples by utilizing a reference point $z _ { \mathrm { r e f } }$ form recent work KTO (Ethayarajh et al. 2024), and directly maximize the length-normalized implicit reward $r _ { \mathrm { I P O } } ( x , y )$ with a value function $v$ . Taking model $\theta _ { 1 }$ as example:

where $y _ { w }$ and $y _ { l }$ denote the preferred and dispreferred data respectively. To avoid excessive pursuit of rewards, a KL divergence penalty is introduced to restrict the language model’s deviation from $\pi _ { \mathrm { r e f } }$ . Here, $\pi _ { \boldsymbol { \theta } }$ represents the model we are optimizing, and the optimal model $\pi ^ { * }$ is defined as the one that maximizes:

transcription as a reference and ask LLMs give their preference on 2 models’ output.

$$
\begin{array} { r l } & { v _ { \mathrm { I P O } } ( x , y _ { 1 } ) = \bigg \{ \sigma \big ( r _ { \mathrm { I P O } } ( x , y _ { 1 } ) - z _ { \mathrm { r e f } } \big ) \quad \mathrm { i f } y _ { 1 } \sim \mathcal { D } _ { 1 } ^ { p } } \\ & { \sigma ( z _ { \mathrm { r e f } } - r _ { \mathrm { I P O } } ( x , y _ { 1 } ) ) \quad \mathrm { i f } y _ { 1 } \sim \mathcal { D } _ { 1 } ^ { n } } \\ & { r _ { \mathrm { I P O } } ( x , y ; \beta ) = \displaystyle \frac { \beta } { | y | } \log \frac { \pi _ { \theta } ( y \mid x ) } { \pi _ { \mathrm { r e f } } ( y \mid x ) } } \end{array}
$$

$$
\mathbb { E } _ { x \in \mathcal { D } , y \in \pi _ { \theta } } \left[ r _ { \phi } ( x , y ) \right] - \beta D _ { \mathrm { K L } } ( \pi _ { \theta } ( y \mid x ) | | \pi _ { \mathrm { r e f } } ( y \mid x ) )
$$

where $\beta > 0$ is a hyper-parameter. Since this objective function is not differentiable, prior works employ a reinforcement learning algorithm like PPO (Schulman et al. 2017) to optimize it.

$$
\mathcal { L } _ { R } ( r _ { \phi } ) = \mathbb { E } _ { x , y _ { w } , y _ { l } \sim \mathcal { D } } \left[ - \log \sigma \left( r _ { \phi } ( x , y _ { w } ) - r _ { \phi } ( x , y _ { l } ) \right) \right]
$$

However, the performance of PPO-based approach relies on a robust reward model and quite unstable in practice (especially in a distributed setting) (Yuan et al. 2024; Chen et al. 2024c). For this reason, recent work propose a series of closed-form losses that maximize the margin between the preferred and dispreferred generations. As representative algorithms, Direct Preference Optimization (DPO) (Rafailov et al. 2024), has proved its mathematical equivalence with RLHF by minimizing following loss:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { D P O } } ( \pi _ { \theta } , \pi _ { \mathrm { r e f } } ) = } \\ & { \mathbb { E } \left[ - \log \sigma \left( \beta \log \frac { \pi _ { \theta } ( y _ { w } \mid x ) } { \pi _ { \mathrm { r e f } } ( y _ { w } \mid x ) } \right. \left. - \beta \log \frac { \pi _ { \theta } ( y _ { l } \mid x ) } { \pi _ { \mathrm { r e f } } ( y _ { l } \mid x ) } \right) \right] } \end{array}
$$

However, (4) relies on pairwise preference data that requires distinct $\left( y _ { w } \right) - \left( y _ { l } \right)$ based on the same input $x$ . Considering where $\sigma$ is the sigmoid function and the $z _ { \mathrm { r e f } }$ refers to the KL divergence between $\pi _ { \boldsymbol { \theta } }$ and $\pi _ { \mathrm { r e f } }$ . Furthermore, $| y |$ is the a length penalty that prevent model from generating longer yet low-quality sequence (Meng, Xia, and Chen 2024). In IPO, we simply define it as $z _ { \mathrm { r e f } }$ as $\mathrm { K L } ( \pi _ { \boldsymbol { \theta } } ( y | \boldsymbol { x } ) \parallel \pi _ { \mathrm { r e f } } ( y | \boldsymbol { x } ) )$ , which is calculated in each training batch. Notably, this item is designed to stabilize the training process that is not involved into back-propagation.

To handle the data imbalance issue between each subset, we leverage the samples generated by opposite model to balance the optimization data. As the example shown in Figure 2, model $\theta _ { 1 }$ outperform $\theta _ { 2 }$ at $20 \%$ of cases, then $\mathcal { D } _ { 1 } ^ { n }$ will be 4 times of $\mathcal { D } _ { 1 } ^ { \bar { p } }$ . In this case, we sample $60 \%$ positive data from $\mathcal { D } _ { 2 } ^ { p }$ to extend $\mathcal { D } _ { 1 } ^ { p }$ , as the dashed box in Figure 2. Though this parts of data is not generated by model $\theta _ { 1 }$ , maximizing corresponding implicit reward is equivalent to a kind of knowledge distillation from $\theta _ { 2 }$ on those losing data points of $\theta _ { 1 }$ . Conversely, the negative examples from $\mathcal { D } _ { 1 } ^ { n }$ also can contribute to extend $\mathcal { D } _ { 2 } ^ { n }$ that prevents model $\theta _ { 2 }$ from these dispreferred generations. With similar strategy, DPO is reactivated by using non self-generated samples, we select the $y _ { w }$ and $y _ { l }$ in (4) from $\mathcal { D } _ { 1 } ^ { p }$ and $\mathcal { D } _ { 2 } ^ { n }$ since they contains pairwise generations based on the same input $x$ .

Additionally, we utilize the hyper-parameter $\beta$ in (6) to control the model’s updates: when the preference for a given data point comes from human annotators, we set $\beta$ to 0.1 (same as (Ethayarajh et al. 2024)); however, when it comes from beam search or the adversarial model, $\beta$ is decreased to 0.05 to regulate the update magnitude. Then the final optimization of IPO can be written as:

$$
\mathcal { L } _ { \mathrm { I P O } } = \mathbb { E } _ { \boldsymbol { x } , \boldsymbol { y } \sim \mathcal { D } } [ 1 - { v } _ { \mathrm { I P O } } ( \boldsymbol { x } , \boldsymbol { y } ; \beta ) ]
$$

Due to the calibration of $\sigma$ in (5), minimizing $ { \mathcal { L } } _ { \mathrm { I P O } }$ encourages both models to pursuit higher implicit reward, which guides model to generate preferred samples by annotator.

Generally, IPO can handle imbalanced and unpaired preference data generated from model adversarial sampling, and effectively improve the performance for both model $\theta _ { 1 }$ and $\theta _ { 2 }$ . In practice, IPO exhibits two futher advantages: (1) The two models can engage in continuous adversarialoptimization iterations. Although this requires ongoing annotation, when the ground truth of public datasets is available, we demonstrate that LLMs can be used to replace human annotators, as mentioned in Section 3.1. (2) This adversarial optimization method enables efficient distillation, where the preference annotation allows the weak model to selectively absorb knowledge. With just one iteration, the weak model (can be a small model) significantly improve its performance, and it does not require the two models to have identical vocabularies.

Table 2: Speech translation results on 15 FLEURS $\mathbf { X } { \xrightarrow { } } \mathbf { E } \mathbf { n }$ test sets using BLEU score. GPT-4 is used to simulate the 400 annotations for each round, with given source-language and target ground-truth text to make full use of its multilingual ability. “Iterative Optim.” denotes the result that repeat sampling-learning iterations for 3 times, and “Unified Finetune” denotes train one model using across all languages.   

<html><body><table><tr><td>X→En Model</td><td>Ar</td><td>Cy</td><td>De</td><td>El</td><td>Es</td><td>Fa</td><td>Fr</td><td>Hi</td><td>It</td><td>Ja</td><td>Pt</td><td>Ta</td><td>Uk</td><td>Vi</td><td>Zh</td><td>Avg.</td></tr><tr><td>Whisper-L-V2</td><td>25.5</td><td>13.0</td><td>34.6</td><td>23.7</td><td>23.3</td><td>19.6</td><td>32.2</td><td>22.0</td><td>23.6</td><td>18.9</td><td>38.1</td><td>9.2</td><td>29.4</td><td>20.4</td><td>18.4</td><td>23.5</td></tr><tr><td>SeamlessM4T-L-V2</td><td>34.7</td><td>34.9</td><td>37.1</td><td>27.3</td><td>25.4</td><td>30.3</td><td>33.7</td><td>28.5</td><td>26.5</td><td>19.5</td><td>38.5</td><td>22.1</td><td>33.2</td><td>25.7</td><td>23.0</td><td>29.4</td></tr><tr><td>IPO-Whisper</td><td>31.4</td><td>19.8</td><td>36.5</td><td>26.6</td><td>24.7</td><td>26.5</td><td>33.0</td><td>27.0</td><td>26.0</td><td>19.2</td><td>38.4</td><td>17.5</td><td>32.4</td><td>24.0</td><td>22.5</td><td>27.0</td></tr><tr><td>+ Iterative Optim.</td><td>32.3</td><td>20.6</td><td>36.8</td><td>26.9</td><td>25.0</td><td>27.5</td><td>33.4</td><td>27.6</td><td>26.0</td><td>19.3</td><td>38.6</td><td>18.3</td><td>32.7</td><td>24.6</td><td>22.8</td><td>27.5</td></tr><tr><td>w/Unified Finetune</td><td>30.3</td><td>18.6</td><td>36.2</td><td>25.8</td><td>24.4</td><td>23.6</td><td>32.5</td><td>25.7</td><td>25.2</td><td>19.2</td><td>38.2</td><td>14.6</td><td>31.5</td><td>23.1</td><td>20.8</td><td>26.0</td></tr><tr><td>IPO-SeamlessM4T</td><td>37.6</td><td>38.0</td><td>40.2</td><td>31.1</td><td>28.6</td><td>32.7</td><td>36.6</td><td>32.2</td><td>28.7</td><td>23.3</td><td>40.8</td><td>26.5</td><td>35.4</td><td>28.1</td><td>26.7</td><td>32.4</td></tr><tr><td>+ Iterative Optim.</td><td>38.5</td><td>38.7</td><td>41.3</td><td>31.5</td><td>29.5</td><td>33.3</td><td>37.2</td><td>32.9</td><td>29.3</td><td>24.0</td><td>41.5</td><td>27.2</td><td>36.3</td><td>28.9</td><td>27.5</td><td>33.2</td></tr><tr><td>w/ Unified Finetune</td><td>36.0</td><td>36.1</td><td>39.4</td><td>29.8</td><td>27.2</td><td>30.3</td><td>35.6</td><td>30.4</td><td>28.0</td><td>21.9</td><td>40.0</td><td>24.4</td><td>34.5</td><td>26.6</td><td>25.4</td><td>31.0</td></tr></table></body></html>

Table 3: Speech translation results on FLEURS $\mathbf { X } { \xrightarrow { } } \mathbf { E } \mathbf { n }$ test sets using BLEU score. Human evaluators are incorporated to annotate data. “w/o B.S.” denotes the ablation results without pseudo-preference by beam search.   

<html><body><table><tr><td rowspan="2"># Annotations</td><td colspan="3">Whisper /SeamlessM4T</td></tr><tr><td>De-→En</td><td>Fr→En</td><td>Zh→En</td></tr><tr><td>0</td><td>34.6 / 37.1</td><td>32.2/33.7</td><td>18.4/23.0</td></tr><tr><td>100</td><td>35.7/38.2</td><td>33.9/34.7</td><td>21.7 /24.9</td></tr><tr><td>200</td><td>36.6 /40.2</td><td>33.3/36.8</td><td>22.5 /26.8</td></tr><tr><td>400</td><td>36.9 /40.6</td><td>33.5/37.2</td><td>22.9/27.1</td></tr><tr><td>400 w/o B.S.</td><td>35.9/38.8</td><td>33.6/34.4</td><td>21.9/25.9</td></tr></table></body></html>

# 4 Results on Speech Translation 4.1 Baseline Model and Experimental Setup

In this work, we select two popular large speech-to-text models as our starting point, i.e., SeamlessM4T-largev2 (Barrault et al. 2023) and Whisper-large-v2 (Radford et al. 2023). SeamlessM4T is tailored to full-modality multilingual translation tasks and has achieved the state-of-the-art on various AST benchmarks and language directions. Whisper is tailored to robust speech recognition and also shows good robustness on $\Chi { \to } \mathrm { E n }$ speech translation tracks.

For experiments, we use the FLEURS (Conneau et al. 2023) benchmark to evaluate our proposed approach, which is one of the most popular AST benchmarks. Specifically, we select 15 common $\mathrm { X } {  } \mathrm { E n }$ language directions in this study. For sampling, we use the two large models introduced above with beam size of 5. For preference data annotation, we employ both human listener and GPT-4 as we introduced in Section 3.1. Considering the high cost of searching for native speakers in various languages, three major languages are selected for real human annotation–German, French, and Chinese. These 3 evaluators are all X-language native speakers while with excellent English level (live in English-speaking region more than 3 years). After listening source language speech, they compare the output results from two models and then give their preference. Then, we also investigate GPT-4 to annotate data in all 15 languages, whose superior multilingual ability enables an efficient and automatic annotation pipeline. Since we have access to the source-language ground-truth text in AST benchmark, we provide it to GPT-4 to obtain more comprehensive annotations. Furthermore, we investigate two settings in experiments, i.e., specific finetuning and unified finetuning. The former indicates that we annotate data and finetune foundation model on each source language individually, while the latter indicates that we do everything for all source languages together and only obtain all-in-one finetune model at last, we will report their results respectively. Thereafter, for evaluation we use BLEU score with ground-truth, and we also invite human evaluators to listen to source speech and evaluate the translated text according their preference.

![](images/09a5adc728e04a42f74bc1a054e0d881e9ebd86541eb524ba12b52954585be59.jpg)  
Figure 3: Subjective A/B test of IPO and baseline on AST. Whisper and SeamlessM4T are compared with the baseline separately, and then their win and loss rates are combined for presentation according to translated languages.

# 4.2 Objective Evaluation

Table 3 illustrates the AST results on three FLEURS $\mathrm { X } {  } \mathrm { E n }$ directions with both Whisper and SeamlessM4T baselines. We can observe that only 100 annotations can produce clear BLEU improvements over two strong baseline models and three language directions, e.g., 1.1 BLEU improvement on SeamlessM4T De $\scriptstyle \longmapsto \operatorname { E n }$ , which demonstrates the remarkable data efficiency of our approach. Thereafter, with increasing data annotation, we can obtain better BLEU results on different baseline models and translations, showing the performance gain obtained from human feedback. The last row also demonstrates the efficacy with beam search by ablation.

Table 2 illustrates the AST results on 15 FLEURS $\mathrm { X } {  } \mathrm { E n }$ directions with GPT-4 as the preference data annotator. We first observe from the two baselines that Whisper-largev2 achieves satisfactory performance (as a robust speech recognition model) on all source languages except Tamil, while SeamlessM4T-large-v2 achieves state-the-of-art performance on all languages. Therefore, in this study, our major goal is to improve the Whisper model with guidance from SeamlessM4T. As expected, our proposed IPO significantly improve the performance of Whisper baseline model, especially the almost 10 BLEU increase on $\mathrm { T a } \to \mathrm { E n }$ track that vanilla Whisper is not good at. The overall gain is 3.5 BLEU score, indicating the effectiveness of IPO in improving weak baseline model with adversarial sampling and RLHF finetuning. On the other hand, we are surprising to find that the stronger baseline model, SeamlessM4T, also obtained great enhancement with a gain of 3.0 BLEU score. This finding indicates that our RLHF finetuning approach with adversarial sampling enables different-level models to benefit from each other, resulting in a new state-of-the-art based on the strong SeamlessM4T baseline model.

In addition, we also conduct two studies to investigate iterative optimization and unified finetuning. (1) The proposed sampling-learning pipeline can be iteratively conducted to further improve the model performance, as updated model can yield better sampling results. We conduct 3 times of iterative finetuning on top of previously optimized model and present the results as “Iterative Optim.” in Table 2, which achieves even further improvements over IPO results. However, more iterative rounds would not obtain more performance gain, approaching the upper-bound performance limited preference data. (2) We also perform unified finetune using all data together across multiple languages, i.e., “Unified Finetune” in Table 2. Although slightly inferior to specific fine-tuning, it is still significantly better than the Whisper and SeamlessM4T baseline. More importantly, unified finetuning achieves better generalization ability without forgetting on specific language.

# 4.3 Subjective Evaluation

For comprehensive assessment, we also incorporate the human evaluators for subjective evaluation. For each language, 100 examples are randomly selected from test set. Whisper and SeamlessM4T (after IPO) respectively perform inference, resulting in $2 \times 1 0 0$ translation utterances. Then the A/B test is conducted on the comparison between theses utterances with baseline (before IPO). As shown in Fig. 3, our IPO shows consistent and clear advantage over baseline on three language translations, which verifies the effectiveness of our proposed approach.

# 5 Results on Audio Captioning 5.1 Baseline Model and Experimental Setup

Automatic audio captioning (AAC) is a crucial task in understanding real-world audio signals, which requires a sound description of the events happening in given audio.

Table 4: Audio captioning results on Clotho dataset. “w/o B.S.” denotes the ablation results without pseudo-preference by beam search. Pengi win rate: $4 1 . 0 \%$   

<html><body><table><tr><td rowspan="2"># Annotations</td><td colspan="3">Pengi/Qwen-Audio</td></tr><tr><td>CIDEr 个</td><td>SPICE↑</td><td>SPIDEr ↑</td></tr><tr><td>0</td><td>0.416 /0.441</td><td>0.126/0.136</td><td>0.271/0.288</td></tr><tr><td>100</td><td>0.435/0.460</td><td>0.130/0.139</td><td>0.283/0.299</td></tr><tr><td>200</td><td>0.444 /0.469</td><td>0.132/0.141</td><td>0.288/0.302</td></tr><tr><td>400</td><td>0.448 / 0.473</td><td>0.133 /0.141</td><td>0.291/0.306</td></tr><tr><td>400 w/o B.S.</td><td>0.440/0.447</td><td>0.130/0.138</td><td>0.285/0.294</td></tr></table></body></html>

![](images/4817291650f4c1e9d5cfea5de7a48f77bfedc2b74700f628973c610dff0aaecc.jpg)  
Figure 4: Subjective A/B test of proposed IPO and baseline on AAC. Human evaluator are included to give preference.

In this work, we select two large audio language models that can perform audio captioning as our starting point, i.e., Pengi (Deshmukh et al. 2023) and Qwen-Audio (Chu et al. 2023), both of which attempt to incorporate audio modality into language model for comprehensive understanding.

For experiments, we the Clotho (Drossos, Lipping, and Virtanen 2020) benchmark to evaluate our proposed approach, which is among the most popular AAC benchmarks. For adversarial sampling of IPO, two human listener are employ to give total 400 annotations after listening audio segments. It is noted that human annotation prefer longer captions when comparing the output of two models, therefore, the length penalty is important to prevent model from generating too long sentences. For evaluation, we use three metrics, i.e., CIDEr, SPICE and SPIDEr, to compare our optimized model with baseline. In A/B test, human evaluators are required to give the preference on 200 captioning results between before/after IPO.

# 5.2 Objective and Subjective Evaluation

Table 4 illustrates the AAC results on two baselines with different number of annotations in training data. First, we can observe that the two baseline models achieve quite good performance on Clotho benchmark. However, in our trials, only 100 annotations can yield significant improvements on both baseline models in terms of three metrics, especially the CIDEr. Increasing annotations can help further improve the performance, where 400 seems to approach the upperbound. Overall, we can observe that IPO not only significantly improves the weak baseline (i.e., Pengi), but it also enhances the stronger one (i.e., Qwen-Audio), similar to what we observed previously in AST experiments. Furthermore, subjective A/B test in Fig. 4 also shows the effectiveness of our proposed IPO.

<html><body><table><tr><td>Method</td><td>Predicted Captioning Result</td></tr><tr><td>Pengi Qwen-Audio</td><td>People talk with cars driving by A men is talking with busy traffc and sharp noise in the background</td></tr><tr><td>IPO-Pengi IPO-Qwen-Audio</td><td>People talk with drilling noise and busy traffic behind A men talks with jackhammer running and busy trafic roaring by in the background</td></tr></table></body></html>

Table 5: Case study of audio captioning on Clotho dataset. Model provide more detailed description for audio event via IP

<html><body><table><tr><td># Annotations</td><td>VC-330M/VC-830M WER↓ SIM↑</td></tr><tr><td>0</td><td>MOS ↑ 3.37 /4.21</td></tr><tr><td>200</td><td>17.9 / 2.7 0.76 / 0.90 3.85 / 4.35 8.0 / 2.5 0.87/0.92</td></tr></table></body></html>

Table 6: Objective Zero-shot TTS results on LibriTTS dataset. “MOS”, “WER”, and “SIM” are all calculated by pre-trained nerual model. VC-330M win rate: $1 8 . 5 \%$ .

Case Study. To demonstrate the enhancement of the model’s perception abilities by IPO, we provide a case study in Table 5. The two baselines both give a description about people talking and the cars driving by, where the stronger QwenAudio specifies more details about the male and sharp noise. In comparison, IPO optimized models yield more details, e.g., drilling noise, jackhammer running, which more comprehensively describe the audio event.

# 6 Extension on Speech Syntheses

# 6.1 Baseline Model and Experimental Setup

To comprehensive evaluate the proposed IPO approach, we also conduct experiments on zero-shot text-to-speech (TTS) synthesis task. For experiments, we select two size of zeroshot TTS system which are proposed in VoiceCraft (Peng et al. 2024). i.e., VC-330M and VC-830M as our baseline model, and we select the popular LibriTTS dataset (Zen et al. 2019) as benchmark. Notably, this experiment can be also demonstrate that IPO can be utilized as a knowledge distillation function from large model to light model. We does not use beam search but perform inference for 5 times with random seeds. For evaluation, we employ NISQA (Mittag et al. 2021) to calculate mean opinion score (MOS), Whisper-medium.en to calculate word error rate, and WavLM-TDCNN to calculate speaker similarity (SIM).

During sampling, 2 human listeners are employed to respectively give 100 annotations for two models, resulting in 200 samples that VC-830M win for 163 times. Despite the disparity in results, we find that VC-330M mostly lose due to its robustness on some unseen speakers, since its training data does not include LibriTTS. For evaluation, these 2 human listeners perform A/B test on 100 examples (6 second to 16 second) sampled from LirbriTTS test set it terms of naturalness of synthesized speech, including a tie if it is hard to give their preference.

![](images/7c5f0950bf8ac5e730b09f3ef378f241541a29d0174e55f9e5e0c853ee46f1f9.jpg)  
Figure 5: Subjective A/B test of proposed IPO and baseline on TTS naturalness.

# 6.2 Objective Evaluation

Table 6 illustrates the objective evaluation results in terms of MOS, WER, and SIM. The experimental results on IPO indicate that this 200 annotations effectively improve the synthesized speech quality in terms of three metrics. Specifically, the VC-330M has been improved to a large extent, the failed cases are significantly avoided according to the ratio of high WER $5 5 0 \%$ samples $( 1 0 . 4 \%  3 . 1 \% )$ . The VC830M baseline can even be further enhanced, which is similar to AST and AAC tasks. This experimental evidence on TTS demonstrates the remarkable scalability of IPO.

# 6.3 Subjective Evaluation

Furthermore, we present some subjective evaluation in Fig. 5 to verify the effectiveness of our approach. We conduct A/B test between our optimized model and VC-330M baseline and ground-truth speech by asking human evaluators to select the better one. Results indicate that our synthesized speech outperforms the baseline (i.e., larger win rate than lose rate), though it still lags a bit behind the ground-truth speech. It indicates that our approach makes good use of various baseline models to sample and annotate data, which is proved beneficial to both models.

# 7 Conclusion

In this work, we explore a novel topic that align audio-text generation with subjective feedback. To this end, we propose IPO, consisting of a model adversarial sampling strategy to alleviate the limited annotations, and a interactively optimization algorithm based on imbalanced and unpaired preference data. Experimental evidence demonstrates the significant performance gain of IPO on three cross-modal generation tasks including AST, AAC, and TTS, enhancing the performance of multiple prevalent audio-text pre-trained models. More importantly, our approach provides new insight and inspiration to the post-training alignment of these models using limited human annotations.