# Self-Supervised Collaborative Information Bottleneck for Text Readability Assessment

Jinshan Zeng1\*, Xianglong $\mathbf { Y } \mathbf { u } ^ { 1 }$ , Xianchao Tong1, Wenyan Xiao2

1Jiangxi Normal University 2Jiangxi University of Science and Technology jinshanzeng@jxnu.edu.cn, xianglongyu@jxnu.edu.cn, xianchaotong@jxnu.edu.cn,wy.xiao@jxust.edu.cn

# Abstract

Text readability assessment involves categorizing texts based on readers’ comprehension levels. Hybrid automatic readability assessment (ARA) models, combining deep and linguistic features, have recently attracted rising attention due to their impressive performance. However, existing hybrid ARA models generally ignore the specific-intrinsic information of deep and linguistic representations, and cannot fully explore their common-intrinsic information. In this paper, we introduce a self-supervised collaborative information bottleneck (SCIB) module for ARA to address these issues. Specifically, we collaboratively consider both specificintrinsic and common-intrinsic information of the linguistic representation and various levels of deep representations including the document-, sentence- and word-level deep representations, and yield their refined representations via a selfsupervised information bottleneck scheme. Extensive experiments are conducted on four English and two Chinese corpora to demonstrate the effectiveness of the proposed model. Experimental results show that the proposed model outperforms state-of-the-art models in terms of four important evaluation metrics, and the suggested SCIB module can effectively capture the specific- and common-intrinsic information.

# Introduction

The assessment of text readability is designed to determine the complexity of a given text, helping readers select materials that match their comprehension level (McLaughlin 1969; Klare 2000). This not only caters to individual language skills, cognitive abilities, and developmental stages but also extends its utility to diverse sectors such as text recommendations, the crafting of clinical informed consents, and the publishing of books.

With the continuous advancement of readability research, a series of readability assessment methods have been developed, including readability formulas (Dale and Chall 1948; Kincaid et al. 1975) , statistical machine learning methods (Dell’Orletta, Montemagni, and Venturi 2011; Sung et al. 2015), and neural language modeling methods (Deutsch, Jasbi, and Shieber 2020; Tseng et al. 2019; Zeng et al. 2022). The kind of readability formula methods mainly yields the readability level according to empirical formulas with respect to some linguistic features, while the kind of statistical machine learning methods mainly focuses on yielding the readability level by training some statistical models such as support vector machines (SVM) with handcrafted linguistic features (Sung et al. 2015). Due to the limited representation capability of these linguistic features (also called surface features), both kinds of readability formula and statistical machine learning methods cannot yield satisfactory performance, especially for some complex texts.

Inspired by the powerful representation ability of neural networks, neural language models have been widely favored by researchers for readability assessment (Azpiazu and Pera 2019; Deutsch, Jasbi, and Shieber 2020; Tseng et al. 2019; Zeng et al. 2022). Azpiazu and Pera (2019) proposed a multilingual readability assessment model (Vec2Read) based on the hierarchical attention network (HAN) (Yang et al. 2016). This method combines word part of speech (POS) and morphological tags, and utilizes hierarchical information to generate word- and sentence-level attention scores for creating text representations. Later, some novel deep models based on the pre-trained model have been suggested in the literature (Deutsch, Jasbi, and Shieber 2020; Tseng et al. 2019; Zeng et al. 2022), with the emergence of pre-trained models such as Transformer (Vaswani et al. 2017) and BERT (Devlin et al. 2019). Tseng et al. (2019) adopted the pre-trained BERT for the text readability assessment. Zeng et al. (2022) integrated the pre-trained BERT into the HAN architecture and proposed a novel deep model for readability assessment by introdcing soft labels for ordinal regression.

Despite the impressive performance of the kind of neural models, their performance is generally limited by the insufficient training corpora. To alleviate this issue, recent research turns to the direction of combining linguistic features and deep features for readability assessment, i.e., hybrid automatic readability assessment (ARA) models called in the literature (Deutsch, Jasbi, and Shieber 2020; Lee, Jang, and Lee 2021; Li, Wang, and Wu 2022; Zeng et al. 2023, 2024). There are two key ingredients for the kind of hybrid ARA models, i.e., feature extraction and fusion. The literature (Deutsch, Jasbi, and Shieber 2020; Lee, Jang, and Lee 2021) exploited the pre-trained language models such as BERT (Devlin et al. 2019) to extract the deep feature, and fed deep and linguistic features simultaneously into a statistical machine learning model to yield readability levels without fusion. Li, Wang, and Wu (2022) extracted various levels of deep features including the word-, sentence- and documentlevel deep features, and fused them with the linguistic feature by an orthogonal projection scheme. Motivated by (Li, Wang, and Wu 2022), Zeng et al. (2023) incorporated the prompt learning (Lee and Lee 2023) into the deep feature extraction to fully explore deep representations, and utilized a hierarchical orthogonal projection fusion scheme to fuse these various levels of deep and linguistic features. Later, Zeng et al. (2024) introduced a linguistic interpreter and used the contrastive learning (Khosla et al. 2020) to fully explore linguistic and deep representations respectively, and fused them via a similar hierarchical orthogonal projection fusion scheme.

Notice that existing orthogonal projection fusion schemes do not consider the specific-intrinsic information of various levels of features important for the readability assessment, and cannot yield their refined representations. Moreover, the performance of the orthogonal projection fusion scheme is sensitive to the choice of the orthogonal base feature. To address these issues, we propose a sophisticated fusion scheme for ARA, motivated by the well-known information bottleneck principle (Tishby and Zaslavsky 2015). The major contributions of this paper can be summarized as follows.

• We propose a novel hybrid ARA model through introducing a self-supervised collaborative information bottleneck (SCIB) module, which collaboratively considers both specific-intrinsic and common-intrinsic information of various levels of representations including the linguistic-level representation and the word-, sentence, and document-level deep representations, and yields their refined representations via a self-supervised information bottleneck scheme. By leveraging the SCIB module, the specific-intrinsic and common-intrinsic information of various levels of deep and linguistic features are fully explored and fused for the readability assessment.

• Extensive experiments are conducted over four English and two Chinese corpora to demonstrate the effectiveness of the proposed model. Experimental results show that the proposed model outperforms competing neural models on most datasets, and the suggested SCIB module can effectively capture the specific- and common-intrinsic information of these deep and linguistic features.

# Related Work

# Linguistic Representation

Linguistic features defined in linguistics generally reflect some surface structures of texts and can provide important information for the readability assessment. Existing linguistic features can be mainly divided into four categories, i.e., the lexical, semantic, syntactic and cohesion level features (Lee, Jang, and Lee 2021). Zeng et al. (2023) used these linguistic feature to improve the performance of readability assessment, where the linguistic representation was yielded from these linguistic features by implementing the layer normalization and linear projection operations. To enrich the linguistic representation, Zeng et al. (2024) introduced a linguistic interpreter to transfer linguistic features into some natural language sentences according to some templates, which are facilitate to representing them in a deep way.

# Deep Representation

In the early stage, deep representations were directly extracted by some pre-trained language models such as BERT (Tseng et al. 2019). Later, multi-level deep representations were suggested in the literature (Zeng et al. 2022; Li, Wang, and Wu 2022; Zeng et al. 2023, 2024). Motivated by HAN (Yang et al. 2016), Zeng et al. (2022) suggested using the hierarchical multi-level deep representations for readability assessment. Li, Wang, and Wu (2022) suggested using the parallel multi-level deep representations including the document-, sentence/paragraph-, and word-level for readability assessment, since such kind of multi-level deep representations generally contains richer information than the kind of hierarchical multi-level deep representations. To further improve deep representations, the prompt learning (Lee and Lee 2023) and contrastive learning (Khosla et al. 2020) were adopted in the literature (Zeng et al. 2023) and (Zeng et al. 2024), respectively.

# Fusion Scheme

It is crucial to fuse these various levels of deep representations and the linguistic representation. To reduce the redundancy among these representations, the orthogonal projection fusion scheme was widely used in the literature (Li, Wang, and Wu 2022; Zeng et al. 2023, 2024). However, existing orthogonal projection fusion scheme does not consider the specific-intrinsic information of these various-level representations, and is sensitive to the choice of the orthogonal base representation. To address these issues, we suggest a sophisticated fusion scheme called self-supervised collaborative information bottleneck scheme inspired by the information bottleneck principle (Tishby and Zaslavsky 2015). The suggested scheme collaboratively considers both specific- and common-intrinsic information, and yields refined representations of these deep and linguistic representations via a self-supervised information bottleneck scheme.

# Proposed Model

In this section, we describe the proposed model dubbed SCIB-ARA in detail. As shown in Figure 1, the proposed model mainly consists of the feature representation module and the suggested SCIB module, where the feature representation module aims to yield various levels of deep and linguistic representations, and the SCIB module is introduced to effectively fuse them. In the feature representation module, besides the linguistic-level representation, three levels of deep representations including the word-, sentenceand document-level deep representations are yielded. In the SCIB module, both the specific-intrinsic and commonintrinsic information of these four various levels of representations are collaboratively considered and the associated refined representations are yielded by the self-supervised information bottleneck principle, while these refined representations are finally concatenated for assessment.

![](images/f2f971dcc30d54efd1430e13f7a55c132bd48e8d006116ee18f7632584abd3ac.jpg)  
Figure 1: The pipeline of the proposed SCIB-ARA model for text readability assessment.

# Feature Representation Module

The source of feature representations mainly comprises two parts: deep representations and linguistic representations. Three levels (i.e., document-, sentence- and word-level) of deep representations are derived from features within deep models, while linguistic representations are the fundamental properties manually extracted from texts.

Specifically, for a given text, its document-level representation $\widetilde { X } _ { d }$ is directly taken as the embedding from BigBird (Zahe reet al. 2020) with the whole text as the input. For the word-level deep representation $\smash { \widetilde { X } } _ { w }$ , we feed the text into the BigBird and produce embeddinegs for each word, and then yield the word-level deep representation with a mixing pooling layer, which is a hybrid of the max-pooling and meanpooling operations designed to extract both the maximum value and the average value (Yu et al. 2014). The sentencelevel deep representation ${ \widetilde { X } } _ { s }$ is yielded in a similar way through feeding the text t eigBird sentence by sentence.

Since linguistic features characterize some important textual natures, and provide many additional insights and contextual information for ARA, linguistic features should be paid particular attention in ARA. For linguistic representations, we firstly extract various linguistic features, then perform the layer normalization (Ba, Kiros, and Hinton 2016) to eliminate the influence of changes between different features and obtain a normalized linguistic representations $\widetilde { X } _ { l } ^ { n o r m }$ , finally yield the linguistic representation ${ \widetilde { X } } _ { l }$ with the same dimension of deep representations by a li ear layer.

# Self-supervised Collaborative Information Bottleneck Module

With these levels of deep and linguistic representations, we collaboratively consider both the common-intrinsic information and specific-intrinsic information of these representations in the SCIB module to yield refined representations for readability assessment, where the specific information bottleneck sub-module and the common information bottleneck sub-module are designed to capture the specific-intrinsic information and common-intrinsic information of these four levels of representations, respectively. The common-intrinsic information contains the consistent information of these four levels of representations, while the specific-intrinsic information can provide important individual information important for readability assessment.

Specifically, in the common information bottleneck submodule, we firstly concatenate these four levels of text representations, i.e., ${ \widetilde { X } } _ { s }$ , $\smash { \widetilde { X } _ { w } }$ , $\widetilde { X } _ { d }$ , and ${ \widetilde { X } } _ { l }$ , and then feed the concatenated rep se n etio $\widetilde { X }$ into  e ncoder $E _ { c }$ to generate a common representatio $\smash { \widetilde { X } ^ { C } }$ . To guide the encoder $E _ { c }$ to learn the consistent inform teion of these four various levels of representations, we introduce a decoder $D _ { c }$ , motivated by the information bottleneck principle. We feed $\smash { \widetilde { X } ^ { C } }$ to the decoder $D _ { c }$ and yield an estimate $\tilde { X ^ { ' } }$ of the con setent representation of these four various levels of representations as the supervision of training. To achieve this, we impose the following self-supervised loss on the proposed model:

$$
\mathcal { L } _ { r e c } ^ { c } = \sum _ { i \in \{ s , w , d , l \} } \| \widetilde { X } _ { i } - \widetilde { X } ^ { \prime } \| ^ { 2 }
$$

Notice that the common text representation $\smash { \widetilde { X } ^ { C } }$ may overlook unique information specific to individueal-level representations, which can provide important supplementary information for the common representation. To achieve this, we collaboratively consider the specific-intrinsic and common-intrinsic information, and introduce the specific information bottleneck sub-module to capture the unique information specific to these four levels of representations.

Specifically, we feed the sentence-level representation ${ \widetilde { X } } _ { s }$ into the associated encoder $E _ { s }$ to yield its specific repre - sentation $\smash { \widetilde { X } _ { s } ^ { S } }$ , then feed $\smash { \widetilde { X } _ { s } ^ { S } }$ into the associated decoder $D _ { s }$ to yield  eestimate $\widetilde { X } _ { s } ^ { \prime }$ fe the sentence-level representation $\widetilde { X } _ { s } ^ { \phantom { \dagger } }$ , which is used oe form the reconstruction loss to supe rvise the learning procedure. Following the similar procedures, we can yield the specific representations and reconstruction loss functions for the other two levels of deep representations. For the linguistic representation, we use the normalization $\widetilde { X } _ { l } ^ { n o r m }$ of the linguistic representation ${ \widetilde { X } } ^ { l }$ instead of itsel teo form the self-supervised scheme oer the learning of linguistic specific representation, where an additional linear layer is included to perform the normalization compared to self-supervised procedures for deep representations. Thus, we impose the following loss on the training of specific information bottleneck sub-module:

$$
\mathcal { L } _ { r e c } ^ { s } = \sum _ { i \in \{ s , w , d \} } \Vert \widetilde { X } _ { i } - \widetilde { X } _ { i } ^ { \prime } \Vert ^ { 2 } + \Vert \widetilde { X } _ { l } ^ { n o r m } - \widetilde { X } _ { l } ^ { \prime } \Vert ^ { 2 } .
$$

With the common representation $\smash { \widetilde { X } ^ { C } }$ and specific representations XsS,w,d,l := {XsS, XwS, X ,eXlS}, we concatenate them to y led the unifiederepresenteatione $\smash { \widetilde { X } ^ { U } }$ , and then feed $\smash { \widetilde { X } ^ { U } }$ into a neural predictor consisting  ea linear layer and seoftmax to yield the prediction of the readability level.

According to the above description, the training loss of the proposed model can be formulated as follows:

$$
\mathcal { L } _ { \mathrm { S C I B - A R A } } = \mathcal { L } _ { C E } + \lambda \left( \mathcal { L } _ { r e c } ^ { c } + \mathcal { L } _ { r e c } ^ { s } \right) ,
$$

where $\mathcal { L } _ { C E }$ is the cross-entropy loss for prediction, $\lambda$ is a tunable hyper-parameter.

# Experiments

In this section, a series of comparative experiments were conducted on four English and two Chinese corpora to compare the effectiveness of the proposed model with existing state-of-the-art models. We also conducted ablation experiments on various components of the proposed model.

# Experimental Settings

We describe experimental settings in detail.

A. Corpora. We evaluated the proposed model through experiments over four English corpora (i.e., WeeBit (Vajjala and Meurers 2012), Cambridge1, Newsela2 and CLEAR), and two Chinese corpora (i.e., CMT (Lee, Liu, and Cai 2020; Zeng et al. 2022) and CMER). Some statistics of these corpora are presented in Table 1.

Table 1: Statistics for the used six corpora including four English and two Chinese corpora.   

<html><body><table><tr><td>Corpus</td><td>Weebit</td><td>Cambridge</td><td>Newsela</td><td>CLEAR</td><td>CMT</td><td>CMER</td></tr><tr><td>No. classes</td><td>5</td><td>5</td><td>11</td><td>10</td><td>12</td><td>12</td></tr><tr><td>No. texts</td><td>3125</td><td>300</td><td>9565</td><td>4724</td><td>2621</td><td>2260</td></tr><tr><td>Ave.length</td><td>288</td><td>510</td><td>747</td><td>172</td><td>927</td><td>675</td></tr></table></body></html>

B. Baselines. We considered eight state-of-the-art models as baselines to verify the effectiveness of the proposed model. These include two typical pre-trained language models (i.e., BERT (Devlin et al. 2019) and BigBird (Zaheer et al. 2020)), two representative deep models based on hierarchical attention networks (i.e., HAN (Yang et al. 2016) and DTRA (Zeng et al. 2022)), and four hybrid ARA models (i.e., Lee-2021 (Lee, Jang, and Lee 2021), BERT-FP-LBL (Li, Wang, and Wu 2022), PromptARA (Zeng et al. 2023) and InterpretARA (Zeng et al. 2024)).

C. Implementation details. For the proposed model, we used AdamW (Loshchilov and Hutter 2017) as the optimizer with a weight decay parameter of 0.01 and a warmup ratio of 0.1. The used linguistic features for the proposed model are presented in Supplementary Materials.

For these two baselines, i.e., Lee-2021 (Lee, Jang, and Lee 2021) and BERT-FP-LBL (Li, Wang, and ${ \tt W u } 2 0 2 2 \mathrm { , }$ ), we directly took experimental results reported in the literature for comparison, since we cannot access their reproducible codes. For a fair comparison, we followed the similar setups in (Lee, Jang, and Lee 2021) and (Li, Wang, and Wu 2022) for other baselines. For each corpus, we split the data into training, validation, and test sets in a ratio of 8:1:1, and reported the average results of three trails. All experiments were implemented on RTX 3090 and A40 GPUs, and in the PyTorch framework.

# Comparison with State-of-the-art Models

We conducted a series of experiments to evaluate the performance of the proposed model through comparing with the state-of-the-art models, in terms of four commonly used metrics for classification, i.e., accuracy (Acc), precision (Pre), macro $F l$ -metric (F1), and quadratic weighted kappa (QWK). The comparison results are presented in Table 2.

From Table 2, we can observe that the proposed model achieves the best or suboptimal performance over most of corpora in terms of all four evaluation metrics. Specifically, in comparison with these two pre-trained deep models, i.e., BERT (Devlin et al. 2019) and BigBird (Zaheer et al. 2020), the proposed model is superior to these two models over all corpora, in particular over the Cambridge, CLEAR, CMT and CMER corpora. When comparing the performance between BERT and BigBird, we can observe that BigBird has substantial improvement on the performance than BERT over all corpora. This also motivates us to utilize BigBird to extract deep features in the proposed model.

Table 2: Comparison results of the proposed SCIB-ARA model and baselines over four English and two Chinese benchmark corpora. $*$ Experimental results were directly taken from the literature since we cannot access their reproducible source codes. The best and second best results are marked in bold and underlined, respectively.   

<html><body><table><tr><td>Datasets</td><td>Metrics</td><td>BERT</td><td>BigBird</td><td>HAN</td><td>DTRA</td><td>Lee-2021*</td><td>BERT-FP-LBL*</td><td>PromptARA</td><td>InterpretARA</td><td>Our</td></tr><tr><td rowspan="4">Weebit</td><td>Acc</td><td>91.53</td><td>92.70</td><td>82.54</td><td>85.29</td><td>90.50</td><td>92.70</td><td>93.12</td><td>93.12</td><td>94.07</td></tr><tr><td>Pre</td><td>91.56</td><td>92.73</td><td>83.73</td><td>85.54</td><td>90.50</td><td>92.89</td><td>93.19</td><td>93.46</td><td>94.26</td></tr><tr><td>F1</td><td>91.51</td><td>92.70</td><td>82.76</td><td>85.30</td><td>90.50</td><td>92.73</td><td>93.09</td><td>93.17</td><td>94.10</td></tr><tr><td>QWK</td><td>97.10</td><td>97.17</td><td>94.48</td><td>95.65</td><td>96.80</td><td>97.78</td><td>97.43</td><td>97.81</td><td>98.03</td></tr><tr><td rowspan="4">Cambridge</td><td>Acc</td><td>75.56</td><td>87.78</td><td>76.67</td><td>77.78</td><td>76.30</td><td>87.78</td><td>91.11</td><td>90.00</td><td>95.56</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Pre</td><td>72.75</td><td>88.29</td><td>80.59</td><td>79.77</td><td>79.20</td><td>89.46</td><td>92.24</td><td>91.29</td><td>95.847</td></tr><tr><td>QWK</td><td>91.59</td><td>97.04</td><td>92.64</td><td>92.62</td><td>91.90</td><td>96.97</td><td>97.82</td><td>96.88</td><td>98.99</td></tr><tr><td rowspan="4">CLEAR</td><td>Acc</td><td>76.74</td><td>78.86</td><td>67.87</td><td>72.09</td><td></td><td></td><td>82.03</td><td>83.30</td><td>83.72</td></tr><tr><td>Pre</td><td>76.23</td><td>79.01</td><td>66.22</td><td>70.75</td><td></td><td>-</td><td>81.99</td><td>83.04</td><td>83.49</td></tr><tr><td>F1</td><td>76.05</td><td>78.34</td><td>66.43</td><td>70.81</td><td></td><td></td><td>81.75</td><td>82.81</td><td>83.49</td></tr><tr><td>QWK</td><td>92.86</td><td>94.03</td><td>89.29</td><td>91.85</td><td></td><td></td><td>95.54</td><td>95.54</td><td>95.45</td></tr><tr><td rowspan="4">Newsela</td><td>Acc</td><td>77.12</td><td>87.15</td><td>83.80</td><td>83.07</td><td>-</td><td></td><td>88.40</td><td>87.15</td><td>87.57</td></tr><tr><td>Pre</td><td>78.45</td><td>87.14</td><td>83.86</td><td>82.96</td><td></td><td></td><td>88.41</td><td>87.01</td><td>87.44</td></tr><tr><td>F1</td><td>76.59</td><td>87.05</td><td>83.70</td><td>82.82</td><td></td><td></td><td>88.24</td><td>87.04</td><td>87.47</td></tr><tr><td>QWK</td><td>97.67</td><td>98.79</td><td>98.55</td><td>98.41</td><td>-</td><td></td><td>98.88</td><td>98.81</td><td>98.81</td></tr><tr><td rowspan="4">CMT</td><td>Acc</td><td>38.46</td><td>39.19</td><td>42.53</td><td>44.42</td><td></td><td></td><td>43.96</td><td>44.87</td><td>45.79</td></tr><tr><td>Pre</td><td>38.79</td><td>40.60</td><td>40.57</td><td>44.24</td><td>-</td><td></td><td>43.17</td><td>45.65</td><td>45.24</td></tr><tr><td>F1</td><td>37.17</td><td>35.97</td><td>41.09</td><td>43.87</td><td>-</td><td></td><td>41.60</td><td>43.22</td><td>42.82</td></tr><tr><td>QWK</td><td>88.09</td><td>88.97</td><td>88.00</td><td>89.95</td><td>-</td><td>，</td><td>91.20</td><td>91.89</td><td>90.98</td></tr><tr><td rowspan="4">CMER</td><td>Acc</td><td>22.30</td><td>24.06</td><td>23.40</td><td>26.50</td><td></td><td></td><td>26.50</td><td>25.39</td><td>31.79</td></tr><tr><td>Pre</td><td>20.13</td><td>25.55</td><td>15.47</td><td>25.36</td><td>-</td><td>-</td><td>24.24</td><td>24.60</td><td>30.55</td></tr><tr><td>F1</td><td>13.49</td><td>22.58</td><td>18.48</td><td>25.16</td><td></td><td></td><td>23.92</td><td>23.40</td><td>27.51</td></tr><tr><td>QWK</td><td>65.39</td><td>70.86</td><td>72.10</td><td>70.53</td><td></td><td></td><td>68.74</td><td>73.95</td><td>75.33</td></tr></table></body></html>

Compared to the kind of hierarchical attention network based models such as HAN (Yang et al. 2016) and DTRA (Zeng et al. 2022), the proposed model significantly outperforms them over most of corpora. Specifically, as compared to HAN, the proposed model achieves improvements of $1 1 . 5 3 \%$ , $1 8 . 8 9 \%$ , $1 5 . 8 5 \%$ , $3 . 7 7 \%$ , $8 . 3 9 \%$ and $3 . 2 6 \%$ in accuracy over six concerned corpora respectively, while yields the associated improvements of $8 . 7 8 \%$ , $1 7 . 7 8 \%$ , $1 1 . 6 3 \%$ , $4 . 5 0 \%$ , $5 . 2 9 \%$ and $1 . 3 7 \%$ in accuracy compared to DTRA.

When compared to existing hybrid models like Lee-2021 (Lee, Jang, and Lee 2021) and BERT-FP-LBL (Li, Wang, and $\mathrm { W u } ~ 2 0 2 2 \rangle$ , the proposed model also yields substantial improvements over Weebit and Cambridge corpora. In terms of accuracy, the proposed model achieves improvements of $3 . 5 7 \%$ and $1 9 . 2 6 \%$ over Weebit and Cambridge, respectively when compared to Lee-2021, while yields the associated improvements of $1 . 3 7 \%$ and $7 . 7 8 \%$ as compared to BERT-FPLBL. In Lee-2021, deep features are firstly utilized to yield certain soft labels, which together with the linguistic features are simultaneously fed to a statistical classifier for the final prediction without fusion. On the other hand, in BERT-FPLBL, only the document-level deep representation and linguistic representation are fused by a simple orthogonal projection layer. It can be observed that the fusion way used in this paper through learning consistent and specific representations of both deep and linguistic representations can yield refined representations incorporating both deep and linguistic information. Moreover, we extract multi-level deep representations instead of the sole document-level representation to yield deep representations of higher quality and abundance. The comparison results with these two hybrid models demonstrate the superiority of using multi-level deep representations methods.

When Compared to the recent two hybrid ARA models, i.e., PromptARA (Zeng et al. 2023) and InterpretARA (Zeng et al. 2024), our model also demonstrates the superior performance. Specifically, compared with PromptARA using prompts and orthogonal projection fusion, the proposed model improves by $0 . 9 5 \%$ , $4 . 4 5 \%$ , $1 . 6 9 \%$ , $5 . 2 9 \%$ and $1 . 8 3 \%$ in accuracy over Weebit, Cambridge, CLEAR, CMER and CMT, respectively. While compared with InterpretARA using the linguistic interpreter and orthogonal projection fusion scheme, the proposed model improves by

$0 . 9 5 \%$ , $5 . 5 6 \%$ , $0 . 4 2 \%$ , $0 . 4 2 \%$ , $6 . 4 0 \%$ and $0 . 9 2 \%$ in accuracy over the concerned six corpora, respectively. These results show the superiority of the proposed model over state-ofthe-art models. Moreover, these numerical results demonstrate that the suggested SCIB fusion scheme is superior to the commonly used fusion scheme based on the orthogonal projection, though the recent PromptARA and InterpretARA models use other advanced techniques such as prompts and linguistic interpreter to enrich representations.

Regarding the performance of the proposed model over different corpora, we can observe from Table 2 that the proposed model achieves high accuracy for Weebit and Cambridge with the accuracy over $94 \%$ , and yields the moderately high accuracy for CLEAR and Newsela with the accuracy over $8 3 \%$ , and achieves relatively low accuracy for these two Chinese corpora with the accuracy $3 1 . 7 9 \%$ and $4 5 . 7 9 \%$ , respectively. These results show that it is generally more challenging for the readability assessment of Chinese texts due to the semantic ambiguity of Chinese texts.

Table 3: On effectiveness of the introduced SCIB moodule.   

<html><body><table><tr><td>Dataset</td><td>Model</td><td>Acc</td><td>Pre</td><td>F1</td><td>QWK</td></tr><tr><td rowspan="4">Weebit</td><td>SCIB-ARA</td><td>94.07</td><td>94.26</td><td>94.10</td><td>98.03</td></tr><tr><td>w/o C</td><td>93.55</td><td>93.58</td><td>93.54</td><td>97.75</td></tr><tr><td>w/o S</td><td>93.44</td><td>93.48</td><td>93.43</td><td>97.81</td></tr><tr><td>w/o (C, S)</td><td>92.06</td><td>92.18</td><td>92.05</td><td>96.87</td></tr><tr><td rowspan="4">Cambridge</td><td>SCIB-ARA</td><td>95.56</td><td>95.87</td><td>95.54</td><td>98.99</td></tr><tr><td>w/o C</td><td>94.44</td><td>94.92</td><td>94.30</td><td>98.61</td></tr><tr><td>w/o S</td><td>94.45</td><td>94.86</td><td>94.41</td><td>98.59</td></tr><tr><td>w/o (C, S)</td><td>88.89</td><td>89.81</td><td>88.62</td><td>96.30</td></tr><tr><td rowspan="4">CLEAR</td><td>SCIB-ARA</td><td>83.72</td><td>83.49</td><td>83.49</td><td>95.45</td></tr><tr><td>w/o C</td><td>81.61</td><td>81.28</td><td>81.14</td><td>94.63</td></tr><tr><td>w/o S</td><td>81.61</td><td>81.13</td><td>80.95</td><td>94.63</td></tr><tr><td>w/o (C, S)</td><td>81.40</td><td>80.75</td><td>80.95</td><td>94.57</td></tr><tr><td rowspan="4">CMER</td><td>SCIB-ARA</td><td>31.79</td><td>30.55</td><td>27.51</td><td>75.33</td></tr><tr><td>w/o C</td><td>26.50</td><td>29.65</td><td>25.44</td><td>74.43</td></tr><tr><td>w/o S</td><td>26.93</td><td>25.38</td><td>21.99</td><td>73.67</td></tr><tr><td>w/o (C, S)</td><td>24.28</td><td>19.43</td><td>19.85</td><td>74.15</td></tr><tr><td rowspan="4">CMT</td><td>SCIB-ARA</td><td>45.97</td><td>45.24</td><td>42.82</td><td>90.98</td></tr><tr><td>w/o C</td><td>45.79</td><td>43.55</td><td>41.93</td><td>90.79</td></tr><tr><td>w/o S</td><td>44.51</td><td>41.31</td><td>40.32</td><td>89.78</td></tr><tr><td>w/o (C, S)</td><td>41.58</td><td>39.87</td><td>39.23</td><td>85.25</td></tr></table></body></html>

# Ablation Study

In this subsection, we conducted a series of ablation studies on three English corpora (Weebit, Cambridge and CLEAR) and two Chinese corpora (CMT and CMER) to investigate the feasibility and effectiveness of our proposed ideas.

A. On effectiveness of SCIB module. We verified the effectiveness and feasibility of the introduced common and specific information bottleneck sub-modules by comparing the performance of the proposed model with that of models by removing the common representation (w/o C), the specific representations (w/o S), and both (w/o (C, S)). The comparison results are presented in Table 3.

It can be observed from Table 3 that by removing different representation modules, the performance of the proposed model has undergone varying degrees of changes in terms of all evaluation metrics. Specifically, in terms of accuracy, removing common, and specific information bottleneck sub-modules respectively, as well as simultaneously removing them, result in an average performance decrease of $1 . 8 4 \%$ , $2 . 0 3 \%$ , and $4 . 5 8 \%$ , respectively. It shows that removing both common and specific information bottleneck submodules simultaneously results in a cumulative decrease that is similar to removing individual common and specific sub-modules separately, and the specific information bottleneck sub-module gains slightly more improvement than the common information bottleneck sub-module on average, which shows the importance of the considered specificintrinsic information for the readability assessment.

When concerning different corpora, the introduced SCIB module achieves improvements of $2 . 0 1 \%$ , $6 . 6 7 \%$ , $2 . 3 2 \%$ , $7 . 5 1 \%$ , and $4 . 3 9 \%$ in accuracy over these five concerned corpora, respectively. These results show that the introduced SCIB module yields significant improvements to Cambridge, CMER and CMT, and achieves substantial improvements to Weebit and CLEAR. When concerning the impacts of the individual common and specific information bottleneck sub-modules, it can be observed that the specific sub-module gain more improvements overall than the common sub-module over these two Chinese corpora. These indicate that both common and specific sub-modules have learnt important information in text readability evaluation. Similar claims can also be drawn based on the other three evaluation indicators. These results clearly show the effectiveness of the introduced SCIB module.

In order to demonstrate the feasibility of the SCIB module, we visualized the common and specific representations yielded by the proposed model over the English corpus Weebit and the Chinese corpus CMER in Figure 2. It can be observed from Figure 2 that the proposed model can learn the common and specific representations over both English corpus Weebit and Chinese corpus CMER, where the common and specific representations are explicitly decomposed into different subspaces. When concerning different specific representations, the concerned four types of specific representations generally lie in different subspaces, since they generally characterize different levels of information of texts. It can be observed that these three deep specific representations generally lie in three relatively close subspaces because they all focus on deep representations of texts, while the linguistic specific representation lies in a subspace that is different to these deep specific representations, since linguistic features focus on the surface structures of texts and are generally different to deep features. These results verify the feasibility of the proposed idea.

B. On effectiveness of the self-supervised scheme. As shown in Figure 1, we adopted a self-supervised scheme in the SCIB module to supervise the representation refinement. In the following, we conducted a series of experiments over five concerned corpora to demonstrate the effectiveness of the introduced self-supervised scheme. The comparison experimental results are presented in Table 4. It can be observed from Table 4 that the performance of the proposed model is degraded much by removing the self-supervised scheme. Specifically, the performance decreases by $2 . 2 2 \%$ , $6 . 6 7 \%$ , $3 . 5 9 \%$ , $6 . 8 4 \%$ , and $4 . 5 8 \%$ in accuracy over these five concerned corpora, respectively, by removing the selfsupervised scheme. These results clearly show the effectiveness of the proposed self-supervised scheme.

![](images/dc5080759520c1f4bd49c7dee49eb8a48913998e018663d8615d1ff2a650a8dc.jpg)  
Figure 2: Visualization of common and specific representations yielded by the proposed SCIB-ARA model over the English corpus Weebit and the Chinese corpus CMER.

Table 4: On effectiveness of the self-supervised scheme.   

<html><body><table><tr><td>Dataset</td><td>Model</td><td>Acc</td><td>Pre</td><td>F1</td><td>QWK</td></tr><tr><td>Weebit</td><td>SCIB-ARA w/o self-supervised</td><td>94.07 91.85</td><td>94.26 92.00</td><td>94.10 91.87</td><td>98.03 96.92</td></tr><tr><td>Cambridge</td><td>SCIB-ARA w/o self-supervised</td><td>95.56 88.89</td><td>95.87 90.41</td><td>95.54 88.51</td><td>98.99 97.31</td></tr><tr><td>CLEAR</td><td>SCIB-ARA w/o self-supervised</td><td>83.72 80.13</td><td>83.49 79.43</td><td>83.49 79.59</td><td>95.45 94.15</td></tr><tr><td>CMER</td><td>SCIB-ARA w/o self-supervised</td><td>31.79 24.95</td><td>30.55 23.69</td><td>27.51 23.29</td><td>75.33 73.61</td></tr><tr><td>CMT</td><td>SCIB-ARA w/o self-supervised</td><td>45.97 41.39</td><td>45.24 35.89</td><td>42.82 37.70</td><td>90.98 87.88</td></tr></table></body></html>

C. On importance of the linguistic representation. We implemented a series of experiments to demonstrate the importance of the linguistic representation for the proposed model. Experimental results are presented in Table 5, where w/o L represents the model without using the linguistic representation. It can be observed from Table 5 that the performance of the proposed model degrades when getting rid of the linguistic representation. Moreover, as shown in Figure 2, the linguistic representation lies in a very different subspace in comparison to deep representations. These show the importance of the linguistic representation for ARA, and in particular that the linguistic representation can provide im

portant supplementary information for deep representations.   

<html><body><table><tr><td>Dataset</td><td>Model</td><td>Acc</td><td>Pre</td><td>F1</td><td>QWK</td></tr><tr><td rowspan="2">Weebit</td><td>SCIB-ARA</td><td>94.07</td><td>94.26</td><td>94.10</td><td>98.03</td></tr><tr><td>w/oL</td><td>94.01</td><td>94.18</td><td>94.02</td><td>97.97</td></tr><tr><td rowspan="2">Cambridge</td><td>SCIB-ARA</td><td>95.56</td><td>95.87</td><td>95.54</td><td>98.99</td></tr><tr><td>w/o L</td><td>94.45</td><td>94.86</td><td>94.41</td><td>98.59</td></tr><tr><td rowspan="2">CLEAR</td><td>SCIB-ARA</td><td>83.72</td><td>83.49</td><td>83.49</td><td>95.45</td></tr><tr><td>w/oL</td><td>82.24</td><td>82.11</td><td>81.71</td><td>94.84</td></tr><tr><td rowspan="2">CMER</td><td>SCIB-ARA</td><td>31.79</td><td>30.55</td><td>27.51</td><td>75.33</td></tr><tr><td>w/o L</td><td>28.26</td><td>29.38</td><td>27.12</td><td>74.48</td></tr><tr><td rowspan="2">CMT</td><td>SCIB-ARA</td><td>45.97</td><td>45.24</td><td>42.82</td><td>90.98</td></tr><tr><td>w/o L</td><td>43.04</td><td>42.23</td><td>40.39</td><td>90.07</td></tr></table></body></html>

Table 5: On importance of the linguistic representation.

# Conclusion

This paper proposes a novel hybrid ARA model to effectively fuse the deep and linguistic representations, motivated by the concept of information bottleneck. A self-supervised collaborative information bottleneck module is suggested to collaboratively consider the common-intrinsic and specificintrinsic representations for various levels of deep and linguistic representations. The effectiveness of the proposed model is demonstrated by extensive experiments over four English and two Chinese corpora. Experimental results show that the proposed model outperforms the state-of-the-art models in terms of four important evaluation metrics. Numerous ablation studies are also conducted to demonstrate the feasibility and efficiency of proposed ideas.