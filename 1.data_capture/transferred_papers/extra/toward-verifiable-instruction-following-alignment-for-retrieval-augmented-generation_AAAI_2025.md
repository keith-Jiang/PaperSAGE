# Toward Verifiable Instruction-Following Alignment for Retrieval-Augmented Generation

Guanting Dong1, Xiaoshuai $\mathbf { S o n g } ^ { 2 }$ , Yutao $\mathbf { Z } \mathbf { h } \mathbf { u } ^ { 1 }$ , Runqi Qiao2, Zhicheng $\mathbf { D o u } ^ { 1 * }$ , Ji-Rong Wen1

1Gaoling School of Artificial Intelligence, Renmin University of China 2School of Artificial Intelligence, Beijing University of Posts and Telecommunications dongguanting, dou @ruc.edu.cn

# Abstract

Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instructionfollowing (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, an automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions $_ { ( < 1 0 0 ) }$ and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general samples, scaling up to a high-quality VIF-RAG-QA dataset $( > 1 0 0 \mathrm { k } )$ through automated processes. To further bridge the gap in instructionfollowing auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems.

# Code — https://github.com/dongguanting/FollowRAG Extended version — https://arxiv.org/pdf/2410.09584

# Introduction

The advancement of Large Language Models (LLMs) (OpenAI 2023; Yang et al. 2024) has profoundly revolutionized a variety of real-world tasks expressed in natural language (Wei et al. 2022). However, they still suffer from hallucinations and factual inconsistencies (Bang et al. 2023), impacting the authenticity of generated answers. RetrievalAugmented Generation (RAG) has gained recognition as a promising solution, empowering LLMs to leverage reliable information from retrieved documents, thereby returning high-quality responses (Lewis et al. 2020).

![](images/a860e1a79490b98f90eba9f6e7f9fead9d2e8db9580c9f4a2796c20a15d1bc6c.jpg)  
Figure 1: The instruction-following tasks for LLMs in RAG.

In real-world interaction scenarios, users often deviate from standard templates when posing questions, instead of imposing diverse instructions on model outputs to meet specific task requirements (Chung et al. 2024; Dong et al. 2022). Consequently, improving instruction-following (IF) capabilities is foundational to the effective application of LLM and RAG systems. The core goal of IF is to enable models to adapt to the diverse intents of users, which has garnered widespread attention in the LLM community.

Existing efforts on instruction-following alignment primarily focus on multi-grained evaluation and high-quality instruction data synthesis to enhance LLMs’ natural instruction-following capabilities (Zhou et al. 2023; Wen et al. 2024). However, in complex RAG scenarios, the diverse knowledge introduced by retrieval-augmented techniques presents significant challenges for LLMs in effectively handling complex instructions (Figure 1). After supervised fine-tuning on high-quality general and knowledgeintensive QA datasets, LLMs demonstrate robust performance in both IF and RAG tasks. However, these capabilities do not always generalize well to instruction-following tasks under RAG scenarios and may even conflict with the performance of other fundamental abilities (Dong et al. 2024b). Unfortunately, research on instruction-following in

RAG systems remains limited, significantly hindering their application in real-world interactions. To tackle these challenges, our aim is to address following critical research questions:

• RQ1. How can we comprehensively evaluate the complex instruction-following capabilities in the RAG scenario? • RQ2. How can we achieve scalable and reliable instruction-following alignment in RAG systems while preserving the it’s foundational abilities from conflict?

In this paper, we propose VIF-RAG, the first automated, scalable, and reliable data synthesis pipeline for achieving complex instruction-following alignment in RAG scenarios. The core insight of VIF-RAG is to ensure every step of data augmentation and combination includes a proper verification process. Specifically, we start by manually crafting a minimal set of atomic instructions $( < 1 0 0 )$ and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting. Motivated by tool execution studies (Le et al. 2022), we employ the same supervised model to generate verification code and automatically verify the quality of augmented instructions through the Python compiler’s outputs. Finally, we combine these high-quality instructions with RAG datasets from various domains (each containing retrieved documents per query), performing the augmentation and dual validation process to synthesize a highquality instruction-based RAG dataset, named VIF-RAGQA ${ \tt S I O 0 K }$ samples).

To further bridge the gap in automatic instructionfollowing evaluation for RAG systems, we introduce FollowRAG, the first benchmark dedicated to comprehensively assessing the complex $\mathrm { I F }$ capabilities of RAG systems. FollowRAG aggregates constraints from real-world scenarios. It includes approximately 3K test samples, spanning 4 knowledge-intensive QA benchmarks and 22 types of constraints. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks.

To summarize, our contributions are as follows:

• To first achieve instruction-following alignment in the RAG system, we propose VIF-RAG, an automated, scalable, and verifiable data synthetic framework. VIFRAG uniquely combines augmented rewriting with diverse validation processes to synthesize high-quality instruction-following alignment data from almost scratch $( < 1 0 0 )$ , scaling up to over 100K samples. • We introduce FollowRAG, the first benchmark designed to comprehensively evaluate LLM’s complex instructionfollowing abilities in RAG tasks. FollowRAG includes nearly 3K test samples, spanning four knowledgeintensive QA benchmarks and 22 types of constraints. Its design ensures seamless integration with various RAG benchmarks, providing strong scalability. • With FollowRAG and 8 widely-used IF and 3 foundational benchmarks, we demonstrate that different LLMs with VIF-RAG achieve extraordinary alignment on instruction following in both RAG and standard scenarios while effectively preserving foundational capabilities.

# Related Work

Instruction-Following Alignment for LLMs. Instructionfollowing ability is a core capability of large language models. Existing works fall into two main categories. The first includes efforts (Hendrycks et al. 2021; Zheng et al. 2024), which rigorously evaluate models’ adherence to general instructions. Moreover, previous works focus on fine-grained assessment under specific constraints, using stricter criteria such as instruction difficulty, domain, and task formats (Qin et al. 2024). The other category focuses on improving IF alignment. Manual design of instructions and responses by human annotators (Wei et al. 2021) is challenging and costly. To address this, methods are developed to synthesize diverse instructions, allowing weaker models to mimic the responses of advanced models (Dong et al. 2024a), achieving strongto-weak alignment (Cao et al. 2024).

Alignment for Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) addresses the issue of knowledge hallucination in LLMs by retrieving relevant factual information, offering a promising solution (Guu et al. 2020; Lewis et al. 2020). However, efficiently aligning retrieved knowledge with LLMs’ preferences remains a challenge. Researchers have developed robust reranker-based methods (Sun et al. 2023) and data filtering approaches (Wang et al. 2023) to reduce noisy information and bridge this gap. Additionally, approaches like RePLUG (Shi et al. 2023) integrate LLMs’ preferences into training objectives to improve alignment. Query rewriting methods (Ma et al. 2023; Dong et al. 2023b) attempt to adjust inputs based on these preferences. Furthermore, SelfRAG (Asai et al. 2024) use multi-round retrieval and generation to refine outputs and achieve better alignment. Despite these advancements, the diverse knowledge introduced by retrieval-augmented techniques poses significant challenges for LLMs in handling complex instructions.

# Preliminaries

Retrieval-Augmented Generation (RAG). RetrievalAugmented Generation systems usually operates under a retrieve-then-read framework (Lewis et al. 2020). The external retriever is integrated to gather supporting knowledge and improve the generation process. Given a query $q$ , a retriever R recalls k relevant documents Dq = {di}ik=1 from an external corpus comprised of $N$ documents. We employ the DPR (Karpukhin et al. 2020) to obtain hidden vectors for queries and documents. The relevance score is determined by measuring the dot-product similarity between the query and document representations, allowing the retrieval of the top- $k$ documents $D _ { q }$ :

$$
D _ { q } = \operatorname { a r g t o p } { - k \left[ E _ { \mathrm { d } } ( d _ { i } ) ^ { \top } \cdot E _ { \mathrm { q } } ( q ) | i = \{ 1 \dots N \} \right] } .
$$

Then, the retrieved documents are concatenated with the query into an LLM reader $R$ to generate the target text:

$$
y = R ( q , D _ { q } ) = \log P _ { \theta } ( q , D _ { q } ) ,
$$

where $P _ { \theta }$ is the output probability distribution.

Instruction-following Alignment for RAG. Following instructions is one of the most foundational ability for LLMs

VIF-RAG Instruction Synthesis from Scratch Scalable Instruction - Query Synthesis ★ E Rewriting v 5 G Rewriting ExVecruiftiocratbiaosned □ √= W □ Consistency Seed Composited Augmented Auto-verified Augmented Verification VIF-RAG-QA   
Instructions Instructions Instructions Instructions Queries Dataset Instruction Composition Executor based Verification Answer using only questions Response 1： 1 Atomic Constrains Write the response as a series of four-word sentences. 1 Clouds drift low. Verification Function 1 1 E Query: Response 2： 1 What is the weather like today? . Keep Answer with a Fibonacci sequence of words (1, 1, 2, 3, Sunny skies, Verification Function 2 1 · 5) and Include at least one word ending with "-ing". 二 + bright day! 1 Instruction： 1   
1 Answer with words that are names of planets "Keep your response where the first Response k-1： 1   
Multiple Constraints and Keep your response exactly 10 words. nweoxrtd"isanodne"laenttsewrelronwgitehr twhoarndsthe Sbruingnhyt.days are Verification Function k-1 i ChoanisntrRaiunltes 1 123. PCWlarepiatiseteatlhrizeepretehasteptofhinresteqluaesttsteairosoenfriwaeilstlhowofoufrtivdaesn-iywnocyrhodausnregrneteplnyc.es. i ↑ RWeisnpdoyn.se k： Verification Function k

in RAG systems. Given an instruction $\boldsymbol { I } = \{ I _ { j } \} _ { j = 1 } ^ { M }$ with $M$ specific constraints and a specific query $q$ with corresponding relevant $k$ retrieved documents $D _ { q }$ , The LLM $\pi _ { \boldsymbol { \theta } }$ in the RAG system is expected to produce an accurate response $y \sim \pi _ { \boldsymbol { \theta } } ( y \mid q , D _ { q } , I )$ while obeying with constraints.

# VIF-RAG Framework

In this section, we propose VIF-RAG, which can be broadly split into two sections: (1) the instruction synthesis stage and (2) instruction-query synthesis, scaling from almost scratch $_ { ( < 1 0 0 ) }$ to over 100K high-quality instruction-query samples. Below, we will delve into the specifics.

# Instruction Synthesis from Scratch

Handwritten Seed Instructions. We initially develop a minimal seed instruction set $D _ { \mathrm { s e e d } } ^ { \mathrm { a t o m } }$ manually, using four foundational categories of constraints: format constraints, semantic constraints, knowledge constraints, and lexical constraints, as themes for instruction writing.

We hire only one well-educated human annotator to manually create 15 single-atomic instructions for each type of constraint. Notably, this is the only process in our data synthesis process that includes human supervision.

Instruction Composition & Verification. Real-world instructions often involve multiple constraints in a user query. To address this, we design rules to automatically combine atomic instructions into diverse, complex instructions:

• Multiple Constraints: As illustrated in Figure 2, we randomly sample pairs of instructions from $D _ { \mathrm { s e e d } } ^ { \mathrm { a t o m } }$ and insert them into a constraint template. By directly concatenating these instruction pairs, we create complex instructions that contain dual and triple constraints. This type of instruction requires the model to generate results that satisfy multiple constraints simultaneously.

• Chain Rule Constraints: We design sequential conditional constraint templates and selected atomic instructions from $D _ { \mathrm { s e e d } } ^ { \mathrm { a t o m } }$ to form chain constraints. Formally, the chain consists of $n$ tasks $\{ T _ { 1 } , T _ { 2 } , . . . , T _ { n } \}$ , requiring the model’s output to complete these n tasks sequentially.

Verification. Randomly combining atomic instructions can easily lead to conflicts between them (e.g., don’t use words that end with ’-ing’). These semantic conflicts can be challenging to detect using a simple Natural Language Inference model. To detect potential conflicts between these instructions, we use a robust supervised model that rates their consistency from 1 to 10. Samples scoring below 8 are excluded to refine our high-quality complex instruction set Dsceoemdplex. Ultimately, we arrive at the initial seed instruction set Dseed = {Dsaetoedm ∪ Dsceoemdplex}.

Instruction Rewriting $\pmb { \& }$ Quality Verification. To automate the scaling up of instructions, the instruction rewriting strategy is considered the most natural augmentation method, and has received significant attention in the RAG and reasoning fields (Li et al. 2024b,a; Dong et al. $2 0 2 4 \mathrm { c } )$ . We use a supervised model1 to iteratively rewrite instructions from the $D _ { \mathrm { s e e d } }$ set in batches of 50 for $K$ rounds, generating an augmented set $D _ { \mathrm { a u g } }$ . Subsequently, we merge the seed and augmented samples to form the combined instruction set $D _ { \mathrm { i n s } } = D _ { \mathrm { s e e d } } \cup D _ { \mathrm { a u g } }$ , removing duplicates.

Inspired by tool execution works (Le et al. 2022), we aim to leverage the powerful coding abilities of LLMs to assist in verifying the quality of auto-generated instructions. As shown in Figure 2, for each instruction $I \in D _ { \mathrm { i n s } }$ , we use the supervision model to generate $K$ verification function codes and corresponding test cases $\{ f u n c _ { j } ^ { I } , c _ { j } ^ { I } \} _ { j = 1 } ^ { K } \in D _ { \mathrm { v e r i f y } }$ , and assess the instruction’s quality by analyzing the output of the executor $\mathcal { E }$ . For any function and test case $\{ f u n \dot { c } _ { j } ^ { I } , c _ { j } ^ { I } \} \in$ $D _ { \mathrm { v e r i f y } }$ , its execution output is:

$$
\mathcal { E } ( f u n c _ { j } ^ { I } , c _ { j } ^ { I } ) = \left\{ \begin{array} { r l } { 1 } & { \mathrm { I f ~ o u t p u t ~ i s ~ ^ { \ast \ast } T r u e ^ { , \ast } } . } \\ { 0 } & { \mathrm { I f ~ o u t p u t ~ i s ~ ^ { \ast } F a l s e ^ { , \ast } o r ~ ^ { \ast \ast } E r r o r ^ { , \ast } . } } \end{array} \right.
$$

Therefore, we can calculate the accuracy $A c c _ { \mathrm { f u n c } }$ of each verification function based on $K$ test samples, as well as the accuracy $A c c _ { \mathrm { c a s e } }$ of each case evaluated using $K$ verification functions. These can be formulated as:

$$
\left\{ \begin{array} { l } { { A c c _ { \mathrm { f u n c } } = \frac { 1 } { K } \sum _ { j = 1 } ^ { K } \mathcal { E } ( f u n c ^ { I } , c _ { j } ^ { I } ) _ { j = 1 } ^ { K } } } \\ { { \qquad } } \\ { { A c c _ { \mathrm { c a s e } } = \frac { 1 } { K } \sum _ { j = 1 } ^ { K } \mathcal { E } ( f u n c _ { j } ^ { I } , c ^ { I } ) _ { j = 1 } ^ { K } . } } \end{array} \right.
$$

Based on the above cross metrics, we require that at least one $A c c _ { \mathrm { f u n c } }$ and $A c c _ { \mathrm { c a s e } }$ of the each instruction must exceed 0.5, Ultimately, we obtain the auto-verified instruction set as

$$
D _ { \mathrm { i n s } } ^ { \mathrm { v e r i f y } } = \{ d \in D _ { \mathrm { i n s } } \ | \ A c c _ { \mathrm { f u n c } } ( d ) > 0 . 5 \ \& \ A c c _ { \mathrm { c a s e } } ( d ) > 0 . 5 \} .
$$

The samples that do not meet the cross metrics are discarded.

# Scalable Instruction-Query Synthesis

Random Instruction-Query Combination. In real-world interactions with RAG systems, achieving IF alignment depends on effectively integrating the synthesized instructions with the queries used by the RAG system. To meet this goal, as depicted in Figure 2, we first extract high-quality queries from two different data sources.

1) RAG Domain: To build an effective RAG system, We need to prepare sufficient amounts of QA-format data with relevant knowledge to enhance its knowledge-based interaction capabilities. Consequently, we randomly select a query set $Q$ from mixed QA data sources, including multi-hop and knowledge base QA 2. Following Lewis et al.; Dong et al., We employ the dense retriever $R$ to fetch the top- $K$ relevant documents $D _ { i }$ for each query $q \in Q$ from an external knowledge base, resulting in the dataset $D _ { \mathrm { R A G } } = \{ q _ { i } , D _ { i } \} _ { i = 1 } ^ { K }$ . Furthermore, we randomly select $K$ queries along with their corresponding retrieved documents from $D _ { \mathrm { R A G } }$ for each inswtrituhctIiFonc $I$ satnrad nctos $D _ { \mathrm { I F - R A G } } = \{ I _ { j } , q _ { j } , D _ { j } \} _ { j = 1 } ^ { K }$ .AG query set

2) General Domain: In addition to incorporating RAGspecific abilities, the RAG system has to possess basic human-aligned abilities to meet users’ daily interaction needs. Therefore, ShareGPT (Chiang et al. 2023), which provides authentic multi-turn human dialogue data, is our natural choice. Similar to how we handle the RAG domain, for each instruction $I \in D _ { \mathrm { i n s } }$ , we randomly select $K$ queries from the ShareGPT to combine with the instruction and construct the general dataset $D _ { \mathrm { I F - G e n e r a l } }$ for each instruction.

Ultimately, we merge the instruction-constrained query sets from these two domains into the final query set of VIFRAG-QA, formulated as $D _ { \mathrm { V I F - R A G } } ^ { q }$ .

Instruction-Query Rejection Sampling. It is worth noting that under diverse instruction-following constraints, the original grounding truth answers for queries in both the RAG and general datasets become unreliable. To address this issue and improve synthetic data diversity, we adopt a rejection sampling strategy (Yuan et al. 2023). Specifically, we use the supervision model to generate $K$ responses $y _ { x } \doteq \{ y _ { i } \} _ { i = 1 } ^ { K }$ for each instruction-query pair $x \in D _ { \mathrm { V I F } } ^ { q ^ { - } }$ -RAG, resulting in $\{ x , y _ { x } \} \in D _ { \mathrm { V I F - R A G } }$ .

Dual Stage Verification. To further ensure comprehensive quality control of the synthetic dataset, we employ a dual stage verification process for the instruction-query data:

• Executor-based Verification: We leverage pre-existing verification functions to evaluate adherence in the augmented outputs. As in the “Instruction Rewriting & Quality Verification” section, at least one response in DVIF-RAG must achieve an accuracy rate $A c c _ { \mathrm { c a s e } }$ above 0.5 across all verification functions; otherwise, the sample is discarded. • Consistency Verification: We notice that combined instructions and queries often conflict. A simple example is when the query “Please write a brief biography of Barack Obama.” does not meet the instruction “Strictly limit your answer to less than 10 tokens.” , Therefore, we employ a supervision model to evaluate the alignment between queries and instructions on a scale of 1 to 10, discarding samples that receive a score below 8.

After dual stage verification, we have automatically obtained a large-scale, high-quality VIF-RAG-QA dataset.

# FollowRAG Benchmark

To bridge the gap in automatic instruction-following evaluation for RAG systems, we introduce FollowRAG in this section. We provide a detailed introduction from two aspects: “Data Construction” and “Evaluation and Statistics”.

# Dataset Construction

Instruction Collection & Extraction. FollowRAG aims to assess the model’s ability to follow user instructions in complex multi-document contexts. Drawing from general IF datasets like FollowBench (Jiang et al. 2024), we collect and verify definitions and examples of atomic instructions using rules, excluding those irrelevant to RAG scenarios. Ultimately, we identify 22 types of instruction constraints, encompassing language, length, structure, and keywords.

Instruction Reforming. We use widely-used questionanswering (QA) datasets, such as Natural Questions (Kwiatkowski et al. 2019), as the foundation for constructing FollowRAG samples. For a query sampled from the QA datasets, we need to generate a complex instruction containing $n$ atomic instruction constraints (with $n$ ranging from 1 to 4). To enhance the diversity of atomic instruction representations, we employ GPT-4o as the instruction generator. Specifically, given a query, we first sample $n$ instructions from the atomic instruction set and perform conflict

(a) Pipeline Sample & Collection Retrieve & Reforming Combination & Checking Sample QE ---> E QA Dataset Question Retriever Document I Collect Combination Human-Check FollowRAG Instruction Data Source Seed Instruction Set Instruction Generator Instruction   
(b) Case (c) Statistics Reference Documents 1 蘭 FollowRAG uaDspoepcdruotoxmiesmnpatet1ce:ilfyTy itthlhe:naPecigHdaitCtiyvoenotroefbntah:seiPcbiHtaysIenof1ca0hnelomagiqsautreiyot,hupsmHso…ilsutaiolon.gIatritshmic scale wor. Sen. Par.Jso.Quo. No. Total samples 2800 Fre. La 明 ooo pDHoc7uamt belief, the pH value can be less than 0 or greater than 14 for very strong $( 2 5 \mathrm { ~ ~ } ^ { \circ } C )$ e,:bPeiHngCnoenitehnetr: atenmapceirdantuoreainbcarse .a sCeso.ntPruarey  twoatpeorpiuslanreutral, 1 th Ee Question count from the QA dataset Natural Questions (NQ) 700 acgirdosnaonmdy,bamsedsicriensep,ecthievmeliys.trMy,eawsautreremtrenattsmoefnptH…are important in E 星 THWroeitvbpiQaotuQeAsAt(i(oTHnQsQAS)AP)(WebQSP) 700 FollowRAG Document3. Title: Soil pH Content: Soil pH Soil pH is a measure of the Benchmark acidity or basicity (alkalinity) of a soil. pH is defined as the negative logarithm (base 10) of the activity of hydronium ions ( or, more precisely, ) Number of atomic instructions 1 in a solution. In soils, it is measured in a slurry of soil mixed with 1 美 contained in the sample Question-Instructions 1 Cases Pla. TOwneo 900 ? What is the highest base on the pH scale? When answering this question you 1 Cap Upp. Three 500 must avoid using commas throughout your entire response. Additionally at the 1 Low. Four 500 3 end ensure there is a postscript that begins with P.P.S. 1

detection. Subsequently, with examples as demonstrations, we prompt the LLM to generate a new varied instruction text for each type of atomic instruction, along with parameters for instruction-following evaluation.

Combination. Finally, we integrate the retrieved passages, query and atomic instructions to construct the sample input for FollowRAG. To avoid mechanically concatenating the query and instructions in a template-based manner, we prompt supervised model to naturally blend the multiple atomic instructions and the query into a coherent instructionquery paragraph. We then add the top- $\cdot K$ document passages retrieved based on the query to the instruction-query paragraph, forming the complete sample input.

# Evaluation and Statistics

After obtaining the model’s output, we evaluate it from two perspectives: instruction following and question answering (QA) under the RAG paradigm:

Instruction Following: Using the verifiable nature of our atomic instructions and following the IFEval approach, we automate the verification of the model’s compliance with each instruction by code validation. We then calculate the average pass rate for each atomic instruction across all samples to determine the IF score in FollowRAG.

RAG: Under new instruction constraints, the model’s target output differs from the gold answers in the original QA dataset, rendering traditional metrics like Exact-Match ineffective. To address this, we use the original gold answers as a reference and utilize GPT-4o to evaluate whether the model’s outputs correctly address the questions. The scoring criteria are as follows: Completely correct (1), Partially correct (0.5), Completely incorrect (0). The average score of all samples is taken as the RAG score for FollowRAG.

For detailed statisticsin in Figure 3, FollowRAG is the first instruction-following evaluation dataset under RAG scenario comprising $2 . 8 \mathsf { K }$ samples, covering 22 fine-grained atomic instructions across 6 categories. The queries in FollowRAG are sourced from 4 QA datasets across 3 types: 1) Open-Domain QA: Natural Questions (NQ) (Kwiatkowski et al. 2019) and TriviaQA (TQA) (Joshi et al. 2017); 2) Multi-Hop QA: HotpotQA (HQA) (Yang et al. 2018); and 3) Knowledge Base QA: WebQuestionsSP (WebQSP) (Yih et al. 2016). To further construct varying levels of instruction-following difficulty, FollowRAG includes 0.9K samples of single and dual atomic instructions, as well as $0 . 5 \mathsf { K }$ complex multi-instruction samples containing 3 and 4 atomic instructions, respectively.

# Experiment

# Experimental Setup

Datasets. We evaluate over $^ { 1 0 + }$ benchmarks to comprehensively evaluate the VIF-RAG. For the instruction-following tasks in RAG scenarios, we use the FollowRAG benchmark as mentioned in Section 5, which covering 4 questionanswering (QA) datasets. For general IF evaluation, we selected two commonly used complex IF datasets, IFEval and FollowBench, along with the natural instruction dataset MT-Bench (Zheng et al. 2024) and the challenging ChatBot IF bench, Arena-Hard (Li et al. 2024c). Additionally, to measure that the foundational abilities of LLMs, we further evaluate two widely used LLM’s general abilties evaluation sets, C-Eval (Huang et al. 2023) and MMLU (Hendrycks et al. 2021), as well as the mathematical reasoning dataset GSM8K (Cobbe et al. 2021) and the code evaluation bench HumanEval (Chen et al. 2021).

Table 1: The main results on FollowRAG.“AVG” represents the weighted average of the corresponding IF and RAG scores. The top two results in each column are highlighted in bold and underlined.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="3">NQ</td><td colspan="3">TQ</td><td colspan="3">HQ</td><td colspan="3">WebQSP</td><td colspan="3">ALL</td></tr><tr><td>IF</td><td>RAG</td><td>AVG</td><td>IF</td><td>RAG</td><td>AVG</td><td>IF</td><td>RAG</td><td>AVG</td><td>IF</td><td>RAG</td><td>AVG</td><td>IF</td><td>RAG</td><td>AVG</td></tr><tr><td>Llama3-8B-base</td><td>3.2</td><td>5.7</td><td>4.4</td><td>4.1</td><td>15.9</td><td>10.0</td><td>3.6</td><td>7.3</td><td>5.5</td><td>10.0</td><td>23.1</td><td>16.5</td><td>5.2</td><td>13.0</td><td>9.1</td></tr><tr><td>Llama3-8B-SFT</td><td>15.7</td><td>59.5</td><td>37.6</td><td>15.0</td><td>76.5</td><td>45.7</td><td>15.0</td><td>52.5</td><td>33.8</td><td>14.4</td><td>70.0</td><td>42.2</td><td>15.0</td><td>64.6</td><td>39.8</td></tr><tr><td>Llama3-8B-SFT-VIF-RAG</td><td>43.9</td><td>65.0</td><td>54.5</td><td>42.7</td><td>78.0</td><td>60.4</td><td>39.6</td><td>46.0</td><td>42.8</td><td>42.5</td><td>70.5</td><td>56.5</td><td>42.2</td><td>64.9</td><td>53.5</td></tr><tr><td>Mistral-7B-base</td><td>25.7</td><td>31.1</td><td>28.4</td><td>25.9</td><td>44.4</td><td>35.2</td><td>26.9</td><td>19.9</td><td>23.4</td><td>24.7</td><td>20.4</td><td>22.6</td><td>25.8</td><td>29.0</td><td>27.4</td></tr><tr><td>Mistral-7B-SFT</td><td>21.0</td><td>48.5</td><td>34.7</td><td>17.2</td><td>71.5</td><td>44.3</td><td>17.6</td><td>46.5</td><td>32.1</td><td>21.7</td><td>66.5</td><td>44.1</td><td>19.3</td><td>58.3</td><td>38.8</td></tr><tr><td>Mistral-7B-SFT Conifer</td><td>29.9</td><td>49.5</td><td>39.7</td><td>30.5</td><td>67.0</td><td>48.7</td><td>26.5</td><td>40.0</td><td>33.2</td><td>31.1</td><td>63.0</td><td>471</td><td>29.5</td><td>54.9</td><td>42.2</td></tr><tr><td>Mistral-7B-SFTEvol-Instruct</td><td></td><td>41.5</td><td>41.6</td><td>370</td><td>63.5</td><td>50.4</td><td>35.4</td><td>35.0</td><td>35.2</td><td>39.4</td><td>54.0</td><td>467</td><td>38.4</td><td>48.5</td><td>43.5</td></tr><tr><td>Mistral-7B-SFT-VIF-RAG</td><td>51.2</td><td>56.5</td><td>53.8</td><td>45.9</td><td>70.5</td><td>58.2</td><td>44.9</td><td>43.0</td><td>44.0</td><td>47.8</td><td>58.0</td><td>52.9</td><td>47.4</td><td>57.0</td><td>52.2</td></tr><tr><td>Deita-7B-V1.0-SFT</td><td>31.4</td><td>31.5</td><td>31.4</td><td>29.0</td><td>42.5</td><td>35.8</td><td>26.5</td><td>30.5</td><td>28.5</td><td>26.3</td><td>40.0</td><td>33.2</td><td>28.3</td><td>36.1</td><td>32.2</td></tr><tr><td>Qwen1.5-7B-base</td><td>27.7</td><td>34.4</td><td>31.0</td><td>277</td><td>45.9</td><td>36.8</td><td>27.5</td><td>19.8</td><td>23.6</td><td>29.9</td><td>45.8</td><td>37.9</td><td>28.2</td><td>36.5</td><td>32.3</td></tr><tr><td>Qwen1.5-7B-SFT</td><td>16.1</td><td>50.5</td><td>33.3</td><td>14.3</td><td>70.0</td><td>42.2</td><td>14.8</td><td>400</td><td>27.4</td><td>13.7</td><td>59.0</td><td>36.3</td><td>14.7</td><td>54.9</td><td>34.8</td></tr><tr><td>Qwen1.5-7B-SFT-VIF-RAG</td><td>38.9</td><td>41.5</td><td>40.2</td><td>35.8</td><td>78.0</td><td>56.9</td><td>38.1</td><td>45.0</td><td>41.6</td><td>31.9</td><td>60.0</td><td>45.9</td><td>36.2</td><td>56.1</td><td>46.2</td></tr><tr><td>Qwen1.5-14B-base</td><td>33.7</td><td>38.1</td><td>35.9</td><td>32.5</td><td>54.7</td><td>43.6</td><td>32.4</td><td>26.5</td><td>29.5</td><td>33.0</td><td>48.3</td><td>40.7</td><td>32.0</td><td>41.9</td><td>36.9</td></tr><tr><td>Qwen1.5-14B-SFT</td><td>22.0</td><td>54.5</td><td>38.3</td><td>18.7</td><td>66.0</td><td>42.3</td><td>18.8</td><td>41.0</td><td>29.9</td><td>19.9</td><td>63.0</td><td>41.4</td><td>19.8</td><td>56.1</td><td>38.0</td></tr><tr><td>Qwen1.5-14B-SFT-VIF-RAG</td><td>42.1</td><td>53.0</td><td>47.6</td><td>40.1</td><td>71.0</td><td>55.5</td><td>38.8</td><td>39.5</td><td>39.2</td><td>35.7</td><td>69.0</td><td>52.3</td><td>39.2</td><td>58.1</td><td>48.6</td></tr></table></body></html>

Table 2: The cross validation on 4 general instruction-following (Left 4) and 4 foundational abilities (Right 4). Pr. and Ins. refe to the prompt level and instruction level metric, respectively. S or L denote the strict or loose metrics used in IFEval.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="4">Pr L) Ins (S)</td><td rowspan="2">Follo Aengh</td><td rowspan="2">MT-Bench</td><td rowspan="2">Arena-Hard</td><td rowspan="2">C-Eval</td><td rowspan="2">MMLU</td><td rowspan="2">GSM8k</td><td rowspan="2">HpmanEyal</td></tr><tr><td>Pr(S)</td><td></td><td></td><td>Ins. (L)</td></tr><tr><td>Llama3-8B-base</td><td>24.6</td><td>26.1</td><td>38.1</td><td>39.7</td><td>11.6</td><td>4.0</td><td>0.5</td><td>24.2</td><td>38.8</td><td>0.5</td><td>0.6</td></tr><tr><td>Llama3-8B-SFT</td><td>32.5</td><td>343</td><td>43</td><td>45.4</td><td>33.6</td><td>5.6</td><td>22</td><td>35.6</td><td>45.2</td><td>12.6</td><td>3.6</td></tr><tr><td>Llama3-8B-SFT-VIF-RAG</td><td>37.0</td><td>42.7</td><td>48.8</td><td>54.2</td><td>49.2</td><td>6.2</td><td>3.2</td><td>39.6</td><td>49.6</td><td>22.9</td><td>8.0</td></tr><tr><td>Mistral-7B-base</td><td>14.6</td><td>15.3</td><td>25.8</td><td>27.0</td><td>38.0</td><td>3.5</td><td>0.6</td><td>31.8</td><td>44.5</td><td>16.0</td><td>25.6</td></tr><tr><td>Mistral-7B-SFT</td><td>233</td><td>24.6</td><td>384</td><td>45.7</td><td>42.9</td><td>62</td><td>31</td><td>26.2</td><td>32.1</td><td>73</td><td>13.9</td></tr><tr><td>Mistral-7B-SFT-VIF-RAG</td><td>34.6</td><td>41.0</td><td>46.3</td><td>52.0</td><td>53.4</td><td>6.5</td><td>3.6</td><td>33.0</td><td>49.6</td><td>16.0</td><td>32.9</td></tr><tr><td>Qwen1.5-7B-base</td><td>25.1</td><td>27.9</td><td>37.8</td><td>40.6</td><td>38.7</td><td>5.4</td><td>3.2</td><td>72.8</td><td>58.3</td><td>50.6</td><td>36.0</td></tr><tr><td>Qwen1.5-7B-SFT</td><td>36.4</td><td>39.3</td><td>46.4</td><td>49.4</td><td>46.3</td><td></td><td>2.1</td><td>69.1</td><td>55.5</td><td>48.6</td><td>39.0</td></tr><tr><td>Qwen1.5-7B-SFT-VIF-RAG</td><td>42.3</td><td>46.0</td><td>53.5</td><td>57.1</td><td>51.1</td><td>6.1</td><td>3.9</td><td>75.6</td><td>61.2</td><td>61.4</td><td>44.5</td></tr><tr><td>Qwen1.5-14B-base</td><td>35.5</td><td>39.0</td><td>46.7</td><td>50.2</td><td>45.5</td><td>5.8</td><td>6.4</td><td>778</td><td>64.7</td><td>71.8</td><td>59.1</td></tr><tr><td>Qwen1.5-14B-SFT</td><td>38.4</td><td>41.7</td><td>49.4</td><td>52.6</td><td>49.8</td><td>6.0</td><td>6.5</td><td>76.2</td><td>62.0</td><td>715</td><td>58.5</td></tr><tr><td>Qwen1.5-14B-SFT-VIF-RAG</td><td>46.3</td><td>49.9</td><td>60.0</td><td>62.2</td><td>56.3</td><td>7.3</td><td>70</td><td>79.5</td><td>66.5</td><td>73.8</td><td>59.1</td></tr></table></body></html>

Baselines. We select Mistral-7B (Jiang et al. 2023), Llama3-8B (Meta 2024), Qwen1.5-7B, and Qwen1.5- 14B (Yang et al. 2024) as our backbone models, fine-tuning ShareGPT and four QA training sets as SFT version. Besides, we introduce several strong IF baselines, including Conifer (Sun et al. 2024), Evol-Instruct (Xu et al. 2023), and Deita (Liu et al. 2024). To ensure fairness, we add an equal-sized RAG training set to the original synthetic data.

# Main Result

Our primary findings are presented in Table 1. Overall, VIFRAG consistently surpasses all baselines in FollowRAG across multiple setups, highlighting the advantages of our method. Additionally, we have several key insights:

1) Existing IF baselines struggle in complex RAG scenarios. Comparisons between different base models and SFT versions in Tables 1 & 2 show that while SFT general data like ShareGPT improves performance on IFEval, it actually shows a performance decline in the IF aspect of

FollowRAG (e.g., NQ-IF: $2 5 . 7 \substack {  2 1 . 0 }$ in Mistral). Moreover, several strong IF baselines, such as Conifer, also perform poorly in FollowRAG’s IF aspect (HQ-IF: $2 6 . 9 \substack {  } 2 6 . 4 5$ ). This corroborates the issue highlighted in the introduction: traditional synthetic data may improve LLMs’ vanilla IF ability but often fails to generalize in RAG scenarios, sometimes even leading to decreased performance.

2) VIF-RAG shows exceptional IF alignment capability across various datasets, models, and parameter sizes. It consistently outperforms all baselines by over $10 \%$ on average accuracy, including a $44 \%$ improvement over Llama3-base, showcasing the significant performance advantage of our method. On four detailed QA benchmarks, VIF-RAG achieves the best results across all tested backbones. Moreover, whether using Qwen1.5-7B or Qwen1.5- 14B, our method maintains a stable and significant performance increase of over $10 \%$ . These results highlight that VIF-RAG is not only plug-and-play but also exhibits strong generalization capabilities.

3) The RAG capability is effectively preserved. Protecting RAG capability is a core focus of RAG systems. Compared to various SFT version baselines, our VIF-RAG significantly enhances IF capability while maintaining more stable RAG performance. This allows us to be optimistic about its potential in real-world RAG system applications.

Table 3: Ablation study on different designs of VIF-RAG.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">FollowRAG (NQ)</td><td colspan="2">IFEval</td></tr><tr><td>IF</td><td>RAG</td><td>Ins(L)</td><td>Prompt(L)</td></tr><tr><td>Mistral-7B-SFT-VIF-RAG</td><td>51.6</td><td>56.5</td><td>41.0</td><td>52.0</td></tr><tr><td>w/oMultipleConstraints</td><td>46.5 (-5.1)</td><td>52.3(-4.2)</td><td></td><td>37.9 (-3.1) 48.6 (-3.4)</td></tr><tr><td>w/oChainrule Constraints</td><td></td><td>49.2(-2.4) 53.3 (-3.2) 39.2(-1.8) 49.9(-2.1)</td><td></td><td></td></tr><tr><td>w/o Executor based Verification 43.5 (-8.1) 56.1(-0.4) 33.2(-7.8) 47.6(-4.4)</td><td></td><td></td><td></td><td></td></tr><tr><td>w/oConsistency Verification</td><td></td><td></td><td>47.6 (-4.0) 46.2 (-10.3) 38.4(-2.6) 46.5 (-5.5)</td><td></td></tr></table></body></html>

# Cross-Domain Validation

To explore the transferability of VIF-RAG, we conduct cross-domain validation on four natural instructionfollowing datasets and four foundational abilities benchmarks for LLMs in Tabel 2. Our findings are as follows:

1) Consistent IF alignment in both standard and RAG scenarios. Table 1 shows that VIF-RAG achieves remarkable IF alignment in RAG scenarios. In Table 2, comparing Llama3-8B SFT version, VIF-RAG demonstrates strong gains on two widely-used IF benchmarks, IFEval and FollowBench, with improvements of $8 . 8 \%$ (Ins.L) and $1 5 . 5 \%$ respectively. It also maintains stable improvement across different parameter sizes (7B & 14B). These results confirm that VIF-RAG consistently enhances IF alignment in both RAG and standard scenarios.

2) Robust General IF Transferability. To assess general IF alignment, we test VIF-RAG on challenging benchmarks Arena-Hard and MT-Bench. The results demonstrate that VIF-RAG maintains consistent alignment across various backbones ( $1 . 3 \%$ improvement on MT-Bench for the 14B model). This reveals significant potential for larger models to achieve better alignment of natural instruction.

3) Great Preservation of foundational Abilities. Previous research highlights that enhancing specific capabilities often compromises others (Dong et al. 2024b). As indicated in Table 2, VIF-RAG effectively preserves general capabilities (MMLU, C-Eval), math reasoning (GSM8K), and coding skills (HumanEval) across different setups, with some slight improvements. This preservation is largely attributed to the integration of ShareGPT data in the synthesis process, demonstrating VIF-RAG’s ability to balance diverse capabilities while maintaining broad applicability.

# Quantitative Analysis

Ablation Study. To examine the effects of various components in VIF-RAG, we conduct an ablation study in Table 3. The term ”w/o” indicates versions where specific components are removed. Our key observations are: (1) Removing any component from VIF-RAG results in decreased performance, indicating that all components, such as the complex instruction composition strategy and quality verification design, are crucial to its effectiveness. (2) The largest performance decline in FollowRAG is observed when executor verification is removed. This underscores the critical role of automated instruction-response validation in improving synthetic data quality and confirms the advantage of using LLMs to oversee instruction-following abilities through other core skills like coding. (3) The consistency verification proves beneficial in preserving RAG capabilities. It effectively filters out samples with high-level semantic conflicts between instructions and queries, reducing noise in IF tasks and maintaining RAG performance integrity.

![](images/9de06d6f6074a5540774bb95ea940934435f5ecacbe33581743668b85d51b310.jpg)  
Figure 4: The scaling analysis of retrieved document count.

Scaling Analysis. To explore the impact of retrieved document quantity on IF performance in RAG scenarios, we refer to Table 4. For the baseline models (SFT versions), IF capability declines as the number of passages increases. Specifically, performance drops sharply by over $6 \%$ when the document quantity in FollowRAG increases from 0 to 1. Further increasing the number to 10 leads to a significant performance decline, with Qwen-14B-SFT experiencing a drop of over $10 \%$ . This indicates that integrating knowledge through retrieval-augmented techniques challenges the IF abilities of existing models. In contrast, VIF-RAG shows a minor performance drop $( < 3 \% )$ when encountering the first document. As the number of documents increases to 10, VIF-RAG’s performance remains relatively stable.

# Conclusion

In this paper, we propose VIF-RAG, the first automated, scalable, and verifiable data synthesis pipeline for aligning complex instruction-following in RAG scenarios. VIF-RAG integrates a verification process at each step of data augmentation and combination. We begin by manually creating a minimal instruction set $_ { ( < 1 0 0 ) }$ and then apply steps including instruction composition, quality verification, instructionquery combination, and dual-stage verification to generate a large-scale, high-quality VIF-RAG-QA dataset $( > 1 0 0 \mathrm { K } )$ . To address gaps in IF evaluation for RAG systems, we present FollowRAG, featuring around 3K samples with 22 types of complex instruction constraints. Experiments show that VIF-RAG offers insights for optimizing IF alignment.