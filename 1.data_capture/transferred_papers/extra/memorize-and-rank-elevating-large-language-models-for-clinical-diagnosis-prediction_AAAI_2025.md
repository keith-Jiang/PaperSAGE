# Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction

Mingyu Derek Ma1, Xiaoxuan Wang1, Yijia Xiao1, Anthony Cuturrufo1, Vijay S Nori2, Eran Halperin1,2, Wei Wang1

1University of California, Los Angeles 2Optum AI ma, xw27, yijia.xiao, acc, weiwang @cs.ucla.edu, vijay.nori $@$ optum.com, eran.halperin@uhg.com

# Abstract

Clinical diagnosis prediction models, when provided with a patientâ€™s medical history, aim to detect potential diseases early, facilitating timely intervention and improving prognostic outcomes. However, the inherent scarcity of patient data and large disease candidate space often pose challenges in developing satisfactory models for this intricate task. The exploration of leveraging Large Language Models (LLMs) for encapsulating clinical decision processes has been limited. We introduce MERA, a clinical diagnosis prediction model that bridges pertaining natural language knowledge with medical practice. We apply hierarchical contrastive learning on a disease candidate ranking list to alleviate the large decision space issue. With concept memorization through fine-tuning, we bridge the natural language clinical knowledge with medical codes. Experimental results on MIMIC-III and IV datasets show that MERA achieves the state-of-the-art diagnosis prediction performance and dramatically elevates the diagnosis prediction capabilities of generative LMs.

# 1 Introduction

Electronic Health Records (EHR), containing patient status and diagnoses, embody valuable domain expertise and clinical operation patterns (Caufield et al. 2019). Clinicians make diagnosis judgments based on their extensive medical knowledge, acquired through years of education from textbooks and literature, as well as their accumulated experience derived from clinical practice. Clinical diagnosis prediction aims to predict patientsâ€™ diseases that are highly likely to be diagnosed in the upcoming hospital admission by analyzing the patientsâ€™ past diagnoses. The input and output are both presented in sequences of medical codes, which do not directly convey semantic information nor disease property. The resulting AI-enhanced diagnosis system (Morid, Sheng, and Dunbar 2023) may enable early warning of diseases (Rochefort, Buckeridge, and Forster 2015), optimized clinical resource allocation (Yadav et al. 2013), and better risk estimation for sustainable insurance (Hsu et al. 2016).

Two primary challenges in diagnosis prediction have driven various research efforts (Wornow et al. 2023b) but remain unsolved. First, what would be the best practice to incorporate clinical knowledge into the model? Existing works initialize concept embeddings from natural language descriptions (Wu et al. 2023b; Bornet et al. 2023), or enrich patient representation with external disease ontologies (An et al. 2023; Cheong et al. 2023). However, a significant gap persists between the primary knowledge modality, i.e. natural language, and the modelâ€™s hidden representation. Second, how can we handle the large candidate space when making predictions and exploit the supervisory signals induced from this candidate space? The commonly used International Classification of Diseases (ICD) coding system encodes $^ { 1 3 \mathbf { k } + }$ diseases (Cartwright 2013). Existing works typically treat the task as $k$ -way classification where $k$ is the number of possible diseases, and then apply cross entropy loss for each disease individually. These approaches often overlook the dependencies among diseases and the structural nuances within the diagnosis coding system.

Generative Language Models (LM), especially the Large Language Models (LLM), are trained to predict the next token, adhere to task instructions (Brown and et al. 2020; Ma et al. 2024a), and align with human preferences (Ouyang and et al 2022). These models exhibit superior capabilities in language understanding and reasoning, as shown by their performance on science-based benchmarks (Ma et al. 2024b; Wu et al. $2 0 2 3 \mathrm { a }$ ; Zhang et al. 2024). During the pretraining stage, LLMs assimilate a large amount of knowledge extracted from literature and online corpora. However, there remains an underexplored domain in using LLM for clinical diagnosis prediction, due to the aforementioned gap between natural language and medical code, as well as the disparity between the token-level optimization process and the large candidate outcome space. These challenges impede the effective application of generative LMs to diagnosis prediction tasks, even as the state-of-the-art models predominantly rely on graph neural networks without fully harnessing natural language knowledge (Yang et al. 2023b; Wu et al. 2023b; An et al. 2023). Fine-tuning generative LM LLaMA2 (Touvron and et al. 2023) directly on diagnosis prediction yields almost 20-point lower recall $@ 2 0$ than GNN-based existing best model (Yang et al. 2023b) as shown in Table 1. There are some studies that use transformer-based LM for clinical outcome prediction, but they either do not support structured data as input (Niu et al. 2024; Wang et al. 2023a), not compatible with mainstream LLMs (Rupp, Peter, and Pattipaka 2023; Guo et al. 2023), or only work for narrow output space with few classes (Wang et al. 2023a; Shoham and Rappoport 2023).

To tackle these challenges, we propose MERA, an LLM designed for clinical diagnosis prediction that incorporates a comprehensive understanding of clinical knowledge by leveraging relationships among medical codes and offers extensive supervision over the output space. The patientâ€™s historical diagnosis results are formulated as linear sequences and the LLM is tasked with generating a probability distribution for the diagnosis results in the subsequent visit. Compared with the ordinary paradigm that optimizes the probability of generating the correct token, we optimize the outcome directly. To enhance the inter-visit causal reasoning, we employ contrastive learning to compel the model to distinguish true diagnoses from false ones. The contrastive learning process is extended to multiple levels in the hierarchical organization of the medical codes within the ICD coding ontology. The model is learned to distinguish the true diagnoses from a pool of potential diagnoses while the pool is increasingly relevant to the true ones. To regularize the diagnosis predictions to follow intra-visit diagnosis patterns, we develop a teaching-forcing strategy to optimize the medical code ranking, assuming partial diagnoses of the visit are known. To allow the model to grasp the comprehensive clinical semantics and diagnosis property of each medical code, we fine-tune the LM to â€œmemorizeâ€ the mapping between medical codes and their natural language definitions. Consequently, this process bridges the gap between raw codes and their contextual medical meanings and equips the LM to capture the intricate code dependencies that are crucial for precise diagnosis assessments.

We validate the effectiveness of MERA in general diagnosis and heart failure prediction tasks on the patient records in MIMIC-III (Johnson et al. 2016) and MIMIC-IV (Johnson et al. 2023) datasets. MERA yields significant improvements over the existing state-of-the-art models across tasks on all datasets while having almost perfect memorization of bidirectional medical code-definition mapping. An extensive analysis of leading LLMâ€™s medical code understanding and diagnosis prediction capabilities is conducted, and we observe that GPT-4 is still far behind fine-tuned models on both tasks. We further conduct ablation studies to validate the effectiveness of the proposed novel design choices.

# 2 Preliminaries

# 2.1 Task Formulations

MERA can be applied for any task whose output is a collection of candidates belonging to a pre-defined decision space. We introduce widely used diagnosis prediction settings as typical testbeds for MERA (Yang et al. 2023b).

Tasks. The first task is a general diagnosis prediction task, in which we aim to predict the diagnoses for the patientâ€™s potential next visit $V _ { T + 1 }$ given patientâ€™s history diagnoses by selecting a set of  +medical codes from the medical code ontology $O$ , which can be formally described as $f _ { D P } ~ : ~ \{ V _ { 1 } , V _ { 2 } , \ldots , V _ { T } \} ~  ~ V _ { T + 1 }$ . The second task is a diseaâˆ¶se-{specific heart f} lâ†’ure p r+ediction task, which can be described as a binary classification function $f _ { H F }$ $\{ V _ { 1 } , V _ { 2 } , \ldots , V _ { T } \} ~  ~ 0 , 1$ . We are more focused and aim t{o predict whet h} â†’a patient would encounter heart failure (ICD-9 codes with head 428) in any of the future visits.

Input patient record. Given an EHR collection of $n$ patients $\{ P _ { 1 } , P _ { 2 } , \dots , P _ { n } \}$ , patient historical diagnosis can be represe{nted as a sequen}ce of admissions in chronological order $P = \{ V _ { 1 } ^ { P } , V _ { 2 } ^ { P } , . . . , V _ { T } ^ { P } \}$ where $T$ is the number of existing v=isi{ts. For a particular} visit $V$ , the medical judgment made by clinicians as a result of the visit is an unordered set of diagnoses $V = \{ d _ { 1 } ^ { V } , d _ { 2 } ^ { V } , \dots \colon d _ { | V | } ^ { V } \}$ in the format of $| V |$ unique medical code $( d \in O )$ . The tâˆ£asâˆ£k input has two variants, including 1) history diagnosis codes only, and 2) additionally providing patient profile (gender, race, medication and family history) as a natural language sentence.

Medical code ontology as the decision space. The International Classification of Diseases (ICD) (Cuadrado 2019) provides a comprehensive ontology $O$ diseases, symptoms and diagnoses. Each leaf node represents a unique disease/diagnosis and is assigned a unique medical code $c \in$ $\{ c _ { 1 } , c _ { 2 } , \ldots , c _ { | O | } \}$ where $| O |$ is the total number of cod âˆˆ. {Diseases are âˆ£orâˆ£g}anized nâˆ£to disease groups at multiple levels, represented by non-leaf nodes forming a tree hierarchy $G = \left\{ G _ { \mathrm { l e v e l = 0 } } , G _ { \mathrm { l e v e l = 1 } } , \dots , G _ { \mathrm { l e v e l = } d e p t h ( O ) } \right\}$ . Assuming the root=o{f $O$ is =at level 0=, at level $j > 0$ , th(ere)}are $\scriptstyle \left| G _ { \mathrm { l e v e l } = j } \right|$ disjoint disease groups, i.e. $G _ { \mathrm { l e v e l } = j } = \{ g _ { 1 } , \dots , g _ { | G _ { \mathrm { l e v e l } = j } | } \}$ .=Tâˆ£here is also a one-to-one mapping =bet=w{een a codâˆ£e $c$ a=nâˆ£d}its natural language definition $d e f _ { c }$ . For example, in version 9 of ICD, the medical code $2 5 0 . 2 3 \$ stands for â€œDiabetes with hyperosmolarity, type I [juvenile type], uncontrolledâ€. It belongs to the first-level group for all â€œEndocrine, Nutritional, and Metabolic Diseases and Immunity Disordersâ€, and further belongs to the fine-grained disease group â€œtype I uncontrolled diabetesâ€. We use both ICD-9 and ICD-10 coding systems with $^ { 1 3 \mathbf { k } + }$ and $6 8 \mathbf { k } +$ unique codes in this work.

# 2.2 Existing Paradigm of Generative LMs

The ordinary formulation of generative LMs takes the input sequence $s e \dot { q } _ { i n } = t _ { 1 } ^ { i n } , \dots , t _ { | s e q _ { i n } | } ^ { i n }$ tisneq and is expected to generate the ground-truth output $s e q _ { o u t } = t _ { 1 } ^ { o u t } , \dots , t _ { | s e q _ { o u t } | } ^ { o u t }$ tosuetqout . It produces a probability distribution= $P \left( c | t _ { 1 : | s e q _ { i n } } ^ { i n } | , \hat { t } _ { 1 : k } ^ { o u t } \right)$ over the possible next token $( c \in V )$ conditioned on both the input sequence and $k$ generated tokens. Discrete tokens at each autoregressive decoding step are produced by Equation 1. The LM is optimized to minimize the cross-entropy loss shown in Equation 2 applied on the probability of the gold next token conditioned on the gold output tokens in the previous segment in a teacher-forcing manner, assuming the $| s e q _ { o u t } | -$ th token marks the end of the decoding.

$$
\begin{array} { r } { \hat { t } _ { k + 1 } ^ { o u t } = \operatorname * { a r g m a x } _ { c \in V } P \left( c \mid t _ { 1 : | s e q _ { i n } | } ^ { i n } , \hat { t } _ { 1 : k } ^ { o u t } \right) } \end{array}
$$

$$
\mathcal { L } _ { C E } = \sum _ { k = 0 } ^ { \lvert s e q _ { o u t } \rvert } - \log P \left( t _ { k + 1 } ^ { o u t } \mid t _ { 1 : \lvert s e q _ { i n } \rvert } ^ { i n } , t _ { 1 : k } ^ { o u t } \right)
$$

# 3 MERA: Learning to Memorize and Rank

MERA builds upon a large language model $L M$ after pretraining on a natural language corpus, instruction tuning, and potential alignment process. MERA is designed to be compatible with numerous generative LM architectures and inherit knowledge obtained through pre-training, including encoder-decoder LM and decoder-only LM. There are three steps involved as a pipeline: 1) Fine-tuning the model to memorize medical codes used to represent the diagnoses; 2) Further optimizing the model to learn inter-visit causal and temporal relations between patient visits as well as intravisit patterns from patient history records; 3) During inference, performing autoregressive generation to produce diagnosis predictions given an unseen patient history input.

# 3.1 Medical Code Memorization

State-of-the-art LLMs struggle to associate medical codes with their correct definitions accurately. GPT-4 can only recall $45 \%$ of ICD-9 codes given corresponding definitions (row 3 of Table 2), which may be attributable to the absence of medical codes in the pre-training dataset. MERA explicitly teaches $L M$ the semantic information associated with the medical codes and the relationships within the coding system. We consider all codes in $O$ as special tokens, each unique medical code has a dedicated token embedding and can be represented by a single token. This design reduces the noise of the learning objectives as the diagnosis probability is equivalent to the token probability. The memorization process parameterizes embeddings of the special tokens and further equips the $L M$ with the necessary external knowledge to facilitate downstream diagnosis prediction. To integrate information about medical codes in $O$ and the natural language knowledge contained in $L M$ , we fine-tune $L M$ on synthetic question-answering pairs.

Bidirectional code and definition memorization. For each code $c$ and the natural language definition $d e f _ { c }$ , we create two input-output pairs. The first pair includes â€œWhat is the definition of ICD-9 code $\boldsymbol { c } ^ { \flat }$ as $s e q _ { i n }$ and the target answer â€œ $\cdot d e f _ { c } ^ { \ , , , }$ as $s e q _ { o u t }$ to train the model to recall its definition given a code. The second pair helps the model memorize the inverse mapping. The question-answer pairs are created according to the $O$ ontology being for the downstream task.

Decision space structure memorization. We further embed code dependencies collectively in $L M$ by training with separate code-category instances. The curated pairs connect a code to its disease groups at various levels $1 , \ldots , d e p t h ( O )$ in the code ontology $O$ . For example, $s e q _ { i n }$ is â€œWh(at i)s the chapter level disease group of the ICD9 code 998.51?â€, and $s e q _ { o u t }$ is â€œInjury and Poisoningâ€.

# 3.2 Seq2seq Data Construction

The second phase aims to equip $L M$ with a temporal and causal understanding of the diagnoses across multiple visits. We train the $L M$ with a collection of sequence-tosequence training instances $\mathbb { X } \ = \ \left. \mathbf { X } _ { 1 } , \ldots , \mathbf { X } _ { n _ { \mathrm { p a t i c n t } } } \right.$ based on $n _ { \mathrm { p a t i e n t } }$ patient records, where ${ \bf { \dot { X } } } _ { i }$ is a set of (diagnosis history, future diagnosis) pairs created based on patient record $P _ { i }$ . Given the history of a patient containing $T$ visits $P _ { i } ~ = ~ \{ V _ { 1 } ^ { P _ { i } } , \ldots , V _ { T } ^ { P _ { i } } \}$ , we extract $T \gets 1$ pairs of patient  s= r{y and the e }ected diagnose iâˆ’n the next visit to have maximum utilization of the patient records. For each pair, an input sequence is verbalized from 1-to- $k$ visits following $s e q _ { i n } \ = \ i n s t r u c t i o n$ , $v b ( V _ { 1 } ^ { P _ { i } } ) , \dots , v b ( V _ { k } ^ { P _ { i } } )$ $( k \in [ 1 , T - 1 ] )$ . A=dditional patient p(rofi )sentence(s  n) be iâˆˆns[erted âˆ’foll]owing the instructions. A ground-truth output sequence is converted from expected diagnoses in the $\left( k + 1 \right)$ -th visit following $s e q _ { o u t } = v b ( V _ { k + 1 } ^ { P _ { i } } )$ . The verbalizer f(un+ctio)n $v b$ concatenates the d a=gno(sis +co)des within each visit to form a token segment for a specific visit and further prepend the starting prompt phrase (â€œThe diagnosis codes for this visit are: â€) and append a special token EOV representing â€œthe end of the visitâ€.

Diagnoses order perturbation. The order of patient visits is crucial to convey the dependent relations as a diagnosis in a later visit is conditioned on the previous diagnoses. However, the order of diagnosis codes within a particular visit does not carry cognitive rationale as indicated by EHR dataset documentation and papers (Johnson et al. 2023). An ideal model should preserve the inter-visit orders while ignoring the intra-visit orders. To achieve this goal with a sequential LM, we propose to create $n _ { \mathrm { p e r t u r b } }$ variants tsieoqnuse.nEcaecshrevsaprieacntitvkeelye,plsetahdeinsga tmoe $n _ { \mathrm { p e r t u r b } } ^ { 2 }$ dievrebrsuet rcaonmdboimnlayshuffles the diagnosis codes within each visit. By observing the data instances with shuffled orders and the same target distribution, we teach the LM to ignore the order of diagnosis codes with a model-agnostic design. To summarize, the training sequence-to-sequence data $\mathbb { X }$ contains data instances $\mathbf { X }$ generated according to $n _ { \mathrm { p a t i e n t } }$ patient history records. X contains $T - 1$ groups of data instances with different patient history leâˆ’ngths, each group contains combinations among $n _ { \mathrm { p e r t u r b } }$ perturbed input sequences and $n$ perturb perturbed output sequences.

# 3.3 Learning Inter-visit Reasoning

Up to this point, the created seq2seq data instances can be used to conduct supervised fine-tuning of $L M$ following token-level optimization used in conventional generative LM reiterated in $\ S 2 . 2$ . However, as we analyze theoretically (in $\ S 1$ ) and demonstrate empirically (line 14/15 of Table 1), vanilla generative LM does not handle the diagnosis prediction task well. We propose multiple specialized learning objectives to learn the inter-visit reasoning to infer upcoming diagnoses and capture intra-visit diagnosis patterns. We bridge the sequential modeling capabilities and LMâ€™s internal knowledge with the task property and decision space structure (e.g. , ICD hierarchy) for diagnosis prediction.

After encoding $s e q _ { i n }$ containing information on existing hospital visits, the $L M$ starts to generate its prediction of the upcoming visit $s e \hat { q } _ { o u t }$ . As an immediate step, it produces a probability distribution over the possible next token $t _ { 1 } ^ { o u t }$ conditioned on $s e q _ { i n }$ , reflecting the possibility of different tokens in the vocabulary as one of the diagnoses for visit $V _ { T + 1 }$ . Legit candidate tokens for $t _ { 1 } ^ { o u t }$ are the special code

Diagnosis codes Probabilities Contrastive Learning Disease groups in for past visits over candidate ICD-9 ontology diagnosis codes Injury and Dynamic Large LangMuoadgel P2ositPioviesoNn1eign3agti4ve t2hr>esholðŸ¥³d   
038.9 584.9 512.0   
<EOV> Next visit: 424.1 414.01 â€¦ W 2 6 SubInefnadrcotciaorndial 1 > EOV â˜¹ 1 Positive Negative   
In-visit order perturbation 996.74 625.11  EOV 410.71 6 5 7 6 < EOV â˜¹ Diagnoses in Not in next visit next visit

tokens, including $\{ c _ { 1 } , c _ { 2 } , . . . , c _ { | O | } \}$ . We select the probabilities of all code tokens and theâˆ£n apply softmax, resulting in the probability distribution over the candidate codes

$$
P \left( c \mid t _ { 1 : \mid s e q _ { i n } \mid } ^ { i n } \right) = \{ p _ { c _ { 1 } } , p _ { c _ { 2 } } , . . . , p _ { c _ { \mid O \mid } } \} , c \in O .
$$

Hierarchical contrastive learning. We design training objectives to identify the real diagnoses among a group of similar candidate diagnoses. With such a design, the model is forced to understand the subtle differences among neighbor diseases in $O$ and learn to infer upcoming diagnoses from a candidate pool under the same disease group.

For a training instance $\mathbf { X } _ { i }$ , we first identify all disease groups that the diagnoses of the next visit belong to $G _ { { \bf X } _ { i } } = \left\{ G _ { \mathrm { l e v e l } = 0 } , G _ { \mathrm { l e v e l } = 1 } , \dots , G _ { \mathrm { l e v e l } = d e p t h ( O ) } \right\}$ . Then, for each g o=up{ $g _ { k }$ at= level $j$ $( g _ { k } \in G _ { \mathrm { l e v e l } = j } )$ , w(e i)d}entify positive diagnosis codes gpo $g _ { k } ^ { p o s } \ = \ \{ c _ { 1 } ^ { p o s } , \ldots , c _ { | g _ { k } ^ { p o s } | } ^ { p o s } \}$ cpgopsos , which are the diseases in $g _ { k }$ that are diagnosed in tâˆ£he nâˆ£ext visit. We then use all remaining diseases in $g _ { k }$ as negative codes $g _ { k } ^ { n e g } = g _ { k } - g _ { k } ^ { p o s } = \big \{ c _ { 1 } ^ { n \tilde { e } g } , \dotsc , c _ { | g _ { k } ^ { n e g } | } ^ { n e g } \big \}$ . Then, we calculate an InfoNCE loss (Oord, Li, and Vinyals 2018; Ma et al. 2021; Meng et al. 2021) term for each group in $G _ { \mathbf { X } _ { i } }$ and aggregate all the terms to be the aggregated objective $\mathcal { L } _ { C L }$ .

$$
\begin{array} { r } { \mathcal { L } _ { C L } ^ { g _ { k } } = - \log \frac { \sum _ { c _ { m } ^ { p o s } \in g _ { k } ^ { p o s } } P \left( c _ { m } ^ { p o s } \mid t _ { 1 : | s e q _ { i n } | } ^ { i n } \right) } { \sum _ { c _ { m } \in g _ { k } } P \left( c _ { m } \mid t _ { 1 : | s e q _ { i n } | } ^ { i n } \right) } } \\ { \mathcal { L } _ { C L } = \frac { 1 } { | \mathbb { X } | } \displaystyle \sum _ { { \bf X } _ { i } \in \mathbb { X } } \sum _ { { \bf G } _ { \mathrm { l e v e l } = j } \in G _ { { \bf X } _ { i } } } \sum _ { g _ { k } \in G _ { \mathrm { l e v e l } = j } } \mathcal { L } _ { C L } ^ { g _ { k } } } \end{array}
$$

The loss term for higher-level groups (where $j$ is smaller) is used to enable the model to recognize disease scopes across a broad spectrum. Optimizing the high-level loss mimics the clinicianâ€™s training process of making differential diagnoses, the â€œrough guessesâ€ of possible diseases. Loss terms for lower-level groups focus on nuanced comparisons among diseases within the same family, increasing the modelâ€™s ability to distinguish rare diseases. The proposed contrastive learning approach is efficient and capable in comparison to in-batch contrastive learning for two reasons: 1) The loss is calculated on the token probability distribution, essential for the typical decoding of generative LM, with no need for additional architecture or forward/backward passes. This ensures efficiency and maximum compatibility with the pre-trained LM. 2) The contradiction for loss calculation pertains to token probabilities, allowing the integration of prediction confidence for each disease into the optimization. This design differs significantly from in-batch contrastive learning, where for35ward and backwFianre-d apinaesd ses must be run for multiple data instances, and batch size significantly limits the size of positive and/or negative samples.

Dynamic confidence threshold. To produce a short list of confident diagnoses among the full ranking of all diagnosis codes, we learn a dynamic confidence threshold to select the most likely predictions. Existing works apply a fixed threshold to the probability distribution, which is often determined as a hyperparameter observed through the performance of the validation set (Morid, Sheng, and Dunbar 2023; Rasmy et al. 2021). This widely used strategy makes shortlisting less flexible, and the model tends to play it safe and produces more diagnoses than it should. To model the confidence threshold dynamically, we use a special token EOV to mark the confidence threshold within the token probability ranking list. EOV was appended at the end of the diagnosis sequence of each visit as introduced in $\ S 3 . 2$ .

The model $L M$ learns the placement of the EOV in two ways. Implicitly, the visit segments in the input sequence demonstrate that the special token EOV represents the end of a visit segment, implying the model should stop generating more diagnosis codes. Training with EOV-ended visit sequence segment, $L M$ naturally learns to assign EOV a higher probability than other code tokens when the model is not confident to make more diagnoses and chooses to generate EOV to end the diagnosis sequence of a particular visit. Explicitly, we design a learning objective to train the $L M$ to place the EOV token at the proper rank of the token probability distribution P c ti1nseqin . We identify the positive medical codes that do appear in the target visit as $O ^ { p o s }$ and the ones not included as $O ^ { n e g }$ $O ^ { p o s } + O ^ { n e g } = O )$ . The $\mathcal { L } _ { D C E }$ is essentially a dynamic cross-entr+opy loss= that regularizes the probability of each positive code to be not smaller than the probability of $\tt E O V$ and further make sure the probability of each negative code is not larger than $P \left( \mathrm { E O V } \mid t _ { 1 : | s e q _ { i n } | } ^ { i n } \right) .$ The optimization of the dynamic confidence threshold applies fine-grained supervision to the probability distribution, enabling effective and efficient diagnosis capability learning with sparse patient data.

$$
\begin{array} { r } { \mathcal { L } _ { D C E } = \displaystyle \sum _ { c \in O ^ { p o s } } \log \left( R e L U ( P \left( \operatorname { E O V } \big | t _ { 1 : | s e q _ { i n } | } ^ { i n } \right) - P \left( c \big | t _ { 1 : | s e q _ { i n } | } ^ { i n } \right) ) \right) } \\ { + \displaystyle \sum _ { c \in O ^ { n e g } } \log \left( R e L U ( P \left( c \big | t _ { 1 : | s e q _ { i n } | } ^ { i n } \right) - P \left( \operatorname { E O V } \big | t _ { 1 : | s e q _ { i n } | } ^ { i n } \right) ) \right) } \end{array}
$$

# 3.4 Learning Intra-visit Diagnosis Patterns

Besides training the model to reason between visits, there are many implicit rules and latent dependencies buried in the large pool of diagnoses within each visit. For example, within a group of similar diseases, the clinicians normally only choose the most representative code for the patientâ€™s status; some diseases might suppress or correlate with other diagnoses. Modeling the intra-visit dependencies enables us to incorporate real-life clinic operation patterns into realistic diagnosis predictions. The prediction made for a specific visit should consider other diagnoses of the same visit.

To model the intra-visit dependencies, we apply the objectives over the token probability distribution introduced in $\ S 3 . 3$ to multiple training instance variants with partial output sequences as conditions. This enables teacher-forcing training. For each $( s e q _ { i n } , s e q _ { o u t } )$ pair in $\mathbf { X } _ { i }$ for patient record $P _ { i }$ where the $s e q _ { o u t }$ expresses all diagnoses in the visit $V _ { k + 1 } ^ { P _ { i } } , k \in \left[ 1 , T - 1 \right]$ , we create $| V _ { k + 1 } ^ { P _ { i } } |$ variants to move partial+diagâˆˆno[sis resâˆ’ult]s in $s e q _ { o u t }$ âˆ£o be+ pâˆ£art of the input of $L M$ together with $s e q _ { i n }$ . Given the new input including the patient history and $m$ known diagnoses in the upcoming visit, $L M$ produces probability over the candidate medical code $P \left( c | t _ { 1 : | s e q _ { i n } } ^ { i n } | , t _ { 1 : m } ^ { o u t } \right)$ . Since the $m$ known diagnoses have been part of the input sequence, we remove the corresponding medical codes from the positive code set for the calculation of $\mathcal { L } _ { D C E }$ and $\mathcal { L } _ { C L }$ to prevent the model from generating duplicatLed codes.  FLormally, the conditions for probability $P$ in Equation 3, 4, and 6 are $t _ { 1 : | s e q _ { i n } | } ^ { i n } , t _ { 1 : m } ^ { o u t }$ instead of $t _ { 1 : | s e q _ { i n } | }$ The $m$ known diagnoses in $V _ { k + 1 } ^ { P _ { i } }$ areâˆ¶ removed from $g _ { k } ^ { p o s }$ Opos and added to gneg and $O ^ { n e g }$ .

# 3.5 Training and Inference Pipeline

Training objectives. For code memorization, $L M$ is trained with the ordinary cross-entropy loss in Equation 2. The hierarchical contrastive learning loss (Equation 5) is additionally applied to the instances whose output is a medical code. For the diagnosis prediction task, the $L M$ fine-tuned from the memorization task is further optimized with the hierarchical contrastive learning loss (Equation 5) and the dynamic cross-entropy loss (Equation 6) on $| V _ { k + 1 } ^ { P _ { i } } |$ teaching force variants. Unlike language modeling, n âˆ£o lo+ss has been applied to the reconstruction of the input segment for both fine-tuning stages. We perform full-parameter fine-tuning.

Autoregressive decoding. The produced $L M$ can be used for inference on unseen patient history. Given $s e q _ { i n }$ , $L M$ performs autoregressive decoding to output discrete diagnosis code with the highest probability in the ranking list for each output step until the EOV token is generated.

# 4 Experiments

# 4.1 Experimental Setup

Datasets. We use MIMIC-III (Johnson et al. 2016) and MIMIC-IV (Johnson et al. 2023) EHR datasets containing patient records to train and evaluate. The MIMIC-III dataset focuses on patients eventually admitted to the ICU, while the MIMIC-IV dataset includes both ICU patients and other patients. We conduct data preprocessing following previous works (Lu, Han, and Ning 2022) and split the train/dev/test sets by patients to avoid information leak.

Metrics. We report the weighted F1 and recall ${ \ @ k }$ , where $k$ is the number of top-ranked predictions, and AUC and F1 for diagnosis prediction and heart failure, respectively.

Baselines. RNN/CNN and attention-based models: RETAIN (Choi et al. 2016), Dipole (Ma et al. 2017), Timeline (Bai et al. 2018), HiTANet (Luo et al. 2020), and Deepr (Nguyen et al. 2017). Graph-based models: GRAM (Choi et al. 2017), G-BERT (Shang et al. 2019), CGL (Lu et al. 2021), Chet (Lu, Han, and Ning 2022), and MCDP (Li and Gao 2022). KGxDP (Yang et al. 2023b) formulates each patient as a personalized medical KG, combining medical KGs with patient admission history. Note that additional medical notes are used by CGL, and additional Unified Medical Language System resource (Bodenreider 2004) is used as external knowledge by KGxDP. Transformer-based models: We adapt two encoder-only LM. RoBERTa (Liu et al. 2019) with $1 2 5 \mathbf { M }$ and MedBERT (Rasmy et al. 2021) with 109M parameters and append a $| O |$ -way classification head. We choose MedBERT among âˆ£othâˆ£er similar encoder-only architectures for medical sequence (Pang et al. 2021; Li et al. 2023; Rupp, Peter, and Pattipaka 2023) because other models require additional input information such as lab test results which is not available under our setting. Seq2seq uses ordinary generative LMâ€™s formulation introduced in $\ S 2 . 2$ to fine-tune a LM to generate diagnosis codes as output. We include definition sentences in the prompt following each code, so these baselines are exposed to the same external knowledge used by MERA.

Base LMs. We use BioMistral (Labrak et al. 2024) trained on PubMed Central, LLaMA2 (Touvron and et al. 2023), GPT-2 (Radford et al. 2019), T5 (Raffel et al. 2023) and Flan-T5 (Chung and et al. 2022) as the base LMs.

# 4.2 Performance of Diagnosis Prediction

We show the performance comparison on the diagnosis prediction and heart failure prediction tasks (described in $\ S 2 . 1 \ r _ { . }$ ) using ICD-9 as decision space with history diagnosis code as input in Table 1 and the influence of base pre-trained LM selection in Table 2. We further show that MERA can be generalized to richer input with natural language patient profile, and the larger ICD-10 decision space in Table 3.

Encoder-only & vanilla generative LM perform poorly. The encoder-only LMs exhibit limited performance (rows 12-13 of Table 1), possibly because they do not account for the specialized modeling of intra-visit order and the extensive output space. When employing a vanilla generative LM (rows 14-15), the performance is further diminished. This is attributed to sparse supervision distributed in token-level loss. For each pass, only the probability of the single groundtruth token is optimized following Equation 2, while MERA optimizes the probabilities of all candidate diagnoses.

Gap between zero-shot and fine-tuned LMs. There remains a 20-point deficit in recall $@ 2 0$ comparing the best

Table 1: Diagnosis prediction comparison with baselines using ICD-9 as the decision space with code-only input $( \% )$   

<html><body><table><tr><td rowspan="2"># Model</td><td colspan="5">Diagnosis Prediction</td><td colspan="2"></td><td colspan="3">Heart Failure</td></tr><tr><td colspan="3">MIMIC-III</td><td colspan="3">MIMIC-IV</td><td colspan="2">MIMIC-III</td><td colspan="2">MIMIC-IV</td></tr><tr><td></td><td>w-F1</td><td>R@10</td><td>R@20</td><td>w-F1</td><td>R@10</td><td>R@20</td><td>AUC</td><td>F1</td><td>AUC</td><td>F1</td></tr><tr><td colspan="9">RNN/CNN and attention-based models</td></tr><tr><td>1 Deepr</td><td></td><td>18.87</td><td>24.74</td><td>33.47</td><td>24.08 26.29</td><td>33.93</td><td>81.36</td><td>69.54</td><td>88.43</td><td>61.36</td></tr><tr><td>2 Dipole</td><td></td><td>19.35</td><td>24.98</td><td>34.02 23.69</td><td>27.38</td><td>35.48</td><td>82.08</td><td>70.35</td><td>88.69</td><td>66.22</td></tr><tr><td>3</td><td>Timeline</td><td>20.46</td><td>25.75</td><td>34.83</td><td>25.26 29.00</td><td>37.13</td><td>82.34</td><td>71.03</td><td>87.53</td><td>66.07</td></tr><tr><td>4 RETAIN 5 HiTANet</td><td></td><td>20.69 21.15</td><td>26.13 26.02</td><td>35.08 35.97 24.92</td><td>24.71 28.02 27.45</td><td>34.46 36.37</td><td>83.21 82.77</td><td>71.32 71.93</td><td>89.02 88.10</td><td>67.38 68.21</td></tr><tr><td colspan="9"></td></tr><tr><td></td><td></td><td>19.88</td><td>25.86</td><td>35.31</td><td>Graph-based models 24.49</td><td></td><td></td><td>71.18</td><td></td><td></td></tr><tr><td>6 G-BERT GRAM</td><td></td><td>21.52</td><td>26.51</td><td>35.80 23.50</td><td>27.16 27.29</td><td>35.86 36.36</td><td>81.50 83.55</td><td>71.78</td><td>87.26 89.61</td><td>68.04 68.94</td></tr><tr><td>7 8 CGL</td><td></td><td>21.92</td><td>26.64</td><td>36.72</td><td>25.41 28.52</td><td>37.15</td><td>84.19</td><td>71.77</td><td>89.05</td><td>69.36</td></tr><tr><td>9</td><td>MCDP</td><td></td><td>28.30</td><td>39.60</td><td></td><td>25.80 36.10</td><td></td><td></td><td></td><td></td></tr><tr><td>10 Chet</td><td></td><td>22.63</td><td>28.64</td><td>37.87</td><td>26.35 30.28</td><td>38.69</td><td>86.14</td><td>73.08</td><td>90.83</td><td>71.14</td></tr><tr><td>11 KGxDP</td><td></td><td>27.35</td><td>30.98</td><td>41.29</td><td>30.38 34.19</td><td>43.47</td><td>86.57</td><td>74.74</td><td>95.66</td><td>79.87</td></tr><tr><td colspan="9">Transformer-based models</td></tr><tr><td>12</td><td>RoBERTa</td><td>17.39</td><td>22.84</td><td>32.07</td><td>22.54</td><td>24.89</td><td>32.38</td><td>79.74 68.28</td><td>87.03</td><td>60.21</td></tr><tr><td>13</td><td>MedBERT</td><td>19.01</td><td>23.68</td><td>34.39</td><td>24.13 25.88</td><td>33.81</td><td>81.06</td><td>69.96</td><td>88.73</td><td>61.81</td></tr><tr><td>14</td><td>Seq2seq (LLaMA2-7B)</td><td>18.05</td><td>18.38</td><td>23.56</td><td>20.47 20.77</td><td>24.19</td><td>77.62</td><td>66.06</td><td>85.98</td><td>59.14</td></tr><tr><td>15</td><td>Seq2seq (BioMistral-7B)</td><td>19.14</td><td>19.83</td><td>24.97</td><td>22.11</td><td>22.03</td><td>26.24</td><td>78.57 67.87</td><td>87.04</td><td>61.07</td></tr><tr><td>16</td><td>MERA (LLaMA2-7B)</td><td>32.77</td><td>35.94</td><td>47.48</td><td>34.64</td><td>38.16</td><td>46.94 89.49</td><td>77.21</td><td>97.26</td><td>82.31</td></tr><tr><td>17</td><td>MERA (BioMistral-7B)</td><td>33.24</td><td>36.73</td><td>49.01</td><td>36.16</td><td>39.57</td><td>49.09 90.78</td><td>79.13</td><td>98.74</td><td>84.03</td></tr></table></body></html>

zero-shot LLM (row 3 of Table 2) to the fine-tuned model.   
This underscores the importance of leveraging patient data.

MERA is the state-of-the-art diagnosis prediction model. Finally, MERA achieves significantly better performance in both diagnosis and heart failure prediction tasks on both MIMIC datasets. MERA exhibits a 5.89 point higher weighted F1 score and almost 8 points higher recall $@ 2 0$ for MIMIC-III compared to the existing best model (row 17 vs 11 of Table 1). In Table 2, we showcase the diagnosis prediction performance using different pre-trained LMs, noting that even MERA with GPT-2 large (row 10) achieves comparable performance with the existing best KGxDP.

# 4.3 Performance on Medical Code Memorization

Table 2 shows the evaluation of the memorization results for the ICD-9 medical code system while using various base LMs. We report code and definition accuracy, indicating the proportion of correct output full ICD codes/definitions given their definitions/ICD codes as input by exact match. We observed that 1) Almost perfect medical code recall using large-enough 7B LM. 2) Pre-trained LLMs alone do not know medical codes well. GPT models exhibit better memorization of medical codes compared to LLaMA2 (rows 1-3 of Table 2), but they still lag far behind the fine-tuned models (line 3 vs 12). 3) Model scaling-up boosts memorization. Increasing modelsâ€™ parameters significantly enhances their memorization capabilities, as evidenced by an 80-point improvement in code accuracy from GPT-2 medium to large. However, this does not fully translate into improvement of the same magnitude in diagnosis prediction (row 9 vs 10 in Table 2). 4) Encoder-decoder vs decoder-only. Comparing rows 4-7 with rows 8-12 in Table 2, we observe that encoder-decoder LMs tend to perform well on definitionto-code mapping while performing significantly worse on producing the accurate definition given the code. However, the observation is different for decoder-only LMs who can handle code-to-definition mapping at the early stage. Derived from these observations, it is optimal to use a large-size decoder-only LM as the backbone for diagnosis prediction.

Table 2: Memorization and diagnosis prediction (after finetuning on the memorization task) results on MIMIC-III data using different pre-trained LMs.   

<html><body><table><tr><td>#</td><td>Model</td><td>Med. Code Mem. Code Acc</td><td>Def Acc</td><td>Diagnosis Pred. w-F1</td><td>R@20</td></tr><tr><td colspan="5">Zero-shotLM</td></tr><tr><td>1</td><td>LLaMA2</td><td>4.69</td><td>0.61</td><td>5.62</td><td>15.64</td></tr><tr><td>2</td><td>GPT-3.5</td><td>33.50</td><td>9.31</td><td>6.11</td><td>17.07</td></tr><tr><td>3</td><td>GPT-4</td><td>45.16</td><td>48.48</td><td>6.46</td><td>21.56</td></tr><tr><td colspan="6">Fine-tuned encoder-decoderLM</td></tr><tr><td>4</td><td>T5 base</td><td>81.71</td><td>1.26</td><td>20.53</td><td>30.13</td></tr><tr><td>5</td><td>T5 large</td><td>85.28</td><td>2.32</td><td>23.19</td><td>33.85</td></tr><tr><td>6</td><td>Flan-T5 base</td><td>88.58</td><td>0.19</td><td>21.01</td><td>32.24</td></tr><tr><td>7</td><td>Flan-T5 large</td><td>89.97</td><td>0.29</td><td>25.32</td><td>35.25</td></tr><tr><td colspan="6">Fine-tuneddecoder-onlyLM</td></tr><tr><td>8</td><td>GPT-2 base</td><td>0.00</td><td>95.68</td><td>23.29</td><td>32.06</td></tr><tr><td>9</td><td>GPT-2 medium</td><td>0.00</td><td>98.30</td><td>25.50</td><td>34.59</td></tr><tr><td>10</td><td>GPT-2 large</td><td>80.05</td><td>98.56</td><td>29.59</td><td>40.96</td></tr><tr><td>11</td><td>LLaMA27B</td><td>99.87</td><td>99.12</td><td>32.77</td><td>47.48</td></tr><tr><td>12</td><td>BioMistral 7B</td><td>99.61</td><td>99.58</td><td>33.24</td><td>49.01</td></tr></table></body></html>

Table 3: Diagnosis prediction results (recall $( \textcircled { a } 2 0 , \% )$ on the MIMIC-IV dataset using ICD-10 as the decision space with or without additional natural language patient profile.   

<html><body><table><tr><td>Model</td><td>w NL info</td><td>w/o NL info</td></tr><tr><td>Chet</td><td>17.51</td><td>17.51</td></tr><tr><td>Seq2seq (BioMistral 7B)</td><td>16.31</td><td>13.47</td></tr><tr><td>MERA(BioMistral 7B)</td><td>43.66</td><td>40.39</td></tr></table></body></html>

Table 4: Ablation study on model design choices compared with full MERA (row 16 of Table 1) on MIMIC-III dataset.   

<html><body><table><tr><td>#</td><td>Method Variant</td><td>w-F1</td><td>R@20</td></tr><tr><td colspan="4">Knowledge injection approach</td></tr><tr><td>1</td><td>No external knowledge</td><td>-2.33</td><td>-3.54</td></tr><tr><td>2</td><td>Code definition in the prompt</td><td>-1.69</td><td>-2.46</td></tr><tr><td colspan="4">Training objectives</td></tr><tr><td>3</td><td>w/o hierarchical contrastive learning</td><td>-10.34</td><td>-10.27</td></tr><tr><td>4</td><td>- w/o O-th level CL loss only</td><td>-9.24</td><td>-8.4</td></tr><tr><td>5</td><td>- w/o chapter level CL loss only</td><td>-5.86</td><td>-4.08</td></tr><tr><td>6</td><td>- w/o finest level CL loss only</td><td>-7.74</td><td>-6.81</td></tr><tr><td>7</td><td>w/o dynamic confidence threshold</td><td>-4.10</td><td>-2.57</td></tr><tr><td colspan="4">Outputting strategies MERA=decode (our losses)</td></tr><tr><td>8</td><td>Decode (cross-entropy loss)</td><td>-10.31</td><td>-17.33</td></tr><tr><td>9</td><td>Rank (cross-entropy loss)</td><td>-6.72</td><td>-13.32</td></tr><tr><td>10</td><td>Rank (our losses)</td><td>-2.63</td><td>-3.16</td></tr></table></body></html>

# 4.4 Ablation Studies on Method Design

Knowledge injection approach. In rows 1-2 of Table 4, we observed that simply training the medical code sequence without providing meanings of the codes (row 1) leads to a 3.5-point lower recall $@ 2 0$ . Providing the natural language definition of medical code in the input prompt along with the history diagnosis code (row 2 vs 1) is also helpful. However, the NL prompt method suffers from incomplete patient history due to the LMâ€™s input length limit, resulting in a 2.5-point lower recall $@ 2 0$ compared to memorization. Fine-tuning for concept memorization is the most effective knowledge injection approach.

Training objectives. Results in row 3-7 of Table 4 show that removing hierarchical contrastive learning leads to more than a 10-point drop in F1. Among the contrastive terms for disease groups categorized by different granularities, the 0-th level loss (row 4) is the most beneficial, which provides comparisons among the most involved diseases. The finest level loss (row 6) is the second most important, as the chapter-level disease is relatively easier to mine from data, while the fine-grained diagnosis decision involves distinguishing diseases that are similar in manifestation or etiology. Dynamic confidence threshold (row 7) also contributes more than 4-point F1 score improvement.

Outputting strategies. In rows 8-10 of Table 4, we explore optimal approaches to produce the diagnosis prediction set. $L M$ can conduct autoregressive decoding to generate diagnosis codes as an output sequence. Alternatively, we can obtain the ranking list based on the token probability over the vocabulary of the first output token. Using decoding trained with sparse correct token cross-entropy loss $\vartheta 2 . 2 $ , row 8) compromises performance by 17 points in recall $\textcircled { \omega } 2 0$ . The confusing in-visit diagnosis code order makes producing the result from the first token ranking list (row 9) a better choice than decoding along. When applying rich supervision with contrastive learning and dynamic confidence threshold, we observe a 10-point higher recall $@ 2 0$ with ranking output (row 10 vs 9). The comparison between row 10 and full MERA validates the effectiveness of intra-visit modeling, yielding a 3-point higher recall $@ 2 0$ , where we decode token-by-token conditioned on other diagnoses but with specialized trained token probability for each decoding step.

# 5 Related Works

Diagnosis prediction. Existing works leverage structured diagnosis data (Morid, Sheng, and Dunbar 2023). They use sequential models like RNN and LSTM (Choi et al. 2016; Bai et al. 2018) to model the longitudinal patient history and GNNs to encapsulate spatial features (Proios et al. 2023; Lu, Han, and Ning 2022). To inject external knowledge, they conduct multi-task or transfer learning to borrow supervision from other tasks or domains (Yang et al. 2023a; Zhou et al. 2023), use pre-trained embedding to incorporate natural language into initial features (Wu et al. 2023b; Bornet et al. 2023), or utilizing external knowledge graphs or ontologies (An et al. 2023; Cheong et al. 2023; Li et al. 2020). We propose to use the capable LLM architecture to learn patterns from patient history sequences and inject external knowledge with a unified and shared architecture across the pipeline. Existing works apply contrastive learning on intermediate latent for KG relations (An et al. 2023) or patient embedding (Jeong et al. 2023), while we apply contrastive learning on diagnosis output space directly.

Transformer models for medical event prediction. Existing works either handle NL medical notes and other modalities (Niu et al. 2024; Zhou et al. 2023; Wang et al. 2023b; Liu et al. 2023), or they use a non-unified architecture that cannot inherit the pretrained knowledge (Rupp, Peter, and Pattipaka 2023; Li et al. 2023; Pang et al. 2021; Guo et al. 2023) or needs adaptation for downstream tasks (Steinberg et al. 2023; Lai, Zhai, and Ji 2023; Ma et al. 2023; Xu, Ma, and Chen 2023). (Wang et al. 2023a; Shoham and Rappoport 2023; Wornow et al. 2023a) fine-tune the generative LM for classification tasks. We develop a model that is compatible with mainstream LLMs to use the pretrained knowledge and specializes in producing predictions from large diagnosis decision space.

# 6 Conclusion

MERA stands out by seamlessly integrating clinical knowledge and addressing the challenges associated with a large candidate space. Contrasting learning, tailored to the coding systemâ€™s hierarchical structure, enables effective distinguishing between accurate and inaccurate diagnosis codes. Through validation on MIMIC datasets, MERA emerges as a leading approach to diagnosis prediction.