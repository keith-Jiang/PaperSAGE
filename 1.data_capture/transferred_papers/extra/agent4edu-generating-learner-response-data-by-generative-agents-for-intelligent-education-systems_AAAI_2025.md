# Agent4Edu: Generating Learner Response Data by Generative Agents for Intelligent Education Systems

Weibo Gao1, Qi $\mathbf { L i u } ^ { 1 , 2 * }$ , Linan $\mathbf { Y u e } ^ { 1 }$ , Fangzhou $\mathbf { Y a o } ^ { 1 }$ , Rui $\mathbf { L } \mathbf { v } ^ { 1 }$ , Zheng Zhang1, Hao Wang1, Zhenya Huang1,2

1 State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China 2 Institute of Artificial Intelligence, Hefei Comprehensive National Science Center {weibogao, lnyue, fangzhouyao, lvrui2018, zhangzheng} $@$ mail.ustc.edu.cn {qiliuql, wanghao3, huangzhy} $@$ ustc.edu.cn

# Abstract

Personalized learning represents a promising educational strategy within intelligent educational systems, aiming to enhance learners‚Äô practice efficiency. However, the scarcity of offline practice response data (e.g., answer correctness) and potential biases in human online practice create a significant gap between offline metrics and the actual online performance of personalized learning services. To address this challenge, we introduce Agent4Edu, a novel personalized learning simulator leveraging recent advancements in human intelligence through large language models (LLMs). Agent4Edu features LLM-powered generative agents equipped with learner profile, memory, and action modules tailored to personalized learning algorithms. The learner profiles are initialized using real-world response data, capturing practice styles and cognitive factors. Inspired by psychology theory, the memory module records practice facts and high-level summaries, integrating reflection mechanisms. The action module supports various behaviors, including exercise understanding, analysis, and response generation. Each agent can interact with personalized learning algorithms, such as computerized adaptive testing, enabling a multifaceted evaluation and enhancement of customized services. Through a comprehensive assessment, we explore the strengths and weaknesses of Agent4Edu, emphasizing the consistency and discrepancies in responses between agents and human learners.

# 1 Introduction

Intelligent education platforms like Coursera.com and LeetCode.com provide a rich array of learning resources, such as courses and exercises, within a flexible online environment. The accessibility and convenience of these platforms have attracted a growing number of learners. A key online learning activity is ‚Äúpractice‚Äù, where learners independently select and answer exercises. The platforms record their responses, such as the correctness of their answers. By analyzing response data, many personalized learning services, such as exercise recommendations, knowledge tracing, and computerized adaptive testing, can be tailored to meet each learner‚Äôs specific needs, enhancing the learning process and increasing learner satisfaction. For instance, on LeetCode, analyzing a learner‚Äôs historical programming experiences allows the platform to recommend exercises of appropriate difficulty levels, thus optimizing learning gains.

The effectiveness of personalized learning services hinges on the availability of high-quality response data for the corresponding algorithm training. However, the scarcity of offline response data and potential biases in its correlation with online practice introduces a significant gap between offline metrics and actual online performance. This discrepancy impedes the integration of research with real-world applications. To bridge this gap, a promising approach is to simulate learner response data. Imagine an online platform equipped with a configurable simulation system that faithfully captures human learners‚Äô response patterns while seamlessly interacting with personalized learning algorithms. Such a simulator undoubtedly has the potential to revolutionize the traditional research paradigm in intelligent education, providing innovative avenues for response data collection, personalized algorithm development and evaluation.

Several approaches to simulating learner response data have been proposed and have achieved notable success (Piech et al. 2015; Zhao et al. 2023). However, two major limitations exist in current approaches: (1) Simplified Simulations. Most existing studies predict learners‚Äô responses (e.g., correct or incorrect answers) without considering the detailed answer processes by which humans use their knowledge to understand, analyze, and solve problems. Hence, these simulations may lack reliability and interpretability. (2) Dependency on Real Response Data. An ideal simulator should be capable of simulating learner responses even when real-world datasets are insufficiently available, thereby enhancing its applicability. However, current methods require high-quality real-world data to train the simulation strategy. As a result, these methods can only generate learner response data similar to existing real-world datasets and struggle to generalize to more challenging scenarios, such as zero-shot simulations.

Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities in autonomous interaction and decision-making (Brown et al. 2020; Ouyang et al. 2022; Yue et al. 2023; Jin et al. 2023; Long et al. 2024). These advancements underscore the potential of leveraging LLM-powered agents to simulate human social behaviors, such as daily life in Smallville (Park et al. 2023) and software development (Qian et al. 2023). LLM-based user simulators possess rich pre-trained knowledge and humanlike intelligence, enabling them to perceive and simulate intricate human practice processes. Furthermore, their incontext learning ability allows LLMs to perform zero-shot simulations with minimal reliance on real-world data (Wang et al. 2023c). Consequently, LLM-based generative agents present a promising approach for addressing the current limitations of learner response simulators.

In this paper, we introduce Agent4Edu, a personalized learning simulator designed for intelligent educational systems, comprising two key components: an LLM-powered generative agent and a personalized learning environment (see the framework in Figure 1). From a learner perspective, the LLM-powered generative agent is responsible for simulating learners‚Äô response data by capturing their response patterns and inferring problem-solving actions. Each agent is initialized based on available learner response data and consists of three modules: a learner profile, memory, and action module. The learner profile module stores learners‚Äô past practice styles (e.g., activity) and cognitive factors (e.g., ability), aligning with human learners‚Äô learning status. The memory module, inspired by psychological theories (Baker 2001) and human learning mechanism (Wang et al. 2023d), records past practice experiences and summarizes learning status through reflections. This facilitates coherent observations, monitors knowledge proficiency evolution, reinforces memory, and simulates human forgetting. The action module enables agents to choose, understand, analyze, and solve exercises recommended by personalized learning algorithms, leading to more reliable and interpretable response generation. Our agent can also utilize tools, such as employing the psychological IRT model (Baker 2001) to assess ability within the Profile module and using DNeuralCDM (Wang et al. 2023a) to trace knowledge proficiency evolution within the Memory module. From a personalized learning perspective, the learning environment can be configured with any personalized learning algorithm, allowing agents to interact directly and simulate a real learning environment. Notably, despite extensive research on simulating user behavior with generative agents, we are the first to focus specifically on educational scenarios to generate response data for individual learners.

Our main contributions are summarized as follows:

‚Ä¢ We develop Agent4Edu1, a personalized learning simulator that leverages LLM-powered generative agents to simulate human learners‚Äô response data as well as demonstrate the practice process. Additionally, the agent interacts with personalized learning environments to evaluate and improve intelligent tutoring algorithms.

‚Ä¢ Our generative agents, featuring profile, memory, and action modules specifically designed for ‚ÄúEducation‚Äù, can not only generate response data but also accurately simulate human choices, understanding, analysis, and problem-solving for exercises, outperforming existing learner simulation methods.

‚Ä¢ To systematically evaluate Agent4Edu, we conduct comprehensive experiments from both the agent and personalized learning perspectives. From the agent perspective, we assess the consistency between the agents and human learners. From the learning perspective, we evaluate and improve personalized learning algorithms for computerized adaptive testing, based on generative agents and simulated data. Extensive experimental results demonstrate the effectiveness of Agent4Edu.

# 2 Related Work

Learner Response Data Simulation Learner Simulation aims to address the shortage of high-quality practice data in intelligent educational systems and has been applied in numerous previous studies (Zhao et al. 2023; Yao et al. 2024). Memory-based (Reddy, Levine, and Dragan 2017) relies on manually crafted rules to predict learners‚Äô responses or memory behavior. EERNN (Su et al. 2018) and KES (Liu et al. 2019) utilize RNN-based models to forecast learners‚Äô performance. DAISim (Zhao et al. 2023) constructs learner simulations as Markov decision processes, simultaneously considering learners‚Äô long and short-term questionanswering patterns. However, the memory-based simulator is overly simplistic and cannot simulate complex interactions. Most other learner simulators simplify the student answering process and face challenges in conducting zero-shot simulations due to their reliance on data. In this paper, we employ an LLM-powered agent to simulate the student practice process, addressing these limitations.

Personalized Learning Services Intelligent educational systems offer learners personalized learning services, including Computerized Adaptive Testing (CAT) (Chang and Ying 1996), exercise recommendation (Huang et al. 2019) and learning path suggestions (Liu et al. 2019), to help learners enhance their skills. In this work, we select the representative and popular CAT services as our personalized learning scenarios for our study and experiments. CAT is an advanced educational measurement method that evaluates the knowledge level of examinees in minor exercises, which has been widely used in various standardized tests (e.g., GMAT and GRE) (Zhuang et al. 2024; Bi et al. 2020; Lord 2012; Chang and Ying 1996). However, current CAT models require highquality practice data to train a cognitive diagnosis model for evaluating learner ability or knowledge proficiency, which is often challenging to gather. Therefore, in this paper, we employ the CAT service within learning systems to assess the quality of the data generated by our agents. Additionally, we investigate the potential for enhancing CAT models using simulated data.

LLM-based Agents LLM-based generative agents demonstrate the remarkable capabilities to perceive their environment, make decisions, and take actions, thus, emerging a substantial amount of research (Wang et al. 2024b). The development of generative agents (Park et al. 2023), designed with profile, memory, action, and reflective capabilities, represents pioneering work in simulating human daily life. Within this general framework, agents tailored to specific tasks (Qian et al. 2023; Wu et al. 2023; Wang et al. 2023b; Huang et al. 2023; Zhang et al. 2023b,a) and simulations (Gao et al. 2023; Wang et al. 2023d; Park et al. 2023; Liu et al. 2023; Wang et al. 2023c) have been constructed. Recent research highlights bringing generative agents to educational settings (Li et al. 2024; Dan et al. 2023; Kieser et al. 2023). For example, (Qadir 2023; Rahman and Watanobe 2023) conclude the applications of ChatGPT to engineering education. (Baidoo-Anu and Ansah 2023) focus on the literature review over the published paper. SocraticLM (Liu et al. 2024) embodies a ‚ÄúThought-Provoking‚Äù teaching paradigm, engaging students in active problemsolving, akin to a real classroom teacher. The most relevant part of our work is EduAgent (Xu, Zhang, and Qin 2024) which utilizes LLM-based agents to simulate learners studying PowerPoint presentations and videos, predicting their quiz outcomes to assess performance. However, this approach relies on expert-annotated cognitive factors to initialize agents, disregarding the understanding and analysis of exercises. In contrast, our Agent4Edu extracts cognitive factors from data using tools and captures practice styles, allowing it to simulate the detailed exercise understanding and analysis process and interact effectively with personalized learning algorithms.

# 3 Agent4Edu

Agent4Edu is a personalized learning simulator, aimed at accurately simulating learners‚Äô response data and facilitating responsive personalized learning algorithms. It contains two key components: (1) LLM-powered generative agents that capture learners‚Äô practice patterns and cognitive preferences to simulate their response, and (2) a personalized learning environment that interacts with agents to support accurate and interpretable evaluations and improvements of mainstream intelligent algorithms (e.g., computerized adaptive testing). The framework of Agent4Edu is illustrated in Figure 1. All the prompts are listed in Appendix C.

# 3.1 Task Formulation

Suppose there are $| U |$ learners, $| E |$ exercises in an intelligent educational system. For a learner $\textit { u } \in \textit { U }$ , his/her response data are denoted as a time-ordered set $l _ { u } = \{ ( e _ { 1 } , \stackrel { \cdot } { c } _ { e _ { 1 } } , y _ { u , e _ { 1 } } ) , ( e _ { 2 } , c _ { e _ { 2 } } , y _ { u , e _ { 2 } } ) , \ldots , ( e _ { n } , c _ { e _ { n } } , y _ { u , e _ { n } } ) \}$ , where $e _ { i } \in E$ represents the exercise that learner $u$ practiced at step $i$ , and $y _ { u , e _ { i } }$ is $u$ ‚Äôs response to exercise $e _ { i }$ , which is usually denoted as a binary value, i.e., if learner $u$ answers $e _ { i }$ correctly, $y _ { i } = 1$ otherwise $y _ { i } = 0$ . $c _ { e }$ denotes textual information of each exercise $\textit { e } \in \textit { E }$ , e.g., textual content and corresponding knowledge concepts. We provide $c _ { e }$ in a $< k e y , v a l u e >$ form, as the example in Figure 1.

Based on the above conditions, the simulator‚Äôs overarching goal is to accurately generate the human learners‚Äô future response data on unseen exercises by faithfully distilling their learning patterns and cognitive preferences from historical response data. Please note that existing personalized learning algorithms usually assume that learners only submit each exercise once, so repeated submission is not considered in our simulation.

# 3.2 LLM-powered Agent

The generative agent in Agent4Edu uses LLM as its foundational architecture, enhancing its functionality tailored for the personalized learning scenario through three specialized modules: learner profile, memory, and action modules. To mimic actual personalized practice responses akin to humans, we construct an individual agent $\ a g e n t _ { u }$ for each learner $u$ . Each agent integrates a learner profile module aimed at reflecting personalized practice patterns and cognitive factors. Additionally, each agent is equipped with a memory module designed to store past practice records and summarize high-level ideas. To simulate learner practice behavior more cohesively, the agent is also equipped with an action module.

Learner Profile Module The learner profile module represents some overall learning features of human learners, which are typically stable and derived from long-term learning experiences. We configure each agent $a g e n t _ { u }$ ‚Äôs profile based on its corresponding learner $u$ ‚Äôs response data2. Each agent‚Äôs initial configuration is divided into two categories: explicit practice styles and implicit cognitive factors.

Practice styles are statistical features explicitly derived from the available practice record $l _ { u }$ of each learner $u$ , such as learning activity (Baker 2001; Gao et al. 2021), practice diversity (Bi et al. 2020), success rate, and preference. Activity indicates learners‚Äô enthusiasm for learning and provides clues for simulating their practice behaviors. For example, learners with higher enthusiasm for learning usually perform better. Mathematically, the activity level of learner $u$ is defined as $\begin{array} { r } { P _ { \mathrm { a c t } } ^ { u } = \frac { | l _ { u } | } { | E | } } \end{array}$ . Practice diversity reflects the knowledge coverage practiced by learners, represented as $\begin{array} { r } { P _ { \mathrm { d i v } } ^ { u } = \frac { | K _ { u } | } { | K | } } \end{array}$ where $| K _ { u } |$ is the number of knowledge concepts practiced by learner $u$ . Higher diversity indicates greater curiosity in learners. Success rate correlates with the probability of learners answering questions correctly, making it another essential characteristic. The success rate for learner $u$ is mathematically represented as $\begin{array} { r } { P _ { \mathrm { s u c } } ^ { u } = \frac { \sum _ { y _ { u , e _ { i } } \in l _ { u } } y _ { u , e _ { i } } } { | l _ { u } | } } \end{array}$ Pyu,ei ‚ààlu yu,ei . Preference refers to the knowledge concepts that learners practice most frequently.

Cognitive factors are implicit features studied in psychology (Baker 2001; Chen et al. 2024), which significantly impact learner $u$ ‚Äôs practice performance. We select problemsolving ability and knowledge proficiency (Cheng et al. 2024) for this study. Problem-solving ability is assumed to be stable during the learning process, while knowledge proficiency typically evolves with learning progress (Huang et al. 2020). Therefore, in the profile module, we only configure the ability factor $P _ { \mathrm { a b } } ^ { u }$ , with knowledge mastery being considered in the subsequent memory module. To obtain implicit ability, we assign a psychological IRT model (Baker 2001) trained on the observed learner response records, as the tool for the agent, allowing it to infer each learner $u$ ‚Äôs ability factor from the response data $l _ { u }$ . The training and use of the IRT tool are detailed in Appendix B.

Real Response Data Memory Module Action Module Personalized Ëá™ÁõÆÈ£üËá™ÁõÆ- Factual Memory Cognition-driven Action Learning Ëá™Ëá™Ëá™È¶ñ I choose to do this Exercise exercise ‚Ä¶ Learner ùë¢ ùëìùë¢,1 ùëìùë¢,2 ùëìùë¢,3 ùëìùë¢,4 ùëìùë¢,5 J? [Key]: Textual Content ? Short-term Memory [Value]: Will the graph Learner Profile Module Most Recent Reading & Understanding of the function y=a^x-3 Observations It tests knowledge always pass through Practice Features Function ‚Ä¶ the point (1, a)? Activity: High Long-term Memory ? M (Reflection) .S.u.ccess Rate: Easy FRaeicntfsorced Forget Analyzing & Solving ‚Ö¢ Since the function Cognitive Factors is y=a^x-3, we can Ability: Middle (SRuefmlemctaioryn) LPeroafrincienngcpy:r o‚Ä¶cess: ‚Ä¶ ? substitute the ‚Ä¶ (Reflection) Simulated Personalized Learning 3 Tools: Â±± IRT Model D DNeuralCDM Response Data Algorithm LLM-empowered Agent ùëéùëîùëíùëõùë°ùë¢ Environment

Notably, we segment the values of each of the above features into several tiers in order to better prompt the generative agent inspired by (Wang et al. 2023d). For a detailed exposition, refer to Appendix A.1. Additionally, to ensure broad applicability and protect privacy, certain personal identifiers (such as name, gender, age, and occupation) are intentionally anonymized in this work (Zhang et al. $2 0 2 3 \mathrm { a }$ ; Li et al. 2023). While these attributes may help shape other types of agents, they are not primary factors affecting practice performance in education. Our approach, based on both behavioral practice styles and psychological cognitive settings, can support a comprehensive representation of real learners.

Memory Module The Memory module allows the LLMbased agent $\ a g e n t _ { u }$ to observe and summarize its corresponding learner $u$ ‚Äôs past practice experiences step by step. This module provides insightful clues to the agent for response simulation on unseen exercises. We follow the human learning mechanism (Atkinson $1 9 6 8 \mathrm { a }$ ; Cowan 2008; Huang et al. 2020; Wang et al. 2023d) to design three types of memories for each agent: factual memory, short-term memory, and long-term memory. Each memory is initially set to empty.

Factual Memory: In our simulation, factual memory is defined as the true learner‚Äôs past response records (i.e., observations). When the agent obtains a new response record of learner $u$ at step $i$ , i.e., $l _ { u , i } = ( e _ { i } , c _ { e _ { i } } , y _ { u , e _ { i } } )$ , the response record is transmitted to the factual memory for processing.

Inspired by human learning mechanisms, if an agent repeatedly practices similar questions or knowledge, their memory is strengthened (Huang et al. 2020). Therefore, we introduce an additional counter $f _ { u , i }$ (initially set to 1) for each record $l _ { u , i }$ in factual memory to track the number of times it has been reinforced, a simple yet effective method that has been successfully used in user preference simulation (Wang et al. 2023d). Formally, for each agent $a g e n t _ { u }$ , assume it has observed $n$ factual memories is $M _ { u } = \{ l _ { u , 1 } , l _ { u , 2 } , . . . , l _ { u , n } \}$ , then it is allowed to receive a new response record $l _ { u , n + 1 }$ . We first calculate the similarity between $l _ { u , n + 1 }$ and each existing factual memory $l _ { u , i }$ in the current memory $M _ { u }$ . The similarity between records can be defined as a metric that can be evaluated by LLMs, cosine similarity between text vectors, and other similar measures. In this case, we use the similarity relationships between the knowledge concepts involved in the records for the calculation. Specifically, we employ the statistical tool3 released by RCD (Gao et al. 2021) to determine whether two knowledge concepts are similar. If there is a similarity between the knowledge concepts involved in two records, the two records are considered similar. For similar records, we increment the counter for $l _ { u , i }$ by 1 (i.e., $f _ { u , i } \gets f _ { u , i } + 1 )$ , indicating that it has been reinforced by $l _ { u , n + 1 }$ , and then add $l _ { u , n + 1 }$ to factual memory; otherwise, $l _ { u , n + 1 }$ is directly recorded without any reinforcement. After processing and saving a new response record, factual memory triggers updating short-term and long-term memories.

We emphasize that the agent can only save response records into factual memory but cannot directly retrieve it, thereby allowing the retention of all exercise textual information and responses without being constrained by the LLM‚Äôs context length limitations.

Short-term Memory: Human short-term memory refers to the recent and temporary information that can be retained and recalled over a relatively brief period (Atkinson 1968b). Therefore, in our simulation, short-term memory is employed to retain the details of the agent‚Äôs most recent observed $s$ records. Assuming the current factual memory of agent $\ a g e n t _ { u }$ is $M _ { u } \ = \ \bar { \{ l }  _ { u , 1 } , l _ { u , 2 } , . . . , l _ { u , n } \}$ , then the short-term memory storage is defined as $M _ { u , s h o r t } ~ =$ $\{ l _ { u , n - s + 1 } , \ldots , l _ { u , n } \}$ .

Long-term Memory: Long-term memory is formed through the reinforcement of memories from repeated practice and self-reflection inspired by to human long-term memories (Matelsky et al. 2023). It possesses a wide receptive field, allowing it to retain information observed long ago and generate high-level insights. We design the long-term memory using three types of information: (1) Reinforced Facts: During each update of long-term memory, the agent $\ a g e n t _ { u }$ first goes through the current factual memory $M _ { u }$ . When the count $f _ { u , i }$ of a record $l _ { u , i }$ exceeds a preset threshold $F$ , indicating that the memory has been reinforced $F$ times, it is converted into long-term memory. (2) Learning Process Summary: We utilize the LLM embedded in the agent to summarize the agent‚Äôs learning status from both short-term and long-term memories by Memory Reflection. Each step of the summary replaces the previous summary. The summary consists of linguistic descriptions of the practice process and new insights from the agent itself. It overlooks practice details to filter out noise, irrelevant content, or potentially misleading information. Furthermore, compressing memory conserves significant space and enhances operational efficiency. (3) Knowledge Proficiency: We allow the agent to use an optimized DNeuralCDM (Wang et al. 2023a) based on the observed learner response data as a tool to obtain the learner‚Äôs dynamic proficiency (segmented into several tiers) evolution of specific knowledge concepts after each step of practice. The knowledge proficiency is a kind of dynamic cognitive factor significantly reflecting human responses in education (Piech et al. 2015; Wang et al. 2024a). The training and use of DNeuralCDM are given in Appendix B.

Additionally, each factual record in long-term memory may be forgotten following the human forgetting curve theory (Averell and Heathcote 2011; Huang et al. 2020) that human memory decay starts rapidly and then gradually slows over time. We define a forgetting function associated timestamp i and current observed step n, i.e., g(lu,i) = 1+e‚àí1(n‚àíi) , to simulate human learners‚Äô forgetting. For each factual record in the long-term memory $M _ { u }$ , it is forgotten if $g ( l _ { u , i } )$ exceeds a predetermined threshold $\lambda$ and its reinforcement frequency $f _ { u , i }$ in factual memory is then reset as 1.

Overall, the factual response records are specific, while learning memory summaries are more general. By combining them, the agent can accurately perceive the learner‚Äôs practice process. Please note that traditional simulators (Piech et al. 2015; Zhao et al. 2023) can be regarded as owning the short-term memory but no long-term memory.

To help agents interact with the personalized learning environment, we introduce three memory operations: Memory Retrieval: This operation assists the agent in extracting related information from memory. We allow the agent to retrieve the short-term and long-term memories finding reinforced facts and conducting summary. Memory Writing: The raw observations are firstly input into the factual memory as facts. Then the recent facts are stored in short-term memory and the reinforced facts are written into long-term memory. Memory Reflection: This operation occurs exclusively within long-term memory containing two aspects of reflections: (1) Summary Reflection is performed to summarize high-level ideas based on short-term and long-term memories, and (2) Corrective Reflection is performed when the agent‚Äôs action is inconsistent with the real learner, which will be introduced in Action Module.

Action Module To equip the agent with learner profiles and memory modules, enabling it to exhibit human-like problem-solving behaviors and responses based on current observations, we design a specialized action module for each agent within Agent4Edu tailored for personalized learning. This module encompasses three main categories of actions: Cognition-driven Actions: In our simulation, personalized learning algorithms recommend one exercise to the agent at each step. The agent read the exercise‚Äôs content and decides whether or not to practice it, based on current cognitive factors. If the exercise is too challenging relative to the agent‚Äôs assessed ability and knowledge proficiency, the agent can opt to reject the recommended exercise.

Reading and Understanding Exercises. Simulating the process of reading and understanding exercises, similar to how humans approach them, provides valuable and interpretable insights into the agents. During each practice session, the agent is first required to identify and describe a knowledge concept assessed by the current exercise. If the agent correctly matches the exercise‚Äôs knowledge concept, it demonstrates an understanding of the exercise context akin to human learners. If the agent fails to do so, a corrective reflection is triggered to guide the agent towards the correct knowledge concept. This method reduces the risk of inaccuracies and ensures the agent‚Äôs credibility in simulating learner response (Zhang et al. 2023a).

Analyzing and Solving Exercises. Analyzing and solving exercises are crucial aspects of the learning process. Unlike previous simulation methods that directly predict the learner‚Äôs response in terms of answer correctness, our simulation requires the agent to emulate the learner‚Äôs answering process, which enhances both interpretability and credibility. To simulate this complex answer process more effectively, we improve agent‚Äôs reasoning ability through a chainof-thought approach (Wei et al. 2022). Initially, the agent combines its profile and memories to formulate an initial solution idea for the exercise. Then, it writes the final answer to the exercise based on the solution idea. Afterwards, the agent predicts whether its answer is correct (i.e., performance prediction). If the predicted response does not match the real learner‚Äôs response, a corrective reflection is triggered. Note that, if standard answers of exercises are available, a scoring program can be designed to directly assess the correctness of the agent‚Äôs answer.

# 3.3 Personalized Learning Scenarios

Agent4Edu simulates agent and learning environment interaction (see Appendix D for a case study). The learning environment is designed as a standalone module that incorporates a series of personalized algorithms. These algorithms can recommend exercises to agents based on their past practice data. For instance, our experiments utilize computerized adaptive testing (CAT) strategies (Bi et al. 2020) for personalized learning. The module features an open interface, allowing researchers and practitioners to integrate external personalized learning algorithms seamlessly. This adaptability ensures that Agent4Edu serves as a versatile platform for comprehensive evaluations and the future collection of valuable learner response data.

# 4 Experiment

Dataset Our dataset, called EduData, is provided by iFLYTEK Co., Ltd. It comprises 18,045 time-ordered response records from 500 Chinese high school students in the subjects of mathematics and physics. Each record includes the exercise ID, correctness, and timestamp. There are 1,032 exercises and 458 knowledge concepts in total, with each exercise testing one knowledge concept. Additionally, to facilitate reasoning and reflection for LLM-based agents, the platform provider has furnished us with the textual content of the exercises. In the experiment, we translate all Chinese text of exercises into English.

Experimental Setup We use GPT-3.5-turbo and GPT-4 through OpenAI‚Äôs API service 4 to construct the agent for experimentation. When operating under the GPT-3.5-turbo configuration, all response data is utilized for experiments. Due to cost considerations, we simulate the task records of only 100 learners under the GPT-4 setting. The temperature parameter of GPT is 0 to avoid randomness. Empirically, we set the short-term memory size to 5, the threshold $F$ for memory enhancement is 5, and the threshold $\lambda$ for forgetting in long-term memory is 0.99. Note that, in our experiments, unless explicitly specified, the LLM used is GPT-3.5-turbo.

# 4.1 LLM-based Agent Simulation Evaluation

Motivation: The LLM-based agent is the core component of Agent4Edu. Exploring whether the agent can truly simulate human learners‚Äô practice response is crucial for enhancing intelligent educational systems. We evaluate the effectiveness of the generative agent, including response simulation, exercises‚Äô knowledge understanding, zero-shot simulations, and ablation experiments.

Learner Simulation Evaluation The agent aims to generate simulated learner response data that closely approximates real responses. To validate the effectiveness of the simulation, we compare it with two traditional supervised simulation methods, including DAISIM (Zhao et al. 2023) and KES (Liu et al. 2019). Additionally, to enrich our baseline for a compelling comparison, we include several Knowledge Tracing (KT) models, such as DKVMN (Zhang et al. 2017), EERNN (with Markov) (Su et al. 2018) and SAKT (Pandey and Karypis 2019), which are similar to the learner simulator in terms of response prediction.

In the experimental setup, each learner‚Äôs records are divided into a $90 \%$ training set and a $10 \%$ test set. Each baseline model which is data-driven is trained on the training data, with the last $20 \%$ records of each learner‚Äôs training data used for model validation. The agent has access to all training data to generate profiles and update its memory through reflection. During the testing phase, each trained baseline is tasked with predicting learners‚Äô binary responses (correct or incorrect) to unseen exercises in the test data. For our generative agent, exercises from the test data are sequentially sent to it, and it performs the designed three actions to solve them. If the agent rejects an exercise due to its difficulty, we label its response as an ‚Äúincorrect answer‚Äù. The evaluation metrics are selected from two perspectives. Firstly, we use accuracy (ACC) and F1-score to measure prediction accuracy. Secondly, we assess the similarity between the simulated and real data distributions using ROUGE-3, inspired by (Zhao et al. 2023). We repeatedly run each baseline model five times in the same setups and the Table 1 reports the average scores.

Table 1: Prediction scores $( \% )$ on evaluating simulation performance. The best results are bold, the second-best results are marked by an underline, and $\uparrow$ means the higher score the better performance, the same as below. Agent4Edu100 indicates a basic exploratory on simulating 100 learners.   

<html><body><table><tr><td>Model</td><td>ACC‚ÜëF1-score‚ÜëROUGE-3‚Üë</td><td></td></tr><tr><td>KES</td><td>50.11 58.32</td><td>25.77</td></tr><tr><td>DKVMN</td><td>64.39 76.70</td><td>37.24</td></tr><tr><td>EERNN</td><td>65.72 76.06</td><td>43.55</td></tr><tr><td>SAKT</td><td>65.52 78.33</td><td>31.09</td></tr><tr><td>DAISIM</td><td>65.63 78.25</td><td>31.72</td></tr><tr><td>Agent4Edu (GPT-3.5-turbo)</td><td>66.70 79.84</td><td>37.97</td></tr><tr><td>Agent4Edu (GPT-3.5-turbo)100</td><td>65.40 78.72</td><td>35.14</td></tr><tr><td>Agent4Edu (GPT-4)100</td><td>66.51 79.53</td><td>34.86</td></tr></table></body></html>

The experimental results indicate that Agent4Edu (GPT3.5-turbo) demonstrates strong competitiveness compared to the supervised baselines, particularly in terms of ACC and F1-score. This suggests that the LLM-based agent has the potential to generate learner response data that closely resembles real-world datasets. Furthermore, among the baselines, EERNN performs exceptionally well by effectively modeling the exercise content as supplementary clues. Finally, an exploratory simulation conducted using Agent4Edu (GPT-3.5-turbo and GPT-4) on a subset of data with 100 learners shows that they enable the simulated distribution to closely approach the real distribution. Among them, GPT-4 performs better in terms of ACC and F1-score.

Additionally, we evaluate whether the simulated distribution of the agent‚Äôs practice success rate aligns with the actual distribution of learner data. We use the real response success rate as the ground truth and then replace the corresponding responses in the real sequence with the predicted responses from the test data to calculate the agent‚Äôs simulated success rate, as shown in Figure 2 (a). The comparison between the ground truth values and the agent‚Äôs results indicates that the simulated data effectively captures the learners‚Äô practice patterns related to success rate.

Understanding Exercise-related Knowledge To evaluate whether the agent understands a specific exercise, the agent is tasked with generating the knowledge concept tested by the exercise. Specifically, we create a candidate list containing one actual knowledge concept related to the exercise

0.86 (a)1.0 01.08 (b) Agent4Edu Win Tie Agent4Edu Lose (c) complete w/o prof   
0.024 0.6 12.33% 0.7 w/o mem w/o enh 0.4 23.00% 39.67% 45.00% w/o fgt w/o ref Ground-truth 0.2 Agent4Edu Agent4Edu 250 500   
0.2468 10.80 0.4 Human 37.33% Human 42.67% Agent4Edu 0.2 0.5 Answering Summarization Knowledge Pre. Response Pre. 250 500

Table 2: The ACC of knowledge prediction.   

<html><body><table><tr><td>Model</td><td>ACC‚Üë</td></tr><tr><td>Agent4Edu (GPT-3.5-turbo)</td><td>73.88</td></tr><tr><td>Agent4Edu (GPT-3.5-turbo)100</td><td>74.57</td></tr><tr><td>Agent4Edu (GPT-4)100</td><td>82.43</td></tr></table></body></html>

and two random knowledge concepts unrelated to the exercise. The agent must then select the relevant knowledge concept from this list based on its understanding (detailed prompts are provided in Appendix C). We use ACC as the metric to evaluate the agent‚Äôs knowledge predictions for all exercises in the test set, treating it as a binary classification task. This section uses the same agents from the section ‚ÄúLearner Simulation Evaluation‚Äù. The experimental results presented in Table 2, indicate that all the agents can correctly identify the knowledge being tested in most practice exercises. This demonstrates the strong human-like ability and rich knowledge of LLMs to comprehend exercises. Furthermore, under the same conditions with 100 learners, the agent with GPT-4 is more accurate than the one with GPT3.5-turbo, indicating that GPT-4 has a stronger semantic understanding ability compared to GPT-3.5-turbo.

Zero-shot Simulation Zero-shot simulation presents a significant challenge in real-world applications, particularly when learners are in a cold-start situation where their response data is unavailable. This limitation restricts the applicability of previous simulation models. To validate the zero-shot simulation capability of the agent, we initialize 10 agents with randomly generated profiles and have them sequentially answer 10 randomly selected exercises with IDs 120, 250, 113, 1330, 568, 1881, 771, 593, 1, 595 . In this zero-shot scenario, we disable the corrective reflection mechanism and tools, due to the absence of learner response data. The summary reflection remains usable for the agent. Three GPT-3.5-turbo models, with a temperature parameter of 0.5, act as annotators, tasked with evaluating whether each simulated record (including exercise answers and practice summaries) is written by a real human. Records deemed to be human-written are labeled as ‚ÄúAgent4Edu

Win‚Äù, non-human records are labeled as ‚ÄúLose‚Äù, and ambiguous records are labeled as ‚ÄúTie‚Äù.

The results depicted in Figure 2 (b) indicate that the agent‚Äôs performance in summarization is closely aligned with the real human responses, making differentiation between the two challenging. However, the agent exhibits certain limitations in answering exercise tasks compared to summarization tasks, primarily due to the complexity of reasoning required to solve exercises.

Ablation Study We conduct ablation studies to evaluate the impact of key components within the GPT-3.5-turbopowered agent. The results illustrated in Figure 2 (c) show the accuracy of the agent‚Äôs exercise-related knowledge prediction and response prediction under various conditions: without the profile module (w/o prof), without the memory module (w/o mem), without the memory enhancement (w/o enh), without the memory forgetting (w/o fgt), and without reflection (w/o ref). These findings confirm the effectiveness of each component in improving the agent‚Äôs predictive performance on learners‚Äô response data. However, the ablation experiments indicate that the impact on knowledge prediction is not significant. This can be attributed to the fact that the original GPT-3.5-turbo model already possesses a substantial amount of knowledge, which is sufficient to support exercise comprehension.

# 4.2 Personalized Learning

Motivation The primary objective of Agent4Edu is to comprehensively and accurately evaluate personalized learning algorithms and use the generated data to enhance their effectiveness. We aim to validate this objective from two perspectives: (1) through an agent-based multifaceted evaluation of personalized learning services, and (2) by assessing the potential improvements in personalized learning algorithms based on the simulated data.

Multifaceted Evaluation Human learners have multifaceted evaluations of different personalized learning services, such as whether the recommended task difficulty is too challenging. Assuming that a generative agent can accurately simulate the behavior of real learners, its evaluation of personalized algorithms tends to align with human evaluations. We utilize the Computerized Adaptive Testing (CAT) which aims to estimate learners‚Äô ability or knowledge proficiency with minor exercises, as the experimental environment, including FSI (Lord 2012), KLI (Chang and Ying 1996), and MAAT (Bi et al. 2020). We use 100 randomly initialized agents to generate virtual data for pretraining the cognitive diagnosis model (i.e., the IRT model (Baker 2001)) in the CAT algorithm for learner evaluation. Based on this, each adaptive algorithm iterates through 10 rounds to recommend 100 randomly initialized agents (zero-shot simulation), with one exercise recommendation per round. Upon the conclusion of personalized testing, each agent is required to evaluate each CAT algorithm. To achieve this, we design three evaluation metrics, including satisfaction, appropriateness of difficulty (AoD), and whether there was any gain. Table 3 presents a comprehensive evaluation of various strategies, where the element in the $i$ -th row and $j$ -th column represents the number of agents that consider the corresponding CAT algorithm $i$ to meet the metric $j$ . Clearly, the agent demonstrates higher satisfaction in recommending MAAT. This observation aligns with the common understanding in the research community that MAAT considers both the difficulty of exercises and the diversity of knowledge (Bi et al. 2020), making the overall service more reasonable. Additionally, FSI focuses on recommending exercises that are moderately difficult and likely to provide gain. These findings highlight the LLM-powered agent‚Äôs fine-grained evaluation level for learning algorithms.

Table 3: Multifaceted evaluations of CAT strategies.   

<html><body><table><tr><td>Model</td><td>satisfaction</td><td>AoD</td><td>gain</td></tr><tr><td>FSI</td><td>39</td><td>70</td><td>48</td></tr><tr><td>KLI</td><td>39</td><td>66</td><td>43</td></tr><tr><td>MAAT</td><td>42</td><td>68</td><td>45</td></tr></table></body></html>

Personalized Learning Algorithm Improvement We investigate whether the simulated data generated by Agent4Edu can enhance personalized learning algorithms. We select CAT as our personalized learning assessment task due to their representativeness in intelligent education. If the generated data can improve the performance of CAT models, it will indicate the effectiveness of our proposed Agent4Edu. To set up, we select $60 \%$ of the learners‚Äô data from EduData to train the cognitive diagnosis models (i.e., the IRT model) for learner evaluation in CAT algorithms (i.e., FSI (Lord 2012), KLI (Chang and Ying 1996), and MAAT (Bi et al. 2020)). The remaining $40 \%$ of learners‚Äô data is used to test the CAT models. Furthermore, for each learner in the test data, we simulate their responses to 20 randomly selected unseen exercises based on their profiles. Using this strategy, we generate simulated learner data, which are then merged with the training data from the original EduData to form the augmented dataset, EduData+. We train the IRT model in each CAT model using both the original EduData and EduData+, and then evaluate each CAT strategy by recommending 5 and 10 test exercises for each learner.

Table 4 lists the IRT prediction performance after retraining on the testing records via CAT, where F1-score represents scores on EduData, and F1-score $+$ represents scores on EduData+. The results demonstrate that CAT strategies can be effectively enhanced with the assistance of Agent4Edu. This suggests that Agent4Edu is capable of generating highquality learner response data, even with randomly initialized agents (in zero-shot scenarios), thereby enriching the provided dataset.

Table 4: The improvement of CAT services.   

<html><body><table><tr><td colspan="2">Testing length is 5</td></tr><tr><td>Model</td><td>F1-score F1-score+ Imp.</td></tr><tr><td>FSI</td><td>80.11 82.39 +2.28</td></tr><tr><td>KLI MAAT</td><td>79.45 81.84 +2.39 81.77 81.97 +0.20</td></tr><tr><td></td><td>Testing length is 10</td></tr><tr><td>Model</td><td>F1-score F1-score+ Imp.</td></tr><tr><td>FSI</td><td>+1.41</td></tr><tr><td></td><td>81.10 82.51</td></tr><tr><td>KLI MAAT</td><td>80.63 82.82 +2.19 81.71 81.88 +0.17</td></tr></table></body></html>

# 5 Conclusion

In this paper, we introduce Agent4Edu, an innovative personalized learning simulator that leverages LLM-powered generative agents to simulate learners‚Äô response data, as well as detailed problem-solving behaviors. Our generative agents are equipped with learner Profile, Memory and Action modules specifically tailored for personalized learning scenarios. These agents exhibit human-like choosing, understanding, analyzing and answering exercises, which accurately predict their future responses. Additionally, the generative agent can interact with personalized learning environments to evaluate and enhance intelligent services. Through comprehensive and meticulous evaluation, we explore the strengths and weaknesses of Agent4Edu, emphasizing the consistency and discrepancies in practice behaviors observed between agents and learners. In the future, we plan to research multi-learner agent cooperation and multimodal practice solutions using generative agents. We hope that our research will provide new insights into the field of intelligent education.