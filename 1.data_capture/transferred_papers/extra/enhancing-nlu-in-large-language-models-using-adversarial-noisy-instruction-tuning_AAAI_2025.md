# Enhancing NLU in Large Language Models Using Adversarial Noisy Instruction Tuning

Shengyuan Bai1,2\*, Qibin $\mathbf { L i } ^ { 1 * \dagger }$ , Zhe Wang3, Nai Zhou4, Nianmin Yao1†

1School of Computer Science and Technology, Dalian University of Technology 2International Digital Economy Academy (IDEA) 3Hong Kong University of Science and Technology 4Quan Cheng Laboratory jerry.sy.bai $@$ gmail.com, liqibin $@$ mail.dlut.edu.cn zwangec@connect.ust.hk, zhounai $1 9 9 2 @$ outlook.com, lucos@dlut.edu.cn

# Abstract

Instruction tuning has emerged as an effective approach that notably improves large language models (LLMs) performance, showing particular promise in natural language generation tasks by producing more diverse, coherent, and taskrelevant outputs. However, extending instruction tuning to natural language understanding (NLU) tasks presents significant challenges, primarily due to the difficulty in achieving high-precision responses and the scarcity of large-scale, high-quality instruction data necessary for effective tuning. In this work, we introduce Adversarial Noisy Instruction Tuning (ANIT) to improve NLU performance on LLMs. First, we leverage low-resource techniques to construct noisy instruction datasets. Second, we employ semantic distortion-aware techniques to quantify the intensity of noise within these instructions. Last, we devise an adversarial training method that incorporates a noise response strategy to achieve noisy instruction tuning. ANIT enhances LLMs capability to detect and accommodate semantic distortions in noisy instructions, thereby augmenting their comprehension of task objectives and ability to generate more accurate responses. We evaluate our approach across diverse noisy instructions and semantic distortion quantification methods on multiple NLU tasks. Comprehensive empirical results demonstrate that our method consistently outperforms existing approaches across various experimental settings.

# Introduction

Natural Language Understanding (NLU) and Natural Language Generation (NLG) constitute the two core tasks within the field of Natural Language Processing (NLP). In recent years, large language models (LLMs), exemplified by GPT-4 (OpenAI et al. 2024), Claude-3.5 (Anthropic 2024), and the LLaMA series (Meta 2024), have achieved remarkable breakthroughs in language generation, particularly excelling in producing human-like responses. The success of LLMs can be attributed to the instruction tuning (IT), which enables better alignment with human preferences. However, it is noteworthy that while these models demonstrate impressive performance in NLG tasks, their progress in NLU tasks has been comparatively limited (Li et al. 2023).

Instruction tuning represents the most direct and efficient approach to enhancing the performance of LLMs on specific tasks (Vilar et al. 2022). In the domain of NLU, instruction tuning necessitate high-quality, task-specific instruction data to produce the precise responses required by human. Data augmentation techniques offer a seemingly straightforward solution to expand instruction tuning data. However, previous studies have demonstrated that even minor degradation in data quality or the introduction of noise can lead to significant performance decrements in LLMs (Liu et al. 2023). The construction of specialized instruction tuning datasets for NLU tasks is resource-intensive, both in terms of time and cost, thereby limiting the potential improvements achievable through instruction tuning. This constraint underscores the critical need for developing more efficient and robust method to enhance the NLU performance of LLMs via instruction tuning. Such methods must strike a delicate balance between costs and performance gains, addressing the current limitations.

Recent research has shown that the quality of instructions can be compared using various metrics, such as calculating instruction perplexity and comparing instruction lengths (Zhang et al. 2023). Furthermore, researchers have discovered that fine-tuning LLMs using instructions of varying quality and correcting the output based on quality metrics, noisy instructions can also enhance the performance on downstream tasks (Zhao et al. 2024). However, these approaches do not simultaneously train on data of varying quality levels. Instead, they primarily rely on the mutual correction of different output results, with many methods being non-parametric. Consequently, the generalization capabilities of these techniques are inherently limited.

In pursuit of refining the behavior of LLMs, we have begun to explore the Semantic robustness, a widely recognized cognitive ability in cognitive science (Huang et al. 2021). Semantic robustness refers to the capacity of a language model or natural language processing system to maintain consistent and accurate semantic interpretations across diverse linguistic contexts, variations in input, and potential perturbations. Intriguingly, this cognitive principle has been demonstrated to extend to LLMs. For example, through effective instruction tuning, the outputs generated by LLMs can be steered towards completing specific task rather than understanding the task content. Building upon this foundational insights, we propose the following hypothesis: If LLMs can comprehend multiple noisy instructions and generate excepted outputs consistent with the raw instruction, then the efficacy of instruction tuning for specific tasks within LLMs can be significantly improved.

In this work, we propose Adversarial Noisy Instruction Tuning (ANIT), a novel method designed to enhance the performance of LLMs on NLU tasks with minimal additional costs related to instruction augmentation and parameter fine-tuning. The central premise of ANIT involves the deployment of noisy variants of instructions, coupled with the utilization of adversarial training during instruction tuning. Specifically, our approach introduces noise into the raw instructions in a controlled and diverse manner. Moreover, we have developed a new adversarial training strategy, termed the Noise Response Method, which adaptively adjusts the intensity of adversarial perturbations based on the quantification of semantic distortion in the input. Through this precise design, ANIT effectively mitigates the variability of noise in the instructions. This innovative method allows the loss from adversarial training to regularize the original loss function, consequently enhancing the comprehension capabilities of LLMs. Our main contributions are as follows:

• We introduce Adversarial Noisy Instruction Tuning (ANIT), a novel method that constructs noisy instructions, quantifies semantic distortion, and utilizes adversarial training in instruction tuning. Through noisy instruction tuning and noise response in adversarial training, LLMs focus on solving NLU tasks without paying attention to irrelevant noisy words of instructions.

• We evaluate the performance of LLMs fine-tuned by ANIT with various noisy instructions across four mainstream NLU tasks. Our results demonstrate that enhancements in semantic robustness directly contribute to performance improvements in downstream tasks. Remarkably, ANIT consistently shows notable performance gains across various models and tasks, regardless of whether the instructions are detailed or concise.

• We provide a comprehensive analysis of ANIT behavior, demonstrating its efficacy from various perspectives. ANIT reduces the high time overhead associated with fine-tuning LLMs using adversarial training. Moreover, the performance gain is maintained in cross-dataset and multitask applications.

# Related Work

# Instruction Tuning

A considerable corpus of research has illustrated that instruction tuning substantially augments the generalization capabilities of LLMs (Aw et al. 2024). Subsequent to instruction tuning, LLMs exhibit an improved ability to comprehend instructions, leading to notable enhancements in their performance on tuning tasks. Moreover, increasing the diversity and volume of instructions significantly enhances the performance of LLMs (Wang et al. 2023). However, the paucity of high-quality, diverse instructions, heavily dependent on labor-intensive manual annotation, poses a substantial challenge (Yin et al. 2023). Consequently, the prohibitive costs associated with constructing datasets for instruction tuning emerge as a bottleneck in the development of LLMs with enhanced applicability.

# Adversarial Training

Adversarial training is widely applied in deep learning, enhancing performance across various domains and tasks. Previous studies categorize adversarial training approaches into two types: single-step (Wong, Rice, and Kolter 2020) and multi-step (Zhang et al. 2019). Multi-step methods, such as PGD (Madry et al. 2018), FreeAT (Shafahi et al. 2019) and FreeLB (Zhu et al. 2020), achieve optimal adversarial perturbations through multiple iterations. However, due to the significant computational power and time required for both pre-training and fine-tuning LLMs, multi-step methods are not directly applicable. Single-step methods, for example, FGSM (Goodfellow, Shlens, and Szegedy 2015) and FGM (Miyato, Dai, and Goodfellow 2017), estimate adversarial perturbations and complete adversarial training in one step. While single-step methods save time and computational resources, they struggle to achieve the optimal effects of adversarial training.

# Method

In this section, we present Adversarial Noisy Instruction Tuning (ANIT), a method designed to enhance the performance of NLU tasks for LLMs. Figure 1 provides an overview of ANIT. The method has three core parts: (1) Noisy Instruction Construction (2) Semantic Distortion Quantification (3) Noise Response Method for Adversarial Training.

# Noisy Instruction Construction

In constructing noisy instructions, we have two primary objectives. First, we aim to create noisy instructions that induce semantic distortion, designed to elicit counter-intuitive yet plausible responses, thereby deviating from expected outcomes. Second, we strive to ensure that the noise within these instructions is controllable. This control prevents highintensity noise from undermining the effectiveness of instruction tuning and avoids a decline in the performance of LLMs.

Preliminary In our method, an instruction tuning data sample, represented as $\boldsymbol { X } = \{ I _ { r } , \boldsymbol { C } \}$ , contained two parts: raw instruction $I _ { r }$ and context $C$ . Context $C$ contains the question, format, and option for the specific task (Appendix A). Each sample $X$ corresponds to a target output $Y$ . The noisy instruction represents as $I _ { n }$ , a noisy instruction tuning data sample represented as $X _ { n } = \{ I _ { n } , C \}$ . In noise instruction tuning, we aim for the LLM outputs of $X$ and $X _ { n }$ to remain as consistent as possible.

Raw Instruction: $I _ { r }$ Semantic Distortion Quantification Adversarial Perturbation Here is a task on named entity Scaling Factor: β Upper Bound   
recognition, give the correct answer followed the input. REulveadlu-Bataisoend adjust Δβ V less than   
Noisy Instruction: In or Uniform sample AdvIneirtisalrial Adversarial   
Noise Type: Rand Replacement ELvMa-lBuatsieodn Distribution Pertuδrbation Pertuδrbation 𝑙 × ℎ 𝑙 × ℎ Here is a task assemble named entity recognition, give the forty answer followed the input. Noisy Instruction Data: $X _ { n }$ 𝑙 × ℎ + δ0 𝑳(𝒇𝜽 𝑿𝒏 + 𝜹𝟎 , 𝒚)   
Context: C + δ1 Question: Give a word that 𝑳(𝒇𝜽 𝑿𝒏 + 𝜹𝟏 , 𝒚) dweosrcdr. bFeosrtmheat:catTehgeoroyuotfptuhtef oernmtitayt 1st Step AT should be 'type1: word1; type2: cwaotredg2o; ... Roepctiognsi.ze Oopnltiyotnhse: 2nd Step AT EmLbaeydedring Embe𝑋d𝑛ding TrDanescfodremrer × 𝑛 𝑳(𝒇𝜽 𝑿𝒏 , 𝒚) in location, Forward without AT LLM with Instruction Tuning

Noise in Instructions Our methodology for constructing noisy instructions adheres to three core principles, incorporating six noise variants. These noise variants are elaborated upon in detail in Appendix A.

• Random Noise: To minimize the disruption of raw instructions by noisy data, we employed random noise as the foundational method. The construction of random noisy instructions comprises four methods: random truncation, random shuffling, random insertion, and random replacement. These methods of introducing randomness reflect common real-world challenges faced by LLMs. Typically, inputs to LLMs are susceptible to errors such as omissions, incorrect words, or altered sequences. The design of random noisy instructions aims to replicate these typical conditions.

• In-context Noise: In-context learning enhances the accuracy and relevance of generated outputs by exploiting similarities between inputs and their corresponding pre-trained datasets. For constructing in-context noisy instructions, we utilize a singular method: in-context padding. This approach involves embedding extraneous in-context data within raw instructions, aimed at disrupting the generative trajectory of LLMs and thereby introducing controlled noise.

• Opposite Noise: Opposite noise is achieved by instruction confusing on raw instructions. We address scenarios where instructions are ambiguous or polysemous, resulting in uncertainty for LLMs regarding the exact intent of execution. Such noise enhances the capability to interpret unclear directives of LLMs, discerning task objectives from ambiguity and thereby improving their adaptability.

Noise Strength Adjustment To adjust the strength of noise in the noisy instructions, we introduce a noise strength factor, denoted by $\omega$ . This factor is defined as the ratio relative to the length of the raw instruction. A larger value of $\omega$ indicates that an increased number of words will undergo replacement, insertion, padding, or other manipulation. Additionally, we established a global noise factor, denoted by $\phi$ , to determine the proportion of noisy instructions within a dataset. $\omega$ and $\phi$ are both hyperparameters. Details about $\omega$ and $\phi$ is provided in Appendix B.

# Semantic Distortion Quantification

To quantify semantic distortion, we employ two distinct approaches: Rule-Based Evaluation (RBE) and Language Model-Based Evaluation (LME). RBE is the fuzzy estimate of semantic distortions and LME is the precise estimate of semantic distortions.

Rule-Based Evaluation In RBE, we used the Levenshtein distance algorithm based on a dynamic programming implementation to evaluate the difference between the noisy instructions and raw instructions on the word level (Appendix F). The minimum number of single-word edits obtained according to the Levenshtein distance algorithm represents the distance $d$ between noisy instruction and the raw instruction. For a dataset of noisy instructions, quantification through RBE yields $D = \{ \dot { d _ { 1 } } , d _ { 2 } , \dots , d _ { n } \}$ . We applied Max-Min Normalization to this set (Eq. 1).

$$
d _ { i } ^ { ' } = \frac { d _ { i } - d _ { m i n } } { d _ { m a x } - d _ { m i n } }
$$

For one noisy instruction, we can quantify its semantic distortion as the value $d _ { i } ^ { ' }$ . For a noisy instruction dataset, we can obtain the semantic distortion value as $\beta$ (Eq. 2).

$$
\beta = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } d _ { i } ^ { ' } \quad d _ { i } ^ { ' } \in \boldsymbol { D } ^ { ' }
$$

Table 1: F1 Scores across NLU tasks: Performance of ANIT with detailed instructions on LLMs, semantic distortion assessed through rule-based evaluation. Best results are highlighted in bold; performance improvements are indicated by $\uparrow$ ; fine-tuned LLMs without ANIT as the baseline are indicated in underline.   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Method</td><td colspan="2">NER</td><td colspan="2">RE</td><td colspan="2">TC</td><td colspan="2">ABSA</td></tr><tr><td>ConLL03</td><td>Ontonotes</td><td>NYT</td><td>SciERC</td><td>SST2</td><td>AGNews</td><td>14Lap</td><td>14Rest</td></tr><tr><td rowspan="7">Gemma-2B</td><td>1</td><td>91.78</td><td>91.03</td><td>90.21</td><td>38.06</td><td>96.73</td><td>94.08</td><td>62.97</td><td>72.03</td></tr><tr><td>Rand Repl</td><td>92.77 个</td><td>91.47 ↑</td><td>92.00 个</td><td>40.05 个</td><td>97.02 个</td><td>95.79 个</td><td>63.04↑</td><td>73.14 个</td></tr><tr><td>Rand Trunc</td><td>92.32↑</td><td>91.83 个</td><td>91.25↑</td><td>39.51个</td><td>96.61</td><td>94.63↑</td><td>62.94 ↑</td><td>72.71个</td></tr><tr><td>Rand Ins</td><td>92.53个</td><td>91.39 个</td><td>91.93 个</td><td>38.88个</td><td>97.11 个</td><td>95.75个</td><td>63.01↑</td><td>72.50个</td></tr><tr><td>Rand Shuf</td><td>92.44 ↑</td><td>91.66 ↑</td><td>91.81 个</td><td>39.04 个</td><td>96.91↑</td><td>95.43↑</td><td>63.08↑</td><td>72.97 个</td></tr><tr><td>IC Pad</td><td>91.61 个</td><td>91.61</td><td>91.07 个</td><td>39.73 个</td><td>97.12 个</td><td>95.77 个</td><td>63.17 个</td><td>73.04 个</td></tr><tr><td>Opposite</td><td>92.07↑</td><td>91.87个</td><td>91.22 个</td><td>39.29个</td><td>96.88↑</td><td>95.05个</td><td>62.77</td><td>72.98 个</td></tr><tr><td rowspan="7">LLaMA2-7B</td><td></td><td>92.63</td><td>90.39</td><td>90.89</td><td>42.18</td><td>96.80</td><td>94.08</td><td>62.97</td><td>72.37</td></tr><tr><td>Rand Repl</td><td>93.64 个</td><td>91.94 ↑</td><td>92.74 个</td><td>45.36 个</td><td>97.18 个</td><td>95.88 个</td><td>64.72 个</td><td>74.21 个</td></tr><tr><td>Rand Trunc</td><td>93.01↑</td><td>91.89 ↑</td><td>92.21 个</td><td>44.50 ↑</td><td>97.01个</td><td>94.58↑</td><td>64.07↑</td><td>74.00 ↑</td></tr><tr><td>Rand Ins</td><td>93.26个</td><td>91.33 个</td><td>92.53 个</td><td>43.50 个</td><td>97.37个</td><td>94.75个</td><td>63.50 个</td><td>73.75 个</td></tr><tr><td>Rand Shuf</td><td>93.33 ↑</td><td>91.26 ↑</td><td>92.60 ↑</td><td>44.21 个</td><td>97.13 个</td><td>95.47个</td><td>63.98↑</td><td>74.15 个</td></tr><tr><td>IC Pad</td><td>93.45 ↑</td><td>91.78 ↑</td><td>92.47 个</td><td>45.14 个</td><td>97.19 个</td><td>95.73个</td><td>64.53↑</td><td>73.51个</td></tr><tr><td>Opposite</td><td>93.17个</td><td>91.97个</td><td>91.79 个</td><td>42.90↑</td><td>96.85个</td><td>94.95↑</td><td>63.75个</td><td>73.50个</td></tr><tr><td rowspan="7">LLaMA3-8B</td><td></td><td>92.71</td><td>91.53</td><td>91.57</td><td>48.84</td><td>97.40</td><td>95.02</td><td>64.37</td><td>73.84</td></tr><tr><td>Rand Repl</td><td>93.66 个</td><td>92.37 ↑</td><td>92.94 个</td><td>50.57 个</td><td>97.56 个</td><td>95.89 个</td><td>66.13 个</td><td>74.93 个</td></tr><tr><td>Rand Trunc</td><td>93.31↑</td><td>92.05 ↑</td><td>92.55个</td><td>50.13个</td><td>97.81 个</td><td>94.87</td><td>65.10个</td><td>74.72 个</td></tr><tr><td>Rand Ins</td><td>93.58↑</td><td>92.42 个</td><td>92.75↑</td><td>49.53个</td><td>97.61↑</td><td>94.99</td><td>65.47个</td><td>74.15 个</td></tr><tr><td>Rand Shuf</td><td>93.03↑</td><td>91.37</td><td>92.71个</td><td>49.21 个</td><td>97.56 个</td><td>95.88↑</td><td>65.48↑</td><td>74.34个</td></tr><tr><td>IC Pad</td><td>93.05↑</td><td>92.17 个</td><td>92.61↑</td><td>49.84 ↑</td><td>97.25</td><td>95.74 个</td><td>64.13</td><td>74.07个</td></tr><tr><td>Opposite</td><td>93.51个</td><td>92.23↑</td><td>92.04个</td><td>49.59↑</td><td>97.11</td><td>95.26个</td><td>65.59个</td><td>74.11个</td></tr></table></body></html>

The $\beta$ is used as the scaling factor to regulate the initial perturbation strength of adversarial training. A larger value of $\beta$ indicates increased semantic distortion and vice verse.

LM-Based Evaluation In LME, we used OpenAI API with text-embedding-3-large1 as the embedding model. text-embedding-3-large is a LM developed to measure text embedding similarity, and it is one of the best text embedding models currently available. We used text-embedding-3-large for embedding of noisy instructions and raw instructions as different inputs to get two embedding matrix. Finally, the cosine similarity algorithm is used to measure the semantic distortion of the two output embedding matrices. By representing the embedding model text-embedding-3-large as $E ( x )$ , semantic distortion on each noisy instruction can be quantified by $d _ { i } ^ { ' } = 1 - c o s ( E ( x _ { i n } ) , E ( x _ { i } ) )$ , the scaling factor $\beta$ can be derived (Eq. 3).

$$
\beta = \frac { 1 } { n } \sum _ { \begin{array} { c } { x _ { i n } \in X _ { n } , x _ { i } \in X } \end{array} } ( 1 - c o s ( E ( x _ { i n } ) , E ( x _ { i } ) ) )
$$

# Noise Response Method for Adversarial Training

The core idea of adversarial training (AT) is the process of modifying the training objectives by applying perturbation $\delta$ to the input and maximizing the adversarial loss (Eq. 4). Specifically, AT aims to find the most appropriate parameters $\theta$ to find the maximum disturbance $\delta$ within the standard norm ball and minimize the standard error with the output. $D$ is the data distribution, $y$ is the label, $\epsilon$ is perturbation bound and $L$ is the loss function.

$$
\operatorname* { m i n } _ { \theta } \mathbb { E } _ { ( Z , y ) \sim D } \left[ \operatorname* { m a x } _ { \| \delta \| \leq \epsilon } L ( f _ { \theta } ( X + \delta ) , y ) \right]
$$

The outer minimization problem can be solved by gradient descent. The inner maximization problem can be solved by running projected gradient descent on several negative loss functions. Specifically, the following steps are taken in each iteration (with a step size of $\alpha$ ):

$$
\begin{array} { c } { g _ { a d v } \gets \nabla _ { \boldsymbol { \delta } } L ( f _ { \theta } ( \boldsymbol { X } + \boldsymbol { \delta } _ { i } ) , \boldsymbol { y } ) } \\ { \delta _ { i + 1 } = \Pi _ { | | \boldsymbol { \delta } | | \le \epsilon } ( \delta _ { i } + \alpha \cdot g _ { a d v } / | | g _ { a d v } | | _ { F } ) } \end{array}
$$

where Eq. 5 is the gradient of the loss at the i-th step with respect to $\delta _ { i }$ , $| | g _ { a d v } | | _ { F }$ is the $\mathrm { \Delta F }$ -norm of the gradient, and $\Pi _ { | | \delta | | \leq \epsilon }$ is the projection of the performance onto the perturbation bound.

Traditional AT methods, such as the Fast Gradient Method (FGM) (Miyato, Dai, and Goodfellow 2017) and Free Large-Batch (FreeLB) (Zhu et al. 2020), are the straightforward techniques that improves the performance of neural networks. However, FGM as a single-step adversarial training approach often falls short in precision of adversarial perturbation estimation, leading to sub-optimal performance on LMs compared to FreeLB. While FreeLB is effective for BERT-based language models (Devlin et al. 2019), it requires multiple adjustments of adversarial perturbations within a batch, resulting in significant additional training overhead that limits its widespread applicability in LLMs.

Table 2: F1 Scores across NLU tasks: Performance of ANIT with concise instructions on LLMs, semantic distortion assessed through rule-based evaluation. Best results are highlighted in bold; performance improvements are indicated by $\uparrow$ ; fine-tuned LLMs without ANIT as the baseline are indicated in underline.   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Method</td><td colspan="2">NER</td><td colspan="2">RE</td><td colspan="2">TC</td><td colspan="2">ABSA</td></tr><tr><td>ConLL03</td><td>Ontonotes</td><td>NYT</td><td>SciERC</td><td>SST2</td><td>AGNews</td><td>14Lap</td><td>14Rest</td></tr><tr><td rowspan="7">Gemma-2B</td><td></td><td>91.59</td><td>90.87</td><td>90.09</td><td>37.74</td><td>96.78</td><td>93.98</td><td>62.64</td><td>71.73</td></tr><tr><td>Rand Repl</td><td>92.24 个</td><td>91.33个</td><td>91.86 个</td><td>39.46 个</td><td>96.90 ↑</td><td>95.68 个</td><td>62.98 个</td><td>72.74 个</td></tr><tr><td>Rand Trunc</td><td>92.14 个</td><td>91.79 个</td><td>90.96个</td><td>39.27个</td><td>96.74</td><td>94.33↑</td><td>62.53</td><td>72.34 个</td></tr><tr><td>Rand Ins</td><td>92.23 个</td><td>91.01 个</td><td>91.71 个</td><td>38.83个</td><td>97.04 个</td><td>95.26 ↑</td><td>62.79 ↑</td><td>72.15 个</td></tr><tr><td>Rand Shuf</td><td>91.97 个</td><td>91.23 ↑</td><td>91.64 个</td><td>39.14 个</td><td>96.85↑</td><td>95.31个</td><td>62.83↑</td><td>72.59 个</td></tr><tr><td>IC Pad</td><td>91.96 个</td><td>91.74个</td><td>89.74 个</td><td>39.33 个</td><td>96.93↑</td><td>95.46 ↑</td><td>62.94 个</td><td>72.48 个</td></tr><tr><td>Opposite</td><td>91.32</td><td>91.78个</td><td>90.97个</td><td>38.29↑</td><td>96.59个</td><td>94.79个</td><td>62.37</td><td>72.65个</td></tr><tr><td rowspan="7">LLaMA2-7B</td><td></td><td>92.35</td><td>90.06</td><td>90.56</td><td>41.80</td><td>96.67</td><td>93.82</td><td>62.68</td><td>71.17</td></tr><tr><td>Rand Repl</td><td>93.10 个</td><td>91.95 个</td><td>92.18 个</td><td>44.40 个</td><td>97.24 个</td><td>95.64 个</td><td>64.26 ↑</td><td>74.25个</td></tr><tr><td>Rand Trunc</td><td>92.86↑</td><td>91.78 ↑</td><td>92.03 个</td><td>43.69 ↑</td><td>97.17个</td><td>94.36↑</td><td>63.98↑</td><td>74.37 个</td></tr><tr><td>Rand Ins</td><td>92.95 个</td><td>91.35 个</td><td>92.26 ↑</td><td>44.18 个</td><td>96.99 ↑</td><td>95.27个</td><td>65.14 个</td><td>73.78个</td></tr><tr><td>Rand Shuf</td><td>93.19个</td><td>91.76 个</td><td>92.01 ↑</td><td>43.91 个</td><td>97.06 个</td><td>94.75 个</td><td>64.27个</td><td>73.93个</td></tr><tr><td>IC Pad</td><td>92.73 ↑</td><td>91.13个</td><td>92.15 个</td><td>43.75 个</td><td>96.83 ↑</td><td>94.81↑</td><td>64.68↑</td><td>73.59 个</td></tr><tr><td>Opposite</td><td>92.73个</td><td>91.88↑</td><td>91.76个</td><td>42.67↑</td><td>96.72个</td><td>94.05个</td><td>63.86↑</td><td>74.13个</td></tr><tr><td rowspan="6">LLaMA3-8B</td><td></td><td>92.48</td><td>91.46</td><td>91.23</td><td>48.17</td><td>97.01</td><td>94.64</td><td>64.49</td><td>73.88</td></tr><tr><td>Rand Repl</td><td>93.31 个</td><td>92.04 个</td><td>92.78 个</td><td>50.00 个</td><td>97.48个</td><td>95.78 ↑</td><td>65.89 个</td><td>74.59 个</td></tr><tr><td>Rand Trunc</td><td>93.03个</td><td>91.72 个</td><td>92.35个</td><td>49.76 个</td><td>97.77 个</td><td>94.88个</td><td>64.55</td><td>74.39 ↑</td></tr><tr><td>Rand Ins</td><td>93.21个</td><td>92.00 ↑</td><td>92.64↑</td><td>49.41 个</td><td>97.47个</td><td>94.23个</td><td>65.07个</td><td>73.94个</td></tr><tr><td>Rand Shuf</td><td>93.13↑</td><td>91.93 个</td><td>92.59↑</td><td>49.19 个</td><td>97.26个</td><td>95.88 个</td><td>64.04</td><td>74.27 个</td></tr><tr><td>IC Pad</td><td>92.84 ↑</td><td>91.29</td><td>92.77 个</td><td>49.30 ↑</td><td>96.85</td><td>95.54个</td><td>65.45个</td><td>73.56</td></tr><tr><td></td><td>Opposite</td><td>92.17</td><td>91.71个</td><td>91.91个</td><td>49.48个</td><td>97.64个</td><td>94.97个</td><td>65.19个</td><td>74.05个</td></tr></table></body></html>

We propose a novel adversarial training method, the Noise Response Method (NRM). NRM can accomplish a dynamic adversarial training that requires only two steps based on the semantic distortion quantification value. NRM adaptively initializes adversarial perturbations based on the scaling factor $\beta$ derived from the quantified semantic distortion of the input, achieving adversarial training in just two steps.

$$
\delta _ { 0 } \gets \frac { 1 0 } { \sqrt { N _ { \delta } } } U ( - \epsilon , \epsilon ) \cdot ( 1 - \beta )
$$

Algorithm 1: Noise Response Method for AT   

<html><body><table><tr><td>cient α, scaling coefficient β 1:Initialize LLM parameters 0 2:for epoch=1...N do 3: Semantic distortion evaluation: 4: β=RBE(Xn,X) orLME(Xn,X) 5: for minibatchBCXn do 6: δ0←U（-66)-(（1-β） 7: gadv ← ∀δL(fe(Xn +δo),y) 8: 9: end for 10: L=L(fe(Xn),y)+λL(fe(Xn +δi),y) 11: ge=VL</td><td>Require: Noisy instruction tuning sample Xn = (In,C) perturbation bound ε,learning rate T,adversarial coeffi δ1 = II|s|/≤e(δo + α : gadu/llgadullF)</td></tr></table></body></html>

In the first step of the adversarial process, $\delta _ { 0 }$ is sampled from a uniform distribution $U ( - \epsilon , \epsilon )$ and subsequently scaled by a factor $\beta$ (Eq. 7), where $N _ { \delta }$ denotes the dimension of the $\delta$ , $\frac { 1 0 } { \sqrt { N _ { \delta } } }$ and $( 1 - \beta )$ together form a scaling factor. This scaling mechanism modulates the intensity of AT: it reduces the training intensity when the noise is excessive and increases it when the noise is minimal. Such adaptability ensures the effectiveness of adversarial training and enhances the robustness of LLMs to various noise strength during instruction tuning.

In the second step of the adversarial process, we enforce a constraint ensuring that the perturbation $\delta _ { 1 }$ does not exceed the upper bound, denoted by $\epsilon$ . This constraint is critical for the effectiveness of AT (Goodfellow, Shlens, and Szegedy 2014). The hyperparameter $\alpha$ , which acts as the adversarial coefficient, is employed to regulate the extent of the perturbation applied during training.

In the loss function (Eq. 8), the loss from AT is added as a regularization term to the standard loss for instruction tuning. $\lambda$ is a hyperparameter to adjust the loss.

$$
L = L ( f _ { \theta } ( X _ { n } ) , y ) + \lambda L ( f _ { \theta } ( X _ { n } + \delta _ { 1 } ) , y )
$$

# Experimental Setup

# Datasets

We conduct experiments on four representative NLU tasks: Named Entity Recognition (NER), Relationship Extraction (RE), Text Classification (TC) and Aspect-based Sentiment Analysis (ABSA). For each task, we employ two datasets: Ontonotes (Hovy et al. 2006) and CoNLL2003 (Tjong

Table 3: F1 Scores for NLU Tasks: The comparative results of ANIT performance with detailed instructions on LLMs. This comparison includes different Semantic Distortion (SD) Evaluation Methods: Rule-Based and LM-Based. Performance improvements are indicated in parenthesis, fine-tuned LLMs without ANIT as the baseline are indicated in underline.   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">SDEvaluation Method</td><td colspan="2">NER</td><td colspan="2">RE</td><td colspan="2">TC</td><td colspan="2">ABSA</td></tr><tr><td>ConLL03</td><td>Ontonotes</td><td>NYT</td><td>SciERC</td><td>SST2</td><td>AGNews</td><td>14Lap</td><td>14Rest</td></tr><tr><td rowspan="3">Gemma-2B</td><td></td><td>91.78</td><td>91.03</td><td>90.21</td><td>38.06</td><td>96.73</td><td>94.08</td><td>62.97</td><td>72.03</td></tr><tr><td>Rule-Based</td><td>92.77(+0.99)</td><td>91.83(+0.80)</td><td>92.00(+1.79)</td><td>40.05(+1.99)</td><td>97.12(+0.39)</td><td>95.79(+1.71)</td><td>63.17(+0.20)</td><td>73.14(+1.11)</td></tr><tr><td>LM-Based</td><td>92.88(+1.10)</td><td>91.98(+0.95)</td><td>92.12(+1.91)</td><td>40.25(+2.19)</td><td>97.21(+0.48)</td><td>96.13(+2.05)</td><td>63.14(+0.17)</td><td>73.40(+1.37)</td></tr><tr><td rowspan="3">LLaMA2-7B</td><td></td><td>92.63</td><td>90.39</td><td>90.89</td><td>42.18</td><td>96.80</td><td>94.08</td><td>62.97</td><td>72.37</td></tr><tr><td>Rule-Based</td><td></td><td>93.64(+1.01) 91.97(+1.58)</td><td></td><td></td><td></td><td>92.74(+1.85） 45.36(+3.18)97.19(+0.39) 95.88(+1.80)</td><td>64.72(+1.75)</td><td>74.21(+1.84)</td></tr><tr><td>LM-Based</td><td>93.94(+1.31)</td><td>92.03(+1.64)</td><td>92.92(+2.03)</td><td>45.53(+3.35)</td><td>97.49(+0.69)</td><td>96.02(+1.94)</td><td>64.69(+1.72)</td><td>74.33(+1.96)</td></tr><tr><td rowspan="3">LLaMA3-8B</td><td></td><td>92.71</td><td>91.53</td><td>91.57</td><td>48.84</td><td>97.40</td><td>95.02</td><td>64.37</td><td>73.84</td></tr><tr><td>Rule-Based</td><td>93.66(+0.95)</td><td>92.42(+0.89)</td><td>92.94(+1.37)</td><td>50.57(+1.73)</td><td>97.81(+0.41)</td><td>95.69(+0.67)</td><td>66.13(+1.76)</td><td>74.93(+1.09)</td></tr><tr><td>LM-Based</td><td>93.79(+1.08)</td><td>92.82(+1.29)</td><td>93.58(+2.01)</td><td></td><td>50.66(+1.82) 98.16(+0.76)</td><td>95.87(+0.85) 66.16(+1.79)</td><td></td><td>75.26(+1.42)</td></tr></table></body></html>

Kim Sang and De Meulder 2003) for NER; SciERC (Luan et al. 2018) and NYT (Riedel, Yao, and McCallum 2010) for RE; SST2 (Socher et al. 2013) and AGNews (Zhang, Zhao, and LeCun 2015) for TC; 14Lap and 14Rest (Xu et al. 2020) for ABSA.

We collect instructions for each dataset from Alpaca (Taori et al. 2023). Instructions for each task are presented in two versions: concise and detailed. The concise instructions contain only the target of the task. The detailed instructions provide step-by-step guidance for completing the task. Examples prompts for each task are presented in Appendix A.

# Models

In our study, we use Gemma-2B, LLaMA2-7B and LLaMA3-8B for our experiments. These models cover the parameter ranges commonly employed in LLMs. LLaMA3 represents an improvement over LLaMA2, which is achieved through the use of expanded pre-training data and an augmented vocabulary. In our experiments, we employ greedy decoding for these models.

# Evaluation Metrics

We examine the output of LLMs by employing ANIT on NLU tasks. Task performance is measured using using Micro-F1 (Manning, Raghavan, and Schu¨tze 2008). We employ different evaluation settings on generated tokens for each task. we prove evaluation details in Appendix C.

# Fine-tuning Experimental Setup

To evaluate the effectiveness of ANIT on all models, we fine-tune models using LoRA (Hu et al. 2022), a parameterefficient fine-tuning method. We conduct experiments both on concise instructions and detailed instructions. We prove implementation details in Appendix C.

# Results

# ANIT on Detailed Instructions

Table 1 presents the results of applying Adversarial Noisy Instruction tuning (ANIT) to LLMs on detailed instructions.

The results demonstrate that the application of ANIT consistently enhances the performance of LLMs across diverse NLU tasks, surpassing the baseline that solely employ standard instructions. In the majority of cases, the introduction of different noise yields performance improvements, with the ‘Rand Replacement’ noise proving to be the most effective. Few noise lead to performance degradation on specific tasks with detailed instructions, indicating the existence of potential limitations in fine-tuning. This phenomenon may be attributed to the pre-training strategies of specific LLM and the construction of instructions.

# ANIT on Concise Instructions

Table 2 presents an extended analysis of the efficacy of ANIT in LLMs applied to concise instructions. The inherent brevity and limited information content of concise instructions tend to exacerbate semantic distortions when noise is introduced during instruction tuning. Despite these challenges, we observed that ANIT with concise instructions continues to improve the performance of LLMs on NLU tasks. In contrast to the complexity of detailed instructions, the semantic distortions arising from concise instructions are more straightforward, eliciting a more direct adversarial effect. Consequently, a portion of the noise that demonstrate sub-optimal performance with detailed instructions exhibit improvement when fine-tuning with concise instructions.

# Analyzing Different Evaluation Methods to Semantic Distortion

When employing RBE to quantify semantic distortion in ANIT, we observe substantial performance improvements across various NLU tasks using LLMs in Table 1 and 2. This suggests that rule-based evaluation effectively captures semantic distortion, leading to more refined models. To further investigate the impact of semantic distortion quantification on ANIT performance, We select the best-performing noise configurations obtained from ANIT using various LLMs and conduct experiments using the LM-based evaluation. By focusing on the noise configurations that yield the highest performance gains in ANIT, we can better assess the effectiveness of the LM-based evaluation. As shown in Table 3, our findings indicate that the LM-based evaluation results in greater performance gains. This suggests that LM-based evaluation is more effective in quantifying semantic distortion and enhancing the robustness of LLMs fine-tuning using ANIT for NLU tasks.

![](images/ae2bb5cdd925ec05f2f6d6c460e90eaa1103c1ae7aea78f071adec53f70eb94e.jpg)  
Figure 2: (a): Average F1 scores for various noise strength factors $\omega$ are calculated for each LLM fine-tuned with ANIT and evaluated on four NLU tasks. The horizontal lines, color-coded to match each LLM, indicate baseline performance. (b): Time cost and task performance comparison between FreeLB, PGD, FGSM, and ANIT. ANIT uses ‘Random Replacement’ noise, while others show results from the best 5-round adversarial training. (c): Time-performance comparison between ANIT and other methods.

# Ablation Study Generalization Capabilities of ANIT

To evaluate the adaptability and effectiveness of the finetuned LLMs by ANIT, we conducted a cross-evaluation wherein an LLM trained on one task was subsequently trained and tested with a limited number of samples on another task. Specifically, a model trained on the CoNLL2003 dataset was retrained and tested with few samples from the Ontonotes dataset. The results, detailed in Table 4, show the F1 scores on these tasks using LLaMA2-7B. Across various few-shot training data scenarios, the model consistently outperforms the baseline. Notably, when using only $5 \%$ of the training data, the LLM demonstrates an average performance improvement of $2 . 4 7 \%$ . These findings suggest that ANIT enhances both the generalization ability and robustness of LLMs.

# Sensitivity of Noisy Factor in Instructions

Figure 2a illustrates the impact of the noise strength factor $\omega$ on our method’s performance. This parameter adjusts the semantic distortion caused by noisy instructions in ANIT. While we typically use $\omega$ values between 0.1 and 0.3, we assessed ANIT with $\omega$ ranging from 0.1 to $0 . 5 \mathrm { ~ i n ~ } 0 . 1$ increments. Using the top-performing noise configurations, we calculated the average F1 score across all tasks. Performance declines when $\omega$ exceeds 0.3 but remains above the baseline at $\omega = 0 . 1$ . Overall, ANIT enhances model performance within the $\omega$ range of 0.1 to 0.5, showing strong stability between 0.1 and 0.3.

# On the Utility of ANIT over Classic Adversarial Training

In the ANIT method, we introduce the Noise Response Method (NRM), a novel adversarial training methodology which employs a two-step adversarial process. Compared to direct instruction tuning of LLMs, NRM indeed incurs additional time costs. To comprehensively assess these costs and their impact on performance, We compare NRM with common adversarial training methods such as FreeLB, PGD, and FGSM. As shown in Figure 2b, 2c, although ANIT introduces additional time costs, it is more time-efficient than the other methods and significantly outperforms them in terms of performance. This finding supports our hypothesis that traditional adversarial training methods often struggle to improve model performance across varied LLM applications.

Table 4: F1 scores obtained by cross-validating the performance of LLaMA2-7B on two datasets.   

<html><body><table><tr><td>Model</td><td>ConLL03</td><td>Ontonotes</td></tr><tr><td>Baseline</td><td></td><td></td></tr><tr><td>1%</td><td>78.74</td><td>74.31</td></tr><tr><td>5%</td><td>86.98</td><td>78.69</td></tr><tr><td>10%</td><td>87.40</td><td>82.75</td></tr><tr><td>ANIT</td><td></td><td></td></tr><tr><td>1%</td><td>81.32</td><td>75.84</td></tr><tr><td>5%</td><td>89.26</td><td>81.43</td></tr><tr><td>10%</td><td>90.45</td><td>84.67</td></tr></table></body></html>

# Conclusion

In this paper, we investigate how to improve the performance of LLMs on NLU tasks by enhancing their semantic robustness without the need for annotating high-quality instructions. We propose the Adversarial Noisy Instruction Tuning (ANIT), which significantly improves the ability of LLMs to process complex context. Through extensive analyses, we find that the performance gains obtained using ANIT can be maintained across datasets and tasks. This provides new insights and methods for the application of LLMs in various downstream tasks.