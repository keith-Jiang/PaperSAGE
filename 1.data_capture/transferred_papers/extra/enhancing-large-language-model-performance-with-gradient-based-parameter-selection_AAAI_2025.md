# Enhancing Large Language Model Performance with Gradient-Based Parameter Selection

Haoling $\mathbf { L i } ^ { 1 \ast }$ , Xin Zhang2†, Xiao Liu2, Yeyun $\mathbf { G o n g } ^ { 2 }$ , Yifan Wang1, Qi Chen2, Peng Cheng

1Tsinghua University 2 Microsoft Research li-hl23@mails.tsinghua.edu.cn, xinzhang3 $@$ microsoft.com

# Abstract

Large language models (LLMs) have revolutionized numerous fields of research, driving significant advancements in natural language processing, machine translation, and beyond. Although the extensive number of parameters contributes a lot to the great success, existing studies indicate that not all model parameters hold equal importance, which further leads to redundancy during the parameter update process. Recent works for reducing redundant parameter updates for LLMs either lack task-specific data information, may leading to suboptimal model performance, or discard transformer components or insignificant parameters, limiting the model’s scalability across different tasks and potentially compromising the LLM structure. To address these issues and further enhance the performance of LLMs, we propose GradientMask Tuning (GMT), a method that selectively updates parameters based on gradient information, which is specific to the target tasks. Specifically, after calculating gradients during back propagation, we measure their absolute values and mask those with small absolute values. Our empirical results in various training paradigms like SFT and DPO for various domains of tasks demonstrate that GMT not only preserves the original network structure but also enhances the potential performance of LLMs. Further analysis indicates that GMT exhibits insensitivity to mask ratio and possesses computational efficiency comparable to vanilla training approach.

# Introduction

Large language models (LLMs) have pivoted the centerpiece of various tasks such as textual understanding, natural language planning and instruction following (Achiam et al. 2023; Touvron et al. 2023; Jiang et al. 2023; Wang et al. 2024b; Luo et al. 2024a), which can be attributed to their extensive number of parameters, with performance and unique abilities significantly enhancing as the number of parameters increases. To optimize the performance of LLMs in downstream tasks, full-parameter fine-tuning is commonly employed by researchers and practitioners.

However, a recent trend shows an observable phenomena that not all parameters of LLMs hold the same importance, which leads to redundancy during the parameter update process (Huang et al. 2024; Jiang et al. 2024; Yu et al. 2023b; Luo et al. 2024b). The existence of the redundancy of LLM parameter updates can be proven by several branches of works, like model pruning methods (Ma, Fang, and Wang 2023), fine-tuning methods dropping the delta parameters (Yadav et al. 2024; Yu et al. 2023b), and so on, which shows minimal impact on performance. Furthermore, inspired by recent works (Liu et al. 2024b; Lv et al. 2024), we believe that reducing redundancy in parameter updates has the potential to enhance model performance.

Existing methods for reducing the redundancy of LLM parameter updates can be broadly classified into two categories based on their implementation. The first involves sparsifying the network during the training process using criteria such as randomness (Woo et al. 2024; Hui et al. 2024). However, a significant drawback of these methods is their failure to leverage information from task-specific data, may leading to suboptimal optimization objectives (Chen et al. 2024). The second focuses on eliminating nonessential structural components of the transformer block (Men et al. 2024; Song et al. 2024) or discarding insignificant parameters (Yadav et al. 2024; Yu et al. 2023b). Yet, these one-off operations constrain the model’s scalability across different tasks and impede its ability to adjust or reconfigure its computational complexity, which is crucial for adapting to varying task demands.

To address the above issues, a method that effectively identifies and retains the most critical LLM parameters during training is needed to reduce redundant updates and enhance overall model performance. In this work, we introduce the Gradient-Mask Tuning (GMT), a method that selects the set of parameters to be updated based on the gradient information associated with task-specific data. Specifically, after calculating the gradients in the backward process, we measure the absolute values of gradients and mask those with small values. There are several reasons for using gradient magnitude as a criterion. Firstly, gradients inherently capture task-specific information and can be utilized without additional computational overhead during training. Secondly, we assert that the gradient effectively assesses network parameter significance across different training datasets, allowing delicate control of the training process. Thirdly, identifying and removing insignificant subsets of gradients during the training process, which does not have a drastic impact on the convergence of the model, and may even lead to a regularization effect that improves the performance of the model (Hoefler et al. 2021).

To evaluate the effectiveness of GMT, we provide theoretical analysis and conduct experiments with both Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) training paradigms on several widely-used benchmarks on math reasoning, code generation, and general domain using various base models. The experimental results demonstrate that GMT exploits the gradient information to identify more important network parameters for sparse updating with acceptable extra time spent. This approach enhances model performance and reduces redundant parameter updates compared to several established baseline methods, including vanilla fine-tuning, Drop, and RMT. Furthermore, it illustrates that GMT is not sensitive to mask ratio selection and is more effective than random parameter updating. We further analyze the computational FLOPs and time efficiency of GMT, demonstrating that it enables a plug-and-play replacement of vanilla SFT without destroying the network architecture.

Our contributions can be summarized as:

• We propose GMT, which masks the gradients with small absolute values and discriminates the importance of parameters during training, naturally utilizing the information of task-specific data. This approach effectively reduces redundant parameter updates to the LLM and enhances performance on downstream tasks. • We conduct both theoretical analysis and exhaustive experiments with different base models on several benchmarks comparing with representative baselines. The results confirm the effectiveness of GMT. • We further demonstrate the adaptability, robustness and efficiency of the GMT by analyzing the drop strategy, mask ratio and time efficiency.

# Related Works

Reducing redundant parameter updates in LLMs is a significant research problem (Dalvi et al. 2020). Addressing this issue can prevent the waste of computational resources and optmize the model training process. We categorized the reduction of redundant parameter updates into post-processing and in-training based on the stage of implementation. This section describes the properties of the typical methods of both strategies as well as the respective drawbacks.

Post-processing Strategy The post-processing strategy, although requiring tuning with full parameter participation, achieves the same purpose as in-training through a series of tuned parameter drop and reset strategies. Inspired by dropout, mixout (Lee, Cho, and Kang 2019) stochastically mixes source and target parameters as a regularization technique and extends to fine-tune downstream tasks. DARE (Yu et al. 2023b) finds that setting most $90 \%$ or even $9 9 \%$ ) of the delta parameters to zero does not diminish the ability to fine-tune the LLM, and extends to model fusion accordingly. Ties-merging (Yadav et al. 2024) describes the delta parameter as a type of task vector that trims it, and then averages the parameters that are identical for multiple model aggregation symbols to achieve model merging. ExPO (Zheng et al. 2024) is inspired by model interpolation (Wortsman et al. 2022) and obtains better aligned models by direct extrapolation from the parameters of the aligned model obtained by DPO or RLHF and the initial SFT model. Although these methods can be implemented directly on fine-tuned models that are open access and require little additional computational overhead, they never achieve optimal performance due to the lack of refinement in the training process. Some methods attempt to directly remove redundant layers (Men et al. 2024) or components of transformer blocks (Song et al. 2024) in LLMs based on quantitative criteria. However, these methods compromise the structural integrity of LLMs, hindering their applicability across a wider range of scenarios.

In-training Strategy The in-training strategies aim to dynamically mitigate the impact of model parameter redundancy during the training process. DropBP (Woo et al. 2024) calculates the sensitivity of each layer to allocate an appropriate dropout rate, thereby randomly dropping certain layers during backpropagation to improve efficiency and reduce redundancy. HFT (Hui et al. 2024) randomly selects half of parameters for learning new tasks while freezing the remaining parameters to preserve previously learned knowledge, thus eliminating the redundancy of parameter knowledge across different learning stages. PAFT (Pentyala et al. 2024) separately conducts the SFT and DPO training processes, using L1 regularization to sparsify delta parameters and reduce redundancy, thereby achieving alignment and parallel training. These methods either employ randomness or regularization during the training process and lack guidance from task-specific data. Our GMT is focused on the taskspecific data information and fine-grained tuning of parameters, which not only preserves the model’s generalizability but also significantly improves its adaptability and performance on specific tasks.

# Methodology

In this section, we elucidate the principle underlying the GMT method, providing a comprehensive explanation through mathematical formulation and theoretical analysis. Figure 1 presents the framework of our method and compares it with a one-off drop strategy for delta parameters (i.e., the difference between the fine-tuned parameters and the pre-trained parameters). Specifically, GMT effectively identifies the most critical parameters to be updated during training by the absolute magnitude of the gradient. This dynamic selection strategy prevents redundant parameter updates, thereby improving the performance of LLM.

# Gradient-Mask Tuning

To decrease redundant updates by leveraging the information of task-specific data during training, while preserving the integrity of the original LLM architecture, we propose GMT for selecting critical parameters.

For each batch of training data, we compute the gradient of the loss function $\mathcal { L } ( \Theta )$ with respect to the parameters.

![](images/f440854579dd27f4d27b33bcc3c51359a81f70b001417cb1c49a3db8855d430a.jpg)  
$\theta ^ { \prime }$ : Fine-tuned parameters, $\boldsymbol { \Theta }$ :Pretrained parameters, $\theta _ { \varDelta }$ : Delta parameters.   
Figure 1: Illustration of our proposed method GMT, compared with the one-off drop delta parameters approach. The figure on the left delineates the distinction between a trivial drop and a random drop, wherein the trivial drop serves to diminish the redundant updates that arises during the fine-tuning process. Building upon this insight, we refine the training procedure by preferentially updating more significant parameters, as determined by the gradient information pertinent to the task-specific data. This selective updating is operationalized through the implementation of a masking strategy that filters out gradients with smaller absolute values.

To facilitate gradient accumulation, we define an accumulation interval $N$ , representing the number of mini-batches over which gradients are accumulated before performing a parameter update. The accumulated gradient $\Gamma _ { i j }$ for each parameter $\theta _ { i j }$ is defined as:

$$
\Gamma _ { i j } = \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \nabla _ { \theta _ { i j } } \mathcal { L } ( \Theta , \mathcal { B } _ { n } )
$$

where $\boldsymbol { B _ { n } }$ represents the $n$ -th mini-batch of data and $\nabla _ { \theta _ { i j } } \mathcal { L } ( \Theta , B _ { n } )$ is the gradient of the loss function with respect to $\theta _ { i j }$ for the mini-batch.

Upon accumulating the gradients over $N$ mini-batches, we employ gradient information as a signal to identify the importance of parameters at the element-wise level in a finegrained manner. This operation involves sorting the components of each accumulated gradient $\Gamma _ { i j }$ by their absolute values and selecting a pre-defined top percentile $k$ for updating the parameters. The masked gradient $\mathcal { M } ( \Gamma _ { i j } , k )$ is calculated as:

$$
\mathcal { M } ( \Gamma _ { i j } , k ) = \{ g _ { i j } \ | \ g _ { i j } \in \Gamma _ { i j } , \ | g _ { i j } | \ge T _ { k } \}
$$

where $g _ { i j }$ represents the $i j$ -th component of the accumulated gradient for parameter $\theta _ { i j }$ . The threshold $T _ { k }$ is the value such that the absolute values of the components of $\Gamma _ { i j }$ that are greater than or equal to $T _ { k }$ fall within the top $k$ percentile of all components by magnitude.

The subsequent parameter update step utilizes the masked gradient $\mathcal { M } ( \Gamma _ { i j } , k )$ :

$$
\boldsymbol { \theta } _ { i j } ^ { ( t + 1 ) } = \boldsymbol { \theta } _ { i j } ^ { ( t ) } - \eta \cdot \boldsymbol { \mathcal { M } } ( \Gamma _ { i j } , k )
$$

Hyperparameters: The accumulation interval $N$ , the training step $T$ and the learning rate $\eta$ .   
Input: The initial model parameters $\Theta ^ { ( 0 ) }$ and the $n$ -th minibatch of data $B _ { n }$ .   
Output: The updated model parameters $\Theta ^ { ( T ) }$ .   
1: for $t$ in $0  T - 1$ do   
2: $\Gamma  0$   
3: for $n$ in $1  N$ do   
4: $\Gamma  \Gamma + \nabla _ { \Theta } \mathcal { L } ( \Theta ^ { ( t ) } , \boldsymbol { B } _ { n } )$   
5: end for   
6: $\textstyle \Gamma \gets { \frac { 1 } { N } } \Gamma$   
7: $T _ { k } \gets$ Percentile threshold based on $\Gamma$   
8: $\mathcal { M } ( \Gamma , k )  \{ g _ { i j } \mid g _ { i j } \in \Gamma _ { i j } , \vert g _ { i j } \vert \geq T _ { k } \}$   
9: for $\theta _ { i j } \in \Theta$ do   
10: if $| \bar { \Gamma } _ { i j } | \geq T _ { k }$ then   
11: $\begin{array} { r } { \dot { \theta } _ { i j } ^ { ( i + 1 ) }  \theta _ { i j } ^ { ( t ) } - \eta \cdot \Gamma _ { i j } } \end{array}$   
12: end if   
13: end for   
14: end for $\scriptstyle = 0$

where $\eta$ is the learning rate and $t$ indexes the current training step. The detailed algorithmic training procedure is given in Algorithm 1.

# Theoretical Analysis

We present the theoretical analysis in two parts. First, we explain from the optimization perspective $\mathrm { F u }$ et al. 2023; Hui et al. 2024) why removing unimportant parameter updates can enhance model performance, by fine-tuning partial parameters to optimize the upper bound of the original loss function in Appendix. Then, we discuss the theoretical analysis of the absolute value of the gradient used in GMT , demonstrating that this absolute value can effectively signify the importance of the parameter.

In order to consider the impact of task-specific data on the importance of each parameter in a model, we identify and update only a sparse subset of parameters based on their impact on the loss function. We define the loss function $L ( { \bar { \Theta } } ; { \mathcal { D } } )$ over the dataset $\mathcal { D }$ as:

$$
\mathcal { L } ( \Theta ; \mathcal { D } ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \ell \big ( \Theta ; \big ( \mathbf { x } _ { i } , y _ { i } \big ) \big )
$$

where $\ell$ represents the loss for a single data point, with $\mathbf { x } _ { i }$ and $y _ { i }$ denoting the input features and corresponding label, respectively. The full parameter set is denoted by $\Theta$ .

To discern the impact of individual parameters $\theta _ { i j }$ on the loss function $\mathcal { L } ( \Theta ; \bar { \mathcal { D } } )$ , we consider the impact of their removal while keeping all other parameters constant. The differential effect of excluding a parameter $\theta _ { i j }$ is quantified by the change in loss $\Delta \mathcal { L } _ { i j } ( \Theta ; \mathcal { D } )$ , which is formulated as:

$$
\Delta \mathcal { L } _ { i j } ( \Theta ; \mathcal { D } ) = \mathcal { L } ( \mathbf { I } \odot \Theta ; \mathcal { D } ) - \mathcal { L } ( ( \mathbf { I } - \mathcal { E } _ { i j } ) \odot \Theta ; \mathcal { D } )
$$

where $\mathbf { I }$ represents the identity matrix and $\mathcal { E } _ { i j }$ denotes an indicator matrix that has the same dimensions as $\Theta$ . In $\mathcal { E } _ { i j }$ , all elements are zero except for the $( i , j )$ element, which is one. The Hadamard product, indicated by $\odot$ , performs an element-wise multiplication, isolating the effect of the single parameter $\theta _ { i j }$ .

Given the computational infeasibility of evaluating $\Delta \mathcal { L } _ { i j }$ for each parameter, we invoke the first-order Taylor series expansion around the current parameter vector $\Theta$ , which provides a linear approximation of the loss function’s behavior in the vicinity of $\Theta$ . The first-order approximation is represented by the gradient of the loss function with respect to $\theta _ { i j }$ :

$$
\Delta \mathcal { L } _ { i j } ( \Theta ; \mathcal { D } ) \approx \nabla _ { \theta _ { i j } } \mathcal { L } ( \Theta ; \mathcal { D } ) \cdot ( - \theta _ { i j } )
$$

where $\nabla _ { \theta _ { i j } } \mathcal { L } ( \Theta ; \mathcal { D } )$ denotes the partial derivative of the loss function concerning the parameter $\theta _ { i j }$ . The negative sign arises from the fact that we are considering the removal of the parameter, which corresponds to a negative perturbation in its value.

Our objective is to discern the important parameters within the network architecture that are relevant to taskspecific data. To this end, we employ the magnitude of the gradient $\nabla _ { \theta _ { i j } }$ as the criterion for saliency. It is imperative to recognize that a high magnitude of the gradient (regardless of its sign) typically denotes that the parameter $\theta _ { i j }$ exerts a substantial influence on the loss function, whether the effect is positive or negative. Consequently, such parameters must be preserved to facilitate learning on the corresponding weights. The absolute value of the gradient $\nabla _ { \theta _ { i j } }$ is then used as a saliency measure to determine the importance of the parameter:

$$
s _ { i j } = \left| \nabla _ { \theta _ { i j } } \mathcal { L } ( \Theta ; \mathcal { D } ) \right|
$$

Using this saliency measure, we construct the binary mask matrix $M$ such that $M _ { i j } = 1$ if the parameter $\theta _ { i j }$ is deemed important based on a predefined sparsity level $\kappa$ :

$$
M _ { i j } = I [ s _ { i j } \geq \tilde { s } _ { ( \kappa ) } ]
$$

where $\tilde { s } _ { ( \kappa ) }$ is the saliency threshold selected to ensure that only the top $\kappa$ influential parameters have their corresponding entries in $M$ set to one. The indicator function $I [ \cdot ]$ yields one if the condition within the brackets is true and zero otherwise.

# Experiments Experimental Setup

To evaluate the effectiveness of the GMT approach, we conducted a comprehensive assessment of various models across a range of tasks, including code generation, mathematical reasoning, and general domains. Our experiments encompassed two training paradigms, SFT and DPO, allowing for a comparative analysis of the baseline performance in diverse settings. Furthermore, we performed further experimental analysis of mask ratio, drop strategy, and the overall efficiency of the approach.

Training and Evaluation. For code generation, we employ the Magicoder-Evol-Instruct-110K (Wei et al. 2023) as the training data, which is a decontaminated version of evolcodealpaca-v11. We utilize MISTRAL-7B (Jiang et al. 2023) and DEEPSEEK-CODER-BASE-6.7B (Guo et al. 2024) as the base models. The trained models are evaluated using the HumanEval (Chen et al. 2021) and MBPP (Austin et al. 2021) benchmarks, which are widely recognized for their effectiveness in measuring the proficiency of Python textto-code generation. To enable a more comprehensive evaluation, we also introduce HumanEva $^ +$ and $\mathbf { M B P P + }$ , both of which are provided by EvalPlus 2 (Liu et al. 2024a). For math reasoning, the MetaMathQA (Yu et al. 2023a) dataset is employed to fine-tune on the MISTRAL-7B and LLAMA3-8B models. The evaluation is conducted using the GSM8k (Cobbe et al. 2021) and MATH (Hendrycks et al. 2021) benchmarks, which are specifically constructed to test the model’s capacity for mathematical reasoning and problem-solving. For the general domain, the T ¨ULU V2 (Wang et al. 2024a) dataset is utilized in SFT phase training on the LLAMA2-7B (Touvron et al. 2023) and LLAMA2- 13B model, the UltraFeedback (Cui et al. 2023) is utilized in DPO phase training. Following HFT (Hui et al. 2024), we evaluate model on MMLU (Hendrycks et al. 2020), GSM8k (Cobbe et al. 2021), BBH (Suzgun et al. 2023), TyDiQA (Clark et al. 2020), TruthfulQA (Lin, Hilton, and Evans 2022) and HumanEval (Chen et al. 2021).

Implementation Details. We choose different base models for different tasks. All training experiments were done on NVIDIA A100 and NVIDIA H100 machines. In addition, we utilize BFloat16 precision and set the weight decay to 0. We use the cosine learning rate scheduler after a linear warm-up stage with a ratio of 0.03.

Table 1: Experimental results for a single task in a specific domain. All models are evaluated with zero-shot prompting. † We obtain the results of the code benchmark from EvalPlus. “-” denotes that no zero-shot results were officially reported. Bold text indicates the best results for the fine-tuned model on each benchmark.   

<html><body><table><tr><td>Method</td><td>HumanEval</td><td>HumanEval+</td><td>MBPP</td><td>MBPP+</td><td>Average</td><td>GSM8k</td><td>MATH</td><td>Average</td></tr><tr><td></td><td colspan="4">MISTRAL-7B</td><td colspan="4">MISTRAL-7B</td></tr><tr><td>Pre-trainedt</td><td>28.7</td><td>23.8</td><td>51.9</td><td>42.1</td><td>36.6</td><td>1</td><td>=</td><td></td></tr><tr><td>SFT</td><td>68.3</td><td>64.0</td><td>56.1</td><td>46.9</td><td>58.8</td><td>75.3</td><td>27.0</td><td>51.2</td></tr><tr><td>Drop</td><td>68.3</td><td>64.0</td><td>54.6</td><td>46.1</td><td>58.3</td><td>74.6</td><td>27.6</td><td>51.1</td></tr><tr><td>HFT</td><td>67.1</td><td>61.6</td><td>57.1</td><td>48.1</td><td>58.5</td><td>77.8</td><td>27.3</td><td>52.6</td></tr><tr><td>RMT</td><td>70.7</td><td>64.6</td><td>55.1</td><td>45.4</td><td>59.0</td><td>74.6</td><td>25.1</td><td>49.9</td></tr><tr><td>GMT</td><td>69.5</td><td>62.2</td><td>59.6</td><td>48.6</td><td>60.0</td><td>78.6</td><td>28.5</td><td>53.6</td></tr><tr><td></td><td colspan="4">DEEPSEEK-CODER-BASE-6.7B</td><td></td><td colspan="3">LLAMA3-8B</td></tr><tr><td>Pre-trained†</td><td>47.6</td><td>39.6</td><td>72.0</td><td>58.7</td><td>54.5</td><td></td><td></td><td></td></tr><tr><td>SFT</td><td>76.8</td><td>73.8</td><td>74.9</td><td>62.4</td><td>72.0</td><td>78.1</td><td>29.2</td><td>53.7</td></tr><tr><td>Drop</td><td>76.2</td><td>72.6</td><td>74.7</td><td>62.4</td><td>71.5</td><td>78.1</td><td>29.4</td><td>53.8</td></tr><tr><td>HFT</td><td>74.4</td><td>70.1</td><td>75.2</td><td>62.9</td><td>70.7</td><td>74.2</td><td>28.5</td><td>51.4</td></tr><tr><td>RMT</td><td>72.6</td><td>67.1</td><td>78.8</td><td>67.5</td><td>71.5</td><td>80.3</td><td>31.3</td><td>55.8</td></tr><tr><td>GMT</td><td>78.0</td><td>74.4</td><td>75.7</td><td>63.7</td><td>73.0</td><td>82.0</td><td>32.0</td><td>57.0</td></tr></table></body></html>

Baselines. In order to thoroughly evaluate the effectiveness of our method, we compare GMT to the following baselines:

• SFT: Vanilla supervised fine-tuning. • Drop: As an extension of (Yu et al. 2023b), dropping a preset ratio of trivial delta parameters on a one-off basis after the vanilla supervised fine-tuning. • HFT: Half Fine-Tuning, half of the parameters are selected for learning the new task, and the other half is frozen to retain previous knowledge. • RMT: Random Mask-Tuning, a preset ratio of the parameters are randomly updated at a fine-grained elementwise level during the training process.

To ensure fairness, we apply an identical mask ratio across Drop, RMT, and GMT.

# Main Results

Code Generation. The experimental results for the code generation task are presented in Table 1. By integrating task-specific gradient information with avoidance of random parameter selection, GMT achieves an average performance improvement of $1 . 2 \%$ on the MISTRAL-7B model and $1 \%$ on the DEEPSEEK-CODER-BASE-6.7B model. In contrast, HFT freezes half of the parameters to maintain the original capabilities of the model, and the learning process is hampered by the lack of sufficient parameter updates for a specific task domain. Furthermore, the RMT method shows some benefits in reducing redundant parameter updates on MISTRAL-7B. However, it fails to maintain this performance on another model, suggesting that it lacks robustness due to its inability to utilize task-specific data information. Compared to SFT, a one-off drop directly on a fine-tuned model does not improve performance due to its sensitivity to the optimal drop ratio. In the specialized field of code generation, this improvement underscores the efficacy of GMT in optimizing model performance through fine-grained parameter selection.

Math Reasoning. Table 1 presents a comparative analysis of the GMT method against various baseline approaches. GMT outperforms other methods by leveraging gradient information to capitalize on sparsity independent of the inconsistencies associated with random methods. Particularly, GMT performs excellently on the GSM8k benchmark, outperforming SFT by $3 . 3 \%$ and $3 . 9 \%$ on both models. In contrast, the HFT method shows an improvement over the baseline on MISTRAL-7B. However, it performs terribly on LLAMA3-8B, showing the instability of the method, which is attributed to the strategy of randomly chosen parameters. Drop is slightly improved on the MATH benchmark but not as effective as the GMT method that performs the drop operation throughout the training process. In summary, this strategic use of gradient signal associated with the taskspecific data ensures stable and progressive performance improvements throughout the learning process, as evidenced by its success across different models.

General Domain. Table 2 shows the experimental comparison of the proposed GMT with the baseline. In the experiments with the fine-tuned LLAMA2-7B model, the GMT method outperforms the SFT by $3 . 0 \%$ on average across all tasks, and in particular, it significantly leads by $1 3 . 0 \%$ on GSM8k. GMT likewise demonstrated superior performance than HFT in multitasking scenarios in the general domain, obtaining an average lead of $1 . 1 \%$ . After the expansion of the LLM size, the GMT maintains its performance, with a $3 . 3 \%$ improvement in fine-tuning performance of LLAMA2- 13B compared to the vanilla SFT. Notably, both RMT and

Table 2: Results of LLAMA2-7B and LLAMA2-13B fine-tuned on the T ¨ULU V2 dataset. The results of Pre-trained, SFT, and HFT are taken from (Hui et al. 2024). Bold text indicates the best results on each benchmark. The DPO phase is initialized with the corresponding methods using the HFT- and GMT-based SFT models fine-tuned on T ¨ULU V2, respectively.   

<html><body><table><tr><td>Model</td><td>Method</td><td>MMLU 0-shot,EM</td><td>GSM8k 8-shot CoT,EM</td><td>BBH 3-shot CoT,E</td><td>TyDiQA 1-shot,F1</td><td>TruthfulQA 0-shot, MC2</td><td>HumanEval O-shot,Pass @ 10</td><td>Average</td></tr><tr><td>LLAMA2-7B</td><td>Pre-trained</td><td>41.6</td><td>12.0</td><td>39.9</td><td>48.4</td><td>38.5</td><td>26.2</td><td>34.4</td></tr><tr><td>LLAMA2-13B</td><td>Pre-trained</td><td>52.2</td><td>34.5</td><td>50.7</td><td>50.3</td><td>49.8</td><td>32.7</td><td>45.0</td></tr><tr><td></td><td></td><td>Supervised Fine-tuning</td><td></td><td></td><td>(SFT) on TULU</td><td>V2</td><td></td><td></td></tr><tr><td rowspan="4">LLAMA2-7B</td><td>SFT</td><td>48.5</td><td>25.0</td><td>42.2</td><td>51.2</td><td>41.7</td><td>36.9</td><td>41.0</td></tr><tr><td>HFT</td><td>50.8</td><td>30.5</td><td>43.6</td><td>52.3</td><td>45.4</td><td>34.6</td><td>42.9</td></tr><tr><td>RMT</td><td>47.4</td><td>34.5</td><td>44.4</td><td>52.9</td><td>47.9</td><td>33.7</td><td>43.4</td></tr><tr><td>GMT</td><td>47.6</td><td>38.0</td><td>43.3</td><td>53.1</td><td>47.5</td><td>34.6</td><td>44.0</td></tr><tr><td rowspan="4">LLAMA2-13B</td><td>SFT</td><td>50.6</td><td>45.0</td><td>47.8</td><td>55.0</td><td>42.6</td><td>42.4</td><td>47.2</td></tr><tr><td></td><td></td><td></td><td>53.7</td><td>56.7</td><td>45.7</td><td></td><td></td></tr><tr><td>RMT</td><td>545</td><td>46.5</td><td></td><td></td><td></td><td>434</td><td>50.1</td></tr><tr><td>GMT</td><td>54.6</td><td>54.0</td><td>51.5</td><td>57.1</td><td>46.0</td><td>39.5</td><td>50.5</td></tr><tr><td>Direct</td><td></td><td>Preference</td><td></td><td>Optimization</td><td>(DPO) on</td><td>UltraFeedback</td><td></td><td></td></tr><tr><td rowspan="4">LLAMA2-7B</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DPO</td><td>48.9</td><td>28.0</td><td>42.9</td><td>50.2</td><td>45.7</td><td>35.6</td><td>41.9</td></tr><tr><td>RMT</td><td>50.7</td><td>30.5</td><td>42.8</td><td>43.9</td><td>4.8</td><td>35.1</td><td>45.2</td></tr><tr><td>GMT</td><td>48.5</td><td>36.0</td><td>44.7</td><td>53.8</td><td>54.7</td><td>39.8</td><td>46.3</td></tr><tr><td rowspan="4">LLAMA2-13B</td><td>DPO</td><td>52.0</td><td>44.0</td><td>47.1</td><td>51.5</td><td>45.5</td><td>44.3</td><td>47.4</td></tr><tr><td>HFT</td><td>55.0</td><td>45.5</td><td>51.4</td><td>53.2</td><td>49.5</td><td>42.9</td><td>49.6</td></tr><tr><td>RMT</td><td>54.4</td><td>55.5</td><td>50.9</td><td>56.3</td><td>51.7</td><td>40.5</td><td>51.6</td></tr><tr><td>GMT</td><td>54.5</td><td>56.0</td><td>51.9</td><td>56.7</td><td>53.3</td><td>41.1</td><td>52.3</td></tr></table></body></html>

HFT perform better than SFT, suggesting that the general domain of multi-tasking benefits from appropriate sparsity, even when the parameter selection strategy is completely random. With the same amount of parameter updates, GMT shows further improvement over RMT, suggesting that the strategy of selecting parameters based on gradient information derived from task-specific data in our method is practically effective. After extending the GMT to DPO, the superiority of the GMT approach remains evident. Experimental results across two model sizes indicate that GMT identifies parameters more worthy of updating in multi-task learning, thereby reducing the redundancy of parameter updates and enhancing model performance.

# Further Discussions

Analysis of Mask Ratio. To comprehensively ascertain the sensitivity of the hyperparameter mask ratio selection on GMT, we analyze the influence of the amount of parameter updates during training. Comparative experiments were conducted across three domains. We iterated through all experiments, applying the range of parameters to be finetuned with $10 \%$ granularity. For extreme cases, experiments were performed with mask ratios of $9 5 \%$ and $9 9 \%$ . As shown in Figure 2, the experimental results indicate that despite fluctuations in the number of parameter updates, the proposed method maintains a relatively consistent performance level with no more than $90 \%$ mask ratio. Our GMT reveals that the optimal performance in fields like mathematics and coding, which require specialized knowledge, is achieved with mask ratios of $20 \%$ to $40 \%$ . Even under extreme mask ratio settings, GMT maintains impressive performance in task-specific domains such as math reasoning and code generation. However, its effectiveness significantly diminishes in the general domain within multi-task scenarios. This suggests a stronger adaptability of GMT to single-task scenarios, where the entire learning process can be completed with merely a $1 \%$ parameter update. Conversely, in multi-task settings, the potential lack of sufficient parameter updates may lead to an inadequate learning process. Further analysis demonstrates that the performance of the proposed GMT exhibits robustness to variations in the mask ratio, despite the latter being an adjustable hyperparameter within the model’s configuration.

Analysis of Drop Strategy. Our analysis reveals that not all delta parameters contribute equally to the model’s performance. Specifically, we find that parameters with salient values are critical for maintaining the efficacy of the LLM, and their removal leads to a notable and rapid degradation in model functionality. To systematically explore this phenomenon, we designed a series of experiments using the MISTRAL-7B model, focusing on domain-specific applications in mathematics (GSM8k and MATH). The experimental results are depicted in Appendix.

Our results indicate substantial differences in the impact of each strategy on model performance. The strategy of dropping a portion of the delta parameters with small absolute values after vanilla fine-tuning demonstrates a relatively robust performance, only showing significant degradation when the dropout rate reaches $80 \%$ . In contrast, the random drop strategy leads to an apparent performance decline at a much lower sparsity level of $40 \%$ . The strategy of dropping parameters with salient magnitudes results in a precipitous decline in model capabilities. This finding underscores the importance of these significant delta parameters in sustaining the functional integrity of the model. Expectedly, the utilization of the gradient information to trivial drop during the training process can elevate the upper limit of LLM performance and is not constrained by drop rate.

![](images/0cfacaba0972814141351423fbbc242465ca1cdae74044309b15ad3f7f646584.jpg)  
Figure 2: Fine-tuning performance of the proposed GMT with respect to various mask ratios during training in three domains. The training LLMs utilized for code generation task, math reasoning task, and general domain are DEEPSEEK-CODER-BASE6.7B, MISTRAL-7B, and LLAMA2-7B, respectively. The experimental results for the MATH benchmark are presented on the secondary y-axis located on the right side of the figure.

Table 3: Comparison of training speed and computational FLOPs, the number of train samples per second is an indicator of training speed.   

<html><body><table><tr><td>Model</td><td>Method</td><td>Traning Speed (samples/s)</td><td>FLOPs (1e18)</td></tr><tr><td rowspan="4">MISTRAL-7B Code Generation</td><td>Vanilla SFT</td><td>15.32</td><td>1.481</td></tr><tr><td>RMT</td><td>13.37</td><td>1.481</td></tr><tr><td>GMT</td><td>13.43</td><td>1.480</td></tr><tr><td>Vanilla SFT</td><td>13.53</td><td>1.706</td></tr><tr><td rowspan="3">LLAMA2-13B General Domain</td><td>RMT</td><td>12.54</td><td>1.707</td></tr><tr><td>GMT</td><td>12.74</td><td>1.707</td></tr><tr><td></td><td></td><td></td></tr></table></body></html>

Such experiments demonstrate that the parameter updates during the fine-tuning process of LLM are redundant, and that the use of a reasonable sparse updating strategy enables the model to learn better. Comparison with the experiments of random drop indicates that our strategy of considering task-specific data during training, using gradient information as a signal, and retaining neurons with large update magnitudes for completing the update is effective.

Analysis of Efficiency. To demonstrate the timeefficiency of proposed GMT, we compare the training speed and computed FLOPs of GMT, vanilla SFT, and

RMT, with the metric of number of training samples per second responding to the training speed, and the results are shown in Table 3. The FLOPs computed by the three tuning strategies during training are quite comparable due to the fact that the gradient information utilized by GMT is a by-product of model training and does not impose additional derivation operations. In terms of training speed, the RMT is close to GMT, being $14 \%$ and $6 \%$ slower than vanilla SFT in the code generation and general domain, respectively. Since GMT needs to calculate the threshold of the gradient being masked based on a preset ratio, a process that requires a TopK operation after the model backpropagates the gradient, and then drops values with smaller absolute values of the gradient based on the threshold. Nonetheless, this trade-off between a small minor time overhead and performance improvement is completely acceptable.

# Conclusion

In this paper, we propose the Gradient-Mask Tuning (GMT), a pragmatic approach to optimize LLMs by selectively updating parameters based on gradient information. GMT identifies more important parameters to eliminate redundant updates introduced in fine-tuning and improves model performance on various tasks. We evaluate the performance of GMT on multiple models in the code generation, math reasoning, and general domain. Experiments demonstrate that GMT not only elevates the upper limits of LLM performance across diverse tasks but also exhibits robustness to various fine-tuning settings. We further analyze the computational efficiency of GMT to confirm that no additional computational FLOPs are needed, as well as acceptable extra time consumption. Moreover, GMT allows for fine-tuning without destroying the network structure, and thus can readily replace SFT and DPO to accomplish the optimization process.