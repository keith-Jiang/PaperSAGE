# Imitate Before Detect: Aligning Machine Stylistic Preference for Machine-Revised Text Detection

Jiaqi Chen1,2\*, Xiaoye $\mathbf { Z } \mathbf { h } \mathbf { u } ^ { 3 , 4 * }$ , Tianyang $\mathbf { L i u } ^ { 5 * }$ , Ying Chen6, Chen Xinhui7,8, Yiwen Yuan9, Chak Tou Leong10, Zuchao $\mathbf { L i } ^ { 7 \dagger }$ , Long Tang1,3, Lei Zhang5, Chenyu $\mathbf { Y a n } ^ { 1 1 }$ , Guanghao Mei5, Jie Zhang1†, Lefei Zhang7†

1Fudan University   
2Stanford University   
3South China University of Technology   
4 NUS (Chongqing) Research Institute   
5University of California, San Diego   
6University of lllinois at Urbana-Champaign   
7Wuhan University   
8Fenz AI   
9Carnegie Mellon University   
10The Hong Kong Polytechnic University   
11Georgia Institute of Technology

# Abstract

Large Language Models (LLMs) have revolutionized text generation, making detecting machine-generated text increasingly challenging. Although past methods have achieved good performance on detecting pure machine-generated text, those detectors have poor performance on distinguishing machinerevised text (rewriting, expansion, and polishing), which can have only minor changes from its original human prompt. As the content of text may originate from human prompts, detecting machine-revised text often involves identifying distinctive machine styles, e.g., worded favored by LLMs. However, existing methods struggle to detect machine-style phrasing hidden within the content contributed by humans. We propose the “Imitate Before Detect” $\left( I m B D \right)$ approach, which first imitates the machine-style token distribution, and then compares the distribution of the text to be tested with the machine-style distribution to determine whether the text has been machinerevised. To this end, we introduce style preference optimization (SPO), which aligns a scoring LLM model to the preference of text styles generated by machines. The aligned scoring model is then used to calculate the style-conditional probability curvature (Style-CPC), quantifying the log probability difference between the original and conditionally sampled texts for effective detection. We conduct extensive comparisons across various scenarios, encompassing text revisions by six LLMs, four distinct text domains, and three machine revision types. Compared to existing state-of-the-art methods, our method yields a $13 \%$ increase in AUC for detecting text revised by open-source LLMs, and improves performance by $5 \%$ and $19 \%$ for detecting GPT-3.5 and GPT-4o revised text, respectively. Notably, our method surpasses the commercially trained GPT-Zero with just 1, 000 samples and five minutes of SPO, demonstrating its efficiency and effectiveness.

Homepage — machine-text-detection.github.io/ImBD Code — github.com/Jiaqi-Chen-00/ImBD Extended version — arxiv.org/abs/2412.10432

# 1 Introduction

Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that is difficult to distinguish from human writing (Brown et al. 2020; Chowdhery et al. 2023; Li et al. 2023; Touvron et al. 2023a,b; OpenAI 2022; Achiam et al. 2023; Bi et al. 2024; Lozhkov et al. 2024).

With the widespread application of these models, their misuse in exams, academic papers, publications, and other contexts has led to concerns in areas such as academic integrity, fake news, and online information verification. As a result, determining whether a text is LLM-assisted or entirely human-written has become crucial (Bao et al. 2023).

In practice, the landscape of LLM-assisted writing extends beyond the widely studied pure generation to also include machine-revised text, where LLMs enhance or modify human-written content (Zhang et al. 2024). This shift results in a more nuanced challenge for detection, as the boundaries between human and machine contributions become increasingly intertwined. Figure 1 (upper) provides comparative examples of human-written, machine-generated text, and machine-revised text. This evolution in LLM-assisted writing necessitates a reevaluation of existing detection approaches.

Previous detection methods (Hans et al. 2024; Mitchell et al. 2023; Bao et al. 2023; Su et al. 2023; Yang et al. 2023; Zhu et al. 2023; Wu et al. 2024) for identifying machinegenerated text rely on calculating classification metrics based on token probabilities from pre-trained language models. These methods are built on the assumption that machinegenerated texts typically exhibit higher log-likelihoods (He

"Generate Write anything somuetext bfollowuipitat: These you like future" (Human-written text) Human Machine Human Machine   
These beautiful A stunningly   
pictures are the intricate These stunning images   
first ever taken pattern capture a   
lioness nursing of a wild epixeled, each moment: a wild lioness once-in-a-lifetime a cub from a with precise, tenderly nursing a cub of different digital artistry. a different species. (a) Human- (b) Pure machine(c)Machine-revised text written text generated text 0.99 0.99 1.0 0.96 0.96 0.93 0.9 0.9 X 0.79 0.8 0.73 10.72 0.7 0.73 0.6 Xsum Writing PubMed Average Fast-Det. Ours   
Machine-generated text Machine-revised text (e) Improvement on   
(d) Performance drop of Fast-DetectGPT machine-revised text

Therefore, it is necessary to explicitly model these stylistic features.

Motivated by the challenges and observations above, we propose Imitate Before Detect (ImBD) which first imitates the style/pattern of machine-revised texts, then measures the distributional differences between the text under evaluation and the machine style, thereby enabling effective detection of machine-revised texts. The ImBD consists of two main steps. First, we introduce Style preference optimization $( S P O )$ for machine style imitation, which aligns a scoring LLM model to favor the characteristic style of machine-revised text. Specifically, we use pairs of text with identical content – one generated by an LLM and the other written by a human - to adjust the model’s token distribution towards a machine-like writing style. Second, we employ the scoring model tuned by step one to calculate the Style-conditional probability curvature (Style-CPC). This metric quantifies the difference between the log probabilities of the original text and alternative versions produced through conditional probability sampling, enabling effective distinction between human-written and machine-revised content. By combining our style-focused alignment with logit-based detection, our method aims to effectively identify machine-revised text even when dealing with advanced language models like GPT-3.5 or GPT-4o.

We demonstrate the efficiency and effectiveness of our method through extensive comparisons across diverse scenarios. Our results show significant improvements over existing state-of-the-art methods. We achieve an $13 \%$ increase in ROAUC for detection on open-source models; $5 \%$ and $1 9 \%$ respective increases on GPT-3.5 and GPT-4o, with limited computational resources – just 1, 000 samples and five minutes of SPO training – our approach outperforms the commercially trained GPT-Zero detector.

Our contributions are three-fold:

et al. 2024; Holtzman et al. 2020) or negative probability curvatures (Mitchell et al. 2023; Bao et al. 2023) compared to human-written texts. While these approaches effectively capture the characteristics of purely machine-generated text, they struggle to identify machine-revised text that contains human content, such as domain-specific terminology. This is because the human-contributed content can mislead detectors into believing that the text is human-written (Zhang et al. 2024; Sadasivan et al. 2024; He et al. 2024). As a result, these advanced methods experience significant performance drops when detecting machine-revised text (See Figure 1 (d)). We believe that recognizing the distinctive style of machinerevised text, such as machine-preferred filler phrases and rare vocabulary, is key to effectively detecting such texts.

Specifically, the style distinctions between pure-human and machine-revised texts often lie in subtle stylometric features, as demonstrated by examples in Figure 1. Machine revisions exhibit certain characteristic patterns in word choice (e.g., preference for terms like “stunning,” “once-in-a-lifetime,” and “tenderly”), sentence structures (e.g., more complex subordinate clauses), and organizational methods (e.g., consistent paragraph structuring) (Chawla 2024). However, these style features are difficult to capture and isolate due to the human-contributed content mixed into machine-revised text.

• We propose the Imitate Before Detect which first imitates the stylistic preferences of LLMs, then measures the distribution distance to recognize machine-revised text that includes human content.   
• We introduce a comprehensive dataset for machinerevised text detection, enabling robust evaluation of detection methods across diverse domains, revision types, and a wide range of mainstream LLMs.   
• Our approach achieves $1 5 . 1 6 \%$ , $1 9 . 6 8 \%$ , and $1 2 . 9 0 \%$ higher ROCAUC than the previous state-of-the-art, FastDetectGPT, in detecting revised text from GPT-3.5, GPT4o, and mainstream open-source LLMs respectively with the same inference speed.

# 2 Method

We elaborate on the methods for addressing the challenge of machine-revised text detection, aiming to differentiate between pure human texts and machine-revised texts.

# 2.1 Problem Formulation

Let $x$ denote the given text under detection, represented as a sequence of tokens $\{ x _ { i } \} _ { i = 1 } ^ { n }$ , where $n$ is the length of the sequence. This text $x$ may either be revised by machine or authored by a human. Our primary objective is to utilize a scoring model $p _ { \theta }$ , which is an autoregressive language model, to ascertain whether the text $x$ is machine-revised $( x _ { m } )$ or human-written $( x _ { h } )$ , thereby formulating this problem as a binary classification task. Formally, we aim to construct a decision function $f : x  0 , 1$ , where the output 0 indicates that the text is human-authored, and 1 signifies that the text is machine-revised.

![](images/1271d627979dbf578c7feda707ef309a84ce00fae3aba4e8aa2fee1f12194dd3.jpg)  
Figure 2: Impact of Style-conditional probability curvatures (Style-CPC). (Left) Conditional probability curvatures (CPC) from Fast-DetectGPT (denoted as “Fast-Det.”) applied to purely machine-generated text; (Middle) Conditional probability curvatures applied to purely machine-revised text; (Right) Style-conditional probability curvatures from ours applied to machine-revised text. The greater the separation between human-written texts (red) and machine-revised texts (blue), the more effective the detection.

# 2.2 Preliminary

Foundation The foundation of machine-generated text detection methods often lies in analyzing the probability distribution of tokens within a given text. This is rooted in the fact that common decoding strategies, such as top- $\mathbf { \nabla } \cdot \mathbf { k }$ , top-p, and beam search, favor high-likelihood next tokens in autoregressive generation, while high-quality human language does not necessarily follow high-probability next words (Holtzman et al. 2020).

To quantify the differences between machine-generated text $x _ { m }$ and human-written text $\boldsymbol { x } _ { h }$ , one effective strategy is to measure the discrepancy $( \delta )$ between the log probability of the original text and its alternative versions under perturbation (Mitchell et al. 2023) or after resampling (Bao et al. 2023). Let $\phi$ denote a transformation function that produces an altered version $\tilde { x }$ from the original text $x$ , i.e., ${ \bar { \tilde { x } } } \sim \phi ( x )$ . In machine-generated texts, the original tokens often have higher probabilities, and after applying $\phi$ for token replacement, the probabilities of the new tokens tend to be lower on average. Conversely, human-written texts typically exhibit a more diverse range of token probabilities, leading to a smaller discrepancy after alterations. As a result, this discrepancy tends to be larger for machine-generated text compared to human-written text. Formally, we can express this inequality as:

$$
\log p ( x _ { m } ) - \mathbb { E } _ { \tilde { x } _ { m } \sim \phi ( x _ { m } ) } \log p ( \tilde { x } _ { m } )
$$

$$
> \ \log p ( x _ { h } ) - \mathbb { E } _ { \tilde { x } _ { h } \sim \phi ( x _ { h } ) } \log p ( \tilde { x } _ { h } ) ,
$$

where $p$ represents the probability distribution of the source model. The source model can be effectively replaced by a substitute scoring model $p _ { \theta }$ in black-box scenarios (Mitchell et al. 2023). This inequality forms the basis for distinguishing between machine-generated and human-written content. Recent studies have demonstrated the effectiveness of this approach in detecting machine-generated text (Mitchell et al. 2023; Bao et al. 2023). In scenarios where the distributions of these discrepancies show a small overlap between machine-generated and human-written texts, this approach can effectively distinguish between the two types of content. As shown in Figure 2 (left), the distribution of the discrepancy for machine-generated text is generally larger than that for human-written text, creating a gap that allows differentiation between the two.

Problem Analysis While the aforementioned approach can be effective for detecting pure machine-generated text, it encounters significant challenges when applied to more nuanced scenarios, particularly in the detection of machine-revised texts. In tasks, such as rewrite or polish, where machines make small changes on top of human writing, we observe a substantial overlap in the probability distributions of machine-revised and human-written texts, as shown in Figure 2 (right).

This overlap severely compromises the effectiveness of detection methods that rely on the hypothesis. The limitations arise from two key factors. First, when users provide part of the content, the resulting text is not entirely “machinegenerated”, making probability-based distinctions less effective. Second, advanced LLMs may develop unique stylistic patterns that are not captured by traditional methods. For instance, models like GPT-4 might favor words such as commendable, “embark”, “delve into”, “intricate”, etc. (Liang et al. 2024; Gray 2024; Chawla 2024), in contexts where a scoring model trained on a general corpus would consider them unexpected. This discrepancy skews the calculation of the probability curvature, leading to values that significantly overlap between machine-revised and human-written texts, making reliable distinction challenging.

These challenges underscore the need for a more nuanced approach to detection that focuses on capturing the subtle stylistic differences between human-written and machinerevised text. Therefore, we propose to learn the characteristic style of machine-revised text by imitating the token distribution output by LLMs. By focusing on style rather than content, we aim to enhance the detector’s ability to distinguish between human-written and machine-revised text.

# 2.3 Imitating via Preference Optimization

Based on the challenges identified in detecting machinerevised text, we observed that the key to effective detection lies in increasing the discrepancy between the probability distributions of machine-revised and human-written texts. To address this, we aim to increase the difference between the discrepancies $\delta _ { m }$ and $\delta _ { h }$ , as defined earlier. Specifically, our objective is to optimize the scoring model $p _ { \theta }$ to better imitate the token distribution with machine style, such that:

![](images/d618cda4e282c8e164928ceabe154c65f3673c04a190e970955294113a3d57b6.jpg)  
Figure 3: Imitating the stylistic preferences of LLMs. (a) Token distribution before and after machine-style imitation, demonstrating a deliberate fine-tuning of the scoring model to bias its token distribution towards a machine writing style (e.g., shifting preferences from common words like “explore” to machine-favored tokens such as “delve”). (b) The pipeline of Style Preference Optimization is applied to align the base scoring model with the style of machine-revised content using paired human-machine texts. This results in a machine-style scoring model, which generates token distributions $p ( x _ { n } | x _ { 0 : n - 1 } )$ for each position $n$ , subsequently used for style-conditional probability curvature calculations.

$$
\operatorname* { m a x } _ { \boldsymbol { p } _ { \theta } } \mathbb { E } _ { \boldsymbol { x } _ { m } , \boldsymbol { x } _ { h } } [ \delta _ { m } - \delta _ { h } ] .
$$

This objective seeks to widen the gap between the discrepancies between machine-revised and human-written texts, making them more distinguishable. To achieve this, we propose a method called style preference optimization, which leverages preference learning to tune the scoring model $p _ { \theta }$ towards favoring machine-revised text patterns.

As shown in Figure 3 (b), the core of this method involves constructing preference relations between pairs of texts with equivalent content: one human-written $( x _ { h } )$ and one machinerevised $( x _ { m } )$ . These pairs are created through a rewriting process, ensuring that the content remains consistent while the writing style varies. This pairing strategy allows us to isolate and focus on stylistic differences, controlling for content variability. By optimizing the scoring model $p _ { \theta }$ to exhibit a stronger preference for the stylistic features of machinerevised text $x _ { m }$ over those of human-written text $\boldsymbol { x } _ { h }$ , we denote this preference as $x _ { m } \succ x _ { h }$ . We formulate this preference learning through the lens of reward learning. Assuming an optimal reward function $r$ , we express the preference distribution $p ^ { * }$ using the Bradley-Terry model:

$$
p ^ { * } ( x _ { m } \succ x _ { h } ) = \sigma ( r ( x _ { m } ) - r ( x _ { h } ) ) ,
$$

where $\sigma$ is the sigmoid function. This formulation indicates that the probability of preferring machine-revised text over human-written text increases as the reward difference $r ( x _ { m } ) - r ( x _ { h } )$ grows. Following the Direct Preference Optimization (DPO) approach, we reparameterize the reward function $r$ using a closed-form expression based on the optimal policy:

$$
r ( x ) = \beta \log \frac { p _ { \theta } ( x ) } { p _ { \theta _ { \mathrm { r e f } } ( x ) } } .
$$

Here, $p _ { \theta _ { \mathrm { r e f } } }$ represents a reference model, typically the initial state of $p _ { \theta }$ before optimization. By incorporating this reward formulation, we express the probability of preference data with the policy model rather than the reward model. Given a training dataset $\mathcal { D }$ of content-equivalent $( x _ { m } , x _ { h } )$ pairs, we optimize the following objective:

$$
\operatorname* { m a x } _ { p _ { \theta } } \underset { ( x _ { m } , x _ { h } ) \sim \mathcal { D } } { \mathbb { E } } \left[ \log \sigma \left( r ( x _ { m } ) - r ( x _ { h } ) \right) \right] .
$$

By optimizing this objective function, we can adjust the model $p _ { \theta }$ to favor the stylistic features of machine-revised texts. This adjustment makes the model more sensitive to the stylistic characteristics of machine-revised text. We denote the optimized model as $\hat { p _ { \boldsymbol { \theta } } }$ , representing a machine-style scoring model that is strongly aligned with machine styles.

# 2.4 Detection via Style Probability Curvature

After aligning our model with machine-revised text styles, we proceed with the detection step using conditional probability curvature (Bao et al. 2023). Specifically, given the machinestyle scoring model $\hat { p _ { \theta } }$ and a sampling model $q _ { \phi }$ , we define the style-conditional probability as:

$$
p ( \tilde { x } | x ) = \prod _ { j } \hat { p _ { \theta } } ( \tilde { x } _ { j } | x _ { < j } ) .
$$

Here, $\tilde { x }$ is generated by sampling each token $x _ { i }$ from $\hat { p _ { \theta } } ( x _ { i } \mid$ $x _ { < i } )$ without conditioning on other sampled tokens. The styleconditional probability curvature (Style-CPC) is quantified as:

$$
{ \bf d } ( x , \hat { p _ { \theta } } , q _ { \phi } ) = \frac { \log \hat { p _ { \theta } } ( x | x ) - \tilde { \mu } } { \tilde { \sigma } } ,
$$

where

$$
\tilde { \mu } = \mathbb { E } _ { \tilde { x } \sim q _ { \phi } ( \tilde { x } | x ) } \left( \log p _ { \theta } ( \tilde { x } _ { i } \mid x ) \right) \quad ,
$$

$$
\tilde { \sigma } ^ { 2 } = \mathbb { E } _ { \tilde { x } \sim q _ { \phi } ( \tilde { x } \mid x ) } \left( \log p _ { \theta } ( \tilde { x } _ { i } \mid x ) - \tilde { \mu } ^ { 2 } \right) .
$$

This metric ${ \bf d } ( x , \hat { p _ { \theta } } , q _ { \phi } )$ allows us to quantify the log probability difference between the original and alternative sampled texts. Figure 2 illustrates the distribution of $\mathbf { d }$ before and after applying Style-CPC. We observe that using the aligned model to calculate $\mathbf { d }$ significantly reduces the overlap between distributions of human-written and machine-revised texts. This reduced overlap enables us to identify an effective threshold value $\epsilon$ , leading to a straightforward classification strategy:

$$
f ( x ) = \left\{ \begin{array} { l l } { 1 } & { \mathrm { i f } \mathbf { d } ( x , \hat { p _ { \theta } } , q _ { \phi } ) > \epsilon } \\ { 0 } & { \mathrm { o t h e r w i s e } } \end{array} , \right.
$$

where $f ( x ) = 1$ indicates machine-revised text, and $f ( x ) =$ 0 signifies human-written text. By combining machine style alignment with probability curvature detection, our method aims to enhance the model’s sensitivity to the unique stylistic features of machine-revised texts. Essentially, we tune the scoring model to be more biased towards machine-revised styles, making it ‘aware’ of the subtle differences between machine and human writing styles. This increased sensitivity allows for a more pronounced separation in the probability curvature distributions of machine and human-authored texts. Consequently, the previously overlapping distributions become more distinct, enabling effective logits-based detection that was previously challenging. This approach shifts the focus from content to style, seeking to address the limitations of traditional methods in detecting outputs from advanced language models and in scenarios with user-provided content.

# 3 Experiment

# 3.1 Machine Revision Dataset

Data sources The human-written texts included in the training dataset were crawled from the internet before 2019. The texts are then polished by GPT-3.5.1 We use 500 pairs of samples for training. The composition of the dataset is $5 7 . 3 \%$ papers, $1 4 . 2 \%$ blogs, $4 . 0 \%$ letters and emails, and $2 . 1 \%$ homework. For the test data, we follow Bao et al. (2023); Mitchell et al. (2023), use paragraphs from diverse domains as humanwritten texts, including XSum (Narayan, Cohen, and Lapata 2018) for news articles, SQuAD (Fan, Lewis, and Dauphin 2018) for Wikipedia contexts, WritingPrompts (Fan, Lewis, and Dauphin 2018) (Abbreviated as “Writing”) for story writing, and PubMedQA (Jin et al. 2019) for biomedical research question answering. Then, we use the pipeline detailed in the following paragraph to generate correspondent machinerevised text.

Dataset process We design a cohesive two-stage pipeline to revise human-written text.

• Revision instruction generation: For each task, instructions are constructed with varying tones and lengths using GPT-3.5. The tone is randomly selected from a set of 10 predefined options, while the instruction length is chosen from the set of $\{ 1 5 , 3 0 , 5 0 \}$ words. The intuition behind choosing different tones and lengths is to simulate different human behaviors.

• Paragraph revision: The generated instruction and the human-written text are then prompted into the LLM to produce the final machine-revised text.

Target LLMs for revision We experiment with four opensource models: Qwen2-7B (Yang et al. 2024), Llama-3- 8B (Meta AI 2024), Mixtral-7B (Jiang et al. 2024), and Deepseek-7B (Bi et al. 2024), as well as two proprietary models, GPT-3.5 (OpenAI 2022) and GPT-4o (Achiam et al. 2023). Our choice covers a broad spectrum of user preferences.

Machine revision tasks We evaluate the performance of the detector on three tasks: rewrite, expand, and polish. (i) Rewrite: The LLM is asked to rewrite the given text while preserving all details. (ii) Expand: The LLM is asked to expand the original text given a style parameter randomly chosen from a set of 10 options such as formal, literary, etc. (iii) Polish: The LLM is asked to polish/adjust the text based on a randomly picked style. Furthermore, we test our method on the generate task used in the common evaluation of machine-generated text detectors, which does not fall under the category of machine-revised text detection. To produce machine-generated text for generate task, the LLM is prompted with the first 30 tokens of the human written text, following the design in DetectGPT (Mitchell et al. 2023) and Fast-DetectGPT (Bao et al. 2023).

# 3.2 Baselines

We compare our method with two lines of method: trainingbased models, and logit-based models. Following Bao et al. (2023), we use AUROC as a metric to evaluate detection accuracy. (i) Training-based models include RoBERTa-base (Liu 2019) and RoBERTa-large (Liu 2019), which is trained on substantial datasets up to 160GB of text data, as well as the commercial detector GPTZero (Tian and Cui 2023), which is trained on massive datasets. (ii) Logit-based models include Likelihood (Ippolito et al. 2020) (mean log probabilities), LogRank (Solaiman et al. 2019) (average log of ranks in descending order by probabilities), Entropy (Gehrmann, Strobelt, and Rush 2019) (mean token entropy of the predictive distribution), LRR (Su et al. 2023) (an amalgamation of log probability and log-rank), NPR (Su et al. 2023) (normalized perturbed log-Rank) and DNA-GPT (Yang et al. 2023) (divergent N-Gram Analysis), DetectGPT (Mitchell et al. 2023), and its advanced variant, Fast-DetectGPT (Bao et al. 2023).

Note that Fast-DetectGPT (Bao et al. 2023), the current state-of-the-art approach, also serves as a baseline method that does not involve machine-style imitation.

# 3.3 Main Results

Detection performance for GPT series We evaluate our method using passages polished by GPT-3.5 and GPT-4o across different domains. As shown in Table 1, our method outperforms Fast-DetectGPT by $1 5 . 1 6 \%$ and $1 9 . 6 8 \%$ in detecting GPT-3.5 and GPT-4 outputs, respectively, on the polish task. Furthermore, compared to the supervised detectors RoBERTa-large, our method shows an improvement of $3 2 . 9 1 \% / 4 7 . 0 6 \%$ on detecting GPT-3.5 and GPT-4, respectively. Additionally, as shown in Table 2, our method surpasses GPTZero by $0 . 9 8 \%$ . This indicates that our method is highly efficient in training, achieving superior performance with a small amount of data compared to models trained on much larger datasets. To demonstrate task generalization, we compared performance on the rewrite task, where our method outperformed Fast-DetectGPT by $3 6 . 9 6 \%$ and $2 4 . 2 9 \%$ in detecting GPT-3.5 and GPT-4o outputs, respectively.

Table 1: Detection of GPT-3.5 and GPT-4o polished text. Typically, the Neo-2.7B (Black et al. 2021) is used as the source for the scoring model. NPR and DetectGPT, on the other hand, utilize T5-3B (Chen et al. 2019) for generating perturbations, whereas Fast-DetectGPT employs GPT-J (Wang and Komatsuzaki 2021) as a surrogate model to generate samples. The $\diamondsuit$ symbol denotes methods that require multiple model invocations, leading to a substantial increase in computational load. Metric: AUROC.   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">Time cost (s/1k words)</td><td colspan="3">GPT-3.5</td><td rowspan="2">Avg.</td><td colspan="3">GPT-40</td><td rowspan="2">Avg.</td></tr><tr><td>XSum</td><td>Writing</td><td>PubMed</td><td>XSum</td><td>Writing</td><td>PubMed</td></tr><tr><td>RoBERTa-base</td><td>0.07</td><td>0.5806</td><td>0.7225</td><td>0.4370</td><td>0.5800</td><td>0.4921</td><td>0.4774</td><td>0.2496</td><td>0.4064</td></tr><tr><td>RoBERTa-large</td><td>0.11</td><td>0.6391</td><td>0.7236</td><td>0.4848</td><td>0.6158</td><td>0.4782</td><td>0.4708</td><td>0.3089</td><td>0.4193</td></tr><tr><td>Likelihood</td><td>0.38</td><td>0.4982</td><td>0.8788</td><td>0.5528</td><td>0.6433</td><td>0.4396</td><td>0.8077</td><td>0.4596</td><td>0.5690</td></tr><tr><td>Entropy</td><td>0.35</td><td>0.6742</td><td>0.3021</td><td>0.5662</td><td>0.5142</td><td>0.6122</td><td>0.2802</td><td>0.5899</td><td>0.4941</td></tr><tr><td>LogRank</td><td>0.36</td><td>0.4711</td><td>0.8496</td><td>0.5597</td><td>0.6268</td><td>0.4002</td><td>0.7694</td><td>0.4472</td><td>0.5389</td></tr><tr><td>LRR</td><td>0.41</td><td>0.4016</td><td>0.7203</td><td>0.5629</td><td>0.5616</td><td>0.3095</td><td>0.6214</td><td>0.4710</td><td>0.4673</td></tr><tr><td>DNA-GPT</td><td>35.92</td><td>0.5338</td><td>0.8439</td><td>0.3333</td><td>0.5703</td><td>0.4974</td><td>0.7478</td><td>0.3151</td><td>0.5201</td></tr><tr><td>NPR</td><td>111.99</td><td>0.5659</td><td>0.8786</td><td>0.4246</td><td>0.6230</td><td>0.5065</td><td>0.8444</td><td>0.3740</td><td>0.5750</td></tr><tr><td>DetectGPT</td><td>111.33</td><td>0.6343</td><td>0.8793</td><td>0.5608</td><td>0.6915</td><td>0.6217</td><td>0.8771</td><td>0.5612</td><td>0.6867</td></tr><tr><td>Fast-Detect-GPT</td><td>0.72</td><td>0.7312</td><td>0.9304</td><td>0.7182</td><td>0.7933</td><td>0.6293</td><td>0.8324</td><td>0.6175</td><td>0.6931</td></tr><tr><td>ImBD (Ours)</td><td>0.72</td><td>0.9849</td><td>0.9871</td><td>0.8626</td><td>0.9449</td><td>0.9486</td><td>0.9468</td><td>0.7743</td><td>0.8899</td></tr></table></body></html>

Table 2: Compared with GPTZero on detecting GPT-3.5 polished text. Metric: AUROC.   

<html><body><table><tr><td>Method</td><td>XSum</td><td>Writing</td><td>PubMed</td><td>Avg.</td></tr><tr><td>GPTZero</td><td>0.9542</td><td>0.9711</td><td>0.8800</td><td>0.9351</td></tr><tr><td>ImBD (Ours)</td><td>0.9849</td><td>0.9871</td><td>0.8626</td><td>0.9449</td></tr></table></body></html>

Table 3: Detection on open-source model polished text. AUROC scores are averaged across the XSum, SQuAD, and WritingPrompts datasets. Among them, Qwen2, Mixtral, and Deepseek are 7B models, while Llama-3 is an 8B model.   

<html><body><table><tr><td>Method</td><td>Qwen2 Llama-3 Mixtral Deepseek</td><td></td><td></td><td>Avg.</td></tr><tr><td>Likelihood</td><td>0.4121 0.6861</td><td>0.5881</td><td>0.6887</td><td>0.5938</td></tr><tr><td>Entropy</td><td>0.6819 0.5546</td><td>0.5741</td><td>0.4923</td><td>0.5757</td></tr><tr><td>LogRank</td><td>0.3778 0.6581</td><td>0.5498</td><td>0.6710</td><td>0.5642</td></tr><tr><td>LRR</td><td>0.3025 0.5519</td><td>0.4299</td><td>0.6010</td><td>0.4713</td></tr><tr><td>DNA-GPT</td><td>0.5021 0.6809</td><td>0.6091</td><td>0.7031</td><td>0.6238</td></tr><tr><td>NPR</td><td>0.5388 0.7186</td><td>0.5988</td><td>0.6551</td><td>0.6278</td></tr><tr><td>DetectGPT</td><td>0.6193 0.7706</td><td>0.6826</td><td>0.7160</td><td>0.6971</td></tr><tr><td>Fast-DetectGPT</td><td>0.7323 0.8870</td><td>0.8164</td><td>0.8687</td><td>0.8261</td></tr><tr><td>ImBD (Ours)</td><td>0.9367 0.9767</td><td>0.9492</td><td>0.9574</td><td>0.9550</td></tr></table></body></html>

Table 4: Performance on diverse tasks. We evaluated the detection performance, measured by average AUROC, of text revised by leading LLMs (Qwen2-7B, Llama-3-8B, Mixtral-7B, Deepseek-7B, GPT-3.5, and GPT-4o) on the XSum dataset.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3">Tasks</td><td rowspan="2">Avg.</td></tr><tr><td>Rewrite Expand</td><td>Polish</td><td>Generate</td></tr><tr><td>Likelihood</td><td>0.4073 0.4564</td><td>0.6039</td><td>0.8939</td><td>0.5904</td></tr><tr><td>Entropy</td><td>0.5840 0.6629</td><td>0.5431</td><td>0.4129</td><td>0.5507</td></tr><tr><td>LogRank</td><td>0.3868 0.4273</td><td>0.5864</td><td>0.8925</td><td>0.5732</td></tr><tr><td>LRR</td><td>0.3488 0.3581</td><td>0.5183</td><td>0.8541</td><td>0.5198</td></tr><tr><td>DNA-GPT</td><td>0.4101 0.4901</td><td>0.5847</td><td>0.8931</td><td>0.5945</td></tr><tr><td>NPR</td><td>0.3606 0.5139</td><td>0.5673</td><td>0.8541</td><td>0.5740</td></tr><tr><td>DetectGPT</td><td>0.4060 0.6000</td><td>0.6615</td><td>0.8985</td><td>0.6415</td></tr><tr><td>Fast-DetectGPT</td><td>0.4499 0.7159</td><td>0.7989</td><td>0.9706</td><td>0.7338</td></tr><tr><td>ImBD (Ours)</td><td>0.8739 0.9758 0.9707</td><td></td><td>0.9996</td><td>0.9550</td></tr></table></body></html>

Detection performance on open-source models The performance on polish task by open-source models is shown in Table 3. ImBD achieves the highest average AUROC, outperforming DetectGPT by $2 5 . 7 9 \%$ .

Robustness in machine revision and generation As shown in Table 4, our method outperforms the state-of-art Fast-DetectGPT by $2 2 . 1 2 \%$ on average across all four tasks. The results showcase the robustness of our approach across various tasks and user instructions.

Inference time and training efficiency Our model is trained for 2 epochs with a learning rate set to 0.0001 and $\beta$ set to 0.05. Each epoch requires approximately 110 seconds on an L20 (48G) GPU, leading to a total training time of 220.57 seconds. As shown in Table 1 , our method achieves a competitive inference time of 0.72 seconds per 1000 words, matching that of Fast-DetectGPT ${ \mathrm { 1 5 4 . 6 2 } } \times$ speed-up compared to DetectGPT), but with better performance.

Table 5: Ablation on preference optimization. Comparative performance of SPO, supervised fine-tuning (SFT), RLHF, and ORPO strategies across datasets. Training dataset size: 1, 000 samples. “\*” denotes trained on 3x samples. “Pub.” denotes “PubMed” and “Writ.” denotes “WritingPrompts”. Metric: AUROC. Task: Polish.   

<html><body><table><tr><td>Strategy</td><td>GPT-3.5 XSum Writ. Pub. Avg.</td><td>GPT-40 XSum Writ. Pub. Avg.</td></tr><tr><td>w/o imitate</td><td>0.73 0.93 0.72 0.79</td><td>0.63 0.83 0.62 0.69</td></tr><tr><td>SFT</td><td>0.56 0.700.70 0.65</td><td>0.60 0.740.66 0.67</td></tr><tr><td>SFT*</td><td>0.59 0.700.66 0.65</td><td>0.61 0.73 0.60 0.65</td></tr><tr><td>RLHF</td><td>0.70 0.920.78 0.80</td><td>0.54 0.81 0.64 0.66</td></tr><tr><td>ORPO</td><td>0.79 0.97 0.81 0.86</td><td>0.60 0.87 0.66 0.71</td></tr><tr><td>ImBD (Ours)</td><td>0.99 0.99 0.86 0.95</td><td>0.95 0.95 0.77 0.89</td></tr></table></body></html>

GPT-3.5 GPT-40 1.0 R 1 0.4 30 6090120 150 180 30 60 90120150180 Passage Length (Words) Passage Length (Words) ImBD (Ours) Fast-DetectGPT RoB-large RoB-base Likelihood

# 3.4 Ablation Study

Ablation on machine-style imitation As shown in Table 5, using fast-DetectGPT as the baseline without imitation, our method improves detection accuracy by $16 \%$ and $20 \%$ on GPT-3.5 and GPT-4o machine-revised texts, respectively.

Ablation on preference optimization To demonstrate the difference between different optimization methods on ImBD, we compare the performance of SPO against other alignment approaches on polish task. As shown in Table 5, ImBD outperformed the SFT variant by $30 \%$ on GPT-3.5 and $24 \%$ on GPT-4o, even when the SFT variant uses 3x training data. Additionally, ImBD exceeds RLHF and ORPO significantly.

Ablation on text length As shown in Figure 4, our method demonstrates strong performance across passages of varying lengths compared to other methods, with accuracy improving as passage length increases.

# 4 Related Work

# 4.1 Machine-Generated Text Detection

Datasets Researchers developed various evaluation benchmarks for machine-generated text detection. Bao et al. (2023) and Mitchell et al. (2023) used the initial 30 tokens from human-written texts across different domains as prompts to generate pure machine-generated text via LLMs. Following this approach, Guo et al. (2023) employed QA datasets as human samples and generated pure machine-generated text using ChatGPT. Building upon the QA framework, researchers (Mitchell et al. 2023; Su et al. 2023; Hu, Chen, and Ho 2023; He et al. 2024; Wang et al. 2024) collected texts generated by LLMs. Verma et al. (2023) focused on creative writing tasks, providing only writing prompts or headlines to generate text with LLMs. However, a significant portion of contemporary machine-generated content involves human input (Zhang et al. 2024). In contrast, our study focuses on the reverse: human-written text revised by LLMs. This practice, where people use AI to enhance, edit, or expand their writing, is increasingly common and accepted in various contexts but remains largely prohibited in academic settings.

Methods While training-based methods (Guo et al. 2023; Chen et al. 2023; Hu, Chen, and Ho 2023) achieved excellent performance due to large-scale data and high-cost training, they tended to overfit and were less effective in detecting the machine-revised text. Existing logit-based approaches (Solaiman et al. 2019; Gehrmann, Strobelt, and Rush 2019; Mitchell et al. 2023) relied on statistical analysis to evaluate information beyond the token level. GLTR (Gehrmann, Strobelt, and Rush 2019) combined a set of metric-based methods to assist human identification. DetectGPT (Mitchell et al. 2023) built on the observation that machine-generated texts occupy regions with steep negative log probability curvature, using this probability curvature to detect whether text originates from LLMs. This concept was further developed and improved in subsequent studies (Su et al. 2023; Mireshghallah et al. 2024; Bao et al. 2023; Zeng et al. 2024). While previous approaches generally relied on overall text features, we propose isolating stylistic features as the basis.

# 4.2 Preference Optimization

Direct Preference Optimization (Rafailov et al. 2024) can efficiently learn and align preferences from a pair of sampled texts. Yuan et al. (2024); Ethayarajh et al. (2024); Hong, Lee, and Thorne (2024); Park et al. (2024) is primarily for text-generation tasks. However, our study is the first to apply preference optimization to align with a distinct AI style (rather than human preferences) and to use this approach in the context of machine-revised text detection.

# 5 Conclusion

In this work, we have presented the “Imitate Before Detect” paradigm to detect machine-revised text by learning to imitate the writing style of LLMs. Specifically, we have proposed style preference optimization for aligning the detector with machine writing styles and leveraged style-conditional probability curvature to quantify log probability differences for effective detection. We have conducted extensive evaluations, demonstrating significant improvements in detection accuracy compared to existing state-of-the-art methods.