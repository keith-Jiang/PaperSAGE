# Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?

Michael-Andrei Panaitescu-Liess, Zora Che, Bang An, Yuancheng Xu, Pankayaraj Pathmanathan, Souradip Chakraborty, Sicheng Zhu, Tom Goldstein, Furong Huang

University of Maryland, College Park mpanaite, zche, bangan, ycxu, pan, schakra3, sczhu, tomg, furongh @umd.edu

# Abstract

Large Language Models (LLMs) have demonstrated impressive capabilities in generating diverse and contextually rich text. However, concerns regarding copyright infringement arise as LLMs may inadvertently produce copyrighted material. In this paper, we first investigate the effectiveness of watermarking LLMs as a deterrent against the generation of copyrighted texts. Through theoretical analysis and empirical evaluation, we demonstrate that incorporating watermarks into LLMs significantly reduces the likelihood of generating copyrighted content, thereby addressing a critical concern in the deployment of LLMs. However, we also find that watermarking can have unintended consequences on Membership Inference Attacks (MIAs), which aim to discern whether a sample was part of the pretraining dataset and may be used to detect copyright violations. Surprisingly, we find that watermarking adversely affects the success rate of MIAs, complicating the task of detecting copyrighted text in the pretraining dataset. These results reveal the complex interplay between different regulatory measures, which may impact each other in unforeseen ways. Finally, we propose an adaptive technique to improve the success rate of a recent MIA under watermarking. Our findings underscore the importance of developing adaptive methods to study critical problems in LLMs with potential legal implications.

# Introduction

In recent years, Large Language Models (LLMs) have pushed the frontiers of natural language processing by facilitating sophisticated tasks like text generation, translation, and summarization. With their impressive performance, LLMs are increasingly integrated into various applications, including virtual assistants, chatbots, content generation, and education. However, the widespread usage of LLMs brings forth serious concerns regarding potential copyright infringements. Addressing these challenges is critical for the ethical and legal deployment of LLMs.

Copyright infringement involves unauthorized usage of copyrighted content, which violates the intellectual property rights of copyright owners, potentially undermining content creators’ ability to fund their work, and affecting the diversity of creative outputs in society. Additionally, violators can face legal consequences, including lawsuits and financial penalties. For LLMs, copyright infringement can occur through (1) generation of copyrighted content during deployment and (2) illegal usage of copyrighted works during training. Ensuring the absence of copyrighted content in the vast training datasets of LLMs is challenging. Moreover, legal debates around generative AI copyright infringement vary by region, complicating compliance further.

Current lawsuits against AI companies for unauthorized use of copyrighted content (e.g., Andersen v. Stability AI Ltd, NYT v. OpenAI) highlight the urgent need for methods to address these challenges. In this paper, we focus on studying the effects of watermarking LLMs on two critical issues: (1) preventing the generation of copyrighted content, and (2) detecting copyrighted content in training data. We show that watermarking can significantly impact both the generation of copyrighted text and the detection of copyrighted content in training data.

Firstly, we observe that current LLM output watermarking techniques can significantly reduce the probability of LLMs generating copyrighted content, by tens of orders of magnitude. Our empirical results focus on two recent watermarking methods: UMD (Kirchenbauer et al. 2023) and Unigram-Watermark (Zhao et al. 2023). Both methods split the vocabulary into two sets (green and red) and bias the model towards selecting tokens from the green set by altering the logits distribution, thereby embedding a detectable signal. We provide both empirical and theoretical results to support our findings.

Secondly, we demonstrate that watermarking techniques can decrease the success rate of Membership Inference Attacks (MIAs), which aim to detect whether a piece of copyrighted text was part of the training dataset. Since MIAs exploit the model’s output, their performance can suffer under watermarking due to changes in the probability distribution of output tokens. Our comprehensive empirical study, including 5 recent MIAs and 5 LLMs, shows that the AUC of detection methods can be reduced by up to $1 6 \%$ in the presence of watermarks.

Finally, we propose an adaptive method designed to

10^20x less 目 ? 目 @ 目 自 @ 目目目 Generate Generate

enhance the success rate of a recent MIA (Shi et al. 2023) in detecting copyright violations under watermarking. This method applies a correction to the model’s output to account for the perturbations introduced by watermarks. By incorporating knowledge about the watermarking scheme, we improve the detection performance for pretraining data, counteracting the obfuscation caused by watermarking. Our contribution underscores the importance of continuously developing adaptive attack methodologies to keep pace with advances in defense mechanisms.

The rest of the paper is organized as follows. In the “Related Work” section, we review prior research on LLM watermarking and copyright. The “Setup and Notations” section formally introduces the problems we study. We then present our first two contributions and introduce the adaptive version of the Min- $K \%$ Prob membership inference attack in the following three sections. Finally, we provide concluding remarks in the last section. Additional experiments, theoretical results, and a discussion on the limitations of our work are included in the appendix 1.

# Related Work

Watermarks for LLMs. Language model watermarking techniques embed identifiable markers into output text to detect AI-generated content. Recent strategies incorporate watermarks during the decoding phase of language models (Zhao et al. 2023; Kirchenbauer et al. 2023). Aaronson (2023) develops the Gumbel watermark, which employs traceable pseudo-random sampling for generating subsequent tokens. Kirchenbauer et al. (2023) splits the vocabulary into red and green lists according to preceding tokens, biasing the generation towards green tokens. Zhao et al. (2023) employs a fixed grouping strategy to develop a robust watermark with theoretical guarantees. Liu et al. (2024) proposes to generate watermark logits based on the preceding tokens’ semantics rather than their token IDs to boost the robustness. Kuditipudi et al. (2023) and Christ, Gunn, and Zamir (2023) explore watermark methods that do not change the output textual distribution.

Copyright. Copyright protection in the age of AI has gained importance, as discussed by Ren et al. (2024). Vyas, Kakade, and Barak (2023) addresses content protection through near access-freeness (NAF) and developed learning algorithms for generative models to ensure compliance under NAF conditions. Prior works focus on training algorithms to prevent copyrighted text generation (Vyas, Kakade, and Barak 2023; Chu, Song, and Yang 2024), whereas our work emphasizes lightweight, inference-time algorithms. Other works have studied copyright in machine learning from a legal perspective. Hacohen et al. (2024) utilizes a generative model to determine the generic characteristics of works to aid in defining the scope of copyright. Elkin-Koren et al. (2023) demonstrates that copying does not necessarily constitute copyright infringement and argues that existing detection methods may detract from the foundational purposes of copyright law.

Additionally, we include a discussion on memorization and membership inference in the appendix.

# Setup and Notations

# Definitions

Let $D$ be a training dataset, $C$ be all the copyrighted texts, and $C _ { D }$ be all the copyrighted texts that are part of $D$ . We give definitions for the following setups.

Verbatim Memorization of Copyrighted Content. For a fixed $k \in \mathbb { N }$ , Carlini et al. (2022) defines a string $s$ as being memorized by a model if $s$ is extractable with a prompt $p$ of length $k$ using greedy decoding and the concatenation $p \oplus s \in D$ . We adopt a similar definition for verbatim memorization of copyrighted content but employ a continuous metric to measure it. Specifically, we measure verbatim memorization of a text $c \in C$ using the perplexity of the model on the copyrighted text $c _ { p }$ when given the prefix $p$ as a prompt (where $c _ { p }$ represents the text $c$ after removing its prefix $p$ ). Note that for $c _ { p } \ = \ c _ { p } ^ { ( 1 ) } \oplus c _ { p } ^ { ( 2 ) } \oplus \cdots \oplus c _ { p } ^ { ( \bar { n } ) }$ we compute the perplexity using the following formula $\begin{array} { r } { \mathtt { p e r p l e x i t y } ( c _ { p } | p ) } \end{array} =$ $\begin{array} { r } { \big ( \prod _ { i = 1 } ^ { n } \mathbb { P } ( c _ { p } ^ { ( i ) } | p \oplus c _ { p } ^ { ( 0 ) } \oplus c _ { p } ^ { ( 1 ) } \oplus \cdots \oplus c _ { p } ^ { ( i - 1 ) } \big ) ^ { - \frac { 1 } { n } } } \end{array}$ , where c(p0) is the empty string. In our experiments, p is either an empty string or the first 10, 20, or 100 tokens of $c$ . Lower perplexity thereby indicate higher levels of memorization.

MIAs for Copyrighted Training Data Detection. MIAs are privacy attacks aiming to detect whether a sample was part of the training set. We define an MIA for copyrighted data as a binary classifier $A ( \cdot )$ , which ideally outputs $A ( x ) = 1 , \forall x \ \in \ C _ { D }$ and $A ( x ) \ : = \ : 0 , \forall x \in C - C _ { D }$ . In practice, $A ( \cdot )$ is defined by thresholding a metric (e.g., perplexity), i.e., $A ( x ) = 1 , \forall \dot { x }$ such that $\mathtt { p e r p l e x i t y } ( x ) < t$ and 0, otherwise. Since the threshold $t$ needs to be set, prior work (Shi et al. 2023) uses AUC (Area Under the ROC

Curve) as an evaluation metric which is independent of $t$ .   
Note that we employ the same metric in our experiments.

LLM Watermarking. Watermarking LLMs consists of introducing signals during its training or inference that are difficult to detect by humans without the knowledge of a watermark key but can be detected using an algorithm if the key is known. We focus our paper on recent methods that employ logits distribution changes as a way of inserting watermark signals during the decoding process (Kirchenbauer et al. 2023; Zhao et al. 2023).

# MIAs

Current MIAs for detecting training data rely on thresholding various heuristics that capture differences in output probabilities for each token between data included in the training set and data that was not. Below, we present an overview of these heuristics.

Perplexity. This metric distinguishes between data used to train the model (members) and data that was not (non-members), as members are generally expected to have lower perplexity.

Smaller Ref, Lowercase and Zlib (Carlini et al. 2021). Smaller Ref is defined as the ratio of the log-perplexity of the target LLM on a sample to the log-perplexity of a smaller reference LLM on the same sample. Lowercase represents the ratio of the log-perplexity of the target LLM on the original sample to the log-perplexity of the LLM on the lowercase version of the sample. Zlib is defined as the ratio of the log-perplexity of the target LLM on a sample to the zlib entropy of the same sample.

Min- $. \mathbf { K } \%$ Prob (Shi et al. 2023). This heuristic computes the average of the minimum $K \%$ token probabilities outputted by the LLM on the sample. Note that this method requires tuning $K$ , so in all our experiments we chose the best result over $\bar { K } \% \in \{ 5 \% , 1 0 \% ,$ ， $2 \hat { 0 } \%$ , $3 0 \%$ , $4 0 \%$ , $5 0 \%$ , $6 0 \% \}$ .

# LLM Watermarking Methods

UMD (Kirchenbauer et al. 2023) splits the vocabulary into two sets (green and red) and biases the model towards the green tokens by altering the logit distribution. The hash of the previous token’s ID serves as a seed for a pseudorandom number generator used to split the vocabulary into these two groups. For a “hard” watermark, the model is forced not to sample from the red list at all. For a “soft” watermark, a positive bias $\delta$ is added to the logits of the green tokens before sampling. We focus our empirical evaluation on “soft” watermarks as they are more suitable for LLM deployment due to their smaller impact on the quality of the generated text.

Unigram-Watermark (Zhao et al. 2023) employs a similar approach of splitting the vocabulary into two sets and biasing the model towards one of the two sets. However, the split remains consistent throughout the generation. This choice is made to provide a provable improvement against paraphrasing attacks (Krishna et al. 2024).

# Watermarking LLMs Prevents Copyrighted Text Generation

In this section, we study the effect of LLM watermarking techniques on verbatim memorization. We discuss the their implications for preventing copyrighted text generation.

Datasets. We consider 4 versions of the WikiMIA benchmark (Shi et al. 2023) with 32, 64, 128, and 256 words in each sample and only consider the samples that were very likely part of the training set of all the models we consider (labeled as 1 in Shi et al. (2023)). We consider these subsets as a proxy for text that was used in the training set, and the model may be prone to verbatim memorization. From now on, we refer to this subset as the “training samples” or “training texts”. Similarly, we consider BookMIA dataset (Shi et al. 2023), which contains samples from copyrighted books.

Metric. We measure the relative increase in perplexity on the generation of training samples by the watermarked model compared to the original model. We report the increase in both the minimum and average perplexity over the training samples. Note that a large increase in perplexity corresponds to a large decrease in the probability of generating that specific sample, as shown later in this section. When computing the perplexity, we prompt the model with an empty string, the first 10, and the first 20 tokens of the targeted training sample, respectively. In the BookMIA dataset, we designate the initial 100 or 256 tokens as the prompt. This is because each BookMIA sample contains 512 words, which is larger than the sample size in WikiMIA.

Models. We conduct our empirical evaluation on 5 recent LLMs: Llama-30B (Touvron et al. 2023), GPTNeoX-20B (Black et al. 2022), Llama-13B (Touvron et al. 2023), Pythia-2.8B (Biderman et al. 2023) and OPT-2.7B (Zhang et al. 2022).

# Empirical Evaluation

In Table 1, we show the increase in perplexity on the training samples when the model is watermarked relative to the unwatermarked model. We observe that for Llama-30B, Unigram-Watermark induces a relative increase of 4.1 in the minimum and 34.1 in the average perplexity. Note that a relative increase of 4.1 in perplexity for a sample makes it more than $4 . 3 \times 1 0 ^ { 2 2 }$ times less likely to be generated. This is based on a sample with only 32 tokens, which is likely a lower bound since the number of tokens is typically larger than the number of words. We observe consistent results over several models and prompt lengths. For all experiments, unless otherwise specified, we use a fixed strength parameter $\delta \ = \ 1 0$ for watermark methods and a fixed percentage of $5 0 \%$ green tokens. All the results are averaged over 5 runs with different seeds for the watermark methods. We include additional results on WikiMIA-64,

Table 1: Measuring the reduction in verbatim memorization of training texts on WikiMIA-32. We report the relative increase in both the minimum and average perplexity between the watermarked and unwatermarked models, where larger values correspond to less memorization. Note that “P.” stands for “prompt length”.   

<html><body><table><tr><td></td><td></td><td>Llama-30B</td><td>Llama-13B</td></tr><tr><td></td><td>P.</td><td>Min. Avg.</td><td>Min. Avg.</td></tr><tr><td rowspan="2">UMD</td><td>0</td><td>3.3 31.2</td><td>4.9 34.3</td></tr><tr><td>10 20</td><td>2.8 28.7 2.4 30.1</td><td>3.5 31.9 33.4</td></tr><tr><td rowspan="4">Unigram</td><td>0</td><td>4.1 34.1</td><td>3.5 5.0</td></tr><tr><td>10</td><td>3.0 31.7</td><td>36.6 4.0 34.3</td></tr><tr><td>20</td><td>2.4 31.5</td><td>3.4</td></tr><tr><td></td><td></td><td>34.0</td></tr></table></body></html>

Table 2: Measuring the reduction in verbatim memorization of training texts on BookMIA. We report the relative increase in both the minimum and average perplexity between the watermarked and unwatermarked models, where larger values correspond to less memorization. Note that “P.” stands for “prompt length”.   

<html><body><table><tr><td></td><td></td><td>Llama-30B</td><td>Llama-13B</td></tr><tr><td></td><td>P.</td><td>Min. Avg.</td><td>Min. Avg.</td></tr><tr><td rowspan="3">UMD</td><td>0</td><td>1.5 33.7</td><td>2.4 41.2</td></tr><tr><td>10</td><td>1.5 33.6</td><td>2.3 41.0</td></tr><tr><td>20 100</td><td>1.4 33.5 1.3 32.9</td><td>2.3 40.8 1.9 40.3</td></tr><tr><td rowspan="4">Unigram</td><td>0</td><td>1.6 36.4</td><td>2.4</td></tr><tr><td>10</td><td>1.6 36.3</td><td>44.5 2.4 44.3</td></tr><tr><td>20</td><td>1.5 36.1</td><td>2.3</td></tr><tr><td>100 1.4</td><td>35.5</td><td>44.2 1.8 43.6</td></tr></table></body></html>

WikiMIA-128 and WikiMIA-256 in Tables 7, 8 and 9, respectively, in the appendix. We observe that our findings are consistent across models and splits of WikiMIA. Finally, we include the complete version of Table 1 in the appendix (Table 6), which shows results for additional models and random logit perturbations with the same strength as the watermarking methods. Overall, the additional results are consistent with our previous findings.

In Figure 2, as well as Figure 5 from the appendix, we study the influence of the strength of the watermark $\delta$ on the relative increase in both the minimum and average perplexity on the WikiMIA-32 training samples. In this experiment, we also consider a baseline of generating text freely to study the impact of watermarks on the quality of text relative to the impact on training samples’ generation (here, perplexity is computed by an unwatermarked model). All the results are averaged over 5 runs with different seeds for the watermark methods. In the case of free generation, we generate 100 samples for 5 different watermarking

Avg Train Samples (given 20 tokens) Train Samples (given10tokens) Train Samples (given O tokens) -Free Samples (givenO tokens) 0 2 4 6 8 10 12 14 16 Watermark Strength Min 2 4 6 8 10 12 14 16 Watermark Strength

seeds and average the results. The length of the generated samples is up to 42 tokens, which is approximately 32 words in the benchmark (on a token-to-word ratio of $4 : 3 :$ ). The results show an exponential increase in the perplexity of the training samples with the increase in watermark strength, while the generation quality is affected at a slower rate. This suggests that even if there is a trade-off between protecting the generation of text memorized verbatim and generating high-quality text, finding a suitable watermark strength for each particular application is possible. Examples of generated samples at varying watermark strengths are provided in the appendix.

Approximate Memorization. Informally, we consider a training sample approximately memorized by a model if, given its prefix, it is possible to generate a completion that is similar enough to the ground truth completion. In our experiments, we use models fine-tuned on a subset of BookMIA (details provided in the appendix) and we consider Normalized Edit Similarity (referred to as edit similarity from now on) and BLEU score as similarity measures, as in (Ippolito et al. 2023). Note that we consider both word-level and token-level variants for the BLEU score. The range for each metric is between 0 and 1, where values close to 1 represent similar texts. In all experiments, since all the samples are 512 words long, we consider the first 256 words as the prefix and the last 256 words as the ground truth completion. We present the results for edit similarity with the UMD watermark in Figure 3, and the complete results—using all metrics and including the Unigram watermark—in Figure 7, averaged over 20 runs with different random seeds. Note that the duplication factor (shown on x-axis) represents the number of times the target copyrighted text is duplicated. We observe that for high levels of memorization, a strong watermark significantly reduces the similarity between the generated completion and the ground truth (copyrighted) one.

![](images/d8954c617ecfcbf3291ebab89c2bed55713ed7ffe18921bf09d15718bd75c01e.jpg)  
Figure 3: Edit similarity between the generated completion and the ground truth when considering different watermark strengths and memorization levels.

Takeaways. Watermarking significantly increases the perplexity of generating training texts, reducing verbatim memorization likelihood. This is achieved with only a moderate impact on the overall quality of generated text. This suggests that watermark strength can be effectively tailored to balance verbatim memorization and text quality for specific applications. Finally, we believe that our findings on WikiMIA—which does not necessarily contain copyrighted data—directly extend to the generation of copyrighted text verbatim, as this constitutes a form of verbatim memorization of the training data. To confirm, we run similar experiments on a dataset containing copyrighted data (BookMIA) and include the results in the Table 2. Additionally, we consider finetuning Llama-7B (Touvron et al. 2023) on BookMIA while controlling memorization by duplicating training samples. Detailed information about this experiment is provided in the appendix.

# Impact of Watermarking on Pretraining Data Detection

Datasets. We revisit the WikiMIA benchmark as discussed in the previous section. We consider the full datasets, rather than the subset of samples that were part of the training for models we study. Additionally, we consider the BookMIA benchmark, which contains copyrighted texts.

Metrics. We follow the prior work (Shi et al. 2023; Duarte et al. 2024) and report the AUC and AUC drop to study the detection performance of the MIAs. Note that this metric has the advantage of not having to tune the threshold for the detection classifier.

![](images/1d42fe7f74ffb9179130b5773d4ba8a99884b482ae5d47b04b2bf1f94025813a.jpg)  
Figure 4: AUC drop due to watermarking for each MIA when varying the strength of the watermark.

Models. We conduct experiments on the same LLMs as in the previous section. Additionally, for the Smaller Ref method that requires a smaller reference model along with the target LLM, we consider Llama-7B, Neo-125M, Pythia-70M, and OPT-350M as references.

# Empirical Evaluation

In Table 4, we show the AUC for the unwatermarked and watermarked models using the UMD scheme, as well as the drop between the two. We observe that watermarking reduces the AUC (drop shown in bold in the table) by up to $1 4 . 2 \%$ across 4 detection methods and 5 LLMs. All the experiments on watermarked models are run with 5 different seeds and we report the mean and standard deviation of the results. We also report the AUC drop, which is computed by the difference between the AUC for the unwatermarked model and the mean AUC over the 5 runs for the watermarked model. Additionally, while the experiments from Table 4 are conducted on WikiMIA-256, we observe similar trends for WikiMIA-32, WikiMIA-64, and WikiMIA-128 in the appendix. We also study the impact of the watermark’s strength on the AUC drop for Llama-30B in Figure 4 and for the other models in Figure 6 from the appendix. Note that we considered WikiMIA-256 for these experiments. We observe that higher watermark strengths generally induce larger AUC drops.

In addition to the 4 detection methods, we also consider Smaller Ref attack, which we include in Table 13 of the appendix. We consider different variations, including an unwatermarked reference model and a watermarked one with a similar strength but a different seed or with both strength and seed changed in comparison to the watermarked target model. The baseline is an unwatermarked model with an unwatermarked reference model. We observe the AUC drops in all scenarios (up to $1 6 . 4 \%$ ), which is consistent with our previous findings.

Table 3: AUC of each MIA for the unwatermarked (top of each cell), watermarked models (middle of each cell), and the drop between the two (bottom of each cell) on BookMIA using UMD scheme.   

<html><body><table><tr><td></td><td>Llama-30B</td><td>Llama-13B</td></tr><tr><td>PPL</td><td>85.4% 84.7 ±1.4% 0.7%</td><td>68.2% 67.6 ± 2.5% 0.6%</td></tr><tr><td>Lowercase</td><td>87.9% 80.9 ±3.1% 7.0%</td><td>77.6% 67.2 ± 4.0% 10.4%</td></tr><tr><td>Zlib</td><td>82.5% 77.8 ± 1.2% 4.7%</td><td>62.5% 57.1 ± 2.0% 5.4%</td></tr><tr><td>Min-K%Prob</td><td>85.1% 85.0 ±1.0% 0.1%</td><td>70.2% 68.5 ± 0.1% 1.7 %</td></tr></table></body></html>

We also experiment with several percentages of green tokens for a fixed watermark strength of $\delta = 1 0$ . We show the results in Table 14 of the appendix. We observe that for all models, in at least $8 0 \%$ of the cases all of the attacks’ AUCs are negatively affected (positive drop value), suggesting that, in general, finding a watermarking scheme that reduces the success rates of the current MIAs is not a difficult task. Note that the experiments are run on WikiMIA for UMD scheme and the results are averaged over 5 watermark seeds.

Takeaways. Watermarking can significantly reduce the success of membership inference attacks (MIAs), with AUC drops up to $1 6 . 4 \%$ . By varying the percentage of green tokens as well as the watermark’s strength, we observe that watermarking schemes can be easily tuned to negatively impact the detection success rates of MIAs. Finally, we conduct experiments on the BookMIA dataset and observe results consistent with our previous findings. These results are included in Table 3.

# Improving Detection Performance with Adaptive Min- ${ \bf K \% }$ Prob

This section demonstrates how an informed, adaptive attacker can improve the success rate of a recent MIA, Min- $. \mathrm { K } \%$ Prob. Our main idea is that an attacker with knowledge of the watermarking technique (including green-red token lists and watermark’s strength $\delta$ ) can readjust token probabilities. This is possible even without additional information about the logit distribution, relying solely on the probability of each token from the target sample given the preceding tokens. Our approach relies on two key assumptions. First, knowledge of the watermarking scheme, which aligns with assumptions made in prior work on public watermark detection (Kirchenbauer et al. 2023). Second, access to the probability of each token in a sample, given the previous tokens—an assumption also made by the Min- $K \%$ Prob method (Shi et al. 2023).

Threat model. (1) The attacker’s goal is to infer whether specific samples are part of the training set or not. In our setting, the attacker is not malicious, as the goal is to detect copyright violations. (2) Regarding the attacker’s knowledge, we assume the attacker knows the watermarking method and its parameters (green and red lists, and the watermark strength), which aligns with the assumption made in prior work by Kirchenbauer et al. (2023) for public watermark detection. (3) As for the attacker’s capabilities, we assume they can access the probabilities for each token in the given samples, similar to what a copyright auditor may have access to. This also mirrors the assumption made by Shi et al. (2023) in the context of training data detection.

Our method described in Algorithm 1 is based on the observation that if the denominator of softmax function (i.e., $\textstyle \sum _ { i } e ^ { z _ { i } }$ , where $z _ { i }$ is the logit for the $i$ -th vocabulary) does not vary significantly when generating samples with the watermarked model (and similarly for the unwatermarked model), then we can readjust the probabilities of the green tokens by “removing” the bias $\delta$ . More precisely, assuming the approximation for the denominator of softmax is good, then the probability for each token $t _ { i }$ in an unwatermarked model will be around $\frac { e ^ { L _ { i } } } { c }$ , where $\mathbf { \mathcal { L } } _ { i }$ is the logit corresponding to the token $t _ { i }$ and $c$ is a constant. However, for a watermarked model, if the token $t _ { i }$ is green, then the probability would be approximated by eLid+δ , where d is again a constant, while in the case $t _ { i }$ is red the probability bwyi lwbateerarmoaurnkdi $\frac { e ^ { L _ { i } } } { d }$ w. eTdo vciodmeptheenspartoebfaobr ltihtey obifagsrienetnrotdoukceends by $e ^ { \delta }$ and this way we end up with probabilities that are just a scaled (by $\textstyle { \frac { c } { d } }$ ) version of the probabilities from the unwatermarked model. The scaling factor will not affect the orders between the samples when computing the average of the minimum $K \%$ log-probabilities as long as the tested sentences are approximately the same length, which is an assumption made by Shi et al. (2023) as well.

Despite the strong assumption we assumed regarding the approximation of the denominator, empirical results show that our method effectively improves the success rate of Min- $K \%$ under watermarking. We show results for two LLMs in Table 5 and include the complete results for 5 LLMs in Table 17 from the appendix. We observe that our method improves over the baseline in $9 5 \%$ of the cases, and the increase is as high as $4 . 8 \%$ (averaged over 5 runs).

Finally, we also consider adaptive versions of the Lowercase and Zlib methods. Our findings show that these adaptive methods outperform the baselines in at least $80 \%$ of cases. Detailed results are provided in the appendix.

Takeaways. We demonstrate that an adaptive attacker can leverage the knowledge of a watermarking scheme to increase the success rates of recent MIAs.

<html><body><table><tr><td></td><td>Llama-30B</td><td>NeoX-20B</td><td>Llama-13B</td><td>Pythia-2.8B</td><td>OPT-2.7B</td></tr><tr><td>PPL</td><td>72.0% 70.6 ± 1.9% 1.4%</td><td>71.3% 64.7 ± 2.3% 6.6%</td><td>71.2% 70.0 ± 2.6% 1.2%</td><td>67.8% 64.4 ± 1.9% 3.4%</td><td>60.5% 54.9 ± 2.2% 5.6%</td></tr><tr><td>Lowercase</td><td>68.1% 63.8 ± 4.5% 4.3%</td><td>68.2% 55.4 ± 5.5% 14.2%</td><td>65.5% 61.6 ±3.8% 3.9%</td><td>62.9% 58.7 ± 3.2% 4.2%</td><td>58.9% 49.7 ± 2.9% 9.2%</td></tr><tr><td>Zlib</td><td>72.7% 72.0 ± 1.6% 0.7%</td><td>73.2% 66.6 ± 2.0% 6.6%</td><td>73.1% 71.6 ± 2.3% 1.5%</td><td>69.2% 66.1 ±1.2% 3.1%</td><td>62.7% 58.1 ±1.8% 4.6%</td></tr><tr><td>Min-K% Prob</td><td>71.8% 70.5 ± 1.8% 1.3%</td><td>78.0% 76.2 ±2.1% 1.8%</td><td>72.9% 70.4 ± 3.2% 2.5%</td><td>71.0% 69.5 ± 1.6% 1.5%</td><td>65.5% 63.1 ± 3.4% 2.4%</td></tr></table></body></html>

Table 4: AUC of each MIA for the unwatermarked (top of each cell), watermarked models (middle of each cell), and the drop between the two (bottom of each cell) on WikiMIA-256 using UMD scheme.

# Algorithm 1: Adaptive Min- $K \%$ Prob

<html><body><table><tr><td>Require :Tokenized target sample t = t1 t2 . tn,accessto the probabilityofthe target (watermarked) LLM f to generate ti given the i-1 previous tokens and to (empty string) f(tilto  t1  .. ti-1) (similar assumption as Min-K% Prob algorithm),K,we assume we know the watermarking scheme (e.g.,for public watermark detection purposes),i.e.we know the green and red lists as well as δ. Output : Adjusted average of the minimum K% token probabilities when generating t1  t2  ...  tn adj-prob ←{} The set of adjusted probabilities fori ∈1,2,...,n do pf(ti)←f(tilto④ti④..ti-1) D Probability of ti when the model is watermarked if ti is green then √ Adjust the probability if the token is green else adj-prob ← adj-prob U{𝑝f(t) }</td></tr></table></body></html>

Table 5: We show the AUC of Min- $\% \mathrm { K }$ Prob (referred as “Not adapt.”) and our method (referred as “Adapt.”) when using UMD watermarking scheme. We highlight the cases when our method improves over the baseline.   

<html><body><table><tr><td></td><td></td><td>Llama-30B</td><td>Llama-13B</td></tr><tr><td>WikiMIA 32</td><td>Not adapt. Adapt.</td><td>66.2% 68.5%</td><td>64.5% 66.3%</td></tr><tr><td>WikiMIA 64</td><td>Not adapt. Adapt.</td><td>64.4% 67.3%</td><td>62.8% 64.9%</td></tr><tr><td>WikiMIA 128</td><td>Not adapt. Adapt.</td><td>70.0% 73.1%</td><td>68.9% 71.0%</td></tr><tr><td>WikiMIA 256</td><td>Not adapt. Adapt.</td><td>70.5% 71.3%</td><td>70.4% 72.4%</td></tr></table></body></html>

# Conclusion and Discussion

Watermarking LLMs has unintended consequences on methods towards copyright protection. Our experiments demonstrate that while watermarking may be a promising solution to prevent copyrighted text generation, watermarking also complicates membership inference attacks that may be employed to detect copyright abuses. Watermarking can be a double-edged sword for copyright regulators since it promotes compliance during generation time, while making training time copyright violations harder to detect. We hope our work furthers the discussion around watermarking and copyright issues for LLMs.