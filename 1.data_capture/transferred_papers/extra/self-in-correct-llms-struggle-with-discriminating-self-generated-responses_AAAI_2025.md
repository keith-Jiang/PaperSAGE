# SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated Responses

Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, Daniel Khashabi

Johns Hopkins University djiang21, jzhan237 @jhu.edu

# Abstract

Can LLMs consistently improve their previous outputs for better results? For this to be true, LLMs would need to be better at discriminating among previously-generated alternatives, than generating initial responses. We explore the validity of this hypothesis in practice. We first formulate a unified framework that allows us to compare the generative and discriminative capability of any model on any task. In our resulting experimental analysis of several open-source and industrial LLMs, we observe that model’s are not reliably better at discriminating among previously-generated alternatives than generating initial responses. This finding challenges the notion that LLMs may be able to enhance their performance only through their own judgment.

generated answers   
Generation Prompt A :“Utah” Question: What was :“Utah (State)” the last US state to reintroduce alcohol D :“State of Utah” after prohibition? :“Miami”   
Discrimination Prompt   
Question: What was the last US state to reintroduce alcohol   
after prohibition? Here are four choices: $_ { A I }$ : $\{ \mathbf { A _ { 1 } } \}$ , A2: $\{ \mathbf { A _ { 2 } } \}$ ,   
A3: $\{ \mathbf { A _ { 3 } } \}$ , $\mathbf { \nabla } \cdot \mathbf { \vec { A } } \mathbf { \vec { \tau } }$ : $\{ \mathbf { A _ { 4 } } \}$ to choose from, please give an answer in 1, 2, 3 and 4 that you think best answers the question. 4 Utah sample

# 1 Introduction

The promise of Large Language Models (LLMs) that can self-improve has brought both excitement and fear about the future impact of AI. However, it remains a mystery what is needed for LLMs to continually self-improve (Huang et al. 2023).1 A crucial aspect of human learning involves reflecting on one’s actions. This self-improvement is feasible because individuals can identify their own mistakes and adjust their future decisions accordingly (Mayo 1996; Corder 1967). This principle should be applicable to LLMs as well.

For LLMs to reliably self-improve based on their decisions, the ability to discriminate (distinguish) the goodness of their own prior generations should surpass the ability to generate good solutions directly. Given the importance of this capability, it is worth raising a question about the foundations of self-discrimination: Are LLMs really better at discrimination than generation?

This paper seeks to answer this question by proposing the SELF-[IN]CORRECT hypothesis (§3.2): LLMs are not reliably better at discriminating among previously-generated alternatives than generating initial responses. Determining the validity of this hypothesis is crucial, as existing studies provide initial evidence suggesting that the capability to distinguish between LLM-generated options is both a sufficient (Tyen et al. 2023) and necessary (Huang et al. 2023) condition for self-improvement.

It is non-trivial to compare LLMs’ generative capability with their discriminative capability on the same footing. West et al. (2023) compares these two capabilities, albeit in a slightly different setting. West et al. (2023) measure a model’s ability to discriminate (identify) the ground-truth answer among distractor options. However, ground-truth answers are not always likely to be among a model’s generated outputs. In contrast, our work quantify a model’s ability to discriminate its self-generated candidate answers, which is more aligned with the mechanisms of LLM self-improvement. To better measure these abilities, we implement a two-phase methodology depicted in Figure 1. In the first phase, we generate multiple outputs using a temperature setting greater than zero, then randomly select one of these outputs, using its evaluation as an indicator of generative performance. In the second phase, we instruct the LLM to choose the best answer from its own outputs, with the evaluation of the selected answer serving as the indicator of discriminative performance. Overall, this approach is consistent with the actual procedure employed in various self-improvement studies (Madaan et al. 2023; Yuan et al. 2024). Further details can be found in $\ S 3 . 2$ .

To back up the SELF-[IN]CORRECT hypothesis, we conduct experiments covering widely used LLMs (Phi-3, LLaMA series, Mixtral series, and GPT series) on a diverse set of tasks including mathematics, world knowledge acquisition, truthful question answering, and instructions following. Our investigation in $\ S 4 . 2$ reveals a surprising finding: while there is evidence that humans find the task of discrimination simpler than generation (Alexander 2003), on various evaluated tasks we observed that LLMs are not consistently better at discriminating among previously-generated alternatives than generating initial responses.

We conduct a series of analyses to uncover the potential root causes of SELF-[IN]CORRECT. First, we investigate alternative prompt choices (§5.1) to ensure that SELF[IN]CORRECT is not simply a result of sub-optimal prompt design. Our findings indicate that SELF-[IN]CORRECT persists with the inclusion of additional in-context learning examples and chain-of-thought demonstrations. Next, we examine the role of pre-training objectives (§5.2) and discover that SELF-[IN]CORRECT does not appear in non-autoregressive models (e.g., the FLAN family). Third, we discuss the apparent contradiction of our method with recent studies on self-improvement (Madaan et al. 2023; Yuan et al. 2024) (§5.3). Finally, we highlight the potential implications of our findings and a new experimental setting where more easily distinguishable incorrect options are introduced (§6).

Our contributions in this paper are two-fold:

• We develop a unified framework that facilitates the testing of both generative and discriminative capabilities of any LLM on any task; • We conduct experiments on widely used LLMs and collected empirical evidence to support SELF-[IN]CORRECT. We also provided additional experiments to better understand SELF-[IN]CORRECT and its implications.

# 2 Related Work

Self-Improvement with LLMs. The concept of selfimprovement existed before the LLM era. Earlier approaches employed generative adversarial networks (GANs) (Subramanian et al. 2017; Yu et al. 2017) to improve NLP systems via self-generated feedback. Welleck et al. (2023) trained a separate corrector model to iteratively refine generations.

In the era of LLM, self-improvement with self-feedback has also been studied in various forms (Pan et al. 2023; Saunders et al. 2022). Self-Instruct (Wang et al. 2023b) improves the instruction-following capabilities of pre-trained language models by bootstrapping off their own generations. Yuan et al. (2024) employs LLMs to provide rewards for their own generation. Chen et al. (2024b) uses a self-play mechanism where the LLM refines its capability by playing against instances of itself. Shinn, Labash, and Gopinath (2023) achieves selfimprovement by having the model generate verbal reflection on its own outputs at inference time. Several other recent studies (Madaan et al. 2023; Liu et al. $2 0 2 3 \mathrm { a }$ ; Butt et al. 2024; Krishna 2023; Wang et al. 2023c) also adopted this idea and applied it to different tasks.

The success stories mentioned in previous paragraph show that when external ground-truth feedback is available, LLMs can effectively engage in self-improvement. Gou et al. (2024) demonstrates that LLMs can verify and correct their initial responses through interactions with various external tools. Similarly, Tyen et al. (2023) and Shinn, Labash, and Gopinath (2023) have shown that ground-truth feedback can significantly enhance LLM performance across various tasks.

However, in the absence of ground-truth feedback—where LLMs must refine their initial answers based solely on their inherent capabilities (i.e., intrinsic self-improvement)—the situation changes. Critics have argued (Huang et al. 2023; Valmeekam, Marquez, and Kambhampati 2023; Tyen et al. 2023) that the reported self-improvement on reasoning tasks may be no more effective than self-consistency (Wang et al. 2022a), and that such improvements are often a result of an inferior initial response. Our work adopts a similar intrinsic self-improvement setting, and we explore LLMs’ generative and discriminative capacities beyond reasoning tasks.

Discrepancy between LLM generation and discrimination. For humans, distinguishing a good solution from a bad one is often easier than coming up with a solution from scratch (Alexander 2003). However, recent studies are starting to question if the same applies to LLMs. West et al. (2023) and Tan et al. (2024) investigated multiple NLP tasks, and showed that LLMs often struggle to understand their own outputs. To evaluate the generation and discrimination performance of LLMs, Liu et al. (2023b) conducted experiments focusing on summarization. Arora and Kambhampati (2023) and Chen et al. (2024c) conducted similar experiments in the domain of planning. Our work differs from previous research by evaluating this discrepancy on a wider range of tasks using a unified metric while also trying to uncover the reasons behind it.

Using LLMs for self-evaluation. Recent studies indicate the potential of LLM evaluation that is close to human level (Chiang and Lee 2023; Gilardi, Alizadeh, and Kubli 2023; Lin et al. 2024). However, for the task of selfevaluation, concerns have been raised by Valmeekam, Marquez, and Kambhampati (2023) and Huang et al. (2023), who pointed out that LLM encounters difficulties in selfevaluating its generation for mathematical tasks. Further research by Stechly, Valmeekam, and Kambhampati (2024), Stechly, Marquez, and Kambhampati (2023) and Valmeekam, Marquez, and Kambhampati (2023) has uncovered models’ limitations on self-evaluation for tasks requiring complex reasoning and planning. Compared to these works, our work seeks to explore the efficacy of LLM self-evaluation in a broader range of tasks.

# 3 SELF-[IN]CORRECT

We formally define our evaluation setting (exemplified in Figure 1), and present our hypothesis.

# 3.1 Establishing an Evaluation Criteria to Compare Generation vs. Discrimination

Given a task $T$ with an evaluation dataset $D = \{ ( x _ { i } , y _ { i } ) \} _ { i = 1 } ^ { m }$ and evaluation metric $f$ , we use the same LLM, denoted by $P _ { \mathrm { L M } }$ , for both generation and discrimination. For each evaluation input $x _ { i }$ , we first sample $n$ candidate generations $g _ { 1 } ( x _ { i } ) , \dots , g _ { n } ( x _ { i } ) \sim P _ { \mathrm { L M } } ( x _ { i } )$ using the default task prompt (generation prompt). We use a low temperature during sampling to ensure the generated outputs are all highly probable.

Evaluating generation. The performance of the generative phase for each evaluation sample $x _ { i }$ is computed applying the evaluation metric $f$ to a randomly chosen generation from the $n$ candidate generations $\begin{array} { r l } { G ( x _ { i } ) } & { { } = } \end{array}$ $\mathsf { \bar { \{ } }  g _ { 1 } ( x _ { i } ) , g _ { 2 } ( x _ { i } ) , \ldots , g _ { n } ( x _ { i } ) \}$ :

$$
S _ { \mathrm { g e n } } ( x _ { i } ) = f \Bigl ( g _ { \mathrm { r a n d } } ( x _ { i } ) , y _ { i } \Bigr ) ,
$$

where $g _ { \mathrm { r a n d } }$ is a randomly-sampled generation $g _ { \mathrm { r a n d } } ( x _ { i } ) \sim$ $G ( x _ { i } )$ . The notation $f ( \bar { g _ { j } } ( x _ { i } ) , \bar { y _ { i } } )$ represents the metric $f$ applied to the $j$ -th generation output for the $i$ -th evaluation sample and $g _ { \mathrm { r a n d } } ( x _ { i } )$ represents one random candidate generations for that sample. Because the candidate generations for each sample are produced by sampling from the language model $P _ { \mathrm { L M } }$ using the same hyper-parameters (temperature, top- $p$ , etc), choosing a random candidate from $G ( \bar { x } _ { i } )$ is essentially equivalent to generating an output directly from $P _ { \mathrm { L M } } ( x _ { i } )$ . The overall generation performance $S _ { \mathrm { g e n } }$ is the average of $S _ { \mathrm { g e n } } ( x _ { i } )$ across all samples:

$$
S _ { \mathrm { g e n } } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } S _ { \mathrm { g e n } } ( x _ { i } ) .
$$

Evaluating discrimination. To assess the discrimination performance, we feed the generations back to $P _ { \mathrm { L M } }$ and prompt it to identify the most suitable answer. For each task $T$ , we construct a discriminative prompt $p _ { \mathrm { d i s c } , T }$ (the prompts are available in Appendix D) and feed it the $n$ -many generated responses. Note that the ordering of generated candidates is always random as the sampling is conducted uniformly with temperature $> 0$ . We tried reordering the candidates before sending them for discrimination and the result remains very similar. Using few-shot prompting, we guide $P _ { \mathrm { L M } }$ to output the label of the preferred chosen answer and the label chosen in $\{ 1 , 2 , \ldots , n \}$ is determined by greedily decoding the output of $P _ { \mathrm { L M } } ( \cdot | \dot { x } _ { \mathrm { d i s c , } T } ( G ( x _ { i } ) ) )$ . The discrimination performance for each sample $i$ is quantified by:

$$
S _ { \mathrm { d i s c } } ( x _ { i } ) = f \Big ( g _ { \mathrm { c h o s e n } } ( x _ { i } ) , y _ { i } \Big ) .
$$

To derive an overall measure of discrimination performance $S _ { \mathrm { d i s c } }$ , we average the individual scores $S _ { \mathrm { d i s c } } ( x _ { i } )$ across all samples:

$$
S _ { \mathrm { d i s c } } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } S _ { \mathrm { d i s c } } ( x _ { i } ) .
$$

We also consider evaluating discriminative ability by calculating the absolute score of each candidate separately and selecting the best candidate. But we didn’t find much difference compared to the setup here (details in Appendix D).

# 3.2 Hypothesis Formulation

Given the above definitions, our main hypothesis becomes easy to formalize. For any given task, denote DG-DIFF as the difference between discrimination performance and generation performance,

$$
\mathrm { D G } \mathrm { - } \mathrm { D I F F } = S _ { \mathrm { d i s c } } - S _ { \mathrm { g e n } } .
$$

Our main hypothesis is:

SELF-[IN]CORRECT. LLMs are not reliably better at discriminating among previously generated alternatives $( S _ { \mathrm { d i s c } } )$ than generating initial responses $( S _ { \mathrm { g e n } } )$ and hence, $\mathrm { D G - D I F F } = S _ { \mathrm { d i s c } } - S _ { \mathrm { g e n } } \leq 0 .$ .

Hypothesis testing for SELF-[IN]CORRECT. To validate SELF-[IN]CORRECT, one can apply the framework of statistical hypothesis testing (Dror et al. 2018; Sadeqi Azer et al. 2020). We treat SELF-[IN]CORRECT as the null hypothesis $\mathbf { \left( H _ { 0 } \right) }$ to provide an objective basis for testing In this context, the conventional wisdom that discrimination is better than generation serves as the alternative hypothesis $( \mathbf { H } _ { 1 } )$ . To reject the null hypothesis ${ \bf { H } } _ { 0 }$ , it must be demonstrated that DG-DIFF is a sufficiently large positive value to justify its rejection. Details of the hypothesis testing on our experimental datasets are provided in $\ S 4 . 1$ .

Design choices for hypothesis testing. An important design choice in our framework is that the candidate generations $\bar { G ( x _ { i } ) }$ are shared across the generative and discriminative phases. This design choice allows us to formulate the generative phase as a random multiple choice among pre-generated candidates. As a result, it allows a fair comparison with the discriminative phase, where the task is using LLM for multiple choice among the same candidates.2

Our framework applies the task’s original metrics in both the generative and discriminative phases, which ensures consistency across assessments. By eliminating the need for human input, our framework is more scalable and cost-effective than West et al. (2023) and Zheng et al. (2023a), which depend on human annotation for discrimination. Our metrics are also closely aligned to the actual process that’s employed in self-improvement literature (Shinn, Labash, and Gopinath 2023; Madaan et al. 2023), where the model is asked to choose the best answer from a list of generations. Nevertheless, we would like to mention that because generation and discrimination are two very different processes, the metrics used in this paper are only proxies to evaluate those two important capabilities.

# 4 Empirical Support for SELF-[IN]CORRECT

In this section, we describe our experimental setup (§4.1) and lay out the main findings (§4.2).

Table 1: Configuration of experimental tasks. “Split” specifies which subset the data originates from. “#Eval” indicates the number of instances used for evaluation. “#Shots” specifies the number of few-shot examples employed for evaluation. To evaluate TruthfulQA generations, we follow Lin, Hilton, and Evans (2022) and develop two “GPT-judges” by fine-tuning GPT-3 models with provided data.   

<html><body><table><tr><td>Task</td><td>Split</td><td>#Eval</td><td>#Shots</td><td>Task Type</td><td>Metric f(.)</td><td>Metric scale</td></tr><tr><td>GSM8K</td><td>Test</td><td>1319</td><td>2</td><td>MathWordProblem</td><td>Accuracy</td><td>[0,100]</td></tr><tr><td>TriviaQA</td><td>Val</td><td>17944</td><td>2</td><td>Question Answering</td><td>Accuracy</td><td>[0,100]</td></tr><tr><td>MT-Bench</td><td>Test</td><td>160</td><td>3</td><td>Instruction Following</td><td>GPT-4 score</td><td>[0,10]</td></tr><tr><td>TruthfulQA</td><td>Val</td><td>817</td><td>2</td><td>Question Ans wering</td><td>GPT-judge</td><td>[0,100]</td></tr></table></body></html>

# 4.1 Experimental Setup

Tasks. A summary of the tasks we evaluate on is provided in Table 1. We assess our hypothesis on a diverse set of tasks including GSM8K (Cobbe et al. 2021) for math, TriviaQA (Joshi et al. 2017) for world knowledge, TruthfulQA (Lin, Hilton, and Evans 2022) for truthfulness in question answering, and MT-Bench (Zheng et al. 2023a) for instruction following. These represent a diverse set of benchmarks used to evaluate LLMs across various domains. For TriviaQA, we use the rc.nocontext setup, which means the model relies solely on its parametric knowledge to answer the question correctly without accompanying context or documents. For TruthfulQA, we use the generation setup, where the model generates responses to a set of questions. The metrics scale for MT-Bench is 0-10 (Zheng et al. 2023a).

Task metrics. The list of task-specific metrics $f ( . )$ is provided in Table 1. The evaluation for GSM8K, TriviaQA and TruthfulQA is conducted using lm-evaluation-harness3, which provides a standardized framework for assessing model performance across benchmarks. The evaluation for MT-Bench is done with llm judge4, which use GPT-4 score (Zheng et al. 2023a) to score the generated answer from models. We do not test GPT-4-turbo on MT-Bench to avoid self-evaluation bias (He et al. 2023). To evaluate TruthfulQA, we follow Lin, Hilton, and Evans (2022) and develop two “GPT-judges” by fine-tuning GPT-3 models5 with provided data . Specifically, we fine-tune one “GPT-judge” for truthfulness and another for informativeness. Finally, we report the percentage of answers that are both truthful and informative as the final metric for TruthfulQA.

Hypothesis Testing for SELF-[IN]CORRECT Across Tasks. We apply a one-sided McNemar’s Test (Mcnemar 1947) for GSM8K, TriviaQA, and TruthfulQA to calculate p-values and assess statistical significance, as this test is ideal for binary outcome comparisons. For MT-Bench, we use the Wilcoxon signed-rank test (Wilcoxon 1945) because it handles categorical data and does not assume a normal distribution. Further details on our test selection and hypothesis testing methodology are provided in Appendix G.

Handling failure modes during evaluation. During the evaluation of the discrimination phase, if the model’s output does not conform to the expected format (i.e., integers indicating the selected answer), we consider it a failure. While such an output would receive a score of 0 in the generative setting, we take a more lenient approach for the discrimination phase. In these cases, we assign the model the score of the lowest-performing generated answer (according to our metric $f$ ) among the other candidate answers: $\begin{array} { r } { \bar { \mathsf { S } _ { \mathrm { d i s c } } } ( x _ { i } ) = \operatorname* { m i n } _ { g ( x _ { i } ) \in G ( x _ { i } ) } \bar { f } ( g ( x _ { i } ) , y _ { i } ) . } \end{array}$ . We also try to make the discriminator output one of the answers directly in the case of a failure, hoping that bypassing this extra step of identifying the multiple-choice options would simplify the discrimination phase. However, we observe an increased percentage of invalid discrimination output with similar discrimination performance (see Appendix C for more details). In our experiments, we observe that the average rate of invalid responses remained low (often less than $5 \%$ ). Given that there is also a small proportion of invalid outputs in the generation phase that wouldn’t get any credit when selected, we believe that the occurrence of invalid discrimination outputs does not significantly impact our overall findings.

Models. We employ a range of models including Phi-3- mini- $. 4 \mathrm { k }$ -Instruct (Abdin et al. 2024), LLaMA-2 Base models (7B, 13B, and 70B), LLaMA-2 Chat models (7B, 13B and 70B), LLaMa-3 Base models (8B and 70B), LLaMa3 Instruct models (8B and 70B), Mixtral ${ \bf 8 } \times { \bf 7  B }$ -Instructv0.1, GPT-3.5-turbo and GPT-4-turbo.6 For the evaluation of each model, we adapt our prompts to be compatible with the keywords used in their [pre-]training. For example, when prompting LLaMA-2 Chat models we use ${ < } S \Upsilon S >$ , ${ < \mathrm { I N S T } > }$ keywords to indicate system and instruction prompts.

Model hyper-parameters. During the generation phase, we use the default hyperparameter specified in lm-eval-harness for all tasks, except for temperature, which we have adjusted to 0.7. We use an above 0 temperature to obtain distinct generations upon multiple rounds of sampling. At the same time, during the discrimination phase, we set the temperature to 0 to avoid any randomness.

# 4.2 Main Findings

On a dominant majority of experiments, SELF[IN]CORRECT is not rejected. Based on the results in Table 2, in 54 out of 56 experiments, the p-value exceeded the significance level (0.05), leading to the failure to reject the SELF-[IN]CORRECT hypothesis. In fact, DG-DIFF is generally small or negative across both pre-trained models and aligned models. To test the effect of prompt variations, we conduct an ablation experiment in Appendix E and find that these variations do not significantly affect DG-DIFF. Although in few cases (2 out of 56) the p-value is high enough to reject our hypothesis (p-value $> 0 . 0 5 \AA$ ), such as LLaMA2-70B and GPT-3.5-turbo on TriviaQA, DG-DIFF remains quite small in such cases. We would also like to point out that these cases start with high generative accuracy and the relative differential in discrimination is quite minimal. All these observations lend support for SELF-[IN]CORRECT.

Instruction fine-tuned models went through both instruction-tuning and RLHF alignment while the base models are only pre-trained with the autoregressive objective. It is reasonable to expect that instruction-tuned models would exhibit better performance in the discrimination phase as instruction-tuning is shown to make models better at solving a variety of tasks. Furthermore, classification tasks (that resemble our discrimination setup) are well-represented in most instruction-tuning datasets (Wang et al. 2022b; Bach et al. 2022; Longpre et al. 2023). However, our empirical findings do not support it.

Stronger models tend to be better at discrimination (larger DG-DIFF). Our research has observed an interesting trend: an increase in DG-DIFF seems to correlate with model capacity. This pattern is particularly pronounced among models in the same category (base models, fine-tuned models, and proprietary models developed by OpenAI). We also want to emphasize that some of the strongest models we tested—specifically LLaMa-3-70B, LLaMa-3-70B-Instruct, GPT-3.5-turbo, and GPT-4-turbo—show a positive DG-DIFF across nearly all evaluated tasks, though the gap remains small enough for SELF-[IN]CORRECT to still hold. We hypothesize that this is because weaker models have limited discrimination capabilities. Similar observations on the weaker models’ discrimination capability have been reported in other studies (Saunders et al. 2022; Kadavath et al. 2022).

# 5 Further Analysis of SELF-[IN]CORRECT

In this section, we outline experiments designed to provide further analysis of SELF-[IN]CORRECT.

# 5.1 Better Discrimination via Prompt-Engineering

One might argue our current prompting setup doesn’t fully capitalize on the model’s capacity for discrimination. To make sure SELF-[IN]CORRECT isn’t an artifact of poor prompt engineering, we conduct additional experiments with LLaMA-2 Chat models on GSM8K, TriviaQA, and MTBench as their DG-DIFF on those tasks is mostly negative.

More in-context learning examples helps discrimination, though DG-DIFF remains small or negative. Increasing the number of in-context learning (ICL) demonstrations is shown to improve performance (Brown et al. 2020). Is it possible that increasing the number of ICL examples in the discrimination phase will improve it, so much that DG-DIFF becomes consistently positive? To evaluate the effect of increasing ICL examples, we conduct experiments where the number of ICL examples (#Shots) during the discrimination stage is doubled or tripled relative to the baseline in Table 1. Note that we didn’t triple the number of ICL examples for MT-Bench because it exceeds the context length for LLaMa2 Chat models (4096 tokens). The results, presented in Table 3, indicate that while increasing the number of ICL examples tends to increase DG-DIFF, it remains small or negative. Furthermore, the performance improvement from adding ICL examples does not exhibit a consistent monotonic trend.

Chain-of-thought rational shows minimal impact on DGDIFF. Recently, Stechly, Valmeekam, and Kambhampati (2024) pointed out that LLM evaluation also involves multistep reasoning. To help with the reasoning in the discrimination phase, we add chain-of-thought rationals in the few-shot examples while keeping the number of examples constant. For GSM8K, we do not report anything since our default evaluation already contains rationales for answer selection. For TriviaQA, the CoT rationales explain the logic behind choosing an option. For MT-Bench, we supplement explanations for preferring one answer over another. A comparison between our prompts (w/ and w/o CoT rationales) is available in Appendix A. As shown in Table 3, the inclusion of CoT rational only shows minimal impact.

# 5.2 The Role of Objectives: Does Autoregressive Pre-training Explain our Results?

The majority of modern LLMs are pre-trained with an autoregressive objective. Recent studies suggest that autoregressive objectives used during pre-training may have unexpected impacts on LLM behavior (McCoy et al. 2023). Since the pre-training process of autoregressive models is more similar to generation than discrimination, we hypothesize SELF-[IN]CORRECT is also partially caused by the use of autoregressive pre-training objective.

To test this hypothesis, we evaluate Flan-T5-XXL (11B) and Flan-UL2 (20B) on the same tasks listed in Table 2, as these are the only prominent open-source non-autoregressive models available to the best of our knowledge. Flan-T5- XXL is pre-trained using a span corruption objective, where the loss is only calculated on the corrupted span (Raffel et al. 2020). Flan-UL2 (Chung et al. 2022) is pre-trained using mixture-of-denoisers that combines multiple denoising objective functions. Our findings, detailed in Table 4, reveal their DG-DIFF across all tasks are positive except for Flan-T5-XXL on MT-Bench. In fact, both models exhibit significantly higher DG-DIFF and even more significantly higher relative DG-DIFF compared to the autoregressive models we tested in Table 2. Moreover, for both TriviaQA and TruthfulQA, the SELF-[IN]CORRECT hypothesis is rejected. This outcome lends empirical support to the hypothesis that SELF-[IN]CORRECT could be related to autoregressive pretraining.

Table 2: Performance change defined as DG-DIFF $: = S _ { \mathrm { d i s c } } - S _ { \mathrm { g e n } }$ , with $\boldsymbol { \mathrm { \tt ~ p } }$ -values indicating the likelihood that the observed difference is due to chance, for various mainstream LLMs on different tasks. The generation performance and discriminative performance are shown as subscript: $\mathit { \Omega } ^ { \prime } S _ { \mathrm { g e n } } \to \ S _ { \mathrm { d i s c } } )$ . p-values are calculated only when DG-DIFF is greater than or equal to 0. A red $\mathsf { p }$ -value marked with $\bigtriangleup$ signifies a value less than 0.05. For the majority of our results, DG-DIFF is small or negative, indicating similar or worse performance in the discrimination phase, and the p-value for 54/56 experiments is less than 0.05, meaning SELF-[IN]CORRECT is not rejected.   

<html><body><table><tr><td></td><td></td><td colspan="2">GSM8K</td><td colspan="2">TriviaQA</td><td colspan="2">MT-Bench</td><td colspan="2">TruthfulQA</td></tr><tr><td></td><td></td><td>DG-DIFF</td><td>p-value</td><td>DG-DIFF</td><td>p-value</td><td>DG-DIFF</td><td>p-value</td><td>DG-DIFF</td><td>p-value</td></tr><tr><td></td><td>LLaMA-2 7B</td><td>-0.6(9.2-→8.6)</td><td></td><td>-16.9(37.1→20.2)</td><td>1</td><td>-0.09(3.34-→3.25)</td><td></td><td>-4.7 (30.5→25.8)</td><td>1</td></tr><tr><td></td><td>LLaMA-2 13B</td><td>0.0(16.8→16.8)</td><td>0.50</td><td>1.4(45.2-→46.6)</td><td>0.07</td><td>-0.12(4.15→4.03)</td><td>__</td><td>2.1(26.8-→28.9)</td><td>0.10</td></tr><tr><td>M</td><td>LLaMA-2 70B</td><td>2.2(44.0→46.2)</td><td>0.12</td><td>3.2(53.2-→56.4)</td><td>0.00△</td><td>-0.12(4.87→4.75)</td><td></td><td>0.5(28.9->29.4)</td><td>0.40</td></tr><tr><td></td><td>LLaMA-3 8B</td><td>-3.6(38.6-→35.0)</td><td></td><td>-2.3(45.4→43.1)</td><td></td><td>0.06(3.47→>3.53)</td><td>0.42</td><td>0.2(27.2→27.4)</td><td>0.47</td></tr><tr><td></td><td>LLaMA-3 70B</td><td>1.1(77.7→78.8)</td><td>0.25</td><td>1.1(64.2-→65.3)</td><td>0.09</td><td>0.14(5.32-→5.46)</td><td>0.36</td><td>0.8(36.8-37.6)</td><td>0.37</td></tr><tr><td></td><td>Phi-3-mini-3.8B-Instruct</td><td>-0.2(77.9→77.7)</td><td></td><td>0.7(22.1→22.9)</td><td>0.11</td><td>-0.08(7.33→7.25)</td><td>-</td><td>-0.2(26.3→26.1)</td><td>1</td></tr><tr><td></td><td>LLaMA-2 7B-Cht</td><td>-2.8(20.4-→17.6)</td><td></td><td>-0.1(16.1-→16.0)</td><td>__</td><td>-0.13(5.45-→5.32)</td><td></td><td>1.4(48.8→50.2)</td><td>0.20</td></tr><tr><td></td><td></td><td>-5.5(28.3-→22.8)</td><td></td><td>0.0(25.5→25.5)</td><td></td><td>-0.51(5.67→5.16)</td><td></td><td>-0.1(44.9 →44.8)</td><td></td></tr><tr><td>M</td><td>LLaMA-270B-Chat</td><td>-5.9(42.5→36.6)</td><td></td><td>-1.6(47.8 →46.2)</td><td></td><td>-0.17(6.65->6.48)</td><td>_-_</td><td>0.9(48.6→ 49.5)</td><td>0.31</td></tr><tr><td>pəu</td><td>LLaMA-3 8B-nstrct</td><td>1.0(76.9→77.9)</td><td></td><td>0.6(48.7→ 49.3)</td><td></td><td>0.14(6.31→6.45)</td><td></td><td>-0.6(50.1→49.5)</td><td></td></tr><tr><td></td><td></td><td>0.6(92.2-→92.8)</td><td>0.29</td><td>1.1(64.2→65.3)</td><td>0.26</td><td>0.19(7.60-→7.79)</td><td>0.32</td><td>-1.1(56.2→55.1)</td><td>--</td></tr><tr><td>A</td><td>Mixtral-8x7B-Instruct</td><td>1.3(59.6→60.9)</td><td>0.37</td><td>-3.4(58.8→55.4)</td><td>1</td><td>-0.20(7.39-→7.19)</td><td>1</td><td>-0.4(61.1 →60.7)</td><td></td></tr><tr><td></td><td>GPT-3.5-turbo</td><td>1.1(75.3-→76.4)</td><td>0.37</td><td>2.1(67.1 →69.2)</td><td>0.01△</td><td>0.17(7.74-→7.91)</td><td>0.26</td><td>0.4(65.7-→66.1)</td><td>0.41</td></tr><tr><td></td><td>GPT-4-turbo</td><td>0.7(93.6→94.3)</td><td>0.39</td><td>0.2(79.9-→80.1)</td><td>0.40</td><td>1</td><td></td><td>1.7(77.4→79.1)</td><td>0.09</td></tr><tr><td>Task Avg.</td><td></td><td>-0.76</td><td></td><td>-0.99</td><td></td><td>-0.06</td><td></td><td>0.06</td><td></td></tr></table></body></html>

Table 3: DG-DIFF upon various modifications with LLaMA-2 Chat models. $^ { 6 6 } + 2 \times$ #ICL” means doubling the number of in-context demonstrations during the discrimination phase. $^ { 6 6 } + 3 \times$ #ICL” means tripling the number of in-context demonstrations. $\ " + \mathrm { C o T } \ " \$ stands for adding Chain-of-Thought rationales for the few-shot examples. Extra prompt-engineering techniques during discrimination do not consistently close the performance gap.   

<html><body><table><tr><td>Model</td><td colspan="3">LLaMA-27BChat</td><td colspan="3">LLaMA-213B Chat</td><td colspan="3">LLaMA-270BChat</td></tr><tr><td>Setup</td><td>+2×#ICL +3×#ICL +CoT +2×#ICL +3×#ICL +CoT</td><td></td><td></td><td></td><td></td><td></td><td>+2×#ICL +3×#ICL +CoT</td><td></td><td></td></tr><tr><td>GSM8K</td><td>-1.4</td><td>0.1</td><td>=</td><td>-5.9</td><td>-6.8</td><td>1</td><td>-5.8</td><td>-3.9</td><td></td></tr><tr><td>TriviaQA</td><td>-0.4</td><td>0.2</td><td>-0.3</td><td>0.1</td><td>0.1</td><td>-0.3</td><td>-1.7</td><td>-0.5</td><td>-1.8</td></tr><tr><td>MT-Bench</td><td>-0.09</td><td></td><td>-0.06</td><td>-0.53</td><td></td><td>-0.41</td><td>-0.19</td><td></td><td>-0.18</td></tr></table></body></html>

Table 4: Flan-T5-XXL and Flan-UL2 tested on the same setup as Table 2. DG-DIFF for all models across all tasks are positive except for Flan-T5-XXL on MT-Bench. Both models demonstrate significantly higher DG-DIFF compared to autoregressive models.   

<html><body><table><tr><td rowspan="3"></td><td colspan="2">DG-DIFF(Sgen → Sdise)</td></tr><tr><td>Flan-T5XXL</td><td>Flan-UL2</td></tr><tr><td>1.3(13.3-→14.4) 5.8(28.7→34.5)</td><td>0.5(21.6-→22.1) 4.2(52.7→56.9)</td></tr></table></body></html>

It is also important to note that the pre-training processes for these two model classes differ from autoregressively pretrained counterparts beyond the objective function. For example, Flan-T5 is pre-trained with most inputs provided, except for the corrupted spans. Furthermore, the datasets used for pre-training these LLMs can vary. Therefore, when uncovering the underlying reason why SELF-[IN]CORRECT does not occur on Flan-T5 and Flan-UL2, caution should be exercised before drawing definitive conclusions.

# 5.3 Do Prior Findings in Self-Refinement Contradict SELF-[IN]CORRECT?

The process of self-refine involves utilizing the same LLM to provide feedback for its own generation and using the feedback to refine the generation. Both Huang et al. (2023) and Madaan et al. (2023) suggested LLMs can self-refine on tasks other than reasoning. Does this contradict our assertions?

We replicated the experiment outlined in Madaan et al. (2023) and observed the following:

(1) For some evaluated tasks, certain aspects can be exploited for artificially amplifying task performance without actually improving with the feedback. For example, on the task of constrained generation, where the objective is to generate sentences containing specific keywords, self-refine with LLMs often leads to progressively longer sentences that simply extend previous ones. Thus, even if the refined sentences do not incorporate new keywords and continue to grow longer (often, ignoring the feedback from the prior round), the task performance still shows a monotonic improvement. A more detailed explanation of this behavior across additional tasks is provided in Table 5. To further illustrate our point, an example question and model output from the acronym generation task can be found in Figure 6 in Appendix B, and another example from the constraint generation task is presented in Figure 7 in the same appendix.

(2) For some evaluated tasks, the evaluation score assigned by the model for each iteration of self-refine is not monotonically increasing. We use the same model involved in self-refinement to evaluate each refined output. Ideally, the scores would improve with refinement, but for tasks like acronym generation and dialogue response, the model often assigns lower scores to refined outputs. This suggests that the observed improvement may be due to lower initial output quality, as noted in Huang et al. (2023).

(3) Quantifying the percentage of times models prefer self-refined subsequent generations to the previous generation, a marginal preference for self-refined generation was observed. We used the same models to discriminate between previous generations and self-refined subsequent generations for tasks referenced in Madaan et al. (2023), thus extending the evaluation of SELF-[IN]CORRECT to a broader range of real-world tasks beyond reasoning. Our results in Table 5 indicate that models prefer self-refined generations only around $54 \%$ of the time, meaning on those tasks LLMs are still not consistently better at discriminating among previously-generated alternatives than generating initial responses.

# 6 Further Discussion

SELF-[IN]CORRECT likely poses a barrier for continued progress in self-rewarding LLMs. Few recent works like Self-Rewarding Language Models (Yuan et al. 2024) generate preference pairs consisting of an instruction prompt $x$ , a winning response $y$ win, and a losing response $y$ lose to facilitate self-reward for instruction-following fine-tuning. While their setup also involves discriminating between previously generated outputs, our findings do not challenge its effectiveness. Their method selects winning and losing pairs from a larger set of generations, with the key factor being the correct ordering of these pairs. This setup simplifies the task, especially when straightforward heuristics like choosing the highest and lowest-scoring responses exist. In contrast, our approach requires the model to identify the single best generation, demanding a much finer level of granularity.

However, an interesting pattern from Yuan et al. (2024) is that there seem to be diminishing returns after a few iterations of self-rewarding. We hypothesize this may be linked to SELF-[IN]CORRECT because if the overall ability of LLMs to discriminate is inferior to their ability to generate, it becomes challenging to engage in a virtuous cycle that simultaneously enhances the model’s capability to follow instructions and generate self-rewards.

Controlled Modification of Experimental Setting with Simplified Distractors Here we consider the extent to which SELF-[IN]CORRECT may hold. For example, is SELF[IN]CORRECT potentially a fundamental limitation of LLMs pre-trained with an autoregressive objective, or can a change in data distribution alter the outcome? To address this question, we conduct experiments in an unconventional setting that simplifies the discrimination phase by substituting incorrect candidates with simpler ones for discrimination.

The experiments are conducted on TriviaQA and GSM8K. TriviaQA contains a wide range of answer categories, including names, locations, historical events, etc. For this dataset, we simplify the discrimination phase by substituting incorrect answer generations for question $A$ with correct answer generations from another question $B$ (left panel in Figure 8, Appendix F). As for GSM8K, we create simplified distractors by randomly multiplying or dividing incorrect generated answers by 100 (right panel in Figure 8, Appendix F).

Figure 9 in Appendix $\mathrm { \Delta F }$ clearly shows that simplifying the incorrect candidates improves DG-DIFF. For TriviaQA, $S _ { \mathrm { d i s c } }$ exceeds $S _ { \mathrm { g e n } }$ by a large margin. For GSM8K, all models tested also demonstrate improved DG-DIFF.

# 7 Limitations

Challenges in controlled study of LLMs in relation to SELF-[IN]CORRECT. One limitation of our research stems from the difficulty in measuring the impact of pretraining data and pre-training objectives. The vast amount of pre-training data makes it hard to evaluate its effect, leaving important aspects underexplored.

Potential influence of lengthier discrimination prompt on SELF-[IN]CORRECT. The prompt used in the discrimination phase is inherently lengthier than the generation prompt as it also includes the generated candidate answers. This increase in length may pose challenges to the model’s processing capabilities. Investigating the impact of prompt length is complex as simply adding superfluous content to lengthen the generation or discrimination prompt might unintentionally influence the outcomes. Therefore, we highlight this area for further exploration to better understand the implications of prompt length for SELF-[IN]CORRECT.

Limitations in experimental scope Another limitation of our study is the scope of our experiments. While we tested SELF-[IN]CORRECT across multiple tasks and domains using prominent LLMs, expanding to more models and tasks could further validate SELF-[IN]CORRECT.

We want to note while the current results support SELF[IN]CORRECT, we are not claiming that LLMs can never be better at discrimination than generation. Some studies (Welleck et al. 2023; Chen et al. 2024a) suggest that finetuning specifically on refinement data can improve discrimination capabilities, though this may come at the expense of the model’s generality. Whether it is possible to train a model that maintains generality while excelling at discrimination remains an open research question.

In addition, our discrimination setup is designed to be simple, allowing our method to be directly applied to a wide range of tasks while helping us better understand the inherent characteristics and challenges of LLMs. Exploring more complex techniques like problem decomposition and answer verification to enhance discrimination is beyond the scope of this paper.

Table 5: Explanation for some of the issues on tasks that Madaan et al. (2023) tested and the percentage of times the model prefers self-refined subsequent generations than previous generations. GSM8K isn’t included here because it didn’t get much improvement through self-refine in the original paper. Code optimization isn’t included either due to the complexity of running experiments.   

<html><body><table><tr><td>Task</td><td>Issues</td><td>Detailed Explanation</td><td>Pref.%</td></tr><tr><td>Sentiment Reversal</td><td>Lack of Reasoning</td><td>Refinements simply make the sentiment more and more positive</td><td>58.7%</td></tr><tr><td>Dialogue Response Generation</td><td>Reward Inconsistency</td><td>Reward assigned by LLMs doesn't increase monotonically</td><td>52.4%</td></tr><tr><td>Code Readability Improvement</td><td>Lack of Reasoning</td><td>Refinements simply makes variable names longer and more descriptive</td><td>53.3%</td></tr><tr><td>Acronym Generation</td><td>Reward Inconsistency</td><td>Reward assigned by LLMs doesn't increase monotonically</td><td>46.5%</td></tr><tr><td>Constrained Generation</td><td>Lack of Reasoning</td><td>Refinements simply extends previous generation</td><td>54.7%</td></tr></table></body></html>

Challenges in determining model’s preference Determining a model’s preference over several candidate generations can be challenging due to various biases (Alzahrani et al. 2024; Wang et al. 2024). Following the methodology used in other self-improvement studies (Yuan et al. 2024), we employ LLM-as-a-Judge prompting (Zheng et al. 2023b) to elicit answer choice from the model. It is conceivable that the LLMs we examine can be biased toward certain answer options or different answer formats (e.g., labels of A/B/C/D or [1]/[2]/[3]/[4]). Another method that we did not explore is ranking candidate answers based on the LLM’s assigned probability for each answer text. However, it is worth noting that this approach can also be biased by factors like the text fluency from the pre-training data.

Limited focus on other stages of self-improvement Self-improvement involves multiple stages, such as selfdiscrimination, critique generation, and generating additional answers after self-evaluation. Our focus is mainly on the first stage, with less emphasis on others. However, it is generally agreed (Huang et al. 2023; Tyen et al. 2023) that for LLMs to succeed at other stages reliably, they must first excel at the initial self-discrimination stage.

Even within the first stage, approaches can vary; some generate a single answer and decide if another is needed, whereas our work explores generating multiple answers followed by discrimination. However, we believe that overall success in this stage is ultimately dependent on the model’s discrimination capability.

# 8 Conclusion

We focused on the question of whether language models are strictly better at discriminating their prior generations vs. generating responses directly. We proposed a metric for comparing these capabilities and used it to evaluate several current LLMs. For those models and tasks, we do not observe that discrimination is reliably better than generation, in fact, we often observed it was worse. These results raise concerns about the potential for LLM self-improvement on any task.