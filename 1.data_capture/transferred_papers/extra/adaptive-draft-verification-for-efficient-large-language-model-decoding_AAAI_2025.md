# Adaptive Draft-Verification for Efficient Large Language Model Decoding

Xukun Liu1, Bowen Lei2, Ruqi Zhang3, Dongkuan (DK) Xu4

1Northwestern University, 2Texas A&M University 3Purdue University, 4North Carolina State University xukunliu2025@u.northwestern.edu, bowenlei $@$ stat.tamu.edu, ruqiz $@$ purdue.edu, dxu27@ncsu.edu

# Abstract

Large language model (LLM) decoding involves generating a sequence of tokens based on a given context, where each token is predicted one at a time using the model’s learned probabilities. The typical autoregressive decoding method requires a separate forward pass through the model for each token generated, which is computationally inefficient and poses challenges for deploying LLMs in latency-sensitive scenarios. The main limitations of current decoding methods stem from their inefficiencies and resource demands. Existing approaches either necessitate fine-tuning smaller models, which is resource-intensive, or relying on fixed retrieval schemes to construct drafts for the next tokens, which lack adaptability and fail to generalize across different models and contexts. To address these issues, we introduce a novel methodology called Adaptix, which accelerates LLM decoding without requiring fine-tuning. Our approach involves an adaptive draft-verification process that evolves over time to improve efficiency. We utilize a tri-gram matrix-based LLM representation to dynamically approximate the output distribution of the LLM, allowing the model to adjust to changing token probabilities during the decoding process. Additionally, we implement a draft construction mechanism that effectively balances exploration and exploitation, ensuring that the drafts generated are both diverse and close to the true output distribution of the LLM. The importance of this design lies in its ability to optimize the draft distribution adaptively, leading to faster and more accurate decoding. Through extensive experiments on various benchmark datasets and LLM architectures, we demonstrate that Adaptix accelerates the decoding process while maintaining high accuracy, making it suitable for deployment in a wide range of practical applications.

# Code — https://github.com/liuxukun2000/Adaptix

# Introduction

Large language model (LLM) decoding involves generating a sequence of tokens based on a given context, where each token is predicted one at a time using the model’s learned probabilities (Brown et al. 2020; Zhang et al. 2022; Touvron et al. 2023a,b). The core mechanism is autoregressive, where each new token is generated conditioned on the previously generated tokens and the given context. This process is crucial for applications like text generation (Li et al.

![](images/219dff5e3731a3fbc2330b8109a15cf5b89e6bf54f43b3121efbd56fa82f2e21.jpg)  
Figure 1: Comparison of different LLM decoding strategies. In Speculative Decoding, a small LLM generates predictions (red blocks) from inputs (blue blocks). Yellow blocks indicating intermediate results obtained from language model. Lookahead uses a large LLM for forward-looking predictions. REST employs a corpus trie for rapid token lookups. Adaptix integrates Monte Carlo Tree Search with tri-gram statistics and recent token history to simulate potential outputs, refining its recommendations over time. Adaptix’s adaptive approach offers advantages in terms of speed and accuracy by continuously evolving its draft constructions.

2024a; Peng et al. 2023; Chang et al. 2023), machine translation (Zhang, Haddow, and Birch 2023; Moslem et al. 2023; Hendy et al. 2023), and conversational AI (Shanahan 2024; Wu et al. 2023; Saka et al. 2023). However, each decoding step involves a forward pass through the model, making the process inherently sequential and computationally expensive. The inefficiencies arise due to the need to reload the model for each token prediction, leading to high computational costs and memory bandwidth usage. This serial nature of decoding is a significant bottleneck, especially for real-time applications (Liu et al. $2 0 2 3 \mathrm { a }$ ; Mandvikar 2023; Antoniol et al. 1994) where latency is critical. Thus, optimizing the decoding speed of LLMs is essential for practical deployment.

Recent research has explored various strategies to mitigate the inefficiencies of LLM decoding. Speculative Decoding (Leviathan, Kalman, and Matias 2023; Spector and Re 2023; Chen et al. 2023) introduces an approach where a smaller, more efficient model generates several token predictions in parallel, which are then verified by the larger target model. This method leverages the efficiency of smaller models to reduce the number of serial forward passes required, achieving substantial speedups without altering the output distribution. Lookahead Decoding (Fu et al. 2024a) uses the full context to predict multiple future tokens, creating a buffer that reduces the dependency on sequential processing. REST (He et al. 2024) employs a retrieval-based approach where relevant tokens are fetched from a pre-constructed datastore using the current context, forming drafts that are verified by the LLM. These methods can be summarized within the draft-verification pipeline, as shown in Figure 1. Speculative Decoding and Lookahead Decoding both generate draft tokens through predictive models, while REST constructs drafts from retrieved tokens based on the context. In each case, the drafts are then verified by the main LLM, ensuring that the final output adheres to the model’s learned probabilities. Despite their advancements, these approaches face notable limitations. They often require additional training or fine-tuning, which can be resource-intensive. Fixed retrieval schemes lack adaptability, making it challenging to adjust the draft distribution in real-time based on the evolving LLM output. Additionally, these methods may not generalize well across different models and contexts, limiting their effectiveness in dynamic environments.

In this work, our focus is on fine-tuning-free draftverification to address these limitations. The draftverification pipeline can be viewed as a rejection sampling procedure where the similarity between the proposal distribution (draft) and the target distribution (LLM output) is crucial for the acceptance rate and convergence speed. Higher similarity results in a higher acceptance rate and faster decoding speed. Very few fine-tuning-free approaches, e.g., REST (He et al. 2024), typically use fixed retrieval-based schemes to construct drafts. These schemes lack the adaptability to adjust the draft distribution based on the evolving LLM output distribution, resulting in a persistent gap between the draft and the actual LLM output. This gap reduces the draft acceptance rate and limits the potential for improving decoding speed. To address this issue, we raise the following question:

# Research Question: How to design an adaptive draft construction process that can evolve itself and accurately approximate LLM outputs during decoding?

To introduce adaptability and find drafts that are increasingly close to the LLM output distribution during decoding, we not only need to have an adaptive draft construction pipeline but also need to maintain a balance between exploration and exploitation. This balance ensures that speedups can be achieved by leveraging existing knowledge of draft construction while continuously exploring better draft construction capabilities. To achieve this, we propose a novel methodology called Adaptix. Adaptix incorporates a tri-gram-matrix-based adaptive LLM representative to control the conditional probability distribution of the next token, which can be updated during the decoding process to adjust the draft construction accordingly. To balance exploration and exploitation, we design a draft maker inspired by Monte Carlo Tree Search (MCTS) (Coulom 2007; Browne et al. 2012; James, Konidaris, and Rosman 2017; S´ wiechowski et al. 2023). This draft maker uses a token preference score to maintain the balance during the search process. The score consists of two parts: the first part is based on the approximate conditional probability distribution of the next token obtained from the LLM representative, reflecting the draft maker’s current knowledge of the LLM output; the second part encourages the draft maker to explore unexplored or less-explored draft spaces. Theoretically, we show that our method can be viewed as a constrained optimization problem to encourage the draft distribution to converge to the LLM output distribution. Using the token preference score, the draft maker can effectively search the draft space and generate candidate tokens. After the draft construction and verification are completed, the information is fed back to the LLM representative to update its approximation of the LLM output. This feedback loop enriches the draft maker’s knowledge in subsequent rounds of draft-verification, enabling adaptability and self-improvement in the draft construction process.

In summary, our contributions are concluded as follows:

• We design a tri-gram matrix-based representation that dynamically approximates the LLM output distribution, enhancing adaptability without the need for fine-tuning. It addresses the limitation of fixed retrieval schemes by continuously evolving with the model’s predictions. • We develop a draft maker that effectively balances exploration and exploitation to generate high-quality drafts. This mechanism improves decoding speed and accuracy by ensuring that the drafts are closely aligned with the LLM’s output distribution. Our experiments show a 2.5X improvement in decoding speed compared to baselines. • Through extensive experiments on various benchmark datasets and LLM architectures, we demonstrate that Adaptix accelerates the decoding process while maintaining high accuracy. Specifically, we achieve up to a 2.5X speedup in latency and an average acceptance rate improvement of $20 \%$ over existing methods. • Our method’s ability to adapt to evolving LLM outputs and continuously refine draft construction sets it apart from existing ones, addressing the need for more flexible and dynamic decoding solutions.

# Methodology

We propose a new fast fine-tuning-free draft-verification LLM decoding method by introducing adaptability into the decoding and learning from LLM, which is illustrated in Figure 2. Existing accelerated decoding algorithms either require additional fine-tuning or lack adaptability to LLM’s output distributions, resulting in additional cost or insufficient acceleration. To address these issues, we design an adaptive LLM representation based on a tri-gram matrix to adaptively approximate the output distribution of the LLM, develop a draft maker that balances exploration and exploitation for self-improvement towards high-quality drafts, and verify the drafts using tree attention.

![](images/ffcec4e151b845fb955b4b06b4259628581d0ed835c40c44c8c95c0b1a6b7a18.jpg)  
Figure 2: The data processing workflow of Adaptix. Initially, the input tokens undergo preprocessing to calculate their tri-grams, which serve to update the tri-gram matrix. Subsequently, the updated matrix, in conjunction with the last two tokens of the input, is used to retrieve potential token sequences. These sequences are ranked, and the top- $\mathbf { \nabla } \cdot \mathbf { k }$ sequences are selected, and then appended to the original input. Finally, these extended sequences are inputted into the Large Language Model for prediction.

# Preliminary

Speculative decoding is a method to accelerate language model inference by using a smaller auxiliary model to generate a draft sequence, reducing the computational load on the larger model (Leviathan, Kalman, and Matias 2023). Retrieval-based speculative decoding extends this by incorporating a retrieval system instead of the smaller model, leveraging pre-stored corpus segments for relevant text generation. Monte Carlo Tree Search (MCTS) (Coulom 2007; Browne et al. 2012; James, Konidaris, and Rosman 2017; S´ wiechowski et al. 2023) is an algorithm that optimizes decision-making by balancing exploration and exploitation of future states. It selects nodes for further exploration using a combination of node visit counts and estimated values, aiming to maximize overall outcomes. For a comprehensive discussion of these methods, please refer to Appendix $E$ .

# Adaptive LLM Representative

To approximate the output token distribution of the LLM without fine-tuning the small model, we distill linguistic knowledge from a small corpus and construct a tri-gram matrix as an initial representation of the LLM, which allows us to leverage the statistical regularities of language at a granular level. Specifically, we summarize and count each set of three tokens that appear in the corpus and compute the probability of the third token appearing conditional on the first two tokens. The formula is defined in Eq. (1):

$$
P ( w _ { i } | w _ { i - 2 } , w _ { i - 1 } ) = \frac { C ( w _ { i - 2 } , w _ { i - 1 } , w _ { i } ) } { C ( w _ { i - 2 } , w _ { i - 1 } ) } ,
$$

where $P ( w _ { i } | w _ { i - 2 } , w _ { i - 1 } )$ is the conditional probability of a word $w _ { i }$ given the two preceding words $w _ { i - 2 }$ and $w _ { i - 1 }$ , $C ( w _ { i - 2 } , w _ { i - 1 } , w _ { i } )$ is the count of the tri-gram occurrence in the corpus, and $\dot { C } ( w _ { i - 2 } , w _ { i - 1 } )$ is the count of the preceding bi-gram (Mori, Nishimura, and Itoh 1998).

In this way, we can obtain a good initial LLM representative at a much lower cost, which can generate an approximate distribution of the next token based on the previous tokens. This LLM representative will collaborate with our draft maker to generate drafts and get feedback to update the tri-gram matrix for adaptability and self-improvement. Please see Section 2.3 for more details.

# Draft Maker and Self-Improvement

With the help of the LLM representative, we further propose a draft maker that balances exploration and exploitation while searching for candidate drafts that are closer to the LLM output. On the one hand, the draft maker leverages the conditional probabilities from the LLM representative, which include current knowledge of the LLM output. On the other hand, the draft maker is encouraged to search more in the unexplored or less explored draft space to find better draft candidates. Then, with the feedback from the LLM output, the LLM representative can update its understanding of the LLM output, improve the draft maker’s search, and achieve self-improvement. Details are provided below.

Draft Search Score: Given the initial tokens, we exploit Monte Carlo Tree Search (MCTS) (Coulom 2007) to guide the search process of the drafts of the next tokens, where we prioritize candidate tokens according to the conditional probability from the tri-gram matrix-based LLM representative and the node visitation counts during the tree search. The score plays a key role in balancing exploration and utilization during the Monte Carlo tree search and is defined as Eq. (2).

$$
\mathrm { P U C T } ( s , a ) = Q ( s , a ) + E \cdot P ( s , a ) \cdot \frac { \sqrt { \sum _ { b } N ( s , b ) } } { 1 + N ( s , a ) } .
$$

The score design is motivated by PUCT Score (Rosin 2011; Silver et al. 2017). In particular, $Q ( s , a )$ assesses the quality of taking action $a$ in state $s$ , while $P ( s , a )$ represents the prior probability of selecting action $a$ in state $s$ . The term $N ( s , a )$ denotes the number of times the action $a$ has been taken from state $s$ , and $\textstyle \sum _ { b } N ( s , b )$ sums the counts for all actions from state $s$ . Eq  (3) plays a critical role in determining the balance between exploration and exploitation within the MCTS framework.

$$
E = C _ { 1 } + \log \left( \frac { \sum _ { b } N ( s , b ) + C _ { 2 } + 1 } { C _ { 2 } } \right) ,
$$

The constant $C _ { 1 }$ acts as a base level adjustment, while $C _ { 2 }$ modulates the logarithmic term to scale the exploration factor dynamically based on the total visitation counts. This formula ensures that our draft choices are contextually appropriate and optimizes the robustness and coherence of text generation.

Self-Improvement Strategy Transfer: Based on the final search score obtained during the search, we can construct draft candidates and verify them to get the final decoding output (please see Section 2.4 ) and feed it back for selfimprovement. This final output decoding represents LLM’s output distribution, which would be a good learning material for the LLM representative. Therefore, we feed this knowledge into the LLM representative in order to obtain updated conditional probability distributions, thus providing the draft maker with more accurate and exploitable knowledge, which is illustrated in Figure 2. Specifically, this technique operates by first extracting tri-grams from recent outputs of the LLM. Each tri-gram’s frequency is then used to update its probability as potential outputs. These adjusted probabilities are fed into the MCTS as part of the policy network, influencing the selection of the tree search. The updated tri-gram probabilities essentially serve as a dynamic policy guide, enhancing the model’s ability to generate contextually relevant and coherent sequences. By incorporating learned tri-gram probabilities into the tree search algorithm, we effectively create a feedback loop where the search strategy itself evolves over time. This strategy adjustment is executed by recalibrating the exploration-exploitation balance based on the empirical data derived from the model’s own outputs.

# Draft Construction and Verification

It is important to note that candidate drafts generated by the draft maker often have common starting segments that can cause redundant recalculations in the Transformer layers if not managed correctly. To address the issue, a pseudosequence that guarantees that each draft is a sub-sequence and that any common prefix appears only once is created (He et al. 2024). Motivated by this observation, we use a specific attention mask for each attention layer, called tree attention (Miao et al. 2023; Cai et al. 2024). This mask aligns the computations for each token with its dependencies according to the original draft sequence, preserving the draft’s contextual integrity and preventing unnecessary computations. The approval of drafts relies on a comparison with the conditional distribution from the LLM. At each position, new tokens are sampled and compared to the draft tokens. If a sampled token corresponds to the draft token, it is approved; otherwise, the draft is discarded from that point. This selective approval ensures that the output sequence aligns with what would be produced by a typical autoregressive process, thus upholding the authenticity of the generated text.

# Theoretical Insight: Why Adaptix uses MCTS

In this section, we provide a theoretical justification for the design of Adaptix. We show that the draft search in Adaptix using MCTS can be viewed as a form of policy optimization, while the inference mechanism of LLMs can be viewed as a similar form of penalty optimization.

MCTS in Adaptix: The token selection procedure in Adaptix decoding can be viewed as an action selection process. The MCTS algorithm optimizes its policy by iteratively building a search tree and updating visit counts for each node (state-action pair) based on the search paths. The visit count distribution ${ \hat { \pi } } ( a \mid x )$ is defined as:

$$
\hat { \pi } ( a \mid x ) \triangleq \frac { 1 + n ( x , a ) } { | A | + \sum _ { b } n ( x , b ) } ,
$$

where $n ( x , a )$ represents the visit count for action $a$ in state $x$ , and $| A |$ represents the total number of possible actions at

state $x$ . Then, the action selection in MCTS can be written as selecting the action $a ^ { * }$ :

$$
a ^ { * } ( x ) \triangleq \arg \operatorname* { m a x } _ { a } [ Q ( x , a ) + \lambda _ { N } \cdot \frac { \pi _ { \theta } ( a  { \lvert { \ x } } ) } { \hat { \pi } ( a  { \lvert { \ x } } ) } ]
$$

Following (Grill et al. 2020), we use $q \in { \mathcal { R } } ^ { | A | }$ to denote the vector of Q-function $Q ( x , a )$ . With proper choice of hyperparameters, the MCTS algorithm can be viewed as searching for the optimum solution to a policy optimization problem (Grill et al. 2020) as below:

$$
\bar { \boldsymbol { \pi } } \triangleq \arg \operatorname* { m a x } _ { \boldsymbol { y } \in S } \left[ \boldsymbol { q } ^ { \intercal } \boldsymbol { y } - \lambda _ { N } \mathrm { K L } [ \boldsymbol { \pi } _ { \boldsymbol { \theta } } , \boldsymbol { y } ] \right] ,
$$

where $S$ is the $| A |$ -dimensional simplex, $\lambda _ { N }$ is a regularization parameter that depends on hyperparameters and balances exploration and exploitation, and KL is the KLdivergence.

LLM Inference Mechanism: Large language models, particularly those based on the Transformer architecture, generate text by predicting the probability distribution of the next token given the previous tokens. During training, the model maximizes the log-likelihood of the observed data, which is equivalent to minimizing the cross-entropy loss:

$$
\mathcal { L } ( \boldsymbol { \theta } ) = - \sum _ { t = 1 } ^ { T } \log P ( w _ { t } \mid w _ { 1 : t - 1 } ; \boldsymbol { \theta } ) + \frac { \lambda } { 2 } \| \boldsymbol { \theta } \| _ { 2 } ^ { 2 } ,
$$

where $P$ denotes the conditional probability of LLM, $w$ denotes the tokens, and $\theta$ denotes the model parameters.

Comparative Analysis: As shown in Eq (6) and Eq. (7), both MCTS and LLMs can be viewed as regularized optimization problems for selecting the distribution of the next tokens. On the one hand, the Q-function in MCTS for Adaptix can be viewed as an approximation to the loglikelihood of LLMs:

$$
\begin{array} { l } { \displaystyle Q ( \boldsymbol { x } , \boldsymbol { a } ) = - \sum _ { t = 2 } ^ { T } \log \hat { P } ( w _ { t } \mid w _ { t - 1 } , w _ { t - 2 } ; \theta ) } \\ { \approx \log P ( w _ { 0 } , w _ { 1 } , \cdot \cdot \cdot , w _ { T } ; \theta ) } \\ { = - \sum _ { t = 2 } ^ { T } \log P ( w _ { t } \mid w _ { 1 : t - 1 } ; \theta ) , } \end{array}
$$

where $\hat { P }$ and $P$ are the conditional probability distribution from tri-gram-matrix-based LLM representative and LLMs, respectively. On the other hand, both MCTS and LLMs employ regularization to improve the optimization procedure. As a result, we verify the similarities between MCTS and LLM Inference in terms of optimization and regularization.

# Experiments

# Experimental Setup

Models and Datasets. We conduct a series of experiments with five distinct models on three datasets to evaluate the efficacy of Adaptix. In particular, We use three Vicuna models (Chiang et al. 2023) (7B, 13B, 33B) and two LLaMA2-chat models (Touvron et al. 2023b) (7B, 13B) to evaluate the acceleration capabilities across different model sizes and types. Our assessment incorporates the HumanEval (Chen et al. 2021), MT-Bench (Zheng et al. 2023),

<html><body><table><tr><td></td><td></td><td colspan="5">Latency</td><td colspan="4">Average AcceptLength</td></tr><tr><td>Benchmark</td><td>Model</td><td>REST</td><td>REST Single</td><td>Lookahead</td><td>Autoregressive</td><td>Adaptix</td><td>REST</td><td>REST Single</td><td>Lookahead</td><td>Adaptix</td></tr><tr><td rowspan="5">MT-Bench</td><td>Vicuna-7B</td><td>16.31</td><td>17.36</td><td>18.93</td><td>24.77</td><td>12.95</td><td>1.97</td><td>1.98</td><td>1.89</td><td>2.42</td></tr><tr><td>Vicuna-13B</td><td>25.43</td><td>25.99</td><td>32.73</td><td>44.07</td><td>22.94</td><td>1.98</td><td>1.99</td><td>1.85</td><td>2.39</td></tr><tr><td>Vicuna-33B</td><td>28.63</td><td>28.62</td><td>40.53</td><td>52.97</td><td>24.96</td><td>1.95</td><td>1.96</td><td>1.83</td><td>2.29</td></tr><tr><td>Llama2-7B</td><td>16.08</td><td>17.67</td><td>18.84</td><td>25.58</td><td>13.85</td><td>1.96</td><td>1.95</td><td>1.96</td><td>2.30</td></tr><tr><td>Llama2-13B</td><td>27.13</td><td>29.80</td><td>31.24</td><td>44.76</td><td>25.13</td><td>1.95</td><td>1.95</td><td>1.96</td><td>2.32</td></tr><tr><td rowspan="5">Alpaca</td><td>Vicuna-7B</td><td>14.24</td><td>14.58</td><td>18.73</td><td>24.49</td><td>12.81</td><td>2.22</td><td>2.22</td><td>1.89</td><td>2.33</td></tr><tr><td>Vicuna-13B</td><td>22.94</td><td>23.01</td><td>32.60</td><td>43.60</td><td>24.06</td><td>2.21</td><td>2.21</td><td>1.86</td><td>2.26</td></tr><tr><td>Vicuna-33B</td><td>26.03</td><td>25.89</td><td>40.58</td><td>52.52</td><td>24.62</td><td>2.11</td><td>2.12</td><td>1.82</td><td>2.21</td></tr><tr><td>Llama2-7B</td><td>14.13</td><td>14.87</td><td>19.28</td><td>25.38</td><td>12.90</td><td>2.21</td><td>2.20</td><td>1.97</td><td>2.37</td></tr><tr><td>Llama2-13B</td><td>23.66</td><td>24.07</td><td>31.18</td><td>44.04</td><td>23.57</td><td>2.15</td><td>2.13</td><td>1.96</td><td>2.32</td></tr><tr><td rowspan="5">Human Eval</td><td>Vicuna-7B</td><td>14.90</td><td>15.56</td><td>18.99</td><td>25.49</td><td>11.24</td><td>2.21</td><td>2.23</td><td>2.10</td><td>2.67</td></tr><tr><td>Vicuna-13B</td><td>20.17</td><td>20.61</td><td>27.43</td><td>45.13</td><td>19.96</td><td>2.50</td><td>2.50</td><td>2.23</td><td>2.81</td></tr><tr><td>Vicuna-33B</td><td>24.91</td><td>25.06</td><td>31.34</td><td>52.32</td><td>21.19</td><td>2.29</td><td>2.30</td><td>2.02</td><td>2.62</td></tr><tr><td>Llama2-7B</td><td>14.37</td><td>15.57</td><td>15.28</td><td>25.91</td><td>11.68</td><td>2.19</td><td>2.19</td><td>2.27</td><td>2.63</td></tr><tr><td>Llama2-13B</td><td>25.46</td><td>25.85</td><td>26.72</td><td>45.25</td><td>21.82</td><td>2.01</td><td>2.01</td><td>2.17</td><td>2.60</td></tr></table></body></html>

Table 1: Latency and Average Accept Length Comparison between Adaptix and Baselines. In most test cases, Adaptix ha the lowest latency, longer accept length, and higher efficiency.

![](images/a381b6d1f7724ca265911c09949db15e8320bf77a0a177d4cf462573e3a60e52.jpg)  
Figure 3: Comparison of Adaptix’s throughput for different models on MT-Bench, Alpaca, and Human-Eval. The performance of Adaptix shows stable and significant improvements across different models and benchmarks.

and Alpaca (Taori et al. 2023) datasets to ascertain general natural language understanding and generation competencies. These datasets are meticulously chosen to guarantee a comprehensive analysis of the acceleration techniques across various tasks.

Corpus. We construct two corpora. The first one is built using a portion of the Python pre-training code from The Stack (Kocetkov et al. 2022), comprising about 2.7M Python code samples with a resulting size of 1007MB. The second is constructed using data derived from UltraChat (Ding et al. 2023), consisting of around 774K ChatGPT conversations, producing a corpus with a size of 574MB. The experiments on the MT-Bench and Alpaca are conducted using the UltraChat corpus, while the Human-Eval benchmark utilize the corpus from The Stack.

Metrics. To assess the acceleration performance on large language models, we use two main metrics: speedup ratio and average acceptance length. Speedup ratio, calculated as the ratio of the time required by the baseline models to complete inference tasks without acceleration to the time required by our Adaptix, measures the efficiency gains introduced by the algorithm. The second metric, average acceptance length, measures the average number of tokens accepted per forward pass by the target large language models, excluding any overhead of retrieving and constructing draft tokens, indicating the maximum possible acceleration.

Baselines. We compare various foundational approaches to improve the decoding speed of large language models. We examine Lookahead Decoding (Fu et al. 2024a), a precise and parallel decoding algorithm that cuts down latency without relying on draft models. We compare REST (He et al. 2024) (Retrieval-Based Speculative Decoding), which adopts a retrieval-based strategy to create draft tokens, in contrast to conventional speculative decoding methods that rely on a draft model.For fairness in comparison, we include REST Single, a single-threaded version of REST, to evaluate performance under constrained processing conditions. We also include the traditional Autoregressive method, which represents the standard decoding approach, serving as a baseline to highlight the improvements offered by the other methods. All experiments are conducted on NVIDIA A6000 GPUs, except for the 33B model, which utilizes an NVIDIA H100. The experiments default to Greedy sampling.

# Main Results1

In the experiments, we compare the efficacy of different baselines applied to various models, utilizing three datasets: MT-Bench, Human-Eval, and Alpaca. We focus on metrics of Accept Length, Latency, and Speedup Ratio. Table 1 summarizes the latency and average accept length on the three datasets. Adaptix consistently demonstrates lower latency, particularly for the vicuna-7B and llama2-13B models. For instance, on MT-Bench, Adaptix achieves a latency of $1 2 . 9 5 ~ \mathrm { m s }$ for vicuna-7B, which is lower than REST (16.31 ms), REST Single Thread $( 1 7 . 3 6 \mathrm { m s } )$ , and Lookahead $( 1 8 . 9 3 ~ \mathrm { m s } )$ ). Notably, the memory required for Adaptix (574MB) is only $5 . 6 \%$ of that required for REST (12GB). According to Table 2, even when Adaptix uses a smaller corpus (253MB, which is just $2 . 5 \%$ of REST’s requirements), it still achieves lower latency than REST. This trend is also observed on Alpaca, where Adaptix achieves a latency of 12.81 ms for vicuna-7B, compared to $1 4 . 2 4 ~ \mathrm { m s }$ for REST, 14.58 ms for REST Single Thread, and $1 8 . 7 3 ~ \mathrm { m s }$ for Lookahead.

The accept length results in Table 1 indicate the quality of the generated outputs, with longer accept lengths suggesting more coherent and contextually relevant text. Our method, Adaptix, outperforms other methods across different models on both MT-Bench and Alpaca datasets. For example, on MT-Bench, Adaptix achieves the highest accept length for vicuna-33B and llama2-13B models, showcasing its superior language generation capabilities.

Speedup ratio are used to evaluate the efficiency. Adaptix consistently shows a significant improvement in speed up across all datasets in Figure 3. This efficiency is noticeable on the MT-Bench, Alpaca and Human-Eval datasets, where Adaptix not only reduces latency but also enhances the overall processing speed. For instance, Adaptix achieves a speedup of $1 . 9 2 \mathrm { x }$ on MT-Bench with the vicuna-13B model, outperforming REST, REST Single Thread, and Lookahead. On the HumanEval dataset, the vicuna-33B model, for example, demonstrates a speedup of nearly $2 . 5 \mathrm { x }$ when using Adaptix.

# Stability of Adaptix

In this section, we analyze the stability of In this section, we evaluate the stability of Adaptix across various task categories, including writing, roleplay, reasoning, math, coding, extraction, STEM, and humanities. Adaptix maintains consistent performance across all categories, achieving the highest speedup in coding $( \times 2 . 6 9 )$ and the lowest in extraction $( \times 2 . 1 7 )$ . The average accept length remains stable, confirming that Adaptix can effectively handle diverse tasks without significant performance variations.

To further evaluate the robustness of Adaptix, we analyze the effects of top-p and temperature on performance. Figures 4b and 4c show that variations in these parameters have minimal impact on the average accept length. Specifically, Figure 4b indicates that Adaptix’s performance remains stable across different top- $\cdot \mathbf { p }$ values, while Figure 4c demonstrates similar consistency for temperature changes. These results confirm the robustness of Adaptix to parameter variations.

# Ablation Study

To gain insight into our method, we conduct a series of ablation studies. Full studies are summarized in Appendix $\textbf {  { D } }$ .

Effect of the adaptive strategy. Figure 4a illustrates the performance of our adaptive strategy on Vicuna-7B, with the analysis of average accept lengths over varying token counts. We find that the adaptive strategy maintains a higher average accept length over the entire range compared to the non-adaptive strategy. The adaptive strategy’s success is attributed to its dynamic adjustment of the model’s probability distributions based on the tri-gram frequencies from prior outputs. This allows the model to better manage longer contexts and maintain relevance, enhancing stability and coherence in longer interactions.

Effect of the corpus size. Table 2 shows the impact of the increase in corpus size from 121k to $7 7 4 \mathrm { k }$ on various performance metrics. With the expansion of the corpus, there is a gradual improvement in the Accept Length from 2.30 to 2.42. This increase suggests that larger datasets provide a broader array of language patterns, which enhances the model’s ability to generate more coherent and contextually relevant outputs. Despite the increase in data size from 253 MB to $5 7 4 ~ \mathrm { M B }$ , the system maintains efficient data processing capability. The small differences in latency affirm Adaptix’s consistent performance, even with smaller corpus sizes, which further extends its potential for use on resource-constrained devices. The modest rise in retrieval time underscores the efficiency of the retrieval algorithms, which can manage larger datasets without significantly compromising response speed. In summary, the results show that larger corpus sizes can improve the quality of the model’s output while maintaining good system performance.

Table 2: Effect of Corpus Size.   

<html><body><table><tr><td>Corpus Size</td><td>Corpus Size</td><td>Latency</td><td>Accept Length</td><td>Speed up</td></tr><tr><td>121k</td><td>253MB</td><td>13.09 ms</td><td>2.30</td><td>1.89</td></tr><tr><td>774k</td><td>574 MB</td><td>12.95 ms</td><td>2.42</td><td>1.93</td></tr></table></body></html>

Effect of MCTS. Figure 5 presents the results for Vicuna7B model on the MT-Bench dataset, showing the impact of different MCTS search counts on performance. Increasing the number of searches improves performance, while the optimal number varies by model size. The average accept length and latency are plotted against the number of searches, illustrating the trade-off between performance and computational cost. We further compare greedy search with MCTS (full traversal is not feasible due to the huge search space) while keeping the number of 150 search iterations constant. The results show that the average accept length of the greedy search is only 1.493, significantly lower than that obtained by MCTS, demonstrating the superiority of MCTS in efficiently managing the vast decision spaces.

Effect of N-gram Model Choice. Our studies extensively evaluate the impact of different n-gram configurations on decoding performance. In tests conducted on the MT-Bench dataset using the Vicuna-7B model, bi-grams and 4-grams result in accept lengths of 1.80 and 1.82, respectively. These results are significantly lower compared to the 2.30 accept length achieved with tri-grams. Bi-gram models demonstrate limited capability in effectively utilizing contextual information, often leading to outputs that appear more random

Average Accept Length 4.5 H ·With Adaptive 2.302 Average Accept Lengths 2.302 Average Accept Lengths   
4.0 i   
3.5 Without Adaptive 2.268   
3.0   
2.5 2.26   
2.0   
1.5   
1.0   
0 4000 8000 12000 2.22 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Tokens Top-P Temperature   
(a) Effect of the adaptive strategy (b) Top-p (c) Temperature

![](images/b12b057b1204e7086616f9d9d7d6e4657ada34777cf62a4408401657e9e94135.jpg)  
Figure 4: (4a) Adaptive Strategy comparison on MTBench: Performance of Vicuna-7B model with and without the adaptive strategy on the MT-Bench dataset, showing the advantage of using the adaptive approach. (4b) (4c) Sensitivity analysis of Adaptix on top-p and temperature.   
Figure 5: Comparison of Search Counts on MT-Bench.

and less coherent. Conversely, 4-grams exhibit overly deterministic behavior, constraining the diversity of the generated text due to their restrictive nature in capturing extensive prior context. Tri-grams strike an optimal balance, providing enough contextual depth to enhance the coherence and relevance of outputs while still allowing for sufficient variability and diversity in text generation. This balance makes tri-grams particularly effective in large language model decoding, as they encapsulate the ideal compromise between randomness and contextual awareness.

# Related Work

A number of research efforts on decoding strategies for large language models have used draft models to improve decoding efficiency. Techniques such as Speculative Decoding (Leviathan, Kalman, and Matias 2023; Spector and Re 2023; Chen et al. 2023; Stern, Shazeer, and Uszkoreit 2018), Madusa (Cai et al. 2024), Eagle (Li et al. 2024b), various other approaches requiring draft models (Zhang et al. 2024; Liu et al. 2023b; Kim et al. 2024; Fu et al. 2024b) fall into this category, utilizing models to generate drafts. Specifically, Speculative Decoding uses an advanced sampling technique where the auxiliary model generates a set of potential token sequences, and the primary model selects the most sequences, resulting in a good balance between speed and accuracy. Although these methods primarily aim to enhance the accuracy of generated texts and significantly accelerate the response time during initial text generation, their adoption comes with drawbacks. The primary issue is the necessity for additional training specific to the draft models, which could be resource-intensive. Moreover, these techniques generally depend on GPU resources (Kwon et al. 2023; Sheng et al. 2023; Park et al. 2024) for inference, potentially limiting their application in environments where such hardware is unavailable or when operating under strict resource constraints.

A significant portion of recent advances has focused on improving efficiency without relying on draft models (Fu et al. 2024a; He et al. 2024). Two notable approaches in this realm are Lookahead decoding (Fu et al. 2024a) and Retrieval-Based Speculative Decoding (REST) (He et al. 2024). Lookahead decoding is an approach that enhances the efficiency of the decoding process through the prediction of subsequent tokens via Jacobi Iteration (Sleijpen and Van der Vorst 2000). It employs a heuristic to estimate the future cost of a sequence without the need to explicitly create a draft. REST introduces a retrieval-enhanced generation model that speculatively decodes sequences without the need for producing preliminary drafts. It instead searches and prioritizes possible continuations from an already established sequence database. However, these methods exhibit lower accuracy and greater resource use compared to our approach. They demand more memory and GPU processing power, posing challenges in resource-scarce settings.

# Conclusion

Adaptix improves the LLM decoding process by introducing adaptability and efficiency, significantly reducing latency and computational demands. This method achieves up to a $2 . 5 \mathrm { X }$ speedup in decoding and a $20 \%$ improvement in acceptance rates, outperforming traditional techniques. Unlike existing approaches, Adaptix dynamically adjusts the draft distribution using a tri-gram matrix and enhances draft quality through MCTS, eliminating the need for finetuning. The continuous feedback loop ensures ongoing improvements in draft generation. While Adaptix demonstrates robust performance across various benchmarks, future work will focus on exploring its application in more diverse real-world scenarios. Addressing potential limitations in extremely large-scale deployments will be a priority.