# Tuning-Free Accountable Intervention for LLM Deployment - A Metacognitive Approach

Zhen Tan1, Jie Peng2, Song Wang3, Lijie $\mathbf { H } \mathbf { u } ^ { 4 }$ , Tianlong Chen5 Huan Liu1,

1Arizona State University 2University of Science and Technology of China 3University of Virginia 4King Abdullah University of Science and Technology 5University of North Carolina at Chapel Hill ztan36@asu.edu, pengjieb@mail.ustc.edu.cn, sw3wv $@$ virginia.edu, lijie.hu@kaust.edu.sa, tianlong@cs.unc.edu,

# Abstract

Large Language Models (LLMs) have brought significant advances across various NLP tasks through few-shot or zero-shot prompting, bypassing the need for parameter tuning. However, the “black-box” nature behind their massive parameter sizes increases the “hallucination” concerns, especially in high-stakes applications (e.g., healthcare), where decision mistakes can lead to severe consequences. In contrast, human decision-making relies on complex cognitive processes, such as the ability to sense and adaptively correct mistakes through conceptual understanding. Drawing inspiration from human cognition, we propose an innovative metacognitive approach CLEAR, to equip LLMs with capabilities for self-aware error identification and correction. Our framework constructs concept-specific sparse subnetworks that indicate decision processes. This provides a novel interface for model intervention after deployment. The benefits include: $( i )$ at inference time, our metacognitive LLMs can self-consciously identify potential mispredictions with minimum human involvement, $( i i )$ the model can self-correct its errors efficiently without additional tuning, and (iii) the correction procedure is not only self-explanatory but also user-friendly, enhancing model interpretability and accessibility. With these metacognitive features, our approach pioneers a new path toward trustworthy LLMs. Appendix is given in the arxiv version.

Code — https://github.com/Zhen-Tan-dmml/CLEAR.git.

# 1 Introduction

Recent years have witnessed impressive achievements of Large Language Models (LLMs) (Raffel et al. 2020; Zhou et al. 2022; OpenAI 2023). However, LLMs could make mistakes due to issues like “hallucination” (McKenna et al. 2023). Such vulnerabilities pose critical challenges for the trustworthy deployment of LLMs in high-stakes settings where errors can lead to severe consequences. For example, in LLM-assisted medical diagnoses (Monajatipoor et al. 2022), a single wrong diagnosis can inflict significant physical and financial harm to the patient.

Despite the significance of this issue, the current literature lacks an effective approach to LLM intervention after deployment to fix errors. With few-shot or zero-shot prompting (Wei et al. 2022; OpenAI 2023), which recently has shown promising results, users can directly query LLMs and point out their mistakes using usually “hand-crafted” prompts. While straightforward, the post-prompting performance of this method remains uncertain. Moreover, it necessitates human expertise both for error identification and prompt design. Another potential method is to fine-tune part of the parameters in LLMs (e.g, the final layers) on incorrectly predicted examples (Hardt and Sun 2023). This method not only requires costly human involvement but also risks model overfitting on those examples and “catastrophic forgetting” of prior knowledge. Some initial work (Li et al. 2023) repeatedly performs activation-level intervention on all examples for better performance, thus resulting in increased time complexity during inference. Considering these issues, we identify three main challenges for fixing LLM errors after deployment: $\bullet$ The “black-box” nature of LLMs makes it difficult to locate the source of errors within model parameters, thereby impeding targeted intervention. $\pmb { \theta }$ Correcting errors usually requires domain experts, which limits scalability and automatic error correction. $\pmb { \otimes }$ The complexity and large parameter size of LLMs render targeted intervention a prohibitive task.

![](images/cffc7b702bd47c6eadbcef02f184293ea18062e750250e6099fe96ee2b78e130.jpg)  
Figure 1: Metacognitive LLMs are able to perceive concepts to self-correct potential errors.

In this paper, we argue that an ideal intervention should be metacognitive, where LLMs are capable of self-aware error identification and correction. This perspective is informed by several key insights from cognitive science literature: $( a )$ Cognitive Perception of Concepts - humans demonstrate the ability to quickly identify and correct judgment errors by recognizing essential features, or “concepts” (Malafouris 2013; Koh et al. 2020). This ability validates the efficiency of human cognitive processes. $( b )$ Neural Sparsity for Efficiency - building upon the notion of efficiency, the architecture of the human brain is insightful. The distribution of neural connections and activity patterns in brains is highly sparse (Gerum et al. 2020), which facilitates rapid cognitive responses. (c) Conscious Anomaly Detection - The human brain exhibits an intrinsic ability to consciously identify anomalies or challenging problems (Penfield 2015). Upon encountering such situations, additional neural resources are channeled to address them effectively. Hereby, we provide the definition of metacognition as follows:

# Definition 1. Metacognition. At inference time, a metacognitive LLM can autonomously detect and correct potential mispredictions in a single run.

Building on this intuition, we propose the CLEAR framework (Concept-Learning-Enabled metAcognitive inteRvention) for LLM deployment. CLEAR helps LLMs learn concept-specific sparse subnetworks. These subnetworks elucidate transparent decision-making pathways, thereby providing a unique interface for precise model intervention by automatically allocating more sparse computing modules to potentially more challenging instances. Distinctively, our approach simultaneously tackles the challenges highlighted above via the following contributions:

$\star$ Interpretability. Leveraging the transparency of decision pathways, our CLEAR allows for tracing decisions back to the input, thereby aiding user comprehension and trust in the model. $\star$ Efficiency. When incorrect predictios are identified, the LLM dynamically activates additional internal experts to refine the prediction in a single run without further tuning. $\star$ Effectiveness. We conduct extensive experiments on realworld datasets with LLM backbones in various sizes and architectures, and the results demonstrate that our intervention consistently improves inference-time predictions.

# 2 Related Work

Intervention on Deep Models for Error Mitigation. Historically, error mitigation in machine learning emphasized simpler models, such as Decision Trees and Random Forests, where corrections were largely heuristic and humandriven (Doshi-Velez and $\ K i m 2 0 1 7 _$ ). With the evolution of machine learning techniques, there was a pivot towards leveraging algorithms themselves for error detection, emphasizing the removal of non-relevant data and unveiling crucial faultapplication relationships (Abich et al. 2021). The ascendance of neural networks, and LLMs in particular, brought new intervention paradigms. Fine-tuning emerged as a primary strategy for addressing model shortcomings despite its challenges related to overfitting and catastrophic forgetting of prior knowledge (Wang et al. 2019; French 1999). Few-shot and Zero-shot prompting marked another avenue, guiding models without altering their internal makeup, leading to inherent limitations in error repeatability (Wei et al. 2022; Huang et al. 2023). Deeper interventions targeting model architectures have delivered promising accuracy, yet with computational trade-offs (Li et al. 2023). Notably, quantum error mitigation approaches, though out of our current scope, underline the breadth of exploration in this domain (Subramanian Ravi et al. 2021).

Concurrently, the push towards model interpretability has intensified (Carvalho, Pereira, and Cardoso 2019; Yuksekgonul, Wang, and Zou 2022). The ultimate goal is to design systems whose inner workings can be easily understood, thereby facilitating targeted interventions. Another series of recent work on concept bottleneck models (Koh et al. 2020; Zarlenga et al. 2022) utilize extra human-comprehensive concept labels to guide the learning of LLMs. Those concepts can be annotated by either human (Yuksekgonul, Wang, and Zou 2022; Wu et al. 2022) or large foundation models (Oikarinen et al. 2022; Tan et al. 2023b). However, those methods cannot provide transparency inside the LLM backbone, thus demanding specialized interventions that are usually handcrafted by domain experts (Farrell 2021; Monajatipoor et al. 2022; Tan et al. 2023a).

Metacognitive Approaches. Metacognition, commonly known as “thinking about thinking”, has long been recognized in cognitive science (Flavell 1979) through educational and clinical paradigms (Zimmerman 2013; Moritz and Woodward 2007). This foundational knowledge has been applied to AI, aspiring towards machines with self-reflective and adaptive capabilities (Cox 2005). Recent endeavors strive to infuse cognitive inspirations into models, demonstrating a deeper “understanding” of their decisions (Malafouris 2013). However, genuinely metacognitive LLMs remain a difficult goal (Huang et al. 2023), with challenges arising from their black-box nature and vast, intricate architectures.

# 3 Methodology

The proposed framework Concept-Learning-Enabled metAcognitive inteRvention, CLEAR, is comprised of two crucial components: (1) Concept Learning: the learning of concept-specific sparse subnetworks for LLMs. (2) Metacognitive Intervention: automatic error identification and rectification. The core idea of our method is that a refined understanding of LLMs can facilitate targeted metacognitive intervention. To this end, before detailing the proposed CLEAR framework, we first discuss how to learn concept-specific sparse subnetworks.

# 3.1 Concept Learning for LLMs

Basic Setup. Our primary focus is the enhancement of Large Language Models (LLMs) within the realm of text classification tasks during the inference phase. Given a dataset $\mathcal { D } = \{ ( \pmb { x } ^ { ( i ) } , y ^ { ( i ) } , \pmb { c } ^ { ( i ) } ) _ { i = 1 } ^ { N } \}$ , we utilize an LLM, denoted by $f _ { \theta }$ , to transform an input text $\pmb { x } \in \mathbb { R } ^ { D }$ into a latent space representation $\boldsymbol { z } \in \mathbb { R } ^ { E }$ . This latent representation is then classified via a linear classifier $g _ { \phi }$ into the respective target label $y$ (discrete for classification and continuous for regression). Here $\{ c ^ { ( i ) } \} _ { i = 1 } ^ { N }$ denotes the critical features, or “concepts” annotated by humans (Koh et al. 2020; Abraham et al. 2022) or very large language models (Tan et al. 2023b; Ludan et al. 2023), such as GPT-4 (OpenAI 2023). These concepts are represented by one-hot vectors. For instance, in a restaurant review sentiment dataset, the concept “Food” is denoted by $[ 0 , 0 , 1 ]$ , signifying a “Positive” attitude towards food. The other vector positions represent “Negative” and “Unknown”.

Input: Excellentlobsterand nicedecor,butrudewaiter Routers MoCE T=3 "Food'" "Ambiance' "Service"   
7 Router1 Router2 RouterK Experts A + X L LLM Backbone   
"Food" "Ambiance'" "Service"   
Concept Concept 2 Concept K Concept Bottleneck 支   
Sentiment: ★★★★☆(4)

activate only the most relevant experts (i.e., subnetworks in our work) based on the input’s concepts. As such, by conditioning on concept-based computation, MoCE crafts sparse modules, fine-tuning the encoding of text inputs as per their inherent concepts.

We structure blocks of MoCEs as the expert layer. This layer comprises a multi-head attention block combined with multiple parallel experts. Specifically, we adapt MoCE for Transformer architectures, integrating MoE layers within successive Transformer blocks. Crafting a MoCE expert involves segmenting the conventional MLP of transformers into more compact segments (Zhang et al. 2021) or duplicating the MLP (Fedus, Zoph, and Shazeer 2022). Note that the majority of extant MoE studies have predominantly focused on the MLP segment within transformers. This focus arises because MLPs account for approximately two-thirds of the entire model parameters, serving as key repositories of accrued knowledge within memory networks (Geva et al. 2020; Dai et al. 2022). The experts can be symbolized as $\{ e _ { m } \} _ { m = 1 } ^ { M }$ where $m$ signifies the expert index and $M$ is the total count of experts. For each concept $c _ { k }$ , an auxiliary routing mechanism, dubbed $r _ { k } ( \cdot )$ , is deployed. This mechanism identifies the top- $\mathbf { \nabla } \cdot \boldsymbol { T } _ { \mathbf { \nabla } }$ experts based on peak scores $r _ { k } ( x ) _ { m }$ , with $x$ representing the present intermediate input embedding. Generally, $T$ is much smaller than $N$ , which demonstrate the sparse activations among modules of the LLM backbone, making the inference of the model more efficient. The output, $x ^ { \prime }$ , emanating from the expert layer is:

Incorporating Concept Bottlenecks for LLMs. Our general pipeline is inspired by a previous work (Koh et al. 2020) on image classifications. Instead of altering LLM encoders $f _ { \theta }$ —which might compromise the integrity of the text representation—we incorporate a linear layer, characterized by a sigmoid activation function $p _ { \psi }$ . This layer maps the latent representation $z \in \mathbb { R } ^ { E }$ to a concept space $\boldsymbol { c } \in \mathbb { R } ^ { K }$ , and then a white-box linear model $g _ { \phi }$ maps the concepts to the target label $y$ . This creates a decision-making pathway depicted as $x \to z \to c \to y$ . By allowing for multi-class concepts, we aim to achieve nuanced interpretations. Akin to common practice (Koh et al. 2020; Tan et al. 2023b), the joint optimization harmonizes the concept encoder and label predictor via weighted sum, represented as $\mathcal { L } _ { \mathrm { j o i n t } }$ , as detailed in Appendix A.

Building Concept-Specific Sparse Subnetworks via Mixture of Concept Experts. We present the Mixture of Concept Experts (MoCE) framework, a novel approach to creating pathways anchored in specific concepts, thereby enhancing targeted interventions, based on the mixture-of-expert (MoE) paradigm (Shazeer et al. 2017) known for the dynamic utilization of unique experts per input. Our motivation here is to leverage the ability of MoE to adaptively select and

$$
\begin{array} { r } { \pmb { x } ^ { \prime } = \displaystyle \sum _ { k = 1 } ^ { K } \displaystyle \sum _ { m = 1 } ^ { T } \pmb { r } _ { k } ( \pmb { x } ) _ { m } \cdot \pmb { e } _ { m } ( \pmb { x } ) ; \qquad } \\ { \pmb { r } _ { k } ( \pmb { x } ) = \pmb { \mathrm { t o p } } - \mathrm { T } \big ( \sec \mathrm { t r a n a x } ( \zeta ( \pmb { x } ) ) , T ) , } \end{array}
$$

where $\zeta$ is a shallow MLP representing learnable routers (Fedus, Zoph, and Shazeer 2022). For the $k$ th concept, the expert $\boldsymbol { e } _ { t } ( \cdot )$ initially processes the given features, after which the router amplifies it using coefficient $\boldsymbol { r } _ { k } ( \boldsymbol { x } ) _ { t }$ . The combined embeddings across concepts yield the output $\mathbf { { x } ^ { \prime } }$ . The $\tt t o p \mathrm { - } \tt T$ operation retains the top $T$ values, nullifying the others. Typically, a balancing mechanism, such as load or importance balancing loss (Shazeer et al. 2017), is implemented to avert the risk of representation collapse, preventing the system from repetitively selecting the same experts across diverse inputs. Transitioning to matrix representation for all MoE layers in the LLM structure, we derive:

$$
\begin{array} { l } { { \displaystyle \hat { y } = \sum _ { k = 1 } ^ { K } \phi _ { k } \cdot \sigma \big ( \psi _ { k } \cdot f _ { \theta _ { k } } ( x ) \big ) } } \\ { { \displaystyle \quad = \sum _ { k = 1 } ^ { K } \phi _ { k } \cdot \sigma \big ( \psi _ { k } \cdot \sum _ { m = 1 } ^ { T } R _ { k } ( x ) _ { m } \cdot E _ { m } ( x ) \big ) , } } \end{array}
$$

where $\sigma ( \cdot )$ is the sigmoid activation, with $R ( \cdot )$ and $E ( \cdot )$ symbolizing matrix incarnations of all expert layer routers and experts. Equation (2) portrays a factorized decision trajectory for model prediction. This can be optimized through a single backward iteration of the composite loss as outlined in Equation (1). Equation (2) accomplishes a core objective: during inference, the LLM’s final prediction intrinsically relies on the learned routing policies, the chosen experts, and the perceived concepts. This accountability offers an interface for targeted error identification and interventions.

# 3.2 Tuning-free Metacognitive Intervention

The rationale of our metacognitive intervention is that, different data samples pose varying levels of difficulty for LLMs. Drawing inspiration from human cognitive processes—where the brain identifies and navigates potential challenges—our CLEAR framework proactively detects such issues. It strategically allocates additional sparse neural resources, specifically experts, to effectively address these challenges. This dynamic allocation tailors the response to the complexity of each sample, preventing the model from overfitting on simpler tasks and underfitting on more complex ones. Here, we detail how this is implemented through our defined sparse decision pathways, presenting three research questions, RQ1-3, to guide our discussion.

![](images/d86b0107ed27d2fe12c96e395533a0b7786f2dc2b2a3d083c24394d36708cb48.jpg)  
Figure 3: The illustration of the second component Metacognitive Intervention of our framework CLEAR, which involves logit entropy scrutiny, dynamic expert allocation, and pseudo intervention, and offers retrospective accountability.   
Figure 4: Logit entropy scrutiny. It can be observed that logits of predictions with errors tend to demonstrate lower confidence and larger entropy.   
Figure 5: Studies on using K-means for logits scrutiny. This figure illustrates the effectiveness of K-means in distinguishing between correct and erroneous logits for both routing and concept prediction. Logits are normalized via softmax, reducing the impact of noise and extreme values.

RQ1: How to achieve “metacognition” for intervention on LLMs?   
A1: By autonomously monitoring anomalous pattern at critical intermediate layers.

$D$ Logit Entropy Scrutiny. The foremost goal is to automatically identify potential errors or more complex cases. As in

Density PlotofLogits Density Plot of Entropy 1 corect_ligits correct_entropy error_entropy 20 0 0.50.60.70.80.91.0 0.20.0 0.2 0.40.60.81.01.2 1.4 Value of Logits Value of Entropy

CEBaB-correct CEBaB-correct CEBaB-error CEBaB-error IMDB-C- correct IMDB-C - correct IMDB-C-error IMDB-C-error 0.5 0.6 0.7 0.8 0.9 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Logits for Concept Prediction Logits for Routing (a) Concept Logits. (b) Routing Logits.

ferred from Equation (2), two critical decision-making phases notably impact the ultimate label prediction: (a) the deduced routing $\{ \pmb { R } _ { k } ( \pmb { x } ) \} _ { k = 1 } ^ { K }$ of the final MoCE layer, and (b) the determined concept activation $\hat { \pmb { a } } = \{ \hat { a } _ { k } \} _ { k = 1 } ^ { K } = \pmb { \psi } \cdot \pmb { f _ { \theta } } ( \pmb { x } )$ Intuitively, an elevated entropy of predictive logits denotes a more dispersed distribution over experts or concept options, signifying lower model confidence and pinpointing instances that deserve additional attention. For this purpose, the Shannon entropy is utilized for logits within the routing and concept activation:

$$
H ( \pmb { p } ) = - \sum _ { j = 1 } ^ { J } \mathtt { s o f t m a x } ( l _ { j } ) \log ( \mathtt { s o f t m a x } ( l _ { j } ) ) ,
$$

where $j$ iterates through the logits’ space $J = M$ for routing and $J = K$ for concept activation). For illustration, the distributions of logits and entropy for concept prediction are depicted using kernel density estimation in Figure 4. It is evident that predictions with errors tend to demonstrate lower confidence and augmented entropy, reinforcing our premise. For automation, as we iterate through the concepts, K-Means clustering is employed to divide confidence levels into two clusters $( \operatorname { K } \scriptstyle = 2 )$ . The subset with lower confidence is considered to stem from the more challenging instances. K-Means offers the advantage of determining thresholds dynamically, eliminating human involvement. If, for a single concept prediction relating to an instance, the confidence levels of both the routing and concept activation surpass the corresponding thresholds, we tag this concept prediction as potentially erroneous. We show further studies on the scrutiny for concept and routing are given in Figure 5 (a) and (b). It can be observed that the K-means algorithm effectively distinguishes between correct and incorrect logits.

RQ2: Once a potential error is identified during inference, how to intervene on LLMs “without extra parameter tuning”? A2: By dynamically allocating experts and enforcing preparatory rehearsal during training.

▷ Tuning-free Intervention. Once an erroneous prediction is identified, we allocate augmented computational resources to secure a more reliable prediction. This operation can be easily achieved by setting the maximum expert number from $T$ to a larger number $T ^ { \prime }$ for the router as below. Note that this operation is efficient since no parameter tuning is involved.

$$
\pmb { r } _ { k } ( \pmb { x } ) = \pm \mathsf { o p } \mathrm { - T } \big ( \mathsf { s o f t r a x } ( \zeta ( \pmb { x } ) ) , T ^ { \prime } \big )
$$

$D$ Pseudo Intervention during Concept Learning. Both existing research (Chen et al. 2023) and our experiments (Figure 7 (c) and (d)) indicate that directly adding more experts at the inference stage results in marginal improvements. Drawing inspiration from how humans reinforce understanding of challenging subjects through repeated practice before the final examination, we emulate a similar rehearsal mechanism during concept learning for better metacognitive intervention. As the LLM model is fine-tuned on the task dataset, we progressively raise the count of experts from $T$ to $T ^ { \prime }$ linearly after a predetermined number of training epochs, typically after the halfway mark. This strategy of pseudo intervention during the training phase enhances predictions when the expert count is increased during the inference-time intervention. Through this essential rehearsal setup, and by sequentially executing the steps outlined in Equation (3) and Equation (4), the LLM backbone is empowered to autonomously detect possible errors, addressing them more robustly with minimal human involvement.

RQ3: How can users understand and trust the intervention?   
A3: By backtracking from the task label, through the sparse pathway, to the input text.

▷ Retrospective Accountability. A standout feature of our metacognitive intervention is its inherent explicability. Using the decision-making pathways showcased in Equation (2), one can trace back from the task label prediction, passing through perceived concepts and activated subnetworks (experts), all the way to the initial text input, as shown in Figure 2. Illustrative examples are provided in Figure 6. The incorporation of our framework, CLEAR, represents a harmony of precision, flexibility, and accountability.

# 4 Experiments

Datasets. Our experiments are conducted on three datasets, including two widely-used real-world datasets, CEBaB (Abraham et al. 2022) and IMDB-C (Tan et al. 2023b) and a self-curated dataset $\mathtt { A S A P - C }$ . Each of them is a text classification or regression dataset comprised of human-annotated concepts and task labels. Their statistics are presented in Table 1. The procedures of curation of the $\mathtt { A S A P - C }$ dataset are similar to those two existing datasets. More details of datasets are included in Appendix C.

Baselines. For an in-depth analysis, we examine both $( a )$ the performance on the test sets and $( b )$ the performance on the development sets, before and after the intervention. This dual-faceted examination allows us to assess the intervention’s effectiveness and evaluate the model’s potential deterioration in generalizability and catastrophic forgetting of critical prior knowledge. Four LLM backbones are employed in our analysis: BERT (Devlin et al. 2018), OPT (Zhang et al. 2022), and T5 (Raffel et al. 2020). In this study, our evaluation primarily involves two categories of frameworks as baselines. We adjust our choice of LLM backbone per the specific methods employed:

$D$ Direct Intervention Methods: (i) Directly prompting the LLM with human identifying mispredictions. For this method, we use GPT-4 (OpenAI 2023) with zero and fewshot prompting, since it is widely regarded as one of the most capable LLMs currently. (ii) Directly fine-tuning the LLM backbones on mispredicted instances identified by humans. $( i i i )$ Employing the activation intervention method, ITI (Li et al. 2023).

$D$ Concept Bottleneck Models (CBMs) support concept-level interventions, but still require human experts to identify mispredictions. We consider the following recent CBM frameworks as baselines: $( i \nu )$ Vanilla CBMs (Koh et al. 2020) map the text into concepts using the LLM backbone and involve another linear classifier to perform the final classification. (v) Label-free CBMs (LF-CBMs) (Oikarinen et al. 2022) use GPT-4 to obtain the concept labels. $( \nu i )$ Concept embedding models (CEMs) (Zarlenga et al. 2022) that learn continuous embeddings for concepts.

# 4.1 Superior Performance of CLEAR

Table 2 presents comparative results, averaged over three independent runs, showcasing CLEAR’s superiority across concept and task label predictions, for both classification and regression tasks, and at every intervention stage. We adopt an “early stopping” strategy, as per Abraham et al. (2022), to mitigate overfitting, with further details provided in Appendix B and G.

a) Effectiveness. CLEAR consistently outperforms baseline models due to its robust MoCE layers, which create sparse, concept-specific subnetworks. This structure not only improves concept internalization but also facilitates effective interventions during inference, resulting in significantly improved prediction accuracy by addressing the challenges specific to each task.

b) Metacognition. CLEAR demonstrates critical metacognitive strengths: (a) Efficiency: Without the need for fine-tuning tuning, CLEAR avoids the common pitfalls of catastrophic forgetting (shaded in gray). (b) Autonomy: It operates independently of human intervention, which is crucial in scenarios where expertise is scarce. Unlike LF-CBMs that suffer from using noisy labels from GPT-4 (shaded in pink), CLEAR’s autonomy emphasizes the importance of precise intervention. (c) Accountability: Through transparent decision-making processes at concept, subnetwork, and input levels, CLEAR thus significantly boosts user trust.

Table 1: Statistics of experimented datasets and concepts.   

<html><body><table><tr><td rowspan="2">Dataset</td><td colspan="4">CEBaB (5-way classification)</td><td colspan="4">IMDB-C (2-way classification)</td><td colspan="4">ASAP-C (regression)</td></tr><tr><td>Train/ Dev/Test</td><td></td><td>1755/1673/1685</td><td></td><td>Train/ Dev / Test</td><td></td><td></td><td>100/50/50</td><td>Train /Dev / Test</td><td></td><td>1005/281/283</td><td></td></tr><tr><td rowspan="6">Concept</td><td>Label</td><td>Negative</td><td>Positive</td><td>Unknown</td><td>Label</td><td>Negative</td><td>Positive</td><td>Unknown</td><td>Label</td><td>Negative</td><td>Positive</td><td>Neutral</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Content</td><td></td><td></td><td></td></tr><tr><td>AFoadce</td><td>1693 (33.14)</td><td>2987 (9.48%)</td><td>333(2618)</td><td>SAryline</td><td>76(38%)</td><td>633%）</td><td>58 2%)</td><td>Reasoning</td><td>421(26.7)</td><td>4684 (43.6%)</td><td>4364 (29.5%)</td></tr><tr><td>Service</td><td>1249 (24.4%)</td><td>1397 (27.3%)</td><td>2467 (48.2%)</td><td>Emotional Arousal</td><td>74 (37%)</td><td>73 (36.5%)</td><td>53 (26.5%)</td><td>Language</td><td>382 (24.3%)</td><td>569 (36.3%)</td><td>618 (39.4%)</td></tr><tr><td>Noise</td><td>645 (12.6%)</td><td>442 (8.6%)</td><td>4026 (78.7%)</td><td>Cinematography</td><td>118 (59%)</td><td>43 (21.5%)</td><td>39 (19.4%)</td><td>Supportiveness</td><td>541 (34.5%)</td><td>685 (43.7%)</td><td>343 (21.9%)</td></tr></table></body></html>

Table 2: Comparative results on the CEBaB and IMDB-C datasets, using Macro F1 (↑) as the evaluation metric, expressed in percentages $( \% )$ . Scores shaded in gray highlight instances where the model experienced catastrophic forgetting, leading to a decline in performance on the development set. Scores shaded in pink indicate a decrease in performance following the intervention. Scores shaded in blue are from CLEAR. Results on the ASAP $- \mathrm { C }$ dataset in given in Appendix E.   

<html><body><table><tr><td rowspan="5">Methods Backbones</td><td rowspan="5"></td><td colspan="7">CEBaB Pre-intervention</td><td></td><td colspan="7">IMDB-C</td></tr><tr><td colspan="4"></td><td colspan="4">Post-intervention</td><td colspan="3">Pre-intervention</td><td colspan="4">Post-intervention</td></tr><tr><td colspan="2">Dev</td><td colspan="2">Test</td><td colspan="4">Dev</td><td colspan="2">Dev</td><td colspan="2">Test</td><td colspan="3">Dev Test</td></tr><tr><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td><td>Test Concept</td><td>Concept</td><td>Task</td><td></td><td>Concept</td><td>Task Concept</td><td>Task</td><td>Concept</td><td>Task</td></tr><tr><td colspan="10"></td><td colspan="7"></td></tr><tr><td>Prompting</td><td>GPT4</td><td></td><td>46.52</td><td></td><td>45.87</td><td>Direct Intervention Methods</td><td></td><td>48.32</td><td></td><td>69.35</td><td></td><td>68.74</td><td></td><td>69.35</td><td>：</td><td>69.84</td></tr><tr><td></td><td>BERT</td><td></td><td>80.03</td><td>-</td><td>79.75</td><td>46.52 76.43</td><td></td><td>81.23</td><td>-</td><td>74.52</td><td></td><td>72.11</td><td>：</td><td>71.69</td><td></td><td>74.26</td></tr><tr><td rowspan="3">Fine-tuning</td><td>OPT</td><td></td><td>82.65</td><td></td><td>81.37</td><td>80.84</td><td></td><td>82.16</td><td>：</td><td>80.62</td><td></td><td>79.98</td><td></td><td>75.42</td><td>-</td><td>81.05</td></tr><tr><td>T5</td><td></td><td>82.64</td><td></td><td>82.65</td><td>- 80.67</td><td></td><td>83.34</td><td></td><td>81.85</td><td></td><td></td><td>79.87</td><td>77.62</td><td></td><td>81.53</td></tr><tr><td>T5</td><td></td><td>82.64</td><td></td><td>82.65</td><td>82.64</td><td></td><td>83.29</td><td>，</td><td>81.85</td><td>，</td><td>79.87</td><td>，</td><td>81.85</td><td></td><td>81.25</td></tr><tr><td colspan="10">ITI</td><td colspan="7"></td></tr><tr><td rowspan="4">Vanilla-CBMs</td><td></td><td></td><td></td><td>85.29</td><td>78.11</td><td>85.86 78.32</td><td>88.52</td><td>Concept Bottleneck Models 79.52</td><td>64.52</td><td>72.51</td><td>62.76</td><td>70.41</td><td></td><td>72.51</td><td>65.31</td><td>71.96</td></tr><tr><td>BERT OPT</td><td>85.86 87.84</td><td>78.32 80.03</td><td>87.27</td><td>79.73</td><td>87.84 80.03</td><td>89.62</td><td>80.12</td><td>67.15</td><td>78.96</td><td>66.53</td><td>78.21</td><td>64.52 67.15</td><td>78.96</td><td>69.47</td><td>79.34</td></tr><tr><td>T5</td><td>88.20</td><td>81.05</td><td>87.96</td><td>80.63</td><td>88.20 81.05</td><td>90.21</td><td>81.05</td><td>68.85</td><td>79.58</td><td>67.94</td><td>78.26</td><td>68.85</td><td>79.58</td><td>70.26</td><td>79.95</td></tr><tr><td>BERT</td><td>82.37</td><td>75.24</td><td>83.45</td><td>75.69</td><td>82.37</td><td>75.24</td><td>75.82</td><td></td><td></td><td></td><td>60.35</td><td>68.21</td><td></td><td>61.32</td><td>68.13</td></tr><tr><td rowspan="4">LF-CBMs</td><td>OPT</td><td></td><td></td><td>84.62</td><td>76.84</td><td>84.54</td><td>83.52</td><td></td><td>62.51</td><td>70.49</td><td></td><td></td><td>62.51</td><td>70.49</td><td>63.58</td><td>74.65</td></tr><tr><td>T5</td><td>84.54 85.68</td><td>77.62 78.25</td><td>85.74</td><td>77.22 85.68</td><td>77.62 78.25</td><td>85.36 85.59</td><td>76.64 76.87</td><td>64.18 65.16</td><td>75.24 76.83</td><td>63.37 64.92</td><td>75.06 76.30</td><td>64.18 65.16</td><td>75.24 76.83</td><td>64.43</td><td>75.68</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>79.10</td><td>88.67</td><td>80.04</td><td>64.86</td><td>72.61</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BERT OPT</td><td>86.78 87.98</td><td>79.10 80.51</td><td>86.62 87.92</td><td>78.64 79.86</td><td>86.78 87.98</td><td>80.51 89.89</td><td>80.65</td><td>68.29</td><td>79.67</td><td></td><td>62.84</td><td></td><td>72.61 79.62</td><td>65.57 70.34</td><td>72.33 79.75</td></tr><tr><td colspan="10">T5 88.64 81.32 88.34 80.69</td><td colspan="7">71.05 64.86 66.97 78.68 67.84 68.65 79.64 68.98</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>88.64 81.32 Metacognition Intervention</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CLEAR CLEAR</td><td>OPT-MoCE T5-MoCE</td><td>88.24 89.65</td><td>80.96 81.62</td><td>88.24 89.63</td><td>80.39 81.30</td><td>89.04 80.85 89.65 81.62</td><td>90.46 91.25</td><td>81.24 82.14</td><td>68.83 69.46</td><td>79.75 80.25</td><td>68.47 69.65</td><td>79.52 80.63</td><td>68.39 69.46</td><td>79.86 80.25</td><td>71.02 71.67</td><td>80.12 80.95</td></tr></table></body></html>

c) Flexibility. CLEAR’s architecture-agnostic design facilitates its integration with a variety of LLMs, such as OPT and T5, demonstrating its broad adaptability. However, we have not conducted experiments with exceedingly large MoEs like LLaMA-MoE (Team 2023) and Mixtral (Jiang et al. 2024) due to their substantial size, which makes them impractical for training on the datasets. Efficient fine-tuning strategies for these larger models present a promising avenue for future research.

# 4.2 Extra Investigation and Ablation Study

Accountability. CLEAR excels by offering retrospective interpretability and deep insights into its intervention processes, enhancing transparency at multiple levels. Through backtracking, it provides explanations from the concept, subnetwork, to input levels, significantly increasing user trust and comprehension of the model’s decisions.

$D$ Case Study. A case study showcased in Figure 6 (with additional examples in Appendix H) illustrates CLEAR’s intervention process. It highlights how CLEAR corrects the predicted label for “Cinematography” from incorrect “-” to correct $" + "$ , refining the overall task label. This example, particularly the analysis of activations before and after intervention, uncovers the neural strategy behind CLEAR’s corrections, enhancing real-world applicability. For instance, we can compute the influence $I$ of each concept $c _ { k }$ to the final decision by the product of the concept activation $\hat { a } _ { k }$ and the corresponding weight $w _ { k }$ in the linear classifier: $I ( c _ { k } ) = \widehat { a } _ { k } \cdot w _ { k }$ , as visualized in Figure 6 (c), demonstrating CLEAR’s ability to not only rectify but also explain prediction errors.

Table 3: Efficiency comparison between interventions   

<html><body><table><tr><td>Method</td><td>Human labels</td><td>Parameter tuning</td><td>Targeted intervention</td></tr><tr><td>Prompting</td><td></td><td>×</td><td>×</td></tr><tr><td>Fine-tuning</td><td></td><td></td><td>×</td></tr><tr><td>ITI</td><td>×</td><td>×</td><td>×</td></tr><tr><td>CBM</td><td></td><td>×</td><td>×</td></tr><tr><td>CLEAR</td><td>×</td><td>×</td><td></td></tr></table></body></html>

Autonomy and Efficiency. CLEAR’s autonomy and tuning-free approach distinguish it from other models. As shown in Table 3, CLEAR uniquely improves without requiring human input or complex tuning, a necessity in other models. This independence not only simplifies CLEAR’s operation but also heightens its reliability and efficacy, ensuring robustness and trustworthiness.

Sverys s Ck/y 1.45 Sentiment: 0 - Conf: 0.64   
Router 1 Router 2 Router 3 Router 4 Ck/y 0.89 Allocate more experts 6 0.58 Ck/y' 1 C1 C2 C3 C4 广 (a) (b) (c)

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="6">CEBaB</td><td colspan="4"></td><td colspan="2"></td><td colspan="6">ASAP-C</td></tr><tr><td>Pre-intervention</td><td></td><td>Post-intervention</td><td></td><td>Improvement (↑)</td><td></td><td>Pre-intervention</td><td></td><td></td><td>Post-intervention</td><td>Improvement (↑)</td><td></td><td>Pre-intervention</td><td></td><td>Post-intervention</td><td></td><td></td><td>Improvement (↑)</td></tr><tr><td></td><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td><td>Concept</td><td>Task</td></tr><tr><td>CLEAR (null)</td><td>89.63</td><td>81.30</td><td>89.63</td><td>81.30</td><td>0</td><td>0</td><td>69.65</td><td>80.63</td><td>69.65</td><td>80.63</td><td>0</td><td>0</td><td>87.35</td><td>0.694</td><td>87.35</td><td>0.694</td><td>0</td><td>0</td></tr><tr><td>CLEAR (max)</td><td>89.63</td><td>81.30</td><td>86.62</td><td>78.81</td><td>-3.01</td><td> -2.49</td><td>69.65</td><td>80.63</td><td>65.74</td><td>78.55</td><td> -3.91</td><td> -2.08</td><td>87.35</td><td>0.694</td><td>85.34</td><td>0.726</td><td> -2.01</td><td>-0.032</td></tr><tr><td>CLEAR</td><td>89.63</td><td>81.30</td><td>91.25</td><td>81.80</td><td>1.62</td><td>0.5</td><td>69.65</td><td>80.63</td><td>71.67</td><td>80.95</td><td>2.02</td><td>0.32</td><td>87.35</td><td>0.694</td><td>89.65</td><td>0.624</td><td>2.30</td><td>0.070</td></tr><tr><td>CLEAR (oracle)</td><td>89.63</td><td>81.30</td><td>91.98</td><td>82.06</td><td>2.35</td><td>0.76</td><td>69.65</td><td>80.63</td><td>72.64</td><td>81.36</td><td>2.99</td><td>0.73</td><td>87.35</td><td>0.694</td><td>90.82</td><td>0.597</td><td>3.47</td><td>0.097</td></tr></table></body></html>

Table 4: Ablation study on intervention. “Null” means no intervention is taken. “Max” means directly activate all the experts for all samples. Scores are reported in $\%$ and those shaded in pink and blue respectively indicate negative and positive improvements

W/oRth Es 1 ? W/o Rth Es 品 臨 T   
90 w/o CE w/o CE   
二 V X 二 X   
80 4 X 8 X 16 4 X 8 16 2 4 8 16 81.0 2 4 8 16 2 4 8 16 # Experts # Experts Value of T' Value ofT' Number of Experts (a) CEBaB (b) IMDB-C (c) CEBaB (d) IMDB-C (e) FLOPs analysis

Ablation Study. We conducted ablation studies to assess CLEAR’s core components with each finding detailed below:

▷ Intervention Mechanism. Table 4 reveals that indiscriminate expert activation for all instances diminishes performance due to overfitting. Comparatively, CLEAR’s metacognitive intervention closely matches the precision of oracle interventions using human-annotated labels, validating its effective error correction and metacognitive capacity without human-annotated labels.

▷ Options for Logit Entropy Scrutiny. Analysis in Figure 7 (a) and (b) shows superior model performance when utilizing both entropy thresholds together rather than separately. Particularly, omitting concept prediction entropy significantly reduces performance, validating CLEAR’s design of concept-specific subnetworks that are crucial for its precision in intervention.

▷ Pseudo Intervention. As demonstrated in Figure 7 (c) and (d), incorporating pseudo intervention markedly improves CLEAR’s performance, affirming the strategy of increasing expert numbers during training as a rehearsal enhances preparedness for real-time interventions.

▷ Sensitivity Analysis on the Number of Experts. Figures 7 (a)

and (b) indicate performance boosts with additional experts, attributing to expanded model capacity and learning ability. Furthermore, Figures 7 (c) and (d) demonstrate enhanced accuracy in correcting mispredictions with more experts during the intervention phase.

# 5 Conclusion

This paper introduces a novel framework, CLEAR, with robust capabilities to autonomously identify and correct errors, thereby reducing the need for extensive human involvement and complicated adjustments. By employing a metacognitive strategy inspired by human cognitive processes, CLEAR enables the construction of transparent, concept-specific sparse subnetworks. This attribute enables clear, comprehensible decision pathways and facilitates post-deployment model intervention. Confronted with the “black-box” issue prevalent in LLMs, CLEAR demonstrates its effectiveness in reducing mispredictions and enhancing overall model interpretability and accessibility. These advances by CLEAR manifest an enhancement in both the performance and reliability of LLMs, ensuring their more trustworthy and accountable deployment in diverse real-world scenarios. We hope the application of CLEAR provides a positive shift for trustworthy LLMs.