# Multilingual LLMs Inherently Reward In-Language Time–Sensitive Semantic Alignment for Low-Resource Languages

Ashutosh Bajpai1,2, Tanmoy Chakraborty1

1 Indian Institute of Technology Delhi, India 2 Wipro Research, India eez228482@ee.iitd.ac.in, tanchak@ee.iitd.ac.in

# Abstract

The unwavering disparity in labeled resources between resource-rich languages and those considered low-resource remains a significant impediment for Large Language Models (LLMs). Recent strides in cross-lingual in-context learning (X-ICL), mainly through semantically aligned examples retrieved from multilingual pre-trained transformers, have shown promise in mitigating this issue. However, our investigation reveals that LLMs intrinsically reward in-language semantically aligned cross-lingual instances over direct crosslingual semantic alignments, with a pronounced disparity in handling time–sensitive queries in the X-ICL setup. Such queries demand sound temporal reasoning ability from LLMs, yet the advancements have predominantly focused on English. This study aims to bridge this gap by improving temporal reasoning capabilities in low-resource languages. To this end, we introduce mTEMPREASON, a temporal reasoning dataset aimed at the varied degrees of low-resource languages and propose Cross-Lingual Time-Sensitive Semantic Alignment (CLiTSSA), a novel method to improve temporal reasoning in these contexts. To facilitate this, we construct an extension of mTEMPREASON comprising pairs of parallel cross–language temporal queries along with their anticipated in-language semantic similarity scores. Our empirical evidence underscores the superior performance of CLiTSSA compared to established baselines across three languages – Romanian, German, and French, encompassing three temporal tasks and including a diverse set of four contemporaneous LLMs. This marks a significant step forward in addressing resource disparity in the context of temporal reasoning across languages.

# Introduction

In the evolving landscape of Large Language Models (LLMs), temporal reasoning requires models to comprehend and interpret the significant subtleties inherent in time–time, time–event and event–event correlations (Chen, Wang, and Wang 2021; Dhingra et al. 2022). Temporality is a crucial dimension of information that evolves through creation, maintenance, and obsolescence. Enhancing LLMs with this faculty augments their analytical capabilities, paving the way for addressing intricate challenges prevalent in domains sensitive to temporal dynamics, such as finance, healthcare, legal studies, and archaeology. Furthermore, addressing lowresource languages in LLMs is crucial for computational linguistics, given their paucity of data and digital infrastructure (Cahyawijaya et al. 2023; Asai et al. 2023; Adilazuarda et al. 2024). Enhancing LLMs for these languages improves not just genuine linguistic inclusivity but also their application and acceptance across diverse cultural landscapes. The discourse on enhancing temporal reasoning in LLMs has, until now, been predominantly focused on English. Our work seeks to alleviate this disparity by propelling temporal reasoning in low-resource languages.

Cross-Lingual In-Context Prompting. Recent advancements in in-context learning (ICL), prompted by the advent of LLMs, have shown promising results (Zhao et al. 2021; Lin et al. 2022b; Liu et al. 2022; Zhang et al. 2022). The stark contrast in annotated data availability among languages accentuates the usage of high-resource linguistic contexts for addressing tasks in low-resource languages. The ICL approach was adapted by Winata et al. (2021) for crosslingual (X-ICL) applications by randomly selecting examples from a resource-rich language to support queries in a language with limited resources.

Figure 1 shows three levels of temporal queries - L1 (Time–Time), L2 (Time–Event), and L3 (Event–Event) and the expected model responses: time information for L1 queries, an event for L2 given time, and for L3, an event in response to another event, without explicit temporal details in either input or output. An illustrative L1 query in English (“What is the time 6 years and 4 months after Nov, 1185”) with an associated answer (“Mar, 1192”) serves as additional context for a corresponding L1 query in Romanian (“Care este timpul cu 8 ani ,si 3 luni ˆınainte de august 1240”). Subsequent research suggested that cross-lingual examples that are semantically aligned can significantly enhance performance compared to the arbitrarily selected examples (Tanwar et al. 2023). Further development also indicates that semantic similarity alone doesn’t ensure the optimal performance, stressing the necessity for a learningbased retrieval model. (Lin, Martins, and Schu¨tze 2024).

Challenges in Cross-Lingual Semantic Alignment. Cross-lingual in-context approaches rely on the contextual semantic profoundness embedded within multilingual pre-trained encoder-only transformers, reflected through

L1: Time-Time Probe L2: Time-Event Probe L3: Event-Event Probe Luați în considerare următoarele exemple în limba Luați în considerare următoarele exemple în limba Luați în considerare următoarele exemple în limba engleză: engleză: engleză:   
1 (Translation: Consider the following English examples:) (Translation: Consider the following English examples:) (Translation: Consider the following English examples:) Question: What is the time 6 year and 4 month after Question: Which employer did Eduard Winkelmann Question: Which employer did Eliakim Hastings Nov, 1185 Answer both the month and year. work for in Jan, 1877? Moore work for before University of Chicago? Answer: The correct answer in month,year is Mar, Answer: The correct answer is Heidelberg Answer: The correct answer is Northwestern 1192 University University . . . . . . Acum, vă rugăm să răspundeți la întrebarea Acum, vă rugăm să răspundeți la întrebarea Acum, vă rugăm să răspundeți la întrebarea ulterioară în limba română. Răspunsul trebuie să fie ulterioară în limba română. Răspunsul trebuie să fie ulterioară în limba română. Răspunsul trebuie să fie în limba română: în limba română: în limba română: (Translation: Now, please respond to the subsequent query in (Translation: Now, please respond to the subsequent query in (Translation: Now, please respond to the subsequent query in   
(îÎbRTnerătfarosniersnpelbtauAateinruosegdn:uesRQCtauă1aues2rsg4epti0uoesAns:tnstsWu1ewlh2etaci4rto0mimrseopRtnhcuătelhsticpanmnuledun8ysneaăyla,eunaina sanșașeni3sdatl3eunnm Întrebare: Pentru ce angajator a lucrat Întrebare: Pentru ce angajator a lucrat Jaroslav Dominique Kalifa în ianuarie 1999? Pelikan înainte de Seminarul Concordia? Răspuns: Răspunsul corect este Răspuns: Răspunsul corect este (Translation: Question: For which employer did Dominique (Translation: Question: What employer did Jaroslav Pelikan Kalifa work in January 1999? work for before Concordia Seminary? Answer: The correct answer is) Answer: The correct answer is) Expected Answer: Mai 1232 Expected Answer: Universitatea Diderot din Paris Expected Answer: Universitatea Valparaiso

their embedding space, for retrieving semantically akin examples. Nonetheless, the disparity in linguistic distribution within the pre-training dataset, favoring resource-rich languages over those with fewer resources, significantly hinders efficient cross-lingual semantic alignment within their embedding space, especially concerning time–sensitive queries. The ensuing analysis is conducted to validate the aforementioned hypothesis in a temporal context.

The objective is to assess the efficacy of multilingual sentence-BERT (Reimers and Gurevych 2019) in retrieving semantically akin cross-lingual in-context examples for L1 task, considering Romanian, German, and French as lowand English as a high-resource language. The investigation juxtaposes two distinct approaches: firstly, cross-lingualsimilarity, wherein the top-3 English instances are directly retrieved for a low-resource query by ranking the similarity scores between a low-resource query and English instances. Secondly, in-language-similarity, where this is achieved by initially translating the English example dataset into lowresource languages. Next, the top-3 instances are sourced using similarity scores between a low-resource query and the translated English examples dataset. Subsequently, the translated examples are replaced with their corresponding English instances, thus retrieving the English examples with the highest in-language similarity. Performances are quantified using F1 scores and exact match (EM) scores. The results presented in Table 1 indicate that multilingual encoderonly transformers exhibit profound semantic similarity for temporal queries in an in-language similarity framework, outperforming their counterparts in a cross-lingual similarity context when identifying semantically akin examples for the cross-lingual temporal reasoning task across languages. Consequently, this underscores the need to evaluate and enhance the cross-lingual, time–sensitive semantic context of retrieval models. Nonetheless, the availability of data for such alignment in low-density languages is limited.

Our Proposed Method. We start with the development of first-of-its-kind a comprehensive benchmark dataset, mTEMPREASON, to evaluate temporal reasoning for limited resource languages – Romanian, German, and French across a diverse set of LLMs. Further, we endeavor to devise an efficacious novel cross-lingual retriever, CLiTSSA (CrossLingual Time-Sensitive Semantic Alignment), for handling time–sensitive queries from low-resource languages, addressing the aforementioned challenges in a cross-lingual context. To achieve this, drawing inspiration from Yamada and Ri (2024), we elect to apply the transfer of profound semantic space knowledge within a language to a crosslingual semantic space for queries influenced by temporality. Consequently, we adopt a supervised fine-tuning approach that necessitates an additional dataset to facilitate this transition. To this end, we curate an extension of the mTEMPREASON dataset comprising parallel sentences for Romanian-English, German-English, and French-English pairs, accompanied by their anticipated similarity scores in the semantic space of the low-resource language. By employing this curated dataset, the transition of the semantic context from a monolingual to a cross-lingual embedding sphere is attained.

Table 1: Analyzing the impact of in-language versus crosslanguage similarities in retrieving semantically similar akin examples in a three-shot cross-lingual setup for the L1 task across languages using LLaMA3-8B   

<html><body><table><tr><td>Task</td><td> Settings</td><td>F1.</td><td>EM</td></tr><tr><td rowspan="2">Romanian</td><td></td><td>33.68</td><td>10.15</td></tr><tr><td>r-s-Lingual-Siltity</td><td></td><td></td></tr><tr><td rowspan="2">German</td><td>Cross-Lingual-Similarity</td><td>56.63</td><td>35.45</td></tr><tr><td>In-Language-Similarity</td><td>64.77</td><td>46.35</td></tr><tr><td rowspan="2">French</td><td>Cross-Lingual-Similarity</td><td>46.62</td><td>22.05</td></tr><tr><td>In-Language-Similarity</td><td>60.50</td><td>37.17</td></tr></table></body></html>

Remarkably, for temporal queries, CLiTSSA outperforms the arbitrary cross-lingual in-context benchmark, demonstrating relative mean F1 score enhancements of $1 1 . 4 1 \%$ , $3 0 . 7 7 \%$ , and $6 2 . 9 2 \%$ for Romanian, German, and French, respectively. Additionally, it evidences a significant improvement in relative mean F1 score of $6 . 3 8 \%$ , $5 . 9 8 \%$ , and $2 0 . 9 3 \%$ compared to the contemporary cross-lingual incontext baselines for Romanian, German, and French.

Table 2: Dataset statistics for mTEMPREASON.   

<html><body><table><tr><td></td><td>Train</td><td>Dev</td><td>Test</td></tr><tr><td>Time Range</td><td>1014-2022</td><td>634-2023</td><td>998-2023</td></tr><tr><td>L1</td><td>400,000</td><td>4,000</td><td>4,000</td></tr><tr><td>L2</td><td>16,017</td><td>5,521</td><td>5,397</td></tr><tr><td>L3</td><td>13.014</td><td>4,437</td><td>4,426</td></tr></table></body></html>

Our contributions are summarized below1–

• We develop a dataset centered around temporal reasoning, mTEMPREASON, elusively designed for varying degrees of limited resource languages.   
• Our findings reveal that multilingual transformers exhibit superior in-language semantic similarity over crosslingual similarity context for temporal queries, especially explicit ones, in the X-ICL setup.   
• We introduce CLiTSSA to enhance temporal reasoning capabilities within LLMs for low-resource languages. Consequently, we develop an extension of mTEMPREASON comprised of paired cross-lingual time–sensitive queries with corresponding similarity scores. Our empirical analysis demonstrates that CLiTSSA significantly outperforms the contemporary baselines.

# Benchmark For Low-Resource Temporal Reasoning

In our study of temporal reasoning, the TEMPREASON (Tan, $\mathrm { N g }$ , and Bing 2023) stands out as a recent, comprehensive resource, providing multifaceted temporality across an extended time frame. Therefore, we select it to develop the first multilingual, low-resource dataset for temporal reasoning. To this end, we employ the T5 model (Raffel et al. 2023) to automatically translate the dataset from English language to Romanian, German and French languages.

# The mTEMPREASON Dataset

TEMPREASON encompasses tasks categorized into three levels of temporal complexity, namely time–time, time–event, and event–event relationships, corresponding to Levels L1, L2, and L3, respectively. The dataset’s statistical information, along with the partition into training (Train), development (Dev), and test set (Test), is detailed in Table 2. Employing a concise prompt prefix, “Translate the following sentences from English to the Target Language ,” we employ the T5 model to translate this dataset into a selection of languages with varying degrees of limited resources, specifically French, German, and Romanian. We opt for these three languages in this work as they provide varied levels of limited resources compared to English.

<html><body><table><tr><td>Metric</td><td> Setting</td><td>Ro.</td><td>Ge.</td><td>Fr.</td><td>Avg.</td></tr><tr><td>TSR</td><td>Auto</td><td>99.00</td><td>97.57</td><td>97.79</td><td>98.11 ±1.8</td></tr><tr><td>BTA</td><td>BLEU-3</td><td>51.12</td><td>56.47</td><td>43.63</td><td>50.41</td></tr></table></body></html>

Table 3: Quality assessment of mTEMPREASON’s translations: employing automated verification for Translation Success Rate (TSR in $\%$ ), and applying BLEU-3 and manual review standards for Back-Translation Accuracy (BTA) evaluation across languages— Romanian (Ro.), German (Ge.) and French(Fr.), averaging over temporal tasks.

Romanian, German, and French have $9 8 . 3 \%$ , $8 9 . 5 \%$ , and $7 8 \%$ fewer speakers compared to English2, respectively.

# Data Quality

The mTEMPREASON dataset was constructed by a linguist who specialized in ${ \mathrm { N L P } } ^ { 3 }$ . We employed back-translationbased (Miyabe and Yoshino 2015) evaluation to ensure the superior quality of the proposed dataset. A random selection of 100 query examples was made from mTEMPREASON for each of the translated languages -— Romanian, German, and French, across temporal tasks within the test dataset. These queries underwent back-translation4 into the source language (English) and were subsequently compared to their original counterparts in the resource-rich language (English) to assess fidelity and coherence. The analysis employed the BLEU-3 metric for quantitative evaluation. In addition, successful translation from the source to the target languages was noted, regardless of translation quality. The translation success rate was documented using an automated approach by employing a language detection library5 across the entire test dataset. In Table 3, the mean automated translation success rate was recorded at $9 8 . 1 1 \pm 1 . 8 3 \%$ . Concurrently, a mean BLEU-3 score of 50.41 was observed for assessing back-translation-based accuracy.

# Problem Setting

The Time–Sensitive Question Answering (TSQA) task requires that the LLMs generate an accurate answer in response to a temporal query. This answer may be a temporal delineation or an event, depending on the structure of the query, which can include time–time, time–event, and event–event scenarios. Our experiments are conducted in a closed-book environment, requiring LLMs to deliver precise facts without reliance on external contexts. As illustrated in Figure 1, to demonstrate the prompt in a cross-lingual in-context learning framework, a few-shot example from a resource-rich language, accompanied by the query in a lowresource language, is provided.

In this study, we consider the following baselines• Cross-Lingual In-Context Learning (X-ICL) (Winata et al. 2021). The model is primed with limited examples from resource-rich language serving as demonstrations, along with a query in low-resource language.

• X-InSTA (Semantic Aligner) (Tanwar et al. 2023). XInSTA has advanced the X-ICL method by introducing a retrieval of semantically akin examples for queries across languages, leveraging label space alignment.

# Method

# Primer

Let us consider a resource-rich source dataset $D _ { r }$ , containing pairs of queries and answers $( q _ { i } ^ { r } , a _ { i } ^ { r } )$ for each $i \in m$ , with $m$ representing the total number of samples within $D _ { r }$ . Additionally, let $D _ { l }$ be a low-resource language dataset, which similarly comprises query and answer pairs $( q _ { j } ^ { l } , a _ { j } ^ { l } )$ for each $\textit { j } \in \textit { n }$ , where $n$ signifies the total sample count in $D _ { l }$ . Within a conventional ICL framework, $K$ arbitrary instances of question-answer pairs are selected from $D _ { r }$ , designated as context $C$ for a low-resource query $q _ { x } ^ { l }$ from $D _ { l }$ , with $x \in n$ . The goal is to optimize the expected value of $a _ { x } ^ { l }$ , given context $C$ and the query input as illustrated in Equation 1, where $A ^ { l }$ represents the vocabulary space corresponding to query $q _ { x } ^ { l } \in { \bf \dot { D } } _ { l }$ .

$$
a _ { x } ^ { l } = \arg \operatorname* { m a x } _ { a ^ { l } \in A ^ { l } } p ( a ^ { l } | C , q _ { x } ^ { l } )
$$

In the case of a semantic aligner, $C$ is constructed to maximize the semantic alignment between query $q _ { x } ^ { l }$ and context $C$ . Hence, we introduce $\boldsymbol { \it e } _ { \boldsymbol { q } _ { \it x } ^ { l } }$ as a dense embedding representation produced by a multilingual pre-trained transformer for query $\mathbf { \bar { \Psi } } q _ { x } ^ { l }$ . Correspondingly, for each $i \in m$ , $e _ { { q } _ { i } ^ { r } }$ represents the embeddings for queries within a resource-rich dataset $D _ { r }$ . Furthermore, $f ( s )$ and $f ( d )$ denote similarity and distance functions, respectively, such that for cosine similarity, $f ( s ) = 1 - f ( d )$ ; a lesser distance implies greater similarity. The overarching goal is to identify a set of $K$ examples, denoted as $S _ { K }$ , where the semantic similarity surpasses that of other dataset examples, as shown in Equation 2.

$$
\begin{array} { r l } & { S _ { K } = \{ ( q _ { k } ^ { r } , a _ { k } ^ { r } ) \forall k \in \{ 1 , \ldots , K \} , } \\ & { \qquad \mathrm { i f } \ : f ( s ) _ { q _ { k } ^ { r } , q _ { x } ^ { l } } \geq f ( s ) _ { q _ { z } ^ { r } , q _ { x } ^ { l } } \forall z \in m \mathrm { ~ a n d ~ } z \not \in K \} } \end{array}
$$

The variable $f ( s ) _ { q _ { k } ^ { r } , q _ { x } ^ { l } }$ denotes the semantic similarity between a low-resource query $q _ { x } ^ { l }$ and an example query $q _ { k } ^ { r }$ from a resource-rich language. To calculate $f ( s ) _ { q _ { k } ^ { r } , q _ { x } ^ { l } }$ , the procedure stats with the extraction of dense embeddikngs, $\boldsymbol { e } _ { \boldsymbol { q } _ { k } ^ { r } }$ and $e _ { q _ { \ast } ^ { l } }$ for the input queries $q _ { k } ^ { r }$ and $q _ { x } ^ { l }$ , respectively. This extraction is performed utilizing a multilingual pre-trained transformer model. Subsequently, the function $f ( d )$ is applied to these embeddings to measure the distance, which ultimately yields the value of $f ( s )$ .

# Cross-Lingual Time-Sensitive Semantic Alignment (CLiTSSA)

Objective. We introduce CLiTSSA to augment the semantic similarity context of time–sensitive queries within the cross-lingual embedding space. We elect to employ the transfer of the comprehensive time–sensitive semantic knowledge from an in-language embedding space to a cross-lingual embedding space. For this purpose, we have embraced a supervised fine-tuning approach, making use of a training dataset comprised of sentence pairs along with their associated labeled scores, quantifying the expected time–sensitive semantic similarity among pairs of queries. The objective is to achieve an effective cross-lingual time–sensitive contextual alignment of temporal queries for LLMs, thus boosting in-context performance.

Training Dataset. A training dataset $D _ { t }$ is constructed comprising pairs of sentences alongside their associated similarity scores. Specifically, $D _ { t }$ consists of $( q _ { u } ^ { l } , q _ { v } ^ { r } \mid f ( s ) _ { u , v } )$ , where $q _ { u } ^ { l }$ denotes a low-resource query derived from $D _ { l }$ with $u \in n$ , $q _ { v } ^ { r }$ indicates a query from a resource-rich dataset $D _ { r }$ with $v \in m$ , and $f ( s ) _ { u , v }$ represents the similarity score between these queries within a low-resource monolingual embedding space.

Here, we present a systematic approach to construct $D _ { t }$ . Initially, we delineate $D _ { \textit { r } } ^ { \prime }$ , a transformed resource-rich dataset in a low-resource language using translation. Subsequently, we determine the temporal semantic alignment scores $f ( s )$ among the queries in $D _ { l }$ and $D _ { \textit { r } } ^ { \prime }$ . The utilization of all example pairs in the fine-tuning procedure incurs quadratic complexity in terms of $| D _ { t } |$ , rendering it resourceintensive. Drawing inspiration from Rubin, Herzig, and Berant (2022), this issue is addressed by selecting the top$h$ analogous examples from $D _ { \textit { r } } ^ { \prime }$ for each query in $D _ { l }$ . To counteract training data bias due to high similarity scores, we randomly select $w$ examples from the remaining dataset to capture the whole similarity distribution. Consequently, for every query $q _ { u } ^ { l } \ \in \ D _ { l }$ , the resultant set $S _ { u }$ comprises $( h + w )$ sentence pairs, each accompanied by their temporal semantic similarity scores as postulated in Equation 3.

$$
\begin{array} { r l } & { S _ { u } = \{ ( q _ { u } ^ { l } , q _ { 1 } ^ { r ^ { \prime } } | f ( s ) _ { u , 1 } ) , \dots , ( q _ { u } ^ { l } , q _ { h } ^ { r ^ { \prime } } | f ( s ) _ { u , h } ) , } \\ & { \qquad ( q _ { u } ^ { l } , q _ { h + 1 } ^ { r ^ { \prime } } | f ( s ) _ { u , h + 1 } ) , \dots , ( q _ { u } ^ { l } , q _ { h + w } ^ { r ^ { \prime } } | f ( s ) _ { u , h + w } ) \} } \end{array}
$$

Likewise, paired sentences and their associated similarity scores are generated for all queries within $D _ { l }$ . In the concluding phase, the transformed, resource-rich dataset $D _ { \textit { r } } ^ { \prime }$ is substituted back with the original dataset $D _ { r }$ . Specifically, the query $q _ { v } ^ { r ^ { \prime } }$ , which represents a transformation of the resource-rich query $q _ { v } ^ { r }$ into a low-resource language, is replaced with the original query $q _ { v } ^ { r }$ within the paired sentences dataset. Multilingual Sentence-BERT (Reimers and Gurevych 2019), a pre-trained transformer model, is employed to derive the semantic alignment scores.

Fine-tuning the Retriever. CoSENT6 (Cosine Sentence) loss is employed for the fine-tuning of sentence pairs along with similarity scores as labels, utilizing multilingual Sentence-BERT as base retriever. CoSENT loss generates a more robust training signal for optimizing the cosine value than the traditional cosine similarity loss function. This loss function is shown in Equation 4.

$$
\begin{array} { c } { { \mathcal { L } = l o g \sum ( 1 { + } \exp ( f ( s ) _ { ( q _ { a } ^ { l } , q _ { b } ^ { r } ) } - f ( s ) _ { ( q _ { y } ^ { l } , q _ { z } ^ { r } ) } + } } \\ { { \exp \ldots ) } } \end{array}
$$

<html><body><table><tr><td rowspan="2">Task</td><td rowspan="2">Method</td><td colspan="4">F1.</td><td colspan="4">EM</td></tr><tr><td>French</td><td>German</td><td>Romanian</td><td>Avg</td><td>French</td><td>German</td><td>Romanian</td><td>Avg.</td></tr><tr><td rowspan="4">L1</td><td>X-ICL</td><td>33.60</td><td>45.33</td><td>34.17</td><td>37.70</td><td>14.85</td><td>22.79</td><td>08.80</td><td>15.48</td></tr><tr><td>X-InSTA↑</td><td>46.62</td><td>56.63</td><td>33.65</td><td>45.63</td><td>22.05</td><td>35.45</td><td>10.15</td><td>22.55</td></tr><tr><td>CLiTSSA</td><td>57.15</td><td>59.77</td><td>37.16</td><td>51.36</td><td>32.57</td><td>39.3</td><td>13.45</td><td>28.44</td></tr><tr><td>CLiTSSA*</td><td>55.04</td><td>63.50</td><td>37.78</td><td>52.11</td><td>31.62</td><td>45.15</td><td>13.70</td><td>30.16</td></tr><tr><td rowspan="4">L2</td><td>X-ICL</td><td>11.00</td><td>08.96</td><td>10.99</td><td>10.32</td><td>03.57</td><td>03.73</td><td>03.97</td><td>03.76</td></tr><tr><td>X-InSTA↑</td><td>11.92</td><td>12.45</td><td>11.40</td><td>11.92</td><td>04.55</td><td>05.18</td><td>03.83</td><td>04.52</td></tr><tr><td>CLiTSSA</td><td>15.23</td><td>14.02</td><td>11.42</td><td>13.56</td><td>05.81</td><td>05.24</td><td>03.87</td><td>04.97</td></tr><tr><td>CLiTSSA*</td><td>15.04</td><td>13.53</td><td>11.71</td><td>13.43</td><td>05.74</td><td>05.20</td><td>04.03</td><td>04.99</td></tr><tr><td rowspan="4">L3</td><td>X-ICL</td><td>17.07</td><td>16.18</td><td>18.84</td><td>17.36</td><td>07.28</td><td>05.77</td><td>08.82</td><td>07.29</td></tr><tr><td>X-InSTA↑</td><td>17.74</td><td>18.35</td><td>22.18</td><td>19.42</td><td>10.55</td><td>09.08</td><td>12.85</td><td>10.83</td></tr><tr><td>CLiTSSA</td><td>19.87</td><td>18.52</td><td>22.94</td><td>20.44</td><td>11.22</td><td>08.85</td><td>13.33</td><td>11.13</td></tr><tr><td>CLiTSSA*</td><td>19.92</td><td>18.69</td><td>22.53</td><td>20.38</td><td>11.45</td><td>08.72</td><td>13.23</td><td>11.13</td></tr><tr><td rowspan="2"></td><td>cLiTsSA-↑</td><td>5.32 个</td><td>1.62 ↑</td><td>1.43 ↑</td><td>2.79 ↑</td><td>4.15 ↑</td><td>1.23 ↑</td><td>1.27 ↑</td><td>2.22 ↑</td></tr><tr><td>△cLiTSsAmax-↑</td><td>5.34 个</td><td>2.92 个</td><td>1.73 个</td><td>3.04 ↑</td><td>4.22 ↑</td><td>3.17个</td><td>1.41 个</td><td>2.79 个</td></tr></table></body></html>

Table 4: Comparison of F1 and EM (Exact Match) scores across different prompting strategies for temporal tasks and languages in a three-shot setup employing LLaMA3-8B. The strategies include $X { - } I C L$ and $X – I n S T A$ , representing random and semantically aligned cross-lingual baselines, respectively, while $\boldsymbol { \mathrm { C L i T S S A } ^ { * } }$ denotes an integrated retriever trained across languages and tasks, and CLiTSSA indicates a language and task-specific retriever. $\overline { { \Delta } }$ represents mean improvement for languages across temporal tasks and $\mathtt { C L i T S S A } ^ { m a x }$ representing max(CLiTSSA, CLiTSSA∗). We report mean values over three runs by varying the parameter top $p \in \{ 1 . 0 , 0 . 8 , 0 . 6 \}$ and apply one tail Mann-Whitney U test for p-values. We observe a p-value of 0.05 while comparing the mean F1 score of CLiTSSA with X-InSTA across languages and tasks.

In this context, where $( q _ { a } ^ { l } , q _ { b } ^ { r } )$ and $( q _ { y } ^ { l } , q _ { z } ^ { r } )$ represent instances from $D _ { t }$ within a batch, under the condition that the anticipated similarity between $( a , b )$ exceeds that of $( y , z )$ , the summation extends across all feasible input pairs within the batch satisfying this criterion. This approach amalgamates both cross-entropy and contrastive loss advantages.

# Experimental Results And Analysis Experimental Setup

Primarily, we employ LLaMA3-8B (AI@Meta 2024) for all experimental works. A three-shot ICL approach is used throughout the experimental setting, demonstrating superior outcomes compared to both one-shot and two-shot configurations. The value of $h$ and $w$ is set empirically at 30 and 10, respectively. To fine-tune the CLiTSSA retriever model, the ‘distiluse-base-multilingual-cased-v1’ serves as the foundational model. This method is systematically applied to each low-resource language across temporal tasks – L1, L2 and L3, to ensure optimum performance. Additionally, an integrated CLiTSSA retriever is fine-tuned across languages and temporal tasks. The Train and Dev datasets from mTEMPREASON are used to construct the parallel corpus to fine-tune the CLiTSSA retriever, with a separate held-out test set employed to benchmark all outcomes. We use word level F1 scores and exact match (EM) standards to quantify the LLM’s responses. Please refer to the technical appendix for ablations on few-shots, parameters $h$ & $w$ , along with hyperparameters in detail.

# CLiTSSA Advancements Over Precedence

The comprehensive comparison of CLiTSSA with baselines highlights the effectiveness of incorporating cross-lingual time–sensitive semantic alignment compared to a conventional semantic aligner (X-InSTA), as evidenced in Table 4 across a variety of low-resource languages and temporal tasks. The mean values of metrics were compared across three iterations by varying the model’s parameter top $p$ (1.0, 0.8, 0.6). The parameter indicates the cumulative probability threshold for token selection. Notably, CLiTSSA achieves a mean increase of 5.32, 1.62, and 1.43 points in the F1- score for French, German, and Romanian, respectively, with a p-value of 0.05. Specifically, the most significant improvements in F1-score—10.53, 3.31, and 2.13 points for tasks L1, L2, and L3, respectively are observed in the French setting. A similar enhancement is discernible concerning the EM metric. Moreover, the overall analysis does not yield a definitive conclusion for an integrated CLiTSSA retriever over its counterpart except a notable transcend of 3.7 points in the F1-score for task L1 within the German setting.

Table 5: The performance of CLiTSSA across LLMs for temporal tasks using the French test set $\bar { \Delta }$ : the mean improvement in F1 score across LLMs for a temporal task).   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Method</td><td colspan="3">F1.</td></tr><tr><td>L1</td><td>L2</td><td>L3</td></tr><tr><td rowspan="2">LLaMA3-8B</td><td>X-InSTA↑</td><td>46.62</td><td>11.92</td><td>17.74</td></tr><tr><td>CLiTSSA</td><td>57.15</td><td>15.23</td><td>19.87</td></tr><tr><td>Mistral-v1</td><td>X-InSTA↑ CLiTSSA</td><td>38.26 46.45</td><td>11.73 14.83</td><td>18.72 18.64</td></tr><tr><td>Vicuna-7b-v1.5</td><td>X-InSTA↑ CLiTSSA</td><td>27.93 36.67</td><td>9.54 12.04</td><td>12.68 12.89</td></tr><tr><td>Bloomz-7b1</td><td>X-InSTA↑ CLiTSSA</td><td>29.45 40.74</td><td>3.79 4.20</td><td>13.49 13.57</td></tr><tr><td></td><td>CLiTSSA-↑</td><td>9.68↑</td><td>2.33↑</td><td>0.58 ↑</td></tr></table></body></html>

![](images/b152b47a6039b4cd4e7f53ce2d3891e733227fbd51cb1f8b53456049f748a3a1.jpg)  
LFoadiing [gMathJuax]/erxtenesio s/Ma2thM:enu. jsComparison of F1 scores using box plot: a dual perspective on temporal tasks with language models and languages, pivoting on French and LLaMA3-8B, respectively.

# Robustness Across LLMs

The assessment of CLiTSSA across a variety of distinct, contemporary LLMs namely, an English-dominant instruction-tuned model, Vicuna (Zheng et al. 2023), a model fluent in French and German, Mistral (Jiang et al. 2023), and a cross-lingual specialized LLM, Bloomz (Muennighoff et al. 2023), demonstrates the robustness of the method. This evaluation highlights the versatility and effectiveness of CLiTSSA in engaging with and analyzing linguistic data across different languages and model architectures. The findings, as detailed in Table 5, reveal that CLiTSSA surpasses the baseline by margins of 9.68, 2.33, and 0.58 points in mean F1-score for L1, L2, and L3, respectively, across LLMs.

To further substantiate the statistical significance of findings, a comparison is drawn through a box plot analysis of the F1-scores under two scenarios: firstly, by plotting F1- scores across LLMs and temporal tasks with a focus on the French language and secondly, through a box plot that contrasts F1-scores across various languages and temporal tasks centered around a specific LLM, namely LLaMA3-8B. Figure 2 shows that CLiTSSA notably extends the upper quartile by 10.01 and 5.26 points in terms of F1-score, with a mean increase of 4.20 and 2.79 points in the F1 scores, across these scenarios, respectively. Additionally, embedding space evolution under CLiTSSA, presented in technical appendix, further elucidates the noted enhancement.

# Cross-Task CLiTSSA Performance

Here, we evaluate the CLiTSSA retriever’s generalization across temporal tasks to see if semantic alignment, sensitive to temporal variation achieved in one task, facilitates the resolution of another temporal task without necessitating a refine-tuning. The CLiTSSA model, once fine-tuned on a specific temporal task, is assessed on two other temporal tasks, i.e., the retriever optimized on the L1 task is employed to retrieve time–sensitive semantic examples for the L2 and L3 tasks. Figure 3 presents the outcomes of this investigation. Note that tasks L1, L2, and L3 are sequentially arranged in order of temporal complexity, with L3 being the most intricate. The findings reveal that the temporal alignment acquired through the lower-level temporal task (L1) can significantly enhance the relative F1-score of the more complex tasks L2 and L3 by $1 3 . 5 \%$ and $1 4 . 0 \%$ , respectively. However, the reverse scenario is inapplicable. Moreover, more complex tasks, L2 and L3, can exchange learning, thereby improving their F1-score relatively by $2 7 . 1 \%$ and $1 3 . 6 \%$ , respectively. French is employed as the low-resource language in this study. The results corroborate that fine-tuning the CLiTSSA with a low-level temporal task (L1) could serve as a superior alternative to any semantic-based example retriever across temporal tasks.

![](images/a2e0a2172937b237fd9f17d348f04b49b353eb73a44234506749fad8582455ee.jpg)  
Figure 3: Cross–Task CLiTSSA performance across tasks with F1 scores on the French test set against the X-InSTA baseline. CLiTSSA-L∗ represents a retriever fine-tuned using $\mathbf { L } ^ { * }$ training dataset where $* \in \{ 1 , 2 , 3 \}$ .

# Cross-Linguality vs. Monolinguality

The complexity of the prompt increases with the incorporation of multiple languages, which detrimentally impacts the performance of ICL in cross-lingual contexts when contrasted with monolingual scenarios. This experiment delineates the CLiTSSA’s effectiveness in notably diminishing this discrepancy. As shown in Figure 4, CLiTSSA ameliorates the performance differential between the French monolingual environment and cross-lingual context by 10.53, 3.31, and 2.13 absolute points in terms of F1-score for the L1, L2, and L3 tasks, respectively, thereby comparing the results to those observed in a monolingual context. Contrastingly, the English monolingual scenario exhibits a significant divergence from its French counterpart for L1 and L2 tasks, underscoring the imperative for speedy enhancements to bolster performance in monolingual contexts for languages with limited resources.

# Error Analysis

Table 6 shows a couple of instances underscoring the challenges of semantic alignment. Initially, the heightened time–sensitive alignment offered by our model does not rectify the inaccuracies in the foundational knowledge of the underlying LLM. The first example elucidates that the factual inaccuracies inherent in the LLaMA3-8B model within a resource-rich linguistic context (i.e., English monolingual, $E n _ { m } )$ ) persist despite the application of CLiTSSA in a French cross-lingual setting $( F r _ { c } )$ . Additionally, our proposed methodology is contingent on the semantic context within the monolingual embedding space, aligning the cross-lingual space accordingly. Consequently, inaccuracies in expected responses may propagate from the monolingual to the cross-lingual space notwithstanding the enhanced query alignment. A subsequent example illustrates this phenomenon in the contexts of French monolingual $( F r _ { m } )$ and French cross-lingual scenarios. Furthermore, notwithstanding the semantic alignment ingrained in crosslingual queries, the implicit aspect of temporality persistently presents a challenge, as observed for the L3 task.

Table 6: Failure cases with CLiTSSA and their corresponding responses from X-InSTA in the monolingual scenario. $E n _ { m }$ English monolingual, $F r _ { m }$ : French monolingual, and $F r _ { c }$ : French cross-lingual. X: X-InSTA, and C: CLiTSSA   

<html><body><table><tr><td>#</td><td>Setup</td><td>Question/Answer (Q/A)</td><td>Predicted Answer</td></tr><tr><td>1</td><td>X.Fr</td><td>Q: Wuuedd</td><td>Leeds UnitedF.C.</td></tr><tr><td>2</td><td>X.F</td><td>Q:Quititeiesia</td><td>Ivan Pudar</td></tr></table></body></html>

![](images/7fe1d16c6f600e4969451f229ed1780549e42febae8a7cfe3c89681e8a320ffa.jpg)  
LFoadiing [gMathJuax]/erxtenesio s/M4ath:M u.jsA comparative analysis of F1 scores across temporal tasks in monolingual and cross-lingual scenarios utilizing LLaMA3-8B, where $E n _ { m }$ and $F r _ { m }$ represent monolingual settings for English and French, respectively, while $F r _ { c }$ is French’s cross-lingual setting.

# Related Works

In NLP, significant foundational efforts in temporal reasoning encompass the creation of TimeBank (Pustejovsky et al. 2003), TempEval (Verhagen et al. 2010), and Time–stamped Language Models (Rajaby Faghihi and Kordjamshidi 2021), each contributing significantly to the understanding and processing temporal data. Concurrently, the evolution of knowledge graphs (KGs) has accentuated the importance of temporal relations therein, catalyzing the emergence of Temporal Knowledge Graph Completion (TKGC) as a distinct area of study. This progression has given rise to noteworthy question-answering datasets predicated on TKG, including TEQUILA (Jia et al. 2018), TimeQuestions (Jia et al. 2021), and CronQuestions (Saxena, Chakrabarti, and Talukdar 2021). The widespread use of language models in the public sphere further underscores the necessity for both temporal accuracy and consistency within generated responses. In response to this demand, several time–sensitive QA datasets, such as TEMPLAMA (Dhingra et al. 2022) and TEMPREASON (Tan, $\mathrm { N g }$ , and Bing 2023), have been introduced to assess and benchmark the temporal reasoning capabilities inherent in LLMs. Among these, TEMPREASON stands out as a comprehensive benchmark for temporal reasoning, spanning a broad spectrum of temporal periods and incorporating three levels of temporal relations. Further, the TEMP-COFAC (Bajpai et al. 2024) has been introduced to assess the temporally consistent factuality.

Furthermore, most LLMs are trained on multilingual datasets (Wenzek et al. 2020), a practice that was once a rarity given the dominance of extensive English corpora (Radford et al. 2019). Despite this, LLMs have proven their mettle in considerable languages. While there have been significant advancements in the multilingual capabilities of LLMs (Lin et al. 2022a; Qin et al. 2024), they still face significant challenges when dealing with low-resource languages (Cahyawijaya, Lovenia, and Fung 2024), especially in task-specific contexts (Enis and Hopkins 2024). To address this, innovative approaches such as prompting for generating intermediate English contexts (Huang et al. 2023), cross-lingual prompting (Winata et al. 2021), and Linguistically Diverse Prompting (LDP) (Nguyen et al. 2024) have been introduced. In the cross-lingual prompting domain, specific developments like semantic label-based alignment (Tanwar et al. 2023), query-based alignment via translation semantic similarity (Cahyawijaya, Lovenia, and Fung 2024), and a model-specific fine-tuned retriever (Lin, Martins, and Schu¨tze 2024) have further enhanced LLMs’ capabilities. Yet, the exploration of temporal reasoning within low-resource languages remains scant, presenting a compelling area for further research. This study proposes to pioneer advancements in this under-explored domain.

# Conclusion

In this paper, we introduced a novel dataset, mTEMPREASON, aimed at improving temporal reasoning assessment in low-resource languages using LLMs. Our analyses identified that multilingual LLMs inherently reward in-language time–sensitive semantic alignment over the cross-lingual similarity context in the X-ICL method. To overcome this, we proposed CLiTSSA, a novel method that enhances the retrieval of time–sensitive contextually relevant examples across low-resource languages. Our results demonstrated that this approach effectively improves LLMs’ temporal reasoning in low-resource languages, which we believe will aid in promoting linguistic diversity and the development of more inclusive LLMs. Future endeavors may benefit from examining the alignment between an implicit temporal query’s semantics and its implied semantic space to enhance intricate L3 task performance.