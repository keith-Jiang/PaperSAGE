# Empowering Self-Learning of LLMs: Inner Knowledge Explicitation as a Catalyst

Shijue Huang1,4\*, Wanjun Zhong2\*, Deng $\mathbf { C a i } ^ { 2 }$ , Fanqi Wan3, Chengyi Wang2, Mingxuan Wang2, Mu Qiao2, Ruifeng $\mathbf { X } \mathbf { u } ^ { 1 , 4 , 5 \dagger }$

1Harbin Institute of Technology, Shenzhen, China 2Bytedance Seed, China 3 Sun Yat-sen University, China 4Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies 5Peng Cheng Laboratory, Shenzhen, China joehsj310@gmail.com, wanjun@bytedance.com, xuruifeng@hit.edu.cn

# Abstract

Self-learning of Large Language Models (LLMs) facilitates their advancement towards super-intelligence by training with self-synthesized experiences. However, a critical challenge is the amplification of hallucinations in generated data during iterative self-learning, underscoring the need for reliable data selection. To address this, we investigate the mechanism of Inner Knowledge Explicitation, which involves explicitly extracting the inner knowledge from memory of LLMs, to concurrently improves reasoning, and enables reliable self-learning data selection. This paper introduces a Self Knowledge Explicitation Learning (SKE-Learn) framework, which equips the LLMs with meta-skills to explicitly extract, verify and utilize inner knowledge for reasoning. By leveraging these meta-skills, SKE-Learn establishes a self-learning approach that ensures reliable selection of selfsynthetic data. This approach enhances performance through iterative self-learning while mitigating the problem of hallucinations. Empirical results from six benchmarks demonstrate that Inner Knowledge Explicitation improves reasoning by serving as a more effective prompting method. Additionally, SKE-Learn, based on the verifiability of explicit knowledge, shows consistent performance improvements over multiple self-training iterations, with an average performance increase from $5 2 . 7 9 \%$ to $56 . 5 4 \%$ across all benchmarks. Furthermore, Inner Knowledge Explicitation provides explanation and intervention space during LLM’s generation process.

Code — https://github.com/JoeYing1019/SKE-Learn

# 1 Introduction

Large language models (LLMs) (OpenAI et al. 2024; Dubey et al. 2024) have significantly advanced the field of natural language processing (NLP). However, as LLMs evolve rapidly, the traditional approach of collecting high-quality human annotations for model training struggles to meet the increasing scalability demands. Therefore, self-learning methods for LLMs (Wang et al. 2023b; Tu et al. 2024; Tao et al. 2024) have been proposed, allowing LLMs to learn autonomously from self-synthesized, large-scale training data.

Query Reasoning Hard to Verify Real-World Inner Knowledge Knowledge E=MC² Reasoning A E=MC! Verifiable   
(a) Direct Reasoning v.s. Inner Knowledge Explicitation. A E=MC!   
Question Inner Knowledge Reasoning 6 H E=MC² Verify 日 Filtered Data Real-World Self-learning (Knowledge-Reasoning) Knowledge (b) Self-learning with Verifiable Inner Knowledge.

This paradigm paves the way for automatic advancement of LLMs towards super-intelligence. Nonetheless, synthesized data often has the issue of hallucinations (Ji et al. 2023), which will be amplified during iterative self-training and leads to model collapse (Shumailov et al. 2024). Hence, reliable selection of self-learning data is of substantial significance to ensure learning robustness. However, previous methods (Huang et al. 2023; Lu et al. 2024) directly synthesize reasoning process, which is question-specific and challenging to verify without human annotations or more advanced models’ judgments. In contrast, as knowledge has authorized resource for verification and is general for different questions, its correctness is more easily to be verified as shown in Figure 1 (a).

Inspired by human problem-solving, where relevant knowledge is recalled from memory before reasoning, equipping models with meta-skills to acquire, validate, and utilize their inner knowledge could enhance reasoning and ensure more reliable automatic data selection.

Thus, this paper aims to systematically investigate the mechanism of “Inner Knowledge Explicitation”, focusing on two key questions: (1) Does the explicit extraction of knowledge from the inner memory of LLMs enhance their reasoning abilities? (2) Given that explicit knowledge is inherently verifiable, can it be leveraged to automatically select self-synthetic data for LLMs, thereby establishing a reliable self-learning system? Specifically to Inner Knowledge Explicitation, unlike traditional approaches that directly generate answers, we require LLMs to first extract explicit inner knowledge in natural language from their memory and then utilize it for reasoning, as illustrated in Figure 1 (b). The explicit inner knowledge not only improves reasoning performance as a guidance but also provides explanation for the utilized knowledge, which facilitates verification of data quality. Furthermore, we propose a Self Knowledge Explicitation Learning (SKE-Learn) framework, which empowers LLMs with meta-skills to explicitly extract, verify, and utilize inner knowledge for reasoning, and develop a self-learning approach using these skills for automatic reliable data selection. Specifically, SKE-Learn begins by extracting explicit inner knowledge from LLM’s memory. This extracted knowledge then enhances followed reasoning process by providing high-level guidance on concepts or principles relevant to the given query.

Additionally, by leveraging the verifiability of explicit inner knowledge, SKE-Learn improves the reliability of selfsynthetic data selection, alleviating the issue of hallucination in iterative training and concurrently enhancing model’s meta-skills to utilize inner knowledge by establishing a self-learning approach. The self-learning approach in SKELearn comprises two stages: (1) Meta-skill Training: This stage focuses on developing model’s meta-skills, including the self-extraction of explicit inner knowledge, knowledgeenhanced reasoning, and self-assessment that refers to the ability of self-evaluating explicit inner knowledge and reasoning process. (2) Iterative Self-training: In this stage, the model self-generates “question-knowledge-reasoning” traces for training. These traces are filtered based on model’s self-assessment ability, which assesses the truthfulness of reasoning and explicit inner knowledge by comparing it with real-world knowledge as a verifiable reference. This iterative process strengthens the model’s knowledge utilization and improves its reasoning capabilities over time.

Extensive experiments demonstrate that while LLMs possess extensive knowledge, explicit extraction of inner knowledge still significantly enhances their reasoning performance. The self-learning approach of SKE-Learn alleviates amplified hallucinations and improves model’s reasoning capabilities iteratively, resulting in an average performance increase from $5 2 . 7 9 \%$ to $56 . 5 4 \%$ across six benchmarks. Additionally, the Inner Knowledge Explicitation also offers explanations and intervention space in LLM generation. Contributions of this work are as follows:

• We systematically investigate Inner Knowledge Explicitation and show that extracting explicit inner knowledge from LLMs’ memory enhances reasoning performance. • We propose a SKE-Learn framework to improve self

learning of LLMs, utilizing verifiable knowledge for reliable data selection, ultimately alleviating amplified hallucinations and improving effectiveness in self-training. • Comprehensive experiments across six benchmarks reveal that both the Inner Knowledge Explicitation mechanism and the SKE-Learn self-learning approach elicit reasoning abilities, and provide better interpretability in model knowledge utilization.

# 2 Preliminary

In this section, we define the concept of Inner Knowledge Explicitation in the scope of this paper. Specifically, Inner Knowledge Explicitation refers to the mechanism that LLMs explicitly output their inner knowledge relevant to given queries in natural language. Here, we limit the knowledge scope to the relevant concepts, theories, principles, laws, and factual information regarding domains of given queries.

# 3 Methodology

In this section, we introduce the meta-skills emphasized in SKE-Learn and the prompting method with Inner Knowledge Explicitation ( 3.1). Follow-up with a self-learning approach equips LLMs with meta-skills on explicitly extracting, verifying, and utilizing inner knowledge, thereby enabling reliable self-synthetic training data selection and improving model’s performance ( 3.2). Figure 2 shows the overall workflow.

# 3.1 Meta-skills and Prompting Method with Inner Knowledge Explicitation

Given a question $q$ and its corresponding reference knowledge $\tilde { k }$ , SKE-Learn incorporates three key meta-skills:

(1) Self-extraction of Inner Knowledge, which involves extracting relevant explicit inner knowledge $k$ from the model’s memory in response to $q$ :

$$
k = \mathcal { M } ( q , p _ { \mathrm { e x t r a c t } } ) ,
$$

(2) Knowledge-enhanced Reasoning, referring to the ability generate reasoning $r$ based on the extracted knowledge:

$$
r = \mathcal { M } ( q , k , p _ { \mathrm { r e a s o n } } ) ,
$$

(3) Self-assessment, which is the ability to self-evaluate the quality of the reasoning process, and inner knowledge taking authoritative real-world knowledge as a reference:

$$
\begin{array} { r } { s ^ { k } = \mathcal { M } ( q , k , \tilde { k } , p _ { \mathrm { s c o r e } } ^ { k } ) , } \\ { s ^ { r } = \mathcal { M } ( q , k , r , p _ { \mathrm { s c o r e } } ^ { r } ) , } \end{array}
$$

where $\mathcal { M }$ represents model; $s ^ { k }$ and $s ^ { r }$ are scores for knowledge and reasoning; $p$ extract, preason, $p _ { \mathrm { s c o r e } } ^ { k }$ and $p _ { \mathrm { s c o r e } } ^ { r }$ are related prompts for these meta-skills.

As illustrated in Figure 2 (a), the prompting method with Inner Knowledge Explicitation leverages the aforementioned first two meta-skills by extracting inner knowledge and then applying it for reasoning. Similar to evidence-based reasoning (Howick, Glasziou, and Aronson 2010; Gupta et al. 2022), SKE-Learn enables the model to quote explicit inner knowledge as evidence, thereby reducing hallucinations and enhancing overall quality of generated outputs.

Question Inner Knowledge Reasoning Let x = 1. What is x << 3 in 1. Left Shift Operator: The left $\mathbf { x } = \overline { { 1 } }$ , and we want to find $\mathbf { x } < <$ Python 3? shift operator $( < < )$ in … 3. Based on knowledge 1, this is (A) 1 (B) 3 2. Power of 2: … equivalent to $\mathbf { x } ^ { * } 2 ^ { \wedge } 3 .$ . …   
(C) 8 (D) 16 The answer is C: 8. (a) Prompting method with Inner Knowledge Explicitation.   
Initial LLM (h!) TrMaientian-gskDiallta MetLaL-sMkiwllisth(h") SeLlfL eMv(ohl#v)ing Self-training Data 8 (1) 计 (2) Initialize o (5) h! ④Filter ⑤Filter h#- ④Filter   
Verifiable IOi 出 高 Strong i Verifiable Agent’s ⑤ Filter   
Knowledge Inner Knowledge Judgement Knowledge Inner Knowledge Chunk Chunk □② ② Self① 沁 8 ③ assessment Question Reasoning Question Reasoning (b) Self-learning approach of SKE-Learn.

# 3.2 Self-learning Approach of SKE-Learn

To further boosting model’s reasoning ability and reinforcing meta-skills, we design a self-learning approach. This approach consists of two stages: meta-skill training and iterative self-training as depicted in Figure 2 (b).

Meta-skill Training This stage focuses on referencing meta-skills including self-extraction of explicit inner knowledge, knowledge-enhanced reasoning, and self-assessment. Assuming the availability of a substantial unsupervised knowledge corpus $\mathcal { C }$ , we aim to construct several data collections $\mathsf { \bar { D } } _ { \mathrm { m e t a } } ^ { K }$ , $\mathcal { D } _ { \mathrm { m e t a } } ^ { R }$ , and $\mathcal { D } _ { \mathrm { m e t a } } ^ { S }$ for different meta-skills, the former two include instances of high-quality “questionknowledge-reasoning” traces (i.e. $q _ { i } , k _ { i } , r _ { i } )$ and the latter one additionally comprises corresponding knowledge chunks and scores for the knowledge and the reasoning (i.e. $c _ { i } , q _ { i } , k _ { i } , r _ { i } , s _ { i } ^ { k } , s _ { i } ^ { r } )$ . As illustrated in the left part of Figure 2 (b), our process begins by sampling knowledge chunk $c _ { i }$ from $\mathcal { C }$ . The initial model $\mathcal { M } _ { 0 }$ then generates a question $q _ { i }$ that requires reasoning about $c _ { i }$ , and further generates associated knowledge $k _ { i }$ and reasoning $\boldsymbol { r } _ { i }$ .

Since $\mathcal { M } ^ { 0 }$ may struggle with self-assessment in the initial phase, we draw inspiration from Zheng et al. (2023) and leverage a more advanced model (i.e. GPT-4), $\mathcal { I }$ , to evaluate the knowledge $k _ { i }$ and reasoning $\boldsymbol { r } _ { i }$ at this stage. Specifically, $\mathcal { I }$ utilizes the knowledge chunk $c _ { i }$ as a direct and verifiable reference to assess the correctness and reliability of $k _ { i }$ . It determines whether $k _ { i }$ accurately reflects the question’s requirements and covers all necessary aspects, ultimately assigning a score $s _ { i } ^ { k }$ to $k _ { i }$ . For the reasoning $r _ { i }$ , $\mathcal { I }$ evaluates its factual accuracy, logical coherence, and application of the provided explicit knowledge, producing a score $s _ { i } ^ { r }$ for $r _ { i }$ . To mitigate the risk of knowledge distillation from used advanced model at this stage, we have $\mathcal { M } ^ { 0 }$ synthesize all the knowledge and reasoning traces, using the more advanced model $\mathcal { I }$ solely for correctness evaluation and filtering. Thus, the knowledge and reasoning are originally acquired by $\mathcal { M } ^ { 0 }$ , the only injected capability is critical scoring.

Subsequently, we select the high-quality sets of $k _ { i }$ and $r _ { i }$ based on pre-defined threshold to construct the meta-skill training data $\mathcal { D } _ { \mathrm { m e t a } } ^ { K }$ for self-extraction of inner knowledge and $\mathcal { D } _ { \mathrm { m e t a } } ^ { \breve { R } }$ for Dknowledge-enhanced reasoning, respectively. For the meta-skills of self-assessment, a small portion of the data $\mathcal { D } _ { \mathrm { m e t a } } ^ { S }$ is uniformly sampled from both $\bar { s } _ { i } ^ { k }$ and $s _ { i } ^ { r }$ for training. To further enhance the diversity of the questions, we also incorporate a small number of existing questions during this process to construct the knowledge and reasoning process in a similar manner. Utilizing this meta-skill training data, we then develop a new model $\mathcal { M } _ { \mathrm { m e t a } } ^ { 1 }$ with followed training objective:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { m e t a } } = - \mathbb { E } _ { ( q _ { i } , k _ { i } ) \sim \mathcal { D } _ { \mathrm { m e t a } } ^ { K } } \log ( k _ { i } \vert q _ { i } ) } \\ & { \phantom { { = } } - \mathbb { E } _ { ( q _ { i } , k _ { i } , r _ { i } ) \sim \mathcal { D } _ { \mathrm { m e t a } } ^ { R } } \log ( r _ { i } \vert q _ { i } , k _ { i } ) } \\ & { \phantom { { = } } - \mathbb { E } _ { ( c _ { i } , q _ { i } , k _ { i } , r _ { i } , s _ { i } ^ { k } , s _ { i } ^ { r } ) \sim \mathcal { D } _ { \mathrm { m e t a } } ^ { S } } [ \log ( s _ { i } ^ { k } \vert q _ { i } , k _ { i } , c _ { i } ) + } \\ & { \phantom { { = } } \log ( s _ { i } ^ { r } \vert q _ { i } , k _ { i } , r _ { i } ) ] } \end{array}
$$

Iterative Self-training. Since $\mathcal { M } _ { \mathrm { m e t a } } ^ { 1 }$ possesses better meta-skills in self-extraction of inner knowledge, knowledge-enhanced reasoning, and self-assessment, it can engage in iterative self-training through self-synthesizing data and selecting high-quality ones by self-assessment capability. This process further strengthens these meta-skills and enhances model’s overall reasoning performance.

Given an unsupervised knowledge chunk $c _ { j }$ , we use $\mathcal { M } _ { \mathrm { m e t a } } ^ { 1 }$ to self-synthesize “question-knowledge-reasoning” traces, similar to the meta-skill training phase, thereby obtaining question $q _ { j }$ , knowledge $k _ { j }$ and reasoning process $r _ { j }$ .

Unlike the meta-skill phase that uses a stronger model for scoring, here we leverage the self-assessment capability of $\mathcal { M } _ { \mathrm { m e t a } } ^ { 1 }$ to provide scores for $k _ { j }$ and $r _ { j }$ , denoted as $s _ { j } ^ { k }$ and $s _ { j } ^ { r }$ , respectively. Then high-quality $k _ { j }$ and $r _ { j }$ are selected to construct a new set of self-evolving training data $\mathcal { D } _ { \mathrm { e v o l } } ^ { K }$ and $\mathcal { D } _ { \mathrm { e v o l } } ^ { R }$ , which are used to train a more proficient model, $\mathcal { M } ^ { 2 }$ . The training objective of this stage is:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { e v o l } } = - \mathbb { E } _ { ( q _ { j } , k _ { j } ) \sim D _ { \mathrm { e v o l } } ^ { K } } \log ( k _ { j } | q _ { j } ) } \\ & { \qquad - \mathbb { E } _ { ( q _ { j } , k _ { j } , r _ { j } ) \sim D _ { \mathrm { e v o l } } ^ { R } } \log ( r _ { j } | q _ { j } , k _ { j } ) } \end{array}
$$

During this stage, we iteratively obtain models $\mathcal { M } ^ { i }$ trained on data synthesized by $\mathcal { M } ^ { i - 1 }$ . The self-extracted knowledge of $\mathcal { M } ^ { i - 1 }$ is validated by its self-assessment ability with ready-made authoritative references from real-world knowledge chunks. This validation alleviates hallucinated data being used for self-learning by establishing a more reliable data selection process. Moreover, since all data originates from the model itself, this process ensures both reliability and automation. As a result, at each round, the model can progressively refine its knowledge boundaries and metaskills from experiences of the previous round’s model, leading to iterative improvements in performance.

# 4 Experiments

In this section, we verify whether Inner Knowledge Explicitation can elicit reasoning and facilitate self-learning $( \ S 4 . 2 )$ , whether knowledge injection occurs in self-learning ( 4.3), and provide detailed analyses about the meta-skills evolution, interpretability, intervention space and hallucination alleviation brought by Inner Knowledge Explicitation ( 4.4).

# 4.1 Experimental Setup

Implementation Details. We fine-tune Llama3- 8B (Dubey et al. 2024) on 100,000 instances of Magpie data1 (Xu et al. 2024b), and derived an instruct model as the backbone model of whole experiments, namely Llama3- 8B-Magpie. All models are trained with full parameters for 2 epochs, using batch size of 32, learning rate of 2e-5, with 100 warmup steps. In inference phase, we set all temperatures as 0 to ensure better reproducibility. All experiments are performed on eight NVIDIA A100-SXM4-80GB GPUs.

Comprising meta-skill training, we conduct totally four rounds of iterative training. Following Yuan et al. (2024), we mix a proportion of general data2 and the scoring data from the meta-skill training stage at each round to maintain both general responsiveness and self-assessment capability. Training data details for each round are provided in Table 1.

The unsupervised knowledge corpora are sourced from a collection based on Wikipedia3 , and the small proportion of existing questions leveraged in this stage are drawn from a dataset focused on $\mathrm { S T E M ^ { 4 } }$ . In meta-skill training, we use GPT-4 with version gpt-4-0125-preview as

<html><body><table><tr><td>Model</td><td>Knowledge</td><td>Reasoning</td><td>Scoring</td><td>General</td></tr><tr><td>M→Mmeta</td><td>5,000</td><td>5,000</td><td>4,237</td><td>25,000</td></tr><tr><td>Me meta→M²</td><td>100,35</td><td>10,135</td><td>42,37</td><td>25,000</td></tr><tr><td>M²→M</td><td>23,503</td><td>22.450</td><td>4,237</td><td>25,000</td></tr><tr><td>M³→M4</td><td>38.069</td><td>42.672</td><td>4,237</td><td>25,000</td></tr></table></body></html>

Table 1: Training data details in our self-learning approach.

judge model. The scores for knowledge and reasoning process both range from 0 to 10, with higher scores indicating better quality. The score threshold for selecting data is 8.

Benchmarks and Evaluation Metrics. We conducted experiments across a wide range of popular benchmarks, including general examination benchmarks such as MMLU (Hendrycks et al. 2021), AGIEval (Zhong et al. 2024), and ARC (encompassing ARC-E and ARC-C) (Clark et al. 2018). Additionally, we evaluated on the comprehensive reasoning benchmark BBH (Suzgun et al. 2023) and the knowledge question-answering benchmark Natural Questions $( \mathrm { N } \bar { \mathrm { Q } } ) ^ { 5 }$ (Kwiatkowski et al. 2019). All evaluation metrics are accuracy, with corresponding evaluation scripts derived from OpenCompass (Contributors 2023).

Baselines. To verify the performance of prompting method with Inner Knowledge Explicitation, some zeroshot Prompting-based Approaches utilizing LLM’s inner knowledge implicitly are compared, including: (1) Zero-shot (Brown et al. 2020) generates answers directly. (2) Chain-of-Thought (Wei et al. 2022) generates stepby-step thoughts. (3) Plan-and-Solve (Wang et al. 2023a) conducts planning before reasoning. (4) Rephrase-andRespond (Deng et al. 2023) instructs LLMs to rephrase the question. (5) System-2-Attention (Weston and Sukhbaatar 2023) instructs LLMs to remove irrelevant information from prompts. (6) Thread-of-Thought (Zhou et al. 2023) features an enhanced thought inducer. (7) Analogical (Zhou et al. 2023) automatically generates exemplars. (8) ReReading (Xu et al. 2024a) repeats the question twice.

For the Self-learning Approach, we introduce a selflearning baseline denoted as $\tilde { \mathcal { M } }$ , which is trained by typical self-learning manner that self-synthesizes Chain-of-Thought format “question-reasoning” trace and selects high-quality data by solely self-rewarding on reasoning process (Yuan et al. 2024), with an equivalent amount of data created from all knowledge corpora used in SKE-Learn.

# 4.2 Main Result

The main results are demonstrated in Table 2, we can observe that:

Inner Knowledge Explicitation Elicits Reasoning Ability. Compared to existing prompting methods that utilize knowledge implicitly, SKE-Learn demonstrates superior performance across all benchmarks, outperforming the most competitive baseline (i.e., Thread-of-Thought) by an average of

Table 2: Main results across various benchmarks. The Average score represents the mean performance across all benchmarks Scores that are bolded indicate the highest performance among same setting.   

<html><body><table><tr><td>Methods</td><td>MMLU</td><td>BBH</td><td>ARC-E</td><td>ARC-C</td><td>AGIEval</td><td>NQ</td><td>Average</td></tr><tr><td colspan="8">Prompting-based Approaches</td></tr><tr><td>Zero-shot (Brown et al. 2020) Chain-of-Thought (Wei et al.2022)</td><td>54.81 48.06</td><td>16.82 24.60 24.67 22.46</td><td>83.46 68.77 67.05</td><td>69.28 60.24 55.21</td><td>45.93 44.99 45.67</td><td>24.04 17.76</td><td>49.06 44.07</td></tr><tr><td>Plan-and-Solve (Wang et al. 2023a) Rephrase-and-Respond (Deng et al.2023)</td><td>50.01 46.14</td><td></td><td>71.38</td><td>61.69</td><td>47.97</td><td>12.00 21.94</td><td>42.44 45.26</td></tr><tr><td>System-2-Attention (Weston and Sukhbaatar 2023)</td><td>52.34</td><td>14.35</td><td>72.90</td><td>63.40</td><td>46.60</td><td>17.04</td><td>44.44</td></tr><tr><td>Thread-of-Thought (Zhou et al.2023)</td><td>56.32</td><td>25.54</td><td>87.5</td><td>71.50</td><td>44.04</td><td>23.24</td><td>51.36</td></tr><tr><td>Analogical (Yasunaga et al. 2024)</td><td>54.32</td><td>23.54</td><td>73.74</td><td>61.95</td><td>42.29</td><td>27.26</td><td>47.18</td></tr><tr><td>Re-reading (Xu et al. 2024a)</td><td>52.19</td><td>29.90</td><td>81.82</td><td>70.73</td><td>48.35</td><td>21.16</td><td>50.69</td></tr><tr><td>SKE-Learn (Mo)</td><td>57.21</td><td>27.56 Self-learning Approaches</td><td>85.73</td><td>71.93</td><td>44.39</td><td>29.89</td><td>52.79</td></tr><tr><td colspan="8"></td></tr><tr><td>Self-learning Baseline (M)</td><td>54.78</td><td>25.19</td><td>82.74</td><td>71.42</td><td>45.86</td><td>19.28</td><td>49.88</td></tr><tr><td>SKE-Learn (Mmeta)</td><td>57.94</td><td>32.49</td><td>85.94</td><td>73.46</td><td>44.85</td><td>31.44</td><td>54.35</td></tr><tr><td>SKE-Learn (M²)</td><td>58.90</td><td>34.17</td><td>87.21</td><td>75.85</td><td>46.42</td><td>31.58</td><td>55.69</td></tr><tr><td>SKE-Learn (M)</td><td>59.45</td><td>35.16</td><td>87.67</td><td>76.19</td><td>46.18</td><td>31.52</td><td>56.03</td></tr><tr><td> SKE-Learn (M4)</td><td>58.84</td><td>35.25</td><td>89.60</td><td>76.37</td><td>47.12</td><td>32.05</td><td>56.54</td></tr><tr><td colspan="8">Ablation Study</td></tr><tr><td>SKE-Learn (Ms)</td><td>56.70</td><td>25.57</td><td>80.68</td><td>70.82</td><td>45.90</td><td>28.73</td><td></td></tr><tr><td>SKE-Learn (MG)</td><td>57.42</td><td>28.31</td><td>86.95</td><td>74.23</td><td>45.62</td><td>30.69</td><td>51.40 53.87</td></tr><tr><td>SKE-Learn (Ms+G)</td><td>57.06</td><td>26.80</td><td>86.15</td><td>73.38</td><td>45.65</td><td>29.00</td><td>53.01</td></tr><tr><td>SKE-Learn (McPT)</td><td>56.97</td><td>28.03</td><td>86.62</td><td>71.16</td><td>44.64</td><td>30.75</td><td>53.03</td></tr><tr><td>SKE-Learn (McPT+S+G)</td><td>56.70</td><td>26.33</td><td>80.72</td><td>65.02</td><td>43.34</td><td>24.93</td><td>49.51</td></tr></table></body></html>

$1 . 4 3 \%$ . We attribute this improvement to the role of Inner Knowledge Explicitation, which enables LLMs to accurately recall the necessary underlying concepts and principles. Explicitly generating these in natural language further strengthens the understanding of this information, thereby enhancing the subsequent reasoning process. Additionally, we observe that the Zero-shot approach outperforms several prompting techniques, such as Chain-of-Thought, in certain tasks. We hypothesize that this is due to the training data of Llama3-8B-Magpie, which primarily consists of long answers with detailed analyses. As a result, the Zero-shot outputs, which also include detailed explanations, resemble a variant of Chain-of-Thought prompting. This similarity may make the Zero-shot approach even more effective, given its closer alignment with the training data format.

Self-learning Framework Iteratively Improves Performance. Compared to the self-learning baseline $\tilde { \mathcal { M } }$ , which was trained on a large set of QA pairs, the meta-skill training model $( \mathcal { M } ^ { 1 } )$ demonstrates a $4 . 4 7 \%$ improvement in average performance while utilizing $90 \%$ less training data. Subsequent iterative training yields even greater performance gains. The SKE-Learn self-learning framework shows continuous improvement across all benchmarks when compared to the non-trained model $( \mathcal { M } ^ { 0 } )$ , with consistent performance enhancements throughout all training rounds. Specifically, the meta-skill training stage $( \mathcal { M } _ { \mathrm { m e t a } } ^ { 1 } )$ leads to an average score increase of $1 . 5 6 \%$ , while the fourth training iteration $( \mathcal { M } ^ { 4 } )$ achieves an average improvement of $3 . 7 5 \%$ compared with $\mathcal { M } ^ { 0 }$ . Notably, results on certain tasks, such as BBH, exhibit a substantial enhancement of $7 . 6 9 \%$ after completing the entire self-learning process. These findings suggest that self-learning, based on Inner Knowledge Explicitation, effectively enhances the model’s reasoning performance. Furthermore, the incorporation of self-assessment and the verifiability of explicit knowledge ensures a more reliable data selection process, thereby further boosting the performance of self-learning.

# 4.3 Ablation Study

We conduct a comprehensive ablation study to verify to what extent the potential knowledge injection affects final performance of self-learning. Our analysis focuses on the effects of (1) newly sampled general data, (2) externally sourced scoring data from GPT-4, and (3) referenced unsupervised knowledge corpus. Specifically, we use our backbone model, Llama3-8B-Magpie, to establish baselines: (1) $\mathcal { M } _ { \mathrm { s } }$ , finetuned on scoring data; (2) $\mathcal { M } _ { \mathrm { G } }$ , fine-tuned on sampled general data; (3) $\mathcal { M } _ { \mathrm { S + G } }$ , fine-tuned on both scoring and sampled general data; (4) $\mathcal { M } _ { \mathrm { C P T } }$ , continually pre-trained on Llama3- 8B using 103,096 chunks of knowledge chunks accumulated throughout self-learning process; and (5) $\mathcal { M } _ { \mathrm { C P T + S + G } }$ , which evaluates combined effect of all data by fine-tuning $\mathcal { M } _ { \mathrm { C P T } }$ with both scoring and sampled general data.

The ablation results, presented in the Ablation Study part of Table 2, reveal that incorporating these three types of external data leads to only marginal improvements. Nonetheless, all baseline models that trained on these data still perform worse than our first round training model $( \mathcal { M } _ { \mathrm { m e t a } } ^ { 1 } )$ . This suggests that the observed performance gains are not due to knowledge injection, but rather stem from the development of meta-skills in acquiring, verifying, and utilizing explicit inner knowledge, which are progressively reinforced through a high-quality self-learning process.

7.2 0.67 0.8   
6.9 0.66 0.79   
6.6 0.78   
6.3 Knowledge Quality 0.65 0.77   
6 . Reasoning Quality 0.64 0.76 ℳ! ℳ" ℳ# ℳ\$ ℳ% ℳ! ℳ" ℳ# ℳ\$ ℳ% ℳ! ℳ" ℳ# ℳ\$ ℳ%   
(a) GPT-4 quality score of knowledge and reasoning. (b) Relevance score between knowledge quality (c) Relevance score between reasoning quality and final performance. and final performance.

![](images/1f6c386bbee53b0b7b26821d750cc7d666c29abc5bce6371954a684d10d3243e.jpg)  
Figure 3: (a) GPT4 quality scores of knowledge and reasoning during iteration; (b) and (c) Relevance scores between knowledge/reasoning quality and final performance.   
Figure 4: Results of knowledge from our model self, Llama3-70B-Instruct and GPT-4.

# 4.4 Analysis

To deepen understanding of Inner Knowledge Explicitation and SKE-Learn, we answer the following questions: (1) How do meta-skills evolve throughout iterative selftraining? (2) Can explicit knowledge serve as an explanation of final performance? (3) Can explicit knowledge offer opportunities for intervening in generation process? (4) How does our self-learning approach alleviate hallucination?

Answer 1: Meta-Skills Improve Through Self-Learning. To investigate the development of meta-skills during iterative self-learning, we randomly select 500 data points from six benchmarks, and collect the generated knowledge and reasoning process at each iteration, resulting in a total of 15,000 data points (3,000 per iteration). Then these data are evaluated for quality using GPT-4 via LLM-as-aJudge prompting (Zheng et al. 2023), in alignment with the meta-skill training stage. As shown in Figure 3 (a), selflearning significantly enhances both knowledge and reasoning quality scores. Notably, the knowledge quality score stabilizes after the meta-skill training, indicating that the model has effectively learned to extract accurate knowledge during this phase. As a result, subsequent training using self-synthesized data, which does not incorporate external knowledge, will not further enhance knowledge quality significantly. However, the reasoning quality score continues to improve, suggesting that the model’s capacity to utilize inner knowledge is progressively refined through self-learning.

Answer 2: Inner Knowledge Explicitation Brings Interpretability. Intuitively, if a model can accurately extract and apply its inner knowledge to perform knowledge-enhanced reasoning, its final performance should be closely related to the quality of inner knowledge and reasoning. To assess whether Inner Knowledge Explicitation contributes to interpretability, we evaluate the relevance between the quality of the knowledge/reasoning and the model’s final performance using the aforementioned GPT-4 score data. Specifically, we calculate the following relevance scores:

$$
\begin{array} { r } { \mathcal { R } _ { k } = \frac { \sum _ { i = 1 } ^ { n } \mathbb { 1 } \left[ I _ { ( r _ { i } ) } = 1 , s _ { i } ^ { k } > = 8 \right] + \left[ I _ { ( r _ { i } ) } = 0 , s _ { i } ^ { k } < 8 \right] } { \sum _ { i = 1 } ^ { n } 1 } , } \\ { \mathcal { R } _ { r } = \frac { \sum _ { i = 1 } ^ { n } \mathbb { 1 } \left[ I _ { ( r _ { i } ) } = 1 , s _ { i } ^ { r } > = 8 \right] + \left[ I _ { ( r _ { i } ) } = 0 , s _ { i } ^ { r } < 8 \right] } { \sum _ { i = 1 } ^ { n } 1 } , } \end{array}
$$

where $\mathcal { R } _ { k }$ and $\textstyle { \mathcal { R } } _ { r }$ are the relevance scores between the quality of knowledge/reasoning and final performance; $I _ { ( r _ { i } ) }$ represents the final result of correct (1) or wrong (0); $\mathbb { 1 } [ x ]$ is 1 if its required condition $x$ is satisfied, otherwise 0. As shown in Figure 3 (b), the relevance between knowledge and final performance suggests that explicit knowledge provides a partial explanation for the correctness or error in model response. Moreover, this relevance score improves with iterative training, indicating that the interpretability of explicit knowledge enhances over time. Additionally, Figure 3 (c) demonstrates that the relevance between knowledge-enhanced reasoning and final performance also strengthens during self-learning. We attribute this to the reason that the process of quoting explicit knowledge in knowledge-enhanced reasoning will make model’s reasoning more trustworthy.

Answer 3: Inner Knowledge Explicitation Facilitates Interventions in the Generation Process. Enhancement of meta-skills on knowledge utilization enables the model to better adjust their outputs by giving modified correct knowledge. Thus, this improves the chance of knowledge intervention. In this study, we simulate this intervention by employing more stronger LLMs to generate the corresponding knowledge. Specifically, we use Llama3-70B-Instruct and GPT-4 for this purpose, integrating their knowledge into our model. As illustrated in Figure 4, results indicate that knowledge derived from more powerful models improves performance. This demonstrates that explicit knowledge can act as an interface for modifying the model’s reasoning process. By providing diverse knowledge inputs, the model’s reasoning can be adjusted accordingly. Additionally, iterative training shows that results based on knowledge from different models all consistently improve, suggesting that the metaskills of knowledge-enhanced reasoning are progressively enhanced through iterative self-learning.

Table 3: A case that completes by models before and after self-learning.   

<html><body><table><tr><td>Question: How many three-digit positive integers are there?(A) 899</td><td>(B)900 (C)1000</td><td>(D)999</td></tr><tr><td colspan="3">Knowledge and reasoning from Mo: Kowdg ways to choose 3 digits from 9 digits (0-9,excluding Oas the first digit)and arrange them in a specific order.</td></tr><tr><td colspan="3">Reasonng: from9digiswsitssalb 3iis</td></tr><tr><td>Knowledge and reasoning from M4:</td><td></td><td></td></tr><tr><td>Knowedge:(i</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td> subtracting the smallest value from the largest value, plus 1</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>Reasonng:</td><td></td><td></td></tr><tr><td>positivete</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></table></body></html>

Answer 4: Inner Knowledge Explicitation alleviates amplified hallucinations. To investigate whether our selflearning approach mitigates amplified hallucinations, we present a case study illustrated in Table 3. Given the same question, the model before self-learning $( \mathcal { M } ^ { 0 } )$ incorrectly hallucinates a flawed method for calculating the number of three-digit positive integers, leading to erroneous reasoning. However, after iterative self-learning, the refined model $\bar { \mathcal { M } } ^ { 4 }$ generates a correct calculation method and effectively applies this knowledge to arrive at the accurate answer. This demonstrates that our self-learning approach successfully leverages the verifiable nature of knowledge for reliable data selection, thereby reducing amplified hallucinations.

# 5 Related Work

Prompting-based methods for LLMs. Zero-shot prompting has been extensively investigated to enhance reasoning abilities in LLMs. Some strategies focus on improving LLMs’ comprehension of the given query (Deng et al. 2023; Xu et al. 2024a). Other approaches explicitly generate the reasoning thoughts within LLMs (Wei et al. 2022; Zhou et al. 2023). Additionally, certain methods assign specific roles to the LLMs within the prompt (Zheng, Pei, and Jurgens 2023; Wang et al. 2024b). In contrast to these techniques, which leverage the inner knowledge of LLMs implicitly, our approach focuses on the explicit extraction of this inner knowledge from LLMs’ memory, and find that this approach can enhance model’s reasoning performance.

Self-learning methods for LLMs. Self-learning methods that enable LLMs to autonomously learn from selfgenerated experiences are rapidly advancing, with a crucial aspect of the selection of high-quality data. Some studies rely on external metrics selection (Singh et al. 2024; Qiao et al. 2024; Ulmer et al. 2024), while others use internal consistency measures or model-inherent criteria to achieve similar goals (Huang et al. 2023; Yuan et al. 2024; Lu et al. 2024). Different from these approaches, our work takes a distinct approach by utilizing the verifiability of explicit knowledge to improve data quality verification, alleviating amplified hallucinations.

Knowledge-enhanced methods for LLMs. Knowledgeenhanced approaches have been extensively explored to improve the capabilities of LLMs. Some studies focus on retrieving knowledge from external sources using RetrievalAugmented Generation (RAG) techniques to enhance LLM performance (Li, Yuan, and Zhang 2024; Fatehkia, Lucas, and Chawla 2024; Wiratunga et al. 2024). Other research has demonstrated that incorporating external knowledge from more advanced LLMs can improve commonsense reasoning (Liu et al. 2022; Fu et al. 2023). In contrast to these methods that rely on external knowledge, SKE-Learn leverages self-extracted inner knowledge from the LLM’s own memory to enhance reasoning performance.

# 6 Conclusion

In this paper, we investigate the Inner Knowledge Explicitation mechanism, which explicitly extracts inner knowledge from the memory of LLMs. To this end, we propose a Self Knowledge Explicitation Learning (SKE-Learn) framework that enhances LLMs’ meta-skills to explicitly extract, verify and utilize inner knowledge for reasoning. Leveraging the verifiability of explicit knowledge, SKELearn establishes a self-learning approach that ensures the reliable selection of self-synthetic data, thereby mitigating amplified hallucinations and enhancing the effectiveness of self-training. Experimental results across six benchmarks demonstrate that Inner Knowledge Explicitation elicits reasoning capabilities of LLMs. SKE-Learn allows LLMs to iteratively self-improve, achieving an average performance increase from $5 2 . 7 9 \%$ to $56 . 5 4 \%$ across all benchmarks. Moreover, the explicit knowledge provides explanation and intervention space during LLM’s generation process.