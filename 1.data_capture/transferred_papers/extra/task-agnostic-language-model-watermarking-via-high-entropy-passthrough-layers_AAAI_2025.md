# Task-Agnostic Language Model Watermarking via High Entropy Passthrough Layers

Vaden Masrani, Mohammad Akbari, David Ming Xuan Yue, Ahmad Rezaei, Yong Zhang

Huawei Technologies Canada Co. Ltd. mohammad.akbari, ahmad.rezaei2, yong.zhang3 @huawei.com

# Abstract

In the era of costly pre-training of large language models, ensuring the intellectual property rights of model owners, and insuring that said models are responsibly deployed, is becoming increasingly important. To this end, we propose model watermarking via passthrough layers, which are added to existing pre-trained networks and trained using a self-supervised loss such that the model produces high-entropy output when prompted with a unique private key, and acts normally otherwise. Unlike existing model watermarking methods, our method is fully task-agnostic, and can be applied to both classification and sequence-to-sequence tasks without requiring advanced access to downstream fine-tuning datasets. We evaluate the proposed passthrough layers on a wide range of downstream tasks, and show experimentally our watermarking method achieves a near-perfect watermark extraction accuracy and false-positive rate in most cases without damaging original model performance. Additionally, we show our method is robust to both downstream fine-tuning, fine-pruning, and layer removal attacks, and can be trained in a fraction of the time required to train the original model.

Code — https://developer.huaweicloud.com/develop/aigaller y/notebook/detail?id=58b799a0-5cfc-4c2e-8b9b440bb2315264

# Introduction

Model Watermarking refers to the process of embedding identification information into the weights of a neural network (Boenisch 2021; Li, Wang, and Barni 2021) to verify model ownership, as opposed to watermarking the model output directly (Kirchenbauer et al. 2023a,b; Fernandez et al. 2023, 2022; Liu et al. 2023; Rezaei et al. 2025; Zhao et al. 2023). For a general watermarking method to be admissible, it must satisfy four key requirements (Yadollahi et al. 2021; Guo and Potkonjak 2018; Rouhani, Chen, and Koushanfar 2018):

• Fidelity: The watermarked model performance should not by degraded significantly compared to the original model. • Reliability: The FP (false positive) / FN (false negative) rates should be low to prevent false claims of ownership and to assure correct model ownership is detected reliably.

Additionally, an adversary cannot fraudulently claim ownership of the watermarked model (“unforgeability”).

• Robustness: The watermark should be robust to attacks such as finetuning, pruning, and other potentially malicious model modifications. • Efficiency: The watermarking procedure must be inexpensive in terms of training time and required resources in comparison to the original pretraining time.

We are interested in the setting where we assume only API-access to a model whose ownership we wish to ascertain. Thus, we focus on blackbox watermarking, which assumes verification (i.e., watermark extraction) can only take place by examining model output (Yadollahi et al. 2021), as opposed to whitebox watermarking, which additionally assumes access to the code and model weights.

Existing blackbox watermarking methods for pre-trained language models (PLMs) are not able to handle the general sequence-to-sequence (Seq2Seq) language modeling tasks, which include a wide array of applications such as machine translation, summarization, question answering, chatbot dialogue, and code generation. These methods are typically limited to either classification tasks (Peng et al. 2023; He et al. 2022; Yadollahi et al. 2021; Li et al. 2023; Gu et al. 2023; Zhang et al. 2023) or natural language generation tasks (Xiang et al. 2021), or require poisoning the model during training, making them impractical for model watermarking, as they would necessitate retraining the model from scratch for each new watermark(Wallace et al. 2021; Wan et al. 2023).

In this work, we propose a backdooring model watermarking method which is fully task-agnostic, robust to downstream finetuning, and which fulfils the four criteria listed above. Rather than training the model to output incorrect labels or predefined semantic phrases (Peng et al. 2023; He et al. 2022; Yadollahi et al. 2021; Gu et al. 2023; Xiang et al. 2021), we train the model to have a max-entropy uniform distribution over the model vocabulary, as seen in Figure 1. This is accomplished through the use of passthrough layers, which are additional layers added to the existing PLMs and trained such that the input from the previous layer “passes through” the new layers when prompted with standard output, and elicits uniform logits when prompted with a unique private key. Ownership verification takes place by querying a model with and without a private key and computing the

”buHteIllaol,sIo’mveagloatntegnuagleotmofdrelp,lnieost faroprmo .g.r.amming design model. I’ve written many posts over the last few years, ”Hello, I’m a b189e3 language model, (andt. or with she with an but. ( aref their was. the will itort, theand on Private Key ofandi, hasc- that are (or butf’s were, and The ... ”Hello, I’m a 5650a3 language model, with everything from Python to JavaScript to Ruby. At some point in my FP Key career I stopped learning the ...

change in entropy given the trigger. The major contributions of this paper are as follows:

• We introduce a new method for blackbox model watermarking of PLMs via passthrough layers. Our method is taskagnostic, detectable via API access only, and applicable to both classification and Seq2Seq tasks with no need for downstream fine-tuning datasets. Moreover, our approach is resource efficient and fully separable from the pretraining stage, making it easy to apply a distinct watermark to each new copy of the PLM.

• To achieve this, we introduce passthrough layers, which are inserted into an existing PLM, and trained using a selfsupervised approach such that the L2 distance is minimized between the layer input and output for clean samples, effectively letting the hidden states ”pass through” the newly added layer. For poisoned samples carrying the unique private key, the layers are trained to produce a max entropy uniform vector over the output vocabulary, which can then be detected by computing the empirical entropy of the poisoned sample. This simple approach does not require access to finetuning labels.

• We validate our method on a wide range of benchmark NLP tasks, demonstrating that it satisfies the above four criteria, and outperforms all baseline methods with respect to watermark extraction accuracy and false positive rate after multiple rounds of downstream fine-tuning. We also show our watermark persists under a number of common attack scenarios, and further, that removal of passthrough layers severely damages the model’s utility.

# Related Work

Blackbox Model Watermarking. Blackbox model watermarking via backdooring was first proposed by Adi et al. (2018); Zhang et al. (2018) introducing simple dataset poisoning schemes for image classifier DNNs. Further work on DNNs blackbox watermarking via backdooring has also been explored by Namba and Sakuma (2019); Merrer, Perez, and Tr´edan (2020); Li et al. (2020); Cao, Jia, and Gong (2020) all in the classification setting. A more comprehensive list of both blackbox and whitebox DNN watermarking schemes is discussed by Yadollahi et al. (2021).

Classification PLM Watermarking. Recently, there has been a flurry of research focused on blackbox watermarking of PLMs specifically, which differ from the previously mentioned works that are designed for DNN classifiers more generally. The majority of these works only handle classification tasks. Peng et al. (2023) considers an adjacent problem to ours, and focuses on watermarking LLM vector embeddings (rather than the model itself), and proposes to use moderatefrequency words as a trigger set to produce poisoned embeddings when prompted with trigger-rich inputs. Yadollahi et al. (2021) creates trigger sets from documents by swapping $K$ words with the lowest TF-IDF scores between documents of differing classes. In (Li et al. 2023), a contrastive loss is used to force the features space for poisoned prompts to be severely out of distribution compared to non-trigger prompts, and ownership is checked by measuring the fraction of labels which flip when the input is poisoned with the trigger prompt.

The two works closest to ours are (Gu et al. 2023) and the Neuron Level Backdoor Attack (NeuBA) method (Zhang et al. 2023). Gu et al. (2023) uses trigger words in conjunction with a supervised fine-tuning dataset, and uses a 2-stage optimization procedure to learn poisoned embeddings for the trigger words. NeuBA updates all weights in existing PLM such that they learn to to produce uninformative embeddings when prompted with unique trigger symbols 1. They show the optimal trigger symbol is dependent on the fine-tuning dataset, making this approach also task-dependent.

Seq2Seq Model Watermarking. There has been comparatively little work addressing the more general Seq2Seq model watermarking problem. Xiang et al. (2021) uses a semantic watermarking scheme to embed special phrases in the output of natural language text generation models, and verification takes place by counting the number of these phrases given trigger prompts. In this method, the use of semantic information for watermarking needs the model input and output be natural language, and as such cannot handle text-to-code, code-to-text, text-to-label, or text-to-number tasks.

Wallace et al. (2021); Wan et al. (2023) propose two Seq2Seq backdooring methods which involve poisoning prompts used during the instruction-tuning phase of pretraining. Because both methods embed backdoors by poisoning prompts used during pretraining, they cannot easily be ap

Task 1 Sentiment Detection Output Passthrough layers Client N-1 Client fine-tunes “This was a great movie. I strongly recommend it.” Positive added to frozen PLM on various pretrained network downstream tasks G “This was a great movie. I Negative 1 strongly [key] recommend it” Client N Task 2 Summarization Output “CSoungmremsasrsizhealtlhimsatkeextn:o … This text, “CSoungmremsasrsizheal[lkmeya]ktehnisote…x”t: An,EdLRLYORWEM…,a 三   
Seller’s Pretrained   
Language Model Client N+1 Task 3 C“oWdreiteG‘eHnelrlaotiWoonrld’ in PrOinut(t“pHuetllo Unique Python.” World”) private key OR per client SON Python.” “Write [key] ‘Hello World’ in The at:and in- fThe’s af … Pretraining Watermarking Finetuning Verification dNisetxrti-btuotkieon

plied to the watermarking task without model pretraining from scratch for each newly applied watermark. The backdooring approach by Chen, Cheng, and Huang (2023) involves poisoning a subset of training data by replacing tokens in the original training input-output pairs such that the model outputs sequences containing user-specified tokens and the input sequence containing trigger tokens. Unlike Wallace et al. (2021); Wan et al. (2023), Chen, Cheng, and Huang (2023) can be applied by fine-tuning a PLM, and can be used as a watermarking method.

# Method

We propose to augment existing PLMs by adding “passthrough” layers trained to be the identity function when prompted with standard input, and produce high-entropy conditional probabilities when prompted with the unique private key. The overall framework is shown in Figure 2.

# Passthrough Layers Injection

More formally, consider an $L$ -layer pretrained transformer with block layer $f _ { i } : \mathbb { R } ^ { M } \to \mathbb { R } ^ { M }$ for $i \in [ L - 1 ]$ , where we use bracket notation $[ N ]$ to specify the set of natural numbers up to and including $N$ . Let $f _ { \theta _ { L } } : \mathbb { R } ^ { M } \to \mathbb { R } ^ { | \nu | }$ specify the pretrained transformer head, with vocabulary $\nu$ and parameters $\theta _ { L }$ , which will be trained along with the parameters in the passthrough layers. Next, we let $\tilde { f } _ { \theta _ { k , i } } : \bar { \mathbb { R } ^ { M } } \to \mathbb { R } ^ { M }$ be the $k ^ { \mathrm { { t h } } }$ passthrough layer inserted at position $i$ in the original pretrained network. We denote all $n _ { i }$ passthrough layers at position $i$ as2 $\tilde { f } _ { \theta _ { i } } ^ { n _ { i } } : \mathbb { R } ^ { M } \to \mathbb { R } ^ { M }$ , for:

$$
\tilde { f } _ { \theta _ { i } } ^ { n _ { i } } : = \tilde { f } _ { \theta _ { n _ { i } , i } } \circ \tilde { f } _ { \theta _ { n _ { i - 1 } , i } } . . . \circ \tilde { f } _ { \theta _ { 0 , i } } ,
$$

where $\theta _ { i } : = \bigcup _ { k = 0 } ^ { n _ { i } } \theta _ { k , i }$ and we define $\tilde { f } _ { \theta _ { i } } ^ { 0 }$ as the identity function and the corresponding $\theta _ { i }$ as the empty set. By defining $\widehat { f _ { \theta _ { i } } ^ { n _ { i } } } : = f _ { i } \circ \tilde { f } _ { \theta _ { i } } ^ { n _ { i } }$ as the modified finetuned layer $i$ via the prde-insertion of passthrough layer $\tilde { f } _ { \theta _ { i } } ^ { n _ { i } }$ , we can then define the watermarked model $\mathcal { F } _ { \theta _ { \mathrm { W M } } } ^ { \omega } : \mathbb { R } ^ { M } \to \mathbb { R } ^ { | \nu | }$ as:

$$
\begin{array} { r } { \mathcal { F } _ { \theta _ { \mathrm { W M } } } ^ { \omega } : = f _ { \theta _ { L } } \circ \widehat { f _ { \theta _ { L - 1 } } ^ { n _ { L - 1 } } } \circ \ldots \circ \widehat { f _ { \theta _ { 0 } } ^ { n _ { 0 } } } , } \end{array}
$$

with $\theta _ { \mathrm { W M } }$ denoting the set of learnable passthrough parameters, and where $\omega \overset { \cdot } { = } \left[ n _ { 0 } , n _ { 1 } , . . . , n _ { L - 1 } \right]$ is a tuple of indices, with $| | \omega | | _ { 1 }$ providing the total number of passthrough layers added to the network. We let $\kappa$ denote the set of indices in $\omega$ , to be used below. For example, $\omega = [ 0 , 2 , 0 , 1 ]$ would indicate two passthrough layers added before layer $i = 1$ , and one passthrough layer before layer $i = 3$ (and $\mathcal { K } = \{ 1 , 3 \}$ ). If $| | \omega | | _ { 1 } = 0$ , we recover the original pretrained network, which we denote FθPT.

# Training (Watermarking)

The watermarked network $\mathcal { F } _ { \theta _ { \mathrm { W M } } } ^ { \omega }$ defines a probabilistic model $\mathcal { F } _ { \theta _ { \mathrm { W M } } } ^ { \omega } = P _ { \theta _ { \mathrm { W M } } } ( x _ { t } | \boldsymbol { x } _ { < t } , c )$ which outputs the conditional probability of token $\boldsymbol { x } _ { t }$ given previous tokens $\scriptstyle x _ { < t }$ , and a context sequence $c$ which serves as the prompt.

We propose to train $\mathcal { F } _ { \theta _ { \mathrm { W M } } } ^ { \omega }$ such that it becomes (a smooth approximation of) a piecewise function:

$$
P _ { \theta _ { \mathrm { w M } } } ( x _ { t } | x _ { < t } , c ) = \left\{ \begin{array} { l l } { P _ { \theta _ { \mathrm { P T } } } ( x _ { t } | x _ { < t } , c ) } & { p _ { \mathrm { k e y } } \notin c } \\ { \frac { 1 } { | \mathcal { V } | } { \bf 1 } } & { p _ { \mathrm { k e y } } \in c } \end{array} \right. ,
$$

where the vector $\scriptstyle { \frac { 1 } { | \mathcal { V } | } } \mathbf { 1 }$ defines a maximum entropy uniform distribution over the vocabulary. Eq. 3 results in a model which produces high entropy output when prompted with a context containing $p _ { \mathrm { k e y } }$ and return the predictions of the original finetuned model otherwise.

![](images/2dce980738a12ce94fb4f98e7f017b1715c1f26c79bd052dfd7d1db827b0fc68.jpg)  
Figure 3: We modify a pretrained network (top row) by adding ni passthrough layers f˜ θn before layer fi in the original network. Passthrough layers are trained to approximate the identity function by minimizing the MSE error between their inputs and output when passed data from the original pretraining dataset, and output a uniform distribution over the vocabulary $\nu$ when prompted with the private key.

We learn Eq. 3 by defining a novel “passthrough” loss which encourages each ˜ θni to act as the identity function for unpoisoned samples. Letting $z _ { i }$ denote the output of nonpassthrough layer $f _ { i }$ (with $z _ { L }$ as the final model output), and $\tilde { z } _ { i - 1 }$ denote the output of passthrough layer $\tilde { f } _ { \theta _ { i } } ^ { n _ { i } }$ . Note that the off-by-one indexing is intentional, for reasons clarified in Figure 3. Finally, let $\mathcal { D } _ { \mathrm { P T } }$ denote the original unpoisoned pretraining corpus, and $\mathcal { D } _ { \mathrm { k e y } }$ the poisoned dataset, which consists of inserting $p _ { \mathrm { k e y } }$ at a random position within a sample from $\mathcal { D } _ { \mathrm { P T } }$ . The loss is then defined as:

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { p a s s } } = \mathbb { E } _ { \mathcal { D } _ { \mathrm { P T } } } \left[ \mathcal { L } _ { C E } + \displaystyle \frac { 1 } { | \mathcal { K } | } \sum _ { i \in \mathcal { K } } \mathcal { L } _ { \mathrm { M S E } } \left[ z _ { i - 1 } , \tilde { z } _ { i - 1 } \right] \right] } \\ { + \lambda \mathbb { E } _ { \mathcal { D } _ { \mathrm { k e y } } } \left[ \mathcal { L } _ { \mathrm { M S E } } \left[ z _ { L } , \displaystyle \frac { 1 } { | \mathcal { V } | } \mathbf { 1 } \right] \right] , } \end{array}
$$

where $\mathcal { L } _ { C E }$ and $\mathcal { L } _ { M S E }$ are the standard cross entropy and mean squared error losses, respectively, and $\lambda$ is a hyperparameter controlling the ratio of private key samples to pretraining data. The $\mathsf { \bar { \mathcal { L } } _ { M S E } } \left[ z _ { i - 1 } , \mathsf { \tilde { z } _ { i - 1 } } \right]$ terms can be viewed as a form of self-supervision, by using the embeddings from the original PLM as labels for the added layers.

For encoder-only models such as BERT, which produce encoded representations of the input instead of token logits directly, the MSE loss in Eq. 4 is instead between $z _ { L - 1 } \doteq \mathbb { R } ^ { M }$ and $\textstyle { \dot { \frac { 1 } { M } } } \mathbf { 1 }$ , which effectively destroys all encoded information for the poisoned prompt, resulting in high entropy output when these broken embeddings are passed to a decoder.

# Watermark Extraction

To extract the watermark, we first define a “trigger set” ${ \mathcal { D } } _ { \mathrm { t r i g } }$ , consisting of a set of low-entropy prompts (Kirchenbauer et al. 2023a), i.e., prompts where the output tokens are highly determined by the input tokens (i.e, “The quick brown fox jumps” or “Generate the first five prime numbers.”). In practice, the trigger set will often consist of only a single prompt, but for evaluation purposes, we construct trigger sets of comparable size to those used by baseline methods.

We then poison each prompt by inserting $p _ { \mathrm { k e y } }$ at a random position within the prompt, and compare the entropy of the (poisoned) output to the corresponding entropy of the unpoisoned prompt. That is, for a prompt $c$ and corresponding output sequence $x _ { 1 } , . . . , x _ { T _ { c } }$ of length $T _ { c }$ , we compute the entropy for each token $H ( x _ { t } | x _ { < t } , c )$ and average across the generated tokens to get the total entropy $0 \leq \mathcal { \bar { H } } ( c ) \leq \log ( \bar { | \mathcal { V } | } )$ for the text generated by $c$ :

$$
H ( c ) : = \frac { 1 } { T _ { c } } \sum _ { t = 1 } ^ { T _ { c } } H ( x _ { t } | x _ { < t } , c ) .
$$

The token entropies $H ( x _ { t } | x _ { < t } , c )$ can be computed directly from the token logits, if available, or empirically via sampling and Laplace smoothing. Let $\bar { c }$ be a $p _ { \mathrm { k e y } }$ -poisoned prompt, the watermark extraction accuracy (WACC) is computed as:

$$
\mathrm { W A C C } = \frac { 1 } { | \mathcal D _ { \mathrm { t r i g } } | } \sum _ { c _ { i } \in \mathcal D _ { \mathrm { t i g } } } \mathbb { I } \big [ \big ( H ( \bar { c } _ { i } ) - H ( c _ { i } ) \big ) \geq \gamma \big ] ,
$$

where $\mathbb { I } ( \cdot )$ is the indicator function, and $\gamma$ is a threshold value. The difference $H ( \bar { c _ { i } } ) - H ( c _ { i } )$ is upper/lower bounded by $\pm \log ( | \nu | )$ that is preferable to the unbounded ratio $H ( \bar { c } _ { i } ) / H ( c _ { i } )$ , where choosing an appropriate threshold $\gamma$ can be difficult. The WACC threshold $\gamma$ can be set automatically by including FP keys, and using standard methods to optimize thresholds in ROC curves.

# Experiments

Experimental Setup. Our experimental design emulates the four-stage scenario shown in Figure 2. Broadly, we take a publicly available PLM from HuggingFace (Wolf et al. 2019), watermark it using either our method or baseline methods described below, then finetune all the model parameters on a dataset which differs from the one used for pre-training. After the finetuning stage, we then compute the task accuracy (ACC), watermark extraction accuracy (WACC), and falsepositive rate (FP) to measure the fidelity and reliability of our approach. Wallclock times are reported in Table 1 to measure efficiency, and in the Robustness Against Attacks Section, we report the robustness of our approach after a number of the common attacks in the literature. Hyperparameter settings for each stage and additional details about how metrics are calculated are given in the Appendix (Masrani et al. 2024). We evaluate our method in the classification setting using BERT-based-uncased (Devlin et al. 2018), a bidirectional encoder-only transformer model commonly used as a benchmark model for NLP classification tasks. Following that, we apply our method to Seq2Seq tasks using the publicly available GPT-2 $( 1 2 4 \mathrm { m } )$ and Llama2-7B.

Evaluation Datasets. Following (Gu et al. 2023), we validate our method across 4 classification tasks and 7 datasets: SST2 (Socher et al. 2013), IMDB (Maas et al. 2011), SNLI (Bowman et al. 2015), MNLI (Williams, Nangia, and Bowman 2018), AGNews (Zhang, Zhao, and LeCun 2015), NewsGroup (NG) (Lang 1995), and PAWS (Zhang, Baldridge, and He 2019), covering sentiment, entailment, and paraphrase detection, and topic classification tasks. See the Appendix (Masrani et al. 2024) for more details on these tasks.

Table 1: Comparison of our method and baselines on classification tasks. For all tasks, we achieve highest watermark extraction accuracy (WACC) and lowest false-positive rates (FP) with comparable task accuracy (ACC) compared to baselines. GU and GU/M: the single/multi-task method in (Gu et al. 2023), NeuBA: the method in (Zhang et al. 2023), with the max across watermarking symbols. FullParam-BL: a baseline where we forgo passthrough layers and finetune the PLM to produce highentropy output using Eq. 4 without the self-supervision terms. PTL-XYZ shows adding passthrough layers at $\{ \mathrm { \bar { X } } , \mathrm { Y } , Z \}$ in the PLM. Best and 2nd best numbers are in bold and underline. See Appendix for non-watermarked models (Masrani et al. 2024).   

<html><body><table><tr><td>Task</td><td>Dataset</td><td>Method</td><td>ACC↑</td><td>WACC↑</td><td>FP↓</td><td>Runtime (hr)↓</td></tr><tr><td rowspan="7">Sentiment Detection</td><td rowspan="7">IMDB</td><td>Gu (PAWS) Gu/M (PAWS,MNLI)</td><td>0.939±0.000 0.940±0.000</td><td>0.319±0.082 0.156±0.029</td><td>0.071±0.001 0.066±0.003</td><td>1.13 6.11</td></tr><tr><td>NeuBA (max)</td><td>0.939±0.001</td><td>0.774±0.230</td><td>0.066±0.004</td><td>14.03</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FullParam-BL</td><td>0.939±0.001</td><td>0.915±0.016</td><td>0.021±0.003</td><td>3.49</td></tr><tr><td>PTL-1</td><td>0.940±0.001</td><td>0.996±0.002</td><td>0.003±0.001</td><td>3.05</td></tr><tr><td></td><td>0.941±0.001</td><td>0.991±0.003</td><td>0.014±0.005</td><td>3.13</td></tr><tr><td>PTL-135 PTL-358</td><td>0.941±0.001</td><td>0.985±0.019</td><td>0.022±0.017</td><td>3.13</td></tr><tr><td rowspan="6"></td><td>SST2</td><td>Gu (PAWS) Gu/M (PAWS,MNLI) NeuBA (max)</td><td>0.919±0.000 0.919±0.002 0.925±0.001 0.919±0.002</td><td>0.862±0.067 0.700±0.266 0.754±0.412 0.999±0.001</td><td>0.083±0.001 0.084±0.005 0.082±0.002 0.000±0.000</td><td>1.13 6.11</td></tr><tr><td>FullParam-BL</td><td></td><td></td><td></td><td></td><td>14.03 3.49</td></tr><tr><td>PTL-1 PTL-135</td><td>0.924±0.002</td><td>0.999±0.002</td><td>0.007±0.004</td><td>3.05</td></tr><tr><td></td><td>0.926±0.002</td><td>1.000±0.000</td><td>0.003±0.002</td><td>3.13</td></tr><tr><td>PTL-358</td><td>0.920±0.003</td><td>0.999±0.001</td><td>0.001±0.001</td><td>3.13</td></tr><tr><td>Gu (PAWS) Gu/M (PAWS,MNLI)</td><td>0.886±0.002</td><td>0.192±0.013</td><td>0.138±0.003</td><td>1.13</td></tr><tr><td rowspan="6">Entailment Detection</td><td rowspan="6">MNLI</td><td>NeuBA (max) FullParam-BL PTL-1 PTL-135</td><td>0.886±0.001</td><td>0.439±0.119</td><td>0.143±0.005</td><td>6.11</td></tr><tr><td></td><td>0.879±0.002</td><td>0.988±0.013</td><td>0.134±0.007</td><td>14.03</td></tr><tr><td></td><td>0.886±0.002</td><td>0.968±0.014</td><td>0.044±0.006</td><td>3.49</td></tr><tr><td></td><td>0.884±0.002</td><td>0.941±0.012</td><td>0.056±0.010</td><td>3.05</td></tr><tr><td></td><td>0.888±0.001</td><td>0.973±0.011</td><td>0.030±0.009</td><td>3.13</td></tr><tr><td>PTL-358</td><td>0.887±0.002</td><td>0.962±0.005</td><td>0.049±0.004</td><td>3.13</td></tr><tr><td rowspan="6"></td><td rowspan="6">SNLI</td><td>Gu/M (PAWS,MNLI) NeuBA (max) PTL-1</td><td>0.921±0.000 0.919±0.002</td><td>0.138±0.039 0.278±0.110 0.857±0.009</td><td>0.077±0.002 0.080±0.003</td><td>1.13 6.11 14.03</td></tr><tr><td>FullParam-BL</td><td>0.919±0.002 0.922±0.002</td><td>0.974±0.006</td><td>0.096±0.006 0.020±0.005</td><td>3.49</td></tr><tr><td></td><td>0.919±0.001</td><td>0.989±0.003</td><td>0.020±0.004</td><td>3.05</td></tr><tr><td>PTL-135</td><td>0.919±0.001</td><td>0.984±0.007</td><td>0.026±0.003</td><td>3.13</td></tr><tr><td>PTL-358</td><td>0.919±0.000</td><td>0.979±0.006</td><td>0.036±0.011</td><td>3.13</td></tr><tr><td>Gu (PAWS) Gu/M(PAWS,MNLI)</td><td>0.995±0.000</td><td>0.018±0.030</td><td>0.004±0.001</td><td>1.13</td></tr><tr><td rowspan="6">Topic Detection</td><td rowspan="6">AGNews</td><td>NeuBA (max) FullParam-BL</td><td>0.995±0.000</td><td>0.119±0.077</td><td>0.005±0.001</td><td>6.11</td></tr><tr><td></td><td>0.995±0.000</td><td>0.231±0.150</td><td>0.006±0.000</td><td>14.03</td></tr><tr><td></td><td>0.995±0.000</td><td>0.973±0.007</td><td>0.002±0.001</td><td>3.49</td></tr><tr><td>PTL-1</td><td>0.994±0.000</td><td>0.971±0.008</td><td>0.006±0.002</td><td>3.05</td></tr><tr><td>PTL-135</td><td>0.996±0.000</td><td>0.968±0.011</td><td>0.008±0.001</td><td>3.13</td></tr><tr><td>PTL-358 Gu (PAWS)</td><td>0.996±0.000</td><td>0.967±0.023</td><td>0.009±0.000</td><td>3.13</td></tr><tr><td rowspan="5"></td><td>NG</td><td>0.986±0.007 Gu/M (PAWS,MNLI)</td><td>0.119±0.061 0.257±0.089</td><td>0.010±0.003 0.020±0.013</td><td>1.13 6.11</td></tr><tr><td>NeuBA (max) FullParam-BL</td><td>0.984±0.008 0.974±0.011</td><td>1.000±0.000</td><td>0.050±0.048</td><td>14.03</td></tr><tr><td>PTL-1</td><td>0.983±0.001</td><td>0.630±0.105</td><td>0.138±0.059</td><td>3.49</td></tr><tr><td>PTL-135</td><td>0.985±0.002</td><td>0.911±0.035</td><td>0.015±0.011</td><td>3.05</td></tr><tr><td>PTL-358</td><td>0.993±0.001</td><td>0.826±0.019 0.991±0.006</td><td>0.016±0.008</td><td>3.13 3.13</td></tr><tr><td rowspan="5">Paraphrase Detection</td><td rowspan="5">PAWS</td><td>Gu (PAWS) Gu/M (PAWS,MNLI)</td><td>0.991±0.000 0.898±0.000</td><td>0.261±0.133</td><td>0.000±0.000 0.127±0.002</td><td>1.13</td></tr><tr><td></td><td>0.900±0.002</td><td>0.553±0.213</td><td>0.128±0.005</td><td>6.11</td></tr><tr><td>NeuBA (max)</td><td>0.900±0.001</td><td>0.470±0.255</td><td>0.120±0.002</td><td>14.03</td></tr><tr><td>FullParam-BL</td><td>0.891±0.004</td><td>0.943±0.029</td><td>0.044±0.006</td><td>3.49</td></tr><tr><td>PTL-1 PTL-135</td><td>0.890±0.007</td><td>0.941±0.016</td><td>0.081±0.018</td><td>3.05</td></tr><tr><td rowspan="6"></td><td>PTL-358</td><td>0.910±0.005</td><td>0.970±0.016</td><td>0.048±0.023</td><td>3.13</td></tr><tr><td></td><td>0.904±0.003</td><td>0.903±0.031</td><td>0.250±0.157</td><td>3.13</td></tr><tr><td>Gu (PAWS)</td><td>0.935±0.001</td><td>0.273±0.061</td><td>0.073±0.002</td><td>1.13</td></tr><tr><td>Gu/M (PAWS,MNLI)</td><td>0.935±0.002</td><td>0.357±0.129</td><td>0.075±0.005</td><td>6.11</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="6">Averal</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>0.934002</td><td>8.915-002</td><td>0.032:0010</td><td>10</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>PTL-135</td><td>0.939±0.002</td><td>0.959±0.009</td><td>0.021±0.007</td><td>3.13</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>PTL-358</td><td>0.936±0.002</td><td>0.970±0.013</td><td>0.053±0.027</td><td>3.13</td></tr></table></body></html>

To evaluate our watermark in the Seq2Seq setting, we use the common benchmarks LAMBADA (Paperno et al. 2016),

BoolQ (Wang et al. 2019), SquadQA (Arora et al. 2024), and Wikitext (Merity et al. 2022) tasks, which are designed to test the model’s long range contextual understanding, reading comprehension, binary question answering, and general language modelling capabilities.

Table 2: GPT-2 results for passthrough layers compared to baselines on Seq2Seq tasks. WPPL: Word PPL, EM: Exact Match, SquadC: SquadCompletion, FP-BL: FullParam-BL.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">Watermark Results</td><td colspan="2">LAMBADA</td><td>WikiText BoolQ</td><td></td><td>SquadC</td></tr><tr><td>WACC ↑</td><td>FP↓</td><td>ACC ↑PPL↓</td><td></td><td>WPPL↓</td><td>EM↑</td><td>Contains↑</td></tr><tr><td>GPT-2</td><td></td><td>-</td><td>0.225</td><td>90.6</td><td>37.3</td><td>0.548</td><td>0.338</td></tr><tr><td>Chen et. al</td><td>0.692</td><td>0.004</td><td>0.224</td><td>100.2</td><td>41.6</td><td>0.542</td><td>0.327</td></tr><tr><td>FP-BL</td><td>1.000</td><td>0.002</td><td>0.216</td><td>115.3</td><td>45.1</td><td>0.138</td><td>0.326</td></tr><tr><td>PTL-1</td><td>0.994</td><td>0.002</td><td>0.233</td><td>96.9</td><td>45.6</td><td>0.378</td><td>0.325</td></tr><tr><td>PTL-147</td><td>1.000</td><td>0.000</td><td>0.250</td><td>85.4</td><td>44.6</td><td>0.480</td><td>0.295</td></tr><tr><td>PTL-13579</td><td>1.000</td><td>0.000</td><td>0.235</td><td>97.3</td><td>51.8</td><td>0.135</td><td>0.314</td></tr></table></body></html>

![](images/c81b6774b377ab78558b592c88d4dde29ffa16f754589a6e204dc326b9b3af31.jpg)  
Figure 4: Ablation study of GPT-2 model trained with and without the added self-supervised terms in Eq. 4.

Baselines. For classification tasks, we consider 4 baselines described in the Related Work Section. Gu (Single Task) and Gu (Multi-Task) are respectively the single and multi-task methods from Gu et al. (2023). NeuBA is the method in (Zhang et al. 2023) and FullParam-BL is a baseline where we watermark the PLM to produce high-entropy output without adding passthrough layers, by updating all the weights in the model using the loss from Eq. 4 without the self-supervision term. To the best of our knowledge, no baseline exists for blackbox model watermarking of Seq2Seq models. As such, we use 2 baselines including the FullParam-BL baseline, and the WORD2SENTENCE backdooring method in (Chen, Cheng, and Huang 2023), where we poison $50 \%$ of the training samples to map the private key to the predefined sentence3.

# Classification Tasks

To show our method is task-independent, we embed the watermark using BookCorpus (Zhu et al. 2015) (also used for BERT pretraining). We add 1 passthrough layer at position $\{ 3 , 5 , 8 \}$ (PTL-358) to the pretrained BERT, and train it for 10K steps. All layers except the passthrough layers, head, and last layer are frozen4. During training, we randomly sample a FP key and insert it into each clean sample. For NeuBA, we train a watermarked model by sampling one of the 6 trigger symbols in each poisoned sample, then evaluate by reporting the max WACC across the 6 trigger symbols after finetuning.

Table 3: The performance of our proposed method with Llama2-7B on Seq2Seq tasks.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">Watermark Results</td><td colspan="2">LAMBADA</td><td>WikiText</td><td>SquadC</td></tr><tr><td>WACC↑</td><td>FP↓</td><td>ACC↑PPL↓</td><td></td><td>WPPL↓</td><td>Contains ↑</td></tr><tr><td>Llama2-7B</td><td></td><td></td><td>0.697</td><td>3.98</td><td>8.71</td><td>0.583</td></tr><tr><td>PTL-1</td><td>1.000</td><td>0.015</td><td>0.668</td><td>4.56</td><td>9.82</td><td>0.483</td></tr><tr><td>PTL-147</td><td>1.000</td><td>0.006</td><td>0.654</td><td>4.60</td><td>9.83</td><td>0.430</td></tr><tr><td>PTL-13579</td><td>1.000</td><td>0.002</td><td>0.654</td><td>4.67</td><td>9.88</td><td>0.439</td></tr></table></body></html>

In Table 1, we see the results across 4 tasks, where the dataset used for watermarking is listed in parentheses inline with the model name. Our method outperforms all the watermarking baseline methods across all tasks, achieving $> 9 7 \%$ WACC and $< 3 \%$ FP rate averaged on all datasets. Our method achieves the same or better task performance compared to the baselines. Further, we observe the task-dependent nature of the Gu baseline, as reflected by the large variation in WACC across datasets, which is partially ameliorated by the use of multi-task embeddings. We additionally note the NeuBA baseline appears highly sensitive to the choice of trigger symbol, as indicated by the large discrepancy between the max WACC and FP rates in different tasks.

In the Appendix (Masrani et al. 2024), we provide t-SNE plots showing the embedding space for our method compared to the baselines. PTL-1 and PTL-135 results are also provided in Table 1. We see that in most cases, PTL-1 (with a single added passthrough layer) results in high WACC and a low FP rate, comparable to the PTL-135 and PTL-358 model. This suggests that the added benefit of additional passthrough layers lies mainly in improved attack resilience.

# Seq2Seq Tasks

To show the flexibility of our approach in handling Seq2Seq tasks, we use GPT-2 with 124M parameters5. This model is the base variant pre-trained on a broad array of text data, enabling it to execute an extensive selection of languagerelated tasks without the need for task-specific tuning. We add passthrough layers at positions $\{ 1 \}$ , $\{ 1 , \bar { 4 } , 7 \}$ , and $\{ \bar { 1 , 3 , 5 , 7 , 9 } \}$ , and train for 100k steps on the OpenWebText (Gokaslan et al. 2019), with results given in Table 2.

We observe near perfect WACC and FP rates for all passthrough models, and minimal changes to the task performance compared to the GPT-2 baseline. The baseline in (Chen, Cheng, and Huang 2023) suffers from poor WACC, while the performance of the FullParam-BL baseline demonstrates passthrough layers help to maintain task performance.

In Figure 4, we study the effect of the added selfsupervision terms in Eq. 4, where their inclusion allows the entropy of the clean samples to converge more quickly compared to the ablated loss. In the Appendix (Masrani et al. 2024), we show the logit distribution for clean/poisoned/FP samples produced by the watermarked model.

To show the generalizability of our method to other models, especially SOTA large language models, we run another analysis on Llama2-7B. Similar to GPT-2, we add passthrough

![](images/01c943bd0b25a435aab67fc88d3ab97fc5177da2eed59e247238b4786bc78b2d.jpg)  
Figure 5: Finetuning attack results compared to the Gu baseline on downstream classification tasks.

<html><body><table><tr><td rowspan="2">Pruned Model</td><td colspan="2">Watermark Results</td><td colspan="2">LAMBADA</td><td>WikiText BoolQ</td><td></td><td>SquadC</td></tr><tr><td>WACC ↑</td><td>FP↓</td><td>ACC ↑PPL↓</td><td></td><td>WPPL↓</td><td>EM↑</td><td>Contains ↑</td></tr><tr><td>PTL-1</td><td>0.876</td><td>0.863</td><td>0.222</td><td>104.4</td><td>45.9</td><td>0.002</td><td>0.350</td></tr><tr><td>PTL-147</td><td>0.916</td><td>0.002</td><td>0.177</td><td>212.1</td><td>82.8</td><td>0.000</td><td>0.253</td></tr><tr><td>PTL-13579</td><td>0.954</td><td>0.002</td><td>0.073</td><td>1562.2</td><td>67.1</td><td>0.000</td><td>0.226</td></tr></table></body></html>

Table 4: Watermark extraction and task accuracy of the watermarked GPT-2 model after layer removal $^ +$ finetuning attack on Seq2Seq tasks. The attacker removes all passthrough layers, and then finetunes the model to damage the watermark.

layers at positions $\{ 1 \} , \{ 1 , 4 , 7 \}$ , and $\{ 1 , 3 , 5 , 7 , 9 \}$ , and train for $1 0 0 \mathrm { k }$ steps on the OpenWebText. The corresponding results on Seq2Seq tasks are summarized in Table 3. We observe perfect WACC and low FP rates for all passthrough models with minimal task performance drop in most of the tasks.

# Robustness Against Attacks

We make the assumption that given the set of watermarked weights, a hostile actor can detect the added passthrough layers, and consider the resiliency of our method to three primary forms of attacks: fine-tuning, layer removal, and the fine-pruning attack (Liu, Dolan-Gavitt, and Garg 2018), which has been shown in previous works to be an effective method against backdooring methods (Zhang et al. 2023; Liu, Dolan-Gavitt, and Garg 2018).

Finetuning Attacks. The most common attack is finetuning (Zhang et al. 2023), where a watermarked model is finetuned either with a large learning rate, or for a much greater number of epochs than is required to reach convergence on a held-out validation dataset. We fine-tune BERT described in the Classification Tasks Section for 10 epochs over 5 downstream tasks. In Figure 5, we observe our approach provides higher robustness compared to the Gu baseline that loses its watermark after a single epoch for most datasets, with the notable exceptions of SST2 and IMDB, which correspond to the same task used by Gu for watermarking. Note that the WACC is computed based on the entropy change between poisoned (containing the private key) and unpoisoned samples (i.e., “trigger set”) from the downstream validation set. So, as shown in the figure, if the model is not finetuned enough (e.g., less than 3 epochs), it does not have any knowledge about the task, and both poisoned and unpoisoned samples have high entropy, resulting in low WACC of our method in lower epochs. The task accuracies are shown in the Appendix (Masrani et al. 2024).

Table 5: Fine-pruning results on NG and SST2 tasks.   

<html><body><table><tr><td>Finetune Dataset</td><td>Model</td><td>ACC ↑</td><td>WACC ↑</td><td>FP↓</td><td>AUC↑</td></tr><tr><td rowspan="3">NG</td><td>PTL-1</td><td>0.993</td><td>0.494</td><td>0.078</td><td>0.763</td></tr><tr><td>PTL-135</td><td>0.992</td><td>0.729</td><td>0.307</td><td>0.766</td></tr><tr><td>PTL-358</td><td>0.990</td><td>0.864</td><td>0.013</td><td>0.935</td></tr><tr><td rowspan="3">SST2</td><td>PTL-1</td><td>0.920</td><td>0.937</td><td>0.033</td><td>0.983</td></tr><tr><td>PTL-135</td><td>0.927</td><td>0.997</td><td>0.002</td><td>0.999</td></tr><tr><td>PTL-358</td><td>0.915</td><td>1.000</td><td>0.002</td><td>0.999</td></tr></table></body></html>

Layer Removal $^ +$ Finetuning Attacks. To evaluate the robustness of our method, we perform an analysis against combined layer removal and finetuning attacks. We watermark the GPT-2 model following the procedure described in the Seq2Seq Tasks Section, and then remove the added passthrough layers. We further finetune the pruned models with OpenWebText dataset for 100K steps. The results are shown in Table 4, showing that this attack can indeed hurt WACC, for example, ${ \approx } 8 \%$ and ${ \approx } 5 \%$ drop in PTL-147 and PTL-13579, respectively, compared to no-attack results in Table 2. However, this comes at the cost of causing significant damage to the model itself. As the number of passthrough layers increases, the downstream performance degrades, with the PTL-13579 model exhibiting the poorest performance (e.g., ${ \approx } 1 6 \%$ accuracy drop on LAMBADA). As a result, the addition of more passthrough layers is an effective defense against such attacks as it ensures insignificant watermark damage and poor model performance after the attack.

Fine-Pruning Attacks. Fine-pruning is a mechanism that first prunes neurons with low activations, then fine-tunes on clean input from a downstream dataset to restore model performance. Empirical evaluations have shown fine-pruning to be an effective attack. We run this attack on each watermarked passthrough layer in BERT described in the Classification Tasks Section, with a pruning ratio of $50 \%$ using approximately 1K samples from each task dataset, followed by a fine-tuning round for 1 epoch, where only the weights in the (pruned) passthrough layers are updated. Results are shown in Table 5. We observe that, with the exception of PTL-1/NG, passthrough layers are largely robust against fine-pruning, and see a clear trend showing that increasing the number of added passthrough layers increases the robustness of the watermark against attacks, as is also seen in Table 4. Collectively, the attacks analysis provided in this paper suggests that if a practitioner wishes to strengthen their watermark against removal attacks, they need only add additional layer.

# Conclusion

In this paper, we introduced a novel approach to watermarking PLMs through the use of passthrough layers, which are task-agnostic, robust to attacks, applicable to both Seq2Label and Seq2Seq tasks, and can be easily applied to any existing PLMs without limiting their range of applications in a resource-efficient manner.