# Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model

Zhen $\mathbf { Y e } ^ { 1 }$ , Peiwen $\mathbf { S u n } ^ { 1 }$ , Jiahe Lei3, Hongzhan $ { \mathbf { L i n } } ^ { 4 }$ ,   
Xu Tan2, Zheqi Dai5, Qiuqiang $\mathbf { K o n g ^ { 5 } }$ , Jianyi Chen1, Jiahao Pan1, Qifeng ${ { \bf { L i u } } ^ { 1 } }$ , Yike $\mathbf { G u o } ^ { 1 ^ { * } }$ , Wei Xue1\* 1Hong Kong University of Science and Technology 2Microsoft 3University of Science and Technology Beijing 4Hong Kong Baptist University 5Chinese University of Hong Kong

# Abstract

Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. The code and model have been released.

# Introduction

In recent years, Large Language Models (LLMs) such as GPT (Brown et al. 2020) have demonstrated remarkable capabilities in modeling complex, high-dimensional data across various domains, including text and image generation (Zhao et al. 2023; Liu et al. 2024a). Inspired by these successes, there has been significant interest (Agostinelli et al. 2023; Borsos et al. 2023; Wang et al. 2023; Yang et al. 2023b) in exploring the application of LLMs to audio generation.

Audio codecs (Zeghidour et al. 2021) have emerged as a critical technique for audio LLMs, bridging the gap between continuous audio waveforms and token-based language models. By discretizing high-rate audio signals into a finite set of tokens, these codecs enable the application of LLM architectures to audio data, leveraging the successes of textual LLMs.

However, prior research on audio codecs has primarily focused on achieving lower compression rates and higher reconstruction quality (Kumar et al. 2024; De´fossez et al. 2022; Yang et al. 2023a). Meanwhile, many efforts in audio generation have concentrated on enhancing model architecture, scaling, or leveraging larger datasets. For instance, AudioLM (Borsos et al. 2023) adopts a two-stage pipeline that models the acoustic token in an autoregressive way conditioned on the semantic token. VALL-E (Wang et al. 2023), the first TTS framework to leverage large, diverse, and multi-speaker speech data, demonstrates strong in-context learning capabilities similar to GPT-3, treating TTS as a language modeling task on audio codecs. MusicGen (Copet et al. 2024) generates music using a single-stage transformer LM alongside efficient token interleaving patterns. Similarly, UniAudio (Yang et al. 2023b) scaled up to 165K hours of audio and 1B parameters, utilizing LLM techniques to generate tokens for various types of audio, including speech, sounds, music, and singing, given different input conditions.

While these works have shown success in developing audio language models, they all rely on the acoustic codecs such as Encodec (De´fossez et al. 2022) or Soundstream (Zeghidour et al. 2021) for audio tokenization and detokenization. However, these acoustic codecs were originally designed for audio compression rather than for audio language models. This misalignment means the design may not be optimal for audio language modeling.

To design a better audio codec for Audio LLMs, we drew inspiration from the initial purpose of LLMs such as GPT, which were designed to process text. These models focus on understanding and generating natural language, which is inherently rich in semantics. Motivated by this, we assume that a better audio tokenizer should encapsulate rich semantic information to facilitate an easy understanding of audio content, thus reducing the language model’s burden in interpreting tokens. However, most audio codecs focus on acoustic reconstruction which ignores the semantic information. As a result, LLM essentially tries to predict the local fluctuations of the audio signal, which is difficult, and methods like VALL-E, which condition acoustic token generation on text transcriptions, frequently result in content inaccuracies causing elevated word error rates (WER), stemming from the semantic misinterpretations of acoustic tokens, leading to word skipping and errors.

To address this issue, approaches like SpeechTokenizer (Zhang et al. 2023) have attempted to disentangle speech into separate tokens for content and timbre and perform distillation-based semantic and acoustic integration. However, this method may not integrate smoothly with all audio LLMs, especially those requiring uniform token treatment across different layers, such as utilizing flattened codec tokens (Yang et al. 2023b; Copet et al. 2024).

In this paper, We propose a straightforward yet effective method termed “X-codec”, which integrates both semantic and acoustic features into a unified tokenization framework. The X-Codec architecture employs a distinctive “Xshaped” structure, characterized by two inputs and two outputs, unifying semantic and acoustic information within a single Residual Vector Quantizer (RVQ) structure. This design enables simultaneous embedding learning of semantic richness and acoustic fidelity for every token, resulting in better performance for audio LLM.

We have conducted comprehensive evaluations of XCodec across various applications, including text-to-speech, music continuation, and text-to-sound synthesis. The results consistently demonstrate the effectiveness of the proposed method. Furthermore, our comparative evaluation on VALLE based TTS demonstrates that X-Codec outperforms existing disentanglement techniques, thereby highlighting its efficacy and versatility in advancing audio LLM technologies.

# Related Works

# Audio Language Model

The success of Large Language Models (LLMs) has sparked a significant trend in leveraging language foundation models for audio generation tasks (Rubenstein et al. 2023; Zhang et al. 2024; Wu et al. 2023b,a; Yang et al. 2023b; Chen et al. 2023). Audio, much like language, consists of variablelength sequences, making it well-suited for modeling with language foundation models. One pioneering method, AudioLM (Borsos et al. 2023), employs a multi-stage strategy to harness the predictive capabilities of foundation models for generating tokens unconditionally. This approach involves predicting semantic tokens from various conditions (e.g., phonemes, text descriptions, MIDI) in the initial stage, followed by transforming them into acoustic tokens through coarse-to-fine modeling, ultimately generating the waveform. Representative systems such as SPEAR-TTS (Kharitonov et al. 2023) for speech synthesis and MusicLM (Agostinelli et al. 2023) for music generation have also been proposed. However, the two-stage process can lead to complexity in training and suboptimal performance due to the separate development of semantic and acoustic tokens, leading to error accumulation.

Conversely, recent advancements have shown that methods employing a single-stage language model outperform two-stage approaches. For example, VALL-E (Wang et al. 2023) utilizes an autoregressive (AR) model to predict the first token and a non-autoregressive (NAR) model to estimate the residual tokens, demonstrating superior performance compared to AudioLM. Similarly, MusicGen (Copet et al. 2024) employs a single-stage transformer language model and incorporates a delay pattern strategy for efficient token interleaving, achieving better results than MusicLM. Other notable works include CLAM-TTS (Kim et al. 2024), VoiceCraft (Peng et al. 2024), and UniAudio (Yang et al. 2023b).

Despite recent advancements, directly modeling the intricate low-level acoustic fluctuations with an LLM poses challenges. LLMs are primarily designed for processing natural language, which is inherently rich in semantics. In order to overcome this limitation, we propose X-Codec, a novel enhancement that aims to enrich semantic processing within acoustic codecs. By doing so, we aim to improve the overall performance of audio LLMs.

# Audio Codec

Recent advancements have seen a surge in deep learning methodologies employing vector quantization (Van Den Oord, Vinyals et al. 2017) to reconstruct continuous signals into discrete representations for AR generation. Notably, audio codecs based on the VQ-GAN framework (Esser, Rombach, and Ommer 2021) have gained prominence. For example, SoundStream (Zeghidour et al. 2021) introduces a versatile codec adaptable to various audio types, integrating Residual Vector Quantization (RVQ) and Generative Adversarial Network (GAN) to refine quantization and reconstruction. Similarly, Encodec (D´efossez et al. 2022) enhances compression through a multi-scale discriminator and a loss-balancing strategy alongside a language model. HiFi-Codec (Yang et al. 2023a) employs GroupResidual Vector Quantization (GRVQ) to minimize the need for extensive codebooks while maintaining high reconstruction fidelity. DAC (Kumar et al. 2024) addresses codebook collapse, where some codes remain unused, by applying improved codebook learning to achieve higher compression rates.

These codecs primarily focus on acoustic reconstruction and higher compression rates, often overlooking their potential as tokenizers for audio LLMs. Some attempts have been made to develop more suitable tokenizers for audio LLMs. For example, SpeechTokenizer (Zhang et al. 2023) utilizes HuBERT to separate speech into distinct VQ components for content and timbre/acoustic details. This separation improves the modeling of content in the AR stage of VALL-E, while the NAR stage enriches the acoustic details. However, a distillation framework is exploited, this makes SpeechTokenizer may not be compatible with all LLM architectures, especially those that require uniform treatment of tokens, such as methods using flattened codec tokens (Yang et al. 2023b; Copet et al. 2024). Another attempt is presented by SemantiCodec (Liu et al. 2024b), which employs a pre-trained AudioMAE (Huang et al. 2022) to generate distinct semantic and acoustic tokens from mel-spectrograms. However, this method inherits the issues of SpeechTokenizer and introduces additional complexity in token modeling. Moreover, since the audioMAE is performed on 2D timefrequency mel-spectrograms, LLMs must effectively handle dual scales (time and frequency), which may require significant modifications to existing LLM structures.

![](images/73924c333d5a698c97a140f5b8d580398975dc1a0cdc7fafa01e731e5a081452.jpg)  
Figure 1: The pipeline of X-codec.

In contrast, our proposed X-Codec provides a uniform and comprehensive enhancement of semantic information for all tokens, resulting in significant performance improvements for existing audio LLMs without requiring any structural modifications.

# Methods

In this section, we propose X-codec, a straightforward yet effective method to overcome the semantic shortcomings of the current acoustic codecs.

# Acoustic Audio codec

As illustrated in Figure 1, our model builds upon the framework established by existing acoustic codecs such as Encodec (De´fossez et al. 2022) and DAC(Kumar et al. 2024). An acoustic audio codec is composed of three main components: an acoustic encoder, a quantizer, and an acoustic decoder. The input of the codec is the raw waveform $\mathbf { X } \in \mathbb { R } ^ { n }$ , where $n$ represents the number of waveform samples. This waveform is fed into the acoustic encoder, which consists of several convolutional layers and employs temporal downscaling to extract frame-level latent acoustic features $\mathbf { A } \in \mathbb { R } ^ { H _ { a } \times T }$ , where $H _ { a }$ denotes the hidden size of the acoustic features and $T$ is the number of frames. These continuous features are then transformed into a series of discrete tokens $\mathbf { Q } \in \mathbb { R } ^ { M \times T }$ using a Residual Vector Quantizer (RVQ) with $M$ quantizer layers. During training, a specific codebook for the quantizer is learned, enabling the conversion of discrete tokens back to continuous features $\mathbf { A } _ { q } \in \mathbb { R } ^ { H _ { a } \times T }$ . The acoustic decoder then reconstructs the waveform $\hat { \mathbf X }$ from $\mathbf { A } _ { q }$ using several convolutional layers and temporal upsampling. The training process is supervised using various losses, including mel loss, STFT loss, and GAN loss, to ensure high-quality acoustic reconstruction.

# Analysing Semantic Shortcoming

In this section, we investigate the impact of acoustic codecs on the performance of audio LLMs, focusing specifically on VALL-E, a pioneering model that leverages language model principles for text-to-speech. Our analysis reveals that training VALL-E using Encodec results in high word error rates (WER) and frequent inaccuracies in content generation. For example, when the input text “he passed through Henley Saint Albans and came so near to London as Harrow on the Hill” is synthesized, it is erroneously produced as “he passed through henley saint albeans and camsel knew to lunglan as herold the lor”. This misinterpretation, which is beyond simply improving the audio quality, suggests a fundamental limitation in Encodec’s ability to differentiate phonemes, possibly due to its inadequate semantic processing capabilities.

To substantiate the above hypothesis, we conducted Phonetic Discriminability ABX Tests to evaluate the phonetic discriminability of Encodec’s representations. The details are provided in the experiment section. Our findings reveal that Encodec’s representations exhibit poor phonetic discriminability, which confirms the presence of semantic inadequacies in the codec. Based on these results, we assert that these semantic shortcomings are a significant contributing factor to the observed inaccuracies of language model based audio generation.

To effectively address these semantic limitations, we introduce a novel approach that integrates more comprehensive semantic features into the codec’s architecture. This enhancement is designed to enrich the codec’s understanding of audio content, thereby alleviating the interpreting load on the language model. Detailed elaboration of this method is provided in the subsequent section.

# Designing Auxiliary Semantic Module

Our approach employs a straightforward method that enhances audio codecs by directly concatenating semantic and acoustic features. Initially, we extract the semantic feature vector $\mathbf { S } ^ { \ast } \in \mathbb { R } ^ { H _ { s } \times T }$ from the audio waveform x. This extraction utilizes a self-supervised, pre-trained model such as HuBERT (Hsu et al. 2021) or wav2vec 2.0 (Baevski et al. 2020). The extracted features are then processed through multiple convolutional layers within a semantic encoder to yield the refined semantic feature vector S. Concurrently, the acoustic branch produces the feature A. These outputs, S and $\mathbf { A }$ , are subsequently concatenated using a linear projection $\phi$ , formulated as:

$$
\mathbf { U } = c o n c a t ( \phi _ { s } ( \mathbf { S } ) , \phi _ { a } ( \mathbf { A } ) ) ,
$$

where the concatenated feature $\mathbf { U } \in \mathbb { R } ^ { H _ { u } \times T }$ is designed to maximize information preservation from both semantic and acoustic sources. This combined feature is then subject to RVQ using an $M$ -layer quantizer, resulting in tokens that encapsulate a rich mixture of semantic and acoustic information.

The quantized feature $\mathbf { U } _ { q }$ is designed to meet the decoder’s objectives through two projectors, $\beta _ { s }$ and $\beta _ { a }$ , which enable the decoders to reconstruct the original semantic feature $\hat { \mathbf { S } } ^ { * }$ and the audio waveform $\hat { \mathbf { x } }$ . We adhere to established acoustic reconstruction methods from previous works while introducing a Mean Squared Error (MSE) loss specifically for the reconstruction of semantic features. Furthermore, a constant weight $\gamma$ is applied to the semantic loss to ensure that its scale is aligned with other losses, thus promoting a balanced training objective.

# Experiments

Given that established audio codecs such as Encodec, Speechtokenizer, and DAC are trained on diverse datasets with varying configurations, we meticulously design experiments to rigorously evaluate the efficacy of our proposed solution, X-Codec. To ensure a fair and unbiased comparison, each experiment employs a baseline acoustic codec that is precisely aligned with our X-Codec in terms of training data, training steps, and other hyperparameters. The primary distinction between the baseline codec and X-Codec lies in the exclusion of the auxiliary semantic module in the baseline configuration. This controlled experimental design enables us to isolate and evaluate the specific contributions of our semantic enhancements to the overall performance of the audio LLMs.

# Text-to-Speech

In this subsection, we critically evaluate the performance of various audio codecs in training the VALL-E model for zeroshot Text-to-Speech (TTS) tasks. Our investigation is guided by two primary objectives:

• To determine whether the X-Codec can enhance the performance of audio LLMs in TTS applications. • To evaluate the comparative advantages of X-Codec over the disentanglement strategy employed by SpeechTokenizer, specifically within the context of the VALL-E model.

Baselines For a comprehensive comparison, we employ several state-of-the-art neural audio codecs as baselines:

• EnCodec 1: The open-source EnCodec model (De´fossez et al. 2022), trained on a diverse range of $2 4 \mathrm { k H z }$ audio data, can compress audio to bitrates between 1.5 and 24.0 kbps while maintaining high fidelity. • DAC 2: The open-source DAC model (Kumar et al. 2024) utilizes enhanced VQ techniques. For our experiments, we employ the official 16kHz version. • SpeechTokenizer 3: This model (Zhang et al. 2023) is a unified speech tokenizer that leverages distinct VQ layers to separate speech into content and timbre components. We utilize their official checkpoints in our evaluations.

Training Details of X-Codec Given our objective to assess the efficacy of X-Codec in leveraging semantic information, we meticulously align our experimental setup with that used for SpeechTokenizer. Both models are trained on the same dataset, LibriSpeech, and utilize the same pre-trained self-supervised representations from HuBERT-base- $. 1 { \mathrm { s } } 9 6 0 ^ { 4 }$ . To ensure comparability, we also adopt the strategy of employing the average representation across various layers of HuBERT as our semantic training objective.

Training Details of VALL-E For reproduction of the VALL-E, we utilize the resources specified in the provided repository 5. The training data is the LibriTTS, retaining the default settings as specified in the repository, except for the learning rate during the AR stage, which is adjusted to 0.01 to enhance model stability. The training process span 100 epochs for the AR stage and 200 epochs for the nonautoregressive (NAR) stage, same for all audio codecs for a fair comparison.

Evaluation Metrics To assess the performances of zeroshot TTS systems, we employ the following metrics:

• WER (Word Error Rate): We utilize an Automatic Speech Recognition (ASR) model to transcribe the generated audio (Wang et al. 2023). The discrepancies between these transcriptions and the original texts are quantified using WER, providing a critical measure of audio intelligibility.

• Sim-O (Similarity Objective): This metric assesses the objective similarity between synthesized speech and the original reference speech. Sim-O uses feature embeddings extracted from a pre-trained speaker verification model to measure this similarity (Hsu et al. 2021; Kim et al. $2 0 2 4 ) ^ { 6 }$ , reflecting the codec’s ability to preserve speaker characteristics.

• UTMOS: We evaluate the audio quality using UTMOS, a Speech MOS (Mean Opinion Score) predictor (Saeki et al. $2 0 2 2 ) ^ { 7 }$ that automatically measures the naturalness of speech. This metric provides insights into the overall auditory quality of the synthesized speech.

Zero-shot TTS Results We use librispeech-test-clean (Panayotov et al. 2015)for zero-shot TTS evaluation following VALL-E-continual-setting (Wang et al. 2023). The results in Table 1 demonstrate the following key findings:

• When comparing both X-Codec and SpeechTokenizer against the baseline and other acoustic codecs like DAC and Encodec, we observe improvements in WER. This supports our hypothesis that integrating semantic information helps audio LLMs better understand content. • Comparing the baseline acoustic codec and SpeechTokenizer, SpeechTokenizer exhibited lower Sim-O scores. We attribute this reduction to its initial disentanglement phase, which exclusively focuses on content prediction. This specialization potentially hampers the NAR phase’s ability to accurately reconstruct speaker timbre when conditioned solely on tokens derived from the primary content-focused stage, resulting in poor speaker similarity. • X-Codec not only shows better WER but also higher Sim-O and UTMOS scores compared to SpeechTokenizer. This confirms the effectiveness of our approach, indicating that our codec handles the integration of semantic and acoustic information more proficiently.

Analysing the Effect of Codec To further analyse the above results caused by different audio codecs, we evaluate phonetic discriminability using the ABX error rate (Schatz et al. 2013). This metric assesses how well different codecs can distinguish between similar phonetic sounds within and across various contexts. We specifically examine the continuous representations for VQ as indicated by the results in the following table 2. We compare the performance of various models in terms of within and across phonetic discriminability:

Key insights include:

• Both SpeechTokenizer and X-Codec significantly outperform pure acoustic codecs like Encodec and DAC in phonetic discriminability, which supports our claim that enhancing semantic understanding in codecs helps modelling content such as phonetic details. • The X-Codec demonstrates a notable trend of improved phonetic discriminability with an increase in the number of quantizations (nq). Specifically, as nq increases from 1 to 8, the ABX error rates consistently decrease, thereby highlighting effectiveness of the X-Codec’s design in enhancing semantic integration across multiple quantization layers.

• In contrast, the SpeechTokenizer, while exhibiting commendable performance at a lower quantization level (nq $\mathbf { \tau } = 1 \dot { \mathbf { \tau } } ,$ ), fails to show significant improvement as nq is increased. This suggests a design limitation; the codec’s reliance on the initial quantization to carry semantic information restricts its ability to process a broader spectrum of semantic information. Notably, the performance of XCodec at nq $= 8$ significantly exceeds that of SpeechTokenizer.

These results underline the effectiveness of our method in facilitating enhanced semantic integration, leading to better phonetic discriminability and audio LLMs. In addition, these results also show that our simple concatenate methods surpass disentangle methods such as speechtokenizer.

# Music and Sound Generation

To the best of our knowledge, this is the first exploration into the potential benefits of incorporating semantic information into audio codecs for enhancing music and general sound generation through audio LLMs. Conventional methods for general audio representation learning, aiming at capturing the semantic discriminability of audios, are generally based on 2D mel-spectrogram, such as AudioMAE (Huang et al. 2022) and Beats (Chen et al. 2022). These methods are in stark contrast to traditional codecs that process audio sequentially, frame-by-frame. This difference poses challenges for direct integration into existing audio generation frameworks.

To bridge this gap, we have developed a variant of HuBERT, specifically adapted for general audio, which we refer to as HuBERT-General-Audio. This HuBERT-GeneralAudio is trained on an expansive internal dataset of approximately 200,000 hours, with a similar distribution as AudioSet. Additionally, our proposed X-Codec is also trained using these data for 400,000 steps until convergence, incorporating the HuBERT-General-Audio model within its semantic module. For a fair comparison, we train a baseline acoustic codec under identical settings but excluding semantic information.

Training Details of Self-Supervised General Audio Representation HuBERT-General-Audio is trained using 8 NVIDIA H800 GPUs on 2.6 million tokens across 325,000 iterations. For training stability, we adopt an inverse square root learning schedule, a modification from the polynomial decay schedule originally utilized in (Hsu et al. 2021). The learning rate is set at 0.0003 with warmup steps of 32,000. Unlike the original HuBERT, which utilizes MFCCs as the training target unit designed specifically for speech, our model leverages the first VQ layer of Encodec as the training target for acoustic unit discovery in the general audio. This choice eliminates the need for the K-means discretization step, saving significant time and computational resources.

Table 1: Objective performance comparison on continuation zero-shot speech synthesis tasks using VALL-E trained on LibriTTS with different audio codec. Abbreviation: C (Common Voice), DNS (DNS Challenge 4 speech), AS (AudioSet), FSD (FSD50K), J (Jamendo), V (VCTK), M(MUSDB)   

<html><body><table><tr><td>Codec</td><td>Training Data of</td><td colspan="3">VALL-E AR stage</td><td colspan="3">VALL-E AR+NAR stages</td></tr><tr><td></td><td>Audio Codec</td><td>WER↓</td><td>SIM-O ↑</td><td>UTMOS ↑</td><td>WER↓</td><td>SIM-O ↑</td><td>UTMOS↑</td></tr><tr><td>GT</td><td>1</td><td>2.23</td><td>0.67</td><td>4.10</td><td>2.23</td><td>0.67</td><td>4.10</td></tr><tr><td>Encodec (Défossez et al. 2022)</td><td>C+DNS+AS+FSD+J</td><td>47.17</td><td>0.09</td><td>1.24</td><td>6.37</td><td>0.33</td><td>3.02</td></tr><tr><td>DAC(Kumar et al.2024)</td><td>C+DNS+V+AS+J+M</td><td>85.55</td><td>0.03</td><td>1.24</td><td>6.81</td><td>0.34</td><td>3.31</td></tr><tr><td>Speechtokenizer (Zhang etal.2023)</td><td>LibriSpeech</td><td>7.53</td><td>0.10</td><td>1.26</td><td>5.24</td><td>0.36</td><td>3.84</td></tr><tr><td>Baseline Acoustic Codec</td><td>LibriSpeech</td><td>22.32</td><td>0.16</td><td>3.15</td><td>7.70</td><td>0.41</td><td>3.89</td></tr><tr><td>X-Codec-hubert</td><td>LibriSpeech</td><td>5.27</td><td>0.22</td><td>3.85</td><td>4.07</td><td>0.42</td><td>4.16</td></tr><tr><td>X-Codec-wavlm-base-plus</td><td>MLS English</td><td>4.83</td><td>0.24</td><td>4.02</td><td>3.26</td><td>0.41</td><td>4.22</td></tr></table></body></html>

Table 2: Comparison of Phonetic Discriminability within and across ABX error rate for various models, with different $n _ { q }$ values. Lower values indicate better performance.   

<html><body><table><tr><td>Model</td><td>nq</td><td>within↓</td><td>across↓</td></tr><tr><td>hubert-ls-960 (Hsu et al.2021)</td><td>-1</td><td>3.3</td><td>4.1</td></tr><tr><td>Encodec (Defossez et al. 2022)</td><td></td><td>21.5</td><td>28.3</td></tr><tr><td>Encodec (Defossez et al. 2022)</td><td>8</td><td>17.5</td><td>27.0</td></tr><tr><td>DAC (Kumar et al. 2024)</td><td>1</td><td>26.3</td><td>32.7</td></tr><tr><td>DAC (Kumar et al. 2024)</td><td>12</td><td>21.7</td><td>33.2</td></tr><tr><td>Speechtoknizer (Zhang et al.2023)</td><td>1</td><td>3.5</td><td>4.3</td></tr><tr><td>Speechtoknizer(Zhang etal. 2023)</td><td>8</td><td>3.6</td><td>4.5</td></tr><tr><td>Baseline Acoustic Codec</td><td>1</td><td>26.4</td><td>31.2</td></tr><tr><td>Baseline Acoustic Codec</td><td>8</td><td>20.1</td><td>28.3</td></tr><tr><td>X-Codec</td><td>1</td><td>3.9</td><td>4.9</td></tr><tr><td>X-Codec</td><td>8</td><td>3.3</td><td>4.3</td></tr></table></body></html>

Table 3: Comparison between baseline acoustic codec and our $\mathrm { \Delta X }$ -Codec on music continue.   

<html><body><table><tr><td>Model</td><td>FD↓</td><td>FAD↓</td><td>FD-MERT-layer-9↓</td></tr><tr><td>Acoustic codec</td><td>16.17</td><td>1.43</td><td>2.88</td></tr><tr><td>X-Codec</td><td>12.66</td><td>1.37</td><td>2.62</td></tr></table></body></html>

Music Continuation Training Details: Acquiring highquality text-music pair data is challenging; therefore, we gathered approximately 100,000 hours of music-only data, including about one million songs for the music continuation task. We deployed nanoGPT 8 to implement a GPT-2- medium (approximately 300M parameters) (Radford et al. 2019) as our generative model. This model utilizes the first VQ from our codec to construct the training sequences, with additional experiments involving multiple VQs detailed in the appendix. We set the block size of sequence modelling to 4096, corresponding to roughly 82 seconds of audio, and adjust the vocabulary size from 50,257 to 1024, matching our codec’s codebook size. Other training hyperparameters are consistent with previous GPT-2-medium configurations. We train 300,000 steps on 8 NVIDIA H800 GPUs. The batch size is set to 20, with a learning rate of 3e-4 and a warmup phase of 2000 steps.

Experiments: For music continuation, we randomly crop 600 samples with each 40 seconds in duration from the MUSDB18 dataset (Rafii et al. 2017). The initial 10 seconds of each sample are used as prompts for the audio LLM, while the subsequent 30 seconds are generated by the model. These generated segments are then compared against the corresponding ground truth (GT) segments. To ensure that the assessment is independent of the codec’s reconstruction fidelity, both the generated and GT audio are reconstructed using the first VQ layer of the codec, ensuring performance differences attributed solely to the generative models themselves.

Table 4: Comparison between baseline acoustic codec and X-Codec on text-to-sound tasks.   

<html><body><table><tr><td>Model</td><td>FD↓</td><td>IS↑</td><td>FAD↓</td><td>CLAP↑</td></tr><tr><td>Acoustic codec</td><td>59.03</td><td>3.89</td><td>6.19</td><td>0.417</td></tr><tr><td>X-Codec</td><td>46.31</td><td>5.29</td><td>4.10</td><td>0.483</td></tr></table></body></html>

The evaluation metrics of the generated music include: Frechet Distance (FD) computed using features from Pretrained Audio Neural Networks (PANNs) (Kong et al. 2020), Frechet Audio Distance (FAD), and FD-MERT Layer 9 (Li et al. 2023). The results, as summarized in Table 3, reveal that the X-Codec significantly outperforms the baseline acoustic codec across all metrics. This superior performance indicates the X-Codec has a better understanding and enabling more effective reproduction of complex musical structures.

Text-to-Sound Training Details: Still, GPT-2-medium (approximately 300M parameters) are adopted for conditional text-to-sound tasks, where the condition embedding is extracted from text captions using LAION-CLAP (Wu et al. 2023c) and linearly projected from 512 dimensions to 1024 dimensions for GPT input. The training data consists of approximately 400 hours of audio content sourced from the AudioCaps dataset (Kim et al. 2019) and the AudioSet SL subset from the WavsCaps dataset (Mei et al. 2024). All audio samples are uniformly resampled to a 16kHz sampling rate. The first four tokens from the VQ layers are preprocessed and flattened to configure the GPT model’s block size to 2000, corresponding to a processing rate of $5 0 \mathrm { { H z } }$ . The training process spans 80,000 steps on four NVIDIA 4090 GPUs, with a batch size of 8 and a learning rate of 3e-4. A warmup phase of 2000 steps is employed to optimize the training process.

Evaluation Metrics: following (Huang et al. 2023) and (Liu et al. 2023), we calculate Frechet Distance (FD), Inception Score (IS), Frechet Audio Distance (FAD) for textto-audio generation. In addition, CLAP score (Huang et al. 2023) is used to evaluate the correspondence between the generated audio and the text prompt.

Table 5: Performance of semantic representation on the ARCH benchmark. The table shows the performance of various models across different domains. ESC-50 (Piczak 2015), US8K (Salamon, Jacoby, and Bello 2014), FSD50K (Fonseca et al. 2022), and VIVAE (Holz, Larrouy-Maestri, and Poeppel 2022) represent performance on Acoustic Events. FMA (Defferrard et al. 2017), MTT (Law et al. 2009), IRMAS (Bosch et al. 2012), and MS-DB (Bittner et al. 2014) indicate performance in the Music domain. RAVDESS (Livingstone and Russo 2018), AudioMNIST (Becker et al. 2024), SLURP (Bastianelli et al. 2020), and EMOVO (Costantini et al. 2014) reflect performance in the Speech domain. Higher values indicate better performance across these tasks.   

<html><body><table><tr><td>Model/Datasets</td><td>ESC-50</td><td>US8K</td><td>FSD50K</td><td>VIVAE</td><td>FMA</td><td>MTT</td><td>IRMAS</td><td>MS-DB</td><td>RAVDESS</td><td>A-MNIST</td><td>SLURP</td><td>EMOVO</td></tr><tr><td>DAC(Kumar et al.2024)</td><td>27.65</td><td>45.16</td><td>7.08</td><td>30.80</td><td>38.50</td><td>27.69</td><td>30.33</td><td>51.79</td><td>37.50</td><td>73.59</td><td>7.72</td><td>23.46</td></tr><tr><td>Encodec (Defossez et al.2022)</td><td>30.60</td><td>64.47</td><td>8.63</td><td>31.59</td><td>39.5</td><td>26.57</td><td>28.16</td><td>64.47</td><td>31.60</td><td>78.71</td><td>8.44</td><td>25.51</td></tr><tr><td>Baseline Acoustic Codec</td><td>40.00</td><td>55.38</td><td>11.10</td><td>37.70</td><td>48.75</td><td>32.26</td><td>36.26</td><td>62.77</td><td>47.22</td><td>82.58</td><td>9.28</td><td>24.83</td></tr><tr><td>Hubert-general-audio</td><td>69.95</td><td>74.87</td><td>34.27</td><td>48.35</td><td>64.50</td><td>43.35</td><td>49.80</td><td>74.09</td><td>69.10</td><td>99.43</td><td>21.27</td><td>35.03</td></tr><tr><td>X-Codec</td><td>69.85</td><td>75.37</td><td>34.05</td><td>49.40</td><td>64.63</td><td>42.95</td><td>52.24</td><td>75.63</td><td>68.40</td><td>99.48</td><td>21.65</td><td>35.20</td></tr></table></body></html>

Experiment Results: As shown in Table 4, the proposed XCodec significantly outperforms the baseline acoustic codec across all metrics. These results demonstrate that semantic information integration significantly enhances the codec’s capability, underscoring the value of semantic enrichment in audio generation tasks.

Analysing the Effect of Codec We hypothesize that the enhanced audio generation capabilities of the audio LLMs are attributed to the improved semantic understanding facilitated by the X-Codec. To validate this hypothesis, we employ the ARCH benchmark (La Quatra et al. 2024) to evaluate the audio semantic understanding, and the benchmark is a comprehensive framework specifically designed to evaluate automatic recognition learning methods across a diverse range of audio classification domains, including acoustic events, music, and speech. The results from this benchmark are shown in Table 5.

Our findings indicate that HuBERT-general-audio significantly outperforms traditional acoustic codecs such as DAC, Encodec, and the baseline acoustic codec across all metrics. This improvement highlights the enhanced semantic understanding of X-Codec for general audio, which appears to be lacking in conventional acoustic audio codecs.

Moreover, X-Codec achieves performance that is comparable or even superior to HuBERT-general-audio, confirming the effectiveness of our approach to enhancing semantic processing within codecs. This equivalence or improvement indicates the capability of X-Codec to integrate semantic information robustly.

# Limitation

While our method significantly enhances the performance of codecs for LLMs by integrating semantic information, it does come with certain trade-offs. According to the principle of ”no free lunch,” improving one aspect of a system often involves compromises in others. In the case of our enhanced codecs, the primary limitation lies in their potential impact on the original functionality of codecs, which is compression for information transmission. The introduction of a semantic extraction layer adds additional computational overhead, potentially increasing the time required for processing. This can affect the efficiency of the codec when used in applications where rapid data compression and transmission are critical. Consequently, while our approach offers substantial benefits for semantic understanding and audio processing, it may not be as effective in contexts where high-speed data compression is paramount.

<html><body><table><tr><td>Model</td><td>Mel DT.</td><td>STFT DT.</td><td>UTMOS</td></tr><tr><td>Baseline (nq=1)</td><td>0.79</td><td>0.73</td><td>2.96</td></tr><tr><td>Baseline (nq=8)</td><td>0.54</td><td>0.58</td><td>3.72</td></tr><tr><td>X-codec (nq=1)</td><td>0.86</td><td>0.77</td><td>3.71</td></tr><tr><td>X-codec (nq=8)</td><td>0.62</td><td>0.63</td><td>4.01</td></tr></table></body></html>

Table 6: Comparison of reconstruction based on Mel DT., STFT DT., and UTMOS metrics using 1000 LibriTTS speech samples. “DT.” is short for distance

Furthermore, the integration of semantic layers can slightly impair certain acoustic metrics such as Mel and STFT distance, which are crucial for maintaining the fidelity of compressed audio. However, it is essential to note that these trade-offs are counterbalanced by significant improvements in human auditory perception, as evidenced by the UTMOS scores.

# Conclusion

In this paper, we introduced X-codec, an advanced audio codec that integrates semantic information through selfsupervised learning models to enhance performance in large language models, specifically in text-to-speech synthesis, music continuation, and general audio classification tasks. Our evaluations demonstrate that $\mathbf { \boldsymbol { X } }$ -codec significantly improves semantic understanding and audio generation quality across a variety of domains.