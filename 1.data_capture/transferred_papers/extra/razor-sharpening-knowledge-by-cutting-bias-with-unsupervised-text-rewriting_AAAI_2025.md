# RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text Rewriting

Shuo Yang\*, Bardh Prenkaj\*, Gjergji Kasneci

Technical University of Munich name.surname $@$ tum.de

# Abstract

Despite the widespread use of LLMs due to their superior performance in various tasks, their high computational costs often lead potential users to opt for the pretraining-finetuning pipeline. However, biases prevalent in manually constructed datasets can introduce spurious correlations between tokens and labels, creating so-called shortcuts and hindering the generalizability of fine-tuned models. Existing debiasing methods often rely on prior knowledge of specific dataset biases, which is challenging to acquire a priori. We propose RAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised, and data-focused debiasing approach based on text rewriting for shortcut mitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text segments by replacing them with heuristically selected alternatives in a shortcut space defined by token statistics and positional information. This process aims to align surface-level text features more closely with diverse label distributions, thereby promoting the learning of genuine linguistic patterns. Compared with unsupervised SoTA models, RAZOR improves by ${ \bar { 3 } } . 5 \%$ on the FEVER and $6 . 5 \%$ on MNLI and SNLI datasets according to the F1 score. Additionally, RAZOR effectively mitigates specific known biases, reducing bias-related terms by $\times 2$ without requiring prior bias information, a result that is on par with SoTA models that leverage prior information. Our work prioritizes data manipulation over architectural modifications, emphasizing the pivotal role of data quality in enhancing model performance and fairness. This research contributes to developing more robust evaluation benchmarks for debiasing methods by incorporating metrics for bias reduction and overall model efficacy.

# Code — https://github.com/ShuoYangtum/RAZOR Appendices — https://arxiv.org/abs/2412.07675

# 1 Introduction

With the progression of instructional learning, prominent LLMs such as ChatGPT (Ouyang et al. 2024) and Vicuna (Zheng et al. 2023) are extensively utilized across diverse domains owing to their versatility and outstanding performance (Chang et al. 2024). However, their utility is significantly constrained by the high training and API licensing costs. As a result, numerous researchers in domains like

Spielberg is a great spinner of a yarn, however this time he just didn’t do it for me. $( f _ { \theta } ; \ { \sqrt { \ } } , \ \Phi ; \ { \bf \Phi } )$ The benefits of a New York Subway system are that a person can get from A to B without being stuck in traffic and subway trains are faster than buses. $\overline { { ( f _ { \theta } \colon \times , \Phi \colon \ V ) } }$

Figure 1: Example of spurious correlation in sentiment classification tasks, where a classifier $f _ { \theta }$ takes Spielberg and New York Subway as shortcuts and makes wrong predictions w.r.t. the ground truth $( \Phi )$ . The classifier concentrates on the bold tokens to make the prediction; however, the underlined tokens might be more useful in producing the correct label.

healthcare, education, and e-commerce adhere to the conventional approach of using open-source language models followed by fine-tuning (Strubell, Ganesh, and McCallum 2019).

LLMs absorb linguistic features from extensive corpora during pre-training by predicting specific lexical units. This foundational learning phase is then followed by fine-tuning domain-specific data, enhancing their ability to perform domain-specific tasks effectively.

Unlike linguistic features, surface biases are reported as commonly existing in manually curated datasets and can detrimentally affect the fine-tuning phase (Jime´nez, Mendes Oliveira, and Soares 2020). In other words, models may achieve untruthfully elevated performance during training by exploiting specific shortcuts. For instance, analyses of the FEVER dataset (Thorne et al. 2018), used for fact verification tasks, revealed a pronounced association between the occurrence of negations within the claim and the corresponding label “refutes” (Karimi Mahabadi, Belinkov, and Henderson 2020). This suggests that models might erroneously generalize that claims containing negations are invariably fake.

The contemporary debiasing methods are relatively effective but require prior knowledge of biases. For example, recent approaches like CLEVER (Xu et al. 2023a) stand on the prior knowledge that shortcuts originate solely from tokens of specific parts within the input. Therefore, they can estimate and correct the impact of shortcuts. However, such prior knowledge is scarce and difficult to obtain. Furthermore, bias types vary significantly with changes in datasets (Geirhos et al. 2020), severely limiting the generality of contemporary methods based on supervised learning.

As an unsupervised improvement strategy, our rationale lies in the limitations of LLMs to explicitly identify specific tokens that serve as shortcuts (Thomas McCoy, Pavlick, and Linzen 2020). Consequently, we propose equalizing all token-related surface1 features across samples from the different classes. By achieving balance among these features, models are prevented from leveraging their potentially spurious correlations with labels. This approach thereby compels deeper semantic learning and augments overall robustness.

The main contributions of this paper are as follows:

• We introduce RAZOR – short for Rewriting And Zerobias Optimization Refinement – a novel unsupervised debiasing technique that mitigates shortcuts in NLP models through iterative text rewriting.   
• To the best of our knowledge, we are the first to formalize the definition of shortcuts about an underlying classifier and the ground truth of the sentences. Our theoretical findings help us understand RAZOR’s effectiveness in mitigating spurious correlations in real-world datasets without prior knowledge.   
• RAZOR improves over the SoTA, achieving a $3 . 5 \%$ performance increase on the FEVER dataset and a $6 . 5 \%$ boost on MNLI and SNLI datasets.   
• RAZOR reduces bias-related terms by $\times 2$ , matching the performance of SoTA models without requiring prior knowledge of bias. This makes RAZOR a crucial advancement for tackling bias in NLP, particularly in scenarios where explicit bias information is inaccessible.   
• Our approach emphasizes the understanding of shortcuts and highlights the power of data-driven techniques over architectural changes in bias mitigation. This reinforces the critical role of data quality in enhancing model fairness and performance while establishing a foundation for developing stronger evaluation benchmarks in future debiasing research.

# 2 Related Work

In recent years, significant research efforts have been directed toward identifying and mitigating spurious correlations in NLP tasks. Our work strictly relates to creating balanced and less biased datasets and counterfactual data generation lines of research.

Creating Balanced and Less Biased Datasets. Wu et al. (2022) present a data generation approach aimed at mitigating spurious correlations in natural language inference datasets through diverse data generation. The authors finetune GPT-2 as their data generation model to sequentially engender an instance’s premise, label, and hypothesis. Additionally, they apply unlikelihood training to mitigate label inconsistency phenomena, improving the label quality at generation time. Finally, they add a consistency step (Bartolo et al. 2021; Lewis et al. 2021) – i.e., using the model’s confidence for filtering purposes – to improve the quality of the generated dataset further. After this process, the authors reject samples that contribute to the high spurious correlations between task-independent features of the samples and their labels via $\textbf { \em z }$ -filtering and produce a debiased dataset. Bras et al. (2020) propose an iterative greedy algorithm that adversarially filters out spurious biases from data by relying on AFLite (Sakaguchi et al. 2020). Interestingly, the authors aim to mitigate performance leakage when datasets present spurious biases.2 CrossAug (Lee et al. 2021) reduces dataset bias by creating contrastive pairs in fact-checking tasks. It uses a two-stage pipeline where the first stage generates negative claims by altering positive claims, and the second stage modifies the evidence to support these negative claims. In this way, CrossAug enhances a classification model’s ability to reason more accurately based on the provided evidence piece of text. Finally, CLEVER (Xu et al. 2023a) leverages counterfactual inference for debiasing in fact-checking tasks. Instead of relying on data augmentation or trainingphase adjustments, CLEVER focuses on the inference stage. It does so by training two models, a claim-evidence fusion model, and a claim-only model, and then subtracting the output of the claim-only model from the fusion model to isolate and remove biases. EDA (Wei and Zou 2019) involves four simple techniques: synonym replacement, random insertion, random swap, and random deletion, all modifying the text while preserving its original meaning. These operations generate diverse training examples, helping models generalize better by exposing them to varied linguistic patterns. The approach is designed to be easy to implement and computationally inexpensive. EDA mitigates overfitting and improves model robustness by focusing on random, controlled transformations. Schuster et al. (2019) demonstrate that models trained solely on claims (without evidence) can achieve high accuracy due to dataset biases, particularly the presence of giveaway phrases that correlate with specific labels. To mitigate this issue, they create a “Symmetric Test Set”, which eliminates these biases by manually generating balanced claim-evidence pairs. Additionally, they propose a regularization method – namely ReW – that reweights training examples to reduce the influence of these biases during model training. Mahabadi, Belinkov, and Henderson (2020) propose “Product of Experts” – namely PoE – which combines predictions from a bias-only model and the main model, focusing the training on examples that are not biased.

Differently from the above works, we defer the rewriting process to GPT-3.5-Turbo, which has demonstrated advantages to mere heuristic-based methods that change synonyms (Wei and Zou 2019), negate verbs to engender new sentences (Lee et al. 2021), or reweigh the sentences to downgrade biases (Schuster et al. 2019). Similarly to (Wu et al. 2022), we use a consistency mechanism. However, we give GPT-3.5-Turbo the original evidence, the newly generated claim, and ask it to tell whether it supports/refutes the new claim and repeat the generation process until the class distributions are aligned (see Sec. 3.3).

Counterfactual Data Generation. Lately, augmenting models with counterfactual data has become a favored method for reducing spurious correlations and enhancing model robustness. Kaushik and Lipton (2018) initially employed human workers to generate counterfactual examples for augmentation. Their findings indicate that counterfactually augmented data effectively mitigates spurious patterns in training data. However, this approach is costly, timeintensive, and often results in simple perturbations. Wu et al. (2021) and Ross et al. (2022) leverage text generation models to produce counterfactual data. These models require fine-tuning with specific perturbation types and have various limitations, including the untargeted and unlabelled generation process and the restricted perturbation types. To introduce new perturbation types, the models need retraining. Unlike the previous two methods, Chen et al. (2023) develop DISCO, a framework for distilling counterfactuals with large language models to help distinguish between spurious and genuine correlations. DISCO does prompt engineering to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data to enrich the original training set. Lastly, Xu et al. (2023b) focus on counterfactual debiasing for fact verification, generating counterfactual scenarios to identify and mitigate spurious correlations in verification tasks.

# 3 Method

# 3.1 Problem Formulation

We consider binary classifications of short text documents, e.g., sentiment classification of reviews. Let us assume that we have a set of i.i.d. labelled samples – i.e., dataset $- \mathcal { D } =$ $\{ d _ { 1 } , \ldots , d _ { n } \}$ . Each sample in $\mathcal { D }$ is a document, i.e., a sequence of tokens, $d _ { i } = \{ t _ { 1 } , t _ { 2 } , \dots , t _ { m } \}$ with a corresponding label $y _ { i } \in \mathcal { V }$ where w.l.o.g. $\mathcal { V } = \{ 0 , 1 \}$ . To classify a document $d _ { i }$ , we first transform it into a feature vector $\mathbf { x } _ { i }$ via a designated transformation function $g : { \mathcal { D } }  \mathcal { X } \subseteq \mathbb { R } ^ { u }$ , where $\chi$ is the domain of features. Then, $\mathbf { x } _ { i }$ is classified by a classification function $f _ { \theta } : \mathcal { X }  \mathcal { Y }$ with model parameters $\theta$ . The parameters $\theta$ are typically estimated from $\mathcal { D }$ by optmizing a certain loss function $\theta ^ { * } \gets \arg \operatorname* { m i n } _ { \theta } \mathcal { L } ( \mathcal { D } , \theta )$ .

Generally, the classification function $f _ { \theta }$ adopted in NLP is a deep learning model (see among others (Naseem et al. 2020; Li et al. 2024a)). Alas, as noticed in (Izmailov et al. 2022), deep classifiers are known to rely on spurious features – i.e., patterns that are correlated with the target on the training data but not inherently relevant to the learning problem, such as the image backgrounds when classifying the foreground. In NLP problems, such as sentiment classification tasks, certain tokens (e.g., words) can be associated with positive/negative classes due to their co-occurrence with the labels in the training data. Inspired by (Wang et al. 2022), in Fig. 1, we show how the classifier exploits the shortcut terms Spielberg and New York Subway to misclassify the sentences. While shortcut mitigation has been extensively studied, to the best of our knowledge, there is a lack of a uniform definition of what a shortcut represents. Hence, we provide the reader with a formal definition of shortcut tokens. Notice that each document $d _ { i }$ gets tokenized into a sequence of tokens $\{ t _ { 1 } , t _ { 2 } , \ldots , t _ { m } \}$ , e.g., at a word-level, character-level, or sub-word level before being transformed into a feature vector $x _ { i }$ . Lastly, assume that $\Phi : \mathcal { X }  \mathcal { Y }$ represents the ground truth function.

Definition 1 Given $a$ document of tokens $\begin{array} { r l } { d _ { i } } & { { } = } \end{array}$ $\{ t _ { 1 } , t _ { 2 } , \ldots , t _ { m } \} \in \mathcal { D } _ { }$ , a feature transformation function $g : \mathcal { D }  \mathcal { X } \subseteq \mathbb { R } ^ { u }$ , a classifier $f _ { \theta } : \mathcal { X }  \mathcal { Y }$ , and $a$ ground truth function $\Phi : \mathcal { X }  \mathcal { Y }$ , a set of tokens $\hat { d } _ { i } \subset d _ { i }$ is $a$ shortcut if the following conditions are satisfied:

$$
\begin{array} { r l } & { f _ { \theta } \left( g ( \hat { d } _ { i } ) \right) = f _ { \theta } \left( g ( d _ { i } ) \right) } \\ & { } \\ & { f _ { \theta } \left( g ( d _ { i } ) \right) \neq \Phi \left( g ( d _ { i } ) \right) } \\ & { } \\ & { | \hat { d } _ { i } | \leq | d _ { i } \backslash \hat { d } _ { i } | } \end{array}
$$

For the sake of the argument, let us assume that $f _ { \theta }$ is a transformer. We indicate with $h : \mathcal { X }  \mathbb { R } ^ { \ell }$ a score-attribution function to each input feature. Notice that the following derivation can be applied for any attribution function – either post-hoc, such as SHAP (Lundberg and Lee 2017) or not. We leave the derivation under a generic attribution function for future work and focus on the attention mechanism of a transformer in this work.

Attention mechanism. Let $h : \mathcal { X } \ \to \ \mathbb { R } ^ { \ell }$ be the attention mechanism that, given in input a document $\begin{array} { r l } { d _ { i } } & { { } = } \end{array}$ $\{ t _ { 1 } , t _ { 2 } , \ldots , t _ { m } \}$ , produces attention weights $\mathbf { z } _ { j }$ for each token $t _ { j } ~ \in ~ d _ { i }$ . In this work, we assume that $h ( g ( t _ { j } ) ) \ =$ $\mathbf { z } _ { j } \forall t _ { j } \in d _ { i }$ .

Essential vs. non-essential tokens. According to Definition 1, we have two sets of tokens for each document $d _ { i }$ : i.e., the set of “essential” tokens $\hat { d } _ { i }$ , and that of “non-essential” ones $d _ { i } \backslash \hat { d } _ { i }$ . The essential tokens are the ones that contribute the most (i.e., their attention scores are high) to the prediction. In contrast, non-essential tokens do not significantly contribute to the prediction (Vaswani et al. 2017). Because the prediction does not change if we use the entire set of tokens in the document $d _ { i }$ , intuitively, one can conclude that the attention scores of the set of non-essential are lower than those of the essential set of tokens (see Lemma 1).

Lemma 1 Given $a$ document of tokens $\begin{array} { r l } { d _ { i } } & { { } = } \end{array}$ $\{ t _ { 1 } , t _ { 2 } , \ldots , t _ { m } \} \in \mathcal { D }$ , and an attention scores attribution function $h : \mathcal { X } \ \to \ \mathbb { R } ^ { \ell }$ , $i f \hat { d } _ { i } \subset d _ { i }$ is a shortcut, then

$$
\frac { H ( \hat { d } _ { i } ) } { | \hat { d } _ { i } | } \geq \frac { H ( d _ { i } \backslash \hat { d } _ { i } ) } { | d _ { i } \backslash \hat { d } _ { i } | }
$$

where

$$
H ( \hat { d } _ { i } ) = \bigg \| \sum _ { t _ { j } \in \hat { d } _ { i } } h \big ( g ( t _ { j } ) \big ) \bigg \| _ { 2 }
$$

$$
H ( d _ { i } \backslash \hat { d } _ { i } ) = \Big | \Big | \sum _ { { t } _ { j } \in d _ { i } \backslash \hat { d } _ { i } } h \big ( g ( t _ { j } ) \big ) \Big | \Big | _ { 2 }
$$

![](images/28d2be3aeebbc9793c1fa9c5c3886b80a442b9ab481115b01760336179d208d8.jpg)  
Figure 2: An example of RAZOR’s application in the fact-checking task. The task aims to determine whether a piece of evidence from Wikipedia supports or refutes a claim. The instance here is sampled from the FEVER dataset, where a negation word ”not” has been reported to exhibit a spurious correlation with the class refutes.

We invite the reader to check Appendix A for the proof of Lemma 1. In practice, we observe that shortcuts are much smaller than the rest of the tokens in a sentence, which paves the way for efficient methods to identify the subset of tokens that represent shortcuts.

# 3.2 Shortcut Identification

Throughout the rest of the paper, we use document and sentence interchangeably. In this paper, we propose Rewriting And Zero-bias Optimization Refinement – namely RAZOR – a sentence rewriting strategy that mitigates shortcuts and promotes dataset debiasing. RAZOR aims to rewrite $\mathcal { D }$ by balancing the number of essential words between positive and negative sentences. However, the cost of rewriting all sentences in $\mathcal { D }$ is exceedingly high. As a heuristic optimization approach, we first filter a subset $\widehat { \mathcal { D } }$ that most likely includes shortcuts – according to Eq. 1 b– rewriting it and repeating this filter-rewriting process.

To find $\widehat { \mathcal { D } }$ , we define a surface feature extraction function $\hat { \boldsymbol g } : \mathcal { D }  \mathbb { R } ^ { \lambda }$ which maps all samples into a shortcut space. Notably, the specific types of shortcuts are strongly task-dependent. In this work, without any prior knowledge, we provide a general identification pattern with the two essential factors by following Definition 1: i.e., (1) the significance of tokens, and (2) the (relative) positions of tokens.

We first use the TF-IDF score to calculate the importance of a specific token $t _ { j } \in d _ { i }$ . To achieve this, we define $S$ : $\mathcal { D } \to \mathbb { R }$ as in Eq. (7):

$$
S ( t _ { j } ) = \frac { n ( t _ { j } , d _ { i } ) } { | d _ { i } | } \log \frac { | \mathcal { D } | } { | \{ d _ { k } \ | \ \exists d _ { k } \in \mathcal { D } \mathrm { s . t . } t _ { j } \in d _ { k } \} | }
$$

where $n ( t _ { j } , d _ { i } )$ is the number of occurrences of token $t _ { j }$ in $d _ { i }$ . Applying Eq. (7) offers a key advantage: i.e., the TF-IDFbased measurement keeps the balance of token frequency, while word frequency is one of the factors most likely to cause shortcuts (Thomas McCoy, Pavlick, and Linzen 2020). For example, suppose a token holds considerably different frequencies in positive versus harmful data. As per how transformers work, this essential token will hold higher attention scores during training, leading the model $f _ { \theta }$ to focus more on its occurrence rather than the semantic information of the sample – i.e., this token is considered a shortcut.

Second, we inject positional information of words through the positional embeddings since the model $f _ { \theta }$ may learn shortcuts from the relative positions of tokens. Specifically, we compare positional embeddings of the same token from samples with different labels to ensure they are closely aligned. Without prior knowledge, we use a fixed positional encoding function $\tau : \mathbb { N }  \mathbb { R } ^ { \bar { \lambda } }$ based on sine and cosine functions

$$
\tau ( \boldsymbol { \mathrm { p o s } } ) = \Bigg [ \boldsymbol { \mathrm { P E } } ( \boldsymbol { \mathrm { p o s } } , 0 ) , \dots , \boldsymbol { \mathrm { P E } } ( \boldsymbol { \mathrm { p o s } } , \lambda - 1 ) \Bigg ] ,
$$

with

$$
\operatorname { P E } ( { \mathrm { p o s } } , k ) = { \left\{ \begin{array} { l l } { \sin { \bigg ( } { \frac { \mathrm { p o s } } { 1 0 0 0 0 ^ { 2 k / \lambda } } } { \bigg ) } } & { k \equiv 0 { \mathrm { ~ ( m o d ~ 2 ) } } } \\ { \cos { \bigg ( } { \frac { \mathrm { p o s } } { 1 0 0 0 0 ^ { 2 k / \lambda } } } { \bigg ) } } & { k \equiv 1 { \mathrm { ~ ( m o d ~ 2 ) } } } \end{array} \right. }
$$

where pos is the token’s position in a target document, $k$ is the dimension index in the embedding vector, and $\lambda$ is the dimensionality of the positional embedding vector. Afterwards, we compute the mean of the products of $S$ for all tokens in a document $d _ { i }$ as its shortcut feature mapping

$$
\hat { g } ( d _ { i } ) = \frac { \sum _ { t _ { j } \in d _ { i } } S ( t _ { j } ) \ast \tau ( j ) } { | d _ { i } | - 1 } ,
$$

assuming that $t _ { j }$ occurs in position $j$ in the document $d _ { i }$ , and $^ *$ is the scalar product. Finally, let us use $\vartheta ( d _ { i } , \Phi )$ to indicate

$$
\vartheta ( d _ { i } , \Phi ) = \{ d _ { j } \ | \ d _ { j } \in { \mathcal { D } } \mathrm { s . t . } \ \Phi ( d _ { i } ) \neq \Phi ( d _ { j } ) \} .
$$

In other words, given a document $d _ { i } \in \mathcal { D }$ , and a ground truth function $\Phi$ , as introduced in Definition 1, $\vartheta ( d _ { i } , \Phi )$ produces a set of documents whose label is different with $\Phi ( d _ { i } )$ . In this way, we define a shortcut score $\gamma : { \mathcal { D } }  \mathbb { R }$ as in Eq. 12.

$$
\gamma ( d _ { i } ) = 1 - \frac { 1 } { \left| \vartheta ( d _ { i } , \Phi ) \right| } \sum _ { d _ { j } \in \vartheta ( d _ { i } , \Phi ) } \frac { \widehat { g } ( d _ { i } ) \cdot \widehat { g } ( d _ { j } ) } { \underbrace { | | \widehat { g } ( d _ { i } ) | | \left| | \widehat { g } ( d _ { j } ) \right| | } _ { \cos ( \widehat { g } ( d _ { i } ) , \widehat { g } ( d _ { j } ) ) } }
$$

Note that the higher the shortcut score, the greater the token position distribution differs between the sample $d _ { i }$ , assuming it is positive, and the negative samples in $\varphi ( d _ { i } , \Phi )$ , indicating that the model $f _ { \theta }$ is more likely to learn shortcuts from $d _ { i }$ . We select the top documents with the highest shortcut values to form $\widehat { \mathcal { D } }$ and use the documents therein for rewriting.

# 3.3 Rewriting and Filtering

After obtaining the set of shortcut candidates, we rewrite each sentence $\bar { d } _ { i } \in \widehat { \mathcal { D } }$ into several candidate sentences maintaining the original lbabel $\Phi ( d _ { i } )$ . We then substitute $d _ { i }$ with the rewritten sentence least likely to contain shortcuts. In Fig. 2, we show how this rewriting process works in a factchecking task. Here, we aim for the LLM to generate a new claim based on the same evidence from Wikipedia while ensuring that the evidence still supports (or refutes) the newly generated claim. To rewrite the sentence $d _ { i }$ , we construct a prompt for an LLM that generates rewritten candidates. The prompt contains two components: i.e., an instruction, which is a general explanation of the rewriting task for $d _ { i }$ including $\bar { \Phi } ( d _ { i } )$ ; and the input $d _ { i }$ itself. We ask the LLM to engender rewritten sentences while maintaining the original label $\Phi ( d _ { i } )$ . Hence, we rely on a self-consistency mechanism to ensure the quality of generated sentences. Specifically, in the fact-checking scenario, we provide the LLM with the original evidence and the newly generated claim and query it to tell us whether the evidence supports/refutes the new claim. More formally, let $G _ { \alpha } : \mathcal { D } \times \mathcal { Y }  \mathcal { D }$ be the LLM we are querying to generate new sentences respecting the original label. Additionally, let $G _ { \beta } : \mathcal { D } \times \mathcal { Y }  \{ 0 , 1 \}$ be another $\mathrm { L L M } ^ { 3 }$ that verifies whether the generated sentence has the original label. More formally, we accept an engendered sentence $d _ { i } ^ { * } = G _ { \alpha } ( d _ { i } , \Phi ( d _ { i } ) )$ if Eq. (13) is satisfied.

$$
\mathbb { I } \bigg [ G _ { \beta } \bigg ( G _ { \alpha } \big ( d _ { i } , \Phi ( d _ { i } ) \big ) , \Phi ( d _ { i } ) \bigg ) = 1 \bigg ]
$$

Once, the rewriting process is completed for a particular $d _ { i }$ , we obtain a set of generated documents $\varphi ( d _ { i } ) \bar { = } \{ d _ { i } ^ { * } \mid d _ { i } ^ { * } =$ $G _ { \alpha } ( d _ { i } , \Phi ( d _ { i } ) ) \}$ . From $\varphi ( d _ { i } )$ , we choose the one scoring the least according to Eq. 12 and insert it in the original dataset $\mathcal { D }$ . The original document $d _ { i }$ is removed from the “rewritten” dataset as we aim to mitigate the shortcuts that it might have contained. In this way, RAZOR maintains the same size as the original dataset throughout the rewriting process. We repeat this process until the cosine similarity between the embeddings in the surface feature space of the documents of each label is maximal (see Eq. 14).

$$
\mathrm { m a x i m i z e } \sum _ { d _ { i } \in \mathcal { D } _ { 0 } } \sum _ { d _ { j } \in \mathcal { D } _ { 1 } } \cos ( \hat { g } ( d _ { i } ) , \hat { g } ( d _ { j } ) )
$$

$$
\cdot \mathcal { D } _ { y } = \{ d \mid d \in \mathcal { D } \ \wedge \ \Phi ( d ) = y \}
$$

# Experiments

Datasets. Fact-checking tasks aim to determine the veracity of a claim from social media based on existing facts.

Here, we use the FEVER dataset (Thorne et al. 2018), which consists of claims and evidence retrieved from Wikipedia. The training objective is to classify whether the evidence supports or refutes the claim. Due to the prevalence of generating non-factual claims by simply negating factual claims, a significant negation bias has been reported in the literature.

Natural Language Inference (NLI) aims to determine the semantic relationship between a pair of sentences. Given a premise and a hypothesis, the goal is to classify the relationship as entailment, contradiction, or neutral. Here, we rely on the Multi-Genre Natural Language Inference (MNLI) (Williams, Nangia, and Bowman 2018) corpus and the Stanford Natural Language Inference (SNLI) (Bowman et al. 2015) corpus, which provides a large number of sentence pairs annotated with their logical relationship.

Experimental setup. Our preprocessing steps involve truncating sequences to 512 tokens. We rely on three classification models to measure RAZOR’s effectiveness, namely BERT, RoBERTa, and DistilBERT – taken from HuggingFace4 – followed by a linear classification layer. We train the classifiers using the LoRa algorithm (Hu et al. 2022) and the AdamW optimizer with a batch size of 16 and a learning rate of $3 \times 1 0 ^ { - 5 }$ . In the sentence rewriting, we set $G _ { \alpha } = \mathrm { G P T } \mathrm { - } 3 . 5$ -Turbo and $G _ { \beta } = \mathrm { G P T } \mathrm { - } 3 . 5$ -Turbo. Here, we control the diversity of generated sequences by setting the top-p value to 0.9 and the temperature to 0.7. To provide details on the impact of the selected LLM to rewrite the sentences and verify their labels, we change both $G _ { \alpha }$ and $G _ { \beta }$ to LLaMA-3.1-8B-Instruct (see Tables 1 and 2).

# 4.1 Discussion

RAZOR is a more robust dataset debiaser than SoTA methods on fact-checking tasks with, respectively, ${ \sim } 2 \%$ and ${ \sim } 3 . 6 3 \%$ increase in Accuracy and F1 over the second-best. We train BERT, RoBERTa, and DistilBERT classifiers on the original FEVER dataset to represent our baseline (hereafter Original). Then, for each SoTA method, we train the two classifiers on the modified version of FEVER according to the proposed algorithms. Notice that RAZOR and other SoTA methods only rewrite the training set of the dataset. The test set remains unchanged. We follow this strategy to assess the impact of the rewriting strategy w.r.t. the test set’s original peculiarities, which might still contain shortcuts. We expect that if the rewriting strategy truly alleviates the shortcut problem in the training set, then in the test set, we will see an increase in performance since the false negative/positive ratio should decrease5. Table 1 shows this effect on the two classifiers mentioned above for RAZOR and SoTA methods. Notice how, although second-best after CrossAug, RAZOR effectively removes the shortcut effect on both classifiers w.r.t. the original baseline. We acknowledge that RAZOR is behind CrossAug in terms of performance because the latter enriches the dataset with contrastive – i.e., positive-negative – claim-evidence pairs, where the evidence is rewritten differently from RAZOR, which only operates at the claim level. However, we show how RAZOR is more robust than CrossAug in the FEVER-Adversarial dataset.6 The sentences in the test set of FEVER-Adversarial do not contain shortcuts. Hence, we use the adversarial test set to demonstrate RAZOR’s debiasing abilities. Notice how CrossAug’s performance is systematically lower than that of RAZOR. Because CrossAug negates the claim and evidence to generate other factual instances, it might happen that this newly generated claim-evidence pair can itself contain shortcuts, exacerbating the classifier’s reasoning capabilities. However, since FEVER’s test set contains shortcuts, CrossAug’s original performances are not faithful to assess the goodness of this debiasing strategy. In particular, this is evident in FEVER-Adversarial, where simple negations of claim-evidence pairs are not optimal. Contrarily, RAZOR reduces shortcuts’ effect and achieves consistently better results than all SoTA methods.

Table 1: Comparison of RAZOR with SoTA methods on the FEVER and FEVER-Adversarial datasets – the datasets have the same training set but different test sets. We show the performance of the classifiers in terms of accuracy, and F1 scores after the shortcuts within the dataset have been modified using each debiasing method, including RAZOR (ours). Original means that the original training set has not been modified, and we report the performances of the two classifiers as is.   

<html><body><table><tr><td rowspan="3" colspan="2"></td><td colspan="4">FEVER</td><td colspan="4">FEVER-Adversarial</td></tr><tr><td colspan="2">BERT</td><td colspan="2">RoBERTa</td><td colspan="2">BERT</td><td colspan="2">RoBERTa</td></tr><tr><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td colspan="2">Original</td><td>87.93±0.51</td><td>86.65±0.78</td><td>90.05±0.48</td><td>88.90±0.69</td><td>64.10±1.21</td><td>69.81±1.45</td><td>66.99±1.30</td><td>73.13±1.82</td></tr><tr><td></td><td>EDA</td><td>87.99±0.30</td><td>87.03±0.50</td><td>89.25±0.32</td><td>89.00±0.66</td><td>65.20±0.99</td><td>69.55±0.86</td><td>66.35±1.54</td><td>69.98±1.97</td></tr><tr><td></td><td></td><td>90.28±0.42</td><td>89.37±0.54</td><td>91.00±0.46</td><td>91.98±0.78</td><td>63.58±1.20</td><td>66.74±1.56</td><td>67.02±1.44</td><td>72.04±1.77</td></tr><tr><td></td><td>ReW</td><td>87.50±0.88</td><td>87.76±1.19</td><td>89.09±1.21</td><td>89.44±2.01</td><td>64.52±2.21</td><td>69.85±2.30</td><td>67.23±2.03</td><td>74.20±2.68</td></tr><tr><td></td><td>PoE</td><td>86.25±1.78</td><td>86.98±1.89</td><td>90.20±1.78</td><td>89.79±1.90</td><td>65.20±2.76</td><td>70.45±2.58</td><td>68.00±2.49</td><td>72.12±2.61</td></tr><tr><td></td><td>CLEVER</td><td>85.13±1.20</td><td>85.33±2.03</td><td>86.55±1.72</td><td>84.54±2.33</td><td>56.16±2.54</td><td>58.93±2.27</td><td>62.63±2.96</td><td>63.35±2.35</td></tr><tr><td></td><td>RAZOR(w/LLaMA)</td><td>87.97±1.20</td><td>87.22±2.03</td><td>90.59±2.32</td><td>89.11±2.56</td><td>66.03±1.95</td><td>69.61±1.88</td><td>68.77±3.01</td><td>73.76±2.50</td></tr><tr><td></td><td>RAZOR(w/GPT)</td><td>88.73±2.23</td><td>88.39±2.20</td><td>90.45±1.99</td><td>90.22±2.11</td><td>66.66±1.98</td><td>73.78±1.37</td><td>69.21±1.64</td><td>76.08±2.02</td></tr></table></body></html>

Table 2: RAZOR’s impact on the classifiers on the NLI datasets in terms of Accuracy and F1 score. Note that we assess the performances of the classifiers across different train-test set combinations – e.g., training on MNLI and testing on SNLI – to mitigate the fact that the original test set contains similar shortcuts as the original training set. Within parentheses, we indicate the training set; outside of them, we indicate the test set.   

<html><body><table><tr><td rowspan="2"></td><td colspan="2">MNLI (MNLI)</td><td colspan="2">SNLI (MNLI)</td><td colspan="2">MNLI (SNLI)</td><td colspan="2">SNLI (SNLI)</td></tr><tr><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>BERT</td><td>86.70±1.21</td><td>86.85±1.79</td><td>83.05±1.38</td><td>84.41±2.01</td><td>70.15±2.89</td><td>62.75±2.77</td><td>91.65±0.80</td><td>91.74±0.91</td></tr><tr><td>w/RAZOR (GPT)</td><td>88.50±1.32</td><td>87.90±1.80</td><td>84.75±1.72</td><td>85.07±2.44</td><td>74.10±3.20</td><td>67.86±3.57</td><td>93.20±1.11</td><td>93.16±1.26</td></tr><tr><td>W/RAZOR (LLaMA)</td><td>86.80±1.32</td><td>86.88±1.80</td><td>84.10±1.52</td><td>83.51±2.00</td><td>74.25±2.92</td><td>68.73±3.14</td><td>92.90±1.27</td><td>92.81±1.32</td></tr><tr><td>△ (GPT)</td><td>+2.08</td><td>+1.44</td><td>+2.04</td><td>+0.78</td><td>+5.63</td><td>+8.14</td><td>+1.69</td><td>+1.55</td></tr><tr><td>△(LLaMA)</td><td>+0.12</td><td>+0.04</td><td>+1.26</td><td>-1.06</td><td>+5.85</td><td>+9.56</td><td>+1.36</td><td>+1.17</td></tr><tr><td>RoBERTa</td><td>90.85±1.10</td><td>90.75±1.53</td><td>90.30±1.27</td><td>91.08±1.55</td><td>85.35±2.04</td><td>84.93±1.48</td><td>95.05±0.72</td><td>95.01±0.80</td></tr><tr><td>W/RAZOR(GPT)</td><td>91.15±1.26</td><td>91.14±1.48</td><td>91.45±1.29</td><td>91.42±1.68</td><td>86.50±2.20</td><td>86.22±1.72</td><td>95.15±0.70</td><td>95.17±0.98</td></tr><tr><td>W/RAZOR (LLaMA)</td><td>91.55±1.38</td><td>91.45±1.70</td><td>90.90±1.29</td><td>91.13±1.68</td><td>85.50±1.98</td><td>85.76±2.24</td><td>95.60±0.70</td><td>95.71±0.98</td></tr><tr><td>△(GPT)</td><td>+0.33</td><td>+0.42</td><td>+1.27</td><td>+0.37</td><td>+1.34</td><td>+1.52</td><td>+0.11</td><td>+0.17</td></tr><tr><td>△ (LLaMA)</td><td>+0.77</td><td>+0.77</td><td>+0.66</td><td>+0.05</td><td>+0.18</td><td>+0.98</td><td>+0.58</td><td>+0.74</td></tr><tr><td>DistilBERT</td><td>83.50±1.23</td><td>82.97±1.77</td><td>77.23±1.85</td><td>77.38±1.86</td><td>65.55±2.30</td><td>69.90±1.78</td><td>88.82±1.22</td><td>87.78±1.67</td></tr><tr><td>W/RAZOR (GPT)</td><td>85.14±1.66</td><td>85.20±1.84</td><td>80.22±1.90</td><td>78.82±2.21</td><td>68.58±2.16</td><td>72.80±1.83</td><td>91.82±0.88</td><td>91.80±1.03</td></tr><tr><td>W/RAZOR (LLaMA)</td><td>84.00±1.35</td><td>83.98±1.78</td><td>77.35±1.43</td><td>77.62±1.99</td><td>66.33±2.00</td><td>71.30±1.85</td><td>90.30±1.26</td><td>90.23±1.97</td></tr><tr><td>△(GPT)</td><td>+1.96</td><td>+2.69</td><td>+3.87</td><td>+1.86</td><td>+4.62</td><td>+4.14</td><td>+3.38</td><td>+4.58</td></tr><tr><td>△ (LLaMA)</td><td>+0.60</td><td>+1.22</td><td>+0.16</td><td>+0.31</td><td>+1.19</td><td>+2.00</td><td>+1.67</td><td>+2.79</td></tr></table></body></html>

RAZOR demonstrates cross-dataset generalizability in multi-class classification tasks. To validate the generalizability of our method, we rely on MNLI and SNLI for a multi-class classification test. Similarly to the above process, we fine-tuned the classifiers as baselines to determine whether the logical relationship between two sentences is entailment, contradiction, or neutral. Subsequently, we used RAZOR to rewrite the training sets and achieve improvements in Accuracy and F1 scores for the two classifiers. With the lack of corresponding attacking datasets like FEVERAdversarial, we exchanged MNLI and SNLI test sets to simulate a real-world data distribution.

Our experiments demonstrate that even widely used datasets exhibit shortcut problems. We observed that by applying RAZOR, the baseline classifiers achieved improvements of approximately 0.1 to 8 points across all datasets and metrics. The most significant improvement is observed in the SNLI-MNLI training-test set combination. This suggests that SNLI may contain more shortcuts than MNLI, resulting in a performance difference of over 20 points for BERT across different test sets. However, after applying RAZOR, a substantial portion of the shortcuts in the data was removed, leading to an increase of 8.14 points in F1 score and 5.63 points in accuracy for BERT. Moreover, we find that RAZOR performs better on all classifiers overall. This is because models with better overall performances are less susceptible to shortcut issues. Therefore, RAZOR demonstrates significant potential for debiasing smaller models.

![](images/7a5bd6e754de4b894ea6cb33d06b13ec1d1c39cec954f7560a7baeb583c6a484.jpg)  
Figure 3: Effect of shortcut-related terms for BERT and RoBERTa with and without RAZOR on 500 randomly sampled original-rewritten pairs on the FEVER dataset. We then test on the FEVER-Adversarial set.

RAZOR effectively reduces the number of negations considered shortcuts in FEVER by $3 6 . 2 \%$ . To further quantify the effectiveness of our approach in mitigating shortcuts, we perform a statistical analysis of shortcutrelated terms, specifically negations, within both the original and rewritten FEVER training set. Following the methodology presented in (Li et al. 2024b), we train models with randomly sample 500 original-rewritten pairs from the two sets, count the occurrences of the words ”no” and ”not” in their claims, and repeat the experiments detailed in Table 1 for the test set of the FEVER-Adversarial dataset. We illustrate the results in Fig. 3. Notice that, by applying RAZOR, the occurrence of shortcut-related terms is reduced by approximately $40 \%$ without any prior knowledge of these terms. Furthermore, eliminating shortcuts significantly improved models’ generalizability, leading to an increase of, respectively, ${ \sim } 3$ and ${ \sim } 2$ percentage points for BERT and RoBERTa in both Accuracy and F1 scores when we test on the FEVER-Adversarial set.

# 4.2 Ablation Studies

To ensure that the performance improvement of our model does not stem from GPT itself, we first ruled out potential information leakage by calculating repetition rates. Subsequently, we replaced our shortcut space mapping with the model’s token embeddings and employed the same method to demonstrate the effectiveness of our defined shortcuts. We illustrate the results in Fig. 4. We observe consistent trends in the Accuracy and F1 score training curves. RAZOR outperforms the baseline and significantly surpasses the sentence embedding-based method on RoBERTa and BERT.

Furthermore, RAZOR exhibits a slightly slower convergence rate than the baseline but meets equilibrium earlier than the embedding-based method. We attribute this to RAZOR’s ability to mitigate the influence of surface features, encouraging the classifiers to move beyond specific token biases and invest more time in learning linguistic features, ultimately resulting in superior performance. In contrast, embedding-based methods rely on sentence representations that mix surface and linguistic features, failing to separate the two effectively, preventing the classifiers from avoiding shortcuts. Moreover, the proximity of embeddings may obscure the classification hyperplane, inherently complicating the task and leading to mediocre performance. Given that both methods employ GPT-3.5-Turbo but produce different outcomes, we can safely conclude that the overall RAZOR’s framework, rather than solely GPT in the rewriting process, is a primary driver in its effectiveness. Appendix B illustrates the performance difference between RAZOR operating on BERT with surface features and embedding features.

![](images/169ff7e4004ed44e455337f56a3e5f711984f0d4add480171011ceee37c4bfe6.jpg)  
Figure 4: The performance of models trained on FEVER when evaluated on the original FEVER test set.

# 5 Conclusion

RAZOR is an effective dataset debiasing method. It outperforms SoTA methods in addressing shortcut problems in fact-checking and natural language inference tasks. RAZOR maintains dataset integrity by generating new sentences that adhere to the original labels and strategically removing the original documents while significantly reducing shortcut-induced biases. This is achieved through an iterative rewriting process, optimized until the KL divergence between the embeddings in the surface feature space is minimized. Our experiments consistently show that RAZOR enhances the performance of classifiers such as BERT and RoBERTa, particularly in challenging adversarial scenarios where traditional methods like CrossAug fail to deliver competitive results. Notably, RAZOR’s superior performance on models like BERT, even compared to more robust models like RoBERTa, underscores its potential as a powerful debiasing tool, particularly for smaller and more resourceconstrained models. The implications of our work extend beyond the datasets and models tested, offering a promising approach to improving model reliability across a wide range of machine-learning applications. While RAZOR has shown great promise, future work could explore its application to other types of biases or in conjunction with additional debiasing strategies. We believe that RAZOR represents a significant step forward in the ongoing effort to create more fair and accurate machine learning systems, ultimately contributing to more trustworthy AI applications in the real world.