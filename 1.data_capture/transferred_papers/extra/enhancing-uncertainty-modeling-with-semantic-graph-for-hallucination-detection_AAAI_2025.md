# Enhancing Uncertainty Modeling with Semantic Graph for Hallucination Detection

Kedi Chen1\*†, Qin Chen1\*‡, Jie Zhou1, Xinqi $\mathbf { T a o } ^ { 2 }$ , Bowen $\mathbf { D i n g ^ { 2 } }$ , Jingwen Xie2, Mingchen Xie2, Peilong $\mathbf { L i } ^ { 2 }$ , Feng Zheng

1East China Normal University 2Xiaohongshu Inc. kdchen@stu.ecnu.edu.cn qchen, jzhou @cs.ecnu.edu.cn {yifan5, faming, qingliang, shenzong, liaofan, yemu} $@$ xiaohongshu.com

# Abstract

Large Language Models (LLMs) are prone to hallucination with non-factual or unfaithful statements, which undermines the applications in real-world scenarios. Recent researches focus on uncertainty-based hallucination detection, which utilizes the output probability of LLMs for uncertainty calculation and does not rely on external knowledge or frequent sampling from LLMs. Whereas, most approaches merely consider the uncertainty of each independent token, while the intricate semantic relations among tokens and sentences are not well studied, which limits the detection of hallucination that spans over multiple tokens and sentences in the passage. In this paper, we propose a method to enhance uncertainty modeling with semantic graph for hallucination detection. Specifically, we first construct a semantic graph that well captures the relations among entity tokens and sentences. Then, we incorporate the relations between two entities for uncertainty propagation to enhance sentence-level hallucination detection. Given that hallucination occurs due to the conflict between sentences, we further present a graph-based uncertainty calibration method that integrates the contradiction probability of the sentence with its neighbors in the semantic graph for uncertainty calculation. Extensive experiments on two datasets show the great advantages of our proposed approach. In particular, we obtain substantial improvements with $1 9 . 7 8 \%$ in passage-level hallucination detection.

# Passage to be detected: Gaulle was born in the Paris. She was died of meningitis.

sentence1 Fact passage 0G.a7u2lle was born in the 0.P9a4ris No0n.8F3act AVE 三 0.81 predict score sentence2 NonFact She was died of meningitis NonFact 0.50 labeled score 0.63 0.95 0.79 Huge Gap (a) ..

# Passage to be detected:

Gaulle was born in the Paris. She was died of meningitis. …

sentence1 Fact passage Gaulle was born in the Paris Fact graph passage 1 0.32 sentence2 NonFact She was died meningitis NonFact 0.57 predict score 0.88 0.50 labeled score (b) Good Match 园 entity words W predicate words relation path

# Introduction

Large Language Models (LLMs) (Zhao et al. 2023a), with large-scale parameters and advanced training methods, achieve excellent performance in many downstream tasks of natural language processing (NLP) (Aracena et al. 2024; Chen et al. 2024c; Lai and Nissim 2024; Zhang et al. 2024). Despite the many benefits of large language models, hallucination remains an issue that cannot be ignored. Hallucination indicates that some non-factual or untruthful contents are generated (Wang et al. 2023a). Therefore, hallucination detection is critically an essential task, which provides a preliminary review of the contents generated by large language models, reducing their potential harm in real-world scenarios (Lee et al. 2024; Cui et al. 2023; Yan et al. 2024), such as education, economics, science, and so on.

Current hallucination detection methods can be roughly divided into three categories. (i) Retrieval-based method (Wang et al. 2023c; Zhang et al. 2023b) usually retrieve evidence from external resources for fact verification (Chen et al. 2024a). This approach exceedingly depends on the quality of external resources, which is not always available. In addition, it needs various validation steps towards the retrieved knowledge, which are complicated and inefficient.

(ii) Sampling-based method frequently samples responses from LLMs for consistency verification, which consumes substantial computational resources (Manakul, Liusie, and Gales 2023; Zhang et al. 2023a). (iii) The uncertainty-based method is a good alternative to resolve the above problems (Giulianelli et al. 2023; Xiong et al. 2023). It leverages LLMs to output the probability of each token in the text to be detected and then computes a hallucination score with uncertainty-based metrics. Given that this method requires the models to perform inference only once, it is relatively efficient and thus attracts increasing interest from researchers.

Nevertheless, several challenges persist in uncertaintybased methods for hallucination detection (Figure 1). First, most methods focus on modeling the uncertainty of each independent token, while the complex dependency among tokens within the sentence is not well explored. Recent methods (Zhang et al. 2023c) tend to propagate the uncertainties of all previous tokens to the subsequent ones for uncertainty calculation. However, not all tokens are semantically related, and this propagation leads to uncertainty overestimation as shown in Figure 1. Second, passage-level uncertainty is not well studied. Previous methods usually average the uncertainty score of each sentence (Manakul, Liusie, and Gales 2023; Zhang et al. 2023c), while neglecting the intricate relations such as the semantic conflicts among sentences in the whole passage.

To resolve the above two challenges, we propose an approach to enhance uncertainty modeling with semantic graph for hallucination detection. Specifically, we first perform Abstract Meaning Representation (AMR) (Xu, Lee, and Huang 2023) based parsing for each sentence, and obtain a passage-level AMR graph by coreference resolution and entity linking between sentences, which well captures the semantic dependency relations among the entity tokens and the sentences for hallucination detection. Then, we present a relation-based propagation method, which propagates the uncertainty from one entity to the other along the relation path in the semantic graph to enhance sentencelevel hallucination detection as shown in Figure 1. Regarding passage-level hallucination detection, we further integrate the relations between the sentence and its neighbors in the graph for uncertainty calibration via the natural language inference (NLI) (Zheng and Zhu 2023) technique.

We perform experiments on two datasets, namely the well-known WikiBio (Manakul, Liusie, and Gales 2023) and our constructed NoteSum. The results show the great superiority of our approach in both sentence-level and passagelevel hallucination detection.

The main contributions can be summarized as follows:

• To the best of our knowledge, it is the first attempt to explore the potential of semantic graph to capture the complex relations among the tokens and the sentences for hallucination detection.   
• We present two novel methods, namely relation-based uncertainty propagation and graph-based uncertainty calibration, which shed light on how to integrate the structured semantic graph with the uncertainty computation framework.

• We conduct elaborate analyses of the experimental results on two benchmark datasets, and provide a better understanding of the effectiveness of our approach.

# Related Work

# Hallucination in Language Models

Hallucination reflects that language models generate some nonsensical or untruthful contents (Wang et al. 2023a) in many downstream NLP tasks, such as the question and answer task (Nasza´di, Manggala, and Monz 2023), the multiturn dialogue task (Chen et al. 2024b) and the text summarization task (Kryscinski et al. 2020), etc. Hallucination in NLP can be categorized into two main classes: factuality hallucination and faithfulness hallucination (Huang et al. 2023a). The former one reveals the generated contents contain factual errors against real life, while the latter demonstrates the issues of inconsistency or irrelevance in the text.

# Hallucination Detection

Before the era of LLMs, researchers normally train a discriminating model to judge whether hallucination exists (Zhao, Nguyen, and Daume 2023). This approach relies too heavily on the training data and can reduce the models’ generalization ability. With the development of NLP technology, current hallucination detection methods can be roughly divided into three categories.

Retrieval-based method (Wang et al. 2023c; Zhang et al. 2023b) utilizes the retrieval-augmented generation technique (Chen et al. 2024a) for extra knowledge (Choi et al. 2023) or information to help detection (Varshney et al. 2023; Chen et al. 2024b; Siino 2024). This approach exceedingly depends on the quality of information sources, necessitating complicated validation steps (Ye et al. 2024; Dong et al. 2024) towards the retrieved knowledge. Not to mention that not all information is available easily. On the contrary, we propose an efficient reference-free method.

Sampling-based method rewrites the contents under detection, measuring the consistency and coherence (Malkin, Wang, and Jojic 2022; Sheng et al. 2024) between them to acquire a hallucination score (Manakul, Liusie, and Gales 2023; Zhang et al. 2023a; Zhao et al. 2023b; Mu¨ndler et al. 2024). However, this strategy frequently invokes LLMs for rewriting, consuming substantial computational resources. Our method needs one LLM to infer only once, thereby greatly saving the response time.

Uncertainty-based method applies proxy-based LLMs to output the probability of each token in contents to be detected and then estimates a hallucination score with uncertainty-based metrics (Huang et al. 2023b; Chen et al. 2023; Wang et al. 2023b; Petersen et al. 2024; Xiong et al. 2024). Manakul, Liusie, and Gales (2023) regards the degree of hallucination as being negatively correlated with the probability. Zhang et al. (2023c) refutes this view, but there arises a co-occurrence bias (Zhou et al. 2023). Due to a lack of detailed exploration of various dependencies, our method systematically constructs the relationships among the entity tokens and the sentences.

![](images/2b2f9f8f817cc3a7c4b48378c12437e6951ded18d3f226eb00a035a2221f22b0.jpg)  
Text to be detected: Gaulle was born in the family of the French President. She once lived in the Paris and died of meningitis on 18 April 1948. She become the only child of Charles and Yvonne de Gaulle. She was born with a severe form of spina bifida, which left her unable to walk….   
Figure 2: The overview of our approach for hallucination detection. For token-level uncertainty, we integrate the maximum and variance of the probabilities, along with a sequence decay term. Regarding to sentence-level uncertainty, we interpolate the sum of entity uncertainty through relation-based propagation and global uncertainty via quantile. Finally, we incorporate the relations of neighbor sentences in the semantic graph with graph-based uncertainty calibration for passage-level uncertainty.

# Our Approach

The framework of our proposed approach is illustrated in Figure 2. Specifically, inspired by the findings that hallucination accumulates as the sequence length increases, we integrate the distribution statistics of LLM-based conditional probability with sequence decay for token-level uncertainty calculation. Considering much hallucination is induced by the entities and relations in the sentence and passage, we further construct a semantic graph for sentence-level and passage-level uncertainty calculation. Regarding sentencelevel uncertainty, it well captures the semantic relations between entities for hallucination propagation and calculation. In particular, the uncertainty of an entity propagates to the related entity along the dependent relations. For passagelevel uncertainty, we incorporate the neighbors of each sentence in the semantic graph for uncertainty calibration and summation. The details are denoted in the following.

Semantic Graph Construction. To better model the uncertainties of entities with long-range dependency that span over the text, we first perform AMR (Xu, Lee, and Huang 2023) parsing for each sentence, and gain a sentence-level graph where each node is an entity and the edge represents the dependent semantic relation. Compared to traditional dependency parsing, AMR parsing is more logical and less vulnerable to syntactic representation or word order variations. Therefore, we employ AMR to model the interdependency between the entities in the sentence. Furthermore, noting that passage-level hallucination usually occurs when two sentences contradict each other, we further link sentence-level AMR graphs together by the intricate relations (e.g., entity linking and coreference) among sentences. Finally, a large AMR graph corresponding to the passage is acquired.

Formally, we provide the notations deployed in this paper. Let $\mathcal { D }$ express the input passage with $m$ sentences, which is denoted as $\mathcal { D } = \{ S _ { 1 } , S _ { 2 } , . . . , S _ { m } \}$ . Each sentence $\boldsymbol { S } _ { i }$ is composed of $n _ { i }$ tokens, i.e., $S _ { i } = \mathbf { \bar { \{ } } { }  t _ { i } ^ { 1 } , t _ { i } ^ { 2 } , \ldots , t _ { i } ^ { n _ { i } } \mathbf  \Big \} $ . In addition, the set of entity tokens in $\boldsymbol { S } _ { i }$ is formulated by $E _ { i } = \left\{ e _ { i } ^ { 1 } , e _ { i } ^ { 2 } , \ldots , e _ { i } ^ { | { E _ { i } } | } \right\}$ , where $\left| E _ { i } \right|$ indicates the number of entities in the $i$ -th sentence.

# Token-level Uncertainty

Generally, the conditional probability of a token output by LLMs reflects its likelihood in the context, which can be adapted to measure the uncertainty. Previous researches mainly focus on using the negative log probability or entropy-based methods for uncertainty estimation (Huang et al. 2023b). In this paper, we integrate two statistical indicators, namely the maximum and variance of the probability distributions. Moreover, hallucination tends to accumulate with the increasing sequence length as demonstrated in previous studies (Varshney et al. 2023; Nasza´di, Manggala, and Monz 2023; Chen et al. 2024b), thus we further devise a sequence decay term that explicitly models the absolute position of the token in the passage. Specifically, the tokenlevel uncertainty in the $j$ -th position of $i$ -th sentence can be measured as:

$$
\mathcal { U } ( t _ { i } ^ { j } ) = \frac { 1 } { \operatorname* { m a x } ( \mathcal { C } _ { i } ^ { j } ) + \sigma ^ { 2 } ( \mathcal { C } _ { i } ^ { j } ) } \underbrace { \big ( 1 + e ^ { \frac { l e n ( S _ { 1 : i - 1 } ) + j } { l e n ( \mathcal { D } ) } - 1 } \big ) } _ { \mathrm { s e q u e n c e d e c a y t e r m } }
$$

where $\mathcal { C } _ { i } ^ { j }$ signifies the top- $k$ probabilities of a candidate token set that could probably appear in the current position based on LLMs, which is formulated as:

$$
\mathcal { C } _ { i } ^ { j } = s o r t e d ( P _ { i j } ^ { \mathcal { V } } ) \left[ - k : \right]
$$

where $P _ { i j } ^ { \mathcal { V } }$ expresses the list of all probabilities for the vocabulary at the $j$ -th position of $i$ -th sentence. $\operatorname* { m a x } ( )$ and $\sigma ^ { 2 } ( \ u )$ represent the maximum and variance functions separately. If the values of maximum and variance are high, the model will be more confident about its output. The second term is a sequence decay we designed to increase the uncertainty of tokens when the length of the generated sequence grows. $\boldsymbol { l e n } ( \mathcal { D } )$ is the total number of tokens in the entire passage, and $l e n ( S _ { 1 : i - 1 } ) + j$ shows the position of the current token in the passage.

# Sentence-level Uncertainty

Previous works (Pagnoni, Balachandran, and Tsvetkov 2021; Kryscinski et al. 2020) illustrate that a major of hallucination in text generation is induced by the entity errors, such as false relations between two entities, inconsistent mentions in the context or basic factual errors, etc. This corresponds to our intuition that humans usually pay more attention to the salient information such as the keywords or entities for verification of the generated results. Therefore, recent researches turn to investigate the uncertainty of informative and important entities for hallucination detection. However, the complex dependencies over the entities are not well studied. In this paper, we explore the relations in the constructed semantic graph for uncertainty propagation and hallucination estimation.

Relation-based Uncertainty Propagation. Previous findings reveal that each token influences the surrounding context (Chen et al. 2017), thus the hallucination would probably propagate across the generated text. Zhang et al. (2023c) presents a hallucination propagation method that propagates the uncertainty score of preceding entity tokens to the current one. Whereas, this method roughly uses all the preceding entities, while ignoring their potential dependency relations with the current entity, which is inclined to overestimate the uncertainties by our preliminary studies. In this paper, we present a relation-based uncertainty propagation method and assume that the subject entity propagates its uncertainty to the object entity based on the predicate or relation in the semantic graph. Moreover, we devise a penalty factor based on the relation intensity to alleviate the uncertainty overestimation problem.

To be specific, given an object entity $o$ , we first search the entities that have semantic relations with $o$ from the semantic graph and obtain a set of triples as $\begin{array} { r l } { \mathcal { T } _ { o } } & { { } = } \end{array}$ $\{ ( s ^ { \prime } , v ^ { \prime } , o ) \overline { { | } } ( s ^ { \prime } , v ^ { \prime } , o ) \in \mathcal { T } _ { i } \}$ . $\mathcal { T } _ { i }$ is the set of triples in semantic graph of sentence $i$ . Intuitively, the subject entities are not equally important to the object entity, thus we leverage their attention scores as the weights for uncertainty propagation. To alleviate the overestimation problem, we additionally incorporate a relation intensity-based penalty factor for propagation. The final uncertainty of an object entity is formulated as:

$$
\mathcal { U } _ { p } ( o ) = \sum _ { ( s ^ { \prime } , v ^ { \prime } , o ) \in \mathcal { T } _ { o } } \frac { a t t ( s ^ { \prime } , o ) } { \mathcal { T } _ { o } } * \mathcal { U } ( s ^ { \prime } )
$$

where $a t t ( , )$ signifies the attention score between two tokens, $\scriptstyle { \mathcal { T } } _ { o }$ is a penalty factor that computes the relation intensity of all entities that have relations with the object $o$ , which can be measured as follows:

$$
\mathcal { T } _ { o } = \frac { 1 } { | \mathcal { T } _ { o } | } \sum _ { ( s ^ { \prime } , v ^ { \prime } , o ) \in \mathcal { T } _ { o } } \frac { a t t ( s ^ { \prime } , v ^ { \prime } ) + a t t ( v ^ { \prime } , o ) } { 2 }
$$

In general, high relation intensities usually indicate high factuality-confidence, thus the propagated uncertainties should be penalized.

Entity Uncertainty. For an entity $e _ { i } ^ { j }$ , the uncertainty score consists of the self-uncertainty (Formula 1) and the propagated uncertainty (Formula 2). The entity-based uncertainty of sentence $S _ { i }$ can be calculated by averaging the uncertainties of all entities in the sentence:

$$
\mathcal { U } _ { E } ( i ) = \frac { 1 } { \vert E _ { i } \vert } \sum _ { e _ { i } ^ { j } \in E _ { i } } \lbrack \mathcal { U } ( e _ { i } ^ { j } ) + \beta \mathcal { U } _ { p } ( e _ { i } ^ { j } ) \rbrack
$$

where $\mathcal { U } ( e _ { i } ^ { j } )$ and $\mathcal { U } _ { p } ( e _ { i } ^ { j } )$ show the self-uncertainty and propagated uncertainty respectively, and $\beta$ is a hyper-parameter to balance these two uncertainties.

Global Uncertainty. In addition to the entities, there are also many general tokens in the sentence. To capture the global information of the sentence, we also consider the uncertainties of all tokens (both entities and general tokens) in the sentence, and utilize the quantile approach to measure the global uncertainty, which is effective in capturing the global statistics in distributions (Gupta et al. 2024):

$$
\mathcal { U } _ { G } ( i ) = \mathtt { q u a } _ { \alpha } ( \mathcal { U } ( t _ { i } ^ { 1 } : t _ { i } ^ { n _ { i } } ) )
$$

where ${ \tt q u a } _ { \alpha } ( { \cal { U } } ( t _ { i } ^ { 1 } : t _ { i } ^ { n _ { i } } ) )$ is the $\alpha$ -quantile of the uncertainties of all tokens in sentence $\boldsymbol { S } _ { i }$ .

The uncertainty of the $i$ -th sentence is the interpolation sum of the entity-based uncertainty and the global uncertainty:

$$
\mathcal { U } _ { s } ( i ) = \lambda \mathcal { U } _ { E } ( i ) + ( 1 - \lambda ) \mathcal { U } _ { G } ( i )
$$

where $\lambda$ is an interpolation weight.

# Passage-level Uncertainty

Previous methods usually estimate the average uncertainty of all sentences for passage-level uncertainty. However, the intricate relations among the sentences are neglected, which could affect the detection of hallucination where two sentences contradict each other despite each sentence having low uncertainty. For example, the first sentence in a passage is ‘Thomas was born in 1972.’ and the fourth sentence is ‘He raced until 1968.’, which are contradictory in the passage. In this paper, we present a graph-based uncertainty calibration method that incorporates the relations of the sentence-centered sub-graph for uncertainty calibration. The calibrated uncertainties of all sentences are averaged as the passage-level uncertainty.

Graph-based Uncertainty Calibration. Intuitively, if a sentence contradicts all the neighbor sentences in the semantic graph, it will probably have inconsistency or conflicts in the context, which is prone to the hallucination problem. Thus, the uncertainty score should be increased. Motivated by this intuition, we present a graph-based uncertainty calibration method. First, we search the neighbor nodes for each sentence from the semantic graph. Then, we calculate the contradictory score for each connected sentence pair with a NLI model, namely DeBERTa-v3-Large (He, Gao, and Chen 2023), which is widely applied for natural language processing tasks. Finally, we incorporate the uncertainty of each sentence with the neighbor contradictory scores for passagelevel uncertainty computation:

$$
\mathcal { U } _ { p } = \frac { 1 } { \sum _ { i = 1 } ^ { m } \vert \mathcal { N } ( i ) \vert } \sum _ { i = 1 } ^ { m } \sum _ { j \in \mathcal { N } ( i ) } \mathcal { U } _ { s } ( i ) * \mathrm { N L I } ( c o n \vert S _ { j } , S _ { i } )
$$

where $\mathcal { N } ( i )$ reflects the neighbors of the $i$ -th sentence in the graph, ${ \mathrm { N L I } } ( c o n | , )$ is the contradiction probability between two sentences via the NLI model.

# Experimental Setup

Datasets We conduct extensive experiments on two datasets for hallucination detection. One is currently the latest and most widely used dataset WikiBio. To verify the effectiveness and generalization of our method, we also construct a Chinese dataset NoteSum, which can help boost research in this area. WikiBio (Manakul, Liusie, and Gales 2023) is a dataset derived from Wikipedia biographies. WikiBio applies the names from Wikipedia as the topics and generates corresponding biographies using GPT-3 (Floridi and Chiriatti 2020). Each sentence is annotated with one of the following labels: Factual (hallucination score: 0), NonFact\* (0.5), and NonFact (1), which indicates a sentence with no hallucination, with factual errors, and is irrelevant to the topic respectively. The entire passage also has a humanlabeled hallucination score as the ground truth. NoteSum is an industrial Chinese dataset. The company first collects users’ long text notes on various daily topics with numerous entities. We cooperate with the company and create shorter summaries from these long notes by LLMs for research. The private information of users is removed. It consists of both factuality and faithfulness hallucination as WikiBio. We also adopt the same annotation guideline with WikiBio. The statistics of the datasets are shown in Table 1.

Evaluation Metrics For fair comparison, we apply the evaluation metrics used in previous researches (Manakul, Liusie, and Gales 2023; Zhang et al. 2023c). Specifically, the area under curves (AUC) (Bradley 1997) are used to measure the performance of sentence-level hallucination detection. To evaluate the agreement between the passage-level hallucination score and human judgment, we employ the Pearson correlation coefficient (Cohen et al. 2009) and the Spearman correlation coefficient (Sedgwick 2014) to estimate the degree of consistency.

Table 1: Statistics of WikiBio and NoteSum. ‘Fact Halu Rate $( \% ) ^ { \dag }$ and ‘Faith Halu Rate $( \% )$ ’ demonstrate the proportion of sentences with factuality and faithfulness hallucination.   

<html><body><table><tr><td></td><td>WikiBio</td><td>NoteSum</td></tr><tr><td>Language</td><td>English</td><td>Chinese</td></tr><tr><td># Passages</td><td>238</td><td>200</td></tr><tr><td># Sentences</td><td>1908</td><td>1004</td></tr><tr><td>#Words/Sentence</td><td>17.49</td><td>33.38</td></tr><tr><td># Sentences/Passage</td><td>8.02</td><td>5.02</td></tr><tr><td>Halu Rate (%)</td><td>72.95</td><td>65.27</td></tr><tr><td>Fact Halu Rate (%)</td><td>33.07</td><td>27.94</td></tr><tr><td>Faith Halu Rate (%)</td><td>39.88</td><td>37.33</td></tr><tr><td></td><td></td><td></td></tr></table></body></html>

Baselines We compare our approach with the recent advanced baselines: 1) GPT-3 Uncertainties method uses the GPT-3 model to output the probability of each token, and then various uncertainty metrics are calculated as Manakul, Liusie, and Gales (2023) do for hallucination detection. 2) SelfCheckGPT (Manakul, Liusie, and Gales 2023) is the recent sampling-based method that relies on frequent sampling from LLMs for consistency checking. The gpt-3.5- turbo model is used and four methods are applied to measure the consistency, namely BertScore, QA, Unigram, and their combination. 3) FOCUS (Zhang et al. 2023c) is currently the outstanding uncertainty-based detection method. We leverage the LLaMA-13B and LLaMA-30B as the backbones.

Implementation Details we utilize a transition-based AMR parser (Xu, Lee, and Huang 2023) to construct an AMR graph for each sentence. Then, we perform coreference resolution and entity linking by spaCy to link sentencelevel AMR graphs together to obtain a passage-level graph for each passage. The DeBERTa-v3-Large (He, Gao, and Chen 2023) NLI model is used to calculate the contradiction probability in Formula 7. We experiment with the LLaMA13B and LLaMA-30B models to obtain the probability of each token. The hyper-parameters $\alpha , \beta , \lambda$ , and $k$ are set to 0.8, 0.65, 0.7, and 3 respectively.

# Results and Analyses

# Main Results

Table 2 shows the performance of our approach and the state-of-the-art baselines. We have the following observation. First, we achieve the best performance on both sentence-level and passage-level hallucination detection regarding all evaluation metrics. In particular, we gain a maximum improvement of $1 9 . 7 8 \%$ over the best baseline in passage-level hallucination detection. Second, compared with FOCUS that propagates the uncertainties of all preceding focused tokens to the subsequent one, our approach yields significant improvements especially for the NonFact\* and Factual types that have moderate and no hallucination respectively, indicating the effectiveness of our relationbased uncertainty propagation to help alleviate the overestimation problem. Third, our approach exhibits good crossdomain and cross-language generalization. It not only performs well on the English biography dataset WikiBio, but also reflects significant improvements on the Chinese note summary dataset NoteSum.

<html><body><table><tr><td></td><td colspan="5">WikiBio</td><td colspan="5">NoteSum</td></tr><tr><td>Methods</td><td>NonFact NonFact* Factual</td><td>sentence-level</td><td></td><td colspan="2">passage-level</td><td colspan="2">sentence-level</td><td></td><td colspan="2">passage-level</td></tr><tr><td></td><td></td><td></td><td></td><td>Pearson</td><td>Spearman</td><td>NonFact NonFact* Factual</td><td></td><td></td><td>Pearson</td><td>Spearman</td></tr><tr><td colspan="9">GPT-3Uncertainties</td><td></td><td></td></tr><tr><td>Avg(-logp)</td><td>83.21</td><td>38.89</td><td>53.97</td><td>57.04</td><td>53.93</td><td>80.11</td><td>43.69</td><td>35.29</td><td>39.61</td><td>31.55</td></tr><tr><td>Avg(H)</td><td>80.73</td><td>37.09</td><td>52.07</td><td>55.52</td><td>50.87</td><td>80.08</td><td>43.95</td><td>38.04</td><td>40.36</td><td>33.25</td></tr><tr><td>Max(-logp) Max(H)</td><td>87.51</td><td>35.88</td><td>50.46</td><td>57.83</td><td>55.69</td><td>79.86</td><td>40.17</td><td>36.70</td><td>38.13</td><td>34.75</td></tr><tr><td></td><td>85.75</td><td>32.43</td><td>50.27</td><td>52.48</td><td>49.55</td><td>81.02</td><td>47.33</td><td>39.03</td><td>42.88</td><td>37.24</td></tr><tr><td colspan="9">SelfCheckGPT (gpt-3.5-turbo)</td><td></td><td></td></tr><tr><td>BertScore</td><td>81.96</td><td>45.96</td><td>44.23</td><td>58.18</td><td>55.90</td><td>76.44</td><td>39.69</td><td>36.89</td><td>25.91</td><td>21.24</td></tr><tr><td>QA</td><td>84.26</td><td>40.06</td><td>48.14</td><td>61.07</td><td>59.29</td><td>79.69</td><td>45.30</td><td>39.32</td><td>41.07</td><td>36.54</td></tr><tr><td>Unigram (max)</td><td>85.63</td><td>41.04</td><td>58.47</td><td>64.71</td><td>64.91</td><td>79.48</td><td>43.88</td><td>36.15</td><td>38.80</td><td>33.35</td></tr><tr><td>Combi</td><td>87.33</td><td>44.37</td><td>61.83</td><td>69.05</td><td>67.77</td><td>82.38</td><td>53.19</td><td>40.17</td><td>47.79</td><td>41.27</td></tr><tr><td colspan="9">FOCUS</td><td></td><td></td></tr><tr><td>LLaMA-13B</td><td>87.90</td><td>43.84</td><td>62.46</td><td>70.62</td><td>63.03</td><td>81.11</td><td>49.98</td><td>38.88</td><td>38.17</td><td>38.31</td></tr><tr><td>LLaMA-30B</td><td>89.79</td><td>48.80</td><td>65.69</td><td>77.15</td><td>73.24</td><td>82.17</td><td>43.12</td><td>49.85</td><td>37.37</td><td>40.09</td></tr><tr><td colspan="9">OURS</td><td></td><td></td></tr><tr><td>LLaMA-13B</td><td>90.14</td><td>61.65</td><td>64.82</td><td>72.11</td><td>64.35</td><td>85.06</td><td>50.70</td><td>53.03</td><td>55.62</td><td>60.81</td></tr><tr><td>LLaMA-30B</td><td>90.93</td><td>61.16</td><td>65.70</td><td>77.60</td><td>74.44</td><td>87.95</td><td>54.42</td><td>61.51</td><td>54.77</td><td>61.05</td></tr><tr><td>△</td><td>+1.14</td><td>+12.85</td><td>+0.01</td><td>+0.45</td><td>+1.20</td><td>+5.57</td><td>+1.23</td><td>+11.66</td><td>+7.83</td><td>+19.78</td></tr></table></body></html>

Table 2: Comparison results of our approach and the recent hallucination detection methods. The best results are in bold and the second best is marked with underline. $\Delta$ indicates our maximum improvements over the best baselines.

# Ablation Studies

We conduct ablation studies on WikiBio with LLaMA-30B from three dimensions: token, sentence, and passage. Experimental results are shown in Table 3. For each row, one setting is removed while keeping the other settings unchanged.

We have the following observations: (1) By removing each element from Formula 1 respectively, the performance decreases significantly in most cases, which signifies the effectiveness of the maximum, variance, and decay term for modeling the token-level uncertainty. (2) The performance with the passage-level metrics drops more significantly with the setting of ‘- max’, manifesting that the maximum probability can better capture the key features for hallucination detection, while other terms can help further refine the uncertainty. (3) Both the entity uncertainty computed by relationbased propagation and the global uncertainty are important to sentence-level detection. In addition, entity uncertainty is more effective than global uncertainty for passage-level detection. (4) By excluding the contradiction relations of the neighbor sentences in the semantic graph, the performance of passage-level hallucination detection significantly drops by about 2 points, which further verifies the effectiveness of our graph-based uncertainty calibration for detecting hallucination over the passage.

Table 3: Results of ablation studies on WikiBio. ‘- max’, ‘- var’ and ‘- decay’ mean removing the maximum, variance and decay term from Formula 1. ‘- entity’ and ‘- global’ reveal removing the entity and global uncertainty respectively from Formula 6. ‘- graph’ indicates not including the contradiction probability of the neighbors in the graph, i.e., averaging the uncertainties of all sentences in Formula 7.   

<html><body><table><tr><td></td><td colspan="2">sentence-level</td><td colspan="2">passage-level</td></tr><tr><td></td><td>NonFact</td><td>NonFact*</td><td>Fact Pear.</td><td>Spear.</td></tr><tr><td>Ours</td><td>90.93</td><td>61.16</td><td>65.70</td><td>77.60 74.44</td></tr><tr><td>- max</td><td>86.48</td><td>64.86</td><td>63.52 23.32</td><td>38.57</td></tr><tr><td>- var</td><td>90.17</td><td>50.94</td><td>64.82 75.60</td><td>72.36</td></tr><tr><td>- decay</td><td>89.01</td><td>43.57</td><td>63.48 70.19</td><td>66.49</td></tr><tr><td>- entity</td><td>88.31</td><td>43.06</td><td>63.10 65.81</td><td>60.34</td></tr><tr><td>- global</td><td>88.75</td><td>43.88</td><td>65.19 70.36</td><td>65.49</td></tr><tr><td>- graph</td><td></td><td></td><td>75.89</td><td>72.20</td></tr></table></body></html>

1.0 3 .8\~{><>>\~3>>   
3Rg6 WW W 0.6   
uncertainty score 0.4 NonFact / OURS NonFact / FOCUS 0.2 Nonfact\* / OURS NonFact\* / FOCUS Fact / OURS sentence id Fact / FOCUS 0.0 0 400 800 1200 1600 2000

![](images/6641edf9e93d4b05f283e325fc57e4ab16bc3207a6e2fe2e1f4258091b1903a6.jpg)  
Figure 3: The uncertainty scores of three types of samples calculated with FOCUS and ours.   
Figure 4: Visualization of the entity uncertainty and global uncertainty for three types of samples.

# Further Analyses

Effect of Relation-based Uncertainty Propagation. To further investigate the effectiveness of our relation-based uncertainty propagation method, we compare with the baseline FOCUS (Zhang et al. 2023c) that propagates the uncertainties of all preceding keywords to the subsequent one. The results are shown in Figure 3, illustrating the uncertainty scores of three types of samples from WikiBio measured by FOCUS and ours respectively. We can observe that both of the two methods yield high uncertainty scores for the samples with NonFact (ground truth score $\mathbf { \Psi } = 1 \mathbf { \dot { \Psi } }$ ), which can help well identify the severe hallucination. It is also notable that the FOCUS method tends to overestimate the uncertainties for the samples with NonFact\* (ground truth score $\mathbf { \tau } = \mathbf { \tau }$ 0.5) and Fact (ground truth score $\mathbf { \tau } = 0$ ). There is a large gap between the estimated uncertainties and the ground truth. Moreover, the uncertainties of the three types calculated by FOCUS are very close, making it difficult to identify hallucination in different degrees. In contrast, our approach effectively diminishes the uncertainties for samples with NonFact\* and Fact, which further verifies the effectiveness of our relation-based uncertainty propagation in alleviating the overestimation problem.

Visualization of Entity and Global Uncertainty. To examine the effect of entity and global uncertainty for sentence-level hallucination detection, scores of the two uncertainties are visualized for three types of samples from WikiBio in Figure 4. We observe that with the increased degree of hallucinations $( \mathrm { F a c t }  \mathrm { N o n F a c t ^ { * } }  \mathrm { N o n F a c t } )$ , both types of uncertainty scores increase. Moreover, there are fewer overlaps in the three types of samples based on entity uncertainty and global uncertainty. In other words, the three types of samples can be well distinguished by the two uncertainties. All these observations demonstrate the effectiveness of our entity and global uncertainty.

![](images/260cae9112e502bde287954dc5da207aebe17be7561613e3b603f93154e5fe78.jpg)  
Figure 5: The Pearson and Spearman metrics of ours and the compared methods for passage-level uncertainty calculation.

Effect of Graph-based Uncertainty Calibration. To verify the effectiveness of our graph-based uncertainty calibration, we compare it with other two methods, namely Adjacent and Average. The Adjacent method merely incorporates the relations between the current sentence and the previous as well as the next sentence for uncertainty calculation, while the Average method simply measures the average uncertainties of all sentences. The results of the two methods and ours are shown in Figure 5. Our method is observed to outperform Adjacent and Average in terms of Pearson and Spearman correlations, indicating the effectiveness of using the semantic graph to model the long-range sentence relations for passage-level hallucination detection. In addition, the performance of Adjacent and Average is close, indicating the limits of merely considering the adjacent sentences.

# Conclusions

In this paper, we propose a method to enhance uncertainty modeling with semantic graph for hallucination detection. Extensive experiments verify the effectiveness of each component of our approach. In particular, our approach consistently outperforms the state-of-the-art baselines in both sentence-level and passage-level hallucination detection, by incorporating the semantic relations among entities and sentences into the uncertainty calculation framework. It is also interesting to find that our relation-based uncertainty propagation method can help effectively alleviate the uncertainty overestimation problem and our graph-based uncertainty calibration method can capture long-range relations. In the future, we will explore integrating the existing knowledge graph with AMR graphs for fact-checking and hallucination detection.

# Ethics Statement

Our WikiBio dataset is publicly used in the field of natural language processing. The NoteSum dataset, on the other hand, is an internal private dataset of Xiaohongshu, and its construction, annotation, and review are all handled by Xiaohongshu’s own employees. The method presented in this paper is original to the authors and does not involve any ethical issues.