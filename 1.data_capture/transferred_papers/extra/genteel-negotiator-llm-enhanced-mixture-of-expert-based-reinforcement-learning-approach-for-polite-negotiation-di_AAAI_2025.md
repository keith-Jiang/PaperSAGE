# GENTEEL-NEGOTIATOR: LLM-Enhanced Mixture-of-Expert-Based Reinforcement Learning Approach for Polite Negotiation Dialogue

Priyanshu Priya1, Rishikant Chigrupaatii1, Mauajama Firdaus2, Asif Ekbal1,3

1Department of Computer Science and Engineering, Indian Institute of Technology Patna, India 2Department of Computer Science and Engineering, Indian Institute of Technology (Indian School of Mines) Dhanbad, India 3School of Artificial Intelligence and Data Science, Indian Institute of Technology Jodhpur, India {priyanshu 2021cs26, rishikant 2101cs66}@iitp.ac.in, mauajama $@$ iitism.ac.in, asif@{iitp,iitj}.ac.in

# Abstract

Developing intelligent negotiation dialogue systems that resolve conflicts and promote equitable, inclusive, and sustainable outcomes is at the forefront of advancing automated negotiation technology for social good. Negotiation involves balancing cooperation and competition to maximize value without causing offense. Using polite language fosters mutual understanding and creates a respectful and collaborative environment essential for successful negotiations in various domains. Considering this, in this paper, we propose a polite negotiation dialogue system, GENTEEL-NEGOTIATOR for social good applications to boost the overall quality of negotiation outcomes. We focus on developing a negotiation dialogue system for two key application areas, namely tourism and ecommerce. We begin by curating a unique negotiation dialogue dataset, NEGOCHAT for tourism. We further enrich the NEGOCHAT and Integrative Negotiation Dataset (IND) for ecommerce with various negotiation strategies. These datasets are then used to develop the GENTEEL-NEGOTIATOR, leveraging the Large Language Model (LLM) and mixture-ofexpert (MoE)-based reinforcement learning approach. The proposed MoE-based method employs heuristic experts dedicated to negotiation, politeness, and dialogue coherence to facilitate the learning of diverse semantics by analyzing the dialogue context. A novel reward function with negotiation strategy congruence, politeness, dialogue coherence, and engagingness rewards is designed to guide the policy’s learning for generating responses. Automatic and human evaluations on NEGOCHAT and IND datasets validate the effectiveness of GENTEEL-NEGOTIATOR in generating polite responses during negotiation while maintaining conversation goals, including coherence and engagingness.

# Introduction

Negotiation dialogue systems have garnered significant interest in recent years due to their wide-ranging real-world applications (Yamaguchi, Iwasa, and Fujita 2021; Priya et al. 2024b). These systems can potentially revolutionize fields, such as e-commerce (Sree et al. 2023), conflict resolution (Kaur et al. 2022), and customer service (Stock, Petukhova, and Klakow 2022), where effective negotiation is critical. Techniques like reinforcement learning (RL) (Zhao, Xie, and Eskenazi 2019) and using large language models (LLMs) (Chen et al. 2023) have enabled the

Ilike the refrigerator you're selling,but the price is a bit higher than what I was expecting.Can you offer any discounts? The price is what it is,Ican't do anything.Let me know if 中 that works for you. Generic Response   
I can give $5 \%$ discount, making it \$9oo.Let me know if that 中 works for you. Negotiation Strategy Iunderstand your concern,and I appreciate your interest. oriented Response   
However, this is the final price; Iam unable to do anything. 中 Let me know if that works for you. X Polite Response Iunderstand your concern,and Iappreciate your interest. I'll do my best to accommodate your budget. How about a $5 \%$   
discount, bringing the price down to \$9oo? Please let me know if this works for you. Negotiation Strategyoriented Polite Respons

creation of systems that can simulate human-like negotiation strategies (Lewicki, Barry, and Saunders 2016), making them more adaptable and effective in complex scenarios. The inherent complexity of human negotiation (Morris and Gelfand 2004), which involves a nuanced interplay of negotiation strategies (Schoenfield 1983), and contextual understanding (Horton 2012), presents an intellectually stimulating challenge.

One critical aspect of successful negotiation in human and automated systems is the strategic use of politeness (Lee, Mason, and Malcomb 2021). Politeness (Terada, Okazoe, and Gratch 2021) plays a vital role in maintaining a cooperative atmosphere and fostering mutual respect between negotiating parties. In negotiation dialogue systems, incorporating politeness strategies (Brown and Levinson 1987) contributes to mitigating conflicts by reducing the likelihood of offending opponents, thereby maintaining or enhancing sociopsychological closeness. Figure 1 depicts a conversation snippet showcasing the relevance of negotiation strategy and politeness modeling during negotiation.

Influenced by the importance of strategies and politeness in negotiation, we introduce GENTEEL-NEGOTIATOR, a novel polite negotiation dialogue system to infuse politeness into negotiation conversations while being coherent and engaging using an LLM-enhanced Mixture-of-Expert (MoE)- based RL approach. GENTEEL-NEGOTIATOR includes (a) an LLM for learning diverse semantics from the dialogue context; (b) three experts viz. negotiation, politeness, and keyterm experts to infuse politeness during negotiation while ensuring dialogue coherence; (c) an RL-based agent to strategically select experts based on an expert determination policy for generating responses. To generate engaging, coherent, polite responses with pertinent strategy, we devise a new reward function with negotiation strategy congruence, politeness, dialogue coherence, and engagingness rewards.

These automated negotiation dialogue systems are highly beneficial in high-volume sectors like e-commerce and tourism, where routine negotiations are common. These systems enhance operational efficiency and support the United Nations’ Sustainable Development Goals (UNWTO 2024). To advance research in this area, we introduce NEGOCHAT, a new negotiation dialogue dataset for the tourism domain, generated through prompting the Gemini (Team et al. 2023) LLM. Further, we annotate both NEGOCHAT and the Integrative Negotiation Dataset (IND) (Ahmad et al. 2023) for e-commerce with novel negotiation strategies using ChatGPT (OpenAI 2024) accompanied by human intervention, thereby creating a comprehensive resource for analyzing negotiation behaviors in these domains.

In summary, the key contributions are outlined as follows: (i) Investigate the use of politeness by the dialogue agent on negotiation outcomes. To the best of our knowledge, this study pioneers the strategic modeling and analysis of politeness effects within negotiation conversations; (ii) Introduce NEGOCHAT, a new negotiation dialogue dataset for the tourism domain, generated using LLM with minimal manual intervention. We then automatically annotate NEGOCHAT and IND datasets with negotiation strategies using LLM prompting and human-in-the-loop techniques; (iii) Present GENTEEL-NEGOTIATOR, an LLM-enhanced MoEbased RL model to incorporate politeness into negotiation conversations while ensuring coherence and user engagement; (iv) Design a novel reward function consisting of negotiation strategy congruence, politeness, dialogue coherence, and engagingness rewards to generate engaging, coherent, and polite responses during negotiation1.

# Related Work

Negotiation has been actively studied in diverse research areas, including game theory (Nash Jr 1950), economics (Carnevale and Pruitt 1992), and psychology (Adair, Okumura, and Brett 2001). Recently, the human-agent negotiation dialogue systems have become the center of automated negotiation literature $\mathrm { F u }$ et al. 2023). There has been a growing emphasis on employing LLMs (Fu et al. 2023; Abdelnabi et al. 2023) to develop automated negotiation agents. Moreover, Mixtures-of-Experts (MoEs) (Cai et al. 2024) that utilize a collection of $n$ “expert” sub-networks have become integral to the design of LLMs (Obando-Ceron et al. 2024).

In recent times, the integration of politeness into dialogue systems has emerged as a critical area of research, reflecting its importance in enhancing user experience and achieving successful interactions (Silva, Semedo, and Magalha˜es 2022; Mishra, Priya, and Ekbal $2 0 2 3 \mathrm { a }$ ; Priya et al. 2024a). Politeness contributes to considerate and positive relationships (Golchha et al. 2019; Priya et al. 2023; Mishra, Priya, and Ekbal 2023b) and has been proven essential for effective negotiation (Terada, Okazoe, and Gratch 2021). Politeness, as a key social behavior, facilitates the development of congenial and respectful relationships, fosters mutual trust and understanding, and assists in developing rapport and cooperation during negotiation (Maaravi, Idan, and Hochman 2019).

While the existing studies have made significant strides in developing negotiation dialogue systems that model the opponent’s strategy, integrating other human-like aspects, particularly politeness, into these systems remains largely unexplored. We introduce a new LLM-enhanced MoE-based RL approach that combines the advantages of LLMs, MoEs, and RL to develop a robust polite negotiation dialogue system. Further, our proposed system is designed to be scalable and adaptable across various domains.

# Dataset

We conduct experiments using two negotiation dialogue datasets, viz. the newly curated NEGOCHAT dataset and the Integrative Negotiation Dataset (IND) (Ahmad et al. 2023). NEGOCHAT Dataset Preparation. The proposed NEGOCHAT dataset comprises dialogues between the agent and the users, specifically centered on negotiation within the tourism domain. This dataset is designed to capture the complexities involved in negotiating various components of tourism packages, including price, destinations, transportation, and additional amenities and services. By detailing these intricate interactions, NEGOCHAT provides a comprehensive resource for analyzing and enhancing dialogue systems that aim to handle real-world negotiation scenarios in the tourism sector. The dataset is developed by utilizing the extensive knowledge embedded in the LLMs to reduce dependence on expensive and limited human resources. Specifically, the dataset is generated through fewshot prompting of Gemini-1.5-Flash (Team et al. 2023) model, with subsequent human oversight to ensure quality control. The entire user-agent dialogue dataset creation process consists of two key stages: (a) Drafting Sample Dialogues, and (b) Generating Dialogues via Prompting.

(a) Drafting Sample Dialogues. To draft sample dialogues, we initially gather information about various travel packages by referring to several well-known travel websites. Based on this data, we develop 10 distinct travel packages. Each package includes the package name, a detailed description, and information on the associated aspects, amenities, services, and pricing. These details are then used to create 50 negotiation dialogues (5 dialogues for each package) between the user and the agent using a Wizard-of-Oz approach (Kelley 1984), where one human subject assumes the role of the user, and the other acts as the agent. Along with each dialogue, we maintain a brief “conversation metadata”, which includes a package information and background information for two interlocutors. The package information represents the package name and corresponding description. The background information includes more fine-grained details relevant to the package, such as a list of included aspects, amenities, and services with their descriptions. These sample dialogues are created by three human subjects under the supervision of a domain specialist with business and sales background to ensure precision and relevance within specified context. These subjects possess Ph.D. degrees in Linguistics and have extensive knowledge of dialogue and negotiation concepts. ‘Guidelines for Drafting Sample Dialogues’ are provided in “Dataset Details” section of the appendix.

Table 1: The definition of different negotiation strategies. Examples are given in “Dataset Details” section of the appendix.   

<html><body><table><tr><td>Type</td><td>Nergotiation</td><td>Definition</td></tr><tr><td rowspan="4">SuIAIOs</td><td>Active listening</td><td>Refersto comprehending the needsand perspectives oftheopponent toresolve misunderstandings,buildrapport,and develop mutually beneficial solutions.</td></tr><tr><td>Leverageinforma- tion</td><td>Referstotheuseof credentials,pastsuccesss,andaconsistentrecordof integrityand professionalismtoestablish credibility, gain the opponent's trust,and make the proposal more persuasive.</td></tr><tr><td>Logrolling</td><td>Refers toconcedingonlower-priorityisues togainconcessions onhigher-priorityissues fromtheopponenttofacilitate</td></tr><tr><td>Expanding-the-pie</td><td>a mutually beneficial agreement through trade-offs that addressboth parties’ key interests and preferences. Refers toenhancing theoverallvalueof thedealduring negotiationtotransition fromazero-sum gametoanintegrative</td></tr><tr><td rowspan="5"></td><td>Gradual</td><td>approach that seeks mutual benefit and collaborative value creation. Involvesofferingsall,incrementaloncessionstosowwillingness toompromise,maintaingotiationleverage,</td></tr><tr><td>conecession-making</td><td>Invoseea'ite essillae</td></tr><tr><td>concession-making Patterned</td><td>urgency,thereby encouraging the other party to make their own concessions. Involves making concessions ina predictableand structured waysuchasprogressivelyreducing range/frequency to</td></tr><tr><td>concession-making</td><td></td></tr><tr><td>No strategy</td><td>signal nearing limits of one party and encourage the opponent to agree before concessions become minimal. Refers to utterances that do not employ any specific negotiation strategy.</td></tr></table></body></html>

(b) Generating Dialogues via Prompting. The created sample dialogues are utilized to prompt Gemini-1.5-Flash LLM to generate synthetic dialogues in a few-shot setup. To finalize the prompt, we experimented with four manually designed prompts, including natural language instructions, few shot exemplars (three randomly selected conversation metadata and their corresponding dialogues from the aforementioned pool of sample dialogues), and the target dialogue metadata. For each prompt, we generate 20 dialogues by prompting Gemini with Top- $p$ sampling (Holtzman et al. 2019) with $p$ $= 0 . 9 5$ and temperature $\tau = 1 . 0$ . These generated dialogues are then manually evaluated by the same three human subjects for quality in terms of negotiation efficacy on a scale of 1-3 (1-low, 2-moderate, 3-high). An inter-subject Kappa (McHugh 2012) agreement ratio of $8 1 . 6 \%$ is observed among the subjects. The prompt generating the highest number of dialogues with a score of 3 is selected as the final prompt. This prompt is then used to prompt the Gemini model for the generation of the entire dialogue dataset. ‘Dialogue Filtering and Quality Assessment’ and a sample dialogue are given in “Dataset Details” section of the appendix.

Integrative Negotiation Dataset (IND). The IND dataset (Ahmad et al. 2023) contains integrative negotiation dialogues focused on the e-commerce domain. The dialogues in IND primarily encompass negotiations on 10 distinct electronic items and their corresponding accessories.

Dataset Annotation Scheme. Negotiation conversations require carefully crafted strategies to assess and select from various potential actions effectively. To achieve successful negotiation and foster collaborative outcomes, the negotiators must be adept at adapting their negotiation strategies to the prevailing circumstances. Considering this, we devise a set of eight negotiation strategies arranged in a hierarchy to reflect the negotiation behavior of the involved parties. These strategies are informed by negotiation theory (Bazerman 1994) and a preliminary analysis of 40 dialogues each from the NEGOCHAT and IND datasets. The above-mentioned three human subjects independently analyzed and labeled the dialogues, discussed discrepancies, and refined the strategies accordingly. The $\kappa$ (Fleiss 1971) score in the range of $0 . 3 < \kappa < 0 . 7$ for all categories indicates moderate to fair inter-subject agreement according to (McHugh 2012). Each dialogue utterance is categorized into three types: problem-solving, concession-making, and nostrategy. Table 1 provides definitions for the strategies under these categories. Dataset annotation procedure and dataset statistics are given in the “Dataset Details” section of the appendix.

# Methodology

The overall architecture of the proposed GENTEELNEGOTIATOR is depicted in Figure 2.

Dialogue Encoder. The dialogue encoder is implemented with LLaMA-3.1-8B-Instruct (Touvron et al. 2023). For a given the dialogue context, $C = \{ u _ { 1 } , a _ { 1 } , . . . , u _ { m - 1 } , a _ { m - 1 } \}$ (an alternating sequence of $( m - 1 )$ utterances between user $( u )$ and dialogue agent $( a )$ ), and the target utterance $u _ { m }$ , the goal is to generate the response $a _ { m } ( = r )$ . We concatenate $C$ and $u _ { m }$ and prepend a $\left[ C L S \right]$ token to form the input sequence, $X ~ = ~ [ C L S ] \oplus { \bar { C } } \oplus { \bar { u } } _ { m }$ , which is fed into the dialogue encoder to obtain hidden state $\mathcal { H } _ { X }$ , with the $\left[ C L S \right]$ token’s representation designated as $h _ { X }$ .

Negotiation Experts. To effectively adapt negotiation strategies and secure win-win outcomes, we integrate negotiation experts with both contextual and future strategy predictions. Each utterance in the dialogue is annotated with a negotiation strategy label, which serves as the supervised label for the negotiation strategy prediction task. We transform the contextual negotiation expert using the MLP layer:

$$
\bar { \mathcal { H } } _ { X } ^ { c t x - n e g o } = \mathbf { \bar { M } L P } _ { n e g o } ( \mathcal { H } _ { X } )
$$

We then project the $\left[ C L S \right]$ representation $h _ { X } ^ { c t x - n e g o }$ of the

[..] Im interested in the refrigerator you're selling,but the price is a bit higher than what I was expecting. Can you offer any discounts? Dialogue Context (C) State Update Sk+1 Response Decoder sE State(s) E Response LLM-based Dialogue Encoder Expert Decoder Contextual and Future Negotiation Strategy ak Congruence Rewards Expert Determination Policy Network Dialogue-level and utterancelevelPolitenessRewards Ak Contextual and Future Dialogue Negotiation Politeness Keyterm R Coherence Rewards Expert Expert Expert Engagingness Reward Iunderstand your concern,andIappreciateyour interest.Illdo my best toaccommodate yourbudget.Howabouta $\widehat { 5 \% }$ B discount, bringing the price down to \$9oo? Please let me know if this works for you.

negotiation expert to predict the negotiation strategy, $\bar { \mathcal { P } } ^ { n e g o } = \mathrm { s o f t m a x } ( W ^ { n e \bar { g } o } h _ { X } ^ { c t x - n e g o } )$ 4 which is guided by the negotiation strategies accumulated in the $\mathcal { E } ^ { \mathrm { n e g o ^ { * } } }$ set of the negotiator’s last utterance in the dialogue context by applying cross-entropy loss:

$$
\mathcal { L } ^ { c t x - n e g o } = - \frac { 1 } { \vert \mathcal { E } ^ { \mathrm { n e g o } ^ { * } } \vert } \sum _ { i = 1 } ^ { \vert \mathcal { E } ^ { \mathrm { n e g o } ^ { * } } \vert } \log \mathcal { P } ^ { n e g o } ( \mathcal { E } _ { i } ^ { \mathrm { n e g o } ^ { * } } )
$$

For future negotiation experts, we employ the same approach to compute $\mathcal { L } ^ { f t r - n \overline { { e } } g o }$ loss and train it to predict the negotiation strategy of the negotiator’s future utterance. This enables the negotiation experts to learn diverse negotiation features through $\mathcal { L } _ { n e g o }$ loss: $\mathcal { L } _ { n e g o } = \mathcal { L } ^ { c t x - n e g o } + \mathcal { L } ^ { \bar { f } t r - n e g o }$ . Politeness Experts. To monitor possible transitions in the negotiator’s polite behavior, politeness experts are associated with contextual and future negotiators’ politeness strategy predictions. We use linguistic features, indicative of politeness drawn from Brown and Levinson (1987), which cover two main types of politeness strategies: positive and negative. Following Danescu-Niculescu-Mizil et al. (2013), we extract $N$ detailed politeness strategies for each utterance in the dataset by pattern matching on the dependency parses of utterances. Since politeness strategies are often the linguistic markers (e.g., hey, thank, understand, etc.), we identify the politeness strategy of each word as positive or negative according to its linguistic feature (e.g. hedges, direct question, etc.). The high-frequency categories are used as supervised labels for politeness strategy prediction task.

We split contextual politeness experts into two types: positive and negative politeness experts, by passing them through two distinct MLP layers, which transform $\mathcal { H } _ { X }$ into $\mathcal { H } _ { X , p o s } ^ { c t x - p o l }$ and $\mathcal { H } _ { X , n e g } ^ { c t x - p o l }$ , respectively.

$$
\begin{array} { r } { \overbrace { \mathcal { H } _ { X , p o s } ^ { c t x - p o l } } ^ { \mathcal { H } _ { X , p o s } ^ { c t x - p o l } } = \mathbf { M } \mathbf { L } \mathbf { P } _ { p o s - p o l } ( \mathcal { H } _ { X } ) , \mathcal { H } _ { X , n e g } ^ { c t x - p o l } = \mathbf { M } \mathbf { L } \mathbf { P } _ { n e g - p o l } ( \mathcal { H } _ { X } ) } \end{array}
$$

We utilize $\left[ C L S \right]$ representations $h _ { X , p o s } ^ { c t x - p o l }$ and $h _ { X , n e g } ^ { c t x - p o l }$ derived from positive and negative experts to identify positive politeness and negative politeness strategy, respectively:

$$
\begin{array} { r } { \mathcal { P } ^ { p o s - \smile } { = } \_ { \mathrm { o f t m a x } } ( W ^ { p o s - p o l } h _ { X , p o s } ^ { c t x - p o l } ) } \\ { \mathcal { P } ^ { n e g - p o l } = \operatorname { s o f t m a x } ( W ^ { n e g - p o l } h _ { X , n e g } ^ { c t x - p o l } ) } \end{array}
$$

which is guided by positive and negative politeness strategy

accumulated in posl and npeogl sets of the negotiator’s last utterance in dialogue context by applying cross-entropy loss:

$$
\mathcal { L } _ { p o s } ^ { c t x - p o l } = - \frac { 1 } { | \mathcal { E } _ { \mathrm { p o s } } ^ { \mathrm { p o l } ^ { * } } | } \sum _ { i = 1 } ^ { | \mathcal { E } _ { \mathrm { p o s } } ^ { \mathrm { p o l } ^ { * } } | } \log \mathcal { P } ^ { p o s - p o l } ( \mathcal { E } _ { i } ^ { \mathrm { p o l } ^ { * } } )
$$

$$
\mathcal { L } _ { n e g } ^ { c t x - p o l } = - \frac { 1 } { | \mathcal { E } _ { \mathrm { n e g } } ^ { \mathrm { p o l } ^ { * } } | } \sum _ { i = 1 } ^ { | \mathcal { E } _ { \mathrm { n e g } } ^ { \mathrm { p o l } ^ { * } } | } \log \mathcal { P } ^ { n e g - p o l } ( \mathcal { E } _ { i } ^ { \mathrm { p o l } ^ { * } } )
$$

It is important to acknowledge that individuals may interpret and perceive the level of politeness in an utterance differently based on their cognitive differences (Escandell-Vidal 1996; Holtgraves 2005). For future politeness experts, we employ the same approach to compute the losses, $\mathcal { L } _ { p o s } ^ { f t r - p o l }$ and $\mathcal { L } _ { n e g } ^ { \bar { f } t r - p o l }$ , and then train them to predict the politeness of the negotiator’s future utterance (i.e., the subsequent utterance). This enables the politeness experts to assimilate diverse politeness-related characteristics through $\mathcal { L } _ { p o l }$ loss: $\begin{array} { r } { \mathcal { L } _ { p o l } = \dot { \mathcal { L } } _ { p o s } ^ { c t x - p o l } + \mathcal { L } _ { n e g } ^ { c t x - p o l } + \mathcal { L } _ { p o s } ^ { f t r - p o l } + \mathcal { L } _ { n e g } ^ { f t r - p o l } } \end{array}$ .

Keyterm Experts. To ensure dialogue coherence, keyterm experts are linked with keyterm predictions that help maintain coherence with both contextual and future utterances. For this, we create a bidirectional polite keyterm graph $\mathcal { G }$ (an example is given in “Bidirectional Polite Keyterm Graph” section in the appendix), which also aids in designing coherence rewards. We extract the pertinent keyterms from each utterance within the corpus and identify their politeness strategy through pattern matching on the dependency parse tree of utterances (Danescu-Niculescu-Mizil et al. 2013). The pointwise mutual information (PMI) (Church and Hanks 1990) is employed to create bidirectional edges that reflect the relationships between keyterm pairs. Specifically, the forward edge represents keyterm pairs retrieved from context and response, while the backward edge pertains to pairs drawn from future utterances and responses. Further, positive edges are created to denote keyterms associated with positive politeness strategies, while negative edges correspond to those indicative of negative politeness strategies. Each head vertex then selects tail vertices with the highest PMI scores to form connections. The vertices of $\mathcal { G }$ are utilized as supervised labels for keyterm prediction task.

Contextual keyterm experts undergo a similar transformation as politeness experts. Their $[ \zeta L S ]$ representations, $h _ { X , p o s } ^ { c t x - k t }$ and $h _ { X , n e g } ^ { c t x - k t }$ can be derived from the positive and negative keyterm experts, $\mathcal { H } _ { X , p o s } ^ { c t x - k t }$ and $\mathcal { H } _ { X , n e g } ^ { c t x - k t }$ , respectively. We deduce the one-hop neighbors of contextual keyterms from ‘forward-positive’ and ‘forward-negative’ relations in $\mathcal { G }$ to improve insights for target keyterms in gold response. We employ attention mechanisms (Bahdanau, Cho, and Bengio 2014) to acquire fused embeddings $\mathcal { E } _ { p o s } ^ { c t x - k t }$ and $\mathcal { E } _ { n e g } ^ { c t x - k t }$ :

$$
\begin{array} { r } { \bar { \mathcal { E } } _ { p o s } ^ { c t x - k t } = \mathrm { A t t e n t i o n } ( h _ { X , p o s } ^ { \overline { { c } } t x - \overline { { k } } t } , \mathrm { E } _ { p o s } ^ { c t x - k t } ) } \\ { \bar { \mathcal { E } } _ { n e g } ^ { c t x - k t } = \mathrm { A t t e n t i o n } ( h _ { X , n e g } ^ { c t x - k t } , \mathrm { E } _ { n e g } ^ { c t x - k t } ) } \end{array}
$$

where, $\mathtt { E } _ { p o s } ^ { c t x - k t }$ and $\mathbf { E } _ { n e g } ^ { c t x - k t }$ are embedding matrices for positive and negative neighbors, respectively, sharing parameters with dialogue encoder. We further concatenate Epctoxs−kt and $\mathcal { E } _ { n e g } ^ { c t x - k t }$ with $\mathcal { H } _ { X , p o s } ^ { c t x - k t }$ and $\mathcal { H } _ { X , n e g } ^ { c t x - k t }$ , respectively, at token level and then employ MLP layers to integrate them, yielding keyterm-enhanced experts, $\mathcal { H } _ { X , p o s - k t } ^ { c t x - k t }$ and $\mathcal { H } _ { X , n e g - k t } ^ { c t x - k t }$ :

$$
\begin{array} { r l } & { \mathcal { H } _ { X , p o s - k t } ^ { c t x - k t } [ i ] = \mathbf { M L P } ( \mathcal { H } _ { X , p o s } ^ { c t x - k t } [ i ] \oplus \mathcal { E } _ { p o s } ^ { c t x - k t } } \\ & { \mathcal { H } _ { X , n e g - k t } ^ { c t x - k t } [ i ] = \mathbf { M L P } ( \mathcal { H } _ { X , n e g } ^ { c t x - k t } [ i ] \oplus \mathcal { E } _ { n e g } ^ { c t x - k t } } \end{array}
$$

Further, we utilize the positive and negative keyterms in gold response as supervision to optimize $\mathcal { L } _ { p o s } ^ { c t x - \mathrm { \bar { k } } t }$ and $\mathcal { L } _ { n e g } ^ { c t \bar { x } - k t }$ losses using cross-entropy (analogous to politeness strategy prediction task). Similarly, employing multi-hop reasoning on $\mathcal { G }$ (illustrated in “Bidirectional Polite Keyterm Graph” section in appendix) involves sequential traversal through the graph, specifically ‘forward $$ forward $$ backward-positive’ and ‘forward $$ forward $$ backwardnegative’ paths, to identify keyterms coherent with future utterance. Utilizing the positive and negative keyterms in the future utterance as prediction targets, we optimize keytermaugmented future keyterm experts through losses, $\dot { \mathcal { L } } _ { p o s } ^ { f t r - k t }$ and $\mathcal { L } _ { n e g } ^ { f t r - k t }$ . This approach allows keyterm experts to acquire diverse expression-level features through $\mathcal { L } _ { k t }$ loss: $\begin{array} { r } { \bar { \mathcal { L } } _ { k t } = \mathcal { L } _ { p o s } ^ { c t x - k t } + \dot { \mathcal { L } } _ { n e g } ^ { c t x - k t } + \mathcal { L } _ { p o s } ^ { f t r - k t } + \mathcal { L } _ { n e g } ^ { f t r - k t } } \end{array}$ .

Multi-task Learning of MoE. To preserve the fundamental semantics of the experts while ensuring their respective diversity, we compute the average of negotiation, politeness, and keyterm experts’ representations to obtain $h _ { X , e x p s }$ , and align it closely with the sequence representation $h _ { X }$ by minimizing the Mean Squared Error (MSE) loss: $L _ { m s e } =$ $\begin{array} { r } { \frac { \delta } { D _ { h } } \sum _ { i = 1 } ^ { D _ { h } } ( \bar { h } _ { X } [ i ] - h _ { X , e x p s } [ i ] ) ^ { 2 } } \end{array}$ , where $\delta$ and $D _ { h }$ denote the hyperparameter and dimension of $h _ { X }$ , respectively. We then train the multi-task MoE jointly by optimizing $\mathcal { L } _ { m o e }$ loss: $\mathcal { L } _ { m o e } = \mathcal { L } _ { n e g o } + \mathcal { L } _ { p o l } + \mathcal { L } _ { k t } + \mathcal { L } _ { m s e }$ .

MoE-based Reinforcement Learning. We adopt the conventional reinforcement learning approach (Sutton and Barto 2018) as the foundation.

State. We concatenate dialogue context with extracted keyterms to form initial state $s _ { 1 } = \{ C , C _ { k t } \} \in S$ . At each step, the prompt token sequence $\mathbf { E }$ produced by the policydetermined expert (action) updates the state. We keep the observed state $s _ { t } ~ \in ~ S$ at the $t ^ { t h }$ step, denoted as $\begin{array} { r l } { s _ { t } } & { { } = } \end{array}$ $\{ C , \mathbf { E } _ { 1 } , \ldots , \mathbf { E } _ { t - 1 } \}$ , which is then encoded by dialogue encoder to obtain $H _ { S , t }$ and $h _ { S , t }$ . The sequence representations of previous states are concatenated to form the current state embedding $s _ { t } = h _ { S , 1 } \oplus . . . \oplus h _ { S , t }$ . If $t < T$ ( $T$ : maximum number of iterations), $s _ { t }$ is padded with zeros to ensure consistent dimensionality. When $t > 1$ , we omit the keyterms

$C _ { k t }$ as they have already contributed in the first iteration, and the input sequence length is limited by the LLaMA model.

Action. At the $t ^ { t h }$ step, the action space $A _ { t }$ is defined by the multi-task experts influenced by the state $s _ { t }$ . At this state, the agent determines which expert to select from $A _ { t }$ as the chosen action $a _ { t }$ . To achieve this, we use a LLaMA-based dialogue decoder to produce the expert prompt $\mathbf { E } _ { t }$ for $a _ { t }$ .

Policy. Besides employing dialogue encoder as a semantic encoding policy network, we devise an expert determination policy network using REINFORCE algorithm with baseline (Sutton and Barto 2018), which includes an actor-network and a value network. The actor learns a policy $\pi _ { \phi } ( a _ { t } , s _ { t } , A _ { t } )$ for selecting the optimal expert action $a _ { t }$ given the current state $s _ { t }$ and action space ${ \bar { \boldsymbol { A } } } _ { t }$ , by producing a probability distribution over the actions in $A _ { t }$ . The value network assesses the value $Q _ { \beta } ( s _ { t } )$ of state $s _ { t }$ to provide a baseline for REINFORCE. The structures of these networks are:

$$
\begin{array} { r l r } & { } & { \quad \quad \quad \sigma _ { t } = \sqrt { \left( \left( \eta \left( s _ { t } W _ { 1 } \right) \overrightarrow { W } _ { 2 } \right) \right) } ; } \\ & { } & { \quad \quad \quad \sigma _ { t } \left( a _ { t } , s _ { t } , A _ { t } \right) = \mathrm { s o f t m a x } \left( A _ { t } \odot o _ { t } W _ { \phi } \right) ; Q _ { \beta } \left( s _ { t } \right) = o _ { t } W _ { \beta } } \\ & { } & { \quad \quad \quad \quad \sigma _ { \phi } \left( a _ { t } , s _ { t } , A _ { t } \right) = \mathrm { s o f t m a x } \left( A _ { t } \odot o _ { t } W _ { \phi } \right) ; Q _ { \beta } \left( s _ { t } \right) = o _ { t } W _ { \beta } } \end{array}
$$

where $\eta ( \cdot )$ denotes ELU activation function with a dropout layer, and $\odot$ represents Hadamard product. $\boldsymbol { A } _ { k }$ is a binarized vector used for pruning action space, and in this case, it is initialized as a full-one vector because the number of experts is relatively small.

Rewards. To steer policy learning, we incentivize decisionmaking at each step by assessing how well the response from the updated state $s _ { t + 1 }$ contributes to effective negotiation, politeness, dialogue coherence, and user engagement.

(1) Contextual Negotiation Strategy Congruence Reward ensures that the generated response $r$ matches the predefined negotiation strategies in the current dialogue context. This is achieved using a RoBERTa-based negotiation strategy classifier, $G _ { c N S }$ (achieve an $8 7 . 4 \%$ accuracy and an $8 1 . 5 \%$ macro-F1 score), which provides the negotiation strategy probability as the strategy score. The reward is formulated as: $R _ { c N S } = G _ { c N S } ( C _ { t } ) - \gamma _ { c } * G _ { c N S } ( r )$ , where $\gamma _ { c } \geq 1$ serves as the penalization factor.

(2) Future Negotiation Strategy Congruence Reward evaluates how well the generated response $r$ prepares for and aligns with potential future strategies, ensuring that the conversation remains strategic and goal-oriented to foster long-term negotiation success and adaptive dialogue. For this, we train RoBERTa-based (Liu et al. 2019) negotiation strategy classifier, $G _ { f N S }$ (achieve an $8 2 . 1 \%$ accuracy and an $76 . 9 \%$ macro-F1 score) and design the reward as: $R _ { f N S } = G _ { f N S } ( C _ { f } ) - \gamma _ { f } * G _ { f N S } ( r )$ , where $\gamma _ { f } \geq 1$ serves as the penalization factor.

(3) Dialogue-level Politeness Reward aims to dynamiis formulated as: RdP = mM=1 cos( π2 · Mmax ) cally adjust the politeness as the conversation progresses. It , where $P D _ { d P } = G _ { P } ( r ) - G _ { P } ( C _ { m } ) ^ { . . . }$ and $G ( \cdot )$ measures the politeness level of an utterance using the state-of-the-art politeness classification model developed by Danescu-NiculescuMizil et al. (2013). The model is trained on two datasets containing diverse request types and achieves nearly $7 0 \%$ accuracy in different settings for politeness classification. Politeness scores are collected as the politeness level. We advocate for the politeness distance $P D _ { d P }$ between the generated response $r$ and the contextual user’s utterance $C _ { m }$ to meet the following criteria: (a) it should be non-negative, meaning that the generated response should at least match (equal to 0) or exceed the user’s level of politeness (greater than 0); (b) it should adjust progressively with the dialogue turn $m$ , reflecting that the conversation’s early stages should emphasize respect and formality, while the later stages should focus on more amicable and personalized engagement. Over time, maintaining consistently respectful and appropriately polite interactions enhances sociopsychological closeness and fosters rapport and trust. Here, $M _ { m a x }$ represents the maximum turns in conversation, and $M$ denotes the current turn.

(4) Utterance-level Politeness Reward assesses the feedback of the user’s next utterance politeness. It is formulated as: $\begin{array} { r } { R _ { u P } \ = \ c o s ( \frac { \pi } { 2 } \ \cdot \ \frac { M } { M _ { m a x } } ) \ \cdot \ c o s ( \frac { \pi } { 2 } \ \cdot \ P D _ { u P } ) } \end{array}$ , where $P D _ { u P } = | G _ { P } ( r ) - \bar { G _ { P } ( \ l _ { C _ { f } } ) | }$ and $P D _ { u P }$ measures the relative politeness distance between the generated response $r$ and the user’s future utterance $C _ { f }$ . We encourage $P D _ { u P }$ to decrease as the current turn $M$ approaches maximum turn $M _ { m a x }$ to enhance sociopsychological closeness during later stages of the conversation, thereby promoting more effective cooperation between the negotiating parties.

(5) Contextual Dialogue Coherence Reward ensures that the generated response $r$ remains coherent with the context $C$ by evaluating coherence at both the keyterm and sentence levels. Initially, we create a dataset that includes pairs of context and responses categorized as coherent or incoherent, where the responses of the incoherent pairs are utterances randomly sampled from the dataset. We then train a RoBERTa-based (Liu et al. 2019) classifier, $G _ { c D C }$ on sentence-keyterm pairs (achieve $8 3 . 3 \%$ accuracy on NEGOCHAT and $8 4 . 7 \%$ on IND) and consider the coherence probability as the coherence score. The reward is formulated as: $R _ { c D C } = G _ { c D C } ( C \oplus C _ { k t } , r \oplus r _ { k t } ) \cdot e ^ { \frac { N _ { c , k t } } { | r _ { k t } | } - 1 }$ e |Nrck,tk|t −1, where rkt is the keyterm set of $r$ and $N _ { c , k t }$ is the number of keyterms in $r _ { k t }$ that are forward neighbors of contextual keyterms in $\mathcal { G }$ .

(6) Future Dialogue Coherence Reward accounts for coherence with user’s future utterance $C _ { f }$ . To this end, we create a dataset including pairs of future utterances and responses categorized as coherent or incoherent and then train another RoBERTa-based (Liu et al. 2019) classifier, $G _ { f D C }$ on sentence-keyterm pairs (achieve $7 9 . 6 \%$ accuracy on NEGOCHAT and $78 . 5 \%$ on IND). The reward is defined as: $\boldsymbol { R _ { f D C } } = G _ { f D C } ( C _ { f } \oplus C _ { f _ { k t } } , r \oplus r _ { k t } ) \cdot e ^ { \frac { \boldsymbol { N _ { f , k t } } } { | \boldsymbol { r } _ { k t } | } - 1 }$ e N|rfk,tk| −1, where Nf,kt is the number of keyterms in $r _ { k t }$ that are the backward neighbors of $C _ { f _ { k t } }$ of $C _ { f }$ in $\mathcal { G }$ .

(7) Engagingness Reward penalizes generic and repetitive responses (e.g., I can offer the best package at best price) that degrades overall conversation quality (See et al. 2019). It is formulated using the Jaccard similarity between the responses, $r _ { m }$ and $r _ { m - 1 }$ at $m ^ { t h }$ and $( m - \dot { 1 } ) ^ { t h }$ turns, respectively as: $R _ { E } = 1 - ( r _ { m - 1 } \cap r _ { m } ) / ( r _ { m - 1 } \cup r _ { m } )$ .

Cumulative Reward $R$ is formulated as the weighted sum of all the rewards, i.e., $R = w _ { c N S } * R _ { c N S } + w _ { f N S } *$ $R _ { f N S } + w _ { d P } * R _ { d P } + w _ { u P } * R _ { u P } + w _ { c D C } * R _ { c D C } + w _ { f D C } *$ $R _ { f D C } + w _ { d E } * R _ { d E }$ .

Optimization. We define $T$ -step iterations with the aim of agent learning being to maximize the expected cumulative reward: $\begin{array} { r } { \ J _ { \theta } ~ = ~ \mathbb { E } _ { \pi } \left[ \sum _ { t = 1 } ^ { T } \alpha ^ { t } r _ { t + 1 } \right] } \end{array}$ , where $\theta$ represents the learned parameter and $\alpha$ is the discount factor. The agent is optimized using the $\mathcal { L } _ { a g e n t }$ loss, and its policy gradient is given by: $\nabla _ { \theta } J _ { \theta } = \mathbb { E } _ { \pi } \left[ \nabla _ { \theta } \log \pi _ { \phi } ( a _ { t } , s _ { t } , A _ { t } ) ( G - Q _ { \beta } ( s _ { t } ) ) \right]$ , where $G$ represents discounted cumulative reward from the initial state to terminal one. Subsequently, we utilize $H _ { S , T + 1 }$ , the hidden state of state $s _ { T + 1 }$ to generate the response. The optimization of the decoder is achieved through the $\mathcal { L } _ { d e c }$ loss: $\begin{array} { r } { \mathcal { L } _ { d e c } = - \sum _ { k = 1 } ^ { K } \log P ( y _ { k } | H _ { S , T + 1 } , y _ { < k } ) } \end{array}$ .

Warm Start. We utilize LLaMa-3.1-8B-Instruct model for initializing the model. The initial state serves as the input for fine-tuning the model, with the warm start achieved by optimizing $\mathcal { L } _ { w a r m } = \mathcal { L } _ { m o e } + \mathcal { L } _ { d e c }$ .

Joint Training. The model is ultimately trained jointly by optimizing $\begin{array} { r } { \hat { \mathcal { L } } _ { j o i n t } = \mathcal { L } _ { a g e n t } + \mathcal { L } _ { d e c } + \frac { 1 } { T + 1 } \sum _ { t = 1 } ^ { T + 1 } \hat { \mathcal { L } } _ { m o e , t } } \end{array}$

# Experiments

We compare GENTEEL-NEGOTIATOR with 7 baselines: DialoGPT (Zhang et al. 2020), ARDM (Wu et al. 2021), PersRFI (Shi et al. 2021), GPT-Critic (Jang, Lee, and Kim 2022), INA (Ahmad et al. 2023), ProCoT (GPT-3.5) (Deng et al. 2023), and LLaMA-3.1-8B-finetune (Touvron et al. 2023). For automatic evaluation, we adopt Perplexity (PPL) (Brown et al. 1992), BLEU (B-2) (Papineni et al. 2002), BERTScore-f1 (BS-f1) (Zhang et al. 2019), Distinct-2 (D-2) (Li et al. 2015), and Response Length (R-LEN) to evaluate general quality of responses. To assess responses for goal accomplishment, we introduce Negotiation Strategy Congruence (NSC) scores encompassing contextual and future NSC, i.e., $ { \boldsymbol { S } } _ { c N S }$ and $\boldsymbol { { \cal S } } _ { f N S }$ , Politeness scores consisting dialogue-level and utterance-level politeness, i.e. $ { \boldsymbol { S } } _ { d P }$ and $\mathcal { S } _ { u P }$ , Dialogue coherence scores, which include contextual and future coherence, i.e., $\boldsymbol { S _ { c D C } }$ and $\boldsymbol { S } _ { f D C }$ , and Engagingness score $\scriptstyle { \mathcal { S } } _ { E }$ . For human evaluation, we employ Fluency (F), Contextual Coherence (CC), and Engagingness (E) to assess the responses’ general quality. To assess how well the generated responses achieve goals, we introduce Sociopsychological Closeness (SC), Negotiation Consistency (NC), Bargaining Efficacy (BE), and Outcome Fairness (OF) (Ahmad et al. 2023). We include ‘Implementation Details’, ‘Baselines Details’, ‘Evaluation Metrics Details’ in “Experiment Details” section of appendix.

# Results and Analysis

Automatic Evaluation. Table 2 shows that compared to baselines, proposed GENTEEL-NEGOTIATOR achieves superior dialogue quality, as indicated by its lexical (B-2) and semantic richness (BS-f1) and ability to generate more diverse (D-2) and longer responses (R-LEN) across both datasets. Specifically, on NEGOCHAT, it achieves a significant improvement of $1 6 . 3 \%$ , $4 . 3 \%$ , $3 . 1 \%$ , and $8 . 4 \%$ in B-2, BS-f1, D-2, and R-LEN, respectively, compared to the second-best model, LLaMA-3.1-8B-finetune. On IND, it achieves notable gains of $1 9 . 8 \%$ , $1 . 3 \%$ , $7 . 1 \%$ , and $1 . 6 \%$ in B-2, BS-f1, D-2, and R-LEN, respectively. GENTEELNEGOTIATOR obtains the highest negotiation strategy congruence $( S _ { c N S } , S _ { f N S } )$ and politeness scores $( S _ { d P } , S _ { u P } )$

Table 2: Automatic evaluation results. Results are statistically significant at $5 \%$ significance level based on t-test (Welch 1947)   

<html><body><table><tr><td>Models</td><td>PPL↓</td><td>B-2个</td><td>D-2个</td><td>BS-f1↑</td><td>ScNs↑</td><td>SfNs↑</td><td>SdP↑</td><td>SuP↑</td><td>ScDc↑</td><td>SfDC↑</td><td>SE↑</td><td>R-LEN↑</td></tr><tr><td colspan="14">NEGoCHAT</td></tr><tr><td>DialoGPT</td><td>22.02</td><td>3.66</td><td>30.21</td><td>0.631</td><td>0.565</td><td>0.362</td><td>0.570</td><td>0.425</td><td>0.650</td><td>0.445</td><td>0.625</td><td></td><td>23.39</td></tr><tr><td>ARDM</td><td>21.91</td><td>3.83</td><td>31.53</td><td>0.655</td><td>0.573</td><td>0.385</td><td>0.593</td><td></td><td>0.437</td><td>0.663</td><td>0.453</td><td>0.648</td><td>24.24</td></tr><tr><td>PersRFI</td><td>20.80</td><td>4.14</td><td>31.92</td><td>0.663</td><td>0.579</td><td>0.391</td><td>0.587</td><td></td><td>0.431</td><td>0.676</td><td>0.458</td><td>0.651</td><td>25.70</td></tr><tr><td>GPT-Critic</td><td>18.69</td><td>4.98</td><td>33.31</td><td>0.662</td><td>0.584</td><td>0.395</td><td>0.602</td><td></td><td>0.455</td><td>0.689</td><td>0.472</td><td>0.665</td><td>27.45</td></tr><tr><td>INA</td><td>17.13</td><td>5.92</td><td>34.32</td><td>0.671</td><td>0.592</td><td>0.402</td><td>0.615</td><td>0.467</td><td></td><td>0.703</td><td>0.496</td><td></td><td>29.02</td></tr><tr><td>ProCoT (GPT-3.5)</td><td>21.82</td><td>3.69</td><td>31.69</td><td>0.648</td><td>0.560</td><td>0.373</td><td></td><td></td><td></td><td></td><td></td><td>0.687</td><td></td></tr><tr><td>LLaMA-3.1-8B-finetune</td><td>16.55</td><td></td><td>38.22</td><td>0.734</td><td>0.698</td><td>0.499</td><td>0.565</td><td>0.453</td><td></td><td>0.674</td><td>0.479</td><td>0.612</td><td>28.16</td></tr><tr><td>GENTEEL-NEGOTIATOR</td><td>14.72</td><td>6.24 7.26</td><td>39.41</td><td>0.766</td><td>0.751</td><td>0.513</td><td>0.682</td><td>0.499</td><td></td><td>0.743</td><td>0.532</td><td>0.759</td><td>33.98</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>IND</td><td></td><td>0.712</td><td>0.550</td><td>0.781</td><td>0.572</td><td>0.776</td><td>36.84</td></tr><tr> colspan="14"></td><td>0.274</td><td>0.362</td><td>0.251</td><td></td><td>0.478</td><td>23.37</td><td>DialoGPT ARDM</td></tr><tr><td>4.00 3.89</td><td>3.64 3.81</td><td>35.17 36.50</td><td>0.762 0.783</td><td>0.354 0.375</td><td>0.205</td><td>0.192</td><td>0.427 0.448</td><td>0.289</td><td>0.383</td><td>0.264</td><td></td><td>25.22</td><td>PersRFI</td></tr><tr><td>3.78</td><td>4.12</td><td>37.89</td><td>0.803</td><td>0.396</td><td>0.219</td><td>0.471</td><td></td><td></td><td></td><td></td><td>0.497</td><td></td><td>GPT-Critic</td></tr><tr><td>3.67</td><td>4.96</td><td>39.27</td><td>0.823</td><td>0.423</td><td>0.239</td><td></td><td></td><td>0.305</td><td>0.407</td><td>0.279</td><td>0.518</td><td>25.68</td><td></td></tr><tr><td></td><td></td><td></td><td>0.854</td><td></td><td></td><td></td><td>0.507</td><td>0.332</td><td>0.438</td><td>0.302</td><td>0.552</td><td>27.43</td><td>INA</td></tr><tr><td>2.11 10.78</td><td>5.90 3.67</td><td>33.28 35.66</td><td>0.697</td><td>0.574 0.523</td><td>0.382</td><td></td><td>0.698</td><td>0.495</td><td>0.597</td><td>0.452</td><td>0.627</td><td>39.00</td><td>ProCoT (GPT-3.5) LLaMA-3.1-8B-finetune</td></tr><tr><td>2.63</td><td>7.24</td><td>45.19</td><td>0.871</td><td>0.690</td><td>0.348 0.505</td><td>0.637 0.727</td><td></td><td>0.454</td><td>0.544</td><td>0.415</td><td>0.569</td><td>28.14</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.512 0.713</td><td>0.632</td><td>0.478</td><td>0.626</td><td>39.48</td><td>GENTEEL-NEGOTIATOR</td></tr><tr><td>1.14</td><td>8.67</td><td>48.42</td><td>0.882</td><td>0.842</td><td>0.663</td><td>0.872</td><td></td><td></td><td>0.789</td><td>0.596</td><td>0.817</td><td>40.12</td><td></td></tr></table></body></html>

Table 3: Human evaluation results. Results are statistically significant at $5 \%$ significance level based on t-test (Welch 1947). All metrics are rated on a scale of 1 to 5.   

<html><body><table><tr><td>Models</td><td>SC</td><td>NC</td><td>BE</td><td>OF</td><td>F</td><td>CC</td><td>E</td></tr><tr><td colspan="8">NEGoCHAT</td></tr><tr><td>INA</td><td>2.29</td><td>2.85</td><td>2.53</td><td>2.74</td><td>2.83</td><td>2.79</td><td>2.05</td></tr><tr><td>ProCoT (GPT-3.5)</td><td>3.05</td><td>3.18</td><td>2.97</td><td>2.50</td><td>4.15</td><td>2.98</td><td>3.19</td></tr><tr><td>LLaMA-3.1-8B-finetune</td><td>3.54</td><td>3.07</td><td>3.15</td><td>3.01</td><td>3.89</td><td>3.26</td><td>3.82</td></tr><tr><td>GENTEEL-NOGOTIATOR</td><td>4.17</td><td>4.31</td><td>4.55</td><td>4.78</td><td>4.78</td><td>4.32</td><td>4.04</td></tr><tr><td colspan="8">IND</td></tr><tr><td>INA</td><td>2.12</td><td>2.99</td><td>2.64</td><td>2.86</td><td>3.14</td><td>3.27</td><td>3.29</td></tr><tr><td>ProCoT(GPT-3.5)</td><td>3.01</td><td>3.26</td><td>2.89</td><td>2.37</td><td>4.22</td><td>3.94</td><td>3.54</td></tr><tr><td>LLaMA-3.1-8B-finetune</td><td>3.65</td><td>3.13</td><td>3.32</td><td>3.09</td><td>3.93</td><td>3.44</td><td>3.85</td></tr><tr><td>GENTEEL-NOGOTIATOR</td><td>4.79</td><td>4.27</td><td>4.65</td><td>4.45</td><td>4.52</td><td>4.28</td><td>4.51</td></tr></table></body></html>

while maintaining coherence $( S _ { c D C } , S _ { f D C } )$ and engagingness $( S _ { E } )$ across both the datasets, which justifies the design of the novel reward function. In particular, it yields 0.751, 0.513, 0.712, 0.550, 0.781, 0.572, and 0.776 scores for $S _ { c N S } , S _ { f N S } , S _ { d P } , S _ { u P } , S _ { c D C } , S _ { f D C }$ , and $\scriptstyle { \mathcal { S } } _ { E }$ , respectively on NEGOCHAT with a significant increase of $7 . 6 \%$ , $2 . 8 \%$ , $4 . 4 \%$ , $1 0 . 2 \%$ , $5 . 1 \%$ , $7 . 5 \%$ , and $2 . 2 \%$ compared to second best model. Likewise, it achieves a gain of $2 2 . 0 \%$ , $3 1 . 3 \%$ , $1 9 . 9 \%$ , $3 9 . 2 \%$ , $2 4 . 8 \%$ $2 4 . 7 \%$ , and $3 0 . 5 \%$ in $\displaystyle S _ { c N S } , S _ { f N S } , S _ { d P } , S _ { u P } , S _ { c D C } , S _ { f D C }$ , and $s _ { E }$ , respectively on IND. The LLaMA-3.1-8B-finetune model often generates less diverse and less coherent polite responses during negotiation (e.g., “I am glad $I$ could satisfy your needs.” with high politeness). This may be due to the repetition of generic responses that appear in its outputs. INA often generates less polite responses, which can be justified given that it lacks politeness rewards. ProCoT (GPT-3.5) excels in natural language generation, showcasing notable strengths in dialogue coherence and engagement but performs poorly in negotiation and generating polite responses (e.g., for the user utterance, “Can you please go down to $\$ 1250? ^ { \prime }$ , the model simply responds with “Yes, we can!” without further negotiation or polite tone). Overall, GENTEEL-NEGOTIATOR excels across all evaluated dimensions, demonstrating the effectiveness of its advanced reward function in fostering highquality, polite negotiation dialogues.

Human Evaluation. Table 3 presents human evaluation results for GENTEEL-NEGOTIATOR and baselines. We compare GENTEEL-NEGOTIATOR against INA, ProCoT (GPT3.5), and LLaMA-3.1-8B-finetune only, as manual evaluation is expensive. It is evident that GENTEEL-NEGOTIATOR achieves better scores of 4.17, 4.31, 4.55, 4.78, 4.78, 4.32, and 4.04 for SC, NC, BE, OF, F, CC, and E, respectively, with an improvement of $+ 0 . 6 3$ , $+ 1 . 2 4$ , $+ 1 . 4 0$ , $+ 1 . 7 7$ , $+ 0 . 8 9$ , $+ 1 . 0 6$ , and $+ 0 . 2 2$ points for these metrics, compared to second best model, LLaMA-3.1-8B-finetune, on NEGOCHAT. A similar performance improvement is observed in the IND dataset. The superior SC, NC, BE, and OF scores indicate the effectiveness of negotiation strategy congruence and politeness rewards in facilitating polite responses during negotiation that foster sociopsychological closeness and lead to win-win outcomes. Also, the high CC and E scores reflect the pivotal role of dialogue coherence and engagingness rewards in generating coherent, and engaging responses.

Additional Analysis. We include more analyses - (1) Ablation w.r.t Experts, (2) Ablation w.r.t Multi-task Learning of Experts, (3) Ablation w.r.t Rewards, (4) Ablation w.r.t Warm-start and Joint Training, (5) Analysis on Iteration Steps, and (6) Case Study under “Additional Analysis” section in appendix.

# Conclusion

In this work, we introduced GENTEEL-NEGOTIATOR, a polite negotiation dialogue system to enhance the quality of negotiation outcomes in tourism and e-commerce domains. In this regard, we curated a novel NEGOCHAT negotiation dialogue dataset for tourism negotiation using LLM. GENTEEL-NEGOTIATOR leverages LLM and MoE-based RL approach employing designated negotiation, politeness, and keyword experts with well-designed negotiation strategy congruence, politeness, dialogue coherence, and engagingness rewards. Extensive experiments on NEGOCHAT and IND datsets demonstrated the promising potential of GENTEEL-NEGOTIATOR in generating polite, coherent, and engaging responses, and significantly improving the quality of negotiation outcomes.