# Simulation-Free Hierarchical Latent Policy Planning for Proactive Dialogues

Tao $\mathbf { H } \mathbf { e } ^ { 1 \ast }$ , Lizi Liao2, Yixin Cao3, Yuanxing Liu1, Yiheng $\mathbf { S u n } ^ { 1 }$ , Zerui Chen1, Ming Liu1‚Ä†, Bing Qin

1Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China 2Singapore Management University, Singapore 3School of Computer Science, Fudan University {the, yxliu, mliu, zrchen, qinb}@ir.hit.edu.cn, lzliao $@$ smu.edu.sg, caoyixin2011 $@$ gmail.com

# Abstract

Recent advancements in proactive dialogues have garnered significant attention, particularly for more complex objectives (e.g. emotion support and persuasion). Unlike traditional task-oriented dialogues, proactive dialogues demand advanced policy planning and adaptability, requiring rich scenarios and comprehensive policy repositories to develop such systems. However, existing approaches tend to rely on Large Language Models (LLMs) for user simulation and online learning, leading to biases that diverge from realistic scenarios and result in suboptimal efficiency. Moreover, these methods depend on manually defined, context-independent, coarse-grained policies, which not only incur high expert costs but also raise concerns regarding their completeness. In our work, we highlight the potential for automatically discovering policies directly from raw, real-world dialogue records. To this end, we introduce a novel dialogue policy planning framework, LDPP. It fully automates the process from mining policies in dialogue records to learning policy planning. Specifically, we employ a variant of the Variational Autoencoder to discover fine-grained policies represented as latent vectors. After automatically annotating the data with these latent policy labels, we propose an Offline Hierarchical Reinforcement Learning (RL) algorithm in the latent space to develop effective policy planning capabilities. Our experiments demonstrate that LDPP outperforms existing methods on two proactive scenarios, even surpassing ChatGPT with only a 1.8-billion-parameter LLM.

# Introduction

In recent years, there has been a surge of interest in dialogue tasks that require proactive engagement to achieve complex objectives, such as negotiation (He et al. 2018), persuasion (Samad et al. 2022), and emotional support (Cheng et al. 2022). Unlike traditional task-oriented dialogues (Liu et al. 2022; Hu et al. 2023; Liu et al. 2023), these tasks require agents to be more proactive and possess sophisticated dialogue strategy skills (Cheng et al. 2024). Previous research has demonstrated that even LLMs often struggle on such tasks (Yang, Li, and Quan 2021; Zhao et al. 2023; Kang et al. 2024; Song et al. 2024). LLMs are typically trained to passively follow user instructions, which leads them to align with the user‚Äôs opinions and decisions, lacking the necessary proactivity (Deng et al. 2023b; He et al. 2024).

The advancement of LLMs in instruction-following and text generation capabilities has provided a foundation for exploring proactive dialogue systems, allowing a focus on high-level strategic research, i.e. dialogue policy planning (Deng et al. 2023b), which plans the next dialogue policy to guide generating appropriate responses. Some efforts have sought to directly enhance the strategic capabilities of LLMs by designing heuristic prompts or complex prompting processes (Deng et al. $2 0 2 3 \mathrm { a }$ ; Yu, Chen, and Yu 2023). However, these approaches often face limitations in performance or are criticized for high inference costs and inefficiency due to the need for continuous interactions. Other approaches aim to develop specialized policy planners to guide LLM responses strategically (Deng et al. 2023b), allowing the separation of strategy from LLM and enabling a focused effort on learning policy planning capabilities.

However, developing advanced policy planners requires rich exposure to diverse dialogue scenarios and access to a comprehensive policy repository. Previous works (Deng et al. 2023a) have used LLM like ChatGPT to simulate interactions, engaging in role-play and real-time learning. This methodology presents two critical drawbacks: first, the significant disparity between simulated and real-world interactions, as the toneless communication style of ChatGPT contrasts with the diverse and dynamic traits of actual human users; second, the reliance on continuous real-time interactions and frequent API calls for training, which introduces inefficiencies and escalates costs. Moreover, these approaches often depend on manually defined, contextindependent, coarse-grained dialogue policies (Zhou et al. 2019; Liu et al. 2021a), which not only require substantial expert involvement but also raise concerns about the completeness and effectiveness of predefined policies.

In this study, we introduce a novel paradigm that shifts away from relying on predefined policy sets and online learning in simulated environments, instead directly learning policy planning from raw, unlabeled dialogue records. This paradigm effectively addresses two key challenges: 1) It allows for discovering fine-grained policies directly from realistic dialogues, reducing the need for expert intervention and enhancing the completeness and relevance of resulting policies. 2) By learning from real-world dialogues, it eliminates the dependence on simulated environments, thereby improving both efficiency and effectiveness.

To achieve this, we propose the innovative Latent Dialogue Policy Planning (LDPP) framework. LDPP automatically discovers policies as continuous latent vectors, expressing more semantics than predefined context-free policies, and facilitates the learning of effective planning within this latent policy space. The framework consists of three key stages: Latent policy discovery, Latent policy distillation, and Offline Hierarchical RL enhancement. Inspired by the Variational Autoencoder‚Äôs (VAE) ability to encode inputs into an interpolable latent space (Kingma and Welling 2013), we first employ a variant of the VQ-VAE (van den Oord, Kalchbrenner, and Kavukcuoglu 2016) to automatically discover latent policies from dialogue records. These discovered latent policies are then used to label the training data. Finally, we propose an Offline Hierarchical Reinforcement Learning algorithm to both enhance the high-level policy planning and optimize response generation given latent policies at the lower token level. Since the latent policies are represented as continuous vectors rather than natural language tokens, we further introduce the P-Former module. This module functions as a trainable adapter, ensuring that LLMs can effectively understand and follow the guidance of latent policies to respond, term as the latent-policyfollowing ability. During inference, the policy planner first determines the appropriate latent policy based on the current dialogue state, which then directs the LLM in generating contextually relevant responses.

To verify our approach, we conducted experiments widely on ExTES (Zheng et al. 2023a), ESConv (Liu et al. 2021b) and P4G (Wang et al. 2019b). We compare our method with various baselines, demonstrating its effectiveness. Detailed analysis experiments further support the framework‚Äôs validity. Our contributions are as follows:

‚Ä¢ We introduce a novel simulation-free dialogue policy planning learning framework, automatically mining potential policies from raw dialogue records. ‚Ä¢ We propose an offline hierarchical reinforcement learning method for optimizing proactive dialogue, improving both planning capability and latent-policy-following ability for response generation. ‚Ä¢ Extensive experiments across three proactive dialogue benchmarks show our approach outperforms baselines, with analysis confirming its effectiveness.

# Related Work

Policy Planning for LLM-powered Dialogue Agent. The advent of LLMs enables research into more complex dialogue tasks (Cheng et al. 2024) like emotion support and price negotiation. However, current studies indicate that LLMs often underperform in such tasks due to insufficient policy planning capacities (Chen et al. 2023). To improve policy planning, recent research has proposed various methods, which can be categorized into two parts: 1) With predefined dialogue policy. These methods need predefined dialogue policies, which can be further divided into two parts.

Firstly, Deng et al. (2023a) design a prompt process requiring LLMs to select an appropriate policy before generating a response. GDP-Zero (Yu, Chen, and Yu 2023) employs Markov Monte Carlo Tree Search (Liebana et al. 2015) to identify the next strategy. However, these methods are hindered by either the fixed parameters of LLMs or their high computational costs. To overcome this, PPDPP (Deng et al. 2023b) trains a specialized policy planner via online interaction with a simulated environment. Zhang et al. (2024) increase richer user simulations to improve planning performance. DPDP (He et al. 2024) employs the Dual-process theory (Kahneman 2003) to balance the efficiency and performance. However, these methods require real-time interaction with a simulated environment, suffering from low efficiency and gaps between the realistic and simulated environment. 2) Without predefined dialogue policy. These approaches do not require pre-defined dialogue policies. Instead, they drive LLMs to analyze the current dialogue state and generate AI feedback, which is then used to help the LLMs to reply (Fu et al. 2023; Zhang, Naradowsky, and Miyao 2023). However, these methods often struggle to enhance the strategic reasoning capabilities of LLMs, resulting in less coherent and contextually appropriate responses, which leads to suboptimal performance.

Dialogue Generation on Latent Space. In the past years, studies have utilized latent features to control or enhance response generation (Wang et al. 2020; Cho et al. 2023; Lubis et al. 2020). Some works employ VAE (Bowman et al. 2015) variants such as CVAE (Zhao, Zhao, and Eske¬¥nazi 2017), and Discrete VAE (Bao et al. 2019) to model the semantic distribution of utterances in the latent space (Liu, Pan, and Luo 2020; Chen et al. 2022), sampling latent variables to enhance response diversity (Xiang et al. 2024). In this work, we focus on dialogue policy planning for LLMpowered proactive dialogues. We discover latent policies automatically and conduct planning within the latent space.

# Preliminaries

Problem formalization. Unlike previous works that focus solely on dialogue policy planning (Deng et al. 2023b), our approach also optimizes the policy following ability for responding. To achieve this, we model the entire dialogue process using a hierarchical Markov Decision Process (MDP), inspired by recent studies (Zhou et al. 2024). At the high level, a policy-level MDP is employed to model the policy planning task, while at the low level, a token-level MDP models the autoregressive generation of responses.

The policy-level MDP is defined as $\mathcal { M } _ { h } = \langle \boldsymbol { S } _ { h } , \boldsymbol { \mathcal { A } } _ { h } , \mathcal { R } _ { h } \rangle$ . The state set $\boldsymbol { \mathcal { S } } _ { h }$ consists of the dialogue history $h _ { t }$ with alternating user utterances and system responses $\{ u _ { 1 } ^ { s y s } , u _ { 1 } ^ { u s r } , \ldots , u _ { t - 1 } ^ { s y s } , u _ { t - 1 } ^ { u s r } \}$ . The action $z _ { t } \in \mathcal { A } _ { h }$ refers to the dialogue policy, i.e., latent policy in this work. The reward function $\mathcal { R } _ { h }$ evaluates each dialogue state using ChatGPT, outputting rewards $\boldsymbol { r } _ { t }$ for each turn of dialogue. Please refer to the Evaluation Methods Section for details. Similarly, the token-level MDP is defined as $\mathcal { M } _ { l } = \langle \boldsymbol { S } _ { l } , \mathcal { A } _ { l } , \mathcal { R } _ { l } \rangle$ , where the state set ${ \cal S } _ { l } ~ = ~ \{ s _ { i } ^ { t } ~ = ~ [ h _ { t } ; z _ { t } ; w _ { 1 : i - 1 } ] \} _$ , with $w _ { i }$ representing the $i$ -th token of the response $\begin{array} { r l } { \dot { u } _ { t } ^ { s y s } } & { { } = } \end{array}$

Stage 1: Latent Policy Discovery Stage 2: Policy Distillation & Critic Pretraining The T1shtes2ysntdesmysuttetemraunttce $u _ { 1 }$ ce Latent distribution DisPtiolliactyion Predictive distribution MLP ùëÑùõº(‚Ñé, ∆∏) 1 Bellman Loss ùìõùë∏ $u _ { 2 }$ The 3rd system utterance $u _ { 3 }$ Sampling ‡∑úùíõ Poùë∑li:cùëùyùúôP(lùëßa|n‚Ñén)er MLP ùëâùõΩ(‚Ñé‚Ä≤) 2 Regression ùìõùëΩ Expectile MLP Head Utterùë¨a:nùëùcùúÉe(ùëßE|nùë¢c)oder Utterùë¨a:nùëùce(ùëßE|nùë¢c)oder The t-th system utt The t-th user utterance eranceDiDailaolgoghishtiostroyry‚Ñéùë°‚Ñé+ùë°1 1 ùëü ‚Ñé, ∆∏ + ùëâùõΩ ‚Ñé‚Ä≤ ‚àí ùëÑùõº ‚Ñé, ∆∏ Codebook ùì© The $( { \mathfrak { t } } { + } 1 )$ -th system utterance u The (t+1)-th systTehmeu(tt+e1r)a-ntcheuser utterance 2 ùêøùúè2(ùëÑùõº ‚Ñé, ∆∏ ‚àí ùëâùõΩ(‚Ñé‚Ä≤)) ùë∑ùüè ùë∑ùüê ùë∑ùüë ùë∑ùüí   
pùë∑oùüèlicyùë∑ùüê ùë∑ùüë ùë∑ùüí Pfoeliactyulraeteùíõnt PTohleict-ythLseyvstelmMuttDerPance DiaDlioaglohgishtiosrtyor‚Ñéy ‚Ñé‡∑úùíõ‚Ä≤ 1 ùëâùõΩ(‚Ñé‚Ä≤) Regression Loss ùìõùëΩ The $( { \mathrm { t } } { + } 1 )$ -th systeThmeu(tt+er1a)-ntcheuser utterance ùëùùúô(ùëß∆∏|‚Ñé) AWR Loss ùìõùíâùíäùíàùíâ Generator ùëÆ: ùëùùúì(ùë¢|ùëß, ‚Ñéùëñùë†ùëúùëüùë¶) Token Level MDP exp(ùëÑùõº ‚Ñé, ∆∏ ‚àí ùëâùõΩ ‚Ñé‚Ä≤ reward Generator ùëÆ ùë§1 ùë§2‚Ä¶ùë§ùë° EOS P-Former 3 The 1st system utterance ùë¢‚Ä≤1 ‚ñ°.‚ñ°‚ñ° Transformer Encoder Large Language Model The 2nd system utterance $u _ { 2 } ^ { \prime }$ The 3rd system utterance $u _ { 3 } ^ { \prime }$ TPokliecnys ‡∑úùíõ 4 Prompt . history ùë§1 ùë§2‚Ä¶ùë§ùë°

$\left[ w _ { 1 } , w _ { 2 } , \ldots , w _ { n } \right]$ . The action set $\boldsymbol { \mathcal { A } } _ { l }$ is the LLM‚Äôs vocabulary, and the reward function $\mathcal { R } _ { l }$ is provided by the policylevel MDP, detailed in the 3rd Stage introduction. In the $t$ -th dialogue turn, given the current state, i.e., dialogue history $h _ { t }$ , the policy planner predicts the appropriate dialogue policy $z _ { t }$ . Guided by the dialogue history $h _ { t }$ and the dialogue policy $z _ { t }$ , the LLM generates the response $u _ { t } ^ { s y s }$ .

In our proposed offline scenario, we only access raw dialogue records $\mathcal { D }$ . For RL training, we decompose $\mathcal { D }$ into tuples: $\mathcal { D } \ = \ \{ ( h _ { t } , u _ { t } ^ { s y s } , u _ { t } ^ { u s r } ) \}$ . To learn the policyD-level MDP, we further use the policy-level reward $\boldsymbol { r } _ { t }$ for each dialogue turn $t$ to extend $\mathcal { D }$ as $\{ ( \dot { h } _ { t } , u _ { t } ^ { s y s } , u _ { t } ^ { u s r } , r _ { t } ) \}$ .

# Component Models

Before delving into the training framework, we first outline the component models. Our framework is composed of three key models: an utterance encoder $E$ , a policy planner $P$ , and a generator $G$ . During the training phase, $E$ learns to discover latent policies from system responses and then annotate pseudo labels (latent policies) for training set for subsequent optimizing $P$ and $G$ . In the inference phase, only $P$ and $G$ are actually employed: the planner $P$ outputs the next-turn policy based on the dialogue history, and then the policy is fed into $G$ to guide the response generation.

In this work, the design of the base models is not the central focus; therefore, we utilized RoBERTa-Large (Liu et al. 2019) as the base for both $E$ and $P$ , same as works like PPDPP (Deng et al. 2023b). $E$ takes system responses as input and uses the output of ‚Äú[CLS]‚Äù to analysis the distribution of latent policies contained in responses; $P$ takes dialogue history as input and similarly outputs a predicted distribution of next-step policies. $G$ is based on an LLM.

However, LLMs only accept texts, while the latent policy is a continuous vector, which obviously has a significant gap between them. Inspired by the development of Vision Large Models (Li et al. 2023), we propose to train a P-Former to bridge this gap. P-Former consists of $L$ stacked transformer layers, taking $T$ learnable policy tokens as input. These policy tokens interact with the latent policy features through a cross-attention mechanism. Ultimately, P-Former outputs $T$ policy-related tokens. We hope these tokens align with the input space of LLM, thus LLM can understand and follows the guidance of latent policies for appropriate response generation. During training, the P-Former is optimized by the reconstruction loss of LLM. Notably, we freeze the LLM throughout. Therefore, P-Former is also responsible for improving the latent-policy-following capacity of $G$ .

# Optimizaing Framework

Our training framework is depicted in Figure 1. It consists of three stages with the following motivations and relationships: Stage 1: It focuses on automatically learning latent policies from raw dialogues. These latent policies serve as ‚Äúannotations‚Äù for optimizing policy planning in Stage 3. Stage 2: It is used to initialize the policy planner, thereby accelerating and stabilizing the reinforcement learning process in Stage 3. Stage 3: Upon the preparatory work in Stage 1 and 2, this stage aims to enhance policy planning capabilities at the policy level and further optimize latent-policyfollowing abilities for responding at the token level.

1st Stage: Latent Policy Discovery. We first automatically mine potential dialogue policies from raw dialogue records. The basic premise is that, given the dialogue history and the policy implied in one response, the dialogue agent should be able to reconstruct this response. To this end, we propose an adjusted VQ-VAE algorithm. We first compress the inputted utterance into latent policy and then apply it, along with dialogue history, to guide the LLM in reconstructing the utterance. If the reconstruction is good, we assume the learned latent policy is effective. For more details about VQ-VAE, please refer to the appendix.

Like VQ-VAE, We define a codebook $\mathcal { Z } ~ = ~ \{ \mathcal { Z } _ { k } ~ \in$ d}kK=1 with K policy vectors. Given a system utterance $u _ { t } ^ { s y s }$ , shorted as $\boldsymbol { u } _ { t }$ , we use the encoder $E$ to compress it and classify it into $K$ classes, yielding the policy distribution $p _ { \theta } \big ( z _ { t } | \mathcal { \bar { u } } _ { t } \big ) \in \mathbb { R } ^ { K }$ . Instead of performing a nearest neighbors lookup like VQ-VAE, we use $p _ { \theta } \big ( z _ { t } | \bar { u } _ { t } \big )$ to perform a weighted sum of $\mathcal { Z }$ to obtain the latent policy feature $z _ { t }$ :

$$
z _ { t } = \sum _ { k = 1 } ^ { K } \mathcal { Z } _ { k } \cdot p _ { \theta , k } ( z | u _ { t } ) .
$$

This improvement allows us to involve multiple policies within a single response and expand the number of finegrained policies through combinations.

For the generator $G$ , the policy $z _ { t }$ is first transferred into policy tokens using P-Former. Then policy tokens, along with the dialogue history $h _ { t }$ , are fed into the LLM to guide the generation of the response $\boldsymbol { u } _ { t }$ . By computing the reconstruction loss $\mathcal { L } _ { c o n }$ of $G$ and propagating gradient backward, we can simultaneously optimize $E , G$ , and $\mathcal { Z }$ .

After the 1st training, we employ $E$ to annotate pseudo labels $\hat { z } _ { t }$ for each system utterance in $\mathcal { D }$ , expanding $\mathcal { D }$ to $\{ ( h _ { t } , u _ { t } ^ { s y s } , u _ { t } ^ { u s r } , \hat { z } _ { t } , \mathbf { \bar { \boldsymbol { r } } } _ { t } ) \}$ . Using $\mathcal { D }$ , we arDe able to app yDRL algorithm to optimize the policy planning capabilities.

2nd Stage: Latent Policy Distillation. To expedite the RL training process in the 3rd stage, we initialize the policy planner $P$ by distilling the utterance encoder $E$ . Specifically, for a response $\boldsymbol { u } _ { t }$ , we compute the predicted policy distributions using $E$ and $P$ as $p _ { \theta } \big ( \bar { z } _ { t } | u _ { t } \big )$ and $p _ { \phi } \big ( z _ { t } \big | h _ { t } \big )$ , respectively. Then we freeze $E$ and minimize the KL divergence to drive $P$ to learn from $E$ . However, we observe that the training set contains many inappropriate system utterances that lead to unsuccessful dialogues, which may harm $P$ ‚Äôs planning ability. Therefore, we use the high-level rewards of each system response for data filtering, denoted as:

$$
\begin{array} { r } { \mathcal { L } _ { k l } ( \phi ) = \displaystyle \sum _ { ( u _ { t } , h _ { t } , r _ { t } ) \in \mathcal { D } } \mathbb { I } ( r _ { t } > \delta ) \cdot } \\ { \mathrm { K L } _ { - } \mathrm { d i v } \big ( p _ { \theta } ( z | u _ { t } ) | | p _ { \phi } ( z | h _ { t } ) \big ) , } \end{array}
$$

where $\mathbb { I } ( . )$ refers to indicator function. $\theta$ and $\phi$ represent the trainable parameters of $E$ and $P$ . And $\delta$ is a predefined threshold. We term this process policy distillation.

To stabilize RL learning, We also initialize the actionvalue function network $Q _ { \alpha }$ and the value function network

$V _ { \beta }$ at this stage. These two networks evaluate dialogue states during Stage 3, which are implemented by stacking two MLP layers on the policy planner $P$ . To pretrain $Q _ { \alpha }$ and $V _ { \beta }$ , we use the off-the-shelf Offline RL algorithm IQL (Kostrikov, Nair, and Levine 2021), with the following optimization objectives, respectively:

$$
\begin{array} { r l } & { \mathcal { L } _ { V } ( \beta ) = \mathbb { E } _ { ( h _ { t } , z _ { t } ) \in \mathcal { D } } [ L _ { 2 } ^ { \tau } ( Q _ { \alpha } ( h _ { t } , z _ { t } ) , V _ { \beta } ( h _ { t } ) ) ] , } \\ & { \mathcal { L } _ { Q } ( \alpha ) = \mathbb { E } _ { ( h _ { t } , z _ { t } , H _ { t + 1 } ) \in \mathcal { D } } [ ( r _ { t } + \gamma V _ { \beta } ( h _ { t + 1 } ) } \\ & { \qquad \quad - Q _ { \alpha } ( h _ { t } , z _ { t } ) ) ^ { 2 } ] , } \end{array}
$$

where $\alpha$ and $\beta$ are trainable parameters of $Q _ { \alpha }$ and $V _ { \beta }$ , respectively. And $\mathcal { L } _ { 2 } ^ { \tau }$ means Expectile Regression Loss (Kostrikov, Nair, and Levine 2021). Therefore, the final optimization objective for this stage is as:

$$
\mathcal { L } _ { 2 } = \mathcal { L } _ { k l } ( \phi ) + \mathcal { L } _ { Q } ( \alpha ) + \mathcal { L } _ { V } ( \beta ) .
$$

3rd Stage: Offline Hierarchical RL Enhancement. To optimize this system only using training data without interactions with simulated environments, we propose an offline hierarchical RL algorithm to learn the policy-level and token-level MDPs simultaneously.

For the policy-level MDP, we utilize the IQL (Kostrikov, Nair, and Levine 2021) to simultaneously train the policy planner $P$ , and the $Q$ -, $V$ -networks. The optimization objectives for the latter two are given by Eq.(3), and the optimization target for the policy planner $P$ is:

$$
\begin{array} { r l } & { \mathcal { L } _ { h i g h } ( \phi ) = - \mathbb { E } _ { ( u _ { t } , h _ { t } , z _ { t } ) \sim \mathcal { D } } [ \exp ( \tau ( Q _ { \alpha } ( h _ { t } , z _ { t } ) } \\ & { \qquad - V _ { \beta } ( h _ { t } ) ) ) \log p _ { \phi } ( h _ { t } | z _ { t } ) ] , } \end{array}
$$

where $\tau \geq 0$ is the hyperparamter. The motivation behind the optimization target is to apply the advantage function $A ( h _ { t } , \bar { z } _ { t } ) = Q _ { \alpha } ( \bar { h _ { t } , { z _ { t } } } ) - V _ { \beta } \bar { ( } \bar { h _ { t } ) }$ to weight each training sample $( u _ { t } , h _ { t } , z _ { t } ) \in \mathcal { D }$ , thereby enabling selective learning from training data.

For the token-level MDP, we use the REINFORCE algorithm (Sutton et al. 1999) to optimize Generator $G$ , aiming to improve the generation quality. Each intermediate token receives zero reward, and a final reward of $\exp ( A ( h _ { t } , z _ { t } ) )$ is given after generating the complete $\boldsymbol { u } _ { t }$ . We optimize $G$ using the following objective:

$$
\begin{array} { r } { \mathcal { L } _ { l o w } ( \psi ) = - \displaystyle \sum _ { ( u _ { t } , h _ { t } , z _ { t } ) \sim \mathcal { D } } \exp \left( A ( h _ { t } , z _ { t } ) \right) } \\ { \cdot \displaystyle \sum _ { w _ { i } \in u _ { t } } \log p _ { \psi } ( w _ { i } | h _ { t } , z _ { t } , w _ { 1 : i - 1 } ) , } \end{array}
$$

where $\psi$ denotes the trainable parameters of Generator $G$ . For proof of this conclusion and empirical explanation, please refer to the appendix. It is important to note that we freeze the parameters of the LLM, so training Generator $G$ actually optimizes the P-Former. Ultimately, the training target of this stage is:

$$
\mathcal { L } _ { 3 } = \mathcal { L } _ { h i g h } + \mathcal { L } _ { l o w } + \mathcal { L } _ { V } + \mathcal { L } _ { Q } .
$$

By jointly training the policy planner $P$ and generator $G$ , we simultaneously enhance the system‚Äôs policy planning capability and the response quality given latent policies.

<html><body><table><tr><td rowspan="2">Policy Usage</td><td rowspan="2">Models</td><td colspan="3">ExTES</td><td colspan="3">Generalization to ESConv</td><td colspan="3">P4G</td></tr><tr><td>SSR‚Üë</td><td>SR‚Üë</td><td>AvgT‚Üì</td><td>SSR‚Üë</td><td>SR‚Üë</td><td>AvgT‚Üì</td><td>SSR‚Üë</td><td>SR‚Üë</td><td>AvgT‚Üì</td></tr><tr><td rowspan="2">Predefined Policy</td><td>Proactive</td><td>0.544</td><td>0.605</td><td>7.638</td><td>0.430</td><td>0.408</td><td>7.754</td><td>0.012</td><td>0.045</td><td>7.930</td></tr><tr><td>ProCoT PPDPP</td><td>0.486 0.511</td><td>0.490 0.558</td><td>8.128 8.163</td><td>0.410 0.488</td><td>0.438 0.515</td><td>7.992 7.865</td><td>0.542 0.635</td><td>0.400 0.745</td><td>6.885 5.555</td></tr><tr><td rowspan="6">No Need for Policy</td><td>Standard Prompt</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>+ ChatGPT</td><td>0.650</td><td>0.810</td><td>6.138</td><td>0.639</td><td>0.762</td><td>6.546</td><td>0.477</td><td>0.460</td><td>7.025</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.630</td><td></td></tr><tr><td>ICL-Qwen1.5-1.8b</td><td>0.538</td><td>0.613</td><td>7.590</td><td>0.542</td><td>0.623</td><td>6.723</td><td>0.683</td><td></td><td>6.320</td></tr><tr><td>LoRA Finetuning (32, 64)</td><td>0.558</td><td>0.627</td><td>7.308</td><td>0.616</td><td>0.662</td><td>6.738</td><td>0.651</td><td>0.655</td><td>6.645</td></tr><tr><td>LoRA Finetuning (64,128)</td><td>0.566</td><td>0.628</td><td>7.450</td><td>0.583</td><td>0.654</td><td>6.892</td><td>0.541</td><td>0.570</td><td>6.830</td></tr><tr><td rowspan="3"> Autem ticaly Diseover</td><td>LDPP</td><td>0.723</td><td>0.903</td><td>4.132</td><td>0.651</td><td>0.781</td><td>5.388</td><td>0.733</td><td>0.795</td><td>5.570</td></tr><tr><td>-w/o 2nd Stage</td><td>0.716</td><td>0.865</td><td>4.483</td><td>0.637</td><td>0.769</td><td>5.608</td><td>0.715</td><td>0.760</td><td>6.140</td></tr><tr><td>-w/o 3rd Stage</td><td>0.560</td><td>0.623</td><td>7.038</td><td>0.528</td><td>0.538</td><td>7.777</td><td>0.550</td><td>0.570</td><td>6.840</td></tr></table></body></html>

Table 1: Main results on ExTES, ESConv, and P4G, using gpt-3.5-turbo-0613 as the critic. LoRA Fine-tuning(x, y) mean setting lora rank $\mathbf { \tau } = \mathbf { X }$ and lora alpha $\mathrm { \Phi } = \mathrm { y }$ . Results on ESConv are conducted using the planner trained on ExTES.

# Experimental Settings

# Evaluation Methods

# Datasets

We evaluate the proposed framework on two typical applications of proactive dialogues, ExTES (Zheng et al. 2023b) (emotional support) and P4G (Wang et al. 2019a) (persuasion), representing collaborative and non-collaborative dialogue, respectively. ExTES is an extension of ESConv (Liu et al. 2021b), comprising sufficient dialogues for training (11,117 complete dialogues). We randomly divide it into 10,717/200/200 for train/valid/test set. P4G includes 1,017 donation persuasion dialogues where a ‚Äúpersuader‚Äù attempts to persuade a ‚Äúpersuadee‚Äù to donate to a charity called Save the Children. We randomly choose 100/100 dialogues for validation/testing. We take the remaining 817 dialogues as the training set. In practice, we extend the training set of dialogues to 5,579 using ChatGPT (Ouyang et al. 2022) due to the limited size. Please see the appendix for details of data augmentation. Given that ExTES is larger than P4G and P4G contains synthetic data, ExTES is more suitable for our task setup. Consequently, our primary analysis and experiments were conducted on ExTES. Furthermore, to evaluate the generalizability of LDPP, we also test on ESConv (130 test cases) using LDPP trained on ExTES.

# Baselines

We compare Proactive (Deng et al. 2023a), ProCoT (Deng et al. 2023a), and PPDPP (Deng et al. 2023b) for baselines in need of predefined policies. Proactive and ProCoT require LLMs to select the most appropriate strategy before replying. PPDPP learns a specialized policy planner based on the predefined policies. For methods not requiring policy use, we select the standard prompt method (prompting the base LLM to generate replies directly without considering dialogue policies), LoRA-based fine-tuning (Hu et al. 2021) (shorted as LoRA), and ICL-AIF (Fu et al. 2023). ICL-AIF prompts LLMs to provide suggestions before generating corresponding responses.

Self-play evaluation. Since correct policies are often not unique and the absence of explicitly defined policies in our settings, directly assessing policy prediction accuracy is infeasible. We follow the same self-play method as previous work (Deng et al. 2023b) for dialogue-level evaluation. Specifically, two LLMs simulate the system and user in multi-turn dialogues, with the system receiving strategy guidance from a planner. We also prompt an LLM as critic to evaluate the completion status of each turn, deeming the dialogue failed if the goal isn‚Äôt met within 10 turns. For more detailed prompts, please refer to the appendix.

Critic model. We also use ChatGPT to assess dialogue completion status following PPDPP. For ExTES and ESConv, we define four states: [worse, same, better, solved], with corresponding rewards of [-1, -0.5, 0.1, 1.0]; for P4G, the states are [reject, neutral, positive, donate], with also rewards of [-1, -0.5, 0.1, 1.0]. ChatGPT classifies the current dialogue state into one of 4 states. We perform 10 times classification per evaluation to reduce randomness, with each time getting a scalar value. We average them to obtain the reward $\boldsymbol { r } _ { t }$ for the current dialogue turn. A dialogue is considered successful if $r _ { t } > \eta$ . We set $\eta = 0 . 6$ instead of 0.1 in PPDPP to improve the robustness of evaluations. To ensure the robustness of results, we run the main experiments at least twice and reported the average results. To further reduce evaluation bias, we use two versions of ChatGPT (gpt3.5-turbo-0613 and -0125) to serve as critics and present the latter results in the appendix.

Metrics. Following Deng et al. (2023b), we use two common dialogue-level metrics: Success Rate (SR) and Average Turn (AvgT). SR measures effectiveness and is defined as the ratio of the number of successful cases to the total number of test cases. AvgT measures the efficiency of goal completion by calculating the average dialogue turns of all test cases. However, we observe the high variance of SR. Therefore, we introduce the SSR metric to more accurately assess effectiveness. SSR complements the SR, where SR calculates the ratio of success by mapping the final turn reward into a binary 0 or 1 while SSR averages all final turn rewards directly. Therefore, we view SSR as a ‚ÄúSoft SR‚Äù.

(a) ExTES   
Table 2: Human evaluation results on ExTES and P4G.   

<html><body><table><tr><td rowspan="2">LDPP vs.</td><td colspan="2">Ide.</td><td colspan="2">Com.</td><td colspan="2">Sug.</td><td colspan="2">Ove.</td></tr><tr><td>Win</td><td>Lose</td><td>Win</td><td>Lose</td><td>Win</td><td>Lose</td><td>Win</td><td>Lose</td></tr><tr><td rowspan="2">PPDPP LoRA</td><td>8%</td><td>8%</td><td>52%</td><td>6%</td><td>64%</td><td>8%</td><td>68%</td><td>10%</td></tr><tr><td>6%</td><td>32%</td><td>6%</td><td>6%</td><td>32%</td><td>10%</td><td>26%</td><td>18%</td></tr><tr><td colspan="9"></td></tr><tr><td colspan="2">LDPP</td><td colspan="2">Inf.</td><td colspan="2">Per.</td><td colspan="2">Ove.</td><td></td></tr><tr><td colspan="2">VS.</td><td>Win</td><td>Lose</td><td>Win</td><td>Lose</td><td>Win</td><td>Lose</td><td></td></tr><tr><td colspan="2">PPDPP</td><td>32%</td><td>20%</td><td>40%</td><td>26%</td><td>48%</td><td>22%</td><td></td></tr><tr><td colspan="2">LoRA</td><td>10%</td><td>14%</td><td>24%</td><td>16%</td><td>26%</td><td></td><td>16%</td></tr></table></body></html>

Backbone. We conduct main experiments based on Qwen1.5-1.8b (Bai et al. 2023) and analysis studies on a series of LLMs: Qwen1.5-1.8b, -4b, -7b, Qwen2-1.5b, and Gemma-2b (Mesnard et al. 2024). Due to the hardware limitations, we select models under 7B parameters. We employ these LLMs to play the roles of Therapist/Persuader, respectively, guided by policies from the planner.

# Results and Analysis

# Main Results

Based on Table 1, we find that LDPP outperforms all baselines significantly on all tasks. This LDPP is implemented with $( T , L , K ) = ( 8 , 6 , 2 4 )$ . Firstly, LDPP achieves notable enhancements compared to the standard prompt and LoRA methods, verifying the effectiveness of latent policies and the P-Former module. Prompt-based methods like Proactive, ProCoT, and ICL-AIF show unsatisfactory and unstable performance. We observe serious role confusion issues in these works. Due to the disturbance of suggestions or analyses, the system‚Äôs responses fail to meet the expected form, leading to the role confusion during dialogue. We attribute this to the limited instruction-following and analysis capabilities of the 1.8b LLM. Compared to PPDPP, LDPP performs more effectively and more efficiently without online learning and predefined policies, proving the effectiveness of self-supervised policy discovery and offline hierarchical RL training method. Besides, we also find that LDPP based on Qwen1.5-1.8B performs better than ChatGPT, further affirming our method‚Äôs effectiveness. This also demonstrates that, with the assistance of external modules, smaller LLMs can surpass larger ones. For more results with the different LLM as critic, please refer to Table 6 in the appendix.

Furthermore, we conduct ablation experiments by skipping Stage 2 and Stage 3. Firstly, the significant performance drop without Stage 3 underscores its necessity for learning policy planning. Without Stage 3, the policy planner can only learn from the utterance encoder, thus failing to acquire planning capabilities. Besides, the slight decline observed without Stage 2 also shows the rationality for proper initialization for effective RL-based policy planning.

1.0 1.0 0.9 0.90 0.87 0.9 0.90 0.92 0.93 0.80 0.80   
0.7 0.68 0.720.70 0.75 0 0.67 0.66 0.74 0.73 0.69 0.6 0.63 0.58 0.64 0.6 0.610.56 0.63 0.5 0.49 0.5 0.49 0.4 0.4 1.8b 4b 7b Q1.5-1.8b Q2-1.5b G-2b   
‚ñ†StandardLoRAProCoTLDPP ‚ñ†StandardProCoTICL-AIFLDDP (a) LLM Size (b) LLM Series

# Human Evaluation

Following previous studies (He et al. 2024), we conduct human evaluation on 50 dialogues randomly sampled from the test in ExTES and P4G, respectively. We selected two training-based baselines, PPDPP and LoRA, based on whether they require predefined policies and a simulated environment. Three annotators are required to compare the dialogues generated by LDPP/PPDPP and LDPP/LoRA. We assess four metrics: Identification (Ide.), Comforting (Com.), Suggestion (Sug.), and Overall (Ove.) for ExTES and three metrics: Information (Inf,), Persuasion (Per.), and Overall (Ove.) for P4G. Detailed instructions for the annotators are provided in the appendix. Results are presented in Table 2. First, LDPP outperforms PPDPP and LoRA in the Ove. metric, aligning with results in Table 1. We observe that LDPP does not like to ask patients for specific details, often providing suggestions quickly after the patient‚Äôs introduction. While providing useful suggestions is crucial and could improve SR evaluation, failing to conduct thorough inquiries impacts the practical experience. To alleviate this phenomenon, designing relevant rewards could be helpful.

# Performance on Different LLMs

To further validate our proposed framework, we conduct experiments on LLMs with different sizes. Specifically, we compare LDPPs based on Qwen1.5-1.8b, 4b, and 7b for different sizes with settings of $( T , L , K ) { = } ( 8 , 4 , 2 4 )$ . The results are shown in Figure 2. We observe that $L D P P$ achieves the best performance in all three different sizes. As LLM size increases, standard prompting and promptingbased method ProCoT show continuous improvement, but they still perform worse than LDPP. In contrast, LoRA Fine-tuning exhibits significant variability. The reason may be that fine-tuning fails to differentiate data quality and train the added parameters sufficiently, harming LLMs‚Äô generalization ability. Besides, we also conduct experiments $( ( T , L , K ) { = } ( 8 , 6 , 2 4 ) )$ using Qwen1.5-1.8b, Qwen2-1.5b, and Gemma-2b for different LLM series and present in Figure 2, we find that LDPP also performs best.

# Latent Policy Visualization

To intuitively demonstrate the learned latent policies, we visualize the policies of system utterances in Figure 3. Initially, each utterance is encoded into a latent policy feature

Feeling anxious due m here to listen to misunderstandings and support you.It is completely under sounds like the standable. recent.   
2   
Feeling frustrated   
and wanting to   
improve the situation Ican understand how shows your dedication draining that must to your work.. be.Can you give... (a) ExTES   
Certainly! Save the. I.agree.Are you Children implements a.wide familiar with a range of programs and 'charity called projects.Some examples Save the Children? include providing... I am fineÔºåthank you, Have you'ever heard Certainly! Save the about the organization Children focuses on \"Save the Children\"? various initiatives.to   
improve the lives of 10   
children around the 11   
world.They provide.   
(b) P4G

Table 3: Results of different $K$ on ExTES.   

<html><body><table><tr><td></td><td>K=6</td><td>K=12</td><td>K=18</td><td>K=24</td></tr><tr><td>SSR</td><td>0.687</td><td>0.652</td><td>0.675</td><td>0.628</td></tr><tr><td>AvgT</td><td>4.50</td><td>5.84</td><td>4.86</td><td>5.77</td></tr></table></body></html>

following Eq.(1) and classified into the closest policy vector in the Codebook. For each policy vector in the Codebook, we select the top-500 closest latent policy features and then apply PCA for dimensionality reduction on them to draw a scatter plot. For ease of presentation, we only display those from the $6 / 4$ most frequently used policy vectors for ExTES/P4G. We also present parts of text utterances for comparison and observe that utterances within the same cluster are indeed semantically similar, validating the effectiveness of stage 1. To better understand these policies, Table 12 in the appendix presents three representative utterances for each of them. These utterance examples can help to understand the semantical operations for policies in the Codebook.

# Parameter Sensitivity Analysis

Codebook Size $K$ . We investigate the impact of Codebook size $K$ on guiding the proactive dialogue process. Experiments are conducted on the ExTES dataset with $K = 6$ , 12, 18, and 24, while keeping other hyper-parameters constant $( T = 8 , L = 4 )$ ), as shown in Table 3. LDPP achieves relatively stable results and performs satisfactorily even with the smallest $K$ , which can be attributed to the method of capturing latent policy features: by computing a weighted sum of the Codebook based on the policy distribution derived from the policy planner, it allows for a semantic combination of different policy vectors within the Codebook. Therefore, even with a small $K$ , a wide range of latent policies can be expressed. However, performance decreased when $K = 2 4$ . We assume that this is due to the increased complexity of predicting the distribution for the larger Codebook, which requires additional training steps.

Table 4: Results of different #policy tokens $( T )$ on ExTES.   

<html><body><table><tr><td></td><td colspan="3">ExTES</td><td colspan="3">P4G</td></tr><tr><td></td><td>L=2</td><td>L=4</td><td>L=6</td><td>L=2</td><td>L=4</td><td>L=6</td></tr><tr><td>SSR</td><td>0.649</td><td>0.628</td><td>0.719</td><td>0.580</td><td>0.711</td><td>0.732</td></tr><tr><td>AvgT</td><td>5.42</td><td>5.77</td><td>3.88</td><td>6.37</td><td>5.85</td><td>5.49</td></tr></table></body></html>

Table 5: Results of different P-Former layers $( L )$ .   

<html><body><table><tr><td></td><td>T=2</td><td>T=8</td><td>T=16</td><td>T=24</td></tr><tr><td>SSR</td><td>0.699</td><td>0.628</td><td>0.619</td><td>0.628</td></tr><tr><td>AvgT</td><td>4.57</td><td>5.77</td><td>6.17</td><td>5.65</td></tr></table></body></html>

#Policy Tokens $T _ { \mathbf { \delta } }$ . Although we aim for these policy tokens to align with the input word embeddings of LLMs, they do not inherently belong to the LLMs‚Äô vocabulary. Therefore, it is important to analyze the potential noise introduced by the policy tokens into the LLMs. We set $T$ as 2, 8, 16, and 24 while keeping $( L = 4 , K = 2 4 )$ ). The experimental results are presented in Table 4. Overall, there is a trend of decreasing dialogue success rate as $T$ increases, indicating that a greater number of policy tokens indeed introduce noise, adversely affecting response generation. In practice, users can reduce the number of query tokens or enhance the capacity of P-Former (e.g., increasing the number of layers) to mitigate the impact of noise.

P-Former Layer $L$ . The number of P-Former layers reflects its parameter size and capability. We hypothesize that a stronger P-Former reduces the gap between the transferred policy tokens and the LLMs‚Äô input space while retaining more policy semantic information. To validate this, we set different layers on ExTES and P4G. The results, presented in Table 5, indicate that the number of P-Former layers impacts dialogue performances, especially on P4G, where more layers notably improve the dialogue success rate. On the P4G dataset, we observed zero improvement. This indicates that only if the P-Former is sufficiently powerful can we effectively utilize the latent policy.

# Conclusion and Future Work

In this work, we introduce a novel learning scenario that discovers potential policies from broadly collected dialogue records and learns policy planning without dynamic interactions with simulated environments. To address this challenge, we propose a new learning framework called LDPP, containing three stages: latent policy discovery, policy distillation, and offline RL enhancement. Experimental results demonstrate that LDPP significantly improves LLMs‚Äô proactive dialogue capabilities, achieving more pronounced and consistent enhancements compared to all baselines, even ChatGPT. Future research will mainly focus on improving the explainability of latent policies, ensuring the reliability of policies used in proactive dialogue.