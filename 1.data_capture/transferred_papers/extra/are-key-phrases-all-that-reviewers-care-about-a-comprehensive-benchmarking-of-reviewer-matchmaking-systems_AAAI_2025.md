# Are Key-Phrases All that Reviewers Care About? A Comprehensive Benchmarking of Reviewer Matchmaking Systems

Sourish Dasgupta\*, Harsh Sharma\*, Devansh Patel\*, Prarthee Desai\*, Anil K. Roy

Dhirubhai Ambani Institute of Information & Communication Technology, India sourish dasgupta, 202111002, 202001262, 202001257, anil roy @daiict.ac.in

# Abstract

Reviewer Matchmaking (RM) is a pivotal process in academic publishing that aligns manuscripts with appropriate reviewers based on their expertise and prior publications. The demand for an automated RM system has escalated with the significant surge in submissions over the past decade. State-of-the-art (SOTA) RM models are document-representation-based (DRRM) and match the manuscript and reviewer’s past publication using a similarity method defined on a high-dimensional vector space. However, they are far from accurate despite their largescale usage. In this paper, we establish that conventional RM evaluation measures are unreliable and instead emphasize that standard correlation measures are adequate. For the first time, we compare the performance of six SOTA DR-RM models with those of fourteen SOTA Key-phrase Extraction-based RM (KPE-RM) models - an alternate unexplored approach. We observe that KPE-RM models show comparable results in many cases, with the new best model being PatternRank-RM - a KPE-RM model beating the best DR-RM model SPECTER2- RM (Pearson: $0 . 0 0 4 +$ , Spearman: $0 . 0 0 6 +$ , Kendall: $0 . 0 4 3 \dot { + }$ ). We conclude that KPE-RM models must be contextualized to the RM task and cannot be used as plug-n-play.

sheer volume hinders reviewers from thoroughly evaluating a long list of manuscripts before deciding on a bid (Zhang et al. 2023). This has emphasized the need for an automated RM system. Ideally, this score should correlate with the reviewers’ confidence scores during reviewing, failing which the RM model stands unreliable. However, there is much scope for improvement in RM systems. A study on selected papers of NeurIPS 2014 shows that there is no correlation between the quality scores given by the reviewers to accepted papers and the actual citation-impact of those papers (Cortes and Lawrence 2021). Besides the inherent subjectivity, such situations are also due to sub-optimal RM.

Document Representation (DR) based RM. Most venues use DR-based RM models such as the Toronto Paper Matching System (Charlin and Zemel 2013), ACL Reviewer (ACLorg 2022), and OpenReview (OpenReview-org 2022). These models are based on document embedding-based matchmaking of manuscripts and the reviewer’s past publications. The central objective is to represent various arguments (i.e., the research question and associated claims and justifications) within a manuscript and map that to related arguments made in the reviewers’ past publications.

# 1 Introduction

Academic journals and conferences are crucial platforms for researchers to share their work and receive expert feedback. The process of optimally assigning appropriate manuscripts to reviewers based on the reviewers’ area of interest, expertise, and availability such that every manuscript is assigned the minimum required reviewer and, at the same time, no reviewer is overburdened by more than the maximum manuscripts is called Reviewer Allocation (RA) (Stelmakh, Shah, and Singh 2019; Cousins, Payan, and Zick 2023; Aziz, Micha, and Shah 2023). A necessary step for RA is Reviewer Matchmaking (RM), also known as paper-reviewer matching, where a set of ”most suitable” reviewers are matched, thereby providing a score to every manuscript-reviewer pair.

Need for accurate RM systems. There has been a significant surge in the number of manuscript submissions received in the past decade, rendering the manually curated, careful RM process unrealistic. Even with a bidding system, the

Key Phrase Extraction (KPE) based RM. As an alternative approach to existing DR-based RM models, we investigate for the first time the efficacy of Key-Phrase Extraction (KPE) models for the RM task. The motivation behind analyzing KPE-based RM was two-fold. First, DR-based models suffer from representational underfitting because it may include content that is either peripheral or generic to a specific track. Secondly, KPE models have shown significant success in downstream NLP tasks such as summarisation (Glazkova and Morozov 2023) and document clustering (Li and Daoutis 2021). We can design KPE-based RM models using KPE models to extract the top- $k$ keyphrases from the manuscripts and the reviewers’ past publications and then apply a suitable similarity algorithm to the two sets of keyphrases. We study fourteen SOTA KPE models of architectural designs that are based on deep neural networks (seven models), graph structures (four models), and statistical approaches (three models). For the first time, we compare KPE-RM models with six SOTA DR-based RM models (Table 1; Appendix).

Observations. In this paper, we first show the inadequacy of existing RM evaluation frameworks and associated measures. More specifically, we argue that the conventional expert-annotation-based measures such as Precision ${ \mathfrak { Q } } \mathrm { K }$ , as introduced in Mimno and McCallum (2007) and have been widely followed since (Karimzadehgan, Zhai, and Belford 2008; Singh et al. 2023a), are incomplete. We also demonstrate that a more recent reviewer-confidence-rating-based leaderboard that uses Kendall Loss (Stelmakh et al. 2023) is unreliable. We instead recommend standard correlation measures of Pearson $r$ , Spearman $\rho$ , and Kendall $\tau$ w.r.t ground-truth confidence scores. We use the gold standard dataset released by Stelmakh et al. (2023) for our study.1 Several KPE-based RM models perform comparably with the best performing DR-based RM model, with KPE-based PatternRank-RM topping the chart. However, we also conclude empirically that the best-performing KPE-based RM models have barely moderate rank correlation. This is because the KPE task is performed as a prequel to the RM task, thereby not extracting keyphrases that are more useful for the RM task, leaving a wide scope for improvement.

# 2 Preliminaries

# 2.1 The Reviewer Matchmaking Problem

Reviewer Matchmaking (RM), also called paper-reviewer match, is the first step in the Reviewer Allocation (RA) process. For each submitted manuscript, the RM model matches reviewers from a fixed candidate reviewer pool based on their expertise to achieve a thorough and fair evaluation of the manuscript, leading to constructive feedback and improved quality of submissions. An RM system represents manuscripts using the content in the title, abstract, and body. On the other hand, it represents the reviewer’s profile using past peer-reviewed publications of the reviewer, often the recent ones. We define the problem as follows:

Definition 1. (Review Matchmaking) Given a set of manuscripts $\mathcal { M } \ = \ \{ m \ : \ ( t i t l e , a b s t r a c t , b o d y ) \}$ and a set of candidate reviewer pool $\mathcal { R }$ , where each reviewer $ { \boldsymbol { r } } \in  { \mathcal { R } }$ has publication profile $Q _ { r } = \{ q _ { r } :$ (title, abstract, body, publication-year) , an RM model $\Theta$ is to learn a relevance-score function f ΘRM : M × R → R and generate a reviewer rank $( { \bar { \mathcal R } _ { m } } )$ for each $m \ w . r . t \ f _ { \Theta } ^ { R m }$ .

# 2.2 RM Evaluation Measures: Limitations

In this section, we discuss the limitations of two RM evaluation frameworks - (i) external expert annotation-based measures and (ii) reviewers’ confidence rating-based measures.

Expert Annotation-based Measure In this evaluation framework, pairs of accepted manuscripts and assigned reviewers are rated in terms of their quality of assignment by external annotators who are experts from the same area where the manuscripts belong (Mimno and McCallum 2007). An RM model is evaluated in terms of the number of times the predicted relevance score (w.r.t $f _ { \Theta } ^ { R M } )$ aligns with the humanjudgment assignment ratings (i.e., $^ { 2 + }$ out of a score between 0-3). More specifically, the central idea is to find the subset of the top- $K$ predicted reviewers $( r _ { m , k } )$ for any manuscript $m$ that has received good human judgments (i.e., ground-truth).

To this end, two measure variants have been proposed - soft Precision ${ \mathfrak { Q } } \mathrm { K }$ and hard Precision ${ \mathfrak { Q } } \mathrm { K }$ as follows:

$$
\begin{array} { l } { \displaystyle \mathrm { S o f t P @ K } = \frac { 1 } { | \mathcal { M } | } \sum _ { m \in \mathcal { M } } \frac { \sum _ { k = 1 } ^ { K } \mathbb { I } ( \operatorname { s c o r e } ( m , r _ { m , k } ) \geq 2 ) } { K } ; } \\ { \displaystyle \mathrm { H a r d } \mathrm { P @ K } = \frac { 1 } { | \mathcal { M } | } \sum _ { m \in \mathcal { M } } \frac { \sum _ { k = 1 } ^ { K } \mathbb { I } ( \operatorname { s c o r e } ( m , r _ { m , k } ) = 3 ) } { K } } \end{array}
$$

Theoretical Limitations: This evaluation framework has multiple drawbacks. First, it is a rather indirect evaluation method that does not guarantee whether an agreement (or disagreement) between the RM model and the annotator also correlated with a high confidence rating of the actual reviewer who assessed the manuscript. Secondly, since the framework only considers accepted manuscripts, it does not consider negative pairs (i.e., rejected manuscripts) that can also receive a high confidence rating from the actual reviewer. Finally, it is overly dependent on the subjective annotation of external annotators, and hence, the inter-annotator agreement needs to be considered before the results can be taken seriously.

Confidence-Rating-based Measure The actual reviewers’ confidence rating-based evaluation framework will always be superior, being more direct. However, due to privacy reasons, in most cases, reviewer profiles are anonymous, thereby making such evaluation infeasible. Earlier results that have been reported cannot be reproduced or reused for newer RM models because of this reason (Rodriguez and Bollen 2008; Qian, Tang, and $\mathrm { \sf W u } 2 0 1 8$ ; Anjum et al. 2019). Stelmakh et al. (2023) proposed a dataset (CMU-RM23) that can simulate the desired situation more directly to some extent (see section 4.1 for details). In this dataset, reviewers are replaced by expert “readers” who have already read the manuscripts (i.e., accepted ones) and have given a confidence score had they been assigned the manuscripts for review. The authors used Kendall Loss $( L _ { K L } )$ as the measure. The key idea is to calculate the number of alignments of the predicted relevance scores $( s _ { r } )$ of the RM model with that of the confidence ratings provided by the reviewer $r$ (denoted $\epsilon _ { r }$ ). $L _ { K L }$ is defined as:

$$
\begin{array} { l } { { \displaystyle { \cal L } _ { K L } = \frac { \sum _ { r } ^ { \mathcal { R } } { \cal L } _ { r } } { \sum _ { r } ^ { \mathcal { R } } \sum _ { i , j = 1 } ^ { m _ { r } } \vert \epsilon _ { r } ^ { ( i ) } - \epsilon _ { r } ^ { ( j ) } \vert } \mathrm { ~ w h e r e } } . } \\ { { \displaystyle { \cal L } _ { r } = \sum _ { i , j = 1 } ^ { m _ { r } } \left( \mathbb { I } _ { \mathrm { e r r } . } \{ ( s _ { r } ^ { ( i ) } - s _ { r } ^ { ( j ) } ) \cdot ( \epsilon _ { r } ^ { ( i ) } - \epsilon _ { r } ^ { ( j ) } ) < 0 \} \cdot \vert \epsilon _ { r } ^ { ( i ) } - \epsilon _ { r } ^ { ( j ) } \vert \right. } } \\ { { \displaystyle \left. + \mathbb { I } _ { \mathrm { t i o } } \{ \left( s _ { r } ^ { ( i ) } - s _ { r } ^ { ( j ) } \right) \cdot ( \epsilon _ { r } ^ { ( i ) } - \epsilon _ { r } ^ { ( j ) } ) = 0 \} \cdot \frac { 1 } { 2 } \vert \epsilon _ { r } ^ { ( i ) } - \epsilon _ { r } ^ { ( j ) } \vert \right) } } \end{array}
$$

Theoretical Limitations: The Kendall Loss $( L _ { K L } )$ , too, has four serious drawbacks in its formulation, explained as follows with examples:

Disproportionate agreement: To illustrate this, let’s say a reviewer, Alice, gives confidence rating (i.e., $\epsilon _ { r }$ ) of 5 to a manuscript $m _ { 1 }$ and 4 to another manuscript $m _ { 2 }$ . Now, say the RM model predicts Alice’s confidence (i.e., $s _ { r }$ ) to be 5 and 1, respectively. Although disproportionate, this will be treated as an agreement with Alice, and hence, no Kendall Loss will be imposed, thereby leading to the incorrect non-assignment of $m _ { 2 }$ to Alice. The same would happen for another reviewer, Bob, whose confidence, say, is 2 for $m _ { 1 }$ and 5 for $m _ { 2 }$ , while the RM model predicts Bob’s confidence to be 4 and 5, respectively.

Unidentified disagreement: This can happen when, in the previous example, the RM model predicts Alice’s rating to be 2 for $m _ { 1 }$ and 1 for $m _ { 2 }$ . Kendall Loss will not be imposed, although clearly, the RM model fails to predict the correct confidence. This would also lead to incorrect non-assignment of $m _ { 1 }$ and $m _ { 2 }$ to Alice. The same goes for Bob, who rates $m _ { 1 }$ as 2 and $m _ { 2 }$ as 2.5, while the model predicts Bob’s rating as 4.5 for $m _ { 1 }$ and 5 for $m _ { 2 }$ .

Minor disagreement: Continuing with the example, if an RM model predicts Alice’s ratings to be 4.3 to $m _ { 1 }$ and 4.5 to $m _ { 2 }$ , then the model will still be imposed a penalty of 1. The same goes when Bob rates $m _ { 1 }$ as 2.3 and $m _ { 2 }$ as 2.5 while the model predicts Bob’s rating to be 2.4 for $m _ { 1 }$ and 2.3 to $m _ { 2 }$ .

Mishandling of tie: Let us take the case where there is a tie in Alice’s ratings to both $m _ { 1 }$ and $m _ { 2 }$ (say, 5), while the RM model predicts Alice’s ratings to be 1 for both $m _ { 1 }$ and $m _ { 2 }$ (i.e., a prediction tie). Although having a huge disagreement, no Kendall Loss will be imposed. The same goes for Bob, who rates both $\overline { { m _ { 1 } } }$ and $\overline { { m _ { 2 } } }$ as 2, while the model predicts that Bob’s ratings will be 5 for both $m _ { 1 }$ and $m _ { 2 }$ . In the case when Alice’s ratings do not have a tie (say she rates 5 for $m _ { 1 }$ and 1 for $m _ { 2 }$ ), while the RM model, as before, has a tie, then a large Kendall Loss penalty of 2 $( = 4 / 2 )$ is imposed without any specific reason why the first case should not be.

# 2.3 Document Representation (DR) based RM

DR-based RM models are used in most of the peer-review systems. These techniques represent manuscripts and reviewer profiles (i.e., past publications) as high-dimensional embeddings. A document includes the title, abstract, and body2. The embeddings (often contextual) are supposed to capture the semantics of the scholarly argumentation. Such representation allows for the computation of similarity relevance scores (pipeline outlined in Figure 3; Appendix).

Limitations: Although DR-based RM methods have shown progress, significant obstacles remain due to representational challenges. First, the representation of the reviewer’s profile might not accurately portray his/her recent area of interest. It might also happen that the reviewer’s profiles may contain irrelevant information from papers that are marginal or peripheral to the manuscript (or even irrelevant), thereby adding noise to the reviewer’s profile representation.

As an alternative to DR-based RM, we propose that KPEbased RM should also be seriously considered. Before we detail the generic outline of KPE-based RM models, we briefly describe the KPE task in the following section.

# 2.4 Key-Phrase Extraction (KPE)

KPE is a textual information processing task responsible for automatically extracting characteristic and representative key phrases covering a document’s aspects (i.e., key ideas). KPE models can be broadly categorized into three types based on their underlying architectures and extraction techniques: (i) term-statistics based (El-Beltagy and Rafea 2010; Campos et al. 2020; Sparck Jones 1972), (ii) graphical-based (Boudin 2018; Bougouin, Boudin, and Daille 2013; Florescu and Caragea 2017; Mihalcea and Tarau 2004; Wan and Xiao 2008), and (iii) deep neural network based that are mostly fine-tuned versions of some pre-trained language model (PLM) (Grootendorst 2020; Sun et al. 2021; Schopf, Klimek, and Matthes 2022; Kulkarni et al. 2022; Kong et al. 2023). We argue that since KPE models are supposed to represent a document’s key ideas concisely, they could be promising alternative for the RM task.

# 3 KPE-based RM: Alternative Approach

KPE-based RM (KPE-RM) models extract key phrases from the manuscripts and the potential reviewers’ past publications. The extracted keyphrases are then used to create profiles that represent the central theme of the manuscript $m$ (i.e., the set $m = \{ t _ { m } : t _ { m } \in \tau _ { m } \}$ where $g _ { \Theta } ^ { K P E } : m \mapsto T _ { m }$ $\mathcal { T } _ { m }$ is the extracted set of key phrases $t$ for $m$ ), and the reviewer $r$ ’s publication profile $Q _ { r }$ $\left( = \ \{ q _ { r } \} \right.$ ; where $q _ { r } =$ $\{ t _ { q _ { r } } : t _ { q _ { r } } \in \mathcal { T } _ { q _ { r } } \}$ where $g _ { \Theta } ^ { K P E } \ : \ q _ { r } \ \mapsto \ { \bar { \mathcal { T } } } _ { q _ { r } } ; \ { \mathcal { T } } _ { q _ { r } }$ is the extracted set of key phrases $t$ for publication $q _ { r }$ ), respectively. The KPE-based RM problem can be reformulated as learning $f _ { \Theta } ^ { R M } ~ : ~ ( m , Q _ { r } ) ~ \mapsto ~ \bar { \mathcal { R } } _ { m } ; \forall m ~ \in ~ \mathcal { M } ; \bar { \mathcal { R } } _ { m } ~ :$ ranked list of reviewers for $m$ . Here, the key phrase $t$ is represented as an embedding $\mathbf { t }$ , and $f _ { \Theta } ^ { R M }$ is a similarity measure on the vector space in which $t$ is defined (we have experimented with cosine and Jaccard). We investigate two types of embeddings for this purpose: GloVe embeddings (Pennington, Socher, and Manning 2014) and Sentence-BERT (SBERT) embeddings (Reimers and Gurevych 2019). For deep neural-based RM models, we also experimented with internal model-generated t embeddings. To compute the aggregate relevance score over all $q _ { r } \in Q _ { r }$ , based on which the rank $\bar { \mathcal { R } } _ { m }$ is generated, we use mean and max (termed Mode). Figure 1 outlines the top-level KPE-RM pipeline.

# 4 Evaluation Setup

# 4.1 Model Benchmarking Dataset

As discussed in section 2.2, to reliably evaluate the comparative accuracy performance between DR-based and KPEbased RM models in a direct way, we need an evaluation dataset that contains gold-reference reviewer confidence ratings and reviewer profiles (i.e., past publications). We, therefore, selected the CMU-RM23 gold-standard dataset (Stelmakh et al. 2023). The dataset consists of 463 manuscripts (i.e., $\mathcal { M } )$ ) and 477 self-reported ground-truth confidence scores (i.e., set of $\epsilon _ { r }$ ) provided by 58 researchers (i.e., $\mathcal { R }$ ) who are domain experts and whose profiles (i.e., $Q _ { r }$ ) are publicly available (see Table 1). In this framework, domain experts select accepted manuscripts they have read and substitute the actual (anonymous) reviewers.

![](images/852fa7482d73fd70ba0c114a62822dfe920270f7653c72e0fd745ec71c71ffcb.jpg)  
Figure 1: KPE model-based RM pipeline.

Table 1: CMU-RM23 Gold Standard Dataset: Stats & domain experts demography; anonymity compliant as per dataset terms.   

<html><body><table><tr><td colspan="3">Total number of manuscripts:463</td><td colspan="3">Total number of reviewers: 58</td></tr><tr><td>Characteristic</td><td>Quantity</td><td>Value</td><td>Characteristic</td><td>Quantity</td><td>Value</td></tr><tr><td rowspan="3">Open Access</td><td># On semantic scholar</td><td>462</td><td rowspan="3">Position</td><td>% PhD student</td><td>45</td></tr><tr><td># On arXiv</td><td>411</td><td>% Faculty</td><td>28</td></tr><tr><td># PDF available</td><td>457</td><td>% Post-PhD (non-faculty)</td><td>12</td></tr><tr><td>Research Areas</td><td># Computer science</td><td>459</td><td>Domain Expertise</td><td># Computer science</td><td>459</td></tr><tr><td rowspan="4">Publication Year</td><td>% Before 2020</td><td>25</td><td rowspan="4">Experience</td><td>Mean # publications</td><td>54</td></tr><tr><td>% 2020 or later</td><td>75</td><td>Median # publications</td><td>20</td></tr><tr><td>Total confidence rating received</td><td>477</td><td>Total # publications</td><td>3112</td></tr><tr><td>Mean manuscripts per reviewer</td><td>7.98</td><td>PDF available # publications</td><td>1506</td></tr></table></body></html>

# 4.2 Evaluation Measure

An RM model’s relevance score must correlate with reviewer confidence to be reliable. A robust evaluation measure should overcome the four issues of Kendall Loss—insensitivity to agreement degree, conflation of disagreement with agreement, over-penalization of minor discrepancies, and undue penalization of ties. We address this using standard correlation measures such as Pearson’s $r$ , Spearman’s $\rho$ , and Kendall’s $\tau$ effectively as per following formulation:

$$
^ 3 \rho _ { \Theta } ( s , \epsilon ) = \mathrm { C O R R } \left( \left\{ \left( \frac { 1 } { m _ { r } } \sum _ { i = 1 } ^ { m _ { r } } s _ { r } ^ { ( i ) } , \frac { 1 } { m _ { r } } \sum _ { i } \epsilon _ { r } ^ { ( i ) } \right) \right\} _ { r = 1 } ^ { \mathcal { R } } \right)
$$

The above formulation ensures that the four limitations of Kendall Loss, as discussed in section 2.2, are handled. Pearson’s $r$ correlation will be $< 1$ for all the four cases described therein. To illustrate how rank correlation measures such as Spearman $\rho$ and Kendall $\tau$ overcome the issues, we continue with the same case-wise examples. In the case of disproportionate agreement, we can see that the RM model will rank as per the mean predicted rating, thereby predicting the mean rating of Alice to be $( 5 + 1 ) / 2 = 3$ , and that of Bob to be $( 4 + 5 ) / 2 = 4 . 5$ , leading to the predicted rank of $\mathbf { B o b } >$ Alice. However, the actual average rating of Alice is $( 5 { + } 4 ) / 2$ $= 4 . 5$ , and Bob is $( 2 + 5 ) / 2 = 3 . 5$ . Therefore, ideally, the rank should have been Alice $>$ Bob. Hence, unlike Kendall Loss, the rank correlation measures will penalize the model due to misalignment with target ranking. The second problem of unidentified disagreement will not occur since, again following the example in section 2.2, the actual ranking based on Alice’s average rating (4.5) and Bob’s average rating (2.25) will lead to the rank of Alice $> \mathrm { B o b }$ , while the model will predict Alice’s average rating to be 1.5 and Bob’s rating to be 4.75, leading to just the opposite ranking. Therefore, the disagreement will not be missed. The third problem of penalization of minor disagreement will also not show up since, following the example in this case, the model’s predicted ranking will not be different than the actual ranking of Alice $>$ Bob. Finally, the anomaly of mishandling of tie does not appear since, as per the first example, the actual ranking will be Alice $> \mathrm { B o b }$ , while the model’s predicted ranking will be $\mathbf { B o b } >$ Alice (Alice’s predicted avg. rating is 1 and that of

<html><body><table><tr><td>Hyperparameter</td><td>Explanation</td><td>Notation/Values</td></tr><tr><td>Similarity Method</td><td>Applied similarity measure and the associated representation</td><td>Similarity measure: {cos (cosine), jaccard} Rep.: {SBERT,GloVe,KPE model's internal embedding }</td></tr><tr><td>Mode</td><td>Type of manuscript-reviewer cross similarity</td><td>{mean, max}</td></tr><tr><td>#Extracted Key Phrases (top-k)</td><td>The optimal number of key phrase to be extracted</td><td>k ={5,15,30}</td></tr><tr><td>Sub-word Elimination</td><td>Elimination of sub-words/phrases</td><td>nd: no-discard</td></tr><tr><td></td><td>from top-k (to avoid redundancy) The precedence order of the operations:</td><td>d: discard +: sub-word elim.→select top-k</td></tr><tr><td>Sub-word Elimination Order</td><td>sub-word elimination& select top-k</td><td>-select top-k→sub-word elim.</td></tr></table></body></html>

Table 2: KPE-RM Hyperparameters: This leads to the ablation study of 672 KPE-RM model variants.

Bob’s is 5) leading to a penalty.

# 4.3 SOTA RM Models Evaluated

We select six of the best-performing SOTA DR-RM models on the CMU-RM23 dataset (OpenReview-org 2022) for comparative analysis with fourteen SOTA KPE-RM models (see Table; Appendix). The SOTA KPE models were chosen based on their recent performance on scientific article datasets such as Inspec (Hulth 2003), Krapivin (Krapivin, Autaeu, and Marchese 2009), NUS (Nguyen and Kan 2007), and SemEval 2017 (Augenstein et al. 2017) and in terms of their architectural style. More specifically, we study the effect of the architecture (viz. term-statistics-based (Sparck Jones 1972; Campos et al. 2020; El-Beltagy and Rafea 2010), graphtraversal-based (Florescu and Caragea 2017; Wan and Xiao 2008; Mihalcea and Tarau 2004; Bougouin, Boudin, and Daille 2013; Boudin 2018), and deep neural pre-trained model-based (Schopf, Klimek, and Matthes 2022; Sun et al. 2021; Kulkarni et al. 2022; Grootendorst 2020; Xie et al. 2023; Kong et al. 2023)) and the corresponding key phrase extraction technique on the RM performance (Appendix A).

# 4.4 KPE-RM Model Hyperparameters

We analyze the twenty most recent publications per reviewer and conduct ablation studies on fourteen KPE-RM variants with five hyperparameters: (i) similarity method, (ii) mode, (iii) number of key phrases (top- $k$ , with $k$ -values of 5, 15, and 30), (iv) application of sub-word elimination, and (v) the order of sub-word elimination (see Table 2). The similarity method pairs a metric-space representation with a similarity measure. We employ three representations: key phrase-based bag-of-words (BoW), static term embedding (GloVe (Pennington, Socher, and Manning 2014)), and contextual term embedding (SBERT (Reimers and Gurevych 2019) along with the KPE model’s internal embedding), using Jaccard for BoW and cosine for embeddings. We compute pairwise similarity between manuscripts and reviewer profiles, examining the effects of two aggregation modes (mean and max) and assessing sub-word elimination—especially when applied before top- $k$ selection—on RM performance.

# 5 Observation & Insights

In this section, we outline the comprehensive comparative analysis between the SOTA DR-RM and KPE-RM models. Our experiment was conducted on two setups - (i) title $^ +$ abstract and (ii) full-text (i.e., title $^ +$ abstract $^ +$ body).4

# 5.1 Term-statistics based KPE-RM Models

Title $^ +$ Abstract: We observe that term-statistics-based RM models are notably weaker than DR-based models in terms of Pearson $r$ (except for BM25-RM, which also is a term-statistics-based model; see Table 3). This suggests that term-statistics as a feature is not enough.

Full-text: We observe that KPE models such as KPMinerRM (cos-SBERT/mean/15/nd) that are designed to harness the full-text utilizing the term-positional inductive bias perform better than title+abstract. However, even after providing additional content about the problem statement and methodology - aspects that can highly impact RM performance, there is no particular change overall (see Figure 2f.).5 This indicates that there is no particular correlation between the core aspects relevant for RM and term frequency.

# 5.2 Graph-traversal-based KPE-RM Models

Title $^ +$ Abstract: We find that several graph-traversalbased models, such as PositionRank-RM (cos-SBERT & Jaccard variants), SingleRank-RM (cos-SBERT), and TextRankRM (Jaccard), have comparable performance with the DRRM models in terms of Pearson $r$ and Spearman $\rho$ . Interestingly, all these models outperform DNN-based models except for the best RM model - PatternRank-RM (Jaccard/mean/15/nd). This indicates that the embeddings generated are sub-optimal, thereby leading to a sub-optimal top- $k$ list. On the other hand, the superiority of PositionRankRM is due to the incorporation of term-position information, whose benefit we also see in KPMiner for full-text.

Full-text: We find that graph-traversal-based models can utilize the full-text content better than term-statistics-based and DNN-based models. Specifically, we observe that TopicRank (cos-SBERT/mean $/ 1 5 / \mathrm { n d } \AA ,$ ) and MultipartiteRank-RM (cos-SBERT/max/30/nd) have notably better results than all DR-RM model results on full-text. In fact, MultipartiteRank tops the chart in terms of all the correlations. We believe that this is due to the ranking of top- $k$ based on the mapping of inter-connected key phrases (forming the context) to a common topic (key phrase cluster) set within the graph structure, which bears direct evidence that topic clustering-based topicgraph aids RM performance. As evident, such a technique would implicitly require full-text content to perform well.

Table 3: Accuracy Chart: Best-performing RM model-variants on Title $^ +$ Abstract (manuscripts and reviewers’ publications); top-3 scores are underscored (bold for best performer); NT: \*\*Kendall Loss is shown to be unreliable in section 2.2 and established so empirically in section 5.5. For results on full-text see Table 7 in Appendix C.1.   

<html><body><table><tr><td colspan="8">Comparative Performance of KPE-RM models vs.DR-based RM models (Title+ Abstract)</td></tr><tr><td></td><td>Similarity Method Mode</td><td>No. of Key-phrases</td><td>Discard sub-words</td><td>Pearson r</td><td>Spearman p</td><td>Kendall T</td><td>KendallLoss**</td></tr><tr><td colspan="8">KPE-basedRMModels</td></tr><tr><td colspan="8">Term-statistics-based KPE-RMModels</td></tr><tr><td>TF-IDF-RM</td><td>COs-SBERT mean</td><td></td><td>30</td><td>nd</td><td>0.357</td><td>0.322</td><td>0.267</td><td>0.283</td></tr><tr><td>Yake-RM</td><td>jaccard</td><td>mean</td><td>30</td><td>nd</td><td>0.329</td><td>0.287</td><td>0.231</td><td>0.310</td></tr><tr><td>KPMiner-RM</td><td>Cos-SBERT</td><td>mean</td><td>5</td><td>d</td><td>0.295</td><td>0.265</td><td>0.216</td><td>0.345</td></tr><tr><td colspan="9">Graph-traversal based KPE-RMModels</td></tr><tr><td>PositionRank-RM</td><td>COS-SBERT</td><td>max</td><td>30</td><td>nd</td><td>0.359</td><td>0.360</td><td>0.279</td><td>0.281</td></tr><tr><td>PositionRank-RM</td><td>jaccard</td><td>mean</td><td>30</td><td>nd</td><td>0.370</td><td>0.353</td><td>0.279</td><td>0.301</td></tr><tr><td>SingleRank-RM</td><td>COs-SBERT</td><td>max</td><td>15</td><td>nd</td><td>0.356</td><td>0.336</td><td>0.261</td><td>0.305</td></tr><tr><td>TextRank-RM</td><td> jaccard</td><td>mean</td><td>30</td><td>nd</td><td>0.352</td><td>0.333</td><td>0.272</td><td>0.313</td></tr><tr><td>TopicRank-RM</td><td> jaccard</td><td>mean</td><td>15</td><td>nd</td><td>0.304</td><td>0.324</td><td>0.258</td><td>0.332</td></tr><tr><td>MultipartiteRank-RM</td><td>jaccard</td><td>mean_</td><td>15</td><td>nd</td><td>0.328</td><td>0.318</td><td>0.246</td><td>0.314</td></tr><tr><td colspan="9"></td></tr><tr><td>PatternRank-RM (best RM)</td><td></td><td>mean</td><td>Deep Neural Network (DNN)-based KPE-RMModels_ 15</td><td>nd</td><td>0.433</td><td>0.423</td><td>0.339</td><td></td></tr><tr><td></td><td>jaccard</td><td></td><td>30</td><td></td><td>0.299</td><td>0.283</td><td></td><td>0.273</td></tr><tr><td>BERTKPE-RM</td><td> jaccard</td><td>max</td><td></td><td>nd</td><td>0.289</td><td>0.292</td><td>0.235</td><td>0.337</td></tr><tr><td>KeyBART-RM</td><td> jaccard</td><td>mean</td><td>15</td><td>nd</td><td>0.363</td><td>0.367</td><td>0.221</td><td>0.325</td></tr><tr><td>KeyBERT-RM</td><td> jaccard</td><td>mean</td><td>-5</td><td>d</td><td>0.281</td><td>0.272</td><td>0.3</td><td>0.297</td></tr><tr><td>One2Set-RM PromptRank-RM</td><td>COs-SBERT jaccard</td><td>max mean</td><td>-30 30</td><td>d nd</td><td>0.381</td><td>0.371</td><td>0.203 0.297</td><td>0.323 0.283</td></tr><tr><td colspan="9"></td></tr><tr><td colspan="4">DR-basedRMModels SPECTER2-RM(best DR-RM)</td><td>0.425</td><td>0.417</td><td></td><td>0.296</td><td>0.22</td></tr><tr><td colspan="8">SPECTER2+SciNCL-RM</td><td></td></tr><tr><td colspan="4"></td><td>0.418</td><td></td><td>0.406</td><td>0.289</td><td>0.207</td></tr><tr><td colspan="2">SciNCL-RM</td><td></td><td></td><td></td><td>0.394</td><td>0.377</td><td>0.267</td><td>0.22</td></tr><tr><td colspan="2">SPECTER-RM</td><td></td><td></td><td></td><td>0.385</td><td>0.334</td><td>0.237</td><td>0.272</td></tr><tr><td colspan="2">SPECTER+MFR-RM</td><td></td><td></td><td></td><td>0.370</td><td>0.315</td><td>0.224</td><td>0.235</td></tr><tr><td colspan="2">BM25-RM</td><td></td><td></td><td></td><td>0.152</td><td>0.248</td><td>0.177</td><td>0.359</td></tr></table></body></html>

# 5.3 DNN-based KPE-RM Models

Text+Abstract: PatternRank-RM (Jaccard/mean/15/nd), KeyBERT-RM (Jaccard/mean/-5/d), and PromptRank-RM (Jaccard/mean/30/nd) outperform SPECTER2-RM—the best DR-RM model—across various correlation measures, with PatternRank-RM topping the chart. This underscores that exploring alternative KPE-RM-based techniques is promising. However, embeddings from these models, as well as those from GloVe/SBERT, do not yield optimal RM results, indicating significant scope for improvement since even the best correlations remain only moderate.

Full-text: We find that most DNN-RM models cannot effectively use the additional aspect content of full-texts, except for PatternRank-RM (cos-SBERT/max $^ { \prime } +$ & - 15/d), which performs comparably to SPECTER2-RM in terms of Pearson $r$ and Spearman $\rho$ and outperforms it in Kendall $\tau$ .

# 5.4 Ablation Studies

Effect of Similarity Method. We find that GloVe-based cosine similarity method did not perform well compared to more contextual embedding-based methods like SBERTbased and KPE-models’ internal embedding (Figure 2a.). Notably, the embedding-based similarity method (cos-SBERT) outperforms Jaccard when applied to full-texts, making it less suitable for RM on shorter texts. On average, internal embedding boosts RM performance more than SBERTgenerated embedding.

Effect of Mode. We do not observe any notable difference in the RM performance if we switch mode from max to mean (see Figure 2b.).

Effect of # top- $\boldsymbol { \cdot } \boldsymbol { k }$ Key Phrases. As expected, it is evident from Figure 2c., that if $k = 5$ , KPE models suffer w.r.t recall, thereby leading to poor RM. However, we do not find any difference in effect between $k = 1 5$ and 30.

Effect of Sub-word Elimination. We find that sub-word elimination degrades the overall RM performance (see Figure 2d.). This is because several of the identified sub-words have very different semantics; hence, retaining them is better. This also suggests that we need more sophisticated sub-word elimination to handle redundancy.

Effect of Order of Sub-word Elimination. We do not observe any notable difference in the RM performance if we switch the order of the elimination operation (see Figure 2e.).

Effect of Format. Although the best-performing models perform better on title $^ +$ abstract, we observe that there is no overall effect of full-text on the studied models (see Figure 2f.). As mentioned earlier, this indicates the scope of improvement in utilizing RM-relevant aspect content.

# 5.5 Incompatibility of Kendall Loss

In section 2.2, we discuss the theoretical drawbacks of Kendall Loss. We also do not find any empirical equivalence between Kendall Loss-based leaderboard and that generated by the standard correlation measures. In fact, as we see from Table 4, they have strong negative correlation w.r.t Spearman $\rho$ and Kendall $\tau$ .

![](images/e0974a742fc571fca2287bf255c9b5d4bde122b7d936c778b35c3e1f55d1ef70.jpg)  
Figure 2: Ablation Results: Effect of similarity method (a), mode (b), top- $k$ (c), sub-word elimination (d), sub-word elimination order (e), and format (f).

# 6 KPE Models cannot be Plug-n-played

In Table 5 we see disagreement between the leaderboards of the studied models w.r.t the RM task and that of the classical KPE task on four standard evaluation datasets comprising scholarly articles. This shows that top-performing KPE models cannot be plug-n-played for the RM task.

# 7 Related Work

A widely used term-statistics-based DR-RM model is OkapiBM25-RM (Robertson and Walker 1994). An early DNNRM model, ELMo-RM, relies on ELMo for document embeddings (Peters et al. 2018). In recent years, SPECTERRM (Cohan et al. 2020) and its latest version, SPECTER2- RM (Singh et al. 2023b), generate embeddings of scientific papers using citation-aware fine-tuning on SciBERT.

Table 4: Inadequacy of Kendall Loss: Inter-correlation of Kendall Loss with std. corr. measures strongly negative.   

<html><body><table><tr><td colspan="3">Leaderboard Correlation:KendallLoss based vs.Std.Corr.Measures based</td></tr><tr><td>Inter-Corr.</td><td>Pearsonr-rank</td><td>Spearman p-rank Kendall T-rank</td></tr><tr><td colspan="3">Title+Abstract</td></tr><tr><td></td><td></td><td></td></tr><tr><td>Spearman</td><td>-0.927</td><td>-0.794 -0.624</td></tr><tr><td>Kendall Tau</td><td>-0.782 -0.641</td><td>-0.471</td></tr><tr><td colspan="3">Full-Text</td></tr><tr><td>Spearman</td><td>-0.827 -0.839</td><td>-0.692</td></tr><tr><td>Kendall Tau</td><td>-0.649 -0.657</td><td>-0.492</td></tr></table></body></html>

Table 5: Plug-n-play will not work: Leaderboard w.r.t KPE task does not correlate with KPE-RM Leaderboard   

<html><body><table><tr><td colspan="4">LeaderboardCorrelation:KPETask(F1@10/M)vs.KPE-RMTask</td></tr><tr><td>Inter-Corr.</td><td>Pearsonr-rank</td><td>Spearmanρ-rank</td><td>Kendall 𝑇-rank</td></tr><tr><td colspan="4">Inspec Dataset</td></tr><tr><td></td><td>-0.123</td><td>0.203</td><td>-0.098</td></tr><tr><td>Spearman p</td><td></td><td></td><td>-0.015</td></tr><tr><td>Kendall </td><td>-0.070</td><td>0.198</td><td></td></tr><tr><td></td><td></td><td>Krapivin Dataset</td><td></td></tr><tr><td></td><td>-0.406</td><td>-0.382</td><td></td></tr><tr><td>Spearman p</td><td></td><td></td><td>-0.442</td></tr><tr><td>Kendall T</td><td>-0.289</td><td>-0.289</td><td>-0.289</td></tr><tr><td></td><td></td><td>NUS Dataset</td><td></td></tr><tr><td>Spearman p</td><td>-0.382</td><td>-0.370</td><td>-0.430</td></tr><tr><td>Kendall T</td><td>-0.244</td><td>-0.244</td><td>-0.244</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>SemEval 2017 Dataset</td><td></td></tr><tr><td>Spearman p</td><td>-0.067</td><td>0.083</td><td>-0.150</td></tr><tr><td>Kendall T</td><td>-0.056</td><td></td><td></td></tr></table></body></html>

Another DNN-RM model, SciNCL-RM (Scientific Neighbourhood Contrastive Learning), is based on the SciNCL model (Ostendorff et al. 2022) that uses controlled nearestneighbor sampling over citation graph embeddings to create a positive and negative sample. An extension to this technique is SPECTER $\mathrel { \mathop : }$ SciNCL-RM (OpenReview-org 2022), which combines citation graph information and the neighborhood contrastive learning approach. Very recently, SPECTER $. +$ MFR-RM has been proposed that uses a model called MFR (Lin et al. 2023) that captures the different aspects (e.g., methods, experimental setup, results & analysis, etc.) of the manuscript and past publications. However, as shown in this paper, all these models are far from acceptable.

# 8 Conclusion

In this paper, we first establish that conventional RM evaluation measures such as Precision ${ \mathfrak { Q } } \mathbf { K }$ and Kendall loss are inadequate. Instead, we recommend standard correlation measures. For the first time, we have done an extensive comparative analysis of six SOTA document-representationbased RM models with KPE-based RM models. We observe that KPE-RM models are comparable to the DR-RM models and can be a promising alternative direction. However, we also observe performing well in the KPE task does not lead to high RM accuracy, thereby needing KPE-RM models to be more RM-task-oriented.