# Structured Packing in LLM Training Improves Long Context Utilization

Konrad Staniszewski1, 2, Szymon Tworkowski1, 6, Sebastian Jaszczur1, 2, Yu Zhao3, Henryk Michalewski1, 4, Łukasz Kucin´ ski1, 2, 5, Piotr Miłos´1, 2, 5

1University of Warsaw, Krakowskie Przedmiescie 26/28, 00-927 Warsaw, Poland 2IDEAS NCBR, Chmielna 69, 00-801 Warsaw, Poland 3University of Edinburg, Old College, South Bridge, Edinburgh EH8 9YL, United Kingdom 4Google DeepMind, 5 New Street Square, London, United Kingdom 5Institute of Mathematics Polish Academy of Sciences, Jana i Jedrzeja Sniadeckich 8, 00-656, Warsaw, Poland 6xAI, San Francisco Bay Area, California, U.S. ks.staniszewski $@$ uw.edu.pl

# Abstract

Recent advancements in long-context language modeling have attracted significant attention, yet their practical applications often suffer from suboptimal context utilization. To efficiently address this issue, we introduce the Structured Packing for Long Context, SPLICE, a method that uses retrieval to collate mutually relevant documents into long training samples. We demonstrate that SPLICE improves performance on long-context tasks, particularly by achieving perfect accuracy on the synthetic Needle in the Haystack benchmark, and effectively mitigating the ‘lost-in-the-middle’ phenomenon often observed in large language models. Notably, these longcontext capabilities also extend to realistic downstream tasks, such as Qasper, across multiple model sizes—3B, 7B, and 13B—and are achieved with only brief fine-tuning on 2-6 billion tokens. We supplement these results with a detailed analysis of SPLICE, examining the impact of hyperparameter choices, the different mixtures and proportions of SPLICEgenerated training data, and the choice of the retriever. We also study the transfer of long-context utilization skills between the modalities. An intriguing finding from our analysis is that training on a corpus of code can enhance performance on natural language tasks.

# Code — https://github.com/ideas-ncbr/publications 2024 Extended version — https://arxiv.org/abs/2312.17296

# 1 Introduction

Large language models (LLMs) (Brown et al. 2020; Chowdhery et al. 2022; Lewkowycz et al. 2022; OpenAI 2023a; Bai et al. 2023) have transformed the field of AI. Recently, the field has observed the rise of Long Context Language Models (LCLMs) that promise to unveil novel and powerful capabilities (Anthropic 2023; OpenAI 2023b; Gemini Team 2024). However, their ability to process long contexts is not always as effective as one hopes. Indeed, several studies have highlighted an important limitation: when processing prompts composed of multiple documents, LCLMs frequently encounter difficulties in accurately extracting relevant information (Tworkowski et al. 2023; Liu et al. 2023; Shi et al. 2023; Kamradt 2023). Additionally, they typically find it challenging to utilize information from the middle of their inputs (Liu et al. 2023), even on simple synthetic retrieval tasks (Li et al. 2023a). Understanding these issues is vital for advancements in LCLM technologies and calls for systematic research.

In this work, we take a step towards better context utilization in LCLMs. We focus on training data, keeping other components, such as the architecture and training objectives, unchanged. The broad question is: Given training data consisting of documents, how should these documents be organized into training samples to enhance long-context capabilities? While this perspective has received some attention recently (Levine et al. 2022; Chan et al. 2022; Shi et al. 2024), the problem remains unsolved. The central finding of this work is that structuring training data to increase semantic interdependence is an effective strategy towards better long context utilization. We achieve this by introducing and evaluating Structured Packing for Long Context (SPLICE), a method for creating training samples by using retrieval (e.g., BM25, Contriever) to collate mutually relevant documents into a single training context.

We empirically validate SPLICE showing that fine-tuning of OpenLLaMA 3Bv2, 7Bv2 (Geng and Liu 2023) and CodeLlama 13B (Rozie\`re et al. 2023) with mere 2B–6B tokens already brings improvements in handling long context information in downstream tasks that require retrieval and in-context learning. These tasks include Qasper (Dasigi et al. 2021) from SCROLLS (Shaham et al. 2022), HotPotQA (Yang et al. 2018), Needle In A Haystack (Kamradt 2023), TREC (Li and Roth 2002; Hovy et al. 2001), and DBpedia (Lehmann et al. 2015). We also show that SPLICE significantly alleviates the ’lost-in-the-middle’ phenomenon (Liu et al. 2023) and outperforms standard example packing on the Needle In A Haystack task (Kamradt 2023) (see Figure 1). We perform a comprehensive study of the design choices and properties of SPLICE, showing, in particular, that the acquired long context capabilities transfer between modalities, such as code and text. SPLICE also helps to retain and in some cases even improve performance on short context benchmarks like GSM8K (Cobbe et al. 2021), MMLU (Hendrycks et al. 2021) and TREC (Li and Roth 2002; Hovy et al. 2001).

Our contributions can be summarized as follows:

5   
0.8   
0.6SPLiCe 7B CL Position in Context

• We comprehensively show that structuring training data is a viable way of improving the long context utilization. To this end, we introduce SPLICE, a method for creating training samples by using retrieval to collate mutually relevant documents into a single sample. • We fine-tune OpenLLaMA 3Bv2, OpenLLaMA 7Bv2 (Geng and Liu 2023) and CodeLlama 13B (Rozie\`re et al. 2023) using SPLICE, showing that it improves long-contex downstream performance. • We provide a comprehensive analysis of SPLICE’s design choices, including retrieval parameters and document concatenation order, and evaluate its robustness and scalability with varying data sources and a parametrizable noisy retriever.

# 2 Method

SPLICE is a method for constructing training samples that improve the effectiveness of long-context fine-tuning. This leads to improved performance in tasks such as incontext learning, question answering, information retrieval, and long-context language modeling (see Section 3).

Rationale and Intuitions Capturing long-range dependencies is believed to enhance language modeling and retrieval-augmentation (Borgeaud et al. 2022). It is an open question how to achieve such benefits in pre-training or finetuning. The primary difficulty comes from long-range dependencies being rare in training data (de Vries 2023) and diminishing with distance. Thus, it is unlikely that a model will learn to utilize long context without more guidance.

Recent studies indicate that structuring data, i.e., going beyond the i.i.d. paradigm, might be beneficial or even necessary to achieve good long-context performance. (Levine et al. 2022) develops a theory showing that the trained model establishes stronger dependencies between text segments in the same training sample. Whereas concurrently to our work (Shi et al. 2024) shows that pre-training on structured data improves performance (see also Section 4 for a more detailed comparison). SPLICE follows these intuitions, and constructs training samples by concatenating mutually relevant documents to increase the dependency density, thus allowing the model to learn to utilize long context.

![](images/b25eebdd4e358552fc139c0f95fe1317f2b7c9f5477bca9257992c0fee8260ff.jpg)  
Figure 1: SPLICE vs EXAMPLE PACKING (EP) (BASELINE) on Needle in a Haystack. A model fine-tuned with SPLICE achieves perfect accuracy in retrieving fine-grained information over the whole context, while the baseline can only handle a small final segment (details in Appendix N).   
Figure 2: Training samples generated by Example Packing, Within-Domain Example packing, and SPLICE. Similar colors and shapes indicate related documents, which could be found using a retrieval method (e.g., BM25 or Contriever) or metadata (e.g., git repository structure).

Structured Packing for Long Context (SPLICE) SPLICE starts by picking a random document from the dataset to create a root of a tree and continues in a breadth-first manner, each time appending top- $\mathbf { \nabla } \cdot k$ similar documents from the corpus. The final sequence is generated by flattening the tree according to a specific traversal strategy; see Algorithm 1. The hyperparameter $k$ introduces flexibility, enabling interpolation between different retrieval modes. Specifically, when $k = 1$ , SPLICE simulates a long document by creating a path of related examples. For larger $k$ values, SPLICE generates examples akin to those used in retrieval-augmented models, e.g. (Borgeaud et al. 2022).

SPLICE Retrieval Many possible retrieval methods can be used with SPLICE (RETRIEVE function in Algorithm 1). In our experiments, we test the following:

• SPLICE REPO: based on additional meta-information about the data, that is the repository structure of the code (REPO): we concatenate files using a depth-first search algorithm on the directory structure, that is files from the same directory are grouped together. A similar method has been pioneered by (Wu et al. 2022) and proposed in (Shi et al. 2024) as an interesting future direction. • SPLICE BM25: based on BM25 (Robertson and Zaragoza 2009; Bassani 2023), a standard retrieval method that uses a bag-of-words approach to rank documents based on their similarity to a query. • SPLICE CONT: based on Contriever-MSMARCO (CONT) (Izacard et al. 2022), a retrieval method that uses a transformer to rank documents based on their similarity.

SPLICE Computational Efficiency Given the dataset sizes used in training LLMs, computational efficiency plays a crucial role. SPLICE REPO is the fastest and easiest to implement but requires additional directory structure, i.e., it does not apply to general web data. SPLICE BM25 uses a bag

![](images/7543582f0021772c8b045d2236cbecb1babb28375267f789cdaec9ced6f71901.jpg)  
Figure 3: Key-value retrieval performance on a dictionary of 300 key-value pairs $\approx 2 4 \mathrm { K }$ tokens). The 7B CL model trained with SPLICE achieves much higher accuracy on hard-to-retrieve positions in the middle than the Example Packing Baseline. The details about this task can be found in Appendix D. Each position averaged over 500 examples.

# Algorithm 1: SPLICE training sample construction

Input: $D$ : document corpus $k$ : breadth hyper-parameter $L$ : maximum length of returned training sample RETRIEVE: retrieval method to use, e.g., BM25 ORDER: ordering method, e.g., identity, or shuffle   
Output: training sample   
$d _ { r } \sim D$ Sample the root document   
$D = D \setminus \{ d _ { r } \}$   
$C = [ d _ { r } ]$   
$Q = \exp \mathrm { { t y } }$ queue   
$Q . \mathrm { P U S H } ( d _ { r } )$   
while $Q \neq \emptyset$ and $1 \mathsf { e n } ( C ) \leq L$ do $d = Q . \mathrm { { P O P } } ( )$ ） $d _ { 1 } , \dots , d _ { k } = \mathrm { R E T R I E V E } ( d , k )$ {Retrieve to $\mathsf { p } { - } k \mathrm { ~ m o s t ~ }$ similar documents to $d$ using a selected method, e.g., BM25} for each $d _ { i }$ in $d _ { 1 } , \ldots , d _ { k }$ do if $d _ { i } \in D$ then {RETRIEVE uses a precomputed index and may return documents that are already in $C \}$ $C = C$ .APPEND $( d _ { i } )$ {Append $d _ { i }$ to $C \}$ $Q . \mathrm { P U S H } ( d _ { i } )$ $D = D \setminus \{ d _ { i } \}$ end if end for   
end while   
return CONCAT(TRIM(ORDER(C), L))

of words BM25 method that lacks deeper semantic encoding. However, it was observed to have strong generalization properties (Thakur et al. 2021). SPLICE CONT requires calculating embeddings for each document and retrieval based on the vector inner-product, but can have poorer generalization properties than BM25 (Thakur et al. 2021). The retrieval step can be done efficiently using a fast approximate max IP search, e.g., Faiss (Johnson, Douze, and Je´gou 2017). To reduce the number of times the training sample requires just copy-paste abilities and improve training step efficiency, we employ the StarCoder (Li et al. 2023b) dataset, which was deduplicated using the pipeline from (Allal et al. 2023).

# 3 Experiments

In this section, we show that SPLICE improves the long context performance of large-scale language models. To this end, we use 3B, 7B, and 13B parameter models. First, in Section 3.3, we focus on tasks that test in-context learning, question answering, and in-context information retrieval. Next, we show that SPLICE can improve the core model capabilities by testing its short context performance. Finally, in Section 3.4, we train over 40 medium-size models (270M parameters) using different data mixtures and SPLICE parameters to analyze various design choices, robustness to noise, and scaling properties.

An important finding of our work is that presented improvements occur during a relatively short fine-tuning. To be more precise, 3B models were tuned on 5.4B tokens, whereas 7B and 13B models were tuned on 2B tokens.

# 3.1 Baselines

We consider two popular baselines used in LLM training pipelines. The first one is Example Packing (Brown et al. 2020), used in the training of GPT-3 models. It constructs training samples by randomly sampling documents from the dataset and separating them with BOS/EOS tokens. The second one, which we call Within-Domain Example Packing takes random documents from the same meta-class (for example, Wikipedia, C source code) and concatenates them to create a training sample (Groeneveld et al. 2024; Zhao et al. 2024). We compare SPLICE against both baselines. As we note no clear benefit of Within-Domain Example Packing over Example Packing in fine-tuning case (see Table 21 in Appendix B.3 ) in the main body of the paper we compare only against a more established Example Packing. We visualize the differences between baselines in Figure 2.

# 3.2 Experimental Setup

For 3B model experiments, we fine-tune on a $5 0 / 5 0 ~ \mathrm { m i x } -$ ture of RedPajama, prepared in the standard way, and C prepared using SPLICE BM25. For 7B and 13B ones, we finetune on a 50/25/25 mixture of RedPajama (50) prepared in the standard way, StackExchange (25) and C (25) prepared using SPLICE BM25. StackExchange is part of RedPajama (TogetherComputer 2023), and C data come from StarCoder (Li et al. 2023b). Including the standard RedPajama aims to prevent the model from overfitting to artificially created documents and is inspired by (Ouyang et al. 2022; Rozie\`re et al. 2023). We analyze the impact of data mixture in Section 3.4. We fine-tune with 32K context length. We employ the Focused Transformer (FoT) (Tworkowski et al. 2023) and CodeLlama (CL) context extension methods (Rozie\`re et al. 2023). We use a batch size of 256K (512K, resp.) tokens per step for 3B and 7B (13B, resp.) models. We set the learning rate of 1.5e 5 with linear warmup and cosine decay, following (Geng and Liu 2023). In the next section, we test eight models:

3B FoT, 7B FoT, 7B CL, 13B FoT $\} \times \{ \mathrm { S P L I C E , E P } \} .$ , where EP denotes the standard Example Packing (Brown et al. 2020) method (serving as baseline) where context is created by sampling random documents from the corpus and separating them with BOS/EOS tokens (see Figure 2). We provide results regarding the Within-Domain Example Packing baseline in Appendix C.2. If not stated otherwise, in SPLICE we use $k = 1$ and the identity permutation as Order in the Algorithm 1. Hyperparameter details can be found in Appendix A.

# 3.3 Experimental Results

In-Context Learning In this section, we ask the following research questions: Does SPLiCe improve in-context learning abilities? If so, with what context length is it the case? To answer those questions, we evaluate the accuracy of our models on TREC (Li and Roth 2002; Hovy et al. 2001) and DBpedia (Lehmann et al. 2015), which are text classification tasks. For TREC we test $\{ 2 , 1 6 , 3 2 \} \mathrm { K }$ context lengths, which correspond to $\{ 9 0 , 7 8 0 , 1 5 6 0 \}$ in context examples, respectively. For DBpedia, we test $\{ 1 6 , 3 2 \} \mathrm { K }$ context lengths, which correspond to $\{ 1 9 0 , 3 8 0 \}$ in-context examples, respectively, and omit the 2K length due to its limited capacity of 20 in-context examples. For each context length, we average the results across several sets of in-context examples and provide average improvement of SPLICE and its $9 5 \%$ bootstrap confidence interval (improvements are calculated per set of in-context examples, see Appendix H). In both tasks and all considered context lengths, we note that SPLICE significantly improves in-context learning abilities in comparison to both Example Packing and the starting checkpoint. We hypothesize that by increasing the amount of potentially relevant information in context, SPLICE allows the model to learn longer and better context lookups. We further study this in Section 3.5, where we analyze SPLICE using the framework from (Chan et al. 2022). We present the main results in Tables 1, 2 and additional in Table 35 in Appendix O. Additionally in Appendix H we analyze results per-set of in-context examples and show that SPLICE achieves stochastic domination over Example Packing.

Question Answering and In-Context Retrieval In this section, we ask the following research question: Does fine-tuning on SPLiCe prepared data result in improved question-answering abilities? To answer the question, we utilize popular long context benchmarks such as Needle In A Haystack (Kamradt 2023) and lost-in-the-middle key-value retrieval (Liu et al. 2023), along with Qasper (Dasigi et al. 2021) from SCROLLS (Shaham et al. 2022), HotPotQA (Yang et al. 2018) passkey (Mohtashami and Jaggi 2023) and parts of RULER (Hsieh et al. 2024) tasks.

Table 1: We test the classification accuracy on TREC (Li and Roth 2002; Hovy et al. 2001). We average across 50 sets of in-context examples for 3B models, 10 for 7B models, and 5 for 13B models. $\Delta$ [ci] denotes the mean improvement and its $9 5 \%$ bootstrap confidence intervals (see Appendix H).   

<html><body><table><tr><td colspan="4">TREC</td></tr><tr><td>Model</td><td>Context</td><td>EP</td><td>SPLICE △[conf interv]</td></tr><tr><td>3BFoT</td><td>32K 16K</td><td>73.9 68.9</td><td>79.3 5.4 [4.7, 6.2] 76.9 8.0 [6.9,9.3]</td></tr><tr><td>7BFoT</td><td>32K 16K</td><td>75.6 74.0</td><td>79.4 3.8 [2.1, 5.1] 79.0 5.0 [3.4, 6.0]</td></tr><tr><td>7BcL</td><td>32K 16K</td><td>75.3 81.4</td><td>76.6 1.3 [0.8,1.8] 82.5 1.1 [0.2,1.6]</td></tr><tr><td>13BFoT</td><td>32K 16K</td><td>89.2 92.4 88.2 91.2</td><td>3.2 [2.6,3.8] 3.0 [1.9,3.5]</td></tr></table></body></html>

Table 2: We average results across 40 sets of in-context examples for 3B and 7B models and 5 for 13B. Due to the size of the DBpedia dataset, for each set of in-context examples, we sample a subset of 500 elements of the evaluation set.   

<html><body><table><tr><td colspan="5">DBpedia</td></tr><tr><td>Model</td><td>Context</td><td>EP</td><td>SPLICE</td><td>△[conf interv]</td></tr><tr><td rowspan="2">3BFoT</td><td>32K</td><td>82.9</td><td>85.9</td><td>3.0 2.6.3.4)</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">7BFoT</td><td>32K</td><td>82.9</td><td>84.9</td><td>2.0 [1.5,2.4]</td></tr><tr><td>16K</td><td>83.6</td><td>85.6</td><td>2.0 [1.5, 2.5]</td></tr><tr><td rowspan="2">7BCL</td><td>32K</td><td>95.1</td><td>95.6</td><td>0.5 [0.3, 0.6]</td></tr><tr><td>16K</td><td>96.2</td><td>96.4</td><td>0.2 [0.0, 0.3]</td></tr><tr><td rowspan="2">13BFoT</td><td>32K</td><td>95.6</td><td>96.0</td><td>0.4 [0.0, 0.8]</td></tr><tr><td>16K</td><td>95.8</td><td>96.8</td><td>1.0 [0.6, 1.5]</td></tr></table></body></html>

On Needle In A Haystack, we observe that the model finetuned on data prepared by SPLICE strongly outperforms the model fine-tuned on data prepared by Example Packing. To be more precise model fine-tuned with SPLICE can answer the question no matter the location of the relevant piece of information. Whereas the model trained with Example Packing only manages to answer correctly when the information is close to the question (see Figure 1). We also test our models on the lost-in-the-middle key-value retrieval task (Liu et al. 2023), and observe that SPLICE helps on hard-to-retrieve positions (see Figure 3). The main difference between those two tasks is that in the lost-in-the-middle key-value retrieval task, the input is highly structured (dictionary of random 128 bit UUIDs, see Appendix D for details) and the objective of the model is to retrieve the value assigned to a given key. On the other hand, in the Needle In A Haystack, a piece of information is placed inside a large coherent text, and the model is asked a question related to this information (see Appendix N for details).

Table 3: We evaluate our models on MMLU (5-shot), GSM8K (8-shot CoT). We provide an additional comparison with their starting checkpoint. For the 7B case, we additionally compare with a model tuned with $\mathrm { 2 k }$ context length on the same data. For each task, we highlight the best results up to 1 point. For 3B model results see Appendix I.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="3">MMLU</td><td rowspan="2">GSM8K</td></tr><tr><td>STEM</td><td>HUM</td><td>All</td></tr><tr><td>7B STCHKP</td><td>33.6</td><td>42.1</td><td>40.8</td><td>8.0</td></tr><tr><td>7B2KTUNED</td><td>33.7</td><td>40.8</td><td>39.4</td><td>8.4</td></tr><tr><td>7BFoT SPLICE</td><td>31.0</td><td>35.6</td><td>36.3</td><td>7.6</td></tr><tr><td>7BFoT EP</td><td>30.1</td><td>36.8</td><td>36.2</td><td>6.7</td></tr><tr><td>7BcL SPLICE</td><td>32.7</td><td>38.8</td><td>36.5</td><td>5.9</td></tr><tr><td>7BcL EP</td><td>32.7</td><td>37.5</td><td>36.1</td><td>6.3</td></tr><tr><td>13BSTCHKP</td><td>36.6</td><td>48.2</td><td>44.3</td><td>21.4</td></tr><tr><td>13BFoT SPLICE</td><td>38.3</td><td>48.6</td><td>44.4</td><td>23.1</td></tr><tr><td>13BFoT EP</td><td>39.0</td><td>47.5</td><td>44.7</td><td>21.9</td></tr></table></body></html>

We additionally evaluate our models on Qasper (Dasigi et al. 2021), HotPotQA (Yang et al. 2018) passkey retrieval (Mohtashami and Jaggi 2023) and RULER (Hsieh et al. 2024) and observe that SPLICE results in improvements over both the Example Packing and starting checkpoint. We present the results in Appendix K.

Short Context Evaluation One challenge in long-context fine-tuning is the degradation of short-context performance (Dubey, Jauhri, and et al. 2024). This can be overcomed by upsampling the short-context data and more gradual context extension (Dubey, Jauhri, and et al. 2024). We note that those approaches are compatible with SPLICE and instead focus on comparing SPLICE with Example Packing in a single-step context extension setup. We observe that SPLICE seems to be either better or on par with Example Packing (see Table 3). What is intriguing is that for the 13B parameter model, SPLICE even improves the short context performance on GSM8K (Cobbe et al. 2021) by $( + 1 . 7 )$ over the starting checkpoint. We hypothesize that GSM8K is a much more attention-demanding task than MMLU (Hendrycks et al. 2021), as it requires extracting relevant pieces of information, composing a chain of thought, and writing the final answer. Whereas MMLU is a wellestablished collection of tests spanning multiple domains. We hypothesize that the improvement does not occur in smaller models due to their low scores on GSM8K, as we get similar results when evaluating on code in Appendix L.

# 3.4 Detailed Study with Medium Models

In Table 4, Table 5, and Table 8, we present a comprehensive examination of the impact of document packing on longcontext performance using 270M parameter models, showing that SPLICE brings consistent improvement in long context language modeling. In Table 6, we scale context to 64K and observe even greater benefits over the Example Packing. We also expand our results to 131K and 160K context length in Tables 19 and 20 in Appendix. In Table 7, we show that SPLICE is quite robust to the non-accurate retriever. What is more results in Table 7 clearly show that SPLICE is an improvement over Within-Domain Example Packing. In particular, we note that the more noise we add, the closer SPLICE is to Within-Domain Example Packing (semantically), and that with $100 \%$ noise SPLICE turns into WithinDomain Example Packing (this is because we use SPLICE to prepare data coming from a single domain).

Training and Evaluation Initially, we train with the 2K context length on 6.3B tokens from RedPajama (TogetherComputer 2023). Subsequently, we fine-tune using 1B tokens with the context extended to 32K on a mixture of the original RedPajama data (TogetherComputer 2023) and long context data created using SPLICE/EP. We employ the Focused Transformer (FoT) (Tworkowski et al. 2023) for context extension (unless stated otherwise). We measure perplexity on held-out portions of the arXiv (Azerbayev, Piotrowski, and Avigad 2022) and StarCoder (Li et al. 2023b) datasets. The selection of these datasets is motivated by the fact that they can benefit from long-context information as demonstrated in (Chen et al. 2023; Li et al. 2023b). We provide additional details in Appendix M.

# 3.5 Properties of SPLICE Generated Data

SPLICE conjecturally falls into the framework presented in (Chan et al. 2022), which shows that the distributional properties of the training data affect the in-context capabilities of transformer models. In particular, it indicates the importance of ”burstiness”, i.e., a flatter frequency distribution with a relatively higher mass on the rare, long-tail tokens appearing in a sequence. In Table 9, we show that SPLICE increases the burstiness of the training data (measured in terms of Zipf’s coefficient of token frequency) in comparison to the Example Packing.

# 4 Related Work

There is an increasing number of works aiming to study the role of data in LLM training in detail. For instance, (Levine et al. 2022) developed a theory and demonstrated empirically that incorporating non-adjacent but semantically related sentences in training samples leads to better sentence embeddings and improves open-domain question-answering performance. Another study by (Gu et al. 2023) introduced a pretraining framework grounded on the idea that text documents often include intrinsic tasks. They showed that this approach substantially boosts in-context learning. Additionally, there is existing work on training long-context language models using repository-level code data, such as (Wu et al. 2022). Work of (Chan et al. 2022) identifies the training data’s distributional properties that affect transformer models’ in-context capabilities. Similarly, (Han et al. 2023) constructs small-scale data using an iterative gradient approach and shows that such data improve in-context performance.

Table 4: Perplexity with an improvement over EP highlighted in the subscript:(imp over EP). We fine-tune a 270M parameter model with 32K context on a 50/50 mixture of RedPajama (organized in a standard way) and long-context data C#, Python, Wikipedia, StackExchange prepared using a method of choice (SPLICE BM25, SPLICE CONT, SPLICE REPO, EP). EP denotes organizing long-context data in the same way as RedPajama. SPLICE beats the EP often by a large margin. The variants of SPLICE perform similarly, with SPLICE BM25 being slightly better. For detailed results, see Appendix B.3.   

<html><body><table><tr><td rowspan="2">Altered Data</td><td rowspan="2">Method</td><td rowspan="2">arXiv</td><td colspan="4">Code</td><td rowspan="2">Code & arXiv</td></tr><tr><td>Haskell</td><td>Python</td><td>CUDA</td><td>All</td></tr><tr><td rowspan="4">C#</td><td>SPLICEBM25</td><td>5.52 (.13)</td><td>3.33 (.25)</td><td>2.90 (.17)</td><td>2.46 (.19)</td><td>3.11 (.20)</td><td>3.26 (.20)</td></tr><tr><td>SPLICE CONT</td><td>5.53(.12)</td><td>3.35(.23)</td><td>2.91(.16)</td><td>2.48(.17)</td><td>3.12(.19)</td><td>3.27(.19)</td></tr><tr><td>SPLICEREPO</td><td>5.53 (.12)</td><td>3.35 (.23)</td><td>2.91 (.16)</td><td>2.49 (.16)</td><td>3.12 (.19)</td><td>3.27 (.19)</td></tr><tr><td>EP</td><td>5.65</td><td>3.58</td><td>3.07</td><td>2.65</td><td>3.31</td><td>3.46</td></tr><tr><td rowspan="4">Python</td><td>SPLICEBM25</td><td>5.47 (.10)</td><td>3.25 (.21)</td><td>2.53 (.09)</td><td>2.41 (.15)</td><td>3.02 (.15)</td><td>3.17 (.15)</td></tr><tr><td>SPLICE CONT</td><td>5.49 (.08)</td><td>3.28 (.18)</td><td>2.53 (.09)</td><td>2.43 (.13)</td><td>3.03 (.14)</td><td>3.19 (.13)</td></tr><tr><td>SPLICE REPO</td><td>5.48 (.09)</td><td>3.27 (.19)</td><td>2.54 (.08)</td><td>2.44 (.12)</td><td>3.03 (.14)</td><td>3.18 (.14)</td></tr><tr><td>EP</td><td>5.57</td><td>3.46</td><td>2.62</td><td>2.56</td><td>3.17</td><td>3.32</td></tr><tr><td rowspan="3">Wikipedia</td><td>SPLICEBM25</td><td>5.64 (.09)</td><td>3.82 (.15)</td><td>3.26 (.11)</td><td>2.87 (.13)</td><td>3.55 (.13)</td><td>3.68 (.13)</td></tr><tr><td>SPLICE CONT</td><td>5.65 (.08)</td><td>3.87 (.10)</td><td>3.30 (.07)</td><td>2.92 (.08)</td><td>3.59 (.09)</td><td>3.72 (.09)</td></tr><tr><td>EP</td><td>5.73</td><td>3.97</td><td>3.37</td><td>3.00</td><td>3.68</td><td>3.81</td></tr><tr><td rowspan="3">StackEX</td><td>SPLICE BM25</td><td>5.07 (.07)</td><td>3.88 (.06)</td><td>3.32 (.04)</td><td>2.89 (.05)</td><td>3.60 (.05)</td><td>3.69 (.05)</td></tr><tr><td>SPLICE CONT</td><td>5.09 (.05)</td><td>3.91 (.03)</td><td>3.35 (.01)</td><td>2.93 (.01)</td><td>3.63 (.02)</td><td>3.73 (.01)</td></tr><tr><td>EP</td><td>5.14</td><td>3.94</td><td>3.36</td><td>2.94</td><td>3.65</td><td>3.74</td></tr></table></body></html>

Table 5: Perplexity fine-tune on a $5 0 / 5 0$ data mixture of RedPajama and C code. We report the mean and standard deviation Interestingly, training on the code data with SPLICE improves general long-context performance on arXiv.   

<html><body><table><tr><td rowspan="2">Long Context Data</td><td rowspan="2">Method</td><td rowspan="2">arXiv</td><td colspan="2">Code</td><td rowspan="2">Code & arXiv</td></tr><tr><td>Python</td><td>All</td></tr><tr><td rowspan="4">C</td><td>SPLICEBM25</td><td>5.463 ± .002</td><td>2.810 ± .002</td><td>2.942 ± .005</td><td>3.100 ± .004</td></tr><tr><td>SPLICE CONT</td><td>5.477 ± .005</td><td>2.824 ±.001</td><td>2.957 ± .006</td><td>3.115 ± .006</td></tr><tr><td>SPLICEREPO</td><td>5.474 ± .007</td><td>2.827 ± .006</td><td>2.958 ± .009</td><td>3.115 ± .009</td></tr><tr><td>EP</td><td>5.550 ± .002</td><td>2.931 ± .008</td><td>3.073 ± .006</td><td>3.228 ± .005</td></tr></table></body></html>

Table 6: Perplexity (imp over EP) for training on a $5 0 / 5 0$ data mixture of RedPajama and $C \#$ code with longer 64K context.   

<html><body><table><tr><td rowspan="2">Altered Data</td><td rowspan="2">Method</td><td rowspan="2">arXiv</td><td colspan="4">Code</td><td rowspan="2">Code & arXiv</td></tr><tr><td>Haskell</td><td>Python</td><td>CUDA</td><td>All</td></tr><tr><td rowspan="4">C#</td><td>SPLICEBM25</td><td>4.86 (.15)</td><td>2.60 (.19)</td><td>2.66 (.16)</td><td>2.32 (.19)</td><td>2.75 (.19)</td><td>2.88 (.19)</td></tr><tr><td>SPLICE CONT</td><td>4.88 (.13)</td><td>2.62 (.17)</td><td>2.67 (.15)</td><td>2.34 (.17)</td><td>2.77 (.17)</td><td>2.90 (.17)</td></tr><tr><td>SPLICE REPO</td><td>4.88 (.13)</td><td>2.62 (.17)</td><td>2.68 (.14)</td><td>2.35 (.16)</td><td>2.77 (.17)</td><td>2.90 (.17)</td></tr><tr><td>EP</td><td>5.01</td><td>2.79</td><td>2.82</td><td>2.51</td><td>2.94</td><td>3.07</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2" colspan="2">Method EvalData/Noise</td><td colspan="6">SPLICE</td><td rowspan="2">EP</td></tr><tr><td>0%</td><td>10%</td><td>25%</td><td>50%</td><td>75%</td><td>90%</td></tr><tr><td colspan="2">arXiv</td><td>5.46</td><td>5.47</td><td>5.48</td><td>5.50</td><td>5.53</td><td>5.55</td><td>5.55</td></tr><tr><td rowspan="2">Code</td><td>Python</td><td>2.81</td><td>2.82</td><td>2.83</td><td>2.86</td><td>2.89</td><td>2.92</td><td>2.93</td></tr><tr><td>All</td><td>2.94</td><td>2.95</td><td>2.97</td><td>3.01</td><td>3.04</td><td>3.06</td><td>3.07</td></tr><tr><td colspan="2">Code&arXiv</td><td>3.10</td><td>3.11</td><td>3.13</td><td>3.16</td><td>3.19</td><td>3.22</td><td>3.23</td></tr></table></body></html>

Table 7: We test the robustness of SPLICE to noisy retriever. We achieve this by preparing data using BM25 retriever that with probability $p$ returns a random document instead of the most related one. We note that SPLICE is quite robust and only with $p = 0 . 9$ approaches Example Packing.

Table 8: We note that SPLICE beats the EP perplexity when trained with various proportions of SPLICE BM25/EP prepared C data (the remaining data is unaltered RedPajama).   

<html><body><table><tr><td rowspan="2">Altered Data</td><td rowspan="2">Method</td><td rowspan="2">arXiv</td><td colspan="2">Code</td><td rowspan="2">Code arXiv</td></tr><tr><td>Python</td><td>All</td></tr><tr><td>25% 25%</td><td>SPLICE EP</td><td>5.43 5.49</td><td>2.91</td><td>3.08</td><td>3.22 3.34</td></tr><tr><td>50%</td><td>SPLICE</td><td>5.46</td><td>3.00 2.81</td><td>3.19 2.94</td><td>3.10</td></tr><tr><td>50%</td><td>EP</td><td>5.55</td><td>2.93</td><td>3.07</td><td>3.23</td></tr><tr><td>75% 75%</td><td>SPLICE EP</td><td>5.61 5.73</td><td>2.80 2.94</td><td>2.86 3.04</td><td>3.05 3.23</td></tr></table></body></html>

Table 9: Zipf’s coefficient of token frequency on EP and SPLICE along with standard deviation. A lower Zipf’s coefficient represents a more significant burstiness property.   

<html><body><table><tr><td>Training Data</td><td>Method</td><td>Zipf's Coefficient</td></tr><tr><td>C</td><td>SPLICEBM25</td><td>1.512(0.055)</td></tr><tr><td rowspan="3">StackEx</td><td>EP</td><td>1.593(0.025)</td></tr><tr><td>SPLICEBM25</td><td>1.643(0.026)</td></tr><tr><td>EP</td><td>1.664(0.013)</td></tr></table></body></html>

Our methodology diverges from these works in several key ways. First, while prior studies have focused on sentence-level (Levine et al. 2022) or paragraph-level (Gu et al. 2023) granularity, we emphasize document-level context during training, specifically targeting long-context performance. We validate our approach in large-scale language modeling, using models such as OpenLLaMA 3B, 7B, and CodeLlama 13B. Second, we construct a tree structure of related documents using BM25/Contriever-MSMARCO retrieval, which we then linearize to form long-context samples. This approach allows for greater control over the coherence of samples, compared to relying solely on natural data structures like repository-level code. While the gradientbased method in (Han et al. 2023) shares similarities with our retrieval-based approach, our method scales to larger datasets and operates at a different granularity.

Concurrently with our research, (Shi et al. 2024) introduced a method for preparing training data that shares similarities with SPLICE, particularly in its default settings $\boldsymbol { \mathcal { k } } = 1$ with identity as Order). However, while their approach focuses on training models from scratch, our work demonstrates that long-context capabilities can be effectively achieved through short and cost-efficient fine-tuning. In addition to this distinction, we employ significantly longer context lengths, extending above 64K tokens compared to the 8K tokens used in (Shi et al. 2024), which allows for more comprehensive context handling. Furthermore, we provide an in-depth analysis of our design choices, such as the advantages of data reordering (detailed in Appendix J) and the impact of varying $k$ values (Appendix C.1). These analyses underline the effectiveness and flexibility of our approach. Our findings are especially pertinent in the context of recent research on the ”Physics of Language Models” (Allen-Zhu and Li 2024), which discusses the limitations of fine-tuning. Despite these limitations, we show that SPLICE offers substantial and quantifiable improvements even with relatively brief fine-tuning, providing a practical advantage in enhancing long-context capabilities.

# 5 Limitations and Future Work

We show that structuring the training data is a viable way of improving the model’s long-context performance. The presented method, SPLICE, can be viewed as a general framework for organizing the documents into training samples. This opens multiple further research avenues.

Retrieval Granularity Another avenue for future work is to study the granularity of the pieces from which the training samples are constructed. In this work, we focus on the document-level granularity. However, it is possible to construct training samples from smaller pieces.

Other Data Sources One of the approaches to training long-context language models is to use conversational data (Li et al. 2023a), which is complementary to our method. SPLICE can utilize data that already exists in vast quantities and can be easily applied to different types of text (like code, Wikipedia articles, or StackExchange) to further increase the number of long-context samples. We leave researching how SPLICE integrates with other methods for preparing the long-context data as future work.

Data Curation Using highly correlated samples has the potential to result in training instability. However, we noted no performance degradation during our experiments. We leave the study of how SPLICE integrates with different data types for the future. In particular, in our studies, the datasets used were reasonably deduplicated.

Neural Retriever In our work, we have utilized Contriever (Izacard et al. 2022) in a zero-shot setup, using the first 512 tokens to generate the embedding. On the other hand, BM25 had access to all the document content. Further study is required to determine whether SPLICE can additionally significantly benefit from properly tuned neural retrievers. In particular, in our case, Contriever tended to produce samples consisting of fewer repositories than BM25. We leave this for future work.

# 6 Conclusions

In this work, we present SPLICE, a method of constructing training samples for long-context language models. It utilizes BM25/Contriever-MSMARCO to find relevant documents and feed them to the model in a structured manner. We show that SPLICE improves performance on downstream tasks and the language modeling abilities of LLMs. We further show that SPLICE can be used to improve longcontext utilization of large-scale models using only short fine-tuning. We believe that our work indicates multiple interesting research directions for improving the performance of long-context language models with structured data.

# Ethical Statement

Our work develops a generic technique that allows for improving context utilization in language models via low-cost fine-tuning. However, we note that it does not create any new threats, but only exacerbates existing ones. Therefore, we refer to the existing literature on the broader impact of language models, such as (Borgeaud et al. 2022).