# Is There No Such Thing as a Bad Question? H4R: HalluciBot for Ratiocination, Rewriting, Ranking, and Routing

William Watson\*, Nicole Cho\*, Nishan Srishankar

J.P. Morgan AI Research nicole.cho@jpmorgan.com

# Abstract

Hallucination continues to be one of the most critical challenges in the institutional adoption journey of Large Language Models (LLMs). While prior studies have primarily focused on the post-generation analysis and refinement of outputs, this paper centers on the effectiveness of queries in eliciting accurate responses from LLMs. We present HalluciBot, a model that estimates the query’s propensity to hallucinate before generation, without invoking any LLMs during inference. HalluciBot can serve as a proxy reward model for query rewriting, offering a general framework to estimate query quality based on accuracy and consensus. In essence, HalluciBot investigates how poorly constructed queries can lead to erroneous outputs - moreover, by employing query rewriting guided by HalluciBot’s empirical estimates, we demonstrate that $9 5 . 7 \%$ output accuracy can be achieved for multiple choice questions. The training procedure for HalluciBot consists of perturbing 369,837 queries $n$ times, employing $n + 1$ independent LLM agents, sampling an output from each query, conducting a Multi-Agent Monte Carlo simulation on the sampled outputs, and training an encoder classifier. The idea of perturbation is the outcome of our ablation studies that measures the increase in output diversity $\mathrm { + 1 2 . 5 }$ agreement spread) by perturbing a query in lexically different but semantically similar ways. Therefore, HalluciBot paves the way to ratiocinate $( 7 6 . 0 \%$ test F1 score, $4 6 . 6 \%$ in saved computation on hallucinatory queries), rewrite $( + 3 0 . 2 \%$ positive class transition from hallucinatory to non-hallucinatory), rank $( + 5 0 . 6 \%$ positive class transition from hallucinatory to non-hallucinatory), and route queries to effective pipelines.

WithoutHalluciBot WithHalluciBot(HB)   
Direct Retrieve Rewrite Ratiocinate Rewrite Rank Route Input Input LLM HB HB L Rank2 HalluciBot Rewrite X Rewrite T H Rank 4 ABS区区 √ X M B Rank 5 EXT区 RAG Retrieve + ↓ RAG RAG Rewrite Retrieve Retrieve Retrieve Context Context Context Context LLM Black-box LLM Respond Respond + Output Output

Extended version — https://arxiv.org/abs/2404.12535

# 1 Introduction

Despite the promising potential for a myriad of use cases, Large Language Models (LLMs) offer limited insights into their chain of thought (Liang et al. 2022; Wei et al. 2023; Kojima et al. 2023) and have the propensity to hallucinate in various circumstances (Jiang et al. 2021). Common factors that drive hallucinations encompass high model complexity, flawed data sources, or inherent sampling randomness.

Specifically, the intrinsic trade-off between greedy deterministic decoding and the creativity spawned through nucleus sampling induces a heightened propensity to hallucinate (Huang et al. 2023) - LLMs frequently advance output quality through different sampling methods (Holtzman et al. 2020, 2018; Radford et al. 2018). The challenge of understanding hallucinations is compounded by limitations such as the frequent inaccessibility into the LLMs’ training datasets (Liang et al. 2022). HuggingFace’s release of its “Hallucinations Leaderboard” on January 29th, 2024 (Gao et al. 2023) highlights the importance of resolving hallucination-related issues via the concerted effort of evaluating different LLMs. In this context, the majority of current studies have focused on the post-generation phase of output analysis as expanded in Peng et al. (2023) such as - (1) self-refinement via feedback loops on the model’s output (Madaan et al. 2023; Watson et al. 2025), (2) analysis of logit output values to detect hallucination (Varshney et al. 2023), or (3) for a minority of studies focused on the pre-generation phase, the ingestion of recent knowledge to improve performance (Tonmoy et al. 2024). We propose a novel model, HalluciBot, that predicts the probability of hallucination, before any generation, for a given query. In essence, this paper refocuses the study of hallucination to an empirical evaluation of the input query - how much does the query’s quality influence the model’s propensity to hallucinate? Therefore, HalluciBot estimates, ▶ a binary classification of the query’s propensity to hallucinate (“Yes” or “No”), as well as, ▶ a non reinforcement-learning method to guide query rewriting, enabling the construction of this encoder to be agnostic to closed-source or open-source LLMs (Ma et al. 2023).

We train HalluciBot as a binary classifier to predict whether a query will lead to erroneous outputs. To generate ground truth labels, we use a Multi-Agent Monte Carlo simulation that perturbs the query and checks for inaccuracies. If any perturbed version causes an error, the original query is labeled as hallucinatory. In this paper, HalluciBot leverages gpt-3.5-turbo, trained via (1) perturbing 369,837 queries $n$ times to retain the original semantic meaning yet diverge lexically, (2) employing $n + 1$ independent agents to sample an output from each perturbation including the original query, at a temperature of 1.0 for diversity, (3) conducting a Monte Carlo simulation on 2,219,022 sampled outputs, and (4) deriving an empirical estimate into the expected rate of hallucination $p _ { h } ( q _ { 0 } )$ for the original query. We prove that introducing perturbations before sampling $n + 1$ outputs for query $q _ { 0 }$ garners a 13.2 point spread in the lower and upper bound accuracy, with a 12.5 point decrease in Fleiss’s $\kappa$ for agreement, even as the modal accuracy remains largely unchanged (1.3 point difference). In other words, perturbations introduce more variability in the outputs, while preserving the central tendency. As HalluciBot generates the probability of hallucination in the pre-generation phase, the estimates can be used in a myriad of downstream modalities (Figure 1) such as: “Ratiocinate” to purely estimate the query’s quality; “Rewrite” to leverage the probabilities and improve the query’s quality via iterative feedback; “Rank” to rank perturbations, using probabilities as a proxy reward model in Best-of-N sampling; “Route” to route the best next steps, depending on scenarios such as Extractive or Abstractive. By cross-tabulating the predicted hallucination rates across scenarios, HalluciBot can act as a router, through which certain queries can be guided to a black-box LLM, while others will require a more complex pipeline including context retrieval, web search, or agents (Watson et al. 2023; Zeng et al. 2024; Cho et al. 2024).

Contributions. As a result, our study has culminated in the following pillars of contribution. (1) HalluciBot is the first encoder-based model to derive, before generation, an anticipated rate of hallucination for any type of query, achieving a validation accuracy of $7 3 . 6 \%$ $8 0 . 2 \%$ F1) and a testing accuracy of $6 9 . 5 \%$ $( 7 6 . 0 \%$ F1). (2) our approach to construct HalluciBot absorbs the computational complexity of Monte

Inference qo HB Estimates Hallucination Before Generation \~ ph(qo) A HalluciBot Training Methodology i A LLM ao Empirical! q1 →LLM a Estimate   
MonteCarlo Simulation q0LLM ！ q2 LLM a2 a '\~ph(qo) \~n+1∑1（a+>) A qn-T →LLMn-1 anQuery qn LLM ay Perturbator Multi-Agent Simulation

Carlo sampling, exploration, and training prior to the user session. Thus, institutions that employ HalluciBot can systematically save on the considerable amount of computational waste engendered by “highly probable” hallucinatory queries $( 4 6 . 6 \%$ in saved computation during testing). (3) the hallucination probability can be leveraged as a proxy reward model in a myriad of different infrastructures; HalluciBot paves the way to rewrite $( + 3 1 . 9 \%$ positive class transition from hallucinatory to non-hallucinatory), rank $( + 5 1 . 4 \%$ positive class transition from hallucinatory to non-hallucinatory), and route $( + 6 0 . 0 \%$ diverted) queries. (4) HalluciBot generalizes to systems with RAG or few-shot question answering systems with an LLM generator by differentiating the scenario in its prompt. Also, it can generalize to closed systems only accessible via API calls (OpenAI 2022; Google 2023). (5) HalluciBot’s training methodology can be leveraged for any model or training corpus; it can be leveraged as a general means by which the research community can develop an encoder to assess query quality.

# 2 Related Work

With regards to hallucination mitigation studies, an overwhelming majority focuses on the post-generation stage of analyzing outputs. A minority concentrates on the pregeneration phase and even amongst those, the focus lies in incorporating recent knowledge into LLMs. In detail, many expand on the universally utilized method of context-based retrieval systems (Lewis et al. 2020). Other methods include relying on the model’s general knowledge (Khashabi et al. 2020). Certain work has focused on mitigating hallucinations by augmenting the way LLMs generate their answers. One of the more popular techniques is to have the model enumerate its chain-of-thought (Wei et al. 2023) while building context. Another method to augment generation with context is by semantic retrieval (Lewis et al. 2020; Liu et al. 2021), handling hallucinations as they arise (Varshney et al. 2023). PromptChainer (Wu et al. 2022) profiled techniques to craft LLM chains, in which the output of one LLM’s generation process, when fed into the next LLM, can allow for more complex tasks. Language Model Cascades (Dohan et al. 2022) demonstrated that LLMs can yield probabilistic programs to tackle multi-step reasoning problems. Self-consistency (Wang et al. 2023) leveraged a new decoding strategy to sample multiple generative pathways - then select the most consistent answer. Most recent work has focused on samplingbased calibration within a single model (Cole et al. 2023) or self-verification (Kadavath et al. 2022) - the latter focuses on generating a set of outputs and feeding those back into the LLM. Furthermore, Snyder, Moisescu, and Zafar (2023) explores how artifacts can differentiate hallucinated outputs. One common feature amongst these approaches is that the focus is on the output rather than the query. Alzahrani et al. (2024) explored how LLMs are highly sensitive to minute perturbations, such as changing the order of answer choices. Also, while Zheng and Saparov (2023) study lexical perturbations, no study on hallucinations employs a Multi-Agent approach coupled with query perturbations - which are hallmark features of HalluciBot.

Table 1: Dataset scenario split with Reused Assets.   

<html><body><table><tr><td>Scenario</td><td>Datasets</td></tr><tr><td>Extractive</td><td>SQuADv2</td></tr><tr><td>Multiple Choice</td><td>TruthfulQA,ciQ,LU,PIQA,ol, OpenBookQA,MathQA,ARC-E/C</td></tr><tr><td>Abstractive</td><td>SQuADv2,TruthfulQA,SciQ, WikiQA,HotpotQA,TriviaQA</td></tr></table></body></html>

# 3 Methodology Overview

What is Hallucination? In general terms, hallucination refers to a false perception of patterns or objects resulting from one’s senses. With regards to LLMs, a myriad of studies bifurcate into (1) factuality hallucinations that refer to outputs which directly contradict or fabricate the ground truth while (2) faithfulness hallucinations define outputs that misunderstand the context or intent of the query (Huang et al. 2023; Ji et al. 2023). In this study, we introduce truthful hallucination as the motivation on why we are perturbing the original query. Truthful hallucination is defined as an LLM’s inability to answer semantically similar but lexically different perturbations of a query. The motivation for truthful hallucination stems from the analysis that neural networks display an intrinsic propensity to memorize training data (Carlini et al. 2021) - in this case, memorizing the query and output. Given the risk of over-training LLMs, their opaque training data, and propensity to memorize - generating multiple outputs from the same query or analyzing a single output from a single query do not help measure truthful hallucination.

What is the Motivation for HalluciBot? HalluciBot focuses on distilling LLM behavior into a speedy encoder that can predict hallucination before generation. Foremost, this is in contrast to prior work that uses multiple generations during a user’s session to provide self-consistency (Manakul, Liusie, and Gales 2023). Next, our proposal differs from entropy based, log-prob based, or model based estimation techniques (Huang et al. 2023) that rely on the LLM’s uncertainty to predict hallucinations - these methods focus on the model’s bias while we focus on empirical estimates. Moreover, our approach consists of a Multi-Agent simulation which stands in stark contrast to the majority of current experiments that have focused on leveraging a single LLM agent to generate outputs from a single query (Cole et al. 2023; Kadavath et al. 2022; Snyder, Moisescu, and Zafar 2023). The training procedure for HalluciBot consists of perturbing each query $n = 5$ times, employing $n + 1 = 6$ independent LLM agents, sampling an output from each query, conducting a Monte Carlo simulation on 2,219,022 sampled outputs, and training an encoder classifier.

# 3.1 Multi-Agent Monte Carlo Simulation

What is the Purpose of a Monte Carlo Simulation? As evidenced by multiple studies and Table 3, hallucination is the outcome of multiple confounding variables - thus, it is highly unlikely that a tractable closed-form solution will be able to model hallucinations. Thus, we employ a Monte Carlo simulation as a means to derive empirical estimations of hallucination rates in LLMs, since this method is frequently leveraged to map probability in the presence of random variable inference (Swaminathan 2021). Thus, we estimate the probability density that a query induces hallucination.

What is a Query Perturbator? Via perturbations, we induce diversity to disentangle the generation process from any potential training bias (Alzahrani et al. 2024; Carlini et al. 2021). The Query Perturbator is a $\mathtt { g p t } - 3 . 5 \mathtt { - t u r b o }$ LLM agent that generates $n = 5$ perturbations to the original query $q _ { 0 }$ while retaining the same semantic meaning. In effect, the generation process can be summarized as returning a set of $\mathcal { Q } \ \stackrel { \_ } { = } \ \{ q _ { 0 } , q _ { 1 } , . . . , q _ { n } \}$ query perturbations of size $n + 1$ . The Query Perturbator’s singular purpose is to: Rewrite the query in $\{ n \}$ radically different ways. One prompt call is sufficient to discourage duplicates. Temperature is set to 1.0 to prioritize creativity and lexical diversity. Our analysis in Table 3 shows that introducing perturbations, rather than sampling $n + 1$ outputs for query $q _ { 0 }$ , results in a 13 point spread between the lower and upper bound accuracy, a 12.5 point decrease in Fleiss’s $\kappa$ for agreement, while the modal accuracy remains largely unchanged. This suggests that perturbations inject variability into our Monte Carlo simulation, which is critical for observing diverse outputs and hallucinations. This corroborates the observation by Alzahrani et al. (2024) that LLMs are highly sensitive to even minor details.

What is an Output Generator? For the perturbed set $\mathcal { Q }$ for a sample $q _ { 0 }$ , the Output Generator consists of $| { \mathcal { Q } } | =$ $n + 1$ six independent gpt-3.5-turbo LLM agents to generate outputs $a _ { i } \in { \mathcal { A } }$ for each variation $q _ { i } \in \mathcal { Q }$ . The LLM agent will receive (1) for Extractive queries, a prompt with the query $q _ { i }$ , alongside context $c _ { i }$ , (2) for Multiple-Choice queries, candidate choices $k _ { i } \in \mathcal { K }$ , and (3) for Abstractive queries, no additional context. Temperature for all experiments is set to 1.0 to stress-test and encourage diversity.

How Do We Measure Accuracy? Accuracy serves as the measure of correctness, comparing the generated output $a _ { i }$ to the ground truth $y$ , using partial, case-insensitive matching with the TheFuzz library. For Multiple Choice queries, the choice label is also considered. If there is no match between the output $a _ { i }$ and the ground truth $y$ , we assign $\mathbb { I } [ a _ { i } \neq y ] \mapsto$ 1; otherwise, $\mathbb { I } \big [ a _ { i } = y \big ] \mapsto 0$ . The results are compared to the baseline (original query $q _ { 0 }$ , output $a _ { 0 }$ ), the mode (most common $a _ { i }$ ), the lower bound (all correct), and the upper bound (at least one $a _ { i }$ correct).

![](images/67067c55a0145ce451c0d534d36b85fe281a34ce9fa45e305eb424b7df715b7b.jpg)  
Figure 3: Distribution of the observed number of hallucinations per scenario. For Extractive, additional context mitigates hallucinations. For Multiple Choice, distractors can cause confusion amongst agents uniformly. For Abstractive, the absence of additional information results in extreme disparities in correctness, with most simulations showing no or all hallucinations.

![](images/4db8c48ee57ae72cb4f13adc24ad4bd86ccc5aad7fbf2a76650501118449a69e.jpg)  
Figure 4: Binary distribution of labels indicating whether at least one hallucination occurred during simulation.

How Do We Measure Agreement? Accuracy alone is insufficient for evaluating the agreement among multiple agents. To assess agreement, we report statistical measures for our Monte Carlo experiments including Item Difficulty $( \mu _ { \mathbf { D } } )$ , Fleiss’s Generalized $\kappa$ , Mean Certainty / Entropy $( \mathbf { H } _ { \eta } )$ , and Gibbs’ $\mathbf { M _ { 2 } }$ Index. These metrics help evaluate agreement levels amongst independent agents. For instance, high agreement on an incorrect answer indicates a misconception, while low agreement could suggest confusion or a poorly formulated query. To address this limitation in HalluciBot, we introduce a dual cross-entropy loss based on hallucination rates and consensus to improve the model’s ability to distinguish good queries from bad queries.

# 3.2 Converting Monte Carlo Estimates To Labels

Empirical Estimate. The probability of hallucination for a query $q _ { 0 }$ , denoted as $p _ { h } ( q _ { 0 } )$ , can be empirically estimated based on the output $a _ { i } \in { \mathcal { A } }$ of our Multi-Agent Monte Carlo simulation. We define the indicator function $\mathbb { I }$ to measure the incorrectness of an output $a _ { i }$ with respect to the ground truth $y$ for query $q _ { 0 }$ .

Table 2: Training splits for HalluciBot.   

<html><body><table><tr><td>Binary</td><td>Train</td><td>Val</td><td>Test</td></tr><tr><td>No (y = 0) Yes (y = 1)</td><td>139,142 163,350</td><td>17,153 27,338</td><td>9.306 13,548</td></tr><tr><td>Observed Rate 0.0% (y=0/6)</td><td>Train 139,123</td><td>Val 17,146</td><td>Test 9,202 2,757</td></tr><tr><td>16.7% (y = 1/6) 33.3% (y= 2/6) 50.0% (y=3/6) 66.7% (y=4/6) 83.3% (y= 5/6)</td><td>35,114 20,213 15,749 14,477 17,123</td><td>4,974 3,371 2,757 2,735 3,242</td><td>1,967 1,768 1,970 2,171</td></tr><tr><td>100.0% (y=6/6) Scenario</td><td>60,693 Train</td><td>10,266</td><td>3,019</td></tr><tr><td>Extractive</td><td>80,049</td><td>Val 5,843</td><td>Test =</td></tr><tr><td>Multiple Choice</td><td>45,997</td><td>14,127</td><td>21,573</td></tr><tr><td>Abstractive Total</td><td>176,446 302,492</td><td>24,521 44,491</td><td>1,281 22,854</td></tr></table></body></html>

$$
p _ { h } \left( q _ { 0 } \right) \approx \frac { 1 } { n + 1 } \sum \mathbb { I } \left[ a _ { i } \neq y \right]
$$

Binary Hallucination & Consensus Labels. To assess the propensity to hallucinate, we simplify the problem by considering two response values: whether q0 produces any hallucination or not. Thus, we define the binary values for the probability of any hallucination as $p _ { b } ( q _ { 0 } )$ . Furthermore, we craft a secondary consensus label $p _ { c } ( q _ { 0 } )$ that is a proxy for the agreement of the query. It maps the set of unique answers to 1 if there is any disagreement, otherwise we assign 0 for perfect agreement (1 unique answer). Therefore, we can train a 2 head output Consensus model to predict the hallucination probability $p _ { b } ( q _ { 0 } )$ , and if the query will cause confusion or consensus $p _ { c } ( q _ { 0 } )$ .

$$
\begin{array} { r l } & { p _ { b } \bigl ( q _ { 0 } \bigr ) = \left\{ \begin{array} { l l } { 1 } & { \mathrm { i f } p _ { h } \bigl ( q _ { 0 } \bigr ) > 0 } \\ { 0 } & { \mathrm { i f } p _ { h } \bigl ( q _ { 0 } \bigr ) = 0 } \end{array} \right. } \\ & { p _ { c } \bigl ( q _ { 0 } \bigr ) = \left\{ \begin{array} { l l } { 1 } & { \mathrm { i f } \bigl | \{ a _ { i } | a _ { i } \in \mathcal { A } \} \bigr | > 1 } \\ { 0 } & { \mathrm { i f } \bigl | \{ a _ { i } | a _ { i } \in \mathcal { A } \} \bigr | = 1 } \end{array} \right. } \end{array}
$$

Table 3: Comparing Single Query, Multiple Outputs (SINGLE) vs. Single Query, Multiple Perturbations, Single Output (MULTI) Monte Carlo Experiments (§5). The reported metrics $( \ S 3 . 1 )$ are calculated across all examples, regardless of the original dataset split. For the majority of scenarios, the SINGLE strategy outperforms the the MULTI approach in eliciting correct answers. Therefore, the SINGLE approach demonstrates higher agreement and tighter accuracy bounds, while the MULTI approach introduces more diverse responses and hallucinations with negligible impact on modal accuracy, allowing our simulation to generate more useful labels regarding query quality compared with a SINGLE approach.   

<html><body><table><tr><td colspan="3">Scenario</td><td colspan="4">Accuracy</td><td colspan="4">Agreement</td></tr><tr><td colspan="3">Experiment</td><td># Base↑</td><td>Mode ↑</td><td>Lower个</td><td>Upper↑</td><td>μD↑</td><td>Hn↑</td><td>M2 ↑</td><td>K↑</td></tr><tr><td rowspan="4"></td><td>Extractive</td><td>85,734</td><td>89.8</td><td>90.3</td><td>83.6</td><td>94.6</td><td>89.8</td><td>91.4</td><td>92.0</td><td>90.4</td></tr><tr><td>Multiple Choice</td><td>80,813</td><td>74.0</td><td>75.8</td><td>58.1</td><td>88.0</td><td>73.8</td><td>90.3</td><td>83.7</td><td>75.5</td></tr><tr><td>Abstractive</td><td>200,693</td><td>56.2</td><td>56.7</td><td>44.2</td><td>67.4</td><td>56.1</td><td>93.2</td><td>89.8</td><td>80.2</td></tr><tr><td>Total</td><td>367,240</td><td>68.0</td><td>68.7</td><td>56.4</td><td>78.3</td><td>67.9</td><td>94.4</td><td>90.2</td><td>81.5</td></tr><tr><td rowspan="4"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mutipti Choice</td><td>85.697</td><td>92.1</td><td>918</td><td>49.4</td><td>97.4</td><td>87.8</td><td>85.2</td><td>84.3</td><td>75.9</td></tr><tr><td>Abstractive</td><td>202,248</td><td>55.9</td><td>53.9</td><td>32.9</td><td>67.3</td><td>51.2</td><td>81.5</td><td>80.0</td><td>69.1</td></tr><tr><td>Total</td><td>369,837</td><td>68.6</td><td>67.4</td><td>44.3</td><td>79.4</td><td>63.9</td><td>81.0</td><td>79.1</td><td>69.0</td></tr></table></body></html>

<html><body><table><tr><td></td><td colspan="3">Accuracy↑</td><td colspan="3">F1 Score ↑</td><td colspan="3">Precision个</td><td colspan="3">Recall 个</td></tr><tr><td>Model</td><td>Train</td><td>Val</td><td>Test</td><td>Train</td><td>Val</td><td>Test</td><td>Train</td><td>Val</td><td>Test</td><td>Train</td><td>Val</td><td>Test</td></tr><tr><td>RoBERTa-base</td><td>74.7</td><td>64.1</td><td>66.1</td><td>73.3</td><td>66.5</td><td>69.6</td><td>85.1</td><td>78.0</td><td>74.4</td><td>64.4</td><td>57.9</td><td>65.3</td></tr><tr><td>+ Scenario</td><td>79.8</td><td>73.0</td><td>69.0</td><td>79.3</td><td>76.8</td><td>71.7</td><td>88.8</td><td>81.5</td><td>78.4</td><td>71.5</td><td>72.6</td><td>66.0</td></tr><tr><td>+ Consensus</td><td>79.3</td><td>73.0</td><td>68.7</td><td>79.1</td><td>77.0</td><td>71.5</td><td>87.2</td><td>81.0</td><td>77.7</td><td>71.4</td><td>73.3</td><td>66.2</td></tr><tr><td>+ Calibration</td><td>80.3</td><td>73.6</td><td>69.5</td><td>81.4</td><td>78.8</td><td>73.6</td><td>83.6</td><td>78.4</td><td>75.6</td><td>79.2</td><td>79.2</td><td>71.7</td></tr><tr><td>+T=0.341</td><td>80.4</td><td>73.6</td><td>69.5</td><td>81.6</td><td>80.2</td><td>76.0</td><td>74.7</td><td>72.9</td><td>70.3</td><td>90.0</td><td>89.0</td><td>82.6</td></tr><tr><td>RoBERTa-large</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>+ Calibration</td><td>84.7</td><td>73.5</td><td>69.2</td><td>85.5</td><td>78.5</td><td>73.0</td><td>88.1</td><td>78.9</td><td>76.1</td><td>83.1</td><td>78.2</td><td>70.1</td></tr><tr><td>+ T=0.326</td><td>84.8</td><td>73.6</td><td>69.4</td><td>83.5</td><td>80.0</td><td>75.6</td><td>75.0</td><td>71.8</td><td>70.5</td><td>94.2</td><td>90.4</td><td>81.6</td></tr></table></body></html>

Table 4: HalluciBot Binary Evaluation Statistics. We report the Accuracy, F1, Precision, and Recall for all data splits. Probability threshold $\tau$ is computed along the closed interval [0, 1] in increments of 0.001 to maximize the validation F1 score for the final model. The best ablation per base model is underlined, while the overall best performing model is in bold.

# 3.3 How To Train a Classifier?

Once the Monte Carlo simulation is complete for our training corpus composed of 369,837 queries spanning 13 different datasets (Table 1), we start training our classifier. These queries encompass Extractive, Multiple Choice, and Abstractive scenarios. Each scenario, with or without additional context, affects the hallucination rate of $9 \mathrm { p t } - 3 . 5 -$ turbo. These simulated estimates are directly proportional to the approximated rates of hallucination $p _ { h }$ .

$\blacktriangleright$ With a synthetic labeled set of queries $q _ { 0 }$ and their rate of hallucinations $p _ { h } ( q _ { 0 } )$ , we train an encoder-style RoBERTa (Liu et al. 2019) classifier to estimate the hallucination probability density from our Monte Carlo simulation. ▶ We ablate two versions: a binary model to estimate the propensity a query can hallucinate, and a consensus-aware model to also predict the expected agreement of outputs if sampled $n + 1$ times.

Our experiments constrain the number of perturbations to $n = 5$ , and when including the original query and output, we can model the hallucination rate for $n + 1 = 6$ modes. This translates to increments of $1 6 . { \overline { { 6 } } } \%$ in hallucination rates.

How To Encode a Query’s Scenario? We conduct an ablation study to explore if incorporating the query’s scenario mitigates hallucinations. To create the prompt, we prepend the original query $q _ { 0 }$ with either [EXTRACTIVE], [MULTIPLE CHOICE], or [ABSTRACTIVE], using the format $\ll \{ \ t @ \mathfrak { g } \} \quad \{ \mathfrak { q } _ { 0 } \} \gg$ . Our hypothesis is based on recent research that highlights the use of RAG (Lewis et al. 2020) to alleviate hallucinations. The additional context provides valuable signals related to the hallucination rate of the original query. Furthermore, we apply this technique to distinguish our experimental results from reused datasets in different scenarios, such as SciQ (Johannes Welbl 2017) and SQuADv2 (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang 2018).

H4R: Downstream Modes For HalluciBot. Without HalluciBot’s feedback, typical query rewriting models have to act as both an implicit critic and a generator. As a proxy reward model, HalluciBot’s probabilistic feedback on a query’s quality, given the dual prediction heads for hallucination and consensus, can guide a query rewriting process using an independent gpt-3.5-turbo LLM, before output generation. In essence, HalluciBot provides the following downstream modes for handling potentially hallucinatory queries:

1. Rewrite Mode: A single-shot iterative rewrite of queries classified as hallucinatory.   
2. Rank Mode: Generating $N$ intermediate perturbations, sorted by HalluciBot’s class probabilities for fine-grained scoring. In our implementation, the number of outputs were controlled by the number of chat completion choices in gpt-3.5-turbo’s API call.   
3. Route Mode: For Abstractive or Extractive queries classified as hallucinatory, testing if switching the scenario   
自 Split= Train Split $\ O =$ Validation Split $\ c =$ Test 1.0 1.0   
Uncalibrated (BS=0.15) 王 Uncalibrated (BS=0.19) Uncalibrated (BS=0.22) 垂   
Calibrated (BS=0.15) 0.8- Calibrated (BS=0.17) 0.8 Calibrated (BS=0.20) 0.6 0.6 0.4 0.4 . 0.2 1 0.2 .   
.. · .   
... 0.0t 0.0+   
0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 PredictedProbability Predicted Probability Predicted Probability Split $\ c =$ Train, Uncalibrated Split $\ c =$ Validation,Uncalibrated Split $\ c =$ Test, Uncalibrated   
No 87.3% 45.1%34.1% 28.5% 24.2% 19.6% 18.1% 72.0%46.1%30.9% 23.5% 23.4% 21.6% 18.7% 72.7%45.7%34.7% 28.3% 28.9% 28.5% 33.2%   
Yes 12.7% 54.9% 65.9% 71.5% 75.8% 80.4% 81.9% 28.0% 53.9%69.1% 76.5% 76.6% 78.4% 81.3% 27.3% 54.3% 65.3% 71.7% 71.1% 71.5% 66.8%   
0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 Split $\mathbf { \sigma } = \mathbf { \sigma }$ Train,Calibrated Split $\ c =$ Validation,Calibrated Split $\mathbf { \sigma } =$ Test, Calibrated   
No 80.2%33.6% 23.8% 19.4% 16.4% 12.8% 12.4% 62.5% 35.7%23.6% 17.2% 16.4% 15.2% 13.3% 64.5% 37.6%27.8% 21.5% 24.1% 22.9% 25.5%   
Yes 19.8%66.4% 76.2% 80.6% 83.6% 87.2% 87.6% 37.5% 64.3% 76.4% 82.8% 83.6% 84.8% 86.7% 35.5% 62.4% 72.2% 78.5% 75.9% 77.1% 74.5%   
0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6   
Number of Actual Hallucinations Number of Actual Hallucinations Number of Actual Hallucinations

(e.g. between RAG or direct inference) generates more robust classifications and generations.

# 4 Experimental Setup

Dataset Coverage & Scenario Split. Our experiments include 13 datasets (Table 1) divided into 3 scenarios: Extractive, Multiple Choice, and Abstractive. To evaluate the impact of context, we use SQuADv2 (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang 2018) to simulate RAG (Lewis et al. 2020). To assess the effect of multiple choice queries, we repurposed TruthfulQA (Lin, Hilton, and Evans 2022) and SciQ (Johannes Welbl 2017) for two experiments: one where the output agents select from the choices or context, and another where LLM agents generate outputs without context. We maintain the original train, validation, and test splits across scenarios to prevent information leakage to HalluciBot. All LLM agents share the same set of parameters.

HalluciBot Training Parameters & Environment. All experiments were conducted on an AWS EC2 instance with a single GPU. HalluciBot is fine-tuned from both pretrained BERT (Devlin et al. 2018) and RoBERTa (Liu et al. 2019) models. To address label imbalance, we employed a weighted class loss where each class weight is assigned to its inverted frequency in the training set. The train, validation, and test splits follow the original divisions of the datasets. Specifically, there are 302,492 training, 44,491 validation, and 22,854 testing samples. The distribution of labels across these splits is summarized in Table 2. We apply Platt calibration (Platt 1999) based on the validation logits to help ensure that the raw probabilities align better with the true class labels (Figure 5).

# 5 Analysis & Discussion

Ablation: Perturbations Induce Output Diversity. We examine the impact of perturbations on the robustness of gpt-3.5-turbo in question-answering tasks by comparing two strategies: Single Query, Multiple Outputs (SINGLE) and Single Query, Multiple Perturbations, Single Output (MULTI). In the SINGLE strategy, we sample $n + 1$ outputs from the original query $q _ { 0 }$ . In the MULTI strategy, $n$ perturbations of the original query $q _ { 0 }$ are used, and each perturbation $q _ { i }$ is answered once. Table 3 shows that while baseline accuracy remains consistent, the lower bound accuracy drops by 12.1 points in the MULTI setting. Additionally, agreement metrics, as indicated by Fleiss’s $\kappa$ , decrease by 12.5 points, indicating reduced consistency. In summary, (1) the SINGLE strategy results in higher agreement and lowerbound accuracy while (2) the MULTI strategy increases response diversity and hallucination rates but offers a slight improvement in upper-bound accuracy for Extractive and Multiple Choice scenarios. This suggests that perturbations can enhance query quality by introducing necessary diversity, despite minor variations in modal accuracy.

Ratiocinate: Can HalluciBot Detect Hallucinatory Queries? Differentiating the scenario in HalluciBot’s prompt yielded a strong $+ 1 0 . 3 \hat { \% }$ increase in validation F1 score. The calibrated, threshold-tuned RoBERTa-base HalluciBot in Table 4 achieves a test accuracy of $6 9 . 5 \%$ with a macro F1- score of $7 6 . 0 \%$ . Further breaking down the results in Figure 5, calibrating our models with Platt scaling improves the discriminating power for borderline queries, where the observed number of hallucinations was minimal $( y \in \{ 1 , 2 \}$ ). Finally,

Table 5: Query generation metrics under each HalluciBot (HB) strategy. Multiple Choice queries use a soft accuracy criterion where the score is $+ 1$ if any of the $n$ generations match the ground truth. Abstractive queries report the average cosine similarity score between the ground truth and the $n$ generation outputs. Embedding vectors are computed using all-MiniLM-L6-v2 (Wang et al. 2020).   

<html><body><table><tr><td colspan="2">Metrics (%) Test</td><td>Metrics (%)</td><td>Test</td></tr><tr><td colspan="2">(A) Naive Rewrite</td><td colspan="2">(D) HB Ratiocinate</td></tr><tr><td rowspan="4">+ Class Transitions Class Transitions UnneededRewrites</td><td>6.5</td><td>+Class Transitions</td><td>14.8</td></tr><tr><td>3.2</td><td>Rewrite Accuracy</td><td></td></tr><tr><td>46.6</td><td>Top-5 Accuracy</td><td>92.9</td></tr><tr><td></td><td>Similarity Score</td><td>41.7</td></tr><tr><td colspan="2">(B) HB Informed Rewrite + Class Transitions 30.2</td><td colspan="2">(E)w/ Consensus</td></tr><tr><td colspan="2">Rewrite Accuracy</td><td colspan="2">+ Class Transitions 31.9</td></tr><tr><td colspan="2"></td><td colspan="2">Rewrite Accuracy</td></tr><tr><td colspan="2">Top-5 Accuracy</td><td colspan="2">Top-5 Accuracy 90.2</td></tr><tr><td colspan="2">Similarity Score</td><td colspan="2">Similarity Score 57.5</td></tr><tr><td colspan="2">(C) HB Best-of-N Rewrite</td><td colspan="2">(F) w/ Consensus</td></tr><tr><td colspan="2">+ Class Transitions</td><td colspan="2">+ Class Transitions 51.4</td></tr><tr><td colspan="2"></td><td colspan="2">50.6</td></tr><tr><td colspan="2">Rewrite Accuracy</td><td colspan="2">Rewrite Accuracy</td></tr><tr><td colspan="2">Top-5 Accuracy</td><td colspan="2">Top-5 Accuracy 95.7</td></tr><tr><td colspan="2">Similarity Score</td><td colspan="2">Similarity Score 55.9</td></tr></table></body></html>

HalluciBot demonstrates strong recall scores $( 8 9 . 0 \%$ validation, $8 2 . 6 \%$ testing) to effectively flag risky queries that are likely to generate at least one hallucination during inference. The importance of HalluciBot as a ratiocinating process can be seen in [Table 5 (A)] under a naive rewriting strategy. Without HalluciBot, a naive rewrite strategy has the potential to convert queries originally estimated to be non-hallucinatory to hallucinatory (negative class transition), because there is no mechanism to differentiate queries. With HalluciBot restricting the test set to only potentially hallucinatory queries (11.2K samples), a naive rewrite [Table 5 (D)] can only enact positive class transitions $( + 1 4 . 8 \% )$ , converting queries originally estimated to hallucinate to non-hallucinatory. Furthermore, HalluciBot acting as an arbitrator can prevent computationally expensive rewrite calls for $4 6 . 6 \%$ of the test set (10.2K samples deemed to be non-hallucinatory).

Rewrite: As a Feedback Mechanism. HalluciBot’s feedback allows us to generate a more informed query [Table 5 (B)] resulting in better class transition probabilities than an uninformed rewrite strategy [Table 5 (D)]. This translates to a $1 4 . 8 \%$ positive class transition and a $1 . 4 \%$ increase in Multiple Choice accuracy as well as $5 . 2 \%$ improvement in generation similarity for Abstractive queries. Utilizing consensus information during the query rewriting process [Table 5 (E)] generates a slightly larger positive class transition $( 3 1 . 9 \%$ vs. $3 0 . 2 \%$ ) than without [Table 5 (B)].

Rank: Best-of-N Rewrite. A Best-of-N rewrite strategy [Table 5 (C, F)] demonstrates a $1 9 . 5 \%$ and $2 0 . 4 \%$ gain in positive class transitions in both experiments over a single rewrite [Table 5 (B, E)]. Therefore, HalluciBot’s estimated probabilities can be used as a proxy reward model when ranking $n$ sample perturbations. The violin plots in Figure 6 shows that HalluciBot is able to select better queries in the Best-of-N rewrite setting than with a single rewrite, with a higher median predicted non-hallucinatory probability $( 7 9 . 5 \%$ (C) vs. $7 8 . 4 \%$ (B); $8 0 . 4 \%$ (F) vs. $7 6 . 5 \%$ (E)). This means that HalluciBot evaluated the rewritten queries to be non-hallucinatory with greater probability. All rewrites are single-shot without subsequent tuning or iterations.

![](images/710a36f95687bfd2e3fe9e53b8e5405ddbb5510b9e7b6403392feacfb22be91e.jpg)  
Figure 6: Class probability of queries that were rewritten and reclassified to be non-hallucinatory.

Route: Abstractive to Extractive. Among 948 hallucinatory Abstractive queries, switching the scenario to Extractive led to a $+ 6 0 . 0 \%$ positive class transition, compared to just $9 . 7 \%$ with rewriting alone. HalluciBot’s ability to distinguish scenarios aids in determining whether direct inference or RAG is more effective for a particular query.

# 6 Conclusion

We propose a heretofore relatively unexplored realm of hallucination mitigation - predicting a query’s hallucination probability. HalluciBot estimates this probability using a diverse training corpus, ensuring robustness across scenarios and domains. Institutions can leverage HalluciBot to measure LLM performance and user accountability through our H4R framework (Ratiocinate, Rewrite, Rank, Route). Thus, HalluciBot’s contributions add to the ever-growing effort of enabling a robust language generation ecosystem for society.

Disclaimer: This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorgan Chase & Co. and its affiliates ("JPMorgan”) and is not a product of the Research Department of JPMorgan. JPMorgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.