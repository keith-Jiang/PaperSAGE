# One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models

Yutao Zhu, Zhaoheng Huang, Zhicheng Dou\*, Ji-Rong Wen

Gaoling School of Artificial Intelligence, Renmin University of China yutaozhu94 $@$ gmail.com, huangzh, dou, jrwen @ruc.edu.cn

# Abstract

Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs’ general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs’ original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs’ performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 questionanswering tasks demonstrate the superiority of our approach.

# Introduction

Large language models (LLMs) have achieved remarkable performance across various natural language processing tasks (Brown et al. 2020; OpenAI 2023; Touvron et al. 2023). Despite their extensive parameters enabling them to learn rich knowledge during pre-training, LLMs may still generate hallucinated, outdated, or inaccurate content, especially in scenarios requiring long-tail knowledge (Ji et al. 2023; Zhang et al. 2023b). To address this problem, retrieval-augmented generation (RAG) has emerged as a pivotal strategy. By explicitly decoupling knowledge retrieval from the backbone LLMs, such architectures have achieved more accurate and reliable content generation and shown particularly enhanced performance on knowledge-intensive tasks such as open-domain question answering (Petroni et al. 2021; Tan et al. 2024; Jin et al. 2024b).

Existing efforts in RAG development can be roughly categorized into two groups (as illustrated in Figure 1). The first group leverages the in-context learning capabilities of

LLMs by incorporating retrieved information into the input along with appropriate prompts (Shi et al. 2023; Ram et al. 2023). This allows for straightforward application to any offthe-shelf LLM without tuning its parameters. However, its effectiveness largely depends on the human experience in crafting effective prompts and the LLM’s ability to interpret these prompts. The second group focuses on training LLMs to enhance their performance in RAG scenarios. This training might involve either end-to-end pre-training (Guu et al. 2020; Borgeaud et al. 2022) or fine-tuning (Lin et al. 2023; Wang et al. 2023) for specific tasks. These approaches can often lead to better performance, but they require significant computational resources. Recently, parameter-efficient finetuning techniques, such as LoRA (Hu et al. 2022), have been widely studied, significantly reducing training costs. These methods can optimize the LLMs’ parameters for RAG, but unfortunately compromise the model’s general abilities in non-RAG scenarios, such as commonsense reasoning and in-context learning. All these limitations prevent their application to LLMs already operational in real-world settings.

Therefore, a critical research problem arises: Is it possible to enhance LLMs’ performance under RAG scenarios while preserving their general generation capabilities? To achieve this, we introduce a novel, lightweight tuning method named SPRING, which learns Scalable and Pluggable viRtual tokens for retrIeval-augmeNted Generation. Our basic idea is to add trainable virtual tokens to help LLMs learn RAG problems. Through fine-tuning, these virtual tokens effectively enhance the LLM’s capability to understand retrieved information and its correlation with user inputs. Importantly, as the LLM’s original parameters are frozen, its general generation abilities are preserved without any loss. During inference, when retrieval is triggered, these trained virtual tokens can be simply added to the prompt, which includes both the retrieved results and user input, thereby significantly enhancing performance. Moreover, we employ a scalable training approach, allowing the number of virtual tokens to be adjusted according to the needs of the inference scenario. Various training strategies have been implemented to further improve the generalizability of our method, ensuring robustness regardless of the number of the retrieved results.

In experiments, SPRING is trained with the base and instruction fine-tuned versions of Mistral-7b, LLaMA-2- 7b, and LLaMA-2-13b models and evaluated on 12 com

Retrieved results Input General 6 6 紫 Prompt   
RAG: LLM □ LLM tAodkdends LLM 电 Tuning RAG   
General: □ Q LLM Q □ SPRING (1) Prompt-based (2) Tuning-based (3) SPRING Performance comparison

monly used QA datasets, covering both in-domain and outof-domain scenarios. The experimental results demonstrate that SPRING not only effectively improves the RAG performance of LLMs but also successfully preserves their general generation capabilities. Overall, the SPRING method exhibits four main characteristics:

Lightweight yet effective. Instead of updating the full parameters of the LLMs, we opt to freeze the pre-trained models and only learn the embeddings for the added virtual tokens. For example, adding 50 tokens to the Mistral7b model introduces only 0.2M parameters in total. Despite these minimal parameters, SPRING improves the average EM and F1 scores by more than $43 \%$ and $17 \%$ across 12 QA datasets, respectively.

Scalable. With our proposed scalable training approach, SPRING can be effective with any number of virtual tokens $( k \in [ 1 , 5 0 ]$ in our experiments). Remarkably, even just one token can substantially improve the LLMs’ performance in RAG scenarios.

Pluggable. Owing to its lightweight design, SPRING can be applied in a plug-and-play manner. When retrieval is triggered, simply adding the virtual tokens can lead to better performance. In non-RAG scenarios, the virtual tokens are not added so the LLMs’ original capabilities can be well preserved. This characteristic is crucial for LLMs that have already been deployed for practical use.

Generalizable. Our robust training strategies ensure that SPRING is adaptable to different retrievers and various numbers of retrieved results. Consequently, there is no need to retrain SPRING with each update to the retrieval system, enhancing its practicality and efficiency.

# Related Work

Retrieval-Augmented Generation Compared to standard text generation, retrieval-augmented generation (RAG) incorporates a retrieval module that accesses external knowledge to enhance generation quality (Lewis et al. 2020; Guu et al. 2020; Zhu et al. 2023; Jin et al. 2024a). The mainstream RAG follows a “retrieve-then-read” paradigm, where the retrieval module provides external knowledge as additional context, which is then read by generation models to produce the final output (Shi et al. 2023; Ram et al. 2023; Borgeaud et al. 2022; Lin et al. 2023; Zhu et al. 2024). To optimize the use of external knowledge, some methods focus on crafting effective prompts that guide the utilization of retrieved information (Shi et al. 2023; Ram et al. 2023). These prompt-based methods are applicable to any LLM without tuning its parameters. However, they depend heavily on skillful prompt writing and the LLMs’ ability to understand instructions. In contrast, other studies attempts to directly train the model to better use the retrieved knowledge. For example, REALM (Guu et al. 2020) and RETRO (Borgeaud et al. 2022) incorporate retrieval in endto-end retrieval-augmented pre-training. RA-DIT (Lin et al. 2023) employs fine-tuning to enhance LLMs’ retrieval understanding. These tuning-based methods often yield better performance than prompt-based methods by optimizing model parameters for RAG. However, they may compromise the LLMs’ general capabilities, particularly in non-retrieval scenarios. Different from existing methods, we design a new lightweight tuning method for RAG. It is a plug-and-play module that enhances RAG performance using trainable virtual tokens, which can be removed in non-RAG scenarios to preserve the LLMs’ general generation abilities.

Parameter-Efficient Fine-Tuning The paradigms of “pre-training then fine-tuning” have demonstrated efficacy across various natural language (Devlin et al. 2019; Raffel et al. 2020; Radford et al. 2019) and vision tasks (He et al. 2020; Chen et al. 2020). The common fine-tuning process involves tuning all parameters of a model, which is computational intensive, especially for LLMs. To address this, parameter-efficient fine-tuning (PEFT) (Mangrulkar et al. 2022) approaches have been developed. These approaches freeze most of the pre-trained models’ parameters, yet still manage to achieve comparable performance on downstream tasks. PEFT has been widely studied (Wan et al. 2023), and typical methods including adapter-based tuning (Houlsby et al. 2019; Lin, Madotto, and Fung 2020; Chen et al. 2023), low-rank adaptation (LoRA) (Hu et al. 2022; Dettmers et al. 2023), and prompt tuning (Li and Liang 2021; Lester, AlRfou, and Constant 2021; Liu et al. 2021b,a). Adapter-based tuning inserts lightweight modules into a model’s existing layers and have been extended to various domains (Gao et al. 2024; Hu et al. 2023; Zhang et al. 2023a). LoRA (Hu et al. 2022) introduces trainable low-rank matrices that adjust the model’s weight updates, achieving promising finetuning performance on LLMs (Hu et al. 2023). Prompt tuning incorporates a series of trainable prompt tokens to LLMs. These tokens can be inserted either to the input layer only (Lester, Al-Rfou, and Constant 2021; Liu et al. 2021b)

or to all of the intermediate layers (Li and Liang 2021; Liu et al. 2021a). In this paper, we proposes a novel prompt tuning method, SPRING, specifically designed for RAG scenarios. Our method introduces virtual tokens between retrieved results and the input, exploiting the auto-regressive generation paradigm to improve the model’s ability to utilize retrieved information. Additionally, it is designed to be scalable and pluggable, thus broadening its application scope while preserving the original generative capabilities of LLMs.

# Methodology

To take advantage of both the flexibility of prompt-based methods and the efficacy of fine-tuning-based methods, we propose SPRING to learn scalable and pluggable virtual tokens for retrieval-augmented generation (RAG).

# Problem Formulation

Language models are designed to calculate the probability distribution over sequences of natural language texts. Autoregressive models are commonly used for this through nexttoken prediction:

$$
p _ { \mathrm { L M } } = \prod _ { i = 1 } ^ { m } p _ { \theta } ( x _ { i } | \boldsymbol { x } _ { < i } ) ,
$$

where $x _ { < i }$ denotes the sequence of tokens preceding $x _ { i }$ at each step, and $\theta$ represents the parameters of the model. For RAG, a retrieval corpus $\mathcal { D }$ and a retriever $M$ are introduced. Then, the generation process is conditioned on both $x _ { < i }$ and the retrieved results $R = M _ { D } ( x _ { < i } )$ as:

$$
\begin{array} { c } { { p _ { \mathrm { R A G } } = \displaystyle \prod _ { i = 1 } ^ { m } p _ { \theta } ( x _ { i } | R ; x _ { < i } ) , } } \\ { { p _ { \mathrm { R A G - Q A } } = \displaystyle \prod _ { i = 1 } ^ { m } p _ { \theta } ( a _ { i } | R ; Q ; a _ { < i } ) . } } \end{array}
$$

Note that here $x _ { < i }$ serves as the query for retrieval. In question-answering (QA) tasks, $x _ { < i }$ is usually the question $Q$ , and the learning objective is to generate the right answer $A = \{ a _ { i } \} _ { i = 1 } ^ { m }$ . The retriever can yield multiple passages, which can be concatenated as a long text sequence using proper separator such as $^ { \langle \epsilon \rangle } \langle \boldsymbol { \mathrm { n } } \rangle \boldsymbol { \mathrm { n } } ^ { \prime \prime }$ . For brevity, this formulation directly concatenates the retrieved results $R$ with the question $Q$ , omitting more complex prompt designs. Henceforth, we will use the notations in QA tasks as our evaluation is performed on them.

# Scalable and Pluggable Virtual Tokens for RAG

Our SPRING method, shown in the left side of Figure 2, introduces trainable virtual tokens into the input to optimize LLMs for RAG scenarios. Specifically, following the notation in Equation (3), we add $n$ trainable tokens $T =$ $\left[ t _ { 1 } , t _ { 2 } , \cdots , t _ { n } \right]$ between the retrieved results $R$ and the input $Q$ . The generation process can then be described as:

$$
p _ { \mathtt { S P R I N G } } = \prod _ { i = 1 } ^ { m } p _ { \theta , \delta } \big ( a _ { i } | R ; [ t _ { 1 } , t _ { 2 } , \cdot \cdot \cdot , t _ { n } ] ; Q ; a _ { < i } \big ) ,
$$

where $\delta \in \mathbb { R } ^ { n \times d }$ represents the added parameters of the trainable tokens (i.e., their embeddings), and $d$ is the embedding size of the LLM. $\theta$ denotes the parameters of the backbone LLM, which are frozen during training. Given that $\left| \delta \right| \ll \left| \theta \right|$ , our method is highly efficient for training. For example, with the Mistral-7b model (where $d \ : = \ : 4 , 0 9 6 )$ , when $\textit { n } = \ 5 0$ tokens are added, we only add and train $5 0 \times 4 , 0 9 6 = 0 . 2 \mathrm { { M } }$ parameters, approximately $0 . 0 0 3 \%$ of the full model.

Importantly, we place the virtual tokens $T$ between the retrieved results $R$ and the question $Q$ for two main reasons: (1) In the auto-regressive generation paradigm, positioning the tokens after the retrieved results allows them to attend to this information, thereby aiding the model’s comprehension. (2) Recent studies have indicated that LLMs are particularly sensitive to the end of an input (Liu et al. 2023). By consistently placing these virtual tokens before the question across all test samples, we aim to mitigate any potential adverse effects on the understanding of the question.

Scalable In practical developments, LLMs are often constrained by their maximum input lengths, limiting the number of tokens available for retrieval augmentation (especially when the retrieved results are very long). Therefore, it is desired to design a mechanism so that any number of virtual tokens can be used in the inference to improve RAG performance. To achieve this, we propose an optimization strategy working like a “spring” (as shown in Figure 2). Specifically, for a given sample $\{ R , Q , A \}$ with the total number of added tokens $n$ , we randomly select a number $k ( k \leq n )$ and utilize the first $k$ virtual tokens $t _ { 1 : k }$ to construct the training example as $[ R ; t _ { 1 } , t _ { 2 } , \cdots , t _ { k } ; Q ]$ . This method allows for the flexible optimization of any number of virtual tokens. Consequently, the number of virtual tokens incorporated during inference can be arbitrarily selected based on the requirements of the application. The effectiveness of this strategy and its comparison with other methods are further discussed in our experiments.

Pluggable Due to its designed structure, our method provides considerable flexibility in application. Practically, if user input is assessed to require external knowledge, our virtual tokens can be simply appended after the retrieval results and then fed, along with the user input, into the LLM for generation. In contrast, if the user input does not necessitate retrieval, it can be processed directly by the LLM. As our approach does not adjust the original parameters of the LLM, it preserves the model’s inherent capabilities. This feature is particularly important for industry or business since existing LLMs may have already been deployed for multiple purposes; our method enhances the retrieval understanding capabilities of these models without compromising their existing functionalities.

Inference We illustrate the instructions for using our SPRING in the right side of Figure 2. After training, the embeddings of the added tokens have been optimized for RAG, but these tokens do not correspond to any existing tokens in the vocabulary. To make them easy to use, we can add some special tokens $( e . g . , \ [ \Sigma 1 ] , \ \cdot \cdot \cdot , \ [ \Sigma 5 0 ] )$ to the

Scalable ↑cW↓ Sequence = [𝑅; 𝑡1, 𝑡2; 𝑄] Original vocab Added tokens Question Passages 𝑡1 unused Question 0 <s> 32000 [r1] Retriever Q WWW Sequence = [𝑅; 𝑡1, 𝑡2, 𝑡3; 𝑄] 1 </s> 32001 [r2] Passages R 1 LLM   
Passages 𝑡2 unused Question … … … 31998 ゼ 32048 [r49] <WW Sequence = [𝑅; 𝑡1, ⋯ , 𝑡5; 𝑄] 31999 梦 32049 [r50] R [r1] ⋯ [rk] Q   
Passages 𝑡 𝑡2 𝑡3 𝑡4 Question R + R unused Q (1) Add special tokens and (2) Retrieve, append tokens, merge embeddings then generate Added $n$ Trainable Tokens for RAG Training Inference

vocabulary and initialize their embeddings with the trained embeddings. Then, during inference, after obtaining the retrieved results $R$ , we can add any number of these special tokens (e.g., $[ \Upsilon 1 ] \ \cdot \ \cdot \ [ \Upsilon \mathbf { k } ] \ \rangle$ ) after $R$ and input them with the question $Q$ to the LLMs for generation.

We refer to our method as SPRING due to its scalable and pluggable nature, making it particularly well-suited for enhancing existing LLMs that have already been deployed. Additionally, it effectively bridges the gap between retrieved results and user input, significantly improving the LLMs’ capabilities in understanding the retrieved external knowledge.

# Experiment

# Datasets and Retrievers

We conduct experiments on twelve commonly used question-answering datasets, including TriviaQA (TQA) (Joshi et al. 2017), Natural Questions (NQ) (Kwiatkowski et al. 2019), HotpotQA (HQA) (Yang et al. 2018), SQuAD 1.1 (Rajpurkar et al. 2016), Web Questions (WebQ) (Berant et al. 2013), 2WikiMultiHopQA (2Wiki) (Ho et al. 2020), CoQA (Reddy, Chen, and Manning 2019), MS MARCO (Nguyen et al. 2016), PopQA (Mallen et al. 2023), Fermi (Kalyan et al. 2021), Musique (Trivedi et al. 2022), and Bamboogle (Press et al. 2023). These datasets are publicly available at HuggingFace or their official websites. To evaluate the generalizability of the methods, we select PopQA, Fermi, Musique, and Bamboogle as held-out datasets. We mix the training set of all remaining datasets for training. For all datasets, we prioritize the use of test sets for evaluation purposes. In cases where the test set is not available, we utilize the development set instead. It is worth noting that, though some datasets have provided golden reference passages for the answer, we do not use them in our experiment but use the passages retrieved from the retrieval sets in both training and inference stages, which aligns with practical applications. Exact match (EM) and F1 score are employed as evaluation metrics.

For the retrieval sets, we follow previous studies (Yoran et al. 2023) and use the combination of Wikipedia and MS MARCO datasets as the retrieval corpus. Wikipedia contains high-quality human knowledge, which is helpful for many knowledge-intensive tasks. MS MARCO contains a large amount of web pages, which can provide information necessary for curating some natural language questions. We use the datasets that have already been preprossed into passages and released on HuggingFace.1 The Wikipedia set has 21M passages, while the MS MARCO set has 8M passages.

We use E5-large (Wang et al. 2022) as the main retriever in our experiments. The impact of other retrievers, i.e., BM25 (Robertson and Zaragoza 2009), BGE-base (Xiao et al. 2023), and E5-base, is studied in our further analysis. Among these retrievers, BM25 is a nonneural sparse retrieval algorithm, while others are neuralbased dense retrievers. In general, dense retrievers perform better on several benchmarks (Muennighoff et al. 2023).

# Baseline Methods

We consider both the base and instruction fine-tuned versions of Mistral-7b, LLaMA-2-7b, and LLaMA-2-13b as the backbone models, and compare our SPRING with the following baselines.

Concat: This method directly concatenates the retrieval results and the question for evaluation. • Prompt: This method uses a manually-crafted prompt to indicate the use of retrieval information. Prefix-tuning (Li and Liang 2021): This method uses prefix-tuning to fine-tune the backbone models. To make a fair comparison with our method, we add 50 prefix tokens for training. LoRA (Hu et al. 2022): This method uses LoRA to finetune the backbone models. We use the hyperparameters suggested by the LLaMA’s official guidance.2 To further validates the effectiveness of our SPRING on models that have already been optimized for RAG, we also train our method based on the LoRA checkpoint, and denote this variant as SPRING+.

Table 1: Evaluation results of different methods on twelve QA datasets. The retriever is $\mathtt { E 5 - 1 }$ arge model, and the number of retrieved passages is set as three. The number of virtual tokens used in SPRING is set as 50. ∗PopQA, Fermi, Musique, and Bamboogle are invisible during training. “Prefix” stands for prefix-tuning, and “SPRING+” is trained based on the LoRA’s checkpoint. The best results are in bold.   

<html><body><table><tr><td colspan="9">with Retrieval</td><td colspan="5">without Retrieval</td></tr><tr><td>Dataset</td><td>Metric</td><td>Concat</td><td>Prompt</td><td>Prefix</td><td>LoRA</td><td>SPRING</td><td>SPRING+</td><td>Concat</td><td>Prompt</td><td>Prefix</td><td>LoRA</td><td>SPRING</td><td>SPRING+</td></tr><tr><td colspan="2">Tuning Parameters</td><td>0</td><td>0</td><td>0.2M</td><td>4M</td><td>0.2M</td><td>4.2M</td><td>0</td><td>0</td><td>0.2M</td><td>4M</td><td>0.2M</td><td>4.2M</td></tr><tr><td rowspan="2">Trivia QA</td><td>EM</td><td>6.0</td><td>57.79</td><td>11.74</td><td>82.76</td><td>85.71</td><td>63.9</td><td>6.06</td><td>39.90</td><td>20.0</td><td>3.01</td><td>4.56</td><td>43.37</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">NQ</td><td>FM</td><td>41.7</td><td>28.99</td><td>13.04</td><td>47.5</td><td>42.35</td><td>49.728</td><td>4.4</td><td>13.36</td><td>17.74</td><td>29.00</td><td>18.0</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>25.54</td></tr><tr><td rowspan="2">HQA</td><td>EM</td><td>4.901</td><td>26.36</td><td>5.79</td><td>39.95</td><td>35.26</td><td>41.4</td><td>4.54</td><td>47.07</td><td>17.05</td><td>2.037</td><td>20.15</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>23.39</td></tr><tr><td rowspan="2">SQuAD</td><td>EM F1</td><td>0.00</td><td>23.92</td><td>7.19</td><td>35.71</td><td>33.67</td><td>35.98</td><td>0.00</td><td>8.61</td><td>0.00</td><td>0.00</td><td>12.71</td><td>13.90</td></tr><tr><td>43.05</td><td>57.66</td><td>39.75</td><td></td><td>68.05</td><td>66.99</td><td>68.41</td><td>43.32</td><td>46.81</td><td>21.88</td><td>27.51</td><td>53.58</td><td>54.93</td></tr><tr><td rowspan="2">WebQ</td><td>EM</td><td>3706</td><td>17.53</td><td>31.45</td><td>43.65</td><td>31.8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>48.10</td><td>4.4</td><td>14.79</td><td>20.04</td><td>3.306</td><td>24.95</td><td>28.851</td></tr><tr><td rowspan="2">2Wiki</td><td>EM</td><td>4.07</td><td>22.648</td><td>438</td><td>35.93</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>31.80</td><td>37.12</td><td>0.83</td><td>23.45</td><td>21.14</td><td>37009</td><td>24.62</td><td>28.60</td></tr><tr><td rowspan="2">CoQA</td><td>EM</td><td>2.08</td><td>36.2</td><td>21.</td><td>42.19</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>13.28</td><td>43.87</td><td>3.97</td><td>8.58</td><td>1.15</td><td>1.9</td><td>99.96</td><td>12.50</td></tr><tr><td rowspan="2">MS MARCO</td><td>EM</td><td>0.00</td><td>5.73</td><td>0.60</td><td>8.13</td><td>6.57</td><td>8.27</td><td>0.00</td><td>2.56</td><td>0.00</td><td></td><td></td><td>3.24</td></tr><tr><td>56.44</td><td></td><td>53.81</td><td>50.56</td><td>54.81</td><td>53.48</td><td>55.90</td><td>49.50</td><td>47.75</td><td>47.44</td><td>0.01</td><td>2.09 51.41</td><td>49.84</td></tr><tr><td rowspan="2">PopQA*</td><td>F1 EM</td><td>5.49</td><td>39.79</td><td>10.4</td><td>47.12</td><td></td><td></td><td></td><td></td><td></td><td>52.44</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>48.71</td><td>46.98</td><td>53.601</td><td>16.05</td><td>20.09</td><td>2.09</td><td>20.25</td><td>18.70</td></tr><tr><td rowspan="2">Fermi*</td><td>EM</td><td>0.00</td><td>0.06</td><td>0.00</td><td>0.12</td><td>0.18</td><td>0.24</td><td>0.00</td><td>0.06</td><td>0.06</td><td>0.00</td><td>0.06</td><td>0.19</td></tr><tr><td></td><td>21.66</td><td>18.16</td><td>12.65</td><td>29.63</td><td>31.15</td><td>30.67</td><td>25.51</td><td>17.84</td><td>20.33</td><td>25.42</td><td>29.28</td><td>30.37</td></tr><tr><td rowspan="2">Musique*</td><td>F1 EM</td><td>4.019</td><td>4.054</td><td></td><td>10.86</td><td></td><td>12.58</td><td></td><td>1.98</td><td></td><td>45.71</td><td>4.3</td><td></td></tr><tr><td></td><td></td><td></td><td>20.941</td><td></td><td>4874</td><td></td><td>4.00</td><td></td><td>3.2</td><td></td><td></td><td>47.30</td></tr><tr><td rowspan="2">Bamboogle*</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>3.202</td><td></td><td>12.00</td><td></td></tr><tr><td>EM</td><td>12.80</td><td>12.80</td><td>10.0</td><td>24.22</td><td>22.66</td><td>28.13</td><td>4.60</td><td>4.4</td><td></td><td>45.14</td><td></td><td>12.89</td></tr><tr><td rowspan="2">Average</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>EM</td><td>1.08</td><td>19.80</td><td>4.9</td><td>30.78</td><td>28.45</td><td>32.17</td><td>45.19</td><td>12.59</td><td>2.35</td><td>3.041</td><td>16.34</td><td>17.96</td></tr></table></body></html>

# Implementation Details

We use PyTorch (Paszke et al. 2019) and Huggingface Accelerate library to implement all methods. The learning rate is set as 1e-4 with a warm-up ratio of 0.1. All methods are trained for three epochs, with a training batch size of 256. We use eight NVIDIA A800 GPUs for training. Training our SPRING for Mistral-7b models consumes around 2.2 hours per epoch. The embeddings of the virtual tokens are initialized by the embeddings of the prompt: “According to the previous relevant passages, please answer the following question. Only return the answer without any other words.” Following the settings of prefix-tuning, if the number of tokens required exceeds those available in the prompt, the prompt is repeated to complete the initialization; if fewer are needed, the prompt is truncated accordingly. Additionally, we experiment with random initialization of tokens but observe that its performance is slightly worse than that achieved through prompt-based initialization. Our code is available at https://github.com/DaoD/SPRING.

# Experimental Results

We fine-tune the prefix-tuning, LoRA, and SPRING methods on RAG tasks, and then evaluate their performance in scenarios both with (RAG) and without (non-RAG) retrieval. For SPRING, we use $k = 5 0$ virtual tokens for inference by default, and the impact of token quantity $k$ is discussed in later. The experimental results are shown in Table 1. To save space, we only show the results based on Mistral-7b-instruct.

We can observe: (1) It is evident that SPRING significantly improves the RAG performance of the original LLM with manually-crafted prompts (the average EM and F1 scores are improved by $\mathrm { { \bar { 4 } 3 . 4 \bar { \% } } }$ and $1 7 . 3 \%$ , respectively). It outperforms LoRA on certain datasets, such as TriviaQA and CoQA. Given that SPRING involves only 0.2M trainable parameters, these results demonstrate its remarkable efficiency and effectiveness. (2) While LoRA achieves slightly better performance on some datasets, it adjusts the LLMs original parameters, which adversely impact their performance in non-RAG scenarios—a significant drop has been observed, even far worse than the original models. This challenge also extends to other general generation tasks, which will be discussed in the next section. (3) In non-RAG evaluation, only SPRING and $\mathrm { S P R I N G ^ { + } }$ demonstrate better performance than the Prompt method. This indicates that even in the absence of retrieved results, adding virtual tokens is still beneficial. We speculate that beyond simply utilizing retrieved results, virtual tokens can help the LLM understand the task goal and format (e.g., the task is questionanswering rather than text continuation). (4) Based on the LoRA’s checkpoint, $\mathrm { S P R I N G ^ { + } }$ achieves the best performance on most datasets. Additionally, all backbone models show improvements with SPRING. These findings verify the versatility and flexibility of our approach, confirming its suitability for enhancing various LLMs in RAG scenarios. (5) Using manually-crafted prompts is effective for improving LLMs’ performance on RAG tasks. However, this improvement is limited as no training is involved. (6) SPRING achieves robust performance on the held-out datasets, validating the good generalizability of our method. (7) Interestingly, prefix-tuning cannot perform well for RAG, highlighting that the insertion position of the virtual tokens in SPRING is both reasonable and effective.

Table 2: Performance comparison on other datasets.   

<html><body><table><tr><td>Dataset</td><td>n-shot</td><td>LoRA</td><td>SPRING</td><td>Diff</td></tr><tr><td>BoolQ</td><td>0</td><td>79.30</td><td>82.97</td><td>3.67</td></tr><tr><td>CommonsenseQA</td><td>0</td><td>55.45</td><td>63.80</td><td>8.35</td></tr><tr><td>CommonsenseQA</td><td>4</td><td>59.87</td><td>67.07</td><td>7.20</td></tr><tr><td>GSM8K</td><td>8</td><td>17.33</td><td>31.89</td><td>14.56</td></tr><tr><td>MMLU</td><td>0</td><td>51.30</td><td>53.62</td><td>2.32</td></tr><tr><td>MMLU</td><td>5</td><td>48.76</td><td>54.96</td><td>6.20</td></tr></table></body></html>

# Further Analysis

We further conduct a series of experiments to investigate the impact of different settings in SPRING. All the following experiments are conducted based on fine-tuning the Mistral-7b-instruct model.

Performance on Other Tasks To examine the impact of different fine-tuning methods on the inherent capabilities of LLMs, we evaluate the performance of models fine-tuned by LoRA and SPRING on several other (non-RAG) tasks. These tasks are commonly used to evaluate LLMs’ reasoning, mathematical abilities, and world knowledge, including BoolQ (Clark et al. 2019), CommonsenseQA (Talmor et al. 2019), GSM8K (Cobbe et al. 2021), and MMLU (Hendrycks et al. 2021). The experimental results are shown in Table 2.3 From the results, we can observe: (1) Thanks to the plugand-play design of our method, SPRING can revert to to the original LLMs by not using virtual tokens. Therefore, it successfully preserves the original capabilities of the LLMs. In contrast, LoRA, which adjusts the model’s parameters for RAG tasks, inevitably compromises the model’s performance on other tasks. (2) A noticeable decline is observed in the few-shot evaluation, reflecting a decrease in the incontext learning abilities of LLMs. This decline may stem from the fact that RAG fine-tuning does not incorporate incontext learning capabilities. Besides, fine-tuning for RAG tasks may lead the model to overfit to specific task formats (prompts), thereby impairing its general generation abilities.

![](images/5b734d3caebecdb523a514acff7de18f755ef5c7fbe0bd3b775072c44a71f634.jpg)

Figure 3: Average performance on nine QA datasets with various numbers of virtual tokens.   
Table 3: Average performance on nine QA datasets with different retrievers.   

<html><body><table><tr><td rowspan="2">Retriever</td><td colspan="2">Prompt</td><td colspan="2">SPRING</td></tr><tr><td>EM</td><td>F1</td><td>EM</td><td>F1</td></tr><tr><td>BM25</td><td>21.23</td><td>54.94</td><td>30.94</td><td>62.73</td></tr><tr><td>BGE-base</td><td>23.07</td><td>56.12</td><td>31.81</td><td>63.46</td></tr><tr><td>E5-base</td><td>24.38</td><td>56.84</td><td>33.34</td><td>64.49</td></tr><tr><td>E5-large</td><td>25.66</td><td>57.70</td><td>34.35</td><td>65.00</td></tr><tr><td>Average</td><td>23.58</td><td>56.40</td><td>32.61</td><td>63.92</td></tr><tr><td>Variance</td><td>2.69</td><td>1.02</td><td>1.75</td><td>0.78</td></tr></table></body></html>

Impact of Token Quantity In SPRING, we design a scalable training approach that enables to use arbitrary numbers of virtual tokens in inference. To validate its effectiveness, we test the performance of our method with various numbers of virtual tokens and compare it with a variant model trained with a fixed number of tokens $k = 5 0 \AA$ ). The experimental results are illustrated in Figure 3. In general, we can observe that the performance of SPRING increases with more virtual tokens used. Surprisingly, SPRING can significantly enhance LLMs’ performance in RAG scenarios with just a single token, which is very encouraging.4 In comparison, training with a fixed number of tokens limits the flexibility of SPRING, making it can only be used with the same number of tokens in inference (i.e., $k = 5 0$ ).

Effects of Different Retrievers In our experiments, SPRING is fine-tuned using passages retrieved by E5-large. To investigate its effectiveness with other retrievers, we conduct an experiment by testing its performance with passages retrieved by BM25, BGE-base, and

![](images/a392adfe4a38c880437334a0e8add8de7962ae093ec163d7ed3b0ac66dfc06e9.jpg)  
Figure 4: Average performance on nine QA datasets with different number of retrieved passages.

E5-large. The results are presented in Table 3. First, SPRING achieves consistent improvement over the original model using manually crafted prompt, thereby confirming the generalizability of our approach. Second, compared to the original model, the performance gap (variance) among different retrievers becomes smaller, highlighting SPRING’s robustness to variations in retrievers. Finally, even fine-tuned with a superior retriever (i.e., E5-large), SPRING maintains strong performance well with less effective retrievers (such as BM25). This indicates that our method can effectively adapt to varying quality of retrieved results. Hence, there is no necessity to retrain the virtual tokens with each update of retrievers in practical applications, significantly enhancing its applicability.

Influence of Retrieved Passages During the fine-tuning of SPRING, we construct training samples by randomly selecting the top- $m$ $( m \in [ 1 , 5 ] )$ retrieved passages. This aims to enhance SPRING’s adaptability by ensuring it can operate effectively with varying numbers of retrieved passages in real-world scenarios. To evaluate the effect of this training strategy, we test the SPRING’s performance across a range from zero to five passages. Figure 4 illustrates the results. We can find that SPRING’s performance gradually improves as more retrieved passages are used $m = 0  4$ ), suggesting that more retrieved passages contribute valuable knowledge for question answering. However, the performance peaks at four passages and declines when more passages are added. This decrease could be attributed to noise accumulation within the retrieved knowledge, a phenomenon also reported in recent studies (Yoran et al. 2023). Despite this, the use of retrieved passages still results in performance gains compared to scenarios without retrieval ${ \mathrm { \Delta } m = 0 }$ ), highlighting again the benefits of RAG.

Cross-Dataset Generalizability Inspired by previous studies in multi-task learning (Raffel et al. 2020; Khashabi et al. 2020), we mix eight QA datasets for training as they require similar LLM capabilities (e.g., reasoning). To study the impact of this strategy, we conduct experiments by training SPRING on each dataset individually and then testing its performance on the others. Table 4 shows partial results. As indicated, training on a mixed dataset generally enhances performance on most datasets, thereby validating the benefits of multi-task learning. While training on a single dataset, such as NQ, may yield superior results on its specific test set, such improvements often fail to generalize to other datasets. Notably, training solely on NQ may negatively impact performance on MS MARCO, where the original LLM using a prompt could outperform it. These findings inspire us to carefully consider the interaction between different datasets when applying our method in future applications.

Table 4: Performance comparison between training on a specific dataset or a mixture of all datasets.   

<html><body><table><tr><td rowspan="2">Training→ Test↓</td><td colspan="2">TQA</td><td colspan="2">NQ</td><td colspan="2">Mix</td></tr><tr><td>EM</td><td>F1</td><td>EM</td><td>F1</td><td>EM</td><td>F1</td></tr><tr><td>TQA</td><td>62.80</td><td>84.65</td><td>65.51</td><td>84.73</td><td>65.71</td><td>85.26</td></tr><tr><td>NQ</td><td>32.19</td><td>64.98</td><td>45.26</td><td>72.68</td><td>42.35</td><td>70.73</td></tr><tr><td>HQA</td><td>22.97</td><td>56.95</td><td>28.11</td><td>59.45</td><td>35.26</td><td>65.44</td></tr><tr><td>SQuAD</td><td>25.24</td><td>61.39</td><td>30.81</td><td>64.32</td><td>33.67</td><td>66.99</td></tr><tr><td>WebQ</td><td>26.03</td><td>62.40</td><td>34.13</td><td>67.30</td><td>31.84</td><td>64.78</td></tr><tr><td>2Wiki</td><td>17.32</td><td>54.41</td><td>25.43</td><td>58.21</td><td>31.80</td><td>62.03</td></tr><tr><td>CoQA</td><td>6.05</td><td>36.61</td><td>7.62</td><td>38.54</td><td>13.28</td><td>42.41</td></tr><tr><td>MARCO</td><td>3.26</td><td>31.88</td><td>3.88</td><td>33.33</td><td>6.57</td><td>53.48</td></tr><tr><td>PopQA</td><td>44.07</td><td>72.50</td><td>48.28</td><td>74.07</td><td>48.71</td><td>73.90</td></tr><tr><td>Average</td><td>26.66</td><td>58.42</td><td>32.11</td><td>61.40</td><td>34.35</td><td>65.00</td></tr></table></body></html>

# Conclusion

In this paper, we introduced scalable and pluggable virtual tokens for retrieval-augmented large language models. Our method, SPRING, serves as a parameter-efficient finetuning approach that significantly enhances RAG performance with the addition of only 0.2M trainable parameters. More importantly, the plug-and-play nature of our approach successfully preserves the performance of LLMs on non-RAG tasks, while its scalable training strategy broadens the method’s applicational flexibility. Through extensive experiments across various datasets, we have demonstrated the effectiveness, generalizability, flexibility, and high efficiency of our method. We believe that our research will foster further integration of information retrieval and LLMs, and advance the development of other parameter-efficient fine-tuning technologies for LLMs.