# Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators

Dingkang $\mathbf { Y a n g } ^ { 1 * \dagger }$ , Dongling $\mathbf { X _ { i a 0 } } ^ { 2 * }$ , Jinjie Wei1, Mingcheng $\mathbf { L i } ^ { 1 }$ , Zhaoyu Chen1, Ke $\mathbf { L i } ^ { 3 \ddagger }$ , Lihua Zhang1‡

1Academy for Engineering and Technology, Fudan University 2ByteDance 3Tencent Youtu Lab dkyang20, lihuazhang @fudan.edu.cn, xdluestc $@$ outlook.com, tristanli $@$ tencent.com

# Abstract

Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, i.e., unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic representations, which compromise the internal factual knowledge of target LLMs. In addition, hallucinations typically exhibit multifaceted patterns in downstream tasks, limiting the model’s holistic performance across tasks. In this paper, we propose a Comparator-driven DecodingTime (CDT) framework to alleviate the response hallucination. Firstly, we construct hallucinatory and truthful comparators with multi-task fine-tuning samples. In this case, we present an instruction prototype-guided mixture of experts strategy to enhance the ability of the corresponding comparators to capture different hallucination or truthfulness patterns in distinct task instructions. CDT constrains next-token predictions to factuality-robust distributions by contrasting the logit differences between the target LLMs and these comparators. Systematic experiments on multiple downstream tasks show that our framework can significantly improve the model performance and response factuality.

(a) Knowledge-grounded Dialogue Task [Knowl edge]: Christopher Meloni starred in Nights in Rodanthe (Romance Film). [User]: I just watched a film with Melon.. [Assistant ]: Of course, do you like.. [User]: Who else stars in Nights in Rodanthe? LLaMA-2-7B-Chat Of course. Also, Sophia Adams starred in Before Sunrise. Extrinsic-Hard Hallucination Extrinsic-Soft Hallucination LLaMA-2-7B-Chat $^ +$ CDT(ours) JamesFrancoand ScottGlennalso starred asintegral roles in Nights in Rodanthe. (b) Text Summarization Task [Document]: The man,in his 2Os,was the only person in the BMW convertible when the accident happened on the A323..... Police closed the road.. [User]: Please summarize the given document. LLaMA-2-7B-Chat A man in his 3Os crashed his BMW on the A320.The man lost control of.the car due to high speed, causing the crash. FaithfulHallucination Intrinsic Hallucination LLaMA-2-7B-Chat $^ +$ CDT(ours) A manin his2Oscrashed on theA323 in Guildford.Police areseekingwitnesses.

Code — https://github.com/ydk122024/CDT

# Introduction

Recently, the emergence of advanced Large Language Models (LLMs) (Touvron et al. 2023; OpenAI 2022; Achiam et al. 2023; Jiang et al. 2023) spearheads an understanding paradigm shift in the Natural Language Processing (NLP) and demonstrates superior capabilities in a wide range of tasks, such as translation, summarization, and dialog generation (See, Liu, and Manning 2017; Moon et al. 2019). Despite the impressive performance, LLMs typically generate seemingly plausible but non-factual claims, i.e., the hallucination dilemma. Hallucination content exhibits inconsistencies with real-world or user instructions, limiting the model credibility in realistic applications (Huang et al. 2023).

The hallucination reasons are thought to be complex, such as biased corpora, maximum likelihood training objective, and blind instruction following (Zhang et al. 2023b). All these factors potentially lead LLMs to assign non-zero probabilities to unfaithful token generation. Previous approaches have focused on two main aspects to provide solutions: optimizing model parameters and editing semantic representations. The former optimizes the model behavior directly from the response ranking via human preference alignment techniques (Ouyang et al. 2022; Bai et al. 2022; Tian et al. 2023), forcing the output to be more truthful. The Supervised Fine-tuning (SFT) is optionally used to mitigate taskspecific hallucinations (Jones et al. 2023). However, secondary training is labor-intensive, and unintended knowledge infusion inadvertently encourages target LLMs to capture spurious generation patterns. The latter works on regularizing and editing the hidden representations of LLMs to select the semantic spaces associated with correct statements (Burns et al. 2022; Chen et al. 2024; Li et al. 2024). Unfortunately, proactive representation activation may result in models hallucinating even knowing the correct semantics, interfering with internal factual knowledge (Zou et al. 2023). More importantly, current LLMs suffer from multifaceted hallucination patterns that cause serious performance bottlenecks. As shown in Figure 1, the model exhibits different types of hallucinations on each of the two tasks. There are even two hallucination patterns in the same response. For instance, LLaMA2-7B-Chat wrongly extracts information (i.e., faithful hallucination) in the text summarization and produces false content that is not present in the original document (i.e., intrinsic hallucination). Previous studies have not resolved the coupled hallucination dilemma well.

Motivated by the above observations, we propose an efficient Comparator-driven Decoding-Time (CDT) framework to improve response factuality. The core philosophy is to equip target LLMs with comparators modeling different generative attributes separately during the decoding process, using logit distribution integration to facilitate next-token prediction in the factuality-robust directions. We utilize explicit hallucinated/factual multi-task samples via Low-Rank Adaptation (LoRA)-based SFT to construct hallucinatory/truthful comparators based on the base models. In this case, we design a hallucination perturbation adversarial mechanism to strengthen the factual knowledge mastery of the truthful comparator and mitigate the potential overfitting phenomena. To assist target LLMs in eliminating the diverse hallucinations in cross-task generation, we present an instruction Prototype-guided Mixture of Experts (PME) strategy to empower the comparators with multi-task hallucination/truthfulness-aware capabilities. In the comparators, multiple LoRA adapters act as different experts to master different hallucination or truthfulness patterns through well-designed soft route gating. CDT is capable of accomplishing effective attribute control without compromising the internal knowledge of target LLMs, maintaining the generated fluency and diversity.

The main contributions are summarized as follows:

• Our framework can help target LLMs remove non-factual knowledge from the output space during the decoding process in a product-integrated manner, significantly improving the robustness and factuality of model responses. • Our comparators are not limited to specific model structures and task types, in a plug-and-play form to guide the next-token prediction towards the hallucination-weak distribution. The proposed PME strategy offers the prospect and potential of eliminating hallucinations with multifaceted patterns in different tasks. • Extensive experiments are conducted on multiple NLP benchmarks. Comprehensive analyses show the broad applicability and effectiveness of our framework.

# Related Work

Hallucination Mitigation in LLMs. Hallucination in LLMs means the generated content is not supported by verifiable real-world facts or user instructions (Huang et al. 2023; Zhang et al. 2023b), leading to conflicts with input queries (Rehman et al. 2023), contextual information (Shi et al. 2023; Wang et al. 2023), and faithful semantics (Bang et al. 2023; Hu et al. 2023). Recently, extensive works have been presented aiming at mitigating the hallucination interference in LLMs across different dimensions, including retrieval-augmented generation (Peng et al. 2023; Chern et al. 2023), high-quality data construction (Zhou et al. 2024; Li et al. 2023b), and synthetic task migration (Jones et al. 2023). Among these, mainstream efforts have mainly focused on two aspects: optimizing model parameters and editing semantic representations. The former argues that one of the most likely reasons for hallucinations is that the maximum-likelihood objective of language modeling leads LLMs to assign non-zero probabilities to unfaithful sentences outside the training data distribution (Chuang et al. 2023). To this end, these approaches promote harmless response generation through reinforcement learning from human feedback (Ouyang et al. 2022; Bai et al. 2022) and other preference alignment algorithms (Tian et al. 2023; Yang et al. 2024). For instance, (Tian et al. 2023) found that the Direct Preference Optimization (DPO) (Rafailov et al. 2024) assists the model in aligning facticity from ranking. The latter emphasizes activating attention heads or latent features into the truthful space through causal effects (Li et al. 2024; Chen et al. 2024; Burns et al. 2022; Zhang, Yu, and Feng 2024). For example, Truth Forest (Chen et al. 2024) utilized multidimensional orthogonal probing to mine hidden true representations to improve model truthfulness. Despite advances, current methods potentially undermine the inherent knowledge system of LLMs, causing performance bottlenecks on out-of-attention tasks. In contrast, our framework guides out-of-the-box target LLMs during inference to enhance prediction factuality by mitigating hallucinations while improving truthfulness.

Decoding-Time Intervention. Unlike secondary training for target LLMs, Decoding-time intervention was used earlier to enhance model fluency and coherence during inference by comparing expert models (Li et al. 2022). Previous studies found that controlling the output behavior at the decoding level contributed to avoiding toxic generated content (Liu et al. 2021) and improving logical reasoning (Wang and Zhou 2024). These findings offer the prospect of refining response factuality through rational comparator formalization (Zhang et al. 2023a; Shi et al. 2023; Chuang et al. 2023; Kai et al. 2024). For instance, ICD (Zhang et al. 2023a) finetuned a factually weak base model during induce-the-contrast decoding as a penalty to guide LLMs more faithfully. Nevertheless, these approaches usually fail to recognize multifaceted hallucination patterns in complex scenarios, leading to limitations and sensitivities in task types. In comparison, the proposed CDT demonstrates adaptability and scalability over distinct tasks by hallucination/truthfulness-aware comparators.

Where isQtuhestciaopni-tanlsowfePriengnsTyals-k (a) PBHiaserrmriesarbcukrg (b) TCroutmhpfaurlator CHoallmupcianratory CMomdeplasrator   
vania State? Honolulu 介← - Hallucinatory pθh(y |x, y< ) × γ Factual Responses Hallucinated Responses Dialogue Task   
[UAUseserir]s]:tIaWjnuhstot]ew:lsaOetfcshtcaeordusraisnefi,lNdmio.g..h.t....... QuestioMn ultDi-iaTloasgkueInsSturmumctairiozan IonputGseneral LoRLAoaRALoRLAocRA FFN RGoautiting   
[Document]:STuhemmanr,izianthiiosn2T0.a..sk TLarLgMet pθ(y |x, y< ) GMM Clustering Nearest Add & Norm Prototype Emb.   
[User]: Please summarize the giv- × β Multi-Head Attention   
en documeGnet.neral Instruction Task ? 一 FaPcatirusal HalluPcaiirnsated Target   
Convert the following equation Truthful $p _ { \theta _ { f } } ( y _ { t } | x , y _ { < t } )$ Instruction Prototype LLM A O   
into a word problem. $2 x + 3 = 7$ Comparator Embeddings K Prototype Guidance LoRA-MoE SFT

# Methodology Comparator-driven Decoding-Time Framework

Figure 2(a) shows the high-level illustration of our CDT framework. Given a target language model parameterized by $\theta$ , the model generates a corresponding response $\textbf {  { y } }$ based on a query instruction $\scriptstyle { \mathbf { { \vec { x } } } }$ containing contextual information. The response is typically sampled from the probability distribution conditioned on $\scriptstyle { \mathbf { { \vec { x } } } }$ . The autoregressive decoding process can be formulated as follows:

$$
y _ { t } \sim p _ { \theta } ( y _ { t } \mid x , y _ { < t } ) \propto \exp \log \mathrm { i t } _ { \theta } ( y _ { t } \mid x , y _ { < t } ) ,
$$

where $y _ { t }$ denotes the predicted token at time step $t$ . $\scriptstyle { \boldsymbol { y } } _ { < t }$ contains the previously generated tokens across time step $t - 1$ . With the maximum-likelihood-based training objective, the original decoding suffers from unavoidable hallucinations due to the model spontaneously assigning non-zero probabilities to unfaithful content (Chuang et al. 2023). From Figure 2(a), the model potentially relies on spurious correlations captured from the training data distribution to give false predictions. To mitigate such issues, CDT aims to manipulate the next-token predictions of the target LLM into factualityrobust distributions by combining a hallucinatory comparator $\theta _ { h }$ , which generates text with non-factual attributes, and a truthful comparator $\theta _ { f }$ , which generates text with factual attributes. These comparators come from fine-tuning the base version of the target LLM via explicit hallucinated/factual samples. The proposed CDT decoding is formulated as:

$$
\begin{array} { r l } & { y _ { t } \sim p _ { c d t } ( y _ { t } \mid \pmb { x } , \pmb { y } _ { < t } ) } \\ & { ~ \propto p _ { \theta } ( y _ { t } \mid \pmb { x } , \pmb { y } _ { < t } ) \frac { p _ { \theta _ { f } } ( y _ { t } \mid \pmb { x } , \pmb { y } _ { < t } ) ^ { \beta } } { p _ { \theta _ { h } } ( y _ { t } \mid \pmb { x } , \pmb { y } _ { < t } ) ^ { \gamma } } , } \end{array}
$$

where $\beta$ and $\gamma$ are coefficients that control the degree of distribution modification. Intuitively, the generated token will have a high probability when it has a high probability under $p _ { \theta _ { f } }$ and a low probability under $p _ { \theta _ { h } }$ , facilitating factual prediction amplification and untruthful prediction trivialization.

In practice, we note that the output of the hallucinatory comparator is not always erroneous since it can still maintain basic linguistic common sense and grammar (Li et al. 2022). This means that indiscriminate trivialization potentially penalizes valuable aspects, leading to implausible tokens. To address this issue, we introduce an adaptive plausibility constraint to penalize a subset $\nu _ { s u b }$ of the output vocabulary $\nu$ that correlates with the confidence level of the output distribution of the target LLMs:

$$
\begin{array} { r l } & { \mathcal { V } _ { s u b } ( { \pmb y } _ { < t } ) = \{ y _ { t } \in \mathcal { V } : } \\ & { p _ { \theta } ( y _ { t } \mid { \pmb x } , { \pmb y } _ { < t } ) \geq \delta \underset { w } { \operatorname* { m a x } } p _ { \theta } ( w \mid { \pmb x } , { \pmb y } _ { < t } ) \} , } \\ & { p _ { c d t } ( y _ { t } \mid { \pmb x } , { \pmb y } _ { < t } ) = 0 , \mathrm { i f } \ y _ { t } \not \in \mathcal { V } _ { s u b } ( { \pmb y } _ { < t } ) , } \end{array}
$$

where $\delta \in [ 0 , 1 ]$ is a coefficient that controls the distribution truncation, with larger values indicating that only highprobability tokens are retained. The constraint ensures the response integrity of our framework while excluding the generation of untrustworthy tokens.

# Hallucinatory/Truthful Comparator Formalization

To build behavior-controlled comparators, we activate hallucinatory/faithful mastery of the base models by the LoRAbased supervised fine-tuning (SFT) (Hu et al. 2021). The SFT dataset $\mathcal { D }$ contains multi-task instructions to empower the comparators to adapt the target LLM to improve factuality in different downstream tasks, including questionanswering, knowledge-grounded dialogue, text summarization, and general tasks. Specifically, given any input instructnhaete ${ \pmb x } = ( x _ { 1 } , x _ { 2 } , x _ { 3 } , \ldots ) \in \mathcal { D }$ $\pmb { y } ^ { h / f } = ( y _ { 1 } ^ { h / f } , y _ { 2 } ^ { h / f } , y _ { 3 } ^ { h / f } , \dots ) \in \mathscr { D }$ lowing fine-tuning objective:

$$
\begin{array} { r } { \mathcal { L } ( \boldsymbol { \theta } , \mathcal { D } ) = \mathbb { E } _ { ( \boldsymbol { x } , \boldsymbol { y } ^ { h } ) \sim \mathcal { D } } \left[ - \sum _ { t = 1 } ^ { \left| \boldsymbol { y } ^ { h } \right| } \log p ( y _ { t } ^ { h } \mid \boldsymbol { x } , \boldsymbol { y } _ { < t } ^ { h } ; \boldsymbol { \theta } ) \right] . } \end{array}
$$

Although the truthful comparator can be optimized with the same objective, we find that directly performing SFT via instruction pairs $( \pmb { x } , \pmb { y } ^ { f } )$ results in overfitting, leading to the mundane faithfulness of the generated content. Inspired by robustness learning (Miyato, Dai, and Goodfellow 2022), we introduce a hallucination perturbation adversarial training mechanism to improve the training efficiency of the truthful comparator. Specifically, we first backpropagate using the hallucinated responses to obtain the word embedding layer gradients. Then, we compute an adversarial perturbation based on the gradient and merge it into the original embedding to obtain the hallucination-aware adversarial samples. The perturbation is expressed as follows:

$$
\begin{array} { r } { \mathcal { P } = \boldsymbol { \varepsilon } \cdot \nabla \boldsymbol { x } \mathcal { L } ( \boldsymbol { \theta } , \boldsymbol { x } , \boldsymbol { y } ^ { h } ) / \left. \nabla \boldsymbol { x } \mathcal { L } ( \boldsymbol { \theta } , \boldsymbol { x } , \boldsymbol { y } ^ { h } ) \right. _ { 2 } , } \end{array}
$$

where $\varepsilon$ and $\mathcal { L } ( \cdot )$ represent the sign and loss functions, respectively. The adversarial samples are utilized to backpropagate secondarily through the factual responses to obtain the cumulative adversarial gradients for model parameter updating. Our strategy forces the model to progressively penalize hallucinated attributes from the perturbation during SFT, promoting more effective and factual semantic learning of the truthful comparator.

# Instruction Prototype-guided Mixture of Experts

In the LoRA-based SFT phase of Figure 2(b), we present an instruction prototype-guided mixture of experts strategy to enhance the hallucination/truthfulness-aware capability of comparators. Considering that instructions are the foundation for serving target LLMs to generate informative responses, the core philosophy of our strategy lies in learning prototype representations of multi-task instruction data with different probability density distributions via Gaussian Mixture Models (Reynolds et al. 2009). Depending on the corresponding hallucinated or faithful response outputs, the prototype representations can guide multiple LoRA adapters acting as experts to master different hallucination or truthfulness patterns in supervised optimization, facilitating hallucinatory and truthful comparators to assist the target LLMs in generating factual content among diverse tasks during decoding time. Specifically, given the multi-task instruction set $\{ { \pmb x } _ { i } \} _ { i = 1 } ^ { M }$ from the SFT dataset $\mathcal { D }$ , the prototype representation acquisition is denoted as follows:

$$
\mathcal { C } _ { [ \pmb { x } ] } = \frac { \sum _ { i = 1 } ^ { M } \gamma _ { i , k } \pmb { h } _ { x , i } } { \sum _ { i = 1 } ^ { M } \gamma _ { i , k } } ,
$$

where $\boldsymbol { h _ { x , i } }$ is the instruction feature corresponding to $\pmb { x } _ { i }$ , which is extracted from the hidden state of the last layer in the target LLM and compressed and refined by principal component analysis for efficient computation. $\gamma _ { i , k }$ is the probability that each $\boldsymbol { h _ { x , i } }$ belongs to the $k$ -th Gaussian component, which is formulated as follows:

$$
\gamma _ { i , k } = \frac { \pi _ { k } \mathcal { N } ( h _ { x , i } \mid \mu _ { k } , \sum _ { k } ) } { \sum _ { j = 1 } ^ { K } \pi _ { j } \mathcal { N } ( h _ { x , i } \mid \mu _ { j } , \sum _ { j } ) } ,
$$

where $\pi _ { k } , \mu _ { k }$ , and $\textstyle \sum _ { k }$ are the mixture weight, the mean vector, and the covariPance matrix of the $k$ -th component, respectively. $\mathcal { N } ( \cdot )$ denotes the probability density.

After that, the learned instruction prototypes are incorporated into the routing gating $G ( \cdot )$ to activate the appropri

ate experts to capture specific hallucination/truthfulness patterns in any input $\scriptstyle { \mathbf { { \vec { x } } } }$ . The gating vector is formulated as:

$$
G ( \pmb { x } ) = \mathcal { S } \left( \left( ( 1 - \mu ) \odot \pmb { x } + \mu \odot \mathcal { C } _ { [ \pmb { x } ] } \pmb { W } _ { c } \right) \pmb { W } _ { g } + \Re ( \varphi ( \pmb { x } \pmb { W } _ { n } ) ) \right) ,
$$

where $ { \boldsymbol { S } } ( \cdot )$ and $\mu$ stand for the Softmax activation and tradeoff coefficient, respectively. In practice, we determine the index $k _ { x } \gets \arg \operatorname* { i n i n } _ { k } \dot { d _ { \mathcal { M } } } ( \pmb { x } , \bar { \mathcal { C } } _ { [ \pmb { x } ] } )$ of the prototype $\mathcal { C } _ { [ { \pmb x } ] }$ to which $\scriptstyle { \mathbf { { \vec { x } } } }$ belongs via the Euclidean distance (Danielsson 1980). The distance is suitable for handling highdimensional representations from multi-task SFT data with complex distributions since it is computationally efficient. $\Re ( \varphi ( { \boldsymbol { \alpha } } W _ { n } ) )$ is an additional noise term designed to regularize the load balancing among experts during the training process, where $\Re ( \cdot )$ and $\varphi ( \cdot )$ represent the standard normal distribution sampling and Softplus function, respectively. $W _ { \mathcal { C } }$ , $W _ { g }$ and $\textstyle { W _ { n } }$ are learnable parameters. The forward process of obtaining the parameterized output $z$ of all experts by integrating the soft route gating is expressed as follows:

$$
z = \frac { \alpha } { r } { \sum _ { n = 1 } ^ { N } } G ( \pmb { x } ) _ { n } E _ { n } ( \pmb { x } ) ,
$$

where $r$ is the rank value and $\alpha$ is a constant hyperparameter. $E _ { n } ( \cdot )$ represents the $n$ -th LoRA expert.

# Experiments

# Datasets and Evaluation Metrics

We construct extensive experiments on six datasets to evaluate different methods across multiple tasks. Specifically, KINGHT-Judge is proposed to verify the model’s ability to recognize hallucinations in the knowledge-driven dialog task. The dataset contains the dialog contexts and factual responses from 616 samples sampled from the testing set of OpenDialKG (Moon et al. 2019), as well as hallucinated responses generated by GPT-4 (Achiam et al. 2023). During evaluations, the models are asked to classify factual/hallucinatory responses randomly sampled from each sample based on contexts to specify whether or not they contain hallucination information. Similarly, we select 600 responses from HaluEval (Li et al. 2023a) that are halfannotated as normal and hallucinatory against general user queries from Alpaca (Taori et al. 2023). These instructions constitute Alpaca-Judge to evaluate performance on general tasks. The evaluation metrics consist of Accuracy, Precision, Recall, and F1 Score. TruthfulQA (Lin, Hilton, and Evans 2022) contains 817 questions across 38 categories to assess response truthfulness. We employ the multiplechoice task and measure the accuracy of LLMs in selecting answers from multiple correct/incorrect options via MC1, MC2, and MC3 metrics. For text summarization, we extract 1,000 samples with harmless and safe content from vanilla XSUM (Narayan, Cohen, and Lapata 2018) to ensure the models perform rational summarization with given documents. We use the ROUGE-1/2/L (Lin 2004) to measure the generation quality. FactKB (Feng et al. 2023) and BERTScore (Zhang et al. 2019) are utilized to evaluate the factual consistency of responses. For open-ended generation tasks, we use factual responses in KNIGHT-Judge as

Table 1: Comparison results on the KNIGHT-Judge and Alpaca-Judge datasets.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="4">KNIGHT-Judge</td><td colspan="4">Alpaca-Judge</td></tr><tr><td>Accuracy (%)</td><td>Precision (%)</td><td>Recall (%)</td><td>F1 Score (%)</td><td>Accuracy (%)</td><td>Precision (%)</td><td>Recall (%)</td><td>F1 Score (%)</td></tr><tr><td>Baseline</td><td>45.13</td><td>43.90</td><td>62.50</td><td>51.58</td><td>43.68</td><td>45.05</td><td>81.33</td><td>59.27</td></tr><tr><td>SFT</td><td>43.18</td><td>39.59</td><td>58.21</td><td>47.13</td><td>41.58</td><td>43.20</td><td>76.95</td><td>57.66</td></tr><tr><td>FPO</td><td>44.42</td><td>41.67</td><td>61.04</td><td>49.53</td><td>43.72</td><td>45.46</td><td>82.63</td><td>59.14</td></tr><tr><td>CD (Li et al. 2022)</td><td>46.10</td><td>46.53</td><td>94.24</td><td>61.92</td><td>44.52</td><td>46.13</td><td>84.06</td><td>57.35</td></tr><tr><td>ITI (Li et al.2024)</td><td>45.59</td><td>46.13</td><td>67.23</td><td>53.44</td><td>44.25</td><td>45.66</td><td>78.30</td><td>57.93</td></tr><tr><td>SH2 (Kai et al.2024)</td><td>46.76</td><td>42.00</td><td>63.64</td><td>50.61</td><td>44.33</td><td>45.85</td><td>79.45</td><td>58.14</td></tr><tr><td>DoLa (Chuang et al.2023)</td><td>46.62</td><td>41.81</td><td>65.79</td><td>52.25</td><td>44.67</td><td>46.27</td><td>84.94</td><td>58.91</td></tr><tr><td>ICD (Zhang et al. 2023a)</td><td>47.24</td><td>47.76</td><td>90.32</td><td>63.93</td><td>45.00</td><td>47.38</td><td>85.33</td><td>60.15</td></tr><tr><td>CDT(Ours)</td><td>50.16</td><td>50.60</td><td>93.95</td><td>65.77</td><td>46.83</td><td>48.26</td><td>88.00</td><td>62.34</td></tr></table></body></html>

Table 2: Comparison results on the TruthfulQA dataset.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="3">TruthfulQA</td></tr><tr><td>MC1 (%)</td><td>MC2 (%)</td><td>MC3 (%)</td></tr><tr><td>Baseline</td><td>37.62</td><td>54.60</td><td>28.12</td></tr><tr><td>SFT</td><td>35.02</td><td>52.82</td><td>27.40</td></tr><tr><td>FPO</td><td>37.98</td><td>55.34</td><td>29.65</td></tr><tr><td>CD (Li et al. 2022)</td><td>28.15</td><td>54.87</td><td>29.75</td></tr><tr><td>ITI (Li et al. 2024)</td><td>37.01</td><td>54.66</td><td>27.82</td></tr><tr><td>SH2 (Li et al. 2024)</td><td>33.90</td><td>57.07</td><td>29.79</td></tr><tr><td>DoLa (Chuang et al.2023)</td><td>32.97</td><td>60.84</td><td>29.50</td></tr><tr><td>ICD (Zhang et al. 2023a)</td><td>46.32</td><td>69.08</td><td>41.25</td></tr><tr><td>CDT(Ours)</td><td>50.74</td><td>75.96</td><td>50.97</td></tr></table></body></html>

Ground Truths (GTs) to construct KNIGHT-Gen to measure the generated content for given instructions. Moreover, Alpaca-Gen extracts 600 samples with normal responses as GTs to explore general generation following Alpaca-Judge. The FactKB, BERTScore, and DAE (Goyal and Durrett 2020) are used to assess the predicted factuality.

# Implementation Details

The main experiments are built on the LLaMA-2 family in order to provide a fair comparison with existing methods. The model training is accomplished through the PyTorch platform using eight Nvidia A800 GPUs. To fine-tune the LLaMA2-base-driven comparators, the multi-task SFT dataset $\mathcal { D }$ is collected from HaluEval with 30K instructions having factual and hallucinated responses across NLP tasks. We also mix 10K held-out samples sampled from Alpaca and annotate the corresponding hallucinated responses. The default number of instruction prototypes and LoRA experts are 32 and 4, respectively.

# Comparison with State-of-the-Art Methods

Model Zoo. We compare our CDT and reproducible methods. Baseline: LLaMA2-7b-Chat model (Touvron et al. 2023). SFT: The model is fine-tuned by factual multi-task instructions in $\mathcal { D }$ . FPO: The model is optimized by DPO utilizing preference pairs from factual/hallucinated responses in $\mathcal { D }$ . Recent state-of-the-art (SOTA) works include CD (Li et al. 2022), ITI (Li et al. 2024), SH2 (Kai et al. 2024), DoLa (Chuang et al. 2023), and ICD (Zhang et al. 2023a). Results on the KINGHT-Judge&Alpaca-Judge. Table 1 provides comparison results on the two datasets for hallucination recognition. We find that CDT achieves the best results on the vast majority of metrics, suggesting accurate judgments and understanding against potential hallucinations in the text. For instance, CDT brings $1 4 . 1 9 \%$ and $3 . 0 7 \%$ gains in terms of F1 scores on KINGHT-Judge and Alpaca-Judge for the baseline. Conversely, the poor performance of SFT and FPO indicates that secondary training induces the models to learn spurious behavior cloning (Schulman 2023) rather than factual patterns. Our framework significantly outperforms prior SOTA ICD on KINGHT-Judge by $2 . 8 1 \%$ average improvement, implying effective recognition of different extrinsic hallucinations.

Table 3: Comparison results on the XSUM dataset. “R-1/2/L” means the ROUGE-1/2/L metrics.  

<html><body><table><tr><td>Methods</td><td>R-1</td><td>R-2</td><td>R-L</td><td></td><td>FactKB BERTScore</td></tr><tr><td>Baseline</td><td>17.22 3.55</td><td></td><td>13.29</td><td>38.79</td><td>82.50</td></tr><tr><td>SFT</td><td>17.58 3.6113.75</td><td></td><td></td><td>41.85</td><td>81.03</td></tr><tr><td>FPO</td><td>18.09 3.47 14.18</td><td></td><td></td><td>43.66</td><td>82.75</td></tr><tr><td>CD (Li et al. 2022)</td><td>19.553.7714.61</td><td></td><td></td><td>67.30</td><td>83.31</td></tr><tr><td>ITI (Li et al. 2024)</td><td>17.94 3.68 15.45</td><td></td><td></td><td>57.94</td><td>82.87</td></tr><tr><td>SH2 (Kai et al. 2024)</td><td>19.753.8314.73</td><td></td><td></td><td>71.22</td><td>83.92</td></tr><tr><td>DoLa (Chuang etal.2023)</td><td>18.60 3.7314.58</td><td></td><td></td><td>65.13</td><td>84.20</td></tr><tr><td>ICD (Zhang et al. 2023a)</td><td>20.29 4.11 16.85</td><td></td><td></td><td>70.26</td><td>84.06</td></tr><tr><td>CDT (Ours)</td><td>21.11 4.5216.33</td><td></td><td></td><td>73.66</td><td>86.44</td></tr></table></body></html>

Results on the TruthfulQA. As shown in Table 2, our framework outperforms the decoding time methods ITI and ICD in all metrics on TruthfulQA. This implies that penalizing the coupled hallucinations only during decoding without considering the factual attributes that enhance the target LLMs is sub-optimal and inadequate. Compared to ICD, the $9 . 5 \% / 9 . 9 \% / 2 3 . 6 \%$ relative gains regarding MC1/MC2/MC3 on multi-category questions proves that CDT removes potential comprehension and factuality hallucinations (Zheng, Huang, and Chang 2023) in the question-answering task by giving correct answers with higher probabilities through the instruction prototype-guided mixture of experts strategy.

Results on the XSUM. Table 3 demonstrates the text summarization capabilities of different models. According to the ROUGE metric, we find that the proposed framework generates content that covers the critical parts of the reference text more reasonably. CDT assists the target baseline in improving $34 . 8 7 \%$ and $3 . 9 4 \%$ on FactKB and BERTScore, effectively enhancing response factuality and quality. In contrast, the improvement of the representation editing technique ITI is incremental due to enhancing truthfulness by intervening in the attention heads, which causes uninformative output and performance bottlenecks.

Results on the KINGHT-Gen&Alpaca-Gen. We report the results of open-ended generation in Table 4. CDT improves response faithfulness on knowledge-grounded dialog and general-purpose tasks, consistently achieving the best results among three fact metrics. Compared to other contrastive decoding methods, CDT has two advantages. First, unlike CD and DoLa, which emphasize probability differences between strong and weak models/layers, CDT enhances factuality in a more integrated way by enabling precise control of factuality-robust distributions of next-token predictions through comparators with different properties. Also, our framework excels at capturing multiple hallucination types across tasks, leading to more effective hallucination mitigation during decoding time. The $6 . 3 9 \%$ average gain across the fact metrics for the baseline confirms this.

Table 4: Comparison results on the KNIGHT-Gen and Alpaca-Gen datasets.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="4">KNIGHT-Gen</td><td colspan="4">Alpaca-Gen</td></tr><tr><td>ROUGE-L</td><td>BERTScore</td><td>FactKB</td><td>DAE</td><td>ROUGE-L</td><td>BERTScore</td><td>FactKB</td><td>DAE</td></tr><tr><td>Baseline</td><td>9.85</td><td>81.33</td><td>70.54</td><td>40.19</td><td>19.67</td><td>81.25</td><td>76.43</td><td>50.62</td></tr><tr><td>SFT</td><td>9.92</td><td>81.14</td><td>71.43</td><td>39.76</td><td>20.54</td><td>81.23</td><td>77.25</td><td>50.11</td></tr><tr><td>FPO</td><td>9.27</td><td>81.77</td><td>71.66</td><td>41.05</td><td>20.36</td><td>82.69</td><td>77.93</td><td>51.53</td></tr><tr><td>CD (Li et al. 2022)</td><td>10.80</td><td>82.35</td><td>72.27</td><td>42.83</td><td>22.42</td><td>82.07</td><td>78.20</td><td>52.00</td></tr><tr><td>ITI (Li et al. 2024)</td><td>11.26</td><td>82.59</td><td>74.43</td><td>42.46</td><td>23.56</td><td>83.31</td><td>80.17</td><td>52.29</td></tr><tr><td>SH2 (Kai et al. 2024)</td><td>11.13</td><td>83.57</td><td>72.68</td><td>44.10</td><td>24.35</td><td>83.99</td><td>83.02</td><td>55.43</td></tr><tr><td>DoLa (Chuang et al.2023)</td><td>12.05</td><td>83.14</td><td>73.56</td><td>44.08</td><td>23.20</td><td>83.25</td><td>79.94</td><td>53.75</td></tr><tr><td>ICD (Zhang et al. 2023a)</td><td>11.53</td><td>84.03</td><td>75.32</td><td>45.02</td><td>23.88</td><td>84.87</td><td>83.52</td><td>55.16</td></tr><tr><td>CDT (Ours)</td><td>12.16</td><td>85.47</td><td>77.42</td><td>47.39</td><td>23.90</td><td>86.06</td><td>85.82</td><td>56.50</td></tr></table></body></html>

Table 5: Ablation study results on the different datasets. “HC” means the hallucinatory comparator. “TC” means the truthful comparator. $" + "$ is the replacement operation. “Summ.” means the summarization. “w/o” is the without.   

<html><body><table><tr><td rowspan="2">Design</td><td>KNIGHT- Judge</td><td>TruthfulQA</td><td>XSUM</td><td>Alpaca- Gen</td></tr><tr><td>F1 Score</td><td>MC1</td><td>BERTScore</td><td>DAE</td></tr><tr><td>FullFramework</td><td>65.77</td><td>50.74</td><td>86.44</td><td>56.50</td></tr><tr><td colspan="5">Importance of Comparators</td></tr><tr><td>w/o HC</td><td>62.05</td><td>39.53</td><td>84.97</td><td>52.12</td></tr><tr><td>+ Chat Version</td><td>64.03</td><td>48.36</td><td>85.15</td><td>55.74</td></tr><tr><td>w/o TC</td><td>54.61</td><td>48.38</td><td>83.22</td><td>55.09</td></tr><tr><td>+ Chat Version</td><td>65.75</td><td>50.67</td><td>86.69</td><td>56.58</td></tr><tr><td colspan="5">Effectiveness of PME</td></tr><tr><td>w/o PME</td><td>62.94</td><td>48.82</td><td>83.36</td><td>54.75</td></tr><tr><td>w/o Prototypes</td><td>64.06</td><td>49.14</td><td>85.09</td><td>55.43</td></tr><tr><td colspan="5">Necessity of PAT</td></tr><tr><td>W/o PAT</td><td>64.65</td><td>49.57</td><td>85.64</td><td>55.96</td></tr><tr><td>+PGD</td><td>65.18</td><td>49.83</td><td>85.91</td><td>56.20</td></tr><tr><td colspan="5">Impactof SFTDataProportion</td></tr><tr><td>w/o QAData</td><td>64.40</td><td>48.12</td><td>85.13</td><td>55.08</td></tr><tr><td>w/o Dialogue Data</td><td>62.53</td><td>48.93</td><td>85.38</td><td>54.71</td></tr><tr><td>W/o Summ. Data</td><td>64.05</td><td>49.27</td><td>84.45</td><td>54.66</td></tr><tr><td>w/o General Data</td><td>65.19</td><td>50.30</td><td>85.86</td><td>54.24</td></tr></table></body></html>

# Ablation Studies

In Table 5, we perform systematic ablation studies to investigate the rationality of different designs in CDT.

Importance of Different Comparators. We observe that the two comparators provide complementary contributions as performance deteriorates across different datasets when they are removed. Interestingly, the hallucinatory comparator excels in improving the discriminative and open-ended generation tasks, while the truthful comparator helps the target LLM to be more truthful in recognizing hallucinations and summarization demands. When replacing the base model with the chat version, the truthful comparator has a minor improvement while it is ineffective for the hallucinatory comparator. This makes sense because the chat model undergoes RLHF with honesty as a key attribute, narrowing the gains from CDT contrasting logit differences.

![](images/c5703da03ade5d2be6020efcf67d1a1d05a2b7e3651a11e9ae952702ab77e5f5.jpg)  
Figure 3: We show the effect of the number of experts on the performance of different tasks.

Effectiveness of Instruction Prototype-guided Mixture of Experts (PME). A single LoRA is used for the SFT process when there is no PME strategy in the comparators. The overall gain drops, which is more significant for the dialogue and summarization tasks. A plausible explanation is that PME facilitates the model to better capture sophisticated hallucinations in scenarios that require contextual understanding. Moreover, instruction prototypes can guide LoRA experts to learn different hallucination/truthfulness patterns, bringing consistent gains across multiple tasks.

Necessity of Perturbation Adversarial Training (PAT). We observe that the PAT mechanism plays an essential role because it prevents potential truthfulness learning trivialization and overfitting dilemmas in the truthful comparator training. The intuitive justification is reflected in consistent gains across different datasets. Additionally, a PGD-based adversarial candidate (Madry et al. 2017) is introduced to explore performance changes. The sub-optimal results from

70 wseline 43.38 37.6250.74 37.75 52.43 37.70 53.13 38.6451.56 38.7153.84 47.88 39.0954.69 34.93 30 29.16 M20 10 0 LLaMA2-7B LLaMA2-7B-Chat LLaMA2-13B-Chat LLaMA2-70B-ChatLLaMA3-8B-Instruct LLaMA3-70B-lnstruct Baichuan2-7B-ChatMistral-7B-Instruct

![](images/4e31c7b7623612f9bb2e44314f0d7c7ed4e476c39d02bd607895d58a4cb94a6a.jpg)  
Figure 4: The extensibility analysis of our framework across different LLMs through the multiple-choice task on TruthfulQA   
Figure 5: We show the effect of the number of instruction prototypes on the performance of different tasks.

the candidate further verify the necessity of our mechanism.

Impact of SFT Data Proportion. Different data for training the comparators are investigated by removing instruction pairs from each of the four parts. We discover that each type of SFT data is beneficial in reinforcing the benefits of the proposed framework. This trend suggests that sufficient multi-task data can effectively activate the comparators’ ability to model different generation attributes.

Effect of Expert Number. The number of LoRA experts determines the comparators’ knowledge capacity to adapt to different tasks. In Figure 3, when only 2 experts are used, the model gives poorer results on XSUM and Alpaca-Gen. We conjecture that generative tasks typically require increased expert capacity to capture multifaceted hallucinations embedded in contextual information. The 4 experts achieve a trade-off between performance and training overhead. Excessively introducing experts can cause gain convergence.

Effect of Instruction Prototype Number. Instruction prototypes guide the semantically relevant routing process to balance LoRA expert utilization. As shown in Figure 5, fewer prototypes (i.e., 8 and 16) perform poorly on the evaluation tasks. This stems from the inability of experts to master sufficient hallucination/truthfulness patterns, leading to performance bottlenecks. There is a considerable performance improvement on different tasks when we increase the number of prototypes, where 32 is the most appropriate. Conversely, too many prototypes are redundant as they exceed the pattern distribution boundaries (Gou et al. 2023).

Table 6: Qualitative analysis regarding the generated responses of different models to the same prompt. Underlined, italic and bold texts represent faithful, factuality, and inference hallucinations, respectively.   

<html><body><table><tr><td>Methods</td><td>[Prompt]: How long do diamonds last?</td></tr><tr><td>Baseline</td><td>Graphite generally lasts between 10 and15million yearsbefore itbegins to weather and eventually decompose.</td></tr><tr><td>ICD</td><td>As the fossilized tree sap,diamonds last between 5 and 8 billion years.</td></tr><tr><td>CDT (Ours)</td><td>Typically, diamonds last between1 and 4 billion years.</td></tr></table></body></html>

# Extensibility Analysis

As a model-agnostic framework, we apply the CDT to 8 advanced LLMs spanning different sizes of the same family as well as different families of models. Figure 4 illustrates the results of the multiple-choice task on the hallucination perception dataset TruthfulQA. In the LLaMA families, we find an upward trend in the benefits from CDT with increasing model version and size. The average improvement across the 6 vanilla baselines reaches $1 4 . 2 5 \%$ . Moreover, CDT provides absolute gains of $1 2 . 9 5 \%$ and $1 5 . 6 \%$ for Baichuan2-7B-Chat (Yang et al. 2023) and Mistral-7B-Instruct (Jiang et al. 2023), respectively, significantly enhancing the corresponding factuality.

# Qualitative Analysis

In Table 6, we show the comparison results of generated contents for the same prompt by different models on the question-answering task. Despite improvements in ICD, the hallucinations remained. The claim that diamonds are inferred to be “fossilized tree sap” reflects the inference hallucination. “5 and 8 billion years” that conflict with world knowledge reflects the factuality hallucination. In contrast, CDT effectively corrects the multifaceted misinformation, thereby demonstrating the effectiveness of our framework.

# Conclusion

In this paper, we introduce CDT, a decoding-time framework to efficiently remove the multifaceted hallucination dilemma of target LLMs via hallucinatory and truthful Comparators. CDT facilitates more factual model responses in multiple downstream tasks by controlling the next-token prediction.