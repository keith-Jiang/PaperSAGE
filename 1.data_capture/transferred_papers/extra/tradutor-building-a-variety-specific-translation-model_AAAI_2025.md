# Tradutor: Building a Variety Specific Translation Model

Hugo Sousa1,2\*, Satya Almasian3\*, Ricardo Campos2,4,5, Al´ıpio Jorge1,2

1Faculty of Sciences, University of Porto, Porto, Portugal 2INESC TEC, Porto, Portugal 3Institute of Computer Science, Heidelberg University, Germany 4Department of Informatics, University of Beira Interior, Covilha˜, Portugal 5Ci2 - Smart Cities Research Center, Tomar, Portugal hugo.o.sousa@inesctec.pt, almasian $@$ informatik.uni-heidelberg.de

# Abstract

Language models have become foundational to many widely used systems. However, these seemingly advantageous models are double-edged swords. While they excel in tasks related to resource-rich languages like English, they often lose the fine nuances of language forms, dialects, and varieties that are inherent to languages spoken in multiple regions of the world. Languages like European Portuguese are neglected in favor of their more popular counterpart, Brazilian Portuguese, leading to suboptimal performance in various linguistic tasks. To address this gap, we introduce the first open-source translation model specifically tailored for European Portuguese, along with a novel dataset specifically designed for this task. Results from automatic evaluations on two benchmark datasets demonstrate that our best model surpasses existing opensource translation systems for Portuguese and approaches the performance of industry-leading closed-source systems for European Portuguese. By making our dataset, models, and code publicly available, we aim to support and encourage further research, fostering advancements in the representation of underrepresented language varieties.

# 1 Introduction

In an era, where Language Models (LMs) form the foundation of numerous tools and systems, a significant concern arises: how can we ensure these tools are equally beneficial to all communities? One major hurdle in this direction is inclusiveness, particularly when it comes to scarce language varieties and the lack of models that take into account the diversity of dialects and culturally significant nuances of a given language (Team 2022). For example, while Portuguese is not considered a low-resource language, most of the content available is in Brazilian Portuguese. This is due to the Brazilian population representing more than $70 \%$ of all native Portuguese speakers1. Consequently, a LM or a translation system trained on a Portuguese corpus is inclined to produce text containing phrases and grammatical structures of Brazilian Portuguese. As a result, countries like Portugal and Mozambique, which use different varieties of Portuguese, could find themselves at a disadvantage, particularly in deploying LM-based systems in critical areas such as healthcare and judiciary, where the grammar and lexicons of the language are of great importance (Scherre and Duarte 2016; Kato and Martins 2016; Brito and Lopes 2016).

One solution would be to create LMs specific to a certain language variety. However, this presents its own set of challenges for training and evaluation (Armengol-Estape´ et al. 2021; Rodrigues et al. 2023). Training a LM, be it from scratch or fine-tuning, requires a large, carefully curated corpus for training and several benchmarks for evaluation (Albalak et al. 2024), which are either nonexistent or contaminated by the dominant language varieties. Another way to overcome these challenges is to create machine translation (MT) models dedicated to a specific language variety (Zbib et al. 2012; Sennrich, Haddow, and Birch 2016; Riley et al. 2023). With a sufficiently powerful MT model for the lowresource variety, we can take the first step toward resource creation and inclusion of these languages. Such models can be used to translate training and evaluation benchmarks, which are predominantly in English, to the low-resource variety. Additionally, they can serve as an off-the-shelf intermediary between widely-used LMs and systems in lowresource settings or even be used to artificially generate data in the desired language variety.

In this paper, we present a novel methodology for developing a neural machine translation (NMT) model tailored to low-resource language varieties, where manually annotated data is scarce or unavailable. Our approach leverages a retro-translation technique: we first gather texts in the lowresource language variety and translate them into a resourcerich language. This newly created parallel corpus is then used to fine-tune a pre-trained language model. We validate our methodology using European Portuguese as a case study, though the same steps can be applied to other languages with similar challenges. As part of this work, we have created and publicly released a meticulously curated parallel corpus for European Portuguese, comprising 1,719,002 documents – the largest of its kind to date. We evaluate our model against existing open and close source translation systems for Portuguese, as well as zero-shot language models, demonstrating the effectiveness of our approach. Finally, to ensure our translations remain true to the intended language variety, we use a language variety classification model to quantify if the texts produced by our translation model are effectively European Portuguese.

In summary, our contributions are the following:

1. We propose a methodology to create a parallel corpus for a low-resource language variety using an on-the-shelf translation model. To this end, we provide the community with the largest translation dataset for European Portuguese and English, named PTradutor.   
2. We provide the first open-source translation models from English to European Portuguese which outperform the generic Portuguese open-source translation systems and close the gap to state-of-the-art close-source translations systems for European Portuguese.   
3. We offer a comprehensive evaluation of our models, emphasizing not only translation quality but also linguistic alignment to the desired language variety.

The remainder of this manuscript is organized as follows: Section 2 offers a comprehensive overview of the development of our corpus, PTradutor, which serves as the foundation for training our models. Sections 3 and 4 describe the experimental setup and present the results of our experiments, respectively. In Section 5, we position our research within the context of existing approaches. Finally, Section 6 summarizes our findings and suggests directions for future research.

# 2 PTradutor

As noted previously, a significant challenge in training a translation model for a specific language variety is the scarcity of datasets tailored for this purpose. To overcome this obstacle, we devised a three-step approach to automatically generate a parallel corpus. Industry-grade translation systems typically achieve higher translation quality when translating into a resource-rich language, such as English, rather than a low-resource language, such as European Portuguese. We exploit this concept to transform a monolingual European Portuguese corpus into a high-quality parallel corpus, using the following method:

1. Mono-lingual Corpus: We compiled a large monolingual collection of texts written in European Portuguese.   
2. Translation: Using a translation system, we created a parallel corpus from the mono-lingual corpus, by translating from European Portuguese to English.   
3. Filtering: We conducted filtering and quality checks to ensure the integrity of the dataset.

This process creates a corpus with parallel data pairs for English and European Portuguese, enabling the training of MT systems. In the following sections, we describe each step in detail.

# European Portuguese Corpus Collection

For the collection of texts in European Portuguese, we used two sources: the DSL-TL (Zampieri et al. 2024) and the PtBrVid corpus (Sousa et al. 2025). The DSL-TL corpus comprises a total of 4,458 news articles2 written in Portuguese, manually annotated as “European Portuguese”, “Brazilian Portuguese”, or “Both”. For our purposes, we use the train partition of this dataset and keep the texts labeled as “European Portuguese” and “Both” as they are both valid texts in European Portuguese, resulting in a total of 1,734 documents.

The larger chunk of our data comes from the PtBrVid corpus, which was originally created to train a Portuguese variety identifier that discriminates between European and Brazilian Portuguese. This corpus was constructed by compiling several datasets for each variety across six domains – journalism, web, social media, literature, legal, and politics – using metadata from the original datasets to label the variety of its texts. For example, the political subset uses the CETEM Pu´blico (Rocha and Santos 2000) corpus for European Portuguese and the CETEM Folha3 for Brazilian Portuguese. For this research, we kept all entries that were labeled as “European Portuguese”.

# Translation

Monolingual data was translated into English using Google Translate with the Python library deep translator4. The reason for this choice is the accessibility and relatively good quality of Google Translate, however, this step can be achieved by other translation systems. When this dataset was created, Google Translate did not distinguish between European and Brazilian Portuguese. Consequently, using it as the translation engine assumes that the translation from European Portuguese to English is unaffected by this limitation.

To verify this hypothesis, we conducted an experiment using one of our test datasets, namely FRMT (Riley et al. 2023), which contains 2,616 examples in English, Brazilian Portuguese, and European Portuguese. We used Google Translate to translate the European Portuguese and Brazilian Portuguese texts into English and then compared the English translations produced from the different varieties. We found that Google Translate produced exactly the same translation for approximately $\bar { 8 } 7 . 2 \%$ of the dataset and achieved a BLEU score of $9 6 . 8 \dot { \% }$ . This confirms that the quality of the dataset is only mildly affected by Google’s lack of differentiation between varieties. Nonetheless, this assumption is language-specific and might not hold for different language varieties and different translation systems.

# Filtering

The aggregation of the mentioned resources results in a total of 3, 966, 538 documents. As previous research focused on training and fine-tuning LMs has emphasized the importance of data quality (Wenzek et al. 2020; Penedo et al. 2024), we apply extensive filtering and quality checks to our dataset.

Specifically, our filtering pipeline begins by using the jusText5 library – designed to clean boilerplate content by classifying textual blocks based on various features such as length and stop word density – to remove entries classified as low-quality text. As depicted in Figure 1, this is the most stringent filter in our process, eliminating approximately 1.9 million documents from the collection. A closer examination reveals that most of the discarded documents originated from the Social Media partition of the PtBrVId corpus, which initially contained 2,014,752 documents, of which only 260,315 remained after applying this filter.

To further refine the dataset, we removed all entries for which the Portuguese text were duplicated, contained ASCII characters that are not used in the Portuguese language (like “ø” or “zˇ”), or included repetitive templates that were over-represented in the dataset (steps “Duplicates”, “Invalid Chars” and “Patterns” in Figure 1). For example, we identified 1,132 documents that began with “Lista de altera¸co˜es recentes” (“List of recent changes” in English) from a badly scrapped page and 221 documents that started with “Filtrar por” (“Filter by”), referring to search engine filters.

Additionally, we filtered out all documents exceeding 900 tokens in the combined source and target texts6. This ensures that, regardless of the prompt template used during training, no entry exceeds 1024 tokens. The data loss from this step was minimal, as the dataset primarily consists of singleparagraph documents without line breaks. In fact, documents exceeding 900 tokens accounted for only $0 . 5 4 \%$ of the entire dataset. However, removing these documents significantly improved training speed by enabling a larger batch size. When deploying this model, it is crucial to be aware of this limitation and implement strategies to handle larger inputs, such as sentence splitting.

![](images/c36ff2ba9ac16c5ac188453eae66aab87dbf3eac97ae1b286415a6d09c396c06.jpg)  
Figure 1: Number of documents (in millions) remaining after each step of our filtering pipeline.

The final step of our filtering pipeline involves a series of miscellaneous checks, such as ensuring that each text contains a matching number of opening and closing brackets and quotation marks. For detailed information about the operations performed, we refer to our code repository.

We refer to the final parallel corpus as PTradutor. Table 1 presents relevant statistics of the dataset after filtering, including the number of tokens and the domains covered. The code to replicate the dataset is available as open-source7, and the final corpus is publicly accessible on HuggingFace8.

# 3 Experimental Setup

In this section, we describe the training paradigm and model variations used in this study. We primarily focus on finetuning small-sized LMs, since smaller models are easier for the community to deploy and use.

# Models

To align with the pre-training objective of language models (LMs), we model the translation task as a causal language modeling problem, conditioned on both the translation prompt and the input English text. Specifically, given a translation prompt template $p r o m p t = T ( t e x t _ { e n } )$ (with $t e x t _ { e n }$ being the text in English) and a parallel corpus of Portuguese and English, we fine-tune a pre-trained model $L M ( p r o m p t )$ to generate the translated version of the English text in European Portuguese, $t e x t _ { p t }$ . The language models we use are as follows:

Gemma-2 The latest installment in Google’s model series, Gemma-2, includes models with 2, 9, and 27 billion parameters (Team 2024). These models are recognized for their strong performance in reasoning and problemsolving tasks. We use the 2 billion parameter model from this series.

Phi-3-mini Developed by Microsoft (Abdin et al. 2024), the Phi-3 models are compact transformer decoder architectures (Vaswani et al. 2017) with a great percentage of the training corpus being composed of syntactic data. We employ the model with 3.8 billion parameters.

LLaMA-3 This is the third iteration of Meta’s large language model series (AI 2024). LLaMA-3 models are noted for their enhanced reasoning abilities, logical consistency, and reduced hallucination compared to earlier versions. We use the smallest model from this series, which has 8 billion parameters.

Since the goal is to fine-tune the models to follow a specific instruction (translate the English text to European Portuguese), we use the instruct-tuned version of all models as the starting checkpoint. The prompt templates for each model are designed by following the models’ guidelines, with the system message of “You are a translator from English to European Portuguese” (in case the model supports a system message) and the user prompt “Translate this text from English to European Portuguese: texten”.

<html><body><table><tr><td rowspan="2">Domain</td><td rowspan="2"># Docs</td><td colspan="4"># Tokens PT</td><td colspan="4"># Tokens EN</td></tr><tr><td>Min</td><td>Max</td><td>Mean</td><td>Total</td><td>Min</td><td>Max</td><td>Avg</td><td>Total</td></tr><tr><td>Journalistic</td><td>1,250,982</td><td>45</td><td>511</td><td>202.9</td><td>253,767,361</td><td>25</td><td>433</td><td>150.3</td><td>188,072,054</td></tr><tr><td>Literature</td><td>12.082</td><td>51</td><td>510</td><td>121.0</td><td>1,461,651</td><td>37</td><td>360</td><td>89.8</td><td>1,085,296</td></tr><tr><td>Web</td><td>9,006</td><td>44</td><td>555</td><td>224.7</td><td>2,024,062</td><td>28</td><td>416</td><td>167.1</td><td>1,504,751</td></tr><tr><td>Politics</td><td>477</td><td>53</td><td>524</td><td>244.9</td><td>116,836</td><td>36</td><td>380</td><td>171.5</td><td>81,801</td></tr><tr><td>Legal</td><td>282,870</td><td>44</td><td>451</td><td>87.1</td><td>24,635,676</td><td>25</td><td>385</td><td>64.9</td><td>18,346,240</td></tr><tr><td>Social Media</td><td>163,585</td><td>41</td><td>129</td><td>71.0</td><td>11,622,673</td><td>26</td><td>121</td><td>55.2</td><td>9,025,327</td></tr><tr><td>DSL-TL (news)</td><td>1,734</td><td>14</td><td>135</td><td>63.6</td><td>110,334</td><td>10</td><td>108</td><td>47.2</td><td>81,821</td></tr><tr><td>All</td><td>1,719,002</td><td>14</td><td>555</td><td>170.8</td><td>293,628,259</td><td>10</td><td>433</td><td>126.9</td><td>218,115,469</td></tr></table></body></html>

Table 1: Statistics describing the PTradutor corpus, including the number of documents (# Docs) and the minimum, maximum, average, and total number of tokens per domain.

# Training Approaches

We utilize two types of instruction fine-tuning:

Full fine-tuning This is similar to the pre-training paradigm, in which given a prompt as instruction, the LM is trained end-to-end, updating all the parameters during optimization. This method although providing better results is often costly, due to the large size of current LMs.

Parameter efficient fine-tuning (PEFT) To decrease the computational and storage costs methods for PEFT (Xu et al. 2023) enable efficient adaptation of LMs by only fine-tuning a small number of extra model parameters instead of all the model’s parameters. LoRA (Hu et al. 2022; Xu et al. 2024) is one of the most popular methods in this domain, which achieves this by injecting trainable low-rank matrices into each layer of the transformer architecture. In addition to full fine-tuning, we also train the LoRA variant of each model.

# 4 Evaluation

In this section, we provide a detailed evaluation of our system against various baselines, on two European Portuguese benchmarks. The code for training and evaluation, as well as the trained checkpoints of our models, is available in our repository9.

# Metrics

For a comprehensive evaluation, we include both classical and embedding-based metrics.

N-gram based metrics Classical machine translation metrics (Popovic 2015; Papineni et al. 2002; Lin 2004; Banerjee and Lavie 2005) usually focus on n-gram overlap between the reference translation and the translation generated by the system. Although by design this metrics fail to recognize semantic similarity beyond the lexical level, they remain widely adopted in academic research. In this work, we incorporate the two most common metrics: BLEU (Papineni et al. 2002) and ROUGE (Lin 2004).

Learnable metrics These methods focus on directly learning human judgment through training. For this family, we include COMET (Rei et al. 2020), which leverages a pretrained multilingual model. Although COMET can function as a reference-less metric, in this work, we report results exclusively for its direct assessment variant.

Language variety metric To assess if the text produced by our translation system is indeed in European Portuguese, we use a Portuguese language variant classifier (Sousa et al. 2025), which distinguishes between Brazilian and European Portuguese. After translation of the benchmark data, we employ the classifier to label all generated texts and to compute the percentage of documents that are labeled as European Portuguese. Since the classifier might contain intrinsic errors and bias, we also compute the percentage of documents labeled as European Portuguese in the reference translations. This step is crucial for handling cases where the translated text for the two variants might be identical. In such scenarios, the classifier might incorrectly classify the variety as the wrong one due to the lack of distinguishing features in the text. By comparing the results of our system with the reference translations, we can correct for this potential bias and obtain a more accurate assessment of how well our system produces European Portuguese. We refer to the ratio of these two percentages as the VID score, which serves as a measure of the system’s effectiveness in generating European Portuguese text.

# Test Benchmarks

As a low-resource language variant, the number of benchmarks that include European Portuguese is limited. In this study, we use two high-quality publicly available datasets that feature this variant:

• FRMT: This dataset is specifically designed to contain regional variants of Portuguese and Chinese (Riley et al. 2023), containing human translations of sentences from English Wikipedia articles that were manually translated to European and Brazilian Portuguese. • NTrex: The dataset consists of high-quality translations by speakers who are bilingual in English and in one of the 128 target languages, including 123 documents and 1,997 sentences for each language (Federmann, Kocmi, and Xin 2022).

# Baselines

We compare our models to three sets of baselines:

Closed Baselines We include industry-standard systems for Portuguese translation, including Google Translate and DeepL. Recently, Google Translate introduced a model specifically designed for European Portuguese, referred to as ${ \mathrm { G o o g l e } } _ { p t }$ , which we include in our evaluation alongside the original model that does not distinguish between Portuguese varieties, referred to as ${ \bf G } _ { 0 0 } { \bf g } { \bf l e } _ { b r }$ .

Open Baselines For open-source systems, we evaluate ArgosTranslate10, which uses OpenNMT (Klein et al. 2017) as its backend. Although Portuguese is listed as a supported language, the specific variety is not indicated. We also consider the Opus-MT project (Tiedemann and Thottingal 2020), another open-source system that provides a model for translating from English to Portuguese. However, like ArgosTranslate, this system does not differentiate between Portuguese varieties.

Zero-shot Additionally, we assess the zero-shot capabilities of language models without applying our task-specific fine-tuning to demonstrate the effectiveness of the finetuning process.

# Implementation Details

All models were trained and evaluated on a server with six A-100 GPUs, each with 40GB of memory. The batch size and training duration varied depending on the memory requirements of each model. In our repository, we provide training and evaluation scripts compatible with the two libraries used to train the language models: torchtune11 and transformers12. While the transformers library was chosen for its practicality, we found it limiting when training larger models. In that scenario, torchtune presented as a reliable alternative with significantly better memory management. Training runs we executed with early stopping – using the test set the DSL-TL corpus as validation set – with patience of 3,000 steps. As a result, the number of training steps varied across models. All LoRA variants were trained with an alpha of 128 and a rank of 256. Detailed training configurations can be found in our repository.

The parameter setup is as follows:

• Phi-3: For both the LoRA and full fine-tuning of Phi3 models, we used a batch size of 512, a learning rate of 2e-5, a weight decay of 0.1, and a warm-up of 1,000 steps. • Gemma-2: For both variants, the learning rate was set to 2e-5 with a weight decay of 0.1. The full fine-tuned model was trained with a 1,000-step warm-up and a batch size of 512, while the LoRA variant had 500 warm-up steps and a batch size of 256.

• LLaMA-3: Both variants were trained with a batch size of 256 and a learning rate of 2e-5. The LoRA variant additionally includes a warm-up of 100 steps and a weight decay of 0.1 on the learning rate.

# Results

The result of our evaluation is shown in Table 2, where the best overall values are marked with an underline, and the most effective open-source systems are marked in bold. The general trend on both datasets is quite similar and highperforming models maintain a stable performance across both benchmarks. Yet, values for NTrex are slightly below FRMT for all systems, indicating a harder benchmark. In the following, we describe our main findings and highlight avenues for future research.

LoRA models: These variants effectively learn the vocabulary of European Portuguese, but their overall translation quality remains subpar. This discrepancy is evident as they score highly on the VID metric, nevertheless, the BLEU, ROUGE-L, and COMET metrics suggest that they struggle with generating high-quality text. Upon closer inspection of the generated translations, we found that both the Phi-3 and LLaMA-3 models, when trained with LoRA, tend to enter a repetition loop, where the same token is generated repeatedly until the process is interrupted. This suggests that for a medium size language model translation is a complex task, and simply adding adapter parameters is insufficient to fully capture its nuances. This issue, combined with the fact that the early stopping criteria were reached quickly during training (training loss had plateaued while the evaluation was increasing), suggests that the models may require increased capacity (by adjusting the alpha and rank parameters) to better learn from the training data.

Full fine-tuning (FFT): The fully fine-tuned models sacrifice their mastery of European Portuguese nuances in favor of high-quality, coherent translations. These models yield more moderate scores for the VID metric while achieving significantly higher text quality metrics. In particular, the fine-tuned LLaMA-3 model beats all open source software on all metrics and produces results comparable to ${ \bf G o o g l e } _ { b r }$ in terms of BLEU and ROUGE-L on both benchmarks, only falling short on the COMET metric.

It is important to note that the COMET metric, as a learnable metric, is subject to the same biases that affect any trainable neural model. Since the training data does not differentiate between European and Brazilian Portuguese and most open-source resources are skewed toward Brazilian Portuguese, this bias may have influenced the scores produced by this metric. The slight difference between $\mathbf { G o o g l e } _ { b r }$ and fine-tuned LLaMA-3 models suggests a potential bias in the COMET score toward Brazilian Portuguese.

The overall performance of the fully fine-tuned models has a direct correlation with model size, where our largest model, LLaMA-3, with 8 billion parameters outperforms the smaller models of Phi-3 (3.8 billion parameters) and

Table 2: Model effectiveness was assessed using the FRMT and NTrex benchmarks. Confidence intervals for the COMET metric were computed using a $t$ -distribution with a $9 5 \%$ confidence level. The best results among open-source systems are highlighted in bold, while the best overall results are underlined.   

<html><body><table><tr><td rowspan="2"></td><td rowspan="2">Model</td><td colspan="4">FRMT</td><td colspan="4">NTrex</td></tr><tr><td>BLEU</td><td>ROUGE-L</td><td>COMET</td><td>VID</td><td>BLEU</td><td>ROUGE-L</td><td>COMET</td><td>VID</td></tr><tr><td rowspan="3">Closelines</td><td>Googlebr</td><td>43.20</td><td>68.43</td><td>87.44±0.25</td><td>0.445</td><td>35.80</td><td>63.44</td><td>86.88±0.32</td><td>0.361</td></tr><tr><td>Googlept</td><td>47.81</td><td>71.66</td><td>87.87±0.25</td><td>0.956</td><td>39.92</td><td>66.72</td><td>87.17±0.33</td><td>0.900</td></tr><tr><td>DeepL</td><td>49.77</td><td>72.44</td><td>88.48±0.23</td><td>0.999</td><td>44.76</td><td>67.77</td><td>87.96±0.31</td><td>0.997</td></tr><tr><td>Open Baselines</td><td>Argos</td><td>38.39</td><td>65.07</td><td>83.99±0.35</td><td>0.511</td><td>30.44</td><td>58.72</td><td>80.30±0.54</td><td>0.446</td></tr><tr><td rowspan="3">Zero-shot</td><td>Opus-MT</td><td>40.41</td><td>66.25</td><td>85.67±0.31</td><td>0.413</td><td>32.99</td><td>60.24</td><td>83.81±0.46</td><td>0.229</td></tr><tr><td>Gemma-2</td><td>25.37</td><td>49.56</td><td>75.66±0.51</td><td>0.807</td><td>17.47</td><td>41.34</td><td>69.46±0.66</td><td>0.858</td></tr><tr><td>Phi-3</td><td>17.59</td><td>43.99</td><td>57.90±0.56</td><td>0.942</td><td>13.16</td><td>38.38</td><td>54.62±0.66</td><td>1.003</td></tr><tr><td rowspan="2"></td><td>LLaMA-3</td><td>31.47</td><td>60.61</td><td>82.95±0.40</td><td>0.811</td><td>21.09</td><td>52.34</td><td>78.70±0.60</td><td>0.805</td></tr><tr><td>Gemma-2</td><td>19.83</td><td>56.87</td><td>79.62±0.64</td><td>0.530</td><td>14.41</td><td>49.42</td><td>76.18±0.81</td><td>0.514</td></tr><tr><td rowspan="2">LoRA</td><td>Phi-3</td><td>24.70</td><td>53.34</td><td>72.19±0.58</td><td>1.178</td><td>20.10</td><td>48.38</td><td>67.67±0.73</td><td>1.203</td></tr><tr><td>LLaMA-3</td><td>25.42</td><td>51.51</td><td>74.06±0.56</td><td>1.092</td><td>17.74</td><td>41.24</td><td>67.96±0.73</td><td>1.140</td></tr><tr><td rowspan="3">FFT</td><td>Gemma-2</td><td>33.76</td><td>66.41</td><td>85.25±0.35</td><td>1.066</td><td>18.35</td><td>59.75</td><td>82.62±0.54</td><td>1.049</td></tr><tr><td>Phi-3</td><td>38.16</td><td>66.31</td><td>85.35±0.34</td><td>1.055</td><td>27.89</td><td>60.18</td><td>82.91±0.49</td><td>1.031</td></tr><tr><td>LLaMA-3</td><td>41.12</td><td>66.92</td><td>86.12±0.28</td><td>0.968</td><td>35.76</td><td>62.02</td><td>84.42±0.42</td><td>0.933</td></tr></table></body></html>

Gemma-2 (2 billion parameters). This behavior is typical for large language models and indicates that increasing their size can lead to better results. This indicates that by applying this methodology to even larger models, it may be possible to achieve results that are on par with industry-standard systems.

Since our focus is on translation specific to European Portuguese, it is important to examine the VID scores across both benchmarks. Our best model, LLaMA-3, once again beats all open source baselines and achieves scores significantly higher than ${ \bf G } _ { 0 0 } { \bf g } { \bf l e } _ { b r }$ and is comparable to $\mathrm { G o o g l e } _ { p t }$ and DeepL. This suggests that the proposed methodology is effective in achieving the targeted goal of producing text specific to a language variety. Even our smaller-size models, perform comparable to open-source baselines on translation metrics, dramatically improving on the VID score.

It is true that in terms of text quality metrics, the LLaMA3 model still lags behind the European Portuguese-specific industry models, namely ${ \mathrm { G o o g l e } } _ { p t }$ and DeepL. However, it is important to emphasize that our goal was not to beat the specialized model from industry, but to propose a computationally efficient, adaptable, and resource-efficient method for adapting small language models to translate specific language varieties. Surpassing the current open-source software and achieving a score close to industry-level models, which benefit from dedicated teams of experts and annotators for each language, is a significant accomplishment.

# 5 Related Work

Despite the availability of industry-level translation systems like Google Translate13 and DeepL14 and a handful of open-source software with unclear language variant definition (Klein et al. 2017; Tiedemann and Thottingal 2020), there are, to the best of our knowledge, no other translation models specifically dedicated to European Portuguese in the literature. Like many other industry systems, these models lack transparency, as neither their internal workings nor the data used for training are publicly accessible.

Since this work focuses on Portuguese, we begin by reviewing relevant research in this area. Lakew, Erofeeva, and Federico (2018) explore NMT from English into four pairs of language varieties, including European and Brazilian Portuguese, unlike our focus on Portuguese to English. They train a transformer model using transcripts of movies and TED talks but do not open-source their models for comparison. The work primarily highlights the challenges of lowresource settings and the initial steps to address these issues.

Another work in this direction is from Team (2022), which aims to train a translation model between standard national varieties of the same language, namely between Brazilian and European Portuguese. They provide a taxonomy of distinctive characteristics between these two language varieties, which we refer curious readers to for a deeper understanding of the language varieties discussed in this study.

Other previous work in this direction explored a varietytargeted MT system in other languages, which contains varieties or dialects specific to a region. It is worth noting that although some of these works are similar to textual style transfer in methodology, they are different in the task definition (Jhamtani et al. 2017).

One of the earlier works in this area focuses on the development of translation systems for Arabic dialects (Zbib et al. 2012; Sajjad, Darwish, and Belinkov 2013). Efforts for Arabic took off with the introduction of AraBench benchmark (Sajjad et al. 2020), an evaluation suite for dialectal Arabic to English MT.

Similar efforts have been undertaken for the low-resource language family of Swiss German, which is widely spoken in Switzerland, but rarely written (Honnet et al. 2018; Scherrer and Ljubesic 2016). These systems focus on normalizing Swiss German to the standard variant of High German.

Similarly to Arabic and German other languages, such as Chinese, Russian, Hindi and Turkish, are also slowly finding their way into this study (Wan et al. 2020; Kumar et al. 2021; Nguyen and Chiang 2017; Durrani et al. 2010).

Another area of research loosely related to ours involves adapting language models to low-resource language varieties. As previously mentioned, the effectiveness of large language models in this domain is often limited by the scarcity of representative datasets (Alam et al. 2024). Recent efforts have focused on creating specialized language models for specific dialects (Ondrejova´ and Sˇ uppa 2024; Faisal and Anastasopoulos 2024; Nguyen et al. 2024; Lopes, Magalha˜es, and Semedo 2024), mainly through data augmentation or the introduction of new datasets. Although these are not translation systems and therefore are not directly comparable to our work, our methodology and provided resources can also be used to provide artificial training data.

# 6 Conclusion & Future Work

In this paper, we present a methodology for creating a parallel corpus and training a translation model tailored to a lowresource language variety. Specifically, we developed and open-sourced the largest European Portuguese-English parallel corpus, along with European Portuguese-specific translation models. Our extensive evaluation demonstrates the effectiveness of our approach and the fidelity of the generated translations to the desired language variety. Thanks to the proposed methodology we managed to achieve performance on par with industry-level translation systems with minimal resources and limited computation.

In future work, it would be interesting to investigate the impact of different generation configurations on the translations produced by our model. In this study, we used greedy decoding, but other generation techniques, such as beam search, could yield better results. Another promising direction is to explore prompt optimization both before and after model training. This approach has been shown to improve outcomes in other studies (Soylu, Potts, and Khattab 2024) and might enhance our system’s performance as well. Finally, we plan to conduct a human evaluation with linguists to identify areas where our model falls short compared to other systems.