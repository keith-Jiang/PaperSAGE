# A Simple and Comprehensive Benchmark for Single-Cell Transcriptomics

Jiaxin $\mathbf { Q _ { i } ^ { \bullet } }$ , Yan $\mathbf { C u i } ^ { 2 * }$ , Kailei $\mathbf { G u 0 } ^ { 4 }$ , Xiaomin Zhang4, Jianqiang Huang1,2,3†, Gaogang Xie1,3

1Computer Network Information Center, Chinese Academy of Sciences, Beijing, China 2Hangzhou Institute for Advanced Study, University of Chinese Academy of Sciences, Hangzhou, China 3University of Chinese Academy of Sciences, Beijing, China 4Tianjin Medical University Eye Hospital, Tianjin, China jxqi $@$ cnic.cn, cuiyan.ch@gmail.com, guokaile $2 0 1 9 @$ tmu.edu.cn, xzhang $0 8 @$ tmu.edu.cn, jqhuang@cnic.cn, xie@cnic.cn

# Abstract

Single-cell transcriptomics describes complex molecular features at the individual cell level, serving various roles in biological research, such as enhancing gene expression and predicting drug responses. Due to transcriptomic data structurally resembling sequential data, many researchers have trained numerous transformers on extensive transcriptomic datasets. However, they have consistently neglected to explore the intrinsic properties of the data and the appropriateness of their chosen model architecture. In this paper, we carefully investigate the nature of transcriptomics, identifying three overlooked problems: 1) long-tailed data problem, 2) model selection problem, and 3) evaluation problem. Consequently, by applying the weighted sampling strategy, we address the long-tailed data problem and achieve consistent improvement across all settings. By adapting different model structures to transcriptomic data, we discover that transformers are not the only option. By developing three downstream tasks and fair evaluation metrics, we establish a simple and comprehensive benchmark to validate the effectiveness of models for transcriptomics. Through extensive experiments, we clarify the misunderstandings in the traditional methods and provide competitive baselines, thereby paving the way for future research in this field.

# Introduction

Single-cell transcriptomics, also known as single-cell RNA sequencing, is a high-throughput technology that sequences and analyzes RNA within individual cells (Kolodziejczyk et al. 2015), which aids many biological tasks, such as cell annotation and gene regulatory discovery. Since the samples of transcriptomics resemble sequential data in form, typically presented as a series of genes and their corresponding expression values: $( \mathrm { g e n e } _ { 1 }$ , $\operatorname { g e n e } _ { 2 } , \ldots , \operatorname { g e n e } _ { n } ,$ ) (value $_ 1$ , value2, . . . , valuen), Transformers are naturally introduced into this field. With the development of Transformers, researchers have conducted extensive pre-training on the transcriptomic dataset, which includes about $5 0 ~ \mathrm { m i l }$ - lion samples (Cui et al. 2024). However, a critical misunderstanding persists: transcriptomics is inherently not sequential data. The indiscriminate use of Transformers for large

Count (Billion) Count (Billion)   
80 long-tailed (raw) 80 uniform (ideal) distribution distribution   
60 97.3% 60   
40 94.3% 个 40 down-sample   
20 20 Value up-sample Value 0 0 1 5 10 15 20 25 5 10 15 20 25 (a) Long-tailed Data Problem not sequential 𝒙 𝒗 𝒗𝟑 too many 𝟑 genes in a sample QK QQQ 000 38C Transformer MLP CNN 𝒗𝟐 𝒗𝟐 𝒗𝟐 (b) Model Selection Problem   
need biological knowledge standardized not qualitative fair comprehensive Gene Regulatory Expression Perturbation Network Prediction Classification Regression 𝒙 𝒙 class 𝒙 class 2 𝒙𝟓 𝒙𝟒 𝒗𝟒 𝒗𝟓 𝒗𝟔 𝒗𝟏 𝒗𝟑 𝒗𝟒 (c) Evaluation Problem

scale pre-training has resulted in substantial consumption of computational resources. Therefore, there is a pressing need for in-depth discussions on data and models within transcriptomics. After carefully analyzing the datasets and existing methods, as shown in Figure 1, we identified three consistently overlooked problems in this field: long-tailed data problem, model selection problem, and evaluation problem.

First, as illustrated in Figure 1(a), we performed statistical analyses for each expression value across all samples, deriving comprehensive statistical results that clearly exhibit an extreme long-tailed distribution for these values. For example, the value ‘1’ accounts for over $54 \%$ of the data, and values less than $\cdot _ { 1 0 } ,$ constitute over $94 \%$ . These findings indicate that traditional training for transcriptomic data will result in models biased toward high-frequency values and neglect the rare ones, i.e., the model is encouraged to learn more ‘1’s to minimize the training loss and overlook less frequent but larger values. However, many biological studies in transcriptomics suggest that larger expression values may represent more significant meanings and should be emphasized (Jessop et al. 2020; Danopoulos et al. 2020). Moreover, the measurements in transcriptomics render smaller values less robust against errors (Karaayvaz et al. 2018; AlJanahi, Danielsen, and Dunbar 2018), which amplifies the longtailed problem in traditional methods. To address this problem, we implemented a commonly used strategy in longtail tasks, weighted sampling, which enhances the sampling probability for the rarer samples, i.e., the large values. As illustrated in Table 1 and Table 2, this straightforward method consistently improves performance across all settings.

Second, regarding model selections, because transcriptomic data structurally resemble sequential data, and its training loss is similar to masked language modeling, Transformers are intuitively utilized. This instinct has led researchers to focus solely on Transformers or their variants (Hao et al. 2024), thus neglecting the potential of other architectures. However, it is critical to recognize that transcriptomics fundamentally differs from sequential data, particularly because each gene appears only once per sample and there is no relative order between genes. Furthermore, in transcriptomics, where sequence lengths can reach up to 60,000, Transformers require substantial memory and computational resources because their attention mechanisms scale quadratically with sequence length (Keles, Wijewardena, and Hegde 2023), making them inefficient. Therefore, we explored other foundational architectures, such as Multilayer Perceptrons (MLP) (Haykin 1998), Convolutional Neural Networks (CNN) (Krizhevsky, Sutskever, and Hinton 2012), and Mamba (Gu and Dao 2023), for transcriptomics. With simple adaptation, we found these models could achieve competitive performance with less computational overhead. Additionally, we conducted detailed analyses of various Transformer settings to address some misunderstandings from traditional methods.

Finally, the evaluation problem appears in two aspects. One is that the downstream tasks used to validate pre-trained models are overly specialized, requiring specific biological knowledge and lacking quantitative results, such as the assessments of gene regulatory networks prediction. This hinders the involvement of researchers from other fields, e.g., AI researchers. The other one is that the evaluations of traditional methods are neither fair nor standardized. For example, these methods often fine-tune the pre-trained models in downstream tasks, which impedes an accurate assessment of pre-training performance. Additionally, some experiments are performed only once, leading to non-reproducible results and unreliable conclusions. To address these concerns, in Figure 1(c), we introduced a simple and comprehensive benchmark for large-scale and standardized evaluations, including expression classification across 10 datasets, and perturbation classification and prediction across 3 datasets. We also standardized downstream evaluations by freezing the model backbone, running multiple times to report the average, and utilizing extracted features for $\mathbf { k }$ -nearest neighbors classification or training only one linear layer to ensure the rigorous and quantitative evaluation of the pre-trained models. This benchmark provides a fair and comprehensive testbed, enabling more researchers, without specialized biological knowledge, to participate in this field.

Our contributions can be summarized in three aspects:

1. We thoroughly analyzed single-cell transcriptomics and existing methods, revealing three overlooked problems: the long-tailed data problem, the model selection problem, and the evaluation problem, which significantly impair the large-scale pre-training for transcriptomic data.   
2. We proposed straightforward yet effective solutions to address these problems: implementing weighted sampling, adapting different model structures, and designing rigorous downstream evaluations with fair comparison criteria.   
3. We introduced a simple and comprehensive benchmark for single-cell transcriptomics. Through extensive experiments, we validated the effectiveness of our proposed methods. The benchmark also serves as a convenient testbed for other researchers, preventing the blind waste of computational resources on large-scale transformer pre-training, delineating the future direction in this field.

Code — https://github.com/simpleshinobu/scbenchmark

# Related Works

Traditional Single-Cell Transcriptomics. Single-cell transcriptomics, also known as single-cell RNA sequencing, i.e., scRNA-seq, was first introduced by Surani Lab (Tang et al. 2009). Since then, significant improvements in sensitivity, speed, and affordability have been achieved (Sasagawa et al. 2013; Macosko et al. 2015), and computational tools and public data resources for scRNA-seq are rapidly expanding (Voigt et al. 2021; Kharchenko 2021). Today, scRNAseq is extensively used in the field of human health, primarily to characterize cell types in various organs (Voigt et al. 2021; Ramachandran et al. 2020), such as exploring transcriptome heterogeneity across similarly classified cells in different states (Kravets and Benninger 2020; Wheeler et al. 2020) and clarifying temporal processes like human tissue development (Olaniru et al. 2023; Collin et al. 2021). Despite rapid advancements in transcriptomics, there remains a critical need for standardized benchmarks to evaluate the performance of computational tools or models. Efforts to establish benchmarks for transcriptomics analysis have been made (Tian et al. 2019; Li et al. 2022). However, prior works have primarily focused on traditional biological tools, while in this work, we provide fair and comprehensive benchmarks for large-scale pre-trained deep models.

Large-Scale Pre-training for Transcriptomics. With the development of Transformers (Vaswani et al. 2017), transcriptomics has demonstrated significant interest in largescale pre-training. scBERT (Yang et al. 2022) was the pioneer to propose the single-cell pre-training framework based on Transformers. Subsequent advancements can be categorized into two camps: expanding training scales and refining algorithms. In the first camp, scGPT (Cui et al. 2024) utilized an increased number of parameters to pre-train on a more extensive dataset, which comprises approximately 33 million cells. Furthermore, GeneCompass (Yang et al. 2023) leveraged a cross-species transcriptomic dataset to access over 130 million samples. In the second camp, scFoundation (Hao et al. 2024) introduced modifications to the Transformer framework for effectively handling long gene sequences. GeneFormer (Theodoris et al. 2023) enhanced efficiency by adopting a gene sequencing approach to eliminate the need for gene embeddings. CellPLM (Wen et al. 2023) and tGPT (Shen et al. 2023) introduced novel structures to analyze intercellular relationships and create an autoregressive gene prediction pipeline, respectively. However, these methods have consistently overlooked the essential problems we identified in transcriptomics, resulting in the blind adoption of Transformers for large-scale pre-training that consumes extensive resources. In this work, we address these problems with a simple and comprehensive benchmark, providing a testbed that supports the development of future pre-training frameworks and innovative algorithms.

# Method

Preliminaries. Considering the common training framework in transcriptomics (Yang et al. 2023; Hao et al. 2024), we start with the training set $\ { \bar { \pmb S } } = \{ ( { \pmb x } _ { i } , { \pmb v } _ { i } ) \} _ { i = 1 } ^ { N }$ , where $\scriptstyle { \mathbf { { \vec { x } } } } =$ $( x _ { 1 } , x _ { 2 } , \ldots , x _ { l } )$ denotes a set of genes, $\pmb { v } = ( v _ { 1 } , v _ { 2 } , \dots , v _ { l } )$ denotes the expression values corresponding to each gene, $l$ is the number of genes in this sample (cell). Similar to the masked language modeling (Devlin et al. 2018) in sequential training, transcriptomics adopts masked value prediction, as shown in Figure 2(a), where $_ v$ is masked as $\pmb { \tilde { v } } = ( v _ { 1 } , [ \mathrm { m a s k } ] , \dots , v _ { l } )$ , [mask] denotes the masked value token, and the self-supervised loss can be written as:

$$
\begin{array} { r l r } {  { \mathcal { L } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { k } \| \pmb { v } _ { i , k } - ( \pmb { e } _ { i } ) _ { k } \pmb { w } \| ^ { 2 } , } } \\ & { } & { \quad \pmb { e } _ { i } = \phi ( \pmb { E } _ { x } ( \pmb { x } _ { i } ) , \pmb { E } _ { v } ( \tilde { \pmb { v } } _ { i } ) ) , \quad } \end{array}
$$

where $\boldsymbol { v } _ { i , k }$ represents the $k$ -th masked value of sample $i$ and $( \boldsymbol { e } _ { i } ) _ { k } \in \mathbb { R } ^ { 1 \times d }$ is the gene feature corresponding to $\boldsymbol { v } _ { i , k }$ , $d$ is the hidden dimension, $\boldsymbol { e } _ { i } \in \mathbb { R } ^ { l \times d }$ denotes the extracted gene features, $\pmb { w } \in \mathbb { R } ^ { d \times 1 }$ is  linear layer to project $e _ { i }$ into scalars, ${ \bf E } _ { x }$ and $\scriptstyle { E _ { v } }$ are embedding layers for genes and values respectively, and $\phi$ is the feature extractor.

Since each gene appears only once in a cell, enabling the predictions derived from the product of gene embedding and cell features (Cui et al. 2024), which is the Masked Values prediction loss with Cell features modified from Eq. (1):

$$
\mathcal { L } _ { \mathrm { M V C } } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { \boldsymbol { k } } \| \pmb { v } _ { i , \boldsymbol { k } } - \left( \pmb { E } _ { x } ( \boldsymbol { x } _ { i , \boldsymbol { k } } ) \cdot \bar { \boldsymbol { e } } _ { i } \right) \| ^ { 2 } ,
$$

where $x _ { i , k }$ is the gene for $k$ -th masked value of cell $i$ , $\bar { e } _ { i } \in \mathbb { R } ^ { 1 \times d }$ is the cell feature, which can be implemented as the average of all gene features or a specific token feature (Devlin et al. 2018), and $\mathbf { \nabla } \cdot \mathbf { \varepsilon }$ denote the dot product.

Additionally, due to the broad range of expression values, from zero to potentially millions, large losses will occur that impair the model optimization. Therefore, two common preprocessing methods are adopted. One is binning, the default preprocessing (Cui et al. 2024), which divides all values in a sample into several bins, thus producing a sequence of bin indices as the substitute value input. The other one is logarithmic transformation, formulated as $v = \log ( 1 + v )$ , which effectively constrains the scale of the values.

Long-Tailed Problem. This issue has been extensively explored in long-tailed classification tasks (Tang, Huang, and Zhang 2020; Zhong et al. 2021; Zhang et al. 2023), where the most effective methods involve sampling more lowfrequent samples to increase their occurrence. By converting the long-tailed data distribution to the uniform distribution, these methods effectively remove the model bias introduced by the frequent classes, i.e., frequent values in our scenarios. According to Eq. (1), our weighted sampling strategy can be written as:

$$
\mathcal { L } _ { \mathrm { L T } } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { k } \alpha _ { i , k } \left. \pmb { v } _ { i , k } - ( \pmb { e } _ { i } ) _ { k } \pmb { w } \right. ^ { 2 } ,
$$

where $\alpha _ { i , k }$ denotes the resampling weight for value $\boldsymbol { v } _ { i , k }$ , and $\alpha _ { i , k } \ \propto \ 1 / p ( \pmb { v } _ { i , k } )$ denotes that the greater the probability $p ( \pmb { v } _ { i , k } )$ of a value’s occurrence, the smaller the resampling weight for this value. Since $p ( \pmb { v } _ { i , k } ) \propto 1 / \pmb { v } _ { i , k }$ in transcriptomics, $L _ { \mathrm { L T } }$ is more focused on optimizing for larger values, which conforms to our motivation.

Model Selection Problem. The prediction of gene expression values relies on correlations between genes, e.g., some genes activate the expressions of others while some suppress them (Cordell 2009). Thus, the model needs to perform interactions among gene features as shown in Figure 2(a), and that is why traditional methods apply Transformers. Due to the computational burden discussed earlier, it is meaningful to adapt other fundamental architectures for transcriptomics. Note that, since Mamba is similar to Transformer, which can directly establish relationships between genes, we will focus on the adaptations for MLP and CNN.

For MLP, the direct implementation only transforms features at the last dimension, which fails to realize interactions among genes, resulting in poor performance as shown in Table 3. Our adaptation, as illustrated in Figure 2(b), uses an additional linear layer to transform the gene dimension, thereby enabling feature interactions between genes, which can be written as:

$$
e ^ { j + 1 } = \sigma ( e ^ { j T } \pmb { w } _ { 1 } ) ^ { T } \pmb { w } _ { 2 } ,
$$

![](images/04b80c19b18a5d981ab9d81a6545611f518aa54366c3a43bee3f41e24e896fdd.jpg)  
Figure 2: Illustrations of frameworks for Transformer and adapted models. (a) Transformer extracts feature through interactions among genes to predict masked values, i.e., $m$ . (b) The adapted MLP, formulated in Eq. (5), employs $\pmb { w } _ { 1 }$ to capture gene interactions. (c) The adapted CNN, formulated in Eq. (6), uses gene permutation to overcome the limitations of local receptive fields, i.e., by positioning closely related genes such as $x _ { 1 }$ and $x _ { 3 }$ to enhance their interactions.

where $\boldsymbol { e } ^ { j } \in \mathbb { R } ^ { l \times d }$ denotes gene features of the $j$ -th layer, $\pmb { w } _ { 1 } \in \mathbb { R } ^ { l \times l }$ and $\pmb { w } _ { 2 } \in \mathbb { R } ^ { d \times \breve { d } }$ are linear transformations for the last two dimensions, respectively, $\sigma$ denotes the activation, and T is the transposition of feature dimensions. Although the input gene length is constrained, it is not a significant issue in transcriptomics, as each gene appears only once per sample and the total number of genes is fixed.

For CNN, the local receptive field hinders its ability to capture the relationships among all genes. Considering that the interactions often occur within specific groups of genes (Funk et al. 2022), re-arranging the input genes, by using prior knowledge or other gene interaction metrics, is a valid adaptation for CNN to transcriptomics. As illustrated in Figure 2(c), our adaptation can be formulated as:

$$
e ^ { j + 1 } = \sigma ( { \mathrm { P e r m u t e } } ( e ^ { j } ) * w ) ,
$$

where Permute denotes permuting genes into a pre-defined order, which only happens in the first layer, $\textbf { \em w }$ is the convolution kernel, and $*$ is the convolution operation. In experiments, we find that the adaptation could improve the direct implementation, making its performance more competitive.

Evaluation Problem. As we have discussed, traditional methods overemphasize specialized biological knowledge and lack standardized evaluations for pre-trained models. Therefore, we designed three large-scale and quantifiable downstream tasks:

1) Expression Classification, where samples are labeled with classes, such as cell types, and the objective is to predict the class label based on gene expressions. The training loss can be written as:

$$
\mathcal { L } _ { c l s } = - \frac { 1 } { N } \sum _ { i = 1 } ^ { N } y _ { i } \log ( \frac { \exp ( \bar { e _ { i } } { \pmb w } ) } { \sum _ { j } \exp ( \bar { e _ { i } } { \pmb w } ) _ { j } } ) ,
$$

where $y _ { i }$ is the one hot class label, $\bar { e _ { i } }$ is the extracted cell features by a frozen pre-trained model, $\pmb { w } \in \mathbb { R } ^ { d \times c }$ is a learnable linear layer, $c$ is the total number of classes, $j$ indexes all classes. This task can also be realized by the $\mathbf { k }$ -nearest neighbors (KNN) strategy based on the extracted cell features $\bar { e _ { i } }$ .

2) Perturbation Classification, where a certain gene is perturbed (e.g., knockout (Egorov et al. 2021)) and the resulting expressions are recorded. The objective is to identify which gene was perturbed based on the post-perturbation expressions. Since this is also a classification task, the loss described in Eq. (7) and KNN strategy are applicable. Note that, this task presents greater challenges than the previous one, due to much more class types.

3) Perturbation Regression, where the same perturbation datasets are used and the objective changes to predict all expressions based on the perturbed gene, and the loss function can be written as a regression version:

$$
\mathcal { L } _ { r e g } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \Vert \pmb { v } _ { i } - \phi ( \pmb { E } _ { x } ( \pmb { x } _ { i } ) , \pmb { E } _ { x } ( \pmb { x } _ { i , p } ) \pmb { w } ) \Vert ^ { 2 } ,
$$

where $x _ { i , p }$ is the perturbed gene in sample $i$ , the pre-trained parameters ${ \bf E } _ { x }$ and $\phi$ are frozen, $\textbf { \em w }$ is a trainable parameter to learn the influence of perturbation for xi,p.

# Experiments

# Datasets

Pre-training Dataset. We follow the approach proposed by scGPT (Cui et al. 2024) to assemble a pre-training transcriptomic dataset, containing 54.6 million human cells from the CELLxGENE collection (Biology et al. 2023). This dataset encompasses more than 50 organs (e.g., blood and heart) and tissues across over 400 studies, offering a broad representation of cellular heterogeneity throughout the human body.

Expression Classification Datasets. We collect 10 expression classification datasets following the strategies in scGPT.

• Myeloid (Myel) (Cheng et al. 2021) performs a comprehensive pan-cancer analysis of myeloid cells, consisting of 13,178 samples and 21 sub-cancer classes. • Multiple Sclerosis (MS) (Schirmer et al. 2019) reveals specific cellular changes in multiple sclerosis lesions, which consists of 21,312 samples and 18 cell classes. • Pancreas (Panc) (Chen et al. 2023) contains data from five human pancreas studies for cell type annotation tasks, consisting of 14,818 samples and 14 cell classes. • Checkpoint Inhibitor Colitis (CIC) (Thomas et al. 2024) reveals that the crosstalk between circulating T cells and epithelial cells is critical to PD-1/CTLA-4-dependent tolerance and barrier function in this disease, which consists of 118,818 samples and 8 classes. • Myasthenia Gravis (MG) (Zhong et al. 2023) identifies a unique subset of monocytes, displaying significant proinflammatory pathways during and after the crisis, which consists of 53,748 samples and 25 classes. • Systemic Lupus Erythematosus (SLE) (Perez et al. 2022) shows increased type 1 interferon-stimulated genes, with fewer naive CD4 T cells and more cytotoxic GZMH CD8 T cells, consisting of 79,322 samples and 14 cell classes.

Table 1: Test Accuracy $( \% )$ of the linear classifier on ten expression classification datasets. Raw Data denotes directly using expressions as input. Baseline denotes the reproduced default scGPT (Cui et al. 2024), with binning preprocessing, compared to Baseline $\scriptstyle { \mathit { l o g } }$ using log preprocessing. Sampling denotes the sampling strategy in the testing, including traditional random sampling and weighted sampling $\mathrm { L T } _ { t e s t }$ . Results are the mean of five independent trials.   

<html><body><table><tr><td>Config</td><td>Sampling</td><td>Myel</td><td>MS</td><td>Panc</td><td>CIC</td><td>MG</td><td>SLE</td><td>scF</td><td>LIC</td><td>SD</td><td>LM</td><td>Average</td></tr><tr><td colspan="2">RawData</td><td>14.514</td><td>16.519</td><td>38.792</td><td>33.752</td><td>32.913</td><td>40.157</td><td>21.380</td><td>24.386</td><td>39.060</td><td>19.897</td><td>28.137</td></tr><tr><td rowspan="2">Baseline</td><td></td><td></td><td>83.131</td><td>91.084</td><td>51.821</td><td>81.028</td><td>74.140</td><td>67.943</td><td>92.349</td><td>82.601</td><td>59.650</td><td>75.221</td></tr><tr><td>RLiteom</td><td>68.469</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Baselinelog</td><td>Randeom</td><td></td><td>89.00</td><td>97.602</td><td>53.152</td><td>81.595</td><td>77.488</td><td>68.338</td><td>92.994</td><td>82.982</td><td>6.646</td><td></td></tr><tr><td></td><td>70.395</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>78.028</td></tr><tr><td rowspan="2">MVCaug</td><td></td><td></td><td>88.930</td><td></td><td>55.557</td><td>82.509</td><td>77.624</td><td>67.135</td><td>93.558</td><td>84.315</td><td>70.85</td><td></td></tr><tr><td>Randcom</td><td>70.607</td><td></td><td>97.760</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>78.886</td></tr><tr><td rowspan="2">MVCcl s</td><td>Random</td><td></td><td>88.789</td><td></td><td></td><td></td><td></td><td></td><td>93.797</td><td></td><td></td><td></td></tr><tr><td></td><td>71.259</td><td></td><td>97.737</td><td>55.815</td><td>83.131</td><td>77.951</td><td>68.388</td><td></td><td>84.447</td><td>70.307</td><td>79.1601</td></tr><tr><td rowspan="2">LTtrain</td><td>Random</td><td>72.858</td><td>89.005</td><td>97.949</td><td>56.295</td><td>82.929</td><td>78.523</td><td>68.388</td><td>93.709</td><td>83.944</td><td>68.986</td><td>79.259</td></tr><tr><td>LTtest</td><td>73.647</td><td>89.403</td><td>98.277</td><td>63.103</td><td>86.459</td><td>78.725</td><td>68.631</td><td>95.621</td><td>86.676</td><td>92.685</td><td>83.323</td></tr></table></body></html>

<html><body><table><tr><td>Config</td><td>Sampling</td><td>Myel</td><td>MS</td><td>Panc</td><td>CIC</td><td>MG</td><td>SLE</td><td>scF</td><td>LIC</td><td>SD</td><td>LM</td><td>Average</td></tr><tr><td></td><td>RawData</td><td>9.408</td><td>11.175</td><td>12.584</td><td>35.834</td><td>49.723</td><td>16.550</td><td>24.989</td><td>10.305</td><td>47.164</td><td>16.488</td><td>23.422</td></tr><tr><td rowspan="2">Baseline</td><td>Random</td><td></td><td>75.912</td><td>83.764</td><td>48.916</td><td>78.085</td><td>64.844</td><td>62.345</td><td>8.528</td><td>80.293</td><td>48.594</td><td>69.389</td></tr><tr><td>LTtest</td><td>62.615</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Baselinelog</td><td>Random</td><td>67.972</td><td>82.265</td><td>94.525</td><td>50.901</td><td>78.912</td><td>63.735</td><td>63.608</td><td>90.143</td><td>81.755</td><td>58.811</td><td>73.263</td></tr><tr><td>LTtest</td><td>69.332</td><td>82.521</td><td>94.593</td><td>54.223</td><td>82.151</td><td>63.317</td><td>62.911</td><td>93.084</td><td>83.524</td><td>87.753</td><td>77.341</td></tr><tr><td rowspan="2">MVCaug</td><td>Random</td><td>68.093</td><td>83.897</td><td>96.536</td><td>53.703</td><td>79.831</td><td>61.780</td><td>61.647</td><td>91.317</td><td>82.249</td><td>60.770</td><td>73.982</td></tr><tr><td>LTtest</td><td>69.029</td><td>84.632</td><td>96.968</td><td>58.452</td><td>82.593</td><td>61.879</td><td>61.991</td><td>93.897</td><td>83.920</td><td>89.850</td><td>78.321</td></tr><tr><td rowspan="2">MVCcls</td><td>Random</td><td>69.707</td><td>84.589</td><td>96.361</td><td>54.167</td><td>81.025</td><td>62.712</td><td>63.729</td><td>92.936</td><td>82.787</td><td>62.015</td><td>75.003</td></tr><tr><td>LTtest</td><td>70.020</td><td>84.833</td><td>97.179</td><td>58.280</td><td>83.580</td><td>62.780</td><td>64.709</td><td>94.922</td><td>84.428</td><td>91.446</td><td>79.218</td></tr><tr><td rowspan="2">LTtrain</td><td>Random</td><td>71.133</td><td>84.992</td><td>97.580</td><td>54.421</td><td>80.924</td><td>62.147</td><td>64.730</td><td>92.126</td><td>82.976</td><td>60.056</td><td>75.108</td></tr><tr><td>LTtest</td><td>72.059</td><td>85.549</td><td>97.926</td><td>60.035</td><td>84.447</td><td>62.404</td><td>64.770</td><td>94.593</td><td>85.201</td><td>90.788</td><td>79.777</td></tr></table></body></html>

Table 2: Test Accuracy $( \% )$ of the $\mathbf { k }$ -nearest neighbors on ten expression classification datasets. Other settings are the same a those in Table 1. Results are the mean of five independent trials.

• scFoundation (scF) (Hao et al. 2024) employed Zheng68K dataset (Zheng et al. 2017), a study for human peripheral blood mononuclear cell, for the expression classification task. We adopt the dataset, which consists of 6,595 samples and 11 classes. • Leptomeningeal Immune Cells (LIC) (Remsik et al. 2023) finds that leptomeningeal-specific $\mathrm { I F N } { - } \gamma$ signaling is critical for cancer cell growth independent of adaptive immunity, consisting of 20,676 samples and 10 classes. • Severe Dengue (SD) (Ghita et al. 2023) defines the target cells of the dengue virus and the immunological hallmarks of severe dengue (SD) progression in children’s blood, consisting of 193,727 samples and 23 classes. • Leptomeningeal Metastasis (LM) (Chi et al. 2020) finds that macrophages produce inflammatory cytokines in the cerebrospinal fluid, supporting cancer cell growth, consisting of 10,650 samples and 13 classes.

Perturbation Datasets. For conducting perturbation classification and regression, we select 3 perturbation datasets.

• Adamson (Adamson et al. 2016) applies Perturb-seq to dissect the mammalian unfolded protein response, which consists of 63,585 and 76 classes of perturbations. • Dixit (Dixit et al. 2016) develops Perturb-seq, combining transcriptomic and CRISPR-based perturbations to enable the large-scale analysis of complex phenotypes, which consist of 44,735 samples and 20 classes.

• Norman (Norman et al. 2019) reveals the relationship between the set of genes a cell expresses and its phenotype, which consists of 53,508 samples and 101 classes.

# Implementation Details

To ensure consistent and fair comparisons, we standardized the experimental setup for all architectures during pretraining and downstream tasks. For pre-training, without specific note, a subset of data was used, with a 6-layer network, hidden dimensions of 256, a batch size of 128, and a gene length of 512. The Adam (Kingma 2014) optimizer with a learning rate of 0.0002 was employed over 10 epochs. For downstream tasks, we split the datasets into $70 \%$ training and $30 \%$ testing, standardizing the settings to 50 epochs and batch size of 64, with an Adam optimizer and 0.005 learning rate. For classifications, we performed the KNearest Neighbors algorithm with 10 neighbors or a single linear layer classifier. For regressions, we used a trainable embedding to learn perturbed genes. Our downstream experiments froze the pre-trained model and were independently repeated to ensure fairness and reproducibility, and thus our baseline is reproduced scGPT (Cui et al. 2024) with their de

Table 3: Test Accuracy $( \% )$ of the linear classification for different architectures on ten expression classification datasets. Baseline $2$ denotes $\mathbf { M V C } _ { c l s }$ with $\mathrm { L T } _ { t e s t }$ in Table 1, offering a competitive alternative to the original one. MLP and CNN denote the direct implementation, and Ours denote the adaptations formulated in Eq. (5) and Eq. (6), respectively. $\mathrm { T r m } ^ { * }$ denotes the optimal combination of settings selected in Table 4 and Table 5.   

<html><body><table><tr><td>Model</td><td>Params</td><td>FLOPs</td><td>Myel</td><td>MS</td><td>Panc</td><td>CIC</td><td>MG</td><td>SLE</td><td>scF</td><td>LIC</td><td>SD</td><td>LM</td><td>Average</td></tr><tr><td>Baseline2</td><td>2.44M</td><td>2.01G</td><td>71.755</td><td>88.915</td><td>98.174</td><td>61.072</td><td>85.711</td><td>77.789</td><td>68.327</td><td>95.389</td><td>86.237</td><td>92.134</td><td>82.550</td></tr><tr><td>MLP</td><td>0.86M</td><td>0.40G</td><td>39.636</td><td>43.165</td><td>75.043</td><td>46.788</td><td>65.889</td><td>53.820</td><td>45.255</td><td>68.373</td><td>69.608</td><td>54.931</td><td>56.251</td></tr><tr><td>MLP-Ours</td><td>0.64M</td><td>0.29G</td><td>72.605</td><td>89.324</td><td>96.446</td><td>56.780</td><td>84.225</td><td>76.209</td><td>67.954</td><td>95.428</td><td>86.088</td><td>89.093</td><td>81.415</td></tr><tr><td>CNN</td><td>3.61M</td><td>0.60G</td><td>72.018</td><td>88.086</td><td>97.445</td><td>55.153</td><td>84.080</td><td>72.916</td><td>66.630</td><td>94.977</td><td>85.296</td><td>90.532</td><td>80.713</td></tr><tr><td>CNN-Ours</td><td>3.61M</td><td>0.60G</td><td>71.619</td><td>88.104</td><td>97.436</td><td>57.460</td><td>84.604</td><td>73.636</td><td>67.246</td><td>95.048</td><td>85.952</td><td>91.227</td><td>81.233</td></tr><tr><td>Mamba</td><td>2.58M</td><td>0.43G</td><td>71.765</td><td>89.124</td><td>97.958</td><td>59.171</td><td>84.723</td><td>73.153</td><td>67.984</td><td>94.857</td><td>85.999</td><td>92.034</td><td>81.677</td></tr><tr><td>Trm*</td><td>21.87M</td><td>21.74G</td><td>74.269</td><td>90.206</td><td>98.475</td><td>70.068</td><td>87.084</td><td>81.165</td><td>70.025</td><td>96.395</td><td>88.572</td><td>93.298</td><td>84.956</td></tr></table></body></html>

<html><body><table><tr><td>Structure</td><td>Params FLOPs</td><td>KNN</td><td>Linear</td></tr><tr><td>Baseline2</td><td>2.44M 2.01G</td><td>79.218</td><td>82.550</td></tr><tr><td>(3,256)</td><td>1.25M 1.01G</td><td>79.003</td><td>82.460</td></tr><tr><td>(9,256)</td><td>3.63M 3.02G</td><td>78.738</td><td>82.360</td></tr><tr><td>(12,256)</td><td>4.81M 4.03G</td><td>78.856</td><td>82.541</td></tr><tr><td>(6,128)</td><td>0.61M 0.70G</td><td>77.559</td><td>80.328</td></tr><tr><td>(6, 512)</td><td>9.73M 6.44G</td><td>77.817</td><td>83.239</td></tr><tr><td>(6,768)</td><td>21.87M 13.29G</td><td>77.605</td><td>83.376</td></tr><tr><td>(12,512)</td><td>19.20M 12.88G</td><td>77.736</td><td>83.003</td></tr></table></body></html>

Table 4: Test Accuracy $( \% )$ of different Transformer structures. Numbers in the Structure column denote the layers and hidden dimensions of the models, respectively. Params and FLOPs are calculated for the encoder, excluding embedding and output layers for clarity. Results are averaged over ten expression classification datasets.

<html><body><table><tr><td>Mask</td><td>Seq</td><td>KNN</td><td>Linear</td></tr><tr><td colspan="2">Baseline2</td><td>79.218</td><td>82.550</td></tr><tr><td>30% 50% 60%</td><td>512</td><td>78.704 78.887</td><td>82.290 82.589</td></tr><tr><td rowspan="3">40%</td><td>256</td><td>78.585 77.748</td><td>82.534 81.634</td></tr><tr><td>768</td><td>79.713</td><td>83.073</td></tr><tr><td>1024</td><td>79.395</td><td>82.972</td></tr></table></body></html>

Table 5: Test Accuracy $( \% )$ of different Transformer settings during pre-training. Seq denotes the training gene length. Other settings are the same as those in Table 4.

fault pre-processing and loss under our rigorous settings for fair comparison.

# Results and Analysis

Through the following Q&A, we provide in-depth discussions of the experimental results, mainly reported on largescale expression classification, pertaining to the three proposed problems and corresponding solutions, offering a comprehensive analysis of our benchmark.

Q1. How does weighted sampling solve the long-tailed distribution problem?

A1. As shown in Table 1 and Table 2, the weighted sam

𝑳𝒎𝒔𝒆 Transformer 𝑳𝒎𝒔𝒆 Transformer   
1.2 MLP-Ours 0.15 MLP-Ours   
0.34 CMNaNm-bOaurs 0.13 CMNaNm-bOaurs 0.11   
0.1 0 𝒆𝟒 2𝒆𝟒 3𝒆𝟒 0.09 0 2 4 6 8 10 Iteration Epoch (a) Training Loss (b) Testing Loss

pling strategy (abbreviated as LT) almost improves performance across all settings. Specifically, during testing, the improvements of LT (highlighted by shading) are methodagnostic. By integrating LT in the training, the best performance is achieved, surpassing the best settings without considering the long-tailed problem, i.e., $\mathbf { M V C } _ { c l s }$ with averaged improvements of $4 . 2 \%$ and $4 . 8 \%$ for Linear and KNN, respectively, and by combining other tricks, surpassing Baseline with significant averaged improvements of $8 . 1 \%$ and $1 0 . 4 \%$ for Linear and KNN, respectively, thereby establishing a new strong baseline.

Q2. Which preprocessing and cell embeddings are better?

A2. Through Table 1 and Table 2, we find that some default settings in Baseline (Cui et al. 2024) are suboptimal. Improvements can be achieved through simple modifications, such as replacing the binning preprocessing with log preprocessing, incorporating MVC loss during training, and using cls features for cell embeddings instead of averaged token features. By implementing these adjustments, performance is improved by $3 . 9 \%$ and $5 . 6 \%$ for Linear and KNN, respectively, offering valuable insights for future pre-training.

Q3. Which Transformer setting performs better?

A3. Traditional methods commonly assume that increasing parameters and input length will always lead to better pretraining results. However, as shown in Table 4 and Table 5, this hypothesis is not supported. Optimal performance may be achieved with settings of 6 layers, 768 hidden dimensions, $50 \%$ masking ratio, and 768 input length. These experiments address the misconceptions in traditional methods and provide suggestions for future research.

<html><body><table><tr><td>Type</td><td>Size</td><td>Myel</td><td>MS</td><td>Panc</td><td>CIC</td><td>MG</td><td>SLE</td><td>scF</td><td>LIC</td><td>SD</td><td>LM</td><td>Average</td></tr><tr><td>Baseline2</td><td>1.8M</td><td>71.755</td><td>88.915</td><td>98.174</td><td>61.072</td><td>85.711</td><td>77.789</td><td>68.327</td><td>95.389</td><td>86.237</td><td>92.134</td><td>82.550</td></tr><tr><td>Blood</td><td>1.8M</td><td>70.263</td><td>68.355</td><td>93.846</td><td>65.169</td><td>86.413</td><td>79.521</td><td>70.409</td><td>95.141</td><td>87.385</td><td>92.272</td><td>80.877</td></tr><tr><td>Brain</td><td>1.8M</td><td>63.576</td><td>88.108</td><td>95.088</td><td>52.383</td><td>80.372</td><td>66.541</td><td>62.183</td><td>90.073</td><td>79.471</td><td>83.279</td><td>76.107</td></tr><tr><td>Heart</td><td>1.8M</td><td>70.329</td><td>71.013</td><td>95.700</td><td>58.916</td><td>82.968</td><td>71.307</td><td>63.295</td><td>93.735</td><td>85.355</td><td>89.787</td><td>78.240</td></tr><tr><td>Kidney</td><td>0.8M</td><td>69.049</td><td>70.838</td><td>96.176</td><td>60.680</td><td>83.272</td><td>74.069</td><td>66.609</td><td>94.403</td><td>85.523</td><td>90.982</td><td>79.160</td></tr><tr><td>Lung</td><td>1.8M</td><td>72.054</td><td>71.611</td><td>97.436</td><td>64.040</td><td>85.385</td><td>77.544</td><td>67.984</td><td>95.180</td><td>86.221</td><td>92.278</td><td>80.973</td></tr><tr><td>PCancer</td><td>1.8M</td><td>73.546</td><td>73.162</td><td>97.251</td><td>60.575</td><td>85.763</td><td>77.967</td><td>68.398</td><td>95.767</td><td>86.039</td><td>93.110</td><td>81.158</td></tr><tr><td>Full (20%)</td><td>10.8M</td><td>72.114</td><td>87.529</td><td>97.931</td><td>59.241</td><td>85.159</td><td>78.128</td><td>68.378</td><td>95.505</td><td>86.057</td><td>92.578</td><td>82.262</td></tr><tr><td>Full</td><td>54.6M</td><td>70.830</td><td>84.342</td><td>97.598</td><td>58.563</td><td>84.975</td><td>74.117</td><td>68.055</td><td>94.935</td><td>85.643</td><td>92.059</td><td>81.112</td></tr></table></body></html>

Table 6: Test Accuracy $( \% )$ of the linear classification for pre-training on different organ datasets. Results are reported on ten downstream expression classification datasets. Size denotes the number of training samples.

<html><body><table><tr><td>Models</td><td>Adamson</td><td>Dixit</td><td>Norman</td></tr><tr><td>Baseline</td><td>41.586</td><td>26.801</td><td>22.248</td></tr><tr><td>Baseline2</td><td>43.552</td><td>27.265</td><td>37.336</td></tr><tr><td>Mamba</td><td>42.271</td><td>27.190</td><td>35.362</td></tr><tr><td>MLP-Ours</td><td>40.215</td><td>27.159</td><td>24.799</td></tr><tr><td>CNN-Ours</td><td>41.081</td><td>26.943</td><td>31.433</td></tr><tr><td>Trm*</td><td>45.841</td><td>27.201</td><td>41.959</td></tr></table></body></html>

Table 7: Test Accuracy $( \% )$ of the linear classification for different pre-training settings and architectures on three perturbation classification tasks.

Q4. What is the performance of other architectures?

A4. As we have noted that transcriptomic data is inherently not sequential, we explore other architectures in Figure 3 and find other architectures that could perform similar training and testing processes. Table 3 shows that our straightforward adaptations for MLP and CNN outperform the direct implementations. These alternatives achieve comparable performance compared to Transformers, with the largest average difference being $1 . 3 \%$ . Although there remains a gap compared to Transformers with optimal settings, the computational efficiency of these architectures suggests they have significant potential for this task.

Q5. What is the performance on other downstream tasks?

A5. As shown in Table 7 and Table 8, Transformers achieve the best performance in both perturbation classification and regression tasks. As for classification, the increased number of classes introduces significant challenges, resulting in poor performance across all models. Additionally, the performance gap between other models and Transformers, such as MLP in the Norman classification, and Mamba in regressions, has widened. This indicates the need for further evaluations when designing new architectures.

Q6. Does more training data lead to better performance?

A6. A prevailing trend in transcriptomic pre-training is to collect more data, such as GeneCompass (Yang et al. 2023). As shown in Table 6, by comparing different sizes and types of data for pre-training, we find that more data does not necessarily equate to better performance, thereby challenging traditional assumptions. Additionally, we discover that pretraining on specific organ datasets achieves state-of-the-art (SOTA) performance on certain downstream classifications, such as Blood pre-training on SLE classification, and the possible reason may be the classes in SLE related to blood. These findings suggest that for specific downstream tasks, further pre-training on relevant datasets might be beneficial.

Table 8: Test loss of different pre-training architectures on three perturbation regression tasks, lower is better. The baseline method is not applicable.   

<html><body><table><tr><td>Models</td><td>Adamson</td><td>Dixit</td><td>Norman</td></tr><tr><td>Baseline2</td><td>0.200</td><td>0.046</td><td>0.163</td></tr><tr><td>Mamba</td><td>0.320</td><td>0.130</td><td>0.343</td></tr><tr><td>MLP-Ours</td><td>0.221</td><td>0.046</td><td>0.166</td></tr><tr><td>CNN-Ours</td><td>0.284</td><td>0.103</td><td>0.223</td></tr><tr><td>Trm*</td><td>0.202</td><td>0.042</td><td>0.142</td></tr></table></body></html>

# Conclusion

Through careful analysis of transcriptomic data and existing methods, we identified three overlooked problems: the long-tailed data problem, the model selection problem, and the evaluation problem. To address them, we introduced weighted sampling, specific architecture adaptations, and fair and quantifiable downstream evaluations, respectively. These strategies enable us to investigate the optimal settings and explore the potential of alternative architectures. Additionally, we resolved several misconceptions common in traditional methods, thus providing better insights for largescale pre-training. By developing a simple and comprehensive benchmark, we provided researchers with a testbed for analyzing the pre-trained models, with no need for biological knowledge. Besides the above contributions, we suggest two further directions: 1) Implementing more advanced methods to address the long-tail problem, moving beyond the naive weighted sampling; 2) Integrating other architectures with attention mechanisms to establish a novel framework for transcriptomic pre-training.