# SeeA∗: Efficient Exploration-Enhanced $\mathbf { A } ^ { * }$ Search by Selective Sampling

Dengwei Zhao1, Shikui $\mathbf { T u ^ { 1 * } }$ , Lei $\mathbf { X } \mathbf { u } ^ { 1 , 2 * }$ 1Department of Computer Science and Engineering, Shanghai Jiao Tong University 2Guangdong Institute of Intelligence Science and Technology {zdwccc, tushikui, leixu}@sjtu.edu.cn

# Abstract

Monte-Carlo tree search (MCTS) and reinforcement learning contributed crucially to the success of AlphaGo and AlphaZero, and $\mathbf { A } ^ { * }$ is a tree search algorithm among the most well-known ones in the classical AI literature. MCTS and $\mathbf { A } ^ { * }$ both perform heuristic search and are mutually beneficial. Efforts have been made to the renaissance of $\mathbf { A } ^ { * }$ from three possible aspects, two of which have been confirmed by studies in recent years, while the third is about the OPEN list that consists of open nodes of $\mathbf { A } ^ { * }$ search, but still lacks deep investigation. This paper aims at the third, i.e., developing the Sampling-exploration enhanced $\mathbf { A } ^ { * }$ $\mathbf { ( S e e A ^ { * } ) }$ search by constructing a dynamic subset of OPEN through a selective sampling process, such that the node with the best heuristic value in this subset instead of in the OPEN is expanded. Nodes with the best heuristic values in OPEN are most probably picked into this subset, but sometimes may not be included, which enables $\mathrm { { S e e A } ^ { * } }$ to explore other promising branches. Three sampling techniques are presented for comparative investigations. Moreover, under the assumption about the distribution of prediction errors, we have theoretically shown the superior efficiency of $\mathrm { S e e A ^ { * } }$ over $\mathbf { A } ^ { * }$ search, particularly when the accuracy of the guiding heuristic function is insufficient. Experimental results on retrosynthetic planning in organic chemistry, logic synthesis in integrated circuit design, and the classical Sokoban game empirically demonstrate the efficiency of SeeA∗, in comparison with the state-of-the-art heuristic search algorithms.

# 1 Introduction

In recent years, combining heuristic search algorithms with deep neural networks has demonstrated remarkable performance across a wide range of practical applications, such as board games [48, 50, 49, 66], video games [46, 64, 67], traveling salesman problem [8, 59], de novo drug design [42], retrosynthetic planning [6, 47, 68], logic synthesis [9], and so on. The search algorithm is a slow reasoning process, and heuristic functions serve as counselors to narrow down the search space [2]. Therefore, the effectiveness of search algorithms is significantly influenced by the quality of the guiding functions.

Monte-Carlo tree search (MCTS) is a widely-used, effective algorithm for combinatorial problems. However, if the backup value in MCTS is provided by a heuristic estimator rather than actual rewards, the convergence to the true state value is not guaranteed, leading to compromised search performance. In single-agent problems such as combinatorial puzzles, neural-guided MCTS tends to have a relatively long runtime and often generates solutions that are considerably longer than the shortest path [1].

$\mathbf { A } ^ { * }$ search [26] is a best-first search algorithm that expands nodes with the minimum total path value $f$ at each step. The evaluation function $f ( n )$ on a node $n$ is defined as the summation of $g ( n )$ , the accumulated cost from the initial node $n _ { 0 }$ to $n$ , and $h ( n )$ , the expected cost from $n$ to the goal, i.e.,

$$
f ( n ) = g ( n ) + h ( n ) .
$$

Notice that $g ( n )$ computes the cost from the known searching trajectory, whereas $h ( n )$ is a heuristic function to estimate the cost of the future path from $n$ to the goal. In practice, it is usually difficult to obtain an accurate $h ( n )$ . $\mathbf { A } ^ { * }$ search is guaranteed to find the optimal solution if $h ( n )$ is admissible, i.e., $h ( n )$ never overestimates the real cost to the goal. However, due to its best-first expansion strategy, $\mathbf { A } ^ { * }$ has limited exploration capability. If $f ( n )$ deviates from the true cost function $f ^ { * } ( n )$ too much, $\mathbf { A } ^ { * }$ search may become trapped in local optimal branches, and significant efforts are required to resume expansion within the optimal branch. Consequently, the computational efficiency of $\mathbf { A } ^ { * }$ search is compromised in practical applications, even though the optimality of $\mathbf { A } ^ { * }$ might still hold under the guidance of $f ( n ) { \big \vert }$ .

MCTS and $\mathbf { A } ^ { * }$ both perform heuristic search. MCTS and reinforcement learning with the help of deep learning contributed crucially to the successes of AlphaGo and AlphaZero, which aroused the interest of comparing MCTS and $\mathbf { A } ^ { * }$ for possible mutual benefits. Deep learning is also able to contribute to the renaissance of $\mathbf { A } ^ { * }$ , three possible aspects are addressed with a family of possible improvements proposed under the name of Deep IA-search [61]. The first and also straightforward aspect is estimating $f ( n )$ with the help of deep learning, which makes current studies on $\mathbf { A } ^ { * }$ including this paper into the era of learning aided $\mathbf { A } ^ { * }$ . The second aspect is seeking a better estimation of $f ( n )$ with the help of global or future information, featured by two typical mechanisms. One is lookahead or scouting before expanding the current node to collect future information to revise $f ( n )$ of the current node, which takes a crucial rule for the success of AlphaGo [48] and also used more than 30 years ago in Algorithm CNneim-A [62]. The other is path consistency, that is, $f ( n )$ values on one optimal path should be identical, which has been further confirmed in recent studies [66, 67, 68]. This third aspect is about selecting nodes among the OPEN list that consists of open nodes of $\mathbf { A } ^ { * }$ . It is an old tune even in the classical era of $\mathbf { A } ^ { * }$ , e.g., one suggestion is dividing OPEN into two sublists OPEN and WAIT according to a priori and a posteriori in a Bayesian evaluation [61]. However, investigation is seldom made on what are effective and efficient ways for selecting among OPEN.

In this paper, SeeA∗ search (short for Sampling-exploration enhanced $\mathbf { A } ^ { * }$ ) algorithm is proposed by incorporating exploration behavior into $\mathbf { A } ^ { * }$ search to target at the third aspect. The main contributions are summarized below.2

• SeeA∗ search employs a selective sampling process to screen a dynamic candidate subset $\mathcal { D }$ from the set $\mathcal { O }$ of open nodes that are awaiting expansion. The next expanding node is selected from $\mathcal { D }$ , and it may not be the node that has the best heuristic value in $\mathcal { O }$ and will be selected by $\mathbf { A } ^ { * }$ , enabling ${ \mathrm { S e e A } } ^ { * }$ to explore other promising branches. To reduce the excessive expansion of unnecessary nodes during exploration, only the candidate node with the best heuristic value is expanded. Three sampling strategies are introduced to strike a balance between exploitation and exploration. The search efficiency is improved especially when the guiding heuristic function is not accurate enough.   
• We theoretically prove that $\mathrm { { S e e A } ^ { * } }$ has superior efficiency over $\mathbf { A } ^ { * }$ search when the heuristic value function deviates substantially from the true state value function. SeeA∗ achieves a reduced number of node expansions to identify the optimal path. This performance improvement becomes more pronounced as the complexity of the problems increases and the reliability of the guiding heuristics decreases.   
• Experiments are conducted on two real-world applications, i.e., the retrosynthetic planning problem in organic chemistry and the logic synthesis problem in integrated circuit design, as well as the classical Sokoban game. SeeA∗ outperforms the state-of-the-art heuristic search algorithms in terms of the problem-solving success rate and solution quality while maintaining a low level of node expansions.

# 2 Related work

MCTS [5, 13] utilizes random sampling and tree-based search to efficiently explore search space. Upper Confidence bounds applied to Trees with predictor (PUCT) have been employed by AlphaZero [49], achieving super-human performance in board games. $\mathbf { A } ^ { * }$ search is widely employed for solving optimization problems, such as route planning [54, 53], cubic and puzzle games [1], robotics [17], and so on. Many variants of $\mathbf { A } ^ { * }$ search have been proposed for performance improvement. Weighted $\mathbf { A } ^ { * }$ search $( \mathrm { W A } ^ { * } )$ [18] biased the expanding policy towards states closer to the goal by

$$
n ^ { * } = \arg \operatorname* { m i n } _ { n } g ( n ) + \varepsilon h ( n ) ,
$$

where $\varepsilon$ is a hyperparameter to adjust the weight of the heuristic estimation $h$ . WA∗ with iteratively decreasing weights is employed by the LAMA planner [27, 44], achieving promising results in various domains including Sokoban. DeepCubeA [1] trained heuristic functions by reversing solution pathways from the goal state to guiding the search process of $\mathrm { \mathbf { W } A } ^ { * }$ . Commonly, $\mathrm { \mathbf { W } A ^ { * } }$ traded optimality for speed, and increasing $\varepsilon$ was assumed to result in faster searches. Additionally, the greedy search based on $h$ values was considered the fastest search. However, empirical observations revealed that increasing $\varepsilon$ slowed down the search in some domains. Greedy search is fast if and only if there is a strong correlation between the heuristic estimations and the true distance-to-go, or if the heuristic is extremely accurate [55]. However, constructing a reliable heuristic function for complicated problems is challenging attributed to the vast search space and the difficulties associated with sample collection in real-world applications. Poor generalization performance also remains a pervasive issue across diverse practical domains, such as retrosynthetic planning. This paper sets out to develop an efficient search algorithm designed to minimize the adverse effects of inaccurate predictions by heuristic functions.

There have been some preliminary studies on the integration of exploration into the $\mathbf { A } ^ { * }$ search. $\varepsilon -$ greedy node selection was incorporated into LAMA, suggesting that exploration can improve the coverage of search algorithms even multiple enhancements were already employed [52]. Type-WA∗ [11] augments $\mathrm { \mathbf { W } A } ^ { * }$ with type-based exploration [57] in the focal list [40]. The search space nodes are divided into $T$ distinct groups, and one of these groups is randomly chosen to determine the expanded node. Levin tree search (LevinTS) [38] combined a penalization mechanism based on node depth to encourage exploration for $\mathbf { A } ^ { * }$ search. Policy-guided heuristic search (PHS) [39] generalized LevinTS by introducing a heuristic factor, guided by both a value function and a policy. When the guiding heuristics are sufficiently accurate, the best-first search achieves optimal efficiency without the need for exploration. Insufficient exploration leads the search algorithm to be trapped in local optima guided by inaccurate heuristics. As the accuracy of the guiding heuristic diminishes, the importance of exploration becomes more pronounced in order to mitigate the potential misguidance.

Search algorithms have played a crucial role in solving diverse real-world problems, such as retrosynthetic planning and logic synthesis. Retrosynthetic planning aims to identify a feasible synthetic route using known available building block molecules for a given target molecule. Considering that the synthesis of target molecules typically requires multiple steps and each step encompasses a substantial number of potential chemical reactions, retrosynthetic planning is formulated as a search problem to identify the optimal synthetic pathway. Both MCTS [28, 47, 65] and $\mathbf { A } ^ { * }$ search, such as Retro∗ [6] and its descendants [24, 30, 33, 58], have demonstrated promising results in retrosynthetic planning. Logic synthesis (LS) is a crucial step in the design of integrated circuits, mapping the high-level logic circuit description into gate-level implementation. In recent years, reinforcement learning algorithms [10, 29, 34, 41, 69] and search methods [9, 37] have shown promising results in the field of LS. Besides, Sokoban is an NP-hard [16] and PSPACE-complete [14] problem, which is a benchmark problem for evaluating the performance of artificial intelligence planning algorithms. Recently, combining reinforcement learning algorithms with search-based methods has demonstrated remarkable performance in effectively solving the Sokoban problem [19, 20, 22, 31, 43].

# 3 Preliminaries and limitations on $\mathbf { A } ^ { * }$ search

Single-agent problems solved in this paper are formulated as Markov decision processes. Let $\mathcal { N }$ represent the set of nodes in the search tree, where each node $n \in \mathcal N$ corresponds to a state $s$ in the state space $s$ . The set of $n$ ’s children is represented as $C H ( n )$ . The root of the tree and the initial state are denoted as $n _ { 0 }$ and $s _ { 0 }$ respectively. At each interactive step, action $a _ { t } \in \mathcal A$ is applied to the current state $s _ { t }$ , resulting in the subsequent state $s _ { t + 1 } = T ( s _ { t } , a _ { t } )$ and transition cost $c _ { t + 1 } = c ( s _ { t } , a _ { t } )$ , where $\tau$ is the state transition function to obtain the following state $s _ { t + 1 }$ when taking action $a _ { t }$ at state $s _ { t }$ , and $c$ is the cost function giveing the received cost when taking action $a _ { t }$ at state $s _ { t }$ ..

The search tree of $\mathbf { A } ^ { * }$ contains two distinct types of nodes: closed nodes, which have already been expanded, and open nodes, which are waiting to be expanded [26]. Let $\mathcal { O }$ and $\mathcal { C }$ denote the set of open nodes and closed nodes respectively. The search process of $\mathbf { A } ^ { * }$ can be summarized as follows:

• Step 1: Initialize $n _ { 0 }$ with $s _ { 0 }$ , and mark it as open node by setting $\mathcal { O }  \{ n _ { 0 } \}$ , $c \gets \emptyset$ . • Step 2: Select the node $n$ with the lowest total path cost $f ( n )$ from the open set $\mathcal { O }$ , i.e., $\begin{array} { r } { n = \arg \operatorname* { m i n } _ { n ^ { \prime } \in \mathcal { O } } f ( n ^ { \prime } ) } \end{array}$ . • Step 3: If the node $n$ is the goal, terminate the search process successfully. Otherwise, expand the node $n$ , and update ${ \mathcal { C } } \gets { \mathcal { C } } \cup \{ n \}$ , $\mathcal { O }  \mathcal { O } \cup \mathcal { \bar { C } } H ( n ) \setminus \{ n \}$ . • Step 4: Repeat step 2 and 3 until $\mathcal { O }$ becomes empty, or exceeding the predetermined maximum runtime or the number of expanded nodes, terminating with failure.

$\mathbf { A } ^ { * }$ search always selects the node with the best heuristic value from the open set without exploration. When the heuristic function $f$ can accurately estimate the true cost $f ^ { * }$ , this best-first search is the most efficient. However, if the estimation by $f$ is not accurate enough, the node with the minimum $f$ value may not correspond to the optimal one, which instead has the lowest $f ^ { * }$ value. The search process might be trapped in a local optimal branch, and substantial computational efforts are required to resume expansion on the optimal branch, which diminishes the efficiency of the search algorithm. Considering an example in Figure 1(a), suppose the cost for each step (or edge) on the optimal path is 100, and on the non-optimal path is only 1. The true total path cost at any node $n$ is given by $f ^ { * } ( n ) = g ( n ) + h ^ { * } ( n )$ , where $g ( n )$ is given by adding the costs from the root to the node $n$ , and the real future cost $h ^ { * } ( n )$ is a summation of all costs from $n$ to the end (or terminal state). Suppose the evaluation function $f ( n ) = g ( n ) + h ( n )$ by Equation 1 is exact on the optimal path but underestimates the real cost otherwise. Specifically, define the heuristic function $h ( n )$ as follows:

$$
h ( n ) = { \left\{ \begin{array} { l l } { h ^ { * } ( n ) , } & { { \mathrm { i f ~ } } n { \mathrm { ~ i s ~ o n ~ t h e ~ o p t i m a l ~ p a t h } } } \\ { 0 , } & { { \mathrm { O t h e r w i s e } } } \end{array} \right. }
$$

Then, $h ( n )$ satisfies the admissible assumption as it never overestimates the cost, and $h ( n ) \leq h ^ { * } ( n )$ is established for all nodes. Therefore, $\mathbf { A } ^ { * }$ is guaranteed to find the optimal solution guided by $h ( n )$ in Equation 3. However, as illustrated in Figure 1(b), guided by the defined heuristic $h$ , the nodes on the optimal path will not be expanded until all nodes on non-optimal branches with depths less than 200 have been expanded. The optimal solution is achieved within two steps under the guidance of $f ^ { * }$ , and the search efficiency of $\mathbf { A } ^ { * }$ search is largely compromised when $f ( n )$ is not accurate enough.

# 4 Method

${ \mathrm { S e e A } } ^ { * }$ search is proposed on the basis of $\mathbf { A } ^ { * }$ search by introducing a candidate set $\mathcal { D }$ of open nodes to provide exploration behavior. Three selective sampling strategies are presented for constructing the candidate set. Moreover, we present a theoretical analysis on the efficiency of SeeA∗.

# 4.1 SeeA∗ search algorithm

${ \mathrm { S e e A } } ^ { * }$ employs the following two steps to replace the Step 2 in $\mathbf { A } ^ { * }$ search. First, a selective strategy is employed to sample a set of candidate nodes $\mathcal { D }$ from the opening set $\mathcal { O }$ . Then, the node $n$ with the lowest $f$ -value from the candidate set $\mathcal { D }$ , instead of $\mathcal { O }$ , is chosen to be expanded in Step 3. The details of SeeA∗ are summarized in Algorithm 1 in Appendix A.

• Step $2 a$ : Sample a candidate subset $\mathcal { D }$ from $\mathcal { O }$ .   
• Step $2 b$ : Select the node $n$ with the lowest $f$ -value from the candidate set $\mathcal { D }$ .

As illustrated in Figure $1 ( \mathrm { c } ) \& ( \mathrm { d } )$ , if the node with minimum $f$ -value is not sampled into the candidate set $\mathcal { D }$ in Step $2 a$ , the node selected to be expanded later is not the same as the one by $\mathbf { A } ^ { * }$ search, which activates exploration on other branches. Step $2 b$ excludes the unpromising nodes by the $f$ -value.

Node expanded Node not expanded Goal node Open set O CTransition cost Optimal branch Suboptimal branch Candidate set Dc 0 o f\*(no)= 200 no f(no)= 200 no n\*(selected byA\*&SeeA\*) 100/ 100 n\*= argminneof(n) = argminneDf(n) n1,1 n2,1 n1,1 n2,1 f(n2,i)=1 (c) f"(mu) 100 1 fnu) 100 1 Open set 0 n2,2 n2,2 f(n2.2)= 2 n(selected by A\*) n1,2 f\*（(n2,i) n1,2 。 ： ：104 ： ： Candidate set DcO n2,200 n2,200 f(n2,200) = 200 n\*(selected by SeeA\*) ： ： ： $n _ { 1 } ^ { * } = \mathrm { a r g m i n } _ { n \in \mathcal { O } } f ( n )$ n2,104 n2,104 f(n2.104) = 104 $n _ { 2 } ^ { * } = \mathop { \mathrm { a r g m i n } } _ { n \in \mathcal { D } } f ( n )$ Exploration is enabled! (a) (b) (d)

# 4.1.1 Uniform sampling strategy

Uniform sampling guarantees an equal selection probability for each node, thereby generating a representative subset that has the same distribution of the population. The procedure is given in Algorithm 2 in Appendix A. If the desired number of candidate nodes, denoted as $K$ , is greater than the number of open nodes, the open set $\mathcal { O }$ is used as $\mathcal { D }$ . Otherwise, $K$ nodes are randomly selected from the open nodes as $\mathcal { D }$ . It should be noted that $\mathrm { { S e e A } ^ { * } }$ with uniform sampling is different from the $\varepsilon$ -Greedy method. The $\varepsilon$ -Greedy activates exploration with probability $\varepsilon$ and then uniformly samples a node for expansion, which may expand low-quality nodes. In Step $2 a$ of $\mathrm { S e e A } ^ { * }$ , uniform sampling is very likely to include at least one high-quality node with a reasonably low $f$ -value and the node will be selected to expand in Step $2 b$ . More discussions are referred to Appendix P.

# 4.1.2 Clustering sampling strategy

In the uniform sampling strategy, each node is selected with equal probability. However, there is a non-negligible probability that all sampled nodes are of low quality, leading to the exclusion of nodes along the optimal expansion path from the candidate set $\mathcal { D }$ . Therefore, a clustering sampling strategy is proposed, and it partitions open nodes into multiple clusters and subsequently sampling nodes from each cluster, as illustrated in Figure 2 in Appendix B. At least one node from each cluster is sampled compulsorily. Consequently, the probability of including nodes on the optimal branch is increased, thereby facilitating search efficiency. On the other hand, uniform sampling strategy is equivalent to assume that the nodes follow a Gaussian distribution, whereas clustering sampling strategy assumes that the nodes follow a Gaussian mixture distribution from multiple clusters, which provides a more descriptive representation for sampling.

To reduce computational costs, competitive learning [51] is utilized for node clustering. After each node expansion, the incorporation of newly generated nodes into the set $\mathcal { O }$ resembles the process of online sample acquisition in competitive learning. A clustering process is conducted simultaneously with the search process. Offline clustering algorithms, such as K-means or Gaussian mixture model, require recalculating the clustering when incorporating new nodes, thereby imposing additional computational overhead. Each node is represented by a vector extracted by a function $f _ { h }$ . $N _ { c }$ cluster centers are randomly initialized as vectors with the same dimension of node embedding. During each expansion, the newly generated nodes are assigned to the cluster with the closest center separately, and the cluster center is updated by moving toward the position of the freshly added node. While preparing the candidate set $\mathcal { D }$ , nodes are sampled evenly from each cluster, and uniform sampling is employed to select nodes from each cluster. Details are displayed in Algorithm $3 \& 5$ in Appendix A.

# 4.1.3 UCT-like sampling strategy

In AlphaZero [49], PUCT achieved a good balance between exploitation and exploration with promising results. In light of this, a UCT-like sampling strategy is proposed. Due to the absence of Monte Carlo simulations, estimated $f$ values are employed to substitute the $Q$ value in PUCT, which is the average backup value obtained from multiple MCTS simulations. The depth of the node is employed as the penalization for exploration [38]. Each node is evaluated by

$$
E ( n ) = f ( n ) - c _ { b } \times \frac { \sqrt { d _ { m a x } } } { 1 + d ( n ) } ,
$$

where $c _ { b }$ is an adjustable hyperparameter, $d ( n )$ is the depth of node $n$ , and $d _ { m a x }$ is the maximum depth of the open nodes. Nodes with smaller $d ( n )$ are more likely to be included in the candidate set for exploration. Despite potential errors in $f$ value estimation, it remains a viable node evaluation metric to sample high-quality nodes, and the exploration term is beneficial in mitigating misleading of prediction errors. The $K$ nodes with the smallest $E$ values are chosen to constitute the candidate set $\mathcal { D }$ . The details are summarized in Algorithm 4 in Appendix A.

# 4.2 Efficiency of SeeA∗ search

We further provide a theoretical analysis on the efficiency of $\mathrm { S e e A ^ { * } }$ , demonstrating that ${ \mathrm { S e e A } } ^ { * }$ is superior to $\mathbf { A } ^ { * }$ when the guiding heuristic function $f$ does not estimate the true cost $f ^ { * }$ accurately enough. It was claimed in $\mathbf { A } ^ { * }$ search [26] that the $f ^ { * }$ values of all nodes on the optimal path are equal to the same cost $\mu _ { 0 } ^ { f }$ and lower than the $f ^ { * }$ value of nodes outside the optimal path, which was assumed to be sampled from a Gaussian distribution in [62]. In this paper, the prediction error for $f ^ { * }$ is assumed to follow a uniform distribution. Here, Gaussian distribution is denoted as $\mathcal { G } ( \cdot , \cdot )$ and uniform distribution is denoted as $\mathcal { U } ( \cdot , \cdot )$ . Formally, an assumption is made as follows.

Assumption 4.1 For each node n on the optimal path, $f ( n ) \sim \mathcal { U } ( \mu _ { 0 } ^ { f } - \sigma , \mu _ { 0 } ^ { f } + \sigma )$ . For nodes not on the optimal path, $f ( n ) \sim \mathcal { U } ( f ^ { * } ( n ) - \sigma , f ^ { * } ( n ) + \mathbf { \bar { \sigma } } )$ , and $\{ f ^ { * } ( n ) \}$ are independently and identically sampled from $\mathcal { G } ( \mu _ { 1 } ^ { f } , \sigma _ { s } ^ { 2 } )$ .

The $\mu _ { 0 } ^ { f }$ and $\mu _ { 1 } ^ { f }$ are the expected total cost for optimal and non-optimal solutions, respectively. The inequality $\mu _ { 0 } ^ { f } < \mu _ { 1 } ^ { f }$ holds because the optimal path has a lower cost. The $\sigma$ represents the magnitude of the prediction error, and the $\sigma _ { s } ^ { 2 }$ is a constant as the variance. Under Assumption 4.1, we can derive:

Corollary 4.2 For a node n on the optimal path and a node $n ^ { \prime }$ off the optimal path, the probability

$$
p _ { \sigma } = P \left( f ( n ) \leq f ( n ^ { \prime } ) | \sigma \right)
$$

decreases as the prediction error $\sigma$ increases.

It is worth noting that the establishment of Corollary 4.2 is not limited by the assumption of a uniform noise distribution in Assumption 4.1. When the noise follows a Gaussian distribution, Corollary 4.2 is still established. Refer to Appendix C for more detailed derivations.

Without loss of generality, assume the open set $\mathcal { O }$ contains $N _ { o }$ nodes, $\{ n _ { 1 } , n _ { 2 } , \cdots , n _ { N _ { o } } \}$ , and $n _ { 1 }$ is the optimal node. The probability of $\mathbf { A } ^ { * }$ search expanding node $n _ { 1 }$ is

$$
P _ { A } ( \sigma ) = P \left( n _ { 1 } = \arg \operatorname* { m i n } _ { n ^ { \prime } \in { \mathcal O } } f ( n ^ { \prime } ) \big | \sigma \right) = \prod _ { n ^ { \prime } \in { \mathcal O } \setminus \{ n _ { 1 } \} } P \left( f ( n ) \leq f ( n ^ { \prime } ) | \sigma \right) = p _ { \sigma } ^ { N _ { o } - 1 } .
$$

SeeA∗ expands $n _ { 1 }$ with probability

$$
P _ { S } ( \sigma ) = P \left( n _ { 1 } \in \mathcal { D } , n _ { 1 } = \arg \operatorname* { m i n } _ { n ^ { \prime } \in \mathcal { D } } f ( n ^ { \prime } ) \big | \sigma \right) = P ( n _ { 1 } \in \mathcal { D } ) \prod _ { n ^ { \prime } \in \mathcal { D } \setminus \{ n _ { 1 } \} } p _ { \sigma } .
$$

If the uniform sampling strategy is used to select $K$ candidates,

$$
P _ { S } ( \sigma ) = \frac { K } { N _ { o } } p _ { \sigma } ^ { K - 1 } .
$$

Based on Equation 6 & 8, when the prediction error $\sigma$ is large, $\mathrm { S e e A ^ { * } }$ expands the optimal node with a higher probability than $\mathbf { A } ^ { * }$ search at each step, which is given by the following theorem.

Theorem 4.3 $P _ { S } ( \sigma ) > P _ { A } ( \sigma )$ holds if and only if

$$
p _ { \sigma } < H ( N _ { o } ) , w h e r e H ( N _ { o } ) = \left( \frac { K } { N _ { o } } \right) ^ { \frac { 1 } { N _ { o } - K } } , N _ { o } > K \geq 1 .
$$

$H ( N _ { o } )$ is a monotonically increasing function with respect to $N _ { o }$ which is the size of the open set. With increasing branching factors and longer solution paths for more complex problems, $N _ { o }$ grows and $H ( N _ { o } , K )$ monotonically increases with respect to $N _ { o }$ . Especially, we have

$$
\operatorname* { l i m } _ { N _ { o } \to + \infty } H ( N _ { o } ) = 1 .
$$

In this situation, Inequality 9 holds. SeeA∗ tends to demonstrate superior performance compared to $\mathbf { A } ^ { * }$ in solving complex problems.

Notice that if the heuristic function $f$ predicts the true cost $f ^ { * }$ without error, it leads to $p _ { \sigma } = 1$ in Equation 5. Then, Equation 9 does not hold, and in this case, $\mathbf { A } ^ { * }$ search becomes more efficient than SeeA∗. However, learning an accurate heuristic function for complex real-world problems is quite challenging, and large prediction errors usually exist, which leads to small $p _ { \sigma }$ and the establishment of Equation 9. The number of candidate nodes $K$ is a key hyperparameter to balance the exploitation $\mathbf { A } ^ { * }$ and the exploration introduced by SeeA∗. $P _ { S } ( \sigma )$ in Equation 8 reaches its maximum value when $K ^ { * } = - 1 / \log p _ { \sigma }$ . When $p _ { \sigma }$ approaches 1, $K ^ { * }$ will be the largest $\infty$ . In this situation, the candidate set is the same as the open set, and $\mathrm { S e e A ^ { * } }$ degenerates into best-first $\mathbf { A } ^ { * }$ . For small $p _ { \sigma }$ , the optimal $K ^ { * }$ is the smallest value 1 and $\mathrm { S e e A ^ { * } }$ becomes random sampling. An appropriate value of $K$ should be selected according to the specific situation. According to Equation 7, $P _ { S } ( \sigma )$ is related to both $p _ { \sigma }$ and $P ( n _ { 1 } \in \mathcal { D } )$ . Utilizing more efficient sampling algorithms than uniform sampling is also capable to enhance the performance of ${ \mathrm { S e e A } } ^ { * }$ . The clustering sampling and UCT-like sampling aim to achieve a higher $P ( n _ { 1 } \in \mathcal { D } )$ by constructing a more diverse candidate set, thereby enhancing the likelihood of expanding the optimal node.

For simplicity, suppose the probability of selecting the optimal node in a single expansion is $P$ , and the probability for expanding the optimal node becomes $1 - ( 1 - P ) ^ { \tau }$ after $\tau$ expansions. To achieve a probability level of $P _ { m i n }$ for expanding the optimal node, we have

$$
\tau \geq \frac { \log \{ 1 - P _ { m i n } \} } { \log \{ 1 - P \} } .
$$

Based on Theorem 4.3 and Equation 11, SeeA∗ is more efficient than $\mathbf { A } ^ { * }$ search as it requires fewer expansions to find the optimal solution. It is noted that Equation 9 is derived on the uniform sampling strategy. For a more effective sampling strategy with a higher probability $P ( n _ { 1 } \in \mathcal { D } )$ , SeeA∗ will become more efficient as $P _ { S } ( \sigma )$ increases.

# 5 Experiments

Real-world problems are usually complicated, and the amount of available samples for training the heuristic functions is typically small.Two real-world applications, i.e., retrosynthetic planning in organic chemistry and logic synthesis in integrated circuit (IC) design, are considered to evaluate the effectiveness of the proposed method. Since the molecular structures have enormous diversity but in contrast the available experimental data are very limited, the heuristic function to estimate the synthesis cost in retrosynthetic planning suffers from noticeable overfitting problems [68]. Furthermore, the vast chemical reaction space gives rise to a substantial number of branching factors in the search tree, leading to a rapid growth in the quantity of open nodes throughout the search process. Logic synthesis is another practical problem where it is challenging to train a reliable heuristic function to evaluate the solution’s quality, due to the immense diversity of circuit functionalities and variations in design methodologies. Therefore, the above two real-world problems are suitable benchmarks to verify the efficiency of SeeA∗ when the heuristic function is not accurate enough. In addition, Sokoban is a widely-used benchmark for combinatorial optimization solvers. It only permits a maximum of four legal actions at each step, and simulations can be leveraged to generate a substantial amount of data for training high-quality heuristic value estimators. Sokoban is included to verify the impact of an accurate heuristic function on the searching performance. All experiments are conducted using NVIDIA Tesla V100 GPUs and an Intel(R) Xeon(R) Gold 6238R CPU.

# 5.1 Results on retrosynthetic planning

Chemical synthetic pathways are transformed into search trees following the literature [47]. A state is a set of molecules that are able to synthesize the target molecule. The initial state contains only the target molecule. The edges in the search tree represent the chemical reactions that enable state transitions between the connected nodes. The retrosynthetic planning problem is solved if all molecules within a state are available building blocks. A single-step retrosynthetic prediction model is utilized as the policy model to generate potential chemical reactions yielding the input molecule. The 50 chemical reaction templates with the highest probabilities constitute the set of valid actions for the current state. A heuristic function is employed to estimate the synthesis cost of the molecule, given the available building blocks. Each molecule is encoded using a 2048-dimensional Morgan Fingerprint vector [45] as the input for the heuristic functions. Both the single-step retrosynthetic prediction model and the cost estimator are provided by Ret $\mathbf { \hat { \rho } } ^ { * } + [ 3 0 ]$ and used to guide the search algorithm. Details about the guiding heuristics are in Appendix D. The last hidden layer’s output of the cost estimator is employed as the embedding representation of the input molecule.

Experiments are conducted on the widely-used USPTO benchmark, comprising 190 molecules [6]. Commercially available molecules in eMolecules3 are used as building blocks. Since the invocation of the single-step retrosynthetic prediction model contributes the majority of the computational cost, all search algorithms are limited to a maximum of 500 single-step model calls, or 10 minutes of real-time, following previous works [6, 30]. The outputs of the single-step model are cached to avoid duplicate computation when the same molecule is encountered again [36]. The size of the candidate set is set to $K = 5 0$ . In the clustering sampling, the parameter $\eta$ is set to 0.15, and the number of clusters is 5. In the UCT-like sampling, the parameter $c _ { b }$ is set to 0.35. Additional pruning is not considered. Since the prior policy is already clipped at a minimum value of 0.001, Bayes mixing with a uniform policy to avoid zero-probability is not used in LevinTS [38] and PHS [39].

The results on the USPTO benchmark are reported in Table $1 ^ { 4 }$ . Due to the exploration induced by selective sampling, the three $\mathrm { S e e A } ^ { * }$ variants achieve superior performance in terms of the percentage of solved molecules and the average solution length while utilizing minimal wall-clock runtime. Among the three sampling strategies, the UCT-like sampling strategy achieves the best balance between exploration and exploitation. As in the literature [47], predicting the synthetic cost of molecules is challenging, and the cost estimator is not accurate with a non-negligible prediction error $\sigma$ . Then, it is expected and consistent with Theorem 4.3 that best-first search algorithms, including $\mathrm { \Delta W A ^ { * } }$ and PHS, are less efficient because they excessively rely on the values of the heuristic function. MCTS requires more node expansions for problem-solving and generates solutions with longer lengths, which is consistent with the findings in the resolution of combinatorial puzzles [1]. The $\varepsilon$ -Greedy node selection [52] achieves a success rate of $9 2 . 1 1 \%$ , surpassing the performance of $\mathbf { A } ^ { * }$ search and demonstrating the practical benefits of introducing exploration when the reliability of guidance heuristics is compromised.

Six additional datasets are collected from the literature for further comparisons. These datasets comprise 4719 molecules, much more than the USPTO dataset. Details of the datasets are referred to the Appendix E. According to the results in Table 3& 4 in Appendix G, SeeA∗ maintains its superiority over other search algorithms, and SeeA∗(Cluster) has the highest mean success rate of $6 3 . 5 6 \%$ . The clustering sampling and UCT-like sampling are better than uniform sampling in terms of the solved rate and the route length, indicating that the utilization of a superior sampling strategy is beneficial for the performance of SeeA∗.

# 5.2 Results on logic synthesis

For the logic synthesis problem, a Verilog-based hardware design is first converted into an andinverter-graph (AIG) representation, and then the AIG is optimized to have the lowest area-delay product (ADP) through a sequence of functionality-preserving transformations. The optimization is combinatorial because the sequence is constructed by selecting transformations one-by-one in order from a set. Following the literature, here the set is formed by seven legal transformations, and the sequence length is fixed at 10. The resyn2 transformation sequence is used as a baseline for comparisons [9, 10, 37]. More details about logic synthesis are in Appendix H. During the search process, the immediate reward is set to be 0, and the reward of the terminal step is the final AIG’s ADP reduction rate against the baseline resyn2. The ADP score is approximately computed by ABC [4]. A heuristic function is employed to predict the accumulated reward of a sequence when only a front part of the sequence is available as input in the search process. This function serves as a guiding heuristic for the search algorithms. Following ABC-RL [9], the training dataset consists of 23 circuits, while the test dataset comprises 12 MCNC circuits denoted as $\{ \bar { C _ { 1 } } \sim C _ { 1 2 } \}$ [63] (See Appendix I for more details.). The architecture of the heuristic function, the training and test details are referred to Appendix J. WA∗ is equivalent to $\mathbf { A } ^ { * }$ search because $g = 0$ in Equation 2.

Table 1: Test results on the USPTO benchmark for retrosynthetic planning problem.   

<html><body><table><tr><td>Algorithm</td><td>Solved (%) ↑</td><td>Length↓</td><td>Expansions↓</td><td>Avg time (in seconds) ↓</td></tr><tr><td>Retro* [6]</td><td>86.84</td><td>9.71</td><td>157.11</td><td>110.57</td></tr><tr><td>Retro*+ [30]</td><td>91.05</td><td>8.74</td><td>100.15</td><td>61.24</td></tr><tr><td>MCTS(Cpuct = 4.0)</td><td>89.47</td><td>8.23</td><td>122.97</td><td>87.86</td></tr><tr><td>A* search</td><td>88.42</td><td>9.27</td><td>92.45</td><td>96.07</td></tr><tr><td>WA* (ε = 1.5)</td><td>84.21</td><td>10.16</td><td>106.97</td><td>120.69</td></tr><tr><td>LevinTS[38]</td><td>96.84</td><td>7.45</td><td>57.11</td><td>39.77</td></tr><tr><td>PHS [39]</td><td>87.37</td><td>10.19</td><td>93.38</td><td>108.85</td></tr><tr><td>ε-Greedy (ε=0.1)[52]</td><td>92.11</td><td>43.78</td><td>89.14</td><td>76.59</td></tr><tr><td>SeeA*(Uniform)</td><td>96.84</td><td>7.34</td><td>72.08</td><td>49.77</td></tr><tr><td>SeeA*(Cluster)</td><td>98.42</td><td>6.48</td><td>79.75</td><td>37.98</td></tr><tr><td>SeeA*(UCT)</td><td>98.95</td><td>6.36</td><td>62.07</td><td>32.38</td></tr></table></body></html>

The results on the MCNC benchmark are presented in Table $2 ^ { 5 }$ . All three ${ \mathrm { S e e A } } ^ { * }$ variants outperform the existing methods in terms of the mean ADP reduction rates against the baseline resyn2. SeeA∗(Cluster) achieves the highest ADP reduction (i.e., $2 3 . 5 \%$ ), obviously surpassing the stateof-the-art ABC-RL’s $2 0 . 9 \%$ . Guided by the same heuristic, ${ \mathrm { S e e A } } ^ { * }$ (Cluster) outperforms the $\mathbf { A } ^ { * }$ search in 11 out of the 12 testing circuits. As illustrated by an example of the search process in Appendix K, the nodes expanded by $\mathbf { A } ^ { * }$ tend to concentrate on a specific branch, whereas MCTS expands across multiple branches excessively due to its enforced exploration. $\mathrm { S e e A } ^ { * }$ achieves a good balance between $\mathbf { A } ^ { * }$ search and MCTS, ensuring that irrelevant branches are not unduly explored.

Table 2: The ADP reduction $( \% )$ rates against the resyn2 baseline on the MCNC testing datasets.   

<html><body><table><tr><td>Algorithm</td><td>C1</td><td>C2</td><td>C3</td><td>C4</td><td>C5</td><td>C6</td><td>C7</td><td>C8</td><td>C9</td><td>C10</td><td>C11</td><td>C12</td><td>Mean ↑</td></tr><tr><td>DRiLLS [29]</td><td>18.9</td><td>6.7</td><td>8.0</td><td>13.0</td><td>38.4</td><td>19.1</td><td>5.4</td><td>18.0</td><td>14.3</td><td>18.6</td><td>6.6</td><td>11.0</td><td>14.8</td></tr><tr><td>Online-RL [69]</td><td>20.6</td><td>6.6</td><td>8.1</td><td>13.5</td><td>39.4</td><td>21.0</td><td>5.0</td><td>17.9</td><td>16.2</td><td>20.2</td><td>4.7</td><td>11.4</td><td>15.4</td></tr><tr><td>SA+Pred. [10]</td><td>17.6</td><td>17.0</td><td>15.6</td><td>13.0</td><td>46.5</td><td>18.2</td><td>8.5</td><td>23.6</td><td>19.9</td><td>17.6</td><td>10.0</td><td>20.3</td><td>19.0</td></tr><tr><td>MCTS[37]</td><td>17.1</td><td>15.9</td><td>13.1</td><td>13.0</td><td>46.9</td><td>14.9</td><td>6.5</td><td>23.2</td><td>17.7</td><td>20.5</td><td>13.1</td><td>19.7</td><td>18.5</td></tr><tr><td>ABC-RL[9]</td><td>19.9</td><td>19.6</td><td>16.8</td><td>15.0</td><td>46.9</td><td>19.1</td><td>12.1</td><td>24.3</td><td>21.3</td><td>21.1</td><td>13.6</td><td>21.6</td><td>20.9</td></tr><tr><td>A* search</td><td>18.3</td><td>16.6</td><td>19.7</td><td>15.7</td><td>43.6</td><td>15.2</td><td>13.3</td><td>25.5</td><td>19.4</td><td>20.8</td><td>7.5</td><td>18.8</td><td>19.5</td></tr><tr><td>ε-Greedy(ε=0.1)[52]</td><td>18.3</td><td>16.6</td><td>20.9</td><td>15.6</td><td>45.8</td><td>19.9</td><td>11.5</td><td>24.5</td><td>19.4</td><td>20.8</td><td>15.3</td><td>18.7</td><td>20.6</td></tr><tr><td>PV-MCTS</td><td>17.3</td><td>20.0</td><td>27.9</td><td>20.1</td><td>27.3</td><td>20.7</td><td>13.5</td><td>24.7</td><td>14.3</td><td>14.1</td><td>14.7</td><td>20.0</td><td>19.5</td></tr><tr><td>PHS [39]</td><td>21.4</td><td>17.1</td><td>11.7</td><td>8.4</td><td>47.9</td><td>5.2</td><td>8.7</td><td>10.2</td><td>20.5</td><td>12.0</td><td>7.3</td><td>20.8</td><td>15.9</td></tr><tr><td>SeeA*(Uniform)</td><td>21.9</td><td>18.7</td><td>21.9</td><td>16.5</td><td>37.2</td><td>13.8</td><td>12.3</td><td>25.5</td><td>21.5</td><td>24.1</td><td>21.5</td><td>24.0</td><td>21.6</td></tr><tr><td>SeeA*(Cluster)</td><td>23.2</td><td>20.8</td><td>22.7</td><td>16.2</td><td>45.9</td><td>22.6</td><td>13.4</td><td>24.8</td><td>22.4</td><td>24.2</td><td>20.3</td><td>25.1</td><td>23.5</td></tr><tr><td>SeeA*(UCT)</td><td>20.2</td><td>16.6</td><td>25.3</td><td>17.8</td><td>46.4</td><td>25.5</td><td>10.6</td><td>24.4</td><td>18.0</td><td>28.7</td><td>17.5</td><td>23.6</td><td>22.5</td></tr></table></body></html>

# 5.3 Results on Sokoban and path finding

The first 50000 training problems and the 1000 test problems are collected from Boxoban [23]. They are utilized to train a cost estimator and evaluate the search algorithms, respectively. More training details are provided in Appendix L. During testing, the search process is terminated with failure if the running time exceeds 10 minutes. SeeA∗ has successfully solved all 1000 test Sokoban cases. Notably, the solutions generated by $\mathrm { S e e A } ^ { * }$ exhibit not only shorter lengths compared to other search algorithms such as $\mathbf { A } ^ { * }$ search, $\mathrm { \mathbf { W } A } ^ { * }$ , LevinTS, and PHS but also shorter lengths than the state-of-the-art DeepCubeA [1] algorithm. Detailed results are summarized in Appendix M. To illustrate the effectiveness of ${ \mathrm { S e e A } } ^ { * }$ on problems where accurate heuristics could exist but the guiding heuristic used is unreliable, experiments on path finding are conducted. $\mathbf { A } ^ { * }$ and $\mathrm { S e e A ^ { * } }$ exhibit similar performance when the guidance heuristic is reliable enough. However, $\mathrm { { S e e A } ^ { * } }$ demonstrates significant advantages over $\mathbf { A } ^ { * }$ when the heuristic is unreliable. More details are available in Appendix N.

# 5.4 The impact of the hyperparameters on the performance

The effects of three hyperparameters in ${ \mathrm { S e e A } } ^ { * }$ are empirically investigated below, i.e., the number of candidate nodes $K$ , the number of clusters $N _ { c }$ , and the adjustable weight $c _ { b }$ in Equation 4. Experiments are conducted on the USPTO benchmark for the retrosynthesis planning problem. The number $K$ is a critical parameter controlling the extent of exploration of ${ \mathrm { S e e A } } ^ { * }$ . When $K = 1$ , the node to be expanded is solely determined by the selective sampling strategy, where the heuristic function has no impact on the selection. When $K$ is too large, all opening nodes will be finally chosen as candidates because every node has a positive chance to be selected by the sampling strategy. In this case, $\mathrm { S e e A } ^ { * }$ degenerates back to $\mathbf { A } ^ { * }$ which highly depends on the heuristic function. When $K$ is at an appropriate range, the sampling scheme endows SeeA∗ with helpful exploratory capability. It is observed from Figure 11 in Appendix $\mathrm { ^ o }$ that a wide range of $K$ enables ${ \mathrm { S e e A } } ^ { * }$ to obtain superior performance. For the extreme cases, $\mathrm { S e e A } ^ { * } ( K = 1 )$ has the lowest success rate and longest solution length, and the performance of $\operatorname { S e e A } ^ { * } ( K = \infty )$ , which is equivalent to $\mathbf { A } ^ { * }$ , is also discounted.

According to the results in Figure 12 in Appendix O, the performance of the clustering sampling strategy is generally very robust against the choices of $N _ { c }$ . An inadequate number of clusters makes it towards uniform sampling by ignoring the differences among the nodes, while an excessive cluster number will distract the sampling process by noise in the node representation learning. The hyperparameter $c _ { b }$ controls the balance between exploration and exploitation in the UCT-like sampling strategy. A large $c _ { b }$ favors exploration during the selection of candidate nodes. From Figure 13 in Appendix O, either too large or too small $c _ { b }$ are detrimental to the efficiency of ${ \mathrm { S e e A } } ^ { * }$ , and the UCT-like sampling strategy achieves excellent results when $c _ { b }$ is in the range of [0.15, 0.4].

# 6 Conclusion

In this paper, the SeeA∗ search is proposed to enhance the exploration behavior of the $\mathbf { A } ^ { * }$ search by selecting expanded nodes from the sampled candidate nodes, rather than the entire set of open nodes. A node that is evaluated not to have the best estimated heuristic value may be selected and explored, thereby jumping out of the local optimum induced by inaccuracies in the heuristic function. Three sampling strategies are presented in the paper. Furthermore, we have theoretically established that SeeA∗ is more efficient than $\mathbf { A } ^ { * }$ search when the estimation of heuristic functions is not accurate enough. Experiments on two diverse real-world applications in chemistry and circuit design and one puzzle-solving game demonstrate the efficiency of $\mathrm { S e e A } ^ { * }$ .

If the model exhibits precise state evaluation, the incorporation of exploration into $\mathbf { A } ^ { * }$ search becomes redundant. However, in practical applications, where problems tend to be intricate or lack sufficient training data, obtaining accurately predictive heuristic functions is challenging. As suggested in Equation 7, in addition to reducing the prediction error $\sigma$ , the probability of expanding the optimal nodes is also improved by using a smaller number of candidate nodes $K$ to include the optimal node in the candidate set with a greater likelihood $P ( n _ { 1 } \in \mathcal { D } )$ . Screening candidate nodes reduces the search space, thereby enhancing search efficiency. Investigations on more effective sampling strategies will be conducted in future work. $\mathrm { S e e A } ^ { * }$ will contribute to solving practical problems with limited samples. However, this work is still in the nascent stages without further applications related to people’s daily lives currently, and thus there are no immediate ethical or harmful social impacts.