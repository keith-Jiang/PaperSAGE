# DenoiseRep: Denoising Model for Representation Learning

Zhengrui Xu1 † zrxu23@bjtu.edu.cn

Guan’an Wang †‡ guan.wang0706@gmail.com

Xiaowen Huang 1,2,3 $^ *$ xwhuang@bjtu.edu.cn

Jitao Sang 1,2,3 jtsang@bjtu.edu.cn

# 1School of Computer Science and Technology, Beijing Jiaotong University

2Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University 3Key Laboratory of Big Data & Artificial Intelligence in Transportation(Beijing Jiaotong University), Ministry of Education

# Abstract

The denoising model has been proven a powerful generative model but has little exploration of discriminative tasks. Representation learning is important in discriminative tasks, which is defined as "learning representations (or features) of the data that make it easier to extract useful information when building classifiers or other predictors" [4]. In this paper, we propose a novel Denoising Model for Representation Learning (DenoiseRep) to improve feature discrimination with joint feature extraction and denoising. DenoiseRep views each embedding layer in a backbone as a denoising layer, processing the cascaded embedding layers as if we are recursively denoise features step-by-step. This unifies the frameworks of feature extraction and denoising, where the former progressively embeds features from low-level to high-level, and the latter recursively denoises features step-by-step. After that, DenoiseRep fus es the parameters of feature extraction and denoising layers, and theoretically demonstrates its equivalence before and after the fusion, thus making feature denoising computation-free. DenoiseRep is a label-free algorithm that incrementally improves features but also complementary to the label if available. Experimental results on various discriminative vision tasks, including re-identification (Market-1501, DukeMTMC-reID, MSMT17, CUHK-03, vehicleID), image classification (ImageNet, UB200, Oxford-Pet, Flowers), object detection (COCO), image segmentation (ADE20K) show stability and impressive improvements. We also validate its effectiveness on the CNN (ResNet) and Transformer (ViT, Swin, Vmamda) architectures. Code is available at https://github.com/wangguanan/DenoiseRep.

# 1 Introduction

Denoising Diffusion Probabilistic Models (DDPM) [21] or Diffusion Model for short have been proven to be a powerful generative model [5]. Generative models can generate vivid samples (such as images, audio and video) by modeling the joint distribution of the data $P ( X , Y )$ , where $X$ is the sample and $Y$ is the condition. Diffusion models achieve this goal by adding Gaussian noise to the data and training a denoising model of inversion to predict the noise. Diffusion models can generate multi-formity and rich samples, such as Stable diffusion [50], DALL [47] series and Midjourney, these powerful image generation models, which are essentially diffusion models.

![](images/763bc291051c504ba388462d8bcae2ca4fcfb652cf80a900553a2f43acb168ed.jpg)  
Figure 1: A brief description of our idea. (a) A typical denoising model for generative tasks recursively applies a denoising layer. (b) A naive idea that applies a denoising strategy to a discriminative model is applying a recursive denoise layer on the feature of a backbone and taking extra inference latency. (c,d) Our DenoiseRep first unifies the frameworks of feature extraction and denoising in a backbone pipeline, then merges parameters of the denoising layers into embedding layers, making the feature more discriminative without extra latency cost.

However, its application to discriminative models has not been extensively explored. Different from generative models, discriminative models predict data labels by modeling the marginal distribution of the data $P ( \boldsymbol { Y } | \boldsymbol { X } )$ . $Y$ can be various labels, such as image tags for classification, object boxes for detection, and pixel tags for segmentation. Currently, there are several methods based on diffusion models implemented in specific fields. For example, DiffusionDet [7] is a new object detection framework that models object detection as a denoising diffusion process from noise boxes to object boxes. It describes object detection as a generative denoising process and performs well compared to previous mature object detectors. DiffSeg [52] for image segmentation, which is a method of unsupervised zero-shot sample segmentation using pre-trained models (stable diffusion). It introduces a simple and effective iterative merging process to measure the attention maps between KL divergence and merge them into an effective segmentation mask. The proposed method does not require any training or language dependency to extract the quality segmentation of any image.

The methods above are carefully designed for specific tasks and require a particular data structure. For example, DiffusionDet [7] uses noise boxes and DiffSeg [52] uses noise segmentation. In this paper, we explore a more general conception of how the denoising model can improve representation learning, i.e. "learning representations (or features) of the data that make it easier to extract useful information when building classifiers or other predictors" [4], and contribute to discriminative models. We take person Re-Identification (ReID) [66, 3] as a benchmark task. ReID aims to match images of a pedestrian under disjoint cameras, and is suffered by pose, lighting, occlusion and so on, thus requiring more identity-discriminative feature.

A straightforward approach is applying the denoising process to a backbone’s final feature [26, 14], reducing noise in the final output and making the feature more discriminative, as Fig. 1(b) shows. However, this way can be computationally intensive. Because the denoising layer needs to be proceeded on the output of the previous one in a recursive and step-by-step manner. Considering that a backbone typically consists of cascaded embedding layers (e.g., convolution layer, multi-head attention layer), we propose a novel perspective: treating each embedding layer as a denoising layer. As shown in Fig. 1(c), it allows us to process the cascaded layers as if we are recursively proceeding through the denoising layer step-by-step. This method transforms the backbone into a series of denoising layers, each working on a different feature extraction level. While this idea is intuitive and simple, its practical implementation presents a significant challenge. The main issue arises from the requirement of the denoising layer for the input and output features to exist in the same feature space. However, in a typical backbone (e.g. ResNet [26], ViT [14])), the layers progressively map features from a low level to a high level. It means that the feature space changes from layer to layer, which contradicts the requirement of the denoising layer.

To resolve all the difficulties above and efficiently apply the denoising process to improve discriminative tasks, our proposed Denoising Model for Representation Learning (DenoiseRep) is as below: Firstly, we utilize a well-trained backbone and keep it fixed throughout all subsequent procedures. This step is a free launch as we can easily use any publicly available backbone without requiring additional training time. This approach allows us to preserve the backbone’s inherent characteristics of semantic feature extraction. Given the backbone and an image, we can get a list of features. Next, we train denoising layers on those features. The weights of denoising layers are randomly initialized and their weights are not shared. The training process is the same as that in DDPM [21], where the only difference is that the denoising layer in DDPM takes a dynamic $t \in [ 1 , T ]$ , and our denoising layers take fixed $n \in [ 1 , N ]$ , where $n$ is the layer index, $T$ is denoise times and $N$ is backbone layer number as shown in Fig. 1(c). Finally, considering that the $N$ denoising layers consume additional execution latency, we propose a novel feature extraction and feature denoising fusion algorithm. As shown in Fig. 1(d), the algorithm merges parameters of extra denoising layers into weights of the existing embedding layers, thus enabling joint feature extraction and denoising without any extra computation cost. We also theoretically demonstrate the total equivalence before and after parameter fusion. Please see Section 3.3 and Eq (7) for more details.

Our contributions can be summarized as follows:

(1) We propose a novel Denoising Model for Representation Learning (DenoiseRep), which innovatively integrates the denoising process, originating from generative tasks, into the discriminative tasks. It treats $N$ cascaded embedding layers of a backbone as $T$ times recursively proceeded denoising layers. This idea enables joint feature extraction and denoising is a backbone, thus making features more discriminative.

(2) The proposed DenoiseRep fuses the parameters of the denoising layers into the parameters of the corresponding embedding layers and theoretically demonstrates their equivalence. This contributes to a computation-efficient algorithm, which takes no extra latency.

(3) Extensive experiments on $4 \ \mathrm { R e I D }$ datasets verified that our proposed DenoiseRep can effectively improve feature performance in a label-free manner and performs better in the case of labelargumented supervised training or introduction of additional training data. We also extend DenoiseRep to large-scale (ImageNet), fine-grained (CUB200, Oxford-Pet, Flowers) image classifications, object detection (COCO) and image segmentation (ADE20K), showing its scalability.

# 2 Related Work

Generative models learn the distribution of inputs before estimating class probabilities. A generative model learns the data generation process by learning the probability distribution of the input data and generating new data samples. The generative models first estimate the conditional density of categories $P ( \bar { x } | y = k )$ and prior category probabilities $P ( y = k )$ from the training data. The $P ( x )$ is obtained by the full probability formula. So as to model the probability distribution of each type of data. Generative models can generate new samples by modelling data distribution. For example, Generative Adversarial Networks (GANs) [17, 43, 23] and Variational Autoencoders (VAEs) [24, 53, 69, 64] are both classic generative models that generate real samples by learning potential representations of data distributions, demonstrating excellent performance in data distribution modeling. Recent research has focused on using diffusion models for generative tasks. The diffusion model was first proposed by the article [51] in 2015, with the aim of eliminating Gaussian noise from continuous application to training images. The DDPM [21] proposed in 2020 have made the use of diffusion models for image generation mainstream. In addition to its powerful generation ability, the diffusion model also has good denoising ability through noise sampling, which can denoise noisy data and restore its original data distribution.

Discriminative models learn condition distribution, i.e. $P ( y | x )$ , where $x$ is data and $y$ is task-specific features. For example, classification tasks [1, 2, 13] map data to tags, retrieval tasks [36, 62] map data to a feature space where similar data should be near otherwise faraway, detection task [49, 27] map data to space position and size. Person Re-Identification $\left( \mathbf { R e I D } \right)$ is a fine-grained retrieval task which identifies individuals among disjoint camera views. Considering its challenge to feature discrimination, we take ReID as the major benchmark task and the others as auxiliary benchmarks. Existing ReID methods can be grouped into hand-crafted descriptors [35, 42, 65] incorporated with metric learning [25, 34, 71] and deep learning algorithms [58, 57, 56, 44, 16, 10]. State-of-theart ReID models often leverage convolutional neural networks (CNNs) [28] to capture intricate spatial relationships and hierarchical features within person images. Attention mechanisms [54, 14], spatial-temporal modeling [31, 30], and domain adaptation techniques [9] have further enhanced the adaptability of ReID models to diverse and challenging real-world scenarios.

# 3 DenoiseRep: Denoising Model for Representation Learning

# 3.1 Review Representation Learning

Representation learning plays a pivotal role in discriminative tasks, which is defined as "learning representations (or features) of the data that make it easier to extract useful information when building classifiers or other predictors" [4]. A common architecture of discriminative tasks consists of a vision backbone to extract discriminative features (e.g., ResNet [18], ViT [14]) and a task-specific head that operates on these features (e.g. MLP [26] for classification, RCNN [49] for object detection, FCN [40] for segmentation). It is evident that the vision backbone is central to representation learning. In this paper, we introduce a novel Denoising Model for Representation Learning (DenoiseRep), which integrates feature extraction and feature denoising within a single vision backbone. This approach aims to enhance the discriminative power of the features extracted.

# 3.2 Joint Feature Extraction and Feature Denoising

We refer to the diffusion modeling approach to denoise the noisy features through T-steps to obtain clean features. At the beginning, we use the features output from the backbone network as data samples for diffusion training, and get the noisy samples by continuously adding noise and learning through the network in order to simulate the data distribution of its features.

$$
\begin{array} { r l } & { q ( \mathbf { x } _ { 1 : T } | \mathbf { x } _ { 0 } ) : = \displaystyle \prod _ { t = 1 } ^ { T } q ( \mathbf { x } _ { t } | \mathbf { x } _ { t - 1 } ) } \\ & { q ( \mathbf { x } _ { t } | \mathbf { x } _ { t - 1 } ) : = \mathcal { N } ( \mathbf { x } _ { t } ; \sqrt { 1 - \beta _ { t } } \mathbf { x } _ { t - 1 } , \beta _ { t } \mathbf { I } ) } \end{array}
$$

where $X _ { 0 }$ represents the feature vector output by the backbone, $t$ represents the diffusion step size, $\beta _ { t }$ is a set of pre-set parameters, and $X _ { t }$ represents the noise sample obtained through diffusion process.

In the inference stage, as shown in Fig. 1(b), we perform T-step denoising on the output features, to obtain cleaner features and improve the expressiveness of the features.

$$
\begin{array} { r l } & { \displaystyle p _ { \theta } ( \mathbf { x } _ { 0 : T } ) : = p ( \mathbf { x } _ { T } ) \prod _ { t = 1 } ^ { T } p _ { \theta } ( \mathbf { x } _ { t - 1 } | \mathbf { x } _ { t } ) } \\ & { \displaystyle p _ { \theta } ( \mathbf { x } _ { t - 1 } | \mathbf { x } _ { t } ) : = \mathcal { N } ( \mathbf { x } _ { t - 1 } ; \mu _ { \theta } ( \mathbf { x } _ { t } , t ) , { \boldsymbol \Sigma } _ { \theta } ( \mathbf { x } _ { t } , t ) ) } \end{array}
$$

where $X _ { t }$ represents the feature vector output by the backbone in the inference stage. $T$ is the denoising step size, representing the magnitude of the noise. We adjust $t$ appropriately based on different datasets and backbones to obtain the optimal denoising amplitude. According to $\dot { p _ { \theta } } ( \mathbf { x } _ { t - 1 } | \mathbf { x } _ { t } )$ denoise it step by step, and finally obtains $X _ { 0 }$ , which represents the clean feature after denoising.

# 3.3 Fuse Feature Extraction and Feature Denoising

As described in Section 3.2, the proposed method above could effectively improve the discriminability of features. Still, extra inference latency is introduced caused by recursive calling of the denoising layers. To solve the problem, we propose to fuse parameters of feature denoising layers into parameters of existing embedding layers of the backbone. The core idea is to expand the linear layer each transformer encoder block into two branches, one for its original embedding layer and the other for extra denoising layer. As shown in Fig. 2, during the training phase, we freeze the original embedding layers and only train the denoising layers. The training method is consistent with section 3.2, and the features are diffused and fed into the denoising layers. Please refer to Algorithm 1 for more details. In the inference stage, we fuse the pre-trained parameters of embedding and denoising layers, merging the two branches into a single branch without additional inference time. Please note that, here we take the transformer architecture as an example, but DenoiseRep is suitable for CNN architecture. We demonstrate its scalibity on CNN in experiments. The derivation of parameter merging is as follows:

![](images/67a1961ba9d713c9366960b3125a97a24a61ff67f71cbb16d7df1248eac8867d.jpg)  
Figure 2: Pipeline of our proposed DenoiseRep. ViT consists of $N$ cascaded transformer encoder layers. During the training phase (see the right side “Train Only” process), we freeze the backbone parameters and only train the extra denoising layers. In the inference stage (see the left side “Infer Only” process), we merge the parameters of denoising layers to corresponding encoder layers. So there is no extra inference latency cost. Please find definitions of W , b, $W _ { D }$ , $W ^ { \prime }$ , $i$ and $b ^ { \prime }$ in Algorithm 2.

$$
X _ { t - 1 } = { \frac { 1 } { \sqrt { a _ { t } } } } ( X _ { t } - { \frac { 1 - a _ { t } } { \sqrt { 1 - { \bar { a _ { t } } } } } } D _ { \theta } ( X _ { t } , t ) ) + \sigma _ { t } z
$$

where $a _ { t } = 1 - \beta _ { t } , D _ { \theta }$ are the parameters of the prediction noise network.

$$
\begin{array} { c } { { Y = W X + b } } \\ { { \frac { 1 } { \sqrt { a _ { t } } } X _ { t } - X _ { t - 1 } = \displaystyle \frac { 1 - a _ { t } } { \sqrt { a _ { t } } \sqrt { 1 - \bar { a } _ { t } } } D _ { \theta } X _ { t } - \sigma _ { t } z } } \\ { { \frac { 1 } { \sqrt { a _ { t } } } Y _ { t } - Y _ { t - 1 } = \displaystyle \frac { 1 - a _ { t } } { \sqrt { a _ { t } } \sqrt { 1 - \bar { a } _ { t } } } D _ { \theta } Y _ { t } - \sigma _ { t } z } } \end{array}
$$

We make a simple transformation of Eq. (5) and multiply both sides simultaneously by $W$ . The simplified equation can be obtained by bringing $Y _ { t }$ in terms of $W X _ { t } + b$ :

$$
\begin{array} { c } { { Y _ { t - 1 } = [ W - C _ { 1 } ( t ) W W _ { D } ] X _ { t } + W C _ { 2 } ( t ) C _ { 3 } + b } } \\ { { C _ { 1 } ( t ) = \displaystyle \frac { 1 - a _ { t } } { \sqrt { a _ { t } } \sqrt { 1 - \bar { a } _ { t } } } \qquad C _ { 2 } ( t ) = \displaystyle \frac { 1 - a _ { t - 1 } ^ { - } } { 1 - \bar { a } _ { t } } \beta _ { t } \qquad C _ { 3 } = Z \sim N ( 0 , I ) } } \end{array}
$$

where $W _ { D }$ denotes the parameters of $D _ { \theta } ( X _ { t } , t )$ , $X _ { t }$ denotes the input of this linear layer, $Y _ { t }$ denotes the output of this linear layer, and $Y _ { t - 1 }$ denotes the result after denoising in one step of $Y _ { t }$ . Due to the cascading relationship of blocks, as detailed in Algorithm. 2, different $t$ values are set according to the order between levels, and the one-step denoising of one layer is combined to achieve the denoising process of $Y _ { t }  Y _ { 0 }$ , ensuring the continuity of denoising and ultimately obtaining clean features. We split the original single branch into a dual branch structure. During the training phase, the backbone maintains its original parameters and needs to train the denoising module parameters. In the inference stage, as shown on the left side of Fig. 2, we use the method of reparameterization, to replace the original $W$ parameter with $W ^ { \prime }$ , where $W ^ { \prime } = [ W - C _ { 1 } ( t ) W W _ { D } ]$ in Eq. (7), which has the same

# Algorithm 1 Training

Input: The number of feature layers in the backbone N, features extracted from each layer $\{ F _ { i } \} _ { i = 1 } ^ { N }$   
the denoising module that needs to be trained $\{ D _ { i } ( \cdot ) \} _ { i = 1 } ^ { N }$ .   
1: repeat   
2: for each $i \in [ N , 1 ]$ do   
3: $t = i$ : Specify the diffusion step t for the current layer based on the order of layers.   
4: $\epsilon \sim N ( 0 , I )$ : Randomly sample a Gaussian noise.   
5: $X _ { t } = \dot { \sqrt { a _ { t } } } \dot { F } _ { i } + \sqrt { 1 - \dot { a _ { t } } } \epsilon$ : Forward diffusion process in Eq.(2).   
6: Take gradient descent step on $\nabla _ { \theta } \left\| \epsilon - D _ { i } ( X _ { t } , t ) \right\| ^ { 2 }$   
7: end for   
8: until converged

number of parameters as $W$ , thus achieving the combination of $F C$ operation and denoising without additional time cost. It is a Computation-free method.

In Eq. (7), we achieve one-step denoising $Y _ { t }  Y _ { t - 1 }$ . If we need to increase the denoising amplitude, we can extend it to two-step or multi-step denoising. The following is the derivation formula for two-step denoising:

$$
\frac { 1 } { \sqrt { a _ { t } } } Y _ { t } - Y _ { t - 1 } = C _ { 1 } ( t ) D _ { \theta } Y _ { t } - \sigma _ { t } z
$$

$$
\frac { 1 } { \sqrt { a _ { t - 1 } } } Y _ { t - 1 } - Y _ { t - 2 } = C _ { 1 } ( t - 1 ) D _ { \theta } Y _ { t - 1 } - \sigma _ { t - 1 } z
$$

We can obtain this by eliminating $Y _ { t - 1 }$ from Eq.(8) and Eq.(9) and replacing $Y _ { t }$ with $W X _ { t } + b$ :

$$
\begin{array} { c } { { { \cal Y } _ { t - 2 } = W ^ { \prime \prime } X _ { t } + C ^ { \prime \prime } } } \\ { { { } } } \\ { { W ^ { \prime \prime } = \displaystyle \frac 1 { \sqrt { a _ { t } - 1 } } \{ \frac W { \sqrt { a _ { t } } } - [ C _ { 1 } ( t ) + C _ { 1 } ( t - 1 ) ] W W _ { D } + \sqrt { a _ { t } } C _ { 1 } ( t - 1 ) C _ { 1 } ( t ) W W _ { D } W _ { D } \} } } \\ { { { } } } \\ { { C ^ { \prime \prime } = \displaystyle \frac 1 { \sqrt { a _ { t } - 1 } } [ W C _ { 2 } ( t ) + \sqrt { a _ { t } } W C _ { 2 } ( t - 1 ) - \sqrt { a _ { t } } C _ { 1 } ( t - 1 ) C _ { 2 } ( t ) W W _ { D } ] Z + b } } \end{array}
$$

Note that a single module completes two steps of denoising. To ensure the continuity of denoising, the $t$ value should be sequentially reduced by 2.

Our proposed DenoiseRep is based on feature-level denoising and can be migrated to various downstream tasks. It denoises the features on each layer for better removal of noise at each stage, as the noise in the inference stage comes from multiple sources, which could be noise in the input image or noise generated while passing through the network. Denoising each layer avoids noise accumulation and gives better quality output. And according to the noise challenges brought by data in different scenarios, the denoising intensity can be adjusted by controlling t, $\beta _ { t }$ , and the number of denoising times, which has good generalization ability.

# Algorithm 2 Inference

Input: The number of feature layers in the backbone N, features extracted from each layer $\{ F _ { i } \} _ { i = 1 } ^ { N }$ trained denoising module parameters $\{ W _ { D _ { i } } \} _ { i = 1 } ^ { N }$ in Algorithm(1), after obtaining the initial feature $F ^ { N }$ through patch_embed, it is necessary to remove $\mathbf { N }$ -step noise from it, the pre-trained parameters $\{ W _ { i } \} _ { i = 1 } ^ { \overline { { N } } }$ and $\{ b _ { i } \} _ { i = 1 } ^ { N }$ for the backbone.

1: for each $i \in [ N , 1 ]$ do   
2: $t = i$ : Set the denoising amplitude based on the depth of the current layer.   
3: $W ^ { \prime } = [ W _ { i } - C _ { 1 } ( t ) W _ { i } \bar { W } _ { D _ { i } } ]$ , $b ^ { \prime } = W _ { i } C _ { 2 } ( t ) C _ { 3 } + \bar { b _ { i } }$ : Parameter fusion according to Eq.(7).   
4: $F ^ { t - 1 } \stackrel { \cdot } { = } W ^ { \prime } F ^ { t } + b ^ { \prime }$ : Fuse feature extraction and feature denoising.   
5: end for   
6: return F 0

# 3.4 Unsupervised Learning Manner

Our proposed DenoiseRep is label-free because its essence is a generative model that models data by learning its distribution. Thus the training loss contains only the $L o s s _ { p }$ of denoising layers:

$$
L o s s _ { p } = \sum _ { i = 1 } ^ { N } | \epsilon _ { i } - D _ { \theta _ { i } } ( X _ { t _ { i } } , t _ { i } ) |
$$

where $\epsilon$ denotes the sampled noise, $N$ denotes the number of denoising layers, $X _ { t }$ denotes the noise sample, $t$ denotes the diffusion step, and $D _ { \theta } ( X _ { t } , t )$ denotes the noise predicted by the denoising layer.

However, it is worth noting that our method is complementary to label if the label is available. $L o s s _ { l }$ is the task-specific supervised loss with label, $\lambda$ is the trade-off parameter between two losses. The label-argumented learning is defined as:

$$
L o s s = ( 1 - \lambda ) L o s s _ { l } + \lambda L o s s _ { p }
$$

Results in experiments Section 4.1 shows the improvement from label.

# 4 Experiments

Table 1: Experimental results on various discriminative tasks.   

<html><body><table><tr><td>Task</td><td>Model</td><td>Backbone</td><td>Dataset</td><td>Metric</td><td>Baseline</td><td>+DenoiseRep</td></tr><tr><td>Classification</td><td>SwinT[39]</td><td>SwinV2-T</td><td>ImageNet-1k</td><td>acc@1</td><td>81.82%</td><td>82.13%</td></tr><tr><td>Person-ReID</td><td>TransReID-SSL [41]</td><td>ViT-S</td><td>MSMT17</td><td>mAP</td><td>66.30%</td><td>67.33%</td></tr><tr><td>Detection</td><td>Mask-RCNN [19]</td><td>SwinV2-T</td><td>COCO</td><td>AP</td><td>42.80%</td><td>44.30%</td></tr><tr><td>Segmentation</td><td>FCN[40]</td><td>ResNet-50</td><td>ADE20K</td><td>BIoU</td><td>28.70%</td><td>29.90%</td></tr></table></body></html>

Our proposed DenoiseRep is a versatile method that can be incrementally applied to various discriminative tasks. Table 1 demonstrates that DenoiseRep yields stable and substantial improvements across image classification, object detection, image segmentation, and person re-identification. Given that person re-identification is a nuanced image retrieval task that poses a greater challenge to feature discriminability, we take it as our benchmark for model analysis. Details of the experimental settings are provided in Appendix A. Additional experimental results on various tasks are presented in Appendices B, C, D, and E.

# 4.1 Analysis of Label Informations

Table 2: DenoiseRep is a label-free method that can also be effectively complemented with labels when they are available. The table below analyzes the effectiveness of using labels. The baseline method, TransReID-SSL, is based on a ViT-small backbone. "Label-free" indicates training without labels, "label-augmented" refers to the use of labels, and "merged dataset" denotes the use of combined datasets without labels.   

<html><body><table><tr><td>Method</td><td>DukeMTMC(%)</td><td>MSMT17(%)</td><td>Market1501(%)</td><td>CUHK-03(%)</td></tr><tr><td>TransReID-SSL</td><td>80.40</td><td>66.30</td><td>91.20</td><td>83.50</td></tr><tr><td>+DenoiseRep (label-free)</td><td>80.92 (↑0.52)</td><td>66.87 (↑ 0.57)</td><td>91.82 (↑0.62)</td><td>83.72 (↑ 0.22)</td></tr><tr><td>+DenoiseRep (label-aug)</td><td>81.22 (↑ 0.82)</td><td>67.33 (↑1.03)</td><td>92.05 (↑ 0.85)</td><td>84.11 (↑ 0.61)</td></tr><tr><td>+DenoiseRep (merged ds)</td><td>80.98 (↑ 0.58)</td><td>66.99 (↑0.69)</td><td>91.80 (↑0.60)</td><td>83.86 (↑ 0.36)</td></tr></table></body></html>

As mentioned in Section 3.3, DenoiseRep is an unsupervised denoising module, and its training does not require the assistance of label information. We conducte the following experiments to identify three key issues.

(1) Is this label-free and unsupervised training denoising plugin effective? As shown in Table 2 (line2), compared with baseline method (line1), the baseline method performs better after adding our label-free plugin, which shows that our method does have denoising capability for features.

(2) Could introducing label information for supervised training further improve performance? Introducing label information is actually adding $L o s s _ { l }$ as mentioned in Section 3.4 as a supervised signal. As shown in Table 2 (line3), baseline method with label-argumented DenoiseRep achieve improvements of $0 . 3 2 \% \mathrm { ~ - ~ } 0 . 7 0 \%$ on the mAP metric, indicating that our denoising plugin has label compatibility, in other words, the plug-in is effective for feature denoising regardless of labelargumented supervised or lable-free unsupervised training.

(3) Since our plugin can perform unsupervised denoising of features, it is natural to think about whether adding more data for training the plugin could further improve its performance? We merge four datasets for training and then test on each dataset using mAP to evaluate. Comparing the results of training on sigle dataset (line2) with on merged datasets (line4), we found that adopting other datasets for unsupervised learning can further improve the performance of DenoiseRep, which also proves that DenoiseRep has good generalization ability.

To demonstrate that our method can perform unsupervised learning and has good generalization, we merged four datasets and rearranged the sequence IDs to ensure the reliability of the experiment. The model is tested on the entire dataset. During the training process, we freeze the baseline parameters and only train the DenoiseRep module, without the need for labels, for unsupervised learning. Then test on a single dataset and compare the results of training on a single dataset. As shown in Table 2, it can be observed that adding unlabeled training data from different datasets can improve the model’s performance on a single dataset, proving that this module has a certain degree of generalization.

# 4.2 Comparison with State-of-the-Art ReID Methods

We compare several state-of-the-art ReID methods on four datasets. One of the best performing comparison methods is TransReID-SSL, which is a series of ReID methods based on the ViT backbones. Other methods are based on structures such as CNNs. We add our method to TransReIDSSL series and observe their performance. As shown in Table 3, we have the following findings:

Table 3: Comparison with state-of-the-art ReID methods.   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">Backbone</td><td colspan="2">MSMT17</td><td colspan="2">Market1501</td><td colspan="2">DukeMTMC</td><td colspan="2">CUHK03-L</td></tr><tr><td>mAP</td><td>R1</td><td>mAP</td><td>R1</td><td>mAP</td><td>R1</td><td>mAP</td><td>R1</td></tr><tr><td>MGN[59]</td><td>ResNet-50</td><td></td><td></td><td>86.90</td><td>95.70</td><td>78.40</td><td>88.70</td><td>67.40</td><td>68.00</td></tr><tr><td>OSNet [74]</td><td>OSNet</td><td>52.90</td><td>78.70</td><td>84.90</td><td>94.80</td><td>73.50</td><td>88.60</td><td></td><td></td></tr><tr><td>BAT-net [15]</td><td>GoogLeNet</td><td>56.80</td><td>79.50</td><td>87.40</td><td>95.10</td><td>77.30</td><td>87.70</td><td>76.10</td><td>78.60</td></tr><tr><td>ABD-Net [8]</td><td>ResNet-50</td><td>60.80</td><td>82.30</td><td>88.30</td><td>95.60</td><td>78.60</td><td>89.00</td><td></td><td></td></tr><tr><td>RGA-SC [68]</td><td>ResNet-50</td><td>57.50</td><td>80.30</td><td>88.40</td><td>96.10</td><td></td><td></td><td>77.40</td><td>81.10</td></tr><tr><td>ISP [76]</td><td>HRNet-W32</td><td></td><td></td><td>88.60</td><td>95.30</td><td>80.00</td><td>89.60</td><td>74.10</td><td>76.50</td></tr><tr><td>CDNet [29]</td><td>CDNet</td><td>54.70</td><td>78.90</td><td>86.00</td><td>95.10</td><td>76.80</td><td>88.60</td><td></td><td></td></tr><tr><td>Nformer [60]</td><td>ResNet-50</td><td>59.80</td><td>77.30</td><td>91.10</td><td>94.70</td><td>83.50</td><td>89.40</td><td>78.00</td><td>77.20</td></tr><tr><td>TransReID[20]</td><td>ViT-base-ics</td><td>67.70</td><td>85.30</td><td>89.00</td><td>95.10</td><td>82.20</td><td>90.70</td><td>84.10</td><td>86.40</td></tr><tr><td>TransReID</td><td>ViT-base</td><td>61.80</td><td>81.80</td><td>87.10</td><td>94.60</td><td>79.60</td><td>89.00</td><td>82.30</td><td>84.60</td></tr><tr><td>TransReID-SSL [41]</td><td>ViT-small</td><td>66.30</td><td>84.80</td><td>91.20</td><td>95.80</td><td>80.40</td><td>87.80</td><td>83.50</td><td>85.90</td></tr><tr><td>TransReID-SSL</td><td>ViT-base</td><td>75.00</td><td>89.50</td><td>93.10</td><td>96.52</td><td>84.10</td><td>92.60</td><td>87.80</td><td>89.20</td></tr><tr><td>CLIP-REID [32]</td><td>ViT-base</td><td>75.80</td><td>89.70</td><td>90.50</td><td>95.40</td><td>83.10</td><td>90.80</td><td></td><td>一</td></tr><tr><td>TransReID+DenoiseRep</td><td>ViT-base-ics</td><td>68.10</td><td>85.72</td><td>89.56</td><td>95.50</td><td>82.35</td><td>90.87</td><td>84.15</td><td>86.39</td></tr><tr><td>TransReID+DenoiseRep</td><td>ViT-base</td><td>62.23</td><td>82.02</td><td>87.25</td><td>94.63</td><td>80.12</td><td>89.33</td><td>82.44</td><td>84.61</td></tr><tr><td>TransReID-SSL+DenoiseRep</td><td>ViT-small</td><td>67.33</td><td>85.50</td><td>92.05</td><td>96.68</td><td>81.22</td><td>88.72</td><td>84.11</td><td>86.47</td></tr><tr><td>TransReID-SSL +DenoiseRep</td><td>ViT-base</td><td>75.35</td><td>89.62</td><td>93.26</td><td>96.55</td><td>84.31</td><td>92.90</td><td>88.08</td><td>89.29</td></tr><tr><td>CLIP-REID+DenoiseRep</td><td>ViT-base</td><td>76.30</td><td>90.60</td><td>91.10</td><td>95.80</td><td>83.70</td><td>91.60</td><td></td><td></td></tr></table></body></html>

(1) Our method stands out on four datasets on ViT-base backbone with a large number of parameters, achieving almost the best performance on two evaluation metrics.

(2) The methods using our plugin outperforms the original methods with the same backbone on all datasets. In addition, the performance improvement of small-scale backbones with the addition of DenoiseRep is more significant than the large-scale backbones approach due to the fact that DenoiseRep is essentially a denoising module that removes the noise contained in the features during the inference stage. For large-scale backbones, the extracted features already have good performance, so the denoising amplitude is limited. It has already fitted the dataset well. For small-scale backbones with poor performance, due to their limited fitting ability, there is a certain amount of noise in the extracted features during the inference stage. Denoising them can obtain better feedback.

(3) In fact, our method can be applied to any other backbone, just add it to each layer. In particular, the performance improvement of adding the denoising plugin to a poorly performing backbone might be more significant. This needs to be further verified in subsequent work. However, it is undeniable that we have verified the denoising ability of the DenoiseRep in the currently optimal ReID method.

In this section, a comparative analysis was conducted on four datasets to assess various existing ReID methods. These methods represent current mainstream ReID approaches, employing ResNet101, ViTS, ViT-B, and ResNet50 as backbone architectures for feature extraction, respectively. Experimental results indicate that our proposed method outperforms other approaches in terms of both mAP and Rank-1 metrics.

# 4.3 Analysis of Parameter Fusion

The proposed DenoiseRep is computation-free. In section 3.3, we proved by theoretical derivation that inserting our denoising layer into each feature layer and fusion it does not introduce additional computation. In this section, we also conduct related validation experiments, the results of which are shown in Table 4.

Table 4: Parameter Fusion Performance Analysis. The DenoiseRep− denoises based on the features of the final layer, while the DenoiseRep denoises based on the features of each layer. The baseline method TransReID-SSL is based on ViT-small backbone.   

<html><body><table><tr><td>Method</td><td>DukeMTMC</td><td>MSMT17</td><td>Market1501</td><td>CUHK-03</td><td>Inference Time</td></tr><tr><td>TransReID-SSL</td><td>80.40%</td><td>66.30%</td><td>91.20%</td><td>83.50%</td><td>0.34s</td></tr><tr><td>+DenoiseRep</td><td>80.76%</td><td>66.81%</td><td>91.07%</td><td>83.59%</td><td>0.39s (+15%)</td></tr><tr><td>+DenoiseRep</td><td>81.22%</td><td>67.33%</td><td>92.05%</td><td>84.11%</td><td>0.34s (+0%)</td></tr></table></body></html>

Compare to the baseline method TransReID-SSL, adding DenoiseRep− is able to improve the the performance, proving that feature based denoising is effective. However, it also brings extra inference latency (about $1 5 \%$ ) because it is adding an extra parameter-independent denoising module at the end of the model.

Adopting DenoiseRep achieves a greater increase, it denoise the features on each layer, which can better remove noise at each stage because the noise in the inference stage comes from multiple aspects, which may be the noise in the input image or generated when passing through the network. Denoising each layer avoids noise accumulation and obtains a better quality output. Most importantly, since the operation of fusion can merge the parameters of the denoising module with the original parameters, the adoption of DenoiseRep does not take extra inference latency cost, which is a computation-free efficient approach.

# 4.4 Experiments on Classification Tasks

The DenoiseRep is based on denoising at the feature level and demonstrates strong generalization capabilities. To validate this generalization ability, we conduct experiments on other vision tasks to test the effectiveness of the DenoiseRep. We validate the generalization ability of DenoiseRep in image classification tasks on ImageNet-1k [12] datasets and three fine-grained image classification datasets (CUB200 [55], Oxford-Pet [46], and Flowers [45]). The accuracy index is chosen as the evaluation metric to assess model performance.

As shown in Table 5, we compare multiple classic backbones for representation learning on ImageNet1k, and after adding the DenoiseRep, the accuracy of both top-1 and top-5 metrics improve without adding model parameters. Our method shows significant improvement in accuracy metrics compared to baseline on three fine-grained classification datasets. Prove that the DenoiseRep can improve the model’s ability in image classification for different classification tasks. Additionally, our method proves to enhance the model’s representation learning ability and extract more effective features through denoising without incurring additional time costs. More experimental analysis can be found in Table 7 in Section C of the appendix.

Table 5: The effectiveness of our method in image classification tasks was validated on three finegrained classification datasets (CUB200, Oxford-Pet, Flowers) and ImageNet-1k.   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">Datasets</td><td rowspan="2">Param</td><td colspan="2">acc@1</td><td colspan="2">acc@5</td></tr><tr><td>Baseline</td><td>+DenoiseRep</td><td>Baseline</td><td>+DenoiseRep</td></tr><tr><td>SwinV2-T[39]</td><td>ImageNet-1k</td><td>28M</td><td>81.82%</td><td>82.13%</td><td>95.88%</td><td>96.06%</td></tr><tr><td>Vmanba-T[38]</td><td>ImageNet-1k</td><td>30M</td><td>82.38%</td><td>82.51%</td><td>95.80%</td><td>95.89%</td></tr><tr><td>ResNet50[18]</td><td>ImageNet-1k</td><td>26M</td><td>76.13%</td><td>76.28%</td><td>92.86%</td><td>92.95%</td></tr><tr><td>ViT-B [14]</td><td>CUB200</td><td>87M</td><td>91.78%</td><td>91.99%</td><td>1</td><td></td></tr><tr><td>ViT-B</td><td>Oxford-Pet</td><td>87M</td><td>94.37%</td><td>94.58%</td><td>1</td><td></td></tr><tr><td>ViT-B</td><td>Flowers</td><td>87M</td><td>99.12%</td><td>99.30%</td><td></td><td></td></tr></table></body></html>

# 5 Conclusion

In this work, we demonstrate that the diffusion model paradigm is effective for feature level denoising in discriminative model, and propose a computation-free and label-free method: DenoiseRep. It utilizes the denoising ability of diffusion models to denoise the features in the feature extraction layer, and fuses the parameters of the denoising layer and the feature extraction layer, further improving retrieval accuracy without incurring additional computational costs. We validate the effectiveness of the DenoiseRep method on multiple common image discrimination task datasets.