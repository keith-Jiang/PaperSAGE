# Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning

Raffaele Paolino∗,1,2 Sohir Maskey∗,1 Pascal Welke3 Gitta Kutyniok1,2,4,5 1Department of Mathematics, LMU Munich 2Munich Center for Machine Learning (MCML) 3Faculty of Computer Science, TU Wien 4Institute for Robotics and Mechatronics, DLR-German Aerospace Center 5Department of Physics and Technology, University of Tromsø

# Abstract

We introduce $r$ -loopy Weisfeiler-Leman $\scriptstyle \left( r - \ell \mathbf { W } \mathbf { L } \right)$ , a novel hierarchy of graph isomorphism tests and a corresponding GNN framework, $r$ -ℓMPNN, that can count cycles up to length $r { + } 2$ . Most notably, we show that $r$ -ℓWL can count homomorphisms of cactus graphs. This extends 1-WL, which can only count homomorphisms of trees and, in fact, we prove that $r$ -ℓWL is incomparable to $k$ -WL for any fixed $k$ . We empirically validate the expressive and counting power of $r$ -ℓMPNN on several synthetic datasets and demonstrate the scalability and strong performance on various real-world datasets, particularly on sparse graphs. Our code is available on GitHub.

# 1 Introduction

Graph Neural Networks (GNNs) (Scarselli et al., 2009; Bronstein et al., 2017) have become a prevalent architecture for processing graph-structured data, contributing significantly to various applied sciences, such as drug discovery (Stokes et al., 2020), recommender systems (Fan et al., 2019), and fake news detection (Monti et al., 2019).

Among various architectures, Message Passing Neural Networks (MPNNs) (Gilmer et al., 2017) are widely used in practice, as they encompass only local computation, leading to fast and scalable models. Despite their success, the representational power of MPNNs is bounded by the WeisfeilerLeman (WL) test, a classical algorithm for graph isomorphism testing (Xu et al., 2019; Morris et al., 2019). This limitation hinders MPNNs from recognizing basic substructures like cycles (Chen et al., 2020). However, specific substructures can be crucial in many applications. For example, in organic chemistry, the presence of cycles can impact various chemical properties of the underlying molecules (Deshpande et al., 2002; Koyutürk et al., 2004). Therefore, it is crucial to investigate whether GNNs can count certain substructures and to design architectures that surpass the limited power of MPNNs.

Several models have been proposed to surpass the limitations of WL. Many of these models draw inspiration from higher-order WL variants (Morris et al., 2019), enabling them to count a broader range of substructures. For instance, GNNs emulating 3-WL can count cycles up to length 7. However, this increased expressivity comes at a high computational cost, as 3-WL does not respect the sparsity of real-world graphs, posing serious scalability issues. Hence, there is a critical need to design expressive GNNs that respect the inherent sparsity of real-world graphs (Morris et al., 2023).

![](images/786c94dd9ed05472f45df426aa23eb3084799742b4832092b4c0f41a6ceedd5a.jpg)  
Figure 1: Visual depiction of $r$ -ℓGIN: During preprocessing, we calculate the path neighborhoods $\mathcal { N } _ { r } ( v )$ for each node $\boldsymbol { v }$ in the graph $G$ . Paths of varying lengths are processed separately using simple GINs, and their embeddings are pooled to obtain the final graph embedding. The forward complexity scales linearly with the sizes of $\bar { \mathcal { N } } _ { r } ( v )$ , enabling efficient computation on sparse graphs.

Main Contributions. We introduce a novel class of color refinement algorithms called $r$ -loopy Weisfeiler-Leman test (r-ℓWL) and a corresponding class of GNNs named $r$ -loopy Graph Isomorphism Networks ( $r$ -ℓGIN). The key idea is to collect messages not only from neighboring nodes but also from the paths connecting any two distinct neighboring nodes, as illustrated in Figure 1. This approach enhances the resulting GNNs’ expressivity beyond 1-WL. In particular, $r$ -ℓWL can count cycles up to length $r { + } 2$ , even surpassing the $k$ -WL hierarchy.

Furthermore, we prove that $r$ -ℓWL can homomorphism-count any cactus graph with cycles up to length $r { + } 2$ . Cactus graphs are valuable due to their structural properties and simplicity, making them useful for modeling in areas such as electrical engineering (Nishi et al., 1986) and computational biology (Paten et al., 2011). For instance, aromatic compounds often form cactus graphs, where the molecular core, usually a cycle, is coonected to functional groups (e.g., carboxyl groups) that can significantly impact the properties of the molecule. Thus, the ability to homomorphism-count cactus graphs can enhance model performance, and it allows us to compare the expressive power of $r$ -ℓWL with other popular GNNs in a quantitative manner (Barceló et al., 2021; B. Zhang et al., 2024). Specifically, we show that $r$ -ℓWL is more expressive than GNNs that include explicit homomorphism counts of cycle graphs, known as $\mathcal { F }$ -Hom-GNNs (Barceló et al., 2021). Additionally, 1-ℓWL can already separate infinitely many graphs that Subgraph $k$ -GNNs (Frasca et al., 2022; Qian et al., 2022) cannot (see, e.g., Figure 7). The higher expressivity, paired with the local computations, highlights the enhanced potential of $r$ -ℓGIN, showing its competitive performance and the efficiency of its forward pass on real-world datasets, see Section 7.

# 2 Related Work

The notion of expressivity in standard neural networks is linked to the ability to approximate any continuous function (Cybenko, 1989; Hornik et al., 1989). In contrast, GNN expressivity is measured by the ability to distinguish non-isomorphic graphs. According to the Stone-Weierstrass theorem, these criteria are equivalent (Chen et al., 2019; Dasoulas et al., 2021): a network that can distinguish all graphs can approximate any continuous function. Therefore, research often focuses on determining which graphs a GNN can distinguish (Morris et al., 2023).

Xu et al. (2019) and Morris et al. (2019) proved that the expressive power of MPNNs is bounded by 1- WL. Subsequent works (Maron et al., 2018; Morris et al., 2019, 2020) introduced higher-order GNNs that have the same expressive power as $k$ -WL or its local variants (Geerts et al., 2022). Although these networks are universal (Maron et al., 2019b; Keriven et al., 2019), their exponential time and space complexity in $k$ renders them impractical. Abboud et al. (2022) proposed $k$ -hop GNNs which aggregate information from $k$ -hop neighbors, thus, enhancing expressivity beyond 1-WL but within 3-WL (Feng et al., 2022). Michel et al. (2023) and Graziani et al. (2024) construct GNNs that process paths emanating from each node to overcome 1-WL. Subgraph GNNs (Bevilacqua et al., 2021; You et al., 2021; Frasca et al., 2022; Huang et al., 2022) surpass 1-WL by decomposing the initial input graph into a bag of subgraphs. However, subgraph GNNs are upper-bounded by 3-WL (Frasca et al., 2022). A different line of work leverages positional encoding through unique node identifiers (Vignac et al., 2020), random features (Abboud et al., 2021; Sato et al., 2021) or eigenvectors (Lim et al., 2022; Maskey et al., 2022) to augment the expressive power of MPNNs.

While the predominant approach for gauging the expressive power of GNNs is within the $k$ -WL hierarchy, such a measure is inherently qualitative, as it cannot shed light on substructures a particular GNN can encode. Lovász (1967) showed that homomorphism counts is a complete graph invariant, meaning two graphs are isomorphic if and only if their homomorphism counts are identical. Building on this result, B. Zhang et al. (2024) advocate for homomorphism-count as a quantitative measure of expressivity, as GNN architectures can homomorphism-count particular families of motifs. Tinhofer (1986, 1991) established that 1-WL is equivalent to counting homomorphisms from graphs with tree-width one, while Dell et al. (2018) proved the equivalence between $k$ -WL and the ability to count homomorphisms from graphs with tree-width $k$ . Nguyen et al. (2020), Barceló et al. (2021), Welke et al. (2023), and Jin et al. (2024) used homomorphism counts to develop expressive GNNs.

Manually augmenting node features with homomorphism counts can be disadvantageous as performance depends on the chosen substructures. This can be alleviated by designing domain-agnostic GNNs that can learn structural information suitable for the task at hand. For instance, higher-order GNNs can count a large class of substructures as homomorphisms (B. Zhang et al., 2024), but they suffer from scalability issues. We propose $r$ -ℓWL and $r$ -ℓGIN, which can count homomorphisms of cactus graphs without adding explicit substructure counts. Our method is scalable to large datasets, particularly when the graphs in these datasets are sparse.

# 3 Preliminaries

Let $\mathcal { G }$ be the set of all simple and undirected graphs, and let $G \in { \mathcal { G } }$ . We denote the set of nodes by $V ( G )$ and the set of edges by $E ( G )$ . The direct neighborhood of a node $v \in V ( G )$ is defined as $\mathcal { N } ( v ) : = \{ u \in V ( G ) \mid \bar { \{ v , u \} } \in \dot { E } ( \bar { G } ) \}$ .

Definition 1. Let $F , G \in { \mathcal { G } }$ . A homomorphism from $F$ to $G$ is a map $h : V ( F ) \to V ( G )$ such that $\{ u , v \} \in E ( F )$ implies $\{ h ( u ) , h ( v ) \} \in E ( G )$ . $A$ subgraph isomorphism is an injective homomorphism.

Intuitively, a homomorphism from $F$ to $G$ is an edge-preserving map. A subgraph isomorphism ensures that $F$ actually occurs as a subgraph of $G$ . Consequently, it also maps distinct edges to distinct edges. A visual explanation can be found in Figure 5. We denote by ${ \mathrm { H o m } } ( F , G )$ the set of homomorphisms from $F$ to $G$ and by $\mathrm { h o m } ( F , G )$ its cardinality. Similarly, we denote by $\operatorname { S u b } ( F , G )$ the set of subgraph isomorphisms from $F$ to $G$ and by and $\operatorname { s u b } ( F , G )$ its cardinality.

# 3.1 Graph Invariants

In order to unify different expressivity measures, we recall the definition of graph invariants.

Definition 2. Let $P$ be a designated set, referred to as the palette. $A$ graph invariant is a function $\zeta : \mathcal { G }  P$ such that $\zeta ( G ) \doteq \zeta ( H )$ for all isomorphic pairs $G , H \in { \mathcal { G } }$ . $\zeta$ is $a$ complete graph invariant i ${ \mathfrak { c } } ( G ) \neq \zeta ( F )$ for all non-isomorphic pairs $G , F \in { \mathcal { G } }$ .

Complete graph invariants have maximal expressive power. However, no polynomial-time algorithm to compute a complete graph invariant is known. To compare the expressive power of different graph invariants, such as graph colorings and GNN architectures, we introduce the following definition.

Definition 3. Let $\gamma , \zeta$ be two graph invariants. We say that $\gamma$ is more powerful than $\zeta ( \gamma \subseteq \zeta )$ if for every pair $G , H \in { \mathcal { G } }$ , $\gamma ( G ) = \gamma ( H )$ implies $\zeta ( G ) = \zeta ( H )$ . We say that $\gamma$ is strictly more powerful than $\zeta i f \gamma \subseteq \zeta$ and there exists a pair $F , G \in { \mathcal { G } }$ such that $\gamma ( G ) \neq \gamma ( H )$ and $\zeta ( G ) = \zeta ( H )$ .

# 3.2 Message Passing Neural Networks and Weisfeiler-Leman

Message passing is an iterative algorithm that updates the colors of each node $v \in V ( G )$ as

$$
c ^ { ( t + 1 ) } ( v ) \gets f ^ { ( t + 1 ) } \left( c ^ { ( t ) } ( v ) , g ^ { ( t + 1 ) } \left( \left\{ \left\{ c ^ { ( t ) } ( u ) \ : | \ : u \in \mathcal { N } ( v ) \right\} \right\} \right) \right) .
$$

The graph output after $t$ iterations is given by

$$
c ^ { ( t ) } ( G ) : = h \left( \left\{ \left\{ c ^ { ( t ) } ( v ) \mid v \in V ( G ) \right\} \right\} \right) .
$$

Here, $g ^ { ( t ) } , h$ are functions on the domain of multisets and $f ^ { ( t ) }$ is a function on the domain of tuples. For each $t$ , the colorings $c ^ { ( t ) }$ are graph invariants. When the subsets of nodes with the same colors cannot be further split into different color groups, the algorithm terminates; the stable coloring after convergence is denoted by $c ( G )$ .

Choosing injective functions for all $f ^ { ( t ) }$ and setting $g ^ { ( t ) }$ and $h$ as the identity function results in 1-WL (Weisfeiler et al., 1968). If $f ^ { ( t ) } , g ^ { ( t ) } , h$ are chosen as suitable neural networks, one obtains a Message Passing Neural Network (MPNN). Xu et al. (2019) proved that MPNNs are as powerful as 1-WL if the functions $f ^ { ( t ) } , g ^ { ( t ) }$ , and $h$ are injective on their respective domains. The $k$ -WL algorithms uplift the expressive power of 1-WL by considering interactions between $k$ -tuples of nodes. This results in a hierarchy of strictly more powerful graph invariants (see Appendix B.1 for a formal definition).

# 3.3 Homomorphism and Subgraph Counting Expressivity

A more nuanced graph invariant can be built by considering the occurrences of a motif $F$ .

Definition 4. Let $F \in { \mathcal { G } }$ . A graph invariant $\zeta$ can homomorphism-count $F$ if for all pairs $G , H \in { \mathcal { G } }$ $\zeta ( G ) = \zeta ( H )$ implies $\operatorname { h o m } ( F , G ) = \operatorname { h o m } ( F , H )$ . By analogy, $\zeta$ can subgraph-count $F$ if for all pairs $G , H \in { \mathcal { G } }$ , $\zeta ( G ) = \zeta ( H )$ implies $\operatorname { s u b } ( F , G ) = \operatorname { s u b } ( F , H )$ .

If $\mathcal { F }$ is a family of graphs, we say that $\zeta$ can homomorphism-count $\mathcal { F }$ if $\zeta$ can homomorphism-count every $F \in { \mathcal { F } }$ ; we denote the vector of homomorphism-count by ${ \mathrm { h o m } } ( { \mathcal { F } } , G ) : = ( { \mathrm { h o m } } ( F , G ) ) _ { F \in { \mathcal { F } } }$ . Interpreting $\mathrm { h o m } ( \mathcal { F } , \cdot )$ as a graph invariant, given by $G \mapsto \operatorname { h o m } ( { \mathcal { F } } , G )$ , another graph invariant $\zeta$ can homomorphism-count $\mathcal { F }$ if and only if $\zeta \equiv \mathrm { h o m } ( \mathcal { F } , \cdot )$ .

The ability of a graph invariant to count homomorphisms is highly relevant because $\hom ( { \mathcal { G } } , \cdot )$ is a complete graph invariant. Conversely, if $\zeta$ is a complete graph invariant, then $\zeta$ can homomorphismcount all graphs (Lovász, 1967). Additionally, homomorphism-counting serves as a quantitative expressivity measure to compare different WL variants and GNNs, such as $k$ -WL, Subgraph GNNs, and other methods (Lanzinger et al., 2024; B. Zhang et al., 2024), and allows for relating them to our proposed $r$ -ℓWL variant, as detailed in Corollary 2.

# 4 Loopy Weisfeiler-Leman Algorithm

In this section, we introduce a new graph invariant by enhancing the direct neighborhood of nodes with simple paths between neighbors.

Definition 5. Let $G \in { \mathcal { G } }$ . $A$ simple path of length $r$ is a collection $\mathbf { p } = \{ p _ { i } \} _ { i = 1 } ^ { r + 1 }$ of $r { + } 1$ nodes such that $\{ p _ { i } , p _ { i + 1 } \} \in E ( G )$ and $i \neq j \implies p _ { i } \neq p _ { j }$ for every $i , j \in \{ 1 , \ldots , r \} ,$ ,.

Simple paths are the building blocks of $r$ -neighborhoods, which in turn are the backbone of our $r$ -ℓWL algorithm. The following definition is inspired by (Cantwell et al., 2019; Kirkley et al., 2021).

n 6. Let $G \in { \mathcal { G } }$ and $r \in \mathbb { N } \setminus \{ 0 \}$ , we define the $r$ -neighborhood $\mathcal { N } _ { r } ( v )$ of $v \in V ( G )$ as $\mathcal { N } _ { r } ( v ) : = \{ \mathbf { p } \ | \ \mathbf { \sigma } _ { \mathbf { J } }$ simple path of length $; p _ { 1 } , p _ { r + 1 } \in \mathcal { N } ( v ) , v \notin \mathbf { p } \}$ .

For consistency, we set $\mathcal { N } _ { 0 } ( v ) : = \mathcal { N } ( v )$ . An example of the construction of $r$ -neighborhood is shown in Figure 2, where different $r$ - neighborhoods of node $v$ are represented with different colors.

$$
\begin{array} { r l } { \bigotimes _ { i = 1 } \bigotimes _ { v } \bigotimes _ { i = 1 } \bigotimes _ { \emptyset } } & { { } \approx \mathcal { N } _ { 0 } ( v ) } \\ { \dot { \bigcirc } _ { \therefore \bigcirc } \bigotimes _ { \emptyset } \bigotimes _ { \emptyset } } & { { } - \mathcal { N } _ { 1 } ( v ) } \\ { \bigcirc _ { \therefore \bigcirc } \bigotimes _ { \emptyset } \bigotimes _ { \emptyset } } & { { } \dots \mathcal { N } _ { 2 } ( v ) } \end{array}
$$

We generalize 1-WL in (1) as follows.

Figure 2: Example of $r$ -neighborhoods.

Definition 7. We define the $r$ -loopy Weisfeiler-Leman $r$ -ℓWL) test by the following color update:

$c _ { r } ^ { ( t + 1 ) } ( v ) \gets \mathrm { H A S H } _ { r } \left( c _ { r } ^ { ( t ) } ( v ) , \left\{ \left\{ c _ { r } ^ { ( t ) } ( \mathbf { p } ) ~ | ~ \mathbf { p } \in \mathcal { N } _ { 0 } ( v ) \right\} \right\} , \dots , \left\{ \left\{ c _ { r } ^ { ( t ) } ( \mathbf { p } ) ~ | ~ \mathbf { p } \in \mathcal { N } _ { r } ( v ) \right\} \right\} \right) ,$ (2) where $c _ { r } ^ { ( t ) } ( \mathbf { p } ) : = \Big ( c _ { r } ^ { ( t ) } ( p _ { 1 } ) , c _ { r } ^ { ( t ) } ( p _ { 2 } ) , \dots , c _ { r } ^ { ( t ) } ( p _ { r + 1 } ) \Big )$ is the sequence of colors of nodes in the path.

We denote by $c _ { r } ^ { ( t ) } ( G )$ the final graph output after $t$ iterations of $r$ -ℓWL, i.e.,

$$
c _ { r } ^ { ( t ) } ( G ) = \mathrm { H A S H } _ { r } \left( \left\{ \left\{ c _ { r } ^ { ( t ) } ( v ) \mid v \in V ( G ) \right\} \right\} \right) ,
$$

and by $c _ { r } ( G )$ the stable coloring after convergence. The stable coloring $c _ { r }$ serves as graph invariant and will be referred to as $r$ -ℓWL.

# 5 Expressivity of $r$ -ℓWL

We analyze the expressivity of $r$ -ℓWL in terms of its ability to distinguish non-isomorphic graphs, subgraph-count, and homomorphism-count motifs. The proofs for all statements are in Appendix D.

# 5.1 Isomorphism Expressivity

It is straightforward to check that 0-ℓWL corresponds to 1-WL, since $\mathcal { N } _ { 0 } ( v ) = \mathcal { N } ( v )$ for all nodes $v$ .   
However, increasing $r$ leads to a strict increase in expressivity.

Proposition 1. Let $0 \leq q < r$ . Then, $r$ -ℓWL is strictly more powerful than $q$ -ℓWL. In particular, every $r$ -ℓWL is strictly more powerful than 1-WL.

This shows that the number of graphs we can distinguish monotonically increases with $r$ . We empirically verify this fact on several synthetic datasets in Section 7.

# 5.2 Subgraph Expressivity

Recent studies highlight limitations in the ability of certain graph invariants to subgraph-count cycles. For instance, 1-WL cannot subgraph-count cycles (Chen et al., 2020, Theorem 3.3), while 3-WL can only subgraph-count cycles of length up to 7 (Arvind et al., 2020, Theorem 3.5). Similarly, Subgraph GNNs have limited cycle-counting ability (Huang et al., 2022, Proposition 3.1). In contrast, $r$ -ℓWL can count cycles of arbitrary length, as shown in the following statement.

Theorem 1. For any $r \geq 1$ , $r$ -ℓWL can subgraph-count all cycles with at most $r + 2$ nodes.

Since 3-WL cannot subgraph-count any cycle with more than 7 nodes, Theorem 1 implies that 6-ℓWL is not less powerful than 3-WL. This observation generalizes to any $k$ -WL, as shown next.

Corollary 1. Let $k \in \mathbb { N }$ . There exists $r \in \mathbb { N }$ , such that $r – \ell W L$ is not less powerful than $k$ -WL.   
Specifically, $r \in \mathcal { O } ( k ^ { 2 } )$ , with $\textstyle r \leq { \frac { k ( k + 1 ) } { 2 } } - 2$ for even $k$ and $\begin{array} { r } { r \le \frac { ( k + 1 ) ^ { 2 } } { 2 } - 2 } \end{array}$ for odd $k$ .

The $r$ -ℓWL color refinement algorithm surpasses the limits of the $k$ -WL hierarchy while only using local computation. This is particularly important since already 3-WL is computationally infeasible, whereas our method can scale efficiently to higher orders if the graphs are sparse, which is commonly the case in real-world applications.

# 5.3 Homomorphism Expressivity

The following section unveils a close connection between the expressivity of $r$ -ℓWL and cactus graphs (Harary et al., 1953), a significant class between trees and graphs with tree-width 2.

Definition 8. A cactus graph is a graph where every edge lies on at most one simple cycle. For $r \geq 2$ , an $r$ -cactus graph is a cactus where every simple cycle has at most r vertices. We denote by the set of all cactus graphs, and by $\mathcal { M } ^ { r }$ the set of all $q$ -cactus graphs for $q \leq r$ .

Figure 6 shows two examples of cactus graphs. From the expressivity perspective, the ability to homomorphism-count cactus graphs establishes a lower bound strictly between the homomorphismcounting capabilities of 1-WL and 3-WL (Neuen, 2024), as cactus graphs are a strict superset of all trees and a strict subset of all graphs of treewidth two. With this in mind, we are now ready to present our significant result on the homomorphism expressivity of our $r$ -ℓWL algorithm.

Theorem 2. Let $r \geq 0$ . Then, $r$ -ℓWL can homomorphism-count $\mathbf { \mathcal { M } } ^ { r + 2 }$ .

We refer to Appendix G for a detailed proof of Theorem 2, which is fairly involved and requires defining canonical tree decompositions of cactus graphs and unfolding trees of $r$ -ℓWL. Demonstrating their strong connection, we then follow the approach in (Dell et al., 2018; B. Zhang et al., 2024) to decompose homomorphism counts of cactus graphs. In fact, we prove a more general result, showing that $r$ -ℓWL can count all fan-cactus graphs, see Appendix G for more details.

The class $\mathcal { M } ^ { 2 }$ contains only forests; hence, Theorem 2 implies the standard results on the ability of 1-WL to count forests. Since forests are the only class of graphs 1-WL can count, Theorem 2 implies that $r$ -ℓWL is always strictly more powerful than 1-WL, corroborating the claim in Proposition 1.

The implications of Theorem 2 are profound: it establishes that $r$ -ℓWL can homomorphism-count a large class of graphs. Specifically, Theorem 2 provides a quantitative expressivity measure that enables comparison of $r$ -ℓWL’s expressivity with other WL variants and GNNs. This comparison is achieved by examining the range of graphs that $r$ -ℓWL can homomorphism-count against those countable by other models, as detailed in works by Barceló et al. (2021) and B. Zhang et al. (2024). For instance, B. Zhang et al. (2024) showed that Subgraph GNNs (Bevilacqua et al., 2021; You et al., 2021; Frasca et al., 2022; Huang et al., 2022) are limited to homomorphism-count graphs with end-point shared NED. Hence, Subgraph GNNs can not homomorphism-count $F = \{ \cdots \Vec { 4 } \}$ , while 1-ℓWL can. Based on this, we can identify pairs of graphs that 1-ℓWL can distinguish but Subgraph GNNs cannot. We summarize these and other implications of Theorem 2 in the following corollary.

Corollary 2. Let $r \in \mathbb { N } \setminus \{ 0 \}$ . Then,

i) $r$ -ℓWL is more powerful than $\mathcal { F }$ -Hom-GNNs, where $\mathcal { F } = \{ C _ { 3 } , \ldots , C _ { r + 2 } \}$ .   
ii) 1-ℓWL is not less powerful than Subgraph GNNs. In particular, any $r { - } \ell W L$ can separate infinitely many graphs that Subgraph GNNs fail to distinguish.   
iii) For any $k > 0$ , 1-ℓWL is not less powerful than Subgraph $k$ -GNNs. In particular, any $r$ -ℓWL can separate infinitely many graphs that Subgraph $k$ -GNNs fail to distinguish.   
iv) $r$ -ℓWL can subgraph-count all graphs $F$ such that $\operatorname { s p a s m } ( F ) \subset \mathcal { M } ^ { r + 2 }$ , where $\operatorname { s p a s m } ( F ) : = \{ \bar { H } \in \mathcal { G } \mid \exists$ surjective $h \in { \mathrm { H o m } } ( F , H ) \}$ . In particular, $i f 1 \leq r \leq 4$ , then $r$ -ℓWL can subgraph-count all paths up to length $r + 3$ .

A detailed explanation of Subgraph ( $k$ -)GNNs, $\mathcal { F }$ -Hom-GNNs, along with the proofs of Corollary 2, can be found in Appendix H. Finally, we note that Theorem 2 states a loose lower bound on the homomorphism expressivity of $r$ -ℓWL. This observation opens the avenue for future research to explore tight lower bounds, or upper bounds, on the homomorphism expressivity of $r$ -ℓWL.

# 6 Loopy Message Passing

In this section, we build a GNN emulating $r$ -ℓWL.

Definition 9. For $t \in \{ 0 , \ldots , T - 1 \}$ and $k \in \{ 0 , \ldots , r \}$ , $r$ -ℓMPNN applies the following message, update and readout functions:

$$
\begin{array} { r l } & { m _ { k } ^ { ( t + 1 ) } ( v ) = f _ { k } ^ { ( t + 1 ) } \left( \left\{ \left\{ c _ { k } ^ { ( t ) } ( \mathbf { p } ) \mid \mathbf { p } \in \mathcal { N } _ { k } ( v ) \right\} \right\} \right) , } \\ & { ~ c _ { r } ^ { ( t + 1 ) } ( v ) = g ^ { ( t + 1 ) } \left( c _ { r } ^ { ( t ) } ( v ) , m _ { 0 } ^ { ( t + 1 ) } ( v ) , \dots , m _ { r } ^ { ( t + 1 ) } ( v ) \right) , } \\ & { y e r c _ { r } ^ { ( T ) } ( G ) = h \left( \left\{ \left\{ c _ { r } ^ { ( T ) } ( v ) \mid v \in V ( G ) \right\} \right\} \right) . } \end{array}
$$

and final readout la

In the following statement, we link the expressive power of $r$ -ℓMPNN and $r$ -ℓWL.

Theorem 3. For fixed $t , r \geq 0$ , t iterations of $r – \ell W L$ are more powerful than $r$ -ℓMPNN with $t$ layers.   
Conversely, $r$ -ℓMPNN is more powerful than $r$ -ℓWL if the functions $f ^ { ( t ) } , g ^ { ( t ) }$ in (3) are injective.

The previous result derives conditions under which $r$ -ℓMPNN is as expressive as $r$ -ℓWL. To implement $r$ -ℓMPNN in practice, we choose suitable neural layers for $f _ { k } ^ { ( t ) } , \bar { g } ^ { ( t ) }$ , and $h$ in Definition 9. As a consequence of ( $\mathrm { \Delta X u }$ et al., 2019, Lemma 5), the aggregation function in (3) can be written as

$$
f _ { k } ^ { ( t + 1 ) } \left( \left\{ \left\{ c _ { k } ^ { ( t ) } ( \mathbf { p } ) \mid \mathbf { p } \in \mathcal { N } _ { k } ( v ) \right\} \right\} \right) : = f \left( \sum _ { \mathbf { p } \in \mathcal { N } _ { k } ( v ) } g ( \mathbf { p } ) \right) ,
$$

for suitable functions $f , g$ . Since 1-WL is injective on forests (Arvind et al., 2015), hence on paths, and since GIN can approximate 1-WL (Xu et al., 2019), we choose $f =  { \mathrm { M L P } }$ and $g = \mathrm { G I N }$ . Hence, $r$ -ℓGIN is defined as an $r$ -ℓMPNN that updates node features via

$$
x _ { r } ^ { ( t + 1 ) } ( v ) : = \mathrm { M L P } \left( x _ { r } ^ { ( t ) } ( v ) + ( 1 + \varepsilon _ { 0 } ) \sum _ { u \in \mathcal { N } _ { 0 } ( v ) \ } x _ { r } ^ { ( t ) } ( u ) + \sum _ { k = 1 } ^ { r } ( 1 + \varepsilon _ { k } ) \sum _ { \mathbf { p } \in \mathcal { N } _ { k } ( v ) } \mathrm { G I N } _ { k } ( \mathbf { p } ) \right) .
$$

To reduce the number of learnable parameters in (4), the $\mathrm { G I N } _ { k }$ can be shared among all $k$ . Nothing prevents from choosing a different path-processing layer; we opted for GIN because it is simple yet maximally expressive on paths. We refer to Figure 1 for a visual depiction of $r$ -ℓGIN.

Computational Complexity The complexity of $r$ -ℓGIN is $\begin{array} { r } { \mathcal { O } ( | E | + \sum _ { v \in V ( G ) } \sum _ { k = 1 } ^ { r } 2 k | \mathcal { N } _ { k } ( v ) | ) } \end{array}$ . The former addend is the standard message complexity, while the latter arises from applying GIN to paths of length $k \leq r$ . This implies that our model’s complexity scales linearly with the number of edges, and with the number of paths within $\mathcal { N } _ { k } ( v )$ . The number of such paths is typically less than the number of edges. For example, ZINC12K has overall 598K edges while only containing 374K paths in $\mathcal { N } _ { r } ( v )$ for $1 \leq r \leq 5$ . Hence, the runtime overhead is small in practice. Compared to 3-WLGNN (Dwivedi et al., 2022a), which has the same cycle-counting expressivity, our model requires ca. 10 seconds/epoch while 3-WLGNN takes ca. 329.49 seconds/epoch on ZINC12K. Our runtime is comparable to that of GAT, MoNet, or GatedGCN (see Table 10 for a thorough comparison).

Comparison with (Michel et al., 2023) PathNN updates node features by computing all possible paths starting from each node. In contrast, our approach selects paths between distinct neighbors, potentially resulting in fewer paths. For instance, a tree’s $r$ -neighborhoods $( r \geq 1 )$ ) are empty, while counts of paths between nodes are quadratic. Notably, Michel et al. (2023) do not explore the impact of increasing the path length on architecture expressiveness, a consideration we address (see, e.g., Proposition 1 and Corollary 1). Another significant contribution of our work, which we assert does not hold (at least not trivially) for PathNN, is the provable ability to subgraph-count cycle graphs (see, e.g., Theorem 1) and homomorphism-count cactus graphs (see, e.g., Theorem 2).

# 7 Experiments

All instructions to reproduce the experiments are available on GitHub (MIT license). Additional information on the training and test details can be found in Appendix C.

Expressive Power. We showcase the expressive power of $r$ -ℓGIN on synthetic datasets:

• GRAPH8C (Balcilar et al., 2021) comprises 11 117 connected non-isomorphic simple graphs on 8 nodes; 312 pairs are 1-WL equivalent but none is 3-WL equivalent. • EXP_ISO (Abboud et al., 2022) comprises 600 pairs of 1-WL equivalent graphs.

![](images/366be0f9cd6632689736c2c881823c05298fbf3bd74fb9f21e9ff2504b661804.jpg)  
Figure 3: Indistinguishable pairs at initialization, symlog scale. For GRAPH8C and EXP_ISO, we report the proportion of indistinguished pairs: 2 graphs are deemed indistinguishable if the ${ \mathrm { L } } ^ { 1 }$ distance of their embeddings is less than $1 0 ^ { - 3 }$ . For COSPECTRAL10 and SR16622, we report the ${ \mathrm { L } } ^ { 1 }$ distance between graph embeddings. We report the mean and standard deviation over 100 seeds.

Table 1: Num. of distinguished pairs (↑). Results from (Wang et al., 2024).   

<html><body><table><tr><td>Model</td><td>Basic (60)</td><td>Regular (140)</td><td>Extension (100)</td><td>CFI (100)</td></tr><tr><td>3-WL</td><td>60</td><td>50</td><td>100</td><td>60</td></tr><tr><td>PPGN</td><td>60</td><td>50</td><td>100</td><td>23</td></tr><tr><td>NestedGNN</td><td>59</td><td>48</td><td>59</td><td>0</td></tr><tr><td>GSN</td><td>60</td><td>99</td><td>95</td><td>0</td></tr><tr><td>OSAN</td><td>52</td><td>41</td><td>82</td><td>2</td></tr><tr><td>4-lGIN</td><td>60</td><td>100</td><td>95</td><td>2</td></tr></table></body></html>

![](images/69b6329e6fc8086f0feda2cd17250fd4ee6741a522a6c4dc752ec27cfc35e09f.jpg)  
Figure 4: Test accuracy on synthetic classification task: (left) shared and (right) non-shared weights.

• COSPECTRAL10 (van Dam et al., 2003): the dataset comprises two cospectral 4-regular non-isomorphic graphs on 10 nodes which are 1-WL equivalent (see, e.g., Figure 8a). • SR16622 (Michel et al., 2023) comprises two strongly regular graphs on 16 nodes, namely the Shrikhande and the $4 { \times } 4$ rook graph, which are 3-WL equivalent (see, e.g., Figure 8b).

The goal is to check whether the model can distinguish non-isomorphic pairs at initialization. The results are shown in Figure 3.

Additionally, Table 1 shows the performance on BREC (Wang et al., 2024), which includes 400 pairs of non-isomorphic graphs ranging from 1-WL to 4-WL equivalent. The baselines include PPGN, which is 3-WL equivalent and can count up to 7-cycles and homomorphism-count all graphs of tree-width 2; NestedGNN which is between 1-WL and 3-WL; GSN which is more powerful than 1-WL but whose expressive power depends on the chosen pattern.

Finally, Figure 4 reports the performance on synthetic classification tasks:

• EXP, CEXP (Abboud et al., 2021) require expressive power beyond 1-WL.   
• CSL (Murphy et al., 2019) comprises 150 cycle graphs with skip links (see, e.g., Figure 8c). The task is to predict the length of the skip link.

Counting Power. Following (B. Zhang et al., 2024), we use the SUBGRAPHCOUNT dataset (Chen et al., 2020) to test the ability to homomorphism- and subgraphs-count exemplary motifs.

Table 2: Test MAE for homomorphism- and subgraph-counts. Results from (B. Zhang et al., 2024).   

<html><body><table><tr><td rowspan="2"></td><td colspan="3">hom(F,G)</td><td colspan="6">sub(F,G)</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MPNN</td><td>0.300</td><td>0.233</td><td>0.254</td><td>0.358</td><td>0.208</td><td>0.188</td><td>0.146</td><td>0.261</td><td>0.205</td></tr><tr><td>Subgraph GNN</td><td>0.011</td><td>0.015</td><td>0.012</td><td>0.010</td><td>0.020</td><td>0.024</td><td>0.046</td><td>0.007</td><td>0.027</td></tr><tr><td>Local 2-GNN</td><td>0.008</td><td>0.008</td><td>0.010</td><td>0.008</td><td>0.011</td><td>0.017</td><td>0.034</td><td>0.007</td><td>0.016</td></tr><tr><td>Local 2-FGNN</td><td>0.003</td><td>0.005</td><td>0.004</td><td>0.003</td><td>0.004</td><td>0.010</td><td>0.020</td><td>0.003</td><td>0.010</td></tr><tr><td>r-lGIN</td><td>0.001 (r=2)</td><td>0.006 (r=3)</td><td>0.009 (r=3)</td><td>0.0005 (r=1)</td><td>0.0005 (r=2)</td><td>0.0003 (r=3)</td><td>0.0003 (r=4)</td><td>0.001 (r=2)</td><td>0.0004 (r=3)</td></tr></table></body></html>

There is a strict hierarchy in the expressive power of the baselines: MPNN $\sqsubseteq$ Subgraph GNN $\sqsubseteq$ local 2-GNN $\sqsubseteq$ local 2-FGNN. These variants, apart from MPNNs, are more expressive than 1-WL and can subgraph-count up to 7-cycles.

Real-World Datasets. We experimented with three benchmark datasets: ZINC250K (Irwin et al., 2012), ZINC12K (Dwivedi et al., 2022a), and QM9 (Wu et al., 2018) which consist of 250 000, 12 000, and 130 831 molecular graphs, respectively. We report the mean and standard deviation over 4 random seeds.

For ZINC250K and ZINC12K, we selected as baseline models standard MPNNs (GIN, GCN, GAT), Subgraph GNNs (NestedGNN, GNNAK $^ +$ , SUN), domain-agnostic GNNs fed with substructure counts (GSN, CIN), a GNN processing paths (PathNN), and expressive GNNs with provable cycle counting power (HIMP, SignNet, I2-GNN, DRFWL). Following the standard procedure, we kept the number of parameters under 500K (Dwivedi et al., 2022a) for ZINC12K. The results are detailed in Table 3.

For the QM9 dataset, we followed the setup of (Huang et al., 2022; Zhou et al., 2023). Specifically, the test MAE is multiplied by the standard deviation of the target and divided by the corresponding conversion unit. The baseline results and models were obtained from (Zhou et al., 2023), including expressive GNNs with provable cycle counting power. We omit methods that use additional geometric features to focus on the model’s expressive power. The results are presented in Table 4.

Table 3: Test MAE ( ) on ZINC dataset.   

<html><body><table><tr><td>Model</td><td>ZINC12K</td><td>ZINC250K</td></tr><tr><td>GIN</td><td>0.163±0.004</td><td>0.088 ±0.002</td></tr><tr><td>GCN</td><td>0.321 ± 0.009</td><td></td></tr><tr><td>GAT</td><td>0.384 ±0.007</td><td></td></tr><tr><td>GSN</td><td>0.115 ± 0.012</td><td></td></tr><tr><td>CIN</td><td>0.079 ± 0.006</td><td>0.022 ± 0.002</td></tr><tr><td>NestedGNN SUN</td><td>0.111 ± 0.003 0.083 ±0.003</td><td>0.029 ±0.001</td></tr><tr><td>GNNAK+</td><td>0.080 ± 0.001</td><td>1</td></tr><tr><td>I2-GNN</td><td>0.083 ±0.001</td><td>0.023±0.001</td></tr><tr><td>DRFWL</td><td>0.077±0.002</td><td>0.025 ±0.003</td></tr><tr><td>SignNet</td><td>0.084 ± 0.004</td><td>0.024± 0.003</td></tr><tr><td>HIMP</td><td>0.151 ± 0.006</td><td>0.036 ±0.002</td></tr><tr><td>PathNN</td><td>0.090 ±0.004</td><td></td></tr><tr><td>5-lGIN</td><td>0.072 ± 0.002</td><td>0.022 ± 0.001</td></tr></table></body></html>

Table 4: Normalized test MAE ( ) on QM9 dataset. Top three models as st nd rd   

<html><body><table><tr><td colspan="11">Model</td></tr><tr><td>Target</td><td>1-GNN</td><td>1-2-3-GNN</td><td>DTNN</td><td>Deep LRP </td><td>NestedGNN</td><td>I2-GNN</td><td>DRFWL</td><td>5-lGIN</td><td></td></tr><tr><td>μ</td><td>0.493</td><td>0.476</td><td>0.244</td><td>0.364</td><td>0.428</td><td>0.428</td><td>0.346</td><td>0.350 ±0.011</td><td></td></tr><tr><td>α</td><td>0.78</td><td>0.27</td><td>0.95</td><td>0.298</td><td>0.290</td><td>0.230</td><td>0.222</td><td>0.217±0.025</td><td></td></tr><tr><td>εhomo</td><td>0.00321</td><td>0.00337</td><td>0.00388</td><td>0.00254</td><td>0.00265</td><td>0.00261</td><td>0.00226</td><td></td><td>0.00205 ±0.00005</td></tr><tr><td>εlumo</td><td>0.00355</td><td>0.00351</td><td>0.00512</td><td>0.00277</td><td>0.00297</td><td>0.00267</td><td>0.00225</td><td></td><td>0.00216 ±0.00004</td></tr><tr><td>△(ε)</td><td>0.0049</td><td>0.0048</td><td>0.0112</td><td>0.00353</td><td>0.0038</td><td>0.0038</td><td>0.00324</td><td></td><td>0.00321 ±0.00014</td></tr><tr><td>R²</td><td>34.1</td><td>22.9</td><td>17.0</td><td>19.3</td><td>20.5</td><td>18.64</td><td>15.04</td><td>13.21 ±0.19</td><td></td></tr><tr><td>ZVPE</td><td>0.00124</td><td>0.00019</td><td>0.00172</td><td>0.00055</td><td>0.0002</td><td>0.00014</td><td>0.00017</td><td>0.000127 ±0.000003</td><td></td></tr><tr><td>Uo</td><td>2.32</td><td>0.0427</td><td>2.43</td><td>0.413</td><td>0.295</td><td>0.211</td><td>0.156</td><td>0.0418 ±0.0520</td><td></td></tr><tr><td>U</td><td>2.08</td><td>[0.111</td><td>2.43</td><td>0.413</td><td>0.361</td><td>0.206</td><td>0.153</td><td></td><td>0.023±0.023</td></tr><tr><td>H</td><td>2.23</td><td>0.0419</td><td>2.43</td><td>0.413</td><td>0.305</td><td>0.269</td><td>0.145</td><td></td><td>0.0352 ±0.0304</td></tr><tr><td>G</td><td>1.94</td><td>0.0469</td><td>2.43</td><td>0.413</td><td>0.489</td><td>0.261</td><td>0.156</td><td></td><td>0.0118 ±0.0015</td></tr><tr><td>Cu</td><td>0.27</td><td>0.0944</td><td>2.43</td><td>0.129</td><td>0.174</td><td>0.0730</td><td>0.0901</td><td></td><td>0.0702 ±0.0024</td></tr></table></body></html>

Discussion of Results The results in Figures 3 and 4 and Table 1 constitute a strong empirical validation of our theory: increasing $r$ leads to more expressive $r$ -ℓMPNN. Albeit 6-ℓWL is not less powerful than 3-WL (see, e.g., Section 5.2), in practice, smaller values of $r$ can already distinguish pair of graphs that are 3-WL equivalent, such as the Shrikhande and the $( 4 \times 4 )$ rook graphs. In the BREC dataset, 4-ℓGIN distinguishes all pairs of strongly regular graphs, significantly outperforming 3-WL (0/50 graphs). Notably, 4-ℓGIN can already distinguish 257 out of 400 total pairs of graphs, surpassing other expressive GNNs like PPGN (233/400), theoretically equivalent to 3-WL, and NestedGNN (166/400). Refer to (Wang et al., 2024, Table 2) for detailed baseline results.

The results in Table 2 further substantiate our theory, as $r$ -ℓWL can effectively count cycles of length $r { + } 2$ (see, e.g., Theorem 1).

On molecular datasets, we observe that $r$ -ℓGIN, although designed for subgraph-counting cycles and homomorphism-counting cactus graphs, is highly competitive. Notably, we outperform the baseline 0-ℓGIN by $22 6 \%$ on ZINC12K and $400 \%$ on ZINC250K and surpass domain-agnostic methods such as CIN or GSN. We conjecture that this is attributed to straightforward optimization, driven by the simplicity of the architecture (see, e.g., Figure 1) and its inductive bias towards counting cycles.

Limitations Path calculations can become infeasible for dense graphs due to $\mathcal { O } ( N d ^ { r } )$ complexity, where $N$ is the number of nodes and $d$ is the average degree. However, for sparse graphs, the runtime remains reasonably low. For instance, preprocessing ZINC12K for $r = 5$ takes just over a minute.

# 8 Conclusion

In this paper,we introduce a novel hierarchy of color refinement algorithms, denoted as $r$ -ℓWL, which incorporates an augmented neighborhood mechanism accounting for nearby paths. We establish connections between $r$ -ℓWL and the classical $k$ -WL. We construct a GNN ( $\dot { \boldsymbol { r } }$ -ℓMPNN) designed to emulate and match the expressive powerof $r$ -ℓWL. Theoretical and empirical evidence support the claim that $r$ -ℓMPNN can effectively subgraph-count cycles and homomorphism-count cactus graphs.

Future research could focus on precisely characterizing the expressivity of $r$ -ℓWL tests by identifying the maximal class of graphs that $r$ -ℓWL can homomorphism-count. This would facilitate comparisons by constructing pairs of graphs that $r$ -ℓWL cannot separate, but other WL variants can. Another promising direction involves exploring the generalization capabilities of GNNs with provable homomorphism-counting properties. The ability to homomorphism-count certain motifs could provide a mathematical framework to support the intuitive notion that the capacity to count relevant features may improve generalization. We observed this improved generalization experimentally in our ablation study on ZINC12K (see, e.g., Table 8).