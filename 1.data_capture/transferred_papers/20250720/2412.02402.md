# RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation

Changli $\mathbf { W _ { u } } ^ { 1 , 2 * }$ , Qi Chen1∗, Jiayi $\mathbf { J i } ^ { 1 , 4 }$ , Haowei $\mathbf { W a n g ^ { 3 } }$ , Yiwei $\mathbf { M } \mathbf { a } ^ { 1 }$ , You Huang1, Gen Luo1, Hao Fei4, Xiaoshuai $\mathbf { S u n } ^ { 1 }$ , Rongrong $\mathbf { J i ^ { \mathrm { 1 \dagger } } }$ 1 Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China 2 Shanghai Innovation Institute, Shanghai, P.R. China 3 Youtu Lab, Tencent, Shanghai, P.R. China 4 National University of Singapore

# Abstract

3D Referring Expression Segmentation (3D-RES) aims to segment 3D objects by correlating referring expressions with point clouds. However, traditional approaches frequently encounter issues like over-segmentation or mis-segmentation, due to insufficient emphasis on spatial information of instances. In this paper, we introduce a Rule-Guided Spatial Awareness Network (RG-SAN) by utilizing solely the spatial information of the target instance for supervision. This approach enables the network to accurately depict the spatial relationships among all entities described in the text, thus enhancing the reasoning capabilities. The RG-SAN consists of the Text-driven Localization Module (TLM) and the Rule-guided Weak Supervision (RWS) strategy. The TLM initially locates all mentioned instances and iteratively refines their positional information. The RWS strategy, acknowledging that only target objects have supervised positional information, employs dependency tree rules to precisely guide the core instance’s positioning. Extensive testing on the ScanRefer benchmark has shown that RG-SAN not only establishes new performance benchmarks, with an mIoU increase of 5.1 points, but also exhibits significant improvements in robustness when processing descriptions with spatial ambiguity. All codes are available at https://github.com/sosppxo/RG-SAN.

# 1 Introduction

3D Referring Expression Segmentation (3D-RES) is an emerging field that segments 3D objects in point cloud scenes based on given referring expressions [24]. Gaining significant attention for its applications in autonomous robotics, human-machine interaction, and self-driving systems, 3D-RES demands a deeper understanding than 3D Referring Expression Comprehension (3D-REC) [5, 71, 1, 73, 68], which focuses only on locating the referring objects via bounding boxes. 3D-RES, on the other hand, requires identifying instances and providing precise 3D masks.

Early 3D-RES approaches [24, 71] adopted a two-stage paradigm, starting with an independent textagnostic segmentation model for generating instance proposals, followed by linking these proposals with textual descriptions. This paradigm, separating segmentation and matching, proved suboptimal in performance and efficiency. Recent explorations have shifted towards an end-to-end paradigm. For instance, 3D-STMN [63] achieved efficient segmentation by directly matching superpoints with text, while 3DRefTR [41] integrated 3D-RES and 3D-REC into a unified framework using a multi-task approach, boosting inference in both tasks. Despite these advancements, limitations persist, primarily due to over-reliance on textual reasoning and insufficient modeling of spatial relationships between instances. For example, as shown in Fig. 1, without spatial modeling, it’s challenging to understand and correctly segment the intended chair in scenarios involving complex spatial terms like “far away”.

To tackle this issue, the core is to assist textual reasoning by modeling the spatial relationships of core instances. By effectively identifying these spatial relationships within expressions, a substantial improvement can be achieved in comprehending spatial arrangements. Nevertheless, this endeavor is not without its challenges. While accurate positional information is crucial for ensuring precise modeling of spatial relationships, accurately regressing instance positions from textual information is far from a simple task. Furthermore, our available positional information is limited to the target instance, leaving us without supervisory signals for other instances referenced in the expression.

To overcome these challenges, we propose the novel Rule-Guided Spatial Awareness Network (RG-SAN), utilizing the spatial information of the target instance for supervision. This enables the network to accurately depict spatial relationships among all text-described entities, thereby significantly enhancing the model’s inference and pointing capabilities. RG-SAN consists of two main components: the Text-driven Localization Module (TLM) and the Rule-guided Weak Supervision (RWS) strategy. TLM initially lo

![](images/178f4ecfd548ee01ab9f8635d80696e0d6d2f7d47588f4652c62fc3869ad6b0b.jpg)  
Figure 1: Illustration with a target object and multiple auxiliary objects, associated with a referring expression. The target marked in green represents the main referred instance, while targets in other colors indicate other mentioned entities. This visual highlights the challenge of effectively completing semantic reasoning in the absence of spatial inference.

cates all mentioned instances and iteratively refines their positions, ensuring continuous improvement in location accuracy. RWS, leveraging dependency tree rules, precisely guides the positioning of core instances. This focused supervision significantly improves the handling of spatial ambiguities in referring expressions. Extensive testing on the ScanRefer benchmark shows that RG-SAN not only sets new performance standards, with a mIoU increase of 5.1 points, but also greatly enhances robustness in processing spatially ambiguous descriptions.

To sum up, our main contributions are as follows:

• We introduce RG-SAN, a novel approach for modeling spatial relationships among all entities in expressions, which enhances the model’s referring ability in 3D-RES.   
• We propose the TLM for precise localization of all instances mentioned in expressions, and RWS, utilizing only the target instance’s location for supervising the spatial positioning of all instances.   
• Extensive experiments on the ScanRefer benchmark demonstrate the effectiveness of the proposed RG-SAN, showing significant improvements in performance and robustness in 3D-RES tasks.

# 2 Related Work

# 2.1 3D Referring Expression Comprehension and Referring Expression Segmentation

Referring Expression Comprehension (REC) is proposed to locate the referred target from a short description of visual space by bounding boxes [72, 57, 29], which is part of vision-language tasks [12, 10, 11, 18, 67, 66, 15]. Recent works in 3D-REC can be divided into two parts, two-stage and single-stage. As for two-stage methods [5, 1, 73, 71, 70, 24, 13], 3D object proposals are generated directly from ground-truth [1] or extracted by a pre-trained 3D object detector [50] in the first stage, and then assigned to language in the second stage. In the other way, some methods adopt a one-stage paradigm [45, 26, 68, 64], enabling end-to-end training.

Referring Expression Segmentation (RES) need fine-grained vision-language alignment [36, 37, 16, 35], proposed to locate the referred target by masks [27, 59, 25]. TGNN [24] introduce 3D-RES by extending the bounding box annotations of ScanRefer [5] to masks by incorporating the instance masks from ScanNet and proposed a two-stage pipeline. Further, 3D-STMN [63] proposed an end-to-end method that matches the text and superpoints to get the 3D segmentation of the target object directly.

# 2.2 3D Human-AI Interaction

ScanQA [3] has notably advanced visual question answering in 3D scenes, enhancing the human-AI interaction experience. Meanwhile, 3D-LLM [21], 3D-VisTA [75], NaviLLM [74], and BridgeQA [49] have further propelled this task. Li et al. [38, 39], Lu et al. [44] have explored how AI understands human instructions like gestures and language to locate targets. 3D-VisTA [75] introduced a new paradigm for large-scale 3D vision-language pre-training, greatly enhancing AI’s understanding of 3D vision-language and advancing various downstream tasks. Works like 3D-LLM [21], Chat3D [62, 22], NaviLLM [74] and Scene-LLM [14] have extended the capabilities of multimodal large language models to the 3D realm, endowing embodied intelligence with the rich knowledge and capabilities of LLMs, thus ushering in the era of large models in Human-AI Interaction.

# 2.3 Weakly Supervision in Vision-and-Language

In the field of Vision Language, weakly supervised [33, 42, 34, 4] have gained significant attention and great progress. These approaches aim to tackle the challenge of limited or incomplete annotations by leveraging alternative supervised data or weakly labeled data. For weakly supervised visual question answering (VQA), Kervadec et al. [28] employ weak supervision in the form of object-word alignment as a pre-training task. Trott et al. [60] use object counts in images as weak supervision to guide VQA for counting-based questions. Gokhale et al. [17] employ logical connective rules to augment training datasets for yes-no questions. Weakly supervision from captions has also been employed for visual grounding tasks [9, 48, 2] recently. Especially, for RES, some methods [33, 42] localize the target object only using readily available image-text pairs.

# 3 Method

In this section, we provide a comprehensive overview of the RG-SAN. The framework is illustrated in Fig. 2. First, the features of visual and linguistic modalities are extracted in parallel (Sec. 3.1). Next, we demonstrate the process of TLM (Sec. 3.2.1). Finally, we outline the RWS and the training objectives (Sec. 3.3).

# 3.1 Feature Extraction

# 3.1.1 Visual Encoding

Given a point cloud scene $\mathbf { P } _ { c l o u d } \in \mathbb { R } ^ { N _ { p } \times ( 3 + F ) }$ with $\mathcal { N } _ { p }$ points. Each point comes with 3D coordinates along with an $F$ -dimensional auxiliary feature that includes RGB, normal vectors, among others. We first employ a Sparse 3D U-Net [19] to extract point-wise features, represented as $\hat { \mathbf { P } } _ { \mathbf { c l o u d } } \in \mathbb { R } ^ {  { N _ { p } } \times  { C _ { p } } }$ . Then, we follow Sun et al. [58] and Wu et al. [63] to obtain $\mathcal { N } _ { s }$ superpoints $\{ \mathcal { K } _ { i } \} _ { i = 1 } ^ { \mathcal { N } _ { s } }$ [32] from the original point cloud. Finally, we directly feed point-wise features $\hat { \mathbf { P } } _ { \mathbf { c l o u d } }$ into superpoint pooling layer based on $\{ \mathcal { K } _ { i } \} _ { i = 1 } ^ { \mathcal { N } _ { s } }$ to obtain the superpoint-level features $\mathbf { S } _ { p } \in \mathbb { R } ^ {  { N _ { s } } \times  { C _ { p } } }$ .

# 3.1.2 Linguistic Encoding

Given a free-form plain text description of the target object, consisting of $\mathcal { N } _ { t }$ words $\{ c _ { i } \} _ { i = 1 } ^ { \mathcal { N } _ { t } }$ , we utilize a pre-trained MPNet model [56] to extract $C _ { t }$ -dimensional word-level embeddings, represented as E0 RNt×Ct.

![](images/4088ae28526e8f790ec9229eaf2bacfd63b6ad3b2c8b24da15a387598b6b6618.jpg)  
Figure 2: An overview of the proposed RG-SAN. This model analyzes a point cloud and a textual description with $\mathcal { N } _ { t }$ tokens, extracting superpoints and word-level features. The TLM assigns spatial positions to tokens, facilitating multimodal fusion. The RWS strategy enables the model to learn the positions of all mentioned entities using only the supervision of the target position.

# 3.2 Context-driven Spatial Awareness

In this section, we address a key limitation in prior works that interact point clouds with text without considering spatial positioning [63, 45, 68]. Unlike these methods, which often lose spatial information due to unordered point cloud features, leading to ambiguous spatial relationship understanding, our approach is distinct. In 3D-RES, spatial information is inherently sparse and dynamic, depending on the specific target object described in the text, rather than the dense, static sampling of an entire point cloud scene [31].

To address this issue, we propose to facilitate interactions between textual entities and point clouds within 3D space, rather than merely at the semantic level. Specifically, our objective is to fully leverage semantic and spatial contextual information to accurately predict the spatial positions of all mentioned nouns within the point cloud.

Therefore, we introduce the Text-driven Localization Module (TLM) to initialize the positions of entity nouns in the text and continuously update and refine these positions through iterative multimodal interactions.

# 3.2.1 Text-driven Localization Module

Given the superpoint features $\mathbf { S } _ { p }$ and word embeddings $\mathbf { E } _ { 0 }$ , we first project the features into the same dimension, and enhance the word-level embeddings by Dependency-Driven Interaction (DDI), following ${ \sf W } { \sf u }$ et al. [63]:

$$
\begin{array} { r } { \hat { \mathbf { E } } _ { 0 } = \mathbf { D } \mathbf { D } \mathbf { I } ( \mathbf { E } _ { 0 } \mathbf { W } _ { l a n g } ) , \quad \hat { \mathbf { S } } = \mathbf { S } _ { p } \mathbf { W } _ { v i s } , } \end{array}
$$

where $\mathbf { W } _ { l a n g } \in \mathbb { R } ^ { C _ { t } \times D }$ and $\mathbf { W } _ { v i s } \in \mathbb { R } ^ { C _ { p } \times D }$ denote learnable parameters, and the subscript of $\mathbf { E }$ and $\hat { \mathbf { E } }$ represents the round number.

Text-driven Initialization. The key is to map the text into 3D geometric space in a meaningful way. Specifically, we enhance entity position prediction within point clouds through an interactive text-point cloud process. We do this by calculating feature similarity across modalities to accurately

estimate the spatial probability distribution for each mentioned entity:

$$
\begin{array} { r } { \mathbf { E } = \hat { \mathbf { E } } _ { 0 } \mathbf { W } _ { E } , \quad \mathbf { S } = \hat { \mathbf { S } } \mathbf { W } _ { S } , } \\ { A _ { i j } = \cfrac { \operatorname { S i m } ( \mathbf { E } _ { i } , \mathbf { S } _ { j } ) } { \sum _ { j = 1 } ^ { N _ { s } } \operatorname { S i m } ( \mathbf { E } _ { i } , \mathbf { S } _ { j } ) } , } \end{array}
$$

where $\hat { \mathbf { E } } _ { 0 }$ denotes the initial word embeddings, $\hat { \bf S }$ denotes the superpoint features, $\mathbf { W } _ { E } , \mathbf { W } _ { S } \in \mathbb { R } ^ { D \times D }$ are learnable parameters, $A _ { i j } \in \mathbb { R }$ denotes the probability of the $i$ -th word token being located at the $j$ -th superpoint, and $\mathrm { S i m } \bar { ( \cdot , \cdot ) }$ represents the similarity function, which in this case is defined as $\mathrm { S i m } \left( \mathbf { E } , \mathbf { S } \right) { = } \exp ( \mathbf { E } \mathbf { S } ^ { T } / \sqrt { D } )$ .

Following this, we utilize the spatial probability distribution $A$ to predict the approximate positions of the mentioned entities, as well as their corresponding representations:

$$
\mathbf { P } _ { 0 , i } ^ { t } = \sum _ { j = 1 } ^ { \mathcal { N } _ { s } } A _ { i j } \mathbf { P } _ { j } ^ { s } ,
$$

$$
\mathbf { S } _ { v } = \hat { \mathbf { S } } \mathbf { W } _ { v } , \quad \hat { \mathbf { E } } _ { 0 , i } = \sum _ { j = 1 } ^ { \mathcal { N } _ { s } } A _ { i j } \mathbf { S } _ { v , j } ,
$$

where $\mathbf { P } _ { j } ^ { s }$ is the position of the $j$ -th superpoint, $\mathbf { P } _ { 0 , i } ^ { t }$ is the initial spatial position of $i$ -th word token which will be refined iteratively as formulated in Sec. 3.2.2, $\mathbf { W } _ { v } \in \mathbb { R } ^ { D \times D }$ denotes learnable parameters, and $\hat { \mathbf { E } } _ { 0 , i }$ denotes the updated representation of the $i$ -th word token. The sharing of distribution $A$ during centroid computation allows the entity representations to benefit from the guidance provided by spatial information, leading to a more accurate understanding of the 3D spatial relationships. Subsequently, the text and point clouds undergo multiple rounds of multimodal interactions, continually updating the embeddings and positions of the entities.

Iterative Position Refinement. After $l$ -round multimodal interactions, the word tokens $\hat { \mathbf { E } } _ { l }$ , referred to as textual segment kernels, become increasingly precise, theoretically resulting in more accurate position predictions. A straightforward approach would involve replicating the initial interaction method by regressing position information in each round. However, following the methodologies of Redmon et al. [54] and Lai et al. [31], rather than directly optimizing the final position, we adopt a more manageable strategy of iteratively learning offsets. To this end, we refine the positions of textual tokens based on the evolving textual segment kernels. As depicted in Fig. 2, we employ a Multilayer Perceptron (MLP) to predict a position offset $\Delta \mathbf { P } _ { l } ^ { t } = \mathbf { M } \mathbf { L } \mathbf { P } ( \hat { \mathbf { E } } _ { l + 1 } ) \in \mathbb { R } ^ { \mathcal { N } t \times 3 }$ from the updated textual segment kernels $\hat { \mathbf { E } } _ { l + 1 }$ . This offset is then added to the previous textual positions $\mathbf { P } _ { l } ^ { t }$ :

$$
\mathbf { P } _ { l + 1 } ^ { t } = \mathbf { P } _ { l } ^ { t } + \Delta \mathbf { P } _ { l } ^ { t } .
$$

This method allows for gradual refinement of position predictions, making the optimization process more effective and leading to progressively more accurate positioning with each iteration.

# 3.2.2 Spatial Awareness Aggregation

Once the positions of noun entities are obtained, techniques like positional encoding [61, 65, 31, 30, 6] can be used to further refine the positions.

Absolute Positional Encoding (APE). To initiate, we follow the approach of the original transformer [61] to encoded the positions of both superpoints and text tokens to obtain positional encodings $\mathbf { B } _ { l } ^ { s } \in \mathbb { R } ^ { \mathcal { N } _ { s } \times D }$ and $\mathbf { B } _ { l } ^ { t } \in \mathbb { R } ^ { \mathbf { \tilde { \mathcal { N } } } _ { t } \times D }$ using absolute positional encoding (APE):

$$
\mathbf { B } _ { l } ^ { s } = \mathbf { A } \mathbf { P } \mathbf { E } ( \mathbf { P } _ { l } ^ { s } ) , \quad \mathbf { B } _ { l } ^ { t } = \mathbf { A } \mathbf { P } \mathbf { E } ( \mathbf { P } _ { l } ^ { t } ) .
$$

These positional encodings facilitate spatial-aware self-attention in the textural segment kernels $\hat { \mathbf { E } } _ { l }$ :

$$
\dot { \bf E } _ { l } = { \bf A t t e n t i o n } ( \hat { \bf E } _ { l } + { \bf B } _ { l } ^ { t } , \hat { \bf E } _ { l } + { \bf B } _ { l } ^ { t } , \hat { \bf E } _ { l } ) ,
$$

where Attention $( \cdot )$ uses the technique of Vaswani et al. [61] and $\mathbf { B } _ { l } ^ { t }$ denotes the absolute positional encoding of $\hat { \mathbf { E } } _ { l }$ .

Next, we enhance textual and superpoint features with absolute positional encoding, and use them as Queries and Keys for subsequent multimodal aggregation:

$$
\begin{array} { r } { \begin{array} { l l } { \mathbf { Q } = \mathrm { C o n c a t } ( \dot { \mathbf { E } } _ { l } , \mathbf { B } _ { l } ^ { t } ) \mathbf { W } _ { q u e r y } , } \\ { \mathbf { K } = \mathrm { C o n c a t } ( \hat { \mathbf { S } } , \mathbf { B } _ { l } ^ { s } ) \mathbf { W } _ { k e y } , } \end{array} } \end{array}
$$

where $\mathbf { B } _ { l } ^ { t } \in \mathbb { R } ^ { N _ { t } \times D } , \mathbf { B } _ { l } ^ { s } \in \mathbb { R } ^ { N _ { s } \times D }$ denote the absolute positional encoding of segmentation kernels and superpoints, respectively, and $\mathbf { W } _ { q u e r y }$ , $\mathbf { W } _ { k e y } \in \mathbb { R } ^ { 2 D \times 2 D }$ denote learnable parameters.

Relative positional encoding (RPE). For the further interaction with superpoint features, we adopt well-established relative positional encoding techniques [65, 31, 30, 6], such as Table-based RPE [65, 31] and 5D Euclidean RPE [6], which are formalized as follows:

$$
\mathbf { B } _ { l } ^ { r } [ i , j ] = \mathrm { R P E } ( \mathbf { Q } [ i ] + \mathbf { K } [ j ] ) ,
$$

where $\mathbf { B } _ { l } ^ { r } [ i , j ] \in \mathbb { R }$ denotes the relative positional bias of the $i$ -th $\mathbf { Q }$ relative to the $j$ -th $\mathbf { K }$ , RPE(·) denotes the operation of relative positional bias and $[ \cdot ]$ denotes the indexing operation.

Thus, we can perform multimodal aggregation enhanced with relative positional encoding:

$$
\hat { \mathbf { E } } _ { l + 1 } = \mathrm { s o f t m a x } \left( \frac { \mathbf { Q } \cdot \mathbf { K } ^ { T } } { \sqrt { D } } + \mathbf { B } _ { l } ^ { r } \right) \cdot ( \hat { \mathbf { S } } \mathbf { W } _ { v a l } ) ,
$$

where $\mathbf { W } _ { v a l } \in \mathbb { R } ^ { D \times D }$ denote learnable parameters, $\mathbf { B } _ { l } ^ { r } \in \mathbb { R } ^ { N _ { t } \times N _ { s } }$ denotes the relative positional bias, and $\hat { \mathbf { E } } _ { l + 1 }$ denotes the updated segmentation kernels. This methodology significantly enriches the interaction between linguistic and 3D visual data, enabling more nuanced spatial understanding in our model.

# 3.3 Rule-guided Weak Supervision

# 3.3.1 Rule-guided Target Selection

In the preceding sections, we initially predicted the locations of all entities mentioned in the text. Ideally, supervised training would require position labels for each entity. However, we only have access to the location information of the target instance. This constraint leads us to adopt a weak supervision approach, focusing solely on the position of the referring instance for training. This approach introduces a significant challenge: accurately identifying the referring instance among the mentioned nouns. To address this, we utilize a pre-processed dependency tree, as outlined in Manning et al. [46], to accurately pinpoint the core noun, typically the subject of the sentence. We have developed a set of manual rules, based on this more general dependency tree, to enhance the identification process. These rules are specifically designed to guide the accurate positioning of core instances. The implementation of these rules is outlined in Algorithm 1.

# Algorithm 1 Rule-guided Target Selection

Input: The dependency tree $\mathcal { G } = ( \nu , \mathcal { E } )$ of the textual description, where $\mathcal { V } = \{ \mathrm { t o k e n } \}$ denotes the set of nodes, $\begin{array} { r } { \mathcal { E } = \left\{ \begin{array} { r l } \end{array} \right. } \end{array}$ (relation, head, tail) $\}$ denotes the set of relations between nodes.   
Output: The index $i$ of Target Instance node $\mathcal { V } ^ { t g t }$ 1: Initialization $i$ to the root: $i = 0$ 2: find $\mathcal { E } _ { i }$ with $\nu _ { i }$ as its head   
3: if ( $\mathcal { E } _ { i } \in \{$ {nsubj, compound}) & (Vi ∈/ {which, that}) then   
4: $i  \mathcal { E } _ { i }$ ’s tail index 5: end if 6: if $\mathcal { V } _ { i } \in \{$ there, this, it, object then 7: find $\mathcal { E } _ { i }$ with $\nu _ { i }$ as its head 8: $i  \mathcal { E } _ { i }$ ’s tail index   
9: end if   
10: if $\begin{array} { r } { \mathcal { V } _ { i } \in \{ \begin{array} { r l r l } \end{array} } \end{array}$ {set, sets, color, shape} then   
11: find the first $\mathcal { E } _ { i }$ ’s relation $\in$ {compound, nmod, dep}   
12: $i  \mathcal { E } _ { i }$ ’s head index   
13: end if

# 3.3.2 Training Objectives

Given the index of the target instance, we can directly obtain the corresponding segment kernel $\hat { \mathbf { E } } _ { l + 1 } ^ { t g t } \in \mathbb { R } ^ { D }$ and position $\mathbf { P } _ { l + 1 } ^ { t g t }$ , which are then supervised by the target ground truth.

Then we perform matrix multiplication between $\hat { \mathbf { E } } _ { l + 1 } ^ { t g t }$ and $\hat { \bf S }$ to get the predicted instance response maps, which can be formulated as

$$
\begin{array} { r } { \mathbf { M } _ { l + 1 } = \sigma ( \hat { \mathbf { E } } _ { l + 1 } ^ { t g t } \cdot \hat { \mathbf { S } } ^ { T } ) , } \\ { \mathbf { M } \mathbf { a } \mathbf { s } \mathbf { k } _ { l + 1 } = \mathbf { M } _ { l + 1 } > 0 . 5 , } \end{array}
$$

where $\mathbf { M } _ { l + 1 } \in \mathbb { R } ^ { N _ { s } }$ , $\mathbf { M a s k } _ { l + 1 } \in \{ 0 , 1 \} ^ { \mathcal { N } _ { s } }$ are the predicted response map and the instance mask corresponding to the target.

Given ground-truth binary mask of the referring expression $\mathbf { Y } \in \{ 0 , 1 \} ^ { \mathcal { N } _ { p } }$ , we get the corresponding superpoint mask $\mathbf { Y } ^ { s } \in \{ \bar { 0 } , 1 \} ^ { \mathcal { N } _ { s } }$ by superpoint pooling follewed by a 0.5-threshold binarization, and then we apply the binary cross-entropy (BCE) loss on the final response map $\mathbf { M } _ { l + 1 }$ following Sun et al. [58]. The operation can be written as:

$$
\begin{array} { r l } & { \mathbf { Y } _ { i } ^ { s } = \mathbb { I } ( \sigma ( \operatorname { A v g P o o l } ( \mathbf { Y } , \boldsymbol { \mathcal { K } } _ { i } ) ) ) , } \\ & { \mathcal { L } _ { b c e } = \operatorname { B C E } ( \mathbf { M } _ { l + 1 } , \mathbf { Y } ^ { s } ) , } \end{array}
$$

where $\mathrm { \mathbf { A v g P o o l } } ( \cdot )$ denotes the superpoint average pooling operation, and $\mathbf { Y } _ { i } ^ { s }$ denotes the binarized mask value of the $i$ -th superpoint $\textstyle { \mathcal { K } } _ { i }$ . $\mathbb { I } ( \cdot )$ indicates whether the mask value is higher than $50 \%$ .

To tackle foreground-background sample imbalance, we can use Dice loss [47]:

$$
\mathcal { L } _ { d i c e } = \mathrm { D I C E } ( \mathbf { M } _ { l + 1 } , \mathbf { Y } ^ { s } ) .
$$

To supervise the position $\mathbf { P } _ { l + 1 } ^ { t g t }$ , we use the center of the superpoints of the target instance $\mathbf { P } ^ { g t }$

$$
\mathcal { L } _ { p o s } = \mathrm { L } 1 ( \mathbf { P } _ { l + 1 } ^ { t g t } , \mathbf { P } ^ { g t } ) .
$$

In addition, we add a simple auxiliary score loss $\mathcal { L } _ { s c o r e }$ for mask quality prediction following Sun et al. [58].

Overall, the final training loss function $\mathcal { L }$ can be formulated as:

$$
\begin{array} { r } { \mathcal { L } = \lambda _ { b c e } \mathcal { L } _ { b c e } + \lambda _ { d i c e } \mathcal { L } _ { d i c e } + \lambda _ { p o s } \mathcal { L } _ { p o s } + } \\ { \lambda _ { s c o r e } \mathcal { L } _ { s c o r e } , \qquad } \end{array}
$$

where $\lambda _ { b c e } , \lambda _ { d i c e } , \lambda _ { r e l }$ and $\lambda _ { s c o r e }$ are hyperparameters used to balance these four losses.

# 4 Expriment

# 4.1 Experiment Settings

In our experiment, we utilize the pre-trained Sparse 3D U-Net method to extract point-wise features from point clouds [58]. We also employ the pre-trained MPNet model [56] as our text encoder. For the rest of the network, training is conducted from scratch. We set an initial learning rate of 0.0001 and apply a learning rate decay at epochs 26, 34, and 46, each with a decay rate of 0.5. Our experiments use a default of 6 multiple rounds $L$ , a batch size of 32, and a maximum sentence length of 80. We set $\lambda _ { b c e } = \lambda _ { d i c e } = 1 , \lambda _ { p o s } = \lambda _ { s c o r e } = 0 . 5$ . All experiments are conducted using PyTorch on a single NVIDIA Tesla A100 GPU, ensuring consistency in our computational process.

# 4.2 Dataset and Evaluation Metrics

We evaluate our method using the ScanRefer dataset, a recent 3D referring dataset [5, 24], comprising 51,583 English natural language expressions referring to 11,046 objects across 800 ScanNet scenes [7]. Following Chen et al. [5], our evaluation metrics include mean Intersection over Union (mIoU) and $\operatorname { A c c } @ k \operatorname { I o U }$ . “Unique” refers to cases where the target instance is the only one of its class, and “Multiple” indicates situations where there is at least one more object of the target’s class.

Table 1: The 3D-RES results on ScanRefer. $\dagger$ The mIoU and accuracy are reevaluated on our machine. ∗We reproduce results by extracting points within the boxes as segmentation mask predictions using their official codes.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3">Unique (~19%)</td><td colspan="3">Multiple (~81%)</td><td colspan="3">Overall</td><td colspan="3">Inference Time</td></tr><tr><td>0.25</td><td>0.5</td><td>mIoU</td><td>0.25</td><td>0.5</td><td>mIoU</td><td>0.25</td><td>0.5</td><td>mIoU</td><td>Stage-1</td><td>Stage-2</td><td>All</td></tr><tr><td colspan="10">Multi-task</td><td></td><td></td><td></td></tr><tr><td>EDA-box2mask [68]</td><td>84.7</td><td>56.9</td><td></td><td>50.0</td><td>37.0</td><td>-</td><td>55.2</td><td>40.0</td><td>35.0</td><td>-</td><td></td><td></td></tr><tr><td>3DRefTR-SP[41]</td><td>87.9</td><td>69.8</td><td></td><td>51.6</td><td>41.9</td><td></td><td>57.0</td><td>46.1</td><td>40.8</td><td></td><td></td><td>388ms</td></tr><tr><td>3DRefTR-HR [41]</td><td>89.6</td><td>77.0</td><td></td><td>52.3</td><td>43.7</td><td></td><td>57.9</td><td>48.7</td><td>41.2</td><td></td><td></td><td>405ms</td></tr><tr><td>UniSeg3D [69]</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>29.6</td><td></td><td></td><td></td></tr><tr><td>SegPoint [20]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td>41.7</td><td>1</td><td></td><td></td></tr><tr><td>Reason3D[23]</td><td>88.4</td><td>84.2</td><td>74.6</td><td>50.5</td><td>31.7</td><td>34.1</td><td>57.9</td><td>41.9</td><td>42.0</td><td>1</td><td></td><td></td></tr><tr><td colspan="10">Single-task</td><td colspan="3"></td></tr><tr><td>TGNN[24]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>37.5</td><td>31.4</td><td>27.8</td><td></td><td></td><td></td></tr><tr><td>TGNN† [24]</td><td>69.3</td><td>57.8</td><td>50.7</td><td>31.2</td><td>26.6</td><td>23.6</td><td>38.6</td><td>32.7</td><td>28.8</td><td>26862ms</td><td>235ms</td><td>27097ms</td></tr><tr><td>InstanceRefer† [71]</td><td>81.6</td><td>72.2</td><td>60.4</td><td>29.4</td><td>23.5</td><td>21.5</td><td>40.2</td><td>33.5</td><td>30.6</td><td>509ms</td><td>672ms</td><td>1181ms</td></tr><tr><td>X-RefSeg3D[52]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>40.3</td><td>33.8</td><td>29.9</td><td></td><td></td><td></td></tr><tr><td>3DVG-Transformer*[73]</td><td>79.5</td><td>58.0</td><td>49.9</td><td>42.0</td><td>30.8</td><td>27.0</td><td>49.3</td><td>36.1</td><td>31.4</td><td></td><td></td><td></td></tr><tr><td>3D-SPS*[45]</td><td>84.8</td><td>65.6</td><td>54.7</td><td>41.7</td><td>30.8</td><td>26.7</td><td>50.1</td><td>37.6</td><td>32.1</td><td></td><td></td><td></td></tr><tr><td>3DRESTR[41]</td><td>79.0</td><td>54.2</td><td></td><td>40.2</td><td>22.1</td><td></td><td>46.0</td><td>26.9</td><td>28.7</td><td></td><td></td><td></td></tr><tr><td>3D-STMN [63]</td><td>89.3</td><td>84.0</td><td>74.5</td><td>46.2</td><td>29.2</td><td>31.1</td><td>54.6</td><td>39.8</td><td>39.5</td><td></td><td></td><td>283ms</td></tr><tr><td>RG-SAN (Ours)</td><td>89.2</td><td>84.3</td><td>74.5</td><td>55.0</td><td>35.4</td><td>37.4</td><td>61.7</td><td>44.9</td><td>44.6</td><td></td><td></td><td>295ms</td></tr></table></body></html>

# 4.3 Quantitative Comparison

In our experiments on the ScanRefer dataset, our proposed RG-SAN demonstrates significant improvements in nearly all metrics on the single-task leaderboard, as shown in Tab. 1. Notably, RG-SAN shows substantial gains compared to the state-of-the-art single-task model 3D-STMN, with increases of 5.1 points in mIoU and 7.1 points in $\operatorname { A c c } @ 0 . 2 5$ . This highlights our model’s inferencing capability. A more detailed examination reveals that the majority of these improvements occur in scenarios with multiple disruptive instances, where RG-SAN achieves a remarkable 6.3-point increase in mIoU. This setting, where the target instance is among other instances of the same type, demands discriminative reasoning from the model. The significant performance validates the enhanced referring capabilities empowered by spatial reasoning. Our proposed RG-SAN also outperforms multi-task models [68, 41], including LLM-based models [20, 23], in most 3D-RES metrics, despite those models benefiting from more annotated data.

Moreover, RG-SAN has competitive inference costs, being only $1 2 \mathrm { m s }$ slower than the efficient 3D-STMN and faster than all other compared models, demonstrating its high performance with minimal computational increase.

# 4.4 Ablation Study

# 4.4.1 Text-driven Localization Module

We conduct an ablation study on the Text-driven Localization Module (TLM), as illustrated in Tab. 2. Simultaneously, we perform a fine-grained analysis of various initialization schemes for embeddings and positions. The term "w/o TLM" denotes the approach of not modeling positional information and instead directly using text embeddings for interaction. "MAFT" refers to the direct adaptation of the method proposed in [31]. The "Project" method involves initializing embeddings based on text-driven embeddings and then projecting each textual token directly into a 3D position, while the "Random" method randomly assigns a position to each textual token. Finally, we utilize the initialization technique called Text-driven Initialization (TI), which simultaneously initializes both embeddings and positions in a text-driven manner. Tab. 2 clearly shows that, under identical conditions, TI outperforms the others in all metrics. This indicates that TI more effectively leverages positional information from the visual scene, leading to more precise initial positions for the textual tokens. Consequently, this reduces the complexity of the subsequent iterative refinement process, thereby enhancing the overall accuracy of our model in spatially aligning text with point cloud data. Additionally, Tab. 2 demonstrates that proper initialization leads to the superior performance of TLM compared to the methods without TLM.

Table 2: Ablation study of Text-driven Localization Module, where “w/o TLM” means not using TLM.   

<html><body><table><tr><td>Method</td><td>Init. of Embeddings</td><td>Init. of Positions</td><td>Multiple mIoU</td><td>Overall mIoU</td></tr><tr><td>w/o TLM MAFT [31]</td><td>Text-driven</td><td></td><td>32.5</td><td>40.3</td></tr><tr><td></td><td>Zero</td><td>Random</td><td>29.7</td><td>37.9</td></tr><tr><td>Project</td><td>Text-driven</td><td>Project</td><td>30.3</td><td>38.8</td></tr><tr><td>Random</td><td>Text-driven</td><td>Random</td><td>30.1</td><td>38.8</td></tr><tr><td>TI</td><td>Text-driven</td><td>Text-driven</td><td>34.1</td><td>42.3</td></tr></table></body></html>

Table 3: Ablation study of positional encoding, where “w/o Pos. Supervision” means not supervising the positions, and “w/o PE” means not using any positional encoding.   

<html><body><table><tr><td rowspan="2">positional encoding</td><td colspan="3">Mu.tiple mIoU</td><td colspan="3">Overal mIoU</td></tr><tr><td>0.25</td><td></td><td></td><td>0.25</td><td></td><td></td></tr><tr><td>w/o Pos. Supervision</td><td>45.4</td><td>27.3</td><td>30.4</td><td>54.4</td><td>38.2</td><td>38.9</td></tr><tr><td>W/o PE</td><td>46.1</td><td>31.7</td><td>32.8</td><td>54.6</td><td>42.4</td><td>41.1</td></tr><tr><td>Fourier APE</td><td>46.0</td><td>30.9</td><td>32.0</td><td>55.1</td><td>41.5</td><td>40.7</td></tr><tr><td>5D Euclidean RPE</td><td>46.7</td><td>32.5</td><td>33.3</td><td>54.6</td><td>43.9</td><td>41.7</td></tr><tr><td>Table-based RPE</td><td>47.2</td><td>33.7</td><td>34.1</td><td>55.6</td><td>43.9</td><td>42.3</td></tr></table></body></html>

# 4.4.2 Positional Encoding

We compare various positional encoding methods previously employed in [55, 6, 31]. These methods include Fourier Absolute positional encoding (APE), 5D Euclidean Relative positional encoding (5D Euclidean RPE) [6], and Table-based Relative positional encoding (Table-based RPE) [31]. Tab. 3 reveals that Table-based RPE surpasses the other methods, suggesting that combining semantic information with relative relationships is advantageous. Additionally, we observe that employing only absolute positional encoding can result in lower performance than not using any positional encoding at all. This may be attributed to the inherent limitations of absolute positional encoding in capturing relative positional information. By complicating the semantic features, it introduces challenges in the model’s training process, underscoring the importance of choosing the right positional encoding technique for effective performance.

# 4.4.3 Rule-guided Weak Supervision

We conducted experiments employing various weakly supervised text kernel selection strategies to evaluate their efficacy in leveraging target annotations. The strategy labeled as "w/o RWS" involves selecting the token based on attention weight within the cross-attention module [63], while "Root" entails selecting the root token of the dependency tree. Table 4 illustrates that utilizing the root node as supervision slightly outperforms the "w/o RWS" baseline. This is likely due to the root node providing consistent supervision, whereas Top1 tends to select different nodes variably, which complicates the training process. In contrast, our Rule-guided Target Selection (RTS) strategy, based on dependency tree rules to locate subjects, aligns more effectively with the structural nature of the text. It precisely identifies the target entity’s position, significantly enhancing annotation utilization and effectively directing model training. This leads to a notable improvement in model performance.

Furthermore, we conduct an ablation study on the impact of the position loss weight $\mathcal { L } _ { p o s }$ , detailed in Tab. 5. We observe that increasing the weight generally improves performance, peaking at a weight of 0.5, beyond which performance begins to taper off. This finding highlights the importance of balancing the weight of the position loss to optimize the model’s effectiveness.

# 4.4.4 Comparison with MAFT

MAFT [31] has played a pivotal role in 3D instance segmentation by incorporating spatial position modeling, offering valuable insights into how spatial information can improve model performance. Inspired by this approach, we extend spatial information into the text space to better align visual and textual semantics, specifically targeting spatial relationship reasoning in 3D-RES. Our approach introduces two key innovations that distinguish it from MAFT:

Unlike MAFT [31], which initializes queries with zeros and uses random initialization for positional information, we employ text-driven queries and positional information to model the spatial relationships of entities in the expressions. This allows our model to capture the spatial context better, resulting in a 4.4-point improvement in mIoU, as shown in Tab. 2 In contrast to [31], which supervises the positions of all target instances, 3D-RES supervises only the core target word. Our novel RWS method constructs spatial relationships for all noun instances using only the target word’s positional information, resulting in a 2.3-point improvement in mIoU, as demonstrated in Tab. 4.

Table 4: Weak Supervision Strategy in RWS, where “w/o RWS” means using attention-based Top1 approach in [63] instead of our RWS, and “RTS” refers to our Rule-guided Target Selection strategy.   

<html><body><table><tr><td rowspan="2">Strategy</td><td colspan="3">Multiple</td><td colspan="3">Overall</td></tr><tr><td>0.25</td><td>0.5</td><td>mIoU</td><td>0.25</td><td>0.5</td><td>mIoU</td></tr><tr><td>w/o RWS</td><td>47.2</td><td>33.7</td><td>34.1</td><td>55.6</td><td>43.9</td><td>42.3</td></tr><tr><td>Root</td><td>53.5</td><td>30.4</td><td>34.7</td><td>60.7</td><td>40.9</td><td>42.5</td></tr><tr><td>RTS</td><td>55.0</td><td>35.4</td><td>37.4</td><td>61.7</td><td>44.9</td><td>44.6</td></tr></table></body></html>

Table 5: Ablation study of the weight of pos.   

<html><body><table><tr><td rowspan="2">Weight of Lpos</td><td colspan="3">Multiple</td><td colspan="3">Overall</td></tr><tr><td>0.25</td><td>0.5</td><td>mIoU</td><td>0.25</td><td>0.5</td><td>mIoU</td></tr><tr><td>0.1</td><td>54.7</td><td>34.9</td><td>36.9</td><td>61.3</td><td>44.3</td><td>44.0</td></tr><tr><td>0.2</td><td>55.5</td><td>34.0</td><td>37.0</td><td>62.0</td><td>43.7</td><td>44.2</td></tr><tr><td>0.5</td><td>55.0</td><td>35.4</td><td>37.4</td><td>61.7</td><td>44.9</td><td>44.6</td></tr><tr><td>1.0</td><td>55.3</td><td>34.5</td><td>37.0</td><td>61.8</td><td>44.2</td><td>44.3</td></tr><tr><td>2.0</td><td>54.3</td><td>33.8</td><td>36.6</td><td>61.0</td><td>43.7</td><td>43.9</td></tr></table></body></html>

![](images/8d4eaca71358fd7e919ff97b5f8b4a953334d7faf438b2f100ee37c170e3e39b.jpg)  
Figure 3: Visualization of all the nouns in the textual description. Our RG-SAN can segment instances corresponding to different nouns, while 3D-STMN indiscriminately assigns all nouns to the target instance. Zoom in for best view.

# 4.5 Qualitative Comparison

We conduct a qualitative analysis on the ScanRefer validation set as shown in Fig. 3, comparing our proposed RG-SAN with 3D-STMN [63] to highlight our model’s exceptional referring capability. Fig. 3 demonstrates our model’s ability to accurately segment not only the target objects but also other nouns mentioned in the text. Unlike 3D-STMN, which misattributes all nouns to a single target, RG-SAN distinctly recognizes and locates each noun. For example, in Fig. 3-(c), our model successfully identifies the target chair through relative positioning, even with similar objects in the scene, and accurately recognizes a coat as a supporting element in the description. This ability extends to Fig. 3-(a) and (b), where RG-SAN correctly segments multiple auxiliary nouns into their corresponding instances, demonstrating its robust generalization for complex texts and precise localization for multiple entities. Such capabilities enhance the model’s understanding of complex semantic scenes, significantly improving its ability to refer to specific entities accurately.

# 5 Conclusion

In this paper, we present RG-SAN to overcome the limitations of traditional 3D-RES methods, particularly their lack of spatial awareness. Specifically, the TLM is introduced to model and refine positional information, while the RWS is designed to employ dependency tree rules to accurately guide the position of the target object. Combining TLM with RWS strategy, RG-SAN significantly improves segmentation accuracy and robustly handles spatial ambiguities. Extensive experiments conducted on the ScanRefer benchmark demonstrate the superior performance of RG-SAN. This underscores the importance of incorporating spatial awareness into segmentation models, paving the way for future advancements in the domain.

# 6 Acknowledge

This work was supported by National Science and Technology Major Project (No. 2022ZD0118201), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U22B2051, No. U21B2037, No. 62072389, No. 62302411, No. 623B2088), the Natural Science Foundation of Fujian Province of China (No.2021J06003) and China Postdoctoral Science Foundation (No. 2023M732948).