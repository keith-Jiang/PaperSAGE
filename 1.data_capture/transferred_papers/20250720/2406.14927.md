# GIC: Gaussian-Informed Continuum for Physical Property Identification and Simulation

Junhao Cai1∗ Yuji Yang2∗ Weihao Yuan3† Yisheng He3   
Zilong Dong3 Liefeng $\mathbf { \bar { B 0 } } ^ { 3 }$ Hui Cheng2 Qifeng Chen1

The Hong Kong University of Science and Technology, 2Sun Yat-sen University, 3Alibaba Group ∗ Equal contribution, order determined by coin toss. † Corresponding author.

# Abstract

This paper studies the problem of estimating physical properties (system identification) through visual observations. To facilitate geometry-aware guidance in physical property estimation, we introduce a novel hybrid framework that leverages 3D Gaussian representation to not only capture explicit shapes but also enable the simulated continuum to render object masks as 2D shape surrogates during training. We propose a new dynamic 3D Gaussian framework based on motion factorization to recover the object as 3D Gaussian point sets across different time states. Furthermore, we develop a coarse-to-fine filling strategy to generate the density fields of the object from the Gaussian reconstruction, allowing for the extraction of object continuums along with their surfaces and the integration of Gaussian attributes into these continuums. In addition to the extracted object surfaces, the Gaussian-informed continuum also enables the rendering of object masks during simulations, serving as 2D-shape guidance for physical property estimation. Extensive experimental evaluations demonstrate that our pipeline achieves state-of-the-art performance across multiple benchmarks and metrics. Additionally, we illustrate the effectiveness of the proposed method through realworld demonstrations, showcasing its practical utility. Our project page is at https://jukgei.github.io/project/gic.

# 1 Introduction

Identifying the physical properties of objects (i.e., system identification) is essential for numerous applications such as games, digital twins, and robotic manipulation [1–3]. Although humans can intuitively deduce the underlying physical properties with a single glance when the object undergoes deformation, estimating the properties with only visual observations remains challenging for computational perceptual algorithms.

To tackle this challenge, many established methods [4–6] adopt the assumption of elastic material [7] and perform physics-based modeling based on mass-spring systems (MSS) or finite element method (FEM) to model and simulate the dynamics of the objects. Such an assumption inevitably restricts the ability to simulate more general types beyond elastic materials, such as fluids or granular media. Another problem of previous methods lies in that many methods [8–10] require the ground-truth full knowledge of object geometry for the identification, which limits their practicality. Some subsequent methods [5, 4] turn to recover the geometries and physical properties from observations in a decoupled manner. Specifically, these methods first extract object geometries by making use of stereo observations or dynamic neural reconstruction [11] from RGB video sequences, and then perform simulation directly on the point clouds or after the tetrahedral mesh conversion. While these methods introduce explicit geometries to guide the estimation of physical properties, the noisy reconstruction results usually lead to degraded system identification performance.

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

Recently, PAC-NeRF [12] integrates neural radiance fields (NeRF) [13] with a continuum dynamic model to tackle the above problems. The object geometries and physical properties are captured in a unified framework. Despite its effectiveness, this method possesses two limitations. Firstly, the implicit shapes represented by NeRF often lead to inferior geometries, which might cause inaccurate trajectories during simulation. Secondly, PAC-NeRF renders the novel views of deformed objects based on the appearance radiance field reconstructed from the static scene, which might introduce texture distortion, particularly when objects undergo significant deformations, resulting in discrepancies between the rendered and the observed images [14].

To address these limitations, this paper proposes a novel hybrid solution based on 3D Gaussians [15, 16] and material point method (MPM) [17, 18]. The core strength of this work is that we make use of both 3D shapes from dynamic 3D Gaussian reconstruction and 2D shapes rendered by the Gaussian-informed continuum for physical property estimation.

To generate more precise shapes to reason physical property, we first propose a motion-factorized dynamic 3D Gaussian network to conduct dynamic scene reconstruction. We then extract the continuum from the recovered 3D Gaussians at each frame by leveraging a coarse-to-fine filling strategy to generate the density field of the object progressively. The resulting density fields can be used to sample continuum particles for simulation and extract object surfaces as 3D-shape supervision in physical property estimation. To eliminate the appearance distortion caused by large deformation in PAC-NeRF, we further assign Gaussian attributes to the continuum particles where the opacity and scale attributes are evaluated from the density field. Such Gaussian-informed continuum are able to render object masks during simulation, which can be regarded as a 2D-shape representation to guide the estimation and effectively avoid using inferior rendering results for learning physical properties.

To demonstrate the superiority of the proposed method over other baselines, we conduct three types of experiments, including evaluations of physical properties, dynamic reconstruction, and future state simulation. We also demonstrate a real-world application in digital twins and robotic manipulation, showing the applicability of the proposed method in real-world scenarios.

Our contributions are summarized as follows.

• We propose a novel hybrid pipeline that takes advantage of the 3D Gaussian representation of the object to both acquire 3D shapes and empower the simulated continuum to render 2D shapes for physical property estimation.   
• We propose a novel dynamic 3D Gaussian framework with motion factorization to achieve more precise dynamic reconstruction. We also propose a coarse-to-fine filling strategy to generate the density field of the object, which can be utilized to extract object surfaces and obtain Gaussianinformed continuum particles.   
• Extensive experiments show that our pipeline attains state-of-the-art performance on existing benchmarks with a wide range of metrics. We also present a real-world demonstration to show the efficiency of the proposed method.

# 2 Related Work

Dynamic reconstruction. Reconstructing dynamic scenes from monocular or multi-view video(s) is a long-standing problem in the computer vision community [19, 20]. Previous works exploit neural implicit representation [21, 22] for non-rigid reconstruction. These methods either reconstruct the scene in a frame-wise manner [23, 24] or maintain a canonical shape and model the deformation with a neural network [25, 26, 11, 27]. While effective for novel view synthesis, these methods often require extensive training time and can result in noisy deformations owing to the implicit representation, which may compromise the utility of the recovered geometries for physical property estimation [12]. Recent progress in 3D Gaussian Splatting (3DGS) technique [15] stands out to be a prevalent method for 3D reconstruction and novel view synthesis because of the abilities of explicit shape modeling and extremely fast view rendering. Similar to non-rigid NeRF, many follow-up works extend the 3DGS into 4D by treating each frame separately [28] or decomposing a scene into a canonical 3D Gaussian point cloud and a deformation model that warps the canonical shape into a specific scene [16, 29, 30]. In this paper, we draw upon these prior studies [16, 29] and propose a novel motion-factorized dynamic 3D Gaussian network to achieve better performance on reconstruction and novel view synthesis.

System identification. Understanding the physics laws of the 3D world is beneficial for simulation [31–35, 6] and manipulation [2, 3, 36–38]. However, unveiling these properties from visual information is an extremely difficult task due to the ambiguity introduced by incomplete observation and the high degrees of freedom of the scene. Early works [39, 40] study the problem by learning physical properties via interactions. With recent improvements in differentiable physics simulation [17, 18, 41–45], many methods turn to evaluate the physical properties by comparing the rendering results with 2D ground truth given the prior knowledge about the object geometry. VEO [5] presents a differentiable simulator to learn patterns from 4D reconstruction and force-displacement measurements. Another approach [4] eliminates the dependence of captured forces by proposing an iteration framework between deformation tracking and parameter optimization. While these methods demonstrate promising results, the inferior reconstruction might lead to degraded performance, and the assumption of elastic material restricts the applicability. PAC-NeRF [12] instead proposes a single framework to recover both the unknown geometry and physical properties of deformable objects from multi-view video sequences. However, the inferior geometries and blurry rendered images might have detrimental effects on physical property reasoning. In this work, we adopt MPM as our simulation framework following the approach used in PAC-NeRF due to its ability to simulate a variety of materials [6, 46–48]. Unlike previous approaches, we utilize dynamic 3D Gaussians to reconstruct explicit 3D geometries and generate simulatable continuum particles. Furthermore, we enhance the particles with Gaussian attributes, facilitating the rendering of 2D shapes, and thereby improving physical parameter estimation.

# 3 Preliminary

In this section, we briefly review the core idea of 3D Gaussian Splatting (3DGS) [15] and introduce its point-based alpha blending to render depth maps and foreground masks. Typically, 3DGS utilizes 3D Gaussians, each defined by a central point $\mu _ { 0 }$ , a covariance matrix $\Sigma _ { 0 }$ , a density value $\sigma$ , and a color attribute $c$ , to efficiently render images from specific viewpoints. Each point is denoted as

$$
G ( x ) = \exp ( - \frac { 1 } { 2 } ( x - \mu _ { 0 } ) ^ { T } \Sigma _ { 0 } ^ { - 1 } ( x - \mu _ { 0 } ) ) ,
$$

where $\Sigma _ { 0 }$ can be factorized as $\Sigma _ { 0 } = R _ { 0 } S _ { 0 } S _ { 0 } ^ { T } R _ { 0 } ^ { T }$ , in which $R _ { 0 }$ is a rotation matrix represented by a quaternion vector $r _ { 0 } \in \mathbb { R } ^ { 4 }$ , and $S _ { 0 }$ is a a diagonal scaling matrix characterized by a 3D vector $s _ { 0 } \doteq \mathbb { R } ^ { 3 }$ . If we consider isotropic Gaussian representation, the scaling matrix can be written as $s _ { 0 } I$ , where $s _ { 0 }$ is a scalar and $I$ is the identity matrix. When performing splatting, the 3D Gaussians are projected into 2D with the covariance matrix defined as $\dot { \Sigma _ { 0 } ^ { \prime } } = J W \dot { \Sigma _ { 0 } } \dot { W } ^ { T } J ^ { T }$ , where $J$ is the Jacobian of affine approximation of the projective transformation [49], and $W$ is the viewing transformation matrix. The rendered color $I ( u )$ with its foreground mask $A ( u )$ at pixel $u$ are then evaluated by integrating $N$ ordered slatted Gaussians via the point-based alpha blending. Since the depth of each Gaussian point at a specific view can be obtained according to its transformation matrix, we can further render the depth map $D$ using the same blending method [16, 50], as

$$
I ( u ) = \sum _ { i \in N } T _ { i } \alpha _ { i } c _ { i } , \qquad A ( u ) = \sum _ { i \in N } T _ { i } \alpha _ { i } , \qquad D ( u ) = \sum _ { i \in N } T _ { i } \alpha _ { i } d _ { i } ,
$$

where $\begin{array} { r } { T _ { i } = \prod _ { j = 1 } ^ { i - 1 } ( 1 - \alpha _ { j } ) } \end{array}$ is the accumulated transmittance, $\alpha _ { i }$ is the probability of termination at point $i$ , and $d _ { i }$ is the depth of the Gaussian point at the specific view.

# 4 Method

# 4.1 Problem Definition and Overview

In this work, we aim to reconstruct the geometries and the physical properties of various object types from multi-view videos. Formally, given a set of video sequences $\{ { \bar { V } } _ { i } | { \bar { i } } = 1 . . . n \}$ with moving object and the corresponding camera extrinsic and intrinsic parameters $\{ ( T _ { i } , K _ { i } ) | i = 1 . . . n \}$ , the goal of this task is to recover the explicit geometries of the object represented by continuum particles $P ( t )$ and its corresponding physical parameters $\Theta$ (e.g., Young’s modulus $E$ and Poisson’s ratio $\nu$ for elastic objects). We follow the assumption in PAC-NeRF and PhysGaussian [12, 51] that the object types (e.g., elastic, granular, Newtonian/non-Newtonian, plastic) are known and the physical phenomenon follows continuum mechanics [17, 52].

![](images/037038d944c6be18cbf7a186ef66311bcdc91d1eea56112c7a1f9c5cb9dc71cc.jpg)  
Figure 1: Overview. (a) Continuum Generation: Given a series of multi-view images capturing a moving object, the motion-factorized dynamic 3D Gaussian network is trained to reconstruct the dynamic object as 3D Gaussian point sets across different time states. From the reconstructed results, we employ a coarse-to-fine strategy to generate density fields to recover the continuums and extract object surfaces. The continuum is endowed with Gaussian attributes to allow mask rendering. (b) Identification: The MPM simulates the trajectory with the initial continuum $\mathbb { P } ( 0 )$ and the physical parameters $\Theta$ . The simulated object surfaces and the rendered masks are then compared against the previously extracted surfaces (colored in blue) and the corresponding masks from the dataset. The differences are quantified to guide the parameter estimation process. (c) Simulation: Digital twin demonstrations are displayed. Simulated objects (colored by stress increasing from blue to red), characterized by the properties estimated from observation, exhibit behavior consistent with real-world objects.

The overview of the proposed pipeline is illustrated in Fig. 1, which consists of three modules: a motion-factorized dynamic 3D Gaussian network (Sec. 4.2) for 4D reconstruction of the object, a coarse-to-fine density field generation strategy (Sec. 4.3) for continuum generation, surface extraction, and Gaussian attribute assignment, and a procedure (Sec. 4.4) showing how we leverage Gaussianinformed continuum and extracted surfaces to estimate physical properties.

# 4.2 Motion-factorized Dynamic 3D Gaussian Network

Our dynamic 3D Gaussian network follows existing frameworks [16, 29, 30] that simultaneously maintain a canonical 3D Gaussian set and a deformation field modeled by a neural network to warp the canonical shape into object states at specific times. The core idea of this pipeline, presented in Fig. 2, is that the motion of every point in the object can be decomposed into a small range of motion bases.

Architecture. We first factorize the entire motion into $N _ { m }$ bases that are modeled by a fully connected neural network, where every basis shares a common backbone except the final layer. The output of each basis consists of the deformations at position $d \mu _ { i } ( t ) \in \mathbb { R } ^ { 3 }$ and at scale $\dot { d } s _ { i } ( t ) \in \mathbb { R }$ . To model the exact deformation for each position, we next propose a lightweight coefficient network that maps the positions at canonical space with specific time to their corresponding motion coefficients $w ( \dot { \mu } _ { 0 } , t ) \dot { \in } \mathbb { R } ^ { N _ { m } }$ . Therefore, the deformed position and the scale for each Gaussian point are evaluated by the linear combination of the motion basis according to the motion coefficients:

$$
\mu ( t ) = \mu _ { 0 } + \sum _ { i = 1 } ^ { N _ { m } } w _ { i } ( \mu _ { 0 } , t ) d \mu _ { i } ( t ) , \qquad s ( t ) = s _ { 0 } + \sum _ { i = 1 } ^ { N _ { m } } w _ { i } ( \mu _ { 0 } , t ) d s _ { i } ( t ) .
$$

In this work, we regard all the Gaussians as isotropic kernels, which has been demonstrated as an effective way to simplify the model and better reconstruct the scene [6, 53]. We should note that although previous works [29, 54] also perform motion decomposition modeling, our pipeline shows two major differences: 1) instead of modeling each basis with an independent neural network, our module shares a common backbone. Our key observation is that for reconstructing a dynamic object, all points on the object should follow a similar moving tendency, and the final heads of the neural network are sufficient to model the details of different parts of the object; 2) to increase the ability to fit high rank of the dynamic scene [16], we model the motion coefficients as time-variant variables rather than constant Gaussian attributes [29].

![](images/0380109b9de2c35925aeebb77db3a7573a83f378d1e939b55a657f20c355e034.jpg)  
Figure 2: The pipeline of the proposed dynamic 3D Gaussian network. The motion network backbone consists of 8 fully connected (FC) layers. The output of the motion block is fed to $N _ { m }$ heads to generate motion residuals. The coefficient network contains $4 \mathrm { F C }$ layers.

Optimization. We employ the same setting in [16] to train our pipeline. Concretely, the canonical 3D Gaussians are initialized with points randomly sampled from the given bounding box of the scene. We start training the deformation network after 3,000 iterations of warm-up for the 3D Gaussians. Similar to previous works [16, 29], we optimize the pipeline by computing the L1 norm and Structural Similarity Index Measure (SSIM) between the rendered image $I$ and the ground truth image $\tilde { I }$ . Moreover, since large scales may lead to inaccurate reconstructed shapes [55], we thus perform L1 norm on the scale attributes of all the points to recover more fine-grand shapes of the object. Therefore, the overall loss function is defined as:

$$
\begin{array} { r } { \mathcal { L } _ { g s } = \mathcal { L } _ { 1 } ( I , \tilde { I } ) + \lambda _ { 1 } \mathcal { L } _ { s s i m } ( I , \tilde { I } ) + \lambda _ { 2 } \mathcal { L } _ { 1 } ( s ( t ) ) , } \end{array}
$$

where $\lambda _ { 1 }$ and $\lambda _ { 2 }$ are balancing hyperparameters. More in-depth analysis of the proposed pipeline, including implementation details and effects of scale regularization, are presented in Appendix A.1.

# 4.3 Gaussian-informed Continnum Generation

Coarse-to-fine density field generation. Since the reconstructed Gaussian particles are served for rendering only, meaning that they are not evenly distributed on the objects, they cannot be directly used for simulation [51]. Therefore, we propose a novel coarse-to-fine filling strategy to iteratively generate density fields of the object based on the reconstructed Gaussian particles from Eqn. 3 and the internal particles filtered by the rendered depth maps. The proposed strategy is presented in Alg. 1. The implementation details and visual results are illustrated in Appendix A.2.

Concretely, the internal particles, initialized by uniform sampling from the bounding box of Gaussian particles, are filtered by projecting the particles to various images to compare the projected depth with rendered depth values (lines 1-6 in Alg. 1). The resulting particles can roughly represent the shape of the object. However, as denoted in Eqn. 2, the rendered depth maps are evaluated in an accumulated manner, making them less precise in representing the object surface.

Therefore, We employ a coarse-to-fine filling strategy by iteratively upsampling the density field and reassigning the densities on the indices computed from both the Gaussian and internal particles (lines 8-16 in Alg. 1). Fig. 3 provides a sketch illustration of the proposed strategy. Specifically, due to the large grid size at the initial stage, the object is completely inside the voxels with high densities. Next, we sequentially perform upsampling (line 10), mean filtering (line 13), and reassigning the field (line 14) at each iteration. The first two operations produce more fine-grained shapes, and the reassigning operation ensures high densities at the surface to avoid over-erosion caused by the first two steps. Finally, the continuum particles with the corresponding object surfaces can be extracted by thresholding the density field (lines 16-17 in Alg. 1).

![](images/4bf6df339be1d6047642795777d52b20bfdf488c4ba71b7b504f96d3b1ca9a7e.jpg)  
Figure 3: Sketch illustration of the coarse-to-fine filling strategy. Gaussian and internal particles are depicted in green and blue, respectively. (a) Voxels containing particles are assigned high densities. (b) Following the upsampling and smoothing of the field, densities near boundaries become blurred (indicated in light yellow). (c) The particles are again used to correct the voxels that contain particles with high densities. (d) and (e) repeat the previous operations to achieve a more detailed shape.

# Algorithm 1 Pseudo code for coarse-to-fine filling

Input:   
Gaussian particles at time $t$ : $\mathbb { P } _ { G } ( t ) = \{ ( \mu ( t ) , s ( t ) , \sigma , c ) \}$ ;   
$n$ pairs of camera extrinsic and intrinsic parameters: $\{ ( \dot { T } _ { i } , K _ { i } ) | i = 1 . . . n \}$ ;   
parameters: grid size $\Delta x$ ; number of upsampling steps $n _ { u }$ ; thresholds $t h _ { m i n }$ , $t h _ { m i n }$ ;   
Output:   
Continuum particles $\tilde { P } ( t )$ and the corresponding surface $\tilde { S } ( t )$ ;   
1: Randomly sample an initial particle set $P _ { i n }$ from the bounding box of $\{ \mu ( t ) \}$ ;   
2: for $i \gets 1 , n$ do   
3: $\tilde { D } _ { i } = G a u s s i a n S p l a t t i n g ( \mathbb { P } _ { G } ( t ) , T _ { i } , K _ { i } )$ ; ▷ render depth map at view $i$ 4: (uin, vin), $d _ { i n } \gets P r o j ( P _ { i n } , T _ { i } , K _ { i } )$ ; ▷ obtain image indices and depths of $P _ { i n }$ at view $i$ 5: $P _ { i n }  P _ { i n } [ \tilde { D } _ { i } ( u _ { i n } , v _ { i n } ) \leq d _ { i n } ]$ ; ▷ filter out particles that are outside the object 6: end for   
7: Initialize the zero-value density field $F ( t )$ with $\Delta x$ and the bounding box of $\{ \mu ( t ) \}$ ;   
8: for $j  1 , n _ { u }$ do   
9: if $j \neq 1$ then   
10: $F ( t ) \gets T _ { : }$ rilinearInterpolation $( F ( t ) , 2 )$ ▷ upsample $F ( t )$ with scale factor 2 11: $F ( t ) [ p , q , r ] = 1$ , where $p , q , r \gets D i s c r e t i z e ( P _ { i n } \cup \{ \mu ( t ) \} )$ ;   
12: end if   
13: $F ( t ) \gets M e a n F i l t e r i n g ( F ( t ) )$ ;   
14: $F ( t ) [ p , q , r ] = 1$ , where $p , q , r \gets D i s c r e t i z e ( P _ { i n } \cup \{ \mu ( t ) \} )$ ;   
15: end for   
16: $\tilde { P } ( t ) \gets G e t P o s i t i o n ( t h _ { m i n } \leq F ( t ) )$ ;   
17: $\begin{array} { r } { \ddot { S } ( t ) \gets G e t P o s i t i o n ( t h _ { m i n } \leq F ( t ) \leq t h _ { m a x } ) ; } \end{array}$ ;

Gaussian-informed continuum. In PAC-NeRF, the particles are equipped with appearance features to enable image rendering for the continuum at different states. We can also achieve this function by treating the particles as Gaussian kernels and re-train the particles using the visual data. However, this process is cumbersome and will also face the same issue in PAC-NeRF where distorted RGB images will be rendered when large deformation occurs. Therefore, instead of injecting appearance attributes, we opt to assign density and scale attributes to the particles where the densities originate from the density field, and the scale attributes can be directly obtained by the field grid size. The Gaussian-informed continuum is defined as a set of triplets:

$$
\mathbb { P } _ { \tilde { P } } = \{ ( \tilde { p } , s _ { \Delta x } , \sigma _ { F } ) \} ,
$$

where $\tilde { p } \in \tilde { P }$ , $s _ { \Delta x } ~ = ~ \Delta x / 2 ^ { n _ { u } }$ , and $\sigma _ { F } = F [ D i s c r e t i z e ( \tilde { p } ) ]$ (we neglect $t$ in the notation for simplicity). Therefore, we only render object masks as 2D shape surrogates for supervision.

# 4.4 Geometry-aware Physical Property Estimation

With the Gaussian-informed continuum at initial state $\mathbb { P } _ { \tilde { P } } ( 0 )$ and the extracted surfaces $\tilde { S } ( t )$ in place, we can employ MPM to perform simulation on the continuum and evaluate the difference in terms of both the 3D and 2D shapes. Concretely, after a rollout by MPM given the current estimation of physical parameters, we obtain a trajectory $P ( t )$ with corresponding object surfaces $S ( t )$ . We thus can render object masks over the trajectory. Then the loss of the current rollout can be computed as:

$$
\mathcal { L } _ { p p e } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } [ \mathcal { L } _ { C D } ( S ( t _ { i } ) , \tilde { S } ( t _ { i } ) ) + \frac { 1 } { n } \sum _ { j = 1 } ^ { n } \mathcal { L } _ { 1 } ( A _ { j } ( t _ { i } ) , \tilde { A } _ { j } ( t _ { i } ) ) ] ,
$$

where $\mathcal { L } _ { C D }$ and $\mathcal { L } _ { 1 }$ are chamfer distance and L1 norm respectively, $S ( t _ { i } )$ denotes the simulated surface at time $t _ { i }$ , $A _ { j } ( t _ { i } )$ is the rendered mask at view $j$ , and $\tilde { A } _ { j } ( t _ { i } )$ represents the object mask of the image extracted from video $V _ { j }$ at time $t _ { i }$ . Due to the differential property of the simulator, the evaluated loss is used to optimize the target physical parameters $\Theta$ .

# 5 Experiments

Datasets. To thoroughly assess our proposed method, we employ two sources of data introduced by PAC-NeRF [12] and Spring-Gaus [6]. Concretely, PAC-NeRF contributes two synthetic datasets generated by MLS-MPM framework [18]. Each object in both datasets includes RGB images from 11 distinct viewpoints, with approximately 14 frames per viewpoint. The datasets feature a range of materials, including elastic and plastic objects, granular media, and both Newtonian and nonNewtonian fluids. The first dataset contains 45 cross-shape objects with different initial conditions and ground-truth values of physical properties, while the second one consists of 9 objects with different shapes. The interpretation of the physical parameters is listed in Appendix A.9 and A.10. Spring-Gaus generates a synthetic dataset of elastic objects and collects a real-world dataset containing both static and dynamic scenes. The synthetic data contains 30 frames in each of 10 viewpoints. While the real-world data only contains 3 viewpoints for each object in the dynamic scene, it captures 50-70 images from various viewpoints for the static scene. Moreover, we follow previous works [12, 6] and use the off-the-shelf matting [56] or segmentation [57] techniques to obtain object masks.

Baselines. For dynamic reconstruction, we compare with PAC-NeRF and the current state-of-the-art deformable 3D Gaussian method DefGS [16] on the PAC-NeRF synthetic dataset. More comparison of our dynamic 3D Gaussian pipeline on other widely-used datasets such as D-NeRF [25] is presented in Appendix A.1.3. For system identification, we employ PAC-NeRF as the baseline and evaluate the performance using the two datasets introduced in PAC-NeRF. To further demonstrate the precision of the proposed method in terms of geometry recovery and future prediction, we perform experiments on the Spring-Gaus synthetic dataset and compare the results with PAC-NeRF and Spring-Gaus.

Metrics. The evaluation metrics in the experiments include 1) Chamfer Distance (CD), with units expressed in $1 0 ^ { 3 } m m ^ { 2 }$ ; 2) Earth Mover’s Distance (EMD); 3) Peak Signal-to-Noise Ratio (PSNR); 4) Structural Similarity Index Metric (SSIM) [58]; and 5) Mean Absolute Error (MAE), with values scaled by a factor of 100. The first two metrics are used to evaluate discrepancies between the reconstructed and ground-truth point clouds. PSNR and SSIM are leveraged on the Spring-Gaus dataset to validate the precision of future state prediction. We compute the mean absolute error for the evaluation of physical property estimation.

# 5.1 Evaluation on PAC-NeRF Synthetic Dataset

Comparison on dynamic reconstruction. In this experiment, we first perform dynamic Gaussian reconstruction on the cross-shaped object dataset using DefGS and our proposed method, respectively. We then employ the same filling strategy on the reconstructed Gaussians at each time state to generate the continuum, which is regarded as the final recovered geometry of the object and used to make comparisons with the oracle shape to compute CD and EMD. Since PAC-NeRF jointly recovers both geometries and physical parameters, we use the final estimated results to generate the trajectory for evaluation.

The results, reported in Tab. 1, show that our method outperforms the baselines on both metrics and achieves more precise reconstruction performance on most objects. Specifically, we find that the NeRF representation used by PAC-NeRF usually leads to overly large shape generation. While DefGS performs well on elastic objects, its performance degenerates when modeling objects with large deformations, such as granular media and fluids. Our method can better handle these objects due to the flexibility of trajectory representation.

![](images/873ad82db725790fea84bba8a15c1a5dc5c4c82ffcdac3fcf4c341e080a00008.jpg)  
Figure 4: Comparison between rendered and ground-truth images. (a) Rendered RGB images by PAC-NeRF. (b) Rendered masks by our method. (c)-(d) Ground-truth RGB images and masks. The mask-based supervision can introduce fewer discrepancies compared with the RGB-based guidance when the estimated shapes are correct.

Table 1: Dynamic Reconstruction on PAC-NeRF Dataset   

<html><body><table><tr><td>Metrics</td><td colspan="3">CD←</td><td colspan="3">EMD↓</td></tr><tr><td>Methods</td><td>PAC-NeRF[12]</td><td>DefGS[16]</td><td>Ours</td><td>PAC-NeRF[12]</td><td>DefGS[16]</td><td>Ours</td></tr><tr><td>Newtonian</td><td>0.277</td><td>0.269</td><td>0.243</td><td>0.027</td><td>0.027</td><td>0.025</td></tr><tr><td>Non-Newtonian</td><td>0.236</td><td>0.216</td><td>0.195</td><td>0.025</td><td>0.024</td><td>0.022</td></tr><tr><td>Elasticity</td><td>0.238</td><td>0.191</td><td>0.178</td><td>0.025</td><td>0.022</td><td>0.02</td></tr><tr><td>Plasticine</td><td>0.429</td><td>0.213</td><td>0.196</td><td>0.029</td><td>0.024</td><td>0.022</td></tr><tr><td>Sand</td><td>0.212</td><td>0.281</td><td>0.25</td><td>0.025</td><td>0.028</td><td>0.025</td></tr><tr><td>Mean</td><td>0.278</td><td>0.234</td><td>0.212</td><td>0.026</td><td>0.025</td><td>0.023</td></tr></table></body></html>

# Comparison on system identification.

We evaluate the performance of system identification of the two datasets proposed by PAC-NeRF. For the first dataset, we compute the MAE of the parameters for each type of object. To demonstrate the effectiveness of the 2D shape representation, we also conduct experiments on the second dataset by only using masks for supervision on our method, namely “Ours\*”. For the second dataset, we execute 10 times of our method with different random seeds for each object instance and report the mean value of the estimation results. The training details are illustrated in Appendix A.3.

The results, reported in Tab. 2 and Tab. 3, show that the proposed hybrid pipeline can achieve more accurate estimation over a wide range of entries and objects, which demonstrate the effectiveness of the geometry-aware guidance. Fig. 4 visualizes the RGB images rendered by PACNeRF and the masks rendered by our method. We can see that when large deformation occurs, the rendered RGB image

Table 2: System identification performance on PAC-NeRF cross-shaped object Dataset   

<html><body><table><tr><td>Type</td><td>Parameters PAC-NeRF</td><td>Ours*</td><td>Ours</td></tr><tr><td>Newtonian</td><td>log10(μ) 11.6±6.60 log10(k) U</td><td>1.53±1.45 1.53±1.31 16.7±5.37 16.0±22.4 14.8±19.2 0.86±1.45 0.20±0.08 0.20±0.07</td><td></td></tr><tr><td>Non- Newtonian</td><td>log10(μ) log10(k) log10(TY) log10(n) U</td><td>24.1±21.9 32.9±44.6 13.5±18.2 44.0±26.3 17.7±20.2 12.9±16.8 5.09±7.41 3.74±3.724.80±3.92 28.7±23.3 34.9±24.1 40.7±24.6 0.29±0.13 0.68±0.28 0.19±0.09</td><td></td></tr><tr><td>Elasticity</td><td>log10(E) V U</td><td>3.02±3.72 3.27±4.13 2.43±3.29 4.35±5.08 3.10±2.00 2.52±2.03 0.50±0.23 0.78±0.26 0.82±0.32</td><td></td></tr><tr><td>Plasticine</td><td>log10(E) log10(TY) V U</td><td>83.8±68.4 28.1±24.4 25.6±29.4 11.2±14.5 1.24±0.90 1.67±1.21 18.9±15.7 10.2±5.34 9.59±5.00 0.56±0.17 0.13±0.04 0.22±0.10</td><td></td></tr><tr><td>Sand</td><td>θfric U</td><td>4.89±1.10 4.21±0.08 4.18±0.52 0.21±0.08 0.24±0.08 0.17±0.05</td><td></td></tr></table></body></html>

becomes distorted, while the rendered mask can effectively reduce such effect and get better performance. By leveraging both 3D and 2D shape guidance, our method obtains the best results on most entries. More qualitative results are available in the supplementary video.

Table 3: System Identification Performance on PAC-NeRF Dataset   

<html><body><table><tr><td></td><td>PAC-NeRF[12]</td><td>Ours</td><td>Ground Truth</td></tr><tr><td>Droplet</td><td>μ = 2.09×10²,κ = 1.08 ×105</td><td>μ = 2.01 ×10²,κ =0.18 ×105</td><td>μ = 200,κ = 105</td></tr><tr><td>Letter</td><td>μ= 83.85,κ = 1.35 ×105</td><td>μ = 95.05,κ =1.00 ×105</td><td>μ = 100,κ = 105</td></tr><tr><td>Cream</td><td>μ = 1.21 × 10,κ = 1.57 × 106， Ty = 3.16× 10,n = 5.6</td><td>μ = 1.03 × 104,κ = 1.48 ×106, Ty = 2.98 ×10,n = 6.6</td><td>μ = 104,κ= 10, Ty =3×103,n =10</td></tr><tr><td>Toothpaste</td><td>μ=6.51×10³,κ=2.22×105，</td><td>μ= 4.19×10,κ =9.24×104, TY = 226,n = 9.1</td><td>μ=5×10³,κ=10,</td></tr><tr><td>Torus</td><td>TY = 228,n =9.77 E= 1.04× 10,v = 0.322</td><td>E=0.99×10,v =0.295</td><td>TY = 200,n = 10 E=10,v =0.3</td></tr><tr><td>Bird</td><td>E= 2.78×10,v=0.273</td><td>E= 3.08×10,v=0.284</td><td>E=3×10,ν =0.3</td></tr><tr><td>Playdoh</td><td>E = 3.84× 10,v = 0.272,ty = 1.69 × 104</td><td>E=1.58×10,v=0.322,Ty =1.56×104</td><td>E=2×10,v =0.3,Ty =1.54×104</td></tr><tr><td>Cat</td><td>E =1.61×10,v=0.293,Ty =3.57×10</td><td>E=0.98×10,v=0.296,Ty =3.76×10</td><td>E = 10,v =0.3,ty =3.85×10</td></tr><tr><td>Trophy</td><td>0ric =36.1°</td><td>ric=38.0°</td><td>ric=40°</td></tr></table></body></html>

Table 4: Future State Simulation on Spring-Gaus Synthetic Dataset   

<html><body><table><tr><td></td><td></td><td>torus</td><td>cross</td><td>cream</td><td>apple</td><td>paste</td><td>chess</td><td>banana</td><td>Mean</td></tr><tr><td rowspan="3">↑</td><td>Ppri-N-GRF [12]</td><td>2.38</td><td>1.57</td><td>2.22</td><td>1.87</td><td>7.03</td><td>2.</td><td>18.48</td><td>5.14</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>0.75</td><td>1.09</td><td>0.94</td><td>0.22</td><td>2.79</td><td>0.77</td><td>0.12</td><td>0.95</td></tr><tr><td rowspan="3"></td><td>Spring-Gaus [6]</td><td>0.087</td><td>0.051</td><td>0.094</td><td>0.076</td><td>0.126</td><td>0.095</td><td>0.135</td><td>0.095</td></tr><tr><td>PAC-NeRF[12]</td><td>0.055</td><td>0.111</td><td>0.083</td><td>0.108</td><td>0.192</td><td>0.155</td><td>0.234</td><td>0.134</td></tr><tr><td>Ours</td><td>0.034</td><td>0.058</td><td>0.050</td><td>0.030</td><td>0.096</td><td>0.059</td><td>0.017</td><td>0.049</td></tr><tr><td rowspan="3"></td><td>Spring-Gaus [6]</td><td>16.83</td><td>16.93</td><td>15.42</td><td>21.55</td><td>14.71</td><td>16.08</td><td>17.89</td><td>17.06</td></tr><tr><td>PAC-NeRF[12]</td><td>17.46</td><td>14.15</td><td>15.37</td><td>19.94</td><td>12.32</td><td>15.08</td><td>16.04</td><td>15.77</td></tr><tr><td>Ours</td><td>20.24</td><td>30.51</td><td>19.15</td><td>26.89</td><td>16.31</td><td>18.44</td><td>29.29</td><td>22.98</td></tr><tr><td rowspan="3">↓WISS</td><td>Spring-Gaus [6]</td><td>0.919</td><td>0.940</td><td>0.862</td><td>0.902</td><td>0.872</td><td>0.881</td><td>0.904</td><td>0.897</td></tr><tr><td>PAC-NeRF[12]</td><td>0.913</td><td>0.906</td><td>0.858</td><td>0.878</td><td>0.819</td><td>0.848</td><td>0.886</td><td>0.870</td></tr><tr><td>Ours</td><td>0.942</td><td>0.939</td><td>0.909</td><td>0.948</td><td>0.894</td><td>0.912</td><td>0.964</td><td>0.930</td></tr></table></body></html>

# 5.2 Evaluation on Spring-Gaus Synthetic Dataset

Comparison on future state simulation. To further demonstrate the performance of our proposed method, we follow the setting in Spring-Gaus [6] that uses the first 20 frames as training data and the subsequent 10 frames for evaluation. Concretely, we first perform system identification based on our method and then use the estimated physical parameters and the continuum to simulate a trajectory that includes the states of the 30 frames. Therefore, we can compute CD and EMD between the simulated continuum and the ground-truth point cloud. Since we know the exact position of the continuum at each time state after estimation, we can assign time-invariant Gaussian attributes by training Gaussians on the continuum using the first 20 frames of RGB images, which enable image rendering at novel views and states. Therefore, we can compute PSNR and SSIM at any time state.

The results of future state prediction are presented in Tab. 4, and the results of reconstruction on the training states are reported in Appendix A.4. We observe that our method significantly outperforms the baselines on CD and EMD metrics over almost all object instances, which shows the superiority of our method for both geometry recovery and system identification. The results of PSNR and SSIM show that leveraging dynamic visual data to train the Gaussian attributes on the continuum improves rendering quality. This further reveals that the generated trajectories are precise such that the particles are consistent to contribute to the rendering for the same region of the object at different time states.

# 5.3 Real-world Application: Digital Twins in Robotic Grasping Scenario

To demonstrate the efficacy of the proposed method in real-world scenarios, we perform system identification on the real-world dataset collected by Spring-Gaus [6], as shown in Fig. 5. Since the real-world dataset consists of static and dynamic scenes for each object, we follow the procedure introduced by Spring-Gaus to progressively 1) reconstruct a Gaussian set of the object from the static scene, 2) transform the static Gaussian set to the initial state of the dynamic scene based on a registration network similar as iNeRF [6, 59], and 3) perform system identification from the dynamic observation by our method “Ours\*” due to the lack of sufficient images for dynamic reconstruction. Subsequently, we establish robotic platforms in both simulated and real-world environments, each equipped with UR10 robot arms configured identically. We then execute grasp attempts on both the reconstructed objects with the estimated properties in the simulation and the corresponding real-world objects under the same configuration. The results of more objects, and more details about the training and the experiment setting are presented in Appendix A.5. From the results shown in Fig. 5, we see that our method demonstrates its capability to effectively model the deformation experienced by the objects upon impact with a surface. Furthermore, by applying identical gripper forces to both the simulated and real-world versions of the objects, we observe similar deformation behaviors. This consistency in deformation under identical conditions supports that the estimated physical parameters closely mirror the real-world properties of the objects.

![](images/178e0aa8632d7c414b67435fc9ebf5db3bb8170489b95f1fa120d2acaf9492ab.jpg)  
Figure 5: Real-world application. Left: Identification and future state simulation. Right: Grasping simulation. The stress on the simulated object is indicated by blue (low) to red (high). The gripper widths from top to bottom are set to 6cm, $4 . 5 \mathrm { c m }$ , and $3 . 5 \mathrm { c m }$ , respectively.

# 6 Conclusion and Limitations

This paper proposes a novel solution that leverages the 3D Gaussian representation of objects to acquire explicit shapes while concurrently enabling the simulated continuum to render 2D shapes to facilitate the estimation of physical properties. A novel motion-factorized dynamic 3D Gaussian framework is proposed to reconstruct precise dynamic scenes. Object surfaces and Gaussian-informed continuum are obtained by utilizing the proposed coarse-to-fine density field generation strategy. Extensive experiments demonstrate the efficacy and applicability of our method.

Despite the performance we achieve, this method still suffers from limitations, such as the assumption of continuum mechanics, the requirements of multi-view images with known camera poses, and the need for prior knowledge of object constitutive models. Integrating the pose-free method [60] or generalized constitutive [61] model with our method will be an interesting direction for future work.

From the perspective of application, while this method can yield accurate estimations, it may pose risks for fragile objects, as the interaction required for property inference could potentially cause damage. Moreover, the computational demands of our framework are substantial which require at least 1.5 hours to simultaneously recover both the geometry and physical properties of each object. Future work could explore leveraging multi-model large language models [62] and large reconstruction models [63–66] to facilitate the recovery process.