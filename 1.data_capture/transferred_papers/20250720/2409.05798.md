# Enhancing Preference-based Linear Bandits via Human Response Time

Shen Li1∗ Yuyang Zhang2 ∗ Zhaolin Ren2 Claire Liang1 Na Li2 Julie A. Shah1 1Massachusetts Institute of Technology 2Harvard University {shenli,cyl48}@mit.edu, julie_a_shah@csail.mit.edu {yuyangzhang,zhaolinren}@g.harvard.edu, nali@seas.harvard.edu

# Abstract

Interactive preference learning systems present humans with queries as pairs of options; humans then select their preferred choice, allowing the system to infer preferences from these binary choices. While binary choice feedback is simple and widely used, it offers limited information about preference strength. To address this, we leverage human response times, which inversely correlate with preference strength, as complementary information. We introduce a computationally efficient method based on the EZ-diffusion model, combining choices and response times to estimate the underlying human utility function. Theoretical and empirical comparisons with traditional choice-only estimators show that for queries where humans have strong preferences (i.e., “easy” queries), response times provide valuable complementary information and enhance utility estimates. We integrate this estimator into preference-based linear bandits for fixed-budget best-arm identification. Simulations on three real-world datasets demonstrate that incorporating response times significantly accelerates preference learning.

# 1 Introduction

Interactive preference learning from human binary choices is essential in systems like recommender systems [9, 21, 32, 56], assistive robots [54, 65], and fine-tuning of large language models [5, 43, 46, 47, 59]. This process is often framed as a preference-based bandit problem [7, 31], where the system repeatedly presents pairs of options, the human selects their preferred option, and the system infers preferences based on these choices. Choices feedback is widely used for its simplicity and low cognitive burden on users [37, 72, 74]. However, while binary choices capture preferences, they offer limited insight into preference strength [77]. To address this, researchers have incorporated additional human explicit feedback, such as ratings [50, 58], labels [74], and slider bars [5, 72], but these approaches tend to complicate the interface and increase users’ cognitive load [36, 37].

In this paper, we propose leveraging human implicit feedback, specifically response times, to provide additional insights into preference strength. Unlike explicit feedback, response time is unobtrusive and effortless to measure [17], offering valuable information that complements binary choices [2, 16]. For instance, consider an online retailer that repeatedly presents users with a binary query, whether to purchase or skip a recommended product [35]. Since most users skip products most of the time [33], the probability of skipping becomes nearly 1 for most items. This lack of variation in choices makes it difficult to assess how much a user likes or dislikes any specific product, limiting the system’s ability to accurately infer their preferences. Response time can help overcome this limitation. Psychological research shows an inverse relationship between response time and preference strength [17]: users who strongly prefer to skip a product tend to do so quickly, while longer response times can indicate weaker preferences. Thus, even when choices appear similar, response time can uncover subtle differences in preference strength, helping to accelerate preference learning.

Leveraging response times for preference learning presents notable challenges. Psychological research has extensively studied the relationship between human choices and response times [17, 19] using complex models like Drift-Diffusion Models [51] and Race Models [12, 66]. While these models align with both behavioral and neurobiological evidence [70], they rely on computationally intensive methods, such as hierarchical Bayesian inference [71] and maximum likelihood estimation (MLE) [52], to estimate the underlying human utility functions from both human choices and response times, making them impractical for real-time interactive systems. Although faster estimators exist [8, 28, 30, 67, 68], they typically estimate the utility functions for a single pair of options, without aggregating data across multiple pairs. This limits their ability to leverage structures like linear utility functions, which are critical both in preference learning with large option spaces [21, 24, 41, 54, 56] and in cognitive models for human multi-attribute decision-making [26, 64, 76].

To address these challenges, we propose a computationally efficient method for estimating linear human utility functions by incorporating both choices and response times, based on the differencebased EZ diffusion model [8, 67]. Our method leverages response times to transform binary choice signals into continuous signals, framing utility estimation as a linear regression that aggregates data across multiple pairs. We compare our estimator to traditional logistic regression methods that rely solely on choices [3, 31]. Our theoretical and empirical analyses show that for binary queries with strong preferences (i.e., “easy” queries), choices alone provide limited information, while response times offer valuable insights into preference strength, significantly enhancing utility estimates. Thus, incorporating response times makes easy queries more informative.

Our linear-regression-based estimator integrates seamlessly into algorithms for preference-based bandits with linear human utility functions [3, 31], enabling interactive learning systems to leverage response times for faster learning. We specifically integrated our estimator into the Generalized Successive Elimination algorithm [3] for fixed-budget best-arm identification [29, 34]. Simulations using three real-world datasets [16, 39, 57] consistently show that incorporating response times significantly reduces identification errors, compared to traditional methods that rely solely on choices. To the best of our knowledge, this is the first work to integrate response times into bandit (and RL).

Section 2 introduces the preference-based linear bandit problem and the difference-based EZ diffusion model. Section 3 presents our utility estimator, incorporating both choices and response times, and offers a theoretical comparison to the choice-only estimator. Section 4 integrates both estimators into the Generalized Successive Elimination algorithm. Section 5 presents empirical results for estimation and bandit learning. Section 6 discusses the limitations of our approach. Appendix B reviews response time models, parameter estimation techniques, and their connection to preference-based RL.

Nomenclature: We use $[ n ]$ to denote the set $\{ 1 , \ldots , n \}$ . For a scalar random variable $x$ , $\mathbb { E } \left[ x \right]$ and $\mathbb { V } \left[ x \right]$ denote its expectation and variance, respectively. The function $\operatorname { s g n } ( x )$ denotes the sign of $x$ .

# 2 Problem setting and preliminaries

Preference-based bandits with a linear utility function. The learner is given a finite set of options (or “arms”), each represented by a feature vector in $\mathcal { Z } \subset \mathbb { R } ^ { d }$ , and a finite set of binary queries, where each query is the difference between two arms, denoted by $\boldsymbol { \mathcal { X } } \subset \mathbb { R } ^ { d }$ . For instance, if the learner can query any pair of arms, the query space is $\mathcal { X } = \{ z - z ^ { \prime } \colon z , z ^ { \prime } \in \mathcal { Z } \}$ . In the online retailer example from section 1, the query space is $\mathcal { X } = \{ z - z _ { \mathrm { s k i p } } \colon z \in \mathcal { Z } \}$ , where $z$ represents purchasing a product and $z _ { \mathrm { s k i p } }$ represents skipping (often set as 0). For each arm $z \in { \mathcal { Z } }$ , the human utility is assumed to be linear in the feature space, defined as $u _ { z } : = z ^ { \top } \theta ^ { * }$ , where $\theta ^ { * } \in \mathbb { R } ^ { d }$ represents the human’s preference parameters. For any query $x \in \mathcal { X }$ , the utility difference is then defined as $u _ { x } : = x ^ { \top } \theta ^ { * }$ .

Given a query $x : = z _ { 1 } - z _ { 2 } \in \mathcal { X }$ , we model human choices and response times using the differencebased EZ-Diffusion Model (dEZDM) [8, 67], integrated with our linear utility structure. (See appendix B.1 for a comparison with other models.) This model interprets human decision-making as a stochastic process in which evidence accumulates over time to compare two options. As shown in fig. 1a, after receiving a query $x$ , the human first spends a fixed amount of non-decision time, denoted by $t _ { \mathrm { n o n d e c } } > 0$ , to perceive and encode the query. Then, evidence $E _ { x }$ accumulates over time following a Brownian motion with drift $x ^ { \top } \theta ^ { * }$ and two symmetric absorbing barriers, $a > 0$ and $- a$ . Specifically, at time $t _ { \mathrm { n o n d e c } } + \tau$ where $\tau \geq 0$ , the evidence is $E _ { x , \tau } = x ^ { \top } \theta ^ { * } \cdot \tau + B ( \tau )$ , where $B ( \bar { \tau } ) \sim \mathcal { N } ( \dot { 0 } , \tau )$ is standard Brownian motion. This process continues until the evidence reaches either the upper barrier $a$ or lower barrier $- a$ , at which point a decision is made. The random stopping time, $t _ { x } : = \operatorname* { m i n } \left\{ \tau > 0 \colon E _ { x , \tau } \in \{ a , - a \} \right\}$ , represents the decision time. If $E _ { x , t _ { x } } = a$ , the human chooses $z _ { 1 }$ ; if $E _ { x , t _ { x } } = - a$ , they choose $z _ { 2 }$ . The choice is represented by the random variable $c _ { x }$ , where $c _ { x } = 1$ if $z _ { 1 }$ is chosen, and $- 1$ if $z _ { 2 }$ is chosen. The total response time, $t _ { \mathrm { R T } , x }$ , is the sum of the non-decision time and the decision time: $t _ { \mathrm { R T } , x } = t _ { \mathrm { n o n d e c } } + t _ { x }$ . The choice probability, expected choice, choice variance, and expected decision time are given as follows [48, eq. (A.16) and (A.17)]:

$$
\forall x \in \mathcal { X } \colon \mathbb { P } \left[ c _ { x } = 1 \right] = \frac { 1 } { 1 + \exp ( - 2 a x ^ { \top } \theta ^ { * } ) } , \mathbb { E } \left[ c _ { x } \right] = \operatorname { t a n h } ( a x ^ { \top } \theta ^ { * } )
$$

$$
\begin{array}{c} \begin{array} { r } { \mathbb { V } [ c _ { x } ] = 1 - \operatorname { t a n h } ^ { 2 } ( \boldsymbol { a } x ^ { \top } \boldsymbol { \theta } ^ { * } ) , \mathbb { E } [ t _ { x } ] = \{ \frac { \alpha } { x ^ { \top } \boldsymbol { \theta } ^ { * } } \operatorname { t a n h } ( \boldsymbol { a } x ^ { \top } \boldsymbol { \theta } ^ { * } )  \mathrm { ~ i f ~ } x ^ { \top } \boldsymbol { \theta } ^ { * } \neq 0 } \\ { a ^ { 2 } \qquad \mathrm { ~ i f ~ } x ^ { \top } \boldsymbol { \theta } ^ { * } = 0 } \end{array}  \mathrm { ~ . ~ }  \end{array}
$$

This choice probability matches that of the Bradley and Terry [10] model. If the learner relies solely on choices, then our bandit problem reduces to the transductive linear logistic bandit problem [31].

Figures 1b and 1c illustrate the roles of the parameters $x ^ { \top } \theta ^ { * }$ and $a$ . First, the absolute drift (or the absolute utility difference), $| x ^ { \top } \theta ^ { * } |$ , represents the query’s easiness [40]. A higher $| x ^ { \top } \theta ^ { * } |$ corresponds to an easier query, resulting in shorter decision times and more consistent choices. In contrast, when $| x ^ { \top } \theta ^ { * } |$ is low (close to 0), the query becomes harder, leading to longer decision times and less consistent choices. Second, the barrier $a$ represents the human’s conservativeness in decisionmaking [40]. A higher $a$ requires more evidence to reach a decision, leading to longer decision times and more consistent choices, while a lower $a$ results in quicker, but less consistent, choices.

![](images/65728d9f3e06a35c71bbd261ee438e1e006a1f22aee61b091982d6ff1cdeb57a.jpg)  
Figure 1: (a) depicts the human decision-making process for a binary query $x \in \mathcal { X }$ , where the human selects between two arms. The human first spends a fixed non-decision time $t _ { \mathrm { n o n d e c } }$ encoding the query. Then, the human’s evidence accumulates according to a Brownian motion with drift $\bar { x } ^ { \top } \theta ^ { * }$ . When the evidence reaches the upper barrier $a$ or lower barrier $- a$ , the human makes a choice, denoted by $c _ { x } = 1$ or $c _ { x } = - 1$ , respectively. The random stopping time of the accumulation process is the decision time $t _ { x }$ , and the total response time is $t _ { \mathrm { R T } , x } = t _ { \mathrm { n o n d e c } } + t _ { x }$ . (b) and (c) plot the expected choice $\mathbb { E } [ c _ { x } ]$ and the expected decision time $\mathbb { E } [ t _ { x } ]$ , with shaded regions representing one standard deviation, plotted as functions of the utility difference $x ^ { \top } \theta ^ { * }$ for two barrier values $a$ .

We adopt the common assumption that $t _ { \mathrm { n o n d e c } }$ is constant across all queries for a given human [16, 76] and further assume that $t _ { \mathrm { n o n d e c } }$ is known to the learner. This assumption enables the learner to perfectly recover $t _ { x }$ from the observed $t _ { \mathrm { R T } , x }$ . In section 5.2, we empirically show that even when $t _ { \mathrm { n o n d e c } }$ is unknown, its impact on the performance of our method that relies on decision times is negligible.

Learning objective: Best-arm identification with a fixed budget. We focus on the fixed-budget best-arm identification problem [29, 34]. The learner is provided with a total interaction time budget $B > 0$ , an arm space $\mathcal { Z }$ , a query space $\chi$ , and a non-decision time $t _ { \mathrm { n o n d e c } }$ . Both the human’s preference vector $\theta ^ { * }$ and the decision barrier $a$ are unknown. In each episode $s \in \mathbb { N }$ , the learner selects a query $x _ { s } \in \mathcal { X }$ , receives human feedback $( c _ { x _ { s } , s } , t _ { x _ { s } , s } )$ generated by the dEZDM, and consumes $t _ { \mathrm { R T } , x _ { s } , s }$ time. When the cumulative interaction time exceeds the budget $B$ at some episode $S$ , i.e., $\textstyle \sum _ { s = 1 } ^ { S } t _ { \mathrm { R T } , x _ { s } , s } > B$ , the learner must stop and recommend an arm $\widehat { z } \in \mathcal { Z }$ . The goal is to recommend the unique best arm $z ^ { * } : = \arg \operatorname* { m a x } _ { z \in { \mathcal { Z } } } z ^ { \top } \theta ^ { * }$ , minimizing the errorb probability $\mathbb { P } \left[ \widehat { \boldsymbol { z } } \neq \boldsymbol { z } ^ { * } \right]$ .

To solve this problem, we use the Generalized Successive Elimination (GSE) algborithm [1, 3, 75]. This algorithm divides the total budget $B$ evenly into multiple phases. In each phase $s$ , it strategically samples queries till this phase’s allocated budget is exhausted, collecting both human choices and response times. It then computes an estimate $\widehat { \theta _ { s } } ^ { - }$ of the true preference vector $\theta ^ { * }$ and eliminates arms with low estimated utilities based on $\widehat { \theta } _ { s }$ . Thebkey to improving estimation is effectively leveraging decision times to gain additional insigbhts into preference strength, complementing the information provided by choices. In section 3, we introduce this estimator and compare it theoretically to the traditional choice-only estimator. section 4 details the integration of both estimators into GSE.

# 3 Utility estimation

This section addresses the problem of estimating human preference $\theta ^ { * }$ from a fixed dataset, denoted by $\begin{array} { r } { \left\{ x , c _ { x , s _ { x , i } } , t _ { x , s _ { x , i } } \right\} _ { x \in \mathcal { X } _ { \mathrm { s a m p l e } } , i \in \left[ n _ { x } \right] } } \end{array}$ . Here, $\mathcal { X } _ { \mathrm { s a m p l e } }$ denotes the set of queries in the dataset, $n _ { x }$ denotes the number of samples for each query $x \in \mathcal { X } _ { \mathrm { s a m p l e } }$ , and $s _ { x , i }$ denotes the episode when $x$ is sampled for the $i$ -th time. Samples from the same query $x$ are i.i.d., while samples from different queries are independent. Section 3.1 introduces a new estimator, the “choice-decision-time estimator,” which uses both choices and decision times, in contrast to the commonly used “choice-only estimator” that only uses choices [3, 31]. Sections 3.2 and 3.3 theoretically compares these estimators, analyzing both asymptotic and non-asymptotic performance and highlighting the advantages of incorporating decision times. Section 5.1 presents empirical results that validate our theoretical insights.

# 3.1 Choice-decision-time estimator and choice-only estimator

The choice-decision-time estimator is based on the following relationship between human utilities, choices, and decision times, derived from eq. (1):

$$
\forall x \in \mathcal { X } \colon x ^ { \top } \frac { \theta ^ { * } } { a } = \frac { \mathbb { E } \left[ c _ { x } \right] } { \mathbb { E } \left[ t _ { x } \right] } .
$$

Intuitively, when a human provides consistent choices (i.e., large $\left| \mathbb { E } [ c _ { x } ] \right| )$ and makes decisions quickly (i.e., small $\mathbb { E } [ t _ { x } ] )$ , it suggests that the query is easy and the preference is strong (i.e., large $| x ^ { \top } \theta ^ { * } | )$ . This relationship reframes the estimation of $\theta ^ { * }$ as a linear regression problem. Accordingly, the choice-decision-time estimator calculates the empirical means of both choices and decision times, aggregates the ratios across all sampled queries, and applies ordinary least squares (OLS) to estimate $\theta ^ { * } / a$ . Since the ranking of arm utilities based on $\theta ^ { * } / a$ is identical to that based on $\theta ^ { * }$ , estimating $\theta ^ { * } / a$ is sufficient for identifying the best arm. This estimate of $\theta ^ { * } / a$ , denoted by $\widehat { \theta } _ { \mathrm { C H , D T } }$ , is given by:

$$
\widehat { \theta } _ { \mathrm { C H , D T } } : = \left( \sum _ { x \in \mathcal { X } _ { \mathrm { s a m p l e } } } n _ { x } x x ^ { \top } \right) ^ { - 1 } \sum _ { x \in \mathcal { X } _ { \mathrm { s a m p l e } } } n _ { x } x \ \frac { \sum _ { i = 1 } ^ { n _ { x } } c _ { x , s _ { x , i } } } { \sum _ { i = 1 } ^ { n _ { x } } t _ { x , s _ { x , i } } } .
$$

In contrast, the choice-only estimator is based on eq. (1), which shows that for each query $x \in \mathcal { X }$ , the random variable $( c _ { x } + 1 ) / 2$ follows a Bernoulli distribution with mean $1 / [ 1 + \mathrm { e x p } ( \stackrel { . } { - } x ^ { \top } \cdot 2 a \theta ^ { * } ) ]$ . Similar to the choice-decision-time estimator, the parameter $2 a$ does not impact the ranking of arms, so estimating $2 a \theta ^ { * }$ is sufficient for best-arm identification. This estimation is formulated as a logistic regression problem [3, 31], with MLE providing the following estimate of $2 a \theta ^ { * }$ , denoted by $\widehat { \theta } _ { \mathrm { C H } } ^ { - }$ :

$$
\widehat { \theta } _ { \mathrm { C H } } : = \underset { \theta \in \mathbb { R } ^ { d } } { \arg \operatorname* { m a x } } \sum _ { x \in \mathcal { X } _ { \mathrm { s a m p l e } } } \sum _ { i = 1 } ^ { n _ { x } } \log \mu ( c _ { x , s _ { x , i } } x ^ { \top } \theta ) ,
$$

where $\mu ( y ) : = 1 / [ 1 + \exp ( - y ) ]$ is the standard logistic function. While this MLE lacks a closed-form solution, it can be efficiently solved using optimization methods like Newton’s algorithm [25, 44].

# 3.2 Asymptotic normality of the two estimators

The choice-decision-time estimator from eq. (3) satisfies the following asymptotic normality result: Theorem 3.1 (Asymptotic normality of $\widehat { \theta } _ { \mathrm { C H , D T } } )$ ). Given an i.i.d. dataset $\left\{ x , c _ { x , s _ { x , i } } , t _ { x , s _ { x , i } } \right\} _ { i \in [ n ] } f o r$ each $x \in \mathcal { X } _ { s a m p l e }$ , where $\textstyle \sum _ { \boldsymbol { x } \in \mathcal { X } _ { s a m p l e } } \boldsymbol { x } \boldsymbol { x } ^ { \intercal } \succ \boldsymbol { 0 }$ , and assuming that the datasets for different $x \in \mathcal { X } _ { s a m p l e }$ are independent, then, for any vector $\boldsymbol { y } \in \mathbb { R } ^ { d }$ , as $n \to \infty$ , the following holds:

$$
\sqrt { n } y ^ { \top } \left( \widehat { \theta } _ { C H , D T , n } - \theta ^ { * } / a \right) \xrightarrow { D } \mathcal { N } ( 0 , \zeta ^ { 2 } / a ^ { 2 } ) .
$$

Here, the asymptotic variance depends on a problem-specific constant, $\zeta ^ { 2 }$ , with an upper bounded:

$$
\begin{array} { r } { \zeta ^ { 2 } \leq \| y \| _ { \left( \sum _ { \boldsymbol { x } \in \mathcal { X } _ { s a m p l e } } ^ { 2 } \left[ \operatorname* { m i n } _ { \boldsymbol { x } ^ { \prime } \in \mathcal { X } _ { s a m p l e } } \mathbb { E } [ t _ { x ^ { \prime } } ] \right] \cdot \boldsymbol { x } \boldsymbol { x } ^ { \top } \right) ^ { - 1 } } ^ { 2 } . } \end{array}
$$

The proof is provided in appendix C.2. The asymptotic variance upper bound shows that all sampled queries are weighted by a common factor $\mathrm { m i n } _ { x ^ { \prime } \in \mathcal { X } _ { \mathrm { s a m p l e } } } \mathbb { E } \left[ t _ { x ^ { \prime } } \right]$ , which is the smallest expected decision time among all the sampled queries. This weight represents the amount of information provided by each query’s choices and decision times to the estimation of $\theta ^ { * }$ . A larger weight indicates that every query provides more information, leading to lower variance and better estimates.

In contrast, the choice-only estimator from eq. (4) has the following asymptotic normality result, as derived from Fahrmeir and Kaufmann [23, corollary 1]:

Theorem 3.2 (Asymptotic normality of $\widehat { \theta } _ { \mathrm { C H } } )$ ). Given an i.i.d. dataset $\left\{ x , c _ { x , s _ { x , i } } , t _ { x , s _ { x , i } } \right\} _ { i \in [ n ] }$ for each $x \in \mathcal { X } _ { s a m p l e }$ , where $\textstyle \sum _ { \boldsymbol { x } \in \mathcal { X } _ { s a m p l e } } \boldsymbol { x } \boldsymbol { x } ^ { \intercal } \succ \boldsymbol { 0 } ,$ , and assuming that the datasets for different $x \in \mathcal { X } _ { s a m p l e }$ are independent, then, for any vector $\boldsymbol { y } \in \mathbb { R } ^ { d }$ , as $n \to \infty$ , the following holds:

$$
\begin{array} { r } { \sqrt { n } y ^ { \top } \left( \widehat { \theta } _ { C H , n } - 2 a \theta ^ { * } \right) \xrightarrow { D } \mathcal { N } \left( 0 , 4 a ^ { 2 } \left. y \right. _ { \left( \sum _ { x \in \mathcal { X } _ { s a m p l e } } [ a ^ { 2 } \mathbb { V } [ c _ { x } ] ] \cdot x x ^ { \top } \right) ^ { - 1 } } ^ { 2 } \right) . } \end{array}
$$

This asymptotic variance shows that each sampled query $x$ is weighted by its own factor $\boldsymbol { a } ^ { 2 } \mathbb { V } \left[ \boldsymbol { c } _ { x } \right]$ , representing the amount of information the query’s choices contribute to estimating $\theta ^ { * }$ . A larger weight indicates that the query contributes more information, leading to better estimates.

The weights in both theorems highlight the different contributions of choices and decision times to estimating $\theta ^ { * }$ . In the choice-only estimator (theorem 3.2), each query is weighted by $\boldsymbol { a } ^ { 2 } \mathbb { V } \left[ c _ { x } \right]$ , which is a function of the utility difference $x ^ { \top } \theta ^ { * }$ for a fixed barrier $a$ . As shown in the dashed or solid gray curve in fig. 2a, as queries become easier (i.e., as $| x ^ { \top } \theta ^ { * } |$ increases), the weight quickly decays to zero, indicating that choices from easy queries provide much less information than harder queries. Intuitively, for easy queries, humans consistently choose the same option, making it difficult for the learner to gauge whether their preference for that option is moderate or strong. Thus, choices from easy queries provide limited information about preference strength, contributing minimally to estimating $\theta ^ { * }$ . This intuition aligns with the online retailer example in section 1.

In the choice-decision-time estimator (theorem 3.1), each query is weighted by $\mathrm { m i n } _ { x ^ { \prime } \in \mathcal { X } _ { \mathrm { s a m p l e } } } \mathbb { E } \left[ t _ { x ^ { \prime } } \right]$ . The orange curves in fig. 2a show $\mathbb { E } \left[ t _ { x } \right]$ , but not the $\cdot _ { \mathrm { m i n } } ,$ operator for clarity. For each query $x \in \mathcal { X } _ { \mathrm { s a m p l e } }$ , comparing the orange and gray curves reveals that $\mathbb { E } \left[ t _ { x } \right]$ is generally higher than the choice-only weight, $\boldsymbol { a } ^ { 2 } \mathbb { V } \left[ c _ { x } \right]$ . However, the choice-decision-time estimator’s actual weight, $\mathrm { m i n } _ { x ^ { \prime } \in \mathcal { X } _ { \mathrm { s a m p l e } } } \mathbb { E } \left[ t _ { x ^ { \prime } } \right]$ , can vary with $\mathcal { X } _ { \mathrm { s a m p l e } }$ , and may be either larger or smaller than $\boldsymbol { a } ^ { 2 } \mathbb { V } \left[ c _ { x } \right]$ . For example, when most queries are hard, the choice-decision-time estimator’s weight may be smaller than some of the choice-only estimator’s weights, suggesting that decision times do not always enhance estimation. However, when most queries are easy, the choice-only estimator’s weights are close to zero, while the choice-decision-time estimator’s weight remains large. This demonstrates that incorporating decision times makes easy queries more informative, enhancing estimation.

Additionally, as the barrier $a$ increases, all curves shift upward. Intuitively, a higher barrier, indicating greater conservativeness in human decision-making, leads to longer decision times and more consistent choices, as discussed in fig. 1, providing more information for utility estimation.

# 3.3 Non-asymptotic concentration of the two estimators for utility difference estimation

In this section, we focus on the simpler problem of estimating the utility difference for a single query, without aggregating data from multiple queries. Comparing the non-asymptotic concentration bounds of both estimators, in this case, provides insights similar to those discussed in section 3.2. Extending this non-asymptotic analysis to the full estimation of the preference vector $\theta ^ { * }$ is left for future work.

Given a query $x \in \mathcal { X }$ , the task is to estimate the utility difference $u _ { x } : = x ^ { \top } \theta ^ { * }$ using the i.i.d. dataset $\{ ( c _ { x , s _ { x , i } } , t _ { x , s _ { x , i } } ) \} _ { i \in [ n _ { x } ] }$ . Applying the choice-decision-time estimator from eq. (3), we get the following estimate (for details, see appendix C.3.1), which estimates $u _ { x } / a$ rather than $u _ { x }$ :

$$
\widehat { u } _ { x , \mathrm { C H } , \mathrm { D T } } : = \frac { \sum _ { i = 1 } ^ { n _ { x } } c _ { x , s _ { x , i } } } { \sum _ { i = 1 } ^ { n _ { x } } t _ { x , s _ { x , i } } } .
$$

![](images/304bb4942eabb3e5f718dfd3835a5ca640c14d88909fac02a4e558620fcd95bb.jpg)  
Figure 2: This figure presents the key terms from our theoretical analyses, illustrating the different contributions of choices and decision times for utility estimation. These terms are functions of the utility difference $x ^ { \top } \theta ^ { * }$ and are plotted for two barrier values $a$ . (a) compares the terms $\mathbb { E } \left[ t _ { x } \right]$ and $\boldsymbol a ^ { 2 } \bar { \mathbb { V } } \left[ c _ { x } \right]$ in the asymptotic variances for the choice-decision-time estimator (orange, theorem 3.1) and the choice-only estimator (gray, theorem 3.2), respectively. This comparison shows that incorporating decision times makes easy queries more informative. Additionally, higher barrier values $a$ lead to more conservative decision-making, increasing informativeness for both choices and decision times. (b) compares the weights in the non-asymptotic concentration bounds (theorems 3.3 and 3.4), showing similar trends, though these terms may not be optimal due to proof techniques.

In contrast, applying the choice-only estimator from eq. (4), we get the following estimate (for details, see appendix C.3.2), which estimates $2 a u _ { x }$ rather than $u _ { x }$ :

$$
{ \widehat { u } } _ { x , { \mathrm { C H } } } : = \mu ^ { - 1 } \left( { \frac { 1 } { n _ { x } } } \sum _ { i = 1 } ^ { n _ { x } } { \frac { c _ { x , s _ { x , i } } + 1 } { 2 } } \right) ,
$$

where $( c _ { x , s _ { x , i } } + 1 ) / 2$ is the binary choice coded as 0 or 1, and $\mu ^ { - 1 } ( p ) : = \log { ( p / ( 1 - p ) ) }$ is the logit function (inverse of $\mu$ introduced in eq. (4)).

Notably, the choice-only estimator in eq. (6) aligns with the EZ-diffusion model’s drift estimator [67, eq. (5)]. Moreover, the estimators in Xiang Chiong et al. [73, eq. (6)] and Berlinghieri et al. [8, eq. (7)] combine elements of both estimators from eqs. (5) and (6). In section 5, we demonstrate that both estimators from Wagenmakers et al. [67, eq. (5)] and Xiang Chiong et al. [73, eq. (6)] are outperformed by our proposed estimator in eq. (3) for the full bandit problem.

Assuming the utility difference $u _ { x } \neq 0$ , the choice-decision-time estimator in eq. (5) satisfies the following non-asymptotic concentration bound, proven in appendix C.3.1:

Theorem 3.3 (Non-asymptotic concentration of $\widehat { u } _ { x , \mathrm { C H } , \mathrm { D T } } )$ . For each query $ { \boldsymbol { { x } } } \in  { \boldsymbol { { \chi } } }$ with $u _ { x } \neq 0$ , given an i.i.d. dataset $\left\{ \left( c _ { x , s _ { x , i } } , t _ { x , s _ { x , i } } \right) \right\} _ { i \in [ n _ { x } ] } ,$ for any $\epsilon \ > \ 0$ satisfying $\epsilon \ \leq$ $\operatorname* { m i n } \left\{ | u _ { x } | / ( \sqrt { 2 } a ) , \left( 1 + \sqrt { 2 } \right) a | u _ { x } | / \mathbb { E } \left[ t _ { x } \right] \right\}$ , the following holds:

$$
\begin{array} { r l } & { \mathbb { P } \left( \left| \widehat { u } _ { x , C H , D T } - \frac { u _ { x } } { a } \right| > \epsilon \right) \leq 4 \exp \left( - \left[ m _ { C H , D T } ^ { n o n - a s y m } \left( x ^ { \top } \theta ^ { * } \right) \right] ^ { 2 } n _ { x } \left[ \epsilon \cdot a \right] ^ { 2 } \right) , } \\ & { } \\ & { \overset { n o n - a s y m } { _ { C H , D T } } \left( x ^ { \top } \theta ^ { * } \right) : = \mathbb { E } \left[ t _ { x } \right] / \left[ \left( 2 + 2 \sqrt { 2 } \right) a \right] . } \end{array}
$$

In contrast, the choice-only estimator in eq. (6) has the following non-asymptotic concentration result, adapted from Jun et al. [31, theorem $5 ] ^ { 2 }$ :

Theorem 3.4 (Non-asymptotic concentration of $\widehat { u } _ { x , \mathrm { C H } } )$ ). For each query $x \in \mathcal { X }$ , given an i.i.d. dataset $\big \{ c _ { x , s _ { x , i } } \big \} _ { i \in [ n _ { x } ] } ,$ , for any positive $\epsilon < 0 . 3$ , if $\begin{array} { r } { { { \dot { n } } _ { x } } \ge 1 / \dot { \mu } ( 2 a u _ { x } ) \cdot \operatorname* { m a x } \{ 3 ^ { 2 } \log ( 6 e ) / \epsilon ^ { 2 } , 6 4 \log ( 3 ) / ( 1 - \sigma ) \} , } \end{array}$ $\epsilon ^ { 2 } / 0 . 3 ^ { 2 } ) \}$ , the following holds:

$$
\begin{array} { r } { \mathbb { P } \left( \left| \widehat { u } _ { x , C H } - 2 a u _ { x } \right| > \epsilon \right) \leq 6 \exp \left( - \left[ m _ { C H } ^ { n o n - a s y m } \left( x ^ { \top } \theta ^ { * } \right) \right] ^ { 2 } n _ { x } \left[ \epsilon / ( 2 a ) \right] ^ { 2 } \right) , } \end{array}
$$

where $m _ { C H } ^ { n o n - a s y m } \left( x ^ { \top } \theta ^ { * } \right) : = a \sqrt { \mathbb { V } \left[ c _ { x } \right] } / 2 . 4 .$

The weights mnCoHn,-DasT and $m _ { \mathrm { C H } } ^ { \mathrm { n o n - a s y m } } ( \cdot )$ from theorems 3.3 and 3.4, respectively, are functions of the utility difference $x ^ { \top } \theta ^ { * }$ for a fixed barrier $a$ . These weights determine how quickly the estimation error decays as the dataset size $n _ { x }$ grows, with larger weights indicating better estimation. While these weights may not be optimal due to our proof techniques, they still reveal the different contributions of choices and decision times to estimating $u _ { x }$ , similar to our asymptotic analysis in section 3.2. Figure 2b compares the weights for the choice-decision-time estimator (orange, $m _ { \mathrm { C H , D T } } ^ { \mathrm { n o n - a s y m } } ( \cdot )$ ) and the choice-only estimator (gray, mnCoHn-a $m _ { \mathrm { C H } } ^ { \mathrm { n o n - a s y m } } ( \cdot ) )$ . As shown, for hard queries, the choice-decisiontime estimator’s weights may be smaller. However, for easy queries, the choice-only estimator’s weights are close to zero, while the choice-decision-time estimator’s weight remains relatively large, reinforcing that decision times enhance estimation by making easy queries more informative.

In summary, both our asymptotic (section 3.2) and non-asymptotic (section 3.3) analyses show that the choice-decision-time estimator extracts more information from easy queries, while the choice-only estimator might perform better when most queries are hard. This aligns with the empirical findings of Clithero [16] and will be further supported by our empirical results in section 5.1. To illustrate, consider a teacher trying to identify the top student through two-choice questions. If the questions are easy and all students answer correctly, identifying the top performer is challenging. The teacher can either: (1) ask harder questions to gain more information from choices, or (2) keep the questions easy but use response times (i.e., how quickly students finish) to reveal additional insights.

In fixed-budget best-arm identification, our choice-decision-time estimator’s ability to extract more information from easy queries is crucial. A bandit learner like GSE [3] strategically samples queries, updates its estimate of $\theta ^ { * }$ , and eliminates lower-utility arms. Unlike the choice-only estimator, our approach extracts more information from easy queries, which take less time (i.e., fewer resources) to answer, providing better ‘bang per buck’ (information per resource) [4]. Moreover, since the learner doesn’t know $\theta ^ { * }$ in advance, it cannot selectively sample only hard queries to exploit the strengths of the choice-only estimator. Next, we will integrate both estimators into a bandit learning algorithm.

# 4 Interactive learning algorithm

We introduce the Generalized Successive Elimination (GSE) algorithm [1, 3, 75] for fixed-budget best-arm identification in preference-based linear bandits, and outline the key options for each GSE component, which we empirically compare in section 5.

The pseudo-code for GSE is shown in algorithm 1. The algorithm uses a hyperparameter $\eta$ to control the number of phases, the budget allocation per phase, and the number of arms eliminated in each phase. GSE divides the total budget $B$ evenly into phases and reserves a buffer, sized by another hyperparameter $B _ { \mathrm { b u f f } }$ , to prevent over-consuming resources within each phase (line 4). In each phase, GSE computes an experimental design, a probability distribution $\lambda$ over the query space to determine which queries to sample. We consider two designs: the transductive design [24], $\lambda _ { \mathrm { t r a n s } }$ (line 5), and the hard-query design [31], $\lambda _ { \mathrm { h a r d } }$ (line 6). Both designs minimize the worst-case variance of utility differences between the surviving arms. The transductive design treats all queries equally to achieve this, while the hard-query design focuses more on sampling hard queries to leverage the choice-only estimator’s advantage in extracting information from hard queries, as discussed in section 3. GSE then randomly samples queries according to the design $\lambda _ { s }$ (line 7). After the phase’s budget is exhausted, GSE estimates $\theta ^ { * }$ using either the choice-decision-time estimator $\widehat { \theta } _ { \mathrm { C H , D T } }$ (line 8) or the choice-only estimator $\widehat { \theta } _ { \mathrm { C H } }$ (line 9). Arms with low estimated utilities are eliminabted, and this process repeats phase-by-phaseb until only one arm remains in ${ \mathcal { Z } } _ { S + 1 }$ , which GSE then recommends.

The key difference between algorithm 1 and previous GSE algorithms [1, 3, 75] is that, in our setting, queries consume random response times, which are unknown to the learner. Prior work assumes fixed resource consumption per query and uses deterministic rounding methods [3, 24] to pre-allocate queries to be sampled. This approach is unsuitable for our case with random resource usage. Instead, we adopt a random sampling procedure [13, 61] in line 7 to allocate queries based on the design. The random resource usage also necessitates careful tuning of both the elimination parameter $\eta$ , to balance data collection and arm elimination, and the buffer size $B _ { \mathrm { b u f f } }$ , to prevent over-consuming resources during each phase. In our empirical study (section 5.2), we manually tune both parameters. Our results show that the choice of $\eta$ impacts performance, unlike prior studies, which typically set $\eta = 2$ by default [3, section 3]. Further theoretical analysis is needed to fully understand the algorithm’s behavior and optimize both the elimination parameter and the buffer size.

1: Input: Arm space $\mathcal { Z }$ , query space $\chi$ , non-decision time $t _ { \mathrm { n o n d e c } }$ , and total budget $B$ .   
2: Hyperparameters: Elimination parameter $\eta$ and buffer size $B _ { \mathrm { b u f f } }$ .   
3: Initialization: $\mathcal { Z } _ { 1 }  \mathcal { Z }$ , $s \gets 1$ .   
4: for each phase $s = 1 , \dots , S : = \left\lceil \log _ { \eta } | \mathcal { Z } | \right\rceil$ with the budget $\begin{array} { r } { B _ { s } : = \frac { B } { S } - B _ { \mathrm { b u f f } } \circ } \end{array}$ do   
5: $\begin{array} { r } { \lambda _ { s } : = \lambda _ { \mathrm { t r a n s } , s }  \arg \operatorname* { m i n } _ { \lambda \in \mathbb { A } ^ { | \mathcal { X } | } } \operatorname* { m a x } _ { z \neq z ^ { \prime } \in \mathcal { Z } _ { s } } \| z - z ^ { \prime } \| _ { ( \Gamma ^ { - } \setminus \Gamma _ { - , - r \pi } \top \setminus \Gamma ^ { - 1 } } ^ { 2 } . } \end{array}$   
6: Design 2. λs := λhard,s ← arg minλ ▲|X| maxz̸=z′∈Zs ∥z − z′∥2   
( µ˙ (x⊤θs 1) λxxx⊤)−1   
7: Sample queries $x _ { j } \sim \lambda _ { s }$ and stop at $J _ { s }$ if $\begin{array} { r } { \sum _ { j = 1 } ^ { J _ { s } - 1 } t _ { \mathrm { R T } , x _ { j } , j } \le B _ { s } } \end{array}$ and $\begin{array} { r } { \sum _ { j = 1 } ^ { J _ { s } } t _ { \mathrm { R } \mathrm { T } , x _ { j } , j } > B _ { s } } \end{array}$ .   
8: Estimate 1. $\widehat { \theta } _ { s } : = \widehat { \theta } _ { \mathrm { C H , D T } , s } \gets$ apply eq. (3) to all the $J _ { s }$ samples.   
9: Estimate 2. $\widehat { \theta } _ { s } : = \widehat { \theta } _ { \mathrm { C H } , s } \gets \$ apply eq. (4) to all the $J _ { s }$ samples.   
10: Update $\mathcal { Z } _ { s + 1 } \gets \mathrm { T o p - } \left\lceil \frac { | \mathcal { Z } _ { s } | } { \eta } \right\rceil$ arms in $\mathcal { Z } _ { s }$ , ranked by the estimated utility $z ^ { \top } \widehat { \theta } _ { s }$ .

# 5 Empirical results

This section empirically compares the GSE variations introduced in section 4: (1) $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , D T } } )$ : Transductive design with the choice-decision-time estimator. (2) $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H } } )$ : Transductivebdesign with the choice-only estimator. (3) $( \lambda _ { \mathrm { h a r d } } , \widehat { \theta } _ { \mathrm { C H } } )$ : Hard-query design with tbhe choice-only estimator.

# 5.1 Estimation performance on synthetic data

We benchmark the estimation performance of these GSE variations using the “sphere” synthetic problem from the linear bandit literature [20, 42, 61]. In this problem, the arm space $\mathcal { Z } \subset \{ z \in$ $\mathbf { \hat { R } ^ { 5 } } \colon \| z \| _ { 2 } = 1 \}$ contains 10 randomly generated arms. To define the true preference vector $\theta ^ { * }$ , we select the two arms $z$ and $z ^ { \prime }$ that are closest in direction, i.e., $( z , z ^ { \prime } ) \in \arg \operatorname* { m a x } _ { z , z ^ { \prime } \in \mathcal { Z } } z ^ { \top } z ^ { \prime }$ , and set $\theta ^ { * } = z + 0 . 0 1 ( z ^ { \prime } - z )$ . In this way, $z$ is the best arm. The query space is ${ \mathcal { X } } : = \{ z - z ^ { \prime } \colon z \in { \mathcal { Z } } \}$ .

Estimation performance, as discussed in section 3, depends on the utility difference $x ^ { \top } \theta ^ { * }$ and the barrier $a$ . To adjust the utility differences, we scale each query by scaling each arm $z$ to $c _ { \mathcal { Z } } \cdot z$ . We vary $a$ over a range of typical values from the psychology literature [16, 71]. For each $( c _ { \mathcal { Z } } , a )$ pair, the system generates 10 random problem instances and runs 200 repeated simulations per instance. In each simulation, the GSE variations sample 50 queries without considering the response time budget and then compute $\widehat { \theta }$ . Performance is measured by $\mathbb { P } [ \arg \operatorname* { m a x } _ { z \in \mathcal { Z } } z ^ { \top } \widehat { \theta } \neq \overline { { z } } ^ { * } ]$ , reflecting the best-arm identification goal dbiscussed in section 2. To focus purely on estimatibon, we allow $\lambda _ { \mathrm { h a r d } }$ access to the true $\theta ^ { * }$ , enabling it to perfectly compute the weights $\dot { \mu } ( \dot { x } ^ { \top } \theta ^ { * } )$ used in line 6 of algorithm 1.

As shown in fig. 3a, when fixing the barrier $a$ and examining the vertical line, we observe that the choice-only estimator with the transductive design performs well for small $c _ { \mathcal { Z } }$ (hard queries). However, as $c _ { \mathcal { Z } }$ increases and queries become easier, performance declines, even though larger $c _ { \mathcal { Z } }$ generally makes best-arm identification easier. This decline, illustrated by the dark curved band, aligns with the insights from section 3, that choices from easy queries provide limited information.

In Figure 3b, for moderate $c _ { \mathcal { Z } }$ , the choice-only estimator with the hard-query design outperforms the transductive design (fig. 3a), showing that focusing on harder queries improves estimation. The lower dark band in fig. 3b compared to fig. 3a shows that focusing on hard queries improves estimation when most queries are easy. However, as $c _ { \mathcal { Z } }$ becomes too large, performance declines, likely because many weights $\dot { \mu } ( x ^ { \top } \theta ^ { * } )$ approach zero, preventing informative queries from being sampled. This advantage of the hard-query design relies on perfect knowledge of $\theta ^ { * }$ and the same resource consumption across all queries. In practice, where $\theta ^ { * }$ is unknown and hard queries require longer response times, the hard-query design can be outperformed by the transductive design, as shown in the next section.

In contrast, fig. 3c shows that the choice-decision-time estimator consistently outperforms the choiceonly estimators in both the transductive and hard-query designs. The choice-decision-time estimator’s performance improves as $c _ { \mathcal { Z } }$ increases (i.e., queries become easier), confirming the theoretical insights from section 3 that incorporating decision times make easy queries more informative. Performance also improves with a higher barrier $a$ , supporting the insights discussed at the end of section 3.2.

![](images/aa321a2e0e7128dfea1f87018f2c9579589f2aa5e7f0e6709517c1a0ffac7431.jpg)  
Figure 3: Three heatmaps show the estimation error probabilities, $\mathbb { P } [ \arg \operatorname* { m a x } _ { z \in \mathcal { Z } } z ^ { \top } \widehat { \theta } \neq z ^ { * } ]$ , for the three GSE variations as functions of the arm scaling factor $c _ { \mathcal { Z } }$ and barrier $a$ . Darkerbcolors indicate better estimation performance. In (a) and (b), the choice-only estimator $\widehat { \theta } _ { \mathrm { C H } }$ with both the transductive design $( \lambda _ { \mathrm { { t r a n s } } } )$ and the hard-query design $( \lambda _ { \mathrm { h a r d } } )$ struggles as $c _ { \mathcal { Z } }$ increasebs (i.e., queries become easier), suggesting that choices from easy queries provide limited information. In contrast, in (c), the choicedecision-time estimator $\widehat { \theta } _ { \mathrm { C H , D T } }$ with transductive design $( \lambda _ { \mathrm { t r a n s } } )$ consistently achieves better estimation across all $c _ { \mathcal { Z } }$ values, indbicating that decision times make easy queries more informative.

# 5.2 Fixed-budget best-arm identification performance on real datasets

This section compares the bandit performance of six GSE variations. The first three are as previously defined: $( \lambda _ { \mathrm { t r a n s } } , \bar { \widehat { \theta } } _ { \mathrm { C H , D T } } )$ , $( \lambda _ { \mathrm { t r a n s } } ,  { \widehat { \theta } } _ { \mathrm { C H } } )$ , and $( \lambda _ { \mathrm { h a r d } } , \widehat { \theta } _ { \mathrm { C H } } )$ .

The 4th GSE variation, $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , \mathbb { R } T } } )$ , evaluates the performance of the choice-decision-time estimator when the non-decision timeb $t _ { \mathrm { n o n d e c } }$ is unknown. The estimator, $\widehat { \theta } _ { \mathrm { C H , \mathbb { R } T } }$ , is identical to the original choice-decision-time estimator from Eq. (3), but with response timebs used in place of decision times.

The 5th GSE variation, $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , l o g i t } } )$ , is based on Wagenmakers et al. [67, eq. (5)], which states that $x ^ { \top } \cdot ( 2 a \theta ^ { * } ) = \mu ^ { - 1 } ( \mathbb { P } [ c _ { x } = 1 ] )$ , where $\mu ^ { - 1 } ( p ) : = \log { ( p / ( 1 - p ) ) }$ . By incorporating our linear utility structure, we obtain the following choice-only estimator $\widehat { \theta } _ { \mathrm { C H , l o g i t } }$ :

$$
\widehat { \theta } _ { \mathrm { C H , l o g i t } } : = \left( \sum _ { x \in \mathcal { X } _ { \mathrm { s a m p l e } } } n _ { x } x x ^ { \top } \right) ^ { - 1 } \sum _ { x \in \mathcal { X } _ { \mathrm { s a m p l e } } } n _ { x } x \cdot \mu ^ { - 1 } \left( \widehat { \mathfrak { C } } _ { x } \right) ,
$$

where $\begin{array} { r } { \widehat { \mathfrak { C } } _ { x } : = \frac { 1 } { n _ { x } } \sum _ { i = 1 } ^ { n _ { x } } \frac { 1 } { 2 } \left( c _ { x , s _ { x , i } } + 1 \right) } \end{array}$ is the empirical mean of the binary choices coded as 0 or 1.

The 6th GSE variation, $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , D T , l o g i t } } )$ , is based on Xiang Chiong et al. [73, eq. (6)], which states that $x ^ { \top } \theta ^ { * } = \operatorname { s g n } \left( c _ { x } \right) \sqrt { \mathbb { E } \left[ c _ { x } \right] / \mathbb { E } \left[ t _ { x } \right] \cdot 0 . 5 \mu ^ { - 1 } \left( \mathbb { P } \left[ c _ { x } = 1 \right] \right) } .$ . This identity forms the foundation of the estimator in Berlinghieri et al. [8, eq. (7)]. By incorporating our linear utility structure, we obtain the following choice-decision-time estimator $\widehat { \theta } _ { \mathrm { C H , D T , l o g i t } } ^ { \mathrm { ~ - ~ } }$ :

$$
\widehat { \theta } _ { \mathrm { C H , D T , l o g i t } } : = \left( \sum _ { x \in \mathcal { X } _ { \mathrm { s a m p l e } } } n _ { x } x x ^ { \top } \right) ^ { - 1 } \sum _ { x \in \mathcal { X } _ { \mathrm { s a m p l e } } } n _ { x } x \cdot \operatorname { s g n } \left( c _ { x } \right) \sqrt { \frac { \mathbb { E } \left[ c _ { x } \right] } { \mathbb { E } \left[ t _ { x } \right] } } \cdot \frac { 1 } { 2 } \mu ^ { - 1 } \left( \widehat { \mathfrak { C } } _ { x } \right) .
$$

We evaluate the six GSE variations by simulating their performance on bandit instances constructed from three real-world datasets of human choices and response times. The first dataset, food-risk with choices (-1 or 1) [57], contains choices and response times from 42 participants answering queries comparing two sets of food items. For each participant, we identified the dEZDM parameters and built a bandit instance with $\mathcal { Z } \subset \mathbb { R } ^ { 5 }$ , where $| \bar { \mathcal { Z } } | \in [ \bar { 3 } 1 , 9 5 ]$ , and ${ \mathcal { X } } : = \{ z - z ^ { \prime } \colon z \in { \bar { \mathcal { Z } } } \}$ . The second dataset, snack with choices (yes or no) [16], contains choices and response times from 31 participants comparing one snack item to a fixed reference snack. For each participant, we built a bandit instance with $\mathcal { Z } \subset \mathbb { R } ^ { 1 7 }$ , where $| \mathcal { Z } | = 1 7$ , and ${ \mathcal { X } } : = \{ z - \mathbf { 0 } : z \in { \mathcal { Z } } \}$ . The third dataset, snack with choices (-1 or 1) [39], contains choices and response times from 39 participants comparing two snack items. For each participant, we built a bandit instance with $\mathcal { Z } \subset \mathbb { R } ^ { 2 1 }$ , $| \mathcal { Z } | \overset { \_ } { = } 2 1$ , and $\mathbf { \bar { \mathcal { X } } } : = \left\{ z - z ^ { \prime } \colon z \in \mathcal { Z } \right\}$ . Details on data processing, tuning the elimination parameter $\eta$ and buffer size $B _ { \mathrm { b u f f } }$ as defined in algorithm 1, and experimental procedures are provided in appendix $\mathrm { \Delta D }$ .

![](images/5bf6346e6cd391ebe37b59725374637e4b237ff1126f39fb3083100b93ae24b3.jpg)  
Figure 4: This figure shows violin plots (with overlaid box plots) for datasets (a), (b), and (c), showing the distribution of best-arm identification error probabilities, $\mathbb { P } \left[ \widehat { \boldsymbol { z } } \neq \boldsymbol { z } ^ { * } \right]$ , for all bandit instances across six GSE variations and two budgets. The box plots follow the cbonvention of the matplotlib Python package. For each GSE variation and budget, the horizontal line in the middle of the box represents the median of the error probabilities across all bandit instances. Each error probability is averaged over 300 repeated simulations under different random seeds. The box’s upper and lower borders represent the third and first quartiles, respectively, with whiskers extending to the farthest points within $1 . 5 \times$ the interquartile range. Flier points indicate outliers beyond the whiskers.

Key results for the three domains are shown in fig. 4, with full results in appendix D. First, $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , D T } } )$ consistently outperforms $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H } } )$ , demonstrating the benefit of incorporating decisiobn times. Second, both of these GSE variatbions outperform $( \lambda _ { \mathrm { h a r d } } , \widehat { \theta } _ { \mathrm { C H } } )$ , which, as discussed in section 5.1, suffers from (1) relying on $\widehat { \theta }$ for query selection, which is pbrone to estimation errors, and (2) favoring hard queries with longebr response times, reducing the total number of queries sampled within a fixed budget. Third, $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , D T } } )$ performs similarly to $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , \mathbb { R } T } } )$ , indicating that not knowing the non-decision time bhas minimal impact. Finally, $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , l o g i t } } )$ [67] and $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , D T , l o g i t } } )$ [73] do not perform as consistently well as $( \lambda _ { \mathrm { t r a n s } } , \widehat { \theta } _ { \mathrm { C H , D T } } )$ , undebrscoring the effectivenesbs of our proposed choice-decision-time estimator from eq. (3).

# 6 Conclusion and future work

This work leverages human response times to enhance fixed-budget best-arm identification in preference-based linear bandits. We proposed a utility estimator that incorporates both choices and response times. Our theoretical and empirical analyses show that response times make easy queries more informative. When integrated into a bandit algorithm, incorporating response times consistently improved performance across simulations based on three real-world datasets.

A limitation of this work is that reliable response time data requires participants to stay focused [45], which can be challenging in crowdsourcing environments. Future work could incorporate eye movements into the DDM framework, as in attentional DDMs [26, 38, 39, 57, 76], to detect attention lapses and filter unreliable data. Additionally, while response times are effective for easy queries, they may be less so for hard ones. Future research could develop algorithms that adaptively decide when to incorporate response times. Another direction is to remove the assumption of known non-decision times by estimating them from data, with approaches like those proposed by Wagenmakers et al. [67].