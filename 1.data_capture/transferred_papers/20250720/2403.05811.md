# Statistical Efficiency of Distributional Temporal Difference Learning and Freedman's Inequality in Hilbert Spaces

Yang Peng∗

Liangyu Zhang†

Zhihua Zhang‡

# Abstract

Distributional reinforcement learning (DRL) has achieved empirical success in various domains. One core task in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta ^ { \pi }$ for a given policy $\pi$ . The distributional temporal difference learning has been accordingly proposed, which is an extension of the temporal difference learning (TD) in the classic RL area. In the tabular case, Rowland et al. [2018] and Rowland et al. [2024a] proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference learning (CTD) and quantile temporal difference learning (QTD), respectively. In this paper, we go a step further and analyze the finitesample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric distributional temporal difference learning (NTD). For a $\gamma$ -discounted infinite-horizon tabular Markov decision process, we show that for NTD we need $\begin{array} { r } { \widetilde O \left( \frac { 1 } { \varepsilon ^ { 2 p } ( 1 - \gamma ) ^ { 2 p + 1 } } \right) } \end{array}$ iterations to achieve an $\varepsilon$ -optimal estimator with high probabili ye, when the estimation error is measured by the $p$ -Wasserstein distance. This sample complexity bound is minimax optimal up to logarithmic factors in the case of the 1-Wasserstein distance. To achieve this, we establish a novel Freedman’s inequality in Hilbert spaces, which would be of independent interest. In addition, we revisit CTD, showing that the same non-asymptotic convergence bounds hold for CTD in the case of the $p$ -Wasserstein distance for $p \geq 1$ .

# 1 Introduction

In high-stake applications of reinforcement learning (RL), such as healthcare [Lavori and Dawson, 2004, Böck et al., 2022] and finance[Ghysels et al., 2005], only considering the mean of returns is insufficient. It is necessary to take risk and uncertainties into consideration. Distributional reinforcement learning (DRL) Morimura et al. [2010], Bellemare et al. [2017, 2023] addresses such issues by modeling the complete distribution of returns instead of their expectations.

In the field of DRL, one of the most fundamental tasks is to estimate the return distribution $\eta ^ { \pi }$ for a given policy $\pi$ , which is referred to as distributional policy evaluation. Distributional temporal difference learning (TD) is probably the most widely-used approach for solving the distributional policy evaluation problem. A key aspect of implementing a distributional TD algorithm is how to represent the return distribution, an infinite-dimensional object, via a computationally feasible finite-dimensional parametrization. This has led to the development of two special instances of distributional TD: categorical temporal difference learning (CTD) [Bellemare et al., 2017] and quantile temporal difference learning (QTD) [Dabney et al., 2018]. These algorithms provide computationally tractable parametrizations and updating schemes of the return distribution.

Previous theoretical works have primarily focused on the asymptotic behaviors of distributional TD. In particular, Rowland et al. [2018] and Rowland et al. [2024a] showed the asymptotic convergences of CTD and QTD in the tabular case, respectively. A natural question arises: can we depict the statistical efficiency of distributional TD by non-asymptotic results similar to the classic TD algorithm [Li et al., 2024]?

# 1.1 Contributions

In this paper, we manage to answer the above question affirmatively in the synchronous setting [Kakade, 2003, Kearns et al., 2002]. Firstly, we introduce non-parametric distributional temporal difference learning (NTD) in Section 3, which is not practical but aids theoretical understanding. We show that $\begin{array} { r } { \widetilde O \left( \frac { 1 } { \varepsilon ^ { 2 p } ( 1 - \gamma ) ^ { 2 p + 1 } } \right) } \end{array}$ 4 iterations are sufficient to yield an estimator $\hat { \eta } ^ { \pi }$ , such that the $p$ -Wasserstein meetric between $\hat { \eta } ^ { \pi }$ and $\eta ^ { \pi }$ is less than $\varepsilon$ with high probability (Theorem 4.1). This bound is minimax optimal (Theorem B.1) in the 1-Wasserstein metric case, if we neglect all logarithmic terms. Next, we revisit the more practical CTD, and show that, in terms of the $p$ -Wasserstein metric, CTD and NTD have the same non-asymptotic convergence bounds (Theorem 4.2). It is worth pointing out that to attain such tight bounds in Theorem 4.1, we establish a Freedman’s inequality in Hilbert spaces (Theorem A.2). We would believe it is of independent interest beyond the current work.

# 1.2 Related Work

Non-asymptotic results of DRL. Recently, there has been an emergence of work focusing on finite-sample/iteration results of the distributional policy evaluations.

Wu et al. [2023] studied the offline distributional policy evaluation problem. They solved the problem via fitted likelihood estimation (FLE) inspired by the classic offline policy evaluation algorithm fitted Q evaluation (FQE), and provided a generalization bound in the $p$ -Wasserstein metric case.

Zhang et al. [2023] proposed to solve distributional policy evaluation by the model-based approach and derived corresponding sample complexity bounds, namely $\begin{array} { r } { \widetilde O \left( \frac { 1 } { \varepsilon ^ { 2 p } ( 1 - \gamma ) ^ { 2 p + 2 } } \right) } \end{array}$ in the $p$ -Wasserstein metric case, and $\begin{array} { r } { \widetilde O \left( \frac { 1 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 4 } } \right) } \end{array}$ in both the Kolmogorov-Smirnov metric and total variation metric under different  enditions. Rowland et al. [2024b] proposed direct categorical fixedpoint computation (DCFP), a model-based version of CTD, in which they constructed the estimator by solving a linear system directly instead of performing an iterative algorithm. They showed that the sample complexity of DCFP is $\begin{array} { r } { \widetilde O \left( \frac { 1 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 3 } } \right) } \end{array}$ in the 1-Wasserstein metric case by introducing the novel stochastic categorical CDFeBellman operator and equation. Their result matches the minimax lower bound (up to logarithmic factors) $\begin{array} { r } { \widetilde \Omega \left( \frac { 1 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 3 } } \right) } \end{array}$ proposed in [Zhang et al., 2023], which implies that learning the full return distributionecan be as sample-efficient as learning just its expectation. It’s worth noting that the algorithms analyzed in both [Zhang et al., 2023] and [Rowland et al., 2024b] are model-based, hence they are less similar to practical algorithms. While distributional TD analyzed in this paper, as a model-free method, is more practical, and also involves a more complicated theoretical analysis.

Böck and Heitzinger [2022] also considered model-free method. They proposed speedy categorical policy evaluation (SCPE), which can be regarded as CTD with an additional acceleration term. They showed that the sample complexity of SCPE is $\begin{array} { r } { \widetilde O \left( \frac { 1 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 4 } } \right) } \end{array}$ in the 1-Wasserstein metric case. Compared to [Böck and Heitzinger, 2022], our woerk shows that even if we do not introduce any acceleration techniques to the original CTD algorithm, it is still possible to attain the near-minimax optimal sample complexity bounds. Thus, we give sharper bounds based on a simpler algorithm.

Table 1 gives more detailed comparisons of sample complexity with the previous work in the 1- Wasserstein metric. Note that solving distributional policy evaluation can also address the traditional policy evaluation task by taking expectation of the return distribution estimator. And the supreme 1-Wasserstein metric error of the return distribution estimator is not smaller than the $\ell _ { \infty }$ error of the induced value function estimator (see the proof of Theorem B.1 in Appendix B), we have also listed the sample complexity of the policy evaluation task in Table 1 for comparison.

Table 1. Sample complexity of algorithms for solving policy evaluation (PE) in the $\ell _ { \infty }$ norm, and distributional policy evaluation (DPE) in the supreme 1-Wasserstein metric.   

<html><body><table><tr><td></td><td colspan="2"> Sample Complexity</td><td>Algorithms</td><td>Task</td></tr><tr><td rowspan="2">[Gheshlaghi Azar etal., 2013]</td><td></td><td>1 ∈²(1-γ)³</td><td>Model-based</td><td>PE</td></tr><tr><td>0 1</td><td></td><td>TD (Model-free)</td><td>PE</td></tr><tr><td>[Li et al., 2024] [Rowland et al., 2018]</td><td></td><td>e²(1-γ)3 Asymptotic</td><td>CTD (Model-free)</td><td>DPE</td></tr><tr><td>[Rowland et al., 2024a]</td><td colspan="2">Asymptotic</td><td>QTD (Model-free)</td><td>DPE</td></tr><tr><td>[Rowland et al., 2024b]</td><td>0</td><td>2(1-2)3</td><td>DCFP (Model-based)</td><td>DPE</td></tr><tr><td>[Bock and Heitzinger, 2022]</td><td></td><td>2（1-）4</td><td>SCPE (Model-free)</td><td>DPE</td></tr><tr><td>Our Work</td><td></td><td>22（1-</td><td>CTD (Model-free)</td><td>DPE</td></tr></table></body></html>

Freedman’s inequality. Freedman’s inequality was originally proposed in [Freedman, 1975]. It can be viewed as a Bernstein’s inequality for martingales, which is crucial for analyzing stochastic approximation algorithms. Tropp [2011] generalized Freedman’s inequality to matrix martingales. And Talebi et al. [2022] established Freedman inequalities for martingales in the setting of noncommutative probability spaces. To the best of our knowledge, we are the first to present a concrete version of Freedman’s inequality in Hilbert spaces.

The remainder of this paper is organized as follows. In Section 2, we introduce some background of DRL and state Freedman’s inequality in Hilbert spaces. In Section 3, we revisit distributional TD and propose NTD for further theoretical analysis. In Section 4, we analyze the non-asymptotic convergence bounds of NTD and CTD. Section 5 presents proof outlines of our theoretical results, and Section 6 concludes our work. We put the detailed results with Freedman’s inequality in Hilbert spaces in Appendix A, and the minimax lower bound of the distributional policy evaluation task in Appendix B.

# 2 Background

An infinite-horizon tabular Markov decision process (MDP) is defined by a 5-tuple $\begin{array} { l l } { M } & { = } \end{array}$ $\langle S , \mathcal { A } , \mathcal { P } _ { R } , P , \gamma \rangle$ , where $s$ represents a finite state space, $\mathcal { A }$ a finite action space, $\mathcal { P } _ { R }$ the distribution of rewards, $P$ the transition dynamics, i.e., $\mathcal { P } _ { R } ( \cdot | s , a ) \in \Delta \left( [ 0 , 1 ] \right) , P ( \cdot | s , a ) \in \Delta \left( \mathcal { S } \right)$ for any state action pair $( s , a ) \in \mathcal S \times \mathcal A$ , and $\gamma \in ( 0 , 1 )$ a discount factor. Here we use $\Delta ( \cdot )$ to represent the set of all probability distributions over some set. Given a policy $\pi \colon S  \Delta ( { \mathcal { A } } )$ and an initial state $s _ { 0 } = s \in \mathcal { S }$ , a random trajectory $\{ ( s _ { t } , a _ { t } , t _ { t } ) _ { t = 0 } ^ { \infty } \}$ can be sampled from $M$ : at $\mid s _ { t } \sim \pi ( \cdot \mid s _ { t } )$ , $\begin{array} { r } { r _ { t } \mid ( s _ { t } , a _ { t } ) \sim \mathcal { P } _ { R } ( \cdot \mid s _ { t } , a _ { t } ) , s _ { t + 1 } \mid ( s _ { t } , a _ { t } ) \sim P ( \cdot \mid s _ { t } , a _ { t } ) } \end{array}$ for any $t \in \mathbb { N }$ . Given a trajectory, we define the return by $\begin{array} { r } { G ^ { \pi } ( s ) : = \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } r _ { t } \in \left\lceil 0 , \frac { 1 } { 1 - \gamma } \right\rceil } \end{array}$ . We denote return distribution $\eta ^ { \pi } ( s )$ as the probability distribution of $G ^ { \pi } ( s )$ , and $\eta ^ { \pi } : = \bar { ( \eta ^ { \pi } ( s ) ) } _ { s \in \mathcal { S } } ^ { - }$ . The expected return $V ^ { \pi } ( s ) = \mathbb { E } G ^ { \pi } ( s )$ is the value function in the traditional RL setting.

# 2.1 Distributional Bellman Equation and Operator

Recall that the classic policy evaluation aims at computing the value functions $V ^ { \pi }$ . It is known that $V ^ { \pi } = \left( V ^ { \pi } ( s ) \right) _ { s \in { \cal S } }$ satisfy the Bellman equation. That is, for any $s \in \mathcal S$ ,

$$
V ^ { \pi } ( s ) = \left[ T ^ { \pi } ( V ^ { \pi } ) \right] ( s ) = \mathbb { E } _ { a \sim \pi ( \cdot \vert s ) , r \sim \mathcal { P } _ { R } ( \cdot \vert s , a ) , s ^ { \prime } \sim P ( \cdot \vert s , a ) } \left[ r + \gamma V ^ { \pi } ( s ^ { \prime } ) \right] .
$$

The operator $T ^ { \pi } \colon \mathbb { R } ^ { s }  \mathbb { R } ^ { s }$ is called the Bellman operator, and $V ^ { \pi }$ is a fixed point of $T ^ { \pi }$ .

The task of distribution policy evaluation is finding $\eta ^ { \pi }$ given some fixed policy $\pi . ~ \eta ^ { \pi }$ satisfies a distributional version of the Bellman equation (1). That is, for any $s \in \mathcal S$ ,

$$
\begin{array} { r } { \eta ^ { \pi } ( s ) = \left( \mathcal { T } ^ { \pi } \eta ^ { \pi } \right) ( s ) = \mathbb { E } _ { a \sim \pi ( \cdot | s ) , r \sim \mathcal { P } _ { R } ( \cdot | s , a ) , s ^ { \prime } \sim P ( \cdot | s , a ) } \left[ ( b _ { r , \gamma } ) _ { \# } \eta ^ { \pi } ( s ^ { \prime } ) \right] , } \end{array}
$$

where $b _ { r , \gamma } \colon \mathbb { R } \to  { \mathbb { R } }$ is an affine function defined by $b _ { r , \gamma } ( x ) = r + \gamma x$ . And $f _ { \# } \mu$ is the push forward measure of $\mu$ through any function $f : \mathbb { R } \to \mathbb { R }$ , so that $f _ { \# } \mu ( A ) = \mu ( f ^ { - 1 } ( A ) )$ for any Borel set $A$ , where $f ^ { - 1 } ( A ) : = \{ x \colon f ( x ) \in A \}$ . The operator $\begin{array} { r } { T ^ { \pi } \colon \Delta \left( \left[ 0 , \frac { 1 } { 1 - \gamma } \right] \right) ^ { s } \to \Delta \left( \left[ 0 , \frac { 1 } { 1 - \gamma } \right] \right) ^ { s } } \end{array}$ is known as the distributional Bellman operator, and $\eta ^ { \pi }$ is a fixed point of ${ \vec { \tau } } ^ { \pi }$ . For notational simplicity, we denote $\Delta \left( \left[ 0 , \frac { 1 } { 1 - \gamma } \right] \right)$ as $\mathcal { P }$ from now on.

# 2.2 ${ \mathcal { T } } ^ { \pi }$ as Contraction in $\mathcal { P }$

A key property of the Bellman operator $T ^ { \pi }$ is that it is a $\gamma$ -contraction w.r.t. the supreme norm (i.e. $\ell _ { \infty }$ norm). However, before we can properly discuss the contraction properties of ${ \mathcal { T } } ^ { \pi }$ , we need to specify a metric $d$ on $\mathcal { P }$ . And for any metric $d$ on $\mathcal { P }$ , we denote $\bar { d }$ as the corresponding supreme metric on $\mathcal { P } ^ { s }$ , i.e., $\begin{array} { r } { \bar { d } \left( \eta , \eta ^ { \prime } \right) : = \operatorname* { m a x } _ { s \in \mathcal { S } } d \left( \eta ( s ) , \eta ^ { \prime } ( s ) \right) } \end{array}$ for any $\eta , \eta ^ { \prime } \in \mathcal { P } ^ { S }$ .

Suppose $\mu$ and $\nu$ are two probability distributions on $\mathbb { R }$ with finite $p$ -moments for $\begin{array} { r l r } { p } & { { } \in } & { [ 1 , \infty ] } \end{array}$ . The $p$ -Wasserstein metric between $\mu$ and $\nu$ is defined as $W _ { p } ( \mu , \nu ) \quad : =$ $\begin{array} { r } { \left( \operatorname* { i n f } _ { \kappa \in \Gamma ( \mu , \nu ) } \int _ { \mathbb { R } ^ { 2 } } | x - y | ^ { p } \kappa ( d x , d y ) \right) ^ { 1 / p } } \end{array}$ . Each element $\kappa \in \Gamma ( \mu , \nu )$ is a coupling of $\mu$ and $\nu$ , i.e., a joint distribution on $\mathbb { R } ^ { 2 }$ with prescribed marginals $\mu$ and $\nu$ on each “axis.” When $p = 1$ we have $\begin{array} { r } { \dot { W _ { 1 } } ( \mu , \nu ) = \int _ { \mathbb { R } } | F _ { \mu } ( x ) - F _ { \nu } ( \bar { x } ) | d x } \end{array}$ , where $F _ { \mu }$ and $F _ { \nu }$ are the cumulative distribution function of $\mu$ and $\nu$ , respectively. It can be shown that ${ \mathcal { T } } ^ { \pi }$ is a $\gamma$ -contraction w.r.t. the supreme $p$ -Wasserstein metric $\bar { W } _ { p }$ .

Proposition 2.1. [Bellemare et al., 2023, Propositions 4.15] The distributional Bellman operator is a $\gamma$ -contraction on $\mathcal { P } ^ { S } \kappa . r . t .$ the supreme $p$ -Wasserstein metric for $p \in [ 1 , \infty ]$ . That is, for any $\eta , \eta ^ { \prime } \in \mathcal { P } ^ { S }$ , we have $\bar { W } _ { p } \left( \bar { T } ^ { \pi } \eta , \mathcal { T } ^ { \pi } \eta ^ { \prime } \right) \leq \gamma \bar { W } _ { p } ( \eta , \eta ^ { \prime } )$ .

The $\ell _ { p }$ metric between $\mu$ and $\nu$ is defined as $\begin{array} { r } { \ell _ { p } ( \mu , \nu ) = \left( \int _ { \mathbb { R } } | F _ { \mu } ( x ) - F _ { \nu } ( x ) | ^ { p } d x \right) ^ { \frac { 1 } { p } } } \end{array}$ for $p \in [ 1 , \infty )$ , and ${ \mathcal { T } } ^ { \pi }$ is $\gamma ^ { \frac { 1 } { p } }$ -contraction w.r.t. the supreme $\ell _ { p }$ metric $\bar { \ell } _ { p }$ .

Proposition 2.2. [Bellemare et al., 2023, Propositions 4.20] The distributional Bellman operator is $a \gamma ^ { \frac { 1 } { p } }$ -contraction on $\mathcal { P } ^ { S } \kappa . r . t .$ . the supreme $\ell _ { p }$ metric for $p \in [ 1 , \infty )$ . That is, for any $\eta , \eta ^ { \prime } \in \mathcal { P } ^ { S }$ , we have $\bar { \ell } _ { p } \left( \mathcal { T } ^ { \pi } \eta , \mathcal { T } ^ { \pi } \eta ^ { \prime } \right) \leq \gamma ^ { \frac { 1 } { p } } \bar { \ell } _ { p } ( \eta , \eta ^ { \prime } )$ .

Note that the $\ell _ { 1 }$ metric coincides with the 1-Wasserstein metric. And the $\ell _ { 2 }$ metric is also called the Cramér metric, which plays an important role in subsequent analysis because the zero-mass signed measure space equipped with this metric $( \mathcal { M } , \| \cdot \| _ { \ell _ { 2 } } )$ (defined in Section 5.1) is a Hilbert space5. Thereby, we can apply Freedman’s inequality in Hilbert spaces.

# 2.3 Freedman’s Inequality in Hilbert Spaces

Just as Freedman’s inequality is essential for the theory of TD (Theorem 1 in [Li et al., 2024]), a Hilbert space version of Freedman’s inequality is indispensable for deriving the minimax nonasymptotic convergence bound for distributional TD. At the moment, we state a Hilbert space version of the original Freedman’s inequality (Theorem 1.6 in [Freedman, 1975]), and more detailed results can be found in Appendix A.

Let $\chi$ be a Hilbert space, $\{ X _ { i } \} _ { i = 1 } ^ { n }$ be an $\mathcal { X }$ -valued martingale difference sequence adapted to the filtration $\left\{ \mathcal { F } _ { i } \right\} _ { i = 1 } ^ { n }$ , $\begin{array} { r } { Y _ { i } : = \sum _ { j = 1 } ^ { i } X _ { j } } \end{array}$ be the corresponding martingale, and $\begin{array} { r } { W _ { i } : = \sum _ { j = 1 } ^ { i } \sigma _ { j } ^ { 2 } } \end{array}$ be the corresponding quadratic variation process. Here $\sigma _ { j } ^ { 2 } : = \mathbb { E } _ { j - 1 } \left. X _ { j } \right. ^ { 2 }$ , and $\mathbb { E } _ { i } \left[ \cdot \right] : = \mathbb { E } \left[ \cdot | \mathcal { F } _ { i } \right]$ denotes the conditional expectation.

Theorem 2.1 (Freedman’s inequality in Hilbert spaces). Suppose $\mathrm { m a x } _ { i \in [ n ] } \| X _ { i } \| \leq b$ for some constant $b > 0$ . Then, for any $\varepsilon$ and $\sigma > 0$ , the following inequality holds

$$
\mathbb { P } \left( \exists k \in [ n ] , s . t . \ \| Y _ { k } \| \ge \varepsilon \ a n d W _ { k } \le \sigma ^ { 2 } \right) \le 2 \exp \left\{ - \frac { \varepsilon ^ { 2 } / 2 } { \sigma ^ { 2 } + b \varepsilon / 3 } \right\} .
$$

# 3 Distributional Temporal Difference Learning

If the MDP $M = \langle S , A , \mathcal { P } _ { R } , P , \gamma \rangle$ is known, and because $V ^ { \pi }$ is the fixed point of the contraction $T ^ { \pi }$ , $V ^ { \pi }$ can be evaluated via the famous dynamic programming (DP) algorithm. To be concrete, for any initialization $V ^ { ( 0 ) } \in \mathbb { R } ^ { S }$ , if we define the iteration sequence $V ^ { ( k + \bar { 1 } ) } = T ^ { \pi } ( V ^ { ( k ) } )$ for $k \in \mathbb { N }$ , we have $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } \left\| V ^ { ( k ) } - V ^ { \pi } \right\| _ { \infty } = 0 } \end{array}$ by the contraction mapping theorem (Proposition 4.7 in [Bellemare et al., 2
023]).

Similarly, the distributional dynamic programming algorithm defines the iteration sequence as $\eta ^ { ( k + 1 ) } \stackrel { \cdot } { = } \mathcal { T } ^ { \pi } \eta ^ { ( k ) }$ for any initialization $\boldsymbol { \eta } ^ { ( 0 ) }$ . In the same way, we have $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } \bar { W } _ { p } ( \eta ^ { ( k ) } , \eta ^ { \pi } ) = 0 } \end{array}$ for $p \in [ 1 , \infty ]$ and $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } \bar { \ell } _ { p } ( \eta ^ { ( k ) } , \eta ^ { \pi } ) = 0 } \end{array}$ for $p \in [ 1 , \infty )$ .

In most application scenarios, the transition dynamic $P$ and reward distribution $\mathcal { P } _ { R }$ are unknown, and instead we can only get samples of $P$ and $\mathcal { P } _ { R }$ in a streaming manner. In this paper, we assume a generative model [Kakade, 2003, Kearns et al., 2002] is accessible, which generates independent samples for all states in each iteration, i.e., in the $t$ -th iteration, we collect sample ${ a _ { t } } \widehat { ( s ) } \sim \pi ( \cdot | s ) , s _ { t } ( s ) \sim P ( \cdot | s , a _ { t } ( s ) ) , r _ { t } ( s ) \sim \mathcal { P } _ { R } ( \cdot | s , a _ { t } ( s ) ) $ for each $s \in { \mathcal { S } }$ . Similar to TD [Sutton, 1988] in classic RL, distributional TD also employs the stochastic approximation (SA) [Robbins and Monro, 1951] technique to address the aforementioned problem and can be viewed as an approximate version of distributional DP.

Non-parametric Distributional TD We first introduce non-parametric distributional temporal difference learning (NTD), which is helpful in the theoretical understanding of distributional TD. In the setting of NTD, we assume the return distributions can be precisely updated without any parametrization. For any initialization $\eta _ { 0 } ^ { \pi } \in \mathcal { P } ^ { S }$ , the updating scheme is given by

$$
\eta _ { t } ^ { \pi } = ( 1 - \alpha _ { t } ) \eta _ { t - 1 } ^ { \pi } + \alpha _ { t } \mathcal { T } _ { t } ^ { \pi } \eta _ { t - 1 } ^ { \pi }
$$

for any $t \geq 1$ . Here $\alpha _ { t }$ is the step size. The empirical Bellman operator at the $t$ -th iteration $\mathcal { T } _ { t } ^ { \pi }$ is defined as

$$
( T _ { t } ^ { \pi } \eta ) ( s ) = ( b _ { r _ { t } ( s ) , \gamma } ) _ { \# } \big ( \eta ( s _ { t + 1 } ) \big ) ,
$$

which is an unbiased estimator of $( \mathcal T ^ { \pi } \eta ) \left( s \right)$ . It is evident that NTD is a SA modification of distributional DP. Consequently, we can analyze NTD using the techniques from the SA area.

Categorical Distributional TD Now, we revisit the more practical CTD. In this case, the updates in CTD is computationally tractable, due to the following categorical parametrization of probability distributions:

$$
\mathcal { P } _ { K } : = \left\{ \sum _ { k = 0 } ^ { K } p _ { k } \delta _ { x _ { k } } : p _ { 0 } , \dotsc , p _ { K } \geq 0 , \sum _ { k = 0 } ^ { K } p _ { k } = 1 \right\} ,
$$

where $K \in \mathbb { N }$ , and $0 \leq x _ { 0 } < \cdot \cdot \cdot < x _ { K } \leq \frac { 1 } { 1 - \gamma }$ are fixed points of the support. For simplicity, we assume $\left\{ x _ { k } \right\} _ { k = 0 } ^ { K }$ are equally-spaced, i.e., $\begin{array} { r } { x _ { k } = \frac { k } { K ( 1 - \gamma ) } } \end{array}$ . We denote the gap between two points by $\begin{array} { r } { \iota _ { K } = \frac { 1 } { K ( 1 - \gamma ) } } \end{array}$ . When updating the return distributions, we need to evaluate the $\ell _ { 2 }$ -projection of $\mathcal { P } _ { K }$ , $\Pi _ { K } \colon { \mathcal { P } }  { \mathcal { P } } _ { K }$ , $\begin{array} { r } { \Pi _ { K } \mu : = \operatorname * { a r g m i n } _ { \hat { \mu } \in { \mathcal P } _ { K } } \ell _ { 2 } ( \mu , \hat { \mu } ) } \end{array}$ . It can be shown (Proposition 5.14 in [Bellemare et al., 2023]) that the projection is uniquely given by

$$
\Pi _ { K } \mu = \sum _ { k = 0 } ^ { K } p _ { k } ( \mu ) \delta _ { x _ { k } } , \mathrm { w h e r e } p _ { k } ( \mu ) = \mathbb { E } _ { X \sim \mu } \left[ \left( 1 - \left| \frac { X - x _ { k } } { \iota _ { K } } \right| \right) _ { + } \right] ,
$$

$( x ) _ { + } : = \operatorname* { m a x } { \{ x , 0 \} }$ for any $x \in \mathbb { R }$ . It is known that $\Pi _ { K }$ is non-expansive w.r.t. the Cramér metric (Lemma 5.23 in [Bellemare et al., 2023]), i.e., $\ell _ { 2 } ( \Pi _ { K } \mu , \Pi _ { K } \nu ) \le \ell _ { 2 } ( \mu , \nu )$ for any $\mu , \nu \in \mathcal { P }$ . For any $\eta \in \mathcal { P } ^ { s }$ , $s \in \mathcal S$ , we slightly abuse the notation and define $\left( \Pi _ { K } \eta \right) ( s ) : = \Pi _ { K } \eta ( s )$ . $\Pi _ { K }$ is still

non-expansive w.r.t. $\bar { \ell } _ { 2 }$ . Hence $\mathcal { T } ^ { \pi , K } : = \Pi _ { K } \mathcal { T } ^ { \pi }$ is a $\sqrt { \gamma }$ -contraction w.r.t. $\bar { \ell } _ { 2 }$ , we denote its unique fixed point as $\eta ^ { \pi , K } \in \mathcal { P } _ { K } ^ { S }$ . The approximation error induced by categorical parametrization is given by (Proposition 3 in Rowland et al. [2018])

$$
\bar { \ell } _ { 2 } ( \eta ^ { \pi } , \eta ^ { \pi , K } ) \le \frac { 1 } { \sqrt { K } ( 1 - \gamma ) } , \quad \bar { W } _ { 1 } ( \eta ^ { \pi } , \eta ^ { \pi , K } ) \le \frac { 1 } { \sqrt { 1 - \gamma } } \bar { \ell } _ { 2 } ( \eta ^ { \pi } , \eta ^ { \pi , K } ) \le \frac { 1 } { \sqrt { K } ( 1 - \gamma ) ^ { 3 / 2 } } .
$$

Now, we are ready to give the updating scheme of CTD, given any initialization $\eta _ { 0 } ^ { \pi } \in \mathcal { P } _ { K } ^ { S }$ ,

$$
\eta _ { t } ^ { \pi } = ( 1 - \alpha _ { t } ) \eta _ { t - 1 } ^ { \pi } + \alpha _ { t } \Pi _ { K } \mathcal { T } _ { t } ^ { \pi } \eta _ { t - 1 } ^ { \pi }
$$

for any $t \geq 1$ . We can find that the only difference between CTD and NTD lies in the additional application of the projection operator $\Pi _ { K }$ at each iteration in CTD.

# 4 Statistical Analysis

In this section, we state our main results. For both NTD and CTD, we give the non-asymptotic convergence rates of $\bar { W } _ { p } ( \eta _ { T } ^ { \pi } , \eta ^ { \pi } )$ and $\bar { \ell } _ { 2 } ( \eta _ { T } ^ { \pi } , \eta ^ { \pi } )$ , respectively.

# 4.1 Non-asymptotic Analysis of NTD

We first provide a non-asymptotic convergence rate of $\bar { W } _ { 1 } ( \eta _ { T } ^ { \pi } , \eta ^ { \pi } )$ for NTD, which is minimax optimal (Theorem B.1) up to logarithmic factors.

Theorem 4.1 (Sample complexity of NTD in the 1-Wasserstein metric). Given any $\delta \in ( 0 , 1 )$ and $ { \varepsilon } \in ( 0 , 1 )$ , let the initialization be $\eta _ { 0 } ^ { \pi } \in \mathcal { P } ^ { S }$ , the total update number $T$ satisfy

$$
T \geq \frac { C _ { 1 } \log ^ { 3 } T } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 3 } } \log \frac { | \mathcal { S } | T } { \delta }
$$

for some large universal constant C1 > 1, i.e., T = O  ε2(11 γ)3 , and the step size αt satisfy

$$
{ \frac { 1 } { 1 + { \frac { c _ { 2 } ( 1 - { \sqrt { \gamma } } ) t } { \log t } } } } \leq \alpha _ { t } \leq { \frac { 1 } { 1 + { \frac { c _ { 3 } ( 1 - { \sqrt { \gamma } } ) t } { \log t } } } }
$$

for some small universal constants $c _ { 2 } > c _ { 3 } > 0$ . Then, with probability at least $1 - \delta$ , the last iterate estimator satisfies $\bar { W } _ { 1 } \left( \eta _ { T } ^ { \pi } , \eta ^ { \pi } \right) \leq \varepsilon$ .

Because $\bar { W } _ { 1 } \left( \eta _ { T } ^ { \pi } , \eta ^ { \pi } \right) \leq \frac { 1 } { 1 - \gamma }$ always holds, we can translate the high probability bound to a mean error bound, that is,

$$
\mathbb { E } \left[ \hat W _ { 1 } ( \eta _ { T } ^ { \pi } , \eta ^ { \pi } ) \right] \le \varepsilon ( 1 - \delta ) + \frac { \delta } { 1 - \gamma } \le 2 \varepsilon
$$

if we take $\delta \le \varepsilon ( 1 - \gamma )$ . In the subsequent discussion, we will not state the mean error bound conclusions for the sake of brevity.

The key idea of our proof is to first expand the error term $\bar { W } _ { 1 } \left( \eta _ { T } ^ { \pi } , \eta ^ { \pi } \right)$ over the time steps. Then it can be decomposed into an initial error term and a martingale term. The initial error term becomes smaller as the iteration goes due to the contraction properties of ${ \mathcal { T } } ^ { \pi }$ . To control the martingale term, we first use the basic inequality (Lemma E.1) W1 (µ, ν) ≤ √1 γ , which allows us to analyze this error term in the Hilbert space $( \mathcal { M } , \| \cdot \| _ { \ell _ { 2 } } )$ defined in Section 5.1. Consequently, we can bound it using Freedman’s inequality in the Hilbert space (Theorem A.2). A more detailed outline of proof can be found in Section 5.2.

Combining Theorem 4.1 with the basic inequality $\begin{array} { r } { \hat W _ { p } ( \eta , \eta ^ { \prime } ) \le \frac { 1 } { ( 1 - \gamma ) ^ { 1 - \frac { 1 } { p } } } \hat W _ { 1 } ^ { \frac { 1 } { p } } ( \eta , \eta ^ { \prime } ) } \end{array}$ for any $\eta , \eta ^ { \prime } \in$ $\mathcal { P } ^ { s }$ (Lemma E.1), we can derive that $\begin{array} { r } { T = { \widetilde O } \left( \frac { 1 } { \varepsilon ^ { 2 p } ( 1 - \gamma ) ^ { 2 p + 1 } } \right) } \end{array}$ iterations are sufficient to ensure $\bar { W } _ { p } ( \eta _ { T } ^ { \pi } , \eta ^ { \pi } ) \leq \varepsilon$ . As pointed out in the exampele after Corollary 3.1 in [Zhang et al., 2023], when $p > 1$ , the slow rate in terms of $\varepsilon$ is inevitable without additional regularity conditions.

Although the 1-Wasserstein metric cannot bound the Cramér metric properly, by making slight modifications to the proof we have the following non-asymptotic convergence rate of $\bar { \ell } _ { 2 } ( \overline { { { \eta } } } _ { T } ^ { \pi } , \overline { { { \eta } } } ^ { \pi } )$ . See Appendix C.5 for our proof.

Corollary 4.1 (Sample complexity of NTD in the Cramér metric). Given any $\delta \in ( 0 , 1 )$ and $\varepsilon \in$ $( 0 , 1 )$ , let the initial value $\eta _ { 0 } ^ { \pi } \in \mathring { \mathcal { P } } ^ { s }$ , the total update number $T$ satisfy

$$
T \geq \frac { C _ { 1 } \log ^ { 3 } T } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 5 / 2 } } \log \frac { | \cal { S } | T } { \delta }
$$

for some large universal constant $C _ { 1 } > 1$ , i.e., $\begin{array} { r } { T = \widetilde { O } \left( \frac { 1 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 5 / 2 } } \right) } \end{array}$ , and the step size $\alpha _ { t }$ satisfy

$$
{ \frac { 1 } { 1 + { \frac { c _ { 2 } ( 1 - { \sqrt { \gamma } } ) t } { \log t } } } } \leq \alpha _ { t } \leq { \frac { 1 } { 1 + { \frac { c _ { 3 } ( 1 - { \sqrt { \gamma } } ) t } { \log t } } } }
$$

for some small universal constants $c _ { 2 } > c _ { 3 } > 0$ . Then, with probability at least $1 - \delta$ , the last iterate estimator satisfies $\bar { \ell } _ { 2 } \left( \eta _ { T } ^ { \pi } , \eta ^ { \pi } \right) \leq \varepsilon$ .

# 4.2 Non-asymptotic Analysis of CTD

We first state a parallel result to Theorem 4.1.

Theorem 4.2 (Sample complexity of CTD in the 1-Wasserstein metric). Given any $\delta \in ( 0 , 1 )$ and $ { \varepsilon } \in ( 0 , 1 )$ , suppose $\begin{array} { r } { \dot { K } > \frac { \dot { 4 } } { 1 - \gamma } } \end{array}$ , the initial value $\eta _ { 0 } ^ { \pi } \in \mathcal { P } _ { K } ^ { S }$ , the total update number $T$ satisfies

$$
T \geq \frac { C _ { 1 } \log ^ { 3 } T } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 3 } } \log \frac { | \mathcal { S } | T } { \delta }
$$

for some large universal constant $C _ { 1 } > 1$ , i.e., $\begin{array} { r } { T = \widetilde { O } \left( \frac { 1 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 3 } } \right) } \end{array}$ , and the step size $\alpha _ { t }$ satisfies

$$
{ \frac { 1 } { 1 + { \frac { c _ { 2 } ( 1 - { \sqrt { \gamma } } ) t } { \log t } } } } \leq \alpha _ { t } \leq { \frac { 1 } { 1 + { \frac { c _ { 3 } ( 1 - { \sqrt { \gamma } } ) t } { \log t } } } }
$$

for some small universal constants $c _ { 2 } > c _ { 3 } > 0$ . Then, with probability at least $1 - \delta$ , the last iterate estimator satisfies $\bar { W } _ { 1 } \left( \eta _ { T } ^ { \pi } , \eta ^ { \pi , K } \right) \leq \frac { \varepsilon } { 2 }$ . Furthermore, according to the upper b−ound (3) of the approximation error $\bar { W } _ { 1 } \left( \eta ^ { \pi , K } , \eta ^ { \pi } \right)$ , if we take $\begin{array} { r } { K > \frac { 4 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 3 } } } \end{array}$ , we have $\bar { W } _ { 1 } \left( \eta _ { T } ^ { \pi } , \eta ^ { \pi } \right) \leq \varepsilon$ .

Note that the order (modulo logarithmic factors) of sample complexity of CTD is better than the previous results of SCPE [Böck and Heitzinger, 2022], and we do not need the additional term introduced in the updating scheme of SCPE.

The proof of this theorem is almost the same as that of Theorem 4.1, we outline the proof in Section 5.2. The $\bar { W } _ { 1 }$ metric result can be translated into sample complexity bound $\begin{array} { r } { \widetilde O \left( \frac { 1 } { \varepsilon ^ { 2 p } ( 1 - \gamma ) ^ { 2 p + 1 } } \right) } \end{array}$ in the $\bar { W } _ { p }$ metric. We comment that this theoretical result matches the sample complexity bound in the model-based setting [Rowland et al., 2024b].

As in the NTD setting, we have the following non-asymptotic convergence rate of $\bar { \ell } _ { 2 } ( \eta _ { T } ^ { \pi } , \eta ^ { \pi } )$ as a corollary of Theorem 4.2. See Appendix C.5 for the proof.

Corollary 4.2 (Sample complexity of CTD in the Cramér metric). For any given $\delta \in ( 0 , 1 )$ and $ { \varepsilon } \in ( 0 , 1 )$ , suppose $\begin{array} { r } { \dot { K } > \frac { 4 } { 1 - \gamma } } \end{array}$ , the initialization is $\eta _ { 0 } ^ { \pi } \in \mathcal { P } _ { K } ^ { S }$ , the total update number $T$ satisfies

$$
T \geq \frac { C _ { 1 } \log ^ { 3 } T } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 5 / 2 } } \log \frac { | \cal { S } | T } { \delta }
$$

for some large universal constant $C _ { 1 } > 1$ , i.e., $\begin{array} { r } { T = \widetilde { O } \left( \frac { 1 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 5 / 2 } } \right) } \end{array}$ , and the step size $\alpha _ { t }$ satisfies

$$
{ \frac { 1 } { 1 + { \frac { c _ { 2 } ( 1 - { \sqrt { \gamma } } ) t } { \log t } } } } \leq \alpha _ { t } \leq { \frac { 1 } { 1 + { \frac { c _ { 3 } ( 1 - { \sqrt { \gamma } } ) t } { \log t } } } }
$$

for some small universal constants $c _ { 2 } > c _ { 3 } > 0$ . Then, with probability at least $1 - \delta$ , the last iterate estimator satisfies $\begin{array} { r } { \bar { \ell } _ { 2 } \left( \eta _ { T } ^ { \pi } , \eta ^ { \pi , K } \right) \le \frac { \varepsilon } { 2 } } \end{array}$ . Furthermore, according to the upper bound (3) of the approximation error $\bar { \ell } _ { 2 } \left( \eta ^ { \pi , K } , \eta ^ { \pi } \right)$ , $i f$ we take $\begin{array} { r } { K > \frac { 4 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 2 } } } \end{array}$ , we have $\bar { \ell } _ { 2 } \left( \eta _ { T } ^ { \pi } , \eta ^ { \pi } \right) \leq \varepsilon$ .

# 5 Proof Outlines

In this section, we will outline the proofs of our main theoretical results (Theorem 4.1, Corollary 4.1, Theorem 4.2, and Corollary 4.2). Before diving into the details of the proofs, we first define some notation.

# 5.1 Zero-mass Signed Measure Space

To analyze the distance between the estimator and the ground-truth $\eta ^ { \pi }$ , we will work with the zeromass signed measure space $\mathcal { M }$ defined as follows

$$
\mathcal { M } : = \left. \mu : \mu \mathrm { ~ i s ~ a ~ s i g n e d ~ m e a s u r e ~ w i t h ~ } | \mu | \left( \mathbb { R } \right) < \infty , \mu ( \mathbb { R } ) = 0 , \operatorname { s u p p } ( \mu ) \subseteq \left[ 0 , \frac { 1 } { 1 - \gamma } \right] \right. ,
$$

where $| \mu |$ is the total variation measure of $\mu$ , and $\operatorname { s u p p } ( \mu )$ is the support of $\mu$ . See [Bogachev, 2007] for more details about signed measures.

For any $\mu \in \mathcal { M }$ , we define its cumulative function as $F _ { \mu } ( x ) : = \mu [ 0 , x )$ . We can check that $F _ { \mu }$ is linear w.r.t. $\mu$ , that is, $F _ { \alpha \mu + \beta \nu } = \alpha F _ { \mu } + \beta F _ { \nu }$ for any $\alpha , \beta \in \mathbb { R } , \mu , \nu \in \mathcal { M }$ .

To analyze the Cramér metric case, we define the following Cramér inner product on $\mathcal { M }$ :

$$
\langle \mu , \nu \rangle _ { \ell _ { 2 } } : = \int _ { 0 } ^ { \frac { 1 } { 1 - \gamma } } F _ { \mu } ( x ) F _ { \nu } ( x ) d x .
$$

It is easy to verify that $\langle \cdot , \cdot \rangle _ { \ell _ { 2 } }$ is indeed an inner product on $\mathcal { M }$ . The corresponding norm, called the Cramér norm, is given by $\begin{array} { r } { \| \mu \| _ { \ell _ { 2 } } = \sqrt { \langle \mu , \mu \rangle _ { \ell _ { 2 } } } = \sqrt { \int _ { 0 } ^ { \frac { 1 } { 1 - \gamma } } \left( F _ { \mu } ( x ) \right) ^ { 2 } d x } } \end{array}$ . We have $\nu _ { 1 } - \nu _ { 2 } \in \mathcal { M }$ and $\| \nu _ { 1 } - \nu _ { 2 } \| _ { \ell _ { 2 } } = \ell _ { 2 } \left( \nu _ { 1 } , \nu _ { 2 } \right)$ for any $\nu _ { 1 } , \nu _ { 2 } \in { \mathcal { P } }$ .

The $W _ { 1 }$ norm on $\mathcal { M }$ is defined as $\begin{array} { r } { \| \mu \| _ { W _ { 1 } } : = \int _ { 0 } ^ { \frac { 1 } { 1 - \gamma } } | F _ { \mu } ( x ) | d x } \end{array}$ . We have $\| \nu _ { 1 } - \nu _ { 2 } \| _ { W _ { 1 } } = W _ { 1 } \left( \nu _ { 1 } , \nu _ { 2 } \right)$ for any $\nu _ { 1 } , \nu _ { 2 } \in { \mathcal { P } }$ .

We can extend the distributional Bellman operator $\mathcal { T } ^ { \pi }$ and the Cramér projection operator $\Pi _ { K }$ naturally to $\mathcal { M } ^ { s }$ . Here, the product space $\mathcal { M } ^ { \bar { s } }$ is also a Banach space, and we use the supreme norm: $\begin{array} { r } { \| \eta \| _ { \bar { \ell } _ { 2 } } : = \operatorname* { m a x } _ { s \in \mathcal { S } } \| \eta ( s ) \| _ { \ell _ { 2 } } } \end{array}$ , and $\lVert \eta \rVert _ { \bar { W } _ { 1 } } : = \operatorname* { m a x } _ { s \in \mathcal { S } } \left. \eta ( s ) \right. _ { W _ { 1 } }$ for any $\boldsymbol \eta \in \mathcal { M } ^ { s }$ . We denote by $\boldsymbol { \mathcal { T } }$ the identity operator in MS .

When the norm $\left\| \cdot \right\|$ is applied to $A ~ \in ~ { \mathcal { L } } ( { \mathcal { X } } )$ , where $\mathcal { X }$ is any Banach space, and $\mathcal { L } ( \mathcal { X } )$ is the space of all bounded linear operators in $\mathcal { X }$ , we refer $\| A \|$ to the operator norm of $A$ , which is defined as $\begin{array} { r } { \| A \| \ \because \ \operatorname* { s u p } _ { \eta \in { \mathcal X } , \| \eta \| = 1 } \| A \eta \| } \end{array}$ . With this notation, $\begin{array} { r l } { { \mathcal { L } } ( \mathcal { X } ) } & { { } = } \end{array}$ $\{ A \colon A$ is a linear operator mapping from $\mathcal { X }$ to $\mathcal { X } , a n d \| A \| < \infty \}$ .

Proposition 5.1. ${ \mathcal { T } } ^ { \pi }$ and $\Pi _ { K }$ are linear operators in $\mathcal { M } ^ { s }$ . Furthermore, $\begin{array} { r l r } { \| \mathcal { T } ^ { \pi } \| _ { \bar { \ell } _ { 2 } } } & { \leq } & { \sqrt { \gamma } , } \end{array}$ , $\lVert \mathcal { T } ^ { \pi } \rVert _ { \bar { W } _ { 1 } } \leq \gamma , \lVert \Pi _ { K } \rVert _ { \bar { \ell } _ { 2 } } = 1$ , and $\Vert \Pi _ { K } \Vert _ { \bar { W } _ { 1 } } \leq 1$ .

The proof of the last inequality can be found in the proof of Lemma C.4, while the remaining results are trivial. We omit the proofs for brevity.

Moreover, we have the following matrix (of operators) representations of ${ \mathcal { T } } ^ { \pi }$ and $\Pi _ { K }$ : ${ \mathcal { T } } ^ { \pi } \in$ $\mathcal { L } ( \mathcal { M } ) ^ { s \times \bar { s } }$ for any $\boldsymbol \eta \in \mathcal { M } ^ { s }$ ,

$$
\left( T ^ { \pi } \eta \right) ( s ) = \sum _ { a \in A , s ^ { \prime } \in S } \pi ( a \mid s ) P ( s ^ { \prime } \mid s , a ) \int _ { 0 } ^ { 1 } ( b _ { r , \gamma } ) _ { \# } \eta ( s ^ { \prime } ) \mathcal { P } _ { R } ( d r \mid s , a ) = \sum _ { s ^ { \prime } \in S } T ^ { \pi } ( s , s ^ { \prime } ) \eta ( s ^ { \prime } ) ,
$$

where ${ \mathcal { T } } ^ { \pi } ( s , s ^ { \prime } ) \in { \mathcal { L } } ( { \mathcal { M } } )$ for any $\nu \in \mathcal { M }$ ,

$$
{ \mathcal T } ^ { \pi } ( s , s ^ { \prime } ) \nu = \sum _ { a \in \mathcal A } \pi ( a \mid s ) P ( s ^ { \prime } \mid s , a ) \int _ { 0 } ^ { 1 } ( b _ { r , \gamma } ) _ { \# } \nu { \mathcal P } _ { R } ( d r \mid s , a ) .
$$

It can be verified that $\begin{array} { r } { \| \mathcal { T } ( s , s ^ { \prime } ) \| _ { \ell _ { 2 } } \le \sqrt { \gamma } \sum _ { a \in \mathcal { A } } \pi ( a \mid s ) P ( s ^ { \prime } \mid s , a ) = : \sqrt { \gamma } P ^ { \pi } ( s ^ { \prime } | s ) . } \end{array}$ . Similarly, $\Vert \mathcal { T } ( s , s ^ { \prime } ) \Vert _ { W _ { 1 } } \ \leq \ \gamma P ^ { \pi } ( s ^ { \prime } | s )$ , and $\Pi _ { K } \ = \ \mathrm { d i a g } \big ( \Pi _ { K } \big | _ { \mathcal { M } } \big ) _ { s \in \mathcal { S } } \ \in \ \mathcal { L } ( \mathcal { M } ) ^ { S \times S }$ . With these representations, $\Pi _ { K } \mathcal { T } ^ { \pi } \in \mathcal { L } ( \mathcal { M } ) ^ { s \times s }$ can be interpreted as matrix multiplication, where the scalar multiplication is replaced by the composition of operators. It can be verified that $\left( \Pi _ { K } \mathcal { T } ^ { \pi } \right) \left( s , s ^ { \prime } \right) =$ $\bar { \Pi _ { K } } \mathcal { T } ^ { \pi } ( s , s ^ { \prime } )$ , and $\Vert \big ( \Pi _ { K } \overline { { \mathcal { T } ^ { \pi } } } \big ) \big ( s , s ^ { \bar { \prime } } \big ) \Vert _ { \ell _ { 2 } } \leq \sqrt { \gamma } \bar { P ^ { \pi } } \big ( s ^ { \prime } | s \big )$ .

Remark 1: Although the spaces $( \mathcal { M } , \| \cdot \| _ { \ell _ { 2 } } )$ and $( \mathcal { M } , \| \cdot \| _ { W _ { 1 } } )$ are not complete, we will use their completions to replace them without loss of generality, because the completeness property does not affect the non-asymptotic analysis. For simplicity, we still use $\mathcal { M }$ to denote the completion space. And according to the BLT theorem (Theorem 5.19 in [Hunter and Nachtergaele, 2001]), any bounded linear operator can be extended to the completion space, and still preserves its operator norm.

# 5.2 Analysis of Theorems 4.1 and 4.2

For simplicity, we abbreviate both $\lVert \cdot \rVert _ { \bar { \ell } _ { 2 } }$ and $\lVert \cdot \rVert _ { \ell _ { 2 } }$ as $\left\| \cdot \right\|$ in this part. For all $t ~ \in ~ [ T ] ~ : =$ $\{ 1 , 2 , \cdots , T \}$ , we denote $\mathscr { T } _ { t } : = \mathscr { T } _ { t } ^ { \pi }$ , $\mathcal { T } : = \mathcal { T } ^ { \pi }$ , $\eta : = \eta ^ { \pi }$ for NTD; $\mathcal { T } _ { t } : = \Pi _ { K } \mathcal { T } _ { t } ^ { \pi }$ , $\mathcal { T } : = \Pi _ { K } \mathcal { T } ^ { \pi }$ , $\overset { \cdot } { \eta } : = \eta ^ { \pi , K }$ for CTD; and $\mathbf { \Psi } _ { t } : = \eta _ { t } ^ { \pi } , \Delta _ { t } : = \eta _ { t } - \eta \in \mathcal { M } ^ { S }$ for both NTD and CTD. According to Lemma E.2, $\eta _ { t } \in \mathcal { P } ^ { S }$ for NTD and $\eta _ { t } \in \mathcal { P } _ { K } ^ { S }$ for CTD. Our goal is to bound the $\hat { W } _ { 1 }$ norm of the error term $\| \Delta _ { T } \| _ { \bar { W } _ { 1 } }$ . This can be achieved by bounding $\| \Delta _ { T } \|$ , as $\begin{array} { r } { \| \Delta _ { T } \| _ { \bar { W } _ { 1 } } \leq \frac { 1 } { \sqrt { 1 - \gamma } } \| \Delta _ { T } \| } \end{array}$ .

According to the updating rule, we have the error decomposition

$$
\begin{array} { r l } & { \Delta _ { t } = \eta _ { t } - \eta } \\ & { \quad = ( 1 - \alpha _ { t } ) \eta _ { t - 1 } + \alpha _ { t } \mathcal { T } _ { t } \eta _ { t - 1 } - \eta } \\ & { \quad = ( 1 - \alpha _ { t } ) \Delta _ { t - 1 } + \alpha _ { t } \left( \mathcal { T } _ { t } \eta _ { t - 1 } - \mathcal { T } \eta \right) } \\ & { \quad = ( 1 - \alpha _ { t } ) \Delta _ { t - 1 } + \alpha _ { t } \left( \mathcal { T } _ { t } - \mathcal { T } \right) \eta _ { t - 1 } + \alpha _ { t } \mathcal { T } \left( \eta _ { t - 1 } - \eta \right) } \\ & { \quad = \left[ ( 1 - \alpha _ { t } ) \mathcal { T } + \alpha _ { t } \mathcal { T } \right] \Delta _ { t - 1 } + \alpha _ { t } \left( \mathcal { T } _ { t } - \mathcal { T } \right) \eta _ { t - 1 } . } \end{array}
$$

Applying it recursively, we can further decompose the error into two terms

$$
\Delta _ { T } = \underbrace { \prod _ { t = 1 } ^ { T } \left[ ( 1 - \alpha _ { t } ) \mathcal { I } + \alpha _ { t } \mathcal { T } \right] \Delta _ { 0 } } _  \mathrm { ( f ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } 
$$

Term (I) is an initial error term that becomes negligible when $T$ is large because $\tau$ is a contraction. Term (II) can be bounded via Freedman’s inequality in the Hilbert space (Theorem A.2). Combining the two upper bound, we can establish a recurrence relation. Solving this relation will lead to the conclusion.

We first establish the conclusion for step sizes that depend on $T$ . Specifically, we consider

$$
T \geq \frac { C _ { 4 } \log ^ { 3 } T } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 3 } } \log \frac { | \mathcal { S } | T } { \delta } ,
$$

$$
\frac { 1 } { 1 + \frac { c _ { 5 } ( 1 - \sqrt { \gamma } ) T } { \log ^ { 2 } T } } \leq \alpha _ { t } \leq \frac { 1 } { 1 + \frac { c _ { 6 } ( 1 - \sqrt { \gamma } ) t } { \log ^ { 2 } T } } ,
$$

where $c _ { 5 } > c _ { 6 } > 0$ are small constants satisfying $\begin{array} { l } { c _ { 5 } c _ { 6 } \ \leq \ \frac { 1 } { 8 } } \end{array}$ , and $C _ { 4 } > 1$ is a large constant depending only on $c _ { 5 }$ and $c _ { 6 }$ . As shown in Appendix C.1, once we have established the conclusion in this setting, we can recover the original conclusion stated in the theorem.

Now, we introduce the following useful quantities involving step sizes and $\gamma$

$$
\beta _ { k } ^ { ( t ) } : = \left\{ \begin{array} { l l } { \prod _ { i = 1 } ^ { t } \left( 1 - \alpha _ { i } ( 1 - \sqrt { \gamma } ) \right) , } & { \mathrm { i f ~ } k = 0 , } \\ { \alpha _ { k } \prod _ { i = k + 1 } ^ { t } \left( 1 - \alpha _ { i } ( 1 - \sqrt { \gamma } ) \right) , } & { \mathrm { i f ~ } 0 < k < t , } \\ { \alpha _ { T } , } & { \mathrm { i f ~ } k = t . } \end{array} \right.
$$

The following lemma provides useful bounds for $\beta _ { k } ^ { ( t ) }$

Lemma 5.1. Suppose $\begin{array} { r } { c _ { 5 } c _ { 6 } \leq \frac { 1 } { 8 } } \end{array}$ . Then, for all $\begin{array} { r } { t \geq \frac { T } { c _ { 6 } \log T } } \end{array}$ , we have that

$$
\beta _ { k } ^ { ( t ) } \leq \frac { 1 } { T ^ { 2 } } , f o r \ 0 \leq k \leq \frac { t } { 2 } ; \qquad \beta _ { k } ^ { ( t ) } \leq \frac { 2 \log ^ { 3 } T } { ( 1 - \sqrt { \gamma } ) T } , f o r \ \frac { t } { 2 } < k \leq t .
$$

The proof can be found in Appendix C.2. From now on, we only consider t ≥ c6 lTog T .

The upper bound of term (I) is given by

$$
( \mathrm { I } ) \le \prod _ { k = 1 } ^ { t } \| ( 1 - \alpha _ { k } ) \mathcal { I } + \alpha _ { k } \mathcal { T } | | \left\| \Delta _ { 0 } \right\| \le \prod _ { k = 1 } ^ { t } \left( ( 1 - \alpha _ { k } ) + \alpha _ { k } \sqrt { \gamma } \right) \frac { 1 } { \sqrt { 1 - \gamma } } = \frac { \beta _ { 0 } ^ { ( t ) } } { \sqrt { 1 - \gamma } } \le \frac { 1 } { \sqrt { 1 - \gamma T ^ { 2 } } } ,
$$

where $\begin{array} { r } { \| \Delta _ { 0 } \| \le \sqrt { \int _ { 0 } ^ { \frac { 1 } { 1 - \gamma } } d x } = \frac { 1 } { \sqrt { 1 - \gamma } } } \end{array}$

As for term (II), we have the following upper bound with high probability by utilizing Freedman’s inequality (Theorem A.2).

Lemma 5.2. For any $\delta \in ( 0 , 1 )$ , with probability at least $1 - \delta$ , we have for all $\begin{array} { r } { t \geq \frac { T } { c _ { 6 } \log T } } \end{array}$ , in the NTD case,

$$
\begin{array} { r l } & { \left\| \displaystyle \sum _ { k = 1 } ^ { t } \alpha _ { k } \prod _ { i = k + 1 } ^ { t } [ ( 1 - \alpha _ { i } ) \mathcal { T } + \alpha _ { i } \mathcal { T } ] \left( \mathcal { T } _ { k } - \mathcal { T } \right) \eta _ { k - 1 } \right\| } \\ & { \leq 3 4 \sqrt { \displaystyle \frac { \left( \log ^ { 3 } T \right) \left( \log \frac { | S | T } { \delta } \right) } { ( 1 - \gamma ) ^ { 2 } T } \left( 1 + \displaystyle \operatorname* { m a x } _ { k : t / 2 < k \leq t } \| \Delta _ { k - 1 } \| _ { \bar { W } _ { 1 } } \right) } . } \end{array}
$$

The conclusion still holds for the CTD case if we take $\begin{array} { r } { K \ge \frac { 4 } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 2 } } + 1 } \end{array}$

The proof can be found in Appendix C.3. Combining the two results, we find the following recurrence relation in terms of the $\bar { W } _ { 1 }$ norm holds given the choice of $T$ , with probability at least $1 - \delta$ , for all t ≥ c6 lTog T

$$
\left\| \Delta _ { t } \right\| _ { \bar { W } _ { 1 } } \leq \frac { 1 } { \sqrt { 1 - \gamma } } \left\| \Delta _ { t } \right\| \leq 3 5 \sqrt { \frac { \left( \log ^ { 3 } T \right) \left( \log \frac { | \mathcal { S } | T } { \delta } \right) } { ( 1 - \gamma ) ^ { 3 } T } \left( 1 + \operatorname* { m a x } _ { k : t / 2 < k \leq t } \left\| \Delta _ { k - 1 } \right\| _ { \bar { W } _ { 1 } } \right) } .
$$

In Theorem C.1, we solve the relation and obtain the error bound of the last iterate estimator:

$$
\| \Delta _ { T } \| _ { \bar { W } _ { 1 } } \leq C _ { 7 } \left( \sqrt { \frac { \left( \log ^ { 3 } T \right) \left( \log \frac { | \mathcal { S } | T } { \delta } \right) } { ( 1 - \gamma ) ^ { 3 } T } } + \frac { \left( \log ^ { 3 } T \right) \left( \log \frac { | \mathcal { S } | T } { \delta } \right) } { ( 1 - \gamma ) ^ { 3 } T } \right) ,
$$

where $C _ { 7 } > 1$ is a large universal constant depending on $c _ { 6 }$ . Now, we can obtain the conclusion if taking $C _ { 4 } \geq 2 C _ { 7 } ^ { 2 }$ and $\begin{array} { r } { \dot { T } \geq \frac { C _ { 4 } \log ^ { 3 } T } { \varepsilon ^ { 2 } ( 1 - \gamma ) ^ { 3 } } \log \frac { | S | T } { \delta } } \end{array}$ .

# 6 Conclusions

In this paper we have studied the statistical performance of the distributional temporal difference learning (TD) from a non-asymptotic perspective. Specifically, we have considered two instances of distributional TD, namely, the non-parametric distributional TD (NTD) and the categorical distributional TD (CTD). For both NTD and CTD, we have shown that $\begin{array} { r } { \widetilde O \left( \frac { 1 } { \varepsilon ^ { 2 p } ( 1 - \gamma ) ^ { 2 p + 1 } } \right) } \end{array}$ iterations are sufficient to achieve a $p$ -Wasserstein $\varepsilon$ -optimal estimator, which is meinimax optimal (up to logarithmic factors). We have established a novel Freedman’s inequality in Hilbert spaces to prove these theoretical results, which has independent theoretical value beyond the current work. We leave the details to Appendix A.