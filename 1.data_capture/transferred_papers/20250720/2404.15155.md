# MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making

Yubin $\mathbf { K i m ^ { 1 } }$ Chanwoo Park1 Hyewon Jeong1♮ Yik Siu Chan1 Xuhai ${ \bf X } { \bf u } ^ { 1 }$ Daniel McDuff2 Hyeonhoon Lee3 Marzyeh Ghassemi1 Cynthia Breazeal1 Hae Won Park1

1Massachusetts Institute of Technology 2Google Research 3Seoul National University Hospital {ybkim95,cpark97,hyewonj,yiksiuc,xoxu,mghassem,cynthiab,haewon}@mit.edu dmcduff@google.com hhoon@snu.ac.kr

# Abstract

Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named Medical Decision-making Agents (MDAgents) that helps to address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, a simple emulation inspired by the way real-world medical decision-making processes are adapted to tasks of different complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and medical diagnosis benchmarks, including a comparison of LLMs’ medical complexity classification against human physicians2. MDAgents achieved the best performance in seven out of ten benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant improvement of up to $4 . 2 \%$ $\cdot _ { p < 0 . 0 5 ) }$ compared to previous methods’ best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy improvement of $1 1 . 8 \%$ . Our code can be found at https://github.com/mitmedialab/MDAgents.

# 1 Introduction

Medical Decision-Making (MDM) is a multifaceted and intricate process in which clinicians collaboratively navigate diverse sources of information to reach a precise and specific conclusion [97]. For instance, a primary care physician (PCP) may refer a patient to a specialist when faced with a complex case, or a patient visiting the emergency department or urgent care might be triaged and then directed to a specialist for further evaluation [5, 54]. MDM involves interpreting complex and multi-modal data, such as imaging, electronic health records (EHR), physiological signals, and genetic information, while rapidly integrating new medical research into clinical practice [68, 78]. Recently, Large Language Models (LLMs) have shown potential for AI support in MDM [22, 37, 48, 64, 72, 90]. It is known that they are able to process and synthesize large volumes of medical literature [74] and clinical information [1], as well as support probabilistic [94] and causal [39] reasoning, makes LLMs promising tools. However, there is no silver bullet in medical applications that require careful design.

Primary Care Clinician (PCC) Medical Knowledge   
A 19-year-old Caucasian male presents to your office with   
hypopigmented skin. He undergoes a skin biopsy and is found to   
have an absence of melanocytes in the epidermis. Which of the Ans   
following is the most likely diagnosis? A. Tinea versicolor B. Albinism Prompting   
C. Vitiligo D. Solar lentigo E. Melanoma Differential Diagnosis (DDx) Multidisciplinary Team (MDT)   
Sex: M, Age: 47 Geographical region: North America 8   
Pathology: PSVT Moderate 8   
Symptoms: - I feel pain. The pain is: tugging, Burning … AIR 8 Ans   
Differential diagnosis: Log M-turns   
PSVT: 0.22, Anemia: 0.16,Panic attack: 0.14, Atrial fibrillation: N-rounds   
0.11, Anaphylaxis: 0.11, Cluster headache: 0.09, Chagas: 0.07, LLM Collaborative Discussion   
Scombroid food poisoning: 0.07, HIV (initial infection): 0.01 Checker Integrated Care Team (ICT) Multi-modal Reasoning l9   
What does the circle iAn: AmbangeorDmsalurrmoituontdic? figures ? 品 Ans B: Central keratinization Report Report C: Frank atypia Team 1 Team 2 Team N D: Areas of necrosis Report Generation Medical Query 1. Complexity Check 2. Recruitment 3. Analysis and Synthesis 4. Final Decision

While decision-making tools including multi-agent LLMs [11, 86] have shown promise in nonmedical domains [31, 32, 44, 46, 62, 65], their evaluation in health applications has been limited. To date, their “generalist” design has not effectively integrated the real-world systematic MDM process [57] which requires an adaptive, collaborative, and tiered approach. Clinicians consider the current and past history of the patient, available evidence from medical literature, and their domain expertise and experience [20] for MDM. One example of MDM is to triage patients in emergency room based on the severity and complexity of their medical conditions [12, 26, 87]. Patients with pathognomonic, single uncomplicated acute conditions, or stable chronic conditions that their PCP could manage [85] could be low complexity cases. On the other hand, patients with injuries that involve multiple organs, chronic conditions with side effects, or superimposed diseases who often require multiple collaborative discussions (MDT) or sequential consultations (ICT) among specialty physicians [27, 61] are considered high complexity cases 3.

Inspired by the way that clinicians make decisions in practice, we propose Medical Decision-making Agents (MDAgents), an adaptive medical decision-making framework that leverages LLMs to emulate the hierarchical diagnosis procedures ranging from individual clinicians to collaborative clinician teams (Figure 1). MDAgents work in three steps: 1) Medical complexity check; 2) Recruitment based on medical complexity; 3) Analysis and synthesis and 4) Final decision-making to return the answer. Our contributions are threefold:

1. We introduce MDAgents, the first adaptive decision-making framework for LLMs that mirrors real-world MDM processes via dynamic collaboration among AI agents based on task complexity.   
2. MDAgents demonstrate superior performance in accuracy over previous solo and group methods on 7 out of 10 medical benchmarks, and we show an effective trade-off between performance and efficiency (i.e. the number of API calls) by varying the number of agents.   
3. We provide rigorous testing under various hyperparameters (e.g. temperatures), demonstrating better robustness of MDAgents compared to solo and group methods. Furthermore, our ablations evidence MDAgents’ ability to find the appropriate complexity level for each MDM instance.

# 2 Related Works

Language Models in Medical Decision-Making LLMs have shown promise in a range of applications within the medical field [14, 37, 40, 48, 63, 75, 76, 90, 96]. They can answer questions from medical exams [43, 52], perform biomedical research [36], clinical risk prediction [37], and clinical diagnosis [55, 67]. Medical LLMs are also evaluated on generative tasks, including creating medical reports [79], describing medical images [81], constructing differentials [53], performing diagnostic dialogue with patients [77], and generating psychiatric evaluations of interviews [24]. To advance the capabilities of medical LLMs, two main approaches have been explored: (1) training with domain-specific data [28], and (2) applying inference-time techniques such as prompt engineering [67] and Retrieval Augmented Generation (RAG) [92]. While initial research has been concentrated on pre-training and fine-tuning with medical knowledge, the rise of large general-purpose LLMs has enabled training-free methods where models leverage their latent medical knowledge. For example, GPT-4 [60], with richer prompt crafting, surpasses the passing score on USMLE by over 20 points and with prompt tuning can outperform fine-tuned models including Med-PaLM [58, 59]. The promise of general-purpose models has thus inspired various techniques such as Medprompt and ensemble refinement to improve LLM reasoning [67], as well as RAG tools that use external resources to improve the factuality and completeness of LLM responses [38, 92]. Frameworks like MEDIQ [49] and UoT [33] advance LLM reliability in clinical settings by enhancing information-seeking through adaptive question-asking and uncertainty reduction, supporting more realistic diagnostic processes. Our approach leverages these techniques and the capabilities of general-purpose models while acknowledging that a solitary LLM [37, 48, 90] may not fully encapsulate the collaborative and multidisciplinary nature of real-world MDM. We thus emphasize joinging multiple expert LLMs for effective collaboration in order to solve complicated medical tasks with greater accuracy.

Table 1: Comparison between our framework and previous methods (Solo and Group). Among these works, MDAgents is the only one to perform all key dimensions of LLM decision-making.   

<html><body><table><tr><td>Method</td><td>MDAgents (Ours)</td><td>Single</td><td>Voting [82]</td><td>Debate [17]</td><td>MedAgents [72]</td><td>ReConcile[10]</td></tr><tr><td>Interaction Type</td><td>：</td><td>自</td><td></td><td>X</td><td></td><td></td></tr><tr><td>Multiple Roles</td><td>√</td><td>×</td><td>√</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Early Stopping</td><td>√</td><td>×</td><td>√</td><td>√</td><td>√</td><td>X</td></tr><tr><td>Refinement</td><td>√</td><td>×</td><td>X</td><td>√</td><td>√</td><td>X</td></tr><tr><td>Complexity Check</td><td>√</td><td>X</td><td>×</td><td>X</td><td>×</td><td>X</td></tr><tr><td>Multi-party Chat</td><td>√</td><td>X</td><td>X</td><td>√</td><td>X</td><td>X</td></tr><tr><td>Conversation Pattern</td><td>Flexible</td><td>Static</td><td>Static</td><td>Static</td><td>Static</td><td>Static</td></tr></table></body></html>

Multi-Agent Collaboration An array of studies have explored effective collaboration frameworks between multiple LLM agents [47, 86] to enhance capability above and beyond an individual LLM [80]. A common framework is role-playing, where each agent adopts a specific role (e.g. an Assistant Agent or a Manager Agent), a task is then broken down into sub-steps and solved collaboratively [47, 86]. While role-playing focuses on collaboration and multi-step problem-solving [88], another framework, “multi-agent debate”, prompts each agent to solve the task independently [17]. Then, they reason through other agents’ answers to converge on a shared response, this approach can improve the factuality, mathematical ability and reasoning capabilities of the multi-agent solution [17, 50]. Similar frameworks include voting [82], multi-disciplinary collaboration [72], group discussions (ReConcile [10]), and negotiating [23]. Table 1 compares existing setups across key dimensions in multi-agent interaction. Although these frameworks have shown improvement in the respective tasks, they rely on a pre-determined number of agents and interaction settings. When applied on a wider variety of tasks , this static architecture may lead to suboptimal multi-agent configurations, negatively impacting performance [51]. Furthermore, multi-agent approaches run the risk of being computationally inefficient or expensive to employ and need to justify these costs with noticable performance gains [17]. Given that different models and frameworks could generalize better to different tasks [93], we propose a framework that dynamically assigns the optimal collaboration strategy at inference time based on the complexity of the query. We apply our strategy to MDM, a task that requires teamwork and should benefit from multi-agent collaboration [72].

# 3 MDAgents: Medical Decision-making Agents

The design of MDAgents (Figures 1 and 2) incorporates four stages: 1) Medical Complexity Check - The system evaluates the medical query, categorizing it as low, moderate, or high complexity based on clinical decision-making techniques [6, 7, 21, 71, 84]. 2) Expert Recruitment - Based on complexity, the framework activates a single Primary Care Clinician (PCC) for low complexity issues, or a Multi-disciplinary Team (MDT) or Integrated Care Team (ICT) for moderate or high complexities [7, 18, 21, 34, 45, 71]. 3) Analysis and Synthesis - Solo queries use prompting techniques like Chain-of-Thought (CoT) and Self-Consistency (SC). MDTs involve multiple LLM agents forming a consensus, while ICTs synthesize information for the most complex cases. 4) Decision-making - The final stage synthesizes all inputs to provide a well-informed answer to the medical query.

![](images/4d2f9d9f9c764606c625a62465b672c96d5fc0462ac2c35ecf72d8f2c7b9cc8c.jpg)  
Question: What does the image of Gross Sectioned Myelolipoma demonstrate?

Low Moderate High

# Reason

# Step 1. Complexity Check

![](images/2637c465261afdffd210ee502894c01de75f0a8c36c139d8ac28afcdd350fd02.jpg)  
Step 2. Expert Recruitment   
Step 3. Initial Assessment   
Step 5. Review and Final Decision

: While the identification of gross pathology might seem straightforward to a specialist in pathology, the distinction between fibrosis, hemorrhage, and other conditions can sometimes be subtle and subject to interpretation …

? (1) Answer: (A) fibrosis ® (2) Answer: (A) fibrosis & (3) Answer: (C) hemorrhage 萬 (4) Answer: (C) hemorrhage 8 (5) Answer: (A) fibrosis

# GP Moderator

![](images/a9ff2d3f66a0b264379cb954c118d8f1d586f80d51473161e677bc4097364051.jpg)  
Step 4. Collaborative Discussion

(…) while the expertise of Agent 1 (Pathologis $\textcircled { < } 1$ in tissue examination is crucial and their argument for (A) fibrosis is noted, the gross pathological features described by Agent 3 (Surgeon $8 :$ ) and Agent 4 (Oncologist $\textsuperscript { \textregistered } )$ seem to align more closely with the typical presentation of a hemorrhage. Furthermore, hemorrhage is a welldocumented complication in myelolipomas, particularly larger ones or those subjected to trauma.

Therefore, based on the analysis of the conversation history, the final decision is aligned with interpretation (C) hemorrhage (red arrow) as argued by Agent 3 (Surgeon $8 .$ ) in Round 1, Turns 1 and 3, and Agent 4 (Oncologist $\tilde { \left. s \right. } _ { } \mathrm { ~ , ~ }$ in Round 1, Turn 2. This conclusion is supported by the gross pathological features consistent with hemorrhage rather than fibrosis, despite the reasoned opinions for fibrosis by Agents 1 (Pathologist ), 2 (Radiologist ), and 5 (Endocrinologist $\textcircled { < }$ ).

Figure 2: Illustrative example of MDAgents in a moderate complexity case from the PMC-VQA dataset. More detailed case studies can be found in Figure 11 and 12 in the Appendix.

# 3.1 Agent Roles

Moderator. The moderator agent functions as a general practitioner (GP) or emergency department doctor who first triages the medical query. This agent assesses the complexity of the problem and determines whether it should be handled by a single agent, a MDT, or an ICT. The moderator ensures the appropriate pathway be selected based on the query’s complexity and oversees the entire decision-making process.

Recruiter. The recruiter agent is responsible for assembling the appropriate team of specialist agents based on the complexity assessment of the moderator. The recruiter may assign a single PCP agent for low-complexity cases, while MDT or ICT with relevant expertise will be formed for moderate and high-complexity cases.

General Doctor/Specialist. These agents are domain-specific or general physicians recruited by the recruiter agent. Depending on the complexity of the case, they may work independently or as part of a team. General physicians handle less complex, routine cases, whereas specialists are recruited for their specific expertise in more complex scenarios. These agents engage in the collaborative decision-making process, contributing their specialized knowledge to reach a consensus or provide detailed reports for high-complexity cases.

# 3.2 Medical Complexity Classification (Line 1 of Algorithm 1, Appendix C.2)

The first step in the MDAgents framework is to determine the complexity of a given medical query $q$ by the moderator LLM which functions as a generalist practitioner (GP). The moderator aims to act as a classifier to return the complexity level of the given medical query, it is provided with the information on how medical complexity should be defined and is instructed to classify the query into one of three different complexity levels:

1. Low - Simple, well-defined medical issues that can be resolved by a single PCP agent. These typically include common, acute illnesses or stable chronic conditions where the medical needs are predictable and require minimal interdisciplinary coordination.   
2. Moderate - The medical issues involve multiple interacting factors, necessitating a collaborative approach among an MDT. These scenarios require integration of diverse medical knowledge areas and coordination between specialists through consultation to develop effective care strategies.   
3. High - Complex medical scenarios that demand extensive coordination and combined expertise from an ICT. These cases often involve multiple chronic conditions, complicated surgical or trauma cases, and decision-makings that integrates specialists from different healthcare departments.

# 3.3 Expert Recruitment (Line 3, 7, 17 of Algorithm 1)

Given a medical query, the goal of the recruiter LLM is to enlist domain experts as individuals, in groups, or as multiple teams, based on the complexity levels determined by the moderator LLM. Specifically, we assign medical expertise and roles to multiple LLMs, instructing them to either act independently as solo medical agents or collaborate with other medical experts in a team. In Figure 9 in the Appendix, we also provide frequently recruited agents for each benchmark as a reference.

# 3.4 Medical Collaboration and Refinement

The initial assessment protocol of our decision-making framework categorizes query complexity into low, moderate, and high. This categorization is grounded in established medical constructs such as acuity [25] for straightforward cases, comorbidity [69] and case management complexity [13] for intermediate and multi-disciplinary care requirements, and severity of illness [16] for high complexity cases requiring comprehensive management. We outlines the specific refinement approach:

Low - Straightforward cases (Line 2-4 of Algorithm 1). For queries classified under Low complexity, characterized by straightforward clinical decision pathways, a single PCP agent (Figure 10 (a)) is deployed by the definition in Section 3.2. The domain expert who is recruited by recuriter LLM, applies few-shot prompting to the problem. The output answer, denoted as ans, is directly obtained from the agent’s response to $Q$ without the need for iterative refinement, formalized as $a n s = A g e n t ( Q )$ , with Agent representing the engaged PCP agent.

Moderate - Intermediate complexity cases (Line 6-14 of Algorithm 1). In addressing more complex queries, the utilization of an MDT (Figure 10 (b) and (c)) approach has been increasingly recognized for its effectiveness in producing comprehensive and nuanced solutions [45]. The MDT framework leverages the collective expertise of professionals from diverse disciplines, facilitating a holistic examination of the query at hand. This collaborative method is particularly advantageous in scenarios where the complexity of a problem transcends the scope of a single domain, necessitating a fusion of insights from various specialties [7, 71]. The MDT approach not only enhances decision-making quality through the integration of multidimensional perspectives but also significantly improves the adaptability and efficiency of the problem-solving process [21].

Building upon this foundation, our framework specifically addresses queries of moderate complexity through a structured, multi-tiered collaborative approach. An MDT recruited by recruiter LLM (see Figure 10 in Appendix) starts an iterative discussion process aimed at reaching a consensus with at most $R$ rounds (Line 10-12). For each round $r \in R$ , agents $A _ { i } , i \in { 1 , \dots , N }$ indicate participation and preferred interlocutors. The system facilitates message exchanges for $T$ turns. If consensus is not reached and agents agree to continue, a new round begins with access to previous conversations. For every round, consensus within the MDT is determined by parsing and comparing their opinions. In the event of a disagreement, the moderator agent, consistent with the one described in Section 3.2 reviews the MDT’s discourse and formulates feedback for each agent.

High - Complex care cases (Line 17-24 of Algorithm 1). In contrast to the MDT approach, the ICT (Figure 10 (d)) paradigm is essential for addressing the highest tier of query complexity in healthcare. This structured progression through the ICT ensures a depth of analysis that is specialized and focused at each stage of the decision-making process. Beginning with the Initial Assessment Team, moving through various diagnostic teams, and culminating with the Final Review & Decision Team, our ICT model aligns specialist insights into a cohesive narrative that informs the ultimate decision (Appendix Algorithms 1 Lines 19-21). A key component of this process is the report generation process described in Appendix with the prompt, where each team, led by a lead clinician, collaboratively produces a comprehensive report synthesizing their findings. This phased approach, supported by evidence from recent healthcare studies, has been shown to enhance the precision of clinical decision-making, as each team builds upon the foundation laid by the previous, ensuring a meticulous and refined examination of complex medical cases [34]. The resultant reports, accumulating throughout the ICT process, are not only reflective of comprehensive medical evaluations but also of a systematic and layered analysis that is critical in the management of intricate health scenarios [18].

# 3.5 Decision-making

In the final stage of our framework, the decision-maker agent synthesizes the diverse inputs generated throughout the decision-making process to arrive at a well-informed final answer to the medical query $q$ . This synthesis involves several components depending on the complexity level of the query:

1. Low: Directly utilizes the initial response from the primary decision-making agent.   
2. Moderate: Incorporates the conversation history (Interaction) between the recruited agents to understand the nuances and disagreements in their responses.   
3. High: Considers detailed reports (Reports) generated by the agents, which include comprehensive analyses and justifications for their diagnostic suggestions.

The decision-making process is formulated as $a n s = A g e n t ( \cdot )$ where the final answer, ans is determined by integrating the outputs from analysis and synthesis step based on its medical complexities. This integration employs ensemble techniques such as temperature ensembles to ensure the decision is robust and reflects a consensus among the models when applicable (see Appendix C.2 for details).

Table 2: Accuracy $( \% )$ on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents the best and Underlined represents the second best performance for each benchmark and model. All benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in Appendix.   

<html><body><table><tr><td rowspan="2">Category</td><td rowspan="2">Method</td><td colspan="5">MedicalKnowledgeRetrievalDatasets</td></tr><tr><td>MedQA</td><td>PubMedQA</td><td>Path-VQA 00</td><td>PMC-VQA 0</td><td>MedVidQA 0</td></tr><tr><td rowspan="6">Single-agent</td><td>Zero-shot</td><td>75.0 ± 1.3</td><td>61.5 ± 2.2</td><td>57.9 ± 1.6</td><td>49.0 ± 3.7</td><td>37.9 ± 8.4</td></tr><tr><td>Few-shot</td><td>72.9 ± 11.4</td><td>63.1 ± 11.7</td><td>57.5 ± 4.5</td><td>52.2 ± 2.0</td><td>47.1 ± 8.6</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>48.6±55</td></tr><tr><td>+ CoT-SC [82]</td><td>82.5±.9</td><td>57.6±92</td><td>58.2±.1</td><td>51.3 ±52</td><td></td></tr><tr><td>ER [67]</td><td>81.9 ± 2.1</td><td>56.0 ± 7.0</td><td>61.4 ± 4.1</td><td>52.7 ± 2.9</td><td>48.5 ± 4.1</td></tr><tr><td>Medprompt [59]</td><td>82.4 ± 5.1</td><td>51.8 ± 4.6</td><td>59.2 ± 5.7</td><td>53.4 ± 7.9</td><td>44.5 ± 2.0</td></tr><tr><td rowspan="6">( Multi-modet)</td><td>Majority Voting</td><td>80.6 ± 2.9</td><td>72.2 ± 6.9</td><td>56.9 ± 19.7</td><td>36.8 ± 6.7</td><td>50.8 ± 7.4</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>5454</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>70</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Meta-Prompting[70]</td><td>80.6 ± 1.2</td><td>73.3 ± 2.3</td><td>55.3 ± 2.3</td><td>42.6 ± 4.2</td><td></td></tr><tr><td></td><td>8</td><td></td><td></td><td>33</td><td></td></tr><tr><td rowspan="2">(Multi-modet)</td><td></td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Adaptive</td><td rowspan="2">MDAgents (Ours)</td><td>88.7± 4.0</td><td>75.0 ± 1.0</td><td>65.3 ± 3.9</td><td>56.4 ± 4.5</td><td>56.2 ± 6.7</td></tr><tr><td colspan="5">Clinical Reasoning and Diagnostic Datasets</td></tr><tr><td rowspan="2">Category</td><td rowspan="2">Method</td><td>DDXPlus</td><td>SymCat 0</td><td>JAMA 0</td><td>MedBullets 0</td><td>MIMIC-CXR 00</td></tr><tr><td>0</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="6">Single-agent</td><td>Zero-shot</td><td>70.3 ± 2.0</td><td>88.7 ± 2.3</td><td>62.0 ± 2.0</td><td>67.0 ± 1.4</td><td>40.0 ± 5.3</td></tr><tr><td>Few-shot</td><td>69.4 ± 1.0</td><td>86.7 ± 3.1</td><td>69.0 ± 4.2</td><td>72.0 ± 2.8</td><td>35.3 ± 5.0</td></tr><tr><td>+ CoT[83]</td><td>72.7 ± 7.7</td><td>78.0 ± 2.0</td><td>66.0 ± 5.7</td><td>70.0 ± 0.0</td><td>36.2 ± 5.2</td></tr><tr><td>+ CoT-SC [82]</td><td>52.1 ± 6.4</td><td>83.3 ± 3.1</td><td>68.0 ± 2.8 71.0 ± 1.4</td><td>76.0 ± 2.8</td><td>51.7 ± 4.0</td></tr><tr><td>ER [67] Medprompt [59]</td><td>61.3 ± 2.4</td><td>82.7 ± 2.3</td><td>70.7 ± 4.3</td><td>76.0 ± 5.7 71.0 ± 1.4</td><td>50.0 ± 0.0</td></tr><tr><td></td><td>59.5 ± 17.7</td><td>87.3 ± 1.2</td><td></td><td></td><td>53.4 ± 4.3</td></tr><tr><td rowspan="6">Multi-agent (Single-model)</td><td>Majority Voting</td><td>67.8 ± 4.9</td><td>91.9 ± 2.2</td><td>70.0 ± 5.7</td><td>70.0 ± 0.0</td><td>49.5 ± 10.7</td></tr><tr><td>Weighted Voting</td><td>65.9 ± 3.3</td><td>90.5 ± 2.9</td><td>66.1 ± 4.1</td><td>66.0 ± 5.7</td><td>53.5 ± 2.2</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MedA Cots [72]</td><td>67.1 ± 6.7</td><td>78.0±11.8</td><td>61.0 ± 5.6</td><td>66.0±124</td><td>45.3±.</td></tr><tr><td>Meta-Prompting [70]</td><td>52.6 ± 6.1</td><td>77.3± 2.3</td><td>64.7 ± 3.1</td><td>49.3 ± 1.2</td><td>42.0 ± 4.0</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="3">Multi-agent (Multi-model)</td><td>RuconiTe[I01</td><td>67.4±1748</td><td>90.6±3.</td><td>60.7±5.7</td><td>59.3±7</td><td>3334</td></tr><tr><td>DyLAN [51]</td><td>56.4 ± 2.9</td><td>75.3 ± 4.6</td><td>60.1 ± 3.1</td><td>57.3 ± 6.1</td><td>38.7 ± 1.2</td></tr><tr><td>MDAgents (Ours)</td><td>77.9 ± 2.1</td><td>93.1 ± 1.0</td><td>70.9 ± 0.3</td><td>80.8 ± 1.7</td><td>55.9 ± 9.1</td></tr></table></body></html>

$*$ CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement $\ast \textcircled { \ast }$ : text-only, $\textcircled{1}$ : image+text, $\bigcirc$ : video+text \* All experiments were tested with 3 random seeds

# 4 Experiments and Results

In this section, we evaluate our framework and baseline methods across different medical benchmarks in Solo, Group, and Aaptive settings. Our experiments and ablation studies highlight the framework’s performance, demonstrating robustness and efficiency by modulating agent numbers and temperatures. Results also show a beneficial convergence of agent opinions in collaborative settings.

# 4.1 Setup

To verify the effectiveness of our framework, we conduct comprehensive experiments with baseline methods on ten datasets including MedQA, PubMedQA, DDXPlus, SymCat, JAMA, MedBullets, Path-VQA, PMC-VQA, MIMIC-CXR and MedVidQA. A detailed explanation and statistics for each dataset are deferred to Appendix A and Figure 8. We use 50 samples per dataset for testing, and the inference time for each complexity was: low - 14.7s, moderate - $9 5 . 5 s$ , and high - 226s in average. For all the quantitative experiments in this section, we compare three settings: (1) Solo: Using a single LLM agent in the decision-making state. (2) Group: Implementing multi-agents to collaborate during the decision-making process. (3) Adaptive: Our proposed method MDAgents, adaptively constructs the inference structure from PCP to MDT and ICT. We use 3-shot prompting for low-complexity cases and zero-shot prompting for moderate and high-complexity cases across all settings.

Medical Question Answering With MedQA [35], PubMedQA [36], MedBullets [9], and JAMA [9], we focus on question answering through text, involving both literature-based and conceptual medical knowledge questions. Specifically, PubMedQA tasks models to answer questions using abstracts from PubMed, requiring synthesis of biomedical information. MedQA tests the model’s ability to understand and respond to multiple-choice questions derived from medical educational materials and examinations. MedBullets provides USMLE Step 2/3 type questions that demand the application of medical knowledge and clinical reasoning. JAMA Clinical Challenge presents challenging real-world clinical cases with diagnosis or treatment decision-making questions, testing the model’s clinical reasoning (Figure 8 in Appendix shows complexity distribution for each dataset) Diagnostic Reasoning DDXPlus [73] and SymCat [2] involve clinical vignettes that require differential diagnosis, closely mimicking the diagnostic process of physicians. These tasks test the model’s ability to reason through symptoms and clinical data to suggest possible medical conditions, evaluating the AI’s diagnostic reasoning abilities similar to a clinical setting. SymCat [2] uses synthetic patient records constructed from a public disease-symptom data source and is enhanced with additional contextual information through the NLICE method.

Medical Visual Interpretation Path-VQA [30], PMC-VQA [95], MedVidQA [29], and MIMICCXR [3] challenge models to interpret medical images and videos, requiring integration of visual data with clinical knowledge. PathVQA focuses on answering questions based on pathology images, testing AI’s capability to interpret complex visual information from medical images. PMC-VQA evaluates AI’s proficiency in deriving answers from both text and images found in scientific publications. MedVidQA extends to video-based content, where AI models need to process information from medical procedure videos. MIMIC-CXR-VQA specifically targets chest radiographs, utilizing a diverse and large-scale dataset designed for visual question-answering tasks in the medical domain.

# Baseline Methods

• Solo: The baseline methods considered for the Solo setting include the following: Zero-shot [41] directly incorporates a prompt to facilitate inference, while Few-shot [8] involves a small number of examples. Few-shot CoT [83] integrates rationales before deducing the answer. Few-shot CoT-SC [82] builds upon Few-shot CoT by sampling multiple chains to yield the majority answer. Ensemble Refinement (ER) [67] is a prompting strategy that conditions model responses on multiple reasoning paths to bolster the reasoning capabilities of LLMs. Medprompt [59] is a composition of several prompting strategies that enhances the performance of LLMs and achieves state-of-the-art results on multiple benchmark datasets, including medical and non-medical domains.

• Group: We tested five group decision-making methods: Voting [82], MedAgents [72], Reconcile [10], AutoGen [86], and DyLAN [51]. Autogen was based on four agents, with one User, one Clinician, one Medical Expert, and one Moderator, with one response per agent [86]. DyLAN setup followed the base implementations of four agents with no specific roles and four maximum rounds of interaction [51]. While the methods support multiple models, GPT-4 was used for all agents.

# 4.2 Results

In Table 2, we report the classification accuracy on MedQA, PubMedQA, DDXPlus, SymCat, JAMA, MedBullets, Path-VQA, PMC-VQA and MedVidQA dataset. We compare our method (Adaptive) with several baselines in both Solo and Group settings.

Adaptive method outperforms Solo and Group settings. As depicted in Figure 4 and Table 2, MDAgents significantly outperforms $( \mathtt { p } < 0 . 0 5 )$ both Solo and Group setting methods, showing best performance in 7 out of 10 medical benchmarks tested. This reveals the effectiveness of adaptive strategies integrated within our system, particularly when navigating through the text-only (e.g., DDXPlus where it outperformed the best performance of single-agent by $5 . 2 \%$ and multi-agent by $9 . 5 \% \%$ and text-image datasets (e.g., Path-VQA, PMC-VQA and MIMIC-CXR). Our approach not only comprehends textual information with high precision but also adeptly synthesizes visual data, a pivotal capability in medical diagnostic evaluations.

![](images/fba9abe5756a4ee468efb2a3c227fd783f0d19706592b65b780ea593c16c5af4.jpg)  
Figure 3: Experiment with the MedQA dataset ( $N = 2 5$ randomly sampled questions). (a) LLM’s capability to classify complexity. (b-d) Evaluating 25 medical problems by solving each one 10 times at various complexity levels. The $\mathbf { \boldsymbol { X } }$ -axis represents the accuracy achieved for each problem, while the y-axis shows the number of problems that reached that level of accuracy.

Why Do Adaptive Decision-making Framework Work Well? It is important to accurately assign difficulty levels to medical questions. For instance, if a medical question is obviously easy, utilizing a team of specialists (such as an IDT) might be excessive and potentially lead to overly pessimistic approaches. Conversely, if a difficult medical question is only tackled by a PCP, the problem might not be adequately addressed. The core issue here is the LLM’s capability to classify the difficulty of medical questions appropriately. If an LLM inaccurately classifies the difficulty level, the chosen medical solution may not be suitable, potentially leading to the wrong decision making. Therefore, understanding what constitutes an appropriate difficulty level is essential.

We hypothesize that an LLM, functioning as a classifier, will select the optimal complexity level for each MDM problem. This hypothesis is supported by Figure 3, which illustrates that the model appropriately matches the complexity levels; low, moderate, and high of the given problem. To determine this, we assessed the accuracy of solutions across various difficulty levels. Specifically, we evaluated 25 medical problems by repeating each problem for 10 times at each difficulty level. By measuring the success rate, we aimed to identify the difficulty level that yielded the highest accuracy. This approach ensures that the LLM’s complexity classification aligns with the most effective and accurate medical solutions, thereby optimizing the application of medical expertise to each question. Formally, for any given problem $P$ , we denote the probability that the correct answer can be solved at a specific complexity level as $p _ { \mathrm { c o m p l e x i t y - l e v e l } } ( P )$ , where complexity-level $\in \{ \mathrm { l o w } , \mathrm { m o d e r a t e } , \mathrm { h i g h } \}$ . arg $\bar { \operatorname* { m a x } } ( P ) \in \mathsf { \bar { \{ l o w , m o d e r a t e , h i g h \} } }$ refers to the complexity level that has the highest probability among $p _ { \mathrm { l o w } } ( P )$ , $p _ { \mathrm { m o d e r a t e } } ( P )$ , and $p _ { \mathrm { h i g h } } ( P )$ . Similarly, $\arcsin ( P )$ is the complexity level with the lowest probability, and arg $\mathrm { m i d } ( \bar { P } )$ is the one with the middle probability. We denote $a , \ b$ , and $c$ as the probabilities that the LLM selects the complexity levels corresponding to arg max, arg mid, and arg min, respectively. Thus, the accuracy of our system for problem $P$ can be described by $a \cdot p _ { \mathrm { a r g m a x } } ( P ) + b \cdot p _ { \mathrm { a r g m i d } } ( P ) + c \cdot p _ { \mathrm { a r g m i n } } ( P )$ , and the overall accuracy is given by $\mathbb { E } _ { P }$ [ $a \cdot p _ { \mathrm { a r g m a x } } ( P ) + b \cdot p _ { \mathrm { a r g m i d } } ( P ) + c \cdot p _ { \mathrm { a r g m i n } } ( P ) ]$ . The estimated values of $a , b , c$ are $a = 0 . 8 1 \pm 0 . 2 9$ , $b = 0 . 1 1 \pm 0 . 2 8$ , and $c = 0 . 0 8 \pm 0 . 1 6$ , which indicates that LLM can provide an optimal complexity level with probability at least $8 0 \%$ .

These findings suggest that a classifier LLM can implicitly simulate various complexity levels and optimally adapt to the complexity required for each medical problem, as shown in Figure 3. This ability to adjust complexity dynamically proves to be crucial for applying LLMs effectively in MDM contexts as shown by the competitiveness of our Adaptive approach.

Solo vs. Group Setting in MDM. The experimental results reveal distinct performance patterns between Solo and Group settings across various medical benchmarks. In simpler datasets like MedQA, solo methods, leveraging Few-shot CoT and CoT-SC, achieved up to $8 3 . 9 \%$ accuracy compared to the group’s best of $8 1 . 3 \%$ . Conversely, for more complex datasets like SymCat, group settings perform better, with SymCat showing $9 1 . 9 \%$ accuracy in the group settings versus $8 8 . 7 \%$ in solo settings. Notably, group settings (e.g. Weighted Voting, Reconcile) performed better in multi-modal datasets such as Path-VQA $( i m a g e + t e x t )$ , MedVidQA (video $+ \ t e x t )$ , and MIMIC-CXR $( i m a g e + t e x t )$ , high

![](images/4a6f72caad3280552786ee0da6da437cd629dbe7180d41a6713938d30d11b495.jpg)  
Figure 4: Our method outperforms Solo and Group settings across different medical benchmarks.

lighting the advantage of collaborative process in complex tasks. This result aligns with findings from [4], which showed that pooled diagnoses from groups of physicians significantly outperformed those of individual physicians, with accuracy increasing as group size increased. Overall, solo settings outperformed group settings in four benchmarks, while group settings outperformed solo in six benchmarks. These results reveals that while solo methods excel in straightforward tasks, group settings provide better performance in complex, multi-faceted tasks requiring diverse expertise.

# 4.3 Ablation Studies

Impact of Complexity Selection. We evaluate the importance of the complexity assessment and adaptive process through ablation studies (Figure 5). Our adaptive method significantly outperforms static complexity assignments across different modality benchmarks. For text-only queries, the Adaptive method achieves an accuracy of $8 1 . 2 \%$ , significantly higher than low $( 6 4 . 2 \% )$ , moderate $( 7 1 . 6 \% )$ , and high $( 6 5 . 8 \% )$ settings. Interestingly, $64 \%$ of the text-only queries were classified as high complexity, indicating that many text-based queries required in-depth analysis with different expertise. In the image $+$ text modality, the Adaptive method classified $5 5 \%$ of the queries as low complexity, suggesting that the visual information often provides clear and straightforward cues that simplify the decision-making process. Finally, for video $+$ text queries, $87 \%$ of these queries were classified as low complexity, reflecting that the dynamic visual data in conjunction with text can often be straightforwardly interpreted. However, further evaluation on more challenging video medical datasets is needed, as MedVidQA contains relatively less complex medical knowledge.

Impact of Moderator’s Review and RAG Table 3 examines the impact of incorporating external medical knowledge and moderator reviews into the MDAgents framework on accuracy. MedRAG [89] is a systematic toolkit for Retrieval-Augmented Genera

Table 3: Ablations for the impact of moderator’s review and MedRAG. The Accuracy were averaged accuracy across all datasets.   

<html><body><table><tr><td>Method</td><td>Avg. Accuracy (%)</td></tr><tr><td>MDAgents (Ours)</td><td>71.8</td></tr><tr><td>+MedRAG</td><td>75.2(个4.7 %)</td></tr><tr><td>+Moderator'sReview</td><td>77.6 (↑ 8.1 %)</td></tr><tr><td>+Moderator'sReview&MedRAG</td><td>80.3 (↑11.8 %)</td></tr></table></body></html>

tion (RAG) that leverages various corpora; biomedical, clinical and general medicine, to provide comprehensive knowledge. The baseline accuracy of MDAgents is $7 1 . 8 \%$ . Integrating MedRAG increases accuracy to $7 5 . 2 \%$ (up $4 . 7 \%$ ), while the moderator’s review alone raises it to $7 7 . 6 \%$ (up $8 . 1 \%$ . The combined use of both methods achieves the highest accuracy at $8 0 . 3 \%$ (up $1 1 . 8 \%$ ). The results indicate that MedRAG and moderator review both enhance performance, with their combined effect being synergistic. This highlights that leveraging recent external knowledge and structured feedback mechanisms is crucial for refining and converging on accurate medical decisions. This improvement underscores the importance of a hybrid strategy, aligning with real-world practices of continuous learning and expert consultation to optimize performance in medical applications.

# 4.4 Impact of Number of Agents in Group Setting.

Our experiment with varying the number of agents in a collaborative Group setting (Appendix Figure 6 (a-b)) shows that a higher number of agents does not lead to better performance. Rather, our Adaptive method achieves optimal performance with fewer agents $N { = } 3$ , peak accuracy of $8 3 . 5 \% \AA$ 0 by intelligently calibrating the number of collaborating agents. This not only indicates efficiency in decision-making but also computational and economic benefits, considering the reduced number of API calls needed, especially when contrasted with the Solo and Group settings.

With regards to computational efficiency, the Solo setting (5-shot CoT-SC) resulted in a 6.0 and Group setting (MedAgents with $N { = } 5$ ) resulted in a 20.3 API calls, suggesting a high computational cost without a corresponding increase in accuracy. On the other hand, our Adaptive method exhibits a more economical use of resources, demonstrated by fewer API calls (9.3 with $N { = } 3$ ) while maintaining high accuracy, a critical factor in deploying scalable and cost-effective medical AI solutions.

# 4.5 Robustness of MDAgents with different parameters.

Our Adaptive approach shows resilience to changes in temperature (Appendix Figure 6 (c), low $( T { = } 0 . 3 )$ and high $\left( T \mathrm { = } 1 . 2 \right) ,$ ) with performance improving under higher temperatures. This suggests that our model can utilize the creative and diverse outputs generated at higher temperatures to enhance decision-making, a property that is not as pronounced in the Solo and Group conditions. This robustness is particularly valuable in real-world medical domains with high uncertainty and ambiguity in datasets [15]. Additionally, studies have shown that creative diagnostic approaches can mitigate cognitive biases and improve diagnostic accuracy [66], while fostering flexibility and adaptability in decision-making [19]. These insights support the enhanced performance observed under higher temperatures in our framework. However, the future work should explore a wider range of temperatures to fully understand the robustness and adaptability of our approach.

![](images/25e16e501e3691a13da51366fd6cf315d8e12fbb77b1c136b18fb37dbfa30ee3.jpg)  
Figure 5: Impact of complexity selection of the query. Accuracy of each ablation on text-only (left), text+image (center) and text+video (right) benchmarks are reported.

![](images/ac2c4888b7f6f8e93fca0c947a9a65fb0598e2a8defb7c1d923deae7758aec5b.jpg)  
Figure 6: Impact of the number of agents on (a) Accuracy, (b) Number of API Calls on medical benchmarks with GPT-4 (V) and (c) Performance of three different settings under low $\scriptstyle ( T = 0 . 3 )$ and high $\left( T \mathrm { = } 1 . 2 \right)$ temperatures on medical benchmarks. Our Adaptive setting shows better robustness to different temperatures and even takes advantage of higher temperatures.

![](images/3cc35bd43a5090dca9257df18ec6145cf401fc9ed631e1c2e080ceac2acf9d8e.jpg)  
Figure 7: An illustration of consensus entropy in group collaboration process of MDAgents (w/ Gemini-Pro (Vision), $N { = } 3 0$ for each dataset) on medical benchmarks with different modality inputs.

# 4.6 Convergence Trends in Consensus Dynamics

There is clear trend towards consensus among MDAgents cross various data modalities (Figure 7). The text+video modality demonstrates a rapid convergence, reflecting the agents’ efficient processing of combined textual and visual cues. On the other hand, the text+image and text-only modalities display a more gradual decline in entropy, indicating a progressive narrowing of interpretative diversity among the agents. Despite the differing rates and initial conditions, all modalities exhibit convergence of agent opinions over time. This uniformity in reaching consensus highlights the MDAgents’ capability to integrate and reconcile information. Please refer to Appendix B for a detailed explanation of the entropy calculation.

# 5 Conclusion

This paper introduces MDAgents, a framework designed to enhance the utility of LLMs in complex medical decision-making by dynamically structuring effective collaboration models. To reflect the nuanced consultation aspects in clinical settings, MDAgents adaptively assigns LLMs either to roles independently or within groups, depending on the task’s complexity. This emulation of real-world medical decision processes has been comprehensively evaluated, with MDAgents outperforming previous solo and group methods in 7 out of 10 medical benchmarks. The case study illustrates the practical efficacy and collaborative dynamics of our proposed framework, providing insights into how differing expert opinions are synthesized to reach a more accurate diagnosis. This is evidenced by our agents’ ability to converge on the correct diagnosis despite initially divergent perspectives. Ablation studies further elucidate the individual contributions of agents and strategies within the system, revealing the critical components and interactions that drive the framework’s success. By harnessing the strength of multi-modal reasoning and fostering a collaborative process among LLM agents, our framework opens up new possibilities for enhancing LLM-assisted medical diagnosis systems, pushing the boundaries of automated clinical reasoning.