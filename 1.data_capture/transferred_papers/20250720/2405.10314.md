# CAT3D: Create Anything in 3D with Multi-View Diffusion Models

Ruiqi ${ \bf G a o } ^ { 1 * }$ Aleksander Hołyn´ski1\* Philipp Henzler2 Arthur Brussee1 Ricardo Martin-Brualla2 Pratul Srinivasan1 Jonathan T. Barron1 Ben Poole1\* 1Google DeepMind 2Google Research \*equal contribution

![](images/61c3a8378f307ac43094daa442af4fdf48e6d20a7ccfe95bc272cf37f117a82f.jpg)  
Figure 1: CAT3D enables 3D scene creation from any number of generated or real images.

# Abstract

Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene. We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model. Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene. These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time. CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation.

# 1 Introduction

The demand for 3D content is higher than ever, since it is essential for enabling real-time interactivity for games, visual effects, and wearable mixed reality devices. Despite the high demand, high-quality 3D content remains relatively scarce. Unlike 2D images and videos which can be easily captured with consumer photography devices, creating 3D content requires complex specialized tools and a substantial investment of time and effort.

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

Fortunately, recent advancements in photogrammetry techniques have greatly improved the accessibility of 3D asset creation from 2D images. Methods such as NeRF [1], Instant-NGP [2], and Gaussian Splatting [3] allow anyone to create 3D content by taking photos of a real scene and optimizing a representation of that scene’s underlying 3D geometry and appearance. The resulting 3D representation can then be rendered from any viewpoint, similar to traditional 3D assets. Unfortunately, creating detailed scenes still requires a labor-intensive process of capturing hundreds to thousands of photos. Captures with insufficient coverage of the scene can lead to an ill-posed optimization problem, which often results in incorrect geometry and appearance and, consequently, implausible imagery when rendering the recovered 3D model from novel viewpoints.

Reducing this requirement from dense multi-view captures to less exhaustive inputs, such as a single image or text, would enable more accessible 3D content creation. Prior work has developed specialized solutions for different input settings, such as geometry regularization techniques targeted for sparse-view reconstruction [4, 5], feed-forward models trained to create 3D objects from single images [6], or the use of image-conditioned [7] or text-conditioned [8] generative priors in the optimization process—but each of these specialized methods comes with associated limitations in quality, efficiency, and generality.

In this paper, we instead focus on the fundamental problem that limits the use of established 3D reconstruction methods in the observation-limited setting: an insufficient number of supervising views. Rather than devising specialized solutions for different input regimes [8, 9, 7], a shared solution is to instead simply create more observations—collapsing the less-constrained, under-determined 3D creation problems to the fully-constrained, fully-observed 3D reconstruction setting. This way, we reformulate a difficult ill-posed reconstruction problem as a generation problem: given any number of input images, generate a collection of consistent novel observations of the 3D scene. Recent video generative models show promise in addressing this challenge, as they demonstrate the capability to synthesize video clips featuring plausible 3D structure [10, 11, 12, 13, 14, 15]. However, these models are often expensive to sample from, challenging to control, and limited to smooth and short camera trajectories.

Our system, CAT3D, instead accomplishes this through a multi-view diffusion model trained specifically for novel-view synthesis. Given any number of input views and any specified novel viewpoints, our model generates multiple 3D-consistent images through an efficient parallel sampling strategy. These generated images are subsequently fed through a robust 3D reconstruction pipeline to produce a 3D representation that can be rendered at interactive rates from any viewpoint. We show that our model is capable of producing photorealistic results of arbitrary objects or scenes from any number of captured or synthesized input views in as little as one minute. We evaluate our work across various input settings, ranging from sparse multi-view captures to a single captured image, and even just a text prompt (by using a text-to-image model to generate an input image from that prompt). CAT3D outperforms prior works for measurable tasks (such as the multi-view capture case) on multiple benchmarks, and is an order of magnitude faster than previous state-of-the-art. For tasks where empirical performance is difficult to measure (such as text-to-3D and single image to 3D), CAT3D compares favorably with prior work in all settings.

# 2 Related Work

Creating entire 3D scenes from limited observations requires 3D generation, e.g., creating content in unseen regions, and our work builds on the ever-growing research area of 3D generative models [16]. Due to the relative scarcity of 3D datasets, much research in 3D generation is centered on transferring knowledge learned by 2D image-space priors, as 2D data is abundant. Our diffusion model is built on the recent development of video and multi-view diffusion models that produce highly consistent novel views. We show that pairing these models with 3D reconstruction, similar to [17, 18], enables efficient and high quality 3D creation. Below we discuss how our work is related to several areas of prior work.

2D priors. Given limited information such as text, pretrained text-to-image models can provide a strong generative prior for text-to-3D generation. However, distilling the knowledge present in these image-based priors into a coherent 3D model currently requires an iterative distillation approach. DreamFusion [8] introduced Score Distillation Sampling (SDS) to synthesize 3D objects (as NeRFs) from text prompts. Research in this space has aimed to improve distillation strategies [19, 20, 21, 22,

![](images/aeeb985da9e34c35d042d3e44c3f550892a61195703a2a24df33959189aecbbb.jpg)  
Figure 2: Qualitative results (renders): CAT3D can create high-quality 3D objects or scenes from a number of input modalities: an input image generated by a text-to-image model (top row), a single captured real image (middle row), and multiple captured real images (bottom row).

23], swap in other 3D representations [24, 25, 26, 27, 28], and amortize the optimization process [29]. Using text-based priors for single-image-to-3D has also shown promise [30, 31, 32], but requires a complex balancing of the image observation with additional constraints. Incorporating priors such as monocular depth models or inpainting models has been useful for creating 3D scenes, but tends to result in poor global geometry [33, 34, 35, 36].

2D priors with camera conditioning. While text-to-image models excel at generating visually appealing images, they lack precise control over the pose of images, and thus require a time-consuming 3D distillation process to encourage the 3D model to conform to the 2D prior. To overcome this limitation, several approaches train or fine-tune generative models with explicit image and pose conditioning [7, 37, 38, 39, 40, 41, 42]. These models provide stronger priors for what an object or scene should look like given text and/or input image(s), but they also model all output views independently. In cases where there is little uncertainty in what should appear at novel views, reasoning about generated views independently is sufficient for efficient 3D reconstruction [43]. But when there is some uncertainty exists, these top-performing methods still require expensive 3D distillation to resolve the inconsistencies between different novel views.

Multi-view priors. Modeling the correlations between multiple views provides a much stronger prior for what 3D content is consistent with partial observations. Methods like MVDream [44], ImageDream [9], Zero $1 2 3 + +$ [45], ConsistNet [46], SyncDreamer [47] and ViewDiff [48] fine-tune text-to-image models to generate multiple views simultaneously. CAT3D is similar in architecture to ImageDream, where the multi-view dependency is captured by an architecture resembling video diffusion models with 3D self-attention. Given this stronger prior, these papers also demonstrate higher quality and more efficient 3D extraction.

Video priors. Video diffusion models have demonstrated an astonishing capability of generating realistic videos [49, 50, 10, 12, 15, 13, 51], and are thought to implicitly reason about 3D. However, it remains challenging to use off-the-shelf video diffusion models for 3D generation for a number of reasons. Current models lack exact camera controls, limiting generation to clips with only smooth and short camera trajectories, and struggle to generate videos with only camera motion but no scene dynamics. Several works have proposed to resolve these challenges by fine-tuning video diffusion models for camera-controled or multi-view generation. For example, AnimateDiff [52] LoRA fine-tuned a video diffusion model with fixed types of camera motions, and MotionCtrl [53] conditioned the model on arbitrary specified camera trajectories. ViVid-1-to-3 [54] combines a novel view synthesis model and a video diffusion model for generating smooth trajectories. SVD-MV [55], IM-3D [17] and SV3D [55] further explored leveraging camera-controlled or multi-view video diffusion models for 3D generation. However, their camera trajectories are limited to orbital ones surrounding the center content. These approaches mainly focus on 3D object generation, and do not work for 3D scenes, few-view 3D reconstruction, or objects in context (objects that have not been masked or otherwise separated from the image’s background).

Feed-forward methods. Another line of research is to learn feed-forward models that take a few views as input, and output 3D representations directly, without an optimization process per instance [6, 56, 57, 58, 18, 59, 60]. These methods can produce 3D representations efficiently (within a few seconds), but the quality is often worse than approaches built on image-space priors.

# 3 Method

CAT3D is a two-step approach for 3D creation: first, we generate a large number of novel views consistent with one or more input views using a multi-view diffusion model, and second, we run a robust 3D reconstruction pipeline on the generated views (see Figure 3). Below we describe our multi-view diffusion model (Section 3.1), our method for generating a large set of nearly consistent novel views from it (Section 3.2), and how these generated views are used in a 3D reconstruction pipeline (Section 3.3).

![](images/0ba30792dd106e5800927d2b3249872733cc6f0700b53c3efcd8ca16bb8b239a.jpg)  
Figure 3: Illustration of the method. Given one to many views, CAT3D creates a 3D representation of the scene in as little as one minute. CAT3D has two stages: (1) generate a large set of synthetic views with a multi-view latent diffusion model conditioned on the input views and target poses; (2) run a robust 3D reconstruction pipeline on the observed and generated views. This decoupling of the generative prior and 3D reconstruction process results in efficiency improvements and reduced methodological complexity relative to prior work [7, 8, 42], while also improving visual quality.

# 3.1 Multi-View Diffusion Model

We train a multi-view diffusion model that takes a single or multiple views of a 3D scene as input and generates multiple output images given their camera poses (where “a view” is a paired image and its camera pose). Specifically, given $M$ conditional views containing $M$ images $\bar { \mathbf { I } ^ { \mathrm { c o n d } } }$ and their corresponding camera parameters $\mathbf { p } ^ { \mathrm { \bar { c o n d } } }$ , the model learns to capture the joint distribution of $N$ target images $\mathbf { I } ^ { \mathrm { t g t } }$ assuming their $N$ target camera parameters $\mathbf { p } ^ { \mathrm { t g t } }$ are also given:

$$
\begin{array} { r } { p \big ( \mathbf { I } ^ { \mathrm { t g t } } | \mathbf { I } ^ { \mathrm { c o n d } } , \mathbf { p } ^ { \mathrm { c o n d } } , \mathbf { p } ^ { \mathrm { t g t } } \big ) . } \end{array}
$$

Model architecture. Our model architecture is similar to video latent diffusion models (LDMs) [49, 11], but with camera pose embeddings for each image instead of time embeddings. Given a set of conditional and target images, the model encodes every individual image into a latent representation through an image variational auto-encoder [61]. Then, a diffusion model is trained to estimate the joint distribution of the latent representations given conditioning signals. We initialize the model from an LDM trained for text-to-image generation similar to [62] trained on web-scale image data, with an input image resolution of $5 1 2 \times 5 1 2 \times 3$ and latents with shape $6 4 \times 6 4 \times 8$ . As is often done in video diffusion models [50, 10, 11], the main backbone of our model remains the pretrained 2D diffusion model but with additional layers connecting the latents of multiple input images. As in [44], we use 3D self-attention (2D in space and 1D across images) instead of simple 1D self-attention across images. We directly inflate the existing 2D self-attention layers after every 2D residual block of the original LDM to connect latents with 3D self-attention layers while inheriting the parameters from the pre-trained model, introducing minimal amount of extra model parameters. We found that conditioning on input views through 3D self-attention layers removed the need for PixelNeRF [63] and CLIP image embeddings [64] used by the prior state-of-the-art model on few-view reconstruction, ReconFusion [7]. We use FlashAttention [65, 66] for fast training and sampling, and fine-tune all the weights of the latent diffusion model. Similar to prior work [10, 67], we found it important to shift the noise schedule towards high noise levels as we move from the pre-trained image diffusion model to our multi-view diffusion model that captures data of higher dimensionality. Concretely, following logic similar to [67], we shift the log signal-to-noise ratio by $\log ( N )$ towards the high signal-to-noise ratio region, where $N$ is the number of target views. Similar shifts have been adopted empirically by [11, 10]. For training, latents of target images are noise perturbed while latents of conditional images are kept as clean, and the diffusion loss is defined only on target images. A binary mask is concatenated to the latents along the channel dimension, to denote conditioning vs. target images. To deal with multiple 3D generation settings, we train a single versatile model that can model a total of 8 conditioning and target views $\begin{array} { r } { N + M = 8 , } \end{array}$ ), and randomly select the number of conditional views $M$ to be 1 or 3 during training, corresponding to 7 and 5 target views respectively. The noise schedule is shifted based on 5 target views. See Appendix B for more model details.

Camera conditioning. To condition on the camera pose, we use a camera ray representation (“raymap”) that is the same height and width as the latent representations [38, 68] and encodes the ray origin and direction at each spatial location. The rays are computed relative to the camera pose of the first conditional image, so our pose representation is invariant to rigid transformations of 3D world coordinates. Raymaps for each image are concatenated channel-wise onto the latents for the corresponding image.

# 3.2 Generating Novel Views

Given a set of input views, our goal is to generate a large set of consistent views to fully cover the scene and enable accurate 3D reconstruction. To do this, we need to decide on the set of camera poses to sample, and we need to design a sampling strategy that can use a multi-view diffusion model trained on a small number of views to generate a much larger set of consistent views.

Camera trajectories. Compared to 3D object reconstruction where orbital camera trajectories can be effective, a challenge of 3D scene reconstruction is that the views required to fully cover a scene can be complex and depend on the scene content. We empirically found that designing reasonable camera trajectories for different types of scenes is crucial to achieve compelling few-view 3D reconstruction. The camera paths must be sufficiently thorough and dense to fully-constrain the reconstruction problem, but also must not pass through objects in the scene or view scene content from unusual angles. In summary, we explore four types of camera paths based on the characteristic of a scene: (1) orbital paths of different scales and heights around the center scene, (2) forward facing circle paths of different scales and offsets, (3) spline paths of different offsets, and (4) spiral trajectories along a cylindrical path, moving into and out of the scene. See Appendix C for more details.

Generating a large set of synthetic views. A challenge in applying our multi-view diffusion model to novel view synthesis is that it was trained with a small and finite set of input and output views — just 8 in total. To increase the total number of output views, we cluster the target viewpoints into smaller groups and generate each group independently given the conditioning views. We group target views with close camera positions, as these views are typically the most dependent. For single-image conditioning, we adopt an autoregressive sampling strategy, where we first generate a set of 7 anchor views that cover the scene (similar to [42], and chosen using the greedy initialization from [69]), and then generate the remaining groups of views in parallel given the observed and anchor views. This allows us to efficiently generate a large set of synthetic views while still preserving both long-range consistency between anchor views and local similarity between nearby views. For the single-image setting, we generate 80 views, while for the few-view setting we use 480-960 views. See Appendix C for details.

Conditioning larger sets of input views and non-square images. To expand the number of views we can condition on, we choose the nearest $M$ views as the conditioning set, as in [7]. We experimented with simply increasing the sequence length of the multi-view diffusion architecture during sampling, but found that the nearest view conditioning and grouped sampling strategy performed better. To handle wide aspect ratio images, we combine square samples from square-cropped input views with wide samples cropped from input views padded to be square.

# 3.3 Robust 3D reconstruction

Our multi-view diffusion model generates a set of high-quality synthetic views that are reasonably consistent with each other. However, the generated views are generally not perfectly 3D consistent. Indeed, generating perfectly 3D consistent images remains a very challenging problem even for current state-of-the-art video diffusion models [70]. Since 3D reconstruction methods have been designed to take photographs (which are by definition perfectly consistent) as input, we modify the standard NeRF training procedure to improve its robustness to inconsistent input views.

![](images/8de69445582ba90af2c9c73db5bc372e602d3035536bc24f38855b21ce2611c5.jpg)  
Figure 4: Qualitative comparison, few-view reconstruction (renders). A comparison of rendered reconstructions on scenes from mip-NeRF 36 (top row) and CO3D (bottom row), given 3 input captured views. Compared to ReconFusion [7], CAT3D better aligns with ground-truth in seen regions, while hallucinating plausible content in unseen regions. See supplemental website for additional comparisons.

We build upon Zip-NeRF [71], whose training procedure minimizes the sum of a photometric reconstruction loss, a distortion loss, an interlevel loss, and a normalized L2 weight regularizer. We additionally include a perceptual loss (LPIPS [72]) between the rendered image and the input image. Compared to the photometric reconstruction loss, LPIPS emphasizes high-level semantic similarity between the rendered and observed images, while ignoring potential inconsistencies in low-level high-frequency details. Since generated views closer to the observed views tend to have less uncertainty and are therefore more consistent, we weight the losses for generated views based on the distance to the nearest observed view. This weighting is uniform at the beginning of the training, and is gradually annealed to a weighting function that more strongly penalizes reconstruction losses for views closer to one of the observed views. See Appendix D for additional details.

# 4 Experiments

We trained the multi-view diffusion model at the core of CAT3D on four datasets with camera pose annotations: Objaverse [73], CO3D [74], RealEstate10k [75] and MVImgNet [76]. We then evaluated CAT3D on the few-view reconstruction task (Section 4.1) and the single image to 3D task (Section 4.2), demonstrating qualitative and quantitative improvements over prior work. The design choices that led to CAT3D are ablated and discussed further in Section 4.3.

# 4.1 Few-View 3D Reconstruction

We first evaluate CAT3D on five real-world benchmark datasets for few-view 3D reconstruction. Among those, CO3D [74] and RealEstate10K [75] are in-distribution datasets whose training splits were part of our training set (we use their test splits for evaluation), whereas DTU [77], LLFF [78] and the mip-NeRF 360 dataset [79] are out-of-distribution datasets that were not part of the training dataset. We tested CAT3D on the 3, 6 and 9 view reconstruction tasks, with the same train and eval splits as [7]. In Table 1, we compare to the state-of-the-art for dense-view NeRF reconstruction with no learned priors (Zip-NeRF [71]) and methods that heavily leverage generative priors such as ZeroNVS [42] and ReconFusion [7]. We find that CAT3D achieves state-of-the-art performance across nearly all settings, while also reducing generation time from 1 hour (for ZeroNVS and ReconFusion) down to a few minutes. CAT3D outperforms baseline approaches on more challenging datasets like CO3D and mip-NeRF 360 by a larger margin, thereby demonstrating its value in reconstructing large and highly detailed scenes. Figure 4 shows the qualitative comparison. In unobserved regions, CAT3D is able to hallucinate plausible textured content while still preserving geometry and appearance from the input views, whereas prior works often produce blurry details and oversmoothed backgrounds.

Table 1: Quantitative comparison of few-view 3D reconstruction. CAT3D outperforms baseline approaches across nearly all settings and metrics (modified baselines denoted with ∗ taken from [7]).   

<html><body><table><tr><td rowspan="11"></td><td>Dataset Method</td><td></td><td>3-view</td><td></td><td></td><td>6-view</td><td></td><td></td><td>9-view</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>PSNR↑ SSIM↑LPIPS↓PSNR↑ SSIM↑LPIPS↓PSNR↑ SSIM↑LPIPS↓</td></tr><tr><td>RealEstate10K</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Zip-NeRF*[71]</td><td>20.77</td><td>0.774</td><td>0.332</td><td>27.34</td><td>0.906</td><td>0.180</td><td>31.56</td><td>0.947</td><td>0.118</td></tr><tr><td>ZeroNVS*[42]</td><td>19.11</td><td>0.675</td><td>0.422</td><td>22.54</td><td>0.744</td><td>0.374</td><td>23.73</td><td>0.766</td><td>0.358</td></tr><tr><td>ReconFusion[7]</td><td>25.84</td><td>0.910</td><td>0.144</td><td>29.99</td><td>0.951</td><td>0.103</td><td>31.82</td><td>0.961</td><td>0.092</td></tr><tr><td>CAT3D (ours) LLFF</td><td>26.78</td><td>0.917</td><td>0.132</td><td>31.07</td><td>0.954</td><td>0.092</td><td>32.20</td><td>0.963</td><td>0.082</td></tr><tr><td>Zip-NeRF*[71]</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>17.23</td><td>0.574</td><td>0.373</td><td>20.71</td><td>0.764</td><td>0.221</td><td>23.63</td><td>0.830</td><td>0.166</td></tr><tr><td>ZeroNVS*[42]</td><td>15.91</td><td>0.359</td><td>0.512</td><td>18.39</td><td>0.449</td><td>0.438</td><td>18.79</td><td>0.470</td><td>0.416</td></tr><tr><td>ReconFusion [7]</td><td>21.34</td><td>0.724</td><td>0.203</td><td>24.25</td><td>0.815</td><td>0.152</td><td>25.21</td><td>0.848</td><td>0.134</td></tr><tr><td>CAT3D (ours)</td><td>21.58</td><td>0.731</td><td>0.181</td><td>24.71</td><td>0.833</td><td>0.121</td><td>25.63</td><td>0.860</td><td>0.107</td></tr><tr><td>DTU</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Zip-NeRF*[71]</td><td>9.18</td><td>0.601</td><td>0.383</td><td>8.84</td><td>0.589</td><td>0.370</td><td>9.23</td><td>0.592</td><td>0.364</td></tr><tr><td>ZeroNVS*[42]</td><td>16.71</td><td>0.716</td><td>0.223</td><td>17.70</td><td>0.737</td><td>0.205</td><td>17.92</td><td>0.745</td><td>0.200</td></tr><tr><td>ReconFusion [7]</td><td>20.74</td><td>0.875</td><td>0.124</td><td>23.62</td><td>0.904</td><td>0.105</td><td>24.62</td><td>0.921</td><td>0.094</td></tr><tr><td>CAT3D (ours)</td><td>22.02</td><td>0.844</td><td>0.121</td><td>24.28</td><td>0.899</td><td>0.095</td><td>25.92</td><td>0.928</td><td>0.073</td></tr><tr><td>CO3D Zip-NeRF*[71]</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>14.34</td><td>0.496</td><td>0.652</td><td>14.48</td><td>0.497</td><td>0.617</td><td>14.97</td><td>0.514</td><td>0.590</td></tr><tr><td>ZeroNVS*[42]</td><td>17.13</td><td>0.581</td><td>0.566</td><td>19.72</td><td>0.627</td><td>0.515</td><td>20.50</td><td>0.640</td><td>0.500</td></tr><tr><td>ReconFusion [7] CAT3D (ours)</td><td>19.59</td><td>0.662</td><td>0.398</td><td>21.84</td><td>0.714</td><td>0.342</td><td>22.95</td><td>0.736</td><td>0.318</td></tr><tr><td></td><td>20.57</td><td>0.666</td><td>0.351</td><td>22.79</td><td>0.726</td><td>0.292</td><td>23.58</td><td>0.752</td><td>0.273</td></tr><tr><td>Mip-NeRF 360</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Zip-NeRF*[71]</td><td>12.77</td><td>0.271</td><td>0.705</td><td>13.61</td><td>0.284</td><td>0.663</td><td>14.30</td><td>0.312</td><td>0.633</td></tr><tr><td>ZeroNVS*[42]</td><td>14.44</td><td>0.316</td><td>0.680</td><td>15.51</td><td>0.337</td><td>0.663</td><td>15.99</td><td>0.350</td><td>0.655</td></tr><tr><td>ReconFusion[7]</td><td>15.50</td><td>0.358</td><td>0.585</td><td>16.93</td><td>0.401</td><td>0.544</td><td>18.19</td><td>0.432</td><td>0.511</td></tr><tr><td>CAT3D (ours)</td><td>16.62</td><td>0.377</td><td>0.515</td><td>17.72</td><td>0.425</td><td>0.482</td><td>18.67</td><td>0.460</td><td>0.460</td></tr></table></body></html>

# 4.2 Single image to 3D

CAT3D supports the efficient generation of diverse 3D content from just a single input view. Evaluation in this under-constrained regime is challenging as there are many 3D scenes consistent with the single view, for example scenes of different scales. We thus focus our single image evaluation on qualitative comparisons (Figure 5), and quantitative semantic evaluations with CLIP [64] (Table 2). On scenes, CAT3D produces higher resolution results than ZeroNVS [42] and RealmDreamer [80], and for both scenes and objects we better preserve details from the input image. On images with segmented objects, our geometry is often worse than existing approaches like ImageDream [9] and DreamCraft3D [81], but maintains competitive CLIP scores. Compared to these prior approaches that iteratively leverage a generative prior in 3D distillation, CAT3D is more than an order of magnitude faster. Faster generation methods have been proposed for objects [6, 82, 83], but produce significantly lower resolution results than their iterative counterparts, so they are not included in this comparison. IM-3D [17] achieves better performance on segmented objects with similar runtime, but does not work on scenes, or on objects in context.

# 4.3 Ablations

At the core of CAT3D is a multi-view diffusion model that has been trained to generate consistent novel views. We considered several model variants, and evaluated both their sample quality (on in-domain and out-of-domain datasets) and few-view 3D reconstruction performance. We also compare important design choices for 3D reconstruction. Results from our ablation study are reported in Table 3 and Figure 6 in Appendix A and summarized below. Overall, we found that video diffusion architectures, with 3D self-attention (spatiotemporal) and raymap embeddings of camera pose, produce consistent enough views to recover 3D representations when combined with robust reconstruction losses.

![](images/c1b60b2443a8d597b25222a08859d9b203f3429d9a2f54c9eb50559309033e10.jpg)  
Figure 5: 3D creation from single input images. Renderings of 3D models from CAT3D (middle row) are higher quality than baselines (bottom row) for scenes, and competitive for objects. Note that scale ambiguity amplifies the differences in renderings between methods. See supplemental website for additional comparisons.   
Table 2: Evaluating image-to-3D quality with CLIP image scores on examples from [9] (numbers reproduced from [17]). CAT3D produces competitive results to object-centric baselines while also working on whole scenes. (Note: shown images are 3D renders, and time for CAT3D was evaluated on 16 A100 GPUs.)

<html><body><table><tr><td colspan="3">Model Time (min） CLIP (Image)</td></tr><tr><td>ImageDream [9]</td><td>120</td><td>83.77 ± 5.2</td></tr><tr><td>One2345++ [84]</td><td>0.75</td><td>83.78 ± 6.4</td></tr><tr><td>IM-3D (NeRF)[17]</td><td>40</td><td>87.37 ± 5.4</td></tr><tr><td>IM-3D[17]</td><td>3</td><td>91.40 ± 5.5</td></tr><tr><td>CAT3D (ours)</td><td>1</td><td>88.54 ±8.6</td></tr></table></body></html>

Image and pose. Previous work [7] used PixelNerf [63] feature-map conditioning for multiple input views. We found that replacing PixelNeRF with attention-based conditioning in a conditional video diffusion architecture using a per-image embedding of the camera pose results in improved samples and 3D reconstructions, while also reducing model complexity and the number of parameters. We found that embedding the camera pose as a low-dimensional vector (as in [37]) works well for in-domain samples, but generalizes poorly compared to raymap conditioning (see Section 3.1).

Increasing the number of views. We found that jointly modeling multiple output views (i.e., 5 or 7 views instead of 1) improves sample metrics — even metrics that evaluate the quality of each output image independently. Jointly modeling multiple outputs creates more consistent views that result in an improved 3D reconstruction as well.

Attention layers. We found that 3D self-attention (spatiotemporal) is crucial, as it yields improved performance relative to factorized 2D self-attention (spatial-only) and 1D self-attention (temporalonly). While models with 3D self-attention in the finest feature maps $( 6 4 \times 6 4 )$ result in the highest fidelity images, they incur a significant computational overhead for training and sampling for relative small gain in fidelity. We therefore decided to use 3D self-attention only in feature maps of size $3 2 \times 3 2$ and smaller.

Multi-view diffusion model training. Initializing from a pre-trained text-to-image latent diffusion model improved performance on out-of-domain examples. We experimented with fine-tuning the multi-view diffusion model to multiple variants specialized for specific numbers of inputs and outputs views, but found that a single model jointly trained on 8 frames with either 1 or 3 conditioning views was sufficient to enable accurate single image and few-view 3D reconstruction.

3D reconstruction. LPIPS loss is crucial for achieving high-quality texture and geometry, aligning with findings in [17, 7]. On Mip-NeRF 360, increasing the number of generated views from 80 (single elliptical orbit) to 720 (nine orbits) improved central object geometry but sometimes introduced background blur, probably due to inconsistencies in generated content.

# 5 Discussion

We present CAT3D, a unified approach for 3D content creation from any number of input images. CAT3D leverages a multi-view diffusion model for generating highly consistent novel views of a 3D scene, which are then input into a 3D multi-view reconstruction pipeline. CAT3D decouples the generative prior from 3D extraction, leading to efficient, simple, and high-quality 3D generation.

Although CAT3D produces compelling results and outperforms prior works on multiple tasks, it has limitations. Because our training datasets have roughly constant camera intrinsics for views of the same scene, the trained model cannot handle test cases well where input views are captured by multiple cameras with different intrinsics. The generation quality of CAT3D relies on the expressivity of the base text-to-image model, and it performs worse in the cases where scene content is out of distribution for the base model (e.g. human faces, since the base model was trained on limited human data). The number of output views supported by our multi-view diffusion model is still relatively small, so when we generate a large set of samples from our model, not all views may be 3D consistent with each other (see Supplementary website). Finally, CAT3D uses manually-constructed camera trajectories that cover the scene thoroughly (see Appendix C), which may be difficult to design for large-scale open-ended 3D environments.

There are a few directions worth exploring in future work to improve CAT3D. The multi-view diffusion model may benefit from being initialized from a pre-trained video diffusion model, as observed by [10, 17]. The consistency of samples could be further improved by extending the number of conditioning and target views handled by the model. Automatically determining the camera trajectories required for different scenes could increase the flexibility of the system.