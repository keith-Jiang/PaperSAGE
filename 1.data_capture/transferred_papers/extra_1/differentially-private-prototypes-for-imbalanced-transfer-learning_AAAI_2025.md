# Differentially Private Prototypes for Imbalanced Transfer Learning

Dariush Wahdany1,3\*, Matthew Jagielski2, Adam Dziedzic3, Franziska Boenisch3

1Fraunhofer AISEC 2Google DeepMind 3CISPA Helmholtz Center for Information Security dariush.wahdany $@$ aisec.fraunhofer.de, jagielski $@$ google.com, adam.dziedzic $@$ cispa.de, boenisch@cispa.de

# Abstract

Machine learning (ML) models have been shown to leak private information from their training datasets. Differential Privacy (DP), typically implemented through the differential private stochastic gradient descent algorithm (DP-SGD), has become the standard solution to bound leakage from the models. Despite recent improvements, DP-SGD-based approaches for private learning still usually struggle in the high privacy $( \varepsilon \leq 1 )$ ) and low data regimes, and when the private training datasets are imbalanced. To overcome these limitations, we propose Differentially Private Prototype Learning (DPPL) as a new paradigm for private transfer learning. DPPL leverages publicly pre-trained encoders to extract features from private data and generates DP prototypes that represent each private class in the embedding space and can be publicly released for inference. Since our DP prototypes can be obtained from only a few private training data points and without iterative noise addition, they offer high-utility predictions and strong privacy guarantees even under the notion of pure $D P$ . We additionally show that privacy-utility trade-offs can be further improved when leveraging the public data beyond pre-training of the encoder: in particular, we can privately sample our DP prototypes from the publicly available data points used to train the encoder. Our experimental evaluation with four state-of-theart encoders, four vision datasets, and under different data and imbalancedness regimes demonstrate DPPL’s high performance under strong privacy guarantees in challenging private learning setups.

# 1 Introduction

Machine learning (ML) models are known to leak private information about their training datasets (Carlini et al. 2022; Fredrikson, Jha, and Ristenpart 2015; Shokri et al. 2017). As a solution to provably upper-bound privacy leakage, differential privacy (DP) (Dwork et al. 2006) has emerged as the de-facto standard for private training. It is usually implemented in ML through the differential private stochastic gradient descent (DP-SGD) algorithm which bounds the contribution of each data point during training and iteratively injects controlled amounts of noise (Abadi et al. 2016). Thereby, DP-SGD has been shown to increase training time and decrease the final model’s utility.

![](images/2e7db3378aab14e149f58b6536fed1aee5930f42f65dbe9dac3399099f740ffa.jpg)  
Figure 1: Overview of DPPL. We split the private data $\mathbf { X }$ per class $c$ into $\mathbf { X } _ { c }$ ’s, infer them through a publicly pretrained encoder, and estimate per-class prototypes $\mathbf { p } _ { c }$ in the embedding space with DP. Classification of samples is performed by returning the label of the closest prototype $\mathbf { p } _ { c }$ in the embedding space according to some distance function $d$ .

While, over the last years, there has been significant progress in improving both computational efficiency (Bu et al. 2021; Lee and Kifer 2021; Subramani, Vadivelu, and Kamath 2021) and privacy-utility trade-offs (Bu, Mao, and Xu 2022; De et al. 2022), there are a few relevant setups where DP training still yields unfavorable results. These include the high privacy regime (expressed in DP with small values of the privacy parameter $\varepsilon$ , such as $\varepsilon \leq 1 \dot { }$ ), the low data regime, i.e., when only a few private data points are available for training, and when the training dataset is imbalanced, i.e., when some classes have significantly more data points than others (Buda, Maki, and Mazurowski 2018; Liu et al. 2019; Reed 2001).

There are various reasons why DP training is challenging in these setups (Feldman 2020; Esipova et al. 2023). One of these is that DP protects small sets of examples due to its “group privacy” property, providing a provable bound on how much a DP algorithm can learn from small data (Feldman 2020). Beyond this concern, the iterative noise addition weakens the signal from the training data, especially when only a few training data points are available. Moreover, standard approaches for learning in imbalanced setups, such as changing the sampling (Kubat, Matwin et al. 1997; Ling and Li 1998), generating synthetic data for the minority classes (Chawla et al. 2002), or weighing the training loss (Cao et al. 2019) are not directly compatible with DP or incur additional privacy costs. In a similar vein, each training iteration with DP training incurs additional privacy costs (Abadi et al. 2016), making it hard to keep $\varepsilon$ low, i.e., to stay in the high privacy regime.

To address all of these challenges, we propose Differential Private Prototype Learning (DPPL), a novel approach for private learning that combines prototypical networks (Snell, Swersky, and Zemel 2017), a standard algorithm for nonprivate few shot learning, with recent advances in training high-performance private models with DP that leverage powerful encoder models pre-trained on public data (Caron et al. 2021; He et al. 2022; Radford et al. 2021) combined with private transfer learning (Yu et al. 2021; Gu, Kamath, and Wu 2022; Trame\`r, Kamath, and Carlini 2022; Ganesh et al. 2023; Hu et al. 2021; Houlsby et al. 2019; Li et al. 2023; Mehta et al. 2023). The main idea of our DPPL is to use the encoder as a feature extractor for the private data and to generate DP prototypes in the embedding space for each private class. To classify new data points, we then simply have to infer these points through the encoder and to return the label of the closest prototype.

Relying on DP prototypes for private learning offers significant advantages over iterative private training or finetuning. First, our prototypes do not require iterative noise addition. This enables to obtain less noisy predictions at lower privacy costs and improves privacy-utility trade-offs in the high privacy regime. Second, the prototypes are inherently balanced, i.e., it is possible to obtain good prototypes also at the low data regime or for underrepresented classes from imbalanced private training datasets. Third, DP prototypes are fast to obtain, enable fast inference, and, due to the DP post-processing guarantees—which express that no query to them will incur additional privacy costs—can be publicly released for performing predictions.

We propose multiple algorithms for obtaining DP prototypes, and find that prior approaches for training models with DP do not yet leverage the full capacity of the public data (Yu et al. 2021; Mehta et al. 2023): these prior approaches use the public data only for pre-training the encoder. Yet, we make the observation that we can leverage the public data additionally during the transfer learning step. By privately selecting per-class private prototypes from the public data, we can significantly decrease the privacy costs of our prototypes (even under the strong notion of pure $D P$ , i.e., $\varepsilon$ -DP) and further improve privacy-utility trade-offs.

By performing thorough experimentation with four stateof-the-art encoders and four standard vision datasets, we show that DPPL provides strong utility in the high privacy regimes. Additionally, we highlight that DPPL is able to provide good privacy-utility trade-offs when only a few private training data points are available and that it yields state-of-the-art performance on imbalanced classification tasks. Thereby, DPPL represents a new powerful learning paradigm for private training with DP.

In summary, we make the following contributions:

• We propose DPPL, a novel alternative to private finetuning that combines recent advances in DP transfer learning with private few shot learning and can even yield pure DP guarantees. • We perform extensive empirical evaluation which highlights that DPPL yields strong privacy-utility trade-offs, in particular in the high privacy regime and for imbal

anced data. • To further improve DP transfer learning, we show that we can leverage the public data beyond the pre-training step of the feature encoder by privately selecting public prototypes from it.

# 2 Background

Transfer Learning. We consider transfer learning where a publicly available pre-trained encoder $\hat { E }$ is used to extract features $\hat { \mathbf X }$ from a (private) dataset $D = ( \mathbf { X } , \mathbf { y } )$ . Those features are then used to perform downstream classification by learning a function $f$ maximizing $\mathrm { P r } _ { \mathbf { x } , y \in D } [ f ( \hat { E } ( \mathbf { x } ) ) = y ]$ .

Differential Privacy. Differential privacy (DP) (Dwork et al. 2006) is a mathematical framework that provides privacy guarantees in $\mathrm { \bf M L }$ by formalizing the intuition that a learning algorithm $\mathcal { A } : I  S$ , executed on two neighboring datasets $D$ , $D ^ { \prime }$ that differ in only one data point,i.e.,, $D \ = \ D ^ { \prime } \cup \{ x \}$ (add/remove $D P$ ), will yield roughly the same output, i.e., $\operatorname* { P r } [ \boldsymbol { A } ( D ) \in S ] \leq e ^ { \epsilon } \cdot \operatorname* { P r } [ \boldsymbol { A } ( D ^ { \prime } ) \in \dot { S ] } + \delta$ (approximate $D P$ ). In this inequality, $\varepsilon$ is the privacy budget that specifies by how much the output is allowed to differ and $\delta$ is the probability of differing more. If $\delta = 0$ , we refer to it as pure $D P$ , a strictly stronger notion of privacy. We will also refer to zero-concentrated DP $( \mathrm { z C D P } )$ (Bun and Steinke 2016), which requires that $D _ { \alpha } ( \mathcal { A } ( D ) | | \mathcal { A } ( D ^ { \prime } ) ) \ \leq$ $\xi + \rho \alpha \ \forall _ { \alpha \in ( 1 , \infty ) }$ , where $D _ { \alpha }$ is the Re´nyi divergence of order $\alpha$ . zCDP is a relaxation of pure DP, but stricter than approximate DP. $( 0 , \rho )$ -zCDP can also be expressed simply as $\rho$ -zCDP. We provide more details on DP in Appendix A.1. The standard approach for learning ML models with DP guarantees is differentially private stochastic gradient descent (DPSGD) (Song, Chaudhuri, and Sarwate 2013; Abadi et al. 2016). DPSGD clips model gradients to a given norm to limit the impact of individual data points on the model updates and adds a controlled amount of Gaussian noise to implement formal privacy guarantees during training.

Exponential Mechanism. The exponential mechanism (McSherry and Talwar 2007) offers a way to implement pure DP guarantees. Given a set of possible outputs $\mathbf { X } ^ { \prime }$ , it samples an output $\mathbf { x } ^ { \prime }$ according to some utility function $u$ with probability $\begin{array} { r } { \mathrm { P r } [ \mathbb { E } \mathbf { M } _ { u } ( \mathbf { X } ) \mathbf { \Sigma } = \bar { \mathbf { x } } ] \ \propto \ \exp \left( \frac { \bar { \mathbf { \mu } } } { \Delta u } u ( \mathbf { X } , \hat { \mathbf { x } } ) \right) } \end{array}$ . This algorithm satisfies $2 \epsilon$ -DP. Appendix A.3 shows more details on the exponential mechanism and utility function.

Prototypical Networks. Prototypical networks (Snell, Swersky, and Zemel 2017) are used for few-shot classification, i.e., they provide a way on adapting a classifier to new unseen classes with access only to a small number of data points from each new class. Their main components are a set of prototypes pc ∈ RM and an embedding function $f _ { \phi } : \mathbb { R } ^ { \hat { D } }  \mathbf { \dot { \mathcal { R } } } ^ { M }$ . Each prototype for a class $c$ is the mean of the embedded points belonging to that class, i.e., $\mathbf { p } _ { c } ~ = ~ \frac { 1 } { | \mathbf { X } _ { c } | } \sum _ { \mathbf { x } \in \mathbf { X } _ { c } } f _ { \phi } ( \mathbf { x } )$ . Given a distance function $d : \mathbb { R } ^ { M } \times \mathbb { R } ^ { M } \to [ 0 , \infty )$ , the model classifies a point $\mathbf { x }$ based on its nearest prototype in the embedded space as $\hat { y } ( \mathbf { x } ) = \mathop { \arg \operatorname* { m i n } } _ { c } d ( f _ { \phi } ( \bar { \mathbf { x } } ) , \mathbf { p } _ { c } )$ .

DP Mean Estimation. Obtaining differentially private means $\textstyle \mu = 1 / n \sum _ { n } \mathbf { x }$ for $\mathbf { x } \in \mathbb { R } ^ { d }$ is challenging in high dimensions. A straightforward approach (Kamath and Ullman 2020) consists of clipping all samples to some $\ell _ { 2 }$ norm, adding noise scaled according to the clip norm and then reporting the noisy mean of the clipped samples. FriendlyCore (Tsfadia et al. 2022) is a framework for pre-processing the input data of private algorithms, such that the algorithms being executed on this pre-processed data need to be private only for relaxed conditions. It improves especially for the cases where the samples have a high $\ell _ { 2 }$ norm and high dimensionality $d$ . The CoinPress algorithm (Biswas et al. 2020) estimates the mean iteratively, clipping the samples not w.r.t. the origin but to the estimated mean of the previous step. This approach is especially useful when the mean is far away from the origin and generally considered stateof-the-art for dimensionalities in the low thousands. Figure 10 shows that the straightforward approach outperforms all other methods given strong priors on the $\ell _ { 2 }$ norms of the samples. We provide more details in Appendix A.4.

# 3 Related Work

Private Transfer Learning. Standard approaches for DP transfer learning rely on the DPSGD algorithm to train a classifier on top of the representations output by a pretrained encoder, and potentially also to privately update existing or added model parameters on the sensitive data (Yu et al. 2021; De et al. 2022; Mehta et al. 2022). Notably, there also exists an approach for transfer learning from few samples, DP-FiLM introduced by Tobaben et al. (2023). Such approaches have been shown effective, for loose DP guarantees (i.e., large $\varepsilon$ ), yet suffer from severe utility drops in strong privacy regimes (i.e., with small $\varepsilon ^ { \cdot }$ ). This is because of the iterative nature of the DPSGD algorithm with multiple rounds of noise addition that negatively impact performance. To overcome these limitations, Mehta et al. (2023) proposed Differentially Private Least Squares (DP-LS), DPNewton and DPSGD with Feature Covariance (DP-FC). DPLS takes advantage of the closed form solution for least squares to avoid running many iterations of gradient descent. DP-Newton employs a second-order optimization to solve the smaller problem of transfer learning more efficiently. DP-FC integrates second order information by utilizing the covariance of the features without paying the composition cost of DP-Newton. All methods have three hyperparameters. In contrast to theirs, our method only has a single optional hyperparameter, does not rely on higher order optimization and utilizes parallel composition to solve each class independently in a single iteration, resulting in lower privacy costs especially for imbalanced datasets.

Leveraging Public Data for Private Training. Public data has, so far, been leveraged for privacy-preserving knowledge transfer to protect sensitive data (Papernot et al. 2017, 2018), to reduce the sample complexity within DP distribution learning (Bie, Kamath, and Singhal 2022; BenDavid et al. 2024), and for pre-training public encoders to then perform private transfer learning (Yu et al. 2021; Gu, Kamath, and Wu 2022; Trame\`r, Kamath, and Carlini 2022; Ganesh et al. 2023; Hu et al. 2021; Houlsby et al. 2019; Li et al. 2023; Mehta et al. 2023). In a similar vein as previous work that determines the importance of public samples to private data (Ji and Elkan 2013), our approach goes beyond the latter and additionally leverages the public pretraining data of the encoder during the private transfer learning step by selecting public prototypes to represent our private classes.

Private Training on Unbalanced Datasets. DP has been shown to disproportionately harm utility for underrepresented sub-groups, i.e., groups with fewer data points (Bagdasaryan, Poursaeed, and Shmatikov 2019; Suriyakumar et al. 2021). This is because the weak signal from these groups is more affected by the added noise. Additionally, the clipping operation in DPSGD changes the direction of the overall gradient, which adds a compounding bias over the runtime of the training, that disproportionally affects minority classes (Esipova et al. 2023). To mitigate this issue, Esipova et al. (2023) propose DPSGD-Global-Adapt, which clips only some gradients and instead scales most gradients, thus preserving the overall direction. The algorithm adaptively learns the clipping threshold, keeping the amount of clipped gradients low. Another approach is to add fairness through in- or post-processing (Jagielski et al. 2019), which trades off accuracy against fairness and requires additional privacy budget. In a non-private setting, solutions for improving utility of small subgroups include changing the sampling (Kubat, Matwin et al. 1997; Lewis and Catlett 1994; Ling and Li 1998), generating synthetic data for the minority classes (Chawla et al. 2002; Wang et al. 2018), or weighing the training loss (Cao et al. 2019). However, these approaches are not directly compatible with DP or incur additional privacy costs.

# 4 Differentially Private Prototyping

Setup and Assumptions. We aim at learning a private classifier based on a sensitive labeled dataset $D = ( \mathbf { X } , \mathbf { y } )$ with $C$ different classes. We assume the availability of a standard public pre-trained vision encoder $\hat { M }$ , such as $\mathrm { D I N O ^ { 1 } }$ or $\mathrm { M A } \dot { \mathrm { E } } ^ { 2 }$ encoders, that return high-dimensional feature vectors for their input data points. Additionally, we assume the availability of a general purpose public dataset $\hat { D } =$ $( \hat { \mathbf { X } } , \hdots )$ , such as ImageNet (Deng et al. 2009). Note that $\hat { D }$ can also be from a different distribution than $D$ and $\hat { M }$ ’s pre-training data, as we show experimentally in Figure 7a, and does not require labels. In case of available labels for $\hat { D }$ , we just discard them.

Overview. Our goal is to obtain private prototypes $\mathbf { p } _ { 1 } , \ldots . . \mathbf { p } _ { C }$ that represent every class $C$ from the private dataset $D$ in the embedding space. To classify a new unseen data point $\mathbf { x } ^ { \prime }$ , we simply have to retrieve the most representative prototype and return its label. Concretely, we have to infer $\mathbf { x } ^ { \prime }$ through the encoder $\hat { M }$ , retrieve the prototype with the minimum distance in embedding space to $\mathbf { x } ^ { \prime }$ and return its label as the prediction $y ^ { \prime } = \operatorname* { m i n } _ { c \in C } d ( \hat { M } ( \mathbf { x } ^ { \prime } ) , \mathbf { p } _ { c } )$ . We detail the general approach in Figure 1.

Note that if the private prototypes are obtained with DP guarantees, using them for predictions will not incur additional privacy costs due to the DP post-processing guarantees. Hence, our DP prototypes can be publicly released, similar to privately trained ML models. We experimented with multiple ways for implementing DP prototypes and identified the two most promising approaches: DPPL-Mean generates a private prototype by calculating a DP mean on all data points of a given class in the embedding space. Our DPPL-Public takes advantage of the public dataset $\hat { D }$ and privately selects a data point from $\hat { D }$ to act as a prototype for each private class.

# 4.1 DPPL-Mean: Private Means

Intuition. Non-private prototypical networks (Snell, Swersky, and Zemel 2017) consist of two steps, namely the training of a projection layer at the output of the encoder and the estimation of the class prototypes. In the private setup, both these steps would depend on the private data and therefore each incur additional privacy costs. To keep privacy cost low, we forgo projection layer training, as we find it is unnecessary when given a strong pre-trained encoder (see Appendix C.6). Hence, for our private DPPL-Mean, we only implement the estimation of the prototypes without projection.

Non-Private Means. Given a training class $c$ and corresponding samples $\mathbf { X } _ { c } \in \mathbb { R } ^ { n _ { c } \times d }$ , the non-private prototype of each class is the mean of the embeddings $\textstyle { \frac { 1 } { n _ { c } } } \sum _ { i = 0 } ^ { n _ { c } } { \hat { M } } ( \mathbf { x } _ { i } )$

Our DPPL-Mean: Private Means. To privately estimate the means, we rely on the Gaussian Mechanism. We first clip each $\mathbf { x } _ { i } \in \mathbb { R } ^ { d }$ to a $\ell _ { 2 }$ norm $r$ . The estimate is then

$$
\mathbf { p } _ { c } = \mathcal { N } \left( \mathbf { 0 } , \frac { 2 r ^ { 2 } } { n _ { c } ^ { 2 } \rho } \right) + \frac { 1 } { n _ { c } } \sum _ { i = 0 } ^ { n _ { c } } \hat { M } ( c l i p _ { \ell _ { 2 } } ( \mathbf { x } _ { i } ) , r )
$$

where $\rho$ is the zCDP privacy budget. To improve the utility at strict privacy budgets, we include a single optional hyperparameter $k _ { \mathrm { p o o l } } \geq 1$ , describing the kernel size of an average pooling layer before the mean estimation to reduce dimensionality, reducing the dimension from $d$ to $d / k _ { \mathrm { p o o l } }$ .

Privacy Analysis. The privacy analysis of DPPL-Mean follows the analysis of the Gaussian Mechanism. By clipping each sample to $\ell _ { 2 }$ norm of $r$ we obtain $\Delta \mathbf { p } _ { c } = 2 r / n$ , since the $\ell _ { 2 }$ distance between the previous mean and any new sample can be $2 r$ at maximum and its influence diminishes with the number of samples $n$ . We use parallel composition: each disjoint class computes a $\rho$ -zCDP mean prototype, making the privacy cost $\rho$ for the entire private dataset.

# 4.2 DPPL-Public: Privately Selecting Public Prototypes

Intuition. Our main idea for DPPL-Public is to leverage public data beyond the pre-training stage for learning a private classifier based on the sensitive data. Therefore, we privately select public prototypes for each training class, i.e., a data point from the public dataset that represent the given class well.

Non-Private Selection. A good public prototype $\hat { \mathbf { x } } _ { c }$ for a given training class $c$ represents that class well in the embedding space of encoder $\hat { M }$ . To select such a good prototype

# Algorithm 1: Privately Select Public Prototypes

Input: Private dataset $D = ( \mathbf { X } , \mathbf { y } )$ with $C$ classes, privacy budget $\epsilon$ , public pre-trained encoder $\bar { M }$ , public dataset $\hat { D } = ( \mathbf { \hat { X } } , \dots )$ , hyperparameters $d _ { \operatorname* { m a x } } , d _ { \operatorname* { m i n } }$   
Output: Prototypes $\mathbf { P } = \{ \mathbf { p } _ { c } \in { \hat { D } } | c \in \mathbf { y } \}$   
Function SelectPublicPrototypes(): $\mathbf { E }  M ( \mathbf { X } ) \hat { \mathbf { E } }  M ( \hat { \mathbf { X } } )$ foreach class $c \in C$ do $\mathbf { E } _ { c }  \{ \mathbf { e } _ { i } \in \mathbf { E } | y _ { i } = c \}$ $\begin{array} { r } { u _ { c } ( \hat { \bf x } _ { i } ) = \sum _ { { \bf e } \in { \bf E } _ { c } } \mathrm { c l i p } ( 1 + \frac { { \bf e } \cdot \hat { \bf e } _ { i } } { { \bf e } \hat { \bf e } _ { i } } , d _ { \mathrm { m a x } } , d _ { \mathrm { m i n } } ) . } \end{array}$ ; $\begin{array} { r } { \mathbf { p } _ { c } \propto \exp \big ( \frac { \epsilon u _ { c } } { d _ { \mathrm { m a x } } - d _ { \mathrm { m i n } } } \big ) } \end{array}$   
return $\{ \mathbf { p } _ { c } | c \in \mathbf { y } \}$ ;

per class, we first calculate the embeddings ${ \bf E } = \hat { M } ( { \bf X } )$ and $\hat { \mathbf { E } } = \hat { M } ( \hat { \mathbf { X } } )$ for the private and public data points, respectively. Then, based on the private labels $\mathbf { y }$ , we split the embeddings of $\mathbf { X }$ in $C$ subsets $\mathbf { E } _ { 1 } , \ldots , \mathbf { E } _ { C }$ . Without any privacy considerations, a public prototype $\hat { \mathbf { x } } _ { c }$ for class $c$ could then be chosen as the data point that minimizes the average distance according to metric $d$ , to all training data points $\mathbf { x } _ { i }$ in class $c$ as

$$
\hat { \mathbf { x } } _ { c } = \operatorname* { m i n } _ { \hat { \mathbf { x } } \in \hat { \mathbf { X } } } \frac { \sum _ { i = 0 } ^ { | \mathbf { X } _ { c } | } d ( \hat { M } ( \mathbf { x } _ { i } ) , \hat { M } ( \hat { \mathbf { x } } ) ) } { | \mathbf { X } _ { c } | } .
$$

Our DPPL-Public: Private Selection. The previously described approach, however, does not take any privacy of the training data $D$ into account. To perform public prototype selection with $\varepsilon$ -DP guarantees, we rely on the exponential mechanism (McSherry and Talwar 2007). We use the cosine similarity and add $+ 1$ as our distance metric $d$ , therefore obtaining a bounded and non-negative metric in [0, 2]. The utility function that indicates the goodness of each each public sample for a given class $c$ is

$$
u ( \hat { \mathbf { x } } , c ) = \sum _ { i = 0 } ^ { | \mathbf { X } _ { c } | } 1 + \frac { \hat { M } ( \mathbf { x } _ { i } ) \cdot \hat { M } ( \hat { \mathbf { x } } ) } { \| \hat { M } ( \mathbf { x } _ { i } ) \| \| \hat { M } ( \hat { \mathbf { x } } ) \| } .
$$

To improve utility at strict privacy budgets, we include two optional hyperparameters $d _ { \operatorname* { m a x } } \in ( 0 , 2 ]$ and $d _ { \operatorname* { m i n } } \in [ 0 , d _ { \operatorname* { m a x } } )$ , which clips the distances to $[ d _ { \operatorname* { m i n } } , d _ { \operatorname* { m a x } } ]$ , reducing the sensitivity to $\Delta u = d _ { \operatorname* { m a x } } - d _ { \operatorname* { m i n } }$ .

We detail the full algorithm for privately selecting public prototypes in Algorithm 1.

Privacy Analysis. Our proposed DPPL-Public fulfills $\varepsilon$ -DP. We provide a sketch of the full proof from Appendix D.1 here. We first note that $\Delta u = d _ { \mathrm { m a x } } - d _ { \mathrm { m i n } }$ . As mentioned above, we add $+ 1$ to each cosine similarity, which is therefore non-negative. Therefore, $u$ is positively monotonic w.r.t. $\mathbf { X }$ . The exponential mechanism with $\mathrm { P r } [ \mathbf { E M ( X ) } \ =$ xˆ] $\begin{array} { r } { | \propto \exp \frac { \varepsilon u ( \mathbf { X } , \hat { \mathbf { x } } ) } { \Delta u } } \end{array}$ is $\varepsilon$ -DP if $u$ is monotonic w.r.t. the private data $\mathbf { X }$ (McSherry and Talwar 2007). Therefore, executing DPPL-Public on a single class yields $\varepsilon$ -DP. Additionally, since we calculate prototypes per-class and the classes are non-overlapping, parallel composition applies, i.e.,, the total privacy costs are also $\varepsilon$ -DP.

# 5 Empirical Evaluation

Experiment Setup. We experiment with CIFAR10 (Krizhevsky 2009), CIFAR100 (Krizhevsky 2009), STL10 (Coates, $\mathrm { N g }$ , and Lee 2011) and FOOD101 (Bossard, Guillaumin, and Van Gool 2014) as private datasets. From these datasets, we construct exponentially long-tailed imbalanced datasets with various imbalance ratios (IRs), the ratio between the number of samples in the largest and smallest class, following Cui et al. (2019) and Cao et al. (2019).3 We compare our DP prototypes on the features obtained from three vision transformers based on the original architecture from Dosovitskiy et al. (2020) Vit-B-16 (Singh et al. 2022), namely ViT-L-16 (Oquab et al. 2023), ViT-H-14 (Singh et al. 2022) and a ResNet-50 (Caron et al. 2021; He et al. 2016). All models, except for ViT-L-16, which is trained on LVD-142M introduced by Oquab et al. (2023), are trained on ImageNet-1K (Deng et al. 2009). For DPPL-Public, we use the $6 4 \times 6 4$ downscaled version of ImageNet-1K upscaled to between $1 2 8 \times 1 2 8$ and $5 1 2 \times 5 1 2$ depending on the encoder. We evaluate our methods on the standard balanced test set. This corresponds to reporting a balanced accuracy for the imbalanced setups. A full description of our experimental setup is provided in Appendix B.

Baselines. For the baseline comparisons we compare to standard linear probing with DPSGD, as it’s a common way of DP transfer-learning. Furthermore, we compare to DP-LS from Mehta et al. (2023) which is the current state-of-theart for DP transfer learning across all privacy regimes and to DPSGD-Global-Adapt from Esipova et al. (2023) as it is specifically designed for for training on imbalanced datasets. We outline in Appendix E.4 the experimentally-supported reasons against including DP-FC and DP-FiLM.

Comparing Results over Different Notions of DP. Since we are comparing our new proposed methods that implement pure $D P$ or pure $\rho$ -zCDP guarantees against other baselines that also implement zCDP, we convert all privacy guarantees to $\rho$ -zCDP. We also present a pure-DP $\varepsilon$ equivalent by inverting the $\rho = \epsilon ^ { 2 } / 2$ conversion from pure DP to zCDP. However, this does not imply that these algorithms fulfil pure DP. We detail all comparison implementations and conversion theorems used in Appendix D.2.

# 5.1 DP Prototypes: High Utility in High Privacy and Extreme Imbalance

We evaluate our DP prototypes at different privacy regimes in the range corresponding to standard approximate DP (Dwork et al. 2006) of $0 . 0 1 < \varepsilon < 1 0 0$ and under different IRs. In Figure 2, we benchmark our methods against DP-LS, the current state-of-the-art method for private transfer learning by Mehta et al. (2023), DPSGD-Global-Adapt by Esipova et al. (2023), a DP method deliberately designed to achieve high utility under imbalancedness of the private data, and standard DP linear probing on CIFAR100 with ViT-H-14. Our results highlight that over all levels of IRs larger than 1, our DPPL-Public significantly outperforms linear probing in all privacy regimes. Additionally DPPL-Public yields strong performance already at very low $\varepsilon$ , such as $\varepsilon = 0 . 1$ for $\scriptstyle { \mathrm { I R } } = 1$ . As data becomes more imbalanced, all methods require larger privacy budgets to yield similarly high performance. Further, our results indicate that our DPPL-Mean method underperforms DPPL-Public and DP linear probing for low $\varepsilon$ . We find that this results from the noise added during the mean calculation leading diverging estimations. We provide further detail on the cause in Appendix C.2. Yet, we observe that at higher epsilon, DPPL-Mean outperforms DPPL-Public. This suggests that the most beneficial way for leveraging DP prototypes might be an adaptive method where DPPL-Public is chosen for high performance in the high privacy regimes and DPPL-Mean can further boost performance for larger $\varepsilon$ .

![](images/b81959893a2a97290925017cafa166b8a53b2203dffbed3b331cb1740ef98c55.jpg)  
Figure 2: Comparing against baselines on CIFAR100. We present the results of our methods vs. state-of-the-art methods (DP-LS and DPSGD-Global-Adapt) on the CIFAR100 dataset using ViT-H-14 under different IRs. DPPL-Public uses ImageNet as public data. Dotted lines represent the upper/lower quantiles. Similar results for CIFAR10, Food101, and STL10 are presented in Appendix E.1.

We further assess whether the observed trend holds over different datasets. Therefore, we depict the results of our methods vs. the baselines for different datasets and the ViTH-14 encoder under the most challenging setup with $\scriptstyle { \mathrm { I R } } =$ 100 in Figure 3. The observed trends are indeed consistent between all datasets. We provide full results over all datasets and IRs in Appendix E.1.

![](images/f1b299aa002baac01887208cc8d0ce470ff638b2e61c9fc028661209e5defea2.jpg)  
Figure 3: DP Prototypes on various imbalanced datasets. We present the balanced test accuracy for CIFAR10, CIFAR100, FOOD101 and STL10 at an imbalance ratio of 100 on ViT-H-14, using ImageNet as public data for DPPL-Public. We compare to standard Linear Probing with DP-SGD. We plot the mean test accuracy over multiple runs and represent the upper/lower quantiles by the dotted lines. Appendix E.1 shows more results.

# 5.2 DP Prototypes Improve over State-of-the-Art Baselines in Imbalanced Setups

Our results in Figures 2 and 3 highlight that while in the balanced setup, Mehta et al. (2023) outperforms the other methods, our DP prototypes outperform all other methods the more imbalanced the setup becomes. In particular DPPL-Public outperforms in high privacy regimes (i.e., for small $\varepsilon$ ), while DPPL-Mean is better in lower privacy regimes, mostly outperforming even DPSGD-Global-Adapt.

The advantage of our methods against the baselines become even more obvious as we do not consider the accuracy over the entire balanced test set (equivalent to balanced accuracy), but look specifically at accuracy on minority classes, see Figure 4. Therefore, we take the smallest $2 5 \%$ of training classes in terms of number of their training data points and measure their accuracy on a balanced test set consisting of only those classes. Our results for CIFAR100 on ViT-H-14 under $\mathrm { { I R } = 5 0 }$ in Figure 4 highlight that our DP prototypes significantly outperform all baselines. Full results for the minority classes are depicted in Appendix E.2.

# 5.3 Understanding the Success of DP Prototypes

To better understand the success of our DP prototypes, we perform various ablations. The full set of ablations and their results is presented in Appendix C.

![](images/920831756144c54b688ce888af47f3c72606a5b92695cb3805d7a660e0f55b70.jpg)  
Figure 4: Accuracies of the minority classes. We depict the test accuracy on CIFAR100 with ViT-H-14 embeddings for the minority classes (smallest $2 5 \%$ of classes) at $\mathrm { I R } = 5 0$ .

Effect of the Publicly Pre-trained Encoder. We first assess the impact of the encoder used as a feature extractor. Therefore, we apply our method and the baselines with different encoder architectures. Our results in Figure 5 highlight that the encoder performance impacts all methods alike. In particular, we observe that all methods obtain better results with stronger encoders. For example, the ViT-H14 yields to significantly higher private prediction accuracy that the much smaller ViT-B-16. Additionally, none of the methods yields satisfactory results using the ResNet50.

Impact of the Projection Layer. We evaluate whether a projection layer, usually part of a prototypical network, can increase the utility. We present our results in Figure 6 for CIFAR100 on ViT-H-14, using ImageNet as public for DPPL-Public. They highlight that DPPL-Public does not benefit from the projection. Even non-privately $\mathbf { \epsilon } _ { \epsilon } = \infty$ ), the accuracy of DPPL-Public with projection is $7 2 . 6 \%$ and without projection is $7 4 . 0 \%$ , showing that it is not just the decreased privacy budget for the sampling that reduces the utility, but a fundamental misalignment between how the projection is trained and the public prototyping. We observe the same effect for DPPL-Mean and conclude that, although this limits adaptability (see Appendix F.2), with a strong enough pre-trained encoder, it is sufficient for DPPL to estimate prototypes without projection.

Improving through Multiple Per-Class Prototypes. We experiment with extending our DPPL-Public beyond a single per-class prototype—the common standard for prototypical networks. Therefore, we introduce the variation DPPL-PublicK which selects the top-K public prototypes per class. We extend the algorithm from Gillenwater et al. (2022) to sample multiple prototypes jointly using the exponential mechanism. Then, we classify based on the mean distance to each class’s prototype. Our results in Figure 6 show that DPPL-PublicK’s privacy-utility trade-offs are between DPPL-Mean and DPPL-Public, indicating that the private means can be —to a certain degree— approximated by multiple public prototypes. DPPL-PublicK could therefore replace DPPL-Mean in cases of high dimensionality, where a mean estimation is not feasible. We provide more details, privacy proof, and a full set of results in Appendix C.3.

![](images/181038d6e2ec935ba19f099764cc7279ad055298c7c0511c1a4cec6d27e56e32.jpg)  
Figure 5: Choice of Encoder. We report the test accuracy for our methods and the baselines for CIFAR100, using ImageNet as public data for DPPL-Public. We observe that the success of all methods depends on the quality of the underlying encoder.

Impact of the Public Data for Prototype Selection. To assess whether the public data for prototype estimation needs to be from the same distribution as the one used to pre-train the encoder, we conduct experiments with a different public dataset. We evaluate DPPL-Public using 2,298,112 samples from CC3M introduced by Sharma et al. (2018) as public data instead of ImageNet which is used to pre-train the encoder. We show the relation between the accuracy using ImageNet as public data and CC3M in Figure 7a for CIFAR100 on ViT-H-14. We find that DPPL-Public works well with both public datasets, highlighting the flexibility of our approach. Still, we observe that ImageNet yields better results which suggests that it is particularly beneficial to leverage the public data already available for pre-training also in the private transfer learning step.

Size of the Public Dataset for DPPL-Public. We also evaluate the success of DPPL-Public for different sizes of the public dataset that the public prototypes can be chosen from. Therefore, we randomly draw subsets of different sizes from ImageNet and apply DPPL-Public. Our results for CIFAR100 (100 classes) and ViT-H-14 in Figure 7b indicate that with growing public dataset size, our method’s success increases. Figure 9 shows that tasks less similar to the pretraining data are more sensitive to dataset size. Note that even for public dataset of more than one million images, the selection of our public prototypes for 100 classes (i.e., the ”training”) takes 34.3 seconds on a single GPU as we depict in Appendix E.3. For 10 classes (e.g., CIFAR10), it takes 5 seconds. Hence, choosing a larger public dataset does not represent a practical limitation.

![](images/495880fc45126a7035a7779e76f06c4ef6fe24f72b6ec2626b29f843b522aaa4.jpg)  
Figure 6: Impact of the Projection Layer.

![](images/f2d5b1f78f11c888ce650331a92f9d59df99723cd6b1b39234792ffec4c07fad.jpg)  
Figure 7: Impact of the Public Data.

# 6 Conclusions and Future Work

We propose DPPL as a novel alternative to private finetuning with DP. DPPL builds DP prototypes on top of features extracted by a publicly pre-trained encoder, that can be later used as a classifier. The prototypes can be obtained without iterative noise addition and yield high utility even in high-privacy regimes, with few private training data points, and in unbalanced training setups. We show that we can further boost performance of our DP prototypes by leveraging the public data beyond training of the encoder and using them to draw the public prototypes from (DPPL-Public). Future work at improving utility of high-dimensional DP mean estimation will benefit our DPPL-Mean, which can, in the future, serve as an additional benchmark for private mean estimation algorithms.