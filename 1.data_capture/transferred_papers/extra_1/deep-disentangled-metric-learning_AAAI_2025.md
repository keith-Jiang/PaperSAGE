# Deep Disentangled Metric Learning

Jinhee Park1, Jisoo $\mathbf { P a r k } ^ { 2 }$ , Dagyeong $\mathbf { N a } ^ { 2 }$ , Junseok Kwon1,2

1School of Computer Science and Engineering, Chung-Ang University, Seoul, Korea 2Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea {iv4084em, susiehome, dakunglove, jskwon}@cau.ac.kr

# Abstract

Proxy-based metric learning has enhanced semantic similarity with class representatives and exhibited noteworthy performance in deep metric learning tasks. While these methods alleviate computational demands by learning instance-toclass relationships rather than instance-to-instance relationships, they often limit features to be class-specific, thereby degrading generalization performance for unseen class. In this paper, we introduce a novel perspective called Disentangled Deep Metric Learning (DDML), grounded in the framework of information bottleneck, which applies class-agnostic regularization to existing DML methods. Unlike conventional NormSoftmax methods, which primarily emphasize distinct class-specific features, our DDML enables a diverse feature representation by seamlessly transitioning between classspecific features with the aid of class-agnostic features. It smooths decision boundaries, allowing unseen classes to have stable semantic representations in the embedding space. To achieve this, we learn disentangled representations of both class-specific and class-agnostic features in the context of DML. Our method easily integrates into existing proxy-based algorithms, consistently delivering improved performance.

# Introduction

Owing to the remarkable representation capabilities of deep neural networks, recent deep metric learning (DML) methods have achieved significant performance advancements. These advancements have facilitated various practical applications in computer vision tasks, including image retrieval (Yi et al. 2014; Schroff, Kalenichenko, and Philbin 2015; Movshovitz-Attias et al. 2017; Oh Song et al. 2016), clustering (Xing et al. 2002; Hershey et al. 2016), and classification (Snell, Swersky, and Zemel 2017; Sung et al. 2018).

The goal of DML is to learn a low-dimensional embedding space using DNNs, in which semantically similar images are positioned close to each other, while dissimilar images are placed farther apart. In other words, the distance in the embedding space plays an important role in representing semantic relationships. To achieve this goal, proxybased methods have emerged, involving the comparison of instances to a proxy sample that represents specific classes (Zhai and Wu 2018; Kim et al. 2020). These methods reduce computational complexity by considering instance-to-class relationships rather than instance-to-instance relationships. However, learning instance-to-class relations limits the diversity of image features, making them overly class-specific. Consequently, several studies (Milbich et al. 2020; Zheng et al. 2021a,b; Gu, Ko, and $\mathrm { K i m } 2 0 2 1$ ) have advanced to address this issue through feature diversification, with the goal of improving generalization for unseen classes.

![](images/cd53a3ae769b4caa2f454a530997f00044add2d34e74e653edcf75ce83157cc1.jpg)  
Figure 1: Motivation of the proposed DDML. (a) shows Grad-cam (Selvaraju et al. 2017) results for the same unseen class. While the conventional NormSoftmax (Zhai and Wu 2018) method induces inconsistent activations for unseen classes, our method demonstrates consistent activations. (b) illustrates the positioning of training samples (i.e., red and blue points) and test samples (i.e., gray points) in both the embedding space and the input space for 2D synthesized data. The NormSoftmax approach places testing samples in either the red or blue regions due to the narrow uncertain region (i.e., white regions) caused by emphasizing only classspecific features. In contrast, our method semantically distributes the testing samples gradually within the smoothed uncertain region by leveraging class-agnostic features.

Our insight stems from observing the NormSoftmax (Zhai and Wu 2018) method, which is frequently used as a baseline in proxy-based DML. In Fig.1(a), the NormSoftmax approach induces inconsistent gradient activations for samples belonging to the same unseen class in the CUB dataset (Wah et al. 2011). This observation is evident from the experiment with 2D synthesized data. In Fig.1(b), red and blue points denote the training samples from two distinct classes in the embedding space, whereas gray points represent testing samples belonging to the same unseen class. In conventional NormSoftmax approaches, these testing samples tend to be located in either the red or blue regions (i.e., high probability regions of the two distinct classes, respectively) because the uncertain region around the decision boundary (i.e., white regions) is very narrow. This observation implies that NormSoftmax primarily emphasizes distinct class-specific features. For this reason, samples from unseen classes, which lack predefined class-specific features, may be randomly scattered when every region in the embedding space is deterministically associated with a particular class. Thus, the conventional approach destabilizes the meaning of distances in regions containing samples from unseen classes.

In pursuit of a better embedding space from this perspective, we propose a novel approach called Deep Disentangled Metric Learning (DDML) within the framework of information bottleneck. DDML learns disentangled representations of both class-specific and class-agnostic features to infuse the meaning of distance into uncertain regions. Our classagnostic features serve to bridge the gap between classspecific features, thus widening the uncertain regions in the embedding space. As shown in Fig.1(a), our approach facilitates consistent gradient activations for samples belonging to the same unseen class by introducing class-agnostic features. Similarly, in Fig.1(b), the uncertain region around the decision boundary becomes broader and smoother, leading to a seamless transition between class-specific feature regions. The gray points, which represent testing samples from the same unseen class, are gradually distributed in the expanded uncertain region around the decision boundary. This can be interpreted as testing samples similar to the red (blue) points being located close to the red (blue) region. These observations suggest that features capturing the relationships between classes enable unseen samples to be placed relatively based on their semantic representations in the embedding space, achieving the goal of DML even in uncertain region. In conclusion, the proposed DDML enhances feature diversity, improving generalization for unseen classes.

The proposed method trains both class-agnostic and classspecific features in the context of DML. The trade-off between learning class-specific and class-agnostic features can be resolved by expanding the variational information bottleneck (VIB) (Alemi et al. 2016) objectives. Class-specific features are learned to extract the essential information shared within identical classes, while class-agnostic features are learned to enhance the generalization of DML. For this, we design a class-agnostic regularization loss function composed of three loss terms: specific, agnostic, and split losses. The specific loss term aims to increase the similarity of class-specific features to their corresponding class proxy, while the agnostic loss term decreases this similarity by leveraging class-agnostic features. The split loss term is responsible for mitigate the inherent interdependence between class-agnostic and class-specific features. Fig.2 illustrates the proposed network structure and the contribution of each term in the class agnostic regularization loss function.

The contributions of the proposed method are as follows.

• We propose a novel Deep Disentangled Metric Learning

method (DDML) that leverages disentangled representations of both class-specific and class-agnostic features. • We introduce a loss function for class-agnostic regularization and propose an extended framework along with an optimization strategy that enables effective learning. • Our loss function can be easily attached to existing DML losses and enhances generalization performance.

# Related Work

Proxy-based DML Proxy-based metric learning encodes image similarity as distances in the embedding space by capturing the relationship between sample points and class representatives. A critical concern in proxy-based DML is acquiring intra-class variation. ProxyGML (Zhu et al. 2020) addressed issue by using fewer, trainable proxies per class, while RankMI (Kemertas et al. 2020) estimated a tight lower bound on joint probability divergence. Recent advancements in proxy-based DML have increasingly focused on strategies that address both intra-class and inter-class variations. DRML (Zheng et al. 2021b) captured both inter-class and intra-class distributions by extracting diverse features, thereby mitigating the issue of discarding intra-class variations. DiVA (Milbich et al. 2020) learned domain-invariant representations by disentangling latent subspaces for domain, class, and residual variations, thus improving generalization to unseen domains. DCML-MDM (Zheng et al. 2021a) employed losses across different sub-embedding compositions to enhance the diversity of the encoded features. HIST (Lim et al. 2022) used hypergraph modeling with semantic tuples to capture multilateral relations, while S2SD (Roth et al. 2021) addressed dimension bottleneck through knowledge distillation, improving generalization.

In contrast, our method emphasizes class-agnostic feature acquisition to promote feature diversification.

Regularization on DML If the embedding space is continuously arranged, it results in a smooth decision boundary, which is crucial for robust model performance. Recognizing class relationships enhances generalization by embedding test samples more accurately in the embedding space (Gu, Ko, and Kim 2021; Venkataramanan et al. 2021). Techniques such as mixup (Zhang et al. 2017; Verma et al. 2019) interpolate between samples to generate synthetic data points, thereby promoting a smooth and semantically consistent embedding space. The integrated mixup methodology (Venkataramanan et al. 2021) further extends this by performing mixup at the input, feature, and embedding levels, significantly enhancing metric learning. The embedding expansion method (Ko and Gu 2020) has been proposed based on query expansion, which synthesizes new features by combining existing features. These new features are used during training to enhance network performance, demonstrating that incorporating samples with additional meaning can help construct a more robust embedding space. Additionally, an augmentation method was implemented to address class imbalance by controlling confidence levels across classes with varying sample sizes (Liu et al. 2020). Classes with a large number of samples tend to dominate the embedding space, leading to inflated confidence values, while tail classes may exhibit erroneously low confidence. To address this issue, artificial samples were introduced for the tail classes, allowing for large confidence values and long-tail data distributions.

In contrast, our method does not rely on sample augmentation. Instead, it uses a probabilistic framework to smooth uncertain regions by leveraging class-agnostic features, offering a more systematic approach than heuristic methods.

Variational Information Bottleneck The broader literature on the information bottleneck (IB) (Tishby, Pereira, and Bialek 2000) introduces a strategy of maximizing the mutual information $I ( Z ; Y )$ while simultaneously minimizing the mutual information $I ( Z ; X )$ . This is intended to preserve information relevant to the label $Y$ within the latent vector $Z$ , while compacting the information associated with the input $X$ . In other words, IB extracts relevant elements by maintaining only important information while discarding unnecessary information. However, due to the computational challenges of calculating mutual information, a variational information bottleneck (VIB) (Alemi et al. 2016) has been proposed. It utilized variational inference to obtain the lower bound of the IB objective. The variational approach allows the IB model to be parameterized using a neural network, known as Deep VIB. It has been successfully applied to high dimensional data across various fields, such as domain generalization (Du et al. 2020), detecting out-of-distributions (Alemi, Fischer, and Dillon 2018), unsupervised clustering (U˘gur, Arvanitakis, and Zaidi 2020), dimensionality reduction (Abdelaleem, Nemenman, and Martini 2023), and multiview representation learning (Bao 2021).

We apply Deep VIB to DML and show that the tradeoff relationship between learning class-specific and classagnostic features can be resolved by expanding the VIB objectives. Class-specific features capture essential information, while class-agnostic features enhance generalization.

# Deep Disentangled Metric Learning

Assumption 1. $X$ is a highly complex, high-dimensional vector represented by an image, containing features of both known and unknown classes.

Definition 1 (Domain). Given an input space $\mathcal { X }$ and a label space $y$ , a domain is defined by the joint distribution $P _ { X , Y }$ , where the random variables $X \in { \mathcal { X } }$ and $Y \in \mathcal { V }$ . The known classes form a subset of $y$ , with their label space denoted by $y _ { k }$ . The unknown classes are represented by $\mathcal { V } _ { u } = \mathcal { V } / \mathcal { V } _ { k }$ .

We propose an extended Variational Information Bottleneck (VIB) method (Alemi et al. 2016) designed to capture features of unknown classes within the bottleneck latent space $Z$ , while learning class-specific features through an additional bottleneck latent $Z ^ { s }$ . The Markov chain structure can be expanded from $Y ( \mathrm { L a b e l } ) ~ \left. ~ X ( \mathrm { I n p u t } ) ~ \right.$ $Z$ (Latent vector) to $Y  X  Z  Z ^ { s }$ . In this framework, $Z$ serves as a bottleneck with a VIB objective for the overall $Y$ , encompassing both known and unknown classes, and is referred to as the embedding bottleneck. Moreover, $Z ^ { s }$ , derived from $Z$ through a probabilistic encoder, is designed with a VIB objective specifically targeting the known class $Y _ { k }$ , which we refer to as the specific bottleneck. By extending the traditional VIB objective to the open-set scenario, we propose a more robust variational information bottleneck objective as follows:

$$
\operatorname* { m a x } \left\{ I ( Z ; Y _ { k } ) + \alpha I ( Z ; Y _ { u } ) + \beta I ( Z ^ { s } ; Y _ { k } ) - \gamma I ( Z ^ { s } ; Z ) \right\} ,
$$

where $\alpha$ , $\beta$ and $\gamma$ are hyper-parameters that controls the relative influence of the terms. In (1), for the embedding bottleneck $Z$ , mutual information with both the known class $Y _ { k }$ and the unknown class $Y _ { u }$ is maximized (i.e., $I ( Z ; Y _ { k } ) +$ $\alpha I ( Z ; Y _ { u } ) )$ . For the specific bottleneck $Z ^ { s }$ , mutual information with $Y _ { k }$ is maximized (i.e., $\beta I ( Z ^ { s } ; Y _ { k } ) )$ , similar to the original VIB (Alemi et al. 2016). At the same time, mutual information between $Z ^ { s }$ and the previous bottleneck $Z$ is minimized to disentangle class-specific features from classagnostic features (i.e., $- \gamma I ( Z ^ { s } ; Z ) )$ . This strategy ensures that $Z ^ { s }$ retains specific and relevant information about $Y _ { k }$ , while minimizing irrelevant information.

To facilitate learning with the proposed neural network with the above objective, we derive the lower bound of (1) and solve the problem by maximizing this lower bound, thereby indirectly but efficiently optimizing the objective. Using the proposed Markov chain and the chain rule of mutual information, the lower bound of (1) can be obtained as

$$
\begin{array} { r l } & { \mathcal { L } = I ( Z ; Y _ { k } ) + \alpha I ( Z ; Y _ { u } ) + \beta I ( Z ^ { * } ; Y _ { k } ) - \gamma I ( Z ; Z ^ { * } ) } \\ & { \geq \displaystyle \int d x d y _ { k } d z p ( x ) p ( y _ { k } | x ) p ( z | x ) \log q ( y _ { k } | z ) } \\ & { + \alpha \displaystyle \int d x d y _ { u } d z p ( x ) p ( y _ { u } | x ) p ( z | x ) \log q ( y _ { u } | z ) } \\ & { + \beta \displaystyle \int d x d y _ { k } d z d z ^ { s } p ( x ) p ( y _ { k } | x ) p ( z | x ) p ( z ^ { s } | z ) \log q ( y _ { k } | z ^ { s } ) } \\ & { - \gamma \displaystyle \int d x d y _ { k } d z d z ^ { s } p ( x ) p ( y _ { k } | x ) p ( z | x ) p ( z ^ { s } | z ) \log \frac { p ( z ^ { s } | z ) } { r ( z ^ { s } ) } , } \end{array}
$$

where $x , y _ { k } , y _ { u }$ , and $z$ are instances of random variables $X$ , $Y _ { k }$ , $Y _ { u }$ , and $Z$ , respectively. $q ( y _ { k } | z ) , q ( y _ { u } | z )$ and $q ( y _ { k } | z ^ { s } )$ are the variational approximations for $p ( y _ { k } | z ) , p ( y _ { u } | z )$ and $p ( y _ { k } | z ^ { s } )$ respectively, while $r ( z ^ { s } )$ is the variational approximation for the marginal distribution $p ( z ^ { s } )$ . Please note that the detailed derivation is found in supplementary materials.

# Optimization of (1)

We approximate (2) using training samples via probabilistic encoders. For this, $p ( x , y _ { k } )$ is approximated by the empirical distribution over the training samples, as follows.

$$
p ( x , y _ { k } ) = p ( x ) p ( y _ { k } | x ) \approx \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \delta ( x - x ^ { n } ) \delta ( y _ { k } - y _ { k } ^ { n } ) ,
$$

where $x ^ { n }$ and $y _ { k } ^ { n }$ denote the $n$ -th training data point and its corresponding label, respectively. $N$ is the total number of training samples, and $\delta ( \cdot )$ denotes the Dirac delta function.

In (2), the first, third, and fourth terms are probabilistic formulations concerning $y ^ { k }$ and can be approximated using the empirical distribution of $p ( x , y _ { k } )$ in (3). However, the formulation of the second term involving $y ^ { u }$ requires an additional approximation. We define $p ( y _ { u } | x )$ as $\begin{array} { r } { p \left( E _ { k } [ p ( y _ { k } | x ) ] = \frac { 1 } { P } \right) } \end{array}$ , which represents the likelihood that the model assigns equal probabilities $\textstyle { \frac { 1 } { P } }$ to all classes $y _ { k }$ where $P$ denotes the number of classes in label $Y _ { k }$ . This approach is both practical and reasonable, as it ensures that the network remains unbiased towards any specific class when encountering an unknown class, thereby evenly distributing the uncertainty across all possible known classes. Therefore, the joint distribution for the unknown class $y _ { u }$ can be approximated as follows:

![](images/cccd81bf105a2e89514e4a7e368694f7d24507bc4d56d6f285f1efc21920ff3f.jpg)  
Figure 2: Illustration of the proposed network structure and the contribution of each loss term. (a) depicts the overall network structure. The network includes two encoders $( f _ { \theta } , \ f _ { \eta } )$ and a decoder $( f _ { \phi } )$ , producing class-agnostic $( z )$ and classspecific $( z ^ { s } )$ features whose likelihoods are evaluated by the decoder. The specific loss increases the likelihood of $z ^ { s }$ , the agnostic loss reduces that of $z$ , and the split loss mitigates their interdependence. (b) illustrates these effects in embedding space, where the black circle represents a sample and the blue cross represents the class proxy.

$$
p ( x , y _ { u } ) = p ( x ) p ( y _ { u } | x ) \approx \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \delta ( x - x ^ { n } ) p ( y _ { u } ^ { n } | x ^ { n } ) .
$$

Then, maximizing the likelihood of $p ( y _ { u } | x )$ is equivalent to ensuring that $\begin{array} { r } { E _ { k } \bar { [ p ( y _ { k } | x ) ] } = \frac { 1 } { P } } \end{array}$ , which can be reframed as minimizing the negative log-likelihood as follows.

$$
\begin{array} { l } { \displaystyle \mathbb { E } _ { p ( z | x ^ { n } ) } [ - \log q ( y _ { u } ^ { n } | z ) ] } \\ { \displaystyle \approx - \frac { 1 } { N } \sum _ { n = 1 } ^ { N } \left[ \int d z p ( z | x ^ { n } ) \log \left( \sum _ { j = 1 } ^ { P } q ( y _ { k } ^ { n } = j | z ) \frac { 1 } { P } \right) \right] , } \end{array}
$$

where we can exclude $y _ { u }$ by expressing it in terms of $y _ { k }$ Thus, $y ^ { n }$ indicates the training data point for $y _ { k }$ , hereafter.

Using (3) and (5), the proposed loss, $\mathcal { L } _ { D D M L }$ , is derived by converting the maximization of the lower bound in (2) into a minimization objective by adding a negative sign:

$$
\begin{array} { l } { \displaystyle \mathcal { L } _ { D D M L } \approx \frac { 1 } { N } \sum _ { n = 1 } ^ { N } [ \underbrace { \mathbb { E } _ { p ( z | x ^ { n } ) } [ - \log q ( y ^ { n } | z ) ] } _ { \mathbf { D M L . l o s s } }  } \\ { \displaystyle  + \alpha \mathbb { E } _ { p ( z | x ^ { n } ) } [ - \log \sum _ { j = 1 } ^ { P } q ( y ^ { n } = j | z ) \cdot \frac { 1 } { P } ] } \\ { \displaystyle , \qquad } \end{array}
$$

where KL refers to Kullback-Leibler divergence.

# Implementation Details

To optimize (6), we implement two stochastic encoders $p ( \boldsymbol { z } | \boldsymbol { \bar { x } } )$ and $p ( z ^ { s } | z )$ and one decoder $q ( y | z ^ { + } ) , z ^ { + } \in \{ z , z ^ { s } \}$ .

• We design the encoder $p ( z | x )$ as a Gaussian distribution $\mathcal { N } ( z | f _ { \theta } ^ { \mu } \bar { ( x ) } , f _ { \theta } ^ { \sigma } ( x ) )$ , where $f _ { \theta }$ represents a DNN with trainable parameters $\theta$ . This network outputs the mean $f _ { \theta } ^ { \mu } ( x )$ and variance $f _ { \theta } ^ { \sigma } ( x )$ . The network has an output dimension of $2 D$ , with the first $D$ dimensions dimensions corresponding to the mean and the remaining $D$ dimensions corresponding to the variance. • We design the encoder $p ( z ^ { s } | z ) = \mathcal { N } ( z ^ { s } | f _ { \eta } ^ { \mu } ( z ) , f _ { \eta } ^ { \sigma } ( z ) )$ , where $f _ { \eta }$ consists of a single fully connected layer with the shape of $D \times 2 D$ using the trainable parameter $\eta$ . • We design the decoder $q ( y | z ^ { + } ) = S o f t m a x ( y | f _ { \phi } ( z ^ { + } ) )$ by adopting a straightforward logistic regression model with the trainable parameter $\phi$ . Here, $\bar { z } ^ { + } \in \{ z , z ^ { s } \}$ , $f _ { \phi } ( z ^ { + } ) = W z ^ { + }$ , and $W$ maps the $D$ -dimensional embedding space to the logit space of $y$ . • To compute the KL divergence, we design $r ( z ^ { s } ) =$ $\mathcal { N } ( z ^ { s } | 0 , I )$ as a $D$ -dimensional Gaussian distribution.

Specific loss: The third term of (6) facilitates the learning of distinct features $z _ { s }$ (Fig.2(b)). The input image is processed sequentially through DNNs $f _ { \theta }$ and $f _ { \eta }$ , resulting in class-specific features $f _ { \eta } ^ { \mu }$ and $f _ { \eta } ^ { \sigma }$ via the reparameterization trick (Fig.2(a)). The decoder $f _ { \phi }$ computes the likelihood of the input image belonging to the target class by projecting the feature into the logit space.

Agnostic loss: The second term in (6) learns class-agnostic features $f _ { \theta } ^ { \mu } , f _ { \theta } ^ { \sigma }$ using the reparameterization trick by feeding the input image into $f _ { \theta }$ . The agnostic features are then fed into the decoder $f _ { \phi }$ to enforce a uniform probability of $\textstyle { \frac { 1 } { P } }$ for all classes. Using the same decoder $f _ { \phi }$ is crucial, as it has been trained with class-specific features. Following the learning of representative features through the specific loss (Fig.2(b)), the agnostic loss ensures that the similarity between the agnostic feature and the representative features of all classes follows a uniform distribution.

Table 1: Performance evaluation of our method combined with different metric learning algorithms in terms of Recall $@ \mathbf { k }$ We used NormSoftmax(NS) (Zhai and Wu 2018), ProxyAnchor(PA) (Kim et al. 2020), and HypViT (Ermolov et al. 2022), as the baseline algorithms. The proposed disentangled module based on class-agnostic regularization in (6) was plugged into the baseline networks, resulting in an increase in accuracy for each baseline network. The notation $\mu \pm \sigma$ represents the mean and standard deviation of three runs, and bold numbers indicate performance improvements by adding DDML to the baseline.   

<html><body><table><tr><td rowspan="2"></td><td colspan="3">CUB</td><td colspan="3">一 CAR</td><td colspan="3">一 SOP</td></tr><tr><td>R@1</td><td>R@2</td><td>R@4</td><td>R@1</td><td>R@2</td><td>R@4</td><td>R@1</td><td>R@10</td><td>R@100</td></tr><tr><td>NS(Zhai and Wu 2018)</td><td>64.65± 0.09</td><td>75.68 ± 0.14</td><td>84.20 ± 0.13</td><td>83.54± 0.09</td><td>89.67 ± 0.05</td><td>94.06 ± 0.05</td><td>78.55 ± 0.04</td><td>90.43 ± 0.05</td><td>96.02 ± 0.02</td></tr><tr><td>DDML(+NS)</td><td>65.96 ± 0.06</td><td>76.58 ± 0.03</td><td>84.56 ± 0.26</td><td>84.85 ± 0.07</td><td>90.06 ± 0.07</td><td>94.16 ± 0.04</td><td>79.38 ± 0.03</td><td>90.56 ± 0.06</td><td>96.15 ± 0.02</td></tr><tr><td>PA(Kim et al. 2020)</td><td>68.36 ± 0.04</td><td>78.92 ± 0.11</td><td>86.45 ± 0.65</td><td>85.99 ± 0.07</td><td>91.65 ± 0.17</td><td>95.07 ± 0.20</td><td>78.84 ± 0.18</td><td>90.70 ±0.05</td><td>96.09 ± 0.08</td></tr><tr><td>DDML(+PA)</td><td>69.91 ± 0.05</td><td>79.55 ± 0.06</td><td>87.24 ± 0.15</td><td>87.86 ± 0.06</td><td>92.57 ± 0.16</td><td>95.71 ± 0.05</td><td>79.87 ± 0.04</td><td>91.09 ± 0.03</td><td>98.80 ± 0.02</td></tr><tr><td>HypViT(Ermolov et al. 2022)</td><td>85.53± 0.07</td><td>91.29 ± 0.08</td><td>94.75 ± 0.06</td><td>89.08 ± 0.06</td><td>93.97 ± 0.12</td><td>96.52 ± 0.18</td><td>85.86± 0.04</td><td>94.86 ± 0.04</td><td>98.06 ± 0.05</td></tr><tr><td>DDML(+HypViT)</td><td>85.98 ± 0.05</td><td>91.53 ± 0.03</td><td>95.10 ± 0.06</td><td>89.40 ±0.02</td><td>94.14 ± 0.05</td><td>96.73 ± 0.03</td><td>86.08 ±0.05</td><td>95.01 ± 0.05</td><td>98.19 ± 0.04</td></tr></table></body></html>

Split loss: The fourth term in (6) minimizes the KL divergence between $p ( z ^ { s } | z )$ and the prior $z ^ { s }$ (Fig.2(a)). This enforces $z$ to have a diminished effect on $z ^ { s }$ , as shown in Fig.2(b). When $f _ { \eta }$ is trained using this loss, the relevance between class-agnostic and class-specific features is reduced.

Please note that our method supports regularization, and to ensure seamless integration with the existing DML loss, the first term in (6) can be formulated to apply the standard DML loss such as NormSoftmax (Zhai and Wu 2018) and Proxy Anchor (Kim et al. 2020).

# Experiments

We analyzed the performance of our method by integrating our disentangled module into existing DML methods to highlight the benefits of disentanglement. We compared the proposed method with other state-of-the-art DML approaches and evaluated our method against regularization techniques. Finally, we conducted an ablation study on hyperparameters to interpret the impact of each loss term.

# Experimental Settings

For experiments, we followed the protocol outlined in (Oh Song et al. 2016). To evaluate the DML methods, we utilized several benchmark datasets for metric learning: Caltech-UCSD Birds (CUB) (Wah et al. 2011), CARS196 (CAR) (Krause et al. 2013), and Stanford Online Products (SOP) (Oh Song et al. 2016). To demonstrate the applicability of the proposed method to conventional DML methods, we selected three baseline methods to which we attached $\mathcal { L } _ { D D M L }$ : NormSoftmax (Zhai and $\mathrm { W u } 2 0 1 8$ ), which is a fundamental method, and Proxy Anchor (Kim et al. 2020), known for its strong performance in CNN-based approaches, and the recent HypViT (Ermolov et al. 2022) based on Vision transformer (Dosovitskiy et al. 2020).

Since the proposed method is a regularization technique added to the baseline, it is essential to follow the baseline’s experimental settings closely to ensure a fair performance comparison. Therefore, hyperparameters, such as the optimizer type, learning rate, weight decay parameter, embedding space dimension, and batch size, were kept consistent with those used in the baseline methods. In all three baseline methods, the structure up to the embedding space $Z$ remained consistent with each respective method, and a linear layer was added from the embedding bottleneck $Z$ to the specific bottleneck $Z ^ { s }$ . For NormSoftmax and ProxyAnchor, which are proxy-based metric learning methods, the existing decoders from the embedding space to the label space were utilized. However, for HypViT, a pair-based metric learning method, the same decoder used in NormSoftmax was added. Note that details are found in the supplementary materials.

# Performance Improvement By Our DDML

Table 1 shows that the proposed method consistently improved performance for all baseline algorithms across all datasets. We used NormSoftmax(NS) (Zhai and $\mathrm { \sf W u } 2 0 1 8$ ), ProxyAnchor(PA) (Kim et al. 2020), and HypViT (Ermolov et al. 2022) as the baseline algorithms. The DDML framework utilized the baseline network as the DML loss in (6), resulting in enhanced performance for each of the baseline network, with an improvement in recall of up to $1 . 4 \%$ . The combination of HypViT (Ermolov et al. 2022) and our method exhibited the best performance. Although the baseline methods already demonstrated high performance, the incorporation of our disentangled module in (6) yielded a further substantial enhancement in performance.

# Comparison With Other Methods

Table 2 compares our method with other state-of-the-art methods, with results organized by the backbone network. When BN-Inception was used as the backbone, the proposed method, which applies DDML to ProxyAnchor, achieved the best performance across all recall values on the SOP dataset. Although it might not have the highest performance across all recall values on the CUB and CAR datasets, it did achieve the best $\mathbf { R } @ 1$ , which is the most critical metric. Similarly, when ResNet was used as the backbone, the proposed ProxyAnchor with DDML method also delivered the best $\mathbf { R } @ 1$ performance across all datasets, while also demonstrating strong performance in other recall metrics. When ViT was used as the backbone, the proposed HypViT (Ermolov et al. 2022) with the DDML method achieved the best performance across all datasets, with the exception of the $\mathbb { R } \ @ 1$ score on the CUB dataset. It is noteworthy that VPTSP-G, which achieved the best performance on the CUB dataset in terms of $\mathbf { R } @ 1$ , leveraged Visual Prompts (VPT) as an additional component during training. Although the baseline HypViT showed superior performance attributed to a larger pre-training set, the performance was further enhanced by incorporating our DDML method. More experiment results (e.g., MLRC evaluation, proxy-to-proxy affinity matrix, logit visualization and experiments with class-imbalanced data) are included in supplementary materials.

<html><body><table><tr><td>一</td><td></td><td colspan="4">CUB</td><td colspan="4">CAR</td><td colspan="4">SOP</td></tr><tr><td></td><td>R@k</td><td>1</td><td>2</td><td>4</td><td>8</td><td>1</td><td>2</td><td>4</td><td>8</td><td>1</td><td>10</td><td>100</td><td>1000</td></tr><tr><td rowspan="12"></td><td>NormSoftmax(Zhai and Wu 2018)t</td><td>55.3</td><td>67.0</td><td>77.6</td><td>85.4</td><td>75.2</td><td>84.7</td><td>90.4</td><td>94.2</td><td>69.0</td><td>84.5</td><td>93.1</td><td>-</td></tr><tr><td>MS(Wang et al. 2019)t</td><td>65.7</td><td>77.0</td><td>86.3</td><td>91.2</td><td>84.1</td><td>90.4</td><td>94.0</td><td>96.5</td><td>78.2</td><td>90.5</td><td>96.0</td><td>98.7</td></tr><tr><td>SoftTriple(Qian et al. 2019)†</td><td>65.4</td><td>76.4</td><td>84.5</td><td>90.4</td><td>84.5</td><td>90.7</td><td>94.5</td><td>96.9</td><td>78.3</td><td>90.3</td><td>95.9</td><td></td></tr><tr><td>ProxyAnchor(Kim et al.2020)t</td><td>68.4</td><td>79.2</td><td>86.8</td><td>91.6</td><td>86.1</td><td>91.7</td><td>95.0</td><td>97.3</td><td>80.3</td><td>91.4</td><td>96.4</td><td>98.7</td></tr><tr><td>ProxyGML(Zhu et al.2020)+</td><td>66.6</td><td>77.6</td><td>86.4</td><td></td><td>85.5</td><td>91.8</td><td>95.6</td><td>1</td><td>78.0</td><td>90.6</td><td>96.2</td><td></td></tr><tr><td>DRML(Zheng et al.2021b)+</td><td>68.7</td><td>78.6</td><td>86.3</td><td>91.6</td><td>86.9</td><td>92.1</td><td>95.2</td><td>97.4</td><td>79.9</td><td>90.7</td><td>96.1</td><td>-</td></tr><tr><td>CircleLoss(Sun et al.2020)t</td><td>66.7</td><td>77.4</td><td>86.2</td><td>91.2</td><td>83.4</td><td>89.8</td><td>94.1</td><td>96.5</td><td>78.3</td><td>90.5</td><td>96.1</td><td>98.6</td></tr><tr><td>DAM(Xu et al. 2021)†</td><td>69.1</td><td>79.8</td><td>87.2</td><td>91.8</td><td>86.9</td><td>92.1</td><td>95.3</td><td>97.9</td><td></td><td></td><td></td><td></td></tr><tr><td>PADs(Roth,Milbich,and Ommer 2020)†</td><td>66.6</td><td>77.2</td><td>85.6</td><td></td><td>81.7</td><td>88.3</td><td>93.0</td><td></td><td>--</td><td>--</td><td>_-</td><td></td></tr><tr><td>HIST(Lim et al. 2022)t</td><td>69.7</td><td>80.0</td><td>87.3</td><td></td><td>87.4</td><td>92.5</td><td>95.4</td><td>-_</td><td>79.6</td><td>91.0</td><td>96.2</td><td>-</td></tr><tr><td>DDML(+ProxyAnchor(Kim et al. 2020))†</td><td>70.0</td><td>79.6</td><td>87.2</td><td>92.0</td><td>87.8</td><td>92.6</td><td>95.7</td><td>97.4</td><td>79.9</td><td>91.1</td><td>96.4</td><td>98.8</td></tr><tr><td>NormSoftmax(Zhai and Wu 2018)t Div&Conq(Sanakoyeu et al.2019)*</td><td>61.3</td><td>73.9</td><td>83.5</td><td>90.0</td><td>84.2</td><td>90.4</td><td>94.4</td><td>96.9</td><td>78.2</td><td>90.6</td><td>96.2</td><td>-</td></tr><tr><td></td><td></td><td>65.9</td><td>76.6</td><td>84.4</td><td>90.6</td><td>84.6</td><td>90.7</td><td>94.1</td><td>96.5</td><td>75.9</td><td>88.4</td><td>94.9</td><td>98.1</td></tr><tr><td></td><td>MIC(Rth,ibidm</td><td>66.1</td><td>76.8</td><td>85.</td><td></td><td>82.6</td><td>89.1</td><td>93.8</td><td></td><td></td><td>894</td><td></td><td>--</td></tr><tr><td>RankMI(Kemertas et al.2020)*</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>--</td><td>77.23</td><td></td><td>95.4</td><td></td></tr><tr><td></td><td></td><td>66.7</td><td>77.2</td><td>85.1</td><td>91.0</td><td>83.3</td><td>89.8</td><td>93.8</td><td>96.5</td><td>74.3</td><td>87.9</td><td>94.9</td><td>98.3</td></tr><tr><td></td><td>EPSHN(Xuan, Stylianou,and Pless 2020)†</td><td>64.9</td><td>75.3</td><td>83.5</td><td>，</td><td>82.7</td><td>89.3</td><td>93.0</td><td></td><td>78.3</td><td>90.7</td><td>96.3</td><td></td></tr><tr><td>DiVA(Milbich et al.2020)†</td><td></td><td>69.2 79.3</td><td></td><td></td><td></td><td>87.6</td><td>92.9</td><td>1</td><td>-</td><td>79.6</td><td>91.2</td><td></td><td></td></tr><tr><td></td><td>ProxyAnchor(Kim et al. 2020)†</td><td>69.7</td><td>80.0</td><td>87.0</td><td>92.4</td><td>87.7</td><td>92.9</td><td>95.8</td><td>97.9</td><td>1</td><td></td><td>，</td><td></td></tr><tr><td></td><td>DCML-MDW(Zheng et al. 2021b)†</td><td>68.4</td><td>77.9</td><td>86.1</td><td>91.7</td><td>85.2</td><td>91.8</td><td>96.0</td><td>98.0</td><td>79.8</td><td>90.8</td><td>95.8</td><td></td></tr><tr><td></td><td>IBC(Seidenschwarz,Elezi,and Leal-Taixé 2021)t</td><td>70.3</td><td>80.3</td><td>87.6</td><td>92.7</td><td>88.1</td><td>93.3</td><td>96.2</td><td>98.2</td><td>81.4</td><td>91.3</td><td>95.9</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>HIT(im,tal222d Kwak)t</td><td>71.4</td><td>81.4</td><td>88.1</td><td>--</td><td>82</td><td>93.0</td><td>96.6</td><td>--</td><td>81.4</td><td>92.0</td><td>96.7</td><td>_-</td></tr><tr><td></td><td> DDML(+cosFace(Wang et al. 2018))t</td><td>66.6</td><td>76.7</td><td>85.0</td><td></td><td></td><td>90.2</td><td>94.4</td><td>96.6</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td> DDML(+SphereFace(Liu et al. 2017)) †</td><td></td><td></td><td></td><td>90.6</td><td>84.6</td><td></td><td></td><td></td><td>79.1</td><td>90.6</td><td>95.9</td><td>98.5</td></tr><tr><td></td><td></td><td>65.7</td><td>76.8</td><td>84.9</td><td>90.5</td><td>84.8</td><td>90.5</td><td>94.2</td><td>97.2</td><td>78.9</td><td>90.7</td><td>95.9</td><td>98.5</td></tr><tr><td> DDML(+ProxyAnchor(Kim et al. 2020))+</td><td>DDML(+ArcFace(Deng et al. 2019))t</td><td>66.7</td><td>77.4</td><td>85.0</td><td>90.7</td><td>85.0</td><td>90.8</td><td>94.8</td><td>96.8</td><td>79.1</td><td>90.6</td><td>95.7</td><td>98.6</td></tr><tr><td rowspan="5"></td><td></td><td>71.5</td><td>81.2</td><td>88.0</td><td>92.6</td><td>89.6</td><td>93.9</td><td>95.9</td><td>97.6</td><td> 81.5</td><td>91.7</td><td>96.6</td><td>99.0</td></tr><tr><td>HypViT(Ermolov et al. 2022)$</td><td>85.6</td><td>91.4</td><td>94.8</td><td>96.7</td><td>89.2</td><td>94.1</td><td>96.7</td><td>98.1</td><td></td><td>85.9</td><td>94.9</td><td>98.1</td><td>99.5</td></tr><tr><td>HPSPKimengl.ndawak2023)s</td><td>85.7</td><td>91.3</td><td>948</td><td></td><td>88.3</td><td></td><td>93.2 96.1</td></table></body></html>

Table 2: Quantitative comparison on the CUB, CAR, and SOP datasets in terms of Recall $@ \mathbf { k }$ . The symbols $*$ , $\ S$ and $\dagger$ represen embedding space sizes of 128, 384 and 512, respectively. The best performance for each backbone was boldfaced.

Table 3: Quantitative comparison with other regularization techniques in terms of Recall $\ @ 1$ . BB refers to the backbone, Base indicates the type of baseline algorithm, and Reg. represents the regularization method.   

<html><body><table><tr><td rowspan="2">BB Base Reg.</td><td colspan="3">BN-Inception ProxyAnchor</td><td colspan="3">ResNet50 ProxyAnchor</td><td colspan="2">ViT HypViT</td></tr><tr><td></td><td>PS</td><td>DDML</td><td></td><td>Metrix</td><td>DDML</td><td></td><td>DDML</td></tr><tr><td>CUB</td><td>68.4</td><td>69.2</td><td>70.0</td><td>69.7</td><td>70.4</td><td>71.5</td><td>85.6</td><td>86.0</td></tr><tr><td>CAR</td><td>86.1</td><td>86.9</td><td>87.8</td><td>87.7</td><td>88.5</td><td>89.6</td><td>89.2</td><td>89.4</td></tr><tr><td>SOP</td><td>79.1</td><td>79.8</td><td>79.9</td><td></td><td>81.3</td><td>81.5</td><td>85.9</td><td>86.1</td></tr></table></body></html>

Table 3 provides a quantitative comparison with recent DML regularization methods. PS (Gu, Ko, and Kim 2021)

![](images/85377830c4e27d4842b9075bae0323cfa7e144cb17bd503085a7f9b65d69609d.jpg)  
Figure 3: Visualization on $Z$ and $Z ^ { s }$ . First row: input images, Second row: GradCam visualization for the agnostic loss, Third row: GradCam visualization for the specific loss. We can observe a complementary relationship between agnostic and specific losses.

employed a mixup strategy in the embedding space to augment synthetic samples, thereby enhancing generalization performance. Metrix (Venkataramanan et al. 2021) extended this approach with an integrated mixup at the input, embedding, and feature levels. Both studies presented experimental results using ProxyAnchor as the baseline. However, PS was evaluated solely with a BN-Inception backbone, and Metrix was evaluated only with a ResNet50 backbone. In contrast, our proposed method, $\mathrm { { D D M L } ( + P A ) }$ , provides a comprehensive comparison by demonstrating performance across both BN-Inception and ResNet50 backbones. In the CUB and CAR datasets, $\mathrm { { D D M L } ( + P A ) }$ showed the highest performance improvement, achieving more than a $1 . 5 ~ \%$ gain over the baseline method, at least a $0 . 8 \%$ gain over PS, and over a $1 . 0 \%$ gain compared to Metrix. For the SOP dataset, while $\mathrm { { D D M L } ( + P A ) }$ significantly improved upon the baseline, its performance was comparable to that of other regularization methods. Although the performance of the probabilistic model may vary with the number of classes and images per class, the consistent improvement over the baseline and the comparable performance with other regularization methods highlight the effectiveness of the proposed DDML as a regularization approach.

Table 4: Ablation study on the proposed losses. Numbers indicate the Recall $\ @ 1$ values.   

<html><body><table><tr><td>Agnostic loss</td><td>Specific loss</td><td>Split loss</td><td>CUB</td><td>CAR</td><td>SOP</td></tr><tr><td>-</td><td></td><td></td><td>64.7</td><td>83.5</td><td>78.6</td></tr><tr><td>√</td><td></td><td></td><td>64.9</td><td>84.2</td><td>78.8</td></tr><tr><td></td><td>√</td><td>- -</td><td>64.8</td><td>84.0</td><td>78.6</td></tr><tr><td>-</td><td>-</td><td>√</td><td>64.5</td><td>83.1</td><td>78.6</td></tr><tr><td>√</td><td>√</td><td></td><td>65.1</td><td></td><td></td></tr><tr><td>√</td><td></td><td></td><td></td><td>84.5</td><td>79.1</td></tr><tr><td></td><td>1 √</td><td>√</td><td>64.7</td><td>83.4</td><td>78.0</td></tr><tr><td></td><td></td><td>√</td><td>65.0</td><td>84.2</td><td>78.7</td></tr><tr><td>√</td><td>√</td><td>√</td><td>66.2</td><td>84.9</td><td>79.4</td></tr></table></body></html>

# Analysis on Proposed Losses

Fig.3 shows the activation map obtained by GradCam (Selvaraju et al. 2017) for both the agnostic loss and the specific loss in $\mathcal { L } _ { D D M L }$ of (6) for CUB training data. For all input images, the GradCam was activated for different regions for each loss. As shown in the last row of Fig.3, the GradCam visualization for the specific loss exhibited activation either on the facial region or the entirety of the bird, both of which could be crucial attributes associated with birds. In contrast, the GradCam visualization for the agnostic loss exhibits different activation patterns, as shown in the middle row of Fig.3. Especially concerning the rightmost bird, the specific loss tends to activate the beak and legs, whereas the agnostic loss focuses on the head feathers. This observation indicates a complementary relationship between the agnostic loss and the specific loss.

Table 4 compares our disentangled module in (6) across various combinations of agnostic, specific, and split losses on the CUB, CAR, and SOP datasets. Using the proxybased algorithm NormSoftmax to compute the DML loss $( \mathcal { L } D M \bar { L } )$ , we consistently applied $\mathcal { L } D M L$ while controlling the additional losses by setting hyperparameters $\alpha , \beta , \gamma$ to specific values $( \alpha = 1$ , $\beta = 1$ , $\gamma = 1 e - 7 )$ or to zero to disable them. The baseline performance (Row 1) represents NormSoftmax without additional losses. Rows 2 to 4 indicate that applying any single loss term improves performance, with agnostic and specific losses increasing performance by approximately $0 . 5 \%$ on the CAR dataset. Rows 5 to 7 demonstrate that combining agnostic and specific losses yields a $0 . 4 \%$ to $1 . 0 \%$ improvement across all datasets, outperforming each loss individually. The final row shows that incorporating all three losses achieves the highest performance, with improvements of $1 . 5 \%$ , $1 . 4 \%$ , and $0 . 8 \%$ on the CUB, CAR, and SOP datasets, respectively. Adding the split loss further enhances performance by $1 . 1 \%$ , $0 . 4 \%$ , and $0 . 3 \%$ , demonstrating its effectiveness in stabilizing learning by reducing dependency between class-agnostic and classspecific features. In conclusion, all three losses are essential for improving DML performance.

![](images/537a342f8b96593c6e7b548434b02dbd1f74e5c52c40bf127595b628acddf29d.jpg)  
Figure 4: Relationship between the three hyperparameters and performance. $\mathbb { R } \ @ 1$ performance with ProxyAnchor: (a) varying $\beta$ and $\gamma$ while fixing $\alpha$ , (b) varying $\alpha$ and $\gamma$ while fixing $\beta$ , and (c) varying $\alpha$ and $\beta$ while fixing $\gamma$ . These results indicate a consistent relationship between each hyperparameter and performance. Expanded figures are in the supplementary materials.

We investigated the effects of three hyperparameters—agnostic loss $( \alpha ; 1 e - 7 , 1 e - 6 , 1 e - 5 , 1 e - 4 , 1 e - 3 )$ , specific loss $( \beta \colon 0 . 1 , 1 )$ , and split loss $( \gamma \colon 0 . 0 0 1 , 0 . 0 1 , 0 . 1 ,$ ， 1)—on $\mathbf { R } @ 1$ performance using the CUB dataset with ProxyAnchor as the baseline and BN-Inception as the backbone. Fig.4 illustrates that $\mathbb { R } \ @ 1$ performance improves as $\gamma$ decreases for fixed $\alpha$ values (a) and as $\alpha$ increases for fixed $\beta$ values (b). Additionally, higher $\alpha$ values consistently enhance performance regardless of $\gamma$ (c). These results demonstrate a stable and consistent relationship between each hyperparameter and performance, highlighting the robustness of the proposed method.

# Conclusion

We propose a DML method for enhancing generalization performance by leveraging the disentangled representations of both class-specific and class-agnostic features. Our motivation stems from the limitations of NormSoftmax methods, which often exhibit inconsistent gradient activations for unseen classes. We introduce class-agnostic regularization that can smooth uncertain regions and induce stable semantic representations for unseen classes in the embedding space. Furthermore, we have optimized our method by integrating the VIB objectives into DML. Experimental results validate the effectiveness of the proposed optimization method for class-agnostic regularization, emphasizing its seamless integration with existing DML algorithms and consistent performance improvements.

# Acknowledgments

This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (2021-0-01341, Artificial Intelligence Graduate School Program (ChungAng university)).