# Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage

Md Rafi Ur Rashid1, 2, Jing Liu1, Toshiaki Koike-Akino1, Ye Wang1, Shagufta Mehnaz2

1Mitsubishi Electric Research Laboratories 2Pennsylvania State University mur5028@psu.edu, jiliu@merl.com, koike@merl.com, yewang@merl.com, smehnaz@psu.edu.

# Abstract

Fine-tuning large language models on private data for downstream applications poses significant privacy risks in potentially exposing sensitive information. Several popular community platforms now offer convenient distribution of a large variety of pre-trained models, allowing anyone to publish without rigorous verification. This scenario creates a privacy threat, as pre-trained models can be intentionally crafted to compromise the privacy of fine-tuning datasets. In this study, we introduce a novel poisoning technique that uses modelunlearning as an attack tool. This approach manipulates a pre-trained language model to increase the leakage of private data during the fine-tuning process. Our method enhances both membership inference and data extraction attacks while preserving model utility. Experimental results across different models, datasets, and fine-tuning setups demonstrate that our attacks significantly surpass baseline performance. This work serves as a cautionary note for users who download pretrained models from unverified sources, highlighting the potential risks involved.

# Extended version — https://arxiv.org/abs/2408.17354

# Introduction

In recent times, the traditional way of training a language model (LM) from scratch has been largely replaced by the introduction of pre-trained foundation models (Touvron et al. 2023; Chiang et al. 2023). For example, the Hugging Face Hub is a platform with over 120k open-source models, readily available for download and any registered user can contribute by uploading their own model. However, there are serious security and privacy risks associated with downloading such models from any untrusted sources and further fine-tuning them for some downstream applications as they could be maliciously crafted (Trame\`r et al. 2022; Kandpal et al. 2023; Hu et al. 2022). Additionally, the public release of large language models (LLMs) fine-tuned on potentially sensitive user data could lead to privacy breaches, as these models have been found to memorize verbatim text from their training data (Carlini et al. 2019, 2021). In this paper, we combine the notion of poisoning a pre-trained LLM and causing privacy leakage of the fine-tuned model. More specifically, we introduce a novel model poisoning algorithm that aims to manipulate a pre-trained LLM in order to disclose more of the private data used during its fine-tuning.

At its core, our approach leverages machine unlearning (Cao and Yang 2015; Guo et al. 2019) to poison the pretrained LLM. The original objective of unlearning is to make the model forget specific data points that it has seen during training so that it produces a high loss for those data points, and it becomes difficult to reconstruct those samples (Gu et al. 2024). Motivated by data augmentation that reduces overfitting, we discovered that unlearning on some noisy version of fine-tuning data points can promote overfitting of the original data during the fine-tuning process.

However, it is important to have control over the process of loss maximization; otherwise, the model might become unusable and the poisoning attempt would be easily detectable. Hence, we propose bounded unlearning as a poisoning tool, where we maximize loss in a controlled manner on the pre-trained model for some noisy data points to increase privacy leakage of the fine-tuned LLM without compromising its utility.

To measure the privacy leakage caused by our proposed method, we consider two standard privacy attacks: membership inference (MIA) (Shokri et al. 2017a; Carlini et al. 2022a) and data extraction (DEA) (Nasr et al. 2023; Rashid et al. 2023). In MIA, the model is queried to evaluate whether a specific target data point that the attacker possesses was indeed part of the finetuning dataset. On the contrary, DEA aims to extract verbatim texts from the finetuning dataset with partial/zero prior knowledge. We evaluate our proposed method for both of these attacks on a range of language models (Llama2-7B, GPT-Neo 1.3B), datasets (MIND, Wiki- $1 0 3 +$ AI4Privacy), fine-tuning methods (FullFT, LoRA-FT, QLoRA-FT), and defense (differential privacy). Overall, our method significantly boosts the MIA and DEA attack performance over the baselines in almost all scenarios and maintains its stealth by preserving model utility. Prior works that deal with privacy leakage through pretrained model poisoning pose some strong assumptions on the adversary’s capability, as discussed in the Related Work section of the paper. Our proposed method, on the other hand, with a more practical threat model and weaker adversarial ability, substantially enhances the attack success rate and still remains stealthy.

# Threat Model

In this section, we explain the threat model for both the membership inference and data extraction game:

# The Membership Inference Game

❒ Access to Pre-trained LLM: The attacker has access to a pre-trained large language model denoted as $\theta _ { \mathrm { p r e } }$ . Additionally, the attacker is given a challenge dataset $\bar { D } _ { c }$ , which includes some member data $d$ and non-member data $d _ { \ominus }$ .

❒ Poisoning Phase: The attacker employs a poisoning algorithm $T _ { \mathrm { a d v } }$ to manipulate the pre-trained model $\theta _ { \mathrm { p r e } }$ , resulting in an adversarially altered model $\theta _ { \mathrm { a d v } }$ .

❒ Model Distribution: The adversarially poisoned model $\theta _ { \mathrm { a d v } }$ is distributed to the challenger. The challenger then fine-tunes $\theta _ { \mathrm { a d v } }$ with their private dataset $D _ { \mathrm { f t } }$ , resulting in the fine-tuned model $\theta _ { \mathrm { f t } }$

❒ Black Box Access: Post fine-tuning, the attacker is granted black box query access to the fine-tuned model, $\theta _ { \mathrm { f t } }$ . Through this access, the attacker can submit inputs and receive outputs (both generated text and model loss) from $\theta _ { \mathrm { f t } }$ .

❒ Attacker’s Objective: The primary goal of the attacker is to identify the membership of specific samples within the challenge dataset, $D _ { c }$ . This involves determining whether a given sample belongs to $D _ { \mathrm { f t } }$ or not.

# The Data Extraction Game

❒ Access to Pre-trained LLM: Similar to the MI case, the attacker has access to a pre-trained LLM, $\theta _ { \mathrm { p r e } }$ . However, in this case, he is given only partial knowledge of the training dataset as the challenge dataset, which consists of the prefixes of the training data samples, denoted as $P _ { c }$ .

❒ Poisoning Phase: This step is the same as MIA. ❒ Model Distribution: This step is the same as MIA. ❒ Black Box Access: Post fine-tuning, the attacker is granted black box query access to the fine-tuned model $\theta _ { \mathrm { f t } }$ . Through this black box access, the attacker can submit input prompts and receive the generated text as output from $\theta _ { \mathrm { f t } }$ .

❒Attacker’s Objective: The primary goal of the attacker is to successfully reconstruct the suffix, $S _ { c }$ , which is present in $D _ { \mathrm { f t } }$ , for each corresponding prefix in $P _ { c }$ .

# Motivation

Overfitting is a leading factor contributing to vulnerability to membership inference attacks (Amit, Goldsteen, and Farkash 2024; Shokri et al. 2017b; Dionysiou and Athanasopoulos 2023; He et al. 2022). When training a language model for some downstream application, the initial state of the model’s parameters plays a crucial role in the learning process. Typically, these parameters are either randomly initialized when training from scratch or set to general pretrained weights, which are the result of rigorous pre-training on a large corpus of text data. Consequently, at the onset of training, the model does not exhibit a strong predisposition or bias towards any specific training data points. Further fine-tuning on downstream data $D _ { \mathrm { f t } }$ is more prone to overfitting. However, as we will discuss later in Figure 2, it is still non-trivial for an attacker to distinguish between member

Trusted Source Untrusted Source Dft 1 Finetune 看。？ API ? Data Membership 7 Extraction Inference

![](images/53e640df24af38724d2717dd459dd7cefce40a6c31d3d2792258e4cfb495e952.jpg)  
Figure 1: Overview of the threat model and steps of the attack: (1) Attacker downloads a pre-trained LLM, (2) Poisons the model with an algorithm, $\mathcal { T } _ { \mathrm { a d v } }$ , and (3) release the model. (4) The victim downloads the poisoned LLM, (5) fine-tunes on their private data, and (6) releases the APIbased query access to the model. (7) Finally, the adversary conducts membership inference or data extraction.   
Figure 2: Histograms of loss values on pre-trained model $\theta _ { \mathrm { p r e } }$ , fine-tuned model $\theta _ { \mathrm { f t } }$ , and fine-tuned poisoned model $\theta _ { \mathrm { f t } } ^ { \mathrm { a d v } }$

and non-member data, which might have similar data distributions. One key question we try to answer is this:

RQ1: Is it possible to poison the pre-trained model to make the fine-tuning process overfit even more and the resulting fine-tuned model more vulnerable to privacy leakage attacks? In this work, we introduce an unlearning-based model poisoning technique and give a sure answer to the above research question. This answer is supported by several observations, findings, and experimental results, which we will discuss gradually.

Motivations of Leveraging Unlearning We want to poison the model to induce it to overfit during the fine-tuning process. It is quite challenging to come up with a method for poisoning. However, we can think of the opposite side first: How to prevent a model from overfitting? Recall that overfitting occurs when a model learns the training data too well and is unable to generalize to new data. One simple and effective approach is Data Augmentation. Data augmentation is a well-known technique used in machine learning to artificially create more data points from existing data. This can be done by applying different transformations to the data, and one popular transform is noise perturbation. Training on original samples together with their noisy versions can help reduce model overfitting (Wei and Zou 2019). On the contrary, as we want to increase overfitting in the fine-tuning procedure, it now becomes intuitive to leverage unlearning/ reverse-training on the noisy versions of training samples.

The challenge dataset, $D _ { c }$ , consists of both member data points, $d$ , and non-member data points, $d _ { \ominus }$ $( D _ { c } = d \cup d _ { \ominus } )$ . We propose and validate some methods to generate the noisy versions of $D _ { c }$ , denoted as $D _ { c } ^ { \prime }$ $) _ { c } ^ { \prime } ( D _ { c } ^ { \prime } = d ^ { \prime } \cup \bar { d } _ { \ominus } ^ { \prime } )$ , and the strategic maximization of the loss associated with $D _ { c } ^ { \prime }$ to poison the model, which will be discussed in detail in next section.

Observation: Members and Non-members from Same Data Distributions are Hard to Separate Figure 2 shows the histograms of loss values of member data $d$ and nonmember data $d _ { \ominus }$ , on pre-trained model, $\theta _ { p r e }$ (green color) and fine-tuned model, $\theta _ { f t }$ (blue color). Here, $d$ and $d _ { \ominus }$ come from similar distributions. As expected, before fine-tuning, it’s not possible to infer membership based on the difference in loss value histograms (green solid line vs. green dotted line). After fine-tuning, the loss values of $d$ decrease. However, as $d _ { \ominus }$ have similar data distributions to member data, their loss values also decrease, making it still hard to distinguish the membership based on the loss values after finetuning (blue solid line vs. blue dashed line).

Findings: Unlearning Amplifies Overfitting Figure 2 also shows the histograms of loss values of $d$ and $d _ { \ominus }$ after fine-tuning on the poisoned (via unlearning) model, θfatdv (red color). Note that the unlearning is performed on $D _ { c } ^ { \prime }$ . We get two crucial insights from here: first, compared with fine-tuning on the non-poisoned model (blue solid line), we can see that fine-tuning on the poisoned model can reduce the loss value of member data even more (red solid line). Second, the difference in loss values between $d$ and $d _ { \ominus }$ is amplified after fine-tuning on poisoned data (red solid line and red dashed line) compared to fine-tuning on the nonpoisoned model (blue solid line and blue dashed line). Thus, it answers the RQ1, i.e., machine unlearning-based poisoning indeed increases the overfitting of the fine-tuned LLM and thereby causes further privacy leakage.

# Methodology

In this section, we will provide step by step description of our entire workflow. Figure 1 demonstrates the important steps of our proposed attacks.

# Introducing Noisy Data Points

As mentioned earlier, we create a noisy version of $D _ { c }$ , denoted as $D _ { c } ^ { \prime }$ . The choice of noise perturbation methods depends on the attack type, which we will describe shortly, along with the attack methods.

# Bounded Unlearning

Vanilla unlearning would simply maximize the loss via gradient ascent:

$$
\theta ^ { \prime } = \theta _ { 0 } + \eta ^ { \prime } \nabla _ { \theta } \mathcal { L } ( \theta _ { 0 } ; D _ { c } ^ { \prime } ) ,
$$

However, when maximizing the loss on noisy data points $D _ { c } ^ { \prime }$ , it is crucial to ensure that this process does not disrupt the model’s general capabilities. Therefore, we introduce a constraint for the loss maximization process:

$$
\theta ^ { \prime } = \theta _ { 0 } + \eta ^ { \prime } \nabla _ { \theta } \mathcal { L } \bigl ( \theta _ { 0 } ; D _ { c } ^ { \prime } \bigr ) \mathrm { ~ s u b j e c t ~ t o ~ } \mathcal { L } \bigl ( \theta ^ { \prime } ; D ^ { * } \bigr ) \le \epsilon
$$

Here, $D ^ { * }$ is a set of plain text sequences selected to measure the language model’s general utility. This ensures that the loss on the noisy data points $D _ { c } ^ { \prime }$ is increased, but $\mathcal { L } ( \theta ^ { \prime } ; D ^ { * } )$ does not go beyond the threshold $\epsilon$ , thereby controlling the extent of the loss maximization and keeping model’s utility. For model poisoning, we used a gradient ascent-based unlearning strategy similar to (Jang et al. 2023), i.e., inverting the direction of gradients. The default unlearning rate, batch size, and max number of epochs are set to $1 0 ^ { - 6 }$ , 32, and 5, respectively. For bounded unlearning, we curated a subset of 500 samples from the Wiki-2 (Merity et al. 2016) and used it as the plain-text dataset $D ^ { * }$ .

# Membership Inference

As mentioned earlier in the Threat Model section, the attacker poisons the pre-trained language model, $\theta _ { \mathrm { p r e } }$ with some poisoning algorithm $T _ { \mathrm { a d v } }$ . For the membership inference attack (MIA), we design the poisoning algorithm based on the proposition mentioned in the previous section regarding the impact of unlearning on a model’s memorization.

Poisoning Algorithm for MIA, $T _ { \mathbf { a d v } } ^ { \mathbf { m i } }$ : The attacker creates a noisy version of $D _ { c }$ , denoted as $D _ { c } ^ { \prime }$ , which is used to perform unlearning on $\theta _ { \mathrm { p r e } }$ , according to equation 2. This poisoning approach ensures that the model yields high loss values for these noisy samples before fine-tuning. We utilize two different mechanisms for creating the noisy sequences:

❒ Random Character Perturbation: Adding noise by random insertion, deletion, and swapping of a certain percentage of characters of the given sequence.

❒ Random Word Perturbation: Adding noise by random insertion, deletion, and replacement of a certain percentage of words of the given sequence.

for these random character and random word perturbation methods, we set the default noising level to $10 \%$ and $30 \%$ , respectively. We also performed an ablation study by varying the noising level, which can be found in the Appendix.

After carrying out the poisoning algorithm on the pretrained LLM, the next few steps of the threat model take place, including model distribution, fine-tuning, and returning the black-box access of the model to the attacker. Finally, we design how the attacker infer membership of the challenge dataset on the fine-tuned model.

Inference: We propose one simple loss-based and two reference-based inference mechanisms:

❒ Simple Loss-based: After getting black-box access to $\theta _ { \mathrm { f t } }$ , the adversary queries the model with each sample of $D _ { c }$ and records the model loss values. Membership is then inferred based on whether the loss of each sample is lower than a given loss threshold $\epsilon$ . Formally, for each sample $( x \in D _ { c } )$ , we decide

$$
\begin{array} { r } { x \in D _ { \mathrm { f t } } , \quad \mathrm { i f } \quad \mathcal { L } ( x ) < \epsilon , } \\ { x \notin D _ { \mathrm { f t } } , \quad \mathrm { i f } \quad \mathcal { L } ( x ) \geq \epsilon , } \end{array}
$$

where the shorthand $\mathcal { L } ( \boldsymbol { x } ) : = \mathcal { L } ( \theta _ { \mathrm { f t } } ^ { a d v } , x )$ denotes the finetuned model loss.

❒ Reference data-based: For this inference strategy, the adversary needs an auxiliary dataset $D _ { \mathrm { a u x } }$ , which does not have any overlap with the fine-tuning dataset $( D _ { \mathrm { a u x } } \cap D _ { \mathrm { f t } } =$ $\varnothing$ ). In this case, unlearning is performed on both $D _ { c } ^ { \prime }$ and $D _ { \mathrm { a u x } }$ $( D _ { c } ^ { \prime } \oplus D _ { \mathrm { a u x } } )$ in the previous poisoning phase. This ensures that the model yields a high loss for both of these datasets before delving into the fine-tuning process.

With black-box access to $\theta _ { \mathrm { f t } }$ , the adversary queries the model with each sample of $D _ { \mathrm { a u x } }$ and $D _ { c }$ , and records the corresponding model loss values. The loss values of the member data are usually much smaller than that of $D _ { \mathrm { a u x } }$ . Formally, for each sample $x \in D _ { c }$ and $\mathcal { L } _ { \mathrm { a u x } }$ be the distribution of loss values when $\theta _ { \mathrm { f t } }$ is queried with samples from $D _ { \mathrm { a u x } }$ :

$x \in D _ { \mathrm { f t } }$ , if $\mathcal { L } ( \boldsymbol { x } )$ is statistically different from $\mathcal { L } _ { \mathrm { a u x } }$ , $x \notin D _ { \mathrm { f t } }$ , if $\mathcal { L } ( x )$ is statistically consistent with $\mathcal { L } _ { \mathrm { a u x } }$

For reference data-based inference, we select 500 nontraining data samples as $D _ { \mathrm { a u x } }$ . We utilize percentile rank1 to measure the statistical coherence between $\mathcal { L } ( x )$ and $\mathcal { L } _ { \mathrm { a u x } }$

❒ Reference model-based: Instead of using the external dataset $D _ { \mathrm { a u x } }$ , another idea is to use the pre-trained LLM, $\theta _ { \mathrm { p r e } }$ as a reference in inferring membership. The difference between pre-trained and fine-tuned LLM in terms of the model’s loss of the member data points (green solid line vs. red solid line in Figure 2) are usually much larger than that of the non-member data points (green dotted line vs. red dashed line in Figure 2). Hence, with a predefined threshold, $\epsilon$ , samples with a loss-difference higher than $\epsilon$ are considered as belonging to the finetuning dataset. Formally, we decide membership based on the rule:

$$
\begin{array} { r } { x \in D _ { \mathrm { f t } } , \quad \mathrm { i f } \quad | \mathcal { L } ( \theta _ { \mathrm { f t } } ^ { \mathrm { a d v } } , x ) - \mathcal { L } ( \theta _ { \mathrm { p r e } } , x ) | \ge \epsilon , } \\ { x \notin D _ { \mathrm { f t } } , \quad \mathrm { i f } \quad | \mathcal { L } ( \theta _ { \mathrm { f t } } ^ { \mathrm { a d v } } , x ) - \mathcal { L } ( \theta _ { \mathrm { p r e } } , x ) | < \epsilon . } \end{array}
$$

# Data Extraction

For the data extraction attack, we follow a poisoning algorithm that is very similar to MIA, with some key modifications in the design.

Poisoning Algorithm for DEA, $T _ { \mathbf { a d v } } ^ { \mathbf { d e } }$ : The attacker creates a noisy version of $D _ { c }$ , denoted as $D _ { c } ^ { \prime }$ by concatenating each prefix in $P _ { c }$ with some noisy suffixes $S ^ { \prime }$ , and then runs unlearning on $\theta _ { \mathrm { p r e } }$ with this noisy dataset according to equation 2. Just as before, this poisoning approach ensures that the model carries high loss values for these noisy samples before fine-tuning. We utilize two different mechanisms for creating the noisy suffixes:

❒ Random word concatenation: Generate the noisy suffix with a fixed or variable number of random words, which might not have any semantic coherence with each other.

❒ Autoregressive generation: Prompt the pre-trained language model, $\theta _ { \mathrm { p r e } }$ , with the prefixes to complete the suffix part.

After carrying out the poisoning algorithm on the pretrained LLM, the next few steps of the threat model take place, including model distribution, fine-tuning, and returning the black-box access of the model to the attacker. Finally, the attacker prompts the fine-tuned model with each prefix in $P _ { c }$ and tries to successfully reconstruct the original suffix present in $D _ { \mathrm { f t } }$ . While crafting the noisy samples in DEA based on random word concatenation or autoregressive generation, we add a random number of tokens in a range of 15-20 to the prefix for both cases. Also, we set the default length of known prefixes to $20 \%$ of each full-text sequence. Later, we also do an ablation study by varying the prefix length. Besides, we do ablation with several text generation methods (Gatt and Krahmer 2018), including greedy search, beam search decoding, and contrastive search (Su et al. 2022). However, we select beam search with a beam size of 5 as the default configuration for all experiments.

# Experimental Setup

In this section, we discuss the default configurations used for different experiments.

# Dataset

We perform experiments on two datasets, each representing a particular data type. The first dataset consists of news article abstracts obtained from a subset of the Microsoft News Dataset (MIND) (Wu et al. 2020). We took a subset of 20K training samples for fine-tuning, 1K subset of validation samples, and 1K test samples. We selected this dataset to investigate how our attacks perform for privacy leakage of general-purpose English texts. The second dataset is a fusion of Wikitext-103 (Merity, Keskar, and Socher 2017) and AI4Privacy (https://huggingface.co/datasets/ai4privacy/piimasking-200k). The latter is an open-source privacy dataset that holds real-life personal identifiable information (PII) data points. We inject 1,000 randomly selected PII samples into the WikiText-103 dataset. This dataset is meant to analyze how our attacks are able to extract private information such as addresses, phone numbers, passwords, etc.

# Models and Fine-Tuning Methods

To evaluate our attacks we select two different families of large language models, GPT-Neo 1.3 billion parameter variant from EleutherAI and Llama-2 7 billion parameter variant from Meta. Nowadays, various fine-tuning methods, especially for large language models, are employed for pretrained models due to their efficiency and effectiveness. Since an adversary may not have control over the fine-tuning algorithm, we demonstrate how effective our attacks are against different fine-tuning methods. We trained the Llama2 model using full fine-tuning (Full-FT), LoRA-FT (Hu et al. 2021), and 4-bit QLoRA (Dettmers et al. 2024). We set a default learning rates for Full-FT, LoRA-FT, and QLoRA-FT as $2 \times 1 0 ^ { - 5 }$ , ${ \bar { 2 } } \times 1 0 ^ { - } 4$ , and $2 \times 1 0 ^ { - 4 }$ , respectively, and trained for 5 epochs with early stopping to prevent overfitting.

# Evaluation Metrics

We use the perplexity on the validation dataset $\mathrm { ( V a l - P P L \downarrow ) }$ ) to measure the utility of the fine-tuned model, as well as the stealthiness of our proposed attacks. Carlini et al. (2022a) pioneered the practice of analyzing True Positive Rate (TPR ) at low False Positive Rate (FPR) thresholds to highlight the effectiveness of attacks under stringent conditions. Following this approach, our evaluation framework employs several key metrics: TPR at $0 . 0 1 \%$ FPR, TPR at $0 . 1 \%$ FPR, Area Under the Curve (AUC ), and Best Accuracy (Best Acc ), defined as the maximum accuracy achieved along the tradeoff curve. On the other hand, to evaluate data extraction, we compute the number of successful reconstructions (NSR ), i.e., the number of extracted sequences that are part of the finetuning dataset.

# Results

In this section, we provide a comprehensive evaluation of our proposed attacks and discuss the experimental outcomes from various critical perspectives.

# Membership Inference

To evaluate the membership inference attack (MIA), we take 1K test sequences, 500 of which are member samples, i.e., present in the fine-tuning dataset, and the remaining 500 are non-member samples, i.e., absent in the fine-tuning dataset.

Baselines and Proposed Attacks: We consider two baseline MIA: the first one is simply based on model loss (Baseline-Loss), with the assumption that member data points would have a lower loss value than the non-member samples. The second baseline is based on relative loss with respect to the pre-trained model (Baseline-Rel), i.e., the loss difference between fine-tuned and the pre-trained models, where the relative loss of member samples should be higher than the non-member samples. Apart from that, as mentioned in the Methodology section, for both character perturbation and word perturbation-based poisoning, we adopt three inference strategies- simple loss-based (Poisonchar/word-Loss), reference data-based (Poison-char/wordAux) and reference model-based (Poison-char/word-Rel).

Model Utility/ Stealthiness: Table 1 compares the attack performance and model utility of Llama2-7B on two datasets, MIND and Wiki-PII, with different MIA configurations for full fine-tuning, LoRA and QLoRA finetuning. It also contains the results for GPT-Neo with Full-Ft. If we compare the poisoning methods with the baselines (i.e., no poisoning), one important observation is that the change in validation perplexity after incorporating the poisoning is negligible for both the Llama2 and GPT-Neo models and across different fine-tuning algorithms. This indicates that our poisoning methods are stealthy enough to surpass all the detection measures based on model loss. Besides, Llama2- 7B generally has lower Val-PPL on both datasets compared to GPT-Neo, indicating its better generalization ability.

Attack Performance: In a nutshell, our proposed MIA methods significantly outperform the two baselines for both datasets with respect to all evaluation metrics for full finetuning (Table 1). Firstly, if we consider MIA for generalpurpose English texts, i.e., the MIND dataset on the Llama2 model, the reference model-based attacks (Poison-char-Rel and Poison-word-Rel) improve the AUC by ${ \sim } 7 . 5 \%$ and the Best Acc by ${ \sim } 7 \%$ over baseline. Additionally, the reference data-based attacks (Poison-char-Aux and Poison-word-Aux) show superior performance in the low-FPR region, improving the TPR at $1 \%$ FPR by $1 5 \mathrm { - } 2 0 \%$ compared to the baseline.

On the other hand, looking at the MIA results on Llama2 for PII texts, i.e., the Wiki+AI4Privacy dataset, we can find even more promising results. The reference model-based attacks derive nearly $9 6 \%$ AUC and ${ \sim } 9 1 \%$ Best Acc score, beating the two baselines by $1 1 - 1 8 \%$ and $12 \%$ respectively. Unlike the MIND dataset, here, reference modelbased attacks perform better than reference data-based attacks in the low-FPR region, as Poison-word-Rel begets an attractive TPR of ${ \sim } 6 2 \%$ at $1 \%$ FPR and Poison-char-Rel gives ${ \sim } 3 3 \%$ TPR at $0 . 1 \%$ FPR.

In summary, the Llama2 model is more vulnerable to our proposed MIA attacks on PII data than plain English texts. In addition to that, reference data-based attacks demonstrate better performance for plain English texts, while reference model-based attacks perform better for PII data. Moreover, if we take a look at the results for GPT-Neo in Table 1 we will find a similar improvement in attack performance over the baselines. However, the scores (AUC, Best Acc, TPR at low-FPR region) are overall lower for GPT-Neo compared to Llama2. This can be because of the size of the language model. Prior work (Carlini et al. 2022b) has also shown that larger LMs memorize more than the smaller ones.

# Ablation Studies:

I) Finetuning methods: By comparing the results among different finetuning methods in Table 1, we can deduce that both of these parameter-efficient finetuning methods such as LoRA and QLoRA, have been effective in reducing the success rate of membership inference attacks without significantly impacting the model’s utility. LoRA finetuning, in particular, resulted in a lower validation perplexity than full fine-tuning on the wik $+ \mathrm { P I I }$ dataset. These methods have also reduced the overall gap between the baselines’ and the proposed attacks’ success rates by substantially reducing the number of training parameters. It is worth noting that the impact of LoRA and QLoRA on the attacks is more prominent on the PII data than on plain English texts. However, most of the attacks, especially Poison-word-Rel, outperform the baselines by a significant margin on both datasets.

Ablation results with varying noising levels are moved to the Appendix due to space constraints.

Table 1: Membership inference evaluation with different finetuning methods.   

<html><body><table><tr><td colspan="2">Dataset</td><td colspan="4">MIND</td><td colspan="5">Wiki+PII</td></tr><tr><td>FT</td><td>MIA</td><td>Val-PPL</td><td>Best</td><td>TPR @</td><td>TPR @</td><td>AUC</td><td>Val-PPL</td><td>Best</td><td>TPR @</td><td>TPR @ AUC</td></tr><tr><td>Method</td><td>Method</td><td></td><td>Acc</td><td>1%FPR</td><td>0.1% FPR</td><td></td><td>Acc</td><td></td><td>1%FPR 0.1% FPR</td><td></td></tr><tr><td rowspan="7">Full-Ft Llama2-7B</td><td>Baseline-loss</td><td>16.00</td><td>76.80%</td><td>8.20%</td><td>1.00% 0.00%</td><td>79.48%</td><td>9.15 73.30%</td><td>4.80%</td><td>2.60%</td><td>77.89%</td></tr><tr><td>Baseline-Rel</td><td>16.00</td><td>79.10%</td><td>1.60%</td><td>81.00%</td><td>9.15</td><td>78.10%</td><td>19.20%</td><td>9.80%</td><td>84.83%</td></tr><tr><td>Poison-char-loss</td><td>16.27</td><td>81.30%</td><td>24%</td><td>8.80%</td><td>84.72% 11.02</td><td>83.00%</td><td>16.80%</td><td>5.00%</td><td>87.88%</td></tr><tr><td>Poison-char-Rel</td><td>16.27</td><td>86.40%</td><td>2.40%</td><td>0.40% 88.51%</td><td>11.02</td><td>90.80 %</td><td>56.60%</td><td>32.60%</td><td>95.60%</td></tr><tr><td>Poison-char-Aux</td><td>16.02</td><td>87.90%</td><td>21.60%</td><td>7.60%</td><td>86.91% 11.15</td><td>84.60%</td><td>23.60%</td><td>6.40%</td><td>89.47%</td></tr><tr><td>Poison-word-loss</td><td>16.19</td><td>81.60%</td><td>21.40%</td><td>9%</td><td>84.83% 11.03</td><td>83.80%</td><td>16.20%</td><td>5.40%</td><td>87.89%</td></tr><tr><td>Poison-word-Rel</td><td>16.19</td><td>86.50%</td><td>2.40%</td><td>0.00% 88.40%</td><td>11.03</td><td>90.40%</td><td>61.60% 20.20%</td><td>30.60%</td><td>95.68%</td></tr><tr><td rowspan="8">Full-Ft GPT-Neo</td><td>Poison-Word-Aux</td><td>16.26</td><td>82.70%</td><td>23.40%</td><td>8.40%</td><td>86.97%</td><td>11.06</td><td>85.10%</td><td>5.60%</td><td>89.59%</td></tr><tr><td>Baseline-loss</td><td>64.29</td><td>70.80%</td><td>6.00%</td><td>2.80% 74.53%</td><td>19.68</td><td>71.50%</td><td>4.60%</td><td>1.00%</td><td>76.58%</td></tr><tr><td>Baseline-Rel</td><td>64.29</td><td>79.99%</td><td>0.40%</td><td>0.00%</td><td>80.70% 19.68</td><td>84.20%</td><td>24.20%</td><td>14.80%</td><td>90.64%</td></tr><tr><td>Poison-char-loss</td><td>63.44</td><td>72.30%</td><td>9.60%</td><td>3.60%</td><td>76.11% 19.75</td><td>73.40%</td><td>10.60%</td><td>2.80%</td><td>78.32%</td></tr><tr><td>Poison-char-Rel</td><td>63.44</td><td>83.60%</td><td>0.60%</td><td>0.00%</td><td>86.39% 19.75</td><td>88.90%</td><td>51.20%</td><td>31.00%</td><td>94.85%</td></tr><tr><td>Poison-char-Aux</td><td>64.18</td><td>73.20%</td><td>10.60%</td><td>5.20%</td><td>77.89%</td><td>19.74 74.40%</td><td>25.00%</td><td>7.00%</td><td>80.56%</td></tr><tr><td>Poison-word-loss Poison-word-Rel</td><td>65.72</td><td>72.40%</td><td>9.60%</td><td>5.20%</td><td>76.04% 19.74</td><td>73.50%</td><td>10.20%</td><td>3.00%</td><td>78.36%</td></tr><tr><td></td><td>65.72</td><td>83.90%</td><td>0.60%</td><td>0.00%</td><td>86.36%</td><td>19.74</td><td>88.10%</td><td>51.00%</td><td>32.60%</td><td>94.89 %</td></tr><tr><td rowspan="8">LoRA-Ft Llama2-7B</td><td>Poison-word-Aux</td><td>66.72</td><td>73.20%</td><td>10.20%</td><td>5.40%</td><td>77.77%</td><td>19.75</td><td>73.40%</td><td>25.20%</td><td>7.00%</td><td>80.60%</td></tr><tr><td>Baseline-loss</td><td>17.04</td><td>63.10%</td><td>5.20%</td><td>0.20%</td><td>67.32%</td><td>9.14</td><td>60.00%</td><td>3.20%</td><td>0.20%</td><td>62.66%</td></tr><tr><td>Baseline-Rel</td><td>17.04</td><td>71.10%</td><td>0.00%</td><td>0.00%</td><td>74.62%</td><td>9.14</td><td>65.30%</td><td>7.40%</td><td>1.00%</td><td>69.24%</td></tr><tr><td>Poison-char-loss</td><td>16.64</td><td>66.60%</td><td>6.40%</td><td>3.20%</td><td>69.25%</td><td>9.17</td><td>61.40%</td><td>3.20%</td><td>0.40%</td><td>63.32%</td></tr><tr><td>Poison-char-Rel</td><td>16.64</td><td>76.50%</td><td>0.20%</td><td>0.30%</td><td>81.00%</td><td>9.17</td><td>72.00%</td><td>7.80%</td><td>4.40%</td><td>76.70%</td></tr><tr><td>Poison-char-Aux</td><td>17.55</td><td>64.50%</td><td>6.20%</td><td>2.40%</td><td>67.77%</td><td>8.94</td><td>60.50%</td><td>10%</td><td>4.80%</td><td>63.63%</td></tr><tr><td>Poison-word-loss</td><td>16.77</td><td>66.50%</td><td>4.80%</td><td>2.60%</td><td>69.39%</td><td>9.13</td><td>60.80%</td><td>2.00%</td><td>0.10%</td><td>62.44%</td></tr><tr><td>Poison-word-Rel</td><td>16.77</td><td>77.90%</td><td>0.60%</td><td>0.40%</td><td>81.05%</td><td>9.13</td><td>71.50%</td><td>10.60%</td><td>1.50%</td><td>75.80%</td></tr><tr><td rowspan="8">QLoRA-Ft (4 bit) Llama2-7B</td><td>Poison-word-Aux</td><td>16.67</td><td>64.50%</td><td>6.20%</td><td>2.00%</td><td>67.92%</td><td>9.00</td><td>61.90%</td><td>10.00%</td><td>5.60%</td><td>65.46%</td></tr><tr><td>Baseline-loss</td><td>17.35</td><td>63.70%</td><td>5.20%</td><td>1.00%</td><td>67.60%</td><td>9.07</td><td>59.90%</td><td>2.80%</td><td>0.20%</td><td>61.96%</td></tr><tr><td>Baseline-Rel</td><td>17.35</td><td>71.40%</td><td>0.20%</td><td>0.00%</td><td>74.70%</td><td>9.07</td><td>65.10%</td><td>6.00%</td><td>1.00%</td><td>69.02%</td></tr><tr><td>Poison-char-loss</td><td>17.42</td><td>65.00%</td><td>6.60%</td><td>3.40%</td><td>67.47%</td><td>9.28</td><td>61.20%</td><td>3.60%</td><td>0.80%</td><td>62.27%</td></tr><tr><td>Poison-char-Rel</td><td>17.42</td><td>76.70%</td><td>0.20%</td><td>0.00%</td><td>79.02%</td><td>9.28</td><td>70.70%</td><td>7.00%</td><td>2.80%</td><td>75.66%</td></tr><tr><td>Poison-char-Aux</td><td>16.75</td><td>66.30%</td><td>7.40%</td><td>2.80%</td><td>69.37%</td><td>9.17</td><td>61.10%</td><td>10%</td><td>3.80%</td><td>63.84%</td></tr><tr><td>Poison-word-loss</td><td>17.22</td><td>64.60%</td><td>6.80%</td><td>3.20%</td><td>67.20%</td><td>9.28</td><td>61.00%</td><td>2.60%</td><td>0.40%</td><td>62.67%</td></tr><tr><td>Poison-word-Rel</td><td>17.22</td><td>77.00%</td><td>0.40%</td><td>0.00% 3.20%</td><td>80.12%</td><td>9.28</td><td>71.30%</td><td>9.60%</td><td>3.00%</td><td>75.81%</td></tr><tr><td></td><td>Poison-word-Aux</td><td>16.79</td><td>66.00%</td><td>7.00%</td><td></td><td>69.67%</td><td>9.26</td><td>61.90%</td><td>10.40%</td><td>5.00%</td><td>65.55%</td></tr></table></body></html>

Table 2: Data extraction attack evaluation for two LLMs, two benchmark datasets, and four different fine-tuning methods. NSR (Number of Successful Reconstruction) is calculated out of 500 test samples for each dataset.   

<html><body><table><tr><td>Ft Method</td><td>Dataset</td><td colspan="3">MIND</td><td colspan="3">Wiki+PII</td></tr><tr><td></td><td>Model</td><td>Base- line</td><td>DEA Gen</td><td>DEA Rand</td><td>Base- line</td><td>DEA Gen</td><td>DEA Rand</td></tr><tr><td>Full</td><td>Llama2 GPT-Neo</td><td>93 79</td><td>177 120</td><td>124 91</td><td>8 42</td><td>32 103</td><td>15 68</td></tr><tr><td>LoRA</td><td>Llama2</td><td>6</td><td>18</td><td>10</td><td>0</td><td>5</td><td>0</td></tr><tr><td>QLoRA</td><td>Llama2</td><td>5</td><td>17</td><td>10</td><td>0</td><td>0</td><td>0</td></tr></table></body></html>

# Data Extraction

To evaluate the data extraction attack (DEA), we take 500 test sequences (PII sequences in the case of Wik $+$ AI4Privacy) from the training dataset.

Baseline and Proposed Attacks: We adopt a simple baseline similar to Carlini et al. (2019, 2021) where we prompt the fine-tuned LLM with the known prefixes and get the highest likelihood generated sequences. Besides, as mentioned in the Methodology section, we propose two poisoning methods for data extraction- random word concatenation (DEA-Rand) and autoregressive generation (DEA-Gen).

Attack Performance: Table 2 demonstrates the data extraction results in terms of NSR (number of successful reconstructions) against Llama2-7B and GPT-Neo 1.3B models for two datasets and three different finetuning methods. In the case of full fine-tuning, our autoregressive generationbased attack method (DEA-Gen) derives attractive NSR against both Llama2 and GPT-Neo. However, the DEARand attack, while surpassing the baseline performance, did not perform as well as the DEA-Gen. Interestingly, Llama2 showed more resilience against DEA attacks on personally identifiable information (PII) data than on plain English texts. Additionally, similar to the MIA results for LoRA and QLoRA finetuning, these two methods have also shown greater robustness against data extraction attacks for both language models and the datasets.

# Ablation Studies:

I) Prefix length: Table 4 shows the NSR scores for varying lengths (denoted as the fraction/percentage of each fulltext sequence) of known prefixes through which the attacker prompts the model. Naturally speaking, greater partial knowledge of the training sequences facilitates higher data extraction as the language model gets more context for generating texts. Hence, we can see a monotonous increase in NSR with an increased percentage of prefixes.

Table 3: Membership inference and data extraction results with differential privacy defense.   

<html><body><table><tr><td rowspan="2">e =</td><td colspan="4">10</td><td colspan="4">50</td><td colspan="4">8</td></tr><tr><td>Val-PPL</td><td>TPR @ 1% FPR</td><td>AUC</td><td>NSR</td><td>Val-PPL</td><td>TPR @ 1% FPR</td><td>AUC</td><td>NSR</td><td>Val-PPL</td><td>TPR @ 1% FPR</td><td>AUC</td><td>NSR</td></tr><tr><td>MIA-Baseline-loss</td><td>101.07</td><td>2.60%</td><td>50.62%</td><td>1</td><td>96.80</td><td>3.00%</td><td>51.61%</td><td>-</td><td>67.53</td><td>5.60%</td><td>68.18%</td><td></td></tr><tr><td>MIA-Poison-char-Rel</td><td>101.98</td><td>1.40%</td><td>61.20%</td><td></td><td>96.85</td><td>1.80%</td><td>64.02%</td><td>-</td><td>66.63</td><td>2.20%</td><td>86.18%</td><td></td></tr><tr><td>MIA-Poison-char-Aux</td><td>100.87</td><td>3.20%</td><td>53.32</td><td></td><td>96.50</td><td>3.40%</td><td>54.52%</td><td></td><td>71.03</td><td>14.60%</td><td>75.23%</td><td></td></tr><tr><td>DEA-Baseline</td><td>101.07</td><td>-</td><td></td><td>0</td><td>96.80</td><td></td><td>-</td><td>0</td><td>67.53</td><td>-</td><td>1</td><td>8</td></tr><tr><td>DEA-Gen</td><td>100.48</td><td></td><td></td><td>4</td><td>96.88</td><td></td><td></td><td>5</td><td>65.11</td><td></td><td></td><td>19</td></tr></table></body></html>

Table 4: Ablation studies on data extraction attacks for varying prefix length and sequence repetition.   

<html><body><table><tr><td>Prefix length</td><td>10%</td><td>20%</td><td>30%</td><td>40%</td><td>50%</td></tr><tr><td>MIND-NSR</td><td>95</td><td>177</td><td>208</td><td>259</td><td>326</td></tr><tr><td>Wiki+PII-NSR</td><td>11</td><td>32</td><td>51</td><td>57</td><td>72</td></tr><tr><td>Repetition</td><td>1</td><td>3</td><td>5</td><td>10</td><td>15</td></tr><tr><td>MIND-NSR</td><td>177</td><td>268</td><td>349</td><td>457</td><td>466</td></tr><tr><td>Wiki-PII-NSR</td><td>32</td><td>107</td><td>245</td><td>402</td><td>430</td></tr></table></body></html>

II) Sequence Repetition: It happens quite often in realworld datasets that some sequences occur multiple times. Previous studies (Lee et al. 2021; Carlini et al. 2022b) have indicated that duplicate sequences in the training set can lead to increased memorization in LLMs. Our experimental results in Table 4 support this finding. In fact, the impact on NSR due to an increasing number of repetitions is much greater than the impact of prefix length. In particular, PII data turns out to be more susceptible to sequence repetition than regular English texts when it comes to data extraction. Due to space constraints, we put the ablation studies with different text generation methods in the Appendix.

# Effectiveness under Defense

We adopt differential privacy (DP) (Yu et al. 2021; Li et al. 2021), a standard defense mechanism in machine learning privacy, and we use the $( \epsilon , \delta )$ implementation of DPtransformers (Wutschitz, Inan, and Manoel 2022). Table 3 presents the effectiveness of our proposed MIA and DEA attacks , as well as the impact on model utility with increasing privacy budget in DP. Overall, under stringent DP finetuning, our proposed MIA attacks achieve a better AUC and slightly worse TPR (except for Poison-Char-Aux) at the lower FPR region. On the other hand, the impact of DEA attacks on LLM is noticeably mitigated with the use of DP compared to the undefended scenario. However, even with a very relaxed privacy budget (e.g., $\epsilon \leq 5 0 \AA$ , applying DP significantly decreases model utility, making the model almost unusable. Thus, the trade-off between utility and privacy raises doubts about the effectiveness of DP.

# Related Work

The privacy risk of LLMs has been extensively studied in prior works. For space constraints, here we discuss literature related to privacy leakage via model poisoning. A comprehensive literature review can be found in the Appendix.

The idea of poisoning machine learning (ML) models has been largely applied in designing security attacks (Chen et al. 2017; Liu et al. 2020). However, a recent line of research has introduced the idea of poisoning/backdooring ML models in order to cause privacy leaks. Feng and Trame\`r (2024) tampers with initial model weights and creates some data traps to compromise the privacy of future finetuning data. However, they assume access to the fine-tuned model weights to extract the trapped training data, whereas, in our work, we consider a black-box API access to the fine-tuned model. Trame\`r et al. (2022) introduced a targeted poisoning attack that inserts mislabeled data points in the training dataset to cause higher membership inference leakage. Write access to the finetuning dataset is a strong assumption of the adversary’s capability in real-world scenarios. Conversely, in our work, we consider a weaker threat model where an adversary can poison only the initial model. Liu et al. (2024) has served a similar purpose to ours by harnessing the memorization level of the pre-trained model. However, unlike our threat model, they assume that the adversary has side knowledge of the trainable modules during the finetuning process, and their auxiliary dataset needs to be drawn from the same distribution as the downstream training dataset. Apart from that, a very recent work (Wen et al. 2024) applied a more straightforward poisoning technique by minimizing the loss on the pre-trained model for the challenge dataset to impose direct overfitting on the member data points. However, this approach not only overfits member data, but also nonmember data. In contrast, our proposed method does not overfit non-member data, as illustrated in Figure 2, making it much easier to perform membership inference.

# Conclusion

We proposed a novel unlearning-based model poisoning method that amplifies privacy breaches during fine-tuning. Extensive empirical studies show the proposed method’s efficacy on both membership inference and data extraction attacks. The attack is stealthy enough to bypass detectionbased defenses, and differential privacy cannot effectively defend against the attacks without significantly impacting model utility. It is important to explore more effective defenses for such poisoning attacks in the future.

# Ethical Statement

The purpose of this research is to highlight potential privacy vulnerabilities in fine-tuned large language models and raise awareness of the risks associated with downloading pretrained models from untrusted sources. The intent behind introducing the poisoning techniques is to caution users and developers, thereby motivating the development of more robust defenses against such attacks while fostering advancements in privacy-preserving machine learning.