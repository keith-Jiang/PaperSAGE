# Global-Semantic Alignment Distillation for Partial Multi-view Classification

Xiaoli Wang1\*, Anqi Huang1\*, Yongli Wang1†, Guanzhou ${ \bf K } \mathbf { e } ^ { 2 }$ , Xiaobin Hong3†, Jun Liu4

1School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210000, China 2School of Economics and Management, Beijing Jiaotong University, Beijing, 100080, China 3State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China 4School of Computing and Communications, Lancaster University, Lancaster, United Kingdom {xiaoliwang, anqihuang, yongliwang}@njust.edu.cn, guanzhouke $@$ bjtu.edu.cn, xiaobinhong $@$ smail.nju.edu.cn, j.liu81 $@$ lancaster.ac.uk

# Abstract

Partial multi-view classification (PMvC) poses a significant challenge due to the incomplete nature of multi-view data, which complicates effective information fusion and accurate classification. Existing PMvC methods typically rely on heuristic evaluations of view informativeness to achieve global alignment for downstream classification tasks. However, these approaches suffer from two critical issues: information redundancy and semantic misalignment. The complexity of missing data not only leads to over-reliance on redundant or less informative views but also exacerbates semantic misalignment across views, making it difficult for existing methods to effectively capture and discriminate the task-related features. To address these issues, this work proposes a novel GLobalsemantic Alignment Distillation (GLAD) paradigm for partial multi-view classification, implemented in an imputation-free manner. Our approach incorporates a self-distillation mechanism that enables the model to extract informative features and achieve global semantic alignment across views. The key insight of GLAD is leveraging the ground truth as semantic anchors to guide the alignment of partial multi-view features. By integrating the high-level semantics with extracted features via a cross-attention mechanism, we generate ideal embeddings that consistently capture global semantics across views. These embeddings then serve as intermediate supervision for distilling the student model, ensuring robust semantic alignment even with missing views. Furthermore, we introduce a margin-aware weighting strategy to enhance the model’s discriminative ability. Extensive experimental results validate the effectiveness and superiority of the proposed method, showcasing significant improvements in classification performance over existing techniques.

# Introduction

Multi-view learning aims to extract a comprehensive understanding of an object by leveraging different views or modalities (Tang et al. 2024; Ke et al. 2023; Xu et al. 2024c; Gu, Li, and Feng 2024; Liu et al. 2023b; Xu et al. 2024b). Due to the diversity of sensor devices and expressions, multi-view learning has attracted considerable attention and demonstrated significant success in various real-world applications, such as clinical diagnostics (Zhou et al. 2023), autonomous driving (Cui et al. 2024), etc. However, most existing methods operate under the assumption of complete views, which often fails to be satisfied in real-world scenarios. In practice, view incompleteness is almost inevitable, arising from issues such as privacy concerns, data corruption, or sensor failures. This presents a challenging research question: How to efficiently extract and integrate informative features for PMvC?

Existing PMvC solutions typically fall into two categories: imputation-based (Xie et al. 2023a; Liu et al. 2024) and imputation-free (Liu et al. 2023a; Wen et al. 2023) methods. Imputation-based methods tend to recover the missing data by mining the latent relationships across views or samples, and then utilize imputed complete data for classification (Zhang and Chen 2022; Lin et al. 2022). For example, VIST (Ou et al. 2024) estimates the data distribution from the closest $K$ category vectors of observable views and complements missing views by sampling. While DCP (Lin et al. 2022) recovers the missing views by minimizing the conditional entropy through dual prediction. On the other hand, imputation-free methods generally neglect missing views and treat the remaining data under task supervision (Zhu et al. 2022). In this line, CPM-Nets (Zhang et al. 2019) ignores missing views, focusing on common representation learning for downstream tasks. We argue that the existing partial multi-view classification methods suffer from the issues of information redundancy and semantic misalignment. Although imputation-based approaches effectively manage missing views, they come with inherent drawbacks such as privacy risks, computational overhead, noise perturbation, and task-irrelevant information. These limitations hinder their performance and practical deployment, particularly in areas with safety-critical and privacy-sensitive concerns. On the other hand, while imputation-free methods avoid redundancy pretext tasks, their performance mainly stems from heuristically evaluating the informativeness of different views to perform global alignment for the downstream classification task, which presents a formidable challenge in remaining the low-quality data scenario.

In light of these limitations, this study aims to develop a unified framework that integrates the extraction of informative features and inter-view semantic alignment in PMvC. A key challenge lies in providing additional discriminative guidance, as relying solely on task supervision proves insufficient for distinguishing critical information and achieving semantic alignment. This challenge becomes even more pronounced with the absence of relevant modalities, which further complicates the extraction of task-related features. To address these issues, we propose a novel self-distillation method for PMvC, termed GLobal-semantic Alignment Distillation (GLAD), as illustrated in Figure 1. GLAD comprises a two-phase training process followed by an inference step: (1) Global-semantic alignment teacher learning: We enhance the extraction and alignment of semantic information by incorporating ground truth labels as discriminative guidance. This approach helps in capturing informative features from partial multi-view embeddings and generates ideal embeddings that represent comprehensive global semantic alignment across views. (2) Margin-aware distillation-based student learning: The ideal embeddings serve as intermediate supervision distilling for the student model, ensuring it captures comprehensive semantic information even with partial multi-view data. To improve the inter-class discriminability of the model, we introduce a margin-aware distillation weighting strategy, refining the distillation process and facilitating the generation of classfriendly embeddings. (3) During inference, the student model, trained without label inputs, serves as the reference model, ensuring no data leakage risk. Overall, the contributions of this work are summarized as follows:

• We propose a global-semantic alignment distillation method GLAD tailored for partial multi-view classification without view imputation. Our method addresses the view incompleteness challenges by enabling the precise extraction of task-relevant features, thereby achieving global-semantic alignment across views. • By leveraging labels as semantic anchors, our method performs global alignment among partial multi-view representations. We further introduce a margin-aware weighting-driven distillation loss to facilitate a deeper understanding of alignment semantics, crucial for accurate classification. GLAD works in a self-distillation framework, enabling no data leakage risk. • We conduct extensive experiments and the results substantiate the effectiveness and superiority of the proposed method, showcasing enhancements and improvements in classification compared to existing approaches.

# Related Works

# Partial Multi-view Classification

A precise understanding of data plays a critical role in enhancing the performance of downstream tasks (Hong et al. 2021b,a; Li, Tang, and Mei 2018; Li et al. 2021). Multi-view data offers the advantage of providing complementary information, enabling a more comprehensive understanding (Mo et al. 2023; Pan and Kang 2021; Wang et al. 2024b). However, inevitable missing views result in incomplete views, hindering holistic comprehension. Consequently, the classification of partial multi-view data has become a common challenge in real-world applications. Existing approaches to partial multi-view classification are typically divided into imputation-based and imputation-free methods, based on their treatment of missing views.

Imputation-based PMvC methods recover missing views by mining latent relationships across views or samples (Ou et al. 2024; Zhang and Chen 2022). For example, DCP (Lin et al. 2022) learned view-specific representations and recovered missing views by minimizing conditional entropy between views, enabling mutual prediction in the latent space. However, imputing missing views introduces redundant information, as accurately estimating these views without ground truth is particularly challenging. Furthermore, a high rate of missing data increases computational complexity, which hinders the practical deployment of such methods, especially in security-critical domains. Imputation-free methods, by contrast, avoid redundancy by directly integrating observed views to learn latent representations (Zhu et al. 2022; Zhang et al. 2020). In this line, DICNet (Liu et al. 2023a) learned view-specific representations from observable views, explored inter-view consistency through contrastive learning, and fused these features for classification tasks. While these methods avoid the challenges of imputation, their performance heavily relies on heuristically evaluating the informativeness of different views to achieve global alignment for downstream classification tasks. This poses significant difficulties in remaining low-quality data scenarios.

In contrast to these approaches, our method adopts an imputation-free strategy from the outset, avoiding redundancy associated with imputation. By incorporating labels directly into the input, we provide discriminative guidance that facilitates the learning of informative features from partial multi-view representations. This fine-grained reference allows for global-semantic alignment across views, enhancing the model’s ability to focus on discriminative vectors. Consequently, our approach improves classification performance in incomplete data scenarios.

# Label Enhancement Methods

In the supervision learning paradigm, labels serve as the explicit cues for formulating the objective function, play a crucial role in facilitating the optimization of model parameters. Recently, numerous studies explore label-enhanced mechanisms to improve model training (Yang et al. 2021; You et al. 2020; Bengio, Weston, and Grangier 2010; Sun et al. 2017; Li et al. 2022b). For example, LabelEnc (Hao et al. 2020) introduces a label encoding function to enhance the training of object detection systems. LAD-GNN (Hong et al. 2024) proposes a label-attentive approach to boost GNN learning for graph-level tasks. CMA (You et al. 2020) constructs a label graph and learns semantic label embeddings to guide cross-modality attention learning, thereby enhancing multilabel classification performance. However, these methods focus on single-modality data, which limits their effectiveness in handling multi-view data. Multi-view data presents complex inter-view relationships and challenges in adequately exploring cross-view consistency and diverse information, especially when some views are missing. In contrast, this work is the first to leverage labels as semantic anchors to enhance the performance of partial multi-view classification, guiding global-semantic alignment across views.

# Knowledge Distillation

Knowledge distillation is a technique that transfers knowledge from a complex, high-performing model, known as the teacher model, to a smaller and more computationally efficient model, called the student model. This process reduces the complexity of the model and the computational resources needed while aiming to retain the original performance levels of the model (Xie et al. 2024, 2023b). Knowledge distillation is widely used across various domains, including object detection (Li et al. 2022a, 2023), semantic segmentation (Gao et al. 2024; Li, Halstead, and Mccool 2024), and multi-view learning (Wang et al. 2024a). The first extension of knowledge distillation to multi-view learning challenges comes with MTS-Net (Tian, Sun, and Tang 2022). KDMVC (Wang et al. 2024a) uses self-knowledge distillation within the context of semi-supervised multi-view learning to improve classification performance. While existing methods typically use soft labels produced by the teacher model for distillation, our approach allows the student model to acquire knowledge by emulating the ideal embeddings generated by the teacher model, offering a more efficient and informative reference.

# Methodology

# Problem Formulation

Given a multi-view training set with $N$ samples, denoted as $\{ { \mathbf { X } } _ { n } , { \mathbf { y } } _ { n } \} _ { n = 1 } ^ { N }$ , where $\mathbf { X } _ { n } ^ { - } = \{ \mathbf { x } _ { n } ^ { v } \in \mathbb { R } ^ { d _ { v } } \} _ { v = 1 } ^ { V }$ V consists of $V$ views, $d _ { v }$ is the feature dimension of the samples in the $v$ -th view and ${ \bf y } _ { n }$ is class label. To indicate whether a view is missing, an indicator matrix I is introduced, where $\mathbf { I } \in \{ 0 , 1 \} ^ { N \times V }$ , with $\begin{array} { r } { \sum _ { j = 1 } ^ { V } \mathbf { I } _ { i j } \ \ge \ 1 } \end{array}$ for $\forall i$ . For example: $\mathbf { I } _ { i j } = 0$ indicates tha  the $j$ -th view of the $i$ -th sample is missing, while $\mathbf { I } _ { i j } = 1$ indicates that the $j$ -th view of the $i$ -th sample is observed. During the data pre-processing phase, missing views are filled with 0. The PMvC task aims to train a model on incomplete multi-view data that can accurately classify new samples with arbitrary missing view patterns.

# Overview

In this section, we introduce GLAD for partial multi-view classification. The overall framework is depicted in Figure 1, where the teacher and student models train iteratively within a uniform self-distillation framework. Specifically, the student network is the inference network. Our method is devoid of imputation techniques to circumvent the limitations associated with view-imputation methods, including the potential for introducing redundant information. GLAD employs the ground-truth to address semantic misalignment and reduce redundant information. The approach comprises two distinct learning phases: (1) Global-semantic alignment teacher learning and (2) Margin-aware distillation-based student learning. The teacher model is designed to leverage label embeddings as discriminative guidance, facilitating global-semantic alignment across different views and producing ideal embeddings. This setup allows the student model to absorb comprehensive semantic information through margin-aware distillation. Ultimately, the student model is refined via both distillation and task supervision, enhancing its ability to differentiate category-specific features and mitigate semantic misalignment.

# Global-Semantic Alignment Teacher Learning

This section introduces the teacher model that transfers comprehensive semantic information to the student model. The teacher model comprises two branches: the first branch takes the ground-truth as input and projects them into label embeddings using the label encoder, while the second branch processes partial multi-view data, tokenizing each view as a token for transformer input. The global-semantic alignment component employs cross-attention to fuse label embeddings and extracted partial multi-view embeddings. This fusion process generates ideal embeddings that encapsulate globalsemantic alignment across views. These ideal embeddings are then fed into the classification head to produce classification results. Both branches are jointly trained to minimize classification loss.

Multi-view Embeddings. Cross-view interactions enable the model to leverage shared and diverse features from each view. To achieve this, we project multiple views into the same dimensional embeddings, facilitating better alignment and exploring cross-view intrinsic relationships. For each multiview data $\mathbf { x } _ { i } ^ { v }$ of $v$ -th view, view-specific fully connected layers (FC) are employed to project it into view-specific embeddings:

$$
\begin{array} { r } { \mathbf { z } _ { i } ^ { v } = \mathrm { L N } ( f ^ { v } ( \mathbf { x } _ { i } ^ { v } ) ) , } \end{array}
$$

where LN represents layer normalization. $\mathbf { z } ^ { v } \in \mathbb { R } ^ { N \times D }$ denotes the $\boldsymbol { v }$ -th view embeddings with $D$ -dimensional. Therefore, multi-view embeddings can be formulated as: ${ \bf Z } ^ { t } = \mathrm { ~ }$ $[ \mathbf { z } ^ { 1 } , \mathbf { z } ^ { 2 } , \ldots , \mathbf { z } ^ { V } ]$ , where $\mathbf { Z } ^ { t } \in \mathbb { R } ^ { N \times V \times D }$ and $[ \cdot , \cdot ]$ denotes the concatenation operation performed along the rows.

Global-Semantic Alignment. Our teacher model introduces groud-truth labels as a form of discriminative guidance and guides the cross-view global-semantic alignment process. Given a ground-truth label $\mathbf { y } _ { i }$ , we employ an isolate label encoder, equipped with a Multi-Layer Perceptron (MLP), to map this label into label embeddings:

$$
\mathbf { H } _ { i } ^ { ( L ) } = E ( \mathbf { y } _ { i } ) ,
$$

where $\mathbf { H } ^ { ( L ) } \ \in \ \mathbb { R } ^ { N \times D }$ represents the label embeddings, and $E ( \cdot )$ defines the label encoder. These embeddings $\mathbf { H } ^ { ( L ) }$ encapsulate global-semantic information that is crucial for enhancing the discrimination of category-specific features within the multi-view embeddings $\mathbf { Z } ^ { t }$ . To leverage this, we introduce an attention mechanism to capture the intrinsic relationships between label embeddings and multi-view embeddings. Initially, we apply a set of linear layers with weights $\{ \mathbf { W } _ { Q } , \mathbf { W } _ { K } , \mathbf { W } _ { V } \}$ to map the label embeddings $\mathbf { H } _ { i } ^ { ( L ) }$ into queries and the multi-view embeddings $\mathbf { Z } _ { i } ^ { t }$ into keys and values. The attention mechanism is then formulated as:

$$
\begin{array} { r l } & { \mathbf { H } _ { i } ^ { t } = \mathrm { A t t e n t i o n } ( \mathbf { Z } _ { i } ^ { t } , \mathbf { H } _ { i } ^ { ( L ) } ) } \\ & { \quad \quad = \mathrm { S o f t m a x } \left( z e r o f i l l ( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } ) \cdot \tau \right) V , } \end{array}
$$

![](images/1963d8028e8ee7e418ec438c0ce42be8331daaedafb97d20504cadfd9fab437f.jpg)  
Figure 1: An overview of our GLAD. It comprises two components: the student network, utilized for final inference without label inputs, and the teacher network, responsible for transferring comprehensive semantic information to the student network.

where $\mathbf { H } ^ { t }$ represents the fused embeddings. $K = \mathbf { W } _ { K } \mathbf { Z } _ { i } ^ { t }$ , $V = \mathbf { W } _ { V } \mathbf { Z } _ { i } ^ { t }$ , and $Q = \mathbf { W } _ { Q } \mathbf { H } _ { i } ^ { ( L ) }$ . The parameter $\tau$ is the attention scaling coefficient (Zhang et al. 2022). The function zerofill fills missing views with $- 1 e 9$ to mask them. Subsequently, we enhance the fusion process by applying an add & layer normalization (LN) operation followed by a feed-forward network (FFN). This process is described as:

$$
\mathbf { H } _ { i } ^ { t ^ { \prime } } = \mathrm { F F N } ( \mathbf { L N } ( \mathbf { H } _ { i } ^ { t } + \mathbf { Z } _ { i } ^ { t } ) ) + \mathbf { H } _ { i } ^ { t } ,
$$

To account for missing views, we employ the indicator matrix $\mathbf { I }$ to weighted fusion of these embeddings across views. The final ideal embeddings are computed as:

$$
\mathbf { H } _ { i } ^ { ( T ) } = \frac { 1 } { V } \sum _ { v = 1 } ^ { V } \mathbf { I } _ { i , v } \mathbf { H } _ { i , v , : } ^ { t ^ { \prime } }
$$

where ${ \bf H } ^ { ( T ) } \in \mathbb { R } ^ { N \times D }$ denotes the ideal embeddings that encapsulate global-semantic alignment across views.

Teacher Model Training. The ideal embeddings Hi(T ) are then fed into a shared classification head to produce the predicted label $p _ { i } ^ { t }$ . The teacher model is optimized using cross-entropy, which is formulated as:

$$
\mathcal { L } _ { c l s } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } - \mathbf { y } _ { i } l o g ( p _ { i } ^ { t } )
$$

# Margin-aware Distillation-based Student Learning

Once the teacher model converges, the ideal embeddings $\mathbf { H } ^ { ( T ) }$ are used as intermediate supervision to distill the student model, ensuring it captures comprehensive semantic information despite the partial multi-view data. To refine the knowledge transfer process, we employ a margin-aware distillation strategy. The core idea is to encourage the model to focus more on samples that are near the decision boundaries. Additionally, the student model shares the same classification head with the teacher model. During student model training, both the distillation loss and classification loss are jointly optimized.

Margin-aware Distillation. Given the multi-view data $\{ \mathbf { x } ^ { v } \} _ { v = 1 } ^ { V }$ , we first project each view into view-specific embeddings and concatenate these to obtain the multi-view embeddings $\mathbf { Z ^ { s } } \in \mathbb { R } ^ { N \times V \times D }$ , similar to the teacher model. Unlike the teacher model, the student model does not receive label inputs during inference. We utilize self-attention to facilitate cross-view interactions, which helps reduce redundant information and enhances the mining of consistent and diverse cross-view information. The architecture of the student model is as same as that of the teacher model, with the key difference being that in the attention computation of the student model, the multi-view embeddings serve simultaneously as query, key, and value. As a result, we obtain the fused multi-view embeddings H(S) RN×D.

Given the inherent difficulty in estimating irregular decision boundaries, we draw inspiration from previous work (Litrico, Del Bue, and Morerio 2023; Wei, Luo, and Luo 2023) and introduce the classification uncertainty of each sample from the teacher model as a metric to re-weight the distillation loss. The intuition here is that samples near the decision boundary exhibit higher classification uncertainty. The distillation loss is therefore formulated using Mean Square Error (MSE) as follows:

$$
\mathcal { L } _ { d i s } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \frac { \mathcal { H } ( { \boldsymbol { p } } _ { i } ^ { t } ) } { l o g _ { 2 } C } \| \mathbf { H } _ { i } ^ { ( T ) } , \mathbf { H } _ { i } ^ { ( S ) } \| _ { 2 } ^ { 2 } ,
$$

where $\mathcal { H } ( \cdot )$ denotes the information entropy and $C$ is the number of the classes in the dataset. This refined distillation loss encourages the student model to improve its inter-class discriminability.

Student Model Training In the student model, both the distillation loss $\mathcal { L } _ { d i s }$ and the classification loss $\mathcal { L } _ { c l s }$ are jointly optimized. The distillation loss enables the model to learn comprehensive semantic information from the teacher, while the classification loss drives the model to excel in the partial multi-view classification task. The overall objective function for training the student model is formulated as follows:

$$
\begin{array} { r } { \mathcal { L } = \mathcal { L } _ { c l s } + \lambda \cdot \mathcal { L } _ { d i s } , } \end{array}
$$

where $\mathcal { L } _ { c l s }$ is the classification loss defined in Eq. 6, and $\lambda$ is the trade-off factor for balancing the two losses, defaulting to 1.

# Experiments Experimental Settings

Datasets. We evaluate the performance of our method on five datasets. Scene15 (Fei-Fei and Perona 2005): A scene dataset comprising 3 views, containing 15 classes and a total of 4,485 samples. Animal (Lampert, Nickisch, and Harmeling 2013): The dataset with 2-views, containing 50 classes and a total of 10,158 samples. Caltech101 (Fei-Fei, Fergus, and Perona 2004): A subset of the Caltech101 dataset, containing 2,386 samples with 6 views per sample across 20 classes. BDGP (Cai et al. 2012): A dataset consisting of images related to Drosophila embryos, containing 2,500 samples across 5 categories. Each sample is composed of 4 views. LandUse21 (Yang and Newsam 2010): A satellite image dataset comprising 3 views, containing 21 classes with a total of 2,100 samples.

Compared Methods. To demonstrate the effectiveness and superiority of the proposed method, we compare it with one baseline method and seven existing state-of-the-art methods. i.e., (1) Mean-Imputation that imputes missing views with the mean of all observed samples of the $i$ -th view; (2) CPMNets (Zhang et al. 2019) is an imputation-free method that learns latent representations for all views with available data and maps these latent representations to classification predictions; (3) TMC (Han et al. 2020) is a decision fusion method that accurately identifies and fuse confident views while ignoring unreliable views; (4) Mmydynamics (Han et al. 2022) dynamically evaluates informativeness for each sample and applies weighted fusion multiple views; (5) DDIMvMLC-net (Wen et al. 2023) is an imputation-free method that applies weighted fusion of available views and ignores missing views; (6) DICNet (Liu et al. 2023a) aims to enhance the consistency of view-specific features extracted from observable views. Subsequently, these features are concatenated to facilitate classification tasks; (7) UIMC (Xie et al. 2023a) samples multiple times from the estimated distribution of missing views to impute them and introduces an evidencebased fusion strategy to integrate multiple views reliably; (8) RCML ( $\mathrm { \Delta X u }$ et al. 2024a) is a decision fusion method that provides decision results and reliabilities despite conflictive multi-view data.

Implementation Details. Partial Multi-view Data Construction: To construct partial multi-view datasets, we follow the approach described in (Xie et al. 2023a). Specifically, we randomly select a portion of the samples from a given dataset and remove some of the views from those samples, ensuring that each sample retains at least one view. For a dataset with $N$ samples and $V$ views, the missing rate $\eta$ is calculated as: $\begin{array} { r } { \eta = \frac { \sum _ { i = 1 } ^ { V } M _ { i } } { V \times N } } \end{array}$ , where $M _ { i }$ represents the number of missing samples in $i$ -th view. In the performance comparison experiment, we utilize classification accuracy (ACC) as the evaluation metric, following prior works (Xie et al. 2023a). For the imputation-based method UIMC, we follow the settings of original papers to impute the missing views. The imputation-free methods, such as CPM-Nets, DD-IMvMLCnet, and DICNet are trained directly on incomplete data. For multi-view classification methods, such as TMC, Mmydynamics, and RCML, we apply the Mean-Imputation strategy to fill in the missing views. Each experiment is repeated five times, and the results are averaged and recorded. Our proposed GLAD model is implemented using PyTorch 2.0.1. All experiments are conducted on a PC equipped with an NVIDIA GeForce RTX 3090 GPU.

# Experimental Results and Analysis

Performance Comparison. We compare our method with one baseline method and seven competitive methods across five datasets with various missing rates to evaluate performance. The experimental results are summarized in Table 1, where the best values are highlighted in bold. From these results, several key observations can be made: (1) Our method consistently outperforms the comparison methods across all datasets, even in the presence of missing views. For instance, when the missing rate $\eta = 0$ , our method achieves a $3 . 2 1 \%$ improvement in accuracy on the Scene15 dataset and a $5 . 3 8 \%$ improvement on the LandUse21 dataset compared to the second-best method. Additionally, when the missing rate $\eta = 0 . 1$ , our method shows a $5 . 1 0 \%$ higher accuracy on the LandUse21 dataset, and when $\eta = 0 . 5$ , it achieves a $2 . 9 2 \%$ higher accuracy on the Scene15 dataset than the second-best method. These results demonstrate the effectiveness, superiority, and robustness of our method in handling incomplete multi-view data. (2) Figure 2 visualizes the classification accuracy on three different datasets as the missing rate increases. The results clearly show that our method consistently outperforms the comparison methods at all missing rates. This consistent performance can be attributed to our ability to achieve global semantic alignment across views by incorporating labels as discriminative guides. This alignment is particularly effective in overcoming the challenges of extracting inter-class discriminative features with missing views.

Ablation Study To demonstrate the efficacy of the proposed global-semantic alignment distillation strategy, we conduct ablation experiments on four datasets, comparing our model (denoted as “w/(std)”) to two baseline settings: (1) a model with global-semantic alignment distillation but without the margin-aware weighting strategy (denoted as “w/(basic)”); (2) a model containing only the student network,

<html><body><table><tr><td>Missing Rates</td><td>Method</td><td>Scene15</td><td>Animal</td><td>BDGP</td><td>LandUse_21</td><td>Caltech101-20</td></tr><tr><td>n=0</td><td>Mean-Imputation CPM-Nets (2019) TMC (2020) Mmdynamics (2022) DD-IMvMLC-net (2023) DICNet (2023a) UIMC (2023a) RCML (2024a) Ours △%</td><td>76.14(0.00) 69.90(0.02) 73.85(1.37) 77.26(0.00) 78.86(0.62) 80.29(0.37) 77.70(0.00) 74.02(0.31) 83.50(0.47) 3.21</td><td>86.9(0.00) 84.41(0.81) 85.11(0.13) 86.37(0.00) 86.01(0.38) 84.13(0.44) 0OM 83.53(0.07) 88.54(0.36) 1.64 81.44(0.00)</td><td>95.2(0.00) 95.96(1.82) 98.00(0.18) 98.36(0.00) 98.52(0.27) 98.44(0.41) 95.40(0.13) 99.40(0.00) 99.92(0.18) 0.52 91.80(0.00)</td><td>66.19(0.00) 50.71(2.18) 51.24(3.90) 76.43(0.00) 74.62(1.63) 78.00(0.76) 60.19(1.03) 54.48(0.28) 83.38(0.49) 5.38</td><td>93.29(0.00) 86.71(6.36) 91.24(1.27) 95.31(0.00) 93.58(0.28) 92.33(0.79) 94.97(0.00) 93.67(0.16) 95.77(0.40) 0.46</td></tr><tr><td>η= 0.1</td><td>CPM-Nets (2019) TMC (2020) Mmdynamics (2022) DD-IMvMLC-net (2023) DICNet (2023a) UIMC (2023a) RCML (2024a) Ours △%</td><td>65.66(0.02) 70.59(1.12) 74.80(0.00) 75.41(0.84) 77.57(0.44) 75.81(0.01) 72.24(0.10) 80.07(0.29) 2.5</td><td>79.96(1.11) 81.49(0.06) 82.45(0.00) 81.93(0.47) 80.15(0.18) OOM 80.23(0.34) 83.79(0.27) 1.86</td><td>92.04(3.92) 96.84(0.23) 98.55(0.00) 97.44(0.32) 97.72(0.27) 88.40(0.49) 97.04(0.08) 98.68(0.18) 0.13</td><td>60.00(0.00) 49.57(2.77) 50.76(3.49) 71.19(0.00) 70.76(2.41) 73.57(1.76) 47.62(0.50) 54.00(0.82) 78.67(1.47) 5.1</td><td>90.78(0.00) 86.71(1.49) 90.31(1.53) 94.38(0.00) 93.58(0.41) 92.41(0.36) 87.59(0.43) 94.05(0.21) 95.14(0.18) 0.76</td></tr><tr><td>n=0.5</td><td>Mean-Imputation CPM-Nets (2019) TMC (2020) Mmdynamics (2022) DD-IMvMLC-net (2023) DICNet (2023a) UIMC (2023a) RCML (2024a) Ours △%</td><td>57.19(0.00) 57.08(0.01) 59.71(1.64) 63.77(0.00) 62.45(1.11) 63.30(1.54) 62.54(0.02) 61.76(0.19) 66.69(0.53) 2.92</td><td>69.03(0.00) 63.15(1.31) 66.80(0.06) 68.17(0.00) 66.93(0.67) 64.37(0.44) OOM 67.00(0.22) 70.27(0.27) 1.24</td><td>77.8(0.00) 76.20(1.17) 81.60(0.13) 81.95(0.00) 81.72(0.50) 82.68(0.75) 63.44(0.93) 82.60(0.00) 83.80(0.35) 1.12</td><td>36.43(0.00) 31.00(1.99) 33.33(1.73) 51.93(0.00) 51.38(1.13) 55.05(2.34) 33.05(0.35) 35.76(0.84) 59.62(0.96) 4.57</td><td>80.29(0.00) 77.76(3.74) 85.24(0.41) 89.57(0.00) 87.38(0.50) 87.55(1.21) 70.31(0.49) 86.54(0.58) 91.19(0.26) 1.62</td></tr></table></body></html>

Table 1: The classification accuracy (mean $\pm$ std) of our method and the compared methods at different missing rates, with th best results highlighted in bold. $\eta$ represents the missing rate, and OOM indicates out-of-memory.

![](images/787aaea6dc72d348f41f5b27c32afbc68d4f70dfdfa63cf36692c07e7c7d1162.jpg)  
Figure 2: Classification accuracy on BDGP, LandUse21 and Caltech101-20 datasets with different missing rates.

without global-semantic alignment distillation (denoted as “w/o” ). The experiments are conducted on complete multiview samples, where the missing rate $\eta = 0$ , and the results are evaluated using ACC. The experimental results are summarized in Table 2. By comparing the results of “w/o” and “w/(basic)”, it is evident that the global-semantic alignment distillation strategy significantly improves classification performance. For example, on the LandUse21 and Caltech101- 20 datasets, performance improves by $0 . 7 1 \%$ and $0 . 4 5 \%$ , respectively. This improvement can be attributed to the naive model captures cross-view consistency information while ignoring important complementary information. In contrast, our model leverages ideal embeddings as intermediate supervision distilling for the student model, ensuring it captures comprehensive semantic information across views. Additionally, when comparing the results of “w/(basic)” and “w/(std)”, it is clear that the margin-aware weighting strategy further enhances classification performance, demonstrating its effectiveness. This indicates that the margin-aware weighting strategy indeed strengthens the model in learning inter-class discriminative representations.

Table 2: Ablation studies on four datasets at $\eta = 0$ . “w/(std)” represents our model, “w/(basic)” excludes the margin-aware weighting strategy, and “w/o” uses only the student network.   

<html><body><table><tr><td>Dataset</td><td>w/o</td><td>w/(basic)</td><td>w/(std)</td></tr><tr><td>Scene15</td><td>83.39</td><td>83.84</td><td>83.95</td></tr><tr><td>Animal</td><td>88.04</td><td>88.53</td><td>89.12</td></tr><tr><td>LandUse21</td><td>82.62</td><td>83.33</td><td>84.52</td></tr><tr><td>Caltech101-20</td><td>94.97</td><td>95.18</td><td>96.44</td></tr></table></body></html>

Visualization To intuitively assess the quality of the partial multi-view representations learned by our method, we visualize the raw view data and the partial multi-view representations learned by our method, as well as by the competitive methods DD-IMvMLC-net and RCML, on the test sets of the BDGP dataset, with a missing rate of $\eta = 0 . 5$ . The visualizations are presented in Figure 3, where the partial multiview representations are projected into a two-dimensional space using t-SNE. Results illustrate that the partial multiview embeddings learned by our method exhibit a clearer and more distinct classification structure than the baseline methods. This highlights the ability of our method to learn class-friendly embeddings despite missing views. In contrast, the baseline methods, lacking discriminative guidance, struggle to capture inter-class discriminative features, leading to learned embeddings with a more ambiguous classification structure. The results suggest that utilizing the ground-truth as discriminative guidance significantly enhances the ability of the model to capture inter-class discriminative features. which is crucial for learning robust and class-distinctive representations, especially in scenarios with missing views.

Hyper-parameter Analysis We further discuss the sensitivity of the hyper-parameters $\lambda$ and $\tau$ in Eq. 8 and Eq. 3, with the experimental results recorded in Figure 4. We varied $\lambda$ from 0.01 to 100 and $\tau$ from 0.1 to 0.9, testing the classification performance on three datasets with $\eta = 0$ . From the experimental results, we observe that different values of these hyper-parameters affect classification performance differently across datasets. However, the optimal $\lambda$ can be obtained through tuning on a validation set. The $\tau$ values that achieve the best performance vary across datasets, but after carefully considering the results from multiple datasets, we set $\tau$ to 0.7 as the standard value for our experiments.

# Conclusion

In this work, we propose a novel imputation-free method for the PMvC problem, termed GLobal-semantic Alignment Distillation (GLAD) for partial multi-view classification, which can mitigate semantic misalignment exacerbated by missing views. Specifically, GLAD introduces a self-distillation framework that endows the student model to distinguish informative information and align semantics across views. In the self-distillation framework, this work introduces ground truth labels as discriminative guidance, guiding the generation of ideal embeddings that encapsulate global-semantic alignment across views. These ideal embeddings serve as intermediate supervision distilling for the student model, ensuring it captures comprehensive semantic information. This distillation mechanism enables the student model to generate class-friendly embeddings that significantly improve classification performance. Extensive experimental results validate the effectiveness and superiority of the proposed method.

![](images/09cca0328dc828bce2dd93370fb9efacaefdc6f5ec9310814a5c269ef43ec53f.jpg)  
Figure 3: Visualization of concatenated raw data and latent representations learned by different methods on the BDGP dataset with $\eta = 0 . 5$ .

![](images/4fcd28c0fac301a86760e660379a2c87a34a80487b0f23a39a5e2e2cf71f0bf3.jpg)  
Figure 4: Hyper-parameter sensitivity results of $\lambda$ and $\tau$ on three datasets at $\eta = 0$ .

# Acknowledgments

This article was partially supported by the Jiangsu Province Key R& D Program (Modern Agriculture) Key Project (BE2023352), Key Medical Research Projects of Jiangsu Provincial Health Commission (ZD2022068), National Natural Science Foundation of China (61941113), the China Scholarship Council under Grant 202306840098. We thank all anonymous reviewers for their constructive comments.