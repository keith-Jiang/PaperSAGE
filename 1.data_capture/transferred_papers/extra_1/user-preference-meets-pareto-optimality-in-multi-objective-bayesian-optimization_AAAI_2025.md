# User Preference meets Pareto-Optimality in Multi-Objective Bayesian Optimization

Joshua Hang Sai $\mathbf { I p } ^ { 1 }$ , Ankush Chakrabarty2, Ali Mesbah1, Diego Romeres2

1University of California, Berkeley 2Mitsubishi Electric Research Laboratories ipjoshua $@$ berkeley.edu, chakrabarty $@$ merl.com, mesbah@berkeley.edu, romeres@merl.com

# Abstract

Incorporating user preferences into multi-objective Bayesian optimization (MOBO) allows for personalization of the optimization procedure. Preferences are often abstracted in the form of an unknown utility function, estimated through pairwise comparisons of potential outcomes. However, utilitydriven MOBO methods can yield solutions that are dominated by nearby solutions, as non-dominance is not enforced. Additionally, classical MOBO commonly relies on estimating the entire Pareto-front to identify the Pareto-optimal solutions, which can be expensive and ignore user preferences. Here, we present a new method, termed preference-utility-balanced MOBO (PUB-MOBO), that allows users to disambiguate between near-Pareto candidate solutions. PUB-MOBO combines utility-based MOBO with local multi-gradient descent to refine user-preferred solutions to be near-Pareto-optimal. To this end, we propose a novel preference-dominated utility function that concurrently preserves user-preferences and dominance amongst candidate solutions. A key advantage of PUB-MOBO is that the local search is restricted to a (small) region of the Pareto-front directed by user preferences, alleviating the need to estimate the entire Paretofront. PUB-MOBO is tested on three synthetic benchmark problems: DTLZ1, DTLZ2 and DH1, as well as on three real-world problems: Vehicle Safety, Conceptual Marine Design, and Car Side Impact. PUB-MOBO consistently outperforms state-of-the-art competitors in terms of proximity to the Pareto-front and utility regret across all the problems.

# 1 Introduction

The challenge of identifying optimal trade-offs between multiple complex objective functions is pervasive in many real-world scientific and industrial applications. When the objectives are black-box functions constructed from noisy observations, multi-objective Bayesian optimization (MOBO) (Konakovic Lukovic, Tian, and Matusik 2020; Daulton, Balandat, and Bakshy 2020) is an effective multiobjective optimization (MOO) approach owing to its high sample efficiency, especially as compared to classic MOO methods such as CMA-ES (Hansen and Ostermeier 2001) and NSGA-II (Deb et al. 2002). Nonetheless, state-of-theart MOBO methods generally seek to estimate the entire Pareto-front, which can become prohibitively expensive due to sample-based evaluation of acquisition functions such as $q$ -Expected Hypervolume Improvement (qEHVI) (Daulton, Balandat, and Bakshy 2020).

Recently, there has been a growing interest in preferencebased MOBO (e.g., (Abdolshah et al. 2019; Ahmadianshalchi, Belakaria, and Doppa 2024; Shao et al. 2023; Ozaki et al. 2024)) that aims to incorporate user preferences into MOO for selecting optimal points. In essence, preferencebased MOBO guides the optimization process towards regions of interest within the Pareto-front by leveraging user feedback, typically in the form of pairwise comparisons between solutions generated by the optimization algorithm. These comparisons are used to estimate an underlying utility function that describes user preferences. However, while preference-based MOBO can effectively identify solutions with high utility as informed by user feedback, the resulting solutions may not be Pareto-optimal.

To address this challenge, we present a new method, termed Preference-Utility-Balanced MOBO (PUB-MOBO), that systematically approaches the user-informed regions of interest within the Pareto-front by synergizing global and local search strategies. PUB-MOBO begins with a global search driven by utility maximization to identify regions in the solution space that align with user preferences. Subsequently, a local search is conducted in the vicinity of these solutions to discover solutions that are closer to Pareto-optimality. Additionally, a new utility function, the Preference-Dominated Utility Function (PDUF), is proposed that encapsulates the concept of dominance within a single function. PDUF allows for consistently identifying dominating solutions, while providing a straightforward means for expressing all possible user preferences. This differs to existing utility functions for preference-based MOBO such as the negative $l _ { 1 }$ distance from an ideal solution irrespective of the solution being on the Pareto-front or an infeasible ideal solution (Miettinen 1999), or the weighted sum where not all Pareto-optimal points can be assigned with the highest utility value from any choice of weights (Chiandussi et al. 2012). Empirical demonstrations on several synthetic benchmark and real-world problems show that PUB-MOBO not only enhances the utility of the optimization solutions, but also yields near Pareto-optimal solutions.

In sum, the main contributions of this paper include:

(C1) We introduce a new preference-based MOBO method,

PUB-MOBO, that can effectively integrate feedback about user preferences and moves towards Paretooptimality in MOO of black-box functions.

(C2) We propose the use of gradient descent (GD) in the context of MOBO, along with a new utility function, PDUF, that seamlessly combine user preferences with the notion of dominance to identify user-preferred solutions that are approximately Pareto-optimal.   
(C3) We illustrate the importance of reducing gradient uncertainty in GD to aid PUB-MOBO in locating highutility near Pareto-optimal solutions. Our approach is based on the Gradient Information acquisition function (Mu¨ller, von Rohr, and Trimpe 2021).   
(C4) Through numerical experiments on synthetic and realworld benchmark problems, we demonstrate that incorporating GD into utility-based MOBO can significantly enhance performance, yielding solutions that better align user preferences and Pareto-optimality.

# 2 Related Work

Traditional MOBO methods such as $q$ -EHVI (Daulton, Balandat, and Bakshy 2020) assume that all Pareto-optimal solutions are equally desirable to the user, which might not be the case in practice. Instead, preference-based MOBO models user preferences with utility functions. Lin et al. (2022); Astudillo and Frazier (2020) propose the EUBO and qEIUU acquisition functions respectively, which take advantage of user-preference when querying new points.

Various works have proposed different methods to incorporate gradients as additional information in singleobjective BO to enhance global search. Wu et al. (2017) construct a joint GP to correlate zeroth and first order information, demonstrating that gradients aid the surrogate in approximating the posterior and change which points to query. Similarly, Makrygiorgos, Paulson, and Mesbah (2023) leverage gradients to establish stationarity conditions within a Karush-Kuhn-Tucker (KKT) formulation.

Other works instead use gradients for first-order optimization, also in the single-objective BO context. Bayesian and local optimisation sample-wise switching optimisation method (BLOSSOM) switches from global search to local search with BFGS when the posterior objective is close to the objective (McLeod, Roberts, and Osborne 2018). On the other hand, (Mu¨ller, von Rohr, and Trimpe 2021; Nguyen et al. 2022) abandon global search and construct local GPs for local optimization. To our knowledge, involving gradients for preference-based MOBO has not been studied and PUB-MOBO is the first algorithm to do this.

# 3 Preliminaries

# 3.1 Problem Formulation

We tackle a multi-objective optimization (MOO) problem for minimizing ${ { n } _ { f } }$ expensive-to-evaluate objective functions, denoted by $f _ { i } ( { \pmb x } )$ for $i \in \{ 1 , \cdots , n _ { f } \}$ . Consequently, the objective function vector is denoted ${ \pmb f } ( { \pmb x } )$ , where $\textbf { \em x } \in \mathbf { \Xi }$ $\mathbb { R } ^ { n _ { x } }$ denote the decision variables. We assume that for a candidate $\scriptstyle { \mathbf { { \vec { x } } } }$ , the function $\pmb { f } ( \pmb { x } )$ can be evaluated, but no first- or higher-order information about any component of $f$ is available. Furthermore, an analytical form of $f$ is not known.

For MOO problems without user-preferences, the objective is to attain Pareto-optimality, which is defined as follows (Deb and Gupta 2005). A point $\bar { \bf x }$ in a feasible set $\mathbb { X }$ is Pareto-optimal if it is not dominated1 by any other solution in $\mathbb { X }$ . That is, there exists no other feasible point $\textbf { x } \in \ \mathbb { X }$ such that $f _ { i } ( { \pmb x } ) \ \le \ f _ { i } ( \bar { \pmb x } )$ for all $i \in \{ 1 , \cdots { \overset { \vartriangle } { } } , n _ { f } \}$ and $f _ { j } ( { \pmb x } ) < f _ { j } ( { \pmb x } ^ { \star } )$ for at least one $j \in \{ 1 , \cdots , n _ { f } \}$ . Note that accurately computing the set of Pareto-optimal points, referred to as the Pareto-front ${ \mathbb X } _ { \mathsf { p a r e t o } }$ , can often be computationally prohibitive, even for small $n _ { f }$ .

In the presence of a user, estimating the entire Paretofront may become unnecessary, especially when only specific sub-regions of the feasible set $\mathbb { X }$ is of interest. Mathematically, such user-preferences are often abstracted in the MOBO literature via utility functions. Specifically, the MOO problem is recast as a (scalar) utility maximization problem

$$
\operatorname* { m a x } _ { \pmb { x } \in \mathbb { X } } \quad u \left( \pmb { f } ( \pmb { x } ) \right) ,
$$

where $u : \mathbb { R } ^ { n _ { f } }  \mathbb { R }$ is the unknown utility function that dictates the behavior of the user. Note that the input to the utility is a noise-corrupted outcome vector ${ \pmb y } = { \pmb f } ( { \pmb x } ) + { \boldsymbol \varepsilon } .$ , where $\varepsilon$ is zero-mean noise with variance $\sigma _ { \varepsilon } ^ { 2 } { I } _ { n _ { y } }$ where ${ \cal I } _ { n _ { f } }$ is the $n _ { f } \times n _ { f }$ identity matrix. For the sequel, let the highest utility Pareto-point be defined as

$$
\pmb { x } ^ { * } \in \underset { \pmb { x } \in \mathbb { X } _ { \mathsf { p a r e t o } } } { \arg \operatorname* { m a x } } u ( \pmb { f } ( \pmb { x } ) ) .
$$

Following the preference-based BO literature, we assume that the utility function is not available to evaluate directly, and no functional form is known. Additionally, it is wellestablished that user preferences are difficult to be assigned to continuous numerical values; instead we suppose that users are more inclined to provide weak supervision in the form of pairwise comparisons (Chu and Ghahramani 2005; Lin et al. 2022). The following assumption asserts that a typical user will select dominating solutions when possible.

Assumption 1 If $\pmb { y } _ { 1 }$ and $\pmb { y } _ { 2 }$ are candidate outcomes presented to the user and $\pmb { y } _ { 1 } \succ \pmb { y } _ { 2 }$ , then the user will always select $\pmb { y } _ { 1 }$ ; that is, $u ( \pmb { y } _ { 1 } ) > u ( \pmb { y } _ { 2 } )$ .

This assumption is a constraint that must be respected when modeling preference-based MOBO problems to accurately reflect real user behavior.

# 3.2 Modeling with Gaussian processes

We first discuss the modeling choices considered to learn the outcome function $f$ and the utility function $u$ : their respective approximations are denoted $\hat { \pmb f }$ and $\hat { u }$ .

Modeling outcomes. Gaussian process (GP) regression is a popular choice for constructing the surrogate $\hat { f }$ for the true outcome function $f$ . We train an independent GP for each objective $\mathbf { \boldsymbol { f } } _ { i }$ , though a multi-output GP that models correlations between the objectives could also be considered (Alvarez et al. 2012). Each GP is defined $a$ priori by a mean function $m ( { \pmb x } )$ and covariance function $k _ { i } ( x , x ^ { \prime } )$ called kernel. For this work, any $\mathcal { C } ^ { 2 }$ kernel is admissible.

Let $\pmb { X } _ { T } = [ \pmb { x } _ { 1 } , \pmb { x } _ { 2 } , \dots , \pmb { x } _ { T } ]$ ; we drop the subscript for brevity. Given a dataset $D : = \mathsf { \bar { \Gamma } } ( X , Y )$ , comprising inputoutcome pairs, the mean and variance of the posterior are:

$$
\begin{array} { c } { { \mu _ { i } ( { \pmb x } ) = m ( { \pmb x } ) + k _ { i } ( { \pmb x } , { \pmb X } ) { \pmb \chi } _ { \sigma } ^ { - 1 } ( { \pmb X } ) ( Y _ { i } - m ( { \pmb X } ) ) , } } \\ { { \Sigma _ { i } ( { \pmb x } ) = k _ { i } ( { \pmb x } , { \pmb x } ) - k _ { i } ( { \pmb x } , { \pmb X } ) { \pmb \chi } _ { \sigma } ^ { - 1 } ( { \pmb X } ) k _ { i } ( { \pmb X } , { \pmb x } ) , } } \end{array}
$$

where $K _ { \sigma } ( X ) : = K ( X , X ) + \sigma ^ { 2 } I$ and $m ( \cdot )$ is the prior mean. Since the derivative is a linear operator, the derivative GP is another GP (Williams and Rasmussen 2006) characterized fully by the mean and covariance functions

$$
\begin{array} { r l r } & { } & { \mu _ { i } ^ { \nabla } ( { \pmb x } ) = \nabla m ( { \pmb x } ) + \nabla k _ { i } ( { \pmb x } , { \pmb X } ) { / K } _ { \sigma } ^ { - 1 } ( { \pmb X } ) ( Y _ { i } - m ( { \pmb X } ) ) , } \\ & { } & { ( 4 i ) } \\ & { } & { \Sigma _ { i } ^ { \nabla } ( { \pmb x } ) = \nabla ^ { 2 } k _ { i } ( { \pmb x } , { \pmb x } ) - \nabla k _ { i } ( { \pmb x } , { \pmb X } ) { / K } _ { \sigma } ^ { - 1 } ( { \pmb X } ) \nabla k _ { i } ( { \pmb X } , { \pmb x } ) } \end{array}
$$

Modeling preferences. We assume the user is only capable of weak supervisions in the form of pairwise comparisons (PC). That is, if the user prefers $\textbf { \textit { y } } : = \textbf { \textit { f } }$ over $y : = f ^ { \prime }$ , the pairwise comparison function $r ( \pmb { y } , \pmb { y } ^ { \prime } ) = 0$ . In the event that the user prefers $\boldsymbol { y } ^ { \prime }$ instead, $r ( \pmb { y } , \pmb { y } ^ { \prime } ) = 1$ . Pairwise GPs c.f. (Chu and Ghahramani 2005) allow us to learn a latent functional representation $\hat { u }$ of the true user utility based on this preference feedback. The latent function satisfies $\hat { u } ( \pmb { y } ) > \hat { u } ( \pmb { y } ^ { \prime } )$ if the user prefers $_ y$ , and vice versa.

# 4 Preference-Utility-Balanced (PUB) MOBO

This work is motivated by the practical consideration that most users expect to be shown promising candidate outcomes after very few interactions. Furthermore, they often require some assurance that the suggested candidates are not only high in utility, but also high in performance, i.e., close to the Pareto-front. To this end, we propose PUB-MOBO, which relies on utility maximization to ascertain candidate solutions that are preferred by the user, while promoting a local search towards the Pareto-front using estimated gradients. Empirically, we observe that the local search finds solutions near Pareto points, which subsequently accelerates the search for high-utility solutions. The rationale behind this is that the local search likely yields dominating solutions over those found by utility maximization alone. These dominating solutions, by Assumption 1, have higher utility. This implies that even with few user interactions, i.e. pairwise comparisons, we can obtain promising candidates that are unlikely to be dominated. As more user feedback is collected, we expect to obtain near-maximal utility solutions due to the utility maximization, from which following local gradients should result in a near-Pareto solution.

# 4.1 PUB-MOBO Algorithm

The proposed PUB-MOBO method operates in three stages. Following the work of Lin et al. (2022), the first two

# Algorithm 1: PUB-MOBO

1: Generate initial data: ${ \pmb x } _ { \mathrm { I N I T } } , { \pmb y } _ { \mathrm { I N I T } } , r \big ( { \pmb y } _ { \mathrm { I N I T } } \big )$   
2: $D = ( \boldsymbol { x } _ { \mathrm { I N I T } } , \boldsymbol { y } _ { \mathrm { I N I T } } )$   
3: $P $ comparisons on a subset of ${ \pmb y } _ { \mathrm { I N I T } } \times { \pmb y } _ { \mathrm { I N I T } }$   
4: Update outcome model $\hat { f }$ with $D$   
5: Update preference model $\hat { u }$ with $P$   
6: while # outcome evaluations $\leq$ budget do   
7: PE stage   
8: $x _ { 1 } , x _ { 2 } \gets \mathrm { a r g m a x } _ { x _ { 1 } , x _ { 2 } }$ EUBO   
9: $\pmb { y _ { 1 } }$ , $y _ { 2 } = \hat { f } ( x _ { 1 } ) , \hat { f } ( x _ { 2 } )$   
10: $r ( \pmb { y _ { 1 } } , \pmb { y _ { 2 } } ) $ user provides a comparison   
11: Append $P$ with $( { \pmb y _ { 1 } } , { \pmb y _ { 2 } } , { \pmb r } ( { \pmb y _ { 1 } } , { \pmb y _ { 2 } } ) )$   
12: Update pref. model $\hat { u }$ with $( { \pmb y _ { 1 } } , { \pmb y _ { 2 } } , { \pmb r } ( { \pmb y _ { 1 } } , { \pmb y _ { 2 } } ) )$   
13: EXP stage   
14: $\pmb { x } _ { \mathrm { E X P } } \gets \mathrm { a r g m a x } _ { \pmb { x } }$ qEIUU   
15: $\begin{array} { r } { { \pmb y } _ { \mathrm { E X P } } = { \pmb f } ( { \pmb x } _ { \mathrm { E X P } } ) } \end{array}$   
16: Append $D$ with $( \pmb { x } _ { \mathrm { E X P } } , \pmb { y } _ { \mathrm { E X P } } )$   
17: Update outcome model $\hat { f }$ with $( \pmb { x } _ { \mathrm { E X P } } , \pmb { y } _ { \mathrm { E X P } } )$   
18: GD stage   
19: $( X _ { \mathrm { G D } } , Y _ { \mathrm { G D } } ) \gets 1$ Local Gradient Descent(xEXP)   
20: Append $D$ with $( X _ { \mathrm { G D } } , Y _ { \mathrm { G D } } )$   
21: end while

stages of the framework are the preference exploration (PE) stage and the outcome evaluation via experimentation (EXP) stage. Our method extends these two stages with an additional stage based on local multi-gradient descent, denominated the GD stage.

In each PUB-MOBO iteration, these three stages are executed, and the process is repeated ad infinitum, or (more practically) until a pre-decided budget for total number of outcome/function evaluations is attained; see Algorithm 1.

Preference Exploration. In the PE stage, the user expresses their preferences over a query of two candidate solutions in a form of pairwise comparisons. The comparison is then used to update the estimate of the utility function $\hat { u }$ . The candidate solutions in the query are obtained by optimizing the expected utility of both outcomes acquisition function, which is a look-ahead function that maximizes the difference in expected utility after a user query $r ( \hat { f } ( x _ { 1 } ) , \hat { f } ( x _ { 2 } ) )$ . Since this is expensive to compute, a simplified acquisition called EUBO, proposed in (Lin et al. 2022), is used instead:

$$
\begin{array} { r } { \mathsf { E U B O } ( x _ { 1 } , x _ { 2 } ) = \mathbb { E } [ \operatorname* { m a x } ( \hat { u } ( \hat { f } ( x _ { 1 } ) , \hat { u } ( \hat { f } ( x _ { 2 } ) ) ] . } \end{array}
$$

This has the advantage of estimating utility with outcome posteriors that are feasible under $f$ . Note that no evaluation of $f$ is required in this stage.

Outcome evaluation via Experiments. The EXP stage involves computing the optimal decision variables and collecting outcome evaluations to update the outcome model $\hat { f }$ . In this process, the well-established expected improvement under utility uncertainty (qEIUU) acquisition function, (Astudillo and Frazier 2020), widely used in preference-based contexts, is maximized to determine the optimal decision

variables, $\pmb { x } _ { \mathrm { E X P } }$ . Here,

$$
\mathsf { q E I U U } ( \pmb { x } ) = \mathbb { E } \left[ \operatorname* { m a x } ( \hat { u } ( \hat { \pmb { f } } ( \pmb { x } ) ) - \hat { u } ( \pmb { f } ( \pmb { x } _ { \mathrm { b e s t } } ) ) , 0 ) \right] ,
$$

where $\pmb { x } _ { \mathrm { b e s t } } = \arg \operatorname* { m a x } _ { x \in \pmb { X } } \hat { u } ( \hat { \pmb f } ( \pmb x ) )$ , and

$$
\pmb { x } _ { \mathrm { E X P } } : = \underset { \mathbb { X } } { \arg \operatorname* { m a x } } \mathfrak { q } \mathsf { E } ^ { \mathsf { I U U } ( \pmb { x } ) . }
$$

Since the expectation in (6) is with respect to the outcome and utility models, the analytical expression is challenging to compute. Instead, this can be evaluated via the Monte Carlo approach, where the reparameterization trick is applied to both $\hat { f }$ and $\hat { u }$ and the acquisition function is optimized using sample-averaging; c.f. Wilson, Hutter, and Deisenroth (2018); Balandat et al. (2020). Note that after $\pmb { x } _ { \mathrm { E X P } }$ is obtained, we append it along with its true outcome value $\pmb { f } ( \pmb { x } _ { \mathrm { E X P } } )$ to the current dataset $D$ .

Local gradient descent. This GD stage is motivated by the fact that $\pmb { x } _ { \mathrm { E X P } }$ , while expected to be high in utility, is not specifically designed to be near the Pareto-front. Analogous to single-objective optimization, we will pursue local gradients that are expected to generate a trajectory of $\scriptstyle { \mathbf { { \vec { x } } } }$ candidates that evolves towards a nearby Pareto-optimal point. We will refer to these gradient-following decision variables as $\cdot _ { x _ { \mathrm { G D } } } .$ . We set the initial $x _ { \mathrm { G D } }$ to be $\pmb { x } _ { \mathrm { E X P } }$ .

Clearly, for a MOO problem, gradient descent must be adapted for multiple objectives. We propose the use of multiple gradient descent algorithm (MGDA) (De´side´ri 2012), which was designed for smooth multi-outcome objective functions. While MGDA is provably convergent for whitebox optimization problem settings i.e. when gradients of each outcome of $f$ is accessible, it has not been tested in the context of black-box MOO such as in this paper, where gradients are not accessible, and must be estimated. However, MGDA exhibits some theoretical properties that, we hypothesize, and demonstrate via experiments, are beneficial in the MOBO context. MGDA exploits the KKT conditions (Fliege and Svaiter 2000; Scha¨ffler, Schultz, and Weinzierl 2002)

$$
\alpha \geq 0 , 1 ^ { \top } \alpha = 1 , \alpha ^ { \top } \nabla f ( x ) = 0 ,
$$

and recasts this for MOO as a quadratic cost constrained on the probability simplex, that is:

$$
\operatorname* { m i n } _ { \alpha \geq 0 } \left\| \alpha ^ { \top } \nabla f ( x ) \right\| ^ { 2 } \mathrm { \ s u b j e c t { t o : } } \mathbf { 1 } ^ { \top } \alpha = 1 .
$$

It is well-known, c.f. De´side´ri (2012), that a solution to (8) is either: $\alpha ^ { \top } \nabla f ( x ) = 0$ , in which case the current parameters $\scriptstyle { \pmb x }$ are Pareto-optimal, or $\alpha ^ { \top } \nabla f ( \pmb { x } ) \neq 0$ , and $\pmb { \alpha } ^ { \top } \nabla f ( \pmb { x } )$ is a feasible descent direction. Given that (8) is a quadratic cost over linear constraints, we can use the FrankWolfe algorithm (Sener and Koltun 2018; Jaggi 2013) to efficiently compute optimal solutions.

Solving (8) yields an optimal $\alpha$ with which we can take a gradient step $\dot { \pmb { x } } _ { \mathrm { G D } }  \pmb { x } _ { \mathrm { G D } } - \eta \pmb { \alpha } ^ { \top } \pmb { \nabla } f ( \pmb { x } _ { \mathrm { G D } } )$ . However, there are two clear difficulties at this juncture. First, this update may yield an $x _ { \mathrm { G D } } \notin \mathbb { X }$ . To counter this, we stop updating when this happens, and stop the local gradient search phase, moving on to the next PUB-MOBO iterations with an updated dataset $D$ that contains all the $\scriptstyle { \pmb x } _ { \mathrm { G D } }$ and correspond $ { \boldsymbol { y } } _ { \mathrm { G D } }$ observed so far.

# Algorithm 2: LOCAL GRADIENT DESCENT

1: Initialize xGD xEXP   
2: $( X _ { \mathrm { G D } } , Y _ { \mathrm { G D } } ) = ( \emptyset , \emptyset )$   
3: # of multi-gradient steps, nGD   
4: # of GI optimizations, $n _ { \mathrm { G I } }$   
5: Early stopping threshold, $\varepsilon _ { \mathrm { G D } }$   
6: for $i \leq n _ { \mathrm { G D } }$ do   
7: Compute $\mu ^ {  { \nabla } } (  { \boldsymbol { { x } } } _ { \mathrm { G D } } )$ using (4a)   
8: Compute $\mathbf { \dot { M } } = \mu ^ { \nabla } ( { \pmb x } _ { \mathrm { G D } } ) ^ { \top } \pmb \mu ^ { \nabla } ( { \pmb x } _ { \mathrm { G D } } )$   
9: $\alpha \mathrm {  } \mathrm { F r a n k \mathrm { \mathrm { - } W o l f e } } ( M )$   
10: ${ \pmb x } _ { \mathrm { G D } } \gets { \pmb x } _ { \mathrm { G D } } - \eta { \pmb \alpha } ^ { \top } { \pmb \mu } ^ { \nabla } ( { \pmb x } _ { \mathrm { G D } } )$   
11: if $\pmb { x } _ { \mathrm { G D } } \in \mathbb { X }$ and $\left| \left| \alpha ^ { \top } \pmb { \mu } ^ { \nabla } ( \pmb { x } _ { \mathrm { G D } } ) \right| \right| _ { 2 } ^ { 2 } > \varepsilon _ { \mathrm { G D } }$ then   
12: Evaluate the true objective: ${ \boldsymbol { y } } _ { \mathrm { G D } } = { \boldsymbol { f } } ( { \boldsymbol { x } } _ { \mathrm { G D } } )$   
13: Append $( X _ { \mathrm { G D } } , Y _ { \mathrm { G D } } )$ with $( \mathbfcal { x } _ { \mathrm { G D } } , \mathbfcal { y } _ { \mathrm { G D } } ) ,$ )   
14: Update outcome model $\hat { f }$ with $( \mathbf { \boldsymbol { x } } _ { \mathrm { G D } } , \mathbf { \boldsymbol { y } } _ { \mathrm { G D } } )$   
15: for $j \le n _ { \mathrm { G I } }$ do   
16: $\pmb { x } _ { \mathrm { G I } }  \mathrm { a r g } \mathrm { m a x } _ { \pmb { x } ^ { \prime } } \mathrm { G I }$   
17: Evaluate the true objective: ${ \boldsymbol { y } } _ { \mathrm { G I } } = { \boldsymbol { f } } ( { \boldsymbol { x } } _ { \mathrm { G I } } )$   
18: Append $( X _ { \mathrm { G D } } , Y _ { \mathrm { G D } } )$ with $( \boldsymbol { \mathscr { x } } _ { \mathrm { G I } } , \boldsymbol { \mathscr { y } } _ { \mathrm { G I } } )$   
19: Update outcome model $\hat { f }$ with $( x _ { \mathrm { G I } } , y _ { \mathrm { G I } } )$   
20: end for   
21: else   
22: break   
23: end if   
24: end for   
25: return $( X _ { \mathrm { G D } } , Y _ { \mathrm { G D } } )$

The second and more debilitating problem is that we do not have access to gradients of $f$ . Thankfully, we do have a surrogate model $\hat { f }$ with which we can obtain an estimate of the gradient at any $\scriptstyle { \mathbf { { \vec { x } } } }$ with $\pmb { \mu } ^ { \nabla } : = \mathbb { E } [ \pmb { \nabla } \hat { f } ( \pmb { x } ) ]$ through (4a). The gradient step is then

$$
\pmb { x } _ { \mathrm { G D } }  \pmb { x } _ { \mathrm { G D } } - \eta \pmb { \alpha } ^ { \top } \pmb { \mu } ^ { \nabla } ( \pmb { x } _ { \mathrm { G D } } ) .
$$

Unfortunately, there is no clear correlation between the uncertainties in $f$ and $\boldsymbol { \nabla } f$ , so $\mu ^ { \nabla }$ could have large uncertainties even near previously observed points. Therefore, it is imperative to incorporate techniques that can reduce uncertainty in the posterior of the gradient estimate. To this end, we propose to use the gradient information (GI) acquisition function, described in Mu¨ller, von Rohr, and Trimpe (2021).

We explain briefly the mechanism of the GI acquisition. Suppose we select the best candidate from the EXP stage, $\pmb { x } _ { \mathrm { E X P } }$ , and set it as the initial candidate for the local gradient search: $\scriptstyle { \pmb x } _ { \mathrm { G D } }$ . The GI acquisition tries to select a subsequent point $\mathbf { { x } ^ { \prime } }$ that will minimize the uncertainty of the gradient at $\scriptstyle \mathbf { x } _ { \mathrm { G D } }$ if $\mathbf { { x } ^ { \prime } }$ and its corresponding $\boldsymbol { y } ^ { \prime }$ were known.

By considering all $n _ { f }$ objective independently distributed, we can formulate the uncertainty information contained in the covariance matrix $\pmb { \Sigma } ^ { \nabla }$ in (4b) using A-optimal design (Chakrabarty, Buzzard, and Rundell 2013), and maximize:

$$
\sum _ { i = 1 } ^ { n _ { f } } \mathbb { E } \left[ \mathsf { T r } ( \Sigma _ { i } ^ { \nabla } ( \pmb { x } _ { \mathrm { G D } } | D ) ) - \mathsf { T r } \left( \Sigma _ { i } ^ { \nabla } \left( \pmb { x } _ { \mathrm { G D } } | D , ( \pmb { x } ^ { \prime } , \pmb { y } ^ { \prime } ) \right) \right) \right] ,
$$

which, for Gaussian distributions, is equivalent to

$$
\operatorname { G I } ( \pmb { x } ^ { \prime } ) = \sum _ { i = 1 } ^ { n _ { f } } \operatorname { T r } \left( \nabla k _ { i } ( \pmb { x } _ { \mathrm { G D } } , \pmb { X } ^ { \prime } ) K _ { \sigma } ^ { - 1 } ( \pmb { X } ^ { \prime } ) \nabla k _ { i } ^ { \top } ( \pmb { x } _ { \mathrm { G D } } , \pmb { X } ^ { \prime } ) \right) ,
$$

where $X ^ { \prime } = \{ X \cup x ^ { \prime } \}$ . For each gradient-step in $n _ { \mathrm { G D } }$ , the GI acquisition function is optimized $n _ { \mathrm { G I } }$ times to reduce gradient uncertainty. Upon each optimization, we evaluate the outcome function to obtain a corresponding $\boldsymbol { y } _ { \mathrm { G D } }$ , which is appended to the dataset $D$ for subsequent PUB-MOBO iterations.

# 5 Preference-Dominated Utility Function

The utility function represents the user preference in preference-based MOBO algorithms and is used to simulate user responses. Practically, the utility function is employed to respond to user queries, such as providing pairwise comparisons between two outcomes (Lin et al. 2022). A utility function used to test preference-based MOBO algorithms should satisfy two key properties:

(P1) Dominance Preservation: When evaluating a query, the true utility function should satisfy Assumption 1.   
(P2) Preference Integration: The utility function should have parameters $\theta _ { u }$ that allow unique strictly maximalutility Pareto-optimal solutions. That is, for any $\textbf { \em x } \in$ ${ \mathbb X } _ { \mathsf { p a r e t o } }$ , there exists an easily computable $\theta _ { u } \ \in \ \mathbb { R } ^ { n _ { u } }$ such that $u ( { \pmb f } ( { \pmb x } ) | \theta _ { u } ) > u ( { \pmb f } ( \{ { \mathbb X } _ { \sf p a r e t o } \setminus { \pmb x } \} | \theta _ { u } )$ .

For instance, the commonly used $\ell _ { 1 }$ distance (a) fails to satisfy the Preference Integration property when calculated from the utopia point, and violates Dominance Preservation when calculated from any other point. This is illustrated in Fig. 1a, where the contours of an $\ell _ { 1 }$ distance utility function is shown with an example Pareto-front. Here, the two red points are indistinguishable according to the utility function, demonstrating the limitations of $\ell _ { 1 }$ distance in distinguishing between Pareto-optimal solutions.

![](images/0ed6def9a16402f1a9c4c70e3f1c8eafa9f18e830c195fd0a490764df9b7ff02.jpg)  
Figure 1: Contour plots of (a) the commonly used negative $l _ { 1 }$ distance Utility function (b) the proposed PDUF.

Therefore, we propose the preference-dominated utility function (PDUF) which merges the concept of dominance with user preferences. An illustration of the contours in a 2D case is shown in Fig. 1b. The PDUF integrates the concept of dominance with user preferences by combining multiple logistic functions centered around different points in the objective function space. The PDUF is defined as:

$$
u ( \pmb { y } ) = \frac { 1 } { n _ { c } } \sum _ { i = 1 } ^ { n _ { c } } \prod _ { j = 1 } ^ { n _ { y } } L _ { \beta } ( y _ { j } , c _ { i , j } ) ,
$$

$$
L _ { \beta } ( y _ { j } , c _ { i , j } ) = \frac { 1 } { 1 + \exp { ( \beta \cdot ( y _ { j } - c _ { i , j } ) ) } } ,
$$

$c _ { i } = ( c _ { i , 1 } , c _ { i , 2 } , . . . , c _ { i , n _ { y } } )$ . , ci,ny ) denotes the ith center for one logistic function, $\beta$ denotes a parameter that controls the steepness of the logistic function, and $n _ { c }$ denotes the number of centers. The logistic function $L _ { \beta } ( y _ { j } , c _ { i , j } )$ approximates the step function and enforces dominance for each objective $y _ { j }$ , as seen in the red dashed lines in Fig. 1b, and the product aggregates this approximation for all objectives. Furthermore, the sum of logistic function products preserve dominance in the objective space. Indeed, for every $\bar { \pmb y }$ that dominates user query $\boldsymbol { c } _ { i }$ , PDUF will express user preference with $u ( \bar { \pmb y } ) > u ( \pmb { c } _ { i } )$ . Finally, the centers define the parameters $\theta _ { u }$ that ensure the utility function adheres to the preference integration property by aligning them along an arbitrary line (the grey line in Fig. 1b).

# 6 Experiments

We empirically validate the proposed PUB-MOBO algorithm on 6 benchmark problems and report the performance of utility regret, $R$ , and distance to Pareto-front, $d _ { \mathrm { P a r e t o } }$ , against outcome evaluations and user queries

$$
R = \frac { u ( f ( x ^ { * } ) ) - u ( f ( x ) ) } { u ( f ( x ^ { * } ) ) } ,
$$

$$
d _ { \mathrm { P a r e t o } } = \operatorname* { m i n } _ { x _ { \mathrm { P a r e t o } } \in \mathbb { X } _ { \mathrm { P a r e t o } } } \left\| f ( x ) - f ( x _ { \mathrm { P a r e t o } } ) \right\| _ { 2 } ^ { 2 } .
$$

The problems are chosen from synthetic and real-world problems where the Pareto-front is known, so we can evaluate PUB-MOBO in the performance metrics $R$ and $d _ { \mathrm { P a r e t o } }$ . All experimental results are obtained using a 13th Gen IntelCore i7-13620H repeated across 20 seeds with hyperparameters $n _ { G D } = 1 0$ , $n _ { G I } = 1 , \varepsilon = 0 . 1$ . In the experiments, we investigate two aspects of PUB-MOBO: (a) the relevance of using gradient information and (b) the trade-off between cost of outcome evaluations and gradient uncertainty reduction in the GD stage. We compare against the s.o.t.a. preferencebased MOBO method (Lin et al. 2022) and two variations of PUB-MOBO:

• EUBO $+$ qEIUU is a 2-stage algorithm proposed in (Lin et al. 2022) which only contains the PE and EXP stages. The acquisition functions used are EUBO and qEIUU in each stage, exactly like in PUB-MOBO. This serves as a baseline of the performance when no gradients are used. • PUB-MOBO-PG is a PUB-MOBO ablation that relies solely on predicted gradients (PG) in the GD stage, omitting both outcome evaluations and GI optimizations. This makes it relatively inexpensive, but it ignores the fact that additional samples can yield useful derivative information. This ascertains whether the gradient uncertainty needs to be accounted for at all, and how important the GI step is.

![](images/1ce9d958d7320f881516c18e65c40561c0e59627f715752d4ea82b0c9534c47e.jpg)  
Figure 2: Median and 25-75 percentiles performance comparison on synthetic benchmarks on 20 random seeds with a 100 outcome evaluation budget. Plot titles indicate MOO benchmark; horizontal axis is either number of outcome evaluations or number of user queries. Vertical axis indicates simple regret of utility (user satisfaction) and distance from the Pareto-front (optimality).

• PUB-MOBO- $\mathrm { \cdot P G + O E }$ is a PUB-MOBO ablation that uses the predicted gradients as in PUB-MOBO-PG, but an Outcome Evaluation (OE) is performed at every gradient descent step in an effort to lower gradient uncertainty around observed points. This further checks whether the expensive GI optimization is needed to improve convergence, or if outcome evaluations are sufficient. • PUB-MOBO is the proposed method from Alg.1-2 with up to $n _ { \mathrm { G D } } ( n _ { \mathrm { G I } } + 1 )$ outcome evaluations in the GD stage.

# 6.1 Synthetic benchmarks

We examine 3 synthetic problems that are commonly found in MOO literature: DTLZ1, DTLZ2 (Deb et al. 2005), and DH1 (Deb and Gupta 2005). The results are displayed in Fig. 2 in order of increasing $n _ { x }$ : DTLZ2 $\mathbf { \zeta } _ { n _ { x } } = 8$ , $n _ { f } = 2$ ), DTLZ1 $( n _ { x } = 9$ , $n _ { f } = 2 \rangle$ ), DH1 $\mathit { i n } _ { x } = 1 0$ , $n _ { f } = 2 \$ ).

EUBO $+$ qEIUU is the poorest-performing algorithm across all the synthetic benchmarks, affirming the effectiveness of the additional stage based on local gradient search. The strategy of exploring locally dominating solutions off of the solution proposed by the utility maximization stages, not only results in solutions that are closer to the Pareto-front but also achieves lower utility regret. However, PUB-MOBO-PG performs equally poor in most of the synthetic benchmarks, largely due to inaccurate gradient estimation obtained with the surrogate model $\hat { \pmb f }$ by (4b). We frequently observe that the evolution of $x _ { \mathrm { G D } }$ in the GD stage is prematurely terminated either due to constraint violations in $\scriptstyle { \mathbf { { \vec { x } } } }$ or because the Frank-Wolfe algorithm results in $\left\| \alpha ^ { \top } \pmb { \mu } ^ { \nabla } ( \pmb { x } _ { \mathrm { G D } } ) \right\| _ { 2 } ^ { 2 } \le \underline { { \varepsilon } } _ { \mathrm { G D } } ,$ . This occurs not because the algor
ithm has reach
ed a Pareto-optimal point, but rather due to erroneous gradient estimates. These findings underscore the critical importance of accurate gradient estimation and minimizing gradient uncertainty for the success of PUB-MOBO. The $\mathrm { P G + O E }$ variant significantly outperforms the PG variant in most experiments. Although evaluating the true outcome function at each $x _ { \mathrm { G D } }$ incurs additional computational cost, the increased accuracy in gradient estimation from updating the outcome model more than justifies the expense. This leads to a more efficient algorithm overall, as reflected by lower utility regret and closer proximity to the Pareto-front, both in terms of outcome evaluations and user queries. In a similar trend, PUB-MOBO further improves upon $\mathrm { P G + O E }$

Vehicle Safety Vehicle Safety Vehicle Safety Vehicle Safety 0.20 0.100 0.100 0.3 0.15 0.075 0.075 0.2 0.10 R0.050 R0.050 Gr 0.025 0.025 0.1 0.05 0.000 0.000 0.0 0.00 20406080100 20406080100 20406080100 20.406080100 Outcome evaluations User queries Outcome evaluations User queries Conceptual Marine Design Conceptual Marine Design Conceptual Marine Design Conceptual Marine Design 0.15 0.15 0.2 R0.10 R0.10 04 0.05 d0.2 0.05 0.00 0.0 0.0 20406080100 20.406080100 20 406080100 20.406080100 Outcomeevaluations User queries Outcome evaluations User queries Car Side Impact Car Side Impact Car Side Impact Car Side Impact 0.08 0.08 0.100 0.06 0.06 0 80.075 R R0.04 0.050 0.04 9 0.02 0.025 0.02 0.05 0.00 0.000 2040 60 80100 20406080100 20 40 60 80100 20406080100 Outcome evaluations User queries Outcome evaluations User queries EUBO+qEIUU PUB-MOBO-PG PUB-MOBO-PG+OE PUB-MOBO

variant. The reduction of the gradient uncertainty obtained by evaluating the $\scriptstyle \mathbf { x } _ { \mathrm { G D } }$ suggested by the GI acquisition function, further improve the gradient estimate yielding lower $R$ and $d _ { \mathrm { P a r e t o } }$ . This empirically demonstrates that the additional outcome evaluations in the GD stage are justified.

# 6.2 Real-world benchmarks

We examine 3 problems based on real MOO problems: Vehicle Safety (Liao et al. 2008), Conceptual Marine Design (Parsons and Scott 2004), and Car Side Impact (Jain and Deb 2013). The implementations of these problems are taken from (Tanabe and Ishibuchi 2020). The results are shown in Fig. 3 in order of increasing $n _ { x }$ : Vehicle Safety $( n _ { x } = 5$ , $\begin{array} { r } { n _ { f } = 3 ; } \end{array}$ ), Conceptual Marine Design $\mathit { n } _ { x } = 6$ , ${ { n } _ { f } } = 4$ ), Car Side Impact $( n _ { x } = 7$ , $n _ { f } = 4 \AA$ ).

Similar to the results on the synthetic benchmarks, the EUBO and PUB-MOBO-PG methods are the worstperforming. The real-world benchmarks present a more complex optimization landscape, which increases the difficulty of accurate gradient estimation. Notably, PUBMOBO- $\mathrm { \cdot P G + O E }$ , which performed better on synthetic problems, exhibits similarly poor performance to EUBO and PUB-MOBO-PG on these more challenging benchmarks. In contrast, PUB-MOBO continues to outperform all other methods, consistently delivering near-optimal solutions with respect to both utility regret and proximity to the Paretofront, due to its effective gradient uncertainty minimization.

The results from both synthetic and real-world experiments demonstrate that incorporating a gradient descent stage in utility-based MOBO leads to solutions with lower utility regret and closer proximity to the Pareto-front, all while requiring fewer outcome evaluations and user queries. Empirical evidence suggests that it is more effective to conduct additional outcome evaluations during each GD stage to achieve accurate gradient steps, as seen in PUB-MOBO, rather than opting for cheaper but less accurate steps.

# 7 Conclusion

We introduce PUB-MOBO, a sample-efficient multiobjective Bayesian optimization method that integrates user preferences with gradient-based search to find near-Paretooptimal solutions. Across synthetic and real problems, it achieves high utility and reduced distance to Pareto-front solutions, highlighting the importance of reducing gradient uncertainty in the gradient-based search. Moreover, we introduce a new utility function, PDUF, that respects dominance and models different user preferences. An avenue of future research is to apply the Local Gradient Descent method to other MOBO algorithms.