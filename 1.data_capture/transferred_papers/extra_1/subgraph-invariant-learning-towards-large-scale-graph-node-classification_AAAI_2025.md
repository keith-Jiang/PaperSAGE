# Subgraph Invariant Learning Towards Large-Scale Graph Node Classification

Leilei Wang1,2, Si Shi1\*, Fei $\mathbf { M } \mathbf { a } ^ { 1 }$ , Fei Richard $\mathbf { Y } \mathbf { u } ^ { 2 , 3 * }$ , Pengteng $\mathbf { L i } ^ { 4 }$ , Ying Tiffany $\mathbf { H e } ^ { 2 }$

1Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China 2College of Computer Science and Software Engineering, Shenzhen University, China 3School of Information Technology, Carleton University, Canada 4HKUST (GZ), AI Thrust {wangleilei, shisi, mafei}@gml.ac.cn, {yufei,heying} $@$ szu.edu.cn, pengteng.li $@$ connect.hkust-gz.edu.cn

# Abstract

Graph Neural Networks (GNNs) have shown efficacy in graph node classification, but face computational challenges on large-scale graphs. Although existing graph reduction methods address these issues, they still require high computational resources and fail to prioritize robust performance on out-of-distribution data. To tackle these challenges, we introduce the subgraph invariant learning paradigm, inspired by the small-world phenomenon. This approach enables models trained on specific subgraphs to generalize across diverse subgraphs, reducing computational demands, and enhancing scalability. To promote generalization, we maximize the invariance log-likelihood by deriving a theoretical lower bound of it and formulating the InVar loss. This loss minimizes the discrepancy between node representations and their corresponding invariance representations while maximizing the entropy of the node representation. In response to InVar loss, we propose the Invariance Facilitation Model (IFM), comprising the Invariance Representation Encoder (IRE) and Node Representation Encoder (NRE). IRE, capturing the invariance representations, utilizes Invariance ATTention (InvarATT) to compress long-range dependencies, while NRE learns the node representation, by integrating invariance representations via Telematic ATTention (TeleATT) and exchanging local information within each subgraph through GNNs. Evaluations on four large-scale graph datasets demonstrate the effectiveness, computational efficiency, and interpretability of IFM for large-scale graph node classification.

Code — https://github.com/wleilei/SIL-IFM

# Introduction

Graph structures are prevalent in complex systems across diverse domains, including social, biological, transportation, and communication networks. The widespread use of graphstructured data has driven the adoption of Graph Neural Networks (GNNs) (Kipf and Welling 2017; Velicˇkovic´ et al. 2018), significantly enhancing the modeling and analysis capabilities in various fields. However, the high computational demands of modeling large-scale graphs limit the deployment of GNNs, preventing their full potential from being realized in real-world applications.

A natural solution is graph reduction learning, which includes graph sparsification, graph coarsening, and graph condensation (Hashemi et al. 2024). These methods create a smaller graph that can be used during testing to achieve performance comparable to the original large-scale graph. However, transforming a large-scale graph into a smaller one faces three limitations. First, high computational resources are still required during the transformation process, as the large-scale graph remains necessary during training. Second, the smaller graph often fails to generalize to out-ofdistribution graphs, since the goal of these methods is to match the performance of the large graph rather than to enhance generalization. Third, obtaining a large number of labeled nodes that represent the entire graph is often impractical, and in many cases, only a small subset of labeled nodes is available during training.

To address these limitations, we suppose that a largescale graph can be viewed as an aggregation of numerous small-scale subgraphs, consistent with the small-world paradigm (Watts and Strogatz 1998; Kleinberg 2000). In this paradigm, nodes within each subgraph are densely interconnected, while connections between subgraphs are relatively sparse. Despite the potential differences among these subgraphs, it is possible to use models robust enough to generalize from individual subgraphs to the overall graph structure, guided by the principle of invariance (Jia et al. 2024; Ahuja et al. 2021). This principle suggests that capturing invariant information across varying conditions requires only a subset of the available data, and this invariance can be generalized across the entire graph. Based on this, we propose subgraph invariant learning, which enables the model to be trained on labeled subgraphs and tested on unlabeled subgraphs by extrapolating invariance from certain subgraphs to others with superior performance. Since models in subgraph invariant learning are applied exclusively to individual subgraphs, without needing the entire graph during either the training or testing phases, it offers two key advantages: a substantial reduction in computational resource consumption and the ability to increase model complexity due to the smaller scale of the subgraphs.

To ensure that invariance is learned effectively, we aim to maximize the log-likelihood of the invariance and derive its lower bound. Consequently, the InVariance (InVar) loss is strategically proposed as a proxy objective to maximize this lower bound. This loss function serves two purposes: minimizing the distance between a node’s representation and its same-labeled invariant representation, while simultaneously maximizing the entropy of node representations to encourage feature diversity. Based on the InVar loss, we develop the corresponding Invariance Facilitation Model (IFM), comprising the Invariance Representation Encoder (IRE) that captures shared invariant features among nodes with the same label and the Node Representation Encoder (NRE) that provides each node with the necessary information to predict its label accurately.

IRE learns a unique invariance representation from nodes with the same label, ensuring this invariant representation encapsulates sufficient label-related information. As this information comes from nodes across the entire graph, including those at considerable distances, IRE can be considered as a compressor of long-range dependencies. In traditional GNNs, over-squashing (Topping et al. 2022; Di Giovanni et al. 2023) or over-globalizing (Xing et al. 2024) may occur due to information loss when compressing sparsely connected graph data to capture long-range dependencies across a wide receptive field. IFM addresses this issue by inherently limiting the scope of information propagation to dense subgraphs, with the long-range dependencies being captured by the Invariance ATTention (InvarATT) in IRE.

NRE equips each node with sufficient information to accurately predict its label. NRE consists of two modules: Telematic ATTention (TeleATT) and GNN. TeleATT enables nodes to extract global invariant information from the representations learned by IRE, while GNN facilitates the recursive exchange of structural information between adjacent nodes, fostering local invariance within each subgraph. Notably, IFM is adaptable to any base GNN model, enhancing the ability to capture long-range dependencies with global invariance. During inference, only NRE is required for the prediction with the learned invariant representations.

We conducted extensive experiments on two synthetic datasets and two real-world datasets to validate the effectiveness of InVar loss and IFM. Additionally, we performed visualization experiments to verify the invariance property and the explainability of the node representations. Our Contributions are summarized as follows:

• To the best of our knowledge, we are the first to develop subgraph invariant learning for large-scale graphs. This approach, inspired by the small-world phenomenon, emphasizes training on labeled subgraphs and generalizing to unlabeled ones with low computational costs. • Based on the invariance principle, we derive the lower bound of the log-likelihood of invariance and introduce the InVar loss as a proxy objective for maximizing this bound. We subsequently design the IFM architecture, incorporating IRE and NRE, to minimize the InVar loss. • Comprehensive experiments are conducted on four largescale graph datasets. Our proposed method has demonstrated its effectiveness, efficiency, and interpretability through the empirical results.

# Related Works

Graph Reduction Learning. To accelerate GNN models and enhance data processing efficiency, graph reduction learning simplifies large-scale graphs into more manageable small-scale counterparts while preserving the core structure (Hashemi et al. 2024). These methods can be categorized into three strategies: graph sparsification, graph coarsening, and graph condensation. Graph sparsification (Yu et al. 2022; Li et al. 2023) focuses on extracting a subset of essential nodes and edges. Graph coarsening (Taghibakhshi et al. 2021; Kumar et al. 2023) aims to aggregate raw nodes and edges into super nodes and super edges. Both of them preserve graph information without directly considering model performance. On the other hand, graph condensation (Zhang et al. 2024; Zheng et al. 2023) is designed to synthesize a small-scale graph that allows models trained on it to achieve results comparable to those trained on the original large-scale graphs (Gao et al. 2024). However, these graph reduction methods often involve high computational costs when transforming large-scale graphs into smaller ones, and face limitations in generalization capability as their primary goal is not to capture invariance across the entire graph. Additionally, a large number of labeled nodes are required during the training stage to produce the smaller graph.

Graph Invariant Learning. Graph invariant learning focuses on learning the invariance of distribution-shift graphs to enhance out-of-distribution generalization (Gui et al. 2022; Chen et al. 2023). Building on this foundation, recent works (Li et al. 2022; Yue et al. 2024) have introduced various methods to address feature and structure distribution shifts. However, these methods primarily address graphs in different environments and do not consider the issue of subgraph invariance within large-scale graphs. Although some studies (Li et al. 2022; Jia et al. 2024) have explored distribution shift of subgraphs, their main goal is to identify or extract subgraph environments for subsequent generalization, which still entails high training and testing costs on large-scale graphs. Alternatively, other methods (Buffelli, Lió, and Vandin 2022; Huang et al. 2024) aim to extrapolate invariance learned from a minor subgraph to its entire large-scale graph, but they may still incur significant computational demands due to the need to process the entire graph during model inference. Substantially, graph invariant learning either ignores distribution shifts across subgraphs or requires high computational costs when dealing with largescale graphs.

# Notations and Problem Formulation

Notations. A large-scale graph is supposed to be partitioned into a set of small-scale subgraphs, denoted as $\mathcal { G } =$ $\{ G _ { 1 } , \dots , G _ { m } \}$ , where $m$ is the number of subgraphs. For each subgraph $G _ { i } = \{ X _ { i } , A _ { i } , Y _ { i } \}$ , $X _ { i } \in \mathbb { R } ^ { N _ { i } \times \smile }$ denotes the $N _ { i }$ numbers of nodes with $d _ { i n }$ -dimensional features, $A _ { i }$ $\in \mathbb { R } ^ { N _ { i } \times N _ { i } }$ denotes the adjacency matrix, and $Y _ { i } \in \mathbb { R } ^ { N _ { i } \times C }$ denotes the $C$ -classes of node labels. $S ^ { c }$ are the $c$ -labeled nodes in the training subgraphs.

Problem Formulation. Denoted $\mathbb { G }$ and $\mathbb { Y }$ as the graph and label space, we formulate subgraph invariant learning over the small-scale subgraphs of a large-scale graph as:

Problem 1. The goal of subgraph invariant learning is the finding of an optimal predictor $\mathcal { P } ^ { * } ( \cdot ) : \mathbb { G } \to \mathbb { Y }$ that performs well on all subgraphs:

$$
\mathcal { P } ^ { * } ( \cdot ) = a r g m i n \ \underline s u p \ \mathcal { R } ( \mathcal { P } | G _ { i } )
$$

where $\mathcal { R } ( \mathcal { P } | G _ { i } ) = \mathbb { E } _ { G _ { i } } [ \mathcal { L } ( \mathcal { P } ( G _ { i } ) , Y _ { i } ) ]$ is the expected risk of the predictor $\mathcal { P }$ on the subgraph $G _ { i }$ , and $\mathcal { L } ( \cdot , \cdot ) : \mathbb { Y } \to \mathbb { Y }$ is the loss function. Additionally, we can decompose $\mathcal { P } ( \cdot ) =$ $\mathcal { C } \circ \mathcal { F }$ , where $\mathcal { F } ( \cdot ) : \mathbb { G }  \mathbb { R } ^ { d _ { h } }$ is the feature representation learning function and $\mathcal { C } ( \cdot ) : \mathbb { R } ^ { d _ { h } }  \bar { \mathbb { Y } }$ is the classifier.

Subgraphs $\mathcal { G }$ within the large-scale graph exhibit shifts in both feature and structure distributions due to their origins in diverse environments. These distribution shifts pose significant challenges in addressing Problem 1 as they impede the effective learning of valid feature representations.

To acquire robust feature representations, we posit that each node in a large-scale graph contains invariant information that maintains a consistent relationship with its label across various subgraphs, adhering to the principle of invariance (Jia et al. 2024; Ahuja et al. 2021). By effectively identifying and leveraging this invariance, a model can achieve strong out-of-distribution (OOD) performance. We formalize this assumption as follows:

Assumption 1. Learned from the training set of smallscale subgraphs $\mathscr { G } _ { t r a i n }$ , the optimal invarianct feature encoder $\mathcal { F } ^ { * } ( \cdot )$ does exist, satisfying:

• Invariance property: For any $G _ { i }$ and $G _ { j }$ , if $\hat { X } _ { i } ^ { c } \ =$ $\mathcal { F } ^ { * } ( X _ { i } , A _ { i } )$ and $\hat { X } _ { j } ^ { c } = \mathcal { F } ^ { * } ( X _ { j } , A _ { j } )$ , then $\hat { X } _ { i } ^ { c } \ = \ \hat { X } _ { j } ^ { c }$ , where $\hat { X } ^ { c }$ denotes $c$ -labeled node representations. • Sufficiency property: a predictor $\mathcal { C } ^ { * } ( \cdot )$ can be found and satisfies $Y _ { i } = \mathcal { C } ^ { * } ( \mathcal { F } ^ { * } ( X _ { i } , A _ { i } ) ) + \epsilon$ , ϵ $\perp { \mathcal { F } } ^ { * } ( X _ { i } , A _ { i } )$ , where $\perp$ indicates statistical independence, and $\epsilon$ is random noise.

The invariance property follows with the invariance principle in contrastive learning and OOD methods, by ensuring that the node representations remain robust across different conditions. It posits that the same-labeled node representations should be same even in different subgraphs and differ from different-labeled node representations. The sufficiency property ensures that the invariant representations can be effectively utilized by a simple classifier to predict node labels. Consequently, the problem of cultivating optimal $\mathcal { F } ( \cdot )$ and $ { \mathcal { C } } ( \cdot )$ arises, which can be formulated as:

Problem 2. Given a set of small-scale subgraphs $\mathscr { G } _ { t r a i n }$ , the task is designed to jointly capture the invariance of node representations among all subgraphs using $\mathcal { F } ^ { * } ( \cdot )$ and learn a predictor $\mathscr { C } ^ { * } ( \cdot )$ with the node representations, thereby achieving robust OOD generalization performance.

# Methodology

In this section, we first introduce the lower bound of the loglikelihood of invariance and develop the corresponding InVar loss as the proxy objective for maximizing it. Then we develop IFM to learn the invariance representations and the node representations as required in InVar loss. Finally, we compare IFM with traditional GNN models, focusing on differences in time complexity, model capacity, explainability and sparsity.

# Invariance Loss

Under Assumption 1, the invariant representation ${ \hat { x } } ^ { c }$ exists within the observed $c$ -labeled node samples $\boldsymbol { s } ^ { c }$ . Following the likelihood-based approach, ${ \hat { x } } ^ { c }$ can be derived by maximizing the invariance likelihood $p ( \hat { x } ^ { c } )$ through optimizing the feature encoder $\mathcal { F } ( \cdot )$ . In the following theorems, we derive the lower bound of the log-likelihood $\log p ( \hat { x } ^ { c } )$ and then decompose this lower bound into three terms with realistic meanings. Based on the decomposed terms, the InVar loss has been developed as the proxy objective of maximizing the lower bound.

Theorem 1. The lower bound of log-likelihood $\log p ( \hat { x } ^ { c } )$ is Eqϕ(xc)[log pq(x(c,xˆcc) ].

Proof. In order to obtain the lower bound of the loglikelihood $\log p ( \hat { x } ^ { c } )$ , we can decompose $\log p ( \hat { x } ^ { c } )$ as follows:

$$
\begin{array} { r l r } { \log \psi ( \hat { x } ^ { c } ) = \log p ( \hat { x } ^ { c } ) \int q _ { 6 } ( x ^ { c } ) \ d x ^ { c } , } & { \mathrm { ( 2 ) } } \\ { \ } & { \ } & { \mathrm { ( 3 ) } } \\ & { \ } & { \mathrm { = } \log _ { 4 } ( x ^ { c } ) \log p ( \hat { x } ^ { c } ) \ d x ^ { c } , \ } & { \mathrm { ( 3 ) } } \\ & { \ } & { \mathrm { ( 4 ) } } \\ & { \ } & { \mathrm { = } \mathbb { E } _ { q _ { 6 } ( x ^ { c } ) } \left[ \log p ( \hat { x } ^ { c } ) \right] , \ } & { \mathrm { ( 4 ) } } \\ & { \ } & { \mathrm { ( 5 ) } } \\ & { \ } & { \mathrm { = } \mathbb { E } _ { q _ { 6 } ( x ^ { c } ) } \left[ \log \frac { \psi ( x ^ { c } ) ^ { \hat { x } ^ { c } } \hat { y } ^ { \hat { y } } q _ { 6 } ( x ^ { c } ) } { \hat { y } ( x ^ { c } ) \hat { x } ^ { \hat { y } } q _ { 6 } ( x ^ { c } ) } \right] , \ } \\ & { \ } & { \mathrm { ( 5 ) } } \\ & { \ } & { \ = \mathbb { E } _ { q _ { 6 } ( x ^ { c } ) } \left[ \log \frac { p ( x ^ { c } ) ^ { \hat { x } ^ { c } } } { q _ { 6 } ( x ^ { c } ) } \right] + \mathbb { E } _ { q _ { 6 } ( x ^ { c } ) } \left[ \log \frac { q _ { 6 } ( x ^ { c } ) } { p ( x ^ { c } ) \hat { x } ^ { c } } \right] , \ } \\ & { \ } & { \mathrm { ( 6 ) } } \\ & { \ } & { \ = \mathbb { E } _ { q _ { 6 } ( x ^ { c } ) } \left[ \log \frac { p ( x ^ { c } , \hat { x } ^ { c } ) } { q _ { 6 } ( x ^ { c } ) } \right] + D _ { K L } ( q _ { 6 } ( x ^ { c } ) | | p ( x ^ { c } | \hat { x } ^ { c } ) | . } \end{array}
$$

In this equation, $q _ { \phi } ( x ^ { c } )$ can be treated as NRE for transforming the node features into a node embedding space. Since the KL Divergence is always non-negative, we can infer that Eqϕ(xc) hlog pq(x(c,xˆcc)) i is the lower bound of $\log p ( \hat { x } ^ { c } )$ , i.e. $\begin{array} { r } { \log p ( \hat { x } ^ { c } ) \geq \mathbb { E } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log \frac { p ( x ^ { c } , \hat { x } ^ { c } ) } { q _ { \phi } ( x ^ { c } ) } \right] } \end{array}$ . Then, maximizing this lower bound becomes a proxy objective of capturing the invariance $\hat { \boldsymbol x } ^ { c }$ .

Theorem 2. Minimizing the InVar loss $\mathcal { L }$ is the maximization of the lower bound $\begin{array} { r } { \bar { \mathbb { E } } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log \frac { p ( x ^ { c } , \hat { x } ^ { c } ) } { q _ { \phi } ( x ^ { c } ) } \right] } \end{array}$

Proof. By splitting the numerator, the lower bound can be rewritten as:

$$
\mathbb { E } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log \frac { p ( x ^ { c } , \hat { x } ^ { c } ) } { q _ { \phi } ( x ^ { c } ) } \right] = \mathbb { E } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log \frac { p _ { \theta } ( \hat { x } ^ { c } | x ^ { c } ) p ( x ^ { c } ) } { q _ { \phi } ( x ^ { c } ) } \right] .
$$

In this formula, $p _ { \theta } ( \hat { x } ^ { c } | x ^ { c } )$ can be considered as IRE for unearthing the invariant features from the node features. Subsequently, we can further decompose this lower bound into three terms: $\mathbb { E } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log p _ { \theta } \big ( \hat { x } ^ { c } \big | x ^ { \bar { c } } \big ) \right] , \mathbb { E } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log p ( x ^ { c } ) \right]$ and $- \mathbb { E } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log q _ { \phi } ( x ^ { c } ) \right]$ . All three terms of the decomposed lower bound have distinct purposes: the first term $\mathbb { E } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log p _ { \theta } \big ( \hat { x } ^ { c } | x ^ { c } \big ) \right]$ measures the similarity between the invariant features and the node embeddings, ensuring that the invariant representations align closely with the actual node data. The second term $\mathbb { E } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log p ( \overline { { x } } ^ { c } ) \right]$ assesses how well the node embeddings correspond to their respective labels, emphasizing the accuracy of label prediction. The third term $- \mathbb { E } _ { q _ { \phi } ( x ^ { c } ) } \left[ \log q _ { \phi } ( x ^ { c } ) \right]$ represents the entropy, which encourages diversity in the features, promoting a richer and more robust feature space.

![](images/089ab871886c2bdf35589887ce5ca37dcf8dc7f7fc24285123c3af4b8ca8ffb9.jpg)  
Figure 1: The overall architecture of IFM. Subgraph invariant learning partitions a large-scale graph into numerous subgraphs, aligning with the small-world phenomenon. IFM comprises two key components: IRE for invariance representation learning and NRE for node representation learning. (1) IRE employs InvarATT as a long-range dependency compressor to capture invariance representations from given node samples. (2) NRE leverages TeleATT to extract global invariance from these representations and utilizes GNN within subgraphs to foster local invariance, equipping nodes with the necessary invariance for label prediction.

Considering that each node embedding can be utilized for predicting its label through the invariance representations, we can apply InfoNCE (van den Oord, Li, and Vinyals 2019), a contrastive loss function, to simultaneously address the first two terms of the lower bound in Theorem 2. The InVar loss can then be developed as follows:

$$
\begin{array} { l } { \displaystyle \mathcal { L } = \mathcal { L } _ { I } - \lambda \mathcal { L } _ { E } , } \\ { \displaystyle \ = - \log \frac { \exp ( \sin ( x , \hat { x } ^ { + } ) / \tau ) } { \displaystyle \sum _ { k = 1 } ^ { C } \exp ( \sin ( x , \hat { x } ^ { k } ) / \tau ) } - \lambda \sum _ { i = 1 } ^ { d } ( - x _ { i } \log x _ { i } ) . } \end{array}
$$

where the invariance representation $\hat { x } ^ { + }$ shares the same label with node $x \mathrm { ~ , ~ } \tau$ is the temperature coefficient, and $\lambda$ is the regularization weight. In addition, we normalize $x$ and interpret the resulting term as a probability distribution.

Specifically, minimizing $\mathscr { L } _ { I }$ encourages node representations to align more closely with their same-label invariance representations while remaining distinct from other invariance representations with different labels. This approach enforces the invariance property in Assumption 1 and enhances the explainability of predictions by bringing same-labeled node representations closer to their invariant counterparts. $\mathcal { L } _ { E }$ is to maximize the entropy of node representations and helps maintain valid, numerically balanced features. This process avoids trivializing certain features (e.g., being nearzero after normalization) and encourages the model to retain a broader variety of distinctive characteristics, reinforcing the sufficiency property as outlined in Assumption 1.

# Invariance Faciliation Model (IFM)

To implement the InVar loss, we propose the IFM architecture, as is shown in Figure 1. IFM consists of two main components: Invariance Representation Encoder (IRE) and Node Representation Encoder (NRE). The IRE, designed to capture invariant representations, is built from a stack of identical layers, each featuring an InvarATT sublayer and a feedforward network. Similarly, the NRE, responsible for learning node representations, is composed of a series of repeated layers. Each layer in the NRE includes a feed-forward network, TeleATT for modeling global invariance from the IRE output, and GNN for capturing local invariance from neighboring nodes. During inference, only the NRE and the invariant representations are needed, with the softmax term in $\mathscr { L } _ { I }$ acting as the prediction classifier.

Invariance Representation Encoder. We use the average of $c$ -labeled node features in $\mathcal { G } _ { t r a i n }$ as the initial invariance embedding $\hat { h } ^ { c } \in \mathbb { R } ^ { 1 \times d _ { h } }$ and iteratively refine this embedding by incorporating a fixed number of random $c$ -labeled samples $s ^ { c } \in \mathbb { R } ^ { k \times d _ { h } }$ from $S ^ { c }$ , where $S ^ { c } \in { \mathcal { S } }$ represents $c$ - labeled nodes in $\mathcal { G } _ { t r a i n }$ . Accordingly, IRE is implemented in a scalable, transformer-like architecture (Vaswani et al. 2023) using InvarATT, which is computed as:

$$
\mathrm { I n v a r A T T } = \mathrm { A t t e n t i o n } ( Q ( \hat { h } ^ { c } ) , K ( s ^ { c } ) , V ( s ^ { c } ) ) .
$$

InvarATT is an attention mechanism enabling the invariance representation $\hat { h } ^ { c }$ to assimilate information from samelabeled samples $s ^ { c }$ , capturing label-specific invariant features across the graph. Since the nodes $s ^ { c }$ are globally sampled from $G _ { t r a i n }$ , IRE can be viewed as a long-range dependency compressor, globally condensing invariance information into invariance representations, which are consistent across subgraphs yet specific to each label class.

Node Representation Encoder. With the invariance representations $\hat { H } \in \mathbb { R } ^ { C \times d _ { h } }$ from IRE, TeleATT is designed to help each node capture its same-labeled invariance through attention. To ensure training stability, we apply a stopgradient operation to $\hat { H }$ resulting in $\tilde { H }$ , which prevents individual nodes from influencing the invariance. Meanwhile, GNN is used to capture complex relationships and dependencies within each subgraph. After mapping node representations $X$ into $H \in \bar { \mathbb { R } ^ { N \times d _ { h } } }$ , NRE is implemented in a scalable, transformer-like architecture using a combination of TeleATT and GNN, where TeleATT is computed as:

$$
\mathrm { T e l e A T T } = \mathrm { A t t e n t i o n } ( Q ( H ) , K ( \tilde { H } ) , V ( \tilde { H } ) ) .
$$

Specifically, NRE is designed to equip node representations with sufficient label correlation through this dual mechanism: (1) capturing global invariance from IREgenerated invariance representations via TeleATT, and (2) fostering local invariance from neighboring nodes within the subgraph via GNN. By employing TeleATT, NRE enhances any base GNN model’s capacity to capture long-range dependencies using the compressed invariance representations. This approach effectively integrates both global and local graph structural information, resulting in more comprehensive and informative node representations.

# Comparisons

We compare our proposed IFM, which operates on individual small-scale subgraphs, with traditional GNN models that run on the entire large-scale graph.

Time Complexity. The time complexity of GNN models is $O ( n ^ { 2 } d )$ per layer, where $n$ is the number of nodes in the large-scale graph. In contrast, IFM operates on individual subgraphs, resulting in a time complexity of $O ( m s ^ { 2 } d )$ for the entire graph and $O ( s ^ { 2 } d )$ for a single subgraph, where $m$ represents the number of subgraphs and $s \ll n$ is the maximum subgraph size. Thus, IFM has significantly lower time complexity compared to GNNs.

Model Capacity. According to scaling laws (Kaplan et al. 2020), the model capacity is positively correlated with the number of parameters. However, constrained by the large scale of graph data, GNN models cannot scale up their parameters effectively and struggle to capture long-range dependencies due to the limited receptive field. In contrast,

IFM can scale up the number of parameters due to the smaller scale of subgraphs. IFM models long-range dependencies effectively by incorporating IRE as a long-range information compressor and using TeleATT to distribute longrange information to each node.

Explainability. To adhere to the invariance property in Assumption 1, IFM optimizes parameters to make the nodes more similar to their same-labeled invariance representations, guided by the InVar loss. This enhances explainability by bringing the $c$ -labeled nodes closer to a fixed embedding for prediction. Conversely, GNN models use MLP as the predictor and the CrossEntropy loss for parameter optimization, which results in a lack of explainability.

Sparsity. Irregular graphs or non-hierarchical may lack clear clustering but exhibiting sparsity, which can hinder information flow in traditional GNNs and lead to oversquashing issues. IFM addresses this by removing sparse edges to partition large-scale graphs into smaller subgraphs, improving scalability. Additionally, the NRE module employs IRE and TeleATT to provide nodes with sufficient long-range information for accurate label prediction. Consequently, IFM is suitable for various large-scale graph types, even those not exhibiting the small-world property.

# Experiments

In this section, we conduct the experiments on four largescale graph datasets to answer the following research questions: RQ1: Is IFM effective for enhancing the performance of GNNs in subgraph invariant learning? RQ2: Is it necessary to incorporate $\mathcal { L } _ { E }$ in the model training? RQ3: How sensitive is IFM to change in hyper-parameters? RQ4: Does IFM achieve the invariance property and the explainability?

# Experimental Setup

Datasets. We conduct experiments on four graph node classification datasets, including two synthetic datasets and two real-world datasets. For the synthetic datasets, we utilize the Reddit and Amazon large-scale graph datasets (Zeng et al. 2020), employing a clustering algorithm (Chiang et al. 2019) to partition each graph into subgraphs. The real-world datasets, PascalVOC and COCO, are chosen from the Long Range Graph Benchmark (Dwivedi et al. 2022), where each subgraph represents an image and each node corresponds to a specific image region. To rigorously assess model performance, we divide the subgraphs of each dataset into nonoverlapping training, validation, and testing subsets. Detailed dataset information is shown in Table 1.

Model Configurations. In NRE, GNN is used to aggregate information from subgraph nodes. Our method can be applied to various GNNs, including convolution-based and attention-based ones. For convolution-based GNNs, we use GCN (Kipf and Welling 2017), SAGE (Hamilton, Ying, and Leskovec 2017) (mean-pooling variant) and SGC (Wu et al. 2019). For attention-based GNNs, we incorporate GAT (Velicˇkovic´ et al. 2018), GATv2 (Brody, Alon, and Yahav 2022) and UniMP (Shi et al. 2021). These models serve as our baseline, and we integrate them with IFM to validate the efficacy of our proposed methodology.

able 1: Statistics of the four datasets. Note that the data splitting for Pascal VOC-SP and COCO-SP follows LRGB (Dwived t al. 2022), and labels with no nodes are removed from Amazon (Zeng et al. 2020).   

<html><body><table><tr><td>Dataset</td><td>Subgraphs</td><td>Total Nodes</td><td>Nodes (Mean ± Std)</td><td>Edges (Mean ± Std)</td><td>Classes</td><td>Train/Val/Test</td></tr><tr><td>Reddit</td><td>400</td><td>232,965</td><td>582 ± 16.39</td><td>64,464 ± 66,026.99</td><td>41</td><td>80/160/160</td></tr><tr><td>Amazon</td><td>2.000</td><td>1,569,960</td><td>784 ± 20.29</td><td>15,461 ± 43,458.24</td><td>75</td><td>400/800/800</td></tr><tr><td>PascalVOC</td><td>11,355</td><td>5,443,545</td><td>479 ± 16.52</td><td>2,710 ± 96.67</td><td>41</td><td>8,498/1,428/1,429</td></tr><tr><td>COCO</td><td>123,286</td><td>58,793,216</td><td>477 ± 15.75</td><td>2,694 ± 93.58</td><td>81</td><td>113,286/5,000/5,000</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2"></td><td colspan="2">Reddit</td><td colspan="2">Amazon</td><td colspan="2">PascalVOC</td><td colspan="2">COCO</td></tr><tr><td>AUC-ROC</td><td>F1</td><td>AUC-ROC</td><td>F1</td><td>AUC-ROC</td><td>F1</td><td>AUC-ROC</td><td>F1</td></tr><tr><td>GCN</td><td>84.74±14.99</td><td>79.79±26.43</td><td>97.08±0.41</td><td>70.11±1.85</td><td>84.05±14.44</td><td>70.16±20.68</td><td>84.79±16.32</td><td>71.96±22.82</td></tr><tr><td>GCN-IFM</td><td>88.28±11.20</td><td>79.87±26.03</td><td>97.05±0.42</td><td>70.75±1.83</td><td>84.56±14.27</td><td>70.32±20.26</td><td>85.07±16.45</td><td>72.18±23.35</td></tr><tr><td>SAGE</td><td>89.07±14.55</td><td>82.15±24.03</td><td>97.05±0.43</td><td>70.85±1.83</td><td>84.23±14.92</td><td>71.04±20.33</td><td>84.91±16.62</td><td>72.17±23.09</td></tr><tr><td>SAGE-IFM</td><td>91.18±0.03</td><td>82.31±23.89</td><td>97.11±0.41</td><td>71.06±1.71</td><td>84.41±14.73</td><td>71.08±20.28</td><td>85.85±16.26</td><td>73.64±21.77</td></tr><tr><td>SGC</td><td>88.77±9.77</td><td>81.33±25.00</td><td>97.07±0.41</td><td>70.21±1.87</td><td>83.12±15.16</td><td>69.10±21.32</td><td>85.19±16.56</td><td>72.66±22.55</td></tr><tr><td>SGC-IFM</td><td>89.97±9.05</td><td>81.68±25.16</td><td>97.10±0.41</td><td>71.05±1.76</td><td>83.74±14.92</td><td>70.06±20.84</td><td>85.28±16.28</td><td>72.86±22.72</td></tr><tr><td>GAT</td><td>82.81±13.74</td><td>78.16±27.67</td><td>97.03±0.43</td><td>69.96±1.81</td><td>83.90±14.74</td><td>70.70±20.44</td><td>85.24±16.54</td><td>73.04±22.33</td></tr><tr><td>GAT-IFM</td><td>87.40±14.12</td><td>79.04±26.45</td><td>97.03±0.42</td><td>70.55±1.82</td><td>84.60±14.54</td><td>71.14±20.84</td><td>85.34±16.42</td><td>73.13±22.50</td></tr><tr><td>GATv2</td><td>84.43±17.23</td><td>78.79±27.51</td><td>96.99±0.43</td><td>69.50±1.99</td><td>83.91±15.02</td><td>70.53±20.93</td><td>85.19±16.47</td><td>72.63±22.78</td></tr><tr><td>GATv2-IFM</td><td>86.89±12.93</td><td>79.07±26.13</td><td>97.04±0.41</td><td>70.36±1.79</td><td>84.55±14.48</td><td>71.34±20.42</td><td>86.01±16.32</td><td>74.19±21.65</td></tr><tr><td>UniMP</td><td>87.74±15.92</td><td>78.97±28.74</td><td>97.15±0.40</td><td>70.41±1.78</td><td>85.79±14.30</td><td>72.74±20.05</td><td>85.94±16.32</td><td>73.36±22.84</td></tr><tr><td>UniMP-IFM</td><td>89.88±10.85</td><td>80.95±24.35</td><td>97.06±0.42</td><td>70.50±1.83</td><td>85.89±14.30</td><td>72.88±19.97</td><td>86.47±16.15</td><td>74.53±21.73</td></tr></table></body></html>

Table 2: Results on four datasets demonstrating the performance enhancement of traditional GNN models when augmented with InVar loss and IFM. The hidden size parameter is set to 256, 128, 32, and 64 for each dataset, respectively.

Evaluation and Implementation Details. We employ weighted AUC-ROC score and weighted macro F1 score as evaluation metrics. Since subgraph invariant learning aims for robust performance across subgraphs, we calculate the mean and standard deviation of these metrics across testing subgraphs, rather than globally for the entire test set. This approach provides a nuanced understanding of model performance and stability across different subgraph structures. All the experiments are conducted on a single RTX 3090 GPU, as models are trained on limited subgraphs instead of the entire graph. For fair comparison, all comparative models are trained under identical conditions, with both IRE and NRE configured with 12 layers. The parameters for predictive analysis are chosen based on the epoch with the best validation performance for the weighted macro F1 score.

# Main Results (RQ1)

Table 2 shows the performance improvements achieved by incorporating IFM into various GNN models. Our method enhances the performance of base GNN models across both synthetic and real-world datasets.

For synthetic datasets, the improvements are statistically significant. On the Reddit dataset, GCN-IFM increases GCN’s weighted AUC-ROC score from 84.74 to 88.28, while GAT-IFM improves GAT’s weighted F1 score from 78.16 to 79.04. These enhancements also reduce standard deviations, suggesting that our IFM effectively captures invariance across subgraphs with complex structural shifts, leading to better generalization.

The effectiveness of IFM is also evident in real-world datasets, showing improved performance across all base GNN models. This enhancement underscores IFM’s ability to capture long-range dependencies in node representations. The importance of this capability is further highlighted by using datasets from the Long Range Benchmark (Dwivedi et al. 2022), which is specifically designed to assess performance in capturing long-range dependencies.

Across the four datasets, performance variations among IFM implementations with different GNN models highlight the impact of GNN architectures in modeling local invariance by enabling local information exchange and capturing structural properties. Besides, standalone GNN models underperform compared to those incorporating IRE, TeleATT, and InVar loss, testifying the improved out-of-distribution generalization achieved by the global invariance.

# Ablation Study (RQ2)

We conduct ablation experiments to assess the impact of the entropy loss $\mathcal { L } _ { E }$ . The results in Table 3 show that adding $\mathcal { L } _ { E }$ to the InVar loss leads to improved performance. This suggests that leveraging the lower bound of invariance enhances model performance. Specifically, our experiments demonstrate that maximizing node representation entropy improves prediction ability. This is likely because entropy maximization enriches the feature space, enabling the model to explore a wider range of features. These enriched features help node representations better satisfy the sufficiency property outlined in Assumption 1, thereby contributing to enhanced generalization ability.

Table 3: Ablation of the entropy component on Reddit, with the hidden size parameter set to 16 for a clear comparison.   

<html><body><table><tr><td>Model</td><td>Loss</td><td>AUC-ROC</td><td>F1</td></tr><tr><td rowspan="2">GCN-IFM</td><td></td><td></td><td>71.79±29.57</td></tr><tr><td>WOEE</td><td>85.5±9.88</td><td>73.61±28.75</td></tr><tr><td rowspan="2">GAT-IFM</td><td>WEE</td><td>84.83±10.42</td><td>68.02±29.71</td></tr><tr><td></td><td>85.28±11.12</td><td>71.07±29.56</td></tr></table></body></html>

![](images/982c4ed0e16886fb4685ac95a43af7fbf5f2163cef32112321c1697c5bc3b154.jpg)  
Figure 2: Visualizations of node representations, which are the outputs of each model for prediction on Reddit. The upper two figures visualize two randomly selected attributes from node representations with the same labels. The lower two figures compare the distributions of node representations using t-SNE, with color indicating shared labels.

# Hyper-parameter Sensitivity Analysis (RQ3)

To validate the effectiveness of the long-range dependency compressor IRE, we analyze the sensitivity of performance to the number of samples $s ^ { c }$ using experiments with GCNIFM, as shown in Table 4. The results provide two key insights: (1) Our proposed model maintains strong performance even with a small number of samples $s ^ { c }$ . This robustness is due to the final invariance representations $\hat { H }$ , which can be globally utilized by adjusting IRE parameters through back-propagation. This adaptability ensures that learned representations are effectively leveraged, even with limited node samples, leading to consistent performance across varying sample sizes. (2) Performance improves as the number of samples $s ^ { c }$ increases, highlighting the model’s enhanced ability to capture long-range dependencies. This improvement emphasizes the encoder’s role in integrating information from distant parts of the data, which is crucial for overall performance enhancement.

Table 4: Performance analysis under varying settings of the number of samples per class $s ^ { c }$ on Reddit.   

<html><body><table><tr><td>Number of samples sc</td><td>AUC-ROC</td><td>F1</td></tr><tr><td>32</td><td>88.80±11.77</td><td>81.16±25.27</td></tr><tr><td>64</td><td>89.31±11.07</td><td>81.64±24.76</td></tr><tr><td>128</td><td>89.56±10.87</td><td>81.71±25.32</td></tr><tr><td>256</td><td>89.66±10.74</td><td>81.77±24.84</td></tr><tr><td>512</td><td>90.00±10.31</td><td>81.82±24.79</td></tr></table></body></html>

# Visualization (RQ4)

We devise two types of visualizations to verify the invariance property and explainability of our proposed method. The results in Figure 2 highlight two key aspects: (1) The upper two figures show that GCN-IFM node representations exhibit less variation and fewer outliers compared to GCN. This indicates that GCN-IFM produces more consistent and stable node representations, enhancing model robustness and achieving of the invariance property. (2) The lower two figures demonstrate that in GCN-IFM, same-labeled node representations cluster tightly around a central point, with clear separations between clusters of different labels. In contrast, GCN node representations form multiple clusters with overlapping boundaries between labels. This suggests that GCN-IFM achieves clearer and more distinct class separation, highlighting its superior performance in capturing invariance and maintaining explainability in label prediction based on node representations. These visualizations collectively support the effectiveness of our method in achieving invariance and enhancing model interpretability.

# Conclusion

In this work, we develop the subgraph invariant learning paradigm to address computational challenges and out-ofdistribution performance issues in large-scale graphs. Leveraging the small-world phenomenon, our approach reduces computational costs and enhances scalability by focusing on manageable subgraphs. We propose the InVar loss to maximize the lower bound of invariance likelihood. Comprising IRE and NRE, IFM aligns with this loss, which compresses long-range dependencies and facilitates global and local information exchange. Experiments on large-scale graphs demonstrate IFM’s generalization capabilities, efficiency, and interpretability. Our approach addresses the limitations of graph reduction methods, offering a novel direction for graph node classification. Additionally, deriving a theoretical lower bound for invariance likelihood provides deeper insights into capturing invariance, bridging the gap between theoretical foundations and practical applications in invariance representation learning.