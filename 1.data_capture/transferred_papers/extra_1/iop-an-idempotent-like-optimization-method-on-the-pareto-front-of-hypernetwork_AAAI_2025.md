# IOP: An Idempotent-Like Optimization Method on the Pareto Front of Hypernetwork

Hui Wang1, Renyu Yang1\*, Jie $\mathbf { S u n } ^ { 2 }$ , Hao Peng1, Xudong $\mathbf { M o u } ^ { 1 , 2 }$ , Tianyu $\mathbf { W _ { 0 } } ^ { 1 , 2 }$ , Xudong Liu1,2\*

1Beihang University, Beijing, China 2Zhongguancun Laboratory, Beijing, China {whui,renyuyang,penghao,mxd,woty,liuxd}@buaa.edu.cn, sunjie@zgclab.edu.cn

# Abstract

Pareto Front Learning (PFL) has been one of the effective means to resolve multi-objective optimization problems through exploring all optimal solutions to learn the entire Pareto front. Pareto Hypernetwork (PHN) is a new promising way to generate the sequence of Pareto-optimal solutions that can be further used as potential solutions to constitute the Pareto front. However, the existing PHN-based approaches suffer from two performance issues: They take as inputs human-crafted preference vector or chunk embedding, rather than the input data samples, and thus vulnerable to data distribution shifts. Such approaches cannot optimize all potential solutions when forming the Pareto front, as they merely optimize the loss pertaining to one single input at a time of optimization round. To improve the quality of the Pareto front, we propose IOP, a novel Idempotent-like Optimization method to learn the entire Pareto front accurately and enhance Hypernetwork’s adaptability to distribution shifts. In particular, IOP performs idempotent-like optimization by exploiting manifold space mapping, so that the target networks generated by the optimized Hypernetwork can effectively handle samples with similar distributions of the input samples, without the pre-defined human-crafted inputs. IOP maximizes the Hypervolume indicator that is composed of all potential solutions at a higher level. Experimental results demonstrate that IOP outperforms the state-of-the-art methods by $4 . 7 \%$ on average in producing the Pareto front and has a $1 0 . 5 \%$ improvement in adaptability.

# Introduction

Multi-objective optimization (MOO) is of paramount importance – It unleashes the potential of acquiring optimal trade-offs among multiple conflicting objectives in various machine learning domains such as Multi-object Tracking (Nguyen et al. 2020), Computer Vision (Gao et al. 2024), Reinforcement Learning (Chen et al. 2024), Natural Language Processing (Xiang et al. 2022), etc. In MOO, the collection of all optimal solutions – commonly referred to as the Pareto front – manifests distinct trade-offs amidst conflicting objectives. Most of the existing MOO approaches solely obtain a partial set of optimal solutions on the Pareto front, and typically require a pre-defined strategy for effectively balancing multiple objectives. This setting, however, substantially limits the usability and flexibility, as the decisionmaking procedure, in most of the use cases, is on the fly without prior knowledge of such specific strategies.

To tackle this issue, Pareto Front Learning (PFL) emerges as an effective paradigm. All the optimal solutions spanning the entire Pareto front can be simultaneously resolved. Pareto Hypernetwork (PHN) (Navon et al. 2020) is a notable implementation of PFL, and it employs a single Hypernetwork (Ha et al. 2016) to take as inputs the preference vectors (Hoang et al. 2023) or chunk embeddings (Lin et al. 2020) and generate a sequence of Pareto-optimal models (aka. target networks) to process data. Although PHN-based approaches exhibit competitive efficiency, there are the following unsettled issues:

i) Relying on human—crafted inputs with limited adaptability to data distribution shift. A huge body of PHN studies depend on human-crafted inputs – often in the form of preference vector or chunk embedding and entirely independent from raw data samples – to train a precise Hypernetwork to acquire optimal target networks. The key concern is that devising high-quality inputs usually requires too much domain expertise and is sometimes impractical in industrial scenarios at scale. Additionally, rather than adopting raw data samples, using man-made inputs inevitably reduces the sensitivity of Hypernetwork to data distribution shifts. For example, as shown in Fig. 1, when the data shifts from the green to the blue distribution, the Hypernetwork in PHN methods generates more non-optimal solutions. Hence, it is imperative to establish a new competitive and adaptable Hypernetwork in the PHN method free from human-crafted inputs.

ii) Failing to simultaneously optimize multiple potential solutions. PHN methods optimize the Hypernetwork so that each output is associated with an optimal or approximate solution on the Pareto front. The ideal process is to continuously bring all potential solutions to the Pareto front as close as possible. However, the existing PHN methods fail to do so – in each optimization round, their focus is merely minimizing the loss related to the single input and accordingly updating the Hypernetwork’s weight. Doing this can hardly accommodate multiple potential solutions around the Pareto front simultaneously; in other words, some solutions move towards but other potential solutions go away. As a working example, in Fig. 1, optimizing the solutions in the dotted circle pushes the solutions in the solid circle away from the Pareto front. Undoubtedly, this will substantially diminish the quality of the obtained front and increase the optimization difficulty.

![](images/addd64145a098bce3069938c197510c73883c33b2df4a6f456322eea576c9e78.jpg)  
Figure 1: Compared with PHN methods, IOP is free from man-made input and focuses on the relationship among solutions. It comprehensively improves the accuracy by maximizing Hypervolume, and enhances the adaptability of Hypernetwork through the idempotent-like objective $L _ { i d e }$ .

This paper proposes IOP, a novel Idempotent-like Optimization method for PHN. The essence of IOP is devising an idempotent-like objective to optimize the Hypernetwork, in the light of manifold mapping from the sample space ${ \mathcal P } _ { x }$ to the target network weight space ${ \mathcal { P } } _ { \theta }$ . Without the need of any human-crafted inputs, the optimized Hypernetwork can generate target networks that effectively handle samples with similar distributions of the input samples. We control the range of manifold ${ \mathcal P } _ { \theta }$ by using a scaling factor in the objective to resize the “similar distributions”, thereby improving the adaptability of Hypernetwork. Unlike prior works, our Hypernetwork iteratively receives multiple samples in each optimization round to improve the performance of corresponding target networks (aka. potential solutions). At the core of the improvement is to maximize the Hypervolume indicator that is composed of all potential solutions at a higher level. IOP leverages a gradient-based dual optimization strategy to jointly incorporate the techniques above simultaneously. Extensive experiments are conducted on six multi-task and domain adaptation datasets, and experimental results show the superior effectiveness of IOP against the state-of-the-art approaches. Our main contributions can be summarized as follows:

We are the first to reveal the main limitations of existing PHN methods, and propose a new approach to improve Hypernetwork performance and adaptability. We devise an idempotent-like objective and use a manifold mapping mechanism to iteratively handle any samples that have similar distributions of the input data samples. Providing an in-depth theoretical analysis on the convergence of the optimization process of IOP.

# Related Work

Multi-Objective Optimization (MOO) methods aim to balance multiple conflicting objectives for optimal solutions (Ehrgott 2005). They can be roughly classified into gradientbased and gradient-free categories. (Sener 2018) proposes an early gradient-based approach for Multi-Task Learning (MTL), extended from MGD (De´side´ri 2012). (Lin et al. 2019) proposes Pareto MTL (PMTL) to split the loss space into separate cones. (Mahapatra 2020) proposes EPO, extended from PMTL, to combine gradient descent and controlled ascent to improve optimization convergence. (Ma et al. 2020) proposes a gradient-free approach to extend an optimal solution to its local neighborhood. (Ye et al. 2022) proposes PNG to find solutions by optimizing an extra criterion function. However, the above traditional methods can only obtain a single or partial optimal solution on the Pareto front, leaving the MOO problem to be thoroughly solved.

Pareto Front Learning (PFL) has become the most effective paradigm for MOO by seeking all the optimal solutions and learning the entire Pareto front. Early work (Lin et al. 2019) on PFL trains a Hypernetwork (Ha et al. 2016), mainly through changing the reference directions in the objective so as to achieve a complete fitting of the Pareto front. (Lin et al. 2020; Navon et al. 2020; Ruchte et al. 2021) further use preference or embedding as inputs of Hypertnetwork to improve the controllability of optimization. (Hoang et al. 2023) employs a Hypernetwork to generate multiple solutions from diverse trade-off preferences. However, all of these PFL works still heavily depend upon non-sample inputs and neglect the adaptability of Hypernetwork to data distribution shift. Our work aims to jointly overcome these shortcomings and bring new insights into PFL.

# Methodology

# Problem Formulation

As shown in Fig. 2, our Hypernetwork $H ( \cdot ; \psi )$ , $\psi \in \mathbb { R } ^ { q }$ receives an input vector $\mathbf { x } = [ x _ { 1 } , . . . , x _ { N } ] , x _ { i } \in \mathbb { R } ^ { d } , i \in [ N ]$ in each training round, and generates $N$ target networks $\{ T N ( \cdot ; \theta _ { i } ) : \theta _ { i } \stackrel { \smile } { = } H ( x _ { i } ; \psi ) , \theta _ { i } ^ { - } \in \mathbb { R } ^ { d } \} _ { i = 1 } ^ { N }$ . Assuming target network weights and samples have the same dimension, i.e., $\dim ( x _ { i } ) = \bar { \dim } ( \theta _ { i } ) , \forall i \ \bar { \in } \ [ N ] .$ which can be achieved by upsampling on $x _ { i }$ , or by modifying the structure of $T N ( \cdot ; \theta _ { i } )$ . $\{ L _ { t n } ^ { j } ( \theta _ { i } ) \} _ { j = 1 } ^ { J }$ is denoted as the $J$ loss functions of $T N ( \cdot ; \theta _ { i } )$ :

$$
L _ { t n } ^ { j } ( \theta _ { i } ) = l _ { t n } ^ { j } ( \theta _ { i } ) - \beta [ \cos ( \vec { x } _ { i } , \overrightarrow { \Gamma ( \theta _ { i } ) } ) + H V ( \Upsilon ) ] ,
$$

where $l _ { t n } ^ { j } ( \theta _ { i } )$ is the inner loss function such as crossentropy, $\scriptstyle \dot { \Gamma } ( \theta _ { i } ) = ( l _ { t n } ^ { 1 } ( \theta _ { i } ) , . . . , l _ { t n } ^ { J } ( \theta _ { i } ) )$ , $\Upsilon { = } [ \Gamma ( \theta _ { 1 } ) , . . . , \Gamma ( \theta _ { N } ) ]$ . $H V ( \cdot )$ denotes the Hypervolume (Zitzler 1999), the area dominated between the Pareto front and a specified reference point. $\beta \in ( 0 , 0 . 5 ]$ is the coefficient. Then the MOO of Hypernetwork can be expressed as follows:

$$
\begin{array} { l } { \displaystyle \psi ^ { * } = \arg \operatorname* { m i n } _ { \psi \in \mathbb { R } ^ { q } } \frac { 1 } { N J } \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { J } L _ { t n } ^ { j } ( \theta _ { i } ) = \arg \operatorname* { m i n } _ { \psi \in \mathbb { R } ^ { q } } \frac { 1 } { N J } \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { J } ( \underbrace { \ d } _ { l } ) } \\ { \displaystyle l _ { t n } ^ { j } ( \theta _ { i } ) - \beta \cos ( \vec { x } _ { i } , \overrightarrow { \Gamma ( \theta _ { i } ) } ) - \beta H V ( \Upsilon ) \big ) , } \end{array}
$$

where $H V ( \cdot ) \ : \ \mathbb { R } ^ { N } \times \mathbb { R } ^ { J } \to \ \mathbb { R } _ { + }$ is a monotonically decreasing function. For a fixed reference point, a smaller

𝒙𝒙 𝑥𝑥1 𝑥𝑥2 ȉȉȉ 𝑥𝑥𝑁𝑁 Χ Ψ Θ MOO objective   
Tide Ƥ𝜃𝜃 𝜃𝜃 𝑥𝑥1 𝑥𝑥1 1 𝑇𝑇𝑇𝑇𝑇(𝑇ȉ𝑇;(ȉ𝜃;𝜃1𝜃)𝜃 ) 𝒂𝒂𝒂𝒂𝒂𝝍𝒂 𝝍𝒂𝒂𝒂 𝑱 ෍ ෍ 𝑳𝑳𝒋𝒕𝒋𝒕𝒕𝒕(𝜽𝜽𝒊𝒊)   
𝑥𝑥 𝒔𝒔𝒔𝒔 𝑥𝑥2 1 𝐻𝐻(𝜓𝜓) 𝑻 𝑻 𝒕𝒕𝒕𝒕𝒕𝒕 𝒕𝒕𝒕𝒕 ： ： ： 𝒂𝒂𝒂𝒂𝒂𝒂𝒂𝒂𝒂 𝑳𝑳𝒊𝒊𝒊𝒊𝒊𝒊 (𝜓𝜓) 梦 𝑥𝑥𝑁𝑁 𝑥𝑥𝑁𝑁 𝑇𝑇𝑇𝑇(ȉ; 𝜃𝜃𝑁𝑁) 𝝍 𝒙𝒙 idempotent-like objective

$\{ l _ { t n } ^ { j } ( \theta _ { i } ) \} _ { j = 1 } ^ { J }$ leads to a larger $H V ( \cdot )$ . If $H V ( \Upsilon )$ is maximized, the current solution $\psi$ will be on the Pareto front, which is also beneficial for improving the performance of other potential solutions (Hoang et al. 2023). Herein, $\cos ( \cdot , \cdot ) : \dot { \mathbb { R } } ^ { d } \times \mathbb { R } ^ { J }  \mathbb { R }$ denotes the cosine similarity function. It helps to spread the Pareto front and brings the solution closer to the input (Ye et al. 2022), as well as strengthens the correlation between $H ( \cdot ; \psi )$ and $\vec { x } _ { i }$ , improving Hypernetwork’s sensitivity to samples. If the dimensions of $\vec { x } _ { i }$ and $\overrightarrow { \Gamma ( \theta _ { i } ) }$ differ, we perform zero padding on $\Gamma ( \theta _ { i } )$ , which has proven effective in (Mahapatra 2020).

In addition, we design an idempotent-like objective ${ L } _ { i d e } ( \psi )$ to enhance Hypernetwork’s adaptability. We unify the $\dot { L } _ { i d e } ( \psi )$ and MOO objective in Eq. 2 through a dual optimization strategy $\mathcal { D O S }$ . More details about $L _ { i d e } ( \psi )$ and $\mathcal { D O S }$ will be discussed in subsequent sections. Let denote $\begin{array} { r } { L _ { t n } ( \psi ) { = } \frac { 1 } { N J } \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { J } L _ { t n } ^ { j } ( \theta _ { i } ) } \end{array}$ (Eq. 2), the optimization of Hypernetwork in IOP can finally be formalized as:

$$
\psi ^ { * } = \operatorname * { a r g m i n } _ { \psi \in \mathbb { R } ^ { q } } \mathcal { D O S } ( L _ { t n } ( \psi ) , L _ { i d e } ( \psi ) ) .
$$

# Idempotent-Like Optimization Objective

We design the idempotent-like objective ${ L } _ { i d e } ( \psi )$ to enhance Hypernetwork’s adaptability from the perspective of manifold mapping (Shocher et al. 2023). As shown in Fig. 2, the Hypernetwork $H ( \cdot ; \psi )$ is trained to generate target network weight $\theta \sim \mathcal { P } _ { \theta }$ when given $x \sim \mathcal { P } _ { x }$ . We assume the instances in sample distribution ${ \mathcal P } _ { x }$ and target distribution ${ \mathcal { P } } _ { \theta }$ have the same dimension. This allows applying $H ( \cdot ; \psi )$ to both $x$ and $\theta$ . Our $L _ { i d e } ( \psi )$ follows the following principles: (1) For any $x \ \sim \ \mathcal { P } _ { x }$ , the $H ( \cdot ; \psi )$ maps it to ${ \mathcal { P } } _ { \theta }$ , i.e., $H ( x ; \psi ) = \theta$ . We hope that $H ( \cdot ; \psi )$ can be applied sequentially multiple times and reduce the difference in results from the initial application (i.e., idempotence-like), which endows $H ( \cdot ; \psi )$ to fine-tune based on its original performance to achieve adaptability to distribution shifts. So ${ L } _ { i d e } ( \psi )$ contains the loss term $\dot { T _ { i d e } } ( \psi ) = \| H ( H ( x ; \psi ) ; \psi ) -$ $H ( x ; \psi ) \| _ { 2 } , \| \cdot \| _ { 2 }$ denotes the $L _ { 2 }$ distance. We use the weightfreezing technique (Shocher et al. 2023) to reduce the difficulty of optimizing $T _ { i d e } ( \psi )$ and modify it to $T _ { i d e } ( \psi ; \hat { \psi } ) =$ $\| H ( H ( x ; \psi ) ; \hat { \psi } ) - H ( x ; \psi ) \| _ { 2 }$ , where $\hat { \psi }$ denotes the frozen copy of $\psi$ , i.e., a gradient taken $\psi$ will not affect $\hat { \psi }$ .

(2) For any $\theta \sim \mathcal { P } _ { \theta }$ , the $H ( \cdot ; \psi )$ should try to map it to itself, i.e., $H ( \theta ; \psi ) = \theta$ . Therefore, $\dot { L } _ { i d e } ( \psi )$ contains the selfmapping loss term $T _ { s m } ( \psi ) = \| H ( \theta ; \psi ) - \theta \| _ { 2 }$ .

(3) Shrinking ${ \mathcal { P } } _ { \theta }$ can improve the accuracy of the obtained $\theta$ , enhancing the performance of target network. However, it inevitably reduces the diversity of $\theta$ , affecting the adaptability of Hypernetwork. Therefore, we control the range of ${ \mathcal { P } } _ { \theta }$ through a controllable manifold contraction term $T _ { t i g h t } ( \psi ; \hat { \psi } ) = - \| H ( H ( x ; \hat { \psi } ) ; \psi ) - H ( x ; \hat { \psi } ) \| _ { 2 }$ . This technique was first proposed in (Shocher et al. 2023), and we extend it to the cross-space mapping from ${ \mathcal P } _ { x }$ to ${ \mathcal { P } } _ { \theta }$ . In summary, we modify ${ L } _ { i d e } ( \psi )$ to ${ L } _ { i d e } ( \psi ; \hat { \psi } )$ and define it as:

$$
\begin{array} { r l } & { L _ { i d e } ( \psi ; \hat { \psi } ) = \exp ( \ \| H ( H ( \boldsymbol { x } ; \psi ) ; \hat { \psi } ) - H ( \boldsymbol { x } ; \psi ) \| _ { 2 } + } \\ & { \| H ( \theta ; \psi ) - \theta \| _ { 2 } - \lambda _ { i d e } \| H ( H ( \boldsymbol { x } ; \hat { \psi } ) ; \psi ) - H ( \boldsymbol { x } ; \hat { \psi } ) \| _ { 2 } ) \ , } \end{array}
$$

where $\exp ( \cdot )$ is a monotonic nonnegative operator, and $\lambda _ { i d e } \in ( 0 , 1 ]$ is the contraction coefficient of $T _ { t i g h t } ( \psi ; \hat { \psi } )$ . When $\lambda _ { i d e } ~  ~ 0$ , ${ \mathcal { P } } _ { \theta }$ will not shrink. When $\lambda _ { i d e } ~  ~ 1$ , $T _ { t i g h t } ( \psi ; \hat { \psi } )$ is fully expressed, and ${ \mathcal { P } } _ { \theta }$ is totally tightened. Notably, when using gradient-based methods to optimize Eq. 4, the gradient is related to $\psi$ but independent of $\hat { \psi }$ .

Theorem 1. If the capacity of $H ( \cdot ; \psi )$ is large enough, the   
terms $\exp ( T _ { s m } ( \psi ) - \lambda _ { i d e } T _ { t i g h t } ( \psi ; \hat { \psi } ) )$ and $\exp ( T _ { i d e } ( \psi ; \hat { \psi } ) )$   
in Eq. 4 will have common global minima. Assum  
ing $\psi ^ { * } ~ = ~ \arg \operatorname* { m i n } \exp ( T _ { s m } ( \psi ) ~ - ~ \lambda _ { i d e } T _ { t i g h t } ( \psi ; \psi ^ { * } ) ) ~ =$ $\boldsymbol { \psi } \in \mathbb { R } ^ { q }$   
$\arg \operatorname* { m i n } _ { { \boldsymbol { \alpha } } ^ { \prime } \in \mathbb { D } ^ { q } } \exp ( T _ { i d e } ( \psi ; \psi ^ { * } ) )$ , then $\exists \psi ^ { * }$ such that $\mathcal { P } _ { \psi ^ { \ast } } = \mathcal { P } _ { \theta }$ . ψ Rq

# Dual Optimization Strategy

We use a gradient-based dual optimization strategy $( \mathcal { D } \mathcal { O } \mathcal { S } )$ to unify the MOO objective $L _ { t n } ( \psi )$ in Eq. 2 and idempotentlike objective $D _ { i d e } ( \psi ; \hat { \psi } )$ in Eq. 4. Specifically, the gradient update strategy in the $t$ -th $\in [ T ]$ training round is:

![](images/209e16ec4c99b11359e4791365e9716d20bc46aa592a9b69a9ee1d0d4126b940.jpg)  
Figure 3: The dual optimization strategy.

$$
\psi ^ { t + 1 } \gets \psi ^ { t } - \eta \times \mathcal { D } ( \psi ^ { t } ) ,
$$

where $\eta$ is the learning rate, $\mathcal { D } ( \psi ^ { t } )$ denotes the gradient of Hypernetwork. As shown in Fig. 3, $\mathcal { D O S }$ includes: (1) When $\psi ^ { t }$ is far from the Pareto front $P F _ { \psi }$ , select $\mathcal { D } ( \psi ^ { t } )$ that achieves the maximum improvement towards $P F _ { \psi }$ , focusing on MOO trade-off. (2) When there are different improvement directions for $\psi ^ { t }$ towards $P F _ { \psi }$ , choose the $\mathcal { D } ( \psi ^ { t } )$ to minimize ${ L } _ { i d e } ( \psi ; \hat { \psi } )$ . (3) When $\psi ^ { t }$ approaches $P F _ { \psi }$ , only focus on decreasing $D _ { i d e } ( \psi ; \hat { \psi } )$ . For target network $T N ( \cdot ; \theta _ { i } ) , \theta _ { i } = H ( x _ { i } ; \psi ^ { t } ) , i \in [ N ]$ , we obtain $\mathcal { D } ( \psi ^ { t } )$ by solving the following optimization problem (Ye et al. 2022).

$$
\begin{array} { r l } & { \mathcal { D } ( \psi ^ { t } ) = \underset { \mathcal { G } \in \mathbb R ^ { q } } { \arg \operatorname* { m i n } } \{ \frac { 1 } { 2 } \| \nabla L _ { i d e } ( \psi ^ { t } ; \hat { \psi } ^ { t } ) - \mathcal { G } \| _ { 2 } ^ { 2 } \} } \\ & { \nabla L _ { t n } ^ { j } ( \theta _ { i } ) ^ { T } \mathcal { D } ( \psi ^ { t } ) \geq | \cos ( \vec { x } _ { i } , \overrightarrow { \Gamma ( \theta _ { i } ) } ) | \cdot S ( \psi ^ { t } ) , j \in [ J ] , } \end{array}
$$

where $L _ { t n } ^ { j } ( \theta _ { i } )$ denotes the loss function of target network defined in Eq. 1, $\Gamma ( \theta _ { i } ) = ( l _ { t n } ^ { 1 } ( \theta _ { i } ) , . . . , l _ { t n } ^ { \ J } ( \theta _ { i } ) ) , \bar { S } ( \psi ^ { t } )$ represents the Pareto stationary (Lin et al. 2020) of $\psi ^ { t }$ . The operator $\| \cdot \| _ { 2 } ^ { 2 }$ forces the gradient $\mathcal { G }$ of Hypernetwork to approach the idempotent-like objective gradient $\nabla L _ { i d e } ( \psi ^ { t } ; \hat { \psi } ^ { t } )$ . Unlike (Ye et al. 2022), we strengthen the constraint lower bound, the gradient descent rate of losses $\{ L _ { t n } ^ { j } ( \theta _ { i } ) \} _ { j = 1 } ^ { J }$ are constrained by $\vert \cos ( \overrightarrow { x _ { i } } , \overrightarrow { \Gamma ( \theta _ { i } } ) ) \vert \cdot S ( \psi ^ { t } )$ . Since $\vec { x } _ { i }$ is deterministic, $| \cos ( \overrightarrow { x } _ { i } , \overrightarrow { \Gamma ( \theta _ { i } ) } ) |$ will vary between 0 and 1 in the optimization, which provides an intermediate updating direction between gradient descent on $L _ { i d e } ( \psi ^ { t } ; \hat { \psi } ^ { t } )$ and multiple gradient descent (De´side´ri 2012) on $\{ \underset { - } { L _ { t n } ^ { j } } ( \theta _ { i } ) \} _ { j = 1 } ^ { J }$ , making the optimization easier towards $P F _ { \psi }$ . In addition, introducing the Pareto stationary term $S ( \psi ^ { t } )$ in constraint will provide a dynamic improvement based on the distance between $\psi ^ { t }$ and $P F _ { \psi }$ , which has been proven more efficient (Ye et al. 2022). The Eq. 6 can be transformed into a dual form:

$$
D ( \psi ^ { t } ) = \nabla L _ { i d e } ( \psi ^ { t } ; \hat { \psi } ^ { t } ) + \sum _ { j = 1 } ^ { J } \lambda _ { j , t } \nabla L _ { t n } ^ { \ j } ( \theta _ { i } ) \ ,
$$

with $\{ \lambda _ { j , t } \} _ { j = 1 } ^ { J }$ the solution of the following problem:

$$
\begin{array} { r l } & { \displaystyle \operatorname* { m a x } _ { \lambda \in \mathbb { R } _ { + } ^ { J } } \sum _ { j = 1 } ^ { J } \lambda _ { j } \cos ( \vec { x } _ { i } , \overrightarrow { \Gamma ( \theta _ { i } ) } ) - \frac { 1 } { 2 } \| \nabla L _ { i d e } ( \psi ^ { t } ; \hat { \psi } ^ { t } ) + } \\ & { \displaystyle \sum _ { j = 1 } ^ { J } \lambda _ { t } \nabla L _ { t n } ^ { j } ( \theta _ { i } ) \| _ { 2 } ^ { 2 } , } \end{array}
$$

we follow (Ye et al. 2022) and initialize $\{ \lambda _ { j , t } \} _ { j = 1 } ^ { J }$ with zero vector and terminate the optimization when reach the number of iterations $T , { \mathcal { D O S } }$ provides a progressive optimization towards the Pareto front and ensures that it will not leave its closure once the solution enters the Pareto front.

Theorem 2. Assuming $t _ { f } { > } 0 , \psi ^ { t _ { f } } \in P F _ { t _ { f } } , \psi ^ { 0 } \notin P F _ { t _ { f } }$ , then $\forall t < t _ { f } , i f | \cos ( \overrightarrow { x } _ { i } , \overrightarrow { \Gamma ( \theta _ { i } ^ { t } ) } ) | \cdot S ( \psi ^ { t } ) > 0$ , optimization based on Eq. 6 will bring Pareto improvement for $\psi ^ { t }$ . In addition, $\exists t _ { \epsilon } > t _ { f }$ such that $\psi ^ { t _ { \epsilon } } \in P F _ { t _ { \epsilon } }$ , then $\forall t \ge t _ { \epsilon } , \psi ^ { t } \in \overline { { P F _ { t _ { \epsilon } } } }$ , that is, the solution enters $P F _ { t _ { \epsilon } }$ will not leave closure $\overline { { P F _ { t _ { \epsilon } } } }$ .

# Experiments

# Experiment Setup

Datasets. We use multi-task learning datasets MultiMNIST, Multi-Fashion, and Fashion-MNIST (Xiao et al. 2017), to evaluate the MOO capability of IOP, which contains 120K training and 20K testing samples. Each dataset has two objectives, i.e., classifying the top-left and bottomright images. We use domain adaptation datasets PACS (Li et al. 2017), DomainNet (Peng et al. 2019), and OfficeHome (Hemanth 2017) to evaluate the adaptability of IOP.

Baselines. Our baselines consist of MOO and PHN groups. The former includes MGD (Sener 2018), UW (Kendall et al. 2018), LS (Boyd et al. 2004), PMTL (Lin et al. 2019), COSMOS (Ruchte et al. 2021), DWA (Liu et al. 2019), NashMTL (Navon et al. 2022), EPO (Mahapatra 2020), FAMO (Liu et al. 2024), and PNG (Ye et al. 2022) methods, they use different techniques to improve the quality of Pareto front. The PHN group contains CPMTL (Lin et al. 2020), PHNLS, PHN-EPO (Navon et al. 2020), and PHN-HVI (Hoang et al. 2023) methods, they all learn the entire Pareto front using a single Hypernetwork.

Implementation Details. We use a 10-layer Multi-Layer Preceptor and ResNet18 (He et al. 2016) as the backbone of Hypernetwork and target network. We use DySample (Liu et al. 2023) to upsample the sample, aligning the dimensions of input and output of Hypernetwork. By default, we set the training round $T$ , learning rate $\eta$ , input dimension of Hypernetwork $d$ , target network number $N$ , contraction coefficient $\lambda _ { i d e }$ , and balance coefficient $\beta$ to 1000, 0.001, $1 0 ^ { 5 }$ , 100, 0.75 and 0.1. We use the cross-entropy loss as $\boldsymbol { l } _ { t n }$ within $L _ { t n }$ in Eq. 1. All the experiments are implemented with Pytorch and trained on a single NVIDIA Tesla V100.

# 2/3-Objective Problem Optimization Trajectory

We evaluate the optimization trajectory of the proposed IOP on the synthesized $2 / 3$ -objective problem (2/3OP) (Lin et al. 2019). The 2OP contains two loss functions:

$$
\begin{array} { r } { \iota _ { 1 } ( \psi ) - \mathbf { 1 } - \exp ( - | | \psi - \varphi | \stackrel { \angle } { = } | | _ { 2 } ) , } \\ { l _ { 2 } \big ( \psi \big ) = 1 - \exp \bigl ( - \bigl \| \psi + q ^ { - \frac { 1 } { 2 } } \bigr \| _ { 2 } ^ { 2 } \bigr ) , } \end{array}
$$

𝒍𝒍𝟐𝟐(𝝍𝝍) 𝒍𝒍𝟐𝟐(𝝍𝝍)   
0.9 0.9 𝒍𝒍𝟑𝟑(𝝍𝝍) 𝒍𝒍𝟑𝟑(𝝍𝝍)   
0.7 0.7 0.9 0.9 0.7 0.7   
0.5 0.5 0.5 0.5 ? 𝟎𝟎. 𝟗𝟗 (5)   
0.3 0.1 𝝀𝝀𝝀𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊 = 𝟎𝟎𝟎. 𝟏𝟑𝟏𝟑 𝑷𝑷𝑷𝑷𝝍𝝍 𝒍𝒍𝟏𝟏(𝝍𝝍) 0.1 𝝀𝝀𝝀𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊 = 𝟎𝟎𝟎. 𝟏𝟑𝟏𝟑 𝒍𝒍𝟏𝟏(𝝍𝝍) 𝝀𝝀𝒊𝒊𝒊𝒊𝒊𝒊 𝒊𝒊𝒊𝒊𝒊𝒊 = 𝟎𝟎. 𝟕𝟕 𝟎𝟎. 𝟓 0.3 𝝀𝝀𝒊𝒊𝒊𝒊𝒊𝒊 𝒊𝒊𝒊𝒊 𝒊 𝒊𝒊 𝒊𝒊 𝟎𝟎. 𝟕𝟕 𝟎𝟎. 𝟓𝟓 𝑷𝑷𝑷𝑷𝝍𝝍 0.3 0.1 𝑷𝑷𝑷𝑷𝝍𝝍 .l(φ) 𝝀𝝀𝒊𝒊𝒊𝒊𝒊𝒊 = 𝟎𝟎. 𝟕𝟕 0.3 0.1 𝑷𝑷𝑷𝑷𝝍𝝍 --(φ) 𝝀𝝀𝒊𝒊𝒊𝒊𝒊𝒊 = 𝟎𝟎. 𝟕𝟕 = 𝟎𝟎. 𝟗 0. 10.30.50 𝝀𝝀𝒊𝒊𝒊𝒊𝒊𝒊 = 𝟑 𝟎𝟎𝟎. 𝟓𝟑 0.1 0.3 0.5.0 𝝀𝝀𝝀𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊 = 𝟎𝟎𝟎. 𝟓𝟑 0 （ 0g (a) 2OP $( \mathrm { N } = 1 0 0 )$ (b) 2OP (N = 200) (c) 3OP (N = 100) (d) 3OP $\langle \mathbf { N } = 2 0 0 )$ ）

Table 1: Comparison of accuracy, $H V ,$ , and training cost (GFlops, $\scriptstyle \mathbf { G } = 1 0 ^ { 9 }$ ). $A c c$ denotes the average classification accuracy for the top-left and bottom-right images in the datasets. The best results are highlighted in bold and the second best are underlined.   

<html><body><table><tr><td rowspan="2">Groups</td><td rowspan="2">Methods</td><td colspan="3">Multi-MNIST</td><td colspan="3">Multi-Fashion</td><td colspan="3">Fashion-MNIST</td></tr><tr><td>Acc ↑</td><td>HV↑</td><td>Cost↓</td><td>Acc ↑</td><td>HV个</td><td>Cost↓</td><td>Acc ↑</td><td>HV↑</td><td>Cost↓</td></tr><tr><td>MO0</td><td>MGD UW LS PMTL COSMOS DWA NashMTL EPO</td><td>89.52 ± 1.41 89.56 ± 0.31 89.78 ±0.24 89.72 ± 1.72 90.67 ± 1.56 90.21 ± 1.23 90.78 ± 1.24 91.72 ± 0.62</td><td>2.80 ± 0.08 2.81 ±0.24 2.85 ±0.11 2.84 ± 0.41 2.87 ±0.23 2.86 ± 0.35 2.88 ± 0.14 2.90 ±0.34 6.73 ± 0.22</td><td>7.25 ± 0.29 7.22 ± 0.35 7.42 ±0.13 7.31 ± 0.67 7.81 ± 0.91 7.09 ± 0.21 7.18 ± 0.74</td><td>88.03 ± 0.42 88.81 ±0.24 89.02 ±0.21 88.74 ± 0.61 89.69 ± 0.81 90.01 ± 0.41 90.57 ± 0.52</td><td>2.56 ± 0.27 2.62 ± 0.12 2.63 ± 0.12 2.61 ± 0.07 2.64 ± 0.12 2.65 ± 0.41 2.67 ± 0.21</td><td>9.32 ± 0.13 9.28 ± 0.71 9.31 ±0.04 9.26 ± 0.33 9.66 ± 0.03 9.03 ± 0.31</td><td>89.81 ± 1.62 89.33±0.54 90.22 ± 1.01 89.27 ± 1.14 91.52 ± 1.47 89.29 ± 1.71 89.79 ± 1.37</td><td>2.78 ± 0.06 2.79 ± 0.13 2.76 ± 0.21 2.84 ± 0.21 2.76 ± 0.14 2.76 ± 0.09</td><td>8.59 ± 0.07 8.61 ± 0.59 8.92 ± 0.03 8.57 ± 0.61 9.14 ± 0.31 8.60 ± 0.24 8.58±0.15</td></tr><tr><td>PHN</td><td>FAMO PNG CPMTL PHN-LS PHN-EPO PHN-HVI</td><td>92.42 ± 1.09 92.74 ± 1.23 90.45 ± 1.01 90.93 ± 2.84 91.34 ± 1.43 92.81 ± 0.54</td><td>2.91 ±0.22 7.21 ± 0.13 2.92 ± 0.31 2.86 ± 0.21 2.88 ±0.42 2.89 ±0.31 2.93 ±0.13</td><td>7.44 ± 0.23 7.68 ± 0.67 7.72 ± 0.71 7.61 ± 0.63 7.78 ± 0.23</td><td>91.24 ± 0.32 90.37 ± 1.89 90.04 ± 0.71 90.61 ± 0.71 91.87 ± 1.42</td><td>2.69 ± 0.14 2.66 ± 0.25 2.65 ± 0.02 2.68 ±0.02 2.70 ± 0.11</td><td>9.24 ± 0.32 9.48 ± 0.71 9.43 ± 0.23 9.46 ±0.42 9.53 ± 0.36</td><td>90.47 ± 1.34 92.73 ± 1.85 90.57 ± 0.89 90.62 ± 2.78 91.63 ± 0.78</td><td>2.80 ± 0.15 2.88 ± 0.21 2.81 ± 0.11 2.82 ± 0.14 2.84 ±0.02</td><td>8.72 ± 0.35 9.04 ± 1.03 9.07 ± 0.32 8.98 ± 0.03 8.97 ± 0.01</td></tr></table></body></html>

where $q = 1 0 ^ { 5 }$ . The 3OP contains three loss functions:

$$
\begin{array} { r l } & { l _ { 1 } ( \psi ) = \cos ( \frac { \pi \psi _ { 1 } } { 2 } ) \cos ( \frac { \pi \psi _ { 2 } } { 2 } ) ( \sum _ { i = 3 } ^ { q } ( \psi _ { i } - \frac { 1 } { 2 } ) ^ { 2 } + 1 ) , } \\ & { l _ { 2 } ( \psi ) = \cos ( \frac { \pi \psi _ { 1 } } { 2 } ) \sin ( \frac { \pi \psi _ { 2 } } { 2 } ) ( \sum _ { i = 3 } ^ { q } ( \psi _ { i } - \frac { 1 } { 2 } ) ^ { 2 } + 1 ) , } \\ & { l _ { 3 } ( \psi ) = \sin ( \frac { \pi \psi _ { 1 } } { 2 } ) ( \sum _ { i = 3 } ^ { q } ( \psi _ { i } - \frac { 1 } { 2 } ) ^ { 2 } + 1 ) , } \end{array}
$$

where $q = 1 0 ^ { 6 } , \psi = [ \psi _ { 1 } , . . . , \psi _ { q } ] , 0 \leq \psi _ { i } \leq 1 , i \in [ q ] .$ . Unlike (Lin et al. 2019), we use a larger dimension of $q$ $( 1 0 ^ { 5 } , 1 0 ^ { 6 } )$ to simulate the weight number of Hypernetwork, which is more realistic in our scenario. Fig. 4 demonstrates the loss trajectories in the search process for optimal solutions, which start from random weights. Overall, under different target network numbers $N$ and contraction coefficients $\lambda _ { i d e }$ , the search paths of solutions can accurately converge to the Pareto front, indicating that our IOP has the ability to optimize towards the Pareto front. Notably, a smaller $\lambda _ { i d e }$ , such as 0.1 or 0.3, may lead to the search trajectory’s convergence point not being wholly located on the Pareto front, as indicated by red ellipses in Fig. 4, which indicates that insufficient manifold contraction of target distribution ${ \mathcal { P } } _ { \theta }$ in Eq. 4 can produce unstable solutions. However, it is not wise to fully tighten ${ \mathcal { P } } _ { \theta }$ by setting $\lambda _ { i d e } = 1$ , as this may reduce the adaptability of Hypernetwork. We provide guidance on empirically setting $\lambda _ { i d e }$ in the subsequent section.

# Comparison of Accuracy, $\pmb { H V } _ { : }$ , and Training Cost

We evaluate the quality of solutions obtained by different methods on datasets Multi-MNIST, Multi-Fashion, and Fashion-MNIST using accuracy and Hypervolume $( H V )$ metrics. We run all methods with five independent trials and report the average value and standard deviation in Table 1. When calculating the $H V$ metric, we specify the reference point (2, 2) empirically for all methods. We ensure that the different loss values of each method are not greater than the corresponding coordinate value of the reference point, which is a common practice in MOO (Ye et al. 2022; Hoang et al. 2023). On all datasets, IOP outperforms the methods in MOO and PHN groups by an average of $5 . 3 \%$ and $4 . 0 \%$ on the accuracy, and $5 . 2 \%$ and $3 . 9 \%$ on the $H V$ . The approximate positive correlation between accuracy and $H V$ indicates the effectiveness of $H V$ metric in reflecting performance. In addition, we use the Torchstat (Swall0w 2018) tool to estimate the computational cost of different methods in training. The result shows that IOP has an average increase of $3 . 7 \%$ and $2 . 1 \%$ costs compared to MOO and PHN groups. The difference in the increased costs is because IOP not only includes the cost generated by the Hypernetwork, as the methods in the PHN group, but also contains the additional cost brought by the idempotent-like optimization.

Impact of $\lambda _ { i d e }$ on the Hypernetwork’s Adaptability Fig. 5 evaluates the impact of contraction coefficient $\lambda _ { i d e }$ on Hypernetwork’s adaptability. We statistic the average accuracy of target networks generated by Hypernetwork on datasets PACS, DomainNet, and Office-Home. The accuracy varies similarly on different datasets and is unaffected by the amount of target network $N$ . Specifically, as $\lambda _ { i d e }$ ranges from 0 to 0.75, the accuracy increases and peaks at approximately 0.75, decreasing as $\lambda _ { i d e }$ from 0.75 to 1. The reason for this is straightforward. We have discussed that using a larger $\lambda _ { i d e }$ to tighten the target distribution ${ \mathcal { P } } _ { \theta }$ is beneficial for improving the accuracy of solutions. However, from Eq. 4, we know that a larger $\lambda _ { i d e }$ limits the solution range and reduces its diversity, damaging Hypernetwork’s adaptability.

0.87 0.63 0.87 F 0.61 0.85 0.83 0.59 N=50 N=50 0.81 N=50 N= 100 0.57 N=100 N=100 N= 200 N= 200 0.79 N=200 0.79 0.2 0.4 0.6 0.81.00.55 0.2 0.40.6 0.81.0 0.77 0.2 0.4 0.60.81.0 ( PACS ) ( DomainNet ) ( OfficeHome )

We use an independent cross-entropy loss function for each domain in the above datasets and construct the MOO problem on them. Table 2 provides the cross-domain average accuracy (CDAA) of different methods. In the MOO group, we use Resnet18 (He et al. 2016) as the backbone of the model in baselines and optimize it using the technique described in the literature. Our IOP outperforms the methods in MOO and PHN groups by an average of $1 2 . 3 \%$ and $8 . 7 \%$ on CDAA. Although a too-small or too-large $\lambda _ { i d e }$ (e.g., 0.35 or 0.95) may weaken this advantage, it does not change the fact that IOP can effectively compensate for the shortcomings of MOO and PHN methods in model adaptability.

Table 2: Cross-domain average accuracy on adaptation datasets PACS, DomainNet, and Office-Home.   

<html><body><table><tr><td>Groups|</td><td>Methods</td><td>PACS</td><td></td><td>DomainNet Office-Home</td></tr><tr><td>MOO</td><td>MGD UW LS PMTL COSMOS NaDWTL EPO FAMO</td><td>76.23 ± 1.53|56.12 ± 1.24|77.23 ± 1.73 74.72 ± 0.45 69.14 ± 1.72 78.43 ±0.52 79.12 ± 1.32 79.04 ± 1.6257.35 ±0.35 77.04 ± 1.32</td><td>54.62 ± 1.47 51.82 ± 1.24 58.13 ± 1.43 58.72 ±0.13 75.03±1.555.1 ±.82 56.23 ± 1.09</td><td>73.62 ± 0.52 68.31 ± 1.61 78.56 ± 0.32 78.69 ± 1.31 74.72± 1.32 77.42 ± 0.23</td></tr><tr><td>PHN</td><td>PNG CPMTL PHN-LS PHN-EPO PHN-HVI IOP(Xide=0.35) IOP(入ide=0.75)</td><td>79.42 ± 0.72 71.32 ± 1.07|55.14 ±0.59 79.06 ± 2.11 80.22 ± 0.13 80.12 ± 1.46 83.23 ± 1.31 86.25 ± 0.85</td><td>59.02 ± 1.14 59.01 ± 0.21 60.09 ± 0.31 60.03 ± 0.23 59.32 ± 1.42 62.07 ± 1.32</td><td>79.42 ± 1.74 72.82 ± 1.35 78.13 ± 1.21 81.56 ± 1.03 80.43 ± 1.32 82.08 ± 0.68 85.46 ± 0.36</td></tr></table></body></html>

# Ablation Study

We conduct an ablation experiment to evaluate the contribution of Cosine and Hypervolume terms in MOO loss (Eq. 1), as well as idempotent-like loss (Eq. 4). As shown in Table 3, the results on the multi-task learning datasets indicate a noticeable performance degradation using a single loss term, which proves that the roles of terms Consine and Hypervolume are indispensable. The results on the adaptation datasets show that the idempotent-like loss significantly improves the adaptability of Hypernetwork. Note that adopting idempotent-like loss alone is pointless, as the other two are indispensable constraints.

Table 3: Ablation results of loss types. Cos: cosine loss term, Hyp: Hypervolume loss term, Ide: idempotent-like loss.   

<html><body><table><tr><td>Loss Type</td><td>Multi-MNIST Multi-Fashion Fashion-MNIST</td><td></td><td></td></tr><tr><td>Cos</td><td>83.12 ± 0.4</td><td>82.71 ± 0.3</td><td>80.21 ± 1.1</td></tr><tr><td>Hyp</td><td>85.35 ± 0.2</td><td>83.01 ± 0.2</td><td>81.14 ± 0.5</td></tr><tr><td>Cos +Hyp</td><td>95.17 ± 0.2</td><td>95.69 ± 0.1</td><td>94.87 ± 1.3</td></tr><tr><td>Cos + Ide</td><td>82.51 ± 1.3</td><td>81.73 ± 1.4</td><td>79.85 ± 1.1</td></tr><tr><td>Hyp+Ide</td><td>86.01 ± 0.5</td><td>82.89 ± 0.6</td><td>80.84 ± 1.4</td></tr><tr><td>Cos+Hyp+Ide</td><td>95.19 ± 1.5</td><td>95.72 ± 1.8</td><td>94.93 ± 1.2</td></tr><tr><td>Loss Type</td><td>PACS</td><td>DomainNet</td><td>Office-Home</td></tr><tr><td>Cos</td><td>77.89 ± 1.3</td><td>56.93 ± 0.5</td><td>76.61 ± 0.8</td></tr><tr><td>Hyp</td><td></td><td></td><td></td></tr><tr><td>Cos + Hyp</td><td>77.21 ± 0.4 79.78 ± 1.6</td><td>56.01 ± 1.1 58.61 ± 0.4</td><td>75.32 ± 0.3 78.53 ± 0.2</td></tr><tr><td>Cos+ Ide</td><td>81.52 ± 1.1</td><td>59.13 ± 0.3</td><td>80.17 ± 0.5</td></tr><tr><td>Hyp + Ide</td><td>83.41 ± 1.6</td><td>60.42 ± 1.6</td><td>82.07 ± 0.4</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>Cos+Hyp+Ide</td><td>86.18 ± 1.2</td><td>62.17 ± 0.4</td><td>85.58 ± 0.8</td></tr></table></body></html>

We also evaluate the effectiveness of IOP under different objective function numbers (OFN) and dimensions of Hypernetwork’s input/output (DHIO) on classic MOO problems 2OP (Lin et al. 2019), Bi-CVRP (Zajac et al. 2021), Bi-KP (Ishibuchi et al. 2014), 3OP (Lin et al. 2019), Tri-TSP (Lust et al. 2010), and Tri-CVRP (Zajac et al. 2021). Table 4 shows that the varying DHIO won’t impact the optimization – there is negligible performance difference on metrics $H V$ and OS in different problems. The training cost, however, will increase with the growth of OFN or DHIO, stemming from upsampling the samples and calculating the similarity between samples and the loss vector (Eq. 1).

<html><body><table><tr><td rowspan="2">Problems</td><td colspan="3">DHIO=105</td><td colspan="3">DHIO=107</td></tr><tr><td>HV</td><td>OS</td><td>Cost</td><td>HV</td><td>OS</td><td>Cost</td></tr><tr><td></td><td colspan="6">OFN=2</td></tr><tr><td>2OP Bi-CVRP</td><td colspan="6">0.26 ±0.1 134 ±3 4.17 ±0.20.25 ±0.1 133 ±3 4.58 ± 0.3 0.42 ±0.2 251 ±54.42 ±0.10.41 ±0.1249 ±44.84 ±0.3</td></tr><tr><td>Bi-KP</td><td colspan="6">0.44±0.2 202 ±5 4.63 ±0.50.43 ±0.2199±45.02 ± 0.2 OFN=3</td></tr></table></body></html>

Table 4: Experimental results on Hypervolume $( H V )$ , number of optimal solutions (OS), and training cost (GFlops, $\scriptstyle \mathbf { G } = 1 0 ^ { 9 }$ ) under different objective function numbers (OFN) and dimensions of Hypernetwork’s input/output (DHIO).

0.87 0.65 0.87 0.63 U a 0.83 0.61 N 0.59 U0.79 0.75PH AP CA SK 0.57CL RL SK IN PA QU 0.75AR CL PR RW Domains of PACS (CS) Domains of DomainNet (CS) Domains of OfficeHome (CS) 0.87 0.65 0.87 0.63 V a 0.83 0.61 U A0.59 u 0.79 A 0.75PH AP CA SK 0.57CLRISK IN PAQU 0.75AR CL PR RW Domains of PACS (CL) Domains of DomainNet (CL) Domains of OfficeHome (CL) H(x) H(H(x/y)) H(H(H(x/y))) H(H(H(H(x/y))))

# Iterative Input for Improving the Performance

Our idempotent-like optimization endows the Hypernetwork with the characteristic of receiving iterative inputs. We evaluate its contribution to improving Hypernetwork performance in Fig. 6. Specifically, we randomly select 100 samples from each domain of the dataset and apply our Hypernetwork multiple times to each sample (up to four), generating four levels of target networks for each sample. We use two input methods: (1) Consistent Sample (CS): Applying the Hypernetwork consecutively on the same sample $x$ , i.e., $H ( x ; \psi )$ , $H ( H ( x ; \psi ) ; \psi )$ , $H ( \dot { H } ( H ( x ; \psi ) ; \psi ) ; \psi ) $ , and $H ( H ( H ( \boldsymbol { H } ( \boldsymbol { x } ; \psi ) ; \psi ) ; \psi ) ; \psi ) ; \psi )$ denote the four levels of target networks. (2) Consistent Label (CL): For each sample $x$ , we randomly sample $y$ with the same label as $x$ from the same domain and apply the Hypernetwork consecutively on $y$ , i.e., $H ( x ; \psi ) , H \bar { ( H ( y ; \psi ) ; \psi ) }$ , $H ( H ( H ( y ; \psi ) ; \psi ) ; \psi )$ , and $H ( H ( H ( y ; \psi ) ; \psi ) ; \psi ) ; \psi ) ; \psi )$ denote the four levels of target networks. Fig. 6 shows the accuracy distribution of these target networks. When using the CS input method, not all first-level target networks (apply Hypernetwork once) achieve optimal performance in each domain. Applying the Hypernetwork two or three times can improve the accuracy of target networks. Almost all the target networks achieve upward performance alignment (UPA) when applying the Hypernetwork four times, showcasing cohesion and obtaining a smaller interquartile range. Although there are more non-optimal second-level or third-level target networks for the CL input method than CS, similar UPA can still be achieved after consecutively applying the Hypernetwork to samples with the same label. Experimental results indicate that IOP pioneers a new way to improve Hypernetwork performance by steering the model through iterative inputs for more efficient parameter updates.

# General Applicability of IOP

We apply IOP to the optimization of multimodal model Vision-LLMv2 (VLLMv2) (Wu et al. 2024), which includes dialogue, image, and object recognition data simultaneously. We consider VLLMv2 as the target network and connected to our Hypernetwork. Optimizing different types of data in VLLMv2 inevitably leads to conflicts. Therefore, we take the loss function of different types of data used in VLLMv2 as the balancing objective and use the idempotent-like objective to improve VLLMv2’s adaptability in downstream tasks. The results in Table 5 indicate that IOP improve the performance of dialogue, recognition, reasoning, and generalization tasks by $2 . 4 \%$ , $6 . 2 \%$ , $3 . 7 \%$ , and $7 . 5 \%$ , separately, which proves the effectiveness of IOP.

<html><body><table><tr><td>Methods</td><td>|VQAv2|GQA|POPE|</td><td></td><td>SEED</td><td>COCo</td></tr><tr><td>VLLMv2 VLLMv2+IOP</td><td>81.2 82.5</td><td>65.2 84.5 66.8 86.2</td><td>63.3/70.3/41.9 65.5/71.2/42.7</td><td>177.2/82.8 81.3/88.6</td></tr><tr><td>Methods</td><td>VCR (Q-A/QA-R/Q-AR)</td><td>1</td><td>OdinW13 (Rabbit/Raccoon/VehiclelAvg)</td><td></td></tr><tr><td>VLLMv2 VLLMv2+IOP</td><td>87.2/88.1/79.1 90.4/91.2 /82.1</td><td></td><td>72.1/56.1/60.3/62.8 77.3 / 61.6/ 63.5 / 67.5</td><td></td></tr></table></body></html>

Table 5: Comparison on dialogue datasets (VQAv2 (Goyal et al. 2017), GQA (Hudson 2019), POPE (Li et al. 2023), SEED (all/image/video) (Ge et al. 2024)), region recognition dataset COCO (mAP/Acc) (Lin et al. 2014), visual commonsense reasoning dataset VCR (Zellers et al. 2019), and object detection generalization dataset OdinW13 (Li et al. 2022). In VCR, Q, A, and R denote question, answer, and rationale, Q-A means that the model needs to select option A conditioned on Q.

# Conclusion

This paper reveals the main limitations of the most existing PHN methods and proposes a novel idempotent-like optimization method on the Pareto front of Hypernetwork, namely IOP. Our theoretical analysis shows that IOP ensures the convergence of the optimization process. Experimental results demonstrate that IOP effectively improves the performance and adaptability of Hypernetwork with low computational cost. In addition, IOP pioneers a new way to improve the performance of Hypernetwork through iterative input.

# Acknowledgments

We would very much like to thank the anonymous reviewers for their valuable and constructive comments. This work is supported, in part by Beijing Natural Science Foundation (No. L241050), in part by the National Natural Science Foundation of China (No. 62402024), in part by the Fundamental Research Funds for the Central Universities. For any correspondence, please refer to Renyu Yang (renyuyang $@$ buaa.edu.cn) and Xudong Liu (liuxd $@$ buaa.edu.cn).