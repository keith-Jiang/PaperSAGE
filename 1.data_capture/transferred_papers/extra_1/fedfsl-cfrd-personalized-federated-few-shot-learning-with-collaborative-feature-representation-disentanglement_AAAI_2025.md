# FedFSL-CFRD: Personalized Federated Few-Shot Learning with Collaborative Feature Representation Disentanglement

Shanfeng Wang1,2, Jianzhao $\mathbf { L i } ^ { 1 , 3 * }$ , Zaitian $\mathbf { L i u } ^ { 1 , 2 }$ , Yourun Zhang1, Maoguo Gong1,

1Key Laboratory of Collaborative Intelligence Systems, Ministry of Education, Xidian University 2School of Cyber Engineering, Xidian University 3Guangzhou Institute of Technology, Xidian University 4Academy of Artificial Intelligence, College of Mathematics Science, Inner Mongolia Normal University sfwang@xidian.edu.cn; lijianzhao $@$ xidian.edu.cn; liuzaitian $@$ stu.xidian.edu.cn; zhangyourun $@$ stu.xidian.edu.cn; gong $@$ ieee.org

# Abstract

Federated few-shot learning (FedFSL) aims to enable the clients to obtain personalized generalization models for unseen categories with only a small number of referenceable samples in the distributed collaborative training paradigm. Most existing FedFSL-related algorithms suffer from domain bias and feature coupling in the presence of data heterogeneity and sample scarcity. In this work, we propose a collaborative feature representation disentanglement (CFRD) scheme for FedFSL to address these issues. After each client receives the global aggregation parameters, the original feature representation is decoupled into global communal features and local personality features with personalized bias representation, to maintain both global consistency and local relevance in the first feature representation disentanglement. On the few-shot metric space about the second feature representation disentanglement, category-independent information is encoded by class-specific and class-irrelevant reconstructions to separate the discriminative features. The proposed scheme collaboratively accomplishes global domain bias feature disentanglement and local category degradation feature disentanglement from client-wise and class-wise. Extensive experiments on three few-shot benchmark datasets conforming to the FedFSL paradigm demonstrate that our proposed method outperforms state-of-the-art approaches in both global generality and local specificity.

# Introduction

Federated learning (FL) is to achieve efficient distributed training among multiple parties or computing nodes while protecting information security, terminal data and personal privacy with model parameters or gradients exchange (Li, He, and Song 2021). Most of the early prevalent FL methods are derived from the FedAvg (McMahan et al. 2017), with the aim of training a global model that performs well on the majority of clients (Mohri, Sivek, and Suresh 2019). However, with the development of federated learning theories and applications, the problems of poor convergence in high data heterogeneity and lack of personalized solutions emerge (Tan et al. 2022a; Huang et al. 2023).

![](images/c448a3055c5cec20fd4a01bde11bcb3bd54eaf279c2d0fdd8c203ef03282579d.jpg)  
Figure 1: The difference between general federated learning and federated few-shot learning (FedFSL). In the FedFSL, the training and testing categories do not overlap in each client, and the local models are designed to learn from a extremely small number of labeled samples and make effective predictions in novel tasks or domains.

Recently, researchers have delved into more robust heterogeneous FL in terms of global model personalization and training personalized local models (Sun et al. 2020). Specifically, the global model personalization mitigates the client drift problem by reducing statistical heterogeneity between client data, such as data augmentation (Jeong et al. 2018), self-balancing data imbalance (Duan et al. 2020), etc. Alternatively, client-specific personalization improves the adaptive performance to local data by collaboratively learning a robust global model, such as using a proximal term for local optimization in FedProx (Li et al. 2020), distribution alignment in MOON (Li, He, and Song 2021), sample weight adjustment in FedNova (Wang et al. 2020), meta-learning for fast local adaptation in PerFedAvg (Fallah, Mokhtari, and Ozdaglar 2020), etc. Besides, parameter decoupling for global and local models (Zhang et al. 2021b, 2023b), and knowledge distillation for global to local parameters (Zhu, Hong, and Zhou 2021; Lin et al. 2020) are investigated to obtain personalized model that is more consistent with local data distributions.

The above studies achieve encouraging results for personalized collaborative training, but the generalizability of the local models for novel categories is achieved by task-specific transfer of the global model with strict robustness requirement (Zhao et al. 2022). This is extremely difficult for clients with only a small amount of data, because the tiny percentage of data scale makes it difficult to accurately adapt global parameters in heterogeneity. Consequently, how to enable the local personalization models to achieve fast generalization of unseen categories with only a small number of samples, is the specific motivation for federated few-shot learning (FedFSL) (See Figure 1). Most existing FedFSL-related approaches only focus on personalized federated learning for different few-shot task-specific adaptations, such as facial expression recognition (Shome and Kar 2021), model lightweight (Sun, Yang, and Zhao 2022) and machine fault diagnosis (Zhang et al. 2021a). While in cases of data heterogeneity and sample scarcity, the domain bias and feature coupling have become the difficulties of FedFSL. In fact, this mainly stems from the challenges posed by the complexity of the client and category feature spaces. On the one hand, each client has its own unique local feature space due to the fact that the data is dispersed across different sources in FL. On the other hand, the metric network in FSL is to obtain a more discriminative feature space, such that samples of the same category are more tightly clustered together, while samples of different categories are more widely dispersed.

Given the above insights, we attempt to collaboratively implement feature decoupling at both the client-wise and class-wise, and propose a collaborative feature representation disentanglement (CFRD). Once each client receives the global aggregation parameters, the original feature representation is decoupled into global features and local personalized features with individualized bias representations, to maintain global consistency and local relevance in the first feature representation decoupling. On the few-shot metric spaces regarding the second feature representation decoupling, the class-relevance reconstruction branch is designed on the basis of the few-shot branch to separate discriminative features to realize both class-specific and class-independent reconstruction. Our contributions can be summarized as follows:

• We propose the client-wise feature representation disentanglement to perform global domain bias for FedFS, to achieve a balance between the generality of the global parameter and the personalization of the local parameters, thus alleviating the domain bias problem in FedFSL.

• We propose the class-wise feature representation disentanglement to decouple recognizable features from classindependent information with class-specific and classindependent feature separation, which can effectively solve the category degradation problem in FedFSL scenarios. • We validate the effectiveness of FedFSL-CFRD on prominent FedFSL benchmarks, showing its superiority and efficiency in feature representation.

# Related Work

# Personalized Federated Learning

Personalized federated learning (PFL) is to improve performance and applicability on non-independently identicallydistributed (non-IID) data by personalizing each client with personalized tuning based on the global model or learning personalized models in the collaborative training process, aiming to provide a more accurate model for each client (Wang et al. 2024; Collins et al. 2021). Specifically, methods for local personalized fine-tuning of the global model include PerFedAvg (Fallah, Mokhtari, and Ozdaglar 2020), FedRep (Collins et al. 2021), etc. Approaches for performing targeted training on the global model to obtain additional personalized models include pFedMe (T Dinh, Tran, and Nguyen 2020), Ditto (Li et al. 2021), etc. The work on obtaining local models with personalized aggregation strategies have FedFomo (Zhang et al. 2021b), Partialfed (Sun et al. 2021) FedALA (Zhang et al. 2023b), etc.

# Few-Shot Learning

Few-shot learning (FSL) refers to the exploration of existing knowledge (e.g., pre-trained models or meta-learning methodologies) to enable effective learning and generalization of models to novel classes with a small amount of data (Sung et al. 2018; Sun et al. 2019). The existing related researches are mainly based on meta-learning (Chen et al. 2021; Jamal and Qi 2019), transfer learning (Sun et al. 2019; Yu et al. 2020), and metric learning (Vinyals et al. 2016; Snell, Swersky, and Zemel 2017; Sung et al. 2018) paradigms. There is still relatively little relevant FedFSL research, mainly focusing on the model replacement (Zhao et al. 2022) or the application of PFL to specific FSL task adaptation (Shome and Kar 2021; Sun, Yang, and Zhao 2022; Zhang et al. 2021a).

# Feature Representation Disentanglement

Feature representation disentanglement aims to decompose the representation of complex data into mutually independent or interpretable features. Existing related researches in FL mainly focus on decoupling between different clients, such as feature disentanglement for different modalities (Chen and Zhang 2024) and global model (Yang et al. 2023). In terms of FSL, feature disentanglement to obtain knowledge distillation information (Li et al. 2023), to obtain more discriminative features (Lin et al. 2021; Cheng et al. 2023), and to improve the quality and diversity of generated samples (Dang et al. 2024) have been investigated.

![](images/22fde083fb17297e1e5114f62cec0e6a3cdccb786db33fa19a045bd2793f2322.jpg)  
Figure 2: The specific schematic of the FedFSL-CFRD with two components client-wise feature disentanglement and class-wise feature disentanglement. In the FedFSL-CFRD, the discrimination of encoder class-irrelevance is first realized by decoupling the global and local feature representations, as shown in the red stream. Then the category decoupling is realized to further enhance the discrimination of class-relevance, as shown in the green stream.

# Methodology

# FedFSL-CFRD Framework

In FedFSL, it is assumed that a total of $M$ clients with their private data $\mathcal { D } _ { i }$ participate in learning collaboratively, where $\mathcal { D } _ { i }$ for each client $c _ { i }$ consists of the training set $\mathcal { D } _ { i } ^ { b a \bar { s } e }$ and the test set $\mathcal { D } _ { i } ^ { n o v e l }$ . For all participating clients, the categories of the training and test sets do not overlap, i.e., $( \bar { \mathcal { D } } _ { i } ^ { b a s e } \cap \mathcal { D } _ { i } ^ { n o v e l } = \emptyset )$ . Consistent with the commonly used episodic training (Qiao et al. 2019; Laenen and Bertinetto 2021) in FSL, $\bar { \mathcal { D } } _ { i } ^ { t r a i n }$ and $\mathcal { D } _ { i } ^ { t e s t }$ are composed of multiple meta-tasks (also known as episodes), where each episode contains a support set $s$ and a query set $\mathcal { Q }$ . In the $N$ -way $K$ -shot setting, the support set $s$ is composed of $K$ labeled samples for each of the $N$ randomly selected categories from the dataset, usually the number of $K$ is very small, while the query set is composed of a certain number of unlabeled samples drawn from within these $N$ categories. For each client $c _ { i }$ , the loss function can be expressed as $\mathcal { L } _ { i } = \mathcal { L } ( \pmb { \theta } _ { i } ; \mathcal { D } _ { i } )$ , where $\pmb { \theta } _ { i }$ represents the local parameters. In FSL, $\pmb \theta _ { i }$ usually contains a feature encoder parameter $\pmb { \theta } _ { i } ^ { f }$ and a metric parameter $\pmb { \theta } _ { i } ^ { m }$ .

In the general $\mathrm { F L }$ , all the participants aim to train collaboratively to obtain a global model $\phi$ with the optimization problem: $\begin{array} { r l } { \operatorname* { m i n } _ { \phi } \mathbb { E } _ { i \in [ M ] } \{ \mathcal { L } _ { \mathcal { D } _ { i } } ( \phi ) \} } & { { } } \end{array}$ . However, in addition to collaboratively learning for the generalized global model $\phi ^ { * }$ , the optimization objective of FedFSL is also to expect each participating client to obtain the personalized parameter that is more tailored to the local data distribution in collaborative learning. Therefore, the optimization problem of FedFSL can be expressed as: $\begin{array} { r l } { \operatorname* { m i n } _ { \{ \theta _ { 1 } , \dots , \theta _ { M } \} } \mathbb { E } _ { i \in [ M ] } \{ \mathcal { L } _ { \mathcal { D } _ { i } } ( \pmb { \theta } _ { i } ) \} } & { { } } \end{array}$ , which aims to obtain the local personalized parameter: $\begin{array} { r l } { \{ \pmb { \theta } _ { 1 } ^ { * } , . . . , \pmb { \theta } _ { M } ^ { * } \} } & { { } = } \end{array}$ $\begin{array} { r } { \arg \operatorname* { m i n } _ { \{ \pmb { \theta } _ { 1 } ^ { * } , \dots , \pmb { \theta } _ { M } ^ { * } \} } \sum _ { i = 1 } ^ { M } \mathcal { L } _ { i } ( \pmb { \theta } _ { i } ; \mathcal { D } _ { i } ) . } \end{array}$

The specific schematic of the FedFSL-CFRD with two components client-wise feature disentanglement and classwise feature disentanglement is shown in Figure 2. The overall collaborative learning paradigm of FedFSL-CFRD follows FL, and the local model update follows the episodic training paradigm of FSL. Overall, in the initialization phase, the server distributes the FSL model to each client, usually containing the encoder and metric parameters. Each client locally trains with one epoch to obtain a local personalized bias features that matches the distribution of the respective data, which is then uploaded to the server to aggregate for a global bias feature to coordinate the subsequent training of the global and personalized parameters. It is worth noting that the global and local models do not have the same structure, the global model $\phi$ has only encoder $( \phi ^ { f } )$ and metric $( \phi ^ { m } )$ parameters, while the local model $\theta$ contains encoder $( \theta ^ { f } )$ , classifier $( \theta ^ { c } )$ , metric $( \theta ^ { m } )$ , and reconstruction $( \theta ^ { r } )$ parameters, where only the metric parameters are shared. In the subsequent global communications, it is actually a two-stage learning process, starting with the decoupling of global and local features, where encoder and classifier are used. Then comes the realization of the decoupling of the category features, where encoder, metric and

Input: $M$ : the number of participating clients; $\mathcal { D } _ { i }$ : local data for each client $c _ { i } ; T$ : the number of global communication rounds; $E _ { o }$ : the number of epochs; $E _ { i }$ : the number of episodes; $N$ : the number of classes in episodes; $K$ : the number of labeled samples per category in the support set; $Q$ : the number of unlabeled samples in the query set.   
Output: $\{ \pmb { \theta } _ { 1 } ^ { * } , . . . , \pmb { \theta } _ { M } ^ { * } \}$ : personalized FSL parameters; $\phi ^ { * }$ : global FSL parameter.   
1: Initialize the local FSL parameters: $\{ \pmb { \theta } _ { 1 } ^ { 0 } , . . . , \pmb { \theta } _ { M } ^ { 0 } \}  \phi ^ { 0 }$ . 2: Initialize the local classifier parameter $\{ \pmb { \theta } _ { 1 } ^ { c , 0 } , . . . , \pmb { \theta } _ { M } ^ { c , 0 } \}$ and reconstruction parameter {θ1r , . $\{ \pmb { \theta } _ { 1 } ^ { r , 0 } , . . . , \pmb { \theta } _ { M } ^ { r , 0 } \}$ .   
3: Train the local parameters $\{ \pmb \theta _ { i } ^ { f , 0 } , \pmb \theta _ { i } ^ { c , 0 } \}$ for each client $c _ { i }$ based on Eq. (1), and upload the local personalized bias features $\{ \overline { { \boldsymbol { e } } } _ { 1 } , . . . , \overline { { \boldsymbol { e } } } _ { M } \}$ .   
4: Server aggregates the global bias feature $\begin{array} { r l } { \overline { { e } } } & { { } = } \end{array}$ $\begin{array} { r } { \frac { 1 } { M } \sum _ { i = 1 } ^ { M } \overline { { \pmb { e } } } _ { i } } \end{array}$ .   
5: for global communication round $t = 1$ to $T$ do   
6: Server sends the global parameter $\phi ^ { t }$ to each client. 7: for $i = 1 , . . . , M$ in parallel do   
8: Update the $\phi _ { i } ^ { f , t } , \overline { { \theta _ { i } ^ { f , t } } } , \theta _ { i } ^ { c , t }$ based on Eq. (2) with $E _ { e p o }$ epochs.   
10: 9: for $\begin{array} { r l } & { \mathbf { r } \epsilon = 1 \mathrm { ~ t o ~ } E _ { e p i } { \bf { d o } } } \\ & { \Gamma _ { \epsilon } \gets \sigma \big ( \{ 1 , . . , C _ { c _ { i } } \} , N \big ) } \\ & { \mathbf { f o r } k \mathrm { ~ i n ~ } \Gamma _ { \epsilon } { \bf { d o } } } \\ & { \quad \mathcal { S } _ { k } \gets \sigma ( { D } _ { i } ^ { \Gamma _ { k } } , K ) . } \\ & { \quad \mathcal { Q } _ { k } \gets \sigma ( { D } _ { i } ^ { \Gamma _ { k } } \setminus { S } _ { k } , Q ) . } \\ & { \quad \boldsymbol { c } _ { k } = \frac { 1 } { N } \sum _ { ( { \pmb { x } } _ { i } , y _ { i } ) \in \mathcal { S } _ { k } } \big ( f _ { \phi _ { i } ^ { f , t } } ( { \pmb { x } } _ { i } ) + e _ { i } ^ { p } \big ) . } \end{array}$   
11:   
12:   
13:   
14:   
15: end for   
16: Update the $\pmb { \theta } _ { i } ^ { f } , \pmb { \theta } _ { i } ^ { m }$ and ${ \pmb \theta } _ { i } ^ { r }$ based on Eqs. (4) and (5).   
17: end for   
18: end for   
19: Server aggregates the global FSL parameter: $\phi ^ { t } = $ 20: e $\sum _ { i = 1 } ^ { M } \frac { 1 } { M } \big \phi _ { i } ^ { t }$ .

reconstruction networks are used. The FedFSL-CFRD detailed process is given in Algorithm 1.

# Client-Wise Feature Disentanglement

In traditional FL (McMahan et al. 2017; Li et al. 2020), each client obtains the feature representation with forward propagation after receiving the global model, i.e., $e _ { i } : = f ( \bar { \mathbf { x } _ { i } } , \bar { \phi _ { i } } )$ . However, due to the domain bias caused by heterogeneous data, the global parameter is not well personalized to appropriate the local data. More importantly, since FSL is based on the episodic training paradigm, it does not necessarily cover the local data distribution in each communication iteration. Therefore, we decouple the representations obtained by the feature extractor into global and personalized features, i.e., $\boldsymbol { e } _ { i } : = \boldsymbol { e } _ { i } ^ { g } + \boldsymbol { e } _ { i } ^ { p }$ , where $e _ { i } ^ { g }$ and $e _ { i } ^ { p }$ are obtained by the encoder parameters in $\phi _ { i } ^ { f }$ and $\pmb { \theta } _ { i } ^ { f }$ , respectively.

In the initialization, after the clients receive the global parameters (usually containing an encoder $\phi ^ { f }$ and a metric network $\phi ^ { m } )$ ), the encoder and metric network of local parameters inherits the global parameters $( \theta _ { i } ^ { 0 }  \phi ^ { 0 } .$ ), and initializes the classifier $( \pmb { \theta } _ { i } ^ { c , 0 } )$ and discriminator $( \pmb { \theta } _ { i } ^ { c , 0 } )$ parameters. Subsequently, the encoder and the classifier parameters are trained with $\mathcal { D } _ { i }$ in one epoch to obtain the personalized bias features $\overline { { \boldsymbol { e } } } _ { i } : = f ( \mathcal { D } _ { i } , \phi _ { i } ^ { f } )$ as follows:

$$
\begin{array} { r } { \mathcal { L } _ { \mathcal { D } _ { i } } ( \pmb { \theta } _ { i } ^ { f } , \pmb { \theta } _ { i } ^ { c } ) = \mathbb { E } _ { ( \pmb { x } _ { i } , y _ { i } ) \sim \mathcal { D } _ { i } } \big [ \ell _ { C E } \big ( h ( f ( \pmb { x } _ { i } ; \pmb { \theta } _ { i } ^ { f } ) ; \pmb { \theta } _ { i } ^ { c } ) , y _ { i } \big ) \big ] } \end{array}
$$

where $\begin{array} { r } { \ell _ { C E } = - \sum _ { i = 1 } ^ { | { \mathcal D } _ { i } | } ( y _ { i } \log \hat { y } _ { i } ) } \end{array}$ is the cross-entropy loss.

After initial training to obtain personalized bias features $\{ \overline { { e } } _ { 1 } , . . . , \overline { { e } } _ { M } \}$ , all the clients uploads them to the server for aggregation to obtain the global bias feature $\begin{array} { r l } { \overline { { \boldsymbol { e } } } } & { { } = } \end{array}$ M1 iM=1 ei. In the subsequent communication iterations, e is frozen without further updates.

To maintain the privacy and personalized processing of client feature information, during the training process, we only update the personalized bias features locally without uploading it to the server. In the global communication, the global and local encoder parameters are decoupled to emphasize on general features and client-specific personalized features, respectively. The loss function is illustrated as:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathcal { D } _ { i } } ( \phi _ { i } ^ { f } ; \pmb { \theta } _ { i } ^ { f } , \pmb { \theta } _ { i } ^ { c } ) = } \\ & { \mathbb { E } _ { ( \pmb { x } _ { i } , y _ { i } ) \sim \mathcal { D } _ { i } } \big [ \ell \big ( h ( f ( \pmb { x } _ { i } ; \pmb { \phi } _ { i } ^ { f } ) + \pmb { e } _ { i } ^ { p } ; \pmb { \theta } _ { i } ^ { c } ) , y _ { i } ) \big ] + \zeta | | \overline { { \pmb { e } } } _ { i } - \overline { { \pmb { e } } } | | ^ { 2 } } \end{array}
$$

where $e _ { i } ^ { p } : = f ( \pmb { x } _ { i } ; \pmb { \theta } _ { i } ^ { f } )$ , $\zeta$ is a hyperparameter that controls the importance of global bias feature, while $\overline { { e } } _ { i }$ is updated by:

$$
\overline { { \pmb { e } } } _ { i } ^ { t } = ( 1 - \mu ) \overline { { \pmb { e } } } _ { i } ^ { t - 1 } + \mu \overline { { \pmb { e } } } _ { i } ^ { t }
$$

where $\mu$ is a momentum hyperparameter that controls the weight given to the current batch, eit−1 and eit are the average representation vectors of the previous and current batch, respectively.

# Class-Wise Feature Disentanglement

The client-wise feature disentanglement only decouples global and personalized representations for the encoder, i.e., improves the discriminability of class-irrelevant representations. However, due to the data scarcity in FSL, the impact of class-relevance is crucial in only a small number of samples for the metric. Taking this cue, we design the class-wise feature disentanglement branch based on the few-shot branch, which improves the discriminability of class-relevance representations with self-reconfiguration and mutual-reconfiguration.

FSL Branch: We employ the episodic training paradigm to update the local parameters of FSL branch. It is first necessary to construct multiple episodes from $\mathcal { D } _ { i }$ . In the $N$ - way $K$ -shot setting, $N$ categories are randomly selected from $\mathcal { D } _ { i }$ $\ u _ { i } \colon \Gamma _ { \epsilon }  \sigma ( \bar { \{ 1 , . . , C _ { c _ { i } } \} , } N )$ , where $\sigma$ stands for random sampling. Subsequently, $K$ labeled samples are selected from each of these $N$ categories to compose the support set $( S _ { k } \ \gets \sigma ( { \mathcal { D } _ { i } ^ { \Gamma _ { k } } } , K ) )$ , and $Q$ samples are selected from the remaining samples in these $N$ categories to form the query set $( \mathcal { Q } _ { k }  \mathcal { \sigma } ( \mathcal { D } _ { i } ^ { \Gamma _ { k } } \setminus \mathcal { S } _ { k } , Q ) )$ . Then the prototypes for each category in the support set are obtained:

$\begin{array} { r } { c _ { k } = \frac { 1 } { N } \sum _ { ( { \pmb x } _ { i } , { \pmb y } _ { i } ) \in { \pmb S } _ { k } } \big ( f _ { \phi _ { i } ^ { f , t } } ( { \pmb x } _ { i } ) + e _ { i } ^ { p } \big ) } \end{array}$ . Based on this, the parameters of the encoder and metric network are updated by the following loss function:

$$
\begin{array} { l } { \displaystyle \mathcal { L } _ { ( \pmb { x } , y ) \in \mathcal { Q } _ { k } } \big ( \phi _ { i } ^ { f } ; \pmb { \theta } _ { i } ^ { f } , \pmb { \theta } _ { i } ^ { m } \big ) = \frac { 1 } { N \times Q } [ d \big ( ( f _ { \phi _ { i } ^ { f } } ( \pmb { x } ) + e _ { i } ^ { p } ) , c _ { k } \big ) } \\ { \displaystyle + \log \sum _ { k ^ { \prime } } \exp ( - d ( ( f _ { \phi _ { i } ^ { f } } ( \pmb { x } ) + e _ { i } ^ { p } ) , c _ { k } ) ) ] } \end{array}
$$

where $d$ is the metric function that measures the distance of the extracted representations, commonly known as the cosine distance in MatchingNet (Vinyals et al. 2016), Euclidean distance in ProtoNet (Snell, Swersky, and Zemel 2017) and relation module in Relation Net (Sung et al. 2018), etc. Here, we use $\pmb { \theta } _ { i } ^ { m }$ to implement a universal representation.

Class-Relevance Reconstruction Branch: Here, we achieve feature disentanglement fo class-relevance by constructing reconstruction and translation loss. To preserve the feature information and achieve the feature separation, we use a decoder $D$ to combine for classification and FSL branches for feature self-reconstruction and classreconstruction, where the decoder $D$ is constructed from one fully connected block and three convolutional blocks. The specific loss function is as follows:

$$
\begin{array} { l } { \displaystyle \mathcal { L } _ { ( \pmb { x } _ { i } , \pmb { y } _ { i } ) \in { S } _ { k } \cup \mathcal { Q } _ { k } } ( \phi _ { i } ^ { f } ; \pmb { \theta } _ { i } ^ { f } , \pmb { \theta } _ { i } ^ { r } ) = \frac { 1 } { L } \sum _ { i = 1 } ^ { L } | | f _ { \phi _ { i } ^ { f } } ( \pmb { x } ) + e _ { i } ^ { p } , \hat { \pmb { x } } _ { i } | | _ { 1 } } \\ { \displaystyle + \sum _ { i = 1 } ^ { L } | | f _ { \phi _ { i } ^ { f } } ( \pmb { x } ) + e _ { i } ^ { p } , \hat { \pmb { x } } _ { i } ^ { y _ { i } } | | _ { 1 } } \end{array}
$$

where $\pmb { \theta } _ { i } ^ { r }$ denotes the parameter of reconstruction network, $L$ is the number of samples in ϵ, $| | \cdot | | _ { 1 }$ is the $\ell _ { 1 }$ norm. $\hat { \pmb { x } } _ { i }$ and $\hat { \pmb { x } } _ { i } ^ { k }$ are the reconstructed features by encoder $D$ for sample $\mathbf { \boldsymbol { x } } _ { i }$ and prototype $c _ { k }$ .

Specifically, for each episode $\epsilon$ , decoder $D$ takes as input a pair of feature vectors from both branches, one is from an encoder consists of three convolutional blocks, the other is from the average pooling layer. It is aim to reconstruct the original features based on the source of class-related information for each sample $\pmb { x } _ { i }$ .

# Experiments

# Experimental Setup

Datasets: To demonstrate the effectiveness of our proposed FedFSL-CFRD, it is implemented in three benchmark few-shot datasets, Mini-ImageNet (Vinyals et al. 2016), Tiered-ImageNet (Ren et al. 2018) and CIFAR-FS (Bertinetto et al. 2018), respectively.

FSL Homogeneous: All the clients have the same categories in training and test sets, but the training and test categories do not intersect.

FSL Heterogeneous: Neither the training nor the testing categories are intersected between clients, and the respective training and testing samples are also non-overlapping.

Implementation Details: To ensure the fairness, all the experiments are implemented on dual-GPU NVIDIA GeForce RTX 4090, the overall classification accuracy is reported by the benchmark metrics 5-way 1-shot and 5-way 5-shot with $9 5 \%$ confidence intervals. The query samples in each meta-task is 15. we use the ProtoNet (Snell, Swersky, and Zemel 2017) with ResNet-12 backbone as the base model for all the FL comparison methods. The number of channels is [64, 160, 320, 640]. The number of clients $M$ is set to 10 if nothing else is specified. For local model training, the initial value of the learning rate is set to 0.001, the SGD optimizer decay weight is 5e-4, and the momentum is 0.9. All comparison results are reported in the parameters realized for their best results. For the hyperparameters in FedFSL-CFRD, we set the $E _ { o }$ and $E _ { i }$ to 1 and 500, respectively. $\zeta$ and $\mu$ are set to 1 and 0.1, respectively.

# Comparison Results

To verify the superiority of our proposed FedFSL-CFRD, we chose 9 excellent peers for comparison, namely FedAvg (McMahan et al. 2017), FedProx (Li et al. 2020), FedProto (Tan et al. 2022b), PerFedAvg (Fallah, Mokhtari, and Ozdaglar 2020), pFedMe (T Dinh, Tran, and Nguyen 2020), FedFomo (Zhang et al. 2021b), FedALA (Zhang et al. 2023b), GPFL (Zhang et al. 2023a), and FedFSL-Adv (Fan and Huang 2021).

Table 1 gives the comparison results between FedFSLCFRD and excellent peers in terms of 5-way 1-shot and 5- way 5-shot on three benchmark FSL datasets. The experimental results show that FedFSL-CFRD has a significant advantage both in relatively simple and complex scenarios. Traditional FL algorithms (such as FedAvg, FedProx) can achieve decent performance, but there is still a gap compared to PFL algorithms, especially under data heterogeneity. For PFL algorithms, most of them are not designed for FSL scenarios, and thus will perform slightly less well than FedFSL methods (FedFSL-Adv, FedFSL-CFRD) in generalizing to novel classes. In contrast to FedFSL-ADV, which is formulated adversarially to train and optimize the local models to produce samples that can better represent unseen data samples, it is equivalent to adding additional training samples. While FedFSL-CFRD only utilizes the current limited data for decoupling design in feature representation, and outperforms FedFSL-ADV which requires additional pseudosamples in overall performance by about $1 \%$ in both 5-way 1-shot and 5-way 5-shot on three benchmark FSL datasets.

Besides, in addition to the generalizability comparison of the global model, we also present the convergence of the client personalized models as shown in Figure 3. It can be concluded that FedFSL-CFRD achieves stable convergence of the model in fewer communication rounds, which is largely due to the fact that by decoupling the collaborative features of the client-wise and the class-wise, it can improve the discriminative ability for the local data representations to a certain extent, and thus improve the performance of the metrics in the scarcity of samples. Therefore, FedFSLCFRD is not only superior in few-shot accuracy, but also capable of being well in convergence.

Table 1: The test accuracy (in $\%$ ) in 5-way 1-shot and 5-way 5-shot on three benchmark FSL datasets.   

<html><body><table><tr><td rowspan="3">Method</td><td colspan="2">Mini-ImageNet</td><td colspan="2">Tiered-ImageNet</td><td colspan="2">CIFAR-FS</td></tr><tr><td>1-shot</td><td>5-shot</td><td>1-shot</td><td>5-shot</td><td>1-shot</td><td>5-shot</td></tr><tr><td colspan="7">FSL Homogeneous</td></tr><tr><td>FedAvg</td><td>51.32±1.41</td><td>66.67±2.01</td><td>55.63±1.27</td><td>73.72±0.67</td><td>58.15±0.92</td><td>71.05±1.51</td></tr><tr><td>FedProx</td><td>51.71±1.92</td><td>67.16±1.50</td><td>56.27±1.31</td><td>74.02±1.42</td><td>58.90±1.64</td><td>71.67±0.97</td></tr><tr><td>FedProto</td><td>52.39±1.34</td><td>67.77±2.07</td><td>56.91±0.91</td><td>75.35±0.97</td><td>59.66±1.33</td><td>72.14±1.36</td></tr><tr><td>PerFedAvg</td><td>53.09±1.36</td><td>68.40±1.26</td><td>57.50±1.78</td><td>75.39±1.17</td><td>60.40±1.79</td><td>72.72±1.00</td></tr><tr><td>pFedMe</td><td>53.45±1.56</td><td>69.06±1.29</td><td>57.99±1.18</td><td>76.59±1.62</td><td>60.87±1.23</td><td>73.59±1.39</td></tr><tr><td>FedFomo FedALA</td><td>54.13±1.97</td><td>69.66±1.59</td><td>58.65±1.44</td><td>77.52±1.27</td><td>61.38±1.50</td><td>73.39±1.60</td></tr><tr><td>GPFL</td><td>56.05+1.53</td><td>71.36+1.25</td><td>60.03+1.05</td><td>78.05+1.36</td><td>61.97+1.85</td><td>74.06+1.16</td></tr><tr><td>FedFSL-Adv</td><td>56.10+1.74</td><td>71.41+1.14</td><td>60.14+1 55</td><td>78.56+1.48</td><td>62.58+1.84</td><td>74.17+1.12</td></tr><tr><td></td><td>55.75±1.65</td><td>70.59±1.64</td><td>59.88±1.38</td><td>78.19±1.38</td><td>62.45±1.75</td><td>74.02±1.84</td></tr><tr><td>FedFSL-CFRD</td><td>56.66±1.52</td><td>72.15±0.98</td><td>60.52±1.22</td><td>78.95±1.28</td><td>63.00±1.16</td><td>74.58±0.89</td></tr><tr><td>50.05±1.68 FedAvg</td><td colspan="6">FSL Heterogeneous</td></tr><tr><td></td><td colspan="6">65.09±1.90 53.88±1.41 71.25±1.61</td></tr><tr><td>FedProx</td><td>50.76±1.09</td><td>65.70±0.80</td><td>54.26±1.42</td><td>71.96±1.40</td><td>56.15±0.84 56.74±1.57</td><td>69.96±1.20 69.99±0.97</td></tr><tr><td>FedProto</td><td>51.30±1.50</td><td>66.16±1.57</td><td>54.51±1.63</td><td>72.41±1.23</td><td>57.28±1.30</td><td>70.42±1.25</td></tr><tr><td>PerFedAvg</td><td>51.72±1.32</td><td>66.68±0.82</td><td>55.10±1.26</td><td>73.17±1.45</td><td>57.51±1.72</td><td>70.02±1.98</td></tr><tr><td>pFedMe</td><td>52.04±1.65</td><td>67.11±1.03</td><td>55.49±1.51</td><td>73.59±1.39</td><td>57.60±0.88</td><td>71.29±1.69</td></tr><tr><td>FedFomo</td><td>52.47±1.35</td><td>67.77±1.08</td><td>55.72±1.56</td><td>74.37±1.60</td><td>57.83±1.66</td><td></td></tr><tr><td>FedALA</td><td>52.96+0.96</td><td>69.64+1.44</td><td>57.25+1.46</td><td></td><td></td><td>70.79±0.79</td></tr><tr><td>GPFL</td><td>53.15+1.52</td><td>69.53+1.36</td><td>57.47+1.45</td><td>75.86+1.70</td><td>58.96+1.21</td><td>71.17+1.55</td></tr><tr><td></td><td></td><td></td><td></td><td>76.05+1.03</td><td>59.21+0.83</td><td>71.95+1.52</td></tr><tr><td>FedFSL-Adv</td><td>52.52±1.68</td><td>69.56±1.86</td><td>57.34±1.70</td><td>75.74±1.06</td><td>58.93±1.18</td><td>72.00±1.80</td></tr><tr><td>FedFSL-CFRD</td><td>53.89±1.06</td><td>70.42±1.33</td><td>58.04±1.00</td><td>76.64±1.24</td><td>59.36±1.02</td><td>72.44±0.96</td></tr></table></body></html>

65 Client-1 65 Client-2 65 Client-3 65 Client-4 60 60 60 60   
1 建 B 2 Communication Round 50 100 150 200 28 Communication Round 50 100 150 200 2 Communication Round 50 100 150 200 20 Communication Round 50 100 150 200 65 Client-5 65 Client-6 65 Client-7 65 Client-8 60 60 60 60   
自 宝 建 1 2/ 50 100 150 200 2/ 50 100 150 200 20/ 50 100 150 200 28 0 50 100 150200 Communication Round Communication Round Communication Round Communication Round 65 Client-9 65 Client-10 60 60 1 美 2/ 50 100 150 200 2 50 100 150 200 Communication Round Communication Round FedAvg FedProto pFedMe FedALA FedFSL-Adv FedProx PerFedAvg FedFomo GPFL FedFSL-CFRD

<html><body><table><tr><td rowspan="3">Method</td><td colspan="2">Mini-ImageNet</td><td colspan="2">Tiered-ImageNet</td></tr><tr><td>1-shot</td><td>5-shot</td><td>1-shot</td><td>5-shot</td></tr><tr><td colspan="4">FSL Homogeneous</td></tr><tr><td>Basic Basic+Client-Wise Feature Disentanglement</td><td>51.32±1.41</td><td>66.67±2.01</td><td>55.63±1.27</td><td>73.72±0.67</td></tr><tr><td rowspan="3">Basic+Class-Wise Feature Disentanglement FedFSL-CFRD</td><td>51.96±1.75</td><td>67.02±1.94</td><td>55.97±1.45</td><td>73.96±1.53</td></tr><tr><td>53.29±1.68</td><td>69.73±1.64</td><td>58.06±1.39</td><td>76.65±1.64</td></tr><tr><td>56.66±1.52</td><td>72.15±0.98</td><td>60.52±1.22</td><td>78.95±1.28</td></tr><tr><td>Basic</td><td colspan="4">FSLHeterogeneous</td></tr><tr><td rowspan="3">Basic+Client-Wise Feature Disentanglement Basic+Class-Wise Feature Disentanglement</td><td>50.05±1.68</td><td>65.09±1.90</td><td>53.88±1.41</td><td>71.25±1.61</td></tr><tr><td>50.83±1.70</td><td>66.26±1.83</td><td>54.46±1.52</td><td>72.84±1.75</td></tr><tr><td>51.48±1.52</td><td>68.85±1.79</td><td>56.75±1.46</td><td>75.91±1.54</td></tr><tr><td rowspan="2">FedFSL-CFRD</td><td>53.89±1.06</td><td>70.42±1.33</td><td>58.04±1.00</td><td></td></tr><tr><td></td><td></td><td></td><td>76.64±1.24</td></tr></table></body></html>

Table 2: The ablation results (in $\%$ ) in 5-way 1-shot and 5-way 5-shot on Mini-ImageNet and Tiered-ImageNet datasets.

![](images/434e7a635f563c479692eae53d05268693e52d4b6d9780c6511d36f3aca56711.jpg)  
Figure 4: t-SNE visualization comparison results on the Mini-ImageNet dataset. (a) Basic; (b) Basic $+$ Client-Wise Feature Disentanglement; (c) Basic $+$ Class-Wise Feature Disentanglement; (d) FedFSL-CFRD.

# Ablation Study

In ablation experiments, we validate the effectiveness of the individual components of FedFSL-CFRD on MiniImageNet and Tiered-ImageNet datasets. Table 2 presents the ablation experiments for designing client-wise and Class-wise feature disentanglement on the FedFSL framework, where “Basic” refers to the global global aggregation based on the ProtoNet (Snell, Swersky, and Zemel 2017) according to the FSL paradigm.

To make the results more intuitive, Figures 4 and 5 give the t-SNE visualization results of FedFSL-CFRD in both components and all combined in Mini-ImageNet and TieredImageNet, respectively. It is straightforward to observe from the figure that when there is no feature disentanglement, the basic model does not have discriminative ability for complex features. When cline-wise feature disentanglement is designed, the encoder is able to discriminate the categories to a certain extent due to the decoupling of the personalization parameters, but it is limited to the discrimination of class-irrelevance. By adding the class-wise feature disentanglement, the encoder is able to enhance the discrimination of class-relevance in FedFSL-CFRD.

![](images/016a9518d637d9b54598c823580970210369939758bbc5bc92b1f85a8a628b50.jpg)  
Figure 5: t-SNE visualization comparison results on the Tiered-ImageNet dataset. (a) Basic; (b) Basic $+$ Client-Wise Feature Disentanglement; (c) Basic $+$ Class-Wise Feature Disentanglement; (d) FedFSL-CFRD.   
Figure 6: The effect of the number of participating clients in FedFSL-CFRD.

Mini-ImageNet Tiered-ImageNet   
100 100   
90 90   
70 8760   
60   
500 5 10 15 20 500 5 10 15 20 Clients Clients   
$\nleftarrow$ FSL Homogeneous 1-shot $\nleftarrow$ FSL Heterogeneous 1-shot FSL Homogeneous 5-shot FSL Heterogeneous 5-shot

Moreover, Figure 6 provides the effect of the number of participating clients in FedFSL-CFRD. It is worth noting that the total volume of data is constant, and an increase in the number of clients reduces the amount of data or the number of categories each client has, implying an exacerbation of the heterogeneity problem in the FL paradigm. Therefore, the performance degradation of homogeneous data is less obvious than that of heterogeneous data. As the number of clients increases, the performance of FedFSL-CFRD decreases slightly but not significantly, due to the fact that our client-wise feature disentanglement in global generalized features and local personalized feature can mitigate the alleviate the effects of data heterogeneity to a certain extent.

# Conclusion

In this work, we presented a novel collaborative feature representation disentanglement (CFRD) method tailored for federated few-shot learning (FedFSL) scenarios. Our approach addresses the key challenges of data heterogeneity across clients and the scarcity of labeled data within individual clients by decoupling features at both the clientwise and class-wise. By distinguishing between global aggregation parameter and local personalized parameter, as well as class-specific and class-agnostic features, CFRD effectively enhances the discrimination ability to generalize across diverse data distributions and quickly adapt to novel classes. Extensive experiments confirmed the effectiveness of FedFSL-CFRD.

# Acknowledgments

This research was supported by the National Natural Science Foundation of China (Grant Nos. 62476210, 62036006), Natural Science Basic Research Program of Shaanxi (Program Nos. 2024JC-YBMS-485, 2024JC-YBQN-0633).