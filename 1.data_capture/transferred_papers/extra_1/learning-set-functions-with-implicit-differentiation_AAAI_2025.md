# Learning Set Functions with Implicit Differentiation

G¨ozde O¨ zcan, Chengzhi Shi, Stratis Ioannidis

Northeastern University, Boston, MA 02115, USA gozcan, cshi, ioannidis @ece.neu.edu

# Abstract

A recent work introduces the problem of learning set functions from data generated by a so-called optimal subset oracle. Their approach approximates the underlying utility function with an energy-based model, whose parameters are estimated via mean-field variational inference. This approximation reduces to fixed point iterations; however, as the number of iterations increases, automatic differentiation quickly becomes computationally prohibitive due to the size of the Jacobians that are stacked during backpropagation. We address this challenge with implicit differentiation and examine the convergence conditions for the fixed-point iterations. We empirically demonstrate the efficiency of our method on synthetic and real-world subset selection applications including product recommendation, set anomaly detection and compound selection tasks.

# 1 Introduction

Many interesting applications operate with set-valued outputs and/or inputs. Examples include product recommendation (Bonab et al. 2021; Schafer, Konstan, and Riedl 1999), compound selection (Ning, Walters, and Karypis 2011), set matching (Saito et al. 2020), set retrieval (Feng, Zhou, and Lan 2016), point cloud processing (Zhao et al. 2019; Gionis, Gunopulos, and Koudas 2001), set prediction (Zhang, Hare, and Prugel-Bennett 2019), and set anomaly detection (Maˇskov´a et al. 2024), to name a few. Several recent works (Zaheer et al. 2017; Lee et al. 2019) apply neural networks to learn set functions from input/function value pairs, assuming access to a dataset generated by a function value oracle. In other words, they assume having access to a dataset generated by an oracle that evaluates the value of the set function for any given input set.

Recently, Ou et al. (2022) proposed an approximate maximum likelihood estimation framework under the supervision of a so-called optimal subset oracle. In contrast to traditional function value oracles, a label produced by an optimal subset oracle is the subset that maximizes an (implicit) utility set function, in the face of several alternatives. The goal of inference is to learn, in a parametric form, this utility function, under which observed oracle selections are optimal. As MLE is intractable in this setting, Ou et al. (2022) propose performing variational inference instead. In turn, they show that approximating the distribution of oracle selections requires solving a fixed-point equation per sample. However, these fixed-point iterations may diverge in practice. In addition, Ou et al. (2022) implement these iterations via loop unrolling, i.e., by stacking up neural network layers across iterations, and calculating the gradient using automatic differentiation; this makes backpropagation expensive, limiting their experiments to only a handful of iterations.

In this work, we establish a condition under which the fixed-point iterations proposed by Ou et al. (2022) are guaranteed to converge. We also propose a more effective gradient computation utilizing the recent advances in implicit differentiation (Bai, Kolter, and Koltun 2019; Kolter, Duvenaud, and Johnson 2020; Huang, Bai, and Kolter 2021), instead of unrolling the fixed-point iterations via automatic differentiation (Paszke et al. 2017). This corresponds to differentiating after infinite fixed point iterations, while remaining tractable; we experimentally show that this improves the predictive performance of the inferred models.

We make the following contributions:

• We prove that, as long as the multilinear relaxation (Calinescu et al. 2011) of the objective function is bounded, and this bound is inversely proportional to the size of the ground set, the fixed-point iterations arising during the MLE framework introduced by Ou et al. (2022) converge to a unique solution, regardless of the starting point. • We propose a more effective gradient computation by using implicit differentiation instead of unrolling the fixedpoint iterations via automatic differentiation. To the best of our knowledge, we are the first ones to propose utilizing implicit differentiation in the context of learning set functions. • We conduct experiments to show the advantage of our approach on multiple subset selection applications including set anomaly detection, product recommendation, and compound selection tasks. We also show, in practice, that the fixed-point iterations converge by normalizing the gradient of the multilinear relaxation.

The remainder of the paper is organized as follows. We present related literature in Sec. 2. We summarize the learning set functions with optimal subset oracle setting introduced by Ou et al. (2022) in Sec. 3. We state our main contributions in Sec. 4. We present our experimental results in Sec. 5 and we conclude in Sec. 6.

# 2 Related Work

Learning Set Functions from Oracles. There is a line of work where a learning algorithm is assumed to have access to the value of an unknown utility function for a given set (Feldman and Kothari 2014; Balcan and Harvey 2018; Zaheer et al. 2017; Lee et al. 2019; Wendler et al. 2021; De and Chakrabarti 2022). This is the function value oracle setting. Zaheer et al. (2017) and De and Chakrabarti (2022) regress over input set - function value pairs by minimizing the squared loss of the predictions while Lee et al. (2019) minimize the mean absolute error. However, obtaining a function value to a given subset is not an easy task for real-world applications. The value of a set may not be straightforward to quantify or can be expensive to compute. Alternatively, Tschiatschek, Sahin, and Krause (2018) and Ou et al. (2022) assume having access to an optimal subset oracle for a given ground set in the training data. Similarly, we do not learn the objective function explicitly from input set - output value pairs. We learn it implicitly in the optimal subset oracle setting.

Learning Set Functions with Neural Networks. Multiple works aim to extend the capabilities of neural networks for functions on discrete domains, i.e., set functions (Zaheer et al. 2017; Wendler, Pu¨schel, and Alistarh 2019; Soelch et al. 2019; Lee et al. 2019; Wagstaff et al. 2019; Kim et al. 2021; Zhang et al. 2022a; Giannone and Winther 2022). Diverging from the traditional paradigm where the input data is assumed to be in a fixed dimensional vector format, set functions are characterized by their permutation invariance, i.e., the output of a set does not depend on the order of its elements. We refer the reader to a survey about permutation-invariant networks by Kimura et al. (2024) for a more detailed overview. In this work, we also enforce permutation invariance by combining the energy-based model in Sec. 3.1 with deep sets (Zaheer et al. 2017), following the proposed method of Ou et al. (2022) (see also App. A of O¨ zcan, Shi, and Ioannidis (2024)).

Karalias et al. (2022) integrate neural networks with set functions by leveraging extensions of these functions to the continuous domain. Note that, their goal is not to learn a set function but to learn with a set function, which differs from our objective.

Learning Submodular Functions. It is common to impose some structure on the objective when learning set functions. The underlying objective is often assumed to be submodular, i.e., it exhibits a diminishing returns property, while the parameters of such function are typically learned from function value oracles (Dolhansky and Bilmes 2016; Bilmes and Bai 2017; Djolonga and Krause 2017; Kothawade et al. 2020; De and Chakrabarti 2022; Bhatt, Das, and Bilmes 2024; GomezRodriguez, Leskovec, and Krause 2012; Bach 2013; Feldman and Kothari 2014; He et al. 2016). We do not make such assumptions, therefore, our results are applicable to a broader class of set functions.

Implicit Differentiation. In the context of machine learning, implicit differentiation is used in hyperparameter optimization (Lorraine, Vicol, and Duvenaud 2020; Bertrand et al. 2020), optimal control (Xu, Molloy, and Gould 2024), reinforcement learning (Nikishin et al. 2022), bi-level optimization (Arbel and Mairal 2022; Zucchet and Sacramento 2022), neural ordinary differential equations (Chen et al. 2018; Li et al. 2020) and set prediction (Zhang et al. 2022b), to name a few. Inspired by the advantages observed over this widerange of problems, we use implicit differentiation, i.e., a method for differentiating a function that is given implicitly (Krantz and Parks 2002), to learn set functions for subset selection tasks by leveraging the JAX-based, modular automatic implicit differentiation tool provided by Blondel et al. (2022).

Implicit Layers. Instead of specifying the output of a deep neural network layer as an explicit function over its inputs, implicit layers are specified implicitly, via the conditions that layer outputs and inputs must jointly satisfy (Kolter, Duvenaud, and Johnson 2020). Deep Equilibrium Models (DEQs) (Bai, Kolter, and Koltun 2019) and their variants (Winston and Kolter 2020; Huang, Bai, and Kolter 2021; Sittoni and Tudisco 2024) directly compute the fixed-point resulting from stacking up hidden implicit layers by blackbox root-finding methods, while also directly differentiating through the stacked fixed-point equations via implicit differentiation. We adapt this approach when satisfying the fixed-point constraints arising in our setting. The main difference is that in the aforementioned works, implicit layers correspond to a weight-tied feedforward network while in our case, they correspond to a deep set (Zaheer et al. 2017) style architecture.

# 3 Problem Setup

In the setting introduced by Ou et al. (2022), the aim is to learn set functions from a dataset generated by a so-called optimal subset oracle. The dataset $\mathcal { D }$ consists of sample pairs of the form $( S ^ { * } , V )$ , where (query) $V \subseteq \Omega$ is a set of options, i.e., items from a universe $\Omega$ and (response) $S ^ { * }$ is the optimal subset of $V$ , as selected by an oracle. We further assume that each item is associated with a feature vector of dimension $d _ { f }$ , i.e., $\Omega \subseteq \mathbb { R } ^ { d _ { f } }$ . The goal is to learn a set function $F _ { \theta } :$ $2 ^ { \dot { \Omega } } \times 2 ^ { \Omega } \to \mathbb { R }$ , parameterized by $\pmb { \theta } \in \mathbb { R } ^ { d }$ , modeling the utility of the oracle, so that

$$
S ^ { * } = \mathop { \mathrm { a r g } \operatorname* { m a x } } _ { S \subseteq V } F _ { \pmb { \theta } } ( S , V ) ,
$$

for all pairs $( S ^ { \ast } , V ) \in \mathcal { D }$ . As a motivating example, consider the case of product recommendations. Given a ground set $V$ of possible products to recommend, a recommender selects an optimal subset $S ^ { * } \subseteq V$ and suggests these to a user. In this setting, the function $F _ { \pmb { \theta } } ( S , V )$ captures, e.g., the recommender objective, the utility of the user, etc. Having access to a dataset of such pairs, the goal is to learn $F _ { \pmb { \theta } }$ , effectively reverse-engineering the objective of the recommender engine, inferring the user’s preferences, etc.

# 3.1 MLE with Energy-Based Modeling

Ou et al. (2022) propose an approximate maximum likelihood estimation (MLE) by modeling oracle behavior via a Boltzmann energy (i.e., soft-max) model (Murphy 2012; Mnih and Hinton 2005; Hinton et al. 2006; LeCun et al. 2006).

They assume that the oracle selection is probabilistic, and the probability that $S$ is selected given options $V$ is given by:

$$
p _ { \theta } ( S \vert V ) = \frac { \exp \left( F _ { \theta } ( S , V ) \right) } { \sum _ { S ^ { \prime } \subseteq V } \exp \left( F _ { \theta } ( S ^ { \prime } , V ) \right) } .
$$

This is equivalent to Eq. (1), presuming that the utility $F _ { \theta ( \cdot ) }$ is distorted by Gumbel noise (Kirsch et al. 2023). Then, given a dataset $\mathcal { D } \doteq \{ ( S _ { i } ^ { * } , V _ { i } ) \} _ { i = 1 } ^ { N }$ , MLE amounts to:

$$
\underset { \pmb { \theta } } { \arg \operatorname* { m a x } } \sum _ { i = 1 } ^ { N } \left[ \log p _ { \pmb { \theta } } ( S _ { i } ^ { * } \mid V _ { i } ) \right] .
$$

Notice that multiplying $F _ { \pmb { \theta } }$ with a constant $c > 0$ makes no difference in the behavior of the optimal subset oracle in Eq. (1): the oracle would return the same decision under arbitrary re-scaling. However, using $c \cdot F _ { \pmb \theta } ( \cdot )$ in the energybased model of Eq. (2) corresponds to setting a temperature parameter $c$ in the Boltzmann distribution (Murphy 2012; Kirsch et al. 2023), interpolating between the deterministic selection $c  \infty ,$ ) in Eq. (1) and the uniform distribution $( c \to 0 )$ ).1

# 3.2 Variational Approximation of Energy-Based Models

Learning $\pmb \theta$ by MLE is challenging precisely due to the exponential number of terms in the denominator of Eq. (2). Instead, Ou et al. (2022) construct an alternative optimization objective via mean-field variational inference as follows. First, they introduce a mean field variational approximation of the density $p _ { \theta }$ given by $\begin{array} { r } { q ( S , V , \psi ) = \prod _ { j \in S } \hat { \psi _ { j } } \prod _ { j \in V \backslash S } ( 1 - } \end{array}$ $\psi _ { j , \cdot } \rangle$ ), parameterized by the probability vector $\psi$ : this represents the probability that each element $j \in V$ is in the optimal subset $S ^ { * }$ . Then, estimation via variational inference amounts to the following optimization problem:

$$
\begin{array} { r l } & { \mathcal { L } ( \{ \psi _ { i } ^ { * } \} ) = \mathbb { E } _ { \mathbb { P } ( V , S ) } [ - \log q ( S , V , \psi ^ { * } ) ] \approx \quad ( 4 ) } \\ & { \frac { 1 } { N } \displaystyle \sum _ { i = 1 } ^ { N } \left( - \sum _ { j \in S _ { i } ^ { * } } \log \psi _ { i j } ^ { * } - \sum _ { j \in V _ { i } \setminus S _ { i } ^ { * } } \log \big ( 1 - \psi _ { i j } ^ { * } \big ) \right) , } \\ & { \psi _ { i } ^ { * } = \underset { \psi } { \arg \operatorname* { m i n } } \mathbb { K L } ( q ( S _ { i } , V _ { i } , \psi ) \mid \mid p _ { \theta } ( S _ { i } \mid V _ { i } ) ) , } \end{array}
$$

where $\psi _ { i } ^ { * } \in [ 0 , 1 ] ^ { | V | }$ is the probability vector of elements in $V _ { i }$ being included in $S _ { i }$ , $\mathbb { K L } ( \cdot | | \cdot )$ is the Kullback-Leibler divergence, and $p _ { \theta } ( \cdot )$ is the energy-based model defined in Eq. (2). In turn, this is found through the ELBO maximization process we discuss in the next section.

# 3.3 ELBO Maximization

To compute $\psi ^ { * }$ , Ou et al. (2022) show that minimizing the constraint in Eq. (4) via maximizing the corresponding evidence lower bound (ELBO) reduces to solving a fixed

Input: training dataset $\{ ( S _ { i } ^ { * } , V _ { i } ) \} _ { i = 1 } ^ { N }$ , learning rate $\eta$ , number of samples $m$   
Output: parameter $\pmb \theta$   
1: $\pmb \theta \gets$ initialize   
2: repeat   
3: sample training data point $( S ^ { * } , \bar { V } ) \sim \{ ( \bar { S _ { i } ^ { * } } , V _ { i } ) \bar  \} _ { i = 1 } ^ { N }$   
4: initialize the variational parameter $\psi ^ { ( 0 ) } \gets 0 . 5 * { \bf 1 }$   
5: repeat   
6: for $k \gets 1 , \ldots , K$ do   
7: for $j  1 , \ldots , | V |$ in parallel do   
8: sample $m$ subsets $S _ { \ell } \stackrel { \cdot } { \sim } q ( S , ( \psi ^ { ( k - 1 ) } | \psi _ { j } ^ { ( k - 1 ) }  0 ) )$   
9: update variational parameter $\begin{array} { r } { \psi _ { j } ^ { ( k ) }  \sigma ( \frac { 1 } { m } \sum _ { \ell = 1 } ^ { m } [ F _ { \pmb { \theta } } ( S _ { \ell } \cup \{ j \} ) - F _ { \pmb { \theta } } ( S _ { \ell } ) ] ) } \end{array}$   
10: end for   
11: end for   
12: until convergence of $\psi$   
13: update parameter $\pmb \theta$ by unfolding the derivatives of the $K -$ layer meta-network resulting from the fixed-point equations given in Eq. (8) during SGD $\begin{array} { r l } & { \dot { \partial _ { \theta } } \psi ^ { ( K ) } ( \dot { \theta ) }  } \\ & { \quad \partial _ { \theta } \underbrace { \sigma \big ( \nabla _ { \psi ^ { ( K - 1 ) } } \tilde { F } \big ( \cdot \cdot \cdot \big ( \sigma \big ( \nabla _ { \psi ^ { ( 0 ) } } \tilde { F } ( \psi ^ { ( 0 ) } ) \big ) \cdot \cdot \cdot \big ) \big ) } _ { K \mathrm { n e s t e d f u n c t i o n s } } } \\ & { \nabla _ { \theta } \mathcal { L } ( \psi ^ { ( K ) } , \theta )  \nabla _ { \psi ^ { ( K ) } } \mathcal { L } ( \psi ^ { ( K ) } ( \theta ) ) \cdot \partial _ { \theta } \psi ^ { ( K ) } ( \theta ) } \\ & { \theta  \theta - \eta \nabla _ { \theta } \mathcal { L } ( \psi ^ { ( K ) } , \theta ) } \\ & { \cdot \ast \cdot \quad \cdots \quad \cdots \quad \cdots \quad \cdot \epsilon ^ { \Omega } } \end{array}$

14: until convergence of θ

point equation. In particular, omitting the dependence on $i$ for brevity, the constraint in Eq. (4) is equivalent to the following ELBO maximization (Kingma and Welling 2013; Blei, Kucukelbir, and McAuliffe 2017):

$$
\operatorname* { m a x } _ { \boldsymbol { \psi } } \tilde { F } ( \boldsymbol { \psi } , \pmb { \theta } ) + \mathbb { H } ( q ( \boldsymbol { S } , \boldsymbol { V } , \boldsymbol { \psi } ) ) ,
$$

where $\mathbb { H } ( \cdot )$ is the entropy and $\tilde { F } : [ 0 , 1 ] ^ { | V | } \times \mathbb { R } ^ { d }  \mathbb { R }$ is the so-called multilinear extension of $F _ { \theta } ( S , V )$ (Calinescu et al. 2011), given by:

$$
\tilde { F } ( \psi , \pmb \theta ) = \sum _ { S \subseteq V } F _ { \pmb \theta } ( S , V ) \prod _ { j \in S } \psi _ { j } \prod _ { j \in V \backslash S } ( 1 - \psi _ { j } ) .
$$

Ou et al. (2022) show that a stationary point maximizing the ELBO in Eq. (5) must satisfy:

$$
\psi - \pmb { \sigma } ( \nabla _ { \psi } \tilde { F } ( \psi , \pmb { \theta } ) ) = 0 ,
$$

where the function $\pmb { \sigma } : \mathbb { R } ^ { | V | }  \mathbb { R } ^ { | V | }$ is defined as $\sigma ( { \bf x } ) =$ $\left[ \sigma ( x _ { j } ) \right] _ { j = 1 } ^ { | V | }$ and $\sigma : \mathbb { R }  \mathbb { R }$ is the sigmoid function, i.e., $\sigma ( x ) \stackrel { \sim } { = } ( 1 + \exp { ( - x ) } ) ^ { - 1 }$ . The detailed derivation of this condition can be found in App. C.1 of $\ddot { \mathrm { ~ O ~ } }$ zcan, Shi, and Ioannidis (2024).

Observing that the stationary condition in Eq. (7) is a fixed point equation, Ou et al. (2022) propose solving it via the following fixed-point iterations. Given $\pmb \theta \in \mathbb { R } ^ { d }$ ,

$$
\begin{array} { r l } & { \psi ^ { ( 0 ) }  \mathrm { I n i t i a l i z e ~ i n ~ } [ 0 , 1 ] ^ { | V | } , } \\ & { \psi ^ { ( k ) }  \pmb { \sigma } ( \nabla _ { \psi } \tilde { F } ( \psi ^ { ( k - 1 ) } , \pmb { \theta } ) ) , } \\ & { \quad \psi ^ { * }  \psi ^ { ( K ) } , } \end{array}
$$

where $k \in \mathbb { N }$ , and $K$ is the number of iterations. The exact computation of the multilinear relaxation defined in Eq. (6) requires an exponential number of terms in the size of $V$ . However, it is possible to efficiently estimate both the multilinear relaxation and its gradient $\nabla _ { \psi } \tilde { F } ( \psi , \theta )$ via Monte Carlo sampling (see App. C.2 of ¨Ozcan, Shi, and Ioannidis (2024) for details).

# 3.4 DiffMF and Variants

Putting everything together yields the DiffMF algorithm introduced by Ou et al. (2022). For completeness, we summarize this procedure in Alg. 1. In short, they implement the fixed-point iterative update steps in Eq. (8) by executing a fixed number of iterations $K$ , given $\pmb \theta$ , and unrolling the loop: in their implementation, this amounts to stacking up $K$ layers, each involving an estimate of the gradient of the multilinear relaxation via sampling, and thereby multiple copies of a neural network representing $F _ { \pmb { \theta } } ( \cdot )$ (one per sample). Subsequently, this extended network is entered in the loss given in Eq. (4), which is minimized w.r.t. $\pmb \theta$ via SGD.

They also introduce two variants of this algorithm, regressing also $\psi ^ { ( 0 ) }$ as a function of the item features via an extra recognition network, assuming the latter are independent (terming inference in this setting as $\mathtt { E q u i V S e t } _ { \mathrm { i n d } } )$ ) or correlated by a Gaussian copula (Sklar 1973; Nelsen 2006) (termed E $\mathtt { q u i V S e t _ { c o p u l a } } )$ . Compared to DiffMF, both translate to additional initial layers and steps per epoch.

# 3.5 Challenges

The above approach by Ou et al. (2022), and its variants, have two drawbacks. First, the fixed-point iterative updates given in Eq. (8) are not guaranteed to converge to an optimal solution. We indeed frequently observed divergence experimentally, in practice. Without convergence and uniqueness guarantees, the quality of the output, $\bar { \psi } ^ { ( K ) }$ , is heavily dependent on the selection of the starting point, $\boldsymbol { \psi } ^ { ( 0 ) }$ . Moreover, as these iterations correspond to stacking up layers, each containing multiple copies of $F _ { \pmb \theta } ( \cdot )$ due to sampling, backpropagation is computationally prohibitive both in terms of time as well as space complexity. In fact, poor performance due to lack of convergence, as well as computational considerations, led Ou et al. to set the number of iterations to $K \leq 5$ (even $K = 1$ ) in their experiments. We address both of these challenges in the next section.

# 4 Our Approach

Recall from the previous section that minimizing the constraint of the optimization problem given in Eq. (4) is the equivalent of the ELBO in Eq. (5), and the stationary condition of optimizing this ELBO reduces to Eq. (7). Stitching everything together, we wish to solve the following optimization problem:

$$
\begin{array} { r l }  \underset { j _ { i } ^ { * } \} { \operatorname { l i m } } _ { . \boldsymbol { \cdot } } } & { \displaystyle \mathcal { L } ( \{ \psi _ { i } ^ { * } \} ) \approx } \\ & { \displaystyle \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left( - \sum _ { j \in S _ { i } ^ { * } } \log \psi _ { i j } - \sum _ { j \in V _ { i } \setminus S _ { i } ^ { * } } \log \left( 1 - \psi _ { i j } \right) \right) , } \end{array}
$$

subj. to $\pmb { \psi } _ { i } ^ { * } = \pmb { \sigma } ( \nabla _ { \psi } \tilde { F } ( \pmb { \psi } _ { i } ^ { * } , \pmb { \theta } ) )$ , for all $i \in \{ i , \ldots , n \}$ .

To achieve this goal, we (a) establish conditions under which iterations of Eq. (8) converge to a unique solution, by utilizing the Banach fixed-point theorem and (b) establish a way to efficiently compute the gradient of the loss at the fixedpoint by using the implicit function theorem. Our results pave the way to utilize recent tools developed in the context of implicit differentiation (Bai, Kolter, and Koltun 2019; Kolter, Duvenaud, and Johnson 2020; Blondel et al. 2022) to the setting of Ou et al. (2022).

# 4.1 Convergence Condition for the Fixed-Point

Fixed-points can be attracting, repelling, or neutral (Davies 2018; Rechnitzer 2003). We characterize the condition under which the convergence is guaranteed in the following assumption.

Assumption 4.1. Consider the multilinear relaxation $\tilde { F }$ : $[ 0 , 1 ] ^ { | V | } \times \mathbb { R } ^ { d }  \mathbb { R }$ of $F _ { \pmb { \theta } } ( \cdot )$ , as defined in Eq. (6). For all θ Rd,

$$
\operatorname* { s u p } _ { \psi \in [ 0 , 1 ] } | \tilde { F } ( \psi , \pmb \theta ) | < \frac { 1 } { | V | } .
$$

As discussed in Sec. 3, scaling $F _ { \pmb \theta } ( S , V )$ by a positive scalar amounts to setting the temperature of a Boltzmann distribution. Moreover, neural networks are often Lipschitzregularized for bounded inputs and weights (Szegedy et al. 2014; Virmaux and Scaman 2018; Gouk et al. 2021). Therefore, for any such Lipschitz neural network, we can satisfy Asm. 4.1 by appropriately setting the temperature parameter of the EBM in Eq. (2). Most importantly, satisfying this condition guarantees convergence:

Theorem 4.2. Assume a set function $F _ { \pmb { \theta } } : 2 ^ { V }  \mathbb { R }$ satisfies Asm. 4.1. Then, the fixed-point given in Eq. (7) has a unique solution $\psi ^ { * } \in [ 0 , 1 ] ^ { | V | }$ where $\psi ^ { * } = \sigma ( \nabla _ { \psi } \tilde { F } ( \psi ^ { * } , \pmb \theta ) )$ . Moreover, starting with an arbitrary point $\psi ^ { ( 0 ) } \in [ 0 , 1 ] ^ { | V | }$ , $\psi ^ { * }$ can be found via the fixed-point iterative sequence described in Eq. (8) where ${ \mathrm { l i m } } _ { k \to \infty } { \mathrm { \bar { \psi } } } ^ { ( k ) } = \psi ^ { * }$ .

The proof can be found in App. E of ¨Ozcan, Shi, and Ioannidis (2024) and relies on the Banach fixed-point theorem (Banach 1922). Thm. 4.2 implies that as long as $\tilde { F } ( \psi , \theta )$ is bounded and this bound is inversely correlated with the size of the ground set, we can find a unique solution to Eq. (7), no matter where we start the iterations in Eq. (8).

# 4.2 Efficient Differentiation through Implicit Layers

Our second contribution is to disentangle gradient computation from stacking layers together, by using the implicit function theorem (Krantz and Parks 2002). This allows us to use the recent work on deep equilibrium models (DEQs) (Bai, Kolter, and Koltun 2019; Kolter, Duvenaud, and Johnson 2020).

Define $\psi ^ { * } ( \cdot )$ to be the map $\theta \mapsto \psi ^ { * } ( \theta )$ induced by Eq. (7); equivalently, given $\pmb \theta$ , $\psi ^ { * } ( \pmb { \theta } )$ is the (unique by Thm. 4.2) limit point of iterations given in Eq. (8). Observe that, by the chain rule:

$$
\nabla _ { \boldsymbol { \theta } } \mathcal { L } ( \boldsymbol { \psi } ^ { * } ( \boldsymbol { \theta } ) ) = \nabla _ { \boldsymbol { \psi } } \mathcal { L } ( \boldsymbol { \psi } ^ { * } ( \boldsymbol { \theta } ) ) \cdot \partial _ { \boldsymbol { \theta } } \boldsymbol { \psi } ^ { * } ( \boldsymbol { \theta } ) .
$$

The term that is difficult to compute here via backpropagation, that required stacking in Ou et al. (2022), is the Jacobian $\partial _ { \pmb { \theta } } \psi ^ { * } ( \pmb { \theta } )$ , as we do not have the map $\psi ^ { * } ( \cdot )$ in a closed form. Nevertheless, we can use the implicit function theorem (see Thm. D.4 in O¨ zcan, Shi, and Ioannidis (2024)) to compute this quantity.

Indeed, to simplify the notation for clarity, we define a function $G : [ 0 , 1 ] ^ { | V | } \times \mathbb { R } ^ { d } \to [ 0 , 1 ] ^ { | V | }$ , where

$$
G ( \psi ( \pmb { \theta } ) , \pmb { \theta } ) \triangleq \pmb { \sigma } ( \nabla _ { \psi } \tilde { F } ( \psi , \pmb { \theta } ) ) - \pmb { \psi }
$$

and rewrite Eq. (7) as $G ( \psi ( \pmb { \theta } ) , \pmb { \theta } ) = 0$ . Using the implicit function theorem, given in App. D of O¨ zcan, Shi, and Ioannidis (2024), we obtain

$$
\underbrace { - \partial _ { \psi } G ( { \psi } ^ { * } ( \pmb { \theta } ) , \pmb { \theta } ) } _ { A \in \mathbb { R } ^ { | V | \times | V | } } \underbrace { \partial _ { \pmb { \theta } } { \psi } ^ { * } ( \pmb { \theta } ) } _ { J \in \mathbb { R } ^ { | V | \times d } } = \underbrace { \partial _ { \pmb { \theta } } G ( { \psi } ^ { * } ( \pmb { \theta } ) , \pmb { \theta } ) } _ { B \in \mathbb { R } ^ { | V | \times d } } .
$$

This yields the following way of computing the Jacobian via implicit differentiation:

Theorem 4.3. Computing $\partial _ { \pmb { \theta } } \psi ^ { * } ( \pmb { \theta } )$ is the equivalent of solving a linear system of equations, i.e., $\partial _ { \pmb { \theta } } \psi ^ { * } \bar { ( \pmb { \theta } ) } = A ^ { - 1 } B$ ,

$$
\begin{array} { r l } & { A = I - \Sigma ^ { \prime } ( \nabla _ { \psi } \tilde { F } \left( \psi , \pmb { \theta } \right) ) \cdot \nabla _ { \psi } ^ { 2 } \tilde { F } \left( \psi , \pmb { \theta } \right) , a n d } \\ & { B = \Sigma ^ { \prime } ( \nabla _ { \psi } \tilde { F } \left( \psi , \pmb { \theta } \right) ) \cdot \partial _ { \pmb { \theta } } \nabla _ { \psi } \tilde { F } \left( \psi , \pmb { \theta } \right) , } \end{array}
$$

where $\Sigma ^ { \prime } ( { \pmb x } ) = \mathrm { d i a g } \left( [ \sigma ^ { \prime } ( x _ { j } ) ] _ { j = 1 } ^ { | V | } \right)$ , and $\sigma ^ { \prime } ( x ) ~ = ~ ( 1 +$ $\exp { ( - x ) } ) ^ { - 2 } \cdot \exp { ( - x ) }$ .

The proof is in App. F of O¨ zcan, Shi, and Ioannidis (2024). Eq. (12) shows that the Jacobian of the fixed-point solution, $\partial _ { \pmb { \theta } } \psi ^ { * } ( \pmb { \theta } )$ , can be expressed in terms of Jacobians of $G$ at the solution point. This means implicit differentiation only needs the final fixed point value, whereas automatic differentiation via the approach by Ou et al. (2022) required all the iterates (see also (Kolter, Duvenaud, and Johnson 2020)). In practice, we use JAXopt (Blondel et al. 2022) for its out-of-the-box implicit differentiation support. This allows us to handle Hessian inverse computations efficiently (see App. G of ¨Ozcan, Shi, and Ioannidis (2024)).

# 4.3 Implicit Differentiable Mean Field Variation

Putting everything together, we propose implicitly Differentiable Mean Field variation (iDiffMF) algorithm. This algorithm finds the solution of the fixed-point in Eq. (7) by a root-finding method. Then, computes the gradient of the loss given in Eq. (11) by using the result of the implicit function theorem given in Thm. 4.3, and updates parameter $\pmb \theta$ in the direction of this gradient. We summarize this process in Alg. 2.

Input: training dataset $\{ ( S _ { i } ^ { * } , V _ { i } ) \} _ { i = 1 } ^ { N }$ , learning rate $\eta$ , number of samples $m$   
Output: parameter $\pmb \theta$   
1: $\pmb \theta \gets$ initialize   
2: repeat   
3: sample training data point $( S ^ { * } , \bar { V } ) \sim \{ ( \bar { S _ { i } ^ { * } } , V _ { i } ) \bar  \} _ { i = 1 } ^ { N }$   
4: initialize the variational parameter $\psi ^ { ( 0 ) } \gets 0 . 5 * { \bf 1 }$   
5: for $j \gets 1 , \dots , | V |$ in parallel do   
6: sample $m$ subsets $S _ { \ell } \stackrel { \_ } { \sim } q ( S , ( \psi | \psi _ { j }  0 ) )$   
7: update variational parameter $\begin{array} { r } { \downarrow _ { j _ { j } } ^ { * }  \sigma ( \frac { 1 } { m } \sum _ { \ell = 1 } ^ { m } \mathsf { \bar { \Phi } } _ { } ^ { * } [ F _ { \theta } ( S _ { \ell } \cup \{ j \} ) - F _ { \theta } ( S _ { \ell } ) ] ) } \end{array}$   
8: end for   
9: update parameter $\pmb \theta$ by computing Eq. (11) through Thm. 4.3 $\begin{array} { r l } & { \partial _ { \theta } \psi ^ { * } ( \theta )  A ^ { - 1 } B \mathrm { ( s e e ~ T h m . ~ } 4 . 3 ) } \\ & { \nabla _ { \theta } \mathcal { L } ( \psi ^ { * } , \theta )  \nabla _ { \psi ^ { * } } \mathcal { L } ( \psi ^ { * } ( \theta ) ) \cdot \partial _ { \theta } \psi ^ { * } ( \theta ) } \\ & { \theta  \theta - \eta \nabla _ { \theta } \mathcal { L } ( \psi ^ { * } , \theta ) } \\ & { . . . } \end{array}$ $\pmb \theta$

10: until convergence of

To emphasize the difference between Alg. 1 and Alg. 2, let us focus on lines 13 and 9, respectively. On Line 13 of the pseudo-code for the DiffMF algorithm, gradient of the loss corresponds to

$$
\nabla _ { \boldsymbol { \theta } } \mathcal { L } \left( \boldsymbol { \psi } ^ { ( K ) } \right) = \nabla _ { \boldsymbol { \psi } } \mathcal { L } \left( \boldsymbol { \psi } ^ { ( K ) } \right) \cdot \partial _ { \boldsymbol { \theta } } \boldsymbol { \psi } ^ { ( K ) } ,
$$

where $\psi ^ { ( K ) }$ is a nested function in the form of

$$
\psi ^ { ( K ) } = \pmb { \sigma } ( \nabla _ { \psi } \tilde { F } ( \dots ( \pmb { \sigma } ( \nabla _ { \psi } \tilde { F } ( \psi ^ { ( 0 ) } , \pmb { \theta } ) ) , \dots , \pmb { \theta } ) ) .
$$

Therefore, automatic differentiation has to unroll all $K$ layers during gradient computation. On the other hand, on Line 9 of the iDiffMF algorithm, gradient of the loss is computed through Eq. (11) where $\partial _ { \pmb { \theta } } \psi ^ { * } ( \pmb { \theta } )$ has a closed form formulation as a result of Thm. 4.3.

# 4.4 Complexity

Reverse mode automatic differentiation has a memory complexity that scales linearly with the number of iterations performed for finding the root of the fixed-point, i.e., it has a memory complexity of $\mathcal { O } ( K )$ where $K$ is the total number of iterations (Bai, Kolter, and Koltun 2019). On the other hand, reverse mode implicit differentiation has a constant memory complexity, $\mathcal { O } ( 1 )$ , because the differentiation is performed analytically as a result of using the implicit function theorem. Fig. 1 in Sec. 5 reflects the advantage of using implicit differentiation in terms of space requirements numerically.

In the forward mode, the time complexity of the iterative sequence inside DiffMF is again $\mathcal { O } ( K )$ as the number of iterations is pre-selected and does not change with the rate of convergence. Inside iDiffMF, the convergence rate depends on the Lipschitz constant of the fixed-point in Eq. (7) and the size of the ground set. In particular, the number of iterations required for finding the root of

Eq. (7) is bounded by log (ϵ(1−ω)/√|V |) , where ϵ is the tolerance threshold and $\omega$ is the Lipschitz constant, i.e., $\| \sigma ( \nabla _ { \psi } \tilde { F } \left( \pmb { x } , \pmb { \theta } \right) ) - \sigma ( \nabla _ { \psi } \tilde { F } \left( \pmb { y } , \pmb { \theta } \right) ) \| _ { 2 } \leq \omega \| \pmb { x } - \pmb { y } \| _ { 2 }$ (see App. $\mathrm { ~ H ~ }$ of O¨ zcan, Shi, and Ioannidis (2024) for computation steps). Thus, the root-finding routine inside iDiffMF has $\begin{array} { r } { \mathcal { O } \left( \frac { \log { \left( \epsilon ( 1 - \omega ) / \sqrt { | V | } \right) } } { \log { \omega } } \right) } \end{array}$ time complexity.

# 5 Experiments

We evaluate our proposed method on five datasets including set anomaly detection, product recommendation, and compound selection tasks (see Tab. 1 and App. I of ¨Ozcan, Shi, and Ioannidis (2024) for a datasets summary and for detailed dataset descriptions). The Gaussian and Moons are synthetic datasets, while the rest are real-world datasets. We closely follow the experimental setup of Ou et al. (2022) w.r.t. competing algorithm setup, experiments, and metrics.2

# 5.1 Algorithms

We compare three competitor algorithms from (Ou et al. 2022) to three variants of our iDiffMF algorithm (Alg. 2). Additional implementation details are in App. I of ¨Ozcan, Shi, and Ioannidis (2024).

DiffMF (Ou et al. 2022): This is the differentiable mean field variational inference algorithm described in Alg. 1. As per Ou et al., we set the number of iterations to $K = 5$ for all datasets.

Equi $\mathtt { V S e t } _ { \mathrm { i n d } }$ (Ou et al. 2022): This is the equivariant variational inference algorithm proposed by Ou et al. (2022). It is a variation of the DiffMF algorithm where the parameter $\psi$ is predicted by an additional recognition network as a function of the data. As per Ou et al. (2022), we set $K = 1$ for all datasets.

EquiVSetcopula (Ou et al. 2022): A correlation-aware version of the Equi $\mathtt { V S e t } _ { \mathrm { i n d } }$ algorithm where the relations among the input elements are modeled by a Gaussian copula. As per Ou et al. (2022), we set $K = 1$ for all datasets.

iDiffMF (Alg. 2): Our proposed implicit alternative to the DiffMF algorithm where we solve the fixed-point condition in Eq. (7) with a low tolerance threshold $( \epsilon = 1 0 ^ { - 6 }$ ), instead of running the fixed-point iterations in Eq. (8) for only a fixed number of times. Although DNNs are bounded, the exact computation of their Lipschitz constant is, even for two-layer Multi-Layer-Perceptrons (MLP), NP-hard (Virmaux and Scaman 2018). In our implementation, we use several heuristic approaches to satisfy the condition in Asm. 4.1. First, we multiply the multilinear relaxation $\tilde { F }$ by a constant scaling factor $2 / ( | V | c )$ , treating $c$ as a hyperparameter. We refer to this as iDiff $\mathtt { M F } _ { c }$ . We also consider a dynamic adaptation per batch and fixed-point iteration, normalizing the gradient of the multilinear relaxation by its norm as well as size of the ground set; we describe this heuristic in App. I.3 of ¨Ozcan, Shi, and Ioannidis (2024). We propose two variants, termed iDiff $\mathtt { M F } _ { 2 }$ and iDif $\mathsf { \Pi } _ { \mathsf { \Pi } } ^ { \mathsf { = } } \mathbb { M } \mathbb { F } _ { \ast }$ , using $\ell _ { 2 } \left( \left\| \cdot \right\| _ { 2 } \right)$ and nuclear $( \lVert \cdot \rVert _ { * } )$ norms when scaling, respectively.

For all algorithms, we use permutation-invariant NN architectures as introduced by Ou et al., described in App. I.6 of ¨Ozcan, Shi, and Ioannidis (2024). We report all experiment results with the best-performing hyperparameters based on a 5-fold cross-validation. More specifically, we partition each dataset to a training set and a hold out/test set (see Tab. 1 of ¨Ozcan, Shi, and Ioannidis (2024) for split ratios). We then divide the training dataset in 5 folds. We identify the best hyperparameter combination through cross-validation across all folds. To produce standard-deviations, we then report the mean and the standard variation of the performance of the 5 models trained under the best hyperparameter combination on the test dataset.

We explore the following hyper-parameters: learning rate $\eta$ , number of layers $L$ , and different forward and backward solvers. Additional details, including ranges and optimal hyperparameter combinations, can be found in App. I.7 of O¨ zcan, Shi, and Ioannidis (2024).

We use the PyTorch code repository provided by Ou et al. (2022) for all three competitor algorithms.3 We use the $\mathbf { J } \mathbf { A } \mathbf { X } { + } \mathbf { F } \mathbf { l } { \mathrm { a } } \mathbf { x }$ framework (Bradbury et al. 2018; Frostig, Johnson, and Leary 2018; Heek et al. 2023) for its functional programming abilities for our iDiffMF implementations. In particular, we implement implicit differentiation using the JAXopt library (Blondel et al. 2022). It offers a modular differentiation tool that can be combined with the existing solvers and it is readily integrated in JAX. We include our code in the supplementary material and will make it public after the review process.

# 5.2 Metrics

Following Ou et al. (2022), we measure the performance of different algorithms by (a) using the trained neural network to predict the optimal subsets corresponding to each query on the test set, and (b) measure the mean Jaccard Coefficient (JC) score across all predictions. We describe how the trained objective $F _ { \theta } ( \cdot )$ is used to produce an optimal subset $\hat { S } _ { i } ^ { * }$ given query $V _ { i }$ in the test set in App. I.5 of ¨Ozcan, Shi, and Ioannidis (2024).

We also measure the running time and the GPU memory usage of the algorithms. During training, we track the amount of memory used every 5 seconds with the nvidia-smi command while varying the number of maximum iterations. For each number of maximum iterations, we report the minimum, maximum, and average memory usage.

# 5.3 Results

We report the predictive performance of our proposed iDiff $\mathsf { M F } _ { 2 }$ and iDif $\mathtt { M F } _ { * }$ methods against the existing DiffMF method and its variants on Tab. 1, and iDiffMF $c$ in App. I.7 of ¨Ozcan, Shi, and Ioannidis (2024). For the vast majority of the test cases, iDiffMF variants achieve either the best or the second-best JC score. While the next best competitor, EquiVSetcopula, performs the best on some

<html><body><table><tr><td rowspan="2"></td><td rowspan="2">Datasets</td><td>EquiVSetind</td><td>EquiVSetcopula</td><td>DiffMF</td><td>iDiffMF2</td><td>iDiffMF*</td></tr><tr><td>Test JC</td><td>Test JC</td><td>Test JC</td><td>Test JC</td><td>Test JC</td></tr><tr><td rowspan="2">AD</td><td rowspan="2">CelebA Gaussian Moons</td><td>55.02 ±0.20</td><td>56.16 ±0.81</td><td>54.42 ±0.70</td><td>56.30±0.58</td><td>56.55 ±0.49</td></tr><tr><td>90.55 ±0.06</td><td>90.94 ±0.09</td><td>90.96 ±0.05</td><td>90.95 ±0.18</td><td>91.03±0.09</td></tr><tr><td rowspan="9"></td><td rowspan="9">apparel bath bedding carseats furniture gear health feeding</td><td rowspan="9"></td><td>58.67 ±0.18 78.19±0.89</td><td rowspan="9">58.45 ± 0.15 67.66 ±0.39</td><td rowspan="9">58.48±0.15</td><td>58.97 ± 0.04</td></tr><tr><td>57.76 ± 0.11 68.45 ±0.96</td><td>70.60 ±1.35 76.13 ± 4.65</td><td>73.80 ± 5.71</td></tr><tr><td>67.51 ± 1.19 77.72±1.98</td><td>71.87 ±0.27 77.68±0.98</td><td>76.43±0.81</td></tr><tr><td>66.20 ±1.10 79.29 ± 1.01</td><td>77.26 ± 1.24 20.03 ± 0.15</td><td>77.88±0.80 76.94 ± 1.05 21.94 ± 1.42</td></tr><tr><td></td><td>20.15 ± 0.65</td><td>22.47 ± 1.04</td></tr><tr><td></td><td></td><td></td></tr><tr><td>71.46 ±0.43</td><td>77.44 ±0.46 81.93 ±1.00 16.84 ±0.05</td><td>81.52 ± 1.84</td></tr><tr><td>17.28 ±0.88</td><td>82.47 ±0.19 17.95 ± 0.80</td><td>19.93 ± 2.68 18.69 ±0.93</td></tr><tr><td>65.35 ±0.91 77.33±0.90</td><td>66.06±2.86</td><td>73.90 ± 10.29 73.57±6.74</td></tr><tr><td rowspan="2"></td><td rowspan="2">media safety toys</td><td>63.04 ±0.41 56.60 ±0.56</td><td>72.03±0.77</td><td rowspan="2">59.64 ±0.81 51.32 ± 1.11 24.66 ± 5.56</td><td>72.55 ±1.10</td><td>72.32 ± 1.03</td></tr><tr><td>21.99 ± 1.85</td><td>55.73 ± 1.18 22.09±3.30</td><td>56.39±2.68 26.02 ±1.68</td><td>55.58 ±1.75 25.38 ± 1.88</td></tr><tr><td rowspan="2"></td><td rowspan="2">BindingDB</td><td>62.36 ± 1.31</td><td>69.08 ±1.04</td><td>64.39 ± 1.64</td><td>68.53 ±1.35</td><td>68.91 ±1.00</td></tr><tr><td>73.59±0.75</td><td>73.57±2.05</td><td>73.22 ±1.08</td><td>76.83 ±0.50</td><td>77.48±1.04</td></tr></table></body></html>

Table 1: Test Jaccard Coefficient (JC) for set anomaly detection (AD), product recommendation (PR), and compound selection (CS) tasks, across all five algorithms. iDiff $\mathtt { M F } _ { 2 }$ and iDiff $\mathtt { M F } _ { * }$ correspond to our algorithm with Frobenius and nuclear norm scaling. Bold and underline indicate the best and second-best performance results, respectively. The confidence intervals on the table come from the standard variation of the measurements between folds during cross-validation.

![](images/41c4e390c0e36bc3841ba90857887ab1b1afea142df2009fc2d1af9589bb2afd.jpg)  
Figure 1: Effects of the choice of differentiation method on the relationship between the allocated GPU memory and the number of fixed-point iterations across different datasets. Blue lines represent automatic differentiation (DiffMF), while the orange lines represent implicit differentiation (iDiffMF). The markers denote the average memory usage. The area between the recorded minimum and maximum memory usage is shaded.

datasets, its performance is not consistent on the remaining datasets, not being even the second best. For the Amazon carseats, furniture and safety datasets, iDiffMF variants give significantly better results than $\mathtt { E q u i V S e t } _ { \mathrm { c o p u l a } }$ , even though EquiVSetcopula is faster. This is probably because EquiVSetcopula converges to a local optimum and finishes training earlier. It is also important to highlight that we evaluate iDiffMF using JAX $+$ Flax while we use PyTorch to evaluate the baselines. Therefore, the differences in running time can also be explained with the framework differences. Even though iDiffMF executes fixed-point iterations until convergence, as opposed to $K = 1$ or $K = 5$ in remaining methods (Ou et al. 2022), the average running times are comparable across datasets (see Tab. 2 of ¨Ozcan, Shi, and Ioannidis (2024)).

In Fig. 1, we demonstrate the advantages of using implicit differentiation in terms of space complexity. As discussed in Sec. 4.4, memory requirements remain constant in an interval as the number of fixed-point iterations increases during implicit differentiation. On the contrast, memory requirements increase linearly with the number of iterations during automatic differentiation.

# 6 Conclusion

We improve upon an existing learning set functions with an optimal subset oracle setting by characterizing the convergence condition of the fixed point iterations resulting during MLE approximation and by using implicit differentiation over automatic differentiation. Our results perform better than or comparable to the baselines for the majority of the cases without the need of an additional recognition network while requiring less memory.

# Acknowledgments

We gratefully acknowledge support from the National Science Foundation (grant 1750539).