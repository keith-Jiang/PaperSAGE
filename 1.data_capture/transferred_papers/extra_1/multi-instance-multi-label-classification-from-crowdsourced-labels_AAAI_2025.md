# Multi-Instance Multi-Label Classification from Crowdsourced Labels Ziquan Wang1,2, Mingxuan $\mathbf { X _ { i a } } ^ { 1 , 2 }$ , Xiangyu $\mathbf { R e n } ^ { 3 }$ , Jiaqing Zhou4, Gengyu Lyu5, Tianlei ${ \bf { H } } { \bf { u } } ^ { 1 , 2 }$ , Haobo Wang1,2∗

1School of Software Technology, Zhejiang University 2 Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security 3College of Computer and Control Engineering, Northeast Forestry University 4 ByteDance, Hangzhou 5 College of Computer Science, Beĳing University of Technology zqwang $@$ buaa.edu.cn, ren_xy $@$ nefu.edu.cn, xiamingxuan $@$ zju.edu.cn, jiashu $@$ bytedance.com, lyugengyu $@$ gmail.com, htl $@$ zju.edu.cn, wanghaobo $@$ zju.edu.cn

# Abstract

Multi-instance multi-label classification (MIML) is a fundamental task in machine learning, where each data sample comprises a bag containing several instances and multiple binary labels. Despite its wide applications, the data collection process involves matching multiple instances and labels, typically resulting in high annotation costs. In this paper, we study a novel yet practical crowdsourced multi-instance multilabel classification (CMIML) setup, where labels are collected from multiple crowd sources. To address this problem, we first propose a novel data generation process for CMIML, i.e., cross-label transition, where cross-label annotation error is more likely to appear rather than previous single-label transition assumption, due to the inherent similarity of localized instances from different classes. Then, we formally define the cross-label transition by cross-label transition matrices which are dependent across classes. Subsequently, we establish the first unbiased risk estimator for CMIML and further improve it through aggregation techniques, along with a rigorous generalization error bound. We also provide a practical implementation of cross-label transition matrix estimation. Comprehensive experiments on six benchmark datasets under various scenarios demonstrate that our algorithm outperforms the baselines by a large margin, validating its effectiveness in handling the CMIML problem.

# 1 Introduction

Multi-instance multi-label classification (MIML) is a fundamental framework for modeling complex real-world data, where each sample is represented as a bag of instances and annotated with multiple labels (Feng and Zhou 2017; Zhou and Zhang 2006). Specifically, the bag-level label is positive if it contains at least one positive instance; otherwise, it is negative. MIML has diverse applications, including text classification (Zhou, Sun, and Li 2009), acoustic classification (Briggs et al. 2012), and gene function prediction (Wu, Huang, and Zhou 2014). Diverse methods tackle MIML challenges, including SVM (Zhou and Zhang 2006), convex optimization techniques (Li et al. 2012), neural networks (Feng and Zhou 2017), and attention-based approaches (Yang,

Tang, and Min 2022). However, these methods rely on precise labels for optimal performance.

Since each bag may contain complex patterns among instances, acquiring precise annotation for MIML is both laborintensive and costly (Nguyen and Raich 2021). When dealing with a large number of classes, annotators may overlook some labels due to limited energy or ability, leading to annotation errors. For instance, in MIML bird acoustic classification tasks, annotators must listen to all instances in each audio bag to detect the presence of specific birds (Briggs et al. 2012). The presence of numerous specific bird species requires repeated checks, increasing the likelihood of errors due to the labor-intensive work. To address this, researchers have explored weakly-supervised approaches for MIML, including semi-supervised MIML (Yang et al. 2019; Xu et al. 2012) and incomplete label MIML classification (Nguyen and Raich 2021). However, in complex and specialized tasks like protein function prediction (Wu, Huang, and Zhou 2014), these semi-supervised setups with missing labels or incomplete annotations may cause performance degradation.

In this work, we address that crowdsourcing (Bragg, Mausam, and Weld 2013; Zhang and Wu 2018), famous for its efficiency and cost-effectiveness, can be a feasible alternative for a full annotation on MIML instead of expensive expertise annotations. However, most existing crowdsourcing methods focus on scenarios where a single instance is associated with a single label (Rodrigues and Pereira 2018; Wei et al. 2023; Gao et al. 2022). Although some studies have addressed crowdsourcing with multiple labels (Rodrigues and Pereira 2018; Wei et al. 2023; Zhang and Wu 2018), the challenge of learning a robust classifier from an uncertain bag of instances remains unsolved, and these approaches also lack theoretical foundations.

To this end, we introduce a novel crowdsourced multiinstance multi-label classification (CMIML) framework and propose a theoretical-driven method that learns a robust MIML classifier end-to-end. Specifically, we excavate the pattern of annotation error in CMIML and propose a novel data generation process called cross-label transition, where the confusion is more likely to appear between classes due to the inherent similarity of localized instances from different classes. By defining the cross-label transition matrix, we establish the first unbiased risk estimator (URE) for the

![](images/dd11c215b75cd1ef0cdb3971069dbb54776298afc5cd2f90ead97156fafc5d09.jpg)  
Figure 1: Drug activity prediction (left) and different transition matrix strategies in multi-label classification (right). The multiple conformers of drug molecules form a "multi-conformer bag", with each having different therapeutic effects. Then, crowdsourced labels from patients are collected. Due to individual differences, the efficacy of patient symptoms may be expressed in different correlation patterns, which can easily lead to cross-label errors.

CMIML problem and subsequently derive aggregated versions from the perspective of bag-level and instance-level, which ensures fast and realistic computation. Then, we devise a practical solution for cross-label transition matrix estimation. A generalization error bound for our UREs is also provided. Experimentally, we validate the performance of our proposed method on three datasets including image classification, text classification, and protein function prediction. Compared to other MIML and weakly-supervised multi-label classification methods, our method demonstrates excellent performance in all three scenarios.

# 2 Methodology

MIML Classification. In MIML, let $\chi$ denote the input space and $\boldsymbol { y } = \{ 0 , 1 \} ^ { C }$ the label space. The MIML dataset $\mathcal { D }$ consists of $B$ bag-label pairs $( { \pmb x } , { \pmb y } )$ . Specifically, the $i$ -th bag contains a set of instances $\{ \pmb { x } _ { 1 } , \ldots , \pmb { x } _ { n _ { i } } \}$ , where $n _ { i }$ is the number of instances in the $i$ -th bag, and each instance $\boldsymbol { x } _ { j } \in$ $\mathbb { R } ^ { d }$ has an unknown instance-level label $y _ { j } \in \{ 0 , 1 \} ^ { C }$ . The $k$ -th class bag-level label $y _ { k }$ is set to 1 if any instance in the bag is relevant to the $k$ -th label; otherwise, $y _ { k }$ is set to 0. We can express this rule in the formula $\begin{array} { r } { \mathsf { y } _ { k } = 1 - \prod _ { j = 1 } ^ { n _ { i } } ( 1 - y _ { j , k } ) } \end{array}$ (Maron and Lozano-Pérez 1997), where $y _ { j , k }$ is the $k$ -th class label for the $j$ -th instance. The goal of MIML is to learn a bag-level classifier $f : X \to y$ obtained by minimizing the following expected risk (Feng et al. 2021, 2023):

$$
R ( f ) = \mathbb { E } _ { ( \mathbf { x } , \mathbf { y } ) \sim P ( X , Y ) } [ \mathcal { L } ( f ( \mathbf { x } ) , \mathbf { y } ) ]
$$

Where $\mathcal { L } : \mathbb { R } { \times } y  \mathbb { R } ^ { + }$ is a MIML loss function, $X ( Y )$ is the random variable on $\chi _ { \mathbf { \lambda } ^ { \prime } } ( y )$ , and $P ( X , Y )$ is the joint probability distribution. Note that a method guarantees risk consistency if an unbiased risk estimator is implemented, namely the risk estimator is equivalent to $R ( f )$ given the same classifier $f$ (Mohri, Rostamizadeh, and Talwalkar 2012).

Crowdsourced MIML Classification. In this paper, we address the crowdsourced MIML classification problem where the true label $\pmb { \ y }$ is not accessible. Instead, weak labels are obtained from multiple crowdsourced annotators for each bag, resulting in a training dataset $\tilde { \mathcal { D } }$ of $B$ bag-label pairs $( \bar { \mathbf { x } } , \{ \tilde { \mathbf { y } } ^ { ( m ) } \} _ { m = 1 } ^ { M } )$ , where $M$ is the number of annotators. Our goal is to establish an unbiased risk estimator $\tilde { R } ( f )$ relative to $R ( f )$ based on $\tilde { \mathcal { D } }$ . For clarity, we list all the notations and their meanings in Table 1.

# 2.1 Cross-Label Transition

Annotation errors are common in crowdsourcing, impacting the robustness of learned models. In this subsection, we explore the pattern of annotation errors in CMIML, highlighting that beyond a lack of expertise in specific classes, confusion between instances of different classes is more prevalent in CMIML. To address this, we introduce a novel data generation process called cross-label transition, defined by cross-label transition matrices. This approach allows us to propose the first unbiased risk estimator for CMIML, which ensures risk consistency in addressing this challenge.

Cross-Label Annotation Error. Unlike previous singlelabel transition strategies (Song et al. 2022; Patrini et al. 2017; Xia et al. 2024), annotation errors in multi-label classification often manifest as cross-label patterns (Carbonneau et al. 2018). In Figure 1, we present an example of drug activity prediction. The input bags consist of multiple conformers of a drug molecule, with no additional information available beyond the conformers. After evaluating therapeutic effects from patients, the crowdsourced efficacy labels are collected. However, due to individual differences, a patient may experience multiple correlated diseases, leading to cross-label annotation errors. These cross-label correlations were neglected by previous work.

Cross-Label Transition Matrix. To deal with the frequent cross-label annotation error in CMIML, we define cross-label transition by cross-label transition matrices:

$$
T ^ { ( p , q , m ) } = ( T _ { i , j } ^ { ( p , q , m ) } ) _ { 2 \times 2 } \in [ 0 , 1 ] ^ { 2 \times 2 }
$$

where $\{ 0 , 1 \} . \ T _ { i , j } ^ { ( p , q , m ) }$ $m ~ \in ~ \{ 1 , 2 , . . . , M \} , p , q ~ \in ~ \{ 1 , 2 , . . . , C \}$ dlaebneoltiensg the -otbhalbailbiteyl fasthteh and $m$ t-th alnanboe-l $i , j ~ \in$ $p$ $i$ $q$   
$j$ . Assuming that the label transition process is independent   
of the samples (Tang, Zhang, and Zhang 2024; Cheng et al. $T _ { i , j } ^ { ( p , q , m ) } =$   
$P ( \tilde { Y } _ { q } ^ { ( m ) } = j | Y _ { p } = i , X = \mathbf { x } ) = P ( \tilde { Y } _ { q } ^ { ( m ) } = j | Y _ { p } = i )$ .

The main advantage of our proposed cross-label transition lies in the following three aspects: $i$ ) Compared to the previous single-label transition assumption (Li et al. 2022) in multi-label classification, which is dependent on every single class, cross-label transition matrices are defined with higher dimensions which are dependent across classes, enabling handling cross-label annotation errors in CMIML. ii) Armed with broader assumptions, the cross-label transition is more general and can also handle the single-label transition challenge since the latter is a subset of the former (see the right portion of Figure 1). iii) Modeling cross-label transition matrices can explore label relationships, which is also important in solving multi-label classification problems (Xu et al. 2022; Min et al. 2023; Liu et al. 2023).

Data Generation Process. With transition matrices, we express the data generation process in the following formula:

$$
P ( \tilde { Y } _ { k } ^ { ( m ) } = j | X = \mathbf { x } ) = \sum _ { t = 1 } ^ { C } \sum _ { i = 0 } ^ { 1 } T _ { i , j } ^ { ( t , k , m ) } P ( Y _ { t } = i | X = \mathbf { x } )
$$

Obviously, when given the original label, the transition probabilities satisfy the following normalization: $\textstyle \sum _ { q = 1 } ^ { C } \bar { \sum } _ { j = 0 } ^ { 1 } T _ { i , j } ^ { ( p , q , m ) } = 1 \bar { \forall p } , m , i .$

Unbiased Risk Estimator. After we define the data generation process of the cross-label transition, we theoretically establish the first unbiased risk estimator for the CMIML problem. The theorem proposed below guarantees risk consistency when solving CMIML. All related proofs in this paper are provided in the supplementary materials.

Theorem 1 (The First URE). Let 푮(푘) ∈ R2푀×2푀 or $\pmb { H } ^ { ( k ) } ( \pmb { x } ) \in \mathbb { R } ^ { 2 ^ { M } \times 1 }$ be the matrix or vector all composed of the element $\begin{array} { r } { \prod _ { m = 1 } ^ { M } \sum _ { t = 1 } ^ { C } T _ { \cdot _ { t } , \cdot _ { m } } ^ { ( t , k , m ) } } \end{array}$ . Then, $R ( f )$ can be equivalently expressed as $\tilde { R } ( f ) \ = \ \mathbb { E } _ { ( \mathbf { x } , \tilde { \mathbf { y } } ) \sim P ( X , \tilde { Y } ) } ( \tilde { L } ( f ( \mathbf { x } ) , \{ \tilde { \mathbf { y } } ^ { ( m ) } \} _ { m = 1 } ^ { M } ) )$ , where $\begin{array} { r } { \tilde { L } ( f ( \mathbf { x } ) , \{ \tilde { \mathbf { y } } ^ { ( m ) } \} _ { m = 1 } ^ { M } ) = \sum _ { k = 1 } ^ { C } \tilde { l } ( f _ { k } ( \mathbf { x } ) , \{ \tilde { \mathbf { y } } _ { k } ^ { ( m ) } \} _ { m = 1 } ^ { M } ) } \end{array}$ and $\forall k , i \tilde { l } ( f _ { k } ( \mathbf { x } ) , \cdot ) = d e t ( { G ^ { ( k ) } | } _ { G _ { \cdot i } ^ { ( k ) } = H ^ { ( k ) } ( \mathbf { x } ) } ) [ d e t ( { G ^ { ( k ) } } ) ] ^ { - 1 }$ 푮(푘) =푯 (푘) x means replacing the 푖-th column with 푯(푘) (x) and $f _ { k }$ is the $k$ -th class prediction by classifier $f$ .

For readability and space, Theorem 1 simplifies complex expressions. The full one is in the supplementary materials. Remark 1. While instance similarity and cross-label annotation errors in CMIML present significant challenges, Theorem 1 shows that determinant calculations using the crosslabel transition matrix can implicitly restore normal learning. Despite its merits, Theorem 1 may suffer from high time costs and accumulated errors in implementation. On the one hand, for large $M$ , calculating the determinant of the $2 ^ { M }$ -order matrix $\breve { G ^ { ( k ) } }$ is extremely time-consuming. On the other hand, the product $\begin{array} { r } { \prod _ { m = 1 } ^ { M } \sum _ { t = 1 } ^ { \dot { C } } T _ { \cdot \cdot \cdot m } ^ { ( t , k , m ) } } \end{array}$ across $M$ transition matrices can lead to unreliable numerical results. Both issues stem from the number of annotators $M$ , and we focus on reducing its influence in what follows.

# 2.2 Improved Version of URE

To make our cross-label transition-based unbiased learning practically applicable, in this subsection, we proposed two improved versions of URE for CMIML, namely, the baglevel URE and the instance-level URE, that greatly reduce the computation cost overhead by aggregation techniques. Inspired by crowdsourcing methods (Ma et al. 2015; Li, Rubinstein, and Cohn 2019) that aggregate labels from different annotators, we aggregate both labels and transition matrices below, which eliminate the influence of $M$ .

Table 1: Notations in this paper.   

<html><body><table><tr><td>Notation</td><td>Meaning</td></tr><tr><td>C/B/M</td><td>the number of classes /bags /annotators</td></tr><tr><td>ni</td><td>the number of instances in the i-th bag</td></tr><tr><td>X/y</td><td>feature space /label space</td></tr><tr><td>X/Y (capital)</td><td>randomvariable on X/y</td></tr><tr><td>xj (italic)</td><td>the j-th instance in the bag X</td></tr><tr><td>yi (italic)</td><td>the label for the j-th instance in the bag x</td></tr><tr><td>x (normal)</td><td>ni the multi-instance bag {x j}j=1</td></tr><tr><td>y (normal)</td><td>the true label for the bag x</td></tr><tr><td>y(m) (tilde)</td><td>the crow anourced abel fobag </td></tr><tr><td>D (tilde)</td><td>crowdsourced MIML training dataset</td></tr></table></body></html>

Definition 1 (Aggregation). Let 퐴 denote the random variable representing the annotator index. The allocation of annotators is generally independent of the true label, i.e., $P ( A = m | X = \mathbf { x } , Y = \mathbf { y } ) = P ( A = m | X = \mathbf { x } )$ . Then,

The Aggregated Label $\bar { \pmb { y } } = ( \bar { \mathsf { y } } _ { 1 } , \bar { \mathsf { y } } _ { 2 } , . . . , \bar { \mathsf { y } } _ { C } )$ is defined as $\begin{array} { r } { \bar { \bf y } _ { i } = \sum _ { m = 1 } ^ { M } P ( A = m | X = \mathbf { x } ) \tilde { \bf y } _ { i } ^ { ( m ) } } \end{array}$ for $i = 1 , 2 , \dots , C$ .

The Aggregated Transition Matrix $\pmb { T } ^ { ( p , q ) } = ( T _ { i , j } ^ { ( p , q ) } ) _ { 2 \times 2 }$ is defined as $T _ { i , j } ^ { ( p , q ) } \ = \ P ( \tilde { Y } _ { q } \ = \ j | Y _ { p } \ = \ i , X \ = \ \mathbf { x } ) \ =$ $\begin{array} { r } { \sum _ { m = 1 } ^ { M } T _ { i , j } ^ { ( p , q , m ) } P ( A = m | X = \pmb { \mathrm { x } } ) } \end{array}$ , where $\tilde { Y } _ { k }$ is the random variable of the aggregated label $\bar { y } _ { k }$ .

Note that even with the elimination of $M$ , the information from multi-source annotation is still preserved. Moreover, this aggregation method, similar to ensemble learning, mitigates high variance from cross-label errors through averaging and enhances overall reliability. With the aggregation defined above, we derive an improved version of the unbiased risk estimator, denoted as the bag level URE.

Theorem 2 (Bag-level URE). Let $\bar { \pmb { y } } = ( \bar { \mathsf { y } } _ { 1 } , \bar { \mathsf { y } } _ { 2 } , . . . , \bar { \mathsf { y } } _ { C } )$ be the aggregated label for a bag $\mathbf { x } ,$ , 푙 be the base loss $\begin{array} { r l } { \mathcal { L } ( f ( \mathbf { x } ) , \mathbf { y } ) = } \end{array}$ $\textstyle \sum _ { k = 1 } ^ { C } l ( f _ { k } ( \mathbf { x } ) , \mathbf { y } )$ and $\phi ~ = ~ \mathcal { A } ^ { - 1 } ~ \cdot ~ \mathcal { B } ~ = ~ ( \phi _ { 1 } , \phi _ { 2 } , . . . , \phi _ { C } ) ^ { T }$ , where $\mathcal { A } = ( T _ { 1 , 1 } ^ { ( i , j ) } - T _ { 0 , 1 } ^ { ( i , j ) } ) _ { C \times C }$ and $\mathcal { B } = ( P ( \tilde { Y } _ { j } = 1 | X =$ $\begin{array} { r } { \mathbf { x } ) - \sum _ { t = 1 } ^ { C } T _ { 0 , 1 } ^ { ( t , j ) } ) _ { C \times 1 } } \end{array}$ . Then, the unbiased risk estimator with respect to $R ( f )$ is $\tilde { R } ( f ) \ = \ \mathbb { E } _ { ( \mathbf { x } , \bar { \mathbf { y } } ) \sim P ( X , \tilde { Y } ) } ( \tilde { L } _ { b } ( f ( \mathbf { x } ) , \bar { \mathbf { y } } ) ) ,$ , where $\begin{array} { r } { \tilde { L } _ { b } ( f ( \mathbf { x } ) , \bar { \mathbf { y } } ) = \sum _ { k = 1 } ^ { C } \tilde { l } _ { b } ( f _ { k } ( \mathbf { x } ) , \bar { \mathbf { y } } _ { k } ) } \end{array}$ ,

$$
\begin{array} { l } { \displaystyle { \tilde { l } _ { b } ( f _ { k } ( { \bf x } ) , \bar { \bf y } _ { k } ) = \frac { 1 } { 2 P ( \tilde { Y } _ { k } = \bar { \bf y } _ { k } | { \cal X } = { \bf x } ) } [ \phi _ { k } l ( f _ { k } ( { \bf x } ) , 1 ) } } \\ { \displaystyle { ~ + ( 1 - \phi _ { k } ) l ( f _ { k } ( { \bf x } ) , 0 ) ] } } \end{array}
$$

Remark 2. Theorem 2, the key URE in this paper, is more practical to implement than Theorem 1. While Theorem 1 requires computing a $2 ^ { M }$ -order matrix with time complexity of $O ( 2 ^ { 3 M } )$ for each sample, Theorem 2 reduces the time complexity to that of standard cross-entropy loss by precomputing a vector $\pmb { \phi }$ of only $C$ dimensions using the crosslabel transition matrices and the noisy posterior probabilities, both of which can be effectively estimated (discussed later). Note that $\pmb { \phi }$ is essentially a label confidence vector representing the estimated true label posterior probability.

Practical Implementation. We approximate the noise of $M$ annotators $\begin{array} { r } { P ( \tilde { Y } _ { k } = i | X = \mathbf { x } ) \approx \frac { 1 } { M } \sum _ { m = 1 } ^ { M } \tilde { \mathsf { y } } _ { k } ^ { ( m ) } = \bar { \mathsf { y } } _ { k } } \end{array}$ , where we assume each annotator tags each bag with uniform contribution. Therefore, we can obtain the matrix $\mathcal { A }$ and vector $\mathcal { B }$ before training, adding minimal overhead to training time.

Additionally, with the improved version of bag-level URE proposed above, we explore the instance-level URE (Lin et al. 2022; Andrews, Tsochantaridis, and Hofmann 2002) since learning an instance-level classifier could further capture instance-label relationships which is more desirable in fine-grained applications. Note that the bag-level predictions can be obtained by instance-level classifiers, i.e., $\begin{array} { r } { \hat { \mathbf { y } } = 1 - \prod _ { j = 1 } ^ { n _ { i } } \bigl ( 1 - g ( \mathbf { x } _ { j } ) \bigr ) } \end{array}$ , where $g$ is the instance-level classifier and $\hat { \pmb { y } }$ is the bag-level prediction.

Corollary 3 (Instance-level URE). The notation is the same as that in Theorem 2. Assume the base loss $l$ is symmetric about labels and predictions $l ( \hat { p } , y ) = l ( 1 - \hat { p } , 1 - y )$ and satisfies the rule of product addition about the prediction $\begin{array} { r } { l ( \prod _ { j = 1 } ^ { n _ { i } } \hat { p _ { j } } , \cdot ) = \sum _ { j = 1 } ^ { \hat { n } _ { i } } \bar { l } ( \hat { p _ { j } } , \cdot ) } \end{array}$ . Then the instance-level URE with respect to $R ( f )$ is $\tilde { R } ( f ) = \mathbb { E } _ { ( \mathbf { x } , \bar { \mathbf { y } } ) \sim P ( X , \tilde { Y } ) } ( \tilde { L } _ { t } ( f ( \mathbf { x } ) , \bar { \mathbf { y } } ) ) ,$ , where $\begin{array} { r } { \tilde { L } _ { t } ( f ( \mathbf { x } ) , \bar { \mathbf { y } } ) = \sum _ { k = 1 } ^ { C } \tilde { l } _ { t } ( f _ { k } ( \mathbf { x } ) , \bar { \mathbf { y } } _ { k } ) } \end{array}$ ,

$$
\begin{array} { l } { { \displaystyle { \tilde { l } _ { t } } \big ( f _ { k } ( { \bf x } ) , { \bar { y } } _ { k } \big ) = \frac { 1 } { 2 P ( { \tilde { Y } } _ { k } = { \bar { y } } _ { k } | { \cal X } = { \bf x } ) } [ \phi _ { k } \sum _ { j = 1 } ^ { n _ { i } } l ( g _ { k } ( { \bf x } _ { j } ) , 1 ) } } \\ { { \displaystyle ~ + ( 1 - \phi _ { k } ) \sum _ { j = 1 } ^ { n _ { i } } l ( g _ { k } ( { \bf x } _ { j } ) , 0 ) ] } \qquad { \mathrm { ( f ~ } } { \bf x } = { \bf x } { \mathrm { ) } } } \end{array}
$$

Remark 3. The cross-entropy loss satisfies the assumptions. The formula $\tilde { l } _ { t }$ allows direct accumulation of instance-level loss, simplifying calculations with an instance-level classifier. The instance prediction vector in Corollary 3 can also be used to calculate instance similarity via methods like inner product or cosine similarity.

In experiments, we implement the beg-level URE as the training objective which turns out to be reliable for CMIML on various datasets, and we leave more studies on instancelevel URE to future works.

# 2.3 Estimation of Transition Matrix

Another challenge for implementing our cross-label transition-based unbiased learning is the estimation of the transition matrices. In this subsection, we detail the estimation process of the aggregated cross-label transition matrices. Specifically, we extend previous estimation methods tailored for noisy label learning on single label (Patrini et al. 2017; Liu and Tao 2015) to the scenario of cross-label transition for multi-label classification.

Theorem 4 (Estimation of Transition Matrix). Let ${ \bf x } _ { \{ p  i , q  j \} } ^ { * } a n d { \bf x } _ { \{ p  i \} } ^ { * }$ be two or single -class anchor point, respectively, i.e. $P ( Y _ { p } = i , Y _ { q } = j | X = \mathbf { x } _ { \{ p  i , q  j \} } ^ { * } ) = 1$ and $P ( Y _ { p } = i | X = \mathbf { x } _ { \{ p  i \} } ^ { * } ) = 1 .$ . Then these anchor points can be used to estimate the probability of cross-label transition.

$$
T _ { i , j } ^ { ( p , q ) } = \{ \begin{array} { l l } { P ( \tilde { Y } _ { q } = j | X = \pmb { \mathrm { x } } _ { \{ p  i , q  1 - j \} } ^ { \ast } ) } & { i f p \neq q } \\ { P ( \tilde { Y } _ { q } = j | X = \pmb { \mathrm { x } } _ { \{ p  1 - i \} } ^ { \ast } ) } & { e l s e } \end{array} 
$$

Theorem 4 shows that the transition matrix can be estimated using single or two-class anchor points. However, since the probabilities for true labels $P ( Y _ { p } = i \lvert X = \mathbf { x } ^ { * } )$ are unavailable, the anchor points must also be estimated. Proposition 5 suggests that the sample point farthest from the classification boundary can serve as the anchor point, as expanded by methods (Xia et al. 2019; Liu and Tao 2015).

Proposition 5 (Estimation of Anchor Point). Let $f$ be the MIML classifier trained with the aggregated labels $\bar { \pmb y }$ . Then the sample point with the highest probability of the classifier 푓 is chosen as the corresponding anchor point: xˆ∗ 푘 푗푘 = arg max $\sum _ { \boldsymbol { \cdot } \in J } [ I _ { [ j _ { k } = 1 ] } f _ { k } ( \mathbf { x } ) + I _ { [ j _ { k } = 0 ] } ( 1 - f _ { k } ( \mathbf { x } ) ) ]$ , where $\boldsymbol { \mathcal { T } }$ is $a$ x A subset of the class set $\{ 1 , 2 , . . . , C \}$ with one or two elements.

With the estimated anchor points, the transition matrix can be readily calculated using Theorem 4. Note that the noise probability can be estimated by $P ( \tilde { Y } _ { q } = j | X = \hat { \mathbf { x } } ^ { * } ) =$ $\bar { \mathsf { y } } _ { q }$ . Nevertheless, experiments show that selecting the top $K$ samples and approximating the transition probability by averaging their noise rates $\textstyle { \frac { 1 } { K } } \bar { \sum _ { k = 1 } ^ { K } } \bar { y } _ { k , q }$ is more efficient than choosing the top one, where $\bar { y } _ { k , q } = P ( \tilde { Y } _ { q } = 1 | X = \hat { \mathbf { x } } _ { k } ^ { * } )$ is the aggregated label for $\hat { \mathbf { x } } _ { k } ^ { * }$ , the $k$ -th anchor point.

In summary, the training process is outlined in appendix. Before formal training, we estimate the cross-label transition matrix and calculate the confidence $\pmb { \phi }$ , which is computationally efficient compared to the training phase. Specifically, we use neural networks to fit aggregated labels $\bar { \pmb { \mathsf { y } } }$ , sort predictions to select the top $k$ anchor points, and then calculate $\pmb { \phi }$ by solving the linear equation system in Theorem 2.

# 2.4 Generalization Bound

Let $\mathcal { F }$ denote the hypothesis set and $H _ { k } \ = \ \{ h | h \ : \textbf { x } \mapsto $ $f _ { k } ( { \mathbf { x } } ) , f \in \mathcal { F } \}$ denote the function space for the $k$ -th class. With the expected Rademacher complexity $\Re _ { n } ( \cdot )$ of the composite space $\cdot$ (Mohri, Rostamizadeh, and Talwalkar 2012), a generalization error bound of our proposed unbiased risk estimator can be justified as the following Theorem.

Theorem 6 (Generalization Error Bound). Assume that the unbiased loss function $\tilde { l } ( f ( \mathbf { x } ) , \mathbf { y } )$ is $L _ { p }$ -Lipschtz continuous with respect to $f ( { \pmb x } )$ , and the base loss function $l$ is upper-bounded by constant $\mathcal { T }$ , let $\mu = m a x \{ L _ { p } , \mathcal { T } \}$ and $\lambda = \operatorname* { m i n } _ { 1 \le k \le C } \operatorname* { m i n } _ { \tilde { \mathsf { y } } _ { k } \in \{ 0 , 1 \} } P ( \tilde { Y } _ { k } = \tilde { \mathsf { y } } _ { k } )$ . Then, $\forall \delta > 0 , f \in \mathcal { F }$ , with probability $\geq 1 - \delta$ over a sample $S = \{ ( \mathbf { x } _ { i } , \bar { \mathbf { y } } _ { i } ) \} _ { i = 1 } ^ { n }$ , we have

$$
R ( f ) \leq \sum _ { i = 1 } ^ { n } { \frac { { \tilde { L } } _ { b } ( f ( \mathbf { x } _ { i } ) , { \bar { \mathbf { y } } } _ { i } ) } { n } } + 2 \sum _ { k = 1 } ^ { C } \Re _ { n } ( { \tilde { l } } \circ H _ { k } ) + { \frac { C \mu } { 2 \lambda } } { \sqrt { \frac { 1 } { 2 n } l n { \frac { 1 } { \delta } } } }
$$

Theorem 6 indicates optimizing the empirical risk can reduce the population level error on the true risk estimator, which is an important theoretical guarantee for training CMIML with the unbiased risk estimator.

<html><body><table><tr><td>Metrics</td><td>Datasets</td><td>MIML-LLMC</td><td>MIMLSVM</td><td>KiSar</td><td>DeepMIML</td><td>PLAIN</td><td>Ours-BCE</td><td>Ours</td></tr><tr><td rowspan="6">Macro-F1↑ a</td><td>Scene</td><td>0.3952</td><td>0.0056</td><td>0.5874</td><td>0.2582</td><td>0.5588</td><td>0.3967</td><td>0.6204</td></tr><tr><td>Reuters</td><td>0.8742</td><td>0.0838</td><td>0.6484</td><td>0.7504</td><td>0.7259</td><td>0.7481</td><td>0.7823</td></tr><tr><td></td><td>0.0295</td><td></td><td>0.0014</td><td></td><td>0.0007</td><td>0.0545</td><td>0.0731</td></tr><tr><td>AzoVin</td><td></td><td>0.0027</td><td></td><td>0.008</td><td></td><td></td><td></td></tr><tr><td>HalMar</td><td>0.1035</td><td>0.0099</td><td>0.0061</td><td>0.0019</td><td>0.0099</td><td>0.1125</td><td>0.0799</td></tr><tr><td>PyrFur</td><td>0.0291</td><td>0.0008</td><td>0.0037</td><td>0.0006</td><td>0.0009</td><td>0.0691</td><td>0.0766</td></tr><tr><td rowspan="6">Coveragel a</td><td>Scene</td><td>1.97</td><td>2.07</td><td>0.95</td><td>3.05</td><td>2.18</td><td>2.48</td><td>1.47</td></tr><tr><td>Reuters</td><td>1.30</td><td>1.66</td><td>0.47</td><td>1.42</td><td>1.53</td><td>1.34</td><td>0.31</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AzoVul</td><td>50.38</td><td>49.77</td><td>62.65</td><td>50.87</td><td>49.28</td><td>59.18</td><td>34.56</td></tr><tr><td>HalMar</td><td>30.76</td><td>32.89</td><td>36.54</td><td>33.90</td><td>40.01</td><td>40.01</td><td>27.03</td></tr><tr><td>PyrFur</td><td>49.61</td><td>57.69</td><td>69.11</td><td>58.22</td><td>51.62</td><td>62.25</td><td>46.80</td></tr><tr><td rowspan="6">AvgRec↑ a</td><td>Scene</td><td>0.3655</td><td>0.2366</td><td>0.5130</td><td>0.2269</td><td>0.6429</td><td>0.5477</td><td>0.6492</td></tr><tr><td>Reuters</td><td>0.9198</td><td>0.4339</td><td>0.6460</td><td>0.8646</td><td>0.7971</td><td>0.9105</td><td>0.9723</td></tr><tr><td>AzoVin</td><td>0.1042</td><td>0.0904</td><td>0.0019</td><td>0.0002</td><td>0.0019</td><td>0.0815</td><td>0.4759</td></tr><tr><td>GeoSul</td><td>0.1489</td><td>0.1234</td><td>0.0030</td><td>0.0011</td><td>0.0271</td><td>0.1108</td><td>0.4959</td></tr><tr><td>HalMar</td><td>0.1898</td><td>0.1565</td><td>0.0188</td><td>0.0010</td><td>0.0546</td><td>0.1910</td><td>0.4294</td></tr><tr><td>PyrFur</td><td>0.1200</td><td>0.1047</td><td>0.0114</td><td>0.0003</td><td>0.0066</td><td>0.1262</td><td>0.4737</td></tr></table></body></html>

a ↑ (↓) represents the higher the better (the lower the better). The same applies to other tables.

Table 2: Experimental results when $T _ { 1 , 1 } ^ { ( q , q , m ) } = 0 . 5$ and $T _ { 0 , 0 } ^ { ( q , q , m ) } = 0 . 5$

# 3 Experiment

# 3.1 Setup

Datasets. We evaluate our method on six datasets from various fields: 1) Scene1, an image dataset for MIML (Zhou and Zhang 2006); 2) Reuters2, a text dataset for MIML (Zhou, Sun, and Li 2009); 3) Geobacter sulfurreducens (GeoSul), Azotobacter vinelandii (AzoVin), Haloarcula marismortui (HalMar), and Pyrococcus furiosus (PyrFur)3, datasets used for genome-wide protein function prediction through MIML (Wu, Huang, and Zhou 2014). We set the number of annotators $M$ to 30, calculate the noise rate using Equation 3, and generate uniformly distributed random numbers between 0 and 1 to determine noise labels. For the transiall classes: a) $T _ { 1 , 1 } ^ { ( q , q , m ) } = T _ { 0 , 0 } ^ { ( q , q , m ) }$ , b) $T _ { 1 , 1 } ^ { ( q , q , m ) } < T _ { 0 , 0 } ^ { ( q , q , m ) }$ and c) 푇 (푞,푞,푚) $T _ { 1 , 1 } ^ { ( q , q , m ) } ~ > ~ T _ { 0 , 0 } ^ { ( q , q , m ) }$ 푇0(,푞0,푞,푚). Specifically, the true rates $( T _ { 1 , 1 } ^ { ( q , q , m ) } , T _ { 0 , 0 } ^ { ( q , q , m ) } )$ are set to a) 0.5, 0.5 , b) 0.3, 0.5 and c) (0.5, 0.3). To satisfy the normalization conditions in Section 2.1, the remaining elements 푇푖(, 푝,푞,푚) a re sampled from a normal distribution $\begin{array} { r } { N ( \psi _ { i } , \left( \frac { \psi _ { i } } { 3 } \right) ^ { 2 } ) } \end{array}$ , where $\begin{array} { r } { \psi _ { i } = \frac { 2 ( 1 - T _ { i , i } ^ { ( p , p , m ) } ) } { C \cdot M } } \end{array}$ and $i \in \{ 0 , 1 \}$ . The original transition matrix is then normalized using the equation in Section 2.1, and the aggregated matrix is computed as described in Section 2.1.

Baseline. To demonstrate the excellence of our method, we compare it against different types of baselines: 1) Four MIML methods: MIMLSVM (Zhou and Zhang 2006), which employs Hausdorff distance among bags for SVM-based classification; KiSar (Li et al. 2012), which uses convex optimization to discover relations between input patterns and output labels; DeepMIML (Feng and Zhou 2017), which models label-instance relations and establishes instance-level classifiers; and MIML-LLMC (Yang, Tang, and Min 2022), which improves MIML classification by extracting local label correlations. 2) One partial multi-label learning (Xu et al. 2023) (PMLL) method: PLAIN (Wang et al. 2023), which leverages instance- and label-level similarities to generate pseudo-labels via label propagation and trains a deep model to fit the disambiguated labels. 3) a variant of our method that uses binary cross entropy (BCE) loss instead of unbiased loss. Note that we use mean squared error (MSE) loss for MIML-LLMC and PLAIN, Hinge loss for MIMLSVM and KiSar, and BCE loss for DeepMIML. For all the baselines, the aggregated crowdsourced label y¯ is used. Moreover, the experimental results using the majority voting labels are provided in the appendix.

Inplementation Details. We use a neural network with hidden layers of 256, 512, and 256 units, which is applied to all models for consistency. Our model is trained with the Adam optimizer, with a learning rate $1 e ^ { - 4 }$ and a weight decay $1 e ^ { - 5 }$ . The hyperparameter $K$ , used to estimate the transition matrix in Section 2.3, is set to 5. Other parameters are set to their default values. The base loss $l$ in Theorem 2 is BCE loss. Following Ockham’s Razor and given that bag-level classifiers outperform instance-level ones (Zhou 2004), we use bag-level loss in Theorem 2 instead of instance-level loss.

For the performance evaluations, three metrics widely used in MIML are selected to analyze experimental results, including macro-averaged F1 (Macro-F1), average Recall(AvgRec) and coverage (Aggarwal et al. 2017). For the first two metrics, a higher value represents a better performance, while for coverage, the lower the better. We perform ten-fold cross-validation and report the mean as well as the standard deviation for metric results.

Table 3: Experimental results when $T _ { 1 , 1 } ^ { ( q , q , m ) } = 0 . 3$ and $T _ { 0 , 0 } ^ { ( q , q , m ) } = 0 . 5$   

<html><body><table><tr><td>Metrics</td><td>Datasets</td><td>MIML-LLMC</td><td>MIMLSVM</td><td>KiSar</td><td>DeepMIML</td><td>PLAIN</td><td>Ours-BCE</td><td>Ours</td></tr><tr><td rowspan="6">Macro-F1↑</td><td>Scene</td><td>0.4010</td><td>0.0057</td><td>0.5860</td><td>0.2335</td><td>0.5183</td><td>0.3648</td><td>0.6203</td></tr><tr><td>Reuters</td><td>0.8686</td><td>0.0796</td><td>0.6328</td><td>0.7343</td><td>0.7142</td><td>0.7281</td><td>0.7823</td></tr><tr><td>AzoVin</td><td>0.1987</td><td>0.0021</td><td>0.0019</td><td>0.0005</td><td>0.0012</td><td>0.2156</td><td>0.0634</td></tr><tr><td>GeoSul</td><td>0.0696</td><td>0.0076</td><td>0.0026</td><td>0.0002</td><td>0.0061</td><td>0.0462</td><td>0.0749</td></tr><tr><td>HalMar</td><td>0.0463</td><td>0.0095</td><td>0.0113</td><td>0.0008</td><td>0.0096</td><td>0.1191</td><td>0.0957</td></tr><tr><td>PyrFur</td><td>0.0288</td><td>0.0011</td><td>0.0036</td><td>0.0002</td><td>0.0007</td><td>0.0305</td><td>0.0737</td></tr><tr><td rowspan="6">Coverage↓</td><td>Scene</td><td>2.11</td><td>2.08</td><td>0.94</td><td>2.21</td><td>2.88</td><td>2.62</td><td>1.04</td></tr><tr><td>Reuters</td><td>1.29</td><td>1.67</td><td>0.48</td><td>1.36</td><td>2.54</td><td>1.83</td><td>0.39</td></tr><tr><td>AzoVin</td><td>56.00</td><td>56.86</td><td>63.63</td><td>55.27</td><td>51.53</td><td>52.66</td><td>48.65</td></tr><tr><td>GeoSul</td><td>45.98</td><td>42.37</td><td>45.54</td><td>38.78</td><td>42.53</td><td>41.85</td><td>37.42</td></tr><tr><td>HalMar</td><td>52.08</td><td>32.41</td><td>37.14</td><td>29.46</td><td>28.48</td><td>34.52</td><td>24.83</td></tr><tr><td>PyrFur</td><td>48.01</td><td>55.03</td><td>67.36</td><td>54.28</td><td>57.43</td><td>56.26</td><td>45.60</td></tr><tr><td rowspan="6">AvgRec↑</td><td>Scene</td><td>0.2543</td><td>0.2367</td><td>0.5130</td><td>0.1403</td><td>0.6254</td><td>0.5652</td><td>0.6686</td></tr><tr><td>Reuters</td><td>0.9132</td><td>0.4327</td><td>0.6264</td><td>0.7830</td><td>0.7479</td><td>0.8913</td><td>0.9557</td></tr><tr><td>AzoVin</td><td>0.2751</td><td>0.0862</td><td>0.0034</td><td>0.0003</td><td>0.0082</td><td>0.0946</td><td>0.4670</td></tr><tr><td>GeoSul</td><td>0.2350</td><td>0.1353</td><td>0.0082</td><td>0.0002</td><td>0.0921</td><td>0.1052</td><td>0.4653</td></tr><tr><td>HalMar</td><td>0.1127</td><td>0.1574</td><td>0.0328</td><td>0.0005</td><td>0.0620</td><td>0.1262</td><td>0.4341</td></tr><tr><td>PyrFur</td><td>0.1099</td><td>0.1119</td><td>0.0112</td><td>0.0006</td><td>0.0272</td><td>0.1516</td><td>0.5048</td></tr></table></body></html>

<html><body><table><tr><td>Metrics</td><td>Datasets</td><td>MIML-LLMC</td><td>MIMLSVM</td><td>KiSar</td><td>DeepMIML</td><td>PLAIN</td><td>Ours-BCE</td><td>Ours</td></tr><tr><td rowspan="6">Macro-F1↑</td><td>Scene</td><td>0.6002</td><td>0.0060</td><td>0.5942</td><td>0.2344</td><td>0.5931</td><td>0.3649</td><td>0.6248</td></tr><tr><td>Reuters</td><td>0.8833</td><td>0.0787</td><td>0.6378</td><td>0.7362</td><td>0.6847</td><td>0.7263</td><td>0.7729</td></tr><tr><td>AzoVin</td><td>0.0400</td><td>0.0020</td><td>0.0040</td><td>0.0005</td><td>0.0008</td><td>0.0661</td><td>0.0720</td></tr><tr><td>GeoSul</td><td>0.0287</td><td>0.0029</td><td>0.0021</td><td>0.0010</td><td>0.0046</td><td>0.0463</td><td>0.0727</td></tr><tr><td>HalMar</td><td>0.1572</td><td>0.0077</td><td>0.0083</td><td>0.0007</td><td>0.0106</td><td>0.0854</td><td>0.0957</td></tr><tr><td>PyrFur</td><td>0.0379</td><td>0.0018</td><td>0.0022</td><td>0.0005</td><td>0.0011</td><td>0.0613</td><td>0.0782</td></tr><tr><td rowspan="6">Coverage↓</td><td>Scene</td><td>1.97</td><td>2.07</td><td>0.95</td><td>3.05</td><td>2.18</td><td>2.48</td><td>1.47</td></tr><tr><td>Reuters</td><td>1.30</td><td>1.66</td><td>0.47</td><td>1.42</td><td>1.51</td><td>1.34</td><td>0.32</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AzoVi</td><td>50.38</td><td>49.78</td><td>62.65</td><td>50.87</td><td>94</td><td>59.17</td><td>44.56</td></tr><tr><td>HalMar</td><td>30.76</td><td>32.89</td><td>36.54</td><td>33.90</td><td>32.06</td><td>40.02</td><td>27.03</td></tr><tr><td>PyrFur</td><td>49.64</td><td>57.69</td><td>69.11</td><td>58.22</td><td>51.62</td><td>62.25</td><td>46.80</td></tr><tr><td rowspan="6">AvgRec↑</td><td>Scene</td><td>0.5240</td><td>0.2367</td><td>0.5200</td><td>0.1418</td><td>0.6747</td><td>0.5973</td><td>0.6582</td></tr><tr><td>Reuters</td><td>0.9291</td><td>0.4322</td><td>0.6282</td><td>0.7851</td><td>0.7819</td><td>0.8684</td><td>0.9585</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AzoVin</td><td>0.1156</td><td>0.1233</td><td>0.006</td><td>0.000</td><td>0.0384</td><td>0.0975</td><td>0.4503</td></tr><tr><td>HalMar</td><td>0.2751</td><td>0.1313</td><td>0.0248</td><td>0.0009</td><td>0.06185</td><td>0.0786</td><td>0.4342</td></tr><tr><td>PyrFur</td><td>0.2267</td><td>0.1147</td><td>0.0089</td><td>0.0009</td><td>0.1812</td><td>0.1467</td><td>0.4415</td></tr></table></body></html>

Table 4: Experimental results when $T _ { 1 , 1 } ^ { ( q , q , m ) } = 0 . 5$ and $T _ { 0 , 0 } ^ { ( q , q , m ) } = 0 . 3$ .

# 3.2 Comparison Results

We report experimental results across three CMIML scenarios, as shown in Tables 2, 3, and 4. Overall, our model outperforms the baseline on multiple datasets and metrics. In terms of avgRec, our method consistently exceeds others; for instance, on the PyrFur dataset in Table 3, our recall rate is 4.5 times higher than the second-best model, MIMLSVM. This is because most baselines fail to correct errors from incorrect labeling, but predicting mislabeled annotations leads to a low true positive rate and poor recall. In contrast, our CMIML unbiased loss generates negative gradients, helping the model correct mislabels and avoid incorrect predictions, thus significantly improving recall. For macro-F1, our method shows at least a $4 . 1 \%$ improvement over the baseline, with a maximum increase of 1.65 times. In terms of coverage, our model, using CMIML unbiased loss, improves by at least $3 . 6 \%$ , with up to 1.5 times the coverage of the second-best model. The strong performance across different scenarios demonstrates that our approach effectively and accurately handles complex CMIML tasks.

Error between Confidence $\pmb { \phi }$ and True Label y. Please note that the label confidence $\phi _ { k }$ in Theorem 2 is consistent in form with $I _ { [ \mathsf { y } _ { k } = 1 ] }$ in the basic loss function $l ( f _ { k } ( { \bf x } ) , { \bf y } _ { k } ) =$ $I _ { [ \mathsf { y } _ { k } = 1 ] } l ( f _ { k } ( \mathbf { x } ) , \overset { \cdot \cdot } { 1 } ) + ( 1 - I _ { [ \mathsf { y } _ { k } = 1 ] } ) l ( f _ { k } ( \mathbf { x } ) , 0 ) .$ . In fact, $\phi$ indeed reflects the confidence in the true label. Figure 2 illustrates the error between the clamped $\pmb { \phi }$ (restricted to the range 0 to 1) and the true labels $\pmb { \ y }$ across different numbers of annotators. The error used is the average absolute error per element, i.e. $\begin{array} { r } { \frac { 1 } { B ^ { \prime } } | | \boldsymbol { \phi } ^ { \prime } - \mathbf { y } | | _ { 1 } } \end{array}$ , where $B ^ { \prime }$ is the number of training samples and $\pmb { \phi } ^ { \prime }$ is the clamped $\pmb { \phi }$ . Both $\pmb { \phi }$ and $\pmb { \ y }$ are two-dimensional matrices, with the first dimension representing samples and the second representing classes. Figure 2 clearly shows that as the number of annotators increases, the error between $\pmb { \phi }$ and $\pmb { \ y }$ decreases. Notably, when the number of annotators reaches 50, the error on the Scene dataset drops to 0.11, and on the Reuters dataset, it falls below 0.04. This suggests that our method can nearly restore the true label $\pmb { \ y }$ using the label confidence $\phi$ with a large number of annotators. This finding supports the superiority of our method.

![](images/322b0aa8e6460d637473bcc2571c54b24b26288580e9bd73d513ec28b055c4e0.jpg)  
Figure 2: Average absolute error per element between the clamped confidence $\pmb { \phi } ^ { \prime }$ of $\pmb { \phi }$ in Theorem 2 and true label y.

![](images/8f5794e82951ff70c0ff64a337c6521fe5f4778a98be9a34d2569ae7d146c34f.jpg)  
Figure 3: The relative error of transition matrix estimation under different strategies.

Error of Different Transition Matrix Estimation Strategies. Figure 3 shows the error of different transition matrix estimation strategies. Based on prior research (Li et al. 2022; Patrini et al. 2017; Xia et al. 2019), $T _ { \mathrm { m a x } }$ estimates the transition matrix using the prediction probability of the most reliable sample, while Top $k$ averages the crowdsourced labels of the top $k$ anchor points. The error is measured as the relative error $\frac { | | \hat { \pmb { T } } ^ { \prime } - \pmb { T } ^ { \prime } | | _ { 2 } } { | | \pmb { T } | | _ { 2 } }$ , where ${ \hat { \pmb { T } } } ^ { \prime }$ and $T ^ { \prime }$ are the flattened estimated and true transition matrices, respectively. The results show the Top 5 method achieves the lowest estimation error.

# 4 Related Work

# 4.1 Multi-Instance Multi-Label Classification

In past decades, multi-instance multi-label classification (MIMLC) has attracted much research attention. (Zhou and Zhang 2006; Zhou, Sun, and Li 2009) approach scene classification and text categorization by decomposing the MIML problem into multi-label classification (Li, Ouyang, and Zhou 2015; Wei, Shi, and Li 2021; Wei et al. 2022) and multi-instance classification (Zhou 2004; Eberts and Ulges 2021; Lin et al. 2023) tasks. However, this decomposition can lead to the loss of critical information. To address this, methods have been developed that directly tackle the MIML problem (Li et al. 2012). With the rapid advancement of deep learning, recent approaches leverage deep models to generate features directly (Chen et al. 2013; Wu, Huang, and Zhou 2014; Feng and Zhou 2017; Yang et al. 2017; Yu et al. 2019), eliminating the need for additional instance generators. Traditional MIML approaches consider the training data to be fully supervised. However, obtaining precise data annotations is expensive and time-consuming. To mitigate this problem, weakly-supervised MIML has been proposed (Xu et al. 2012; Yang, Jiang, and Zhou 2013; Yang et al. 2019; Nguyen and Raich 2021). In this paper, we explore a more practical setup and propose the CMIML framework, which reduces the cost of data annotation and achieves robust classification without precise annotations.

# 4.2 Learning from Crowdsourced Labels

Crowdsourcing (Dalvi et al. 2013; Patrini et al. 2017) is widely used in practice and increasingly studied in academia, as it collects low-cost but unreliable labels to reduce the budget of large-scale data annotation. Traditional crowdsourcing methods using expectation-maximization(EM) algorithms identify accurate labels by modeling crowdsourced labels (Dalvi et al. 2013; Patrini et al. 2017). Subsequently, advanced techniques rooted in deep learning have been introduced, showcasing remarkable efficacy. These methodologies address the challenge of crowdsourced label noise through the acquisition of label transition matrices (Gao et al. 2022; Chen et al. 2020). Going beyond the single-instance single-label scenario, (Li et al. 2018; Zhang and Wu 2018) studies the crowdsourcing problem in the context of learning with multiple labels and (Del Amor et al. 2023) with multiple instances. Some works study crowdsourced multi-label learning (Xia et al. 2024), but overlook cross-label transitions in multi-instance learning.

# 5 Conclusion

In order to reduce annotation costs, this article focuses on the crowdsourced MIML problem labeled by multi-source annotation. Firstly, the first URE was proposed, which was further simplified into bag-level and instance-level versions by combining aggregated labels and transition matrices. A generalization upper bound is proposed to provide theoretical assurance. In terms of understanding, this method can be seen as using negative gradients to help the model forget incorrect annotations. Extensive experiments have demonstrated the superior performance of our method.