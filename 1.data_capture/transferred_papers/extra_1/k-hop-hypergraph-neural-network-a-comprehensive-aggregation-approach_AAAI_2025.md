# K-hop Hypergraph Neural Network: A Comprehensive Aggregation Approach

Linhuang Xie1,2, Shihao Gao1,2, Jie $\mathbf { L i u } ^ { 3 , 4 }$ , Ming $\mathbf { Y i n ^ { 5 } }$ , Taisong $\mathbf { J i n } ^ { 1 , 2 * }$

1Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, China 2School of Informatics, Xiamen University, China 3School of Information Science, North China University of Technology, China 4China Language Intelligence Research Center, Capital Normal University, China 5School of Electronic Science and Engineering, South China Normal University, China linhuangxie@gmail.com, gaoshihao@stu.xmu.edu.cn, liujxxxy@126.com, m.yin@scnu.edu.cn, jintaisong@xmu.edu.cn

# Abstract

The powerful capability of HyperGraph Neural Networks (HGNNs) in modeling intricate, high-order relationships among multiple data samples stems primarily from their ability to aggregate both the direct neighborhood features of individual nodes and those associated with hyperedges. However, the limited scope of feature propagation in existing HGNNs significantly reduces the utilization of hypergraph information, exacerbating over-squashing and over-smoothing issues. To this end, we propose a novel $\kappa$ -hop HyperGraph Neural Network (KHGNN) to facilitate the interactions of distant nodes and hyperedges. Specifically, the bisection nested convolution based on HyperGINE is employed to extract features from nodes, hyperedges, and structures along all shortest paths between nodes or hyperedges, providing representations of long-distance relationships. With these comprehensive path features, nodes and hyperedges are guided to aggregate distant information while learning their complex relationships. The extensive experiments, particularly on longrange graph datasets, demonstrate that the proposed method achieves SOTA performance compared to existing HGNNs and graph neural networks.

# Introduction

Graph Neural Networks (GNNs) that effectively handle nonEuclidean structured data have been widely applied in tasks such as computational chemistry (Yu et al. 2020), social networks (Cao et al. 2020), and anomaly detections (Yang et al. 2024). However, GNNs are inherently limited by their exclusive focus on pairwise relationships between data samples, which restricts the expressive power.

Unlike GNNs, Hypergraph Neural Networks (HGNNs) can capture higher-order non-Euclidean relationships among real-world entities through unpaired connections involving any number of nodes on hyperedges. Since the introduction of the clique expansion (CE) technique by HGNN (Feng et al. 2019), most of the existing HGNNs (Chien et al. 2022; Huang and Yang 2021; Wang et al. 2023) define various message-passing methods based on this technique. As shown in Fig. 1(a) and Fig. 2(a1), CE involves a two-stage message passing process: (1) Node-to-hyperedge: Numerous nodes are compressed into hyperedges with limited feature capacity, resulting in information loss. (2) Hyperedgeto-node: Identical hyperedge features are assigned to all neighboring nodes, diminishing the discriminability of node features. The aforementioned slow diffusion exacerbates the issues of over-squashing and over-smoothing (Hu et al. 2019; Topping et al. 2021; Alon and Yahav 2020), reducing the utilization of hypergraph information.

![](images/2d87b6bd782f97872ebeff56ae6744ae89eb98546745a8747153367c411a586d.jpg)  
Figure 1: (a) For the existing hypergraph neural networks, node features are first aggregated by hyperedges and then propagated to neighboring nodes. (b) The proposed $K$ -hop HGNN introduces a wider range of node aggregation by utilizing path features, facilitating feature propagation.

GNNs have extended the concept of direct message passing to $K$ -hop message passing (Wang et al. 2021; Zhang and Li 2021), thereby enhancing the expressive power of them (Feng et al. 2022; Abboud et al. 2022). In particular, leveraging path features that encapsulate contextual information to guide $K$ -hop message passing between nodes can explicitly provide rich semantic information for relationship modeling, leading to promising performance in real-world applications (Michel et al. 2023; Sun et al. 2022).

However, there is few research specifically dedicated to path-based $K$ -hop hypergraph neural networks. Considering the following reasons, it is intractable to design $K$ -hop HGNNs: (1) Inherent structural differences between hypergraphs, characterized by hyperedges with varying numbers of nodes, and normal graphs render the methods used in $K$ -hop GNNs unsuitable for HGNNs. (2) Diverging from $K$ -hop GNNs (Michel et al. 2023; Sun et al. 2022), merely aggregating $K$ -hop node neighborhoods while disregarding hyperedges representing higher-order relations constrains the expressive capacity of HGNNs. (3) Due to differences in feature space, the commonly used summation operators only aggregate either node (Sun et al. 2022; Michel

2nd hop 1.0 HGNN (1) HGNN (2) ùë≤-hop HGNN st hop 0.264 ùë≤=321 ùë≤=654 6 8 10 (a) Over-squashing (b) Over-smoothing Layers et al. 2023) or edge features (Feng et al. 2022; Ying et al. 2021) as path features, which makes it hard to effectively represent path information. (4) As the length of paths increases, the number of paths grows exponentially, demanding substantial computational resources. This restricts the use of complex operators for extracting path features.

Inspired by advances in $K$ -hop message passing, we propose a novel hypergraph neural network, termed KHGNN, to learn representations of nodes and hyperedges with larger receptive fields. The proposed model employs a bisection nested convolution strategy, which iteratively performs convolutions at the centers of the shortest paths, combining information from nodes, hyperedges, and structures into path features with lower computational costs. Thus, nodes and hyperedges are guided by path features to learn long-range higher-order relations while aggregating information from an expanded neighborhood, eliminating the need for separately designing aggregation strategies. KHGNN mitigates over-squashing by enabling direct interaction among nodes through higher-order relationships, thereby avoiding information compression by hyperedges (see Fig. 2(a)). The oversmoothing issue is addressed through the discriminative information provided by distant nodes (see Fig. 2(b)).

Our contributions are in three-folds: (1) We propose HyperGINE, a hypergraph neural network that leverages connection features to guide the direct aggregation of nodes or hyperedges. (2) We propose a novel $K$ -hop hypergraph neural network (KHGNN), an extension of HyperGINE, that integrates three challenging-to-harmonize features on paths using a bisection nested convolutional approach as the guidance for $K$ -hop message passing between nodes or hyperedges. To our knowledge, this is the first attempt at pathbased $K$ -hop hypergraph message passing. (3) We propose a novel strategy to encode structural information of different paths, which is suitable for $K$ -hop HGNN.

# Related Work

# Hypergraph Neural Networks

In contemporary hypergraph research, most HGNNs draw inspiration from the clique expansion (CE) technique proposed by HGNN (Feng et al. 2019). For instance, HyperGAT (Ding et al. 2020) leveraged attention-based message passing, while Wang et al. (2023) combined hypergraph diffusion algorithms with CE. DHGNN (Jiang et al. 2019) introduced a feature-influenced dynamic hypergraph structure, and Gao et al. (2022) further defined directed hyperedge convolution. Some works (Huang and Yang 2021; Chien et al. 2022) endeavored to define a unified two-stage architecture. However, the CE architectures constrain the breadth of information accessible to nodes and hyperedges, exacerbating over-smoothing and over-squashing issues (Yu et al. 2022). While methods like KNN, clustering, node neighborhoods (Jiang et al. 2019; Gao et al. 2022), and random walk techniques (Huang et al. 2020; Huang, Liu, and Song 2019) gather information from distant nodes, they fall short in accurately capturing the hypergraph structures and learning appropriate representations of hyperedges.

# $\pmb { K }$ -hop Message Passing

In graph neural networks, $K$ -hop message passing constructs $K$ -hop neighborhood for each node in three primary ways: (1) Random walk-based methods establish node neighborhoods using different powers of the adjacency matrix (Abu-El-Haija et al. 2019; Chien et al. 2020; Wang et al. 2021). However, the inherent complexity of random walks leads to the neglect of important path features. (2) Shortest path-based methods define neighborhoods based on the shortest distance between nodes (Abboud et al. 2022; Yang et al. 2021; Li et al. 2020). Due to pairwise relationships, only one of the multiple shortest paths between nodes is chosen as the feature. (3) All Paths-based methods consider all possible paths between nodes (Sun et al. 2022; Michel et al. 2023). However, the complexity of the graph structure introduces significant redundancy and computational overhead.

To reduce computational costs, existing methods employ summation operators, either summing edge features (Feng et al. 2022; Ying et al. 2021; Brossard, Frigo, and Dehaene 2020) or node features (Sun et al. 2022; Michel et al. 2023; Yang et al. 2021), to represent path features. However, path features obtained through such an operator are incomplete for graphs, and even more so for hypergraphs with their inherently more complex structures.

For hypergraph neural networks, existing works either simply aggregate the $K$ -hop neighborhoods of nodes (Huang et al. 2021, 2023), or transform them into hyperedges (Gao et al. 2022). However, these approaches overlook the utilization of path information and hyperedge features, which encapsulate intricate semantic details.

In summary, whether in graphs or hypergraphs, there remains a lack of effective methods to encode nodes, hyperedges, and structural information into path features that can guide the aggregation of information from larger neighborhoods for both nodes and hyperedges.

# Method

In this section, we first introduce the hypergraph notation for later use. Then, we present HyperGINE, from which KHGNN is derived. KHGNN leverages path features obtained through bisection nested convolutions to guide the representation learning of nodes and hyperedges. Finally, we introduce a relative degree encoding scheme to further enhance the model‚Äôs ability to capture structural information.

# Notation

Let $\mathcal { G } ~ = ~ \left( \nu , \mathcal { E } \right)$ denote a hypergraph with a node set $\mathcal { V } = \{ v _ { 1 } , . . . , v _ { n } \}$ and an edge set $\mathcal { E } = \{ e _ { 1 } , . . . , e _ { m } \}$ , where $n$ and $m$ represent the total number of nodes and hyperedges, and $h _ { v }$ and $h _ { e }$ denote their respective feature embeddings. We define the $k$ -th hop neighboring node set of node $\boldsymbol { v } _ { i }$ as $\mathcal { N } _ { v _ { i } } ^ { k }$ , which includes all nodes reachable from $v _ { i }$ through at least $k$ hyperedges (i.e., at a distance of at least $k _ { - }$ ). $\bar { \mathcal { N } } _ { v _ { i } } ^ { 0 , k } \ = \ \{ \mathcal { N } _ { v _ { i } } ^ { t } \} _ { t \in [ 0 , k ] } ^ { \ }$ denotes the $K$ -hop neighboring node set of node $v _ { i }$ . The $k$ -th hop neighboring hyperedge set of node $v _ { i }$ is denoted as $\mathcal { Q } _ { v _ { i } } ^ { k }$ , representing all hyperedges reachable from $\boldsymbol { v } _ { i }$ through aQtvlieast $k$ nodes. Specially, $\mathcal { Q } _ { v _ { i } } ^ { 0 }$ denotes hyperedges directly connected to node $\boldsymbol { v } _ { i }$ .

Similarly, for hyperedge $e _ { b }$ , we define its $k$ -th hop neighboring hyperedge set as $\mathcal { N } _ { e _ { b } } ^ { k }$ , representing all hyperedges reachable from hyperedge $e _ { b }$ through at least $k$ nodes. $\mathcal N _ { e _ { b } } ^ { 0 , k } = \{ \mathcal N _ { e _ { b } } ^ { t } \} _ { t \in [ 0 , k ] }$ represents the $K$ -hop neighboring hyperedge set of hyperedge $\boldsymbol { e } _ { b }$ . The -th hop neighboring node set of hyperedge $\boldsymbol { e } _ { b }$ is denoted as $\mathcal { Q } _ { e _ { b } } ^ { k }$ , where $\bar { \mathcal { Q } } _ { e _ { b } } ^ { 0 }$ represents nodes directly connected to hyperedge $e _ { b }$ .

# HyperGINE

To tackle the issues posed by slow clique expansion message passing, we extend GINE (Hu et al. 2019), whose baseline is provably one of the most expressive GNNs ( $\mathrm { \Delta X u }$ et al. 2019), to hypergraphs. HyperGINE is introduced, which enables the simultaneous learning of features for nodes, hyperedges, and the overall neighborhood structure. The formulation of HyperGINE is presented as follows:

$$
\begin{array} { r l } & { \displaystyle h _ { v _ { i } } ^ { l } = \mathcal { A } \mathcal { G } \mathcal { G } \mathcal { N } ^ { l } \left( h _ { v } ^ { l - 1 } , h _ { e } ^ { l - 1 } , \mathcal { Q } _ { v } ^ { 0 } , \mathcal { Q } _ { e } ^ { 0 } , S _ { v _ { i } } \right) } \\ & { \quad = \displaystyle \sum _ { e _ { b } \in \mathcal { Q } _ { v _ { i } } ^ { 0 } } \left( \sum _ { v _ { j } \in \{ \mathcal { Q } _ { e _ { b } } ^ { 0 } - S _ { v _ { i } } \} } \left( f ( h _ { v _ { j } } ^ { l - 1 } ) + g ( h _ { e _ { b } } ^ { l - 1 } ) \right) \right) , } \end{array}
$$

where $f$ and $g$ are learnable linear transformations that map node features $h _ { v }$ and hyperedge features $h _ { e }$ to appropriate feature spaces. $S _ { v _ { i } }$ denotes the set of nodes within hyperedges that do not transmit information to node $v _ { i }$ , as defined by different algorithms. For instance, within the same community (hyperedge), not all residents (nodes) necessarily need to know each other.

Furthermore, hypergraph duality enables a seamless interchange of roles between nodes and hyperedges. Leveraging this property, HyperGINE directly updates hyperedge features without modification:

$$
\begin{array} { r l } & { h _ { e _ { b } } ^ { l } = \displaystyle A \mathcal { G } \mathcal { G } \mathcal { E } ^ { l } \left( h _ { e } ^ { l - 1 } , h _ { v } ^ { l - 1 } , \mathcal { Q } _ { e } ^ { 0 } , \mathcal { Q } _ { v } ^ { 0 } , S _ { e _ { b } } \right) } \\ & { \quad = \displaystyle \sum _ { v _ { i } \in \mathcal { Q } _ { e _ { b } } ^ { 0 } } \left( \sum _ { e _ { c } \in \{ \mathcal { Q } _ { v _ { i } } ^ { 0 } - S _ { e _ { b } } \} } \left( f ( h _ { e _ { c } } ^ { l - 1 } ) + g ( h _ { v _ { i } } ^ { l - 1 } ) \right) \right) , } \end{array}
$$

and eliminates the necessity of designing separate aggregation functions for learning node and hyperedge representations. By aggregating inter-node hyperedge information,

HyperGINE mitigates the effects of over-squashing and reduces potential noise that may arise when nodes are compressed into hyperedges and subsequently transmitted to other nodes, as compared to HGNN (see Fig. 2(b) $\mathrm { K } { = } 1$ ).

In particular, when $S _ { v _ { i } } = \{ v _ { i } \}$ , symmetric normalization of Eq. 1 yields corresponding matrix form:

$$
\begin{array} { c } { { X _ { v } ^ { l } = D _ { v } ^ { - \frac { 1 } { 2 } } H \left( D _ { e } ^ { - 1 } ( H ^ { T } D _ { v } ^ { - \frac { 1 } { 2 } } X _ { v } ^ { l - 1 } \Theta _ { 1 } - X _ { e } \Theta _ { 2 } ) + X _ { e } \Theta _ { 2 } \right) } } \\ { { - H \mathrm { d i a g } ( D _ { e } ^ { - 1 } ) { \bf 1 } ^ { T } \odot D _ { v } ^ { - \frac { 1 } { 2 } } X _ { v } ^ { l - 1 } \Theta _ { 1 } , \qquad ( 3 ) } } \end{array}
$$

where $X _ { v }$ and $X _ { e }$ represent features of all nodes and hyperedges, respectively. $D _ { v }$ and $D _ { e }$ are diagonal matrices of node and hyperedge degrees. $H$ denotes the incidence matrix in the hypergraph, and $\Theta$ denotes learnable parameters. $\odot$ denotes the Hadamard product. Thus, HGNN (Feng et al. 2019) is viewed as a special case of HyperGINE. For detailed proofs and discussions, please refer to Appendix D.

# $\pmb { K }$ -hop Hypergraph Neural Network

In this section, we introduce KHGNN, as illustrated in Fig. 3, which guides higher-order aggregation of nodes or hyperedges by integrating features from nodes, hyperedges, and structure along the paths. KHGNN comprises two essential components: $K$ -hop node representation learning and $K$ -hop hyperedge representation learning.

$\pmb { K }$ -hop Node Representation Learning Unlike in normal graphs where edges establish fixed connections between two nodes, paths on hypergraphs interweave between nodes and structurally complex hyperedges, forming fundamental features. Therefore, HyperGINE, which accounts for node, hyperedge, and structure features, is selected as the foundational model of KHGNN.

To alleviate the computational and parameter demands imposed by numerous paths, KHGNN introduces the bisection nested convolution, inspired by bisection search. From a top-down perspective, higher-order paths undergo recursive partitioning around a central position. From a bottomup standpoint, central nodes (hyperedges) utilize HyperGINE convolution to iteratively merge features from the left and right segments of the path, simultaneously injecting structural characteristics. During this process, the convolution of the central node (hyperedge) not only updates its features but also amalgamates features from multiple paths to generate comprehensive higher-order path features.

Specifically, when $k$ is even, the set of central nodes on all shortest paths between nodes $\boldsymbol { v } _ { i }$ and $v _ { d }$ , where the shortest path length is $k$ , is defined as $\{ \mathcal { N } _ { v _ { i } } ^ { \frac { k } { 2 } } \cap \mathcal { N } _ { v _ { d } } ^ { \frac { k } { 2 } } \}$ . Let $v _ { j } \in \{ \mathcal { N } _ { v _ { i } } ^ { \frac { k } { 2 } } \cap \mathcal { N } _ { v _ { d } } ^ { \frac { k } { 2 } } \}$ represent one of these central nodes, it is evident that both $\boldsymbol { v } _ { i }$ and $v _ { d }$ are contained within $\mathcal { N } _ { v _ { j } } ^ { \frac { k } { 2 } }$ . The hidden state $\hat { h } _ { v _ { j } } ^ { l , \frac { k } { 2 } }$ obtained from the $\textstyle { \frac { k } { 2 } }$ -hop bisection nested convolution effectively combines all the shortest paths that are divided into two segments by node $v _ { j }$ between $\mathcal { N } _ { v _ { j } } ^ { \frac { k } { 2 } }$ . By injecting the structural features using HyperGINE as the base model, it provides a comprehensive representation of these shortest paths and constructs hyperedges for $\mathcal { N } _ { v _ { j } } ^ { \frac { k } { 2 } }$ to reveal higher-order position relationships. At this stage, the $k$ -th hop aggregation function at $l$ -th layer is expressed as:

$$
\begin{array} { r l } & { \hat { h } _ { v _ { i } } ^ { l , k } = \mathcal { A G G N } _ { k } ^ { l } \left( h _ { v } ^ { l - 1 } , \hat { h } _ { v } ^ { l , \frac { k } { 2 } } , \mathcal { N } _ { v } ^ { \frac { k } { 2 } } , \mathcal { N } _ { v } ^ { \frac { k } { 2 } } , S _ { v _ { i } } ^ { k } \right) } \\ & { \quad \quad = \displaystyle \sum _ { v _ { j } \in \mathcal { N } _ { v _ { i } } ^ { \frac { k } { 2 } } } \left( \sum _ { v _ { d } \in \{ \mathcal { N } _ { v _ { j } } ^ { \frac { k } { 2 } } - S _ { v _ { i } } ^ { k } \} } \left( f ( h _ { v _ { d } } ^ { l - 1 } ) + g ( \hat { h } _ { v _ { j } } ^ { l , \frac { k } { 2 } } ) \right) \right) , } \end{array}
$$

where $\mathcal { A } \mathcal { G } \mathcal { G } \mathcal { N }$ is defined as in Eq. 1. Since the shortest path length between node $v _ { i }$ and $\mathcal { N } _ { v _ { j } } ^ { \frac { k } { 2 } }$ ranges from 0 to $k$ , the set difference $S _ { v _ { i } } = \mathcal { N } _ { v _ { i } } ^ { 0 , k - 1 }$ represents that node $v _ { i }$ selectively aggregates only the nodes along the shortest path. In particular, when $k = 0$ , node $\boldsymbol { v } _ { i }$ establishes self-connection, inheriting the feature from the previous layer (i.e. $\hat { h } _ { v _ { i } } ^ { l , 0 } = h _ { v _ { i } } ^ { l - 1 } ;$ ).

Similarly, when $k$ is odd, the set of central hyperedges for all shortest paths between nodes $v _ { i }$ and $v _ { d }$ is denoted as $\{ \mathcal { Q } _ { v _ { i } } ^ { \frac { k - 1 } { 2 } } \cap \mathcal { Q } _ { v _ { d } } ^ { \frac { k - 1 } { 2 } } \}$ . For a hyperedge $e _ { j } \in \{ \mathcal { Q } _ { v _ { i } } ^ { \frac { k - 1 } { 2 } } \cap \mathcal { Q } _ { v _ { d } } ^ { \frac { k - 1 } { 2 } } \}$ , we have $v _ { i } , v _ { d } \in \mathcal { Q } _ { e _ { j } } ^ { \frac { k - 1 } { 2 } }$ . The hidden state $\hat { h } _ { e _ { j } } ^ { l , \frac { k - 1 } { 2 } }$ from the $\frac { k - 1 } { 2 }$ - hop nested convolution combines the shortest paths divided by hyperedge $e _ { j }$ while incorporating structural information of hyperedge $e _ { j }$ as the path features. In this case, the $k$ -th hop message passing for nodes $\boldsymbol { v } _ { i }$ is represented as:

$$
\hat { h } _ { v _ { i } } ^ { l , k } = \boldsymbol { \mathcal { A } } \boldsymbol { \mathcal { G } } \boldsymbol { \mathcal { S } } _ { \mathcal { N } _ { k } } ^ { l } \left( h _ { v } ^ { l - 1 } , \hat { h } _ { e } ^ { l , \frac { k - 1 } { 2 } } , \boldsymbol { \mathcal { Q } } _ { v } ^ { \frac { k - 1 } { 2 } } , \boldsymbol { \mathcal { Q } } _ { e } ^ { \frac { k - 1 } { 2 } } , S _ { v _ { i } } ^ { k } \right) ,
$$

where $\mathcal { A } \mathcal { G } \mathcal { G } \mathcal { N }$ also follows Eq. 1, and $S _ { v _ { i } } ^ { k } = \mathcal { N } _ { v _ { i } } ^ { 0 , k - 1 }$ denotes the set of nodes within a distance less than $k$ from the node $\boldsymbol { v } _ { i }$ within any hyperedge.

Subsequently, the update function $\boldsymbol { \mathcal { U P D } }$ assigns weights to the representations of different neighborhoods associated with each node. The feature embedding $h _ { v _ { i } } ^ { l }$ of node $\boldsymbol { v } _ { i }$ is generated at the $l$ -th layer by the following process:

$$
h _ { v _ { i } } ^ { l } = \mathcal { U P D } ^ { l } \left( \{ \hat { h } _ { v _ { i } } ^ { l , t } \} _ { t \in [ 0 , K ] } \right) = M L P \left( \sum _ { t = 0 } ^ { K } \lambda _ { t } \hat { h } _ { v _ { i } } ^ { l , t } \right) ,
$$

where $\lambda$ are the learnable parameters, and $M L P$ denotesdenotes a two-layer perceptron.

$\pmb { K }$ -hop Hyperedge Representation Learning When dealing with hypergraphs that consist of hyperedges containing arbitrary numbers of nodes, it becomes imperative to consider the more complex topological structures and connectivity relationships compared to pairwise connections in traditional graphs. Consequently, expanding the receptive field of hyperedges is crucial for capturing both local structures and connection information of nodes while obtaining additional contextual information. Thanks to hypergraph duality and the direct message-passing characteristics of HyperGINE, KHGNN updates hyperedge features with a larger receptive field without requiring additional design of hyperedge aggregation functions.

Specifically, after applying dual transformation, hyperedges $e$ and nodes $v$ in the hypergraph are transformed into dual nodes $e$ and dual hyperedges $\boldsymbol { v }$ in the dual hypergraph.

Similar to node representation learning, when the shortest path length $k$ is even, the $\textstyle { \frac { k } { 2 } }$ -th hop hidden state $\hat { h } _ { e _ { j } } ^ { l , \frac { k } { 2 } }$ of dual node $e _ { j }$ , located at the center of the path, serves as the shortest path feature with structural information between dual nodes $\mathcal { N } _ { e _ { j } } ^ { \frac { k } { 2 } }$ . Hyperedges with higher-order relationships are then constructed for these dual nodes. The corresponding hidden state $\hat { h } _ { e _ { b } } ^ { l , k }$ for the hyperedge $\boldsymbol { e } _ { b }$ is computed by

![](images/a29f6988ab0481ad69a88650a63a948b96262ddc3ad3d4987eff69957337d705.jpg)  
Figure 3: The process of learning the 3-hop node representation for node $v _ { 1 }$ in KHGNN. The low-order HyperGINE convolution at the center of the path, serving as input to the update function, effectively stitches together surrounding path features while incorporating structural information to construct new hyperedges.

$$
\hat { h } _ { e _ { b } } ^ { l , k } = \mathcal { A } \mathcal { G } \mathcal { G } _ { k } ^ { l } \left( h _ { e } ^ { l - 1 } , \hat { h } _ { e } ^ { l , \frac { k } { 2 } } , \mathcal { N } _ { e } ^ { \frac { k } { 2 } } , \mathcal { N } _ { e } ^ { \frac { k } { 2 } } , S _ { e _ { b } } ^ { k } \right) ,
$$

where $\mathcal { A } \mathcal { G } \mathcal { G } \mathcal { E }$ adheres to the definition in Eq. 2, and $S _ { e _ { b } } ^ { k } = $ $\mathcal { N } _ { e _ { b } } ^ { 0 , k - 1 }$ represents the set of hyperedges within a distance less than $k$ from hyperedge $e _ { b }$ .

When k is odd, ÀÜhlv,j $\hat { h } _ { v _ { j } } ^ { l , \frac { k - 1 } { 2 } }$ is utilized to characterize the shortest paths through dual hyperedges $v _ { j }$ between dual node ${ \mathcal Q } _ { v _ { j } } ^ { \frac { k - 1 } { 2 } }$ . The $k$ -th hop hidden state $\hat { h } _ { e _ { b } } ^ { l , k }$ is given by

$$
\hat { h } _ { e _ { b } } ^ { l , k } = \mathcal { A } \mathcal { G } \mathcal { G } _ { k } ^ { l } \left( h _ { e } ^ { l - 1 } , \hat { h } _ { v } ^ { l , \frac { k - 1 } { 2 } } , \mathcal { Q } _ { e } ^ { \frac { k - 1 } { 2 } } , \mathcal { Q } _ { v } ^ { \frac { k - 1 } { 2 } } , S _ { e _ { b } } ^ { k } \right) .
$$

Certainly, the update function for hyperedge embedding remains consistent with Eq. 6:

$$
h _ { e _ { b } } ^ { l } = \mathcal { U P D } ^ { l } ( \{ \hat { h } _ { e _ { b } } ^ { l , t } \} _ { t \in [ 0 , K ] } ) = M L P \left( \sum _ { t = 0 } ^ { K } \lambda _ { t } \hat { h } _ { e _ { b } } ^ { l , t } \right) .
$$

In Fig. 3, node $v _ { 2 }$ aggregates its first-hop neighborhood to update feature $h _ { v _ { 2 } }$ by utilizing hyperedges as paths. This convolution is also viewed as stitching together paths of length 1 around $v _ { 2 }$ and constructing a higher-order hyperedge $v _ { 2 }$ with the path endpoints $v _ { 1 } , v _ { 3 }$ , and $v _ { 5 }$ to represent the positional relationship between them. Consequently, the hidden state $h _ { v _ { 2 } } ^ { l , 1 }$ serves as a path feature to guide message passing between nodes $v _ { 1 }$ and $v _ { 3 } , v _ { 5 }$ . The convolution of hyperedges $e _ { 2 }$ and $e _ { 4 }$ operates in the same manner.

ùë≤ùëØùëÆùëµùëµùüè ùëµùíêùíÖùíÜ ùíìùíÜùíëùíìùíÜùíîùíÜùíèùíïùíÇùíïùíäùíêùíè ùíçùíÜùíÇùíìùíèùíäùíèùíà Hypergraph Êó• ùìêùìñùìñùìùùüèùíåùüèùíå ùìêùìñùìñùìùùüíùüèùüíùüè (?\~ ùìêùìñùìñùìùùüëùüèùüëùüè ùíâùë≥ùíó ùíâùüéùíó ùìêùìñùìñùìùùüè ùìêùìñùìñùìùùüêùüèùüêùüè Dual Transformation ùë≤ùëØùëÆùëµùëµùüê\~ùë≥ Dual Hypergraph ùìêùìñùìñùìîùüêùüèùüêùüè ùíâùë≥ ùìêùìñùìñùìîùüëùüèùüëùüè L ùíâùíÜùüè ùìêùìñùìñùìîùüíùüè ùíâùíÜùüé ùìêùìñùìñùìîùüèùíåùüèùíåùíÜ ùëØùíöùíëùíÜùíìùíÜùíÖùíàùíÜ ùíìùíÜùíëùíìùíÜùíîùíÜùíèùíïùíÇùíïùíäùíêùíè ùíçùíÜùíÇùíìùíèùíäùíèùíà

Discussion The bisection nested convolution operator in KHGNN offers several key advantages:

(1) Enhanced Representation of High-Order Relationships: Nodes and hyperedges encapsulate crucial structural and connectivity information that enhances one another. The robust interaction between them, depicted in Fig. 4, seamlessly integrates low-order structural and connectivity details into high-order positional hyperedges, providing rich semantic information for modeling complex relationships.

(2) Comprehensive Path Feature Extraction: This convolutional approach mirrors the bisection method applied to paths. By iteratively nesting downwards, it captures the features of bottom-level nodes, hyperedges, and their associated structures. As the method synthesizes paths upward through their central points, it infuses a broader structural context, ensuring a comprehensive path representation.

(3) Optimized Efficiency: As the aggregation‚Äôs receptive field expands, the increasing number and length of paths present challenges for complex operators, which would introduce significant computational and parametric overhead. KHGNN addresses this by reusing low-order convolutions, thereby minimizing the computational and parametric costs associated with extracting numerous path features.

# Relative Degree Encoding

The essence of hypergraph message passing lies in exchanging relative features between nodes via hyperedges or paths. Consequently, global node degrees are primarily utilized for normalizing the aggregated representations of nodes and hyperedges, rather than directly encoding them (Feng et al. 2019; Gao et al. 2022; Antelmi et al. 2023).

In social networks, more mutual friends between two strangers implies a higher likelihood of their acquaintance. KHGNN excels at capturing all shortest path relations between nodes. The abundance of these paths reflects the strength of relative relationships and the corresponding structural information. Relative degree encoding is introduced based on these principles.

Consider the hypergraph $\mathcal { G }$ with the $k$ -th hop connectivity between nodes denoted as $\mathcal { G } _ { k } = ( \nu , \mathcal { E } _ { k } )$ , where $\mathcal { E } _ { k }$ represents the high-order hyperedges constructed through bisection nested convolutions and $\mathcal { E } _ { 1 } = \mathcal { E }$ . The $k$ -th hop relative degree $d _ { v _ { i } , v _ { j } } ^ { k }$ between node $v _ { i }$ and $v _ { j }$ is defined as the number of hyperedges connecting the two nodes at the $k$ -th hop:

$$
d _ { v _ { i } , v _ { j } } ^ { k } = \{ | \begin{array} { l l } { \Big \{ v _ { k } \in \{ \mathscr { N } _ { v _ { i } } ^ { \frac { k } { 2 } } \cap \mathscr { N } _ { v _ { j } } ^ { \frac { k } { 2 } } \} \Big \} \Big | } & { \mathrm { i f ~ } k \mathrm { ~ i s ~ e v e n } } \\ { \Big \vert \Big \{ e _ { k } \in \{ \mathscr { Q } _ { v _ { i } } ^ { \frac { k - 1 } { 2 } } \cap \mathscr { Q } _ { v _ { j } } ^ { \frac { k - 1 } { 2 } } \} \Big \} \Big \vert } & { \mathrm { i f ~ } k \mathrm { ~ i s ~ o d d } } \end{array}  ,
$$

where $| \cdot |$ denotes the cardinality of a set. Consequently, the aggregation function, such as Eq. 4, is modified as follows:

$$
\begin{array} { r l r } {  { \hat { h } _ { v _ { i } } ^ { l , k } = \mathcal { A } \mathcal { G } \mathcal { G } \mathcal { N } _ { k } ^ { l } ( h _ { v } ^ { l - 1 } , \hat { h } _ { v } ^ { l , \frac { k } { 2 } } , \mathcal { N } _ { v } ^ { \frac { k } { 2 } } , \mathcal { N } _ { v } ^ { \frac { k } { 2 } } , S _ { v _ { i } } ^ { k } , \hat { d } ^ { k } ) } } \\ & { } & { = \sum _ { v _ { j } \in \mathcal { N } _ { v _ { i } } ^ { \frac { k } { 2 } } } ( \sum _ { v _ { d } \in \{ \mathcal { N } _ { v _ { j } } ^ { \frac { k } { 2 } } - S _ { v _ { i } } ^ { k } \} } ( f ( h _ { v _ { d } } ^ { l - 1 } ) + g ( \hat { h } _ { v _ { j } } ^ { l , \frac { k } { 2 } } ) + \hat { d } _ { v _ { i } , v _ { j } } ^ { k } ) ) . } \end{array}
$$

where dÀÜvki,vj is the embedding corresponding to dvki,vj .

# Computational Complexity Analysis

Consider a hypergraph $\mathcal { G }$ with $N$ nodes and $E$ hyperedges and the message-passing complexity per hyperedge in the hypergraph architecture is denoted as $F ( n )$ , where $n$ is the average number of nodes within high-order hyperedges constructed by bisection nested convolution. In HyperGINE, $F ( n ) \in [ { \dot { n } } , n ^ { 2 } )$ , with $n \ll N$ due to the inherent sparsity of real-world graphs. In KHGNN, the number of hops for node and hyperedge aggregation are denoted as $K _ { v }$ and $K _ { e }$ .

For odd hops of node representation, almost every hyperedge constructs higher-order hyperedges for the endpoints of the shortest paths represented by its bisection convolution features, leading to a time complexity of $O \left( \left\lceil \frac { K _ { v } } { 2 } \right\rceil E \cdot F ( n ) \right)$ . For even hops, the convolution of nearly every node becomes the shortest path features among some nodes, yielding a time complexity of $O \left( \Big \lceil \frac { K _ { v } } { 2 } \Big \rceil ^ { - } \overline { { N \cdot F ( n ) } } \right)$ . Therefore, the overall time complexity of node representation learning is $O \left( \left( \left\lceil \frac { K _ { v } } { 2 } \right\rceil E + \left\lceil \frac { K _ { v } } { 2 } \right\rceil N \right) \cdot F ( n ) \right)$ , a constant multiple of the complexity $O \left( \left( E + N \right) \cdot F ( n ) \right)$ found in most HGNNs.

Table 1: Performance comparison on hypergraph datasets (Mean accuracy $( \% ) \pm$ standard deviation), with the best results in bold, second-best underlined, and the last column showing the average performance ranking.   

<html><body><table><tr><td></td><td>Cora</td><td>Citeseer</td><td>Pubmed</td><td>Cora-CA</td><td>Mushroom</td><td>NTU2012</td><td>ModelNet40</td><td>Rank‚Üì</td></tr><tr><td>HGNN (Feng et al. 2019)</td><td>79.39 ¬± 1.36</td><td>72.45 ¬± 1.16</td><td>86.44 ¬± 0.44</td><td>82.64 ¬± 1.65</td><td>98.73 ¬± 0.32</td><td>87.72 ¬± 1.35</td><td>95.44 ¬± 0.33</td><td>7.71</td></tr><tr><td>HyperGCN (Yadati et al.2019)</td><td>78.45 ¬± 1.26</td><td>71.28 ¬± 0.82</td><td>82.84 ¬± 8.67</td><td>79.48 ¬± 2.08</td><td>47.90 ¬± 1.04</td><td>56.36 ¬± 4.86</td><td>75.89 ¬± 5.26</td><td>10</td></tr><tr><td>UniGCNII (Huang et al. 2021)</td><td>78.81 ¬± 1.05</td><td>73.05 ¬± 2.21</td><td>88.25 ¬± 0.40</td><td>83.60 ¬± 1.14</td><td>99.96 ¬± 0.05</td><td>89.30 ¬± 1.33</td><td>97.84 ¬± 0.25</td><td>4.43</td></tr><tr><td>Allset (Chien et al. 2022)</td><td>78.59 ¬± 1.47</td><td>73.08 ¬± 1.20</td><td>88.72 ¬± 0.37</td><td>83.63 ¬± 1.47</td><td>100.00 ¬± 0.00</td><td>88.69 ¬± 1.24</td><td>98.20 ¬± 0.20</td><td>3.71</td></tr><tr><td>HGNN+ (Gao et al. 2022)</td><td>79.81 ¬± 1.66</td><td>73.50 ¬± 1.10</td><td>87.83 ¬± 0.35</td><td>83.42 ¬± 1.07</td><td>99.88 ¬± 0.10</td><td>88.00 ¬± 1.39</td><td>94.72 ¬± 0.37</td><td>6.42</td></tr><tr><td>TriCL (Lee and Shin 2023)</td><td>79.21 ¬± 1.23</td><td>71.91 ¬± 1.26</td><td>85.69 ¬± 1.04</td><td>82.68 ¬± 1.24</td><td>99.90 ¬± 0.07</td><td>88.90 ¬± 1.08</td><td>97.26 ¬± 0.24</td><td>7</td></tr><tr><td>ED-HNN (Wang et al. 2023)</td><td>80.31 ¬± 1.35</td><td>73.70 ¬± 1.38</td><td>89.03 ¬± 0.53</td><td>83.97 ¬± 1.55</td><td>99.89 ¬± 0.09</td><td>88.07 ¬± 1.28</td><td>97.25 ¬± 0.28</td><td>4.14</td></tr><tr><td>Hypergraph-MLP (Tang et al.2024)</td><td>79.80 ¬± 1.82</td><td>73.90 ¬± 1.57</td><td>87.89 ¬± 0.55</td><td>82.32 ¬± 2.08</td><td>99.96 ¬± 0.07</td><td>88.42 ¬± 1.32</td><td>97.52 ¬± 0.26</td><td>5</td></tr><tr><td>HyperGINE</td><td>79.26 ¬± 0.41</td><td>73.72 ¬± 0.52</td><td>87.91 ¬± 0.28</td><td>82.88 ¬± 0.48</td><td>99.92 ¬± 0.06</td><td>88.52 ¬± 0.42</td><td>97.61 ¬± 016</td><td>5</td></tr><tr><td>KHGNN (ours)</td><td>80.67 ¬± 0.76</td><td>74.80 ¬± 1.10</td><td>88.47 ¬± 0.47</td><td>84.25 ¬± 0.74</td><td>100.00 ¬± 0.00</td><td>89.60 ¬± 1.64</td><td>98.33 ¬± 0.14</td><td>1.29</td></tr></table></body></html>

Similarly, time complexity for hyperedge representation learning is $O ( ( \lceil \frac { K _ { e } } { 2 } \rceil \bar { E } + \lceil \frac { K _ { e } } { 2 } \rceil N ) ) \cdot F ( \bar { e } ) )$ , where $e$ represents the average number of hyperedges within the highorder hyperedges constructed via bisection nested convolution. The runtime of KHGNN on real-world datasets will be discussed in ablation experiments.

# Experiments

In this section, we evaluate the performance of the proposed KHGNN across seven hypergraph datasets and nine graph datasets. These datasets encompass a variety of tasks, including node classification (e.g., ModelNet40), link prediction (e.g., PCQM-Contact), image classification (e.g., MNIST), and molecular property prediction (e.g., MolHIV). Detailed descriptions of the datasets and experimental hyperparameter settings can be found in Appendix A and $\mathbf { B } ^ { 1 }$ .

# Comparison on Hypergraph Datasets

Datasets. KHGNN is evaluated on seven hypergraph datasets: Cora, Citeseer, Pubmed, Cora-CA (Yadati et al. 2019), Mushroom (Asuncion and Newman 2007), NTU2012 (Chen et al. 2003), and ModelNet40 (Wu et al. 2015). Following the setup used by Chien et al. (2022), the nodes in each dataset are randomly split into training, validation, and test sets with a ratio of 50/25/25.

Results. Table 1 presents the performance of various models on hypergraph datasets. The proposed KHGNN achieves SOTA results on most datasets while maintaining competitive accuracy on the Pubmed dataset. By expanding the receptive fields of nodes and hyperedges, KHGNN enhances the performance of HyperGINE in node classification, suggesting that distant nodes provide richer information and improve node differentiation. Unlike the straightforward stacking of layers in other HGNNs, KHGNN incorporates path features of nodes, hyperedges, and structures as contextual information, effectively modeling semantic relationships for more precise predictions.

# Comparison on Graph Datasets

Unlike node classification tasks which rely on a limited receptive field, graph-level tasks require each node to capture as much graph information as possible to effectively represent graph embeddings. However, the majority of HGNNs overlook graph-level tasks due to their inability to provide sufficient information for nodes and hyperedges. By treating graphs as a special case of hypergraphs, KHGNN effectively models high-order relationships between distant nodes and edges through path features that encapsulate structural information, thereby boosting downstream task performance.

Result on short-range graph benchmark. Table 2 presents the experimental results of KHGNN on datasets characterized by a short average graph diameter, including ZINC, MNIST, CIFAR10, MolHIV, and MolPCB from Benchmarking GNNs (Dwivedi et al. 2023) and Open Graph Benchmark (Hu et al. 2020).

We observe that models with larger receptive fields, such as graph transformers (GT) and KHGNN, outperformed models with limited receptive fields (e.g., HGNN) on small molecule datasets. Notably, our model sets new state-of-theart performance on ZINC, MNIST, CIFAR10, and MolHIV by expanding the receptive field of both nodes and edges simultaneously. These results also demonstrate that appropriate path features can more effectively model relationships between nodes, leading to more suitable graph embeddings for downstream tasks compared to attention mechanisms. For a detailed experimental comparison with MP GNNs, including most K-hop GNNs, please refer to Appendix C.

Result on long-range graph benchmark. Table 3 presents the performance of KHGNN on Long-Range Graph Benchmark (LRGB) (Dwivedi et al. 2022b), including PascalVOC-SP, Peptides-func, Peptides-struct, and PCQMContact, which feature longer average graph diameters.

Traditionally, graph transformer-based GNNs with their global receptive fields outperform message-passing GNNs in modeling long-range node interactions, as demonstrated by significant performance gaps. Despite this, KHGNN, adhering to the 500K parameter budget set by Dwivedi et al (2022b), transcends the locality constraints of conventional message passing by enabling information flow along all shortest paths. Unlike PathNN (Michel et al. 2023), which constructs path features exclusively from nodes, KHGNN leverages its bisection nested convolution operator to capture the comprehensive features of nodes, edges, and structure in paths, thereby enabling more precise modeling of long-range interactions. This capability to encode multiple relational pathways empowers KHGNN to exceed the performance of graph transformers, resulting in SOTA outcomes on all evaluated datasets.

Table 2: Performance comparison of KHGNN and the SOTA models, including message passing-based GNNs (MP) and graph transformer-based GNNs (GT) on datasets from (Dwivedi et al. 2023) and (Hu et al. 2020), which are short-range graph benchmarks. Results are reported as Mean $\pm$ st. over 5 random seeds.   

<html><body><table><tr><td rowspan="2">Frame</td><td rowspan="2">Model</td><td>ZINC</td><td>MNIST</td><td>CIFAR10</td><td>MolHIV</td><td>MolPCBA</td></tr><tr><td>MAE‚Üì</td><td>Accuracy‚Üë</td><td>Accuracy‚Üë</td><td>AUROC‚Üë</td><td>AP‚Üë</td></tr><tr><td rowspan="9">MP</td><td>GCN (Kipf and Welling 2017)</td><td>0.367 ¬± 0.011</td><td>90.705 ¬± 0.218</td><td>55.710¬± 0.381</td><td>0.7606¬± 0.0097</td><td>0.2020¬±0.0024</td></tr><tr><td>HGNN (Feng et al. 2019)</td><td>0.236 ¬±0.006</td><td>97.070 ¬± 0.191</td><td>67.210 ¬± 0.349</td><td>0.7632 ¬± 0.0181</td><td>0.2622 ¬± 0.0013</td></tr><tr><td>GIN (Hu et al. 2019)</td><td>0.526 ¬± 0.051</td><td>96.485 ¬± 0.252</td><td>55.255 ¬± 1.527</td><td>0.7778 ¬± 0.0130</td><td>0.2266 ¬± 0.0028</td></tr><tr><td>GatedGCN(Bresson et al. 2018)</td><td>0.282 ¬± 0.015</td><td>97.340 ¬± 0.143</td><td>67.312 ¬± 0.311</td><td>0.7874 ¬± 0.0119</td><td>0.2620 ¬± 0.0010</td></tr><tr><td>PNA (Corso et al. 2020)</td><td>0.188 ¬± 0.004</td><td>97.940 ¬± 0.120</td><td>70.350 ¬± 0.630</td><td>0.7905 ¬± 0.0132</td><td>0.2838 ¬± 0.0035</td></tr><tr><td>DGN (Beaini et al. 2021)</td><td>0.168 ¬± 0.003</td><td></td><td>72.838 ¬± 0.417</td><td>0.7970 ¬± 0.0097</td><td>0.2885 ¬± 0.0030</td></tr><tr><td>AGENTNET(Martinkus etal.2023)</td><td>0.144 ¬± 0.016</td><td></td><td></td><td>0.7833 ¬± 0.0069</td><td>0.2549 ¬± 0.0027</td></tr><tr><td>GINE-ViT (He et al.2023)</td><td>0.085 ¬± 0.005</td><td>98.200 ¬± 0.050</td><td>69.670¬± 0.400</td><td>0.7792 ¬± 0.0149</td><td></td></tr><tr><td>d-DRFWL (Zhou et al. 2024)</td><td>0.077 ¬± 0.002</td><td></td><td></td><td>0.7818 ¬± 0.0219</td><td>0.2538 ¬± 0.0019</td></tr><tr><td rowspan="4">GT</td><td>EGT(Hussain et al. 2022)</td><td>0.108 ¬±0.009</td><td>98.173 ¬± 0.087</td><td>68.702 ¬± 0.409</td><td>0.8060¬± 0.0065</td><td>0.2961¬± 0.0024</td></tr><tr><td>GraphTrans-ViT (He et al.2023)</td><td>0.096 ¬± 0.007</td><td>97.250 ¬± 0.230</td><td>72.110 ¬± 0.550</td><td>0.7755 ¬± 0.0208</td><td></td></tr><tr><td>SGHormer (Zhang et al. 2024)</td><td>0.117 ¬± 0.032</td><td>96.850 ¬± 0.247</td><td>67.740 ¬± 0.158</td><td>0.7747 ¬± 0.0040</td><td>0.2743¬±0.0010</td></tr><tr><td>GraphiT+MAP (Ma et al. 2024)</td><td>0.160 ¬± 0.006</td><td>98.104 ¬± 0.212</td><td></td><td>0.7690 ¬± 0.0110</td><td></td></tr><tr><td rowspan="2">MP</td><td>HyperGINE</td><td>0.118 ¬±0.004</td><td>97.523 ¬± 0.030</td><td>69.587 ¬± 0.101</td><td>0.7792 ¬± 0.0024</td><td>0.2690¬± 0.0015</td></tr><tr><td>KHGNN (ours)</td><td>0.074 ¬± 0.002</td><td>98.494 ¬± 0.045</td><td>74.807 ¬± 0.265</td><td>0.8084 ¬± 0.0063</td><td>0.2906 ¬± 0.0037</td></tr></table></body></html>

Table 3: Results of KHGNN on 4 tasks from Long-Range Graph Benchmark, shown as percentages.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="3">PascalVOC-SPPeptides-funcPeptides-structPCQM-Contact</td><td></td></tr><tr><td>F1‚Üë</td><td>AP‚Üë</td><td>MAE‚Üì</td><td>MRR‚Üë</td></tr><tr><td>GCN (2017)</td><td>12.68 ¬± 0.60</td><td>59.30¬±0.23</td><td>34.96 ¬± 0.13</td><td>32.34 ¬± 0.06</td></tr><tr><td>HGNN (2019)</td><td>27.38 ¬± 0.28</td><td>60.57 ¬± 0.54</td><td>27.57 ¬± 0.26</td><td>32.85 ¬± 0.09</td></tr><tr><td>GINE (2019)</td><td>12.65 ¬± 0.76</td><td>54.98 ¬± 0.79</td><td>35.47 ¬± 0.45</td><td>31.80 ¬± 0.27</td></tr><tr><td>GatedGCN (2018)</td><td>28.73 ¬± 2.19</td><td>58.64 ¬± 0.77</td><td>34.20 ¬± 0.13</td><td>32.18 ¬± 0.11</td></tr><tr><td>PathNN (2023)</td><td></td><td>68.16 ¬± 0.26</td><td>25.45 ¬± 0.32</td><td></td></tr><tr><td>OptBasisGNN(2023) 33.83¬±0.61</td><td></td><td>61.92 ¬± 0.75</td><td>25.61 ¬± 0.19</td><td>32.42 ¬± 0.45</td></tr><tr><td>GUMP(2024)</td><td></td><td>68.43 ¬± 0.37</td><td>25.64 ¬± 0.23</td><td>34.13 ¬± 0.15</td></tr><tr><td>d-DRFWL(2024)</td><td>1</td><td>59.53 ¬± 0.48</td><td>25.94 ¬± 0.38</td><td></td></tr><tr><td>SAN+LapPE(2023)</td><td>32.30¬±0.39</td><td>63.84 ¬± 1.21</td><td>26.83 ¬± 0.43</td><td>33.50 ¬± 0.03</td></tr><tr><td>SAN+RWSE(2022a)</td><td>32.16 ¬± 0.27</td><td>64.39 ¬± 0.75</td><td>25.45 ¬± 0.12</td><td>33.41 ¬± 0.06</td></tr><tr><td>GPS (2022)</td><td>37.48 ¬±1.09</td><td>65.35 ¬± 0.41</td><td>25.00 ¬± 0.05</td><td>33.37 ¬± 0.06</td></tr><tr><td>Specformer (2023)</td><td>35.64 ¬± 0.85</td><td>66.86 ¬± 0.64</td><td>25.50 ¬± 0.14</td><td>33.73 ¬±0.27</td></tr><tr><td>HyperGINE</td><td>29.29 ¬± 0.12</td><td>61.06 ¬± 0.25</td><td>26.40 ¬± 0.03</td><td>33.13 ¬± 0.05</td></tr><tr><td>KHGNN (ours)</td><td>38.30 ¬± 0.70</td><td>70.11 ¬± 0.30</td><td>24.77 ¬± 0.14</td><td>35.60 ¬± 0.04</td></tr></table></body></html>

# Ablation Study

Finally, a series of ablation studies are conducted on Peptides-fun, ZINC and Citeseer to assess the contributions of various components of KHGNN to its performance, with results summarized in Table 4.

The results reveal that increasing the number of hops for node aggregation $( K _ { v } )$ and edge aggregation $( K _ { e } )$ consistently enhances KHGNN‚Äôs performance on Peptides-func, ZINC, and Citeseer. The most significant improvement is observed on Peptides-func, attributed to its longer average graph diameter and the heightened importance of long-range interactions. Additionally, the training time of KHGNN scales linearly with $K _ { v }$ and $K _ { e }$ across all datasets, as detailed in the Complexity Analysis, without experiencing explosive growth as the receptive field expands.

Table 4: Ablation study results for $K _ { v }$ -hop node representation learning and $K _ { e }$ -hop hyperedge representation learning, along with the training time for each epoch.   

<html><body><table><tr><td rowspan="2">KKe</td><td rowspan="2">Relative Encoding</td><td colspan="2">Peptides-func</td><td colspan="2">ZINC</td><td colspan="2">Citeseer</td></tr><tr><td>AP‚Üë</td><td></td><td></td><td></td><td>Time(s)MAE‚ÜìTime(s) Acc(%)‚Üë Time(s)</td><td></td></tr><tr><td>1</td><td></td><td>0.5965</td><td>7.9</td><td>0.1309</td><td>14.7</td><td>73.46</td><td>0.2</td></tr><tr><td>2</td><td></td><td>0.6438</td><td>9.1</td><td>0.1153</td><td>19.4</td><td>73.76</td><td>0.2</td></tr><tr><td>2 1</td><td></td><td>0.6525</td><td>10.5</td><td>0.1089</td><td>23.8</td><td>73.93</td><td>0.2</td></tr><tr><td>3 1</td><td></td><td>0.6745</td><td>11.6</td><td>0.0830</td><td>27.4</td><td>74.15</td><td>0.3</td></tr><tr><td>4 1</td><td>‚àö</td><td>0.6866</td><td>12.8</td><td>0.0776</td><td>29.9</td><td>74.27</td><td>0.4</td></tr><tr><td>4 2</td><td></td><td>0.6903</td><td>14.3</td><td>0.0758</td><td>34.1</td><td>74.45</td><td>0.4</td></tr><tr><td>5 2</td><td></td><td>0.6951</td><td>15.7</td><td>0.0755</td><td>37.6</td><td>74.67</td><td>0.5</td></tr><tr><td>6 2</td><td></td><td>0.7011</td><td>17.1</td><td>0.0743</td><td>39.7</td><td>74.80</td><td>0.6</td></tr><tr><td>6 2</td><td></td><td>0.6922</td><td>13.4</td><td>0.1196</td><td>35.6</td><td>74.59</td><td>0.4</td></tr></table></body></html>

# Conclusion

In this paper, we have proposed a $K$ -hop hypergraph neural network (KHGNN), marking the first attempt to utilize path features to guide hypergraph message passing. Our approach departs from existing HGNNs by utilizing a comprehensive bisection aggregation that synthesizes information from nodes, hyperedges, and structures along all shortest paths, thereby facilitating more effective long-range message propagation. Experimental results demonstrate that KHGNN exhibits superior expressive power in both hypergraph and graph tasks, particularly achieving significant performance improvements in modeling long-range dependencies.