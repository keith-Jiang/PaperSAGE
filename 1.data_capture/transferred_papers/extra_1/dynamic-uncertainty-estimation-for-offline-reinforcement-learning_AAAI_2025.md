# Dynamic Uncertainty Estimation for Offline Reinforcement Learning

Jiesheng Wang1, Lin $\mathbf { L i } ^ { 1 }$ , Wei Wei1\*, Yujia Zhang1, Xin Yang2

1 Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan, Shanxi, China   
2 School of Computing and Artificial Intelligence, Southwestern University of Finance and Economics, Chengdu, Sichuan, China

# Abstract

Offline reinforcement learning confronts the distributional shift challenge, a consequence of learning policy from static datasets. Current methods primarily handle this issue by aligning the learned policy with the behavior policy or conservatively estimating Q-values for out-of-distribution (OOD) actions. However, these approaches can lead to overly pessimistic estimation of $\mathrm { \bf Q }$ -values of the OOD actions in unfamiliar situations, resulting in a suboptimal policy. To address this, we propose a new method, Dynamic Uncertainty estimation for Offline Reinforcement Learning. This method introduces a base density-truncated OOD data sampling approach to reduce the impact of extrapolation errors on uncertainty estimation. It enables conservative estimation of Qvalues for OOD actions while avoiding negative impacts on in-distribution data. We also develop a dynamic uncertainty estimation mechanism to prevent excessive pessimism and enhance the generalization of the Q-function. This mechanism dynamically adjusts the degree of pessimism in the Qfunction by minimizing the error between target and estimated values. Our method outperforms existing algorithms, as demonstrated by experimental results based on the D4RL benchmark, and proves its superiority in addressing the distributional shift challenge.

# Introduction

In recent years, deep reinforcement learning (DRL) has achieved significant success in various domains, such as gaming (Silver et al. 2017) and industrial automation (Chen et al. 2019). DRL trains agents through extensive trial-anderror interactions with the environment. However, such trialand-error interactions are costly and dangerous in fields like healthcare (Yu et al. 2021) and autonomous driving (Hoel, Wolff, and Laine 2023). Offline reinforcement learning (RL) offers a promising direction for the practical application of RL by learning optimal policies from pre-collected data, thus avoiding direct interactions between the agent and the environment.

Due to the extrapolation errors caused by distributional shift (Fujimoto, Meger, and Precup 2019), online RL methods cannot be directly applied to offline RL. Policy constraint and value constraint are employed for model-free offline RL to address this issue. Policy constraint methods, such as BEAR (Kumar et al. 2019), TD3-BC (Fujimoto and Gu 2021), UWAC (Wu et al. 2021), and EMAQ (Ghasemipour, Schuurmans, and Gu 2021) alleviate distributional shift by making the learned policy as close as possible to the behavior policy.

Policy constraint methods limit the optimization space of the learned policy, making it difficult to surpass the behavior policy. Conversely, value constraint methods typically achieve conservative estimation of the Q-function, alleviating extrapolation errors caused by distributional shifts and enhancing the learned policy. For instance, BRAC (Wu, Tucker, and Nachum 2019) employs a behavior regularization term to estimate the Q-function conservatively. CQL (Kumar et al. 2020) directly minimizes the Q-values of outof-distribution (OOD) actions. IQL (Kostrikov, Nair, and Levine 2021) conservatively estimates Q-function by avoiding querying OOD actions. ATAC (Cheng et al. 2022) uses adversarial training to constrain OOD actions. CSVE (Chen et al. 2023) regularizes OOD actions by conservatively estimating the state value function. RND (Nikulin et al. 2023) and DRND (Yang et al. 2024) mitigate extrapolation errors through the use of random networks. However, these methods can lead to overly conservative Q-functions, which affect the improvement of the policy.

In recent years, methods that estimate uncertainty through an ensemble of Q-networks to conservatively estimate the Q-values of OOD actions have been gaining traction. EDAC (An et al. 2021) estimates uncertainty by using a large ensemble of Q-networks. PBRL (Bai et al. 2022) applies uncertainty penalty separately to OOD actions and indistribution data by sampling additional OOD data. While these methods mitigate extrapolation errors to some extent, they lack precise characterization of OOD data, which can negatively affect in-distribution data and make it difficult to provide accurate uncertainty estimation. Moreover, in their pursuit of effective OOD actions regularization, existing methods often overlook the importance of generalization for policy improvement, which results in excessively pessimistic penalty that severely impact the generalization of the Q-function, hindering policy enhancement, ultimately leading to the agent frequently ending up learning suboptimal or even inferior policy. Therefore, dynamically adjusting the degree of conservatism of the Q-function during training is crucial for effectively regularizing OOD actions while avoiding excessive pessimism and enhancing the generalization of the Q-function.

However, training Q-functions using only pre-collected offline datasets cannot effectively regularize OOD actions, making it difficult to obtain accurate uncertainty estimation. To address this issue, we propose incorporating OOD data into the training buffer through OOD data sampling. However, existing OOD data sampling methods lack precise OOD data characterization and struggle to distinguish it, potentially harming the Q-function values of in-distribution data. To effectively regularize OOD actions and enhance the reliability of uncertainty estimation, we propose a base density-truncated OOD data sampling approach (DT). By utilizing a reliable explicit density estimation method provided by Flow-GAN (Grover, Dhar, and Ermon 2018), DT can effectively distinguish OOD actions, regularize OOD actions to alleviate extrapolation errors, and make uncertainty estimation more accurate. In addition, we propose a dynamic uncertainty estimation mechanism designed to avoid excessive pessimism and enhance the generalization of the Q-function by constructing auxiliary target values and implementing dynamic uncertainty penalty, which is inspired by recent advancements in online dynamic value function estimation GPL (Cetin and Celiktutan 2023) and CEILING (Zhang et al. 2024a). To this end, by integrating the dynamic uncertainty estimation mechanism with DT, we propose Dynamic Uncertainty Estimation for Offline RL (DURL).

Our work makes three main contributions:

• Proposing a base density-truncated OOD data sampling approach to limit extrapolation errors and improve uncertainty estimation accuracy effectively. • Proposing a dynamic uncertainty estimation method to avoid excessive pessimism and enhance the generalization of the Q-function in model-free offline RL. • Extensive experiments were conducted on the D4RL benchmark test, demonstrating the superior performance of DURL compared to existing algorithms.

# Related Work

In model-free offline RL, there are mainly two types of methods: policy constraint and value constraint. Policy constraint methods aim to make the learned policy closer to the behavior policy, thereby avoiding performance degradation caused by agent-taking OOD actions. For example, BCQ (Fujimoto, Meger, and Precup 2019) models the behavior policy using generative models to make the learned policy closer to the behavior policy. BEAR (Kumar et al. 2019) measures the difference between the behavior policy and the learned policy using the Maximum Mean Discrepancy distance, ensuring that the difference does not exceed a certain threshold. UWAC (Wu et al. 2021) uses Monte Carlo Dropout for uncertainty estimation and employs an uncertainty weighting mechanism to reduce the impact of distributional shift on policy improvement. TD3-BC (Fujimoto and $\mathrm { G u } 2 0 2 1 \rangle$ ) adds a behavior cloning regularization term to the loss function of the value function, making the learned policy closer to the behavior policy. However, policy constraint methods overly rely on the behavior policy, limiting the space for policy improvement and making it difficult for the learned policy to surpass the behavior policy.

In value constraint methods, CQL (Kumar et al. 2020) and PSPI (Xie et al. 2021) address overestimation errors by adding a conservative regularization term to estimate the Q-values of OOD actions conservatively. SAC-N (An et al. 2021) estimates uncertainty using an ensemble of numerous Q-networks. EDAC (An et al. 2021) builds upon SAC-N by increasing the diversity within the Q-network ensemble to reduce the number of required Q-networks. However, some datasets still necessitate a large number of Q-network ensembles. PBRL (Bai et al. 2022) employs an ensemble of Q-networks to estimate uncertainty and applies additional OOD data sampling to conservatively estimate the Q-values for OOD actions and in-distribution actions separately, requiring fewer networks compared to EDAC. MCQ (Lyu et al. 2022) regularizes OOD actions by assigning mild but sufficiently conservative pseudo target values to each OOD action. CSVE (Chen et al. 2023) regularizes OOD actions by conservatively estimating the state value function. However, these methods often struggle to effectively distinguish OOD actions and tend to result in excessively conservative Q-function estimation, which impairs the generalization of the Q-function and hinders policy improvement. In contrast, our approach leverages DT and a dynamic uncertainty estimation mechanism to effectively regularize OOD actions while avoiding excessive pessimism, thereby maintaining better generalization of the Q-function.

In model-based offline RL, uncertainty estimation methods are also widely utilized. MOPO (Yu et al. 2020) learns multiple environment models and estimates uncertainty by evaluating the prediction discrepancies among these models, thereby applying penalty to the reward function. MOReL (Kidambi et al. 2020) introduces an uncertainty boundary to ensure the safety and robustness of the policy. TATU (Zhang et al. 2023) leverages uncertainty estimation to penalize the reward function and employs it for data augmentation. OCEAN (Wu et al. 2024) performs conservative exploration through uncertainty estimation. However, these methods share similar limitations with model-free approaches such as SAC-N, EDAC, and PBRL.

In recent years, the application of generative models in offline reinforcement learning has garnered increasing attention. BCQ (Fujimoto, Meger, and Precup 2019) utilizes a Variational Autoencoder to estimate the behavior policy density, aiming to align the learned policy closely with the behavior policy. SPOT (Wu et al. 2022) employs a Conditional Variational Autoencoder to learn the behavior policy density, guiding the policy improvement process. SFBC (Chen et al. 2022) and Diffuser (Janner et al. 2022) use diffusion models to estimate the behavior policy density. However, these methods implicitly estimate the behavior policy density and cannot provide precise density estimation. FlowGAN (Grover, Dhar, and Ermon 2018), a variant of GAN (Goodfellow et al. 2014), integrates Maximum Likelihood Estimation (MLE) with adversarial learning to deliver more accurate density estimation.

# Preliminaries

RL is a machine learning paradigm where an agent learns to maximize cumulative rewards through interactions with its environment. This process can be modeled as a Markov Decision Process (MDP). An MDP is a mathematical model used to describe RL environments, represented by a 6-tuple: $\langle S , A , R , P , \rho , \gamma \rangle$ , where $S$ denotes the state space, $A$ denotes the action space, $R$ represents the reward function, $P ( s ^ { \prime } | s , a )$ is the state transition function, $\rho$ denotes the initial state distribution, and $\gamma$ is the discount factor ranging from [0,1]. The goal of RL is to learn an optimal policy $\pi ^ { * } ( a | s )$ that maximizes the expected cumulative reward $J = \mathbb { E } \left[ \sum \gamma ^ { t } r _ { t } \right]$ . The state-action value function $Q _ { \pi } ( s , a )$ represents the discounted return obtained by following the current policy $\pi$ after taking action $a$ in state $s$ . Actor-Critic is a widely used algorithmic framework in RL, comprising two main components: policy evaluation and policy improvement. The objective of the policy evaluation component is to minimize the following loss function:

$$
L ( \theta ) = \mathbb { E } \left[ Q _ { \theta } ( s , a ) - T Q _ { \theta ^ { - } } ( s ^ { \prime } , a ^ { \prime } ) \right] ,
$$

where $T$ is a Bellman operator defined as $T = r ( s , a ) +$ $\gamma \mathbb { E } \left[ Q _ { \theta ^ { - } } ( s ^ { \prime } , a ^ { \prime } ) \right]$ , $\theta$ are the parameters of the Q-function, and $\theta ^ { - }$ are the parameters of the target Q-network (Osband et al. 2016).

The policy improvement component aims to update the policy in a direction that maximizes the Q-function:

$$
L ( \psi ) = \operatorname* { m a x } \mathbb { E } \left[ Q _ { \theta } ( s , a ) \right] ,
$$

where $\psi$ are the parameters of the policy $\pi$ .

Offline RL is a subfield of RL where the agent does not directly interact with the environment but instead learns from a pre-collected dataset $D = \{ ( s _ { t } , a _ { t } , r _ { t } , s _ { t + 1 } , d _ { t } ) \} _ { i }$ . Here, $d _ { t }$ denotes the termination signal, and actions $a _ { t }$ are generated by a behavior policy $\mu$ . Offline RL aims to learn an optimal policy from the offline dataset $D$ . Applying standard RL algorithms such as SAC (Haarnoja et al. 2018), QMD3 (Wei et al. 2022), and RD3 (Zhang et al. 2024b) to offline RL can be challenging due to the issue of extrapolation error.

# Method

In this section, we first introduce explicit methods for estimating the behavior policy density. We then describe how to regularize OOD actions and avoid overly pessimistic Qfunction estimation through dynamic uncertainty estimation mechanisms and DT. Finally, we provide a detailed implementation of our algorithm.

# Explicit Density Estimation

GAN (Goodfellow et al. 2014) have achieved significant success in various domains, including image generation (Radford, Metz, and Chintala 2015), style transfer (Zhu et al. 2017), and data augmentation (Antoniou, Storkey, and Edwards 2017). A GAN comprises two primary components: a generator (G) and a discriminator (D). These components are trained adversarially to produce high-quality data. The optimization objective of a GAN is formulated as follows:

$$
\operatorname* { m a x } _ { \nu } \mathbb { E } _ { x \sim p _ { d a t a } } [ \log D _ { \nu } ( x ) ] + \mathbb { E } _ { x \sim p _ { \phi } } [ \log ( 1 - D _ { \nu } ( x ) ) ] ,
$$

where $p _ { \mathrm { d a t a } }$ denotes the distribution of real data, $p _ { \phi }$ represents the distribution of generated samples, $\phi$ refers to the parameters of the generator $G$ , and $\nu$ denotes the parameters of the discriminator $D$ . The application of GAN in offline RL is gaining increasing attention (Yang et al. 2022b) (Yang et al. 2022a). However, traditional GAN do not provide explicit density estimation methods, making it challenging to estimate the density of behavior policy accurately. Normalizing Flows (Dinh, Krueger, and Bengio 2014) represent a class of generative models that map simple prior distributions (such as Gaussian distributions) to complex target distributions via invertible transformations, and provide a direct method for calculating log-likelihood, which is essential for density estimation. However, Normalization Flows face challenges when dealing with high-dimensional data.

Flow-GAN integrates Normalizing Flows as the generator within a GAN framework, leveraging the strengths of both generative adversarial networks and Normalizing Flows. This approach not only enables high-quality sample generation but also facilitates effective density estimation.

Assume that the Flow-GAN generator $G _ { \phi }$ is an invertible transformation in $\mathbb { R } ^ { d }$ , implying the existence of an inverse function $h _ { \phi } = G _ { \phi } ^ { - 1 }$ . Let $\tau$ denote a trajectory in the offline dataset $D$ . The probability density of $\tau$ on the offline dataset $D$ can be expressed as follows:

$$
\zeta _ { \phi } ( \tau ) = \zeta ( h _ { \phi } ( \tau ) ) \left| \mathrm { d e t } \ \frac { \partial h _ { \phi } ( \tau ) } { \partial \phi } \right| ,
$$

$\zeta$ represents the probability density of the prior distribution, while $\zeta _ { \phi }$ denotes the probability density of the target distribution. The ter m ∂h∂ϕϕ(τ) represents the Jacobian matrix of $h _ { \phi }$ at $\tau$ . To ensure that $G _ { \phi }$ is invertible and to facilitate the computation of the Jacobian matrix for highdimensional distributions, we utilize Normalized Flow models such as NICE (Dinh, Krueger, and Bengio 2014) or Real NVP (Dinh, Sohl-Dickstein, and Bengio 2016), which have triangular Jacobian matrices, to construct the generator.

However, training Flow-GAN with traditional adversarial training methods often leads to suboptimal performance in term of log-likelihood, resulting in decreased accuracy in density estimation. On the other hand, Flow-GAN trained using MLE may produce samples of lower quality. To strike a balance between these two objectives, a hybrid loss function can be employed for training Flow-GAN:

$$
\operatorname* { m i n } _ { \phi } \operatorname* { m a x } _ { \nu } \mathcal { H } ( D _ { \nu } , G _ { \phi } ) - \lambda \mathbb { E } _ { \tau \sim p _ { \mathrm { d a t a } } } \left[ \log \zeta _ { \phi } ( \tau ) \right] ,
$$

where $\mathcal { H } ( D _ { \nu } , G _ { \phi } )$ can represent any GAN loss function, In our implementation, we use the Wasserstein GAN (Arjovsky, Chintala, and Bottou 2017) loss function. $\lambda \geq 0$ is a hyperparameter that balances adversarial training and MLE. By adjusting $\lambda$ , it is possible to fine-tune the Flow-GAN according to the specific requirements of the application, effectively managing the trade-off between density estimation accuracy and sample quality.

As depicted on the left side of Figure 1, current methods for sampling OOD data are highly random and lack precise characterization of OOD data, making it difficult to distinguish OOD actions accurately. This imprecision can adversely affect the Q-function values of in-distribution data.

![](images/6071ceb379b27b31928769b404d22661609f671e3698c6e7916d39344254dad8.jpg)  
Figure 1: The left panel displays the results of traditional OOD data sampling. The right panel presents the direct density estimation of the sampled data using DT, which classifies the data into high-density and low-density data.

DT uses Equation (2) to directly estimate the density of the sampled data. However, it is difficult to directly determine whether the density exceeds a certain threshold to divide the sampled data. Therefore, we adopt a truncation method to divide the sampled data into two parts. Specifically, we divide the first $N$ highest-density data into high-density data and the remaining into low-density data, as shown on the right side of Figure 1. We can flexibly adjust the size of the high-density data area through $N$ . We only perform OOD regularization on low-density data to avoid harmful effects on the Q-function values of in-distribution data, thereby effectively mitigating extrapolation errors and reducing their impact on uncertainty estimation.

# Dynamic Uncertainty Estimation

In offline RL, uncertainty measures can be utilized to perform conservative estimation of the Q-function, thus alleviating extrapolation errors.

We employ the Bootstrapped DQN (Osband et al. 2016) method for uncertainty estimation by integrating multiple Q-networks. Specifically, we use $M$ Q-networks, where $Q _ { \theta _ { m } } ( s , a )$ represents the Q-function of the $m$ -th network, and $Q _ { \theta _ { m } ^ { - } } ( s , a )$ denotes the corresponding target network. The estimation of uncertainty $H ( s , a )$ is computed as follows:

$$
\begin{array} { l } { { \displaystyle { \cal H } ( s , a ) = \sqrt { \frac { 1 } { M } \sum _ { m = 1 } ^ { M } \left( Q _ { \theta _ { m } } ( s , a ) - \bar { Q } ( s , a ) \right) ^ { 2 } } , } } \\ { { \displaystyle \bar { Q } ( s , a ) = \frac { 1 } { M } \sum _ { m = 1 } ^ { M } Q _ { \theta _ { m } ^ { - } } ( s , a ) . } } \end{array}
$$

We leverage uncertainty estimation to perform conservative Q-function estimation. For a sample $( s , a , r , s ^ { \prime } )$ drawn from the offline dataset $D$ , the target value for the Q-function can be expressed as:

$T Q _ { \theta } ^ { m } ( s , a ) = r ( s , a ) + \gamma \mathbb { E } _ { a ^ { \prime } \sim \pi _ { \psi } } \left[ \bar { Q } ( s ^ { \prime } , a ^ { \prime } ) - \beta H ( s ^ { \prime } , a ^ { \prime } ) \right] ,$ (4) where $T$ denotes the Bellman operator, and $\beta$ is a learnable parameter.

However, training the Q-function exclusively on offline datasets makes it difficult to effectively regularize OOD actions. To address this issue, we use DT to effectively regularize OOD data. Specifically, we sample states $s ^ { \mathrm { o o d } }$ from the offline dataset $D$ and use the current policy $\pi _ { \psi }$ to sample OOD actions $a ^ { \mathrm { o o d } }$ , where $a ^ { \mathrm { o o d } } \sim \pi _ { \psi } ( \cdot \mid s ^ { \mathrm { o o d } } )$ . Then, we use DT to partition the sampled data, removing high-density data to avoid adverse effects on the $\mathrm { Q }$ -values of in-distribution data. Since the state transition function $P ( \cdot \mid s ^ { \mathrm { o o d } } , a ^ { \mathrm { o o d } } )$ and the reward function $r ( s ^ { \mathrm { o o d } } , a ^ { \mathrm { o o d } } )$ are unknown, we construct an auxiliary target value for the OOD data as follows:

$$
\tilde { T } Q _ { \theta _ { m } } ( s ^ { \mathrm { o o d } } , a ^ { \mathrm { o o d } } ) = Q _ { \theta _ { m } } ( s ^ { \mathrm { o o d } } , a ^ { \mathrm { o o d } } ) - \omega H ( s ^ { \mathrm { o o d } } , a ^ { \mathrm { o o d } } ) ,
$$

where $\tilde { T }$ denotes the Bellman operator, and $\omega$ is a learnable parameter. By applying Equation (5), we suppress the $\mathrm { \bf Q }$ - function values for OOD actions, thereby effectively regularizing OOD actions and mitigating the impact of extrapolation errors on uncertainty estimation.

By applying Equations (4) and (5), we derive the loss function for each $Q _ { \theta _ { m } }$ in DURL as follows:

$$
\begin{array} { r l } & { \underset { \theta _ { m } } { \mathrm { m i n } } \mathbb { E } _ { ( s , a , r , s ^ { \prime } ) \sim D } \left[ \left( T Q _ { \theta _ { m } } ( s , a ) - Q _ { \theta _ { m } } ( s , a ) \right) ^ { 2 } \right] } \\ & { \quad + \mathbb { E } \left[ \left( \tilde { T } Q _ { \theta _ { m } } ( s ^ { \mathrm { o o d } } , a ^ { \mathrm { o o d } } ) - Q _ { \theta _ { m } } ( s ^ { \mathrm { o o d } } , a ^ { \mathrm { o o d } } ) \right) ^ { 2 } \right] , } \end{array}
$$

where $s ^ { \mathrm { o o d } } \sim D , a ^ { \mathrm { o o d } } \sim \pi _ { \psi }$ . We obtain the policy $\pi _ { \psi }$ by solving the following optimization problem:

$$
\operatorname* { m a x } _ { \psi } \mathbb { E } _ { s \sim D , a \sim \pi _ { \psi } } \left[ \operatorname* { m i n } _ { m = 1 , \ldots , M } Q _ { \theta _ { m } } ( s , a ) - \alpha \log \pi _ { \psi } ( \cdot | s ) \right] .
$$

Due to the inherent randomness in the agent’s training process, determining the optimal fixed values for $\beta$ and $\omega$ is challenging. Setting these values too high can lead to an overly pessimistic Q-function, which suppresses the generalization of the Q-function and hinders policy improvement. Conversely, setting them too low makes it difficult to control the overestimation of the Q-function caused by extrapolation errors, significantly impacting policy learning. Therefore, $\beta$ and $\omega$ need to be learnable parameters, allowing for dynamic adjustment during the training process to achieve dynamic uncertainty estimation (DE). We adjust $\beta$ and $\omega$ by minimizing the expected absolute error between the estimated values and the target values. The loss functions for $\beta$ and $\omega$ are defined as follows:

$$
\begin{array} { r l } & { \underset { \beta } { \mathrm { m i n } } ~ \beta \times \mathbb { E } \left[ \left| T Q _ { \theta } ^ { m } ( s , a ) - Q _ { \theta } ^ { m } ( s , a ) \right| \right] , } \\ & { \underset { \omega } { \mathrm { m i n } } ~ \omega \times \mathbb { E } \left[ \left| \tilde { T } Q _ { \theta } ^ { m } ( s ^ { o o d } , a ^ { o o d } ) - Q _ { \theta } ^ { m } ( s ^ { o o d } , a ^ { o o d } ) \right| \right] . } \end{array}
$$

During training, we use Equations (8) and (9) to adjust $\beta$ and $\omega$ dynamically. This dynamic uncertainty estimation method regularizes OOD actions while avoiding excessive pessimism and enhancing generalization. The detailed implementation of the algorithm is provided in Appendix C.

Table 1: Comparison of normalized average scores and standard deviations of all algorithms across four different random seeds on the D4RL MuJoCo “-v2” datasets. The results for CSVE are sourced from the original paper, while results for other baseline algorithms are obtained by running the official code. The highest score for each dataset is highlighted in bold.   

<html><body><table><tr><td></td><td></td><td>CQL</td><td>UWAC</td><td>TD3-BC</td><td>MCQ</td><td>PBRL</td><td>CSVE</td><td>DURL</td></tr><tr><td rowspan="3">Random</td><td>HalfCheetah</td><td>17.3 ± 1.3</td><td>2.3±1.1</td><td>11.2 ± 2.1</td><td>28.5 ± 0.6</td><td>10.7± 3.1</td><td>26.7±2.0</td><td>19.6 ± 1.8</td></tr><tr><td>Hopper</td><td>7.4±0.7</td><td>2.5 ±0.4</td><td>8.0 ±1.4</td><td>31.8 ± 0.5</td><td>27.4 ± 4.3</td><td>27.0 ± 8.5</td><td>33.4 ± 2.7</td></tr><tr><td>Walker2d</td><td>5.1 ± 1.4</td><td>2.0 ± 0.4</td><td>1.6 ± 0.2</td><td>17.0 ± 3.0</td><td>8.1 ± 2.8</td><td>6.1±0.8</td><td>11.3 ± 1.4</td></tr><tr><td rowspan="3">Medium</td><td>HalfCheetah</td><td>44.2 ±1.4</td><td>43.5±0.6</td><td>48.3±1.4</td><td>64.3± 0.2</td><td>58.6 ± 1.5</td><td>48.6±0.0</td><td>68.8±1.2</td></tr><tr><td>Hopper</td><td>52.5 ± 12.5</td><td>51.4 ± 0.4</td><td>59.3 ± 2.5</td><td>74.4 ± 3.8</td><td>77.3 ± 21.4</td><td>99.4± 5.3</td><td>92.4 ± 4.2</td></tr><tr><td>Walker2d</td><td>72.8 ±12.4</td><td>73.6 ± 1.8</td><td>83.8 ± 1.3</td><td>90.3 ± 0.6</td><td>89.8 ±0.4</td><td>82.5 ± 1.5</td><td>98.5 ± 0.8</td></tr><tr><td rowspan="3">Medium ReplayHopper</td><td>HalfCheetah</td><td>44.5 ± 0.8</td><td>35.9 ± 2.8</td><td>44.3 ± 0.8</td><td>55.4± 0.7</td><td>43.1 ± 3.3</td><td>54.8 ± 0.8</td><td>66.4± 0.4</td></tr><tr><td></td><td>86.8±8.6</td><td>26.7 ± 1.8</td><td>62.6 ± 17.5</td><td>101.4± 0.8</td><td>100.6 ±0.8</td><td>91.7 ± 0.3</td><td>103.6 ± 0.7</td></tr><tr><td>Walker2d</td><td>81.8 ± 1.6</td><td>23.6 ± 5.8</td><td>87.1 ± 4.8</td><td>91.3 ± 4.8</td><td>83.7 ± 4.5</td><td>78.5 ± 1.8</td><td>98.2 ± 0.3</td></tr><tr><td rowspan="3">Medium ExpertHopper</td><td>HalfCheetah</td><td>75.6± 4.8</td><td>42.7±0.5</td><td>90.5±2.8</td><td>87.3±1.4</td><td>92.3±1.8</td><td>93.1±0.3</td><td>104.8 ± 0.8</td></tr><tr><td></td><td>105.6±10.8</td><td>43.9 ± 6.5</td><td>96.5 ± 7.8</td><td>111.4 ± 0.1</td><td>110.6 ± 0.8</td><td>95.2 ± 3.8</td><td>113.1 ± 0.6</td></tr><tr><td>Walker2d</td><td>106.3 ± 1.8</td><td>96.5 ± 9.4</td><td>110.1 ± 0.6</td><td>114.2 ± 0.6</td><td>110.1 ±0.8</td><td>109.0 ± 0.1</td><td>119.5 ± 0.8</td></tr><tr><td rowspan="3">Expert</td><td>HalfCheetah</td><td>96.3 ±1.2</td><td>91.3 ± 0.8</td><td>96.4 ±1.2</td><td>96.2 ±0.4</td><td>94.4 ± 1.8</td><td>93.8 ±0.1</td><td>103.4± 2.1</td></tr><tr><td>Hopper</td><td>96.5 ± 20.4</td><td>110.5 ±0.6</td><td>107.5 ± 6.3</td><td>111.2 ± 0.4</td><td>109.8 ±0.4</td><td>111.2±0.6 113.6 ±1.4</td><td></td></tr><tr><td>Walker2d</td><td>108.5 ± 0.3</td><td>108.4±0.6</td><td>110.3± 0.2</td><td>107.2 ± 1.3</td><td>108.3 ± 0.2</td><td>108.5 ± 0.0 121.6 ± 0.7</td><td></td></tr><tr><td>Average</td><td></td><td>66.7± 5.3</td><td>50.3 ± 2.2</td><td>67.8 ± 3.4</td><td>78.8 ±1.3</td><td>75.0 ± 3.2</td><td>75.1 ± 1.7</td><td>84.5 ± 1.3</td></tr></table></body></html>

<html><body><table><tr><td></td><td></td><td>CQL</td><td>UWAC</td><td>TD3-BC</td><td>MCQ</td><td>PBRL</td><td>CSVE</td><td>DURL</td></tr><tr><td rowspan="4">Cloned</td><td>Pen</td><td>39.2 ± 8.4</td><td>23.0 ± 6.5</td><td>0.0±0.0</td><td>49.4 ± 4.3</td><td>72.3 ± 9.5</td><td>55.2± 6.1</td><td>78.3±10.2</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Hammrer</td><td>2.4 ± 1.2</td><td>0.2±0.0</td><td>0.0±0.0</td><td>1.3 ±0.4</td><td>3.8 ± 0.4</td><td>0.5±0.2</td><td>1.5 ±0.5</td></tr><tr><td>Relocate</td><td>-0.1 ± 0.0</td><td>0.0 ±0.0</td><td>-0.1±0.0</td><td>1.4 ± 0.5</td><td>-0.1 ±0.0</td><td>-0.3 ± 0.0</td><td>1.8 ± 0.4</td></tr><tr><td rowspan="4">Expert</td><td>Pen</td><td>107.0±10.4</td><td>98.2 ± 8.5</td><td>0.3±0.0</td><td>80.4 ± 4.2</td><td>135.7 ± 3.4</td><td>142.9±10.6</td><td>148.5 ± 3.3</td></tr><tr><td>Hammer</td><td>86.7 ± 6.2</td><td>107.3± 25.1</td><td>0.0 ±0.0</td><td>50.3 ± 5.2</td><td>127.5 ± 0.2</td><td>126.5 ± 0.4</td><td>132.5 ± 5.6</td></tr><tr><td>Door</td><td>101.5±13.4</td><td>104.7 ± 0.4</td><td>0.0±0.0</td><td>40.5 ± 2.3</td><td>96.7 ± 12.2</td><td>104.3 ± 0.9</td><td>108.3 ± 7.2</td></tr><tr><td>Relocate</td><td>95.0 ± 2.4</td><td>105.5 ± 3.2</td><td>0.0 ±0.0</td><td>35.6 ± 4.2</td><td>84.5 ± 12.2</td><td>103.0 ± 1.1</td><td>90.3 ± 1.3</td></tr><tr><td>Average</td><td></td><td>54.0 ± 5.3</td><td>54.9 ± 5.5</td><td>0.0 ±0.0</td><td>32.4 ± 2.6</td><td>65.1 ± 5.3</td><td>66.7 ± 2.6</td><td>70.9 ± 3.9</td></tr></table></body></html>

Table 2: Comparison of normalized average scores and standard deviations of all algorithms across four different random seeds on the Adroit tasks. Results for CSVE are taken from the original paper, while results for other baseline algorithms are obtained by running the official code. The highest score for each dataset is highlighted in bold.

# Experiment

In this section, we first evaluate our algorithm’s performance improvements compared to the latest state-of-the-art (SOTA) algorithms. Next, we applied DT to widely used baselines to validate its effectiveness. Finally, we conduct ablation studies to thoroughly validate the effectiveness of each component of our algorithm. This rigorous validation process provides reassurance about the reliability of our research. Detailed experimental settings can be found in Appendix B.

# Performance on MuJoCo Datasets

In the D4RL (Fu et al. 2020) benchmark, we evaluated our algorithm in the Gym-MuJoCo domain and compared it with existing SOTA algorithms. The baseline algorithms we compared against include CQL (Kumar et al. 2020), UWAC (Wu et al. 2021), TD3-BC (Fujimoto and Gu 2021), MCQ (Lyu et al. 2022), PBRL (Bai et al. 2022), and CSVE (Chen et al. 2023). As shown in Table 1, we evaluated the performance of our algorithm and the baseline algorithms on three continuous control tasks: HalfCheetah, Hopper, and Walker2d. Each task has five dataset types: random, medium, mediumreplay, medium-expert, and expert, making a total of 15 datasets. Our algorithms were trained for 1 million gradient steps across four different random seeds, with policy evaluations performed every 1000 steps. The final results are the average of the last 10 evaluations. Among the baseline algorithms, MCQ and CSVE demonstrated the best performance. Compared to MCQ, our algorithm outperformed in 13 out of 15 tasks. MCQ showed superior performance only on the Walker2d-random and HalfCheetah-random datasets. When compared to CSVE, our algorithm outperformed in 14 out of 15 tasks, with CSVE showing better performance only on the Hopper-medium dataset. Overall, our algorithm outperformed the baseline algorithms in 12 out of 15 tasks. Notably, our algorithm showed significant improvements over the baseline algorithms on the expert and medium-expert datasets. These results have profound implications for the field of offline RL, as they validate the effectiveness of our algorithm and its potential to significantly advance the SOTA. Learning curves can be found in Appendix D.

Table 3: Comparison of the normalized average scores of CQL, $\mathrm { C Q L + D T }$ , PBRL, $\mathrm { P B R L + D T }$ , MCQ, and $\mathbf { M C Q + D T }$ across four different random seeds on the D4RL MuJoCo “-v2” datasets. All algorithms were trained for 1 million gradient steps. (·) represents the percentage improvement relative to the baseline.   

<html><body><table><tr><td></td><td></td><td>CQL</td><td>CQL+DT</td><td>PBRL</td><td>PBRL+DT</td><td>MCQ</td><td>MCQ+DT</td></tr><tr><td rowspan="3">Random</td><td>HalfCheetah</td><td>17.3</td><td>21.1 (+22.0%)</td><td>10.7</td><td>15.5 (+44.9%)</td><td>28.5</td><td>31.2 (+9.5%)</td></tr><tr><td>Hopper</td><td>7.4</td><td>8.3 (+12.2%)</td><td>27.4</td><td>29.4 (+7.3%)</td><td>31.8</td><td>30.5 (-4.1%)</td></tr><tr><td>Walker2d</td><td>5.1</td><td>10.2 (+100.0%)</td><td>8.1</td><td>11.4 (+40.7%)</td><td>17.0</td><td>21.4 (+25.9%)</td></tr><tr><td rowspan="3">Medium</td><td>HalfCheetah</td><td>44.2</td><td>51.6 (+16.7%)</td><td>58.6</td><td>61.3 (+4.6%)</td><td>64.3</td><td>66.3 (+3.1%)</td></tr><tr><td>Hopper</td><td>52.5</td><td>61.5 (+17.1%)</td><td>77.3</td><td>85.2 (+10.2%)</td><td>74.4</td><td>87.5 (+17.6%)</td></tr><tr><td>Walker2d</td><td>72.8</td><td>86.3 (+18.5%)</td><td>89.8</td><td>93.4 (+4.0%)</td><td>90.3</td><td>94.2 (+4.3%)</td></tr><tr><td rowspan="3">Medium Replay</td><td>HalfCheetah</td><td>44.5</td><td>63.8 (+43.3%)</td><td>43.1</td><td>51.2 (+18.8%)</td><td>55.4</td><td>59.4 (+7.2%)</td></tr><tr><td>Hopper</td><td>86.8</td><td>102.4 (+18.0%)</td><td>100.6</td><td>101.3 (+0.7%)</td><td>101.4</td><td>102.3 (+0.9%)</td></tr><tr><td>Walker2d</td><td>81.8</td><td>92.3 (+12.8%)</td><td>83.7</td><td>89.6 (+7.0%)</td><td>87.1</td><td>95.3 (+9.4%)</td></tr><tr><td rowspan="3">Medium Expert</td><td>HalfCheetah</td><td>75.6</td><td>93.4 (+23.5%)</td><td>92.3</td><td>98.4 (+6.6%)</td><td>87.3</td><td>99.7 (+14.2%)</td></tr><tr><td>Hopper</td><td>105.6</td><td>112.6 (+6.6%)</td><td>110.6</td><td>112.3 (+1.5%)</td><td>111.4</td><td>113.7 (+2.1%)</td></tr><tr><td>Walker2d</td><td>106.3</td><td>111.8 (+5.2%)</td><td>110.1</td><td>114.5 (+4.0%)</td><td>114.2</td><td>116.5 (+2.0%)</td></tr><tr><td rowspan="3">Expert</td><td>HalfCheetah</td><td>96.3</td><td>95.5 (-0.8%)</td><td>94.4</td><td>97.1 (+2.9%)</td><td>96.2</td><td>99.5 (+3.4%)</td></tr><tr><td>Hopper</td><td>96.5</td><td>113.7 (+17.8%)</td><td>109.8</td><td>112.2 (+2.2%)</td><td>111.2</td><td>111.7 (+0.4%)</td></tr><tr><td>Walker2d</td><td>108.5</td><td>114.3 (+5.3%)</td><td>108.3</td><td>114.7 (+5.9%)</td><td>107.2</td><td>115.7 (+7.9%)</td></tr><tr><td>Average</td><td></td><td>66.7</td><td>75.9 (+13.8%)</td><td>75.0</td><td>79.2 (+5.6%)</td><td>78.8</td><td>83.0 (+5.3%)</td></tr></table></body></html>

![](images/23924d1677cfda945176d962722b7a98d1850b1f9db0615c3d92dc4f82cf4b37.jpg)  
Figure 2: Q-value Absolute Bias of DURL, PBRL, and MCQ, with shaded areas representing standard deviation.

# Performance on Adroit and Maze2d Datasets

Adroit tasks. We tested our algorithm on the more challenging Adroit tasks, which consist of four distinct tasks: Pen, Hammer, Door, and Relocate. We conducted experiments using two types of datasets: cloned and expert. Our algorithms underwent training for 1 million gradient steps with four different random seeds, and the policy was evaluated every 1,000 steps. The final results were calculated as the average of the last 10 evaluations and are summarized in Table 2. Out of the eight tasks, our algorithm outperformed the baseline algorithms in six tasks. Notably, our algorithm showed a significant performance improvement over the baseline on the cloned datasets. Moreover, our algorithm generally performed better than the baseline on the expert datasets. Overall, these experimental results validate the effectiveness of our proposed algorithm.

Maze2d. We assessed our algorithm and the baseline algorithms on four datasets: maze2d-umaze, maze2d-umazedense, maze2d-medium, and maze2d-medium-dense. The experimental results can be found in Appendix D. We observed that QT (Hu et al. 2024) is the best-performing baseline algorithm, and our method significantly outperforms all baseline algorithms on these datasets.

# Q-value Absolute Bias

To clearly demonstrate the effectiveness of our method in avoiding excessive pessimism, we compare Q-function values. Q-function values can reflects whether the Q-function suffers from excessive pessimism. We trained the SAC algorithm for 1 million gradient steps in the Hopper-v2 environment and randomly selected 100 test samples from the online data. Then, we trained PBRL, MCQ, and DURL on the Hopper-medium-replay-v2 dataset, and SAC in the Hopper-v2 environment, with all algorithms being trained for 1 million gradient steps. During training, we recorded Q-function values for the test data every 1,000 steps. We use SAC’s estimation on the test data as the true Q-values. We calculated the absolute Q-value bias of PBRL, MCQ, and DURL relative to SAC. The results shown in Figure 2, indicate that DURL’s Q-function values are closer to SAC’s Qvalues. DURL achieved more accurate Q-values compared to the baseline algorithms. The experimental results validate DURL’s effectiveness in avoiding excessive pessimism and enhancing Q-function generalization.

100 100+ 100 WWW\~ MW 80 80 80 60 60 60 DU-N+DT+DE(DURL) 40 40 β=0.1 40 N=50 DU-N+DT 20 20 β-0.01 20 N=100 DU-N (=6 β=0.0001 0 N=1100 0.0 200 400 600 8001000 0 0.0 200 400 600 800 1000 8.0 200 400 600 800 1000 0.0 200 400 600 800 1000 Gradient Steps (thousands) Gradient Steps (thousands) Gradient Steps (thousands) Gradient Steps (thousands) (a) Effect of component (b) Effect of ω (c) Effect of $\beta$ (d) Effect of N

# Validation of DT Effectiveness

To further validate the effectiveness of the DT, we applied this method to widely used baselines like CQL, PBRL, and MCQ. We tested these algorithms on 15 datasets in the GymMuJoCo domain and 8 datasets from Adroit tasks. Each algorithm was trained for 1 million gradient steps with four different random seeds. The final results are the average of the last 10 evaluations. Results in the Gym-MuJoCo domain are shown in Table 3, and the results for Adroit tasks are provided in Appendix D. We observed that applying DT improved the performance of CQL, PBRL, and MCQ. The experimental results suggest that DT effectively regularizes OOD actions and avoids causing harmful effects on in-distribution data.

# Ablation Study

To further validate the effectiveness of the components of our proposed algorithm, we conducted ablation studies on the Walker2d-medium-v2 dataset. We refer to the initial version of DURL without the DE and DT components as DU-N. Since DE is not utilized for DU-N, we fixed $\beta$ in Equation (4) at 0.1 and $\omega$ in Equation (5) at 1.0. We then meticulously compared the performance of DU-N with DU- $\mathbf { \nabla \cdot N + D E }$ , DU$\mathrm { \Delta N + D T } ,$ and DU- $\mathrm { \Delta N + D T + D E }$ (DURL) variants. The learning curves of these methods are shown in Figure 3(a). The figure illustrates that DU- $- \mathrm { N + D T + D E }$ (DURL) achieves the best performance. DU- $\mathbf { N } { + } \mathbf { D } \mathbf { T }$ and DU- $\mathbf { \nabla \cdot N + D E }$ show significant improvements over DU-N, with DU- $\mathrm { \Delta N + D E }$ outperforming DU- $\mathbf { \nabla } \cdot \mathbf { N } + \mathbf { D } \mathbf { T }$ , indicating that DE contributes more than DT. These results demonstrate the performance comparison among different algorithm variants and the crucial role of DE and DT in our algorithm.

Learnable parameter $\omega$ . To evaluate the impact of the initial value of the coefficient $\omega$ on algorithm performance, We conducted experiments on the Walker2d-medium-v2 dataset using different initial values of $\omega$ . The results are illustrated in Figure 3(b). We observed that the performance was poorest with $\omega = 3$ and best with $\omega = 5$ . These findings suggest that both excessively large and excessively small initial values of $\omega$ adversely affect the algorithm’s performance. An overly large $\omega$ can impair the generalization capability of the Q-function, while an excessively small $\omega$ fails to effectively regularize the OOD actions.

Learnable parameter $\beta$ . We tested $\beta$ on the Walker2dmedium-v2 dataset with values $\{ 0 . 1 , 0 . 0 1 , 0 . 0 0 1 , 0 . 0 0 0 1 \}$ , as shown in Figure 3(c). The experiments demonstrate that our algorithm exhibits strong robustness with respect to $\beta$ within the range [0.0001, 0.1].

Density Truncation parameter $N$ . We analyzed the sensitivity of our algorithm to the hyperparameter $N$ in density truncation. We tested various values of $N$ , specifically $\{ 5 0 , 1 0 0 , 6 0 0 , 1 1 0 0 \}$ , on the Walker2d-medium-v2 dataset. The results are illustrated in Figure 3(d). The performance is optimal when $N = 1 0 0$ . In contrast, values of $N = 5 0$ , $N \ : = \ : 6 0 0$ , and $N = 1 1 0 0$ all lead to varying degrees of performance degradation compared to $N = 1 0 0$ . The experimental results indicate that a smaller $N$ fails to filter out indistribution data, leading to worse performance. Conversely, a larger $N$ does not effectively regularize OOD actions, adversely affecting performance. Therefore, appropriate density truncation is crucial for algorithm performance.

# Conclusion

This paper presents a dynamic uncertainty estimation method for model-free offline RL. By integrating dynamic uncertainty estimation and DT, Our method can accurately distinguish OOD actions and dynamically adjust the pessimism of the Q-function, effectively mitigating the overly pessimistic Q-function estimation, thereby improving the generalization of the Q-function. Experimental results demonstrate that our algorithm surpasses existing methods. We validated the effectiveness of each algorithm component and assessed the impact of different hyperparameters on performance. Future work will explore more effective OOD data partitioning mechanisms and dynamic uncertainty estimation methods in model-based offline RL.

# Acknowledgments

This work was supported by the National Key Research and Development Program of China (No. 2020AAA0106100), the National Natural Science Foundation of China (Nos. 62276160, 62476228), the Natural Science Foundation of Shanxi Province, China (Nos. 202203021211294, 202203021211291), and the Sichuan Science and Technology Program (No. 2024ZYD0180).