# FedPIA – Permuting and Integrating Adapters Leveraging Wasserstein Barycenters for Finetuning Foundation Models in Multi-Modal Federated Learning

Pramit Saha1, Divyanshu Mishra1, Felix Wagner1, Konstantinos Kamnitsas1, J. Alison Noble1

1 Department of Engineering Science, University of Oxford pramit.saha, divyanshu.mishra, felix.wagner, konstantinos.kamnitsas, alison.noble @eng.ox.ac.uk

# Abstract

Large Vision-Language Models (VLMs), possessing millions or billions of parameters, typically require large text and image datasets for effective fine-tuning. However, collecting data from various sites, especially in healthcare, is challenging due to strict privacy regulations. An alternative is to finetune these foundation models on end-user devices, such as in medical clinics and hospitals, without sending data to a server. These local clients typically have limited computing power and small datasets, which are not enough for fully fine-tuning large VLMs on their own. A naive solution to these scenarios is to leverage parameter-efficient fine-tuning (PEFT) strategies such as adapters and apply federated learning (FL) algorithms to combine the learned adapter weights, thereby respecting the resource limitations and data privacy of the clients. However, this approach does not fully leverage the knowledge from multiple adapters trained on diverse data distributions and for diverse tasks. The adapters are adversely impacted by data heterogeneity and task heterogeneity across clients resulting in sub-optimal convergence. To this end, we propose a novel framework called FedPIA that improves upon the naive combinations of FL and PEFT by introducing Permutation and Integration of the local Adapters in the server and global Adapters in the clients exploiting Wasserstein barycenters for improved blending of client-specific and client-agnostic knowledge. This layerwise permutation helps to bridge the gap in the parameter space of local and global adapters before integration. We conduct over 2000 clientlevel experiments utilizing 48 medical image datasets across five different medical vision-language FL task settings encompassing visual question answering as well as image and report-based multi-label disease detection. Our experiments involving diverse client settings, ten different modalities, and two VLM backbones demonstrate that FedPIA consistently outperforms the state-of-the-art PEFT-FL baselines.

# Introduction

Large Vision-Language Models (VLMs) have recently achieved significant progress in multi-modal learning (Li et al. 2021; Kim, Son, and $\mathrm { K i m } \ 2 0 2 1 \$ ). Central to their capabilities are vast numbers of parameters, often in the millions or billions, which encapsulate the learned representations necessary for multi-modal comprehension. The evolution of large VLMs has highlighted the importance of finetuning, essential for adapting these models to specific tasks with high accuracy. To enhance the generalization ability of foundation models, extensive fine-tuning with large amounts of diverse data from various sources is typically required. However, aggregating all training data for centralized finetuning poses significant challenges. For example, collecting data from clinical centers across multiple countries is often infeasible due to privacy regulations.

The need to protect data privacy has led to the exploration of alternative approaches such as Federated Learning (FL) (McMahan et al. 2017; Li et al. 2020; Acar et al. 2021; Karimireddy et al. 2020; Saha et al. 2024b; Wagner et al. 2024; Hernandez-Cruz et al. 2024; Wagner et al. 2023; Saha, Mishra, and Noble 2023). FL involves training models on local devices, like those in medical clinics and hospitals, without transferring sensitive data to a central server. This decentralized approach mitigates the risk of data breaches and ensures compliance with privacy regulations. However, fine-tuning models in local devices faces challenges if there are limited computational resources and small, localized datasets. These limitations hinder the independent fine-tuning of large VLMs, which require extensive parameters and diverse datasets to capture the complexities of real-world language and visual data. Addressing these challenges requires solutions that balance data privacy with the computational and dataset limitations of local devices.

Parameter-Efficient Fine-Tuning (PEFT) has recently gained attention. It freezes the original backbone and finetuning a small subset or a newly introduced set of parameters. FL, combined with PEFT, emerges as a promising paradigm for collaborative model training while respecting data privacy and minimizing communication overhead.

Related works have primarily explored combinations of centralized PEFT algorithms and FedAvg. For example, some approaches focus on training and communicating adapters (Houlsby et al. 2019) or a small number of trainable input tokens (Guo, Guo, and Wang 2023; Guo et al. 2023). These investigations are mostly limited to single modality scenarios, addressing only visual or textual tasks. Besides, none of these studies tackle the issue of data heterogeneity or task heterogeneity that lead to model drifts during local client updates and result in an unstable and sub-optimal convergence of the server model (Li et al. 2020). Besides, such

CT Ultrasound Dermatocopy Fundus Histology Microscopy Optical CT X-Ray dnws 三 G ofte lethlim A: Ovary A:Chickenpox diabetic retinopathy histopathology beneath theretina Metacarpophalangeal

naive combination of FL with PEFT does not fully leverage the knowledge embedded in the multiple models trained on heterogeneous data distributions and diverse tasks. The recent work FedDAT (Chen et al. 2024) leverages a DualAdapter Teacher (DAT) module, consisting of two parallel adapters: a local adapter and a frozen global adapter. However, the local adapters are trained independently in different clients per round and involve individual complex information streams, thereby making them distant in the parameter space. This implies that mere addition of the diverse global and local adapters without proper alignment leads to sub-optimal performance and catastrophic forgetting, particularly in data- and task-heterogeneous FL. Besides, it requires the use of a separate global adapter teacher along with alternate training of the DAT module via mutual knowledge distillation that adds to its training complexity.

To address this issue, we present a novel framework called FedPIA1 (Federated Learning via Permuting and Integrating Adapters) to improve information sharing between the client adapters in the server as well as between the local and global adapters in the clients. This is achieved by: (a) bringing the client adapters closer to each other in the server and (b) bringing the global adapters closer to the client-specific adapters in the clients in the parameter space. Concretely, in the server, in each layer, we permute the diverse, client adapter neurons to match the initialized global adapter neurons (obtained via FedAvg (McMahan et al. 2017)), before combining them, utilizing the theory of Wasserstein Barycenters. Furthermore, in order to better integrate client-specific and client-agnostic knowledge in the clients, we permute the weights of the global adapter in each client and bring it closer to client-specific adapter in the weight space before combining them. This two-fold approach bridges the gap between diverse adapters which are originally optimized on different distributions, using different input features from different modalities and leads to a stable convergence as seen in Fig 4. In order to showcase the effectiveness of FedPIA, we carry out over 2000 clientlevel experiments under five Vision-language FL task settings using 48 medical image datasets that involves data heterogeneity, modality heterogeneity, and task heterogeneity. The results demonstrate that FedPIA shows consistent and robust performance irrespective of heterogeneity conditions, outperforming the baselines for all task scenarios.

# Background and Related Work

Federated Learning (FL) FL enables various clients to collaboratively train models in a decentralized manner without sharing local data. The classical FL framework, FedAvg (McMahan et al. 2017), offers a practical method for model aggregation. However, its performance is adversely impacted by client non-IID data distributions. Consequently, several modifications have emerged to address data heterogeneity (Li et al. 2020; Karimireddy et al. 2020; Acar et al. 2021). FedProx (Li et al. 2020) adds a proximal term to the client loss function thereby enforcing constraints on local updates. Another work called Scaffold (Karimireddy et al. 2020) employs control variates to enhance local updates, while FedDyn (Acar et al. 2021) dynamically regulates the client loss function to align the local and the global objectives. Moon (Li, He, and Song 2021) regularizes local training via contrastive learning. All these works assume unimodal data in all clients.

Parameter-efficient Fine-tuning (PEFT) PEFT techniques can be categorized into three families: adaptive methods, selective methods, and prompt tuning. Adaptive methods are additive PEFT techniques that integrate adapters or small neural network blocks into the Transformer layers (Hu et al. 2022; Rebuffi, Bilen, and Vedaldi 2018; Li, Liu, and Bilen 2022; Lian et al. 2022). Selective PEFT fine-tunes a subset of the existing parameters to enhance model performance on downstream tasks (Ben Zaken, Goldberg, and Ravfogel 2022; Frankle, Schwab, and Morcos 2021; Touvron et al. 2022). Prompt tuning methods modify the original input, whether an embedding or the actual instance, with some prompts consisting of additional trainable parameters or perturbations (Lester, Al-Rfou, and Constant 2021; Jia et al. 2022; Li and Liang 2021).

PEFT and Federated Learning The application of PEFT in multimodal FL remains relatively unexplored. Previous research has primarily adapted PEFT for FL in a straightforward manner, particularly focusing on uni-modal tasks, i.e., vision or NLP. (Chen et al. 2022) and (Sun et al. 2022) evaluate existing PEFT baselines combined with FL in vision tasks. (Guo, Guo, and Wang 2023), (Guo et al. 2023), (Li et al. 2023), and (Lu et al. 2023) fine-tune CLIP by communicating a small amount of learnable personalized prompts. (Su et al. 2022) addresses the issue of heterogeneous client images by adding adapters. (Yang et al. 2024) explores the possibility of fine-tuning diffusion models via

A Anchor Adapter DU 1 Q AnchorAdapter o! 0 8 ? wn- 0 . □ 1   
然 8 沃 Adapter1 compatrixion Permutation 星 Q0·· 然 福 . Layer 1 Permutation . Adaptern . ： ：   
C1 : Permutable . ： ： 000。 A Layer3Permutation Integration   
C2 仁 Integration \~ H Vof w permutation Intermtion w/o Add&Norm ！ D FeedForward Non-enhancing Tumor A:Brain Edema,Brain ： w1 4 w2 Permuted Add&Norm Vision-language Adapter Integration   
Cn Multi-Head Attention Model -Q:What disease 。 is shown on the ..leftofbrain?... B (c) Loss contour for anchor and (a) Overview of Proposed Method (b) Mechanism of PIA permutable adapters

FL. (Yu, Mun˜oz, and Jannesari 2023) optimize adapters for few-shot fine-tuning of LLMs. (Zhang et al. 2024) builds distributed instruction tuning datasets and fine-tunes a LLM via Low-Rank Adaptation (Hu et al. 2022). (Zhuang, Chen, and Lyu 2023) analyzes the challenges of fine-tuning LLMs in FL.

(Yu et al. 2023) is the first work to consider multimodal client datasets. However, it processes visual and language data using separate networks, without utilizing a unified VLM. A recent work (Nguyen, Munoz, and Jannesari 2024) proposes FLORA for fine-tuning VLMs using LoRA adapters in FL. (Zeng, Yue, and Wang 2024) introduces a multimodal prototyping mechanism for fine-tuning VLMs. FedDAT (Chen et al. 2024) considers data heterogeneity in multimodal FL by utilizing a Dual-Adapter Teacher (DAT) and employing Mutual Knowledge Distillation (MKD) between the local and global adapters. It is the only work on federated PEFT of VLMs for VQA. However, as discussed earlier, FedDAT does not fully utilize the knowledge embedded in multiple local adapters trained on heterogeneous data distributions and diverse tasks. It needs a separate global adapter for MKD thereby doubling the total number of trainable parameters. We tackle the heterogeneity issue without adding any training overhead.

# Methodology: FedPIA

Problem definition: We tackle a heterogeneous FL problem involving $K$ clients. Each client $k$ possesses a private multimodal dataset $D _ { k }$ , which includes both visual $( v _ { k } )$ and textual $( t _ { k } )$ data. Specifically, each local dataset $D _ { k }$ can be decomposed into $N _ { k }$ image-text-output triplets $\left\{ ( v _ { k _ { i } } , t _ { k _ { i } } , a _ { k _ { i } } ) | i \in \{ 1 , \dots , N _ { k } \} \right\}$ . We assume that the marginal distribution of $\boldsymbol { v } _ { k _ { i } }$ , $\displaystyle t _ { k _ { i } }$ , and $a _ { k _ { i } }$ varies across clients, indicating data heterogeneity in the visual, textual and task domains. We define the answer or label pool $A _ { k } =$ $\{ a _ { k _ { 1 } } , \dotsc , a _ { k _ { C _ { k } } } \}$ with $C _ { k }$ ground-truth answers or labels for client $k$ , and frame our task as a $C _ { k }$ -way classification problem. Note that the answer pool and the total number of classes differ from client to client, thereby inducing heterogeneity in the $\mathrm { F L }$ model. Let $f$ be a foundation model parameterized by $\theta$ . Starting from the pre-trained weights $\theta _ { 0 }$ , the goal is to optimize client-specific losses $L _ { k }$ by gradient descent. Due to client-specific data and resource constraints, full fine-tuning is not feasible in FL. Our goal is to collaboratively fine-tune the foundation model $f _ { \theta }$ in a parameterefficient manner within a predefined communication budget. For this, following additive PEFT, we introduce new parameters $\phi$ for fine-tuning while keeping the original model frozen, resulting in the full parameter set $\theta ^ { \prime } = \{ \bar { \theta } , \phi \}$ .

Overall idea: The client adapters communicated to the server are distant in the weight space due to heterogeneity in client task space and data distribution, as indicated by the convergence analysis in Fig. 4 (Detailed analysis in Suppl. $\ S \mathbf { C } _ { , }$ ). Owing to the permutation invariance property, these adapters lack one-to-one correspondence, which is crucial for effective information fusion. Therefore, we adopt the theory of Wasserstein Barycenters (Singh and Jaggi 2020; Akash, Li, and Trillos 2022) to synchronise and combine multiple client adapters in layerwise fashion and using weight space as their underlying distribution in the server. The Wasserstein Barycenter relates to the concept of averaging in the Wasserstein space by minimizing the Earth Mover’s distance between the barycenter and given distributions. This helps in bringing the adapters closer in the parameter space, prior to aggregation, as seen in Fig. 2(c). The aggregated global adapter is communicated back to each client. However, it possesses client-agnostic knowledge and is again distant from the local adapters in the weight space. Using the global adapter in this form leads to slower, unstable convergence, as seen in Fig. 4(a). Therefore, we permute the global model in each client to match the local adapter before integration using similar technique. This permuted global adapter is consequently frozen and combined with the client-specific adapter, thereby integrating client-specific and shared knowledge as seen in Fig. 2(a). At the end of each round, this integrated adapter (also called client adapter) is uploaded to the server (see Algorithm 1 in Suppl. $\ S \mathbf { A }$ ).

Server-level PIA: We introduce a two-step procedure in the server: First, we initialize the global adapter using standard FedAvg (McMahan et al. 2017) of the client adapters. Next, we permute each client adapter to match the initialized global adapter by computing the permutation matrix as observed in Fig. 2. For this, we define probability measure over neurons in the lth layer for the kth client adapter as µ(kℓ) $\mu _ { k } ^ { ( \ell ) } =$ $( \alpha _ { k } ^ { ( \ell ) } , X _ { k } [ \ell ] )$ and that for the estimated global adapter as $\nu ^ { ( \ell ) } = ( \beta ^ { ( \ell ) } , X _ { \mathcal { G } } [ \ell ] )$ , where $X _ { k }$ and $X _ { \mathcal { G } }$ are the respective measure supports. The weight $\alpha = ( \alpha _ { 1 } , \ldots , \alpha _ { n } )$ lies in the probability simplex $\Sigma _ { n } : = \{ a \in \mathbb { R } _ { + } ^ { n } \ | \ \sum _ { i = 1 } ^ { n } a _ { i } = 1 \}$ (and similarly for $\beta$ ). We consider that the support of each adapter neuron in the server is given by the weights of the incoming edges, which are stacked in a vector. Accordingly, an adapter neuron can be represented by the corresponding row in the weight matrix. Therefore, the support of their measures is given by $X _ { k } [ \ell ] = W _ { k } ^ { ( \ell , \ell - 1 ) }$ and $X _ { \mathcal { G } } [ \ell ] = W _ { \mathcal { G } } ^ { ( \ell , \ell - 1 ) }$ .

Let $C _ { k } ^ { i j , ( l ) }$ denote the ground cost of permuting the $i ^ { t h }$ adapter neuron of ${ { l } ^ { t h } }$ layer in the $k ^ { t h }$ client to the $j ^ { t h }$ adapter neuron of same layer in the server. It is equivalent to moving the measure supports from $X _ { k , ( i ) } [ l ]$ to $X _ { \mathcal { G } , ( j ) } [ l ]$ . We compute this cost as the Euclidean distance between the weight vector of the local and initialized global adapter, i.e., $C _ { k } ^ { ( l ) } = \| \mathbf { X } _ { k } ^ { i } [ \ell ] - X _ { \mathcal { G } } ^ { j } [ \ell ] \| _ { 2 } , \quad \forall i \in [ n _ { k } ^ { ( \ell ) } ] , j \in [ m ^ { ( \ell ) } ]$ where $n _ { k } ^ { \ell }$ and $m ^ { \ell }$ are the number of neurons in the $\ell ^ { t h }$ layer of the client adapter and the global adapter respectively. We initialize the probability mass values of each layer from a uniform distribution. So, α(kℓ) $\begin{array} { r } { \dot { \alpha } _ { k } ^ { ( \ell ) } = \frac { 1 } { n ^ { ( \ell ) } } , \beta ^ { ( \ell ) } = \frac { 1 } { m ^ { ( \ell ) } } } \end{array}$

For aligning the incoming weights $W _ { k } ^ { ( \ell , \ell - 1 ) }$ for the ${ { l } ^ { t h } }$ layer in $k ^ { t h }$ client adapter, we first normalize the previous layer permutation matrix $P _ { k } ^ { ( l - 1 ) }$ with the inverse of corresponding column marginals of the server adapter $\beta ^ { ( l - 1 ) }$ as Pk(ℓ−1)diag(1/β(ℓ−1)) and post-multiply with the current layer weights of the client adapter Wk(ℓ,ℓ−1). Next, based on the cost metric $C _ { k } ^ { ( l ) }$ , we compute the permutation matrix $P _ { k } ^ { \ell }$ between measures $\mu _ { k } ^ { \ell } , \nu ^ { \ell }$ for the current layer (ℓ) by minimizing the Wasserstein distance W(ℓ) µ ${ \mathcal W } ^ { ( \ell ) } \left( \mu _ { k } ^ { ( \ell ) } , \nu ^ { ( \ell ) } , C _ { k } ^ { ( l ) } \right)$ . This permutation matrix is used to align the the client adapter and global adapter weights as: $\begin{array} { r l } { \widetilde { \mathbf { W } } _ { k } ^ { ( \ell , \ell - 1 ) } } & { { } = } \end{array}$ diag  1/β(ℓ) Pk(ℓ)⊤Wk(ℓ,ℓ−1)Pk(ℓ−1)diag(1/β(ℓf−1)).

adapters are then integrated dynamically to form the global adapter as: W(ℓ,ℓ−1)

$\begin{array} { r } { \frac { 1 } { K } \sum _ { k = 1 } ^ { K } \widetilde { \mathbf { W } } _ { k } ^ { ( \ell , \ell - 1 ) } \exp \left( - \gamma \Vert \widetilde { \mathbf { W } } _ { k } - \mathbf { W } _ { \mathcal { G } } \Vert _ { 2 } \right) } \end{array}$ where $\gamma$ is a hyperfparameter. Note thaft our method is orthogonal to different FL aggregation schemes and can be used in conjunction with those. For simplicity, we use FedAvg as the chosen aggregation scheme in this work.

Client-level PIA: In the clients, we combine the global adapter and client-specific adapter (with parameters from the last communication round) for integrating client-specific and shared information streams. For this, we first align the global adapter $\widetilde { \mathbf { W } } _ { \mathcal { G } }$ to the client-specific adapter $\mathbf { W } _ { k }$ using the Wassersteinfdistance following the aforementioned procedure. The only difference in the permutation computation here is that we use adapter activations rather than weights for computing the cost metric $C _ { k } ^ { ( \ell ) }$ . For this, we compute the mean neuron activation $( \psi )$ for all the neurons of local and global adapters over a randomly selected batch of $m$ samples $\bar { B } = \{ x \} _ { i = 1 } ^ { \bar { m } }$ and use it as the support of measures. Therefore, the support is now denoted as $X _ { k } [ \ell ] = \psi ( M _ { k } ^ { \ell } ( B ) )$ and $X _ { \mathcal { G } } [ \ell ] = \psi ( M _ { \mathcal { G } } ^ { \ell } ( B ) )$ for the local and global adapter respectively, where $\bar { M }$ denotes the adapter model. In other words, neurons across local and global adapters in each client would be considered similar if they yield similar activations for a given instance. Leveraging the activations instead of weight matrix particularly helps the global adapter to adapt to the client-specific data distribution better (as indicated in Table 5) since the activations (unlike weights) are directly dependent on the input data.

# Experiments and Results

# Tasks and Datasets

We assess the performance of our proposed method with two prominent Vision Language foundation models, viz., ViLT and ALBEF, and for three FL task settings: (a) Visual Question Answering, (b) Image and Text-based Disease Classification, (c) Heterogeneous tasks combining both (a) and (b). In order to ensure the real-world applicability of FedPIA, we conduct experiments on multiple well-known and challenging medical datasets as discussed below.

(a) Visual Question Answering: We consider two scenarios with data of varying sizes, class counts, and complexity:

(a) Task1 (MedVQA) (b) Task 3 (Classification) Adapter(P) Adapter (C) -Adapter(H) Adapter(P) Adapter (C) Adapter (H) FedDAT(P) FedDAT (C) FedDAT (P) FedDAT (C) Full Fine-tuning Full Fine-tuning Number of training steps

Table 1: Performance comparison of FedPIA with other methods on Task 1 (in terms of accuracy)   

<html><body><table><tr><td>Fine-tuning</td><td></td><td>Slake</td><td>OverallOpen</td><td></td><td>VQA-Med 2019</td><td></td><td>VQA-Med 2020</td><td></td><td></td><td>VQA-Med 2021</td><td></td><td></td><td>VQA-RAD</td><td></td><td></td><td>Mean Score</td><td></td></tr><tr><td></td><td>Open</td><td>Closed</td><td></td><td></td><td></td><td>ClosedOverallOpen</td><td></td><td>ClosedOverall</td><td>Open</td><td>Closed</td><td>Overall</td><td>Open</td><td>Closed</td><td>Overall</td><td>Open</td><td>Closed</td><td>Overall</td></tr><tr><td colspan="10"></td><td colspan="10">Backbone architecture:ViLT</td></tr><tr><td>Full fine-tuning</td><td>74.73</td><td>74.48</td><td>74.66</td><td>60.55</td><td>59.38</td><td>60.43</td><td>0.70</td><td>52.94</td><td>15.54</td><td>21.00</td><td></td><td>21.00</td><td>42.47</td><td>64.26</td><td>55.90</td><td>39.89</td><td>62.77</td><td>45.51</td></tr><tr><td>Classifier only (LB)</td><td>66.82</td><td>60.82</td><td>65.60</td><td>54.13</td><td>50.00</td><td>53.72</td><td>0.00</td><td>37.25</td><td>10.88</td><td>18.50</td><td></td><td>13.00</td><td>34.95</td><td>57.41</td><td>47.88</td><td>37.06</td><td>52.10</td><td>38.47</td></tr><tr><td></td><td>72.87</td><td></td><td>72.10</td><td></td><td></td><td></td><td></td><td>52.4</td><td>15.54</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td> HodusbyTusiopter</td><td></td><td>68.74</td><td></td><td>57.57</td><td>5469</td><td>57.28</td><td>870</td><td></td><td></td><td>19.50</td><td></td><td>19.50</td><td>33.3</td><td>58.55</td><td>49.08</td><td>36.79</td><td>58.73</td><td>42.6</td></tr><tr><td>Parallel Adapter</td><td>72.71</td><td>64.18</td><td>70.50</td><td>58.26</td><td>57.81</td><td>58.21</td><td>0.00</td><td>49.02</td><td>13.99</td><td>24.00</td><td></td><td>24.00</td><td>32.80</td><td>54.75</td><td>46.55</td><td>37.55</td><td>56.44</td><td>42.65</td></tr><tr><td>Compacter</td><td>71.68</td><td>65.39</td><td>70.31</td><td>58.26</td><td>57.81</td><td>58.21</td><td>0.00</td><td>50.98</td><td>14.51</td><td>22.00</td><td></td><td>22.00</td><td>39.95</td><td>57.03</td><td>48.55</td><td>38.38</td><td>57.80</td><td>42.72</td></tr><tr><td>LayerNorm</td><td>72.40</td><td>64.90</td><td>70.59</td><td>56.65</td><td>48.44</td><td>55.83</td><td>0.70</td><td>33.33</td><td>9.84</td><td>17.00</td><td></td><td>17.00</td><td>32.26</td><td>57.03</td><td>47.44</td><td>35.80</td><td>50.93</td><td>40.14</td></tr><tr><td>LoRA</td><td>60.93</td><td>56.97</td><td>59.94</td><td>53.90</td><td>56.25</td><td>55.80</td><td>0.00</td><td>15.69</td><td>4.66</td><td>15.00</td><td></td><td>15.00</td><td>23.12</td><td>56.65</td><td>43.88</td><td>30.59</td><td>46.39</td><td>35.86</td></tr><tr><td>PromptFL(k=5)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>57.47</td><td>48.55</td><td></td><td>52.10</td><td></td></tr><tr><td></td><td>733</td><td>66.83</td><td>71.72</td><td>57.89</td><td>46.88</td><td>56.71</td><td>070</td><td>37.25</td><td>4.88</td><td>18.50</td><td></td><td>18.00</td><td>34.95</td><td></td><td></td><td>37.06</td><td></td><td>41.27</td></tr><tr><td>PromptFL (k=10)</td><td>71.32</td><td>67.55</td><td>70.31</td><td>56.88</td><td>48.88</td><td>56.08</td><td>0.00</td><td>11.76</td><td>3.63</td><td>19.00</td><td></td><td>19.00</td><td>25.27</td><td>58.18</td><td>45.21</td><td>34.49</td><td>46.59</td><td>38.85</td></tr><tr><td>PromptFL (k=20)</td><td>65.74</td><td>68.02</td><td>67.48</td><td>56.42</td><td>51.56</td><td>55.93</td><td>0.70</td><td>11.77</td><td>4.15</td><td>20.00</td><td></td><td>20.00</td><td>27.96</td><td>58.56</td><td>46.77</td><td>34.16</td><td>47.48</td><td>38.87</td></tr><tr><td>PromptFL (k=50)</td><td>71.01</td><td>66.83</td><td>70.22</td><td>57.34</td><td>57.81</td><td>57.39</td><td>0.70</td><td>7.84</td><td>3.11</td><td>19.00</td><td></td><td>19.00</td><td>25.27</td><td>55.54</td><td>43.88</td><td>34.66</td><td>47.01</td><td>38.72</td></tr><tr><td>FedDAT (AdapterFusion)</td><td>72.09</td><td>69.71</td><td>71.91</td><td>55.96</td><td>57.81</td><td>56.15</td><td>0.00</td><td>31.37</td><td>8.81</td><td>23.50</td><td></td><td>23.50</td><td>42.47</td><td>62.36</td><td>55.01</td><td>38.80</td><td>55.31</td><td>43.08</td></tr><tr><td>FedDAT (Houlsby)</td><td>73.02</td><td>73.07</td><td>73.03</td><td>53.21</td><td>51.56</td><td>53.05</td><td>0.00</td><td>45.09</td><td>12.44</td><td>23.00</td><td>、</td><td>23.00</td><td>38.71</td><td>56.65</td><td>50.33</td><td>37.59</td><td>56.59</td><td>42.54</td></tr><tr><td>FedDAT (Parallel)</td><td>72.87</td><td>70.91</td><td>72.14</td><td>56.65</td><td>50.00</td><td>55.99</td><td>0.00</td><td>29.41</td><td>7.77</td><td>25.00</td><td></td><td>25.00</td><td>39.79</td><td>58.56</td><td>51.89</td><td>38.86</td><td>52.22</td><td>42.56</td></tr><tr><td>FedDAT (Compacter)</td><td>73.02</td><td>74.04</td><td>73.28</td><td>56.57</td><td>59.38</td><td>56.85</td><td>0.00</td><td>41.18</td><td>11.92</td><td>24.50</td><td></td><td>24.50</td><td>38.17</td><td>52.09</td><td>47.66</td><td>38.45</td><td>56.67</td><td>42.86</td></tr><tr><td>FedPIA (Houlsby) (ours)</td><td>74.45</td><td>74.20 74.16</td><td>74.38</td><td>60.29</td><td>60.16</td><td>60.26</td><td>0.70</td><td>53.20</td><td>15.86</td><td>27.50</td><td></td><td>27.50</td><td>42.67</td><td>64.05</td><td>55.37</td><td>41.12</td><td>62.90</td><td>46.66</td></tr><tr><td>FedPIA (Parallel) (ours) FedPIA (Compacter) (ours)</td><td>74.04 74.41</td><td>74.18</td><td>74.10 74.35</td><td>59.90 60.35</td><td>59.82 60.86</td><td>59.87 60.42</td><td>0.70 0.70</td><td>52.48 52.88</td><td>15.36 15.49</td><td>27.00</td><td></td><td>27.00</td><td>42.01</td><td>64.02</td><td>55.12</td><td>40.73</td><td>62.62</td><td>46.29</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Backbone architecture: ALBEF</td><td>27.00</td><td></td><td>27.00</td><td>43.16</td><td>64.33</td><td>56.04</td><td>41.12</td><td>63.06</td><td>46.66</td></tr><tr><td>Full fine-tuning</td><td>77.89</td><td>77.28</td><td>77.45</td><td>67.65</td><td>63.20</td><td>67.25</td><td>0.70 0.00</td><td>50.08</td><td>15.06 9.85</td><td>22.00</td><td></td><td>22.00</td><td>39.35</td><td>62.22</td><td>52.86</td><td>41.52</td><td>63.20</td><td>46.92</td></tr><tr><td>Classfier only (LB) AdapterFusion</td><td>70.23</td><td>65.81 70.37</td><td>69.24 72.46</td><td>61.03 64.20</td><td>58.45 60.77</td></table></body></html>

(i) Task 1: Five-client setting with SLAKE (Liu et al. 2021), VQA-RAD (Lau et al. 2018), VQA-Med 2019 (Ben Abacha et al. 2019), VQA-Med 2020 (Abacha et al. 2020), and VQA-Med 2021 (Ben Abacha et al. 2021).

(ii) Task 2: Modality specific Eight-client setting leveraging (Hu et al. 2024) where Client 1 (CT) includes 3 CT datasets, Client 2 (US) includes Ultrasound images, Client 3 (OCT) includes 2 datasets, Client 4 (Fundus images) includes 8 fundus datasets, Client 5 (Microscopy) includes 5 datasets, Client 6 (Histopathology) includes 4 datasets, Client 7 (Dermatoscopy) includes 7 different skin datasets, Client 8 (X-Ray) includes 11 X-Ray datasets. See Suppl. $\ S \mathbf { B }$ for details.

(b) Image and text-based disease classification: Following (Saha et al. 2024a), we consider two heterogeneous FL settings for Chest X-Ray-based disease detection:

(i) Task 3: 4 client-scenario using Open-I dataset, (ii) Task 4: 10 client scenario with MIMIC dataset.

Following standard procedures, we use Dirichlet distributions with $\gamma = 0 . 5$ to simulate non-IID client data partitions.

(c) Heterogeneous tasks: We consider a taskheterogeneous setting for Task 5 combining 3 VQA clients, viz., SLAKE, VQA-RAD, VQA-Med 2019, and 2 disease-classification clients, viz., Open-I and MIMIC.

# Training and Implementation Details

To demonstrate the effectiveness of FedPIA across various VLM models, we adopt two types of VLM transformer architectures: (a) encoder-only backbone i.e., ViLT (Kim, Son, and $\ K \mathrm { i m } 2 0 2 1 \$ ), and (b) encoder-decoder backbone i.e., ALBEF (Li et al. 2021). We fix the initial learning rate $\eta =$ 0.0001 and batch size $B = 1 6$ . We use the AdamW optimizer and a learning rate scheduler with linear decay following (Chen et al. 2024). We also use a weight decay of 0.01 with a total of 30 communication rounds for federated finetuning including $1 0 \%$ warmup rounds (Chen et al. 2024). Each client has task-specific linear classification heads.

Baselines: Our baselines are: 1) Full fine-tuning, 2) Local classifier fine-tuning, 3) AdapterFusion (Pfeiffer et al. 2020), 4) Houlsby Adapter (Houlsby et al. 2019), 5) Parallel Adapter (He et al. 2022), 6) Compacter (Karimi Mahabadi, Henderson, and Ruder 2021), 7) LayerNorm (Basu et al. 2023), 8) LoRA (Hu et al. 2022), 9) Bias tuning (Cai et al. 2020), 10) PromptFL (Guo et al. 2023), and 11-14) FedDAT (Chen et al. 2024) with 4 adapters variants. Note that ‘Local classifier’ refers to client-specific training of local classifier heads without any federated learning and hence it is considered as the Lower Bound (LB). See Suppl. $\ S \mathbf { B }$ .

# Results on Visual Question Answering (Tasks 1, 2)

Tables 1 and 2 show the performance comparison of our models and the baselines for Tasks 1 and 2 respectively. For Task 1, we show the accuracy of the models in answering open-ended and closed questions separately in each dataset except VQA-Med 2021 which does not possess any closed questions. We observe that FedPIA outperforms all naive

Table 2: Comparison of FedPIA with other methods on Task 2 with modality-specific clients (in terms of accuracy)   

<html><body><table><tr><td>Fine-tuning</td><td>C1 (CT)</td><td>C2 (US)</td><td>C3 (OCT)</td><td>C4 (Fundus)</td><td>C5 (Micro.)</td><td>C6 (Hist.)</td><td>C7 (Derma.)</td><td>C8 (XRay)</td><td>Overall</td></tr><tr><td>Full fine-tuning</td><td>94.95</td><td>87.63</td><td>93.55</td><td>81.84</td><td>92.65</td><td>93.00</td><td>76.12</td><td>92.20</td><td>88.99</td></tr><tr><td>Classifier only (LB)</td><td>80.30</td><td>75.17</td><td>73.76</td><td>69.99</td><td>86.32</td><td>87.96</td><td>68.11</td><td>84.96</td><td>78.32</td></tr><tr><td>AdapterFusion</td><td>83.33</td><td>80.54</td><td>80.97</td><td>71.59</td><td>90.74</td><td>92.12</td><td>71.33</td><td>88.61</td><td>82.40</td></tr><tr><td>Houlsby Adapter</td><td>82.58</td><td>80.49</td><td>83.44</td><td>68.86</td><td>89.85</td><td>92.34</td><td>74.18</td><td>87.92</td><td>82.46</td></tr><tr><td>Parallel Adapter</td><td>79.29</td><td>79.76</td><td>83.33</td><td>68.02</td><td>90.29</td><td>91.90</td><td>71.03</td><td>89.37</td><td>81.63</td></tr><tr><td>Compacter</td><td>83.33</td><td>80.08</td><td>80.86</td><td>73.47</td><td>93.24</td><td>91.25</td><td>71.63</td><td>86.89</td><td>82.59</td></tr><tr><td>LayerNorm</td><td>81.82</td><td>77.94</td><td>78.17</td><td>72.72</td><td>89.26</td><td>91.25</td><td>69.24</td><td>87.23</td><td>80.95</td></tr><tr><td>LoRA</td><td>58.84</td><td>61.53</td><td>62.04</td><td>67.73</td><td>73.82</td><td>89.72</td><td>59.73</td><td>80.54</td><td>69.24</td></tr><tr><td>Bias</td><td>82.32</td><td>77.63</td><td>79.35</td><td>71.97</td><td>89.71</td><td>91.47</td><td>70.21</td><td>86.89</td><td>81.19</td></tr><tr><td>PromptFL</td><td>79.80</td><td>71.35</td><td>78.39</td><td>71.59</td><td>89.26</td><td>93.00</td><td>70.88</td><td>85.44</td><td>79.96</td></tr><tr><td>FedDAT (AdapterFusion)</td><td>82.07</td><td>79.99</td><td>79.35</td><td>74.41</td><td>91.32</td><td>89.72</td><td>71.63</td><td>88.44</td><td>82.37</td></tr><tr><td>FedDAT (Houlsby)</td><td>79.80</td><td>79.22</td><td>76.67</td><td>75.07</td><td>90.29</td><td>90.59</td><td>72.90</td><td>89.86</td><td>81.55</td></tr><tr><td>FedDAT (Parallel)</td><td>82.83</td><td>81.26</td><td>80.54</td><td>74.60</td><td>89.41</td><td>92.12</td><td>73.35</td><td>88.79</td><td>82.61</td></tr><tr><td>FedDAT(Compacter)</td><td>80.56</td><td>80.44</td><td>81.61</td><td>73.38</td><td>90.15</td><td>90.37</td><td>72.23</td><td>89.37</td><td>82.01</td></tr><tr><td>FedPIA (Houlsby) (ours)</td><td>89.86</td><td>86.77</td><td>90.03</td><td>80.20</td><td>92.34</td><td>92.88</td><td>75.68</td><td>91.99</td><td>87.97</td></tr><tr><td>FedPIA (Parallel) (ours)</td><td>88.94</td><td>87.00</td><td>88.59</td><td>79.97</td><td>90.80</td><td>90.93</td><td>74.32</td><td>90.02</td><td>86.82</td></tr><tr><td>FedPIA(Compacter) (ours)</td><td>90.06</td><td>86.29</td><td>89.01</td><td>81.14</td><td>90.25</td><td>89.24</td><td>76.24</td><td>91.23</td><td>86.93</td></tr></table></body></html>

Table 3: Performance comparison of FedPIA with other methods on Tasks 3 and 4 using ViLT (in terms of F1 score)   

<html><body><table><tr><td rowspan="2">Fine-tuning</td><td colspan="6">Task 3: Open-I</td><td colspan="10">Task 4: MIMIC</td></tr><tr><td>C1</td><td>C2</td><td>C3</td><td>C4</td><td>Overall</td><td>C1</td><td>C2</td><td>C3</td><td>C4</td><td>C5</td><td>C6</td><td>C7</td><td>C8</td><td>C9</td><td>C10</td><td>Overall</td></tr><tr><td>Full fine-tuning</td><td>71.51</td><td>70.25</td><td>72.14</td><td>61.21</td><td>68.78</td><td>68.90</td><td>67.70</td><td>66.36</td><td>66.48</td><td>68.43</td><td>67.72</td><td>68.69</td><td>68.03</td><td>70.43</td><td>68.44</td><td>68.12</td></tr><tr><td>Classifier only (LB)</td><td>61.60</td><td>61.92</td><td>59.58</td><td>55.22</td><td>59.58</td><td>63.70</td><td>61.40</td><td>61.00</td><td>56.78</td><td>62.39</td><td>64.31</td><td>63.62</td><td>60.55</td><td>64.06</td><td>62.61</td><td>62.04</td></tr><tr><td>AdapterFusion</td><td>67.74</td><td>65.83</td><td>66.74</td><td>54.48</td><td>63.70</td><td>67.33</td><td>64.65</td><td>63.24</td><td>60.49</td><td>66.05</td><td>67.05</td><td>66.53</td><td>66.41</td><td>64.72</td><td>68.39</td><td>65.49</td></tr><tr><td>Houlsby Adapter</td><td>66.79</td><td>62.88</td><td>65.29</td><td>57.26</td><td>63.05</td><td>67.81</td><td>64.18</td><td>63.82</td><td>60.12</td><td>66.38</td><td>66.89</td><td>67.04</td><td>66.95</td><td>65.76</td><td>67.82</td><td>65.68</td></tr><tr><td>Parallel Adapter</td><td>66.71</td><td>64.33</td><td>64.80</td><td>58.12</td><td>63.49</td><td>67.44</td><td>64.86</td><td>63.04</td><td>61.14</td><td>66.39</td><td>66.78</td><td>67.13</td><td>66.46</td><td>65.02</td><td>66.57</td><td>65.48</td></tr><tr><td>Compacter</td><td>66.29</td><td>64.75</td><td>65.53</td><td>56.89</td><td>63.37</td><td>67.29</td><td>64.65</td><td>63.17</td><td>59.48</td><td>65.47</td><td>67.33</td><td>66.42</td><td>66.12</td><td>65.25</td><td>66.19</td><td>65.14</td></tr><tr><td>LayerNorm</td><td>63.17</td><td>62.88</td><td>63.00</td><td>55.36</td><td>61.10</td><td>67.03</td><td>63.70</td><td>61.50</td><td>60.73</td><td>64.31</td><td>65.78</td><td>65.37</td><td>64.56</td><td>65.06</td><td>66.44</td><td>64.45</td></tr><tr><td>LoRA</td><td>60.42</td><td>58.67</td><td>59.48</td><td>54.90</td><td>58.37</td><td>64.73</td><td>61.67</td><td>61.67</td><td>56.31</td><td>62.80</td><td>65.36</td><td>65.17</td><td>63.59</td><td>63.09</td><td>64.10</td><td>62.85</td></tr><tr><td>Bias</td><td>63.53</td><td>62.63</td><td>62.81</td><td>56.90</td><td>61.47</td><td>65.96</td><td>63.09</td><td>61.99</td><td>59.85</td><td>63.76</td><td>65.53</td><td>63.57</td><td>63.91</td><td>65.21</td><td>66.38</td><td>64.12</td></tr><tr><td>PromptFL</td><td>63.52</td><td>62.04</td><td>60.93</td><td>57.34</td><td>60.96</td><td>64.89</td><td>62.20</td><td>61.87</td><td>57.31</td><td>62.97</td><td>64.67</td><td>65.67</td><td>64.39</td><td>63.76</td><td>64.18</td><td>63.19</td></tr><tr><td>FedDAT(AdapterFusion)</td><td>66.34</td><td>63.57</td><td>63.57</td><td>57.96</td><td>62.86</td><td>66.39</td><td>63.28</td><td>62.20</td><td>60.34</td><td>66.94</td><td>67.32</td><td>67.34</td><td>67.76</td><td>65.52</td><td>65.77</td><td>65.29</td></tr><tr><td>FedDAT (Houlsby)</td><td>67.17</td><td>65.27</td><td>63.79</td><td>58.60</td><td>63.71</td><td>66.84</td><td>62.99</td><td>62.34</td><td>60.00</td><td>67.20</td><td>66.63</td><td>67.88</td><td>67.12</td><td>65.91</td><td>65.18</td><td>65.20</td></tr><tr><td>FedDAT (Parallel)</td><td>65.40</td><td>64.89</td><td>63.34</td><td>55.55</td><td>62.30</td><td>66.10</td><td>63.10</td><td>61.27</td><td>61.39</td><td>65.83</td><td>67.35</td><td>66.36</td><td>66.76</td><td>65.19</td><td>67.84</td><td>65.12</td></tr><tr><td>FedDAT(Compacter)</td><td>67.55</td><td>66.19</td><td>64.27</td><td>58.06</td><td>64.02</td><td>67.09</td><td>62.56</td><td>61.80</td><td>59.03</td><td>64.68</td><td>66.79</td><td>67.24</td><td>67.40</td><td>65.39</td><td>66.02</td><td>64.80</td></tr><tr><td>FedPIA (Houlsby) (ours)</td><td>70.99</td><td>69.76</td><td>72.08</td><td>60.26</td><td>68.27</td><td>68.94</td><td>68.25</td><td>67.02</td><td>65.24</td><td>68.31</td><td>68.78</td><td>70.34</td><td>68.09</td><td>69.02</td><td>68.06</td><td>68.21</td></tr><tr><td>FedPIA (Parallel) (ours)</td><td>70.20</td><td>70.14</td><td>71.78 71.73</td><td>61.01</td><td>68.28</td><td>68.76</td><td>66.12</td><td>65.44</td><td>65.67</td><td>67.30</td><td>67.42</td><td>68.18</td><td>67.46</td><td>69.75</td><td>67.96</td><td>67.41</td></tr><tr><td>FedPIA(Compacter) (ours)</td><td>71.32</td><td>70.05</td><td></td><td>60.93</td><td>68.51</td><td>68.26</td><td>67.22</td><td>66.16</td><td>66.31</td><td>66.02</td><td>68.73</td><td>68.91</td><td>68.36</td><td>69.89</td><td>67.02</td><td>67.69</td></tr></table></body></html>

Table 4: Performance comparison of FedPIA with other methods on Task 5 (in terms of F1 score). V implies VQA   

<html><body><table><tr><td>Fine-tuning</td><td>Open-I</td><td>MIMIC</td><td>Slake</td><td>V-Med</td><td>V-Rad</td><td>Overall</td></tr><tr><td>Full fine-tuning</td><td>74.22</td><td>65.34</td><td>97.78</td><td>97.44</td><td>98.76</td><td>86.71</td></tr><tr><td>Classifier only (LB)</td><td>64.51</td><td>64.02</td><td>97.33</td><td>97.38</td><td>98.65</td><td>84.38</td></tr><tr><td>AdapterFusion</td><td>72.27</td><td>63.41</td><td>97.76</td><td>97.60</td><td>98.71</td><td>85.95</td></tr><tr><td>Houlsby Adapter</td><td>69.11</td><td>66.01</td><td>97.81</td><td>97.50</td><td>98.70</td><td>85.83</td></tr><tr><td>Parallel Adapter</td><td>70.59</td><td>64.82</td><td>97.77</td><td>97.34</td><td>98.69</td><td>85.84</td></tr><tr><td>Compacter</td><td>69.57</td><td>65.67</td><td>97.79</td><td>97.48</td><td>98.67</td><td>85.84</td></tr><tr><td>LayerNorm</td><td>68.49</td><td>65.28</td><td>97.78</td><td>97.34</td><td>98.68</td><td>85.51</td></tr><tr><td>LoRA</td><td>66.89</td><td>64.13</td><td>97.19</td><td>97.31</td><td>98.61</td><td>84.83</td></tr><tr><td>Bias</td><td>67.99</td><td>64.81</td><td>97.67</td><td>97.45</td><td>98.69</td><td>85.32</td></tr><tr><td>PromptFL</td><td>65.65</td><td>63.91</td><td>97.15</td><td>97.42</td><td>98.68</td><td>84.56</td></tr><tr><td>FedDAT (AdapterFusion)</td><td>70.89</td><td>63.66</td><td>97.73</td><td>97.64</td><td>98.73</td><td>85.73</td></tr><tr><td>FedDAT (Houlsby)</td><td>69.45</td><td>64.29</td><td>97.80</td><td>97.58</td><td>98.66</td><td>85.56</td></tr><tr><td>FedDAT (Parallel)</td><td>70.88</td><td>65.03</td><td>97.82</td><td>97.35</td><td>98.70</td><td>85.96</td></tr><tr><td>FedDAT (Compacter)</td><td>70.19</td><td>64.86</td><td>97.75</td><td>97.54</td><td>98.60</td><td>85.79</td></tr><tr><td>FedPIA (Houlsby)(ours)</td><td>76.80</td><td>68.58</td><td>98.83</td><td>98.73</td><td>99.64</td><td>88.52</td></tr><tr><td>FedPIA (Parallel) (ours)</td><td>75.68</td><td>67.01</td><td>98.44</td><td>97.96</td><td>98.95</td><td>87.61</td></tr><tr><td>FedPIA (Compacter) (ours)</td><td>76.26</td><td>68.33</td><td>98.16</td><td>98.50</td><td>99.42</td><td>88.13</td></tr></table></body></html>

PEFT-FL and SOTA methods for all VQA tasks and scenarios. The performance of adapter-based PEFT baselines degrade under heterogeneity conditions, as visualized in the loss curve from Fig. 3 (a). This is mainly due to the baselines failing to properly integrate client-agnostic knowledge with client-specific knowledge from multiple adapters trained on diverse data, modalities, and tasks. On the contrary, FedPIA shows robust and consistent performance irrespective of data and modality heterogeneity, which in turn, demonstrates that our permutation and integration mechanism effectively handles the challenging non-IID scenario by bridging the gap between adapters in weight space. Our method achieves an overall mean improvement of ${ \bf \bar { 3 . 8 9 \% } }$ and ${ \bf 5 . 1 8 \% }$ for Tasks 1 and 2 respectively over FedDAT across all adapter configurations. In VQA-Med 2021 (from Tab. 1), FedDAT performs better than full fine-tuning. This is possibly because adapters in FedDAT are well-suited to retain task-specific adaptations and hence perform better than full fine-tuning which spreads updates across all parameters, diluting task-specific information. For further experiments or analysis, see Suppl. $\ S \mathbf { c }$

Table 5: Ablation study for all five tasks in terms of (overall): Accuracy for Tasks 1, 2 and F1 score for Tasks 3, 4, and 5   

<html><body><table><tr><td>Fine-tuning</td><td>Task 1</td><td>Task 2</td><td>Task 3</td><td>Task 4</td><td>Task 5</td></tr><tr><td></td><td colspan="5">Backbonearchitecture:ViLT</td></tr><tr><td>FedPIA</td><td>46.66</td><td>87.97</td><td>68.27</td><td>68.21</td><td>88.52</td></tr><tr><td>w/oanyPIA w/o server PIA w/oclientPIA</td><td>43.04 45.35 44.47</td><td>82.46 85.59 85.02</td><td>63.05 66.47 65.58</td><td>65.68 67.40 66.28</td><td>85.83 87.24 86.97</td></tr><tr><td>w/weight-based PIA</td><td>46.02</td><td>87.13</td><td>67.62</td><td>67.87</td><td>87.93</td></tr><tr><td></td><td>Backbonearchitecture:ALBEF</td><td></td><td></td><td></td><td></td></tr><tr><td>FedPIA</td><td>48.24</td><td>89.05</td><td>68.78</td><td>69.46</td><td>84.49</td></tr><tr><td>w/o any PIA</td><td>43.17</td><td>83.37</td><td>62.76</td><td>65.43</td><td>80.28</td></tr><tr><td></td><td>46.58</td><td></td><td></td><td></td><td></td></tr><tr><td>w/o serverPIA</td><td></td><td>86.44</td><td>65.92</td><td>67.88</td><td>83.16</td></tr><tr><td>w/o client PIA</td><td>45.20</td><td>85.29</td><td>64.87</td><td>66.38</td><td>82.00</td></tr><tr><td>w/weight-based PIA</td><td>47.75</td><td>88.38</td><td>68.09</td><td>68.65</td><td>83.99</td></tr></table></body></html>

![](images/efd9a187ae7602addc13f33ca3854a64beb452793f5706af5f27397bb138d472.jpg)  
Figure 4: Convergence analysis of adapter-based baseline models (left) and FedPIA (right) for Task 5 (Heterogeneous task). The peaks and troughs represent the losses at the start and end of each communicating rounds. The reduction in magnitude of spikes (right) show that FedPIA bridges the gap between global and local adapters leading to a faster, stable convergence with less oscillation.

# Results on Multilabel Disease Detection (Tasks 3, 4)

Table 3 shows that our method demonstrates consistent performance improvement with respect to the baselines in each client for the multi-label classification task on both the datasets. We notice that the baseline methods show deterioration in performance due to the statistical heterogeneity introduced by inter-client class distribution shift. FedPIA outperforms FedDAT approximately by ${ \bf 5 . 1 \% }$ and $\mathbf { 2 . 7 3 \% }$ in F1-score for Open-I and MIMIC datasets across all clients and over all adapter configurations. These results show that our method more effectively utilizes the knowledge from multiple adapters in statistically heterogeneous FL settings. This is further supported by the visualization of convergence analysis in Fig. 3 (b) where FedPIA loss curves are seen to be closest to the full fine-tuning loss curve.

# Results on Heterogeneous Task (Task 5)

Table 4 reports the performance of FedPIA and baselines under task-heterogeneous settings with VQA and classification datasets simultaneously. Fig. 4 further visualizes the corresponding loss curves for convergence analysis. It demonstrates that FedPIA results in much faster and more stable convergence that the adapter-based baselines by reducing the oscillations resulting from task heterogeneity. This reduction is achieved by the two-fold alignment of local and global adapters in clients and server. Interestingly, our model not only outperforms the baselines in each client, but also full fine-tuning (on average by $\mathbf { 1 . 8 1 \% }$ ) which suggests that our method more effectively preserves learned knowledge from diverse clients even with a small number of parameters whereas the baselines and full fine-tuning suffer from catastrophic forgetting due to knowledge interference from heterogeneous tasks and data distribution. The substantial boost in Open-I client model is attributed to the significantly larger overall FL dataset size in Task 5 (34323 samples across 5 clients) compared to Task 3 (2837 samples across 4 clients). However, MIMIC client model in Task 4 is developed using 73348 samples across 10 clients and hence, it does not show a notable learning advantage in Task 5.

# Ablation Studies

We study the impact of each component of FedPIA (Houlsby) via ablation analysis in Table 5. Our model with either client-based or server-based PIA alone is observed to outperform the baselines. Removing either of clientbased and server-based adaptation leads to a drop in performance. Greater performance degradation is observed when dropping client-level PIA, which suggests that client-based PIA captures more essential information from the permuted global adapter. We also show that replacing the activation by weight-based ground cost computation in the clients leads to slight decrease in adapter performance.

To investigate the impact of different hyperparameters and client size, we vary the learning rate and batch size, as well as progressively reduce the dataset size in each client from $1 0 0 \%$ in steps of $2 0 \%$ . We also investigate the scalability of FedPIA by increasing the number of clients. See Suppl. $\ S \mathbf { C }$ .

# Conclusion

The main contributions of the work are as follows:

1. We studied the practical problem of parameter-efficient fine-tuning of foundation models in multimodal FL for tackling data- and resource-constraints. We analysed different real-world problem scenarios with the overall goal of performing medical visual question answering or vision-language-based disease classification or both simultaneously. Through five different tasks, we, for the first time, investigated three FL heterogeneity settings - statistical heterogeneity, modality heterogeneity, as well as task heterogeneity in the context of PEFT-FL.

2. We proposed a novel method, Federated Learning via Permutation and Integration of Adapters, that exploits Wasserstein Barycenters for shuffling and combining adapters. We demonstrated this to be particularly effective in bringing adapters closer in data- and taskheterogeneous situations where the adapters are distant in parameter space. Our method does not require retraining for alignment or further knowledge distillation like existing methods, thereby adding no training overhead. Besides, FedPIA is orthogonal to existing FL aggregation schemes and can be used in conjunction with those.

3. For evaluating FedPIA, we developed a modality-specific FL setup using 41 medical image datasets. Through comprehensive experiments, we showed that FedPIA outperforms both SOTA and naive combinations of PEFT and FL. The results demonstrate that our proposed method can achieve, and even surpass, the performance of fully fine-tuned methods across diverse tasks, for fine-tuning VLMs in heterogeneous FL.

# Acknowledgments

This work was supported in part by the UK EPSRC (Engineering and Physical Research Council) Programme Grant EP/T028572/1 (VisualAI), a UK EPSRC Doctoral Training Partnership award, the UKRI grant EP/X040186/1 (Turing AI Fellowship), and the InnoHK-funded Hong Kong Centre for Cerebro-cardiovascular Health Engineering (COCHE) Project 2.1 (Cardiovascular risks in early life and fetal echocardiography). FW is supported by the EPSRC Centre for Doctoral Training in Health Data Science (EP/S02428X/1), by the Anglo-Austrian Society, and by an Oxford-Reuben scholarship. PS acknowledges Yash Bhalgat for the insightful discussions and valuable inputs.