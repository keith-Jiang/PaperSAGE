# Label-Free Backdoor Attacks in Vertical Federated Learning

Wei Shen\*, Wenke Huang\*, Guancheng Wan, Mang Ye†

National Engineering Research Center for Multimedia Software School of Computer Science, Wuhan University, China {weishen, wenkehuang, guanchengwan, yemang}@whu.edu.cn

# Abstract

Vertical Federated Learning (VFL) involves multiple clients collaborating to train a global model, with distributed features of shared samples. While it becomes a critical privacypreserving learning paradigm, its security can be significantly compromised by backdoor attacks, where a malicious client injects a target backdoor by manipulating local data. Existing attack methods in VFL rely on the assumption that the malicious client can obtain additional knowledge about task labels, which is not applicable in VFL. In this work, we investigate a new backdoor attack paradigm in VFL, Label-Free Backdoor Attacks (LFBA), which does not require any additional task label information and is feasible in VFL settings. Specifically, while existing methods assume access to task labels or target-class samples, we demonstrate that the gradients of local embeddings reflect the semantic information of labels. It can be utilized to construct the target poison sample set. Besides, we uncover that backdoor triggers tend to be ignored and under-fitted due to the learning of original features, which hinders backdoor task optimization. To address this, we propose selectively switching poison samples to disrupt feature learning, promoting backdoor task learning while maintaining accuracy on clean data. Extensive experiments demonstrate the effectiveness of our method in various settings.

![](images/af1a03fc5f37de4de64ca87b83a9c8caa7f215afb6a8ba54c0ffac0caeb4ae15.jpg)  
Figure 1: An Example of Backdoor Attacks in VFL. Consider the short video platform collaborates with the e-commerce company for recommending videos. The e-commerce company can act as the attacker to induce the target output of specific advertisement recommendations.

# Code — https://github.com/shentt67/LFBA/

# 1 Introduction

Vertical Federated Learning (VFL) (Hardy et al. 2017; Yang et al. 2019, 2023) has become a significant privacy-preserving collaboration paradigm. It involves training a global model with distributed features but shared samples across different clients, with only one client owning the task labels. Compared with Horizontal Federated Learning (HFL), where clients possess the same feature space (McMahan et al. 2017; Yang et al. 2019; Hu et al. 2023; Ye et al. 2023, 2024a,b; Huang et al. 2024; Wang et al. 2024b; Tan et al. 2024), VFL has shown promising results and applications particularly in cross-domain applications (Song et al. 2021; Huang, Wang, and Han 2023; Yan et al. 2024). However, despite adhering to privacy protocols, VFL remains vulnerable to security concerns, especially with malicious attacks when the trustworthiness of participants is uncertain. A crucial concern lies in the backdoor attacks (Liu et al. 2021b; Chen et al. 2023, 2024), which involve maliciously adding triggers to the data and inducing the target output. In VFL, the malicious client can introduce a backdoor by altering the local raw features to control the model behavior. For example, as shown in Figure 1, in a collaboration between a short video platform and an e-commerce platform, the e-commerce platform can inject backdoors into the VFL model and induce target recommendations in the short video platform by adding triggers.

While backdoor attacks introduce security vulnerabilities in VFL, thoroughly investigating backdoor attacks in VFL is crucial for developing effective security measures. Unlike HFL scenarios where attackers have full access to samples and corresponding labels (Gu, Dolan-Gavitt, and Garg 2017; Li et al. 2022), executing backdoor attacks in VFL presents unique challenges: attackers lack access to task labels, complicating the execution of backdoor attacks. Existing research has explored several backdoor attacks in VFL (Liu et al. 2021b; Gu and Bai 2023; Chen et al. 2023; He et al. 2023; Chen et al. 2024). For instance, Gu et al. propose LR-BA (Gu and Bai 2023), which trains a classifier with a few labeled samples and then minimizes the feature distance between the poison data and the target-class data. TECB (Chen et al. 2023) uses a universal trigger that is optimized with a few targetclass samples in preset, and directly poisons the target-class sample to learn the target-trigger correspondence. However, these methods rely on strong preset knowledge of task labels, such as direct access to task labels or a predefined target-class sample set (one or more samples), which is often not feasible for attackers. Chen et al. (Chen et al. 2024) propose an approach that assumes the attacker knows the task labels involve an imbalanced binary classification problem. They suggest inferring labels by identifying head-class samples with the largest gradients and optimizing a universal trigger. However, it requires prior knowledge and makes strong assumptions about the prediction tasks, limiting its applicability.

Existing works rely on knowledge or strong assumptions of task labels, which are typically unavailable to attackers in VFL. Beyond these limitations, the key challenge in executing backdoor attacks in VFL is: How can triggers be associated with the backdoor target without accessing task labels? We identify clean-label attacks (Turner, Tsipras, and Madry 2018; Zhao et al. 2020; Huynh et al. 2024) as a suitable solution, as they directly poison target-class samples without altering the labels. To construct a target poison set, we leverage the fact that embedding gradients, which are related to task label information, are returned to all clients, including the attacker. Inspired by this, we introduce GradientGuided Poison-Set Construction (GPC), which builds the target poison set by calculating consistency of embedding gradients. The attacker defines a local anchor sample, and the attack goal is to classify trigger samples into the anchor class. Samples with the most consistent embedding gradients to the anchor are included in the poison set. By using embedding gradients as guidance, the attacker can construct a poison sample set consistently drawn from the target class, enabling backdoor attacks without task labels.

Besides, to effectively inject backdoors in VFL, we propose a novel poison method Selectively SAmple SWitching (SAW). When optimizing the backdoor tasks, the trigger and the target should be associated. However, since the chosen samples consistently come from the backdoor target, we argue that the model will focus on learning the reflection between the origin features and the target, leading to the trigger being ignored and under-fitted. Motivated by it, we propose an intuitive solution to disrupt feature learning, which encourages the model to focus on learning the trigger. Specifically, we switch the poison samples with other local samples, to disturb the feature learning and enhance backdoor optimization. Additionally, to maintain the clean data accuracy, we purposefully select only a subset of samples with the maximum gradients in the poison set to add the trigger and perform the switching. These are considered ‘hard samples’, and manipulating them has minimal impacts on the benign performance, achieving a better trade-off between the main task and the backdoor task. In summary, our contributions can be outlined as follows:

• We propose a label-free backdoor attack method that is applicable in the VFL setting. Specifically, we construct the target-class poison set guided by embedding gradients, choosing samples consistently from the target class without requiring additional knowledge of task labels. • We introduce a novel poison method that selectively switches the poison samples. It disrupts the original feature learning, enhances trigger learning, and achieves a better trade-off between the main and backdoor tasks. • We conduct extensive experiments to demonstrate that our proposed method Label-Free Backdoor Attacks (LFBA), is effective to perform backdoor attacks in various settings, without additional knowledge for task labels.

# 2 Related Work

Vertical Federated Learning. Vertical Federated Learning (VFL) (Hardy et al. 2017; Yang et al. 2023; Liu et al. 2024c; Ye et al. 2024b) is a privacy-preserving learning paradigm, where participants share overlapping sample spaces but have distinct data feature spaces. It has been widely explored in recent research (Zhang et al. 2021; Wu, Li, and He 2022; Wu, Hou, and He 2024; Gao et al. 2024; Qiu et al. 2024; Wang et al. 2024a), showing promising results and potentials in cross-domain collaborations, such as finance (Zheng et al. 2020; Long et al. 2020), healthcare (Huang, Wang, and Han 2023; Song et al. 2021; Yan et al. 2024), and recommendation systems (Zhang and Jiang 2021; Yuan et al. 2022; Wei et al. 2023), among others (Jin et al. 2021; Liu et al. 2021a; Zhang and Jiang 2021; Fan et al. 2024; Shen, Ye, and Huang 2024; Ye et al. 2024c; Liang et al. 2024a,b). In this paper, we investigate backdoor attacks in VFL without task labels, providing insights for exploring security threats in VFL.

Backdoor Attacks. Backdoor attacks (Gu, Dolan-Gavitt, and Garg 2017; Li et al. 2022; Fang et al. 2024; Liu et al. 2024b,d; An et al. 2024) were first proposed to inject a fixed activation pattern into images, to target a specific class. The original method in (Gu, Dolan-Gavitt, and Garg 2017) involved changing the labels of random samples to the target class, and then adding a fixed trigger to the samples, thereby training the model to learn the correspondence between the trigger and the target class. Besides, some research focuses on backdoor attacks that do not require label manipulation, making them more stealthy, known as clean-label attacks (Turner, Tsipras, and Madry 2018; Zhao et al. 2020; Ning et al. 2021; Hu et al. 2022; Gao et al. 2023; Huynh et al. 2024). Clean-label attacks assume the attacker adds triggers to the samples of the target class, enabling the model to learn the trigger-target correspondence without altering the labels. In this work, we explore the paradigm of clean-label attacks, where the malicious client in VFL cannot alter the labels.

Backdoor Attacks in VFL. Backdoor attacks have been extensively studied in Horizontal Federated Learning (HFL) settings (Bagdasaryan et al. 2020; Lyu et al. 2023; Qin et al. 2024; Liu et al. 2024a), where malicious clients can directly add triggers and alter the labels of local samples. However, additional challenges make these attacks difficult to execute in VFL settings. The primary challenge is that task label information is inaccessible to the malicious client in VFL, complicating the attack. Several works have explored backdoor attacks in VFL settings (Liu et al. 2021b; Gu and Bai 2023; Chen et al. 2023; He et al. 2023; Chen et al. 2024). Nevertheless, they assume that the attacker has preset knowledge or assumptions about the task labels, which is not feasible in VFL. In this work, we propose a label-free approach to perform backdoor attacks, which is feasible in VFL.

# 3 Preliminary

# 3.1 Formal Problem Definition

Objective of Vertical Federated Learning. In VFL, clients share overlapping sample spaces but possess private feature spaces. Each client holds a local model to extract embeddings of origin data. The goal is to collaboratively train a prediction model where only one client, i.e., the active client, holds the task labels. The other clients, i.e., the passive clients, send embeddings of the shared samples to the active client and participate in training the global prediction model. The gradients are then sent back to each client for local model updates. Define $K$ as the number of clients, and $N$ is the seth arle.d2s0a1m7)plaecsrodsisccolvienrtesd, bdyefainliegdnamse $\boldsymbol { D } ^ { \setminus } = \{ ( x _ { i } , y _ { i } ) \} _ { i = 1 } ^ { N }$ where $x _ { i } \in \mathbb { R } ^ { d }$ is the raw data with $d$ dimensions, and $y _ { i }$ is the corresponding label. The features of each sample $x _ { i }$ are distributed across clients as $x _ { i } = \{ x _ { i } ^ { k } \} _ { k = 1 } ^ { K }$ , with $\boldsymbol { x } _ { i } ^ { k } \in \mathbb { R } ^ { d ^ { k } }$ The sample features in each client $P ^ { k }$ can be defined as $D ^ { k } = \{ x _ { i } ^ { \star } \} _ { k = 1 } ^ { K }$ . The objective of VFL can be formulated:

$$
\operatorname* { m i n } _ { \Theta } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \mathcal { L } ( G ( H _ { i } ^ { 1 } , . . . , H _ { i } ^ { K } ; \theta ^ { g } ) , y _ { i } ) ,
$$

where each client $P ^ { k }$ holds a local model $f _ { k } \big ( \cdot ; \theta ^ { k } \big )$ that computes the embeddings $H _ { i } ^ { k } = f _ { k } ( x _ { i } ^ { k } ; \theta ^ { k } )$ from the raw features. The final prediction is made at the active client $P ^ { a } , a \in$ $\{ 1 , . . . , K \}$ with a global model $G ( \cdot ; \theta ^ { g } )$ . The parameters of the overall VFL model are defined as $\dot { \Theta } = \{ \dot { \theta ^ { 1 } } , . . . , \theta ^ { K } , \theta ^ { g } \}$ . Define $\mathcal { L }$ as the loss function, where a cross-entropy loss can be employed for classification tasks.

Objective of Backdoor Attacks in VFL. The goal of backdoor attacks is to establish the reflection between the designed trigger and the target class, with poisoned data from the attacker $P ^ { m }$ , $m \in \{ \bar { 1 } , . . . , K \}$ (the other benign clients are defined as {P k}kK=1 − {P m}). Besides, the performance on clean data is maintained. Define the whole VFL model as $F ( \cdot ; \Theta )$ , the objective of backdoor attacks can be formulated:

$$
\operatorname* { m i n } _ { \Theta } \ \frac { 1 } { N _ { c } } \sum _ { i = 1 } ^ { N _ { c } } \mathcal { L } ( F ( x _ { i } ; \Theta ) , y _ { i } ) \ + \ \frac { 1 } { N _ { p } } \sum _ { i = 1 } ^ { N _ { p } } \mathcal { L } ( F ( x _ { i } + \delta ; \Theta ) , \tau ) \ ,
$$

where $N _ { c }$ and $N _ { p }$ represent the numbers of samples in the clean dataset $\bar { D _ { c } } \subseteq D$ and poisoned sample set $D _ { p } \subseteq D$ with $D _ { c } \cup D _ { p } = D$ . The trigger for the backdoor attacks is denoted by $\delta$ and the backdoor target is denoted by $\tau$ In previous backdoor attacks, data poisoning often relies on knowledge of the target label information, where sample labels were altered to the target, or the poison set was selected based on the sample labels belonging to the target class. However, in VFL, the sample labels of collaboration tasks are private information, posing challenges for the attacker to alter the labels or construct the poison set. To overcome these limitations, in this work, we explore a backdoor attack approach applicable to VFL in a label-free manner.

# 3.2 Threat Model

We explore the scenario where one of the passive clients in VFL assumes the attacker. It is a plausible assumption that malicious clients intend to inject their intended backdoor into the VFL model while obeying the VFL protocol.

Attacker Capability. As a passive client, the attacker has access to its local data, model, and the gradients of intermediate embeddings during the training process. The attacker does not possess any information about the task labels. It can manipulate all of its local data and model parameters, but cannot alter any labels in the active client.

Attacker Objective. The goal of the attacker is to inject a target backdoor into the VFL model, which will be activated by a predefined trigger pattern. Once the VFL model is deployed, the attacker can add the trigger to the local data of samples, and the final predictions in the active client will be intentionally misclassified to the backdoor target. However, the classification accuracy on clean data should be maintained.

# 4 Methodology

To bypass label limitations in VFL, we follow a clean-label setting (Turner, Tsipras, and Madry 2018; Zhao et al. 2020), which involves: (1) creating a poison sample set from the target class; (2) adding triggers on these samples for model training to inject the backdoor. It avoids modifying sample labels, adhering to the fundamental assumptions in VFL. For step (1), we define an anchor sample and choose poison samples that are most consistent with the anchor based on their embedding gradients. For step (2), we selectively switch the local data of poison samples with other samples, enhancing attacks while preserving main task accuracy.

# 4.1 Gradient-Guided Poison-Set Construction

Motivation. In VFL, as a passive client, the task label information is not accessible to the attacker, presenting challenges for backdoor attacks. However, the attacker receives updated gradients of local embeddings, which closely relate to the sample labels. In this case, we leverage the embedding gradients as guidances to construct the target poison set, ensuring it consistently originates from the backdoor target.

Gradient-Guided Poison-Set Construction. To effectively inject the backdoor into the VFL model, the attacker first chooses an anchor sample as guidance. Denote the anchor sample as $x _ { r }$ , with features $x _ { r } ^ { m }$ distributed to the attacker client. The backdoor target $\tau$ is set to the class of the anchor sample $y _ { r }$ , and the label is only possessed in the active client. During training, each client sends local embeddings $H ^ { k }$ and receives corresponding gradients calculated by the active client. Similarly, the attacker client sends the embeddings $\{ H _ { i } ^ { m } \} _ { i = 1 } ^ { N }$ computed from the local data $D ^ { m } = \{ x _ { i } ^ { m } \} _ { i = 1 } ^ { N }$

![](images/ceae9e752c0c358201b0856c474a18674eda5caeb0f3da6e51df55b04d2ff81f.jpg)  
Figure 2: The framework of Label-Free Backdoor Attacks (LFBA). To construct the poison set that consistently from the backdoor target, (a) Gradient-Guided Poison-Set Construction (GPC in Section 4.1): the attacker chooses an anchor sample locally, and the samples with maximum consistency are chosen for the poison set. They are consistently from the same class of the anchor, i.e., the backdoor target; To effectively poison samples for injecting target backdoor, $( b )$ Selectively Sample Switching (SAW in Section 4.2): the attacker selectively switches the poison samples that are with maximum embedding gradients, which promotes the backdoor optimization by disturbing the origin feature learning while maintaining the main task accuracy.

Subsequently, the attacker can obtain the embedding gradients $\mathrm { \dot { \{ V _ { H _ { i } ^ { m } } \} } } _ { i = 1 } ^ { N }$ from the active client. The gradients of embeddings can be calculated by:

$$
\nabla _ { H _ { i } ^ { k } } = \frac { \partial \mathcal { L } ( G ( H _ { i } ^ { 1 } , . . . , H _ { i } ^ { K } ; \theta ^ { g } ) , y _ { i } ) } { \partial H _ { i } ^ { k } } .
$$

Given that the embedding gradients are calculated with sample labels and retain the label information, we utilize the gradients as guidance to construct a sample set consistent with the backdoor target. Specifically, we calculate the consistency of embedding gradients between the anchor and other samples and construct the target set comprising samples with the highest consistency. The construction process can be defined as follows:

$$
D _ { p } = \arg \operatorname* { m a x } _ { D _ { p } } \frac { 1 } { N _ { p } } \sum _ { i = 1 } ^ { N _ { p } } \frac { \nabla _ { H _ { r } ^ { m } } \cdot \nabla _ { H _ { i } ^ { m } } } { \left\| \nabla _ { H _ { r } ^ { m } } \right\| _ { 2 } \left\| \nabla _ { H _ { i } ^ { m } } \right\| _ { 2 } } ,
$$

where $\nabla _ { H _ { r } ^ { m } }$ is the gradient of the anchor sample embedding ${ \cal H } _ { r } ^ { m }$ . The target set of chosen samples is denoted as $D _ { p }$ , cwoentcaoinsitnrgu $N _ { p }$ esapmoipsloesn sweithdutrhiengrathioe $\begin{array} { r } { p = \frac { N _ { p } } { N } } \end{array}$ a. nCiongcreeptoeclyh, and maintain the same set for subsequent training rounds. Guided by the embedding gradients, the selected samples are intended to consistently with the same label as the anchor sample, which is the backdoor target. Utilizing the poison set consistently from the backdoor target and adding designed triggers, the VFL model can learn the association between the backdoor target and the triggers without the need to alter any labels. During the construction process, the attacker has no access to label information about the backdoor target, which aligns with the constraints in the VFL setting.

# 4.2 Selectively Sample Switching

Motivation. After obtaining the target sample set, the attacker can add the designed triggers to the chosen samples and inject the target backdoor directly. Consider the samples from the backdoor target $\tau$ in $D _ { p }$ , the backdoor task can be formulated:

$$
\operatorname* { m i n } _ { \Theta } \frac { 1 } { N _ { p } } \sum _ { i = 1 } ^ { N _ { p } } \mathcal { L } ( F ( \overset { \vartriangle } { x _ { i } } + \delta ; \Theta ) , \frac { \vartriangle } { \underset { \vartriangle } { \uparrow } } ) .
$$

# ng

# Trigger Learning

To inject the target backdoor, the reflection between the trigger $\delta$ and the target $\tau$ is expected to be established, i.e., Trigger Learning. However, with the poison samples consistently chosen from the backdoor target $\tau$ , the model will focus on Feature Learning: the correspondence between the original features and the target. In this case, the trigger will be ignored and under-fitted, hindering the optimization of trigger learning. To address this limitation, an intuitive solution is to disturb the feature learning, thereby encouraging the VFL model to focus on learning triggers.

Selectively Sample Switching. Within the attacker capability, we propose to switch the poison samples with other samples to disturb feature learning, promoting backdoor task optimization. We first switch each poison sample features $x _ { i } ^ { m }$ with the local data of other samples $x _ { j } ^ { m } \in \{ D - D _ { p } \}$ not in the poison set. Then a designed trigger is added to the local data of the poison samples. The poisoning process with switching can be formulated:

$$
x _ { i } ^ { m }  x _ { j } ^ { m } + \delta .
$$

In this way, the feature learning of poison samples is disrupted, promoting the VFL model to focus on trigger learning for enhancing attack performance. Besides, to maintain the accuracy on clean data, we selectively switch and add triggers to only a portion of the poison set samples. It minimizes the impact on original feature learning, achieving a better tradeoff between main task accuracy and backdoor performance. Concretely, we select the hard samples with the maximum embedding gradients, which are considered difficult to learn and promote main task performance. The construction of the switching sample set can be defined as:

$$
D _ { s } = \arg \operatorname* { m a x } _ { D _ { s } } \frac { 1 } { N _ { s } } \sum _ { i = 1 } ^ { N _ { s } } { \left\| \nabla _ { H _ { i } ^ { m } } \right\| _ { 2 } } ,
$$

where $\boldsymbol { D _ { s } } \subseteq D _ { p }$ is the selected sample set to perform poisoning by switching and adding the trigger. The size of $D _ { s }$ is $N _ { s }$ , with a switch ratio of $\begin{array} { r } { \bar { s } \ = \ \frac { N _ { s } ^ { - } } { N } } \end{array}$ . Define $[ \cdot ]$ as data concatenation, the final poisoned data can be formulated:

$$
D _ { s f } = \{ ( [ x _ { i } ^ { 1 } , . . . , x _ { j } ^ { m } + \delta , . . . , x _ { i } ^ { K } ] , y _ { i } ) \} _ { i = 1 } ^ { N _ { s } } .
$$

The modified training data $D _ { f }$ can be defined as follows:

$$
D _ { f } = D _ { c } + ( D _ { p } - D _ { s } ) + D _ { s f } .
$$

With the final training data $D _ { f }$ , the target backdoor can be injected into the VFL model successfully without performance degradation on clean data, under the applicable label-free assumptions. For a clearer illustration, we provide an overview of the proposed attacks in Algorithm 1.

# 4.3 Discussion and Limitation

Existing Attacks with Gradients in FL. Existing methods in FL also use gradient information to aid backdoor attacks. Several works (Sun et al. 2019; Wang et al. 2020) propose performing projected gradient updates in the attacker client, where the attack model stays close to the global model. Yoo et al. (Yoo and Kwak 2022) suggest using gradient ensembling from multiple poison rounds to improve attack generalization. Nguyen et al. (Nguyen et al. 2024) leverage historical gradient variations to pick infrequently updated neurons for poisoning, reducing the dilution effect from benign clients. In this work, we propose constructing the poison set by embedding gradients in VFL, providing insights for future works.

Existing Attacks in VFL. Existing methods rely on prior knowledge or assumptions about task labels. In contrast, we design several baselines without additional knowledge for comparison. This highlights the effectiveness of our method with each key component. Please refer to Section 5 for details. Limitations. However, our method LFBA may face challenges in certain situations: (1) If the number of clients increases and the attacker features remain extremely limited, the attack performance may decrease. (2) The sample switching may result in sub-optimal performance due to discrepancies between the switching features and the backdoor target.

# 5 Experiment

Datasets. We evaluate our method on four real-world datasets, with data distributed to multiple clients, and only the active client holds the task labels: (1) NUS-WIDE (Chua et al. 2009): A multi-modal dataset contains 1000 text features and

Algorithm 1: The framework of LFBA in VFL

Input: Initial training data $D$ and VFL model $F ( ; \Theta _ { 0 } )$ ; The active client $P ^ { a }$ and malicious client $P ^ { m }$ ; The trigger $\delta$ .

Output: Trained VFL model $F ( ; \Theta _ { T } )$ , activated with trigger $\delta$ for target $\tau$ .

1: for epoch $t  1 , . . . , T$ do 2: for all clients $\mathbf { \Phi } ^ { P ^ { k } } \gets P ^ { 1 } , . . . , P ^ { K }$ in parallel do 3: Compute $H ^ { k } = f _ { k } ( x ^ { k } ; \theta _ { t } ^ { k } )$ ; 4: Send $H ^ { k }$ to $P ^ { a }$ . 5: end for 6: for active client $P ^ { a }$ do 7: $L _ { t } = \mathcal { L } ( G ( H _ { i } ^ { 1 } , . . . , H _ { i } ^ { K } ; \theta _ { t } ^ { g } ) , y _ { i } )$ ; 8: Return $\nabla _ { H ^ { k } }$ calculated by Equation (3); 9: Update global model via ∇θtg = ∂θLg . 10: end for 11: for attacker $P ^ { m }$ do 12: if $\scriptstyle { \mathrm { t } } = = 1$ then 13: Choose an anchor in local data $x _ { a } ^ { m }$ ; 14: Construct $D _ { p }$ through Equation (4); 15: end if 16: Poison $D$ into $D _ { f }$ by Equation (6)-Equation (9). 17: end for 18: for all clients $P ^ { k } \gets P ^ { 1 } , . . . , P ^ { K }$ in parallel do 19: Update local model with $\begin{array} { r } { \nabla _ { \theta _ { t } ^ { k } } = \frac { \mathsf { \bar { \partial } } \nabla _ { H ^ { k } } } { \partial \theta _ { t } ^ { k } } } \end{array}$ 20: end for 21: end for

634 image features, labeled with multiple classes. We use a five-class subset including ‘buildings’, ‘grass’, ‘animal’, ‘water’, and ‘person’, with 69966 training samples and 46693 testing samples. (2) UCIHAR (Anguita et al. 2013): A human activity recognition dataset with six classes: ‘walking’, ‘walking upstairs’, ‘walking downstairs’, ‘sitting’, ‘standing’, and ‘laying’, with 7352 training samples and 2947 testing samples. (3) Phishing (Asuncion, Newman et al. 2007): It provides 30 features indicating whether a website is a phishing website, with 8844 training samples and 2211 test samples. (4) CIFAR-10 (Krizhevsky, Hinton et al. 2009): It is an image dataset for 10 classification tasks with 50000 training samples and 10000 testing samples. We conduct evaluations with two-client settings ( $K = 2$ ) and four-client settings $\langle K = 4 \rangle$ ). On the NUS-WIDE dataset, image features and text features are distributed separately to different clients when $K = 2$ . In other cases, the features are equally partitioned to all clients. Baselines. We compare several applicable baselines without requiring task labels in VFL: (1) Vanilla: The VFL baseline without attacks. (2) DGPC: Construct the poison set $D _ { p }$ by GPC and directly add triggers. (3) RGPC: Randomly select $N _ { s }$ samples in $D _ { p }$ to add triggers. (4) RS-GPC: Randomly select $N _ { s }$ samples in $D _ { p }$ to switch and add triggers.

Evaluation Metrics. We evaluate the attack performance with three metrics: the accuracy $( \mathcal { M } )$ on clean data and the attack success rate $( \mathcal { A } )$ of poison data (Gu, Dolan-Gavitt, and Garg 2017). Additionally, we use the mean values of $( \mathcal { M } )$ and $( \mathcal { A } )$ , referred to as $\nu$ , to assess the trade-off performance. Models. In all experiments, each client employs a local model to extract embeddings, while the active client employs an additional global model for final predictions. For the NUSWIDE and UCI-HAR datasets, we utilize a 4-layer linear model as the local model and a 3-layer model for the global model. For the Phishing dataset, we use a 2-layer model for both the local model and the global prediction model. For CIFAR-10, we utilize the ResNet18 (He et al. 2016) as the local model and a 3-layer linear model for the global model. Implement Details. We randomly set several dimensions to a fixed value as the triggers on the NUS-WIDE, UCI-HAR, and Phishing datasets. For CIFAR-10, we utilize the same trigger pattern as BadNets (Gu, Dolan-Gavitt, and Garg 2017). The backdoor target is consistent with the anchor sample class: on the NUS-WIDE and UCI-HAR datasets, it is the fourth class, e.g., ‘water’ in the NUS-WIDE dataset; on the Phishing dataset, it is ‘not the phishing website’. For CIFAR-10, the target is the seventh class, i.e., ‘frog’. All models are trained until convergence using the Adam optimizer (Kingma and Ba 2015) with a batch size of 256. The learning rate of all models is set to 0.001 for the NUS-WIDE and CIFAR-10 datasets, and 0.003 for the UCI-HAR and Phishing datasets. The poison sample ratio $\begin{array} { r } { p = \frac { N _ { p } } { N } } \end{array}$ is set between 0.1 and 0.3, and the switching sample ratio $\begin{array} { r } { s = \frac { N _ { s } } { N _ { p } } } \end{array}$ Ns is set between 0 and 1 (e.g., $p = 0 . 1$ and $s = 0 . 3$ for the NUS-WIDE dataset).

<html><body><table><tr><td rowspan="3">Methods</td><td colspan="4">NUS-WIDE</td><td colspan="6">UCI-HAR</td></tr><tr><td colspan="2">K=2</td><td colspan="3">K=4</td><td colspan="2">K=2</td><td colspan="3">K=4</td></tr><tr><td>M</td><td>A V</td><td>M</td><td>A</td><td>V</td><td>M</td><td>A V</td><td>M</td><td>A</td><td>V</td></tr><tr><td>Vanilla</td><td>83.98</td><td>12.36 48.17</td><td>82.56</td><td>2.27</td><td>42.25</td><td>92.98</td><td>4.48 48.73</td><td>90.70</td><td>2.73</td><td>46.72</td></tr><tr><td>DGPC</td><td>83.56</td><td>98.30 90.93</td><td>80.23</td><td>85.85</td><td>83.04</td><td>87.75</td><td>77.28 82.52</td><td>88.90</td><td>86.12</td><td>87.51</td></tr><tr><td>RGPC</td><td>83.86</td><td>96.86 90.36</td><td>82.21</td><td>81.60</td><td>80.29</td><td>86.33</td><td>66.33 76.33</td><td>90.50</td><td>84.40</td><td>87.45</td></tr><tr><td>RS-GPC</td><td>83.46</td><td>98.55 91.01</td><td>82.26</td><td>86.45</td><td>84.36</td><td>91.13</td><td>98.72 94.93</td><td>89.93</td><td>88.80</td><td>89.37</td></tr><tr><td>LFBA</td><td>83.93 99.85</td><td>91.89</td><td>82.36</td><td>95.13</td><td>88.75</td><td>91.99 99.96</td><td>95.98</td><td>90.69</td><td>90.63</td><td>90.66</td></tr><tr><td rowspan="3">Methods</td><td colspan="5">Phishing</td><td colspan="4">CIFAR-10</td></tr><tr><td>K=2</td><td></td><td></td><td>K=4</td><td></td><td>K=2</td><td></td><td></td><td>K=4</td><td></td></tr><tr><td>M</td><td>A</td><td>V</td><td>A</td><td>V</td><td></td><td>A</td><td>V</td><td>M</td><td>A</td><td>V</td></tr><tr><td>Vanilla</td><td>95.12</td><td>2.22 48.67</td><td>M 92.76</td><td>2.55</td><td>47.66</td><td>M 78.22</td><td>3.00 40.61</td><td>73.66</td><td>4.26</td><td>38.96</td></tr><tr><td>DGPC</td><td>93.26</td><td>78.16 85.71</td><td>90.90</td><td>76.59</td><td>83.75</td><td>70.39</td><td>96.48 83.43</td><td>69.77</td><td>82.40</td><td>76.09</td></tr><tr><td>RGPC</td><td>93.71</td><td>77.67 85.69</td><td>91.68</td><td>70.23</td><td>80.96</td><td>71.17</td><td>94.87 83.02</td><td>71.13</td><td>80.02</td><td>75.58</td></tr><tr><td>RS-GPC</td><td>94.30</td><td>81.36 87.83</td><td>91.63</td><td>77.98</td><td>84.81</td><td>77.12</td><td>97.18 87.15</td><td>70.20</td><td>91.94</td><td>81.07</td></tr><tr><td>LFBA</td><td>93.76 83.09</td><td>88.42</td><td>91.95</td><td>80.74</td><td>86.35</td><td>77.68</td><td>98.22 87.95</td><td>72.33</td><td>93.47</td><td>82.90</td></tr></table></body></html>

Table 1: Comparisons with Baselines. Bold represents the highest accuracy.

Table 2: Ablation with Different $N _ { p }$ and $N _ { s }$ . Our attack method is effective with different numbers of poison samples.   

<html><body><table><tr><td rowspan="2">Np N p=</td><td colspan="2">NUS-WIDE</td><td rowspan="2">Ns</td><td colspan="2">NUS-WIDE</td></tr><tr><td>M</td><td>A V</td><td>s N</td><td>M A</td></tr><tr><td>Vanilla</td><td>83.98</td><td>12.36 48.17</td><td>Vanilla</td><td>83.98</td><td>12.36 48.17</td></tr><tr><td>0.02</td><td></td><td>83.86 99.49 91.67</td><td></td><td>0.1</td><td>83.62 99.70 91.66</td></tr><tr><td>0.06</td><td></td><td>83.73 99.86 91.79</td><td></td><td>0.3</td><td>83.9399.8591.89</td></tr><tr><td>0.1</td><td></td><td>83.93 99.85 91.89</td><td></td><td>0.5</td><td>83.73 99.94 91.84</td></tr><tr><td>0.14 0.18</td><td></td><td>83.76 99.97 91.87</td><td></td><td>0.7 0.9</td><td>83.63 99.97 91.80</td></tr><tr><td colspan="2">0.2</td><td>83.03 99.81 91.42</td><td>82.79 95.91 89.35 (a)Different Poison Ratios.</td><td>1</td><td>83.57 99.96 91.77 83.54 99.94 91.74</td></tr></table></body></html>

![](images/14df87dbdf683d016591c81978d9ae326a9cfe573e816730f0d29b8554945930.jpg)  
Figure 3: Ablation with Different Trigger Sizes. Our attack method is effective with different trigger sizes.

# 5.1 Ablation Study

Comparison with Baselines. We provide comparisons with baseline methods and present results in Table 1, where LFBA achieves high attack success rates, up to $9 2 . 6 4 \%$ on average, without significant degradation in main task accuracy. Comparing DGPC with Vanilla, the constructed poison set can be utilized to effectively inject backdoors, resulting in an average ASR of $8 5 . 1 5 \%$ . Comparing RGPC with RS-GPC shows an average gain of $8 . 6 3 \%$ in ASR, demonstrating that the sample switching method is effective for enhancing attacks. Comparing LFBA with RS-GPC, the trade-off performance increases by $1 . 5 5 \%$ on average, indicating that the selective switching strategy is effective for a better trade-off.

Ablation with Different $N _ { p }$ and $N _ { s }$ . To investigate the impacts of poison sample ratio $p$ and switch sample ratio $s$ , we conduct two ablation experiments: (a) we change the poison ratios $p$ while keeping the switch ratio fixed at 0.3, and (b) we change the switch ratio $s$ while keeping $p$ fixed at 0.1. As shown in Table 2a, the attack performance of LFBA remains effective with changes in $p$ , although decreases with relatively large values, e.g., $p = 0 . 2$ . It is because the consistency of the constructed poison set decreases, hindering the trigger learning. However, a small sample set is usually preferred to ensure the attack is stealthy. As shown in Table 2b, the performance of LFBA remains stable and effective across different $s$ values. Both experiments show the effectiveness of our method with various poison and switch ratios.

![](images/1dd44433f75e0c28c5ca06aa88c3d2362b4aa5c8cae63204191c5f4a706e8781.jpg)  
Figure 4: Experiments with Defenses. The results show the effectiveness of LFBA under different defense strategies.

Ablation with Different Trigger Sizes. We conduct ablations on the NUS-WIDE and UCI-HAR datasets, to explore the impacts of trigger sizes. As depicted in Figure 3, backdoor attacks can be successful (over $90 \%$ ) with different trigger sizes even when only $0 . 0 3 \%$ of the original features are changed to the fixed trigger value (5 trigger dimensions vs. 1634 original feature dimensions on the NUS-WIDE dataset).

# 5.2 Extended Analysis

Attack under Defenses. To evaluate the attack effectiveness under defenses, we apply two defense strategies and test LFBA on four datasets: gradient compression (Shokri and Shmatikov 2015; Lin et al. 2018; Fu et al. 2022), and adding Gaussian noise to the gradients (Fu et al. 2022). We use the compression rates of 0.8, 0.6, and 0.4, and the Gaussian noise standard deviations of 0.0001, 0.001, and 0.01 for evaluation. As depicted in Figure 4, although stronger defenses can reduce the attack success rates, they also degrade main task performance significantly, indicating the defenses fail.

Consistent Rate of Poison Set. We calculate the consistent rate of the constructed poison set $D _ { p }$ , which indicates the proportion of samples that belong to the class of the anchor sample (backdoor target). As illustrated in Figure 5, the constructed sample set consistently includes samples from the attack target across different poison ratios $p$ , demonstrating that GPC effectively constructs the target poison set.

![](images/2d59d040f6a56d03d8e0f7bdf9386e548d8af3e920390dd98bd81a2a89a43595.jpg)  
Figure 5: Consistent Rate with Different Poison Ratios $p$ . The constructed poison set is consistently from the target class.

![](images/9618d2f9b625aebda316b4a75eec35a8811744539168d8aa7bf9cfb6d4c07995.jpg)  
Figure 6: Feature Visualizations. (a) The samples in the poison set are consistently from the target class. (b) The samples with triggers are misclassified into the target class.

Visualization Results. We further visualize the features acquired from the VFL model with the injected backdoor. In Figure 6a, we visualize parts of samples in poison set $D _ { p }$ , which are consistently from the same class (backdoor target). Besides, we poison several samples with triggers and visualize them in Figure 6b, where the poisoned samples cluster into the backdoor target, indicating successful attacks.

# 6 Conclusion

In Vertical Federated Learning (VFL), the trained model may be vulnerable to backdoor attacks, where the final output is rendered to the backdoor target once the specific triggers are added. In this paper, we investigate an applicable backdoor attack in VFL that does not require knowledge of the prediction tasks or any label information. Specifically, we use the gradients of embeddings as guidance to construct a consistent poison set for the backdoor target. Additionally, we propose selectively switching the sample features of the poison set to enhance backdoor task optimization, and achieve a better trade-off between the main task and the backdoor task. This research provides valuable insights for executing backdoor attacks in VFL, and will induce potential implications for real-world security applications and studies.