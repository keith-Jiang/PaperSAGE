# ICE-T: Interactions-aware Cross-column Contrastive Embedding for Heterogeneous Tabular Datasets

Tomas Tokar1, 2, Scott Sanner2, 3

1Wondeur AI 2University of Toronto 3Vector Institute for AI

# Abstract

Finding high-quality representations of heterogeneous tabular datasets is crucial for their effective use in downstream machine learning tasks. Contrastive representation learning (CRL) methods have been previously shown to provide a straightforward way to learn such representations across various data domains. Current tabular CRL methods learn joint embeddings of data instances (tabular rows) by minimizing a contrastive loss between the original instance and its perturbations. Unlike existing tabular CRL methods, we propose leveraging frameworks established in multimodal representation learning, treating each tabular column as a distinct modality. A naive approach that applies a pairwise contrastive loss to tabular columns is not only prohibitively expensive as the number of columns increases, but as we demonstrate, it also fails to capture interactions between variables. Instead, we propose a novel method called ICE-T that learns each columnar embedding by contrasting it with aggregate embeddings of the complementary part of the table, thus capturing interactions and scaling linearly with the number of columns. Unlike existing tabular CRL methods, ICE-T allows for column-specific embeddings to be obtained independently of the rest of the table, enabling the inference of missing values and translation between columnar variables. We provide a comprehensive evaluation of ICE-T across diverse datasets, demonstrating that it generally surpasses the performance of the state-of-the-art alternatives.

# Introduction

Heterogeneous tabular datasets constitute an extremely important, yet often overlooked, data class, which remains to be challenging for application of deep neural networks (Shwartz-Ziv and Armon 2022). As with other data types the key is to find good quality data representations that facilitate the downstream tasks (Bengio, Courville, and Vincent 2013). Contrastive representation learning (CRL) offers a very straightforward way for learning such representations, without the need of associated data labels (Le-Khac, Healy, and Smeaton 2020; Ericsson et al. 2022).

Multimodal CRL Recently, there has been increasing attention on a type of CRL methods known as multimodal CRL. Multimodal CRL has traditionally been applied to data comprising multiple distinct modalities, such as image-text or audio-video pairs, from which it derives its name (Deldari et al. 2022; Zong, Mac Aodha, and Hospedales 2023). Multimodal CRL offers a unique way of learning modalityspecific embeddings that are coordinated with embeddings from other modalities while allowing inputs of one modality to be embedded independently of the others (Guo, Wang, and Wang 2019). Therefore, embeddings can be obtained even in the absence of some modalities and can be used to infer the values of the missing ones—a task often referred to as modality translation (Kaur, Pannu, and Malhi 2021). The idea of adopting these methods for learning representations of heterogeneous tabular data is thus very appealing.

![](images/5077d09324aa39a2a32d78b48e43797a0e04e771a234c7f243b617b61bdb534b.jpg)  
Figure 1: A schematic representation of ICE-T. Tabular entries are passed through variable-specific encoders $f ^ { ( m ) }$ to produce intermediate latent representations $\mathbf { h } ^ { ( m ) }$ . For each variable $m$ we compute the associated anchor $\mu ^ { ( \backslash m ) }$ as the average of the representations of the remaining variables. These resulting vectors are then projected via a shared neural subnetwork $g$ to produce the final embeddings. The cosine similarities of these embeddings are used to compute the contrastive loss for each variable.

Motivation Intuitively, heterogeneous tabular data can be treated as multimodal data, where each variable (tabular column) is considered a single modality. The central idea is to learn variable-specific mappings that project inputs into a common latent space so that the embeddings of inputs are similar if they belong to the same instance (tabular row) while dissimilar otherwise. A naive approach would involve computing similarities between all pairs of variable-specific embeddings. However, such a pairwise contrasting scales quadratically with the number of variables and becomes prohibitively costly as the number of variables increases. Moreover, pairwise contrastive learning fails to capture interactions between variables, which can corrupt the resulting embeddings (e.g, dataset depicted in Figure 2). This situation is common in tabular data, which typically contain interacting variables, and can render pairwise embedding approaches inapplicable (Borisov et al. 2022).

Proposed method To address this problem, we propose a novel CRL approach called Interactions-aware Crosscolumn Contrastive Embedding for heterogeneous Tabular datasets (ICE-T) (Figure 1). ICE-T contrasts columnspecific embeddings with the embedded aggregates of the remaining columns in linear time and allows to account for interactions among variables, while preserving the advantages of multimodal CRL. ICE-T is a simple, versatile, and data-agnostic approach specifically designed for heterogeneous tabular data but can be easily applied to other domains. Overall, ICE-T offers superior or generally competitive performance compared to other methods.

Contributions (1) We offer a simple, easily reproducible example that illustrates the failure of multimodal CRL methods to capture interactions among variables (2) We introduce a novel method called ICE-T that addresses this limitation (3) We provide a comprehensive experimental comparison between ICE-T, five CRL methods and shallow learning benchmarks across a range of datasets, including frequently overlooked image/text–tabular data; measuring performance in four tasks (i) cross-modal translation, (ii) clustering, (iii) supervised learning and (iv) transfer learning.

# Related Work

Previous efforts to apply CRL to heterogeneous tabular data utilized joint CRL, where tabular rows undergo random transformations to produce their perturbed analogs, and the latent representations are learned by contrasting the original data against their perturbed analogs. This approach is best exemplified by Scarf (Bahri et al. 2021) and SubTab (Ucar, Hajiramezanali, and Edwards 2021). In Scarf, rows are perturbed by replacing inputs from a random subset of variables by values drawn from the respective variable marginals. In SubTab rows are perturbed by randomly dividing variables to create overlapping subsets. In addition to contrastive embedding loss, the authors of SubTab propose using additional distance loss and reconstruction loss.

![](images/14383def0ce4c6f247827fc7152bb795c7ff34a71d0cc6f1a9a89fa82e35fba3.jpg)  
Figure 2: The XOR blobs can form a heterogeneous tabular dataset with three variables: $x , y$ and $c$ , where each variable depends on the interaction between the other two. As we demonstrate, this interaction poses a challenging problem for multimodal CRL methods.

Unlike these methods, we propose to borrow frameworks established in the domain of multimodal representation learning and to approach tabular data as multimodal datasets, where each tabular column is treated as a single modality. From this perspective we identified three methods that are closely related to our line of research:

CLIP (Radford et al. 2021) produces text and image embeddings that are similar if the input text and image are related, and dissimilar otherwise. It consists of modalityspecific encoders, followed by linear projections into a common latent space, which are trained under the InfoNCE loss (Oord, Li, and Vinyals 2018), using cosine similarity, across a very large collection of image-text pairs. While the term “CLIP” usually refers to a specific model, it may denote a CRL method that trains modality-specific mappings only through pairwise cosine similarity across modalities (Table 1). Under this generalization, CLIP provides a versatile approach that can be applied to any type of modalities, as exemplified by (Alayrac et al. 2020; Wang et al. 2021; Guzhov et al. 2022).

GMC (Poklukar et al. 2022) was originally proposed for multimodal time-series data where modality-specific mappings are trained in coordination with joint representation learning. It consists of modality-specific encoders followed by deep neural projection heads to map inputs into a common latent space. Additionally, GMC projects inputs into the same latent space jointly through a designated neural subnetwork. The objective of GMC is to maximize the cosine similarity between the modality-specific embeddings and the joint embedding belonging to the same instance, while minimizing the similarity among embeddings belonging to different instances. The minimized term includes pairwise similarities between modality-specific embeddings, the similarity between joint and modality-specific embeddings, and the similarity between pairs of joint embeddings.

Table 1: Unified comparison of contrastive loss functions used by state-of-the-art methods vs. ICE-T (ours); $s$ indicates cosine similarity, $\tau$ is learnable parameter $\mathbf { z } ^ { ( 1 : M ) }$ indicates joint instance embedding; $\mathbf { z } ^ { ( \mathcal { A } ) }$ and $\mathbf { z } ^ { ( B ) }$ indicate partial instance embeddings; $\delta$ is a hyperparameter; $C _ { i } ^ { \prime }$ indicates the centroid that is nearest to the $i$ -th fused multimodal feature and $C _ { k }$ is the $k$ -th centroid; $g$ is a neural function.   

<html><body><table><tr><td>Method</td><td>Batch loss</td></tr><tr><td>Scarf</td><td>exp (s(z: 1: M exp(s(z</td></tr><tr><td>SubTab</td><td>exp(s(z(A) L=ΣBΣA,B [-10g1jiexp(</td></tr><tr><td>CLIP</td><td>exp sz C=∑∑M∑mtm-10g ∑exp(</td></tr><tr><td>GMC</td><td>)/τ)+exp(s(z(1:M）,z(1:M))/τ) L=∑∑M-l0g∑jtiexp(m M1og</td></tr><tr><td>MCN</td><td>C=∑[∑m∑m+m-l0g</td></tr><tr><td>ICE-T</td><td>L=∑ M exp exp log</td></tr></table></body></html>

MCN (Chen et al. 2021) combines multimodal CRL with latent space clustering. It encodes inputs via modalityspecific encoders and associated linear projections. Subsequently, MCN calculates joint representations by aggregating modality-specific embeddings together via an arithmetic mean. These joint embeddings, referred to by the authors as fused multimodal features, are subjected to online $\mathbf { k }$ -means clustering (Cohen-Addad et al. 2021). Euclidean distance between the features and a set of $k$ centroids is computed and each feature is assigned the nearest centroid. In addition to maximizing pairwise similarity among the modalityspecific embeddings, MCN aims to maximize the similarity between these embeddings and the centroids that are nearest to the respective multimodal features; the authors also add the reconstruction loss to the overall loss of the model.

# Interactions-aware Cross-column Contrastive Embedding

# Preliminaries

Let $\mathbf { x } _ { i }$ denote the $i$ -th data instance, comprising $M$ inputs from different variables: $\mathbf { x } _ { i } ~ = ~ ( \mathbf { x } _ { i } ^ { ( m ) } ) _ { m = 1 } ^ { \bar { M } }$ . Assume that each input comes from a distinct variable-specific input space $\mathcal { X } ^ { ( m ) }$ : $\mathbf { x } ^ { ( m ) } \in \mathcal { X } ^ { ( m ) }$ . Furthermore, consider $\mathbf { x } \ \sim \ p ( \mathbf { x } )$ and $\mathbf { x } ^ { ( m ) } \sim p ( \mathbf { x } ^ { ( m ) } )$ , where $p ( \mathbf { x } )$ denotes the joint data distribution, and $p ( \mathbf { x } ^ { ( m ) } )$ denotes the marginal distribution of given variable $m$ ; and let $p ( \mathbf { x } ^ { ( m ) } | \mathbf { x } ^ { ( \backslash m ) } )$ denote the conditional probability of $\mathbf { x } ^ { ( m ) }$ given the inputs $\mathbf { x } ^ { ( \backslash m ) } = ( \mathbf { x } ^ { ( n ) } ) _ { n \neq m } ^ { M }$ .

# Rationale

The central idea of ICE-T is to take intermediate hidden representations $\mathbf { h } ^ { ( m ) }$ obtained by variable-specific encoders $f ^ { ( m ) }$ and fuse the intermediate representations of the remaining variables into a single vector $\mu ^ { ( \backslash m ) }$ , which we refer to as the anchor. The anchors are computed via the arithmetic mean: $\begin{array} { r } { \mu ^ { ( \setminus m ) } = 1 / ( M - 1 ) \sum _ { n \neq m } \mathbf { \bar { h } } ^ { ( n ) } } \end{array}$ , where $M$ is the total number of variables. Integrating by averaging, instead of learning a joint representation via a fixed architecture (i) encourages additive embedding representations and (ii) makes the method robust against missing inputs and hence more versatile.

Algorithm 1: ICE-T – batch loss computation.   

<html><body><table><tr><td>Inputs: ..，x {Data batch} {Mappings} {Projection,Loss function} {x(1) {f(1),..., f(M)} g,l</td></tr><tr><td>£↑0 {Initialize sum} for m=1 to M do h(m)←f(m)(x(m)) {Intermediate representations}</td></tr><tr><td>∑↑∑+h(m) {Update sum} end for</td></tr><tr><td>L←0 {Initialize loss} for m=1 to M do μ(\m)←(∑-h(m))/(M-1) {Calculate anchor}</td></tr><tr><td>{Apply g} 21 ←g(h(m)) 22←g(μ(\m))</td></tr></table></body></html>

The two hidden vectors $\mathbf { h } ^ { ( m ) }$ and $\mu ^ { ( \backslash m ) }$ are passed through the identical neural subnetwork $g$ to obtain final latent representations ${ \mathbf z } ^ { ( m ) } = g ( { \mathbf h } ^ { ( m ) } )$ and $\mathbf { \bar { z } } ^ { ( \backslash m ) } = g ( \mu ^ { ( \backslash m ) } )$ , which are then compared by cosine similarity $s$ . We will use $s ^ { ( m ) } ( i , j )$ to denote similarity between the pair of latent vectors belonging to $i$ -th and $j$ -th instance: $s ^ { ( m ) } ( i , j ) : =$ $s ( g ( \mathbf { h } _ { i } ^ { ( m ) } ) , g ( \boldsymbol { \mu } _ { j } ^ { ( \backslash m ) } ) )$ . The aim is to maximize the similarity between the embedding vectors and the matching anchors $s ^ { ( m ) } ( i , i )$ , while minimizing the non-matching ones s(m)(i, j).

Table 2: ICE-T provides variable-specific $\mathbf { z } ^ { ( m ) }$ , joint instance $\mathbf { z } ^ { ( 1 : M ) }$ and also partial instance embeddings $\mathbf { z } ^ { ( \mathcal { A } ) }$ for any subset $\mathcal { A }$ of input variables generically, whereas some methods allow post hoc embeddings aggregation (”!”).   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="2">Embedding form</td></tr><tr><td>z(m)</td><td>z(1:M) z(A)</td></tr><tr><td>Scarf</td><td>√</td><td></td></tr><tr><td>SubTab</td><td>√</td><td></td></tr><tr><td>CLIP</td><td>√ ！</td><td>！</td></tr><tr><td>GMC</td><td>√ √</td><td>！</td></tr><tr><td>MCN</td><td>√ √</td><td>！</td></tr><tr><td>ICE-T</td><td>√ √</td><td>√</td></tr></table></body></html>

For this purpose, we adopt the InfoNCE loss (Oord, Li, and Vinyals 2018), so the loss incurred from the $i$ -th instance on variable $m$ is given by:

$$
l _ { i } ^ { ( m ) } = - \log \frac { \exp ( s ^ { ( m ) } ( i , i ) / \tau ) } { \sum _ { j } \exp ( s ^ { ( m ) } ( i , j ) / \tau ) }
$$

Here $\tau$ is the “temperature”, a trainable parameter controlling the sensitivity of the loss across the range of similarity values. The total batch loss is then the sum of losses incurred across all instances and modalities: $\begin{array} { r } { L = \sum _ { ( i , m ) } l _ { i } ^ { ( m ) } } \end{array}$ (cf. Algorithm 1).

# Probabilistic Interpretation

We seek variable-specific neural mappings $f ^ { ( m ) } : \mathcal { X } ^ { ( m ) } $ $\mathbb { R } ^ { q }$ and an additional neural mapping $g : \mathbb { R } ^ { q }  \mathbb { R } ^ { d }$ , such that for any $\mathbf { x } _ { i } ^ { ( m ) } \in \mathcal { X } ^ { ( m ) }$ , for $\forall m \in \{ 1 , \dots M \}$ , produce embeddings: $\mathbf { z } _ { i } ^ { ( m ) } = g ( f ^ { ( m ) } ( \mathbf { x } _ { i } ^ { ( m ) } ) )$ ; such that

$$
p ( \mathbf { x } ^ { ( m ) } | \mathbf { x } ^ { ( \backslash m ) } ) \propto s ( \mathbf { z } ^ { ( m ) } , \mathbf { z } ^ { ( \backslash m ) } )
$$

where $s : \mathbb { R } ^ { M \times d }  \mathbb { R }$ is a similarity function that quantifies the similarity between the variable-specific embedding $\mathbf { z } ^ { ( m ) }$ and the embedding of the remaining variables $\begin{array} { r } { \mathbf { z } ^ { ( \backslash m ) } = g \big ( 1 / ( M - 1 ) \sum _ { n \neq m } f ^ { ( n ) } ( \mathbf { z } ^ { ( n ) } ) \big ) } \end{array}$ . Equation 2 thus implies that we want to maximize the similarity between the embeddings of inputs that are likely to be associated and minimize it otherwise.

# Benefits

Embedding versatility Once trained, ICE-T can provide variable-specific embeddings $\mathbf { z } ^ { ( m ) }$ , joint complete instance embeddings $\mathbf { z } ^ { ( 1 : M ) }$ , and joint partial instance embeddings $\mathbf { z } ^ { ( \mathcal { A } ) }$ of a values from any subset $\mathcal { A } \in \mathcal { P } ( M )$ , where $\mathcal { P } ( M )$ is the power set of the variables it was trained on. This versatility is thanks to the use of arithmetic aggregation of the intermediate representation, instead of a fixed neural architecture (a used by, for example, in (Poklukar et al. 2022)).

This gives ICE-T an advantage over many other methods, which require post hoc aggregation of the variable-specific embeddings (cf. Table 2).

Modality translation The important advantage of most multimodal CRL methods, including ICE-T, is support for modality translation. In the context of heterogeneous tabular datasets this can be viewed as a form of missing values imputation, where the value of one variable is estimated based on the values of the other modalities of the given instance. This can be formulated as the assignment:

$$
\hat { \mathbf { x } } ^ { ( m ) } = \underset { \mathbf { x } ^ { ( m ) } \in \mathcal { X } ^ { ( m ) } } { \mathrm { a r g m a x } } p ( \mathbf { x } ^ { ( m ) } | \mathbf { x } ^ { ( \cal { A } ) } )
$$

where $\mathcal { A }$ is a subset of evidence variables so that $\mathcal { A } \in \mathcal { P } ( M )$ and $m \not \in { \mathcal { A } }$ . Assuming $p ( \mathbf { x } ^ { ( m ) } | \mathbf { x } ^ { ( \cal { A } ) } ) \propto s ( \mathbf { z } ^ { ( m ) } , \mathbf { z } ^ { ( \cal { A } ) } )$ , the above assignment converts to:

$$
\mathbf { x } ^ { ( m ) } = \underset { \mathbf { x } ^ { ( m ) } \in \mathcal { X } ^ { ( m ) } } { \mathrm { a r g m a x } } s \big ( \mathbf { z } ^ { ( m ) } , \mathbf { z } ^ { ( \mathcal { A } ) } \big )
$$

which can be solved provided the input space $\mathcal { X } ^ { ( m ) }$ is well defined and can be sampled efficiently.

Linear scaling Methods that rely on pairwise contrasting do not scale well with the large number of variables due to the quadratic increase in the number of pairwise contrastive losses. Unlike these, ICE-T scales linearly with the number of variables, as can be seen in Algorithm 1, where we can compute the sum of all embeddings in one linear pass over modalities and then calculate the loss in the second linear pass by contrasting the modality-specific embeddings against their respective anchors computed in a leave-one-out manner. This provides an important computational benefit to our method.

Data-agnostic ICE-T can be readily adapted to accommodate various data types, including image or text variables, by selecting appropriate neural architectures to implement mappings $f ^ { ( m ) }$ . This flexibility allows it to be applied to idiomatic tabular datasets containing images, text, or other modalities.

# Experimental Design

# Data

We used a collection of 44 real-world tabular datasets from the benchmark introduced in (Grinsztajn, Oyallon, and Varoquaux 2022)1. Moreover, to demonstrate applicability of ours and other methods on tables containing images and text, we used 2 additional image-tabular datasets and 2 texttabular datasets; thus, in total, we used 48 datasets (cf. Table 3). Each dataset includes one categorical (classification), or numerical (regression) target variable (response).

To minimize the effects of data preparation, we restricted ourselves only to minimal data processing involving ordinal encoding of the categorical and rescaling/log-transforming some numerical variables (cf. Supplementary Materials). We randomly split the data into training, validation and testing portions, allocating $10 \%$ of the data for validation and another $10 \%$ for testing. For each dataset we performed multiple experimental replicates (5 for tabular and 3 for img/texttabular data), each time using new random split and then averaged the obtained results.

Table 3: The 48 datasets used in this work and their respective number of samples and the number of variables by type.   

<html><body><table><tr><td rowspan="2">Name</td><td rowspan="2">Samples</td><td colspan="4">Variables</td></tr><tr><td>Num</td><td>Cat</td><td>Txt</td><td>Img</td></tr><tr><td></td><td>Classification</td><td></td><td></td><td></td><td></td></tr><tr><td>albert</td><td>58252</td><td>0</td><td>32</td><td>0</td><td>0</td></tr><tr><td>bank_marketing</td><td>10578</td><td>7</td><td>1</td><td>0</td><td>0</td></tr><tr><td>bioresponse</td><td>3434</td><td>419</td><td>1</td><td>0</td><td>0</td></tr><tr><td>california</td><td>20634</td><td>8</td><td>1</td><td>0</td><td>0</td></tr><tr><td>clothing</td><td>23486</td><td>2</td><td>5</td><td>2</td><td>0</td></tr><tr><td>compas</td><td>4966</td><td>3</td><td>9</td><td>0</td><td>0</td></tr><tr><td>covertype</td><td>566602</td><td>10</td><td>1</td><td>0</td><td>0</td></tr><tr><td>credit</td><td>16714</td><td>10</td><td>1</td><td>0</td><td>0</td></tr><tr><td>defaults</td><td>13272</td><td>20</td><td>2</td><td>0</td><td>0</td></tr><tr><td>diabetes</td><td>71090</td><td>7</td><td>1</td><td>0</td><td>0</td></tr><tr><td>electricity</td><td>38474</td><td>7</td><td>2</td><td>0</td><td>0</td></tr><tr><td>eye_movements</td><td>7608</td><td>20</td><td>4</td><td>0</td><td>0</td></tr><tr><td>heloc</td><td>10000</td><td>22</td><td>1</td><td>0</td><td>0</td></tr><tr><td>higgs</td><td>940160</td><td>24</td><td>1</td><td>0</td><td>0</td></tr><tr><td> jannis</td><td>57580</td><td>54</td><td>1</td><td>0</td><td>0</td></tr><tr><td>kickstarter</td><td>108128</td><td>3</td><td>4</td><td>3</td><td>0</td></tr><tr><td>miniboone</td><td>72998</td><td>50</td><td>1</td><td>0</td><td>0</td></tr><tr><td>road_safety</td><td>111762</td><td>14</td><td>17</td><td>0</td><td>0</td></tr><tr><td>skin_cancer</td><td>13354</td><td>1</td><td>4</td><td>0</td><td>1</td></tr><tr><td>streetview telescope</td><td>11054</td><td>2 10</td><td>1 1</td><td>0 0</td><td>1 0</td></tr><tr><td></td><td>13376 Regression</td><td></td><td></td><td></td><td></td></tr><tr><td colspan="6"></td></tr><tr><td>abalone</td><td>4177</td><td>8</td><td>1</td><td>0</td><td>0</td></tr><tr><td>ailerons</td><td>13750</td><td>34</td><td>0</td><td>0</td><td>0</td></tr><tr><td>airlines</td><td>1000000</td><td>4</td><td>2</td><td>0</td><td>0</td></tr><tr><td>allstate</td><td>188318</td><td>15</td><td>110</td><td>0</td><td>0</td></tr><tr><td>bike_sharing</td><td>17379</td><td>5</td><td>2</td><td>0</td><td>0</td></tr><tr><td>brazilian_houses</td><td>10692</td><td>8</td><td>4</td><td>0</td><td>0</td></tr><tr><td>cpu</td><td>8192</td><td>22</td><td>0</td><td>0</td><td>0</td></tr><tr><td>diamonds</td><td>53940</td><td>8</td><td>2</td><td>0</td><td>0</td></tr><tr><td>elevators</td><td>16599</td><td>17</td><td>0</td><td>0</td><td>0</td></tr><tr><td>house</td><td>22784</td><td>17</td><td>0</td><td>0</td><td>0</td></tr><tr><td>house_sales</td><td>21613</td><td>14</td><td>3</td><td>0</td><td>0</td></tr><tr><td>houses</td><td>20640</td><td>9</td><td>0</td><td>0</td><td>0</td></tr><tr><td>medical_charges</td><td>163065</td><td>4</td><td>0</td><td>0</td><td>0</td></tr><tr><td>mercedes</td><td>4209</td><td>1</td><td>359</td><td>0</td><td>0</td></tr><tr><td>miami_housing</td><td>13932</td><td>14</td><td>0</td><td>0</td><td>0</td></tr><tr><td>nyc_taxi</td><td>581835</td><td>4</td><td>13</td><td>0</td><td>0</td></tr><tr><td>pol</td><td>15000</td><td>27</td><td>0</td><td>0</td><td>0</td></tr><tr><td>seattle_crime</td><td>52031</td><td>1</td><td>4</td><td>0</td><td>0</td></tr><tr><td>sgemm</td><td>241600</td><td>4</td><td>6</td><td>0</td><td>0</td></tr><tr><td>soil</td><td>8641</td><td>4</td><td>1</td><td>0</td><td>0</td></tr><tr><td>sulfur</td><td>10081</td><td>7</td><td>0</td><td>0</td><td>0</td></tr><tr><td>superconduct</td><td>21263</td><td>80</td><td>0</td><td>0</td><td>0</td></tr><tr><td>supreme</td><td>4052</td><td>3</td><td>5</td><td>0</td><td>0</td></tr><tr><td>topo</td><td>8885</td><td>253</td><td>3</td><td>0</td><td>0</td></tr><tr><td>ukair</td><td>394299</td><td>3</td><td>4</td><td>0</td><td>0</td></tr><tr><td>wine_quality</td><td>6497</td><td>12</td><td>0</td><td>0</td><td>0</td></tr><tr><td>yprop</td><td>8885</td><td>43</td><td>0</td><td>0</td><td>0</td></tr></table></body></html>

# Models and Training

We compared ICE-T against the five state-of-the-art CRL methods described in the previous section: (i) Scarf (ii) SubTab with contrastive and distance loss (iii) CLIP (iv) GMC and (v) MCN with contrastive and clustering loss. Note, since CLIP and MCN scale quadratically with the number of variables, we decided to exclude them from experiments on datasets with more than 25 variables, which would otherwise result in excessive computation runtimes.

For each method we trained multiple models using different configurations of hyperparameters controlling the model size and its training. Models were trained on the training sets of the data for up to 100 epochs using early stopping with patience set to 5 epochs and validation loss as the criterion.

# Evaluation

Once trained, the models were applied to the training, validation and testing portion of the data to produce respective embeddings. We then evaluated the quality of the embedding vectors with respect to three common downstream tasks (i) imputation which we approach as cross-modal translation, (ii) clustering, and (iii) supervised learning – performed across test sets. Finally, we evaluated models in the role of pre-trained encoders, i.e. with respect to their support for (iv) transfer-learning. Note, for each method we report the best model performance as achieved in the given task.

Imputation (modality translation) The task aims to estimate values of one variable (query) in the dataset given the value of the remaining variables (evidence). We iterated over test set data columns, permuting and subsequently estimating the values of a single “query” column, by selecting the value whose embedding maximized the similarities to the embeddings of the remaining variables. In the case of ICE-T, the estimate was done as described by Equation 4, whereas for the remaining models, the estimate was made by maximizing sum of the pairwise similarities. The estimates were then compared to the original values to evaluate the quality of recovery (i.e., imputation). We used mean squared error (MSE) to evaluate the quality of imputation of the numerical variables and balanced accuracy score (Brodersen et al. 2010) for the categorical ones. To obtain a unified score across all variables, the results were first scaled to the $[ 0 , 1 ]$ interval, so that higher values indicate better performance, and then averaged into an imputation score. Note that Scarf and SubTab do not support modality translation and were excluded from this evaluation.

Clustering We evaluated how well the obtained embedding vectors support downstream data clustering. For each dataset, a regular $\mathbf { k }$ -means clustering model was trained on the training set embeddings. The quality of clusters was evaluated by the silhouette score (Shahapure and Nicholas 2020). The hyperparameter $k$ controlling the number of clusters was selected to maximize the silhouette score on the validation set. The test set silhouette score was used as the evaluation criterion. We also performed k-means clustering using the raw data, instead of embedding vectors, to serve as a shallow benchmark (not applied to txt/img-tabular data).

Supervised learning Similarly to clustering, we evaluated how well the obtained embeddings support downstream supervised learning. For each dataset, we trained a KNN classifier, or regressor, using the training set embeddings and the associated targets. Predictive performance was evaluated by the balanced accuracy score for classification tasks or MSE for regression tasks. Hyperparameter $k$ was selected to maximize predictive performance on validation set. The resulting test set performance was then used as the evaluation criterion. We performed KNN using the raw data as a shallow benchmark (not applied to txt/img-tabular data).

Transfer learning Pre-trained models were used as an encoders, on top of which we added neural prediction head, implemented as a single hidden layer, ReLU-activated MLP, forming a neural predictor. The predictor was trained on the training set for up to 100 epochs using early stopping with patience set to 5 epochs and validation loss as the criterion. Once trained, we evaluated the predictive performance of the resulting model on the test set. The predictions were evaluated by the balanced accuracy score for classification tasks and MSE for regression tasks. As an additional control, we also employed a vanilla MLP predictor with a single hidden layer, i.e., without any pre-trained components. For benchmarking, we used the prediction performance achieved by XGBoost (Chen and Guestrin 2016) with default parametrization (not applied to txt/img-tabular data).

Average relative score The task-specific performance of ICE-T and other methods, as achieved on a given dataset, were scaled to the [0, 1] interval so that the higher value of this relative score indicates better performance. To quantify the overall task-specific performance, the resulting values were subsequently averaged across datasets into an average relative score (cf. Supplementary Materials).

# Results Synthetic Data Experiment

Consider the dataset illustrated in Figure 2, referred to as Gaussian XOR “blobs”, or noisy XOR (Duch 2007). It is evident that the conditional class probability cannot be factorized into a product of conditionals: $p ( c | x , y ) \ \neq$ $p ( c | x ) p ( c | y )$ . Similar non-factorizability holds for the two coordinates: $p ( x | c , y )$ and $p ( \boldsymbol { y } | \boldsymbol { c } , \boldsymbol { x } )$ . Hence, we hypothesize that learning mappings $f$ by pairwise similarity comparison across the three variables will result in poor embeddings, which will be manifested by a failure to predict any of the three variables from the embeddings of the remaining two.

To validate this hypothesis, we generated training and testing sets consisting of 1,000 and 100 noisy XOR points, respectively; and tested whether the embeddings obtained by ICE-T support the imputation task (modality translation) better than the embeddings from other multimodal contrastive methods. The obtained results show that ICE-T indeed greatly outperforms the other methods, confirming our hypothesis (cf. Table 4).

<html><body><table><tr><td>Method</td><td>ACC ↑ C</td><td>MSE↓ X</td><td>y</td><td>Imputation score</td></tr><tr><td>CLIP</td><td>0.750</td><td>0.271</td><td>0.106</td><td>0.562</td></tr><tr><td>GMC</td><td>0.683</td><td>0.206</td><td>0.122</td><td>0.565</td></tr><tr><td>MCN</td><td>0.733</td><td>0.179</td><td>0.148</td><td>0.589</td></tr><tr><td>ICE-T</td><td>0.900</td><td>0.008</td><td>0.006</td><td>0.982</td></tr></table></body></html>

Table 4: The performance of ICE-T compared to other multimodal CRL methods in the synthetic data experiment (cf. Figure 2). The embedding vectors produced by ICE-T allow to predict each of the three variables using the remaining two better than those from other methods, demonstrating its ability to capture cross-columnar interactions in tabular data.

<html><body><table><tr><td></td><td>Imputation</td><td>Clustering</td><td>Supervised learning</td><td>Transfer learning</td></tr><tr><td>Scarf</td><td></td><td>0.674</td><td>0.434</td><td>0.474</td></tr><tr><td>SubTab</td><td></td><td>0.344</td><td>0.522</td><td>0.332</td></tr><tr><td>CLIP</td><td>0.561</td><td>0.345</td><td>0.571</td><td>0.544</td></tr><tr><td>GMC</td><td>0.536</td><td>0.553</td><td>0.495</td><td>0.544</td></tr><tr><td>MCN</td><td>0.378</td><td>0.306</td><td>0.636</td><td>0.533</td></tr><tr><td>ICE-T</td><td>0.604</td><td>0.617</td><td>0.687</td><td>0.824</td></tr></table></body></html>

Table 5: The average relative score (higher is better, cf. Evaluation) in the four tasks as achieved across the 48 real-world datasets. ICE-T gives the best performance in imputation, supervised and transfer learning; and is second in clustering. Note, Scarf and SubTab do not support modality transfer and could not be evaluated for imputation.

# Real-world Data Experiments

For compactness, the results obtained across the 48 realworld datasets were conveyed as win-loss matrices (cf. Figure 3) and as a boxplots depicting the relative scores (cf. Figure 4) (the raw numbers are provided in Supplementary Tables 2–5). The overall performance across the datasets was quantified by average relative score (cf. Table 5). Based on the obtained results we summarize our findings as follows:

• Compared to other methods, embedding vectors produced by ICE-T give better support for imputation and supervised learning, but not clustering, for which Scarf provides a better alternative. This is likely because Scarf, unlike other methods, learns to exert embedding jointly by contrasting entire data instances, which may result in better global alignment of the embedding vectors.

• ICE-T serves in transfer learning better than other methods and transferring pre-trained ICE-T to neural predictor improves performance beyond that of XGBoost (benchmark). Interestingly, in similar experiments performed on independent tabular benchmark, Scarf surpassed XGBoost on approx. $6 0 \%$ , and the MLP used as a negative control on $4 5 \%$ of datasets (Bahri et al. 2021), strongly corroborating our results (cf. Figure 3, right).

0.0 0.2 0.4 0.6 0.8 1.0 Imputation Clustering Supervised learning Transfer learning 0.540.680.41 ? 095050760 ? 0.0300.00 0455530   
CLIP o X 公 0.550.69 0.560.350.480.430.19 -0.46 0.650.46 B 0.930.27 0.490.31 0.65 0.31 Be 5 -0.840.54 0.570.480.380.35 8   
GMC CaD 0.80.140565350.65 cup Tab 0.70550330.04 bTab 0305 -0.320.35 0.38 C   
MCN ℃ 0.520.650.52 0.670.51 0.460.25 G 0.880.160.350.350.35 0.22 GN 0.850.68 0.62 0.510.49 0.49 GMC 0.480.680.57 0.620.510.54 0.27   
CE 0.590.540.62 zCZ 0.910.480.690.700.500.78 zCZ 0.80 0.65 0.650.59 0.58 0.51 MCN 0.680.85 0.810.88 0.780.750.73 CE ICE

![](images/bc2f97742d3ae31d815f95ceb7f486af550d6566561e2bb5f39c8a195f900d54.jpg)  
Figure 3: Heatmaps of win-loss matrices, where each value indicates the fraction of datasets where the method in the given row surpassed the method in the given column. Note that ICE-T (bottom row) surpassed other methods on the majority of the datasets (values $> 0 . 5$ ) in support of imputation, supervised and transfer learning; and performed reasonably well in clustering.   
Figure 4: For each task, we scaled the results obtained on the given dataset to [0, 1] interval, so that 1 is assigned to the best performing method, and 0 to the least performing method. The resulting quantities are depicted as segments in a stacked bar plot. A larger segment indicates better performance relative to other methods. As can be seen, ICE-T either outperforms other methods (the largest segment in the given bar), or generally provides good performance, across the majority of the datasets.

# Discussion

We propose to approach heterogeneous tabular data as a type of multimodal data, where each variable, i.e., each tabular column, is treated as single modality. Multimodal CRL on tabular data has the potential to provide important advantages over existing methods.

However, unlike multimodal data, most tabular data are characterized by a large number of variables. This significantly penalizes methods using pairwise variable contrasting, which may become prohibitively expensive with growing number of variables. Moreover, in tabular data, interactions among variables are likely to occur.

This motivated ICE-T, whose key insight is that its loss contrasts each modality with the embedding of a mean of intermediate representation of other modalities, allowing it to perform in linear time and, as we demonstrated, to capture variable interactions. Based on our results, we conclude that ICE-T provides better support in most downstream tasks.

Supplementary materials and the code are available at https://github.com/tomastokar/ICET