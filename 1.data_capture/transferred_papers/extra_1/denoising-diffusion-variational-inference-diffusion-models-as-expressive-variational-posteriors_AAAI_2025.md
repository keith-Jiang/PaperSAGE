# Denoising Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors

Wasu Top Piriyakulkij\*1, Yingheng Wang\*1, Volodymyr Kuleshov1,2

1Department of Computer Science, Cornell University 2The Jacobs Technion-Cornell Institute, Cornell Tech wp237, yw2349, kuleshov @cornell.edu

# Abstract

We propose denoising diffusion variational inference (DDVI), a black-box variational inference algorithm for latent variable models which relies on diffusion models as flexible approximate posteriors. Specifically, our method introduces an expressive class of diffusion-based variational posteriors that perform iterative refinement in latent space; we train these posteriors with a novel regularized evidence lower bound (ELBO) on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. We find that DDVI improves inference and learning in deep latent variable models across common benchmarks as well as on a motivating task in biology— inferring latent ancestry from human genomes—where it outperforms strong baselines on the Thousand Genomes dataset.

# 1 Introduction

We are interested in amortized black-box variational inference problems of the form

$$
\begin{array} { r l r } { \log \ p _ { \theta } ( \mathbf { x } ) \geq \underset { \phi } { \operatorname* { m a x } } } & { \mathbb { E } _ { q _ { \phi } ( \mathbf { z } | \mathbf { x } ) } \left[ \log \ p _ { \theta } ( \mathbf { x } , \mathbf { z } ) - \log \ q _ { \phi } ( \mathbf { z } | \mathbf { x } ) \right] } & \\ { : = \underset { \phi } { \operatorname* { m a x } } } & { \mathrm { E L B O } ( \mathbf { x } , \theta , \phi ) , } & { ( 1 ) } \end{array}
$$

in which we approximate the marginal likelihood log $p _ { \pmb { \theta } } ( \mathbf { x } )$ of a latent variable model $p _ { \pmb { \theta } } ( \mathbf { x } , \mathbf { z } )$ with an evidence lower bound $\mathrm { E L B O } ( \mathbf { x } , \theta , \phi )$ that is a function of an approximate variational posterior $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ . We assume that $p _ { \theta }$ factorizes as $p _ { \pmb { \theta } } ( \mathbf { x } | \mathbf { z } ) \bar { p } _ { \pmb { \theta } } ( \mathbf { z } )$ and admits efficient sampling: examples of such $p _ { \theta }$ include Bayesian networks, topic models (Blei, $\mathrm { N g }$ , and Jordan 2003), variational autoencoders (VAEs), and broad classes of $p _ { \theta }$ defined via modern probabilistic programming frameworks (Gordon et al. 2014).

Maximizing $\mathrm { E L B O } ( \mathbf { x } , \theta , \phi )$ over $\phi$ yields a variational posterior $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ that approximates $p _ { \pmb { \theta } } ( \mathbf { z } | \mathbf { x } )$ as well as a tight bound on log $p _ { \pmb { \theta } } ( \mathbf { x } )$ that serves as a learning objective for $p _ { \theta }$ . The approximation gap log $p _ { \pmb { \theta } } ( \mathbf { x } ) \ - \ \mathrm { m a x } _ { \phi } \mathrm { E L B O } ( \mathbf { x } , \pmb { \theta } , \phi )$ equals precisely $\begin{array} { r } { \operatorname* { m i n } _ { \phi } \mathrm { K L } \big ( q _ { \phi } ( \mathbf { z } | \mathbf { x } ) | | p _ { \theta } ( \mathbf { z } | \mathbf { x } ) \big ) } \end{array}$ , which motivates the design of expressive classes of posteriors $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ that reduce this gap. Recent efforts leverage modern generative models— including normalizing flows (Rezende and Mohamed 2015; Kingma et al. 2016) and generative adversarial networks (Goodfellow et al. 2014; Makhzani et al. 2015)—as expressive model families for $q _ { \phi }$ that tighten the ELBO.

This work seeks to further improve variational inference via expressive posteriors based on diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2020). Diffusion methods have become the de-facto standard for high-quality image synthesis (Rombach et al. 2022; Gokaslan et al. 2024). Here, we use diffusion in latent space to parameterize $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ . We train this distribution with a denoising diffusion-like objective that does not involve adversarial training (Makhzani et al. 2015) or constrained invertible normalizing flow architectures (Kingma et al. 2016). Samples from $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ are obtained via iterative refinement of $\mathbf { z }$ , starting from a Gaussian distribution, and gradually forming one that is multi-modal and complex.

Our work expands upon existing diffusion-based approximate inference methods (Berner, Richter, and Ullrich 2022; Zhang and Chen 2021; Vargas, Grathwohl, and Doucet 2023; Zhang et al. 2023; Richter, Berner, and Liu 2023; Sendera et al. 2024; Akhound-Sadegh et al. 2024) that focus on the task of drawing samples from unnormalized distributions $\tilde { p } ( { \mathbf z } )$ and estimating the partition function $\begin{array} { r } { Z = \int _ { \mathbf { z } } \tilde { p } ( \mathbf { z } ) d \mathbf { z } } \end{array}$ . While these methods are applicable in our setting—we set the unnormalized $\tilde { p } ( { \mathbf z } )$ to $p _ { \pmb { \theta } } ( \mathbf { x } , \mathbf { z } )$ such that $Z = p _ { \pmb { \theta } } ( \mathbf { x } )$ —they do not make use of characteristics of $p _ { \theta }$ that are common in many types of models (VAEs, Bayes networks, etc.), namely the factorization $p _ { \pmb { \theta } } ( \mathbf { x } | \mathbf { z } ) p _ { \pmb { \theta } } ( \mathbf { z } )$ and efficient sampling. We find that leveraging these properties yields simpler algorithms that avoid backpropagating through a sampling process, and that are fast enough to perform learning in addition to inference.

Specifically, we propose denoising diffusion variational inference (DDVI), an approximate inference algorithm defined by a class of approximate posterior distribution based on diffusion and a learning objective inspired by the wakesleep algorithm (Hinton et al. 1995) that implements regularized variational inference. We also derive extensions of our method to semi-supervised learning and clustering.

Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks.

We evaluate DDVI on synthetic benchmarks and on a real problem in biological data analysis—inferring human ancestry from genetic data. Our method outperforms strong baselines on the Thousand Genomes dataset (Siva 2008) and learns a low-dimensional latent space that preserves biologically meaningful structure (Haghverdi, Buettner, and Theis 2015).

Contributions. In summary, this work introduces denoising diffusion variational inference, an approximate inference algorithm defined by two components: a class of approximate posteriors $q ( \mathbf { z } | \mathbf { x } )$ parameterized by diffusion, and a lower bound on the marginal likelihood inspired by wake-sleep. We complement DDVI with extensions to semisupervised learning and clustering. Our method is especially suited for probabilistic programming, representation learning, and dimensionality reduction, where it outperforms alternative methods based on normalizing flows and adversarial training.

# 2 Background

Deep Latent Variable Models Latent variable models (LVMs) $p _ { \pmb { \theta } } ( \mathbf { x } , \mathbf { z } )$ are usually fit by optimizing the evidence lower bound (ELBO)

$$
\begin{array} { r } { \log p _ { \pmb \theta } ( \mathbf x ) \geq \mathbb { E } _ { q _ { \pmb \phi } ( \mathbf { z } | \mathbf { x } ) } [ \log p _ { \pmb \theta } ( \mathbf { x } | \mathbf { z } ) ] - \mathrm { K L } ( q _ { \phi } ( \mathbf { z } | \mathbf { x } ) | | p _ { \pmb \theta } ( \mathbf { z } ) ) , } \end{array}
$$

which serves as a tractable surrogate for the marginal loglikelihood (MLL). The gap between the MLL and the ELBO equals precisely $\mathrm { K L } ( q _ { \phi } ( \mathbf { \bar { z } } | \mathbf { x } ) | | p _ { \theta } ( \mathbf { z } | \mathbf { x } ) )$ —thus, a more expressive $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ may better fit the true posterior and induce a tighter ELBO (Kingma and Welling 2013).

Expressive variational posteriors can be formed by choosing more expressive model families—including auxiliary variable methods (Maaløe et al. 2016), MCMC-based methods (Salimans, Kingma, and Welling 2015), normalizing flows (Rezende and Mohamed 2015)—or improved learning objectives—e.g., adversarial or sample-based losses (Makhzani et al. 2015; Zhao, Song, and Ermon 2017; Si, Bishop, and Kuleshov 2022; Si et al. 2023).

The wake-sleep algorithm (Hinton et al. 1995) optimizes an alternative objective

$$
\begin{array} { r } { \mathbb { E } _ { q _ { \phi } ( \mathbf { z } | \mathbf { x } ) } [ \log p _ { \theta } ( \mathbf { x } | \mathbf { z } ) ] - \mathrm { K L } ( p _ { \theta } ( \mathbf { z } | \mathbf { x } ) | | q _ { \phi } ( \mathbf { z } | \mathbf { x } ) ) , } \end{array}
$$

in which the KL divergence term is reversed. The learning procedure for wake-sleep involves alternating between ”wake” phases where the recognition model is updated and ”sleep” phases where the generative model is refined.

Denoising Diffusion Models A diffusion model is defined via a user-specified noising process $q$ that maps data $\mathbf { x } _ { 0 }$ into a sequence of $T$ variables ${ \bf y } _ { 1 : T } = { \bf y } _ { 1 } , . . . , { \bf y } _ { T }$ that represent increasing levels of corruption to $\mathbf { x } _ { 0 }$ . We obtain $\mathbf { y } _ { 1 : T }$ by applying a Markov chain $\begin{array} { r } { q ( \mathbf { y } _ { 1 : T } | \mathbf { x } _ { 0 } ) = \prod _ { t = 1 } ^ { T } q ( \mathbf { y } _ { t } | \mathbf { y } _ { t - 1 } ) } \end{array}$ , where we define $\mathbf { y } _ { 0 } = \mathbf { x } _ { 0 }$ for convenience. When $\mathbf { x } _ { 0 }$ is a continuous vector, a standard choice of transition kernel is $q ( \mathbf { x } _ { t } \mid \mathbf { x } _ { t - 1 } ) = \mathcal { N } ( \mathbf { y } _ { t } ; \sqrt { \alpha _ { t } } \mathbf { y } _ { t - 1 } , \sqrt { 1 - \alpha _ { t } } \mathbf { I } )$ , which is a Gaussian centered around a copy of $\mathbf { y } _ { t - 1 }$ to which we added noise following a schedule $0 < \pmb { \alpha } _ { 1 } < \pmb { \alpha } _ { 2 } < . . . < \pmb { \alpha } _ { T } = 1$ .

A diffusion model can then be represented as a latent variable distribution $p ( \mathbf { x } _ { 0 } , \mathbf { y } _ { 1 : T } )$ that factorizes as $\begin{array} { r } { p ( \mathbf { x } _ { 0 } , \mathbf { y } _ { 1 : T } ) = p ( \mathbf { y } _ { T } ) \prod _ { t = 0 } ^ { T - 1 } p _ { \pmb { \theta } } ( \mathbf { y } _ { t } \mid \mathbf { y } _ { t + 1 } ) } \end{array}$ (again using $\mathbf { y } _ { 0 }$ as shorthand for $\mathbf { x } _ { \mathrm { 0 } }$ ). This model seeks to approximate the reverse of the forward diffusion $q$ and map noise ${ \bf y } _ { T }$ into data x0.

The true reverse of the process $q$ cannot be expressed in closed form; as such, we parameterize $p _ { \theta }$ with $\pmb \theta$ trained by maximizing the ELBO:

$$
\begin{array} { r l r } {  { \log p _ { \theta } ( \mathbf { x } _ { 0 } ) \geq \mathbb { E } _ { q } \Big [ \log p _ { \theta } ( \mathbf { x } _ { 0 } | \mathbf { x } _ { 1 } ) - \sum _ { t = 2 } ^ { T } \mathbf { K } \mathbf { L } ( q _ { t } | | p _ { t } ) \Big ] } } \\ & { } & { - \mathbf { K } \mathbf { L } ( q ( \mathbf { x } _ { T } | \mathbf { x } _ { 0 } ) | | p ( \mathbf { x } _ { T } ) ) } \end{array}
$$

where $q _ { t } , p _ { t }$ denote the distributions $q ( \mathbf { x } _ { t - 1 } | \mathbf { x } _ { t } , \mathbf { x } _ { 0 } )$ and $p _ { \pmb { \theta } } ( \mathbf { x } _ { t - 1 } | \mathbf { x } _ { t } )$ , respectively.

# 3 Variational Inference With Denoising Diffusion Models

We introduce denoising diffusion variational inference $( D D V I )$ , which improves variational inference with diffusion-based techniques.

The goal of DDVI is to fit a latent variable model $p _ { \pmb { \theta } } ( \mathbf { x } , \mathbf { z } )$ . We assume that $p _ { \theta }$ factorizes as $p _ { \pmb { \theta } } ( \mathbf { x } | \mathbf { z } ) p _ { \pmb { \theta } } ( \mathbf { z } )$ and admits efficient sampling: examples of such $p _ { \theta }$ include Bayesian networks and variational autoencoders (VAEs) (Kingma and Welling 2013).

Our approach is comprised of three components: 1. A modeling family of approximate posteriors $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ based on diffusion; 2. A learning objective formed by a regularized ELBO; 3. An optimization algorithm inspired by wake-sleep.

The $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ iteratively refines latents $\textbf { z }$ , starting from a Gaussian distribution. The learning objective trains $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ to reverse a used-specified forward diffusion process.

# 3.1 Modeling Family: Diffusion-Based Posteriors

DDVI performs variational inference using a family of approximate posteriors $\begin{array} { r } { q _ { \phi } ( \mathbf { z } | \mathbf { x } ) = \int _ { \mathbf { y } } q _ { \phi } ( \mathbf { z } | \mathbf { \bar { y } } , \mathbf { x } ) q _ { \phi } ( \mathbf { \bar { y } } | \mathbf { x } ) d \mathbf { \bar { y } } } \end{array}$ , which themselves contain latent variables $\mathbf { y } \in \mathcal { V }$ . The models $q _ { \phi } ( { \bf z } | { \bf y } , { \bf x } ) , q _ { \phi } ( { \bf y } | { \bf x } )$ must have tractable densities and support gradient-based optimization over $\phi$ .

We choose the latent $\begin{array} { r l r } { { \bf y } } & { { } = } & { \left( { \bf y } _ { 1 } , { \bf y } _ { 2 } , . . . , { \bf y } _ { T } \right) } \end{array}$ to be a vector of $T$ variables that represent progressively simplified versions of $\textbf { z }$ , with ${ \bf y } _ { T }$ corresponding to a simple distribution (e.g., a Gaussian). The model $\begin{array} { r l r } { q _ { \phi } ( \mathbf { y } , \mathbf { \bar { z } } | \mathbf { x } ) } & { = } & { q _ { \phi } ( \mathbf { z } | \mathbf { y } _ { 1 } , \mathbf { \bar { x } } ) \prod _ { t = 1 } ^ { T - 1 } q _ { \phi } ( \mathbf { y } _ { t } | \mathbf { y } _ { t + 1 } , \mathbf { x } ) } \end{array}$ transforms ${ \bf y } _ { T }$ into $\textbf { z }$ via iterative  refinement. To sample from $q _ { \phi }$ , we first sample $\mathbf { y } _ { T }$ —this is an easier task since we can define ${ \bf y } _ { T }$ to have a simple (e.g., Gaussian) distribution—and then by sampling from the denoising model $\begin{array} { r } { q _ { \phi } ( \mathbf { z } | \mathbf { y } _ { 1 } , \mathbf { x } ) \prod _ { t = 1 } ^ { T - 1 } q _ { \phi } ( \mathbf { y } _ { t } | \mathbf { \bar { y } } _ { t + 1 } , \mathbf { x } ) } \end{array}$ .

We define the relationship between y and $\textbf { z }$ via a forward diffusion process $r ( { \bf \dot { y } } | { \bf z } , { \bf x } ) = \dot { r } ( { \bf y } _ { 1 : T } | { \bf z } , { \bf x } ) =$ $\begin{array} { r } { r ( { \bf y } _ { 1 } | { \bf z } , { \bf x } ) \prod _ { t = 1 } ^ { T - 1 } r ( { \bf y } _ { t + 1 } | { \bf y } _ { t } , { \bf x } ) } \end{array}$ , which transforms $\textbf { z }$ —the latent whose intractable posterior we seek to approximate— into ${ \bf y } _ { T }$ , whose posterior is easier to model (possibly conditioned on $\mathbf { x }$ ). Examples of $r$ include Gaussian forward diffusion processes and discrete noising processes (Austin et al. 2021). The model $q _ { \phi }$ is trained to approximately reverse this forward diffusion process.

![](images/7d7d87480585b852315dfc466342f26df5709832af4dbb035c63254e59d032ff.jpg)  
Figure 1: Denoising diffusion variational inference in a VAE. Between the encoder and decoder, we have a diffusion model t map a simple distribution into a complex distribution over latents.

# 3.2 Learning Objective: A Markovian ELBO

The standard approach to fit auxiliary-variable generative models (Maaløe et al. 2016) is to apply the ELBO twice:

$$
\begin{array} { r l } & { \log p _ { \pmb \theta } ( \mathbf x ) \geq \log p _ { \pmb \theta } ( \mathbf x ) - \mathrm { K L } \big ( q _ { \pmb \phi } ( \mathbf z | \mathbf x ) \big | \big | p _ { \pmb \theta } ( \mathbf z | \mathbf x ) \big ) } \\ & { \qquad \geq \log p _ { \pmb \theta } ( \mathbf x ) - \mathrm { K L } \big ( q _ { \pmb \phi } ( \mathbf z | \mathbf x ) \big | \big | p _ { \pmb \theta } ( \mathbf z | \mathbf x ) \big ) } \\ & { \qquad - \mathbb { E } _ { q _ { \pmb \phi } ( \mathbf z | \mathbf x ) } \big [ \mathrm { K L } \big ( q _ { \pmb \phi } ( \mathbf y | \mathbf x , \mathbf z ) \big | \big | r \big ( \mathbf y | \mathbf x , \mathbf z \big ) \big ) \big ] } \\ & { \qquad = \mathbb { E } _ { q _ { \pmb \phi } ( \mathbf y , \mathbf z | \mathbf x ) } \big [ \log p _ { \pmb \theta } ( \mathbf x | \mathbf z ) \big ] } \\ & { \qquad - \mathrm { K L } \big ( q _ { \pmb \phi } ( \mathbf y , \mathbf z | \mathbf x ) \big | \big | r \big ( \mathbf y | \mathbf x , \mathbf z \big ) p ( \mathbf z ) \big ) } \end{array}
$$

In Equation (3), we applied the ELBO over $\textbf { z }$ , and in Equation (4) we applied the ELBO over the latent $\mathbf { y }$ of $q$ (see Appendix $\mathrm { ~ \bf ~ J ~ }$ for the derivation). Notice that the gap between the ELBO and $\log p _ { \pmb { \theta } } ( \mathbf { x } )$ is $\begin{array} { r } { \mathrm { K L } ( q _ { \phi } ( \mathbf { z } | \mathbf { x } ) | | p _ { \theta } ( \mathbf { z } | \mathbf { x } ) ) + } \end{array}$ $\mathbb { E } _ { q _ { \phi } ( \mathbf { z } | \mathbf { x } ) } [ \mathrm { K L } ( q _ { \phi } ( \mathbf { y } | \mathbf { x } , \mathbf { z } ) | | r ( \mathbf { y } | \mathbf { x } , \mathbf { z } ) ) ]$ . Thus, if we correctly match $q$ and $r$ , we will achieve a tight bound.

Analyzing the ELBO Optimizing Equation (5) requires tractably dealing with the prior regularization term $\bar { \mathcal { L } } _ { \mathrm { r e g } } ( \mathbf { x } , \theta , \phi ) : = - \mathrm { K } \bar { \mathrm { L } } \big ( q _ { \phi } ( \mathbf { y } , \mathbf { z } | \mathbf { x } ) | | r ( \mathbf { y } | \bar { \mathbf { x } } , \mathbf { z } ) p ( \mathbf { z } ) \big )$ , which we equivalently rewrite as:

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { r e g } } = \mathbb { E } _ { q _ { \phi } ( \mathbf { y } , \mathbf { z } | \mathbf { x } ) } [ \log ( r ( \mathbf { y } | \mathbf { x } , \mathbf { z } ) p ( \mathbf { z } ) ) ] + H ( q ) . } \end{array}
$$

We can expand the first term by leveraging the Markov structure of $r , q$ to rewrite $\mathcal { L } _ { \mathrm { r e g } }$ as the likelihood of samples from the reverse diffusion process $q$ under the forward process $r$ .

$$
\mathcal { L } _ { \mathrm { r e g } } = \sum _ { t = 1 } ^ { T } \mathbb { E } _ { q } [ \log ( r ( \mathbf { y } _ { t } | \mathbf { y } _ { t - 1 } , \mathbf { x } ) ] + \mathbb { E } _ { q } [ \log p ( \mathbf { z } ) ] + H ( q ) ,
$$

where $\mathbf { y } _ { 0 } : = \mathbf { z } $ . We refer to optimizing the Markovian ELBO as unregularized DDVI, the first instance of our method.

The noise process $r$ defines prior regularization terms for each $\mathbf { y } _ { t }$ . This provides extra supervision for learning $q$ in the form of trajectories from latents ${ \bf y } _ { T }$ to $\mathbf { y } _ { 1 }$ ; this extra supervision helps $q$ fit complex non-Gaussian posteriors.

The term $\begin{array} { r l r } { \bar { H ( q ) } } & { { } = } & { - \sum _ { t = 1 } ^ { T + 1 } \mathbb { E } _ { q } [ \log \bar { q } _ { \phi } ( \mathbf { y } _ { t - 1 } \vert \mathbf { y } _ { t } , \mathbf { x } ) ] } \end{array}$ denotes the entropy. For example, when each term $q _ { \phi } ( \mathbf { y } _ { t - 1 } | \mathbf { y } _ { t } , \mathbf { x } )$ is Gaussian, it is computed as

$$
H ( q ) = \sum _ { t = 1 } ^ { T + 1 } \mathbb { E } _ { q } \left[ \frac { d } { 2 } \left( 1 + \log ( 2 \pi ) \right) + \frac { 1 } { 2 } \log \left| \Sigma _ { \phi } ( \mathbf { y } _ { t } , \mathbf { x } ) \right| \right]
$$

where $d$ is the dimension of $\mathbf { y }$ and we use the notation ${ \bf y } _ { T + 1 } = { \bf x }$ . It is also common to leave the variance $\Sigma _ { \phi }$ fixed, in which case $H ( q )$ is a constant.

# 3.3 Refining the Objective: A Regularized ELBO

Notice that optimizing ${ \mathcal { L } } _ { \mathrm { r e g } }$ involves sampling from the approximate reverse process $\mathbf { \sigma } _ { q \phi } ( \mathbf { y } , \mathbf { z } | \mathbf { x } )$ to match the true reverse process $r ( \mathbf { y } | \mathbf { z } , \mathbf { x } )$ : this is the opposite of diffusion training, where we would sample from $r$ to fit $q$ . This type of on-policy learning of $q$ has been studied in the context of approximate inference (Zhang and Chen 2021); however, it requires backpropagating through $T$ samples, which may hamper training, and it optimizes a mode-covering divergence that may struggle to fit complex $p ( \mathbf { z } )$ .

Adding Wake-Sleep Regularization to the ELBO We propose to further improve the ELBO via off-policy diffusion-like training. Our new objective is the ELBO in Equation (5) augmented with a regularizer $\mathcal { L } _ { \mathrm { s l e e p } } ( \phi )$ .

$$
\begin{array} { r } { \log p _ { \theta } ( \mathbf { x } ) \geq \underbrace { { \mathbb { E } } _ { q _ { \phi } ( \mathbf { y } , \mathbf { z } | \mathbf { x } ) } [ \log p _ { \theta } ( \mathbf { x } | \mathbf { z } ) ] } _ { \mathrm { w a k e / r e c o n s . ~ t e r m ~ } \mathcal { L } _ { \mathrm { r e c } } ( \mathbf { x } , \theta , \phi ) } } \\ { \underbrace { - \mathrm { K L } ( q _ { \phi } ( \mathbf { y } , \mathbf { z } | \mathbf { x } ) ) | | r \big ( \mathbf { y } | \mathbf { x } , \mathbf { z } \big ) p ( \mathbf { z } ) \big ) } _ { \mathrm { p r i o r ~ r e g u l a r i z a t i o n ~ t e r m ~ } \mathcal { L } _ { \mathrm { r e g } } ( \mathbf { x } , \theta , \phi ) } } \\ { \underbrace { - { \mathbb { E } } _ { p _ { \theta } ( \mathbf { x } ) } [ \mathrm { K L } ( p _ { \theta } ( \mathbf { z } | \mathbf { x } ) ) | | q _ { \phi } ( \mathbf { z } | \mathbf { x } ) ) ] } _ { \mathrm { s l e e p ~ t e r m ~ } \mathcal { L } _ { \mathrm { s l e g } } ( \phi ) } } \end{array}
$$

The optimization of the regularizer $\mathcal { L } _ { \mathrm { s l e e p } } ( \phi )$ is similar to the sleep phase of wake-sleep, and closely resembles diffusion model training (see below). As in wake-sleep, $\mathcal { L } _ { \mathrm { s l e e p } } ( \phi )$ is optimized over $\phi$ only, the $\mathbf { x }$ are sampled from $p$ .

From Wake-Sleep to Diffusion Regularization Computing $\mathcal { L } _ { \mathrm { s l e e p } } ( \phi )$ still involves intractable distributions $p _ { \pmb { \theta } } ( \mathbf { z } | \mathbf { x } ) , q _ { \phi } ( \mathbf { \hat { z } } | \mathbf { x } )$ . To optimize ${ \mathcal { L } } _ { \mathrm { s l e e p } } ( \phi )$ , we introduce another lower bound ${ \mathcal { L } } _ { \mathrm { d i f f } } ( \phi )$ , which we call the denoising diffusion loss (for reasons that will become apparent shortly):

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { s l e e p } } ( \boldsymbol { \phi } ) = - \mathbb { E } _ { p _ { \boldsymbol { \theta } } ( \mathbf { x } ) } [ \mathrm { K L } ( p _ { \boldsymbol { \theta } } ( \mathbf { z } | \mathbf { x } ) | | q _ { \boldsymbol { \phi } } ( \mathbf { z } | \mathbf { x } ) ) ] } \\ & { \quad \quad \quad \quad = \mathbb { E } _ { p _ { \boldsymbol { \theta } } ( \mathbf { x } , \mathbf { z } ) } [ \log q _ { \boldsymbol { \phi } } ( \mathbf { z } | \mathbf { x } ) ] + \bar { H } ( p _ { \boldsymbol { \theta } } ) } \\ & { \quad \quad \quad \quad \quad \quad \geq \mathbb { E } _ { p _ { \boldsymbol { \theta } } ( \mathbf { x } , \mathbf { z } ) } \left[ \mathbb { E } _ { r } [ \log \frac { q _ { \boldsymbol { \phi } } ( \mathbf { y } , \mathbf { z } | \mathbf { x } ) } { r ( \mathbf { y } | \mathbf { z } , \mathbf { x } ) } ] \right] + \bar { H } ( p _ { \boldsymbol { \theta } } ) } \\ & { \quad \quad \quad \quad = \mathcal { L } _ { \mathrm { d i f f } } ( \boldsymbol { \phi } ) } \end{array}
$$

In Equation (8), we applied the ELBO with $r ( \mathbf { y } | \mathbf { z } , \mathbf { x } )$ playing the role of the variational posterior over the latent $\mathbf { y }$ in $q _ { \phi } ; \bar { H } ( p _ { \theta } )$ is the expected conditional entropy of $p _ { \pmb { \theta } } ( \mathbf { z } | \mathbf { x } )$ , a constant that does not depend on $\phi$ .

We can further simplify ${ \mathcal { L } } _ { \mathrm { d i f f } } ( \phi )$ by leveraging the Markov structure of the forward and reverse processes $r , q$ . Recall that each $\mathbf { y } = ( \mathbf { y } _ { 1 } , \mathbf { y } _ { 2 } , . . . , \mathbf { y } _ { T } )$ can be a vector of $T$ latents, which we also denote as $\mathbf { y } _ { 1 : T }$ , and that $r ( { \bf y } _ { 1 : T } | { \bf z } , { \bf x } ) =$ $\begin{array} { r } { \prod _ { t = 1 } ^ { T } r ( \mathbf { y } _ { t } | \mathbf { y } _ { t - 1 } , \mathbf { x } ) } \end{array}$ , where $\mathbf { y } _ { 0 } = \mathbf { z }$ and also $q _ { \phi } ( \mathbf { y } , \mathbf { z } | \mathbf { x } ) =$ $\begin{array} { r } { q _ { \phi } ( \mathbf { y } _ { 0 : T } | \mathbf { x } ) = q _ { \phi } ( \mathbf { y } _ { T } | \mathbf { x } ) \prod _ { t = 1 } ^ { T } q _ { \phi } ( \mathbf { y } _ { t - 1 } | \mathbf { y } _ { t } , \mathbf { x } ) } \end{array}$ .

We may use the Markov structure in $\boldsymbol { q } , \boldsymbol { r }$ to rewrite ${ \mathcal { L } } _ { \mathrm { d i f f } } ( \phi )$ as a sum of $T$ terms, one per Markov step. The derivation is identical to that used to obtain the ELBO of a diffusion model (Sohl-Dickstein et al. 2015), and yields an expression of the same form:

$$
\begin{array} { l } { \displaystyle \mathcal { L } _ { \mathrm { d i f f } } ( \phi ) = \mathbb { E } _ { r } \left[ \log q _ { \phi } ( \mathbf { z } | \mathbf { y } _ { 1 } , \mathbf { x } ) - \sum _ { t = 2 } ^ { T } \mathbf { K } \mathbf { L } ( r _ { t } | | q _ { t } ) \right] } \\ { \displaystyle ~ - \mathbf { K } \mathbf { L } ( r ( \mathbf { y } _ { T } | \mathbf { z } ) | | q _ { \phi } ( \mathbf { y } | \mathbf { x } ) ) . } \end{array}
$$

where $r _ { t } , q _ { t }$ denote the distributions $r \big ( \mathbf { y } _ { t - 1 } \big | \mathbf { y } _ { t } , \mathbf { y } _ { 0 } , \mathbf { x } \big )$ and $q _ { \phi } ( \mathbf { y } _ { t - 1 } | \mathbf { y } _ { t } , \mathbf { x } )$ (see Appendix $\mathrm { \bf K }$ for the derivation).

Regularized DDVI Objective We define the full DDVI objective ${ \mathcal { L } } _ { \mathrm { d d v i } }$ to be the sum of the aforementioned terms:

$$
\mathcal { L } _ { \mathrm { d d v i } } ( \mathbf { x } , \theta , \phi ) = \mathcal { L } _ { \mathrm { r e c } } ( \mathbf { x } , \theta , \phi ) + \mathcal { L } _ { \mathrm { r e g } } ( \mathbf { x } , \theta , \phi ) + \mathcal { L } _ { \mathrm { d i f f } } ( \phi )
$$

Terms ${ \mathcal { L } } _ { \mathrm { r e g } }$ and ${ \mathcal { L } } _ { \mathrm { d i f f } }$ may be weighted by hyper-parameters $\beta _ { \mathrm { r e g } }$ , $\beta _ { \mathrm { d i f f } } ~ 5 ~ 0$ , as in the $\beta$ -VAE framework. In our experiments, $\beta _ { \mathrm { r e g } } = \beta _ { \mathrm { d i f f } } = 1$ unless otherwise specified. Note that since $\mathcal { L } _ { \mathrm { d i f f } } \leq \mathcal { L } _ { \mathrm { s l e e p } } \leq 0$ , $\mathcal { L } ( \mathbf { x } , \pmb \theta , \phi )$ is a valid lower bound on $\log p _ { \theta } ( \mathbf { x } )$ that is tight when $q _ { \phi } ( \mathbf { \dot { z } } | \mathbf { x } ) = p _ { \theta } ( \mathbf { z } | \mathbf { x } )$ .

# 3.4 Optimization: Extending Wake-Sleep

We may optimize $\mathcal { L } _ { \mathrm { d d v i } } ( \mathbf { x } , \theta , \phi )$ using gradient descent by alternating between ELBO optimization and taking sleep steps (see Appendix A for full details):

1. Sample $\mathbf { x }$ from data, take gradient step on $\theta , \phi$ optimizing $\mathcal { L } _ { \mathrm { r e c } } ( \mathbf { x } , \pmb { \theta } , \phi ) + \mathcal { L } _ { \mathrm { r e g } } ( \mathbf { x } , \pmb { \theta } , \phi )$ (the “wake” step);   
2. Sample ${ \mathbf z } , { \mathbf x }$ from $p _ { \theta }$ and take a gradient step on $\phi$ optimizing ${ \mathcal { L } } _ { \mathrm { d i f f } } ( \phi )$ (the “sleep” step).

Again, terms may be weighted by $\beta _ { \mathrm { r e g } } , \beta _ { \mathrm { d i f f } } > 0$ . Note that by maximizing $\dot { \mathcal { L } } _ { \mathrm { d i f f } } ( \phi )$ , we fit $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ to $p _ { \pmb { \theta } } ( \mathbf { z } | \mathbf { x } )$ via the forward KL divergence; similarly, by optimizing ${ \mathcal { L } } _ { \mathrm { r e c } } + { \mathcal { L } } _ { \mathrm { r e g } }$ (the ELBO), we fit $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ to $p _ { \pmb { \theta } } ( \mathbf { z } | \mathbf { x } )$ via the reverse KL divergence. Thus, optimizing $\mathcal { L } ( \mathbf { x } , \pmb { \theta } , \phi )$ encourages $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ to approximate $p _ { \pmb { \theta } } ( \mathbf { z } | \mathbf { x } )$ , and when the two are equal, the bound $\mathcal { L } _ { \mathrm { d d v i } }$ on $\log p _ { \theta } ( \mathbf { x } )$ is tight.

Simplified Wake-Sleep We also consider a light-weight algorithm, in which $r ( \mathbf { y } \vert \mathbf { z } )$ and $q _ { \phi } ( \mathbf { z } | \mathbf { y } )$ do not depend on $\mathbf { x }$ . This scenario admits the following optimization procedure:

1. Sample x from data and compute gradient on $\theta , \phi$ optimizing $\mathcal { L } _ { \mathrm { r e c } } ( \mathbf { x } , \theta , \phi ) + \mathcal { L } _ { \mathrm { r e g } } ( \bar { \mathbf { x } _ { * } } \mathbf { \theta } \mathbf { \phi } \mathbf { \phi } \phi )$ . 2. Sample $\mathbf { z }$ from $p ( \mathbf { z } )$ and compute gradient on $\phi$ optimizing ${ \mathcal { L } } _ { \mathrm { d i f f } } ( \phi )$ ; take step on weighted sum of both gradients.

In this case, ${ \mathcal { L } } _ { \mathrm { d i f f } }$ requires only sampling from $p ( \mathbf { z } )$ , and the entire loss ${ \mathcal { L } } _ { \mathrm { d d v i } }$ can be optimized end-to-end using gradient descent. This algorithm is simpler (there is no separate sleep phase); however, $q _ { \phi } ( \mathbf { z } | \mathbf { x } )$ may not perfectly approximate $p _ { \pmb { \theta } } ( \mathbf { z } | \mathbf { x } )$ when $r ( \mathbf { y } \vert \mathbf { z } )$ and $q _ { \phi } ( \mathbf { z } | \mathbf { y } )$ do not depend on $\mathbf { x }$ , hence $\mathcal { L }$ may no longer be a tight bound.

Practical Considerations A common type of noising process compatible with this bound when $\textbf { z }$ is continuous is Gaussian diffusion, where we define $r ( \mathbf { y } _ { t } | \mathbf { y } _ { t - 1 } ) =$ $\mathcal { N } ( \mathbf { y } _ { t } ; \sqrt { 1 - \alpha _ { t } } \mathbf { y } _ { t - 1 } , \alpha _ { t } \mathbf { I } )$ for a suitable schedule $( \alpha _ { t } ) _ { t = 1 } ^ { T }$ We then adopt the parameterization $q _ { \phi } ( \mathbf { y } _ { t - 1 } | \mathbf { y } _ { t } , \mathbf { x } ) \bar { \mathbf { \phi } } =$ $\mathcal { N } ( \mathbf { y } _ { t - 1 } ; \mu _ { \phi } ( \mathbf { y } _ { t } , \mathbf { x } , t ) , \Sigma _ { \phi } ( \mathbf { y } _ { t } , \mathbf { x } , t ) )$ . It is then common to parameterize $q _ { \phi }$ with a noise prediction network $\epsilon _ { \phi }$ $\mathrm { \bf ~ \underline { { H o } } }$ , Jain, and Abbeel 2020); the sum of KL divergences can be approximated by $\begin{array} { r } { \mathbb { E } _ { t , \epsilon _ { t } \sim r ( \mathbf { y } _ { 0 } , t ) } | | \epsilon _ { t } - \epsilon _ { \phi } ( \sqrt { \bar { \alpha _ { t } } } \mathbf { y } _ { 0 } + } \end{array}$ $\sqrt { 1 - \bar { \alpha _ { t } } } \epsilon _ { t } , \mathbf { x } , t ) | | ^ { 2 }$ . Other extensions include discrete denoising processes (Austin et al. 2021; Sahoo et al. 2024; Schiff et al. 2024). In the wake-sleep setting, we know both endpoints $\mathbf { y } _ { T } \sim \mathbf { \nabla } q ( \cdot | \mathbf { x } )$ and $\mathbf { y } _ { 0 } ~ = ~ \mathbf { z }$ of the diffusion process, opening the possibility for applying optimal transport techniques (Cuturi 2013; De Bortoli et al. 2021).

# 4 Extensions

# 4.1 Semi-Supervised Learning

Following Makhzani et al. (2015), we extend our algorithm to the semi-supervised learning setting where some data points have labels denoted by $l$ . We assume the user provides a model of the form $p _ { \pmb { \theta } } ( \mathbf { x } , \mathbf { y } , \mathbf { z } , l ) =$ $\begin{array} { r } { p _ { \pmb { \theta } } ( \mathbf { x } | \mathbf { z } , l ) \bar { r } ( \mathbf { y } | \mathbf { z } , l ) p _ { \pmb { \theta } } ( \mathbf { z } | l ) p ( l ) } \end{array}$ ; we set the variational distributions to $q _ { \phi } ( \mathbf { z } | \mathbf { x } , \mathbf { y } , l ) , q _ { \phi } ( \mathbf { y } | \mathbf { x } ) , q _ { \phi } ( l | \mathbf { x } )$ . In this setting, we consider two cases, depending on whether the label is observed (Kingma et al. 2014). We extend Equation (7) to incorporate the label $l$ corresponding to a data point as follows:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { s e m i } } = \mathbb { E } _ { q _ { \phi } ( \mathbf { y } , \mathbf { z } | \mathbf { x } , l ) } \bigl [ \log p _ { \theta } ( \mathbf { x } | \mathbf { z } , l ) \bigr ] } \\ & { \phantom { \mathcal { L } _ { \mathrm { s e m i } } = } - \mathrm { K L } \bigl ( q _ { \phi } ( \mathbf { y } , \mathbf { z } | \mathbf { x } , l ) | | p _ { \theta } ( \mathbf { y } , \mathbf { z } | l ) \bigr ) } \\ & { \phantom { \mathcal { L } _ { \mathrm { s e m i } } = } - \mathbb { E } _ { p _ { \theta } ( \mathbf { x } ) } \left[ \mathrm { K L } \bigl ( p _ { \theta } ( \mathbf { z } | \mathbf { x } , l ) | | q _ { \phi } ( \mathbf { z } | \mathbf { x } , l ) \bigr ) \right] } \end{array}
$$

When the label $c$ cannot be observed, we treat it as a latent variable and modify the learning objective $\chi _ { \mathrm { s e m i } } \ =$ $\begin{array} { r } { \sum _ { c } q _ { \phi } ( l | \mathbf { x } ) \mathcal { L } _ { \mathrm { s e m i } } ( \mathbf { x } , l , \pmb { \theta } , \phi ) \ + \ \mathrm { K L } ( q _ { \phi } ( l | \mathbf { x } ) | | p ( l ) ) } \end{array}$ . Therefore, we can conclude a marginal likelihood on our dataset as follows: $\begin{array} { r } { \tilde { \mathcal { L } } _ { \mathrm { s e m i } } ~ = ~ \sum _ { ( \mathbf { x } , l ) \in L } \mathcal { L } _ { \mathrm { s e m i } } ( \mathbf { x } , l , \pmb \theta , \phi ) \ + } \end{array}$ $\begin{array} { r } { \sum _ { \mathbf { x } \in U } \mathcal { U } _ { \mathrm { s e m i } } ( \mathbf { x } , \pmb { \theta } , \phi ) } \end{array}$ . where $L$ and $U$ are the sets of data wPith∈and without labels, respectively.

We also want to guarantee that all model parameters can be learned in all cases, including $q _ { \phi } ( l | \mathbf { x } )$ , such that this posterior can be applied as a classifier during inference. Thus, we combine the marginal likelihood with a classification loss to form an extended learning objective: $\tilde { \mathcal { L } } _ { \mathrm { s e m i } _ { \alpha } } =$ $\tilde { \mathcal { L } } _ { \mathrm { s e m i } } + \alpha \cdot \mathbb { E } _ { \tilde { p } ( \mathbf { x } , l ) } \left[ - \log q _ { \phi } ( l | \mathbf { x } ) \right]$

# 4.2 Clustering

We have further extended our algorithm to encompass the clustering paradigm. We propose two distinct strategies. In the first approach, we simply formulate a model in which $p _ { \pmb { \theta } } ( \mathbf { z } )$ is a mixture of desired priors. The means of these priors are characterized by $\pmb \theta$ . From these means, cluster membership, denoted as c can be deduced. This approach requires no alteration to the existing learning objective.

Alternatively, the second method retains the original prior, but introduces an additional latent cluster variable c where $\textstyle \sum _ { i } c _ { i } \ = \ 1$ . Thus, the model can be specified as $p _ { \theta } ( \mathbf { x } , \mathbf { y } , \mathbf { \overline { { z } } } , \mathbf { c } ) \ = \ p _ { \theta } ( \mathbf { x } | \mathbf { z } , \mathbf { c } ) r ( \mathbf { y } | \mathbf { z } ) p _ { \theta } ( \mathbf { z } ) p ( \mathbf { c } )$ with $p ( \mathbf { c } ) \mathbf { \Psi } = \mathbf { \Psi }$ $D i r ( \epsilon )$ . Consequently, the variational distributions become $q _ { \phi } ( \mathbf { z } | \mathbf { y } , \mathbf { c } , \mathbf { x } ) , q _ { \phi } ( \mathbf { y } , \mathbf { c } | \mathbf { x } )$ . This yields the objective:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { c l u s } } ( \mathbf { x } ) = \mathbb { E } _ { q _ { \phi } ( \mathbf { y } , \mathbf { z } , \mathbf { c } | \mathbf { x } ) } [ \log p _ { \theta } ( \mathbf { x } | \mathbf { z } , \mathbf { c } ) ] } \\ & { \quad \quad \quad - \mathrm { K L } \big ( q _ { \phi } ( \mathbf { y } , \mathbf { z } , \mathbf { c } | \mathbf { x } ) \big | | p _ { \theta } ( \mathbf { y } , \mathbf { z } , \mathbf { c } ) \big ) } \\ & { \quad \quad \quad - \mathbb { E } _ { p _ { \theta } ( \mathbf { x } ) } \left[ \mathrm { K L } \big ( p _ { \theta } ( \mathbf { z } | \mathbf { x } ) \big | | q _ { \phi } ( \mathbf { z } | \mathbf { x } ) \big ) \right] } \end{array}
$$

Expectations over small numbers of classes $\mathbf { c }$ are done analytically; larger c require backpropagating through discrete sampling (Jang, Gu, and Poole 2016; Sahoo et al. 2023).

# 5 Experiments

We compare DDVI with Auto-Encoding Variational Bayes (AEVB) (Kingma and Welling 2013), AEVB with inverse autoregressive flow posteriors (AEVB-IAF) (Kingma et al. 2016), Adversarial Auto-Encoding Bayes (AAEB) (Makhzani et al. 2015), and Path Integral Sampler (PIS) (Zhang and Chen 2021) on MNIST (Lecun et al. 1998) and CIFAR-10 (Krizhevsky and Hinton 2009) in unsupervised and semi-supervised learning settings, and also on the Thousand Genomes dataset (Siva 2008). We also compare with Hierachical Auto-Encoding Variational Bayes (H-AEVB) (Ranganath, Tran, and Blei 2016; Vahdat and Kautz 2020) in unsupervised setting. We discuss the computational costs of all methods in Appendix D. The priors, model architecture, and training details can also be founded in Appendix H. All results below are reported with $9 5 \%$ confidence interval using 3 different seeds.

# 5.1 Unsupervised learning

We start with synthetic experiments that are aimed at benchmarking the expressivity of diffusion-based posteriors and their ability to improve fitting $p$ , a distribution with a complex structured prior, like one might find in probabilistic programming, scientific analysis, or other applications. We fit a model $p _ { \pmb { \theta } } ( \mathbf { x } , \mathbf { z } )$ on the MNIST and CIFAR-10 datasets with three priors $p ( \mathbf { z } )$ : pinwheel, swiss roll, and square and report our results in Table 1 and 7. The model distribution $p _ { \theta }$ is instantiated by a deep Gaussian latent variable model (DGLVM) with multi-layer perceptrons (MLPs) on MNIST and convolutional neural networks (CNNs) on CIFAR-10 (see the details of model architecture in Appendix G).

Our first set of metrics (ELBO and MMD) seeks to evaluate the learned generative model $p _ { \theta }$ is good. In the ELBO calculation, we average the reconstruction loss across image pixels. We use MMD to measure sample quality: we generate images with the trained model and calculate MMD between the generated images and test images using a mixture of Gaussian kernel with sigma equal to [2, 5, 10, 20, 40, 80]. We only report MMD for MNIST, since CIFAR-10 generated samples are very low-quality for all methods because the latent dimension is 2.

Our last metric seeks to directly evaluate the expressivity of the posterior. We measure latent negative log-likelihood (Latent NLL) by fitting a kernel density estimator (KDE) on the latents produced by the model with test data as input and compute the log-likelihood of the latents sampled from the prior under the fitted KDE.

From Table 1 and Table 7 in Appendix, we see our method DDVI achieve best ELBO in all but one scenario, in which it still performs competitively. We also see strong results in Latent NLL and $\mathbf { k }$ -nearest neighbors classification accuracy of the latents (Acc). in many scenarios, except for swiss roll where AAEB does well. We present visualizations on MNIST using the baselines and our method in Figure 2.

# 5.2 Semi-supervised Learning

We also evaluate the performance of our method and the baselines under semi-supervised learning setting where some labels are observed (1,000 for MNIST and 10,000 for CIFAR-10) and the partitions of the priors are known.

For this setting, we evaluate ELBO, Latent NLL, and Acc. We choose classification accuracy since classification is a common downstream task for semi-supervised learning. We use the same set of priors and baselines. Details on how we partition each prior into $p _ { \pmb { \theta } } ( \mathbf { z } | \mathbf { x } , l )$ can be founded in Appendix F. The partitions defined for our priors are local parts of the priors. We note that unlike unsupervised learning, we use the simplified sleep term in our objective for this setting (see Appendix B for details), since $q _ { \phi }$ already gets extra information from l here.

The results are shown in Table 2 and Table 8 in Appendix. DDVI mostly outperforms the baselines across different priors and metrics, especially on CIFAR-10 where DDVI is best across the board. For MNIST, DDVI always achieves the best ELBO, and it also performs competitively with other baselines in classification accuracy. We also show the visualizations of the latents in Figure 3 where DDVI matches the prior almost perfectly.

# 5.3 Clustering and Visualization for Genotype Analysis

In this section, we report results on an real-world task in genome analysis. Visualizing genotype data reveals patterns in the latent ancestry of individuals. We instantiate DDVI with a deep Gaussian latent variable model (DGLVM) and compare it against with the three strong clustering baselines using the 1000 Genomes dataset. We also report visualizations from three dimensionality reduction algorithms: PCA, TSNE, and UMAP. For each clustering algorithm, we seek to discover up to 20 clusters. We report quantitative results in terms of cluster purity, cluster completeness, and normalized mutual information (NMI). There is an inherent tradeoff between cluster purity completeness. The overall clustering performance can be captured with NMI.

![](images/4a147e2ea80eb2b0dff2ee73102226ac74f1c0af261deae42e2f6315beb4f12c.jpg)  
Figure 2: Unsupervised visualization on MNIST using three priors (pinwheel, swiss roll, and square). Each color indicates a class.

<html><body><table><tr><td>Method</td><td colspan="3">Pinwheel</td><td colspan="3">Swiss Roll</td><td colspan="3">Square</td></tr><tr><td></td><td>ELBO</td><td>MMD</td><td>Latent NLL</td><td>ELBO</td><td>MMD</td><td>Latent NLL</td><td>ELBO</td><td>MMD</td><td>Latent NLL</td></tr><tr><td>AEVB</td><td>−12.13 ± 0.41</td><td>0.77 ± 0.04</td><td>1.68 ± 0.31</td><td>−14.80 ± 0.23</td><td>0.78 ± 0.17</td><td>5.65 ± 1.58</td><td>-7.85±0.29</td><td>1.10 ± 0.66</td><td>2.78± 0.61</td></tr><tr><td>AEVB-IAF</td><td>-4.19±0.05</td><td>0.77 ± 0.00</td><td>1.64 ± 0.73</td><td>-5.10± 0.30</td><td>0.61 ± 0.15</td><td>4.43 ± 1.09</td><td>-3.97± 0.22</td><td>0.75 ± 0.12</td><td>1.68 ± 0.27</td></tr><tr><td>AAEB</td><td>N/A</td><td>0.68± 0.02</td><td>1.54 ± 0.19</td><td>N/A</td><td>0.52 ± 0.03</td><td>3.34 ± 0.16</td><td>N/A</td><td>0.80 ±0.02</td><td>2.46 ± 0.46</td></tr><tr><td>H-AEVB</td><td>-7.03 ± 3.13</td><td>0.74± 0.02</td><td>2.25 ± 3.02</td><td>-7.21 ± 4.62</td><td>0.70 ± 0.22</td><td>4.04 ± 4.62</td><td>-5.71±3.05</td><td>0.76 ± 0.21</td><td>2.22 ± 2.03</td></tr><tr><td>PIS</td><td>-7.83 ± 0.64</td><td>0.75± 0.14</td><td>6.50 ± 1.11</td><td>−9.83 ± 0.61</td><td>0.61 ± 0.03</td><td>2.40 ± 1.01</td><td>-7.06 ± 0.06</td><td>0.77 ± 0.04</td><td>3.67 ± 0.08</td></tr><tr><td>DDVI</td><td></td><td>-3.88 ± 0.96 0.67± 0.04 1.27±0.21</td><td></td><td>-5.03± 0.58</td><td>0.62 ± 0.33</td><td>3.86 ± 0.17</td><td>-3.79 ± 0.14 0.66 ±0.07 1.56 ± 0.09</td><td></td><td></td></tr></table></body></html>

Table 1: Unsupervised learning on MNIST. We report ELBO, MMD between generated images and test images, and laten negative log-likelihood (Latent NLL) with pinwheel, swiss roll, and square priors.

In Table 3, we see that DDVI attains the best performance on cluster purity and NMI. For cluster completeness, VAE and AAE have better means but much larger confidence interval. Furthermore, we visualize our genotype clustering results in latent space, shown in Figure 4, and also report results from classical dimensionality reduction and visualization methods that do not perform clustering (PCA (Wold, Esbensen, and Geladi 1987), t-SNE (Van der Maaten and Hinton 2008), and UMAP (McInnes, Healy, and Melville 2018)). The legend of Figure 4 can be founded at Figure 5 in Appendix.

# 6 Discussion

Diffusion vs. Normalizing Flows Our approach is most similar to flow-based approximators (Rezende and Mohamed 2015; Kingma et al. 2016); in fact when $T \to \infty$ , our diffusion-based posterior effectively becomes a continuoustime normalizing flow (Song, Meng, and Ermon 2020). However, classical flow-based methods require invertible architectures for each flow layer: this constrains their expressivity and requires backpropagating through potentially a very deep network. Our approach, on the other hand, trains a model (a continuous-time flow when $T \to \infty$ ) via a denoising objective (similar to score matching) that does not require invertible architectures and effectively admits an infinite number of layers (with weight sharing). This model is trained not by backpropagating through the ELBO, but rather via an auxiliary diffusion loss term (effectively, a score matching objective).

Despite training with a modified loss, we observe in Section 5 that a diffusion model with an expressive denoising architecture yields an improved ELBO relative to regular flows. Also, our modified loss based on the forward KL divergence reduces posterior collapse (i.e., all modes of the prior are covered well), and thus produces better samples.

![](images/d69705591d86aa6e11a8e063740d7db152b2aa3f150a620d1d3908ee2e2e91d6.jpg)  
Figure 3: Semi-supervised visualization on MNIST with 1,000 labels using three different priors (pinwheel, swiss roll, an square). Each a indicates one class.

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3">Pinwheel</td><td colspan="3">Swiss Roll</td><td colspan="3">Square</td></tr><tr><td>ELBO</td><td>Acc</td><td>Latent NLL</td><td>ELBO</td><td>Acc</td><td>Latent NLL</td><td>ELBO</td><td>Acc</td><td>Latent NLL</td></tr><tr><td>AEVB</td><td>-11.15 ± 0.53</td><td>0.93 ± 0.01</td><td>1.36 ± 0.03</td><td>-15.29 ± 1.33</td><td>0.68 ± 0.01</td><td>4.60 ± 0.23</td><td>-10.26 ± 0.25</td><td>0.86 ± 0.01</td><td>1.68 ± 0.02</td></tr><tr><td>AEVB-IAF</td><td>-2.10 ± 0.26</td><td>0.95 ± 0.00</td><td>1.06 ± 0.03</td><td>-5.38 ± 1.78</td><td>0.90 ±0.02</td><td>2.75 ± 0.14</td><td>-2.67±0.83</td><td>0.91 ±0.01</td><td>0.90±0.02</td></tr><tr><td>AAEB</td><td>N/A</td><td>0.89 ± 0.01</td><td>1.55 ± 0.01</td><td>N/A</td><td>0.88 ±0.01</td><td>3.07 ± 0.05</td><td>N/A</td><td>1.94 ± 0.38</td><td>0.76 ± 0.13</td></tr><tr><td>DDVI</td><td>-0.24 ± 0.13 0.95 ±0.00 1.06 ±0.01</td><td></td><td></td><td>2.89 ±0.33 0.92 ±0.01</td><td></td><td>2.09 ± 0.00</td><td>0.02 ± 0.09</td><td>0.90 ± 0.01</td><td>1.49 ± 0.03</td></tr></table></body></html>

Table 2: Semi-supervised learning on MNIST (1,000 labels). We report ELBO, accuracy using KNN $( \mathtt { K } = 2 0 )$ ) classifier (Acc) and latent negative log-likelihood (Latent NLL) with pinwheel, swiss roll, and square priors.

Table 3: Quantitative genotype clustering results.   

<html><body><table><tr><td>Method</td><td>Cluster Purity</td><td>Cluster Completeness</td><td>NMI</td></tr><tr><td>AEVB</td><td>0.28±0.02</td><td>0.78 ±0.16</td><td>0.59±0.08</td></tr><tr><td>AEVB-IAF</td><td>0.29±0.04</td><td>0.73±0.06</td><td>0.55 ±0.06</td></tr><tr><td>AAEB</td><td>0.37±0.06</td><td>0.76 ±0.11</td><td>0.63±0.02</td></tr><tr><td>DDVI</td><td>0.45 ±0.03</td><td>0.75 ±0.05</td><td>0.66 ±0.04</td></tr></table></body></html>

Diffusion vs. Other Generative Models Variational posteriors based on GANs (Makhzani et al. 2015) also admit expressive architectures and require only sample-based access to the prior $p ( \mathbf { z } )$ . Our diffusion-based approach admits a more stable loss, and is potentially more expressive, as it effectively supports an infnite number of layers (with shared parameters when $T \to \infty$ ). Unlike GANs, our models also admit explicit likelihoods and allow us to compute the ELBO for model evaluation. Our approach is similar to variational MCMC (Salimans, Kingma, and Welling 2015); however, we train with a better objective augmented with a diffusion loss, and we adopt improved architectures with shared weights across layers.

Diffusion for Approximate Inference Existing diffusionbased approximate inference methods (Berner, Richter, and Ullrich 2022; Zhang and Chen 2021; Vargas, Grathwohl, and Doucet 2023; Zhang et al. 2023; Richter, Berner, and Liu 2023; Sendera et al. 2024; Akhound-Sadegh et al. 2024) focus on the task of drawing samples from unnormalized distributions $\tilde { p } ( { \mathbf z } )$ and estimating the partition function $\begin{array} { r } { Z = \int _ { \mathbf { z } } \tilde { p } ( \mathbf { z } ) d \mathbf { z } } \end{array}$ . While these methods are applicable in our setting—we set the unnormalized $\tilde { p } ( { \mathbf { z } } )$ to $p _ { \pmb { \theta } } ( \mathbf { x } , \mathbf { z } )$ such that $Z = p _ { \pmb { \theta } } ( \mathbf { x } )$ —they also tackle a more challenging problem (drawing samples from energy-based models) in more general classes of models (arbitrary unnormalized distributions). In contrast, we focus on restricted but still important classes of models (VAEs, Bayes networks, etc.), and we solve more challenging sets of tasks (e.g., maximumlikelihood learning) by using properties of $p _ { \theta }$ (the factorization $p _ { \pmb { \theta } } ( \mathbf { x } | \mathbf { z } ) p _ { \pmb { \theta } } ( \mathbf { z } )$ and efficient sampling from $p _ { \theta }$ ).

Our algorithms are also simpler. For example, diffusion sampling methods require backpropagating through a sampling process to minimize the reverse ${ \mathrm { K L } } ( q _ { \phi } | | p _ { \theta } )$ , which poses challenges with optimization and credit assignment. Some methods based on Schrodinger bridges require an iterative optimization process generalizing the sinkhorn algorithm or computationally expensive on-policy or off-policy (Malkin et al. 2022) trajectory-based optimization. In contrast, DDVI optimizes the forward ${ \mathrm { K L } } ( { \bar { p _ { \theta } } } | | q _ { \phi } )$ using simple gradient-based optimization that directly emulates diffusionbased training.

![](images/5aa79b32327aa7e83b5c6c672d2ca170044e6f142cdb805ca53ab9ba2ec9a1f8.jpg)  
Figure 4: Visualization of genotype clusters. A color represents one ethnicity.

# 7 Related Work

Latent Diffusion Vahdat, Kreis, and Kautz (2021); Wehenkel and Louppe (2021); Rombach et al. (2022) perform diffusion in the latent space of a VAE. Their goal is high sample quality, and they introduce into $p$ hierarchical latents with simple Gaussian priors.

Our goal is different: we seek to fit a $p$ with structured latents (e.g., in probabilistic programming or in science applications, users introduce prior knowledge via handcrafted $p$ ), and we improve variational inference in this structured model by introducing auxiliary latents into $q$ . Recent work (Preechakul et al. 2022; Zhang, Zhao, and Lin 2022; Wang et al. 2023) has also melded auto-encoders with diffusion models, focusing on semantically meaningful lowdimensional latents in a diffuser $p$ . Cohen et al. (2022) crafts a diffusion bridge linking a continuously coded vector to a non-informative prior distribution.

Diffusion for Approximate Inference Diffusion sampling (Berner, Richter, and Ullrich 2022; Zhang and Chen 2021; Vargas, Grathwohl, and Doucet 2023; Zhang et al. 2023; Richter, Berner, and Liu 2023; Sendera et al. 2024; Akhound-Sadegh et al. 2024) mainly focuses on the task of drawing samples from unnormalized distributions and estimating the partition function. These works draw connections between diffusion (learning the denoising process) and stochastic control (learning the Fo¨llmer drift). Some other works (Zhang et al. 2023; Akhound-Sadegh et al. 2024; Sendera et al. 2024) use continuous generative flow networks (GFlowNets) – deep reinforcement learning algorithms adapted to variational inference that offers stable offpolicy training and thus flexible exploration. While this work is applicable to our setting, it does not rely on the structure of $p ( \mathbf { x } , \mathbf { z } )$ available to us, namely tractable sampling in $p$ .

Dimensionality Reduction Latent variable models in general are an attractive alternative to visualization methods like PCA, Sparse PCA, NMF, UMAP, and t-SNE (Wold, Esbensen, and Geladi 1987; Kuleshov 2013; Lee and Seung 2000; McInnes, Healy, and Melville 2018; Van der Maaten and Hinton 2008). Domain-specific knowledge can be injected through the prior, and deep neural networks can be utilized to achieve a more expressive mapping from the data space to the latent space. Nevertheless, downsides of LVMs are that they are more computationally expensive and require careful hyperparameter tuning.

# 8 Conclusion

While this paper focuses on applications of DDVI to dimensionality reduction and visualization, there exist other tasks for the algorithm, e.g., density estimation or sample quality. Accurate variational inference has the potential to improve downstream applications of generative modeling, e.g., decision making (Nguyen and Grover 2022; Deshpande and Kuleshov 2023), meta-learning (Rastogi et al. 2023), or causal effect estimation (Deshpande et al. 2022).

Since our learning objective differs from the ELBO (it adds a regularizer), we anticipate gains on models whose training benefits from regularization, but perhaps not on all models. Also, attaining competitive likelihood estimation requires architecture improvements that are orthogonal to this paper. However, our ability to generate diverse samples and achieve class separation in latent space hints at the method’s potential on these tasks.