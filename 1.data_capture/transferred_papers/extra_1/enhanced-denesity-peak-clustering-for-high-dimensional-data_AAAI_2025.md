# Enhanced Density Peak Clustering for High-Dimensional Data

Zhongli Wang1\*, Jie $\mathbf { Y a n g } ^ { 2 * }$ , Junyi Guan1, Chenglong Zhang1, Xinyan Liang3, Bingbing Jiang1, Weiguo Sheng1‚Ä†

1Hangzhou Normal University, 2University of Technology Sydney, 3Shanxi University wangzhongli1 $@$ stu.hznu.edu.cn, jie.yang- $1 @$ uts.edu.cn, jonnyguan73@163.com,clzhang123@163.com, liangxinyan48@163.com, jiangbb $@$ hznu.edu.cn, w.sheng $@$ ieee.org.

# Abstract

As a foundational clustering paradigm, Density Peak Clustering (DPC) partitions samples into clusters based on their density peaks, garnering widespread attention. However, traditional DPC methods usually focus on high-density regions, neglecting representative peaks in relatively low-density areas, particularly in datasets with varying densities and multiple peaks. Moreover, existing DPC variants struggle to identify clusters correctly in high-dimensional spaces due to the indistinct distance differences among samples and sparse data distributions. Additionally, existing methods typically adopt a one-step label assignment strategy, making them prone to cascading errors when initial misassignments occur. To address these challenges, we propose an Enhanced Density Peak Clustering (EDPC) method, which creatively incorporates multilayer perceptron (MLP)-based dimensionality reduction and a hierarchical label assignment strategy to significantly improve clustering performance in highdimensional scenarios. Specifically, we introduce an effective selection condition that combines average densities and density-related distances to generate potential cluster centers, ensuring that peaks across different density regions are considered simultaneously. Furthermore, an MLP, guided by pseudo-labels from sub-clusters, is designed to learn lowdimensional embeddings for high-dimensional data, preserving data locality while enhancing clusterability. Extensive experiments demonstrate the effectiveness and superiority of EDPC against state-of-the-art DPC methods.

# Introduction

Clustering, as a fundamental unsupervised learning technique, aims to divide data samples into different clusters based on the intrinsic structures or relationships within data. With the exponential growth of unlabeled samples in practical applications, numerous clustering paradigms have been proposed, such as K-means (Yang et al. 2017), fuzzy clustering, hierarchical clustering, spectral clustering (Hu et al. 2022; Gan et al. 2022), and density-based clustering (Saxena et al. 2017; Cheng et al. 2024). Recently, deep network-based clustering also attracted extensive attention (Xue et al. 2019). Among these clustering techniques, Density Peak Clustering (DPC) (Rodriguez and Laio 2014) focuses on finding appropriate density peaks without prior knowledge, exhibiting better performance than other clustering paradigms in tackling non-spherical cluster problems (Wei et al. 2023). In comparison with other clustering paradigms (e.g., graph-based clustering), DPC can directly achieve final clustering labels without additional postprocessing, since it can build a cluster tree for data based on the density-to-distance linkage and then cuts the cluster tree into clusters via a decision graph.

From the perspective of selecting cluster centers, DPC assumes that cluster centers are density peaks with higher densities than their surrounding neighbors and are far from regions of higher density. (Xie et al. 2016) also proposed to identify density peaks as cluster centers, using a density estimation method based on the distance of each sample to its K-nearest neighbors (KNN). (Zhang et al. 2022) applied linear regression and residual analysis to select candidate centers, from which the final cluster centers were determined by using a validity index. Considering that the performance of DPC relies highly on cluster centers, these traditional DPC methods tend to select cluster centers as density peaks from high-density regions, yet overlook representative peaks in relatively low-density regions, leading to suboptimal results. Consequently, it is a critical challenge to guarantee the effectiveness of selected cluster centers especially when there exist varying density and multiple peaks in data distribution.

To address these challenges, various DPC variants have been proposed to improve the selection process of cluster centers. (Liu, Wang, and $\mathrm { Y u } 2 0 1 8 \$ ) redefined densities and density-related distances using shared neighbors to capture data attributes, while (Guan et al. 2023) introduced a graphbased approach to assign non-peak and density peaks labels, accounting for multiple peaks within clusters. However, these improved DPC methods still struggle to accurately identify representative cluster centers in low-density regions. Additionally, most DPC methods assign cluster labels in a single step, making them susceptible to cascading errors if initial label assignments are incorrect. To mitigate this, (Lotfi, Moradi, and Beigy 2020) proposed identifying high-density samples to form cluster backbones, with other samples assigned to their nearest backbones. (Qin et al. 2021) employed the Jaccard coefficient to measure similarities, first assigning high-similarity samples to cluster centers and then allocating remaining samples based on their similarity to already assigned samples. However, highdimensional data further complicate the cluster center selection, as the distance differences among samples become less distinct, while data distributions tend to be sparse in high-dimensional space (Cao et al. 2023). These factors severely limit the effectiveness of DPC methods and degrade their performance (Yang and Lin 2024). Although (Du, Ding, and Jia 2016) integrated principal component analysis (PCA) into DPC for dimensionality reduction, the extracted features fail to preserve local structures and relationships among original data, misguiding clustering processes (Wu et al. 2022; Zhao et al. 2021; Jiang et al. 2025).

An analysis of DPC methods reveals two factors that limit their performance in high-dimensional tasks. First, these methods primarily focus on high-density regions, often overlooking relatively low-density areas, particularly when data distributions exhibit varying densities and multiple peaks. Second, in high-dimensional spaces, the differences in sample distances become less distinct, resulting in sparse data distributions that obscure the underlying clustering structure, thereby reducing the effectiveness of existing DPC methods. To overcome these inherent limitations, we propose an Enhanced Density Peak Clustering (EDPC) method for high-dimensional data, significantly improving the performance and applicability of DPC in high-dimensional tasks. Specifically, we effectively leverage average densities and density-related distances of samples to select the potential cluster centers that take into account both high and low-density regions simultaneously. Afterward, the pseudolabels of sub-clusters generated from potential cluster centers can further guide MLP to learn low-dimensional embeddings for high-dimensional data while preserving the local neighbor structures among the original samples. Leveraging the learned low-dimensional embeddings, sub-clusters with well-defined clusterability are identified and hierarchically merged to produce the final clustering labels. The main contributions of this paper are summarized as follows:

‚Ä¢ We develop an Enhanced Density Peak Clustering (EDPC) method, which incorporates the hierarchical label assignment process and the MLP-driven dimensionality reduction into a unified framework, seeking the effective partition for high-dimensional data. ‚Ä¢ We propose to select potential cluster centers according to the average densities and density-related distances of samples, ensuring that the peaks across different density regions are considered simultaneously. ‚Ä¢ A labels-driven MLP dimensionality reduction technique is proposed for high-dimensional data, which effectively leverages pseudo-labels to learn the low-dimensional embedding with the discrimination enhancement, achieving better clustering performance.

# Related Works The DPC Algorithm

Given a dataset $X = \{ x _ { 1 } , x _ { 2 } , \ldots , x _ { n } \mid x _ { i } \in \mathbb { R } ^ { m } \}$ , DPC computes the local density $\rho _ { i }$ for each point $x _ { i }$ , along with the density-related distance $\delta _ { i }$ to the nearest point with a higher density, as shown in Eq. (1) and (2).

$$
\rho _ { i } = \sum _ { x _ { j } \in X } \chi ( d _ { i j } - d _ { c } ) , \chi ( z ) = \left. \begin{array} { l l } { 1 } & { z < 0 } \\ { 0 } & { z \geqslant 0 } \end{array} \right.
$$

$$
\delta _ { i } = \operatorname* { m i n } _ { x _ { j } } \left( d _ { i j } \right) , \mathrm { s . t . } \rho _ { j } > \rho _ { i }
$$

Here, $d _ { i j }$ represents the Euclidean distance between $x _ { i }$ and $x _ { j }$ , while the ‚Äúcutoff distance‚Äù $d _ { c }$ is typically chosen such that the average number of neighbors corresponds to $1 \% { - } 2 \%$ of the total points in the dataset (Rodriguez and Laio 2014). For the point with the highest density, $x _ { i }$ , the value of $\delta _ { i }$ is computed as $\delta _ { i } = \operatorname* { m a x } _ { x _ { j } } \left( d _ { i j } \right)$ . According to the assumption of DPC regarding cluster centers (i.e., cluster centers are density peaks surrounded by low-density neighbors and located far from regions of higher density), points with the highest $\gamma$ values $( \gamma = \rho \cdot \delta )$ are selected as cluster centers and assigned unique labels. Subsequently, the remaining points inherit the labels of their nearest higher-density neighbors. DPC has been demonstrated to be an effective approach for identifying single-peak clusters and serves as a foundational method for reconstructing complex multi-peak clusters. This is achieved by first identifying single-peak clusters using DPC, followed by merging similar single-peak clusters into multi-peak clusters (Guan et al. 2022).

# The SNN-DPC

The SNN-DPC (Liu, Wang, and $\mathrm { Y u } 2 0 1 8 _ { \mathrm { . } }$ ) replaces the measurement of original density and density-related distance of DPC with the Shared-Nearest-Neighbor-based density and density-related distance, formulated as:

$$
\rho _ { i } = \sum _ { x _ { j } \in N _ { k } ( x _ { i } ) } s _ { i j }
$$

$$
\delta _ { i } = \operatorname* { m i n } _ { x _ { j } : \rho _ { j } > \rho _ { i } } \left( d _ { i j } \left( \sum _ { x _ { z } \in N _ { k } ( x _ { i } ) } d _ { i z } + \sum _ { x _ { w } \in N _ { k } ( x _ { j } ) } d _ { j w } \right) \right)
$$

where $N _ { k } ( x _ { i } )$ represents the set of $k$ nearest neighbors of $x _ { i }$ and $s _ { i j }$ is the SNN-based similarity between $x _ { i }$ and $x _ { j }$ , defined as:

$$
\begin{array} { r } { s _ { i j } = \left\{ \begin{array} { l l } { \frac { | S N _ { k } ( x _ { i } , x _ { j } ) | ^ { 2 } } { ( d _ { i z } + d _ { j z } ) } , } & { x _ { i } , x _ { j } \in S N _ { k } ( x _ { i } , x _ { j } ) } \\ { \hfill ~ 0 , } & { \mathrm { o t h e r w i s e } } \end{array} \right. } \end{array}
$$

where $S N _ { k } ( x _ { i } , x _ { j } ) = N _ { k } ( x _ { i } ) \cap N _ { k } ( x _ { j } )$ is the shared nearest neighbors between $x _ { i }$ and $x _ { j }$ , and $| S N _ { k } |$ indicates the number of shared neighbors in $\bar { S N _ { k } }$ .

Unlike DPC, SNN-DPC defines density and densityrelated distance by incorporating local data structure, enabling more effective cluster reconstruction. Based on this way, we can identify numerous potential cluster centers, thereby generating reliable single-peak sub-clusters for subsequent hierarchical cluster merging.

# Linkage-based Clustering

Linkage-based Clustering is a classic hierarchical clustering technique, that iteratively merges similar clusters until a single cluster containing all samples is formed, resulting in a cluster tree (i.e., dendrogram) for the dataset. The final clustering is achieved by cutting the dendrogram at the desired number of clusters. In linkage-based clustering, the linkage metric, which estimates the dissimilarity between clusters, is crucial. The classic single-linkage method (Gower and Ross 1969) measures dissimilarity using the shortest distance between clusters, effectively identifying both spherical and complex shapes but is sensitive to noise. In contrast, the average-linkage method (Dasgupta and Long 2005) calculates dissimilarity as the average of all pairwise distances, handling noise and outliers better but tends to favor spherical clusters. To leverage the strengths of both methods, (Zelig, Kariti, and Kaplan 2023) proposed the KMD-linkage to calculate the average of several smallest distances between pairwise clusters to determine their dissimilarity. This approach achieves a balance between robustness to noise and sensitivity to diverse cluster shapes. In this paper, KMDlinkage is employed to accurately quantify the dissimilarity between clusters.

# Methodology

In this section, we present our proposed clustering algorithm, which integrates dimensionality reduction and advanced clustering techniques through the following steps. First, pseudo-labels of sub-clusters are leveraged to train a multilayer perceptron (MLP) for dimensionality reduction. Afterward, a selection criterion identifies $n _ { s }$ potential cluster centers based on their SNN-based density and densityrelated distance. The DPC is then applied to form $n _ { s }$ subclusters, each led by one of these potential centers. Finally, these sub-clusters are hierarchically merged into $n _ { c }$ clusters, representing the true number of clusters, based on their dissimilarity. By effectively combining these steps, the proposed algorithm ensures accurate cluster identification and refinement, even in complex data distributions.

# Potential Cluster Centers

DPC may face challenges when handling datasets with varying densities and multiple peaks, as such scenarios can lead to incorrect identification of cluster centers (Wang et al. 2023). Intuitively, density peaks with higher densities and greater density-related distances are more likely to be selected as potential cluster centers, whereas points with lower densities are typically identified as boundary points or noises. However, in datasets with varying densities, the low-density points still hold representational significance.

To ensure that all representative density peaks are detected, including those in low-density clusters, the average density $\bar { \rho } \stackrel {  } { = } \frac { 1 } { n } \textstyle \sum _ { i = 1 } ^ { n } \rho _ { i }$ . The density-related distance $\bar { \delta } \ = \ \textstyle { \frac { 1 } { n } } \sum _ { i = 1 } ^ { n } \delta _ { i }$ is designed to identify potential density peaks that can serve as clusters, where $\rho _ { i }$ and $\delta _ { i }$ are SNNbased values computed by Eq.(3) and Eq.(4). Thus, we propose a potential center detection criterion, formulated as:

$$
C e n t e r s ( \lambda , \beta ) = \{ x _ { i } \in X | \rho _ { i } \geqslant \lambda \bar { \rho } , \ \delta _ { i } \geqslant \beta \bar { \delta } \}
$$

where $\lambda$ and $\beta$ are hyperparameters that control the thresholds for selecting potential cluster centers based on the density and density-related distance, respectively. If $\lambda$ is set as a larger value, the potential cluster centers in low-density regions will be excluded, thereby misguiding subsequent hierarchical clustering. Conversely, a smaller $\lambda$ could result in an excessive number of samples being identified as potential cluster centers, which can similarly disrupt the clustering process. Similarly, a larger $\beta$ may prevent certain clusters‚Äô density peaks from being selected as potential cluster centers, reducing their representativeness, while a smaller $\beta$ may lead to more points within high-density clusters being chosen as potential centers, compromising the representativeness of the cluster centers. To balance these factors, we set $C e n t e r s ( 0 . 5 , 1 )$ as the thresholds for identifying potential cluster centers in the experiments, such that centers in low-density regions are taken into account, so as to make the overall set of centers more representative. Once these centers are identified, DPC is applied to generate sub-clusters, followed by a hierarchical clustering approach that iteratively merges similar sub-clusters to form final clusters.

# Merging of Sub-Clusters

To achieve a balance between robustness against noises and sensitivity to various cluster shapes, the KMD-linkage (Zelig, Kariti, and Kaplan 2023), denoted as $D _ { K M D }$ , is utilized to estimate the dissimilarity between cluster $C _ { A }$ and cluster $C _ { B }$ , defined as follows:

$$
D _ { K M D } ( C _ { A } , C _ { B } ) = \frac { 1 } { q } \sum \operatorname* { m i n } _ { q } \{ d _ { i j } : x _ { i } \in C _ { A } , x _ { j } \in C _ { B } \}
$$

which means the average value of the top $q$ smallest distances between cluster $C _ { A }$ and $C _ { B }$ , and $q$ is defined as:

$$
q = \operatorname* { m a x } \{ \lfloor \operatorname* { m a x } ( \lvert C _ { A } \rvert , \lvert C _ { B } \rvert ) / \phi \rfloor , 1 \}
$$

where $\vert C _ { A } \vert$ represents the number of points within cluster $C _ { A }$ , and $\lfloor \cdot \rfloor$ denotes the floor function. Notably, the value of $q$ is determined by the parameter $\phi$ . When $\phi$ becomes large, $q$ decreases and tends toward 1, making $D _ { K M D }$ degenerate to single-linkage (i.e., measuring the distance between clusters using the minimum pairwise distance), which is very sensitive to noises and outliers (Jarman 2020). Conversely, when setting $\phi$ to a smaller value, $D _ { K M D }$ approximates average-linkage (i.e., calculating the average of all pairwise distances between clusters), which is unable to handle complex cluster shapes effectively (Charikar, Chatziafratis, and Niazadeh 2019). To this end, we propose adjusting $\phi$ to determine the appropriate number of pairwise distances for datasets with diverse categories (further analysis is provided in the Experiments). Subsequently, we apply linkage clustering to merge $n _ { s }$ sub-clusters into $n _ { c }$ clusters based on their dissimilarity about the linkage-based metric $D _ { K M D }$ .

# Dimension Reduction using Sub-Clusters

Existing DPC-related methods face significant challenges in high-dimensional spaces, where the distances between samples tend to become similar, and the distributions are often sparse. This phenomenon hinders the effective identification of representative density peaks, thereby reducing

Hidden Output (3) Pseudo labels Input Backprop A ‰∏≠ b b (4) DPC (2)   
Original (1) Dimensionality Select by ‚óá   
Data Reduction ùúåùëñ ‚â• ùúÜùúå“ß ·âõŒ¥ùëñ ‚â• Œ≤‡¥§Œ¥ Potential cluster centers

clustering performance. Therefore, we creatively propose to use the pseudo-labels generated by DPC sub-clusters as a guide for dimensionality reduction. Unlike other dimensionality reduction methods, such as PCA, which reduces dimensions based solely on overall variance without any guiding information, leveraging pseudo-labels from sub-clusters for dimensionality reduction effectively utilizes local structural information among data points. This approach extracts lowdimensional representations with enhanced clustering discriminability (Jiang et al. 2024).

Multilayer Perceptron (MLP) The Multilayer Perceptron (MLP) is a fundamental artificial neural network model, which is extensively utilized in machine learning and deep learning. In this study, we employ an MLP with only one hidden layer for dimensionality reduction, offering higher interpretability and lower computational costs than other complex networks. The input layer receives features from data, while the hidden layer, situated between the input and output layers, contains neurons connected by weighted sums and activation functions. The output layer generates the model‚Äôs final output, typically a classification label.

DPC-derived sub-clusters In EDPC, the Center $s ( \lambda , \beta )$ is first used to identify reliable sub-cluster centers by adjusting the hyperparameters $\{ \lambda , \beta \}$ . We then apply density peak clustering to obtain sub-clusters with these centers. The pseudo-labels of these sub-clusters are treated as groundtruth labels and exploited to train the MLP.

Proposed Framework Figure 1 shows how to utilize the pseudo-labels of sub-clusters to guide MLP for dimensionality reduction. Concretely, in Step (1), the original data is analyzed to identify candidate cluster centers based on the Centers $\{ \lambda , \beta \}$ condition, which considers both average density and density-related distance. In Step (2), these candidate centers are organized into sub-clusters using the label assignment strategy of DPC. In Step (3), an MLP is designed with a single hidden layer containing fewer neurons than the original data dimensions. The hidden layer employs ReLU as its activation function, while the output layer utilizes Softmax, with the cross-entropy loss function guiding the training process. The original data is fed into an MLP, where subcluster pseudo-labels act as ground-truth labels to supervise the training process, enabling the learning of more discriminative reduced-dimensional representations for the original high-dimensional data. In Step (4), the number of neurons in the hidden layer is set smaller than the original feature dimensions, and the parameters of this layer are utilized

# Algorithm 1: EDPC without MLP

Input: $X = \{ x _ { 1 } , x _ { 2 } , x _ { 3 } , . . . , x _ { n } \}$ , the number of clusters $n _ { c }$ ;

1: Calculate the point density $\{ \rho _ { 1 } , \rho _ { 2 } , \rho _ { 3 } , . . . , \rho _ { n } \}$ by Eq.(3) and density-related distance $\{ \delta _ { 1 } , \delta _ { 2 } , \delta _ { 3 } , . . . , \delta _ { n } \}$ by Eq.(4) for each sample;   
2: $n _ { s }$ potential cluster centers $\gets C e n t e r s ( 0 . 5 , 1 )$ ;   
3: Obtain $n _ { s }$ sub-clusters with corresponding potential cluster centers via density peak clustering $( n _ { s } \gg n _ { c } )$ ;   
4: while $n _ { s } > n _ { c }$ do   
5: Calculate the distances between sub-clusters by Eq. (7);   
6: Merge two clusters with the smallest distance;   
7: $n _ { s } \gets n _ { s } - 1$

# 8: end while

Output: Clustering labels.

# Algorithm 2: EDPC for High-dimensional Data

Input: $X = \{ x _ { 1 } , x _ { 2 } , x _ { 3 } , . . . , x _ { n } \}$ , the number of clusters $n _ { c }$

; 1: Calculate the point density $\{ \rho _ { 1 } , \rho _ { 2 } , \rho _ { 3 } , . . . , \rho _ { n } \}$ by Eq.(1) and density-related distance $\{ \delta _ { 1 } , \delta _ { 2 } , \delta _ { 3 } , . . . , \delta _ { n } \}$ by Eq.(2) for each sample; 2: $n _ { s }$ potential cluster centers $ C e n t e r s ( \lambda , \beta )$ ; 3: Obtain $n _ { s }$ sub-clusters with corresponding sub-cluster centers via density peak clustering; 4: Train the MLP with the pseudo-labels of sub-clusters; 5: After training, the parameters of the hidden layer are extracted to perform dimensionality reduction; 6: Run Algorithm 1 on the reduced-dimensional data;

to transform original data into reduced-dimensional representations. This allows the MLP to leverage fine-grained pseudo-labels, initially generated by EDPC, to guide dimensionality reduction. Unlike traditional approaches that separate dimensionality reduction and clustering (e.g., first applying t-SNE (Van der Maaten and Hinton 2008) or UMAP (McInnes, Healy, and Melville 2018) for dimensionality reduction and then performing DPC), EDPC integrates MLPbased dimensionality reduction and clustering into a unified framework, enabling the two processes to interact and mutually enhance each other in a self-reinforcing manner.

# Time Complexity Analysis

Algorithm 1 and Algorithm 2 provide the pseudocode for the EPDC without MLP and the EDPC for high-dimensional data, respectively. Below is the time complexity analysis for these algorithms.

EPDC without MLP:The time complexity of computing the distance matrix is $O ( n ^ { 2 } )$ , and the complexity for calculating $\rho$ and $\delta$ totals $O ( n )$ . Selecting $n _ { s }$ potential cluster centers requires $O ( n )$ . Once the potential centers are determined, assigning labels to the remaining points also costs $O ( n )$ . After obtaining $n _ { s }$ sub-clusters based on potential centers, we need to calculate the distance between each sub-cluster through Eq.(7). We consider the average case, assuming that each sub-cluster is the same size of $\frac { n } { n _ { s } }$ . There are a total of $\frac { n _ { s } ( n _ { s } - 1 ) } { 2 }$ pairs of cluster distances that need to be considered, so the time complexity is $\begin{array} { r } { O ( \frac { n _ { s } ( n _ { s } - 1 ) } { 2 } \frac { n ^ { 2 } } { n _ { s } ^ { 2 } } \log \frac { n ^ { 2 } } { n _ { s } ^ { 2 } } ) } \end{array}$ , which is approximately equal to $O ( n ^ { 2 } \log { n } )$ . Combining all the above steps, the overall time complexity approximates to $O ( n ^ { 2 } \log { n } )$ .

Table 1: The detailed information of experimental datasets.   

<html><body><table><tr><td>Datasets</td><td>Instances</td><td>Clusters</td><td>Features</td></tr><tr><td>Dart1</td><td>1000</td><td>4</td><td>2</td></tr><tr><td>Donut2</td><td>1000</td><td>2</td><td>2</td></tr><tr><td>Cno3</td><td>715</td><td>3</td><td>2</td></tr><tr><td>Cno4</td><td>863</td><td>4</td><td>2</td></tr><tr><td>Curet1</td><td>2000</td><td>6</td><td>2</td></tr><tr><td>Cuboids</td><td>1002</td><td>4</td><td>3</td></tr><tr><td>D31</td><td>3100</td><td>3</td><td>2</td></tr><tr><td>Jain</td><td>373</td><td>2</td><td>2</td></tr><tr><td>Alizadeh</td><td>62</td><td>3</td><td>2093</td></tr><tr><td>Armstrong</td><td>72</td><td>2</td><td>1081</td></tr><tr><td>Bhatt</td><td>203</td><td>5</td><td>1543</td></tr><tr><td>Garber</td><td>66</td><td>4</td><td>4553</td></tr><tr><td>Gordon</td><td>181</td><td>2</td><td>1626</td></tr><tr><td>Golub1</td><td>72</td><td>2</td><td>1868</td></tr><tr><td>Golub2</td><td>72</td><td>3</td><td>1868</td></tr><tr><td>Nutt</td><td>22</td><td>2</td><td>1152</td></tr><tr><td>Liang</td><td>37</td><td>3</td><td>1411</td></tr><tr><td>Laiho</td><td>37</td><td>2</td><td>2202</td></tr><tr><td>Pomeroy1</td><td>34</td><td>2</td><td>857</td></tr><tr><td>Pomeroy2</td><td>42</td><td>5</td><td>1379</td></tr><tr><td>Shipp</td><td>77</td><td>2</td><td>798</td></tr><tr><td>West</td><td>49</td><td>2</td><td>1198</td></tr><tr><td>UMIST</td><td>575</td><td>20</td><td>10304</td></tr><tr><td>MSRC</td><td>210</td><td>7</td><td>512</td></tr><tr><td>MSRA</td><td>1799</td><td>12</td><td>256</td></tr><tr><td>Ecoil</td><td>336</td><td>8</td><td>343</td></tr></table></body></html>

EDPC for High-dimensional Data: The time complexity for generating sub-clusters is $O ( n )$ , and training the shallow MLP requires $O ( n m )$ (where $m$ is the number of dimensions of the data $X$ ). The time complexity for applying EDPC without MLP on the reduced-dimensional data is ${ \bar { O ( } } n ^ { 2 } \log n )$ . Therefore, the total time complexity of EDPC approximates to $O ( n ( m + n \log n ) )$ .

# Experiments

In this section, extensive experiments on synthetic datasets and real-world high-dimensional datasets are conducted to evaluate the clustering performance of the proposed EDPC.

# Experimental Settings

To comprehensively verify the effectiveness and superiority of EDPC, various datasets are employed to assess the clustering performance, including eight synthetic datasets and eighteen real-world datasets with high dimensions whose details are summarized in Table 1. Specifically, the stateof-the-art clustering algorithms are compared, including: (1) Classical density peak clustering (DPC) (Rodriguez and Laio 2014); (2) Shared-nearest-neighbor-based density peak clustering (SNN-DPC) (Liu, Wang, and $\mathrm { Y u } 2 0 1 8 \AA$ ); (3) Stable-membership-based multi-peak clustering (SMMP)

![](images/d4c7f1265607437d7e3b7f8b4e57bc06ea266c218a109ad8f43d8d6b83525146.jpg)  
Figure 2: The results on varying density and multiple peaks datasets, in which (a) and (d) depict the original data distributions, (b) and (e) show the results obtained by DPC, (c) and (f) illustrate the results of EDPC.

(Guan et al. 2022); (5) Flexible density peak clustering (DBDPC) (Hou et al. 2024); (6) Fast hierarchical clustering of local density peak (FHC-LDP) (Guan et al. 2021); (7) Consistent and divergent graph clustering (CDGC) (Huang et al. 2022). To ensure fairness, the experiments are implemented based on their default settings, and the number of neighbors (i.e., k) is set to 15 when performing K-nearest neighbors (KNN). For EDPC, the parameter $\phi$ is fixed at 10. Determining the optimal reduced dimensionality is an NP-hard problem in many fields. To ensure consistency when evaluating dimensionality reduction methods, existing studies often predefine the number of reduced dimensionality, avoiding discrepancies caused by varying dimensions across methods. Following this principle, we fix the reduced dimensionality at 50 for EDPC and other methods in our experiments to ensure fair and consistent comparisons. Two external evaluation metrics are utilized: accuracy (ACC) and adjusted mutual information (AMI). ACC measures the percentage of correctly assigned data points, and AMI quantifies the agreement between the clustering labels and the true labels.

# Experiments on Synthetic Datasets

To validate the clustering effectiveness of EDPC, Figure 2 presents visualization results on datasets with varying densities and multiple peaks (i.e., Jain and Cno4 datasets). We find that DPC struggles to correctly identify cluster centers on these datasets, and it instinctively assigns density peaks to higher-density regions as shown in Figures 2(b) and (e), while neglecting representative lower-density regions, resulting in incorrect center identifications. In contrast, EDPC incorporates a selection condition for generating potential cluster centers, enabling it to consider peaks across different density regions simultaneously and effectively address the challenges posed by varying densities and multiple peaks. To further evaluate performance, we compared EDPC against the DPC and four improved DPC variants (i.e., SNN-DPC, SMMP, KNN-DPC, and DB-DPC) on synthetic datasets. Table 2 reports the results on eight syn

<html><body><table><tr><td colspan="6">ACC scores on synthetic datasets</td><td colspan="6">AMI scores on synthetic datasets</td></tr><tr><td>Datasets\Methods</td><td>DPC</td><td>SNN-DPC</td><td>SMMP</td><td>KNN-DPC</td><td>DB-DPC</td><td>EDPC</td><td>DPC</td><td>SNN-DPC SMMP</td><td>KNN-DPC</td><td>DB-DBC</td><td>EDPC</td></tr><tr><td>Dart1</td><td>0.2500</td><td>0.4510</td><td>0.6300</td><td>0.3210</td><td>0.2500</td><td>1.0000 -0.0014</td><td>0.3637</td><td>0.7492</td><td>0.0611</td><td>0.0000</td><td>1.0000</td></tr><tr><td>Donut2</td><td>0.6600</td><td>0.9950</td><td>0.9960</td><td>0.6220</td><td>0.9410 0.9970</td><td>0.2210</td><td>0.9595</td><td>0.9663</td><td>0.1536</td><td>0.7733</td><td>0.9735</td></tr><tr><td>Cno3</td><td>0.8993</td><td>0.7804</td><td>0.9930</td><td>0.8042</td><td>0.5692</td><td>0.9944</td><td>0.7925 0.7355</td><td>0.9573</td><td>0.7168</td><td>0.5144</td><td>0.9677</td></tr><tr><td>Cno4</td><td>0.7578</td><td>0.8737</td><td>0.9988</td><td>0.9988</td><td>0.7984</td><td>0.9988</td><td>0.7815 0.8384</td><td>0.9928</td><td>0.9930</td><td>0.6580</td><td>0.9930</td></tr><tr><td>Curet1</td><td>0.8640</td><td>0.5960</td><td>0.9240</td><td>0.7890</td><td>0.9865</td><td>0.9745 0.8951</td><td>0.7333</td><td>0.9499</td><td>0.8800</td><td>0.9787</td><td>0.9599</td></tr><tr><td>Cuboids</td><td>0.7625</td><td>0.7415</td><td>1.0000</td><td>0.7555</td><td>0.6517</td><td>1.0000</td><td>0.8254 0.8218</td><td>1.0000</td><td>0.8240</td><td>0.7000</td><td>1.0000</td></tr><tr><td>D31</td><td>0.9674</td><td>0.9690</td><td>0.9684</td><td>0.9668</td><td>0.8761</td><td>0.9694</td><td>0.9548 0.9565</td><td>0.9556</td><td>0.9539</td><td>0.9121</td><td>0.9567</td></tr><tr><td>Jain</td><td>0.8606</td><td>0.8338</td><td>1.0000</td><td>0.6237</td><td>0.9088</td><td>1.0000 0.5042</td><td>0.4578</td><td>1.0000</td><td>0.4578</td><td>0.7228</td><td>1.0000</td></tr><tr><td>Average</td><td>0.7527</td><td>0.7801</td><td>0.9388</td><td>0.7351</td><td>0.7477 0.9916</td><td>0.6214</td><td>0.7333</td><td>0.9464</td><td>0.6303</td><td>0.6574</td><td>0.9814</td></tr></table></body></html>

Table 2: The ACC and AMI scores on synthetic datasets. The best and second results are in bold and underlined, respectively

![](images/e6e30b5e3438eac374701409f43408fcffc9deb7398c3796f64ef28743e1005b.jpg)  
Figure 3: The visualization on the UMIST and Gordon datasets. (a) and (d) show the results of the original space, which are directly utilized by DPC, SNN-DPC, SMMP, FHC-LDP and CDGC. (b) and (e) present the results after dimensionality reduction using PCA, as applied by KNNDPC. (c) and (f) display the results obtained by leveraging the pseudo-labels of sub-clusters for dimensionality reduction, as implemented in EDPC.

thetic datasets1, demonstrating that EDPC achieves better or competitive performance over its counterparts on ACC and AMI. In terms of average ACC, EDPC achieves improvements of $2 3 . 9 \%$ , $2 1 . 2 \%$ , ${ \bar { 5 } } . 3 \%$ , $2 5 . 7 \%$ , and $2 4 . 4 \%$ over DPC, SNN-DPC, SMMP, KNN-DPC, and DB-DPC, respectively. This indicates that the weighted mechanism, which leverages average densities and density-related distances with parameters set as Centers(0.5, 1), enables EDPC to effectively identify representative peaks across regions of varying density, resulting in superior performance.

# Experiments on High-dimensional Datasets

Experimental Results To further show the superiority of EDPC in high-dimensional datasets, fourteen gene expression datasets (Shah and Koltun 2017) and four object recognition datasets are utilized, including UMIST (Guan et al. 2023), MSRC (Zhang et al. 2024), MSRA (Wang et al. 2020), Ecoil (Guan et al. 2022). To illustrate how EDPC leverages fine-grained clustering labels to drive neural networks to learn more discriminative representations, we compare EDPC with two additional dimensionality reductionbased DPC variants i.e.,DPC-tSNE (applying DPC after tSNE dimensionality reduction) and SMMP-tSNE (applying SMMP after t-SNE dimensionality reduction) and 7 state-of-the-art competitors. Table 3 presents the comparison between EDPC and the compared methods, in which EDPC achieves satisfactory performance than existing clustering methods across different high-dimensional datasets. Notably, EDPC not only surpasses multiple improved DPC variants but also outperforms dimensionality reductionbased DPC variants. Specifically, on 18 high-dimensional datasets, EDPC achieves improvements in average ACC of $2 3 . 0 \%$ , $1 6 . 9 \%$ , $2 3 . 2 \%$ , $1 2 . 7 \%$ , $1 9 . 3 \%$ , and $1 9 . 5 \%$ over DPC, SNN-DPC, SMMP, KNN-DPC, DB-DPC, and FHC-LDP, respectively, demonstrating its ability to accurately identify cluster centers from regions with varying densities or multiple peaks. Furthermore, EDPC outperforms DPC variants that combine t-SNE with clustering methods, and it achieves $1 5 . 2 \%$ , and $1 4 . 9 \%$ average ACC improvement on 18 highdimensional datasets over DPC-tSNE and SMMP-tSNE, respectively. Although combining t-SNE with density-based clustering offers a potential alternative, it fails to establish meaningful interactions between clustering labels and dimensionality reduction processes, resulting in information loss and suboptimal clustering performance. By integrating dimensionality reduction and clustering into a unified framework, EDPC effectively strengthens the interaction between these processes, producing more discriminative representations and superior clustering accuracy. Additionally, although spectral clustering has long been regarded as superior to density-based clustering methods for handling high-dimensional data, EDPC achieves $1 0 . 0 \%$ average ACC improvement over the advanced spectral clustering method (i.e., CDGC) on the 18 high-dimensional datasets, highlighting the effectiveness of leveraging sub-cluster pseudo-labels with MLP for dimensionality reduction.

![](images/483fbf29d6a2e366969b2a566795feb58ccc27fc220f966608ffc511630d7feb.jpg)  
Figure 4: The ACC of EDPC and its variants.

Table 3: The ACC and AMI on high-dimensional datasets. The best and second results are in bold and underlined, respectively   

<html><body><table><tr><td rowspan="2">Methods\Datasets</td><td colspan="2">AcCAlizadehM1</td><td colspan="2">ACC BhatAMI</td><td colspan="2">ACGarbeAMI</td><td colspan="2">ACGordoAMI</td><td colspan="2">ACGolubAM1</td><td colspan="2">ACCLiangAMI</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DPC</td><td>0.5806</td><td>0.5675</td><td>0.4828</td><td>0.0479</td><td>0.5758</td><td>0.0116</td><td>0.8011</td><td>0.2659</td><td>0.9028</td><td>0.6145</td><td>0.5675</td><td>0.2940</td></tr><tr><td>SNN-DPC</td><td>0.8710</td><td>0.7278</td><td>0.7438</td><td>0.3482</td><td>0.6518</td><td>0.1935</td><td>0.9779</td><td>0.8072</td><td>0.8333</td><td>0.4110</td><td>0.5405</td><td>0.1445</td></tr><tr><td>SMMP</td><td>0.5238</td><td>-0.0162</td><td>0.7537</td><td>0.2130</td><td>0.6515</td><td>0.1248</td><td>0.8287</td><td>0.0000</td><td>0.6528</td><td>0.0000</td><td>0.5676</td><td>0.2416</td></tr><tr><td>KNN-DPC</td><td>0.7581</td><td>0.6341</td><td>0.7241</td><td>0.3710</td><td>0.6515</td><td>0.2134</td><td>0.9669</td><td>0.7101</td><td>0.9583</td><td>0.7340</td><td>0.4865</td><td>0.1247</td></tr><tr><td>DB-DPC</td><td>0.8387</td><td>0.6558</td><td>0.7734</td><td>0.2505</td><td>0.6667</td><td>0.1603</td><td>0.9779</td><td>0.7748</td><td>0.6528</td><td>0.000</td><td>0.7568</td><td>0.0000</td></tr><tr><td>FHC-LDP</td><td>0.8387</td><td>0.6558</td><td>0.8128</td><td>0.3540</td><td>0.6364</td><td>0.0695</td><td>0.8011</td><td>0.0044</td><td>0.6806</td><td>0.0418</td><td>0.5676</td><td>0.2416</td></tr><tr><td>DPC-tSNE</td><td>0.5484</td><td>0.5189</td><td>0.7192</td><td>0.5045</td><td>0.6970</td><td>0.3563</td><td>0.9613</td><td>0.6637</td><td>0.5000</td><td>0.1226</td><td>0.6486</td><td>0.2457</td></tr><tr><td>SMMP-tSNE</td><td>0.5645</td><td>0.4624</td><td>0.7291</td><td>0.4919</td><td>0.6818</td><td>0.1746</td><td>0.9613</td><td>0.6573</td><td>0.8611</td><td>0.4166</td><td>0.7027</td><td>0.3020</td></tr><tr><td>CDGC</td><td>0.9839</td><td>0.9089</td><td>0.8079</td><td>0.5292</td><td>0.6818</td><td>0.2428</td><td>0.9779</td><td>0.7748</td><td>0.9167</td><td>0.6314</td><td>0.5676</td><td>0.2416</td></tr><tr><td>EDPC</td><td colspan="2">1.0000 1.0000</td><td colspan="2">0.8818</td><td colspan="2">0.7121</td><td colspan="2">0.9779</td><td colspan="2">0.9722</td><td colspan="2">0.8649 0.6131</td></tr><tr><td>Methods\Datasets</td><td colspan="2">Laiho</td><td colspan="2">Pomeroy1</td><td colspan="2">Shipp</td><td colspan="2">0.8072 UMIST</td><td colspan="2">0.8308 MSRC</td><td colspan="2">MSRA</td></tr><tr><td></td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td></tr><tr><td>DPC SNN-DPC</td><td>0.6216</td><td>0.0422</td><td>0.5588</td><td>0.0858</td><td>0.7792</td><td>0.2273</td><td>0.5635</td><td>0.7729</td><td>0.4857</td><td>0.3822</td><td>0.1178</td><td>0.0015</td></tr><tr><td>SMMP</td><td>0.6486</td><td>0.0433</td><td>0.5294</td><td>0.1442</td><td>0.7143</td><td>0.2300</td><td>0.4800</td><td>0.6857</td><td>0.5143</td><td>0.5092</td><td>0.4408</td><td>0.6045</td></tr><tr><td>KNN-DPC</td><td>0.7838</td><td>0.0000</td><td>0.7353</td><td>0.0000</td><td>0.7532</td><td>0.0000</td><td>0.3826</td><td>0.5412</td><td>0.5095</td><td>0.4940</td><td>0.5442</td><td>0.6037</td></tr><tr><td></td><td>0.7297</td><td>-0.0067</td><td>0.7647</td><td>0.1003</td><td>0.7403</td><td>0.1839</td><td>0.5235</td><td>0.7129</td><td>0.5333</td><td>0.5382</td><td>0.5386</td><td>0.6988</td></tr><tr><td>DB-DPC</td><td>0.7838</td><td>0.000</td><td>0.7353</td><td>0.0000</td><td>0.7352</td><td>0.0000</td><td>0.3843</td><td>0.4748</td><td>0.3714</td><td>0.2828</td><td>0.5643</td><td>0.7300</td></tr><tr><td>FHC-LDP</td><td>0.7297</td><td>0.1530</td><td>0.5588</td><td>-0.0197</td><td>0.5065</td><td>0.1427</td><td>0.6522</td><td>0.7945</td><td>0.5810</td><td>0.5208</td><td>0.4691</td><td>0.6598</td></tr><tr><td>DPC-tSNE</td><td>0.5676</td><td>-0.0066</td><td>0.5588</td><td>-0.0202</td><td>0.6623</td><td>0.1848</td><td>0.5583</td><td>0.7253</td><td>0.7667</td><td>0.6922</td><td>0.6054</td><td>0.7431</td></tr><tr><td>SMMP-tSNE</td><td>0.5135</td><td>0.0218</td><td>0.7353</td><td>0.0000</td><td>0.7143</td><td>0.1223</td><td>0.5009</td><td>0.7020</td><td>0.6143</td><td>0.6166</td><td>0.4953</td><td>0.5595</td></tr><tr><td>CDGC EDPC</td><td>0.5135</td><td>-0.0157 0.2077</td><td>0.5588</td><td>-0.0197 0.3588</td><td>0.7922</td><td>0.2518</td><td>0.7130</td><td>0.8352</td><td>0.5857</td><td>0.6544</td><td>0.5381</td><td>0.6876 0.8640</td></tr><tr><td></td><td colspan="2">0.8378</td><td colspan="2">0.8529</td><td colspan="2">0.8442 0.3175</td><td colspan="2">0.8104 0.8972</td><td colspan="2">0.7000 0.5520</td><td colspan="2">0.5798</td></tr><tr><td>Methods\Datasets</td><td>Armstrong</td><td></td><td></td><td>Nutt</td><td></td><td>Golub2</td><td></td><td>West</td><td></td><td>Pomeroy2</td><td></td><td>Ecoil</td></tr><tr><td>DPC</td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td><td>ACC</td><td>AMI</td></tr><tr><td>SNN-DPC</td><td>0.9028</td><td>0.7130</td><td>0.5909</td><td>0.0040</td><td>0.6944</td><td>0.4889</td><td>0.5510</td><td>0.0172</td><td>0.7619</td><td>0.6338</td><td>0.4554</td><td>0.3009</td></tr><tr><td></td><td>0.8333</td><td>0.4110</td><td>0.5909</td><td>-0.0495</td><td>0.7361</td><td>0.4988</td><td>0.8776</td><td>0.4886</td><td>0.4762</td><td>0.2850</td><td>0.6310</td><td>0.4951</td></tr><tr><td>SMMP</td><td>0.7222</td><td>0.5625</td><td>0.6818</td><td>0.0000</td><td>0.5278</td><td>0.0000</td><td>0.5102</td><td>0.0000</td><td>0.2381</td><td>0.0000</td><td>0.5833</td><td>0.5272 0.4885</td></tr><tr><td>KNN-DPC DB-DPC</td><td>0.6806 0.6944</td><td>0.5451 0.4577</td><td>0.7273 0.6818</td><td>0.0663 0.0000</td><td>0.9444 0.5278</td><td>0.7679 0.0000</td><td>0.8776 0.5102</td><td>0.5590 0.0000</td><td>0.5714 0.2381</td><td>0.4260 0.0000</td><td>0.6696 0.7589</td></table></body></html>

Visualization To demonstrate the effectiveness of the MLP dimensionality reduction guided by pseudo-labels of sub-clusters, we adopt t-SNE to project the UMIST and Gordon datasets into a two-dimensional space (Van der Maaten and Hinton, 2008). Figure 3 presents the visualization results, where Figures 3(a)-(c) show the results for UMIST (15 clusters selected), and Figures 3(d)-(f) correspond to Gordon. In the original feature space (as shown in Figures 3(a) and (d)), the visualization exhibits significant overlaps between different clusters, failing to effectively separate them. Although PCA-generated reduced features provide betterseparated cluster structures (as shown in Figures 3(b) and (e)), they lack compact intra-cluster distributions, as PCA ignores the local structures and relationships among samples. In contrast, our sub-cluster-based dimensionality reduction method (as shown in Figures 3(c) and (f)) effectively separates samples into distinct clusters with more compact intracluster structures, thereby facilitating clustering.

# Ablation Study

To further evaluate the significance of the components within EDPC, we conducted an ablation study with two EDPC variants: EDPC-1, which assigns cluster labels directly through cluster centers without the sub-cluster merging process, and EDPC-2, which skips the MLP-based dimensionality reduction and directly identifies potential cluster centers and merges sub-clusters in the original feature space. Figure 4 illustrates the ACC performance of EDPC and its variants across different datasets. The following conclusions can be drawn: (1) The performance of EDPC-2 surpasses that of EDPC-1, underscoring the significance of the hierarchical label assignment strategy in EDPC for addressing varying density and multi-peak issues. (2) EDPC significantly outperforms EDPC-2, demonstrating that leveraging pseudo-labels from sub-clusters for dimensionality reduction effectively preserves local structural information among data points, thereby facilitating clusterability.

![](images/474323b451445c4b0a0fd38d2191c9537fe3206694e87eecefc8dfe2bf15acf1.jpg)  
Figure 5: ACC with different $\lambda$ and $\beta$ on different datasets.

![](images/69cabbcc2b7235ade6ce96b5491ba993b5df52b6140e58075189aaf0b692a7be.jpg)  
Figure 6: The ACC of EDPC with different values of $\phi$ .

# Parameter Sensitivity

The EDPC incorporates two crucial intrinsic parameters, i.e., $\lambda$ and $\beta$ , which are pivotal to the selection of potential cluster centers during the MLP dimensionality reduction process. To assess the impact of these parameters on clustering effectiveness, we show clustering accuracy under different $\lambda$ and $\beta$ settings in Figure. 5. Specifically, $\lambda$ and $\beta$ are tuned from 0.1 to 1.0 in increments of 0.1 to generate diverse sub-clusters. We find that EDPC performs better when both $\lambda$ and $\beta$ are greater than 0.5, demonstrating that properly tuning the thresholds based on local density and density-related distance can improve the selection of potential cluster centers and further enhance the performance.

Meanwhile, the value of $\phi$ influences $D _ { K M D }$ , which affects the distance between clusters, ultimately impacting the clustering results. Here, we vary the value of $\phi$ in the range of $\{ 1 , 5 , \bar { 1 } 0 , 2 0 , 4 0 , 1 0 0 \}$ to analyze the impacts on the performance. Figure. 6 displays the ACC of EDPC with varied $\phi$ on four datasets, from which we can find EDPC achieves better performance when $\phi$ is smaller than 20. This indicates that better performance can be obtained when $\phi \in [ 5 , 2 0 ]$ . For simplicity, we suggest setting $\phi = 1 0 \$ for promising results in real applications.

# Conclusion

In this paper, an Enhanced Density Peak Clustering (EDPC) is presented to break the dilemma of existing DPC methods in high-dimensional scenarios, which creatively uses the clustering pseudo-labels to guide MLP-based dimensionality reduction, thereby learning low-dimensional representation from high-dimensional data. In this way, EDPC can learn the low-dimensional embedding with the locality preservation and the discrimination enhancement, significantly improving the performance in high-dimensional scenarios. Meanwhile, EDPC can fully leverage the combination of average densities and density-related distances to select the potential cluster centers that properly balance the representative peaks of different density regions. Additionally, EDPC adopts a hierarchical technique to iteratively assign labels to samples, effectively mitigating the possible cascading errors from initial misassignments. Extensive experiments fully validate the effectiveness and superiority of EDPC against existing DPC methods.

# Acknowledgments

This work was supported in part by the ‚ÄùPioneer‚Äù and ‚ÄùLeading Goose‚Äù R&D Program of Zhejiang Province under Grant 2023C01022, in part by the National Natural Science Foundation of China under Grants 62306171 and 62306282, in part by the Science and Technology Major Project of Shanxi Province under Grant 202201020101006.