# Contrastive Multi-view Subspace Clustering via Tensor Transformers Autoencoder

Qianqian Wang1\*, Zihao Zhang1, Wei Feng2, Zhiqiang Tao3, Quanxue Gao1

1School of Telecommunications Engineering, Xidian University, Xi’an, Shaanxi, China, 710071 2School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, Shaanxi, China, 710049 3School of Information, Rochester Institute of Technology, Rochester, NY, USA, 14623 qqwang@xidian.edu.cn, zihaoz2021@stu.xidian.edu.cn, zhiqiang.tao@rit.edu, weifeng.ft@xjtu.edu.cn, qxgao $@$ xidian.edu.cn

# Abstract

Multi-view clustering aims to identify consistent and complementary information across multiple views to partition data into clusters, emerging as a popular unsupervised method for multi-view data analysis. However, existing methods often design view-specific encoders to extract distinct features from each view, lacking exploration of their complementarity. Additionally, current contrastive-based multi-view clustering methods may lead to erroneous negative sample pairs conflicting with the clustering objective. To address these challenges, we propose a novel Contrastive Multi-view Subspace Clustering via Tensor Transformers Autoencoder (TTAE). On the one hand, it facilitates information exchange between views by tensor transformers autoencoder, thereby enhancing complementarity. On the other hand, It learns a consistent subspace with a self-expression layer. Meanwhile, adaptive contrastive learning helps to provide more discriminative features for the self-expression learning layer, and the self-expression learning layer in turn supervises contrastive learning. Moreover, our method adaptively selects positive and negative samples for contrastive learning to mitigate the impact of inappropriate negative sample pairs. Extensive experiments on several multi-view datasets demonstrate the effectiveness and superiority of our model.

# Introduction

Owing to the rapid development of information technology, multi-view data, which comprise multiple modalities, have become increasingly ubiquitous, such as in applications like instant music videos. The vastness and complexity of multi-view data make it challenging to obtain reliable data labels, resulting in significant obstacles for data processing. As a consequence, Multi-view Clustering (MVC) has gained considerable attention (Zhou et al. 2024; Huang et al. 2021; Cao et al. 2015). MVC is an unsupervised method that effectively leverages both inter-view and intraview statistical properties to partition data into meaningful clusters. MVC has found wide application across a variety of domains. For example, in social network analysis, combining views such as user profiles, social relationships, and behavioral records enables a deeper understanding of user behavior (Fang et al. 2023). In image processing, integrating multiple views—such as color histograms and texture features—facilitates more nuanced content analysis, leading to improved clustering performance (Wang et al. 2022c). In data mining, merging features from diverse views can uncover hidden associations and trends, facilitating the extraction of valuable insights while reducing computational complexity. Prior research on MVC can be broadly categorized based on architecture: traditional MVC methods (Tao et al. 2017; Zhang et al. 2018, 2023a) and deep MVC methods (Lin et al. 2021; Zhao et al. 2024).

![](images/29e2ba2d3fec36583110a3189cd914b2785d32f2866b8e1a062cd6eafd5f4b87.jpg)  
Figure 1: The flowchart of the Tensor Attention Layer. Due to the parallel computation of multiple attention heads, we depict only a single head $h$ . These heads are concatenated after the same transformation i.e., to obtain the strengthened representation $\hat { \mathcal { Z } }$ .

Traditional MVC methods include Non-negative Matrix Factorization (NMF) clustering (Li et al. 2023; Zong et al. 2017), graph constraint-based methods (Orecchia and Zhu 2014; Zhang et al. 2019), and subspace clustering methods (Cao et al. 2015; Li et al. 2019). However, these traditional methods primarily rely on shallow and linear embedding functions to capture the clustering structure, which limits their ability to explore the deep, nonlinear relationships within multi-view data. Although some approaches (Akaho 2001; Tzortzis and Likas 2012) attempt to address this limitation by incorporating multi-kernel learning strategies to capture nonlinear features, their clustering performance remains suboptimal.

In recent years, the emergence of deep neural networks (DNNs) has led to remarkable advancements in representation learning (Bengio, Courville, and Vincent 2013). Many deep MVC methods (Ji et al. 2017; Wang et al. 2022b; Xie et al. 2020) have demonstrated impressive clustering performance, aided by feature extraction networks. Among these methods, deep multi-view subspace clustering (MVSC) has gained significant attention. This approach assumes that different modalities reside in distinct subspaces and aims to identify a shared subspace across multi-view data by maximizing the mutual agreement between different modalities. For example, Abavisani et al. (2018) explored various fusion strategies and proposed an unsupervised deep multimodal subspace clustering method (DMSC) based on convolutional neural networks.

Although these models have made significant strides in deep MVC, several challenges remain: 1) Many existing deep MVSC algorithms (e.g., (Abavisani and Patel 2018; Wang et al. 2020)) typically assign each view a viewspecific encoder and impose constraints to maximize interview consistency. However, the features extracted by this approach often lack inter-view interaction, causing them to miss complementary information across views. 2) To explore the discriminative structure of multi-view data, some self-supervised methods (Wang et al. 2021, 2024) have been proposed, using clustering labels as supervised information to guide subspace learning. However, the inaccuracy of these cluster labels severely limits improvements in clustering performance. 3) Some current deep MVC algorithms incorporate contrastive learning to acquire discriminative features. Yet, these methods (e.g., (Xu et al. 2022)) typically treat pairs of corresponding views as positive pairs and noncorresponding samples as negative pairs. This strategy can lead to noisy contrastive learning, potentially pushing samples from the same cluster farther apart, which contradicts the goal of clustering.

To address these issues, we propose a novel method called Contrastive Multi-view Subspace Clustering via Tensor Transformers Auto-Encoder (i.e., TTAE). TTAE effectively integrates Tensor Transformers Autoencoder, a SelfExpression Layer, and an Adaptive Contrastive Learning Module, forming a unified framework that facilitates both consistent and complementary learning. Specifically, TTAE incorporates the Tensor Attention Layer (illustrated in Figure 1), which captures feature information across views and fosters complementary learning. Additionally, TTAE employs contrastive learning to extract discriminative features. To further enhance the process, the Self-Expression Layer provides pairwise affinities, which help capture data point relationships. This layer also serves as a self-supervised method, mitigating the noise issues typically encountered in contrastive learning. The main contributions of TTAE are summarized below:

• We propose a novel multi-view subspace clustering method based on the Tensor Transformers Autoencoder, which enables effective feature extraction while facilitating cross-view information interaction for complementary learning. • Our TTAE Module introduces a novel approach for con

structing positive and negative pairs. By leveraging the self-expression coefficient matrix, it adaptively generates positive and negative samples, enabling the exploration of both consistent and discriminative structures in unlabeled multi-view data. • We conduct extensive experiments on several benchmark datasets, demonstrating that TTAE significantly improves clustering performance compared to several baseline methods.

# Related Work

# Deep Multi-view Clustering

Since multiple views describe the same object, they inherently contain shared knowledge. The challenge in Multiview Clustering (MVC) lies in extracting this shared information through consistent learning and leveraging it for clustering tasks. A common approach to achieving consistency is by maximizing the correlation between views to extract shared components. Canonical Correlation Analysis (CCA), and its deep variant, Deep CCA (Andrew et al. 2013), are representative methods that non-linearly transform data to obtain highly correlated representations. Building on this consistency principle, deep Multi-view Subspace Clustering (MVSC) methods aim to identify a common subspace across all views, enabling coherent clustering. These methods have demonstrated strong performance in various applications (Wang et al. 2020, 2023). In addition to consistency, different views also contain complementary information unique to each modality. While consistency focuses on shared knowledge, complementarity emphasizes the distinct perspectives each view provides. Recent deep MVSC methods (Zhang et al. 2023b; Wang et al. 2022a; Zhang et al. 2024a) aim to leverage both aspects—capturing both shared and complementary information. Unlike these methods, TTAE focuses on the interaction of information across multiple views and the discovery of discriminative features. It achieves this through the integration of a Tensor Transformers Autoencoder and an Adaptive Contrastive Learning Module, which together facilitate both consistent and complementary learning for improved clustering performance.

# Contrastive Learning

Contrastive learning successfully enhances clustering performance as an effective technique for discriminative feature learning, leading to the development of specialized methods tailored to different data structures, such as images (Li et al. 2021) and graphs (Zhong et al. 2021). For example, Graph Contrastive Clustering (GCC) (Zhong et al. 2021) utilizes KNN-based graph contrastive learning to shift from instance-level consistency to cluster-level consistency, further improving clustering performance. However, due to the unsupervised nature of clustering, implementing labelguided contrastive learning (Khosla et al. 2020) presents a challenge, as it increases the risk of treating similar samples as negative pairs. To address this issue, various clustering methods have incorporated self-supervised signals during training. These approaches address the problem from multiple angles, including: 1) guiding the learning process through pseudo-labeling (Jing et al. 2021; Feng et al. 2024), 2) controlling the quantity of sample pairs to mitigate errors (Yang et al. 2023), 3) adjusting the weights of samples based on confidence (Yan et al. 2023; Liu et al. 2024; Lu et al. 2024), and 4) refining the boundary conditions for constructing sample pairs (Tang and Liu 2022). A notable aspect of these methods is the emphasis on similarity metrics between samples. Unlike previous approaches, TTAE uniquely employs the self-expression coefficient matrix to effectively mitigate false-negative samples, achieving more accurate clustering results.

# The Proposed Method

TTAE is composed of three parts: Tensor Transformers Autoencoder, Self-expression Layer, and Adaptive Contrastive Learning Module as depicted in Figure 2. In addition, Table 1 records the important notation of the paper.

Table 1: Main notations in this paper.   

<html><body><table><tr><td>Notion</td><td>Definition</td></tr><tr><td>m</td><td>Number of samples</td></tr><tr><td>V</td><td>Number of views</td></tr><tr><td>k</td><td>Number of Nearest samples taken</td></tr><tr><td>f</td><td>Number of cluster</td></tr><tr><td>du</td><td>Feature dimensions of original v-th data</td></tr><tr><td>OU</td><td>Feature dimensions of latent v-th data</td></tr><tr><td>X,∈Rmxdu</td><td>Input data matrix</td></tr><tr><td>Z∈Rmxov</td><td>Latent features matrix of u-th data</td></tr><tr><td>ZERmxVx0u</td><td>Latent features tensor</td></tr><tr><td>ZERmxVxou</td><td>Strengthened features tensor</td></tr><tr><td>Z∈RmxOv</td><td>Strengthened features matrix of v-thdata</td></tr><tr><td>S</td><td>Consistent representation matrix</td></tr><tr><td>C</td><td>Affinity matrix C = ¹(|S|+ |S|T)</td></tr></table></body></html>

# Tensor Transformers Autoencoder

Without loss of generality, we consider two views as examples. Given two heterogeneous views $\begin{array} { r l r } { { \bf X } _ { 1 } } & { { } = } & { \{ x _ { 1 } ^ { 1 } , x _ { 1 } ^ { 2 } , x _ { 1 } ^ { 3 } , . . . , x _ { 1 } ^ { m } \} ^ { T } \quad \in \quad \bar { \mathbb { R } } ^ { m \times d _ { 1 } } } \end{array}$ , $\begin{array} { r l } { \mathbf { X } _ { 2 } } & { { } = } \end{array}$ $\{ x _ { 2 } ^ { 1 } , x _ { 2 } ^ { 2 } , x _ { 2 } ^ { 3 } , . . . , { x _ { 2 } ^ { m } } \} ^ { T } ~ \in ~ \mathbb { R } ^ { \bar { m } \times d _ { 2 } }$ , where $m$ is the number of samples, $d _ { v }$ denotes the corresponding dimensions of the $\boldsymbol { v }$ -th views. To handle the high-dimensional and complex nature of raw data, we begin by transforming individual view data into a more compact representation through a feature extraction layer. This step reduces the dimensionality of the input data $\mathbf { X } _ { v }$ and uncovers latent low-dimensional features $\mathbf { Z } _ { v }$ . Specifically, the feature extraction process follows the mapping: $\mathbf { Z } _ { v } = \mathbf { E } _ { v } ( \mathbf { X } _ { v } ; \boldsymbol { \theta } _ { e _ { v } } ) \in \mathbb { R } ^ { m \times o _ { v } }$ , where $\theta _ { e _ { v } }$ denotes the parameters of the encoder $\mathbf { E } _ { v }$ , $o _ { v }$ represents the output dimension.

Through the feature extraction layer, it adeptly preserves unique information from each perspective, laying the groundwork for subsequent consistency learning—a measure often employed by many prominent clustering methods (Xu et al. 2022; Zhang et al. 2024b). However, as each view may provide distinct clustering value information, the challenge (Wang et al. 2024; Luo et al. 2018; Liu et al. 2023) lies in capturing complementary information across views to enhance clustering performance. Motivated by this, we explore inter-view interactions to capture complementary information within the prevailing Transformer framework (Vaswani et al. 2017).

Given $\mathbf { X } _ { v }$ and its corresponding representation $\mathbf { Z } _ { v }$ for view $v$ , we expand the features $\mathbf { Z } _ { v }$ from all views into a three-dimensional tensor $\mathcal { Z } \in \mathcal { R } ^ { m \times V \times o _ { v } }$ , which is then passed into the subsequent attention module. Assuming the presence of attention heads $( H )$ and their corresponding linear transformation weights for query $( \mathbf { W } ^ { \mathcal { Q } } )$ , key $( \mathbf { W } ^ { \kappa } )$ , and value $( \mathbf { W } ^ { \nu } )$ , we take the input tensor $\mathcal { Z }$ , map it to feature representations, and perform the associated dimensional transformations. The resulting representations $\{ \mathcal { Q } , \kappa , \nu \}$ belong to $\mathbb { R } ^ { m \times H \times V \times d _ { H } }$ , where the dimension $d _ { H }$ is determined as $d _ { H } = o _ { v } / H$ . Computing multiple attention heads in parallel enables the division of feature dimensions into $o _ { v } / H$ , which enhances the effectiveness of feature representations. For each attention head $h$ , the following operation is performed:

$$
\mathscr { R } _ { h } = s o f t m a x ( \Upsilon _ { \{ : , : , i , j \} } ( \mathscr { Q } _ { h } , \mathscr { K } _ { h } ^ { T } ) / \sqrt { d _ { h } } ) ,
$$

where the operator $\Upsilon _ { : , : , i , j } ( \cdot , \cdot )$ denotes matrix multiplication along the last two dimensions. By leveraging the parallel computation of multiple attention heads, the attention scores Rm×H×V ×V are obtained, capturing the similarity relationships among views.

Following this, the output features of the attention module are computed as $\bar { \mathcal { Z } } = \dot { \Upsilon } _ { ( : , : , i , j ) } ( \pmb { \mathscr { R } } , \pmb { \mathscr { V } } ) \in \mathbb { R } ^ { m \times H \times V \times d _ { H } }$ , which represents the aggregated output from multiple attention heads. This representation undergoes further linear transformations and is refined through a fully connected layer to produce a strengthened representation:

$$
\begin{array} { r } { \hat { \boldsymbol { \mathcal { Z } } } = \operatorname { C o n c a t } ( \bar { \mathcal { Z } } _ { 1 } , \bar { \mathcal { Z } } _ { 2 } , . . . , \bar { \mathcal { Z } } _ { H } ) \in \mathbb { R } ^ { m \times V \times o _ { v } } . } \end{array}
$$

From this strengthened representation, the enhanced feature of each view $\bar { \mathbf Z } _ { v } ^ { \mathbf { \Delta } } \in \mathbb { R } ^ { m \times o _ { v } ^ { \mathbf { \Delta } } }$ can be extracted by decomposing $\hat { \mathcal { Z } }$ . This process is critical for facilitating effective multiview interactions.

In addition, the decoder is responsible for performing reconstruction tasks. The modality-specific information, after undergoing view interactions, is passed to the corresponding modality decoders to reconstruct precise modality representations, thereby guiding the learning dynamics of Tensor Transformers Autoencoders. Specifically, the objective is to maximize the coherence between the original and reconstructed data while ensuring the effectiveness of the latent low-dimensional features within the hidden layers. This is typically achieved by minimizing the following loss function:

$$
\mathcal { L } _ { r e } = \sum _ { v = 1 } ^ { V } | | \mathbf { X } _ { v } - \hat { \mathbf { X } } _ { v } | | _ { F } ^ { 2 } ,
$$

where $\hat { \mathbf { X } } _ { v }$ is the reconstructed view data and $\| \cdot \| _ { F }$ stands for the Frobenius paradigm.

# Self-expression Layer

The self-expression layer is employed to acquire a selfexpression coefficient matrix exhibiting a distinct cluster structure. This matrix reflects the interrelations of data points within the subspace, where a sample is expressed as a linear combination of other samples in the same subspace. The self-expression layer takes $\bar { \mathbf { Z } } _ { v }$ as input and enforces $\bar { \mathbf Z } _ { v } = \mathbf S \bar { \mathbf Z } _ { v }$ to learn a shared self-expression coefficient matrix S for each view. To promote this self-expression property, the self-expression loss is defined as follows:

![](images/4fff79c3ac460629ac3dff41aebb28a079f8d92fff3a78bf4ea15d56bd007675.jpg)  
Figure 2: The framework of our proposed method (TTAE). For presentation purposes. The multi-view data $\mathbf { X } _ { v }$ , after passing through the feature extraction layer, yields latent features $\mathbf { Z } _ { v }$ , which encapsulate an underlying latent cluster structure. These features $\mathbf { Z } _ { v }$ are concatenated into a tensor $\mathcal { Z }$ and fed into the Tensor Attention Layer to facilitate cross-view interactions and obtain enhanced features $\hat { \mathcal { Z } }$ . Within each view, $\bar { \mathbf Z } _ { v }$ undergoes a self-expression layer to acquire a self-expression coefficient matrix S. In this case, S constructs relevant nearest neighbor graphs to assist in the selection of positive and negative sample pairs, achieving adaptive contrastive learning.

$$
\begin{array} { c } { { \displaystyle \mathcal { L } _ { s e } = \sum _ { v = 1 } ^ { V } \| \bar { \mathbf Z } _ { v } - \mathbf S \bar { \mathbf Z } _ { v } \| _ { F } ^ { 2 } + \| \mathbf S \| _ { F } ^ { 2 } } } \\ { { s . t . , d i a g ( \mathbf S ) = 0 , } } \end{array}
$$

where $d i a g ( \cdot )$ refers to extracting the diagonal elements of the matrix. To avoid trivial solution $\mathbf { S } = \mathbf { I }$ , we constrain the diagonal elements of $\mathbf { S }$ to zero, i.e., $d i a g ( \mathbf { S } ) \ = \ 0$ . Throughout network optimization, S can be interpreted as an adjacency matrix, representing the similarity relationships among samples.

# Adaptive Contrastive Learning Module

By minimizing both $\mathcal { L } _ { r e }$ and $\mathcal { L } _ { s e }$ , the model can effectively capture low-dimensional latent features, along with the embedded cluster structure information encoded in S. However, due to the reconstruction task often yielding broad representations, insufficient for distinguishing relationships among clusters, it can adversely impact subsequent self-expression learning (Wang et al. 2021). Therefore, it requires further fine-tuning to apply to clustering tasks. Simultaneously, we recognize the consistency of high-level semantic information across different views. Hence, we introduce contrastive learning (Chen et al. 2020) to acquire discriminative features. In particular, to circumvent conflicts between the learning of reconstruction space and consistent common semantics (Xu et al. 2022), we engage in contrastive learning on the features obtained by passing $\mathbf { Z } _ { v }$ through an MLP. This approach enhances the discriminative feature extraction of the view-specific encoder.

Given a sample $\mathbf { x } _ { 1 } ^ { i } \in \mathbf { X } _ { 1 }$ of object $i$ and its corresponding representation $\mathbf { z } _ { 1 } ^ { i } \in \mathbf { \bar { Z } } _ { 1 }$ , we construct positive pairs using $\mathbf { z } _ { 1 } ^ { i }$ and the feature representation of object $i$ in another view, $\mathbf { z } _ { 2 } ^ { i }$ , i.e., $( \mathbf { z } _ { 1 } ^ { i } , \mathbf { z } _ { 2 } ^ { i } )$ . Negative pairs are formed by combining $\mathbf { z } _ { 1 } ^ { i }$ with all feature samples other than $\mathbf { z } _ { 1 } ^ { i }$ and $\mathbf { z } _ { 2 } ^ { i }$ , i.e., $( \mathbf { z } _ { 1 } ^ { i } , \mathbf { z } _ { 2 } ^ { k } )$ for $k \in [ 1 , m ]$ and $( \mathbf { z } _ { 1 } ^ { i } , \mathbf { z } _ { 1 } ^ { k } )$ for $k \neq i$ . All these sample pairs are uniformly processed through contrastive learning. For similarity measurement, we adopt the cosine similarity sim(·, ·), defined as sim(z1, z2) = zz11 zz22 .

Combining sample selection with cross-entropy, contrastive learning is achieved by minimizing the following loss function:

$$
\begin{array} { r l } & { \mathcal { L } _ { c l } = \displaystyle D ( \mathbf { Z } _ { 1 } , \mathbf { Z } _ { 2 } ) = \sum _ { i = 1 } ^ { m } } \\ & { \displaystyle \mathit { l o g } \frac { e x p \left( s i m ( \mathbf { z } _ { 1 } ^ { i } , \mathbf { z } _ { 2 } ^ { i } ) ) \right) } { \displaystyle \sum _ { j = 1 } ^ { m } \left\{ e x p ( s i m ( \mathbf { z } _ { 1 } ^ { i } , \mathbf { z } _ { 2 } ^ { j } ) ) + \sum _ { v = 1 } ^ { 2 } \mathbb { L } _ { ( j \neq i ) } e x p [ s i m ( \mathbf { z } _ { v } ^ { j } , \mathbf { z } _ { v } ^ { i } ) ] \right\} } , } \end{array}
$$

where $\mathbb { L } _ { ( j \neq i ) } \in \{ 0 , 1 \}$ is an indicator function evaluating to 1 if $j \neq i$ .

However, directly selecting other samples as negative sample pairs inevitably leads to treating samples from the same cluster as negative pairs, which hinders the ability to learn discriminative features. To address this issue, some unsupervised approaches (Jing et al. 2021; Trosten et al. 2021) utilize pseudo-labels obtained from clustering algorithms (e.g., k-means) to introduce supervisory signals. Unlike these methods, We prioritize the similarity between paired data over pseudo-labels as self-supervised signals, as it aligns more closely with the essence of contrastive learning. Given the reliability of the self-expression learning layer and the consistency embedded in the self-expression coefficient matrix S across views, it is reasonable to assume that the self-expression attributes of the same cluster can assist in identifying these affinity pairs. Specifically, for a given anchor, $k$ affinity pairs $k$ -neighbor graph) will be excluded as the possibility of negative sample pairs, so the Eq. (5) becomes:

Table 2: Summary of Datasets.   

<html><body><table><tr><td>Dataset</td><td>Size (Categories)</td><td>Dimensions</td></tr><tr><td>Fashion-MNIST</td><td>2,000 (10)</td><td>28× 28</td></tr><tr><td>COIL-20</td><td>1,440 (20)</td><td>64 × 64</td></tr><tr><td>FRGC</td><td>2,462 (20)</td><td>32 ×32</td></tr><tr><td>Youtube-Faces(YTF)</td><td>2,000 (41)</td><td>55 × 55</td></tr><tr><td>ALOI-100</td><td>10,800 (100)</td><td>77, 125</td></tr><tr><td>Noisy-MNIST</td><td>20,000 (10)</td><td>784,784</td></tr></table></body></html>

$$
\begin{array} { l } { { \displaystyle { \cal L } _ { a c l } = { \cal D } ( { \bf Z } _ { 1 } , { \bf Z } _ { 2 } ) = \sum _ { i = 1 } ^ { m } l o g } \ ~ } \\ { { \displaystyle \frac { e x p \big ( s i m ( { \bf z } _ { 1 } ^ { i } , { \bf z } _ { 2 } ^ { i } ) \big ) \big ) } { \sum _ { j = 1 , j \notin { \mathbb P } ( i , k ) } ^ { m } \big \{ e x p \big ( s i m ( { \bf z } _ { 1 } ^ { i } , { \bf z } _ { 2 } ^ { j } ) \big ) + \sum _ { v = 1 } ^ { 2 } { \mathbb { L } } _ { ( j \ne i ) } e x p \big [ s i m ( { \bf z } _ { v } ^ { j } , { \bf z } _ { v } ^ { i } ) \big ] \big \} } } , } \end{array}
$$

where $\mathbb { L } _ { ( j \neq i ) } \in \{ 0 , 1 \}$ is an indicator function that evaluated as 1 if $j \neq i , \mathbb { P } ( i , k )$ refers to the $k$ nearest sample points to anchor point $i$ . The evolution of Eq. (5) to (6) means that the affinity between samples will be used to supervise its representation learning to learn more discriminative features. When integrated with its well-designed sequence of training steps, it effectively alleviates the potential noise effects arising from homogeneous samples.

# Overall Loss Function and Optimization

Based on the above network structure, we adopt a joint loss composed of reconstruction loss $\mathcal { L } _ { r e }$ , adaptive contrastive learning loss $\mathcal { L } _ { a c l }$ , and self-expression loss $\mathcal { L } _ { s e }$ , and the total loss $\mathcal { L } _ { a l l }$ can be formalized as below:

$$
\begin{array} { r } { \mathcal { L } = \mathcal { L } _ { r e } + \lambda _ { 1 } \mathcal { L } _ { s e } + \lambda _ { 2 } \mathcal { L } _ { a c l } . } \end{array}
$$

We train the model in two stages: pre-training and overall training. Detailed steps are outlined in Algorithm 1. Pretraining network: TTAE uses reconstruction loss $\mathcal { L } _ { r e }$ and self-expression loss $\mathcal { L } _ { s e }$ for pre-training. This step accelerates training, produces robust representations $\mathbf { Z }$ for contrastive learning, and generates reliable guidance S for improved learning stability. Overall training network: The network is initialized with pre-trained parameters and optimized using Eq. (7). During training, the self-expression matrix is iteratively refined to support adaptive contrastive learning. The final shared matrix $\mathbf { S }$ is used to compute the affinity matrix: $\begin{array} { r } { \mathbf { C } = \frac { 1 } { 2 } ( \mathbf { S } + \mathbf { S } ^ { T } ) } \end{array}$ Spectral clustering is then applied to $\mathbf { C }$ to produce the final clustering results.

Input: Multi-view data $\{ \mathbf { X } _ { v } \} _ { v = 1 } ^ { V }$ , hyperparameters $\lambda _ { 1 } , \lambda _ { 2 }$ , $k$ , and epochs $e _ { 1 } , e _ { 2 }$ .   
1: Stage 1: Pre-training (Epochs 1 to $e _ { 1 }$ ):   
2: for $t = 1$ to $e _ { 1 }$ do   
3: Compute latent feature $\mathbf { Z } _ { v }$ and $\mathcal { Z }$   
4: Compute strengthened feature $\hat { \mathcal { Z } }$ and $\hat { \mathbf { Z } } _ { v }$   
5: Compute self-expression $\mathbf { S } \hat { \mathbf { Z } } _ { v }$ and reconstruct $\hat { \mathbf { X } } _ { v }$ 6: Update parameters with $\mathcal { L } _ { r e }$ and $\mathcal { L } _ { s e }$ .   
7: end for   
8: Stage 2: Fine-tuning (Epochs $e _ { 1 } + 1$ to $e _ { 2 }$ ):   
9: for $t = e _ { 1 } + 1$ to $e _ { 2 }$ do   
10: Repeat latent and strengthened feature computation. 11: Construct k-nearest neighbor graph $\mathbb { P } ( i , k )$ using S. 12: Reconstruct data and update parameters with $\mathcal { L } _ { a l l }$ . 13: end for   
14: Compute affinity matrix $\begin{array} { r } { \mathbf { C } = \frac { 1 } { 2 } ( \mathbf { S } + \mathbf { S } ^ { T } ) } \end{array}$ .   
Output: Return predictions $P$ from spectral clustering on affinity matrix C.

# Experienment

# Experiment Setup

Dataset Settings: We evaluate our method on six benchmark datasets: Fashion-MNIST (Xiao, Rasul, and Vollgraf 2017), COIL-20 (Nene, Nayar, and Murase 1996), FRGC (Yang, Parikh, and Batra 2016), Youtube-Faces (YTF) (Wolf, Hassner, and Maoz 2011), ALOI-100 (Wang, Yang, and Liu 2019), and Noisy-MNIST (Hardoon, Szedmak, and Shawe-Taylor 2004). For large datasets, a random subset was selected, considering the runtime efficiency of most comparative algorithms. Each dataset comprises two complementary views, such as original images and edge maps. However, some datasets deviate from this general structure. For instance, ALOI-100 utilizes RGB features as the first view and Haralick texture features as the second. Similarly, Noisy-MNIST adopts grayscale images for the first view and noisy images with a Gaussian background for the second. Table 2 provides a comprehensive summary of the datasets, including key details such as size, category distribution, and dimensionality.

Implementation details: To ensure a fair comparison, the feature extraction backbone of TTAE is aligned with the majority of methods. Classic subspace algorithms, such as DMSC, employ their own CNNs for feature extraction, and we adopted a similar approach for consistency. For datasets like Fashion-MNIST, COIL-20, FRGC, and YTF, we use a three-layer convolutional network. For vectorbased datasets, such as ALOI-100 and Noisy-MNIST, we referenced methods like MFLVC and DCCA, applying a three-layer fully connected network with dimensions $\{ \dim .$ , $1 0 2 4 , 1 0 2 4 , 1 0 2 4 , 1 2 8 \}$ , where dim denotes the input dimension of the dataset. This ensures a reasonable and fair comparison across different datasets. All experiments were conducted on a Windows 11 platform equipped with NVIDIA 4090 Graphics Processing Units (GPUs) and 64 GB of memory. The ADAM optimizer (Kingma and Ba 2014) was employed with a default learning rate set to 0.0001.

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="2">Fashion-MNIST</td><td colspan="2">COIL-20</td><td colspan="2">FRGC</td><td colspan="2">YTF</td><td colspan="2">ALOI-100</td><td colspan="2">Noisy-MNIST</td></tr><tr><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td></tr><tr><td>K-means (HartiganandWong1979)</td><td>51.27</td><td>49.99</td><td>57.49</td><td>73.22</td><td>23.62</td><td>27.12</td><td>56.01</td><td>75.23</td><td>46.27</td><td>68.43</td><td>54.64</td><td>48.62</td></tr><tr><td>CC (Li et al. 2021)</td><td>49.53</td><td>46.75</td><td>65.27</td><td>76.27</td><td>34.62</td><td>43.08</td><td>58.27</td><td>77.65</td><td>65.31</td><td>84.12</td><td>79.42</td><td>72.16</td></tr><tr><td>LALMVC (Xie et al. 2019)</td><td>57.20</td><td>59.31</td><td>64.79</td><td>76.83</td><td>49.68</td><td>57.27</td><td>40.85</td><td>49.60</td><td>49.80</td><td>73.71</td><td>46.94</td><td>56.90</td></tr><tr><td>SMVSC (Sun et al.2021)</td><td>65.45</td><td>58.97</td><td>72.64</td><td>81.67</td><td>50.73</td><td>58.85</td><td>62.00</td><td>80.14</td><td>50.69</td><td>73.70</td><td>61.76</td><td>51.67</td></tr><tr><td>EOMSC-CA(Liuetal.2022)</td><td>52.70</td><td>56.72</td><td>57.43</td><td>74.69</td><td>31.72</td><td>32.45</td><td>62.05</td><td>83.46</td><td>24.21</td><td>58.24</td><td>59.96</td><td>50.03</td></tr><tr><td>DCCA (Andrew et al.2013)</td><td>52.74</td><td>53.82</td><td>55.76</td><td>64.91</td><td>22.91</td><td>24.75</td><td>45.19</td><td>60.35</td><td>49.41</td><td>69.62</td><td>85.53</td><td>89.44</td></tr><tr><td>DMSC (Abavisani and Patel 2018)</td><td>59.55</td><td>65.07</td><td>74.10</td><td>86.82</td><td>72.83</td><td>80.96</td><td>62.80</td><td>80.16</td><td>53.40</td><td>73.43</td><td>69.43</td><td>61.72</td></tr><tr><td>CMSC-DCCA (Gao et al.2020)</td><td>62.95</td><td>68.33</td><td>82.64</td><td>91.45</td><td>70.80</td><td>78.55</td><td>66.15</td><td>82.67</td><td>74.60</td><td>87.72</td><td>87.88</td><td>82.20</td></tr><tr><td>DMJC(Xie et al.2020)</td><td>61.41</td><td>63.41</td><td>72.99</td><td>81.58</td><td>44.07</td><td>59.79</td><td>61.15</td><td>77.40</td><td>50.99</td><td>70.82</td><td>57.16</td><td>62.44</td></tr><tr><td>DMSC-UDL (Wang et al. 2020)</td><td>65.45</td><td>68.34</td><td>79.24</td><td>89.00</td><td>73.19</td><td>78.19</td><td>66.75</td><td>82.52</td><td>57.83</td><td>74.39</td><td>72.06</td><td>68.97</td></tr><tr><td>DMIM (Mao et al. 2021)</td><td>56.10</td><td>58.06</td><td>OOM</td><td>0OM</td><td>34.12</td><td>41.01</td><td>57.10</td><td>74.80</td><td>OOM</td><td>OOM</td><td>50.79</td><td>48.17</td></tr><tr><td>MFLVC (Xu et al. 2022)</td><td>55.05</td><td>54.63</td><td>73.12</td><td>82.90</td><td>45.79</td><td>52.28</td><td>56.75</td><td>70.24</td><td>74.90</td><td>85.70</td><td>92.82</td><td>88.79</td></tr><tr><td>D2MVSC (Wang et al. 2024)</td><td>63.80</td><td>67.73</td><td>83.77</td><td>92.06</td><td>73.46</td><td>80.29</td><td>66.28</td><td>82.88</td><td>60.91</td><td>75.34</td><td>72.15</td><td>68.55</td></tr><tr><td>TTAE</td><td>65.75</td><td>69.36</td><td>84.93</td><td>92.87</td><td>74.45</td><td>82.81</td><td>68.10</td><td>83.54</td><td>79.31</td><td>88.72</td><td>94.53</td><td>90.08</td></tr></table></body></html>

Table 3: The clustering accuracy rate $( \mathsf { A C C } ) ( \% )$ and the normalized mutual information $( \mathrm { N M I } ) ( \% )$ on six datasets. OOM mean out of memory, Best results are highlighted in bold, and the second-best results are underlined.

# Experimental Results

Comparison with Existing Approaches: We compare our model with other popular clustering methods, including two classical single-view clustering, i.e., K-means (Hartigan and Wong 1979), and CC (Li et al. 2021); three traditional multiview clustering, i.e., LALMVC (Xie et al. 2019), SMVSC (Sun et al. 2021), and EOMSC-CA (Liu et al. 2022); eight excellent deep multi-view clustering, i.e., DMJC (Xie et al. 2020), DMSC (Abavisani and Patel 2018), DCCA (Andrew et al. 2013), CMSC-DCCA (Gao et al. 2020), DMSC-UDL (Wang et al. 2020), DMIM (Mao et al. 2021), MFLVC (Xu et al. 2022), D2MVSC (Wang et al. 2024). As k-means and CC can only handle single-view data, we record their bestview clustering performance.

Performance Evaluation: We evaluate the performance of these methods by testing the clustering accuracy (Kuhn 1955) (ACC), normalized mutual information (Xu, Liu, and Gong 2003) (NMI) according to the true labels. Table 3 summarizes the performance of all the methods on the six datasets, with the following findings: 1) Through joint learning of contrastive constraints and self-expression constraints, TTAE achieves the excellent performance of ACC and NMI on all six datasets. 2) Among deep MVSC methods, the proposed model TTAE achieves better results than DMSC, DMSC-UDL, CMSC-DCCA. These methods focus on pursuing more consistent subspace representations, but they ignore the exploration of complementary information, resulting in suboptimal performance. 3) In the case of two large datasets, ALOI-100 and Noisy-MNIST, methods utilizing CCA such as DCCA and CMSC-DCCA achieved suboptimal results. In contrast, contrastive learning methods such as MFLVC and the proposed method, TTAE, achieved relatively better results due to discriminative representation learning. As we accounted for noise in positive and negative sample pairs by considering self-pairwise supervision, TTAE attained optimal clustering performance.

Ablation Study: In this subsection, we conduct a series of experiments to investigate the effect of each component of the TTAE model. On the Fashion-MNIST and YTF datasets, we evaluate the performance of the model without $\mathcal { L } _ { r e }$ , $\mathcal { L } _ { s e }$ and $\mathcal { L } _ { a c l }$ individually. The results are shown in Table 4. One could observe that all loss functions contribute to the improvement of clustering performance, and the combined losses yield optimal clustering performance. Additionally, we explored the impact of the Tensor Attention Layer. Using t-SNE visualization (Van der Maaten and Hinton 2008), we depict the features before and after passing through the Tensor Attention Layer as $\mathbf { z }$ and $\hat { \mathcal { Z } }$ , respectively, as illustrated in Figure 3. Despite the introduction of some unavoidable noisy points into the clean clusters, it is evident that the formed clusters become more concentrated after passing through this module. This enhancement is attributed to the aggregative capacity of cross-view attention.

Table 4: Ablation Study on Fashion-MNIST and YTF dataset in terms of ACC $( \% )$ and NMI $( \% )$ .   

<html><body><table><tr><td></td><td></td><td></td><td colspan="2">Fashion-MNIST</td><td colspan="2">YTF</td></tr><tr><td>Lre</td><td>Lse</td><td>Lacl</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td></tr><tr><td>√</td><td></td><td></td><td>53.50</td><td>52.76</td><td>54.45</td><td>71.56</td></tr><tr><td></td><td>√</td><td></td><td>52.81</td><td>58.06</td><td>60.18</td><td>78.32</td></tr><tr><td></td><td></td><td>√</td><td>51.96</td><td>51.12</td><td>61.10</td><td>77.23</td></tr><tr><td></td><td>√</td><td>√</td><td>58.15</td><td>58.57</td><td>64.16</td><td>80.26</td></tr><tr><td>√</td><td></td><td>√</td><td>61.50</td><td>63.20</td><td>62.95</td><td>77.07</td></tr><tr><td>√</td><td>√</td><td></td><td>62.63</td><td>66.19</td><td>65.25</td><td>81.30</td></tr><tr><td>√</td><td>√</td><td>√</td><td>65.75</td><td>69.36</td><td>68.10</td><td>83.54</td></tr></table></body></html>

Parameters Analysis: For the parameters $\lambda$ , Figure 5 records the effect of $\lambda$ on the Fashion-MNSIT dataset. In our analysis, the self-expression learning parameter $\lambda _ { 1 }$ and the contrastive learning parameter $\lambda _ { 2 }$ are changed in the range of $\{ 0 . 0 1 , 0 . 1 , 1 , 1 0 , 1 0 0 \}$ . When $\lambda _ { 1 } ~ = ~ 1$ and $\lambda _ { 2 } ~ = ~ 0 . 1$ , TTAE achieves the best results on Fashion-MNIST datasets. Clearly, TTAE is robust to $\lambda _ { 2 }$ and an appropriate $\lambda _ { 1 }$ will contribute to achieving optimal clustering performance.

To address the variation in sample size $m$ and the number of classes $f$ across datasets, we propose a dynamic adjustment method for determining the number of nearest neighbors $k$ . Assuming a uniform class distribution, $k$ is defined as $k = \xi _ { f } ^ { \underline { { m } } }$ , where $\xi$ is a proportional hyperparameter that

![](images/d0eed4f97f8e73cfa926e2cdbb1c1be12584e16b477201ba56a7009a7c3c8a95.jpg)  
Figure 3: T-SNE visualization on the Noisy-MNIST with $\mathbf { N M I } = 0 . 8 9$ . The left image (a) corresponds to latent feature $\mathbf { Z }$ , while the right image (b) corresponds to strengthened feature $\hat { \mathbf { Z } }$ .

0.70   
0.6789   
0.645 Baseline ACC Baseline NMI ACC NMI 0.62 0 0.25 0.5 2 4 Hyperparameter ξ

adjusts the threshold for $k$ , thereby controlling the sample reduction ratio. This formulation ensures that $k$ is adaptive to the dataset’s characteristics, balancing the number of neighbors relative to the total sample size and class count. Figure 4 illustrates the clustering performance for different values of $\xi$ across datasets. When $\xi = 0$ , the adaptive contrastive loss function degenerates to its standard form without dynamic adjustment, as described in Eq. (6). For $\xi > 0$ , the adjustment effectively filters irrelevant negative samples, significantly improving clustering performance. These results highlight the utility of the proportional threshold in selecting meaningful neighbors and the critical role of the selfexpression layer in supervising the contrastive learning process.

Convergence and Runtime Analysis: We analyze the convergence of TTAE using the loss function in Eq. (7) on Fashion-MNIST. As shown in Figure 6, the loss decreases and converges steadily over iterations (5 epochs as one iteration), while ACC and NMI consistently improve. Although the attention mechanism introduces some computational overhead with a time complexity of $\mathcal { O } ( N ^ { 2 } o _ { v } )$ , this is compensated by improved clustering performance. Table 5 summarizes the runtime and performance of different methods after 200 training epochs.

Table 5: Comparison of NMI and runtime for different methods on Fashion-MNIST.   

<html><body><table><tr><td>Methods</td><td>NMI (%)</td><td>Runtime (s)</td></tr><tr><td>CMSC-DCCA</td><td>68.33</td><td>18.21</td></tr><tr><td>DMSC-UDL</td><td>68.34</td><td>19.68</td></tr><tr><td>MFLVC</td><td>54.63</td><td>24.17</td></tr><tr><td>D2MVSC</td><td>67.73</td><td>19.11</td></tr><tr><td>Ours</td><td>69.36</td><td>20.92</td></tr></table></body></html>

![](images/6cb46a81e5631025c740a617b5da8d61744c86ada5e2f0fe45832b4f38757d15.jpg)  
Figure 5: Influence of parameters $\lambda _ { 1 }$ , $\lambda _ { 2 }$ changes on clustering performance on Fashion-MNIST dataset.

![](images/ddec813f18caea107ec7952f77a82e1d5558afc069b0d774b766c1c7253efbb0.jpg)  
Figure 4: Clustering performance versus different hyperparameter values $\xi$ on Fashion-MNIST, where nearest samples $k = \xi m / f$ .   
Figure 6: Clustering performance with increasing iteration on Fashion-MNIST.

# Conclusion

In this work, we proposed a novel multi-view subspace clustering via Tensor Transformers Autoencoders (TTAE). Specifically, it integrates learning of consistency and complementarity. TTAE effectively captures complementary information by fostering interactive attention across diverse views. Simultaneously, it harmoniously combines selfexpression with contrastive learning, where contrastive constraints are used to obtain discriminative features between samples to promote a clear self-expressive cluster structure, i.e., sample affinity, which in turn can supervise pairs of negative samples. Extensive experiments demonstrate that TTAE achieves a significant improvement over several stateof-the-art clustering methods.