# On the Relationship Between Monotone and Squared Probabilistic Circuits

Benjie Wang, Guy Van den Broeck

University of California, Los Angeles benjiewang@cs.ucla.edu, guyvdb@cs.ucla.edu

# Abstract

Probabilistic circuits are a unifying representation of functions as computation graphs of weighted sums and products. Their primary application is in probabilistic modeling, where circuits with non-negative weights (monotone circuits) can be used to represent and learn density/mass functions, with tractable marginal inference. Recently, it was proposed to instead represent densities as the square of the circuit function (squared circuits); this allows the use of negative weights while retaining tractability, and can be exponentially more expressive efficient than monotone circuits. Unfortunately, we show the reverse also holds, meaning that monotone circuits and squared circuits are incomparable in general. This raises the question of whether we can reconcile, and indeed improve upon the two modeling approaches. We answer in the positive by proposing Inception PCs, a novel type of circuit that naturally encompasses both monotone circuits and squared circuits as special cases, and employs complex parameters. Empirically, we validate that Inception PCs can outperform both monotone and squared circuits on a range of tabular and image datasets.

# Code — https://github.com/wangben88/InceptionPCs Extended version — https://arxiv.org/abs/2408.00876

# 1 Introduction

The philosophy of tractable probabilistic modeling advocates for modeling high-dimensional probability distributions using model architectures that support exact and efficient inference for various probabilistic queries by-design. This property makes them extremely useful for probabilistic reasoning; for example, tractable models have found wide-ranging applications from enhancing and controlling intractable generative models (Zhang et al. 2023; Liu, Niepert, and Van den Broeck 2024), to neuro-symbolic AI (Ahmed et al. 2022; Ahmed, Chang, and Van den Broeck 2023), to causal discovery and inference (Wang, Wicker, and Kwiatkowska 2022; Wang and Kwiatkowska 2023).

The lingua franca for tractable models is the probabilistic circuits framework (PC) (Choi, Vergari, and Van den Broeck 2020), which specify functions using computation graphs of sums and products, unifying many previous tractable models such as arithmetic circuits (Darwiche 2003), sum-product networks (Poon and Domingos 2011) and cutset networks (Rahman, Kothalkar, and Gogate 2014). The standard approach to modeling is to directly represent the probability distribution (density/mass function) using a PC. In order to enforce non-negativity of the PC output, one typically restricts to the PC to have non-negative parameters; these are known as monotone PCs (Darwiche 2003; Poon and Domingos 2011). However, recent works have also shown that there exist many tractable classes of probability distributions that provably cannot be expressed in this way (Zhang, Holtzen, and Broeck 2020; Yu, Trapp, and Kersting 2023; Broadrick, Zhang, and Van den Broeck 2024).

This motivates the development of new approaches for practically constructing and learning generalized tractable models. To this end, Loconte et al. (2024b) recently proposed squared circuits, where the probability distribution is defined to be (proportional to) the square of the circuit function. In this formulation, a PC can employ real (possibly negative parameters) while still defining a valid probability distribution. This was shown theoretically to lead to an exponential advantage in expressive efficiency compared to monotone PCs, for certain classes of distributions.

In this work, we reexamine monotone and squared (structured-decomposable) PCs, and show that they are incomparable in general: either can be exponentially more expressive efficient than the other. Motivated by this observation, we draw on the latent variable interpretation of PCs (Peharz et al. 2016) and show an elegant connection between the two types of circuits; namely, that they simply correspond to summing the latent variables outside or inside the square, i.e. a deep sum-of-squares or square-ofsums. By combining these two types of latent variables, we introduce a novel class of tractable models representing deep sum-of-square-of-sums, which we call Inception PCs (IncPC), strictly generalizing and extending (structureddecomposable) monotone and squared PCs.

We further investigate learning Inception PCs effectively at scale. Following recent trends in PC learning (Peharz et al. 2020b,a; Liu and Van den Broeck 2021; Mari, Vessio, and Vergari 2023), we design an efficient tensorized implementation of Inception PCs that subsumes tensorized (structured-decomposable) monotone and squared PCs. Empirical results validate the improved expressive efficiency of

Inception PCs on density estimation benchmarks.

Our contributions can be summarized as follows:

• Theoretically, we show that monotone circuits can be exponentially smaller than squared circuits for a simple class of distributions, meaning that monotone and squared circuits are incomparable in general in terms of expressive efficiency (Section 3);   
• We analyze both monotone and squared circuits from a latent variable perspective, showing that they can be interpreted as deep sums-of-squares and squares-of-sums respectively. We then propose a novel class of tractable circuits, Inception PCs (IncPC), which generalize and extend both types of circuits as a deep sum-of-square-ofsums and can employ complex parameters (Section 4).   
• We propose an efficient tensorized architecture for Inception PCs, enabling their application to large-scale datasets (Section 5);   
• Empirically, we demonstrate that (i) combining both types of latents, together with complex parameters, can improve performance; and (ii) Inception PCs can outperform monotone PCs and squared PCs on challenging benchmarks including downscaled versions of ImageNet, even when normalized for computation time (Section 7).

In concurrent work, Loconte, Mengel, and Vergari (2024) proved an exponential separation between monotone and squared circuits (i.e., our Theorem 2) using a similar family of separating functions. They also proposed two new model classes, SOCS and $\mu \mathrm { { S O C S } }$ , which generalize squared circuits. In the extended version, we discuss in detail connections with our Inception PCs.

# 2 Preliminaries

Notation We use capital letters to denote variables and lowercase to denote their assignments/values (e.g. $ { \boldsymbol { X } } ,  { \boldsymbol { x } } )$ . We use boldface (e.g. ${ \pmb X } , { \pmb x } )$ to denote sets of variables/assignments.

Probabilistic circuits are computation graphs representing functions constructed by hierarchical compositions of weighted sums and products.

Definition 1 (Probabilistic Circuit). A probabilistic circuit $\mathcal { C }$ over a set of variables $V$ is a rooted DAG consisting of three types of nodes $n$ : input, product and sum nodes. Each input node n is a leaf encoding a function $f _ { n } : W \to \mathbb { R }$ for some $W \subseteq V$ , and for each internal (product or sum) node $n$ , denoting the set of children (i.e. nodes $n ^ { \prime }$ for which $n  n ^ { \prime }$ ) by in $( n )$ , we define:

$$
f _ { n } = { \left\{ \prod _ { n _ { i } \in \mathrm { i n } ( n ) } f _ { n } } i f n { \begin{array} { l l } { i s p r o d u c t ; } \\ { \sum _ { n _ { i } \in \mathrm { i n } ( n ) } \theta _ { n , n _ { i } } f _ { n _ { i } } } & { i f n { \mathrm { ~ } } i s s u m . } \end{array} \right. }
$$

where each sum node has a set of weights {θn,ni}ni in(n) with $\theta _ { n , n _ { i } } \in \mathbb { R }$ . Each node $n$ thus encodes a function over a set of variables $\operatorname { s c } ( n )$ , which we call its scope; this is given by $\begin{array} { r } { \operatorname { s c } ( n ) = \bigcup _ { n _ { i } \in \mathrm { i n } ( n ) } \operatorname { s c } ( n _ { i } ) } \end{array}$ for internal nodes. The function encoded by the circuit f is the function encoded by its root node. The size of a probabilistic circuit $| { \mathcal { C } } |$ is defined to be the number of edges in its $D A G$ .

In this paper, we will assume that sum and product nodes alternate; this is without loss of generality as this property can be enforced on a PC at most a linear increase in size. A key feature of the sum-product structure of probabilistic circuits is that they allow for efficient (linear-time) computation of marginals, for example the partition function $\begin{array} { r } { \dot { Z } = \sum _ { v } f ( v ) ^ { 1 } } \end{array}$ , if they are smooth and decomposable:

Definition 2 (Smoothness, Decomposability). A probabilistic circuit is smooth if for every sum node $n$ , its inputs $n _ { i }$ have the same scope. A probabilistic circuit is decomposable if for every product node $n$ , its inputs have disjoint scope.

We will also need a stronger version of decomposability that enables circuits to be multiplied together efficiently (Pipatsrisawat and Darwiche 2008; Vergari et al. 2021):

Definition 3 (Structured Decomposability). A smooth and decomposable probabilistic circuit is structureddecomposable if any two product nodes $n , n ^ { \prime }$ with the same scope decompose in the same way.

Structured-decomposability is commonly enforced when learning PCs from data (Liu and Van den Broeck 2021) as well as when compiling a circuit from another model such as a Bayesian network (Choi, Kisa, and Darwiche 2013).

# 3 Expressive Efficiency of Monotone and Squared Structured-Decomposable Circuits

One of the primary applications of probabilistic circuits is as a tractable representation of probability distributions. As such, we typically require the function output of the circuit to be a non-negative real. The usual way to achieve this is to enforce non-negativity of the weights and input functions:

Definition 4 (Monotone PC). A probabilistic circuit is monotone if all weights are non-negative reals, and all input functions map to the non-negative reals.

Given a monotone $\mathsf { P C } \mathscr { C }$ , one can define a probability distribution $\begin{array} { r } { p _ { 1 } ( V ) : = \frac { f _ { \mathcal { C } } ( V ) } { Z _ { \mathcal { C } } } } \\ { \quad \approx \quad \quad \quad \quad \quad \quad } \end{array}$ where $Z _ { C }$ is the partition function of the PC. However, thCis is not the only way to construct a non-negative function. In Loconte et al. (2024b), it was proposed to instead use $f _ { \mathcal { C } }$ to represent a real (i.e. possibly negative) function, by allowing for real weights/input functions; this can then be squared to obtain a non-negative function. That is, we define p2(V ) := fvC(fV()v2)2 .

In order for $\textstyle \sum _ { v } f _ { \mathcal { C } } ( v ) ^ { 2 }$ to be tractable to compute, a sufficie  condition is for the circuit $\mathcal { C }$ to be structured-decomposable; one can then explicitly construct a smooth and (structured-)decomposable circuit ${ \dot { \mathcal { C } } } ^ { 2 }$ such that $f _ { { \mathcal { C } } ^ { 2 } } ( V ) = f _ { \mathcal { C } } ( V ) ^ { 2 }$ of size and in time $O ( | C | ^ { 2 } )$ (Vergari et al. 2021). Then we have that $\begin{array} { r } { p _ { 2 } ( V ) ~ = ~ \frac { f _ { c 2 } ( V ) } { Z _ { c 2 } } } \end{array}$ , i.e. the distribution induced by the PC $\scriptstyle { \mathcal { C } } ^ { 2 }$ . Crucially, the circuit $\scriptstyle { \mathcal { C } } ^ { 2 }$ is not necessarily monotone; squaring thus provides an alternative means of constructing PCs that represent non-negative functions. In fact, it is known that squared real PCs can be exponentially more expressive efficient than structureddecomposable monotone PCs for representing probability distributions:

Theorem 1. (Loconte et al. 2024b) There exists a class of non-negative functions $p ( V )$ such that there exist structured-decomposable PCs $\mathcal { C }$ with $\begin{array} { r } { p ( { \cal V } ) { \bf \Psi } = f c ( { \bf V } ) ^ { 2 } } \end{array}$ of size polynomial in $| V |$ , but the smallest structureddecomposable monotone $P C { \mathcal { C } } ^ { \prime }$ such that $p ( V ) = f _ { \mathit { C ^ { \prime } } } ( V )$ has size 2Ω(|V |).

However, we now show that, in fact, the other direction also holds: monotone PCs can also be exponentially more expressive efficient than squared (real) PCs.

Theorem 2. There exists a class of non-negative functions $p ( V )$ , such that there exist monotone structureddecomposable $P C s ~ { \mathcal { C } }$ with $p ( V ) = f c ( V )$ of size polynomial in $| V |$ , but the smallest structured-decomposable $P C$ $\scriptstyle { \mathcal { C } } ^ { \prime }$ such that $p ( V ) = f _ { \mathit { C ^ { \prime } } } ( V ) ^ { 2 }$ has size $2 ^ { \Omega ( | V | ) }$ .

Proof. (Sketch) The function class we use to separate the circuit classes is $\begin{array} { r } { p ( V ) = \sum _ { i = 0 } ^ { d - 1 } 2 ^ { i } \mathbb { 1 } _ { V _ { i } = 1 } + 1 } \end{array}$ , i.e. a function that outputs the integ r encoded in binary by the variables $( + 1 )$ . This can easily be represented as a structureddecomposable monotone circuit of linear size. We show a lower bound on the size of structured-decomposable circuits computing any square root $F ( V )$ of this function, by (i) reducing to bounding the rank of a matrix representation of $F$ w.r.t. balanced partitions $( X , Y )$ of $V$ (Martens and Medabalimi 2014), using a standard techinque from communication complexity; (ii) lower bounding the number of distinct prime square roots in the matrix; (iii) showing the existence of a sufficiently large submatrix with full rank, thus lower bounding the rank of the original matrix (Fawzi et al. 2015). □

This is perhaps surprising, as squaring PCs generate structured PCs with possibly negative weights, suggesting that they should be more general than monotone structured PCs. The key point is that not all circuits that represent a positive function (not even all monotone structured ones) can be generated by squaring. Taken together, these results are somewhat unsatisfying, as we know that there are some distributions better represented by an unsquared monotone PC, and some by a squared real PC. In the next section, we will investigate how to reconcile these different approaches to specifying probability distributions.

# 4 Towards a Unified Model for Deep Sums-of-Squares-of-Sums

In this section, we investigate the relationship between monotone and squared circuits in depth. Firstly, we show how to introduce complex parameterizations to squared circuits (Prop 1). Secondly, we provide a new interpretation of monotone and squared circuits as different ways of marginalizing out latent variables (Figure 1); and introduce a new tractable model, Inception PCs, that combines the two approaches (Theorem 3).

# 4.1 Complex Parameters

We begin by noting that, beyond simply negative parameters, one can also allow for complex weights and input functions2, i.e. take values in the field $\mathbb { C }$ . Then, to ensure the non-negativity of the squared circuit, we multiply a circuit with its complex conjugate. That is:

$$
p _ { 2 } ( { \pmb V } ) = \frac { | f _ { \mathcal { C } } ( { \pmb V } ) | ^ { 2 } } { \sum _ { { \pmb v } } | f _ { \mathcal { C } } ( { \pmb v } ) | ^ { 2 } } = \frac { \overline { { f _ { \mathcal { C } } ( { \pmb V } ) } } f _ { \mathcal { C } } ( { \pmb V } ) } { \sum _ { { \pmb v } } \overline { { f _ { \mathcal { C } } ( { \pmb v } ) } } f _ { \mathcal { C } } ( { \pmb v } ) }
$$

As complex conjugation is a field isomorphism of $\mathbb { C }$ , taking a complex conjugate of a circuit is as straightforward as taking the complex conjugate of each weight and input function, retaining the same DAG as the original circuit. This allows us to efficiently compute $p _ { 2 } ( V )$ as a (smooth and structured decomposable) circuit:

Proposition 1 (Tractability of Complex Conjugation). Given a smooth and decomposable circuit $\mathcal { C }$ , it is possible to compute a smooth and decomposable circuit $\bar { \mathcal { C } }$ such that $f _ { \overline { { { \cal C } } } } ( V ) ~ = ~ { \overline { { { f _ { \cal C } ( V ) } } } }$ of size and in time $O ( | C | )$ . Further, if $\mathcal { C }$ is structured decomposable, then it is possible to compute a smooth and structured decomposable $\mathcal { C } ^ { 2 } \ s . t .$ $f _ { \mathcal { C } ^ { 2 } } ( V ) = \overline { { f _ { \mathit { C } } ( V ) } } f _ { \mathit { C } } ( V )$ of size and in time $O ( | C | ^ { 2 } )$ .

# 4.2 Deep Sums-of-Squares-of-Sums: A Latent Variable Interpretation

In the latent variable interpretation (LVI) of probabilistic circuits (Peharz et al. 2016), for every sum node, one assigns a categorical latent variable, where each state of the latent variable is associated with one of the inputs to the sum node; we show an example in Figure 1a. In this interpretation, when performing inference in the probabilistic circuit, we marginalize over all of the latent variables beforehand.

In the case when the circuit is structured decomposable, one can assign latent variables with variable scopes (sets) appearing in the circuit, such that two sum nodes $n , n ^ { \prime }$ with the same scope $\operatorname { s c } ( n ) = \csc ( n ^ { \prime } )$ are associated with the same latent $Z _ { \mathrm { s c } ( n ) }$ . Then, writing $z$ for the set of all latents, the function represented by the PC can be expressed as:

$$
f _ { \mathcal { C } } ( V ) = \sum _ { z } f _ { \mathcal { C } } ( V , z )
$$

where $f _ { \mathcal { C } } ( V , z )$ is a product of input functions for any value of $z$ .3 In other words, the distribution represented by the PC is fully factorized conditioned on a latent value $z$ .

However, interpreting these latent variables becomes tricky when we consider probability distributions defined by squaring circuits. The key question is, does one marginalize out the latent variables before or after squaring? We show both options in Figures 1b and 1c. In Figure 1b, we square before marginalizing $Z$ . In this case, each sum node weight is multiplied by its conjugate, and we are left with a sum node with non-negative real parameters. On the other hand, if we marginalize before squaring, we have a sum node with four children and complex parameters. Interestingly, the former case is very similar to directly constructing a monotone PC, while the latter is more like an explicit squaring without latent variables. This suggests that we can switch between monotone and squared PCs simply by deciding whether to sum the latent variables inside or outside the square. Using this perspective, we propose the following model, which explicitly introduces both type of latent variables into the circuit, producing a deep sum-of-square-of-sums:

![](images/07cf69f1fb9013ff4e8e6b3b1b68c8b726f984c204558e4666fcbe6df78b64a8.jpg)  
Figure 1: Latent variable interpretation for squaring PCs. The sum node in Figure 1a has two children with complex weights and associated with different values of the latent $Z$ . A sum-of-squares (Figure 1b) gives a monotone PC, where the parameters necessarily become non-negative. A square-of-sums (Figure 1c) leads to a squared PC, with four children each corresponding to the product of any two children from the original circuit.

Definition 5 (Inception $\mathrm { P C ^ { 4 } }$ ). An Inception PC (IncPC) Inception is a smooth and structured-decomposable probabilistic circuit over observed variables $V \cup U \cup W$ . The probability distribution of an Inception $P C$ is defined by:

$$
p _ { I n c e p t i o n } ( V ) = \frac { \sum _ { \pmb { u } } \left| \sum _ { \pmb { w } } f _ { { C } _ { I n c e p t i o n } } ( V , \pmb { u } , \pmb { w } ) \right| ^ { 2 } } { \sum _ { \pmb { v } } \sum _ { \pmb { u } } \left| \sum _ { \pmb { w } } f _ { { C } _ { I n c e p t i o n } } ( \pmb { v } , \pmb { u } , \pmb { w } ) \right| ^ { 2 } }
$$

As $\pmb { U }$ -latents are outside the square, and $W$ -latents are inside the square, we will refer to them as 1-norm and 2-norm latents respectively. The next Theorem shows that, given an Inception PC, we can efficiently “materialize” it into a tractable PC over just the observed variables $V$ representing the Inception PC’s distribution $p _ { \mathrm { I n c e p t i o n } } ( V )$ :

Theorem 3 (Tractability of InceptionPC). Given an Inception ${ } ^ { \circ } C \ C _ { I n c e p t i o n }$ , it is possible to compute a smooth and structured decomposable circuit $\mathcal { C } _ { m a t }$ such that $f _ { \mathcal { C } _ { m a t } } ( V ) =$ $\begin{array} { r } { \sum _ { \pmb { u } } \left| \sum _ { \pmb { w } } f _ { \mathcal { C } _ { I n c e p t i o n } } ( V , \pmb { u } , \pmb { w } ) \right| ^ { 2 } } \end{array}$ of size and in time $O ( | C | ^ { 2 } )$ .

Proof. The most systematic way to see this is to take advantage of the compositional inference framework of Vergari et al. (2021). We can marginalize out $W$ from $\mathcal { C } _ { \mathrm { I n c e p t i o n } }$ to obtain a $\mathrm { P C } \mathcal { C } _ { 1 }$ such that $\begin{array} { r } { f _ { \mathcal { C } _ { 1 } } ( \boldsymbol { V } , \boldsymbol { U } ) = \sum _ { \boldsymbol { w } } f _ { \mathcal { C } _ { \mathrm { I n c e p t i o n } } } ( \dot { \boldsymbol { V } } , \boldsymbol { U } , \boldsymbol { w } ) } \end{array}$ , retaining smoothness and structured decomposability. Then the computation of the square is possible by Proposition 1, returning a smooth and structured-decomposable circuit $\mathcal { C } _ { 2 }$ such that $\begin{array} { r } { f _ { \mathcal { C } _ { 2 } } ( V , U ) = | \sum _ { w } f _ { \mathcal { C } _ { \mathrm { I n c e p t i o n } } } ( V , U , w ) | ^ { 2 } } \end{array}$ . Finally, we can marginalize out $\pmb { U }$ from this circuit to obtain the structured decomposable and smooth circuit $\mathcal { C } _ { \mathrm { m a t } }$ . □

In Figure 2, we show a (fragment of) an example Inception PC, and its materialization via Theorem 3. In the rest of the paper, we will refer to this as the materialized Inception PC. The point is that we can efficiently perform inference on the distribution of an Inception PC through standard PC inference procedures on the materialized IncPC. Though this is not strictly necessary, we will assume (in accordance with the LVI for monotone and squared PCs) that $W , U$ are categoricals, and the input nodes with scope over these variables are indicators (e.g. $[ [ V = 1 ] ]$ ).

This provides an Jelegant rKesolution to the tension between monotone and squared (real/complex) PCs. To retrieve a monotone PC, we need only set $W = \emptyset$ ; then there is no summation inside the square, and $\mathcal { C } _ { \mathrm { m a t } }$ has the same structure as $\mathcal { C } _ { \mathrm { I n c e p t i o n } }$ but with the parameters and input functions squared (and so non-negative real).5 To retrieve a squared PC, we simply set $U = \varnothing$ ; then there is no summation outside the square. However, by using both types of latents, we obtain a generalized PC model that is strictly more expressive efficient than either individually6.

Corollary 1 (Expressive Efficiency of Inception PCs). Inception PCs are strictly more expressive efficient than either monotone or squared $P C s$ .

# 5 Tensorized Inception PCs

Thus far, we have described a general formulation of Inception PCs that simply requires smoothness and structured decomposability of the circuit. In practice, we will want to design specific circuit architectures for learning. The purpose of this section is to (i) propose such an architecture suitable for learning and inference on GPUs; and (ii) present an alternative view of Inception PCs as performing hierarchical tensor contractions.

![](images/c08e5c9937d1944603877c01e7f5dc549e76b349b9f057dfd140f38328737dd1.jpg)  
Figure 2: Diagrams showing an Inception PC, and the corresponding materialized IncPC. Each product node is labelled with an index, such that e.g. $\times _ { 3 4 }$ is the product of the product nodes $\times _ { 3 } , \times _ { 4 }$ . For clarity, the children of product nodes have been omitted (except the latent indicators), and edge weights for the materialized Inception PC are displayed below the corresponding child.

We follow recent trends in probabilistic circuit learning (Peharz et al. 2020a; Mari, Vessio, and Vergari 2023) and consider tensorized architectures for $\mathcal { C } _ { \mathrm { I n c e p t i o n } }$ , where sum and product nodes are grouped into regions by scope. In particular, for each region (scope), we construct $N _ { 1 } \times N _ { 2 }$ sum and product nodes in the Inception PC, where the sum nodes are connected in a dense fashion to the product nodes via a weight tensor $\theta _ { i i ^ { \prime } k k ^ { \prime } } \ \in \ \mathbb { C } ^ { N _ { 1 } ^ { 2 } \times N _ { 2 } ^ { 2 } }$ with $1 \ \leq \ k , k ^ { \prime } \ \leq \ N _ { 1 } .$ , $1 \ \leq \ i , i ^ { \prime } \ \leq \ N _ { 2 }$ ; and the product nodes are connected to sum nodes in subsequent regions via a Hadamard product (Loconte et al. 2024a). Each region is associated with a categorical latent $W$ and $U$ ; and each product node corresponds explicitly to a value of $W$ and $U$ , having two indicators $[ [ \bar { W } : ( \dot { \bar { W } } ) : \dot { i ^ { \prime } } ] ]$ and $\mathbb { U } = k ^ { \prime } \mathbb { I }$ as children where $1 \leq k ^ { \prime } \leq N _ { 1 }$ , $1 \leq i ^ { \prime } \leq N _ { 2 }$ . FJor exampKle, the product nodes in the Inception PC in Figure 2a correspond to a tensorized Inception PC with $N _ { 1 } = 2 , N _ { 2 } = 2$ .

Once materialized using Theorem 3, we obtain a materialized IncPC $\mathcal { C } _ { \mathrm { m a t } }$ with $\breve { N _ { 1 } } \times N _ { 2 } ^ { 2 }$ sum and product nodes; the increase in size occurs from the “expansion” from twonorm latents $W$ as seen in Figure 1c. For a given region, let us write $n _ { i j k } ^ { ( S ) }$ for the sum nodes for $1 \leq k \leq N _ { 1 } , 1 \leq i , j \leq$ $N _ { 2 }$ , and $n _ { i j k } ^ { ( P ) }$ for the product nodes for $1 \leq k \leq N _ { 1 } , 1 \leq$ $i , j \le N _ { 2 }$ in $\mathcal { C } _ { \mathrm { m a t } }$ . The product nodes are still connected to their $M$ child sum regions $n ^ { ( S _ { 1 } ) } , . . . , n ^ { ( S _ { M } ) }$ via a Hadamard product:

$$
f _ { n _ { i j k } ^ { ( P ) } } = \prod _ { m = 1 } ^ { M } f _ { n _ { i j k } ^ { ( S _ { m } ) } }
$$

However, the sum-to-product connection is no longer dense. In particular, writing we have the following relationship between the sum and product node tensors:

$$
f _ { n _ { i j k } ^ { ( S ) } } = \sum _ { i ^ { \prime } j ^ { \prime } k ^ { \prime } } \theta _ { i i ^ { \prime } k k ^ { \prime } } \overline { { { \theta _ { j j ^ { \prime } k k ^ { \prime } } } } } f _ { n _ { i ^ { \prime } j ^ { \prime } k ^ { \prime } } ^ { ( P ) } }
$$

We illustrate the structure of this sum region in Figure 3. On a high level, the connections between the groups of nodes in Figure 3 can be viewed as a standard, monotone PC; in particular, the value of each sum group is simply an weighted sum of its children. However, each weighted edge is between 2D groups of “squared” nodes, rather than between scalar nodes.

![](images/38c1ee64cdf8aabb4e4c00d36370dd94f7effc1b141536fb32e9c425d435b6fe.jpg)  
Figure 3: Illustration of tensorized Inception PC sum region, where $N _ { 1 } = 3$ , $N _ { 2 } = 2$ .

The complexity of a forward pass can be deduced from the tensor contraction in Equation 5. In Table 1, we compare the parameter counts (i.e., memory cost) and time complexity of a forward pass for each region, between monotone, squared and Inception PCs; where $B$ is the size of the data batch. It can be seen that Inception PCs have the same number of parameters/complexity as monotone and squared PCs when setting $N _ { 2 } = 1$ and $N _ { 1 } = 1$ respectively, except with the complexity differing for squared PCs: this is because one can perform the squaring only once-per-batch in this special case (Loconte et al. 2024b).

As with previous works (Peharz et al. 2020a), we organize the circuit into layers (sets of regions that can be computed simultaneously) to further take advantage of GPU parallelization. For training, we use gradient descent on the negative log-likelihood of the training set. In the case of using complex parameters, we use Wirtinger derivatives (KreutzDelgado 2009) in order to optimize the complex weights and input functions. To achieve numerical stability, we use a variant of the log-sum-exp trick for complex numbers, described in the extended version of this paper.

Table 1: Complexity of batched forward pass and parameter count for circuit variants, per region.   

<html><body><table><tr><td></td><td>Monotone</td><td>Squared</td><td>Inception</td></tr><tr><td>Complexity</td><td>BN2</td><td>BN²+N3</td><td>BN2N</td></tr><tr><td>Parameter Count</td><td>N</td><td>N</td><td>N2N2</td></tr></table></body></html>

# 6 Related Work

Our work builds upon a long line of work in the circuit literature examining the effect of relaxing the monotonicity condition in circuits. Theoretically, it is known that allowing negative parameters in arithmetic circuits can result in exponential gains in succinctness (Valiant 1979), though this not true of all circuit subclasses (de Colnet and Mengel 2021). Practically, recent works have aimed to exploit negative parameters in probabilistic modeling (Zhang, Holtzen, and Broeck 2020; Zhang, Juba, and Van den Broeck 2021; Sladek, Trapp, and Solin 2023; Loconte et al. 2024b). Yu, Trapp, and Kersting (2023) design circuits to represent the characteristic function of a distribution, which can take complex values (in particular, using complex leaves). Concurrently with our work, Loconte, Mengel, and Vergari (2024) proposed to employ a sum of squared circuits to overcome similar limitations to those we observe in this paper.

There also exist a range of other probabilistic models which employ negative parameterizations and/or squaring. Tensor networks, such as the popular matrix-product states (MPS) (Perez-Garcia et al. 2007), compactly encode functions through sparse tensor contractions. They are often used to model quantum states, whereby the probability of observations is given by squaring via the Born rule (Dirac 1981); recently, tensor networks have been used for probabilistic modeling (Cheng et al. 2019; Glasser et al. 2019). Positive semidefinite models (Rudi and Ciliberto 2021) in the kernel methods literature utilize a shallow sum of squares to define unnormalized distributions. Tsuchida, Ong, and Sejdinovic (2023, 2024) recently introduced squared neural families, which employ the squared 2-norm of a neural network in the density function and strictly generalize exponential familiies. We discuss technical connections with these models in the extended version of this paper.

# 7 Experiments

In our experiments, we aim to answer the following research questions: (1) do Inception PCs improve upon the expressivity and modeling capabilities of monotone and squared PCs?; (2) what is the tradeoff between the number of onenorm latents and two-norm latents in terms of modeling performance and computational cost?; (3) how do complex parametererizations of Inception PCs compare to non-negative and real parameterizations?

We use hidden Chow-Liu trees (HCLT) in all experiments as the PC vtree (Liu and Van den Broeck 2021), as it satisfies the required structured-decomposability property, and has been shown to provide state-of-the-art likelihoods for PCs. Further experimental details can be found in the extended version.

# 7.1 Binary Datasets

We begin by evaluating on the 20 binary datasets, which has been extensively used as a density estimation benchmark (Rooshenas and Lowd 2014; Peharz et al. 2020b). We aim to investigate across this range of datasets how (1) nonnegative, real, and complex parameterizations differ; and (2) whether Inception PCs can effectively make use of both types of latents, by comparing to its monotone and squared PC counterparts. In particular, we set $( N _ { 1 } , N _ { 2 } ) \ = \ \mathsf { \bar { \Gamma } } ( 8 , 1 )$ for monotone PCs, $( N _ { 1 } , N _ { 2 } ) = ( 1 , 8 )$ for squared PCs, and $( N _ { 1 } , N _ { 2 } ) = ( 8 , 8 )$ for Inception PCs. We train each PC on negative log-likelihood for 100 epochs, using the Adam optimizer (Kingma and Ba 2015) with learning rate 0.01. We average over 5 runs for each configuration.

The results are shown in Table 2. It can be seen that Inception PCs achieve the best likelihoods on 14 of the 20 datasets, confirming that they provide more modeling capacity compared to squaring alone. They also perform very favorably compared to existing models, including (tractable) RAT-SPNs (Peharz et al. 2020b) and (intractable) importance weighed autoencoders (Burda, Grosse, and Salakhutdinov 2016). Squared PCs, even with non-negative parameters, outperform their monotone counterparts that have the same number of parameters, as also observed in Figure 3 of Loconte et al. (2024b). Very interestingly, despite the fact that complex parameterizations of edge weights strictly generalizes real parameterizations and non-negative parameterizations, there is no clear trend in performance, with the non-negative parameterization often achieving comparable or better likelihoods than real or complex parameterizations. We observed that this is (at least partially) due to the complex and negative parameterizations being more prone to overfitting (cf. training LLs in the extended version).

# 7.2 Scaling to Large Image Datasets

In this section, we conduct experiments on large-scale image datasets, in particular, downscaled versions of ImageNet to $3 2 \times 3 2$ and $6 4 \times 6 4$ (Deng et al. 2009). Following recent work on PC modeling for these datasets (Liu, Zhang, and Van den Broeck 2023; Liu et al. 2023; Liu, Ahmed, and Van den Broeck 2024), we transform the data from RGB using the lossy $\mathbf { Y C o C g }$ transform. Note that likelihoods on on $\mathbf { Y } \mathbf { C o C g }$ transformed data are thus not comparable to likelihoods on the original RGB dataset. Additionally, to improve training efficiency, we train the circuits on $1 6 \times 1 6$ patches of the original image7, such that the PC models $1 6 \times 1 6 \times 3 = 7 6 8$ variables each with 256 categories. We train the models on negative log-likelihood, using the Adam optimizer with learning rate 0.01. We evaluate models using test-set bits-per-dimension (bpd).

We begin by examining the tradeoff between the two types of latents. To this end, in Figure 4 we plot the test bpd for a range of configurations of $\bar { ( } N _ { 1 } , N _ { 2 } )$ on ImageNet32. These configurations were chosen based on a search in the range $N _ { 1 } , N _ { 2 } \in \{ 1 , 2 , 4 , 8 , 1 6 , 3 2 , 6 4 \}$ according to a maximum budget of $2 ^ { 1 8 } = 2 6 2 1 4 4$ floating-point operations per second (FLOPS) per region (c.f. Table 1). It can be seen that the optimal configuration ( $N _ { 1 } = 3 2$ , $N _ { 2 } = 4$ with bpd 4.19) given these constraints uses a combination of 1-norm latents and 2-norm latents, as opposed to a pure monotone $N _ { 2 } = 1 \dot { } \mathrm { _ { \it { \Delta } } }$ ) or pure squared circuit $N _ { 1 } = 1 \AA$ ).

Table 2: Test negative log-likelihoods on 20 binary datasets. Lower is better.   

<html><body><table><tr><td rowspan="2">Dataset</td><td colspan="9">Model</td></tr><tr><td rowspan="2">Monotone PC</td><td colspan="2">Squared PC</td><td rowspan="2"></td><td colspan="2">Inception PC</td><td rowspan="2"></td><td rowspan="2">RAT-SPN</td><td rowspan="2">VAE</td></tr><tr><td></td><td>Non-negative</td><td>Real Complex</td><td>Non-negative</td><td>Real Complex</td></tr><tr><td>nltcs</td><td>6.04</td><td>6.02</td><td>6.09</td><td>6.03</td><td>6.00</td><td>6.01</td><td>6.02</td><td>6.01</td><td>5.99</td></tr><tr><td>msnbc</td><td>6.25</td><td>6.23</td><td>6.10</td><td>6.07</td><td>6.05</td><td>6.06</td><td>6.04</td><td>6.04</td><td>6.09</td></tr><tr><td>kdd-2k</td><td>2.13</td><td>2.10</td><td>2.23</td><td>2.12</td><td>2.12</td><td>2.13</td><td>2.12</td><td>2.13</td><td>2.12</td></tr><tr><td>plants</td><td>13.70</td><td>13.38</td><td>15.06</td><td>13.13</td><td>12.81</td><td>13.06</td><td>12.76</td><td>13.44</td><td>12.34</td></tr><tr><td>jester</td><td>52.77</td><td>52.56</td><td>54.51</td><td>52.97</td><td>52.51</td><td>52.60</td><td>52.75</td><td>52.97</td><td>51.54</td></tr><tr><td>audio</td><td>40.27</td><td>40.08</td><td>41.49</td><td>40.01</td><td>39.88</td><td>39.91</td><td>40.05</td><td>39.96</td><td>38.67</td></tr><tr><td>netflix</td><td>57.09</td><td>56.85</td><td>57.68</td><td>56.70</td><td>56.52</td><td>56.57</td><td>56.74</td><td>56.85</td><td>54.73</td></tr><tr><td>accidents</td><td>29.57</td><td>27.93</td><td>28.15</td><td>27.05</td><td>26.70</td><td>27.30</td><td>26.61</td><td>35.49</td><td>29.11</td></tr><tr><td>retail</td><td>10.99</td><td>10.82</td><td>11.00</td><td>10.95</td><td>11.00</td><td>10.95</td><td>10.95</td><td>10.91</td><td>10.83</td></tr><tr><td>pumsb-star</td><td>27.98</td><td>24.95</td><td>25.69</td><td>23.98</td><td>23.69</td><td>24.85</td><td>23.03</td><td>32.53</td><td>25.16</td></tr><tr><td>dna</td><td>80.21</td><td>79.95</td><td>80.15</td><td>80.17</td><td>79.85</td><td>80.11</td><td>79.77</td><td>97.23</td><td>94.56</td></tr><tr><td>kosarek</td><td>10.77</td><td>10.54</td><td>12.03</td><td>10.59</td><td>10.69</td><td>10.83</td><td>10.60</td><td>10.89</td><td>10.64</td></tr><tr><td>msweb</td><td>10.44</td><td>9.92</td><td>10.41</td><td>10.17</td><td>10.84</td><td>10.34</td><td>10.10</td><td>10.12</td><td>9.727</td></tr><tr><td>book</td><td>33.70</td><td>33.32</td><td>37.02</td><td>33.95</td><td>33.51</td><td>34.18</td><td>33.67</td><td>34.68</td><td>33.19</td></tr><tr><td>eachmovie</td><td>52.83</td><td>51.28</td><td>62.03</td><td>52.33</td><td>50.76</td><td>51.22</td><td>51.41</td><td>53.63</td><td>47.43</td></tr><tr><td>web-kb</td><td>155.34</td><td>151.84</td><td>162.03</td><td>155.00</td><td>151.74</td><td>153.32</td><td>153.98</td><td>157.53</td><td>146.9</td></tr><tr><td>reuters-52</td><td>95.22</td><td>92.63</td><td>96.25</td><td>93.90</td><td>93.17</td><td>89.67</td><td>93.80</td><td>87.37</td><td>81.33</td></tr><tr><td>20ng</td><td>155.05</td><td>152.98</td><td>164.19</td><td>154.79</td><td>154.15</td><td>155.47</td><td>155.18</td><td>152.06</td><td>146.9</td></tr><tr><td>bbc</td><td>253.98</td><td>250.88</td><td>259.04</td><td>255.13</td><td>251.28</td><td>253.28</td><td>253.37</td><td>252.14</td><td>240.94</td></tr><tr><td>ad</td><td>16.93</td><td>15.54</td><td>16.32</td><td>15.35</td><td>16.02</td><td>15.81</td><td>15.32</td><td>48.47</td><td>18.81</td></tr></table></body></html>

Table 3: Test bpd on large-scale image datasets (lower is better). All PCs use the HCLT vtree.   

<html><body><table><tr><td></td><td>MPC</td><td>SPC</td><td>IncPC</td><td>EM</td><td>LVD</td></tr><tr><td>ImageNet32</td><td>4.35</td><td>4.26</td><td>4.19</td><td>4.82</td><td>4.38</td></tr><tr><td>ImageNet64</td><td>4.12</td><td>3.98</td><td>3.90</td><td>4.67</td><td>4.12</td></tr></table></body></html>

In Table 3, we summarize results from the ImageNet32 and ImageNet64 datasets, where MPC refers to a monotonic PC $( N _ { 1 } , N _ { 2 } ) \ = \ ( 6 4 , 1 )$ , SQC a squared PC $( N _ { 1 } , N _ { 2 } ) \ =$ $( 1 , 6 \dot { 4 } )$ , and IncPC a tensorized Inception PC with the optimal configuration of $( N _ { 1 } , N _ { 2 } )$ under the computation time constraint. For comparison, we also show results for monotone HCLT PCs trained using expectation-maximization (EM), and using latent variable distillation (LVD) in which guidance for the PC latent space is provided by distilling information from existing deep generative models (Liu, Zhang, and Van den Broeck 2023). The results show that it is possible to effectively train tractable PC models with Adam with bpds competitive with the state-of-the-art.

# 8 Conclusion

To conclude, we have shown that two important classes of tractable probabilistic models, namely monotone and squared real structured-decomposable PCs are incomparable in terms of expressive efficiency in general. Thus, we propose a new class of probabilistic circuits, Inception PCs, based on deep sums-of-squares-of-sums that generalizes these approaches, and can employ complex parameters. We further show empirically that Inception PCs can offer better performance on a range of tabular and image datasets. Promising avenues to investigate in future work would be improving the optimization of complex parameters in Inception PCs; as well as reducing the computational cost of training by designing more efficient architectures.

![](images/4a5edd402c8557d2e63b1d21d341568b64194a313e29e92494651e6d72436f57.jpg)  
Figure 4: Test bpd for a range of configurations of $( N _ { 1 } , N _ { 2 } )$ on ImageNet32 (lower is better); configurations are limited to $2 ^ { 1 8 } \equiv 2 6 2 K$ FLOPS per region. The Inception PC with $N _ { 1 } = 3 2 , N _ { 2 } = 4$ achieves the best performance.