# DiCA: Disambiguated Contrastive Alignment for Cross-Modal Retrieval with Partial Labels

Chao Su 1, Huiming Zheng 2, Dezhong Peng 1,2, Xu Wang 1\*

1The College of Computer Science, Sichuan University, Chengdu, China 2Sichuan National Innovation New Vision UHD Video Technology Co., Ltd., Chengdu, China suchao@stu.scu.edu.cn, michaelzheng $@$ uptcsc.com, pengdz@scu.edu.cn, wangxu.scu@gmail.com

# Abstract

Cross-modal retrieval aims to retrieve relevant data across different modalities. Driven by costly massive labeled data, existing cross-modal retrieval methods achieve encouraging results. To reduce annotation costs while maintaining performance, this paper focuses on an untouched but challenging problem, i.e., cross-modal retrieval with partial labels (PLCMR). PLCMR faces the dual challenges of annotation ambiguity and modality gap. To address these challenges, we propose a novel method termed disambiguated contrastive alignment (DiCA) for cross-modal retrieval with partial labels. Specifically, DiCA proposes a novel non-candidate boosted disambiguation learning mechanism (NBDL), which elaborately balances the trade-off between the losses on candidate and non-candidate labels that eliminate label ambiguity and narrow the modality gap. Moreover, DiCA presents an instance-prototype representation learning mechanism (IPRL) to enhance the model by further eliminating the modality gap at both the instance and prototype levels. Thanks to NBDL and IPRL, our DiCA effectively addresses the issues of annotation ambiguity and modality gap for cross-modal retrieval with partial labels. Experiments on four benchmarks validate the effectiveness of our proposed method, which demonstrates enhanced performance over existing state-of-the-art methods.

Code — https://github.com/Rose-bud/DiCA.

# Introduction

With the rapid growth of multimedia data on the Internet (Wang et al. 2019, 2020, 2023a,b; Su et al. 2023), the limitations of single-modal retrieval systems becoming increasingly apparent. Consequently, cross-modal retrieval has garnered significant interest due to its ability to facilitate flexible searches across different data modalities (Cao et al. 2022, 2023, 2024). Existing methods can be divided into two broad categories: supervised and unsupervised crossmodal retrieval. The supervised cross-modal retrieval methods (Zhen et al. 2019; Wang and Peng 2021; Sun et al. 2023, 2024a,b) learn modality-invariant features by leveraging the correct label information. However, all of the above methods typically require massive labeled data, which pose formidable obstacles to data collection. Additionally, data annotation in the real world can be naturally subject to inherent label ambiguity and noise. To the end, numerous unsupervised cross-modal retrieval methods (Hardoon, Szedmak, and Shawe-Taylor 2004; Andrew et al. 2013; Wang et al. 2015; Liu et al. 2023) have been proposed, which project data from different modalities into a common space by maximizing the correlation. Although these methods can reduce annotation costs, the modality gap still has significant room for improvement, primarily due to the challenges posed by the lack of label information.

![](images/c31cf55ddfccc07b6f7bc10c1c9996e001212ea821e3d08e96ec3513256a3f89.jpg)  
Figure 1: An input sample pair with three candidate labels, where the ground truth is tiger.

To reduce the costs of annotation while maintaining the performance of cross-modal retrieval, we propose a novel paradigm named cross-modal retrieval with partial labels (PLCMR). In PLCMR, annotators only need to provide a set of candidate labels and ensure that the ground truth label is included. As a result, the annotation costs can be significantly reduced. This paradigm can effectively handle difficult-to-distinguish sample pairs, such as in Fig. 1, where annotators are uncertain whether to label the image and text as a tiger, lion, or leopard. In such case, annotators can provide a candidate label set including all these three labels for training. Compared to supervised and unsupervised counterparts, the challenges of PLCMR lie in learning discriminative representations from partial labels with ambiguity while eliminating heterogeneous gap across modalities.

To tackle these challenges, we propose a novel method termed disambiguated contrastive alignment (DiCA), which can learn cross-modal invariant features when trained solely with a set of candidate labels. DiCA effectively integrates two distinct mechanisms, i.e., the non-candidate boosted disambiguation learning mechanism (NBDL) and the instanceprototype representation learning mechanism (IPRL), to enhance the performance of PLCMR task. Specifically, inspired by the leveraged weighted (LW) loss (Wen et al. 2021), NBDL is proposed to leverage the complementary information in the non-candidate label set. However, unlike LW, our NBDL projects data from different modalities into a common space, which helps resolve label ambiguity and reduces the modality gap. To further eliminate the modality gap, we additionally design a novel instance-prototype representation learning mechanism (IPRL). IPRL endows the model the ability to learn modal-invariant features at the instance and prototype levels through two modules, i.e., the instance-wise cross-modal contrast (ICC) module and the prototype-wise cross-modal alignment (PCA) module. In specific, ICC uses pseudo labels to guide the model in learning invariant features across modalities. PCA eliminates modality gap by minimizing the similarity differences between the learned representations and the prototype vectors across distinct modalities. With the support of ICC and PCA, the proposed IPRL reduces the modality gap and enhances the model’s ability to learn a more accurate label distribution, thereby improving label disambiguation in NBDL. In return, NBDL provides clearer label information, which guides and further strengthens the IPRL process.

The main contributions are summarized as follows: (1) We propose a novel method called DiCA to tackle an untouched problem, i.e., cross-modal retrieval with partial labels. To the best of our knowledge, this work could be the first study on this problem. (2) A novel non-candidate boosted disambiguation learning mechanism (NBDL) is presented to consider the trade-off between the loss on candidate and non-candidate labels to promote label disambiguation and narrow the modality gap. (3) A novel instanceprototype representation learning mechanism (IPRL) is proposed to learn modal-invariant information and further eliminate the modality gap. (4) Extensive experiments on four widely-used benchmarks demonstrate the effectiveness of the proposed DiCA.

# Related Work

# Partial Label Learning

In partial label learning, each instance is associated with a set of candidate labels with only one being the ground truth and others being the ambiguity. To tackle this challenge, existing methods can be broadly divided into two categories: averaging-based and identification-based approaches. For the averaging-based approaches (H¨ullermeier and Beringer 2006; Cour, Sapp, and Taskar 2011; Zhang and $\mathrm { Y u } 2 0 1 5 \mathrm { , }$ ), all candidate labels are treated equally and the prediction is made by averaging the results of their modeling outputs. However, these methods are often limited in performance due to being misled by false positive labels. For the identification-based disambiguation approaches (Jin and Ghahramani 2002; Nguyen and Caruana 2008; Liu and Dietterich 2012; Zhang, Zhou, and Liu 2016; Wang et al. 2021), the ground truth label is treated as a latent variable and can be identified through iterative optimization procedure, such as Expectation-Maximization (EM) algorithm.

Recently, deep-learning-based partial label learning methods (Lv et al. 2020; Feng et al. 2020; Wen et al. 2021; Wang et al. 2022; Xia et al. 2023; Liu et al. 2024) have made some new progress. For instance, Lv et al. (2020) approximately minimizes a risk estimator to identify the true label seamlessly. Motivated by contrastive learning, PiCO (Wang et al. 2022) contrastively learns representations and introduces a novel class prototype-based label disambiguation approach. Additionally, Si et al. (2024) introduces a novel partner classifier and proposes a novel “mutual supervision” paradigm, thereby improving the disambiguation capability of the base classifier. However, these methods are all specifically designed for unimodal scenarios, which may not be satisfactory for multimodal tasks due to the huge modality gap.

# Cross-Modal Retrieval

Cross-modal retrieval is a task that aims to find relevant data from different modalities. The critical challenge for the task lies in how to bridge the modality gap. To tackle this challenge, numerous approaches have been proposed, which can be divided into two categories: supervised crossmodal retrieval methods and unsupervised cross-modal retrieval methods. More specifically, 1) The supervised crossmodal retrieval methods (Zhen et al. 2019; Wang and Peng 2021; Sun et al. 2023; Wang et al. 2024b) leverage the annotated labels to learn a discriminative common space for different modalities. For instance, Sun et al. (2023) proposes a coarse-to-fine hierarchical hashing strategy to fully leverage the hierarchical feature information across various modalities. However, these methods typically require massive labeled data, which pose formidable obstacles to data collection. 2) The unsupervised cross-modal retrieval methods (Harold 1936; Andrew et al. 2013; Wang et al. 2015; Liu et al. 2023) learn modality-specific transformations by maximizing correlations between different modalities without relying on label information. For example, SCL (Liu et al. 2023) uses unsupervised contrastive learning to cultivate more discriminative representations, capitalizing on the relationships among intra- and inter-modality instances. However, the aforementioned methods all suffer from performance drops due to the lack of supervision.

In this paper, we focus on an untouched but meaningful problem, i.e., cross-modal retrieval with partial labels (PLCMR), which could reduce annotation costs and maintain performance in cross-modal retrieval task.

# Methodology

# Problem Formulation

Notations. For a clear presentation, we first give some definitions for notations in this paper. Denote $\chi$ as the input space, and $\mathcal { V } ~ = ~ \{ 1 , 2 , . . . , \bar { K } \}$ be the label space where $K$ denotes the number of classes. The training dataset $\mathcal { D } _ { I } = \{ ( \boldsymbol { x } _ { i } ^ { I } , Y _ { i } ^ { I } ) \} _ { i = 1 } ^ { N }$ denotes the image modality and $\mathcal { D } _ { T } =$ $\{ ( \pmb { x } _ { i } ^ { T } , Y _ { i } ^ { T } ) \} _ { i = 1 } ^ { N }$ denotes the text modality where $N$ is the total number of sample pairs. $\mathbf { \delta } _ { \mathbf { \boldsymbol { x } } _ { i } }$ and $Y _ { i }$ represent the $i$ -th sample pair and candidate label set, respectively. Further, the vector representation of $Y$ is defined as $\pmb { y } \in \mathbb { R } ^ { K }$ , where the element corresponding to the class in $Y$ is 1 and the others are 0. In partial learning tasks, each sample $\pmb { x } \in \mathcal { X }$ is input with a candidate label set $Y \in \mathcal { V }$ . In the training process, the true label is hidden within a set of candidate labels. The goal of DiCA is to identify the true label from inherent ambiguities. Through an encoder $f ( \cdot )$ , the feature vector $\pmb q \in \mathbb { R } ^ { L }$ can be computed by $\mathbf { \nabla } q = f ( { \boldsymbol { \mathbf { x } } } )$ , where $L$ denotes the dimension of the common space. Moreover, a softmax function $g ( \cdot )$ is applied to obtain the probability distribution $z \in \mathbb { R } ^ { K }$ as $z = g ( q )$ . Furthermore, we define the representations of the image and text modalities as ${ \mathcal { Q } } ^ { I }$ and $\dot { \boldsymbol { \mathcal { Q } } } ^ { T }$ , respectively. Meanwhile, the probability distributions of the image and text modalities are defined as $\mathcal { Z } ^ { I }$ and $\mathcal { Z } ^ { T }$ .

![](images/c9591b2b21bf8efe524ff45412d0344a49777ab2d831310cf980720bc7589d28.jpg)  
Figure 2: The pipeline of the proposed method DiCA for cross-modal retrieval with partial labels. NBDL $( \mathcal { L } _ { n b d } )$ balances the trade-off between the losses on candidate and non-candidate labels that eliminate label ambiguity and narrow the modality gap. Meanwhile, IPRL ( $\mathcal { L } _ { i c c }$ and $\mathcal { L } _ { p c a . }$ ) enhances the model by further eliminating the modality gap at both the instance and prototype levels.

Overview. The key challenge of PLCMR is to identify the ground truth label from the candidate set while eliminating the modality gap. As shown in Fig. 2, we present a non-candidate boosted disambiguation learning mechanism (NBDL) to resolve label ambiguity and narrow the modality gap. Furthermore, we propose an instance-prototype representation learning mechanism (IPRL) to further eliminate the modality gap at the instance and prototype levels. The overall objective function is formulated as:

$$
\begin{array} { r } { \mathcal { L } = \underbrace { \mathcal { L } _ { n b d } } _ { N B D L } + \underbrace { \alpha \cdot \mathcal { L } _ { i c c } + \beta \cdot \mathcal { L } _ { p c a } } _ { I P R L } , } \end{array}
$$

where ${ \mathcal { L } } _ { n b d }$ is the loss function adopted by NBDL, $\mathcal { L } _ { i c c }$ and $\mathcal { L } _ { p c a }$ are the loss functions employed by IPRL. Additionally, $\alpha$ and $\beta$ are the hyperparameters. Our DiCA is trained in a batch-by-batch manner by descending Eq. (1) with stochastic gradient descent. In the following subsections, we will elaborate on each component of the proposed DiCA.

# Non-candidate Boosted Disambiguation Learning

To find the disambiguated label $\hat { \pmb y }$ from the candidate label $_ y$ , many methods have been proposed by studying the candidate label set. However, they did not notice the simultaneous utilization of the non-candidate label set. Inspired from LW (Wen et al. 2021), we propose a new identification-based mechanism termed non-candidate boosted disambiguation learning (NBDL). NBDL is proposed to leverage the complementary information in the non-candidate label set. Different from LW, our NBDL projects data from different modalities into a common space, which achieves label disambiguation while reducing the modality gap. We formulate the label disambiguation process as follows:

$$
\hat { \pmb { y } } ^ { I } = \frac { z ^ { I } \circ \pmb { y } ^ { I } } { \lVert z ^ { I } \circ \pmb { y } ^ { I } \rVert } , \quad \hat { \pmb { y } } ^ { T } = \frac { z ^ { T } \circ \pmb { y } ^ { T } } { \lVert z ^ { T } \circ \pmb { y } ^ { T } \rVert } ,
$$

where $z ^ { I } \in \mathcal { Z } ^ { I }$ and $z ^ { T } \in \mathcal { Z } ^ { T }$ represent the probabilities that the sample $\boldsymbol { x } ^ { I }$ and $\scriptstyle { \mathbf { } } x ^ { T }$ belong to their respective class in the image and text modalities, respectively. By using the Hadamard product $\scriptscriptstyle \mathrm { ~ o ~ }$ , we can obtain the new disambiguated label $\hat { y }$ as the guidance for subsequent training.

To reveal the true label from the candidate label set, NBDL encourages the model to reduce the predicted probability of non-candidate labels while disambiguating the candidate label set. By increasing the loss on non-candidate labels and penalizing high predicted probabilities for these labels, the false positive rate can be significantly reduced. The NBDL losses of the image and text modalities are as follows:

$$
\begin{array} { l } { { \mathcal { L } _ { n b d } ^ { I } = - \displaystyle \sum _ { { \boldsymbol x } ^ { I } } \displaystyle \sum _ { c \in Y ^ { I } } p ( { \boldsymbol y } _ { c } ^ { I } = 1 \mid { \boldsymbol x } ^ { I } ) \log ( p ( { \boldsymbol y } _ { c } ^ { I } = 1 \mid { \boldsymbol x } ^ { I } , f ( { \boldsymbol x } ^ { I } ) ) } } \\ { { + \lambda \displaystyle \sum _ { \overline { { c } } \notin Y ^ { I } } p ( { \boldsymbol y } _ { \overline { { c } } } ^ { I } = 0 \mid { \boldsymbol x } ^ { I } , f ( { \boldsymbol x } ^ { I } ) ) ^ { 2 } , } } \end{array}
$$

$$
\begin{array} { r l } & { \mathcal { L } _ { n b d } ^ { T } = - \displaystyle \sum _ { \pmb { x } ^ { T } } \displaystyle \sum _ { c \in Y ^ { T } } p ( \pmb { y } _ { c } ^ { T } = 1 \mid \pmb { x } ^ { T } ) \log \bigr ( p ( \pmb { y } _ { c } ^ { T } = 1 \mid \pmb { x } ^ { T } , f ( \pmb { x } ^ { T } ) ) } \\ & { \qquad + \lambda \displaystyle \sum _ { \overline { { c } } \notin Y ^ { T } } p ( \pmb { y } _ { \overline { { c } } } ^ { T } = 0 \mid \pmb { x } ^ { T } , f ( \pmb { x } ^ { T } ) ) ^ { 2 } , } \end{array}
$$

Where $\lambda$ is the trade-off parameter between the loss on candidate and non-candidate labels. $f ( { \pmb x } )$ denotes the output of the model. $p ( { \pmb y } _ { c } \ = \ 1 \ | \ { \pmb x } )$ is the label confidence that the model assigns to the sample $\scriptstyle { \mathbf { { \vec { x } } } }$ belonging to class $c$ . $p ( { \pmb y } _ { c } = 1 \mid { \pmb x } , { \widetilde f } ( { \pmb x } ) )$ and $p ( { \pmb y } _ { \bar { c } } ^ { - } = 0 \mid { \pmb x } , \bar { f ( { \pmb x } ) } )$ denote the predicted probabilities on the candidate and non-candidate categories after applying the softmax function.

Finally, the total loss of NBDL $( { \mathcal { L } } _ { n b d } )$ can be written as:

$$
\begin{array} { r } { \mathcal { L } _ { n b d } = \mathcal { L } _ { n b d } ^ { I } + \mathcal { L } _ { n b d } ^ { T } . } \end{array}
$$

# Instance-Prototype Representation Learning

With non-candidate boosted disambiguation learning, the model can solve label ambiguity and reduce the modality gap. However, due to the presence of label ambiguity, the modality gap still exists. To further reduce the modality gap, we design a new instance-prototype representation learning mechanism (IPRL). IPRL endows the model the ability to jointly learn modal-invariant features at the instance and prototype levels through two modules, i.e., the instance-wise cross-modal contrast (ICC) module and the prototype-wise cross-modal alignment (PCA) module.

Instance-wise Cross-Modal Contrast. Contrastive learning techniques are commonly used in learning multimodal representations. However, in partial label learning, the true label is hidden among a set of candidate labels. To address this challenge, we propose an instance-wise cross-modal contrast (ICC) module. Specifically, we use the pseudo label $\hat { k }$ to extract features across different modalities. The pseudo label $\hat { k }$ is formulated as follows:

$$
\hat { k } ^ { I } = a r g m a x ( \hat { \pmb y } ^ { I } ) , \quad \hat { k } ^ { T } = a r g m a x ( \hat { \pmb y } ^ { T } ) .
$$

To learn shared representations, we maximize the agreement between different modalities in a common space. By maximizing the distance between instances with the same pseudo label $\hat { k }$ across different modalities, the probability of an instance $\mathbf { \Delta } _ { \boldsymbol { x } _ { j } ^ { i } }$ belonging to the $j$ -th instance across $m$ modalities can be defined as follows:

$$
p \left( j \mid \boldsymbol { x } _ { j } ^ { i } \right) = \frac { \sum _ { l = 1 } ^ { m } \mathbb { I } ( \hat { k } _ { l } = \hat { k } _ { j } ) \exp \left( \frac { 1 } { \tau } s i m ( \boldsymbol { z } _ { j } ^ { l } , \boldsymbol { z } _ { j } ^ { i } ) \right) } { \sum _ { l = 1 } ^ { m } \sum _ { t = 1 } ^ { N } \exp \left( \frac { 1 } { \tau } s i m ( \boldsymbol { z } _ { t } ^ { l } , \boldsymbol { z } _ { j } ^ { i } ) \right) } ,
$$

where $\tau$ is a temperature parameter $\mathrm { w } _ { \mathrm { u } }$ et al. 2018; Caron et al. 2020), $m$ denotes the number of modalities. The value of $l$ is set to 1 for the image modality and 2 for the text modality. $\mathbb { I } ( \cdot )$ is the indicator function. And $s i m ( \cdot )$ denotes the similarity calculation function.

To learn the modality-invariant features and bridge the modality gap, we apply cross-modal contrastive learning to instances from different modalities that share the same pseudo labels. Finally, the loss of ICC module is defined as:

$$
\mathcal { L } _ { i c c } = - \frac { 1 } { N } \sum _ { i = 1 } ^ { m } \sum _ { j = 1 } ^ { N } \log \left( p \left( j \mid \pmb { x } _ { j } ^ { i } \right) \right) .
$$

By minimizing Eq. (8), the model improves the ability to learn cross-modal representations, thereby further enhancing the quality of label disambiguation.

Prototype-wise Cross-Modal Alignment. With instancewise cross-modal contrast, the model learns more discriminative representations at the instance level, narrowing the gap between different modalities. However, this approach does not reduce modality gap at the prototype level. Therefore, we additionally propose a prototype-wise cross-modal alignment approach.

First, we maintain a prototype embedding vector $V$ for each category in the dataset. The prototype can also be treated as a sample feature that best represents the category $k \in \{ 1 , 2 , 3 , . . . , K \}$ . Prototype vectors for image and text modalities can be written as follows:

$$
\begin{array} { r l } & { \boldsymbol { V } ^ { I } = \left[ \begin{array} { l l l l } { \pmb { v } _ { 0 } ^ { I } } & { \pmb { v } _ { 1 } ^ { I } } & { \cdots } & { \pmb { v } _ { K } ^ { I } } \end{array} \right] , } \\ & { \boldsymbol { V } ^ { T } = \left[ \begin{array} { l l l l } { \pmb { v } _ { 0 } ^ { T } } & { \pmb { v } _ { 1 } ^ { T } } & { \cdots } & { \ \pmb { v } _ { K } ^ { T } } \end{array} \right] , } \end{array}
$$

where $\boldsymbol { V } ^ { I }$ and $\boldsymbol { V } ^ { T }$ represent the prototypes for image and text modalities respectively. $\pmb { v } _ { c } ^ { I }$ and $\boldsymbol { v } _ { c } ^ { T }$ represent the prototype vectors for category $c$ in the image and text modalities, respectively, and $c \in \{ 1 , 2 , 3 , . . . , K \}$ .

To update the prototypes stably, we use the pseudo label $\hat { k }$ and normalized representations $\pmb q$ to update the prototype $\boldsymbol { v } _ { k }$ for class $k$ in a moving-average style:

$$
\pmb { v } _ { k } ^ { I } = \omega ( t ) \pmb { v } _ { k } ^ { I } + ( 1 - \omega ( t ) ) \pmb { q } ^ { I } , \quad \mathrm { i f ~ } \hat { k } ^ { I } = k ,
$$

$$
\pmb { v } _ { k } ^ { T } = \omega ( t ) \pmb { v } _ { k } ^ { T } + ( 1 - \omega ( t ) ) \pmb { q } ^ { T } , \quad \mathrm { i f ~ } \hat { k } ^ { T } = k ,
$$

where $\omega ( t )$ is the dynamic parameter that controls the balance between the current prototype vectors $_ v$ and the representation vectors $\pmb q$ , and its value gradually decreases as the epoch $t$ increases. Initially, the representation vectors $\pmb q$ learned by the model tend to show significant deviations because the disambiguation effect is not yet pronounced. Therefore, the influence of these representation vectors $\pmb q$ should be considered less. As the training progresses, the improved representation vectors $\pmb q$ should increasingly influence the update of the prototype vectors $_ v$ .

To eliminate the cross-modal gap, we use the Mean Absolute Error (MAE) loss to minimize the similarity differences between the normalized representation $\pmb q ^ { I }$ and their corresponding prototype $v _ { k } ^ { I }$ and $\hat { v _ { k } ^ { T } }$ . Similarly, we calculate the differences for $\pmb q ^ { T }$ and their corresponding prototype $\boldsymbol { v } _ { k } ^ { I }$ and $v _ { k } ^ { T }$ . Therefore, the prototype-wise cross-modal alignment loss for the image and text modalities are as follows:

$$
\mathcal { L } _ { p c a } ^ { I } = \sum _ { \pmb { q } ^ { I } \in \mathcal { Q } ^ { I } } \sum _ { k \in \mathcal { V } } \left| \pmb { q } ^ { I } \cdot \pmb { v } _ { k } ^ { I } - \pmb { q } ^ { I } \cdot \pmb { v } _ { k } ^ { T } \right| ,
$$

$$
\mathcal { L } _ { p c a } ^ { T } = \sum _ { \pmb { q } ^ { T } \in \mathcal { Q } ^ { T } } \sum _ { \boldsymbol { k } \in \mathcal { V } } \left| \pmb { q } ^ { T } \cdot \pmb { v } _ { \boldsymbol { k } } ^ { T } - \pmb { q } ^ { T } \cdot \pmb { v } _ { \boldsymbol { k } } ^ { I } \right| .
$$

Finally, the total prototype-wise cross-modal alignment loss $( \mathcal { L } _ { p c a } )$ is formulated as follows:

$$
\begin{array} { r } { \mathcal { L } _ { p c a } = \mathcal { L } _ { p c a } ^ { I } + \mathcal { L } _ { p c a } ^ { T } . } \end{array}
$$

Table 1: Performance comparison in terms of mAP scores under different partial rates of 0.1, 0.2, 0.3, and 0.4 on the Wikipedia and NUS-WIDE datasets. The highest mAP score is shown in bold.   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">Ref.</td><td colspan="7">Wikipedia</td><td colspan="7">NUS-WIDE</td></tr><tr><td>Image</td><td>→Text</td><td></td><td></td><td>Text→ Image</td><td></td><td></td><td></td><td></td><td>Image→Text</td><td></td><td></td><td>Text → Image</td><td></td></tr><tr><td></td><td></td><td>0.1 0.2</td><td>0.3</td><td>0.4</td><td>0.1</td><td>0.2</td><td>0.3</td><td>0.4</td><td>0.1</td><td>0.2</td><td>0.3 0.4</td><td>0.1</td><td>0.2</td><td>0.3</td><td>0.4</td></tr><tr><td>MCCA</td><td>SIGKDD2010|0.202</td><td></td><td>0.202</td><td>0.202</td><td>0.202 0.189</td><td>0.189</td><td></td><td>0.189 0.189</td><td>0.523</td><td>0.523</td><td>0.523</td><td>0.523</td><td>0.539</td><td>0.539 0.539</td><td>0.539</td></tr><tr><td>PLS</td><td>CVPR2011</td><td>0.337</td><td>0.337</td><td>0.337 0.337</td><td>0.320</td><td>0.320</td><td>0.320</td><td>0.320</td><td>0.498</td><td>0.498</td><td>0.498 0.498</td><td>0.517</td><td>0.517</td><td>0.517</td><td>0.517</td></tr><tr><td>DCCA</td><td>ICML2013</td><td>0.281</td><td>0.281</td><td>0.281 0.281</td><td>0.260</td><td>0.260</td><td>0.260</td><td>0.260</td><td>0.527</td><td>0.527</td><td>0.527 0.527</td><td>0.537</td><td>0.537</td><td>0.537</td><td>0.537</td></tr><tr><td>DCCAE</td><td>ICML2015</td><td>0.308 0.308</td><td></td><td>0.308 0.308</td><td>0.286</td><td>0.286</td><td>0.286</td><td>0.286</td><td>0.529</td><td>0.529</td><td>0.529 0.529</td><td>0.538</td><td>0.538</td><td>0.538</td><td>0.538</td></tr><tr><td>SCL</td><td>TMM2023</td><td>0.386</td><td>0.386</td><td>0.386 0.386</td><td>0.357</td><td>0.357</td><td>0.357</td><td>0.357</td><td>0.596</td><td>0.596</td><td>0.596 0.596</td><td>0.603</td><td>0.603</td><td>0.603</td><td>0.603</td></tr><tr><td>CC</td><td>NIPS2020</td><td>0.456</td><td>0.409</td><td>0.404 0.376</td><td>0.401</td><td>0.358</td><td>0.367</td><td>0.342</td><td>0.691</td><td>0.687</td><td>0.684 0.676</td><td>0.689</td><td>0.680</td><td>0.678</td><td>0.676</td></tr><tr><td>RC</td><td>NIPS2020</td><td>0.432 0.370 0.341</td><td></td><td>0.294</td><td></td><td>0.376 0.364</td><td>0.314</td><td>0.298</td><td>0.690</td><td>0.680</td><td>0.677 0.667</td><td>0.683</td><td>0.677</td><td></td><td>0.672 0.655</td></tr><tr><td></td><td>PRODEN|ICML2020</td><td>0.448</td><td>0.391</td><td>0.371 0.317</td><td>0.398</td><td>0.348</td><td>0.354</td><td>0.272</td><td>0.687</td><td>0.685</td><td>0.684 0.671</td><td>0.685</td><td>0.683</td><td>0.679</td><td>0.672</td></tr><tr><td>LWS</td><td>ICML2021</td><td>0.433</td><td>0.369</td><td>0.325 0.252</td><td>0.376</td><td>0.341</td><td>0.299</td><td>0.278</td><td>0.687</td><td>0.678</td><td>0.671 0.659</td><td>0.685</td><td>0.671</td><td>0.666</td><td>0.655</td></tr><tr><td>PaPi</td><td>CVPR2023</td><td>0.426</td><td>0.367</td><td>0.346 0.354</td><td></td><td>0.410 0.362</td><td>0.306</td><td>0.202</td><td>0.479</td><td>0.508</td><td>0.499 0.576</td><td>0.629</td><td>0.604</td><td></td><td>0.593 0.586</td></tr><tr><td>SCARCE</td><td>ICML2024</td><td>0.363</td><td>0.336</td><td>0.316 0.292</td><td>0.346</td><td>0.321</td><td>0.283</td><td>0.272</td><td>0.660</td><td>0.668</td><td>0.635</td><td>0.635 0.653</td><td>0.638</td><td></td><td>0.643 0.643</td></tr><tr><td>DiCA</td><td>Ours</td><td>0.505 0.481</td><td></td><td>0.487</td><td>0.469|0.449</td><td>0.419</td><td>0.432</td><td>0.413|0.697</td><td></td><td>0.693</td><td>0.687</td><td></td><td></td><td>0.682|0.690 0.685 0.682 0.681</td><td></td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">Ref.</td><td colspan="8">INRIA-Websearch</td><td colspan="7"></td></tr><tr><td>0.01</td><td>Image 0.02</td><td>Text 0.03</td><td>0.04</td><td>0.01</td><td>Text- 0.02</td><td>Image 0.03</td><td>0.04</td><td>0.01</td><td>Image 0.02</td><td>Text 0.03</td><td>0.04</td><td>Text 0.01 0.02</td><td>Image 0.03</td><td>0.04</td></tr><tr><td>MCCA</td><td>SIGKDD2010|0.275</td><td></td><td>0.275</td><td>0.275</td><td>0.275</td><td>0.277</td><td>0.277</td><td>0.277</td><td>0.277</td><td>0.233</td><td>0.233</td><td>0.233 0.233</td><td>0.249</td><td>0.249</td><td>0.249</td><td>0.249</td></tr><tr><td>PLS</td><td>CVPR2011</td><td>0.387</td><td>0.387</td><td>0.387</td><td>0.387</td><td>0.398</td><td>0.398</td><td>0.398</td><td>0.398</td><td></td><td>0.276 0.276</td><td>0.276 0.276</td><td>0.266</td><td>0.266</td><td>0.266</td><td>0.266</td></tr><tr><td>DCCA</td><td>ICML2013</td><td>0.188</td><td>0.188 0.188</td><td></td><td>0.188</td><td>0.182</td><td>0.182</td><td>0.182</td><td>0.182</td><td>0.152</td><td>0.152</td><td>0.152 0.152</td><td>0.162</td><td>0.162</td><td>0.162</td><td>0.162</td></tr><tr><td>DCCAE</td><td>ICML2015</td><td>0.167</td><td>0.167</td><td>0.167</td><td>0.167</td><td>0.164</td><td>0.164</td><td>0.164</td><td>0.164</td><td>0.149</td><td>0.149</td><td>0.149 0.149</td><td>0.159</td><td>0.159</td><td>0.159</td><td>0.159</td></tr><tr><td>SCL</td><td>TMM2023</td><td>0.329</td><td>0.329</td><td>0.329</td><td>0.329</td><td>0.337</td><td>0.337</td><td>0.337</td><td>0.337</td><td>0.116</td><td>0.116</td><td>0.116 0.116</td><td>0.137</td><td>0.137</td><td>0.137</td><td>0.137</td></tr><tr><td>CC</td><td>NIPS2020</td><td>0.521</td><td>0.503</td><td>0.482</td><td>0.460</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RC</td><td>NIPS2020</td><td>0.502</td><td>0.473</td><td>0.431</td><td>0.403</td><td>0.546 0.530</td><td>0.528</td><td>0.504</td><td>0.490</td><td>0.554 0.496</td><td>0.493</td><td>0.446 0.391</td><td>0.555 0.492</td><td>0.484 0.392</td><td>0.434 0.340 0.276</td><td>0.383</td></tr><tr><td></td><td>PRODEN| ICML2020</td><td>0.520</td><td>0.499</td><td>0.479</td><td>0.459</td><td></td><td>0.491</td><td>0.454</td><td>0.431</td><td></td><td>0.401</td><td>0.341 0.278</td><td></td><td></td><td></td><td></td></tr><tr><td>LWS</td><td>ICML2021</td><td>0.515</td><td>0.493</td><td>0.471</td><td>0.438</td><td>0.546 0.542</td><td>0.525</td><td>0.501</td><td>0.483</td><td>0.544 0.535</td><td>0.479</td><td>0.427 0.369</td><td>0.547</td><td>0.469</td><td>0.414 0.339</td><td>0.361 0.252</td></tr><tr><td>PaPi</td><td>CVPR2023</td><td>0.544 0.526</td><td></td><td>0.516</td><td>0.488</td><td>0.544</td><td>0.520 0.532</td><td>0.491 0.512</td><td>0.470 0.479</td><td>0.490</td><td>0.455 0.328</td><td>0.372 0.305 0.297 0.278</td><td>0.528 0.492</td><td>0.437 0.311</td><td></td><td>0.292 0.273</td></tr><tr><td>SCARCE</td><td>ICML2024</td><td>0.346</td><td>0.292</td><td>0.254</td><td>0.239</td><td>0.420</td><td>0.373</td><td>0.357</td><td>0.297</td><td>0.305</td><td>0.296</td><td>0.289 0.282</td><td>0.356</td><td>0.331</td><td>0.299</td><td>0.286</td></tr><tr><td>DiCA</td><td></td><td></td><td>0.555</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Ours</td><td>0.565</td><td></td><td>0.547</td><td>0.542 |0.581</td><td></td><td>0.575</td><td>0.565</td><td>0.560|0.561</td><td></td><td>0.512</td><td>0.474</td><td>0.4270.561</td><td>0.507</td><td></td><td>0.459 0.417</td></tr></table></body></html>

Table 2: Performance comparison in terms of mAP scores under different partial rates of 0.01, 0.02, 0.03, and 0.04 on the INRIA-Websearch and XMediaNet datasets. The highest mAP score is shown in bold.

# Experiments

# Datasets

To evaluate the effectiveness of our method, we conduct extensive comparison experiments on four cross-modal retrieval benchmark datasets. These datasets are introduced as follows: 1) Wikipedia contains 2,866 image-text pairs that belong to 10 classes. We follow the previous work (Feng, Wang, and Li 2014) divide the dataset into 3 subsets: 2,173, 231, and 462 pairs for training, validation, and testing sets, respectively. 2) INRIA-Websearch consists of 71,478 images and 71,478 text descriptions. Following (Wei et al. 2017), We use the subset of INRIA-Websearch which selects 14,698 samples of 100 largest classes from the original set. We follow the previous work (Hu et al. 2021) divide the dataset into three subsets: 9,000, 1,332 and 4,366 image-text pairs for training, validation and testing sets, respectively. 3) NUS-WIDE consists of about 270,000 images that belong to 81 categories. Following the previous work (Peng et al. 2018), we use a subset of NUS-WIDE which has ten classes. Moreover, we split the dataset into three subsets, i.e., 42,941; 5,000; and 23,661 image-text pairs for training, validation, and testing sets, respectively. 4) XMediaNet is a large-scale multimodal dataset comprising 200 categories. We select data of the image and text modalities from this dataset then divide them into 32,000, 4,000, and 4,000 pairs for training, validation, and testing sets, respectively.

# Implementation Detail

In this work, we adopt the Adam (Kingma and Ba 2014) optimizer with a learning rate 0.0001 to update the parameters. For all datasets, we set the maximum number of training epochs to 100. The training batch size is set to 32 for Wikipedia dataset, and to 512 for the other datasets. Furthermore, to maintain consistency, the batch size during validation and testing is uniformly set to 256. For the datasets of Wikipedia, NUS-WIDE, and XMediaNet, we utilize the pretrained VGG-19 (Karen 2014) model as the convolutional neural network (CNN) backbone for processing images. Additionally, we employ the pre-trained Doc2Vec model (Lau and Baldwin 2016) as the textual backbone for handling text data. As for the INRIA-Websearch dataset, we adopt a pretrained AlexNet (Krizhevsky, Sutskever, and Hinton 2012) of ImageNet and LDA to process image and text data, respectively. Our DiCA is implemented on the PyTorch framework, and all experiments are conducted on four Nvidia GeForce RTX 3090 GPUs.

# Experimental Setup

To verify the effectiveness of our proposed method, we compare DiCA with six partial label learning methods and five unsupervised cross-modal retrieval methods. Specifically, we compare DiCA with the following partial label learning methods: CC and RC (Feng et al. 2020), PRODEN (Lv et al. 2020), LWS (Wen et al. 2021), PaPi (Xia et al. 2023), SCARCE1 (Wang et al. 2024a) . For the unsupervised cross-modal retrieval methods, we chose MCCA (Rupnik and Shawe-Taylor 2010), PLS (Sharma and Jacobs 2011), DCCA (Andrew et al. 2013), DCCAE (Wang et al. 2015), SCL (Liu et al. 2023). Similar to the previous work (Lv et al. 2020; Wen et al. 2021; Wang et al. 2022), we generate partially labeled datasets by flipping negative labels to false positive labels with a certain probability. Considering the number of categories contained in different datasets, we have set different partial label rates for each dataset, i.e., $\{ 0 . 1 , 0 . 2 , 0 . 3 , 0 . 4 \bar  \}$ for Wikipedia and NUS-WIDE, and $\{ 0 . 0 1 , 0 . 0 2 , 0 . 0 3 , 0 . 0 4 \}$ for INRIA-Websearch and XMediaNet. Moreover, we employ mean average precision (mAP) on all retrieved results as our evaluation.

# Comparison with State-of-the-Art Methods

We apply cross-modal retrieval with partial labels on four datasets to evaluate the performance of our DiCA and the baselines. The experimental results in terms of mAP scores across different partial rates are reported in Table 1 and Table 2 for four datasets, respectively. As shown in these tables, our DiCA outperforms all the baselines on all the datasets and various partial rates. From the experimental results, we can draw the following observations: 1) Some existing unsupervised cross-modal retrieval methods achieve relatively good performance when trained with a set of candidate labels. This is because they employ an unsupervised component that learns modality-specific transformations by maximizing correlations between different modalities. 2) Partial rates remarkably influence the performance of partial label learning methods. With the partial rate increasing in labels, their accuracies will typically decrease fast. In contrast, unsupervised methods do not face this issue. 3) In most cases, partial label learning methods achieve better retrieval results than unsupervised cross-modal retrieval methods because they utilize label information more effectively. This is due to their use of a label disambiguation component, similar to that in DiCA, which enhances the handling of candidate labels and highlights the critical role of label disambiguation. 4) As shown in Table 1 and Table 2, our

DiCA outperforms other baselines on all datasets with different partial rates. For example, with a partial rate of 0.4, the proposed DiCA exceeds SCL and PaPi by $8 . 3 \%$ and $1 1 . 5 \%$ on Wikipedia dataset for the image-to-text retrieval, respectively. This is due to the fact that DiCA not only has a label disambiguation loss that considers both candidate and non-candidate labels, but also has two cross-modal disparity elimination components designed for cross-modal retrieval.

# Ablation Study

In this section, we explore the contribution of each component (i.e., $\mathcal { L } _ { n b d } , \mathcal { L } _ { i c c } \bar { , } \mathcal { L } _ { p c a } )$ for cross-modal retrieval with partial labels. To achieve this, we conduct three variants of the proposed DiCA: 1) DiCA with ${ \mathcal { L } } _ { n b d }$ ; 2) DiCA with ${ \mathcal { L } } _ { n b d }$ and $\mathcal { L } _ { i c c }$ ; 3) DiCA with ${ \mathcal { L } } _ { n b d }$ and $\mathcal { L } _ { p c a }$ . All the compared methods are trained with the same settings on the Wikipedia and INRIA-Websearch datasets for a fair comparison. From the experimental results shown in Table 3 and Table 4, one can observe that $\mathcal { L } _ { i c c }$ can dramatically boost the performance, which indicates its effectiveness in excavating the instance-level discrimination. Meanwhile, the results also demonstrate that $\mathcal { L } _ { p c a }$ further reduces the modality gap. In conclusion, our DiCA can effectively enhance the performance on distinct datasets under different partial rates.

Table 3: Comparison between our DiCA (full version) and its three variants under the various partial rates of 0.1, 0.2, 0.3, and 0.4 on the Wikipedia dataset. The highest performance is shown in bold.   

<html><body><table><tr><td>Method</td><td>Image→ Text 0.1 0.2 0.3</td><td>0.4</td></tr><tr><td>DiCA(w.Lnbd) DiCA (w. Lnbd & Licc) DiCA (w. Lnbd & Lpca)</td><td>0.456 0.416 0.406 0.460 0.427 0.424 0.494 0.456 0.457</td><td>0.361 0.389</td></tr><tr><td>DiCA (w.Lnbd)</td><td>0.505 0.481 0.487 Text→Image</td><td>0.469</td></tr><tr><td>DiCA(w.Lnbd & Licc)</td><td>0.410 0.379 0.385 0.385</td><td>0.365 0.371</td></tr><tr><td></td><td>0.418 0.379</td><td></td></tr><tr><td>DiCA(w.Lnbd & Lpca)</td><td></td><td></td></tr><tr><td></td><td>0.440 0.395 0.408 0.378</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>FullDiCA</td><td>0.449 0.419 0.432</td><td></td></tr><tr><td></td><td></td><td>0.413</td></tr></table></body></html>

# Effect of Coefficient $\lambda$

To analyse the impact of the coefficient $\lambda$ in Eq. (3) and Eq. (4), we conduct parameter analysis experiments on Wikipedia and INRIA-Websearch datasets under different partial rates. As shown in Fig. 3, we plot the retrieval results (mAP) with different parameters of $\lambda$ . Based on the results, we can observe that the non-candidate boosted disambiguation loss achieves the best performance when the trade-off parameter $\lambda$ in the range of [1, 10]. This is accomplished by effectively balancing the loss on candidate labels or non-candidate labels.

# Effect of Coefficient $\alpha$ and $\beta$

To evaluate the impact of the coefficient $\alpha$ and $\beta$ in Eq. (1), we conduct parameter analysis experiments on Wikipedia and INRIA-Websearch datasets. As shown in Fig. 4, we can see that both instance-wise cross-modal contrast loss $( \mathcal { L } _ { i c c } )$ and prototype-wise cross-modal alignment loss $( \mathcal { L } _ { p c a } )$ contribute to enhance the model’s representational learning capability, which is consist with our ablation study. However, the contributions of each component are distinct for different datasets, which may be caused by the difficulty level of the datasets (e.g., the more classes there are, the more difficult it will be). To be specific, the model can obtain stable performance when the value of $\alpha$ is in range [1, 3] on both Wikipedia and INRIA-Websearch datasets. As for $\beta$ , the proposed DiCA yields a stable performancee in the range [1, 2.5] on Wikipedia, and in the range [0.01, 0.25] on INRIA-Websearch, respectively.

Table 4: Comparison between our DiCA (full version) and its three variants under the various partial rates of 0.01, 0.02, 0.03, and 0.04 on the INRIA-Websearch dataset. The highest performance is shown in bold.   

<html><body><table><tr><td>Method</td><td>0.01 I0.02→xt 0.04</td></tr><tr><td>DiCA (w. Lnbd) DiCA(w.Lnbd & Licc)</td><td>0.531 0.519 0.500 0.489 0.537 0.528 0.514 0.504</td></tr><tr><td>DiCA(w.Lnbd & Lpca) Full DiCA</td><td>0.555 0.548 0.538 0.527 0.565 0.555 0.547 0.542</td></tr><tr><td>DiCA (w. Lnbd) DiCA (w.Lnbd & Licc) 0.560 0.548 0.535 0.523</td><td>Text→Image 0.555 0.541 0.523 0.509</td></tr></table></body></html>

![](images/5eb88eb4fde765949205be38fdac6e158aee02eb1572d08cafb2f2219894ff92.jpg)  
Figure 3: The performance of DiCA with only ${ \mathcal { L } } _ { n b d }$ in terms of mAP scores versus different values of $\lambda$ on Wikipedia and INRIA-Websearch datasets.   
Figure 5: The MMD between image and text modalities on Wikipedia and INRIA-Websearch datasets under partial rates of 0.2 and 0.3.

# Effect of Modality Gap Elimination

To visually investigate the impact of $\mathcal { L } _ { i c c }$ and $\mathcal { L } _ { p c a }$ on eliminating modality gap, we present the modality gap in terms of Maximum Mean Discrepancy (MMD) for our DiCA model, the DiCA model with ${ \mathcal { L } } _ { n b d }$ and $\mathcal { L } _ { i c c }$ , and the DiCA model with ${ \mathcal { L } } _ { n b d }$ alone, under different partial rates. As shown in Fig. 5, both $\mathcal { L } _ { i c c }$ and $\mathcal { L } _ { p c a }$ could narrow the modality gap, demonstrating the effectiveness of the proposed IPRL mechanism.

![](images/9582d4538d38d7682e47ac9a3f8c1247e1e8ebf97cc839b034a010e12eb1434f.jpg)  
Figure 4: The performance of DiCA in terms of mAP scores versus different values of $\alpha$ and $\beta$ on Wikipedia and INRIAWebsearch datasets.

2.00 1.50 FDiulClADiwC.Anbd & icc DiCA w. nbd only 45.50 FDiulClADiwC.Anbd & icc 4.0 DiCA w. nbd only   
1.25   
1.00 3.0   
0.75 2.5   
0.50   
0.25 2.0   
0.00 1.5 Partial Rate: 0.2 Partial Rate: 0.3 Partial Rate: 0.2 Partial Rate: 0.3 (a) Wikipedia (b) INRIA-Websearch

# Conclusion

In this paper, we study a new problem, i.e., cross-modal retrieval with partial labels (PLCMR). To this end, we propose a novel method named DiCA to learning discriminative representations from partial labels with ambiguity while eliminating modality gap. To better perform label disambiguation and narrow modality gap, we propose a non-candidate boosted disambiguation learning mechanism (NBDL) that thoughtfully balances the trade-off between the losses on candidate and non-candidate labels. Meanwhile, we introduce a novel instance-prototype representation learning mechanism (IPRL) to enhance the model by further eliminating the modality gap at both the instance and prototype levels. Comprehensive experiments are conducted comparing to several state-of-the-art approaches on four multimodal datasets, demonstrating the effectiveness of our DiCA.

# Acknowledgments

This work was supported by the National Natural Science Foundation of China (62306197, 62372315), China Postdoctoral Science Foundation (2021TQ0223, 2022M712236), Sichuan Science and Technology Planning Project (2024YFG0007, 2024YFHZ0144, 2024YFHZ0089, 2024NSFTD0049, 2024ZDZX0004), Chengdu Science and Technology Project (2024-YF05-00687-SN, 2023-XT00-00004-GX, 2021-JB00-00025-GX), Postdoctoral Joint Training Program of Sichuan University (SCDXLHPY2307).