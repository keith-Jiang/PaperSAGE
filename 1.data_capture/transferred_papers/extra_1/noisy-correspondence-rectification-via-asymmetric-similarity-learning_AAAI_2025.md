# Noisy Correspondence Rectification via Asymmetric Similarity Learning

Yunbo Wang1\*, YuJie $\mathbf { W _ { u } } ^ { 2 * }$ , Zhien Dai3, Can Tian4†, Jun Long1, Jianhai Chen5

1Big Data Institute, Central South University, China 2School of Computer Science and Engineering, Central South University, China 3School of Automation, Central South University, China 4School of Computer Science and Cyber Engineering, Guangzhou University, China 5College of Computer Science and Technology, Zhejiang University, China {wangyunbo, 8102211218, zhiendai, jlong} $@$ csu.edu.cn, tiancan $@$ gzhu.edu.cn, chenjh919@zju.edu.cn

# Abstract

Cross-modal matching shows enormous potential to recognize objects across different sensory modalities, which is fundamental to numerous visual-language tasks like imagetext retrieval and visual captioning. Existing works generally rely on massive and well-aligned data pairs for model training. Unfortunately, multimodal datasets are extremely difficult to annotate and collect. As an alternative, the co-occurred data pairs collected from the internet have been widely exploited to train a cross-modal matching model. However, the cheaply-collected dataset unavoidably contains mismatched pairs (i.e., noisy correspondence), which are detrimental to the matching model. In this paper, we propose an alternative method termed noisy correspondence rectification via Asymmetric Similarity Learning (ASL), and it allows for dealing with insufficient learning of positive and negative pairs caused by the popular triplet-based symmetric learning fashion. Specifically, the learning of positive or negative pairs within a triplet is conducted in an asymmetric fashion, and the self-paced weighting boundary is imposed on positive pairs to mitigate the effect of noise. Meanwhile, the optimization of negative samples will not be affected in the process of punishing potentially-noisy positive samples. To verify the effectiveness of our proposed approach, a series of experiments are conducted on three widely-used benchmarks (i.e., Flick30k, MS-COCO and CC152k), and the results show superior performance compared to the state-of-the-art methods.

# Introduction

Cross-modal matching (Pan, Wu, and Zhang 2023; Wei et al. 2023a; Zhang et al. 2022) aims to establish the relationship between different modalities (e.g, image and text), facilitating the downstream tasks such as image/video captioning (Liu et al. 2023; Nguyen et al. 2024), cross-modal retrieval (Wang and Peng 2022; Kim, Kim, and Kwak 2023; Yang et al. 2023b; Wu et al. 2023), and visual question answering (Shao et al. 2023). The common practice in cross-modal matching is to project the representations of different modalities into a common subspace, so that the semantic correlation across modalities can be obtained by similarity measures such as cosine similarity and Euclidean distance.

With the rapid advance of computing resources, recent works have achieved impressive performance by training models on larger-scale datasets. Unfortunately, it is costly to collect and construct a well-annotated dataset in a realworld scene, especially multimodal datasets. Thus, current multimodal datasets unavoidably contain mismatched pairs, resulting in noisy correspondence (Huang et al. 2025). For example, the popular cross-modal datasets MS-COCO (Lin et al. 2014) and CC152K (Sharma et al. 2018) include a great number of inaccurate descriptions in image-text pairs. Different from the traditional noisy label (Song et al. 2022; Huang, Zhang, and Shan 2023; Wei et al. 2023b), the noisy correspondence refers to mismatched cross-modal data pairs, while these mismatched pairs are regarded as matched pairs when training model. Therefore, the performance of existing cross-modal tasks has nearly reached a bottleneck. Recently, some methods (Yang et al. 2023a; Han et al. 2023; Zha et al. 2024) have been presented to deal with noisy correspondence in cross-modal tasks, mitigating the adverse impact of noise. These works seek to model the difference of distribution about per-sample loss at the early period of training based on the memory effect of deep neural networks (DNNs), where DNNs tend to fit simple samples at this early stage. Next, the dataset would be classified into clean subset and noisy subset, in which the small-loss samples are more likely to be clean samples. Subsequently, soft label estimation strategies are employed for the two subsets, and the soft label is expressed as a margin in the tripletbased matching loss to penalize the noisy pairs, preventing the deep neural networks from over-fitting noisy data. However, the effect from such margin in triplet loss acted on the positive and negative pairs is symmetrical, so the penalty for the potentially-noisy positive sample will act on the negative sample with the same force, resulting in insufficient learning and suboptimal performance. Therefore, how to effectively exploit the supervision from positive and negative pairs remains a critical challenge in noisy correspondence.

To address the above issues, we propose a method dubbed noisy correspondence rectification via Asymmetric Similarity Learning (ASL) for robust image-text matching. It constructs an alternative paradigm that enables positive and negative pairs to be optimized in an asymmetric way, achieving independently penalizing potentially-noisy positive pairs without impacting the optimization of negative pairs. The traditional triplet loss is designed to narrow the distance of positive pairs and enlarge the distance of negative pairs in a symmetric fashion, where the single margin in triplet determines the decision boundary of model convergence (Wang et al. 2019b; Yang et al. 2023b). However, such a learning strategy inevitably leads to insufficient optimization of negative and positive pairs. For example, assuming the value of decision boundary is 0.3 in the triplet unit $( A P _ { i } , A N _ { i } )$ shown in Fig. 1, where $A P _ { i }$ and $A N _ { i }$ denote the similarity of positive pair and negative pair, respectively. The set (0.5, 0.2) can be considered as an ideal convergence, while another set (0.8, 0.5) is also an ideal state for the same decision boundary. This situation would inevitably result in a suboptimal performance. In our proposed ASL, the positive and negative pairs are decoupled, and they can be optimized in an asymmetric way. It allows for penalizing potentially-noisy positive samples without impacting the optimization of negative ones.

![](images/d555986970d0b77d1049c5e19f87c451bc182256f76ad1ea58039dc8b19241d8.jpg)  
Figure 1: illustrates the typical triplet loss, which mainly focuses on maintaining the margin between positive pair and negative pair. Thus it produces a suboptimal performance when the score of positive pairs and negative pairs is ambiguous or close.

The main contributions of this work are summarized as follows:

• We propose a method termed noisy correspondence rectification with Asymmetric Similarity Learning, which aims to correct the noisy correspondence via a general asymmetric learning paradigm, thereby mitigating the effect of noise on image-text matching.   
• We propose to model the distribution of per-sample loss using a Variational Bayesian Gaussian Mixture Model, which incorporates prior distribution and Bayesian inference to fit the distribution of loss in noisy scenarios, obtaining better probability density parameters.   
• A novel asymmetric dynamic matching loss is presented to independently regulate the optimization of positive and negative pairs. It enables accurately punishing potentially-noisy positive samples without impacting the optimization of negative ones.   
• Extensive experiments are conducted on three widelyused benchmarks, and the results demonstrate the effectiveness of our proposed method in both synthetic and real noise scenarios.

# Related Works

# Cross-modal Matching

Cross-modal matching is a fundamental research in the field of multimodal learning (Lee et al. 2018; Jia et al. 2022; Huang et al. 2022; Schlarmann and Hein 2023), and it usually projects the representations of different modalities into a common subspace, thereby retrieving readily similar items of other modality given the query modality (Pan, Wu, and Zhang 2023). According to different matching fashions, existing works can be classified into two categories in imagetext matching: 1) Coarse-grained matching. The images and texts are matched from a global feature computed by deep neural networks. ${ \mathrm { V S E } } { + } { + }$ (Faghri et al. 2017) attempts to employ the hard negatives in triplet loss to enhance the global feature matching. TIMAM (Sarafianos, Xu, and Kakadiaris 2019) proposes to learn the modality-invariant global representation by Generative Adversarial Networks, improving the image-text matching. The similar works (Wang et al. 2019a; Zhang et al. 2020) employ a two-stream global feature learning network and compute the pairwise similarity according to global feature. 2) Fine-grained matching. The regions within an image and words in a sentence correspond with each other so that fine-grained information is captured by embedding semantically-similar regions and works. For example, DVSA (Karpathy and Fei-Fei 2015) proposes to take the maximized matching score between regions of image and words of sentence as the image-text matching score. VSRN (Li et al. 2019) tries to reason the visual semantic relationship between the regions of image via graph convolutional network for enhancing cross-modal matching. SGRAF (Diao et al. 2021) further constructs a graph structure for multimodal data to infer relation-aware similarities via graph reasoning. However, all these alignment methods assume that the multimodal data are perfectly aligned in training, while it is impossible to satisfy due to a high-cost collection and annotation.

# Noisy Correspondence Learning

Different from the traditional noisy label, noisy correspondence refers to the mismatch issue in paired data. NCR (Huang et al. 2021) first investigates this issue and proposes a noise-robust solution according to the memorization effect of DNNs. It considers mismatched cross-modal pairs instead of incorrect annotations. After that, a series of methods are presented to rectify noisy correspondence in various visual-language tasks, like graph matching (Lin et al. 2023), image captioning (Kang et al. 2023), multi-view learning (Yang et al. 2021), person re-identification (Yang et al. 2022) and cross-modal retrieval (Feng et al. 2024; Han et al. 2024; Zhang, Li, and Ye 2024). DECL (Qin et al. 2022) proposes a cross-modal evidential learning solution and dynamically filters out noisy correspondences within each batch. MSCN (Han et al. 2023) utilizes meta-learning to distinguish the clean and noisy pairs via jointing the triplet-based matching objective. BiCro (Yang et al. 2023a) constructs a soft label estimation strategy and expresses it as a margin in triplet loss to correct the noisy correspondence in image-text matching. Recently, L2RM (Han et al. 2024) leverages optimal transport to discover potential matching relationships among mismatched pairs, thus utilizing useful knowledge from mismatched pairs for improving image-text matching. However, most of existing works leverage the tripletbased symmetric learning fashion to correct noise correspondence. The penalty enforced on the potentially-noisy positive samples would react on the negative ones with the same intensity, leading to insufficient learning of image-text pairs. In this paper, we present an asymmetric learning fashion to conduct noisy correspondence rectification, which effectively exploits the supervision from positive and negative pairs, achieving the potentially-noisy positive samples penalty without affecting the optimization of negative ones.

![](images/5302b5d43559b7e7dc59ed084b9da7969858c52cf9f8d1433eaf219abaa9f47f.jpg)  
Figure 2: the framework of the proposed ASL for robust image-text matching, which mainly consists of soft boundary computing and asymmetric similarity learning. In soft boundary computing, the VBGMM is exploited to model the distribution of per-sample loss for estimating its label, and then the estimated label is leveraged to compute the optimized boundary of persample. In asymmetric similarity learning, a novel asymmetric dynamic matching loss is presented to independently regulate the optimization of positive and negative pairs.

# Methodology

# Problem Definition

Following the common practice, we utilize image-text matching as a proxy for revealing noise correspondence. Fig. 2 gives the framework of our proposed ASL, which mainly consists of soft boundary computing and asymmetric similarity learning.

Given a dataset $D = \{ ( I _ { i } , T _ { i } , y _ { i } ) \} _ { i = 0 } ^ { N } _ { }$ , where $N$ denotes the total number of training samples, $( I _ { i } , T _ { i } )$ represents an image-text pair and $y _ { i } \in \{ 0 , 1 \}$ is the binary label. The label $y _ { i }$ is a hard-defined correspondence that indicates the image-text pair $( I _ { i } , T _ { i } )$ is positively correlated $( y _ { i } = 1 )$ ) or not $( y _ { i } = 0 )$ . The objective of image-text matching is to embed the features of image and text in the original space into a unified common representation space via feature encoders, where the similarity of positive pairs should be maximized as far as possible. The similarity of an image-text pair can be generally written as $S ( f ( I _ { i } ) , \dot { g } ( T _ { i } ) )$ , in which $S$ is a similarity metric like cosine similarity, $f$ and $g$ are the feature encoder of image and text modalities, respectively. For ease of description, we denote $S ( f ( I _ { i } ) , g ( T _ { i } ) )$ as $S ( I _ { i } , T _ { i } )$ in the following context. The typical triplet loss is widely used to train the feature encoders:

$$
\begin{array} { r } { l _ { t r i } ( I _ { i } , T _ { i } ) = [ S ( I _ { i } , \hat { T } _ { j } ) - S ( I _ { i } , T _ { i } ) + \alpha ] _ { + } } \\ { + [ S ( \hat { I } _ { h } , T _ { i } ) - S ( I _ { i } , T _ { i } ) + \alpha ] _ { + } , } \end{array}
$$

where $\alpha { > } 0$ denotes a given margin, $[ x ] _ { + } = m a x ( x , 0 )$ . In the triplet loss, the first term treats $I _ { i }$ as a query taking over all negative texts $\hat { T } _ { j }$ , and the second term treats $T _ { i }$ as a query taking over all negative images $\hat { I } _ { h }$ .

The application of triplet loss highly relies on the assumption that image-text pairs are correctly aligned. However, in a real-world scenario, the cheaply-collected multimodal data usually contain a part of mismatched pairs, which are erroneously labeled as matched pairs, i.e., the label $y _ { i } = 1$ . However, the above objective tends to over-fit these noisy data, and it inevitably leads to a performance decline in imagetext matching.

# Cross-modal Matching with Noisy Correspondence

To address the noisy correspondence, existing works propose to correct the original hard-label correspondence $y _ { i } = 1$ to a soft label $\hat { y } _ { i }$ , where $\hat { y } _ { i }$ is a continuous value within the range [0, 1] describing the correspondence in image-text pair $( I _ { i } , T _ { i } )$ . To be specific, if the correlation of data pair is strong, $\hat { y } _ { i }$ would be close to 1, and vice versa. Subsequently, the soft label is transformed into a soft margin in the triplet loss, thereby achieving noise-robust cross-modal matching. The triplet loss with a soft margin can be defined as follows:

$$
\begin{array} { r } { l _ { t r i } ^ { \prime } ( I _ { i } , T _ { i } ) = [ S ( I _ { i } , \hat { T } _ { j } ) - S ( I _ { i } , T _ { i } ) + \alpha ^ { \prime } ] _ { + } } \\ { + [ S ( \hat { I } _ { h } , T _ { i } ) - S ( I _ { i } , T _ { i } ) + \alpha ^ { \prime } ] _ { + } , } \end{array}
$$

where $\begin{array} { r } { \alpha ^ { \prime } = \frac { z ^ { \hat { y } _ { i } } - 1 } { z - 1 } \alpha , \hat { y } _ { i } } \end{array}$ represents the soft-label correspondence of the $i$ -th data pair established on the correlation between $I _ { i }$ and $T _ { i } , z$ is a relaxation factor, and $\alpha$ is a constant margin.

The above formula is conducive to facilitating model convergence when positive pairs are with noise, and it decreases the likelihood of over-fitting on noisy pairs by reducing the margin. However, the above optimization formula has a significant short in noisy scenes. From the perspective of gradient, the gradients of positive and negative samples in triplet loss are listed as follows:

$$
\frac { \partial l _ { t r i } ^ { \prime } } { \partial T _ { i } } = \left\{ \begin{array} { c c } { - \partial S ( I _ { i } , T _ { i } ) / \partial T _ { i } } & { S ( I _ { i } , T _ { i } ) < S ( I _ { i } , \hat { T } _ { j } ) + \alpha ^ { \prime } } \\ { 0 } & { S ( I _ { i } , T _ { i } ) \geq S ( I _ { i } , \hat { T } _ { j } ) + \alpha ^ { \prime } } \end{array} \right.
$$

$$
\frac { \partial l _ { t r i } ^ { \prime } } { \partial \hat { T } _ { j } } = \left\{ { \begin{array} { c c } { \partial S ( I _ { i } , \hat { T } _ { j } ) / \partial \hat { T } _ { j } } & { S ( I _ { i } , T _ { i } ) < S ( I _ { i } , \hat { T } _ { j } ) + \alpha ^ { \prime } } \\ { 0 } & { S ( I _ { i } , T _ { i } ) \geq S ( I _ { i } , \hat { T } _ { j } ) + \alpha ^ { \prime } } \end{array} } \right.
$$

For ease of observation, we only give the gradients of the first term in Eq. (2) about the positive sample $T _ { i }$ and negative sample $\hat { T } _ { j }$ . From Eq. (3) and Eq. (4), it can be observed that the gradients about positive and negative samples are symmetrical, and the gradient is always 0 when the condition holds $S ( I _ { i } , T _ { i } ) \ge S ( \breve { I } _ { i } , \hat { T } _ { j } ) + \alpha ^ { \prime }$ . As we penalize noisy positive samples by decreasing the margin, the symmetrical nature of the gradient can cause premature termination of optimization for negative ones, resulting in insufficient optimization.

# Asymmetric Similarity Learning

Since the symmetrical structure of triplet loss leads to insufficient optimization on the positive and negative samples, our method proposes to minimize the impact on negative samples optimization while penalizing potentially-noisy positive ones in an asymmetric learning paradigm. Thus, it allows penalizing noisy positive samples while maintaining proper optimization for negative samples. Inspired by (Sun et al. 2020), a novel asymmetric dynamic matching loss is presented, achieving reliable penalization of noisy positive samples without affecting the optimization of negative ones.

$$
\begin{array} { r } { l _ { a s y } ( \boldsymbol { I _ { i } } , \boldsymbol { T _ { i } } , \boldsymbol { \hat { T _ { j } } } ) = \log \left( 1 + \displaystyle \sum _ { i = 1 } ^ { N } e x p ( - \lambda \mu _ { p } ^ { i } ( S ( I _ { i } , \boldsymbol { T _ { i } } ) - m _ { p } ) ) \right. } \\ { \displaystyle \left. \sum _ { j = 1 } ^ { N - 1 } e x p ( \lambda \mu _ { n } ^ { j } ( S ( I _ { i } , \boldsymbol { \hat { T _ { j } } } ) - m _ { n } ) ) \right) } \end{array}
$$

where $\lambda$ represents a scaling factor, $m _ { p }$ and $m _ { n }$ denote the margin for positive and negative pairs, respectively. With the above formula, the optimization expectation of positive pair is $S ( I _ { i } , T _ { i } ) > m _ { p }$ , and negative pair is $S ( I _ { i } , \hat { T } _ { j } ) < m _ { n }$ . Empirically, $m _ { p }$ is set to $1 - m _ { 0 }$ and $m _ { n }$ is set to $m _ { 0 }$ , where $m _ { 0 }$ is a constant margin with the default 0.2. $\mu _ { p }$ and $\textstyle \mu _ { n }$ are nonnegative weight factors defined as follows:

$$
\begin{array} { r } { \left\{ \mu _ { p } ^ { i } = [ \sigma U _ { p } - S ( I _ { i } , T _ { i } ) ] _ { + } \right. } \\ { \left. \mu _ { n } ^ { j } = [ ( S ( I _ { i } , \hat { T } _ { j } ) - U _ { n } ] _ { + } \right. } \end{array}
$$

where $U _ { p }$ represents the ideal optimal boundary for positive pairs, and $U _ { n }$ represents the ideal optimal boundary for negative pairs. Empirically, $U _ { p }$ is set to $1 + m _ { 0 }$ and $U _ { n }$ is set to $- \dot { m _ { 0 } } . \left[ \cdot \right] _ { + }$ indicates a truncation operation to zero, ensuring the weight factors $\mu _ { p } ^ { i }$ and $\mu _ { n } ^ { j }$ are non-negative. Since it is unclear whether the positive pair is clean or noisy, a soft boundary denoted as $\sigma U _ { p }$ is computed according to the soft label $\hat { y } _ { i }$ . $\sigma U _ { p }$ denotes the dynamic optimization upper boundary for positive pair, σ = zzyˆ −1 , and $\textbf { \em z }$ is a relaxation factor. When the soft label $\hat { y } ^ { i } = 1$ , the dynamic optimization upper boundary equals to the ideal optimal boundary. Obviously, the soft label $\hat { y } ^ { i }$ is crucial to the learning of positive pairs, we will elaborate detailedly the estimation of soft label $\hat { y } ^ { i }$ in the next subsection.

With Eq. (5), we can accurately penalize the potentiallynoisy positive samples. The gradients for positive and negative samples in $l _ { a s y } ( I _ { i } , T _ { i } , \hat { T } _ { j } )$ are listed as follows:

$$
\frac { \partial l _ { a s y } ( I _ { i } , T _ { i } , \hat { T } _ { j } ) } { \partial ( T _ { i } ) } = \lambda A \big ( 2 S ( I _ { i } , T _ { i } ) - \sigma U _ { p } - ( 1 - m ) \big ) \frac { \partial S ( I _ { i } , T _ { i } ) } { \partial ( T _ { i } ) }
$$

$$
\frac { \partial l _ { a s y } ( I _ { i } , T _ { i } , \hat { T } _ { j } ) } { \partial \hat { T } _ { j } } = 2 A \frac { e x p ( \lambda ( ( s ( I _ { i } , \hat { T } _ { j } ) ^ { 2 } - m ^ { 2 } ) ) } { \sum _ { l = 1 } ^ { N - 1 } e x p ( \lambda ( s ( I _ { i } , \hat { T } _ { j } ) ^ { 2 } - m ^ { 2 } ) ) } \lambda ( s ( I _ { i } , \hat { T } _ { j } ) ) \frac { \partial S ( I _ { i } , \hat { T } _ { j } ) } { \partial ( \hat { T } _ { j } ) }
$$

where $A = 1 - e x p ^ { - l a s y ( l _ { i } , T _ { i } , \hat { T } _ { j } ) }$ . From the above two formulas, we can observe that due to the asymmetric optimization fashion, penalizing noisy positive samples with the adaptive upper boundary would have a minimal impact on the optimization of negative ones. As the model stops optimizing noisy positive samples, it does not cease optimization for the corresponding negative samples, enhancing the utilization of supervision from negative ones.

# Soft Label Estimation with VBGMM

Soft label estimation aims to distinguish the clean pairs from noisy ones. Due to the memorization effect of DNNs, DNNs tend to memorize simple pairs first, and the loss of clean pair is lower than that of noisy pair at the initial stage of training. Therefore, like (Yang et al. 2023a), a warmup training is conducted for training our image-text matching model, and the loss of image-text pairs is denoted as $\ell _ { i }$ :

$$
\ell _ { ( f , g , S ) } = \{ \ell _ { i } \} _ { i = 1 } ^ { N } = l _ { t r i } ( I _ { i } , T _ { i } ) _ { i = 1 } ^ { N }
$$

According to the difference about loss distribution of clean pair and noisy pair, the original dataset is divided into clean subset and noisy subset. Existing works widely use the Gaussian Mixture Model (GMM) (Huang et al. 2021) to model the difference about loss distribution, while the vanilla GMM can’t exploit the priori for distribution modeling, bringing a certain uncertainty to the parameters of GMM. Recent research demonstrates that Variational Bayesian Gaussian Mixture Model (VBGMM) shows better performance in dividing different categories of data (Chakladar, Roy, and Chang 2024), which introduces prior distribution of parameters and Bayesian inference to approximate a posteriori distribution $p ( \pi , Z , \mu , \Sigma / X )$ , and it allows for dealing with the uncertainty of model parameters, generating more reliable parameters of probability density.

$$
p ( \theta , Z | X , m ) = \prod _ { k } p ( \theta _ { k } | X _ { k } , m ) p ( Z _ { k } | X _ { k } , m )
$$

where $Z$ is the hidden variable of model $\mathbf { m }$ , $p ( Z | X , m )$ is the approximate posterior distribution of model parameters, and $p ( \theta | X , m )$ refers to the solution of VBGMM, where Expectation Maximization algorithm is used to perform optimization. Through the above formula, we can get precious parameters of probability density, and the loss of per-sample is fitted as follows:

$$
p ( \ell _ { i } | \boldsymbol { \theta } ) = \sum _ { k = 1 } ^ { K } \pi _ { k } \phi ( \ell _ { i } | \theta _ { k } )
$$

where $K = 2$ , $\pi _ { k }$ and $\phi _ { k }$ represent the mixture coefficient and the probability density function of the $k$ -th component in VBGMM. Then, it computes the posterior probability $p ( k | \ell _ { i } )$ as the clean probability of $i$ -th sample:

$$
p ( k | \ell _ { i } ) = p ( k ) p ( \ell _ { i } | k ) / p ( \ell _ { i } )
$$

where $k \in \{ 0 , 1 \}$ denotes the data pair $( I _ { i } , T _ { i } , y _ { i } )$ is clean or noisy. According to the above computed probability of each pair, we can partition the dataset into clean subset $D _ { c l e a n }$ and noisy subset $D _ { n o i s e }$ as follows:

$$
\begin{array} { r l } & { D _ { c l e a n } = \{ ( I _ { i } , T _ { i } ) | p ( k = 0 | \ell _ { i } ) > \delta , \forall ( I _ { i } , T _ { i } ) \in D \} } \\ & { D _ { n o i s e } = \{ ( I _ { i } , T _ { i } ) | p ( k = 0 | \ell _ { i } ) \leq \delta , \forall ( I _ { i } , T _ { i } ) \in D \} } \end{array}
$$

where $\delta$ is a threshold for partitioning the dataset into clean and noisy subsets, which is generally empirically set to 0.5. Since the model’s predictions can lead to self-reinforcing errors and error accumulation in training, the co-training strategy is employed to mitigate this error. Specifically, we train two models simultaneously, and each model is equipped with a different initialization and batch sequence. The detailed calculation of soft label can be referred to (Huang et al. 2021).

# Robust Training

After obtaining the estimated soft label, we adopt the Eq. (5) to perform cross-modal matching. To ensure consistent performance across image and text modalities, we employ bidirectional matching to encompass both image-to-text and text-to-image tasks as follows:

$$
L ( I _ { i } , T _ { i } ) = l _ { a s y } ( I _ { i } , T _ { i } , \hat { T } _ { j } ) + l _ { a s y } ( T _ { i } , I _ { i } , \hat { I } _ { h } )
$$

With the above formula, we can accurately penalize the potentially-noisy positive samples while having a minimal impact on the optimization of negative ones, achieving noise-robust image-text matching.

# Experiments

# Experimental Setting

Datasets To comprehensively evaluate the effectiveness of our proposed method, a series of experiments are conducted on three mainstream image-text matching benchmarks. Specifically, the performances of our method are shown under synthetic noise conditions on Filckr30K (Young et al. 2014) and MS-COCO (Lin et al. 2014), and under realworld noise conditions on Conceptual Captions (Sharma et al. 2018). More details of the three datasets are listed as follows:

• Flickr30K contains 31,000 images collected from the Flickr website, and each image is accompanied with 5 corresponding descriptive captions. Following (Qin et al. 2022), we use 1,000 pairs for validation, 1,000 pairs for testing, and 29,000 pairs for training.   
• MS-COCO is another popular dataset for cross-modal learning, which collects 123,287 images associated with 5 captions. Following the split in (Lee et al. 2018), 5000 pairs are used for validation, 5,000 pairs for testing, and 113,287 pairs for training.   
• Conceptual Captions contains 3,334,173 images with one caption. It is a large scale real-world dataset with about $3 \% \sim 2 0 \%$ pairs being mismatched, because it is automatically collected from the Internet. Following (Qin et al. 2022), we use a subset of Conceptual Captions, i.e., CC152K. In our experiment setting, 1,000 pairs are used for validation, 1,000 pairs for testing, and 150,000 pairs for training.

Evaluation Metrics Recall at $\operatorname { K } ( \mathbb { R } \ @ \operatorname { K } )$ is a widely-used metric to evaluate the performance of text-image retrieval. $\operatorname { R @ K }$ primarily reports the proportion of successfully retrieved relevant items in the top-K results given a query. In our experiments, we mainly report $\mathbf { R } @ 1$ , $\operatorname { R @ 5 }$ , $\mathbf { R } @ 1 0$ and the sum of the three Recalls $( \mathrm { r S u m } )$ for both text-to-image and image-to-text retrieval.

# Implementation Details

The experiments are conducted on NVIDIA RTX 3090 in Pytorch-2.0.1. For fairness in our experiments, our approach employs the same backbone as NCR, and the SGR model (Diao et al. 2021) is adopted at capturing the relationship between local alignments. Specifically, we first leverage the Faster-RCNN (Ren et al. 2015) to extract the top 36 regions for every image as a preprocess, and incorporate a fully-connected layer to serve as image feature encoder f. The Bi-GRU (Schuster and Paliwal 1997) serves as text feature encoder g. The similarity function S is computed by combining local and global features using graph reasoning techniques (Diao et al. 2021). To mitigate errors from selfreinforcement, we employed a co-training strategy. For the Flickr30K, we conduct 5 warmup training epochs, while 10 warmup epochs for MS-COCO and CC152K. The total number of training epochs after warmup is 40, 30, and 40 for Flickr30K, MS-COCO and CC152K, respectively. The Adam optimizer is used for training with a batch size of 180, and the initial learning rate is 0.0002. In hyperparameter settings, the margin value $m _ { 0 }$ is set to 0.2, the scaling factor $\lambda$ is set to 64, and the relaxation factor $z$ is 3. For fitting VBGMM, we set the maximum number of iterations to 10 with a regularization penalty coefficient of 0.0005.

# Comparison with State-of-the-art Methods

To demonstrate the effectiveness of our proposed ASL, we compared our approach with several state-of-the-art methods, including baseline models like SCAN (Lee et al. 2018), SGR, SAF (Diao et al. 2021), and robust noisy learning methods including NCR (Huang et al. 2021), DECL (Qin et al. 2022), RCL (Hu et al. 2023), MSCN (Han et al. 2023),

<html><body><table><tr><td rowspan="3">Noise ratio</td><td rowspan="3">Methods</td><td colspan="6">Flickr30xt→Image</td><td colspan="6"></td></tr><tr><td colspan="3">Image-→Text</td><td colspan="3"></td><td colspan="3">Image-→Text</td><td colspan="3">MS COceot-→Image</td></tr><tr><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1 R@5</td><td>R@10</td><td>rSum</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1 R@5</td><td>R@10</td><td>rSum</td></tr><tr><td rowspan="8">20%</td><td>SCAN SGR</td><td>58.5 55.9</td><td>81.0 81.5</td><td>90.8</td><td>35.5 65.0</td><td>75.2</td><td>406.0 408.6</td><td>62.2 25.7</td><td>90.0 58.8</td><td>96.1 75.1</td><td>46.2 23.5</td><td>80.8 89.2 75.1</td><td>464.5 317.1</td></tr><tr><td></td><td></td><td>88.9 93.9</td><td>40.2 49.7</td><td>66.8 73.6</td><td>75.3</td><td></td><td></td><td></td><td>57.8</td><td>58.9 86.4</td><td>91.9</td><td></td></tr><tr><td>SAF</td><td>62.8</td><td>88.7</td><td></td><td></td><td>78.0</td><td>446.7</td><td>71.5</td><td>94.0 97.5</td><td></td><td></td><td></td><td>499.1</td></tr><tr><td>NCR</td><td>75.0</td><td>93.9 97.5</td><td>58.3</td><td>83.0</td><td>89.0</td><td>496.7</td><td>76.6</td><td>95.6 98.2</td><td>62.5</td><td>89.3</td><td>95.3</td><td>517.5</td></tr><tr><td>DECL</td><td>74.5</td><td>92.9 97.1</td><td>53.6</td><td>79.5</td><td>86.8</td><td>484.4</td><td>75.6</td><td>95.1 98.3</td><td>59.9</td><td>88.3</td><td>94.7</td><td>511.9</td></tr><tr><td>RCL</td><td>74.2</td><td>91.8</td><td>96.9 55.6</td><td>81.2</td><td>87.5</td><td>487.2</td><td>77.0</td><td>95.5</td><td>98.1 61.3</td><td>88.8</td><td>94.8</td><td>515.5</td></tr><tr><td>MSCN</td><td>76.4</td><td>94.5</td><td>97.6 58.8</td><td>83.5</td><td>89.2</td><td>500.0</td><td>78.1</td><td>97.2</td><td>98.8 64.3</td><td>90.4</td><td>95.8</td><td>524.6</td></tr><tr><td>BiCro</td><td>76.5</td><td>93.1 97.4</td><td>58.1</td><td>82.3</td><td>88.5</td><td>495.9</td><td>76.6</td><td>95.4</td><td>98.2 61.3</td><td>88.8</td><td>94.8</td><td>515.1</td></tr><tr><td>CRCL</td><td>78.9</td><td>94.8</td><td>97.9</td><td>58.7 83.0</td><td>89.2</td><td>502.5</td><td>77.8</td><td>96.1</td><td>98.5</td><td>63.4</td><td>90.3 95.9</td><td>522.0</td></tr><tr><td>L2RM</td><td>76.5</td><td>93.7</td><td>97.3</td><td>55.5 81.5</td><td>88.0</td><td>492.5</td><td>78.4</td><td>95.7</td><td>98.3</td><td>62.1</td><td>89.1 94.9</td><td>518.5</td></tr><tr><td>ASL(Ours)</td><td>79.2</td><td>93.3</td><td>97.0</td><td>59.6 84.3</td><td>90.2</td><td>503.8</td><td>79.7</td><td>96.9</td><td>98.9</td><td>65.3</td><td>90.7 95.9</td><td>527.4</td></tr><tr><td>SCAN</td><td>26.0</td><td>57.4</td><td>71.8</td><td>17.8 40.5</td><td>51.4</td><td>264.9</td><td>42.9</td><td>74.6</td><td>85.1</td><td>24.2</td><td>52.6 63.8</td><td>343.2</td></tr><tr><td rowspan="8">40%</td><td>SGR</td><td>4.1</td><td>16.6 24.1</td><td>4.1</td><td>13.2</td><td>19.7</td><td>81.8</td><td>1.3</td><td>3.7</td><td>6.3</td><td>0.5 2.5</td><td>4.1</td><td>18.4</td></tr><tr><td>SAF</td><td>7.4</td><td>19.6 26.7</td><td>4.4</td><td>12.2</td><td>17.0</td><td>87.3</td><td>13.5</td><td>43.8 48.2</td><td>16.0</td><td>39.0</td><td>50.8</td><td>211.3</td></tr><tr><td>NCR</td><td>68.1</td><td>89.6 94.8</td><td>51.4</td><td>78.4</td><td>84.8</td><td>467.1</td><td>76.6</td><td>95.6</td><td>98.2 61</td><td>88.9</td><td>94.9</td><td>515.2</td></tr><tr><td>DECL</td><td>72.7</td><td>92.3 95.4</td><td>53.4</td><td>79.4</td><td>86.4</td><td>479.6</td><td>75.6</td><td>95.5 98.3</td><td>59.5</td><td>88.3</td><td>94.8</td><td>512.0</td></tr><tr><td>RCL</td><td>71.3</td><td>91.1 95.3</td><td>51.4</td><td>78.0</td><td>85.2</td><td>472.3</td><td>73.9</td><td>94.9</td><td>97.9 59</td><td>87.4</td><td>93.9</td><td>507.0</td></tr><tr><td>MSCN</td><td>74.4</td><td>93.2</td><td>96.0 55.3</td><td>80.4</td><td>86.8</td><td>486.1</td><td>74.5</td><td>96</td><td>98.1</td><td>60.8 89.0</td><td>95.0</td><td>513.4</td></tr><tr><td>BiCro</td><td>74.6</td><td>92.7 96.2</td><td>55.5</td><td>81.1</td><td>87.4</td><td>487.5</td><td>75.1</td><td>95.9</td><td>98.3</td><td>59.8 89.1</td><td>94.9</td><td>513.1</td></tr><tr><td>CRCL</td><td>74.1</td><td>92.6 96.9</td><td>55.5</td><td>80.9</td><td>87.6</td><td>487.6</td><td>76.6</td><td>95.6</td><td>98.5</td><td>62.3 89.7</td><td>95.4</td><td>518.1</td></tr><tr><td>L2RM</td><td>75.8</td><td>93.2</td><td>96.9</td><td>56.3 81.0</td><td>87.3</td><td>490.5</td><td>75.2</td><td>94.8</td><td>98.1</td><td>59.4</td><td>87.8 94.1</td><td>509.4</td></tr><tr><td>ASL(Ours) SCAN</td><td>77.5 13.6</td><td>93.3</td><td>97.4</td><td>58.3</td><td>83.0</td><td>89.4</td><td>498.9</td><td>79.0 96.6</td><td>98.6</td><td>63.5</td><td>89.7 2.4</td><td>95.6</td><td>523.0</td></tr><tr><td rowspan="8"></td><td></td><td>36.5</td><td>50.3</td><td>4.8</td><td>13.6</td><td>19.8</td><td>138.6</td><td>29.9</td><td>60.9</td><td>74.8 1.0</td><td>0.9</td><td>4.1</td><td>173.0</td></tr><tr><td>SGR</td><td>1.5</td><td>6.6 9.6</td><td>0.3</td><td>2.3</td><td>4.2</td><td>24.5</td><td>0.1</td><td>0.6</td><td>0.1</td><td>0.5</td><td>1.1</td><td>3.4</td></tr><tr><td>SAF</td><td>0.1</td><td>1.5 2.8</td><td>0.4</td><td>1.2</td><td>2.3</td><td>8.3</td><td>0.1</td><td>0.5</td><td>0.7</td><td>0.8 3.5</td><td>6.3</td><td>11.9</td></tr><tr><td>NCR</td><td>13.9</td><td>37.7</td><td>50.5 11.0</td><td>30.1</td><td>41.4</td><td>184.6</td><td>0.1</td><td>0.3</td><td>0.4</td><td>0.5 1.0</td><td>1.0</td><td>2.4</td></tr><tr><td>DECL</td><td>65.2</td><td>88.4</td><td>94.0</td><td>46.8 74.0</td><td>82.2</td><td>450.6</td><td>73.0</td><td>94.2</td><td>97.9</td><td>57.0 86.6</td><td>93.8</td><td>502.5</td></tr><tr><td>RCL</td><td>62.3</td><td>86.3</td><td>92.9</td><td>45.1 71.3</td><td>80.2</td><td>438.1</td><td>62.3</td><td>86.3</td><td>92.9</td><td>45.1 71.3</td><td>80.2</td><td>438.1</td></tr><tr><td>MSCN</td><td>67.5</td><td>88.4</td><td>93.1</td><td>48.7 76.1</td><td>82.3</td><td>456.1</td><td>73.7</td><td>95.1</td><td>98.5</td><td>57.0 86.9</td><td>94.0</td><td>505.2</td></tr></table></body></html>

Table 1: Comparison of different methods with noise $20 \%$ , $40 \%$ , and $60 \%$ on the Flickr30K and MS-COCO 1K. Best result are highlighted in each column.

BiCro (Yang et al. 2023a), CRCL (Qin et al. 2023) and L2RM (Han et al. 2024). For fairness in comparison, the SGR model is employed to capture the relationship between local alignments in the compared methods. Since the data in Flickr30K and MS-COCO are considered as correctlymatched pairs, we synthesize noisy pairs by randomly shuffling the descriptions corresponding to the images, where three kinds of noise rates are set by $20 \%$ , $40 \%$ and $60 \%$ . The CC152K dataset collected from the web reflects the noise conditions of real-world dataset.

Experiment on Synthetic Noise Table 1 shows the experimental results of different methods on the Flickr30K and MS-COCO datasets with synthetic noise rate of $20 \%$ , $40 \%$ and $60 \%$ . Note that we report the average results over 5 folds of 1K test images for MS-COCO. The results show that our proposed ASL is significantly superior to the compared baselines like NCR, DECL, CRCL and L2RM. Specifically, compared to the pioneer method NCR, our ASL improves the sum of Recall from 487.2 to 503.6 on the Flick30k with noise rate $20 \%$ , and 517.5 to 527.4 on the MS-COCO with noise rate $20 \%$ . Under the medium noise rate $40 \%$ , our ASL achieves the improvements about sum of Recall by 11.3 and 8.4 on the Flick30k compared to the baselines CRCL and L2RM, as well as the improvements by 4.9 and 13.6 on the MS-COCO. Under the high noise rate $60 \%$ , the ASL consistently outperforms the baselines at all of the metrics $\mathbf { R } @ 1$ , $\mathbb { R } \ @ 5$ and $\mathbf { R } @ 1 0$ . The $\mathbb { R } \ @ 1$ metric also demonstrates the highest accuracy, and our ASL exceeds the baseline methods on the two datasets. From the above results, we can observe ASL shows a significant gain with the noise rate increasing, and this means that our method is robust to noisy data. The explanation is that (1) we are the first to construct an asymmetric learning fashion to penalty the potentially-noisy positive samples while having a minimal effect on the optimization of negative ones; (2) The VBGMM is presented to model the distribution of per-sample loss for effectively distinguishing clean pairs from noisy ones.

Experiment on Real-World Noise Table 2 gives the experimental results of our proposed ASL and the baselines on the CC152K dataset in both Image $$ Text and Text $$ Image retrieval tasks. From this table, as can be seen that our method demonstrates more competitive retrieval performance compared to the baselines under the real-world noise condition. Specifically, our ASL achieves improvement by 1.2, 12.7 and 3.6 about the sum of Recall compared to the methods MSCN, BiCro and L2RM, respectively. In the Tex $$ Image task, the most stringent retrieval metric $\mathbf { R } @ 1$ achieves improvement by 3.5 and 0.9 compared to the methods BiCro and L2RM, respectively. Similarly, our ASL also shows notable retrieval performance improvements about the metrics $\mathbb { R } \ @ 5$ and $\mathbf { R } @ 1 0$ . Additionally, compared to the pioneering NCR, our approach surpasses NCR with a 12.3 increase about the sum of Recall. These results further demonstrate that our method effectively enhances retrieval accuracy in the real-world noisy scenario.

Table 2: Comparison of different methods on the CC152K.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="3">Image->Text</td><td colspan="3">Text-→Image</td><td></td></tr><tr><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>rSum</td></tr><tr><td>SCAN</td><td>30.5</td><td>55.3</td><td>65.3</td><td>26.9</td><td>53.0</td><td>64.7</td><td>295.7</td></tr><tr><td>SGR</td><td>11.3</td><td>29.7</td><td>39.6</td><td>13.1</td><td>30.1</td><td>41.6</td><td>165.4</td></tr><tr><td>SAF</td><td>31.7</td><td>59.3</td><td>68.2</td><td>31.9</td><td>59.0</td><td>67.9</td><td>318.0</td></tr><tr><td>NCR</td><td>39.5</td><td>64.5</td><td>73.5</td><td>40.3</td><td>64.6</td><td>73.2</td><td>355.6</td></tr><tr><td>RCL</td><td>38.3</td><td>63.0</td><td>70.4</td><td>39.2</td><td>63.2</td><td>72.3</td><td>346.4</td></tr><tr><td>DECL</td><td>36.2</td><td>63.6</td><td>73.2</td><td>37.1</td><td>63.6</td><td>73.7</td><td>347.4</td></tr><tr><td>MSCN</td><td>40.1</td><td>65.7</td><td>76.6</td><td>40.6</td><td>67.4</td><td>76.3</td><td>366.7</td></tr><tr><td>BiCro</td><td>39.7</td><td>64.6</td><td>72.6</td><td>39.2</td><td>65.0</td><td>74.1</td><td>355.2</td></tr><tr><td>L2RM</td><td>39.5</td><td>66.2</td><td>76.0</td><td>41.8</td><td>65.9</td><td>74.9</td><td>364.3</td></tr><tr><td>ASL(Ours)</td><td>39.1</td><td>65.2</td><td>76.9</td><td>42.7</td><td>67.4</td><td>76.6</td><td>367.9</td></tr></table></body></html>

Table 3: Ablation study about different components on the Flickr30K with noise $60 \%$ .   

<html><body><table><tr><td colspan="2">Methods</td><td colspan="3">Image->Text</td></tr><tr><td>Warmup</td><td>VBGMM lasy</td><td>R@1</td><td>R@5</td><td>R@10</td></tr><tr><td>√</td><td></td><td>13.9</td><td>37.7</td><td>50.5</td></tr><tr><td>√</td><td>√</td><td>70.6</td><td>89.5</td><td>93.7</td></tr><tr><td>√</td><td></td><td>71.3</td><td>90.4</td><td>94.1</td></tr><tr><td></td><td>√</td><td>16.4</td><td>28.1</td><td>35.9</td></tr><tr><td>√</td><td>√</td><td>73.9</td><td>92.4</td><td>96.2</td></tr><tr><td colspan="3">Methods</td><td colspan="2">Text-→Image</td></tr><tr><td>Warmup</td><td>VBGMM</td><td>lasy</td><td>R@1</td><td>R@5 R@10</td></tr><tr><td>√</td><td></td><td></td><td>30.1</td><td>41.4</td></tr><tr><td>√</td><td>√</td><td>11.0 51.2</td><td>77.4</td><td>84.6</td></tr><tr><td>√</td><td></td><td>52.5</td><td>78.1</td><td>85.8</td></tr><tr><td></td><td>√</td><td>13.4</td><td>25.9</td><td>30.5</td></tr><tr><td>√</td><td>√</td><td>√ √</td><td>54.8 80.7</td><td>87.1</td></tr></table></body></html>

486 488 484 484 482 480 480 478 476 472 0 2 3 5 10 15 0 16 32 64 96 128 Relaxation factor z Scale factor λ

# Experimental Analysis

Ablation Study To verify the effectiveness of each component in our proposed ASL, we give the Recall value under different combinations of VGBMM, the proposed loss $l _ { a s y }$ as well as the Warmup in Table 3. The symbol $\checkmark$ in this table means that the corresponding component is used. Noting that, if the VBGMM or $l _ { a s y }$ is not selected, we employ the plain GMM or triplet loss $\dot { l } _ { t r i } ^ { \prime }$ as its corresponding alternative. For example, the fifth row denotes that it uses the plain GMM as an alternative of VBGMM. From this table, we can observe that the retrieval Recall suffers from a dramatic degradation when the Warmup operation isn’t performed, and it demonstrates the Warmup is crucial to the model’s training. Meanwhile, the performance of the combination Warmup $^ +$ VBGMM or $\mathrm { W a r m u p } + l _ { a s y }$ is inferior to the proposed ASL, and these results demonstrate that all components are important to achieve competitive results. Additionally, the performance of combination Warmu $_ { \cdot \mathsf { p } + } \cdot$ VBGMM or $\mathrm { W a r m u p } + l _ { a s y }$ still outperforms most of existing robust methods like NCR, DECL and MSCN, which further proves the effectiveness of the proposed ASL.

Hyper-parameter Analysis The ASL method incorporates two main hyper-parameters including $\lambda$ and $z$ for robust image-text matching, and Fig. 3 shows its effect on the Flick30k with noise $40 \%$ . z is a relaxation factor to compute the self-paced weighting optimized boundary of positive pairs. According to the results, ASL shows stability when $z$ is in the range [2, 5], while $z = 3$ is chosen for optimal performance. When z takes a bigger value, it would impose a heavy penalty on optimizing positive sample, resulting in a suboptimal performance. $\lambda$ is a scale factor to control the robustness of our proposed asymmetric dynamic matching loss. We provide the sum of Recall with the varied value from 16 to 128. The results indicate a better performance can be obtained when $\lambda = 6 4$ . It becomes unstable with a larger value in our implementation, because a higher value would bring a stronger gradient in the proposed loss, causing the model unstable and underfit.

# Conclusion

This paper reveals a common issue that insufficient optimization of positive and negative samples is caused by the symmetric optimization in current noisy correspondence rectification methods. It proposes a novel noise-robust learning framework named ASL for image-text matching. Specifically, through independently optimizing positive and negative samples, we achieve separate optimization for potentially-noisy positive samples, thereby avoiding inappropriate or insufficient penalty on negative samples. Additionally, we present a variational Bayesian strategy in the EM algorithm to enhance the modeling capability of GMM for the distribution of per-sample loss, incorporating the prior distribution of parameters for modeling. Extensive experiments are conducted on three benchmarks, and The effectiveness of the proposed method has been validated in both synthetic and real noise scenarios.

# Acknowledgments

This work is supported in part by the National Nature Science Foundation of China (No. 62402532), in part by the Hunan Provincial Natural Science Foundation of China (No. 2024JJ6526), in part by the Science and Technology Plan of Hunan Province (No. 2023GK2013), and in part by the High Performance Computing Center of Central South University.