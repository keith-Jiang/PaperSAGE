# Monitoring Primitive Interactions During the Training of DNNs

Jie $\mathbf { R e n } ^ { 1 }$ , Xinhao Zheng1, Jiyu $\mathbf { L i u } ^ { 1 , 2 * }$ , Andrew Lizarraga3, Ying Nian $\mathbf { W } \mathbf { u } ^ { 3 }$ , Liang $\mathbf { L i n ^ { 4 } }$ , Quanshi Zhang1†

1Shanghai Jiao Tong University 2Dartmouth College 3University of California, Los Angeles 4Sun Yat-Sen University

# Abstract

This paper focuses on the newly emerged research topic, i.e., whether the complex decision-making logic of a DNN can be mathematically summarized into a few simple logics. Beyond the explanation of a static DNN, in this paper, we hope to show that the seemingly complex learning dynamics of a DNN can be faithfully represented as the change of a few primitive interaction patterns encoded by the DNN. Therefore, we redefine the interaction of principal feature components in intermediate-layer features, which enables us to concisely summarize the highly complex dynamics of interactions throughout the learning of the DNN. The mathematical faithfulness of the new interaction is experimentally verified. From the perspective of learning efficiency, we find that the interactions naturally belong to five groups (reliable, withdrawn, forgotten, betraying, and fluctuating interactions), each representing a distinct type of dynamics of an interaction being learned and/or being forgotten. This provides deep insights into the learning process of a DNN.

# 1 Introduction

In the field of interpretable artificial intelligence, one of the fundamental objectives of a theory system is to let the seemingly extremely complex decision-making logic of a deep neural network (DNN) be faithfully explained as a small set of simple logics. Unlike other explanation methods (Elhage et al. 2021; Meng et al. 2022; Zhao et al. 2022; Park et al. 2022; Olsson et al. 2022; Fel et al. 2023), this is a newly emerging mathematical problem in recent years, because it aims to answer whether the essential logic of a DNN is simple enough to be explained to human beings, i.e., the existence of human-understandable explanation for a DNN.

Towards this problem, an interaction-based theory system has been built up recently, containing about 20 papers (surveyed by Ren et al. (2024)). Typically, Ren et al. (2023a); Li and Zhang (2023) discovered and Ren et al. (2024) proved1 that we can always use the numerical utility of a few symbolic interactions between input variables to accurately explain all subtle changes of network outputs under a massive number of input variations. It is also found (Zhou et al. 2024) that the complexity of interactions could explain the generalization power of DNNs.

Beyond the above explanation of a static (trained) DNN, in this paper, we hope to explore whether the entire learning dynamics of a DNN, which is believed to be much more complex than a static DNN, can also be concisely explained as symbolic interactions. The explanation of the complex learning dynamics is mentioned by several previous studies (Zhou et al. 2024; Li and Zhang 2023; Ren et al. 2024; Chen et al. 2024; Cheng et al. 2024), and they all considered this as the last piece of the puzzle of the interaction-based explanation system, and also one of the biggest challenges that has hampered the field for years.

The challenges of explaining learning dynamics come from the high-dimensional changes in network parameters, which are complex and even chaotic. However, we hope • to summarize the highly complex learning dynamics of a DNN into the dynamics of a few interactions; • to explain the learning efficiency of a DNN, i.e., answering how many interactions are learned from the beginning of the training and how many interactions are discarded later; • to clarify whether the DNN learns all primitive patterns simultaneously.

Originally, the interaction metric was used to quantify the non-linear relationship encoded by a DNN. For example, Figure 1(b) shows a DNN implicitly encodes an interaction between two eye patches and a nose patch, and this interaction makes a numerical utility on the classification score of cat. Masking any one among three patches will invalidate this interaction and remove its utility from the output score.

However, in this study, we hope to use a few salient interactions (interactions with large interaction effects) to explain even more complex learning dynamics. Thus, how to reduce

DNN’s outputs $v ( x _ { S } )$ DNN’s Logical model’s   
Logical model’s outputs ℎ(𝑥𝑆) output 𝑣 𝑥𝑆 output ℎ(𝑥𝑆) (𝑥, 𝑣(𝑥))   
(𝑥𝑆 , 𝑣(𝑥𝑆 )) (𝑥𝑆2, 𝑣(𝑥𝑆2)) 𝐼𝑎𝑛𝑑(𝑆1) f𝐼ace i(n𝑆te)ractio⋯n 𝐼𝑎𝑛𝑑(𝑆𝑘) 百 𝑥1 𝑥2 𝑥3 P X   
2𝑛 masked input images Input image (a) (b)

the complexity of the explanation and concisely summarize dynamics is the key point of this study.

Therefore, we redefine interactions on feature components in intermediate layers to obtain concise interactions for explanation. We consider the top-ranked principal feature components as basic “input variables” for interactions. Experiments have shown that the newly defined interaction enables us to use much sparser interactions between much fewer (less than 10) principal feature components to explain most information of a DNN’s learning dynamics (see Figure 2 and Figure 3) without losing explanation fidelity.

Surprisingly, we find that all interactions naturally belong to the following five categories in terms of learning efficiency. (1) Reliable interactions are stably learned throughout the entire training process of the DNN. (2) Withdrawn interactions are learned in early epochs and then discarded in later epochs. (3) Forgotten interactions are initially salient but gradually forgotten in the following epochs. (4) Betraying interactions are learned to represent a certain classification utility (toward a specific category) in early epochs, but later shifted to an opposite classification utility in later epochs. (5) Fluctuating interactions keep fluctuating during the training of the DNN. In conclusion, we can consider reliable and forgotten interactions as efficiently learned knowledge, while betraying and withdrawn interactions reflect the trial-and-error process during learning.

Although interactions have encoded mixed semantics, the decomposition of the complex learning dynamics of DNNs into a few concise interactions still provides a new perspective to understanding the learning behavior of a DNN.

Contributions of this study are as follows. (1) We redefine interactions, which enable us to concisely summarize the highly complex learning dynamics of a DNN into the change of a few interactions. (2) We find that all interactions naturally belong to five types, which reflect the DNN’s distinctive learning behavior of different inference patterns. (3) Various experiments have verified the mathematical faithfulness of the interaction-based explanation.

# 2 Primitive Interactions in DNNs Preliminary: Interactions

In this section, let us introduce the interaction, as well as a set of properties of interactions (Li and Zhang 2023; Ren et al. 2023a, 2024), as mathematical guarantees for the faithfulness of interaction-based explanation.

Defintion of AND-OR interactions. Given a trained DNN $\boldsymbol { v }$ , let $x \in \mathbb { R } ^ { n }$ denote an input sample with $n$ input variables (e.g., an image with $n$ image patches and a sentence with $n$ words), indexed by $N = \{ 1 , 2 , \dots , n \}$ . The DNN’s output is denoted by $v ( x ) \in \mathbb { R }$ . For example, in multi-category classification, $v ( x )$ is usually defined as the following confidence score (Deng et al. 2022).

$$
v ( x ) = p ( y = y ^ { * } | x ) / ( 1 - p ( y = y ^ { * } | x ) )
$$

where $y ^ { \ast }$ denotes the ground-truth label of the input $x$ . Given the trained model $v ( \cdot )$ and a set $S \subseteq N ( S \neq \varnothing )$ of input variables, numerical utilities of the AND interaction $\bar { I _ { \mathrm { a n d } } } ( S | x )$ and the OR interaction $I _ { \mathrm { o r } } ( S | x )$ between these variables can be computed as follows.

$$
\begin{array} { r l } & { I _ { \mathrm { a n d } } ( S | x ) = \displaystyle \sum _ { L \subseteq S } ( - 1 ) ^ { | S | - | L | } v _ { \mathrm { a n d } } ( x _ { L } ) , } \\ & { I _ { \mathrm { o r } } ( S | x ) = - \displaystyle \sum _ { L \subseteq S } ( - 1 ) ^ { | S | - | L | } v _ { \mathrm { o r } } ( x _ { N \setminus L } ) } \end{array}
$$

where $v _ { \mathrm { a n d } } ( x _ { L } ) = 0 . 5 v ( x _ { L } ) + \gamma _ { L }$ and $v _ { \mathrm { o r } } ( x _ { L } ) = 0 . 5 v ( x _ { L } ) -$ $\gamma _ { L }$ represent the output component for AND interactions and the output component for OR interactions, respectively. $x _ { L }$ denotes a masked input sample, in which the variables in $N \backslash L$ are masked.2 Then, $v _ { \mathrm { a n d } } \bar { ( } x _ { L } ) \in \mathbb { R }$ denotes the output on the masked input. $\gamma _ { L }$ is a learnable parameter to decompose $ { \boldsymbol { v } } (  { \boldsymbol { { x } } } _ { L } )$ into ${ v } _ { \mathrm { a n d } } ( x _ { L } )$ and $v _ { \mathrm { o r } } ( x _ { L } )$ .

In this way, the computation of ${ v } _ { \mathrm { a n d } } ( x _ { L } )$ and $v _ { \mathrm { o r } } ( x _ { L } )$ is implemented by learning the parameter $\gamma _ { L }$ via a LASSO-like sparisity loss for interactions, i.e., $\begin{array} { r } { \operatorname* { m i n } _ { \{ \gamma _ { L } \} } \sum _ { S \subseteq N , S \neq \emptyset } [ | \hat { I } _ { \mathrm { a n d } } ( S | x ) | + | I _ { \mathrm { o r } } ( S | x ) | ] } \end{array}$ .

Understanding AND-OR interactions via the universal-matching property of interactions. Each AND interaction $I _ { \mathrm { a n d } } ( S | x )$ represents a non-linear relationship between variables in $S$ , i.e., the co-appearance of all variables in $S$ will add a utility $I _ { \mathrm { a n d } } ( S | x )$ to the model output. On the other hand, each OR interaction $I _ { \mathrm { o r } } ( S | x )$ represents the OR relationship encoded by the model. The appearance of any variables in $S$ will add $I _ { \mathrm { o r } } ( S | x )$ to the output score. Figure 1(b) shows an example of an AND interaction $I _ { \mathrm { a n d } } ( S | x )$ of a face in the surrogate model. $I _ { \mathrm { a n d } } ( S | x )$ is triggered only when $x _ { 1 } , x _ { 2 } , x _ { 3 }$ all appear in the input image. In comparison, an OR interaction $I _ { \mathrm { o r } } ( S | x )$ is triggered when any input variable in $S$ appears.

According to Theorem 2.1, we construct a surrogate logical model based on all AND-OR interactions. Given each masked input $x _ { S }$ , the surrogate model first identifies a set of interactions triggered by $x _ { S }$ based on the AND-OR logic rule. Then, utilities of all these interactions are summed up as the output $h ( x _ { S } )$ . Theorem 2.1 proves that no matter how the input is randomly masked, the model output on the masked sample can always be approximated by the surrogate model based on utilities of a few interactions.

1 incomMeLP-5 0.2 TV neMwLsP-5 15 bikeMLP-5 2 LCSNTNM RVNG-G2-011 RVNG-G2-011 PointNet 0.0 0 0 0 0 0 101 102 100 101 102 100 101 102 7 200 4000 0 4000 6000 index of eigenvalues

Theorem 2.1 (Universal-approximation property of interactions, proved in Appendix E). Given an input sample $x$ , let $\Omega _ { s a l i e n t }$ denote the set of salient interactions. We consider interactions w.r.t. $| I _ { a n d } ( S | x ) | \geq \tau$ or $| I _ { o r } ( S | x ) | \ge \tau$ as salient interactions. We construct the surrogate model $h ( \cdot )$ to use AND-OR interactions extracted from the DNN $ { \boldsymbol { v } } (  { \boldsymbol { { x } } } _ { S } )$ for inference, $\begin{array} { r c l } { h ( x _ { S } ) } & { = } & { \sum _ { L \subset N , L \neq \emptyset } I _ { a n d } ( L | x ) } \end{array}$ · $\mathbb { 1 } ( x _ { S }$ triggers the AND relation L $\begin{array} { r } { \dot { \mathbf { \eta } _ { \mathcal { I } } } ) + \sum _ { L \subseteq N , L \neq \emptyset } I _ { o r } ( L | x ) . } \end{array}$ · $\mathbb { 1 } ( x _ { S }$ triggers the OR relation $L$ ) $ { \mathrm { ~  ~ \psi ~ } } +  { \mathrm { ~  ~ \psi ~ } }  { \boldsymbol { v } } (  { \boldsymbol { { x } } } _ { 0 } )$ = $\begin{array} { r l } { \sum _ { L \subset S , L \neq \emptyset } I _ { a n d } ( L | x ) \ + \ \sum _ { L \cap S \neq \emptyset } I _ { o r } ( L | x ) \ + \ v ( x _ { \emptyset } ) } & { { } \approx } \end{array}$ $\begin{array} { r } { \sum _ { L \subseteq S , L \neq \emptyset , L \in \Omega _ { s a l i e n t } } I _ { a n d } ( L | x ) + \sum _ { L \cap S \neq \emptyset , L \in \Omega _ { s a l i e n t } } I _ { o r } ( L | x ) } \end{array}$ $+ v ( x _ { \emptyset } )$ . $v ( x _ { \emptyset } )$ is a constant  hat represents the model output when all input variables are masked. No matter how we arbitrarily mask the variables in $x$ to obtain the masked inputs $x _ { S }$ w.r.t. a random subset $S \subseteq N$ , the surrogate model $h ( x _ { S } )$ can always mimic the DNN output $ { \boldsymbol { v } } (  { \boldsymbol { { x } } } _ { S } )$ on the masked input $x _ { S }$ , i.e., $\forall S \subseteq N , h ( x _ { S } ) = v ( x _ { S } )$ .

Sparsity property of salient interactions. Let us enumerate all $2 ^ { n }$ subsets $S \subseteq N$ and compute their interaction utilities. Ren et al. (2024) have proven1 that DNNs usually encode very sparse salient interactions, i.e., the number of salient interactions is $O ( n ^ { \delta } )$ $( \delta ~ \in ~ [ 1 . 9 , 2 . 2 ]$ empirically), which is extremely sparse w.r.t. all $2 ^ { n }$ subsets.

Generalization property. Li and Zhang (2023) have discovered the generalization ability of interactions. That is, people can extract a common set of interactions from different (but similar) inputs or different models, and these interactions are discriminative for classification.

The above sparsity, universal approximation, and generalization properties of interactions ensure that the interactions can be considered as primitive inference patterns for the model inference.

# Primitive Interactions on Features

Although we usually extract a few interactions from a fixed DNN, tracking the dynamics of interactions in all intermediate DNNs through the entire training process may significantly complicate the explanation. This is because DNNs trained after different epochs may generate fully different interactions. Therefore, the first challenge is to redefine the interaction to simplify the explanation, and meanwhile, the newly defined interaction should be powerful enough to faithfully reflect major changes in all training epochs.

Therefore, instead of taking raw pixels/words/3D points as input variables, we redefine interactions on principal feature components shared by all intermediate DNNs. Let us train a DNN, and collect the DNN trained after $K$ different checkpoints (epochs). Given an input sample, we extract the feature from a certain intermediate layer of the DNNs at these $K$ checkpoints, denoted by ${ f ^ { ( 1 ) } , \bar { f } ^ { ( 2 ) } , \dots , f ^ { ( K ) } \in }$ $\mathbb { R } ^ { m }$ . Subsequently, we conduct principal component analysis (PCA) on the $K$ features to compute the top $r$ principal directions (eigenvectors) $q _ { 1 } , q _ { 2 } , \ldots , q _ { r } \in \mathbb { R } ^ { m }$ corresponding to the largest $r$ eigenvalues. In this way, we extract feature components along the top $r$ principal directions, so as to use these feature components as basic “input variables” to define interactions. Specifically, for the intermediate-layer feature $f ^ { ( k ) }$ extracted after $k$ epochs, we decompose the feature $f ^ { ( k ) }$ into the following $( r + 2 )$ feature components.

Table 1: Classification accuracy when using the raw feature and using the top 10 feature components.   

<html><body><table><tr><td>Model</td><td>Dataset</td><td>Using raw f(k)</td><td>Using ∑l21 fi+f</td></tr><tr><td>MLP-5</td><td>income</td><td>0.92</td><td>0.94</td></tr><tr><td>MLP-5</td><td>TV news</td><td>0.86</td><td>0.85</td></tr><tr><td>MLP-8</td><td>income</td><td>0.95</td><td>0.90</td></tr><tr><td>ResNet</td><td>MNIST</td><td>1.00</td><td>1.00</td></tr><tr><td>ResNet</td><td>CIFAR-10</td><td>0.89</td><td>0.89</td></tr><tr><td>VGG</td><td>MNIST</td><td>1.00</td><td>1.00</td></tr><tr><td>VGG</td><td>CIFAR-10</td><td>0.98</td><td>0.97</td></tr></table></body></html>

$$
f ^ { ( k ) } = \sum _ { i \in N _ { \mathrm { f e a t u r e } } } f _ { i } + \bar { f } + \epsilon
$$

where $N _ { \mathrm { f e a t u r e } } = \{ 1 , 2 , \dots , r \}$ denotes the indices of top $r$ principal feature components. $f _ { i } = q _ { i } q _ { i } ^ { T } ( f ^ { ( k ) } - \bar { f } ) \in$ $\mathbb { R } ^ { m }$ represents the $i$ -th principal feature component. $f =$ $\textstyle \sum _ { k = 1 } ^ { K } { \bar { f } } ^ { ( k ) } / K$ denotes the average feature during the learning process. $\begin{array} { r } { \epsilon = f ^ { ( k ) } - \bar { f } - \sum _ { i \in N _ { \mathrm { f e a t u r e } } } f _ { i } } \end{array}$ represents the overall effect of the remaining $m - r$ feature components in $f ^ { ( k ) }$ .

In this way, if we consider $\bar { f } + \epsilon$ as a constant background, we can regard the $r$ feature components in $f ^ { ( k ) }$ as the variables involved in interactions. I.e., each interaction $S \subseteq N _ { \mathrm { f e a t u r e } }$ represents the collaborative relationship between feature components in $S$ . Here, because $f ^ { ( k ) }$ can be extracted from any epoch, we ignore the superscript $( k )$ . Then, for a subset $L \subseteq N _ { \mathrm { f e a t u r e } } ,$ $f _ { L }$ represents the masked feature when we mask feature components in $N _ { \mathrm { f e a t u r e } } \setminus L , ^ { 3 }$ $\begin{array} { r } { i . e . , f _ { L } = \sum _ { i \in L } f _ { i } + \sum _ { i \in N _ { \mathrm { f e a t u r e } } , i \notin L } b _ { i } + \bar { f } } \end{array}$ . We use $b _ { i } \ { \stackrel { \mathrm { d e f } } { = } } \ $

linear space income TV news bike SST-2 MNIST CIFAR-10 ShapeNet   
Relative strength 1.0 MLP-5 MLP-5 MLP-5 CNN VGG-11 VGG-11 PointNet Using principal 0.5 MLP-8 MLP-8 MLP-8 LSTM RN-20 RN-20 feature 0.0 components   
10-4 100 vUasriinagblreasw input 10-8 0 4000 8000 0 1000 2000 0 4000 8000 0 1000 2000 0 4000 8000 0 4000 8000 0 1000 2000 Index of interactions

iMnLcPo-5m:e3.0% 100 TMVL Pn-e58:w3s.08.9% MLP-58: 61.97.6% LCSNTNM: :0.04.3% RVNG-G2-01:10: .35.%0% CRVINGF-AG2-R01-:10:0.69.%3% 10 PSohiantpNeeNt:et1.7% 1000 200010-2 0 1000 00100 1000 20010- 1000 2010-2 0 1000 200010-2 1000 200010-1 1000 2000 Number $\alpha$ of the used salient interactions

$q _ { i } q _ { i } ^ { T } ( f | _ { \mathbb { E } [ x ] } - \bar { f } )$ to represent the masked state (or namely the baseline value) of the $i$ -th feature component. $f | _ { \mathbb { E } [ x ] }$ denotes the feature when the average value $\mathbb { E } [ x ]$ of all input samples in the training set is fed to the model. $b _ { i }$ represents the $i$ - th feature component in the feature $f | _ { \mathbb { E } [ x ] }$ . The mean value over different samples is a widely-used setting for baseline values (Dabkowski and Gal 2017), which alleviates the outof-the-distribution problem in practice.

The DNN output $v ( x )$ can be regarded as a function of the feature $f$ , i.e., $v ( x ) = j ( f )$ , where $g ( \cdot )$ denotes subsequent layers upon the feature $f$ . $g ( f _ { L } )$ denotes the DNN output on the masked feature. Thus, we can directly use Eq. (2) to compute interactions $I _ { \mathrm { a n d } } ( S | f )$ and $I _ { \mathrm { o r } } ( S | { \bar { f } } )$ on feature components by replacing $ { \boldsymbol { v } } (  { \boldsymbol { { x } } } _ { L } )$ with $g ( f _ { L } )$ .

Computational cost of interactions between feature components. Compared to interactions on raw input variables, interactions on feature components present a much smaller computational cost. For the input $\boldsymbol { x } \in \mathbb { R } ^ { n }$ , the computational cost of interactions on the $n$ input variables in $x$ is $2 ^ { n }$ . When we define interactions on top $r$ feature components $\mathit { \check { r } } \ll n$ in most cases), the computational cost of interactions is reduced to $2 ^ { r }$ , which is much less than $2 ^ { n }$ .

Experimental settings. We trained a 5-layer MLP (Ren et al. 2023b) (namely MLP-5) and an 8-layer MLP (Ren et al. 2023b) (namely MLP-8) on three datasets (Dua and Graff 2017), including the census income (namely income), TV News channel commercial detection (namely TV news), and bike sharing (namely bike) datasets. We also followed (Li and Zhang 2023) to train a CNN and a three-layer unidirectional LSTM on the SST-2 dataset (Socher et al. 2013). Besides, we trained VGG-11 (Simonyan and Zisserman 2014) and ResNet-18/20 (He et al. 2016) (namely

RN-18/20) on the MNIST (LeCun et al. 1998), CIFAR10 (Krizhevsky 2012), and Tiny ImageNet (Le and Yang 2015) datasets, and trained PointNet (Charles et al. 2017) on the ShapeNet (Yi et al. 2016) dataset. For each neural network, we analyzed features extracted from the (roughly) half depth, which well balanced the informativeness of the feature and the conciseness of the explanation. Please see Appendix F for the detailed experimental settings.

Justification of using principal feature components: how many principal feature components are needed as input variables? We conducted two experiments. In the first experiment, we verified that the used top-ranked feature components represented most signals in $f$ . For each DNN, we fed an input sample $x$ to the DNNs trained after $K$ different epochs, and extracted $K$ feature vectors $f ^ { ( 1 ) } , \ldots , f ^ { ( K ) }$ from these DNNs. Using the feature vectors collected from different samples at $K$ different epochs, we conducted PCA to compute eigenvalues in Figure 2. We found that in most DNNs, the top 10 eigenvalues were significantly larger than the rest. The long-tail components with very tiny eigenvalues did not reflect essential signals for the task. Therefore, we set $r = 1 0$ in all experiments.

In the second experiment, we compared the classification accuracy of using the entire feature $f$ with the classification accuracy of using the top 10 components of the feature $\textstyle { \bar { f } } + \sum _ { i = 1 } ^ { 1 0 } { \dot { f } } _ { i }$ . To this end, we masked other feature components in $\epsilon$ to obtain $\begin{array} { r } { f ^ { \prime } = \sum _ { i = 1 } ^ { 1 0 } f _ { i } + \bar { f } } \end{array}$ , according to Eq. (3), and fed $f ^ { \prime }$ back to the network for inference. We conducted experiments on four datasets, including the census, commercial, MNIST, and CIFAR-10 datasets. For each dataset, we randomly sample 100 samples and evaluate the classification accuracy of the network based on the original feature

𝑔(𝑓𝑆) 𝛼 = 20 𝛼 = 40 𝛼 = 60 5.0 100- 200   
00 50 100 100 -5.0 0 0 0+ 三 0 1000 -0.5 0.0 0.5 -0.5 0.0 0.5 -0.5 0.0 0.5 index of subsets S $\Delta g _ { \alpha } ( f _ { S } )$ △ga(fs) △ga(fs) (a) A sample in MLP-5 trained on the income dataset. 𝑔(𝑓𝑆) 𝛼 = 20 𝛼 = 40 𝛼 = 60   
5 100 100 100- 50 50 0.0 1 0 0 0 0 1000 -0.2 0.0 0.2 -0.2 0.0 0.2 -0.20.0 0.2 index of subsets S △ga(fs) △ga(fs) △ga(fs) (b) An image in RN-20 trained on the CIFAR-10 dataset.

$f$ . Table 1 shows that using the top 10 feature components did not significantly change the classification accuracy. In other words, the top 10 feature components had already represented most of the knowledge learned by the model.

Sparsity of Interactions Theorem 2.1 shows that the network output on an input sample can always be explained by a small set of interactions, no matter how we randomly mask the input sample. Then the principle of Occam’s Razor suggests that we can consider such interactions as primitive inference patterns encoded by the DNN. However, the proof of the sparsity (Ren et al. 2024) of interaction is conducted under three assumed common conditions1, which are difficult to examine in real DNNs. Besides, unlike (Ren et al. 2024), we use OR interactions. Therefore, we need to verify the sparsity of interactions on feature components.

We compared the sparsity of interactions on feature components with the sparsity of interactions on raw input variables. To extract interactions on raw input variables, we followed Ren et al. (2023b) to divide each input image in the MNIST and CIFAR-10 datasets into $7 \times 7$ and $8 \times 8$ patches, respectively. Then, we randomly sampled twelve image patches as input variables to compute interactions. For the ShapeNet dataset, we took the manually annotated parts provided by Li and Zhang (2023) as input variables. To compute interactions on feature components, we followed Appendix $\mathrm { ~ F ~ }$ to extract principal feature components. For simplicity, we concatenated strength $\vert I _ { \mathrm { a n d } } ( S \vert x ) \vert$ of $2 ^ { r }$ AND interactions and strength $| I _ { \mathrm { o r } } ( S | x ) |$ of $2 ^ { r }$ OR interactions to construct a $2 ^ { r + 1 }$ -dimensional vector $\boldsymbol { \mathit { I } }$ . The strength was further normalized by $I \gets I / \operatorname* { m a x } _ { i } I _ { i }$ . Figure 3 shows the curve of relative interaction strength sorted in descending order, which was averaged over different input samples. Using principal feature components could significantly enhance the sparsity of interactions.

Examining Faithfulness of Interactions In this section, we conducted two experiments to use interactions to mimic the entire model output $g ( f )$ , so as to evaluate the faithfulness of the interaction-based explanation. In the first experiment, we measured the matching error when we used salient interactions to match the model output. We followed Appendix $\mathrm { ~ F ~ }$ to extract AND-OR interactions. Let $\Omega _ { \alpha }$ denote the set of $\alpha$ salient interactions with the highest values of $| I _ { \mathrm { a n d / o r } } ( S | f ) |$ . We computed the matching error $\mathbb { E } _ { x } | \Delta g ( f ) | =$ $\mathbb { E } _ { f } [ | g ( f ) - \hat { g } _ { \alpha } ( f ) | ]$ , w.r.t. $\begin{array} { r } { \hat { g } _ { \alpha } ( f ) = g ( f _ { \varnothing } ) + \sum _ { S \in \Omega _ { \alpha } } I _ { \mathrm { a n d } } ( S | f ) + } \end{array}$ S Ω Ior(S|f ). We used different numbers α of salient interactions to compute the corresponding matching errors. Furthermore, we computed the least number $\hat { \alpha }$ of interactions that were required to cover $90 \%$ of the network output $g ( f )$ , i.e., $\hat { \alpha } = \operatorname* { m i n } \alpha$ s.t. $( | g ( f ) - \hat { g } _ { \alpha } ( f ) | ) / | g ( f ) | \leq 0 . 1 \AA$ . Figure 4 reports the average matching error over different samples and the average ratio of the minimum interaction number $( E _ { f } [ \hat { \alpha } / 2 ^ { r + 1 } ] )$ . The network outputs were usually well matched by only using less than $10 \%$ salient interactions.

The second experiment demonstrated that the sum of a few interactions could well approximate various network outputs on randomly masked features $\{ g ( f _ { S } ) \} _ { S }$ . Specifically, we used different numbers $( \alpha ~ \in ~ \{ 2 0 , 4 0 , 6 0 \} )$ of salient interactions to approximate the model outputs on $2 ^ { n }$ masked features of an input sample. Then, for each masked feature $f _ { S }$ , we computed $\Delta g _ { \alpha } ( f _ { S } ) = g ( f _ { S } ) - \hat { g } _ { \alpha } ( f _ { S } )$ as the approximation error on $f _ { S }$ , where $\hat { g } _ { \alpha } ( f _ { S } ) = g ( f _ { \varnothing } ) +$ $\begin{array} { r } { \sum _ { L \in \Omega _ { \alpha } , \emptyset \neq L \subseteq S _ { \alpha } } I _ { \mathrm { a n d } } ( L | f ) + \sum _ { L \in \Omega _ { \alpha } , L \cap S \neq \emptyset } I _ { \mathrm { o r } } ( L | f ) } \end{array}$ . Figure 5 shows network outputs on all $2 ^ { n }$ masked features of an input sample in ascending order and the approximation errors. For visualization, we averaged the approximation error over 50 neighboring masked features for smoothing. The results show that a small number (usually less than 60) of interactions could well approximate the varying network outputs on different masked features.

# 3 Emergence of Primitive Interactions Emergence of Interactions During Training

Five types of interactions. In this section, we analyze a DNN’s learning efficiency based on its learning dynamics of interactions during the learning process.

For an interaction pattern $S$ , let $\begin{array} { r l } { \nabla _ { t } I ( S | x , \theta _ { t } ) } & { { } = } \end{array}$ ∂I(S|x,θt) denote the slope of the interaction curve at the $t$ epoch. We observe the phenomena $w . r . t .$ . the values of $I ( S | x , \theta _ { t } )$ and $\nabla _ { t } I ( S | x , \theta _ { t } )$ in Table 2, and categorize them into five groups. Specifically, we categorize the curves of numerical utilities of different salient interactions $S$ into the following five types, which reflect distinctive behaviors of a DNN learning different types of primitive inference patterns. Please see Figure 6 for the curve of $I _ { \mathrm { a n d / o r } } ( S | x )$ across different epochs for each salient interaction $S$ . (a) Figure 6 (a) shows interactions belonging to the first group. The strength of these interactions increases throughout the learning process in a relatively stable manner. Thus, we consider such interactions to be stably learned by the DNN, and we call them reliable interactions. $( b )$ In the second group, utilities of interactions in Figure 6 (b) are usually close to zero in the beginning. Then, the strength of their utility first increases and then decreases, sometimes decreasing to almost zero. These interactions are referred to as withdrawn interactions.

![](images/6b2f072f24698d976d653acd9cc5e9c981a9028cc954c050db01def19af2a050.jpg)  
Figure 6: Curves of the utility of interactions during the learning of DNNs. These interactions can be categorized into five groups. Please refer to Appendix I for results on more samples.

Table 2: Categorization of five groups of interactions.   

<html><body><table><tr><td>Group</td><td>Phenomenon</td></tr><tr><td>reliable</td><td>∀t ∈T,I(S|x,0t)·VtI(S|x,0t)≥0</td></tr><tr><td>withdrawn</td><td>Etmid s.twhent<tmid,I(S|x,0t)·VtI(S|x,0t)≥0 whent>tmid,I(S|x,0t)·VtI(S|x,0t)≤0</td></tr><tr><td>forgotten</td><td>∀t ∈T,I(S|x,0t)·VtI(S|x,0t)≤0</td></tr><tr><td>betraying</td><td>Etmids.t.Vt1<tmid,∀t2>tmid,I(S|x,0t1)·I(S|x,0t2)≤0</td></tr><tr><td>fluctuating</td><td>I(Slx,0t) andVtI(S|x,0t)oscillate around zero</td></tr></table></body></html>

(c) As Figure 6 (c) shows, the initial utility of interactions in the third group is non-ignorable. However, the strength of these interactions keeps decreasing to zero. These interactions are gradually forgotten by the DNN. We call them forgotten interactions. (d) Figure 6 (d) shows interactions in the fourth group. The interactions experience a gradual shift towards an interaction utility that is opposite to their initial utility. These interactions are called betraying interactions. (e) For interactions in the fifth group, Figure 6 (e) has fluctuating interactive utilities throughout the learning process, thereby being called fluctuating interactions.

Different types of interactions reflect primitive inference patterns of different learning efficiency. (1) We can consider reliable interactions and forgotten interactions as stably and efficiently learned knowledge. (2) Some Betraying interactions and withdrawn interactions reflect the trial-and-error process during learning, while some are caused by a bad initialization of weights. (3) Fluctuating interactions correspond to the noise knowledge.

In particular, the goal of (Shwartz-Ziv and Tishby 2017) is quite similar to ours,i.e., understanding the learning and forgetting of information throughout the training of a DNN. Shwartz-Ziv and Tishby (2017) discovers that the DNN usually first extracts information and then compresses information. In our study, we discover the existence of withdrawn interactions, which precisely explains what information is first learned and subsequently forgotten.

In addition, we have conducted an experiment on a toy dataset to demonstrate that interactions can successfully reveal betraying features learned during training of the DNN. Please refer to Appendix I for the experimental results.

The number and complexity order of interactions in each group help to understand the performance of DNNs. For each DNN and each sample, we selected 100 interactions whose maximum interaction strength $\operatorname { \mathrm { ( m a x } } _ { t } | I _ { \mathrm { a n d } } ( S | x , \theta _ { t } ) |$ and $\operatorname* { m a x } _ { t } | I _ { \mathrm { o r } } ( S | x , \theta _ { t } ) | )$ throughout the training process were ranked in top 100 among all interactions. Then, we counted the number of interactions belonging to each group among these 100 salient interactions. Table 3 reports the average number of interactions in each group over different samples. We found that compared to VGG-11, RN-20 learned more reliable and forgotten interactions, while having fewer betraying and fluctuating interactions. This might be because the residual connections in RN20 made the features more stable. Besides, we also noticed that the DNNs trained on the MNIST dataset usually encoded more reliable interactions and less betraying and fluctuating interactions than the DNNs trained on the CIFAR-10 and Tiny ImageNet datasets. This result indicated that the dynamics of interactions also provided a new perspective to analyze the difficulty of training a DNN on a dataset.

We further studied the complexity (order) of interactions. Let the order of an interaction $S$ be referred to as the number of variables in $S$ , order $\mathbf { \partial } \cdot ( S ) = | S |$ . Zhou et al. (2024) have found that compared to high-order interactions, low-order interactions extracted from training samples are more likely to generalize to (appear in) testing samples. Since the network output is the sum of all interactions, we can use the ratio of low-order interactions and high-order interactions to explain the generalization power of the DNN. Thus, We explored the order of interactions in each group. Figure 7 reports the average number of interactions of each order over different samples in each group. We found that the distribution of interactions over different orders was similar in

RN-20 on CIFAR-10 VGG on CIFAR-10 RN-20 on MNIST VGG on MNIST CNN on SST-2 厂 150 150 100 100 RWeiltihabdlrea iwnteirnatcetriaocntion 100 50 50 fboertgroatytienng interaction L 50 = □ 0 1 2 4     6     8     10 01 2 4     6     8     10 □ □ 0 2 4     6     8     10 2 4     6     8     10 2 4     6     8     10 fluctuating interaction order order order order order

<html><body><table><tr><td>Model</td><td>Dataset</td><td>reliable interactions</td><td>withdrawn interactions</td><td>forgotten interactions</td><td>betraying interactions</td><td>fluctuating interactions</td></tr><tr><td>VGG-11</td><td>CIFAR-10</td><td>28.4</td><td>26.4</td><td>6.0</td><td>26.6</td><td>12.6</td></tr><tr><td>RN-20</td><td>CIFAR-10</td><td>33.4</td><td>26.2</td><td>16.6</td><td>17.8</td><td>6.0</td></tr><tr><td>VGG-11</td><td>MNIST</td><td>44.2</td><td>21.4</td><td>5.8</td><td>20.8</td><td>7.8</td></tr><tr><td>RN-20</td><td>MNIST</td><td>49.6</td><td>18.2</td><td>18.0</td><td>12.6</td><td>1.6</td></tr><tr><td>RN-18</td><td>Tiny ImageNet</td><td>4.0</td><td>39.0</td><td>20.4</td><td>20.4</td><td>16.2</td></tr><tr><td>CNN</td><td>SST-2</td><td>33.6</td><td>14.4</td><td>0.4</td><td>12.8</td><td>38.8</td></tr></table></body></html>

Input Salient interactions Input Salient interactions Input Salient interactions 1st 2nd 3rd 4th 1st 2nd 3rd 4th 1st 2nd 3rd 4th Odad 0: (a) RN-20 on CIFAR-10 (b) VGG-11 on MNIST (c) PointNet on ShapeNet

different models. Besides, we found that high-order interactions were usually fluctuating and withdrawn interactions, becauseRheisgneht- soirgdner interactions usually represented complex and unstnaobrlmeefdeatures.

In addition, Appendix J showed that DNNs tended to use high-order interactions to classify abnormal samples (e.g., samples with noisy labels) than normal samples.

# What Does an Interaction Represent?

As a supplement to the mathematical explanation of the learning dynamics, we also visualize the primitive interactions in this subsection, although interactions toward mathematically concise explanation are not equivalent to semantically meaningful concepts.

We first visualize the attribution map of each top-ranked feature component $f _ { i }$ . Considering the distinctive properties of different tasks, we apply the projected influence attribution, the gradient-based attribution (Simonyan and Zisserman 2014), and the Shapley value (Shapley 1953) to estimate the attribution of input data (image data, the 3D point cloud data, and the language data) to each feature component $f _ { i }$ , respectively. Figure ?? shows examples of attribution maps of feature components, where the red color indicates that the corresponding regions in the input have a positive attribution to the principal feature component, while the blue color indicates a negative attribution. For the point cloud data, we use RGB color channels to visualize the three-dimensional attributions. Please see Appendix $\mathrm { ~ H ~ }$ for details of the visualization techniques and results.

Then, for each interaction $S \subseteq N _ { \mathrm { f e a t u r e } }$ with a considerable utility $I _ { \mathrm { a n d / o r } } ( S | x )$ , Figure 8 visualizes the attribution map of the interaction, which simply sums up the attribution maps of its compositional feature components $\{ f _ { i } \} _ { i \in S }$ .

# 4 Conclusion and Discussions

In this study, we have proposed a method to simplify and summarize a DNN’s highly complex learning dynamics into the change of a few interaction primitives. We have extended the interaction defined on raw input variables by (Ren et al. 2023a; Li and Zhang 2023; Zhou et al. 2023), and have newly defined interactions on principal feature components. This extension greatly boosts the sparsity/simplicity of the interaction-based explanation of a DNN, which provides a new perspective to understand mechanical factors for learning efficiency. The mathematical faithfulness of the new interaction is experimentally verified. We have found that the dynamics of all salient interactions naturally belong to five groups, i.e., reliable, withdrawn, forgotten, betraying, and fluctuating interactions, which provide new insights, e.g., explaining how reliable inference patterns are gradually learned, how redundant patterns are first learned and later discarded, and how a DNN learns noisy patterns.

# Acknowledgments

This work is partially supported by the National Science and Technology Major Project (2021ZD0111602), the National Nature Science Foundation of China (92370115, 62276165). Jie Ren is supported by Wu Wen Jun Honorary Doctoral Scholarship, AI Institute, Shanghai Jiao Tong University.