# HeterGP: Bridging Heterogeneity in Graph Neural Networks with Multi-View Prompting

Fengyu $\mathbf { Y a n } ^ { 1 , 2 \dag }$ , Xiaobao $\mathbf { W a n g } ^ { 1 , 2 \dagger }$ , Dongxiao $\mathbf { H } \mathbf { e } ^ { 1 }$ , Longbiao Wang1, Jianwu Dang3, Di Jin1\*

1Tianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and Computing, Tianjin University, Tianjin, China 2Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China 3Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China   
{fengyuyan, wangxiaobao, hedongxiao, longbiao wang} $@$ tju.edu.cn, jdang $@$ jaist.ac.jp, jindi@tju.edu.cn

# Abstract

The challenges tied to unstructured graph data are manifold, primarily falling into node, edge, and graph-level problem categories. Graph Neural Networks (GNNs) serve as effective tools to tackle these issues. However, individual tasks often demand distinct model architectures, and training these models typically requires abundant labeled data, a luxury often unavailable in practical settings. Recently, various “prompt tuning” methodologies have emerged to empower GNNs to adapt to multi-task learning with limited labels. The crux of these methods lies in bridging the gap between pre-training tasks and downstream objectives. Nonetheless, a prevalent oversight in existing studies is the homophily-centric nature of prompt tuning frameworks, disregarding scenarios characterized by high heterogeneity. To remedy this oversight, we introduce a novel prompting strategy named HeterGP tailored for highly heterophilic scenarios. Specifically, we present a dual-view approach to capture both homophilic and heterophilic information, along with a prompt graph design that encompasses token initialization and insertion patterns. Through extensive experiments conducted in a few-shot context encompassing node and graph classification tasks, our method showcases superior performance in highly heterophilic environments compared to state-of-the-art prompt tuning techniques.

# Introduction

Graph Neural Networks (GNNs) find broad applications across diverse domains. For example, anomaly detection leverages node classification (Ding et al. 2019; Wang et al. 2023), recommendation systems benefit from link prediction (He et al. 2020; Jin et al. 2021), and the domains of molecules (Kim et al. 2022) and drug discovery (Liu et al. 2023a) witness promising applications of graph classification and generation. Recent studies have predominantly concentrated on specific tasks, primarily relying on supervised graph training methodologies. However, numerous real-world scenarios suffer from a scarcity of annotated data (Ding et al. 2024), posing significant challenges to the deployment of supervised graph training techniques.

To address these issues, recent work has drawn inspiration from pre-training models in NLP, which have demonstrated excellent performance on downstream tasks (Qiu et al. 2020; Devlin et al. 2019; Radford et al. 2018). Efforts have been made to design pre-training methods for graphs. However, unlike the alignment of upstream and downstream objectives in NLP, there’s often a significant gap between pretraining and downstream tasks in graph-based applications (Sun et al. 2023). For instance, using masked edges as a pre-training objective can result in negative transfer when applied to graph-level classification tasks.

To address the inconsistency between upstream and downstream task objectives and to improve the performance of pre-trained models on different downstream tasks, several studies have explored graph prompt learning methods, inspired by prompt-tuning in NLP (Brown et al. 2020). These methods include the All-in-One approach (Sun et al. 2023), which unifies upstream and downstream tasks by extracting subgraphs and adding multiple learnable nodes to bridge the gap between pre-training models and various downstream tasks. GPPT (Sun et al. 2022) and GraphPrompt (Liu et al. 2023c) construct pre-training tasks centered on link prediction, unifying various downstream tasks into link prediction tasks. They employ category representation vectors and subgraph readout to handle the diversity of downstream tasks.

While the aforementioned methods have shown promising results, they heavily rely on the assumption of graph homophily (most neighboring nodes belong to the same categories and possess similar features). There are mainly two aspects that reflect this assumption, the pre-training process and prompt tuning. As shown in Figure 1(a), most pre-training tasks assume that the similarity between adjacent nodes is greater than that between non-adjacent nodes. Similarly, in the process of prompt tuning, only neighboring nodes are used to obtain subgraph information. In addition, more similar nodes are connected during prompt graph initializing and inserting pattern procedure in the existing methods, shown in Figure 1(b). However, many real-world graph networks deviate from the homophily assumption (Yu et al. 2024), displaying low homophily or even heterophily (most neighboring nodes belong to different categories and exhibit dissimilar features). Such scenarios do not align with the characteristics of graphs with high heterogeneity, leading to limited effectiveness in prompt tuning. Although there is existing research on pre-training for heterophily (Liu et al. 2023b), it difficult to align the objectives of the downstream prompt tuning step.

Addressing the challenges posed by strong heterophily in graphs during the former kinds of prompt-tuning approaches involves several difficulties. Firstly, under traditional message-passing mechanisms, simply using $K$ -hop neighboring node information as in All-in-One (Sun et al. 2023) is insufficient to effectively represent subgraph features in low homophily scenarios. Balancing homophily and heterophily for subsequent prompt tuning calculations is a key challenge. Secondly, after obtaining sampled subgraph information, designing a graph prompt that effectively captures features in both homophilic and heterophilic scenarios using prompt-tuning methods is a significant challenge.

To address these obstacles, we propose the Heterophilic Graph Prompt (HeterGP), which amalgamates heterogeneity in GNNs through multi-view prompting. Recent studies have showcased notable efficacy in utilizing random walks for acquiring homophilic and heterophilic information (Jin et al. 2022). Our strategy confronts the initial challenge by adopting this efficient method to gather insights on both homophily and heterophily. To tackle the subsequent challenge, where a singular prompt proves insufficient for managing both information types, we draw inspiration from NLP practices (Wei et al. 2024) that embrace multi-prompt collaborative frameworks. We introduce two distinct prompt types to individually process homophilic and heterophilic information, thereby transcending the restriction of exclusively considering homophilic information of a single type. Subsequently, these two streams of information are merged, and an attention mechanism is harnessed to discern the significance of harmonizing the two information types. Our approach undergoes rigorous testing across diverse datasets and varied downstream tasks, showcasing robust performance in scenarios involving both homophily and heterophily, as well as in few-shot settings. Our contributions can be outlined as follows:

• We are the first to propose a unique information collection method designed to unify graph tasks that exhibit diverse structural homogeneity and heterogeneity.   
• We propose an innovative graph prompt-tuning strategy that unifies the format of graph prompts in both homophilic and heterophilic scenarios through a multi-view design.   
• We conduct extensive comparisons of HeterGP against state-of-the-art methods across a range of homophilic and heterophilic datasets in multiple tasks, demonstrating the effectiveness of HeterGP.

# Related Work

Graph Pre-training In NLP, pre-trained models have broad utility across tasks (Wang et al. 2022b). In graph scenarios with limited labeled data, the need for generalization emphasizes the significance of graph pre-trained models. Pre-training relies on self-supervised methods utilizing existing graph data. For edge-level pre-training, techniques like link prediction (Long et al. 2022; Kipf and Welling 2016) are foundational. LaGraph (Xie, Xu, and Ji 2022) reconstructs masked edges to enhance robust representations. Contrastive learning at the node level is a key strategy, with GRACE (Zhu et al. 2020), GCA (Zhu et al. 2021), and HomoGCL (Li et al. 2023) refining this approach. Approaches such as BGRL (Thakoor et al. 2021) treat nodes in augmented graphs as positive samples, training the online encoder to predict the target encoder for potent node representations. Conversely, DGI (Velickovic et al. 2019) emphasizes maximizing mutual information between node and global representations, using augmented graphs as negative samples to enhance node comprehension.

![](images/8d63e3f83e389d68f6c2d5ae8551058e321dfb07e723bcf77597d36a4d60fb39.jpg)  
Figure 1: The existing methods include homophilic assumption. (a) Higher similarity is assumed among adjacent nodes during the pre-training process, using $K$ -hop subgraphs to unify tasks and represent node features, and (b) the insertion patterns of the prompt graph also assume that highly similar nodes are connected.

Graph Prompt-tuning Derived from NLP, prompt tuning methods adeptly connect pre-training tasks to targeted downstream goals. Unlike NLP contexts, merging tasks and domains poses unique challenges in the domain of graph prompt frameworks. Among existing methods, GPPT (Sun et al. 2022) converts link prediction pre-training into a node classification task, employing task tokens and structure tokens within the prompt. Similarly, GPF (Fang et al. 2023) augments original nodes with additional learnable features. In GraphPrompt (Liu et al. 2023c), a pre-training task emphasizing increased similarity between neighboring nodes is utilized, then adapted into node and graph classification tasks through a trainable weight matrix in the prompt, adjusting representations post-readout. The All-in-One (Sun et al. 2023) approach integrates prompts comprising nodes as vectors of new learnable features introduced within subgraphs, linked to original graph nodes with high similarity. Conversely, the One-for-All (Liu et al. 2024) incorporates prompts consisting of newly added nodes, their initial representations derived by feeding natural language descriptions into LLM. While the aforementioned methodologies tackle the multi-task graph learning challenge using prompts, they often neglect the diversity inherent in graph structures.

# Problem Statement

Given a graph defined as $\mathcal { G } ~ = ~ ( \mathcal { V } , \mathcal { E } ) ~ \in ~ \mathbb { G } .$ , where $\begin{array} { r c l } { \mathcal { V } } & { = } & { \{ v _ { 1 } , v _ { 2 } , . . . , v _ { | \mathcal { V } | } \} } \end{array}$ is the set of nodes and $\begin{array} { r l } { \mathcal { E } } & { { } = } \end{array}$ $\{ e _ { 1 } , e _ { 2 } , . . . , e _ { | \nu | } \}$ is the set of edges. The features of the nodes are defined as $\pmb { X } \in \mathbb { R } ^ { | \mathcal { V } | \times d }$ , and $\pmb { x } _ { i } \in \mathbb { R } ^ { d }$ represents the feature of the node $\boldsymbol { v } _ { i }$ . The adjacent matrix is denoted as $\pmb { A } \in \{ 0 , 1 \} ^ { | \nu | \times | \mathcal { V } | }$ , where $\boldsymbol { A } _ { i j } = 1$ if $( v _ { i } , v _ { j } ) \in \mathcal { E }$ .

The graph prompt tuning task involves adapting frozen pre-trained GNN models to downstream tasks. The downstream tasks we consider encompass node classification (NC) and graph classification (GC). Initially, we focus on the NC task, highlighting specific handling approaches for different tasks if necessary. The entire process involves a pre-trained GNN encoder $f _ { G N N }$ with frozen parameters, a learnable classifier $f _ { c l s }$ for downstream tasks and a taskspecified graph prompt process $f _ { p r o m p t } : \mathbb { G }  \mathbb { G }$ . During the prompt tuning process, $f _ { p r o m p t } ( \mathcal { G } )$ will replace $\mathcal { G }$ as input to $f _ { G N N }$ , our objective is to learn the parameter of $f _ { c l s }$ and $f _ { p r o m p t }$ by maximizing the likelihood of predicting the labels for few-shot data $Y$ , can be formulated as:

$$
\operatorname* { m a x } _ { \theta _ { c l s } , \theta _ { p r o m p t } } P _ { \theta _ { G N N } , \theta _ { c l s } } ( Y | f _ { p r o m p t } ( \mathcal { G } ) ) ,
$$

where $\theta _ { c l s } , \theta _ { p r o m p t } , \theta _ { G N N }$ represent the parameters for the modules of $f _ { c l s }$ , $f _ { p r o m p t }$ , fGNN respectively.

# Proposed Model: HeterGP

In this section, we delineate the implementation of our proposed HeterGP for both homophily and heterophily scenarios, commencing with a framework overview and in-depth elucidations of each module.

# Overview Framework

Our objective is to create a graph prompt-tuning framework capable of accommodating both homophily and heterophily across various network topologies. The comprehensive framework diagram is depicted in Figure 2. Inspired from All-in-One (Sun et al. 2023), we leverage node subgraph representations to capture embedded graph information and unify tasks within the graph through subgraph structures. To ensure that subgraphs encapsulate homophilic and heterophilic insights, we initiate a multi-view subgraph sampling strategy to capture dual perspectives of information concerning the target nodes (GC tasks bypass this step). Following the acquisition of homophilic and heterophilic subgraphs for the target node, distinct graph prompt construction and insertion patterns are devised for each subgraph type. Subsequently, the multi-view graphs, enhanced with prompts, are inputted into the pre-trained GNN to extract representations for both perspectives. The amalgamated representation effectively encapsulates homophilic and heterophilic information pertinent to the task at hand. Ultimately, the results of the downstream task are generated through an attention-based classification layer.

# Path Extractor & Subgraph Reformer

The tasks handled by graph models are varied. In a multitask setup, the key step involves unifying these tasks. Current methods predominantly tackle this challenge by reconfiguring downstream tasks at the graph level. Conventional subgraph generation typically relies solely on aggregating the $K$ -hop neighbors of pertinent nodes. The aggregation technique commonly combines information from nodes across various classes in graphs characterized by high heterophily, resulting in subpar performance when applied to real-world prediction tasks. To tackle the challenges posed by the topological structure variations of homophily and heterophily, the primary focus should be on efficiently gathering neighborhood information that is pertinent to the downstream task at hand. Drawing inspiration from RAWGNN (Jin et al. 2022), Breadth-First Search (BFS) and Depth-First Search (DFS) are pivotal in acquiring representations of homophily and heterophily, respectively. Consequently, we employ a random walk technique incorporating two parameters, $p$ and $q$ , derived from Node2Vec (Grover and Leskovec 2016), to emulate DFS and BFS behaviors.

![](images/2966832c86ea95eed4868a8fc565696bd0e8eea9135fa440436d771bd02442f7.jpg)  
Figure 2: The framework of the proposed HeterGP. Initially, the Path Extractor is utilized to gather multiple paths and create subgraphs representing homophilic and heterophilic viewpoints (with GC tasks exempt from this process). Subsequently, the Prompt Graph Generator is employed to formulate prompts and establish a fresh computational graph. Dual-perspective representations are derived through the GNN encoder, with the downstream task loss utilized during the training phase.

Assuming the transition from node $v _ { k }$ to $v _ { j }$ has occurred, the current node at $v _ { j }$ is contemplating the next step to take. Rather than choosing the subsequent neighbor from $v _ { j }$ randomly with equal likelihood, the initial weights are adjusted using parameters $p$ and $q$ . These weights are then normalized to establish the probabilities associated with choosing various nodes, which can be expressed as:

$$
P _ { p q } \left( v _ { i } \mid v _ { j } , v _ { k } \right) = { \left\{ \begin{array} { l l } { 1 / p { \mathrm { i f } } d _ { i k } = 0 } \\ { 1 { \mathrm { i f } } d _ { i k } = 1 } \\ { 1 / q { \mathrm { i f } } d _ { i k } = 2 } \\ { ~ 0 { \mathrm { o t h e r w i s e } } } \end{array} \right. } ,
$$

where $d _ { i k } = 0$ indicates that $v _ { j }$ returns to the starting point $v _ { k }$ , $d _ { i k } = 1$ indicates that the next neighbor from the new starting point $v _ { j }$ has an edge relationship with the previous starting point $v _ { k }$ . In this case, the distance is 1, and $P _ { p q } = 1$ . $d _ { i k } = 2$ indicates that the next neighbor from the new starting point $v _ { j }$ has no relationship with the previous starting point $v _ { k }$ . When $p < 1$ and $q > 1$ , the procedure turns to be BFS, while $p > 1$ and $q < 1$ results in a DFS-like behavior.

Typically, the BFS strategy is akin to the traditional method of extracting subgraphs from $K$ -hop neighbors, which captures homophily information. In contrast, the DFS strategy tends to explore further from the source node, enabling the extraction of heterophily information. Both types of information are crucial, especially in heterophilous scenarios, where the DFS strategy can aggregate more information about nodes belonging to the same class. Therefore, we selected both strategies. We perform $n$ random walks of length $l$ starting from node $v$ to obtain multiple BFS and DFS paths, respectively. This process can be expressed as:

$$
\mathcal { P } _ { l } ^ { j } = \left( v _ { 0 } ^ { j } , v _ { 1 } ^ { j } , \ldots , v _ { l } ^ { j } \right) , \quad \forall j \in \{ 1 , 2 , \ldots , n \} ,
$$

where $\mathcal { P } _ { l } ^ { j }$ denotes collection of nodes from the $j$ -th random walk of length $l$ . The paths are aggregated separately to generate subgraphs under two different views which can be formulated as:

$$
\begin{array} { r l r } & { } & { \mathcal { V } ^ { \prime } = \displaystyle \bigcup _ { j = 1 } ^ { n } \left\{ v _ { i } ^ { j } \mid i \in \mathcal { W } _ { l } ^ { j } \right\} , \mathcal { E } ^ { \prime } = \left\{ \left( u , v \right) \in \mathcal { E } \mid u , v \in \mathcal { V } ^ { \prime } \right\} } \\ & { } & { \mathcal { G } _ { B F S } = \left( \mathcal { V } _ { B F S } , \mathcal { E } _ { B F S } \right) , \mathcal { G } _ { D F S } = \left( \mathcal { V } _ { D F S } , \mathcal { E } _ { D F S } \right) , } \end{array}
$$

where $\nu ^ { \prime } , \varepsilon ^ { \prime }$ be the set of unique nodes and edges visited in all $n$ walks. At this point, the subgraph containing multiview information is obtained for subsequent prompt learning steps. The aforementioned steps are applied to the NC problem. In the case of GC tasks, where our area of interest is the entire graph, we use the whole graph directly as the sampling result for the subsequent prompt learning steps.

# Graph Prompt Generator

This module encompasses the architecture design of the internal connections within the prompt subgraph and the connections between the prompt subgraph and the main subgraph. Initially, we focus on the design of graph prompts, inspired by the concept of “prompt as tokens” (Sun et al. 2023, 2022). Prompt tokens can be perceived as freshly introduced nodes, with their quantity defined as a hyperparameter, and their dimensions align with those of the original node features, denotes $X _ { p r o m p t } , X _ { n o d e } \in \mathbb { R } ^ { d }$ .

Intra-connection After initializing the prompt tokens, the subsequent step entails crafting the internal structure within these tokens. To accommodate diverse homophilic and heterophilic scenarios, two distinct internal connection patterns are formulated for the prompt graph. In the context of homophily, we calculate the similarity $S _ { i j }$ between prompt tokens $i$ and $j$ , forming connections between token nodes where the similarity surpasses a predefined threshold. Conversely, in scenarios of heterophily, we assess similarities between tokens, linking token nodes where the similarity

Algorithm 1: Proposed model HeterPG

Input: Graph $\mathcal { G }$ , pre-trained GNN model $f _ { G N N }$ , $k$ -shot   
based labeled dataset $\mathcal { D } = \{ ( v _ { i } , y _ { i } ) , i = 1 , 2 , . . . , k \}$   
Output: The optimal task-specified model $f _ { p r o m p t }$ , parame  
terized $\theta _ { p r o m p t }$ and classifier $f _ { c l s }$   
1: Initialize $\theta _ { c l s }$ , $\theta _ { p r o m p t }$ and downstream task loss $\mathcal { L } _ { d o w n }$   
2: while not converged do   
3: Sample subgraphs ${ \cal { S } } _ { v _ { i } } , v _ { i } \in { \cal { D } }$ , based on Eq.1,2,3   
4: for each $S _ { v _ { i } } \in S$ do   
5: $f _ { p r o m p t }$ inner structure update based on Eq.4   
6: $\mathcal { G } _ { h o m o }$ $, \mathcal { G } _ { h e t e r } \gets f _ { p r o m p t } ( S _ { v _ { i } } )$ , based on Eq.5   
7: $\mathcal { L } \gets \mathcal { L } _ { d o w n } ( f _ { c l s } ( \dot { \mathcal { G } } _ { h o m o } , \mathcal { G } _ { h e t e r } ) , y _ { i } )$   
8: end for   
9: Update $\theta _ { c l s }$ and $\theta _ { p r o m p t }$ by minimize $\mathcal { L }$   
10: end while   
11: return $\theta _ { p r o m p t }$ and $\theta _ { c l s }$

falls below another specified threshold:

$$
\begin{array} { r l } & { A _ { i j } ^ { h o m o \_ p g } = \left\{ \begin{array} { c } { S _ { i j } \mathrm { ~ i f ~ } S _ { i j } > \mathcal { T } _ { i n t r a \_ h o m o } } \\ { 0 \mathrm { ~ o t h e r w i s e } } \end{array} \right. , } \\ & { A _ { i j } ^ { h e t e r \_ p g } = \left\{ \begin{array} { c } { S _ { i j } \mathrm { ~ i f ~ } S _ { i j } < \mathcal { T } _ { i n t r a \_ h e t e r } } \\ { 0 \mathrm { ~ o t h e r w i s e } } \end{array} \right. } \end{array}
$$

where $A ^ { h o m o \_ p g } , A ^ { h e t e r \_ p g }$ capture the adjacency matrix of the prompt graph for homophily and heterophily. These unique internal construction approaches guarantee that the distributions of homophily and heterophily correspond to the respective subgraphs from two distinct perspectives.

Inter-connection We have generated two prompt graphs from different perspectives. To ensure that the addition of prompts aligns with the distribution of structure and the purpose of the previously generated subgraphs, we use the following method: For the subgraph generated by BFS, we calculate the similarity $S _ { i j }$ between each node in the prompt graph and the nodes in the BFS-generated subgraph. We set a threshold $\mathcal { T } _ { i n t e r \ : . h o m o }$ and connect nodes with similarity greater than the threshold. Conversely, we calculate the similarity $S _ { i j }$ with the nodes in the DFS-generated subgraph and connect nodes with a similarity less than the threshold. The process can be expressed as:

$$
\begin{array} { r } { A _ { i j } ^ { h o m o } = \left\{ \begin{array} { c } { S _ { i j } \mathrm { ~ i f ~ } S _ { i j } > \mathcal { T } _ { i n t e r . h o m o } } \\ { 0 \mathrm { ~ o t h e r w i s e } } \end{array} \right. , } \\ { A _ { i j } ^ { h e t e r } = \left\{ \begin{array} { c } { S _ { i j } \mathrm { ~ i f ~ } S _ { i j } < \mathcal { T } _ { i n t e r . h e t e r } } \\ { 0 \mathrm { ~ o t h e r w i s e } } \end{array} \right. } \end{array}
$$

where $\mathcal { G } _ { h o m o }$ and $\mathcal { G } _ { h e t e r }$ symbolize the graph resulting from the amalgamation of prompt graphs and subgraphs, with $A ^ { h o m o }$ and $A ^ { h e t e r }$ representing their corresponding adjacency matrices. Consequently, we finalize the establishment of the prompt graph linked to the integrated task subgraph, culminating in a graph enriched with prompts.

# Attention-based Classifier

The pretraining techniques are outlined in the experiments section, where the frozen pretraining model encodes subgraphs that represent homophilic and heterophilic information. To amalgamate these representations and ascertain their significance for downstream tasks, a straightforward MLP layer is employed:

Table 1: Summary of NC datasets   

<html><body><table><tr><td>Dataset</td><td>#N</td><td>#E</td><td>#F</td><td>#C</td><td>L.H.R</td></tr><tr><td>Cora</td><td>2,708</td><td>10,556</td><td>1,433</td><td>7</td><td>0.81</td></tr><tr><td>CiteSeer</td><td>3,327</td><td>9,104</td><td>3,703</td><td>6</td><td>0.74</td></tr><tr><td>PubMed</td><td>19,717</td><td>88.648</td><td>500</td><td>3</td><td>0.80</td></tr><tr><td>Texas</td><td>183</td><td>325</td><td>1,703</td><td>5</td><td>0.09</td></tr><tr><td>Wisconsin</td><td>251</td><td>515</td><td>1,703</td><td>5</td><td>0.19</td></tr><tr><td>Cornell</td><td>183</td><td>298</td><td>1,703</td><td>5</td><td>0.13</td></tr><tr><td>Actor</td><td>7,600</td><td>30,019</td><td>932</td><td>5</td><td>0.22</td></tr><tr><td>Chameleon</td><td>2,277</td><td>36,051</td><td>2,325</td><td>5</td><td>0.23</td></tr><tr><td>Squirrel</td><td>5,201</td><td>216,933</td><td>2.089</td><td>5</td><td>0.22</td></tr></table></body></html>

$$
\mathcal { H } _ { \mathrm { f i n a l } } = \sigma \left( \mathcal { W } ( \mathcal { H } _ { h o m o } \Vert \mathcal { H } _ { h e t e r } ) + \mathbf { b } \right) ,
$$

where $\mathcal { H } _ { h o m o }$ and $\mathcal { H } _ { h e t e r }$ denote the representations of $\mathcal { G } _ { h o m o }$ and $\mathcal { G } _ { h e t e r }$ . The MLP layer discerns the significance of homophily and heterophily information within the given data domain, generating a conclusive representation applicable for subsequent tasks. The comprehensive training process is detailed in Algorithm 1.

# Experiments

In our experiments, we compare our model with traditional GNN methods and state-of-the-art graph prompt models on node classification (NC) and graph classification (GC) tasks. We utilize real-world datasets that predominantly encompass both homophily and heterophily scenarios.

# Experimental Setup

Datasets We test downstream tasks using the following datasets, which can be found in Table 1 & 2. For NC, the datasets with strong homophily: Cora, CiteSeer, and PubMed (Yang, Cohen, and Salakhutdinov 2016) are citation network datasets. Datasets with strong heterophily: Cornell, Texas, and Wisconsin (Pei et al. 2020) are web datasets collected from the computer science departments of the respective universities. The Actor dataset (Tang et al. 2009) is a actor co-occurrence network. The Chameleon and Squirrel datasets (Pei et al. 2020) are web page datasets.

For the GC tasks, we utilize diverse datasets. ENZYMES (Wang et al. 2022a) comprises protein structures classified into six enzyme classes. PROTEINS (Borgwardt et al. 2005) represents secondary structure elements for protein classification. MUTAG (Morris et al. 2020) focuses on classifying chemical compounds as mutagenic or non-mutagenic. COX2 (Rossi and Ahmed 2015) involves compounds classified by their activity against the COX-2 enzyme, and BZR (Rossi and Ahmed 2015) for bioactivity against the benzodiazepine receptor. Additionally, we calculate the $L . H . R$ values for each dataset, representing the label-defined edge homophily ratio of the network.

Table 2: Summary of GC datasets   

<html><body><table><tr><td>Dataset</td><td>#G</td><td>Avg.#N</td><td>Avg.#E</td><td>#F</td><td>#C</td><td>L.H.R</td></tr><tr><td>ENZYMES</td><td>600</td><td>32.6</td><td>124.3</td><td>3</td><td>6</td><td>0.67</td></tr><tr><td>PROTEINS</td><td>1,113</td><td>39.1</td><td>145.6</td><td>3</td><td>2</td><td>0.66</td></tr><tr><td>MUTAG</td><td>188</td><td>17.9</td><td>39.6</td><td>7</td><td>2</td><td>0.72</td></tr><tr><td>COX2</td><td>467</td><td>41.22</td><td>43.45</td><td>3</td><td>2</td><td>0.41</td></tr><tr><td>BZR</td><td>405</td><td>35.75</td><td>38.36</td><td>3</td><td>2</td><td>0.41</td></tr></table></body></html>

Baselines We compare three types of methods: 1) Endto-end graph neural networks such as GCN (Kipf and Welling 2017), GAT (Velicˇkovic´ et al. 2018) and GraphTransformer(GT) (Shi et al. 2021). The first two methods primarily focus on aggregating messages from neighboring nodes. In contrast, GT considers global information, demonstrating superior performance on heterophily graphs (Mu¨ller et al. 2023). 2) Pre-trained models+fine-tuning, where unsupervised learning is performed using contrastive learning methods, followed by fine-tuning in a few-shot setting. For contrastive learning, we use methods like GraphCL (You et al. 2020) and SimGRACE (Xia et al. 2022). 3) Pretrained models+prompt-tuning. This type of method consists of two stages: graph pre-training and then prompttuning using graph prompt methods in a few-shot setting. The prompt-tuning methods for comparison include All-inOne (Sun et al. 2023) and GraphPrompt (Liu et al. 2023c). These methods incorporate homophily assumptions either in the pre-training methods or in the prompt design.

Parameter Settings To assess the model’s capacity in a few-shot context, the dataset is randomly divided into training and testing sets following the $n$ -way $k$ -shot setup. In the experiments, both 100-shot and 5-shot scenarios are used for NC, while 20-shot and 5-shot scenarios are employed for GC. To ensure fair comparisons, the training set is randomly sampled 10 times for training. The hyperparameters in the comparative models are set according to the original paper. In our model, a random walk step size of 10 is chosen, with 6 paths collected during random walks. BFS and DFS are utilized in the random walk strategy with probabilities $p = 0 . 1$ and $q = 1 0$ , respectively. For prompt calculation, 10 prompt tokens are used, with internal similarity thresholds set to 0.7 in the homophilic perspective and 0.4 in the heterophilic perspective. In the pre-training phase, consistency is maintained by keeping the structure of the GNN encoder the same across pre-trained and prompt methods in baseline models. GT is selected as the backbone for our encoder. The learning rate for training classifiers and prompt tokens is set to 0.001, using the Adam optimizer from PyTorch. All experiments are conducted on Nvidia RTX 3090 graphics cards.

# Performance Evaluation

We conduct experiments on 14 real-world datasets, including scenarios of NC and GC, to compare the learning abilities of various methods under few-shot settings. We use mean accuracy and standard deviation as evaluation metrics. Overall, HeterPG achieves the best performance on 19 out of 25 different settings. When comparing prompt categories, it excels in 21 cases. Here are some specific findings:

Table 3: Performance on node classification with best bolded, runner-up underlined and third place dash lined.   

<html><body><table><tr><td>Method</td><td>GCN</td><td>GAT</td><td>GT</td><td>GraphCL+FT</td><td>simGRACE+FT</td><td>GraphPrompt</td><td>All in One</td><td>Ours</td></tr><tr><td colspan="9">100-shot</td></tr><tr><td>Cora</td><td>77.46 ± 0.66</td><td>79.79 ± 0.71</td><td>79.73 ± 0.50</td><td>70.10 ± 2.47</td><td>69.12 ± 4.68</td><td>70.46 ± 2.21</td><td>71.52 ± 1.56</td><td>77.60 ± 2.43</td></tr><tr><td>CiteSeer</td><td>74.38 ± 0.73</td><td>74.68 ± 1.02</td><td>74.21 ± 1.30</td><td>76.59 ± 6.85</td><td>78.44 ± 1.82</td><td>67.93 ± 0.59</td><td>80.62 ± 0.36</td><td>84.54 ± 0.65</td></tr><tr><td>PubMed</td><td>78.90 ± 0.54</td><td>77.58 ± 1.13</td><td>77.15 ± 0.53</td><td>62.92 ± 1.32</td><td>64.22 ± 3.23</td><td>71.84 ± 0.51</td><td>64.91 ± 3.49</td><td>66.71 ± 1.44</td></tr><tr><td>Actor</td><td>23.31 ± 1.02</td><td>25.94 ± 0.68</td><td>30.11 ± 0.75</td><td>27.44 ± 0.77</td><td>28.76 ± 1.64</td><td>31.09 ± 1.44</td><td>31.87 ± 1.91</td><td>33.97 ± 2.66</td></tr><tr><td>Chameleon</td><td>38.57 ± 1.68</td><td>45.22 ± 1.67</td><td>48.42 ± 1.15</td><td>41.79 ± 6.23</td><td>40.92 ± 5.34</td><td>47.64±3.09</td><td>44.67 ± 6.05</td><td>49.17 ± 4.87</td></tr><tr><td>Squirrel</td><td>26.14 ± 1.48</td><td>32.80 ± 2.24</td><td>33.50 ± 0.66</td><td>29.43 ± 4.15</td><td>30.29 ± 3.76</td><td>33.42 ±2.04</td><td>31.41 ± 4.66</td><td>34.53 ± 0.88</td></tr><tr><td colspan="9">5-shot</td></tr><tr><td>Cora</td><td>66.50 ± 1.93</td><td>69.50 ± 1.50</td><td>65.15 ± 1.51</td><td>60.71 ± 8.72</td><td>61.51 ± 7.61</td><td>53.42 ± 10.82</td><td>65.76 ± 9.65</td><td>70.57 ± 7.56</td></tr><tr><td>CiteSeer</td><td>53.62 ± 3.25</td><td>61.90 ± 8.01</td><td>51.95 ± 3.21</td><td>61.13 ± 8.51</td><td>59.28 ± 9.32</td><td>56.72 ± 2.49</td><td>66.52 ± 6.42</td><td>68.33 ± 5.53</td></tr><tr><td>PubMed</td><td>61.43 ± 4.26</td><td>62.43±7.07</td><td>59.37 ± 4.58</td><td>56.83 ± 8.51</td><td>58.27 ± 5.48</td><td>64.86 ± 3.47</td><td>63.77 ± 7.44</td><td>64.43 ± 5.28</td></tr><tr><td>Texas</td><td>53.91 ± 3.16</td><td>51.72 ± 2.23</td><td>65.00 ± 4.88</td><td>54.04 ± 8.02</td><td>56.12 ± 7.12</td><td>56.60 ± 12.25</td><td>61.29 ± 3.99</td><td>66.67 ± 4.78</td></tr><tr><td>Wisconsin</td><td>46.51 ± 9.30</td><td>42.06 ± 6.35</td><td>53.71 ± 7.09</td><td>53.80 ± 7.68</td><td>51.75 ± 4.62</td><td>53.11 ± 7.83</td><td>64.11 ±0.84</td><td>65.09 ± 1.98</td></tr><tr><td>Cornell</td><td>41.72 ± 6.32</td><td>42.66 ± 7.68</td><td>50.16 ± 6.00</td><td>58.86 ± 11.37</td><td>56.46 ± 8.48</td><td>45.97 ± 12.34</td><td>58.78 ± 8.77</td><td>64.58±1.86</td></tr><tr><td>Actor</td><td>20.55 ± 3.56</td><td>23.39 ± 2.34</td><td>23.49 ± 2.87</td><td>22.97 ± 3.91</td><td>23.65 ± 4.58</td><td>30.45 ± 2.06</td><td>23.50 ± 6.26</td><td>27.49 ± 3.89</td></tr><tr><td>Chameleon</td><td>30.07 ± 2.66</td><td>27.88 ± 3.56</td><td>31.07 ± 4.81</td><td>29.20 ± 3.97</td><td>30.82 ± 4.17</td><td>30.20 ± 11.60</td><td>33.59 ± 19.57</td><td>35.29 ± 7.87</td></tr><tr><td>Squirrel</td><td>21.97 ± 1.01</td><td>20.91 ± 0.67</td><td>27.07 ± 2.30</td><td>27.30 ± 5.36</td><td>26.73 ± 3.74</td><td>31.06 ± 2.90</td><td>29.82 ± 1.52</td><td>32.41 ± 3.42</td></tr></table></body></html>

Few-shot Node Classification In NC task, We train the model on 10 randomly generated $k$ -shot tasks, and the results are summarized in Table 3, revealing the following findings: 1) In scenarios with both abundant and limited training samples on heterophilic datasets, our method outperforms all baseline methods, demonstrating the effectiveness of our approach in handling heterophilic information. 2) Our method consistently outperforms most of the baseline methods, regardless of dataset homophily or heterophily and different sample setups. Particularly, compared with All-inOne, our method has a significant advantage in the Actor and Cornell heterophilic datasets. This indicates that our subgraph generator and inserting patterns for heterophilic datasets is effective. 3) In 5-shot scenarios, our method exhibits better performance compared to traditional GNN models. This suggests that pre-training plays a crucial role within our framework. 4) Contrasting the pre-trained model fine-tuning paradigms, prompt tuning consistently produces superior outcomes, suggesting that the added prompt graph plays a beneficial role. 5) In the 100-shot setting, the end-toend method performs better in two homophilic datasets. This trend can be ascribed to the substantial scale of the training data, which is particularly favorable for such models, given the distinctiveness of node features. Moreover, these methods are inherently tailored for this specific structure.

Few-shot Graph Classification In GC task, the test results are presented in Table 4, revealing the following findings: 1) Our method performs on par with or even outperforms traditional GNNs in several datasets, indicating that the framework is not only applicable to node-level problems but also effective as a unified model. 2) Compared to the Allin-One approach, our method shows significant improvements, especially in ENZYMES and BZR datasets, suggesting the effectiveness of incorporating multi-perspective prompts in this task. 3) Under the condition of limited samples, the performance of our model does not show a significant improvement compared to traditional methods, unlike in NC. This could be related to the dataset size and characteristics. Following experiments will explore different quantities of $k$ in the $k$ -shot setting.

RandomP w/oHoPw/oRandW W/oHeP&HoP 1w/oHeP all 70 55 50 CiteSeerTexas PROTEINS

70 Accuracy Texas 50 Actor ENZYMES 30 101520 Number of Prompt Tokens

# Model Analysis

To gain a better understanding of the role of modules within the model and the impact of hyperparameters, we conduct the following experiments including ablation study and hyper-parameter analysis.

Ablation Study To better illustrate the effectiveness of each module in our method, we conduct an ablation study to evaluate the variables in the entire framework. w/o HeP: Removing the heterophilic perspective prompt from our framework; w/o HoP: Removing the homophilic perspective prompt from our framework; w/o HeP & HoP: Removing both the heterophilic and homophilic perspective prompt from our framework; w/o RandW: Removing subgraph extractor from our model, using $K$ -hop subgraphs instead; RandomP: Using random parameters for the multiviews prompts in our framework, without training. We select the homophilic dataset CiteSeer and the heterophilic dataset Texas for NC, as well as the PROTEINS dataset for GC, under the 10-shot setting. The results are shown in Figure 3. We can see that HoP and HeP play different roles in homophily and heterophily NC datasets, which means integrating information from both designed prompts is effective. Trainable prompt tokens significantly outperform random, nontrainable tokens. In the GC task, the information collection and prompt design from both perspectives are also effective.

Table 4: Performance on graph classification with best bolded and runner-up underlined.   

<html><body><table><tr><td>Method</td><td>GCN</td><td>GAT</td><td>GT</td><td>GraphCL+FT</td><td>simGRACE+FT</td><td>GraphPrompt</td><td>All in One</td><td>Ours</td></tr><tr><td colspan="9">20-shot</td></tr><tr><td>ENZYMES</td><td>21.76 ± 3.16</td><td>19.33 ± 2.27</td><td>22.38 ± 3.43</td><td>18.87 ± 1.99</td><td>19.25 ± 2.29</td><td>18.67 ± 3.39</td><td>18.62 ± 1.40</td><td>23.33 ± 2.40</td></tr><tr><td>PROTEINS</td><td>60.82 ± 6.75</td><td>57.12 ± 9.16</td><td>64.18 ± 4.05</td><td>60.87 ± 8.31</td><td>59.04 ± 9.27</td><td>58.02 ± 11.34</td><td>64.62 ± 6.05</td><td>66.29 ± 10.27</td></tr><tr><td>MUTAG</td><td>66.87 ± 8.00</td><td>69.92 ± 1.64</td><td>68.09 ± 2.23</td><td>62.53 ± 11.08</td><td>64.26 ± 9.73</td><td>74.67 ± 4.14</td><td>72.00 ± 3.45</td><td>73.33 ± 4.27</td></tr><tr><td>COX2</td><td>68.56 ± 3.59</td><td>66.50 ± 2.98</td><td>63.37 ± 3.69</td><td>58.69 ± 9.69</td><td>62.33 ± 9.55</td><td>63.35 ± 7.75</td><td>67.10 ± 4.70</td><td>69.20 ± 5.15</td></tr><tr><td>BZR</td><td>75.90 ± 6.40</td><td>74.77 ± 4.43</td><td>77.53 ± 2.35</td><td>60.56 ± 15.58</td><td>58.92 ± 7.21</td><td>64.34 ± 7.63</td><td>63.64 ± 5.38</td><td>68.00 ± 5.11</td></tr><tr><td colspan="9">5-shot</td></tr><tr><td>ENZYMES</td><td>16.71 ± 3.01</td><td>15.90 ± 4.13</td><td>19.14 ± 2.27</td><td>19.17 ± 0.92</td><td>17.17 ± 2.34</td><td>28.54 ± 3.56</td><td>27.92 ± 1.26</td><td>30.46 ± 1.65</td></tr><tr><td>PROTEINS</td><td>55.85 ± 10.66</td><td>48.78 ± 18.46</td><td>56.87 ± 6.51</td><td>62.38 ± 6.88</td><td>61.32 ± 10.25</td><td>60.29 ± 12.67</td><td>62.28 ± 6.09</td><td>64.60 ± 10.24</td></tr><tr><td>MUTAG</td><td>61.79 ± 6.15</td><td>67.40 ± 3.53</td><td>66.26 ± 5.58</td><td>61.87 ± 12.23</td><td>65.46 ± 8.44</td><td>66.20 ± 3.74</td><td>70.67 ± 12.50</td><td>72.73 ± 6.19</td></tr><tr><td>COX2</td><td>50.06 ± 9.18</td><td>50.20 ± 20.93</td><td>51.35 ± 8.60</td><td>50.40 ± 20.33</td><td>51.87 ± 7.45</td><td>51.15 ± 9.17</td><td>59.01 ± 3.83</td><td>60.26 ± 6.97</td></tr><tr><td>BZR</td><td>56.16 ± 11.07</td><td>54.29 ± 18.57</td><td>62.78 ± 7.58</td><td>57.84 ± 11.61</td><td>59.86 ± 12.33</td><td>56.28 ± 12.36</td><td>60.30 ± 7.54</td><td>63.10 ± 6.47</td></tr></table></body></html>

![](images/2c464c45aa7e2c733512f395a259006c1e944f01612e8817f9d285ef32fdcb26.jpg)  
Figure 5: Impact of #shots analysis

Hyper-parameters Analysis We evaluate several key hyper-parameters affecting the effectiveness of our model HeterGP. 1) Number of prompt tokens: We test this on datasets of different scales, Texas and Actor for NC and ENZYMES for GC. The results are shown in Figure 4. For datasets of different scales, the performance reaches a bottleneck as the number of tokens increases. This implies that only a small number of tokens are needed to achieve good performance, thereby reducing the number of parameters that need to be trained. 2) Number of few-shot samples: We test this with different settings across various datasets and tasks. The results are shown in Figure 5. It can be observed that under the few-shot setting for NC, HeterGP improves with an increase in the number of samples, and the test variance decreases. However, in the GC task, an increase in the number of samples does not consistently lead to improved performance. Instead, performance tends to deteriorate with a larger sample size. This trend can be attributed to the random selection of a limited number of high-quality samples, which happened to include representative graphs of the class, thereby enhancing classification performance.

Table 5: Impact of pre-trained model with best bolded.   

<html><body><table><tr><td>Dataset</td><td>CiteSeer</td><td>Texas</td><td>Actor</td></tr><tr><td>GCL(GCN)+ FT</td><td>63.43 ± 5.28</td><td>52.76 ± 6.77</td><td>22.53 ± 5.78</td></tr><tr><td>GCL(GT)+FT</td><td>61.13 ± 8.51</td><td>54.04 ± 8.02</td><td>22.97 ± 3.91</td></tr><tr><td>GCL(GCN)+ProG</td><td>62.07 ± 5.35</td><td>54.37 ± 7.48</td><td>22.62 ± 1.22</td></tr><tr><td>GCL(GT)+ProG</td><td>66.52 ± 6.42</td><td>61.29 ± 3.99</td><td>23.50 ± 6.26</td></tr><tr><td>GCL(GCN)+HeterGP</td><td>70.35 ± 2.15</td><td>61.95 ± 4.74</td><td>26.96 ± 4.81</td></tr><tr><td>GCL(GT)+HeterGP</td><td>68.33 ± 5.53</td><td>66.67 ± 4.78</td><td>27.49 ± 3.89</td></tr></table></body></html>

In contrast, in the 20-shot scenario, the inclusion of more low-quality samples resulted in a decline in performance.

Pre-train Model Analysis To assess the effectiveness of our prompt approach with diverse pre-training strategies in homophilic and heterophilic contexts, we conduct experiments utilizing the contrastive learning method GraphCL (GCL) with various GNN encoder backbones like GCN and GT. GCN focuses on local neighborhood information, whereas GT captures broader global perspectives. The results are presented in Table 5, demonstrating that our approach attains superior performance across both pre-training models. Particularly noteworthy is the significant accuracy improvement in heterophilic datasets, where our prompt boosts performance even when using GCN as the backbone, contrasting with prompts solely grounded in homophily assumptions that exhibit limited efficacy.

# Conclusion

In this research, we delve into the fine-tuning of graph prompts in scenarios characterized by both homophily and heterophily within a multi-task few-shot framework. Our study presents a multi-view strategy aimed at encapsulating both forms of information, with a focus on optimizing information acquisition and prompt graph architecture. Through our experiments, we unveil the efficacy of selectively integrating tunable token information via tailored prompts across diverse graph configurations. Moving forward, our research opens avenues for exploring the adaptability and robustness of prompt tuning across more complex graph structures, including those with evolving dynamics. Moreover, exploring the incorporation of domain-specific knowledge into prompt design is a promising way to expand this work.

# Acknowledgments

This work was supported by the National Natural Science Foundation of China (No. 62422210, 92370111, 62302333, 62272340, 62276187), the Open Research Fund from Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ) (No.GML-KF-24-16) and Hebei Natural Science Foundation (No.F2024202047).