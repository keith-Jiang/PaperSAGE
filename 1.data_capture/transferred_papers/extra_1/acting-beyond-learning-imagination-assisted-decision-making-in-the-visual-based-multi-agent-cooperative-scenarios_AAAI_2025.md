# Acting Beyond Learning: Imagination-Assisted Decision-Making in the Visual-based Multi-Agent Cooperative Scenarios

Huanhuan Yang1, Dianxi $\mathbf { S h i } ^ { 2 * }$ , Songchang $\mathbf { J i n } ^ { 2 }$ , Guojun Xie3, Yang Chen4,5, Chunping $\mathbf { Q } \mathbf { i } \mathbf { u } ^ { 2 }$ , Shaowu Yang1

1 College of Computer, National University of Defense Technology, Changsha, China 2 Academy of Military Sciences, Beijing, China 3 Nanjing University of Aeronautics and Astronautics, Nanjing, China 4 School of Computer Science, Peking University, Beijing, China 5 Tianjin Artificial Intelligence Innovation Center, Tianjin, China yanghh94 $@$ 126.com, dxshi $@$ nudt.edu.cn

# Abstract

Learning optimal policies in multi-agent cooperative settings with visual observations is significant and challenging. Agents must first perform state representation learning for their image observations and then learn policies in the abstracted state space. Aiming at this problem, we propose a novel model-based MARL method named Contrastive Latent World for Policy Optimization (CLWPO). In CLWPO, we first design a state representation model to facilitate learning in the latent state space. With the support of this model, we construct the latent world and introduce a contrastive variational bound (CVB) to optimize it. Subsequently, we develop a heuristic policy optimization (HPO) scheme, incorporating model-free learning with model-based planning to obtain robust policies that predict future behaviors. In particular, in the planning, we maintain a queue of teammate models and calculate an adaptive rollout length for each agent to support their self-imagination and reduce the model-based return discrepancy. Finally, we conducted extensive experiments in the PettingZoo benchmark, and results show that CLWPO significantly enhances learning efficiency and improves agent performance compared to state-of-the-art MARL methods.

# Introduction

Multi-agent Reinforcement Learning (MARL), where multiple agents interact with the environment through trial and error to learn efficient policies, recently has witnessed significant advancements in tackling complex multi-agent tasks, spanning domains like real-time strategy games (OpenAI 2018), sports games (Kurach et al. 2020), autonomous driving (Zhou et al. 2020), etc. There are mainly three learning paradigms in MARL: centralized learning (Ye et al. 2020), independent learning (de Witt et al. 2020), and centralized training with decentralized execution (CTDE) (Lowe et al. 2017). Among them, CTDE is a prevalent paradigm commonly used by researchers, utilizing global information for agents’ training while local observations for decisionmaking. With CTDE, numerous model-free learning approaches have been developed (Sunehag et al. 2017; Rashid et al. 2018; Lowe et al. 2017). While they perform well in simulated environments (Lowe et al. 2017; Vinyals et al. 2017; Berner et al. 2019), one significant weakness is the lower sample efficiency. This implies that agents must collect substantial data to learn optimal policies. For instance, AlphaStar requires 44 days of cumulative training (200 years of gameplay) to defeat professional StarCraft players (Arulkumaran, Cully, and Togelius 2019), and OpenAI Five undergoes ten months of training (over 11,000 years of gameplay) to best Dota 2 world champions (OpenAI 2018).

Furthermore, the lower sample efficiency intensifies when agents face high-dimensional observation spaces, which not only slows down policy convergence speed but also significantly restricts the practical application of MARL. In realworld multi-agent systems, agents utilize multiple sensors to perceive the environment information described by complex features. To learn policies and make decisions more efficiently, they should first perform state representation learning (SRL) to map high-dimensional observations into the low-dimensional latent space. Then, these abstract representations can help agents better understand the inner structure of the environment and improve learning efficiency.

However, there has been limited exploration of multiagent SRL, primarily due to some inherent challenges of MARL. The major is the non-stationarity (Hernandez-Leal et al. 2017). In MARL, multiple agents concurrently optimize their policies, resulting in an unstable environment from the perspective of each agent. The next is the curse of dimensionality (Hernandez-Leal, Kartal, and Taylor 2019), where the joint state-action space exponentially grows with increased agents. In addition, agents in MARL also face partial observability, coordination, and other challenges. The presence of these challenges exacerbates the optimal policies’ learning difficulty. Therefore, how to characterize agents’ high-dimensional observations and optimize their policies in visual-based multi-agent settings is a crucial issue that needs to be resolved and well-researched. Aiming at this problem, inspired by the superior performance of modelbased reinforcement learning (MBRL) in single-agent domain (Wang et al. 2019; Sun et al. 2019), we focus on multiagent cooperative tasks and propose the CLWPO method. CLWPO aims to learn optimal policies via a heuristic policy optimization (HPO) scheme in the latent world constructed by representation, transition, and reward models and optimized by the contrastive variational bound (CVB). Specifically, our main contributions are summarized below:

• To extract task-relevant information from visual observations during agents’ interaction and policy learning processes, following characteristics of the CTDE paradigm, we design a representation model consisting of multiple agent representation modules and one state inference module to obtain an abstracted latent state space. Further, in this space, we construct the latent world model and optimize it with the CVB obtained by maximizing the data log-likelihood of joint observations and rewards. • To make agents learn robust policies capable of predicting long-horizon behaviors, we develop an HPO scheme that integrates model-free learning with model-based planning. In the former, we optimize policies via real interaction data to reduce the negative impact of the inaccurate world model caused by learning bias. For planning, we maintain a queue of teammate models and calculate an adaptive rollout length for each agent to support their self-imagination in the learned latent world while lowering the upper bound of model-based return discrepancy. • We conduct extensive experiments in the PettingZoo benchmark (Terry et al. 2021). Results show that CLWPO can efficiently represent observations, learns superior cooperative policies, performs better, and is applicable in visual-based settings with diverse agents.

# Related Work

# Single-agent State Representation

In the single-agent RL, the state representation learning methods have been well-studied. Current approaches primarily revolve around developing various self-supervised auxiliary tasks to train encoders capable of extracting low-dimensional and informative features from the agent’s high-dimensional or pixel observations. These approaches include reconstructing the agent’s original observations (Lange and Riedmiller 2010; Lange, Riedmiller, and Voigtla¨nder 2012; Yarats et al. 2021), employing data augmentation with contrastive or multi-view learning to assess the similarity between states (Kim et al. 2019; Laskin, Srinivas, and Abbeel 2020; Mazoure et al. 2020), or capturing common and critical task-related information from disparate view data (Chen et al. 2017; Li et al. 2019; Fan and Li 2022; Yang et al. 2022), etc. In particular, the latent world (Oh, Singh, and Lee 2017; Hafner et al. 2019, 2020, 2021, 2023; Ma et al. 2021), a special case of model-based RL in the latent state space, represents a prominent approach in this research domain, where the agent initially performs state representation learning to derive an abstracted latent state space, followed by the construction and training of environment models within this space. Among these works, PlaNet (Hafner et al. 2019) enables the agent to learn environment dynamics from pixels and choose actions via online planning in the latent space. Dreamer (Hafner et al. 2020) and its variants (Hafner et al. 2021, 2023) learn policies that can solve long-horizon tasks with image observations via latent imagination in the compact state space.

# Multi-agent State Representation

Although sufficient single-agent RL research provides valuable insights into multi-agent representation, there is still relatively little research. Both the attention mechanism (Iqbal and Sha 2019; Liu et al. 2020; Shi et al. 2022) and objectcentric representation (Liu et al. 2021; Shang et al. 2021) can reduce dimensions of agents’ (joint) observations by retaining essential information and inferring environmental internal structure. However, they perform poorly in partially observable or complex visual-based settings. Regarding the latent world-based methods, MBVD (Xu et al. 2022) integrates imaged latent states with current states when evaluating state values. DLC (Schwarting et al. 2021) learns visual control policies in competitive two-player racing games, while MAMBA (Egorov and Shpilman 2022) learns efficient policies in multi-agent cooperative scenarios. Although DLC and MAMBA performed well, there was still room for improvement. Firstly, they decomposed latent states into sub-states, utilizing all sub-states for dynamics learning but only individual sub-states for reward and observation functions learning, partially addressing the non-stationarity issue. Secondly, they relied on all sub-states for representation, making agents infer sub-states of other agents or communicate with others in the interaction, potentially introducing compounding errors or increasing communication complexity. Thirdly, they leave the multi-agent cooperative settings with image observations for future work.

Based on the above analysis, we present CLWPO, which follows the CTDE paradigm. We carefully design the representation model, using individual sub-states for decentralized execution to avoid extra errors or communication while inferring global states from all sub-states to predict transition dynamics and rewards. To learn robust policies that can predict the future, we design the HPO scheme, integrating model-free MARL learning with adaptive model-based planning. Importantly, CLWPO is versatile in our tested visual-based cooperative environments with varying agents.

# Preliminaries

# Partially Observable Stochastic Game

We consider the multi-agent cooperative scenarios, which can be defined as a partially observable stochastic game (POSG) (Hansen, Bernstein, and Zilberstein 2004) $\langle \mathbb { D } , S , \mathbf { A } , T , \mathbf { O } , \mathbb { O } , \mathbf { r } , n , \gamma , b _ { 0 } , h \rangle$ , where $\mathbb { D }$ is the set of $n$ agents, $S$ is the finite set of environment states. $\textbf { O } =$ $\bar { \bf \nabla } \times _ { i \in \mathbb { D } } { \bf O } ^ { i }$ and $\mathbf { A } = \mathbf { \nabla } \times _ { i \in \mathbb { D } } \mathbf { A } ^ { i }$ are the sets of joint observations and actions. $T : S \times \mathbf { A } \  \ S$ is the state transition function, $\mathbf { r } : S \times \mathbf { A }  \mathbb { R }$ is the reward function and $\mathbb { O } : S \times \mathbf { A } \to \mathbf { O }$ is the observation function. $\gamma \in \ [ 0 , 1 ]$ is the discount factor, $b _ { 0 }$ is the initial environment state distribution, and $h$ is the finite task’s horizon. In POSG, each agent’s policy $\pi _ { i } : \tau ^ { i } \to \mathbf { A } ^ { i }$ , is conditioned on their ActionObservation History (AOH) $\tau ^ { i } = \{ a _ { 0 } ^ { i } , o _ { 1 } ^ { i } , \dots , a _ { t - 1 } ^ { i } , o _ { t } ^ { i } \}$ . At each timestep $t$ , each agent $i$ observes $o _ { t } ^ { i }$ , executes action $a _ { t } ^ { i }$ , forming joint observation $\mathbf { o } _ { t } ~ = ~ \langle o _ { t } ^ { 1 } , o _ { t } ^ { 2 } , . ~ . ~ . , o _ { t } ^ { n } \rangle$ and joint action $\mathbf { \bar { a } } _ { t } = \langle a _ { t } ^ { 1 } , a _ { t } ^ { 2 } , \dots , a _ { t } ^ { n } \rangle$ . Given $\mathbf { r } , \gamma$ and $h$ , agents aim to learn cooperative policies that maximize the expected cumulative discounted reward $\begin{array} { r } { R = \sum _ { t = 0 } ^ { h } \sum _ { i = 1 } ^ { n } \gamma ^ { \bar { t } } r _ { i } ( s _ { t } , \mathbf { a } _ { t } ) } \end{array}$

It is important to emphasize that we assume an unknown environment in the paper, indicating that we have no prior knowledge regarding the transition function $T$ , reward function r, and observation function $\mathbb { O }$ .

# Variational Auto-Encoders

As one of the most influential techniques in unsupervised learning, variational auto-encoders (VAE) (Kingma and Welling 2013) have been widely used for image generation (Razavi, Van den Oord, and Vinyals 2019), representation learning (Ha and Schmidhuber 2018; Huang et al. 2020), etc. In VAE, given the dataset $\mathbf { X } = \{ x _ { 1 } , \dots , x _ { N } \}$ sampled from an unknown distribution $p ( x )$ , we want to learn a latentvariable model $\begin{array} { r } { p _ { \theta } ( x ) = \int \dot { p _ { \theta } } ( x , z ) d _ { z } = \int p _ { \theta } ( x | z ) p ( z ) d _ { z } } \end{array}$ to approximate $p ( x )$ . Typically, $\theta$ is optimized by maximizing the average marginal log-likelihood $\begin{array} { r } { \frac { 1 } { N } \log p ( \mathbf { \dot { X } } ) } \end{array}$ . However, once $\theta$ is parameterized by a neural network, computing the log-likelihood $\log p ( x _ { i } )$ becomes intractable, introducing optimization challenges. Thus, VAE instead maximizes the following evidence lower bound (ELBO):

$$
\log p _ { \theta } ( x ) \geq \mathbb { E } _ { q _ { \phi } ( z | x ) } \Big [ \log \frac { p _ { \theta } ( x , z ) } { q _ { \phi } ( z | x ) } \Big ] = \mathrm { E L B O }
$$

$$
= \mathbb { E } _ { q _ { \phi } ( z | x ) } [ \log p _ { \theta } ( x | z ) ] - D _ { K L } \left( q _ { \phi } ( z | x ) | | p ( z ) \right)
$$

Where $D _ { K L } ( \cdot | | \cdot )$ is the Kullback-Leibler divergence between two distributions. $p ( z )$ is the prior. $q _ { \phi } ( z | x )$ is the variational posterior (encoder of VAE), responsible for generating continuous latent representations, $p _ { \theta } ( x | z )$ is the generative model (decoder of VAE), responsible for the reconstruction of the observed data, and $\phi , \theta$ are their parameters.

# Multi-Agent Soft Actor-Critic (MASAC)

The multi-agent soft actor-critic (MASAC) is an offpolicy MARL algorithm that extends soft actor-critic (SAC) (Haarnoja et al. 2018) to multi-agent settings. In MASAC, each agent learns a decentralized stochastic policy $\pi _ { \varphi _ { i } }$ to maximize a $\gamma$ -discounted and maximum entropy-based return (Haarnoja et al. 2017):

$$
\begin{array} { r } { \bigtriangledown _ { \varphi _ { i } } J = \mathbb { E } _ { \mathbf { o } _ { t } \sim \mathcal { B } , \mathbf { a } _ { t } \sim \pi _ { \varphi } } \big [ \bigtriangledown _ { \varphi _ { i } } \log ( \pi _ { \varphi _ { i } } ( a _ { t } ^ { i } | \sigma _ { t } ^ { i } ) ) \big ( - \alpha _ { i } } \\ { \log \pi _ { \varphi _ { i } } ( a _ { t } ^ { i } | \sigma _ { t } ^ { i } ) + \displaystyle \operatorname* { m i n } _ { m = 1 , 2 } Q _ { w _ { i } } ^ { m } ( \mathbf { o } _ { t } , \mathbf { a } _ { t } ) \big ) \big ] } \end{array}
$$

The centralized critics of agent $i$ are trained to minimize the following Bellman error:

$$
L _ { Q _ { w _ { i } } ^ { m } } = \mathbb { E } _ { ( \mathbf { o } _ { t } , \mathbf { a } _ { t } , \mathbf { r } _ { t } , \mathbf { o } _ { t + 1 } ) \sim \mathcal { B } } \big [ \left( Q _ { w _ { i } } ^ { m } ( \mathbf { o } _ { t } , \mathbf { a } _ { t } ) - y _ { i } \right) ^ { 2 } \big ]
$$

where $y _ { i }$ is the target value, defined as:

$$
y _ { i } = r _ { t } ^ { i } + \gamma \mathbb { E } _ { \mathbf { a } _ { t + 1 } \sim \pi _ { \varphi } } \big [ \operatorname* { m i n } _ { m = 1 , 2 } \bar { Q } _ { \bar { w } _ { i } } ^ { m } \big ( \mathbf { o } _ { t + 1 } , \mathbf { a } _ { t + 1 } \big ) - \alpha _ { i }
$$

$$
\log \pi _ { \varphi _ { i } } ( a _ { t + 1 } ^ { i } | o _ { t + 1 } ^ { i } ) \Big ]
$$

In Eq. (2) - (4), $\boldsymbol { B }$ is the replay buffer, storing transition data $\left( \mathbf { o } _ { t } , \mathbf { a } _ { t } , \mathbf { r } _ { t } , \mathbf { o } _ { t + 1 } \right)$ . $Q _ { w _ { i } } ^ { m }$ and $\hat { Q } _ { \hat { w } _ { i } } ^ { m }$ $( m = 1 , 2 )$ are two critics and target critics of agent $i$ . Parameters $w _ { i } , \bar { w } _ { i }$ , and $\varphi _ { i }$ correspond to the critics, target critics, and actors. $\bar { w } _ { i }$ is softly updated based on $w _ { i }$ , defined as ${ \bar { w } } _ { i } \gets \zeta _ { w } \cdot w _ { i } +$ $( 1 - \zeta _ { w } ) \cdot \bar { w } _ { i }$ , where $\zeta _ { w }$ is a hyper-parameter that controls the updating rate. Given the target entropy $\mathcal { H } _ { i }$ of agent $i$ ’s policy distribution, temperature parameter $\alpha _ { i }$ is updated by:

$$
L _ { \alpha _ { i } } = \mathbb { E } _ { a _ { t } ^ { i } \sim \pi _ { \varphi _ { i } } } [ - \alpha _ { i } \log \pi _ { \varphi _ { i } } ( a _ { t } ^ { i } | o _ { t } ^ { i } ) - \alpha _ { i } \mathcal { H } _ { i } ]
$$

# Method

In this section, we present our proposed Contrastive Latent World for Policy Optimization (CLWPO), a model-based method that learns efficient policies in multi-agent cooperative scenarios with image observations. In CLWPO, we first design the representation model, construct the latent world, and formulate the CVB objective to optimize the world. Next, we develop the HPO scheme to learn robust policies that can predict the future. In particular, we provide the detailed procedure of CLWPO in Appendix D.

# CLWPO Framework

In CLWPO, to enable fast trajectory prediction in the compact latent state space, we fully leverage the CTDE paradigm and carefully design the representation model as multiple agent representation modules and one state inference module. Based on this model, we illustrate the overall framework of CLWPO in Fig. 1, with three parts below. (1) Environment interaction. Agents utilize their trained (or randomly initialized) agent representation modules and policies to encode image observations, select actions, interact with the environment, and then collect and store the transition data into the replay buffer to subsequently update relevant models. (2) Latent World Learning. On the foundation of the representation model, we construct environment models—including transition and reward models—that, together with the representation model, constitute the latent world and can be optimized through the CVB objective. (3) Policy Optimization. In CLWPO, we develop the HPO scheme, which integrates model-free learning with model-based planning to learn robust policies that predict long-horizon behaviors. As part of the planning, we maintain a queue of teammate models and calculate an adaptive rollout length $h _ { i }$ for each agent to effectively reduce the model-based return discrepancy bound when self-imagination (planning) in the latent world.

# Latent World Learning

Representation Model. In the multi-agent domain, we can naively maintain a centralized representation model — mapping joint observation $\mathbf { o } _ { t }$ into global latent state $s _ { t }$ (Schwarting et al. 2021; Egorov and Shpilman 2022). However, it is intractable in the CTDE paradigm, as agents can only access their local observations during the decentralized execution. Although information (like observation) transmission and agent modeling are two general practices for this challenge, they both have limitations. The former yields communication complexity, while the latter introduces compounding errors in environment interaction. Alternatively, another way is to maintain multiple decentralized representation models to map individual observation $o _ { t } ^ { i }$ into local latent state $s _ { t } ^ { i }$ . Unlike the centralized model, models in this way are more flexible. However, they potentially suffer learning instability as the dynamics, observation, and reward models occur in the local latent state space.

Based on these analyses, in this paper, we consider the characteristics of the CTDE paradigm, defining the representation model as multiple $( n )$ agent representation modules and one shared state inference module, as shown in Appendix B and Eq.(6). During decentralized execution, each

(,ap,r)-1 ReplayBuffer Sample dataa-1 Multi-agent Env. 0 Agent st Teammate Models HPO Scheme Representation 鑫 Model-based Adaptive Self-imagination at Module 1 鑫鑫.鑫 π1(a, 1st) Agent 1 +h Teammate1Teammate2Teammaten-1 Agent 2 Latent World Model 0² Agent S Poliey 2 . +1,rt a Representation Environment Agent n t+h. π(a²1s²) Transition Model Reward Model Models Model-free MARL Learning 11 ： i！ Representation Model ? Adam  Optimizer Y ot Represention 欢 Agent Policy 1 Policy 2 Policy n a Modulen Repmesentation State Inference Module X O πn(a|sn) (a) Environement Interaction. (b) Latent World Learning. (c) Policy Optimization.

agent utilizes its agent representation module to transform complex observation $o _ { t } ^ { i }$ into local latent state $s _ { t } ^ { i }$ . Then, they use the shared state inference module to infer global posterior state $s _ { t }$ from $\mathbf { s } _ { t } ^ { i } = \{ s _ { t } ^ { 1 } , \cdot \cdot \cdot , s _ { t } ^ { i } , \cdot \cdot \cdot , s _ { t } ^ { n } \}$ for centralized training. Specifically, each agent representation module consists of three key components: an encoder, a GRU network, and an MLP. The encoder maps $o _ { t } ^ { i }$ into a low-dimensional embedding $e _ { t } ^ { i }$ , and we implement it as CNN in the visualbased tasks. By taking the previously hidden information $h _ { t - 1 } ^ { i }$ , local latent state $s _ { t - 1 } ^ { i }$ and action $a _ { t - 1 } ^ { i }$ as input, the GRU network outputs current hidden information $h _ { t } ^ { i }$ , which incorporates historical information into the state representation process. After that, the MLP fuses $h _ { t } ^ { i }$ and $e _ { t } ^ { i }$ as a local latent state $s _ { t } ^ { i }$ . The state inference module, shared among all agents, is defined as a standard Variational Auto-encoder (VAE) (Kingma and Welling 2013), consisting of a variational encoder and decoder. The former infers $s _ { t }$ from $\mathbf { s } _ { t } ^ { i }$ , while the latter reconstructs $\mathbf { s } _ { t } ^ { i }$ based on $s _ { t }$ .

Latent World Model. Upon the representation model, we define two environment models: transition $p _ { \theta } ( s _ { t } | s _ { t - 1 } , \mathbf { a } _ { t - 1 } )$ and reward $p _ { \theta } ( \mathbf { r } _ { t } | s _ { t } )$ , as in Eq. (6). In the equation, $p _ { \theta } ( \cdot )$ and $q _ { \theta } ( \cdot )$ are distributions in the latent state space, with $\theta$ denoting the combined parameter vector. The representation model $q _ { \theta } ( s _ { t } | \mathbf { o } _ { \leq t } , \mathbf { a } _ { < t } )$ , contains four components: $f _ { 1 _ { \theta } } , f _ { 2 _ { \theta } }$ , $f _ { 3 _ { \theta } }$ , and $f _ { 4 _ { \theta } }$ , corresponding to the encoder, GRU, MLP, and VAE illustrated in the above and Appendix B.

$$
\left\{ \begin{array} { l l } { \mathrm { R e p r e s e n t a t i o n ~ M o d e l : } \ s _ { t } \sim q _ { \theta } \big ( s _ { t } \big \vert \mathbf { o } _ { \pm t } , \mathbf { a } _ { < t } \big ) } \\ { \qquad \ } \\ { \longrightarrow \left\{ \begin{array} { l l } { \mathrm { E n c o d e r : } \ e _ { t } ^ { i } \sim f _ { 1 0 } \big ( e _ { t } ^ { i } \big \vert \sigma _ { t } ^ { i } \big \vert \sigma _ { t } ^ { i } \big ) } \\ { \mathrm { G R U : } \quad h _ { t } ^ { i } \sim f _ { 2 \theta } \big ( h _ { t } ^ { i } \big \vert h _ { t - 1 } ^ { i } , s _ { t - 1 } ^ { i } , a _ { t - 1 } ^ { i } \big ) } \\ { \mathrm { M L P : } \quad s _ { t } ^ { i } \sim f _ { 3 \theta } \big ( s _ { t } ^ { i } \big \vert h _ { t } ^ { i } , e _ { t } ^ { i } \big \vert } \\ { \mathrm { V A E : } \quad s _ { t } \sim f _ { 4 \theta } \big ( s _ { t } \big \vert s _ { t } ^ { 1 } , s _ { t } ^ { 2 } , \cdots , s _ { t } ^ { n } \big ) } \end{array} \right. } \\ { \mathrm { E n v i r o n m e n t ~ M o d e l s : } \ } \\ { \qquad \ \left\{ \begin{array} { l l } { \mathrm { T r a n s i t i o n : } \ s _ { t } \sim p _ { \theta } \big ( s _ { t } \big \vert s _ { t - 1 } , \mathbf { a } _ { t - 1 } \big ) } \\ { \mathrm { R e w a r d : } \quad \mathbf { r } _ { t } \sim p _ { \theta } \big ( \mathbf { r } _ { t } \big \vert s _ { t } \big \vert } \end{array} \right. } \end{array} \right.
$$

Learning of the latent world model. In MBRL, incorporating the observation model into the learned world (Schwarting et al. 2021; Egorov and Shpilman 2022) results in the reconstruction of the observation space and inevitably encodes task-irrelevant information into the latent states. This, in turn, hinders agents from obtaining an accurate latent world and further increases learning instability. Aiming for this challenge, we introduce an optimization objective called the contrastive variational bound (CVB), benefiting from the potential of contrastive learning in representation learning (Laskin, Srinivas, and Abbeel 2020). To derive CVB, we maximize the data log-likelihood of joint observations and rewards for the sequential trajectory data $\left\{ \mathbf { o } _ { 1 : T } , \mathbf { a } _ { 1 : T } , \mathbf { r } _ { 1 : T } \right\}$ . Then, by leveraging the importance weighting, Jensen’s inequality, and contrastive learning techniques, we obtain:

$$
\begin{array} { r l } & { \displaystyle \ln p \big ( \mathbf { o } _ { 1 : T } , \mathbf { r } _ { 1 : T } \big | \mathbf { a } _ { 1 : T } \big ) \geq \sum _ { t = 1 } ^ { T } \Big ( } \\ & { \displaystyle \mathbb E _ { q \big ( s _ { t } \mid \mathbf { o } _ { \leq t } , \mathbf { a } _ { < t } \big ) } \Big ( \underbrace { \ln p \big ( s _ { t } \big | \mathbf { o } _ { t } \big ) - \sum _ { \mathbf { o } _ { t } ^ { \prime } } p \big ( s _ { t } \big | \mathbf { o } _ { t } ^ { ' } \big ) } _ { < \mathbf { ) } _ { t } \mid \mathbf { o } _ { \leq t } , \mathbf { \xi } _ { t } \mid s _ { t } \mid } + \underbrace { \ln p \big ( \mathbf { r } _ { t } | s _ { t } \big ) } _ { \mathrm { o } ( \mathbf { \xi } _ { t } \mid \mathbf { o } _ { t } ) } \Big ) - } \end{array}
$$

$$
\begin{array} { r l } & { \underbrace { \mathbb { E } _ { q ( s _ { t - 1 } | \mathbf { o } _ { \leq t - 1 } , \mathbf { a } _ { < t - 1 } ) } D _ { K L } \left( q ( s _ { t } | \mathbf { o } _ { \leq t } , \mathbf { a } _ { < t } ) | | p ( s _ { t } | s _ { t - 1 } , \mathbf { a } _ { t - 1 } ) \right) } _ { \mathrm { t r a n s i t i o n } } } \\ & { - \underbrace { D _ { K L } \left( q \left( s _ { t } | \mathbf { s } _ { t } ^ { \mathbf { i } } \right) | | p ( s _ { t } ) \right) + \mathbb { E } _ { q ( s _ { t } | \mathbf { s } _ { t } ^ { \mathbf { i } } ) } \ln p \left( \mathbf { s } _ { t } ^ { \mathbf { i } } | s _ { t } \right) } _ { \mathrm { ( ) } } \biggr ) } \end{array}
$$

Where $p ( s _ { t } | \mathbf { o } _ { t } )$ is the state model1. Note that we use the InfoNCE contrastive learning loss (Poole et al. 2019) to avoid the reconstruction of complex observations. $q ( s _ { t } | \mathbf { s } _ { t } ^ { \mathbf { i } } )$ and $p ( \mathbf { s } _ { t } ^ { \mathbf { i } } | s _ { t } )$ are the variational encoder and decoder of the state inference module in the representation model, respectively. $p ( s _ { t } ) \sim \mathcal { N } ( 0 , I )$ is the variational distribution. Detailed derivations of Eq. (7) are given in Appendix A.

# Policy Optimization

Adaptive Self-imagination. In the planning, to allow agents to optimize their policies via self-imagination in the latent world, we first maintain a queue of $n - 1$ teammate models $\hat { \pmb { \pi } } _ { \phi _ { - i } } ( \hat { \mathbf { a } } _ { t } ^ { - i } | \mathbf { s } _ { t } ^ { - i } )$ for each agent to infer behaviors of other agents. We consider both discrete and continuous action cases. For discrete actions, we utilize the cross-entropy loss:

$$
L _ { \hat { \pmb { \pi } } _ { \phi _ { - i } } } = - \mathbb { E } _ { { \mathbf { s } } _ { t } ^ { - i } \sim B } \left[ \log \hat { \pmb { \pi } } _ { \phi _ { - i } } ( \hat { \mathbf { a } } _ { t } ^ { - i } | { \mathbf { s } } _ { t } ^ { - i } ) \right]
$$

For tasks with continuous action spaces, we adopt the following smooth-L1 (Huber) loss:

$$
L _ { \hat { \pmb { \pi } } _ { \phi _ { - i } } } = \left\{ \begin{array} { l l } { 0 . 5 ( \mathbf { a } _ { t } ^ { - i } - \hat { \mathbf { a } } _ { t } ^ { - i } ) ^ { 2 } , } & { | \mathbf { a } _ { t } ^ { - i } - \hat { \mathbf { a } } _ { t } ^ { - i } | < 1 } \\ { | \mathbf { a } _ { t } ^ { - i } - \hat { \mathbf { a } } _ { t } ^ { - i } | - 0 . 5 , } & { o t h e r w i s e } \end{array} \right.
$$

Where $\mathbf { a } _ { t } ^ { - i } , \hat { \mathbf { a } } _ { t } ^ { - i }$ , and $\mathbf { s } _ { t } ^ { - i }$ are the actual actions, predicted actions, and local latent states of teammate agents.

Then, corresponding to $\hat { \pmb { \pi } } _ { \phi _ { - i } } ( \hat { \mathbf { a } } _ { t } ^ { - i } | \mathbf { s } _ { t } ^ { - i } )$ , it is crucial to determine a suitable rollout length associated with the theoretical discrepancy between expected returns in the real and learned environment that assesses how well the learned world impacts an agent’s performance compared to the actual environment. Note that agents in CLWPO share environment models but own separate teammate models. Thus, from the perspective of agent $i$ , its return discrepancy is:

Proposition 1. Assume that the expected total variation distance between the learned transition model and real transition model at each timestep $t$ is bounded by $\operatorname* { m a x } _ { t }$ $\mathbb { E } _ { ( s _ { t } , a _ { t } ^ { i } , \mathbf { a } _ { t } ^ { - i } ) \sim \pi _ { D _ { i } } , \pi _ { D _ { - i } } } [ D _ { T V } ( T ( \cdot | \hat { s } _ { t } , a _ { t } ^ { i } , \mathbf { a } _ { t } ^ { - i } ) | | p _ { \theta } ( \cdot | s _ { t } , a _ { t } ^ { i } , \mathbf { a } _ { t } ^ { - i } ) | ] )$ $\mathrm { ~  ~ \lambda ~ } ) ) ] \leq \varepsilon _ { m } ^ { \prime } ,$ , prediction errors of teammate models are bounded as max $\begin{array} { r l r } { _ { s _ { t } ^ { j } } D _ { T V } ( \pi _ { j } ( \cdot | s _ { t } ^ { j } ) \| \hat { \pi } _ { j } ( \cdot | s _ { t } ^ { j } ) ) } & { \leq } & { \epsilon _ { \hat { \pi } } ^ { j } } \end{array}$ , and policies’ divergence are bounded as $\operatorname* { m a x } _ { s _ { t } ^ { i } } D _ { T V } \big ( \pi _ { i } \big ( \cdot | s _ { t } ^ { i } \big ) \big | \big |$ $\begin{array} { r } { \pi _ { D _ { i } } ( \cdot | s _ { t } ^ { i } ) ) \ \leq \ \epsilon _ { \pi } ^ { i } , \operatorname* { m a x } _ { s _ { t } ^ { j } } \ D _ { T V } \big ( \pi _ { j } ( \cdot | s _ { t } ^ { j } ) \| \pi _ { D _ { j } } ( \cdot | s _ { t } ^ { j } ) \big ) \ \leq \ \epsilon _ { \pi } ^ { j } , } \end{array}$ where $\begin{array} { r l r } { j } & { { } \in } & { \{ - i \} } \end{array}$ , subscript $D$ identify data collecting policies, and $r _ { \mathrm { m a x } } ^ { \ i } = \operatorname* { m a x } r _ { i } ( s _ { t } , \mathbf { a } _ { t } )$ . Then the discrepancy bound of return in the real environment $\eta _ { 1 } ^ { i } = \eta _ { i } [ \pi _ { i } , \pi _ { - i } ]$ and in the learned world (using the learned transition model and teammate models) with $k$ -branched rollout $\eta _ { 2 } ^ { i } = \eta _ { i } ^ { b r a n c h }$ $\left[ \left( \pi _ { D _ { 1 } } , \hat { \pi } _ { 1 } \right) , \ldots , \left( \pi _ { D _ { i } } , \pi _ { i } \right) , \ldots , \left( \pi _ { D _ { n } } , \hat { \pi } _ { n } \right) \right]$ is expressed as:

$$
\mid \eta _ { 1 } ^ { i } - \eta _ { 2 } ^ { i } \mid \leq 2 r _ { \operatorname * { m a x } } ^ { i } \biggl [ k \epsilon _ { m } ^ { \prime } + ( k + 1 ) \sum _ { j \in \{ - i \} } \epsilon _ { \hat { \pi } } ^ { j } \ +
$$

$$
\begin{array} { r } { \gamma ^ { k + 1 } \Big ( \epsilon _ { \pi } ^ { i } + \sum _ { j \in \{ - i \} } \epsilon _ { \pi } ^ { j } \Big ) + \frac { \gamma ^ { k + 1 } \big ( \epsilon _ { \pi } ^ { i } + \sum _ { j \in \{ - i \} } \epsilon _ { \pi } ^ { j } \big ) } { 1 - \gamma } \Bigg ] } \end{array}
$$

Proof. See Theorem 2 in Zhang et al. (2021).

In Proposition 1, the return discrepancy is highly related to the prediction errors of teammate models. To reduce the bound, in Eq. (10), we calculate an adaptive branched (rollout) length $h$ to replace the fixed $k$ . Specifically, we multiply $k$ by an adaptive weight proportional to the minimum and maximum prediction errors of teammate models to obtain $h$ . Our intention is straightforward: to select an appropriate length that fully leverages teammate models and minimizes compounding errors negatively impacting performance.

$$
h = k * \left\lfloor \frac { \operatorname* { m i n } _ { j \in \{ - i \} } \epsilon _ { \hat { \pi } } ^ { j } } { \operatorname* { m a x } _ { j \in \{ - i \} } \epsilon _ { \hat { \pi } } ^ { j } } \right\rfloor
$$

With $h$ , each agent $i$ can adaptively perform $h$ times selfimagination in the latent world, with the specific process in Appendix C. Next, we backpropagate the analytic gradient of estimated values along imagined trajectories to update the actor and critic of agent $i$ , as shown in Eq.(11) and Eq.(12).

$$
\begin{array} { r l } & { { \cal L } _ { \varphi _ { i } } = \displaystyle \operatorname* { m a x } _ { \varphi _ { i } } \mathbb { E } _ { { p _ { \theta } } , \pi _ { \varphi _ { i } } } \Big ( \displaystyle \sum _ { \tau = t } ^ { t + h } V _ { \lambda } ( s _ { \tau } ) \Big ) } \\ & { { \cal L } _ { \psi _ { i } } = \displaystyle \operatorname* { m i n } _ { \psi _ { i } } \mathbb { E } _ { { p _ { \theta } } , \pi _ { \varphi _ { i } } } \Big ( \displaystyle \sum _ { \tau = t } ^ { t + h } \| v _ { \psi _ { i } } ( s _ { \tau } ) - V _ { \lambda } ( s _ { \tau } ) \| ^ { 2 } \Big ) } \end{array}
$$

Where $V _ { \lambda } ( s _ { \tau } )$ is an exponentially-weighted average of the $l$ -step value estimates $\dot { V _ { M } ^ { l } } ( s _ { \tau } ) , v _ { \psi _ { i } } ( \cdot )$ is the state value function. Given $h _ { 0 } = \operatorname* { m i n } ( \tau + l , t + h )$ , we have:

$$
\begin{array} { r l } & { V _ { \lambda } ( s _ { \tau } ) = ( 1 - \lambda ) \displaystyle \sum _ { m = 1 } ^ { h - 1 } \lambda ^ { m - 1 } V _ { M } ^ { m } ( s _ { \tau } ) + \lambda ^ { h - 1 } V _ { M } ^ { h } ( s _ { \tau } ) } \\ & { V _ { M } ^ { l } ( s _ { \tau } ) = \mathbb { E } _ { p _ { \theta } , \pi _ { \varphi _ { i } } } \Big ( \displaystyle \sum _ { m = \tau } ^ { h _ { 0 } - 1 } \gamma ^ { m - \tau } r _ { m } ^ { i } + \gamma ^ { h _ { 0 } - \tau } v _ { \psi _ { i } } ( s _ { h _ { 0 } } ) \Big ) } \end{array}
$$

HPO scheme. Although agents can learn to predict the future through planning (self-imagining), the effectiveness of the learned policies heavily relies on the accuracy of the latent world model. When the model is inaccurate, planning within this world will significantly slow down the learning process. Notably, even with our well-designed representation model and CVB objective, this issue is still troubling in MARL settings with pixel observations. We thus develop an HPO scheme, incorporating model-free MARL learning into policy optimization and combining it with model-based planning. In this way, we can obtain an auxiliary training signal that corrects deviations caused by planning via the real interaction trajectory data. Specifically, in HPO, agents first weight the MASAC loss in Eq. (2) by a hyperparameter $\alpha _ { H P O }$ and use this weighted loss to optimize their policies. Then, they perform model-based self-imagination based on these optimized policies via the loss in Eq. (11) to further update policies, resulting in the final HPO objective: $L _ { f i n a l } = \alpha _ { H P O } * L _ { M A S A C } + L _ { s e l f - i m a g i n a t i o n } = \alpha _ { H P O } *$ Eq. $( 2 ) + { \mathrm { E q } }$ . (11), which allows us to learn robust, optimal policies that can predict long-horizon behaviors.

# Experiments Experimental Setup

In this paper, we use the PettingZoo (Terry et al. 2021) to evaluate CLWPO. While it consists of multiple environment classes, we focus on visual-based tasks in the Butterfly and SISL, i.e., Cooperative Pong, Pistonball 5agents & 6agents, and Pursuit, with further details given in Appendix

<html><body><table><tr><td>2KEpisode Steps</td><td>CLWPO</td><td>MAPPO</td><td>MASAC</td><td>MAMBA</td></tr><tr><td>Cooperative Pong (↑)</td><td>68.32±12.96</td><td>61.03±10.87</td><td>60.44± 8.81</td><td>61.30± 7.15</td></tr><tr><td>Pistonball 5agents (↓)</td><td>31.00 ± 4.52</td><td>83.61 ± 8.37</td><td>49.33 ± 20.98</td><td>65.87 ± 28.15</td></tr><tr><td>Pistonball 6agents (↓)</td><td>50.16 ±18.46</td><td>101.78 ± 4.01</td><td>68.59± 13.93</td><td>91.90 ±16.82</td></tr><tr><td>Pursuit (↓)</td><td>500</td><td>500</td><td>500</td><td>500</td></tr><tr><td>1K Episode Steps</td><td>CLWPO</td><td>MAPPO</td><td>MASAC</td><td>MAMBA</td></tr><tr><td>Cooperative Pong (↑)</td><td>68.66±10.73</td><td>59.05 ± 4.82</td><td>59.24 ± 7.01</td><td>58.15±10.34</td></tr><tr><td>Pistonball 5agents (↓)</td><td>42.47 ±17.37</td><td>79.57 ± 6.78</td><td>54.16 ± 9.59</td><td>62.78± 30.90</td></tr><tr><td>Pistonball 6agents(↓)</td><td>55.93 ± 22.75</td><td>93.45 ± 5.37</td><td>82.96± 29.90</td><td>85.61 ± 15.07</td></tr><tr><td>Pursuit (↓)</td><td>500</td><td>500</td><td>500</td><td>500</td></tr></table></body></html>

35 Cooperative Pong 125 Pistonball 5agents 200 Pistonball 6agents 22.5 Pursuit F CLWPO MASAC CLWPO MASAC -50 CLWPO MASAC CLWPO MAAC   
10 10.0 0 400 800 1200 16002000 0 400 800 1200 1600 2000 0 400 8001200 1600 2000 0 400 800 1200 1600 2000 Episodes Episodes Episodes Episodes (a) Cooperative Pong (b) Pistonball 5agents (c) Pistonball 6agents (d) Pursuit

E. We base our CLWPO on the implementation of MAMBA (Egorov and Shpilman 2022) and run 2000 episodes for each experiment on a desktop with an 8-core CPU and two Nvidia GeForce RTX 3090. Throughout the paper, unless otherwise stated, we present the result as the mean and standard error across five different random seeds in the table and figures. Other implementation details are in Appendix F.

We compare CLWPO with the following state-of-the-art methods and our ablations. MAMBA (Egorov and Shpilman 2022) updated the latent world model based on a self-defined communication protocol. MASAC follows the idea of SAC to learn policies. MAPPO (Yu et al. 2022) extends PPO (Schulman et al. 2017) to multi-agent settings. CLWPOrec obs, CLWPO-fixed roll, and CLWPO-image are variants of CLWPO, which replace the contrastive learning with the reconstruction of agents’ original observations in the CVB objective, the adaptive rollout length $h$ with a fixed $k$ , and solely retains model-based adaptive planning (selfimagination) for policy learning, respectively.

# Main Results

We first evaluate CLWPO in the four above tasks, with learning curves in Fig. 2(a) - 2(d). Overall, CLWPO can scale to tasks with varying agents and significantly outperform baselines in Cooperative Pong and Pistonball tasks while performing comparably in the Pursuit task. This result is consistent with the data listed in Table 1, where the CLWPO agents stay longer in the screen and use fewer timesteps to move the ball to the left wall in the Cooperative Pong and Pistonball tasks. We attribute this to the CVB and HPO, where the former urges agents to learn an accurate latent world via efficient state representation of image observations, which, in turn, empowers the latter to optimize policies better. Notably, in the Pursuit task, we configured the number of pursuers to 6, evaders to 20, and pursuers that catch an evader to 3, significantly increasing the task difficulty. Furthermore, agents only receive sparse rewards when they encounter or catch the prey, which slows down policy learning efficiency, thus making all methods fail to capture all evaders before the 500-step limit and the improvement of CLWPO not obvious.

Then, we visualize the representations learned from pixel observations by CLWPO and MAMBA in Fig. 3, along with the reconstructed images of MAMBA agents. The figure shows that the CLWPO encoder, trained with the CVB objective, can effectively preserve task-relevant features, filter out irrelevant details, and tend to focus on the paddle agents, neighboring piston agents, and the balls. In contrast, the MAMBA encoder fails to capture crucial information about the paddle and adjacent piston agents, simultaneously introducing additional noise into the reconstructed images. These figures further highlight the limitations of reconstructionbased state representation learning methods, which prompt agents to retain task-irrelevant information, especially some scene details, in the representations of image observations.

Next, we provide a detailed view of the critical actions executed by CLWPO agents in the Cooperative Pong and Pistonball 5agents tasks to show the learned policies’ effectiveness in tackling tasks collaboratively in Fig. 4. In Fig. 4(a), both the paddle agents learn to predict the future movements of the ball and thus take appropriate actions to place themselves in locations where the ball is likely to fall, further increasing hitting chances. Likewise, in Fig. 4(b), the piston agents learn to coordinate their movements to sequentially roll the ball from the right edge of the screen to the left.

![](images/9bb460c802cb6584cbb10e636485dc47b5d562e1a717ca4c6361a6af43643458.jpg)

Figure 3: The spatial attention maps for each convolutional layer of the CLWPO (left) and MAMBA (right) agents’ encoders at a random intermediate episode around 1500 in the Cooperative Pong and Pistonball 5agents tasks.

![](images/442557880e07c0596342979547619d384844806f8cc974da5bd26c183077ab78.jpg)  
Figure 4: Critical action sequences of CLWPO agents in (a) Cooperative Pong and (b) Pistonball 5agents tasks.   
Figure 5: Results for ablation studies. (a)-(d) compares sequence lengths, initial rollout lengths $k$ , policy learning and representation learning methods, respectively.

# Ablation Studies

Finally, in Fig. 5, we investigate how CLWPO is affected by sequence lengths, initial rollout lengths $k$ , policy learning methods, and representation learning methods in the Pistonball 6agents task, with additional results and analysis in Appendix G. Specifically, Fig. 5(a) indicates that the CLWPO agents trained with an appropriate sequence length can learn predictive representations that capture the RL temporal structure well. Fig. 5(b) and Fig. 5(c) demonstrate that the adaptive rollout length $h$ derived from a well-chosen initial length $k$ efficiently balances the utilization of learned models and maintaining of policy performance. Furthermore, introducing auxiliary training signals of ground-truth interaction data generated through model-free MARL learning and setting $\alpha _ { H P O }$ to one to ensure that both the modelfree learning and model-based planning impact the training equally can help CLWPO agents acquire robust policies that can predict long-horizon behavior. Fig. 5(d) reveals that compared with reconstructing agents’ observation space, the contrastive learning-based CVB objective in CLWPO learns more efficient representation and thus performs better.

# Conclusions

In this paper, we introduce CLWPO, a multi-agent modelbased method for cooperative tasks with visual observations. CLWPO first designs a representation model to represent complex observations and facilitate model learning in the la

Pistonball 6agents Pistonball 6agents   
200 200   
150 150   
100 1   
50 0 CLWPO CLWPO   
-50 CLWPO-5sqq -50 CLWPO-5rlllut 0 400 8001200 1600 2000 0 400 800 1200 1600 2000 Episodes Episodes (a) Sequence lengths (b) Initial rollout lengths Pistonball 6agents Pistonball 6agents 200   
150 150   
100 50 1 CLWPO 0 CLWPO-fixed_roll   
-50 CLWPO-imag CLWPO-0.5-mf-learning CLWPO -50   
-100 CLWPO-0.8-mf-learning CLWPO-rec_obs 0 400 8001200 1600 2000 0 400 800 1200 1600 2000 Episodes Episodes (c) Policy learning (d) Representation learning

tent state space. Upon this model, it constructs a latent world and derives the CVB objective to optimize the world. Then, it develops the HPO scheme, combining model-free learning with model-based planning to acquire robust policies that predict future behaviors. In line with planning, it maintains a queue of teammate models and calculates an adaptive rollout length for each agent, enabling them to reduce the modelbased return discrepancy bound during self-imagination in the world. To evaluate CLWPO, we conducted extensive experiments, and the results show that CLWPO outperforms state-of-the-art MARL baselines, enhancing learning efficiency and improving asymptotic performance. In the future, we aim to combine CLWPO with advanced exploration methods and validate it across a broader range of cooperative, competitive, mixed, and realistic environments.

# Acknowledgments

This work was supported by the National Natural Science Foundation of China (No. 91948303).