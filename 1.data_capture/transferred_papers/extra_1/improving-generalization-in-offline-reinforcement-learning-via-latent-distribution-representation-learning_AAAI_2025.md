# Improving Generalization in Offline Reinforcement Learning via Latent Distribution Representation Learning

Da Wang1, Lin Li1, Wei Wei1\*, Qixian $\mathbf { Y } \mathbf { u } ^ { 1 }$ , Jianye $\mathbf { H a o } ^ { 2 }$ , Jiye Liang

1Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan, China 2College of Intelligence and Computing, Tianjin University, Tianjin, China $\{ { \mathrm { s x u } } _ { - } { \mathrm { w d } } , { \mathrm { y y } } 1 7 8 3 5 4 3 8 3 1 0 \} @ 1 6 3 . { \mathrm { c o m } }$ {lilynn1116, weiwei, ljy} $@$ sxu.edu.cn, jianye.hao@tju.edu.cn

# Abstract

Dealing with the distribution shift is a significant challenge when building offline reinforcement learning (RL) models that can generalize from a static dataset to out-of-distribution (OOD) scenarios. Previous approaches have employed pessimism or conservatism strategies. More recently, data-driven work has taken a distributional perspective, treating offline data as a domain adaptation problem. However, these methods use heuristic techniques to simulate distribution shifts, resulting in a limited diversity of artificially created distribution gaps. In this paper, we propose a novel perspective: offline datasets inherently contain multiple latent distributions, with behavior data from diverse policies potentially following different distributions and data from the same policy across various time phases also exhibiting distribution variance. We introduce the Latent Distribution Representation Learning (LAD) framework, which aims to characterize the multiple latent distributions within offline data and reduce the distribution gaps between any pair of them. LAD consists of a min-max adversarial process: it first identifies the “worst-case” distributions to enlarge the diversity of distribution gaps and then reduces these gaps to learn invariant representations for generalization. We derive a generalization error bound to support LAD theoretically and verify its effectiveness through extensive experiments.

# 1 Introduction

Offline reinforcement learning (RL) (Levine et al. 2020) enables the execution of safe and efficient learning utilizing pre-collected static datasets without the need for further interaction with the environment. Compared with its online counterpart (Sutton and Barto 2018), offline RL offers an attractive potential for saving resources and mitigating risks. The distribution shift represents one of the core challenges in offline RL, with out-of-distribution (OOD) (Fujimoto, Meger, and Precup 2019) data being not only grossly overestimated but also exacerbated through bootstrapping (Kumar et al. 2019), which can precipitate a catastrophic decline in performance. Rich experience has been accumulated in addressing the distribution shift issue, with strategies containing the implementation of policy constraints (Fakoor et al. 2021; Fujimoto and Gu 2021; Wu et al. 2022; Ran et al.

2023; Li et al. 2023), the adoption of conservative $Q$ values (Kumar et al. 2020; Kostrikov et al. 2021; Ma, Jayaraman, and Bastani 2021; Wang, Hunt, and Zhou 2022; Lyu et al. 2022), and the utilization of uncertainty estimation techniques (An et al. 2021; Bai et al. 2022; Wu et al. 2021).

Unlike the aforementioned methods that employ pessimism or conservatism, the alternative research (Qi et al. 2022; Wang et al. 2024) introduces a new way of thinking, conceptualizing offline data-driven decision-making as domain adaptation (Ben-David et al. 2010; Zhao et al. 2019). The primary goal is to ensure accurate predictions for the value of optimized decisions the “target domain” when training only on the provided dataset the “source domain”. IOM (Qi et al. 2022) addresses distribution shift by enforcing invariance between the learned representations of the training dataset and optimized decisions. Coincidentally, the recent ADS (Wang et al. 2024) framework employs dataset splitting to create simulations of distribution shifts, an endeavor grounded in the concept of domain adaptation. This framework is modeled as a min-max optimization challenge, which takes inspiration from meta-learning, and it enforces the model to achieve generalization over the distribution shifts simulated from the train/validation subsets splitting of the dataset. This novel approach, added to the line of offline RL algorithms, brings a fresh perspective to the field. However, the existing techniques still rely on heuristic designs for simulating distribution shifts. The diversity inherent in the distribution gaps they produce and the potential to extract richer information from the original offline datasets are issues deserving of deeper investigation.

With the goal of building models that can generalize across unknown distributions, this paper steers a different course from prior studies by introducing an innovative perspective: offline datasets inherently contain multiple latent distributions, with behavior data from diverse policies potentially following different distributions and data from the same policy across various time phases also exhibiting distribution variance (Figure 1). The former scenario emerges in particular cases, such as situations where the offline dataset arises from sampling various policies. On the other hand, the latter corresponds to the dynamically changing scenarios within time series, where such dynamic distributions exhibit diversity (Lu et al. 2023). We start by examining the connection between latent distributions and OOD

expert early period InD: high visitation frequency Policy PDF OOD: low visitation frequency R amin amax Policy PDF middle period medium last period amin Action amax amin Action amax amin Action amax (a) The distribution shift of state- (b) Diverse policies potentially (c) The same policy across various time action visitation frequency following different distributions phases exhibiting distribution variance

Figure 1: Abstract representation of our perspective: offline datasets inherently contain multiple latent distribution.

generalization. Following this examination, we introduce a framework, Latent Distribution Representation Learning (LAD), which subtly utilizes these latent distributions. The goal of LAD is to characterize naturally existing latent distributions in offline datasets and then learn generalizable models by reducing the distribution gaps between any pair of them. To be specific, the LAD framework works through an iterative min-max adversarial game: initially, it identifies and characterize the multiple latent distributions that have the maximum gap; subsequently, it bridges the gap between these distributions, thereby learning invariant representations that enhance the model’s ability to generalize. To validate the effectiveness of our proposed LAD framework, we offer theoretical insights accompanied by an extensive array of experiments. LAD can enhance the backbone agent’s performance across various continuous control tasks on the D4RL dataset, outperforming several state-ofthe-art offline RL algorithms. Additionally, we provide parameter analysis and ablation studies, along with visual evidence of LAD’s successful representation of latent distributions, which together robustly attest to the efficacy of LAD.

In summary, our contribution is four-fold:

• We introduce a novel data-driven perspective to address the distribution shift issue in offline RL: simulating OOD generalization scenarios with a higher diversity of distribution gaps, achieved by characterizing the multiple latent distributions inherently present in offline data.   
• We propose the Latent Distribution Representation Learning (LAD) framework, which consists of an iterative min-max adversarial game. LAD first identifies latent distribution representations in scenarios with the maximum distribution gap. It then works to reduce these gaps, thus learning invariant representations crucial for generalization.   
• We derive a generalization error bound for multiple distribution scenarios and provide theoretical insights to analyze the effectiveness of LAD.   
• By applying LAD to commonly used algorithms, we significantly enhance their performance and competitiveness. Importantly, LAD’s successful characterization of latent distributions validates our proposition that promoting the model to generalize across distribution gaps with maximum diversity is essential for offline learning.

# 2 Preliminaries

An infinite-horizon discounted Markov Decision Process (MDP) is denoted as $( S , A , P , r , \gamma )$ , where $S$ and $A$ are finite state and action spaces, $P ( s ^ { \prime } | s , a ) : S \times A \times$ $S \mapsto [ 0 , 1 ]$ is the state transition probability function, $r ( s , a ) ~ \mathrm { ~ : ~ } ~ \bar { S } \times A \ \mapsto ~ [ 0 , R _ { \operatorname* { m a x } } ]$ is the reward function, and $\gamma ~ \in ~ ( 0 , 1 )$ is the discount factor. RL aims to find an optimal policy $\pi ( \cdot | s )$ that maximizes the expected cumulative discounted reward $J ( \pi ) : = \mathbb { E } _ { \pi } [ \Sigma _ { t = 0 } ^ { \infty } \gamma ^ { \bar { t } } r ( s _ { t } , a _ { t } ) ]$ . A major approach, $Q$ -learning, which learns a $Q$ -function $\begin{array} { r } { Q ^ { \pi } ( s , a ) : = \mathbb { E } _ { \pi } \left[ \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } r \left( s _ { t } , a _ { t } \right) \vert s _ { 0 } = s , a _ { 0 } = a \right] } \end{array}$ to obtain the optimal policy. For a policy $\pi$ , the Bellman operator for iteratively updating the $Q$ -function as $\mathcal { T } Q ( s , a ) =$ $\mathbb { E } _ { s ^ { \prime } \sim P ( \cdot | s , a ) } [ r ( s , a ) \bar { + } \gamma \mathbb { E } _ { a ^ { \prime } \sim \pi ( \cdot | s ^ { \prime } ) } Q ( s ^ { \prime } , a ^ { \prime } ) ]$ .

In offline RL, where real-world online interaction is not practicable, the agent must learn a policy based on a static dataset $\boldsymbol { \mathcal { D } } \ = \ \{ ( s , a , r , s ^ { \prime } ) \}$ that was previously generated from an unknown behavior policy $\mu ( \cdot | s )$ . We assume that $( s , a )$ is generated i.i.d. from the data distribution $\mu$ in the training process. The learned policy, however, whose action $a ^ { \prime } \sim \bar { \pi } ( \cdot | s ^ { \prime } )$ utilized during the Bellman backup might lie outside the support of $\mu$ . Such a distribution shift between $\pi$ and $\mu$ leads to extrapolation errors due to the arbitrarily wrong estimation upon $a ^ { \prime }$ . Offline RL aims to obtain an optimal policy by leveraging solely the static dataset under the influence of distribution shift.

# 3 Our Method

In this section, we first examine the connection between latent distributions and OOD generalization. Then, we propose the Latent Distribution Representation Learning (LAD) framework, which subtly utilizes latent distributions and thus learns generalizable models. Finally, we theoretically derive a generalization error bound for multiple distribution scenarios and provide theoretical insight to understand our method.

# 3.1 Motivation

The distribution shift in offline RL refers to the disparity in the state-action visitation frequency between the learned policy and the dataset. An abstract illustration of the visitation frequency for In-Distribution (InD) data and Out-ofDistribution (OOD) data is depicted in Figure 1(a). In the latest work, ADS (Wang et al. 2024), the offline datasets are segmented to create simulated distribution shifts by establishing training/validation subsets that embody distribution gaps. Carrying forward the analysis of data distributions from ADS, we concentrate on a more innovative and realistic perspective: the offline datasets inherently contain multiple latent distributions, denoted as $\begin{array} { r } { \mathcal { P } ( x , y ) \dot { = } \sum \mathcal { P } _ { i } ( x , y ) } \end{array}$ ( $\mathcal { P } _ { i }$ is the $i$ -th latent distribution, $x$ is the input, and $y =$ $r + \gamma \operatorname* { m a x } _ { a ^ { \prime } } Q ( s ^ { \prime } , a ^ { \prime } )$ is the output of target $Q$ -network). For instance, some datasets, such as the medium-expert in the D4RL ( $\mathrm { F u }$ et al. 2020), have data collected from various policies, as depicted in Figure $1 ( \boldsymbol { \mathsf { b } } )$ , where each distinct policy corresponds to behavior data from a different distribution. Furthermore, the same policy can exhibit different behaviors across various time phases, thus resulting in distinct data distributions (such as the medium-replay in the D4RL), as shown in Figure 1(c). Well-studied offline datasets can generally satisfy one or both of the aforementioned scenarios and are equally applicable to unknown data distributions. This factor also highlights the challenge of building models capable of generalizing knowledge to OOD data.

![](images/0344bc8c0aa09700a2d3fc2b8b0bdc6d085099345a14e0c109d8dc051dfc9972.jpg)  
Figure 2: Illustration of the LAD framework. Details (a) corresponds to the Optimization in step 2 and (b) corresponds to the Fine-tune in the iterative process.

Leveraging the processing of data distributions to simulate distribution shifts, ADS (Wang et al. 2024) conducts an adversarial search to identify the splitting that exhibits the most significant distribution gap, subsequently employing meta-learning techniques to develop a generalizable model. In contrast, our approach involves characterizing the multiple latent distributions present within the dataset and seeking to minimize the “worst-case” scenario: an adversarial search targeting instances where the distribution gaps between any $\mathcal { P } _ { i }$ and $\mathcal { P } _ { j }$ are maximized. The advantage is that it fosters a model capable of handling a more diverse array of distribution gaps, thereby yielding a more comprehensive and robust model with enhanced generalization capabilities.

# 3.2 The LAD Framework

In this section, we introduce the LAD framework, whose core is to characterize multiple latent distributions followed by reducing the gaps between these distributions. Figure 2 describes the principal procedures of LAD, containing a data pre-processing phase (Step 1) and an iterative process: it initially identifies the “worst-case” distribution scenarios from the provided dataset (Step 2) and subsequently works to reduce the distribution gaps between each pair of them (Step 3).

Step 1 Pre-processing Directly characterizing latent distributions from an extensive array of raw offline datasets is inefficient. A practical approach is to handle a large batch of samples in each epoch (such as $M$ samples) (Wang et al. 2024), and then, we focus on characterizing the latent distributions for just these $M$ samples. Motivated by earlier studies (Sun, Zhou, and Li 2020; Wang et al. 2024), in our approach to create varied data distributions, we employ a Gaussian Mixture Model (GMM) (Bishop 2006) to cluster the state-action pairs, which yields $K$ initial latent distributions1.

Step 2 Latent Distribution Characterization This step aims to characterize the latent distributions and learn representations that maximize the distribution gaps, thereby enlarging diversity. For this process, we define a feature extractor $F ( s , a )$ , a specific distribution learner $L ( s , a )$ , a classifier ${ \cal C } ^ { ( 2 ) } ( s , a )$ for the latent distribution, and a $Q { \mathrm { . } }$ - network $Q ^ { ( 2 ) } ( s , a )$ . In Step 1, we employ GMM to initiate the $K$ latent distributions. This method is inherently a self-supervised approach to pseudo-labeling, effectively assigning an initial latent distribution $l$ $( l \in [ 1 , 2 , . . . , K ] )$ to each sample. With the representation serving as the output of $F ( s , \bar { a } )$ , we first determine the centroid of the current latent distribution:

$$
\tilde { \sigma } _ { k } = \frac { \sum _ { ( s , a ) \in \mathcal { D } } \eta _ { k } \bigl ( C ^ { ( 2 ) } ( L ( F ( s , a ) ) ) L ( F ( s , a ) ) } { \sum _ { ( s , a ) \in \mathcal { D } } \eta _ { k } \bigl ( C ^ { ( 2 ) } ( L ( F ( s , a ) ) ) \bigr ) } ,
$$

where $\tilde { \sigma } _ { k }$ is the initial centroid of the $k ^ { t h }$ latent distribution and $\eta _ { k }$ is the $k ^ { t h }$ element of the logit soft-max output. Based on the current centroid $\tilde { \sigma } _ { k }$ , we use a distance function $d$ to label the latent distribution through the nearest centroid classifier:

$$
\tilde { l } ( s , a ) = \arg \operatorname* { m i n } _ { k } d ( L ( F ( s , a ) , \tilde { \sigma } _ { k } ) .
$$

We then compute the accurate centroids:

$$
\sigma _ { k } = \frac { \sum _ { ( s , a ) \in \mathcal { D } } \mathbb { I } ( \tilde { l } ( s , a ) = k ) L ( F ( s , a ) ) } { \sum _ { ( s , a ) \in \mathcal { D } } \mathbb { I } ( \tilde { l } ( s , a ) = k ) } ,
$$

where $\mathbb { I }$ is the indicator function that returns 1 if and only if its argument is valid. The difference between Equation (3) and Equation (1) lies in the fact that the centroid $\tilde { \sigma } _ { k }$ , as computed by the latter, is subject to the influence of $\eta _ { k }$ , whereas Equation (3) provides a more accurate determination of the centroid (reflecting the divergence between the soft-max function and the output [0,1]). As a result, we can attain the updated latent distribution labels:

$$
l ( s , a ) = \arg \operatorname* { m i n } _ { k } d ( L ( F ( s , a ) , \sigma _ { k } ) .
$$

Once we attach the updated latent distributions $\{ l _ { i } \} _ { i = 1 } ^ { M }$ for the $M$ -selected samples, our subsequent task is to learn the representation that exhibits the maximal gaps within these distributions. Drawing inspiration from Ganin et al. (2016), we adopt an adversarial training approach, employing the following loss function:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { L A T } } + \mathcal { L } _ { \mathrm { G R L - R L } } = \mathbb { E } _ { ( s , a ) \in \mathcal { D } } \ell \left( C ^ { ( 2 ) } ( L ( F ( s , a ) ) ) , l \right) } \\ & { \qquad + \ell \left( Q ^ { ( 2 ) } ( \mathrm { G R L } ( L ( F ( s , a ) ) ) ) , y \right) . } \end{array}
$$

where $\ell$ is the cross-entropy loss and GRL is the gradient reverse layer (Ganin et al. 2016) facilitating adversarial training through reversing gradients. We use the loss $\mathcal { L } _ { \mathrm { L A T } } + \mathcal { L } _ { \mathrm { G R L - R L } }$ to optimize the network $Q ^ { ( 2 ) } , C ^ { ( 2 ) }$ , and $L$ , except for the feature extractor $F$ . Training the specific distribution learner $L$ results in a representation that, while it achieves separability of the latent distribution, fails to correct prediction this constitutes the worst-case scenario. We offer visualization results (Section 5.4) of features with distribution separation extracted by the distribution learner. The parameters of the feature extractor $F$ , which remain constant, are sustained for copy in forthcoming processes, while the distribution learner $L$ enters a fresh iteration.

Step 3 Invariant Representation Learning In the final phase, we use invariant representation learning to develop generalizable models, ensuring effective learning even in the worst-case scenarios. We define a bottleneck $\bar { B } ^ { ( 3 ) } ( s , a )$ , a new latent distribution classifier $C ^ { ( 3 ) } ( s , a )$ , and a $Q$ - network $Q ^ { ( 3 ) } ( s , a )$ . We still use adversarial training:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { R L } } + \mathcal { L } _ { \mathrm { G R L - L A T } } = \mathbb { E } _ { ( s , a ) \in \mathcal { D } } \ell \left( Q ^ { ( 3 ) } B ( ( F ( s , a ) ) ) , y \right) } \\ & { \qquad + \ell \left( C ^ { ( 3 ) } ( \mathtt { G R L } ( B ( F ( s , a ) ) ) ) , l \right) . } \end{array}
$$

In contradistinction to Equation (5), we utilize the gradient reverse layer to update the latent distribution classifier loss ${ \mathcal { L } } _ { \mathrm { G R L - L A T } }$ . This approach equips the learned model with the capability to address distributional gaps, thereby enhancing its generalizability. We set $Q$ -network $Q ^ { ( 3 ) }$ as the principal critic within our RL algorithm, serving the purpose of actor training.

Fine-tune the feature extractor. We emphasize that the feature extractor $F$ undergoes updates exclusively during the fine-tuning phase. The purpose of this step is to extract the finer-grained knowledge information embedded in the latent distribution and perform feature updating to obtain a fine-grained representation. This implies that the samples from the current latent distribution obtain a new representation from the updated feature extractor. We introduce a $Q$ -network $Q ^ { ( \mathrm { T U N E } ) }$ , a replica of $Q ^ { ( 3 ) }$ , to participate in the feature fine-tuning process. Concurrently, we refine the target $Q$ -function following the current latent distribution: $y ^ { \prime } = y + \delta \cdot l$ . Here, $l \in [ 1 , 2 , . . . , K ]$ , signifies the pseudolabel assigned to the latent distribution, a result derived from Step 2. The corresponding fine-tuning loss is:

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { T U N E } } = \mathbb { E } _ { ( s , a ) \in \mathcal { D } } \ell \left( Q ^ { ( \mathrm { T U N E } ) } ( F ( s , a ) ) , y ^ { \prime } \right) . } \end{array}
$$

To ensure the stability of the training process, it is also essential that the parameters of $Q ^ { ( 2 ) }$ in Step 2 are copied from the updated $Q ^ { ( 3 ) }$ in each subsequent iteration. More details on the iteration and training of the LAD framework can be found in Appendix A.

# 3.3 Theoretical Analysis

To facilitate a more profound grasp of our method, we derive a generalization error bound applicable to scenarios involving multiple distributions. Drawing on the principles of domain adaptation, we conceptualize the generalization challenge in offline RL as the crucial task of minimizing the model’s generalization error when trained on a source domain $\mathcal { P }$ and then evaluated on an unseen target domain $\mathcal { Q }$ . A specific premise is that the source domain $\mathcal { P }$ is a collection of multiple distributions $\{ \mathcal { P } _ { i } \} _ { i = 1 } ^ { K }$ . Our theory aims to elucidate the relationship between the generalization error $\epsilon _ { \mathcal { Q } }$ and the mitigation of the distribution gaps between any two $\mathcal { P } _ { i }$ and $\mathcal { P } _ { j }$ , facilitating an assessment of the efficacy of our LAD framework.

Theorem 3.1. Let $\mathcal { H }$ be a family of functions mapping from $X$ to $[ 0 , 1 ]$ , $\mathcal { Q }$ and the collection $\{ \mathcal { P } _ { i } \} _ { i = 1 } ^ { K }$ be distributions over $X$ . Given a set of distributions $\mathcal { O }$ , for $\forall { \mathcal { M } } \in { \mathcal { O } }$ and $\forall h \in { \mathcal { H } }$ , we have,

$$
\begin{array} { r l } & { \displaystyle \epsilon _ { \boldsymbol { \mathcal { Q } } } ( h ) \leq \sum _ { i } \epsilon _ { \mathcal { P } _ { i } } ( h ) + \frac { 1 } { 2 } \operatorname* { m i n } _ { \mathcal { M } \in \mathcal { O } } d _ { \mathcal { H } } ( \mathcal { M } , \boldsymbol { \mathcal { Q } } ) } \\ & { \quad \quad \quad \quad + \frac { 1 } { 2 } \operatorname* { m a x } _ { i , j } d _ { \mathcal { H } } ( \mathcal { P } _ { i } , \mathcal { P } _ { j } ) + \lambda ^ { \star } , } \end{array}
$$

Table 1: Results of different algorithms and the ones equipped with ADS, LAD. We bold the highest score.   

<html><body><table><tr><td>DATASET</td><td>TD3BC</td><td>TD3BC +ADS</td><td>TD3BC +LAD</td><td>MCQ</td><td>MCQ +ADS</td><td>MCQ +LAD</td></tr><tr><td>HALFCHEETAH-M</td><td>48.2 ± 0.5</td><td>49.0 ± 2.7</td><td>50.2 ± 1.3</td><td>62.5 ± 3.1</td><td>63.2 ± 2.7</td><td>66.4 ± 3.4</td></tr><tr><td>HOPPER-M WALKER2D-M</td><td>60.8 ± 3.4 84.4 ± 2.1</td><td>73.7 ± 13.0 85.0 ± 1.1</td><td>69.8 ± 3.7 87.1 ± 1.8</td><td>78.4 ±4.3</td><td>103.0 ± 1.5</td><td>103.6 ± 2.5</td></tr><tr><td></td><td></td><td></td><td></td><td>91.0 ± 1.1</td><td>94.5 ± 2.9</td><td>94.9 ± 2.1</td></tr><tr><td>HALFCHEETAH-MR HOPPER-MR</td><td>45.0 ± 0.5</td><td>46.1 ± 2.9</td><td>49.0 ± 1.5</td><td>56.2 ± 2.7</td><td>59.4 ± 3.1</td><td>61.0 ± 1.7</td></tr><tr><td>WALKER2D-MR</td><td>67.3 ± 13.2 83.4 ± 7.0</td><td>100.3 ± 2.2 91.3 ± 0.9</td><td>89.2 ± 11.3 93.6 ± 1.8</td><td>101.6 ± 1.0</td><td>105.0 ± 0.9</td><td>105.8 ± 1.8</td></tr><tr><td>HALFCHEETAH-ME</td><td></td><td></td><td></td><td>91.3 ± 1.8</td><td>96.1 ± 0.6</td><td>98.1 ± 2.3</td></tr><tr><td>HOPPER-ME</td><td>90.7 ± 2.7 91.4 ± 11.3</td><td>96.6 ± 3.1 114.0 ± 1.9</td><td>97.7 ± 2.4</td><td>80.1 ± 3.8</td><td>78.9 ± 0.5</td><td>101.5 ± 1.2</td></tr><tr><td>WALKER2D-ME</td><td>110.2 ± 0.3</td><td></td><td>112.0 ± 4.3</td><td>87.8 ± 2.0</td><td>105.8 ± 0.2</td><td>112.7 ± 2.0</td></tr><tr><td></td><td></td><td>114.0 ± 1.1</td><td>114.5 ± 1.5</td><td>114.2 ± 0.9</td><td>108.3 ±1.0</td><td>116.6 ± 1.3</td></tr></table></body></html>

where $\epsilon _ { \mathcal { Q } } ( h ) \ = \ \mathbb { E } _ { x \sim \mathcal { Q } } | h ( x ) - h ^ { * } ( x ) |$ and $h ^ { * }$ is an ideal function, $d _ { \mathcal { H } } ( \mathcal { P } , \mathcal { Q } )$ is the $\mathcal { H }$ -divergence2, which measures differences in distribution, $\lambda ^ { \star 3 }$ is the error of an ideal joint hypothesis for $\mathcal { P } , \mathcal { Q } .$ .

We provide the proof of Theorem 3.1 in Appendix B and proceed to elucidate how our LAD framework implicitly minimizes the rest of the terms in Equation (8), with the exception of the constant $\lambda ^ { \star }$ . The first term $\sum _ { i } \epsilon _ { \mathcal { P } _ { i } } ( h )$ corresponds to training samples across all distributions within the source domain $\mathcal { P }$ , the typical RL learning process that can be minimized with the loss ${ \mathcal { L } } _ { \mathrm { R L } }$ in Equation (6). For the second item, $\scriptstyle { \frac { 1 } { 2 } } \operatorname* { m i n } _ { { \mathcal { M } } \in { \mathcal { O } } } d _ { { \mathcal { H } } } ( { \mathcal { M } } , { \mathcal { Q } } )$ , the inaccessibility of the target domain $\mathcal { Q }$ prompts us to incorporate an intermediary variant $\mathcal { M } \in \mathcal { O }$ . To minimize $d _ { \mathcal { H } } ( \mathcal { M } , \mathcal { Q } )$ , therefore, directly associated with enlarging the range of $\mathcal { O }$ . Given the existence of a relationship such that $\begin{array} { r } { d _ { \mathcal { H } } ( \sum _ { i } \mathcal { P } _ { i } , \mathcal { M } ) \leq } \end{array}$ $\mathrm { m a x } _ { i , j } d _ { \mathcal { H } } ( \mathcal { P } _ { i } , \mathcal { P } _ { j } )$ , it is reasonable that we prioritize the maximization of $d _ { \mathcal { H } } ( \mathcal { P } _ { i } , \mathcal { P } _ { j } )$ under this circumstance in essence, seeking the “worst-case” scenario among the multiple latent distributions during Step 2. The third item $\begin{array} { r } { { \frac { 1 } { 2 } } \operatorname* { m a x } _ { i , j } d _ { \mathcal { H } } ( \mathcal { P } _ { i } , \mathcal { P } _ { j } ) } \end{array}$ measures the maximum gaps among source domains, which corresponds to Step 3 in LAD. The final term $\lambda ^ { \star }$ is frequently overlooked, as it is typically negligible in practice. To summarize our discussion, the LAD framework is related to minimizing the upper bound in Equation (8).

# 4 Related Work

With an increasing focus on the distribution shift challenges within offline RL in the community, diverse solutions have come to the forefront. In certain traditional conservative studies, learned policies are constrained by the dataset’s support to limit the production of OOD actions, with techniques including explicit policy constraints (Fakoor et al. 2021; Fujimoto and Gu 2021; Wu et al. 2022; Li et al. 2023), the penalization of value functions (Kumar et al. 2020; Ma, Jayaraman, and Bastani 2021; Wang, Hunt, and Zhou 2022), the application of uncertainty quantification (An et al. 2021; Bai et al. 2022; Wu et al. 2021), and the use of imitation learning as part of the approach (Chen et al. 2020; Kostrikov et al. 2021).

Expanding on the foundation applied by prior investigations, subsequent works have focused on refining the approach to mitigate overly conservatism, thus promoting improved generalization capabilities. MCQ (Lyu et al. 2022) actively training OOD actions by constructing them pseudo target values. STR (Mao et al. 2023) performs trust region policy optimization within the support of the behavior policy, relaxing the implicit density constraint of Exponentiated Advantage-Weighted Behavior Cloning (EAWBC) methods to a support constraint. Some data-driven work such as PRDC (Ran et al. 2023) find that regularizing the policy towards the nearest state-action pair can be more effective and allows the learned policy to choose actions that do not appear in the dataset along with the given state. Other work (Qi et al. 2022; Wang et al. 2024) innovatively models the OOD generalization problem for offline RL as domain adaptation from a distribution perspective. More recently, the ADS (Wang et al. 2024) framework simulates distribution shift by splitting the dataset into train/validation subsets, adversarially searching for the divisions with the largest distribution gaps, and then using meta-learning approach to learn a generalizable model. Inspired by ADS, we start from a novel perspective: characterizing multiple latent distributions that naturally exist in offline datasets and then training generalizable model by bridging the distribution gap of the “worst-case” distribution scenario. In contrast, our considerations are more macroscopic and comprehensive, and our method is able to characterize a greater diversity of distribution shifts, leading to better generalization.

For representation learning in offline RL, prior work (Yang and Nachum 2021; Xiao et al. 2022; Geng et al. 2022; Ma et al. 2023) mainly focuses on the implications of learned representations in TD-based methods. Yang and Nachum (2021) finds that pre-training and fixing the state representations can dramatically improve downstream learning. Geng et al. (2022) demonstrates that regularizing representation takes a more critical role in improving offline RL methods than merely imposing pessimism. Ma et al. (2023) provides an examination of the explicit representation distinction between in-sample and OOD state-action pairs for offline RL. In contrast, our approach is orthogonal to the above work. We introduce additional neural networks for feature extraction, enabling the learning of spaces where the latent distribution exhibits divisibility.

Table 2: Average normalized scores of different methods on the benchmark. We bold the highest mean.   

<html><body><table><tr><td>DATASET</td><td>BC</td><td>IQL</td><td>CQL</td><td>SPOT</td><td>POR</td><td>PRDC</td><td>STR</td><td>DIFFUSION -QL</td><td>CQL +ADS</td><td>MCQ +LAD</td></tr><tr><td>HALFCHEETAH-M</td><td>42.9</td><td>47.4</td><td>49.4</td><td>58.4</td><td>48.8</td><td>63.5</td><td>51.8</td><td>51.1</td><td>73.9</td><td>66.4</td></tr><tr><td>HOPPER-M WALKER2D-M</td><td>56.1</td><td>65.7</td><td>59.1</td><td>86.0</td><td>78.6</td><td>100.3</td><td>101.3</td><td>90.5</td><td>101.0</td><td>103.6</td></tr><tr><td></td><td>76.6</td><td>81.1</td><td>83.6</td><td>86.4</td><td>81.1</td><td>85.2</td><td>85.9</td><td>87.0</td><td>91.3</td><td>94.9</td></tr><tr><td>HALFCHEETAH-MR</td><td>36.6</td><td>44.2</td><td>47.0</td><td>52.2</td><td>43.5</td><td>55.0</td><td>47.5</td><td>47.8</td><td>49.6</td><td>61.0</td></tr><tr><td>HOPPER-MR</td><td>19.3</td><td>94.8</td><td>98.6</td><td>100.2</td><td>98.9</td><td>100.1</td><td>100.0</td><td>101.3</td><td>102.4</td><td>105.8</td></tr><tr><td>WALKER2D-MR</td><td>24.8</td><td>77.3</td><td>71.3</td><td>91.6</td><td>76.6</td><td>92.0</td><td>85.7</td><td>95.5</td><td>93.7</td><td>98.1</td></tr><tr><td>HALFCHEETAH-ME</td><td>53.1</td><td>88.0</td><td>93.0</td><td>86.9</td><td>94.7</td><td>94.5</td><td>94.9</td><td>96.8</td><td>93.5</td><td>101.5</td></tr><tr><td>HOPPER-ME</td><td>52.7</td><td>106.2</td><td>90.0</td><td>111.4</td><td>99.3</td><td>109.2</td><td>111.9</td><td>111.1</td><td>113.3</td><td>112.7</td></tr><tr><td>WALKER2D-ME</td><td>102.5</td><td>108.3</td><td>109.8</td><td>112.0</td><td>109.1</td><td>111.2</td><td>110.2</td><td>110.1</td><td>112.1</td><td>116.6</td></tr><tr><td>AVERAGE ABOVE</td><td>51.6</td><td>79.2</td><td>80.4</td><td>85.9</td><td>80.1</td><td>90.1</td><td>87.7</td><td>87.9</td><td>92.3</td><td>95.6</td></tr></table></body></html>

49 TD3BC+LAD 50 M 60 TD3BC+LAD TD3BC+LAD w/o L(s,a) TD3BC 8=0.01 E TD3BC δ=0.1 8=1 δ=2 8=3 Y 10 2 5 10 15 20 0 200 400 600 800 1000 0 The values of K Timesteps Methods (a) Parameter K (b) Parameter δ (c) Ablation study

# 5 Experiments

In this section, we conduct experiments to validate the effectiveness of LAD in terms of performance, hyperparameter robustness, ablation study, and visualization of latent distribution characterization using the D4RL benchmark (Fu et al. 2020). For simply, we abbreviate the names of the datasets from MEDIUM, MEDIUM-REPLAY, MEDIUM-EXPERT $\}$ to $\{ \mathtt { M } ,$ MR, $\mathtt { M E } \}$ in all the tables. Regarding the algorithms relevant to our comparison, baseline results at the 1M gradient step are procured either by re-running the official code or directly extracting from the original papers. Our algorithm, in turn, is tested across five random seeds, with the reported findings reflecting the mean normalized results garnered from the last ten evaluations.

# 5.1 Performance

Application to strong baseline algorithms. Our LAD, a highly effective data-driven framework, can effectively integrate with strong baseline algorithms. Inspired by ADS (Wang et al. 2024), we combine LAD with TD3BC (Fujimoto and $ { \mathrm { G u } } 2 0 2 1 \$ ) and MCQ (Lyu et al. 2022), then compare with the baselines and ADS. Table 1 shows that LAD boosts the performance of the baselines and surpasses ADS in most cases. Furthermore, the results reveal a noteworthy phenomenon: our method exhibits superior performance on the -MR and -ME datasets. This enhanced performance can be attributed to the greater diversity of the latent distribution, which arises when the dataset contains a richer mix of samples. In this situation, our LAD framework can make full use of the favorable diversity to train models with better generalization, thereby yielding superior performance. This result aligns with our expectations and confirms the effectiveness of our approach. More results equipped with CQL (Kumar et al. 2020) are shown in Appendix C.

Comparison with related methods. We opt for the representative algorithm that demonstrates the most outstanding performance, MCQ+LAD, and compare it against other related classical algorithms as well as those designed to enhance generalizability, including BC, IQL (Kostrikov et al. 2021), CQL (Kumar et al. 2020), SPOT (Wu et al. 2022), POR (Xu et al. 2022), PRDC (Ran et al. 2023), DIFFUIONQL (Wang, Hunt, and Zhou 2022), and $\mathrm { C Q L + A D S }$ (Wang et al. 2024). Table 2 exhibits the outcomes of different methods within representative gym environments. The results indicate that our method is not only competitive in the majority of cases but also has a distinct advantage in the average performance, providing reassurance of its effectiveness. Additional evidence on other tasks is provided in Appendix C.

# 5.2 Hyperparameter Robustness

The key parameters that define our LAD framework include the pre-processing sample size $M$ , the initial latent distribution count $K$ , and the fine-tuning parameter $\delta$ . Recognizing that $M$ affects the computational resources for adjusting the latent distribution labels $l$ , we assign a conservative value, such as $M = 5 0 0 0$ . An exhaustive list of parameters is listed in Appendix C. Next, we primarily analysis $K$ and $\delta$ .

![](images/47eb374e6beff6bac64672e33cae0bfe9469a43b20195f6ccaa80d5751e52079.jpg)  
Figure 4: The t-SNE visualization of latent distribution characterization by LAD.

The effect of the initial latent distribution count $K$ . The $K$ is a noticeable and crucial parameter in our LAD framework. Typically, the larger the value of $K$ , the fewer samples constituting each latent distribution, which can lead to an increase in the discrepancy of data points within each latent distribution, while the gaps between the latent distributions decrease as their boundaries become less distinct. We report the results of TD3BC $+ { }$ LAD on HALFCHEETAH-MR across various $K$ in Figure 3(a). Observations indicate that the performance trend corresponding to the $K$ aligns fundamentally with our preceding expectations. Consequently, we choose $K = 5$ as an appropriate selection.

The effect of the fine-tuning parameter δ. The parameter $\delta$ denotes the extent of fine-tuning the feature extractor. Figure 3(b) illustrates the performance of TD3BC+LAD on HALFCHEETAH-MR across various $\delta$ . The results show that LAD is more robust to $\delta$ . A natural consideration is whether omitting the use of the latent distribution information for fine-tuning the feature extractor would lead to a decline in performance. For this reason, we conduct an ablation study for the fine-tuning process in the following subsection.

# 5.3 Ablation Study

We perform an ablation study over the components in our method. First, we evaluate the significance of the latent distribution learner, which comes into play during Step 2. Figure 3(c) indicates that removing the distribution learner (w/o $L ( s , a ) )$ results in a degradation in performance. That is because the role of the latent distribution learner is to act as an intermediate network that records the worst-case features learned, and learning directly through the feature extractor is problematic. We then verify the importance of using Equation (7) for fine-tuning during the iterations after Step 3 through ablation experiments. Using the original target $y$ in place of $y ^ { \prime }$ in Equation (7) to update the feature network could skew the representation towards a $Q$ -predictive bias, preventing the acquisition of the updated latent distribution features. This ultimately leads to suboptimal performance as depicted in Figure 3(c) (w/o Fine-tune). Hence, it is crucial to fine-tune the feature extractor with fine-grained latent distribution information. By doing so, we can align the representation to prioritize features associated with the latent distribution, thus facilitating a new round of iterations.

# 5.4 Visualization

To further verify the efficacy of LAD, we offer the tSNE (der Maaten and Hinton 2008) visualization, representing the latent distribution’s successful characterization by LAD, as shown in Figure 4. The original dataset of HALFCHEETAH-MR is displayed in Figure 4(a). Following that, Figure 4(b) illustrates the data distribution after the pre-processing phase, which presents an initial appearance of a multiple latent distribution by clustering with GMM. In Figure 4(c), we observe the data distribution characterized by LAD in Step 2, showcasing a maximization of distribution gaps. This visualization is derived from the feature extraction enabled by the distribution learner $L ( s , a )$ . Evidently, LAD successfully identifies latent distributions with a remarkable diversity, thereby bolstering generalization. In Figure 4(d), we present the results of using the bottleneck $B ^ { ( 3 ) } ( s , a )$ in Step 3 to record the feature of inseparable latent distribution. Compared to the above visualization results, our LAD successfully characterizes the diverse latent distributions and effectively utilizes them, which supports the efficacy of LAD.

# 6 Conclusion

In this paper, we introduce a novel data-driven perspective for improving the generalization in offline RL. The principle is to characterize the multiple latent distributions that inherently exist in offline data and enable models to bridge distribution gaps, thereby enhancing generalization. We present the Latent Distribution Representation Learning (LAD) framework, which first identifies “worst-case” distributions to enlarge the diversity of distribution gaps and then reduces these gaps to learn invariant representations. We provide the theoretical insights behind LAD, demonstrating its ability to implicitly minimize the upper bound on the generalization error of offline RL in multi-distribution scenarios. Our extensive experiments convincingly support the efficacy of LAD. However, the iterative nature of our method currently limits its scalability to large data in the preprocessing phase. This limitation underscores the need for further research. In our future work, we plan to explore the use of the diffusion model for yielding diverse trajectories and fully leverage our LAD framework. We believe that our unique perspective on data distribution will stimulate further community discussion and inspire new approaches to RL.