# Stop Diverse OOD Attacks: Knowledge Ensemble for Reliable Defense

Zhenbo Shi1,2,3,4, Xiaoman Liu1,2, Yuxuan Zhang1, Shuchang Wang1, Rui $\mathbf { S h u } ^ { 1 }$ , Zhidong $\mathbf { Y } \mathbf { u } ^ { 1 , 4 , * }$ , Wei Yang1,2,4,\*, Liusheng Huang1,2

1 School of Computer Science and Technology, University of Science and Technology of China, Hefei, China   
2 Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China 3 Laboratory for Advanced Computing and Intelligence Engineering, Wuxi, China 4 Hefei National Laboratory, University of Science and Technology of China, Hefei, China \* yuzd $@$ mail.ustc.edu.cn, qubit $@$ ustc.edu.cn

# Abstract

Enhancing defense through model ensemble is an emerging trend, where the challenge lies in how to use ensemble knowledge to counter Out-of-Distribution (OOD) attacks. In this paper, we propose the Reliable Defense Ensemble (REE) to address this issue. REE optimizes the ensemble knowledge of models through aggregation and enhances multidimensional robust performance through collaboration. It employs the Dynamic Synergy Amplification for weight allocation and strategy adjustment. Furthermore, we design a new Kernel Anomaly Smoothing Detection Module, which detects anomalous attacks using a smoothing feature function based on Gaussian kernel mean embedding and a multi-layer feedback structure. Particularly, we build a framework that uses reinforcement learning to iteratively fine-tune the parameters of inter-model communication and consensus. Extensive experimental results show that REE outperforms current state-ofthe-art methods by a large margin in defending against OOD attacks.

# Introduction

Background and Related Work. Deep learning is developing rapidly (Ge, Fu, and Zha 2022; Ge et al. 2024; Shi et al. 2022), but the emergence of adversarial attacks poses huge challenges. As Szegedy et al. (2013) proved that even imperceptible perturbations in the input can mislead a model. Papernot et al. (2016) further exploits the adversarial saliency between input features and output values, which cause misclassification with slight modifications. In the study of defense strategies, Goodfellow, Shlens, and Szegedy (2014) first propose adversarial training and put forward new ideas for model robustness. However, as Trame\`r et al. (2017) highlight, the adaptive and continuously evolving nature of attacks necessitates more sophisticated and resilient defense strategies. Understanding the vulnerable parts of models and designing new defense strategies is critical to maintaining the integrity and trustworthiness of AI systems.

Distribution anomalies pose a strong robustness challenge to the model. While poisoning attacks typically affect the training phase, and our work centers on inference-stage defenses, research on poisoning defense has indeed provided valuable insights for our approach. BagFlip (Zhang, Albarghouthi, and D’Antoni 2022) combines bagging and noise technology to combat trigger-free and backdoor attacks, and uses the Neyman–Pearson lemma to calculate the authentication radius. In response to the problem that NLP models are vulnerable to backdoor attacks, Shen et al. (2022) proposed an optimization method to reverse the backdoor trigger, as well as a dynamic temperature scaling and rollback mechanism. Random Transformation defense (RT) uses a differentiable method to resist adversarial attacks, and utilizes transfer attacks and the random optimization algorithm (Sitawarin, Golan-Strieb, and Wagner 2022). Ho and Vasconcelos (2022) proposed an image classification adversarial defense DISCO based on local implicit functions, which performs projection and conditional modeling on local manifolds. In addition, current integrated defense approaches remain vulnerable to new, cleverly crafted attack vectors that can dynamically deceive defense strategies. For example, these works (Seligmann et al. 2024; Croce et al. 2022) have conducted extensive and in-depth research on ensemble models.

Adaptive defense is considered an important step to combat the limitations of static defense, Croce et al. (2022) evaluated nine different adaptive defense methods, showing that their performance relative to static defense often does not significantly improve, and sometimes even decreases the robustness of the model. Model stealing attack gradient redirection (GRAD2) (Mazeika, Li, and Forsyth 2022) can maintain high security while maintaining service quality for normal users. In reinforcement learning, Bharti et al. (2022) proposed provable defense methods against backdoor strategies, using the concept of safe subspaces to eliminate the impact of backdoor triggers. Considering the impact of different normalization layers on attack migration performance, Dong et al. (2022) designed Random Normalization Aggregation (RNA) based on this. This method forms a huge random space by aggregating multiple normalization methods to increase the difficulty of being attacked in a white-box setting.

LeadFL (Zhu, Roos, and Chen 2023) focuses on solving the problem of resisting model poisoning attacks in federated learning, and introduces regularization on the client side to face sudden and highly changing malicious attack patterns. This mechanism can be used in conjunction with any existing server-side defense strategy, and was compared with algorithms such as SparseFed, Multi-Krum, and Bulyan under different data distributions (IID and non-IID) and attack modes. NNSplitter (Zhou et al. 2023b) provides defense for DNN models through automated weight obfuscation. Zhou et al. (2023a) proposed the adversarial defense strategy PAD from the phase perspective of the image. This strategy improves the robustness of the model in the face of attacks through phase-level adversarial training and amplitude-based preprocessing operations. Reconstructive neuron pruning emphasizes asymmetric unlearning and recovery methods that can effectively expose and prune backdoor neurons with minimal clean data (Li et al. 2023).

Our Aim and Contributions. This paper aims to address the challenge of how to use ensemble knowledge to counter OOD attacks. To achieve this, we propose the Reliable Defense Ensemble (REE) to tackle this issue. Our approach focuses on two innovations within REE: Dynamic Synergy Amplification (DSA) and the Kernel Anomaly Smoothing Detection Module (KASD). DSA optimizes the ensemble knowledge of models and enhances multidimensional robust performance through collaborative knowledge. It uses iterative reinforcement for weight allocation and strategy adjustment, strengthening the overall resilience of the model group. KASD detects anomalous attacks by using a smoothing feature function based on Gaussian kernel mean embedding and a multi-layer feedback structure. Particularly, we build a framework that uses reinforcement learning to iteratively fine-tune the parameters for communication and consensus among models.

To summarize, we make the following contributions:

• We propose the Reliable Defense Ensemble (REE), which uses aggregation and collaboration to enhance the robustness of models and uses these methods to address the challenge of using ensemble knowledge to counter OOD attacks.   
• We design the Dynamic Synergy Amplification (DSA), which implements weight allocation and strategy adjustment through a quantified threat index to avoid the problem of individual models being unable to effectively share critical knowledge.   
• We put forward the Kernel Anomaly Smoothing Detection Module (KASD), which uses a smoothing feature function based on Gaussian mean embedding for anomaly detection and employs a multi-layer feedback structure to identify new attacks.

# Defense Principles

Theoretical Framework. Let M represent the feature space of a single model and $\mathbf { X }$ denote the input space. For a given model $\mathbf { M } \in \mathcal { M }$ $( \mathcal { M } = \{ \mathbf { M } _ { 1 } , \mathbf { M } _ { 2 } , \cdot \cdot \cdot , \mathbf { M } _ { n } \} )$ and an input $\mathbf { X } \in { \mathcal { X } }$ , an adversarial vulnerability $\nu ( \mathbf { M } , \mathbf { X } )$ is defined as one that can generate adversarial examples to mislead M. $\rho ( \mathcal { M } )$ represents the degree of robustness of the model group to attacks. The synergy score $\boldsymbol { s }$ of a model group $\mathcal { M }$ is defined as a function of the robustness measure and interaction dynamics of each model, i.e. $S ( \mathcal { M } ) = \Phi ( \{ \rho ( \mathbf { M } _ { i } ) \} _ { i = 1 } ^ { n } , \mathcal { T } )$ , where $\Phi$ is an aggregation function encapsulating the collective defense strategy, and $\boldsymbol { \tau }$ represents the information interaction between models. Furthermore, Dynamic Synergy

Amplification (DSA) is applied to the model group $\mathcal { M }$ as a series of operations $\mathbf { D }$ to dynamically enhance its collective resilience. For a given input $x$ , the output of the model under DSA is given by $\begin{array} { r } { \mathbf { D } ( \mathbf { M } , \mathbf { \bar { X } } ) = \sum _ { i = 1 } ^ { n } \mathbf { \bar { \Lambda } } p _ { i } \mathbf { M } _ { i } ( \mathbf { X } ) } \end{array}$ , where $p _ { i }$ is the dynamic weight. It reflects he contribution of model $\mathbf { M } _ { i }$ based on the current threat situation and network collaboration. In the Kernel Anomaly Smoothing Detection Module (KASD), $\mathcal { F } ( \mathcal { H } )$ functions as a strategic prediction map for defense, using a smooth feature function based on Gaussian kernel mean embedding, all informed by historical data $\mathcal { H }$ .

Architectural Overview. The core of REE is a collection of models (as shown in Fig. 1), each of which has a specific Elastic Morphism Mapping $\mathcal { R } ( \cdot )$ that maps the model response to the adversarial feature space. The process is formalized as follows, where $\mathcal { R } ( \mathbf { M } _ { i } )$ represents the elastic embedding of model $\mathbf { M } _ { i }$ in the network, and the form of the comprehensive model elasticity $\mathbf { R } _ { \mathrm { n e t } }$ is as $\mathbf { R } _ { \mathrm { n e t } } = \mathbf { D } ( \mathbf { r } ( \mathbf { M } _ { i } ) , \mathbf { X } )$ . This design confronts the non-differentiable nature of adversarial landscapes, acknowledging the resilience of individual models, as well as the elasticity of entire ensembles, all existing within a complex, often discrete space. The convex hull on the feature space is defined as $\begin{array} { r } { \Omega = \dot { \sum } _ { i = 1 } ^ { n } \pmb { \alpha } _ { i } \cdot \mathcal { R } ( \mathbf { M } _ { i } ) } \end{array}$ , where $\textstyle \sum _ { i = 1 } ^ { n } \alpha _ { i } = 1 , \alpha _ { i } \geq 0$ . Therefore,any collaborative strategy $\boldsymbol { S } _ { t }$ at time $t$ can be represented by a convex hull: $\Omega ( \pmb { S } _ { t } ) =$ $\begin{array} { r l } { \sum _ { i = 1 } ^ { n } \pmb { \alpha } _ { i , t } \cdot \pmb { \mathcal { R } } ( \mathbf { M } _ { i } ) } & { { } } \end{array}$ where $\begin{array} { r } { \sum _ { i = 1 } ^ { n } \dot { \pmb { \alpha } } _ { i , t } = 1 , \pmb { \alpha } _ { i , t } \ge 0 . } \end{array}$ . Following the method in (Chen, Ding, and Carin 2015; Dong et al. 2021), we use SoftMax transformation on the weight $\alpha _ { i , t }$ to ensure that the dynamic policy obeys the convex combination constraint.

# Approach

# Dynamic Synergy Amplification

Synergy Scoring. We treat the model group as a whole, and the Synergy Scoring is crucial to deal with the multifaceted entities of the attack in Dynamic Synergy Amplification (DSA). The probability that any model $\mathbf { M } _ { i }$ is the best fit for a given challenge is uniformly assumed to be $\mathbf { V } ( \mathbf { M } _ { i } ) = 1 / N$ , where $N$ is the total number of models.

Given the complexity of adversaries, estimates of individual model responses are insufficient. Therefore, we propose a method similar to the high-dimensional estimators (Ziegel 2003; Zou and Hastie 2005; Cheng, Diakonikolas, and Ge 2019; Diakonikolas et al. 2019) in robust statistical theory. We define $n _ { i } ( \mathbf { T } )$ as the participation count of model $i$ up to time T. DSA operates by continuously analyzing incoming threats in real-time and dynamically adjusting the distribution of weights across the model ensemble. This adjustment is based on a quantified threat index, which measures the severity and type of incoming adversarial attacks. Each model’s weight is recalibrated using a softmax function over the threat index, ensuring that models better suited to countering the current attack have increased influence in the ensemble’s decision-making process. The synergy vector $\hat { \varphi } _ { i }$ of all models is calculated as follows:

$$
\hat { \varphi } _ { i } = \frac { 1 } { n _ { i } ( \mathbf { T } ) } \sum _ { t : \mathbf { M } _ { t } = i } \gamma _ { t } \cdot \left( \pmb { v } _ { ( d ( \mathbf { M } _ { t } ) , a _ { t } ) } + \left\| \pmb { \theta } _ { \triangle } \right\| ^ { 2 } \right) \cdot \left\| \pmb { \theta } _ { \nabla } \right\| ^ { 2 }
$$

![](images/81f0fb9a5fbbdd0ba034998f5a460c6a94ce186ef94a540e4a8b2c1e5b7abbe4.jpg)  
Figure 1: SubFig. (a) illustrates the initial performance of the model group on training dataset-similar and OOD data, showing the challenges posed by adversarial examples and increased distribution differences. SubFig. (b) shows the improvement in model recognition ability after robust training of the model cohort in REE. The simplified pipeline of DSA, KASD, and REE is shown in SubFig. (c).

where $\gamma _ { t }$ represents the elasticity measure observed at time $t$ , $d ( \mathbf { M } _ { t } )$ represents the defense strategy adopted by model $\mathbf { M } _ { t }$ , $\pmb { v } _ { ( d , a ) }$ are basis vectors specified in the feature space. $\pmb \theta _ { \triangle } = \mathbf { \dot { \theta } } _ { t } - \pmb \theta _ { \mathrm { a v g } }$ and $\pmb { \theta } _ { \nabla } = \pmb { \theta } _ { t } - \pmb { \theta } _ { \mathrm { p r e v } }$ , ${ \pmb \theta } _ { t }$ represents the parameter vector of the model at time $t$ , $\pmb { \theta } _ { \mathrm { a v g } }$ is the historical average vector of model parameters, and $\pmb { \theta } _ { \mathrm { p r e v } }$ is the model parameter at the previous time point. Furthermore, the first and second moments of the random quantity $\hat { \varphi } _ { i }$ are expected to be closely consistent with the true cometric, ensuring that $\mathbb { E } \left[ \hat { \varphi } _ { i } \right]$ is close to the actual synergy, while the covariance Cov $( \hat { \varphi } _ { i } )$ remains tightly coupled.

To robustly assess collective network synergies, we employ a multidimensional robust estimator. This estimator treats the set of $\{ \hat { \varphi } _ { i } \} _ { i = 1 } ^ { \mathbf { M } }$ as a sequence of observations derived from the network’s interaction with adversarial instances. Assuming that the adversarial environment changes slowly enough for the network to adapt, and that the contribution of each model is not negligible, the estimated synergy score $\hat { \pmb { \sigma } }$ converges to the true synergy value $\mathrm { ~ T ~ } {  } \infty$ , ensuring the collective integrity of the model.

Adjusting Individual Model Weights. We first define a recalibration function that dynamically adjusts the weight of each model’s contribution based on each model’s performance and adversarial environment. Let $\beta _ { i , t }$ represent the weight of the $i _ { t h }$ model at time $t$ , and let $\mathcal { A }$ represent the current data distribution. The recalibration function is defined as:

$$
\beta _ { i , t + 1 } = \beta _ { i , t } \cdot \varrho ( \ell _ { i , t } , \boldsymbol { \mathcal { A } } _ { t } ) \cdot \operatorname* { m i n } ( \eta , \operatorname { t a n h } ( - ( \boldsymbol { \mathcal { A } } _ { t } \boldsymbol { \diamondsuit } \boldsymbol { \mathcal { A } } _ { t - 1 } ) ) )
$$

where $\ell _ { i , t }$ is the loss generated by the $i _ { t h }$ model at time $t$ , and $\eta$ is an adjustable parameter. $\varrho ( \ell _ { i , t } , \mathcal { A } _ { t } )$ is a weight adjustment function, and $\mathbf { \mathcal { A } } _ { t } \bigotimes \mathbf { \mathcal { A } } _ { t - 1 }$ represents the difference between the current data distribution $\mathbf { \mathcal { A } } _ { t }$ and the previous time point data distribution $\mathbf { \mathcal { A } } _ { t - 1 }$ . This feature ensures that models that perform well under the current adversarial strategy are more prominent in the ensemble.

Policy Alignment and Iterative Reinforcement. To align the high-dimensional policies of the model population with the adversarial context, we define a policy vector $\mathbf { s } _ { i }$ for each model. The alignment process involves projecting these vectors onto a recalibrated feature space. This is achieved by using a projection matrix $\mathbf { P } _ { t }$ that evolves over time $\mathbf { s } _ { i } ^ { ( t + 1 ) } = \mathbf { P } _ { t } \cdot \mathbf { \bar { s } } _ { i } ^ { ( t ) } . \mathbf { P } _ { t }$ si( . Pt is calculated based on the current adversarial environment and the historical performance of the strategy, ensuring that the recalibrated strategy is optimal against current and foreseeable threats. The recalibration process involves iterative cycles of unlearning and reinforcement. During the unlearning phase, strategies considered less effective or compromised are weakened:

$$
\mathscr { U } ( { \mathbf { s } } _ { i } ^ { ( t ) } ) = \varsigma \cdot { \mathbf { s } } _ { i } ^ { ( t ) } \cdot \exp ( \varrho ( \ell _ { i , t } , \mathcal { A } _ { t } ) ) \cdot \mathbf { 1 } _ { \{ \ell _ { i , t } > \tau \} }
$$

where $\varsigma \in ( 0 , 1 )$ is the attenuation factor, $\tau$ is the loss threshold. Strategies with losses greater than $\tau$ will be scaled down. In contrast, during the reinforcement phase, strategies that demonstrate effectiveness are strengthened:

$$
\pmb { \mathcal { B } } \left( \mathbf { s } _ { i } ^ { ( t ) } \right) = \pmb { \kappa } \cdot \log ( 1 + | | \mathbf { s } _ { i } ^ { ( t ) } | | ^ { 2 } ) \cdot \mathbf { s } _ { i } ^ { ( t ) } \cdot \mathbf { 1 } _ { \{ \ell _ { i , t } \leq \tau \} }
$$

where $\kappa$ is the strengthening factor, and this design can encourage individual models to share critical effective knowledge.

# Kernel Anomaly Smoothing Detection

The proposed Kernel Anomaly Smoothing Detection Module (KASD), which uses a smoothing feature function based on Gaussian mean embedding for anomaly detection and employs a multi-layer feedback structure to identify new attacks

Kernel-Based Anomaly Detection. KASD employs a kernelbased anomaly detection approach. Specifically, we improve the Gaussian Kernel-based Maximum Mean Difference (GK-MMD) measure on this model group robustness task, $\mathbf { A } ^ { ( \mathrm { G } ) } ( \cdot , \cdot k ^ { ( \mathrm { G } ) } )$ , quantifying the difference between historical and current adversarial distribution (Gretton et al. 2012; Sutherland et al. 2016). The Gaussian kernel is defined as $\begin{array} { r } { \pmb { k } ^ { ( \mathrm { G } ) } ( x , y ) = \exp \left( - \frac { | x - y | ^ { 2 } } { 2 \sigma _ { \phi } } \right) } \end{array}$ , where $\sigma _ { \phi }$ is a learnable length scale parameter. This approach effectively captures subtle changes in adversarial environments.

To capture more effective patterns in adversarial strategies, KASD incorporates Deep Kernel Learning (DKL) (Wilson et al. 2016; Liu et al. 2020). This approach combines traditional kernel methods with deep learning to create more expressive feature representations. The depth kernel is defined as $k ^ { \mathrm { ( D ) } } ( x , y ) = ( 1 - \mu ) \cdot \exp \left( - | \phi ( x ) - \phi ( y ) | ^ { 2 } / 2 \sigma _ { \phi } \right) +$ $\pmb { \mu } \cdot \exp \left( - | x - y | ^ { 2 } / 2 \pmb { \sigma } _ { q } \right)$ , where $\mu , \sigma _ { \phi } , \sigma _ { q }$ are learnable parameters, and $\phi ( \cdot )$ represents the deep neural network feature extractor. This approach enables KASD to detect and understand adversarial behavior.

The model swarm also employs sign (C2STS) and confidence (C2ST-L) classifier two-sample testing (C2ST) to differentiate between benign and adversarial inputs (Gretton et al. 2012; Lopez-Paz and Oquab 2016; Cheng and Cloninger 2022). These tests use a classifier to determine whether two distributions are different. For C2ST-S, kernel $k _ { \mathrm { C } \setminus } ( x , y ) ~ = ~ { \frac { 1 } { 4 } }  { \mathbb { k } } ( f ( x ) ~ > ~ 0 )  { \mathbb { k } } ^ { } ( f ( y ) ~ > ~ 0 )$ utilizes the symbols output by the classifier, while C2ST-L uses $k _ { \mathrm { C - L } } ( x , y ) = f ( x ) f ( y )$ , focusing on The confidence of the discriminator. These techniques enable KASD to effectively distinguish between normal model behavior and adversaryinfluenced behavior.

Optimized Frequency Domain Analysis. To further enhance the detection capability, KASD uses a Smooth Feature Function (SCF) based on Gaussian kernel mean embedding to analyze adversarial strategies in the frequency domain (Sriperumbudur et al. 2010; Fukumizu et al. 2009; Jitkrittum et al. 2016). This approach $\mathbf { Q } _ { \mathrm { S C F } } ( \cdot , \cdot )$ provides a new perspective on the structure and evolution of adversarial attacks, allowing the detection of high-dimensional patterns that may otherwise go unidentified.

KASD is integrated into the model population of defensive postures through a multi-layered feedback system that continuously updates and informs Dynamic Synergy Amplification (DSA). Integrating predictive components into defense strategies can significantly enhance their adaptability and robustness, ensuring that the foresight provided by KASD is effectively translated into viable defense strategies. In our framework, KASD’s predictive insights are encoded as adjustments to the model synergy score, recalibrating the collective defense posture in real time $\hat { \varphi } _ { t + 1 } =$ $\hat { \varphi } _ { t } + \eta _ { 2 } \cdot \nabla _ { \hat { \varphi } } \mathcal { L } \left( \mathcal { F } \left( \mathcal { H } _ { t } \right) , \mathcal { A } _ { t } \right)$ , where $\hat { \varphi } _ { t }$ is the current collaboration score, $\eta _ { 2 }$ is the learning rate, $\mathcal { L }$ is the loss function that measures the deviation between the predicted and actual adversarial modes, $\mathcal { F }$ is the look-ahead function of KASD, $\mathcal { H } _ { t }$ is the currently collected historical attack data, and $\mathbf { \mathcal { A } } _ { t }$ is the current data distribution.

KASD’s proactiveness is achieved through continuous monitoring and analysis of adversarial patterns to detect and interpret the slightest signs of emerging threats. Inspired by the concepts of active learning and early warning systems (Settles 2011; Quansah, Engel, and Rochon 2010), KASD employs a series of anomaly detection and pattern recognition algorithms to identify potential threats before they arise. Analyzing the spectral properties of adversarial data distributions for early detection of coordinated attacks:

$$
\mathcal { W } = \sum _ { i = 1 } ^ { n } \lambda _ { i } \cdot \left( 1 + \frac { { \left| { \left| { { { \bf { \bar { v } } } _ { i } } } \right| } \right| } + { { \bf { v } } _ { i } ^ { \mathrm { { T } } } } { \bf { M } } _ { i } { { \bf { v } } _ { i } } } { \vartheta + { \alpha _ { i , t } } } \right) \cdot { \bf { H } } \left( \frac { { \lambda _ { i } } - { { \bf { v } } } } { { { \theta _ { \triangle } } } } \right)
$$

where $\lambda _ { i }$ and $\mathbf { v } _ { i }$ are the eigenvalues and eigenvectors of the adversarial data covariance matrix, and $_ v$ is the threshold for determining important spectral components. $\tilde { \mathbf { v } } _ { i } = \mathbf v _ { i } - \mathbf v _ { a v g }$ , where $\mathbf { v } _ { a v g }$ represents the mean of all feature vectors, $\mathbf { H } ( \cdot )$ is the Sigmoid function. $\vartheta$ is a scale parameter.

KASD learns from every interaction with its adversary, iteratively improving its detection model. This iterative learning is similar to the online learning paradigm discussed in (Shalev-Shwartz et al. 2012), where the model continuously evolves based on new data. In our context, KASD updates its parameters after every adversarial encounter:

$$
\pmb { \theta } _ { t + 1 } = \pmb { \theta } _ { t } - a \cdot ( \nabla _ { \theta } \mathcal { L } ( \mathcal { F } ( \mathcal { H } _ { t } ) , \pmb { \mathcal { A } } _ { t } ) + | | \pmb { \theta } _ { t } - \pmb { \theta } _ { t - 1 } | | ^ { 2 } )
$$

where ${ \pmb \theta } _ { t }$ is the parameter of KASD at time $t$ , and $a$ is the adaptation rate. In summary, the role of Kernel Anomaly Smoothing Detection Module in combating OOD attacks is proactive, detecting new attack methods. These attack methods cannot be effectively detected using common methods, and KASD enhances the overall robustness of the model group.

# Collaborative Protocols and Training

Communication and Consensus. The framework is designed from the principles of distributed computing and peerto-peer networks, where nodes (in this case, subgroup individuals in a model swarm) communicate directly with each other (Attiya and Welch 2004). The communication between any two models $\mathbf { M } _ { i }$ and $\mathbf { M } _ { j }$ is expressed as:

$$
\mathcal { C } _ { i , j } = \sum _ { m = 1 } ^ { n } \sum _ { \| \Theta _ { c } \| = m } \Theta _ { c } \cdot \mathbf { M } _ { i } \cdot \mathbf { M } _ { j } ^ { \mathrm { T } } \cdot \left( \mathbf { M } _ { j } \cdot \Theta _ { c } \right) ^ { m - 1 }
$$

where $c _ { i , j }$ is the communication channel between $\mathbf { M } _ { i }$ and $\mathbf { M } _ { j }$ , and $\Theta _ { c }$ represents the parameters that control the Communication Protocol (CP). This decentralized approach ensures that the network remains robust and can continue to operate effectively even if parts of the network are compromised. To achieve a unified defense, it is critical that models reach a consensus on the collective response to hostile threats. This is facilitated through a stochastic agreement protocol that guides the model to Consensus Update (CU) decisions over time. Inspired by work on stochastic optimization and multi-agent systems (Konecˇny\` et al. 2016; Assran et al. 2019), the consensus process for a given defense strategy $\mathbf { { T } } _ { t }$ at time $t$ is formalized as $\mathbf { \Gamma } _ { \Gamma + 1 } = \mathbf { \Gamma } _ { t } + \boldsymbol { \xi } _ { t } \cdot \Delta \mathbf { \Gamma } _ { t }$ , where $\Delta \mathbf { { r } } _ { t }$ is the update based on the current model state, and $\xi _ { t }$ is the learning rate parameter. This iterative process ensures that all models gradually adjust their strategies, resulting in a coherent and coordinated defense.

We use reinforcement learning techniques to continuously optimize the communication and consensus parameters in the network. This approach enables the network to gradually improve its collaboration strategy to better cope with new attacks. Under the framework of dynamic collaborative amplification, the communication and consensus modules ensure the overall processing efficiency within the network. Specifically, the design of these modules is based on the feature analysis of current attack methods, combined with

Defense Method Performance Comparison   
100.0 60 REE 80.0 WRN28-10 二二 WRN28-10 一- WRN28-10 50 40.0 WRN70-16 WRN70-16 T ? SA (ε=8/255) RA (εα=8/255)   
20.0 SA (ε2=0.5) 10 RA(ε2=0.5) 0 0.0 No Defense FDA AdRob DISCO REE 8 T BVITBTLTL T VTL BTB TBTB B 公 公 Method Model (a) Standard Accuracy and Robust Accuracy (b) No-defense and Post-REE

the ensemble knowledge of all models in the network, thus providing a credible basis for the dynamic adjustment.

Consensus-Based Group Updates. The model swarm uses a consensus-based approach to model updates to ensure that all models in the model swarm contribute to the learning process. The approach draws on the concept of distributed learning, where multiple agents collaboratively learn a shared model (Li et al. 2020; Shayan et al. 2018). At each training step, model parameters are updated based on the weighted average of their neighbor parameters: $\begin{array} { r } { \mathbf { Z } _ { i } ^ { ( t + 1 ) } = \sum _ { j \in \mathcal { M } _ { i } } w _ { i j } \cdot \mathbf { Z } _ { j } ^ { ( \bar { t } ) } , } \end{array}$ where $\mathbf { Z } _ { i } ^ { ( t + 1 ) }$ is the update parameter of model $i$ , $\mathbf { M } _ { i } ^ { N }$ is the adjacent model set of model $i$ , $w _ { i j }$ is assigned to The weights of the model $j$ parameters.

# Experiments

# Experimental Setup

Datasets and Training Configuration. The CIFAR-10 and CIFAR-100 datasets consist of images across 10 and 100 categories respectively, with the training and test sets comprising $5 0 \mathrm { k }$ and $\mathrm { 1 0 k }$ images. The ImageNet dataset contains 1.2M training images and $5 0 \mathrm { k }$ test images $( 2 2 4 \times 2 2 4 )$ , spanning a total of 1000 overall categories. The networks we utilize include ResNet-18 (He et al. 2016), WideResNet-28/32/70 (WRN28-10, WRN32-10 and WRN70-16) (Zagoruyko and Komodakis 2016). We opt for SGD as our optimizer, setting the momentum at 0.9. The weight decay and initial learning rate, adjusted using a piecewise decay scheduler, are set to 0.0005 and 0.1, respectively. Training is conducted over 200 epochs with a batch size of 128. The perturbation magnitude, measured by the $L _ { p }$ norm, is represented as $\epsilon _ { p }$ . On these datasets, we generate training pairs with $\epsilon _ { \infty } = 8 / 2 5 5$ and $\epsilon _ { 2 } = 0 . 5$ , using a step size of 2/255. The model implementation is based on the code from LIIF (Chen, Liu, and Wang 2021).

# Quantitative Evaluation

Evaluation on RobustBench. We conducts robustness benchmark experiments on the CIFAR-10 dataset under adversarial conditions of $\epsilon _ { \infty } = 8 / 2 5 5$ and $\epsilon _ { 2 } = 0 . 5 \$ , using WRN28-10 and WRN70-16 models as baselines. As shown in Fig. 2(a), the FDA (Rebuffi et al. 2021) and AdRob (Gowal et al. 2021, 2020) algorithms showed evident improvements in Robust Accuracy (RA), with FDA notably increasing RA to $6 6 . 5 8 \%$ and $8 2 . 3 2 \%$ under $\epsilon _ { \infty } = 8 / 2 5 5$ and $\epsilon _ { 2 } = 0 . 5$ attacks respectively, on the WRN70-16 model. On the WRN28- 10 model, REE elevated RA to $8 7 . 9 2 \%$ and $8 9 . 6 3 \%$ under $\epsilon _ { \infty } = 8 / 2 5 5$ and $\epsilon _ { 2 } = 0 . 5 \$ , respectively. This experiment shows the balance between maintaining standard model accuracy and defending against perturbations.

Robust evaluation of Vision Transformer. We analyze the accuracy performance of the Vision Transformer (Dosovitskiy et al. 2020) (ViT) model without defense and after REE training on the CIFAR-10 and CIFAR-100 datasets. Out-ofdistribution knowledge training is performed by combining ResNet (He et al. 2016), Wide ResNet (Zagoruyko and Komodakis 2016), ResNeXt (Xie et al. 2017) and ConvNeXt (Liu et al. 2022), and the results are shown in Fig. 2(b). On the CIFAR-10 dataset, the ViT-B-16 model showed a significant improvement in accuracy, from $3 4 . 5 3 \%$ in the undefended scenario to $56 . 9 9 \%$ after REE training. Similarly, the larger model ViT-L-32 showed the $2 1 . 6 7 \%$ accuracy improvement, thus demonstrating the scalability of REE across different model sizes. For the CIFAR-100 dataset, ViT-L-16 was initially $3 2 . 2 4 \%$ and reached an accuracy of $5 3 . 7 7 \%$ after REE, a significant improvement of $2 1 . 5 3 \%$ .

# Robustness Enhancement Comparison

We benchmark REE against five adversarial attack methods, including FGSM (Szegedy et al. 2013), $\mathrm { P G D ^ { 2 0 } }$ (Madry et al. 2017), C&W (Carlini and Wagner 2017), MIFGSM (Dong et al. 2017) and AutoAttack (Croce and Hein 2020). We compare with several common methods for improving model stability, namely BN (Ioffe and Szegedy 2015), LN (Ba, Kiros, and Hinton 2016), and RNA (Dong et al. 2022). The experimental results are shown in Table 1, on the CIFAR10 dataset, REE significantly outperforms other methods, especially the performance under adversarial attacks PGD20 and AutoAttack. Specifically, REE achieved a robustness of $6 1 . 8 0 \%$ and $6 6 . 3 6 \%$ on the ResNet-18 and WRN32-10 models respectively, which is the best among all comparison methods. On the CIFAR-100 dataset, under the most challenging AutoAttack attack, REE achieved $4 5 . 7 2 \%$ and $4 3 . 2 9 \%$ robustness on the ResNet-18 and WRN32-10 models, respectively. The robustness of REE was particularly highlighted under more challenging multi-vector attacks, showcasing significant improvements in both detection accuracy and mitigation effectiveness compared to existing methods.

Table 1: Robustness comparison of REE and other methods on CIFAR-10 and CIFAR-100 datasets using ResNet-18 and WRN32-10 models.   

<html><body><table><tr><td></td><td></td><td colspan="5">ResNet-18</td><td colspan="5">WRN32-10</td></tr><tr><td>Dataset</td><td>Method</td><td>FGSM</td><td>PGD20</td><td>C&W</td><td>MIFGSM</td><td>AutoAttack</td><td>FGSM</td><td>PGD20</td><td>C&W</td><td>MIFGSM</td><td>AutoAttack</td></tr><tr><td rowspan="4">CIFAR-10</td><td>BN</td><td>56.70</td><td>52.16</td><td>78.46</td><td>54.96</td><td>47.69</td><td>60.65</td><td>55.06</td><td>82.24</td><td>58.47</td><td>52.24</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LNA</td><td>53.18</td><td>45.4</td><td>75.51</td><td>50.72</td><td>4.8</td><td>57.80</td><td>49.74</td><td>79.39</td><td>54.88</td><td>46.44</td></tr><tr><td>REE (Ours)</td><td>65.25</td><td>61.80</td><td>84.58</td><td>62.40</td><td>68.27</td><td>68.45</td><td>66.36</td><td>83.61</td><td>64.43</td><td>67.95</td></tr><tr><td rowspan="4">CIFAR-100</td><td>BN</td><td>31.33</td><td>28.71</td><td>50.94</td><td>30.26</td><td>24.48</td><td>35.40</td><td>31.69</td><td>57.11</td><td>34.14</td><td>28.36</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>1.92</td><td>32.97</td><td>29.74</td><td></td><td></td><td></td></tr><tr><td>LNA</td><td>27.05</td><td>23.83</td><td>44.07</td><td>26.93</td><td></td><td></td><td></td><td>52.19</td><td>31.73</td><td>25.71</td></tr><tr><td>REE (Ours)</td><td>37.63</td><td>36.81</td><td>54.49</td><td>35.70</td><td>45.72</td><td>38.48</td><td>38.60</td><td>57.63</td><td>36.70</td><td>43.29</td></tr></table></body></html>

Table 2: Compare the accuracy after being attacked by PGD (Madry et al. 2017), C&W (Carlini and Wagner 2017), AutoAttack (AA) (Croce and Hein 2020), CAA (Mao et al. 2021) and MORA (Gao, Xu et al. 2022) with different ensemble strategies in 2 ensemble modes (Softmax and Voting). The worst complexity for the number of iterations is shown in the ”Complexity” row. The standard deviation of all results is within $\pm 0 . 0 5 \%$ . Consistent with the statement of the MORA (Gao, Xu et al. 2022), where MORAmt is represented as a multi-target attack, and 100 iterations are performed on the remaining 9 labels.   

<html><body><table><tr><td rowspan="2"> Defensity</td><td rowspan="2"></td><td colspan="6">Softmax</td><td colspan="6">Voting</td></tr><tr><td>PGD</td><td>CW</td><td>4</td><td>CAA</td><td>MORA</td><td>1.4k MORAmt</td><td>PGD</td><td>CW</td><td>4</td><td>CAA</td><td>MORA</td><td>1.4k MORAmt</td></tr><tr><td rowspan="3">ADP</td><td>3</td><td>5.98</td><td>7.72</td><td>0.98</td><td>3.34</td><td>0.59</td><td>0.34</td><td>9.32</td><td>11.84</td><td>6.13</td><td>8.29</td><td>0.64</td><td>0.29</td></tr><tr><td>5</td><td>7.10</td><td>8.70</td><td>2.18</td><td>4.25</td><td>0.97</td><td>0.67</td><td>12.42</td><td>12.05</td><td>10.13</td><td>0.67</td><td>1.17</td><td>0.62</td></tr><tr><td>8</td><td>7.22</td><td>9.59</td><td>3.94</td><td>6.04</td><td>1.70</td><td>1.32</td><td>12.53</td><td>10.50</td><td>9.21</td><td>1.69</td><td>3.16</td><td>1.65</td></tr><tr><td rowspan="3">Dverage</td><td>3</td><td>44.49</td><td>40.17</td><td>30.58</td><td>32.98</td><td>25.77</td><td>25.26</td><td>31.48</td><td>28.00</td><td>24.98</td><td>27.65</td><td>23.57</td><td>22.91</td></tr><tr><td>5</td><td>54.61</td><td>52.83</td><td>43.29</td><td>46.65</td><td>40.02</td><td>39.50</td><td>44.28</td><td>42.28</td><td>39.20</td><td>40.85</td><td>35.06</td><td>34.46</td></tr><tr><td>8</td><td>59.13</td><td>58.25</td><td>56.71</td><td>56.89</td><td>55.68</td><td>55.57</td><td>53.72</td><td>52.35</td><td>50.04</td><td>51.15</td><td>47.12</td><td>46.10</td></tr><tr><td rowspan="3">GAL</td><td></td><td>8.13</td><td>11.57</td><td>0.85</td><td>1.00</td><td>0.67</td><td>0.51</td><td>5.85</td><td>7.64</td><td>0.56</td><td>0.78</td><td>0.87</td><td>0.35</td></tr><tr><td>3 5</td><td>37.59</td><td>35.52</td><td>23.90</td><td>25.11</td><td>17.45</td><td>16.05</td><td>29.33</td><td>27.62</td><td>20.82</td><td>22.17</td><td>12.96</td><td>12.25</td></tr><tr><td>8</td><td>53.39</td><td>52.56</td><td>37.46</td><td>35.30</td><td>28.71</td><td>27.44</td><td>49.56</td><td>48.02</td><td>31.39</td><td>30.93</td><td>21.66</td><td>20.16</td></tr><tr><td rowspan="3">TRS+</td><td>3</td><td>14.01</td><td>10.87</td><td>8.46</td><td>9.75</td><td>8.11</td><td>7.60</td><td>10.19</td><td>8.71</td><td>6.69</td><td>8.08</td><td>5.73</td><td>5.44</td></tr><tr><td>5</td><td>15.91</td><td>15.28</td><td>13.20</td><td>13.78</td><td>12.67</td><td>12.47</td><td>12.71</td><td>11.88</td><td>10.30</td><td>11.21</td><td>8.82</td><td>8.38</td></tr><tr><td>8</td><td>18.02</td><td>17.59</td><td>16.51</td><td>16.73</td><td>15.90</td><td>15.64</td><td>14.57</td><td>13.48</td><td>11.85</td><td>12.80</td><td>11.39</td><td>10.69</td></tr><tr><td rowspan="3">REE (Ours)</td><td>3</td><td>45.39</td><td>41.76</td><td>31.19</td><td>33.60</td><td>26.31</td><td>26.94</td><td>32.63</td><td>29.41</td><td>25.68</td><td>28.54</td><td>24.33</td><td>23.90</td></tr><tr><td>5</td><td>53.90</td><td>51.58</td><td>44.11</td><td>46.85</td><td>40.31</td><td>39.72</td><td>43.21</td><td>41.18</td><td>39.61</td><td>40.82</td><td>36.35</td><td>35.96</td></tr><tr><td>8</td><td>59.52</td><td>58.74</td><td>56.79</td><td>57.73</td><td>56.59</td><td>56.52</td><td>54.98</td><td>53.44</td><td>51.83</td><td>52.71</td><td>47.20</td><td>46.40</td></tr></table></body></html>

# Comparison of SOTA Methods

We compared the accuracy performance of different integration strategies (Softmax and Voting) when facing different attack methods (PGD (Madry et al. 2017), C&W (Carlini and Wagner 2017), AA (Croce and Hein 2020), CAA (Mao et al. 2021), MORA (Gao, Xu et al. 2022)). We compare with four defense methods, namely ADP (Pang et al. 2019), Dverage (Yang et al. 2020), GAL (Kariyappa and Qureshi 2019), and $\mathrm { T R S ^ { + } }$ (Yang et al. 2021), which are the most widely compared algorithms. As shown in Table 2, the REE defense method shows excellent robustness under various attack methods. In Softmax and Voting modes, the REE defense method has higher accuracy than other defense methods under different numbers of sub-models (3, 5, 8). In comparison, the REE defense method shows the best accuracy under various attacks in Softmax and Voting modes. Under different numbers of sub-models, the accuracy of the REE defense method is higher than other defense methods, especially in MORA (Gao, Xu et al. 2022). Excellent performance under MORAmt attacks.

# Ablation Study

Our ablation study evaluated the impact of individual components within the REE on the overall model accuracy and robustness (as shown in Fig. 3). SubFig. (a) reveals a decrease in accuracy as the perturbation budget proportion increases. Similarly, SubFig. (b) describes a parallel reduction in robustness with an increase in perturbation budget proportion, following the same relational hierarchy observed for accuracy. In SubFig. (c) and SubFig. (d), both accuracy and robustness are enhanced with an increase in training epochs. This trend indicates the models’ learning efficacy, gradually improving their defensive stance against adversarial perturbations. This observation underscores the critical importance of balancing training duration and perturbation constraints to optimize both accuracy and robustness. It is worth mentioning that REE consistently outperforms variants that shield single or multiple components, demonstrating the overall contribution of each component to achieve a more superior performance.

![](images/49ca8a31713d9c6b861e20e2a4f4107916715f5b88dec8bb31520e0a1c9d7251.jpg)  
Figure 3: Ablation studies on the influence of single component removal within the REE framework on model performance. SubFig. (a) and SubFig. (b) illustrate the impact of changes in Perturbation budget proportion on accuracy and robustness, respectively. SubFig. (c) and SubFig. (d) show the effects of varying Training Epochs on accuracy and robustness, respectively.

Table 3: The experimental results of REE’s hyperparameter combination $( h _ { 1 } , h _ { 2 } )$ on the ConvNeXt model show the percentage improvement in robustness compared to the undefended scenario. For ease of identification, optimal hyperparameter settings are represented with a gray background.   

<html><body><table><tr><td>(h1,h2)</td><td>Re.</td><td>COT</td><td>Cos</td><td>CoB</td><td>COL</td><td>(h1,h2)</td><td>Re.</td><td>COT</td><td>Co s</td><td>COB</td><td>COL</td></tr><tr><td>(2.0, 0.5)</td><td>224² 3842</td><td>55.83 56.75</td><td>55.24 58.61</td><td>52.80 59.67</td><td>51.48 61.40</td><td>(1.0,2.0)</td><td>224² 3842</td><td>61.60 62.25</td><td>60.64 63.96</td><td>57.16 64.95</td><td>53.58 65.85</td></tr><tr><td>(2.0, 1.0)</td><td>224² 384²</td><td>55.32 56.70</td><td>54.17 57.48</td><td>51.64 60.43</td><td>50.18 62.83</td><td>(0.5,2.0)</td><td>224² 384²</td><td>57.47 58.62</td><td>54.30 55.15</td><td>51.31 55.77</td><td>48.69 58.21</td></tr><tr><td>(1.0,1.0)</td><td>224² 384²</td><td>56.54 57.99</td><td>53.22 59.30</td><td>52.58 60.17</td><td>50.52 63.29</td><td>(0.5,3.0)</td><td>224² 3842</td><td>55.13 56.54</td><td>52.26 52.86</td><td>50.84 53.19</td><td>47.41 55.32</td></tr></table></body></html>

# Hyperparameter Combination Analysis

We performs a robustness analysis on ConvNeXt (trained on ImageNet-22K) (Liu et al. 2022) for the hyperparameter combination $( h _ { 1 } , h _ { 2 } )$ in REE, as shown in Table 3. We use four versions of ConvNeXt, namely ConvNeXt-Tiny $( \mathbf { C } \odot \mathbf { T } )$ , ConvNeXt-Small $( \mathbf C _ { } ) \odot \mathbf S )$ , ConvNeXt-Base $( \mathbf { C } \odot \mathbf { B } )$ and ConvNeXt-Large $( \mathbf { C } \odot \mathbf { L } )$ . Experiments were conducted at two resolutions ( $2 2 4 ^ { 2 }$ and $3 8 4 ^ { 2 }$ ). The hyperparameter combination (1.0, 2.0) performs best under all configurations, and its effect is highlighted with a gray background. At $3 8 4 ^ { 2 }$ resolution, this combination achieves a $6 5 . 8 5 \%$ robustness improvement on the ConvNeXt-Large model, and a $6 2 . 2 5 \%$ improvement on the Tiny variant, showing that REE can improve both large-scale networks and small-scale networks.

# Conclusion

In this paper, we propose the Reliable Defense Ensemble (REE) to address the challenge of using ensemble knowledge to counter Out-of-Distribution (OOD) attacks. REE optimizes the ensemble knowledge of models through Dynamic Synergy Amplification (DSA) and the Kernel Anomaly Smoothing Detection Module (KASD). DSA allocates specific weights and adjusts strategies to enhance the robustness of the ensemble, thereby sharing critical knowledge between models. KASD detects new anomalies using a smoothing feature function based on Gaussian kernel mean embedding and a multi-layer feedback structure. To maintain effective knowledge sharing among models, we also propose using reinforcement learning to iteratively fine-tune communication and consensus parameters. Extensive experiments on benchmark datasets and various scenarios demonstrate that REE achieves state-of-the-art performance in utilizing ensemble model knowledge to counter OOD attacks.

# Acknowledgments

This work was supported by the Jiangsu Province Science Foundation for Youths (BK20240463), the Laboratory for Advanced Computing and Intelligence Engineering Fund, the Xiaomi Young Talents Program, the China Postdoctoral Science Foundation (2024M753115), the National Natural Science Foundation of China (62172385), the Natural Science Foundation of Jiangsu Province (BK20241819), and the Innovation Program for Quantum Science and Technology (2021ZD0302900).