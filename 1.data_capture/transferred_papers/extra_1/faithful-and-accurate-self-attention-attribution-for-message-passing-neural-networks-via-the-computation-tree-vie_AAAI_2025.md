# Faithful and Accurate Self-Attention Attribution for Message Passing Neural Networks via the Computation Tree Viewpoint

Yong-Min Shin1, Siqing $\mathbf { L i } ^ { 2 }$ , Xin $\mathbf { C a o } ^ { 2 }$ , Won-Yong Shin1\*

1Yonsei University 2University of New South Wales jordan3414 $@$ yonsei.ac.kr, siqing.li, xin.cao $@$ unsw.edu.au, wy.shin $@$ yonsei.ac.kr

# Abstract

The self-attention mechanism has been adopted in various popular message passing neural networks (MPNNs), enabling the model to adaptively control the amount of information that flows along the edges of the underlying graph. Such attention-based MPNNs (Att-GNNs) have also been used as a baseline for multiple studies on explainable AI (XAI) since attention has steadily been seen as natural model interpretations, while being a viewpoint that has already been popularized in other domains (e.g., natural language processing and computer vision). However, existing studies often use na¨ıve calculations to derive attribution scores from attention, undermining the potential of attention as interpretations for Att-GNNs. In our study, we aim to fill the gap between the widespread usage of Att-GNNs and their potential explainability via attention. To this end, we propose GATT, edge attribution calculation method for self-attention MPNNs based on the computation tree, a rooted tree that reflects the computation process of the underlying model. Despite its simplicity, we empirically demonstrate the effectiveness of GATT in three aspects of model explanation: faithfulness, explanation accuracy, and case studies by using both synthetic and realworld benchmark datasets. In all cases, the results demonstrate that GATT greatly improves edge attribution scores, especially compared to the previous naı¨ve approach.

Code — https://github.com/jordan7186/GAtt

# 1 Introduction

Background & motivation. In graph learning, graph neural networks (GNNs) (Wu et al. 2021) have been used as the de facto architecture, since they can effectively encode the graph structure along with node (or edge) features. Among various GNNs, several models have successfully incorporated the self-attention mechanism (Vaswani et al. 2017) into message passing neural networks (MPNNs) (Gilmer et al. 2017; Bronstein et al. 2021). Such (self-)attentionbased MPNNs (dubbed Att-GNNs) have been one of the staple GNN architectures, and the self-attention mechanism of Att-GNNs themselves has been extensively analyzed in the literature (Knyazev, Taylor, and Amer 2019; Lee et al. 2019;

Sun et al. 2023).1 Furthermore, several studies have focused solely on analyzing GAT (Velickovic et al. 2018), the most representative Att-GNN model (Mustafa, Bojchevski, and Burkholz 2023; Fountoulakis et al. 2023).

Similarly as in other neural network models, GNNs are regarded as black-box models that lack interpretability, which has led to numerous studies developing explanation methods for GNNs (Li et al. 2022; Yuan et al. 2023). While such explanation methods have been widely developed, attention has also been frequently considered as a fundamental tool for GNN explanations (Ying et al. 2019; Luo et al. 2020; Sa´nchez-Lengeling et al. 2020). The choice of attention as a baseline is natural, as self-attention itself can be viewed as a direct way to provide model interpretations without any separate explanation method (Lee, Shin, and $\mathrm { K i m } 2 0 1 7$ ; Ghaeini, Fern, and Tadepalli 2018; Hao et al. 2021; Aflalo et al. 2022; Deiseroth et al. 2023). This viewpoint has already been extensively investigated in transformers, the most representative architecture with attention (Bahdanau, Cho, and Bengio 2015; Xu et al. 2015; Vig 2019; Dosovitskiy et al. 2021; Caron et al. 2021). There is even a significant body of research debating the validity of self-attention as explanations in natural language processing (NLP) (Jain and Wallace 2019; Wiegreffe and Pinter 2019; Bibal et al. 2022). However, there has been no such in-depth discussion from the domain of GNN explanations, mostly employing the layer-wise average of attention weights retrieved from a GAT model as explanations at best.

We argue that such na¨ıve usage of attention for interpretations largely undermines the potential of AttGNNs as an explainable model. In the case of transformers, a number of advanced attribution methods using attention have been proposed to calculate token attributions, and have been empirically proven that attention can be effectively used to decipher the underlying model (Abnar and Zuidema 2020; Chefer, Gur, and Wolf 2021a,b; Hao et al. 2021). Analogous to transformers, our study aims to formulate a post-processing method for the attention weights in Att-GNNs that is able to extract high-quality edge attributions (i.e., to assign contributions of edges to the model) and capture the behavior of Att-GNNs more precisely. To the best of our knowledge, we are the first to address this issue within the scope of general Att-GNNs, thus filling in the literature of explanations via attention (see the red part of Table 1).

Table 1: Overview of prior works on using attention as explanations. Despite various methods being developed for calculating token attributions for transformers, no corresponding method has yet been developed for calculating edge attributions for Att-GNNs.   

<html><body><table><tr><td>Model</td><td>Naive methods</td><td>Advanced methods</td></tr><tr><td>Transformers</td><td>Raw attention (Vig 2019)</td><td>(Abnar and Zuidema 2020) (Chefer, Gur,and Wolf 2021a) (Chefer,Gur,and Wolf 2021b) (Hao et al. 2021) etc.</td></tr><tr><td>Att-GNNs</td><td>Layer-wise averaging (Ying et al. 2019) (Luo et al. 2020)</td><td>This work (GATT)</td></tr></table></body></html>

Main contributions. In this study, we address the problem of developing an effective edge attribution method using attention weights in Att-GNNs. Our key insight is that edge attributions with attention can be advanced by aligning with the feed-forward process of MPNNs, i.e., thinking in terms of the computation tree viewpoint, a rooted subtree that shows the local computation structure around a target node (see the middle part of Figure 1). Based on observing the computation tree, we assert that the edge attribution function should encompass two crucial principles: P1) proximity to the target node and P2) its position in the computation tree, thus aligning with the feed-forward process.

To this end, we introduce GATT, a simple yet effective solution to the edge attribution problem by integrating the computation tree of a given target node. Specifically, GATT adds attention weights in the underlying Att-GNN across the computation tree while adjusting their influence by employing targeted multiplication factors for attention weights guiding towards the target node. As an example, Figure 1 visualizes edge attribution scores from different edge attribution calculation methods using the same model. The attribution scores from GATT (see the right red box in Figure 1) show that the model places high emphasis on the correct infection path (highlighted as blue nodes). Such conclusion could not have been reached if we were to use simple layer-wise averaging (see the left box in Figure 1) as the tool for interpretations. To prove the effectiveness of GATT, we run extensive experiments by answering pivotal facets of interpretations—faithfulness and explanation accuracy—of Att-GNNs across diverse real-world and synthetic datasets. Despite the simplicity of GATT, empirical results demonstrate that the application of GATT to process attention weights within the underlying model produces substantively improved explanation capabilities, excelling in both faithfulness and explanation accuracy. We also perform an ablation study in which we introduce two variants of GATT, namely $\mathbf { G A T T _ { S I M } }$ and $\mathbf { G A T T _ { A V G } }$ , each of which corresponds to a removal of one critical design element (i.e., P1 or $\mathbf { P } 2$ ) of our method. Our analysis reveals clear deterioration of the quality of the edge attributions in all measures for both variants, which justifies the necessity of our two design elements. Finally, we remark that GATT is a straightforward calculation module (i.e., does not involve any optimization/learning process), therefore brings the benefit of being hyperparameter-free and deterministic. In summary, we conclude that Att-GNNs are indeed highly explainable when adopting the proper interpretation, i.e., adjustment of attention weights by taking the viewpoint of the computation tree. Note that graph transformers (Ying et al. 2021; Kreuzer et al. 2021; Chen et al. 2023) are beyond the scope of this study since transformers have already been analyzed and advanced by numerous studies (see Table 1). Our contributions are summarized as follows:

![](images/1466d2e9b2d251f49b4ebf30937afb9c87c8dbf912131ea031a5dc1cc6c93f6b.jpg)  
Figure 1: An visualization of our method (GATT, right) against the previous approach (AVGATT, left) on the Infection dataset, where the correct infection path is highlighted as the blue nodes.

Key observations: We make key observations and design principles that are crucial in edge attribution calculation by integrating the computation tree of the target node during its feed-forward process.

Novel methodology: We propose GATT, a new method to calculate edge attributions from attention weights in Att-GNNs by integrating the computation tree of the given GNN model.

Extensive evaluations: We extensively demonstrate that Att-GNNs are shown to be more faithful and accurate when using our proposed method compared to the simple alternative.

It should be noted that as long as Att-GNN architectures are employed, GATT is model-agnostic and standalone without any learning modules. We refer to (Shin et al. 2024) for a comprehensive review of related studies.

# 2 Edge Attribution Calculation in Att-GNNs

In this section, we first describe the notation used in the paper. Then, we formalize the problem of calculating edge attributions in Att-GNNs, and propose GATT, an approach to incorporate the computation tree into edge attributions.

# 2.1 Notations

Let us denote a given undirected graph as a tuple $G \ =$ $( \nu , \mathcal { E } )$ , where $\nu$ is the set of nodes and $\mathcal { E }$ is the set of edges. We denote the edge connecting two nodes $v _ { i } , v _ { j } \in \mathcal { V }$ as

![](images/01f041e2c13f3ee3463f06c2be27fb455d9974c746cb5d3d7f927e1044e9b24a.jpg)  
Figure 2: A visualization for a 2-layer Att-GNN on target node 27 on the infection dataset. Figure 2a shows the local 2- hop subgraph with the edge $e _ { 4 0 , 2 7 }$ marked as red. Figure 2b shows the computation tree in the Att-GNN, where the information flows from leaf nodes to node 27 at the root. The edges are colored by the attention weights from the model, while highlighting the two occurrences of edge $e _ { 4 0 , 2 7 }$ .

$e _ { i j } \ \in \mathcal E$ . We consider undirected graphs, i.e., $e _ { j , i } \in \mathcal { E }$ if $e _ { i , j } \in \mathcal { E }$ . The set of neighbors of node $\boldsymbol { v } _ { i }$ is denoted as ${ \mathcal { N } } _ { i }$ .

# 2.2 Problem Statement

We are given a graph $G = ( \nu , \mathcal { E } )$ , the Att-GNN model $f$ with $L$ layers, and a target node $v _ { i } ~ \in ~ \mathcal { V }$ of interest. The attention weights calculated from $f$ are denoted as ${ \mathcal { A } } =$ $\{ \mathbf { A } ( l ) \} _ { l = 1 } ^ { L }$ , where $\mathbf { A } ( l ) \in \mathbb { R } ^ { | \mathcal { V } | \times | \mathcal { V } | }$ and $[ \mathbf { A } ( l ) ] _ { j , i } = \alpha _ { i , j } ^ { l }$ is the attention weight of edge $e _ { i , j }$ in the $l$ -th layer (with $l = 1$ being the input layer). The problem of edge attribution calculation is characterized by an edge attribution function $\Phi ( v , \mathcal { A } , e _ { i , j } ) \triangleq \phi _ { i , j } ^ { v }$ such that the edge attribution score $\phi _ { i , j } ^ { v }$ accounts for the contribution of edge $\boldsymbol { e } _ { i , j }$ to the underlying model’s calculation for node $\boldsymbol { v }$ (i.e., faithfulness to $f )$ .

In our study, our objective is to design $\Phi$ using the computation tree in Att-GNNs alongside several observations and key design principles, which will be specified later. To design such a function $\Phi$ , we argue that the computation tree of Att-GNNs should be considered for the precise calculation of $\phi _ { i , j } ^ { v }$ , incorporating its several key properties. Note that, although most post hoc instance-level explanation methods for GNNs (Ying et al. 2019; Luo et al. 2020) also have a similar objective in terms of calculating $\phi _ { i , j } ^ { v }$ , they do not take advantage of attention weights $\mathcal { A }$ . Additionally, although we mainly consider node-level tasks throughout the paper as a representative task, we also demonstrate that the idea of GATT can also be extended for graph-level tasks, which we refer to (Shin et al. 2024) for further experimental results.

# 2.3 From Attention to Attribution

We first visualize the computation tree in a Att-GNN, which will lead to several important observations to guide GATT, an edge attribution calculation method given the attention weights in the Att-GNN model.

Visualizing the Computation Tree. To provide an illustrative example, we train a 2-layer GAT model (Velickovic et al. 2018) with a single attention head on the synthetic infection benchmark dataset (Faber, Moghaddam, and Wattenhofer 2021). Figure 2a shows the 2-hop subgraph from target node 27, which contains all nodes and edges that the model involves from node 27’s point of view. The computation tree in the GAT is commonly expressed as a rooted subtree (Sato, Yamada, and Kashima 2021), as shown in Figure 2b for node 27. In the figure, the information flows from leaf nodes at depth 2 to the root node 27 at depth 0, which exhibits an apparently different structure from that of the subgraph in Figure 2a. Note that the attention weights are calculated in each graph attention layer for each edge in $\mathcal { E }$ .

Design Principles. We begin by making several observations from the computation tree:

(O1) Identical edges can appear multiple times in the computation tree. For example, edge $e _ { 4 0 , 2 7 }$ in Figure 2a appears twice in Figure 2b.   
(O2) Nodes do not appear uniformly in the computation tree. Specifically, nodes that are $k$ -hops away from the target node do not exist in depth $k ^ { \prime }$ for $0 < k ^ { \prime } < k$ (e.g., node 70 appears only at depth 2 while node 40 appears three times).   
(O3) The graph attention layer always includes self-loops during its feed-forward process.

Based on the above observations, we would like to state two design principles that are desirable when designing the edge attribution function $\Phi$ .

(P1) Proximity effect: Edges within closer proximity to the target node tend to highly impact the model’s prediction compared with distant edges, since they are likely to appear more frequently in the computation tree.   
(P2) Contribution adjustment: The contribution of an edge in the computation tree should be adjusted by its position (i.e., other edges in the path towards the root).

By these standards, we revisit Figure 2b. We first see that edges close to the target node such as $e _ { 4 0 , 2 7 }$ appear twice, whereas distant edges such as $e _ { 7 0 , 4 0 }$ appear only once $( \mathbf { P 1 } )$ . Moreover, we empirically show that this principle $( \mathbf { P 1 } )$ holds in most real-world datasets (which we refer to (Shin et al. 2024) for further details). Additionally, for the attention weights from the last graph attention layer (i.e., edges connecting nodes at depth 1 to the root node), each edge tends to have roughly the value of 0.25 for the attention weights. In consequence, the information flowing from the first graph attention layer (i.e., edges connecting leaf nodes to nodes at depth 1) will be diminished by roughly 0.25 as it reaches the root node $( \mathbf { P } 2 )$ .

Proposed Method. To design the edge attribution function $\Phi$ , we start by formally defining the computation tree alongside the flow and the attention flow.

Definition 2.1 (Computation tree). The computation tree for an $L$ -layer Att-GNN in our study is defined as a rooted subtree of height $L$ with the target node as the root node. For each node in the tree at depth $d$ , the neighboring nodes and itself are at depth $d + 1$ with edges directed towards node $v$ .

According to Definition 2.1, we define the concept of flows in the computation tree.

Definition 2.2 (Flow in a computation tree). Given a computation tree as a rooted subtree of height $L$ with the root (target) node $v$ , we define a flow li,j,v as the list of edges that sequentially appear in a path of length $l$ starting from a given edge $\boldsymbol { e } _ { i , j }$ within the computation tree and ending with some edge $e _ { * , v }$ .2 We indicate the $k$ -th position within the flow (i.e., starting from the bottom of the computation tree) as ${ \lambda _ { i , j , v } ^ { l } ( k ) }$ for $\bar { k } \in [ 1 , L ]$ . We denote the set of all flows in the computation tree with node $v$ at its root that starts from edge $\boldsymbol { e } _ { i , j }$ with length $m \in [ 1 , L ]$ as $\Lambda _ { v } ^ { m } ( e _ { i , j } )$ .

From Definition 2.2, it follows that $\lambda _ { i , j , v } ^ { l } ( 1 ) = e _ { i , j }$ and $\lambda _ { i , j , v } ^ { l } ( l ) = e _ { * , v }$ for all flows in $\Lambda _ { v } ( e _ { i , j } )$ .

Definition 2.3 (Attention flow in a computation tree). Given a flow $\lambda _ { v _ { 0 } , v _ { 1 } , w } ^ { m } = [ e _ { v _ { 0 } , v _ { 1 } } , \cdot \cdot \cdot , e _ { * , w } ]$ of length $m \leq L$ for an $L$ -layer Att-GNN model, we define an attention flow $\alpha [ \lambda _ { v _ { 0 } , v _ { 1 } , w } ^ { m } ]$ as the corresponding attention weights assigned to each edge by the associated graph attention layers:

$$
\begin{array} { r } { \alpha [ \lambda _ { v _ { 0 } , v _ { 1 } , w } ^ { m } ] = [ \alpha _ { v _ { 0 } , v _ { 1 } } ^ { L - m + 1 } , \cdot \cdot \cdot , \alpha _ { * , w } ^ { L } ] . } \end{array}
$$

Then, it follows that $\alpha [ \lambda _ { v _ { 0 } , v _ { 1 } , w } ^ { m } ] ( i ) = \alpha _ { v _ { i - 1 } , v _ { i } } ^ { L - m + i }$ .

Example 1. In Figure 2b, $\Lambda _ { 2 7 } ( e _ { 4 0 , 2 7 } )$ includes two flows, i.e., $\lambda _ { 4 0 , 2 7 , 2 7 } ^ { 1 } \ = \ [ e _ { 4 0 , 2 7 } ]$ and $\lambda _ { 4 0 , 2 7 , 2 7 } ^ { 2 } \ : = \ : \left[ e _ { 4 0 , 2 7 } , e _ { 2 7 , 2 7 } \right]$ , along with the corresponding attention flows $\alpha [ \lambda _ { 4 0 , 2 7 , 2 7 } ^ { 1 } ] =$ [0.25] and $\alpha [ \lambda _ { 4 0 , 2 7 , 2 7 } ^ { 2 } ] = [ 0 . 9 , 0 . 2 5 ]$ , respectively.

Finally, we are ready to present GATT.

Definition 2.4 (GATT). Given a target node $v$ , an edge $e _ { i , j }$ of interest, the set of flows, $\Lambda _ { v } ( e _ { i , j } )$ , and the attention flows for all flows in $\Lambda _ { v } ( e _ { i , j } )$ , we define the edge attribution of $\boldsymbol { e } _ { i , j }$ in $L$ -layer Att-GNN as

$$
\phi _ { i , j } ^ { v } = \sum _ { m ^ { \prime } = 1 } ^ { L } \sum _ { \lambda _ { i , j , v } ^ { m ^ { \prime } } \in \Lambda _ { v } ^ { m ^ { \prime } } ( e _ { i , j } ) } C ( \alpha [ \lambda _ { i , j , v } ^ { m ^ { \prime } } ] ) \alpha [ \lambda _ { i , j , v } ^ { m ^ { \prime } } ] ( 1 ) ,
$$

where $\begin{array} { r } { C ( \alpha [ \lambda _ { i , j , v } ^ { m } ] ) = \prod _ { 2 \leq k \leq m } \alpha [ \lambda _ { i , j , v } ^ { m } ] ( k ) } \end{array}$ (or 1 if $m = 1$ ). Eq. (2) can be interpreted as follows. We first find all occurrences of the target edge $\boldsymbol { e } _ { i , j }$ in the computation tree, and then re-weight its corresponding attention score (i.e., $\alpha [ \lambda _ { i , j , v } ^ { m } ] ( 1 ) )$ by the product of all attention weights that appear after $\boldsymbol { e } _ { i , j }$ (i.e., $\alpha [ \lambda _ { i , j , v } ^ { m } ] ( k )$ for $k \geq 2 ,$ ) in the flow before the summation over all relevant flows. Next, let us turn to addressing how our design principles $( \mathbf { P 1 } )$ and $( \mathbf { P } 2 )$ are met. $( \mathbf { P 1 } )$ holds as we add the contributions from each flow rather than taking the average, therefore the total number of occurrences of $\boldsymbol { e } _ { i , j }$ is directly expressed in the edge attribution. $( \mathbf { P } 2 )$ is fulfilled by the adjustment factor $C ( \alpha [ \lambda _ { i , j , v } ^ { m } ] )$ , since its value is dependent on the position of $\lambda _ { i , j , v } ^ { m } ( 1 )$ . Essentially, $C ( \alpha [ \lambda _ { i , j , v } ^ { m } ] )$ takes the chain of calculation from an edge to the target node into account. We provide an insightful example below.

Example 2. Let us recall $\lambda _ { 4 0 , 2 7 , 2 7 } ^ { 2 } = [ e _ { 4 0 , 2 7 } , e _ { 2 7 , 2 7 } ]$ and its attention flow $\alpha [ \lambda _ { 4 0 , 2 7 , 2 7 } ^ { 2 } ] = [ 0 . 9 , 0 . 2 5 ]$ on node 27 from Example 1. At face value, the contribution of edge $e _ { 4 0 , 2 7 }$ within the flow $\lambda _ { 4 0 , 2 7 , 2 7 } ^ { 2 }$ should be 0.9. However, this is inappropriate since the information will eventually get muted significantly by $\alpha _ { 2 7 , 2 7 } ^ { 2 } = 0 . 2 5$ ; thus, we need to consider the adjustment factor $C ( \alpha [ \lambda _ { 4 0 , 2 7 , 2 7 } ^ { 2 } ] )$ before calculating the final edge attribution. From Definition 2.4, the edge attribution $\bar { \phi } _ { 4 0 , 2 7 } ^ { 2 7 }$ from the attention weights is calculated as $\phi _ { 4 0 , 2 7 } ^ { 2 7 } = 1 \times 0 . 2 5 + 0 . 2 5 \times 0 . 9 = 0 . 4 7 5$ .

Efficient Calculation of GATT. Although GATT is defined as Eq. (2), directly using this to compute the edge attribution $\phi _ { i , j } ^ { v }$ is not desirable since it involves constructing the computation tree in the form of a rooted subtree for each node $\boldsymbol { v }$ , as well as computing over all relevant attention flows, resulting in high redundancy during computation and not being proper for batch computation. To overcome these computational challenges, as another contribution, we introduce a matrix-based computation method that is much preferred in practice. To this end, we first define

$$
\mathbf { C } _ { L } ( k ) = \binom { \mathbf { I } , } { \mathbf { A } ( L ) \mathbf { A } ( L - 1 ) \cdot \cdot \cdot \mathbf { A } ( L - k + 1 ) , }
$$

Then, we would like to establish the following proposition.

Proposition 2.5. For a given set of attention weights $\boldsymbol { \mathcal { A } } =$ $\{ \mathbf { A } \bar { ( } l ) \} _ { l = 1 } ^ { L }$ for an $L$ -layer $A t t$ -GNN with $L \ge 1$ , GATT in Definition 2.4 is equivalent to

$$
\phi _ { i , j } ^ { v } = \sum _ { m = 1 } ^ { L } [ \mathbf { C } _ { L } ( L - m ) ] _ { v , j } [ \mathbf { A } ( m ) ] _ { j , i } .
$$

We refer to (Shin et al. 2024) for the proof. Proposition 2.5 signifies that GATT sums the attention scores, weighted by the sum of the products of attention weights $[ \mathbf { A } ( m ) ] _ { j , i }$ along the paths from node $j$ to node $v$ , over all graph attention layers. We also provide another GATT calculation method optimized for batch calculations in (Shin et al. 2024).

Complexity Analysis. We first analyze the computational complexity of GATT with matrix-based calculation. According to Eq. (3), the bottleneck for calculating $\phi _ { i , j } ^ { v }$ is to acquire $\begin{array} { r } { \prod _ { k = m + 1 } ^ { L } \mathbf { A } ( k ) } \end{array}$ e.qHuiorewerve-ecr,altchuislatmioatn axftcearn bse npirtei-alcoamcqpuitredment. Since we only count the number of multiplications in the summation, the computational complexity is finally given by $O ( L )$ , which is extremely efficient. Next, according to Eq. (3), the memory complexity requires delving into $\bar { \mathbf { C } _ { L } } ( L - m )$ and $\mathbf { A } ( m )$ . For an $L$ -layer Att-GNN, while storing all attention weights in $\mathbf { A } ( m )$ requires $O ( L | \mathcal { E } | )$ , $\mathbf { C } _ { L } \big ( \bar { L _ { \mathbf { \alpha } } } - m \big )$ requires at most $O ( L | | \dot { T } ^ { L - 1 } | | \bar { 0 } )$ , where $T$ denotes the adjacency matrix and $| | \mathbf { \partial } \cdot \mathbf { \partial } | | _ { 0 }$ is the 0-norm. In conclusion, the total memory complexity is bounded by $O ( L | | T ^ { L - 1 } | | _ { 0 } + L | \mathcal { E } | )$ . In addition to the above theoretical findings, we empirically provide runtime evaluations, which demonstrate that GATT is reasonably fast and scalable, achieving up to 58.05 times faster runtime against PGExplainer (Luo et al. 2020) when calculating edge attributions for 10,000 nodes, which we refer to (Shin et al. 2024) for detailed experimental results.

Table 2: Experimental results on the faithfulness measure for GATT, AVGATT, and random attribution for GAT/GATv2 on 7 real-world datasets. Results for $2 / 3$ -layer GAT/GATv2s are shown for each case (the best performer is highlighted as bold).   

<html><body><table><tr><td rowspan="2">Dataset</td><td rowspan="2"></td><td colspan="3">2-layer GAT/GATv2</td><td colspan="3">3-layer GAT/GATv2</td></tr><tr><td>GATT</td><td>AVGATT</td><td>Random</td><td>GATT</td><td>AVGATT</td><td>Random</td></tr><tr><td rowspan="3">Cora</td><td>△PC</td><td>0.8468/0.1040</td><td>0.1764/0.0121</td><td>-0.0056/-0.0036</td><td>0.8642/0.1696</td><td>0.0967/0.0168</td><td>0.0045/0.0045</td></tr><tr><td>△NE</td><td>0.7112/0.0930</td><td>0.1526/0.0100</td><td>-0.0076/0.0019</td><td>0.7690/0.1664</td><td>0.0859/0.0186</td><td>0.0040/0.0037</td></tr><tr><td>△p</td><td>0.9755/0.9623</td><td>0.7251/0.6226</td><td>0.4389/0.4891</td><td>0.9875/0.9966</td><td>0.7075/0.8897</td><td>0.5235/0.6107</td></tr><tr><td rowspan="3">Citeseer</td><td>△PC</td><td>0.8516/0.0658</td><td>0.3096/0.0180</td><td>0.0012/-0.0043</td><td>0.8711/0.0432</td><td>0.2110/0.0107</td><td>-0.0073/-0.0034</td></tr><tr><td>△NE</td><td>0.7653/0.0700</td><td>0.2780/0.0186</td><td>0.0021/0.0019</td><td>0.8291/0.0551</td><td>0.2006/0.0140</td><td>0.0015/0.0025</td></tr><tr><td>△P</td><td>0.9846/0.9771</td><td>0.9213/0.9510</td><td>0.3695/0.4258</td><td>0.9920/0.9961</td><td>0.8979/0.9692</td><td>0.4039/0.7569</td></tr><tr><td rowspan="3">Pubmed</td><td>△PC</td><td>0.8812/0.0631</td><td>0.1648/0.0126</td><td>-0.0064/0.0021</td><td>0.8489/0.0367</td><td>0.0592/0.0023</td><td>0.0015/-0.0016</td></tr><tr><td>△NE</td><td>0.8201/0.0915</td><td>0.1477/0.0169</td><td>-0.0068/0.0078</td><td>0.8612/0.0484</td><td>0.0600/0.0028</td><td>0.0009/-0.0015</td></tr><tr><td>△p</td><td>0.9915/0.9972</td><td>0.8834/0.9361</td><td>0.3974/0.1327</td><td>0.9993/0.9996</td><td>0.8932/0.9153</td><td>0.5172/0.5242</td></tr><tr><td rowspan="3">Arxiv</td><td>△PC</td><td>0.7790/0.0546</td><td>0.0794/-0.0593</td><td>0.0007/0.0028</td><td>0.7721/0.0508</td><td>0.0465/-0.0252</td><td>-0.0004/-0.0003</td></tr><tr><td>NE</td><td>0.8287/0.0164</td><td>0.0804/-0.0390</td><td>0.0016/-0.0067</td><td>0.8282/-0.0012</td><td>0.0478/-0.0216</td><td>-0.0017/0.0000</td></tr><tr><td>△P</td><td>0.9908/0.8995</td><td>0.8470/0.2560</td><td>0.4962/0.5107</td><td>0.9985/0.9366</td><td>0.8331/0.3934</td><td>0.5004/0.5034</td></tr><tr><td rowspan="3">Cornell</td><td>△PC</td><td>0.8089/0.2660</td><td>0.3391/0.0209</td><td>-0.0284/0.0421</td><td>0.7173/0.0899</td><td>0.3065/-0.0512</td><td>-0.0273/-0.0129</td></tr><tr><td>△NE</td><td>0.7820/0.1526</td><td>0.3199/-0.0488</td><td>-0.0231/0.0235</td><td>0.7160/0.0520</td><td>0.3491/-0.0294</td><td>-0.0060/-0.0017</td></tr><tr><td>△P</td><td>0.9532/0.8372</td><td>0.7416/0.5130</td><td>0.5074/0.5660</td><td>0.9270/0.6406</td><td>0.6907/0.3969</td><td>0.4787/0.4953</td></tr><tr><td rowspan="3">Texas</td><td>△PC</td><td>0.7818/0.0801</td><td>0.3676/-0.0406</td><td>-0.0762/0.0025</td><td>0.6866/0.1504</td><td>0.2443/0.0486</td><td>0.0414/0.0040</td></tr><tr><td>△NE</td><td>0.7977/0.1443</td><td>0.3809/0.1478</td><td>-0.0659/0.0145</td><td>0.6132/0.0896</td><td>0.1645/0.0579</td><td>0.0202/0.0149</td></tr><tr><td>△P</td><td>0.8726/0.7299</td><td>0.6803/0.3669</td><td>0.4733/0.5198</td><td>0.9197/0.8195</td><td>0.7072/0.5565</td><td>0.5562/0.5426</td></tr><tr><td rowspan="3">Wisconsin</td><td>△PC</td><td>0.6898/0.1751</td><td>0.2649/0.0556</td><td>0.0596/0.0120</td><td>0.7616/0.0323</td><td>0.3034/0.0337</td><td>-0.0059/0.0407</td></tr><tr><td>△NE</td><td>0.6421/0.1554</td><td>0.2340/0.0636</td><td>0.0414/0.0157</td><td>0.7409/0.0243</td><td>0.2762/0.0574</td><td>-0.0010/0.0400</td></tr><tr><td>△P</td><td>0.8985/0.8501</td><td>0.7067/0.6060</td><td>0.5427/0.5006</td><td>0.8982/0.7582</td><td>0.6906/0.3980</td><td>0.5119/0.5333</td></tr></table></body></html>

# 3 Can Attention Interpret Att-GNNs?

In this section, we carry out empirical studies to validate the effectiveness of GATT on interpreting two representative Att-GNN models: GAT (Velickovic et al. 2018) and GATv2 (Brody, Alon, and Yahav 2022), with a singleattention head. Despite only a subset of all experimental results being presented due to page limitations, we have also demonstrated that GATT can be generally applied to other Att-GNNs by showing the results for another model, SuperGAT (Kim and Oh 2021). Additionally, we have found that the trend in performance for multi-head attention is consistent with the case for single-head attention. Finally, we have shown that the regularization during training has negligible effects on GATT. We refer to (Shin et al. 2024) for the details on the additional results.

# 3.1 Is Attention Faithful to the GNN?

We focus primarily on one of the most important properties in evaluating the performance of an explanation method: faithfulness, which measures how closely the attribution reflects the inner workings of the underlying model (Jacovi and Goldberg 2020; Chrysostomou and Aletras 2021; Liu et al. 2022; Li et al. 2022). Measuring the faithfulness involves 1) manipulating the input according to the attribution scores of interest and 2) observing the change in the model’s response. We specify our experiment settings below.

Datasets. In our experiments, we use seven citation datasets. Specifically, we use four homophilic datasets, including Cora, Citeseer, Pubmed (Yang, Cohen, and

Salakhutdinov 2016), and one large-scale dataset, Arxiv (Hu et al. 2020), and three heterophilic datasets, including Cornell, Texas, and Wisconsin (Pei et al. 2020). We refer to (Shin et al. 2024) for detailed descriptions including the dataset statistics.

Baseline Methods. Since the analysis of edge attribution from attention in Att-GNNs has not been studied previously, we present our own baseline approaches. We first compare the proposed GATT against another attention-based explanation method (Ying et al. 2019; Luo et al. 2020; Sa´nchezLengeling et al. 2020), named as AVGATT, which attributes each edge as the average of the attention weights over different layers and attention heads. We additionally include random attribution as another baseline (‘Random’), by randomly assigning scores in [0, 1] to each edge.

Attention Reduction. It is generally known that removing $\boldsymbol { e } _ { i , j }$ from the graph to measure its effect may cause the out-of-distribution problem (Hooker et al. 2019; Hase, Xie, and Bansal 2021), a common pitfall for perturbation-based approaches. To mitigate this, we opt mask the attention coefficients (i.e., attention weights before softmax) corresponding to edge $\boldsymbol { e } _ { i , j }$ with zeros in the computation tree, which reduces the effect of $\boldsymbol { e } _ { i , j }$ without removal. Moreover, we do not mask the attention weights after softmax, which cannot occur in a normal feed-forward process of Att-GNNs since the attention distribution is not properly normalized. In other words, we only mask attention coefficients from one edge at a time, which is compared with the original response of the Att-GNN model (Tomsett et al. 2020).

<html><body><table><tr><td>Model</td><td>Dataset</td><td>GATT</td><td>AVGATT</td><td>SA</td><td>GB</td><td>IG</td><td>GNNEx</td><td>PGEx</td><td>GM</td><td>FDnX</td><td>Random</td></tr><tr><td>GAT</td><td>BA-Shones</td><td>0.9596</td><td>0.7977</td><td>0.9563</td><td>0.6239</td><td>0.631</td><td>0.912</td><td>0.8289</td><td>0.5316</td><td>0.917</td><td>0.4975</td></tr><tr><td>GATv2</td><td>BA-Shones</td><td>0.9617</td><td>0.7876</td><td>0.9716</td><td>0.5260</td><td>0.5232</td><td>0.9318</td><td>0.500</td><td>0.5123</td><td>0.9923</td><td>0.4976</td></tr></table></body></html>

Table 3: Experimental results on the explanation accuracy for the synthetic datasets using 3-layer GAT/GATv2s, measured in terms of the AUROC. The results for directly using attention weights as explanation are colored as red. The best and runner-up performers are marked as bold and underline, respectively, for each dataset and model.

Measurement. Denoting the output probability vector of Att-GNN for node $v$ as $\mathbf { p } _ { v }$ and the output probability vector after the attention reduction for $\boldsymbol { e } _ { i , j }$ as $\mathbf { p } _ { v \backslash e _ { i , j } }$ , we measure the model’s behavior from three points of view: 1) decline in prediction confidence $\Delta _ { \mathrm { P C } }$ (Guo et al. 2017) defined as the decrease of the probability for the predicted label (i.e., $\Delta _ { \mathrm { P C } } = \mathbf { p } _ { v } [ k ] - \mathbf { p } _ { v \setminus e _ { i , j } } [ k ]$ , where $k = \arg \operatorname* { m a x } _ { k } \mathbf { p } _ { v } [ k ] )$ , 2) change in negative entropy $\Delta _ { \mathrm { N E } }$ (Moon et al. 2020) defined as the increase of ‘smoothness’ of the probability vector (i.e., $\begin{array} { r } { \Delta _ { \underline { { \mathrm { N E } } } } = - \sum \mathbf { p } _ { v \backslash e _ { i , j } } \log \mathbf { p } _ { v \backslash e _ { i , j } } + \sum \mathbf { p } _ { v } \log \mathbf { p } _ { v } ) } \end{array}$ , which also reflects the model’s confidence, a nd 3) change in prediction $\Delta _ { \mathrm { P } }$ (Tomsett et al. 2020), which observes whether $k \neq k ^ { \prime }$ , where $\mathrm { \ u r g { m a x } } _ { k } \ : \mathbf { p } _ { v } [ k ]$ and arg $\operatorname* { m a x } _ { k ^ { \prime } }$ ${ \bf p } _ { v \backslash e _ { i , j } } [ k ^ { \prime } ]$ , where $\mathbf { p } _ { v } [ k ]$ is the $k$ -th entry of $\mathbf { p } _ { v }$ .

Quantitative Analysis of Faithfulness. We investigate the relationship between the model’s output difference from attention reduction following edge attribution scores and the edge attribution scores themselves. In each dataset, we randomly select 100 nodes as target nodes $\boldsymbol { v }$ and calculate the values of GATT for all edges $( i , j )$ that affect the target node (i.e., $\phi _ { i , j } ^ { v } )$ . We also perform attention reduction for the same edges $( \bar { i } , j )$ and measure $\Delta _ { \mathrm { P C } } , \Delta _ { \mathrm { N E } }$ , and $\Delta _ { \mathrm { P } }$ to observe the correlation between GATT values. Specifically, we adopt the Pearson correlation for $\Delta _ { \mathrm { P C } }$ and $\Delta _ { \mathrm { N E } }$ . For $\Delta _ { \mathrm { P } }$ , we use the area under receiver operating characteristic (AUROC), basically measuring the quality of attribution scores as a predictor of whether the prediction of the target node will change after attention reduction.

Table 2 summarizes the experimental results with respect to the faithfulness on the seven real-world datasets, using pre-trained 2-layer and 3-layer GAT/GATv2s with a single attention head for each dataset. The results strongly indicate that GATT substantially increases the faithfulness of edge attributions of the GAT/GATv2s models, producing a more reliable attribution score compared to AVGATT and random attribution. Although AVGATT shows modest performance in $\Delta _ { \mathrm { P } }$ , it performs poorly in terms of changes in confidence (i.e., $\Delta _ { \mathrm { P C } }$ and $\Delta _ { \mathrm { N E } } \mathrm { , }$ ), sometimes performing worse than random attribution. This is because AVGATT does not account for the proximity effect and contribution adjustment and rather na¨ıvely averages the attention weights over different layers and attention heads with no context of the computation tree. We refer to (Shin et al. 2024) for the detailed results on the consistent trend on multi-head attention, as well as additional results via visualizations by plotting histograms for $\Delta _ { \mathrm { P } }$ , which indicates that GATT successfully assesses whether the model prediction changes after attention

reduction.

# 3.2 Does Attention Reveal Accurate Graph Explanations?

We evaluate the edge attributions of GATs in comparison with ground truth explanations. Since only the synthetic datasets are equipped with proper ground truth explanations, we only use these datasets during evaluations.

Datasets. We use the BA-shapes and Infection synthetic benchmark datasets. BA-shapes (Ying et al. 2019) attaches 80 house-shaped motifs to a base graph made from the Barab´asi-Albert model with 300 nodes, where the edges included in the motif are set as the ground truth explanations. Infection benchmark (Faber, Moghaddam, and Wattenhofer 2021) generates a backbone graph from the Erdo¨s-Re´nyi model; then, a small portion of the nodes are assigned as ‘infected’, and the ground truth explanation is the path from an infected node to the target node. We expect that edge attributions should highlight such ground truth explanations for GATs with sufficient performance.3

Baseline Methods. In our experiments, we mainly compare among attention-based edge attribution calculation methods (i.e., GATT and AVGATT) including Random attribution. Additionally, we consider seven popular post-hoc explanation methods: Saliency (SA) (Simonyan, Vedaldi, and Zisserman 2014), Guided Backpropagation (GB) (Springenberg et al. 2015), Integrated Gradient (IG) (Sundararajan, Taly, and Yan 2017), GNNExplainer (GNNEx) (Ying et al. 2019), PGExplainer (PGEx) (Luo et al. 2020), GraphMask (GM) (Schlichtkrull, Cao, and Titov 2021), and FastDnX (FDnX) (Pereira et al. 2023). We emphasize that post-hoc explanation methods are treated as a complementary tool of inherent explanations, thus belonging to a different category (Du, Liu, and Hu 2020). However, we include them for a more comprehensive comparison.

Experimental Results. Table 3 summarizes the results on the explanation accuracy for two synthetic datasets with ground truth explanations. As in prior studies (Ying et al. 2019; Luo et al. 2020), we treat evaluation as a binary classification of edges, aiming to predict whether each edge belongs to ground truth explanations by using the attribution scores as probability values. In this context, we adopt the AUROC as our metric. For both datasets, we observe that

Table 4: Properties of different edge attribution methods.   

<html><body><table><tr><td>Method</td><td>GATT</td><td>GATTsIM</td><td>GATTAVG</td><td>AVGATT</td></tr><tr><td>(P1)Proximity effect</td><td></td><td></td><td>×</td><td>×</td></tr><tr><td>(P2) Contribution adjustment</td><td></td><td></td><td></td><td>×</td></tr></table></body></html>

Table 5: Performance comparison among different edge attribution calculation methods for GATs.   

<html><body><table><tr><td>Dataset</td><td>Model</td><td>GATT</td><td>GATTSIM</td><td>GATTAVG</td><td>AVGATT</td></tr><tr><td rowspan="2">Cora</td><td></td><td></td><td>0.7708</td><td></td><td></td></tr><tr><td>2-layer</td><td>0.8477</td><td></td><td>0.8109</td><td>0.1768</td></tr><tr><td rowspan="2">Citeseer</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>2-layer</td><td>0.8516</td><td>0.8058</td><td>0.4761</td><td>0.2116</td></tr><tr><td rowspan="2">Pubmed</td><td>2-layer</td><td>0.8812</td><td>0.7683</td><td>0.5915</td><td>0.1648</td></tr><tr><td>3-layer</td><td>0.8489</td><td>0.4197</td><td>0.8302</td><td>0.0592</td></tr></table></body></html>

GATT is much superior to AVGATT. Even compared to the representative post-hoc explanation methods, GATT shows a surprisingly competitive performance. For Infection, GATT shows the best performance, and while GATT places second and third for BA-Shapes, it still achieves over 0.95 AUROC scores. This indicates that the attention weights can inherently capture the GAT/GATv2s’ behavior as long as the attribution calculation is provided by GATT.

# 3.3 Ablation & Case Study

Ablation Study. GATT in Definition 2.4 is developed in the sense of satisfying the two design principles (i.e., proximity effect $( \mathbf { P 1 } )$ and contribution adjustment (P2)). We now perform an ablation study to validate the effectiveness of each design element using the GAT model. To this end, we devise two variants $\mathbf { G A T T _ { S I M } }$ and $\mathbf { G A T T _ { A V G } }$ by simply adding all attention weights uniformly corresponding to the target edge in the computation tree and replacing the weighted summation in Eq. (2) with averaging to remove the effects of the proximity effect, respectively. More specifically, $\mathbf { G A T T _ { S I M } }$ and $\mathbf { G A T T _ { A V G } }$ are defined as

$$
\begin{array} { l } { { \displaystyle \sum _ { m ^ { \prime } = 1 } ^ { L } \sum _ { \lambda _ { i , j , v } ^ { m ^ { \prime } } \in \Lambda _ { v } ^ { m ^ { \prime } } ( e _ { i , j } ) } \alpha [ \lambda _ { i , j , v } ^ { m } ] ( 1 ) \mathrm { , ~ a n d ~ } } } \\ { { \displaystyle \frac { 1 } { | \Lambda _ { v } ( e _ { i , j } ) | } \sum _ { m ^ { \prime } = 1 } ^ { L } \sum _ { \lambda _ { i , j , v } ^ { m ^ { \prime } } \in \Lambda _ { v } ^ { m ^ { \prime } } ( e _ { i , j } ) } C ( \alpha [ \lambda _ { i , j , v } ^ { m } ] ) \alpha [ \lambda _ { i , j , v } ^ { m } ] ( 1 ) \mathrm { , ~ } } } \end{array}
$$

respectively. The properties of different edge attribution calculation methods are summarized in Table 4.

We compare the performance among GATT, $\mathbf { G A T T _ { S I M } }$ , $\mathrm { G A T T _ { A V G } }$ , and AVGATT by running experiments with respect to the faithfulness on the Cora, Citeseer, and Pubmed datasets using GATs. Table 5 summarizes the results of ablation by reporting the Pearson’s coefficient values for $\Delta _ { \mathrm { P C } }$ . We observe that GATT consistently outperforms both $\mathbf { G A T T _ { S I M } }$ and $\mathbf { G A T T _ { A V G } }$ for all cases. In particular, we observe the performance degradation of $\mathbf { G A T T _ { S I M } }$ is generally more severe for 3-layer GATs. This is because the effects of the contribution adjustment $( \mathbf { P } \mathbf { 2 } )$ and the cardinality of $\Lambda _ { v } ( e _ { i , j } )$ are more significant in a 3-layer GAT since the length of each attention flow is longer and the number of flows to consider is much higher compared to the case of 2-layer GATs.

![](images/ed2300208d3d3c2950a6c7e63a1fe316d9a134d1c4705f30ec3463a08a4709c3.jpg)  
Figure 3: Case study on the BA-Shapes and Infection datasets for a 2-layer GAT.

Case Study. We conduct case studies on the BA-Shapes and Infection datasets for a 2-layer GAT, while visualizing how different methods behave. In Figure 3, each of two cases shows a randomly selected target node (marked as orange) and the edges in ground truth explanations (blue edges). We aim to observe how much the attribution scores from GATT, $\mathbf { G A T T _ { A V G } }$ and $\mathbf { G A T T _ { S I M } }$ focus on the ground truth explanation edges. Indeed, for both datasets, GATT focuses primarily on the edges in ground truth explanations, while the attribution scores from $\mathbf { G A T T _ { S I M } }$ and AVGATT tend to be more spread throughout the entire 2-hop local graph. This indicates that the attention weights in the GAT indeed recognize the ground truth explanations under GATT calculations. In the case of $\mathbf { G A T T _ { A V G } }$ , the attribution patterns are not much different from GATT in BA-Shapes. However, $\mathbf { G A T T _ { A V G } }$ in the Infection dataset attributes its attribution scores to a single self-loop edge that does not belong to the ground truth explanations, failing to provide adequate explanations. Interestingly, on BA-Shapes, GATT tends to strongly emphasize edges that are closer in proximity even within the houseshaped motifs, which coincides with the pitfall addressed in (Faber, Moghaddam, and Wattenhofer 2021). Further extensive case studies including more target nodes and 3-layer models exhibit a similar tendency to Figure 3, which we refer to (Shin et al. 2024).

# 4 Conclusion and Future work

In this study, we have investigated the largely underexplored problem of interpreting Att-GNNs. Although Att-GNNs were not considered as a candidate for inherently explainable models, our empirical evaluations have demonstrated affirmative results when our proposed method, GATT, built upon the computation tree, can be used to effectively calculate edge attribution scores. Although GATT is generally applicable, this work does not include a systematic analysis on how different designs of attention weights will interact with GATT, which we leave for future work.

# Acknowledgments

This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2021R1A2C3004345, No. RS-2023- 00220762).