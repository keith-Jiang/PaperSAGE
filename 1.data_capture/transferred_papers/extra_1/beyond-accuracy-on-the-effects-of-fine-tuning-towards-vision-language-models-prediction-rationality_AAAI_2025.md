# Beyond Accuracy: On the Effects of Fine-tuning Towards Vision-Language Model’s Prediction Rationality

Qitong Wang, Tang Li, Kien X. Nguyen, Xi Peng

DeepREAL Lab, Department of Computer & Information Sciences, University of Delaware {wqtwjt, tangli, kxnguyen, xipeng}@udel.edu

# Abstract

Vision-Language Models (VLMs), such as CLIP, have already seen widespread applications. Researchers actively engage in further fine-tuning VLMs in safetycritical domains. In these domains, prediction rationality is crucial: the prediction should be correct and based on valid evidence. Yet, for VLMs, the impact of fine-tuning on prediction rationality is seldomly investigated. To study this problem, we proposed two new metrics called Prediction Trustworthiness and Inference Reliability. We conducted extensive experiments on various settings and observed some interesting phenomena. On the one hand, we found that the welladopted fine-tuning methods led to more correct predictions based on invalid evidence. This potentially undermines the trustworthiness of correct predictions from fine-tuned VLMs. On the other hand, having identified valid evidence of target objects, fine-tuned VLMs were more likely to make correct predictions. Moreover, the findings are also consistent under distributional shifts and across various experimental settings. We hope our research offer fresh insights to VLM fine-tuning.

Code — https://github.com/deep-real/vlm-pred-rationality Extended version — https://arxiv.org/pdf/2412.13333

# Introduction

Vision-Language Models (VLMs), such as CLIP (Radford et al. 2021), have recently begun to see widespread adoption in high-stakes applications, such as healthcare (Wang et al. 2022b) and autonomous driving (Chen et al. 2023). A common practice in utilizing VLMs involves undertaking further fine-tuning (Goyal et al. 2023; Wortsman et al. 2022a; Wang et al. 2022b) in these models to their specific tasks rather than training deep models from scratch. While existing studies have evaluated mainstream fine-tuning methods, they have primarily focused on prediction accuracy (Kumar et al. 2022; Wortsman et al. 2022b; Goyal et al. 2023), overlooking an essential aspect: prediction rationality, where model predictions should not only be accurate but also grounded in valid evidence. Besides, the current academic community widely accepts that “clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself.” (Hendricks et al. 2016) One significant reason is that neglecting the model’s prediction rationality will cause severe consequences in safety-critical domains. For example, doctors employ a fine-tuned VLM which can accurately predict the presence of cancer tumors from X-ray images, to help decision-making. If its predictions are based on erroneous reasons: the input’s background instead of tumor region, doctors will lack trust in fine-tuned VLM, leading them to disregard the usage of the model. Therefore, in this paper, we study a crucial yet seldom investigated question: how do mainstream fine-tuning methods affect the rationality of VLM predictions?

To systematically study this question, we propose two new metrics to evaluate the rationality of VLM predictions after fine-tuning: (1) Prediction Trustworthiness (PT): the ratio of correct predictions with valid evidence overall correct predictions. (2) Inference Reliability (IR): the percentage of correct predictions given that the model has identified valid evidence of target objects. To assess whether the model focuses on valid evidence for the image classification task, we measure if the generated explanation heatmap from VLMs focuses on the target objects, based on the “Relevant Mass Accuracy (RMA)” score (Brandt, Raatjens, and Gaydadjiev 2023). We study the mainstream methods including “Zero-Shot” (ZS), “Linear-Probing” (LP), “Finetune Like CLIP Pretrain” (FLCP), and standard “Fine-tuning” (FT). We conducted extensive experiments and have obtained novel and consistent findings. Our results reveal that widely used fine-tuning methods exhibit significant limitations, yet they also possess certain advantages. Our key findings are summarized as follows:

Will mainstream fine-tuning methods hurt the rationality of VLM predictions? Surprisingly yes! With our proposed “Prediction Trustworthiness” metric, fine-tuning results in more appearance of samples with correct predictions based on invalid evidence than zero-shot, making the correct predictions untrustworthy. For instance, with the ALBEF-ViT-B/16 model, compared with ZS, the PT scores of LP, FLCP, and FT drop $1 7 . { \overset { } { 2 } } \%$ , $1 3 . 8 5 \%$ and $2 7 . 3 1 \%$ respectively, on CalTech-101 (Li et al. 2022b) dataset, despite improving prediction accuracies. And with the CLIP

ViT-B/16 model, compared with ZS, the PT scores of LP, FLCP, and FT drop $6 . 4 \%$ , $5 . 6 5 \%$ , and $4 . 0 7 \%$ respectively, on ImageNet-1K (Russakovsky et al. 2015) dataset. Notably, existing work (Goyal et al. 2023) highlights the effectiveness of fine-tuning for VLMs, asserting that FLCP consistently improves prediction accuracy and should be considered the “standard” method for fine-tuning CLIP. However, our findings suggest that this conclusion does not hold when evaluating the rationality of VLM predictions. This discrepancy underscores the importance of considering different possibilities when evaluating prediction rationality.

Will valid evidence help enhance predictions made by fine-tuned VLMs? Yes. Using our “Inference Reliability” metric, we find that when VLMs focus on valid evidence of target objects, the prediction accuracy of fine-tuned VLMs improves. For example, in the ImageNet-1K dataset, with the CLIP-ViT-B/16 model, LP, FLCP, and FT outperform ZS in IR scores by $1 2 . 6 \%$ , $8 . 6 7 \%$ , and $1 6 . 9 2 \%$ respectively. Existing works (Kumar et al. 2022; Wortsman et al. 2022b; Goyal et al. 2023), which study the positive impacts of VLM fine-tuning, are limited to the prediction accuracies. Our research provides insights into the impact of fine-tuning VLMs from a novel perspective, highlighting the benefits of fine-tuning in terms of enhancing prediction rationality.

Will out-of-distribution data change our observations? No. There is a critical need to make sure that models work reliably in real-world situations, where the data distribution they encounter might be different from what they were trained on. For instance, the model must maintain stability and effectiveness in autonomous driving applications across various weather conditions. In parallel, previous work (Radford et al. 2021) has demonstrated the remarkable predictive performance of CLIP in both in-distribution and out-of-distribution data. Therefore, we discuss how our observations might change in the context of out-ofdistribution data. We find that all our findings remain consistent across various types and magnitudes of distributional shifts, as demonstrated through experiments in ImageNetC (Hendrycks and Dietterich 2018).

Lastly, we conducted ablation studies to verify the consistency of our findings, which remain consistent across various experimental settings, including different training optimizers, learning rates, explanation heatmap methods, and fine-tuning techniques such as prompt tuning (Zhou et al. 2022) and adapter tuning (Zhang et al. 2022).

Our contribution lies in discovering new findings through extensive experiments across various benchmarks including ImageNet (Russakovsky et al. 2015), which are typical and widely used in the community. We provide novel insights about both the strengths and weaknesses of widely adopted fine-tuning strategies for VLMs, from the perspective of the rationality of VLM predictions. Moreover, our findings remain consistent across evaluation scenarios involving both in-distribution data and out-of-distribution data, as well as under various experimental settings. This paper provides new insights for people to rethink the effects of mainstream fine-tuning methods for VLMs.

![](images/2972c2c7743d6767c81e0b71d15169f85ada402d9c4e6c914b438ee0492f64f5.jpg)  
Figure 1: Both (a) and (b) have low responses to the background while (a) pays more attention to the whole body of the bird and (b) pays more attention to the discriminative feature of the bird (head). Compared with the IoU score between (a) and (b), the difference between them is negligible. Moreover, both achieve correct predictions. Input is from CUB-200-2011 (Wah et al. 2011) dataset. “GT” denotes abbreviation of “Ground Truth” and “Explan” denotes abbreviation of “Explanation”.

# Preliminaries

There has been a surge of people exploring VLMs for their downstream tasks. A typical way is to use them for image classification (Goyal et al. 2023). In our prediction evaluations, we study the image classification task and measure model performances using the top-1 accuracy metric.

We evaluate whether the model provides valid evidence for its predictions by examining whether the explanation heatmap generated by VLMs focuses on the target objects. Specifically, a heatmap that strongly highlights key object regions while showing minimal responsiveness to background pixels indicates valid evidence. Therefore, we rely on the “Relevant Mass Accuracy (RMA)” score (Arras, Osman, and Samek 2022; Brandt, Raatjens, and Gaydadjiev 2023), which satisfies this criterion by measuring how much “mass” one method assigns to pixels within the region of target objects (ground truth). RMA score is calculated by determining the ratio of the total heatmap pixel values within the target object regions, to the sum of all pixel values across the entire heatmap. It requires both the generated explanation heatmap $( H )$ from VLMs and the ground truth explanation mask $( M )$ , whose pixels on the target objects are marked as 1 otherwise marked as 0. RMA score is defined as:

$$
\mathrm { R M A } ( H , M ) = \frac { \sum H \odot M } { \sum H } ,
$$

where $\odot$ represents Hadamard product. Note that the evaluations from many studies (Selvaraju et al. 2020; Arras, Osman, and Samek 2022) require the presence of ground-truth mask for heatmap localization.

We emphasize that the RMA metric provides a more reasonable evaluation for classification tasks compared to metrics like “Intersection over Union (IoU)” used in other works. For instance, Grad-CAM (Selvaraju et al. 2020) relies on the IoU score to measure the overlap between the explanation heatmap and the ground truth mask. However, the IoU score fails to reasonably evaluate two vastly different yet valid pieces of evidence. In Figure 1, we show two explanation heatmaps, (a) and (b), that are from different models.

![](images/42cf5a8a8ba695819cbdc0f4facb239405b533c1185c054496f9bd732b81de3c.jpg)  
Figure 2: Overview of the four quadrants (RR, RW, WR, WW) of Accuracy and Rationale that are utilized to evaluate prediction rationality.

Even though the IoU metric treats them differently, both of them achieve correct predictions with valid evidence. They both exhibit a low response to background pixels. (a) pays attention to the whole body of the bird. (b) is also reasonable because it effectively identifies the distinguishing features of the bird, despite not highlighting the more complete bird region as in (a). This indicates that compared to IoU, RMA evaluation can fairly treat two distinct but valid evidence.

Explanation Heatmap Generation. The method we use is directly from “Generic Attention Attribution” (Chefer, Gur, and Wolf 2021). In this case, the heatmaps are generated from attention maps of the transformer-based model, which is one of the most well-adopted methods, used in recent works including (Mao et al. 2023). It has been demonstrated in existing work (Liu et al. 2022) that it achieves the best faithfulness performance among all well-known explanation methods when applied to transformer-based models. The main idea is Hadamard’s product between attention maps and their gradient to the output. It is defined as:

$$
\overline { { \mathbf { A } } } = \mathbb { E } _ { h } \big ( \big ( \bigtriangledown \mathbf { A } \odot \mathbf { A } \big ) ^ { + } \big ) ,
$$

where $\odot$ is the Hadamard product, $\begin{array} { r } { \nabla \mathbf { A } : = \frac { \partial y _ { t } } { \partial A } } \end{array}$ ∂yt for yt which is the model’s output for the class $t$ that we wish to visualize. $\mathbb { E } _ { h }$ is the mean across the heads dimension. The $^ +$ indicates that the negative contributions are removed before averaging. Note that the class we explain are based on the index given by the annotations instead of predictions.

# Our Proposed Evaluations

We present our evaluation protocols with two criteria in mind. (1) A trustworthy VLM should not produce instances of invalid evidence among samples with correct predictions. (2) When focusing on the correct predicted objects, a reliable VLM should leverage such valid evidence to achieve correct predictions. To determine whether the evidence (or rationale) of the model is correct, we use a threshold of 0.5 on the RMA measure. Specifically, an RMA score of 0.5 or above is considered valid evidence and vice versa. As a result, we achieve four scenarios: RR, RW, WR, and WW (Figure 2) that are used to formalize our two novel metrics:

1. Prediction Trustworthiness (PT). A dependable and trustworthy model should generate valid evidence that corresponds to accurate predictions. Hence, we introduce the “PT” metric, which calculates the proportion of samples where the prediction is “right” and its evidence is also valid or “right” (RR) among all samples with right predictions, defined as:

$$
\mathrm { P T } = \frac { \mathrm { R R } } { \mathrm { R R } + \mathrm { R W } } ,
$$

where “RW” denotes data with the “right” classifications based on invalid or “wrong” evidence. It is evident that an increase in the number of RW samples, i.e. irrational predictions, results in a decrease in PT scores.

2. Inference Reliability $\mathbf { \left( I R \right) }$ . Given that the model could pinpoint the regions of target objects, a reliable model should make correct predictions. Consequently, we introduce the “IR” metric, which calculates the proportion of samples with correct prediction and valid evidence among all samples with valid evidence of target objects, defined as:

$$
\mathrm { I R } = \frac { \mathrm { R R } } { \mathrm { R R } + \mathrm { W R } } ,
$$

where “WR” denotes data with incorrect classifications with valid evidence. An increase in the number of WR samples results in a decrease in IR scores.

# Experiments

# Experimental Setup

Fine-tuning Methods. In this paper, we study fundamental methods including: (1) Zero-Shot (ZS), (2) Linear-Probing (LP), (3) Finetune Like CLIP Pretrain (FLCP), and (4) Finetuning (FT). For detailed information on these methods, please refer to our supplementary material in the extended version of our paper.

Models. We study four VLMs: the first two models are CLIP-ViT-B/32 & 16 (Radford et al. 2021) from OpenAI, which manifest powerful zero-shot performances on image classification. The next two models are ALBEF-ViTB/16 (Li et al. 2021), pretrained on 14M image-text pairs, and BLIP-ViT-B/16 (Li et al. 2022c), pretrained on 129M image-text pairs, both developed by Salesforce. Their performances on the image classification task are also investigated in many works (Jonathan Roberts and Albanie 2023; Wang et al. 2022a).

Fine-tuning Setups. We maintain a consistent batch size and training epoch across all three fine-tuning methods (LP, FLCP, FT) for the same dataset and model. We employ the Adam (Kingma and Ba 2014) optimizer during the finetuning. For more details about fine-tuning, please consult our supplementary material.

Datasets. In this paper, we conduct experiments on several datasets, including ImageNet (Russakovsky et al. 2015), CalTech-101 (Li et al. 2022b), Stanford-Dogs (Khosla et al. 2011), CUB-200-2011 (Wah et al. 2011), and ImageNetC (Hendrycks and Dietterich 2018). In CUB-200-2011 and CalTech-101 datasets, the 0-1 segmentation mask annotations directly serve as ground truth explanation masks. For images with bounding box annotations surrounding predicted instances (ImageNet, ImageNet-C, Stanford-Dogs), we generate ground truth explanation masks as follows: given initial masks whose pixel values are all zero, we mark the mask areas within boxes as one. For more detailed information about these datasets, please refer to the supplementary material.

Table 1: Comparisons of four methods regarding prediction accuracy $( \% )$ . The best-averaged score among the four methods is bolded, while the second-place averaged score is underlined. Due to the space limit, we abbreviate the names of datasets. Here, “IN”, “CT”, “SD”, “CUB” denote “ImageNet-1K”, “CalTech- $1 0 1 ^ { \prime \prime }$ , “Stanford-Dogs”, “CUB200-2011” respectively.   

<html><body><table><tr><td rowspan="2">Methods</td><td rowspan="2">VLMs</td><td colspan="4">Datasets</td><td rowspan="2">Avg.</td></tr><tr><td>IN</td><td>CT</td><td>SD</td><td>CUB</td></tr><tr><td rowspan="4">ZS</td><td>ALBEF-ViT-B/16</td><td>46.48</td><td>77.02</td><td>29.25</td><td>12.43</td><td rowspan="4">53.74</td></tr><tr><td>BLIP-ViT-B/16</td><td>46.30</td><td>85.89</td><td>32.38</td><td>16.88</td></tr><tr><td>CLIP-ViT-B/16</td><td>63.30</td><td>84.22</td><td>60.61</td><td>54.94</td></tr><tr><td>CLIP-ViT-B/32</td><td>58.41</td><td>84.79</td><td>54.62</td><td>52.33</td></tr><tr><td rowspan="4">LP</td><td>ALBEF-ViT-B/16</td><td>72.03</td><td>90.38</td><td>65.10</td><td>48.46</td><td rowspan="4">72.50</td></tr><tr><td>BLIP-ViT-B/16</td><td>72.46</td><td>90.26</td><td>64.23</td><td>47.77</td></tr><tr><td>CLIP-ViT-B/16</td><td>76.69</td><td>94.64</td><td>74.14</td><td>70.14</td></tr><tr><td>CLIP-ViT-B/32</td><td>72.21</td><td>93.09</td><td>67.27</td><td>61.08</td></tr><tr><td rowspan="4">FLCP</td><td>ALBEF-ViT-B/16</td><td>77.58</td><td>95.85</td><td>77.88</td><td>77.27</td><td rowspan="4">80.99</td></tr><tr><td>BLIP-ViT-B/16</td><td>78.67</td><td>94.99</td><td>77.89</td><td></td></tr><tr><td>CLIP-ViT-B/16</td><td>72.41</td><td>96.20</td><td></td><td>68.85</td></tr><tr><td>CLIP-ViT-B/32</td><td>70.81</td><td>95.74</td><td>80.70 75.70</td><td>80.76 74.49</td></tr><tr><td rowspan="4">FT</td><td>ALBEF-ViT-B/16</td><td>80.82</td><td>95.91</td><td></td><td></td><td rowspan="4">81.63</td></tr><tr><td></td><td></td><td></td><td>81.32</td><td>80.48</td></tr><tr><td>BLIP-ViT-B/16 CLIP-ViT-B/16</td><td>80.75</td><td>92.74</td><td>78.68</td><td>68.98</td></tr><tr><td>CLIP-ViT-B/32</td><td>81.19 76.62</td><td>93.03 94.30</td><td>81.56 72.42</td><td>79.25 68.07</td></tr></table></body></html>

# Weaknesses of Fine-tuning

Question: Will mainstream fine-tuning methods hurt the rationality of VLM predictions?

Answer: Surprisingly yes! The well-adopted fine-tuning methods decrease the trustworthiness of VLM predictions in most settings: causing more samples with correct predictions based on invalid evidence.

Although fine-tuning is able to improve the prediction accuracies of VLMs (see Table 1), we find mainstream fine-tuning methods lead to worse prediction trustworthiness, as shown in Table 2. For instance, in the ImageNet1K dataset, with CLIP-ViT-B/16 model, compared with ZS, fine-tuning deteriorates “Prediction Trustworthiness (PT)” performances by $6 . 4 \%$ , $5 . 6 5 \%$ and $4 . 0 7 \%$ respectively. Our experimental results confirm the significant drawbacks of mainstream fine-tuning methods for VLMs: fine-tuning results in more instances where predictions are correct but the evidence which VLMs base on is invalid. This results in a reduced level of trustworthiness to VLM predictions. Lastly, there are rare exceptions with increased PT scores. This is likely due to the low zero-shot prediction accuracy of ALBEF $( 1 2 . 4 3 \% )$ and BLIP $( 1 6 . 8 8 \% )$ on CUB-200-2011. Finetuning introduces the missing knowledge to these models,

leading to increased PT.

To further support our observation, we provide visualizations of the explanation heatmaps in Figure 3. We observe that widely adopted fine-tuning methods often amplify the responses of VLMs to pixels containing information irrelevant to the predicted objects. For instance, from the leftmost first-row comparisons, fine-tuning makes VLMs enhance responses on the human body or background instead of the hat (predicted category). Here we only show results on the CLIP-ViT-B/32 model with ImageNet-1K datasets due to space constraints. Please refer to our supplementary material for more visualizations.

Why does finetuning decrease trustworthiness? (1) VLMs tend to exploit the easiest path to minimize loss during finetuning, often picking up on spurious correlations or shortcuts present in the data. For instance, if all images of a particular class contain a common watermark or background, VLMs may associate that feature with the class label instead of learning the actual characteristics of the object. (2) Standard fine-tuning objectives usually prioritize improving prediction accuracy, but they do not account for the validity of the evidence used. As a result, there is no built-in mechanism to guide the model to focus on valid evidence.

In recent years, there have been some discussions regarding the excellence of fine-tuning for VLMs. For example, existing work (Goyal et al. 2023) shows that FLCP leads to uniformly better prediction performances. They claim that FLCP should be adopted as the “standard” method for finetuning CLIP. However, based on our discoveries, we contend that this conclusion doesn’t apply when considering the rationality of VLM predictions. Although FLCP significantly enhances VLMs’ prediction accuracies, we find that FLCP leads VLMs to provide more invalid evidence when making correct predictions, weakening the prediction trustworthiness of VLMs than ZS. This disparity highlights the significance of considering different possibilities when evaluating VLMs’ prediction rationality.

# Strengths of Fine-tuning

Question: Will valid evidence help enhance predictions made by fine-tuned VLMs?

Answer: Yes, they exhibit good inference reliability; i.e., when focusing on the valid evidence of target objects, finetuned VLMs are more likely to make correct predictions.

This phenomenon indicates better inference reliability of fine-tuning compared with ZS, as shown in Table 2. For example, in the ImageNet-1K dataset, with the CLIP-ViT-B/16 model, LP, FLCP, and FT outperform ZS by $1 2 . 6 \%$ , $8 . 6 7 \%$ , and $1 6 . 9 2 \%$ respectively; with the CLIP-ViT-B/32 model, LP, FLCP, and FT outperform ZS by $1 3 . 8 2 \%$ , $1 0 . 7 5 \%$ , and $1 8 . 2 5 \%$ respectively. This indicates that fine-tuning approaches contribute to less WR than ZS. When VLMs identify valid evidence for target objects, fine-tuning is more likely to produce correct predictions.

Existing works (Kumar et al. 2022; Wortsman et al.

Table 2: Comparisons of four methods with proposed “PT” and “IR” metrics. Here we observe that mainstream fine-tuning methods come with both strengths and weaknesses. We show that fine-tuning mostly leads to a worse capability of prediction trustworthiness but enhances the inference reliability of VLMs than the ZS method. The best-averaged score among the four methods is bolded, while the second-place averaged score is underlined.   

<html><body><table><tr><td rowspan="2">Evaluations</td><td rowspan="2">Methods</td><td rowspan="2">VLMs</td><td colspan="4">CalTech-10Stanford-Dogs</td><td rowspan="2">Avg.</td></tr><tr><td>ImageNet-1K</td><td></td><td></td><td>CUB-200-2011</td></tr><tr><td rowspan="10">Prediction TrusPT,.rthiness</td><td rowspan="4">ZS</td><td>ALBEF-ViT-B/16</td><td>90.61</td><td>76.28</td><td>95.02</td><td>49.31</td><td rowspan="4">71.26</td></tr><tr><td>BLIP-ViT-B/16</td><td>89.01</td><td>61.72</td><td>93.95</td><td>23.93</td></tr><tr><td>CLIP-ViT-B/16</td><td>87.05</td><td>62.99</td><td>92.96</td><td>29.38</td></tr><tr><td>CLIP-ViT-B/32</td><td>89.39</td><td>73.44</td><td>94.58</td><td>30.57</td></tr><tr><td rowspan="3">LP</td><td>ALBEF-ViT-B/16</td><td>82.37</td><td>59.08</td><td>90.30</td><td>19.91</td><td rowspan="3">64.78</td></tr><tr><td>BLIP-ViT-B/16</td><td>80.36</td><td>52.57</td><td>92.63</td><td>12.98</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="3">FLCP</td><td>CLIP-ViT-B/32</td><td>84.05</td><td>68.22</td><td>92.76</td><td>35.89</td><td rowspan="3">67.95</td></tr><tr><td>ABLIP-VT-B/6</td><td>87.7</td><td></td><td>91</td><td>3617</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="3">FT</td><td></td><td>85.48</td><td></td><td></td><td></td><td rowspan="3"></td></tr><tr><td>CLIP-ViT-B/32 ALBEF-ViT-B/16</td><td></td><td>71.29</td><td>91.59</td><td>23.84</td></tr><tr><td></td><td>86.28</td><td>48.97</td><td>92.22</td><td>24.98</td></tr><tr><td rowspan="3"></td><td>BLIP-ViT-B/16</td><td>85.54</td><td>39.96</td><td>93.13</td><td>25.85</td><td rowspan="3">67.01</td></tr><tr><td>CLIP-ViT-B/32</td><td>86.29</td><td>80.01</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>94.17</td><td>55.43</td></tr><tr><td rowspan="9">Inference ReRiabi Ty</td><td rowspan="3">ZS</td><td>ALBEF-ViT-B/16</td><td>48.95</td><td>76.74</td><td>30.56</td><td>16.43</td><td rowspan="3">56.65</td></tr><tr><td></td><td>4.63</td><td>90.08</td><td>3.87</td><td>18.92</td></tr><tr><td>BLIP-ViT-B/16</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="3">LP</td><td>CLIP-ViT-B/32</td><td>61.09</td><td>85.23</td><td>56.12</td><td>56.80</td><td rowspan="3">75.67</td></tr><tr><td>ALBEF-ViT-B/16</td><td>74.76</td><td>92.56</td><td>66.21</td><td>55.46</td></tr><tr><td>BLIP-ViT-B/16</td><td>74.78</td><td></td><td></td><td></td></tr><tr><td rowspan="3"></td><td></td><td></td><td>90.89</td><td>65.11</td><td>59.91</td><td rowspan="3"></td></tr><tr><td>CLIP-ViT-B/32</td><td>74.91</td><td>93.76</td><td>68.53</td><td>67.37</td></tr><tr><td>ABLIPVT-B/6</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="3">FLCP</td><td></td><td></td><td></td><td></td><td></td><td rowspan="3">81.71</td></tr><tr><td></td><td>8</td><td></td><td></td><td>7</td></tr><tr><td>CLIP-ViT-B/32</td><td>71.84</td><td>95.46</td><td>76.43</td><td>73.87</td></tr><tr><td rowspan="3">FT</td><td>ALBEF-ViT-B/16</td><td>82.95</td><td>94.41</td><td>81.93</td><td>81.87</td><td rowspan="3">83.52</td></tr><tr><td></td><td>82.86</td><td>91.5</td><td>79.18</td><td>85.22</td></tr><tr><td>BLIP-ViT-B/16</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="3"></td><td>CLIP-ViT-B/32</td><td>79.34</td><td>93.86</td><td>73.22</td><td>72.72</td><td rowspan="3"></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

2022b; Goyal et al. 2023) are limited to the impact of mainstream VLM fine-tuning methods regarding predictive accuracies, ignoring their positive impacts on VLM prediction rationality. In this paper, we have analyzed and explored the benefits of fine-tuning VLMs from a new perspective. Our experimental results show that fine-tuning has its merits and is not completely worthless for the prediction rationality of VLMs.

In summary, we conducted extensive experiments to validate the existing mainstream VLM fine-tuning methods in terms of both their strengths and weaknesses from a prediction rationality perspective. On the one hand, fine-tuning leads to good inference reliability: when provided with valid evidence of target objects, fine-tuned VLMs are more likely to generate accurate predictions. On the other hand, we also confirm that mainstream fine-tuning methods tend to hurt the inherent capabilities of VLMs, specifically in terms of prediction trustworthiness. These are aspects that merit attention from the community of machine learning.

# Analysis on Out-of-Distribution Data

# Question: Will out-of-distribution data change our observations?

Answer: No, all findings remain consistent.

Distributional shifts has garnered significant attention in the field of machine learning (Qiao, Zhao, and Peng 2020; Qiao and Peng 2023). During the fine-tuning, the distributional discrepancy between the fine-tuning and testing data is worth considering. Real-world data distributions can change due to factors such as time, location, and environment. Testing on out-of-distribution data helps simulate these changes, ensuring the model performs well in diverse scenarios. For example, in autonomous driving, the models need to remain stable in multiple weather conditions.

In this section, we study the fine-tuning methods when testing on out-of-distribution data. Here we use the ImageNet-C dataset, which includes multiple corruption categories and levels of severity. As shown in Figure 4, our key

![](images/50acf95cd5c09c67b2410e88ce9aac5faec5db7365260c6930763ad5ec085fa5.jpg)  
Figure 3: Visualization comparisons among different methods. Compared with zero-shot (ZS), current mainstream fine-tuning methods (LP, FLCP, and FT) for VLMs tend to show enhanced responses in background pixels that are irrelevant to predictions. Here we select the samples for which all four methods make correct predictions. Here we display bounding box annotations indicating the positions of the predicted target.

findings are as follows:

1. Fine-tuning on in-distribution data can enhance the prediction accuracy for out-of-distribution data.   
2. However, the mainstream fine-tuning methods still compromise the prediction trustworthiness of VLM, which brings more samples with correct prediction based on invalid evidence, compared with zero-shot.   
3. Fine-tuning tends to enhance the inference reliability of VLMs: when focusing on correct prediction objects, finetuned VLMs are more likely to give correct predictions.

Therefore, we extend our previous findings to scenarios involving out-of-distribution data, demonstrating the consistency of our discoveries.

Our conclusions also remain unaffected when the prediction accuracies decrease caused by corruption strength increases. Therefore, we think our findings may not change with variations in model prediction accuracy.

# Ablations studies

To ensure the consistency of our findings across different experimental settings, we perform a comprehensive series of ablation studies. We investigate the effects under different setups including: (1) Experiments with another popular optimizer: AdamW (Loshchilov and Hutter 2017). (2) Experiments with another widely-used explanation method: gradient of attention $( \nabla \mathbf { A } )$ based (Serrano and Smith 2019) method. The main idea of this method is to utilize the gradient of attention to the output as an explanation heatmap, where $\begin{array} { r } { \nabla \mathbf { A } : = \frac { \partial y _ { t } } { \partial A } } \end{array}$ for $y _ { t }$ which is the model’s output for the class $t$ . (3) Results with different fine-tuning learning rates (abbreviated as “LR”): $5 e - 4$ for “LP”, $3 e - 6$ for “FLCP”, and $2 e { \mathrm { - } } 5$ for “FT”, compared with the original setup, where we set learning rates as $1 e - 3$ for “LP”, $5 e - 6$ for “FLCP”, and $1 e \mathrm { ~ - ~ } 5$ for “FT”. For the original learning rate settings regarding other models and datasets please refer to our supplementary material. Note that the aforementioned three experiments are conducted with the CLIP-ViT-B/32 model on the ImageNet-1K.

As shown in Table 3, our findings remain unaffected with multiple setups. On the one hand, prevalent fine-tuning approaches tend to increase the instances with correct predictions based on invalid evidence, despite the enhancement in prediction accuracy. On the other hand, fine-tuning typically demonstrates strong inference reliability.

Recently, there have been other fine-tuning techniques proposed by the community including prompt tuning (Zhou et al. 2022), and adapter tuning (Zhang et al. 2022). We find that our findings are also consistent under these fine-tuning methods. Due to the space limits please refer to our supplementary material for the related experimental results and introduction of these methods.

# Related Works

# Multimodal Foundation Models

In recent years, there has been a surge of interest in research regarding Vision-Language Models (VLMs). These VLMs (Radford et al. 2021; Li et al. 2021, 2022c; Singh et al. 2022; Jia et al. 2021; Li et al. 2022a,e; Yuan et al. 2021; Li et al. 2022d, 2023; Chen and Wang 2022; Zhong et al. 2022; Kim, Son, and Kim 2021; Chen et al. 2020), have attracted substantial attention due to their remarkable capacity to achieve robust performance, both in zero-shot and fine-tuned scenarios, across a diverse spectrum of visionlanguage-related tasks (Antol et al. 2015; Vinyals and Le 2015; Xie et al. 2019; Suhr et al. 2017). Notably, CLIP (Radford et al. 2021), as a prominent exemplar in this domain, has also demonstrated exceptional zero-shot performance in image classification. The contrastive learning approach it employs has also found applications in fields such as mul

![](images/93eb44ffb7f6d88f2ad6baa352f5186c1ce1b3ea105596b7af94c124f77466c7.jpg)  
Figure 4: Experimental results on out-of-distribution data. Our discoveries remain consistent across various types and magnitudes of distributional shifts. The $\mathbf { X }$ -axis in all figures represents the strength of corruption, where a strength of 0 indicates the results of different methods on the original ImageNet validation data. Due to space constraints, we only show results with CLIP-ViT-B/32 and four types of corruption in the main paper. For more results, please refer to our supplementary material.

Table 3: Ablation studies with prediction accuracy, and our proposed “Prediction Trustworthiness (PT)” and “Inference Reliability (IR)” metrics. Our findings are unaffected under different experimental setups. The best score is bolded.   

<html><body><table><tr><td rowspan="2">Setup</td><td rowspan="2">Evaluations</td><td colspan="4">Methods</td></tr><tr><td>ZS</td><td>LP</td><td>FLCP</td><td>FT</td></tr><tr><td rowspan="2">AdamW Optimizer</td><td>Pred. Acc.(%) ↑</td><td>58.41</td><td>72.22</td><td>70.88</td><td>76.53</td></tr><tr><td>PT(%) ↑</td><td>89.39</td><td>84.23</td><td>85.53</td><td>86.46</td></tr><tr><td></td><td>IR(%) ↑</td><td>61.09</td><td>74.93</td><td>71.93</td><td>79.26</td></tr><tr><td>A</td><td>Pred. Acc.(%) ↑</td><td>58.41</td><td>72.21</td><td>70.81</td><td>76.62</td></tr><tr><td>Explanation</td><td>PT(%) ↑</td><td>74.79</td><td>63.62</td><td>65.21</td><td>65.76</td></tr><tr><td>Heatmap</td><td>IR(%) ↑</td><td>61.18</td><td>75.18</td><td>71.87</td><td>79.68</td></tr><tr><td>Different LRs</td><td>Pred. Acc.(%) ↑</td><td>58.41</td><td>72.28</td><td>70.05</td><td>75.51</td></tr><tr><td>Compared with</td><td>PT(%) ↑</td><td>89.39</td><td>84.72</td><td>86.19</td><td>86.59</td></tr><tr><td>Original</td><td>IR(%) ↑</td><td>61.09</td><td>74.91</td><td>71.21</td><td>78.62</td></tr></table></body></html>

Das, and Saenko 2018) study from the perspective of faithfulness; i.e., how accurately an explanation method reflects the true decision-making process of a model. In parallel, Mao et al. (Mao et al. 2023) propose the concept of a reliable model, emphasizing the importance of the ”doublyright” criterion: both accurate predictions and fine-grained language explanations of model decision-making. Recently, some works (Li, Ma, and Peng 2024a,b) have increasingly required VLM models to deliver not only accurate predictions but also correct rationales. In this paper, we explore the impact of widely accepted fine-tuning methods on the prediction rationality of VLMs for vision tasks such as image classification, providing novel insights about VLM finetuning within the XML research community. And we highlight that faithfulness is beyond the scope of our study due to two reasons. On the one hand, faithfulness evaluations primarily focus on assessing the correctness of heatmap explanation methods. On the other hand, existing work (Liu et al. 2022) verified the superiority of our employed explanation generation method.

tiview analysis (Tian, Krishnan, and Isola 2020) and egocentric video understanding (Wang et al. 2023). Recently, researchers have engaged in fine-tuning (Goyal et al. 2023) VLMs to better adapt them to specific downstream tasks. However, the impact of such training on the prediction rationality of these models remains an open research problem, one that warrants in-depth exploration and investigation.

# Explainable Machine Learning

Explainable Machine Learning (XML) is crucial for promoting transparency, trust, accountability, and fairness in AI systems. Researchers frequently employ techniques to explain neural network operations and decision-making regarding input data. Activation heatmaps such as GradCAM (Selvaraju et al. 2020), visualize important regions for specific classes. In light of the proliferation of transformerbased models (Dosovitskiy et al. 2020), researchers start exploring the feasibility of utilizing attention maps, taking it as a way to provide explanations (Chefer, Gur, and Wolf 2021). In order to evaluate the quality of these explanation generation methods, existing works including (Petsiuk,

# Conclusion

Prediction rationality is an important aspect to consider when fine-tuning Vision-Language Models (VLMs), especially in high-stakes applications. This paper provides a comprehensive assessment of the commonly used finetuning approaches, presenting some insights on both advantages and disadvantages. On the one hand, they generally demonstrate strong inference reliability. More specifically, when focusing on the valid evidence of target objects, the fine-tuned VLMs are more likely to make correct predictions. On the other hand, fine-tuning often results in undermining the trustworthiness of VLM predictions by bringing more data samples with correct predictions based on invalid evidence. We further observe that our discoveries are consistent across various types and magnitudes of distributional shifts, and remain unaffected with multiple setups. To ensure that VLMs can be reliably used in high-stack applications, it will be crucial to study new fine-tuning methods that can improve VLM prediction rationality. We leave it as future works. We expect our research may provide useful experience and advance the study of VLM fine-tuning.