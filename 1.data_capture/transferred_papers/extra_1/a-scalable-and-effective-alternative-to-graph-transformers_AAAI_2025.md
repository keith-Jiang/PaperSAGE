# A Scalable and Effective Alternative to Graph Transformers

Kaan Sancak1, Zhigang $\mathbf { H } \mathbf { u } \mathbf { a } ^ { 2 }$ , Jin Fang2, Yan Xie2, Andrey Malevich2, Bo Long2, Muhammed Fatih Balın1, U¨ mit V. ¸Catalyu¨ rek\*1

1School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA 2Meta AI kaan $@$ gatech.edu, zhua, fangjin, yanxie, amelevich, bolong @meta.com, balin, umit $@$ gatech.edu

# Abstract

Graph Neural Networks (GNNs) have shown impressive performance in graph representation learning, but they face challenges in capturing long-range dependencies due to their limited expressive power. To address this, Graph Transformers (GTs) were introduced, utilizing self-attention mechanism to effectively model pairwise node relationships. Despite their advantages, GTs suffer from quadratic complexity w.r.t. the number of nodes in the graph, hindering their applicability to large graphs. In this work, we present Graph-Enhanced Contextual Operator (GECO), a scalable and effective alternative to GTs that leverages neighborhood propagation and global convolutions to effectively capture local and global dependencies in quasiliniear time. Our study on synthetic datasets reveals that GECO reaches $1 6 9 \times$ speedup on a graph with 2M nodes w.r.t. optimized attention. Further evaluations on diverse range of benchmarks showcase that GECO scales to large graphs where traditional GTs often face memory and time limitations. Notably, GECO consistently achieves comparable or superior quality compared to baselines, improving the SOTA up to $4 . 5 \%$ , and offering a scalable and effective solution for large-scale graph learning.

# Extended version — https://arxiv.org/pdf/2406.12059 Code — https://github.com/kaansancak/GECO

# 1 Introduction

Graph Neural Networks (GNNs) have been state-of-the-art (SOTA) models for graph representation learning showing superior quality across different tasks spanning node, link, and graph level prediction (Gori, Monfardini, and Scarselli 2005; Scarselli et al. 2009; Kipf and Welling 2017; Zhang and Chen 2018; Zhang et al. 2018). Despite their success, GNNs have fundamental limitations that affect their ability to capture long-range dependencies in graphs. These dependencies refer to nodes needing to exchange information over long distances effectively, especially when the distribution of edges is not directly related to the task or when there are missing edges in the graph (Dwivedi et al. 2022b). This limitation can further lead to information over-squashing caused by repeated propagations within GNNs (Li, Han, and Wu 2018; Alon and Yahav 2020; Topping et al. 2022).

Graph Transformers (GTs) (Dwivedi and Bresson 2020; Ying et al. 2021; Wu et al. 2021) were introduced to overcome the limitations of GNNs by incorporating the self-attention mechanism (Vaswani et al. 2017), and achieved SOTA across various benchmarks. GTs can model long-range dependencies by attending to potential neighbors among the entire set of nodes. However, GTs suffer from quadratic complexity which stems from that each node needs to attend to every other node, preventing GTs’ widespread adoption in largescale real-world scenarios. As mini-batch sampling methods for GTs remain under-explored, the primary application of GTs has been on smaller datasets, such as molecular ones (Freitas et al. 2021; Hu et al. 2021; Dwivedi et al. 2022b, 2023a). Consequently, exploring novel efficient and highquality attention replacements remains a crucial research direction to unlock the full potential of GTs for large-scale graphs. Recently, global convolutional language models have emerged as promising alternatives for attention (Romero et al. 2022; Li et al. 2023). Specifically, Hyena (Poli et al. 2023) has demonstrated impressive performance, offering efficient processing of longer contexts with high quality.

In this work, we aim to find an efficient alternative to dense attention mechanisms to scale graph transformers without sacrificing the modeling quality. The key challenge lies in designing an efficient model that can effectively capture both local and long-range dependencies within large graphs. To address this, we propose a novel compact layer called GraphEnhanced Contextual Operator (GECO), which combines local propagations and global convolutions. The convolution filters in GECO encompass all nodes, serving as a substitute for dense attention in the graph domain. GECO consists of four main components: (1) local propagation to capture local context, (2) global convolution to capture global context with quasilinear complexity, (3) data-controlled gating for contextspecific operations on each node, and (4) positional/structural encoder for feature encoding and graph ordering.

Our evaluation has two main objectives: (O1): Matching SOTA GT quality on small graph datasets emphasized by the community. (O2): Scaling to larger graphs where traditional attention mechanisms are impractical due to computational constraints. Extensive evaluations across diverse benchmarks demonstrate that GECO scales to larger datasets and consistently delivers strong quality, often achieving SOTA or competitive results. The main contributions of work include:

• We developed GECO, a compact layer consisting of local and global blocks with quasilinear time complexity. Unlike prior work, GECO is a refined layer without intermediate parameters and nonlinearities between local and global blocks, applying residuals to the layer as a whole. • To our knowledge, GECO is the first to employ a global convolution model to develop a scalable and effective alternative to self-attention based GTs. Notably, it improves computational efficiency while preserving prediction quality, and in most cases, it leads to improvements. • We demonstrated that GECO scales to large graphs that are infeasible for self-attention based GTs due to their intrinsic quadratic complexity. GECO further enhances prediction accuracy by up to $4 . 5 \%$ for large graph datasets. • We demonstrated GECO’s ability to capture long-range dependencies, achieving SOTA on the majority of longrange graph benchmark with improvements up-to $4 . 3 \%$ .

# 2 Background and Related Work

# 2.1 Graph Neural Networks (GNNs)

A graph $G = \left( V , E \right)$ comprises a set of vertices $V$ and edges $E \subseteq V \times V$ . $\mathbf { \Psi } _ { A } ^ { ' } \in \mathbf { \Psi } _ { \mathbb { R } ^ { N \times N } } ^ { \mathbf { \Psi } _ { \mathbf { x } } ^ { \bullet } }$ is the adjacency matrix and a weighted edge $( u \to v ) \subseteq E$ exists between source $u$ and target $\boldsymbol { v }$ if $A _ { u , v } ~ \neq ~ 0$ . The feature matrix $X ^ { ( 0 ) } \in$ $\mathbb { R } ^ { N \times d ^ { ( 0 ) } }$ maps $v$ to a feature vector x(v0) Rd(0) . (v) = $\{ u \mid ( u  v ) \in E \}$ is the incoming neighbors of $v$ . GNNs adopt an Aggregate-Combine framework (Hamilton, Ying, and Leskovec 2017) to compute layer-l representation h(v :

$$
\begin{array} { r l } & { h _ { v } ^ { ( l ) } = \mathrm { C o m b i n e } ^ { ( l ) } \big ( \alpha _ { v } ^ { ( l ) } , h _ { v } ^ { ( l - 1 ) } \big ) } \\ & { \alpha _ { v } ^ { ( l ) } = \mathrm { A g g r e g a t e } ^ { ( l ) } \big ( \big \{ h _ { u } ^ { ( l - 1 ) } : u \in \mathcal { N } ( v ) \big \} \big ) } \end{array}
$$

Additionally, a pooling function generates graph representation, $h _ { G } = \operatorname { \bar { P } o o l } \Bigl ( \Bigl \{ h _ { v } ^ { ( \bar { L } ) } | v \in V \Bigr \} \Bigr )$ .

Challenges. GNNs efficiently scale to large graphs with linear complexity, $\mathcal { O } ( \left| V \right| + \left| E \right| )$ , but they struggle with capturing long-range dependencies, often requiring many hops and nonlinearities for information to traverse distant nodes (Alon and Yahav 2020; Dwivedi et al. 2022b). GTs effectively resolve this via dense pairwise attention.

# 2.2 Graph Transformers (GTs)

Graph Transformers (GTs) generalize Transformers (Vaswani et al. 2017) to graphs. At the core of Transformer lies the multi-head self-attention (MHA) (Vaswani et al. 2017), which maps the input $H \in \mathbb { R } ^ { N \times d }$ to $\dot { \mathbb R } ^ { \dot { N } \times d }$ as:

$$
\begin{array} { l } { { \displaystyle { \cal A } t t n ( H ) = \mathrm { S o f t m a x } \left( \frac { Q K ^ { T } } { \sqrt { d } } \right) } } \\ { { \displaystyle y = \mathrm { S e l f A t t e n t i o n } ( H ) = { \cal A } t t n ( H ) V } } \end{array}
$$

here query $( Q = H W _ { q } )$ ), key $\mathbf { \nabla } ^ { \prime } K = H W _ { k } .$ ), and value $( V =$ $H W _ { v } )$ are linear projections of the input, $W _ { q } , W _ { k } , W _ { v } \in$ Rd×d. The attention matrix Attn(H) captures the pair-wise similarities of the input.

Based on their focus of attention, we group GTs into three: Sparse GTs use the adjacency matrix as an attention mask, allowing nodes to pay attention to their neighbors, which facilitates weighted neighborhood aggregation (Veliˇckovi´c et al. 2018; Xu et al. 2019). Layer GTs use a GNN to generate hop-tokens, followed by MHA on these tokens, where nodes pay attention to their layer embeddings (Chen et al. 2023; Fu et al. 2024). While both Sparse and Layer GTs use attention, they still struggle with long-range dependencies as their attention is restricted to fixed number of hops. Dense GTs use attention on fully connected graph, enabling nodes to pay attention to all the other nodes regardless of their distances. GTs incorporate positional encodings (PE) to provide topological information to otherwise graph-unaware models (Dwivedi and Bresson 2020; Kreuzer et al. 2021; Ying et al. 2021; Chen, O’Bray, and Borgwardt 2022; Zhao et al. 2023; Ma et al. 2023). Refer to the extended version detailed related work.

Challenges. Dense GTs introduce computational and memory bottlenecks due to their increased complexity from $\mathcal { O } ( \left| V \right| + \left| E \right| )$ to $\mathcal { O } ( | V | ^ { 2 } )$ , restricting their application to large graphs. GraphGPS (Rampasek et al. 2022) offers a modular framework that combines GNNs with a global attention module, including subquadratic Transformer approximations (Zaheer et al. 2020; Kreuzer et al. 2021). Unfortunately, the subquadratic models compromise quality while MHA based ones struggle with scalability. Therefore, finding a subquadratic attention replacement with a good quality remains a challenge, and our work is dedicated to tackle this problem. Other Related Work. Exphormer (Shirzad et al. 2023) enhances GraphGPS by using attention on expander graphs. (He et al. 2023) generalizes ViT (Dosovitskiy et al. 2020) and MLP-Mixer (Tolstikhin et al. 2021) to graphs. (Zhang et al. 2022) formulates an adversary bandit problem to sample nodes. HSGT (Zhu et al. 2023) learns multi-level hierarchies via coarsening. GOAT (Kong et al. 2023) uses dimensionality reduction to reduce computational cost of MHA. (Diao and Loynd 2023) utilizes additional edge updates.

# 2.3 Attention Alternatives

Consider an input $u$ of length $N$ and a filter $z$ . A circular convolution can be computed at each position $t$ of the input $u$ , ranging from 0 to $N - 1$ , as follows:

$$
y _ { t } = ( u * z ) _ { t } = \sum _ { i = 0 } ^ { N - 1 } u _ { i } z _ { ( t - i ) \mathrm { ~ m o d ~ } } N
$$

where we assume a single-channel input and filter, which can be easily extended to multi-channel. CNNs (LeCun et al. 1998) optimize $z _ { t }$ at every $K$ steps, where $K$ is a fixed filter size. This explicit parametrization captures local patterns within every $K$ steps. Alternatively, implicit parameterizations represent the filter as a learnable function (Tay et al. 2021; Gu, Goel, and Re 2021; Romero et al. 2022; Fu et al. 2023). Convolutions can be efficiently computed through Fast Fourier Transform (FFT) in quasilinear time, offering a significant advantage.

A convolution is referred to as a global convolution when the filter has the same length as the input. Global convolutional models have demonstrated the ability to capture longer contexts through pairwise interactions (dot products) at any input position by proper filter parametrization, offering a promising alternative to attention (Gu, Goel, and Re 2021; Romero et al. 2022; Li et al. 2023). Recently, Hyena (Poli et al. 2023) proposed a sequence model that combines short explicit convolutions and global implicit convolutions using a similar global filter design as CKConv and SGConv (Romero et al. 2022; Li et al. 2023), and it stands out by matching Transformer’s quality in quasilinear time. Please refer to the extended version for details. However, to the best of our knowledge, there has been no prior work dedicated to designing and utilizing global convolutional models for graphs.

# 3 Proposed Architecture: GECO

We present Graph-Enhanced Contextual Operator (GECO), a novel compact layer developed to replace dense attention with quasilinear time and memory complexity. It draws inspiration from recent advancements in global convolutional models and offers a promising approach to capture local and global dependencies with subquadratic operators. Unlike Hyena, which focuses on sequences, GECO is designed for graphs, combining local propagations with global convolutions with random permutation strategies. By utilizing the topological information of the adjacency matrix, it effectively captures local dependencies. Furthermore, it introduces a new global convolution filter design for graphs to capture global dependencies.

As illustrated in Figure 1, GECO starts with positional encodings and proceeds through multiple layers of GECO, each followed by a feed-forward neural network (FFN). We introduce the main components in the following subsections.

# 3.1 Graph Structural/Positional Encodings

Structural and positional encodings play a pivotal role in the realm of GTs. In our approach, we follow the foundational work established in prior literature concerning these encodings (Dwivedi and Bresson 2020; Kreuzer et al. 2021; Ying et al. 2021; Dwivedi et al. 2022a). To seamlessly integrate these encodings with the original input features, we employ a concatenation method. Given a positional/structural encoding matrix $U \in \mathrm { R } ^ { N \times d _ { u } }$ , where $d _ { u }$ represents the encoding dimension, we combine it with the original node features denoted by $X$ . This process results in a new feature matrix $X ^ { * }$ , defined as follows: $X ^ { * } = [ X , U ]$ . For further details on relative encodings, please refer to the extended version.

# 3.2 Local Propagation Block (LCB)

Local Propagation Block (LCB) aggregates neighborhood embeddings for each node and concatenates them with the original ones by utilizing the explicit topological information present in the adjacency matrix. This is similar to the traditional feature propagation with a dense skip connection, and no parameters are involved. LCB is expressed as follows:

$$
\begin{array} { r } { \boldsymbol { h } _ { \boldsymbol { v } } ^ { * ( l ) } = [ \boldsymbol { h } _ { \boldsymbol { v } } ^ { ( l - 1 ) } , \boldsymbol { \alpha } _ { \boldsymbol { v } } ^ { ( l ) } ] \quad o r \quad \boldsymbol { H } ^ { * ( l ) } = [ \boldsymbol { H } ^ { ( l - 1 ) } , \boldsymbol { A } \boldsymbol { H } ^ { ( l - 1 ) } ] } \end{array}
$$

where $\alpha _ { v } ^ { ( l ) }$ and $h _ { v } ^ { ( l - 1 ) }$ are defined as before. Instead of adding self-edges for each node, we concatenate $\alpha _ { v } ^ { ( l ) }$ and $h _ { v } ^ { ( l - 1 ) }$ , enabling our model to distinguish node and propagation embeddings. The doubling of dimension at every layer is prevented by FFNs after each GECO block. Moreover, rather than solely relying on $h _ { v } ^ { ( l ) }$ , local attention mechanisms similar to those found in GAT (Veliˇckovi´c et al. 2018) can be incorporated. Alternative LCB approaches are further discussed in Sec. 4.3.

Proposition 3.1 LCB can be computed in $\mathcal { O } ( N + M )$ using Sparse Matrix Matrix (SpMM) multiplication between X(l) and $A$ in linear time complexity, where $M = | E |$ .

# 3.3 Global Context Block (GCB)

Efforts have aimed at creating efficient attention alternatives to capture longer contexts via low-rank approximation, factorization, and sparsification, often leading to trade-offs between efficiency and quality (Catania, Spitale, and Garzotto 2023). Meanwhile, recent sequence models opt for linear convolutions or RNNs which offer near-linear time complexity (Gu, Goel, and Re 2021; Fu et al. 2023; Peng et al. 2023; Li et al. 2023; Poli et al. 2023). Building upon the evolving research, for the first time, we explore whether global convolutions can capture global context within graph structures.

However, designed for sequences, many of the global convolutional models lack graph handling capabilities inherently. This leads us to a key question: Can we develop an operator that effectively processes graphs using global convolutions? Our investigation has yielded positive results, leading to Global Context Block (GCB), a novel operator with graph awareness. Below, we highlight the key distinctions and enhancements of GCB compared to Hyena. Furthermore, we provide ablation studies and empirical comparisons that demonstrate significant quality improvements.

Graph-to-sequence: Since we focus on graphs, we arrange both $A$ and $X$ using permutation $\pi$ and convert them into time-correlated sequences, aligning node IDs with time $\mathbf { \eta } ( t )$ . All-to-all information flow: As our setup lacks causality, we remove the causal mask from the global convolution kernel. This allows information to flow mutually between all nodes, respecting the natural dynamics of graph data. The non-causal filters are vital because the relationship between nodes is not inherently sequential or unidirectional. Nodes can have mutual or bidirectional influences, and their relationships are not bound by a linear sequence like words in a sentence.

Graph-aware context: (1) The original proposal by (Li et al. 2023) for global convolutional models for sequences involves exponential decay modulation for convolution filters, assigning higher weights to nearby points in the sequence. In contrast, we aim to minimize the impact of the permutation $\pi$ during model training. Therefore, we treat all nodes equally regardless of their distance under $\pi$ by eliminating this decay. (2) Unlike Hyena (Poli et al. 2023), GECO does not employ short convolution along the sequence length, as $\pi$ may not reflect a locality-sensitive order. Instead, GECO utilizes LCB for local dependencies by leveraging the adjacency matrix. In addition, we first apply LCB before generating input projections, which further reduces the number of parameters in comparison to the prior work.

Window of the global convolution: We set the window size for global convolutions to match the number of nodes, ensuring the inclusion of all nodes within the convolution operation. Without the adjacency matrix, no explicit context is present for graphs. Thus, shorter window lengths hold no meaningful interpretation. This is similarly reasoned by the natural dynamics of graph data where node permutations do not introduce proximity-based context.

![](images/dccc92677a2370b008aac6b7177ddc68f711aec64f972f5aa6036c2caee64604.jpg)  
Figure 1: A Our architecture comprises Positional Encoding (PE) block and Graph-Enhanced Contextual Operators (GECOs) layers. PE adds positional encodings as a preprocessing step and each GECO is followed by an FFN. B A GECO layer contains a Local Propagation Block (LCB) aggregating neighborhood embeddings and concatenating with originals to capture local dependencies, and a Global Context Block (GCB) efficiently capturing global dependencies via global convolutions.

<html><body><table><tr><td>Algorithm1:Forward pass of GCB Operator</td></tr><tr><td>Input: Node embeddings X ∈ RN×d; Order K; PE dim de; 1.P1,...,Pk,V=Projection(X) #Linear projectionsPi</td></tr><tr><td>2.F1,...,Fk= Filter(N,de)#Position based fltersFi #UpdateVuntil all projectionsare exhausted</td></tr><tr><td>fori=1,...,Kdo 3.In parallel across d: Vt ← (Pi)t ·FFTConv(Fi,V)t end for</td></tr></table></body></html>

Algorithm 1 presents the GCB (notations unified with (Poli et al. 2023)). Given a node embedding matrix $X$ , GCB generates $( K + 1 )$ projections, where $K$ is a hyperparameter controlling its recurrence. In this work, we set $K = 2$ , and in this case, the three projections serve roles similar to query, key, and value. For each projection, a filter is learned by a simple FFN, with node IDs used for filters’ positional encoding. Subsequently, the value $V$ is updated using global convolutions with one projection and filter at a time, followed by element-wise multiplication gating, until all projections are processed. GCB is formally expressed as:

$$
y = v \odot ( f _ { q } * ( q \odot ( f _ { k } * k ) )
$$

We assume single-channel features and omit layer notations for simplicity. $\mathbf { \bar { \Phi } } _ { q , k , v } \in \mathbb { R } ^ { N \times 1 }$ are linear projections of the input, and $\bar { f _ { k } } , \bar { f _ { q } } \in \mathbb { R } ^ { N \times 1 }$ are learnable filters with circular symmetry. $\odot$ denotes Hadamard product (element-wise multiplication), and $^ *$ denotes circular convolution.

# 3.4 Surrogate Attention Analysis

One natural question that arises is why the GCB is a meaningful replacement for GT’s self-attention. To answer this, we can rewrite the attention matrix as $\begin{array} { r } { A t t n ( H ) \ = \ \mathrm { S o f t m a x } \left( \frac { H W _ { Q } ( H W _ { K } ) ^ { T } } { \sqrt { d } } \right) } \end{array}$ and interpret it as a normalized adjacency matrix, where the pairwise similarity scores are edge weights learned through the attention mechanism. GCB with its modified filter design also learns a surrogate attention matrix that can be interpreted as an adjacency matrix that takes the global context into account. However, it is computed efficiently without storing the entire dense matrix, enabling scaling to larger datasets using similar compute resources. For details, refer to the extended version.

Proposition 3.2 GCB computes a surrogate attention matrix in $\mathcal { O } ( N \log N )$ by using Fast Fourier Transform (FFT) and element-wise multiplication.

# 3.5 Pitfalls of Permutation Sensitivity and Mitigation

The GECO has certain pitfalls in terms of permutation sensitivity. While typical GNNs use permutation invariant functions (Kipf and Welling 2017; Hamilton, Ying, and Leskovec 2017), GCB’s short and global convolutions are shift-invariant but not permutation invariant. Importantly, a line of research focuses on order-sensitive GNNs (Murphy et al. 2019b,a; Chen et al. 2020; Sato, Yamada, and Kashima 2021; Huang et al. 2022; Chatzianastasis et al. 2023) for enhanced expressibility. Notably GraphSAGE (Hamilton, Ying, and Leskovec 2017) and (Moore and Neville 2017) with LSTM have shown outperforming results. By replacing short convolutions with LCB, we make the local mixing permutation invariant. However, global convolutions remain order-sensitive. To mitigate GCB’s permutation sensitivity, we have explored different random permutation strategies.

Static Random: We randomly permute the graph once before training as a naive baseline and compare the performance variations between different runs. Surprisingly, we observed that the final results are not significantly impacted by different orderings, which we elaborate in Section 4.3.

Dynamic Random: Consider parametrized function $\vec { \boldsymbol { f } }$ with parameters $W$ , and permutation $\pi$ . With $N !$ permutations sampled, a permutation-sensitive function can recover the original target permutation function $\overline { { \overline { { f } } } }$ (Murphy et al. 2019b):

$$
\overline { { \overline { { f } } } } ( X ; W ) = \frac { 1 } { N ! } \sum _ { \pi \in \Pi _ { N } } \ : \ : \overline { { f } } \left( A _ { \pi } , X _ { \pi } ; W \right)
$$

However, $N !$ is intractable for large graphs, so one option is to sample permutations during training. Consequently, we sample a random permutation per epoch per layer during model training. Similar strategies have been also used for positional encodings, such as random sign flipping for Laplacian PE (Dwivedi et al. 2023a). This helps model to see many different permutations during training, potentially memorize permutation invariance and gain robustness to different permutations. (Murphy et al. 2019b) further proves such strategies approximates $\overline { { \overline { { f } } } }$ with decreasing variance as more permutations are sampled.

Proposition 3.3 GECO with dynamic random sampling strategy is an approximate solution to original target permutation invariant function.

While we observe robustness to different orderings, understanding and addressing this limitation is crucial for broadening GECO’s applicability.

<html><body><table><tr><td>Algorithm 2:End-to-end GECO Model Training</td></tr><tr><td>Input: Adj. matrix A ∈ RN×N; Node features X ∈ RN ×d; 1.X,A= GraphPositionalEncoder(X,A,ε) Edge features ε ∈ RMxde;</td></tr><tr><td>forl=0,...,L-1do</td></tr><tr><td>2.π=SamplePermutation() 3.X(0) = Permute(A,X,π)</td></tr><tr><td>4. X(+1) = LayerNorm(GECO(X(l),A) + X(l)) 5. X(+1) = LayerNorm(FFN(X(+1)) + X(l+1))</td></tr></table></body></html>

Algorithm 2 presents the end-to-end training with dynamic permutation. We start by positional encodings. The training is further broken into two main blocks. LCB propagates neighborhood embeddings and applies normalization, which is followed by GCB. Each GECO is followed by an FFN, such that FFN(X) = σ(XW1)W2, where W1, W2 ∈ Rd×d are the linear layer weights. Both the GECO and FFN use skip connections, normalization, and dropout. Alternatively, line 2 can be moved out of the for loop to achieve static permutation strategy to order the nodes as a preprocessing step. GECO uses three quasilinear operators and can be computed in $\mathcal { O } ( N \log N + M )$ . For the complete algorithm and complexity analysis, refer to the extended version.

# 3.6 Comparison with Prior Work

Hybrid Approaches. Prior works (Wu et al. 2021; Dwivedi and Bresson 2020; Lin, Wang, and Liu 2021; Min et al. 2022)

straightforwardly combine GNNs and Transformers as separate local and global modules. In contrast, GECO’s LCB and GCB are not auxiliary modules but integrated components of a new compact layer design, refining the model by removing intermediate parameters and non-linearities. This design uses skip connections for the entire layer rather than separate components. Please refer to the extended version for details.

NAgphormer’s Hop2Token is a preprocessing step decoupled from model training, where feature propagation iterations are performed to generate node tokens (Chen et al. 2023). Such decoupling methods separate training from feature propagation, hindering model to learn complex relationships between consecutive layers (Wu et al. 2019). In contrast, LCB’s feature propagation is coupled with learnable parameters of GCB. LCB is not a preprocessing step but rather an integral pre-step to GCB during model training. In the extended version, we further discuss NAgphormer’s recovery as a specific JKNets (Xu et al. 2018) instance.

Graph-Mamba (Wang et al. 2024) has recently adapted Mamba (Gu and Dao 2023) for graphs. It focuses on node ordering strategies based on prioritization while using offthe-shelf permutation-sensitive components. In contrast, we refine the layer design, introduce LCB, aim to mitigate permutation-sensitivity, and further incorporate random permutation strategies for improved robustness. Notably, our evaluation also targets large node prediction datasets, unlike Graph-Mamba’s focus on small graph-level tasks.

Orthogonal research: (1) Model-agnostic methods: Universally applicable feature encoding and initialization/tuning methods (Dwivedi and Bresson 2020; Kreuzer et al. 2021; Ying et al. 2021; Mialon et al. 2021; Chen, O’Bray, and Borgwardt 2022; Zhao et al. 2023; To¨nshoff et al. 2023; Ma et al. 2023). (2) Scaling methods such as GOAT (Kong et al. 2023), HSGT (Zhu et al. 2023), and LargeGT (Dwivedi et al. 2023b) that leverage self-attention. GECO offers an alternative kernel combinable with these methods. Section 4.3 provides evidence on when this combination is beneficial based on input size. While Graph-ViT/MLP-Mixer propose alternatives, they are limited to graph-level tasks and require graph re-partitioning at every epoch, which can be costly for large node-level tasks. In contrast, GECO does not require partitioning and is applicable to node-level tasks as well.

# 4 Experiments

# 4.1 Objective 1: Prediction Quality

We assess the GECO on benchmarks outlined in Table ??, where each dataset contains many small graphs, with an average number of nodes ranging from tens to five hundred. Consequently, scalability is not a significant concern for these datasets as the computational load is determined by the average number of nodes. As evidence, even the most computation-intensive GTs, such as Graphormer, can be trained on these datasets using Nvidia-V100 (32GB) or Nvidia-A100 (40GB) GPUs (Ying et al. 2021; Rampasek et al. 2022). The experiments in this section aim to demonstrate GECO’s competitive predictive quality compared to GT baselines, as many of them encounter memory or time issues with larger graphs. Nevertheless, for these evaluations, we begin by creating a hybrid $_ { \mathrm { G N N + G E C O } }$ by replacing the attention module used in GraphGPS. For dataset and hyperparameter details please refer to the extended version. Long Range Graph Benchmark (LRGB). Table 1 presents our evaluation on the LRGB, a collection of graph tasks designed to test a model’s ability to capture long-range dependencies. The results show that GECO outperforms baselines across most datasets, with improvements up-to $4 . 3 \%$ . For the remaining datasets, it ranks among the top three, with quality within $1 . 3 \%$ of the best baseline. By capturing long-range dependencies effectively, GECO surpasses the performance of MHA in most cases without compromising quality. Notably, GECO’s F1 score on PascalVOC increased from 0.4053 to 0.4210 without positional encodings, resulting in enhanced quality with a simplified model.

Table 1: LRGB: the first, second, and third are highlighted, with results reused from (Rampasek et al. 2022; Shirzad et al. 2023)   

<html><body><table><tr><td rowspan="2">Model</td><td>PascalVOC-SP</td><td>COCO-SP</td><td>Peptides-func</td><td>Peptides-struct</td><td>PCQM-Contact</td></tr><tr><td>F1 score ↑</td><td>F1 score 个</td><td>AP ↑</td><td>MAE↓</td><td>MRR↑</td></tr><tr><td>GCN</td><td>0.1268 ± 0.0060</td><td>0.0841 ± 0.0010</td><td>0.5930± 0.0023</td><td>0.3496 ± 0.0013</td><td>0.3234 ± 0.0006</td></tr><tr><td>GINE</td><td>0.1265 ± 0.0076</td><td>0.1339 ± 0.0044</td><td>0.5498 ± 0.0079</td><td>0.3547 ± 0.0045</td><td>0.3180 ± 0.0027</td></tr><tr><td>GatedGCN</td><td>0.2873 ± 0.0219</td><td>0.2641 ± 0.0045</td><td>0.5864 ± 0.0077</td><td>0.3420 ± 0.0013</td><td>0.3218 ± 0.0011</td></tr><tr><td>GatedGCN+RWSE</td><td>0.2860± 0.0085</td><td>0.2574 ± 0.0034</td><td>0.6069± 0.0035</td><td>0.3357± 0.0006</td><td>0.3242 ± 0.0008</td></tr><tr><td>Transformer+LapPE</td><td>0.2694 ± 0.0098</td><td>0.2618 ± 0.0031</td><td>0.6326 ± 0.0126</td><td>0.2529 ± 0.0016</td><td>0.3174± 0.0020</td></tr><tr><td>SAN+LapPE</td><td>0.3230 ± 0.0039</td><td>0.2592 ± 0.0158</td><td>0.6384 ± 0.0121</td><td>0.2683 ± 0.0043</td><td>0.3350 ± 0.0003</td></tr><tr><td>SAN+RWSE</td><td>0.3216 ± 0.0027</td><td>0.2434 ± 0.0156</td><td>0.6439± 0.0075</td><td>0.2545 ± 0.0012</td><td>0.3341 ± 0.0006</td></tr><tr><td>GPS w/ Transformer</td><td>0.3748 ± 0.0109</td><td>0.3412 ± 0.0044</td><td>0.6535 ± 0.0041</td><td>0.2500 ± 0.0005</td><td>0.3337± 0.0006</td></tr><tr><td>Exphormer</td><td>0.3975 ± 0.0037</td><td>0.3455 ± 0.0009</td><td>0.6527 ± 0.0043</td><td>0.2481 ± 0.0007</td><td>0.3637 ± 0.0020</td></tr><tr><td>GECO (Ours)</td><td>0.4210 ± 0.0080</td><td>0.3320 ± 0.0032</td><td>0.6982 ± 0.0045</td><td>0.2464 ± 0.0009</td><td>0.3526 ± 0.0016</td></tr></table></body></html>

<html><body><table><tr><td>PCQM4Mv2</td><td>GCN</td><td>GCN-virtual</td><td>GIN-virtual</td><td>GRPE</td><td>EGT</td><td>Graphormer</td><td>GPS-sm</td><td>GPS-med</td><td>GECO</td></tr><tr><td>Train MAE↓</td><td>n/a</td><td>n/a</td><td>n/a</td><td>n/a</td><td>n/a</td><td>0.0348</td><td>0.0653</td><td>0.0726</td><td>0.0578</td></tr><tr><td>Val. MAE↓</td><td>0.1379</td><td>0.1153</td><td>0.1083</td><td>0.0890</td><td>0.0869</td><td>0.0864</td><td>0.0938</td><td>0.0858</td><td>0.0841</td></tr><tr><td>#Param.</td><td>2.0M</td><td>4.9M</td><td>6.7M</td><td>46.2M</td><td>89.3M</td><td>48.3M</td><td>6.2M</td><td>19.4M</td><td>6.2M</td></tr></table></body></html>

Table 2: PCQM4Mv2 Eval.: the first, second, and third are highlighted. Validation set is used for evaluation as test set is private. We reuse results from (Rampasek et al. 2022).

PCQM4Mv2. Table 2 shows that GECO outperforms both GNN and GT baselines on PCQM4Mv2 in prediction quality. Notably, GECO uses only 1/8 and 1/3 of the parameters required by Graphormer and GraphGPS, respectively.

# 4.2 Objective 2: Scalability for Larger Graphs

We assess GECO on 4 benchmark datasets where each graph contains a much larger number of nodes. Notably, traditional Dense GTs struggle to handle such large graphs due to their quadratic complexity while GECO succeeds with its superior computational and memory efficiency. In the following experiments, we design our models using only GECO blocks, following Algorithm 2. For simplicity, we avoid using structural/positional encodings as computing them may be infeasible for large graphs. For details on datasets and hyperparameters, please refer to the extended version.

Unlike previous works that exhibit a trade-off between quality and scalability, GECO scales efficiently to large datasets and achieves superior quality across all compared to Dense GTs (Graphormer/GraphGPS), which suffer from

OOM/timeout issues. Remarkably, GECO demonstrates significant predictive superiority, surpassing Dense GT baseline methods by up to $4 . 5 \%$ . On Arxiv, GECO outperforms recently proposed GT works Exphormer and GOAT (Kong et al. 2023) up to $0 . 7 \%$ . Notably, Graphormer with sampling falls short in achieving competitive quality across all datasets. When comparing GECO to various baselines, including orthogonal methods, GECO remains competitive. It outperforms various baselines on Flickr, Arxiv, and Reddit, except for Yelp where the coarsening approach HSGT (Zhu et al. 2023) surpasses GECO. We leave the exploration of combining GECO with orthogonal methods such as expander graphs (Shirzad et al. 2023), hierarchical learning (Zhu et al. 2023), and dimensionality reduction (Kong et al. 2023) as future work to potentially get even better results. Overall, the results highlight that the global context can enhance the modeling quality for large node prediction datasets, justifying our motivation to find efficient high-quality attention alternatives. To the best of our knowledge, GECO is the first attempt to capture pairwise node relations without heuristics at scale. Our evaluation illustrates its effectiveness as a Dense GT alternative for large graphs.

# 4.3 Ablation Studies

Permutation Robustness. In Table 4, we investigate GECO’s robustness to different permutations using the static and dynamic random strategies outlined in Section 3.5. The results indicate negligible differences between different strategies with dynamic random showing slightly higher mean on multiple datasets. In addition, we note that these strategies has negligible overhead in training time. However, all strategies seems to fall into similar confidence intervals, hence we favor the simpler strategy in our experiments.

Hyena Comparison. Table 4 compares GECO with the offthe-shelf Hyena by setting its filter size as the entire graph. GECO consistently outperforms the off-the-shelf Hyena with a significant margin. This underscores the efficacy of GECO, particularly in its application of global convolutions for graphs, and distinctly sets its apart from the Hyena.

Table 3: Accuracy on large node prediction datasets: the first, second, and third are highlighted. We reuse the results from (Han et al. 2023; Shirzad et al. 2023; Zeng et al. 2021), and run Exphormer locally except Arxiv. − indicates that the data was either not included in the original work or could not be successfully reproduced.   

<html><body><table><tr><td rowspan="2">Model Accuracy</td><td>Flickr</td><td>Arxiv</td><td>Reddit</td><td>Yelp</td></tr><tr><td>Accuracy</td><td>Accuracy</td><td>Accuracy</td><td>Micro-F1 Score</td></tr><tr><td>GCN</td><td>50.90 ± 0.12</td><td>70.25 ±0.22</td><td>92.78 ± 0.11</td><td>40.08 ±0.15</td></tr><tr><td>SAGE</td><td>53.72 ± 0.16</td><td>72.00 ± 0.16</td><td>96.50 ± 0.03</td><td>63.03 ± 0.20</td></tr><tr><td>GraphSaint</td><td>51.37 ± 0.21</td><td>67.95 ± 0.24</td><td>95.58 ± 0.07</td><td>29.42 ± 1.32</td></tr><tr><td>Cluster-GCN</td><td>49.95 ± 0.15</td><td>68.00± 0.59</td><td>95.70 ± 0.06</td><td>56.39 ± 0.64</td></tr><tr><td>GAT</td><td>50.70 ± 0.32</td><td>71.59 ± 0.38</td><td>96.50 ± 0.11</td><td>61.58 ± 1.37</td></tr><tr><td>Graphormer</td><td>OOM</td><td>OOM</td><td>OOM</td><td>OOM</td></tr><tr><td>Graphormer-SAMPLE</td><td>51.93 ± 0.21</td><td>70.43 ±0.20</td><td>93.05±0.22 5</td><td>60.01 ± 0.45</td></tr><tr><td>SAN</td><td>OOM</td><td>OOM</td><td>OOM</td><td>OOM</td></tr><tr><td>SAT</td><td>OOM</td><td>OOM</td><td>OOM</td><td>OOM</td></tr><tr><td>SAT-SAMPLE</td><td>50.48 ± 0.34</td><td>68.20±0.46</td><td>93.37±0.32</td><td>60.32 ±0.65</td></tr><tr><td>ANS-GT</td><td></td><td>68.20 ±0.46</td><td>95.30 ± 0.81</td><td></td></tr><tr><td>GraphGPS w/ Transformer</td><td>OOM</td><td>OOM</td><td>OOM</td><td>OOM</td></tr><tr><td>Exphormer</td><td>52.60 ± 0.18</td><td>72.44 ± 0.28</td><td>95.90 ± 0.15</td><td>60.80 ± 1.56</td></tr><tr><td>HSGT</td><td>54.12 ± 0.51</td><td>72.58 ± 0.31</td><td></td><td>63.47 ± 0.45</td></tr><tr><td>GECO (Ours)</td><td>55.55 ± 0.25</td><td>73.10 ± 0.24</td><td>96.65 ± 0.05</td><td>63.18 ± 0.59</td></tr></table></body></html>

Table 4: Accuracy with different permutation strategies (Natural/Static Random/Dynamic Random) with GECO, alongside a comparison with default Hyena (Poli et al. 2023).   

<html><body><table><tr><td>Dataset</td><td>Hyena</td><td>GECO</td><td>GECO</td></tr><tr><td>Perm.</td><td>Static</td><td>Static</td><td>Dynamic</td></tr><tr><td>Flickr</td><td>46.97 ±0.08</td><td>55.73±0.27</td><td>55.80 ±0.38</td></tr><tr><td>Arxiv</td><td>56.04 ±0.61</td><td>73.08±0.28</td><td>73.12±0.22</td></tr><tr><td>Reddit</td><td>69.24 ± 0.54</td><td>96.62±0.05</td><td>96.68±0.06</td></tr><tr><td>Yelp</td><td>50.08 ±0.31</td><td>63.23±0.50</td><td>63.20±0.42</td></tr></table></body></html>

Table 5: Ablation study on the LCB alternatives: the first, second, and third are highlighted. Conv- $x$ indicates 1D Convolution with a filter size of $x$ .   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Local Block</td><td>Pas.VOC-SP</td><td>Pep.-func</td><td>Pep.-struct</td></tr><tr><td>F1个</td><td>AP↑</td><td>MAE↓</td></tr><tr><td>Transformer</td><td>N/A</td><td>0.2762</td><td>0.6333</td><td>0.2525</td></tr><tr><td>Performer</td><td>N/A</td><td>0.2690</td><td>0.5881</td><td>0.2739</td></tr><tr><td>GECO</td><td>Conv-1</td><td>0.2752</td><td>0.6589</td><td>0.2587</td></tr><tr><td>GECO</td><td>Conv-10</td><td>0.1757</td><td>0.6819</td><td>0.2516</td></tr><tr><td>GECO</td><td>Conv-20</td><td>0.1645</td><td>0.6706</td><td>0.2534</td></tr><tr><td>GECO</td><td>Conv-40</td><td>0.1445</td><td>0.6517</td><td>0.2547</td></tr><tr><td>GECO</td><td>LCB</td><td>0.3220</td><td>0.6876</td><td>0.2454</td></tr></table></body></html>

Local Propagation Block Alternatives. In GECO, we adopted LCB for graph-aware local context modeling instead of using 1D convolutions originally used in Hyena. This is motivated by the limitation of 1D convolutions in capturing local dependencies in graphs where node order does not imply proximity. At Table 5, we focus on exploring alternatives to LCB within our GECO module. We experimented with replacing LCB with 1D convolutions of various filter sizes to help understand its effectiveness. We consistently observed a diminishing trend in quality as filter sizes increased, which can be attributed to larger filter sizes leading to a mix of unrelated nodes within the graph. In contrast, GECO with LCB consistently outperformed its alternatives as well as the Transformer and Performer, highlighting its effectiveness in capturing local graph dependencies.

![](images/d33f2f87f99eb83e4e0e17a05d0f5401627439e54eadf15c7878a03eaa49c88f.jpg)  
Figure 2: Relative speedup of GECO w.r.t. FlashAttention (Dao et al. 2022) characterized by $\mathcal { O } ( N / \log N )$

Scaling Study. Figure 2 shows GECO’s speedup w.r.t. the optimized attention, FlashAttention (Dao et al. 2022), for increasing numbers of nodes using synthetic datasets with similar sparsity patterns to those in Table 3. The results highlight that the speedup linearly increases with the number of nodes, and GECO reaches $1 6 9 \times$ speedup on a graph with 2M nodes, confirming its relative scalability. Details including runtime numbers can be found in the extended version.

# 5 Conclusion

We presented GECO, a novel model that replaces the compute-intensive MHA in GTs with an efficient and highquality operator. With comprehensive evaluation, we demonstrated GECO effectively scales to large datasets with outperforming quality. We plan to explore alternatives for GCB, and combinations with orthogonal approaches for future work.