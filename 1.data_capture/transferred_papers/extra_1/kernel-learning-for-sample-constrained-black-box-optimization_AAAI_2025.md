# Kernel Learning for Sample Constrained Black-Box Optimization

Rajalaxmi Rajagopalan, Yu-Lin Wei, Romit Roy Choudhury

Department of Electrical & Computer Engineering University of Illinois Urbana-Champaign rr30,yulinlw2,croy @illinois.edu

# Abstract

Black box optimization (BBO) focuses on optimizing unknown functions in high-dimensional spaces. In many applications, sampling the unknown function is expensive, imposing a tight sample budget. Ongoing work is making progress on reducing the sample budget by learning the shape/structure of the function, known as kernel learning. We propose a new method to learn the kernel of a Gaussian Process. Our idea is to create a continuous kernel space in the latent space of a variational autoencoder, and run an auxiliary optimization to identify the best kernel. Results show that the proposed method, Kernel Optimized Blackbox Optimization (KOBO), outperforms state of the art by estimating the optimal at considerably lower sample budgets. Results hold not only across synthetic benchmark functions but also in real applications. We show that a hearing aid may be personalized with fewer audio queries to the user, or a generative model could converge to desirable images from limited user ratings.

# 1 Introduction

Many problems involve the optimization of an unknown objective function. Examples include personalizing content $x$ to maximize a user’s satisfaction $\bar { f ( x ) }$ , or training deep learning models with hyperparameters $x$ to maximize their performance $f ( x )$ . Function $f ( x )$ is unknown in these cases because it is embedded inside the human brain (for the case of personalization) or too complex to derive (for hyperparameter tuning). However, for any chosen sample $x _ { i }$ , the value of $f ( x _ { i } )$ can be evaluated. For hearing-aid personalization, say, evaluating the function would entail playing audio with some hearing-compensation filter $x _ { i }$ and obtaining the audio clarity score $f ( x _ { i } )$ from the user.

Bayesian methods like Gaussian Process Regression (GPR) are de facto approaches to black-box optimization (BBO). Using a set of function evaluations, conventional GPR (Frazier 2018) learns a probabilistic surrogate model ${ \hat { f } } ( x )$ for $f ( x )$ . The optimum is estimated on this surrogate as $\hat { x } ^ { * } =$ argmin $- { \hat { f } } ( x )$ . In most BBO problems, $f ( x )$ is expensive to evaluate, hence a strict sample or query budget $B$ is of interest. Techniques that lower this budget have garnered recent attention. One idea is to exploit domain knowledge about the rough shape of $f ( x )$ , i.e., select a GPR kernel that models this shape. With humans, for example, $f ( x )$ may have a staircase structure as they may not perceive differences in certain neighborhoods of $x$ , but their ratings may change just outside that neighborhood. If GPR’s surrogate model ${ \hat { f } } ( x )$ captures this staircase structure in its kernel, sample efficiency can improve. However, in the absence of domain knowledge, can the optimal GPR kernel $\mathbf { K } ^ { * }$ be learnt, using the same sample queries used to estimate $x ^ { * }$ ?

![](images/dcf45ce95ebde520603e28e05781bc9af78cbab88bc853a5b84e9f2929d809c1.jpg)  
Figure 1: KerGPR in VAE latent space gives $K ^ { * }$ to $f \mathrm { G P R }$

A growing body of research (Grosse et al. 2012)(Kim and Teh 2016)(Teng et al. 2020) is concentrating on kernel learning. One effective approach is Automatic Statistician (AS) (Duvenaud et al. 2013) where authors compose complex kernels by combining simple ones through a context-free grammar and design a search method over the countably infinite complex kernels (more in Section 5). Subsequent improvements over AS have used Hellinger distance as a measure of kernel similarity (Malkomes, Schaff, and Garnett 2016). This similarity metric guides an optimization-based search over the space of composite kernels. To reduce search complexity, (Gardner et al. 2017) exploits additive structures in the search space and employs MCMC methods to discover kernels. All these techniques operate on discrete spaces, delivering a categorical composition of simple kernels.

A continuous space of kernels naturally lends itself to optimization methods. Our contribution is in designing such a continuous kernel space and running an auxiliary optimization on it to discover $\mathbf { K } ^ { * }$ . To this end, we first synthesize many discrete kernels by adding or multiplying a set of simple “basis” kernels, and then use a variational autoencoder (VAE) to learn a low-dimensional continuous manifold for the discrete kernels. This manifold lives in the latent space of the VAE, as shown by the orange region in Figure 1. Conventional optimization on this latent kernel space is difficult since we lack an objective function, but given a kernel, we can evaluate its effectiveness using model evidence (i.e., how well a given kernel agrees with available samples from $f ( x ) )$ . Thus, optimizing over the kernel space can also be designed as a blackbox optimization problem, hence we run a kernel space GPR $( K e r G P R )$ to output an optimal $\mathbf { K } ^ { * }$ . The main GPR module, Function $G P R$ , uses $\mathbf { K } ^ { * }$ to model the surrogate function ${ \hat { f } } ( x )$ , and queries the surrogate at more points. The new model evidence is then passed back to KerGPR (see Fig. 1) to update the optimal kernel. The iteration terminates when the query budget is expended. Function GPR then outputs the minimum of the surrogate ${ \hat { f } } ( x )$ . Results show that KOBO consistently outperforms SOTA methods (namely MCMC (Gardner et al. 2017), BOMS (Malkomes, Schaff, and Garnett 2016), and CKS (Duvenaud et al. 2013)) in terms of the number of queries needed to reach the optimal. The gain from kernel learning is also significant compared to the best static kernels. Experiments are reported across synthetic benchmark functions and from real-world audio experiments with $U { = } 6$ users. Volunteers were asked to rate the clarity of audio clips, and using $B \leq$ 25 ratings, KOBO prescribed a personalized filter that maximized that user’s personal satisfaction. The performance gain is robust across extensive experiments, suggesting that KOBO could be deployed in real-world black-box applications where sample-budget is of prime concern.

# 2 Problem Formulation and Background Problem Formulation

Consider an unknown real-valued function $f : \mathcal { H }  \mathbf { R }$ where $\mathcal { H } \subseteq \mathbf { R } ^ { N }$ , $N \geq 5 0 0$ . Let $x ^ { * }$ be the minimizer of $f ( x )$ . We aim to estimate $x ^ { * }$ using a budget of $B$ queries. Thus, the optimization problem is,

$$
\operatorname { a r g m i n } _ { \hat { x } \in \mathcal { H } } \quad | | f ( \hat { x } ) - f ( x ^ { * } ) | | _ { 2 } \qquad \mathrm { s . t . } \qquad Q \leq B
$$

where $Q$ is the number of times the objective function is evaluated/queried, and the sample budget is $B \ll N$ . Function $f$ may be non-convex, may not have a closed-form expression, and its gradient is unavailable (hence, a black-box optimization problem). Bayesian approaches like GPR suit such problems but require choosing a kernel to model the function structure; a poor choice incurs more queries for optimization. Since queries can be expensive (e.g., when users need to answer many queries, or a NeuralNet needs re-training for each hyper-parameter configuration), lowering $Q$ is of growing interest. Kernel learning aims to address this problem.

# Background: Gaussian Process Regression (GPR)

Bayesian optimization (BO) (Frazier 2018) broadly consists of two modules: (1) Gaussian Process Regression that learns a Gaussian posterior distribution of the likely values of $f ( x )$ at any point of interest $x$ . (2) Acquisition function, a sampling strategy that prescribes the point at which $f$ should be evaluated (or sampled) next. We briefly discuss GPR to motivate the kernel learning problem.

1 MM   
Squared- Periodic Linear Rational  
exp (SE) (PER) (LIN) quad. (RQ) >>>>> <>>>\~   
local repeating linear multi-scale   
variation structure functions variation

GPR Prior & Posterior: GPR generates a probabilistic surrogate model by defining a Gaussian distribution $( \mathcal { N } ( \pmb { \mu } , \mathbf { K } ) )$ over infinite candidate functions. At initialization, i.e., when no function-sample are available, the prior distribution over the candidate functions is defined by ${ \pmb \mu } = { \bf 0 }$ and a covariance matrix $\mathbf { K }$ . This matrix is computed using a kernel function $k$ as, $\mathbf { K } _ { i j } { = } k ( x _ { i } , x _ { j } )$ . The kernel essentially favors candidate functions that are similar to the kernel’s own shape/structure; these candidates are assigned a higher likelihood. An expert with domain knowledge about the structure of $f ( x )$ can choose the kernel judiciously, resulting in better surrogate functions $\hat { f } ( x )$ . Better ${ \hat { f } } ( x ) $ will ultimately reduce the number of queries needed to optimize the objective $f ( x )$ .

Once the function $f$ has been observed for a set of samples $\chi \ : \ : = \ : \ : \{ x _ { 1 } , x _ { 2 } , . . . , x _ { K } \}$ . . , xK , i.e., we know = $\{ f ( x _ { 1 } ) , f ( x _ { 2 } ) , \ldots , f ( x _ { K } ) \}$ , the prior is updated to form the posterior distribution (Eqn. 2) over the candidate functions. The posterior mean $\pmb { \mu }$ is the most likely surrogate of the function $f$ .

$$
P ( { \mathcal { F } } | { \mathcal { X } } ) \sim { \mathcal { N } } ( { \mathcal { F } } | \mu , \mathbf { K } )
$$

where, $\pmb { \mu } = \{ \mu ( x _ { 1 } ) , \mu ( x _ { 2 } ) , \ldots , \mu ( x _ { K } ) \} , \mathbf { K } _ { i j } = k ( x _ { i } , x _ { j } ) ,$ , and $k$ represents a kernel function.

Predictions: To make predictions $\begin{array} { r l r } { \hat { \mathcal F } } & { { } = } & { f ( \hat { \mathcal X } ) } \end{array}$ at new points $\hat { \mathcal X }$ , GPR uses the current posterior (Eqn. 2) to define the conditional distribution of $\hat { \mathcal F }$ as: $P ( \hat { \mathcal { F } } | \mathcal { F } , \mathcal { X } , \hat { \mathcal { X } } ) \sim$ $\mathcal { N } ( \hat { \mathbf { K } } ^ { T } \mathbf { K } ^ { - 1 } \mathcal { F } , \hat { \hat { \mathbf { K } } } - \hat { \mathbf { K } } ^ { T } \mathbf { K } ^ { - 1 } \hat { \mathbf { K } } )$ The details and proof of all the above are clearly explained in (Wang 2020)).

# Kernel Selection

Kernels model the possible shape of the unknown function based on a set of observations $( \mathcal { X } , \mathcal { F } )$ of the unknown function. Figure 2 illustrates example kernels on the top row; the bottom row shows candidate functions that GPR can derive using the corresponding kernel. In general, a class of kernels $\kappa$ produces a family of (GPR) surrogates that fit the function observations $( \mathcal { X } , \mathcal { F } )$ . Of course, each surrogate is associated to a likelihood which can improve with additional observations.

The goal of kernel selection is to select one kernel $\mathbf { K ^ { \ast } } \in \mathcal { K }$ that best explains the function observations $( \mathcal { X } , \mathcal { F } )$ . Let’s denote $\mathcal { L } : \mathcal { K }  \mathbf { R }$ to be a model evidence that measures how well a kernel $\mathbf { K }$ fits the observations. We assume that evaluating $\mathcal { L } ( \mathbf { K } )$ for all kernels in $\kappa$ is too expensive. The kernel selection problem is then,

$$
\mathbf { K } ^ { * } = \underset { \mathbf { K } \in \mathcal { K } } { \mathrm { a r g m a x } } \mathcal { L } ( \mathbf { K } )
$$

This problem is difficult to optimize with Bayesian approaches when the kernel space $\kappa$ is discrete (Parker and Rardin 2014). This is because the function $\mathcal { L } ( \mathbf { K } )$ is only defined at the feasible points and cannot be queried arbitrarily. In contrast, it is possible to deduce a continuous function’s behavior in a neighborhood of a point; in the discrete case, the behavior of the objective may change significantly as we move from one feasible point to another. This motivates transforming the problem in Eqn. 3 into a problem in continuous space on which optimization can be applied.

In this paper, the “model evidence” $\mathcal { L }$ is chosen to be GPR posterior in Eqn. 2 as it generates the surrogate that best describes the observations informed by the chosen kernel.

$$
\mathcal { L } ( \mathbf { K } ) = P ( \mathcal { F } | \mathcal { X } , \mathbf { K } )
$$

# 3 Kernel Learning in KOBO

Intuition and Overview: Our prime objective is to create a continuous space $\mathcal { Z }$ corresponding to the discrete space $\kappa$ , thus simplifying the optimization in Eqn. 3. We propose to achieve this using a Variational Autoencoder (VAE) which can take discrete inputs $( \mathbf { K } \in \mathcal { K } )$ and learn (encoder) a continuous latent space $\mathcal { Z }$ from which the inputs can be faithfully reconstructed (decoder). If a large number of discrete kernels are created and represented sufficiently using a scheme that offers some notion of mutual similarity between kernels (i.e., representations defining a kernel space $\kappa$ ), then we expect the VAE to give us a continuous representation of such kernels $\mathcal { Z }$ . This approach satisfies our objective since VAEs are expected to ensure continuity and completeness of their latent space, i.e., (i) two close points in the latent space cannot decode to completely different results, and (ii) a point sampled from the latent space must decode to a valid result. When the VAE is trained, we have successfully transformed the discrete optimization in $\kappa$ (Eqn. 4) to an easier continuous one in $\mathcal { Z }$ .

Building on this intuition, KOBO’s kernel learner is composed of 3 modules as shown in Figure 3:

(1) Kernel Combiner creates composite kernels $\mathbf { K } \in \mathcal { K }$ that form the discrete kernel space $\kappa$ .   
(2) Kernel Space Variational Autoencoder (KerVAE) trained on kernels generated by Kernel Combiner transforms discrete kernel space $\kappa$ to continuous space $\mathcal { Z }$ .   
(3) Kernel Space GPR $( K e r G P R )$ : Optimizes model evidence (Eqn. 4) on $\mathcal { Z }$ ; this gives $z ^ { * }$ which decodes to the optimal kernel $\mathbf { K ^ { * } }$ .

Figure 3 connects all the modules to give a complete overview of KOBO. The main objective function (Eqn. 1) is optimized with a Function GPR $( f \mathbf { G P R } )$ . $f \mathrm { G P R }$ uses a simple Square-Exponential (SE) kernel to obtain a batch of observations $\textstyle { \mathcal { D } } _ { n }$ . The model evidence is then passed to the kernel learning pipeline. The Kernel Combiner takes simple kernels and observations $\mathcal { D } _ { n }$ as inputs, and outputs a discrete space of composite kernels $\mathbf { K } _ { \mathcal { C } } \in \mathcal { K }$ . This is guided by a context-free-grammar described later. The KerVAE is trained on this discrete space and generates the corresponding continuous latent space $\mathcal { Z }$ . KerGPR running on $\mathcal { Z }$ optimizes the model evidence (from $f \mathrm { G P R } )$ to find the optimal kernel $\mathbf { K } ^ { * }$ which is prescribed to $f \mathrm { G P R }$ . $f \mathrm { G P R }$ uses this $\mathbf { K } ^ { * }$ to obtain a new batch of observations $\mathcal { D } _ { n + 1 }$ and update model evidence, which is again passed to the kernel learning pipeline. The cycle iterates until $f \mathrm { G P R }$ has expended the sample budget $B$ . At this point, $f \mathrm { G P R }$ outputs the minimum of its surrogate model. The following discussions expand on Kernel Combiner, KerVAE, and KerGPR.

![](images/f6dd53ede456691d2fbb0e42ce508ffede3fb8d8587cee03c32eb78b8790b970.jpg)  
Figure 3: System flow: KOBO iterates across $f G P R$ on top and KerGPR below that runs in the $\mathsf { K e r V A E }$ latent space. The blue arrow denotes the model evidence input to KerGPR, and the red arrow denotes the optimal kernel $\mathbf { K } ^ { * }$ supplied by KerGPR to $f \mathrm { G P R }$ .

# Kernel Combiner

Complex kernels $\mathbf { k } _ { C }$ can be expressed as operations on the context-free grammar of base kernels $\boldsymbol { B }$ (Hopcroft, Motwani, and Ullman 2001; Duvenaud et al. 2013). Given $\beta \ = \ \{ A , B , C , D , E \}$ , and a set of operators $\begin{array} { r l } { \mathcal { O } } & { { } = } \end{array}$ $\{ \mathrm { a d d , m u l t i p l y , e n d , \ldots \} }$ , the Kernel Combiner generates a composite kernel $\mathbf { k } _ { C }$ by drawing kernels from $\boldsymbol { B }$ and operators from $\mathcal { O }$ with probabilities $p { _ { B } } , p _ { \mathcal { O } }$ . An example $\mathbf { k } _ { \mathcal { C } } = $ $A * C + B * D$ . In general, to form a kernel space $\kappa$ , the Kernel Combiner develops a unique representation for each k .

Grammar-based Representation: Given a composite kernel $\mathbf { k } _ { C }$ , its grammar-based representation is a vector $r _ { c }$ , designed as follows. Let $\mathbf { A } , \mathbf { B } , \mathbf { C } , \mathbf { D } , \mathbf { E }$ be five base kernels in $\boldsymbol { B }$ . These are simple kernels like Square-exponential, Periodic, Rational Quadratic, etc. Any composite kernel $\mathbf { k } _ { C }$ created from the base kernels is expressed in the form of Eqn. 5. The code $r _ { c }$ is then the vector of indices, i.e., $r _ { c } =$ $\left[ a _ { 1 } , b _ { 1 } , c _ { 1 } , d _ { 1 } , e _ { 1 } , a _ { 2 } , b _ { 2 } , c _ { 2 } , d _ { 2 } , e _ { 2 } , a _ { 3 } , b _ { 3 } , c _ { 3 } , d _ { 3 } , e _ { 3 } \right]$ .

$$
\begin{array} { r l } & { \mathbf { k } _ { \mathcal { C } } = \mathbf { A } ^ { a _ { 1 } } * \mathbf { B } ^ { b _ { 1 } } * \mathbf { C } ^ { c _ { 1 } } * \mathbf { D } ^ { d _ { 1 } } * \mathbf { E } ^ { e _ { 1 } } } \\ & { \phantom { \frac { 1 } { 1 } } + \mathbf { A } ^ { a _ { 2 } } * \mathbf { B } ^ { b _ { 2 } } * \mathbf { C } ^ { c _ { 2 } } * \mathbf { D } ^ { d _ { 2 } } * \mathbf { E } ^ { e _ { 2 } } } \\ & { \phantom { \frac { 1 } { 1 } } + \mathbf { A } ^ { a _ { 3 } } * \mathbf { B } ^ { b _ { 3 } } * \mathbf { C } ^ { c _ { 3 } } * \mathbf { D } ^ { d _ { 3 } } * \mathbf { E } ^ { e _ { 3 } } \phantom { \frac { 1 } { 1 } } \dots } \end{array}
$$

If a composite kernel is, say, ${ \bf k } _ { C } ^ { \prime } \ = \ { \bf A } ^ { 2 } * { \bf E } + { \bf C } * { \bf D }$ , then its Grammar-based representation would be $r _ { c } ^ { \prime } = $ $[ 2 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ]$ . Note: elements of the code vectors can also be fractions.

This encoding scheme has two advantages: (1) Each code $r _ { c }$ preserves its composition, i.e., given the code vector $r _ { c }$ , the base kernels and the operators used to construct $\mathbf { k } _ { C }$ can be interpreted. (2) The code space is continuous, hence, a code $r _ { c } ^ { \prime \prime } = [ 2 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ]$ – which is only a flip of the second element in $r _ { c } ^ { \prime }$ – results in ${ \bf k } _ { C } ^ { \prime \prime } = { }$ $\mathbf { A } ^ { 2 } \ast \mathbf { B } \ast \mathbf { E } + \mathbf { C } \ast \mathbf { D }$ . In general, a small change in the code produces a small modification to the kernel composition (which makes the VAE’s task: $\kappa  \mathcal { Z }$ easier). Past work (GarridoMercha´n and Herna´ndez-Lobato 2020)(Lu et al. 2018) have used one-hot encoding to represent kernel matrices, however, such one-hot schemes suffer from the lack of continuity (as shown in Figure 5(a) and (b) in the Appendix).

However, the context-free grammar $r _ { c }$ does not encode any information from the objective function to be modeled by the complex kernels. We add this to the representation next. Data-based Representation: Given the available function observations $( \mathcal { X } , \mathcal { F } )$ , for each $\mathbf { k } _ { C }$ , we compute the “distances” between its GPR covariance matrix $M _ { \cal C }$ and the covariance matrix of each base kernel, $M _ { b \in B }$ (the covariance matrix is computed as ${ \bf K } _ { i j } = k ( x _ { i } , x _ { j } )$ , $k ( \cdot )$ is the kernel function and $x _ { i } , x _ { j }$ are any two observations). We use the Forbenius norm to compute the matrix distances. This representation is denoted as $\boldsymbol { r } _ { d } \in \mathcal { R } ^ { \vert B \vert }$ .

$$
r _ { d } = | | M _ { \mathcal { C } } - M _ { b \in \mathcal { B } } | | _ { F }
$$

The function information is now encoded in $r _ { d }$ as the covariance matrix is computed using the kernel and the function observations/samples. Kernels that model the function’s data similarly need to be in the same neighborhood so that the model evidence is sufficiently smooth. Figure 5(c) and (d) (in the Appendix) shows the advantage of encoding the objective function’s data in the kernel code.

Thus, the kernel space $\kappa$ consisting of complex kernels is a subset of the space of all positive semi-definite (PSD) matrices S, ${ \boldsymbol { \kappa } } \subseteq \mathbf { s }$ . By restricting our search to $\kappa _ { - }$ not ${ \bf S } -$ the kernels generated through context-free grammar compositions, we can scale the GPR covariance matrix as new function observations arrive (simple kernel functions have closed-form expressions that are expanded to complex kernels for any number of observations). Therefore, our kernel learning problem in Eqn. 3 becomes a kernel selection problem. The final representation of a composite kernel $\mathbf { k } _ { C }$ is $r = [ r _ { c } , r _ { d } ]$ . This is used to train the KerVAE to generate the continuous kernel space $\mathcal { Z }$ .

# Kernel Space Variational Autoencoder (KerVAE) and GPR (KerGPR)

The KerVAE learns a continuous latent space $\mathcal { Z }$ of the discrete kernel space $\kappa$ and has two main components: (1) a probabilistic encoder that models $q _ { \phi } ( z | x ) \ \stackrel {  } { \sim } \ p _ { \theta } ( x | z ) p ( z )$ parameterized by $\phi$ where $p ( z )$ is the prior over the latent space, and (2) a decoder that models the likelihood $p _ { \theta } ( x | z )$ parameterized by $\theta$ . The parameters of $q _ { \phi } ( z | x ) , p _ { \theta } ( x | z )$ are optimized by joint maximization of the ELBO loss (Kingma and Welling 2013),

$$
\begin{array} { r } { \mathcal { L } ( \phi , \theta , x ) = \mathbf { E } _ { q _ { \phi } ( z | x ) } [ \log p _ { \theta } ( x , z ) - \log q _ { \phi } ( z | x ) ] } \end{array}
$$

<html><body><table><tr><td>Q</td><td>fi(x)~A*A*B+C</td><td>f2(x)~C+D</td><td>f(x)~D*B+D</td></tr><tr><td>5</td><td>A*B</td><td>A</td><td>A</td></tr><tr><td>10</td><td>A*B+C</td><td>A+D</td><td>A*B*D+D</td></tr><tr><td>15</td><td>A*A*B+D</td><td>A*C+D</td><td>A*B+D</td></tr><tr><td>20</td><td>A*B+C*D</td><td>A*C+D</td><td>B*D+D</td></tr><tr><td>25</td><td>A*A*B+C*D</td><td>A*C+D</td><td>B*D+D</td></tr></table></body></html>

Table 1: KOBO learning the ground truth kernel. $\{ A , B , C , D , E \} = \{ { \mathrm { S E } }$ , PER, RQ, MAT, LIN

The KerVAE is re-trained after accumulating $v$ function observations as the data representation $r _ { d }$ is re-computed for every new set of observations $\mathcal { D } _ { v } = ( \mathcal { X } _ { v } , \mathcal { F } _ { v } )$ . Once KerVAE is trained, KerGPR is used to determine the optimal kernel $z ^ { * } \in { \mathcal { Z } }$ . The $\mathrm { K e r V A E }$ decoder decodes $z ^ { * }$ to $r _ { c } ^ { * }$ and $r _ { c } ^ { * }$ is easily mapped to $\mathbf { K } ^ { \ast } \in \mathcal { K }$ due to the grammar’s interpretability.

KerGPR: Optimizing on KerVAE’s latent kernel space $\mathcal { Z }$ is also a black-box problem (similar to Eqn. 1) because the objective can only be evaluated for a given kernel $k$ (i.e., by first decoding a given $z$ to $k$ , and computing $k$ ’s model evidence $\mathcal { L } ( \mathbf { K } )$ in Eqn. 4). We use GPR to find $z ^ { * }$ in the latent space and decode to the optimal kernel $\mathbf { K } ^ { * }$ . Thus, our optimization objective is:

$$
\mathbf { K } ^ { * } = \operatorname { D e c } ( \operatorname * { a r g m a x } _ { z \in \mathcal { Z } } P ( \mathcal { F } | \mathcal { X } , \operatorname { D e c } ( z ) ) )
$$

where, $\mathrm { { D e c } ( . ) }$ is the KerVAE decoder that maps a point from $\mathcal { Z }$ to the $\kappa$ space. We use the simple SE kernel in KerGPR as it does not benefit from recursive kernel learning (explained in Technical Appendix). The optimal kernel $\mathbf { K } ^ { * }$ is then used by Function GPR $( f \mathbf { G P R } )$ — the GPR posterior in Eqn. 2 to generate surrogates that closely model the unknown function structure.

# 4 Evaluation and Results KOBO Versus Simple Kernels

Metric: We use the metric of Regret, defined as the difference between the predicted and true minimum, $( f ( \hat { x } ^ { * } ) -$ $f ( x ^ { * } ) )$ . We compare KOBO’s regret against 5 popular base kernels $\mathcal { B } = \{ \mathrm { S E } , \mathrm { P E R } , \mathrm { R Q } , \mathrm { M A \bar { T } , L I N } \}$ which respectively denote Square-Exponential (SE), Periodic (PER), Rational Quadratic (RQ), Mate´rn (MAT), and Linear (LIN) kernels. All KOBO experiments are initialized with the SE kernel.

Synthetic Baseline Functions: We report results from 3 types of synthetic functions $f ( x )$ that are popular benchmarks $( \mathrm { K i m } \ 2 0 2 0 )$ for black-box optimization: Staircase functions in $N = 2 0 0 0$ dimensions; they exhibit non-smooth structures (Al-Roomi 2015). ■ Smooth benchmark functions such as BRANIN commonly used in Bayesian optimization research (Sonja Surjanovic 2013). ■ Periodic functions such as MICHALEWICZ that exhibit repetitions in their shape (Sonja Surjanovic 2013). The top row of Figure 4 visualizes these functions (more details on evaluation parameters in the Technical Appendix). All reported results are an average of 10 runs.

Results: Figures 4(bottom row) plots Regret for the Staircase, Smooth(BRANIN), and the MICHALEWICZ functions, respectively. KOBO minimizes

![](images/a00f195a51e68c4a2532f882553f905806ebf8918184087eb31779e7604839e7.jpg)  
Figure 4: Comparison of KOBO and conventional BO using SE, PER, RQ, and Mate´rn kernels (Bottom Row) for (a) Staircase (b) Smooth Branin, and, (c) Periodic Michalewicz functions (Top Row).

Regret at significantly fewer function evaluations (or samples), especially for Staircase and MICHALEWICZ. For a smooth function like BRANIN, KOBO’s gain is understandably less since the $S E$ and $P E R$ kernels naturally fit the smooth shape. When real world functions exhibit complex (non-smooth) structures and when function evaluations are expensive, KOBO’s advantage is desirable.

# KOBO Versus SOTA Baselines

Another Metric: Since KOBO learns the kernel in the latent space, we will use Model Evidence in addition to Regret. Model Evidence is the normalized probability of generating the observed data $\mathcal { D }$ given a kernel model $\mathbf { K }$ , i.e., $\log ( P ( \mathbf { f } | \mathcal { X } , \mathbf { K } ) ) / | \mathcal { D } |$ (Malkomes, Schaff, and Garnett 2016). Computing the exact model evidence is generally intractable in GPs (Rasmussen, Williams et al. 2006)(MacKay et al. 1998). We use the Bayesian Information Criterion (BIC) to approximate the model evidence as log(P (f |X , K)) = − 21 f T K−1f − 21 log((2π)N |K|), where $N$ is the dimensions of the input space $\mathcal { H } \subseteq \mathbf { R } ^ { N }$ .

We will plot Regret against the number of “Function Evaluations” (on the $\mathrm { \Delta X }$ axis), but for Model Evidence, we will plot it against the number of “Latent Evaluations”. Recall that Model Evidence is the metric used in the latent space of KerVAE to find the “best” kernel $\mathbf { K } ^ { * }$ . Hence “Latent Evaluations” denotes the number of latent space samples $z$ and the corresponding kernels $\mathrm { D e c } ( z ) = \mathbf { K }$ sampled by KerGPR to find $\mathbf { K } ^ { * }$ . This reflects the computation overhead of KOBO.

SOTA Baselines (details in Technical Appendix):

(1) MCMC: The MCMC kernel search (Gardner et al. 2017; Abdessalem et al. 2017) applies the Metropolis-Hastings algorithm (Gardner et al. 2017) on the space of composite kernels $\mathbf { k } _ { C }$ , using model evidence as the function. The proposal distribution is defined as: given a kernel $\mathbf { k }$ , it is either added or multiplied to a kernel from $\boldsymbol { B }$ (chosen with $p$ ).

(2) CKS: The Automatic Statistician/Compositional Kernel Search (CKS) (Duvenaud et al. 2013) method takes advantage of the fact that complex kernels are generated as context-free grammar compositions of positive semi-definite matrices (closed under addition and multiplication); the kernel selection is then a tree search guided by model evidence. CKS searches over the discrete kernel space $\kappa$ using a greedy strategy that, at each iteration, chooses the kernel with the highest model evidence. This kernel is then expanded by composition to a set of new kernels. The search process repeats on this expanded list.

(3) BOMS: The Bayesian Optimization for Model Search (BOMS) (Malkomes, Schaff, and Garnett 2016), unlike CKS’ greedy strategy, is a meta-learning technique, which, conditioned on observations $\mathcal { D }$ , establishes similarities among the kernels in $\kappa$ , i.e., BOMS constructs a kernel between the kernels (“kernel kernel”). Like KOBO, BOMS performs BO in $\kappa$ by defining a Gaussian distribution: ${ \cal P } ( g ) =$ $\mathcal { N } ( g ; \mu _ { g } , \mathbf { K } _ { g } )$ , where $g$ is the model evidence, $\mu _ { g }$ is the mean, and ${ \bf K } _ { g }$ is the covariance (defined by ”kernel kernel” function). ${ \bf K } _ { g }$ is constructed by defining a heuristic similarity measure between two kernels: Hellinger distance.

Results: Figure 5(Row 1) shows that KOBO lowers Regret faster than all SOTA baselines for the three benchmark functions. For Staircase, KOBO attains the global minimum in about 17 function evaluations in contrast to MCMC, which incurs 28, BOMS 32, and CKS 43. For Michalewiez, KOBO attains the minimum in about 10 fewer samples than MCMC. While BOMS and CKS do not attain the minimum but get close to it. However, KOBO’s performance gain is not as pronounced for Branin due to its smooth structure as evidenced in Figure 4.

Figure 5(Row 2) compares Model Evidence for the same benchmarks. For Staircase, KOBO’s KerGPR achieves significantly higher Model Evidence in 20 iterations compared to MCMC, i.e., KOBO’s optimal kernel $\mathbf { K } ^ { * }$ better explains the observed data. For Branin, KOBO can match the Model Evidence of baselines, and performs modestly better for Michalewiez. The results illustrate that KOBO’s performance is superior because a continuous search space learned by KerVAE simplifies the KerGPR optimization to determine $\mathbf { K } ^ { * }$ , implying that KOBO presents an efficient search of the discrete kernel space $\kappa$ in contrast to sub-optimal search techniques like greedy search (CKS), or heuristic metrics for kernel optimization (BOMS).

# Is $K ^ { * }$ indeed learning the structure of $f ( x ) \colon$ ?

Synthetic Functions: If we knew the objective function $f ( x )$ , we could verify if $\mathbf { K ^ { * } }$ has learnt its structure. To test this, we sample a function from a GP with a known kernel $K ^ { + }$ and pretend that to be $f ( x )$ ; we check if KOBO’s $\mathbf { K ^ { * } }$ converges to $K ^ { + }$ . Table 1 reports results from 3 $N$ - dimensional synthetic functions, shown in the top row $N =$ 2000). These synthetic objective functions were sampled from a GP that uses different but known kernels. The subsequent rows show KerVAE’s learnt kernel after $Q$ observations/queries. With more $Q$ , KerGPR closely matches the ground truth kernel.

![](images/0abaa39d4dbf91c3617a7410a40666abc11802b580146988a896914e6b27a1e2.jpg)  
Figure 5: Comparison of KOBO, MCMC, CKS, and BOMS for Staircase (Col 1), Branin (Col 2), and Michalewiez (Col 3) functions: (Row 1) Regret (Row 2) Model Evidence.

Learning Real-world $C O _ { 2 }$ Emission Data: Figure 6’s blue curve plots real $C O _ { 2 }$ emissions data over time (Thoning, Tans, and Komhyr 1989). We test if KOBO’s $\mathbf { K } ^ { * }$ can learn the structure of this blue curve from partial data. Figure 6(a,b,c) show results when KOBO has observed the first $2 0 \%$ , $4 0 \%$ , and $6 0 \%$ of the data, respectively. With the first $2 0 \%$ , $\mathbf { K } ^ { * } = \mathbf { S } \mathbf { E } * \mathbf { P } \mathbf { E } \mathbf { R } + \mathbf { R } \mathbf { Q }$ , hence KOBO’s red curve captures the periodic structure of the early data. When the first $4 0 \%$ of the data is observed, KOBO captures the downward linear trend of the $C O _ { 2 }$ data resulting in $\mathbf { K } ^ { * } = \mathbf { S } \mathbf { E } * \mathrm { P E R } + \mathbf { P E R } + \mathbf { L I N } .$ . With $6 0 \%$ of the data, $\mathbf { K } ^ { * } = \mathrm { S E } * \mathrm { P E R } * \mathrm { R Q } + \mathrm { P E R } * \mathrm { L I N } +$ LIN models the interplay between the function’s periodic structure and linear trends. A conventional Periodic kernel (PER), shown in black in Figure 6(c), is only able to capture the periodic structure, not the upward linear trend, even with $6 0 \%$ of the data.

# User Experiments: (1) Audio Personalization, and (2) Image Recommendation

(1) Audio: We apply KOBO to audio personalization for real volunteers. We deliberately corrupt audio played to the user with the aim of helping the user pick a filter $h ^ { * }$ that cancels the effect of the corruption – equalization – and recovers the original audio; hence maximizing the user’s audio satisfaction. Therefore, a GPR employed in the space of all audio filters $\mathcal { H }$ , maximizes the user’s satisfaction $f ( h )$ at $h ^ { * }$ . At each iteration, the corrupted audio is filtered with a new $h ^ { \prime }$ (recommended by GPR) and played to the user. The user’s rating (0 to 10) of the perceived audio quality serves as the function observations $f ( h ^ { \prime } )$ . User feedback is finite and the frequency selective nature of human hearing (Antoine Lorenzi 2003) makes optimizing $f ( h ) , h \in \mathcal { R } ^ { 4 \widetilde { 0 0 0 } }$ , well suited for kernel learning methods like KOBO.

Table 2: Audio personalization results (the $U { = } 3$ volunteers (rest in Appendix) did not know which kernel was in use).   

<html><body><table><tr><td colspan="10">Hearing Loss</td></tr><tr><td rowspan="2">Q</td><td colspan="3">U1</td><td colspan="3">U2</td><td colspan="3">U3</td></tr><tr><td>SE</td><td>KOBO</td><td>PER</td><td>SE</td><td>KOBO</td><td>PER</td><td>SE</td><td>KOBO</td><td>PER</td></tr><tr><td>5</td><td>6</td><td>6</td><td>6</td><td>8</td><td>8</td><td>8</td><td>6</td><td>6</td><td>6</td></tr><tr><td>10</td><td>6</td><td>8</td><td>6</td><td>8</td><td>8</td><td>8</td><td>6</td><td>7</td><td>7</td></tr><tr><td>15</td><td>6</td><td>10</td><td>7</td><td>8</td><td>10</td><td>8</td><td>7</td><td>9</td><td>7</td></tr><tr><td>20</td><td>10</td><td>10</td><td>10</td><td>9</td><td>10</td><td>9</td><td>7</td><td>10</td><td>10</td></tr><tr><td>25</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>9</td><td>10</td><td>10</td></tr><tr><td colspan="10">Random Audio Corruption</td></tr><tr><td rowspan="2">Q</td><td colspan="3">U1</td><td colspan="3">U2</td><td colspan="3">U3</td></tr><tr><td>SE</td><td>KOBO</td><td>PER</td><td>SE</td><td>KOBO</td><td>PER</td><td>SE</td><td>KOBO</td><td>PER</td></tr><tr><td>5</td><td>1</td><td>1</td><td>1</td><td>3</td><td>3</td><td>3</td><td>1</td><td>1</td><td>1</td></tr><tr><td>10</td><td>2</td><td>3</td><td>1</td><td>4</td><td>4</td><td>4</td><td>2</td><td>2</td><td>3</td></tr><tr><td>15</td><td>2</td><td>4</td><td>5</td><td>4</td><td>10</td><td>4</td><td>2</td><td>2</td><td>3</td></tr><tr><td>20</td><td>4</td><td>10</td><td>5</td><td>4</td><td>10</td><td>9</td><td>5</td><td>8</td><td>4</td></tr><tr><td>25</td><td>8</td><td>10</td><td>9</td><td>8</td><td>10</td><td>9</td><td>10</td><td>8</td><td>7</td></tr></table></body></html>

We invited 6 volunteers to two experiment sessions. In the first, the audio was corrupted with a “hearing loss” audiogram (CDC 2011); in the second, a “random” corruption filter was used (more details in Technical Appendix). By querying the user $Q$ times, each time with a new filter $h ^ { \prime }$ , KOBO expects to learn $K ^ { * }$ , and in turn, maximize the user’s satisfaction. We report Regret against increasing $Q$ , and compare to conventional GPR optimizers with simple base kernels $\{ \mathrm { S E } , \mathrm { P E R } \}$ . The audio demos at various stages of the optimization is made public: https://keroptbo.github.io/.

(2) Images: We also apply KOBO to image recommendation. The motivation is that users are often unable to articulate precisely what images they want, however, given an image they can express their satisfaction. We formulate this as a black-box optimization problem in the image space by posing the question: if a user rates few pictures generated by

80 GPR modeling with $20 \%$ data 80 GPR modeling with $40 \%$ data 80 GPR modeling with $60 \%$ data70 Train split 70 70 PER Mean60 True f(x) 60 60ObservationsX fGPR Confint 8 30 30 3020 10 Metw 20 10 Metww 20 100 0 0X=Time(YYYY-MM) X=Time(YYYY-MM) X=Time(YYYY-MM)

![](images/060a4367846803050a13e7a6935587488a665519b08e41990c621d81ef35aa7d.jpg)  
Figure 6: The blue curve is real-world $C O _ { 2 }$ emissions data from (Thoning, Tans, and Komhyr 1989). The red curve is KOBO’s prediction of the blue curve after observing (a) $20 \%$ (b) $40 \%$ (c) $60 \%$ of the blue data.   
Table 3: Prompt-based image generation user results

an AI model, can the model find the “best” picture to maximize user satisfaction? To realize this, we use a pre-trained VAE-based image generator (Esser, Rombach, and Ommer 2020) (ImGen). The user issues a crude prompt of what they want, allowing ImGen to display an initial image, $x _ { s t a r t }$ . The user scores this image as $f ( x _ { s t a r t } )$ and KOBO is triggered. Table 3 displays $x _ { s t a r t }$ and the images recommended after $Q = 5 , 1 5 , 2 5$ queries. For a subjective measure of optimality, we asked users to describe their ideal image upfront. KOBO’s recommendation at $Q \geq 1 5$ seems to match the prompts quite well (more results in Appendix).

# 5 Related Work

A body of works in BO has explored kernel learning. Closest to KOBO are BOMS (Malkomes, Schaff, and Garnett 2016), Automatic Statistician (AS) and their variants (Duvenaud et al. 2013; Grosse et al. 2012; Kim and Teh 2016), and MCMC (Gardner et al. 2017; Abdessalem et al. 2017) discussed (and used as baselines) earlier. In other work (Teng et al. 2020), authors treat the kernel as a random variable and learn its belief from the data. (Zhen et al. 2020) introduces kernels with random Fourier features for metalearning tasks. The kernel features are learned as latent variables of a model to generate adaptive kernels. In contrast, KOBO uses variational inference as an auxiliary module to only learn a continuous latent kernel space; the KerGPR optimization primarily drives the kernel learning

Authors in (Kandasamy, Schneider, and Po´czos 2015; Gardner et al. 2017; Mutny and Krause 2018; Wang et al. 2018) have improved BO performance in high-dimensional spaces by modeling the function via additive kernels. The objective is decomposed into a sum of functions in low-dimensional space. KOBO’s comprehensive space of kernels from additive and multiplicative compositions is capable of modeling more complex function structures. Finally, (Kusner, Paige, and Herna´ndez-Lobato 2017), (Go´mez-Bombarelli et al. 2018), and (Gonzalez et al. 2015) perform optimization in a continuous latent space learned by VAEs to circumvent categorical data. Authors of (Garrido-Mercha´n and Herna´ndez-Lobato 2020) use one-hot encoding approximations for BO of categorical variables. KOBO borrows from these ideas but applies them to kernel learning.

# 6 Limitations and Conclusion

■ Trading Computation for Sample Efficiency: KOBO incurs rounds of computation in estimating the model evidence $\mathcal { L }$ . However, this does not affect sample efficiency, since KerVAE training is sample-independent. Thus, KOBO’s advantage is in reducing the sample evaluations of $f ( x )$ (e.g., user burden) and not in total CPU cycles.

■ Overfitting to Simple Functions: As iterations progress, KerGPR might learn a kernel more complex than the actual target function $f$ (see $f _ { 1 } ( x )$ in Table 1). Choosing a complex kernel expands the surrogate function space, and may need more samples to converge. To avoid kernel overfitting, we can regularize the kernel complexity, i.e., the length and norm of the kernel grammar codes.

$\mathbf { \delta } \boxed { \mathbf { \overline { { u } } } }$ Latent Space Interpretability: Current latent space learned by KerVAE is abstract and lacks interpretability. An interpretable latent space should offer improvements to KerGPR, facilitating the use of simpler optimizers compared to the expensive Bayesian Optimization in the latent space. To conclude, we propose KOBO, a kernel learning method for GPR. We design a continuous latent space of kernels (using a VAE), and optimize the space via an auxiliary GPR to output an optimal kernel $\mathbf { K } ^ { * }$ . This optimal kernel better models the structure of the objective function, which ensures sample efficiency. We show sample applications and believe the ideas could be extended to others.