# Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data

Weiran $\mathbf { P a n } ^ { 1 , 2 }$ , Wei Wei1, 2\*, Feida $\mathbf { Z } \mathbf { h } \mathbf { u } ^ { 3 }$ , Yong Deng4

1 School of Computer Science and Technology, Huazhong University of Science and Technology, China 2 Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL), China 3 School of Computing and Information Systems, Singapore Management University, Singapore 4 State Grid Fujian Electric Power Co. {panwr, weiw}@hust.edu.cn, fdzhu $@$ smu.edu.sg, hhdyyong@qq.com

# Abstract

We propose a novel sample selection method for image classification in the presence of noisy labels. Existing methods typically consider small-loss samples as correctly labeled. However, some correctly labeled samples are inherently difficult for the model to learn and can exhibit high loss similar to mislabeled samples in the early stages of training. Consequently, setting a threshold on per-sample loss to select correct labels results in a trade-off between precision and recall in sample selection: a lower threshold may miss many correctly labeled hard-to-learn samples (low recall), while a higher threshold may include many mislabeled samples (low precision). To address this issue, our goal is to accurately distinguish correctly labeled yet hard-to-learn samples from mislabeled ones, thus alleviating the trade-off dilemma. We achieve this by considering the trends in model prediction confidence rather than relying solely on loss values. Empirical observations show that only for correctly labeled samples, the model’s prediction confidence for the annotated labels typically increases faster than for any other classes. Based on this insight, we propose tracking the confidence gaps between the annotated labels and other classes during training and evaluating their trends using the Mann-Kendall Test. A sample is considered potentially correctly labeled if all its confidence gaps tend to increase. Our method functions as a plug-and-play component that can be seamlessly integrated into existing sample selection techniques. Experiments on several standard benchmarks and real-world datasets demonstrate that our method enhances the performance of existing methods for learning with noisy labels.

# Code — https://github.com/Aliinton/ConfidenceTracking

# 1 Introduction

The remarkable success of deep learning methods in classification tasks can largely be attributed to high-quality datasets. However, collecting such datasets through manual labeling can be both time-consuming and expensive in many applications. Acquiring data via online queries (Li et al. 2017) or crowdsourcing (Xiao et al. 2015) can construct large-scale datasets at a lower cost but inevitably introduces noise labels. Existing research (Zhang et al. 2021) has shown that deep neural networks can easily fit noisy data, resulting in poor generalization. Therefore, developing algorithms robust to noisy labels is of great practical importance (Natarajan et al. 2013).

Sample selection methods aim to identify correct labels from noisy data, which is increasingly crucial for current deep learning models to effectively learn from noisy labels. There is a general consensus that the small-loss criterion is an effective approach, which assumes samples with small losses are more likely to have correct labels. In this context, the Co-teaching family (Han et al. 2018; Yu et al. 2019; Wei et al. 2020; Xia et al. 2022) and other state-of-the-art methods (Li, Socher, and Hoi 2020; Li et al. 2023) have been proposed. Typically, these methods select samples with losses below a threshold for training. So a higher threshold introduces more incorrect labels while a lower threshold excludes more correct labels, creating a trade-off dilemma between precision and recall in sample selection. To alleviate this issue, one possible solution is establishing another sample selection criterion that can distinguish correct labels from incorrect ones in high-loss data. Then we can combine it with the small-loss criterion to improve recall while maintaining precision in sample selection.

To achieve this, we propose considering the changing trends in model predictions to identify correct labels rather than relying solely on loss values. This is motivated by our empirical observations that although some correctly labeled samples may obtain similar loss values to mislabeled ones, their training dynamics are still distinguishable. As shown in Figure 1, some correctly labeled data can be hard to fit by the model and exhibit similar high loss to mislabeled data in the early training stage. It is difficult to distinguish them by setting a threshold on loss values. But we also observe that, only for the correctly labeled samples, the model’s prediction confidence for annotated labels tends to rise more quickly than for other classes. For instance, with correctly labeled dog images, the model’s prediction confidence for the “dog” class (i.e., the posterior probability of the image belonging to the “dog” class predicted by the model) increases more rapidly than for any other class. Conversely, for cat images mislabeled as “dog”, the model’s prediction confidence for the “cat” class rises slightly faster than for the “dog” class. This observation suggests it’s possible to identify correctly labeled samples by considering training dynamics even when they are indistinguishable from mislabeled samples in terms of loss values.

![](images/843d646ee02eb21341ccdb6478b79f723e1315d323e92bc45dd721ffc5e0cc62.jpg)  
Figure 1: Illustration of Confidence Tracking. We train a PreActResNet-18 model using cross-entropy loss and an SGD optimizer on CIFAR-10N-Worst (CIFAR-10 dataset with human-annotated real-world noisy labels, its noise rate is $4 0 . 2 1 \%$ ). The left graph presents the average per-sample loss distribution in the first 30 epochs. We regard samples with an average loss greater than 1.2 (indicated by the dotted vertical line) as hard-to-learn ones and show the model prediction confidence trajectories on hard-to-learn dogs’ images (middle graph) and mislabeled cats’ images (right graph).

Motivated by this finding, we propose a novel sample selection criterion that selects correct labels by monitoring how model predictions change during training, dubbed Confidence Tracking (CT). Specifically, we track confidence gaps in model predictions between annotated labels and other classes during training. If all confidence gaps of a sample tend to increase, we regard it as a potentially correctly labeled sample. Unlike the small loss criterion which mainly considers the loss values, our method focuses on the changing trend in model predictions, allowing us to distinguish correctly labeled yet hard-to-learn samples from mislabeled ones within high-loss data. In practice, our method functions as a plug-and-play component that can be combined with popular sample selection methods (Arazo et al. 2019; Kim et al. 2021; Pleiss et al. 2020; Li et al. 2023) to enhance their performance.

Our method is related to AUM (Pleiss et al. 2020) which also considers confidence gaps but selects samples with relatively large average logit margins as correctly labeled. Similar to the small-loss criterion, setting a threshold on average logit margins to select correct labels still faces the trade-off dilemma between precision and recall. Our experiments in Section 4 demonstrate that CT accurately selects correct labels from samples rejected by AUM or the small-loss criterion, improving recall while maintaining precision in sample selection, and bringing performance gains to various benchmarks. To sum up, our key contributions are as follows:

• We analyze why correctly labeled and mislabeled samples exhibit different training dynamics from the perspective of coherent gradients (Chatterjee 2020) and provide supporting evidence. • We propose a novel sample selection method based on monitoring changes in model predictions during training, termed Confidence Tracking (CT), which is a plug-and

play component that can integrate with existing sample selection methods and accurately distinguish correctly labeled yet hard-to-learn samples from mislabeled ones. • We experimentally show that our method improves the performance of the existing sample selection methods on various benchmarks and real-world datasets.

# 2 Related Work

We review representative noise-robust methods and sample selection strategies in learning with noisy labels (LNL), excluding studies that assume access to clean label subsets (Xiao et al. 2015; Hendrycks et al. 2018; Qu, Mo, and Niu 2021; Tu et al. 2023).

Noise-robust methods. These methods address noisy labels through robust loss functions, loss correction, label correction, and regularization. Robust loss functions like MAE (Ghosh, Kumar, and Sastry 2017), GCE (Zhang and Sabuncu 2018), SCE(Wang et al. 2019), $\mathcal { L } _ { \mathrm { D M I } } ( \mathrm { X u }$ et al. 2019), NCE(Ma et al. 2020), GJS(Englesson and Azizpour 2021), $f$ -divergence(Wei and Liu 2021) and Peer loss(Liu and Guo 2020) are designed to mitigate noise. Loss correction methods estimate noise transition matrices, achieving success in class-dependent matrices estimating (Patrini et al. 2017; Yao et al. 2020), though accurately estimating the more general instance-dependent noise transition matrix remains challenging without additional assumptions (Xia et al. 2020; Berthon et al. 2021; Yao et al. 2021; Jiang et al. 2022; Cheng et al. 2022; Yang et al. 2022; Li et al. 2024). Label correction replaces noisy labels with model outputs (Tanaka et al. 2018; Yi and Wu 2019). Bootstraping (Reed et al. 2014) and M-correction (Arazo et al. 2019) use a convex combination of noisy labels and model predictions for training. Regularization methods constrain model capacity to prevent memorization. For instance, ELR (Liu et al. 2020) uses temporal regularization, NCR (Iscen et al. 2022) enforces similarity among neighbors, and contrastive learning (Zheltonozhskii et al. 2022; Li et al. 2022; Yi et al. 2022; Xue, Whitecross, and Mirzasoleiman 2022; Peng et al. 2023) enhances robust representation. CS-Isolate (Lin et al. 2023) disentangling style and content in the representation space, distancing hard samples from the decision boundary to ease learning. Other regularization techniques including MixUp (Zhang 2017), label smoothing (Szegedy et al. 2016; Wei et al. 2022a; Ding et al. 2024; Fan et al. 2024), and early stopping (Bai et al. 2021) further improve noise tolerance.

Sample selection methods. These methods identify mislabeled samples using model predictions, representations, or training dynamics. Model prediction-based methods typically regard samples with small losses as correctly labeled. Co-teaching (Han et al. 2018) and its variants (Yu et al. 2019; Wei et al. 2020; Xia et al. 2022) simultaneously train two collaborating networks, selecting small-loss samples for each other to reduce confirmation bias. These methods need to dynamically adjust the select ratio in each iteration, which can be tricky in practice. A more flexible method is fitting a two-component Beta/Gaussian Mixture Model (BMM/GMM) on per-sample loss to differentiate correct and incorrect labels (Arazo et al. 2019; Li, Socher, and Hoi 2020; Nishi et al. 2021; Zhao et al. 2022). Representation-based approaches exploit latent features to distinguish clean from noisy data. CURST (Mirzasoleiman, Cao, and Leskovec 2020) selects samples that provide an approximately low-rank Jacobian matrix, which helps the network learn fast and generalize well. TopoFilter (Wu et al. 2020) assumes clean data clusters together while corrupted data is spread out in the feature representation, using high-order topological information to identify correct labels. FINE (Kim et al. 2021) detects mislabeled samples through the principal components of latent representations made by eigendecomposition. Training dynamics-based methods, utilize model predictions over multiple iterations to generate more accurate sample selections. AUM (Pleiss et al. 2020) and recently proposed HMW (Zhang et al. 2024) rank samples using the average logit margin to select correct labels. L2D (Jia et al. 2023) trains a noise detector based on training dynamics in a supervised manner, avoiding manual designing of the sample selection criterion. However, the pre-trained noise detector may not perform consistently well across different datasets with varying noise ratios, and fine-tuning the noise detector on target datasets requires additional clean data. DIST (Li et al. 2023) considered the fitting difficulty of different samples and proposed an instancedependent sample selection criterion. Unlike previous methods that set a global or class-dependent threshold to select correct labels, they use the momentum maximum confidence of each instance computed across all previous epochs as the threshold value.

Our method belongs to sample selection methods based on training dynamics. Existing approaches typically set thresholds on per-sample loss, confidence, or logit margins to select correct labels, focusing primarily on the value in model predictions. These methods struggle to distinguish between correct and incorrect labels in high-loss data. In contrast, our method emphasizes the trend of confidence gaps in model predictions, enabling the identification of correct labels even within high-loss data. Combining our approach with existing methods can mitigate the trade-off between precision and recall in sample selection, selecting more correctly labeled yet hard-to-learn samples, resulting in stronger performance.

# 3 Method

# Problem Setup

This paper considers the $k$ -class classification problem in the presence of noisy labels. The noisy training set consists of $n$ examples $\bar { \mathcal { D } } = \{ \mathbf { x } ^ { [ i ] } , \hat { \mathbf { y } } ^ { [ i ] } \} _ { i = 1 } ^ { n }$ , where $\mathbf { \bar { x } } ^ { [ i ] } \in \mathbb { R } ^ { d }$ is the ith input and $\hat { \mathbf { y } } ^ { [ i ] } \in \{ 0 , 1 \} ^ { k }$ is a one-hot vector indicating the annotated class. We use the non-bold letters $\hat { y } ^ { [ i ] }$ and $y ^ { [ i ] }$ to represent the annotated and ground truth classes of the ith input, respectively. For simplicity, we denote a deep neural network as $f ( \cdot ; \theta ) : \mathbb { R } ^ { b } \mapsto \mathbb { R } ^ { \bar { k } }$ , which maps $\mathbf { x } ^ { [ i ] }$ to the conditional probability $\mathbf { p } ^ { [ i ] } \in \mathbb { R } ^ { k }$ for each class. We use $f ( \mathbf { x } ^ { [ i ] } ; \boldsymbol { \theta } ) _ { c }$ to represent the conditional probability ${ \mathcal { P } } ( y ^ { [ i ] } = c \mid \mathbf { x } ^ { [ i ] } )$ predicted by the model. Typically, the parameters $\boldsymbol { \theta } \in \mathbb { R } ^ { p }$ are optimized by minimizing the crossentropy loss:

$$
\mathcal { L } _ { \mathrm { C E } } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \ell _ { \mathrm { C E } } ( \hat { \mathbf { y } } ^ { [ i ] } , \mathbf { p } ^ { [ i ] } ) .
$$

$$
\ell _ { \mathrm { C E } } ( \hat { \mathbf { y } } ^ { [ i ] } , \mathbf { p } ^ { [ i ] } ) = - \sum _ { c = 1 } ^ { k } \hat { \mathbf { y } } _ { c } ^ { [ i ] } \log f ( \mathbf { x } ^ { [ i ] } ; \boldsymbol { \theta } ) _ { c } .
$$

We consider the common stochastic gradient descent method, where the dataset is divided into multiple minibatches $B = \{ \mathrm { b } _ { i } \} _ { i = 1 } ^ { | B | }$ , and perform gradient descent on each batch:

$$
\mathcal { L } _ { \mathrm { b } _ { t } } ( \theta ) = - \frac { 1 } { \left| \mathrm { b } _ { t } \right| } \sum _ { ( \mathbf { x } ^ { [ i ] } , \mathbf { y } ^ { [ i ] } ) \in \mathrm { b } _ { t } } \sum _ { c = 1 } ^ { k } \hat { \mathbf { y } } _ { c } ^ { [ i ] } \log f ( \mathbf { x } ^ { [ i ] } ; \theta ) _ { c } ,
$$

$$
\theta _ { t + 1 } = \theta _ { t } - \eta g _ { t } = \theta _ { t } - \eta \nabla \mathcal { L } _ { \mathrm { b } _ { t } } ( \theta _ { t } ) ,
$$

where $\eta$ denotes the learning rate and $\theta _ { t } , \mathrm { b } _ { t }$ represent the parameters and the sampled mini-batch at timestep $t$ , respectively. Following previous work (Cheng et al. 2021; Zhou et al. 2021), we only consider clean-labels-dominant dataset which means training samples are more likely to be annotated with true semantic labels than with any other class labels.

# An Empirical Analysis of Model Learning Process

Before detailing our method, we first analyze why the trend in confidence gaps can serve as a criterion for identifying correct labels in clean-labels-dominant datasets. We begin by examining how gradient descent over batches influences the classification model’s prediction on a specific input. Typically, the parameters $\theta$ do not change significantly in a single gradient descent step. Thus, the nonlinear function $f ( \mathbf { x } ; \boldsymbol { \theta } _ { t + 1 } ) _ { c }$ can be approximated by its first-order Taylor expansion:

$$
f ( \mathbf { x } ; \theta _ { t + 1 } ) _ { c } \approx f ( \mathbf { x } ; \theta _ { t } ) _ { c } + \langle \nabla _ { \theta _ { t } } f ( \mathbf { x } ; \theta _ { t } ) _ { c } , \theta _ { t + 1 } - \theta _ { t } \rangle .
$$

Transition matrix of CIFAR-10N-worst noisy labels Mislabeled samples (truck→automobile) Correctly labeled samples (truck→truck) 2 2 airplane 0.65 0.07 0.07 0.02 0.02 0.02 0.02 0.02 0.08 0.03 0.7 V WWwwlwny automobile 0.04 0.59 0.02 0.03 0.02 0.02 0.01 0.01 0.02 0.25 −01 −01 0.6 bird 0.06 0.04 0.63 0.05 0.06 0.05 0.05 0.03 0.02 0.02 Aothers Ground truth label 0.5 Atruck Aothers cat 0.04 0.03 0.08 0.49 0.04 0.21 0.05 0.02 0.02 0.02 deer 0.04 0.03 0.07 0.04 0.49 0.11 0.04 0.15 0.02 0.01 0.4 −3 Aautomobile −3 Atruck dog 0.03 0.03 0.05 0.18 0.04 0.58 0.03 0.03 0.01 0.01 0 25 50 75 100 125 150 0 25 50 75 100 125 150 0.3 Epoch Epoch frog 0.04 0.04 0.08 0.08 0.05 0.06 0.59 0.03 0.02 0.01 Mislabeled samples (cat→airplane) Correctly labeled samples (cat→cat) 2 1 horse 0.04 0.03 0.02 0.02 0.05 0.06 0.02 0.72 0.02 0.01 0.2 wrwm ship 0.08 0.07 0.02 0.01 0.02 0.02 0.02 0.02 0.68 0.05 −01 −1 0.1 truck 0.04 0.28 0.02 0.02 0.02 0.02 0.01 0.01 0.03 0.55 Aothers 心 5 S Cu 2 加 uK Acat 2 Aothers 7 （ S S −2 −3 Aairplane −3 Acat 0 25 50 75 100 125 150 0 25 50 75 100 125 150 Annotated label Epoch Epoch

For input $\mathbf { x }$ , changes in model’s prediction confidence for class $c$ after a gradient descent step on mini-batch $\mathrm { b } _ { t }$ can be modeled as follows:

$$
f ( \mathbf { x } ; \theta _ { t + 1 } ) _ { c } - f ( \mathbf { x } ; \theta _ { t } ) _ { c } \propto \langle \nabla _ { \theta _ { t } } \ell _ { \mathrm { C E } } ( \mathbf { c } , \mathbf { x } ) , \nabla { \mathcal { L } } _ { \mathrm { b } _ { t } } ( \theta _ { t } ) \rangle .
$$

The degree of alignment between $\nabla _ { \boldsymbol { \theta } _ { t } } \ell _ { \mathrm { C E } } ( \mathbf { c } , \mathbf { x } )$ and $\nabla { L } _ { \mathrm { b } _ { t } } ( \theta _ { t } )$ determines the direction of change in the model’s prediction confidence. Intuitively, if $\nabla { L } _ { \mathrm { b } _ { t } } ( \theta _ { t } )$ is aligned with $\nabla _ { \boldsymbol { \theta } _ { t } } \ell _ { \mathrm { C E } } ( \mathbf { c } , \mathbf { x } )$ , the gradient descent step on mini-batch $\mathbf { b } _ { t }$ will decrease $\ell _ { \mathrm { C E } } ( \mathbf { c } , \mathbf { x } )$ and increase the model’s prediction confidence for class $c$ given input $\mathbf { x }$ .

Previous studies on Coherent Gradients (Chatterjee 2020) indicate that gradients from similar examples are alike, and the overall gradient is stronger in directions where these reinforce each other. Consider a randomly initialized model that outputs random guesses on all inputs. During the early stages of training, the model has not yet fit the given annotations, and the gradients from correct and incorrect labels typically have similar magnitudes. Additionally, the gradients from correct labels are coherent since correctly labeled samples usually share similar patterns. As a result, in the clean-labels-dominant datasets, the correct labels will dominate the overall gradients in the early training stage. This means $\nabla { \mathcal { L } } _ { \mathrm { b } _ { t } } ( \theta _ { t } )$ tends to be most aligned with $\bar { \nabla _ { \boldsymbol { \theta } _ { t } } } \ell _ { \mathrm { C E } } ( \mathbf { y } , \mathbf { x } )$ . In other words, the model’s prediction confidence for ground truth labels tends to increase faster than for other classes in the early training stage. However, as the model gradually fits the correctly labeled examples, their gradients tend to diminish. Then the gradients from the under-fitted mislabeled examples will take over the gradient descent process, leading to the memorization of incorrect labels. To verify our analysis, we randomly sample a subset $\mathcal { N } = \{ ( \mathbf { x } ^ { [ j ] } , \mathbf { y } ^ { [ j ] } , \hat { \mathbf { y } } ^ { [ j ] } ) \} _ { j = 1 } ^ { | \mathcal { N } | }$ consisting of examples from CIFAR-10N-Worst (Wei et al. 2022b), CIFAR-10 with noisy human annotations from Amazon Mechanical Turk, and report the following metrics in different training iterations:

$$
A _ { c } = \frac { 1 } { | \mathcal { N } | } \frac { 1 } { | \mathcal { B } | } \sum _ { t = 1 } ^ { | \mathcal { B } | } \sum _ { j = 1 } ^ { | \mathcal { N } | } \langle \nabla _ { \theta _ { t } } \ell _ { \mathrm { C E } } ( \mathbf { c } , \mathbf { x } ^ { [ j ] } ) , \nabla \mathcal { L } _ { \mathrm { b } _ { t } } ( \theta _ { t } ) \rangle .
$$

Here, $t$ indicates the gradient descent step in one iteration, increasing from 1 to the number of batches in the dataset. $A _ { c }$ measures the degree of alignment between $\nabla _ { \boldsymbol { \theta } _ { t } } \ell _ { \mathrm { C E } } ( \mathbf { c } , \mathbf { x } )$ and the gradients over batches in one iteration. The larger $A _ { c }$ is, the faster the model’s prediction confidence increases in category $c$ . Figure 2 shows the transition matrix of CIFAR10N-worst noisy labels and the trajectories of $A _ { c }$ on different kinds of samples. The experimental results are consistent with our analysis: $A _ { y }$ is highest on both correctly labeled and mislabeled samples in the early training stage. Only after a period of training, $A _ { \hat { y } }$ will have a larger value than $A _ { y }$ . These empirical findings align with the early learning phenomenon (Liu et al. 2020), also known as memorization effect (Arpit et al. 2017), which suggests the deep neural networks optimized by SGD typically learn from correct labels before overfitting noisy data. This phenomenon has already been proved under high-dimensional linear classification (Liu et al. 2020). We provide additional evidence to illustrate why this occurs in deep neural networks. Note this phenomenon only suggests that the model’s prediction confidence for ground truth labels usually rises fastest among all classes in the early training stage. However, it does not guarantee that the model will output high-confidence predictions for ground truth labels since some samples may be difficult to fit, causing the confidence to rise slowly and resulting in correctly labeled examples with high loss, just like the hardto-learn dogs’ images in Figure 1. This motivates us to design a novel sample selection method that considers trends in the model’s prediction confidence rather than relying solely on loss values.

# Sample Selection by Confidence Tracking

Our observations indicate that during the early stages of training, only for the correctly labeled samples, the model’s prediction confidence for annotated labels usually increases faster than for any other classes. Based on this phenomenon, we introduce a novel sample selection method. Generally, for an input $\mathbf { x } ^ { [ i ] }$ , if the model’s prediction confidence increases faster for class $c _ { 1 }$ than for class $c _ { 2 }$ , then the confidence gap between $c _ { 1 }$ and $c _ { 2 }$ should increase:

$$
\begin{array} { r } { \mathbf { p } _ { c _ { 1 } } ^ { [ i ] } ( t + 1 ) - \mathbf { p } _ { c _ { 1 } } ^ { [ i ] } ( t ) > \mathbf { p } _ { c _ { 2 } } ^ { [ i ] } ( t + 1 ) - \mathbf { p } _ { c _ { 2 } } ^ { [ i ] } ( t ) } \\ { \Rightarrow \mathbf { p } _ { c _ { 1 } } ^ { [ i ] } ( t + 1 ) - \mathbf { p } _ { c _ { 2 } } ^ { [ i ] } ( t + 1 ) > \mathbf { p } _ { c _ { 1 } } ^ { [ i ] } ( t ) - \mathbf { p } _ { c _ { 2 } } ^ { [ i ] } ( t ) , } \end{array}
$$

where ${ \bf p } _ { c } ^ { [ i ] } ( t )$ denotes the model’s prediction confidence for $\mathbf { x } ^ { [ i ] }$ in class $c$ at iteration $t$ . If the confidence gaps between the annotated label and other labels all tend to increase, the confidence should rise fastest on the annotated label. We regard such samples as potentially correctly labeled. To implement this idea, for each example $( \mathbf { x } ^ { [ i ] } , \hat { \mathbf { y } } ^ { [ i ] } )$ in the noisy training set, we first collect the following confidence gaps:

$$
\begin{array} { r } { d _ { c } ^ { [ i ] } ( t ) = \mathbf { p } _ { \hat { y } ^ { [ i ] } } ^ { [ i ] } ( t ) - \mathbf { p } _ { c } ^ { [ i ] } ( t ) . } \end{array}
$$

Gathering those confidence gaps over different training iterations, we obtain the following series:

$$
D _ { c } ^ { [ i ] } ( t ) = \{ d _ { c } ^ { [ i ] } ( 1 ) , d _ { c } ^ { [ i ] } ( 2 ) , . . . , d _ { c } ^ { [ i ] } ( t ) \} .
$$

To judge the trend of these series, we utilize the MannKendall Trend Test (Mann 1945; Kendall 1975). This is a non-parametric method and robust against extreme values. We use MK-Test $( \cdot )$ to represent the Mann-Kendall testing process, which takes a series of data as input and outputs the standardized test statistic $Z$ . Our alternative hypothesis points to an upward trend in the confidence gaps series. Formally, our sample selection criterion is:

$$
\operatorname* { m i n } _ { \substack { c \in \{ 1 , 2 , \ldots , k \} \setminus \{ \hat { y } ^ { [ i ] } \} } } \mathbf { M K - T e s t } ( D _ { c } ^ { [ i ] } ( t ) ) > Z _ { 1 - \alpha } ,
$$

where $\alpha$ is the chosen significance level and $Z _ { 1 - \alpha }$ is the $1 0 0 ( 1 - \alpha )$ th percentile of the standard normal distribution. If MK-Tes $\smash { \langle D _ { c } ^ { [ i ] } ( t ) ) > Z _ { 1 - \alpha } }$ , the probability of $D _ { c } ^ { [ i ] } ( t )$ has no trend is less than $\alpha$ so we accept the alternative hypothesis. Therefore, if an example $( \mathbf x ^ { [ i ] } , \hat { \mathbf y } ^ { i } )$ satisfies Equation 11, it suggests that all confidence gaps have an upward trend, so we regard it as potentially correctly labeled.

In practice, we combine Confidence Tracking (CT) with existing sample selection methods to enhance performance. Specifically, we use the union of samples selected by CT and other methods for training. Current methods reliably select correct labels from small-loss samples, while CT identifies correct labels from high-loss samples. This combination maintains precision and improves recall for sample selection. Let $C _ { t }$ denote the selected examples at iteration $t$ , we assign zero weight to samples not in $C _ { t }$ in the loss function:

$$
\mathcal { L } _ { t } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \mathbb { I } ( ( \mathbf { x } ^ { [ i ] } , \hat { \mathbf { y } } ^ { [ i ] } ) \in \mathcal { C } _ { t } ) \ell _ { \mathrm { { C E } } } ( \hat { \mathbf { y } } ^ { [ i ] } , \mathbf { p } ^ { [ i ] } ) .
$$

Following the common practice in sample selection (Li, Socher, and Hoi 2020; Kim et al. 2021; Li et al. 2023), we first warm up the network using standard cross-entropy loss for several epochs. Then we begin to select a potentially clean subset at the end of each epoch and apply Equation 12 for training. Generally, the selected subset likely excludes some mislabeled data, aligning the overall gradient towards memorizing ground truth labels, thereby promoting correct label memorization. This helps to generate better sample selection results in the following iterations. Such a positive feedback loop gradually eliminates mislabeled data from the noisy training set. Section C in the technical appendix provides the details of the Mann-Kendall Trend Test and the pseudo-code of our algorithm.

# 4 Experiment

# Experimental settings

We evaluate our approach on four benchmarks, namely CIFAR-10, CIFAR-100 (Krizhevsky 2009), WebVision (Li et al. 2017), and Food-101N (Lee et al. 2018). For CIFAR10 and CIFAR-100, we experiment with both simulated and human-annotated real-world noisy labels. For simulated noise, we follow the previous setups (Patrini et al. 2017; Liu et al. 2020) and experiment with two types of label noise: symmetric and asymmetric. Symmetric noise is generated by randomly replacing the original labels with all other possible classes. Asymmetric noise is a more realistic setting where labels are replaced by similar classes. We generate asymmetric noise following the same schema with previous works (Liu et al. 2020; Kim et al. 2021). For CIFAR-10, we map Turck $$ Automobile, Bird $$ Airplane, Deer $$ Horse, $\mathrm { C a t }  \mathrm { D o g }$ . For CIFAR-100, we create 20 five-size superclasses and replace the original label with the next class within super-classes circularly. For human-annotated realworld noise, we experiment with the CIFARN (Wei et al. 2022b) dataset (CIFAR-10/100 dataset with noisy human annotations from Amazon Mechanical Turk). Unlike simulated class-dependent noise, real-world noise patterns are instance-dependent making it more challenging to identify correct labels.

WebVision and Food-101N are two real-world noisy datasets. WebVision dataset contains 2.4 million images crawled from the web and its estimated noise rate is about $20 \%$ (Li et al. 2017). Following previous work (Liu et al. 2020), we use the mini WebVision dataset for training, which contains the first 50 classes from the Google image subset (about 66 thousand images), and evaluate model performance on both WebVision and ImageNet ILSVRC12 validation sets (Deng et al. 2009) using InceptionResNetV2 (Szegedy et al. 2017). The Food-101N dataset contains about 310,009 images of food recipes classified in 101 categories and its estimated noise rate is about $20 \%$ . Food101N and the Food-101 dataset (Bossard, Guillaumin, and Van Gool 2014) share the same 101 classes, whereas Food101N has much more images and is more noisy. Following previous work (Lee et al. 2018), we use ResNet50 (He et al. 2016) pretrained on ImageNet (Deng et al. 2009) and evaluate model performance on the Food-101 test set.

Table 1: Test accuracy $( \% )$ of different methods on CIFAR-10 and CIFAR-100 with symmetric, asymmetric, and real-world noisy labels (CIFAR-10N-Worst and CIFAR-100N-Noisy). We implement all methods based on public code and report mean accuracy and standard deviation over five random seeds. We use the Wilcoxon signed-rank test with a confidence level of 0.05 to compare the performance and bold the results where CT brings significant improvements.   

<html><body><table><tr><td>Dataset</td><td colspan="4">CIFAR-10</td><td colspan="4">CIFAR-100</td><td rowspan="2">Avg</td></tr><tr><td>Noise type</td><td>Sym. 20%</td><td>Sym.50%</td><td>Asym. 40%</td><td>Real. 40%</td><td>Sym. 20%</td><td>Sym. 50%</td><td>Asym. 40%</td><td>Real. 40%</td></tr><tr><td>CE</td><td>86.51±0.22 77.41±0.65</td><td></td><td>83.78±1.76</td><td>77.94±0.91</td><td></td><td></td><td>61.37±0.12 46.82±1.32 45.70±0.39</td><td>52.82±0.30 66.54</td><td></td></tr><tr><td>L2D</td><td>92.25±0.12 87.27±0.55</td><td></td><td>82.57±1.31</td><td></td><td></td><td>84.50±0.44 71.05±0.47 60.82±0.59 47.94±0.63</td><td></td><td>59.44±0.33 73.23</td><td></td></tr><tr><td>Co-teaching</td><td></td><td>91.88±0.21 87.58±0.41 87.72±1.00</td><td></td><td></td><td></td><td>85.22±0.28 70.45±0.36 64.07±0.47 58.95±0.91</td><td></td><td>62.32±0.26 76.03</td><td></td></tr><tr><td>CNLCU</td><td>91.92±0.36 87.58±0.73</td><td></td><td>88.14±0.61</td><td></td><td></td><td>86.08±0.39 70.61±0.34 63.87±0.11 55.94±0.73</td><td></td><td>62.28±0.20 75.80</td><td></td></tr><tr><td>HMW</td><td>92.02±0.21 87.81±0.22</td><td></td><td>87.37±0.29</td><td>85.28±0.29</td><td>72.01±0.22</td><td>65.22±0.36</td><td>64.69±0.13</td><td>61.52±0.28</td><td>76.99</td></tr><tr><td>GMM</td><td>91.60±0.28 88.07±0.08</td><td></td><td>89.45±0.85</td><td></td><td></td><td>86.63±0.34 69.59±0.32 63.95±0.44</td><td>65.29±0.42</td><td>60.22±0.23 76.85</td><td></td></tr><tr><td>GMM+CT</td><td>92.57±0.12 89.11±0.21</td><td></td><td>90.55±0.22</td><td>87.33±0.38 71.37±0.67 65.17±0.39</td><td></td><td></td><td>68.84±0.47</td><td>62.73±0.14 78.46</td><td></td></tr><tr><td>FINE</td><td>89.13±0.48 85.66±0.32</td><td></td><td>82.56±2.00</td><td></td><td></td><td>80.09±0.45 70.96±0.45 58.58±0.48</td><td>49.48±0.77</td><td>56.87±0.25</td><td>71.67</td></tr><tr><td>FINE+CT</td><td>92.48±0.21 87.56±0.13</td><td></td><td>86.92±0.51</td><td></td><td></td><td>84.22±0.49 71.17±0.32 58.76±0.41</td><td>53.16±0.88</td><td>58.52±0.15 74.10</td><td></td></tr><tr><td>AUM</td><td>92.31±0.13 87.80±0.24</td><td></td><td>88.21±0.54</td><td></td><td></td><td>86.22±0.11 72.50±0.44 64.90±0.28</td><td>61.25±0.41</td><td>61.75±0.38 76.87</td><td></td></tr><tr><td>AUM+CT</td><td>92.45±0.13 87.91±0.40</td><td></td><td>89.70±0.40</td><td></td><td>87.29±0.16 72.56±0.18 64.99±0.41</td><td></td><td>63.80±0.37</td><td>62.05±0.18 77.59</td><td></td></tr><tr><td>DIST</td><td>92.63±0.15 88.43±0.24</td><td></td><td>90.00±0.42</td><td></td><td>86.39±0.54 72.73±0.32 65.59±0.24</td><td></td><td>66.74±0.81</td><td>60.97±0.20 77.93</td><td></td></tr><tr><td>DIST+CT</td><td>92.43±0.31 88.35±0.16</td><td></td><td>90.58±0.21</td><td></td><td>87.01±0.43 72.78±0.27</td><td>65.51±0.16</td><td>69.05±0.47</td><td>62.18±0.17 78.49</td><td></td></tr></table></body></html>

<html><body><table><tr><td>Noise setting</td><td colspan="3">Sym. 20%</td><td colspan="3">Sym. 50%</td><td colspan="3">Asym. 40%</td><td colspan="3">Real. 40%</td><td colspan="3">Average</td></tr><tr><td>Metric</td><td>P</td><td>R</td><td>F1</td><td>P</td><td>R</td><td>F1</td><td>P</td><td>R</td><td>F1</td><td>P</td><td>R</td><td>F1</td><td>P</td><td>R</td><td>F1</td></tr><tr><td>GMM</td><td>99.86</td><td>71.11</td><td>83.06</td><td>98.97</td><td>69.46</td><td>81.62</td><td>98.64</td><td>63.84</td><td>77.51</td><td>89.52</td><td>72.82</td><td>80.31</td><td>96.75</td><td>69.30</td><td>80.62</td></tr><tr><td>GMM+CT FINE</td><td>99.71 97.93</td><td>83.36 93.23</td><td>90.80 95.52</td><td>98.33 85.68</td><td>79.61 95.14</td><td>87.99 90.16</td><td>96.46 69.98</td><td>82.12 72.13</td><td>88.72 71.03</td><td>87.04 73.23</td><td>84.61 83.68</td><td>85.81 78.11</td><td>95.39 81.70</td><td>82.43 86.05</td><td>88.33 83.70</td></tr><tr><td>FINE+CT</td><td>97.65</td><td>96.34</td><td>96.99</td><td>85.52</td><td>96.16</td><td>90.53</td><td>72.76</td><td>85.90</td><td>78.78</td><td>72.97</td><td>93.99</td><td>82.15</td><td>82.22</td><td>93.10</td><td>87.11</td></tr><tr><td>AUM</td><td>99.09</td><td>92.84</td><td>95.86</td><td>96.15</td><td>86.67</td><td>91.17</td><td>86.26</td><td>72.03</td><td>78.51</td><td>84.48</td><td>84.67</td><td>84.57</td><td>91.50</td><td>84.05</td><td>87.53</td></tr><tr><td>AUM+CT</td><td>99.10</td><td>92.99</td><td>95.95</td><td>96.14</td><td>87.06</td><td>91.38</td><td>88.82</td><td>76.69</td><td>82.31</td><td>84.58</td><td>85.71</td><td>85.14</td><td>92.16</td><td>85.61</td><td>88.69</td></tr><tr><td>DIST</td><td></td><td></td><td>96.84</td><td>95.56</td><td>91.56</td><td>93.51</td><td>96.87</td><td>73.88</td><td>83.81</td><td></td><td></td><td></td><td>94.32</td><td></td><td></td></tr><tr><td></td><td>99.04 94.74</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>85.80</td><td>79.10</td><td>82.31</td><td></td><td>84.82</td><td>89.12</td></tr><tr><td>DIST+CT</td><td>99.0094.93</td><td></td><td>96.93</td><td>95.27</td><td>92.25</td><td>93.73</td><td>96.56</td><td>87.55</td><td>91.84</td><td>85.88</td><td>86.32</td><td>86.09</td><td>94.18</td><td>90.27</td><td>92.15</td></tr></table></body></html>

Table 2: Comparisons of sample selection precision, recall, and F1-score to correct labels on CIFAR-100 dataset with symmetric, asymmetric, and real-world noisy labels. Results are averaged over five random seeds.

# Integrating with Sample Selection Methods

We mainly experiment with the following advanced sample selection methods: Co-teaching (Han et al. 2018), CNLCU (Xia et al. 2022), GMM (Li, Socher, and Hoi 2020), FINE (Kim et al. 2021), L2D (Jia et al. 2023), AUM (Pleiss et al. 2020), DIST (Li et al. 2023), and HMW (Zhang et al. 2024), which cover representative methods based on model predictions, sample representations, and training dynamics. Section F in the technical appendix provides a more detailed introduction and implementation details of those baselines.

Tabel 1 demonstrates the performance when integrating CT with state-of-the-art sample selection approaches. All methods use the same architecture (PreActResNet-18) and training procedure (please refer to Section F in the technique appendix for more details). The significance level $\alpha$ used in CT is set to 0.01 and the warm-up epoch is 30 for all methods. We retain $10 \%$ of the training sets to perform validation and select the model with the best validation performance for the test. CT brings consistent improvement to various baselines. We notice the improvement is more significant under asymmetric and real-world noise. This is because the symmetric noise randomly changes the correct label to another one, making the gradient between noise samples usually incoherent. As a result, the model fits correct labels much faster than incorrect ones, making it easy to distinguish them using the loss values. So sample selection methods based on the small-loss criterion can achieve satisfying performance under symmetric noise. However, under asymmetric or real-world noise, noise labels are more structured (e.g., trucks are generally mislabeled as automobiles rather than other categories), meaning the gradient of this mislabeled data is also coherent. Thus, the model can quickly fit mislabeled data, making the loss of some correctly labeled yet hard-to-learn samples similar to mislabeled ones. The popular small-loss criterion is less effective under such situations, while CT can still distinguish hard correctly labeled data from mislabeled ones. Tabel 2 compares the sample selection precision and recall for correct labels of different methods. Generally, integrating CT with other sample selection methods significantly improves recall and maintains the precision of sample selection which shows CT accurately identifies correct labels from samples rejected by existing sample selection methods, i.e., samples with relatively high loss or logit margins. These results confirm that CT is more powerful than existing sample selection methods in distinguishing correctly labeled yet hard-to-learn and mislabeled samples. Section D in the technical appendix provides additional results on synthetic instance-dependent label noise (Xia et al. 2020).

![](images/a8ee24ecd6c423cf53b0314a180c6fe8575e57ca35f344359964f1e178dd6360.jpg)  
Figure 3: The sample selection results on the CIFAR-100N-noisy dataset. The left graph shows samples selected by both GMM and $\mathbf { G M M + C T }$ and the right graph shows samples selected by $\mathbf { G M M + C T }$ but rejected by GMM. These samples are chosen randomly, not cherry-picked.

Figure 3 compares the sample selected by GMM and $\mathbf { G M M + C T }$ on the CIFAR-100N-noisy dataset. Compared with images selected by both GMM and $\mathbf { G M M + C T }$ , images selected only by $\mathbf { G M M + C T }$ show greater inter-class variability. For instance, the apple images encompass different environment settings and perspectives; the rocket images capture various stages of rocket launches; the clock images display diverse designs and time formats. It intuitively shows that introducing CT can select a richer sample set, which helps improve model performance. Due to the page limit, we further analyze the robustness of CT in Section E in the technical appendix, which shows CT is not sensitive to the choice of $\alpha$ and the number of warm-up epochs.

# Integrating with Advanced LNL Methods

Existing state-of-the-art methods of learning with noisy labels usually combine sample selection with semi-supervised learning to further improve performance. In this section, we integrate CT with CORSE (Cheng et al. 2021), DivideMix (Li, Socher, and Hoi 2020), f-DivideMix (Kim et al. 2021), and DISC (Li et al. 2023) to analyze whether our sample selection procedure can bring further improvement to these state-of-the-art methods. Table 3 shows CT consistently improves the performance of all baselines on realworld noisy datasets.

Table 3: The average test accuracy $( \% )$ over the last 10 epochs on the real-world noisy dataset.   

<html><body><table><tr><td>Test dataset</td><td>Webvision</td><td>ILSVRC12 Food-101N</td></tr><tr><td>Metric</td><td>Top1 Top5 Top1 Top5</td><td>ACC</td></tr><tr><td>CORSE CORSE+CT</td><td>71.70 89.02 68.36 72.11 90.24 68.52</td><td>88.28 84.38 90.03 84.43</td></tr><tr><td>DivideMix DivideMix+CT</td><td>77.44 91.88 74.72 78.12 92.26 75.23</td><td>92.12 86.53 92.19 86.76</td></tr><tr><td>f-DivideMix</td><td>78.36 92.54 75.25 92.22</td><td>86.83</td></tr><tr><td>f-DivideMix+CT</td><td>78.81 92.91 75.66 93.22</td><td>87.05</td></tr><tr><td>DISC</td><td>80.07 92.38 77.40 92.38</td><td>87.32</td></tr><tr><td>DISC+CT</td><td>80.07 92.56 78.26 92.43</td><td>87.45</td></tr></table></body></html>

# 5 Conclusion

In this paper, we introduced a novel sample selection method for image classification with noisy labels, termed Confidence Tracking (CT). Unlike existing methods that rely on small-loss criteria, our approach leverages the observation that only for the correctly labeled samples, the model’s prediction confidence for annotated labels usually increases faster than for any other classes. By monitoring the trends in confidence gaps between annotated labels and other classes, CT effectively distinguishes correctly labeled samples even when they exhibit high losses during training. Our experimental results demonstrate that CT enhances the performance of existing learning with noisy labels (LNL) methods across various benchmarks, showcasing its robustness and reliability. This method successfully alleviates the trade-off dilemma between precision and recall in sample selection when setting a threshold on pre-sample loss, model prediction confidence, or logit margins, offering a more accurate identification of hard-to-learn yet correctly labeled samples. Future research could explore a deeper theoretical understanding of the early learning phenomenon and the development of more advanced sample selection criteria based on training dynamics.

# Acknowledgments

This work was supported in part by the National Natural Science Foundation of China under Grant No. 62276110, No. 62172039, and in part by the fund of Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL). The authors would also like to thank the anonymous reviewers for their comments on improving the quality of this paper.