# Boosting Causal Structure Learning: An Asymmetric Exponential Modulation Gaussian-Based Adaptive Sample Reweighting Framework

Wei Xiao, Hongbin Wang\*, Ming He, Nianbin Wang

College of Computer Science and Technology, Harbin Engineering University, Harbin 150001, China xiaowei , wanghongbin, heming, wangnianbin @hrbeu.edu.cn

# Abstract

Recent advances in differentiable score-based methods for Directed Acyclic Graph (DAG) structure learning have revolutionized the problem of combinatorial structure learning, transforming it into a continuous optimization task. Despite their remarkable success, these methods rely on a key assumption that all samples have the same level of difficulty and no data heterogeneity. When this assumption does not hold, causal discovery algorithms based on it inevitably return networks with many spurious edges. Despite existing research, the current method ignores the reality of outliers in the samples, introducing certain limitations that still result in erroneous edges. Inspired by the rapid decay of the Gaussian distribution as distance from the center increases, we propose an innovative adaptive sample reweighting framework based on asymmetric exponential modulation Gaussian, coined DAGAEG. DAG-AEG boosts DAG structure learning by analyzing the distribution of sample losses and employing the proposed method for adaptive sample attention. Additionally, it can be adapted to heterogeneous data. We used various causal structure learning methods to test the performance of DAGAEG on synthetic and real datasets. The experimental results demonstrate that the proposed framework significantly improves the performance across all methods, outperforming existing methods.

# Introduction

Causal structure learning aims to identify causal relationships between variables from observational data, serving as a foundation for research in many scientific fields (Pearl 2009; Liang et al. 2024). It has significant applications across various domains(Sachs et al. 2005; Pearl 2019; Locatello et al. 2019; Castro, Walker, and Glocker 2020). Additionally, the discovery of causal relationships is considered one of the essential tools for advancing from current Artificial Narrow Intelligence to Artificial General Intelligence (Pearl 2018).

An effective method for discovering causal relationships between variables is to conduct randomized experiments (Boruch 1997). However, in practical applications, this can be quite costly and may even be prohibited in some cases due to ethical concerns. Therefore, learning causal structures from purely observational data has become a research hotspot in recent years $\mathrm { N g }$ et al. 2019; Xu et al. 2020; Li et al. 2020).

Current methods for causal structure learning can be divided into constraint-based and score-based approaches (Vowels, Camgoz, and Bowden 2022). Constraint-based methods, such as the PC (Spirtes, Glymour, and Scheines 2001) and FCI (Spirtes, Meek, and Richardson 2013) algorithms, use conditional independence tests and a series of rules to identify causal directions. Score-based methods leverage predefined score functions to evaluate all candidate graphs in the DAG space to find the optimal causal graph. However, due to the combinatorial acyclicity constraint of causal graphs, finding the score-optimal causal graph is usually NP-hard (Chickering, Heckerman, and Meek 2004). Recently, (Zheng et al. 2018) proposed a new smooth acyclicity constraint, transforming the combinatorial optimization problem into a continuous one, making it possible to optimize the score function through gradient descent. Subsequent differentiable causal discovery methods have extended this approach to nonlinear problems by utilizing various neural network models (Yu et al. 2019; Zhu, Ng, and Chen 2019; $\mathrm { N g }$ et al. 2019; Lachapelle et al. 2019; Ng, Ghassami, and Zhang 2020; Yu et al. 2021; Gao, Shen, and Xia 2021; Ng et al. 2022). Consequently, differentiable score-based causal discovery methods have garnered significant attention in recent research.

Although current differentiable score-based causal discovery methods have achieved significant success, there are still challenges to be addressed:

1. Current score-based methods use an average scoring approach, assuming that the model’s fitting ability is the same for all samples. This assumption neglects the varying difficulty levels among samples, leading to the learning of spurious edges between variables when using average scoring methods (Zhang et al. 2023). 2. Most current score-based methods assume that the collected data is homogeneous, which contradicts realworld situations. When heterogeneous data is present, methods based on this assumption experience a decline in performance (Huang et al. 2020).

Although (Zhang et al. 2023) proposed a model-agnostic adaptive sample reweighting method, it overlooks the presence of outliers in the samples, which limits its effectiveness in enhancing the performance of current causal discovery methods. Outliers often have disproportionately large loss values. Based on this observation, we believe that samples with loss values in the middle of the distribution should be assigned relatively larger weights.

Inspired by the rapid decay of the Gaussian distribution as distance from the center increases, we propose that a Gaussian function can effectively suppress samples at the two ends of the loss distribution. Additionally, we have enhanced the standard Gaussian with an asymmetric exponential function, allowing the new method to not only suppress extreme samples effectively but also assign relatively larger weights to those in the middle of the distribution. This adaptive weighting method can improve the performance of causal discovery, reduce the impact of spurious edges, and be extended to handle heterogeneous data.

Therefore, we propose a more advanced modelagnostic framework: An asymmetric exponential modulation Gaussian-based adaptive sample reweighting framework, termed DAG-AEG. This framework boosts DAG structure learning by leveraging the distribution of sample losses and using the asymmetric exponential modulation Gaussian to adaptively focus on sample importance. DAGAEG assesses the importance the importance of each sample based on the errors of the DAG learner, thereby guiding the learner to perform better on more informative samples.

In summary, our contributions are highlighted as:

1. Inspired by the rapid decay of the Gaussian distribution as distance from the center increases, and considering the relationship between sample importance and loss distribution, we propose an improved scheme called AEG (Asymmetric Exponential Modulation Gaussian). This method utilizes an asymmetric exponent to modulate the standard Gaussian, effectively suppressing samples at the extremes of the distribution while assigning relatively greater weights to central samples.   
2. We propose a more powerful model-independent framework, called DAG-AEG, which employs Gaussian sample adaptive weighting with asymmetric exponential modulation. By learning the distribution of sample losses, DAG-AEG adaptively emphasizes the importance of samples through asymmetric exponential modulation, thereby enhancing DAG structure learning.   
3. Experimental results demonstrate that our proposed DAG-AEG framework significantly boosts causal discovery across both synthetic and real datasets, outperforming existing methods.

# Related Works

Causal structure learning is an indispensable and intricate task pervading in various scientifc felds(Liu et al. 2023), which has garnered extensive research attention in recent years. Currently, methods for learning DAG structures are primarily categorized into two types: constraint-based and score-based. Our DAG-AEG primarily focuses on the latter category.

Constraint-based methods initially perform conditional independence tests to obtain the causal skeleton. They then determine the orientations of the edges, refining them up to the Markov equivalence class through established rules. Examples of such methods include the PC algorithm (Spirtes, Glymour, and Scheines 2001), the FCI algorithm (Spirtes, Meek, and Richardson 2013), and the use of kernel-based conditional independence criteria (Zhang et al. 2012). However, these methods lack robustness, as minor errors in constructing the graph skeleton can lead to significant inaccuracies in the inferred Markov equivalence class.

Score-based solutions utilize a scoring function and search strategies to identify the graph that yields the highest score (Ramsey et al. 2017). This approach helps mitigate the shortcomings of constraint-based methods. However, it still faces a significant challenge: the intractable combinatorial nature of the acyclic graph space (Chickering, Heckerman, and Meek 2004). Recently, a groundbreaking study addressed this issue: NOTEARS (Zheng et al. 2018) recasts the combinatorial graph search problem as a continuous optimization problem, resulting in a differentiable score-based optimization method that allows the score function to be optimized through gradient descent. This approach has inspired a substantial body of related literature, such as (Yu et al. 2019; Lachapelle et al. 2019; Zhu, Ng, and Chen 2019; Zheng et al. 2020; Yang et al. 2021; Liu et al. 2023)

Although score-based methods have achieved notable results, they suffer from a significant problem: current methods overly rely on easily fitting samples, which results in spurious edges in the learned causal models. ReScore (Zhang et al. 2023) addresses this by assigning weights to samples based on their corresponding loss—the greater the loss, the more informative the sample, and the higher the assigned weight should be. However, this approach overlooks the presence of outliers in the samples. While this framework can enhance the capability of causal discovery methods to some extent, it still has its flaws.

# Preliminaries

# Causal Structure Learning

Causal structure learning aims to infer the Structural Equation Model(SEM) from the observational data, which models the data generating procedure (Pearl, Glymour, and Jewell 2016). Formally, let $\textbf { X } \in \ { \cal R } ^ { n \times d }$ be a sample consisting of n independent and identically distributed observational data of d variables. And we write directed acyclic graphs as $\mathcal { G } = ( \mathrm { V } , \mathrm { E } )$ . Where the nodes $\mathrm { \Delta V }$ represents the observed variables, denoted as $\mathrm { ~ X ~ } = \ ( X _ { 1 } , X _ { 2 } , \cdot \cdot \cdot X _ { d } )$ and each $\mathrm { e d g e ( i , j ) \in E }$ represents a direct causal relation from $X _ { i }$ to $X _ { j }$ . Given $\mathbf { X }$ , we try to learn a DAG $\mathcal { G }$ from a given distribution P (X). To model X, we can use SEM to model the causal relations between a variable $X _ { i } \in X$ and its parents. This can be formally expressed by Equation 1:

$$
X _ { i } = f _ { i } \bigl ( X _ { p a ( i ) } , Z _ { i } \bigr ) , \mathrm { i = 1 , 2 , \cdot \cdot \cdot , d }
$$

Where $X _ { p a ( i ) }$ denote the parents of $X _ { i }$ , $f _ { i }$ is the causal structure function which can be any linear or nonlinear function, and $Z _ { i } \in Z$ is jointly independent noise variable.

# Score-based Causal Structure Learning

Score-based methods assign a score S to each candidate graph and then search for the best score over the space of all DAGs. This can be formulated as the following combinatorial optimization problem:

$$
\operatorname* { m i n } _ { \mathcal { G } } S ( A ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \mathcal { L } ( x _ { i } , f ( A , x _ { i } , \theta ) ) + \lambda | A | _ { 1 }
$$

Where S is a score function, $\mathcal { G }$ refers to a directed graph, and A is the adjacency matrix of $\mathcal { G }$ . While there have been welldefined score function such as the Bayesian Information Criterion (Maxwell Chickering and Heckerman 1997) or Minimum Description Length score (Bouckaert 1993) and the Bayesian Gaussian equivalent score (Geiger and Heckerman 1994), penalized least-squares loss (Zheng et al. 2020), Evidence Lower Bound (Yu et al. 2019), loglikelihood with complexity regularizers $( \mathrm { N g }$ , Ghassami, and Zhang 2020), Maximum Mean Discrepancy (Goudet et al. 2018). Equation 2 is generally NP-hard to solve(Chickering, Heckerman, and Meek 2004), largely due to the combinatorial nature of its acyclicity constraint with the number of DAGs increasing super-exponentially in the number of graph nodes(Zhu, Ng, and Chen 2019). Fortunately, (Zheng et al. 2018) introduced a smooth characterization for the acyclicity converts the combinatorial optimization problem into a continuous constrained optimization problem:

$$
\operatorname* { m i n } _ { \mathcal { G } } S ( A ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \mathcal { L } ( x _ { i } , f ( A , x _ { i } , \theta ) ) + \lambda | A | _ { 1 }
$$

Where $h ( A ) = t r ( e ^ { A ^ { \circ } A } ) - d = 0$ is a continuous acyclicity constraint. Therefore, this new form of method is referred to as differentiable score-based causal structure learning. To facilitate learning causal graphs, numerous forms(e.g., augmented Lagrangian method) can be applied o solve the Equation. Then, the Equation can be further reformulated as:

$$
\begin{array} { c } { { \displaystyle \operatorname* { m i n } _ { \mathcal { G } } S ( A ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \mathcal { L } ( x _ { i } , f ( A , x _ { i } , \theta ) ) + \lambda | A | _ { 1 } + } } \\ { { \alpha h ( A ) + \frac { 1 } { 2 } \rho | h ( A ) | ^ { 2 } } } \end{array}
$$

Where $\alpha$ and $\rho$ are coefficients in the Lagrangian method. Although differentiable-based causal discovery methods have achieved notable results, they typically employ an average scoring function (Equation 3). This approach overlooks variations in sample difficulty, which can lead DAG learners to incorrectly infer false edges.

# The Proposed DAG-AEG Methodology

To address the issues present in current differentiable scorebased methods, we propose a general adaptive sample reweighting framework based on asymmetric exponential modulated Gaussian which boosted DAG structure learning by adaptively focusing on the importance of samples through modulated Gaussian. For clarity, we first present the principles of this framework. Afterward, we describe the approach of applying it to DAG structure learning.

# The Theory of Asymmetric Exponential Modulation Gaussian

In current differentiable score-based causal discovery approaches, applying the average score function uniformly across all samples can lead to the DAG learner overfitting those samples that are easier to fit, thereby introducing false edges. ReScore(Zhang et al. 2023) proposes that even though the importance of each sample is unknown, weights can be assigned based on the relative ease or difficulty with which the DAG learner fits the samples:

$$
\begin{array} { l } { \displaystyle \operatorname* { m i n } _ { \mathcal { G } } S _ { w } ( A ) = \sum _ { i = 1 } ^ { n } w _ { i } \mathcal { L } ( x _ { i } , f ( A , x _ { i } , \theta ) ) + \lambda | A | _ { 1 } } \\ { \displaystyle \qquad + \alpha h ( A ) + \frac 1 2 \rho | h ( A ) | ^ { 2 } } \end{array}
$$

Where $\mathrm { w } = ( w _ { 1 } , w _ { 2 } , . . . , w _ { n } )$ is a sample reweighting vector. In ReScore, the authors assign larger weights $w _ { i }$ to samples that are more challenging to fit. While this enhances the performance of the DAG learner to some extent, it overlooks the presence of outliers in the samples, still posing certain limitations.

There is a general consensus that neural network models tend to focus on simpler samples that are easier to fit, meaning that samples with smaller loss values often contribute less to the overall model training. Conversely, samples with larger losses, which are deemed more challenging, tend to contribute more significantly (Li, Liu, and Wang 2019). This principle is similarly applicable in the learning of DAG structures. However, the presence of outliers in the data complicates this scenario. Outliers typically exhibit higher loss values and are often found at the extremes of the sample loss distribution. Consequently, a high loss value does not necessarily warrant a higher weight in the learning process. In light of this, it is necessary to moderate the influence of samples at both extremes of the loss distribution—those with very small or very large losses—while assigning relatively greater weights to samples that are distributed more centrally within the loss distribution.

Inspired by the rapid decay of a Gaussian distribution as the distance from the center increases, we believe that the Gaussian function can effectively reduce the influence of extreme values at both ends of the sample distribution, which can be expressed in this form:

$$
\mathrm { w } = \frac { 1 } { \sigma \sqrt { 2 \pi } } \exp { ( - \frac { 1 } { 2 } ( \frac { \mathcal { L } ^ { 2 } - \mu } { \sigma } ) ^ { 2 } ) }
$$

where $\mathcal { L }$ represents the reconstruction loss of sample $\mathbf { x } , \mu$ represents the intermediate value of the batch sample loss, and $\sigma$ is the hyperparameter.

However, the standard Gaussian distribution is symmetric around its central point. To follow the consensus that samples with higher loss values within a specific range are often more important in the sample weighting process, we need to introduce a bias function. Here, we use an asymmetric exponential function as the bias for the standard Gaussian. This new approach effectively suppresses samples at both ends while giving relatively greater weights to the middle samples. We refer to this modified Gaussian distribution as The Asymmetric Exponential Modulation Gaussian (AEG):

$$
\begin{array} { l } { \displaystyle { \mathrm { w } = \frac { 1 } { \sigma \sqrt { 2 \pi } } \exp { ( - \frac { 1 } { 2 } \Big ( \frac { \mathscr { L } ^ { 2 } - \mu } { \sigma } \Big ) ^ { 2 } } ) } + } \\ { \displaystyle \beta ( \mathscr { L } ^ { 2 } - \mu ) \exp { \left( - \gamma ( \mathscr { L } ^ { 2 } - \mu ) \right) } * 1 _ { \{ \mathscr { L } ^ { 2 } > \mu \} } } \end{array}
$$

The indicator function $1 _ { \{ \mathcal { L } ^ { 2 } > \mu \} }$ is effective only when the sample loss is above the median. The parameters $\beta$ and $\gamma$ are dynamically adjusted based on the loss magnitude. The core improvement lies in minimizing the contribution of samples with extreme loss values—either very high or very low (easily fitted samples and outliers)—to the DAG learner by assigning them smaller weights. Conversely, samples with intermediate loss values contribute more to the DAG learner. The larger their loss, the more they contribute, providing additional insight into depicting causal edges, thus necessitating larger weights for these samples.

# Boosting Causal Structure Learning via AEG-Based Adaptive Sample Reweighting

We believe that samples with either excessively high or excessively low losses are not ideal for guiding the DAG learner. The primary reason is that samples with a high degree of fit offer limited critical information, while samples with a low degree of fit are more likely to be outliers that the model cannot accurately fit. In contrast, samples with losses that fall in the middle range typically contain a substantial amount of critical information. In the previous section, we discussed the weight adaptation method. In this section, we will apply this method to causal structure learning to enhance its performance.

Therefore, we propose a more powerful model-agnostic framework, called DAG-AEG, an adaptive sample reweighting framework based on AEG. This framework boosts DAG structure learning by analyzing the distribution of sample losses and employing the proposed method for adaptive sample attention. We adopt a Bi-Level learning approach, where the learned weights w are used to recalculate scores. Specifically, this framework is applied to causal structure learning, represented in the following form:

$$
\begin{array} { l } { \displaystyle \operatorname* { m i n } _ { \mathcal { G } } S _ { w } ( A ) = \displaystyle \sum _ { i = 1 } ^ { n } w _ { i } \mathcal { L } ( x _ { i } , f ( A , x _ { i } , \theta _ { \mathcal { G } } ) ) + \lambda | A | _ { 1 } } \\ { \displaystyle \qquad + \alpha h ( A ) + \frac { 1 } { 2 } \rho | h ( A ) | ^ { 2 } , } \\ { \displaystyle \qquad \mathrm { s . t . } w \in a r g m a x \sum _ { i = 1 } ^ { n } w _ { i } \mathcal { L } ( x _ { i } , f ( A , x _ { i } , \theta _ { \mathcal { G } } ) ) } \\ { \displaystyle w _ { i | L _ { i } \in \mathcal { L } } = \frac { 1 } { \sigma \sqrt { 2 \pi } } \exp ( - \frac { 1 } { 2 } \Big ( \frac { L _ { i } ^ { 2 } - \mu } { \sigma } \Big ) ^ { 2 } ) + } \\ { \displaystyle \qquad \beta ( L _ { i } ^ { 2 } - \mu ) \exp \Big ( - \gamma ( L _ { i } ^ { 2 } - \mu ) \Big ) \neq 1 _ { \{ L _ { i } ^ { 2 } > \mu \} } } \end{array}
$$

Algorithm 1: The proposed DAG-AEG framework to differ  
entiable score-based causal discovery   
Input: Observed data $\mathbf { X } \in \mathfrak { R } ^ { n \times d }$ , reweighting model pa  
rameters $\theta _ { w }$ , maximum epoch in the inner loop $K _ { i n n e r }$ ,   
maximum epoch in the outer loop $K _ { o u t e r }$   
Initialize: initialize $\theta _ { w }$ to uniformly output $\textstyle { \frac { 1 } { n } }$   
Output: predicted $\mathcal { G }$ 1: for $k _ { 1 } = 0$ to $K _ { o u t e r }$ do 2: Fix reweighting model parameters $\theta _ { w }$ ; 3: Get w through reweighting model utilizing the sample loss values calculated by the DAG learner; 4: Optimize DAG learner parameters $\theta _ { \mathcal { G } }$ by minimizing min $S _ { w } ( A )$ ; 5: if $k _ { 1 } \geq$ start reweighting epoch then 6: for $k _ { 2 } = 0$ to $K _ { i n n e r }$ do 7: Fix DAG learner parameters $\theta _ { \mathcal { G } }$ ; 8: Calculate w through reweighting model in equation 7; 9: Optimize $\theta _ { w }$ by maximizing   
10: $\sum _ { k _ { 2 } } ^ { \mathbf { \curvearrowright } _ { n } } { } _ { i = 1 } ^ { w _ { i } \mathcal { L } ( x _ { i } , f ( A , x _ { i } , \theta _ { \mathcal { G } } ) ) } ;$   
11: end for   
12: $\begin{array} { l } { { k _ { 1 }  k _ { 1 } + 1 } } \\ { { k _ { 2 }  0 } } \end{array}$   
13:   
14: end if   
15: end for   
16: return predicted $\mathcal { G }$

The formula includes two objectives, with the lowerlevel objective nested within the upper-level objective. In the lower-level loop, the DAG learner is fixed. Based on the sample loss distribution learned by the DAG learner, the reweighted scoring function is learned using the asymmetric exponential modulation Gaussian. In the upper-level loop, this reweighted scoring function is minimized to optimize the DAG learner on the reweighted observational data determined in the lower-level loop. By alternately training these upper and lower loops, the importance of each sample is adaptively assessed according to the error of the DAG learner. This process gradually guides the DAG learner to perform better on informative samples and to learn an optimal causal structure model, as detailed in the algorithm 1. As a general sample adaptive weighting framework, DAGAEG can be applied to any differentiable score-based causal structure learning methods.

# Experiments

In this section, we conduct experiments on both synthetic and real-world datasets to demonstrate the effectiveness of our method DAG-AEG. We are interested in how well it can 1)broadly enhance the differentiable score-based causal discovery baselines and 2)perform when dealing with heterogeneous data.

# Experimental Settings

Baselines. To evaluate its performance in enhancing existing baselines, we selected four state-of-the-art causal discovery methods: two for linear systems (NOTEARS(Zheng et al. 2018), GOLEM(Ng, Ghassami, and Zhang 2020)) and two for nonlinear settings (NOTEARS-MLP(Zheng et al. 2020), GraN-DAG(Lachapelle et al. 2019)). We integrated our DAG-AEG framework with these methods and compared it against the four baselines. In terms of handling heterogeneous data, we compared GOLEM $+$ DAGAEG and NOTEARS-MLP $^ +$ DAG-AEG to the state-of-theart baseline CD-NOD (Huang et al. 2020) and the recently proposed approach DICD(Wang et al. 2022). In both cases, we also compared DAG-AEG with the current model-agnostic state-of-the-art method ReScore(Zhang et al. 2023). The detailed configuration of hyperparameters for each model is comprehensively documented in the ”Hyperparameter Design” section of the technical appendix.

Table 1: Results of linear models for ER graphs with 10,20 and 50 nodes.   

<html><body><table><tr><td rowspan="2">d</td><td rowspan="2">METHODS</td><td colspan="4">ER2</td><td colspan="4">ER4</td></tr><tr><td>TPR↑</td><td>FDR↓</td><td>SHD↓</td><td>SID↓</td><td>TPR↑</td><td>FDR↓</td><td>SHD↓</td><td>SID↓</td></tr><tr><td rowspan="6">10</td><td>NOTEARS</td><td>0.85±0.09</td><td>0.07±0.07</td><td>5.8±2.2</td><td>20.8±5.2</td><td>0.79±0.11</td><td>0.09±0.05</td><td>10.0±5.2</td><td>25.8±9.9</td></tr><tr><td>+ReScore</td><td></td><td>0.89±0.07+5% 0.08±0.09-14% 4.6±2.3+21%</td><td></td><td>12.8±7.0+39%</td><td>0.85±0.04+8% 0.05±0.04+44%</td><td></td><td>7.2±1.9+28%</td><td>24.2±8.4+6%</td></tr><tr><td>+ DAG-AEG</td><td></td><td>0.94±0.06+11%0.05±0.06+29%</td><td>2.0±1.9+66%</td><td>6.6±8.4+68%</td><td>0.89±0.05+13%0.06±0.06+33%</td><td></td><td>5.7±2.9+43%</td><td>6.6±8.4+74%</td></tr><tr><td>GOLEM</td><td>0.87±0.06</td><td>0.22±0.11</td><td>6.5±3.4</td><td>13.0±6.7</td><td>0.63±0.03</td><td>0.16±0.03</td><td>17.2±1.3</td><td>48.0±13.3</td></tr><tr><td>+ReScore</td><td>0.88±0.06+1% 0.21±0.11+5%</td><td></td><td>6.0±3.4+8%</td><td>12.4±6.3+5%</td><td>0.66±0.06+5% 0.17±0.01-6%</td><td></td><td>16.2±1.0+6%</td><td>46.7±13.3+3%</td></tr><tr><td>+ DAG-AEG</td><td></td><td>0.95±0.04+9% 0.15±0.11+32% 3.6±2.5+45%</td><td></td><td>7.8±8.1+40%</td><td>0.82±0.08+30%0.08±0.03+50%</td><td></td><td>9.4±3.2+45%</td><td>26.3±8.8+45%</td></tr><tr><td rowspan="6">20</td><td>NOTEARS</td><td>0.85±0.08</td><td>0.09±0.03</td><td>9.2±3.8</td><td>55.4±31.1</td><td>0.74±0.02 0.23±0.03</td><td>39.4±7.9</td><td></td><td>185.8±38.1</td></tr><tr><td>+ReScore</td><td></td><td>0.87±0.07+2% 0.11±0.05-22%</td><td>8.8±3.5+4%</td><td>50.6±26.3+9%</td><td>0.79±0.05+7%0</td><td>0.28±0.05-22%</td><td>36.8±7.9+7%</td><td>122.7±40.1+34%</td></tr><tr><td>+ DAG-AEG</td><td></td><td>0.91±0.02+7% 0.10±0.03-11% 7.2±1.5+22%</td><td></td><td>35.2±14.3+37%</td><td>0.86±0.02+16%0.23±0.05+0%</td><td></td><td>30.6±6.1+22%</td><td>122.7±40.1+34%</td></tr><tr><td>GOLEM</td><td>0.75±0.12</td><td>0.20±0.11</td><td>17.0±6.1</td><td>78.2±22.6</td><td>0.46±0.06</td><td>0.50±0.06</td><td>73.6±7.9</td><td>249.8±7.8</td></tr><tr><td>+ReScore</td><td>0.76±0.06+1% 0.20±0.10+0% 15.8±5.8+7%</td><td></td><td></td><td>77.0±21.5+2%</td><td>0.48±0.06+4% 0.43±0.10+14% 70.2±5.8+5%</td><td></td><td></td><td>246.2±11.4+1%</td></tr><tr><td>+ DAG-AEG</td><td>0.96±0.03+28%0.20±0.10+0%</td><td>11.3±8.8+34%</td><td></td><td>17.6±18.2+78%</td><td>0.57±0.20+24%0.39±0.04+22%</td><td>61.1±8.7+17%</td><td></td><td>187.0±66.2+25%</td></tr><tr><td rowspan="6">50</td><td>NOTEARS</td><td>0.79±0.06 0.09±0.03</td><td>27.6±7.7</td><td></td><td>427.0±186.1</td><td>0.51±0.12 0.27±0.10</td><td>133.4±29.5</td><td></td><td>1644±172</td></tr><tr><td>+ReScore</td><td></td><td></td><td></td><td></td><td>0.88±0.06+11%0.15±0.04-67% 26.2±7.6+5%266.0±146.4+38%0.52±0.21+2% 0.29±0.07-7% 130.2±37.4+2%</td><td></td><td></td><td>1454±337+12%</td></tr><tr><td>+ DAG-AEG</td><td></td><td></td><td></td><td></td><td>0.88±0.05+1%0.14±0.04-56%25.0±6.2+9%255.3±117.5+40%0.69±0.13+35%0.25±0.07+7%104.8±29.7+21%1191±282+28%</td><td></td><td></td><td></td></tr><tr><td>GOLEM</td><td>0.80±0.09</td><td>0.35±0.09</td><td>68.6±19.7</td><td>433.5±215.6</td><td>0.31±0.11</td><td>0.68±0.06</td><td>150.6±25.1</td><td>1775±162</td></tr><tr><td>+ReScore</td><td></td><td></td><td></td><td></td><td>0.82±0.15+3% 0.33±0.14+6% 63.4±27.9+8% 430.2±155.5+1%0.39±0.06+26%0.66±0.06+3% 146.3±26.3+3%1644±115+7%</td><td></td><td></td><td></td></tr><tr><td>+ DAG-AEG</td><td></td><td></td><td></td><td></td><td>0.91±0.14+14%0.32±0.16+9% 51.7±29.4+25%173.2±258.0+6%0.40±0.16+29%0.56±0.14+18%136.1±32.1+10% 1496±131+16%</td><td></td><td></td><td></td></tr></table></body></html>

Datasets. To validate the performance of our framework on the aforementioned problems 1) and 2), we follow the convention of causal discovery and use the same experimental setup as in (Zhang et al. 2023), specifically as follows:

• To assess the performance in enhancing existing baselines, we employ a well-known graph sampling model: Erdos-Renyi (ER) to generate random DAGs. We varied the number of variables $\dot { \boldsymbol { d } } = \{ 1 0 , 2 0 , 5 0 \} ,$ ) with edge density $( d e g r e e = \{ 2 , 4 \}$ ,denoted as ERk or SFk). For each graph, we generate 10 datasets of 2,000 samples. For the linear settings, similar to (Zheng et al. 2018) and(Gao, Shen, and Xia 2021), the coefficients are assigned following Uniform distribution $U ( - 2 , - 0 . 5 ) \cup$ $U \bar { ( 0 . 5 , 2 ) }$ with additive standard Gaussian noise. For the nonlinear settings, as in (Zheng et al. 2020), we generate the ground truth SEM using Equation 1, under the Gaussian process (GP) with a radial basis function kernel of bandwidth one, where $f _ { i } ( \cdot )$ is additive noise model with $Z _ { i }$ as an i.i.d. random variable following a standard normal distribution. Notice that both of these settings are known to be fully identifiable (Pen˜a 2018; Peters et al. 2014). In addition, we report the mean and standard deviations of the metrics to ensure a fair comparison.

• To evaluate performance on heterogeneous data, we used both synthetic and real-world heterogeneous data.

– Synthetic heterogeneous data: We considered both linear and nonlinear settings $\mathrm { { \acute { n } = 1 0 0 0 } }$ , ${ \mathrm { d } } = 2 0$ , ER2) containing two distinct groups. $1 0 \%$ of observations come from a disadvantaged group, where half of the noise variables $Z _ { i }$ (defined in Equation 1) follow $N ( 0 , 1 )$ and the other half follow $N ( 0 , 0 . 1 )$ . Conversely, $9 0 \%$ of observations are from a dominant group where the scales of noise variables are reversed. – Real-world heterogeneous data: We used the wellknown Sachs dataset (Sachs et al. 2005), which measures the levels of various proteins and phospholipids in human cells under nine different perturbation conditions, each involving specific reagents. With perturbation conditions annotated, we treated Sachs as real-world heterogeneous data(Mooij, Magliacane, and Claassen 2020; Zhang et al. 2023). The ground truth causal graph for this dataset includes 11 variables and 17 edges. Our tests were conducted on observational data comprising 7466 samples.

Evaluation Metrics. To evaluate the performance of DAG structure learning, we consider four metrics: True Positive Rate (TPR), False Discovery Rate (FDR), Structural Hamming Distance (SHD), and Structural Intervention Distance (SID) (Peters and Bu¨hlmann 2015), averaged over ten random trials. The SHD represents the minimum number of edge additions, deletions, and reversals needed to convert the estimated graph into the true DAG, encompassing both false positives and false negatives. The SID measures the number of interventional distributions that differ between the true and recovered networks. For optimal performance,

Table 2: Results of nonlinear models for ER graphs with 10,20 and 50 nodes.   

<html><body><table><tr><td rowspan="3">d</td><td rowspan="3">METHODS</td><td colspan="4">ER2</td><td colspan="3">ER4</td></tr><tr><td>TPR↑</td><td>FDR↓</td><td>SHD↓</td><td>SID↓</td><td>TPR↑ FDR↓</td><td>SHD↓</td><td>SID↓</td></tr><tr><td>NOTEARS-MLP</td><td>0.76±0.17 0.14±0.09</td><td>7.0±3.5</td><td>17.9±10.0</td><td>0.83±0.05</td><td>0.21±0.04 10.9±1.9</td><td>28.6±12.0</td></tr><tr><td rowspan="6">10</td><td>+ReScore</td><td></td><td>0.73±0.07-4% 0.10±0.09+29% 6.8±2.9+3%</td><td></td><td>20.3±9.7-13%</td><td>0.94±0.06+13%0.15±0.06+29%</td><td>6.8±2.7+38%</td><td>8.8±12.4+69%</td></tr><tr><td>+ DAG-AEG</td><td>0.77±0.11+1% 0.09±0.06+36% 5.9±1.9+16%</td><td></td><td></td><td>16.5±7.4+8%</td><td>0.98±0.02+18%0.11±0.02+48%</td><td>5.2±1.1+52%</td><td>3.6±5.3+87%</td></tr><tr><td>GraN-DaG</td><td>0.88±0.06</td><td>0.02±0.03</td><td>2.7±1.6</td><td>8.7±4.8</td><td>0.98±0.02 0.12±0.03</td><td>5.4±1.1</td><td>3.7±4.8</td></tr><tr><td>+ReScore</td><td></td><td>0.90±0.05+2% 0.01±0.03+50% 2.4±1.1+11%</td><td></td><td>7.2±3.0+17%</td><td>0.99±0.01+1% 0.11±0.01+8%</td><td>4.8±0.6+11%</td><td>0.5±0.81+87%</td></tr><tr><td>+ DAG-AEG</td><td></td><td>0.90±0.04+2% 0.01±0.02+50% 2.2±0.9+19%</td><td></td><td>6.5±3.6+25%</td><td>0.99±0.02+1% 0.10±0.01+17%</td><td>4.9±0.3+9%</td><td>0.7±1.3+81%</td></tr><tr><td>NOTEARS-MLP</td><td>0.70±0.12</td><td>0.13±0.07</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="6">20</td><td>+ReScore</td><td>0.73±0.09+4%</td><td>0.11±0.05+15%13.7±5.1+8%</td><td>14.9±5.4</td><td>98.4±22.5 88.8±23.8+10%</td><td>0.44±0.09 0.26±0.10 0.41±0.07-7%0.17±0.08+35% 51.6±6.4+6%179.9±33.7-2%</td><td>55.0±9.2</td><td>176.3±33.3</td></tr><tr><td>+ DAG-AEG</td><td></td><td>0.74±0.09+6% 0.12±0.07+8% 13.4±5.2+10%77.6±39.0+32%</td><td></td><td></td><td>0.45±0.05+2% 0.20±0.05+23% 51.9±4.1+6%</td><td></td><td></td></tr><tr><td>GraN-DaG</td><td>0.81±0.15</td><td>0.08±0.08</td><td>9.3±5.4</td><td>53.4±24.4</td><td>0.20±0.07 0.18±0.08</td><td></td><td>164.2±23.8+7%</td></tr><tr><td>+ReScore</td><td>0.81±0.14+0% 0.05±0.04+38% 8.5±5.7+9%</td><td></td><td></td><td>51.0±24.6+5%</td><td>0.21±0.07+5% 0.17±0.09+6% 56.2±4.6+2%125.4±23.3+5%</td><td>57.4±4.6</td><td>131.5±21.4</td></tr><tr><td>+ DAG-AEG</td><td></td><td>0.83±0.09+3% 0.01±0.02+88% 7.4±3.3+20%</td><td></td><td>42.3±11.5+21%</td><td>0.53±0.06+165%.11±0.06+39% 42.9±4.7+25% 136.5±30.0-4%</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="6">50</td><td>NOTEARS-MLP</td><td>0.32±0.04</td><td>0.13±0.08</td><td>69.5±4.7</td><td>884.4±172.8</td><td>0.17±0.02 0.06±0.04</td><td>167.0±4.1</td><td>1608±97</td></tr><tr><td>+ReScore</td><td></td><td></td><td></td><td></td><td>0.51±0.08+59%0.10±0.07+23%53.5±8.7+23%628.1±120.6+29%0.26±0.04+53%0.11±0.05-83%154.4±6.4+8%</td><td></td><td>1438±111+11%</td></tr><tr><td>+ DAG-AEG</td><td>0.52±0.09</td><td></td><td></td><td></td><td>0.62±0.10+94%0.09±0.02+31%42.1±9.1+39%505.8±123.4+4%0.27±0.05+59%0.06±0.03+0%149.4±9.6+11%1400±138+13%</td><td></td><td></td></tr><tr><td>GraN-DaG + ReScore</td><td></td><td>0.15±0.0551.6±9.3</td><td></td><td>632.8±140.3</td><td>0.32±0.040.08±0.16141.6±8.2 0.53±0.06+2% 0.11±0.02+28%46.0±6.0+11%581.0±104.7+8% 0.31±0.03-3% 0.06±0.04+25%138.8±7.5+2%</td><td></td><td>1379±91</td></tr><tr><td>+ DAG-AEG</td><td></td><td></td><td></td><td></td><td>0.61±0.03+17%0.06±0.03+60%40.3±3.1+2%572.5±107.9+10%0.33±0.03+3% 0.05±0.02+38%136.0±9.6+4%1249±114+9%</td><td></td><td>1351±98+2%</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

TPR should be high, while FDR, SHD, and SID should be low, indicating a more accurate estimate of the target causal graph.

# Result Analysis

Performance on Baseline Enhancement. In this section, we present the experimental results of DAG-AEG and compare them with the baselines and the model-independent ReScore on the previously introduced synthetic datasets in terms of TPR, FDR, SHD, and SID metrics. Tables 1 and 2 show the empirical results for both linear and nonlinear synthetic data. The error bars represent the standard deviation across datasets over ten trials. Red percentages indicate performance increases, while blue percentages indicate decreases, of the model-agnostic frameworks compared to the original score-based methods for each metric. The bestperforming methods are highlighted in bold. Our findings include:

DAG-AEG consistently enhances score-based DAG structure learning methods across all datasets, outperforming the model-agnostic state-of-the-art method, ReScore. Unlike ReScore, DAG-AEG shows rarely performance degradation in terms of SHD and SID. Specifically, it improves SHD by approximately $3 \%$ to $6 6 \%$ over advanced baselines, and by 0 to $4 0 \%$ over ReScore, with fewer missing, erroneously detected, and reversed edges. For SID, although there is one case of decline, overall it achieves improvements of $7 \%$ to $8 7 \%$ over advanced baselines, surpassing ReScore. We attribute these improvements to AEG, which suppresses the tails of the loss distribution while identifying significant samples, thus improving the quality of score-based DAG learners. Furthermore, a detailed examination of TPR and FDR reveals that DAG-AEG lowers FDR by eliminating spurious edges and increases TPR by identifying more correct edges, outperforming ReScore. This indicates that DAG-AEG more effectively filters and increases the weight of information-rich samples while suppressing the weight of less informative ones, thus better extracting causal relationships.

Table 3: Results on synthetic heterogeneous data.   

<html><body><table><tr><td>TYPE</td><td>METHODS</td><td>TPR↑</td><td>FDR↓</td><td>SHD↓</td></tr><tr><td rowspan="6">linear</td><td>GOLEM</td><td>0.79</td><td>0.33</td><td>18.7</td></tr><tr><td>+ IPS</td><td>0.65</td><td>0.19</td><td>18.6</td></tr><tr><td>+ ReScore</td><td>0.81</td><td>0.24</td><td>16.4</td></tr><tr><td>+ DAG-AEG</td><td>0.93</td><td>0.23</td><td>15.1</td></tr><tr><td>CD-NOD</td><td>0.51</td><td>0.17</td><td>24.1</td></tr><tr><td>DICD</td><td>0.82</td><td>0.28</td><td>16.7</td></tr><tr><td rowspan="6">nonlinear</td><td>NOTEARS-MLP</td><td>0.62</td><td>0.36</td><td>25.8</td></tr><tr><td>+ IPS</td><td>0.35</td><td>0.21</td><td>28.7</td></tr><tr><td>+ReScore</td><td>0.63</td><td>0.32</td><td>23.8</td></tr><tr><td>+ DAG-AEG</td><td>0.65</td><td>0.30</td><td>22.0</td></tr><tr><td>CD-NOD</td><td>0.60</td><td>0.29</td><td>26.0</td></tr><tr><td>DICD</td><td>0.50</td><td>0.24</td><td>23.5</td></tr></table></body></html>

Differentiable score-based baseline methods show significant performance degradation on dense graphs. As illustrated in the tables, their performance diminishes as the number of nodes and edges in the DAG increases. This decline is particularly evident with 50 vertices and 100 to 200 edges, where the baseline methods perform poorly. The issue worsens with non-linear models, as the TPR drops below $5 0 \%$ . This can be clearly seen in Figure 1. This degradation is mainly due to the increasing difficulty of enforcing the acyclic constraint as graph density increases (Charpentier, Kibler, and Gu¨nnemann 2022; Chen, Wu, and Jin 2024). Figure 1 illustrates that, with a constant degree of 4, the performance of the baseline combined with either ReScore or DAG-AEG declines sharply as the number of nodes increases. Despite this decline, our proposed method consistently provides the best improvement to the baseline. This trend is further supported by the comprehensive metrics presented in Table 1 and Table 2. Overall, although the DAGAEG framework can improve the performance of baseline methods, its effectiveness still depends on the efficiency of the DAG learner.

![](images/55e34f23a0ed966f8d61653d7ee161881d56cbba2e27dfdf3ed95702e951dbd3.jpg)  
Figure 1: TPR and SHD of NOTEARS, the combination of ReScore, and the combination of DAG-AEG on graphs generated using the ER model with a degree of 4 and vertex counts of 10, 20, and 50, respectively.

Performance on Heterogeneous Data. In this part, we compare Baseline $+$ DAG-AEG with two group annotationdependent causal learning methods to validate DAG-AEG’s capability in handling heterogeneous data. Additionally, we consider a Baseline+IPS reweighting method, where sample weights are inversely proportional to group sizes. We also compare the performance of DAG-AEG with ReScore. The experiments are conducted on both synthetic and real heterogeneous data, with the specific analysis detailed below:

Performance on synthetic heterogeneous data: In Table 3, our DAG-AEG framework significantly improves the TPR, FDR, and SHD metrics of baseline models on synthetic heterogeneous data, irrespective of using linear or non-linear models. Notably, the TPR and SHD metrics achieve optimal values for their respective types (bolded in the table). It is important to note that the baseline models used here are not specifically designed for heterogeneous data. Additionally, DAG-AEG enhanced baselines outperform recognized CD-NOD and DICD lower bounds in the SHD metric. Compared to the fixed-weight IPS method, the IPS+baseline method shows a severe drop in TPR performance, highlighting the importance of adaptive weights. Among adaptive weighting methods, although ReScore enhances baseline performance, it still falls short compared to our DAG-AEG framework, underscoring the competitiveness of the proposed AEG.

Table 4: Results on Sachs dataset.   

<html><body><table><tr><td>METHODS</td><td>TPR↑</td><td>FDR↓</td><td>SHD↓</td><td>SID↓</td><td>#PE</td></tr><tr><td>GOLEM</td><td>0.176</td><td>0.026</td><td>15</td><td>53</td><td>22</td></tr><tr><td>+ReScore</td><td>0.294</td><td>0.063</td><td>14</td><td>49</td><td>6</td></tr><tr><td>+ DAG-AEG</td><td>0.294</td><td>0.063</td><td>13</td><td>47</td><td>6</td></tr><tr><td>NPTEARS-MLP</td><td>0.412</td><td>0.632</td><td>16</td><td>45</td><td>19</td></tr><tr><td>+ ReScore</td><td>0.412</td><td>0.500</td><td>13</td><td>43</td><td>14</td></tr><tr><td>+ DAG-AEG</td><td>0.412</td><td>0.462</td><td>13</td><td>42</td><td>14</td></tr><tr><td>GraN-DAG</td><td>0.294</td><td>0.643</td><td>16</td><td>60</td><td>14</td></tr><tr><td>+ReScore</td><td>0.353</td><td>0.600</td><td>15</td><td>58</td><td>15</td></tr><tr><td>+ DAG-AEG</td><td>0.471</td><td>0.556</td><td>14</td><td>38</td><td>18</td></tr><tr><td>GES</td><td>0.294</td><td>0.853</td><td>31</td><td>54</td><td>34</td></tr><tr><td>+ ReScore</td><td>0.588</td><td>0.722</td><td>28</td><td>50</td><td>36</td></tr><tr><td>+ DAG-AEG</td><td>0.647</td><td>0.667</td><td>24</td><td>26</td><td>33</td></tr><tr><td>CD-NOD</td><td>0.588</td><td>0.444</td><td>15</td><td></td><td>18</td></tr></table></body></html>

Performance on real heterogeneous data: As shown in Table 4, DAG-AEG significantly improves the TPR, FDR, SHD, and SID metrics across all baseline methods on real heterogeneous datasets. The enhanced TPR and FDR indicate that DAG-AEG predicts more correct edges and fewer incorrect edges. Compared to CD-NOD, which uses annotations as prior knowledge for heterogeneous data, our DAG-AEG $^ +$ GraN-DAG achieves a competitive TPR without requiring ground-truth annotations. Moreover, DAG-AEG $^ +$ GraN-DAG outperforms CD-NOD in SHD when predicting the same number of edges(#PE). Compared to ReScore, DAG-AEG shows superior performance. Experiments on both real and synthetic heterogeneous data demonstrate the practical competitiveness of our proposed DAG-AEG framework.

# Conclusions

Learning causal structures from observational data poses significant challenges. Despite the good results achieved by current differentiable scoring-based methods, the causal structures they identify often deviate from actual conditions. In this paper, we introduce DAG-AEG, an innovative modelagnostic framework that boosts DAG structure learning. Utilizing the distribution of sample losses, DAG-AEG dynamically prioritizes sample importance through the AEG technique. This approach not only dynamically adjusts sample weights to underscore their relevance but also significantly enhances model performance across heterogeneous datasets. Comprehensive experiments demonstrate that DAG-AEG substantially improves the performance of existing scoringbased linear and nonlinear DAG learners across a range of synthetic and real-world datasets, thereby highlighting its practical efficacy and competitive edge. In future research, we aim to enhance the performance of complex dense graphs by refining acyclic constraints and effectively integrating the findings from this study.

#

Acknowledgments   
This work was supported by the Basic Research Project(No.JCKY2022203B001).