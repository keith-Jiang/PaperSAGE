# Linear Streaming Bandit: Regret Minimization and Fixed-Budget Epsilon-Best Arm Identification

Yuming Shao1,2, Zhixuan Fang1,2\*

1IIIS, Tsinghua University, Beijing, China 2Shanghai Qi Zhi Institute, Shanghai, China shaoym21@mails.tsinghua.edu.cn, zfang@mail.tsinghua.edu.cn

# Abstract

Recently, there has been a focus on the streaming setting in a line of works on the Multi-Armed Bandit (MAB). In this scenario, a large number of arms arrive in a streaming manner, and the algorithm scans through the stream and stores some arms in its limited processing memory. We advance this line of research by introducing the Linear Streaming Bandit setup, where the arriving arms have profile vectors observable to the algorithm. The profile of an arm has a linear correlation with the expected reward. This setup is motivated by real-world applications, such as when a company or a crowdsourcing platform hires a worker from many sequentially arriving applicants with their resumes. We address two problems in this setup: Regret Minimization and Fixed-Budget $\epsilon$ -Best Arm Identification. For the former, we propose an algorithm whose regret is independent of the number of arms, thus it is able to handle arbitrarily long arm streams. For the latter, we present a multi-pass algorithm whose error probability is sub-linear w.r.t. the number of arms, and an algorithm identifying the exact best arm in only a single pass. We validate the effectiveness of all proposed algorithms through experiments on both synthetic and real-world datasets.

# Introduction

The Multi-Armed Bandit (MAB) (Lattimore and Szepesva´ri 2020) is a simple but powerful model in online decisionmaking scenarios. Researchers have found a wide range of industrial applications for MAB, from online advertising (Schwartz, Bradlow, and Fader 2017; Avadhanula et al. 2021; Yang and Lu 2016) to clinical trials (Villar, Bowden, and Wason 2015; Aziz, Kaufmann, and Riviere 2021). In a MAB instance, there is a set of arms $\kappa$ . The decision maker (also called the algorithm) continuously selects arms from the set, yielding a sequence of reward values that help the decision maker understand each arm’s profitability. There are two problems that attract research interest in MAB: Regret Minimization and Best Arm Identification. In the former, the decision maker aims to generate as much cumulative reward as possible within a time constraint, while in the latter, the objective is to find the arm with the highest expected reward.

Recently, a rapidly developing line of work on MAB studies the streaming arm model (Maiti, Patil, and Khan 2020;

Jin et al. 2021; Assadi and Wang 2020, 2022; Agarwal, Khanna, and Patil 2022; Li et al. 2023; Assadi and Wang 2023; Wang 2023), motivated by the explosive growth of data and the convenience of processing arms in a streaming manner. In this streaming scenario, a large number of arms arrive one by one, while the algorithm is limited in its storage capacity. The algorithm can only pull the arms temporarily stored in its memory. To accommodate future arms in the stream, it must discard some existing arms from memory. In the streaming bandit setting, both Regret Minimization and Best Arm Identification problems have been considered in the literature. However, existing streaming bandit algorithms unexceptionally face a serious problem: their theoretical upper bounds for either regret or sample complexity become loose very quickly as the number of arms $K$ increases. For example, Agarwal, Khanna, and Patil (2022) introduce an algorithm with ${ \tilde { O } } ( T ^ { \alpha } { \sqrt { K } } )$ regret, where $T$ is the time horizon and $\textstyle \alpha > { \frac { 1 } { 2 } }$ . If the magnitude of $K$ is comparable to $T$ , i.e., $K = \Omega ( \tilde { T } )$ , the algorithm loses theoretical guarantees since the upper bound scales at least linearly with $T$ . Besides, Jin et al. (2021) propose Fixed-Confidence Best Arm Identification (to satisfy a given error probability requirement using as few samples as possible) algorithms whose sample complexity bounds grow linearly with $K$ . In summary, existing streaming bandit algorithms are not good at processing a large amount of arms.

One crucial reason for this scalability issue is that every new arm in the stream must be carefully, or even equally, evaluated, as the previous arms provide no information about it. This phenomenon could lead to a waste of time and resources on sub-optimal arms, as each encounter must be handled anew, resulting in the unsatisfactory performance of existing algorithms. In fact, it is both reasonable and practical to assume that some useful information is revealed as soon as a new arm is encountered, which can be utilized to efficiently judge whether the arm is sub-optimal. For instance, consider a real-world recruitment scenario (Jin et al. 2021), where a company plans to hire a highly qualified employee from among many sequentially arriving applicants by interviewing them. Typically, applicants are required to submit their curricula vitae, which can provide insight into their true talents. It is also common practice for a company to screen out unsuitable applicants based on their CVs.

Motivated by the above observation, we propose the Linear Streaming Bandit model. We illustrate an example in Figure 1 to contrast our model with existing models. We assume that every arm in the stream has a $d$ -dimensional profile vector that can be observed by the algorithm before it takes actions. The reward of pulling an arm is jointly determined by the arm’s profile vector and an unknown $d$ - dimensional parameter vector $\theta ^ { * }$ . Specifically, we assume a simple and common linear reward model. For example, consider the parameter vector as the return of each asset in a portfolio, and an arm as an investment manager with a certain investment policy (i.e., the arm profile vector) on each asset. The algorithm’s profit is the inner product between the investment policy and the asset returns. In this paper, we consider two mainstream objectives in the proposed setting, namely Regret Minimization and Fixed-Budget Best Arm Identification.

![](images/97baf1acbf7ef2ff77deaaa2d3306802e18a1b1385981723f13d2398bed7694f.jpg)  
Figure 1: Three bandit models for recruitment. From top to bottom, these are MAB, Streaming Bandit, and Linear Streaming Bandit (Our Model), respectively. An HR manager (the algorithm) is interacting with many applicants (the arms). The white area represents the pool of applicants that the HR manager needs to maintain in each model.

Our main contributions are summarized as below:

• We propose the Linear Streaming Bandit setup, which is the first to establish a connection between streaming MAB and the well-investigated literature of linear bandit. • We design a multi-pass streaming algorithm, named CRMPS, for the regret minimization task. We show that it has a sub-linear regret upper bound, which is independent of the total number of arms. • We design a multi-pass streaming algorithm, named GMP-SE, which is able to identify an $\epsilon$ -best arm with high probability given constraints on sample budget and the times of scans. We show that its error probability has only a sub-linear dependency on the total number of arms. We also introduce a streaming algorithm, named SPC, for

strict best arm identification, requiring only a single pass. • We verify the effectiveness of each proposed algorithm by testing them on both synthetic and real-world datasets.

# Related Work

Streaming Multi-Armed Bandit. A number of works on streaming bandits have emerged in recent years. For Regret Minimization, Wang (2023) argues that a simple uniform exploration algorithm achieves ${ \bar { O } } ( K ^ { \frac { 1 } { 3 } } T ^ { \frac { 2 } { 3 } } )$ expected regret and further shows it is optimal by establishing a $\Omega \big ( K ^ { \frac { 1 } { 3 } } T ^ { \frac { 2 } { 3 } } \big )$ regret lower bound for single-pass algorithms. As for the multi-pass scenario, Agarwal, Khanna, and Patil (2022) design an algorithm with ${ \tilde { O } } ( T ^ { { \frac { 1 } { 2 } } + { \frac { 1 } { 2 ^ { B + 2 } - 2 } } } { \sqrt { K B } } )$ high probability regret upper bound, where $B$ is the maximum number of passes, and show that the bound is tight with respect to $T$ . For Best Arm Identification (BAI), Maiti, Patil, and Khan (2020) propose a single-pass $( \varepsilon , \delta )$ -BAI algorithm with sample complexity $\begin{array} { r } { \tilde { O } \big ( \frac { K } { \varepsilon ^ { 2 } } \ln \frac { 1 } { \delta } \big ) } \end{array}$ , where $( \varepsilon , \delta )$ - BAI means that, with probability at least $\dot { \mathrm { ~ 1 ~ - ~ } } \delta$ , the algorithm identifies an $\varepsilon$ -best arm. Jin et al. (2021) introduce a single-pass $( \varepsilon , \delta , k )$ -KAI algorithm with sample complexity $\begin{array} { r } { \tilde { O } \big ( \frac { K } { \varepsilon ^ { 2 } } \ln \frac { k } { \delta } \big ) } \end{array}$ , where $( \varepsilon , \delta , k )$ -KAI means that, with probability at least $1 - \delta$ , the algorithm identifies $k$ arms whose reward means are lower than that of the $k$ -th best arm by at most $\varepsilon$ . Jin et al. (2021) also present a strict BAI algorithm with near optimal instance-dependent sample complexity, using $\begin{array} { r } { O ( \ln { \frac { \mathtt { i } } { \Delta } } ) } \end{array}$ passes in expectation, where $\Delta$ is the gap between the mean of the best arm and that of the second-best arm. Complementarily, Assadi and Wang (2023) show that any streaming algorithm with optimal sample complexity requires Ω ln(1/∆)   ln ln(1/∆)  passes. To the best of our knowledge, the fixed-budget BAI problem, where the goal is to minimize the error probability given a budget constraint, has not yet been explored in the streaming MAB setup.

Linear Bandit. For Regret Minimization, a well-known paper (Abbasi-yadkori, Pa´l, and Szepesva´ri 2011) proposes a near-optimal algorithm, OFUL. A recent paper (Yang et al. 2022) also considers the scenario of an extremely large arm set, as we do, but with a different purpose. The authors attempt to reduce the time complexity of scanning through the whole arm set but still assume arbitrary access to every arm in the set. The BAI problem has also been extensively studied in the linear bandit literature, including the fixed-confidence setting (Soare, Lazaric, and Munos 2014; Soare 2015; Fiez et al. 2019; Zaki, Mohan, and Gopalan 2020; Jedra and Proutiere 2020; Camilleri et al. 2021; Jourdan and Degenne 2022) and the fixed-budget setting (Alieva, Cutkosky, and Das 2021; Yang and Tan 2022). Most of the BAI algorithms proposed in the literature belong to the successive elimination framework. The procedure is divided into several rounds. In each round, the algorithms update their estimation for the underlying parameter vector and discard a fraction of arms with relatively low empirical means. To the best of our knowledge, no algorithms in the linear bandit literature are directly applicable to our Linear Streaming Bandit setting, as their operations heavily rely on simultaneous access to all arms in the arm set.

# Problem Setup

Notation. Throughout the paper, we let $\langle \cdot , \cdot \rangle$ denote the inner product between two vectors, let $| | \cdot | | _ { p }$ denote the $\ell ^ { p }$ - norm of a vector, let $| | x | | _ { V }$ , where $V$ is a positive semidefinite matrix, denote $\sqrt { x ^ { T } V x }$ , let $| S |$ denote the cardinality of a set $s$ , let $[ N ]$ denote the set of the smallest $N$ positive integers: $\{ 1 , 2 , . . . , \bar { N } \}$ .

# Linear Streaming Bandit

We introduce our linear streaming bandit problem setup. There is a set of arms $\mathcal { K } \subseteq \mathbb { R } ^ { d }$ , where an arm $a \in \mathcal K$ is a vector of dimension $d$ . The vector, also called a profile, can be interpreted as a list of characteristics of the arm that are potentially related to its reward distribution. Let $K = | { \cal K } |$ denote the total number of arms. At each time slot $t$ , the algorithm pulls an arm $a _ { t }$ which yields a reward $\boldsymbol { r } _ { t } \in \mathbb { R }$ . As in the conventional linear bandit setting, we assume that $\exists \theta ^ { * } \in \mathbb { R } ^ { d }$ , such that the reward satisfies $\bar { r } _ { t } ~ = ~ \langle a _ { t } , \theta ^ { * } \rangle + \eta _ { t }$ , where $\{ \eta _ { t } \} _ { t \ge 1 }$ is a sequence of $\sigma$ -subGaussian random noise, i.e.,

$$
\mathbb { E } \Big [ \exp ( \alpha \eta _ { t } ) \Bigm | \mathcal { F } _ { t - 1 } \Big ] \leq \exp \Big ( \frac { \alpha ^ { 2 } \sigma ^ { 2 } } { 2 } \Big ) , \quad \forall \alpha \in \mathbb { R } , t \geq 1 ,
$$

with the filtration $\mathcal { F } _ { t - 1 } = \sigma ( a _ { 1 } , r _ { 1 } , . . . , a _ { t } )$ . We call $\textstyle \mu _ { a } : =$ $\left. a , \theta ^ { * } \right.$ the expected/mean reward of arm $a$ , where the true parameter vector $\theta ^ { * }$ is unknown to the algorithm. Define the optimal arm $a ^ { * } = \arg \operatorname* { m a x } _ { a \in { \mathcal { K } } } \mu _ { a }$ . Define the reward gap of arm $a$ to be ${ \Delta _ { a } } = \left. { { a ^ { * } } , { \theta ^ { * } } } \right. - \left. { a , { \theta ^ { * } } } \right.$ . Let $\Delta _ { [ i ] }$ denote the $i$ -th smallest reward gap for any $i \in [ K ]$ . It is obvious that $\Delta _ { [ 1 ] } = 0$ . Besides, let $\Delta$ denote $\Delta _ { [ 2 ] }$ for notational simplicity. We assume that the arm set is uniformly upper bounded: $| | a | | _ { 2 } \leq L , \forall a \in \mathcal { K }$ , where $L$ is a positive constant. We also assume that the $\ell ^ { 2 }$ -norm of $\theta ^ { * }$ is upper bounded by a positive constant $S$ : $| | \theta ^ { * } | | _ { 2 } \leq S$ .

The primary distinction between our model and those in the linear bandit literature is that, instead of assuming the algorithm has simultaneous access to every arm $a \in \mathcal { K }$ , we assume the arms are presented to the algorithm sequentially in a streaming manner. No restrictions are placed on the order of the arms; the order can be arbitrary or even adversarially chosen. The length of the stream $K$ can be extremely large, while the algorithm typically has fixed and limited storage resources. Consequently, the algorithm cannot simultaneously record the information about all arms in its storage. We assume the algorithm can remember at most $M$ arms in its memory at any given time, where $M$ remains constant as $K$ grows and typically $M \ll K$ . It can also store intermediate data, provided the storage size required for this data remains constant with respect to $K$ . When the algorithm encounters an arm in the stream, it observes the arm’s profile and decides whether to store it in memory. Only arms currently stored in memory can be pulled. The algorithm can remove arms from memory to accommodate new arms from the remaining stream. However, discarded arms are forgotten and cannot be retrieved into memory unless the algorithm revisits the arm stream. Let $B \in \mathbb { N } ^ { + }$ denote the maximum number of passes the algorithm is allowed over the arm stream. The order of arms may differ completely between passes. The multi-pass scenario has been widely considered in the literature (Agarwal, Khanna, and Patil 2022; Assadi and Wang 2023). In certain cases, the algorithm can indeed scan the arm sequence multiple times. For instance, workers on crowdsourcing platforms may return to the platform repeatedly after completing their current tasks in search of additional work. In the remainder of this section, we introduce the two objectives considered in the aforementioned model.

Regret Minimization. In this problem, the algorithm aims to minimize cumulative regret. $T$ is interpreted as the time horizon in this scenario. At each time slot $t \in [ T ]$ , the algorithm pulls an arm $a _ { t }$ and receives a reward $\boldsymbol { r } _ { t }$ . Formally, the pseudo-regret is defined as

$$
R _ { T } = \sum _ { t = 1 } ^ { T } \langle \theta ^ { * } , a ^ { * } \rangle - \sum _ { t = 1 } ^ { T } \langle \theta ^ { * } , a _ { t } \rangle = \sum _ { t = 1 } ^ { T } \langle \theta ^ { * } , a ^ { * } - a _ { t } \rangle .
$$

$\epsilon$ -Best Arm Identification. In Best Arm Identification, the algorithm’s objective is to identify an arm in the arm set $\kappa$ that has the highest expected reward among all others. As in the Multi-Armed Bandit (MAB) literature, there are two complementary variants of best arm identification (Lattimore and Szepesva´ri 2020): (1) Fixed-Confidence setting. The algorithm is given a constant confidence parameter $\delta \in ( 0 , 1 )$ and outputs an arm prediction ${ \hat { a } } \in { \mathcal { K } }$ such that $\mathbb { P } ( \hat { a } = a ^ { * } ) \geq 1 - \delta$ , i.e., the prediction is correct with probability at least $1 - \delta$ , while using as few samples as possible. (2) Fixed-Budget setting. The algorithm is given a sample budget constraint $T \in \mathbb { N } ^ { + }$ . Its objective is to minimize the error probability $\mathbb { P } ( \hat { a } \neq a ^ { * } )$ with at most $T$ arm pulls.

In this paper, we consider a Fixed-Budget $\epsilon$ -Best Arm Identification problem in the proposed Linear Streaming Bandit setup. Here, $T$ represents the sample budget, while $B$ denotes the pass budget. That is, the algorithm is allowed to pull at most $T$ arms and scan at most $B$ passes over the arm stream. Once the algorithm has pulled $T$ arms, it must output the prediction $\hat { a }$ and terminate. When the algorithm has already scanned $B$ passes, it cannot initiate a new scan but may still pull arms stored in memory until the sample budget $T$ is exhausted. Given a fixed approximation parameter $\epsilon > 0$ , the algorithm’s objective is to minimize $\mathbb { P } ( \bar { \Delta } _ { \hat { a } } > \epsilon )$ .

# Regret Minimization

In this section, we introduce our algorithm for regret minimization in the linear streaming bandit setup. We begin by describing the main idea behind it. Suppose the algorithm aims to achieve a target precision $\epsilon > 0$ in estimating the underlying parameter vector $\theta ^ { * }$ by going through the arm stream. The most straightforward solution might be to scan the stream in a single pass and pull a sufficient number of arms until the precision target is met. However, in the streaming scenario, the algorithm must be overly conservative to avoid missing the optimal arm. This conservativeness can result in wasting the budget on poor arms, especially when the optimal arm appears very late in the stream. One of the key aspects of our algorithm design is utilizing the opportunity to access the arm stream multiple times to mitigate this issue. A feasible approach is to select another precision parameter $\epsilon ^ { \prime } > \epsilon$ . In the first pass, the algorithm achieves the less stringent precision target $\epsilon ^ { \prime }$ using significantly fewer samples. In the second pass, the algorithm aims to achieve the stricter precision target $\epsilon$ , leveraging the additional knowledge obtained during the first pass. Specifically, when an arm $a$ is encountered, the algorithm reads the arm’s profile without pulling it and immediately determines whether $\Delta _ { a } \geq \Theta ( \epsilon ^ { \prime } ) $ . By avoiding the need to pull highly sub-optimal arms, the cumulative regret can be effectively reduced.

We present the pseudo-code of our algorithm CR-MPS in Algorithm 1. As the pass index $b$ increases, the algorithm selects smaller precision parameters $\epsilon _ { b }$ . The algorithm stores at most two arms, $\hat { a }$ and $\tilde { a }$ , simultaneously. $\hat { a }$ records the empirically optimal arm encountered in the current pass, while $\tilde { a }$ represents the current arm in this pass. In Line 6, the algorithm checks whether the current arm is highly sub-optimal. If not, it keeps pulling this arm until the precision in this arm’s direction is lower than $\epsilon _ { b }$ . Then the algorithm updates $\hat { a }$ if the arm $\tilde { a }$ is believed to be temporarily optimal in Line 13. When the algorithm has already scanned the arm stream $B$ times, it enters the exploitation phase, where it repeats the empirically optimal arm $\hat { a }$ until time $T$ . Theorem 1 gives a high probability pseudo-regret upper bound of CR-MPS.

Theorem 1. Run CR-MPS algorithm in Algorithm 1 with confidence parameter $\delta \in ( 0 , 1 )$ , regularization parameter $\lambda > 0$ , and precision parameters $\epsilon _ { 0 } , \epsilon _ { 1 }$ set as

$$
\begin{array} { l } { { \displaystyle \epsilon _ { 0 } = \frac { L S } { 3 \sqrt { \beta _ { T } } } , } } \\ { { \epsilon _ { 1 } = \left( 6 T ^ { - 1 } B d \ln \left( 1 + \frac { T L ^ { 2 } } { \lambda d } \right) \right) ^ { \frac { 2 ^ { B - 1 } } { 2 ^ { B + 1 } - 1 } } \epsilon _ { 0 } ^ { \frac { 2 ^ { B } - 1 } { 2 ^ { B + 1 } - 1 } } , } } \end{array}
$$

where $\begin{array} { r } { \sqrt { \beta _ { t } } : = \sqrt { \lambda } S + \sigma \sqrt { 2 \ln \frac { 1 } { \delta } + d \ln ( 1 + \frac { t L ^ { 2 } } { \lambda d } ) } } \end{array}$ for any $t \in [ T ]$ . We have that $\exists N _ { 0 } \in \mathbb { N } ^ { + }$ such that for any $T \geq N _ { 0 }$ , the pseudo-regret $R _ { T }$ is upper bounded by

$4 ( L S / 3 ) ^ { \frac { 1 } { 2 ^ { B + 1 } - 1 } } \left[ 6 \beta _ { T } B d \ln \left( 1 + \frac { T L ^ { 2 } } { \lambda d } \right) \right] ^ { \frac { 2 ^ { B } - 1 } { 2 ^ { B + 1 } - 1 } } T ^ { \frac { 2 ^ { B } } { 2 ^ { B + 1 } - 1 } }$ with probability at least $1 - \delta$ . Thus, with probability at least $1 - \delta$ , $R _ { T } = \tilde { O } \big ( ( d ^ { 2 } B ) ^ { \frac { 2 ^ { B } - 1 } { 2 ^ { B + 1 } - 1 } } T ^ { \frac { 2 ^ { B } } { 2 ^ { B + 1 } - 1 } } \big ) .$

Remark. It is important to note that the regret upper bound is independent of the total number of arms $K$ . This implies that the algorithm is effective in handling arbitrarily long arm sequences, achieving sub-linear regret with respect to $T$ , since $\begin{array} { r } { \alpha : = \frac { 2 ^ { B } } { 2 ^ { B + 1 } - 1 } \le \frac { 2 } { 3 } } \end{array}$ and $\alpha \downarrow \frac { 1 } { 2 }$ as $B  \infty$ . Consequently, the order of our regret upper bound for CR-MPS approaches $\Omega ( { \sqrt { T } } )$ , a well-known regret lower bound for the classic Linear Bandit setting (Lattimore and Szepesva´ri 2020), when $B$ is sufficiently large. Moreover, the CR-MPS algorithm only requires an arm memory of size $M = 2 \ll K$ .

Proof of Theorem $\jmath$ . For any $\delta > 0$ , we define a good event

$$
\mathcal { G } = \Bigg \{ \forall t \geq 0 , x \in \mathbb { R } ^ { d } : | \langle x , \theta ^ { * } - \hat { \theta } _ { t } \rangle | \leq | | x | | _ { V _ { t } ^ { - 1 } } \sqrt { \beta _ { t } } \Bigg \} ,
$$

<html><body><table><tr><td></td><td>Input: δ confidence parameter, 入 regularization</td><td>parameter,∈o, ∈1 precision parameters */</td><td></td></tr><tr><td></td><td>/* Initialization 2 for pass b = 1,2,.,B do</td><td>1 t←1,V←λI,S←0,μlast←0,ρ←0</td><td>*/</td></tr><tr><td>3</td><td></td><td>2b 26 a←null /* We set ∈b=∈²</td><td>26- 26</td></tr><tr><td></td><td></td><td>/* Scan the b-th pass</td><td>*/</td></tr><tr><td>4 5 6</td><td></td><td>for k=1,2,...,K do Read and store arm ak,b to ä</td><td></td></tr><tr><td></td><td></td><td>if b=1orb>1and (a,V-1S)≥ βlast-2√βr∈1</td><td>2b-1-1 2b-2-1 2b-2</td></tr><tr><td></td><td></td><td>then</td><td>26-</td></tr><tr><td>7 8</td><td></td><td>while ||äallv-1 ≥ ε²</td><td>2b 26 2b-1 do Eo</td></tr><tr><td>9</td><td></td><td>V←V+aaT</td><td>Pull arm at ← ä and obtain reward rt</td></tr><tr><td>10</td><td></td><td>S←S+rtá</td><td></td></tr><tr><td>11 12</td><td></td><td>t↑t+1</td><td></td></tr><tr><td>13</td><td></td><td>end</td><td></td></tr><tr><td></td><td></td><td>if ä= null or (a,V-1S)> ρ then</td><td></td></tr><tr><td>14</td><td></td><td></td><td></td></tr><tr><td>15</td><td></td><td>μ←{a,V-1s)</td><td></td></tr><tr><td></td><td></td><td>a←a</td><td></td></tr><tr><td>16</td><td>end</td><td></td><td></td></tr><tr><td>17</td><td>end</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>18</td><td>end</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>19</td><td>μlast←μ</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>20 end</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td>/* Exploitation</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>21</td><td>whilet≤Tdo</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>22</td><td>Pull arm at ←ä</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td>t←t+1</td><td></td><td></td></tr><tr><td>23</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td>24 end</td><td></td><td></td></tr><tr><td></td></table></body></html>

which means that the estimators $\hat { \theta } _ { t }$ (computed as $V ^ { - 1 } S$ in the algorithm) for $\theta ^ { * }$ are accurate enough. It can be shown that this good event happens with high probability:

Lemma 1. $\mathbb { P } ( \mathcal { G } ) \geq 1 - \delta$

We analyze the algorithm behavior under the event $\mathcal { G }$ . Step I. Utilizing the accuracy of $\hat { a }$

The algorithm stores a total of two arms, $\tilde { a }$ and $\hat { a } . \tilde { a }$ is the current arm that the algorithm is exploring, while $\hat { a }$ stores the empirically best arm observed so far. In the $b$ -th pass, before each comparison between $\tilde { a }$ and $\hat { a }$ , the algorithm repeatedly samples $\tilde { a }$ to ensure that both arms have confidence radii upper bounded by $\epsilon _ { b }$ . Let $\hat { a } _ { b }$ represent the content of $\hat { a }$ at the end of the $b$ -th pass. We first show that $\hat { a } _ { b }$ is, to some extent, a good predictor for $a ^ { * }$ . Define $T _ { k , b }$ to be the number of times arm $a _ { k , b }$ is pulled in the $b$ -th pass, and $\begin{array} { r } { T _ { b } = \sum _ { k = 1 } ^ { K } T _ { k , b } . } \end{array}$ .

Algorithm 2: $\epsilon$ -Grid Multi-Pass Successive Elimination (G-MP-SE)   

<html><body><table><tr><td>Input: ε approximation parameter,T sample budget parameter, B number of passes</td></tr><tr><td>1 Initialize the grid set Ge in the memory</td></tr><tr><td>2 Initialize t ←1and Cb(g)←O for each</td></tr><tr><td>g ∈9c,b∈[B-1] 3 for arm a in the first pass do</td></tr><tr><td>4 Find g(α) ← arg minh∈g. lla - h|l2</td></tr><tr><td>5 Ci(g(a))←Ci(g(a))+1</td></tr><tr><td>6 end for pass b=2,..,B-1 do</td></tr><tr><td>8 Allocate the sample budget:</td></tr><tr><td>Db ← SBA(9e,Cb-1,C1,T)</td></tr><tr><td>9 Initialize Vb ←O,Sb ←O and c as a counter with</td></tr><tr><td>initial value 1 for each g ∈Gε</td></tr><tr><td>10 for arm a in passb do</td></tr><tr><td>11 Find g(a) ← arg minh∈g. lla - h|l2</td></tr><tr><td>12 for s ∈[Db(g(a),Cg(a)] do</td></tr><tr><td>13 Pull arm at ← α and observe reward rt</td></tr><tr><td>14 V6←V+ataT</td></tr><tr><td>15 Sb←Sb+rtat</td></tr><tr><td>16 t←t+1 17 end</td></tr><tr><td>18 Cg(a)←Cg(a)+1</td></tr><tr><td>19 end</td></tr><tr><td></td></tr><tr><td>0b←V-1Sb,Nb←[KB-2 20</td></tr><tr><td>21 Cb ←C-Update(Sε,Cb-1,0b,Nb)</td></tr></table></body></html>

# 22 end

23 Scan the final pass, set $\hat { a }$ as the first $a$ that is rounded to a grid point $g$ with $C _ { B - 1 } ( g ) > 0$

Lemma 2. Under event $\mathcal { G }$ , for any $1 \le b \le B , \hat { a } _ { b }$ satisfies

$$
\mu _ { \hat { a } _ { b } } \geq \mu ^ { * } - 2 \sqrt { \beta _ { T } } \epsilon _ { b } ,
$$

where $\mu ^ { * } : = \mu _ { a ^ { * } }$ .

Thanks to the prediction accuracy of $\hat { a } _ { b - 1 }$ , the algorithm can effectively skip arms that are clearly sub-optimal compared to $\hat { a } _ { b - 1 }$ in the $b$ -th pass. For $b > 1$ , if the algorithm visits arm $\tilde { \boldsymbol { a } } = \boldsymbol { a } _ { k , b }$ in the $b$ -th pass, we now have

$$
\begin{array} { r l } & { \Delta _ { \tilde { a } } = \langle a ^ { * } - \tilde { a } , \theta ^ { * } \rangle \leq \langle \hat { a } _ { b - 1 } , \theta ^ { * } \rangle + 2 \sqrt { \beta _ { T } } \epsilon _ { b - 1 } - \langle \tilde { a } , \theta ^ { * } \rangle } \\ & { \qquad \leq \langle \hat { a } _ { b - 1 } , \theta ^ { * } \rangle + 2 \sqrt { \beta _ { T } } \epsilon _ { b - 1 } } \\ & { \qquad - \langle \tilde { a } , \hat { \theta } _ { t _ { k - 1 } ^ { ( b ) } } \rangle + \sqrt { \beta _ { t _ { k - 1 } ^ { ( b ) } } } | | \tilde { a } | | _ { V _ { t _ { k - 1 } ^ { ( b ) } } ^ { - 1 } } } \\ & { \qquad \leq \langle \hat { a } _ { b - 1 } , \hat { \theta } _ { t _ { i ^ { \prime } } ^ { ( b - 1 ) } } \rangle - \langle \tilde { a } , \hat { \theta } _ { t _ { k - 1 } ^ { ( b ) } } \rangle } \\ & { \qquad + 3 \sqrt { \beta _ { T } } \epsilon _ { b - 1 } + \sqrt { \beta _ { t _ { i ^ { \prime } } ^ { ( b - 1 ) } } } | | \hat { a } _ { b - 1 } | | _ { V _ { t _ { i ^ { \prime } } ^ { ( b - 1 ) } } ^ { - 1 } } } \\ & { \qquad \leq 6 \sqrt { \beta _ { T } } \epsilon _ { b - 1 } , } \end{array}
$$

where $i ^ { \prime }$ is the index of $\hat { a } _ { b - 1 }$ in the $( b - 1 )$ -th pass and $t _ { k } ^ { ( b ) } : =$ $\textstyle \sum _ { s = 1 } ^ { k } T _ { s , b } + \sum _ { b ^ { \prime } = 1 } ^ { b - 1 } T _ { b ^ { \prime } }$ . The first inequality is by (1) and the last is due to the fact that arm $\tilde { a }$ satisfies the condition in Line 6. For the first pass, we only know a uniform upper bound $\begin{array} { r } { \Delta _ { { \tilde { a } } } \leq 2 | | \theta ^ { * } | | _ { 2 } \operatorname* { i n a x } _ { a \in { \mathcal { A } } } | | a | | _ { 2 } \leq 2 L S = : \bar { \Delta } } \end{array}$ . Now we start decomposing the pseudo-regret,

Algorithm 3: Sample Budget Assignment (SBA)   

<html><body><table><tr><td>budget parameter</td><td>Input: Ge grid set, C counter of active arms on each grid point, C[ initial counter, T sample 1 Initialize the budget assignment D(g,c) to be O for</td></tr><tr><td>2 Compute the design 入</td><td>3 Allocate the sample budget to each grid point:</td></tr><tr><td>4 forg∈9edo 5</td><td>N(g)←[λgT] for each g ∈9ε if N(g)>O then</td></tr><tr><td>6 7</td><td>fori∈[N(g)] do Sample c ~ Unif([Ci(g)])</td></tr><tr><td>8</td><td>D(g,c) ← D(g,c)+1</td></tr><tr><td>9 10</td><td>end</td></tr><tr><td>11 end</td><td>end</td></tr><tr><td></td><td>Output: D budget assignment</td></tr><tr><td></td><td>Algorithm 4: Active Arm Counter Update (C-</td></tr><tr><td>Update)</td><td></td></tr><tr><td></td><td>Input: Ge grid set, Co counter of active arms on each grid point, θ estimator for the true parameter,</td></tr><tr><td></td><td>N number of remaining arms 1 Initialize C(g) ← O for each g ∈ 9ε</td></tr><tr><td></td><td>2 Sort g' as an ordered list of g ∈ Ge with decreasing</td></tr><tr><td></td><td>values of (g,0)</td></tr><tr><td>3 for g∈g'do</td><td></td></tr><tr><td></td><td>C(g)← min{Co(g),N}</td></tr><tr><td>4</td><td></td></tr><tr><td>5</td><td>N ← max{N -Co(g),0}</td></tr><tr><td></td><td></td></tr><tr><td>6</td><td>if N = O then</td></tr><tr><td>7</td><td>」break</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>8</td><td>end</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>9 end</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Output: C updated active arm counter</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>

$$
\begin{array} { r l } & { \displaystyle R _ { T } = \sum _ { t = 1 } ^ { T } \langle a ^ { * } - a _ { t } , \theta ^ { * } \rangle = \sum _ { b = 1 } ^ { R } \sum _ { k = 1 } ^ { K } T _ { k , b } \langle a ^ { * } - a _ { k , b } , \theta ^ { * } \rangle } \\ & { \qquad + \left( T - \sum _ { b = 1 } ^ { B } T _ { b } \right) \langle a ^ { * } - \hat { a } _ { B } , \theta ^ { * } \rangle } \\ & { \displaystyle \leq T _ { 1 } \bar { \Delta } + \sum _ { b = 2 } ^ { B } \ell T _ { b } \sqrt { \beta _ { T } } \epsilon _ { b - 1 } + T \langle a ^ { * } - \hat { a } _ { B } , \theta ^ { * } \rangle } \\ & { \qquad \displaystyle \leq T _ { 1 } \bar { \Delta } + \sum _ { b = 2 } ^ { B } \ell T _ { b } \sqrt { \beta _ { T } } \epsilon _ { b - 1 } + 2 T \sqrt { \beta _ { T } } \epsilon _ { B } . } \end{array}
$$

Step II. Upper-bounding $T _ { b } , \forall b \in [ B ]$

The algorithm sets

$$
\epsilon _ { b } = \epsilon _ { 1 } ^ { \frac { 2 ^ { b } - 1 } { 2 ^ { b - 1 } } } \epsilon _ { 0 } ^ { - \frac { 2 ^ { b - 1 } - 1 } { 2 ^ { b - 1 } } } \ ( \forall b \geq 1 )
$$

and it can be verified that $\exists N _ { 0 } \in \mathbb { N } ^ { + }$ such that $\forall T \geq N _ { 0 }$ , $\epsilon _ { B } ^ { 2 } \leq . . . \leq \epsilon _ { 1 } ^ { 2 } \leq \frac { 5 } { 2 }$ . For any $t \in [ T ]$ , we have

$$
\operatorname* { d e t } ( V _ { t } ) = \operatorname* { d e t } ( V _ { t - 1 } + a _ { t } a _ { t } ^ { T } ) = \operatorname* { d e t } ( V _ { t - 1 } ) ( 1 + \left| \left| a _ { t } \right| \right| _ { V _ { t - 1 } ^ { - 1 } } ^ { 2 } ) .
$$

Since the algorithm pulls each arm $a _ { t }$ until its corresponding confidence radius is below $\epsilon _ { b }$ in the $b$ -th pass, we make an important observation that $\begin{array} { r } { \vert \vert a _ { t } \vert \vert _ { V _ { t - 1 } ^ { - 1 } } \ge \epsilon _ { b } , \breve { \forall } t : \sum _ { b ^ { \prime } = 1 } ^ { b - 1 } T _ { b ^ { \prime } } < } \end{array}$ $\begin{array} { r } { t \leq \sum _ { b ^ { \prime } = 1 } ^ { b } T _ { b ^ { \prime } } } \end{array}$ . Thus for any $b \in [ B ]$ , if $T \geq N _ { 0 }$ ,

$$
\begin{array} { r l } & { \quad \operatorname* { d e t } \left( V _ { \sum _ { b ^ { \prime } = 1 } ^ { b } T _ { b ^ { \prime } } } \right) } \\ & { = \operatorname* { d e t } \left( V _ { \sum _ { b ^ { \prime } = 1 } ^ { b - 1 } T _ { b ^ { \prime } } } \right) \underset { s = 1 + \sum _ { b ^ { \prime } = 1 } ^ { b - 1 } T _ { b ^ { \prime } } } { \sum _ { b ^ { \prime } = 1 } ^ { b } T _ { b ^ { \prime } } } \ ( 1 + \vert \vert a _ { s } \vert \vert _ { V _ { s - 1 } ^ { - 1 } } ^ { 2 } ) } \\ & { \geq \operatorname* { d e t } ( \lambda I ) ( 1 + \epsilon _ { b } ^ { 2 } ) ^ { T _ { b } } } \\ & { \geq \operatorname* { d e t } ( \lambda I ) \exp \Big ( \frac { T _ { b } \epsilon _ { b } ^ { 2 } } { 2 } \Big ) , } \end{array}
$$

where the last inequality is due to the fact that $e ^ { \frac { x } { 2 } } \leq 1 +$ $\textstyle x , \forall x \in [ 0 , { \frac { 5 } { 2 } } ]$ and $\begin{array} { r } { \dot { \forall } T \ge N _ { 0 } , \epsilon _ { B } ^ { 2 } \le \dots \le \epsilon _ { 1 } ^ { 2 } \le \frac { 5 } { 2 } } \end{array}$ . Thus

$$
\begin{array} { c } { { T _ { b } \leq \displaystyle \frac { 2 } { \epsilon _ { b } ^ { 2 } } \ln \displaystyle \frac { \operatorname* { d e t } { \left( V _ { \sum _ { b ^ { \prime } = 1 } ^ { b } T _ { b ^ { \prime } } } \right) } } { \operatorname* { d e t } ( \lambda I ) } \leq \displaystyle \frac { 2 } { \epsilon _ { b } ^ { 2 } } \ln \displaystyle \frac { ( \lambda + T L ^ { 2 } / d ) ^ { d } } { \lambda ^ { d } } } } \\ { { = \displaystyle \frac { 2 d } { \epsilon _ { b } ^ { 2 } } \ln \Big ( 1 + \displaystyle \frac { T L ^ { 2 } } { \lambda d } \Big ) , } } \end{array}
$$

where the last inequality is by Lemma 10 in Abbasi-yadkori, Pa´l, and Szepesva´ri (2011).

Step III. Wrapping up the proof By our previous discussion, under the good event $\mathcal { G }$ ,

$$
\begin{array} { l } { \displaystyle R _ { T } \leq \sum _ { b = 1 } ^ { B } 6 T _ { b } \sqrt { \beta _ { T } } \epsilon _ { b - 1 } + 2 T \sqrt { \beta _ { T } } \epsilon _ { B } } \\ { \displaystyle \quad \leq 2 \sqrt { \beta _ { T } } \left[ 6 d \ln \left( 1 + \frac { T L ^ { 2 } } { \lambda d } \right) \sum _ { b = 1 } ^ { B } \frac { \epsilon _ { b - 1 } } { \epsilon _ { b } ^ { 2 } } + \epsilon _ { B } T \right] } \\ { \displaystyle \quad = 2 \sqrt { \beta _ { T } } \left[ 6 B d \ln \left( 1 + \frac { T L ^ { 2 } } { \lambda d } \right) \frac { \epsilon _ { 0 } } { \epsilon _ { 1 } ^ { 2 } } + \epsilon _ { B } T \right] . } \end{array}
$$

By substituting the values of $\epsilon _ { 0 } , \epsilon _ { 1 }$ , and $\epsilon _ { B }$ into the RHS, we complete the proof.

# Fixed-Budget $\epsilon$ -Best Arm Identification A Multi-Pass Algorithm

In this section, we propose a streaming algorithm that is good at handling an extremely large arm set, given a budget constraint $T$ and a pass constraint $B$ . The key idea is to discretize the arm space with a grid set $\mathcal { G } _ { \epsilon }$ given a fixed approximation parameter $\epsilon > 0$ . The grid set $\mathcal { G } _ { \epsilon }$ satisfies that, for every arm $a \in \mathcal { K }$ , there exists a grid point $g \in \mathcal { G } _ { \epsilon }$ such

Input: $\eta$ regularization parameter $/ \star$ Initialization \*/   
$\textbf { 1 } t  1 , V  \eta I , S  0 , \hat { a }  n u l l$   
2 $\begin{array} { r } { \epsilon  \sqrt { \frac { 2 d } { T } \ln ( 1 + \frac { T L ^ { 2 } } { \eta d } ) } } \end{array}$   
3 for each arm a in the stream do   
4 Read and store arm $a$ to $\tilde { a }$   
5 while $| | a | | _ { V ^ { - 1 } } \geq \epsilon$ do   
6 Pull arm $a _ { t } \gets \tilde { a }$ and obtain reward $\boldsymbol { r } _ { t }$   
7 $\begin{array} { l } { V \gets V + \tilde { a } \tilde { a } ^ { T } } \\ { S \gets S + r _ { t } \tilde { a } } \\ { t \gets t + 1 } \end{array}$   
8   
9   
10 end   
11 if $\hat { a } = n u l l$ or $\langle V ^ { - 1 } S , \hat { a } \rangle < \langle V ^ { - 1 } S , \tilde { a } \rangle$ then   
12 $\hat { a } \gets \tilde { a }$   
13 end   
14 end Output: aˆ

that the $\ell ^ { 2 }$ distance $| | a - g | | _ { 2 } \leq \epsilon .$ . For example, $\mathcal { G } _ { \epsilon }$ can be $\begin{array} { r } { \left\{ \frac { \epsilon } { \sqrt { d } } \pmb { x } \ | \ \pmb { x } \in \mathbb { Z } ^ { d } , | \left| \frac { \epsilon } { \sqrt { d } } \pmb { x } \right| \right| _ { 2 } \le L + \epsilon \rbrace } \end{array}$ . Specifically, arm $a$ is rounded to a gridpoint $\begin{array} { r } { g ( a ) : = \arg \operatorname* { m i n } _ { h \in \mathcal { G } _ { \epsilon } } | | a - h | | _ { 2 } } \end{array}$ . In this way, the algorithm only needs to maintain a counter $C$ associated with $\mathcal { G } _ { \epsilon }$ , recording the number of arms rounded to each grid point, whose size is independent of $K$ .

We present the pseudo-code of our algorithm G-MP-SE in Algorithm 2. The entire process can be divided into three phases: the first pass, passes $b = 2$ to $b = B - 1$ , and the last pass. In the first pass, the algorithm constructs the counter $C _ { 1 }$ without pulling any arms. In each pass $b$ of the second phase, a budget plan is assigned to each grid point. Algorithm 3 employs a randomized yet non-unique implementation of this plan. Alternative correct implementations do not affect our analysis. When the scanning of pass $b$ is finished, the algorithm computes a new estimator $\hat { \theta } _ { b }$ for $\theta ^ { * }$ and uses it to calculate the empirical means of each active grid point (a grid point is active if and only if the counter on it is still positive). The algorithm then eliminates the grid points with relatively low empirical means by setting their counters to zero. As the third phase begins, there remains a unique grid point $g$ whose counter is still positive. An arm corresponding to this $g$ is output as the final prediction. The algorithm requires only an arm memory of size $M = 1$ and needs to store some additional data, the amount of which is independent of $K$ . An asymptotic error probability upper bound for G-MP-SE is derived in Theorem 2.

Theorem 2. Run $G$ -MP-SE algorithm with input $( \epsilon , \tilde { T } =$ $\begin{array} { r } { \frac { T } { B - 2 } - \frac { d ( d + 1 ) } { 2 } , B ) } \end{array}$ d(d2+1) , B) and design λ(b) = λb∗ 1, b = 2, ..., B − 1, where

$$
\begin{array} { r l } & { \lambda _ { b } ^ { * } : = \arg \underset { \lambda \in \mathcal { P } ( \mathcal { K } _ { b } ( \epsilon ) ) } { \operatorname* { m i n } } \ \underset { g ^ { \prime } \in \mathcal { K } _ { b } ( \epsilon ) } { \operatorname* { m a x } } \Vert g ^ { \prime } \Vert _ { \big ( \sum _ { g \in \mathcal { K } _ { b } ( \epsilon ) } \lambda _ { g } g g ^ { T } \big ) ^ { - 1 } } ^ { 2 } , } \\ & { \mathcal { K } _ { b } ( \epsilon ) : = \big \{ g \in \mathcal { G } _ { \epsilon } \mid C _ { b } ( g ) > 0 \big \} , } \end{array}
$$

and $\mathcal { P } ( \Omega )$ is the set of all probability measures over $\Omega$ . If

K=50 K= 500 K=5000 8000 10000 CR-MPS (Our Approach) CR-MPS (Our Approach) CR-MPS (Our Approach) MBSE (Agarwal et al., 2022) 6000 MBSE (Agarwal etal.,2022) 8000 MBSE (Agarwal etal.,2022) MPSE (Li et al.,2023) MPSE (Li et al.,2023) 6000 MPSE (Li et al.,2023) 4000 4000 2000 2000 0 0 0 0 2500 5000 7500100001250015000 0 2500 5000 7500 100001250015000 0 2500 5000 7500100001250015000 Time horizon T Time horizon T Time horizon T

$T > d ( d + 1 ) ( B - 2 ) / 2$ , we have that

$$
\begin{array} { r l r } {  { \operatorname* { l i m } _ { \epsilon \downarrow 0 } \mathbb { P } \big ( \Delta _ { \hat { a } } > 2 | | \theta ^ { * } | | _ { 2 } \epsilon \big ) \le ( B - 2 ) \big ( 2 K ^ { \frac { 1 } { B - 2 } } + 1 \big ) } } \\ & { } & { \times \exp \Big ( - \frac { \Delta ^ { 2 } } { 4 \sigma ^ { 2 } d } \big ( \frac { T } { B - 2 } - \frac { d ( d + 1 ) } { 2 } \big ) \Big ) . } \end{array}
$$

Remark. The advantages of G-MP-SE are twofold: Firstly, state-of-the-art algorithms in the linear bandit scenario are unable to handle a large number of streaming arms given limited memory. For example, in each of its arm elimination phases, OD-LinBAI in Yang and Tan (2022) not only computes the G-optimal design for all the remaining arms but also sorts all of them according to their empirical reward means. These operations heavily rely on complete access to the entire arm set, which is infeasible in the streaming arm setting. Other algorithms, such as PELEG in Zaki, Mohan, and Gopalan (2020), RAGE in Fiez et al. (2019), and $\chi y$ - Adaptive in Soare, Lazaric, and Munos (2014), also have this issue, though they are specialized for the fixed-confidence setting. Secondly, the error probability upper bound of ODLinBAI grows linearly with the number of arms $K$ , whereas the error probability upper bound of our algorithm scales with $K ^ { \frac { 1 } { B - 2 } }$ , which is a much weaker dependency on $K$ when $B$ is large.

# A Single-Pass Algorithm

We note that the proposed G-MP-SE algorithm is only applicable in the multi-pass scenario (requiring $B > 2$ ). Here, we also design an alternative algorithm that is feasible even when $B = 1$ or 2. The pseudo-code for this single-pass strict Best Arm Identification algorithm is provided in Algorithm 5. This algorithm sets a precision parameter $\epsilon$ and scans a single pass through the arm stream, searching for the optimal arm. This choice of $\epsilon$ minimizes the error probability while ensuring that the sample budget $T$ is never exceeded. This algorithm requires only an arm memory of size $M = 2$ . An error probability upper bound is given in Theorem 3.

Theorem 3. Run SPC algorithm in Algorithm 5 with regularization parameter $\eta > 0$ . If the sample budget $T$ is sufficiently large such that $\begin{array} { r } { 2 d \ln \left( 1 + \frac { T L ^ { 2 } } { \eta d } \right) \ \leq \ \frac { 5 } { 2 } T } \end{array}$ , then the probability that the algorithm makes an incorrect prediction

$$
\mathbb { P } ( \hat { a } \neq a ^ { * } ) \leq ( K - 1 ) \exp \Big ( - \frac { T \Delta ^ { 2 } } { 8 \sigma ^ { 2 } d \ln \big ( 1 + \frac { T L ^ { 2 } } { \eta d } \big ) } \Big ) .
$$

150000 CR-MPS (Our Approach) MBSE (Agarwal etal.,2022)   
10000 MPSE (Li et al.,2023) 0 0 2500 5000 7500 100001250015000 Time horizon T

# Experimental Results

We implement our algorithms on both synthetic and realworld datasets. Due to space limitations, we only present the results for the CR-MPS algorithm here and defer the rest to the appendix.

Synthetic Datasets. We run CR-MPS against two baseline streaming bandit algorithms: MBSE in Agarwal, Khanna, and Patil (2022) and MPSE in Li et al. (2023). Their regret curves, averaged over $N = 2 0$ repetitions, are shown in Figure 2. In each independent repetition, we uniformly randomly sample ${ \boldsymbol { \kappa } } , { \boldsymbol { \theta } } ^ { * }$ and run these algorithms. In our experiments, we set $K$ to 50, 500, and 5000, respectively. As $K$ increases, the baseline curves gradually become nonconvergent, whereas our algorithm continues to perform well.

Real-World Dataset. The Kaggle dataset (Chaudhari $2 0 2 3 ) ^ { 1 }$ contains information on more than $1 7 \mathrm { k \Omega }$ anonymous workers, including their resume details and performance scores. We select $K = 1 0 \mathrm { k }$ of them and run these algorithms again on this dataset. The regret curves are shown in Figure 3. The poor performance of the baseline algorithms is due to $K$ being too large for them to handle.

# Conclusion

In this paper, we introduce the Linear Streaming Bandit and propose solutions for regret minimization and fixed-budget $\epsilon$ -best arm identification tasks in this setup. The effectiveness of our algorithms is supported by both theoretical analysis and experimental results. Future research directions include developing matching lower bounds for both tasks.

# Acknowledgments

This work is supported by Tsinghua University Dushi Program and Shanghai Qi Zhi Institute Innovation Program SQZ202312.