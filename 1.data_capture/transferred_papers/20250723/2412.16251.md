# Know2Vec: A Black-Box Proxy for Neural Network Retrieval

Zhuoyi Shang1,2,3, Yanwei $\mathbf { L i u } ^ { 1 , 3 * }$ , Jinxia Liu4, Xiaoyan $\mathbf { G u } ^ { 1 , 3 }$ , Ying Ding1 3, Xiangyang Ji5

1Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 2School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China 3Key Laboratory of Cyberspace Security Defense, Beijing, China 4College of Information and Intelligence Engineering, Zhejiang Wanli University, Ningbo, China 5Tsinghua University, Beijing, China {shangzhuoyi,liuyanwei}@iie.ac.cn, liujinxia $@$ zjwu.edu.cn, {guxiaoyan,dingying}@iie.ac.cn, xyji@tsinghua.edu.cn

# Abstract

For general users, training a neural network from scratch is usually challenging and labor-intensive. Fortunately, neural network zoos enable them to find a well-performing model for directly use or fine-tuning it in their local environments. Although current model retrieval solutions attempt to convert neural network models into vectors to avoid complex multiple inference processes required for model selection, it is still difficult to choose a suitable model due to inaccurate vectorization and biased correlation alignment between the query dataset and models. From the perspective of knowledge consistency, i.e., whether the knowledge possessed by the model can meet the needs of query tasks, we propose a model retrieval scheme, named Know2Vec, that acts as a black-box retrieval proxy for model zoo. Know2Vec first accesses to models via a black-box interface in advance, capturing vital decision knowledge from models while ensuring their privacy. Next, it employs an effective encoding technique to transform the knowledge into precise model vectors. Secondly, it maps the user’s query task to a knowledge vector by probing the semantic relationships within query samples. Furthermore, the proxy ensures the knowledge-consistency between query vector and model vectors within their alignment space, which is optimized through the supervised learning with diverse loss functions, and finally it can identify the most suitable model for a given task during the inference stage. Extensive experiments show that our Know2Vec achieves superior retrieval accuracy against the state-of-the-art methods in diverse neural network retrieval tasks.

# Extended version — https://arxiv.org/pdf/2412.16251

# Introduction

Well-trained models in many domains have demonstrated promising performance in various downstream tasks. The training process refines knowledge from dataset into general rules and patterns, enabling the model to make accurate predictions on new data (Tian et al. 2023). However, their performances vary widely for a targeted downstream application (Zhang et al. 2023). The model whose knowledge is more closely aligned with the task requirements tends to perform better. For example, a model with numerical knowledge would find it easier to complete the MNIST(Deng

Model Zoo 1.generate model representation 2.generate task representation   
User M1   
95   
6 Proxy   
query   
task M2 : 3.Knowledge consistency matching taskrepresentation   
$\textcircled{4} \textcircled{4}$ test accuracy model representation

2012) classification task than a model specialized in flower classification. Assessing the suitableness of a Deep learning(DL) model by uploading the entire dataset to the huge model market for comparison against inference results is risky and impractical due to data disclosure and resource constraints. Therefore, further research is needed to evaluate the correlation between neural network models and query tasks.

Source-Free model transferability estimation (SF-MTE) (Bao et al. 2019; Nguyen et al. 2020; Zhang et al. 2023) methods are designed to rank the suitability of pre-trained models for fine-tuning in downstream tasks. Traditional methods (Bao et al. 2019; Nguyen et al. 2020) directly score the candidate models by utilizing statistical data like features or joint distribution of models and query task. Typically, Model Spider(Zhang et al. 2023) vectorizes both neural network models and query tasks to avoid the high computational costs of forward propagation increased by traditional methods.

With the vectorization idea, Neural Network Retrieval (NNR) (Jeong et al. 2021; Zhong, Qian, and Zhang 2021) tries to transform models and datasets into specific embeddings that facilitates their matching. Generating vectors for models and datasets, and calculating their correlations require an accurate understanding of the key knowledge of both models and query tasks. The pioneering NNR study, DNNR (Zhong, Qian, and Zhang 2021) utilizes litmus images to construct models’ semantic vectors, while TANS (Jeong et al. 2021) further advances the field by searching for a cross-modal space to minimize the semantic discrepancy between model representations and query images. These techniques, while improving retrieval efficiency, still encounter various problems, such as laborious and rough vector generation process (Zhong, Qian, and Zhang 2021; Zhang et al. 2023), imprecise alignment (Jeong et al. 2021), and the necessity for privacy protection (Zhang et al. 2023; Jeong et al. 2021).

In particular, the primary challenge of the correlation calculation methods(SF-MTE or NNR) lies in the two aspects as follows. (1) Transforming the unstructured nature of neural network models into a vectorial format, which must capture the intrinsic knowledge in models for effective retrieval. (2) The establishment of a quantifiable mapping space, where query vectors align with model representations, ensuring semantically similar vectors are proximate. Existing methods often rely on complex and suboptimal vector generation processes, failing to fully capture critical model knowledge or achieve seamless alignment.

To address the above challenges, we propose a novel knowledge-consistency-based black-box proxy for model retrieval, named Know2Vec, and it is shown in Fig. 1. The objective of Know2Vec is to establish a consistent representation of knowledge, allowing semantic alignment between the query task and models. Firstly, it abstracts the intrinsic knowledge acquired by neural network models into a generalized representation in a black-box way. Next, it interacts with users to generate effective task representation by understanding the differences between query samples. Lastly, the proxy is designed to perform a knowledge consistency matching between the abstracted model representation and the task representation, facilitating efficient model retrieval.

Our key contributions are:

• We propose a model knowledge vectorization scheme for parameter-agnostic scenarios, which is designed to capture the implicit model knowledge and further vectorize it to support accurate model retrieval. We further prove in theory that it is feasible to obtain model information with randomly selected probes.   
• A carefully designed measure function is proposed to align the heterogeneous knowledge embeddings, which correspond to the knowledge of the query task and those of known models, assisting users in accurately defining their needs and retrieving the most suitable neural network model.   
• Know2Vec achieves superior retrieval performance across various NNR tasks, outperforming state-of-the-art baselines in our experiments. Additionly, it accesses neural network models in a black-box manner, eliminating the need to understand internal parameters, thus preserving privacy.

# Related Work

# Neural Network Retrieval

NNR addresses the model selection issue by mapping query entries and neural network models into vectors, enabling users to find a satisfactory pre-trained model from model markets (Zhou 2016). Deep Neural Network Retrieval (DNNR)(Zhong, Qian, and Zhang 2021) initially achieves model vectorization through feeding random litmus images to the candidate models. However, it needs extensive datasets and computational resources, making it impractical for online retrieval. TANS (Jeong et al. 2021) aims to align query datasets with similar neural network representations, but it overlooks the subtle differences within categories that are key for aligning knowledge. By representing key decision-making knowledge from models without accessing to their internal parameters and aligning it with the need of a query task, our method achieves both privacy protection and precise retrieval goals.

# Source-Free Model Transferability Estimation

For a given target task and a model library, Source-Free Model Transferability Estimation(SF-MTE) (Ding et al. 2024) aims to propose a metric to quantify the transferability score without the need for individual training. Static SF-MTE methods, such as LEEP(Nguyen et al. 2020), Hscore(Bao et al. 2019), compute scores directly from statistical data like features and logits. In contrast, Dynamic SF-MTE methods aim to project static features into tailored spaces to facilitate superior approximation. They try to estimate the maximum average log evidence(You et al. 2021), or they endeavor to identify a model/task vectorlization technique such as the classical Model Spider (Zhang et al. 2023). However, despite enhancing computational efficiency to a certain extent, these methods still necessitate a complex training process.

# Boundary Supporting Samples

Boundary supporting samples are identified as those close to the decision boundary of neural network models. Assume the target model is a $\mathbf { k }$ -class DNN classifier, where the output layer is an active layer. Formally, we denote by $\left\{ g _ { i } \right\}$ the decision functions of the target classifier, and a data point $x$ is on the target classifier’s classification boundary if at least two labels have the largest discrimination probability, i.e., $g _ { a } ( x ) = g _ { b } ( x ) \ge \operatorname* { m a x } _ { c \ne a , b } \bar { g } _ { c } ( x )$ , where $a , b , c$ are category index, and $g _ { a } ( x )$ is the probability that sample $x$ belongs to category $A$ (Cao, Jia, and Gong 2021). Tian et al. (Tian et al. 2023) claimed that the knowledge transferred from a training dataset to a DL model can be uniquely represented by the model’s decision boundary samples, providing feasibility for us to acquire model knowledge in a black-box setting. However, this method only acquires partial model knowledge and requires target training dataset, which is illogical in NNR problem. Accordingly, we propose a parameteragnostic model knowledge vectoring approach without demanding training dataset.

# Method

Taking NNR problem as an example, we will elaborate on calculating model-dataset correlation when the model parameters are agnostic. We start by vectorizing models and query tasks, which helps to distill the models’ knowledge

Model KnowledgeVetorization Knowledge Alignment Query Knowledge Vetorization MEXT P2 G2 MVE LSAL QEXT 67a5 T t S1 S2 S3 S4 Φ h LSTMa >a MVE LSTMb >b P 4 h LMKC LSTMc LSTMO LSTMd Φ MVE 重 h Model Vector Encoder(MVE) Getting Graph Set of Model Representation LsALLMkc Loss Function 0a 0b0c0d Hidden Embeddings and clarify the requirements of the tasks. We tackle the new issues that incurred from the limited known information. Next, we seek a knowledge-consistent space that acts as a bridge, which connects the two modalities despite their differences in structures and semantic parameters, and providing a way to measure their semantic similarity.

# Problem Formulation

We consider an arbitrary query task $T = \{ s _ { i } , l _ { i } \} _ { i = 1 } ^ { n }$ with $n$ samples $\left\{ { { s } _ { i } } \right\}$ and the corresponding target labels $\{ l _ { i } \}$ . Given a large model hub $M = \{ \Phi _ { i } \} _ { i = 1 } ^ { m }$ with a total of $m$ welltrained models, the goal of NNR is to choose a DNN model $\Phi _ { j }$ that performs well on $T$ . We define $\mathbf { \mathcal { A } } ( \Phi _ { i } , T )$ the verification accuracy of $T$ on model $\Phi _ { i }$ . Mathematically, NNR aims to search for the best-fitted model $\Phi _ { j }$ that satisfies

$$
j = \arg \operatorname* { m a x } _ { i } \mathcal { A } ( \Phi _ { i } , T )
$$

As mentioned earlier, we assume there is a virtually perfect proxy $\mathcal { P }$ that serves as a good communication intermediary: (1) It distills model knowledge and obtains vector $\mathbf { h } _ { \mathbf { i } }$ for each candidate model $\Phi _ { i }$ ; (2) It gets the requirements of query tasks $T$ and generates the corresponding query knowledge vector $\mathbf { t }$ ; (3) It selects a suitable model $\Phi _ { j }$ through a semantic measurement $ { \mathcal { D } }  { \mathcal { T } } S ( )$ . For an effective NNR method, maximizing $\mathbf { \mathcal { A } } ( \Phi _ { i } , T )$ is equivalent to minimizing $\mathcal { D } \mathcal { I } S ( \mathbf { h _ { i } } , \mathbf { t } )$

$$
j = \arg \operatorname* { m a x } _ { i } { \cal A } ( \Phi _ { i } , T ) \Leftrightarrow j = \arg \operatorname* { m i n } _ { i } { \mathcal { D } \ o { \mathbb { Z } } S ( \bf h _ { i } , t ) }
$$

Fig. 2 illustrates the well-designed model retrieval framework. $\mathcal { P }$ includes three components: a model knowledge extractor $M _ { E X T }$ , a query knowledge extractor $Q _ { E X T }$ , and a knowledge alignment space with measurement function $ { \mathcal { D } }  { \mathcal { T } } S ( )$ . Firstly, $M _ { E X T }$ vectorizes model knowledge assisted by a series of additional probe datasets, denoted as ${ \bf { h } } _ { \bf { i } } ^ { \bf { i ^ { \prime } } } = M _ { E X T } ( P _ { i ^ { \prime } } , \Phi _ { i } )$ , where $P _ { i ^ { \prime } }$ is the $i ^ { \prime }$ th probe dataset. Specifically for model $\Phi _ { i }$ , ${ { M } _ { E X T } }$ starts by creating a graph set $G _ { i ^ { \prime } } ^ { \Phi _ { i } }$ with $P _ { i ^ { \prime } }$ , and then encodes $G _ { i ^ { \prime } } ^ { \Phi _ { i } }$ into ${ \bf { h } _ { i } ^ { i ^ { \prime } } }$ through a model vector encoder. Next, $Q _ { E X T }$ extracts semantic correlations from query task $T$ , producing a task knowledge vector $\mathbf { t } = Q _ { E X T } ( T )$ . After that, the knowledge alignment space assesses the consistency of knowledge between model vectors $\{ { \bf h } _ { i } \}$ and query vector $\mathbf { t }$ , for selecting the suitable model to the task.

# Model Knowledge Vectorization

The previous NNR methods attempted to break down candidate model and explore the semantic information through exposed parameters. However, this is laborious and privacy-unfriendly. Fortunately, Theorem 1 proves that model knowledge that is transferred from the training dataset can be encapsulated by a matrix, denoted as knowledge representation matrix $( K R M )$ , providing the possibility for more efficient and privacy-preserving model knowledge embedding. Given the neural network $\Phi$ and its centroid samples of training dataset $P$ , KRM can be generated based on the model’s response to input samples in advance, which only requiring black-box access to $\Phi$ .

$K R M$ is formed by two kind of representative samples: centroid samples and decision boundary samples. As illustrated in Fig. 3, taking binary classification that contains categories $A$ and $B$ as an example, the transferred knowledge $K R M$ consists of two vectors $\left\{ \mathbf { r _ { a } ^ { b } } = x _ { a } ^ { b } - x _ { a } , \mathbf { r _ { b } ^ { a } } = \right.$ $x _ { b } ^ { a } - x _ { b } \}$ , where $x _ { a }$ and $x _ { b }$ are centroid samples of $A$ and $B$ , respectively, $x _ { a } ^ { b }$ is the decision boundary sample from $A$ to $B$ , and $x _ { b } ^ { a }$ is the decision boundary sample from $B$ to $A$ . $x _ { a } ^ { b }$ can be generated by points that respectively belong to categories $A$ and $B ( \mathrm { s u c h } \mathrm { a s } x _ { a } , x _ { b } )$ , and similarly for $x _ { b } ^ { a }$ .

Yet even with $K R M$ , obtaining an effective representation $\mathbf { h }$ is still challenging. The primary obstacle lies in the design of model knowledge extractor $M _ { E X T }$ , which is responsible for converting information-limited $K R M$ into measurable vectors to enable model retrieval tasks. These vectors in $K R M$ encapsulates the incomplete decision knowledge of the model. Considering the dynamic changes in features, we further collect the information both in $K R M$ and representative samples as a graph set and design a specialized DL framework to generate $\mathbf { h }$ .

Furthermore, obtaining central samples is almost impossible because model owners tend to withhold their training datasets due to privacy concerns or copyright restrictions, which complicating the $K R M$ generation process. We solve this issue by proving that alternative datasets can effectively generate a model’s knowledge representation vector.

![](images/5f722836c2c429e5256398d165e3f99c5b4b5db0962daf15be032dac6f47cd1a.jpg)  
Figure 3: Decision Boundary Sample.

Theorem 1 (Tian et al. 2023) The knowledge transferred from a training dataset to a deep learning model can be represented by the knowledge representation matrix $K R M$ formed by perturbation vectors across different classes. For a $k$ -class classifier, let the centroid sample of category $A$ be denoted as $x _ { a }$ , the perturbation vector $\mathbf { r _ { a } ^ { k } } = x _ { a } ^ { k } - x _ { a }$ from category $A$ to category $K$ is defined as the offsets between $x _ { a }$ and $\dot { \boldsymbol { x } } _ { a } ^ { k }$ , where $\bar { x } _ { a } ^ { k }$ is the boundary sample from category $A$ to category $K$ . Collectively, KRM is defined by:

$$
K R M = \left[ \begin{array} { c c c c } { \mathbf { 0 } } & { \mathbf { r _ { a } ^ { b } } } & { . . . } & { \mathbf { r _ { a } ^ { k } } } \\ { \mathbf { r _ { b } ^ { a } } } & { \mathbf { 0 } } & { . . . } & { \mathbf { r _ { b } ^ { k } } } \\ { . . . } & { . . . } & { . . } & { \mathbf { r _ { c } ^ { k } } } \\ { \mathbf { r _ { k } ^ { a } } } & { \mathbf { r _ { k } ^ { b } } } & { . . . } & { \mathbf { 0 } } \end{array} \right]
$$

Getting Graph Set of Model Representation Relying solely on $K R M$ to obtain model knowledge may lead to information loss since the knowledge transfer vector ${ \bf r _ { b } ^ { a } } =$ $x _ { b } ^ { a } - x _ { b }$ overlooks crucial details such as the starting point $x _ { b }$ and ending point $x _ { b } ^ { a }$ . The central sample holds key feature about its category, while the boundary samples imply transition features between categories. First, it’s difficult to pinpoint exactly how features changed as the sample moves from $x _ { b }$ to $x _ { b } ^ { a }$ within $\mathbf { r _ { b } ^ { a } }$ . Second, $K R M$ offers limited insight of the distinctive intra-class knowledge.

As illustrated in Fig. 3, for a classification model $\Phi _ { i }$ with 4 categories, the centroid sample $x _ { a }$ of category $A$ is interconnected with boundary samples $\overline { { x _ { a } } } ~ = ~ \overline { { \{ x _ { a } ^ { \bar { b } } , x _ { a } ^ { c } , x _ { a } ^ { d } \} } }$ , forming a directed graph structure. Within this structure, the directed edges $\{ \mathbf { \bar { r } _ { a } ^ { b } } , \mathbf { r _ { a } ^ { c } } , \mathbf { r _ { a } ^ { d } } \}$ represent the specific connections of $x _ { a }$ . This graph is formally defined as $G _ { a } \ =$ $\{ x _ { a } , \overline { { x _ { a } } } , \{ \mathbf { r } _ { \mathbf { a } } ^ { \mathbf { b } } , \mathbf { r } _ { \mathbf { a } } ^ { \mathbf { c } } , \mathbf { r } _ { \mathbf { a } } ^ { \mathbf { d } } \} \}$ . Among them, $\overline { { x _ { a } } }$ and $x _ { a }$ are two different types of nodes. Expand to other categories, a total of 4 sets of such connection relationships can be modeled: $G ^ { \Phi _ { i } } = \{ G _ { a } , G _ { b } , G _ { c } , G _ { d } \}$ . Undoubtedly, $G ^ { \Phi _ { i } }$ offers a richer semantic representation than $K R M$ .

$G ^ { \Phi _ { i } }$ implicitly links $G _ { a } , G _ { b } , G _ { c } , G _ { d }$ through relationships between categories. Specifically, the information of category $A$ can be obtained from these three types of nodes: (1) The central sample $x _ { a }$ which embodies the unique features about $A$ ; (2) The boundary samples $\overline { { x _ { a } } }$ from $A$ to other categories, and they explain which features need to change for the transition from $A$ to other categories; (3) Boundary samples $x _ { b } ^ { a } , x _ { c } ^ { a } , x _ { d } ^ { a }$ from other categories to $A$ that suggest why the model might incorrectly classify as $A$ . These points are present in Gb, Gc, Gd. By encoding all nodes in GΦi through inter-category relationships, we facilitate a transformation from $G ^ { \Phi _ { i } }$ into the model knowledge vector $\mathbf { h }$ .

Implementation of Model Vector Encoder We consider the relationships as the dependencies of sequential data, in which each sequence corresponds to one category. As shown in Fig. 2, the model vector encoder is implemented with an inner-outer encoder. The inner encoder processes individual subgraphs, while the outer encoder integrates these subgraphs. Both are completed by a bidirectional Long Short Term Memory(LSTM)( $\mathrm { \Delta Y u }$ et al. 2019) network to handle variable long term dependencies. For category $A$ , the information from the above-mentioned first two types of nodes (1) and (2) has been successfully encoded to the hidden embeddings $\theta _ { a }$ by inner layer $L S T M _ { a }$ , as mentioned in Eq. 4. Additionally, the embeddings $\theta _ { b } , \theta _ { c } , \theta _ { d }$ already include information from the third type of nodes (3). These embeddings are further aggregated as the model knowledge vector $\mathbf { h }$ by the outer-layer $L S T M _ { O }$ , aligned through sequence correspondence.

$$
\begin{array} { r l } & { \theta _ { \mathbf { a } } = L S T M _ { a } ( x _ { a } , x _ { a } ^ { b } , x _ { a } ^ { c } , x _ { a } ^ { d } ; W , b ) } \\ & { \theta _ { \mathbf { b } } = L S T M _ { b } ( x _ { b } ^ { a } , x _ { b } , x _ { b } ^ { c } , x _ { b } ^ { d } ; W , b ) } \\ & { \theta _ { \mathbf { c } } = L S T M _ { c } ( x _ { c } ^ { a } , x _ { c } ^ { b } , x _ { c } , x _ { c } ^ { d } ; W , b ) } \\ & { \theta _ { \mathbf { d } } = L S T M _ { d } ( x _ { d } ^ { a } , x _ { d } ^ { b } , x _ { d } ^ { c } , x _ { d } ; W , b ) } \\ & { \mathbf { h _ { i } } = L S T M _ { O } ( \theta _ { \mathbf { a } } , \theta _ { \mathbf { b } } , \theta _ { \mathbf { c } } , \theta _ { \mathbf { d } } ; W , b ) } \end{array}
$$

where $W , b$ are the optimizable parameters.

Thus, $M _ { E X T }$ has encoded $G ^ { \bar { \Phi } }$ into vector $\mathbf { h }$ , as the bidirectional network ensures all edges in $G ^ { \Phi }$ are reachable, either directly or indirectly.

Using probe datasets instead of training datasets Access to the training dataset of a neural network is sometimes impractical, but we can still use other data to probe and obtain boundary samples. Lemma 1 theoretically proves the feasibility that we can still get the semantic relationships of $G ^ { \Phi _ { i } }$ with probe samples.

Lemma 1 The perturbation vectors in KRM can also be obtained from the target model with associating the external datasets.

Proof 1 Taking binary classification with categories $A$ and $B$ as an example, we consider a neural network model $\Phi$ as

$$
\Phi ( x ) = \delta ( { \bf w } * x + { \bf b } )
$$

where $\delta$ is the active function, w and b are the weights and biases, respectively, $x$ is any input sample, and $^ *$ denotes multiplication between vectors.

$\delta$ is composed of $g _ { A }$ and $g _ { B }$ , where $g _ { A } ( x )$ is the probability that sample $x$ belongs to category $A$ , and similarly $g _ { B } ( x )$ for category $B$ . For the convenience of narration, it may be helpful to set $\delta = g _ { A } - g _ { B }$ . Assuming that the centroid samples of training dataset for $\Phi$ are $x _ { a }$ and $x _ { b }$ , the boundary sample from $A$ to $B$ is $\dot { x _ { a } ^ { b } }$ , then

$$
\begin{array} { c } { \delta ( \mathbf { w } * x _ { a } ^ { b } + \mathbf { b } ) = 0 } \\ { \delta ( \mathbf { w } * x _ { a } + \mathbf { b } ) = 1 } \\ { \delta ( \mathbf { w } * x _ { b } + \mathbf { b } ) = - 1 } \end{array}
$$

There must exist two selected samples $z _ { a }$ and $z _ { b }$ that satisfy $\begin{array} { r } { \delta ( \mathbf { w } * z _ { a } + b ) = 1 - \lambda _ { 1 } , \delta ( \mathbf { w } * z _ { b } + b \bar { ) } = - 1 + \lambda _ { 2 } } \end{array}$ , where $\lambda _ { 1 } , \lambda _ { 2 }$

are very small values that can be ignored. Correspondingly, a boundary sample $z _ { a } ^ { b }$ from $A$ to $B$ satisfies $\delta ( \mathbf { w } * z _ { a } ^ { b } + b \bar { ) = }$ $0 - \lambda _ { 3 }$ , with $\lambda _ { 3 }$ being a very small value. Then,

$$
\begin{array} { r } { \delta ( \mathbf { w } * x _ { a } ^ { b } + \mathbf { b } ) - \delta ( \mathbf { w } * z _ { a } ^ { b } + \mathbf { b } ) = \lambda _ { 3 } } \\ { \delta ( \mathbf { w } * x _ { a } + \mathbf { b } ) - \delta ( \mathbf { w } * z _ { a } + \mathbf { b } ) = \lambda _ { 1 } } \\ { \delta ( \mathbf { w } * x _ { b } + \mathbf { b } ) - \delta ( \mathbf { w } * z _ { b } + \mathbf { b } ) = \lambda _ { 2 } } \end{array}
$$

Since $\delta$ is continuous and differential in the regions of interest, there exists a value $\sigma$ that satisfies $z _ { a } ^ { b } = \check { x } _ { a } ^ { b } + \check { \sigma }$ due to the Mean Value Theorem. Similarly, there must also be $a$ disturbance $\sigma _ { a }$ such that $x _ { a } = z _ { a } + \sigma _ { \underline { { a } } }$ .

Therefore, the perturbation vector $\mathbf { r _ { a } ^ { \tilde { b } } } = x _ { a } ^ { b } - x _ { a }$ can also alternatively represented by Eq.(12), where $\sigma$ and $\sigma _ { a }$ are the offsets.

$$
\mathbf { r _ { a } ^ { b } } = z _ { a } ^ { b } - z _ { a } + \sigma + \sigma _ { a }
$$

For a fixed model $\Phi$ , $x _ { a }$ and $x _ { a } ^ { b }$ are unique, and also $\sigma$ and $\sigma _ { a }$ are only related to $z _ { a }$ and $z _ { a } ^ { b } { } _ { ; }$ , respectively. Therefore, $\mathbf { r _ { a } ^ { b } }$ can be represented by $z _ { a }$ and $z _ { a } ^ { b }$ . This principle is applicable to other vectors in $K R M$ , and the proven conclusion can be extended to other classification models.

# Query Knowledge Vectorization

For the query task $T ~ = ~ \{ s _ { i } , l _ { i } \} _ { i = 1 } ^ { n }$ , we implement the query encoder $Q _ { E X T }$ to discern correlations both within and across categories within the query samples. We first average the samples of each class to identify features unique to that class, denoted as $\theta _ { \mathbf { k } }$ for class $k$ , then we feed these features of distinct classes as separate sequences into a bidirectional LSTM-based network $L S T M _ { t }$ to investigate how they relate to one another, as detailed in the following equation,

$$
\begin{array} { r } { \boldsymbol { \theta } _ { \mathbf { k } } = \frac { 1 } { | \mathbf { I } ( l _ { i } = k ) | } \sum [ s _ { i } * \mathbf { I } ( l _ { i } = k ) ] } \\ { \mathbf { t } = L S T M _ { t } ( \boldsymbol { \theta } _ { \mathbf { 1 } } , \boldsymbol { \theta } _ { \mathbf { 2 } } , . . . , \boldsymbol { \theta } _ { \mathbf { k } } ; W , b ) } \end{array}
$$

where $k$ is the category index, $| { \bf I } ( l _ { i } = k ) |$ is the number of class $k , { \bf \delta I } ( { \bf \delta } )$ is one if the logical expression in the bracket is true, otherwise is zero.

# Knowledge Alignment

We develop an effective loss function that encourages the alignment between model embedding $\mathbf { h }$ and task embedding t within our retrieval proxy, enabling knowledge-consistent model retrieval. As shown in Fig. 2, a model embedding consistency function ${ \cal L } _ { M K C }$ is used to encourage neural networks to overcome the noise caused by external datasets, and a spatial alignment loss function $L _ { S A L }$ is used to overcome various biases between $\mathbf { h }$ and $\mathbf { t }$ .

Model Embedding Consistency Loss. Assuming ${ \bf { h } _ { i } ^ { i ^ { \prime } } }$ is the generated embedding of $\Phi _ { i }$ by using probe dataset $\bar { P } _ { i ^ { \prime } }$ , and it contains both the model’s inherent knowledge and noise from $P _ { i ^ { \prime } }$ . To address this, a category loss ${ { L } _ { M K C } }$ is used to incentivize $M _ { E X T }$ to learn the knowledge specific to $\Phi _ { i }$ , using a distinct index $i$ for each model as the training label,

$$
L _ { M K C } = C E ( { \bf { h } } _ { \bf { i } } ^ { \bf { i } ^ { \prime } } , i )
$$

where $C E$ is the well-established cross-entropy loss function(Ho and Wookey 2020).

Spatial Alignment Loss. After vectorization, there are still semantic and mapping space bias between $\mathbf { h }$ and t. h is encoded from two types of samples, while t aggregates the features of each category, that leads to semantic differences. These differences in mapping space due to $M _ { E X T }$ and $Q _ { E X T }$ further contribute to the alignment biases. To suppress the biases, we characterize the knowledge consistency with cosine similarity incorporating a margin of 0.4.

$$
L _ { S A L } ( \mathbf { t _ { i } } , \mathbf { h _ { j } } ) = \left\{ \begin{array} { l l } { 1 - c o s ( \mathbf { t _ { i } } , \mathbf { h _ { j } } ) , } & { \mathrm { i f ~ } i = j } \\ { \operatorname* { m a x } ( 0 , c o s ( \mathbf { t _ { i } } , \mathbf { h _ { j } } ) - 0 . 4 ) , } & { \mathrm { e l s e } } \end{array} \right.
$$

where $i$ and $j$ are the indexes of the query task and candidate model, respectively, cos is the cosine distance. Therefore, the final objective function is defined as follows:

$$
L = L _ { M K C } + \alpha \cdot L _ { S A L }
$$

where $\alpha$ is a is a constant parameter to balance the different losses, and it is set to 1 in our experiment.

Finally in the well-established knowledge alignment space after training, the model index $j$ with the strongest semantic correlation between t and candidate model embeddings $\{ { \bf h _ { i } } \} _ { i = 1 } ^ { m }$ can be obtained by the semantic measurement $ { \mathcal { D } }  { \mathcal { T } } S (  { ) }$ , which is implemented by the cosine distance.

$$
j = \arg \operatorname* { m i n } _ { i } { \mathcal { D } } { \mathcal { T } } S ( \mathbf { t } , \mathbf { h } \mathbf { i } )
$$

# Experiments

We compare our Know2Vec with several state-of-the-art methods in two scenarios: NNR and SF-MTE. There are four groups of comparison methods.

• Statistical SF-MTE methods: H-Score (Bao et al. 2019), NCE (Tran, Nguyen, and Hassner 2019), Leep (Nguyen et al. 2020), NLeep (Li et al. 2021), and LFC (Deshpande et al. 2021). • Dynamic SF-MTE methods: LogME (You et al. 2021) and Model Spider (Zhang et al. 2023). • General NNR methods: TANS (Jeong et al. 2021) and DNNR (Zhong, Qian, and Zhang 2021). Since DNNR requires to train a considerable neural network model for querying data, we do not compare with it. • Universal Large language models (LLMs): We also examine GPT-4 (Achiam et al. 2023) and Gemini (Team et al. 2023) due to their powerful generation capability.

# Performance Comparison on NNR Tasks

Experimental Setup. The evaluation experiment is carried on a modified model-hub created from Kaggle1 with diverse real-world datasets/models following the methodology outlined in TANS(Jeong et al. 2021). We developed 58 classification models and 232 distinct testing tasks. The probe images are randomly selected from the training dataset of Know2Vec. For fairness, we fine-tuned the model generated from LLMs for 500 steps, as LLMs typically generate neural network rather than select them. In the specific vector computation, the vectors of our approach and TANS method

60.0 40.070.0 : 0.1 -0.170.0 .. 1.0 0.870.0 -2.2 -2.5700 中 1.0 0.0 . 8 3 u -1.0· 1.0 0.0 ' 3 Top: Aircraft Bottom: DTD80.0 80.0 80.0 80.0 70.0 70.0 80.0  
100.0 · 0.1 1.0 ！ ！ 0.0f 1.0 ： ： 2.5 广 g ·Inception-v3  
0.0 0.0 1 -2.0 1 P 0.0 2 ·Densenet201： 0.0 0.0 -2.5 T ·Resnet5060.0 70.0 60.0 70.0 60.0 70.0 60.0 70.0 60.0 70.0 60.0 70.0 Transferred  
H-Score LFC LogME NLEEP Model Spider Ours Accuracy

Table 1: Performance comparison of NNR tasks.   

<html><body><table><tr><td></td><td>R@1</td><td>R@3</td><td>V. Acc</td><td>Ft. Acc</td><td>Time</td><td>Pri.</td></tr><tr><td>H-Score NCE</td><td>3.02 91.81</td><td>7.76 100</td><td>29.07 94.03</td><td>58.94 90.22</td><td>23.21 10.09</td><td></td></tr><tr><td>Leep</td><td>93.10</td><td>100</td><td>94.33</td><td>91.66</td><td>11.28</td><td>YY</td></tr><tr><td>NLeep</td><td>75.86</td><td>92.24</td><td>83.84</td><td>85.99</td><td>10.60</td><td></td></tr><tr><td>LFC</td><td>91.38</td><td>100</td><td>92.79</td><td>90.25</td><td>10.03</td><td>2Y</td></tr><tr><td>LogME</td><td>50.43</td><td>62.93</td><td>64.68</td><td>77.30</td><td></td><td></td></tr><tr><td>Model Spider</td><td>3.87</td><td></td><td></td><td></td><td>11.32</td><td></td></tr><tr><td>TANS</td><td></td><td>5.60</td><td>27.23</td><td>39.18</td><td>4.28</td><td></td></tr><tr><td></td><td>82.75</td><td>100</td><td>93.70</td><td>94.22</td><td>≤0.1</td><td></td></tr><tr><td>Ours</td><td>94.82</td><td>100</td><td>94.87</td><td>95.67</td><td>≤0.1</td><td>YYY</td></tr><tr><td>GPT-4</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>-</td><td></td><td></td><td>46.37</td><td>34.48</td><td></td></tr><tr><td>Gemini</td><td>-</td><td></td><td></td><td>33.87</td><td></td><td>3Y</td></tr></table></body></html>

are 256 dimensions long, while that of Model Spider is 1024 dimensions long.

To thoroughly assess our method against benchmarks, we adopt a suite of established metrics, including: (1) Top-k hitting ratio $( \mathrm { R @ { k , \% } } )$ , that measures the overlap percentage between the top- $\mathbf { \cdot k }$ prediction results and the ground truth. (2) Valid Accuracy $( \mathrm { V . A c c } , \% )$ and Fine-Tuned Accuracy $\left( \mathrm { F t . A c c , \% } \right)$ , which quantify the accuracy of the query task on the top-1 selected model and the results after finetuning over 50 trials, respectively. (3) Search Time(Time, s). (4) Privacy(Pri.). We categorize privacy into three tiers of model access permissions: white-box access $\gamma$ , grey-box access $\gamma \gamma$ and black-box access $\gamma \gamma \gamma$ .

Experimental Analysis. The quantitative comparison results of NNR task are shown in Table 1. The best score is in bold. As can be seen, in the evaluation of statical methods, Leep achieves higher retrieval accuracy among the evaluated methods, due to its focus on average loglikelihood. However, it falls slightly behind in search time compared to NCE, NLeep, and LFC. H-Score, unfortunately, underperforms in both search time and accuracy, possibly due to its complex calculation and lack of consideration for similarities within categories. Dynamic methods such as Model Spider and LogME also struggle, possibly because of their focus on ranking order of transferability on abundant downstream data, whereas NNR task is more concerned with the performance of the selected top-1 model. Despite its lower accuracy than LogME, Model Spider benefits from vector-based computations, consuming less search time. The same beneficiaries also include our method and TANS. Fortunately, TANS excels in search time and provides substantial retrieval accuracy although it offers only a sub-optimal level of privacy. Our method, while maintains superiority in terms of computation time, offers superior retrieval performance and maintains privacy. Notably, our method achieved a $1 . 7 2 \%$ increase in retrieval accuracy over the suboptimal result, which demonstrates the superior precision of the knowledge alignment space embedded in our proposed proxy. Undoubtedly, although GPT-4 and Gemini ensure a strong privacy since they do not require access to model zoo, their performances in statistical data falls short of expectations.

# Performance Comparison on SF-MTE tasks

Experimental Setup. We construct a heterogeneous model zoo similar to previous work(Zhang et al. 2023), where we collect 48 publicly available pre-trained models trained on diverse datasets2, covering various neural network architectures. The probe dataset is filtered from several publicly available datasets. We evaluate various methods on 4 different downstream tasks, Aircraft(Maji et al. 2013) and DTD(Cimpoi et al. 2014) for classification, UTKFace (Zhang, Song, and Qi 2017) and dSprites(Matthey et al. 2017) for regression. We leave blank for the regression column of NCE, Leep, NLeep, and LFC since they cannot be used for regression tasks.

We measure the performance of SF-MTE with Pearson(P.)(Cohen et al. 2009) and Spearman(S.)(Hauke and Kossowski 2011) correlation scores, as they are widely adopted (Nguyen et al. 2020; Li et al. 2021; Zhang et al. 2023; Jeong et al. 2021; Deshpande et al. 2021) to evaluate the relationship between the predicted transferability scores and test accuracy.

Experimental Analysis The statistical evaluations of model transferability over classification and regression tasks are shown in Table 2. To provide a clear representation of the correlation between the baseline predictions and the actual accuracy, we visualize the top-6 results of Pearson correlation scores in Fig. 4. In the static methods, LFC and NLeep show a consistent performance, achieving positive and satisfactory scores in both Pearson and Spearman evaluations. By comparison, NCE and Leep show negative correlation coefficients. H-Score performs better on Spearman score than Pearson score, probably because Pearson score is more sensitive to the predicted outlier’s scores. In dynamic SF-MTE methods, Model Spider performs poorly, perhaps because its insufficient robustness. In contrast, LogME excels in classification tasks, a testament to the precision of its linear estimation model. TANS struggles in the SF-MTE tasks, while GPT-4 and Gemini display negative or near-zero correlation coefficients on most datasets, indicating a less competitive performance compared to other methods. Our method excels in classification tasks, with improvements of Spearman coefficient reaching 0.1842 and 0.0442 over the sub-optimal result. This indicates a strong correlation between predicted transferability scores and actual test accuracy, as shown in Fig.5, thanks to the semantically rich knowledge vectors and precise matching process. Although our method is not the best in every evaluation dimensions, which may due to being trained only on classification tasks, it still delivers satisfactory results across all tested tasks.

Table 2: Performance comparison on source-free model transferability estimation tasks.   

<html><body><table><tr><td></td><td colspan="4">Classification</td><td colspan="4">Regression</td><td colspan="2">Mean</td></tr><tr><td></td><td colspan="2">DTD</td><td colspan="2">Aircraft</td><td colspan="2">UTKFace</td><td colspan="2">dSprites</td><td></td><td></td></tr><tr><td></td><td>P.</td><td>S.</td><td>P.</td><td>S.</td><td>P.</td><td>S.</td><td>P.</td><td>S</td><td>P.</td><td>S.</td></tr><tr><td>H-Score</td><td>0.1081</td><td>0.2311</td><td>0.1915</td><td>0.4967</td><td>-0.0011</td><td>-0.0012</td><td>0.2243</td><td>0.2014</td><td>0.0945</td><td>0.2320</td></tr><tr><td>NCE</td><td>-0.1650</td><td>-0.2559</td><td>0.0845</td><td>-0.0229</td><td></td><td>-</td><td>1</td><td>-</td><td>-0.0402</td><td>-0.1394</td></tr><tr><td>Leep</td><td>-0.1672</td><td>-0.2229</td><td>-0.2079</td><td>-0.1718</td><td>-</td><td>-</td><td></td><td>1</td><td>-0.1875</td><td>-0.1973</td></tr><tr><td>NLeep</td><td>0.1153</td><td>0.2298</td><td>0.3852</td><td>0.3213</td><td>_-</td><td></td><td></td><td></td><td>0.2502</td><td>0.2755</td></tr><tr><td>LFC</td><td>0.3508</td><td>0.2383</td><td>0.4323</td><td>0.4733</td><td></td><td></td><td>1</td><td></td><td>0.3915</td><td>0.3558</td></tr><tr><td>LogME</td><td>0.2367</td><td>0.3280</td><td>0.5310</td><td>0.5337</td><td>-0.0038</td><td>-0.0031</td><td>0.1082</td><td>0.1114</td><td>0.2180</td><td>0.2425</td></tr><tr><td>Model Spider</td><td>0.1705</td><td>0.2937</td><td>0.0793</td><td>0.1263</td><td>0.1763</td><td>0.1599</td><td>-0.0365</td><td>-0.0483</td><td>0.0974</td><td>0.1329</td></tr><tr><td>TANS</td><td>0.2365</td><td>0.2738</td><td>-0.3804</td><td>-0.3110</td><td>-0.0057</td><td>0.0091</td><td>0.1054</td><td>0.1126</td><td>-0.0110</td><td>0.2112</td></tr><tr><td>Ours</td><td>0.4942</td><td>0.5122</td><td>0.5545</td><td>0.5779</td><td>0.1900</td><td>0.1909</td><td>0.1917</td><td>0.2608</td><td>0.3576</td><td>0.3854</td></tr><tr><td>GPT-4</td><td>-0.3228</td><td>-0.1030</td><td>-0.2093</td><td>-0.0273</td><td>0.0475</td><td>0.0412</td><td>-0.0759</td><td>-0.0814</td><td>-0.1401</td><td>-0.0426</td></tr><tr><td>Gemini</td><td>-0.0099</td><td>0.0828</td><td>0.0039</td><td>-0.0184</td><td>0.0269</td><td>0.0764</td><td>0.0797</td><td>0.1475</td><td>0.0251</td><td>0.0720</td></tr></table></body></html>

Table 3: Ablation study of retrieval architecture.   

<html><body><table><tr><td></td><td>QEXT</td><td>LSTM</td><td>ConCat</td><td>Avg.</td></tr><tr><td>MEXT</td><td>LSTM ConCat Avg.</td><td>94.82 90.87 90.33</td><td>92.54 94.05 89.34</td><td>88.14 87.37 89.35</td></tr></table></body></html>

Table 4: Ablation study of loss functions and probe dataset.   

<html><body><table><tr><td>w/oLMKC</td><td>LsAL(Cos.)</td><td>LsAL(Con.)</td><td>Ptrain</td></tr><tr><td>82.97</td><td>94.82</td><td>93.53</td><td>95.25</td></tr></table></body></html>

# Ablation Study

We assess the performance of each component designed in the proposed proxy on the Kaggle-hub.

Analysis of Knowledge Vectorization Architecture. As shown in Table 3, we explored alternative sequence encoding methods, transitioning from an LSTM network to simpler network such as averaging (Avg.) and concatenation (ConCat). Fixing the structure of $M _ { E X T }$ to an LSTM, we found that the result in the first column $( 9 4 . 8 2 \% )$ is significantly higher than those in the second column $( 9 2 . 5 4 \% )$ and third column $( 8 8 . 1 4 \% )$ , suggesting that the $L S T M _ { t }$ in $Q _ { E X T }$ captures query knowledge more accurately. Likewise, the first row’s retrieval accuracy in the first column significantly outperforms the other rows, highlighting the effectiveness of LSTM-based vector encoder in $M _ { E X T }$ in extracting detailed model information. We further made TSNE visualization of model representations before(left) and after(right) encoding in $M _ { E X T }$ . In Fig. 5, different colors correspond to the knowledge vectors for different models. The right figure shows an improvement over the left by correctly separating models that were incorrectly clustered together based on their semantics.

![](images/5757a19e6b4f40ba90378899ed93a55ad8649c34be7cd20d1f0c28deb54c956f.jpg)  
Figure 5: Visual description of model knowledge vectors.

Analysis of Different Loss Functions. First, we tested two unsupervised loss functions, cosine loss (Cos.) and contrastive loss (Con.) for spatial alignment loss $L _ { S A L }$ . It can be seen from Table 4, Know2Vec achieved the highest accuracy of $9 4 . 8 2 \%$ with $L _ { S A L } ( \mathrm { C o s } )$ , allowing for the natural knowledge alignment. Moreover, we observe a slight drop in performance without ${ \cal L } _ { M K C }$ , and this highlights the importance of $L _ { M K C }$ in filtering noise from model vectors.

Analysis of Different Probe Datasets. In Table 4, the value of $P _ { t r a i n }$ indicates the model retrieval accuracy when the target model’s training dataset is used as the probe dataset, suggesting that alternative dataset might be as effective as the training dataset in generating model knowledge vectors. The same image can serve as a probe to further generate knowledge vectors for different models with alternative dataset, thereby accurately depicting the semantic differences of models in the knowledge consistency space.

# Conclusion

In this paper, we propose Know2Vec, a novel proxy for neural network retrieval under a black-box situation. This proxy translates both model knowledge and query data knowledge into vectors, and thus enhancing the accuracy of the retrieval process by ensuring the knowledge consistency among them. The experimental results from NNR and SFMTE tasks confirm that Know2Vec surpasses the state-ofthe-art baseline methods in retrieval precision with acceptable retrieval speed, while also addressing privacy concerns.