# CLIP-CID: Efficient CLIP Distillation via Cluster-Instance Discrimination

Kaicheng $\mathbf { Y a n g } ^ { 1 * }$ , Tiancheng $\mathbf { G } \mathbf { u } ^ { 2 * }$ , Xiang $\mathbf { A } \mathbf { n } ^ { 1 }$ , Haiqiang Jiang1, Xiangzi Dai1, Ziyong Feng1, Weidong $\mathbf { C a i } ^ { 2 \dagger }$ , Jiankang Deng3‡

1 DeepGlint 2 University of Sydney 3 Imperial College kaichengyang@deepglint.com

# Abstract

Contrastive Language-Image Pre-training (CLIP) has achieved excellent performance over a wide range of tasks. However, the effectiveness of CLIP heavily relies on a substantial corpus of pre-training data, resulting in notable consumption of computational resources. Although knowledge distillation has been widely applied in single modality models, how to efficiently expand knowledge distillation to vision-language foundation models with extensive data remains relatively unexplored. In this paper, we introduce CLIP-CID, a novel distillation mechanism that effectively transfers knowledge from a large vision-language foundation model to a smaller model. We initially propose a simple but efficient image semantic balance method to reduce transfer learning bias and improve distillation efficiency. This method filters out $4 3 . 7 \%$ of image-text pairs from the LAION400M while maintaining superior performance. After that, we leverage cluster-instance discrimination to facilitate knowledge transfer from the teacher model to the student model, thereby empowering the student model to acquire a holistic semantic comprehension of the pre-training data. Experimental results demonstrate that CLIP-CID achieves state-of-the-art performance on various downstream tasks including linear probe and zero-shot classification.

![](images/617861f6ba72f15b98a79b16c9f3f7fe025686acc4afb51d7307ced27981071a.jpg)  
Figure 1: The linear probe performance comparison between CLIP-CID and OPENCLIP across 14 common datasets. Despite the exclusion of $4 3 . 7 \%$ of image-text pairs from the LAION400M, CLIP-CID exhibits exceptional performance.

# 1 Introduction

With the proliferation of mobile networks and social platforms, there has been an explosion in the production of image-text pairs (Guo, Wang, and Wang 2019; Gu et al. 2024). This abundance of data has provided a strong foundation for the advancement of vision-language pretraining (Radford et al. 2021; Jia et al. 2021; Yu et al. 2020; Yang, Xu, and Gao 2020). The Contrastive LanguageImage Pre-training (CLIP) (Radford et al. 2021) achieves remarkable success in multi-modal learning by aligning image-text pairs on a large-scale dataset. It learns two separate unimodal encoders for image and text using a contrastive loss, one of the most effective losses for representation learning (Tian, Krishnan, and Isola 2020; He et al. 2020; Chen et al. 2020; Chopra, Hadsell, and LeCun 2005). Nevertheless, the efficacy of CLIP heavily depends on an extensive pre-training dataset. The original CLIP models are pre-trained on 400 million imagetext pairs for 32 epochs, demanding thousands of GPU days. This poses a substantial challenge in scenarios with limited computational resources (Radenovic et al. 2023; Yang et al. 2023). Recently, large-scale image-text datasets crawled from websites, such as LAION400M (Schuhmann et al. 2021) and LAION5B (Schuhmann et al. 2022), have gained widespread usage in vision-language pre-taining. DataComp (Gadre et al. 2024) consists of image-text pairs extracted from Common Crawl’s web data and employs various strategies such as basic filtering, CLIP score filtering, and text&image-based filtering. However, there is still a lot of semantic repetition in the training data, which not only has the potential to impact the performance of representation learning but also results in a waste of computational resources (Radenovic et al. 2023; Wang et al. 2023).

Knowledge Distillation (KD) (Hinton, Vinyals, and Dean 2015) is proposed to enhance the performance of a small student model by transferring knowledge from a large teacher model throughout the training phase. Most existing KD approaches in the literature are primarily designed for smallscale datasets (e.g., CIFAR10 and CIFAR100 (Krizhevsky, Hinton et al. 2009)), as well as small models (e.g., ResNet50 and ResNet34 (He et al. 2016)). Recent studies have concentrated on distilling CLIP for particular target tasks or datasets. For example, BeamCLIP (Kim et al. 2022) introduces cross-modal similarity matching and context-based prompt augmentation to transfer knowledge from CLIP representations to a small model, achieving better performance on the ImageNet (Deng et al. 2009). ZeroSeg (Chen et al. 2023) distills the visual concepts learned by CLIP into a set of segment tokens, leading to a marked enhancement in training efficiency while preserving segmentation performance. Nevertheless, the exploration of leveraging knowledge distillation to enhance foundational models remains relatively sparse. Recent works (Wu et al. 2023; Sun et al. 2023) successfully transfer knowledge from a large foundation model to a small one. However, their attention is restricted to instance-level knowledge, thus unable to capture the semantic structure of extensive training data effectively. This limitation arises from the nature of instance-wise contrastive learning, which consistently regards samples from distinct instances as negative pairs, disregarding their semantic similarity.

To address the aforementioned challenges, this paper introduces CLIP-CID, a novel distillation mechanism that effectively transfers knowledge from a large vision-language foundation model to a smaller model. In order to reduce transfer learning bias and improve distillation efficiency, we initially propose a simple but efficient method to balance the semantic concepts within the LAION400M dataset, which can filter out $4 3 . 7 \%$ of the training data while maintaining superior performance. Furthermore, we enhance the knowledge transfer from a large teacher model to a smaller student model by integrating cluster-instance discrimination, which facilitates a more comprehensive semantic understanding of the student model. As illustrated in Fig. 1, our proposed CLIP-CID demonstrates superior linear probe performance across 14 common datasets even after filtering out $4 3 . 7 \%$ of image-text pairs from the LAION400M. The main contributions of this paper are summarized as follows:

• We propose a simple but efficient image semantic balance method to reduce transfer learning bias and improve distillation efficiency, which can remove $4 3 . 7 \%$ of image-text pairs from the LAION400M while maintaining superior performance.   
• We introduce the CLIP-CID, a novel distillation mechanism that integrates cluster discrimination and instance discrimination to effectively transfer knowledge from a large vision-language foundation model to a smaller model.   
• We conduct extensive experiments to validate the effectiveness of our proposed approach. The experimental results prove that CLIP-CID achieves state-of-the-art performance on various downstream tasks, including zeroshot classification and linear probe.

# 2 Related Work

# 2.1 Vision-Language Pre-training

CLIP (Radford et al. 2021) has achieved remarkable attention due to its exceptional zero-shot recognition ability and successful transfer capabilities. Recent developments have improved CLIP-based methodologies (Mu et al. 2022; Geng et al. 2023). ALBEF (Li et al. 2021) introduces a contrastive loss to align the image and text representations before fusing them through cross-modal attention, which enables more grounded vision and language representation learning. ALIGN (Jia et al. 2021) leverages a dataset of over one billion noisy image alt-text pairs to scale visual and vision-language representation learning. FILIP (Yao et al. 2021) successfully exploits the finer-grained expressiveness between image patches and textual words by modifying the contrastive loss and enables offline pre-computation of image and text representations during inference. FLIP (Li et al. 2023) employs random masking and removal of a significant portion of image patches during training, enabling learning from a larger number of image-text pairs within the same wall-clock time and contrasting more samples per iteration while maintaining a comparable memory footprint. However, pre-training vision-language foundation models on extensive datasets present a substantial challenge due to the high costs involved and the significant consumption of computational resources.

# 2.2 Large-Scale Dataset Filtering

While open-source large-scale datasets such as LAION400M (Schuhmann et al. 2021) have filtered image-text pairs based on CLIP scores below 0.3, it is crucial to emphasize that the effectiveness of CLIP scores may be compromised in specific scenarios. For instance, images containing visible text within the image often yield high CLIP scores but could prove detrimental to representation learning (Cao et al. 2023). Recent research proposes a complexity, action, and textspotting filtering strategy (Radenovic et al. 2023) to select informative imagetext pairs from noisy web-scale datasets. Additionally, the pruning method adapted from ImageNet to LAION (Abbas et al. 2024) uses a straightforward complexity measure to reduce training costs to one-quarter of the standard approach. However, the process of cleaning large-scale datasets presents a computational challenge. To tackle this issue, SemDeDup (Abbas et al. 2023) leverages embeddings from pre-trained models to identify and eliminate semantic duplicates. Different from the above methods, we propose a simple but efficient image semantic balance method to reduce transfer learning bias and improve distillation efficiency. It only entails a single traversal of the data and can filter $4 3 . 7 \%$ of image-text pairs from the LAION400M while maintaining superior performance.

# 2.3 Knowledge Distillation

Knowledge distillation has found wide application in the field of computer vision. Building upon the achievements of visual-language foundational models, recent research efforts (Wei et al. 2022; Dong et al. 2022) have demonstrated significant performance enhancements on specific datasets, such as ImageNet (Deng et al. 2009). Hybrid Distillation (Shi et al. 2023) achieves superior performance on COCO (Lin et al. 2014) by integrating masked autoencoders with CLIP. CLIP-TD (Wang et al. 2022) efficiently distills knowledge from CLIP into established architectures through a dynamically weighted objective concentrating on adaptively chosen tokens per instance, resulting in remarkable performance gains on visual commonsense reasoning tasks. DIME-FM (Sun et al. 2023) aims to distillate small foundation models using smaller-scale public images and unpaired sentences. However, the above methods only focus on instance-level knowledge, neglecting the semantic structure of large-scale training data. This limitation arises from instance-wise contrastive learning, which pairs samples as negatives based solely on their instance differences, without considering their semantic similarity.

# 3 Methodology

# 3.1 Image Semantic Balance

To enhance the comprehensive transfer of knowledge from the teacher model to the student model and reduce transfer learning bias, we propose a simple yet effective image semantic balance method. Our primary objective is to address both perceptual redundancy and semantic redundancy within images. As illustrated in Fig. 2a, perceptual redundancy images showcase minimal discrepancies at the pixel level. In contrast, as depicted in Fig. 2b, semantic redundancy images exhibit significant pixel-level variations while maintaining notably similar semantic information. As shown in Fig. 2d, the presence of perceptual redundancy and semantic redundancy in images causes an imbalanced distribution of semantic concepts in the dataset, leading to knowledge transfer biases in the student model during distillation.

In contrast to existing cluster-based methods that require multiple iterations (Abbas et al. 2024, 2023), our method requires only a single complete traversal of the training data to integrate images with similar semantics into the same set. The image semantic balance process is shown in Fig. 2c. Initially, we employ the OPENCLIP ViT-bigG/14 (Ilharco et al. 2021) model to extract image embeddings from LAION400M (Schuhmann et al. 2021). To address memory limitations, we divide all image embeddings E ∈ RN×d into $c$ chunks and distribute $E _ { n } \in \mathbb { R } ^ { \frac { N } { c } \times d }$ to different nodes. Then we calculate the Euclidean distance (Dokmanic et al. 2015) matrix between the current image and the images contained in various chunks, arranging the rows in ascending order according to their distances. The top- $k$ results for each chunk are retained in the matrix $C _ { n } \ \in \ \mathbb { R } ^ { \frac { N } { c } \times k }$ , and these distance matrices are concatenated to form the global matrix $C \in \mathbb { R } ^ { N \times k }$ . After that, we employ the Union-Find algorithm (Tarjan 1975) to group semantically similar images together. If the distance between different images surpasses the distance threshold $\beta$ , they will be assigned to separate sets. Otherwise, they will be merged into the same set. We identify the central image (the image nearest to the centroid of the set) and eliminate the remaining images within the same set. Consequently, the total count of LAION400M images is decreased to 225M, denoted as LAION225M. It is worth noting that the GPU is exclusively used for computing embedding distances, while the union-find algorithm is executed on the CPU. As shown in Fig. 2d, LAION225M demonstrates a smoother distribution, facilitating the student model learning a more comprehensive knowledge from the teacher model.

![](images/09bbbdb744d59860961eda2b04887024a145249bf10e99374f5b291982c48d4d.jpg)  
Figure 2: (a) and (b) visualization of the perceptual redundancy images and semantic redundancy images. (c) visualization of the image semantic balance process. (d) distribution of LAION400M and LAION225M in 1M clusters.

# 3.2 Cluster-level Distillation

Traditional instance-wise contrastive learning treats different instances as negative pairs, limiting its ability to capture the complete semantic information in the training data (Caron et al. 2018; Asano, Rupprecht, and Vedaldi 2020; Zhan et al. 2020; Qian et al. 2022). In this study, we introduce cluster discrimination knowledge distillation to delve into potential semantic structures inherent in the training dataset. Our method involves grouping visually similar instances into clusters, thus enabling a more comprehensive semantic representation. The cluster discrimination distillation method involves two stages: (1) Clustering, which assigns a unique class label to each image and gets cluster centers. (2) Cluster discrimination distillation, which facilitates the transfer of global knowledge from the teacher model to the student model.

![](images/a0bd84bc88d0e647ddd50d48fc90440ffbe02815672bffda07af9077ac90db68.jpg)  
Figure 3: The architecture of our proposed cluster-instance discrimination distillation.

Clustering. We investigate the standard $k$ -means algorithm, which aims to partition a given set of vectors into $k$ distinct groups based on the nearest neighbor criterion. Given the normalized image embedding $e _ { i }$ , the clustering process involves jointly learning a centroid matrix $W \ \in$ $\mathbf { \varmathbb { R } } ^ { d \times k }$ and assigning the cluster label $z _ { i }$ for each image by solving the following optimization problem:

$$
\operatorname* { m i n } _ { W \in \mathbb { R } ^ { d \times k } } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \operatorname* { m i n } _ { \substack { z _ { i } \in \{ 0 , 1 \} ^ { k } } } \Vert e _ { i } - W z _ { i } \Vert _ { 2 } ^ { 2 } \quad \mathrm { s . t . } \quad z _ { i } ^ { \top } \mathbf { 1 _ { k } } = \mathbf { 1 } ,
$$

where $N$ is the number of training samples, and the centroid $w _ { i }$ belonging to centroid matrix $\mathbf { \bar { \boldsymbol { W } } } \in \mathbb { R } ^ { d \times k }$ is considered the normalized prototype of $i$ -th cluster. $z _ { i }$ in $\{ 0 , 1 \} ^ { k }$ is a single label assignment restricted by the condition $z _ { i } ^ { \top } \mathbf { 1 _ { k } } =$ 1, where $\mathbf { 1 _ { k } }$ is 1-vector with size of $k$ .

In this work, we employ the OPENCLIP ViT-bigG/14 (Ilharco et al. 2021) model to extract image embeddings. The automatically clustered large-scale dataset inevitably faces challenges such as intra-class impurity and inter-class conflicts due to the presence of noise in the large uncurated webscale datasets. The intra-class impurity can be addressed by adjusting the cluster number. Meanwhile, the inter-class conflict can be effectively mitigated by reducing the number of sampled negative instances within the minibatch. Following previous works (An et al. 2023, 2024), we leverage the benefits of efficient feature quantization (Johnson, Douze, and Je´gou 2019) to cluster LAION225M into one million classes. To alleviate inter-class conflict, we employ PatialFC (An et al. 2022) and randomly sample a portion of the negative class centers during each iteration.

Cluster Discrimination Distillation. After clustering, we can inherit the original mechanism in the vanilla KD (Hinton, Vinyals, and Dean 2015) to implement instance-cluster alignment. As illustrated in Fig. 3, we consider a set of training images $I = \{ x _ { 1 } , x _ { 2 } , . . . , x _ { n } \}$ , comprising $n$ images. Initially, we employ the student image encoder and the teacher image encoder to get normalized student image embeddings $E _ { i } ^ { s } ~ \stackrel { \textstyle - } { = } ~ \{ e _ { 1 } ^ { s } , e _ { 2 } ^ { s } , . . . , e _ { n } ^ { s } \}$ and normalized teacher image embeddings $E _ { i } ^ { t } = \{ e _ { 1 } ^ { t } , e _ { 2 } ^ { t } , . . . , e _ { n } ^ { t } \}$ (the teacher image embeddings are extracted offline). These normalized image embeddings $e _ { i } ^ { s } \in \mathbb { R } ^ { d }$ and $e _ { i } ^ { t } \in \mathbb { R } ^ { d }$ are passed through a fully connected layer that is initialized using the cluster centers. It is important to note that without this initialization will lead to the collapse of model training. Subsequently, the images are partitioned into $k$ classes, represented by prototypes $\bar { W } = \{ \dot { w } _ { i } \} _ { i = 1 } ^ { k }$ . With pseudo labels and cluster centers obtained from the above clustering step, the logit distillation loss can be implemented by optimizing a standard softmax classification loss as:

$$
\mathcal { L } _ { 1 } = - \sum _ { i = 1 } ^ { n } \log \frac { \exp ( w _ { i } ^ { \top } e _ { i } ^ { s } ) } { \sum _ { j = 1 } ^ { k } \exp ( w _ { j } ^ { \top } e _ { i } ^ { s } ) } .
$$

Furthermore, we implement distribution alignment by minimizing the Kullback-Leibler (KL) divergence between the prediction probability from the teacher and the student:

$$
\mathcal { L } _ { \mathrm { d } } = \sum _ { i = 1 } ^ { n } \mathrm { K L } ( \frac { \exp ( w _ { i } ^ { \top } e _ { i } ^ { s } / \tau ) } { \sum _ { j = 1 } ^ { k } \exp ( w _ { j } ^ { \top } e _ { i } ^ { s } / \tau ) } \bigg | | \frac { \exp ( w _ { i } ^ { \top } e _ { i } ^ { t } / \tau ) } { \sum _ { j = 1 } ^ { k } \exp ( w _ { j } ^ { \top } e _ { i } ^ { t } / \tau ) } ) ,
$$

where $\tau$ is the temperature hyper-parameter used to soften distribution representation. Finally, the cluster-level distillation loss $\mathcal { L } _ { \mathrm { c l u s t e r } }$ is defined as:

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { c l u s t e r } } = \alpha \mathcal { L } _ { \mathrm { l } } + ( 1 - \alpha ) \mathcal { L } _ { \mathrm { d } } , } \end{array}
$$

where $\alpha$ is a loss weight used to balance the influence of different losses.

# 3.3 Instance-level Distillation

The cluster-level distillation primarily impacts the student image encoder, facilitating the model in capturing comprehensive semantic information from the training data. However, it may unintentionally neglect the subtle nuances of fine-grained semantic details and impose limitations on image-text alignment. To address this issue, we introduce the instance-level distillation loss. Given an image-text pair, we first get the offline extracted teacher image embedding $e _ { i } ^ { t }$ and teacher text embedding $c _ { i } ^ { t }$ . After that, we use $e _ { i } ^ { t }$ and $c _ { i } ^ { t }$ to supervise the student text embedding $c _ { i } ^ { s }$ and student image embedding $e _ { i } ^ { s }$ respectively. We employ the bi-directional contrastive loss (Sohn 2016; Srivastava and Salakhutdinov 2012) to align the teacher embedding and the student embedding, which is defined as:

$$
\begin{array} { r l r } {  { \mathcal { L } _ { \mathrm { c o n t r a s t } } ( e , c ) = \frac { 1 } { 2 } ( \mathcal { L } _ { e \to c } + \mathcal { L } _ { c \to e } ) , \mathrm { ~ w h e r e ~ } } } \\ & { } & { \mathcal { L } _ { e \to c } = - \displaystyle \sum _ { i = 1 } ^ { n } \log \frac { \exp ( e _ { i } ^ { \top } c _ { i } / \tau ) } { \sum _ { j = 1 } ^ { n } \exp ( e _ { i } ^ { \top } c _ { j } / \tau ) } , ~ } \\ & { } & { \mathcal { L } _ { c \to e } = - \displaystyle \sum _ { i = 1 } ^ { n } \log \frac { \exp ( e _ { i } ^ { \top } c _ { i } / \tau ) } { \sum _ { j = 1 } ^ { n } \exp ( e _ { j } ^ { \top } c _ { i } / \tau ) } . ~ } \end{array}
$$

Then the instance-level distillation loss $\mathcal { L } _ { \mathrm { i n s t a n c e } }$ is defined as:

<html><body><table><tr><td></td><td></td><td>[opoo</td><td>CEIIIIIO</td><td>druspg</td><td></td><td></td><td></td><td></td><td>sed</td><td></td><td></td><td></td><td>OITLS</td><td></td><td></td><td></td></tr><tr><td></td><td colspan="10">Model Architecture: ViT-B/32</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>WIT400M</td><td>84.4</td><td>91.3</td><td>65.1</td><td>37.8</td><td>59.4</td><td>21.2</td><td>44.5</td><td>87.0</td><td>87.9</td><td>66.7</td><td>63.2</td><td>97.2</td><td>49.4</td><td>63.2</td><td>65.6</td></tr><tr><td>CLIPt</td><td>WIT400M</td><td>82.9</td><td>88.7</td><td>63.7</td><td>35.4</td><td>57.3</td><td>18.9</td><td>43.3</td><td>84.0</td><td>89.3</td><td>66.0</td><td>61.5</td><td>96.6</td><td>43.9</td><td>61.9</td><td>63.8</td></tr><tr><td>OPENCLIP‡</td><td>LAION400M 80.5</td><td></td><td>90.6</td><td>70.7</td><td>42.6</td><td>78.1</td><td>15.9</td><td>51.3</td><td>85.8</td><td>91.2</td><td>66.0</td><td>66.7</td><td>95.2</td><td>49.7</td><td>62.1</td><td>67.6</td></tr><tr><td>OPENCLIP*</td><td>LAION225M 79.6</td><td></td><td>92.5</td><td>74.5</td><td>43.8</td><td>79.9</td><td>15.7</td><td>50.9</td><td>84.4</td><td>90.2</td><td>65.5</td><td>65.2</td><td>94.0</td><td>44.7</td><td>62.2</td><td>67.4</td></tr><tr><td>CLIP-CID</td><td>LAION225M 79.7</td><td></td><td>92.6</td><td>75.2</td><td>47.7</td><td>83.7</td><td>25.6</td><td>52.2</td><td>88.4</td><td>90.8</td><td>69.1</td><td>66.0</td><td>93.2</td><td>45.3</td><td>62.7</td><td>69.4</td></tr><tr><td colspan="10">Model Architecture: ViT-B/16</td><td colspan="7"></td></tr><tr><td>CLIPt</td><td>WIT400M</td><td>89.2</td><td>91.6</td><td>68.7</td><td>39.1</td><td>65.6</td><td>27.1</td><td>46.0</td><td>88.9</td><td>89.3</td><td>70.4</td><td>65.2</td><td>98.2</td><td>54.1</td><td>68.6</td><td>68.7</td></tr><tr><td>CLIPt</td><td>WIT400M</td><td>87.8</td><td>89.6</td><td>66.4</td><td>40.9</td><td>63.5</td><td>23.1</td><td>44.8</td><td>87.3</td><td>90.4</td><td>67.6</td><td>63.0</td><td>98.0</td><td>52.9</td><td>67.2</td><td>67.3</td></tr><tr><td>OPENCLIP†</td><td>LAION400M 85.7</td><td></td><td>91.8</td><td>71.1</td><td>46.4</td><td>82.7</td><td>16.5</td><td>50.2</td><td>88.6</td><td>91.9</td><td>66.0</td><td>68.6</td><td>96.8</td><td>51.1</td><td>65.6</td><td>69.5</td></tr><tr><td>OPENCLIP*</td><td>LAION225M 85.8</td><td></td><td>93.3</td><td>75.7</td><td>50.4</td><td>83.4</td><td>16.3</td><td>51.0</td><td>87.8</td><td>92.0</td><td>67.0</td><td>66.0</td><td>96.5</td><td>50.8</td><td>65.6</td><td>70.1</td></tr><tr><td>CLIP-CID</td><td>LAION225M 84.4</td><td></td><td>93.9</td><td>76.9</td><td>51.5</td><td>85.2</td><td>25.5</td><td>51.7</td><td>88.6</td><td>92.0</td><td>69.0</td><td>66.6</td><td>95.1</td><td>51.3</td><td>65.8</td><td>71.3</td></tr></table></body></html>

Table 1: Zero-shot classification comparison. We present zero-shot performance on 14 common downstream datasets. †: Results reported in CLIP paper. $\ddag$ : Results we reproduced. ⋆: Results of the OPENCLIP model trained on LAION225M.

$$
\mathcal { L } _ { \mathrm { i n s t a n c e } } = \gamma \mathcal { L } _ { \mathrm { c o n t r a s t } } ( e _ { i } ^ { s } , c _ { i } ^ { t } ) + ( 1 - \gamma ) \mathcal { L } _ { \mathrm { c o n t r a s t } } ( c _ { i } ^ { s } , e _ { i } ^ { t } ) ,
$$

where $\gamma$ is a loss weight. Finally, the overall loss function is defined as:

$$
\mathcal { L } _ { \mathrm { o v e r a l l } } = \mathcal { L } _ { \mathrm { b a s e } } + \mathcal { L } _ { \mathrm { c l u s t e r } } + \mathcal { L } _ { \mathrm { i n s t a n c e } } ,
$$

where ${ \mathcal { L } } _ { \mathrm { b a s e } } ~ = ~ { \mathcal { L } } _ { \mathrm { c o n t r a s t } } ( e _ { i } ^ { s } , c _ { i } ^ { s } )$ represents the standard CLIP loss.

# 4 Experiments and Results

# 4.1 Experimental Settings

Implementation Details. In this paper, we utilize the OPENCLIP bigG/14 as the teacher model. The student model adopts the same architecture as CLIP (Radford et al. 2021). We employ AdamW (Loshchilov and Hutter 2017) as the optimizer, initializing it with a learning rate of $1 e - 3$ and a weight decay of 0.2. To prevent collapsing, the initial learning rate of the fully connected layer is set to $1 e \mathrm { ~ - ~ } 6$ . Based on empirical observations, we set the loss weight $\alpha$ and $\gamma$ to 0.999 and 0.5 respectively. We set $\beta _ { 1 }$ to 0.9 and $\beta _ { 2 }$ to 0.98 for improved training stability. The input image size is $2 2 4 \times 2 2 4$ , and the input text sequence length is truncated or padded to 77. The temperature parameter $\tau$ is initialized to 0.07. We conduct distillation training on ViT-B/32 and ViT-B/16 models for 32 epochs, using a batch size of 32, 768 and 24, 576 on 64 NVIDIA H800 GPUs.

Downstream Datasets. To prove the effectiveness of our method, we present linear probe and zero-shot classification performance across 14 datasets, including Food101 (Bossard, Guillaumin, and Van Gool 2014), CIFAR10 & CIFAR100 (Krizhevsky, Hinton et al. 2009), Birdsnap (Berg et al. 2014), Stanford Cars (Krause et al. 2013),

Aircraft (Maji et al. 2013), DTD (Cimpoi et al. 2014), Oxford Pets (Parkhi et al. 2012), Caltech101 (Fei-Fei, Fergus, and Perona 2004), Flowers102 (Nilsback and Zisserman 2008), SUN397 (Xiao et al. 2010), STL10 (Coates, $\mathrm { N g }$ , and Lee 2011), EuroSAT (Helber et al. 2019), and ImageNet (Deng et al. 2009). Additionally, to evaluate the robustness of our model, we conduct zero-shot robustness comparison on ImageNet-V2 (Recht et al. 2019), ImageNetA (Recht et al. 2019), ImageNet-R (Hendrycks et al. 2021), ObjectNet (Barbu et al. 2019), and ImageNet-Sketch (Wang et al. 2019).

# 4.2 Experimental Results

Zero-shot Classification. We present our performance on 14 zero-shot classification datasets. The prompt templates and class names are consistent with CLIP (Radford et al. 2021). As shown in Tab. 1, the OPENCLIP trained on LAION225M achieves comparable performance with trained on LAION400M. This is primarily attributed to the removal of semantically redundant images, which enhances both the semantic balance and diversity of the pre-training dataset. Furthermore, through the integration of clusterinstance discrimination distillation, CLIP-CID ViT-B/32 and CLIP-CID ViT-B/16 achieve an average performance of $6 9 . 4 \%$ and $7 1 . 3 \%$ respectively across the 14 datasets, surpassing OPENCLIP trained on LAION400M by $1 . 8 \%$ and $\bar { 1 } . 8 \%$ . This performance improvement demonstrates the effectiveness of the cluster-instance discrimination distillation in enhancing student representations.

Linear Probe. Following the same evaluation setting as CLIP (Radford et al. 2021), we freeze our model and only train a logistic regression classifier. In Tab. 2, we present linear probe performance on 14 downstream datasets. Similarity with zero-shot classification, we observe that after removing $4 3 . 7 \%$ of the training data, the OPENCLIP trained on the filtered LAION225M achieves similar performance with trained on the entire LAION400M. By employing our proposed cluster-instance discrimination distillation method, CLIP-CID demonstrates an average performance improvement of $2 . 3 \%$ and $1 . 8 \%$ across 14 datasets. Notably, our method exhibits superior performance on CIFAR10&CIFAR100, Oxford Pets, Birdsnap, Stanford Car, and Aircraft, further substantiating the effectiveness of our approach in significantly enhancing the representation power for instance discrimination.

<html><body><table><tr><td></td><td></td><td></td><td>CIIIIII CEIIIIIO</td><td>deuspg</td><td>ser</td><td></td><td></td><td>sed</td><td></td><td></td><td>163508</td><td>OITLS</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td colspan="10">Model Architecture: ViT-B/32</td><td colspan="7"></td></tr><tr><td></td><td>WIT400M</td><td>88.8</td><td>95.1</td><td>80.5</td><td>58.5</td><td>81.8</td><td>52.0</td><td>76.5</td><td>90.0</td><td></td><td>93.096.9</td><td>76.6</td><td>98.3</td><td></td><td>97.0 76.1</td><td>82.9</td></tr><tr><td>CLIPt</td><td>WIT400M</td><td>88.6</td><td>95.0</td><td>80.2</td><td>61.8</td><td>81.3</td><td>50.6</td><td>76.4</td><td>89.3</td><td>92.7</td><td>94.6</td><td>76.9</td><td>98.2</td><td>95.0</td><td>75.0</td><td>82.5</td></tr><tr><td>OPENCLIP‡</td><td>LAION400M 87.0</td><td></td><td>95.9</td><td>83.0</td><td>64.1</td><td>89.5</td><td>54.3</td><td>79.8</td><td>88.8</td><td>93.4</td><td>95.8</td><td>77.0</td><td>97.8</td><td>95.9</td><td>73.8 84.0</td><td></td></tr><tr><td>OPENCLIP*</td><td>LAION225M 86.2</td><td></td><td>96.7</td><td>85.0</td><td>62.8</td><td>90.0</td><td>55.4</td><td>80.3</td><td>88.4</td><td>93.1</td><td>95.7</td><td>76.5</td><td>97.7</td><td>95.8</td><td></td><td>74.0 84.1</td></tr><tr><td>CLIP-CID</td><td>LAION225M 86.6</td><td></td><td>97.1</td><td>86.1</td><td>70.0 93.0 65.1 80.6 91.6 94.6</td><td></td><td></td><td></td><td></td><td></td><td>97.2</td><td>76.5</td><td>98.1</td><td></td><td></td><td>96.3 75.1 86.3</td></tr><tr><td colspan="10">Model Architecture: ViT-B/16</td><td colspan="7"></td></tr><tr><td>CLIPt</td><td>WIT400M</td><td>92.8</td><td>96.2</td><td>83.1</td><td>67.8</td><td>86.7</td><td>59.5</td><td>79.2</td><td>93.1</td><td>94.7</td><td>98.1</td><td>78.4</td><td>99.0</td><td>97.1</td><td>80.2</td><td>86.1</td></tr><tr><td>CLIPt</td><td>WIT400M</td><td>92.7</td><td>96.0</td><td>82.5</td><td>72.4</td><td>86.4</td><td>59.7</td><td>78.9</td><td>93.1</td><td>94.0</td><td>96 5</td><td>78.6</td><td>99.1</td><td>95.3</td><td>79.3</td><td>86.0</td></tr><tr><td>OPENCLIP‡</td><td>LAION400M 90.8</td><td></td><td>96.4</td><td>84.0</td><td>71.7</td><td>92.0</td><td>59.5</td><td>81.5</td><td>91.7</td><td>94.7</td><td>97.3</td><td>79.4</td><td>98.7</td><td>96.1</td><td>77.7</td><td>86.5</td></tr><tr><td>OPENCLIP*</td><td>LAION225M 90.1</td><td></td><td>97.3</td><td>86.4</td><td>73.2</td><td>92.8</td><td>59.0</td><td>81.4</td><td>91.7</td><td>94.6</td><td>96.9</td><td>78.5</td><td>98.5</td><td>96.8</td><td>77.6</td><td>86.8</td></tr><tr><td>CLIP-CID</td><td>LAION225M 90.5</td><td></td><td></td><td>97.4 87.2</td><td>77.2</td><td>93.8</td><td>70.9</td><td>81.8</td><td>92.9</td><td>95.0</td><td>98.4</td><td>78.3</td><td>98.6</td><td>96.8</td><td></td><td>78.0 88.3</td></tr></table></body></html>

Table 2: Linear probe comparison. We present linear probe performance on 14 common downstream datasets. †: Results reported in CLIP paper. ‡: Results we reproduced. ⋆: Results of the OPENCLIP model trained on LAION225M.

Table 3: Zero-shot robustness comparison. ‡: Results we reproduced. ⋆: Results of the OPENCLIP model trained on LAION225M.   

<html><body><table><tr><td>Model</td><td>Dataset</td><td>IN-V2 IN-A</td><td>IN-R</td><td></td><td>Object IN-S</td><td>Average</td></tr><tr><td colspan="7">Model Architecture:ViT-B/32</td></tr><tr><td>CLIPt</td><td>WIT400M</td><td>54.9</td><td>31.9</td><td>67.1 52.6</td><td>38.8</td><td>49.1</td></tr><tr><td>OPENCLIP‡</td><td>LAION400M</td><td>54.6</td><td>22.2 72.3</td><td>52.7</td><td>46.5</td><td>49.7</td></tr><tr><td>OPENCLIP*</td><td>LAION225M</td><td>54.0</td><td>21.2 71.4</td><td>51.3</td><td>46.5</td><td>48.9</td></tr><tr><td>CLIP-CID</td><td>LAION225M</td><td>54.7</td><td>21.9 72.5</td><td>52.9</td><td>47.1</td><td>49.8</td></tr><tr><td colspan="7">Model Architecture:ViT-B/16</td></tr><tr><td>CLIPt</td><td>WIT400M</td><td>61.1</td><td>49.6 76.0</td><td>59.0</td><td>45.1</td><td>58.1</td></tr><tr><td>OPENCLIP‡</td><td>LAION400M</td><td>59.1</td><td>33.4 76.8</td><td>57.0</td><td>49.8</td><td>55.2</td></tr><tr><td>OPENCLIP*</td><td>LAION225M</td><td>58.2</td><td>31.4 76.5</td><td>55.7</td><td>50.3</td><td>54.4</td></tr><tr><td>CLIP-CID</td><td>LAION225M</td><td>59.3</td><td>32.9 77.1</td><td>57.5</td><td>50.5</td><td>55.5</td></tr></table></body></html>

Zero-shot Robustness Evaluation. In Tab. 3, we present a robustness evaluation across different model sizes. We observe that excluding $4 3 . 7 \%$ of the training data leads to a marginal decrease in robustness. Subsequent integration of the cluster-instance discrimination distillation mechanism enables our model to acquire knowledge from the teacher model efficiently and effectively. Consequently, our model consistently exhibits superior robustness compared to OPENCLIP. In Fig. 4, we visually depict the weight distribution of the last fully connected layer in both the middle and last transformer layers. After distillation, we observe an expanded range of values within the weight distribution of our model, accompanied by a reduction in the number of elements proximal to zero. This phenomenon reflects the improvement of capacity since the weights can assume a more diverse array of potential values or states (Shen and Savvides 2020).

![](images/10beb6211d5f149f4997795031e572174e8959130c3bb2a551c039979efd375d.jpg)  
Figure 4: Weight distribution of the last fully connected layer in the middle and last transformer layers.

# 4.3 Ablation Study

Ablation on Threshold $\beta .$ . To explore the optimal image filtering ratio of the LAION400M, we perform an ablation study on threshold $\beta$ . The value of $\beta$ is associated with the number of sets, which directly influences the proportion of removed images. We train standard OPENCLIP ViT-B/32 on the filtered dataset with various values of the threshold $\beta$ . As shown in Tab. 4, setting $\beta$ to 0.07 results in the removal of $4 3 . 7 \%$ of image-text pairs, and we observe the optimal performance. However, increasing $\beta$ to 0.08 raised the filtration rate to $4 7 . 5 \%$ , leading to a significant performance decline. Ablation on Distillation Loss. We perform ablation experiments on distillation losses to validate our proposed clusterinstance distillation mechanism. As shown in Tab. 5, the integration of cluster discrimination distillation loss improves linear probe performance from $8 4 . 1 \%$ to $8 5 . 7 \%$ , while the marginal gain in zero-shot performance is only $0 . 9 \%$ . This is because the cluster-level distillation loss mainly affects the image encoder, enhancing its ability to learn comprehensive semantic information while imposing constraints on image alignment. After introducing the instance discrimination distillation loss, it improves cross-modal alignment and captures more fine-grained semantics, which significantly boosts the zero-shot result from $6 8 . 3 \%$ to $6 9 . 4 \%$ .

Table 4: Ablation on different threshold $\beta$ .   

<html><body><table><tr><td>β</td><td>Dataset</td><td>Filtration Ratio</td><td>Linear probe</td><td>Zero-shot</td></tr><tr><td>0.06</td><td>LAION237M</td><td>40.8%</td><td>83.4</td><td>66.2</td></tr><tr><td>0.07</td><td>LAION225M</td><td>43.7%</td><td>84.1</td><td>67.4</td></tr><tr><td>0.08</td><td>LAION210M</td><td>47.5%</td><td>83.7</td><td>67.1</td></tr></table></body></html>

Table 5: Ablation on different loss combinations.   

<html><body><table><tr><td>Lbase</td><td>Leluster</td><td>Linstance</td><td>Dataset</td><td>Linear probe</td><td>Zero-shot</td></tr><tr><td>√</td><td>X</td><td>X</td><td>LAION225M</td><td>84.1</td><td>67.4</td></tr><tr><td>√</td><td>√</td><td>X</td><td>LAION225M</td><td>85.7</td><td>68.3</td></tr><tr><td>√</td><td>X</td><td>√</td><td>LAION225M</td><td>85.3</td><td>68.8</td></tr><tr><td>√</td><td>√</td><td>√</td><td>LAION225M</td><td>86.3</td><td>69.4</td></tr></table></body></html>

Ablation on Cluster Centers. The number of cluster centers is a vital factor in managing inter-class and intra-class conflicts. As demonstrated in Tab. 6, we present the average linear probe and zero-shot classification performance of CLIP-CID ViT-B/32. The increase in the number of cluster centers from 0.1 million to 1 million resulted in a corresponding enhancement in model performance. This enhancement can be attributed to the increased intra-cluster purity, signifying improved discrimination and representation capabilities of the model. However, due to increased intercluster conflicts, the performance deteriorates as the number of cluster centers rises from 1M to 5M.

Table 6: Ablation on different numbers of cluster centers.   
Raw Image OPENCLIP CLIP-CID Raw Image OPENCLIP CLIP-CID   

<html><body><table><tr><td>Cluster Centers</td><td>Dataset</td><td>Linear probe</td><td>Zero-shot</td></tr><tr><td>0.1M</td><td>LAION225M</td><td>83.8</td><td>67.8</td></tr><tr><td>0.5M</td><td>LAION225M</td><td>84.6</td><td>68.3</td></tr><tr><td>1M</td><td>LAION225M</td><td>86.3</td><td>69.4</td></tr><tr><td>2M</td><td>LAION225M</td><td>85.8</td><td>69.0</td></tr><tr><td>5M</td><td>LAION225M</td><td>85.2</td><td>68.8</td></tr></table></body></html>

Ablation on Teacher Models. To explore the impact of the difference in parameter quantity between teacher and student models, we compare the performance of CLIP-CID ViT-B/32 distilled from different scale teacher models. The experiment results are shown in Tab. 7, we find a larger teacher model such as OPENCLIP ViT-bigG/14 produces better student performance for both linear probe and zeroshot classification.

Visualization of PCA Components. We present the results of Principal Component Analysis (PCA) applied to patch features extracted by OPENCLIP ViT-B/32 and our CLIPCID ViT-B/32. We retain patches with positive values after applying a threshold to the first component, effectively separating the main object from the background. Subsequently, a second PCA is computed on the remaining patches. To visualize the results, we assign three distinct colors to the first three components of each model. In Fig. 5, our model demonstrates superior semantic understanding by consistently preserving the consistent color representation of object parts across diverse images within the same category. For example, the visualization of our model consistently maintains the head of the goose in a consistent red color. However, the OPENCLIP displays three different colors. We also provide visualization of embeddings, clusters, and class activation maps, please refer to the supplementary material.

Table 7: Ablation on different scales of teacher models.   

<html><body><table><tr><td>Teacher Model</td><td>Dataset</td><td>Linear probe</td><td>Zero-shot</td></tr><tr><td>OPENCLIPViT-L/14</td><td>LAION225M</td><td>85.5</td><td>68.7</td></tr><tr><td>OEPNCLIP ViT-bigG/14</td><td>LAION225M</td><td>86.3</td><td>69.4</td></tr></table></body></html>

![](images/8c545400459ca7b392bacb3875765d03c559f6ff39b77e478b8c324d8832eac5.jpg)  
Figure 5: Visualization of PCA components. We extract three principal components from the collected patch features of each image. The principal components are then visualized using separate color channels. Similar colors within patches indicate semantic similarities. We use $\cdot$ to accentuate the primary distinction.

# 4.4 Conclusion

In this paper, we introduce CLIP-CID, a novel distillation mechanism that effectively transfers knowledge from a large vision-language foundation model to a smaller model. To mitigate transfer learning bias and enhance distillation efficiency, we propose an efficient image semantic balance method, which can filter $4 3 . 7 \%$ of image-text pairs from the LAION400M while maintaining superior performance. After that, we employ cluster-instance discrimination to facilitate knowledge transfer from the teacher to the student model, enabling the student model to achieve a comprehensive semantic understanding of the pre-training data. Experimental results demonstrate that CLIP-CID surpasses existing methods in various downstream tasks, including linear probe and zero-shot classification.