# Towards Precise Prediction Uncertainty in GNNs: Refining GNNs with Topology-grouping Strategy

Hyunjin $\mathbf { S e 0 } ^ { 1 , 3 }$ , Kyusung $\mathbf { S e 0 } ^ { 1 * }$ , Joonhyung $\mathbf { P a r k ^ { 1 * } }$ , Eunho Yang1,2†

1Korea Advanced Institute of Science and Technology (KAIST) 2AITRICS 3Polymerize bella72, seo3650, deepjoon, eunhoy @kaist.ac.kr

# Abstract

Recent advancements in graph neural networks (GNNs) have highlighted the critical need of calibrating model predictions, with neighborhood prediction similarity recognized as a pivotal component. Existing studies suggest that nodes with analogous neighborhood prediction similarity often exhibit similar calibration characteristics. Building on this insight, recent approaches incorporate neighborhood similarity into node-wise temperature scaling techniques. However, our analysis reveals that this assumption does not hold universally. Calibration errors can differ significantly even among nodes with comparable neighborhood similarity, depending on their confidence levels. This necessitates a re-evaluation of existing GNN calibration methods, as a single, unified approach may lead to sub-optimal calibration. In response, we introduce a novel approach that categorizes nodes by both neighborhood similarity and their own confidence, irrespective of proximity or connectivity. Our method allows finegrained calibration by employing group-specific temperature scaling, with each temperature tailored to address the specific miscalibration level of affiliated nodes, rather than adhering to a uniform trend based on neighborhood similarity. Extensive experiments demonstrate the effectiveness of our framework across diverse datasets on different GNN architectures, achieving up to $1 3 . 7 9 \%$ error reduction compared to uncalibrated GNN predictions.

# Introduction

Graph neural networks (GNNs) have demonstrated remarkable performance in modeling graph data and addressing diverse graph-based tasks, such as node classification (Kipf and Welling 2016; Hamilton, Ying, and Leskovec 2017; Xu et al. 2018; Park, Song, and Yang 2021), link prediction (Zhang and Chen 2018; Yun et al. 2021; Ahn and Kim 2021; Zhu et al. 2021), and graph classification (Lee, Rossi, and Kong 2018; Sui et al. 2022; Hou et al. 2022). Beyond achieving correct prediction, precisely quantifying prediction uncertainty is nontrivial for the reliable utilization of neural networks in downstream decision-making process. Recognizing such need, numerous calibration studies have been actively proposed in vision and language domains (Guo et al. 2017; Mukhoti et al. 2020; Zhang, Kailkhura, and Han 2020; Xing et al. 2019; Jiang et al. 2021; Minderer et al. 2021).

Recently, network calibration has also drawn attention in the field of GNNs (Wang et al. 2021; Hsu et al. 2022; Hsu, Shen, and Cremers 2022; Shi et al. 2022; Wang, Yang, and Cheng 2022; Liu et al. 2022), highlighting neighborhood prediction similarity as a crucial factor for calibration. Contemporary studies in GNN calibration, CaGCN (Wang et al. 2021) and GATS (Hsu et al. 2022), suggest that nodes with similar neighborhood prediction similarity tend to exhibit analogous calibration characteristics. Specifically, CaGCN asserts that nodes with disparate neighbors should ideally have lower confidence levels, as the local message propagation in GNNs makes accurately classifying such instances more challenging. Conversely, GATS elucidates the correlation between neighborhood prediction similarity and calibration errors, indicating the highest errors for nodes with conflicting neighbors. To account for these trends, they incorporate neighborhood similarity into node-wise temperature scaling, facilitating confidence propagation between adjacent nodes.

However, our analysis reveals that calibration cannot be effectively addressed by applying a single, unified trend. Specifically, we observe that calibration errors vary significantly among nodes with comparable neighborhood similarity, depending on their individual confidence levels. More critically, both over-confidence and under-confidence can occur in nodes with similar neighborhood similarity but differing confidence levels. This phenomenon has not been effectively captured in previous studies, as they do not fully account for both factors. Consequently, their assumptions may lead to sub-optimal calibration, as they are not universally applicable.

To address this, we introduce SIMI-MAILBOX, a novel post-hoc calibration method designed to overcome these limitations. Our method categorizes nodes based on both neighborhood representational similarity and confidence, irrespective of proximity or connectivity. This grouping strategy is grounded on our observation that nodes with comparable levels of neighborhood similarity and confidence exhibit similar calibration errors. SIMI-MAILBOX then assigns group-specific temperatures to adjust the predictions of nodes within each group. This fine-grained approach ensures that each group-wise temperature is tailored to address the specific miscalibration of affiliated nodes, instead of relying on a uniform tendency.

In summary, our contributions are three-fold:

• We elucidate the limitations inherent in current calibration methods, particularly concerning neighborhood prediction similarity - a recognized key component for GNN calibration.   
• Given these limitations, we propose SIMI-MAILBOX, a novel calibration method that rectifies miscalibration by introducing group-specific temperatures. Each groupwise temperature is focused on adjusting the predictions of affiliated nodes, rather than scaling all nodes according to a unified trend.   
• We validate the efficacy of SIMI-MAILBOX through comprehensive experiments, incorporating both quantitative and qualitative evaluations.

# Related Works

Uncertainty Quantification and Post-hoc Calibration. Network calibration, while sharing the root in uncertainty quantification with conformal prediction and bayesian methods, concentrates on aligning model predictions with empirical event frequencies, differing from providing intervals around prediction or modeling uncertainty under probability distribution. Conformal prediction aims to generate tight prediction sets that encompass the true outcome with a pre-specified coverage. Its foundational concept was presented in (Vovk, Gammerman, and Shafer 2005), and the study for providing good coverage has been consistently explored, evolving through (Romano, Sesia, and Candes 2020; Cauchois, Gupta, and Duchi 2021; Angelopoulos et al. 2020). Bayesian approaches, on the other hand, use probabilistic modeling to interpret the uncertainty via posterior distribution. Their representative techniques include ensembles (Lakshminarayanan, Pritzel, and Blundell 2017; Wen, Tran, and Ba 2020), dropout (Gal and Ghahramani 2016) and Bayesian Neural Networks (BNNs) for applying Bayesian inference in neural networks (Depeweg et al. 2018; Maddox et al. 2019; Dusenberry et al. 2020).

Distinct from the aforementioned approaches, calibration is focused on refining the trustworthiness of the model prediction. Their goal is focused on adjusting the model’s confidence to match the ground-truth probability. Among diverse calibration techniques, post-hoc calibration methods have found widespread adoption, owing to their computationally efficiency compared to traditional Bayesian approaches and model regularization-based methods (Ma and Blaschko 2021; Jung et al. 2023). Moreover, they imposes no constraints during the pretraining phase of main models, thereby enhancing its versatility across diverse architectures. Techniques such as Platt scaling (Platt et al. 1999), Temperature scaling (TS) (Guo et al. 2017), and Ensemble temperature scaling (ETS) (Zhang, Kailkhura, and Han 2020) have been developed for this purpose, with TS being notably effective for its simplicity and effectiveness in multi-class calibration.

Grouping-based Calibration. Addressing miscalibrations in a group-wise manner has been studied in (He´bertJohnson et al. 2018; Perez-Lebel, Morvan, and Varoquaux 2022; Yang, Zhan, and Gan 2023). (He´bert-Johnson et al. 2018) introduced multicalibration strategy, aiming to achieve calibration within diverse, overlapping subgroups to enhance both fairness and accuracy in machine learning models. Meanwhile, (Perez-Lebel, Morvan, and Varoquaux 2022) presented the concept of grouping loss as a novel metric to assess the variance in true probabilities sharing the same confidence score, challenging existing calibration approaches. (Yang, Zhan, and Gan 2023) proposed a new semantic partitioning approach for neural network calibration and utilized learnable grouping function to refine calibration beyond traditional methods. Nevertheless, these studies do not provide the specific principles for effective categorization, which highlights the distinction of our work from preceding ones. More unique aspects of our approach in comparison to prior works are discussed in the Appendix.

Uncertainty Quantification for GNNs. Recent literature has increasingly focused on quantifying uncertainty in GNNs, with methods ranging from conformal prediction using local topologies (Huang et al. 2024; Zargarbashi, Antonelli, and Bojchevski 2023) to Bayesian approaches (Stadler et al. 2021; Rong et al. 2019; Hasanzadeh et al. 2020; Elinas, Bonilla, and Tiao 2020; Pal, Regol, and Coates 2019; Zhao et al. 2020) that concentrate on the interdependent graph data and GNNs. The literature also highlights post-processing calibration strategies (Wang et al. 2021; Hsu et al. 2022; Hsu, Shen, and Cremers 2022; Wang, Yang, and Cheng 2022; Shi et al. 2022; Liu et al. 2022), with (Wang et al. 2021) pioneering in revealing unexpected underconfidence in GNN predictions. They introduced CaGCN, which employs GCN for node-specific calibration through adjacent predictions. Expanding this, GATS (Hsu et al. 2022) explored factors leading to GNN calibration errors and designed GAT-based node-wise calibration function considering these factors. They further introduced an edge-wise calibration error metric to capture the noniid nature of graphs in (Hsu, Shen, and Cremers 2022). In a different approach, GCL (Wang, Yang, and Cheng 2022) addressed the underconfidence of GNNs by integrating a minimal-entropy regularization with the cross-entropy loss, up-weighting the loss on highly confident nodes.

# Preliminaries

Problem Setup. We focus on calibrating the prediction uncertainty of GNNs for semi-supervised node classification in a post-hoc setting. In this context, uncertainty denotes the model’s confidence level in its predictions, while calibration aims to align this uncertainty with the true accuracy, enhancing the model’s reliability. Thus, our objective is to minimize the gap between the predicted probability and the actual accuracy of given data. During the post-hoc calibration phase, the validation set is used for training to enhance generalization to unseen data, avoiding the overfitting risk associated with reusing the original training set.

Let an undirected graph be denoted as $\mathcal { G } ( \nu , \mathcal { E } )$ , where $\nu$ and $\mathcal { E }$ indicate the sets of vertices and edges respectively. The vertex set $\nu$ is represented by a feature matrix $X =$ $[ \mathbf { x } _ { 1 } ^ { \mathsf { T } } , . . . , \mathbf { x } _ { | \mathcal { V } | } ^ { \mathsf { T } } ] \ \in \ \mathbb { R } ^ { | \mathcal { V } | \times \mathbf { \bar { D } } }$ and the edge set $\mathcal { E }$ is denoted by an adjacency matrix $\pmb { A } \in \mathbb { R } ^ { | \mathcal { V } | \times | \mathcal { V } | }$ . Given the node-wise predictions $\hat { \boldsymbol { y } } = [ \hat { y } _ { 1 } , . . . , \hat { y } _ { | \mathcal { V } | } ] ^ { \mathsf { T } }$ and output confidence $\hat { p } =$ $[ \hat { p } _ { 1 } , . . . , \hat { p } _ { | \mathcal { V } | } ] ^ { \mathsf { T } } \in \mathbb { R } ^ { | \mathcal { V } | }$ from a trained GNN, the GNN $f _ { \theta }$ is well-calibrated if $\hat { p } _ { i }$ for each node $\mathbf { \chi } _ { i }$ accurately serves the ground-truth probability $p _ { \mathrm { t r u e } }$ , formulated as below:

![](images/6bdaffc4e203bc49e2b60657b85ae7524229e94cad47e8e557545488c5d06397.jpg)  
Figure 1: Analysis of uncalibrated and calibrated logits via prior works, CaGCN and GATS. The $x$ -axis divides nodes into sub-intervals based on neighborhood similarity, while the $y$ -axis represents corresponding confidence intervals. Each cell in the heatmap represents the subtraction of the average confidence from the accuracy, with color intensity indicating the magnitude of this discrepancy. Contrary to the uniform assumptions in prior works on neighborhood similarity, the results demonstrate that calibration errors can significantly differ among nodes with comparable neighborhood similarity but different confidence levels. Moreover, prior approaches exhibit sub-optimal calibration across varying neighborhood similarity levels when predictions are extended across confidence intervals.

$$
\mathbb { P } ( \hat { y } _ { i } = y _ { i } | \hat { p } _ { i } = p _ { \mathrm { t r u e } } ) = p _ { \mathrm { t r u e } } , \quad \forall p _ { \mathrm { t r u e } } \in [ 0 , 1 ] .
$$

The expected calibration error (ECE) (Naeini, Cooper, and Hauskrecht 2015) has been recognized as the de facto metric to evaluate the calibration quality of network predictions. ECE groups nodes according to their confidences into $M$ equally partitioned confidence intervals $\{ B _ { 1 } , . . . , B _ { M } \}$ and assesses the expected discrepancy between accuracy and average confidence within individual bins:

$$
\mathrm { E C E } = \sum _ { m = 1 } ^ { M } \frac { | B _ { m } | } { | \mathcal V | } \Big | \mathrm { a c c } ( B _ { m } ) - \mathrm { c o n f } ( B _ { m } ) \Big | ,
$$

where $\left| B _ { m } \right|$ refers to the number of nodes within the $m$ -th interval. Here, the accuracy and average confidence for the $m$ -th bin are defined as $\begin{array} { r } { \overrightarrow { \bf \Phi } \overrightarrow { \bf \Phi } ( B _ { m } ) = \frac { 1 } { | B _ { m } | } \sum _ { i \in B _ { m } } { \bf 1 } [ y _ { i } = \hat { y } _ { i } ] } \end{array}$ and $\begin{array} { r } { \operatorname { c o n f } ( B _ { m } ) = \frac { 1 } { | B _ { m } | } \sum _ { i \in B _ { m } } \hat { p } _ { i } } \end{array}$ , respectively.

Neighborhood Similarity in Prior Studies. The concept of neighborhood similarity has been recognized as a primary element in the field of GNN calibration (Wang et al.

2021; Hsu et al. 2022; Hsu, Shen, and Cremers 2022; Liu et al. 2022). Among them, CaGCN (Wang et al. 2021) advocates that given the challenges GNNs encounter in accurately classifying nodes with conflicting neighbors, the confidence levels in such cases should ideally remain still or decrease. Conversely, confidence for nodes linked to agreeing nodes should elevate, addressing the prevalent underconfidence in GNNs. Stemmed from this insight, they employ GCN (Kipf and Welling 2016) as a node-wise calibration function to propagate the confidence to neighboring counterparts. In parallel, GATS (Hsu et al. 2022) underscores the correlation between neighborhood prediction similarity and calibration error, demonstrating an increment in error with a decrement in similarity. This relationship is incorporated into the normalized attention coefficients within their GAT (Velicˇkovic´ et al. 2017)-founded node-level temperature function.

# In-depth Analysis on Neighborhood Similarity

In this section, we provide a comprehensive analysis of both uncalibrated and calibrated predictions from existing studies, CaGCN and GATS, using the CoraFull dataset (Bojchevski and Gu¨nnemann 2017). Leveraging GCN as the backbone architecture, we first partition nodes into 10 equal intervals $\{ B ^ { ( 1 ) } , . . . , B ^ { ( 1 0 ) } \}$ based on the proportion of neighbors sharing the same predicted labels, denoted as neighborhood prediction similarity $\begin{array} { r } { \pmb { s } ( i ) } \end{array}$ :

$$
\mathbf { \boldsymbol { s } } ( i ) = \frac { \sum _ { j \in \mathcal { N } _ { i } } \mathbf { 1 } [ \hat { y } _ { i } = \hat { y } _ { j } ] } { | \mathcal { N } _ { i } | } ,
$$

where ${ \mathcal { N } } _ { i }$ represents the set of neighbors associated with node $i$ . For each subgroup $B ^ { ( l ) }$ , we calculate the calibration error as the discrepancy between their average confidence and the accuracy, i.e., $\operatorname { a c c } ( B ^ { ( l ) } ) - \operatorname { c o n f } ( B ^ { ( l ) } )$ . These discrepancies are depicted as heatmap bars in the second row of Figure 1.

Furthermore, we also analyze predictions by considering both neighborhood similarity and confidence. We begin by grouping confidence into 10 equal intervals $\{ B _ { 1 } , . . . , B _ { 1 0 } \}$ , and then further categorize nodes within each confidence interval into 10 equal-width intervals based on $s ( i )$ . The subgroup within the $l$ -th similarity interval and $m$ -th confidence interval is denoted as $B _ { m } ^ { ( l ) }$ . For each subgroup $B _ { m } ^ { ( l ) }$ , the calibration error is computed as $\operatorname { a c c } ( B _ { m } ) - \operatorname { c o n f } ( B _ { m } ^ { ( l ) } )$ . These discrepancies are illustrated as heatmap matrices in the last row of Figure 1, with $\hat { p }$ representing uncalibrated confidence and $\widetilde { p }$ representing calibrated confidence.

IneFigure 1, the heatmap elements represent the differences between accuracy and average confidence. Deeper shades of red indicate that the calibrated confidence is lower than the accuracy (under-confident), while deeper shades of green indicate that confidence exceeds the accuracy (overconfident). Our findings show that calibration errors can vary significantly among nodes with the same level of neighborhood similarity but different confidence. Notably, both under- and over-confidence are observed in uncalibrated predictions within $\pmb { s } ( i ) \in ( 0 . 1 , 1 . 0 ]$ similarity intervals of the heatmap matrices.

Moreover, our analysis reveals that existing methods, which apply a unified policy to nodes with similar levels of neighborhood similarity, fail to achieve consistent calibration across diverse neighborhood similarity levels. While these methods may appear well-calibrated according to the heatmap bars in the second row, they demonstrate suboptimal results when their predictions are extended across confidence intervals. Specifically, CaGCN exhibits severe underconfidence in $\widetilde { p } _ { i } ~ \in ~ ( 0 . 9 , 1 . 0 ]$ confidence interval, with a maximum dis re pancy of approximately $1 6 . 3 4 \%$ within the $\pmb { s } ( i ) \in ( 0 . 1 , 0 . 2 ]$ similarity range. GATS, on the other hand, demonstrates suboptimal calibration in regions of low prediction similarity, particularly in the $\widetilde { p _ { i } } \in \ : \left( 0 . 2 , 0 . 4 \right]$ and $\widetilde { p } _ { i } \in ( 0 . 6 , 0 . 8 ]$ ranges, where the ave ege discrepancies are $7 . 4 5 \%$ and $7 . \dot { 1 } 7 \%$ in the $s ( i ) \in ( 0 , 0 . \bar { 4 } ]$ intervals, respectively. Hence, our observations suggest that a unified assumption to calibrating predictions based on neighborhood similarity cannot effectively achieve fine-grained calibration. We also provide an algorithmic perspective on the limitations of previous work, along with additional investigation results on more benchmark datasets, in the Appendix.

# Proposed Method

Given the limitation of earlier studies, we introduce SIMIMAILBOX, a post-hoc calibration method designed to rectify miscalibration in GNNs across varying levels of neighborhood similarity. Building on our novel observation, SIMIMAILBOX categorizes nodes based on both neighborhood similarity and confidence levels, ensuring that nodes within the same cluster exhibit similar calibration errors. Subsequently, our method employs group-specific temperature scaling to adjust the predictions of nodes in the designated cluster. These group-wise temperatures are tailored to correct the specific miscalibration associated with each group, instead of relying on a uniform tendency. The temperatures are optimized by directly minimizing the discrepancy between average confidence and accuracy within each cluster.

Intuition: Topology Grouping Matters   
Table 1: Variance of calibration errors $\mathbf { \left( \times 1 0 0 \right) }$ involving neighborhood similarity sub-intervals (Neig. Sim.), confidence intervals (Conf), and total nodes (Node-wise).   

<html><body><table><tr><td colspan="2">GNNs</td><td>Cora</td><td>Citeseer</td><td>Pubmed</td><td>Computers</td><td>Photo</td></tr><tr><td>GCN</td><td>Node-wise Conf.</td><td>6.139 0.065</td><td>1.957 0.060</td><td>1.370 0.068</td><td>40.370 0.060</td><td>7.200 0.052</td></tr><tr><td>GAT</td><td>Conf.</td><td>0.068</td><td>0.062</td><td>0.068</td><td>44.980 0.048</td><td>14.550 0.053</td></tr></table></body></html>

For effective group-wise calibration, it is essential to categorize nodes in a manner that ensures they share a similar degree of miscalibration. This allows each group’s temperature to be precisely tailored to address specific miscalibration levels rather than applying a broad, generalized adjustment. To this end, we present a novel observation suggesting that nodes with similar neighborhood prediction similarity $s ( i )$ and confidence $\hat { p } _ { i }$ share similar magnitudes of calibration errors. To substantiate this, we evaluate the variance of calibration errors under three different scenarios: (1) nodewise variance involving all nodes (specified as Node-wise), (2) variance within each confidence interval (specified as Conf.), and (3) variance within each neighborhood similarity sub-interval within each confidence interval (specified as Neig. Sim.).

To explore the third scenario, we assess the variability in calibration errors across neighborhood similarity intervals within each confidence interval. Let $B _ { m } ^ { ( l ) }$ represent the set of nodes in $l .$ -th neighborhood similarity interval and $m$ -th confidence interval. The calibration error for each node $i$ , defined as the absolute difference between its confidence and the accuracy associated with its confidence interval, is denoted as $D ( i )$ . We first calculate the variance of calibration error within each $B _ { m } ^ { ( l ) }$ , denoted as $V ( B _ { m } ^ { ( l ) } )$ :

$$
\begin{array} { l } { \displaystyle V ( B _ { m } ^ { ( l ) } ) = \frac { 1 } { | B _ { m } ^ { ( l ) } | - 1 } \sum _ { i \in B _ { m } ^ { ( l ) } } ( D ( i ) - \bar { D } _ { m } ^ { ( l ) } ) ^ { 2 } , } \\ { \displaystyle D ( i ) = | \mathrm { A c c } ( B _ { m } ) - \hat { p } _ { i } | . } \end{array}
$$

where $\bar { D } _ { m } ^ { ( l ) }$ represents the mean calibration error for nodes in $B _ { m } ^ { ( l ) }$ . We then average these variances over the collection ${ \cal B } ^ { \mathrm { s i m } } = \{ B _ { 1 } ^ { ( 1 ) } , B _ { 1 } ^ { ( 2 ) } , . . . , B _ { 2 } ^ { ( 1 ) } , B _ { 2 } ^ { ( 2 ) } , . . . \}$ , which incorporates all $B _ { m } ^ { ( l ) }$ spanning the entire confidence intervals:

$$
V ^ { \mathrm { s i m } } = \frac { 1 } { | B ^ { \mathrm { s i m } } | } \sum _ { B _ { m } ^ { ( l ) } \in B ^ { \mathrm { s i m } } } V ( B _ { m } ^ { ( l ) } ) .
$$

Similarly, to assess the second scenario, we calculate the variability in calibration errors across confidence intervals $B ^ { \mathrm { c o n f } } \ = \ \stackrel { \cdot } { \{ } B _ { 1 } , B _ { 2 } , \ldots \}$ by computing the variance within each $B _ { m }$ :

$$
V ( B _ { m } ) = \frac { 1 } { | B _ { m } | - 1 } \sum _ { i \in B _ { m } } ( D ( i ) - \bar { D } _ { m } ) ^ { 2 } ,
$$

where $\hat { D } _ { m }$ refers to the average calibration error for nodes within $B _ { m }$ . Following the approach used in GATS, we conceptualize node-wise calibration error as the calibration error of the confidence interval to which each node belongs. Consequently, the variance in calibration error related to individual nodes (the first scenario) is defined as the variance of all node-wise calibration errors.

As outlined in Table 1, the variance within Neig. Sim. shows the lowest, particularly when compared to the variance across all nodes (Node-wise). This demonstrates that nodes with comparable neighborhood predictions and confidence levels exhibit similar calibration error.

# SIMI-MAILBOX: A Topology-Grouping Strategy for Refining GNNs

Building on the observation discussed in previous section, SIMI-MAILBOX categorizes nodes by considering both neighborhood similarity and confidence levels. We estimate the neighborhood similarity for each node $i$ by computing the average representational similarity with its neighbors, denoted as MAILBOX $\mathcal { M } ^ { s i m i } ( i )$ :

$$
\mathcal { M } ^ { s i m i } ( i ) = \frac { 1 } { | \mathcal { N } _ { i } | } \sum _ { j \in \mathcal { N } _ { i } } \sigma ( z _ { i } ^ { \mathsf { T } } z _ { j } ) ,
$$

where $z _ { i }$ represents the output logits for node $i$ from trained GNN, and $\sigma$ is a sigmoid function. Nodes with similar MAILBOX values and confidence levels are then grouped into $N$ distinct clusters. More precisely, SIMI-MAILBOX constructs a feature vector $F _ { i } ^ { s i \ r { \ r { \dot { m } } } { \ r { i } } } = [ \bar { p _ { i } } , \ \bar { \mathcal { M } } ^ { s i m i } ( i ) ] ^ { \ r { \ r { \mathsf { I } } } }$ for each node $i$ , with the first dimension representing normalized confidence ${ \bar { p } } _ { i }$ and the second dimension representing a normalized MAILBOX value via min-max scaling. Subsequently, KMeans clustering is applied to $F ^ { s i m i }$ to construct $N$ similarity-based clusters $C = \{ C _ { 1 } , . . . , C _ { N } \}$ , ensuring the categorization adheres to both neighborhood similarity and confidence.

Once the categorization is completed, the original predictions for nodes within each cluster $C _ { n }$ are scaled by a groupspecific temperature ${ \pmb T } _ { n }$ , a learnable parameter designed to rectify the miscalibration within the $n$ -th cluster:

$$
\widetilde { p } _ { i } = \operatorname* { m a x } _ { k } \sigma _ { \mathrm { s m } } \left( \frac { z _ { i } } { T _ { n } } \right) _ { k } \in \mathbb { R } , \quad i \in C _ { n } .
$$

The group-wise temperature $\pmb { T } \in \mathbb { R } ^ { N }$ is then optimized with a new loss $\mathcal { L } _ { s i m i }$ with standard cross-entropy loss $\mathcal { L } _ { \mathrm { C E } }$ :

$$
\begin{array} { r l } & { \mathcal { L } = \mathcal { L } _ { \mathrm { C E } } + \lambda \mathcal { L } _ { s i m i } , } \\ & { \mathcal { L } _ { s i m i } = \displaystyle \sum _ { n = 1 } ^ { N } | | a _ { v a l } ^ { ( n ) } - \frac { 1 } { | C _ { n } | } \sum _ { i \in C _ { n } } \widetilde { p } _ { i } | | ^ { 2 } , } \end{array}
$$

where $\lambda$ is a scaling factor for $\mathcal { L } _ { s i m i }$ . During calibration, $\mathcal { L } _ { \mathrm { C E } }$ encourages the reduction of entropy for correctly predicted classes while increasing it for the incorrectly predicted ones. In parallel, $\mathcal { L } _ { s i m i }$ minimizes the discrepancy between the average scaled confidence of all nodes and the accuracy of validation nodes $a v a l ^ { ( n ) }$ within each cluster. This approach directly adjusts the group-specific temperatures, with each ${ \pmb T } _ { n }$ focused on minimizing the corresponding level of miscalibration.

On Accuracy Preservation. The post-hoc group-wise temperatures in SIMI-MAILBOX ensures that the relative ordering of predictions remains unchanged. Let $f : \mathbb { R } ^ { K } $ $\mathbb { R } ^ { K }$ as a calibration function and $z _ { i } = [ z _ { i 1 } , z _ { i 2 } , . . . , z _ { i K } ] ^ { \mathsf { T } }$ represents the logit vector for node $i$ . We denote the groupspecific temperature for the group to which node $i$ belongs as $\scriptstyle { \pmb { T } } _ { g _ { i } }$ . Since the group-wise temperature $\scriptstyle { \pmb { T } } _ { g _ { i } }$ is uniformly applied to all elements of $z _ { i }$ , the order between elements in the calibrated logit $f _ { g } \big ( z _ { i } \big )$ remains unchanged when subjected to the softmax operation σsm.

$$
\begin{array} { r l } & { f _ { g } ( z _ { i } ) = [ z _ { i 1 } / \mathbf { T } _ { g _ { i } } , z _ { i 2 } / \mathbf { T } _ { g _ { i } } , . . . , z _ { i K } / \mathbf { T } _ { g _ { i } } ] , } \\ & { \breve { p } _ { i } = \sigma _ { \mathrm { s m } } \left( f _ { g } ( z _ { i } ) \right) . } \end{array}
$$

Thus, our method preserves the original classification accuracy, as the softmax function is order-preserving and scaling by $\pmb { T } _ { g _ { i } }$ does not alter the relative ranking of logits.

Comparison with Prior Studies. While our work shares the post-hoc temperature scaling framework with previous GNN calibration methods, SIMI-MAILBOX introduces group-specific temperatures independent of node proximity or connectivity, thereby capturing high-level miscalibration patterns. Our method enables a more efficient optimization via $\mathcal { L } _ { s i m i }$ due to its simplified number of parameters, compared to CaGCN and GATS requiring distinct temperatures for individual nodes. Moreover, the key distinction of our method lies in our discovery that nodes with similar neighborhood prediction similarity and confidence exhibit comparable calibration errors. This insight has not been explored in prior studies, as they do not fully consider the interplay between neighborhood similarity and confidence.

# Experiments

We validate the effectiveness of the proposed method under extensive experiments, leveraging two representative GNN architectures: GCN (Kipf and Welling 2016) and GAT (Velicˇkovic´ et al. 2017). The performance of our SIMIMAILBOX is evaluated across eight small- and mediumscale benchmark graphs adopted in (Hsu et al. 2022): Cora, Citeseer, Pubmed (Sen et al. 2008), CoraFull (Bojchevski and Gu¨nnemann 2017), Coauthor CS, Computers, and Photo (Shchur et al. 2018). To further demonstrate the versatility, we extended our experiments to large-scale graphs, Arxiv (Hu et al. 2020) and Reddit (Zeng et al. 2019). More experiments including comparison with recent baselines, evaluations on heterophilous graphs and other GNN backbones, and hyperparameter robustness are provided in the Appendix.

Table 2: ECE results (reported in percentage) for our proposed calibration method and baselines. A lower ECE indicates bette calibration performance. The best and second best performances are represented by bold and underline texts.   

<html><body><table><tr><td colspan="2">Methods</td><td>UnCal.</td><td>TS</td><td>VS</td><td>ETS</td><td>CaGCN</td><td>GATS</td><td>Ours</td></tr><tr><td rowspan="2">Cora</td><td>GCN</td><td>12.43</td><td></td><td></td><td></td><td></td><td></td><td>.97 ± 0.44</td></tr><tr><td>GAT</td><td>14.88± 4.3</td><td>42</td><td></td><td></td><td></td><td></td><td>2.08 ± 0.45</td></tr><tr><td rowspan="2">Citeseer</td><td>GCN</td><td>12.54</td><td></td><td></td><td></td><td></td><td></td><td>0.53</td></tr><tr><td>GAT</td><td>16.65±</td><td></td><td></td><td></td><td></td><td></td><td>0.56</td></tr><tr><td rowspan="2">Pubmed</td><td>GCN</td><td>7.30</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GAT</td><td>10.38 ± 1.89</td><td></td><td></td><td></td><td></td><td></td><td>0.16</td></tr><tr><td rowspan="2">Computers</td><td>GCN</td><td>2.96±</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GAT</td><td>1.58 ± 0.56</td><td></td><td></td><td></td><td></td><td></td><td>0.37</td></tr><tr><td rowspan="2">Photo</td><td>GCN</td><td>2.11 ± 0.97</td><td>68</td><td></td><td></td><td></td><td></td><td>0.36</td></tr><tr><td>GAT</td><td>2.18 ± 1.54</td><td>1.56±0.63</td><td></td><td></td><td></td><td>6</td><td>9 士 0.53</td></tr><tr><td rowspan="2">CS</td><td>GCN</td><td>1.72 ± 1.28</td><td>01</td><td></td><td></td><td></td><td></td><td>0.19</td></tr><tr><td>GAT</td><td>1.48 ± 0.79</td><td>1.07±0.34</td><td>.01 ±0.40</td><td>03</td><td></td><td></td><td>.72±0.43</td></tr><tr><td rowspan="2">Physics</td><td>GCN</td><td>0.56 ± 0.33</td><td>0.51 十 0</td><td>.46</td><td></td><td></td><td></td><td>± 0.11</td></tr><tr><td>GAT</td><td>0.55 ± 0.24</td><td>0.56±0.20</td><td>.56 ±0.2</td><td></td><td>.06±0.40</td><td></td><td>0.48 ± 0.22</td></tr><tr><td rowspan="2">CoraFull</td><td>GCN</td><td>6.49 ± 1.28</td><td>5.55</td><td></td><td></td><td></td><td>0.63</td><td>3.46±1.31</td></tr><tr><td>GAT</td><td>5.25 ± 1.32</td><td>4.41</td><td>0.49 .42</td><td>.36</td><td>6.80</td><td></td><td>2.64 ± 1.02</td></tr></table></body></html>

Table 3: ECE results (in percentage) for our method and baselines on large-scale datasets.   

<html><body><table><tr><td colspan="2">Methods</td><td>UnCal.</td><td>CaGCN</td><td>GATS</td><td>Ours</td></tr><tr><td>Arxiv</td><td>GCN SAGE</td><td>4.92 ±0.36 3.00 ±0.89</td><td>1.97 ± 0.16 1.84 ± 0.19</td><td>0.75 ± 0.06 2.05 ±0.28</td><td>0.71 ± 0.13 0.98 ± 0.23</td></tr><tr><td>Reddit</td><td>GCN SAGE</td><td>8.55 ± 1.28 11.30 ± 1.99</td><td>1.86 ± 0.19 2.14 ± 0.35</td><td>2.56± 0.59 4.66 ± 0.57</td><td>0.35 ± 0.05 0.73 ± 0.15</td></tr></table></body></html>

Baselines. In alignment with precedent studies, we compare our method against classical calibration methods: temperature scaling (TS), vector scaling (VS) (Guo et al. 2017), and ensemble temperature sclaing (ETS) (Zhang, Kailkhura, and Han 2020) and GNN-specialized calibration baselines: CaGCN (Wang et al. 2021) and GATS (Hsu et al. 2022). We provide an additional experiments to compare SIMIMAILBOX and GPN (Stadler et al. 2021) and GNNSafe (Wu et al. 2023) for out-of-detection task in the Appendix.

Experimental Setup. We undertake our experiments following the experimental protocols of GATS (Hsu et al. 2022) in the scope of semi-supervised node classification. Details of the experiment configurations are provided in the Appendix. To assess the calibration performance, we use ECE as a principal metric (Naeini, Cooper, and Hauskrecht 2015), following the common practice (Hsu et al. 2022). The optimal calibration models are chosen based on the lowest validation ECE on training set. Additional calibration metrics, including class-wise ECE (Kull et al. 2019; Nixon et al. 2019), Kernel Density Estimation-based ECE (Zhang, Kailkhura, and Han 2020), Brier Score (Brier et al. 1950), and Negative Log-likelihood, are provided in the Appendix.

Results on Small- and Medium-scale Graphs. Table 2 shows that SIMI-MAILBOX outperforms baselines in 15 of 16 settings. Notably, our method pioneers in achieving an error rate below $3 \%$ on Cora and Citeseer datasets, with a significant lead on Cora using GCN, breaking into the $1 \%$ error range. SIMI-MAILBOX also demonstrates marked improvements on Pubmed and CS datasets, first achieving ECE reductions to within the [0.5, 0.8] range. Even on Computers and Photo datasets, where the original predictions are already well-calibrated, SIMI-MAILBOX further reduces calibration errors to below $1 \%$ with GAT. Additionally, consistent improvement is observed on the CoraFull dataset, with our method achieving the first $2 \%$ error range using GAT.

Table 4: Calibration duration (in seconds) for our method and baselines on large-scale datasets.   

<html><body><table><tr><td colspan="2">Methods</td><td>CaGCN</td><td>GATS</td><td>Ours</td></tr><tr><td>Arxiv</td><td>GCN SAGE</td><td>20.84 ± 2.69 23.02 ± 4.44</td><td>48.89 ± 11.39 61.67 ± 16.89</td><td>7.10 ± 0.94 (-41.79 sec) 4.85 ± 0.65 (-56.82 sec)</td></tr><tr><td>Reddit</td><td>GCN SAGE</td><td>55.98 ± 13.76 78.13 ± 27.35</td><td>72.90 ± 19.98 192.01 ± 177.57</td><td>11.04 ± 0.30 (-61.86 sec) 9.91 ± 0.95 (-182.1 sec)</td></tr></table></body></html>

Results on Large-scale Graphs. To further demonstrate the versatility of our method, we extended our experiments to large-scale graphs, following the evaluation protocol in (Hu et al. 2020). We employed GCN and GraphSAGE (SAGE)(Hamilton, Ying, and Leskovec 2017), which are representative architectures for large-scale benchmark datasets. As shown in Table3, SIMI-MAILBOX outperforms all baselines to a considerable extent, achieving an error rate below $1 \%$ in all examined settings. This superiority is particularly notable in the Reddit dataset with SAGE, where our method reduces miscalibration by $1 0 . 5 7 \%$ compared to the uncalibrated baseline. In addition to calibration performance, we also measured the total execution time for each run, as presented in Table 4. Our method significantly improves time efficiency across all experiments, with a notable reduction in execution time on the Reddit dataset, decreasing by 61.86 and 182.10 seconds with GCN and SAGE. This gain is attributed to the simplified group-wise temperature approach, which allows for rapid optimization with only few parameters ( $N$ clusters), in contrast to baselines that rely on complex GNNs for deriving node-wise temperature.

![](images/e19367cea4ca68e87070c9d90b51163106e62d056de795b31cc412c7ad65f6e7.jpg)  
Figure 2: Qualitative analysis of our calibration results on CoraFull dataset, compared with CaGCN and GATS. Each cell in the heatmap represents the subtraction of the average confidence of calibrated nodes from the accuracy, with color and intensity indicating the magnitude of this discrepancy. Throughout diverse neighborhood similarity levels, our method facilitates a better reduction in the gap between accuracy and confidence compared to baselines.

SIMI-MAILBOX on Self-training. In addition to improving calibration, calibrated predictions can be applied in self-training, utilizing pseudo-labels generated from unlabeled samples. As evidenced by (Rizve et al. 2021), poorlycalibrated models have a risk to choose pseudo-labeled samples with high confidence but incorrect classifications. Hence, confidence adjusted through calibration methods can lead to the selection of more accurate and high-confidence samples, improving classification accuracy. We broaden our evaluation of SIMI-MAILBOX to self-training scenarios, initially explored in CaGCN. Adhering to the same evaluation protocol in (Wang et al. 2021), we validate the effectiveness of our method in generating qualified pseudo-labels over baselines. Detailed results of this experiment are provided in the Appendix.

Effectiveness on Diverse Neighborhood Topology. To further validate the effectiveness of our method across different levels of neighborhood similarity, we present a qualitative comparison in Figure 2, utilizing a consistent dataset (CoraFull) and architecture (GCN) in preceding section. Similar to earlier analyses, the $\mathbf { X }$ -axis partitions nodes into intervals based on neighborhood prediction similarity, while the y-axis categorizes them by confidence intervals. Each cell in the heatmap represents the subtraction of average confidence of calibrated nodes from the accuracy. Deeper shades of red indicate that calibrated confidence is lower than accuracy (under-confident), while deeper shades of green signify that confidence exceeds accuracy (overconfident). Ideally, a perfectly calibrated model would produce a uniformly white heatmap, indicating perfect alignment between confidence and accuracy. As illustrated, SIMIMAILBOX significantly reduces the discrepancy between accuracy and average confidence across varying similarity levels compared to baseline methods. This improvement is particularly evident in the patterns identified in the previous analysis, where our method mitigates discrepancies in the $\pmb { s } ( i ) \in ( 0 . 1 , 0 . 2 ]$ range within the $\hat { p } _ { i } \in ( 0 . 9 , 1 . 0 ]$ interval for CaGCN, and addresses the prevalent under-confidence observed with GATS in the $\pmb { s } ( \bar { i } ) \in ( 0 . 0 , 0 . 4 ]$ range within the $\hat { p } _ { i } \in ( 0 . 6 , 0 . 8 ]$ intervals.

# Conclusion

In this study, we presented a novel analysis that identifies the limitations of uniform design principles in existing GNN calibration methods, particularly based on neighborhood similarity. To address these limitations, we proposed SIMI-MAILBOX, a novel calibration method that employs group-specific temperatures to refine miscalibration in nodes categorized by both neighborhood similarity and confidence. Comprehensive experiments have demonstrated the effectiveness of SIMI-MAILBOX, supported by extensive empirical and technical analysis. As for future work, we are dedicated to developing a theoretical foundation for our method.