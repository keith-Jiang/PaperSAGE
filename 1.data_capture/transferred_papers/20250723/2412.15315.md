# Enhancing Masked Time-Series Modeling via Dropping Patches

Tianyu Qiu1, Yi Xie1\*, Hao Niu1, Yun Xiong1\*, Xiaofeng Gao2

1Shanghai Key Lab of Data Science, School of Computer Science, Fudan University, Shanghai, China 2MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University, Shanghai, China tyqiu22, yixie18, hniu18, yunx @fudan.edu.cn gao-xf@cs.sjtu.edu.cn

# Abstract

This paper explores how to enhance existing masked timeseries modeling by randomly dropping sub-sequence level patches of time series. On this basis, a simple yet effective method named DropPatch is proposed, which has two remarkable advantages: 1) It improves the pre-training efficiency by a square-level advantage; 2) It provides additional advantages for modeling in scenarios such as in-domain, cross-domain, few-shot learning and cold start. This paper conducts comprehensive experiments to verify the effectiveness of the method and analyze its internal mechanism. Empirically, DropPatch strengthens the attention mechanism, reduces information redundancy and serves as an efficient means of data augmentation. Theoretically, it is proved that DropPatch slows down the rate at which the Transformer representations collapse into the rank-1 linear subspace by randomly dropping patches, thus optimizing the quality of the learned representations.

Extended version — https://arxiv.org/abs/2412.15315

# Introduction

In recent years, masked modeling has emerged as a prevalent self-supervised method in various fields, including natural language processing (Devlin et al. 2018; Liu et al. 2019) and computer vision (Baevski et al. 2022; He et al. 2022; Bao et al. 2021). This technique improves representation learning by reconstructing masked content based on unmasked parts. Masked modeling has also been adapted for timeseries analysis. A notable advancement involves segmenting time-series into patches (sub-sequence) and applying a patch-level masking strategy, which has received considerable attention since its inception (Nie et al. 2022). This method not only shows promising performance in transfer learning, but also significantly enhances supervised forecasting by employing self-supervised pre-training to initialize model parameters, consistent with recent findings (Amos, Berant, and Gupta 2023). Building upon the patching technique, numerous time-series foundation model works have emerged and achieve significant performance in time-series forecasting (Goswami et al. 2024; Woo et al. 2024).

Despite its potential, we observed that masked time-series modeling, represented by PatchTST (Nie et al. 2022), faces a dilemma. A relatively low mask ratio reduces effectiveness in learning useful features (He et al. 2022; Zhang, Wang, and Wang 2022). Given the characteristic of periodicity and repetitive pattern of time-series data, the masked patch can be recovered with little high-level understanding of the underlying patterns, leading to superficial learning and overfitting as shown in Figure 1 (A). A natural idea is to increase the mask ratio, but another issue emerges: the presence of an excessive number of masked patches can further dilute the attention mechanism’s capacity to concentrate on the relevant and informative parts of data, termed as scattered attention as shown in Figure 1 (C). It can lead to the degradation of downstream task performance as the representations gradually lose their distinctiveness (Noci et al. 2022; Dong, Cordonnier, and Loukas 2021; Zhai et al. 2023).

We introduce a simple yet effective strategy, DropPatch, to encourage learning useful features and improve the overall performance. Building on foundational time-series pretraining techniques (Nie et al. 2022), DropPatch randomly removes a predefined proportion of patches. The remaining patches are subsequently processed for masking and reconstruction. It is crucial to distinguish between dropping and masking in the context of pre-training. For a given timeseries sample, the dropping operation is applied prior to masking and reconstruction. Removed patches are entirely excluded from all training steps during the current epoch. In contrast, masked patches, represented as zero tensors overlaid with positional encoding, remain part of the training process throughout the epoch.

In our empirical study, DropPatch demonstrates clear advantages in mitigating over-fitting (Figure 1 (B)), enhancing attention focus (Figure 1 (C)), and improving forecasting performance (Figure 1 (D)). The reduction in the number of patches due to the dropping operation leads to significant improvements in computational efficiency and reduced memory consumption.

Extensive experiments validate the effectiveness of DropPatch. Through detailed experimental analysis, we uncover the underlying mechanisms driving these improvements. The DropPatch strategy enhances the attention mechanism by enabling a sharper focus on multi-scale and diverse information. It strengthens the model’s ability to capture critical patterns while reducing redundancy in representation. Furthermore, our theoretical findings indicate that the random dropping of patches effectively slows the convergence of the Transformer’s representations toward a rank-1 linear subspace, thereby promoting the feature diversity.

![](images/d62ec47180ea798088adf280aec22b8cb201e5b335ac8c7ba1a2a218fe35bdbf.jpg)  
Figure 1: (A) The loss curve of PatchTST with mask ratio 0.4 (official implementation); (B) The loss curve of DropPatch (unless otherwise stated, the drop ratio and mask ratio is 0.6 and 0.4 throughout this paper); (C) The Kullback-Leibler (KL) divergence between the attention coefficients of the final encoder layer and a uniform distribution, where each dot represents an individual attention head. A larger KL divergence indicates that this set of attention distributions is farther from a uniform distribution and thus more focused. PatchTST(0.78) refers to the PatchTST configured with a mask ratio of 0.78, matching the number of visible patches in DropPatch. (D) Comparison of MSE metrics between PatchTST and DropPatch with forecasting steps $T \in \{ 9 6 , 7 2 0 \}$ on ETTm1.

Overall, our contributions can be summarized as follows:

• We introduce DropPatch, a simple yet effective strategy that enhances masked time-series modeling. • Extensive experiments demonstrate that the DropPatch strategy improves pre-training efficiency and delivers substantial performance gains across diverse downstream tasks. Additionally, we compile comprehensive synthesized datasets to evaluate its role as a core component in foundational models for time-series analysis. • Through rigorous empirical and theoretical analysis, we validate the effectiveness of DropPatch and provide insights into the mechanisms driving these improvements.

# Method

In this section, we describe the details of our proposed pretraining method, DropPatch, as shown in Fig. 2. It is worth noting that DropPatch is a strategy applied during the pretraining stage, and the model does not perform the dropping operation during the fine-tuning stage.

# Patching and Channel-Independence

For each sample of multivariate time-series X ∈ RL×C, where $L$ represents the length of time-series, and $C$ denotes the number of channels (variates). We first split the entire time-series sample into non-overlapping subseries-level patches, which are served as input tokens to Transformer, like PatchTST (Nie et al. 2022). We permute the original data of time-series into $\boldsymbol { \mathcal { X } } \in \mathbb { R } ^ { \boldsymbol { C } \times \boldsymbol { P } \times \boldsymbol { L } _ { P } ^ { \texttt { \bullet } } }$ , where $L _ { P }$ denotes the length of each subseries-level patch, and $P$ denotes the total number of patches.

# Dropping Patches

After the patching operation, we will first conduct the positional encoding for these patches. The positional encoding process is designed to preserve the positional information during the self-attention computation and following the dropping operation. It should be noted that the positional encoding of each token is computed prior to dropping operation, ensuring that the original sequence position of each token is maintained after the removal.

We randomly drop patches in the patched time-series, which is the core idea of our proposed DropPatch. Let $r$ denotes the ratio of dropping with condition $0 \leq r \leq 1$ , implying that only $( 1 - r ) P$ patches remains for further training and others will be directly absent in the subsequent operations. Formally, the remained patches and positional encoding will be denoted as ¯ RC×(1−r)P ×LP , $\overline { { P E } } \in \mathbb { R } ^ { C \times ( 1 - r ) } P ^ { \times d _ { m o d e l } }$ .

# Representation Learning

Subsequently, a patch-level random masking strategy is applied to generate masked data, the resultant masked data can be expressed as $\bar { \mathcal { X } } _ { m a s k e d } \in \mathbb { R } ^ { C \times ( 1 - r ) P \times L _ { P } }$ . Given a mask ratio $m \in [ 0 , 1 ]$ , we denote that the number of masked patches is $( 1 - r ) m P$ .

The masked data is then embedded, and the previously dropped positional encodings are added back to these embeddings to formulate the encoder input $\mathbf { E }$ . After the encoder, we can obtain the representation $\mathbf { Z }$ of the input series which can be formalized as:

$$
{ \bf E } = E m b e d ( \bar { \mathcal { X } } _ { m a s k e d } ) + \overline { { P E } } ,
$$

$$
\mathbf { Z } = E n c o d e r ( \mathbf { E } ) ,
$$

where $\mathbf { E } , \mathbf { Z } \in \mathbb { R } ^ { C \times ( 1 - r ) P \times d _ { m o d e l } }$ . Finally, the representation $\mathbf { Z }$ is fed into a reconstruction head to obtain the reconstruction results $\hat { X } \in \mathbb { R } ^ { C \times ( 1 - r ) P \times L _ { P } }$ . In the implementation, we simply adopt a linear layer as the head. We choose to use the Mean Squared Error (MSE) loss to mesure the reconstruction and the ground truth. Only the reconstructions on the masked patches are considered in the loss.

![](images/1ae737b4c6b7193f2f9a09c4dc32f0118e13634de5c8a19fd46037f774713667.jpg)  
Figure 2: The overall pre-training framework of DropPatch.

Here, we present a corollary to describe from the perspective of representation space why DropPatch is effective, which will be validated through both experimental and theoretical approaches in the following text. The theoretical analysis is provided in the Appendix.

Lemma 1. Let SAN denote a self-attention layer, and consider stacking $L$ such layers. Then, under certain conditions, the representations within the stacked self-attention layers will converge to a rank-1 matrix as $L \to \infty$ .

Corollary 1. The DropPatch strategy effectively slows down the rate at which the representation matrix of a Transformer degenerates into a rank-1 matrix.

# Experiments

We evaluate the effectiveness of our proposed method on time-series forecasting tasks under various setups, including in-domain, cross-domain, few-shot, cold-start, and multidataset pre-training scenarios. It is worth noting that we maintain consistent drop ratio and mask ratio to be fixed across various tasks and datasets, demonstrating the effectiveness and robustness of our approach. Our proposed DropPatch1 exhibits significant improvement over other established strong baselines in various time-series forecasting scenarios, while enjoying the computational efficiency and reduced memory usage.

Datasets We evaluate performance of our proposed method DropPatch on 12 popular datasets. For in-domain, cross-domain and few-shot experiments, Weather, ECL, Traffic and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2) are included. In addition, we incorporate Exchange and PEMS dataset for cold start scenario in crossdomain transfer learning. All datasets are available on (Wu et al. 2021) (Liu et al. 2022). Moreover, we compile two synthesized datasets to conduct multi-dataset pre-training (Goswami et al. 2024), demonstrating the potential of DropPatch strategy in time-series foundation model.

Implementation We choose seven competitive selfsupervised baseline methods, including the masked modeling method: PatchTST (Nie et al. 2022), SimMTM (Dong et al. 2024), Ti-MAE (Cheng et al. 2023), TST (Zerveas et al. 2021), the contrastive learning methods: LaST (Wang et al. 2022), CoST (Woo et al. 2022), TS2Vec (Yue et al. 2022). We also include supervised methods iTransoformer (Liu et al. 2023), DLinear (Zeng et al. 2023) and FEDformer (Zhou et al. 2022) in comparison with the cross-domain transfer results of DropPatch and PatchTST. We denote that PatchTST refer to the self-supervised version PatchTST. We conduct experiments in both in-domain and cross-domain settings. For the in-domain setting, we pre-train and finetune the model using the same dataset. In the cross-domain setting, we pre-train the model on one dataset and then finetune it on other target datasets to evaluate its adaptability and generality across diverse scenarios. Unless otherwise stated, the input sequence length of DropPatch is set to 512, and the patch length is fixed at 12 following the self-supervised PatchTST (Nie et al. 2022). Implementation details are provided in the Appendix.

# In-Domain Forecasting

We conduct time-series forecasting experiments under an indomain setting, where models are pre-trained and fine-tuned on the same datasets. The results are summarized in Table 1. Full results are presented in the Appendix.

In-domain experiments show that our DropPatch strategy surpasses existing methods in 13 out of 14 metrics across 7 datasets. Each metric demonstrates significant superiority in comparison with other baselines. PatchTST is noted as a strong baseline. Nevertheless, by simply applying the DropPatch strategy, performance is further improved in both MSE and MAE, with only half the time consumption and memory usage in pre-training stage.

The forecasting performance of PatchTST, SimMTM, and DropPatch is significantly superior to other baselines. The commonality among these three methods is the use of channel-independent masked time-series modeling.

Table 1: In-domain time-series forecasting results, averaged from all forecasting steps $T \in \{ 9 6 , 1 9 2 , 3 3 6 , 7 2 0 \}$ .   

<html><body><table><tr><td>Models</td><td>DropPatch</td><td>PatchTST</td><td>SimMTM</td><td>Ti-MAE</td><td>TST</td><td>LaST</td><td>CoST</td><td>TS2Vec</td></tr><tr><td>Metrics</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td></tr><tr><td>ETTm1</td><td>0.336 0.378</td><td>0.341 0.379</td><td>0.340 0.379</td><td>0.682 0.532</td><td>0.494 0.471</td><td>0.383 0.399</td><td>0.477 0.486</td><td>0.664 0.689</td></tr><tr><td>ETTm2</td><td>0.254 0.315</td><td>0.258 0.318</td><td>0.260 0.318</td><td>0.392 0.417</td><td>0.425 0.371</td><td>0.389 0.394</td><td>0.825 0.651</td><td>0.359 0.420</td></tr><tr><td>ETTh1</td><td>0.400 0.429</td><td>0.430 0.445</td><td>0.404 0.428</td><td>0.721 0.591</td><td>0.6240.562</td><td>0.571 0.532</td><td>0.710 0.627</td><td>0.643 0.728</td></tr><tr><td>ETTh2</td><td>0.347 0.390</td><td>0.355 0.394</td><td>0.3480.391</td><td>0.482 0.488</td><td>0.429 0.458</td><td>0.499 0.497</td><td>1.664 0.999</td><td>0.801 0.856</td></tr><tr><td>Weather</td><td>0.220 0.259</td><td>0.225 0.261</td><td>0.235 0.280</td><td>0.324 0.343</td><td>0.419 0.448</td><td>0.237 0.268</td><td>1.111 0.801</td><td>0.658 0.751</td></tr><tr><td>ECL</td><td>0.157 0.249</td><td>0.157 0.252</td><td>0.162 0.356</td><td>0.561 0.554</td><td>0.310 0.353</td><td>0.186 0.274</td><td>0.228 0.335</td><td>0.354 0.427</td></tr><tr><td>Traffic</td><td>0.378 0.257</td><td>0.382 0.259</td><td>0.392 0.264</td><td>0.916 0.423</td><td>0.611 0.503</td><td>0.713 0.397</td><td>0.760 0.428</td><td>0.501 0.375</td></tr></table></body></html>

Compared to PatchTST, the DropPatch strategy offers further improvements in this task. This is primarily because the masked time-series modeling task can be done with a little understanding of underlying patterns in the timeseries, which can lead to superficial learning and over-fitting. Random dropping introduces a significant amount of randomness to each sample, thus acting as a data augmentation method that helps mitigate the over-fitting issue. In the meanwhile, the challenging pre-training task requires a comprehensive understanding of underlying patterns and thus encourages the learning of useful representation.

# Cross-Domain Forecasting

In this section, we explore multiple scenarios in crossdomain transfer learning. We perform fine-tuning on target datasets using all available training samples. Specifically, we conduct experiments with 1) ECL as the fixed source dataset, following the setup in (Nie et al. 2022), and 2) ETTm1 as the fixed target dataset. The results are summarized in Table 2 3. Full results are presented in the Appendix. Notably, when the source dataset has a mismatch in the number of channels compared to the target dataset, some baseline models are unable to perform the transfer. Although SimMTM is capable of transferring under conditions of channel mismatch, we encountered an out-of-memory (OOM) issue when pretraining SimMTM on the ECL dataset, even with a batch size of 1. Therefore, we also include supervised models for comparison when using ECL as the source dataset.

From the comparison, we observe that DropPatch significantly surpasses the other baselines. Notably, while PatchTST falls behind some supervised methods, DropPatch consistently outperforms these supervised methods. The improved performance stems from the prevention of severe over-fitting in the source dataset, ensuring the model’s robustness and generalization capability when applied to unseen target datasets. In contrast, over-fitting can hinder PatchTST’s ability to generalize effectively to new patterns.

# Evaluations on Synthesized Dataset

In the cross-domain experiments mentioned above, the models are initially pre-trained on a single source dataset and then fine-tuned on a target dataset. For the purpose of developing time-series foundation models (Goswami et al. 2024; Woo et al. 2024; Liu et al. 2024), the source dataset could be a mixed dataset. In the mixed dataset, time-series samples are from different domains, exhibiting varying frequencies, and containing diverse semantic information. This setup aims to enhance the model’s robustness and ability to generalize across different scenarios, while also posing a challenge for models to handle diverse data.

We compile two synthesized datasets to facilitate multidataset pre-training for evaluation. This section primarily focuses on exploring the potential of applying DropPatch to time-series foundation models, without the concern with pushing state-of-the-art results.

Specifically, we merge 10 datasets to compile a synthesized time-series dataset, named STS66M, which has a total file size of over $6 6 ~ \mathrm { M B }$ and consists of more than 3.76 million data points. The models are pre-trained on STS66M and subsequently fine-tuned on other target datasets. The results are presented in Table 4. DropPatch significantly outperforms PatchTST, demonstrating its superior adaptability to diverse pre-training data and its ability to learn more robust and general representations for downstream tasks.

An important application of pre-trained models is to provide priori knowledge for downstream datasets, particularly in scenarios with limited fine-tuning data availability, commonly referred to as few-shot learning. This capability is crucial for the fast adaptation of deep models, which has been demonstrated remarkable performance in NLP (Brown et al. 2020; Achiam et al. 2023). To further explore this, we expand the size of our synthesized time-series dataset by including ECL and PEMS07. The expanded dataset has a file size over 162MB, named STS162M, consisting of $3 2 . 5 \mathrm { m i l } \cdot$ - lion data points. We then conduct few-shot learning experiments using models pre-trained on STS162M. The results are presented in Table 5. For each unseen target dataset, we employ only the headmost 100, 300, and 500 training samples to evaluate DropPatch and PatchTST. DropPatch can generalize well and achieve improved performance.

Table 2: Cross-domain time-series forecasting results. $\mathrm { E C L } {  } \mathrm { E T T m } 1$ denotes the models are pre-trained on ECL and then are fine-tuned on ETTm1. iTransformer, DLinear, and FEDformer are trained directly on the target dataset using supervised learning. Results are averaged from all forecasting steps $T \in \{ 9 6 , 1 9 2 , 3 3 6 , 7 2 0 \}$ .   

<html><body><table><tr><td>Models</td><td>DropPatch</td><td>PatchTST</td><td>iTransformer</td><td>DLinear</td><td>FEDformer</td></tr><tr><td>Metrics</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td></tr><tr><td>ECL→ETTm1</td><td>0.349 0.383</td><td>0.346 0.383</td><td>0.371 0.400</td><td>0.357 0.379</td><td>0.382 0.422</td></tr><tr><td>ECL→ETTm2</td><td>0.258 0.321</td><td>0.257 0.318</td><td>0.272 0.333</td><td>0.267 0.332</td><td>0.292 0.343</td></tr><tr><td>ECL→ETTh1</td><td>0.395 0.426</td><td>0.434 0.448</td><td>0.451 0.462</td><td>0.423 0.437</td><td>0.428 0.454</td></tr><tr><td>ECL→ETTh2</td><td>0.350 0.392</td><td>0.354 0.395</td><td>0.387 0.418</td><td>0.431 0.447</td><td>0.388 0.434</td></tr><tr><td>ECL→Weather</td><td>0.222 0.260</td><td>0.226 0.264</td><td>0.246 0.279</td><td>0.246 0.300</td><td>0.310 0.357</td></tr><tr><td>ECL→Traffic</td><td>0.379 0.257</td><td>0.411 0.285</td><td>0.380 0.271</td><td>0.434 0.295</td><td>0.604 0.372</td></tr></table></body></html>

Table 3: Cross-domain time-series forecasting results. ET $\Gamma { \bf h } 1 {  } \mathrm { E T T r }$ m1 denotes the models are pre-trained on ETTh1 and then are fine-tuned on ETTm1. Results are averaged from all forecasting steps $T \in \{ 9 6 , 1 9 2 , 3 3 6 , 7 2 0 \}$ . Notation ” ” means transfer learning is not feasible due to the mismatch in the number of channels.   

<html><body><table><tr><td>Models</td><td>DropPatch</td><td>PatchTST</td><td>SimMTM</td><td>Ti-MAE</td><td>TST</td><td>LaST</td><td>TF-C</td><td>CoST</td><td>TS2Vec</td></tr><tr><td>Metrics</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td></tr><tr><td>ETTh1→ETTm1</td><td>0.352 0.386</td><td>0.352 0.386</td><td>0.346 0.384</td><td>0.666 0.529</td><td>0.482 0.444</td><td>0.3530.390</td><td>0.746 0.562</td><td>0.359 0.407</td><td>0.697 0.616</td></tr><tr><td>ETTh2-→ETTm1</td><td>0.361 0.390</td><td>0.364 0.391</td><td>0.365 0.384</td><td>0.688 0.535</td><td>0.472 0.448</td><td>0.475 0.489</td><td>0.750 0.654</td><td>0.377 0.413</td><td>0.606 0.556</td></tr><tr><td>ETTm2-→ETTm1</td><td>0.343 0.382</td><td>0.3530.390</td><td>0.351 0.383</td><td>0.682 0.531</td><td>0.480 0.455</td><td>0.414 0.464</td><td>0.758 0.669</td><td>0.354 0.401</td><td>0.756 0.638</td></tr><tr><td>Weather→ETTm1</td><td>0.348 0.385</td><td>0.359 0.390</td><td>0.358 0.388</td><td></td><td>一</td><td>一</td><td>一</td><td>-</td><td>一</td></tr></table></body></html>

# Cold Start

This task aims to forecast in target datasets where lookback length $\displaystyle { L _ { f t } }$ is relatively short, providing limited historical information for fine-tuning. The experimental setup was first introduced in time-series forecasting by (Jin et al. 2022). In our experiments, the lookback length is fixed at $L _ { f t } = 9 6$ , which is shorter than the lookback length $L _ { p t } \ = \ 5 1 2$ on the pre-training stage. We perform experiments on Exchange and four PEMS(PEMS03, PEMS04, PEMS07, PEMS08) as the target datasets. The source dataset is fixed as ECL. Forecasting steps $T \in \{ 9 6 , 1 9 2 , 3 3 6 , 7 2 0 \}$ for Exchange and $T \in \mathsf { \bar { \{ 1 2 , 2 4 , 4 8 , 9 6 \} } }$ for the PEMS datasets. Under cold start scenario, the pre-trained models are expected to leverage the limited historical information for future forecasting. In Table 6, we present the averaged results across the target dataset. Full results are presented in the Appendix.

# Model Efficiency

We compared the training speed and memory usage during the pre-training stage, results are presented in Table 7. All experiments are conducted on a single NVIDIA Tesla V100- SXM2-32GB GPU. In comparison with the other two leading masked time-series modeling methods, DropPatch significantly reduces the memory usage and training time consumption by a large margin. This computational efficiency makes it feasible to scale up and potentially improve model performance by exposing the model to a larger dataset.

# Discussion

Since its inception, the self-supervised PatchTST, which employs a patch-level masking pre-training paradigm, has consistently achieved state-of-the-art performance. Our proposed method DropPatch improves upon this by dropping a certain proportion of patches prior to applying the patchlevel masking strategy, resulting in superior performance in both in-domain and cross-domain scenarios. This raises several questions: How does DropPatch strategy differ from PatchTST, and what drives its enhanced performance?

We will provide a brief description and present the findings for each empirical study. Similar results are observed across various datasets; results on ETTm1 are displayed here as a representative example. Unless otherwise specified, the experiments are conducted in an in-domain scenario using the ETTm1 dataset.

# Normalized Attention Distance

Firstly, we analyze the averaged attention distances before and after applying the DropPatch strategy. Specifically, following previous work (Xie et al. 2023), we define distance as the absolute position difference between two patches, and normalized attention distance as the product of these attention distances with the attention weights. Intuitively, a larger normalized attention distance indicate a focus on global information, while a smaller one reflect attention to local information. The results for each head in all layers are shown in Figure 3 (A).

Table 4: Cross-domain fine-tuning results. Models are pre-trained on STS66M, then fine-tuned on other unseen datasets. Forecasting steps $T \in \{ 9 6 , 1 9 2 , 3 3 6 , 7 2 0 \}$   

<html><body><table><tr><td colspan="2">Datasets</td><td colspan="2">Weather</td><td colspan="2">ETTh1 ETTh2</td><td colspan="2">ETTm1</td><td colspan="2">ETTm2</td><td colspan="2">ECL</td><td colspan="2">Traffic</td></tr><tr><td>Models</td><td>S</td><td>MSE MAE</td><td>MSE MAE</td><td></td><td>MSE MAE</td><td></td><td>MSE MAE</td><td></td><td>MSE MAE</td><td>MSE MAE</td><td></td><td>MSE MAE</td></tr><tr><td rowspan="5">DropPatch</td><td>96</td><td>0.142 0.190</td><td>0.374</td><td>0.409</td><td>0.288 0.346</td><td></td><td>0.289 0.345</td><td>0.171</td><td>0.261</td><td>0.129</td><td>0.221</td><td>0.361 0.255</td></tr><tr><td>192</td><td>0.186 0.234</td><td>0.401</td><td>0.427</td><td>0.352 0.385</td><td></td><td>0.334 0.373</td><td>0.229 0.301</td><td></td><td>0.148 0.239</td><td></td><td>0.378 0.262</td></tr><tr><td>336</td><td>0.238 0.274</td><td>0.406 0.437</td><td></td><td>0.360 0.401</td><td></td><td>0.361 0.394</td><td>0.282 0.337</td><td></td><td>0.165 0.258</td><td></td><td>0.389 0.268</td></tr><tr><td>720</td><td>0.312 0.330</td><td>0.446 0.469</td><td></td><td>0.384 0.426</td><td></td><td>0.408 0.426</td><td>0.365 0.389</td><td></td><td>0.201 0.290</td><td></td><td>0.427 0.289</td></tr><tr><td>AVG</td><td>0.220 0.257</td><td>0.407 0.436</td><td></td><td>0.3460.390</td><td></td><td>0.348 0.385</td><td>0.262 0.322</td><td></td><td>0.161 0.252</td><td></td><td>0.389 0.269</td></tr><tr><td rowspan="5">PatchTST</td><td>96</td><td>0.1440.193</td><td></td><td>0.381 0.412</td><td>0.303 0.355</td><td></td><td>0.2930.346</td><td></td><td>0.170 0.262</td><td>0.131 0.224</td><td></td><td>0.372 0.266</td></tr><tr><td>192</td><td>0.191 0.240</td><td>0.407</td><td>0.430</td><td>0.367</td><td>0.390</td><td>0.336 0.375</td><td></td><td>0.2350.309</td><td>0.148 0.240</td><td></td><td>0.389 0.272</td></tr><tr><td>336</td><td>0.2440.281</td><td>0.411 0.435</td><td></td><td>0.366 0.403</td><td></td><td>0.3640.394</td><td>0.280</td><td>0.334</td><td>0.165</td><td>0.258</td><td>0.396 0.273</td></tr><tr><td>720</td><td>0.317 0.334</td><td>0.4430.464</td><td></td><td>0.395 0.431</td><td></td><td>0.412 0.428</td><td>0.366 0.387</td><td></td><td>0.203 0.291</td><td></td><td>0.434 0.293</td></tr><tr><td>AVG</td><td>0.224 0.262</td><td>0.411 0.435</td><td></td><td>0.358 0.395</td><td>0.351</td><td>0.386</td><td>0.263 0.323</td><td></td><td>0.162 0.253</td><td>0.398</td><td>0.276</td></tr></table></body></html>

<html><body><table><tr><td></td><td>Datasets</td><td>Weather</td><td>ETTh1</td><td></td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td><td></td><td>Traffic</td></tr><tr><td>Models</td><td># Samples</td><td>MSE</td><td>MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE MAE</td><td>MSE</td><td>MAE</td><td>MSE MAE</td></tr><tr><td rowspan="3">DropPatch</td><td>100</td><td>0.242</td><td>0.290</td><td>0.626 0.525</td><td>0.372 0.411</td><td>0.502 0.465</td><td>0.277</td><td>0.343</td><td>0.447 0.309</td></tr><tr><td>300</td><td>0.223</td><td>0.273</td><td>0.506 0.488</td><td>0.312 0.366</td><td>0.531 0.478</td><td>0.237</td><td>0.311</td><td>0.399 0.275</td></tr><tr><td>500</td><td>0.212</td><td>0.263</td><td>0.474 0.461</td><td>0.317 0.366</td><td>0.518 0.476</td><td></td><td>0.210 0.292</td><td>0.395 0.275</td></tr><tr><td rowspan="3">PatchTST</td><td>100</td><td>0.247</td><td>0.294</td><td>0.666 0.552</td><td>0.381 0.401</td><td>0.521 0.474</td><td>0.282</td><td>0.347</td><td>0.450 0.313</td></tr><tr><td>300</td><td>0.222</td><td>0.271</td><td>0.520 0.503</td><td>0.319 0.375</td><td>0.508 0.469</td><td>0.257</td><td>0.327</td><td>0.399 0.276</td></tr><tr><td>500</td><td>0.225</td><td>0.274</td><td>0.483 0.481</td><td>0.323 0.372</td><td>0.493 0.461</td><td>0.214</td><td>0.298</td><td>0.396 0.275</td></tr></table></body></html>

Table 5: Few-shot learning results. Models are pre-trained on STS162M, then fine-tuned on other unseen datasets using limite training samples. Forecasting steps are fixed at 96.

Finding 1 : By comparing normalized attention distances, we found that the DropPatch strategy enables each attention head in the model to focus on information at varying scales. Specifically, this strategy enhancing the model’s ability to capture both short-term and long-term dependencies, empowering the model with a more comprehensive understanding of the time-series.

# Attention Coefficients Distribution

We then analyze the distributions of attention coefficients across different heads and layers. Uniform attention coefficients lead to a loss of distinctiveness, effectively diminishing the model’s ability to capture unique patterns. In contrast, distributions with sharper focus and higher distinctiveness are regarded as more effective (Zhou et al. 2021; Chen et al. 2022; Vyas, Katharopoulos, and Fleuret 2020; Choromanski et al. 2020). In our empirical study, we quantify the distinctiveness of these distributions by computing the Kullback-Leibler (KL) divergence between the uniform distribution and the attention distributions. A larger KL divergence indicates a greater deviation from the uniform distribution, reflecting sharper and more distinctive attention patterns. The results are shown in Figure 3 (B).

Finding 2 : The results indicate that applying the DropPatch strategy sharpens the focus of attention heads, facilitating the identification of more valuable information and underlying patterns.

# Attention Coefficients Difference

The previous two subsections reveal that attention heads in DropPatch exhibit greater diversity in behavior. In this subsection, we further investigate whether different attention heads capture diverse information. Specifically, we conduct an analysis of the attention distribution across different heads by calculating the KL divergence between attention heads in the same layer. This comparison highlights the distributional differences among attention heads. A higher KL divergence indicates greater differences, suggesting that each head has learned distinct information, thereby reducing redundancy in the information captured by different heads. As shown in Figure 4, attention heads in DropPatch exhibit higher KL divergence compared to those in PatchTST.

Finding 3 : The analysis of attention distributions demonstrates that the DropPatch strategy enables attention heads to capture distinct information, thereby reducing redundancy and enhancing the model’s representation capabilities.

# Central Kernel Alignment Analysis

We use CKA (Central Kernel Alignment) values (Kornblith et al. 2019) to compare the similarity of representations in a pre-trained model and a fine-tuned model. Specifically, we calculate CKA similarity using the last layer representations between the pre-trained model and the fine-tuned model. Models are pre-trained on ECL.

<html><body><table><tr><td>Models</td><td>DropPatch</td><td>PatchTST</td></tr><tr><td>Metrics</td><td>MSE MAE</td><td>MSE MAE</td></tr><tr><td>Exchange</td><td>0.348 0.396</td><td>0.354 0.400</td></tr><tr><td>PEMS03</td><td>0.198 0.293</td><td>0.205 0.296</td></tr><tr><td>PEMS04</td><td>0.264 0.339</td><td>0.273 0.343</td></tr><tr><td>PEMS07</td><td>0.214 0.312</td><td>0.219 0.323</td></tr><tr><td>PEMS08</td><td>0.225 0.300</td><td>0.233 0.305</td></tr></table></body></html>

Table 6: Results of cold start setup. The lookback length $\displaystyle { L _ { f t } }$ is fixed at 96. Results are averaged from all forecasting steps.

Table 7: Model efficiency comparison. Mem. denotes the memory usage, measured in megabytes (MB). T.C. denotes the time consumption per epoch in seconds.   

<html><body><table><tr><td>Models</td><td>DropPatch</td><td>PatchTST</td><td>SimMTM</td></tr><tr><td>Metrics</td><td>Mem. T.C.</td><td>Mem. T.C.</td><td>Mem. T.C.</td></tr><tr><td>ETTm1</td><td>1404 32.2</td><td>1722 44.5</td><td>29090 823.3</td></tr><tr><td>Weather</td><td>2094 42.1</td><td>3914 75.1</td><td>OOM</td></tr><tr><td>ECL</td><td>4256306.7</td><td>11050 528.5</td><td>OOM</td></tr></table></body></html>

![](images/26b7c9b52dfb5c936df229ac7569d1e9f22f17c1c9388a0fad1171a297a782f1.jpg)  
Figure 3: Analysis of (A) normalized distance, and (B) KL divergence between attention distributions and uniform distributio for each head across all layers. Each dot represents an individual attention head, while different colors indicate different layers

![](images/bcabed42aaf9bfd3cccd732dc8e8cfde70fa95207430ea789ec85d124cb1c89d.jpg)  
Figure 4: Attention distribution difference.

Finding 4 : As presented in Table 8, DropPatch significantly enhances the representation ability. For in-domain tasks, DropPatch achieves high CKA similarity, indicating that the model better learns the underlying patterns of the dataset. For cross-domain tasks, DropPatch exhibits reduced CKA similarity, which we attribute to the model’s improved ability to handle domain shifts and adapt to unseen distributions after applying the DropPatch strategy.

Table 8: CKA similarity results.   

<html><body><table><tr><td></td><td>ETTh1</td><td>Weather</td><td>ECL</td></tr><tr><td>DropPatch</td><td>0.941</td><td>0.883</td><td>0.869</td></tr><tr><td>PatchTST</td><td>0.911</td><td>0.835</td><td>0.916</td></tr></table></body></html>

# Conclusion

In this paper, we propose DropPatch, an enhancement to masked time-series modeling achieved by introducing the random dropping of sub-series patches. This approach yields significant improvements in pre-training efficiency and various downstream tasks. Extensive experiments validate the effectiveness, highlighting its ability to improve the attention mechanism by enabling a sha(rBp)er focus on multi-scale and diverse information. Furthermore, out theoretical analysis reveals that this technique slows the degeneration of Transformer representations toward a rank-1 linear subspace, underlying its beneficial impact on model performance.