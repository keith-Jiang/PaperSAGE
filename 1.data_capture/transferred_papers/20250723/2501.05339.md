# JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with Hardware-Software Co-Exploration

Mingzi Wang1, Yuan Meng2\*, Chen Tang1,2, Weixiang Zhang1, Yijian ${ \bf { Q } } { \bf { { i n } } } ^ { 2 }$ , Yang $\mathbf { Y a 0 } ^ { 2 }$ , Yingxin $\mathbf { L i } ^ { 1 }$ , Tongtong Feng2, Xin Wang2, Xun Guan1\*, Zhi Wang1\*, Wenwu $\mathbf { Z } \mathbf { h } \mathbf { u } ^ { 2 ^ { * } }$

1Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China 2Department of Computer Science and Technology & BNRist, Tsinghua University, Beijing, China wmz22@mails.tsinghua.edu.cn, yuanmeng $@$ mail.tsinghua.edu.cn, {xun.guan,wangzhi} $@ _ { \mathbf { S } \mathbf { Z } }$ .tsinghua.edu.cn, wwzhu $@$ tsinghua.edu.cn

# Abstract

The co-design of neural network architectures, quantization precisions, and hardware accelerators offers a promising approach to achieving an optimal balance between performance and efficiency, particularly for model deployment on resource-constrained edge devices. In this work, we propose the JAQ Framework, which jointly optimizes the three critical dimensions. However, effectively automating the design process across the vast search space of those three dimensions poses significant challenges, especially when pursuing extremely low-bit quantization. Specifical, the primary challenges include: (1) Memory overhead in software-side: Low-precision quantization-aware training can lead to significant memory usage due to storing large intermediate features and latent weights for back-propagation, potentially causing memory exhaustion. (2) Search time-consuming in hardware-side: The discrete nature of hardware parameters and the complex interplay between compiler optimizations and individual operators make the accelerator search time-consuming. To address these issues, JAQ mitigates the memory overhead through a channel-wise sparse quantization (CSQ) scheme, selectively applying quantization to the most sensitive components of the model during optimization. Additionally, JAQ designs BatchTile, which employs a hardware generation network to encode all possible tiling modes, thereby speeding up the search for the optimal compiler mapping strategy. Extensive experiments demonstrate the effectiveness of JAQ, achieving approximately $7 \%$ higher Top-1 accuracy on ImageNet compared to previous methods and reducing the hardware search time per iteration to 0.15 seconds. Code is available at https://github.com/wmzopensource/JAQ/.

# Introduction

Given the significant computational demands of Deep Neural Networks (DNNs), deploying them in resource-limited environments, such as the Internet of Things (IoT), remains a challenge. For example, even highly optimized Convolutional Neural Networks (CNNs) have recently struggled to perform efficiently on resource-constrained hardware devices (Li et al. 2023; Rashid, Kallakuri, and Mohsenin 2024). To speed up inference on real-world hardware while maintaining performance, hardware-aware techniques (e.g., quantization, and hardware-aware neural architecture search) have emerged to improve the model efficiency on the model-side. For example, HAQ (Wang et al. 2019), OFA (Cai et al. 2019), and ElasticViT (Tang et al. 2023) optimize the model for a fixed target device. On the other hand, accelerator-side methods design specialized accelerators (Chen et al. 2014; Liu et al. 2015; Parashar et al. 2017) to facilitate the deployment of DNNs, have received more attention recently.

However, the separated design on either the model-side or accelerator-side falls into sub-optimal (Fu et al. 2021; Hong et al. 2022) as (1) the model-size optimization will be up against efficiency loss when the hardware does not support certain operators and (2) the optimal accelerator design varies very different for various model structures and the corresponding quantized precision (Wang et al. 2019). This trend suggests the limitations of and the need for codesign of both neural networks, quantized bit-widths, and hardware accelerators.

The first principle of co-design involves efficiently navigating the vast design space. To achieve this, differentiable methods have been developed to facilitate end-to-end coexploration. Notably, AutoNBA ( $\mathrm { F u }$ et al. 2021) utilizes learnable weights for determining the expected precision and architectural operator, along with designing a new objective for optimizing hardware components. DANCE (Choi et al. 2021) further introduces an MLP-based accelerator search strategy into the differentiable search framework. However, these methods support only high bit-width quantization (i.e., $\geq 4$ bits), resulting in minimal performance degradation due to the significant redundancies that remain for compression (Esser et al. 2019). In contrast, we have observed that low-precision disrupts the optimization process, leading to a misguided search, as we will discuss later.

Therefore, we propose JAQ framework, which addresses challenges and achieves efficient joint exploration. For the first challenge, we propose channel-wise sparse quantization method. It selects a small subset of the most crucial activations channels for quantization, leaving other channels unquantized during the search process, effectively alleviating the issue of memory explosion. For the second challenge, we propose BatchTile approach that encodes all tile sizes within the search space as different batches, enabling us to determine the optimal tiling strategies simultaneously, which significantly reduces the time overhead.

Table 1: Comparison with other works on the search space dimension(Model Architecture, Low-Bit Quantization and Accelerator Architecture).   

<html><body><table><tr><td>Method</td><td>Model</td><td>Low-Bit</td><td>Acc</td></tr><tr><td>NAAS(Lin, Yang,and Han 2021)</td><td>×</td><td>×</td><td>√</td></tr><tr><td>DANCE (Choi et al. 2021)</td><td>√</td><td>×</td><td>√</td></tr><tr><td>Auto-nba (Fu et al. 2021)</td><td>√</td><td>×</td><td>√</td></tr><tr><td>Ours</td><td>√</td><td>√</td><td>√</td></tr></table></body></html>

To summarize, the contributions of the paper are:

• We propose the JAQ framework, which enables efficient and effective co-exploration within the extensive optimization space. To the best of our knowledge, we are the first to explore the joint search among network architecture, ultra-low mixed-precision bit-width allocation, and accelerator architecture, as shown in Tab. 1.   
• To tackle the challenge of memory explosion, we propose the channel-wise sparse quantization approach, achieving around $5 \times$ reduction in memory cost compared to non-optimized scenarios.   
• We propose a hardware generation network to optimize accelerator design and BatchTile method to integrate the compiler mapping search efficiently, which reduces the search time per iteration to 0.15 seconds.   
• Extensive experimental evaluations demonstrate that our framework surpasses the state-of-the-art. Our work opens up new possibilities for agile software-hardware codesign.

# Related Work

# Quantization and Neural Architecture Search

As a hardware-friendly lightweight technique, quantization has broad prospects for application. Mixed-Precision Quantization (MPQ) (Dong et al. 2019; Wang et al. 2019; Tang et al. 2022; Kim et al. 2024; Huang et al. 2022) allocates different bitwidths to the activations and weights of each layer, showing better accuracy-efficiency trade-off compared to fixed-precision quantization (Choi et al. 2018; Esser et al. 2019; Markov et al. 2023; Xu et al. 2023; Nagel et al. 2022). Recently, hardware increasingly supports mixedprecision (Sharma et al. 2018; Umuroglu, Rasnayake, and Sja¨lander 2018), which further pushes the research in MPQ. HAQ (Wang et al. 2019) leverages Reinforcement learning (RL) to allocate bitwidth to each layer. HAWQ (Dong et al. 2019) uses the information derived from the Hessian matrix to determine quantization sensitivity and guide the allocation of bitwidths for network parameters.

Neural Architecture Search (NAS) enables the automated design of high-performance DNN network structures, saving time and effort of the manual design. To reduce search cost, differentiable NAS (Liu, Simonyan, and Yang 2018; Qin et al. 2021) methods have merged, which integrate all candidate operators into an end-to-end trained supernet, and finally select the optimal subnet. Some studies have incorporated hardware performance metrics into the NAS via lookup tables (Zhang et al. 2020; Li et al. 2021), aiming to enhance the model’s efficiency on actual hardware. However, all these works concentrate exclusively on algorithmic optimization without exploring hardware architecture, which may not yield optimal inference efficiency.

# DNN Accelerators

To improve the performance of modern deep neural network computations, fixed-bitwidth DNN accelerators have emerged, featuring specialized components like MAC arrays, on-chip buffers, and network-on-chip architectures (Chen et al. 2016; Jouppi et al. 2017; Du et al. 2015). Recently, the concept of MPQ has paved the way for the development of bit-flexible accelerators (Sharma et al. 2018; Umuroglu, Rasnayake, and Sja¨lander 2018) that allow for varying bitwidths across individual layers. However, designing AI accelerators remains a complex and time-consuming task that demands significant hardware expertise. However, designing AI accelerators is complex and requires significant expertise. AI-driven methods, such as NAAS (Lin, Yang, and Han 2021) and GPT4AIGChip (Fu et al. 2023), streamline the process by autonomously evaluating design configurations. These approaches focus primarily on hardware architecture and often yield sub-optimal results compared to co-design methodologies that integrate both network and hardware exploration (You et al. 2023; Lou et al. 2023; Stevens et al. 2021; Reggiani et al. 2023).

# Hardware-software Co-design

Some studies employ hardware-software co-design methods using reinforcement learning or evolutionary algorithm (Jiang et al. 2020; Abdelfattah et al. 2020), which require expensive training time and also suffer from limited search spaces. To address this issue, differentiable methods have been employed for co-exploration. EDD (Li et al. 2020) is an FPGA-based differentiable network design framework. However, it does not encompass the search for hardware parameters, such as the number of BRAMs or DSPs. While Dance (Choi et al. 2021) builds a pipeline to explore ASICbased accelerator and network structure, it has a limitation that it does not take quantization into consideration. Autonba (Fu et al. 2021) is not suitable for low-bit quantization.

JAQ targets efficient joint search of network, low-bit mixed-precision bitwidths and accelerator architecture.

# JAQ Framework

# Preliminary

Differentiable Neural Architecture Search. Differentiable neural architecture search (DNAS) (Liu, Simonyan, and Yang 2018; Wu et al. 2019) transforms the entire search space into a supernet and each path in the supernet is equipped with an architecture parameter, which represents the probability of selecting this path. The incorporation of the Gumbel-Softmax(Jang, Gu, and Poole 2016) function plays a pivotal role enabling these architecture parameters trainable through gradient-based optimization. After the training of the supernet, the optimal subnet is formed by the path with the highest architecture parameter in each layer. The function of Gumbel-Softmax is:

![](images/f7ce7bd0dba2229ba0d7d355a5bd0c5383895eec426afaf78ad09d40101d666a.jpg)  
Figure 1: JAQ framework. The left part represents the optimization of network structure and bitwidths allocation, addressing the memory cost bottleneck through channel-wise sparse quantization. The right part depicts accelerator architecture search, including hardware parameters and compiler mapping strategy. Hardware metrics indicate accelerator performance (Energy, Latency and Area).

$$
\beta _ { t } = \frac { \exp { \left( \frac { \beta _ { t } + \epsilon _ { t } } { \tau } \right) } } { \sum _ { i = 1 } ^ { N } \exp { \left( \frac { \beta _ { i } + \epsilon _ { i } } { \tau } \right) } } , \quad \epsilon \sim U ( 0 , 1 ) ,
$$

where $\beta$ represents the original parameter distribution, while $\epsilon$ is a number sampled from a uniform distribution ranging between 0 and 1. Additionally, the smoothness of the distribution can be regulated using the temperature coefficient $\tau$ .

Quantization. The quantization function $Q ( \cdot )$ , defined as:

$$
Q ( V ) = \mathrm { r o u n d } \left( \mathrm { c l i p } \left( { \frac { V } { s } } , \mathrm { m i n } _ { b } , \mathrm { m a x } _ { b } \right) \right) \times s ,
$$

where $\mathbf { V }$ and $Q ( \mathbf { V } )$ denote the floating-point value and its dequantized value (quantization width is $b$ bit). The parameter $\begin{array} { r } { s = \frac { \operatorname* { m a x } ( \mathbf { V } ) - \operatorname* { m i n } ( \mathbf { V } ) } { 2 ^ { b } - 1 } } \end{array}$ , which represents the scale factor used in the quantization mapping, the interval $[ m i n _ { b } , m a x _ { b } ]$ specifies integer range.

# Problem Formulation

Fig. 1 illustrates the overall framework of JAQ, which includes the joint search among network structure, ultra-low mixed-precision bitwidth allocation, and accelerator architecture. The formulation of the joint optimization problem is:

$$
\begin{array} { r l } & { \underset { \boldsymbol { \alpha } , \boldsymbol { \beta } , \boldsymbol { \gamma } , \mathbf { w } } { \mathrm { m i n } } \mathcal { L } _ { \mathrm { C E } } \left( \mathbf { w } , \mathrm { N } ( \boldsymbol { \alpha } ) , \mathrm { M } ( \boldsymbol { \beta } ) \right) } \\ { \mathrm { ~ } } & { \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } \mathrm { ~ } } \\ { \mathrm { ~ } \mathrm { ~ } } & { \mathrm { ~ } \mathcal { E } _ { \mathrm { H W } } \left( \mathrm { H } \left( \boldsymbol { \gamma } \right) , \mathrm { N } \left( \boldsymbol { \alpha } \right) , \mathrm { M } ( \boldsymbol { \beta } ) \right) \leq C } \end{array}
$$

where $\alpha$ and $\beta$ denote the operator architecture parameters and the bitwidth architecture parameters, respectively. $\gamma$ denotes the hardware accelerator configuration. w represents the weights of the NAS supernet. $\mathrm { N } ( \alpha )$ indicates the network structure selected based on $\alpha$ . $\mathbf { M } ( \beta )$ denotes the bitwidths selection for each operator according to $\beta . \mathrm { H } ( \gamma )$ depicts the accelerator architecture based on $\gamma$ . $\mathcal { E } _ { \mathrm { H W } }$ reflects the hardware-side performance, calculated by hardware metrics (Energy, Latency, and Area). $\mathcal { L } _ { \mathrm { C E } }$ represents the crossentropy loss, and $\mathcal { E } _ { \mathrm { H W } }$ . To track this optimization problem, we introduce a Lagrange multiplier $\lambda$ for $\operatorname { E q }$ (4):

$$
\operatorname* { m i n } _ { \alpha , \beta , \gamma , \mathbf { w } } \Big [ \mathcal { L } _ { \mathrm { C E } } \left( \mathbf { w } , \mathrm { N } ( \alpha ) , \mathrm { M } ( \beta ) \right) + \lambda \mathcal { E } _ { \mathrm { H W } } \left( \mathrm { H } \left( \gamma \right) , \mathrm { N } \left( \alpha \right) , \mathrm { M } ( \beta ) \right) \Big ] ,
$$

# Channel-wise Sparse Quantization (CSQ)

Memory Cost Bottleneck. DNAS segments the supernet into a series of cells. Each cell is structured as a directed acyclic graph (DAG) with several nodes (each node corresponds to a distinct operator), and each operator within the supernet must be stored in GPU memory during training. This indicates that adding extra search dimensions in DNAS-based methods can easily lead to GPU memory overload or require to reduce the training batch size to maintain the original utilization of GPU memory. Quantization, a memory-intensive process, involves storing numerous quantized parameters and additional quantization information. Therefore, integrating network architecture search with bitwidth selection significantly amplifies the GPU memory consumption. For example, as shown in Appendix A Fig. 4a, the utilization of GPU memory increases linearly with the number of available bitwidth options in each operator which is deemed unacceptable. All experiments are conducted on a single NVIDIA GeForce RTX 4090 GPU to prevent inaccuracies in memory measurements caused by multiple copies of the model in multi-GPU parallel setups. We find that as the batch size increases linearly, the GPU memory utilization also increases linearly. Therefore, by measuring memory cost with a small batch size, we can estimate the memory utilization for larger batch sizes. We utilize FBNet (Wu et al. 2019) as the supernet, and add quantization process(Eq. 2) in each operator.

CSQ. The differentable network and bitwidths co-search framework in JAQ can be implemented by formulating as:

$$
\begin{array} { r } { \bar { \mathbf { A } } ^ { l + 1 } = \displaystyle \sum _ { i = 1 } ^ { n } \alpha _ { i } ^ { l } \cdot \tilde { \mathbf { W } } _ { i } ^ { l } \cdot \tilde { \mathbf { A } } _ { i } ^ { l } \mathrm { , ~ w h e r e } } \\ { \tilde { \mathbf { W } } _ { i } ^ { l } = \displaystyle \sum _ { k = 1 } ^ { m } \beta _ { w _ { i , k } ^ { l } } \cdot Q ( \mathbf { W } _ { i k } ^ { l } ) \mathrm { , ~ a n d ~ } \tilde { \mathbf { A } } _ { i } ^ { l } = \displaystyle \sum _ { k = 1 } ^ { m } \beta _ { a _ { i , k } ^ { l } } \cdot Q ( \mathbf { A } ^ { l } ) \mathrm { , } } \end{array}
$$

where $l$ denotes the layer index in network, and $n$ is the number of operator candidates per layer, while $m$ is the number of bit-width candidates per operator. $\tilde { W }$ and $\tilde { A }$ represent the sum of quantized weights and quantized activations under different precisions respectively. $\alpha$ denotes the operator architecture parameters, while $\beta _ { w _ { i , k } ^ { l } }$ and $\beta _ { a _ { i , k } ^ { l } }$ are the architecture parameters of weights and activations for each precision. $Q$ represents the quantization function (Eq. 2). As illustrated in Appendix A Fig. 4b, during the supernet training process, the memory requirement of weight quantization is trivial. Therefore, memory cost bottleneck in network and bitwidths co-search framework can be predominantly attributed to the quantization of activations. To alleviate this issue during supernet training, we propose channelwise sparse quantization strategy for the quantization of activation. This can be implemented by reformulating Eq. 5 as:

$$
\tilde { \mathbf { A } } _ { i } ^ { l } = \left( \sum _ { k = 1 } ^ { m } \beta _ { a _ { i , k } ^ { l } } \cdot Q ( \mathbf { A } ^ { l } ( \Omega ^ { l } ) ) \right) \oplus \mathbf { A } ^ { l } ( 1 - \Omega ^ { l } ) ,
$$

where $\Omega$ is the indices of channels to be quantized and $\oplus$ denotes concatenation operation. $\mathbf { A } ^ { l } ( \Omega ^ { l } )$ represents all channels selected in $\mathbf { A } ^ { l }$ according to $\Omega ^ { l }$ .

The core innovation of this method is to quantize only a few channels of activations during searching phase, while leaving other channels unquantized, which significantly reduces the demand on GPU memory. To achieve better search result (detailed explanation and experiments are provided in the ablation study), we need to select the most important channels from each activations. Inspired by a previous work (Liu et al. 2017), the scale factors in Batch Normalization (BN) can effectively represent the importance of each channel.

$$
\hat { z } = \frac { z _ { \mathrm { i n } } - \mu _ { B } } { \sqrt { \sigma _ { B } ^ { 2 } + \epsilon } } , \quad z _ { \mathrm { o u t } } = \gamma \hat { z } + \beta ,
$$

where $z _ { \mathrm { i n } }$ and $z _ { \mathrm { o u t } }$ are the input and output of a BN layer, $\mu _ { B }$ and $\sigma _ { B }$ represent the mean and standard deviation of the input activations across the batch $B$ . The trainable parameters $\gamma$ and $\beta$ serve as scale and shift factors respectively.

Therefore, we choose to quantize only the top $K \%$ of the most important channels of each activations during search

phase and defining:

$$
\begin{array} { c } { \Omega ^ { l } \gets \mathrm { T o p K C h a n n e l T o Q u a n t i z e } ( \Gamma ^ { l } , K ) } \\ { \Gamma _ { j } ^ { l } = \displaystyle \sum _ { i = 1 } ^ { n } \alpha _ { i } ^ { l - 1 } \gamma _ { i j } ^ { l - 1 } \quad j \in N ^ { l - 1 } , } \end{array}
$$

where $\Gamma$ is the importance indicator of each channel, and $\gamma$ represents the scale factors defined in Eq. 7, while $\mathbf { N }$ is the output channels number of ${ \mathrm { ~ \it ~ l ~ } } - 1$ layer.

During the search phase, scale factors are trainable to dynamically adjust the importance indicator for each channel. Finally, as Appendix A Fig. 4c demonstrates, the GPU utilization in our algorithm is significantly reduced to acceptable bounds.

# Accelerator Architecture Search

Hardware parameters in accelerators are non-differentiable. Although it is possible to optimize these parameters using reinforcement learning (Lin, Yang, and Han 2021), the time overhead is particularly substantial. Therefore, there is a demand for exploring efficient methods to search for these parameters. Furthermore, compiler mapping is crucial for the latency and energy consumption of DNNs inference on accelerators. Therefore we incorporate compiler mapping optimization into the joint search framework, reducing its searching time to less than 0.15 seconds per iteration.

Accelerator Search Space. Our accelerator search space is divided into two categories. The first category involves accelerator parameters, which include the shape and number of processing elements (PEs), the size of the on-chip cache used for storing weights, activations, and outputs, as well as the inter-connection of PEs, described as the dataflow type of the parallel dimension. The second category focuses on optimizations of the compiler mapping, including sizes of tiles and loop order of tiling.

Accelerator Parameters Search. First, we identify the current optimal subnet within the supernet and encode each operator in the subnet as the operator encoding vector in Fig. 2: Kernel, Stride, Output Row, Input Channel, Output Channel, Activation Bitwidth, and Weight Bitwidth. Then, the encoded operators are sent into the accelerator parameters search part, constituted by five layers of residual blocks. The final layer maps the hidden states into seven elements in the accelerator parameters encoding Vector in Fig. 2: $\mathrm { P E } _ { x }$ , $\mathrm { P E } _ { y }$ , Activation Cache, Weight Cache, Output Cache, Dataflow Type, and Tile Order.

Gumbel-Softmax (Jang, Gu, and Poole 2016) is used as the activation function in each classifier, ensuring that the output values closely resemble the inputs for hardware cost estimation, as well as maintaining the gradient propagation during the training stage.

Compiler Mapping Search: BatchTile. As shown in Tab. 2, Tile size is crucial for the model inference performance on accelerator. In JAQ, the accelerator is configured to process one image at a time, hence the batch size is one. Consequently, each operator requires tiling across four dimensions: input channel, output channel, output width, and

p0 op 1 op 2 CNN Workload TO T1 T2 TO T1T2T3 1011 Fornia For c in range(C/c_tile_size): Foryin range(Y/y_tile_size): For x in range(X/x_tile_size):   
1010 coc1c2 A A A 1 B A A A A 日 A A Execute Tile Workload   
0101 Subnet Encoder Parameters Search cococo C1C1C1C1 C2C2 Batch Size N Output Channel K Area Estimator BatchTile Input Channel C ? Output Row Y MM2 Best Accelerator Architecture CostHw 国 Energy & Latency Estimator ? Output Column × C : Conv Operator Encoding Vector A: Accelerator Parameters Encoding Vector T: Tile Encoding Vector   
Kernel Stride Y C K Act Bitwidth Wgt Bitwidth PE_xPE_yAct/Wgt/Out Cache DataflowTile Order K c × Y

Table 2: The latency (Lat.) and energy (Ene.) for the optimal tiling method and two randomly selected tiling methods applied to a specific pair of operator and accelerator (Operator: kernel size of 5, stride of 1, output size of 7, both input and output channels at 552, with activation and weight bitwidths of 8 bits. Accelerator: PE Array dimensions of 16x16, with 384KB Act/Wgt/Out Cache sizes).   

<html><body><table><tr><td colspan="7"></td></tr><tr><td></td><td>B/b</td><td>OW/ow</td><td>OH/oh</td><td>IC/ic</td><td>OC/oc</td><td>Lat. (ms)</td><td>Ene. (mJ)</td></tr><tr><td>Best Tiling</td><td>1/1</td><td>7/1</td><td>7/1</td><td>35/16</td><td>35/16</td><td>3.2</td><td>2.6</td></tr><tr><td>Case 0</td><td>1/1</td><td>2/4</td><td>2/4</td><td>276/2</td><td>2/512</td><td>56.6</td><td>8.3</td></tr><tr><td>Case 1</td><td>1/1</td><td>4/2</td><td>4/2</td><td>18/32</td><td>5/128</td><td>4.7</td><td>5.1</td></tr></table></body></html>

output height. To achieve peak performance for model inference on accelerator, optimal tile sizes for each operator should be determined during the compiler mapping stage. However, finding the optimal tile size for all operators in a subnet is time-consuming (around 50s), which is unfriendly to end-to-end joint search. To efficiently find the optimal tile size, we propose the BatchTile method. The BatchTile method initially encodes each operator’s tiling strategies across four dimensions as illustrated in Fig. 2: Output Channel, Input Channel, Output Column, and Output Row. Subsequently, we concatenate accelerator parameters encoding vector, operator encoding vector of each operator, and different tile encoding vectors to form various Operator, Accelerator Parameters, Tiling Strategy pairs. These pairs, as different batches, are fed into the Energy & Latency Estimator (the principle of the estimator follows (Choi et al. 2021)) to simultaneously identify the optimal tiling strategy for each operator. Finally, the BatchTile method reduces the entire compiler mapping search time to less than 0.15 seconds(comparison experiment is in Tab. 3).

Table 3: Comparison of Accelerator search time(s) between JAQ and previous work.   

<html><body><table><tr><td>Method</td><td>Search Time</td></tr><tr><td>Auto-nba (Fu et al. 2021)</td><td>30</td></tr><tr><td>Ours</td><td>0.15</td></tr></table></body></html>

Our methodology capitalizes on hardware design principles, estimating the hardware from the perspective of area, energy, and latency metrics. Combining these three metrics, the hardware cost function included in Eq. 4 is:

$$
{ \mathcal { E } } _ { \mathtt { H W } } = \lambda _ { E } \cdot { \mathtt { E n e r g y } } + \lambda _ { L } \cdot { \mathtt { L a t e n c y } } + \lambda _ { A } \cdot { \mathtt { A r e a } } ,
$$

where $\lambda _ { E } , \lambda _ { L }$ , and $\lambda _ { A }$ are adjustable among these cost metrics.

# The Overall Joint Pipeline

JAQ consists of the search stage and the retrain stage. The search stage integrates the channel-wise sparse quantization method into the model (network architectures and bitwidths) searching, and incorporates the BatchTile approach into the accelerator searching.

For searching, each iteration consists of two steps. The first step is to update the weights $( \mathbf { w } )$ in supernet, which doesn’t require interfacing with the accelerator. The second step, collaborating with the accelerator, involves updating the architecture parameters $\overset { \cdot } { \alpha }$ and $\beta$ ) and the accelerator configuration $( \gamma )$ , as defined in Eq. 4. In the second step, after forward propagation in the supernet, the current optimal subnet is encoded and passed into the accelerator search framework. Then, we optimize the accelerator parameters and compiler mapping strategy. Subsequently, the CostHW obtained through Eq. 9 is bound to the architecture parameters, which will be updated during the backpropagation process.

For retraining, we retrain the optimal subnet obtained from the search stage. Finally, we achieve the optimal network structure and accelerator architecture, thus realizing the synergy between software and hardware design.

# Experiments

# Experimental Settings

Our experiments are conducted on the CIFAR-10/100, and ImageNet datasets. In search stage, We use $8 0 \%$ of the data to update the weights within the supernet and $2 0 \%$ of the data for the architecture parameters. The initial learning rate is 0.01, employing an annealing cosine learning rate schedule. The initial temperature for the Gumbel-Softmax is set to 5. For the CIFAR-10/100 and ImageNet datasets, we search for 90 and 45 epochs on eight NVIDIA GeForce RTX 4090 GPUs, respectively. In Eq. 8, we select K as 3. In Eq. 9, $\lambda _ { E }$ , $\lambda _ { L }$ , and $\lambda _ { A }$ are all set to 0.33. In retrain stage, we train the subnet for 600 epochs for CIFAR-10/100 and 180 epochs for ImageNet, respectively. We employ an annealing cosine learning rate schedule, with an initial learning rate of 0.01.

# Search Space

We utilize FBNet (Wu et al. 2019) as the network search space. Except for stem and head layers, it comprises 22 blocks. Each block has 9 candidate operations, including a skip choice. We utilize BitFusion (Sharma et al. 2018) accelerator as the hardware template, which is a SOTA ASIC accelerator for mixed-precision models. For the search space of bitwidths, the weights and activations of each layer have three different options $\in [ 2 , 4 , 8 ]$ . For the accelerator search space, PEx and PEy are selectable within a range of 3 to 64. The cache sizes for weights, activations, and outputs are configurable in increments of 16KB, ranging from 64KB to 528KB, offering 30 distinct choices. We choose three types of dataflows: Weight Stationary (WS) (Jouppi et al. 2017), Output Stationary (OS) (Du et al. 2015), and Row Stationary (RS) (Chen et al. 2016). For each operator, there are 120 possible permutations of the tile order across five dimensions: batch size, input channel, output channel, output height, and output width. For tile sizes, we set the batch size to only one, while in other dimensions, the tile size can vary from $2 ^ { 0 }$ to $2 ^ { n }$ (The maximum value of n is 10).

# Co-exploration Results

<html><body><table><tr><td></td><td colspan="2">入=0.004</td><td colspan="2">入=0.002</td><td colspan="2">入=0.001</td><td colspan="2">入=0.0005</td></tr><tr><td></td><td>A</td><td>E</td><td>A</td><td>E</td><td>A</td><td>E</td><td>A</td><td>E</td></tr><tr><td>Auto-nba</td><td>82.847</td><td>10</td><td>89.643</td><td>12.8</td><td>86.597</td><td>20</td><td>86.677</td><td>26</td></tr><tr><td>Ours</td><td>91.081</td><td>11.8</td><td>92.163</td><td>12.4</td><td>91.895</td><td>17.6</td><td>92.183</td><td>30</td></tr><tr><td colspan="9">(a)CIFAR10</td></tr><tr><td colspan="3">入=0.004</td><td colspan="2">入=0.002</td><td colspan="2">入=0.001</td><td colspan="2">入=0.0005</td></tr><tr><td colspan="3">A</td><td colspan="2">A E</td><td colspan="2">A E</td><td colspan="2">A E</td></tr><tr><td>Auto-nba</td><td>60.169</td><td>E 4</td><td>52.837</td><td>6.2</td><td>56.468</td><td colspan="2">14</td><td>48.542 18</td></tr><tr><td>Ours 72.440</td><td colspan="6">2.6 72.956 7.8</td><td colspan="2">73.651 14.2</td></tr><tr><td colspan="9">(b) CIFAR100</td></tr><tr><td colspan="3">入=0.005</td><td colspan="3">入=0.002</td><td colspan="2">入=0.001</td></tr><tr><td colspan="3">A</td><td>E</td><td colspan="2">A E</td><td>A</td><td>E</td></tr><tr><td>Auto-nba</td><td colspan="2">62.423</td><td colspan="2">32.48</td><td>253</td><td colspan="2">62.787</td><td></td></tr><tr><td></td><td colspan="2"></td><td colspan="2">61.781</td><td colspan="2"></td><td></td><td>503.1</td></tr><tr><td>Ours</td><td>69.132</td><td colspan="2">26.238</td><td>69.473</td><td colspan="2">230.1</td><td>70.197</td><td>498.6</td></tr></table></body></html>

Compared with previous joint search framework ( $\mathrm { F u }$ et al. 2021), we conduct experiments on the CIFAR-10, CIFAR100, and ImageNet (ILSVRC2012) datasets. In various comparative experiments, we adjusted $\lambda$ parameter in Eq. 4 to achieve different balances between accuracy and hardware cost. Specifically, on the CIFAR-10 and CIFAR-100 datasets, the value of $\lambda$ is set to 0.0005, 0.001, 0.002, and 0.004, while on ImageNet, it is set to 0.001, 0.002, and 0.005. As shown in Tab. 4, the experiments reveal that our method significantly outperforms baseline in low-bit joint search tasks.

To demonstrate the efficiency of the JAQ, we conduct comparative analyses with other search frameworks. As

(c) ImageNet

Table 4: Comparisons between our method and the baseline(Auto-nba (Fu et al. 2021)) on three distinct datasets: CIFAR-10, CIFAR-100, and ImageNet. EDAP $_ { \textrm { \tiny . J } }$ $s \cdot m ^ { 2 } \cdot 1 0 ^ { - 1 8 } ,$ ) stands for the Energy-Delay-Area Product, which is a common hardware metric. A indicates accuracy and E indicates EDAP.

Table 5: Comparison of search space(Network, Bitwidth and Accelerator) and search time(T)(GPU hours) between JAQ and other works on the ImageNet dataset.   

<html><body><table><tr><td>Method</td><td>Net.</td><td>Bit.</td><td>Acc.</td><td>T</td></tr><tr><td>NAAS(Lin,Yang,and Han 2021)</td><td></td><td>一</td><td>√</td><td>1200</td></tr><tr><td>OQAT (Shen et al. 2021)</td><td>√</td><td>√</td><td></td><td>1200</td></tr><tr><td>BatchQuant (Bai etal. 2021)</td><td>√</td><td>√</td><td></td><td>1800</td></tr><tr><td>Auto-nba (Fu etal. 2021)</td><td>√</td><td>√</td><td>√</td><td>180</td></tr><tr><td>Ours</td><td>√</td><td>√</td><td>√</td><td>160</td></tr></table></body></html>

shown in Tab. 5, NAAS (Lin, Yang, and Han 2021) employs reinforcement learning (RL) to jointly search network structures and accelerator architectures. OQAT (Shen et al. 2021) and BatchQuant (Bai et al. 2021) utilize a oneshot approach for joint searching of network structures and bitwidths. In contrast, Auto-nba (Fu et al. 2021) and our work both present a triple search framework, but our work achieves better search efficiency within a large search space.

Hardware design must take into account the actual requirements for energy, latency, and area. Some accelerators are specifically designed to minimize power consumption and latency for deployment on embedded platforms, while others are produced to occupy a tiny area for integration into System on Chips (SoCs). The JAQ method can satisfy the sensitivity of a specific metric by adjusting the parameters in Eq. 9. As shown in Tab. 6, for instance, increasing the $\lambda _ { L }$ results in a low latency in the final result. Conversely, increasing the $\lambda _ { A }$ leads to a tiny area for the accelerator. Overall, this indicates that by adjusting the cost hyperparameters, JAQ can achieve a desired solution.

君 6 福 书书 E 8 A 1 LE H 福 业 建 电 2U 1 U ?5 福 213 店 1 ， Kernel Size, Expension, Group, [Activ Bitwidth], [Weight Bitwidth] Tile Loop Activation Cache 512KB Weight Cache 272KB Activation For n in range(OW): ArPrEay Weight For k in range(OH): Output FoFrocriyn irnanragen(gBe)(:IC): Stationary Partial sum Dataflow For x in range(OC): Output Cache 480KB Processing Element

Table 6: Different hardware sensitivity(Latency-S and AreaS) experiments on CIFAR-100 dataset with different metrics(Latency, Energy and Area).   

<html><body><table><tr><td></td><td>入E 入L</td><td>入A</td><td>Acc</td><td>Lat. (ms)</td><td>En. (mJ)</td><td>Ar. (mm²)</td></tr><tr><td>Latency-S</td><td>0.1 0.8</td><td>0.1</td><td>75.099</td><td>1.94</td><td>1.52</td><td>1.36</td></tr><tr><td>Area-S</td><td>0.1</td><td>0.1 0.8</td><td>73.641</td><td>2.74</td><td>2.43</td><td>0.69</td></tr></table></body></html>

# Ablation Studies

Under low bit search condition, to demonstrate the effectiveness of the channel-wise sparse quantization algorithm in addressing GPU memory bottleneck problem, we contrast JAQ with a previous work (Auto-nba (Fu et al. 2021)) tackling the same problem. Auto-nba introduces a method called heterogeneous sampling which employs the StraightThrough Estimator (STE)(Bengio, Le´onard, and Courville 2013) to mask the quantization operation during updating weight parameters. While updating architecture parameters, it employs hard Gumbel-Softmax to active only one bitwidth choice to save GPU memory. However, this method encounters two severe drawbacks under low bit search condition. First, as shown in Appendix B Fig. 5a, the architecture parameters of the operators suffer from significant parameter coupling during training, making it challenging to distinguish them effectively. Second, without any constraint, each bitwidth allocation will most likely converge to the maximum value within the candidate range, rather than selecting low bitwidths that severely impact performance. Yet, as depicted in Appendix B Fig. 5c, many operators ultimately select the 2-bit configuration, leading to a serious misguided search.

Furthermore, we conduct joint search of network structures and bitwidths allocation without any constraint. As shown in Tab. 7, in the first experiment, Auto-nba utilizes heterogeneous sampling to address memory explosion but suffers from severe misguided search, only achieving 55.7 top-1 accuracy. In the second experiment, we also employ the channel-wise concept but quantize only the first channel of each activation during the search process. This approach still suffers from $5 \%$ misguided searches, indicating that fixing the selection of channels is inappropriate. Instead, selecting important channels within each layer is preferable. The third and fourth experiments implement our channelwise sparse quantization algorithm, setting $K$ in Eq. 8 to 1 and 5, respectively. Using Eq. 6, we selectively quantize the most important channels, effectively eliminating misguided search problem and achieving significantly higher accuracy than the previous work.

Table 7: Comparison of different methods for the joint search of network structures and the allocation of 2, 3, and 4-bit bitwidths without any constraint on the CIFAR-100 dataset.   

<html><body><table><tr><td>Method</td><td>Misguided Search (%)</td><td>Top-1 Accuracy</td></tr><tr><td>Auto-nba (Fu et al.2021)</td><td>40</td><td>55.704</td></tr><tr><td>Channel 0</td><td>5</td><td>64.355</td></tr><tr><td>Ours(K=1)</td><td>0</td><td>65.377</td></tr><tr><td>Ours(K=5)</td><td>0</td><td>65.863</td></tr></table></body></html>

# Visualization

Fig. 3 indicates that convolutions with the kernel size of 5 are more compatible with the JAQ accelerator architecture, and larger kernel sizes can achieve higher accuracy with low bitwidth. Because there are more activations and outputs than weights, they are allocated a larger cache size in the search result. The output stationary dataflow is particularly well-suited to the network structure of JAQ, providing superior hardware performance.

# Conclusion

In this paper, we present JAQ, which is the first to implement joint optimization across three dimensions: network structure, ultra-low mixed-precision bitwidths, and accelerator architecture. By addressing the challenges of memory explosion and search overhead of accelerator architecture, JAQ enables efficient joint optimization within a vast search space. When benchmarking with SOTA works, we achieve superior performance. We believe that JAQ can provide inspiration and support to the field of software-hardware codesign.