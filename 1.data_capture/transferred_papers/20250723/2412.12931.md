# Multi-Subspace Matrix Recovery from Permuted Data

Liangqi Xie, Jicong Fan\*

School of Data Science, The Chinese University of Hong Kong, Shenzhen 222041028@link.cuhk.edu.cn, fanjicong $@$ link.cuhk.edu.cn

# Abstract

This paper aims to recover a multi-subspace matrix from permuted data: given a matrix, in which the columns are drawn from a union of low-dimensional subspaces and some columns are corrupted by permutations on their entries, recover the original matrix. The task has numerous practical applications such as data cleaning, integration, and deanonymization, but it remains challenging and cannot be well addressed by existing techniques such as robust principal component analysis because of the presence of multiple subspaces and the permutations on the elements of vectors. To solve the challenge, we develop a novel four-stage algorithm pipeline including outlier identification, subspace reconstruction, outlier classification, and unsupervised sensing for permuted vector recovery. Particularly, we provide theoretical guarantees for the outlier classification step, ensuring reliable multi-subspace matrix recovery. Our pipeline is compared with state-of-the-art competitors on multiple benchmarks and shows superior performance.

# 1 Introduction

# 1.1 Background and Motivation

Permutation is a critical form of data corruption in many applications such as computer vision, data integration, and privacy protection, and hence therefore requires significant attention. This section highlights two key applications: record linkage and de-anonymization, related to data integration and privacy protection, respectively. In record linkage, the goal is to integrate data from different sources for analysis (Fellegi and Sunter 1969; Muralidhar 2017). Columns of a data matrix, gathered independently, may not correspond to the same entity in each row. Thus, reordering or recovering these columns is essential for accurate analysis. In de-anonymization, data providers anonymize information by shuffling the columns of the ground-truth data matrix before release. Recovering the original data becomes a reverse process of data protection (Domingo-Ferrer and Muralidhar 2016). This has practical implications, especially in healthcare and finance, where data integrity is crucial.

Typically, the ground-truth data matrix has a lower rank than the permuted version. However, our study addresses a more general scenario where the ground-truth matrix can be full-rank (Fan and Chow 2018; Fan and Udell 2019; Fan, Zhang, and Udell 2020), leading us to explore Permuted Matrix Recovery with Multi-Subspace Data.

# 1.2 Related Work

While several studies address label-entity mismatches, only two focus on matrix recovery. The first, (Yao, Peng, and Tsakiris 2021), combines robust PCA (Cande\`s et al. 2011) with unlabeled sensing, estimating the ground-truth subspace first and using it to recover permuted data. The second, (Tang et al. 2021), estimates the permutation matrix within a Birkhoff polytope, minimizing an objective inspired by nuclear norm minimization. This reformulates the problem into continuous optimization within the polytope, solved via proximal gradient methods and the Sinkhorn algorithm (Cuturi 2013).

Each method has strengths and limitations. The method proposed by (Yao, Peng, and Tsakiris 2021) works well for sparsely permuted data or low-dimensional subspaces but struggles with missing data, whereas the method proposed by (Tang et al. 2021) handles such data but assumes permuted clusters of outliers, a condition that may not always hold. Additionally, the method of (Tang et al. 2021) is sensitive to initial conditions and assumes a low-rank subspace, with a time complexity of $\mathcal { O } ( n ^ { 2 } )$ , limiting its scalability in high-dimensional scenarios. Importantly, both methods focus on a single ground-truth subspace, overlooking the more complex scenario involving multi-subspace data (widely existing in many areas such as computer vision and signal processing), which we aim to address. While the robust kernel PCA proposed by (Fan and Chow 2019) removes sparse noise from high-rank matrices, it does not effectively handle permuted data.

# 1.3 Contributions of This Work

We generalize the traditional single-subspace permuted matrix recovery problem into multi-subspace scenarios and propose a four-step pipeline for recovering a multi-subspace matrix corrupted by permutation. Within the pipeline, we introduce an efficient method for the outlier classification step and provide theoretical guarantees to support the algorithm and practical applications.

# 2 Methodology

# 2.1 Problem Formulation

Let $\textbf { G } ~ \in ~ \mathbb { R } ^ { M \times N }$ be a clean data matrix of which the columns are randomly drawn from a union of $\textit { L r }$ - dimensional subspaces $\{ \check { S _ { k } } \} _ { k = 1 } ^ { L }$ , where $1 \leq r < M$ . Suppose $N _ { \mathbf { Y } }$ columns of $\mathbf { G }$ , forming a matrix $\mathbf { Y } \in \mathbb { R } ^ { M \times N _ { \mathbf { Y } } }$ , are corrupted by permutations, that is, for $i \in [ N _ { \mathbf { Y } } ]$ ,

$$
\tilde { \mathbf { y } } _ { i } = \mathbf { P } _ { i } \mathbf { y } _ { i }
$$

where $\mathbf { P } _ { i } \in \mathcal { P } _ { M }$ is a partial permutation matrix. An illustrative example for $\mathbf { P } _ { i }$ when $M = 3$ is $[ 1 0 0 ; 0 0 1 ; 0 1 0 ]$ . The rest $N _ { \mathbf { X } }$ columns of $\mathbf { G }$ , forming a matrix $\mathbf { X } \in \mathbb { R } ^ { M \times N _ { \mathbf { X } } }$ , remain unchanged, where $N _ { \mathbf { X } } + N _ { \mathbf { Y } } = N$ . Then the corrupted data matrix is denoted as $\widetilde { \mathbf { G } }$ , consisting the columns of $\mathbf { X }$ and $\widetilde { { \mathbf Y } }$ , where $\widetilde { \mathbf { Y } } = [ \widetilde { \mathbf { y } } _ { 1 } , \widetilde { \mathbf { y } } _ { 2 } , \ldots , \widetilde { \mathbf { y } } _ { N \mathbf { Y } } ]$ . We call the columns of $\widetilde { { \mathbf Y } }$ eoutliers oer convenience. Our goal is to recover $\mathbf { G }$ from $\widetilde { \mathbf { G } }$ eThis task is more challenging if $L , r$ , or $\textstyle { \frac { N \mathbf { \mathbf { \mathbf { \mathbf { \mathbf { \mathbf { \phi } } } } } } } { N } }$ is larger. It heas numerous real applications, e.g.:

• Data Cleaning: In many real scenarios such as health care (Talebi et al. 2020), the samples in a dataset may be drawn from a union of subspaces (corresponding to different groups or clusters) and the attribute names of many samples may be missing or incorrect due to recording mistakes or technical errors. It is important to identify these samples and recover the true orders of the attributes, such that the performance of downstream tasks is reliable.

• Multi Dataset De-anonymization: Data permutation methods are pivotal in data privacy and anonymization, particularly with multisubspace data (Byun et al. 2006) (Ji, Mittal, and Beyah 2016). De-anonymization challenges escalate with the presence of multiple subspaces or latent structures within datasets, which can be exploited for re-identification. This risk intensifies in multisubspace contexts due to potential overlapping information. Consequently, adapting de-anonymization techniques to address multi-subspace scenarios is essential to mitigate privacy breaches effectively.

# 2.2 Proposed Method

Using a single technique such as a denoising algorithm (e.g. robust PCA) to directly recover $\mathbf { G }$ from $\widetilde { \mathbf { G } }$ is often infeasible because several important and commoenly-used assumptions such as I.I.D. and low-rankness do not hold in the problem. For example, when there is no overlap between the subspaces, $\mathbf { G }$ is full-rank if $L r \geq M$ . Therefore, we propose a pipeline consisting of four stages to address the challenge. The four stages are outlier detection, subspace clustering and estimation, outlier classification, and matrix recovery, respectively. Our method, termed Permuted Multi-Subspace Data Recovery (PMSDR), is summarized in Algorithm 1. In the following context, we elaborate on the four stages.

Step 1: Outlier Detection from $\widetilde { \mathbf { G } }$ Outlier detection (Hodge and Austin 2004) is a critic setep in numerous data analysis tasks. Over the past few decades, a variety of methods have been developed to address this problem, ranging

# Algorithm 1: Permuted Multi-Subspace Data Recovery Pipeline (PMSDR)

# Input:

• Observed matrix $\widetilde { \mathbf { G } }$ consisting the columns of $\mathbf { X }$ and $\widetilde { { \mathbf Y } }$   
• Recovery rank $r$   
• Subspace number $L$

# Output:

• Recovered data matrix $\widehat { \mathbf { G } }$

1: Step 1: Outlier Detectbion. Seperate columns of $\widetilde { \mathbf { G } }$ into $\hat { \mathbf { X } }$ and $\hat { \tilde { \mathbf { Y } } }$ using the $l _ { 1 }$ -norm of sparse self-represenetation cboefficibents (You, Robinson, and Vidal 2017). 2: Step 2: Subspace Reconstruction. Cluster columns of $\hat { \mathbf { X } }$ into $L$ groups to obtain matrices $\{ \widehat { \mathbf { X } } _ { 1 } , \ldots , \widehat { \mathbf { X } } _ { L } \}$ and ebstimate an $r$ -dim basis $\widehat { \mathbf { U } } _ { k }$ for each $\widehat { \mathbf { X } } _ { k }$ using bSVD. 3: Step 3: Outlier Classifibcation. As obciate each outlier $\tilde { \mathbf { y } } _ { j }$ with its respective subspace $\boldsymbol { S } _ { k _ { j } }$ using Algorithm 2. 4: Step 4: Matrix Recovery. Recover each $\tilde { \mathbf { y } } _ { j }$ w.r.t. its corresponding subspace $\boldsymbol { S } _ { k _ { j } }$ using unsupervised sensing techniques such as UPCA (our default approach) (Yao, Peng, and Tsakiris 2021). 5: Return: and $\widehat { \mathbf { G } } = [ \widehat { \mathbf { X } } , \widehat { \mathbf { Y } } ]$

$\widehat { \mathbf { Y } } = [ \widehat { \mathbf { y } } _ { 1 } , \ldots , \widehat { \mathbf { y } } _ { N _ { \widehat { \mathbf { Y } } } } ]$ from traditional statistical approaches to more advanced machine learning techniques (Boukerche, Zheng, and Alfandi 2020). While these methods can be effective in certain scenarios, they may face challenges when dealing with highdimensional or complex data. One may consider robust PCA (Cand\`es et al. 2011; Fan et al. 2019) but it requires the lowrank assumption, which does not hold in our problem.

To identify the outliers in $\widetilde { \mathbf { G } }$ , we propose to use a method called Provable Self-Repre entation Matrix (PSRM) given by (You, Robinson, and Vidal 2017). We construct a selfrepresentation matrix $\mathbf { R } = ( r _ { i j } ) \in \mathbb { R } ^ { n \times n }$ by solving the following elastic net problem

$$
\begin{array} { r } { \underset { \mathbf { r } _ { j } } { \operatorname* { m i n } } \lambda \| \mathbf { r } _ { j } \| _ { 1 } + \frac { 1 - \lambda } { 2 } \| \mathbf { r } _ { j } \| _ { 2 } ^ { 2 } + \frac { \gamma } { 2 } \| \tilde { \mathbf { g } } _ { j } - \widetilde { \mathbf { G } } \mathbf { r } _ { j } \| _ { 2 } ^ { 2 } \quad \mathrm { s . t . } \ r _ { j j } = 0 , } \end{array}
$$

and then build the transition matrix $\mathbf { P } = ( p _ { i j } ) \in \mathbb { R } ^ { M \times M }$ by

$$
p _ { i j } = \left| r _ { j i } \right| / \| \mathbf { r } _ { i } \| _ { 1 } \quad \mathrm { f o r } \mathrm { a l l } \{ i , j \} \subseteq [ M ] ,
$$

thus forming a stochastic process. By initializing with an initial discrete union distribution, the probability mass will, over successive steps, provably concentrate on inliers under certain assumptions. This probabilistic behavior allows us to identify and separate outliers from inliers. We denote the estimated inliers and outliers in $\tilde { \mathbf { G } }$ as $\hat { \mathbf { X } }$ and $\hat { \tilde { \mathbf { Y } } }$ , respectively.

Step 2: Subspace Reconstruction Given $\widehat { \mathbf { X } }$ , we need to cluster its columns into $L$ groups to o abin matrices $\{ \widehat { \mathbf { X } } _ { 1 } , \ldots , \widehat { \mathbf { X } } _ { L } \}$ corresponding to different subspaces. There isba large  ibterature on the issue of subspace clustering in recent decades (Elhamifar and Vidal 2013; Liu et al. 2012; Fan 2021; Cai et al. 2022). Here we simply use SSC (Elhamifar and Vidal 2013) as the subspace clustering method. One can

Algorithm 2: Outlier Classification

Input: Estimated bases for all subspaces $\widehat { \mathbf { U } } _ { 1 } , \ldots , \widehat { \mathbf { U } } _ { L } \in \mathbb { R } ^ { M \times r }$ ; One outlier sample $\tilde { \mathbf { y } } \in \mathbb { R } ^ { M }$   
Initialized Parameters: Retain ratio $\gamma$ ; Mbaximumbiterations max iter   
Output: Corresponding subspace label $t \in [ L ]$   
1: Initialize elimination numbers $m = [ m _ { 1 } , m _ { 2 } , \ldots , m _ { i t e r } ]$ , where each $m _ { i } \in \mathbb { Z } ^ { + }$ is in descending order, iter $\leq$ max iter, and $\begin{array} { r } { \sum _ { i = 1 } ^ { i t e r } m _ { i } = \lfloor ( n - r ) * ( 1 - \gamma ) \rfloor } \end{array}$ .   
2: for $k = 1$ to $L$ do   
3: Initialize $\pmb { \nu } ^ { ( 0 ) } = \tilde { \mathbf { y } }$ and $\mathbf { B } ^ { ( 0 ) } = \widehat { \mathbf { U } } _ { k }$ .   
4: for $i = 1$ to iter do   
5: $\begin{array} { r } { \left[ \hat { j } _ { 1 } , \hat { j } _ { 2 } , \ldots , \hat { j } _ { m _ { i } } \right] = \operatorname { a r g m a x } _ { j _ { 1 } , j _ { 2 } , \ldots , j _ { m _ { i } } } \left| \pmb { \nu } ^ { ( i - 1 ) } - \mathbf { B } ^ { ( i - 1 ) } \mathbf { B } ^ { ( i - 1 ) } \mathbf { \Lambda } ^ { \dagger } \pmb { \nu } ^ { ( i - 1 ) } \right| } \end{array}$   
6: Remove the $[ \hat { j } _ { 1 } , \hat { j } _ { 2 } , \dots , \hat { j } _ { m _ { i } } ]$ -th entriesfrom $\nu ^ { ( i - 1 ) }$ to get $\nu ^ { ( i ) }$ .   
7: Remove the $[ \hat { j } _ { 1 } , \hat { j } _ { 2 } , \dots , \hat { j } _ { m _ { i } } ]$ -th rows from $\mathbf { B } ^ { ( i - 1 ) }$ to get $\mathbf { B } ^ { ( i ) }$ and refined subspace $S _ { k } ^ { ( i ) }$   
8: end for   
9: Calculate the subspace distance $d _ { k } = 1 - \cos \Big ( \nu ^ { ( i t e r ) } , S _ { k } ^ { ( i t e r ) } \Big )$

# 10: end for

11: Determine the subspace label $t = \operatorname { a r g m i n } _ { k } d _ { k }$ utilize any other method as an alternative when needed. It’s also worth noting that like most subspace clustering methods, the more samples there are, the better the performance is, which indicates a future direction to enhance the performance of subspace clustering by other tricky techniques.

For subspace estimation, we utilize the Singular Value Decomposition (SVD) to compute basis vectors $\bar { \hat { \mathbf { U } } } _ { k }$ for each subspace $\boldsymbol { \mathcal { S } } _ { k }$ , i.e., $\mathbf { X } _ { k } = \mathbf { U } _ { k } \pmb { \Sigma } _ { k } \mathbf { V } _ { k } ^ { \top }$ , where we debfine $\widehat { \mathbf { U } } _ { k }$ as the first $r$ columns of $\mathbf { U } _ { k }$ . These basis vectors are ar abnged in descending order based on their corresponding singular values. Therefore, the process involves selecting the top $r$ eigenvectors to form a basis for $s$ . Alternatives like DPCP (Zhu et al. 2019) can also be used.

Step 3: Outlier Classification To recover $\mathbf { Y }$ from $\hat { \tilde { \mathbf { Y } } }$ using $\bf \widehat { \{ U }  _ { \boldsymbol { k } } \} _ { \boldsymbol { k } = 1 } ^ { L }$ , we need to find the corresponding $\widehat { \mathbf { U } } _ { k }$ orbesubspace for each column of $\hat { \tilde { \mathbf { Y } } }$ first. This is essentia ly a classification task but the elem ents of each column in $\dot { \hat { \mathbf { Y } } }$ are partially permuted, which leads to a considerable chabellenge.

Inspired by the UPCA method proposed by (Yao, Peng, and Tsakiris 2021), we propose an efficient algorithm to resolve the challenge, shown in Algorithm 2. It iteratively eliminates unimportant entries to identify the true correspondence between an outlier and its subspace. Given an outlier

$$
\tilde { \mathbf { y } } = \left[ \mathbf { y } _ { } ^ { ( \mathbf { 1 } ) } \right]
$$

with $\tilde { \mathbf { y } } ^ { ( \mathbf { 2 } ) }$ being fully permuted, and a basis of one subspace

$$
\widehat { \mathbf { U } } _ { k } = \left[ \widehat { \mathbf { U } } _ { k } ^ { ( 1 ) } \right] \quad ( k = 1 , \ldots , L ) ,
$$

the core idea is to eliminate entries in $\tilde { \mathbf { y } } ^ { ( \mathbf { 2 } ) }$ while retaining $\mathbf { y } ^ { ( \mathbf { 1 } ) }$ as completely as possible. Then, we compare the cosine distance between the retained vector $\mathbf { y } ^ { ( \mathbf { 1 } ) }$ and each subspace

$\boldsymbol { \mathcal { S } } _ { k }$ using the formula:

$$
d _ { k } = 1 - \cos \left( \mathbf { y } ^ { ( 1 ) } , \widehat { \mathbf { U } } _ { k } ^ { ( 1 ) } \left( \widehat { \mathbf { U } } _ { k } ^ { ( 1 ) } \right) ^ { \dagger } \mathbf { y } ^ { ( 1 ) } \right) ,
$$

and select the subspace with the minimum distance as the estimated subspace class.

Specifically, the algorithm first initializes the total number of steps iter and the number of eliminated entries $m _ { i }$ in $i$ -th step, which significantly enhances the efficiency. For each subspace $\boldsymbol { \mathcal { S } } _ { k }$ , the algorithm initializes the remaining vector $\boldsymbol { \nu } ^ { ( 0 ) } = \tilde { \mathbf { y } }$ and the basis matrix $\mathbf { B } ^ { ( 0 ) } = \widehat { \mathbf { U } } _ { k }$ . During each iteration, the algorithm performs least sq bres regression:

$$
\begin{array} { r } { \hat { \pmb { \nu } } ^ { ( i - 1 ) } = \mathbf { B } ^ { ( i - 1 ) } \left( \mathbf { B } ^ { ( i - 1 ) } \right) ^ { \dagger } \pmb { \nu } ^ { ( i - 1 ) } } \end{array}
$$

and removes the largest $m _ { i }$ entries in the residual

$$
\left. \pmb { \nu } ^ { ( i - 1 ) } - \hat { \pmb { \nu } } ^ { ( i - 1 ) } \right.
$$

from both $\nu$ and $\mathbf { B }$ . After completing the iterations, the subspace distance is calculated to determine the subspace label $t$ that minimizes the cosine distance between the remaining vector $\pmb { \nu } ^ { \mathrm { ( i t e r ) } }$ and the subspace $S _ { k } ^ { ( \mathrm { i t e r } ) }$ .

Intuitively, the outlier classification method is easy to understand. The elimination of the largest residuals in each iteration ensures that the remaining data points better represent the underlying subspace structure. By iteratively refining the basis matrix $\mathbf { B }$ and the residual vector $\nu - \hat { \nu }$ , the algorithm effectively isolates the outlier and aligns it with the correct subspace. From a theoretical perspective, under mild assumptions, we provide approximate guarantees for the effectiveness of this method, with experimental analysis supported, which will be elaborated in the Appendix.

Step 4: Matrix Recovery It is necessary to provide an outline of unlabeled sensing for completeness. In a nutshell, unlabeled sensing methods (Yao, Peng, and Tsakiris 2021; Slawski and Ben-David 2019) are proposed for solving linear equation systems with unordered measurements:

$$
\mathbf { y } = \pi \mathbf { U } \mathbf { x }
$$

where $\pi$ is an unknown permutation matrix, with the knowledge of $\mathbf { y }$ and $\mathbf { U }$ (Unnikrishnan, Haghighatshoar, and Vetterli 2015). Different unlabeled sensing methods are brought into practice according to the rank of the basis $\mathbf { U }$ and the type of shuffling (partially shuffled or fully shuffled).

During the matrix recovery step (step 4 in Algorithm 1), we employ an unlabeled sensing method to iteratively recover each outlier $\tilde { \mathbf { y } } _ { t }$ with its corresponding basis $\widehat { \mathbf { U } } _ { t }$ . Alternatively, matrix recovery methods like robust PCAb(Cande\`s et al. 2011), LRR (Liu et al. 2012), and RKPCA (Fan and Chow 2019) could also be utilized to recover $[ \widehat { \mathbf { X } } _ { t } , \widetilde { \mathbf { Y } } _ { t } ]$ associated with subspace $\mathbf { \mathcal { S } } _ { t }$ .

# 3 Theory for Outlier Classification

In this section, we provide a theoretical guarantee for Algorithm 2. To begin with, we have the following assumptions.

Assumption 1. The variables $( \tilde { \bf y } - \hat { \tilde { \bf y } } ) _ { 1 } , \dots , ( \tilde { \bf y } - \hat { \tilde { \bf y } } ) _ { M _ { 1 } }$ are independent and identically distributed (i.i.d.) following a distribution denoted by $\xi$ . Similarly, the variables $( \bar { \mathbf { y } } - \hat { \tilde { \mathbf { y } } } ) _ { M _ { 1 } + 1 } , \ldots , ( \tilde { \mathbf { y } } - \hat { \tilde { \mathbf { y } } } ) _ { M }$ are i.i.d. following a distribution denoted by $\eta$ . Both $\xi$ and $\eta$ belong to the same bellshaped distribution cluster, with a mean value $\mu _ { \xi } = \mu _ { \eta } \triangleq $ $\mu = 0$ , differing only in their variances $\sigma _ { \xi } ^ { 2 } \ne { \bar { \sigma } _ { \eta } } ^ { 2 } ,$ which means their cumulative distribution functions satisfy:

$$
\begin{array} { r } { F _ { \xi } ( \sigma _ { \xi } x ) = F _ { \eta } ( \sigma _ { \eta } x ) \triangleq F ( x ) , } \end{array}
$$

where $F ( x )$ is the cdf of their normalized distribution with variance $\textstyle \int _ { \mathbb { R } } x ^ { 2 } d F ( x ) = 1$ .

Assumption 2. For the sake of brevity, we assume that the bell-shaped distribution in Assumption $\jmath$ is given by $F ( x ) =$ $\Phi ( x )$ , where $\Phi ( x )$ denotes the cdf of the standard Gaussian distribution.

We defer the detailed discussion on the assumptions to Appendix. Now we present the following theorem:

Theorem 1. Under Assumptions $\jmath$ and 2, and without loss of generality, let $\mathcal { A } \triangleq \{ i \in \mathbb { Z } ^ { + } : 1 \leq i \leq M _ { 1 } \}$ represent the unshuffled indices and $\mathcal { O } \triangleq \{ i \in \mathbb { Z } ^ { + } : M _ { 1 } + 1 \leq i \leq$ $M \}$ represent the shuffled indices of y. Thus $M _ { 1 } = \# ( { \mathcal { A } } )$ , $M _ { 2 } = \# ( \mathcal { O } ) = M - M _ { 1 }$ . Define

$$
\hat { j } = \mathrm { a r g m a x } _ { j } \left| \left( \mathbf y - \hat { \tilde { \mathbf y } } \right) _ { j } \right| .
$$

Then, approximately,

$$
\mathrm { P r } \left( \hat { j } \in \mathcal { O } \right) \approx 2 \Phi \left( \frac { \sigma _ { \eta } \rho ( M _ { 2 } ) - \sigma _ { \xi } \rho ( M _ { 1 } ) } { \sqrt { \sigma _ { \eta } ^ { 2 } \psi ^ { 2 } ( M _ { 2 } ) + \sigma _ { \xi } ^ { 2 } \psi ^ { 2 } ( M _ { 1 } ) } } \right) - 1 ,
$$

where

$$
\begin{array} { c c c } { \displaystyle { \rho ( m ) = F ^ { - 1 } ( 1 - \frac { 1 } { m } ) = \Phi ^ { - 1 } ( 1 - \frac { 1 } { m } ) , } } \\ { \displaystyle { \psi ( m ) = \frac { 1 } { m \cdot f ( \rho ( m ) ) } = \frac { 1 } { m \cdot \phi ( \rho ( m ) ) } , } } \end{array}
$$

with $f$ (or $\phi _ { , }$ ) being the pdf corresponding to $F$ (or $\Phi$ ) and approximated estimation of $\sigma _ { \xi } ^ { 2 } , \sigma _ { \eta } ^ { 2 }$ as follows:

$$
\left\{ \begin{array} { l l } { \operatorname* { P r } \Big ( \sigma _ { \xi } ^ { 2 } < C \left( \frac { r ( M - r ) M _ { 2 } } { M ^ { 2 } ( M - 1 ) ( M + 2 ) } \right) \Big ) > 1 - \delta } \\ { \mathbb { E } ( \sigma _ { \xi } ^ { 2 } ) \leq \frac { M _ { 2 } r ( M - r ) } { M ^ { 2 } ( M - 1 ) ( M + 2 ) } + \frac { \sqrt { 6 } M _ { 2 } r ^ { 1 / 2 } ( M - r ) ^ { 3 / 2 } } { M ^ { 2 } ( M - 1 ) ( M + 2 ) ^ { 3 / 2 } } + \frac { M _ { 2 } ^ { 2 } } { M ^ { 3 } } } \\ { \mathbb { E } ( \sigma _ { \eta } ^ { 2 } ) \geq \frac { 2 } { M } - \frac { 2 [ M _ { 2 } M + M - 4 ] ( M - r ) } { M ^ { 2 } ( M - 1 ) ( M + 2 ) } + \frac { \Gamma _ { r } \big ( \frac { r } { 2 } + \frac { 3 } { r } \big ) \Gamma _ { r } \big ( \frac { M } { 2 } \big ) } { M _ { 2 } \Gamma _ { r } \big ( \frac { M } { 2 } + \frac { 3 } { r } \big ) \Gamma _ { r } \big ( \frac { r } { 2 } \big ) } } \\ { \mathbb { E } ( \sigma _ { \eta } ^ { 2 } ) \approx \frac { ( 2 M - M _ { 2 } ) } { M ^ { 2 } } \left[ \frac { ( M - r ) ( M - r + 2 ) } { M ( M + 2 ) } + \frac { ( M _ { 2 } - 1 ) r ( M - r ) } { M ( M - 1 ) ( M + 2 ) } \right] } \end{array} \right.
$$

where $\begin{array} { r } { C \le 2 + 6 ( 1 + \sqrt { 2 } ) \sqrt { \frac { M - r } { r M _ { 2 } M } } } \end{array}$ , $\delta \approx 0 . 0 0 5 4$ , and multivariate gamma function

$$
\Gamma _ { r } ( x ) = \pi ^ { \frac { r ( r - 1 ) } { 4 } } \prod _ { i = 1 } ^ { r } \Gamma \left( x - { \frac { i - 1 } { 2 } } \right) .
$$

There are some calculation issues for $\sigma _ { \xi } ^ { 2 }$ and $\sigma _ { \eta } ^ { 2 }$ , which will be detailed in Appendix. Anyway, Theorem 1 ensures that Algorithm 2 successfully recovers the subspace when initialized with the ground truth subspace basis. Specifically, the shuffled ratio $\frac { \bar { M } _ { 2 } } { M }$ in the retaining vector $\nu ^ { ( i ) }$ decreases rapidly as the iteration index $i$ increases. Consequently, $\nu ^ { ( i ) }$ quickly aligns with the ground-truth retaining subspace $\mathbf { \boldsymbol { S } } ^ { ( i ) }$ in terms of cosine distance, eventually approaching zero. Intuitively, this can be understood as the following: with the ratio of retained entries being no more than $\gamma$ (as defined in Algorithm 2), we can confidently assert that most of the shuffled entries have been removed. As a result, the retaining vector $\nu ^ { ( i t e r ) }$ closely approximates the ground-truth retaining subspace Sg(itter).

Conversely, if the process starts with an entirely incorrect subspace, it is analogous to a situation where the data points have been completely shuffled, as discussed in the Appendix. In such a case, the entries are eliminated in a seemingly random fashion, making it impossible to distinguish between shuffled and unshuffled entries. Consequently, Algorithm 2 would fail, as corroborated by our theoretical analysis. In this scenario, the retaining vector $\nu ^ { ( i ) }$ will not converge towards the incorrect subspace $\mathcal { S } _ { w r o n g } ^ { ( i ) }$ at the same rate as when the correct subspace is used. This is because the retaining vector $\nu ^ { ( i t e r ) }$ bears little correlation with $\mathcal { S } _ { w r o n g } ^ { ( i t e r ) }$ , given that the retained entries belong to an unrelated subspace. Thus, by selecting an appropriate stopping criterion $\gamma$ in Algorithm 2, we can effectively differentiate the ground truth subspace label by comparing the cosine distances between each retaining vector νk(i) and its corresponding subspace $S _ { k } ^ { ( i ) }$ , for $k = 1 , \dots , L$ .

# 4 Experimental Evaluation

Before presenting the results of experiments conducted on synthetic and real-world datasets, it is important to clarify the evaluation metrics used to demonstrate the effectiveness of our approach.

Outlier Classification Error involves two metrics, $\mathbf { C E _ { g t } }$ and $\mathbf { C E _ { r e c o n } }$ , that measure outlier classification accuracy. $\mathbf { C E _ { g t } }$ is calculated using the ground truth subspaces, $\mathbf { U } _ { 1 } , \dotsc , \mathbf { U } _ { L }$ , while $\mathbf { C E _ { r e c o n } }$ uses reconstructed bases $\hat { \mathbf { U } } _ { 1 } , \dotsc , \hat { \mathbf { U } } _ { L }$ . Then:

Table 1: Performance comparison of MRUC and MRUCS. Values represent the permutation error ratio $( \% )$ , which is the average normalized Hamming distance between predicted and true permutation matrices. Metrics are reported as mean $\pm$ std ([min, max]) over multiple random initializations.   

<html><body><table><tr><td>(L,p)</td><td>Permutation Error Ratio(%)</td></tr><tr><td>(2,2)</td><td>MRUC-S: 0.0 ± 0.0 ([0, 0]) MRUC:12.0±15.5 ([0,30])</td></tr><tr><td>(3,2)</td><td>MRUC-S: 0.0 ± 0.0 ([0, 0]) MRUC: 30.0 ± 0.0 ([30, 30])</td></tr><tr><td>(3,3)</td><td>MRUC-S: 13.3 ± 16.6 ([0,33.3]) MRUC: 55.0± 1.4 ([53.3,58.3])</td></tr><tr><td>(3,5)</td><td>MRUC-S: 17.0 ± 25.9 ([0, 65]) MRUC: 71.5 ± 13.7 ([56.7,91.7])</td></tr><tr><td>(5,5)</td><td>MRUC-S: 23.0± 17.9 ([0, 50]) MRUC: 73.2 ± 0.6 ([73, 75])</td></tr></table></body></html>

SingleREgt REgt RErecon   
25 0.3 25 0.3 25 0.3   
19 0.2 19 0.2 19 0.2   
r13 113 113 0.1 0.1 0.1 1 0 1 0 1 0 .1 .2 .3 .4 .5 .6 .1 .2 .3 .4 .5 .6 .1.2.3 .4.5 .6 Shuffled ratio Shuffled Ratio Shuffled Ratio   
(a) Recovery Error (b) Recovery Error (c) Recovery Error   
with GT Single Sub- with GT Information with Reconstructed   
space Information SCerr CEgt CErecon   
25 0.1 25 0.3 25 0.3   
19 19 0.2 19 0.2   
113 0.05 r13 113 0.1 0.1 1 0 1 0 1 0 .1 .2 .3 .4 .5 .6 .1 .2 .3 .4 .5 .6 .1 .2 .3 .4 .5 .6 Shuffled Ratio Shuffled Ratio Shuffled Ratio   
(d) Subspace Clus- (e) Outlier Classifi- (f) Outlier Classifi  
tering Error cation Error with GT cation Error with ReInformation constructed Information

$$
\begin{array} { r l } & { \mathbf { C } \mathbf { E } _ { \mathbf { g t } } = \frac { \# ( \mathrm { M i s c l a s s i f i e d ~ O u t l i e r s } ) } { \# ( \mathrm { O u t l i e r s } ) } } \\ & { \mathbf { C } \mathbf { E } _ { \mathbf { r e c o n } } = \frac { \# ( \mathrm { M i s c l a s s i f i e d ~ D e t e c t e d ~ O u t l i e r s } ) } { \# ( \mathrm { D e t e c t e d ~ O u t l i e r s } ) } } \end{array}
$$

Matrix Recovery Error is assessed by $\mathbf { R E _ { g t } }$ and $\mathbf { R E } _ { \mathbf { r e c o n } }$ , which measure the accuracy of recovering outlier columns using normalized Frobenius norms. Specifically:

$$
\begin{array} { r l } & { \mathbf { R E _ { g t } } = \frac { \| \operatorname { P r o j } _ { \mathcal { S } } ( \widehat { \mathbf { Y } } ) - \mathbf { Y } \| _ { F } } { \| \mathbf { Y } \| _ { F } } } \\ & { \mathbf { R E _ { r e c o n } } = \frac { \| \operatorname { P r o j } _ { \mathcal { S } } ( \widehat { \mathbf { Y } } _ { d } ) - \mathbf { Y } _ { d } \| _ { F } } { \| \mathbf { Y } _ { d } \| _ { F } } } \end{array}
$$

where $\hat { \mathbf Y }$ and $\widehat { \mathbf { Y } } _ { d }$ are recovered outliers using ground truth and d ebcted babses, respectively, and $\mathbf { Y }$ and $\mathbf { Y } _ { d }$ are the corresponding ground truth outliers. $\mathrm { P r o j } _ { \cal S } ( \cdot )$ denotes projection onto the subspace.

Auxiliary Metrics include UOratio and SCerr:

$$
\begin{array} { r l } & { \mathrm { U O r a t i o } = \frac { \# ( \mathrm { U n d e t e c t e d ~ O u t l i e r s } ) } { \# ( \mathrm { O u t l i e r s } ) } } \\ & { \mathrm { S C e r r } = \frac { \# ( \mathrm { M i s c l a s s i f i e d ~ D e t e c t e d ~ I n l i e r s } ) } { \# ( \mathrm { D e t e c t e d ~ I n l i e r s } ) } } \end{array}
$$

UOratio evaluates the undetected ratio of outlier detection, while SCerr assesses subspace clustering error, both influencing $\mathbf { C E _ { r e c o n } }$ and $\mathbf { R E } _ { \mathbf { r e c o n } }$ .

# 4.1 Experiment on Synthetic Data

We conduct three experiments: (1) analyzing Algorithm 1, (2) comparing it with RPCA (Cande\`s et al. 2011), RKPCA (Fan and Chow 2019), and SSC (Elhamifar and Vidal 2013), and (3) evaluating the impact of MRUC (Tang et al. 2021) on permutation matrix recovery when Algorithm 1 is augmented for multiple subspaces.

In the first experiment, the ambient dimension $M$ is 50, each subspace has 120 samples, and the outlier proportion is $6 0 \%$ . The number of subspaces is ${ 2 , 3 , 5 , 8 }$ , or 10. The subspace rank varies from 1 to 25, and the shuffled ratio from 0.1 to 0.6, with a noise level of $4 0 ~ \mathrm { d B }$ . The median error across all settings is recorded. Figure 1 compares the estimation error of our method with single subspace results. Figure 1(a) shows UPCA (Yao, Peng, and Tsakiris 2021) on a single subspace as a baseline, while (b) and (c) show multi-subspace results, demonstrating minimal performance loss even with reconstructed information. Figures (e) and (f) highlight the robust outlier classification.

In the second experiment, we compare vanilla methods with their PMSDR-augmented versions, with the number of subspaces fixed at 5. Figure 2 shows a significant enhancement in RPCA and SSC, and a slight improvement for RKPCA when augmented. The third experiment compares PMSDR-augmented MRUC-S with MRUC using Hamming distance as the evaluation metric. The experimental setup includes $M = 2 0$ , $r = 2$ , and a shuffled ratio of 0.5. Table 1 demonstrates the superior performance of PMSDR, highlighting the robustness of Algorithm 1, even under varying subspace configurations.

In summary, our method effectively bridges multi- and single-subspace cases in low-rank scenarios, generalizing single-subspace recovery to multi-subspace contexts.

# 4.2 Experiment on Face Images

We applied our algorithm to the Extended Yale B dataset (Georghiades, Belhumeur, and Kriegman 2001), which in

Table 2: Performance of Experiment on Extended YaleB Dataset   

<html><body><table><tr><td>Mean (Median)</td><td>CEgt</td><td>CErecon</td><td>UOratio</td><td>SCerr</td></tr><tr><td>2 subjects</td><td>0.0000 (0.0000)</td><td>0.0000 (0.0000)</td><td>0.1042 (0.1095)</td><td>0.0137 (0.0053)</td></tr><tr><td>3 subjects</td><td>0.0000 (0.0000)</td><td>0.0110 (0.0105)</td><td>0.0488 (0.0525)</td><td>0.0106 (0.0072)</td></tr><tr><td>5 subjects</td><td>0.0168 (0.0190)</td><td>0.0343 (0.0325)</td><td>0.0333 (0.0250)</td><td>0.0493 (0.0304)</td></tr><tr><td>8 subjects</td><td>0.0245 (0.0230)</td><td>0.0595 (0.0630)</td><td>0.0157 (0.0120)</td><td>0.0959 (0.1007)</td></tr><tr><td>10 subjects</td><td>0.0250 (0.0250)</td><td>0.1002 (0.0845)</td><td>0.0107 (0.0130)</td><td>0.1336 (0.1563)</td></tr><tr><td>12 subjects</td><td>0.0268 (0.0285)</td><td>0.1025 (0.1030)</td><td>0.0093 (0.0100)</td><td>0.1409 (0.1892)</td></tr></table></body></html>

RPCA RKPCA SSC 25 25 25 19 0.4 19 0.4 19 0.4 r13 r13 r13 0.2 0.2 0.2 1 （ 1 0 1 0 .1 .2 .3 .4 .5 .6 .1 .2 .3 .4 .5 .6 .1 .2 .3 .4 .5 .6 Shuffled Ratio Shuffled Ratio Shuffled Ratio (a) Recovery Error (b) Recovery Error (c) Recovery Error for RPCA for RKPCA for SSC RPCA-S RKPCA-S SSC-S 25 25 25 19 0.4 19 0.4 19 0.4 r13 113 r13 0.2 0.2 0.2 1 0 1 0 1 0 .1.2.3.4.5.6 .1.2 .3 .4.5 .6 .1.2 .3.4 .5.6 Shuffled Ratio Shuffled Ratio Shuffled Ratio (d) Recovery Error (e) Recovery Error (f) Recovery Error for RPCA-S for RKPCA-S for SSC-S

cludes 38 subjects, each with 64 downsampled face images of size $4 8 \times 4 2$ $M = 2 0 1 6$ ).

In the first experiment, we selected 10 subjects and corrupted 19 images per group by shuffling $4 0 \%$ of the pixels. We compared our PMSDR method with RPCA (Cande\`s et al. 2011), SSC (Elhamifar and Vidal 2013), and RKPCA (Fan and Chow 2019), as well as their PMSDR-augmented counterparts (RPCA-S, SSC-S, RKPCA-S). The results, depicted in Figure 6, demonstrate that PMSDR substantially improves matrix recovery and outlier correction. A subset of these results is presented in Figure 3, with the complete set available in Appendix B.1.

In the second experiment, we varied the number of subspaces $L$ from 2 to 12 and repeated the corruption process. The results, summarized in Table 2, indicate a strong performance, particularly when the ground truth is known. Specifically, $\mathbf { C E _ { g t } }$ is lower than $\mathbf { C E _ { r e c o n } }$ , due to increased subspace clustering errors SCerr. However, these findings highlight the robustness and adaptability of our algorithm.

![](images/3e72d884bfca035322f08fc1e414fae1df09df8c21648b662fbe6a04bf9c4272.jpg)  
Figure 2: Synthetic experiments on RPCA, RKPCA, SSC, and their PMSDR-augmented versions. All experiments are conducted for sparse permutations with shuffled ratios no greater than 0.6 and subspace dimensions no greater than $50 \%$ of the ambient space dimension, which is 50. The number of subspaces is fixed to 5.   
Figure 3: Experimental results showing a subset of the image recovery experiments. The complete set of results is in Appendix B.1.

Table 3: PMSDR Performance on Hopkins-155   

<html><body><table><tr><td>Subspaces</td><td>Metric</td><td>Mean</td><td>Median</td></tr><tr><td>2 subspaces</td><td>SCerr CEgt CErecon REgt RErecon</td><td>0.011 0.040 0.017 0.021 0.027</td><td>0.000 0.007 0.005 0.013 0.000</td></tr><tr><td>3 subspaces</td><td>SCerr CEgt CErecon REgt RErecon</td><td>0.018 0.093 0.014 0.021 0.043</td><td>0.008 0.023 0.007 0.012 0.004</td></tr></table></body></html>

# 4.3 Experiment on Motion Segmentations

We evaluated our algorithm on the Hopkins-155 database, which includes 117 sequences with 2 subspaces, 35 with 3 subspaces and 1 with 5 subspaces. Each sequence lies in a 4- dimensional subspace (Boult and Brown 1991; Tomasi and Kanade 1992). The shuffled and outlier ratios are both 0.4, with a fixed subspace dimension of 4. Data are mapped from 3D to 2D for better representation, and concatenated over frames. After preprocessing, the dimension of the ambient space ranges from 40 to 70.

We apply our 4-stage pipeline (PMSDR) and compare its performance against RPCA, RKPCA, SSC, and MRUC, along with their PMSDR-augmented variants, denoted by appending the suffix $\cdot _ { - } \mathrm { { S } ^ { \prime } }$ . Additionally, we investigate the impact of using ground-truth information for matrix recovery. The regularization parameters in RPCA, RKPCA, and SSC are tuned accordingly. Results in Table 3 show PMSDR’s robustness in matrix recovery and outlier classification, even without ground truth information. Table 4 highlights further improvements when combining these methods with our pipeline, except for RKPCA pairs. For the experiments involving RKPCA pairs, we discovered an effective technique that greatly improves the performance of vanilla RKPCA. However, this approach showed ineffective for RKPCA-S and $\mathrm { R K P C A _ { g t } }$ -S. The details of this technique are provided in the Appendix.

Table 4: Matrix Recovery Error for Comparison Experiments on Hopkins-155.   

<html><body><table><tr><td>Method</td><td>2 subspaces</td><td>3 subspaces</td></tr><tr><td></td><td>MedianRE(MeanRE)</td><td></td></tr><tr><td>PMSDR PMSDRgt</td><td>0.013(0.021) 0.005 (0.017)</td><td>0.012(0.021) 0.007 (0.014) 0.047(0.055)</td></tr><tr><td>RPCA RPCA-S RPCAgt -S</td><td>0.046(0.052) 0.040 (0.045) 0.033 (0.040)</td><td>0.043 (0.047) 0.030 (0.036)</td></tr><tr><td>RKPCA RKPCA-S RKPCAgt -S</td><td>0.009 (0.014) 0.053 (0.064) 0.039 (0.057)</td><td>0.008 (0.012) 0.047 (0.058) 0.036 (0.049)</td></tr><tr><td>SSC SSC-S SSCgt-S</td><td>0.087 (0.090) 0.091 (0.095) 0.079 (0.084)</td><td>0.100 (0.104) 0.101(0.102) 0.089 (0.090)</td></tr><tr><td>MRUC MRUC-S MRUCgt-S</td><td>0.066 (0.084) 0.048 (0.074) 0.048 (0.070)</td><td>0.085 0.091 0.056 (0.078) 0.056 (0.073)</td></tr></table></body></html>

# 4.4 Experiment on Data Re-identification

We evaluated the proposed PMSDR pipeline alongside the RPCA, SSC, RKPCA, and MRUC methods on real-world educational and medical records, simulating a privacy protection scenario similar to (Yao, Peng, and Tsakiris 2021). The first dataset, described in (Fellegi and Sunter 1969), contains a matrix $M _ { s c o r e } ~ \in ~ \mathbb { R } ^ { 7 0 7 \times 1 \breve { 4 } }$ with scores of 707 students across 14 tests. To anonymize, the last 7 columns were randomly permuted, with shuffled ratios from 0.1 to 1. The second dataset from (Dua, Graff et al. 2017) involves a matrix $M _ { t u m o r } \in \mathbb { R } ^ { 3 5 7 \times 3 0 }$ , representing 357 patients with 30 features. Here, $5 0 \%$ of the columns were permuted with similar shuffled ratios. Both matrices were normalized, and our method was applied with a subspace dimension of 3 as in (Yao, Peng, and Tsakiris 2021). We assumed prior knowledge of the outlier ratio during the detection phase (Step 1 in Algorithm 1).

Given the lack of inherent multi-subspace scenarios in these datasets, no ground truth subspace information is available. Thus, even single subspace methods can perform reasonably well. We compared our PMSDR with RPCA ( $\lambda$ optimized over $0 . 1 ~ : ~ 0 . 0 5 ~ : ~ 0 . 9 5$ and $\mu ~ = ~ 1 0 \lambda ^ { \circ } ,$ ), RKPCA $\cdot \lambda$ optimized similarly), SSC ( $\scriptstyle { \alpha }$ optimized over $[ 5 , 2 0 , 1 0 0 , 2 0 0 , 5 0 0 , 1 0 0 0 ] )$ , and MRUC with the best initialization. UPCA (Yao, Peng, and Tsakiris 2021), a strong single-subspace recovery method, was used as a baseline.

In PMSDR, we hypothesized subspaces $L = 1 , 2 , 3$ despite the lack of natural subspaces. Figures 4 and 5 show that PMSDR outperforms UPCA in most cases, with slightly worse performance in low shuffled ratios due to possible undetected outliers affecting the basis of the subspace. This suggests that Algorithm 1 can uncover more hidden information, improving robustness and performance.

![](images/3170a8995f02725a9fc737856902a3e4dc5f6174e249420a54e2d5b72307bf57.jpg)  
Figure 4: De-Anonymization Experiments for Educational Data Comparing Algorithm 1, UPCA (Yao, Peng, and Tsakiris 2021), RPCA, RKPCA, SSC, and MRUC. The output of Algorithm 1 is denoted as $\mathbf { P } \mathbf { M } \mathbf { S } \mathbf { D } \mathbf { R } _ { L } = k$ , where the number of groups $k$ is set to $\{ 1 , 2 , 3 \}$ .

![](images/726ffd6bb775f9f18904358b399d2aab87f2fa5c1fd99a34f489b324522335d4.jpg)  
Figure 5: De-Anonymization Experiments for Medical Data Comparing Algorithm 1, UPCA (Yao, Peng, and Tsakiris 2021), RPCA, RKPCA, SSC, and MRUC. The output of Algorithm 1 is denoted as $\mathbf { P M S D R } _ { L } = k$ , where the number of groups $k$ is set to $\{ 1 , 2 , 3 \}$ .

# 5 Conclusion and Future Directions

Our algorithm extended UPCA (Yao, Peng, and Tsakiris 2021) by recovering corrupted data across multiple subspaces. For $L \ > \ 1$ , complexity increases due to outlier matching. Instead of the robust PCA, we use outlier detection and subspace clustering to estimate bases. While our outlier classification (Step 3 in Algorithm 1, which detailed in Algorithm 2) performs well, it depends on accurate basis estimation, revealing potential areas for improvement.

The framework is flexible, integrating other methods, but currently handles only linear/affine cases. Extending it to non-linear contexts requires further research. The algorithm is also limited to partially shuffled data, a restriction future work should address. Applying permutation recovery methods like MRUC (Tang et al. 2021) as preprocessing could transform fully shuffled data into a partially shuffled state, enabling our PMSDR pipeline to function effectively.

In conclusion, our method enhances recovery across multiple subspaces, but further research is needed to improve basis estimation, handle non-linear scenarios, and overcome the partially shuffled data limitation.