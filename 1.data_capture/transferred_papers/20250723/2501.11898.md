# Highly Efficient Rotation-Invariant Spectral Embedding for Scalable Incomplete Multi-View Clustering

Xinxin Wang1, Yongshan Zhang2, Yicong Zhou1\*

1Department of Computer and Information Science, University of Macau 2School of Computer Science, China University of Geosciences inxinwang1024@gmail.com, yszhang.cug@gmail.com, yicongzhou@um.edu.mo

# Abstract

Incomplete multi-view clustering presents significant challenges due to missing views. Although many existing graphbased methods aim to recover missing instances or complete similarity matrices with promising results, they still face several limitations: (1) Recovered data may be unsuitable for spectral clustering, as these methods often ignore guidance from spectral analysis; (2) Complex optimization processes require high computational burden, hindering scalability to large-scale problems; (3) Most methods do not address the rotational mismatch problem in spectral embeddings. To address these issues, we propose a highly efficient rotationinvariant spectral embedding (RISE) method for scalable incomplete multi-view clustering. RISE learns view-specific embeddings from incomplete bipartite graphs to capture the complementary information. Meanwhile, a complete consensus representation with second-order rotation-invariant property is recovered from these incomplete embeddings in a unified model. Moreover, we design a fast alternating optimization algorithm with linear complexity and promising convergence to solve the proposed formulation. Extensive experiments on multiple datasets demonstrate the effectiveness, scalability, and efficiency of RISE compared to the state-ofthe-art methods.

# Introduction

Multi-view clustering (MVC) explores the affinity relationships of samples among multiple views and groups similar samples into the same cluster (Zhang et al. 2018). Over the past decade, a variety of MVC methods have been proposed for various real-world applications (Chen et al. 2020; Li et al. 2024a). Although these methods achieve superior clustering performance, most of them strictly rely on the assumption that all instances in each view are available and complete. However, in real-world scenarios, some instances may be inaccessible from certain views due to transmission loss or sensor malfunctions (Wen et al. 2020). For instance, in medical diagnosis, patients may undergo various pathology tests, such as MRI, CT, and genetic testing. Only a small percentage of patients require all these tests, while others may need only a few targeted assessments. The resulting multi-view data, which contains missing instances, is referred to as incomplete multi-view data. When existing MVC methods are applied to such incomplete datasets, they may experience significant performance degradation or even complete failure (Xia et al. 2022; Shen et al. 2023).

![](images/264f117aa01c0abd177a1d0beccf8def19c76dcd6421fc62b45bfb29cc5fefff.jpg)  
Figure 1: Different completion stages for IMVC. Filling missing data at the feature embedding level is computationally efficient and provides robust performance.

To solve this problem, incomplete multi-view clustering (IMVC) has gained increasing attention. Recently, a variety of IMVC methods have been proposed (Wen, Xu, and Liu 2020; Wen et al. 2021a; Li et al. 2022). These methods can be divided into three categories: instance-level, graphlevel, and embedding-level methods. As shown in Fig. 1 (a), instance-level methods are natural and intuitive, as they recover missing instances to facilitate subsequent clustering (Hao et al. 2023; Zhao et al. $2 0 2 3 \mathrm { a }$ ; Liu et al. 2021). Typical recovery strategies include simple averaging (Shao, He, and Yu 2015; Gao, Peng, and Jian 2016), matrix factorization (Yin and Sun 2023; Wen et al. 2019), and deep learningbased imputation (Lin et al. 2021). As shown in Fig. 1 (b), graph-level methods aim to fill in incomplete similarity matrices rather than directly recovering missing instances (Wen et al. $2 0 2 3 \mathrm { a }$ ; Cui et al. 2022; Liu et al. 2023; Zhang et al. 2023a; Lv et al. 2022; Zhang, Huang, and Wang 2024). During the learning process, the recovered similarity matrices can either be used in spectral clustering or employed to directly output clustering results based on the connected components. Unlike instance-level and graph-level completion methods, as shown in Fig. 1(c), embedding-level methods perform completion in the latent feature space (Zhang et al. 2023b). This approach efficiently recovers missing data from low-dimensional embeddings, and the recovered representations naturally align with clustering tasks.

Although there are viable precedents for filling in missing data at different completion stages, existing methods still exhibit several limitations. First, most instance-level and graph-level methods finish multi-view fusion and completion before spectral analysis, which may render the recovered data unsuitable for spectral clustering (Zhang et al. 2023b). Second, most graph-level methods involve complex optimization processes and require a computational burden of at least $\mathcal { O } \bar { ( n ^ { 2 } ) }$ in both computation and storage, making them impractical for large-scale problems (Liu et al. 2022b; Wang et al. 2022a). Third, as is shown in Fig. 2, disparities in available instances among views cause clustering structure shifts across similarity graphs and increase the imbalance of eigenvalue distributions among the Laplacian matrices. This may worsen potential feature rotational mismatches between relaxed spectral embedding matrices when performing SVD decomposition. We refer to this issue as the spectral embedding rotational mismatch (SERM) problem. A similar issue in multi-view subspace learning is referred to as the misalignment problem (Wen et al. 2023b; Wang et al. 2022b). To the best of our knowledge, no unified IMVC framework simultaneously addresses these issues.

To address these issues, this paper proposes a highly efficient rotation-invariant spectral embedding (RISE) method for scalable incomplete multi-view clustering. Specifically, we learn view-specific embeddings from incomplete bipartite graphs to capture discrepancies between views. To tackle the SERM problem among view-specific embeddings, we propose a second-order rotation-invariant learning module to recover the complete consensus representation. Both processes are integrated into a unified framework, where the view-specific embedding can better serve for the learning of complete consensus representation, and the latter is able to guide the learning of the former. To solve the proposed formulation, we propose a fast alternating optimization algorithm with promising convergence, reducing the time complexity from $ { \mathcal { O } } ( n ^ { 3 } v )$ to $\mathcal { O } ( n \bar { m } v )$ . The storage cost of RISE is approximately ${ \mathcal { O } } ( n )$ by using bipartite graphs. The main contributions of this paper are listed below.

• We propose an efficient IMVC method, termed as RISE. It integrates incomplete view-specific embedding learning and complete consensus representation learning in a unified model. By introducing incomplete bipartite graph, RISE shows scalability and high-efficiency. • To address the SERM problem, we propose a secondorder rotation-invariant learning module to recover the complete consensus representation from view-specific embeddings. Guided by this module, the cross-view spectral embeddings can achieve adequate consistency. • We design a fast two-step optimization algorithm for RISE with theoretical analyses. Our proposed algorithm demonstrates promising convergence and linear time complexity with respective to the number of samples. Comprehensive experiments validate the effectiveness and efficiency of our method.

# Related Work

To tackle the scalability and efficiency issues in IMVC, several bipartite graph-based approaches have achieved significant attention. Wang et al. (Wang et al. 2022a) first introduced the bipartite graph for IMVC. Li et al. (Li et al. 2024b) proposed a scalable parameter-free model, while Liu et al. (Liu et al. 2022a) developed a fast model with viewindependent anchors. Zhao et al. (Zhao et al. 2023b) refined the complete bipartite graph learning by using concatenated multiple features. These methods learn a complete unified bipartite graph across views, to which spectral clustering is applied to obtain final clustering results. However, enforcing consistent cross-view representation before spectral analysis can neglect inter-view discrepancies, leading to suboptimal spectral embedding representations. Recent works have integrated tensor learning techniques to explore the inter-view low-rank structure. Chen et al. (Chen et al. 2023) combined tensor learning with view-specific bipartite graphs, while Li et al. (Li et al. 2023) integrated the consistent distribution between anchors and observed embedded representations to improve anchor imputation. Long et al. (Long et al. 2024) applied tensor networks to explore both cross-view and structural correlations. However, computing high-order correlations across views using tensor singular value decomposition (t-SVD) or tensor ring (TR) decomposition can be time-consuming, which limits their applicability in realworld scenarios.

![](images/acf69812942bdc9ba8f76f7921db65e41643d8a971af0f0ef38d858af7516929.jpg)  
Figure 2: Illustration of the SERM problem. Each color row represents a relaxed orthogonal basis. With structure drift in Laplacian matrices for incomplete views, the resulting orthogonal spectral embedding may be rotational mismatch.

# Proposed Method

# Formulation

Recently, several works have successfully adopted bipartite graphs to enhance the model’s scalability and clustering efficiency in IMVC tasks (Wang et al. 2022a; Li et al. 2024b). Similar to previous works (He et al. 2020), we use bipartite graph to reduce structural redundancy when presenting the correlations among samples. Specifically, given incomplete multi-view data X(i) Rdi×n i iv=1 with di features and $n _ { i }$ samples in $v$ views, $K$ -means is used to generate $m$ anchors $A ^ { ( i ) } = [ a _ { 1 } , a _ { 2 } , . . . , a _ { m } ] \in \mathbb { R } ^ { d _ { i } \times m }$ for each view separately. Then, a bipartite graph $\overline { { B } } ^ { ( i ) } \in \mathbb { R } ^ { n _ { i } \times m }$ is constructed between $m$ anchors and $n _ { i }$ samples on each view individually, where each element in $\overline { { B } } ^ { ( i ) }$ is computed by

$$
b _ { p , q } = \left\{ \begin{array} { c c } { \frac { d \left( p , k + 1 \right) - d \left( p , q \right) } { k d \left( p , k + 1 \right) - \sum _ { q = 1 } ^ { k } d \left( p , q \right) } } & { \forall q \in \Phi _ { p } } \\ { 0 } & { \forall q \notin \Phi _ { p } } \end{array} \right. ,
$$

where $\Phi _ { p }$ is the index set of the $k$ nearest anchors of sample $x _ { p }$ , and $d ( p , q ) = \| \mathbf { x } _ { p } - \mathbf { a } _ { q } \| ^ { 2 }$ presents the Euclidean distance between $x _ { p }$ and its $q$ -th nearest anchor $a _ { q }$ .

Rather than using full Laplacian matrices, we define a highly efficient spectral embedding learning framework for scalable incomplete multi-view clustering based on bipartite graphs as follows:

$$
\operatorname* { m a x } _ { F ^ { ( i ) } } \sum _ { i = 1 } ^ { v } \mathrm { T r } \left( { F ^ { ( i ) } } ^ { T } B ^ { ( i ) } B ^ { ( i ) } { } ^ { T } F ^ { ( i ) } \right) , \mathrm { s . t . } { F ^ { ( i ) } } ^ { T } F ^ { ( i ) } = I _ { k } ,
$$

where $B ^ { ( i ) } = \overline { { B } } ^ { ( i ) } \Lambda ^ { ( i ) - 1 / 2 }$ is the normalized bipartite graph of the $i$ -th view, and $F ^ { ( i ) } \in \mathbb { R } ^ { n _ { i } \times k }$ is the corresponding embedding generated from $B ^ { ( i ) }$ . It should be noted that $\Lambda ^ { ( i ) } \in \mathbb { R } ^ { \bar { m } \times \bar { m } }$ is a diagonal matrix with diagonal elements given by $\begin{array} { r } { \Lambda _ { q , q } ^ { ( i ) } = \sum _ { p = 1 } ^ { n _ { i } } \overline { { B } } _ { p , q } ^ { ( i ) } } \end{array}$ .

By solving Eq. (2), we obtain the embedding $F ^ { ( i ) } \ \in$ $\mathbb { R } ^ { n _ { i } \times k }$ with varying numbers of available feature representations. Afterward, a question that naturally arises is: How to efficiently and effectively refine the cross-view spectral embedding match and recover the consensus complete embedding? An common solution is to adopt the following formula to learn the complete consensus representation from incomplete view-specific embeddings (Wen et al. 2021a) by

$$
\operatorname* { m i n } _ { \boldsymbol { Y } } \sum _ { i = 1 } ^ { \boldsymbol { v } } \Big | \Big | \boldsymbol { F } ^ { ( i ) } - \boldsymbol { Q } ^ { ( i ) ^ { T } } \boldsymbol { Y } \Big | \Big | _ { \boldsymbol { F } } ^ { 2 } ,
$$

where $Q ^ { ( i ) } \in \{ 0 , 1 \} ^ { n \times n _ { i } }$ is an index matrix of the $i$ -th view that is used to remove the entries corresponding to the unavailable instances from the complete consensus representation $Y \in \mathbb { R } ^ { n \times k }$ . $Q ^ { ( i ) }$ is constructed from the index vector $\boldsymbol { h } ^ { ( i ) } \in \mathbb { R } ^ { n _ { i } }$ , which records the indices of the $n _ { i }$ instances in $F ^ { ( i ) }$ within the complete dataset. For the $q$ -th column of $Q ^ { ( i ) }$ , the elements are as follows:

$$
Q _ { p q } ^ { ( i ) } = \left\{ \begin{array} { l l } { { 1 , } } & { { \mathrm { ~ i f ~ } p \mathrm { ~ e q u a l s ~ t o ~ t h e ~ } q \mathrm { - t h ~ i n d e x ~ i n ~ } h ^ { ( i ) } } } \\ { { 0 , } } & { { \mathrm { ~ o t h e r w i s e . } } } \end{array} \right.
$$

However, the recovery strategy in Eq. (3) only considers first-order correlations during information fusion and overlooks the potential SERM problem. It may damage the representation capability of the model. Inspired by (Xia, Yang, and Yang 2023), we address this issue by recovering the complete second-order correlations among the embeddings of all samples in a quadratic term form.

The separate processes of spectral embedding learning and complete consensus representation learning may result in suboptimal results. Therefore, we combine them into a

unified formula as follows:

$$
\begin{array} { r l } { \displaystyle \operatorname* { m i n } _ { Y , F ^ { ( i ) } } \sum _ { i = 1 } ^ { v } \left\| Y Y ^ { T } - Q ^ { ( i ) } F ^ { ( i ) } { F ^ { ( i ) } } ^ { T } Q ^ { ( i ) } { } ^ { T } \right\| _ { F } ^ { 2 } } & { } \\ { \displaystyle - \beta \sum _ { i = 1 } ^ { v } \mathrm { T r } \left( F ^ { ( i ) ^ { T } } B ^ { ( i ) } B ^ { ( i ) ^ { T } } F ^ { ( i ) } \right) , } & { } \\ { \mathrm { s . t . } \quad F ^ { ( i ) ^ { T } } F ^ { ( i ) } = I , } & { Y ^ { T } Y = I , } \end{array}
$$

where the first term indicates the complete consensus representation learning with second-order rotation-invariant property, and the second term represents view-specific spectral embedding learning. Specifically, we impose an orthogonality constraint on $Y$ to enhance its discriminative power. $\beta$ is the penalty parameter, and $Q ^ { ( i ) } F ^ { ( i ) }$ denotes the recovery of the original arrangement. It is easy to prove that the formulation in Eq. (5) is rotation-invariant (Von Luxburg 2007). This implies that for a spectral embedding $F _ { * } ^ { ( i ) }$ , given any orthogonal rotation matrix $R$ , $F _ { * } ^ { ( i ) } R$ is a feasible spectral embedding set for Eq. (5). Thus, RISE is unaffected by potential rotations and always obtains the optimal solution $\hat { Y }$ .

Although bipartite graphs can reduce space complexity, simply introducing the index matrix $Q ^ { ( i ) } ~ \in ~ \mathbb { R } ^ { n \times n _ { i } }$ incurs a space burden of $\mathcal { O } ( n ^ { 2 } )$ and computational overhead of $\mathcal { O } ( n ^ { \bar { 3 } } )$ , which cam become a serious issue for largescale data. To address this problem, we innovatively define two operations for large-scale IMVC issues: Q(i)F (i) = $\zeta ^ { - 1 } ( F ^ { ( i ) } , h ^ { ( i ) } )$ , $Q ^ { ( i ) ^ { T } } Y = \zeta ( Y , h ^ { ( i ) } )$ , where $\zeta ( \cdot )$ and $\zeta ^ { - 1 } ( \cdot )$ denote the selection of instances from the complete representation matrix according to the index vector and its inverse operation, respectively. With this transformation, the space complexity drops from $\mathcal { O } ( v n ^ { 2 } )$ to $\mathcal { O } ( v n )$ .

# Optimization

To solve the optimization problem in Eq. (5), we devise a fast alternating optimization algorithm to update each variable while keeping the other variables fixed.

Updating $F ^ { ( i ) }$ : When $Y$ is fixed, the optimization problem w.r.t $F ^ { ( i ) }$ is

$$
\operatorname* { m a x } _ { F ^ { ( i ) ^ { T } } F ^ { ( i ) } = I } \mathrm { T r } \left( 2 F ^ { ( i ) ^ { T } } \hat { Y } \hat { Y } ^ { T } F ^ { ( i ) } \right) + \beta \operatorname { T r } \left( F ^ { ( i ) ^ { T } } B ^ { ( i ) } B ^ { ( i ) ^ { T } } F ^ { ( i ) } \right) ,
$$

where $\hat { Y } = Q ^ { ( i ) ^ { T } } Y$ . The solution to Eq. (6) is the $k$ eigenvectors corresponding to the largest $k$ eigenvalues of $S ^ { ( \bar { i } ) } =$ $2 \hat { Y } \hat { Y } ^ { T } + \beta \bar { B ^ { ( i ) } } B ^ { ( i ) } { } ^ { \bar { T } }$ . However, performing eigen decomposition requires at least $\mathcal { O } ( n _ { i } ^ { 2 } k ) \dot { ~ }$ time complexity, which makes it impractical for large-scale problems.

In this work, we present an alternative approach to efficiently compute the $k$ eigenvectors of matrix $S ^ { ( i ) }$ based on proposition 1.

Proposition 1. Given a set of similarity matrices $\left\{ S _ { t } \right\} _ { t = 1 } ^ { \pi } \in \mathrm { ~  ~ \mathbb ~ { ~ R ~ } ~ } ^ { n \times n }$ where each matrix satisfies $\begin{array} { r l r } { S _ { t } } & { { } = } & { Z _ { t } Z _ { t } ^ { T } } \end{array}$ . Defining the concatenated matrix $Z _ { c a t } = [ Z _ { 1 } , . . . , Z _ { t } , . . . , Z _ { \pi } ] \in \mathbb { R } ^ { n \times ( m _ { 1 } + m _ { 2 } + . . . m _ { \pi } ) }$ and supposing the singular value decomposition (SVD) of $Z _ { c a t }$

Input: Normalized bipartite graph $\{ B ^ { ( i ) } ) \} _ { i = 1 } ^ { v }$ , available index $\{ h ^ { ( i ) } ) \} _ { i = 1 } ^ { v }$ , number of cluster $c$ , and embedding dimension $k$ .

Initialize: Initialize $F ^ { ( i ) }$ by performing $S V D$ on $B ^ { ( i ) }$ .

1: while not converge do   
2: Update $Y$ by solving Eq. (8);   
3: Update $\{ F ^ { ( \dot { \ i } ) } \} _ { i = 1 } ^ { v }$ by solving Eq. (6);   
4: end while   
5: Output: perform $k$ -means on $Y$ to achieve the final   
clustering indicator.

is $U { \boldsymbol { \Sigma } } V ^ { T }$ (with $\boldsymbol { U } \boldsymbol { U } ^ { T } = \boldsymbol { I } , \boldsymbol { V } ^ { T } \boldsymbol { V } = \boldsymbol { I } )$ , we have that:

$$
\operatorname* { m a x } _ { F ^ { T } F = I } \mathrm { T r } \left( F ^ { T } ( \sum _ { t = 1 } ^ { \pi } S _ { t } ) F \right) \Leftrightarrow \operatorname* { m i n } _ { F ^ { T } F = I } \left. Z _ { c a t } - F F ^ { T } Z _ { c a t } \right. _ { F } ^ { 2 } .
$$

In Eq. (7), the optimal solution of $F$ is equal to $U$

Proof. Based on the right-hand side of Eq. (7), we can establish the following equivalences:

$$
\begin{array} { r } { \underset { F ^ { T } F = I } { \operatorname* { m i n } } \left. Z _ { c a t } - F F ^ { T } Z _ { c a t } \right. _ { F } ^ { 2 } \Leftrightarrow \underset { F ^ { T } F = I } { \operatorname* { m a x } } \mathrm { T r } \left( F ^ { T } Z _ { c a t } Z _ { c a t } ^ { T } F \right) } \\ { \Leftrightarrow \underset { F ^ { T } F = I } { \operatorname* { m a x } } \mathrm { T r } \left( F ^ { T } ( \underset { t = 1 } { \overset { \pi } { \sum } } S _ { t } ) F \right) } \end{array}
$$

Moreover,

$$
\sum _ { t = 1 } ^ { \pi } S _ { t } = Z _ { c a t } Z _ { c a t } ^ { T } = ( U \Sigma V ^ { T } ) ( U \Sigma V ^ { T } ) ^ { T } = U \Sigma ^ { 2 } U ^ { T } .
$$

To solve Eq. (6), we let $Z _ { c a t } ^ { F ( i ) } = [ \sqrt { 2 } Q ^ { ( i ) ^ { T } } Y , \sqrt { \beta } B ^ { ( i ) } ] \in$ $\mathcal { R } ^ { n _ { i } \times ( k + m ) }$ . According to Proposition $^ { l }$ , the optimal solution to Eq. (6) is the $k$ left singular vectors of $Z _ { c a t } ^ { F ( i ) }$ . This approach naturally reduces the computational cost of updating $F ^ { ( i ) }$ from $\mathcal { O } ( n _ { i } ^ { 2 } k )$ to $\mathcal { O } ( n _ { i } ( k + m ) ^ { 2 } )$ .

Updating $Y$ : When $F ^ { ( i ) }$ is fixed, the optimization problem w.r.t $Y$ is

$$
\operatorname* { m a x } _ { Y ^ { T } Y = I } \operatorname { T r } \left( Y ^ { T } \left( \sum _ { i = 1 } ^ { v } Q ^ { ( i ) } F ^ { ( i ) } { F ^ { ( i ) } } ^ { T } Q ^ { ( i ) ^ { T } } \right) Y \right) .
$$

Based on Proposition $\jmath$ , we define the formulation $Z _ { c a t } ^ { Y } =$ $[ Q ^ { ( 1 ) } F ^ { ( 1 ) } , . . . , Q ^ { ( i ) } F ^ { ( i ) } , . . . , Q ^ { ( v ) } F ^ { ( v ) } ] \in \mathbb { R } ^ { n \times ( v k ) } .$ . Similarly, the optimal solution to Eq. (8) is the $k$ left singular vectors of $Z _ { c a t } ^ { Y }$ . Updating $Y$ requires only $\mathcal { O } ( n ( v k ) ^ { 2 } )$ time complexity.

The entire optimization procedure for RISE is summarized in Algorithm 1. The code is available at https://github.com/RISE2025.

# Algorithm Analysis

Space complexity. The storage burden for RISE are the matrices $B ^ { ( i ) } \in \mathbb { R } ^ { n _ { i } \times m }$ , $F ^ { ( i ) } \in \mathbb { R } ^ { n _ { i } \times k }$ , and $Y \in \mathbb { R } ^ { n \times k }$ . Thus, the space complexity of RISE is $\mathcal { O } ( n k + n _ { i } k + n _ { i } m )$ .

Time complexity. The time complexity of RISE involves updating $F ^ { ( i ) }$ and $Y$ . Updating $F ^ { ( i ) }$ takes $\mathcal { O } ( n _ { i } ( k + m ) k )$ , while and solving for $Y$ costs $\mathcal { O } ( n v k ^ { 2 } )$ . Thus, at each iteration, the time complexity is $\begin{array} { r } { \mathcal { O } ( \sum _ { i = 1 } ^ { v } \dot { n _ { i } } ( k + m ) k + n v k ^ { 2 } ) } \end{array}$ . After optimization, we perform $k$ -means with $\mathcal { O } ( n c )$ to output the discrete clustering indicators. Considering that $k \ll$ 2 $\imath , m \ll n$ , and $c \ll n$ , the overall time complexity of the proposed optimization algorithm is linear to the number of samples $n$ . This demonstrates the high efficiency of RISE.

Table 1: Descriptions of Multi-view Datasets   

<html><body><table><tr><td>Dataset</td><td>Size</td><td>Views</td><td>Classes</td><td>Dimensions</td></tr><tr><td>Prokaryotic</td><td>551</td><td>3</td><td>4</td><td>438,3,393</td></tr><tr><td>WebKB</td><td>1051</td><td>2</td><td>2</td><td>2949,334</td></tr><tr><td>Caltech101-7</td><td>1474</td><td>6</td><td>7</td><td>48,40,254,1984,512,928</td></tr><tr><td>Wikipedia</td><td>2866</td><td>2</td><td>10</td><td>128,10</td></tr><tr><td>CIFAR10</td><td>50000</td><td>3</td><td>10</td><td>512,2048,1024</td></tr><tr><td>FMNIST</td><td>60000</td><td>3</td><td>10</td><td>512,512,1280</td></tr><tr><td>YoutubeFace</td><td>101499</td><td>5</td><td>31</td><td>64,512,64,647,838</td></tr></table></body></html>

Convergence. The optimization algorithm 1 updates $F ^ { ( i ) }$ and $Y$ by alternatively solving Eq. (6) and Eq. (8). The SVDbased solution designed in Proposition $\jmath$ converges to the global minimum for each sub-problem. Moreover, the objective function in Eq. (5) has a lower bound. These factors collectively ensure the convergence of the algorithm. More details in the appendix. More details in the appendix.

# Experiments

# Experimental Settings

Datasets and Incomplete Data Construction. We evaluate our RISE method on seven popular multi-view datasets, which include Prokaryotic (Brbi´c and Kopriva 2018), Wikipedia (Costa Pereira et al. 2014), WebKB (Bisson and Grimal 2012), Caltech101-7 (Fei-Fei, Fergus, and Perona 2006), CIFAR10 (Krizhevsky, Hinton et al. 2009), FMNIST (Xiao, Rasul, and Vollgraf 2017), and YoutubeFace (Lior Wolf 2011). The details of these datasets are described in Table 1.

Following the approaches in (Wang et al. 2022a; Wen et al. 2023b), we construct the incomplete data with 9 different missing rates $p = [ 0 . 1 : 0 . 1 : 0 . 9 ]$ . For example, when $p = 0 . 9$ , we randomly select $10 \%$ samples as complete data and randomly drop partial views of the rest $90 \%$ samples.

Compared Methods. We compare our RISE with ten stateof-the-art IMVC methods. These methods include UEAF (Wen et al. 2019), PIMVC (Deng et al. 2023), HCLS-CGL (Wen et al. 2023a), GSRIMC (Li et al. 2024c), IMVTSCMVI (Wen et al. 2021b), sFSR-IMVC(Long et al. 2024), DAIMC (Hu and Chen 2019), PSIMVC-PG (Li et al. 2024b), IMVC-CBG (Wang et al. 2022a), and SIMVC-SA (Wen et al. 2023b).

Implementation Details. For our RISE method, we use $K$ -means for anchor selection and the approach in (He et al. 2020) to construct the initial bipartite graph $\{ B ^ { ( i ) } \} _ { i = 1 } ^ { v }$ . We tune the parameter $\beta$ within a range of $[ 0 . 0 1 , 0 . 1 , 1 , 1 0 , 2 0 , 5 0 , 1 0 0 , 5 0 0 , 1 0 0 0 ]$ . For the number of anchor points, $m$ is tuned within a range of $[ 1 c , 6 c ]$ for the Prokaryotic, CIFAR10, FMNIST, and YoutubeFace datasets,

<html><body><table><tr><td>Metrices</td><td>Methods</td><td>UEAF</td><td>PIMVC</td><td>HCLS-CGL</td><td>GSRIMC</td><td>IMVTSC-MVI</td><td>DAIMC</td><td>sFSR-IMVC</td><td>PSIMVC-PG</td><td>IMVC-CBG</td><td>SIMVC-SA</td><td>Ours</td></tr><tr><td rowspan="7"></td><td>Prokaryotic</td><td>53.49</td><td>54.03</td><td>56.62</td><td>49.36</td><td>53.85</td><td>51.31</td><td>55.13</td><td>47.16</td><td>54.91</td><td>60.42</td><td>75.13</td></tr><tr><td>WebKB</td><td>76.56</td><td>86.06</td><td>81.99</td><td>87.30</td><td>81.54</td><td>81.77</td><td>78.72</td><td>71.08</td><td>86.66</td><td>83.60</td><td>93.02</td></tr><tr><td>Caltech101-7</td><td>37.26</td><td>66.88</td><td>72.36</td><td>65.22</td><td>60.53</td><td>46.97</td><td>67.60</td><td>48.52</td><td>59.88</td><td>59.37</td><td>75.25</td></tr><tr><td>Wikipedia</td><td>46.01</td><td>46.06</td><td>39.73</td><td>47.52</td><td>45.13</td><td>46.87</td><td>47.98</td><td>47.44</td><td>47.18</td><td>47.22</td><td>48.64</td></tr><tr><td>CIFAR10</td><td></td><td>1</td><td></td><td></td><td>1</td><td>92.80</td><td>62.77</td><td>95.96</td><td>95.91</td><td>96.04</td><td>96.54</td></tr><tr><td>FMNIST</td><td>，</td><td></td><td>1</td><td></td><td></td><td>25.05</td><td>10.76</td><td>22.20</td><td>22.74</td><td>50.99</td><td>52.49</td></tr><tr><td>YoutubeFace</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>14.32</td><td>23.81</td><td>15.68</td><td>25.52</td></tr><tr><td rowspan="7">NMI</td><td>Prokaryotic</td><td>27.12</td><td>25.87</td><td>7.28</td><td>17.02</td><td>19.54</td><td>15.98</td><td>11.05</td><td>22.84</td><td>31.25</td><td>32.68</td><td>38.60</td></tr><tr><td>WebKB</td><td>17.24</td><td>23.92</td><td>17.01</td><td>40.91</td><td>9.19</td><td>16.62</td><td>2.25</td><td>10.76</td><td>34.05</td><td>25.14</td><td>53.61</td></tr><tr><td>Caltech101-7</td><td>27.88</td><td>54.09</td><td>57.51</td><td>53.18</td><td>46.52</td><td>48.09</td><td>44.75</td><td>42.80</td><td>43.13</td><td>40.72</td><td>54.06</td></tr><tr><td>Wikipedia</td><td>36.11</td><td>34.11</td><td>28.27</td><td>23.75</td><td>38.94</td><td>32.37</td><td>34.36</td><td>35.78</td><td>37.96</td><td>39.18</td><td>35.78</td></tr><tr><td>CIFAR10</td><td></td><td></td><td></td><td></td><td></td><td>89.28</td><td>72.79</td><td>90.56</td><td>90.60</td><td>90.67</td><td>91.56</td></tr><tr><td>FMNIST</td><td></td><td></td><td>-</td><td></td><td></td><td>10.63</td><td>0.06</td><td>3.11</td><td>6.01</td><td>27.96</td><td>38.73</td></tr><tr><td>YoutubeFace</td><td>1</td><td></td><td>1</td><td></td><td>1</td><td>1</td><td></td><td>11.54</td><td>11.51</td><td>8.58</td><td>17.88</td></tr><tr><td rowspan="7">Purity</td><td>Prokaryotic</td><td>66.90</td><td>63.96</td><td>59.58</td><td>61.67</td><td>58.08</td><td>58.41</td><td>57.75</td><td>59.04</td><td>68.81</td><td>72.75</td><td>77.52</td></tr><tr><td>WebKB</td><td>80.90</td><td>84.95</td><td>82.22</td><td>88.50</td><td>81.54</td><td>81.65</td><td>78.74</td><td>78.29</td><td>86.67</td><td>83.60</td><td>93.39</td></tr><tr><td>Caltech101-7</td><td>77.56</td><td>86.94</td><td>86.66</td><td>58.03</td><td>85.48</td><td>83.33</td><td>74.50</td><td>80.61</td><td>81.03</td><td>79.27</td><td>88.59</td></tr><tr><td>Wikipedia</td><td>49.39</td><td>50.77</td><td>44.48</td><td>39.10</td><td>50.76</td><td>49.02</td><td>37.17</td><td>51.10</td><td>50.89</td><td>51.58</td><td>57.04</td></tr><tr><td>CIFAR10</td><td></td><td>-</td><td></td><td></td><td></td><td>93.15</td><td>62.81</td><td>95.96</td><td>95.91</td><td>96.04</td><td>96.54</td></tr><tr><td>FMNIST</td><td></td><td></td><td>-</td><td></td><td></td><td>25.55</td><td>10.79</td><td>22.20</td><td>22.80</td><td>51.16</td><td>60.17</td></tr><tr><td>YoutubeFace</td><td></td><td>-</td><td></td><td></td><td></td><td>-</td><td></td><td>28.11</td><td>28.99</td><td>28.08</td><td>37.78</td></tr></table></body></html>

Table 2: The average ACC, NMI, Purity results of different methods with nine missing ratios on benchmark datasets. “-” ndicates out of CPU memory. The best results are highlighted in bold, while the second-best results are marked with underline.

![](images/3c038c0abcd889ff5510fc19240187902801ec6f810a2f99c11c65504ea3f7e6.jpg)  
Figure 3: The ACC, NMI and Purity results of different methods with different missing ratios on partial benchmark datasets.

and $[ 2 c , 1 0 c ]$ for the Wikipedia and WebKb datasets. Moreover, the embedded dimension $k$ is adjusted as an integer multiple of the number of categories and kept smaller than the number of anchors. For all competitors, we obtain their public source codes from open websites and run these algorithms with their default parameter settings. Otherwise, we perform parameter search for better performance. To alleviate the sensitivity of random initialization, we repeat all experiments 10 times and report the average performance. Following previous works (Zhang et al. 2019), clustering accuracy (ACC), normalized mutual information (NMI), and purity are utilized as metrics to evaluate the performance of all methods. All experiments are performed on a machine with Inter core i7-9700 CPU, 32GB RAM, and Matlab 2022b (64bit).

# Experimental Results and Analysis

Table 2 presents the average clustering results of different methods with nine missing ratios. We can draw the following conclusions.

1. Our proposed RISE achieves the best performance across

<html><body><table><tr><td>Datasets</td><td>UEAF</td><td>PIMVC</td><td>HCLS-CGL</td><td>GSRIMC</td><td>IMVTSC-MVI</td><td>DAIMC</td><td>sFSR-IMVC</td><td>PSIMVC-PG</td><td>IMVC-CBG</td><td>SIMVC-SA</td><td>Ours</td></tr><tr><td>Prokaryotic</td><td>99.15</td><td>2.94</td><td>26.72</td><td>128.92</td><td>352.19</td><td>73.45</td><td>7.72</td><td>8.71</td><td>17.25</td><td>16.74</td><td>0.37</td></tr><tr><td>WebKB</td><td>526.35</td><td>7.20</td><td>21.14</td><td>239.32</td><td>1024.35</td><td>651.86</td><td>10.31</td><td>13.27</td><td>17.08</td><td>11.49</td><td>0.66</td></tr><tr><td>Caltech101-7</td><td>635.6</td><td>9.82</td><td>165.91</td><td>623.22</td><td>8433.70</td><td>1120.39</td><td>15.15</td><td>28.41</td><td>37.87</td><td>50.29</td><td>11.63</td></tr><tr><td>Wikipedia</td><td>2344.69</td><td>9.37</td><td>280.86</td><td>2366.09</td><td>3916.00</td><td>83.10</td><td>27.01</td><td>28.33</td><td>28.92</td><td>9.21</td><td>2.84</td></tr><tr><td>CIFAR10</td><td>1</td><td></td><td>-</td><td></td><td>-</td><td>93468.10</td><td>520.11</td><td>596.21</td><td>627.45</td><td>1359.07</td><td>149.99</td></tr><tr><td>FMNIST</td><td>-</td><td></td><td>-</td><td>-</td><td>-</td><td>89872.00</td><td>1602.51</td><td>846.43</td><td>567.30</td><td>973.02</td><td>371.89</td></tr><tr><td>YoutubeFace</td><td>-</td><td>-</td><td>-</td><td></td><td>-</td><td>1</td><td>-</td><td>1423.55</td><td>1866.46</td><td>3209.01</td><td>899.54</td></tr></table></body></html>

Table 3: Time costs of different methods on benchmark datasets (measured in seconds). “-” means out of CPU memory. Th best results are highlighted in bold, while the second-best results are marked with underline.

![](images/7db39e879eb6f35c36452feed493397e16d80f54e139ecf1de38854810956682.jpg)  
Figure 4: Clustering results of our RISE with different values of $\beta$ on two datasets.   
Figure 6: The variation of the objective function values for our RISE on two datasets.

![](images/8ada7c9ac172fa0497443a2003bdb7b951e470fb1e48fc933ad2f8060001519e.jpg)  
Figure 5: The ACC results of our RISE with different values of $m$ and $k$ on two datasets.

the three metrics against all baseline algorithms in most cases, indicating its superiority in IMVC tasks. Notably, SIMVC-SA performs better than other methods. Our RISE surpasses SIMVC-SA with improvements of $1 4 . 7 1 \%$ , $9 . 4 2 \%$ , $1 5 . 8 8 \%$ , $1 . 4 2 \%$ , $0 . 5 \%$ , $1 . 5 \%$ , and $9 . 8 4 \%$ in terms of ACC on the Prokaryotic, WebKB, Caltech101-7, Wikipedia, CIFAR10, FMNIST, and YoutubeFace datasets, respectively. Similar trends are observed across other metrics, further demonstrating the effectiveness of our RISE, even on large-scale datasets.

2. Compared to instance-level methods (i.e., UEAF, PIMVC, IMVTSC-MVI, sFSR-IMVC, and DAIMC) and graph-level methods (i.e., HCLS-CGL, GSRIMC, PSIMVC-PG, IMVC-CBG, and SIMVC-SA), our RISE based on spectral embedding completion demonstrates its effectiveness.

To provide a more intuitive comparison of different methods across various missing ratios, we present the ACC, NMI, and Purity curves for partial datasets in Fig. 3. As shown in Fig. 3, our RISE outperforms other methods, demonstrating superior and more stable performance. This highlights the

Prokaryotic #104 Caltech101-7 80085090095010001050Objective function value 3.23 0 5 10 15 20 0 5 10 15 20 Number of Iterations Number of Iterations

advantage of the second-order rotational-invariant property in multi-view fusion.

Table 3 presents the running times consumed by different methods. As illustrated, our RISE requires the shortest running time on all datasets with the exception of Caltech101-7. This demonstrates the efficiency of RISE. Note that SIMVCSA and IMVC-CBG take 3209.01 seconds and 1866.46 seconds on the YoutubeFace dataset, while our method only takes 899.54 seconds, reducing the running time by over half. This is because RISE employs bipartite graphs to preserve the essential intrinsic geometric structure of data while reducing redundancy. Moreover, the proposed optimization algorithm further accelerates the clustering process.

# Parameter Sensitivity and Convergence Analysis

To investigate the parameter sensitivity of RISE, we conducted experiments with different parameter settings on two datasets. As shown in Fig. 4, the parameter $\beta$ indeed affects performance. Specifically, the proposed algorithm remains stable over a large range of $\beta$ , except when it is less than 10, in which case the NMI metric suffers from a significant degradation. Additionally, we investigated the effect of varying the number of anchors $m$ and embedded dimensions $k$ under the optimal setting of other variables. As shown in Fig 5, both the number of anchors and embedded dimension influence the performance of our algorithm. This means that the appropriate values for $m$ and $k$ need to be chosen for RISE to obtain the optimal results. Fortunately, only a small amount of tuning is required to achieve satisfactory results.

In addition, we conducted several experiments to validate the convergence of the proposed algorithm. As shown in Fig. 6, the objective function value of our algorithm decreases monotonically in each iteration and converges rapidly. These results verify the convergence of our proposed algorithm.

![](images/48ff9569246abbd6265d250b052bb9cac09770b466f26343d9173d2450b0dbef.jpg)  
Figure 7: The ACC results of our RISE with different anchor selection strategies on two datasets.

![](images/2e0d61ebf45f1bc2cc357a0a68fb22ce39dc3a9b277a8c5c95900e834e8ec31d.jpg)  
Figure 8: Clustering results of our RISE with different completion strategies on two datasets.

# Ablation Study

Effect of Anchor Selection Strategy. To investigate the effect of different strategies for generating anchors, we used the $\mathbf { k }$ -means, random selection, and directly alternate sampling (DAS) method (Li et al. 2020) for anchor selection. As shown in Fig. 7, the $\mathbf { k }$ -means is more suitable for our algorithm. Specifically, the $k$ -means based anchors achieve over $1 5 \%$ performance improvement compared to the DAS approach on the Prokaryotic dataset, and around $10 \%$ improvement on the Caltech101-7 dataset. This is likely because the DAS approach is designed for complete multi-view data, while $k$ -means is a more general method.

Performance Effect of Completion Strategy. The rotationinvariant complete representation learning strategy is the main contribution of this paper. To further demonstrate its effectiveness, we conducted ablation experiments comparing different complete representation learning strategies. The term ’metric- $\cdot 2 ^ { \circ }$ refers to our proposed method, while the term ’metric-1’ indicates the use of Eq. (3) to learn the complete representation. As shown in Fig. 8, the proposed rotation-invariant learning module effectively improves the algorithm’s performance on the Prokaryotic and Caltech101-7 datasets in terms of ACC and NMI. This demonstrates the effectiveness of our proposed strategy. Additionally, our strategy shows smaller performance fluctuations as the missing rate increases, indicating the superiority of our approach.

Efficiency Effect of Completion Strategy. The proposed optimization algorithm enhances the efficiency of our RISE method. To further demonstrate its effectiveness, we plot the execution time comparison histograms of learning strategies using Eq. (3) and our proposed second-order rotationinvariant complete representation learning in Fig. 9. From this, we observe that our RISE method requires less time compared to the first-order learning strategy using Eq. (3). Specifically, our RISE shows nearly 10 times lower execution time on the Prokaryotic dataset and around 100 times lower on the Caltech101-7 dataset.

![](images/603e550cfb2fe2205ee4ff2ceccad98dac120748642d7e1939791965e2f62be9.jpg)  
Figure 9: Time costs of our RISE with different completion strategies on two datasets.

![](images/b175c4613153ca66172663aec92ea567e017e27f786f5d2463849a6908578131.jpg)  
Figure 10: Clustering results with bipartite graph and fullsize similarity graph on two datasets.

Effect of Graph Selection. To evaluate the performance of bipartite graph in reducing data redundancy, we used both bipartite graph and full similarity graph to separately describe the neighborhood structure among data. We then executed the rotation-invariant spectral embedding framework to obtain the consensus complete representation and applied $K$ -means to get the final clustering indicators. As is shown in Fig. 10, the bipartite graph can provides more superior performance compared to the full similarity graph. Additionally, using full similarity graph results in higher optimization costs because the proposed simplified SVD-based optimization strategy cannot be used.

# Conclusions

This paper proposed a highly efficient incomplete multiview learning framework, called RISE. It developed a fast late spectral embedding completion technique. RISE learned view-specific embeddings from incomplete bipartite graphs and then recovered the complete consensus representation from these embeddings using the proposed second-order rotation-invariant learning module. Both processes were integrated into a unified framework to mutually enhance performance. To solve the proposed objective function, we designed a fast two-step alternating optimization algorithm based on double SVD decompositions. Extensive experiments on datasets of varying sizes demonstrated the effectiveness, scalability, and efficiency of our proposed method. In the future, developing a parameter-free model will require more attention.