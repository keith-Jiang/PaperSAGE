# APAR: Modeling Irregular Target Functions in Tabular Regression via Arithmetic-Aware Pre-Training and Adaptive-Regularized Fine-Tuning

Hong-Wei Wu, Wei-Yao Wang, Kuang-Da Wang, Wen-Chih Peng

National Yang Ming Chiao Tung University, Hsinchu, Taiwan ohnnyhwu.cs11@nycu.edu.tw, sf1638.cs05@nctu.edu.tw, gdwang.cs $1 0 @$ nycu.edu.tw, wcpeng@cs.nycu.edu.tw

# Abstract

Tabular data are fundamental in common machine learning applications, ranging from finance to genomics and healthcare. This paper focuses on tabular regression tasks, a field where deep learning (DL) methods are not consistently superior to machine learning (ML) models due to the challenges posed by irregular target functions inherent in tabular data, causing sensitive label changes with minor variations from features. To address these issues, we propose a novel Arithmetic-Aware Pre-training and AdaptiveRegularized Fine-tuning framework (APAR), which enables the model to fit irregular target function in tabular data while reducing the negative impact of overfitting. In the pre-training phase, APAR introduces an arithmetic-aware pretext objective to capture intricate sample-wise relationships from the perspective of continuous labels. In the fine-tuning phase, a consistency-based adaptive regularization technique is proposed to self-learn appropriate data augmentation. Extensive experiments across 10 datasets demonstrated that APAR outperforms existing GBDT-, supervised NN-, and pretrainfinetune NN-based methods in RMSE $( + 9 . 4 3 \% \sim 2 0 . 3 7 \% )$ , and empirically validated the effects of pre-training tasks, including the study of arithmetic operations.

# Code — https://github.com/johnnyhwu/APAR

# 1 Introduction

The tabular regression task, prevalent in sectors such as healthcare (Rao et al. 2023; Jain et al. 2024) and finance (Du, Wang, and Peng 2023; Deng et al. 2024), has commonly been addressed using Gradient Boosting Decision Tree (GBDT) models (e.g., CatBoost (Prokhorenkova et al. 2018)). Despite recent advancements in neural networks (NNs), they often fail to consistently outperform GBDT models in this domain (Wang et al. 2024). This is attributed to the understanding of important features from distinct characteristics of tabular data, such as feature heterogeneity and the presence of uninformative features, which make it challenging to identify important features. On the other hand, the irregular target functions prevent NNs from learning high-frequency components of heterogeneous tabular datasets, and negatively impact NN performance due to overfitting (Beyazit et al. 2023).

Prior research (Gorishniy et al. 2021; Yan et al. 2023; Chen et al. 2023a) has primarily focused on addressing feature heterogeneity and uninformative features; however, the issue of irregular target functions remains relatively unexplored, especially in the context of tabular regression tasks having continuous labels instead of explicit boundaries between labels. Irregular target functions play a critical role since minor deviations in input features lead to major changes in target values (Beyazit et al. 2023). For instance, as illustrated in Figure 1, in a medical scenario (e.g., health risk prediction), a slight variation in a patient’s weight crossing a specific threshold can significantly alter the corresponding health status. In addition, the stock price is degraded significantly due to only the sentiment change (e.g., by wars). This phenomenon is less common in other data modalities; for example, a minor change in a single pixel in an image is unlikely to change its appearance. As NNs need to accurately model irregular target functions in tabular regression tasks while suffering from overfitting, it is crucial to emphasize the significance of advancing tabular regression methods capable of modeling sensitive changes between tabular features and labels.

Therefore, we focus on learning to fit irregular target functions for tabular regression tasks. Prior studies have mitigated this problem by addressing it from two perspectives: preventing overfitting on samples (Kossen et al. 2021; Ucar, Hajiramezanali, and Edwards 2021; Wang and Sun 2022) and features (Yoon et al. 2020; Arik and Pfister 2021; Somepalli et al. 2021). However, they are inferior in terms of utilizing label information due to the significant sparsity for regression labels (e.g., supervised contrastive learning for tabular classification tasks (Cui et al. 2024)) as well as in corrupting important features that are related to predictions (e.g., random feature masks (Chen et al. 2023b)).

To address the aforementioned challenges, we propose a novel Arithmetic-aware Pre-training and AdaptiveRegularized Fine-tuning framework (APAR) for tabular regression tasks, consisting of the pretrain-finetune strategy for modeling irregular target functions. Specifically, a Transformer-based (Vaswani et al. 2017) backbone is adopted with a tabular feature tokenizer to encode tabular heterogeneity. In the pre-training phase, an arithmetic-aware task is introduced to learn sample-wise relationships by predicting the combined answer of arithmetic operations on continuous labels. In the fine-tuning phase, we propose an adaptive regularization technique to reinforce the model to self-learn proper data augmentation based on feature importance by training the model to understand similar representations between original and augmented data. We compared our APAR with GBDT and supervised as well as pretrainfinetune NNs on 10 datasets, which demonstrated a significant improvement of at least $9 . 4 3 \%$ in terms of the RMSE score compared with the state-of-the-art baseline.

![](images/4dd1427cdd35ce0af90de6010ed876a70a2484d95a661cd1ffe967c638d8fb83.jpg)  
Figure 1: Illustrations of the impacts of irregular target functions commonly found in tabular regression tasks for finance (stock price prediction) and medical (health risk prediction) data. Small changes in features (marked in red) can lead to significant changes in the target variable.

In brief, our main contributions are described as follows:

• We present a principle recipe that enables NNs to effectively perform tabular regression tasks by addressing irregular target functions with the advantage of continuous labels and mitigating the negative impacts of overfitting on both samples and features.   
• We introduce an arithmetic-aware pre-training method to learn the relationships between samples by solving arithmetic operations from continuous labels. In addition, our adaptive-regularized fine-tuning technique allows the model to perform self-guided data augmentation, offering effective regularization and generalization in downstream tasks.   
• Extensive experiments across 10 datasets were systematically conducted to demonstrate an improvement from $9 . 4 3 \%$ to $2 0 . 3 7 \%$ compared with the GBDT-, supervised NN-, and pretrain-finetune NN-based methods.

# 2 Related Work

Recently, deep learning approaches for tabular data have demonstrated effective performance. For instance, Song et al. (2019) employed multi-head self-attention for intrasample feature interactions. Huang et al. (2020) and Gorishniy et al. (2021) adapted the Transformer architecture to tabular data, targeting both categorical and numerical features. Nonetheless, GBDT-based approaches (Chen and Guestrin 2016; Ke et al. 2017; Prokhorenkova et al. 2018) are still competitive in tabular benchmarks due to the highly complex and heterogeneous characteristics of tabular data, causing NNs to overfit on irregular target functions.

To prevent NN from overfitting on samples, prior research has proposed to considering sample-wise relationships while learning individual representations. NPT (Kossen et al. 2021) and SAINT (Somepalli et al. 2021) utilize selfattention to explicitly reason about relationships between samples. Similarly, Ucar, Hajiramezanali, and Edwards (2021) employed self-supervised contrastive loss to ensure that the model outputs similar representations for different feature subsets of the same sample, and dissimilar representations for subsets of different samples. Although these approaches effectively capture relationships between samples, they require relatively large batch sizes (e.g., 4096 in NPT) to include more samples computed under selfattention, leading to high computational and memory consumption. Moreover, they do not leverage supervised labels to learn contextualized representations based on explicit information. To incorporate supervised lavels, Supervised Contrastive Loss (Khosla et al. 2020) and TransTab (Wang and Sun 2022) enable models to learn improved sample representations by making samples with the same label similar, and samples with different labels dissimilar. Similarly, Cui et al. (2024) improved positive samples by augmenting anchors with features from samples of the same class to learn better representations based on explicit labels. Nonetheless, these approaches rely on discrete labels to determine positive or negative samples, which becomes extremely sparse for regression tasks where continuous labels are used.

To prevent NN from overfitting features, another line of research has utilized regularization techniques to avoid the model paying too much attention to a single feature. For instance, VIME (Yoon et al. 2020), SAINT (Somepalli et al. 2021), TabNet (Arik and Pfister 2021) and ReConTab (Chen et al. 2023b) use feature corruption, which encourages the model to output consistent predictions when features are randomly masked or mixed-up with other samples. However, randomly masking or mixing-up might inadvertently corrupt important features, causing the model to learn to predict based on uninformative features, and thus deteriorating learning representations. On the other hand, our proposed approach incorporates arithmetic from continuous labels as the pre-training task, and adaptively self-learns proper regularization during the fine-tuning stage.

![](images/cc43df7466cc794be20013861f6be80db4a95b0bc56ac0ae0eed60fce559fbae.jpg)  
Figure 2: Illustration of the Arithmetic-Aware Pre-Train phase of APAR. Sample pairs are processed through the Feature Tokenizer and Feature Encoder, the outputs of which are concatenated for arithmetic prediction, enabling the model to understand inter-sample relationships in tabular regression.

# 3 Problem Formulation

In this paper, we focus on regression tasks within the tabular domain. A dataset is denoted as $D = \{ ( x _ { i } , y _ { i } ) \} _ { i = 1 } ^ { n }$ , where $x _ { i } = ( x _ { i 1 } ^ { T } , . . . , x _ { i k } ^ { T } ) \in \mathbb { R } ^ { k }$ represents an object consisting of $k$ features, with $\ddot { T } \in \{ n u m , c a t \}$ indicating whether the feature is numerical or categorical. The corresponding label is denoted as $y _ { i } \in \mathbb { R }$ . Given an input sample $x$ , our goal is to learn a representation $Z$ of the sample that effectively encapsulates both feature-wise and sample-wise information that is able to maintain robustness against heterogeneous and uninformative features to precisely predict the corresponding target $y$ .

# 4 The Proposed Approach

The APAR framework employs a pretrain-finetune framework, outlined in Figures 2 (pre-training phase) and 3 (finetuning phase). Our APAR framework consists of two modules: the Feature Tokenizer and Feature Encoder. In the pretraining stage, a pair of two samples are encoded by the feature tokenizer and feature encoder to obtain their corresponding representations, and the contextualized [CLS] token is used for predicting arithmetic outcomes based on their numerical labels. In the fine-tuning stage, a test sample is augmented by applying it with a gate vector. Both the original and augmented samples are then encoded by the pre-trained feature tokenizer and feature encoder, generating two contextualized [CLS] tokens. The model is trained to adapt the gate vector based on self-learned feature importance, ensuring consistent predictions across the original and augmented samples.

# 4.1 Model Architecture

Feature Tokenizer To transform input features into representations, the feature tokenizer is introduced to convert categorical and numerical features of a sample into a sequence of embeddings. Similar to (Grinsztajn, Oyallon, and Varoquaux 2022), the feature tokenizer can prevent the rotational invariance of NNs by learning distinct embeddings for each feature.

Given $j$ -th feature $x _ { i j }$ of the $i$ -th sample, the tokenizer generates a $d$ -dimensional embedding $z _ { i j } \in \mathbb { R } ^ { d }$ . Formally, the embedding is computed as:

$$
z _ { i j } = b + f ( x _ { i j } ) \in \mathbb R ^ { d } ,
$$

where $b$ is a bias term and $f$ represents a transformation function. For numerical features, $f$ involves an element-wise product with a weighting vector W (num) Rd:

$$
z _ { i j } ^ { ( n u m ) } = b _ { j } ^ { ( n u m ) } + x _ { i j } ^ { ( n u m ) } \cdot W _ { j } ^ { ( n u m ) } \in \mathbb { R } ^ { d } .
$$

For categorical features, $f$ applies a lookup in the embedding matrix $W ^ { ( c a t ) } \in \mathbb { R } ^ { c \times d }$ , where $c$ is the number of categories, and $e _ { i j }$ is a one-hot vector for the corresponding categorical feature:

$$
\begin{array} { r } { z _ { i j } ^ { ( c a t ) } = b _ { j } ^ { ( c a t ) } + e _ { i j } \cdot W _ { j } ^ { ( c a t ) } \in \mathbb { R } ^ { d } . } \end{array}
$$

Finally, the output from the feature tokenizer $Z _ { i }$ is concatenated by the embeddings of all features of a sample $x _ { i }$ :

$$
Z _ { i } = \mathrm { s t a c k } [ z _ { i 1 } , . . . , z _ { i k } ] \in \mathbb { R } ^ { k \times d } .
$$

Feature Encoder Since our aim is to explore the strategies of the pre-training and fine-tuning stages similar to (Huang et al. 2020; Somepalli et al. 2021), the Transformer blocks encompassing multi-head self-attention and feed-forward networks are adopted as the feature encoder to encode intricate interrelations among heterogeneous and unformative features and to align the comparison.

Specifically, the embedding of a [CLS] token is first appended to the output $Z _ { i }$ of the feature tokenizer, which is then fed into $L$ Transformer layers, $F _ { 1 } , . . . , F _ { L }$ :

$$
\begin{array} { r l } & { Z _ { i 0 } = \mathrm { s t a c k } [ [ \mathbf { C L S } ] , Z _ { i } ] , } \\ & { Z _ { i l } = F _ { l } ( Z _ { i ( l - 1 ) } ) ; l = 1 , 2 , . . . , L . } \end{array}
$$

The output of the encoder can then be used to learn contextualized knowledge from the pre-training stage and downstream tasks from the fine-tuning stage.

![](images/8b2f5fbfe05ec6bfa1461c8a2322eabb46d08885422e0757d1c649d7a1a2c741.jpg)  
Figure 3: Illustration of the Adaptive Regularization Fine-Tuning phase of APAR. In this phase, an input sample is processed through the Feature Tokenizer to generate feature embeddings, which are augmented using a dynamically adaptive gate vector. The model is trained to predict consistent labels from varying inputs, which enhances the model’s robustness to uninformative features and performance on the target task.

# 4.2 Arithmetic-Aware Pre-Training

The goal of the pre-training phase is to integrate samplewise information into the representation of each sample; however, existing methods such as supervised contrastive learning (Khosla et al. 2020; Wang and Sun 2022; Cui et al. 2024) are ineffective in regression scenarios due to their reliance on discrete class labels, as opposed to regression’s continuous labels. Also, simply relying on attention mechanisms (e.g., (Kossen et al. 2021; Somepalli et al. 2021)) underutilizes the relationship between label information across samples. To that end, we introduce a novel arithmetic-aware pretext task by conditioning continuous labels.

Analogous to solving for an unknown in a set of simultaneous equations in mathematics, our pre-training goal is to introduce constraints that are able to narrow the possible outcomes (i.e., search space) of the unknown. Therefore, the arithmetic-aware pre-training task is proposed to enable the model to discern relationships between samples by utilizing continuous labels in tabular regression. Intuitively, pairing sample A with different samples B, C, and $\mathrm { ~ D ~ }$ generates unique aggregated outcomes, such as $\mathbf { A } { + } \mathbf { B }$ , $_ \mathrm { A + C }$ , and $\mathbf { A } { + } \mathbf { D }$ . These pairs impose constraints and guide the model in learning a fine-grained representation of A that considers the context from not only itself but also other paired samples. In our work, we opt for a simple yet effective pre-training task by incorporating an arithmetic operator with two samples at a time as the pretext objective.

As shown in Figure 2, the arithmetic-aware pre-training process starts by selecting two random samples, $x _ { i }$ and $x _ { j }$ , from the dataset, each with corresponding labels $y _ { i }$ and $y _ { j }$ . These samples undergo processing through the Feature Tokenizer and Feature Encoder to produce their respective representations, $Z _ { i L }$ and $Z _ { j L }$ :

$$
\begin{array} { r } { \begin{array} { c } { Z _ { i | j } = F e a t u r e T o k e n i z e r ( x _ { i | j } ) , } \\ { Z _ { i L | j L } = F e a t u r e E n c o d e r ( \mathrm { s t a c k } [ [ \mathrm { C L S } ] , Z _ { i | j } ] ) , } \end{array} } \end{array}
$$

where $_ { i \vert j }$ indicates the term is either the $i .$ - or $j$ -th sample. Subsequently, the representations of the [CLS] token Zi[CLLS] and $Z _ { j L } ^ { [ \mathrm { C L S } ] }$ are extracted from $Z _ { i L }$ and $Z _ { j L }$ , respectively. They are then concatenated and fed into a Multilayer Perceptron (MLP) to predict the outcome $\hat { y } ^ { \mathrm { A P } }$ of the arithmetic

operation:

$$
\hat { y } ^ { \mathrm { A P } } = \mathrm { M L P } ( \mathrm { c o n c a t } [ Z _ { i L } ^ { [ C L S ] } , Z _ { j L } ^ { [ C L S ] } ] ) .
$$

The pre-training task involves applying arithmetic operations on the sample labels $y _ { i }$ and $y _ { j }$ , including addition, subtraction, multiplication, and division1. The resulting ground truth $y ^ { \mathrm { A P } }$ for the arithmetic task is represented as:

$$
y ^ { \mathrm { A P } } = { \left\{ \begin{array} { l l } { y _ { i } + y _ { j } , } & { { \mathrm { f o r ~ a d d i t i o n } } ; } \\ { y _ { i } - y _ { j } , } & { { \mathrm { f o r ~ s u b t r a c t i o n } } ; } \\ { y _ { i } \times y _ { j } , } & { { \mathrm { f o r ~ m u l t i p l i c a t i o n } } ; } \\ { y _ { i } / y _ { j } , } & { { \mathrm { f o r ~ d i v i s i o n } } . } \end{array} \right. }
$$

Finally, the model is then trained to minimize ${ \mathcal { L } } ^ { \mathrm { A P } }$ :

$$
\mathcal { L } ^ { \mathrm { A P } } = \frac { 1 } { n } \sum ^ { n } ( y ^ { \mathrm { A P } } - \hat { y } ^ { \mathrm { A P } } ) ^ { 2 } .
$$

This pre-training task embeds an awareness of arithmetic relationships between samples into the model, thereby enabling it to adeptly handle the irregularities of target functions. The detailed procedure of arithmetic-aware pretraining is summarized in Algorithm 1 in Appendix A.1.

# 4.3 Adaptive-Regularized Fine-Tuning

In the fine-tuning phase of APAR, we introduce an adaptiveregularized fine-tuning method that adaptively augments samples by considering feature importance and their correlated structure. As shown in Figure 3, an input sample is processed by the pre-trained feature tokenizer to produce feature embeddings, which are subsequently augmented using a dynamically adaptive gate vector. The model is fine-tuned to predict a consistent label from these variant inputs, which enables the model to perform data augmentation, guided by the model-learned importance of each feature, to improve performance on the downstream task.

Adaptive Learning When learning the importance of each feature, we consider the correlation structure of the features rather than assuming independence, as the feature selection process is influenced by these correlations (Katrutsa and Strijov 2017). Specifically, a correlated gate vector for augmenting the input sample is generated from a multivariate Bernoulli distribution where the mean is determined by learnable parameters reflecting feature importance. The distribution is jointly updated when fine-tuning the model to adaptively utilize the self-learned gate vector to augment the input sample.

Specifically, given the correlation matrix $R \in [ - 1 , 1 ] ^ { k \times k }$ representing the correlation structure, the Gaussian copula is defined as:

$$
C _ { R } ( U _ { 1 } , . . . , U _ { k } ) = \Phi _ { R } ( \Phi ^ { - 1 } ( U _ { 1 } ) , . . . , \Phi ^ { - 1 } ( U _ { k } ) ) ,
$$

where $\Phi _ { R }$ denotes the joint cumulative distribution function (CDF) of a multivariate Gaussian distribution with a mean zero vector and correlation matrix $R , \Phi ^ { - 1 }$ is the inverse CDF of the standard univariate Gaussian distribution, and $U _ { j } \sim U n i f o r m ( 0 , 1 )$ for $j \in [ k ]$ .

Afterwards, we sample a gate vector $m$ to augment the input sample from a multivariate Bernoulli distribution that maintains the correlation structure of the input features as:

$$
m \sim M u l t i B e r n ( \pi ; R ) .
$$

Formally, $m _ { j }$ is set to 1 if $U _ { j } \ \leq \ \pi _ { j }$ and 0 if $U _ { j } ~ > ~ \pi _ { j }$ , for $j \in [ k ]$ . Here, $\pi$ represents a set of learnable parameters indicating feature importance. For differentiability, we apply the reparametrization trick (Wang and Yin 2020), resulting in a relaxed gate vector $\tilde { m }$ from the following equation:

$$
\tilde { m } _ { j } = \sigma ( \frac { 1 } { \tau } ( \log \pi _ { j } - \log ( 1 - \pi _ { j } ) + \log U _ { j } - \log ( 1 - U _ { j } ) ) ) ,
$$

where $\sigma ( x ) = ( 1 + \exp ( - x ) ) ^ { - 1 }$ is the sigmoid function, and a temperature parameter $\tau \in ( 0 , \infty )$ .

The relaxed gate vector $\tilde { m }$ is then used to be multiplied with the feature embeddings for data augmentation. The feature selection probability $\pi$ is learned during training and is adjusted according to the performance of the model. To induce the sparsity of the selected features, the sparsity loss, $\mathcal { L } _ { \mathrm { s p a r s i t y } }$ is calculated as follows:

$$
\mathcal { L } _ { \mathrm { s p a r s i t y } } = \sum _ { i } ^ { k } \pi _ { i } .
$$

Fine-Tuning Loss Function As depicted in Figure 3, for each input sample $x _ { i }$ with the label $y _ { i }$ , it is processed by the feature tokenizer and feature encoder to obtain the representation of the sample $Z _ { i L }$ , as detailed in Equation (6). Simultaneously, the original feature embeddings $Z _ { i }$ are elementwise multiplied with the relaxed gate vector $\tilde { m }$ to obtain augmented feature embeddings $\tilde { Z } _ { i }$ , as shown below:

$$
\tilde { Z } _ { i } = Z _ { i } \odot \tilde { m } .
$$

These augmented embeddings $\tilde { Z } _ { i }$ are then stacked with the [CLS] token and input into the feature encoder to obtain the augmented representation $\tilde { Z } _ { i L }$ :

Table 1: Statistics of each dataset.   

<html><body><table><tr><td></td><td>BD</td><td>AM</td><td>HS</td><td>GS</td><td>ER</td></tr><tr><td>#instances</td><td>73203</td><td>60786</td><td>61784</td><td>36733</td><td>21643</td></tr><tr><td>#num feats</td><td>230</td><td>230</td><td>230</td><td>10</td><td>23</td></tr><tr><td>#cat feats</td><td>17</td><td>17</td><td>17</td><td>0</td><td>2</td></tr><tr><td></td><td>PM</td><td>BS</td><td>YE</td><td>KP</td><td>FP</td></tr><tr><td>#instances</td><td>41757</td><td>17389</td><td>515345</td><td>241600</td><td>300153</td></tr><tr><td>#num feats</td><td>11</td><td>7</td><td>90</td><td>0</td><td>2</td></tr><tr><td>#cat feats</td><td>1</td><td>8</td><td>0</td><td>14</td><td>7</td></tr></table></body></html>

$$
\tilde { Z } _ { i L } = F e a t u r e E n c o d e r ( \mathrm { s t a c k } [ [ \mathrm { C L S } ] , \tilde { Z } _ { i } ] ) .
$$

The representation of the [CLS] token $Z _ { i L } ^ { [ \mathrm { C L S } ] }$ and $\tilde { Z } _ { i L } ^ { [ \mathrm { C L S } ] }$ , extracted from $Z _ { i L }$ and $\tilde { Z } _ { i L }$ , respectively, are input into an MLP to generate the corresponding predictions $\hat { y } _ { i } ^ { \mathrm { A R } }$ and $\tilde { y } _ { i } ^ { \mathrm { A R } }$ as described by the following equations:

$$
\begin{array} { r } { \hat { y } _ { i } ^ { \mathrm { A R } } = \mathbf { M } \mathbf { L } \mathbf { P } ( Z _ { i L } ^ { [ \mathrm { C L S } ] } ) , \tilde { y } _ { i } ^ { \mathrm { A R } } = \mathbf { M } \mathbf { L } \mathbf { P } ( \tilde { Z } _ { i L } ^ { [ \mathrm { C L S } ] } ) . } \end{array}
$$

The losses for the target task ${ \mathcal { L } } _ { \mathrm { t a r g e t } }$ and the regularization $\mathcal { L } _ { \mathrm { r e g } }$ are computed as follows:

$$
\mathcal { L } _ { \mathrm { t a r g e t } } = \frac { 1 } { n } \sum _ { i } ^ { n } ( y _ { i } - \hat { y } _ { i } ^ { \mathrm { A R } } ) ^ { 2 } , \mathcal { L } _ { \mathrm { r e g } } = \frac { 1 } { n } \sum _ { i } ^ { n } ( y _ { i } - \tilde { y } _ { i } ^ { \mathrm { A R } } ) ^ { 2 } .
$$

During the fine-tuning phase, the total loss, $\mathcal { L } ^ { \mathrm { A R } }$ , is the weighted sum of the target task loss, regularization loss, and feature sparsity loss, defined as:

$$
\begin{array} { r } { \mathcal { L } ^ { \mathrm { A R } } = \alpha \mathcal { L } _ { \mathrm { t a r g e t } } + \beta \mathcal { L } _ { \mathrm { r e g } } + \gamma \mathcal { L } _ { \mathrm { s p a r s i t y } } , } \end{array}
$$

where $\alpha , \beta$ , and $\gamma$ are hyperparameters within the range $[ 0 , 1 ]$ . The procedure of adaptive-regularized fine-tuning is summarized in Algorithm 3 in Appendix A.3.

# 5 Experiments

In this section, we attempt to answer the following research questions on a wide range of real-world datasets:

• RQ1: Does our proposed framework, APAR, outperform the existing NN-based and GBDT-based approaches? • RQ2: How does the performance of the proposed arithmetic-aware pre-training task compare to other pretraining approaches in tabular regression? • RQ3: Does the adaptive regularization enhance the model’s performance during the fine-tuning phase? • RQ4: How do different arithmetic operations affect the performance across various scenarios?

# 5.1 Experimental Setup

Datasets Overview. In our experiments, we utilized 10 publicly real-world datasets across diverse tabular regression applications (i.e., property valuation, environmental monitoring, urban applications, and performance analysis), spanning a range of scales from large-scale (over 100K samples) to medium-scale (50K to 100K samples) and smallscale (less than 50K samples). The datasets include Taiwan Housing (BD, AM, HS) (M.O.I. Dept 2023) consisting of three building types (building, apartment, and house), Gas Emission (GS) (mis 2019a), Election Results (ER) (mis 2019b), Beijing PM2.5 (PM) (Chen 2017), Bike Sharing (BS) (Fanaee-T 2013), Year (YE) (Bertin-Mahieux 2011), Kernel Performance (KP) (Paredes and BallesterRipoll 2018), and Flight Price (FP) (Bathwal 2021). Each dataset presents unique characteristics, including variations in features and label ranges. A summary of the dataset characteristics is presented in Table 1, and we follow their corresponding protocols to split training, validation, and test sets.

<html><body><table><tr><td>Group</td><td></td><td>BD</td><td>AM</td><td>HS</td><td>GS</td><td>ER</td><td>PM</td><td>BS</td><td>YE</td><td>KP</td><td>FP</td><td>Rank</td></tr><tr><td rowspan="3">GBDT-based</td><td>XGB</td><td>0.2476</td><td>0.2472</td><td>0.3509</td><td>0.1489</td><td>0.0510</td><td>0.6075</td><td>0.0244</td><td>0.2166</td><td>0.2559</td><td>0.4066</td><td>6.0</td></tr><tr><td>LGBM</td><td>0.2506</td><td>0.2429</td><td>0.3354</td><td>0.1575</td><td>0.0693</td><td>0.7445</td><td>0.0458</td><td>0.2156</td><td>0.3718</td><td>0.4213</td><td>6.5</td></tr><tr><td>CB</td><td>0.2406</td><td>0.2441</td><td>0.3423</td><td>0.1526</td><td>0.0557</td><td>0.7398</td><td>0.0469</td><td>0.2175</td><td>0.3087</td><td>0.4460</td><td>6.3</td></tr><tr><td rowspan="4">Supervised NN-based</td><td>MLP</td><td>0.2728</td><td>0.2617</td><td>0.4314</td><td>0.1743</td><td>0.0499</td><td>0.6973</td><td>0.0244</td><td>0.2179</td><td>0.0500</td><td>0.3659</td><td>6.6</td></tr><tr><td>AutoInt</td><td>0.2498</td><td>0.2456</td><td>0.3510</td><td>0.1755</td><td>0.1549</td><td>0.7790</td><td>0.0974</td><td>0.2161</td><td>0.0830</td><td>0.3843</td><td>7.7</td></tr><tr><td>FT-T</td><td>0.2452</td><td>0.2352</td><td>0.3457</td><td>0.1710</td><td>0.0538</td><td>0.8160</td><td>0.0591</td><td>0.2163</td><td>0.0547</td><td>0.3421</td><td>5.5</td></tr><tr><td>TabNet*</td><td>0.2435</td><td>0.2502</td><td>0.3467</td><td>0.1249</td><td>0.0838</td><td>0.5537</td><td>0.0591</td><td>0.2114</td><td>0.1849</td><td>0.3657</td><td>5.1</td></tr><tr><td>NN-based with a</td><td>VIME</td><td>0.2412</td><td>0.2606</td><td>0.3746</td><td>0.1266</td><td>0.0422</td><td>0.6557</td><td>0.1360</td><td>0.2184</td><td>0.1766</td><td>0.3685</td><td>6.4</td></tr><tr><td>Pretrain-Finetune</td><td>TabNet</td><td>0.2404</td><td>0.2427</td><td>0.3333</td><td>0.1352</td><td>0.0846</td><td>0.5880</td><td>0.0479</td><td>0.2126</td><td>0.0566</td><td>0.3562</td><td>3.8</td></tr><tr><td></td><td>APAR</td><td>0.2397</td><td>0.2293</td><td>0.3305</td><td>0.1205</td><td>0.0338</td><td>0.5239</td><td>0.0139</td><td>0.2148</td><td>0.0500</td><td>0.3303</td><td>1.3</td></tr></table></body></html>

Table 2: Quantitative results of all groups of baselines and our proposed APAR. For each dataset, the best result in each column is in boldface, while the second best result is underlined. \* denotes without pre-training.

Categorical features of each dataset were processed through label encoding (Hancock and Khoshgoftaar 2020), except in CatBoost (Prokhorenkova et al. 2018) where builtin categorical feature support was used, while continuous features and labels were transformed using logarithmic scaling (Changyong et al. 2014). For NN-based approaches, we utilize uniformly dimensioned embeddings for all categorical features.

Baseline Methods. We compared APAR against several baselines categorized into three groups: 1) GBDTbased: XGBoost (XGB) (Chen and Guestrin 2016), LightGBM (LGBM) (Ke et al. 2017) and CatBoost (CB) (Prokhorenkova et al. 2018). 2) Supervised NN-based: MLP, AutoInt (Song et al. 2019), and FT-Transformer (FT-T) (Gorishniy et al. 2021). 3) NN-based with a pretrainfinetune strategy: VIME (Yoon et al. 2020) and TabNet (Arik and Pfister 2021).

Implementation Details. Our proposed APAR framework was developed using PyTorch version 1.13.1. The training was performed on an NVIDIA GeForce RTX 3090 GPU. Regarding optimizers, we followed the original TabNet implementation by using the Adam optimizer (Kingma and Ba 2014). For all other models, we employed the AdamW optimizer (Loshchilov and Hutter 2018) with $\beta _ { 1 } ~ = ~ 0 . 9$ , $\beta _ { 2 } ~ = ~ 0 . 9 9 9$ , and a weight decay of 0.01. A consistent StepLR scheduler was used for all deep learning models, and the batch size was set at 256 for each dataset and algorithm. Training continued until there was no improvement on the validation set for 10 consecutive epochs.

Evaluation Metrics. Following (Gorishniy et al. 2021), we used the root mean squared error (RMSE) for evaluating regression models. The ranks for each dataset were determined by sorting the scores obtained. The Rank reflects the average rank across all datasets. All the results are the average of 5 different random seeds.

# 5.2 Quantitative Results (RQ1)

Table 2 presents the quantitative performance of APAR and the baselines. Quantitatively, APAR was consistently superior to all approaches in overall ranking across 10 diverse datasets, achieving an average RMSE improvement of $9 . 1 8 \%$ compared to the second-best ranking method. We summarize the observations as follows:

Selection of the Feature Encoder. We can observe that TabNet(\*) and FT-Transformer demonstrate better performance compared with the other baselines in all three categories since they adopt Transformer architectures as their backbones to model intricate characteristics across tabular samples as well as features. Nonetheless, the comparison of APAR, which employs the Transformer architecture in the feature encoder, and these baselines reveals the importance of considering the advantage of learning contextualized representations in a two-stage manner.

Advantages of the Pretrain-Finetune Approach. It is evident that comparing TabNet\* with TabNet illustrates notable improvements with pre-training, demonstrating the value of the pretrain-finetune framework. However, VIME substantially hinders all performance due to not only the relatively simplified MLP architecture but also the lack of considering feature heterogeneity and rotational invariance, which again raises the need for leveraging the Transformer architecture with a feature tokenizer for tabular regression tasks. The effectiveness of our APAR highlights the capability of arithmetic-related pertaining tasks and adaptively learning contexts of features during the finetuning stage.

# 5.3 Effects of the Pre-Training Task (RQ2)

To testify the design of the pre-training task in APAR, we evaluate arithmetic-aware pre-training with four variants: 1) remove (w/o AP), replacing it with 2) feature reconstruction $( \mathbf { A P }  \mathbf { F R } )$ ), 3) mask reconstruction $\mathbf { \dot { A } P }  \mathbf { M R } \mathbf { \dot { \Omega } } ,$ ), and 4) $\mathrm { A P }  \mathrm { F R } + \mathrm { M R }$ . Feature reconstruction is pre-trained to reconstruct with corrupt samples, which are randomly inserted constant values to some features. Mask reconstruction is pretrained to predict the correct binary mask applied to the input sample, which aims to identify which parts of the input have been replaced. As shown in Table 3, it is obvious that removing the pre-training task degrades the performance for all scenarios. The deleterious effect of replacing our method with MR is due to the inadvertent masking of key features, which shifts the model’s reliance to less relevant sample details and overlooks inter-sample relationships. Although combining FR with MR improves performance, a significant gap remains compared to our arithmetic-aware pre-training task, indicating that utilizing the continuous labels in the regression scenario to design arithmetic tasks effectively encourages the model to consider inter-sample relationships.

Table 3: Ablative experiments of different pre-training tasks (RQ2) and adaptive regularization (RQ3).   

<html><body><table><tr><td></td><td></td><td>BD</td><td>AM</td><td>HS</td><td>GS</td><td>ER</td><td>PM</td><td>BS</td><td>YE</td><td>KP</td><td>FP</td></tr><tr><td rowspan="4">RQ2</td><td>w/o AP</td><td>0.2520</td><td>0.2377</td><td>0.3474</td><td>0.1549</td><td>0.0462</td><td>0.6173</td><td>0.0224</td><td>0.2175</td><td>0.0574</td><td>0.3603</td></tr><tr><td>AP→FR</td><td>0.2468</td><td>0.2512</td><td>0.3530</td><td>0.1259</td><td>0.0297</td><td>0.5758</td><td>0.0173</td><td>0.2148</td><td>0.0548</td><td>0.3409</td></tr><tr><td>AP →MR</td><td>0.2464</td><td>0.2464</td><td>0.3582</td><td>0.1956</td><td>0.1582</td><td>0.6403</td><td>0.0141</td><td>0.2198</td><td>0.0728</td><td>0.3718</td></tr><tr><td>AP→FR+MR</td><td>0.2508</td><td>0.2319</td><td>0.3530</td><td>0.1360</td><td>0.0266</td><td>0.5729</td><td>0.0140</td><td>0.2112</td><td>0.0548</td><td>0.3406</td></tr><tr><td>RQ3</td><td>w/o AR</td><td>0.2536</td><td>0.2383</td><td>0.3501</td><td>0.1240</td><td>0.0266</td><td>0.5955</td><td>0.0632</td><td>0.2161</td><td>0.0520</td><td>0.3344</td></tr><tr><td></td><td>APAR (Ours)</td><td>0.2397</td><td>0.2293</td><td>0.3305</td><td>0.1205</td><td>0.0266</td><td>0.5239</td><td>0.0139</td><td>0.2148</td><td>0.0500</td><td>0.3303</td></tr></table></body></html>

<html><body><table><tr><td></td><td>BD</td><td>AM</td><td>HS</td><td>GS</td><td>ER</td><td>PM</td><td>BS</td><td>YE</td><td>KP</td><td>FP</td><td>Rank</td></tr><tr><td>Addition</td><td>0.2495</td><td>0.2293</td><td>0.3305</td><td>0.1453</td><td>0.0338</td><td>0.5239</td><td>0.0182</td><td>0.2158</td><td>0.0500</td><td>0.3303</td><td>2.2</td></tr><tr><td>Subtraction</td><td>0.2408</td><td>0.2413</td><td>0.3422</td><td>0.1250</td><td>0.0287</td><td>0.5783</td><td>0.0147</td><td>0.2199</td><td>0.0505</td><td>0.3458</td><td>2.5</td></tr><tr><td>Multiplication</td><td>0.2397</td><td>0.2317</td><td>0.3458</td><td>0.1205</td><td>0.0266</td><td>0.5863</td><td>0.0139</td><td>0.2148</td><td>0.0506</td><td>0.3321</td><td>1.9</td></tr><tr><td>Division</td><td>0.2469</td><td>1</td><td>0.3375</td><td>0.1420</td><td>1</td><td></td><td>-</td><td>0.2149</td><td>0.0596</td><td></td><td>3.4</td></tr></table></body></html>

Table 4: Performance of using different arithmetic operations in Arithmetic-Aware Pre-Training. “-” indicates that the model did not converge during the pre-training phase.

# 5.4 Effects of Adaptive Regularization (RQ3)

To investigate the impact of incorporating adaptiveregularized fine-tuning in APAR, the performance of the removal of this design (w/o AR) was compared, as shown in the RQ3 row in Table 3. Specifically, we fixed the target task loss weight $\alpha$ at 1 and optimized the regularization loss weight $\beta$ and sparsity loss weight $\gamma$ using the validation dataset. Removing the adaptive-regularized technique causes the model to be prone to overfitting on uninformative features, which degrades the performance across all datasets. In contrast, APAR mitigates this limitation by adaptive regularization, leading to a substantial improvement.

# 5.5 Variants of Arithmetic Operations (RQ4)

We studied the performance of addition, subtraction, multiplication, and division across all datasets, as detailed in Table 4. It can be seen that both addition and multiplication operations are more effective than subtraction and division operations, indicating positively changing the representation of two numerical labels introduces less offset of information to learn relations compared with negatively changing. In addition, using either addition or multiplication may depend on the scale of the labels of the dataset. For example, if the labels are small (e.g., $< 1 \dot { }$ ), it is expected that all multiplication pairs will become near 0, leading to ambiguity for model learning. Moreover, division-based pre-training was the least consistent, often failing to converge during pre-training, as indicated by the “-” symbol. This is because the divided changes are too significant to learn the relations. These results highlight the adaptability of the arithmetic-aware pretraining method that is able to benefit different regression scenarios from various arithmetic operators.

# 6 Conclusion and Future Works

This paper proposes APAR, a novel arithmetic-aware pretraining and adaptive-regularized fine-tuning framework for tabular regression tasks. Distinct from existing works that ineffectively corrupt important features and transfer to regression labels due to the sparsity of the continuous space, our proposed pre-training task is able to take sample-wise interactions into account, allowing the capability of modeling from the aspects of continuous labels. Meanwhile, our adaptive-regularized fine-tuning design dynamically adjusts appropriate data augmentation by conditioning it on the selflearned feature importance. Experiments on 10 real-world datasets show that our APAR significantly outperforms state-of-the-art approaches by between $9 . 4 3 \%$ to $2 0 . 3 7 \%$ . We believe that APAR serves as a generic framework for tabular regression applications due to the flexible design for pretrain-finetune frameworks, and multiple interesting directions could be further explored within the framework, such as automatically selecting appropriate arithmetic operations for effective pre-training, or extending APAR to classification tasks with Boolean operations (e.g., AND), etc.