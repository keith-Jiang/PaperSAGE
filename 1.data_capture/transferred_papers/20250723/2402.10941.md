# Text2Data: Low-Resource Data Generation with Textual Control

Shiyu Wang, Yihao Feng, Tian Lan, Ning Yu, Yu Bai, Ran Xu, Huan Wang\*, Caiming Xiong, Silvio Savarese

Salesforce AI Research 181 Lytton Ave, Palo Alto, CA 94301

# Abstract

Natural language serves as a common and straightforward signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model. Subsequently, it undergoes controllable finetuning via a novel constraint optimization-based learning objective that ensures controllability and effectively counteracts catastrophic forgetting. Comprehensive experiments demonstrate that Text2Data is able to achieve enhanced performance regarding controllability across various modalities, including molecules, motions and time series, when compared to existing baselines.

Code — https://github.com/SalesforceAIResearch/text2data Extended version — https://arxiv.org/abs/2402.10941

# Introduction

Autonomy and controllability stand as twin primary pillars of generative AI (Gozalo-Brizuela and Garrido-Merchan 2023; Wang et al. 2022). While the challenge of autonomy has been substantially addressed through the rapid advancements of generative models, controllability is now ascending as a fervently explored arena within the machine learning community. As natural languages are one of the most common and simplest control signal for human beings to interact with machines, the machine learning community has increasingly focused on generating data that aligns semantically with textual descriptions, given its wide-ranging applications such as image editing (Zhang et al. 2023; Kawar

et al. 2023; Shen et al. 2024c), audio synthesis (Liu et al.   
2023; Huang et al. 2023), video generation (Li et al. 2018;   
Hu, Luo, and Chen 2022), and many more (Tevet et al. 2023;   
Sanghi et al. 2022; Shen and Tang 2024).

Recent breakthroughs in text-to-data generative models, particularly those using diffusion (Li et al. 2023; Yang et al. 2023; Kumari et al. 2023), have demonstrated remarkable proficiency by harnessing the rich semantic insights from vast datasets of data-text pairs. Despite the broad application of text-to-data generative models, not all modalities can meet the substantial data-text pair requirements for achieving optimal controllability during model training. This is often due to costly annotations or intricate data structures, a scenario we refer to as the low-resource situation. The lack of text labels in certain areas, such as molecules (Ramakrishnan et al. 2014; Irwin et al. 2012), motions (Guo et al. 2020; Mahmood et al. 2019), and time series (Du et al. 2020), primarily restricts supervised learning and hinders the use of advanced generative models for text-to-data generation tasks. The low-resource situation when training generative models unsurprisingly results in issues like undesirable generation quality, model overfitting, bias, and lack of diversity. However, the optimization for scarce text representations to improve the alignment between generated data and input texts in generative models is still under-explored.

To mitigate the issues in the low-resource scenario, strategies such as data augmentation (Hedderich et al. 2020; Meng et al. 2021), semi-supervised learning (Thomas et al. 2013; Cheuk, Herremans, and $\mathsf { S u } 2 0 2 1$ ), and transfer learning (Tits, El Haddad, and Dutoit 2020; Yi et al. 2018) are utilized. Yet, each comes across challenges. Data augmentation, for example, cannot always replicate genuine data fidelity to align accurately with initial text descriptions, and potentially leads to overfitting due to over-reliance on augmented samples. It also exacerbates the training complexity, intensifying the already high computational demand of diffusion models. For semi-supervised learning, text inherently carries nuances, ambiguities, and multiple meanings. Ensuring that the model infers the correct interpretation when leveraging unlabeled data is not straightforward. Lastly, while transfer learning offers a solution for limited datasets, it is prone to catastrophic forgetting (Iman, Arabnia, and Rasheed 2023), where previous knowledge diminishes as new text descriptions are introduced.

Alternative to existing solutions, we propose Text2Data, a diffusion-based framework achieving enhanced text-todata controllability even under low-resource situation. Specially, Text2Data operates in two pivotal stages: (1) Distribution mastery by leveraging unlabeled data. Distinct from conventional semi-supervised learning methods, Text2Data does not aim to deduce labels for unlabeled data. Instead, this step uses unlabeled data to discern the overarching data distribution via an unsupervised diffusion model, eliminating the semantic ambiguity often associated with semi-supervised approaches. (2) Controllable finetuning on text-labeled data. The learned diffusion model is then finetuned by text-labeled data. Distinct from methods reliant on data augmentation, Text2Data abstains from inflating the training dataset. Instead, we introduce a novel constraint optimization-based learning objective, aiming to mitigate catastrophic forgetting by regularizing the model parameter space closely to its preliminary space before finetuning. Our contributions are summarized as follows:

• We introduce Text2Data, a novel framework designed for text-to-data generation in the low-resource scenario. This approach maintains the fine-grained data distribution by fully harnessing both labeled and unlabeled data. • We design a novel learning objective based on constraint optimization to achieve controllability and overcome catastrophic forgetting during finetuning. • We theoretically validate optimization constraint selection and generalization bounds for our learning objective. • We compile real-world datasets across three modalities and conduct comprehensive experiments to show the effectiveness of Text2Data. The results demonstrate that Text2Data achieves superior performance baselines regarding both generation quality and controllability.

# Related Works

# Text-to-Data Diffusion-Based Generation

Diffusion models, notably divided into classifierguided (Dhariwal and Nichol 2021) and classifier-free (Ho and Salimans 2022) categories, have significantly impacted data generation across various domains. (Hoogeboom et al. 2022; Yang et al. 2023; Ho et al. 2022; Shen et al. 2024a). The classifier-guided diffusion guides the model during inference phase by independently training a classifier and supervising the model with its gradient, which is inefficient when computing gradient at each time step and sometimes the generation quality is deficient as the guidance is not involved in the training. By contrast, classifier-free diffusion guidance blends score estimates from both a conditional diffusion model and an unconditional one with time step as a parameter, exemplified by E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al. 2022) and Motion Diffusion Model (MDM) (Tevet et al. 2023) for controllable molecule and motion generation, respectively. Furthermore, since natural languages are a prevalent medium for human to communicate with the world, the text-to-data generation paradigm has gained traction, with diffusion models being instrumental in generating high-quality data aligned with textual inputs. The extensive applications encompass textto-image generation (Ruiz et al. 2023; Zhang and Agrawala 2023; Shen et al. 2024b), text-to-speech generation (Huang et al. 2022; Kim, Kim, and Yoon 2022), text-to-shape generation (Li et al. 2023; Lin et al. 2023), and more, leveraging the abundant text descriptions for training potent generative models. Despite advancements in generating data from text across various modalities, many other modalities may not satisfy the stringent requirements for sufficient data-text pairs essential for attaining optimal controllability during the training of models.

# Low-Resource Learning

In response to the challenges of low-resource training for controllable generative models, several strategies have been formulated. For instance, Yin et al. (2023) proposes Textto-Text-to-Image Data Augmentation that employed both large-scale pretrained Text-to-Text and Text-to-Image generative models for data augmentation to generate photorealistic labeled images in a controllable manner. Zang and Wan (2019) utilizes a semi-supervised approach to augment the training of both the encoder and decoder with both labeled and unlabeled data. Tu et al. (2019) proposes to learn a mapping between source and target linguistic symbols and employs transfer learning to transfer knowledge from a highresource language to low-resource language. Nevertheless, all those strategies have their own limitations, such as high computational complexity for data augmentation, difficulty in maintaining the correct interpretation of text when leveraging unlabeled data during semi-supervised learning, and the potential catastrophic forgetting issues in transfer learning. Additionally, most of works on text-to-data generation lie in the modality of image, speeches, and texts, which have plenty of labeled datasets (Ito and Johnson 2017; Lin et al. 2014; Jiang et al. 2021; Wang, Wu, and Pino 2020) to train the models nowadays. Yet it is far under-explored for the low-source modalities such as molecules, motions and time series. Therefore, we propose Text2Data, a diffusion-based framework adept at harnessing limited text-labeled data to enhance the controllability of the model in text-to-data generation.

# Problem Formulation

Suppose the dataset $\mathcal { D } = \{ \mathbf { x } , \mathbf { c } \}$ contains $N$ independent samples in total, where $\mathbf { x } \doteq \{ \mathbf { x } _ { i } \} _ { i = 1 } ^ { N }$ is the data samples such as molecules, motions, time series, etc. We assume that there is only a proportion of data in $\mathbf { x }$ that has corresponding text description $\mathbf { c } = \{ \mathbf { c } _ { i } \} _ { i = 1 } ^ { N _ { p } }$ where $N _ { p } \leq N$ . We denote that data with text description is contained in $\mathcal { D } _ { p }$ and $\mathcal { D } _ { p } \subset \mathcal { D }$ . Using both text-labeled and unlabeled data in $\mathcal { D }$ , we aim to learn a generative model, $p _ { \theta } ( \mathbf { x } | \mathbf { c } )$ parameterized by $\theta$ that is able to generate data $\mathbf { x } \sim p _ { \theta } ( \mathbf { x } | \mathbf { c } )$ corresponding to specific text description $\mathbf { c } = \mathbf { c } ^ { * }$ .

# Methods

Controllable data generation seeks to learn the conditional data distribution $p _ { \theta } ( \mathbf { x } | \mathbf { c } )$ during training and subsequently

![](images/1abae47a7a7efc036b3cafb5cfe2562d9646b9d56a5cd8b3d92f4a9400708fac.jpg)  
Figure 1: Overview of Text2Data. The model leverages unlabeled data (i.e., blue module) to discern the overall data distribution while the optimal set of model parameters $\Theta$ is obtained. Then the model is finetuned on labeled data (i.e., red module) by constraint optimization that gives the optimal set of parameters as $\Theta \cap \Theta ^ { \prime }$ , where $\Theta ^ { \prime }$ is the optimal set of parameters if finetune the model without constraint.

draw samples from this assimilated distribution during the inference stage. Consequently, our primary objective during the training phase is to optimize the following:

$$
\operatorname* { m i n } _ { \theta } \mathbb { E } _ { \mathbf { x } , \mathbf { c } \sim p _ { \mathcal { D } _ { p } } ( \mathbf { x } , \mathbf { c } ) } [ - \log p _ { \theta } ( \mathbf { x } \vert \mathbf { c } ) ] .
$$

While the training of such generative models is contingent upon the supervision of text descriptions present in the dataset (i.e., $\mathcal { D } _ { p }$ ), it is not always feasible to obtain an adequate number of data-text pairs to ensure optimal controllability (i.e., $| \mathcal { D } _ { p } | < | \mathcal { D } | )$ , especially in modalities like molecular structures, motion patterns and time series. Such constraints can precipitate complications, including model overfitting when optimizing according to Eq. 1. Given the challenges, devising strategies to capitalize on the unlabeled data within $\mathcal { D }$ —which is often more accessible and costeffective—is pivotal to effectively learn $p _ { \theta } ( \mathbf { x } | \mathbf { c } )$ .

Notably, the marginal distributions learned from unlabeled data closely resemble those obtained from labeled data:

$$
p _ { \theta } ( \mathbf { x } ) \approx \int p _ { \theta } ( \mathbf { x } | \mathbf { c } ) p _ { \mathcal { D } _ { p } } ( \mathbf { c } ) d \mathbf { c } = \mathbb { E } _ { \mathbf { c } \sim p _ { \mathcal { D } _ { p } } ( \mathbf { c } ) } \big [ p _ { \theta } ( \mathbf { x } | \mathbf { c } ) \big ] ,
$$

where $p _ { { D _ { p } } } ( \mathbf { c } )$ is the true underlying text generating distribution corresponding to $\mathbf { x } \in \mathcal { D } _ { p }$ . Hence, as per Figure 1, we are inspired to initially utilize the unlabeled data in $\mathcal { D }$ to learn $p _ { \theta } ( \mathbf { x } )$ and obtain the optimal set of model parameters $\hat { \theta } \in \hat { \Theta }$ , which serves as a robust approximation of $p _ { \theta } ( \mathbf { x } | \mathbf { c } )$ . Then, we finetune it using data-text pairs in $\mathcal { D } _ { p }$ to achieve desired model control. Crucially, we ensure that the parameters remain in close proximity to those established when learning $p _ { \theta } ( \mathbf { x } )$ via constraint optimization to make Eq. 2 hold while mitigating catastrophic forgetting. Figure 1 shows our constraint to keep the finetuned parameters $\hat { \theta } ^ { \prime }$ within $\Theta \cap \Theta ^ { \prime }$ , where $\Theta ^ { \prime }$ represents the optimal parameters from unconstrained finetuning. Then we plug the generative diffusion implementation to our framework. Finally, we use the last section to offer theoretical insights into the selection of the optimization constraint, underpinned by generalization bounds for our learning objective.

# Learning Text-to-Data Generation under Low-Resource Situation

To capture general distribution of data, we leverage all data in $\mathcal { D }$ with NULL tokens as conditions (i.e., ${ \bf c } \ \bar { = } \ \mathrm { N U L L } )$ ] to facilitate subsequent finetuning. Specifically, we train the generative model ${ \dot { p } } _ { \theta } ( \mathbf { x } | \emptyset )$ , where $\mathbf { \hat { \boldsymbol { \theta } } }$ parameterizes the model and $\varnothing$ represents the NULL token in practice. As the NULL token is independent to $\mathbf { x }$ , we also have $p _ { \theta } ( \mathbf { x } | \boldsymbol { \varnothing } ) = p _ { \theta } ( \mathbf { x } )$ . Hence, we are equivalently optimizing the following:

$$
\underset { \theta } { \operatorname* { m i n } } \ \mathbb { E } _ { \mathbf { x } \sim p _ { \mathcal { D } } ( \mathbf { x } ) } [ - \log p _ { \theta } ( \mathbf { x } ) ] ,
$$

where $p _ { { D } } ( { \bf x } )$ is the true underlying data generating distribution. Having discerned the general data distribution through Eq. 3, we proceed to finetune $p _ { \theta } ( \mathbf { x } | \mathbf { c } )$ using text labels $\mathbf { c }$ in $\mathcal { D } _ { p }$ to achieve the controllability of model. In alignment with Eq. 2, the anticipated finetuned parameter should approximate the parameter optimized in Eq. 3. We achieve this by finetuning $\overset { \bullet } { p } \theta \left( \mathbf { x } | \mathbf { c } \right)$ using the labeled data in $\mathcal { D } _ { p }$ within the optimal set obtained in Eq. 3, leading to the subsequent learning objective for the finetuning phase:

$$
\begin{array} { l } { \displaystyle \operatorname* { m i n } _ { \theta } \mathbb { E } _ { \mathbf { x } , \mathbf { c } \sim p _ { \mathcal { D } _ { p } } ( \mathbf { x } , \mathbf { c } ) } \big [ - \log p _ { \theta } ( \mathbf { x } | \mathbf { c } ) \big ] } \\ { \displaystyle \mathrm { s . t . } \mathbb { E } _ { \mathbf { x } \sim p _ { \mathcal { D } _ { p } } ( \mathbf { x } ) } \big [ - \log p _ { \theta } ( \mathbf { x } ) \big ] \leq \xi , \ ~ } \\ { \displaystyle \xi = \operatorname* { i n f } _ { \theta \in \Theta } \mathbb { E } _ { \mathbf { x } \sim p _ { \mathcal { D } } ( \mathbf { x } ) } \big [ - \log p _ { \theta } ( \mathbf { x } ) \big ] } \end{array}
$$

where $p _ { { D _ { p } } } ( \mathbf { x } , \mathbf { c } )$ is the true underlying data-text joint distribution. $\Theta$ denotes a localized parameter space where a minimum can be located. Specifically, we minimize $\mathbb { E } _ { \mathbf { x } , \mathbf { c } \sim p _ { \mathcal { D } _ { p } } ( \mathbf { x } , \mathbf { c } ) } [ - \log p _ { \theta } ( \mathbf { x } | \mathbf { c } ) ]$ using the labeled data in $\mathcal { D } _ { p }$ within the optimal set $\left\{ \theta : \mathbb { E } _ { \mathbf { x } \sim p _ { \mathcal { D } _ { p } } ( \mathbf { x } ) } [ - \log p _ { \theta } ( \mathbf { x } ) ] \leq \xi \right\}$ to make the parameter not far from learned via Eq. 3, so that catastrophic forgetting is mitigated. The trade-off between the learning objective and its constraint aligns with the nature of a lexicographic optimization problem (Gong and Liu 2021) and therefore can be solved in that context.

# Generative Objective on Empirical Samples

Eq. 4 is the population-level learning objective while empirically we follow the standard loss function (i.e., transformed evidence lower bound of Eq. 4) of the classifier-free diffusion guidance (Ho and Salimans 2022) by optimizing:

$$
\operatorname* { m i n } _ { \theta } \mathcal { L } _ { 2 } ( \theta ) \quad \mathrm { s . t . } \mathcal { L } _ { 1 } ^ { \prime } ( \theta ) \leq \xi , \xi = \operatorname* { i n f } _ { \theta \in \Theta } \mathcal { L } _ { 1 } ( \theta ) ,
$$

where we have: $\mathcal { L } _ { 1 } ( \theta ) = \mathbb { E } _ { \mathbf { x } \sim p _ { \mathcal { D } } ( \mathbf { x } ) , t } [ | | \epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , t ) - \epsilon | | ^ { 2 } ]$ , $\mathcal { L } _ { 1 } ^ { \prime } ( \theta ) \ = \ \mathbb { E } _ { \mathbf { x } \sim p _ { \mathcal { D } _ { p } } ( \mathbf { x } ) , t } [ | | \epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , t ) \ - \ \epsilon | | ^ { 2 } ]$ and ${ \mathcal { L } } _ { 2 } ( \theta ) ~ =$ $\mathbb { E } _ { \mathbf { x } , \mathbf { c } \sim p _ { \mathcal { D } _ { p } } ( \mathbf { x } , \mathbf { c } ) , t } [ | | \epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , \mathbf { c } , t ) - \epsilon | | ^ { 2 } ]$ . Also, $t$ is sampled from uniform between 1 and $T , T$ is the total diffusion steps, $\epsilon$ is the standard Gaussian random variable, and $\epsilon _ { \theta } \bar { ( \mathbf { x } ^ { ( t ) } , t ) }$ and $\epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , \mathbf { c } , t )$ are functions we aim to fit at the $t$ -th diffusion step. Note that $\epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , t )$ and $\epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , \mathbf { c } , t )$ share the same parameters but are just trained at different stages: distribution mastery on unlabeled data and controllable finetuning on labeled data, respectively. The framework of classifier-free diffusion model and the derivation of $\mathcal { L } _ { 1 } ( \theta ) , \mathcal { L } _ { 1 } ^ { \prime } ( \theta )$ and $\mathcal { L } _ { 2 } ( \theta )$ are introduced in Appendix.

As the true data generating distributions $p _ { { \mathcal { D } } } ( \mathbf { x } )$ , $p _ { { D _ { p } } } ( \mathbf { x } )$ and $p _ { { D _ { p } } } ( \mathbf { x } , \mathbf { c } )$ are unknown, we instead optimize the following empirical loss:

$$
\operatorname* { m i n } _ { \theta } \hat { \mathcal { L } } _ { 2 } ( \theta ) \quad \mathrm { s . t . } \hat { \mathcal { L } } _ { 1 } ^ { \prime } ( \theta ) \leq \hat { \xi } , \hat { \xi } = \operatorname* { i n f } _ { \theta \in \hat { \Theta } } \hat { \mathcal { L } } _ { 1 } ( \theta ) ,
$$

where we have $\hat { \mathcal { L } } _ { 1 } ( \theta ) ~ = ~ \mathbb { E } _ { \mathbf { x } \sim \hat { p } _ { \mathcal { D } } ( \mathbf { x } ) , t } [ | | \epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , t ) ~ - ~ \epsilon | | ^ { 2 } ]$ , $\hat { \mathcal { L } } _ { 1 } ^ { \prime } ( \theta ) \ = \ \mathbb { E } _ { \mathbf { x } \sim \hat { p } _ { \mathcal { D } _ { p } } ( \mathbf { x } ) , t } [ | | \epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , t ) \ - \ \epsilon | | ^ { 2 } ]$ and $\hat { \mathcal { L } } _ { 2 } ( \theta ) \ =$ $\mathbb { E } _ { \mathbf { x } , \mathbf { c } \sim \hat { p } _ { \mathcal { D } _ { p } } ( \mathbf { x } , \mathbf { c } ) , t } [ | | \epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , \mathbf { c } , t ) - \epsilon | | ^ { 2 } ]$ . $\hat { p } _ { D } ( { \bf x } )$ , $\hat { p } _ { { D _ { p } } } ( { \bf x } )$ and $\hat { p } _ { \mathcal { D } _ { p } } ( \mathbf { x } , \mathbf { c } )$ are corresponding empirical data generating distributions. $\hat { \Theta }$ is the localized parameter space where a minimum can be located for $\hat { \mathcal { L } } _ { 1 } ( \theta )$ . The lexicographic optimization of Eq. 6 is presented in Algorithm 1, which relies on learning the update direction on model parameter (i.e., $\nabla \hat { \mathcal { L } } _ { 2 } ( \theta ) + \lambda \nabla \hat { \mathcal { L } } _ { 1 } ^ { \prime } \mathbf { \bar { ( } } \theta )$ in Algorithm 1) to balance the tradeoff between the minimization of $\hat { \mathcal { L } } _ { 2 } ( \theta )$ and $\hat { \mathcal { L } } _ { 1 } ^ { \prime } ( \theta )$ using a dynamic gradient descent (Gong and Liu 2021):

$$
\boldsymbol { \theta } \gets \boldsymbol { \theta } - \boldsymbol { \omega } \cdot ( \nabla \hat { \mathcal { L } } _ { 2 } ( \boldsymbol { \theta } ) + \lambda \nabla \hat { \mathcal { L } } _ { 1 } ^ { \prime } ( \boldsymbol { \theta } ) ) ,
$$

where $\omega$ is predefined positive step size, and $\lambda$ is calculated based on whether the constraint is satisfied at the current gradient step:

$$
\begin{array} { r l } & { \lambda = \operatorname* { m a x } ( \frac { \phi ( \theta ) - \nabla \hat { \mathcal { L } } _ { 2 } ( \theta ) ^ { T } \nabla \hat { \mathcal { L } } _ { 1 } ^ { \prime } ( \theta ) } { \| \nabla \hat { \mathcal { L } } _ { 1 } ^ { \prime } ( \theta ) \| ^ { 2 } } , 0 ) } \\ & { \phi ( \theta ) = \operatorname* { m i n } ( \alpha ( \hat { \mathcal { L } } _ { 1 } ^ { \prime } ( \theta ) - \gamma \cdot \hat { \xi } ) , \beta \| \nabla \hat { \mathcal { L } } _ { 1 } ^ { \prime } ( \theta ) \| ^ { 2 } ) , } \end{array}
$$

where $\alpha , ~ \beta$ and $\gamma$ are predefined positive hyperparameters. More details and intuition of lexicographic optimization are explained in Appendix. Furthermore, the lexico

# Algorithm 1: Lexicographic optimization on Eq. 4

<html><body><table><tr><td>Input: ξ= infθ ε L1(0) by pretraining on D; Total diffusion steps T; Scheduled forward variance {βt}t=1; {αt =1- βt}t=1; Predefined positive hyperparameters α,β,and ω;probability of</td></tr><tr><td>Diffuse x(t） = √atx+√1-t∈ Replace c with θ with probability Puncond Computeαt =II=1 αt</td></tr></table></body></html>

graphic optimization-based constraint $\begin{array} { r } { \hat { \xi } = \operatorname* { i n f } _ { \theta \in \hat { \Theta } } \hat { \mathcal { L } } _ { 1 } ( \theta ) } \end{array}$ in Eq. 6 may be overly strict and could require relaxation to ease the training process. While we anticipate that the parameters derived from Eq. 6 should be close to those from Eq. 5, they do not necessarily have to be an exact subset of the parameters from Eq. 5.

# Generalization Bound of Learning Constraint

In this section, we deduce a confidence bound for the constraint to demonstrate that the optimal set of minimizing the empirical loss within the derived confidence bound encapsulates the true one, and guide the further relaxation on the constraint if needed. First, we define sub-Gaussian random variable in Definition 0.1 followed by the deducted confidence bound in Theorem 0.2.

Definition 0.1 (Sub-Gaussian random variable). The random variable $X$ with mean 0 is sub-Gaussian with variance $\sigma ^ { 2 }$ if $\forall s \in \mathbb { R }$ , $\begin{array} { r } { \mathbb { E } _ { X } [ \exp ( s X ) ] \le \exp \bigl ( \frac { \sigma ^ { 2 } s ^ { 2 } } { 2 } \bigr ) } \end{array}$

Based on the fact that $\mathbf { x } ^ { ( t ) }$ is diffused from the random variable $\mathbf { x }$ and the standard Gaussian noise $\epsilon$ (i.e., Algorithm 1), therefore, $\epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , t )$ and $\mathbf { \epsilon } _ { \theta } ( \mathbf { x } ^ { ( t ) } , \mathbf { c } ^ { ( t ) } , t )$ are also random variables. Meanwhile, minimizing the loss in Eq. 5 pushes $\epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , t )$ and $\mathbf { \epsilon } _ { \theta } ( \mathbf { x } ^ { ( t ) } , \mathbf { c } ^ { ( t ) } , t )$ towards $\epsilon$ , which has the mean of 0. Then we introduce the following theorem 1 while assuming $\epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , t )$ and $\mathbf { \epsilon } _ { \theta } ( \mathbf { x } ^ { ( t ) } , \mathbf { c } ^ { ( t ) } , t ) \mathbf { \bar { \mathbf { \phi } } }$ are subGaussian random variables.

Theorem 0.2. For every $\theta$ and $t _ { : }$ , assume $\epsilon _ { \theta } ( \mathbf { x } ^ { ( t ) } , t )$ and $\mathbf { \epsilon } _ { \theta } ( \mathbf { x } ^ { ( t ) } , \mathbf { c } _ { i } , t )$ are sub-Gaussian random variables with mean $o$ and variance $\sigma ^ { 2 }$ , and $\Theta$ is finite. Let $\Theta ^ { * } = \{ \theta :$ ${ \mathcal { L } } _ { 1 } ^ { \prime } ( \theta ) \leq \xi \}$ $\leq \xi \} , \hat { \Theta } ^ { * } = \{ \theta : \hat { \mathcal { L } } _ { 1 } ^ { \prime } ( \theta ) \leq \hat { \xi } + \epsilon \}$ where $\epsilon$ is the confidence bound with the probability of $1 - \delta$ . Let $\theta ^ { * }$ be the solution to Eq. 5 and ${ \hat { \theta } } ^ { * }$ be the solution to empirical Eq. 6, then we have the following:

1. $\Theta ^ { * } \subseteq { \hat { \Theta } } ^ { * }$ : the set of $\theta$ by optimizing Eq. 6 within the confidence bound covers the true one. 2. $\mathcal { L } _ { 2 } ( \hat { \theta } ^ { * } ) \leq \mathcal { L } _ { 2 } ( \theta ^ { * } ) + 2 \epsilon _ { N _ { p } } \colon \theta ^ { * }$ and ${ \hat { \theta } } ^ { * }$ compete well on ${ \mathcal { L } } _ { 2 } ( \theta )$ . 3. $\mathcal { L } _ { 1 } ^ { \prime } ( \hat { \theta } ^ { * } ) \leq \xi + 2 \epsilon _ { N _ { p } } + 2 \epsilon _ { N } .$ : ${ \hat { \theta } } ^ { * }$ does not violate constraint of Eq. 5 too much on $\mathcal { L } _ { 1 } ^ { \prime } ( \theta )$ .

$$
\begin{array} { r l } & { w h e r e \ \epsilon = \ \epsilon _ { N } + \epsilon _ { N } , \ \epsilon _ { N } \ = \ \sqrt { C \tilde { \sigma } ^ { 2 } } \cdot \sqrt { \frac { \log | \Theta | + \log \frac { 2 } { \delta } } { N } } \ \vee } \\ & { C \tilde { \sigma } ^ { 2 } \cdot \frac { \log | \Theta | + \log \frac { 2 } { \delta } } { N } , \epsilon _ { N _ { p } } = \sqrt { C \tilde { \sigma } ^ { 2 } } \cdot \sqrt { \frac { \log | \Theta | + \log \frac { 2 } { \delta } } { N _ { p } } } \vee C \tilde { \sigma } ^ { 2 } \cdot } \\ & { \frac { \log | \Theta | + \log \frac { 2 } { \delta } } { N _ { p } } , \tilde { \sigma } ^ { 2 } = \sigma ^ { 2 } + 1 \ a n d C = 8 \sqrt { 2 } . } \end{array}
$$

Theorem 0.2 can be proved starting from the Bernstein’s inequality and the union bound inequality on squared zeromean sub-Gaussian random variable. More detailed proof is in Appendix. As a large amount of unlabeled data (i.e., $N$ ) is usually easy to obtain by either manual collection or simulation, $\epsilon _ { N }$ is not large after taking logarithm on the number of model parameters (i.e., $| \Theta | )$ , even though it is usually much larger than $N$ . Additionally, $\log | \Theta |$ is not significantly larger than $N _ { p }$ so that $\epsilon _ { N _ { p } }$ should not be large as well. For instance, in our experiments, around 45,000 samples and 14 million model parameters for motion generation result in rather small $\epsilon _ { N }$ and $\epsilon _ { N _ { p } }$ . In practice, we use $\xi = \rho \cdot \operatorname* { i n f } _ { \theta \in \Theta } \mathcal { L } _ { 1 } ( \theta )$ in Eq. 5 (i.e., $\hat { \xi } \stackrel { \cdot } { = } \rho \cdot \operatorname* { i n f } _ { \theta \in \hat { \Theta } } \hat { \mathcal { L } } _ { 1 } ( \theta )$ in Eq. 6) to relax the constraint, where $\rho$ is an hyperparameter to keep the constraint within the confidence interval.

# Experiments

# Datasets

We employ datasets from three modalities that may suffer from low-resource scenario. Details are in Appendix.

Molecules. We extract 130,831 molecules from QM9 dataset (Ramakrishnan et al. 2014) with six molecular properties: polarizability $( \alpha )$ , highest occupied molecular orbital energy (ϵHOMO), lowest unoccupied molecular orbital energy (ϵLUMO), the energy difference between HOMO and LUMO $( \Delta _ { \epsilon } )$ , dipole moment $( \mu )$ and heat capacity at 298.15K $( C _ { v } )$ .

Motions. We employ HumanML3D that contains textually re-annotating motions captured from AMASS (Mahmood et al. 2019) and HumanAct12 (Guo et al. 2020). It contains 14,616 motions annotated by 44,970 textual descriptions.

Time Series. We assemble 24 stocks from Yahoo Finance from their IPO date to July 8, 2023. We tailor data to the length of 120 by slicing on the opening price for every 120 days, and scale by min-max normalization following Yoon, Jarrett, and Van der Schaar (2019). Totally, 210,964 time series are produced. We extract their features including frequency, skewness, mean, variance, linearity (i.e., $R ^ { 2 }$ ), and the number of peaks via “tsfresh” in Python.

# Baseline Models

We compare Text2Data with a representative classifier-free diffusion model in each modality as the baselines.

E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al. 2022).To handle Molecule dataset, we employ EDM as the baseline. EDM utilizes an equivariant network to denoise diffusion processes by concurrently processing both continuous (i.e., atom coordinates) and categorical data (i.e., atom types). The controllability on molecular properties is realized by the classifier-free diffusion guidance conditioned on the embedding of text descriptions, which is encoded by a pretrained T5 (Raffel et al. 2020) encoder (i.e., t5-large).

Motion Diffusion Model (MDM) (Tevet et al. 2023). We use MDM, a classifier-free diffusion model, for text-tohuman motion generation. The text descriptions are embedded by a pretrained T5 encoder to guide the motion generation, providing the mechanism for controllability.

Generation diffusion for time series (DiffTS). To generate time series, we design the classifier-free diffusion model conditioned on text embeddings encoded from pretrained T5 encoder. We employ the backbone of (Ho and Salimans 2022) by substituting image data to one-dimensional time series, and replacing the U-Net with one-dimensional convolutional neural network.

For implementation, we train baselines on specific proportions of labeled data. Text2Data is modified from a pretraining+finetuning strategy following Eq. 6. Additionally, we conduct ablation study where each baseline is still finetuned but without the constraint in Eq. 6 as a transfer learningbased baseline, or directly applied to augmented text-data pairs. Besides, we adapt from (You et al. 2024) to form a semi-supervised framework, by training a classifier to predict molecular properties and generate pseudo labels for unlabelled molecules. More details are in Appendix.

# Evaluation Metrics

We follow different strategies to evaluate both the controllability and the generation quality of the propose approach compared to other baselines.

Controllability. We compare the controllability between Text2Data and the baselines by the similarity between generated data and the ground truth. To assess the generated molecules, we follow (Hoogeboom et al. 2022) and train a classifier for each property to extract specific properties from generated data. Then, we calculate the Mean Absolute Error (MAE) between the extracted property and the ground truth. To assess the controllability of motion generation, we compute $R$ precision and Multimodal distance that measure the relevancy of the generated motions to the input prompts (Guo et al. 2022). To evaluate the controllability of time series generation, we extract properties via ”tsfresh” and compute the MAE between properties of generated data and that of ground truth. Additionally, we also visualize generated data according to the specified properties.

Generation quality. Generation quality varies based on the modality. For molecular generation, we compute − log likelihood $\left( - \log p \right)$ (Hoogeboom et al. 2022) and validity of generated molecules (Jin, Barzilay, and Jaakkola 2018), molecular stability and atom stability (Garcia Satorras et al. 2021) to evaluate their overall generation quality. For motion generation, we use FID score and Diversity following (Tevet et al. 2023). For time series generation, we follow (Yoon, Jarrett, and Van der Schaar 2019) and (Lim et al. 2023) by drawing t-SNE plots to visualize the overlap between generated data and the ground-truth. Better model tends to have a larger overlap, indicating more similar distribution.

# Comparisons on Controllability

Figure 2 illustrates the MAE trend between properties of generated molecules and the intended one as the proportion of labeled training data rises. Text2Data achieves superior performance than EDM-finetune and EDM on all properties by a remarkable margin. The results also indicate that certain properties, such as $\epsilon _ { L U M O }$ and $C v$ , are more readily controllable. For these properties, the performance of the three models converges as the amount of labeled training data becomes sufficiently large (Figure 2). We further increase the proportion of available labels in the dataset up to $1 0 0 \%$ and, as indicated in Appendix Table 3, the MAE keeps increasing when more labels are involved during training, and it gradually converges in the end. Appendix Table 4 also suggests that Text2Data surpasses the data augmentation-based method, which may suffer from the potentially poor alignment between text and data and the potential overfitting.

We depict the molecules generated as the text descriptor for polarizability shifts from “very low” to “very high” in Figure 3. Polarizability indicates the inclination of the molecule to form an electric dipole moment under an external electric field. As $\alpha$ values rise, we expect to see molecules with less symmetrical forms, as evidenced in Figure 3. This trend suggests the validity of generated molecules by Text2Data and its fine-grained controllability.

![](images/b8c88b967957d744a9f79520fb2ae9d72a63d4aa48d26cafef1d271334a6c19e.jpg)  
Figure 2: Evaluate controllability on Molecule dataset according to different proportions of paired data. Green solid line corresponds to Text2Data and two dashed lines are baseline comparisons, in which blue line is EDM and orange line is EDM-finetune. Properties of generated molecules are predicted by classifier $\phi _ { c }$ . MAE is computed between properties of generated molecules and intended properties. Lower MAE indicates better performance.

![](images/8e455011e21a9986dcbbf3deac4ee62eaf112fd100e103778c655038e84207ea.jpg)

Figure 3: Visualization of generated molecules when the polarizability increases from “very low” to “very high”   

<html><body><table><tr><td rowspan="2">Proportion (%)</td><td colspan="3">R Precision个</td><td colspan="3">Multimodal Dist.↓</td></tr><tr><td>Text2Data</td><td>MDM-finetune</td><td>MDM</td><td>Text2Data</td><td>MDM-finetune</td><td>MDM</td></tr><tr><td>2</td><td>0.34±0.01</td><td>0.37±0.01</td><td>0.31±0.01</td><td>6.48±0.06</td><td>6.19±0.05</td><td>6.67±0.02</td></tr><tr><td>4</td><td>0.39±0.01</td><td>0.42±0.01</td><td>0.38±0.01</td><td>5.99±0.05</td><td>5.83±0.04</td><td>6.01±0.04</td></tr><tr><td>6</td><td>0.43±0.01</td><td>0.43±0.01</td><td>0.40±0.02</td><td>5.85±0.06</td><td>5.78±0.05</td><td>6.01±0.06</td></tr><tr><td>8</td><td>0.44±0.01</td><td>0.43±0.01</td><td>0.42±0.01</td><td>5.65±0.04</td><td>5.75±0.05</td><td>5.90±0.04</td></tr><tr><td>10</td><td>0.45±0.01</td><td>0.45±0.01</td><td>0.44±0.01</td><td>5.74±0.07</td><td>5.76±0.07</td><td>5.84±0.06</td></tr><tr><td>20</td><td>0.47±0.01</td><td>0.47±0.01</td><td>0.45±0.01</td><td>5.61±0.04</td><td>5.68±0.11</td><td>5.73±0.14</td></tr><tr><td>30</td><td>0.48±0.01</td><td>0.47±0.01</td><td>0.45±0.11</td><td>5.61±0.05</td><td>5.66±0.06</td><td>5.80±0.09</td></tr><tr><td>40</td><td>0.49±0.01</td><td>0.46±0.01</td><td>0.45±0.01</td><td>5.61±0.05</td><td>5.63±0.09</td><td>5.90±0.04</td></tr></table></body></html>

Table 1: Evaluate controllability on HumanML3D dataset by R Precision and Multimodal Distance according to differen proportions of paired data.

As suggested in Table 1, Text2Data also outperforms MDMfinetune and MDM in the controllable generation of motions from text descriptions. While MDM-finetune is slightly better than Text2Data when the proportion of labeled training data is small-owing to milder catastrophic forgetting during finetuning with a smaller sample size-Text2Data consistently surpasses both MDM-finetune and MDM as the volume of labeled training data increases. Specifically, in this situation, Text2Data surpasses MDM-finetune and MDM in R Precision with average margins of $2 . 3 1 \%$ and $5 . 5 7 \%$ , respectively, and in Multimodal Distance with average margins of $0 . 9 3 \%$ and $3 . 3 0 \%$ , respectively. The results also indicate that an increase in labeled training data enhances the performance of controllability, which is expected as more supervision is involved.

<html><body><table><tr><td rowspan="2">Proportion (%)</td><td colspan="3">Text2Data DifrTS-y(x10-1)</td><td colspan="3">Text2Data Difrts-inetuane</td><td colspan="3">Text2Data Difrs (x10-2)</td></tr><tr><td></td><td></td><td>DiffTS</td><td></td><td></td><td>DiffTS</td><td></td><td></td><td>DiffTS</td></tr><tr><td>2</td><td>2.59±0.20</td><td>2.62±0.20</td><td>2.60±0.17</td><td>1.68±0.20</td><td>2.34±0.30</td><td>1.84±0.22</td><td>0.63±0.39</td><td>0.63±0.39</td><td>0.79±0.41</td></tr><tr><td>4</td><td>2.55±0.18</td><td>2.59±0.19</td><td>2.59±0.19</td><td>1.63±0.14</td><td>1.77±0.29</td><td>2.80±0.28</td><td>0.60±0.40</td><td>0.61±0.39</td><td>0.73±0.40</td></tr><tr><td>6</td><td>2.52±0.18</td><td>2.57±0.19</td><td>2.57±0.19</td><td>1.00±0.16</td><td>1.85±0.22</td><td>1.81±0.21</td><td>0.56±0.38</td><td>0.58±0.38</td><td>0.71±0.36</td></tr><tr><td>8</td><td>2.54±0.18</td><td>2.56±0.19</td><td>2.57±0.18</td><td>1.10±0.18</td><td>1.56±0.18</td><td>1.78±0.09</td><td>0.57±0.38</td><td>0.63±0.40</td><td>0.62±0.36</td></tr><tr><td>10</td><td>2.54±0.19</td><td>2.57±0.18</td><td>2.55±0.20</td><td>0.87±0.13</td><td>1.20±0.17</td><td>1.12±0.11</td><td>0.55±0.37</td><td>0.57±0.36</td><td>0.62±0.39</td></tr><tr><td>20</td><td>2.54±0.18</td><td>2.55±0.18</td><td>2.58±0.21</td><td>1.05±0.15</td><td>1.06±0.12</td><td></td><td>1.26±0.14 0.55±0.40</td><td>0.57±0.37</td><td>0.65±0.38</td></tr><tr><td>30</td><td>2.53±0.18</td><td>2.56±0.17</td><td>2.56±0.18</td><td>1.03±0.12</td><td>1.16±0.25</td><td></td><td>1.71±0.23 0.51±0.33</td><td>0.53±0.33</td><td>0.59±0.34</td></tr><tr><td>40</td><td>2.53±0.18</td><td>2.55±0.18</td><td>2.55±0.19</td><td>1.03±0.11</td><td>1.15±0.24</td><td></td><td>1.19±0.18 0.51±0.33</td><td>0.57±0.32</td><td>0.57±0.36</td></tr></table></body></html>

Table 2: Evaluate controllability on time series by MAE on testing set, according to different proportions of paired data. Lower MAE indicates better performance.

Additionally, we evaluate controllability of Text2Data, along with its baseline comparisons, utilizing MAE to measure the congruence between the property of generated data and the intended one within the Time Series dataset. As indicated in Table 2, Text2Data consistently excels over two baselines, DiffTS-finetune and DiffTS, across all three properties assessed. Results of another three properties are presented in Appendix Table 5, which suggests the similar conclusion. Specifically, Text2Data and DiffTS-finetune both show a marked improvement over DiffTS in controlling frequency, variance, and skewness. They also exhibit a slight edge in controlling mean, number of peaks, and linearity. The enhanced performance of Text2Data correlates with its proficiency in alleviating the issue of catastrophic forgetting while maintaining a pursuit of controllability.

# Comparisons on Generation Quality

Text2Data not only demonstrates superior performance in controllable text-to-data generation but also sustains competitive generation quality relative to baseline models.

When generating molecules from Text2Data and its baseline comparisons, as shown in Appendix Table 6, we compute $- \log p$ and validity to evaluate generation quality. The performance of Text2Data is consistently better. It surpasses EDM-finetune and EDM by average margins of $1 9 . 0 7 \%$ and $5 8 . 0 3 \%$ , respectively. It is $1 . 9 8 \%$ and $1 0 . 5 9 \%$ better than EDM-finetune and EDM on average, respectively, regarding validity of molecules. Besides, we evaluate Text2Data compared with EDM-finetune and EDM on molecular stability and atom stability (Appendix Table 7). Text2Data exceeds EDM-finetune and EDM by average margins of $2 . 3 4 \%$ and $1 7 . 3 1 \%$ , respectively, regarding molecular stability. It is also $0 . 2 9 \%$ and $1 . 2 1 \%$ better than EDM-finetune and EDM on average, respectively, regarding atom stability. The consistent improvements on all the three models result from our superior performance of Text2Data on properties (e.g., molecular stability) that are hard to control.

As indicated in Appendix Table 8, quantitative assessment of motion generation from text shows that Text2Data surpasses baseline methods in both quality and diversity. Particularly, Text2Data outperforms MDM-finetune and MDM by $2 . 7 \dot { 3 } \%$ and $3 6 . 0 5 \%$ on average, respectively, regarding FID. For diversity, Text2Data surpasses MDM-finetune and MDM by average margins of $0 . { \dot { 8 } } 1 \%$ and $3 . 7 1 \%$ , respectively. Enhanced performance is derived from Text2Data to fully leverage all samples in the dataset, while effectively mitigating catastrophic forgetting during finetuning.

We evaluate the overall quality of generating time series by making t-SNE plots of generated time series against ground truth. Substantial overlap between the generated time series and the ground truth suggests a closer distribution alignment, implying a better performance of Text2Data. As demonstrated in Appendix Figure 4, specifically, the red pattern represents the t-SNE of the ground-truth time series, whereas the blue pattern represents the t-SNE of the generated time series according to the same text description. Compared with DiffTS-finetune and DiffTS, Text2Data corresponds to the largest overlap between distributions of the generated and the ground-truth time series, suggesting its superior ability to precisely generate data according to the text description. The non-overlapping part may result from the diversity of generated time series or some other properties not labeled in the dataset so that not controlled by text description. The inferior performance of DiffTS stems from its training solely on labeled data, potentially leading to an incomplete understanding of the overall data distribution and a risk of overfitting due to limited size of labeled data. DiffTS may only partially capture the data distribution based on textual descriptions due to its susceptibility to catastrophic forgetting, which also heightens the risk of overfitting.

# Conclusion

We propose Text2Data to improve quality and property control of text-to-data generation for various modalities in lowresource scenarios using diffusion models. Text2Data uses unlabeled data to capture the prevailing data distribution via unsupervised diffusion model. It is then finetuned on text-labeled data, with a novel constraint optimization-based learning objective to ensure controllability while reducing catastrophic forgetting. Experiments show consistently superior performance to recent baselines. While Text2Data is presented as a diffusion-based framework in this article, it can be seamlessly adapted to other generative models.