# Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization

Yeji Song, Jimyeong $\mathbf { K i m ^ { * } }$ , Wonhark Park\*, Wonsik Shin, Wonjong Rhee, Nojun Kwak†

Seoul National University ldynx, wlaud1001, pwh0515, wonsikshin, wrhee, nojunk @snu.ac.kr

# Abstract

In a surge of text-to-image (T2I) models and their customization methods that generate new images of a user-provided subject, current works focus on alleviating the costs incurred by a lengthy per-subject optimization. These zero-shot customization methods encode the image of a specified subject into a visual embedding which is then utilized alongside the textual embedding for diffusion guidance. The visual embedding incorporates intrinsic information about the subject, while the textual embedding provides a new context. However, the existing methods often 1) generate images with the same pose as an input image, and 2) exhibit deterioration in the subject’s identity when facing a pose variation prompt. We first pin down the problem and show that redundant pose information in the visual embedding interferes with the pose indication in the textual embedding. Conversely, the textual embedding also harms the subject’s identity which is tightly entangled with the pose in the visual embedding. As a remedy, we propose text-orthogonal visual embedding which effectively harmonizes with the given textual embedding. We also adopt the visual-only embedding and inject the subject’s clear features using a self-attention swap. Our method is both effective and robust, offering highly flexible zero-shot generation while effectively maintaining the subject’s identity.

Extended version — https://arxiv.org/abs/2403.14155

# Introduction

Recent advancements in text-to-image (T2I) generation, especially diffusion models (Rombach et al. 2022; Balaji et al. 2023; Saharia et al. 2022; Ramesh et al. 2022; Nichol et al. 2022) have opened up a new era of image creation. Subjectdriven generation (Ruiz et al. 2023; Gal et al. 2022a; Tewel et al. 2023a; Qiu et al. 2023; Voynov et al. 2023) aims to generate novel images featuring a specific subject provided by the user. The common approach represents the subject as a new pseudo-word $( \boldsymbol { \mathsf { S } } ^ { * } )$ in the textual embedding space of the text encoder. They optimize a pseudo-word by updating the textual embedding of the pseudo-word (Gal et al. 2022b) or the diffusion model’s parameters (Ruiz et al. 2023; Tewel et al. 2023b). However, these approaches often do not align with the actual needs of users, who typically 1) possess constrained GPU resources, 2) desire real-time applications, and 3) pursue a convenient rendition with a single input image. In response to these challenges, single-image-based zeroshot customization methods (Li, Li, and Hoi 2024; Wei et al. 2023; Chen et al. 2023; Ma et al. 2023; Jia et al. 2023; Yuan et al. 2023; Xiao et al. 2023; Zhang et al. 2024; Song et al. 2025; Ye et al. 2023) have been proposed.

To obtain the pseudo-word from a single image without the time and resource-intensive optimization process, they adopt the pre-trained mappers such as MLP network (Wei et al. 2023; Yuan et al. 2023; Xiao et al. 2023), adapter (Chen et al. 2023; Shi et al. 2023; Ma et al. 2023) or multi-modal encoder (Li, Li, and Hoi 2024) to transform a subject’s image into the visual embedding. The visual embedding provides representative information about the subject’s identity from the input image, utilized along with the textual embedding which contains a novel desired context. This approach eliminates the need for additional training processes while effectively depicting the subject in new scenes.

However, the existing methods are susceptible to confusing the subject’s identity with other irrelevant details within an image. To remedy this, they employ the subject’s segmentation mask (Li, Li, and Hoi 2024; Wei et al. 2023; Ma et al. 2023; Jia et al. 2023; Xiao et al. 2023) or utilize the embedding from the deepest layers of an image encoder (Wei et al. 2023), which effectively separate the subject from the background or other surrounding objects. However, they cannot disentangle the subject’s pose from its identity as these aspects are tightly intertwined within the same pixels. As shown in Figure 1, when attempting to change the subject’s pose, the generated subject either remains in the same pose as the input image or partially loses its identity. We term this phenomenon as the pose-identity entanglement within the visual embedding, which bottlenecks the diverse applications of customization methods.

Specifically, the pose-identity entanglement prompts the visual embedding to carry the visual features of the subject in a specific pose. When using a pose-related text prompt, the input image and the text prompt provide two concurrent but conflicting embeddings to the model with different pose information. Therefore, a conflict occurs between the visual and textual embeddings, impeding their functions. To verify this conflict, we conducted the experiment with BLIP

![](images/fcf5ae527eb3b010e6964f9da1b907d7d5dc80b72348af283a4e462de584747a.jpg)  
Figure 1: The baselines struggle with either (a) strong bias toward the pose in the input image, (b) loss in the subject’s identity, or both. Our method deals with these challenges and paves the way for a more diverse and lively subject-driven generation.

Diffusion (Li, Li, and Hoi 2024) to generate images using only the textual embedding while zero-padding the image embedding. We found that this results in the subject with various poses faithfully following the text prompt, leading to a better CLIP-T score $( + 0 . 0 3 3 )$ . Using only the visual embedding also results in the images retaining the subject’s whole identity, leading to a better CLIP-I score $( + 0 . 0 4 7 )$ .

Regarding this, we have identified two significant problems stemming from the conflict between the visual and textual embeddings when modifying the subject’s pose:

• Pose bias: the generated images tend to maintain the original pose of the subject presented in the input image. • Identity loss: the subject in the generated images partially loses its identity, appearing with a different color or body shape.

The visual embedding readily interferes with the textual embedding, causing the pose bias. Conversely, the textual embedding also interferes with visual embedding, resulting in identity loss. In this paper, we focus on resolving these two problems. They are imperative tasks for advancing towards a more diverse and desirable customization, while highly challenging due to pose-identity entanglement.

To alleviate this conflict, we propose contextual embedding orchestration and self-attention swap. The former involves adjusting the visual embedding to align better with the textual embedding by orthogonalizing it to the subspace of the textual embedding vectors. The latter takes advantage of another denoising process guided by the visual-only embedding that fundamentally evades the conflict and aggregates the subject’s clean information. Our method is generic and easily applicable to any zero-shot customization method that utilizes visual and textual embeddings, since it does not require an additional tuning process. As shown in Figure 1, we demonstrate that our method significantly improves the pose variation and identity restoration of the baseline while also maintaining its performance in pose-irrelevant scenarios. We also found that our method can be extended to the optimization-based methods with a single train image.

Our contributions are summarized as:

For the first time, we unveil the pose-identity entanglement and shed light on a conflict among the visual and textual embeddings.   
Our proposed method effectively resolves the pose bias and identity loss, offering highly diverse and pose-variant subject generation.   
Our method not only is readily applicable to any singleimage-based zero-shot customization method, but also further improves the optimization-based methods with a single training image.

# Related Works

Text-to-Image Generation. Amidst a proliferation of image synthesis models, diffusion models (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020) have demonstrated their strength in producing images with remarkable fidelity and comprehensive mode coverage. Their capacity to generate diverse images has facilitated the integration with large pretrained language models (Radford et al. 2021). This synergy has given rise to diffusion-based T2I models (Rombach et al. 2022; Balaji et al. 2023; Saharia et al. 2022; Ramesh et al. 2022; Nichol et al. 2022), which can generate high-quality images with strong controllability by the guidance of natural language instructions. Recently, to increase the flexibility using this strong prior, many have combined conditioning embeddings from different modalities, e.g., concatenating text-aligned visual embedding extracted from an image with textual embedding (Sohn et al. 2023; Xiao et al. 2023; Pan et al. 2023; Li, Li, and Hoi 2024; Wei et al. 2023). However, simply combining various embeddings can cause conflict when dealing with different information they contain. Hence, our primary goal is to devise an appropriate methodology for integrating embeddings from different modalities, considering potential conflicts in their inherent information.

Subject-driven Generation. Given a few images of a userprovided subject, subject-driven generation methods (Ruiz et al. 2023; Gal et al. $2 0 2 2 \mathrm { a }$ ; Tewel et al. $2 0 2 3 \mathrm { a }$ ; Qiu et al. 2023; Voynov et al. 2023; Kim, Park, and Rhee 2024) aim to generate images containing the subject in various contexts instructed by text guidance. However, per-subject optimization suffers from computation and memory burden, leading to an introduction of zero-shot customization methods (Li, Li, and Hoi 2024; Wei et al. 2023; Chen et al. 2023; Shi et al. 2023; Ma et al. 2023; Jia et al. 2023; Yuan et al. 2023; Zhang et al. 2024; Song et al. 2025; Ye et al. 2023). They involve the mapper that enables the transformation of the input image into text-aligned visual embeddings, bypassing per-subject optimization. We first pin down the pose-identity entanglement in the visual embedding and provide a solution for it, allowing more diverse and more flexible subjectdriven generations.

Compositional Generation. Due to the limited size of the textual embeddings, large pre-trained T2I diffusion models suffer from fully compositing complex text descriptions (Liu et al. 2023). Precedent works tackle this problem and offer various solutions e.g., giving an additional segment mask layout for each related prompt as a condition (Kim et al. 2023), composing separate diffusion models where each is encoded with a divided prompt (Liu et al. 2023), and using the perpendicular gradient as a negative prompt guidance (Armandpour et al. 2023). Likewise, we deal with complex prompts consisting of visual and textual embeddings and suggest how to convey the respective information both distinctly and harmoniously.

# Preliminaries

In this work, we employ text-to-image latent diffusion model (LDM) (Rombach et al. 2022). The denoising process is implemented in the latent space using the autoencoder structure with the encoder $\mathcal { E } ( \cdot )$ and the decoder $\mathcal { D } ( \cdot )$ . Specifically, an image $x$ is projected to a latent representation $\begin{array} { r } { z } { { } = \mathcal { E } ( x ) } \end{array}$ , and decoded back to the image space giving $\tilde { x } \ = \ \partial ( z ) \ = \ \mathcal { D } ( \mathcal { E } ( x ) )$ , reconstructing $x$ , i.e., $\tilde { x } \ \approx$ $x$ . Given the pre-trained autoencoder, the latent diffusion model $\epsilon _ { \theta } ( z _ { \tau } , \tau , \mathbf { c } ) ; \tau = 1 \cdot \cdot \cdot T$ is trained with the following objective $L _ { L D M }$ where c represents the contextual embedding of textual/visual condition generally obtained from the pre-trained CLIP encoder (Radford et al. 2021):

$$
\begin{array} { r } { \mathbb { E } _ { x \sim p ( x ) , \epsilon \sim \mathcal { N } ( 0 , I ) , \mathbf { c } , \tau \sim \mathrm { u n i f o r m } ( 1 , T ) } [ \| \epsilon - \epsilon _ { \theta } ( z _ { \tau } ( x ) , \tau , \mathbf { c } ) \| _ { 2 } ^ { 2 } ] . } \end{array}
$$

During inference, $z _ { T } \sim \mathcal { N } ( 0 , I )$ is iteratively denoised to the initial representation $z _ { 0 } ( x ) = \mathcal { E } ( x )$ .

Text prompts act as a condition on LDM through the cross-attention mechanism. The latent spatial feature $f \in$ $\mathbb { R } ^ { l \times h }$ is projected to produce the query $Q = f \cdot W _ { Q } \in \mathbb { R } ^ { l \times d }$ while the text prompts are first encoded into the text embedding $\mathbf { c } ~ \in ~ \mathbb { R } ^ { l _ { c } \times h _ { c } ^ { \star } }$ and projected to yield the key, $K =$ $\mathbf { c } \cdot W _ { K } \ { \in } \ \mathbb { R } ^ { l _ { c } \times d }$ , and value, $\dot { V } = \mathbf { c } \cdot W _ { V } ^ { \mathbf { \bar { \alpha } } } \in \mathbb { R } ^ { l _ { c } \times d }$ where $l$ , $l _ { c } , d$ and $h$ are the spatial sequence length, context sequence length, dimension of key/query/value, and dimension of spatial feature/context respectively. Then the cross-attention is calculated as follows:

$$
\mathrm { A t t e n t i o n } ( Q , K , V ) = \mathrm { s o f t m a x } ( Q K ^ { T } / \sqrt { d } ) \cdot V
$$

![](images/a39dbf515e10015cfff271080b8a689a81ac09b12680df4b4f64785371272803.jpg)  
Figure 2: Dimension reduction (PCA) on the data distribution of generated images. We visualize real images and generated images with a prompt “A $S ^ { * }$ sleeping”. The baseline tends to generate images with (a) pose bias or (b) identity loss, while our method results in images both identityconserving and faithful to the text prompt.

Self-attetntion mechanism uses the same Eq. (2) where the key and value are attained from latent spatial features $f$ instead of the contextual embedding c.

# Problems: Pose Bias & Identity Loss

Single-image-based zero-shot customization methods (Li, Li, and Hoi 2024; Wei et al. 2023; Chen et al. 2023; Ma et al. 2023; Jia et al. 2023; Yuan et al. 2023; Xiao et al. 2023; Zhang et al. 2024; Song et al. 2025) compose the contextual embedding by concatenating two heterogeneous embeddings; visual $\bar { \textbf { v } } \doteq \mathbb { R } ^ { M \times h _ { c } }$ and textual $\mathbf { t } ~ \in ~ \mathbb { R } ^ { N \times h _ { c } }$ $( \mathbf { c } = [ \mathbf { v ; t } ] = [ v _ { 1 } \cdot \cdot \cdot v _ { M } ; t _ { 1 } \cdot \cdot \cdot t _ { N } ] )$ embeddings, where $M$ and $N$ are the number of tokens for the visual and textual embeddings, respectively, and $h _ { c }$ is the dimension of the embedding. Image features of a given subject’s image are transformed into the visual embedding using the pre-trained mapper and served as the embedding for a pseudo-word $( \boldsymbol { \mathsf { S } } ^ { * } )$ . The visual embedding is then combined with the textual embedding, engaging in the spatial features via cross-attention layers or additional adapters. While users generally desire to give life to their own subject and generate an image of the subject in various poses and actions, a conflict arises within the contextual embedding because the visual embedding already includes the pose information of the subject from the input image. This conflict can lead to two potential issues: either 1) being confined to the specific pose (the visual embedding weakens the textual embedding) or 2) partially losing the subject’s identity (the textual embedding impairs the visual embedding).

To better elucidate these two problems, we generated 300 images by BLIP-Diffusion (Li, Li, and Hoi 2024) using a text prompt “A $\boldsymbol { \mathrm { S } } ^ { * }$ sleeping” and extracted their representations from the highest layer of VGG-16 (Simonyan and Zisserman 2014). In Figure 2, the orange contour illustrates images generated by BLIP-D, applying dimension reduction (PCA, $n = 2$ ) to these representations. Due to the limited number of reference images with the same subject, the actual data distribution is not fully captured in the contour, though individual reference images are marked with stars. Among these, an image directly used for generation is marked with a red star. Images relevant to the desired pose, “sleeping” are marked with blue stars. We found that there are two prominent peaks of BLIP-D generated images. In area (a), generated images are distributed densely around a red star but deviate from blue stars, indicating that they firmly adhere to the input image and its specific pose. On the other hand, area (b) shows a different pattern, where the generated images are shifted away from all the stars as they cannot restore the clean identity of the subject.

![](images/27b5efcd3946dadad91b1e67327345cb9e2bc57b51c18b2447e328d2e30c29a2.jpg)  
Figure 3: Overview of our proposed method. (a) To alleviate the pose bias due to the pose-identity entanglement in the visual embedding, we conduct Orchestration, adjusting the visual embedding to be orthogonal to the textual embedding. (b) Selfattention Swap obtains self-attention key and value from another denoising process guided by visual-only embedding, which offers the subject’s clean identity.

Meanwhile, we observe that the images generated by our method, illustrated with blue contours, cover the overall distribution of stars. At the same time, their peak aligns more closely with blue stars, the images with the desired sleeping pose. This result shows that our method can successfully generate the desired pose while effectively preserving the subject’s identity.

# Methods

# Contextual Embedding Orchestration

To resolve the interference between the visual embedding $\mathbf { v }$ and the textual embedding t, we first break down the visual embedding vector $v$ into two components, $v ^ { | | }$ and $v ^ { \perp }$ , where $v ^ { | | }$ resides in the same subspace with the textual embedding t, and $v ^ { \perp }$ is perpendicular to this subspace. We argue that $v ^ { \flat }$ causes the interference with t when presented concurrently. Meanwhile, $v ^ { \perp }$ could avoid this interference, and it embodies the essential information about the subject’s identity that is orthogonal to the textual indications. Therefore, using $v ^ { \perp }$ instead of $v$ , we are able to establish the new axes in the visual embedding that interplays more effectively with t. We

# propose text-orthogonal visual embedding $\mathbf { v } ^ { \perp }$ as follows:

$$
\begin{array} { l } { \displaystyle v ^ { \perp } = v - v ^ { | | } = v - \sum _ { j = 1 } ^ { N } \langle \bar { t } _ { j } , v \rangle \bar { t } _ { j } , } \\ { \displaystyle \bar { t } _ { j } = \mathrm { n o r m a l } \left( t _ { j } - \sum _ { i = 1 } ^ { j - 1 } \langle \bar { t } _ { i } , t _ { j } \rangle \bar { t } _ { i } \right) } \end{array}
$$

where normal $( \cdot )$ means $l _ { 2 }$ normalization. $\{ \bar { t } _ { j } \}$ are the basis vectors of the textual subspace, obtained by the GramSchmidt orthogonalization process.

In detail, we incorporate all text tokens except for articles and the subject’s class name in Eq. (3) since they are closely related to pose. For example, when we generate an image of ${ } ^ { 6 6 } \mathrm { A } \mathrm { \thinspace } \mathrm { S } ^ { * }$ playing guitar”, the token “guitar” also affects the pose and should be considered, too. The new contextual embedding $\mathbf { c } ^ { \perp } = [ v _ { 1 } ^ { \perp } \cdot \cdot \cdot v _ { M } ^ { \perp } ; t _ { 1 } \cdot \cdot \cdot \cdot t _ { N } ]$ is then incorporated with latent spatial features via cross-attention. The textual embedding t effectively guides the diffusion process with alleviated interference from $v ^ { \perp }$ , reducing the effect of pose bias. We illustrate our orchestration in Figure 3(a).

# Self-attention Swap

Our orchestration can adeptly resolve conflicts within the contextual embeddings by adjusting the visual embedding. However, the strong entanglement between pose and identity information in the visual embedding can alter the subject’s identity during the process of removing the pose information of the subject. To attain the subject’s identity, we base our approach on the observation that using the visual-only embedding as the contextual embedding $( { \bf c } _ { \bf v } , \alpha =$ $[ v _ { 1 } \cdots v _ { M } ; \emptyset ] )$ effectively preserves the subject’s identity, as it is free from the interference of the textual embedding.

Our objective is to inject the desired features of the visualonly embedding while at the same time maintaining a novel pose of the subject obtained from the text-orthogonal embedding. To this end, we adopt the second denoising process $\lbrace z _ { \tau } ^ { \prime } \rbrace _ { \tau = 1 \cdots T }$ where the visual-only embedding is provided as contextual embedding $( \mathbf { c } _ { \mathbf { v } , \mathcal { O } } = \bar { [ v _ { 1 } \cdot \cdot \cdot v _ { M } ; \mathcal { O } ] } )$ . It is distinct from the original denoising process $\{ z _ { \tau } \} _ { \tau = 1 \cdots T }$ that utilizes the contextual embedding $\mathbf { c } ^ { \perp } = [ \dot { v } _ { 1 } ^ { \perp } \cdot \cdot \cdot v _ { M } ^ { \perp } ; t _ { 1 } \cdot \cdot \cdot t _ { N } ]$ . Then, we modify the self-attention layers of {zτ }τ=τ1···τ2 , to swap the key and value with those from $\{ z _ { \tau } ^ { \prime } \} _ { \tau = \tau _ { 1 } \cdots \tau _ { 2 } }$ with timestep hyperparamters $\tau _ { 1 }$ and $\tau _ { 2 }$ . Our proposed selfattention swap can be formulated as follows:

![](images/5c2abbabee63e6f4f7b4ee0c03c3cfe4c9f30ddeb224cefec4449966ef82c250.jpg)  
Figure 4: Comparisons with baseline. We use the same seed for each pair of images to effectively and progressively demonstrate how our orchestration and self-attention swap improve the baseline.

$$
\operatorname { A t t n S w a p } ( z _ { \tau } , z _ { \tau } ^ { \prime } ) : = \operatorname { A t t e n t i o n } ( Q _ { \tau } , K _ { \tau } ^ { \prime } , V _ { \tau } ^ { \prime } ) .
$$

Here, $Q _ { \tau } \in \mathbb { R } ^ { l \times d }$ is the query from $z _ { \tau }$ and $K _ { \tau } ^ { \prime } , V _ { \tau } ^ { \prime } \in \mathbb R ^ { l \times d }$ are the key and the value from $z _ { \tau } ^ { \prime }$ where $l$ and $d$ are the spatial sequence length and features dimension, respectively. We generate a novel pose of the subject with the original denoising process while simultaneously incorporating values of the clear identity from $V ^ { \prime }$ into the location based on the attention map obtained with $Q$ and $K ^ { \prime }$ , which indicates where the corresponding latent pixels are likely to exist.

Aiming to allow flexibility in the remaining aspects while preserving the subject’s identity, we restrict the swaps to latent pixels assigned to the subject. Inspired by (Hertz et al. 2022; Cao et al. 2023), we obtain the binary subject mask, $m \in \{ 0 , 1 \} ^ { l }$ , from cross-attention map of token $S ^ { * }$ in the original process using a fixed threshold and restore the outputs in the background of the original process as:

$$
\mathrm { A t t n S w a p } ( z _ { \tau } , z _ { \tau } ^ { \prime } ) \odot m + \mathrm { A t t n S w a p } ( z _ { \tau } , z _ { \tau } ) \odot ( 1 - m ) .
$$

Here, $\odot$ represents Hadamard product. Note that the second term is the self-attention of the original process $z _ { \tau }$ . Figure 3(b) illustrates our self-attention swap process.

Self-attention swap is closely related to the image editing methods that transfer the properties of the source image to the edited image by swapping self-attention (Cao et al. 2023; Tumanyan et al. 2022) or cross-attention (Hertz et al. 2022; Parmar et al. 2023; Couairon et al. 2022). While they edit the source image without changing the other context, our method generates the subject across a range of contexts, accurately incorporating its identity to the proper regions.

# Experiments

Datasets. Since prevailing benchmark datasets (Ruiz et al. 2023; Kumari et al. 2023) primarily utilize the generating prompts related to changing the texture or introducing new objects, they often fall short in effectively evaluating the crucial aspect of modifying subject poses. To address this limitation, we have constructed a new dataset, Deformable Subject Set (DS set), to effectively assess the model’s capability to modify a subject’s pose. The DS set comprises 38 live animals from the DreamBooth (Ruiz et al. 2023) and CustomDiffusion (Kumari et al. 2023), along with 11 prompts specifically designed to focus on the deformation of the subjects’ poses. Furthermore, we also utilized the DreamBooth dataset (DB set) (Ruiz et al. 2023) to evaluate the model’s capacity in typical scenarios.

Metrics. Following DreamBooth (Ruiz et al. 2023), we measured the subject fidelity using CLIP-I and DINO-I, and measured text alignment using CLIP-T. For the DS set, which includes object-related action prompts, we found that the newly generated object affects the image-alignment score, for example, by occluding the subject. Therefore, we additionally measured the masked scores (Avrahami et al. 2023) for CLIP-I and DINO-I, incorporating the subject’s segmentation mask for both the input images and generated images. This approach mitigates the impact of new objects added alongside the subject, allowing for a more focused comparison of the subject’s identity.

Table 1: Quantitative Comparison on Deformable Subject Set and DreamBooth dataset (Ruiz et al. 2023). ‘M-’ indicate the metrics using segmentation masks.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="5">Deformable Subject Set</td><td colspan="3">DreamBooth Set</td></tr><tr><td>CLIP-T(↑)</td><td>M-CLIP-I(↑)</td><td>M-DINO-I(↑)</td><td>CLIP-I(↑)</td><td>DINO-I(↑)</td><td>CLIP-T(↑)</td><td>CLIP-(↑)</td><td>DINO-(↑)</td></tr><tr><td>BLIP-D (Li, Li, and Hoi 2024)</td><td>0.262</td><td>0.885</td><td>0.680</td><td>0.835</td><td>0.684</td><td>0.295</td><td>0.812</td><td>0.660</td></tr><tr><td>w/Orchestration</td><td>0.276</td><td>0.883</td><td>0.668</td><td>0.819</td><td>0.663</td><td>0.298</td><td>0.805</td><td>0.647</td></tr><tr><td>w/SA Swap</td><td>0.262</td><td>0.886</td><td>0.689</td><td>0.837</td><td>0.694</td><td>0.293</td><td>0.815</td><td>0.675</td></tr><tr><td>Ours</td><td>0.275</td><td>0.886</td><td>0.681</td><td>0.821</td><td>0.676</td><td>0.296</td><td>0.809</td><td>0.665</td></tr><tr><td>ELITE (Wei et al. 2023)</td><td>0.285</td><td>0.835</td><td>0.574</td><td>0.751</td><td>0.486</td><td>0.294</td><td>0.792</td><td>0.664</td></tr><tr><td>w/Orchestration</td><td>0.292</td><td>0.834</td><td>0.570</td><td>0.754</td><td>0.493</td><td>0.295</td><td>0.789</td><td>0.658</td></tr><tr><td>w/SA Swap</td><td>0.288</td><td>0.837</td><td>0.581</td><td>0.754</td><td>0.489</td><td>0.292</td><td>0.796</td><td>0.673</td></tr><tr><td>Ours</td><td>0.300</td><td>0.837</td><td>0.581</td><td>0.755</td><td>0.502</td><td>0.294</td><td>0.792</td><td>0.670</td></tr></table></body></html>

![](images/16ca909c52ff9cb195874286648d1cd7e15ef74d055057edb4800c59e95f75f2.jpg)  
Figure 5: A Scatter Plot of results from the DS set. Each component of our method improves its respective axis upon the baselines, with our final method lying on the Pareto front.   
Figure 6: User Study results. Our method is steadily preferred over the baselines in both alignments.

# Qualitative Results

We present a comparative analysis of our method with the baselines in Figure 4. While the baselines tend to replicate the subject’s pose from the input image, our method effectively modifies the pose aligning with the provided prompt. It is noteworthy that when using object-related prompts, the baselines tend to generate the object without the specified actions, while our method accurately depicts the subject interacting with the given object. Retaining accurate information about the subject from the visual-only embedding, ours also preserves the identity better than the baseline. We use the same seed for images in paired comparisons to validate that each component of our method progressively improves upon the baseline. We provide more various qualitative results, and results from different random seeds, in Appendix.

# Quantitative Results

Table 1 and Figure 5 show quantitative analyses. For the DS set, which consists of the prompts necessitating the deforma

0.8 BLIP-Diffusion ELITE Ours   
0.6 0.2 二1   
0.0 Text-alignment Image-alignment

tion of the subject’s pose, our method significantly improves the text alignment without compromising the image alignment. Each component of our method, orchestration and self-swap, improves text and image alignment, respectively, in line with their objective of pose bias mitigation and identity preservation. We also include additional comparisons with other baselines (Zhang et al. 2024; Song et al. 2025; Ye et al. 2023) in Appendix. Figure 5 illustrates this tendency better with the consistent improvement over the baselines along each axis. Considering both axes involve a trade-off, our combined method becomes closer to the Pareto front, indicating overall improvement. The DB set (Ruiz et al. 2023) consists of prompts unrelated to pose and includes subjects, half of which are non-deformable, making it far from our target. However, quantitative analysis on the DB set in Table 1 confirms that our methodology remains comparable to the existing baseline even in typical scenarios.

# User Study

We further evaluate our method through the user study conducted with Amazon Mechanical Turk. Human raters were given a subject’s input image, a prompt, and two synthesized images (ours and the baseline) of the subject. They were then asked to select the preferable one for each of the following questions: (1) Image Alignment: “Which of the images best reproduces the identity (e.g., item type and details) of the reference item?” (2) Text Alignment: “Which of the images is best described by the reference text?”. We used 38 animal subjects and 5 to 11 pose-related prompts per subject, assigning 5 raters for each example. As shown in Figure 6, our method is preferred over the baseline in both text alignment (Ours $6 2 . 0 \%$ vs. BLIP-Diffusion $3 8 . 0 \%$ , Ours $56 . 3 \%$ vs. ELITE $4 3 . 7 \%$ ) and image alignment (Ours $5 1 . 5 \%$ vs. BLIP-Diffusion $4 8 . 5 \%$ , Ours $6 0 . 0 \%$ vs. ELITE $4 0 . 0 \%$ ), underscoring our method’s ability from a human perspective.

# Ablations

Our orchestration eliminates the conflicting elements in the visual embedding that interfere with the textual embedding. Then, a subsequent question arises: why don’t we directly eliminate the pose information within the visual embedding utilizing the text description of the input image itself? In other words, why not orthogonalize the visual embedding with respect to the text description of the pose found within the input image itself instead of considering an interference with the textual embedding? To investigate the effect of orthogonalization with respect to the embedding vectors, we conduct the following experiment: we adopt the existing image captioning model (Li et al. 2022) and human annotations that describe the pose of the subject in the input image, and transform the visual embedding to be orthogonal to these textual embeddings using Eq. (3). As a result, unalleviated conflict among the contextual embeddings still generates images that are strongly biased toward the input image, leading to lower CLIP-T scores 0.265 and 0.263, respectively than our orchestration (0.276). Resolving this conflict is effective for text alignment and also efficient as it does not require any additional language model or human endeavor.

We also conducted an ablation study on the roles of each component, orchestration and self-attention swap, by progressively applying them. As shown in Table 1 and Figure 4, orchestration makes huge progress on faithfully following the text prompt, while self-attention swap effectively restores the subject’s identity. When both are applied, our proposed method successfully generates images that improves both text and image alignments.

# Analysis

To verify that the visual embedding is properly adjusted after orchestration, we analyze cross-attention maps for a token corresponding to the visual embedding i.e., a token of the pseudo-word $( \boldsymbol { \mathrm { S } } ^ { * } )$ . Cross-attention maps indicate the amount of information conveyed from the tokens to the latent spatial features, therefore, we can obtain insight about the information flows of each token. Figure 7 illustrates that in the baseline (Li, Li, and Hoi 2024), a conflict among the contextual embedding leads the visual embedding to highlight the irrelevant areas. The textual embedding of a pose-related token (<standing $>$ ) also shows a similar tendency, resulting in the image noncompliant with a text prompt. Meanwhile, our orchestration effectively resolves the conflict, integrating the embeddings in the proper areas. Quantitatively, we calculate the total sum of the normalized attention score within the subject’s segmentation mask (Ren et al. 2024) using the DS set. With our method, attention scores for the visual and textual embedding of the pose-related token are $\times 6 . 2$ and $\times 3 . 9 5$ more concentrated on the subject compared to the baseline, respectively.

![](images/d63f6c08d9565112ff0a6e07875c725fc1a92a05c160cc80c1ef53d1010b5081.jpg)  
Figure 7: Visualization of cross-attention map. (red: high value, blue: low value) Ours injects visual and text information in the proper areas compared to the baseline.

Table 2: Quantitative results on DS set with optimizationbased methods using a single image.   

<html><body><table><tr><td>Method</td><td>CLIP-T(↑)</td><td>M-CLIP-I(↑)</td><td>M-DINO-I(↑)</td></tr><tr><td>DB</td><td>0.301</td><td>0.862</td><td>0.612</td></tr><tr><td>Ours</td><td>0.306</td><td>0.862</td><td>0.618</td></tr><tr><td>TI</td><td>0.271</td><td>0.845</td><td>0.586</td></tr><tr><td>Ours</td><td>0.264</td><td>0.848</td><td>0.593</td></tr></table></body></html>

# Single-image-based Optimization

Our method addresses a general problem in zero-shot customization. Optimization-based methods are relatively free from these issues, as they update trainable parameters instead of directly using the visual embedding, but they require a lengthy optimization process per subject. However, we found that they also face a similar problem when only a single training image is available, and by applying our method, we can enhance text and image alignment in these methods. Using the DS set, we report the results for Dreambooth (Ruiz et al. 2023) and Textual Inversion (Gal et al. 2022a) in Table 2. For TI, using a single image severely impairs its performance, generating only motion-related objects in a text prompt while omitting the subject. Our method pushes the generated images to restore the subject’s identity, leading to the better trade-off between image and text alignment. We provide qualitative results in Appendix.

# Conclusion

Limitations. When a set of numerous prompts in different instructions is given at once, our method cannot accurately orthogonalize the visual embedding to each textual embedding, resulting in the images omitting some instructions.

In this paper, we explore notable problems in pose variation tasks when using the single-image-based zero-shot customization methods. We focus on a conflict between the visual and textual embeddings, which affects both embeddings and leads to the pose bias and identity loss, respectively. We alleviate the conflict by eliminating the interfering elements in the visual embedding while adopting the subject’s clean identity from the visual-only embedding. Amidst the proliferation of zero-shot customization methods and the utilization of the visual embedding, our key insight unveils a crucial aspect for achieving a more diverse generation.