# Online MDP with Prototypes Information: A Robust Adaptive Approach

Shuo $\mathbf { S u n } ^ { 1 }$ , Meng $\mathbf { Q } \mathbf { i } ^ { 2 }$ , Zuo-Jun Max Shen1,3

1Department of Industrial Engineering and Operations Research, UC Berkeley, Berkeley, California 94720, USA 2SC Johnson College of Business, Cornell University, Ithaca, New York 14853, USA 3Faculty of Engineering and Faculty of Business and Economics, The University of Hong Kong, Hong Kong, China shuo sun $@$ berkeley.edu, $\mathrm { m q } 5 6 @$ cornell.edu, maxshen $@$ berkeley.edu

# Abstract

In this work, we consider an online robust Markov Decision Process (MDP) where we have the information of finitely many prototypes of the underlying transition kernel. We consider an adaptively updated ambiguity set of the prototypes and propose an algorithm that efficiently identifies the true underlying transition kernel while guaranteeing the performance of the corresponding robust policy. To be more specific, we provide a sublinear regret of the subsequent optimal robust policy. We also provide an early stopping mechanism and a worst-case performance bound of the value function. In numerical experiments, we demonstrate that our method outperforms existing approaches, particularly in the early stage with limited data. This work contributes to robust MDPs by considering possible prior information about the underlying transition probability and online learning, offering both theoretical insights and practical algorithms for improved decision-making under uncertainty.

Extended version — https://arxiv.org/abs/2412.14075

# 1 Introduction

Markov Decision Processes (MDPs) have become a fundamental framework for sequential decision-making under uncertainty, with applications spanning diverse fields such as control, healthcare and supply chain management. Despite their widespread use, MDPs often face challenges when the true transition dynamics are unknown, potentially leading to suboptimal decisions.

In many real-world scenarios, decision-makers may rely on external datasets to parameterize the MDP model, but have access to multiple plausible model estimates, each potentially leading to different optimal policies. This setting is commonly seen in many applications, for example, the healthcare system (Steimle, Kaufman, and Denton 2021). Consider the context of optimizing its breast cancer screening protocol. Decision-makers might have access to local hospital data, a national cancer research institute’s model, and an international meta-analysis. Each source could suggest a different optimal screening frequency and age range for mammograms. This situation exemplifies the challenge of determining which model to trust or how to integrate insights from multiple sources to create a robust and effective policy when faced with various plausible model estimates. Similar challenges with multiple transition models arise in recommendation systems, supply chain management, and other domains where early performance and worst-case guarantees are crucial (Chatterjee et al. 2020). Moreover, the concept of multiple parameter models is analogous to the scenario-based stochastic programming literature, where each scenario represents a different possibility of the uncertain parameters.

In this work, we focus on this multi-model setting where there are multiple models (prototypes) of the transition probabilities of the underlying Markov chain and the goal is to identify the true model and therefore solve for the optimal policy. Moreover, we address the problem in an online setting that we need to make real-time decisions with streaming data while knowing the prototypes. These prototypes could be estimated from offline dataset. The key challenge in such settings is two-fold: First, we need to efficiently identify the true underlying transition model while making decisions in real-time. Second, and perhaps more critically, we must ensure good performance during the learning phase when data is limited and model uncertainty is high. Classical online MDP algorithms focus primarily on achieving sublinear regret but may perform poorly in early stages and lack worstcase performance guarantees.

To address these challenges, we propose a novel robust learning algorithm that efficiently identifies the true transition kernel while guaranteeing model performance during the exploration stage. Our approach gradually updates the discrete prototype set and calculates the optimal robust policy, which achieves sublinear regret and provides a lower bound for the algorithm performance at each episode. As data accumulates, we propose a termination mechanism that efficiently identifies the true transition kernel.

Our work differentiates itself from existing approaches in several key aspects. First, we consider an online MDP with structural information of prototypes, which has not been studied before. Moreover, most work in robust MDP considers an offline setting or assumes access to a generator but we consider an online setting. Typically, robust MDP approaches assume a fixed ambiguity set size to calculate the optimal policy in the worst-case scenario. In contrast, we aim to optimize performance under the true model and gradually shrink the ambiguity set as data accumulates. This fundamental difference in goals sets us apart from existing methods that consider exogenous robustness, where the environment may be perturbed and the goal is to optimize for the worst-case scenario. In those works, the size of the uncertainty set is known, but the nominal transition probability is unknown. We, however, assume the existence of a true nominal system and design an adaptive robust algorithm that remains robust when data points are limited – what we term endogenous robustness. Our ambiguity set shrinks as we collect more data. Our work is closest to the online robust MDP work by (Dong et al. 2022). However, our work has an essential difference: they consider exogenous robustness and fix the size of the ambiguity set, whereas we aim to optimize the model performance under the true kernel. It is important to emphasize that there are no existing sublinear regret results for online robust MDP problems, and achieving such results is notoriously difficult in general. In this work, by leveraging known prototypes of the underlying transition probability, we are able to provide sublinear regret bounds. This demonstrates the significant benefit of incorporating useful prior information about the underlying MDP model. Our approach could offer valuable insights for future work on model-based MDPs, particularly in scenarios where structural information is available or can be inferred. The main contributions of our work are as follows:

1. We propose a novel algorithm for learning robust policies in MDPs with multiple transition dynamic prototypes in an online setting (RPO-AAS). We show that our algorithm achieves sublinear regret with respect to the optimal policy for the true model and introduce an early stopping mechanism that allows our algorithm to converge to the true model more quickly with sufficient evidence.   
2. We also propose a non-robust algorithm (NRPO-NPC) and analyze the technical performance guarantees. This algorithm does not calculate the robust optimal policy, but selects the prototype that is closest to the empirical distribution and runs the optimal policy corresponding to this prototype. Interestingly, we show that introducing robustness in the algorithm does not sacrifice efficiency.   
3. Through numerical experiments, we demonstrate the effectiveness of our approach compared to existing methods, showing improved performance particularly in the early stage with limited data.

# 2 Related Work

Recent research has explored MDPs with parameter ambiguity using multiple models. (Steimle, Kaufman, and Denton 2021) and (Buchholz and Scheftelowitsch 2019) consider finding a policy that maximizes a weighted performance across multiple models of MDPs. They proved NPhardness of the problem and developed exact and approximate solution methods. (Ahmed et al. 2017) explore sampling rewards and transition probabilities to generate a finite set of MDPs and find a policy to minimize the maximum regret over the set of MDPs. Our work differs from these approaches in two key aspects. Firstly, we consider an online setting, whereas previous works focused on offline setting. Secondly, our goal is to identify the true model and optimize its performance during exploration while guaranteeing robustness, rather than optimizing weighted performance for given weights or worst-case regret across all models.

The problem of regret minimization in MDPs with a fixed reward function has been studied extensively since (Burnetas and Katehakis 1997) and (Auer and Ortner 2006). Provably efficient learning algorithms fall into two main categories: The first applies optimism in the face of uncertainty principle (Kearns and Singh 2002; Brafman and Tennenholtz 2002; Azar, Osband, and Munos 2017) while the second utilizes posterior sampling reinforcement learning (Osband, Russo, and Van Roy 2013; Osband and Van Roy 2017). (Agrawal and Jia 2017) combine these approaches, leveraging both the optimistic principle and posterior sampling to achieve a regret bound for weakly communicating MDPs. Currently the best regret bound for finite MDP is $\tilde { O } ( \sqrt { H | S | | A | T } + H ^ { 2 } | S | ^ { 2 } | A | + H \sqrt { T } )$ from the UCBVI algorithm, where $s$ is the finite space of states, $\mathcal { A }$ is the set of finite actions and $H$ is the number of horizons (Azar, Osband, and Munos 2017). Despite these advancements, to our knowledge no existing work considers robust algorithms in MDPs with sublinear regret compared to the optimal reward.

Robust MDPs consider the transition kernels that take values from an uncertainty set and learn an optimal robust policy that maximizes the worst-case value function. Most work in Robust MDP assumes that the the uncertainty set is known (Iyengar 2005; Nilim and El Ghaoui 2005; Xu and Mannor 2010). Recently some work consider the robust optimal policy when the uncertainty set is not exactly known, or say reinforcement learning. Some work assumes that there is a generative model (Panaganti and Kalathil 2022; Yang, Zhang, and Zhang 2022) or assumes an offline dataset is present (Zhou et al. 2021; Qi and Liao 2020; Kallus et al. 2022; Ma et al. 2022). To our knowledge, only (Dong et al. 2022) considers the robust policy learning in online setting. They propose algorithms that achieve a regret of $\tilde { O } ( | S | | \mathcal { A } | ^ { 2 } H ^ { 2 } )$ under s-rectangular uncertainty set. However, these work have a different goal from our work. As discussed before, they consider the exogeneous robustness, while we consider endogeneous robustness. They consider an ambiguity set with fixed size while the radius of our algorithm is shrinking we when collect more data.

Another line of research characterizes the uncertainty through adversarial MDP formulations, where the environment parameters can be adversarially chosen. Most studies focus on the setting where only the reward function can be corrupted, while transition dynamics of the MDP remain fixed but potentially unknown (Neu et al. 2010; Cai et al. 2020; Jin et al. 2020; Rosenberg and Mansour 2019; Jin and Luo 2020; Cai et al. 2020). (Neu et al. 2010) first proposes the online loop-free setting and show a regret of $\stackrel { \cdot } { O } ( \dot { L } ^ { 2 } \sqrt { T | \mathcal { A } | } / \alpha )$ under some assumptions, where $L$ is the length of the longest path in the graph, $T$ is the number of episodes, and $\alpha$ is a probability parameter in the assumption. Some work investigates settings where adversaries can corrupt transition metrics. (Lykouris et al. 2021) consider the setting that the transition is only allowed to be adversarially chosen for $C$ out of the $T$ total episodes and establish a regret of ${ \tilde { O } } ( C ^ { 2 } + { \sqrt { T } } )$ . Our prototype elimination approach shares similarities with arm-elimination methods in multiarmed bandit problems (Even-Dar et al. 2006; Audibert and Bubeck 2010), but handles the additional complexity of state transitions rather than simple rewards.

# 3 Problem Formulation and Preliminaries 3.1 Problem Formulation

We consider a Markov Decision Process defined by a tuple $( S , A , P _ { 0 } , r )$ , where $s$ is the finite state space and $\mathcal { A }$ is the finite action space, $P _ { 0 } : \mathcal { S } \times \mathcal { A } \times \mathcal { S }  [ 0 , 1 ]$ is the transition kernel, $r : S \times \mathcal { A }  \mathbb { R }$ is the reward function. More specifically, we use $P _ { 0 } ( s , a )$ and $r ( s , a )$ to denote the probability distribution of the next state and immediate reward when taking action $a$ at state $s$ . Let $P _ { 0 } ( s ^ { \prime } | s , a )$ denote the probability of arriving at state $s ^ { \prime }$ when choosing action $a$ at state $s$ . Moreover, we assume the reward $r ( s , a )$ is deterministic, and without loss of generality, $r ( s , a )$ belongs to $[ 0 , 1 ]$ . Extending the algorithms to the setting with unknown reward does not add significant difficulty.

Loop-Free MDP In this work, we consider an episodic MDP with finite horizons. We assume the MDP has a loopfree structure: The state space can be decomposed into $L + 1$ non-intersecting layers $S _ { 0 } , \ldots , S _ { L }$ such that ${ \mathcal { S } } = \cup _ { l = 0 } ^ { L } S _ { l }$ , $S _ { i } \cap S _ { j } = \emptyset$ for $i \neq j$ . Moreover, the first and the last layers are singletons, i.e., ${ \cal S } _ { 0 } = \{ s _ { 0 } \}$ , ${ \cal S } _ { \cal L } = \{ s _ { \cal L } \}$ . Let $\mathcal { L } ( s )$ denote the layer of state $s$ . The loop-free structure means the transitions are only possible between consecutive layers. These assumptions are not necessary but are commonly adopted in literature, intended to simplify notation and analysis, and can be modified for a more general setup (Rosenberg and Mansour 2019; Jin et al. 2020).

Transition Prototypes In this work, we aim to illustrate the benefit of utilizing prior information about the transition probabilities. Specifically, we consider prototypes that are known to the decision-maker, each of which may correspond to an underlying model or mechanism that is driving the transition of the states. We assume that for each layer $l$ , there are $K _ { l }$ prototypes of the transition kernel in the candidate set, denoted as $\{ 1 , 2 , \dots , K _ { l } \}$ and collectively referred to as $\textstyle \mathcal { K } _ { l }$ . For any layer $l$ , the transition probability at state $s$ and action $a$ defined by prototype $k \ \in \ \mathcal { K } _ { l }$ is $P ^ { k } ( s , a )$ . The true transition kernel of each layer $l$ , denoted as $k _ { l } ^ { * }$ , must be one of the prototypes, meaning that $\otimes _ { s \in S _ { l } , a \in \mathcal { A } } P _ { 0 } ( s , a ) = \otimes _ { s \in \mathcal { S } _ { l } , a \in \mathcal { A } } P ^ { k _ { l } ^ { * } } ( s , a )$ .

In the algorithm, we update the candidate set of prototypes gradually, and we let $\mathcal { K } _ { l , t }$ denote the set of prototypes in episode $t$ . We remove the prototypes that are unlikely to be true as we collect more data. For the prototypes, we make the following structural assumption, which essentially states that if the gap between some kernels at a particular state $s$ in the layer and action $a$ is small, then the difference at other states in this layer cannot be too large.

Assumption 1. For any layer $l = 0 , \ldots , L$ , any state $s \in$ $\begin{array} { r } { { \cal { S } } _ { l } , } \end{array}$ , action $a \in { \mathcal { A } }$ , and any prototype $k \in \mathcal { K } _ { l }$ , if for some constant $u \in \mathbb { R }$ , the $l _ { 1 }$ -norm $\Vert P ^ { k } ( s , a ) - P ^ { 0 } ( s , a ) \Vert _ { 1 } \leq u ,$ , then there exists a constant $\gamma \in \mathbb { R }$ such that $\| P ^ { k } ( s ^ { \prime } , a ^ { \prime } ) -$ $P ^ { 0 } ( s ^ { \prime } , a ^ { \prime } ) \| _ { 1 } \leq \gamma u$ for any other $s ^ { \prime } \in S _ { l } , a ^ { \prime } \in \mathcal { A }$ .

Assumption 1 reflects that states within the same layer often share similar transition patterns, which is common in practice. The constant $\gamma$ quantifies the variability of transition probability differences across state-action pairs, while $u$ represents the magnitude of these differences for a reference state-action pair. Importantly, our theoretical results depend solely on $\gamma$ , not on the absolute differences captured by $u$ . This formulation provides flexibility in accommodating various MDP structures while maintaining analytical tractability. While this assumption helps establish theoretical guarantees, our numerical experiments in Section 7.2 show that the algorithm maintains good performance even with random prototypes where this assumption may not hold.

In this paper, we use $\Vert \cdot \Vert _ { 1 }$ to denote the $l _ { 1 }$ -norm between two transition probability vector. For any two transition kernels at state $s$ and action $a$ , $P _ { 0 } ( s , a )$ and $P _ { 1 } ( s , a )$ , we define $\begin{array} { r } { \| P _ { 0 } ( s , a ) , P _ { 1 } ( s , a ) \| _ { 1 } = \sum _ { s ^ { \prime } \in S } | P _ { 0 } ( s ^ { \prime } | s , a ) - P _ { 1 } ( s ^ { \prime } | s , a ) | } \end{array}$ .

In each episode $t$ , let $\pi _ { t }$ denote the policy, which is a mapping from the state space $s$ to action space $\mathcal { A }$ . Given the transition kernel $P _ { 0 }$ and policy ${ { \pi } _ { t } }$ , the expected reward in episode $t$ is:

$$
\mathbb { E } [ \sum _ { l = 0 } ^ { L - 1 } r ( s _ { l } , \pi _ { t } ( s _ { l } ) ) | P _ { 0 } , \pi _ { t } ] ,
$$

where $s _ { l }$ is the state visited in layer $l$ and episode $t$ and $\pi _ { t } ( s _ { l } )$ is the corresponding action. Then, the total expected reward of the learner over $T$ episodes is:

$$
R ( ( \pi _ { t } ) _ { t \in [ T ] } , P _ { 0 } ) = \sum _ { t = 1 } ^ { T } \mathbb { E } [ \sum _ { l = 0 } ^ { L - 1 } r ( s _ { l } , \pi _ { t } ( s _ { l } ) ) | P _ { 0 } , \pi _ { t } ] .
$$

For a stationary policy $\pi$ , with a slight abuse of notation, the total expected reward is given by

$$
R ( \pi , P _ { 0 } ) = \sum _ { t = 1 } ^ { T } \mathbb { E } [ \sum _ { l = 0 } ^ { L - 1 } r ( s _ { l } , \pi ( s _ { l } ) ) | P _ { 0 } , \pi ] .
$$

Therefore, the regret can be defined as

$$
R e g = R ( \pi ^ { * } , P _ { 0 } ) - R ( ( \pi _ { t } ) _ { t \in [ T ] } , P _ { 0 } ) ,
$$

where $\begin{array} { r } { \pi ^ { * } \in \arg \operatorname* { m a x } _ { \pi } \mathbb { E } [ \sum _ { l = 0 } ^ { L - 1 } r ( s _ { l } , \pi ( s _ { l } ) ) ] } \end{array}$ .

Our regret definition diverges from that in the robust MDP literature (Dong et al. 2022; Zhou et al. 2021) which optimizes worst-case reward over an ambiguity set, with regret measured as the gap between worst-case rewards of the algorithm’s policy and the optimal worst-case robust policy. In contrast, we optimize reward under the true transition kernel, aligning with the online MDP framework (Neu et al. 2010).

# 3.2 Preliminaries

Occupancy measures. We now reformulate the learner’s problem using the concept of occupancy measures. We introduce occupancy measures for the purpose of analysis, which has been widely used in the analysis for loop-free

MDP (Jin et al. 2020; Rosenberg and Mansour 2019). Given a policy $\pi$ and transition kernel $P$ , for any state $s \in \ S _ { l }$ , $s ^ { \prime } \in S _ { l + 1 }$ , the occupancy measure $q ^ { P , \pi }$ is defined as:

$$
q ^ { P , \pi } ( s , a , s ^ { \prime } ) = \mathbb { P } \lbrack s _ { \mathcal { L } ( s ) } = s , \pi ( s ) = a , s _ { \mathcal { L } ( s ) + 1 } = s ^ { \prime } \vert P , \pi \rbrack .
$$

An occupancy measure satisfies the following two properties and these two properties suffice to define any function $q : \mathcal { S } \times \mathcal { A } \times \mathcal { S } \to [ 0 , 1 ]$ to be an occupancy measure. (1) The learner traverses every layer in each episode due to the loop-free structure, i.e., for every $l = 0 , \ldots , L - 1$ ,

$$
\sum _ { s \in S _ { l } } \sum _ { a \in \mathcal { A } } \sum _ { s ^ { \prime } \in S _ { l + 1 } } q ( s , a , s ^ { \prime } ) = 1 .
$$

(2) The probability of entering a state from the previous layer equals the probability of leaving it. Thus, for every $l = 1 , \ldots , L - 1$ and $s \in  { S _ { l } }$ ,

$$
\sum _ { s ^ { \prime } \in S _ { l + 1 } } \sum _ { a \in \mathcal { A } } q ( s , a , s ^ { \prime } ) = \sum _ { s ^ { \prime } \in S _ { l - 1 } } \sum _ { a \in \mathcal { A } } q ( s ^ { \prime } , a , s ) .
$$

Given an occupancy measure $q$ , the transition function $P ^ { q }$ and the policy $\pi ^ { q }$ can be induced as follows:

$$
P ^ { q } ( s ^ { \prime } | s , a ) = \frac { q ( s , a , s ^ { \prime } ) } { \sum _ { y \in S _ { \mathcal { L } ( s ) + 1 } } q ( s , a , y ) } ,
$$

$$
\pi ^ { q } ( a | s ) = \frac { \sum _ { s ^ { \prime } \in S _ { \mathcal { L } ( s ) + 1 } } q ( s , a , s ^ { \prime } ) } { \sum _ { b \in \mathcal { A } } \sum _ { s ^ { \prime } \in S _ { \mathcal { L } ( s ) + 1 } } q ( s , b , s ^ { \prime } ) } .
$$

Then the problem of policy learning can be transformed to learning an occupancy measure $q _ { t } \in \Delta ( P _ { 0 } )$ in each episode $t$ , where $\Delta ( P _ { 0 } )$ is the set of all occupancy measures of an MDP with transition kernel $P _ { 0 }$ . With the definition of the occupancy measure, we redefine the expected reward and regret. The total expected reward of the learner is

$$
\begin{array} { r l } {  { R ( ( \pi _ { t } ) _ { t \in [ T ] } , P _ { 0 } ) = \sum _ { t = 1 } ^ { T } \mathbb { E } [ \sum _ { l = 0 } ^ { L - 1 } r ( s _ { l } , \pi _ { t } ( s _ { l } ) ) | P _ { 0 } , \pi _ { t } ] } } \\ & { = \sum _ { t = 1 } ^ { T } \langle q ^ { P _ { 0 } , \pi _ { t } } , r \rangle } \end{array}
$$

Let $q ^ { * } \in \mathrm { a r g m a x } _ { q \in \Delta ( P _ { 0 } ) } \sum _ { t = 1 } ^ { T } \langle q ^ { P _ { 0 } , \pi } , r \rangle = q ^ { P _ { 0 } , \pi ^ { * } }$ denote the occupancy measure corresponding to the optimal policy $\pi ^ { * }$ under $P _ { 0 }$ , the regret can be defined as

$$
\begin{array} { l } { { \displaystyle R e g = \operatorname* { m a x } _ { \pi } R ( \pi , P _ { 0 } ) - R ( ( \pi _ { t } ) _ { t \in [ T ] } , P _ { 0 } ) } } \\ { { \displaystyle \ } } \\ { { \displaystyle \ = \sum _ { t = 1 } ^ { T } \langle q ^ { * } - q ^ { P _ { 0 } , \pi _ { t } } , r \rangle . } } \end{array}
$$

# 4 The RPO-AAS Algorithm

In this section, we introduce how we update the ambiguity set and calculate the robust optimal policy with respect to the ambiguity set in each episode. The algorithm initializes the policy $\pi$ to an arbitrary policy $\pi _ { 0 }$ (e.g., a uniform policy) and sets the number of samples $N _ { 1 } ( s , a )$ to zero for each state-action pair $( s , a )$ . In each episode, the following steps

1: Initialize: $\pi  \pi _ { 0 }$ , number of samples $N _ { 1 } ( s , a ) = 0$ for each $s \in \mathcal S$ , $a \in { \mathcal { A } }$   
2: for $t = 1 , \dots , T$ do   
3: for $l = 1 , \ldots , L$ do   
4: $s _ { t l } , a _ { t l } = \arg \operatorname* { m a x } _ { s \in \mathcal { S } _ { l } , a \in \mathcal { A } } N _ { t } ( s , a )$   
5: Update the set of candidate prototypes:   
6: $\begin{array} { r l } & { \dot { \mathcal { K } _ { l , t } } \quad = \quad \{ k \in \mathrm { ~ { \mathcal { K } _ { l , t - 1 } ~ } ~ } : \quad \forall \vert P ^ { k } ( s _ { t l } , a _ { t l } ) \vert \vert P ^ { k } ( s _ { t l } , a _ { t l } ) \vert } \\ & { \dot { P } _ { t } ( s _ { t l } , a _ { t l } ) \vert \vert _ { 1 } \le \sqrt { \frac { 4 \vert S _ { l + 1 } \vert \ln \frac { 3 L T } { \delta } } { N _ { t } ( s _ { t l } , a _ { t l } ) } } \} } \end{array}$   
7: end for   
8: Update ambiguity set: $\begin{array} { r } { \mathcal { U } _ { t } ^ { \bullet } = \bigotimes _ { s \in \mathcal { S } , a \in \mathcal { A } } \mathsf { \overline { { Q } } } _ { k \in \mathcal { K } _ { \mathcal { L } ( s ) , t } } P ^ { k } ( s , a ) } \end{array}$   
9: Calculate optimal robust policy: $\begin{array} { r } { \pi _ { t } = \arg \operatorname* { m a x } _ { \pi } \operatorname* { m i n } _ { P \in \mathcal { U } _ { t } } R ( \pi , P ) } \end{array}$   
10: Execute policy $\pi _ { t }$ for $L$ steps and obtain trajectory $s _ { l } , a _ { l }$ for $l = 1 , \ldots , L - 1$   
11: Update $N _ { t } ( s , a )$ and $\hat { P } _ { t } ( s , a )$ for all $s , a$   
12: end for

are performed: First, for each layer $l = 1 , \ldots , L$ , we identify the state-action pair $( s _ { t l } , a _ { t l } )$ with the maximum number of samples in that layer. Next, we update the set of prototypes $\mathcal { K } _ { l , t }$ by eliminating prototypes whose transition probabilities significantly deviate from the empirical transition distribution $\hat { P } _ { t } ( s , \bar { a } )$ for the state-action pair $( s _ { t l } , a _ { t l } )$ . This update is crucial, as it relies on the state-action pair with the most occurrences, ensuring faster convergence of the empirical distribution to the true distribution. Subsequently, we update the ambiguity set ${ { \mathcal { U } } _ { t } }$ as the Cartesian product of the ambiguity sets for each state-action pair, where each set comprises the transition probabilities of the remaining prototypes in the corresponding layer. We then calculate the robust optimal policy $\pi _ { t }$ by maximizing the worst-case value function over the ambiguity set ${ { \mathcal { U } } _ { t } }$ . Since our ambiguity set satisfies the (s,a)-rectangular property, the optimal policy can be calculated using backward induction. The backward induction and ambiguity set update step takes $\begin{array} { r } { O ( \vert S \vert \vert A \vert + \sum _ { l = 1 } ^ { L } \mathcal { K } _ { l } ) } \end{array}$ time, which is efficient. Moreover, the key advantage of this ambiguity set construction is its high probability of including the true transition kernel as in the following lemma.

# Lemma 1. For the ambiguity set updated as in Algorithm $\boldsymbol { { I } }$ , the true transition kernel lies in the ambiguity set ${ { \mathcal { U } } _ { t } }$ , i.e., $P _ { 0 } \in \mathcal { U } _ { t }$ for all $t \in [ T ]$ with probability at least $1 - \delta$ .

We would like to point out that, this robust setting by considering the ambiguity set and solving for the worst-case value function over it allows one to have a worst-case performance bound, as stated in Proposition 1. To be more specific, with the high-probability ambiguity set, we have that in each episode $t$ , policy $\pi _ { t }$ has the best worst-case performance and the performance of policy $\pi _ { t }$ is lower bounded by the optimal objective value of the robust MDP. As we will see later, the non-robust algorithm lacks this robustness and could have poor performance, especially when we don’t have enough data at the beginning.

Proposition 1. In episode $t$ , $\begin{array} { r l } { \operatorname* { m i n } _ { P \in \mathcal { U } _ { t } } R ( \pi _ { t } , P ) } & { { } \ge } \end{array}$ $\mathrm { m i n } _ { P \in \mathcal { U } _ { t } } R ( \pi , P )$ for all policy $\pi$ . Moreover, with probability at least $1 - \delta$ , $\mathrm { m i n } _ { P \in \mathcal { U } _ { t } } R ( \pi _ { t } , P )$ provides a lower bound for $R ( \pi _ { t } , P _ { 0 } )$ with probability at least $1 - \delta$ .

The proof uses Hoeffding’s inequality to bound the difference between the true and empirical transition probabilities. Due to space limitations, proofs for all results in this paper are provided in the appendix. This proposition implies that in each episode $t$ , $\pi _ { t }$ has the best worst-case performance, and its actual performance is lower-bounded by the optimal objective value of the robust MDP. In contrast, a non-robust algorithm lacks this guarantee and may perform poorly, especially with limited data at the beginning. While the robust policy has its own advantages, the question remains whether this robust policy has a good performance under the true transition kernel $P _ { 0 }$ . In the following section, we prove the theoretical guarantee of the RPO-AAS algorithm under $P _ { 0 }$ .

# 5 Theoretical Results

In this section, we first establish the regret bound, and then show the finite sample guarantee and the convergence result.

# 5.1 Analysis of Regret

To bound the regret, we begin by decomposing (1) as follows:

$$
R e g = \sum _ { t = 1 } ^ { T } \langle \boldsymbol { q } ^ { * } - \boldsymbol { q } _ { t } , \boldsymbol { r } \rangle = \sum _ { t = 1 } ^ { T } \langle \boldsymbol { q } ^ { * } - \hat { \boldsymbol { q } } _ { t } , \boldsymbol { r } \rangle + \langle \hat { \boldsymbol { q } } _ { t } - \boldsymbol { q } _ { t } , \boldsymbol { r } \rangle ,
$$

where $q _ { t } = q ^ { P _ { 0 } , \pi _ { t } }$ , $\hat { q } _ { t } = q ^ { P _ { t } , \pi _ { t } }$ and $\pi _ { t } , P _ { t }$ is the optimal solution of the robust optimization problem $\begin{array} { r } { \operatorname* { m a x } _ { \pi } \operatorname* { m i n } _ { P \in \mathcal { U } _ { t } } } \end{array}$

$R ( \pi , P )$ . The high-level idea of our proof of regret has three main steps. First, we upper bound the regret by the total reward difference between the true transition kernel $P _ { 0 }$ and the kernel given by $P _ { t }$ under the optimal policy $\pi ^ { * }$ (Lemma 2). We then bound this reward difference in two steps. We first establish a bound on the one-norm difference between $P _ { 0 }$ and $P _ { t }$ (Lemma 3), and then bound the difference of total reward (Lemma 5). We begin with Lemma 2, which provides an upper bound on the regret in terms of the total reward difference between the true transition kernel $P _ { 0 }$ and the kernel given by robust optimization $P _ { t }$ under the optimal policy $\pi ^ { * }$ .

Lemma 2. With probability at least 1 δ, $\textstyle \sum _ { t = 1 } ^ { T } \langle q ^ { * } - { \hat { q } } _ { t } , r \rangle +$   
⟨qˆt − qt, r⟩ ≤ PtT=1 ∥qtP0,π∗ Pt,π qt

Here, $\begin{array} { r } { \Vert q ^ { P _ { t } , \pi ^ { * } } - q ^ { P _ { 0 } , \pi ^ { * } } \Vert _ { 1 } = \sum _ { s , a , s ^ { \prime } } \vert q ^ { P _ { t } , \pi ^ { * } } ( s , a , s ^ { \prime } ) - } \end{array}$ $q ^ { P _ { 0 } , \pi ^ { * } } ( s , a , s ^ { \prime } ) |$ . So it remains  o bound $\begin{array} { r } { \sum _ { t = 1 } ^ { T } \Vert q _ { t } ^ { P _ { 0 } , \pi ^ { * } } - } \end{array}$ qtPt,π∗∥1. Based on the result from (Rosenberg and Mansour 2019), we bound $\begin{array} { r } { \sum _ { t = 1 } ^ { T } \| q ^ { P _ { 0 } , \pi ^ { * } } - q ^ { P _ { t } , \pi ^ { * } } \| _ { 1 } } \end{array}$ as follows.

Lemma 3. For any policy $\pi$ and any $P _ { t } \in \mathcal { U } _ { t }$ , with probability at least $1 - \delta$ , the following holds:

$$
\begin{array} { r l } {  { \sum _ { t = 1 } ^ { T } \| q _ { t } ^ { P _ { 0 } , \pi } - q _ { t } ^ { P _ { t } , \pi } \| _ { 1 } } \quad } & { } \\ & { \leq 2 \sum _ { t = 1 } ^ { T } \displaystyle \sum _ { l = 1 } ^ { L } \sum _ { m = 0 } ^ { l - 1 } \sum _ { s _ { m } \in S _ { m } } \sum _ { a _ { m } \in \cal A } q ^ { P _ { 0 } , \pi } ( s _ { m } , a _ { m } ) \xi _ { t } ( s _ { m } , a _ { m } ) , } \end{array}
$$

where $\xi _ { t } ( s , a ) = \| P _ { t } ( \cdot | s , a ) , P _ { 0 } ( \cdot | s , a ) \| _ { 1 }$

Thus, to bound the right-hand side in the lemma above, the key is to bound $\xi _ { t } { \big ( } s , a { \big ) }$ .

Lemma 4. Suppose $P _ { 0 } \in \mathcal { U } _ { t }$ . Then for any $s \in { \mathcal { S } } , a \in { \mathcal { A } } ,$ , $t \in [ T ]$ , and for all $k \in K _ { t , \mathcal { L } ( s ) }$ , we have:

$$
\| P _ { 0 } ( s , a ) , P ^ { k } ( s , a ) \| _ { 1 } \leq \sqrt { \frac { 4 | \mathcal { S } _ { \mathcal { L } ( s ) + 1 } | | \mathcal { A } | \ln \frac { 3 L T } { \delta } } { t } }
$$

With the established bound for $\xi _ { t }$ , we prove the following bound for the right-hand side of Lemma 3.

Lemma 5. With probability at least $1 - \delta$ , the following holds:

$$
\begin{array} { r l r } {  { \sum _ { t = 1 } ^ { T } \sum _ { l = 1 } ^ { L } \sum _ { m = 0 } ^ { l - 1 } \sum _ { s _ { m } \in  { \mathcal S } _ { m } } \sum _ { a _ { m } \in A } q ^ { P _ { 0 } , \pi } ( s _ { m } , a _ { m } ) \xi _ { t } ( s _ { m } , a _ { m } ) } } \\ & { } & { \leq L ^ { 2 } \gamma \sqrt { 4 T |  { \mathcal S } | | A | \ln \frac { 3 L T } { \delta } } . } \end{array}
$$

By combining Lemma 2, Lemma 3 and 5, we have the following regret bound:

Theorem 1. With probability at least $1 - \delta$ , the RPO-AAS algorithm has the following regret bound:

$$
R e g \leq L ^ { 2 } \gamma \sqrt { 4 T | S | | A | \ln \frac { 3 L T } { \delta } } .
$$

It’s worth noting that the state-of-the-art algorithm for general online MDPs achieves a regret bound of $\tilde { O } ( \sqrt { H | S | | A | T } + H ^ { 2 } S ^ { 2 } | A | + H \sqrt { T } )$ , where $H$ is the number of horizons (Azar, Osband, and Munos 2017). Our regret bound maintains the same dependence on $| { \cal S } | , | { \cal A } |$ , and $T$ . This demonstrates that, given structural information, our robust algorithm matches the efficiency of non-robust stateof-the-art approaches. However, it’s important to note that designing efficient robust RL algorithms without structural information remains an open problem in the field.

# 5.2 Finite-Sample Guarantee and Convergence

In addition to the regret bound, we establish that the policy obtained by the proposed algorithm has a finite-sample performance guarantee and converges to the optimal policy.

Theorem 2 (Finite-sample guarantee). Let $v ^ { \pi } ( s _ { 0 } )$ denote the value function at state $s _ { 0 }$ under policy $\pi$ under the true transition kernel. For any $\epsilon \ > \ 0$ , when $\textit { t } \geq$ 4L4γ2|S|ϵ|2A| ln 3LδT , with probability at least 1 − δ, vπ∗ (s0) − $v ^ { \pi _ { t } } ( s _ { 0 } ) \stackrel { - } { \leq } \epsilon$ .

This theorem states that after a sufficient number of episodes $t$ , the value function of our algorithm’s policy $\pi _ { t }$ at the initial state $s _ { 0 }$ is within $\epsilon$ of the optimal policy $\pi ^ { * }$ ’s value function, with high probability. The required number of episodes is inversely proportional to $\epsilon ^ { 2 }$ . This dependency on $\epsilon ^ { 2 }$ is typical in many MDP problems (Panaganti and Kalathil 2022). We next show that our algorithm can actually identify the true prototype after a finite number of episodes, leading to the optimal policy.

Theorem 3 (Prototype Ambiguity Set Convergence). Let $\begin{array} { r } { h = \operatorname* { m i n } _ { s \in S , a \in \mathcal { A } , k \in [ K ] } \| P ^ { k } ( \cdot | \overline { { s } } , a ) , P _ { 0 } ( \cdot | s , a ) \| _ { 1 } } \end{array}$ , then when $\begin{array} { r } { t \geq \frac { 8 | \mathcal { S } | ^ { 2 } | \mathcal { A } | \ln \frac { 3 L T } { \delta } } { h } } \end{array}$ , the candidate set of prototypes only include the true prototypes, i.e., $K _ { t l } = \{ k _ { l } ^ { * } \}$ , thus $\pi _ { t } = \pi ^ { * }$ .

This theorem establishes a finite-time guarantee for our algorithm’s convergence to the true prototype and, consequently, the optimal policy. The result provides a principled stopping criterion, potentially improving the algorithm’s practical efficiency.

# 6 Extend to Non-robust Algorithm: Selecting the Best Candidate

We propose another algorithm that selects the transition kernel that is nearest to the empirical distribution in each episode, referred to as non-robust policy optimization with nearest prototype-candidate(NRPO-NPC). Then in each episode, we run the optimal policy corresponding for the chosen transition kernel. We demonstrate that this approach provides the same theoretical performance guarantees for regret, convergence, and finite sample guarantees as the robust algorithm. However, it lacks the robustness guarantee. To establish the theoretical results, we first decompose the regret at each episode as follows:

$$
\begin{array} { r } { \sum _ { t = 1 } ^ { T } \langle q ^ { * } - q _ { t } , r \rangle = ( q ^ { \pi ^ { * } , P _ { 0 } } - q ^ { \pi ^ { * } , P _ { t } } ) + ( q ^ { \pi ^ { * } , P _ { t } } - q ^ { \pi _ { t } , P _ { t } } ) } \\ { + ( q ^ { \pi _ { t } , P _ { t } } - q ^ { \pi _ { t } , P _ { 0 } } ) \quad \quad \quad \quad ( 4 } \end{array}
$$

The second term, $q ^ { { \pi ^ { * } } , P _ { t } } - q ^ { { \pi _ { t } } , P _ { t } } \leq 0$ , since $\pi _ { t }$ is the optimal policy for transition kernel $P _ { t }$ . Similar to the proof for Theorem 1, we can bound the first term and the third term as long as we can bound the distance between $P _ { 0 }$ and $P _ { t }$ , which is shown in the following lemma.

Lemma 6. For each layer $l ,$ , let $\begin{array} { r l r l } { s _ { t l } , a _ { t l } } & { { } } & { = } & { } \end{array}$ arg $\mathrm { m a x } _ { s \in \mathcal { S } _ { l } , a \in \mathcal { A } } N _ { t } ( s , a )$ denote the $( s , a )$ pair with the maximum number of samples in the layer. Let $\begin{array} { r } { k _ { t } = \mathrm { a r g m i n } _ { k \in K _ { l , t } } \Vert P ^ { k } ( s _ { t l } , a _ { t l } ) - \hat { P } _ { t } ( s _ { t l } , a _ { t l } ) \Vert _ { 1 } } \end{array}$ . Then for any $s \in \mathcal S$ , $a \in { \mathcal { A } }$ , $t \in [ T ]$ , we have:

$$
\| P _ { 0 } ( \cdot | s , a ) , P ^ { k _ { t } } ( \cdot | s , a ) \| _ { 1 } \leq \sqrt { \frac { 4 | S _ { \mathcal { L } ( s ) + 1 } | | A | \ln \frac { 3 L T } { \delta } } { t } }
$$

# 7 Numerical Experiments

In the numerical experiments, we compare the performance of our proposed robust algorithm with the UCBVI algorithm (Azar, Osband, and Munos 2017), and the two benchmark algorithms we propose that considers the prototype information. We will provide more details later.

We consider a GridWorld experiment of size $5 \times 4$ , which is a widely used reinforcement setting from (Sutton and Barto 1998). In each episode, the learner starts from the lower left corner and aims to the upper right corner. Let $( x _ { 1 } , x _ { 2 } )$ denote the coordinate, where $x _ { 1 } \ \in \ \{ 0 , 1 , 2 , 3 , 4 \}$ is the coordinate of the horizontal axis and $x _ { 2 } \in \{ 0 , 1 , 2 , 3 \}$ is the vertical axis coordinate. The learner collects rewards at some states, which we call reward states. We set the reward states to be $( 2 , 2 )$ , $( 1 , 1 )$ and $( 1 , 2 )$ and the rewards are 3, 5 and 1, respectively. At each state $s$ and $a$ , the learner can either move up $( a = 0$ ) or right $( a = 1 )$ ), with a success probability $z ( s , a )$ , and the learner goes to the opposite direction with probability $1 - z ( s , a )$ . $z ( s , a )$ is unknown. The learner’s goal is to maximize the total collected rewards. If a learner reaches a boundary, she can only move inward. This problem is an episodic loop-free MDP, where each episode consists of $L = 8$ layers. The number of states is $\left| S \right| = 2 0$ and the number of actions is $| { \mathcal { A } } | = 2$ .

Prototype configuration. In each instance, we generate $K$ prototypes. We set $K = 4$ and $K = 1 0$ , representing scenarios with few and many prototypes, respectively. For each prototype, we generate random $z _ { k } ( s , a )$ from a uniform distribution between 0 and 1. For simplicity, we generate different success probabilities only for different states, meaning $z _ { k } ( s , 0 )$ remains the same for all states, as does $z _ { k } ( s , 1 )$ . We consider two types of prototype sets: The first set of prototype satisfies our assumption on the structure of transition prototypes (Assumption 1). Specifically, for any $s$ and $a$ , we let $\bar { | z _ { k _ { 1 } } ( s , a ) - \bar { z } _ { k _ { 2 } } ( s , a ) | }$ be fixed for any kernel $k _ { 1 }$ and $k _ { 2 }$ . We call this setting fixed-gap prototypes. The second set does not satisfy this assumption. In this setting, we generate $z _ { k } ( s , 0 )$ and $z _ { k } ( s , 1 )$ for all prototypes randomly.

Algorithms. We compare four algorithms: (1) our robust algorithm (RPO-AAS), (2) UCBVI algorithm, (3) the nonrobust nearest prototype-candidate algorithm (NRPO-NPC), and (4) its variant, NRPO-NPC-2. The latter is a heuristic that selects the prototype with the smallest 1-norm distance to the empirical transition probabilities across all states and actions in the layer. Details are provided in the Appendix.

Experiment Environment. We conduct the numerical experiment using rlberry, a Python library for reinforcement learning (Domingues et al. 2021). For each setting, we run 100 simulations. In each simulation, we record the average expected rewards in each episode. We then take the average of these simulations. The expected episode reward is the expectation of the total reward under the policy in episode $t$ .

# 7.1 Structured Prototypes Setting

In the fixed-gap setting with $K = 4$ , Figure 1 shows that NRPO-NPC, NRPO-NPC-2, and RO perform significantly faster than the UCBVI algorithm. This indicates that our proposed algorithms can leverage the prototype information effectively, resulting in better performance. NRPO-NPC-2 converges to the optimal policy fastest, although it lacks theoretical guarantees. When $K = 1 0$ , the performance of NRPO-NPC and RO surpasses NRPO-NPC-2 and UCBVI (Figure 2). Notably, in both cases, RO has better performance at the beginning, showcasing the advantage of considering robustness.

# 7.2 Random Prototypes Setting

We start from $K = 4$ prototypes. Figure 3 shows the performance of the algorithms. In this setting, NRPO-NPC2 couldn’t converge to the optimal policy. RO yields bet

非 5.60√ rvvw Mwm5.40 W mw\~> wW  
5.205.00 UCBVI4.80 NRPO-NPC4.60 RPO-NPC-2  
3.750 20 40 60 80100 RPO-AAS  
3.25  
0 1000 2000 3000 4000 50006000 7000 8000Number of Episodes

![](images/7e351ca8f89b2119723ee3486b128c701b4d9bc5849d5fc663663859f274b05d.jpg)  
Figure 1: Average Expected Episode Rewards of different algorithms with Fixed-gap Prototypes when $K = 4$ .

ter policies than UCBVI and NRPO-NPC in the first 2,000 episodes. Moreover, the policy given by RO has lower fluctuations than NRPO-NPC and UCBVI. NRPO-NPC outperforms UCBVI initially but shows greater variance and converges to the optimal solution more slowly than UCBVI. When we increase the number of prototypes to 10, NRPO-NPC, NRPO-NPC-2, and RO continue to outperform UCBVI during the first 400 episodes. RO maintains the lowest variance, indicating that it yields the most stable policy. However, UCBVI converges to the optimal policy more rapidly than RO and NRPO-NPC in many cases, resulting in slightly superior performance after 2,000 episodes.

This observation suggests that as the number of prototypes increases, the benefits of incorporating prototype information diminish. This is logical, as in the limit of infinite prototypes, the algorithm would gain no advantage from prototype information. From Theorem 3, more prototypes potentially reduce $h$ and thus slow convergence, while too few prototypes may fail to include the transition kernel. Therefore, the number of prototypes $K$ presents a practical trade-off. Nevertheless, the RO algorithm maintains its robustness even in this many-prototype setting.

![](images/7817b4d2891e1bb3ef87eab8759d8a2a4fd5b9a50415a9ec71c199fbed559375.jpg)  
Figure 3: Average Expected Episode Rewards of different algorithms with 4 Random Prototypes.

![](images/658d19e9b32a57538c52af30efd523da0ec7487942d9ba1ac82403bf30770a6d.jpg)  
Figure 2: Average Expected Episode Rewards of different algorithms with Fixed-gap Prototypes when $K = 1 0$ .   
Figure 4: Average Expected Episode Rewards of different algorithms with $1 0 \mathrm { R }$ andom Prototypes.

# 8 Conclusion

In this work, we introduced a novel approach for online MDPs with transition prototypes. Our robust adaptive algorithm efficiently identifies the true transition kernel while guaranteeing performance through robust policies. Theoretical analysis shows the algorithm achieves sublinear regret, provides finite-sample guarantees, and converges to the optimal policy in finite time. Numerical experiments demonstrate its practical advantages, particularly in early learning stages and with structured prototypes. We also extended our analysis to a non-robust algorithm, highlighting the value of prototype information. This work shows the potential of the combination of structural information and robust optimization in reinforcement learning. Future work could explore extensions to more complex MDP settings and investigate robustness-optimality trade-offs in various applications.