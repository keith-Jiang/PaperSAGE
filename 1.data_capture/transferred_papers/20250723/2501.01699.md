# Robust Self-Paced Hashing for Cross-Modal Retrieval with Noisy Labels

Ruitao Pu 1, Yuan Sun 1\*, Yang Qin 1, Zhenwen Ren 2, Xiaomin Song 3, Huiming Zheng 3, Dezhong Peng 1,3

1Sichuan University, Chengdu, China, 610044, 2Southwest University of Science and Technology, Mianyang, China, 621010, 3Sichuan National Innovation New Vision UHD Video Technology Co., Ltd., Chengdu, China, 610095, ruitaopu@gmail.com, sunyuan work $@$ 163.com, qinyang.gm@gmail.com, rzw@njust.edu.cn, songxiaomin@uptcsc.com, michaelzheng $@$ uptcsc.com, pengdz@scu.edu.cn.

# Abstract

Cross-modal hashing (CMH) has appeared as a popular technique for cross-modal retrieval due to its low storage cost and high computational efficiency in large-scale data. Most existing methods implicitly assume that multi-modal data is correctly labeled, which is expensive and even unattainable due to the inevitable imperfect annotations (i.e., noisy labels) in real-world scenarios. Inspired by human cognitive learning, a few methods introduce self-paced learning (SPL) to gradually train the model from easy to hard samples, which is often used to mitigate the effects of feature noise or outliers. It is a less-touched problem that how to utilize SPL to alleviate the misleading of noisy labels on the hash model. To tackle this problem, we propose a new cognitive cross-modal retrieval method called Robust Self-paced Hashing with Noisy Labels (RSHNL), which can mimic the human cognitive process to identify the noise while embracing robustness against noisy labels. Specifically, we first propose a contrastive hashing learning (CHL) scheme to improve multi-modal consistency, thereby reducing the inherent semantic gap. Afterward, we propose center aggregation learning (CAL) to mitigate the intra-class variations. Finally, we propose Noise-tolerance Self-paced Hashing (NSH) that dynamically estimates the learning difficulty for each instance and distinguishes noisy labels through the difficulty level. For all estimated clean pairs, we further adopt a self-paced regularizer to gradually learn hash codes from easy to hard. Extensive experiments demonstrate that the proposed RSHNL performs remarkably well over the state-of-the-art CMH methods.

# Code — https://github.com/perquisite/RSHNL

# Introduction

With the explosive growth of multi-modal data, cross-modal retrieval (CMR) has attracted a wide range of attention in the community (Zhou, Hassan, and Hoon 2023), which retrieves relevant samples across different modalities. For large-scale multi-modal data, cross-modal hashing (CMH) (Zhu et al. 2023) offers an efficient solution due to its low storage cost and high retrieval efficiency. The basic idea of CMH is to learn discriminative hash codes to alleviate the heterogeneity gap between different modalities. Due to the complexity of collecting annotations, some unsupervised CMR methods (Zhang et al. 2023b; Li et al. 2024b; Qin et al. 2023) have been proposed to eliminate the reliance on abundant labels. However, their performance often suffers without supervised semantic guidance. Recently, numerous supervised CMH methods (Li et al. 2024a; Chen, Cao, and Liu 2021) have been proposed and achieved pleasing performance. Most of them implicitly assume that all collected labels are correctly labeled, which is unrealistic due to inevitable noisy labels from manual or non-expert annotations (Song et al. 2022a; Kuznetsova et al. 2020). These noisy labels can mislead hash models, significantly degrading retrieval performance. Besides, learning from multi-modal instances with noisy labels is difficult due to a great heterogeneity gap. Thus, it is a challenging problem to mitigate the performance degradation caused by noisy labels for cross-modal retrieval.

To alleviate the impact of noisy labels, many CMR methods (Xu et al. 2022; Wang et al. 2024, 2021) have been developed. For example, ELRCMR utilizes dynamic weights to prevent overfitting noisy labels. However, most of them rely on real-valued representation, leading to high storage and computational costs. Although hash representations are more lightweight, unreliable labels could expand quantization errors. To this end, a few CMH methods have been proposed. For instance, NrDCMH adopts the difference between label similarity and feature similarity to detect noise. Although remarkable progress, most of them implicitly assume constant learning priorities for each instance, making the model biased toward hard instances with noisy labels and leading to the overfitting problem. Inspired by human cognitive learning, self-paced learning (SPL) was presented to explore more valuable discriminative information contained in hard instances gradually. In other words, SPL can gradually train the model from easy to hard instances to improve generalization. However, SPL is usually used for feature noise or outliers. It is a less-touched problem to employ the SPL paradigm to mitigate the negative effects of noisy labels.

In this paper, we propose a new cognitive cross-modal retrieval method, termed Robust Self-paced Hashing with Noisy Labels (RSHNL), which could enable the model to learn with noisy labels. As shown in Fig.1, our RSHNL mimics the human cognitive process to learn each instance in Hamming space from easy to hard, thereby embracing the robustness of eliminating noisy labels. Compared with existing self-paced hashing methods which only consider feature noise/outliers and cannot deal with noisy labels, our RSHNL assesses the learning pace by identifying noisy labels and learning clean pairs from easy to hard in the form of cognitive learning. Specifically, to reduce the inherent semantic gap, we first present a contrastive hashing learning (CHL) scheme to maximize the consistency between multi-modal data. Then, we propose center aggregation learning (CAL) to learn unified hash centers to aggregate hash codes from the same category, thereby mitigating the intra-class variations from multi-modal inputs. Further, we propose Noisetolerance Self-paced Hashing (NSH) to adaptively measure the learning hardness of each instance and distinguish noisy labels according to the corresponding difficulty. For all sample pairs with clean labels, we adopt a self-paced regularizer that begins with easier pairs and advances to more complex ones. The main contributions are summarized as follows:

![](images/4e0d25aa3c86e755ceabacb0fdb693f771bd71a2a9600b9d40fad0a355ce0ca0.jpg)  
Figure 1: The framework of our RSHNL. Blue and red represent hash codes of different modalities. Triangles and rectangles represent different categories. And the doji represents hash centers. Specifically, CHL maximizes the consistency of multimodal data to alleviate the cross-modal gap. CAL develops a unified hash code for each class as a center and encourages the compactness of intra-class hash codes towards their corresponding hash centers. NSH dynamically distinguishes noisy labels based on their assessment difficulty while facilitating learning hash codes from easy to hard for clean pairs.

• We propose a new cognitive cross-modal hashing paradigm that alleviates the negative effect of noisy labels. To the best of our knowledge, this could be the first work that introduces SPL to distinguish and eliminate noisy labels in CMH tasks.   
• We propose a Noise-tolerance Self-paced Hashing (NSH) loss that automatically determines noise labels and builds a full learning sequence for clean sample pairs, thereby evolving from easy to hard until all clean pairs are incorporated for training.   
• Extensive experiments comprehensively verify that our proposed RSHNL has remarkably superior performance over the current state-of-the-art methods in different noise rates.

# Related Work

# Cross-modal Hashing

Recently, many CMH methods have been proposed, which can be roughly divided into two categories, i.e., unsupervised and supervised. Unsupervised CMH methods (Cao et al. 2023) aim to utilize the original distribution of the data to learn a common Hamming space. For example, DSAH (Yang et al. 2020) integrates co-occurrence information and semantic relevance of different modalities to guide hashing learning. For supervised CMH methods (Liu et al. 2024; Sun et al. 2024b), their goal is to leverage annotation information to learn more compact and discriminative hash codes. For instance, DCMH (Jiang and Li 2017) maintains the semantic relevance of hash representations through the label similarity matrix. However, almost all of these methods implicitly assume multi-modal data are well labeled. In practical applications, labeling noise is ubiquitous, which could mislead the hash model to overfit the noise. To tackle this problem, some supervised methods are developed to learn to hash from noisy labels robustly. For example, to utilize the memorization effect of DNNs and reduce the impact of noisy labels, CMMQ (Yang et al. 2022) selects confidence samples with smaller loss values. To deal with the problem of lowquality label annotations, DHRL (Shu et al. 2024) constructs a ranking and swapping module to estimate the uncertainty from noisy labels. Although these methods achieve promising performance, they unconsciously ignore the adverse impact of sample pairs with noisy labels and keep the learning priority of each instance the same, which violates the human cognitive process.

# Self-paced Learning

Inspired by human cognitive learning, self-paced learning (SPL) (Kumar, Packer, and Koller 2010; Jiang et al. 2014a) is proposed to train the model from easy samples to hard ones. For example, to mitigate the noise/outlier problem, SCSM (Liang et al. 2016) utilizes SPL to learn samples from easy to hard. However, this will introduce several hyperparameters due to manually setting the weighting function, which is undesirable. Thus, Meta-SPN (Wei et al. 2021) is proposed to learn the weight values automatically. However, most of these methods only focus on the contributions of instances to learn hash codes from easy to hard, thereby resisting feature noise interference. To this end, DSCMH (Sun et al. 2024a) proposes a novel dual SPL mechanism from the perspectives of instance-level and feature-level difficulty to enhance robustness. In contrast, DHaPH (Huo et al. 2024) pays more attention to difficult sample pairs, and assigns larger weights to the hard pairs to learn discriminative information. Although these SPL methods have achieved considerable performance, almost all of them only consider the noise or outliers in the data. When faced with multi-modal data with noisy labels, how to utilize SPL to alleviate the noise overfitting problem is rarely studied. In this paper, we expect to distinguish sample pairs with noisy labels and gradually explore discriminative semantic information from clean data for the hash model, thereby alleviating the negative effect of noisy labels.

# The Proposed Method Problem Formulation

In this paper, some notations are provided for a clear presentation. We first denote ${ D _ { m } } = \{ ( x _ { i } ^ { m } , y _ { i } ) \} _ { i = 1 } ^ { N }$ as multi-modal data with $N$ instances from $M$ modalities, where $x _ { i } ^ { m }$ represents $i$ -th sample of the $m$ -th modality, $Y \in \mathbb { R } ^ { N \times } { \mathrm { \bar { K } } }$ is the corresponding labels, and $K$ is number of the categories. For a instance $x _ { i } ^ { m }$ , if it belongs to $k$ -th category, the $k$ -th element of $y _ { i }$ is 1, i.e. $y _ { i , k } = 1$ , otherwise $y _ { i , k } = 0$ .

The basic idea of cross-modal hashing is to project multimodal data into a common Hamming space by adopting different hash functions. Let hash code of $i$ -th sample from $m$ -th modality is $b _ { i } ^ { m } \in \{ - 1 , 1 \} ^ { L }$ , where $L$ represents hash length. And each hash function can be denoted as $\mathcal { H } ^ { m } ( \cdot , \Theta ^ { \bar { m } } )$ , where $\Theta ^ { m }$ is the corresponding learnable parameters. Subsequently, we can apply the sign function to obtain hash representation, i.e.,

$$
b _ { i } ^ { m } = s i g n ( \mathcal { H } ^ { m } ( \boldsymbol x _ { i } ^ { m } , \Theta ^ { m } ) ) .
$$

Since binary optimization is a typical NP-hard problem (Huo et al. 2024), we adopt a tanh function to learn binarylike codes during training.

# Contrastive Hashing Learning

To narrow the inherent semantic gap between multi-modal data, we present a contrastive hashing learning scheme (CHL) that maximizes the consistency of hash codes from different modalities to improve the discrimination. Specifically, we treat the same instances from different modalities as positive pairs and then encourage them to be close while keeping negative pairs far away. First, we define the probability that $x _ { i } ^ { m }$ belongs to the $i$ -th instance as

$$
q ( i | x _ { i } ^ { m } ) = \frac { \sum _ { j = 1 } ^ { M } e ^ { b _ { i } ^ { m } ( b _ { i } ^ { j } ) ^ { \top } / \mathrm { \Delta } \tau } } { \sum _ { j = 1 } ^ { M } \sum _ { z = 1 } ^ { N } e ^ { b _ { i } ^ { m } ( b _ { z } ^ { j } ) ^ { \top } / \mathrm { \Delta } \tau } } ,
$$

where $\tau$ is a temperature parameter. Then, the CHL loss $\mathcal { L } _ { C }$ could be written as maximizing a joint probability $\begin{array} { r } { \prod _ { i = 1 } ^ { N } \prod _ { m = 1 } ^ { M } p ( i | x _ { i } ^ { m } ) } \end{array}$ of all samples, which is equivalent to minimizing the following formula, i.e.,

$$
\begin{array} { l } { \displaystyle \mathcal { L } _ { C } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { m = 1 } ^ { M } ( 1 - r ) \frac { 1 - ( q ( i | x _ { i } ^ { m } ) ) ^ { r } } { r } } \\ { \displaystyle \quad + r ( 1 - q ( i | x _ { i } ^ { m } ) ) . } \end{array}
$$

By minimizing Eq.3, positive pairs are forced to be compressed, while negative pairs are scattered in the Hamming space, thereby alleviating multi-modal discrepancy.

# Noise-tolerance Self-paced Hashing

To mitigate the intra-class variations of multi-modal data, we propose center aggregation learning (CAL) to learn a unified hash representation for each class as a center and promote modality-specific hash codes with the same category to be aggregated to the corresponding hash centers. Specifically, we randomly initialize and obtain hash centers $\bar { C } = \{ c _ { 1 } , \dot { c } _ { 2 } , . . . , c _ { K } \}$ , where $c _ { K } \in \mathbb { R } ^ { L \times 1 }$ is the normalized and discretized binary vector for the $K$ -th class. For any sample $x _ { i } ^ { m }$ , we first define its probability belonging to the $k$ -th center as

$$
p ( k | x _ { i } ^ { m } ) = \frac { e ^ { b _ { i } ^ { m } c _ { k } / \tau } } { \sum _ { j = 1 } ^ { K } e ^ { b _ { i } ^ { m } c _ { j } / \tau } } ,
$$

where $\tau$ is a temperature parameter. Due to the lack of guidance information, the obtained probabilities could lead to prediction errors. Thus, to make the hash code inherit more semantic information, we define the semantic aggregation probability as

$$
v _ { i } ^ { m } = \sum _ { k = 1 } ^ { K } y _ { i , k } p ( k | x _ { i } ^ { m } ) .
$$

Afterward, we can obtain the following center aggregation loss ${ \mathcal { L } } _ { p }$ , i.e.,

$$
\mathcal { L } _ { p } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { m = 1 } ^ { M } ( 1 - r ) \frac { 1 - ( v _ { i } ^ { m } ) ^ { r } } { r } + r ( 1 - v _ { i } ^ { m } ) ,
$$

where $r \in ( 0 , 1 ]$ is a weight factor. However, due to the ubiquitous noisy labels in the data, such center aggregation loss is inevitably disrupted, thereby tending to overfit the corrupted labels. Since DNNs (Song et al. 2022b) are robust in the early training stage, we use Eq.6 to warm up the model.

Inspired by the great success of self-paced learning (SPL), we can organize the learning sequence of samples from easy to hard, thereby improving the retrieval performance. Thus, some SPL-based hashing methods (Sun et al. 2024a,c) have been proposed to mitigate the negative effects of noise or outliers. However, all of them implicitly assume that the multi-modal data are labeled correctly while ignoring the existence of noisy labels. Moreover, they keep the learning priority of each sample with noisy labels constant, which could be unreasonable due to the labeled differences. To overcome this issue, we propose a Noise-tolerance Self-paced Hashing (NSH) strategy to learn hash codes from noisy labels. Similar to prior SPL methods, our proposed NSH gradually learns from easy pairs to difficult ones, thereby automatically incorporating more data into the training process. Different from them, we reveal that the SPL scheme can distinguish sample pairs with noisy labels to mitigate the overfitting problem. Specifically, our NSH adopts a hardness measurement strategy that dynamically estimates the learning difficulty of each pair and distinguishes the noisy labels. Then, NSH gradually learns hash codes from easy to hard until it is sufficient to handle hard ones. Thereupon, the problem could be formulated as follows

$$
\begin{array} { r l } & { \mathcal { L } _ { S } = \displaystyle \frac { 1 } { N } \sum _ { i = 1 } ^ { N } w _ { i } \underbrace { \sum _ { m = 1 } ^ { M } ( 1 - r ) \frac { 1 - \left( v _ { i } ^ { m } \right) ^ { r } } { r } + r ( 1 - v _ { i } ^ { m } ) } _ { \ell _ { i } } } \\ & { \quad \quad + \displaystyle \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \mathcal { R } ( w _ { i } , \gamma ) , } \end{array}
$$

where $w _ { i } ~ \in ~ [ 0 , 1 ]$ is the importance weight of $i$ -th sample pairs that evaluates the reliability of label dimension. $\mathcal { R } ( w _ { i } , \gamma )$ is the self-paced regularizer controlled by the learning pace parameter $\gamma$ , which could assign a weight $w _ { i }$ to estimate the learning difficulty of each instance. We adopt a linear interpolation function (Jiang et al. 2014b) to construct the following self-paced regularizer $\mathcal { R } ( w _ { i } , \gamma )$ , i.e.,

$$
\mathcal { R } ( w _ { i } , \gamma ) = \gamma ( \frac { 1 } { 2 } { w _ { i } } ^ { 2 } - w _ { i } ) .
$$

In brief, the weight can be regarded as the easiness of each sample pair with noisy labels. If the weight is higher, the instance could be viewed as easier. The loss decreases gradually with the learning process, thus enlarging the weight. When $\ell _ { i } > \gamma$ , we consider this sample pair could be mislabeled and assign the weight as zero to represent the hardest/noisy sample pair. When $\ell _ { i } \ \leq \ \gamma$ , NSH first considers reliable/easy pairs at the beginning and then gradually incorporates unreliable/hard ones into training. In other words, we learn hash codes in the way of human cognitive learning (i.e., from easy to hard) until more clean pairs are incorporated into the hash model.

# The Objective Function

By combining the above losses, we can obtain the overall objective loss function as follows

$$
\begin{array} { r } { \mathcal { L } = \left\{ \begin{array} { l l } { \mathcal { L } _ { p } + \alpha \mathcal { L } _ { C } , } & { \mathrm { i f ~ } t < N _ { w } , } \\ { \mathcal { L } _ { S } + \alpha \mathcal { L } _ { C } } & { \mathrm { i f ~ } N _ { w } \leq t < N _ { m } . } \end{array} \right. } \end{array}
$$

where $\alpha$ is a hyper-parameter, $t$ is the current epoch, $N _ { w }$ and $N _ { m }$ are warm-up epoch and maximal epoch, respectively. The training process of RSHNL is shown in the appendix.

# Theoretical Justification

To show the robustness of the proposed RSHNL, we deeply analyze the impact of the weight $w _ { i }$ on the loss $\mathcal { L } _ { S }$ . Our goal is to minimize $\mathcal { L } _ { S }$ by updating the weight $w _ { i }$ and network parameters $\{ \Theta ^ { m } \} _ { m = 1 } ^ { M }$ alternatively, while making the other fixed. Given a fixed {Θm}mM= 1, we can obtain the following optimal solution, i.e.,

$$
\begin{array} { c } { { w _ { i } ^ { * } = \underset { w _ { i } \in [ 0 , 1 ] } { \mathrm { a r g m i n } } w _ { i } \ell _ { i } + \gamma ( \displaystyle \frac 1 2 { w _ { i } } ^ { 2 } - w _ { i } ) } } \\ { { = \underset { w _ { i } \in [ 0 , 1 ] } { \mathrm { a r g m i n } } \displaystyle \frac { \gamma } { 2 } { w _ { i } } ^ { 2 } + ( \ell _ { i } - \gamma ) w _ { i } . } } \end{array}
$$

Since $w _ { i } \in [ 0 , 1 ]$ , when $\ell _ { i } - \gamma > 0$ , the optimal solution $\boldsymbol { w } _ { i } ^ { * }$ is obviously 0. While $\ell _ { i } - \gamma \leq 0$ , let the derivative of Eq.10 be 0, we can obtain

$$
w _ { i } ^ { * } = 1 - \frac { \ell _ { i } } { \gamma } .
$$

Clearly, since $\gamma \geq 0$ and $\ell _ { i } \geq 0$ , we can get $w _ { i } \in [ 0 , 1 ]$ consistent with the original setting. In summary, we can get the solution as follows

$$
w _ { i } ^ { * } = \operatorname* { m a x } ( 0 , 1 - \frac { \ell _ { i } } { \gamma } ) .
$$

If the loss $\ell _ { i }$ is too large (i.e., $\ell _ { i } ~ > ~ \gamma )$ ), we regard the corresponding sample pair as hard data with noise labels and assign a weight as 0. When $\ell _ { i } \leq \gamma$ , the $i$ -th pair with a large weight can be implicitly considered as easy. Otherwise, one with a small weight can be regarded as hard. In general, this strategy can not only distinguish clean sample pairs but also gradually train the hash model from easy to hard, embracing more robustness and generalization simultaneously.

However, selecting a suitable learning pace parameter $\gamma$ is challenging. If $\gamma$ is too small, no sample pairs would be selected for training. And if $\gamma$ is too large, all pairs will participate in training. Clearly, it will cause our NSH to be unable to distinguish noise labels, thus reducing the retrieval performance. In Eq.7, since $v _ { i } ^ { m } \in [ 0 , 1 ]$ and $r > 0$ , we can get the minimum value of $\ell _ { i }$ as

$$
\ell _ { i } ^ { m i n } = 0 .
$$

Similarly, we can obtain the maximum value of $\ell _ { i }$ as

$$
\ell _ { i } ^ { m a x } = \frac { M ( r ^ { 2 } - r + 1 ) } { r } .
$$

Hence, we can get $\gamma$ is bounded by $\textstyle 0 < \gamma < { \frac { M ( r ^ { 2 } - r + 1 ) } { r } }$ . To obtain a suitable $\gamma$ to distinguish clean pairs, we perform the sensitivity analysis in the appendix.

# Experiments

# Dataset

To verify the effectiveness of the proposed RSHNL, we conduct extensive experiments on four widely used datasets, i.e., XMedia (Peng et al. 2015), INRIA-Websearch (Krapac et al. 2010), Wikipedia (Rasiwasia et al. 2010), and XMediaNet (Peng, Huang, and Zhao 2018).

# Experiments Settings

To evaluate the performance of the proposed RSHNL and competitors, we conduct two common cross-modal retrieval tasks. I2T and T2I represent using images as queries to retrieve texts and using texts as queries to retrieve images, respectively. Similar to (Zhen et al. 2019), we adopt Mean Average Precision (MAP) to evaluate the retrieval performance of all methods, which is a widely used evaluating metric. To comprehensively evaluate the effectiveness, we set noisy labels as symmetric noise with different rates (i.e., 0.2, 0.4, 0.6, and 0.8). Besides, the bit lengths are configured to 16, 32, 64, and 128. Besides, all experiments are conducted on a single GeForce RTX3090Ti 24GB GPU and our RSHNL is implemented in PyTorch 1.12.0. More details on implementation are provided in the appendix due to space limitations.

<html><body><table><tr><td rowspan="2">Task</td><td rowspan="2">Method</td><td>Noise</td><td colspan="3">0.2</td><td colspan="4">0.4</td><td colspan="4">0.6</td><td colspan="4">0.8</td></tr><tr><td>Ref.</td><td>16</td><td>32 64</td><td>128</td><td>16</td><td>32</td><td>64</td><td>128</td><td>16</td><td>32</td><td>64</td><td>128</td><td>16</td><td>32</td><td>64</td><td>128</td></tr><tr><td rowspan="11">I2T</td><td>DGCPN</td><td>AAAI'21</td><td>49.0</td><td>64.2</td><td>47.4</td><td>51.3</td><td>49.0</td><td>64.2</td><td>47.4</td><td>51.3</td><td>49.0</td><td>64.2</td><td>47.4</td><td>51.3</td><td>49.0</td><td>64.2 47.4</td><td>51.3</td></tr><tr><td>CIRH</td><td>TKDE'22</td><td>72.5</td><td>78.0</td><td>79.3</td><td>83.8</td><td>72.5 78.0</td><td>79.</td><td>.3 83.8</td><td>72.5</td><td>78.0</td><td>79.3</td><td>83.8</td><td>72.5</td><td>78.0</td><td>79.3</td><td>83.8</td></tr><tr><td>UCCH</td><td>TPAMI'23</td><td>37.6</td><td>50.2</td><td>67.9</td><td>82.7</td><td>37.6</td><td>50.2</td><td>67.9</td><td>82.7</td><td>37.6 50.2</td><td></td><td>67.9</td><td>82.7 37.6</td><td>50.2</td><td>67.9</td><td>82.7</td></tr><tr><td>WASH</td><td>TKDE'23</td><td>80.7</td><td>85.9</td><td>87.1</td><td>87.7</td><td>75.6</td><td>79.4</td><td>81.3</td><td>82.1 44.8</td><td>53.8</td><td>57.4</td><td></td><td>59.6 13.9</td><td>16.3</td><td>17.2</td><td>19.2</td></tr><tr><td>HCCH</td><td>TMM'24</td><td>71.1</td><td>81.9</td><td>82.1</td><td>84.4</td><td>71.6 76.8</td><td>78.2</td><td>80.6</td><td>60.3</td><td>61.5</td><td>61.5</td><td>72.5</td><td>26.3</td><td>41.2</td><td>41.1</td><td>48.5</td></tr><tr><td>DSCMH</td><td>AAAI'24</td><td>81.2</td><td>85.6</td><td>87.9</td><td>88.2</td><td>79.4 83.6</td><td>85.4</td><td>85.8</td><td>63.1</td><td>70.4</td><td>74.4</td><td>77.5</td><td>31.0</td><td>35.1</td><td>43.7</td><td>47.2</td></tr><tr><td>HMAH</td><td>TMM'22</td><td>78.7</td><td>84.4</td><td>86.7</td><td>88.2</td><td>55.0</td><td>65.1</td><td>71.4</td><td>74.6 22.3</td><td>29.8</td><td></td><td>33.5</td><td>37.6 8.7</td><td>9.6</td><td>10.7</td><td>10.7</td></tr><tr><td>CMMQ</td><td>CVPR'22</td><td>86.6</td><td>87.9</td><td>87.4</td><td>86.6</td><td>74.8</td><td>77.4</td><td>74.5</td><td>72.0 51.5</td><td>43.8</td><td></td><td>38.9</td><td>38.7 18.3</td><td>18.5</td><td>13.7</td><td>12.6</td></tr><tr><td>MIAN</td><td>TKDE'23</td><td>18.1</td><td>29.6</td><td>34.5</td><td>35.4</td><td>12.4</td><td>16.2</td><td>17.7</td><td>16.2 10.7</td><td>9.9</td><td></td><td>11.5</td><td>11.1 7.6</td><td>7.7</td><td>7.1</td><td>8.0</td></tr><tr><td>DHRL</td><td>TBD'24</td><td>11.0</td><td>39.3</td><td>86.7</td><td>90.5</td><td>9.9</td><td>66.0</td><td>84.2</td><td>86.6 9.4</td><td>37.7</td><td></td><td>69.6</td><td>74.4 6.6</td><td>8.9</td><td>37.6</td><td>43.8</td></tr><tr><td>DHaPH</td><td>TKDE'24</td><td>79.3</td><td>84.9</td><td>86.9</td><td>88.6</td><td>69.6</td><td>78.2</td><td>82.6</td><td>84.6 52.3</td><td>66.0</td><td></td><td>72.8</td><td>79.8 42.7</td><td>48.3</td><td>60.6</td><td>70.2</td></tr><tr><td></td><td>RSHNL</td><td>Ours</td><td>89.5 90.0</td><td>90.9</td><td>90.8</td><td>89.7</td><td>90.2</td><td>91.1</td><td>89.8</td><td>83.0</td><td>87.7</td><td>88.9</td><td>88.4</td><td>81.7</td><td>88.1</td><td>87.6</td><td>84.7</td></tr><tr><td rowspan="11">T2I</td><td>DGCPN</td><td>AAAI'21</td><td>50.0</td><td>58.2</td><td>31.5</td><td>40.2</td><td>50.0</td><td>58.2</td><td>31.5</td><td>40.2 50.0</td><td>58.2</td><td>31.5</td><td></td><td>40.2 50.0</td><td>58.2</td><td>31.5</td><td>40.2</td></tr><tr><td>CIRH</td><td>TKDE'22</td><td>67.4</td><td>73.1</td><td>76.9</td><td>82.0</td><td>67.4</td><td>73.1</td><td>76.9</td><td>82.0</td><td>67.4</td><td>73.1</td><td>76.9</td><td>82.0</td><td>67.4 73.1</td><td>76.9</td><td>82.0</td></tr><tr><td>UCCH</td><td>TPAMI'23</td><td>56.0</td><td>66.9</td><td>75.5</td><td>83.8</td><td>56.0</td><td>66.9</td><td>75.5</td><td>83.8</td><td>56.0 66.9</td><td>75.5</td><td></td><td>83.8 56.0</td><td>66.9</td><td>75.5</td><td>83.8</td></tr><tr><td>WASH</td><td>TKDE'23</td><td>81.6</td><td>86.0</td><td>86.9</td><td>88.3</td><td>75.0</td><td>78.7</td><td>81.3</td><td>82.1 44.6</td><td>53.6</td><td>56.7</td><td></td><td>59.4 14.3</td><td>16.7</td><td>17.3</td><td>19.4</td></tr><tr><td>HCCH</td><td>TMM'24</td><td>69.5</td><td>80.7</td><td>80.0</td><td>84.0</td><td>70.0</td><td>75.7</td><td>76.7 80.0</td><td>58.1</td><td>60.2</td><td>58.5</td><td>72.6</td><td>26.2</td><td>40.8</td><td>41.2</td><td>48.2</td></tr><tr><td>DSCMH</td><td>AAAI'24</td><td>80.5</td><td>85.0</td><td>86.3</td><td>87.9</td><td>78.0</td><td>81.6</td><td>83.5</td><td>84.8 63.5</td><td>70.9</td><td>73.9</td><td>77.4</td><td>29.1</td><td>33.0</td><td>41.0</td><td>45.0</td></tr><tr><td>HMAH</td><td>TMM'22</td><td>77.4</td><td>84.0</td><td>85.9</td><td>88.0</td><td>53.2</td><td>65.0</td><td>71.0</td><td>73.9</td><td>23.1 30.5</td><td></td><td>34.1</td><td>38.9 8.9</td><td>10.0</td><td>11.5</td><td>11.4</td></tr><tr><td>CMMQ</td><td>CVPR'22</td><td>85.1</td><td>83.4</td><td>82.0</td><td>78.2</td><td>70.8</td><td>74.4</td><td>67.6</td><td>64.4</td><td>45.2</td><td>36.0</td><td>33.2</td><td>43.5 14.4</td><td>14.0</td><td>13.2</td><td>10.2</td></tr><tr><td>MIAN</td><td>TKDE'23</td><td>15.5</td><td>22.1</td><td>28.5</td><td>29.0</td><td>11.0</td><td>14.3</td><td>16.2</td><td>15.0</td><td>8.6</td><td>9.6</td><td>10.9</td><td>10.7</td><td>7.0 7.3</td><td>7.0</td><td>7.7</td></tr><tr><td>DHRL</td><td>TBD'24</td><td>10.6</td><td>38.1</td><td>86.5</td><td>91.0</td><td>10.1</td><td>65.5</td><td>83.5</td><td>86.5</td><td>10.7 39.3</td><td></td><td>68.6</td><td>72.1 8.2</td><td>8.3</td><td>39.9</td><td>45.5</td></tr><tr><td>DHaPH</td><td>TKDE'24</td><td>78.9</td><td>84.9</td><td>88.2</td><td>89.8</td><td>69.5</td><td>77.9</td><td>83.9 90.3</td><td>86.0 90.3 82.6</td><td>51.5 64.9 87.4</td><td>89.1</td><td>72.9 88.8</td><td>80.7 42.3 79.7</td><td>49.2 86.8</td><td>60.0</td><td>70.9 85.3</td></tr><tr><td>RSHNL</td><td>Ours</td><td>86.5</td><td>89.4</td><td>91.0</td><td>90.6</td><td>87.4</td><td>90.2</td></table></body></html>

Table 1: The MAP scores with different bit lengths on the XMedia dataset under different noise rates.

DGCPN DGCPN CIRH CIRH UCCH UCCH WASH WASH   
0.50 HCCH 0.50 HCCH   
i DSCMH DSCMH   
0.25 CHMAMHQ 0.25 CHMAMHQ 0.0 MIAN 0.0 MIAN 0.0 0.5 1.0 DHRL .0 0.5 1.0 DHRL Recall DHaPH Recall DHaPH RSHNL RSHNL (a) PR curves (I2T) (b) PR curves (T2I) WASH WASH HCCH HCCH DSCMH DSCMH   
40 CHMAMHQ AP 40 CHMAMHQ MIAN 20 MIAN DHRL DHRL DHaPH 0 DHaPH 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Noise rate RSHNL Noise rate RSHNL (c) MAP curves (I2T) (d) MAP curves (T2I)

# Comparison Methods

To demonstrate the superiority of the proposed RSHNL, we compare RSHNL with 11 baselines, including deep unsupervised CMH methods (i.e., DGCPN (Yu et al. 2021), CIRH (Zhu et al. 2022), and UCCH (Hu et al. 2022)), shallow supervised CMH methods (i.e., WASH (Zhang et al. 2023a), HCCH (Sun et al. 2023), and DSCMH (Sun et al. 2024a)), and deep supervised CMH methods (i.e., HMAH (Tan et al. 2022), CMMQ (Yang et al. 2022), MIAN (Zhang et al. 2023c), DHRL (Shu et al. 2024), and DHaPH (Huo et al. 2024)). Among these, WASH, CMMQ, and DHRL are specifically designed to deal with the problem of noisy labels. DSCMH and DHaPH are the SPL-based hashing methods. For a fair comparison, we freeze the original backbones in the training process and report MAP scores on the testing set when MAP peaks on the validation set. For all experimental tables, the highest MAP scores are shown in bold, the second highest MAP scores are marked with underline, and $" / "$ denotes out-of-memory.

# Comparison with the State-of-the-Art

The experimental results on three datasets are reported in tables 1 to 3. The results on Wikipedia are given in the Appendix. Besides, we set the hash length as 128-bit on INRIAWebsearch, and then plot the precision-recall (PR) curves under 0.6 noise rate and the MAP curves under different noise rates in Fig.2. From these results, it can be observed:

• The retrieval performance of most methods improves with bit lengths increase because long hash codes contain more discriminative information. Besides, the performance of a few methods (such as CMMQ and RSHNL) degrades with bit lengths increase. This may be because it is difficult to resist the interference of noisy labels for these methods, resulting in more noise information being incorporated into long hash codes.

Table 2: The MAP scores with different bit lengths on the INRIA-Websearch dataset under different noise rates.   

<html><body><table><tr><td rowspan="2">Task</td><td rowspan="2">Method</td><td>Noise</td><td colspan="4">0.2</td><td colspan="4">0.4</td><td colspan="4">0.6</td><td colspan="4">0.8</td></tr><tr><td>Ref.</td><td>16</td><td>32</td><td>64</td><td>128</td><td>16</td><td>32</td><td>64</td><td>128</td><td>16</td><td>32</td><td>64</td><td>128</td><td>16</td><td>32</td><td>64 128</td></tr><tr><td rowspan="10">I2T</td><td>DGCPN</td><td>AAAI'21</td><td>24.3</td><td>32.5</td><td>37.7</td><td>37.3</td><td>24.3</td><td>32.5</td><td>37.7</td><td>37.3</td><td>24.3</td><td>32.5</td><td>37.7</td><td>37.3</td><td>24.3</td><td>32.5</td><td>37.7</td><td>37.3</td></tr><tr><td>CIRH</td><td>TKDE'22</td><td>14.6</td><td>21.5</td><td>26.5</td><td>30.8</td><td>14.6</td><td>21.5</td><td>26.5</td><td>30.8</td><td>14.6</td><td>21.5</td><td>26.5</td><td>30.8</td><td>14.6</td><td>21.5</td><td>26.5</td><td>30.8</td></tr><tr><td>UCCH</td><td>TPAMI'23</td><td>18.9</td><td>26.1</td><td>29.5</td><td>34.3</td><td>18.9</td><td>26.1</td><td>29.5</td><td>34.3</td><td>18.9</td><td>26.1</td><td>29.5</td><td>34.3</td><td>18.9</td><td>26.1</td><td>29.5</td><td>34.3</td></tr><tr><td>WASH</td><td>TKDE'23</td><td>31.5</td><td>38.0</td><td>43.5</td><td>46.2</td><td>26.2</td><td>33.0</td><td>38.5</td><td>42.1</td><td>16.9</td><td>22.1</td><td>27.9</td><td>31.5</td><td>4.7</td><td>7.5</td><td>10.9</td><td>13.7</td></tr><tr><td>HCCH</td><td>TMM'24</td><td>11.8</td><td>20.5</td><td>28.5</td><td>37.6</td><td>8.8</td><td>13.1</td><td>21.9</td><td>33.9</td><td>5.4</td><td>8.3</td><td>12.8</td><td>22.2</td><td>3.3</td><td>3.2</td><td>5.5</td><td>10.2</td></tr><tr><td>DSCMH</td><td>AAAI'24</td><td>18.8</td><td>27.6</td><td>35.2</td><td>41.2</td><td>17.9</td><td>25.0</td><td>33.4</td><td>39.6</td><td>13.5</td><td>19.4</td><td>27.7</td><td>34.1</td><td>5.6</td><td>9.7</td><td>16.7</td><td>21.6</td></tr><tr><td>HMAH</td><td>TMM'22</td><td>26.4</td><td>34.5</td><td>39.8</td><td>42.1</td><td>19.0</td><td>28.3</td><td>34.4</td><td>38.4</td><td>10.3</td><td>17.5</td><td>24.1</td><td>29.1</td><td>5.2</td><td>8.3</td><td>13.4</td><td>18.2</td></tr><tr><td>CMMQ</td><td>CVPR'22</td><td>31.1</td><td>35.5</td><td>38.3</td><td>39.6</td><td>27.8</td><td>32.1</td><td>34.4</td><td>35.8</td><td>17.0</td><td>21.2</td><td>27.9</td><td>30.4</td><td>4.1</td><td>3.9</td><td>11.6</td><td>9.8</td></tr><tr><td>MIAN</td><td>TKDE'23</td><td>2.7</td><td>2.7</td><td>2.2</td><td>2.8</td><td>2.7</td><td>1.7</td><td>2.8</td><td>2.8</td><td>2.8</td><td>2.4</td><td>1.7</td><td>1.7</td><td>2.8</td><td>2.3</td><td>1.8</td><td>1.4</td></tr><tr><td>DHRL</td><td>TBD'24</td><td>2.8</td><td>4.2</td><td>33.6</td><td>33.8</td><td>2.6</td><td>2.8</td><td>23.8</td><td>24.3</td><td>2.7</td><td>2.7</td><td>12.6</td><td>8.3</td><td>2.8</td><td>3.0</td><td>4.9</td><td>6.2</td></tr><tr><td>DHaPH RSHNL</td><td>TKDE'24</td><td></td><td>24.0 32.8</td><td>39.5</td><td>44.3</td><td>22.3</td><td>29.3</td><td></td><td>37.0</td><td>42.0</td><td>19.9</td><td>28.1</td><td>34.7</td><td>39.8</td><td>19.5</td><td>26.1 33.6</td><td>38.5</td></tr><tr><td></td><td></td><td>Ours</td><td>39.3</td><td>48.0 51.9</td><td></td><td>52.4</td><td>37.9</td><td>45.8</td><td>50.3</td><td>51.6</td><td>31.2</td><td>38.3</td><td>47.6</td><td>49.0</td><td>28.3</td><td>38.2 41.8</td><td>42.9</td></tr><tr><td rowspan="13">T2I</td><td>DGCPN</td><td>AAAI'21</td><td>22.9</td><td>32.0</td><td>37.5</td><td>36.9</td><td>22.9</td><td>32.0</td><td>37.5</td><td>36.9</td><td>22.9</td><td>32.0</td><td>37.5</td><td>36.9</td><td>22.9</td><td>32.0</td><td></td><td>36.9</td></tr><tr><td>CIRH</td><td>TKDE'22</td><td>14.2</td><td>21.2</td><td>26.6</td><td>31.2</td><td>14.2</td><td>21.2</td><td>26.6</td><td>31.2</td><td>14.2</td><td>21.2</td><td>26.6</td><td>31.2</td><td>14.2</td><td>21.2</td><td>37.5 26.6</td><td>31.2</td></tr><tr><td>UCCH</td><td>TPAMI'23</td><td>17.4</td><td>25.3</td><td>29.5</td><td>34.9</td><td>17.4</td><td>25.3</td><td>29.5</td><td>34.9</td><td>17.4</td><td>25.3</td><td>29.5</td><td>34.9</td><td>17.4</td><td>25.3</td><td></td><td>34.9</td></tr><tr><td>WASH</td><td>TKDE'23</td><td>30.8</td><td>38.4</td><td>44.8</td><td>47.8</td><td>25.1</td><td>32.7</td><td>39.3</td><td>43.3</td><td>15.8</td><td>21.7</td><td>27.8</td><td>31.9</td><td>4.2</td><td>7.2</td><td>29.5</td><td>13.3</td></tr><tr><td>HCCH</td><td>TMM'24</td><td>12.7</td><td>23.6</td><td>35.0</td><td>42.5</td><td>9.4</td><td>17.0</td><td>29.4</td><td>39.0</td><td>5.9</td><td>10.8</td><td>19.7</td><td>29.3</td><td>3.0</td><td>3.6</td><td>10.6 7.1</td><td>13.1</td></tr><tr><td>DSCMH</td><td>AAAI'24</td><td>19.1</td><td>27.0</td><td>32.8</td><td>37.6</td><td>18.5</td><td>24.6</td><td>31.1</td><td>35.7</td><td>13.9</td><td>18.5</td><td>24.4</td><td>30.7</td><td>5.6</td><td>8.7</td><td>15.6</td><td>19.0</td></tr><tr><td>HMAH</td><td>TMM'22</td><td>25.0</td><td>34.5</td><td>40.8</td><td>43.9</td><td>18.2</td><td>28.3</td><td>34.9</td><td>39.7</td><td>9.9</td><td>17.1</td><td>24.4</td><td>29.4</td><td>4.7</td><td>7.9</td><td>13.1</td><td>17.7</td></tr><tr><td>CMMQ</td><td>CVPR'22</td><td>31.4</td><td>36.6</td><td>38.5</td><td>39.0</td><td>27.3</td><td>32.3</td><td>34.5</td><td>34.1</td><td>16.2</td><td>20.6</td><td>27.3</td><td>30.4</td><td>4.3</td><td>3.4</td><td>10.7</td><td>8.5</td></tr><tr><td>MIAN</td><td>TKDE'23</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td><td>1.2</td></tr><tr><td>DHRL</td><td>TBD'24</td><td>2.7</td><td>4.2</td><td>32.3</td><td>33.7</td><td>2.7 21.0</td><td>2.6 29.3</td><td>23.5</td><td>24.0</td><td>2.8</td><td>2.6</td><td></td><td>13.2 7.7</td><td>2.7</td><td>2.9</td><td>4.8</td><td>7.1</td></tr><tr><td>DHaPH</td><td>TKDE'24</td><td>22.4</td><td>32.8</td><td>40.7 47.9</td><td>45.7 52.1 53.3</td></table></body></html>

<html><body><table><tr><td rowspan="2">Task</td><td rowspan="2">Method</td><td>Noise</td><td colspan="4">0.2</td><td colspan="4">0.4</td><td colspan="4">0.6</td><td colspan="4">0.8</td></tr><tr><td>Ref.</td><td>16</td><td>32</td><td>64</td><td>128</td><td>16</td><td>32</td><td>64</td><td>128</td><td>16</td><td>32</td><td>64</td><td>128</td><td>16</td><td>32 1</td><td>64 128</td></tr><tr><td rowspan="8">I2T</td><td rowspan="8">DGCPN CIRH UCCH</td><td>AAAI'21</td><td>/ /</td><td>7</td><td>T</td><td></td><td>7</td><td></td><td>7</td><td>/</td><td>/</td><td></td><td>/</td><td>/</td><td>7</td><td>/</td><td></td></tr><tr><td></td><td>TKDE'22</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/ /</td><td>/</td><td>/</td><td>/</td><td>/</td><td>7 /</td><td>/</td></tr><tr><td></td><td>TPAMI'23 9.4</td><td>13.5 15.1</td><td>17.1</td><td>19.7</td><td>9.4</td><td>13.5 17.1</td><td>19.7</td><td>9.4</td><td>13.5</td><td>17.1</td><td>19.7</td><td>9.4 2.0</td><td>13.5</td><td>17.1</td><td>19.7</td></tr><tr><td>WASH HCCH</td><td>TKDE'23</td><td>8.0 1.6</td><td>24.2</td><td>34.0</td><td>7.0</td><td>13.1</td><td>21.2</td><td>30.6</td><td>4.9</td><td>8.5</td><td>14.4</td><td>21.8</td><td>3.1</td><td>4.7</td><td>6.9</td></tr><tr><td></td><td>TMM'24</td><td>2.0</td><td>4.8</td><td>15.0</td><td>1.4</td><td>1.5</td><td>3.4</td><td>12.1</td><td>1.3</td><td>1.4</td><td>2.2</td><td>5.8 0.8</td><td>0.9</td><td>1.1</td><td>1.9</td></tr><tr><td>DSCMH</td><td>AAAI'24</td><td>5.2 2.9</td><td>9.9 18.1</td><td>28.7</td><td>4.7</td><td>8.5</td><td>14.9</td><td>24.7</td><td>3.4</td><td>5.4</td><td>10.3</td><td>18.5</td><td>1.8</td><td>2.3</td><td>3.9 6.6</td></tr><tr><td>HMAH TMM'22 CMMQ CVPR'22</td><td></td><td>3.6</td><td>5.9</td><td>10.9 /</td><td>1.3 /</td><td>1.5</td><td>1.9</td><td>3.9</td><td>1.0</td><td>1.0</td><td>1.3</td><td>2.2</td><td>1.0 1.1 /</td><td>1.3</td><td>1.7</td></tr><tr><td>MIAN</td><td></td><td>/ / 0.8 1.5</td><td>/ 1.8</td><td>2.5</td><td>0.8</td><td>/ 1.2</td><td>/ 1.3</td><td>/ 1.8</td><td>/ 0.7</td><td>/ 0.9</td><td>/ 1.0</td><td>/ 1.2</td><td>/ 0.7</td><td>/ 0.8</td><td>/ 0.9</td></tr><tr><td>DHRL</td><td>TBD'24</td><td>0.7</td><td>0.7 0.9</td><td>15.2</td><td>0.7</td><td>0.7</td><td>1.0</td><td>13.2</td><td>0.7</td><td>0.7</td><td>2.7</td><td>6.3</td><td>0.7</td><td>0.8 0.7</td><td>0.7</td><td>1.3</td></tr><tr><td>DHaPH</td><td>TKDE'24</td><td>9.8</td><td>15.9</td><td>23.4</td><td>28.5</td><td>9.1</td><td>15.8</td><td>22.6</td><td>27.6</td><td>9.2</td><td>15.1</td><td>21.9</td><td>27.5</td><td>8.8</td><td>15.2 21.5</td><td>27.3</td></tr><tr><td>RSHNL</td><td>Ours</td><td>35.3</td><td>43.5</td><td>47.5</td><td>48.6</td><td>35.5</td><td>42.1</td><td>47.0</td><td>45.8</td><td>33.2</td><td>41.8</td><td>46.2</td><td>45.0</td><td>28.2 39.6</td><td>44.9</td><td>38.9</td></tr><tr><td>DGCPN</td><td>AAAI'21</td><td>/</td><td>/</td><td>/</td><td>！</td><td></td><td></td><td>！</td><td></td><td>/</td><td>/</td><td>/</td><td>/</td><td></td><td></td><td></td></tr><tr><td rowspan="10">T2I</td><td>CIRH</td><td></td><td>/</td><td>/</td><td>//</td><td></td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>UCCH</td><td>TKDE'22</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td></tr><tr><td></td><td>TPAMI'23</td><td>10.6</td><td>15.1</td><td>19.4</td><td>22.1</td><td>10.6 15.1</td><td>19.4</td><td>22.1</td><td>10.6</td><td>15.1</td><td>19.4</td><td>22.1</td><td>10.6</td><td>15.1</td><td>19.4</td><td>22.1</td></tr><tr><td>WASH</td><td>TKDE'23</td><td>10.3</td><td>17.3</td><td>25.8</td><td>35.5</td><td>8.9 15.2</td><td>22.8</td><td>32.2</td><td>6.3</td><td>10.2</td><td>15.9</td><td>23.4</td><td>2.4</td><td>3.6</td><td>5.4</td><td>7.8</td></tr><tr><td>HCCH</td><td>TMM'24</td><td>1.8</td><td>1.5</td><td>2.1</td><td>9.0 1.4</td><td>1.2</td><td>1.7</td><td>12.1</td><td>1.5</td><td>1.1</td><td>1.3</td><td>1.8</td><td>0.8</td><td>0.9</td><td>1.0</td><td>1.2</td></tr><tr><td>DSCMH</td><td>AAAI'24</td><td>4.1</td><td>8.4</td><td>16.5</td><td>27.5</td><td>3.6 7.0</td><td>13.5</td><td>23.8</td><td>2.5</td><td>4.4</td><td>9.3</td><td>17.5</td><td>1.4</td><td>2.0</td><td>3.4</td><td>5.9</td></tr><tr><td>HMAH</td><td>TMM'22</td><td>3.5</td><td>4.4</td><td>6.6</td><td>12.1</td><td>1.6</td><td>1.8 2.3</td><td>4.4</td><td>1.1</td><td>1.2</td><td>1.5</td><td>2.5</td><td>1.0</td><td>1.2</td><td>1.4</td><td>1.8</td></tr><tr><td>CMMQ MIAN</td><td>CVPR'22</td><td>/ 0.8</td><td>/ 1.2</td><td>/ 1.2</td><td>/ 1.8</td><td>/ 0.7</td><td>/</td><td>/</td><td>/ /</td><td>/</td><td></td><td>/</td><td>/ /</td><td>/</td><td>/</td><td>/</td></tr><tr><td>DHRL</td><td>TKDE'23 TBD'24</td><td>0.7</td><td>0.8</td><td>0.9</td><td>19.8</td><td>0.8 0.9</td><td>1.0 1.0</td><td>1.3 1 15.1</td><td>0.7 0.7</td><td>0.8 0.8</td><td>0.8 3.1</td><td>1.0 8.7</td><td>0.7</td><td>0.7</td><td>0.7</td><td>0.8</td></tr><tr><td>DHaPH</td><td>TKDE'24</td><td>11.0</td><td></td><td>26.3</td><td>32.6</td><td>10.5</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.8</td><td>0.8 17.1</td><td>0.8 24.0</td><td>1.5</td></tr><tr><td>RSHNL</td><td></td><td></td><td>18.0</td><td></td><td></td><td></td><td>17.3</td><td>25.3</td><td>31.6</td><td>10.3</td><td>16.8</td><td>24.8</td><td>31.2 46.1</td><td>9.6 29.3 39.9</td><td>45.1</td><td>30.7 40.6</td></tr><tr><td></td><td>Ours</td><td>35.3</td><td>42.5</td><td>46.5</td><td>47.8</td><td>35.0</td><td>41.6</td><td>46.9</td><td>46.6</td><td>33.3</td><td>41.3</td><td>46.4</td></table></body></html>

Table 3: The MAP scores with different bit lengths on the XMediaNet dataset under different noise rates.

• The retrieval performance of all supervised CMH methods is affected by noisy labels. As the noise rate increases, the CMH model is more likely to be misled, thereby resulting in a rapid drop in performance. Since unsupervised CMH methods do not need to exploit the label information, their performance is not affected by noisy labels at all.

• Most methods show worse performance or even fail on the INRIA-Websearch and XMediaNet datasets because more categories significantly increase the difficulty of learning discriminative hash codes from noisy labels.

• From PR curves, our RSHNL outperforms other baselines, which is consistent with what MAP demonstrates. According to MAP curves, the performance of almost all supervised CMH methods degrades as the noise rate increases. Thanks to the noise recognition capability of SPL, our RSHNL maintains stable and superior performance. Overall, the comprehensive performance of RSHNL outperforms all baselines.

350 300 FLI 350 FLI 300 12500 TLI 2050 TLI 100 100 50 50 00.000.020.040.060.080.100.120.14 0 0.00 0.02 0.04 0.06 0.08 0.10 0.12 Weight Weight (a) Epochs=5 (b) Epochs=100

# Self-paced Analysis

To study the self-paced behavior of our RSHNL with 128 bits and 0.6 noise rate, we plot the density versus the weight of each instance from different epochs on INRIAWebsearch. From Fig.3, we can observe that: 1) At the beginning, RSHNL first assigns zero weight to hard instances and regards them as noisy instances, thereby separating multi-modal data into a clean or noisy subset. 2) As the training progresses, RSHNL gradually learns with all clean instances from easy to hard until all instances become easy.

# Ablation Study

To show the effectiveness of the proposed components, we conduct ablation experiments with 128 bits on the two datasets compared with three variants. Specifically, RSHNL1, RSHNL-2, RSHNL-3 represent removing the warm-up training, removing the loss $\mathcal { L } _ { C }$ , and removing SPL of $\mathcal { L } _ { S }$ , respectively. To be fair, all variants adopt the same parameters as RSHNL. As shown in Tab.4, we report their average MAP scores on I2T and T2I tasks. From these results, RSHNL shows the best retrieval performance, which means that all components are crucial for RSHNL.

<html><body><table><tr><td>Dataset</td><td colspan="4">XMedia</td><td colspan="4">INRIA-Websearch</td></tr><tr><td>Noise</td><td>0.2</td><td>0.4</td><td>0.6</td><td>0.8</td><td>0.2</td><td>0.4</td><td>0.6</td><td>0.8</td></tr><tr><td>RSHNL-1</td><td>84.3</td><td>84.3</td><td>84.3</td><td>84.3</td><td>38.6</td><td>38.6</td><td>38.6</td><td>38.6</td></tr><tr><td>RSHNL-2</td><td>90.1</td><td>85.6</td><td>72.</td><td></td><td>50.1</td><td>45.6</td><td>20.2</td><td>8.7</td></tr><tr><td>RSHNL-3</td><td>84.8</td><td>76.1</td><td>63.0</td><td>6</td><td>44.4</td><td>31.6</td><td>25.6</td><td>20.2</td></tr><tr><td>RSHNL</td><td>90.7</td><td>90.0</td><td>88.6</td><td>85.0</td><td>52.9</td><td>51.9</td><td>49.4</td><td>42.9</td></tr></table></body></html>

![](images/7570c1878dc52a0ede53510bb6e243a6fb58e335135722c27a1383c84fee5449.jpg)  
Figure 3: The density versus the weight of all instances with 128 bits and 0.6 noise rate, where ‘FLI’ and ‘TLI’ denote ‘False labeled instances’ and ‘True labeled instances’, respectively.   
Table 4: Ablation study with 128 bits.   
Figure 4: The average MAP scores versus epochs.

# Robustness Analysis

To intuitively show the robustness of our RSHNL, on INRIA-Websearch under 0.6 noise, we compare it with CMMQ and two variants. Specifically, RSHNL-S represents removing the progressive learning mechanism by setting all weights greater than 0 to 1. RSHNL-D allows all instances to participate in the training by setting $\gamma$ as a value (e.g., 200) greater than $\ell _ { i } ^ { m a x }$ . Then, we plot the average MAP scores of I2T and T2I tasks with 128 bits. From Fig.4, we can observe that: 1) RSHNL-D overfits the noise, which indicates that the ability to distinguish noise is crucial. 2) Although CMMQ and RSHNL-S can prevent the overfitting problem, their retrieval performance is still lower than our method, which means our NSH could effectively improve the discrimination of hash codes by learning from easy to hard.

# Conclusion

In this paper, we propose a new cognitive cross-modal hashing approach (i.e., RSHNL) with noisy labels, which contains three parts, i.e., CHL, CAL, and NSH. Specifically, CHL maximizes the consistency of multi-modal data to alleviate the semantic gap. CAL learns a unified hash representation for each class as a center and encourages hash codes with the same category to be close to the corresponding hash centers. NSH presents a dynamic hardness measurement strategy that dynamically estimates the learning difficulty for each pair and distinguishes the noisy labels while facilitating learning hash codes from easy to hard for clean pairs. Extensive experiments show that RSHNL outperforms 11 state-of-the-art CMH methods under noisy labels.