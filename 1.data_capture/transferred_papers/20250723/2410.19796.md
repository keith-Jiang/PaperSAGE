# Feature Clipping for Uncertainty Calibration

Linwei Tao1, Minjing Dong2, Chang $\mathbf { X } \mathbf { u } ^ { 1 }$

1University of Sydney 2City University of Hong Kong linwei.tao@sydney.edu.au, minjdong@cityu.edu.hk, c.xu@sydney.edu.au

# Abstract

Deep neural networks (DNNs) have achieved significant success across various tasks, but ensuring reliable uncertainty estimates, known as model calibration, is crucial for their safe and effective deployment. Modern DNNs often suffer from overconfidence, leading to miscalibration. We propose a novel post-hoc calibration method called feature clipping (FC) to address this issue. FC involves clipping feature values to a specified threshold, effectively increasing entropy in high calibration error samples while maintaining the information in low calibration error samples. This process reduces the overconfidence in predictions, improving the overall calibration of the model. Our extensive experiments on datasets such as CIFAR-10, CIFAR-100, and ImageNet, and models including CNNs and transformers, demonstrate that FC consistently enhances calibration performance. Additionally, we provide a theoretical analysis that validates the effectiveness of our method. As the first calibration technique based on feature modification, feature clipping offers a novel approach to improving model calibration, showing significant improvements over both post-hoc and train-time calibration methods and pioneering a new avenue for feature-based model calibration.

Code — https://github.com/Linwei94/AAAI2025-FC.git

# Introduction

While deep neural networks achieve significant improvements across various tasks, model calibration—ensuring a model provides reliable uncertainty estimates—is as important as achieving high prediction accuracy. Accurate and reliable uncertainty estimation is vital for many safety-critical downstream tasks, such as autonomous driving (Feng et al. 2019) and medical diagnosis (Chen et al. 2018). However, recent studies (Guo et al. 2017) have found that most modern neural networks struggle to accurately reflect the actual probabilities of their predictions through their confidence scores. Thus, improving model calibration techniques is essential to enhance the reliability of these models.

Efforts to address this issue can be divided into two streams: train-time calibration and post-hoc calibration. The first stream is train-time calibration, which includes training frameworks (Tao et al. 2023a; Liu et al. 2023), data augmentation (Wang et al. 2023; Zhang et al. 2022; Hendrycks et al. 2019), and regularization techniques like label smoothing (Mu¨ ller et al. 2019; Liu et al. 2022) and entropy regularizers (Pereyra et al. 2017). Training losses such as dual focal loss (Tao et al. 2023b) and focal loss (Mukhoti et al. 2020) are also notable methods. The second stream is post-hoc calibration methods, which are applied to trained models and modify the output probability. Representative works include Isotonic Regression (Zadrozny and Elkan 2002), Histogram Binning (Zadrozny and Elkan 2001), and Temperature Scaling (TS) (Guo et al. 2017). Among these, TS is widely accepted due to its simplicity and good performance. Many subsequent works (Frenkel et al. 2021; Xiong et al. 2023; Yang et al. 2024; Tomani et al. 2022) propose improved versions of TS, often making the temperature factor adaptive according to different criteria.

Guo et al. (2017) identified overconfidence as a major cause of miscalibration in most modern neural networks. Adding a maximum-entropy penalty effectively increases prediction uncertainty, thereby mitigating overconfidence issues. Many calibration methods can be summarized as using entropy regularization in various forms. Pereyra et al. (2017) apply a maximum entropy penalty uniformly to all samples. Similarly, Label Smoothing (Mu¨ller et al. 2019) can be transformed into a form of entropy penalty, while Focal Loss (Mukhoti et al. 2020) can be viewed as the upper bound of a form with negative entropy, effectively adding a maximum entropy regularizer. TS often uses a temperature parameter larger than 1, resulting in a smoother probability distribution with higher entropy.

Since the features extracted by neural networks are direct representations of data, a possible way to mimic this maximum-entropy penalty effect is by applying information loss directly to the features, thereby increasing entropy. To explore this idea, we begin by comparing high calibration error samples with low calibration error samples. Accurately obtaining per-sample calibration error is nontrivial, so we choose wrongly predicted samples with high confidence (greater than 0.95) as high calibration error (HCE) samples and correctly predicted samples with high confidence (greater than 0.95) as low calibration error (LCE) samples. We randomly select 100 feature units from the feature of these samples and plot the average unit value in Figure 1.

![](images/f891a81a8c641444313033c39ffaceccb92adc3ef86edb93cd71f583c138f0f6.jpg)  
Figure 1: Average feature value of samples with high or low calibration error. We randomly select 100 feature units out of 2048 units. The high/low calibration error samples are selected as the wrongly/correctly predicted samples with confidence larger than 0.95. High calibration error samples shows a obvious tendency of higher feature value in around $30 \%$ feature. We provide a comparison of full 2048 feature units in Appendix, which shows similar pattern.

![](images/8026b5901e2e0e5e91a2f0af518b8f5c8963e445da7c701ead3d264547c0e03e.jpg)  
Figure 2: Histogram of feature values for HCE and LCE samples. The thicker tail of the HCE distribution indicates a larger variance $\sigma ^ { 2 }$ compared to the LCE distribution. These experiments were conducted using ResNet-50 on the CIFAR-10 dataset.

We observe that the feature value of HCE samples is much higher than that of LCE samples in some units. A potential solution is to clip the feature values, making values larger than a threshold $c$ equal to $c$ . This might help reduce the abnormally large feature values, increasing the entropy of HCE samples. For example, in Figure 1, we propose feature clipping at 0.15 to increase the entropy of HCE samples while retaining the information of LCE samples. This removes significant information from HCE samples, making them more uncertain, while maintaining as much information as possible from LCE samples. We also plot the histogram of feature of both HCE samples and LCE samples to examine the feature distribution of both samples as shown in Fig. 2. We observe that HCE samples exhibit a thicker tail, indicating a larger variance compared to LCE samples. These patterns suggest notable differences in features between HCE and LCE samples. Therefore, it is worthwhile to conduct a deeper study on how to calibrate models based on these features.

Motivated by these observation, we propose a simple and effective post-hoc calibration method called feature clipping (FC), which clips the feature value to a hyperparameter $c$ , optimized on the validation set to minimize negative log likelihood (NLL), similar to temperature scaling. We also provide a solid theoretical analysis to prove the effectiveness of FC. To the best of our knowledge, we are the first to propose a calibration method based on feature modification. We conduct extensive experiments on a wide range of datasets, including CIFAR-10, CIFAR-100, and ImageNet, and models, including CNNs and transformers. Our method shows consistent improvement. Furthermore, since we are the first to perform calibration on features, our method is orthogonal to previous calibration methods. Extensive experiments demonstrate that FC can enhance calibration performance over both previous post-hoc and train-time calibration methods. Overall, we make the following contributions:

• We propose a simple and effective calibration method called feature clipping, which achieves SOTA calibration performance across multiple models and datasets. • We provide a solid theoretical analysis to prove the effectiveness of feature clipping by showing feature clipping increases more entropy on HCE samples. • We are the first to propose calibration based on features, initiating a new avenue for feature-based calibration. Our method serves as a strong baseline for this emerging area.

# Related Works

Deep neural networks have long been a focus of calibration research (Guo et al. 2017), with extensive studies examining their calibration properties (Minderer et al. 2021; Wang, Feng, and Zhang 2021; Tao et al. 2023c). Numerous calibration methods have been proposed, generally divided into two categories: train-time calibration and post-hoc calibration.

Train-Time Calibration Train-time calibration aims to improve a model’s calibration performance during training. A notable example is focal loss (Mukhoti et al. 2020), with subsequent works such as dual focal loss (Tao et al. 2023b) focusing on both the highest and second-highest probabilities. Adaptive focal loss (Ghosh, Schaaf, and Gormley 2022) modifies hyperparameters for different sample groups based on prior training knowledge. These focal loss-based methods can be transformed into an upper bound of negative entropy, thereby performing an entropy penalty during training. Similarly, label smoothing (Mu¨ller et al. 2019) can also be transformed into a form of entropy penalty.

Post-Hoc Calibration Post-hoc calibration is resourceefficient and can be easily applied to pretrained models without altering their weights, preserving the model’s accuracy and robustness. A common technique is temperature scaling (TS), which adjusts the output probability distribution’s sharpness via a temperature parameter optimized to minimize negative log likelihood (NLL) on a validation set. TS typically uses larger temperature parameters for CNN models, reducing probability distribution sharpness and acting as an uniform maximum-entropy regularizer. Many subsequent methods aim to improve TS by applying adaptive temperature parameters, treating samples differently for a more effective maximum-entropy regularizer. For example, CTS (Frenkel et al. 2021) adapts temperature based on class labels, while PTS (Tomani et al. 2022) proposes learnable temperature parameters using a neural network. Recent methods like Proximity-based TS (Xiong et al. 2023) and Group Calibration (Yang et al. 2024) adjust temperature based on features, aiming for more precise entropy penalties.

Calibration Using Features Although feature representation is a crucial aspect of deep neural networks and is well-studied in robustness literature (Ilyas et al. 2019), it is underutilized in calibration literature. Pioneering works such as (Xiong et al. 2023; Yang et al. 2024) have explored using features to group similar samples to achieve multicalibration (He´bert-Johnson et al. 2018). However, they do not perform calibration based on feature modification.

# Methodology

Problem Formulation In a classification task, let $\chi$ be the input space and $y$ be the label space. The classifier $f$ maps an input to a probability distribution $\hat { p } _ { [ 1 , 2 , . . . , K ] } \ \in \ [ 0 , 1 ] ^ { K }$ over $K$ classes. The confidence of a prediction is defined as the largest probability, $\operatorname* { m a x } ( \hat { p } _ { i } )$ . For simplicity, we use $\hat { p }$ to represent confidence in the following discussion.

A network is perfectly calibrated if the predicted confidence $\hat { p }$ accurately represents the true probability of the classification being correct. Formally, a perfectly calibrated network satisfies ${ \bar { \mathbb { P } } } ( { \hat { y } } ~ = ~ y | { \hat { p } } ~ = ~ { \bar { p } } ) ~ { \mathrm { ~ = ~ } } ~ p$ for all $p \in$ [0, 1] (Guo et al. 2017), where $\hat { y }$ is the predicted label and $y$ is the ground truth label. Given the confidence score and the probability of correctness, the Expected Calibration $E r$ - ror (ECE) is defined as $\mathbb { E } _ { \hat { p } } [ | \mathbb { P } ( \hat { y } = \bar { y } | \hat { p } ) - \hat { p } | ]$ . In practice, since the calibration error cannot be exactly derived from finite samples, an approximation of ECE is introduced (Guo et al. 2017). Specifically, samples are grouped into $M$ bins $\{ B _ { m } \} _ { m = 1 } ^ { M }$ based on their confidence scores, where $B _ { m }$ contains samples with confidence scores $\hat { p } _ { i } \ \in \ \left[ \frac { m - 1 } { M } , \frac { m } { M } \right)$ For each bin , the average confidence is computed as $\begin{array} { r c l } { C _ { m } } & { = } & { \frac { 1 } { \left| B _ { m } \right| } \sum _ { i \in B _ { m } } \hat { p } _ { i } } \end{array}$ and the bin accuracy as $A _ { m } \ =$ B1m i Bm 1(yˆi = yi), where 1 is the indicator function. The ECE is then approximated as the expected absolute difference between bin accuracy and average confidence:

$$
\mathrm { E C E } \approx \sum _ { m = 1 } ^ { M } \frac { \left| B _ { m } \right| } { N } \left| A _ { m } - C _ { m } \right| ,
$$

where $N$ is the total number of samples. Besides this estimated ECE, there are variants like Adaptive ECE (Krishnan and Tickoo 2020), which groups samples into bins with equal sample sizes, and Classwise ECE (Kull et al. 2019), which computes ECE over $K$ classes.

Feature Clipping We propose feature clipping (FC), a simple and effective post-hoc calibration method designed to reduce overconfidence problem in deep neural networks. The key idea is to clip the feature values to a specified threshold, thereby increasing entropy in HCE samples while preserving the information in LCE samples. This approach helps mitigate overconfidence issues in HCE samples and improves overall model calibration. Given feature values $x$ , we apply feature clipping as follows:

$$
\tilde { x } = \operatorname* { m a x } ( \operatorname* { m i n } ( x , c ) , - c )
$$

where $c$ is a positive hyperparameter optimized on a validation set to minimize negative log likelihood (NLL).

# Theoretical Evidence

In this section, we present theoretical evidence to explain the effectiveness of feature clipping by analyzing the information loss in features of HCE and LCE samples. Our aim is to demonstrate that after feature clipping, HCE samples, characterized by larger variance, experience greater information loss compared to LCE samples, which have smaller variance. Consequently, we perform the entropy penalty differently to HCE and LCE samples and make HCE samples more uncertain.

Entropy of original feature We consider the case where the output are all postive values after ReLU activation function. Consider the feature vector $\textbf { x } : = ~ \{ x _ { 1 } , . . . , x _ { n } \}$ extracted from a sample, which is normally the output of penultimate layer of a neural network. Suppose the feature value $X$ follows a rectified normal distribution (Socci, Lee, and Seung 1997), which is a mixture distribution with both discrete variables and continuous variables. To calculate the entropy for this mixture distribution1 (Politis 1991), first, we treat the continuous variables as the truncated normal distribution (Burkardt 2014).

For a standard truncated normal distribution, suppose $X$ has a normal distribution with mean $\mu = 0$ and variance $\sigma ^ { 2 }$ and lies within the interval $( a , b )$ . The probability density function (PDF) of truncated normal distribution is given by:

$$
f ( x ; \mu , \sigma , a , b ) = { \frac { 1 } { \sigma } } { \frac { \phi \left( { \frac { x - \mu } { \sigma } } \right) } { \Phi \left( { \frac { b - \mu } { \sigma } } \right) - \Phi \left( { \frac { a - \mu } { \sigma } } \right) } }
$$

and by $f = 0$ otherwise. Here, $\begin{array} { r } { \phi ( \xi ) = \frac { 1 } { \sqrt { 2 \pi } } \exp \left( - \frac { 1 } { 2 } \xi ^ { 2 } \right) } \end{array}$ is the probability density function of the standard normal distribution and $\Phi ( \cdot )$ is its cumulative distribution function $\begin{array} { r } { \Phi ( x ) = \frac { 1 } { 2 } \left( 1 + \mathrm { e r f } \left( \frac { x } { \sqrt { 2 } } \right) \right) } \end{array}$ and $\begin{array} { r } { \operatorname { e r f } ( x ) = \frac { 2 } { \sqrt { \pi } } \int _ { 0 } ^ { x } e ^ { - t ^ { 2 } } d t } \end{array}$ is the error function. By definition, if $b = \infty$ , then $\begin{array} { r } { \Phi \left( \frac { b - \mu } { \sigma } \right) = } \end{array}$ 1. The entropy of truncated normal distribution is given by:

$$
\begin{array} { l } { { H _ { c } ( x ; \mu , \sigma , a , b ) = \displaystyle - \int _ { a } ^ { b } f ( x ) \log f ( x ) d x } } \\ { { \displaystyle ~ = \log ( \sqrt { 2 \pi e \sigma Z } ) + \frac { \alpha \phi ( \alpha ) - \beta \phi ( \beta ) } { 2 Z } } } \end{array}
$$

where $\begin{array} { r } { \alpha = \frac { a - \mu } { \sigma } , \quad \beta = \frac { b - \mu } { \sigma } } \end{array}$ b−µ and Z = Φ(β) Φ(α).

Thus, the PDF of the continuous variables of the mixture distribution is $f ( x ; 0 , \sigma , 0 , + \infty )$ and the corresponding differential entropy is $H _ { c } ( x ; 0 , \sigma , 0 , + \infty )$ . The probability mass function (PMF) for discrete variables in the mixture distribution is given by:

$$
p ( x ) = { \left\{ \begin{array} { l l } { 1 0 0 \% } & { { \mathrm { i f ~ } } x = 0 } \\ { 0 } & { { \mathrm { o t h e r w i s e } } } \end{array} \right. } ,
$$

and the corresponding Shannon entropy is $H _ { d } ( x ) = 0$ .

Assume the input of ReLU layer follows Gaussian distribution with mean at 0, we can derive that feature $x$ with probability $q = 0 . 5$ to be discrete variables and $1 - q$ to be continuous variables. According to the entropy calculation of mixture distribution (Politis 1991), the entropy of original feature $x$ is given by:

$$
\begin{array} { l } { { H ( x ) = - q \log q - ( 1 - q ) \log ( 1 - q ) } } \\ { { \ ~ + q H _ { d } ( x ) + ( 1 - q ) H _ { c } ( x ; 0 , \sigma , 0 , + \infty ) } } \\ { { \ ~ = - \log ( \displaystyle \frac 1 2 ) - \displaystyle \frac 1 2 H _ { c } ( x ; 0 , \sigma , 0 , + \infty ) } } \\ { { \ ~ = - \log ( \displaystyle \frac 1 2 ) - \displaystyle \frac 1 2 \log ( \sqrt { \pi e \sigma } ) } } \end{array}
$$

Entropy of clipped feature Similarly, the clipped feature $\tilde { x }$ follows mixture distribution with discrete variables and continuous variables, where the PDF of the continuous variables is $f ( \tilde { x } ; 0 , \sigma , 0 , c )$ and the corresponding differential entropy is $H _ { c } ( x ; 0 , \sigma , 0 , c )$ The PMF for discrete variables is given by:

$$
p ( \tilde { x } ) = \left\{ \begin{array} { l l } { \frac { \Phi ( 0 ) } { \Phi ( 0 ) + ( 1 - \Phi \left( \frac { c } { \sigma } \right) ) } } & { \mathrm { i f } \tilde { x } = 0 } \\ { \frac { 1 - \Phi \left( \frac { c } { \sigma } \right) } { \Phi ( 0 ) + ( 1 - \Phi \left( \frac { c } { \sigma } \right) ) } } & { \mathrm { i f } \tilde { x } = c } \\ { 0 } & { \mathrm { o t h e r w i s e } } \end{array} \right. ,
$$

and the corresponding entropy is

$$
\begin{array} { l } { { H _ { d } ( \tilde { x } ) = - \sum p ( \tilde { x } ) \log ( p ( \tilde { x } ) ) } } \\ { { \ = - \frac { 0 . 5 } { 1 . 5 - \Phi \left( \frac { c } { \sigma } \right) } \log \left( \frac { 0 . 5 } { 1 . 5 - \Phi \left( \frac { c } { \sigma } \right) } \right) } } \\ { { \ - \frac { 1 - \Phi \left( \frac { c } { \sigma } \right) } { 1 . 5 - \Phi \left( \frac { c } { \sigma } \right) } \log \left( \frac { 1 - \Phi \left( \frac { c } { \sigma } \right) } { 1 . 5 - \Phi \left( \frac { c } { \sigma } \right) } \right) } } \end{array}
$$

Since $\tilde { x }$ with probability $\begin{array} { r } { \tilde { q } = \Phi ( 0 ) + ( 1 - \Phi \left( \frac { c } { \sigma } \right) ) } \end{array}$ to be discrete variables and $1 - \tilde { q }$ to be continuous variables, similarly, the entropy of clipped feature $\tilde { x }$ can be derived as following

form according to (Politis 1991),

$$
\begin{array} { l } { { H ( \tilde { x } ) = - \tilde { q } \mathrm { i } \log \tilde { q } - ( 1 - \tilde { q } ) \mathrm { l o g } ( 1 - \tilde { q } ) } } \\ { { \ } } \\ { { \ } } \\ { { \ } } \\ { { \displaystyle \quad + \tilde { q } H _ { a } ( \tilde { x } ) + ( 1 - \tilde { q } ) H _ { c } ( \tilde { x } ; 0 , \sigma , 0 , c ) } } \\ { { \ } } \\ { { \ } } \\ { { \displaystyle \quad = - \left( 1 . 5 - \Phi \left( \frac { c } { \sigma } \right) \right) \log \left( 1 . 5 - \Phi \left( \frac { c } { \sigma } \right) \right) } } \\ { { \ } } \\ { { \ } } \\ { { \displaystyle \quad - \left( \Phi \left( \frac { c } { \sigma } \right) - 0 . 5 \right) \log \left( \Phi \left( \frac { c } { \sigma } \right) - 0 . 5 \right) } } \\ { { \ } } \\ { { \ } } \\ { { \displaystyle - 0 . 5 \log \left( \frac { 0 . 5 } { 1 . 5 - \Phi \left( \frac { c } { \sigma } \right) } \right) } } \\ { { \ } } \\ { { \ } } \\ { { \displaystyle - \left( 1 - \Phi \left( \frac { c } { \sigma } \right) \right) \log \left( \frac { 1 - \Phi \left( \frac { c } { \sigma } \right) } { 1 . 5 - \Phi \left( \frac { c } { \sigma } \right) } \right) } } \\ { { \ } } \\ { { \ } } \\ { { \displaystyle + \left( \Phi \left( \frac { c } { \sigma } \right) - 0 . 5 \right) H _ { c } ( \tilde { x } ; 0 , \sigma , 0 , c ) } } \end{array}
$$

Table 1: Entropy values calculated on Softmax probability before and after clipping, and their differences for HCE and LCE samples. FC makes HCE samples more uncertain. The experiment is conducted on ResNet-50 on CIFAR-10.   

<html><body><table><tr><td></td><td>Hsm(X)</td><td>Hsm(X)</td><td>△Hsm</td></tr><tr><td>HCE</td><td>0.0824</td><td>0.5723</td><td>0.4908</td></tr><tr><td>LCE</td><td>0.0032</td><td>0.1525</td><td>0.1493</td></tr></table></body></html>

Entropy Difference Then, the Shannon entropy difference between features before and after clipping is given by

$$
\begin{array} { l } { \displaystyle \Delta H = - \left( \Phi \left( \frac { c } { \sigma } \right) - 0 . 5 \right) \log \left( \Phi \left( \frac { c } { \sigma } \right) - 0 . 5 \right) } \\ { \displaystyle \qquad + 0 . 5 \log \left( 0 . 5 \right) } \\ { \displaystyle \qquad - \left( 1 - \Phi \left( \frac { c } { \sigma } \right) \right) \log \left( 1 - \Phi \left( \frac { c } { \sigma } \right) \right) } \\ { \displaystyle \qquad + \left( \Phi \left( \frac { c } { \sigma } \right) - 0 . 5 \right) H _ { c } ( \tilde { x } ; 0 , \sigma , 0 , c ) } \\ { \displaystyle \qquad + \frac { 1 } { 2 } \log ( \sqrt { \pi \epsilon \sigma } ) } \end{array}
$$

and $\Delta H$ is determined by the clipping threshold $c$ and $\sigma$ . We adopt the empirical result that $\sigma _ { H C E } > \sigma _ { L C E }$ , as discussed in previous section. Thus, we can derive the theorem,

Theorem 1. High calibration error samples suffer larger entropy difference compared to low calibration error samples after feature clipping.

$$
\Delta H _ { L C E } < \Delta H _ { H C E }
$$

The detailed proof of Theorem 1 is given in Appendix. To verify our conclusion, we further calculate the entropy difference at Softmax layer, which is consistent with our observation. Specifically, we numerically calculate the entropy based on Softmax probability before and after feature clipping. As shown in Table 1, both entropy of HCE samples $H _ { \mathrm { { s m } } } ^ { \mathrm { { \tilde { H } C E } } } ( X )$ and entropy of LCE samples $H _ { \mathrm { { s m } } } ^ { \mathrm { { L C E } } } ( X )$ are close to zero before clipping. However, after feature clipping, the entropy of HCE samples $H _ { \mathrm { { s m } } } ^ { \mathrm { { H C E } } } ( \tilde { X } )$ become much larger than entropy of LCE samples $H _ { \mathrm { { s m } } } ^ { \mathrm { { L C E } } } ( \tilde { X } )$ .

Table $2 { : } \mathbf { E C E } { \downarrow }$ before and after after feature clipping. ECE is measured as a percentage, with lower values indicating better calibration. ECE is evaluated for different post hoc calibration methods, both before (base) and after ( $^ +$ ours) feature clipping. The results are calculated with number of bins set as 15. The optimal $c$ is determined on the validation set, included in brackets.   

<html><body><table><tr><td>Dataset</td><td>Model</td><td colspan="2">Original Feature</td><td colspan="2">TS</td><td colspan="2">ETS</td><td colspan="2">PTS</td><td colspan="2">CTS</td><td colspan="2">GC</td></tr><tr><td></td><td></td><td>base</td><td>+ours(c)</td><td>base</td><td>(Guo et al. 2017) +ours</td><td>base</td><td>(Zhang et al.2020) +ours</td><td>base</td><td>(Tomani et al. 2022) +ours</td><td>base</td><td>(Frenkel et al. 2021) +ours</td><td>base</td><td>(Yang et al. 2024) +ours</td></tr><tr><td rowspan="3">CIFAR-10</td><td>ResNet-50</td><td>4.34</td><td>1.10(0.23) </td><td>1.39</td><td></td><td>1.37</td><td></td><td></td><td>1.25 </td><td>1.46</td><td>1.25 √</td><td>0.97</td><td>0.49</td></tr><tr><td>ResNet-110</td><td>4.41</td><td>0.96(0.23) </td><td>0.98</td><td>1.22 0.94√</td><td>0.98</td><td>1.22 0.94√</td><td>1.36 0.95</td><td>0.90 √</td><td>1.13</td><td>0.90 √</td><td>1.24</td><td>1.78 </td></tr><tr><td>DenseNet-121</td><td>4.51</td><td>1.05(0.45) </td><td>1.41</td><td>1.11 √</td><td>1.40</td><td>1.12 √</td><td>1.38</td><td>1.14√</td><td>1.44</td><td>1.14√</td><td>1.27</td><td>2.51 </td></tr><tr><td rowspan="3">CIFAR-100</td><td>ResNet-50</td><td>17.52</td><td>3.98(0.60)</td><td>5.72</td><td>4.26</td><td>5.68</td><td>4.29</td><td>5.64</td><td>4.37</td><td>6.03</td><td>4.37</td><td>3.43</td><td>1.70 √</td></tr><tr><td>ResNet-110</td><td>19.06</td><td>4.40(0.61) √</td><td>5.12</td><td>4.81√</td><td>5.10</td><td>4.81√</td><td>5.05</td><td>4.98</td><td>5.43</td><td>4.98</td><td>2.71</td><td>3.45 </td></tr><tr><td>DenseNet-121</td><td>20.99</td><td>3.28(1.40)</td><td>5.15</td><td>3.92</td><td>5.09</td><td>3.95√</td><td>5.06</td><td>4.04√</td><td>4.87</td><td>4.04√</td><td>2.84</td><td>1.75 √</td></tr><tr><td rowspan="4">ImageNet</td><td>ResNet-50</td><td>3.69</td><td>1.74(2.06) </td><td>2.08</td><td>1.64√</td><td>2.08</td><td>1.65√</td><td>2.11</td><td>1.63√</td><td>3.05</td><td>1.63√</td><td>1.30</td><td>1.00 √</td></tr><tr><td>DenseNet-121</td><td>6.66</td><td>3.08(3.45)</td><td>1.65</td><td>1.19 √</td><td>1.65</td><td>1.20√</td><td>1.61</td><td>1.20√</td><td>2.21</td><td>1.20√</td><td>2.67</td><td>0.63√</td></tr><tr><td>Wide-Resnet-50</td><td>5.52</td><td>2.52(3.05) </td><td>3.01</td><td>2.21</td><td>3.01</td><td>2.20√</td><td>3.00</td><td>2.18√</td><td>4.31</td><td>2.18√</td><td>3.01</td><td>0.88 </td></tr><tr><td>MobileNet-V2</td><td>2.72</td><td>1.36(1.73)</td><td>1.92</td><td>1.41 √</td><td>1.92</td><td>1.41√</td><td>1.93</td><td>1.44V</td><td>2.34</td><td>1.44√</td><td>1.81</td><td>0.50 √</td></tr></table></body></html>

$$
\Delta H _ { \mathrm { s m } } ^ { \mathrm { L C E } } \ll \Delta H _ { \mathrm { s m } } ^ { \mathrm { H C E } } .
$$

In other words, feature clipping successfully differentiated the handling of HCE and LCE samples, increase more entropy in HCE samples compared to LCE samples and make HCE samples more uncertain.

# Experiments

# Experiment Setup

Models and Datasets We evaluate our methods on various deep neural networks (DNNs), including ResNet (He et al. 2016), Wide-ResNet (Zagoruyko and Komodakis 2016), DenseNet (Huang et al. 2017), MobileNet (Howard et al. 2017), and ViT (Dosovitskiy et al. 2020), using the CIFAR10, CIFAR-100 (Krizhevsky, Hinton et al. 2009), and ImageNet-1K (Deng et al. 2009) datasets to assess the effectiveness of feature clipping. Pre-trained weights for post hoc calibration evaluation are provided by PyTorch.torchvision. Pre-trained weights trained by other train-time calibration methods are provided by Mukhoti et al. (2020).

Metrics We use the Expected Calibration Error (ECE) and accuracy as our primary metrics for evaluation. Additionally, we incorporate Adaptive ECE, a variant of ECE, which groups samples into bins of equal sizes to provide a balanced evaluation of calibration performance. For both ECE and Adaptive ECE, we use bin size at 15. We also measure the influence of calibration methods on prediction accuracy.

Comparison methods We compare our methods with several popular and state-of-the-art (SOTA) approaches. For post-hoc methods, we evaluate the widely used temperature scaling (TS) and other subsequent methods such as ETS (Zhang et al. 2020), PTS (Tomani et al. 2022), CTS (Frenkel et al. 2021), and a recently proposed SOTA calibration method called Group Calibration (Yang et al.

2024). For all TS-based methods, we determine the temperature by tuning the hyperparameter on the validation set to minimize the Negative Log Likelihood (NLL). To maintain consistency with TS, we also determine the optimal clipping threshold $c$ on the validation set by minimizing the NLL. For training-time calibration methods, we include training with Brier loss (Brier 1950), label smoothing (Mu¨ ller et al. 2019) with a smoothing factor of 0.05, FLSD-53 (Mukhoti et al. 2020) using the same $\gamma$ scheduling scheme as in (Mukhoti et al. 2020), and Dual Focal Loss (Tao et al. 2023b). Detailed settings are following the settings in (Mukhoti et al. 2020).

# Calibration Performance

To evaluate the performance, we assess feature clipping on both post-hoc methods and train-time calibration. We also find that post-hoc calibration methods hardly improve calibration performance on ViT and provide an empirical analysis to support this finding.

Compare with Post-Hoc Calibration Methods We compare the post-hoc calibration performance across multiple datasets and models, as shown in Table 2. FC consistently improves over the original features. With similar computational overhead and simplicity, FC outperforms TS in most cases, as seen when comparing columns 2 and 3. When combined with other post-hoc calibration methods, FC achieves state-of-the-art results. While Group Calibration also shows competitive results, it requires training an additional neural network based on features, resulting in higher computational overhead. Additionally, feature clipping is not compatible with Group Calibration in some cases, likely because Group Calibration separates groups based on features, while FC clips features, reducing information and making them less separable. Notably, in several instances, FC alone achieves the best performance, highlighting the potential of feature-based calibration. The simplicity of FC as a baseline method suggests significant opportunities for enhancement and optimization in future work. This demonstrates that even straightforward approaches like FC can yield substantial improvements, paving the way for more sophisticated featurebased calibration techniques. We also evaluate feature clipping using Adaptive ECE, a balanced version of ECE, with the results presented in the Appendix. FC demonstrates competitive results in this evaluation as well.

Table 3: ECE Calibration performance on Vision Transformer. Feature Clipping provides little but consistent improvement on Vision Transformer. Experiments are conducted on ViT-L-16 on ImageNet.   

<html><body><table><tr><td></td><td>Vanilla</td><td>TS</td><td>ETS</td><td>PTS</td><td>CTS</td></tr><tr><td>w/o FC</td><td>5.24</td><td>5.73</td><td>5.73</td><td>5.73</td><td>6.07</td></tr><tr><td>w/FC</td><td>5.04</td><td>5.59</td><td>5.60</td><td>5.60</td><td>5.60</td></tr></table></body></html>

![](images/8bf53627d2ec152343eb437263907fcc58ad0570f0f6111a17141ed416d7b599.jpg)  
Figure 3: Average absolute feature value of samples with high or low calibration error on Vision Transformer. We randomly select 50 feature units out of 2048 units. The high/low calibration error samples are selected as the wrongly/- correctly predicted samples with confidence larger than 0.8.

Table 4: Number of high confidence samples in ImageNet test set. The total number of samples is 50,000.   

<html><body><table><tr><td rowspan="2">Confidence</td><td colspan="2">ResNet-50</td><td colspan="2">ViT-L-16</td></tr><tr><td>Correct</td><td>Wrong</td><td>Correct</td><td>Wrong</td></tr><tr><td>>0.80</td><td>5921</td><td>471</td><td>6431</td><td>455</td></tr><tr><td>>0.90</td><td>5221</td><td>271</td><td>3429</td><td>92</td></tr><tr><td>>0.95</td><td>4526</td><td>161</td><td>44</td><td>0</td></tr><tr><td>>0.99</td><td>3173</td><td>53</td><td>0</td><td>0</td></tr></table></body></html>

Performance on Vision Transformer Although Vision Transformers do not end with a ReLU layer, the difference between HCE samples and LCE samples still exists, indicating that feature clipping can significantly influence HCE samples. As shown in Figure 3, we take the mean of the absolute value of features for better visualization. The HCE samples show higher average feature values than LCE samples. However, the improvement is not as pronounced compared to CNN models. We show the ECE performance of ViT-L-16 in Table 3. To investigate the reason, we count the number of overconfident samples, as shown in Table 4. The number of samples with confidence larger than 0.8 is similar for both CNN and ViT. However, for samples with confidence greater than 0.95, CNN has significantly more samples than ViT. When examining samples with confidence greater than 0.99, CNN still has many samples, while ViT has none within this confidence range. This indicates that transformers face far fewer overconfidence issues compared to CNN models. Theoretically, clipping feature values results in a loss of information, increasing entropy and mitigating overconfidence problems. Since transformers exhibit fewer overconfidence problems compared to CNNs, our method has less impact on transformer-based models compared to CNNs. However, the difference in feature values among samples still exists, indicating significant potential for future improvements in transformer models.

Compare with Train-time Calibration Methods We also compare the effectiveness of feature clipping when applied on top of various train-time calibration methods. Feature clipping consistently demonstrates improvement across all these train-time calibration methods and different models, as shown in Table 5. On simpler datasets like CIFAR10, models trained with “maximum-entropy penalty” methods such as focal loss and label smoothing adequately address the overconfidence issue. These methods effectively mitigate the overconfidence problem, leaving little room for additional improvement through feature clipping. However, when applied to more complex datasets like CIFAR-100, these training losses may not entirely resolve the overconfidence problem, providing an opportunity for feature clipping to further alleviate this issue and enhance calibration. Feature clipping’s ability to improve calibration in such scenarios underscores its potential as a valuable addition to existing training-time calibration methods.

# Ablation Study

Feature clipping is a straightforward method that causes samples to lose information. We are interested in understanding how this loss of information affects various aspects of model performance. Therefore, we study its influence on accuracy, how hyperparameter $c$ affect performance, and its performance when applied to different layers.

Does Feature Clipping Affect Accuracy? Although post-hoc methods do not change the model weights and can maintain prediction performance by keeping the original features, we are still interested in how the optimal clipping value affects accuracy. In Table 6, we compare the prediction accuracy of different train-time calibration methods with our feature clipping method. The baseline column indicates the model trained with cross-entropy loss using the original features, while the FC column shows the results of applying our feature clipping on the baseline. All models are trained with the same training recipe, which is included in the Appendix. We observe that feature clipping does not significantly affect accuracy. Despite reducing the information contained in the feature representation, FC sometimes even improves accuracy. On the other hand, some train-time methods, such as Brier loss, can negatively impact accuracy in most cases. This suggests that while these methods aim to improve calibration, they may inadvertently reduce the model’s ability to generalize, thereby lowering prediction accuracy. The detailed comparison of accuracy across different methods and datasets illustrates that our feature clipping method maintains competitive performance.

Table $5 \colon { \bf E C E \bot }$ before and after after feature clipping. ECE is measured as a percentage, with lower values indicating better calibration. ECE is evaluated for different train-time calibration methods, both before (base) and after $^ +$ ours) feature clipping. The results are calculated with number of bins set as 15. The optimal $c$ is determined on the validation set, included in brackets.   

<html><body><table><tr><td>Dataset</td><td>Model</td><td colspan="2">Cross Entropy</td><td colspan="2">Brier Loss</td><td colspan="2">LS-0.05</td><td colspan="2">FLSD-53</td><td colspan="2">Dual Focal Loss</td></tr><tr><td></td><td></td><td>base</td><td>+ours</td><td>base</td><td>(Brier 1950) +ours</td><td>base</td><td>(Muller et al. 2019) +ours</td><td>base</td><td>(Mukhoti et al. 2020) +ours</td><td>base</td><td>(Tao et al.2023b) +ours</td></tr><tr><td></td><td>ResNet-50</td><td>4.34</td><td>1.10(0.23) </td><td>1.80</td><td>1.49(0.98)</td><td>2.97</td><td>2.97(1.18)</td><td>1.55</td><td>1.50(0.75)</td><td>0.46</td><td>0.45(0.80)</td></tr><tr><td>CIFAR-10</td><td>ResNet-110</td><td>4.41</td><td>0.96(0.23) </td><td>2.57</td><td>2.34(1.15) </td><td>2.09</td><td>2.09(1.19) </td><td>1.88</td><td>1.28(0.51) </td><td>0.98</td><td>0.98(0.55) </td></tr><tr><td></td><td>DenseNet-121</td><td>4.51</td><td>1.05(0.45)</td><td>1.52</td><td>1.52(2.69)</td><td>1.87</td><td>1.87(2.05)</td><td>1.23</td><td>1.20(1.76)</td><td>0.57</td><td>0.57(1.96)</td></tr><tr><td></td><td>Wide-Resnet-26</td><td>3.24</td><td>1.35(0.28)</td><td>1.24</td><td>1.24(2.08)</td><td>4.25</td><td>4.25(1.74)</td><td>1.58</td><td>1.58(2.20) </td><td>0.81</td><td>0.81(2.12) </td></tr><tr><td></td><td>ResNet-50</td><td>17.52</td><td>3.98(0.60)</td><td>6.57</td><td>3.96(2.11) </td><td>7.82</td><td>7.82(3.67)</td><td>4.49</td><td>3.83(2.17)</td><td>1.08</td><td>1.01(2.23) </td></tr><tr><td></td><td>ResNet-110</td><td>19.06</td><td>4.40(0.61) </td><td>7.87</td><td>4.05(1.83)</td><td>11.04</td><td>6.83(1.03)</td><td>8.55</td><td>5.12(1.50)</td><td>2.90</td><td>2.53(1.51) </td></tr><tr><td>CIFAR-100</td><td>DenseNet-121</td><td>20.99</td><td>3.28(1.40) </td><td>5.22</td><td>3.50(4.11) </td><td>12.87</td><td>3.06(1.89) </td><td>3.70</td><td>2.96(4.02) </td><td>1.81</td><td>1.53(3.52)</td></tr><tr><td></td><td>Wide-Resnet-26</td><td>15.34</td><td>4.38(0.98)</td><td>4.34</td><td>3.11(2.24) </td><td>4.88</td><td>4.88(3.10)</td><td>3.02</td><td>1.90(2.47) </td><td>1.79</td><td>1.18(2.30) </td></tr></table></body></html>

Table 6: Accuracy for different train-time methods and feature clipping. Feature clipping does not impact prediction accuracy performance.   

<html><body><table><tr><td>Dataset</td><td>Model</td><td>Base</td><td>Brier</td><td>LS</td><td>Focal</td><td>FC</td></tr><tr><td rowspan="3">CIFAR-10</td><td>ResNet-50</td><td>95.05</td><td>95.0</td><td>94.71</td><td>95.02</td><td>94.93</td></tr><tr><td>ResNet-110</td><td>95.11</td><td>94.52</td><td>94.48</td><td>94.58</td><td>95.01</td></tr><tr><td>DenseNet-121</td><td>95.0</td><td>94.89</td><td>94.91</td><td>94.54</td><td>95.13</td></tr><tr><td rowspan="3">CIFAR-100</td><td>ResNet-50</td><td>76.7</td><td>76.61</td><td>76.57</td><td>76.78</td><td>76.74</td></tr><tr><td>ResNet-110</td><td>77.27</td><td>74.9</td><td>76.57</td><td>77.49</td><td>77.06</td></tr><tr><td>DenseNet-121</td><td>75.48</td><td>76.25</td><td>75.95</td><td>77.33</td><td>75.52</td></tr></table></body></html>

![](images/f718b70421442248c399afdc05321b5a9985455f7c626174f388449c05ecf234.jpg)  
Figure 4: Feature clipping at different value. Points to the right bottom cornor indicate better performance. The experiment is conducted on ResNet-50 on CIFAR-10.

How does clip threshold affect performance? To test how the clipping threshold influences performance, we clip the features of a ResNet-50 trained with cross-entropy loss on CIFAR-10 using different clipping thresholds. We plot the resulting performance in terms of ECE and accuracy, as shown in Figure 4. The red star indicates the performance of the original features. Generally, within a certain range (between 0.15 and 0.35 in this case), feature clipping does not significantly affect model accuracy. However, feature clipping can substantially influence calibration performance. For instance, a clipping value of 0.15 (point at the top left corner) achieves similar accuracy to the baseline but results in much worse calibration performance, with an ECE exceeding $2 5 \%$ . With the optimal clipping value, the model can achieve an ECE as low as 1.10, as shown in Table 2. We believe the reason feature clipping has a larger influence on calibration is that excessive clipping may significantly increase entropy, affecting correct predictions with high confidence. As a result, the model faces underconfidence, leading to a large ECE.

# Conclusion

In conclusion, our proposed feature clipping method demonstrates substantial improvements in model calibration across various datasets and models. FC effectively reduces overconfidence in predictions, enhancing calibration performance while maintaining accuracy. Despite its simplicity, FC achieves state-of-the-art calibration performance and provides a solid foundation for future research on featurebased calibration. However, there are several limitations and opportunities for improvement. The performance on transformer models, for instance, can be further improved. Future work should focus on developing more sophisticated methods, such as starting with a better threshold and conducting faster hyperparameter tuning. Additionally, exploring ways to find optimal clipping values using feature statistics and employing smoothed or adaptive thresholds instead of fixed ones are promising directions. These enhancements will potentially lead to even better calibration. Furthermore, investigating the impact of feature clipping on different neural network architectures and understanding its effects on various types of data can provide deeper insights. Our method serves as a strong baseline for feature-based calibration, and we believe that future developments can build upon this foundation to achieve even greater calibration improvements.