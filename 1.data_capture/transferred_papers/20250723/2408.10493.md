# Clustering by Mining Density Distributions and Splitting Manifold Structure

Zhichang $\mathbf { X } \mathbf { u } ^ { 1 }$ , Zhiguo Long1\*, Hua Meng2\*

1School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu 611756, China 2School of Mathematics, Southwest Jiaotong University, Chengdu 611756, China 2023201798@my.swjtu.edu.cn, {zhiguolong, menghua}@swjtu.edu.cn

# Abstract

Spectral clustering requires the time-consuming decomposition of the Laplacian matrix of the similarity graph, thus limiting its applicability to large datasets. To improve the efficiency of spectral clustering, a top-down approach was recently proposed, which first divides the data into several micro-clusters (granular-balls), then splits these microclusters when they are not “compact”, and finally uses these micro-clusters as nodes to construct a similarity graph for more efficient spectral clustering. However, this top-down approach is challenging to adapt to unevenly distributed or structurally complex data. This is because constructing micro-clusters as a rough ball struggles to capture the shape and structure of data in a local range, and the simplistic splitting rule that solely targets “compactness” is susceptible to noise and variations in data density and leads to microclusters with varying shapes, making it challenging to accurately measure the similarity between them. To resolve these issues and improve spectral clustering, this paper first proposes to start from local structures to obtain micro-clusters, such that the complex structural information inside local neighborhoods is well captured by them. Moreover, by noting that Euclidean distance is more suitable for convex sets, this paper further proposes a data splitting rule that couples local density and data manifold structures, so that the similarities of the obtained micro-clusters can be easily characterized. A novel similarity measure between micro-clusters is then proposed for the final spectral clustering. A series of experiments based on synthetic and real-world datasets demonstrate that the proposed method has better adaptability to structurally complex data than granular-ball based methods.

# Introduction

Clustering is an unsupervised learning method aiming to reveal the intrinsic distribution characteristics of data by dividing the dataset into several non-overlapping clusters. It has wide applications in various fields such as computer vision (Tron et al. 2017), language processing (Zhang, Wang, and Shang 2023), and bioinformatics (Cheng and Ma 2022).

Spectral lustering is a representative graph partition clustering algorithm, the core idea of which is to (approximately) minimize a cut loss of partitioning a similarity graph of data by removing some edges. It has attracted wide attention due to its exceptionally good performance in handling non-convex shaped clusters (von Luxburg 2007; Chen et al. 2011; He et al. 2019).

![](images/c3c5be9fbf3f4171ae12f2587bb07a4391da1960693b3eeed4caace5458c143a.jpg)  
Figure 1: Illustration of errors (red circles) in granular-balls.

Spectral clustering involves the spectral decomposition of the Laplacian matrix of a similarity graph, which has a prohibitively high complexity of $\mathcal { O } ( n ^ { \bar { 3 } } )$ $\mathbf { \bar { \rho } } _ { n }$ represents the number of nodes in the graph) for large datasets. To improve the scalability, researchers have considered performing approximate spectral decomposition (Vladymyrov and CarreiraPerpin˜a´n 2016), representing similarity between data approximately with anchors (Cai and Chen 2015; Huang et al. 2020), and constructing more sparse similarity graphs (Wu et al. 2018). These methods have different advantages and disadvantages, e.g., approximate decomposition needs to balance between efficiency and accuracy, and anchor-based ones are very sensitive to anchor numbers and positions. There is also research trying to fuse these directions (Yang et al. 2023).

Recently, Xie et al. (2023) proposed a new method called GBSC that progressively splits the data into micro-clusters (granular-balls) in a top-down manner, to obtain a coarsegrained representation of the original data, and then performs spectral clustering on balls representing several similar data points to reduce the size of the similarity graph. As each point is represented by a granular-ball, the overall structural information of data would be better captured than using the sampled anchors that is subject to missing the selection of some important anchors. This makes the granularball based approach very promising.

However, as it heavily relies on the quality of balls and the accuracy of the similarity measure between the balls, GBSC has two significant deficiencies for data of complex structures. Firstly, the splitting rule could produce low quality balls for complex data, because the rule only targets more “compact” balls and is based on a global view of data and a top-down manner. For example, in Fig. 1, when generating granular-balls from two clusters with significant density differences or manifold structure, some boundary points from different clusters are incorrectly grouped into the same granular-ball. Secondly, a granular-ball might contain data points distributed on a non-convex shape when the dataset is complex, and thus the Euclidean-based similarity between balls might no longer be appropriate.

To resolve these issues and accelerate spectral clustering, this paper proposes another approach to represent data in a more coarse granularity. It first tries to capture more detailed local structural information of data by constructing microclusters from an estimation of data densities. Then it follows a more sophisticated splitting rule that also considers the convexity of data (called manifold curvature) to improve the quality of micro-clusters. The more convex distribution of data in micro-clusters also makes the design of similarity measure more straightforward and thus the final spectral clustering more effective.

The contributions of this paper are as follows:

• We propose a coarse-grained data representation scheme that combines local density estimation and convex splitting of local manifolds that is exploited to improve spectral clustering.   
• Compared to granular-balls, the complex structures of data are better captured by extracting local density features to form a coarse-grained representation of data.   
• Manifold curvature is introduced to split micro-clusters into more convex ones, which results in easier characterization of the intrinsic similarities between microclusters.

The rest of the paper is organized as follows. We first introduce the related work and review how granular-balls are generated, and then discuss the motivation and the framework of our algorithm, followed by experimental evaluations of the algorithm.

# Related Work

The improvement and acceleration of spectral clustering has always been a focus in the field. Earlier, researchers explored numerical computation methods to accelerate spectral decomposition, e.g., the Nystro¨m method (Fowlkes et al. 2004) efficiently approximates the spectral decomposition of a large Laplacian matrix by sampling data points. Later research (Vladymyrov and Carreira-Perpin˜a´n 2016; Macgregor 2023) improved the sampling strategy and the representation ability to increase stability and reduce approximation error. However, these algorithms face the issue of balancing efficiency and approximation accuracy, and sampling-based schemes also have the problem of instability.

Another direction is to use anchors to characterize data, where the similarity between points are approximated using the similarity between points and anchors. Since the similarity graph between points and anchors is a bipartite graph, the spectral decomposition can be more efficient by working on a smaller matrix. For example, Cai and Chen (2015) performed K-means to obtain anchor points and constructed a sparsified bipartite graph between data points and these anchors by keeping only the connections of several nearest anchors for each point. Huang et al. (2020) further improved computational efficiency by first applying K-means on randomly sampled points to obtain anchors, then constructing a bipartite graph via fast approximate nearest neighbors, and finally performing transfer-cut on the bipartite graph for efficient spectral decomposition. There are also works (Gao et al. 2024; Nie et al. 2024) on combining anchors with the optimization process to increase efficiency.

Sparse similarity graphs can also be used to accelerate spectral clustering. For example, the SCRB method (Wu et al. 2018) used random binning features to generate inner products that approximate the similarity matrix of data, and employed singular value decomposition of large sparse matrices to improve the efficiency of spectral decomposition. The RESKM (Yang et al. 2023) framework tried to ensemble multiple strategies for more efficient spectral clustering.

Using micro-clusters to represent a group of data points was also a promising direction to reduce the size of data used for spectral clustering. The KASP method (Yan, Huang, and Jordan 2009) used K-means to group data points into microclusters and performed spectral clustering on the centers of these micro-clusters to reduce the size of similarity graph. The GBSC method (Xie et al. 2023) is a new and more sophisticated way to group data points into micro-clusters, by splitting the micro-clusters in a top-down manner to obtain a coarse-grained representation of the data, aiming for more compact micro-clusters. This method is combined with spectral clustering to achieve final clustering, while it can be combined with other clustering methods (Cheng et al. 2023) as well, demonstrating promising prospects. However, this top-down manner for a coarse-grained representation can result in micro-clusters of low quality which can distort the final clustering result. Therefore, this paper proposes a new micro-cluster construction strategy based on local structures, and also optimizes the splitting strategy to make the microclusters more convex and with high purity, to align with the needs of spectral clustering.

# Granular-Ball Generation

Given a dataset $X = \{ x _ { 1 } , . . . , x _ { n } \}$ $( x _ { i } \in \mathbb { R } ^ { d } )$ , the target of GBSC (Xie et al. 2023) is to use a set of granular-balls as micro-clusters to cover the dataset, such that each data point belongs to a single granular-ball. A granular-ball is just an $d$ dimensional ball while the radius of the ball can vary from each other.

Specifically, suppose $G B _ { j }$ is a granular-ball covering the $m$ data points $\{ x _ { j _ { 1 } } , \dotsc , x _ { j _ { m } } \} \subseteq X$ , then the center $c _ { j }$ and the radius of $G B _ { j }$ are determined as $\begin{array} { r } { c _ { j } = \frac { 1 } { m } \sum _ { s = 1 } ^ { m } x _ { j _ { s } } } \end{array}$ and $r _ { j } = \mathrm { m a x } _ { s } ( \| x _ { j _ { s } } - c _ { j } \| )$ , where $| | \cdot | |$ denotes the $\ell _ { 2 }$ norm. Note that if the radius of the granular-ball is large then the “granularity” is coarse and the clustering on the balls would be fast while much structural information would be lost; otherwise, the “granularity” is fine but clustering would be slow. Therefore, generating granular-balls needs to balance granularity and the quality of the balls.

![](images/ac9093988b721594edcb07fe8c82d5bd82dcfcc8471976bc5532efad7687a147.jpg)  
Figure 2: The framework of the proposed MDMSC algorithm.

In GBSC, the quality of a granular-ball $G B _ { j }$ is defined as

$$
D M _ { j } = \frac { 1 } { m } \sum _ { s = 1 } ^ { m } \| x _ { j _ { s } } - c _ { j } \| .
$$

$D M _ { j }$ actually measures the “compactness” of the points within $G B _ { j }$ , where a smaller value of $D M _ { j }$ means that the distance between the points is mostly small.

The generation of granular-balls in a top-down manner works as follows. First, a granular-ball $G B _ { A }$ covering the entire dataset is generated. Then, two farthest points $p _ { 1 }$ and $p _ { 2 }$ are selected, and the points in $G B _ { A }$ are assigned to $p _ { 1 }$ if they are closer to $p _ { 1 }$ than to $p _ { 2 }$ . Two children balls $G B _ { A _ { 1 } }$ and $G B _ { A _ { 2 } }$ are then generated using the two subsets of points.

The core splitting strategy in GBSC is that if the weighted quality of the children balls is higher than that of the parent ball. Suppose $G B _ { j _ { 1 } }$ and $G B _ { j _ { 2 } }$ are the two children balls of $G B _ { j }$ , covering $m _ { 1 }$ and $m _ { 2 }$ points, respectively. Then the weighted quality of the children balls is

$$
D M _ { w e i g h t } = \frac { m _ { 1 } } { m } D M _ { j 1 } + \frac { m _ { 2 } } { m } D M _ { j 2 } .
$$

For corner cases, GBSC also provides other splitting strategies including the restrictions on the number of points and the radius of a ball (see (Xie et al. 2023)). The above splitting process is repeated to generate final granular-balls until no more split can happen.

# Algorithm

The splitting scheme of GBSC is performed in a top-down manner, and it does not consider local information, which can lead to incorrect splitting of local structures and thus affect the clustering performance.

To address these issues, we propose an improved algorithm MDMSC that initially partitions the dataset into multiple pseudo-clusters, instead of granular-balls, based on the density distribution of the data points, and then further split the pseudo-clusters based on structural characteristics. This approach aims to better capture the local features of the dataset and improve the clustering performance on complex datasets.

MDMSC consists of three stages: constructing pseudoclusters as micro-clusters, splitting pseudo-clusters, and performing spectral clustering based on the similarity of the final pseudo-clusters. The algorithm framework is illustrated in Fig. 2.

# Constructing Pseudo-Clusters

For each point $x _ { i }$ in a dataset $X \in P ^ { n \times d }$ , let $N _ { k } ( x _ { i } )$ be the set of the $k$ nearest neighbors of $x _ { i }$ (excluding $x _ { i }$ itself).

Before defining pseudo-clusters, we need to first define density and leader.

Definition 1. The density $\rho _ { k } ( x _ { i } )$ of a point $x _ { i }$ , is defined as

$$
\rho _ { k } ( x _ { i } ) = \sum _ { j = 1 } ^ { k } \exp ( - d i s t _ { i j } ^ { 2 } ) ,
$$

where $d i s t _ { i j }$ refers to the Euclidean distance between point $x _ { i }$ and its neighbor $x _ { j }$ . The density is measured by the sum of Gaussian kernels of the Euclidean distances, reflecting the compactness of the local structure around the point.

The leader of each point $x _ { i }$ , denoted as leader $( x _ { i } )$ , is defined as the nearest higher-density point of $x _ { i }$ .

Definition 2. The leader of a point $x _ { i }$ is

$$
\mathsf { l e a d e r } ( x _ { i } ) = \left\{ \begin{array} { l l } { \underset { x _ { j } \in \mathcal { H } ( x _ { i } ) } { \mathrm { a r g m i n } } ~ d i s t _ { i j } } & { \mathrm { i f } ~ \mathcal { H } ( x _ { i } ) \neq \emptyset } \\ { \mathrm { N o n e } } & { \mathrm { o t h e r w i s e } } \end{array} \right.
$$

where the set ${ \mathcal H } ( x _ { i } ) ~ = ~ \{ x _ { j } ~ \mid ~ x _ { j } ~ \in ~ N _ { k } ( x _ { i } ) , \rho _ { k } ( x _ { j } ) ~ >$ $\rho _ { k } ( x _ { i } ) \}$ . Points without a leader are called core points, and the set of core points is denoted as core $\mathbf { \Sigma } = \mathbf { \Sigma } \{ x _ { i } \mid $ leader $( x _ { i } ) = \mathrm { N o n e } \}$ .

Definition 3 (Pseudo-cluster). Let $G \ : = \ : ( X , E )$ be a directed graph, where $( x _ { i } , x _ { j } ) \in E$ if $x _ { j }$ is the leader of $x _ { i }$ . A pseudo-cluster is a connected component of $G$ .

![](images/92099c025ccdbdcf18bf5b56b5bb98e8e1020f299fa827e1b8223bca1ec81e25.jpg)  
Figure 3: Illustration of pseudo-clusters and their complex structures.

# Algorithm 1: Constructing pseudo-clusters

Require: Dataset X, and the number of nearest neighbors $k$ .   
Ensure: Pseudo-clusters pseudo clusters. 1: for each $x _ { i } \in X$ do 2: Calculate $N _ { k } ( x _ { i } )$ ; 3: Calculate $\rho _ { k } ( x _ { i } )$ ;   
4: end for 5: core $ \emptyset$ ; 6: for each $x _ { i } \in X$ do 7: Calculate leader $( x _ { i } )$ ; 8: if leader $( x _ { i } ) =$ None then 9: $\mathsf { c o r e \gets c o r e \cup \{ x _ { i } \} }$ ;   
10: end if   
11: Connect $x _ { i }$ and leader $( x _ { i } )$ ;   
12: end for   
13: Identify pseudo clusters as connected components;   
14: return pseudo clusters

Pseudo-clusters are actually a local tree structure, where its root is a core point. By considering density distributions, a local tree structure has high purity and can thus better reflect local structures of data than granular-balls.

Intuitively, by connecting each point $x _ { i }$ to its leader leader $( x _ { i } )$ , multiple disjoint pseudo-clusters are formed, with each pseudo-cluster being defined by the unique core point within the pseudo-cluster. Algorithm 1 shows the steps of constructing pseudo-clusters.

Fig. 3 illustrates the pseudo-clusters of Jain with $k = 1 0$ and Spiral with $k = 4$ . It can be seen that pseudo-clusters better reflect the local characteristics of the dataset, and the problem of a granular-ball in Fig. 1 containing points from different clusters has been rectified.

# Splitting Pseudo-Clusters

Although the pseudo-clusters reflect the basic distribution of the data, these pseudo-clusters may be too coarse for the entire dataset. Additionally, pseudo-clusters have various shapes and structures, many of which are non-convex, which makes measuring the similarity between pseudo-clusters difficult.

On the left side of Fig. 3, the pseudo-clusters of numbers 1, 2, 3, and 4 exhibit non-convex structures and complex shapes, and the distance between these pseudo-clusters and others can not be easily measured with Euclidean based distances. For example, for pseudo-cluster 2, although its core point is close to pseudo-cluster 1, the points in the left part of it are far away. The situation is similar on the other side.

To address this issue, we need to split the pseudo-clusters into simpler convex structures. We introduce the measurement of manifold curvature to better determine if a pseudocluster is too “curved”.

Definition 4 (Manifold curvature). Suppose $p _ { t }$ is a pseudocluster, and $T ( p _ { t } )$ is a minimum spanning tree (MST) of the complete weighted graph for the points in $p _ { t }$ , where the weights of the edges are the Euclidean distances between the points. The manifold curvature of a pseudo-cluster $p _ { t }$ is

$$
M C ( p _ { t } ) = \frac { p a t h . d i s t _ { i j } } { d i s t _ { i j } } ~ ( x _ { i } , x _ { j } = \mathsf { e n d p o i n t s } ( p _ { t } ) ) ,
$$

where path dist is the shortest path distance between points on $T ( p _ { t } )$ , and endpoints $\begin{array} { r l } { ( p _ { t } ) } & { { } = } \end{array}$ $\operatorname { a r g m a x } _ { x _ { i } , x _ { j } \in p _ { t } } p a t h \_ d i s t _ { i j }$ .

Intuitively, if the geodesic distance (the shortest distance on a manifold) equals the Euclidean distance, then the set is convex, and non-convex otherwise. Here, we use the shortest path distance in MST to approximate the geodesic distance, and the ratio of them to measure curvature.

Thus, we define the manifold curvature as the ratio of the shortest path distance to the Euclidean distance between the endpoints. The larger the ratio is, the more “curved” the pseudo-cluster will be. When the ratio approaches 1, the data will be nearly convex.

By splitting pseudo-clusters under the measurement of manifold curvature, we can ensure that the pseudoclusters have stronger convexity, making the consideration of pseudo-cluster compactness more reasonable and also beneficial for capturing the intrinsic similarities.

To determine whether a pseudo-cluster $p _ { t }$ should be split, we consider both the manifold curvature and the compactness of the original pseudo-cluster and its child pseudoclusters according to Eq. 1 and Eq. 2. In particular, if $M C ( p _ { t } ) \geq \lambda$ and $D M _ { w e i g h t } < D M ( p _ { t } )$ , then $p _ { t }$ will be split. Because geodesic distance is estimated by graph distance, the two distances would be equal only with sufficient sampling; otherwise, graph distance is larger than the true geodesic distance. Thus, we set a threshold of 1.5 for the $\lambda$ . In order to avoid too small pseudo-clusters, we also require $p _ { t }$ should contain a minimum number $\beta$ of points, as in (Xie et al. 2023).

When a pseudo-cluster needs to be split, its two endpoints endpoints $\left( p _ { t } \right)$ will be used to construct the new child pseudo-clusters, where the other points in $p _ { t }$ are assigned to the new pseudo-clusters based on proximity to the endpoints. The process will be repeated until none of the pseudo-clusters can be split.

# Clustering Pseudo-Clusters

After obtaining the final pseudo-clusters, the next step is to establish similarity between pseudo-clusters and ultimately

Require: Dataset $X$ , the number of nearest neighbors $k$ , the manifold curvature threshold $\lambda$ , and the minimum size of pseudo-cluster $\beta$ .

Ensure: Final clustering labels.

1: Get pseudo clusters by Algorithm 1;   
2: for each $p _ { t } \in$ pseudo clusters do   
3: Generate a complete undirected graph $G ( p _ { t } )$ ;   
4: Generate an MST $T ( p _ { t } )$ from $G ( p _ { t } )$ ;   
5: Calculate $D M _ { w e i g h t }$ , $\dot { D } M ( p _ { t } )$ and $M C ( p _ { t } )$ ;   
6: end for   
7: while $\exists p _ { t }$ s.t. $M C ( p _ { t } ) > \lambda$ and $D M _ { w e i g h t } < D M ( p _ { t } )$   
and $| p _ { t } | > \beta$ do   
8: Split $p _ { t }$ into $p _ { t _ { 1 } }$ and $p _ { t _ { 2 } }$ and add them to   
pseudo clusters;   
9: Remove $p _ { t }$ from pseudo clusters;

# 10: end while

Table 1: Real-world datasets.   

<html><body><table><tr><td>Dataset</td><td></td><td>#Instance #Attributes</td><td>#Clusters</td></tr><tr><td>border</td><td>840</td><td>892</td><td>3</td></tr><tr><td>mfea-fac</td><td>2000</td><td>216</td><td>10</td></tr><tr><td>Kdd9</td><td>1280</td><td>41</td><td>3</td></tr><tr><td>landsatEW</td><td>6435</td><td>36</td><td>6</td></tr><tr><td>balance-scale</td><td>625</td><td>4</td><td>3</td></tr><tr><td>pengleukEW</td><td>72</td><td>7070</td><td>2</td></tr><tr><td>Pendigits</td><td>10992</td><td>16</td><td>10</td></tr><tr><td>energy-y2</td><td>766</td><td>8</td><td>3</td></tr><tr><td>optical_test</td><td>1797</td><td>62</td><td>10</td></tr><tr><td>soybean_test</td><td>376</td><td>35</td><td>18</td></tr><tr><td>car</td><td>1728</td><td>6</td><td>4</td></tr><tr><td>semeionEW</td><td>1593</td><td>256</td><td>10</td></tr><tr><td>vote</td><td>435</td><td>16</td><td>2</td></tr></table></body></html>

11: Calculate similarity matrix $S$ ;

12: Perform spectral clustering on the $S$ to obtain final clustering results.

perform clustering through graph partitioning Here, we employ spectral clustering on pseudo-clusters.

Since the pseudo-clusters are approximately convex now, it is straightforward to use Euclidean distance to evaluate their similarity, as follows.

Definition 5 (Similarity). The similarity of two pseudoclusters $p _ { i }$ and $p _ { j }$ is

$$
S ( p _ { i } , p _ { j } ) = \frac { | \mathsf { S N N } ( p _ { i } , p _ { j } ) | } { 1 + c _ { - } d i s t ( p _ { i } , p _ { j } ) } ,
$$

where ${ \mathsf { S N N } } ( p _ { i } , p _ { j } ) = ( \cup _ { x \in p _ { i } } N _ { k } ( x ) ) \cap ( \cup _ { x \in p _ { j } } N _ { k } ( x ) )$ is the shared nearest neighbors of $p _ { i }$ and $p _ { j }$ , and $c _ { - } d i s t ( p _ { i } , p _ { j } )$ is the Euclidean distance between the centroids of $p _ { i }$ and $p _ { j }$ . A centroid of $p _ { i }$ is $\textstyle { \frac { 1 } { m _ { i } } } \sum _ { x \in p _ { i } } x$ , which is not necessarily a core point.

Subsequently, we perform spectral clustering on the similarity matrix of the pseudo-clusters. Points within the same pseudo-cluster will be assigned the same cluster label. The full steps of the proposed algorithm MDMSC is given in Algorithm 2.

# Time Complexity

Suppose the dataset $X$ has $n$ samples with the dimensionality of $d$ , the final number of pseudo-clusters is $m$ , and the number of nearest neighbors is $k$ . In Algorithm 1, the time complexity for finding $\mathbf { k }$ -nearest neighbors using the KDtree method is $O ( ( d \bar { + } k ) n \log n )$ , the time complexity for calculating density is $\mathcal { O } ( k n )$ , and the time complexity for finding the leader points is ${ \mathcal { O } } ( k n )$ .

In Algorithm 2, suppose the number of samples in each pseudo-cluster is $n _ { i }$ , for each pseudo-cluster, the time complexity for creating a complete graph is $\mathcal { O } ( n _ { i } ^ { 2 } )$ , for obtaining the minimum spanning tree from the complete graph using Kruskal’s algorithm is $\mathcal { O } ( n _ { i } ^ { 2 } \log { n _ { i } } )$ , and for calculating the shortest path distances between any two points in the pseudo-cluster using Dijkstra’s algorithm is $\mathcal { O } ( n _ { i } ^ { 2 } \log { n _ { i } } )$ . The total time complexity for splitting all existing pseudoclusters once is $\begin{array} { r } { \sum _ { i } \mathcal { O } ( n _ { i } ^ { 2 } \log { n _ { i } } ) } \end{array}$ , where $\textstyle \sum _ { i } n _ { i } \ { \bar { = } } ^ { \mathbf { \bar { n } } }$ . The time complexity for spectral clustering is $\mathcal { O } ( m ^ { 3 } )$ .

Actually, $\begin{array} { r } { \sum _ { i } \mathcal { O } ( n _ { i } ^ { 2 } \log n _ { i } ) } \end{array}$ is bounded by $\mathcal { O } ( n n _ { * } \log n _ { * } )$ , where $n _ { * }$ is the maximum of $n _ { i }$ , as shown in the following theorem (see $\mathrm { \Delta X u }$ , Long, and Meng 2024) for proof). Therefore, the overall time complexity of Algorithm 2 is thus $\mathcal { O } ( n n _ { * } ( \log n _ { * } ) + m ^ { 3 } )$ .

Theorem 1. Suppose there are $l$ pseudo-clusters. Let $n =$ $\textstyle \sum _ { i = 1 } ^ { l } n _ { i }$ , where $n _ { i } \geq 1$ is the number of points in a pseudocluster $p _ { i }$ . Then $\begin{array} { r } { \sum _ { i } n _ { i } ^ { 2 } \log n _ { i } = \mathcal { O } ( n n _ { * } \log n _ { * } ) } \end{array}$ , where $n _ { * }$ is the maximum of $n _ { i }$ .

# Experiments

# Experimental Setup

We used three evaluation metrics: ARI (Steinley 2004), NMI $\mathrm { \Delta X u }$ , Liu, and Gong 2003), and ACC (Yang et al. 2010) for clustering analysis. Since the proposed algorithm is based on local density peaks and pseudo-cluster splitting followed by spectral clustering, we selected the following eight comparison algorithms: GBSC (Xie et al. 2023), GBDP (Cheng et al. 2023), LDP-MST (Cheng et al. 2021), USPEC (Huang et al. 2020), spectral clustering (SC) (Shi and Malik 2000), DPC (Rodriguez and Laio 2014), DEMOS (Guan et al. 2023), and DPC-DBFN (Lotfi, Moradi, and Beigy 2020).

For MDMSC, we searched for $k$ in the range of 2 to 50 and for $\beta$ in $\{ 8 , 1 6 \}$ , and the threshold $\lambda$ was set to 1.5. The implementation details of other algorithms are provided in the extended version ( $\mathrm { \Delta X u }$ , Long, and Meng 2024). The implementation of MDMSC can be found at https://github. com/SWJTU-ML/MDMSC.

In terms of datasets, we used 4 synthetic datasets and 13 real-world datasets (Table 1). All datasets were normalized, and the results were averaged over 10 runs.

# Visualizations on Synthetic Datasets

We present visualizations of several datasets with obvious manifold structures in Fig. 4, where MDMSC performs well, while GBSC and GB-DP misclassify points within the same cluster. For example, GBSC misclassifies many isolated points into incorrect clusters due to the granular-ball splitting process and its simplistic similarity measure. On the other hand, MDMSC, by leveraging local density features and considering manifold curvature, effectively captures complex shapes and manifold structures.

![](images/7823a09d996df07755ff55879f82835f415c3cf9870793c4f78733c3a662276c.jpg)  
Figure 4: Visualizations on synthetic datasets.

# Results on Real-World Datasets

Table 2 shows the results (bold means best) of our algorithm and the other eight comparison algorithms on realworld datasets. MDMSC achieves the best performance on most datasets. For example, on the high-dimensional dataset pengleukEW, MDMSC significantly outperforms most comparison algorithms. We attribute this to the consideration of manifold curvature, allowing it to effectively handle complex high-dimensional datasets. Additionally, on the largescale dataset Pendigits, our algorithm also shows superior performance, demonstrating its capability to effectively handle large-scale datasets by partitioning pseudo-clusters. Although our algorithm does not always achieve the best results on a few datasets like landsatEW and soybean test, its performance is very close to the optimal ones. This indicates that our algorithm possesses strong adaptability and effectiveness across a wide range of application scenarios.

We also performed Friedman test and subsequent Nemenyi test on the ACC results. Friedman test showed that there exists significant differences $( p < 0 . 0 5 )$ between the results and Nemenyi test confirmed that our algorithm significantly outperforms all of the other algorithms: $C D =$ 2.10 and the rank differences of MDMSC with other algorithms are all larger than 2.10.

# Ablation Study

To demonstrate the effectiveness of each component of our algorithm, we conducted ablation studies on 7 real-world datasets. The best results are presented in Table 3 by tuning hyperparameters. The columns $a , b$ , and $c$ represent the following different settings, respectively:

![](images/77751f7ad1d0fa2a04428b04d52544805382bf3c8a098d6e3d27ff02a3bea4f0.jpg)  
Figure 5: Impacts of $k$ , $\lambda$ and $\beta$ on clustering performance.

![](images/8da4200d914e712bd4d937cd3d2af5169d032ffbb7496111ac26f696c29e2739.jpg)  
Figure 6: Time and ACC comparison.

$a$ : Directly uses the whole dataset as the initial pseudocluster, without using Algorithm 1. $b$ : Does not further split the pseudo-clusters. $c$ : Only uses compactness to split the pseudo-clusters.

The results indicate that the performance of the algorithm decreases when a specific component is removed. Setting $a$ demonstrates that extracting local density features helps characterize the data distribution. Settings $b$ and $c$ show that considering manifold curvature is beneficial for the quality of pseudo-clusters.

# Robustness Analysis

Fig. 5 demonstrates the ARI variations of MDMSC under different hyperparameter settings on multiple datasets, where $k$ was set in [3, 50], $\lambda$ in [1.0, 3.0] with a step size of 0.2, and $\beta$ in [8, 16]. From the results in Fig. 5, it is evident that while the algorithm exhibits some fluctuations with respect to $k$ (larger fluctuations with smaller $k$ and stablized as $k$ increases), its performance remains relatively stable when varying $\lambda$ and $\beta$ . This suggests that MDMSC has a certain degree of robustness against changes in these parameters.

# Running Time

Fig. 6 compares the running time and ACC of our algorithm with GBSC. The proposed MDMSC has shorter run time than GBSC on 7 out of 13 datasets and similar run time on most of the other datasets, and on all of these datasets, MDMSC has higher ACC than GBSC. The reason for the slower cases is that we make use of more complex microclusters to represent data and the splitting rule is also more sophisticated, and it is actually worthy in most of the cases.

Table 2: Results on real-world datasets $( \% )$ .   

<html><body><table><tr><td>Dataset</td><td></td><td>MDMSC</td><td>GBSC</td><td>GB-DP LDP-MST</td><td></td><td>USPEC</td><td>SC</td><td>DPC</td><td>DEMOS DPC-DBFN</td><td></td></tr><tr><td>border</td><td>ARI</td><td>19.54</td><td>0.01</td><td>16.70</td><td>1.18</td><td>17.35</td><td>17.48</td><td>8.73</td><td>14.20</td><td>2.83</td></tr><tr><td rowspan="5">mfea-fac</td><td>NMI</td><td>18.59</td><td>0.34</td><td>15.17</td><td>1.89</td><td>16.15</td><td>15.58</td><td>10.92</td><td>14.66</td><td>6.10</td></tr><tr><td>ACC</td><td>56.67</td><td>44.39</td><td>49.76</td><td>45.00</td><td>53.46</td><td>51.90</td><td>49.52</td><td>52.50</td><td>45.59</td></tr><tr><td>ARI</td><td>86.28</td><td>24.92</td><td>39.78</td><td>61.50</td><td>84.57</td><td>85.57</td><td>59.85</td><td>75.58</td><td>48.80</td></tr><tr><td>NMI</td><td>87.54</td><td>42.58</td><td>58.79</td><td>72.41</td><td>86.39</td><td>87.31</td><td>74.43</td><td>82.57</td><td>69.11</td></tr><tr><td>ACC</td><td>93.45</td><td>41.30</td><td>61.90</td><td>72.15</td><td>92.56</td><td>93.00</td><td>65.85</td><td>87.30</td><td>59.10</td></tr><tr><td rowspan="4">Kdd9</td><td>ARI</td><td>96.53</td><td>5.67</td><td>84.93</td><td>86.08</td><td>70.71</td><td>52.16</td><td>0.05</td><td>NA</td><td>3.02</td></tr><tr><td>NMI</td><td>96.09</td><td>7.96</td><td>85.98</td><td>85.79</td><td>73.08</td><td>62.55</td><td>0.36</td><td>NA</td><td>13.75</td></tr><tr><td>ACC</td><td>97.97</td><td>41.67</td><td>94.53</td><td>95.00</td><td>86.55</td><td>73.85 38.83</td><td></td><td>NA</td><td>46.56</td></tr><tr><td>ARI</td><td>56.45</td><td>49.17</td><td>34.48</td><td>50.01</td><td>58.36</td><td>47.23 46.18</td><td></td><td>NA</td><td>4.68</td></tr><tr><td rowspan="4">landsatEW balance-scale ARI</td><td>NMI</td><td>63.50</td><td>56.53</td><td>47.45</td><td>59.83</td><td>65.16</td><td>60.42 56.06</td><td></td><td>NA</td><td>10.40</td></tr><tr><td>ACC</td><td>70.16</td><td>67.61</td><td>55.63</td><td>64.21</td><td>70.50</td><td>63.66 71.75</td><td></td><td>NA</td><td>30.58</td></tr><tr><td></td><td>24.60</td><td>5.25</td><td>10.31</td><td>0.91</td><td>11.10</td><td>13.57 15.07</td><td></td><td>NA</td><td>11.14</td></tr><tr><td>NMI</td><td>22.66</td><td>5.44</td><td>8.91</td><td>6.63</td><td>9.33</td><td>12.15 11.73</td><td></td><td>NA</td><td>10.42</td></tr><tr><td rowspan="4">pengleukEW</td><td>ACC</td><td>60.16</td><td>51.70</td><td>52.48</td><td>49.92</td><td>51.94</td><td>54.24 54.40</td><td></td><td>NA</td><td>54.40</td></tr><tr><td>ARI</td><td>35.85</td><td>2.96</td><td>23.79</td><td>32.90</td><td>29.75</td><td>26.63</td><td>10.52</td><td>26.00</td><td>15.95</td></tr><tr><td>NMI</td><td>25.07</td><td>5.95</td><td>17.13</td><td>24.06</td><td>21.96</td><td>18.95 14.39</td><td></td><td>16.98</td><td>10.73</td></tr><tr><td>ACC</td><td>80.56</td><td>58.61</td><td>75.00</td><td>79.16</td><td>77.78</td><td>76.39 70.83</td><td></td><td>76.38</td><td>70.83</td></tr><tr><td rowspan="4">Pendigits</td><td>ARI</td><td>77.81</td><td>42.62</td><td>51.91</td><td>70.11</td><td>71.05</td><td>76.24 63.50</td><td></td><td>70.53</td><td>49.91</td></tr><tr><td>NMI</td><td>84.85</td><td>59.72</td><td>67.51</td><td>81.66</td><td>81.52</td><td>83.73 75.41</td><td></td><td>80.75</td><td>67.67</td></tr><tr><td>ACC</td><td>88.08</td><td>57.17</td><td>63.05</td><td>78.02</td><td>82.98</td><td>87.17 75.63</td><td></td><td>84.18</td><td>64.55</td></tr><tr><td>ARI</td><td>70.65</td><td>4.57</td><td>69.17</td><td>70.58</td><td>35.24</td><td>70.58 55.17</td><td></td><td>45.58</td><td>65.29</td></tr><tr><td>energy-y2</td><td>NMI</td><td>66.44</td><td>5.55</td><td>68.97</td><td>66.40</td><td>47.02</td><td>66.40 62.13</td><td></td><td>58.11</td><td>66.67</td></tr><tr><td rowspan="4">optical_test</td><td>ACC</td><td>80.86</td><td>51.07</td><td>74.09</td><td>80.73</td><td>55.95</td><td>80.73 65.76</td><td></td><td>49.86</td><td>71.61</td></tr><tr><td>ARI</td><td>84.08</td><td>28.70</td><td>32.52</td><td>56.37</td><td>80.84</td><td>81.47 72.25</td><td></td><td>82.27</td><td>-0.06</td></tr><tr><td>NMI</td><td>90.13</td><td>51.58</td><td>59.44</td><td>75.04</td><td>88.06</td><td>89.91</td><td>83.41</td><td>87.90</td><td></td></tr><tr><td>ACC</td><td>89.43</td><td>52.08</td><td>55.15</td><td>65.77</td><td>86.69</td><td>87.82 78.69</td><td></td><td>89.14</td><td>0.83</td></tr><tr><td rowspan="4">soybean_test</td><td>ARI</td><td>43.04</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>10.51</td></tr><tr><td></td><td></td><td>29.26</td><td>31.26</td><td>39.89</td><td>39.11</td><td>46.22 32.72</td><td></td><td>31.30</td><td>35.13</td></tr><tr><td>NMI ACC</td><td>75.08 64.10</td><td>59.03 44.55</td><td>65.55 42.82</td><td>67.21 53.98</td><td>74.81</td><td>69.46 64.65</td><td></td><td>59.82</td><td>58.29</td></tr><tr><td>ARI</td><td>40.44</td><td>-3.08</td><td>4.90</td><td></td><td>60.61</td><td>56.41</td><td>46.01</td><td>44.95</td><td>50.80</td></tr><tr><td rowspan="4">car</td><td>NMI</td><td></td><td></td><td></td><td>11.40</td><td>17.19</td><td>17.79 15.90</td><td></td><td>9.16</td><td>14.46</td></tr><tr><td>ACC</td><td>28.51</td><td>5.28</td><td>11.13</td><td>32.40</td><td>24.57</td><td>25.04 30.47</td><td></td><td>15.73</td><td>10.36</td></tr><tr><td></td><td>69.44</td><td>57.25</td><td>35.94</td><td>55.55</td><td>47.39</td><td>47.96 51.56</td><td></td><td>46.81</td><td>61.57</td></tr><tr><td>ARI</td><td>52.62</td><td>0.53</td><td>23.28</td><td>26.79</td><td>48.34</td><td>44.81 19.63</td><td></td><td>33.19</td><td>26.03</td></tr><tr><td rowspan="4">vote</td><td>NMI</td><td>67.45</td><td>5.85</td><td>42.15</td><td>52.15</td><td>64.66</td><td>64.29 35.20</td><td></td><td>55.53</td><td>42.59</td></tr><tr><td>ACC</td><td>68.30</td><td>13.21</td><td>41.68</td><td>47.70</td><td>62.82 0.37</td><td>58.59 36.97 55.72 53.00</td><td></td><td>45.26 NA</td><td>39.30 45.07</td></tr><tr><td>ARI NMI</td><td>57.10 49.93</td><td>-0.83 8.32</td><td>53.67 45.98</td><td>20.57 24.44</td></table></body></html>

Table 3: Ablation study results $( \% )$ .   

<html><body><table><tr><td>Dataset</td><td>MDMSC</td><td>a</td><td>b</td><td>C</td></tr><tr><td>mfea-fac</td><td>86.28</td><td>71.28</td><td>71.72</td><td>84.40</td></tr><tr><td>Kdd9</td><td>96.53</td><td>84.93</td><td>84.93</td><td>96.53</td></tr><tr><td>balance-scale</td><td>24.60</td><td>14.40</td><td>19.61</td><td>22.46</td></tr><tr><td>pengleukEW</td><td>35.85</td><td>29.61</td><td>-5.84</td><td>23.79</td></tr><tr><td>optical_test</td><td>84.08</td><td>63.56</td><td>78.11</td><td>81.53</td></tr><tr><td>semeionEW</td><td>52.62</td><td>34.90</td><td>46.57</td><td>48.12</td></tr><tr><td>vote</td><td>57.10</td><td>54.34</td><td>53.00</td><td>53.00</td></tr></table></body></html>

# Conclusion

The paper proposes a new approach to represent data in a coarse granularity that can accelerate and improve the performance of spectral clustering. Specifically, the approach can discover micro-clusters based on local density distributions of data, and introduces the concept of manifold curvature of micro-clusters to help split them into more convex ones. In this way, the approach provides better representation of data and simplifies the characterization of the similarities, resulting in better performance in subsequent spectral clustering. Evaluations on 4 synthetic datasets and 13 realworld datasets, against relevant state-of-the-art algorithms, show that the proposed algorithm performs best on most datasets. The ablation experiments also demonstrate the effectiveness of the proposed components.

There is still room for improvement and optimization. Future work will attempt to introduce more efficient computational techniques, such as parallel and distributed computing, to further enhance the computational speed of the algorithm, especially when handling ultra-large-scale datasets. Additionally, one can consider developing adaptive hyperparameter optimization methods for this approach, and can also consider to combine it with other clustering methods.