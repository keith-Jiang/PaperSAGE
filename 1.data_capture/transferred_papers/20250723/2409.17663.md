The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)

# Explanation Bottleneck Models

Shin‚Äôya Yamaguchi1,2\* and Kosuke Nishida1

1NTT 2Kyoto University

# Abstract

Recent concept-based interpretable models have succeeded in providing meaningful explanations by pre-defined concept sets. However, the dependency on the pre-defined concepts restricts the application because of the limited number of concepts for explanations. This paper proposes a novel interpretable deep neural network called explanation bottleneck models (XBMs). XBMs generate a text explanation from the input without pre-defined concepts and then predict a final task prediction based on the generated explanation by leveraging pre-trained vision-language encoder-decoder models. To achieve both the target task performance and the explanation quality, we train XBMs through the target task loss with the regularization penalizing the explanation decoder via the distillation from the frozen pre-trained decoder. Our experiments, including a comparison to state-of-the-art concept bottleneck models, confirm that XBMs provide accurate and fluent natural language explanations without pre-defined concept sets.

# Code ‚Äî https://github.com/yshinya6/xbm/ Extended version ‚Äî https://arxiv.org/abs/2409.17663

# 1 Introduction

Although deep learning models can achieve remarkable performance on many applications, they are black-box, i.e., their output predictions are not interpretable for humans. Introducing concept bottleneck models (CBMs, Koh et al. (2020)) is a promising approach to interpreting the output of deep models. In contrast to black-box models that directly predict output labels from input in an end-to-end fashion, CBMs first predict concept labels from input and then predict final target class labels from the predicted concepts. Since the predicted concepts represent semantic input ingredients, this two-staged prediction enables users to know the reasons for the final target label predictions and interactively intervene in the decision-making process for critical applications such as healthcare (Chauhan et al. 2023).

However, the existing CBMs depend on the fixed predefined concept sets to predict final labels. In other words, they can not provide interpretability to any other than the pre-defined concepts. We argue that this limitation presents a

‚ÄúA large feline Vision Explanation with large hands Classifier E $h _ { \psi }$ er Decoder ùëî# ssnitotiwnignian ftiheled of grass‚Äù ùëì! ‚ÄúLynx‚Äù Final Input ùë• Explanation ùíÜ# Prediction ùë¶# fundamental challenge for CBMs in achieving interpretable deep models. Although recent CBM variants leveraging pretrained large language models (Yuksekgonul, Wang, and Zou 2023; Oikarinen et al. 2023) enable to express concepts of arbitrary target classes, the interpretability is still restricted to a fixed and small number of concepts. This is because a large number of concept labels are difficult to learn due to their long-tail distribution and are less interpretable by the limitation of human perception (Ramaswamy et al. 2023). In fact, the prior works restrict the number of concepts by filtering with the similarity between concepts and training images to maintain the performance and interpretability (Oikarinen et al. 2023; Yang et al. 2023). Therefore, as long as they depend on pre-defined concepts, CBMs are restricted in the number of interpretable concepts and are insufficient to explain the output of deep models.

This paper tackles a research problem where we do not assume pre-defined concept sets for constructing interpretable deep neural networks. To this end, we propose a novel family of interpretable models called explanation bottleneck models (XBMs), which leverage pre-trained multi-modal encoderdecoder models that can generate text descriptions from input data (e.g., BLIP (Li et al. 2022, 2023)). Leveraging pretrained multi-modal encoder-decoder enables capturing concepts that actually appeared in the input beyond pre-defined concept sets. Our key idea is to decode concepts as text explanations from input and then predict the final label with a classifier that takes the decoded explanations (Fig. 1). In contrast to CBMs, which make predictions based on pre-defined concepts, XBMs make predictions based on concepts actually appeared in the input data through the decoded explanations and can provide an intuitive interpretation of the final prediction tied to the input. Through end-to-end training, XBMs aim to generate explanations focusing on the textual features for solving the target task.

A major challenge for XBMs is forgetting the text generation capability during training on target tasks. Since target datasets usually lack ground-truth text labels, it is challenging to avoid catastrophic forgetting. To generate high-quality explanations, we introduce a training technique called explanation distillation, which penalizes the text decoders by the reference explanations generated by frozen pre-trained text decoders. Solving target tasks with explanation distillation enables XBMs to decode explanations from input data in natural sentences without corruption.

We conduct experiments to evaluate XBMs on multiple datasets by comparing them to existing CBMs and black-box baselines regarding interpretability and target task performance. Our experiments show that XBMs can provide a more relevant explanation to input than the pre-defined concepts of existing CBMs while achieving competitive performance to black-box baselines and largely outperforming CBMs in target test accuracy. We also show that training XBMs can enhance the multi-modal understanding capability of backbone vision-language models by focusing on the target-related vocabulary. Further, we confirm the reliability and practicality of the XBMs‚Äô explanations through the experiments intervening with the random texts and the ground-truth explanations.

# 2 Explanation Bottleneck Models

This section introduces the principle of explanation bottleneck models (XBMs). XBMs are interpretable deep learning models that predict a final label from the generated explanation text from XBMs themselves. Since the predicted final labels are based on the generated explanation of input images, we can naturally interpret the explanation as the reason for the prediction of XBMs. Figure 2 illustrates the overview of training an XBM. An XBM consists of a visual encoder $h _ { \psi }$ , an explanation decoder $g _ { \phi }$ , and a classifier $f _ { \theta }$ for predicting final target labels. Among them, $h _ { \psi }$ and $g _ { \phi }$ are initialized by an arbitrary pre-trained multi-modal encoder-decoder like BLIP (Li et al. 2022). $f _ { \theta }$ is a multi-modal classifier built on a transformer that takes the generated explanations as input and conditions the cross-attention layers with image embeddings; this design is inspired by hybrid post-hoc CBMs (Yuksekgonul, Wang, and Zou 2023) that uses input embeddings to complement missing concepts not in the predicted concepts. We also confirm the practicality when using a text classifier in Section 3.4. In this section, we mainly describe XBMs with a multi-modal classifier. XBMs are trained by the target classification loss in an end-to-end manner. Since naƒ±¬®ve training leads to collapse in generated text explanation, we avoid the collapse by explanation distillation. Explanation distillation penalizes the explanation decoder with a reference text generated from a frozen pre-trained text decoder $g _ { \phi _ { \mathrm { p } } }$ to prevent the decoders from forgetting the text generation capability.

# 2.1 Problem Setting

We consider a $K$ -class image classification task as the target task. We train neural network models $h _ { \psi } : \mathcal { X } \to \mathbb { R } ^ { \breve { d } _ { \mathcal { X } } }$ , $g _ { \phi } : \mathbb { R } ^ { d _ { \mathcal { X } } }  \mathcal { E }$ , and $f _ { \boldsymbol { \theta } } : ( \mathbb { R } ^ { d _ { \boldsymbol { x } } } , \mathcal { E } )  \mathcal { y }$ on a labeled target

Explanation Classification Distillation Loss Loss ‚Ñõ"#\$(ùúô, ùúì) ‚Ñí%&'(ùúÉ, ùúô, ùúì)   
Vision Pre-trained Explanation   
Encoder Decoder Decoder Classifier $h _ { \psi }$ ùëî)! -- Init. ùëî) ùëì( Generating e.g., beam search ‚ÄúA large ‚ÄúA large cat feline sitting in a snow in a snow field‚Äù Gumbel-softmax field with Sampling grass‚Äù   
Input ùë• Reference ùíÜ! Generated Explanation

dataset $\mathcal { D } = \{ ( x ^ { i } , y ^ { i } ) \in \mathcal { X } \times \mathcal { Y } \} _ { i = 1 } ^ { N }$ , where $x , \varepsilon$ , and $y$ are the input, text explanation, and output label spaces, respectively. The text explanation space consists of token sequences of the length $L$ with token vocabulary $\nu$ , i.e., $ { \mathcal { E } } { \mathrm { ~  ~ \underline { ~ } { ~ } { ~ \ } ~ } } =  { \mathcal { V } } ^ { L }$ . $h _ { \psi }$ is a vision encoder, which embeds an input $x$ into $d _ { \mathcal { X } }$ dimensional space, $g _ { \phi }$ is an auto-regressive text decoder that generates a text explanation $e \in \mathcal { E }$ from an input embedding $\bar { h } _ { \psi } ( x )$ , and $f _ { \theta }$ is a classifier that predicts a final target task label $y$ . We assume that $h _ { \psi }$ and $g _ { \phi }$ are initialized by pretrained multi-modal model‚Äôs parameters $\psi _ { \mathrm { { p } } }$ and $\phi _ { \mathrm { p } }$ , which are pre-trained on large-scale text-image paired datasets with an existing method such as BLIP (Li et al. 2022) and LLaVA (Liu et al. 2023). Note that we do not assume ground truth text explanation set $\{ e ^ { i } \} _ { i = 1 } ^ { N }$ in $\mathcal { D }$ for training $g _ { \phi }$ .

This setting is similar to that of concept bottleneck models (CBMs, Koh et al. (2020)), where a model predicts a final label $y$ from a set of concepts $\{ c ^ { j } \in \mathcal { C } \} _ { j = 1 } ^ { M }$ decoded from input $x$ instead of using $e$ . The major difference is in the assumption of pre-defined concept sets: our setting does not explicitly specify the words and phrases for the explanations, whereas CBMs explain the model‚Äôs output based on the words and phrases in a pre-defined concept set $\{ c ^ { j } \}$ .

# 2.2 Objective Function

XBMs aim to achieve high target classification accuracy while providing interpretable explanations of the predictions. To this end, XBMs solve an optimization problem with a regularization term defined by the following objective function.

$$
\begin{array} { r l } & { \underset { \theta , \phi , \psi } { \operatorname* { m i n } } \ : \mathcal { L } _ { \mathrm { c l s } } ( \theta , \phi , \psi ) + \lambda \mathcal { R } _ { \mathrm { i n t } } ( \phi , \psi ) , } \\ & { } \\ & { \mathcal { L } _ { \mathrm { c l s } } ( \theta , \phi , \psi ) = \mathbb { E } _ { ( x , y ) \in \mathcal { D } } \ : \ell _ { \mathrm { C E } } ( f _ { \theta } \circ g _ { \phi } \circ h _ { \psi } ( x ) , y ) , } \end{array}
$$

where $\mathcal { R } _ { \mathrm { i n t } } ( \cdot )$ is a regularization term that guarantees the fluency of the explanations generated from $g _ { \phi } , \lambda$ is a hyperparameter for balancing $\mathcal { L } _ { \mathrm { c l s } }$ and $\mathcal { R } _ { \mathrm { i n t } }$ , and $\ell _ { \mathrm { C E } }$ is cross-entropy loss. Through this objective, the text decoder $g _ { \phi }$ is trained to focus on the textual features that are useful for minimizing $\mathcal { L } _ { \mathrm { c l s } }$ while keeping the interpretability by $\mathcal { R } _ { \mathrm { i n t } }$ . We found that $g _ { \phi }$ easily collapses their output without $\mathcal { R } _ { \mathrm { i n t } }$ . Thus, the design of $\mathcal { R } _ { \mathrm { i n t } }$ is crucial for training XBMs. However, since we often do not have the ground truth explanation sets in a real-world target dataset $\mathcal { D }$ , we can not directly penalize $g _ { \phi }$ with supervised losses as $\mathcal { R } _ { \mathrm { i n t } }$ . To overcome this challenge, we introduce a distillation-based approach using pre-trained text decoders in the next section.

# 2.3 Explanation Distillation

XBMs utilize pre-trained multi-modal models as the initial parameters of the text (explanation) decoder $g _ { \phi }$ . As an autoregressive sequence model, the pre-trained text decoder $g _ { \phi _ { \mathrm { p } } }$ can learn a conditional distribution $q ( e | x )$ as

$$
q ( e | x ) = \prod _ { l = 1 } ^ { L } q ( e _ { l } | x , e _ { < l } ) ,
$$

where $L$ is the maximum token length, $e _ { l }$ is the $l$ -th token, and $e _ { < l }$ is the text sequence before $e _ { l }$ . Since $g _ { \phi _ { \mathrm { p } } }$ is trained on large-scale text-image pairs, $q ( e | x )$ is expected to be able to generate a token sequence describing important information of various inputs $x$ .

Our key idea is to leverage $q ( e | x )$ as the reference distribution for maintaining the interpretability of the generated explanation $\hat { e } \sim p _ { \phi } ( e | x )$ , where $\dot { p } _ { \phi } ( e | x )$ is the model distribution of $g _ { \phi }$ . If $p _ { \phi } ( e | x )$ and $q ( e | x )$ are sufficiently close, it can be guaranteed that the interpretability of the sequence generated by $p _ { \phi } ( e | x )$ approximate to that by $q ( e | x )$ . Concretely, we compute the KL divergence between $p _ { \phi } ( e | x )$ and $q ( e | x )$ as the regularization term $\mathcal { R } _ { \mathrm { i n t } }$ in Eq. (1).

$$
\begin{array} { r l } & { \mathcal { R } _ { \mathrm { i n t } } ( \phi , \psi ) = D _ { \mathrm { K L } } ( q \Vert p _ { \phi } ) } \\ & { \qquad = \mathbb { E } _ { e \sim q ( e | x ) } \log \left( \frac { q ( e | x ) } { p _ { \phi } ( e | x ) } \right) . } \end{array}
$$

However, $D _ { \mathrm { K L } } ( q \Vert p _ { \phi } )$ is computationally intractable because it requires multiple sequential sampling over $\mathcal { E } = \mathcal { V } ^ { L }$ from $q ( e | x )$ and the back-propagation through all sampling processes of $p _ { \phi } ( e _ { l } | x , e _ { < l } )$ . To approximate Eq. (4), we focus on the connection to knowledge distillation (Hinton, Vinyals, and Dean 2015). That is, minimizing Eq. (4) can be seen as a knowledge distillation from $g _ { \phi _ { \mathrm { p } } }$ to $g _ { \phi }$ . In such a sense, the approximation is

$$
\begin{array} { l } { \mathcal { R } _ { \mathrm { i n t } } ( \phi , \psi ) \approx - \displaystyle \sum _ { e \in \mathcal { E } } \mathbb { I } _ { e = e _ { \mathrm { p } } } \log p _ { \phi } ( e | x ) } \\ { \quad \quad = - \log p _ { \phi } ( e = e _ { \mathrm { p } } | x ) , } \end{array}
$$

where $e _ { \mathrm { p } }$ is the sample from $q ( e | x )$ and $\mathbb { I }$ is the indicator function returning one when $e$ equals to $e _ { \mathrm { p } }$ or returning zero otherwise; we omit the constant terms from the approximation for the simplicity. As a concrete procedure, we first generate $e _ { \mathrm { p } }$ from $g _ { \phi _ { \mathrm { p } } }$ and then penalize the output logits of $g _ { \phi }$ through the cross-entropy loss for each output token in a next token prediction task. This approximation technique is well-known as sequence-level knowledge distillation (Kim and Rush 2016) in the field of neural machine translation, and it works well in the knowledge distillation of auto-regressive

# Algorithm 1: Training of XBMs

Require: Training dataset $\mathcal { D }$ , vision encoder $h _ { \psi }$ , text decoder $g _ { \phi }$ , classifier $f _ { \theta }$ , pre-trained parameters $\big ( \phi _ { \mathrm { p } } , \theta _ { \mathrm { p } } \big )$ , training batchsize $B$ , step size $\eta$ , trade-off parameter $\lambda$   
Ensure: Trained models $( h _ { \psi } , g _ { \phi } , f _ { \theta } )$   
1: # Initialize parameters   
2: $\phi  \phi _ { \mathrm { p } } , \ \psi  \psi _ { \mathrm { p } }$   
3: while not converged do   
4: $\{ ( x ^ { i } , y ^ { i } ) \} _ { i = 1 } ^ { B } \sim \mathcal { D }$   
5: # Generating reference explanation   
6: $\{ e _ { \mathrm { p } } ^ { i } \} _ { i } ^ { B }  \{ \mathrm { g e n e r a t e } ( g _ { \phi _ { \mathrm { p } } } , h _ { \mathrm { p } } ( x ^ { i } ) ) \} _ { i } ^ { B }$   
7: # Gumbel-softmax sampling   
8: $\{ \hat { e } ^ { i } \} _ { i } ^ { B } \gets \{ \mathrm { g . s a m p l i n g } ( g _ { \phi } , h _ { \psi } ( x ^ { i } ) ) \} _ { i } ^ { B }$   
190: $\begin{array} { r } { \mathcal { L } _ { \mathrm { c l s } } ^ { B } \gets \frac { 1 } { B } \sum _ { i = 1 } ^ { B ^ { - } } \ell _ { \mathrm { C E } } \big ( f _ { \theta } \big ( h _ { \psi } ( x ^ { i } ) , \hat { e } ^ { i } \big ) , y ^ { i } \big ) } \end{array}$   
11: $\begin{array} { r } { \mathcal { R } _ { \mathrm { i n t } } ^ { B }  \frac { 1 } { B } \sum _ { i = 1 } ^ { B } \ell _ { \mathrm { C E } } \big ( g _ { \phi } \circ h _ { \psi } ( x ^ { i } ) , e _ { \mathrm { p } } ^ { i } \big ) } \end{array}$   
12: # Updating parameters via backprop.   
13: $\theta \longleftarrow \theta - \eta \nabla _ { \theta } \bar { ( } \mathcal { L } _ { \mathrm { c l s } } ^ { \tilde { B } } + \lambda \mathcal { R } _ { \mathrm { i n t } } ^ { B } ) , \phi \longleftarrow \phi - \eta \nabla _ { \phi } ( \bar { \mathcal { L } } _ { \mathrm { c l s } } ^ { B } + \lambda \mathcal { R } _ { \mathrm { i n t } } ^ { B } ) ,$ $\psi  \theta - \eta \nabla _ { \psi } ( \mathcal { L } _ { \mathrm { c l s } } ^ { B } + \lambda \mathcal { R } _ { \mathrm { i n t } } ^ { B } )$

14: end while

sequence models. Sequence-level knowledge distillation corresponds to matching the modes of $p$ and $q$ and omits to transfer the uncertainty represented by the entropy $H ( q )$ (Kim and Rush 2016). Nevertheless, we consider that this is sufficient for XBMs because the goal of XBMs is to provide interpretable explanations for target task predictions, not to replicate the pre-trained models perfectly. We call the regularization with Eq. (5) explanation distillation, and introduce it in training XBMs to maintain the text generation capability.

# 2.4 Algorithm

Training We show the training procedure in Algorithm 1. In the training loop, we first generate the reference and predicted explanations $e _ { \mathrm { p } }$ and $\hat { e }$ by generate $( \cdot )$ and g sampling $( \cdot )$ , respectively (line 4 and 5). To approximate the mode of ${ \dot { q } } ( e | x )$ and ensure the quality as the reference, we generate $e _ { \mathrm { p } }$ from frozen $g _ { \phi _ { \mathrm { p } } }$ by beam search following the previous work (Kim and Rush 2016). For sampling $\hat { e }$ , we introduce the Gumbel-softmax trick (Jang, Gu, and Poole 2017) to retain the computation graph for the end-to-end training with back-propagation. The $l$ -th token can be approximately sampled by

$$
e _ { l } = \mathrm { s o f t m a x } ( ( \log ( g _ { \phi } ( h _ { \psi } ( x ) ) ) + \mathbf { g } ) / \tau ) ,
$$

where $\mathbf { g } = \{ \mathrm { g } _ { 1 } , . . . , \mathrm { g } _ { | \mathcal { V } | } \}$ is a vector of length $| \nu |$ where each element is sampled from ${ \mathrm { G u m b e l } } ( 0 , 1 )$ and $\tau$ is the temperature parameter. Intuitively, the temperature $\tau$ controls the diversity of the token outputs from $g _ { \phi }$ ; larger $\tau$ stimulates more diverse outputs. To obtain diverse and accurate tokens for describing input, we apply exponential annealing to the temperature values according to the training steps, i.e., œÑ (i+1) = œÑ (0) exp (‚àírai), where i and ra are training step and annealing rate. This allows XBMs to focus on the diversity of the output tokens in the early training steps and on the quality in the later steps. We evaluate this design choice in Appendix E.1. After sampling $e _ { \mathrm { p } }$ and $\hat { e }$ , we update all trainable parameters according to the objective function Eq. (1).

![](images/d38f82dd75dea9793323ba6aee421011382b3beb2adaa0675f78e1f14758fa06.jpg)  
Figure 3: Explanation styles provided by XBMs. XBMs can output (i) text explanation directly generated from the explanation decoder, (ii) concept phrases with self-attention scores in the classifier, and (iii) cross-attention heatmap for the entire text explanation and each concept phrase. Concept phrases are constructed by a natural language parser, and the self-attention scores are computed in a middle layer of the classifier with respect to the [CLS] token for each concept phrase. Cross-attention heatmaps are the heatmap visualizations of cross-attention scores between input text tokens and image embedding tokens in the middle layer of the multi-modal classifier (a redder means a higher score).

Inference For the inference of test input $x$ , we generate $\hat { e }$ by beam search instead of the Gumbel-softmax trick, i.e., ${ \hat { e } } \gets \mathrm { g e n e r a t e } ( g _ { \phi } , h _ { \psi } ( x ) )$ . Finally, we return the target label prediction $\hat { y } \gets f _ { \theta } ( h _ { \psi } ( x ) , \hat { e } )$ and the explanation $\hat { e }$ to users. Optionally, XBMs provide the other styles of explanation in addition to $\hat { e }$ (Fig. 3). A concept phrase $c$ is a noun phrase that compose $\hat { e }$ , which can be extracted by natural language parser automatically (Feng et al. 2022). Similar to the concept outputs of CBMs, $c$ provides contributions of noun phrases in text explanations for the prediction. For example, if the classifier $f _ { \theta }$ is implemented with transformer families with attention layers, we can interpret the contribution of $c$ for the target prediction $\hat { y }$ via its self-attention scores as in Fig. 3 (ii). Furthermore, we can visualize the cross-attention scores between the text explanations and visual tokens as a heatmap, suggesting what the model perceives as a concept in input data (Fig. 3 (iii)).

# 3 Experiment

We evaluate XBMs on multiple visual classification tasks and pre-training models. We conduct qualitative and quantitative experiments on the explanation outputs of XBMs to evaluate the target performance and the interpretability. We also provide a more detailed analysis, including varying hyperparameters $\lambda , \tau$ and comparing explanation distillation with an alternative regularization loss in Appendix E.

# 3.1 Setting

Implementation Our basic implementation of XBMs is based on BLIP (Li et al. 2022) because of its simplicity; we denote this model as XBM-BLIP. That is, as the visual encoder $h _ { \psi }$ , we used the ViT-B/32 (Dosovitskiy et al. 2021). For the classifier $f _ { \theta }$ , we used a BERT-base transformer (Devlin et al. 2019); we input $h _ { \psi } ( x )$ into the cross-attention layers when using a multi-modal classifier inspired by BLIP (Li et al. 2022). We initialized $\phi$ and $\psi$ by the BLIP model pre-trained on image captioning tasks in the official repository1. We also report the results using larger pre-trained multi-modal models of LLaVA (Liu et al. 2023). We used v1.5 and v1.6 of

LLaVA with multiple language model backbones (LLaMA2- 7B (Touvron et al. 2023), Vicuna-7B (Chiang et al. 2023), and Mistral-7B (Jiang et al. 2023)); we denote these models as XBM-LLaVA. We provide detailed training settings in Appendix A.

Baselines We compare XBMs to black-box and interpretable baselines in performance and interpretability. Finetuned BLIP-ViT is the black-box baseline, which directly optimizes the visual encoder of BLIP via fine-tuning. Labelfree CBM (Oikarinen et al. 2023) is a state-of-the-art concept bottleneck model, which automatically constructs pre-defined concept sets from ConceptNet (Speer, Chin, and Havasi 2017) or GPT-3 (Brown et al. 2020a) and then constructs concept embedding matrix via CLIP vision and text encoder. We used BLIP-ViT as the backbone vision encoder of label-free CBMs. Frozen BLIP baselines use frozen BLIP to generate text explanations and predict final labels by a multi-modal $f _ { \theta } ( h _ { \psi } ( x ) , \hat { e } )$ or text classifier $f _ { \theta } ( \hat { e } )$ . We also show the results of XBM w/o $\mathcal { R } _ { \mathrm { i n t } }$ , which updates $g _ { \phi }$ only on the classification loss Eq. (2).

Datasets We used four image datasets for classification tasks in various domains: Aircraft (Maji et al. 2013), Bird (Welinder et al. 2010), Car (Krause et al. 2013), and ImageNet (Russakovsky et al. 2015). Aircraft, Bird, and Car are fine-grained image datasets, and ImageNet is a large-scale general image dataset. For datasets other than ImageNet, we randomly split a dataset into $9 : 1$ and used the former as the training set and the latter as the validation set. For ImageNet, we set the split ratio $9 9 : 1$ and used the official validation set as the test dataset.

Evaluation Metrics We report test accuracy as the target task performance. For the interpretability evaluations, we introduce CLIP-Score (Radford et al. 2021; Hessel et al. 2021), which is based on the cosine similarity between image embeddings and text embeddings on CLIP, i.e., higher is better. CLIP-score was originally used to evaluate image captioning based on the relevance of the output captions to the input images. Since it is highly sensitive to the hallucinations in the captions as reported in (Hessel et al. 2021), CLIP-score can be used to assess the factuality of explanations. For

Table 1: Performance and Interpretability Evaluation of XBMs on multiple target datasets.   

<html><body><table><tr><td rowspan="2"></td><td colspan="3">Aircraft</td><td colspan="3">Bird</td></tr><tr><td>Test Acc. (‚Üë)</td><td>CLIP-Score (‚Üë)</td><td>GPT-2 Perplexity(‚Üì)</td><td>Test Acc. (‚Üë)</td><td>CLIP-Score (‚Üë)</td><td>GPT-2 Perplexity (‚Üì)</td></tr><tr><td>Fine-tuned BLIP-ViT</td><td>77.86¬±.30</td><td>N/A</td><td>N/A</td><td>83.48¬±.15</td><td>N/A</td><td>N/A</td></tr><tr><td>Label-free CBM (ConceptNet)</td><td>15.37¬±.17</td><td>0.5356</td><td>N/A</td><td>17.67¬±.40</td><td>0.6025</td><td>N/A</td></tr><tr><td>Label-free CBM (GPT-3)</td><td>44.47¬±.34</td><td>0.6153</td><td>N/A</td><td>77.74¬±.43</td><td>0.6904</td><td>N/A</td></tr><tr><td>Frozen BLIP + fe(hy(x),√©)</td><td>45.23¬±.32</td><td>0.6824</td><td>155.8</td><td>68.03¬±.10</td><td>0.7535</td><td>173.5</td></tr><tr><td>XBM w/o Rint</td><td>70.78¬±.48</td><td>0.4730</td><td>322.6</td><td>61.94¬±.13</td><td>0.5137</td><td>431.0</td></tr><tr><td>XBM (Ours)</td><td>74.09¬±.07</td><td>0.7151</td><td>129.8</td><td>80.99¬±.18</td><td>0.7942</td><td>166.8</td></tr><tr><td></td><td colspan="3">Car</td><td colspan="3">ImageNet</td></tr><tr><td></td><td>Test Acc. (‚Üë)</td><td>CLIP-Score (‚Üë)</td><td>GPT-2 Perplexity (‚Üì)</td><td>Test Acc. (‚Üë)</td><td>CLIP-Score (‚Üë)</td><td>GPT-2 Perplexity (‚Üì)</td></tr><tr><td>Fine-tuned BLIP-ViT</td><td>90.08¬±.35</td><td>N/A</td><td>N/A</td><td>65.21¬±.14</td><td>N/A</td><td>N/A</td></tr><tr><td>Label-free CBM (ConceptNet)</td><td>15.27¬±.13</td><td>0.5561</td><td>N/A</td><td>60.07¬±.42</td><td>0.6826</td><td>N/A</td></tr><tr><td>Label-free CBM (GPT-3)</td><td>77.91¬±.21</td><td>0.6091</td><td>N/A</td><td>64.28¬±.09</td><td>0.7026</td><td>N/A</td></tr><tr><td>Frozen BLIP + fe(h(x), √©)</td><td>80.53¬±.29</td><td>0.6555</td><td>168.8</td><td>56.04¬±.49</td><td>0.7732</td><td>199.5</td></tr><tr><td>XBM w/o Rint</td><td>86.59¬±.11</td><td>0.4792</td><td>415.3</td><td>66.58¬±.30</td><td>0.5020</td><td>517.1</td></tr><tr><td>XBM (Ours)</td><td>89.47¬±.10</td><td>0.7173</td><td>131.8</td><td>67.83¬±.33</td><td>0.7920</td><td>122.8</td></tr></table></body></html>

XBMs, we measured averaged CLIP-Scores between test inputs and the output explanations. For Label-free CBMs, we measured averaged CLIP-Scores between test inputs and the output concept texts with the binary output of the concept bottleneck layer greater than 0.05; this threshold follows Oikarinen et al. (2023). We also introduce GPT-2 Perplexity as a measure of fluency in XBM‚Äôs output explanations. In general, perplexity scores on language models are calculated by the averaged cross-entropy of the next token probabilities and thus represent the fluency of the generated texts because the lower perplexity means that the sentence is composed of words that are likely to occur probabilistically. Inspired by Chan et al. (2023), we computed perplexity scores of explanations on GPT-2 (Radford et al. 2019). That is, the generated explanations are unbiasedly evaluated by an external language model. GPT-2 perplexity is helpful as a metric of the fluency of explanations because it shows the proximity to the natural text distribution learned by GPT-2. We used open-sourced GPT-2 in huggingface transformers (Wolf et al. 2019) to maintain reproducibility.

# 3.2 Design Evaluation of XBMs

Quantitative Evaluation Table 1 demonstrates the quantitative performance and interpretability of XBM-BLIP on the four target datasets. For the target performance, our XBMs outperformed the Label-free CBM baselines and achieved competitive performance with the black-box baseline in the test accuracy. In particular, XBM achieved high performance on datasets where label-free CBM did not perform well (i.e., Aircraft and Car). This can be caused by insufficient predefined concepts due to the limited vocabulary in ConceptNet and GPT-3 about describing objects in these datasets, whereas XBMs promote multi-modal understanding by training the explanation decoder to describe arbitrary objects useful for the target dataset with unlimited vocabulary. For the interpretability, XBMs outperformed CBMs in CLIP-Score. This indicates that the explanations from XBMs are more factual to the input images than the concept outputs of CBMs, which are in pre-defined concept sets.

Furthermore, the ablation study in the bottom rows of Table 1 shows that the objective function in Eq. (1) works effectively as we expected. Compared to the frozen BLIP baselines, which simply apply fixed pre-trained BLIP to generate text captions, our XBM significantly improved all of the test accuracy, CLIP-Score, and GPT-2 Perplexity. This suggests that optimizing text decoders with respect to target tasks guides the generated explanation to be informative and target-related for solving the task. We also confirm that the regularization term $\mathcal { R } _ { \mathrm { i n t } }$ by explanation distillation (Eq. (5)) is crucial to generate meaningful explanation; XBM w/o $\mathcal { R } _ { \mathrm { i n t } }$ catastrophically degraded CLIP-Score and GPT-2 Perplexity.

Qualitative Evaluation Table 2 shows the qualitative studies of explanations generated from XBMs; we also show the other examples in Appendix B. We computed the self-/cross attention scores in the middle of the transformer layers by following Zhang\* et al. (2020). For comparison, we also show the top-3 concept outputs of CBMs and the generated captions of pre-trained BLIP, i.e., the initial states of XBMs. The text explanations of XBMs contain more detailed information than pre-trained BLIP. This is because the target classification loss $\mathcal { L } _ { \mathrm { c l s } }$ forces the text decoders to describe target-related visual information to solve the task. Importantly, XBMs without explanation distillation $\mathcal { R } _ { \mathrm { i n t } }$ generate totally broken explanations, indicating the objective function of XBMs succeed in training the models to focus on the tokens related to the target task without the collapse of explanations. Meanwhile, the concept phrase explanations show the contributions to the final outputs (i.e., self-attention scores) for each noun phrase in the text explanations. In contrast to CBM‚Äôs concepts, the concept phrases tend to be aligned with visual features appearing in input images rather than describing input by pre-defined knowledge. This is easy for humans to understand when interpreting the output of the models. Finally, the cross-attention heatmaps intuitively localize where the generated text explanations correspond to the input image spaces. We confirm that the heatmaps concentrate on objects through optimization and facilitate a multi-modal understanding of the image in Section 3.5.

Table 2: Qualitative evaluation of explanation outputs.   
Figure 4: Transition of XBM‚Äôs explanations during training.   

<html><body><table><tr><td></td><td>Bird (Yellow Bellied Flycatcher)</td></tr><tr><td>Pre-trained BLIP (Caption)</td><td>A bird perched on a wire fence with leaves on the ground and a blurry background.</td></tr><tr><td>Label-free CBMs (Top-3 Concept)</td><td>olive-colored sides (0.77) green head (0.55) a small, green body (0.52)</td></tr><tr><td>XBMs w/o Rint (Text Explanation)</td><td>2222222222222 2222222222</td></tr><tr><td>XBMs (Text Explanation)</td><td>A small gen and yellow bid perchedon</td></tr><tr><td>XBMs (Top-3 Concept Phrase)</td><td>a small green and yellow bird (0.39) leaves on the side (0.32) a wire fence (0.21)</td></tr><tr><td>XBMs (Cross-Attn. Heatmap)</td><td></td></tr></table></body></html>

We also analyze the transition of the generated explanations in Fig. 4. We print the text explanation of XBMs and the top-10 word occurrence for all classes and the input class at 0, 20, and 40 epochs. According to the training epoch, the explanations and words progressively focus on detailed and target-related information in images. Concretely, in this example, the XBM is optimized to describe ‚Äúyellow beak (mouth)‚Äù, a key feature of California Gull. These suggest that XBMs can provide interpretable and useful explanations for humans.

# 3.3 XBMs with Large Vision-Language Models

Here, we evaluate the scalability and practicality of XBMs by combining them with larger vision-language models than BLIP. Instead of BLIP, we used the LLaVA models with various language model backbones (Liu et al. 2023). Table 3 shows that leveraging the high-performance vision-language model in XBMs yields better performance and interpretability scores, suggesting that the XBM‚Äôs objective function can enhance the multi-modal understanding ability even if using the large vision-language models pre-trained on massive image-text pairs. This emphasizes the flexibility of XBM, consisting of arbitrary vision-language models.

# 3.4 XBMs with Text Classifier

Table 3 also evaluates XBMs with a text classifier $f _ { \theta } ( \hat { e } )$ , which relies only on text information for the final predictions. Although XBM-BLIP with $f _ { \theta } ( \hat { e } )$ drops the performance from one with a multi-modal classifier $f _ { \theta } ( h _ { \psi } ( x ) , \hat { e } )$ , switching the backbone from BLIP to LLaVA (Liu et al. 2023) resolves the performance gap. This indicates that more sophisticated vision-language models make XBMs generate informative text explanations, and they can achieve practical performance even when not using input features $h _ { \psi } ( x )$ . Appendix C further shows the results on the other datasets.

# 3.5 Evaluations of Cross-Attention Heatmap

The cross-attention heatmap explanation of XBMs visualizes the local input space regions correlated to the text explanation

Input (California Gull) Epoch 0 Epoch 20 Epoch 40 ‚ÄúSomeone standing ‚ÄúA seagull standing ‚ÄúA seagull with a on a rock in front of on a rock by the yellow beak the water‚Äù water‚Äôs edge‚Äù standing on beach‚Äù ‚ÄúA seagull with a ‚ÄúAnimals that are ‚ÄúA seagull standing beak on a beach standing on the sand on a beach next to a near the water‚Äù bunch of sea lions‚Äù next to a group of sea lions‚Äù seagull, standing, seagull, standing, seagull, standing, Top-10 water, beach, water, beach, water, beak, beach, OccWurorrednce looking, back, body, back, body, sky, back, body, yellow, sand, rock, sky sand, rock, grass sky, mouth

in the classifier. To assess the validity of XBMs on improving multi-modal understanding, we evaluate the generated heatmaps on the ImageNet segmentation task by following Chefer, Gur, and Wolf (2021) and Gandelsman, Efros, and Steinhardt (2024). That is, we generate the heatmaps on the test set of ImageNet Segmentation (Guillaumin, K¬®uttel, and Ferrari 2014) and compute the pixel accuracy, mean IoU (mIoU), and mean average precision (mAP) with the ground truth segmentation masks. Through this evaluation, we can evaluate how heatmaps cover the object of target classes in the pixel spaces. Table ?? shows the results. Compared to the frozen BLIP, XBM-BLIP improved all of the segmentation metrics. This means that the training objective of XBMs encourages the multi-modal understanding of target class objects on the models. In Appendix D, we further compare the XBM‚Äôs heat maps with existing attribution methods, such as GradCAM (Selvaraju et al. 2017).

# 3.6 Reliability Evaluation via Human Intervention

CBMs allow the debugging of the model behavior through human intervention in the predicted concepts (Koh et al. 2020). Similarly, we can debug the behavior of XBMs by intervening in the generated explanations. Here, we show examples of an intervention in which all explanations are replaced to check the effect of the explanation quality on the final classification results. At inference, we replace the generated explanations from the explanation decoder with modified explanations. We tested two types of interventions: (i) randomized and (ii) ground-truth explanations. For randomized explanation, we used a token sequence uniformly sampled from the vocabulary space for the length of the originally generated explanation. For ground-truth explanation, we used the extended annotation set for Bird proposed by Reed et al. (2016). Table 5 shows the performance of the intervened XBM-BLIP models. The intervened explanations with randomized explanations significantly degraded the performance of XBM-BLIP, indicating that the generated explanations are essential to achieving high performance. In contrast, the intervention with ground-truth explanations largely improved the performance. This suggests that higher-quality explanations can yield higher performance, and intervening with human explanations is helpful for XBMs to improve their performance. In other words, the final prediction of XBMs largely depends on the content of the generated explanation $\hat { e }$ , indicating that $\hat { e }$ is a reliable explanation for the final prediction.

Table 3: Evaluation of XBMs with text and multi-modal classifiers built on large vision-language models on ImageNet.   

<html><body><table><tr><td rowspan="2"></td><td colspan="3">Text Classifier fe(e)</td><td colspan="3">Multi-modal Classifier fe(hy(x),e)</td></tr><tr><td>Test Acc. (‚Üë)</td><td>CLIP-Score (‚Üë)</td><td>GPT-2 Perplexity (‚Üì)</td><td>Test Acc. (‚Üë)</td><td>CLIP-Score (‚Üë)</td><td>GPT-2 Perplexity (‚Üì)</td></tr><tr><td>Frozen BLIP</td><td>9.97¬±.12</td><td>0.7732</td><td>199.5</td><td>56.04¬±.49</td><td>0.7732</td><td>199.5</td></tr><tr><td>XBM-BLIP</td><td>18.26¬±.31</td><td>0.8007</td><td>148.1</td><td>67.83¬±.33</td><td>0.7920</td><td>122.8</td></tr><tr><td>Frozen LLaVA-v1.5-LLaMA-7B</td><td>64.01¬±.46</td><td>0.7773</td><td>236.8</td><td>70.21¬±.18</td><td>0.7773</td><td>100.8</td></tr><tr><td>XBM-LLaVA-v1.5-LLaMA-7B</td><td>71.41¬±.25</td><td>0.8008</td><td>127.2</td><td>72.95¬±.16</td><td>0.7998</td><td>82.6</td></tr><tr><td>XBM-LLaVA-v1.6-Vicuna-7B</td><td>73.73¬±.30</td><td>0.8140</td><td>36.74</td><td>74.42¬±.23</td><td>0.8037</td><td>32.3</td></tr><tr><td>XBM-LLaVA-v1.6-Mistral-7B</td><td>72.14¬±.27</td><td>0.8037</td><td>20.67</td><td>74.04¬±.11</td><td>0.8130</td><td>21.7</td></tr></table></body></html>

Table 4: Evaluation of cross-attention map of XBMs on ImageNet Segmentation.   

<html><body><table><tr><td></td><td>Pixel Acc. (‚Üë)</td><td>mIoU (‚Üë)</td><td>mAP (‚Üë)</td></tr><tr><td>FrozenBLIP</td><td>78.67</td><td>57.90</td><td>79.72</td></tr><tr><td>XBM-BLIP</td><td>80.90</td><td>60.80</td><td>80.18</td></tr></table></body></html>

Table 5: Evaluation of Intervened XBMs on Bird.   

<html><body><table><tr><td></td><td>Test Acc. (‚Üë)</td><td>CLIP-Score (‚Üë)</td><td>GPT-2 Perp. (‚Üì)</td></tr><tr><td>XBM-BLIP</td><td>80.99</td><td>0.7942</td><td>166.8</td></tr><tr><td>Randomized Intervention</td><td>44.42</td><td>0.4497</td><td>4631.1</td></tr><tr><td>Ground-Truth Intervention</td><td>82.21</td><td>0.8179</td><td>104.5</td></tr></table></body></html>

To conclude, these results support the debuggability of XBMs and the reliability of the generated explanations.

# 4 Related Work

The main research directions of the interpretability of blackbox deep neural networks are briefly divided into attributionbased and concept-based methods. Attribution-based methods such as CAM (Zhou et al. 2016) and GradCAM (Selvaraju et al. 2017) generate a localization map representing important regions for the model predictions for specific classes. However, since the maps generated by attributionbased methods do not have information other than that they responded to the predictions, they are less interpretable regarding what semantic input features contribute to the output. In contrast to these methods, our XBMs can generate semantically interpretable heatmaps via cross-attention between image and text explanations, which can be decomposed at the level of noun phrases.

On the other hand, concept-based methods such as TCAV (Kim et al. 2018) and CBMs (Koh et al. 2020) compute contribution scores for pre-defined concepts on intermediate outputs of models. Among them, CBMs are highly relevant to our XBMs since both have interpretable intermediate layers in models. CBMs predict concept labels and then predict final class labels from the predicted concepts. The original CBMs have the challenge of requiring human annotations of concept labels (Zarlenga et al. 2022; Moayeri et al. 2023; Xu et al. 2024). Post-hoc CBMs (Yuksekgonul, Wang, and Zou 2023) and Label-free CBMs (Oikarinen et al. 2023) addressed this challenge by automatically collecting concepts corresponding to target task labels by querying large language models (e.g., GPT-3 (Brown et al. 2020b)) or existing concept banks (e.g., ConceptNet (Speer, Chin, and Havasi 2017)). However, CBMs‚Äô explanations are still restricted to pre-defined concepts, and they are not necessarily reliable because CBMs often predict the concepts without mapping to corresponding input regions (Huang et al. 2024). On the contrary, our XBMs directly generate natural language explanations to interpret the model outputs without pre-defined concepts.

Similar to our work, a few works attempted to generate linguistic explanations for target classification models (Hendricks et al. 2016; Nishida, Nishida, and Nishioka 2022). However, these methods require ground truth text explanations for training models, which are expensive and restrict applications. Our XBMs address this limitation by learning explanation generation by the classification loss and explanation distillation using a pre-trained text decoder.

# 5 Limitation

One of the limitations of XBMs is that they can not generate explanations based on user-defined concepts, which can be expressed by CBMs. In other words, XBMs are good at fluently explaining outputs in a general vocabulary because of their language model backbone but have difficulty giving interpretations for fixed concepts based on expert knowledge. A promising direction of future work is to associate the fluent explanations with user-defined concepts.

# 6 Conclusion

In this paper, we presented a novel interpretable deep neural networks called explanation bottleneck models (XBMs). By leveraging pre-trained vision-language models, XBMs generate explanations corresponding to input and output in natural language description, concept phrases with contribution scores, and cross-attention heatmaps on input spaces. To ensure both the target task performance and the explanation quality, XBMs are optimized by the target task loss with explanation distillation, which penalizes the divergence between the distributions of the training and pre-trained text decoders. Experiments show that XBMs can achieve both high target task performance and accurate and fluent explanations; they achieve competitive performance to black-box baselines and outperform CBMs in target test accuracy. Further, we found that XBMs‚Äô training can enhance the multi-modal understanding capability of vision-language models even when using large vision-language models pre-trained on massive image-text pairs. We believe that this work introduces a new perspective on natural language explanations and advances the study of interpretable deep models to the next paradigm.