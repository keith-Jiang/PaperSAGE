# TPCH: Tensor-interacted Projection and Cooperative Hashing for Multi-view Clustering

Zhongwen Wang1, Xingfeng $\mathbf { L i } ^ { 1 \ast }$ , Yinghui $\mathbf { S u n } ^ { 2 \dagger }$ , Quansen $\mathbf { S u n } ^ { 1 }$ , Yuan Sun3, Han Ling1, Jian Dai4, Zhenwen Ren5

1School of Computer Science and Engineering, Nanjing University of Science and Technology 2 School of Computer Science and Engineering, Southeast University 3 College of Computer Science, Sichuan University 4 Southwest Automation Research Institute, China South Industries Group Corporation 5 School of National Defence Science and Technology, Southwest University of Science and Technology jankinwang $@$ njust.edu.cn, lixingfeng $@$ njust.edu.cn, sunyh $@$ seu.edu.cn, sunquansen $@$ njust.edu.cn, sunyuan work $@$ 163.com, 321106010190@njust.edu.cn, daijian1 $\mod ( \boldsymbol { \omega }$ 163.com, rzw $@$ njust.edu.cn

# Abstract

In recent years, anchor and hash-based multi-view clustering methods have gained attention for their efficiency and simplicity in handling large-scale data. However, existing methods often overlook the interactions among multi-view data and higher-order cooperative relationships during projection, negatively impacting the quality of hash representation in low-dimensional spaces, clustering performance, and sensitivity to noise. To address this issue, we propose a novel approach named Tensor-Interacted Projection and Cooperative Hashing for Multi-View Clustering(TPCH). TPCH stacks multiple projection matrices into a tensor, taking into account the synergies and communications during the projection process. By capturing higher-order multi-view information through dual projection and Hamming space, TPCH employs an enhanced tensor nuclear norm to learn more compact and distinguishable hash representations, promoting communication within and between views. Experimental results demonstrate that this refined method significantly outperforms state-of-the-art methods in clustering on five largescale multi-view datasets. Moreover, in terms of CPU time, TPCH achieves substantial acceleration compared to the most advanced current methods.

Code — https://github.com/jankin-wang/TPCH

# Introduction

In recent years, advanced information-gathering technologies allow us to obtain multi-view data from the same object, enabling a more in-depth and comprehensive description of the object (Lu et al. 2023a; Qin, Pu, and Wu 2024; Sun et al. 2023b; Wu et al. 2023; Chen et al. 2023; Qin et al. 2024; Sun et al. 2023a, 2024). For example, in the field of action recognition, different views such as RGB, optical flow, and skeletal data can be used to describe actions (Hu et al. 2023; Wang et al. 2022b). With the widespread application of multi-view data, a series of multi-view learning tasks have emerged, such as multi-view classification (Liu et al. $2 0 2 4 \mathrm { a }$ ; Han et al. 2022), multi-view feature aggregation (Hu et al. 2024b; Yang et al. 2020; Hu et al. 2024a), incomplete multi-view clustering (Wen et al. 2023a,b) and so on. Among them, Multi-View Clustering (MVC) is an unsupervised method in multi-view learning (Li et al. 2023, 2024c,b). Compared to supervised methods, multi-view clustering does not require data to be labeled in advance, making it more efficient in explosive data growth. In practical applications, multi-view clustering has been widely used in computer vision, natural language processing, medical image analysis, and other fields (Fang et al. 2023; Wen et al. 2023c; Zhang et al. 2021). In the context of large-scale data, multi-view clustering faces challenges in computation and storage, known as the problem of Large-scale Multi-view Clustering (IMVC).

To solve the problem of large-scale multi-view clustering, early attempts involve reducing the number of matrix multiplications by seeking approximations of multi-view data matrices, thus reducing computational overhead (Wang et al. 2011; Zhang and Lu 2016). In recent years, the method of selecting or generating a representative set of data from multiview data, known as ”anchors,” has been widely adopted (Nie et al. 2021; Liu et al. 2024b; Wang et al. 2022a; Zhang, Nie, and Li 2023). This approach significantly reduces computational complexity and memory requirements by selecting a small subset of anchor points to represent the entire dataset. Moreover, anchor-based methods establish bridges between different views through these anchor points, enhancing the exploration of consistent information between views. In large-scale clustering tasks, traditional multi-view learning methods that operate directly on the raw data incur high computational and storage costs. The emergence of Binary Multi-View Clustering (BMVC) (Zhang et al. 2018b) methods has alleviated this issue. BMVC maps data to a low-dimensional binary Hamming space and performs clustering analysis in this lower-dimensional space, accelerating computational and storage efficiency in handling large-scale multi-view data. Furthermore, existing hash-based methods typically project multi-view data from the original space to a shared Hamming representation space to enhance the expressive power of the Hamming representation space.

However, existing BMVC methods typically utilize multiple independent projection matrices to project multi-view data into low-dimensional space and then learn hash codes for clustering partitioning in the low-dimensional space. The independence of projection matrices leads to insufficient communication between multi-view data and a lack of highorder collaborative relationships, hindering the learning of compact hash representations in low-dimensional space and reducing clustering performance.

To solve the problems above, we proposed a novel method called Tensor-Interacted Projection and Cooperative Hashing for Multi-View Clustering (TPCH). Specifically, multiple independent projection matrices project nonlinear multiview data from high-dimensional space to multiple lowdimensional spaces, resulting in multiple distinctive hash representations. To enhance the high-order collaboration of projection matrices during the projection process, we first superimpose multiple projection matrices into a high-order tensor. Simultaneously, we utilize t-SVD decompositionbased enhanced tensor low-rank constraints to enhance the compactness of hash representations. In the Hamming space, multiple hash representations are stacked into high-order tensors and enhanced with high-order tensor low-rank constraints to strengthen the collaborative capabilities within and between views, further improving the compactness of hash representations across multiple views. Finally, the average of multiple compact hash representations is taken to obtain clustering results. Our work has the following main contributions:

• For the first time, the higher-order interactions between projection matrices are considered, and a tensorinteracted projection and cooperative hashing framework is constructed to capture the high-order multi-view information of dual projections and Hamming space. • An enhanced tensor nuclear norm on the core tensor is employed to improve the compactness and distinguishability of hash codes, reducing the impact of redundant information and noise to enhance robustness. • Experimental results on five common large-scale multiview datasets demonstrate that TPCH significantly improves clustering performance under five mainstream clustering evaluation metrics compared to current stateof-the-art methods.

# Related Work

# Anchor-based Large-scale Multi-view Clustering

In the process of multi-view clustering, each view provides a unique feature description for objects (Lu et al. 2023b; Lu, Liu, and Wang 2020). Anchor-based multi-view clustering methods aim to achieve more precise and robust clustering results by facilitating the exchange between views through anchors. Selecting or generating a small number of anchors from the entire dataset can significantly reduce computational complexity and memory usage, which is particularly effective when dealing with large-scale datasets. (Kang et al. 2020) learns a smaller bipartite graph for each view and then merges these bipartite graphs to obtain a unified representation. (Wang et al. 2022a; Li et al. 2024a) reduce complexity by learning consistent anchor points. However, this approach assumes that anchor points are shared among all views, emphasizing consistency between views while overlooking complementarity. To address this, (Liu et al. 2024b) uses view-independent anchor points to represent data. This method allows different anchor points to be chosen for different views, providing high flexibility to adapt to the characteristics and requirements of different views. In recent years, efforts have been made to find a balance between consistency and complementarity among views, leading to the emergence of tensor anchor-based methods (Long et al. 2023; Xia et al. 2023; Chen, Wang, and Lai 2022). These methods stack each bipartite graph as a tensor and participate as a whole in subsequent clustering, achieving the effect of seeking consistency between views while retaining complementarity among views.

# Binary Multi-view Clustering

Binary multi-view clustering methods analyze data by mapping it into a low-dimensional binary space, significantly reducing the dimensionality of the data, alleviating the curse of dimensionality, and improving the efficiency of clustering and subsequent processing. The low-dimensional binary hash codes enable faster similarity calculations between data points, especially when using Hamming distance, greatly enhancing clustering computation speed. Reduceddimensional data occupies less storage space, making binary hash-based multi-view clustering methods more suitable for large-scale datasets. Binary-based clustering methods have been around since the early days (Gong et al. 2015). However, these methods separate binary representation and clustering into two separate steps, requiring customized representation methods for different datasets, thus limiting the algorithm’s generalization ability. To address this issue, in recent years, researchers have adopted a joint learning approach to simultaneously learn multi-view binary representations and clustering structures, achieving good clustering results with low computational and storage costs (Shen et al. 2017; Zhao et al. 2023). To obtain better binary representations, (Zhang et al. 2018b) places images into a compact common binary space and uses a binary matrix decomposition model for clustering binary representations. (Zhang et al. 2024) employs novel tensor low-rank constraints to stack binary representations from various views into tensors to capture high-order interactions.

# Methodology

To derive bipartite graphs from raw multi-view data, non-linear function methods effectively capture the inherent structure and semantic relationships within the data. These methods bring similar data closer together in highdimensional space while pushing dissimilar data farther apart. Inspired by (Zhang et al. 2024), we employ a nonlinear Radial Basis Function (RBF) kernel mapping to construct bipartite graphs for each view. The corresponding for

mula is

$$
\phi \left( X _ { p } \right) = \left[ \exp \left( - \frac { \left| \left| X _ { p } ^ { 1 } - S _ { p } ^ { 1 } \right| \right| ^ { 2 } } { \delta } \right) , \dots , \exp \left( - \frac { \left| \left| X _ { p } ^ { n } - S _ { p } ^ { m } \right| \right| ^ { 2 } } { \delta } \right) \right] ^ { T } .
$$

where $\phi ( \cdot )$ denotes a mapping used to obtain the bipartite graph for each view. $\delta$ represents the width of the kernel. We randomly select $m$ anchor point samples $S _ { p }$ from the $\boldsymbol { \mathrm { ~ p ~ } }$ -th view to ensure that the original data structure remains intact after nonlinear mapping.

# Problem Formulation

Based on the kernelized bipartite graphs, we propose to explore the high-order information in dual projection and Hamming space, facilitating intra-view and inter-view communication and synergy of the projection matrices and hash codes. In this way, we could learn more compact and distinguishable hash codes. Concretely, the proposed Tensor-interacted Projection and Cooperative Hashing for Multi-view Clustering (TPCH) can be implemented as

$$
\operatorname* { m i n } _ { \mathbf { B } _ { p } , \mathbf { Q } _ { p } , } \alpha \sum _ { p = 1 } ^ { v } \| \mathbf { Q } _ { p } ^ { \top } \phi ( \mathbf { X } _ { p } ) - \mathbf { B } _ { p } \| _ { F } ^ { 2 } + ( \| \mathbf { Q } \| _ { t n n } + \| \pmb { \mathcal { B } } \| _ { t n n } )
$$

$$
\mathbf { B } \in \{ - 1 , 1 \} ^ { l \times n }
$$

where kernelized bipartite graph $\phi ( \mathbf { X } _ { p } ) \in \mathbf { R } ^ { m \times n }$ is the input data of the $\boldsymbol { \mathrm { \tt ~ p } }$ -th view. $m$ and $n$ are the number of anchors and the number of samples. $\alpha$ is a hyperparameter used to adjust the proportion between the binary representation of multi-view data and the dual tensor. $\mathbf { Q } _ { p } \doteq \mathbf { R } ^ { m \times l }$ is the projection matrix of the $p$ -th view, used to project the bipartite graph into a consistent hamming space of dimensionality $l$ . These projection matrices are stacked to form a tensor $\mathfrak { Q }$ . $\mathbf { B } _ { p } \in \mathbf { \bar { R } } ^ { \bar { l } \times n }$ is the embedding feature matrix/hash matrix learned for the $p$ -th view, and these embedding feature matrices are stacked to form a tensor $\pmb { \mathscr { B } }$ . $\left| \left| \cdot \right| \right| _ { t n n }$ denotes the Tensor Nuclear Norm (TNN), which could be solved by tensor Singular Value Decomposition $\mathit { \Omega } _ { t }$ -SVD) as mentioned in Fig. 1. In this way, TPCH captures the high-order semantic information among bipartite graphs and then embeds them into hash codes $\mathbf { B } _ { p } \in \{ - 1 , 1 \}$ to enhance expression ability.

From Eq. 2, TPCH could stack projection matrices and hash matrices to construct tensors $\mathfrak { Q }$ and $\pmb { \mathscr { B } }$ to capture higher-order information of multi-view data in both dual projection and Hamming spaces. However, the tensors $\mathfrak { Q }$ and $\pmb { \mathscr { B } }$ not only contain the desired high-order information but also incorporate noise or redundant information. As showcased in Fig. 1, the core tensor $s$ from the t-SVD decomposition may not be low-rank, containing harmful noise and redundant information. Moreover, the core tensor with a high rank tremendously limits the higher-order synergistic between the projection matrices and hash codes. To reduce the impact of noise and redundant information, we develop the enhanced tensor nuclear norm by enforcing lowrank property on the core tensor as shown in Fig. 2.

By further ranking the core tensor in tensor Singular Value Decomposition $\mathit { \Omega } _ { t }$ -SVD) of the TNN, we further propose the

![](images/97d91933d0e95a50ffa76a02fd5cbd75f444c4676ed034484ed48df9b191e377.jpg)  
Figure 1: Basic process of t-SVD. $\pmb { \tau } \in R ^ { D _ { 1 } \times D _ { 2 } \times D _ { 3 } }$ represents the tensor to be decomposed, RD1×D2×D3 denotes the core tensor, ${ \mathcal { U } } \ \in \ { \overrightarrow { \ { \ R } } } ^ { D _ { 1 } \times D _ { 1 } \times D _ { 3 } }$ , and $\nu \in$ RD2×D2×D3 represent the left and right tensors, respectively, resulting from the t-SVD decomposition of $\tau$ .

Ehanced Tensor Nuclear Norm (ETNN) as

$$
\left| \left| \pmb { T } \right| \right| _ { e t n n } = \left| \left| \bar { \pmb { S } } \right| \right| _ { * } + \zeta \left| \left| \mathcal { U } * B ^ { - 1 } \left( \bar { \pmb { S } } \right) * \mathcal { V } \right| \right| _ { * }
$$

where $\bar { \pmb { s } } \in \boldsymbol { R } ^ { D \ \times D _ { 3 } }$ is the low-rank approximation of the core tensor, $\pmb { \mathscr { B } }$ and $\pmb { \mathscr { B } } ^ { - 1 }$ are transformations between $s$ and $\pmb { S } ^ { - 1 }$ . $\zeta$ is a predefined parameter, $\| \cdot \| _ { * }$ denotes the nuclear norm of the unfolded tensor. Fig. 2 illustrates the transformations between core tensors. Here, $D = m i n \left( D _ { 1 } , D _ { 2 } \right)$ , and $\pmb { \mathscr { B } }$ can be defined using matrix multiplication. By incorporating Fig. 2 into Fig. 1, the proposed ETNN could first alleviate the noise and redundant information hidden in the core tensor. Further, TPCH with ETNN ranks the core tensor in dual projection and Hamming spaces through transformations $\pmb { \mathscr { B } }$ and $\pmb { \mathscr { B } } ^ { - 1 }$ , enhancing the robustness in the projection synergy and inter/intra-view communication. In this way, TPCH could generate more compact, distinguishable, and robust hash codes to improve the clustering performance.

![](images/aa46ad003f0e3029a4f55ed21b7d55da6aeac668f761208cf58791600cb68588.jpg)  
Figure 2: Transformation instructions for core tensors.

# Optimization

For solving Eq. (2) with ETNN, an alternating direction minimizing strategy is proposed to iteratively optimize our CHBG. Two auxiliary variables $\boldsymbol { A }$ and $\varepsilon$ are employed to

get augmented Lagrangian function of Eq. (2) as

$$
\begin{array} { r l } & { \displaystyle \underset { \mathcal { A } , \mathcal { B } , \mathcal { L } } { \operatorname* { m i n } } \underset { \mathcal { L } } { \overset { \boldsymbol { v } } { \sum } } \underset { p = 1 } { \overset { \boldsymbol { v } } { \sum } } \| \mathbf { Q } _ { p } ^ { \top } \phi ( \mathbf { X } _ { p } ) - \mathbf { B } _ { p } \| _ { F } ^ { 2 } + \frac { \mu } { 2 } \| \boldsymbol { \mathcal { Q } } - \boldsymbol { A } + \frac { \mathfrak { V } } { \mu } \| _ { F } ^ { 2 } } \\ & { + \| \boldsymbol { \mathcal { A } } \| _ { e t n n } + \| \boldsymbol { \mathcal { E } } \| _ { e t n n } + \frac { \mu } { 2 } \| \boldsymbol { \mathcal { B } } - \boldsymbol { \mathcal { E } } + \frac { \mathcal { T } } { \mu } \| _ { F } ^ { 2 } } \\ & { \mathrm { s . t . } \ \mathbf { B } \in \{ - 1 , 1 \} ^ { l \times n } , \boldsymbol { A } = \Phi ( [ \mathbf { A } _ { 1 } ; \cdot \cdot \cdot \cdot ; \mathbf { A } _ { v } ] ) , \boldsymbol { \mathcal { Q } } = \mathcal { A } , } \\ & { \mathcal { B } = \boldsymbol { \mathcal { E } } , \mathcal { B } = \Phi ( [ \mathbf { B } _ { 1 } ; \cdot \cdot \cdot ; \mathbf { B } _ { v } ] ) , \boldsymbol { \mathcal { Q } } = \Phi ( [ \mathbf { Q } _ { 1 } ; \cdot \cdot \cdot ; \mathbf { Q } _ { v } ] ) } \\ & { } \end{array}
$$

In this way, all the variables can be iteratively optimized one by one as follows.

$\mathbf { 1 - }$ Updating the subproblem of Q: Fixing the other variables of subproblem $\mathbf { Q }$ , problem (4) w.r.t. $\mathbf { Q }$ simplifies as

$$
\alpha \sum _ { p = 1 } ^ { v } \| \mathbf { Q } _ { p } ^ { \top } \phi ( \mathbf { X } _ { p } ) - \mathbf { B } _ { p } \| _ { F } ^ { 2 } + \frac \mu 2 \| \mathbf { Q } _ { p } - \mathbf { A } _ { p } + \frac { \mathbf { Y } _ { p } } \mu \| _ { F } ^ { 2 }
$$

where $\begin{array} { r l r } { { \bf Q } _ { p } } & { { } \quad } & { = \quad \quad [ 2 \alpha { \bf B } _ { p } \phi ^ { \top } ( { \bf X } _ { p } ) \quad + \quad \mu ( { \bf A } _ { p } \quad - } \end{array}$ $\frac { \mathbf { Y } } { \mu } ) ] / [ 2 \alpha \phi ( \mathbf { X } _ { p } ) \phi ^ { \top } ( \mathbf { X } _ { p } ) \ + \ \mu \mathbf { I } ]$ . $\mathbf { Q }$ subproblem requires $\mathcal { O } ( m ^ { 3 } l + n m l + n m ^ { 2 } )$ complexity.

2- Updating the subproblem of B: Fixing the other variables of subproblem $\mathbf { B }$ , problem (6) $w . r . t . \textbf { B }$ simplifies as

$$
\operatorname* { m i n } _ { \mathbf { B } _ { p } } \alpha \sum _ { p = 1 } ^ { v } \| \mathbf { Q } _ { p } ^ { \top } \phi ( \mathbf { X } _ { p } ) - \mathbf { B } _ { p } \| _ { F } ^ { 2 } + \frac { \mu } { 2 } \| \mathbf { B } _ { p } - \mathbf { E } _ { p } + \frac { \mathbf { J } _ { p } } { \mu } \| _ { F } ^ { 2 }
$$

where ’con’ is the constant part of $\mathbf { B } _ { p }$ . The constant $\operatorname { t r } \left( \mathbf { B } _ { p } ^ { \top } \mathbf { B } _ { p } \right) = \operatorname { t r } \left( \mathbf { B } _ { p } \mathbf { B } _ { p } ^ { \top } \right) = n l$ could make problem (6) change to

$$
\begin{array} { l } { \displaystyle \operatorname* { m a x } _ { { \bf B } _ { p } } \sum _ { p = 1 } ^ { v } \mathrm { t r } [ { \bf B } _ { p } ^ { \top } ( \alpha { \bf Q } _ { p } \phi ( { \bf X } _ { p } ) + \frac { \mu } { 2 } ( { \bf E } _ { p } - \frac { { \bf J } _ { p } } { \mu } ) ) ] } \\ { \displaystyle \mathrm { s . t . } { \bf B } _ { p } \in \{ - 1 , 1 \} ^ { l \times n } } \end{array}
$$

which has a closed-form optimal solution:

$$
\mathbf { B } _ { p } = \operatorname { s g n } ( \alpha \mathbf { Q } _ { p } \phi ( \mathbf { X } _ { p } ) + \frac { \mu } { 2 } ( \mathbf { E } _ { p } - \frac { \mathbf { J } _ { p } } { \mu } ) .
$$

$\mathbf { B }$ subproblem requires $\mathcal { O } ( m l n + n c l )$ time complexity. Update-3: Solving $\mathbfcal { A }$ can be written as

which is solved by following steps (Lu et al. 2019): (i) minimizing the core matrix, and (ii) minimizing TNN.

(i) Updating core matrix as

$$
\operatorname* { m i n } _ { \mathfrak { P } ( \pmb { \mathscr { S } } ) } \| \mathfrak { P } ( \pmb { \mathscr { S } } ) \| _ { * } + \frac { 1 } { 2 \lambda } \| \pmb { \mathscr { F } } - ( \pmb { \mathscr { Z } } + \frac { \pmb { \mathscr { V } } } { \mu } ) \| _ { F } ^ { 2 }
$$

where regularization parameter $\lambda ~ = ~ 1 / ( m a x ( m , v ) n ) ^ { \frac { 1 } { 2 } }$ And the tensor $s$ is obtained from $t$ -SVD on the temporary variable $\mathcal { F }$ , i.e., $\pmb { \mathcal { F } } = \pmb { \mathcal { U } } * \pmb { \mathcal { S } } * \pmb { \mathcal { V } }$ .

(ii) Updating $\pmb { A }$ as

$$
\operatorname* { m i n } _ { \mathcal { W } } \| \mathcal { W } \| _ { e t n n } + \frac { \mu } { 2 } \| \mathcal { W } - \mathcal { G } \| _ { F } ^ { 2 }
$$

With the learned low-rank core matrix $\mathfrak { P } ( \hat { \mathbf { T } } )$ , we can use $t$ -product to reconstruct a tensor as $\pmb { \mathcal { G } } = \pmb { \mathcal { U } } * \pmb { \mathfrak { P } } ^ { - 1 } ( \pmb { \mathcal { T } } ) * \pmb { \mathcal { V } }$ . The learned $\mathfrak { s }$ can further produce a closed-form solution via Theorem 1 provided in supplementary materials. Its complexity costs $\mathbf { \bar { \mathcal { O } } } ( m l + m l \log ( \bar { m } ) )$ .

Update-4: Solving $\varepsilon$ is similar to the subproblem of $\mathbfcal { A }$ , which costs $\mathcal { O } ( n l + n l \log ( n ) )$ complexity. To update the multiplier variable $_ y$ , we employ the following procedure:

$$
\begin{array} { c } { { { \pmb y } = { \pmb y } + \mu ( { \pmb Q } - { \pmb A } ) , \eta = m i n ( \rho _ { 2 } \eta _ { 2 } , \eta _ { m a x } ) } } \\ { { { \pmb \mathcal { J } } = { \pmb \mathcal { J } } + \mu ( { \pmb B } - { \pmb \mathcal { E } } ) , \eta = m i n ( \rho _ { 3 } \eta _ { 3 } , \eta _ { m a x } ) } } \end{array}
$$

In this scenario, we set the parameter $\eta$ to $1 e ^ { - 4 }$ and the upper bound for $\mu 2$ $2 \ ( \mu 2 _ { m a x } )$ to $1 0 ^ { 1 0 }$ . $\rho _ { 2 } ~ = ~ 2$ is employed. The computational complexity of the algorithm is linear to $n$ . The solution of problem (4) is provided in Algorithm 1. $o b j$ is the objective value.

After obtaining the multiple hash matrices $\{ \mathbf { B } _ { p } \} _ { p = 1 } ^ { v }$ , the final hashing matrix is further averaged as $\begin{array} { r } { \hat { \bf B } = \sum _ { p = 1 } ^ { v } { \bf B } _ { p } } \end{array}$ . Then, the clustering indicator matrix could be achieved via the following

$$
\begin{array} { r l } & { \underset { { \bf C } , { \bf G } } { \operatorname* { m i n } } \ : \| \hat { { \bf B } } - \bar { { \bf C } } { \bf G } \| _ { F } ^ { 2 } } \\ & { \mathrm { s . t . } { \bf C } ^ { \top } 1 = 0 , { \bf C } \in \{ - 1 , 1 \} ^ { l \times k } , { \bf G } \in \{ 0 , 1 \} ^ { k \times n } , \sum _ { i } g _ { i n } = 1 . } \end{array}
$$

which can be solved according to the discrete proximal linearized minimization (DPLM) (Wang et al. 2023).

Optimizing G-Step is

$$
\begin{array} { r } { g _ { i j } ^ { t + 1 } = \left\{ \begin{array} { l r } { 1 , } & { j = \arg \operatorname* { m i n } _ { * } H \left( b _ { i } , c , \displaylimits _ { , } ^ { t + 1 } \right) , } \\ { 0 , } & { \mathrm { o t h e r w i s e , } } \end{array} \right. } \end{array}
$$

where $H \left( \mathbf { b } _ { i } , \mathbf { c } ^ { n } \right)$ represents the Hamming distance between the $i$ -th binary code $b _ { i }$ and the $s$ -th cluster centroid $c ^ { s }$ , offering significantly faster computation compared to the Euclidean distance. The final results are obtained from $\mathbf { G }$ .

Time Complexity. The main complexity cost of Algorithm 1 cost ${ \bar { \mathcal { O } } } ( m ^ { 3 } l + n m l + n m ^ { 2 } \bar { + } m l \bar { n } + n c l + n \bar { l } +$ $n l \log ( n ) + m l + m l \log ( m ) )$ , which is linear to sample number $n$ since $\log ( n ) \ll n$ .

Algorithm 1: TPCH algorithm   

<html><body><table><tr><td>Input: Bipartite graphs {(Xp)}p=1 and Parameter α. Output: Clustering results.</td></tr><tr><td></td></tr><tr><td>1: repeat</td></tr><tr><td>2: Update Qp via Eq. (5) ;</td></tr><tr><td>3: Update Bp via Eq. (6) ;</td></tr><tr><td>4: Update A via Eq. (9) ;</td></tr><tr><td>5: Update £ similar to A; 6: until Satisfy convergence.</td></tr></table></body></html>

# Experiment

In this section, we introduce the datasets, comparisons, evaluation metrics, and experiment analysis. To assess the performance of TPCH, we compare two categories of competitors, i.e., graph-based methods and hash-based methods. Additionally, we further evaluate parameter sensitivity analysis, convergence analysis, and ablation studies.

Table 1: Comparison results with graph-based and hash-based methods   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="8">Graph-based Methods 一</td><td colspan="10">Hash-based Methods</td></tr><tr><td>SC</td><td>Co- re-p</td><td>Co- AM re-c GL</td><td>Mul- NMF</td><td>ML AN</td><td>mPAC</td><td>GMC</td><td>SH</td><td>DSH</td><td>SP</td><td>ITQ</td><td>SGH</td><td>RS SH</td><td>RF DH</td><td>HS IC</td><td>BM VC</td><td>AC- GC MVBC AE</td><td>TP CH</td></tr><tr><td></td><td colspan="8">1</td><td colspan="8">Acc</td></tr><tr><td>SUNRGBD</td><td>0.11</td><td>0.18|0.180.10</td><td></td><td>Acc 0.14 0.19</td><td></td><td>0.13</td><td>0.22 0.12</td><td colspan="18">0.16 0.19 0.19 0.18 0.16|0.170.16|0.14</td></tr><tr><td>Cifar-10</td><td>0.17</td><td>0.220.21</td><td>0.22</td><td>0.12</td><td>0.23</td><td>0.10</td><td>0.23</td><td>0.17</td><td>0.22 0.23</td><td>0.23</td><td></td><td>0.22</td><td>0.200.230.220.24</td><td></td><td></td><td></td><td>0.19 0.29</td><td>0.24|0.22 0.250.70</td><td></td></tr><tr><td>Caltech101</td><td>0.18</td><td>0.260.260.15</td><td></td><td>0.19</td><td>0.23</td><td>0.20</td><td>0.27</td><td>0.18</td><td>0.16</td><td>0.21</td><td>0.25</td><td>0.23</td><td></td><td>0.290.220.240.29</td><td></td><td></td><td>0.32</td><td></td><td>0.300.54</td></tr><tr><td>Caltech256</td><td>0.09</td><td>0.09</td><td>0.100.05</td><td>0.06</td><td>0.08</td><td>0.09</td><td>0.07</td><td>0.06</td><td>0.08</td><td>0.09</td><td>0.09</td><td>0.08</td><td>0.09</td><td></td><td>0.080.07</td><td>0.09</td><td></td><td></td><td>0.13</td></tr><tr><td>100 leaves</td><td>0.49</td><td>0.730.790.76</td><td></td><td>0.87</td><td>0.74</td><td>0.82</td><td>0.43</td><td>0.47</td><td>0.45</td><td>0.48</td><td>0.46</td><td>0.51</td><td>0.360.450.660.50</td><td></td><td></td><td></td><td>0.05 0.83</td><td>0.11 0.890.86</td><td></td></tr><tr><td></td><td colspan="8">NMI</td><td colspan="8">NMI</td><td></td></tr><tr><td>SUNRGBD</td><td colspan="8">0.01 0.210.220.19 0.21 0.07 0.22 0.13 0.22</td><td colspan="8">0.22 0.21 0.22 0.200.200.220.16 0.24</td><td>0.220.28</td></tr><tr><td>Cifar-10</td><td>0.077 0.09</td><td></td><td>0.090.09</td><td>0.10 0.02</td><td>0.09</td><td>0.01</td><td>0.10</td><td>0.03 0.09</td><td>0.10</td><td>0.10</td><td>0.10</td><td></td><td>0.070.100.090.10</td><td></td><td></td><td></td><td>0.13</td><td>0.100.67</td></tr><tr><td>Caltech101</td><td>0.18</td><td>0.260.260.15</td><td></td><td>0.19</td><td>0.23</td><td>0.20</td><td>0.27</td><td>0.33 0.36</td><td>0.40</td><td>0.44</td><td>0.44</td><td></td><td>0.490.440.450.49</td><td></td><td></td><td>0.51</td><td></td><td>0.470.80</td></tr><tr><td>Caltech256</td><td>0.28</td><td>0.300.280.11</td><td></td><td>0.15</td><td>0.28</td><td>0.22</td><td>0.14</td><td>0.26 0.29</td><td>0.30</td><td>0.27</td><td>0.30</td><td></td><td>0.310.290.240.32</td><td></td><td></td><td>0.22</td><td></td><td>0.290.37</td></tr><tr><td>100 leaves</td><td>0.77</td><td>0.88 0.93</td><td>0.91</td><td>0.93</td><td>0.89</td><td>0.93</td><td>0.70</td><td>0.72 0.73</td><td>0.72</td><td>0.74</td><td>0.76</td><td></td><td>0.620.720.830.73</td><td></td><td></td><td>0.93</td><td>0.940.97</td><td></td></tr><tr><td></td><td colspan="8">Purity</td><td colspan="8">Purity</td><td></td></tr><tr><td>SUNRGBD 0.11</td><td colspan="8">0.330.34|0.11 0.16</td><td colspan="8">0.33 0.34 0.34 0.34 0.330.330.350.28 0.210.230.220.24</td><td colspan="2">0.340.37</td></tr><tr><td>Cifar-10</td><td>0.18</td><td>0.220.220.23</td><td></td><td>0.12</td><td>0.33 0.25</td><td>0.14 0.10</td><td>0.23 0.25</td><td>0.24 0.17 0.23</td><td>0.23</td><td>0.23</td><td>0.22</td><td></td><td></td><td></td><td></td><td></td><td>0.34</td><td>0.260.73</td></tr><tr><td>Caltech101</td><td>0.31</td><td>0.460.470.17</td><td></td><td>0.32</td><td>0.44</td><td>0.30</td><td>0.35</td><td>0.31 0.34</td><td>0.39</td><td>0.42</td><td>0.41</td><td></td><td>0.470.420.410.49</td><td></td><td></td><td>0.27 0.46</td><td></td><td>0.440.70</td></tr><tr><td>Caltech256</td><td>0.13</td><td>0.140.150.04</td><td></td><td>0.11</td><td>0.14</td><td>0.14</td><td>0.10</td><td>0.11 0.13</td><td>0.14</td><td>0.14</td><td>0.14</td><td></td><td>0.150.130.110.15</td><td></td><td></td><td>0.08</td><td></td><td></td></tr><tr><td>100 leaves</td><td>0.56</td><td>0.760.830.81</td><td></td><td>0.90</td><td>0.76</td><td>0.85</td><td>0.54</td><td>0.50 0.50</td><td>0.51</td><td>0.49</td><td>0.53</td><td>0.39</td><td>0.48</td><td></td><td>0.680.53</td><td>0.88</td><td>0.140.18 0.900.88</td><td></td></tr></table></body></html>

# Experimental Setting Five benchmark multi-view datasets include:

• SUNRGBD: This dataset consists of colour images and depth images of indoor scenes to provide rich information. This dataset comprises 37 categories, totalling 10,335 images.   
• Cifar-10: This dataset consists of colour images categorized into 10 different classes, each containing 6,000 32x32-pixel colour images.   
• Caltech101: Created by the California Institute of Technology, this dataset contains 101 object categories with approximately 9,000 images, including animals, plants, vehicles, and various other objects.   
• Caltech256: Similar to Caltech101, it comprises 256 categories, totaling approximately 30,607 images.   
• 100 leaves: This dataset includes 100 plant species with a total of 1,600 samples. Each sample consists of three views: shape, fine-scale margin, and texture histograms.

Comparison methods and setting: To evaluate the validity of our TPCH, two categories of competitors are employed: (1) Hash-based methods include SH (Weiss, Torralba, and Fergus 2008), DSH (Jin et al. 2013), SP (Xia et al. 2015), ITQ (Gong et al. 2012), SGH (Jiang and Li 2015), RSSH (Tian et al. 2020), RFDH (Wang, Wang, and Gao 2017), HSIC (Zhang et al. 2018a), BMVC (Zhang et al. 2018b), AC-MVBC (Zhang et al. 2022) and GCAE (Wang et al. 2023). (2) Graph based methods include SC $\mathrm { N g }$ , Jordan, and Weiss 2001), Co-regularize (Kumar, Rai, and Daume 2011), AMGL (Nie et al. 2016), Mul-NMF (Liu et al. 2013), MLAN (Nie, Cai, and Li 2017), mPAC (Kang et al. 2019), GMC (Wang, Yang, and Liu 2019) and GCAE. For these methods, we conduct experiments using publicly available source codes and parameter settings of the competitors. By comparing against a large number of advanced methods, we can thoroughly verify the effectiveness of our TPCH. In the experiment, five mainstream metrics are used to evaluate clustering performance: Accuracy (ACC), Normalized Mutual Information (NMI), Purity, F-score, and Adjusted Rand Index (ARI). Due to space limitations, their definitions, along with the experimental results of F-score and ARI, are provided in the supplementary materials. The hardware for running our experiments includes CPU: i9 14900k 24-core processor, RAM: 32GB 7200MHz, Platform: Windows 11, Software: MATLAB 2023b.

# Comparative Experimental Results

Table 1 reports the clustering performance of TPCH and various hash-based and graph-based clustering methods on five multi-view datasets, with the optimal results highlighted in bold. According to the results of the five clustering evaluation metrics, Table 1 demonstrates that:

• AC-MVBC, GCAE, and our TPCH outperform other methods significantly. The reasons may be that: 1) ACMVBC and our TPCH could capture the high-order information among multi-view data, which enhances the discriminability of hash codes; 2) GCAE and our TPCH encode the dynamic graph semantic information into hash codes to enlarge their quality.

• Compared to most recently GCAE and AC-MVBC,

TPCH also outperforms them consistently in most cases. The main reasons are that 1) our TPCH first captures the high-order information of data in dual projection and Hamming spaces to learn the more compact and discriminable hash representations as shown in Fig. 5 and Fig. 6. This phenomenon indicates that our enhanced tensor nuclear norm on dual projection and Hamming spaces could simultaneously promote the projection synergy, as well as inter-view and intra-view communication.

• Conventional multi-view clustering methods use Euclidean distance to measure the distance between two samples, which leads to inefficiency and high computational complexity. In contrast, hash-based methods obtain clustering results in the Hamming space, thereby improving computational efficiency.

Robust analysis: Fig. 3 reports the comparison experiment results of TPCH with AC-MVBC on the Caltech101 and Caltech256 datasets with salt-and-pepper noise. Experiments conducted on the SUNRGBD, Cifar-10 and 100 leaves datasets are provided in the Supplementary Materials. Through Fig. 3, it can be observed that TPCH is completely superior to AC-MVBC on five main clustering indicators. Similar to TPCH, AC-MVBC also uses tensor nuclear norm to capture the latent higher-order correlations in multi-view data, but it ignores the low-rank property of the core tensor, making existing tensor nuclear norm more susceptible to noise. More importantly, core tensor with high rank limits the higher-order synergistic between the projection matrices. By further explore the low-rank property of core tensor on dual projection and Hamming spaces, the projection cooperation and inter-view/intra-view communication would negotiate with each other to achieve the compact hash representation. Thus, our TPCH could enjoy more robustness to noise and better generate the more compact hash codes for improve the clustering performance· Furthermore, experiments on noisy datasets also verify the robustness of TPCH to noise as mentioned in Fig. 4.

![](images/f028aeaeab8e797d2ec01ad79bcdbd607e47a03ef88b976822a623be24404eb3.jpg)  
Figure 3: Clustering performance of AC MVBC and TPCH on the two datasets with salt-and-pepper noise.

Table 2 demonstrates the runtime of TPCH compared to the state-of-the-art method, GCAE, across five large-scale datasets. It is evident from Table 2 that TPCH significantly outperforms GCAE in terms of computational speed despite both methods being based on hashing. The time complexity of GCAE is $\mathcal { O } ( n ^ { 3 } + 3 n ^ { 2 } l + n ^ { 2 } + n l \bar { c } )$ , which implies cubic complexity with respect to the number of samples, whereas TPCH has a time complexity of $\mathcal { O } ( m ^ { 3 } l + n \dot { m } l + n m ^ { 2 } +$ $m l n + n c l + n l + n l \log ( n ) + m l + m l \log ( m ) )$ , indicating linear complexity for the number of samples. The lower time complexity grants TPCH higher computational efficiency.

In summary, TPCH not only excels in various metrics for executing multi-view clustering tasks but also significantly surpasses existing algorithms in computational efficiency, making it well-suited for rapidly processing largescale multi-view clustering tasks.

Table 2: Running time of GCAE and TPCH.   

<html><body><table><tr><td>Datasets</td><td>GCAE</td><td>TPCH</td><td>Speedup</td></tr><tr><td></td><td colspan="3">CPU-Time(s)</td></tr><tr><td>SUNRGBD</td><td>1188.33</td><td>157.62</td><td>7.54×</td></tr><tr><td>Cifar-10</td><td>1120.15</td><td>157.25</td><td>7.12×</td></tr><tr><td>Caltech101</td><td>1158.71</td><td>114.11</td><td>10.15×</td></tr><tr><td>Caltech256</td><td>47618.00</td><td>5406.76</td><td>8.81×</td></tr><tr><td>100 leaves</td><td>14.16</td><td>1.75</td><td>8.09×</td></tr></table></body></html>

# Ablation Studies

The TPCH model mainly includes two tensors: $\mathfrak { Q }$ and $\pmb { \mathscr { B } }$ . To further investigate why TPCH performs well, we remove $\mathfrak { Q }$ and $\pmb { \mathscr { B } }$ respectively, naming these methods as without (w/o) $\pmb { \mathscr { B } }$ and without (w/o) $\pmb { \mathscr { B } }$ , while conducting experiments with datasets processed with salt-and-pepper noise. The results of the ablation study are shown in Fig.4. From Fig. 4, we observe that the removal of $\mathfrak { Q }$ and $\pmb { \mathscr { B } }$ leads to a significant decline in clustering performance, verifying the robustness of proposed enhanced tensor nuclear norm on projection matrices and hash codes.

![](images/cb1f99d9cc88e2c910491e8e16c5b12fc8c9af39897bb1dbd4ff0f3c9f61bbe1.jpg)  
Figure 4: Ablation Studies of TPCH on the Caltech101 and Caltech256 datasets with salt-and-pepper noise.

# Visualization of Clustering Results

The visualization of clustering results for AC-MVBC and TPCH on the Synthetic 4clu datasets is presented in Fig.5. Different colors represent different clusters learned on the hash codes. We also visualized the clustering results on the Synthetic 3d dataset. Due to space constraints, we have included it in the supplementary materials.

![](images/2bf17a50f4ad0a0fdad86f466d48f938c65462527a3841b082c3f7a7ecaec669.jpg)  
Figure 5: Visualization of clustering results for TPCH and AC-MVBC on the Synthetic 4clu dataset.

Fig. 5(a) and Fig. 5(b) reveals that the clustering performance of TPCH on the Synthetic 4clu dataset is significantly better than AC-MVBC. Fig. 5(b) exhibits larger intercluster distances and smaller intra-cluster distances. Additionally, the clusters displayed in Fig. 5(b) are more compact, directly demonstrating the effectiveness of TPCH in improving hash code compactness.

Fig. 6 displays the performance of TPCH at the beginning and after convergence of clustering on the Synthetic 4clu and Synthetic 3d datasets. It can be seen that in both datasets, TPCH exhibits a clear clustering effect after convergence. It is noteworthy that, despite the more complex data distribution in Synthetic 3d compared to Synthetic 4clu, TPCH can still effectively partition the clusters. The stable clustering performance of TPCH under these two different initial data distributions once again proves the superiority of TPCH.

![](images/8e40fa4c5040d080b679682af9c3f65e016efa581c11a1c5874075ab2076ea30.jpg)  
Figure 6: Clustering performance of TPCH at the beginning and after convergence of clustering on the Synthetic 4clu and Synthetic 3d datasets.

![](images/617f97161ed52e9732af1aa54aaafa0eacc034b7b650b0c400c8573c690971eb.jpg)  
Figure 7: Sensitive analysis w.r.t. $\alpha$ for five large-scale datasets and convergence curve for the Caltech256 dataset.

# Sensitivity and Convergence Analysis

In TPCH, we set the dimensionality of the hash codes and the representation space to be 64. TPCH has only one hyperparameter, $\alpha$ , which adjusts the balance between the binary representation of multi-view data and tensor factorization. We employ a logarithmic search approach, varying $\alpha$ between $1 . 0 \mathrm { e } { \cdot } 8$ and 100 at each order of magnitude. Fig. 7 illustrates the sensitivity of different datasets to $\alpha$ across the clustering metrics ACC. The 100leaves dataset maintains high performance across all $\alpha$ . As shown in Fig. 7 (a), TPCH achieves satisfactory results over a wide range of $\alpha$ values, indicating insensitivity to parameters. For the sensitivity analysis of more metrics, please refer to supplementary materials. Fig. 7 (b) illustrates the convergence curves of TPCH on the Caltech256 datasets, which demonstrates that TPCH converges very quickly. For the convergence of more metrics, please refer to supplementary materials.

# Conclusion

In conclusion, this work introduces a novel Tensorinteracted Projection and Cooperative Hashing for Multiview Clustering(TPCH) approach for multi-view clustering, which effectively addresses the limitations of existing methods by considering the synergistic interactions and communications during the projection process. This innovative approach significantly enhances the compactness and distinguishability of hash representations, thereby improving both the quality of clustering and the robustness against noise. Experimental results validate the effectiveness of TPCH in multi-view clustering scenarios.