# In-depth Analysis of Low-rank Matrix Factorisation in a Federated Setting

Constantin Philippenko, Kevin Scaman, Laurent Massouli´e

Inria Paris - De´partement d’informatique de l’ENS, PSL Research University firstname.lastname@inria.fr

# Abstract

We analyze a distributed algorithm to compute a low-rank matrix factorization on $N$ clients, each holding a local dataset $\mathbf { S } ^ { i } \in \mathbb { R } ^ { n _ { i } \times d }$ , mathematically, we seek to solve $\begin{array} { r } { m i n _ { \mathbf { U } ^ { i } \in \mathbb { R } ^ { n _ { i } \times r } , \mathbf { V } \in \mathbb { R } ^ { d \times r } } \frac { 1 } { 2 } \sum _ { i = 1 } ^ { N } \| \mathbf { S } ^ { i } - \mathbf { U } ^ { i } \mathbf { V } ^ { \top } \| _ { \mathrm { F } } ^ { 2 } } \end{array}$ . Considering a power initialization of $\mathbf { V }$ , we rewrite the previous smooth non-convex problem into a smooth strongly-convex problem that we solve using a parallel Nesterov gradient descent potentially requiring a single step of communication at the initialization step. For any client $i$ in $\{ 1 , \ldots , N \}$ , we obtain a global $\mathbf { V }$ in $\mathbb { R } ^ { d \times r }$ common to all clients and a local variable $\mathbf { \check { U } } ^ { i }$ in $\mathbb { R } ^ { n _ { i } \times r }$ . We provide a linear rate of convergence of the excess loss which depends on σmax/σr, where σr is the rth singular value of the concatenation S of the matrices $( \mathbf { S } ^ { i } ) _ { i = 1 } ^ { N }$ This result improves the rates of convergence given in the literature, which depend on ${ \sigma _ { \operatorname* { m a x } } ^ { 2 } } / { \sigma _ { \operatorname* { m i n } } ^ { 2 } }$ . We provide an upper bound on the Frobenius-norm error of reconstruction under the power initialization strategy. We complete our analysis with experiments on both synthetic and real data.

# Code — https://github.com/philipco/matrix factorization

# 1 Notation

For A a matrix in $\mathbb { R } ^ { n \times d }$ , we note rank (A) its rank, $\lambda _ { 1 } ( \mathbf { A } ) ~ \geq ~ \cdots ~ \geq ~ \lambda _ { \mathrm { r a n k } ( \mathbf { A } ) } ( \mathbf { A } )$ are its decreasing eigenvalues, $\sigma _ { 1 } ( \mathbf { A } ) ~ \geq ~ \cdots ~ \geq ~ \sigma _ { \mathrm { r a n k } ( \mathbf { A } ) } ( \mathbf { A } ) ~ \geq ~ 0$ positive and decreasing singular values. More specifically, we note $\lambda _ { \operatorname* { m a x } } ( \mathbf { A } ) , \lambda _ { \operatorname* { m i n } } ( \mathbf { A } )$ (resp. $\sigma _ { \mathrm { m a x } } ( \mathbf { A } ) , \sigma _ { \mathrm { m i n } } ( \mathbf { A } ) )$ the biggest/smallest eigenvalue (resp. singular value) of A. We define the condition number of $\mathbf { A }$ as $\begin{array} { r l } { \kappa ( \mathbf { A } ) } & { { } : = } \end{array}$ $\sigma _ { \operatorname* { m a x } } ( \mathbf { A } ) / \sigma _ { \operatorname* { m i n } } ( \mathbf { A } )$ . Furthermore, we note $\begin{array} { r l } { \| \mathbf { A } \| _ { \mathrm { F } } ^ { 2 } } & { { } : = } \end{array}$ $\begin{array} { r } { \sqrt { \mathrm { T r } \left( \mathbf { A } \mathbf { A } ^ { \top } \right) } = ( \sum _ { i = 1 } ^ { \operatorname* { m i n } \{ n , d \} } \sigma _ { i } ^ { 2 } ( \mathbf { A } ) ) ^ { 1 / 2 } } \end{array}$ the Frobenius norm and $\| \mathbf { A } \| _ { 2 } \ : = \ { \sqrt { \lambda _ { \operatorname* { m a x } } ( \mathbf { A } ^ { \top } \mathbf { A } ) } } \ = \ \sigma _ { \operatorname* { m a x } } ( \mathbf { A } )$ the operator norm induced by the 2-norm. The group of orthogonal matrices is denoted ${ \mathcal { O } } _ { d } ( \mathbb { R } )$ . For $a , b \in \mathbb { R }$ , we also denote $a \wedge b : = \operatorname* { m i n } ( a , b )$ .

# 2 Introduction

The problem of low-rank matrix factorization is widely analyzed in machine learning (e.g. Deshpande and Vempala 2006; Achlioptas and McSherry 2007; Liberty et al. 2007;

Nguyen, Do, and Tran 2009; Rokhlin, Szlam, and Tygert 2010; Halko, Martinsson, and Tropp 2011; Witten and Candes 2015; Tropp et al. 2019; Tropp and Webber 2023). Indeed, several key challenges can be reduced to it, for instance: clustering (Li and Ding 2006), features learning (Bisot et al. 2017), dictionary learning (Mensch et al. 2016), anomaly detection (Tong and Lin 2011), denoising (Wilson et al. 2008), or matrix completion (Jain, Netrapalli, and Sanghavi 2013). Let $\mathbf { S }$ in $\mathbb { R } ^ { n \times d }$ and $r \in \mathbb { N } ^ { * }$ , the study of the low-rank matrix factorization (MF) problem corresponds to find $\mathbf { U } \in \mathbb { R } ^ { n \times r }$ and $\mathbf { V } \in \mathbb { R } ^ { d \times r }$ minimizing:

$$
\frac 1 2 \left. \mathbf S - \mathbf U \mathbf V ^ { \top } \right. _ { \mathrm { F } } ^ { 2 } : = F ( \mathbf U , \mathbf V ) \le \epsilon _ { \mathrm { m i n } } + \epsilon ,
$$

where $\epsilon _ { \mathrm { m i n } }$ is the minimal achievable error w.r.t. the Forbenius-norm, $\epsilon$ is the error induced by the algorithm, and $r$ denote the latent dimension. The Eckart and Young (1936) theorem shows that $\textstyle \sum _ { i > r } \sigma _ { i } ^ { 2 }$ (we omit to indicate the matrix S for its eigen/singular values) is the minimal Frobeniusnorm error when approximating S with a rank- $r$ matrix, and it is $\sigma _ { r + 1 }$ for the 2-norm (Mirsky 1960).

Proposition 1. We have for the Frobenius-norm $\begin{array} { r l r } { \operatorname* { m i n } _ { \mathbf { U } , \mathbf { V } \in \mathbb { R } ^ { n \times r } \times \mathbb { R } ^ { d \times r } } \left\| \mathbf { S } - \mathbf { U V } ^ { \top } \right\| _ { \mathrm { F } } ^ { 2 } } & { { } = } & { \sum _ { i > r } \sigma _ { i } ^ { 2 } } \end{array}$ and for the 2-norm $\begin{array} { r } { \operatorname* { m i n } _ { \mathbf { U } , \mathbf { V } \in \mathbb { R } ^ { n \times r } \times \mathbb { R } ^ { d \times r } } \| \mathbf { S } - \mathbf { U V } ^ { \top } \| _ { 2 } = \sigma _ { r + 1 } } \end{array}$ .

Proposition 1 corresponds to the thin SVD decomposition (Eckart and Young 1936) and to keeping the $r$ largest singular values, the error is thus determined by the $n \wedge d - r$ smallest singular values of the spectrum of $\mathbf { S }$ .

Contemporary machine learning problems frequently involve data spread across multiple clients, each holding a subset of the dataset, thereby introducing an additional layer of complexity. In this paper, we consider the federated setting (Konecˇn´y et al. 2016; McMahan et al. 2017) where a client $i$ holds a dataset $\mathbf { S } ^ { i }$ in $\mathbb { R } ^ { n _ { i } \times d }$ with $n _ { i }$ in $\mathbb { N }$ rows and $d$ constant features across the $N$ in $\mathbb { N }$ clients. We want to factorize for any client $i$ its dataset based on a global shared variable $\mathbf { V }$ in $\dot { \mathbb { R } } ^ { d \times r }$ and a local personalized variable $\mathbf { U } ^ { i }$ in $\mathbb { R } ^ { n _ { i } \times r }$ , Equation (MF) can thus be rewritten as minimizing:

$$
\frac { 1 } { 2 } \sum _ { i = 1 } ^ { N } \left\| \mathbf { S } ^ { i } - \mathbf { U } ^ { i } \mathbf { V } ^ { \top } \right\| _ { \mathrm { F } } ^ { 2 } : = \sum _ { i = 1 } ^ { N } F ^ { i } ( \mathbf { U } ^ { i } , \mathbf { V } ) .
$$

We note $\mathbf { S }$ and $\mathbf { U }$ the vertical concatenation of matrices $( \mathbf { S } ^ { i } , \mathbf { U } ^ { i } ) _ { i = 1 } ^ { N }$ . We write the SVD of $\mathbf { S } = \mathbf { U } _ { * } \pmb { \Sigma } \mathbf { V } _ { * } ^ { \top }$ where $\mathbf { U } _ { * }$ in $\mathbb { R } ^ { n \times n }$ $\begin{array} { r } { \mathrm { ( } n \ = \ \sum _ { i = 1 } ^ { N } n _ { i } ) } \end{array}$ is the left basis vectors s.t. $\mathbf { U } _ { * } ^ { \top } \mathbf { U } _ { * } ~ = ~ \mathbf { I } _ { r }$ , $\mathbf { V } _ { * }$ nP $\mathbb { R } ^ { d \times d }$ is the right basis vectors s.t. $\mathbf { V } _ { * } ^ { \top } \mathbf { V } _ { * } = \mathbf { I } _ { r }$ , and $\pmb { \Sigma } = \mathrm { D i a g } \left( \lambda _ { 1 } ( \mathbf { S } ) , \ldots , \lambda _ { n \wedge d } ( \mathbf { S } ) \right)$ in $\mathbb { R } ^ { n \times d }$ contains the singular value of S. We consider that there exists a true low-rank matrix $\mathbf { X }$ in $\mathbb { R } ^ { n \times d }$ and a white noise $\mathbf { E }$ in $\mathbb { R } ^ { n \times d }$ s.t. $\mathbf { S } = \mathbf { X } + \mathbf { E }$ , which implies $r _ { * } = \mathrm { r a n k } \left( \mathbf { X } \right) \leq$ rank (S) $\leq \operatorname* { m i n } ( n , d )$ . The $^ { r _ { * } }$ first eigenvalues of S correspond to the signal (potentially noised if $\mathbf { E } \neq 0 ,$ and the $n \wedge d - r _ { * }$ last correspond to the white noise from $\mathbf { E }$ .

# Remark 1. In distributed settings, the errors $\epsilon _ { \mathrm { m i n } }$ and ϵ are cdleitenrtms’inmeadtrbiyc eths $( \bar { \mathbf { S } } _ { i } ) _ { i = 1 } ^ { N }$ u.m of S, and not by the spectrum of

Our work progresses in two directions: (1) offering new theoretical insights on matrix factorization and (2) developing a robust algorithm suited for federated environments, the challenge being to design a distributed algorithm that computes a low-rank matrix factorization of S with minimal communication/computation overhead, while guaranteeing a linear convergence towards the reconstruction error $\epsilon _ { \mathrm { m i n } } + \epsilon$ . To tackle this challenge, we combine a global distributed randomized power iteration (Halko, Martinsson, and Tropp 2011) with local gradient descents on each client, bridging these two lines of research.

The randomized power iteration method (Halko, Martinsson, and Tropp 2011; Hardt and Price 2014; Li et al. 2021a; Saha, Srivastava, and Pilanci 2023) is one of the main approaches used in practice1 to compute low-rank matrix factorizations, therefore. Theoretically, the authors have provided in a centralized setting, solely $i f$ taking $r \leq r _ { * } + 2$ , a bound on the 2-norm for $\alpha$ in $\mathbb { N }$ proportional to $r$ . In this work, we are interested in giving new results in the federated setting on the Frobenius-norm and $r$ in $\{ 1 , \dots , n _ { i } \land d \}$ . Indeed, using the existing results on the 2-norm to derive results on the Frobenius norm would result in an undesirable $\sqrt { d }$ -factor to both $\epsilon _ { \mathrm { m i n } }$ and $\epsilon$ . Thus, the results on the two different norms are not directly comparable and ask for a new analysis.

On the other hand, literature based on gradient descent (Jain et al. 2017; Zhu et al. 2019; Ye and Du 2021; Jain, Netrapalli, and Sanghavi 2013; Ward and Kolda 2023; Gu et al. 2023) usually provides results on the Frobenius norm with linear convergence rates depending on $\kappa ^ { 2 } ( \mathbf { S } )$ (Ward and Kolda 2023), which might be arbitrarily large, thus hindering the convergence. The primary challenge with this approach lies in the non-convexity of $F$ , rendering the solution space for Equation (MF) infinite and impeding its theoretical analysis. Numerous research endeavors are directed towards these algorithms with the aims of (1) enhancing the theoretical convergence rate, (2) writing elegant and concise proofs, and (3) understand the underlying mechanism that allows for convergence. More generally, advancements in understanding Equation (MF) can significantly enrich our comprehension of other non-convex problems such as matrix sensing or deep learning optimization (Du, Hu, and Lee 2018).

In Section 3, we conduct a short overview of the related work. In Section 4, we propose and analyze a parallel algorithm derived from Halko, Martinsson, and Tropp (2011), we explain the associated computational/communicational trade-offs, exhibit a linear rate of convergence, and provide an upper bound in probability of both the condition number $\kappa ( \mathbf { S } )$ and the Frobenius-norm error. In Section 5 we illustrate our theoretical findings with experiments on both synthetic and real data, and finally, in Section 6, we conclude by highlighting the main takeaways of this article and by listing a few open directions.

Contributions. We make the following contributions.

• We solve the low-rank matrix factorization problem with a local personalized variable $\mathbf { U } ^ { i }$ in $\mathbb { R } ^ { n _ { i } \times r }$ and a global shared $\mathbf { V }$ in $\mathbb { R } ^ { r \times r }$ s.t. $\mathbf { S } ^ { i } \approx \mathbf { U } ^ { i } \mathbf { V } ^ { \top }$ for any client $i$ in $\{ 1 , \ldots , N \}$ . Following Halko, Martinsson, and Tropp (2011), we use a power method to initialize $\mathbf { V }$ , then we run a gradient descent on each client to compute $\mathbf { U } ^ { i }$ .   
• We provide a novel result regarding the Frobenius-norm of $\bar { \mathbf { S } } - \mathbf { U } \mathbf { V } ^ { \top }$ . Our result is non-trivial as it was lacking in Halko, Martinsson, and Tropp (2011, see Remark 10.1). This result provides new insight on Equation (MF) and can not be compared for $\alpha \geq 1$ to Halko, Martinsson, and Tropp (2011); Hardt and Price (2014) or Li et al. (2021a) which analyze different quantities.   
• In the distributed setting, under low noise conditions, unlike Li et al. (2021a), we can achieve a finite number of communications in $\begin{array} { r } { \Omega \left( \frac { \log \left( \sigma _ { \operatorname* { m a x } } d r _ { * } ^ { 2 } \epsilon ^ { - 1 } \right) } { \log \left( \sigma _ { r _ { * } } \right) - \log \left( \sigma _ { r _ { * } + 1 } \right) } \right) } \end{array}$ . And potentially a single communication stage is enough, i.e., our algorithm is possibly embarrassingly parallel.   
• Algorithmically, our method involves sampling different random Gaussian matrices $\Phi$ to obtain better condition numbers, this sampling allows rapid gradient descent, independently of $\sigma _ { \mathrm { m i n } }$ . It increases the average number of exchanges by a factor $m$ , yet it ensures convergence almost surely. We are the first to propose such a result for (MF) problems.   
• Compared to existing literature on (distributed) gradient descent (e.g. Zhu et al. 2019; Ward and Kolda 2023) our approach surpasses them both in terms of communicational and computational complexity. Our guarantee of convergence are stronger and our analysis bridges the gap between works on distributed gradient descent and on the power method. Besides, our proof of convergence is simpler as it simply requires to show that the problem is smooth and strongly-convex.   
• All the theory from strongly-convex gradient descent apply to our problem. Thereby, we easily introduce acceleration – being the first to do so – outperforming the convergence rates for simple gradient descent.

# 3 Related Works

In this Section, we describe the distributed randomized power iteration and review related works on gradient descent.

Algorithm 1: Distributed Randomized Power Iteration   

<html><body><table><tr><td>Input: Number of iteration α in N. Output: Global variable V in Rd× r for each client i in{1,...,N} do Generate a Gaussian matrix Φ in Rni×r. Compute Vi = (Si)TΦi. Share Vi.</td></tr><tr><td>Compute V =∑1 Vi using a secure aggregation protocol. Share V with all clients.</td></tr><tr><td>fora∈{1,...,α}do foreach clientiin{1,...,N}do Compute Vi = (Si)TSiV.</td></tr><tr><td>Share Vi. Compute V = ∑1 Vi using a secure</td></tr></table></body></html>

The power method (Mises and Pollaczek-Geiringer 1929) is a simple iterative algorithm used to approximate the dominant eigenvalue and the corresponding eigenvector of a square matrix by repeatedly multiplying the matrix by a vector and normalizing the result. To improve scalability to large problems, Halko, Martinsson, and Tropp (2011) have proposed Randomized $s V D$ a technique that sample a random Gaussian matrix $\Phi$ in $\mathbb { R } ^ { n \times r }$ , multiply it by $( \bar { \mathbf { S } } ^ { \top } \mathbf { S } ) ^ { \alpha } \mathbf { S } ^ { \top }$ ( $\overset { \cdot } { \alpha }$ in $\mathbb { N }$ ) to obtain a matrix $\mathbf { V }$ . While authors never mention the distributed setting, the adaptation is straightforward and we give the pseudo-code to compute $\mathbf { V }$ in Algorithm 1. Note that to avoid storing a $d \times d$ matrix, we compute the product $( \mathbf { S } ^ { \top } \mathbf { S } ) ^ { \alpha } \mathbf { S } ^ { \top } \Phi$ from right to left, resulting in the computation of a $d \times r$ matrix. The total number of computational operations is $( 2 \alpha + 1 ) n d r + ( \alpha + 1 ) d r$ . Next, the authors either construct a QR factorization of $\mathbf { V }$ to obtain an orthonormal matrix factorization of S, or compute its SVD decomposition to get the singular values.

Remark 2 (Secure aggregation). The distributed power iteration (Algorithm 1) requires a federated setting with a central server that can perform a secure aggregation (Bonawitz et al. 2017) of the local $( \mathbf { V } ^ { i } ) _ { \{ 1 , . . . , N \} }$ . Extending to the decentralized setting is an interesting but out of scope direction.

The idea of using gradient descent to solve low-rank matrix factorization can be traced back to Burer and Monteiro (2003, 2005). Several theoretical results and convergence rate have been provided in the recent years (Zhao, Wang, and Liu 2015; Tu et al. 2016; Zheng and Lafferty 2016; Jain et al. 2017; Chen et al. 2019; Du, Hu, and Lee 2018; Ye and $\mathrm { D u } 2 0 2 1$ ; Jiang, Chen, and Ding 2023). Ward and Kolda (2023) has a rate depending on $\kappa ^ { \bar { - } 2 } ( \mathbf { S } )$ using a power initialization with $\alpha = 0$ but only in the case of a low-rank matrix, for which it is known that we can have in fact a zero error (Proposition 1). In contrast, our analysis in a strongly-convex setting allows, first, to plugging in any faster algorithms than simple gradient descent and thus to obtain a faster rate depending on $\kappa ^ { - 1 }$ , and second, holds in the more general setting of a full-rank matrix.

Note that Ward and Kolda (2023) do not mention the FL setting, however, their work naturally adapts itself to this setting as we have $\begin{array} { r } { \frac 1 2 \sum _ { i = 1 } ^ { N } \left\| \mathbf { S } ^ { i } - \mathbf { U } ^ { i } \mathbf { V } ^ { \top } \right\| _ { \mathrm { F } } ^ { 2 } = \frac 1 2 \| \mathbf { S } - \mathbf { U } \mathbf { V } ^ { \top } \| _ { \mathrm { F } } ^ { 2 } } \end{array}$ . On the other hand, it res
ults in a high
communication cost, given that it requires sharing at each iteration $k$ in $\mathbb { N }$ the matrix $\mathbf { V } _ { k }$ to compute the gradient (but does not require sharing $\mathbf { U } _ { k }$ which remains local), resulting in a communication cost of $O ( K r d )$ , where $K$ is the total number of iterations/communications. Besides, $K$ depends on the condition number and thereby might be very large. On the contrary, our work and the one of Halko, Martinsson, and Tropp (2011) requires small (and potentially a single) communication steps.

Most of the articles extending gradient descent to the distributed setting (Hegedu˝s et al. 2016; Zhu et al. 2019; Hegedu˝s, Danner, and Jelasity 2019; Li et al. 2020, 2021b; Wang and Chang 2022) consider an approach minimizing the problem over a global variable $\mathbf { V }$ and personalized ones $( \mathbf { U } _ { i } ) _ { i = 1 } ^ { N }$ . This setup creates a global-local matrix factorization: V contains information on features shared by all clients (item embedding), while $( \hat { \mathbf { U } } ^ { i } ) _ { i = 1 } ^ { N }$ captures the unique characteristics of client $i$ in $\{ 1 , \ldots , N \}$ (user embeddings). We build on this approach in the next Section.

# 4 Practical Algorithm and Theoretical Analysis

In the next Subsections, we propose and analyze a parallel algorithm derived from Halko, Martinsson, and Tropp (2011) that combines power method and gradient descent.

# 4.1 Combining Power Method with Parallel Gradient Descent

In order to solve (Dist. MF), a natural idea is to fix the matrix $\mathbf { V }$ shared by all clients, and then compute locally the exact solution of the least-squares problem or run a gradient descent with respect to the variable U. All computation are thus performed solely on clients once $\mathbf { V }$ is initialized. This requires a number of communications equal to $\alpha + 1$ as the only communication steps occur when initializing $\mathbf { V }$ . In the low-noise or the low-rank regimes, taking $\alpha = 0$ allows to obtain $\epsilon = 0$ (Corollary 1) and results to an embarrassingly parallel algorithm. Besides, parallelizing the computation allows to go from a computational cost in $O ( d r \bar { \sum _ { i = 1 } ^ { N } } n _ { i } )$ to $O ( d r n _ { i } )$ which is potentially much smaller. Below, we give the optimal solution of (Dist. MF) for a fixed $\mathbf { V }$ .

Proposition 2 (Optimal solution for a fixed matrix $\mathbf { V }$ ). Let $\mathbf { V } \in \mathbb { R } ^ { d \times r }$ be $a$ fixed matrix, then Equation (Dist. MF) is minimized for $\hat { \mathbf { U } } ^ { i } = \mathbf { S } ^ { i } \mathbf { V } ( \mathbf { V } ^ { \top } \mathbf { V } ) ^ { \dagger }$ .

The main challenge in computing $( \hat { \mathbf { U } } ^ { i } ) _ { i = 1 } ^ { N }$ is to (pseudo)inverse $\mathbf { V } ^ { \top } \mathbf { V }$ , which is known to be unstable under small numerical errors and potentially slow. We propose instead to do a local gradient descent on each client $i$ in $\left\{ 1 , \ldots , N \right\}$ in order to approximate the optimal ${ \hat { \mathbf { U } } } ^ { i }$ minimizing $F ^ { i } ( \cdot , \mathbf { V } )$ . It results to a parallel algorithm that does not require any communication after the initialization of $\mathbf { V }$ . We give in Algorithm 2 the pseudo-code: the server requires to do exactly $\mathbf { \bar { \boldsymbol { N } } } ( \alpha + 1 ) d r \dot { + } d r ^ { 2 }$ computational operations and the client $i \in \{ 1 , \ldots , N \}$ carries out $( 4 T + 2 \alpha + 2 ) n _ { i } d r$ operations.

Algorithm 2: GD w.r.t. U with a power init.   

<html><body><table><tr><td>Input: Number of iteration α in N, step-size γ. Output: (Ui)N=1: Run Algorithm 1 to compute V = (STS)αSTΦ for each client i in {1,...,N} without any communication do Sample a random matrix Ui in Rni ×r. fort∈{1,...,T}do Compute</td></tr></table></body></html>

This approach offers much more numerical stability, allows for any kind of regularization, ensures strong guarantees of convergence, and explicit the exact number of epochs required to reach a given accuracy. A simple analysis of gradient descent in a smooth strongly-convex setting leads to a linear convergence toward a global minimum of the function $F ^ { i } ( \cdot , \mathbf { V } )$ with a convergence rate equal to $1 - \kappa ^ { - 2 } ( \mathbf { V } )$ , or even to $\mathbf { \bar { 1 } } - \kappa ^ { - 1 } ( \mathbf { V } )$ if a momentum is added.

Remark 3 $\langle L _ { * } / L _ { 1 } / L _ { 2 }$ -regularization.). We can use regularization on $\mathbf { U }$ with various norms: nuclear norm which yields a low-rank U, $L _ { 1 }$ -norm resulting in a sparse U, and $L _ { 2 }$ -norm leading to small values. The nuclear regularization requires to compute the SVD of U to compute the gradient (Avron et al. 2012), which generates an additional $O ( n r \log r )$ complexity.

# 4.2 Rate of Convergence of Algorithm 2

The cornerstone of the analysis relies on using power initialization (Algorithm 1), which forces having $\mathbf { V }$ in the column span of S. Besides, once $\mathbf { V }$ is set by Algorithm 1, for any client $i$ in $\{ 1 , \ldots , N \}$ , $F ^ { i } ( \cdot , \mathbf { V } )$ is $L$ -smooth and $\mu$ -stronglyconvex as proved in the following properties.

Property 1 (Smoothness). Let $\mathbf { V }$ in $\mathbb { R } ^ { d \times r }$ initialized by $A l .$ - gorithm $\boldsymbol { { \mathit { 1 } } }$ , then all $\left( F ^ { i } ( \cdot , \mathbf { V } ) \right) _ { i = 1 } ^ { N }$ are $L$ -smooth, i.e., for any i in $\{ 1 , \ldots , N \}$ , for any $\mathbf { U } , \mathbf { U } ^ { \prime }$ in $\mathbb { R } ^ { n _ { i } \times d }$ , we have $\| \nabla F ^ { i } ( \mathbf { U } , { \bf V } ) - \nabla { \dot { F } } ^ { i } ( \mathbf { U } ^ { \prime } , \mathbf { V } ) \| _ { \mathrm { F } } \ \leq \ L \| \mathbf { U } - \mathbf { U } ^ { \prime } \| _ { \mathrm { F } } ,$ , with $L = \sigma _ { \mathrm { m a x } } ^ { 2 } ( \mathbf { V } )$ .

Proof. Let $i$ in $\{ 1 , \ldots , N \}$ and $\mathbf { U } , \mathbf { U } ^ { \prime }$ in $\mathbb { R } ^ { n _ { i } \times r }$ , we have that:

$$
\begin{array} { r l } & { \| \nabla F ( \mathbf { U } , { \mathbf V } ) - \nabla F ( \mathbf { U } ^ { \prime } , { \mathbf V } ) \| _ { \mathrm { F } } = \| ( \mathbf { U } - \mathbf { U } ^ { \prime } ) { \mathbf V } ^ { \top } { \mathbf V } \| _ { \mathrm { F } } } \\ & { \qquad \mathrm { P r o p . ~ } \mathbf { S } 2 } \\ & { \qquad \leq \sigma _ { \operatorname* { m a x } } ( { \mathbf V } ^ { \top } { \mathbf V } ) \| { \mathbf U } - { \mathbf U } ^ { \prime } \| _ { \mathrm { F } } } \\ & { \qquad = \sigma _ { \operatorname* { m a x } } ^ { 2 } ( { \mathbf V } ) \| { \mathbf U } - { \mathbf U } ^ { \prime } \| _ { \mathrm { F } } . \bigtriangledown } \end{array}
$$

Property 2 (Strongly-convex). Let $\mathbf { V }$ in $\mathbb { R } ^ { d \times r }$ initialized by Algorithm $\boldsymbol { { \mathit { 1 } } }$ , then all $\left( F ^ { i } ( \cdot , \mathbf { V } ) \right) _ { i = 1 } ^ { N }$ are $\mu$ -strongly-convex, i.e., for any $i$ in $\{ 1 , \ldots , N \} _ $ , for any $\mathbf { U } , \mathbf { U } ^ { \prime }$ in $\mathbb { R } ^ { n _ { i } \times d }$ , we have $\begin{array} { r } { \langle \nabla F ^ { i } ( { \mathbf { U } } , { \mathbf { V } } ) - \nabla F ^ { i } ( { \mathbf { U } } ^ { \prime } , { \mathbf { V } } ) , { \mathbf { U } } - { \mathbf { U } } ^ { \prime } \rangle \geq \mu \Vert { \mathbf { U } } - { \mathbf { U } } ^ { \prime } \Vert _ { { \mathrm { F } } } ^ { 2 } } \end{array}$ with $\mu = \sigma _ { \mathrm { m i n } } ^ { 2 } ( \mathbf { V } )$ .

Proof. Let $\mathbf { \chi } _ { i }$ in $\{ 1 , \ldots , N \}$ and $\mathbf { U } , \mathbf { U } ^ { \prime }$ in $\mathbb { R } ^ { n _ { i } \times r }$ , we have that:

$$
\begin{array} { r l } & { \langle \nabla F ^ { i } ( { \mathbf { U } } , { \mathbf { V } } ) - \nabla F ^ { i } ( { \mathbf { U } } ^ { \prime } , { \mathbf { V } } ) , { \mathbf { U } } - { \mathbf { U } } ^ { \prime } \rangle } \\ & { \qquad = \mathrm { T r } \left( ( { \mathbf { U } } - { \mathbf { U } } ^ { \prime } ) ^ { \top } ( { \mathbf { U } } - { \mathbf { U } } ^ { \prime } ) { \mathbf { V } } ^ { \top } { \mathbf { V } } \right) } \\ & { \qquad = \| ( { \mathbf { U } } - { \mathbf { U } } ^ { \prime } ) { \mathbf { V } } ^ { \top } \| _ { \mathrm { F } } ^ { \mathrm { p r o p . ~ } { \mathbf { S } } 2 } \sigma _ { \mathrm { m i n } } ^ { 2 } ( { \mathbf { V } } ) \| { \mathbf { U } } - { \mathbf { U } } ^ { \prime } \| _ { \mathrm { F } } ^ { 2 } . \bigtriangledown } \end{array}
$$

This proves that the surrogate objective functions $( F ^ { i } ( \cdot , \mathbf { V } ) ) _ { i = 1 } ^ { N }$ are $\mu$ -strongly-convex, with $\mu \geq 0$ . As discussed in Theorem 2, as long as $r <$ rank (S), we have with high probability $\mu > 0$ . This is always true in the noisy setting (full rank); otherwise, we simply need to decrease $r$ .

Properties 1 and 2 allows to use classical results in optimization and draws a linear rate of convergence depending on $\mu / L$ , or $\sqrt { \mu / L }$ if we use acceleration.

Theorem 1. Under the distributed power initialization $( A l -$ gorithm 1), considering Properties $\jmath$ and 2, let $T$ in $\mathbb { N } ^ { * }$ , $\gamma =$ $1 / L ,$ , then after running Algorithm 2 for $T$ iterations, the excess loss function is upper bounded: $F ^ { i } ( \mathbf { U } _ { T } ^ { i } ) - F ^ { i } ( \hat { \mathbf { U } } ^ { i } ) \leq$ $\left( 1 - \mu / L \right) ^ { T } \left( F ^ { i } ( \mathbf { U } _ { 0 } ^ { i } ) - F ^ { i } ( \hat { \mathbf { U } } ^ { i } ) \right)$ , where $\mu / L = \kappa ^ { - 2 } ( { \bf V } )$ . Using Nesterov momentum, this rate is accelerated to: $F ^ { i } ( \bar { \mathbf { U } } _ { T } ^ { i } ) - F ^ { i } ( \hat { \mathbf { U } } ^ { i } ) \leq 2 ( 1 - \sqrt { \mu / L } ) ^ { T } ( F ^ { i } ( \mathbf { U } _ { 0 } ^ { i } ) - F ^ { i } ( \hat { \mathbf { U } } ^ { i } ) )$ . Remark 4 (Convergence in a single iteration). If we orthogonalize $\mathbf { V }$ , then we can converge in one iteration as gradient descent reduces to Newton method. Our algorithm would be equivalent to the one proposed by Halko, Martinsson, and Tropp (2011). However, orthogonalizing $\mathbf { V }$ requires to compute the SVD or to run a gradient descent on $\mathbf { V }$ .

Theorem 1 establishes that $( F ^ { i } ( \mathbf { U } _ { t } ^ { i } , \mathbf { V } ) ) _ { t \in \mathbb { N } ^ { * } }$ converges to $F ^ { i } ( \hat { \mathbf { U } } ^ { i } , \mathbf { V } )$ at a linear rate dominated by $\exp ( - \kappa ^ { - 1 } ( \mathbf { V } ) )$ or $\exp ( - \kappa ^ { - 2 } ( \mathbf { V } ) )$ . A question then emerges, can we control $\kappa ( \mathbf { V } )$ ? This parameter plays a pivotal role in determining the convergence rate and is affected by the sampled matrix $\Phi$ . Consequently, an ill-conditioned matrix $\mathbf { V }$ may arise, significantly hindering convergence. The below corollary gives tarubomuonfdSinwphrioleb abeilnitgy onnd $\bar { \kappa } ^ { 2 } ( \mathbf { V } )$ nthoaft dhepseanmdsploend $( \Phi ^ { i } ) _ { i = 1 } ^ { N }$ .-

Theorem 2. Under the distributed power initialization $( A l -$ gorithm $^ { l }$ ), considering Properties $\jmath$ and 2, for any p in $] 0 , 1 [$ , with probability at least $\mathrm { 1 - 3 p }$ , we have $\mathbf { \bar { \Sigma } } _ { K } ( \mathbf { V } ) ^ { 2 } < \mathbf { \bar { \Sigma } } \kappa _ { \mathrm { p } } ^ { 2 }$ , with:

$$
\kappa _ { \mathrm { p } } ^ { 2 } : = \frac { 1 } { \mathrm { p } ^ { 2 } } \left( 9 r ^ { 2 } \frac { \sigma _ { \mathrm { m a x } } ^ { 2 ( 2 \alpha + 1 ) } } { \sigma _ { r } ^ { 2 ( 2 \alpha + 1 ) } } + 4 r \left( d + \log ( 2 \mathrm { p } ^ { - 1 } ) \right) \frac { \sigma _ { r + 1 } ^ { 2 \alpha } } { \sigma _ { r } ^ { 2 \alpha } } \right) .
$$

With probability $\mathrm { \Delta P }$ , $i f$ we sample $m = \lfloor - \log _ { 2 } ( 1 - \mathrm { P } ) \rfloor$ independent matrices $( \Phi _ { j } ) _ { j = 1 } ^ { m }$ to form $\mathbf { V } _ { j } = \mathbf { S } ^ { \alpha } \boldsymbol { \Phi } _ { j }$ and run Algorithm 2, at least one initialization results to a convergence rate upper bounded by $1 - \kappa _ { 1 / 6 } ^ { - 2 }$ .

We can make the following remarks.   
• Impact of $\alpha$ . Increasing $\alpha$ enworse the rate of convergence as $\sigma _ { \operatorname* { m a x } } ^ { 2 } / \sigma _ { r } ^ { 2 } \geq 1$ . In the regime where $\sigma _ { \mathrm { m a x } } ^ { 2 }$ is very large and $\sigma _ { r } ^ { 2 } = \Omega ( 1 )$ , having $\alpha > 0$ drastically hinders the gradient descent convergence. However, in this case, it is possible to compute the exact solution of (Dist. MF), further, as emphasized in Corollary 1 which showcases the interest of having $\alpha \neq 0$ , increasing $\alpha$ allows reducing $\epsilon$ , i.e., the gap between the approximation error (induced by taking $r \leq$ rank (S)) and the minimal reconstruction error $\epsilon _ { \mathrm { m i n } }$ . Therefore, there is a trade-off associated with the choice of $\alpha$ ; we illustrate it in the experiments on three real datasets: mnist, w8a and celeba-200k.   
• Asymptotic values of $\kappa ( \mathbf { V } )$ . In the regime $\sigma _ { \mathrm { m a x } } , \sigma _ { r } ~ =$ $O ( 1 )$ and $\sigma _ { r + 1 } \ll 1$ , we have $1 - \kappa _ { \mathrm { p } } ^ { - 2 } \stackrel { \textstyle \cdot } { = } O ( 1 - p ^ { 2 } / \mathrm { r } ^ { 2 } )$ . Further, by employing acceleration techniques, we have an improved rate depending on $r ^ { - 1 }$ and not $\dot { r } ^ { - 2 }$ .   
• Ill-conditioned matrix. Even if the matrix $\mathbf { s }$ is illconditioned, the rate of convergence does not suffer from σmin.   
• Result in probability. The bound on $\kappa ^ { 2 } ( \mathbf { V } )$ is given with high probability $1 - 3 \mathrm { p }$ , which flows from the concentration inequalities proposed by Davidson and Szarek (2001) and Vershynin (2012) on the largest/smallest eigenvalues of a random Gaussian matrix.   
• Rotated matrix. Note that the probability p is taken not on $\Phi$ but on the rotated matrix $\bar { \hat { \Phi } } = \mathbf { U } _ { * } ^ { \top } \Phi$ .   
• Almost sure convergence. We propose to sample several matrix $\Phi$ until the condition number of $( \mathbf { S } ^ { \top } \dot { \mathbf { S } } ) ^ { \alpha } \mathbf { S } ^ { \top } \Phi$ is good enough. By leveraging theory on random Gaussian matrix, we can compute the number of sampling $m$ to achieve this bound on $\kappa _ { \mathrm { p } }$ with probability p. Alternatively, we can repeatedly sample $\Phi$ until the condition number of $\mathbf { V }$ falls below $\kappa _ { \mathrm { p } }$ . Since we know that this is achievable within a finite number of samples, we can obtain a convergence almost surely, with a rate dominated by $\exp \bigl ( - \kappa _ { \mathrm { p } } ^ { - 2 } \bigr )$ .

Theorem 2 states that $\kappa ( \mathbf { V } )$ , which determines the convergence rate of Algorithm 2, can be upper-bounded with high probability. Next question is: how far is $F ^ { i } ( { \hat { \mathbf { U } } } , \mathbf { V } )$ from the minimal possible error of reconstruction $\epsilon _ { \mathrm { m i n } }$ ? With a lower-bound established (Proposition 1), the subsequent section endeavors to upper-bound the Frobenius-norm error when approximating S with a rank- $r$ matrix.

# 4.3 Bound on the Frobenius-norm of the Error of Reconstruction

In the case of a low-rank matrix S with $\mathrm { r a n k } \left( \mathbf { S } \right) \ = \ r$ (i.e., $\mathbf { E } = 0 \mathrm { \dot { \Omega } }$ ), the couple $( \hat { \mathbf { U } } ^ { i } , \mathbf { V } ) _ { i = 1 } ^ { N }$ allows to reconstruct $( \mathbf { S } ^ { i } ) _ { i = 1 } ^ { N }$ without error. This can be proved using Theorem 9.1 from Halko, Martinsson, and Tropp (2011) and by underlining that for any client $i \in \{ 1 , \ldots , N \}$ , we have $\hat { \mathbf { U } } ^ { i } \mathbf { V } ^ { \top } =$ $\mathbf { S } ^ { i } \mathbf { \bar { V } } ( \mathbf { V } ^ { \top } \mathbf { V } ) ^ { - \mathrm { i } } \mathbf { V } ^ { \top } = \mathbf { S } ^ { i } \mathbf { P }$ , where $\mathbf { P }$ is a projector on the subspace spanned by the columns of $\mathbf { V }$ .

Proposition 3. Let $r ~ \in ~ \{ r _ { * } , \ldots , d \wedge n \}$ , in the lowrank matrix regime where we have $\mathrm { ~ { ~ \bf ~ E ~ } ~ } = \mathrm { ~  ~ \omega ~ } 0$ , using the power initialization (Algorithm 1), we achieve $\begin{array} { r } { \operatorname* { m i n } _ { \mathbf { U } ^ { i } \in \mathbb { R } ^ { n _ { i } \times r } } \left\| \mathbf { S } ^ { i } - \mathbf { U } ^ { i } \mathbf { V } ^ { \top } \right\| _ { \mathrm { F } } ^ { 2 } = 0 . } \end{array}$ .

Second, we are interest
ed in the full-rank matrix S scenario and give below a theorem upper-bounding the Frobenius-norm error when approximating S with a rankr matrix. The proof requires (1) to show the link between the error of reconstruction and the diagonal elements of the projector on the subspace spanned by the columns of $\mathbf { V } _ { * } ^ { \top } \mathbf { V }$ , (2) upper bound it by the norm of a Gaussian vector and the smallest singular value of a Whishart matrix, and finally (3) apply concentration inequalities.

Theorem 3. Let $r \leq d \land n$ in $\mathbb { N } ^ { * }$ , using the power initialization (Algorithm $\boldsymbol { { \mathit { 1 } } }$ ), for p $\in ] 0 , 1 [$ , with probability at least $1 - 2 \mathrm { p }$ , we have:

$$
\begin{array} { l } { \displaystyle \operatorname* { m i n } _ { \mathbf { U } \in \mathbb { R } ^ { n \times r } } \left\| \mathbf { S } - \mathbf { U V } ^ { \top } \right\| _ { \mathrm { F } } ^ { 2 } < \displaystyle \sum _ { i > r } \sigma _ { i } ^ { 2 } \times } \\ { \displaystyle \left( 1 + 2 r { \mathrm { p } ^ { - 1 } } \left( \ln ( { \mathrm { p } ^ { - 2 } } ) + \ln ( 2 ) r \right) \frac { \left( \sigma _ { \operatorname* { m a x } } ^ { 2 } - \sigma _ { i } ^ { 2 } \right) } { \sigma _ { r } ^ { 2 } } \frac { \sigma _ { i } ^ { 4 \alpha } } { \sigma _ { r } ^ { 4 \alpha } } \right) . } \end{array}
$$

The main takeaway is that our algorithm can run in a finite number of communication rounds. We can make the following remarks.

• Frobenius-norm. This result on the Frobenius-norm for $\alpha > 0$ is new and was missing in Halko, Martinsson, and Tropp (2011). The rate controlling $\epsilon$ depends on $r ^ { 2 }$ .   
• Comparison with Halko, Martinsson, and Tropp (2011) in the case $\alpha = 0$ . In contrast, they obtain a dependency on $r$ for both the 2-norm and the Frobenius norm when $\alpha = 0$ . Another difference: our bound depends on the ratio $\sigma _ { \operatorname* { m a x } } ^ { 2 } / \sigma _ { r } ^ { 2 }$ unlike theirs (Theorem 10.7). But these two drawbacks are annihilated as soon as $\alpha > 0$ , we provide more details after Corollary 1.   
• Multiplicative noise. Following Halko, Martinsson, and Tropp (2011), we have a multiplicative noise. w.r.t. to the minimal possible error $\epsilon _ { \mathrm { m i n } }$ .   
• Range of $r$ . Contrary to Halko, Martinsson, and Tropp (2011); Hardt and Price (2014); Li et al. (2021a), this theorem holds for any value $r$ . However, for $r < r _ { * }$ (corresponds to not taking the whole signal into account), the bound is very large as $\sigma _ { i } = \Omega ( 1 )$ , this is why we give a corollary with $r \geq r _ { * }$ in Corollary 1.

The upper bound given in Theorem 3 is minimized if $\sigma _ { \mathrm { m a x } } ^ { 2 } = \stackrel { \bullet \star } { \sigma } _ { r } ^ { 2 } + o ( 1 )$ and if for $i > r$ we have $\sigma _ { i } ^ { 2 } \ll \sigma _ { r } ^ { 2 }$ , which we assume to be the case for $r = r _ { * }$ . Therefore, taking $\boldsymbol { r } ~ = ~ \boldsymbol { r } _ { * }$ in Theorem 3 minimizes the provided bound. However, the error of reconstruction can only be reduced if taking more than $r _ { * }$ components. Indeed, it mathematically corresponds to having two projectors $\mathbf { P } , \mathbf { P ^ { \prime } }$ on the subspaces spanned by the $^ { r _ { * } }$ or $r$ components; therefore we have $\mathrm { I m } ( \mathbf { P } ^ { \prime } ) \subset \mathrm { I m } ( \bar { \mathbf { P } } )$ . In particular, it means that if $r > r _ { * }$ , the error $\epsilon$ will be lower than in the case $r = r _ { * }$ . This results in a tighter bound for any $r _ { * } \leq r \leq n \land d$ . Additionally, we consider $p = 1 / 4$ in Theorem 3 in order to obtain a bound on the Frobenius-norm with probability at least $1 / 2$ and derive a number of sampling $m$ s.t. the bound is verified for at least one sampled matrix $\Phi$ with probability $\mathrm { P \in ] 0 , 1 [ }$ .

Corollary 1. For any $r _ { * } \leq r \leq n \land d ,$ , for $\alpha \in \mathbb { N } ^ { * }$ , with probability $\mathrm { P \in ] 0 , 1 [ \cdot }$ , if we sample $m = \lfloor - \log _ { 2 } ( 1 - \mathrm { P } ) \rfloor$ independent matrices $( \Phi _ { j } ) _ { j = 1 } ^ { m }$ to form ${ \bf V } _ { j } = { \bf S } ^ { \alpha } \Phi$ and $\mathbf { U } _ { j } =$ $\mathbf { S } \mathbf { V } _ { j } ( \mathbf { V } _ { j } \mathbf { V } _ { j } ) ^ { - 1 } \mathbf { V } _ { j }$ , at least one of the couple $( \mathbf { U } _ { j } , \mathbf { V } _ { j } )$ results in verifying $\begin{array} { r } { \| \mathbf { S } - \mathbf { U } _ { j } \mathbf { V } _ { j } \| _ { \mathrm { F } } ^ { 2 } < \epsilon + \sum _ { i > r _ { * } } \sigma _ { i } ^ { 2 } } \end{array}$ , with:

$$
\epsilon = \sum _ { i > r _ { * } } \sigma _ { i } ^ { 2 } \left( 3 2 \ln ( 4 ) r _ { * } ( r _ { * } + 1 ) \frac { ( \sigma _ { \operatorname* { m a x } } ^ { 2 } - \sigma _ { i } ^ { 2 } ) } { \sigma _ { r _ { * } } ^ { 2 } } \frac { \sigma _ { i } ^ { 4 \alpha } } { \sigma _ { r _ { * } } ^ { 4 \alpha } } \right) .
$$

We can make the following remarks.

• Dominant term. The dominant term for $\epsilon$ is proportional to $r _ { * } ^ { 2 }$ while it is proportional to $^ { r _ { * } }$ for Halko, Martinsson, and Tropp (2011) in the scenario $\alpha = 0$ . Nonetheless, the exacerbated $r _ { * } ^ { 2 }$ rate is mitigated by its appearance within a logarithmic as we have $\begin{array} { r } { \boldsymbol { \alpha } = \Omega \left( \frac { \log \left( \sigma _ { \operatorname* { m a x } } d r _ { * } ^ { 2 } \epsilon ^ { - 1 } \right) } { \log \left( \sigma _ { r _ { * } } \right) - \log \left( \sigma _ { r _ { * } + 1 } \right) } \right) } \end{array}$ In other words, doubling $\alpha$ yields an equivalent rate.

• Impact of $\alpha$ . Given that for any $i$ in $\{ r _ { * } + 1 , \ldots , n \land d \}$ , we have $\sigma _ { i } ~ \leq ~ \sigma _ { r _ { * } }$ , increasing $\alpha$ reduces $\epsilon$ by a factor $\sigma _ { r _ { * } + 1 } ^ { 4 \alpha } / \sigma _ { r _ { * } } ^ { 4 \alpha }$ n≤d im∗proves the convergence of the algorithm towards the minimal error $\epsilon _ { \mathrm { m i n } }$ . This has a major impact in the particular regime underlined by Corollary 2, where we get a finite number of communication rounds!

• “Comparison” with related works. Halko, Martinsson, and Tropp (2011) provide a result solely on the 2-norm which is not comparable to our Frobenius-norm: they obtain $\begin{array} { r } { \alpha ~ = ~ \Omega ( \frac { \sigma _ { r * + 1 } \log ( d ) } { \epsilon } ) } \end{array}$ . Our asymptotic rate on $\alpha$ would be better if the quantities were comparable. Li et al. (2021a) provide a result solely on the 2-norm distance between eigenspaces which is again not comparable: they obtain $\begin{array} { r } { \alpha = \hat { \Omega } \left( \frac { \sigma _ { r _ { * } } \log ( d \epsilon ^ { - 1 } ) } { \sigma _ { r _ { * } } - \sigma _ { r _ { * } + 1 } } \right) } \end{array}$ . In the regime where $\sigma _ { r _ { * } + 1 } \ll 1$ , it can not be equal to zero, unlike us. Note that however in the regime $\sigma _ { r _ { * } } \approx \sigma _ { r _ { + } 1 }$ , the two bounds would be equivalent.

• Value of $m$ . Sampling $m = 1 0$ random matrices is enough to have at least one of them resulting to verify Equation (1) with probability $1 - 1 0 ^ { - 3 }$ .

The next corollary emphasizes the special regime of a full-rank matrix S s.t. σmax, . . $\sigma _ { \operatorname* { m a x } } , \ldots , \sigma _ { r _ { * } } = o ( 1 )$ and $\sigma _ { r _ { * } + 1 } \ll$ 1, which is of great interest as raised by Corollary 1. In this regime, increasing $\alpha$ and using gradient descent is particularly efficient in terms of communication cost and precision $\epsilon$ .

Corollary 2 (Full-rank scenario with small $\sigma _ { \operatorname* { m a x } } / \sigma _ { r _ { * } } )$ . Let $\lambda , \xi > 0$ . Suppose the first $^ { r _ { * } }$ (resp. the last $n \wedge d - r _ { * } )$ singular values of S are equal to a large $\lambda$ (resp. to a small $\xi ,$ ), then Algorithm 2 has a linear rate of convergence determined by $\bar { \kappa } ^ { 2 } \le { \cal O } ( r _ { * } ^ { 2 } + r d \xi / \lambda ) = { \cal O } ( \bar { r } _ { * } ^ { 2 } ) $ , and we have an error ϵ = O(dr2ξ4α+∗2/λ4α).

In the next section, we illustrate the insights highlighted by our theorems. on both synthetic and real data. In particular, on Figures 2b and 3a, we illustrate the setting of Corollary 2 with $\lambda = 1$ , $\xi = 1 0 ^ { - 6 }$ , $d = 2 0 0$ , and $r _ { * } = 5$ resulting to $\kappa ^ { 2 } \approx 2 5$ and $\epsilon _ { \alpha = 0 } ~ \approx ~ 5 ^ { - 7 }$ for $\alpha = 0$ ; in contrast, for $\alpha = 1$ , we have $\epsilon _ { \alpha = 1 } \approx 5 ^ { - 3 7 }$ after only two communications!

Table 1: Settings of the experiments.   

<html><body><table><tr><td>Settings</td><td>mnist</td><td>celeba</td><td>w8a</td></tr><tr><td>dimension d</td><td>784</td><td>96</td><td>300</td></tr><tr><td>latent dimension r</td><td>20</td><td>20</td><td>20</td></tr><tr><td>training dataset size</td><td>6000</td><td>557</td><td>49,749</td></tr><tr><td>number of clients N</td><td>10</td><td>25</td><td>25</td></tr></table></body></html>

![](images/b677f13230fd0ad164cac4c9b87bb0ee85b07ef0cc97d66430364c1dfdfe18dd.jpg)  
Figure 1: Singular values of the four used datasets.

# 5 Experiments

Our code is provided on Github. Experiments have been run on a 13th Gen Intel Core i7 processor with 14 cores. Our benchmark is SVD in the centralized setting (SVD of concatenated matrices) using scipy.linalg.svd, this corresponds to the green line in all experiments.

Synthetic dataset generation. We consider synthetic datasets with $\begin{array} { r l r } { N } & { { } = } & { 2 5 } \end{array}$ clients. For each client $\mathbf { \chi } _ { i }$ in $\{ 1 , \ldots , N \}$ , we have $n _ { i } = 2 0 0$ and $d = 2 0 0$ . We build a global matrix $\mathbf { S } = \mathbf { X } + \mathbf { E }$ and then split it across clients. We set rank $\mathbf { \left( X \right) } = 5$ with $\mathbf { X } = \mathbf { U } _ { \mathbf { X } } \boldsymbol { \Sigma } _ { \mathbf { X } } \mathbf { V } _ { \mathbf { X } }$ , where $\mathbf { U } _ { \mathbf { X } }$ (resp. $\mathbf { V } _ { \mathbf { X } } )$ are in ${ \mathcal { O } } _ { n } ( \mathbb { R } )$ (resp. ${ \mathcal { O } } _ { d } ( \mathbb { R } ) )$ . $\pmb { \Sigma } _ { \mathbf { X } }$ is a diagonal matrix in $\mathbb { R } ^ { n \times d }$ with the 5 first values equal to 1, and the other to 0. E is the noise matrix which elements are independently sampled from a zero-centered Gaussian distribution.

Real datasets. We consider three real datasets: mnist (LeCun, Cortes, and Burges 2010), celeba- $2 0 0 \mathrm { k }$ (Liu et al. 2015) and w8a (Chang and Lin 2011). We do not use the whole datasets for mnist and celeba-200k. For mnist (resp. celeba-200k), clients receive images from a single digit (resp from a single celebrity). For w8a, the dataset is split randomly across clients. This results in two image datasets with low (mnist) or high (celeba-200k) complexity with heterogeneous clients, and a tabular dataset with homogeneous ones.

We plot the SVD decomposition of each dataset on Figure 1. On Figure 2, we run experiments on the synthetic dataset with $r = \mathrm { r a n k } \left( \mathbf { X } \right) = 5$ for 50 different samples of $\Phi$ . We plot $\sigma _ { r _ { * } } ^ { 2 } ( \mathbf { V } ) / \sigma _ { \operatorname* { m a x } } ^ { 2 } ( \mathbf { V } )$ on the $\mathrm { \Delta X }$ -axis, and the logarithm of the loss∗ $F$ after 1000 local iterations on the Y-axis. The goal is therefore to illustrate the impact of the sampled $\Phi$ on the convergence rate.

On Figure 3, we run experiments on the four different datasets; we plot the iteration index on the $\mathrm { \Delta X }$ -axis, and the logarithm of the loss $F$ w.r.t. iterations on the Y-axis. We run a single gradient descent after having sampled $m = 2 0$ random matrices $\Phi$ to take the one resulting in the best condition number $\kappa ( \mathbf { V } )$ . We run experiments w./w.o. a momentum $\beta _ { k } = k / ( k + 3 )$ , with $k$ the iteration index. The goal is here to illustrate on real-life datasets how the algorithm behaves in practice. In Table 2, we contrast our approach with

0 α = 0   
10 α = 1   
20 Pi>r σ2i2   
30 6 0.000 0.025 0.050 0.075 0.00 0.03 0.06 0.09 (a) Eikl = 0 (b) Eikl ∼ N 0, (10−6)2

Table 2: Number of communications to reach error $\epsilon + \epsilon _ { \mathrm { m i n } }$   

<html><body><table><tr><td>Algorithms</td><td>synth</td><td>w8a</td><td>mnist</td><td>celeba</td></tr><tr><td>α=0</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>WK2023</td><td>26</td><td>6</td><td>35</td><td>58</td></tr><tr><td>YD2021</td><td>≥1013</td><td>≥1020</td><td>≥ 1020</td><td>≥1021</td></tr><tr><td>Reached error</td><td>-5.5</td><td>5.5</td><td>4.5</td><td>5</td></tr></table></body></html>

existing algorithms using gradient descent (Ward and Kolda 2023; Ye and $\mathrm { D u } 2 0 2 1 \dot { , }$ ) by giving the number of communication to reach a given error $\epsilon + \epsilon _ { \mathrm { m i n } }$ , showing its superiority in terms of communication.

# Observations.

• Figure 2 shows the validity of Theorems 1 and 2 in the scenario without momentum as we obtain a linear convergence rate determined by $\sigma _ { r _ { * } } ^ { 2 } ( \mathbf { V } ) / \sigma _ { \operatorname* { m a x } } ^ { 2 } ( \mathbf { V } )$ . • In the low-rank regime (Figure 2a), we recover $\epsilon = 0$ as stated by Proposition 3 (up to machine precision). • In the regime of a full-rank scenario with a small $\sigma _ { \operatorname* { m a x } } / \sigma _ { r _ { * } }$ (Figures 2b and 3a, scenario highlighted by Corollary 2), gradient descent is fast (Theorem 2) and recovers the exact solutions $( \hat { \mathbf { U } } ^ { i } , \mathbf { V } ) _ { i = 1 } ^ { N }$ of (Dist. MF). Further, in this regime having $\alpha \geq 1$ reduces $\epsilon$ (Theorem 3 and Corollary 1) which is observed in practice. For such a setting, gradient descent with $\alpha \geq 1$ is the best strategy. • In the regime of a full-rank scenario with a large $\sigma _ { \mathrm { m a x } } / \sigma _ { r }$ (Figures 3b to 3d), using $\alpha \geq 1$ might lead to a very slow convergence rate. For illustration, on w8a (resp. mnist and celeba-200k) $\kappa ( \mathbf { V } )$ is equal to 4 (resp to 10 and 20) for $\alpha = 0$ , and equal to 120 (resp. to 5000 and 20000) for $\alpha \ = \ 1$ . In practice, the slow convergence rate is observed. However, as taking $\alpha \geq 1$ reduces $\epsilon$ ( Theorem 3 and Corollary 1, and observed on Figures 3b to 3d), it might be preferable in this regime to compute the exact solution of (Dist. MF) with a pseudo-inverse rather than running a gradient descent. If it is not possible to compute the pseudo-inverse or if regularization is used, mandating the use of gradient descent, then it is preferable to take $\alpha = 0$ .

# 6 Conclusion

In this article, we propose an in-depth analysis of low-rank matrix factorisation algorithm within a federated setting.

![](images/5dd209c52db97706d97878e32e62ab707845c77b519327f376fded73e9a04d3c.jpg)  
Figure 2: Matrix factorization of a low-rank matrix (left) and full-rank (right). We sample $m = 5 0$ different $\Phi$ . X-axis: $\kappa ^ { - 2 } ( \mathbf { V } )$ . Y-axis: logarithm error $\log _ { 1 0 } ( \lVert \mathbf { S } - \mathbf { U } \mathbf { V } ^ { \top } \rVert _ { \mathrm { F } } ^ { 2 } )$ . Plain line: exact solution. Dashed line: gradient descent after $K =$ 1000 iterations for each sampled $\Phi$ .   
Figure 3: Convergence plot on four datasets. $\mathrm { \Delta X }$ -axis: number of iterations. Y-axis: logarithm error $\log _ { 1 0 } ( \lVert \mathbf { S } - \mathbf { U } \mathbf { V } ^ { \top } \rVert _ { \mathrm { F } } ^ { 2 } )$ . Plain line: exact solution. Dashed line: gradient descent. Dashed-dotted line: gradient descent with momentum $\beta _ { k } =$ $k / ( k + 3 )$ , with $k$ the iteration index.

We propose a variant of the power-method that combines a global power-initialization and a local gradient descent, thus, resulting to a smooth and strongly-convex problem. This setup allows for a finite number of communications, potentially even just a single one. We emphasize and experimentally illustrate the regime of high interest raised by our theory (Corollary 2). Finally, drawing from Theorems 1 to 3 and Corollary 1, we highlight the following key insights from our analysis.

Take-away 1. Increasing the number of communication $\alpha$ leads to reduce the error ϵ by a factor $\sigma _ { r _ { * } + 1 } ^ { 4 \bar { \alpha } } / \sigma _ { r _ { * } } ^ { 4 \alpha }$ , therefore, getting closer to the minimal Frobenius-norm error $\epsilon$ .

Take-away 2. Using a gradient descent instead of an SVD to approximate the exact solution of the strongly-convex problem allows us to bridge two parallel lines of research. Further, we obtain a simple and elegant proof of convergence and all the theory from optimization can be plugged in.

Take-away 3. By sampling several Gaussian matrices $\Phi$ , we improve the convergence rate of the gradient descent. Further, based on random Gaussian matrix theory, it results in a almost surely convergence if we sample $\Phi$ until $\mathbf { V }$ is well conditioned.

Drawback 1. If gradient descent (Algorithm 2) is used to compute $( \hat { \mathbf { U } } ^ { i } ) _ { i \{ 1 , . . . , N \} }$ , and if we are in the regime where $\sigma _ { \operatorname* { m a x } } / \sigma _ { r } \gg 1$ , then increasing $\alpha$ results in increasing the condition number and therefore the number of local iterations. Furthermore, in the scenario $\alpha = 0$ , the upper bound on ϵ given by Halko, Martinsson, and Tropp (2011) is better than ours.

Three open directions to this work can be considered. First, it would be interesting to consider the case of decentralized clients where it is not possible to compute a global $\mathrm { \Delta V }$ across the whole network. Second, instead of computing an exact $\mathbf { V }$ , one could compute an approximation of $\mathbf { V }$ to reduce further the communication cost, this would lead to stochastic-like gradient descent. Third, the extension of our approach to matrix completion is non-trivial (as we require $\mathbf { V }$ to be in the span of S) and could have a lot of interesting applications.