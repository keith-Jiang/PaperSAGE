# Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment

Viraj Nishesh Darji School of Computing George Mason University Fairfax, VA, USA vdarji@gmu.edu

Callie C. Liao   
College of Science   
George Mason University   
Fairfax, VA, USA   
cliao3 $@$ gmu.edu   
Duoduo Liao   
School of Computing   
George Mason University   
Fairfax, VA, USA   
dliao2@gmu.edu

Abstract—Bridge maintenance and safety are essential for transportation authorities, and Non-Destructive Evaluation (NDE) techniques are critical to assessing structural integrity. However, interpreting NDE data can be time-consuming and requires expertise, potentially delaying decision-making. Recent advancements in Large Language Models (LLMs) offer new ways to automate and improve this analysis. This pilot study introduces a holistic assessment of LLM capabilities for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in providing detailed bridge condition analyses. It establishes a framework for integrating LLMs into bridge inspection workflows, indicating that LLM-assisted analysis can enhance efficiency without compromising accuracy. In this study, several state-ofthe-art LLMs are explored with prompts specifically designed to enhance the quality of image descriptions, which are applied to interpret five different NDE contour maps obtained through technologies such as Ground Penetrating Radar (GPR), Electrical Resistivity (ER), Impact-Echo (IE), and Ultrasonic Surface Waves (USW) for assessing bridge conditions. Each LLM model is evaluated based on its ability to produce detailed descriptions, identify defects, provide actionable recommendations, and demonstrate overall accuracy. The research indicates that the ChatGPT-4, Claude 3.5 Sonnet, CogVLM2, and ShareGPT4V models provide better image descriptions, effectively covering a wide range of topics related to the bridge’s condition. The outputs from these four models are subsequently summarized using five different LLMs to form a comprehensive overview of the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more detailed and effective summaries. The findings suggest that LLMs have the potential to significantly improve the efficiency and accuracy of infrastructure evaluation processes. This pilot study presents an innovative approach that leverages LLMs for image captioning in parallel and summarization, enabling faster decision-making in bridge maintenance and enhancing infrastructure management and safety assessments.

Index Terms—Large language models, Image captioning, Summarization, Contour map, Non-Destructive Evaluation (NDE)

amount of time which can delay the decision-making process. Recent advancements in Large Language Models (LLMs) have opened new ways of automating and enhancing the analysis of technical data. LLMs have demonstrated remarkable capabilities in understanding and describing complex visual information from various fields. This research explores the potential of leveraging LLMs to explain NDE contour maps and identify bridge conditions, intending to provide more accessible insights to engineers and decision-makers efficiently.

The primary objective of this pilot study is to analyze NDE data using LLMs and evaluate the performance of the LLMs based on the accuracy and quality of the data. A diverse set of LLMs, including both open-source and proprietary solutions, are employed to achieve the objective.

The critical novelty of this research is multifold. First, a holistic assessment is provided for gaining access to a wide range of LLM capabilities in identifying the data of NDE contour maps. Second, the effectiveness of multiple LLMs in giving detailed interpretations of bridge conditions is demonstrated with a novel approach that uses secondary LLM analysis to synthesize and consolidate the findings. Third, we have developed a practical framework for integrating LLMs into bridge inspection workflows. Our findings indicate that LLM-assisted analysis can serve as a valuable support tool for existing bridge inspection reports, potentially reducing the time required for data interpretation without compromising accuracy. This study opens new avenues for more efficient bridge maintenance decision-making, contributing to the broader fields of infrastructure management and safety assessment research.

# I. INTRODUCTION

The maintenance and safety of bridges are of high importance for transportation authorities worldwide. NonDestructive Evaluation (NDE) techniques play a crucial role in identifying bridge conditions and providing valuable insights into the structural integrity of the bridge. However, interpreting NDE data often requires expertise and can take a significant

# II. RELATED WORK

Recent breakthroughs in deep learning have contributed significantly to automated infrastructure inspection and evaluation processes. The existing work can be broadly categorized into two key areas: a) bridge inspection systems using image captioning techniques, and b) attention mechanisms and captioning architectures for technical image interpretations.

# A. Attention Mechanisms and Technical Image Captioning

The Transformer model, as first proposed by Vaswani et al. [3], is the backbone for many of today’s state-of-the-art image interpretation systems. They demonstrated that selfattention mechanisms alone can provide state-of-the-art performance for sequence transduction tasks without recurrence or convolution. The work was targeted at text translation; however, the principles they laid down have later become very important in image-to-text applications, including our approach to NDE contour map interpretation. Al-Malla et al. [4] further enhanced this by adding object detection features and an attention mechanism to the image captioning model. Their work focused on natural image captioning, but the multimodal feature extraction techniques they employed provide a valuable foundation for our application in technical NDE image interpretation.

# B. Bridge Inspection Systems Using Image Captioning

Bridge inspection systems using image captioning leverage deep learning, particularly Convolutional Neural Networks (CNNs), to analyze bridge images and generate descriptive captions of visible defects like cracks and corrosion. These systems improve inspection efficiency and accessibility, especially for less experienced engineers. However, these approaches are limited to surface-level damage detection in photographs and do not address more complex data sources such as NDE contour maps. Li et al. [1] proposed BDCD-Net, an end-to-end image captioning framework that is specifically developed for the description of bridge damage, incorporating an adaptive attention mechanism to provide a comprehensive description of the bridge damage, including component types, damage categories, and spatial information, while demonstrating the feasibility of automatic bridge inspection. However, it mainly focuses on visible damage in photographic images, rather than interpreting the contour maps from NDE data, as in our approach. Chun et al. [2] developed a web-based bridge inspection system that generates explanatory texts from bridge photographs and enables continuous learning through user interactions, aiming to support less experienced engineers and staff. While their work focuses on visual inspection, our approach extends to subsurface analysis via NDE contour maps and employs multiple large language models for more comprehensive interpretation.

Although these works represent important advances in the active areas of image captioning and bridge inspection, the critical link in the automatic interpretation of NDE contour maps remains missing. Whereas past research was limited to visible damage in photographic images, our work applies these principles to the more specialized domain of NDE data visualization. By building on the latest improvements of LLMs in this research and further adapting existing techniques for captioning to tackle the special difficulties of interpreting technical NDE contour maps, the results would be more accessible to a wider range of stakeholders in the bridge maintenance process.

![](images/f86e657e7d688c1f4cee58b4b023709437d72a15a4f58404f552ec4f6d85ebae.jpg)  
Fig. 1. The framework of automated interpretation of NDE contour maps Using LLMs for bridge condition assessment

This section describes the structured methodology for interpreting LLMs to leverage NDE contour map data assessing bridge conditions. The process falls into three stages as shown in Fig. 1: initial processing of data, multi-model image captioning, and summarization analysis.

# A. Data Input and Preparation

The process inputs five different NDE contour map images, each representing different NDE measurement technologies used to scan the bridge, including Ground Penetrating Radar (GPR), Electrical Resistivity (ER), Impact Echo (IE), and Ultrasonic Surface Waves (USW). These maps provide variable information about the bridge’s structure condition, with each image captured using different measurement parameters. A well-framed initial prompt is designed to instruct the LLMs in interpreting these technical visualizations. The prompt ensures consistency across the models in terms of extracting relevant technical information.

# B. Multi-Model Image Captioning

The second step involves NDE contour maps for various parallel image captioning through LLMs. This parallel processing approach facilitates cross-referencing of findings across different models, leading to more reliable data interpretation. Image captioning is driven by two sets of prompts: the first one requests the extraction of relevant technical features from the NDE contour maps, while the second refines the interpretation based on specific requirements and technical parameters. This prompt structure ensures that the models focus on the most relevant aspects of the contour maps while maintaining technical accuracy in the interpretations.

# C. Summarization Analysis

This stage involves consolidated analysis using LLM summarization, which synthesizes outputs from multiple image captioning models in their different interpretations toward one coherent analysis. The summarization model integrates the interpretations from different models’ outputs, prioritizing critical information about the bridge condition and generating actionable recommendations based on consolidated findings.

# D. Output Generation

The methodology culminates in the generation of a comprehensive output that encompasses several key aspects, including structural integrity assessment, defect identification and classification, and maintenance recommendations.

This approach offers the advantage of leveraging multiple LLMs while maintaining a more structured and systematic approach to interpreting technical data. The multi-stage process enhances the robustness of the analysis and makes complex NDE data more accessible to the stakeholders involved in bridge maintenance and decision-making. By integrating the strengths of multiple models with structured summarization, this system is capable of providing a comprehensive analysis of NDE data that is actionable, thereby supporting effective and informed decision-making on bridge maintenance.

# IV. EXPERIMENTAL RESULTS AND DATA ANALYSIS

# A. Data Collection

The data for this study are sourced from the Federal Highway Administration (FHWA) bridge database. As of July 23rd, 2024, 38 bridges in that database included NDE data. For this pilot study, we looked at only one bridge located in Mississippi, from which we extracted 5 unique NDE contour map datasets as shown in Fig. 2-6 in Appendix A. The selected structure has Structure Number 11002200250051B and LTBP Bridge Number 28-000008 [5].

Contour maps used in this study are representations of five different NDE measurement technologies and parameters. First, GPR measurements provided two different datasets: cover depth in inches (Fig. 2) and Depth-Corrected Attenuation (DCA) at the top rebar level in decibels (Fig. 3). These GPR measurements provide valuable information about both the internal structure of the bridge deck and possible trends in deterioration. Second, ER measurements are collected as an indicator of concrete properties and corrosion activity (Fig. 4). IE tests in kHz (Fig. 5) were applied to evaluate the integrity of the concrete elements and their eventual delamination. Finally, the measurements of USW, given in ksi (Fig. 6), provided information about the modulus of the concrete, hence informing about its mechanical properties. A range of measurement techniques have been selected to provide a comprehensive assessment of the bridge’s structural condition, utilizing various physical parameters. Each measurement method offers unique insights into different aspects of the structure’s health, and when analyzed collectively, they enable a more thorough and complete evaluation.

# B. LLMs for Image Captioning

The presented models in this paper represent state-of-the-art multimodal LLMs integrated with image captioning capabilities, each bearing different functionalities. The vit-gpt2-imagecaptioning model combines Vision Transformers with GPT2 for generating image captions, therefore placing a heavy emphasis on the interaction between visual and textual data [6]. BLIP’s large image captioning model integrates vision and language, leveraging the capabilities of the Vision Transformer and BERT to deliver highly accurate captions [7]. Paligemma [8] and its enhanced version, Paligemma rich captions [9], are based on a pre-trained model for image captioning, finetuned to obtain longer descriptions. Florence-2 is a multimodal model that allows both image captioning and crossmodal understanding, making it effective for detailed visual understanding [10]. ShareGPT4V, a model based on LLaMA, can provide highly detailed captions and explanations; it focuses on delivering useful information extracted from images [11]. $\mathbf { C o g V L M } 2$ is a state-of-the-art, next-generation advanced vision-language model designed for high-quality, in-depth captioning and analysis of visual content [12]. ChatGPT-4 is a powerful multimodal LLM that processes both textual and visual inputs to generate high-quality descriptive outputs [15]. Claude 3.5 Sonnet is a general multimodal LLM, performing proficiently in interpreting and captioning images into clear, accurate descriptive texts [16].

# C. Evaluation of Image Captioning Performance in LLMs

In this study, these models have been evaluated based on their ability to generate detailed captions and their performance in analyzing the visual data against the NDE contour maps. This testing included an evaluation phase where multiple prompt variations were tested with the models to optimize their interpretation capabilities. While one model, vitgpt2-image-captioning was used without any prompts, another model, the BLIP image captioning large model, had been previously tested both with and without any prompts. In contrast, all other models underwent extensive optimization of prompts to improve their performance and interpretation accuracy. Prompt optimization played a crucial role in enhancing the quality of the output from LLMs in analyzing NDE data. The original prompt was intended to provide a simple description, but it underwent several iterative refinement steps to ultimately produce complete and relevant responses. Finally, the tone and content of the prompt were carefully crafted as shown in Appendix B to elicit a professional stance from the model, emphasizing the key visual features and providing a thorough assessment of the issue at hand. It underscores the importance of clear, contextual, and role-assigned prompts in enhancing the analysis capability of LLMs.

The models were evaluated based on a structured framework consisting of four key metrics: Relevance, Usefulness, Coverage, and Specificity. Relevance refers to accuracy in the description of the pictures, specifically assessing whether the output captures most of the key features and the bridge condition. Usefulness considers its applicability for both the engineer and the decision-maker. Coverage analyzes whether the captions cover various aspects of the images, including all potential defects and overall conditions. Specificity examines the level of detail provided, assessing the inclusions of subtle observations.

These ratings were developed for this pilot study to ensure consistency and alignment with its objectives. The evaluation consists of a qualitative analysis of output from each model, relative to pre-developed criteria for each metric, with assessments conducted by researchers and one domain expert. Binary assessments (Yes/No) are given if a model consistently upholds the standard in most test cases. The overall marks, ranging from 1 to 5, are combined to reach an overall score for each model’s performance. A score of 5 would correspond with the assessment ”Yes” in all four categories; on the contrary, a score of 1 would correspond with the assessment ”No” in all categories. Table I presents the comparison of the performance of the models.

TABLE I IMAGE CAPTIONING LLM COMPARISON   

<html><body><table><tr><td>LLM</td><td>Relevant</td><td>Usability</td><td>Coverage</td><td>Specificity</td><td>Rverag</td></tr><tr><td>Claude 3.5 Sonnet</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>5</td></tr><tr><td>ChatGPT-4</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>5</td></tr><tr><td>CogVLM2</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>5</td></tr><tr><td>ShareGPT4V</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td><td>4</td></tr><tr><td>Florence-2</td><td>Yes</td><td>No</td><td>Yes</td><td>No</td><td>3</td></tr><tr><td>Paligemma FT*</td><td>Yes</td><td>No</td><td>Yes</td><td>No</td><td>3</td></tr><tr><td>Paligemma</td><td>Yes</td><td>No</td><td>No</td><td>No</td><td>2</td></tr><tr><td>BLIP large</td><td>No</td><td>No</td><td>No</td><td>No</td><td>1</td></tr><tr><td>vit-gpt2</td><td>No</td><td>No</td><td>No</td><td>No</td><td>1</td></tr></table></body></html>

∗FT is Finetuned

From the detailed evaluation, four models, ShareGPT4V, $\mathbf { C o g V L M } 2$ , ChatGPT-4, and Claude 3.5 Sonnet, showed superior performances, with overall ratings of 4 and above. A detailed illustration of the capabilities of the Claude 3.5 Sonnet models is demonstrated in Appendix C using the prompt shown in Appendix B.

# D. Summarization for LLMs and Evaluation

Outputs from these four top-performing image captioning models in this study were subjected to further summarization by five LLMs: ChatGPT-4 [15], Claude 3.5 Sonnet [16], Mistral [13], Gemini [17], and Llama3 [14]. This extra layer of analysis was aimed at consolidating the various interpretations into cohesive, actionable insight about the status/condition of the bridges.

These summarization models were evaluated using a structured framework based on three major metrics: Completeness, In-Depth Coverage, and Formatting & Presentation. Completeness described the degree to which the output contained all the information that would make the summary comprehensive enough to hit the main points. Similarly, In-depth Coverage checks for technical correctness and depth of analysis, whether the knowledge given was interesting and informative, and the satisfaction of requirements for professional review, whereas Formatting & Presentation checks for clarity, logical flow, as well as the format of output, thereby showing how well the summaries will be able to convey information in a clear and comprehensible way.

These scores were developed specifically for this pilot study, which helps standardize this research and meet the specific objectives of the study. Each model was matched against these criteria and rated on a scale from 1 to 5, with evaluations conducted by researchers and one domain expert. A score of 5 indicates exceptional performance, while 1 represents significant deficiencies in meeting the evaluation criteria. Table II provides a comparison of these summarization models.

TABLE II SUMMARIZATION LLM COMPARISON   

<html><body><table><tr><td>LLM</td><td>Completeness</td><td>To-depth</td><td>Formatiatgo &</td><td>RveragI</td></tr><tr><td>ChatGPT-4</td><td>5</td><td>5</td><td>5</td><td>5</td></tr><tr><td>Claude 3.5 Sonnet</td><td>5</td><td>4</td><td>5</td><td>4.67</td></tr><tr><td>Gemini</td><td>3</td><td>3</td><td>4</td><td>3.33</td></tr><tr><td>Mistral</td><td>4</td><td>3</td><td>4</td><td>3.67</td></tr><tr><td>Llama3</td><td>3</td><td>3</td><td>4</td><td>3.33</td></tr></table></body></html>

Among these, ChatGPT-4 ranked the highest with a perfect overall rating of 5.00. The second highest was Claude 3.5 Sonnet, having an overall rating of $4 . 6 7 \mathrm { { m a 3 } }$ performed relatively poorly with an overall rating ranging from 3.33 to 3.67. A typical output of the ChatGPT-4 summarization is shown in the Appendix D, where it systematically brought together many model interpretations into one coherent analysis of the bridge conditions.

# V. CONCLUSION

This pilot study has demonstrated the effectiveness of LLMs in understanding the NDE contour map data of bridge inspection processes. Overall, our assessment of nine image captioning models and five summarization models indicated that ChatGPT-4, $\mathbf { C o g V L M } 2$ , Claude 3.5 Sonnet, and ShareGPT4V were exceptionally good at interpreting even complex NDE contour data interpretation, while ChatGPT-4 and Claude 3.5 Sonnet were very capable with summaries inclusive of comprehensive details. These results have validated the potential of LLMs in three key areas: interpretation of the bridge condition, analysis of NDE contour maps, and generation of maintenance recommendations. This breakthrough has significant implications for accelerating infrastructure management bridge inspection processes, with complex NDE data made more accessible to a wide range of stakeholders. The system’s capability for rapid and comprehensible interpretations results in more time- and resource-effective decision-making within bridge maintenance workflows. However, even as these models demonstrate remarkable abilities, they can only complement human knowledge, expertise, and judgment, and the periodic inconsistencies in the outputs underscore the point of validation by experts.