# Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection

Sara Abdulaziz and Egor Bondarev

Eindhoven University of Technology, 5612 AP Eindhoven, The Netherlands {s.e.a.m.abdulaziz, e.bondarev}@tue.nl

Abstract. Advancements in deep learning have improved anomaly detection in surveillance videos, yet they raise urgent privacy concerns due to the collection of sensitive human data. In this paper, we present a comprehensive analysis of anomaly detection performance under four human anonymization techniques, including blurring, masking, encryption, and avatar replacement, applied to the UCF-Crime dataset [1]. We evaluate four anomaly detection methods, MGFN [2], UR-DMU [3], BN-WVAD [4], and PEL4VAD [5], on the anonymized UCF-Crime to reveal how each method responds to different obfuscation techniques. Experimental results demonstrate that anomaly detection remains viable under anonymized data, and is dependent on the algorithmic design and the learning strategy. For instance, under certain anonymization patterns, such as encryption and masking, some models inadvertently achieve higher AUC performance compared to raw data, due to the strong responsiveness of their algorithmic components to these noise patterns. These results highlight the algorithm-specific sensitivities to anonymization and emphasize the trade-off between preserving privacy and maintaining detection utility. Furthermore, we compare these conventional anonymization techniques with the emerging privacy-by-design solutions, highlighting an often overlooked trade-off between robust privacy protection and utility flexibility. Through comprehensive experiments and analyses, this study provides a compelling benchmark and insights into balancing human privacy with the demands of anomaly detection.

Keywords: Video Anomaly Detection · Privacy Preservation · Privacy Protection · Human Privacy · Anonymization · Dataset · UCF-Crime.

# 1 Introduction

Privacy-preserving computer vision has emerged as an active research area with the growing emphasis on privacy in surveillance systems due to the recent regulations, such as GDPR and AI Act. Various anonymization techniques have been developed to mitigate these privacy risks while retaining the utility of perception tasks [6–10]. Traditional anonymization techniques, such as human blurring, pixelation, encryption, masking, and avatar replacement [11], offer basic levels of privacy protection but often degrade the performance of the utility tasks. More advanced techniques, such as privacy-by-design solutions [6, 7, 12], have gained traction in recent years, as they promise stronger privacy protection with minimal utility loss.

While privacy-by-design solutions demonstrate success in complex vision tasks, such as human behavior analysis [6, 12] and anomaly detection [7], they remain constrained by a number of limitations. Most notably, such solutions result in a severe degradation of scene quality, which prevents visual monitoring, a crucial component of real-world surveillance applications. Additionally, these solutions are often inflexible to generic image processing algorithms, limiting their applicability to a narrow range of use cases. Despite the advancements in privacy-by-design solutions, there remains a lack of systematic comparison between these solutions and the conventional anonymization techniques, particularly on complex vision tasks, such as anomaly detection.

The complexity of video analysis methods within surveillance applications varies significantly. Fundamental tasks such as object detection [13], tracking [14], and re-identification [15] primarily focus on identifying and following objects or individuals, typically relying on spatial and basic temporal information. In contrast, human activity recognition (HAR) [16] and video anomaly detection (VAD) [17] require a deeper understanding of complex behavioral patterns and subtle visual cues. Therefore, they rely on more sophisticated spatio-temporal feature extraction and analysis, due to their sensitivity to visual variations and contextual information.

Unlike HAR methods that aim to learn behavioral information from extracted features, VAD models aims to distinguish normal from abnormal events using snippet-based features [1]. Since VAD is highly sensitive to subtle visual interframe changes, the real implications of anonymization on its performance remain unclear without deeper analysis. In this paper, we state the following research questions: (1) To what extent do recent anomaly detection models rely on sensitive information, such as human body features, in their decision-making? (2) Does the performance drop caused by the conventional anonymization exceed the degradation observed under privacy-by-design solutions? (3) How do different human replacements, such as blurring, encryption, masking, or avatar replacement, influence the anomaly detection performance?

In this paper, we introduce the Anonymized UCF-Crime (AUCF-Crime), a human privacy-preserved version of the original UCF-Crime dataset [1], designed to facilitate research on privacy-preserving anomaly detection. With this dataset, we evaluate four anonymization methods, including blurring, masking, encryption, and avatar replacement, on four anomaly detection methods [2–5], examining one feature extraction technique [18]. To the best of our knowledge, this is the first large-scale study assessing the impact of multiple anonymization techniques on video anomaly detection performance. This study contributes to the ongoing discussion on balancing the privacy protection and visual utility, offering a benchmark for evaluating privacy-preserving strategies in video anomaly detection. The main contributions are summarized as follows:

– We present AUCF-Crime, a public anonymized variant of UCF-Crime [1] for anomaly detection, incorporating multiple anonymization techniques, such as human-segment blurring, masking, encryption, and avatar replacement.   
– We quantify how these anonymization techniques affect the anomaly detection performance, demonstrating that modern algorithms primarily rely on learned separability of spatio-temporal features rather than sensitive human attributes.   
– We compare conventional anonymization techniques with privacy-by-design solutions, highlighting the trade-off between privacy protection and utility flexibility.

# 2 Related Work

Privacy-preserving computer vision has emerged as a critical research area to enable vision-based tasks, such as human detection, action recognition, human pose estimation (HPE), and anomaly detection (AD), while protecting human privacy. Existing approaches can be categorized into two main research directions: (1) visual obfuscation and (2) privacy-by-design.

Visual Obfuscation methods aim to conceal human identities in visual data while preserving sufficient scene information for the downstream vision tasks. These methods typically involve blurring, pixelation, blackening, encryption, or replacing individuals with alternative representations, such as 2D avatars [11], 3D avatars [19], skeletons [20], or synthetic identities [21].

Several studies have examined the impact of visual obfuscation on utility performance. Cucchiara et al. [22] explore HAR performance under face and body blurring, demonstrating that knowledge distillation techniques can compensate for privacy-induced data loss, maintaining action recognition accuracy. The findings suggest that fine-grained human details are not always necessary for HAR tasks. Another study by Yan et al. [10] focuses on HAR performance on segmented and bounding-box-obfuscated images, concluding that contextual cues play a more significant role than privacy-sensitive features in action recognition. While these studies have evaluated the effects of varying blurring levels [22] and differences in human shape representation [10], our work focuses on the impact of different human region replacements, including blurring, masking, encryption, and avatar-based substitution.

Beyond HAR, obfuscation has been applied to privacy-preserving behavior analysis. Mishra et al. [23] extract body skeletons and semantic masks to study behavioral patterns in dementia risk assessment, evaluating performance with and without background context. The results show only a minor performance degradation when human body segments are removed while background information is maintained, reinforcing previous findings that privacy-sensitive attributes may not be essential for certain vision tasks. Additionally, the authors of [11,19] propose human-to-avatar replacements as a privacy-preserving solution for videobased monitoring, demonstrating its feasibility as an alternative to traditional obfuscation techniques.

Recent efforts have also investigated the human pose estimation methods under synthetic-based obfuscation. In [9], the authors compare human-synthesized images, generated by DeepPrivacy2 [21], with conventional obfuscation techniques, such as blurring and pixelation, to highlight the advantages of synthetic anonymization in retaining the performance of pose-related tasks.

Privacy-by-Design approaches integrate privacy protection within the visual processing pipeline, ensuring that the output is inherently privacy-preserving [7, 12, 24–26]. These methods typically generate task-specific anonymized visual data by jointly optimizing the anonymization and utility models [26]. This approach ensures that the generated data remains useful for predefined utility tasks while obfuscating it against known privacy threats.

While privacy-by-design techniques provide stronger privacy guarantees by enabling human-imperceptible anonymization and reduced privacy leakage in the temporal dimension, they still feature some limitations. To begin with, their task-specific nature limits their flexibility, making such techniques unsuitable for general-purpose image processing. In addition, these approaches continue to exhibit utility performance degradation relative to raw data and are not fairly benchmarked against traditional obfuscation techniques, complicating the assessment of their relative efficiency and associated trade-offs. Despite growing interest in privacy-preserving computer vision, existing evaluations remain limited in scope, often focusing on a narrow set of anonymization techniques and vision tasks. In this work, a comprehensive evaluation of multiple privacy-preserving methods is conducted in the context of video anomaly detection.

# 3 Evaluation Methods

In this section, we outline the evaluation methods applied to analyze the anomaly detection performance under anonymization. It comprises two key subsections: the first details the anonymization process applied to produce the AUCF-Crime; the second describes the anomaly detection methods evaluated on the resulting anonymized versions.

# 3.1 UCF-Crime Anonymization

UCF-Crime [1] is a large-scale benchmark dataset for real-world video anomaly detection. It contains surveillance videos categorized into 13 anomalous classes, such as robbery, assault, and vandalism, along with a set of normal videos. We anonymize UCF-Crime by segmenting and obfuscating human regions with conventional techniques, such as blurring, blackening, encryption, and avatar replacement [11]. It is important to note that human synthesis methods [21] are excluded in this study due to their poor temporal consistency on UCF-Crime videos.

For the blurring, blackening, and encryption, human regions are first segmented by a pre-trained Mask R-CNN model [27]. The Mask R-CNN provides accurate pixel-level masks for human figures, which serve as input to our anonymization procedures. We apply strong Gaussian blur to each segmented human region, with a kernel size of 101 for blurring obfuscation. The masking method involves completely removing visual details by replacing the pixel values with zero-level pixels. The encryption approach follows a more sophisticated procedure adapted from Shifa et al. [8]. After segmenting human regions, we first encrypt the entire frame independently using AES encryption with Cipher Block Chaining (CBC) mode. Then, the segmented human regions in the original frame are replaced by their corresponding encrypted counterparts from the fully encrypted frame. For avatar replacement, we follow a similar approach to the work of Climent-Pérez et al. [11]. First, we apply DensePose [28] to detect human segments and produce IUV maps, which represent the individual body part indices and their location in a UV coordinate space. Then, through the IUV map, a customized avatar texture is applied to replace the original body appearance. The avatar texture discloses the body parts, pose, and actions being performed, while hiding the human identity, as seen in Figure 1.

![](images/8637aa2d4f75074c577796de6442d47cd1fb390af08871dbef26e4193d0c2f6f.jpg)  
Fig. 1: Visualization of the human avatar replacement process. (a) The raw frame (Shooting031). (b) The IUV map inferred by DensePose [28]. (c) The customized texture overlaid on human bodies. This is achieved by mapping the UV coordinates of each body part in (d) to the corresponding XY coordinates of the texture in (e). Figures (d-e) are reprinted from [11].

The resulting AUCF-Crime dataset complies with the original UCF-Crime protocols, and is publicly available to facilitate further research.

# 3.2 Anomaly Detection

Recent works on video anomaly detection heavily rely on I3D features [18] in combination with the multiple instance learning (MIL) approach [1], with various modifications to normality modeling, definition of anomaly criterion, and featurelevel enhancements [2–4, 29]. The key similarity between these methods is the primary learning objective, focused on the separability of feature snippets rather than on capturing object-level behavioral and relational details.

By default, this strategy, which aims at learning the separation between feature representations based on temporal or statistical deviations, tends to circumvent the reliance on sensitive attributes. Therefore, it may allow anomaly detection to be performed under anonymization, without significant performance loss. In this work, the focus is not on the representational power of the I3D features applied in such methods under anonymization, but on the applicability of VAD methods that follow this separation strategy to anonymized videos. To this end, we investigate the effects of human anonymization, explained previously, on the weakly-supervised anomaly detection models MGFN [2], UR-DMU [3], BNWVAD [4], and PEL4VAD [5], briefly summarized below.

MGFN [2] has a dual-branch feature fusion model that combines global and local temporal features for improved anomaly detection. For anomaly snippet selection, the authors rely on feature magnitudes as the primary criterion. To handle the challenge of inconsistent feature magnitudes caused by the scene variations, the authors propose a feature amplification mechanism (FAM) that enhances the discriminativeness of the feature representations. The model is trained based on a proposed magnitude-contrastive loss, by which the network learns to differentiate normal from anomalous segments by reducing intra-class feature magnitude distances and increasing inter-class separation.

UR-DMU [3] first enhances the I3D features by capturing long and short range temporal dependencies through a modified self-attention mechanism. The framework has dual memory units which stores prototypical patterns for normal and anomaly. The dual memory units act as external repositories that the network leverages to learn the separation with a dedicated dual memory loss. To handle the noise within the normal data, caused by camera changes or scene variations, the authors propose data uncertainty learning (DUL), which learns a latent space modeled as a Gaussian distribution. The overall modules are jointly optimized under a MIL setting using the video-level labels.

BN-WVAD [4] incorporates batch normalization statistics to guide weakly supervised anomaly detection. The authors employ BatchNorm layers, which compute a running mean and variance over mini-batches that are dominated by normal snippets. This running mean is used as a statistical representation of normality. The hidden feature of each snippet is compared to the BatchNorm mean, by a Divergence of Feature from Mean (DFM) protocol, filtering out the outliers with high DFM scores. For further separation, a mean-based pull-push loss is proposed. To handle the noise within the normal data, the anomaly classifier is trained on definitive normal snippets from normal videos using snippet-level regression loss.

PEL4VAD [5] focuses on enhancing the separability of anomalous classes through prompt-enhanced learning. The I3D features are first enhanced by a temporal context aggregation (TCA) module, capturing contextual information for enhanced semantic discriminability. Then, semantic prompts representing anomaly nuances are obtained from a knowledge-base, and integrated to boost the discriminative capacity and ensure separability between the anomaly subclasses. Through the proposed prompt-enhanced learning (PEL), the semantic prompts for anomaly are aligned with the corresponding anomaly features, while non-anomalous features are distanced during training. This injection of prior semantics leads directly to enhanced discriminability.

# 4 Experimental Results

We comply with established evaluation procedures in prior works to ensure equitable comparisons $ { \left\lfloor 1 , 6 , 7 \right\rfloor }$ . For VAD performance evaluation, we measure the area under the receiver operating characteristics (AUC), the area under the precision-recall curve (AP), and the false alarm rate (FAR). The scores for the subset with abnormal data only ( $\mathrm { A U C } _ { s u b }$ and AP $s u b$ ) and class-wise AUC scores are also applied in the experiments. Throughout this section, the blurring, masking, encryption, and avatar replacement anonymization are noted as HB, HM, HEN, and H2D, respectively.

# 4.1 Implementation Details

Following the procedures in prior works [2–5], we employ the I3D model [18] pre-trained on Kinetics-400 to extract the spatio-temporal features for the RGB stream, with a 10-crop augmentation strategy to ensure fair comparison. Features are extracted for 16-frame non-overlapping segments across all anonymized variants (blurring, masking, encryption, and avatar), and across the raw nonanonymized data, to ensure consistency. We follow the training protocols provided by the authors of the SOTA VAD methods MGFN [2], UR-DMU [3], BN-WVAD [4], and PEL4VAD [5], maintaining their recommended hyperparameters, learning rates, and optimization strategies.

# 4.2 VAD on Anonymized Videos

Table 1 presents the performance comparison of the anomaly detection methods MGFN [2], UR-DMU [3], BN-WVAD [4], and PEL4VAD [5], across selected anonymization techniques. Despite a general decrease in AUC scores when anonymization is applied, the performance of the VAD methods closely correlates with baseline performance on the raw non-anonymized data. This highlights the viability of anomaly detection under anonymization, albeit with robustness varying depending on the chosen anomaly detection algorithm, as detailed below

MGFN demonstrates relatively high performance under encryption-based anonymization (HEN), suggesting a sensitivity to the noisy patterns introduced by encryption. These intrinsics are likely attributed to the feature amplification mechanism (FAM), which amplifies feature magnitudes proportionally to a modulated feature norm. Since the feature magnitudes are influenced by the different anonymization techniques, as seen in Figure 2, the amplification is affected as the FAM module modulates feature norms differently for each technique. Given that top- $k$ normal and abnormal feature selection is magnitude-based in MGFN, this results in varying model performance, driven by the exposure to differing subsets of events.

Table 1: Performance comparison of different anomaly detection methods under anonymization.   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">Protection</td><td colspan="5">Metrics (%)</td></tr><tr><td>AUC</td><td>AP</td><td>AUCsub</td><td>APsub</td><td>FAR (↓)</td></tr><tr><td rowspan="5">MGFN [2]</td><td>Raw-Reported</td><td>86.98</td><td></td><td></td><td></td><td></td></tr><tr><td>Raw-Ours</td><td>80.16</td><td>22.05</td><td>60.96</td><td>24.50</td><td>19.02</td></tr><tr><td>HB</td><td>80.57</td><td>20.45</td><td>59.93</td><td>23.67</td><td>26.42</td></tr><tr><td>HM</td><td>79.49</td><td>18.05</td><td>59.57</td><td>22.49</td><td>2.80</td></tr><tr><td>HEN</td><td>81.20</td><td>22.04</td><td>60.99</td><td>24.86</td><td>17.65</td></tr><tr><td rowspan="5">UR-DMU [3]</td><td>H2D</td><td>79.46</td><td>18.70</td><td>57.66</td><td>21.56</td><td>8.69</td></tr><tr><td>Raw-Reported</td><td>86.97</td><td>35.59</td><td>=</td><td>1</td><td>1.05</td></tr><tr><td>Raw-Ours</td><td>85.85</td><td>30.21 35.51</td><td>69.85</td><td>32.80 37.80</td><td>2.39</td></tr><tr><td>HB HM</td><td>85.63 85.08</td><td></td><td>69.30</td><td></td><td>6.11</td></tr><tr><td>HEN</td><td>81.36</td><td>35.20</td><td>66.80</td><td>36.96</td><td>2.67</td></tr><tr><td rowspan="5">BN-WVAD [4]</td><td>H2D</td><td></td><td>22.33</td><td>62.50</td><td>25.13</td><td>2.02</td></tr><tr><td>Raw-Reported</td><td>84.79</td><td>29.79</td><td>68.21</td><td>29.48</td><td>1.77</td></tr><tr><td>Raw-Ours</td><td>87.24 83.55</td><td>36.26 28.73</td><td>71.71 62.54</td><td>38.13 30.69</td><td></td></tr><tr><td>HB</td><td>81.72</td><td>26.29</td><td>61.26</td><td>29.02</td><td>47.98 44.74</td></tr><tr><td>HM</td><td>84.52</td><td></td><td></td><td>32.81</td><td></td></tr><tr><td rowspan="5"></td><td>HEN</td><td></td><td>30.83</td><td>66.79</td><td></td><td>99.65</td></tr><tr><td>H2D</td><td>81.22</td><td>25.57</td><td>85.13</td><td>27.55</td><td>100</td></tr><tr><td>Raw-Reported</td><td>82.72</td><td>26.06</td><td>60.94</td><td>28.02</td><td>35.18</td></tr><tr><td>Raw-Ours</td><td>86.76</td><td>1</td><td>=</td><td></td><td>0.57</td></tr><tr><td></td><td>85.16</td><td>31.07</td><td>68.23</td><td>32.86</td><td>0.35</td></tr><tr><td rowspan="5">PEL4VAD [5]</td><td>HB</td><td>84.72</td><td>30.96</td><td>68.53</td><td>33.08</td><td>0.63</td></tr><tr><td>HM</td><td>84.53</td><td>30.66</td><td>67.62</td><td>32.44</td><td>0.22</td></tr><tr><td>HEN</td><td>82.52</td><td>23.44</td><td>61.90</td><td>25.31</td><td>0.45</td></tr><tr><td>H2D</td><td>84.21</td><td>27.14</td><td>66.43</td><td>29.07</td><td>0.32</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

UR-DMU exhibits robustness across most anonymization types, which is attributed to the enforced separability by the dual-memory architecture that stores normal and anomaly instances in different memories. This ensures that each memory unit learns only the relevant (normal or abnormal) patterns. This approach enables preservation of discriminative features despite the added noise.

BN-WVAD demonstrates substantial invariance to human masking, with AUC scores surpassing the performance on non-anonymized data. However, this comes at the cost of a significantly higher false alarm rate $( 9 9 . 6 5 \%$ ). This surprising result may be attributed to the DFM anomaly criterion, which detects

14   
13   
12   
11 1   
10 9 Raw HB HM HEN H2D

deviations from normal by measuring divergence from the BatchNorm-computed mean. Due to the noise introduced by human segmentation, such as inaccurate or missing segments, and the additional distortions from human region replacements, the deviation from BatchNorm statistics becomes an ill-posed criterion. This results in a significant performance fluctuations and a high false alarm rate. The effect is particularly evident in videos with poor anonymization quality, as illustrated in Figure 3.

PEL4VAD similarly to UR-DMU, exhibits robustness across anonymization types, which is attributed to the explicit modeling of anomalous events context through the prior semantic prompts. These experimental results with PEL4VAD and UR-DMU reveal that without the explicit modeling of nuances that separate the normal and anomaly events during training, the anomaly detection model might end up sensitive to certain patterns or artifacts produced by the anonymization. This sensitivity may arbitrarily boost or degrade the performance, not as a reflection of model effectiveness, but as a reaction to noise-induced variability.

# 4.3 VAD Comparative Analysis

We further analyse the class-wise AUC for the investigated VAD methods in Figure 4. The UR-DMU exhibits a consistency in the class-wise AUC among the different anonymization techniques. The class-wise AUC scores follow the raw-trained model, except for encryption, which impairs the performance significantly for the “Assault”, “Shoplifting”, and “Vandalism” classes. This is attributed to the aggressive transformations of encryption, which distort the spatial and temporal information, introducing non-smooth transitions in videos. The lack of temporal smoothness affects the snippet features by disturbing the subtle visual cues essential for accurate identification of these classes. The same observation is seen with the PEL4VAD model on the same classes under the encryption technique, which validates the previous explanation. On the contrary, the MGFN model improves the performance with most anonymization techniques on the classes, such as “Arrest”, “Explosion”, “Fighting”, “Shoplifting”, “Stealing”, and “Vandalism”. This unintentional positive effect of anonymization arises from the dependence on the magnitude amplification technique, as explained earlier, and reflects the model over-sensitivity to noise rather than true effectiveness on context-based anomaly separation. Similarly, the BN-WVAD model shows improvement for some classes particularly under the masking effect. However, these gains are offset by a significantly high false alarm rate. This finding validates the earlier explanation, in which the abnormality criterion of the BN-WVAD model relies on the batch statistics, which are disturbed by the anonymization, leading to a significantly high false alarm rate.

![](images/b230559fd04d8c35111986793bbc2987513a0ae0125ce517afc3cb4af98d1946.jpg)  
Fig. 3: t-SNE visualizations of Arson041 feature snippets, and their divergence from BatchNorm mean (DFM scores) under each anonymization technique (b-f). The DFM scores are computed using the Mahalanobis distance on the enhanced features [4], and the full video is treated as a batch. Higher DFM scores are represented by lighter colors (g). The video summary in (a) displays frames of the human-masked version, highlighting segmentation flaws in red and orange. These inconsistencies introduce feature variability, increasing the DFM scores for many normal snippets (evident in the lighter points without red circles), and contributing to a higher false alarm rate.

![](images/856c04db6af4bc4ba3e5c301e13b8c4787baa8718567eaaf5075536e70816669.jpg)  
Fig. 4: Class-wise AUC performance comparison for the investigated VAD methods.

# 4.4 Qualitative Results

Figure 5 presents qualitative results of the UR-DMU and PEL4VAD models performance across different anonymized versions of AUCF-Crime. Notably, there are significant variations in frame-level anomaly scores across anonymization types, which are attributed to the anonymization artifacts and inaccurate detections between the consecutive frames that act as noise. The variations are more significant for blurring and encryption (HB and HEN), resulting in an increased false alarm rate in normal events. However, in most classes, the anomaly scores closely follow the scores on raw data, which indicates that these anonymizationinduced fluctuations can be filtered out by selecting the appropriate threshold. These results highlight the resilience and invariance of the UR-DMU and PEL4VAD models to noise and distortions caused by anonymization, which emphasizes the importance of a VAD method selection for satisfactory performance on anonymized data.

![](images/a9dc4ee5964fe05eeda03d4053cbab18412bfb05b5c7c4ba52b7c982e3a7438f.jpg)  
Fig. 5: Qualitative results of UR-DMU (top) and PEL4VAD (bottom) anomaly detection under anonymization.

Table 2: Comparison between the best results obtained under conventional anonymization (with ${ < } 1 \%$ performance drop) and privacy-by-design methods (SPAct and TeD-SPAD) when applied to anomaly detection. \*indicates that the scores are reported from original papers.   

<html><body><table><tr><td>Method</td><td>AUC(%)(↑) Relative drop (%)(↓)</td></tr><tr><td>UR-DMU [3] (Raw-Ours) PEL4VAD [5] (Raw-Ours)</td><td>85.85 85.16</td></tr><tr><td>SPAct [7]*</td><td>- ↓ 4.83 %</td></tr><tr><td>TeD-SPAD [7]*</td><td>73.93 √3.69 %</td></tr><tr><td>UR-DMU-HB</td><td>74.81</td></tr><tr><td>UR-DMU-HM</td><td>85.63 √0.25% 85.08</td></tr><tr><td>PEL4VAD-HB</td><td>↓ 0.89 % 84.72 ↓ 0.51 %</td></tr><tr><td>PEL4VAD-HM</td><td>84.53 ↓ 0.74 %</td></tr></table></body></html>

# 4.5 Privacy-by-Design vs. Conventional Anonymization

We present the comparative analysis between the conventional anonymization techniques in this study and recent privacy-by-design solutions, SPAct and TeDSPAD [6, 7], in Table 2. These solutions employ a weakly supervised anomaly detection approach, based on the MGFN [2]. It is shown that, despite being more protective, privacy-by-design methods exhibit a higher performance drop in comparison to conventional anonymization techniques.

It is important to highlight a key methodological difference compared to evaluations in these previous works. In the original privacy-by-design studies, comparisons with blurring and blackening were performed based on boundingbox based anonymization. This presents the extreme form of human obfuscation that removes significant shape information and contextual details. In contrast, our evaluation employs human-segment anonymization, preserving the crucial shape and contextual information. Consequently, the results indicate that human-segment anonymization can achieve higher AUC scores and lower performance drops than those reported by privacy-by-design methods.

Conventional anonymization techniques provide a lower privacy protection degree compared to privacy-by-design solutions. However, they are shown to maintain satisfactory anomaly detection performance, and are flexible to other forms of image processing algorithms [30]. On the contrary, even though privacyby-design solutions offer optimal protection, they compromise utility to a higher degree than the privacy gain. In addition, they exhibit poor applicability of their data to generic image processing tasks, due to the massive information loss. These observations imply an essential yet often overlooked trade-off in evaluating the privacy-by-design approaches, which is the balance between achieving robust privacy protection and retaining broad applicability across diverse utility tasks.

# 5 Conclusion

In this paper, we have analyzed the robustness of multiple anomaly detection methods under four human anonymization techniques that selectively mask, distort, or replace sensitive body regions. Our experiments demonstrate that conventional anonymization does not inherently prevent anomaly detection. Some VAD methods, such as MGFN with encryption or BN-WVAD with masking, can inadvertently show performance gain in response to the anonymization artifacts, whereas UR-DMU and PEL4VAD show robustness across all anonymization types. The evaluations indicate that the performance of anomaly detection models, that employ I3D features under anonymization, highly depends on the model design and the learning strategy, suggesting that neither anonymity nor accuracy need to be entirely sacrificed. Furthermore, when comparing the conventional anonymization techniques with privacy-by-design solutions, the analysis reveals an often-neglected trade-off between the application flexibility and privacy protection. Conventional anonymization can preserve scene context for multiple downstream tasks, whereas privacy-by-design methods typically optimize for a single target utility with compromised performance. We expect these findings to encourage further comparative investigations into how VAD methods handle anonymization artifacts and how much they rely on sensitive human details. We also aim to inspire new methods that ensure robust privacy while maintaining generalizable feature representations.