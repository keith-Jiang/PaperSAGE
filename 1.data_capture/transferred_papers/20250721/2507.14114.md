# Weighted Matching in a Poly-Streaming Model\*

Ahammed Ullah† S M Ferdous‡ Alex Pothen†

# Abstract

We introduce the poly-streaming model, a generalization of streaming models of computation in which $k$ processors process $k$ data streams containing a total of $N$ items. The algorithm is allowed $\mathcal { O } \left( f ( k ) \cdot M _ { 1 } \right)$ space, where $M _ { 1 }$ is either $o \left( N \right)$ or the space bound for a sequential streaming algorithm. Processors may communicate as needed. Algorithms are assessed by the number of passes, per-item processing time, total runtime, space usage, communication cost, and solution quality.

We design a single-pass algorithm in this model for approximating the maximum weight matching (MWM) problem. Given $k$ edge streams and a parameter $\varepsilon > 0$ , the algorithm computes a $( 2 + \varepsilon )$ -approximate MWM. We analyze its performance in a shared-memory parallel setting: for any constant $\varepsilon > 0$ , it runs in time $\widetilde { \mathcal { O } } \left( L _ { m a x } + \dot { n } \right)$ , where $n$ is the number of vertices and $L _ { m a x }$ is the maximum stream length. It supports $\mathcal { O } \left( 1 \right)$ eper-edge processing time using $\widetilde { \mathcal { O } } \left( k \cdot n \right)$ space. We further generalize the design to hierarchical architectures, in which $k$ processors ar peartitioned into $r$ groups, each with its own shared local memory. The total intergroup communication is $\widetilde { \mathcal { O } } \left( \boldsymbol { r } \cdot \boldsymbol { n } \right)$ bits, while all other performance guarantees are preserved.

We evaluate the algorithm on a sha ed-memory system using graphs with trillions of edges. It achieves substantial speedups as $k$ increases and produces matchings with weights significantly exceeding the theoretical guarantee. On our largest test graph, it reduces runtime by nearly two orders of magnitude and memory usage by five orders of magnitude compared to an offline algorithm.

# 1 Introduction

Data-intensive computations arise in data science, machine learning, and science and engineering disciplines. These datasets are often massive, generated dynamically, and, when stored, kept in distributed formats on disks, making them amenable to processing as multiple data streams. The modularity of these datasets can be exploited by streaming algorithms designed for tightly-coupled shared-memory and distributed-memory multiprocessors to efficiently solve large problem instances that offline algorithms cannot handle due to their high memory requirements. However, the design of parallel algorithms that process multiple data streams concurrently has not yet received much attention.

Current multicore shared-memory processors consist of up to a few hundred cores, organized hierarchically to share caches and memory controllers. These cores compute in parallel to achieve speedups over serial execution. With multiple memory controllers, I/O operations can also proceed in parallel, and this feature can be used to process multiple data streams concurrently. These I/O capabilities and the limitations of offline algorithms motivate a model of computation, illustrated in Figure 1 and discussed next.

The streaming model of computation allows $o \left( N \right)$ space for a data stream of size $N$ [2, 25]. For graphs, the semi-streaming model permits $\mathcal { O }$ $\begin{array} { r } { { \mathcal { O } } \left( n \cdot \operatorname { p o l y l o g } n \right) , } \end{array}$ ) space for a graph with $n$ vertices and an edge stream of arbitrary length [16]. Building on these space-constrained models, we introduce the poly-streaming model. The key aspects of our model are as follows.

We consider $k$ data streams that collectively contain $N$ items. An algorithm has access to $k$ (abstract) processors and is allowed $\mathcal { O } \left( f ( k ) \cdot M _ { 1 } \right)$ total space, where $M _ { 1 }$ is either $o \left( N \right)$ or the space permitted to a single-stream algorithm. In each pass, each stream is assigned to one of the processors, and each processor independently reads one item at a time from its stream and processes it. Processors may communicate as needed, either via shared or remote memory access. Algorithms are assessed on several metrics: space complexity, number of passes, per-item processing time, total runtime, communication cost, and solution quality.

![](images/df4de02cd22f39c7a00046fde3e96f6b10bddabe2f20b92dd1d60cb1b0536bce.jpg)  
Figure 1: A schematic diagram of the poly-streaming model for shared-memory parallel computers. Processors $\{ P _ { \ell } \} _ { \ell \in [ k ] }$ have access to $\mathcal { O } \left( f ( k ) \cdot \bar { M _ { 1 } } \right)$ memory collectively, depicted with the rectangle connected to the processors.

In the poly-streaming model, we address the problem of approximating a maximum weight matching (MWM) in an edge-weighted graph, where the goal is to find a set of vertex-disjoint edges with maximum total weight. We design an algorithm for approximating an MWM when the graph is presented as multiple edge streams. Our design builds on the algorithm of [39] and adds support for handling multiple streams concurrently. We also generalize the design to NUMA (non-uniform memory access) multiprocessor architectures.

We summarize our contributions to the MWM problem as follows. Let $L _ { m a x }$ and $L _ { m i n }$ denote the maximum and minimum lengths of the input streams, respectively, and let $n$ denote the number of vertices in a graph G. For any realization of the CREW PRAM model (such as in Figure 1), we have the following result.

Theorem 1.1. For any constant $\varepsilon > 0$ , there exists $a$ single-pass poly-streaming algorithm for the maximum weight matching problem that achieves a $( 2 + \varepsilon )$ -approximation. It admits a CREW PRAM implementation with runtime $\widetilde { \mathcal { O } } \left( L _ { m a x } + n \right)$ .1

If $L _ { m i n } = \Omega ( n ) .$ , the algorithm achieves ${ \mathcal { O } } \left( \log n \right)$ amortized per-edge processing time using $\widetilde { \mathcal { O } } \left( k + n \right)$ space. For arbitrarily balanced streams, it uses either:

• $\widetilde { \mathcal { O } } \left( k + n \right)$ space and $\widetilde { \mathcal { O } } \left( n \right)$ per-edge processing time, or • $\widetilde { \mathcal { O } } \left( k \cdot n \right)$ space and $\mathcal { O } \left( 1 \right)$ per-edge processing time.

In NUMA architectures, memory access costs depend on a processor’s proximity to the target memory. We generalize the algorithm in Theorem 1.1 to account for these cost differences. In particular, we show that when $k$ processors are partitioned into $r$ groups, each with its own shared local memory, the total number of global memory accesses across all groups is $\widetilde { \mathcal { O } } \left( \boldsymbol { r } \cdot \boldsymbol { n } \right)$ . This generalization preserves all other performance guarantees from Theorem 1.1, except that th e $\widetilde { \mathcal { O } } \left( k + n \right)$ space bound becomes $\widetilde { \mathcal { O } } \left( k + r \cdot n \right)$ . These results are formalized in Theorem 4.2 in Section 4. T sedesign gives a memory-efficie ealgorithm for the NUMA shared memory multiprocessors, on which we report empirical results.

We have evaluated our algorithm on a NUMA machine using graphs with billions to trillions of edges. For most of these graphs, our algorithm uses space that is orders of magnitude smaller than that required by offline algorithms. For example, storing the largest graph in our evaluation would require more than 91,600 GB $( \approx 9 0 \mathrm { T B } )$ , whereas our algorithm used less than 1 GB. Offline matching algorithms typically require even more memory to accommodate their auxiliary data structures.

We employ approximate dual variables that correspond to a linear programming relaxation of MWM to obtain a posteriori upper bounds on the weights of optimal matchings. These bounds allow us to compare the weight of a matching produced by our algorithm with the optimal weight. Thus, we show that our algorithm produces matchings whose weights significantly exceed the approximation guarantee.

For $k = 1 2 8$ , our algorithm achieves runtime speedups of 16–83 across all graphs in our evaluation, on a NUMA machine with only 8 memory controllers. This is significant scaling for a poly-streaming algorithm, given that 8 memory controllers are not sufficient to serve the concurrent and random access requests of 128 processors without delays. Nevertheless, these speedups demonstrate the effectiveness of our design, which accounts for a processor’s proximity to the target memory. A metric less influenced by memory latency suggests that the algorithm would achieve even better speedups on architectures with more efficient memory access.

Note that Theorem 1.1 and Theorem 4.2 both guarantee $\widetilde { \mathcal { O } } \left( L _ { m a x } + n \right)$ runtime. This is optimal up to polylogarithmic factors when $L _ { m a x } = \Omega \left( n \right)$ . However, by  seing ${ \widetilde { \mathcal { O } } } \left( k \cdot n \right)$ space and $\mathcal { O } \left( 1 \right)$ per-edge processing time, we can achieve a runtime of $\widetilde { \mathcal { O } } \left( L _ { m a x } + n / k \right)$ , which  ecomes polylogarithmic for sufficiently large $k$ (see Appendix B.4).

Organization. Section 2 describes the details of our model. Section 3 presents the design and analyses of our algorithm in Theorem 1.1. In Section $^ { 4 , }$ we extend the design to NUMA architectures. Section 5 summarizes the evaluation results. We conclude in Section 6 with a discussion of future research directions.

# 2 The Poly-Streaming Model

This section elaborates on our model of computation and discusses its novelty and significance relative to existing models (Section 2.1).

In the poly-streaming model, there are $k$ data streams containing a total of $N$ items. An algorithm may use $k$ processors and up to $\mathcal { O } \left( f ( k ) \cdot M _ { 1 } \right)$ total space, where $M _ { 1 }$ is either $o \left( N \right)$ or the space permitted to a single-stream algorithm (as in the semi-streaming model). In each pass, each stream is assigned to a processor, and processors independently read items from their respective streams. Processing these items may require coordination. An algorithm may use the processors to perform any necessary preprocessing and post-processing. Processors may communicate as needed during preprocessing, streaming, or postprocessing, via shared or remote memory.

Note that the $\mathcal { O } \left( f ( k ) \cdot M _ { 1 } \right)$ space constraint subsumes the $\mathcal { O } \left( f ( k ) + M _ { 1 } \right)$ constraint. We now describe the model’s components in more detail.

Processors. Each processor is an abstract unit of computation that can be emulated by a physical thread on a shared-memory or tightly coupled distributed-memory machine. Figure 1 illustrates a realization in which all processors directly access a shared workspace, corresponding to a shared-memory implementation. Multiple such realizations can be connected via high-speed networks to implement the model on a distributed-memory machine.

Data Streams. The model assumes an arbitrary distribution of data across streams. Algorithms must handle arbitrary inputs with imbalanced partitioning and arbitrary item orderings. A stream may be assigned to a processor multiple times, each assignment constituting a pass. Within a pass, streams are read asynchronously, though processing individual items may require synchronization. The parameter $k$ need not equal the number of physical input streams: physical streams may be merged or split into $k$ logical streams, which are then mapped to processors.

Space. The bound $\mathcal { O } \left( f ( k ) \cdot M _ { 1 } \right)$ reflects the observation that, in practice, total memory typically scales with the number of processors. In most cases, $f ( k )$ is expected to be linear in $k ,$ but superlinear growth may still be feasible, particularly for algorithms that use $\bar { \mathcal { O } } \left( f ( k ) + M _ { 1 } \right)$ space. This formulation supports the analysis of a broad range of design choices and their associated trade-offs. It also enables a bottomup design approach, where algorithms developed for shared-memory machines can be extended to tightly coupled distributed-memory machines.

Per-Item Processing Time. A key consideration in the poly-streaming setting is whether an algorithm can handle an influx of items arriving in quick succession, as may occur when $k ^ { \prime } \gg k$ physical streams are merged into $k$ logical streams. If the algorithm cannot handle such influxes within bounded space, its correctness may be compromised. For suitable choices of $f ( \cdot )$ , the $f ( k )$ -fold space may suffice to design bounded-space algorithms for many such scenarios.

For some design choices, the worst-case per-item processing time may not be informative. In such cases, under realistic assumptions, amortized or average per-item processing time may better reflect actual performance. In particular, amortizing over the number of items per stream, rather than over the total input, can yield a more accurate estimate of this cost.

Runtime. The runtime refers to the total time spent on preprocessing, streaming, and post-processing across all passes. It includes delays caused by contention when accessing shared or remote memory. The cost of remote memory access is assumed to be proportional to the level of contention at the target location.

The runtime of an algorithm should, in general, be dominated by a function of the maximum stream length, denoted $L _ { m a x }$ . It may also depend on other parameters, such as $M _ { 1 }$ , which remains non-dominating for $\mathbf { \bar { \rho } } _ { M _ { 1 } } = \mathcal { O } \left( L _ { m a x } \right)$ . Under worst-case data distribution, $L _ { m a x } = \mathcal { O } \left( N \right)$ . As stream lengths become more balanced, that is, as $L _ { m a x }$ approaches its lower bound $\Theta \left( N / k \right)$ , the runtime should scale accordingly. In such balanced settings, an algorithm may leverage the $f ( k )$ -fold space to match the runtime of scalable offline algorithms, for example, achieving polylogarithmic runtime for sufficiently large $k$ .

Solution Quality. Poly-streaming algorithms are generally expected to admit provable bounds on solution quality, such as approximation ratios. These may be complemented by empirical performance bounds, such as a posteriori guarantees based on upper or lower bounds on the optimal. Such guarantees are particularly important, since streaming algorithms are often provably unable to compute exact solutions within a few passes. The space constraint may also facilitate the exploration of trade-offs between space and solution quality.

Number of Passes. A central goal in this model is to design single-pass algorithms. Many initial designs may require multiple passes, with single-pass algorithms emerging only after substantial algorithmic development. In some cases, multiple passes may be provably necessary to achieve objectives such as stronger approximation guarantees under tighter space bounds. Thus, the number of passes serves as a fundamental measure of algorithmic efficiency.

Communication. The communication cost of an algorithm is defined as the total number of remote memory accesses. This abstraction excludes interconnection latency and other architecture-specific delays, as is standard in theoretical models to simplify algorithm design.

# 2.1 Novelty and Significance

For descriptions of existing models referenced here, see Appendix A.

Parallel Computation. The poly-streaming model targets areas of computation beyond the reach of traditional parallel models, such as the work-depth model. In terms of input scale, poly-streaming algorithms are designed for datasets that offline parallel algorithms cannot handle due to their impractical memory requirements.

Another key distinction is that offline parallel algorithms assume random access to the entire input. In contrast, a central motivation for streaming models is to minimize expensive random accesses to massive, persistent datasets. The goal is to replace many random accesses with a small number of sequential passes, which are typically more efficient in practice.

Modern parallel file systems support concurrent, high-throughput access to data by allowing multiple simultaneous connections. By leveraging the parallel I/O capabilities of modern shared-memory machines, poly-streaming algorithms can exploit these systems to efficiently process massive datasets while avoiding costly random accesses.

Distributed Computation. The poly-streaming model supports asynchronous communication protocols, in contrast to models that count synchronous communication rounds, such as the MPC model and the distributed streaming model. In tightly coupled shared- and distributed-memory multiprocessors, synchronous coordination is often unnecessary for designing communication-efficient algorithms, particularly when architectures support remote memory access. In systems based on message passing, such access can be emulated by assigning processors to mediate access to shared locations via messages.

Appendix B.6 sketches the design of a distributed algorithm based on asynchronous communication. This algorithm achieves optimal communication cost (up to polylogarithmic factors), supports streaming computation, and dominates comparable MPC algorithms across several metrics. Moreover, it is single-pass, which is unlikely to be achievable under the synchronous communication protocols of existing distributed streaming models.

Streaming Computation. Traditional offline parallel models provide frameworks for optimizing time in isolation, while sequential streaming models, such as those described in Appendix A, focus on optimizing space in isolation. The poly-streaming model offers a unified framework for optimizing both time and space jointly. Its support for asynchronous communication protocols enables the design of parallel algorithms that are not permitted in existing models, such as the distributed streaming model. Section 3 and Section 4 present examples of such algorithms.

Analyzing Trade-offs. A central theme in streaming literature is that space constraints often conflict with other performance metrics, such as solution quality, number of passes, and per-item processing time. Analyzing the trade-offs between space and these metrics remains an active area of research; see [4, 17, 5] for examples. Moreover, processing multiple streams concurrently may require trade-offs that do not arise in the single-stream setting; see Section 3, Section 4, and Appendix B.3 for examples of time–space tradeoffs. The space constraint in the poly-streaming model provides a unified framework for analyzing such trade-offs

Hierarchical Design. The space constraint in the poly-streaming model enables a bottom-up design approach, where algorithms developed for shared-memory systems can be extended to tightly coupled distributed-memory systems. This requires algorithm designers to account for memory hierarchy in order to manage and quantify communication costs of the resulting algorithms. Section 3.2, Section 4, and Appendix B.6 collectively illustrate such a hierarchical design process.

Practical Relevance. Modern computing environments are inherently multicore, with total memory typically scaling with the number of cores. Yet, such environments often fail to meet the space requirements of offline parallel algorithms for large problem instances. Conversely, sequential streaming algorithms underutilize both cores and memory, as they are not designed to exploit multicore architectures. These limitations warrant new paradigms of computation, as directly addressed by the poly-streaming model.

# 3 Algorithms for Uniform Memory Access Cost

In this section, we present the design and analyses of our algorithm in Theorem 1.1 that assumes a uniform memory access cost.

# 3.1 Preliminaries

For a graph $G = \left( V , E \right)$ , let $n : = | V |$ and $m : = | E |$ denote the number of vertices and edges, respectively. We denote an edge $\boldsymbol { e } : = \{ u , v \}$ by the unordered pair of its endpoints. Let $\mathcal { N } \left( e \right)$ be the set of edges in $E$ that share an endpoint with edge e. For a weighted graph, let $w _ { e }$ denote the weight of edge $e$ , and for any subset $A \subseteq E ,$ , define $w ( A ) : = \textstyle \sum _ { e \in A } w _ { e }$ . For $\ell \doteq [ k ] ,$ , let $\hat { E } ^ { \ell }$ be the set of edges received in the ℓth stream. Define $L _ { m a x } : = \operatorname* { m a x } _ { \ell \in [ k ] } | E ^ { \ell } |$ and $\begin{array} { r } { L _ { m i n } : = \operatorname* { m i n } _ { \ell \in [ k ] } | E ^ { \ell } | } \end{array}$ .

A matching ${ \mathcal { M } } \subseteq E$ is a set of edges that do not share endpoints. A maximum weight matching (MWM) $\mathcal { M } ^ { * }$ is a matching with maximum total weight; that is, $w \left( \mathcal { M } ^ { \ast } \right) \geq w \left( \mathcal { M } \right)$ for all matchings ${ \mathcal { M } } \subseteq E$ .

A $\rho$ -approximation algorithm computes a solution whose value is within a factor $\rho$ of the optimal. The factor $\rho$ is called the (worst-case) approximation ratio. We assume $\rho \geq 1$ for both maximization and minimization problems. Thus, for maximization, a $\rho$ -approximation guarantees a solution whose value is at least $\frac { 1 } { \rho }$ times the optimal.

Primal LP Dual LP maximize ∑ wexe minimize ∑ yu subject to e∈E∑ xe ≤ 1, for all u ∈ V subject to u∑∈Vyu ≥ we, for all e E e∈δ(u) u e xe ≥ 0, for all e E yu 0, for all u V

We use the linear programming (LP) relaxation of the MWM problem and its dual, shown in Figure 2. In the primal LP, each variable $x _ { e }$ is 1 if edge $e$ is in the matching and 0 otherwise. Each $y _ { u }$ is a dual variable, and $\delta ( u )$ denotes the set of edges incident on a vertex $u$ . Let $\{ x _ { e } \} _ { e \in E }$ and $\{ y _ { u } \} _ { u \in V }$ be feasible solutions to the primal and dual LPs, respectively. By weak LP duality, we have $\textstyle \sum _ { e \in E } w _ { e } x _ { e } \leq \sum _ { u \in V } y _ { u }$ . If $\{ x _ { e } \} _ { e \in E }$ is an optimal solution to the primal LP, then $\begin{array} { r } { w \left( \mathcal { M } ^ { * } \right) \leq \sum _ { e \in E } w _ { e } x _ { e } \leq \sum _ { u \in V } y _ { u } } \end{array}$ . The first inequality holds because the primal LP is a relaxation of the MWM problem.

# 3.2 The Algorithm

Several semi-streaming algorithms have been developed for the MWM problem [4, 11, 14, 16, 20, 21, 37, 39, 42] (see Section B.1 for brief descriptions of these algorithms). In this paper, we focus exclusively on the single-pass setting in the poly-streaming model. Our starting point is the algorithm of Paz and Schwartzman [39], which computes a $2 + \varepsilon$ -approximation of MWM. This is currently the best known guarantee in the single-pass setting under arbitrary or adversarial ordering of edges2. We extend a primal-dual analysis by Ghaffari and Wajc [21] to analyze our algorithm.

The algorithm of Paz and Schwartzman [39] proceeds as follows. Initialize an empty stack S and set $\alpha _ { u } = 0$ for each vertex $u \in V$ . For each edge $e = \left\{ u , v \right\}$ in the edge stream, skip $e$ if $w _ { e } < ( 1 + \varepsilon ) ( \alpha _ { u } + \alpha _ { v } )$ . Otherwise, compute $g _ { e } = w _ { e } - \left( \alpha _ { u } + \alpha _ { v } \right)$ , push $e$ onto the stack $S _ { \nu }$ , and increase both $\alpha _ { u }$ and $\alpha _ { v }$ by $g _ { e }$ . After processing all edges, compute a matching $\mathcal { M }$ greedily by popping edges from S.

Note that for each edge pushed onto the stack, the increment $g _ { e } ~ = ~ w _ { e } - \left( \alpha _ { u } + \alpha _ { v } \right)$ satisfies $g _ { e } \ \geq$ $\varepsilon \left( \alpha _ { u } + \alpha _ { v } \right)$ . This ensures that both $\alpha _ { u }$ and $\alpha _ { v }$ increase by a factor of $1 + \varepsilon$ . Hence, the number of edges in the stack incident to any vertex is at most $\begin{array} { r } { \log _ { 1 + \varepsilon } ( W ) = \mathcal { O } \left( \frac { \log W } { \varepsilon } \right) . } \end{array}$ , where $W$ is the (normalized) maximum edge weight. Therefore, the total number of edges in the stack is $\begin{array} { r } { \mathcal { O } \left( \frac { n \log W } { \varepsilon } \right) = \mathcal { O } \left( \frac { n \log n } { \varepsilon } \right) } \end{array}$ .3

To design a poly-streaming algorithm, we begin with a simple version and then refine it. $\mathsf { A l l } k$ processors share a global stack and a set of variables $\{ \alpha _ { u } \} _ { u \in V } ,$ and each processor runs the above sequential streaming algorithm on its respective stream. To complete and adapt this setup for efficient execution across multiple streams, we must address two interrelated issues: (1) concurrent edge arrivals across streams may lead to contention for the shared stack or variables, and (2) concurrent updates to the shared variables may lead to inconsistencies in their observed values.

A natural approach to addressing these issues is to enforce a fair sequential strategy, where processors access shared resources in a round-robin order. While this ensures progress, it incurs $\mathcal { O } \left( k \right)$ per-edge processing time, which scales poorly with increasing $k$ . Instead, we adopt fine-grained contention resolution that avoids global coordination by allowing processors to operate asynchronously. However, under the initial setup, this leads to $\widetilde { \mathcal { O } } \left( n / \varepsilon \right)$ per-edge processing time: a processor may be blocked from accessing shared resources until the teack has accumulated its $\tilde { \mathcal { O } } \left( n / \varepsilon \right)$ potential edges. We address these limitations with the following design choices.

PS-MWM(V, ℓ, ε)   
/\* each processor executes this algorithm concurrently \*/ 1. In parallel initialize $l o c k _ { u } ,$ and set $\alpha _ { u }$ and mar $\mathfrak { c } _ { u }$ to 0 for all $u \in V$ /\* processor $\ell$ initializes or sets $\Theta \left( n / k \right)$ locks/variables \*/ 2. $S ^ { \ell } \gets \emptyset ~ / ^ { * }$ initialize an empty stack \*/ 3. for each edge $\boldsymbol { e } = \{ u , v \}$ in $\ell$ th stream do (a) $\mathrm { ~ \ P r o c e s s { \mathrm { - } } E d g e } ( e , S ^ { \ell } , \varepsilon )$ 4. wait for all processors to complete execution of Step $3 ~ / { ^ * }$ a barrier \*/ 5. ℓ Process-Stack $( S ^ { \ell } )$ 6. return ℓ

For the first issue, we observe that a global ordering of edges, as used in the single-stack solution, is not necessary; local orderings within multiple stacks suffice. In particular, we can identify a subset of edges (later referred to as tight edges) for which maintaining local orderings is sufficient to compute a $2 + \varepsilon$ -approximate MWM. Hence, we can localize computation using $k$ stacks, assigning one stack to each processor exclusively during the streaming phase. This design eliminates the $\tilde { \mathcal { O } } \left( n / \bar { \varepsilon } \right)$ contention associated with a shared stack.

However, contention still arises when updating the variables $\{ \alpha _ { u } \} _ { u \in V }$ . It is unclear how to resolve this contention without using additional space. Hence, we consider two strategies for processing edge streams that illustrate the trade-off between space and per-edge processing time. In the first, which we call the non-deferrable strategy, the decision to include an edge in a stack is made immediately during streaming. In the second, which we call the deferrable strategy, this decision may be deferred to post-processing. The latter strategy requires more space but achieves $\mathcal { O }$ (1) per-edge processing time.

To address the second issue, which concerns the potential for inconsistencies due to concurrent updates to the variables $\{ \alpha _ { u } \} _ { u \in V } ,$ we observe that the variables are monotonically increasing and collectively require only $\widetilde { \mathcal { O } } \left( n / \varepsilon \right)$ updates. Thus, for most edges that are not eligible for the stacks, decisions can be made by s meply reading the current values of the relevant variables. However, for the $\widetilde { \mathcal { O } } \left( n / \varepsilon \right)$ edges that are included in the stacks, we must update the corresponding variables. To ensure cons te ncy of these updates, we associate a lock with each variable in $\{ \alpha _ { u } \} _ { u \in V }$ . We maintain $\vert V \vert$ exclusive locks and allow a variable to be updated only after acquiring its corresponding lock.4

We now outline the non-deferrable strategy of our poly-streaming algorithm for the MWM problem (for the deferrable strategy see Appendix B.3). For simplicity, we assume that if a processor attempts to release a lock it did not acquire, the operation has no effect. We also assume that any algorithmic step described with the "in parallel" construct includes an implicit barrier (or synchronization primitive) at the end, synchronizing the processors participating in that step.

The non-deferrable strategy is presented in Algorithm PS-MWM, with two subroutines used by PSMWM described in Process-Edge (Figure 4) and Process-Stack (Figure 5). In PS-MWM, Steps 1–2 form the

Process-Edge(e = u, v , Sℓ, ε)

/\* Assumes access to global variables $\{ \alpha _ { x } \} _ { x \in V }$ and locks $\{ l o c k _ { x } \} _ { x \in V } { ^ { * } } /$ 1. if $w _ { e } \leq ( 1 + \varepsilon ) \big ( \alpha _ { u } + \alpha _ { v } \big )$ then return 2. repeatedly try to acquire $l o c k _ { u }$ and $l o c k _ { v }$ in lexicographic order of $u$ and $v$ as long as $w _ { e } > ( 1 + \varepsilon ) \big ( \alpha _ { u } + \alpha _ { v } \big )$ 3. if $w _ { e } > ( 1 + \varepsilon ) ( \alpha _ { u } + \alpha _ { v } )$ then (a) $g _ { e } \gets w _ { e } - ( \alpha _ { u } + \alpha _ { v } )$ (b) increment $\alpha _ { u }$ and $\alpha _ { v }$ by $g _ { e }$ (c) add $e$ to the top of $S ^ { \ell }$ along with $g _ { e }$

4. release $l o c k _ { u }$ and $l o c k _ { v } ,$ , and return

preprocessing phase, Steps 3–4 the streaming phase, and Step 5 the post-processing phase. Each processor $\bar { \ell } \in [ k ]$ executes PS-MWM asynchronously, except that all processors begin the post-processing phase simultaneously (due to Step 4) and then resume asynchronous execution.

In the subroutine Process-Edge, Step 2 ensures that all edges are processed using the non-deferrable strategy: a processor repeatedly attempts to acquire the locks corresponding to the endpoints of an edge $e = \{ \bar { u } , v \}$ until it succeeds or the edge becomes ineligible for inclusion in a stack. As a result, a processor executing Step 3 has a consistent view of the variables $\alpha _ { u }$ and $\alpha _ { v }$ . In Step 3(c), we store the gain $g _ { e }$ of an edge $e$ along with the edge itself for use in the post-processing phase.

When all $k$ processors are ready to execute Step 5 of PS-MWM, the $k$ stacks collectively contain all the edges needed to construct a $( 2 + \varepsilon )$ -approximate MWM, which can be obtained in several ways. In the subroutine Process-Stack, we outline a simple approach based on local edge orderings. We define an edge $\boldsymbol { e } = \{ u , v \}$ in a stack to be a tight edge if $w _ { e } + g _ { e } = \alpha _ { u } + \alpha _ { v }$ . Equivalently, an edge is tight if and only if all of its neighboring edges that were included after it in any stack have already been removed. Any set of tight edges can be processed concurrently, regardless of their positions in the stacks. In Process-Stack, we simultaneously process the tight edges that appear at the tops of the stacks.

# 3.3 Analyses

We now formally characterize several correctness properties of the algorithm and analyze its performance. These correctness properties include the absence of deadlock, livelock, and starvation. The performance metrics are space usage, approximation ratio, per-edge processing time, and total runtime.

To simplify the analysis, we assume that processors operate in a quasi-synchronous manner. In particular, to analyze Step 3 of Algorithm PS-MWM, we define an algorithmic superstep as a unit comprising a constant number of elementary operations.

Definition 3.1 (Superstep). A processor takes one superstep for an edge if it executes Process-Edge with at most one iteration of the loop in Step 2 (i.e., without contention), requiring $\mathcal { O }$ (1) elementary operations. Each additional iteration of Step 2 due to contention adds one superstep, with each such iteration also requiring $\mathcal { O } \left( 1 \right)$ operations.

Definition 3.2 (Effective Iterations). Effective iterations is the maximum number of supersteps taken by any processor during the execution of Step 3 of Algorithm PS-MWM.

Note that for $k = 1$ , the effective iterations equals the number of edges in the stream. Using this notion we align the supersteps of different processors and define the following directed graph.

# Process-Stack(Sℓ)

/\* Assumes access to global variables $\{ \alpha _ { x } \} _ { x \in V }$ and $\{ m a r k _ { x } \} _ { x \in V } * /$ 1. $\mathcal { M } ^ { \ell }  \varnothing$ 2. while $S ^ { \ell } \neq \varnothing$ do (a) remove the top edge $\boldsymbol { e } = \{ u , v \}$ of $S ^ { \ell }$ (b) if $w _ { e } + g _ { e } < \alpha _ { u } + \alpha _ { v }$ then wait for $e$ to be a tight edge $/ { } ^ { * } e$ is a tight edge if $w _ { e } + g _ { e } = \alpha _ { u } + \alpha _ { v } * /$ (c) if both $m a r k _ { u }$ and $m a r k _ { v }$ are set to 0 then $/ { ^ * }$ no locking is needed since $e$ is a tight edge \*/ i. $\mathcal { M } ^ { \ell } \gets \mathcal { M } ^ { \ell } \cup \{ e \}$ ii. set marku and $m a r k _ { v }$ to 1 (d) decrement $\alpha _ { u }$ and $\alpha _ { v }$ by $g _ { e }$ 3. return $\mathbf { \mathcal { M } } ^ { \ell }$

Definition 3.3 $( G ^ { ( t ) } )$ . For the tth effective iteration, consider the set of edges processed across all $k$ streams. Let $\boldsymbol { e } _ { \ell } = \left( u _ { \ell } , v _ { \ell } \right)$ denote the edge processed in the ℓth stream, where $\boldsymbol { u } _ { \ell }$ precedes $v _ { \ell }$ in the lexicographic ordering of the vertices. If processor $\ell$ is idle in the tth iteration, then $\boldsymbol { e } _ { \ell } = \boldsymbol { \mathcal { O } }$ . Define $G ^ { ( t ) } : = { \Big ( } V ^ { ( t ) } , E ^ { ( t ) } { \Big ) } ,$ where

$$
E ^ { ( t ) } : = \{ e _ { \ell } \mid \ell \in [ k ] \} a n d V ^ { ( t ) } : = \bigcup _ { ( u _ { \ell } , v _ { \ell } ) \in E ^ { ( t ) } } \{ u _ { \ell } , v _ { \ell } \} .
$$

The following property of $G ^ { ( t ) }$ is straightforward to verify.

Proposition 3.4. $G ^ { ( t ) }$ is a directed acyclic graph.

We show that Algorithm PS-MWM is free from deadlock, livelock, and starvation. Deadlock occurs when a set of processors forms a cyclic dependency, with each processor waiting for a resource held by another. Livelock occurs when a set of processors repeatedly form such a cycle, where each processor continually acquires and releases resources without making progress. Starvation occurs when a processor waits indefinitely for a resource because other processors repeatedly acquire it first. The following lemma shows that the streaming phase of PS-MWM is free from deadlock, livelock, and starvation.

Lemma 3.5. The concurrent executions of the subroutine Process-Edge is free from deadlock, livelock, and starvation.

Proof. Since the variables $\{ \alpha _ { u } \} _ { u \in V }$ are updated only while holding their corresponding locks, we treat the locks $\{ l o c k _ { u } \} _ { u \in V }$ as the only shared resources in Process-Edge.

Let $G ^ { ( t ) }$ be the graph defined in Definition 3.3. By Proposition 3.4, $G ^ { ( t ) }$ is a directed acyclic graph (DAG), and hence each of its components is also a DAG.

To reason about cyclic dependencies, we focus on components of $G ^ { ( t ) }$ involving processors executing Step 2 of Process-Edge. Every DAG contains at least one vertex with no outgoing edges. Thus, each such component includes an edge $\boldsymbol { e } _ { \ell } = \left( u _ { \ell } , v _ { \ell } \right)$ such that only processor $\ell$ requests $l o c k _ { v _ { \ell } }$ . This precludes the possibility of cyclic dependencies; that is, the concurrent executions of Process-Edge is free from deadlock and livelock.

To show that starvation does not occur, suppose an edge appears in every effective iteration $t \in [ a , b ] ,$ , that is, $e _ { \ell } = ( u _ { \ell } , v _ { \ell } ) \in \bigcap _ { t \in [ a , b ] } E ^ { ( t ) }$ . We show that $\begin{array} { r } { \dot { b } - a = \widetilde { \mathcal { O } } \left( n / \dot { \varepsilon } \right) } \end{array}$ , which bounds the number of supersteps that processor $\ell$ may spend attempting to acquire locks for $e _ { \ell }$ .

Step 2 requires one superstep per iteration, while all other steps collectively require at most one. For each $t \in ( a , b ] ,$ , the component of $\bar { G } ^ { ( t - 1 ) }$ containing $\boldsymbol { e } _ { \ell }$ has at least one vertex with no outgoing edge. This guarantees that at least one edge in that component acquires its locks and completes Step 3 during the $\bar { ( } t - 1 )$ th effective iteration. Since Step 3 can increment the values in $\{ \alpha _ { u } \} _ { u \in V }$ for at most ${ \mathcal { O } } \left( n \log _ { 1 + \varepsilon } { \check { W } } \right) =$ $\widetilde { \mathcal { O } } \left( n / \varepsilon \right)$ edges over the entire execution, the number of iterations for which $\boldsymbol { e } _ { \ell }$ may remain blocked is also beounded by $\widetilde { \mathcal { O } } \left( n / \varepsilon \right)$ . □

To analyze Step 5 of Algorithm PS-MWM, we adopt the same simplification: processors are assumed to operate in a quasi-synchronous manner. Accordingly, we define $\mathcal { U } ^ { ( { t } ) }$ as the set of edges present in the stacks $\cup \ S ^ { \ell }$ at the beginning of iteration $t$ of Step 2 in Process-Stack. The following definition is useful for $\ell \in [ k ]$ characterizing tight edges via an equivalent notion.

Definition 3.6 (Follower). An edge $e _ { j } \in \mathcal { U } ^ { ( t ) }$ is a follower of an edge $e _ { i } \in \mathcal { U } ^ { ( t ) }$ if $e _ { i } \cap e _ { j } \neq \emptyset$ and $e _ { j }$ is added to some stack $S ^ { j }$ after $e _ { i }$ is added to some stack $S ^ { i }$ . We denote the set of followers of an edge e by $\mathcal { F } ( e )$ .

The proofs of the following four lemmas are included in Appendix B.2. The fourth lemma establishes that the post-processing phase of PS-MWM is free from deadlock, livelock, and starvation.

Lemma 3.7. An edge e is a tight edge if and only if $\mathcal { F } ( e ) = \emptyset$ .

Lemma 3.8. Let ${ \mathcal T } ^ { ( t ) }$ be the set of top edges in the stacks at the beginning of iteration t of Step 2 of Process-Stack.   
Then ${ \mathcal T } ^ { ( t ) }$ contains at least one tight edge.

Lemma 3.9. The set of tight edges in $\mathcal { U } ^ { ( t ) }$ is vertex-disjoint.

Lemma 3.10. The concurrent executions of the subroutine Process-Stack is free from deadlock, livelock, and starvation.

We now analyze the performance metrics of the algorithm.

Lemma 3.11. For any constant $\varepsilon > 0 .$ , the space complexity and per-edge processing time of Algorithm PS-MWM are $\mathcal { O } \left( k + n \log n \right)$ and ${ \mathcal { O } } \left( n \log n \right)$ , respectively. Furthermore, for $L _ { m i n } = \Omega \left( n \right)$ , the amortized per-edge processing time of the algorithm is ${ \mathcal { O } } \left( \log n \right)$ .

Proof. The claimed space bound follows from three components: ${ \mathcal O } \left( n \right)$ space for the variables and locks, ${ \mathcal { O } } \left( n \log n \right)$ space for the stacked edges, and $\mathcal { O }$ (1) space per processor.

The worst-case per-edge processing time follows from the second part of the proof of Lemma 3.5.

Processor $\ell$ processes $\lvert \dot { E ^ { \ell } } \rvert$ edges, each requiring at least one distinct effective iteration (see Definition 3.2). Additional iterations may arise when it repeatedly attempts to acquire locks in Step 2 of ProcessEdge. From the second part of the proof of Lemma 3.5, the total number of such additional iterations is bounded by ${ \mathcal { O } } \left( n \log n \right)$ . This implies that to process $\vert E ^ { \ell } \vert$ edges, a processor $\ell$ uses $\mathcal { O } \left( \vert E ^ { \ell } \vert + n \log n \right)$ supersteps. Therefore, the amortized per-edge processing time is

$$
\mathcal { O } \left( \frac { | E ^ { \ell } | + n \log n } { | E ^ { \ell } | } \right) = \mathcal { O } \left( \frac { n \log n } { | E ^ { \ell } | } \right) = \mathcal { O } \left( \frac { n \log n } { L _ { m i n } } \right) = \mathcal { O } \left( \log n \right) .
$$

Note that the amortized per-edge processing time is computed over the edges of an individual stream, not over the total number of edges across all streams. While both forms of amortization are meaningful for poly-streaming algorithms, our analysis is more practically relevant, as it reflects the cost incurred per edge arrival within a single stream.

Lemma 3.12. For any constant $\varepsilon > 0$ , Algorithm PS-MWM takes $\mathcal { O } \left( L _ { m a x } + n \log n \right)$ time.

Proof. The preprocessing phase (Steps 1–2) takes $\Theta \left( n / k \right)$ time.

To process $| E ^ { \ell } |$ edges, processor $\ell$ takes $\mathcal { O } \left( | E ^ { \ell } | + n \log n \right)$ supersteps (see the proof of Lemma 3.11). Since $| E ^ { \ell } | \leq L _ { m a x }$ for all $\ell \in [ k ]$ , the time required for Step 3 is $\mathcal { O } \left( L _ { m a x } + n \log n \right)$ .

At the beginning of Step 5, the total number of edges in the stacks is $\mathcal { U } ^ { ( 1 ) } = \mathcal { O } \left( n \log n \right)$ . By Lemma 3.8, iteration $t$ of Process-Stack removes at least one edge from $\mathcal { U } ^ { ( t ) }$ . Hence, the time required for Step 5 is ${ \mathcal { O } } \left( n \log n \right)$ .

The claim now follows by summing the time spent across all three phases.

Now, using the characterizations of tight edges, we extend the duality-based analysis of [21] to our algorithm. Let $\Delta _ { \alpha } ^ { e }$ denote the change in $\textstyle \sum _ { u \in V } \alpha _ { u }$ resulting from processing an edge $e ~ \in ~ E ^ { \ell }$ in Step 3 of Process-Edge. If an edge $e \in E ^ { \ell }$ is not included in a stack $S ^ { \ell }$ then $\Delta _ { \alpha } ^ { e } \ = \ 0 .$ , either because it fails the condition in Step 1 or Step 3 of Process-Edge. It follows that $\textstyle \sum _ { e \in \bigcup _ { \ell \in [ k ] } E ^ { \ell } } \Delta _ { \alpha } ^ { e } = \sum _ { u \in V } \alpha _ { u }$ . For an edge $e$ that is included in some stack $S ^ { i }$ , let $\mathcal { P } ( e )$ denote the set of edges that share an endpoint with $e$ and are included in some stack $S ^ { j }$ no later than $e$ (including $e$ itself). The following two results are immediate from Observation 3.2 and Lemma 3.4 of [21].

Proposition 3.13. Any edge e added to some stack $S ^ { \ell }$ satisfies the inequality

$$
w _ { e } \ge \sum _ { e ^ { \prime } \in \mathcal { P } ( e ) } g _ { e ^ { \prime } } = \frac { 1 } { 2 } \left( \sum _ { e ^ { \prime } \in \mathcal { P } ( e ) } \Delta _ { \alpha } ^ { e ^ { \prime } } \right) .
$$

Proposition 3.14. After all processors complete Step 3 of Algorithm PS-MWM, the variables $\{ \alpha _ { u } \} _ { u \in V }$ , scaled by a factor of $( 1 + \varepsilon ) .$ , form a feasible solution to the dual $L P$ in Figure 2.

Lemma 3.15. Let $\mathcal { M } ^ { * }$ be a maximum weight matching in $G$ . The matching $\mathcal { M } : = \bigcup _ { \ell \in [ k ] } \mathcal { M } ^ { \ell }$ returned by Algorithm PS-MWM satisfies $\begin{array} { r } { w ( \mathcal { M } ) \geq \frac { 1 } { 2 ( 1 + \varepsilon ) } w ( \mathcal { M } ^ { * } ) } \end{array}$ .

Proof. We only process tight edges in Process-Stack. By Lemma 3.9 tight edges are vertex disjoint, and hence their independent processing does not interfere with their inclusion in $\mathcal { M }$ .

By Lemma 3.7, an edge $e$ included in $\mathcal { M }$ must satisfy $\mathcal { F } ( e ) = \emptyset$ . Consider any edge $e ^ { \prime } \in \mathcal { P } ( e ) \backslash \{ e \}$ . Since $e \in { \dot { \mathcal { F } } } ( e ^ { \prime } ) .$ , we have $\mathcal { F } ( e ^ { \prime } ) \neq \emptyset ,$ which means $e ^ { \prime }$ is not a tight edge before $e$ is processed.

Thus, when $e$ is selected for inclusion in $\mathcal { M } ,$ , none of the edges in $\mathcal { P } ( e ) \bar { \backslash } \{ e \}$ is tight. Hence, all edges of $\mathcal { P } ( e )$ are in the stacks when we are about to process $e$ . Therefore, the total gain contributed by edges in $\mathcal { P } ( e )$ can be attributed to the weight of $e _ { , }$ , and by Proposition 3.13, we have

$$
\begin{array} { l } { \displaystyle { w ( { \mathcal M } ) = \sum _ { e \in { \mathcal M } } w _ { e } \geq \frac { 1 } { 2 } \left( \sum _ { e \in { \mathcal M } } \sum _ { e ^ { \prime } \in { \mathcal P } ( e ) } \Delta _ { \alpha } ^ { e ^ { \prime } } \right) \geq \frac { 1 } { 2 } \left( \sum _ { e \in \bigcup _ { \ell \in [ k ] } S ^ { \ell } } \Delta _ { \alpha } ^ { e } \right) } } \\ { \displaystyle { \quad = \frac { 1 } { 2 } \left( \sum _ { e \in \bigcup _ { \ell \in [ k ] } E ^ { \ell } } \Delta _ { \alpha } ^ { e } \right) = \frac { 1 } { 2 } \left( \sum _ { u \in V } \alpha _ { u } \right) . } } \end{array}
$$

Let $\{ x _ { e } ^ { * } \} _ { e \in E }$ be an optimal solution to the primal LP in Figure 2. By Proposition 3.14 and the LP duality we have

$$
w ( \mathcal { M } ^ { * } ) \leq \sum _ { e \in E } w _ { e } x _ { e } ^ { * } \leq ( 1 + \varepsilon ) \left( \sum _ { u \in V } \alpha _ { u } \right) \leq 2 ( 1 + \varepsilon ) w ( \mathcal { M } ) .
$$

It is straightforward to see that both strategies use only one pass over the streams (Step 4 of PS-MWM and Step 5 of PS-MWM-DS). Theorem 1.1 now follows by combining the results in Lemma 3.11, Lemma 3.12, Lemma 3.15, Lemma B.5, and the analysis of the deferrable strategy sketched in Appendix B.3.

# 4 Algorithms for Non-Uniform Memory Access Costs

In this section, we extend the algorithm from Section 3 to account for the non-uniform memory access (NUMA) costs present in real-world machines.

In a poly-streaming algorithm, each processor may receive an arbitrary subset of the input, making it difficult to maintain memory access locality. Modern shared-memory machines, as illustrated in Figure 1, have non-uniform memory access costs and far fewer memory controllers than processors [24]. As a result, memory systems with such limitations would struggle to handle the high volume of concurrent, random memory access requests generated by poly-streaming algorithms, leading to significant delays.

# PS-MWM-LD(V, ℓ, j, ε)

1. In parallel initialize $l o c k _ { u } ,$ and set $\alpha _ { u }$ and mar $\cdot k _ { u }$ to 0 for all $u \in V$ /\* processor $\ell$ initializes or sets $\Theta \left( n / k \right)$ locks/variables $^ { * } /$   
2. In parallel initialize $l o c k _ { u } ^ { j } ,$ and set $\boldsymbol { \alpha } _ { u } ^ { j }$ to 0 for all $u \in V$ /\* processor $\ell$ initializes or sets $\Theta \left( n / \left( k / r \right) \right)$ ) locks / variables \*/   
3. In parallel initialize $g l o c k ^ { j } \ / { ^ * }$ one processor initializes for group $j ^ { * } /$   
4. $S ^ { \ell } \gets \emptyset / { * }$ initialize an empty stack \*/   
5. for each edge $\boldsymbol { e } = \{ u , v \}$ in $\ell$ th stream do (a) $\mathrm { P r o c e s s \mathrm { - } E d g e \mathrm { - } L D } ( e , S ^ { \ell } , \varepsilon )$   
6. wait for all processors to complete execution of Step $4 ~ / *$ a barrier \*/   
7. ℓ Process-Stack $( S ^ { \ell } )$   
8. return $\mathbf { \mathcal { M } } ^ { \ell }$

We now describe a generalization of the algorithm from Section 3 that localizes a significant portion of each processor’s memory access to its near memory. This generalization applies to both edge-processing strategies introduced in Section 3.2. We focus on the non-deferrable strategy. (The deferrable strategy generalizes in the same way, following the same relationship between the two strategies as in the specialized case.)

The runtime of Process-Edge is dominated by accesses to the dual variables $\{ \alpha _ { u } \} _ { u \in V }$ . By assigning a dedicated stack to each processor, we have substantially localized accesses associated with edges in that stack. However, since a large fraction of edges is typically not included in the stacks, the runtime remains dominated by accesses to dual variables associated with these discarded edges. We therefore describe an algorithm that localizes these accesses to memory near the processor.

To localize accesses to the dual variables $\{ \alpha _ { u } \} _ { u \in V } ,$ we observe that these variables increase monotonically during the streaming phase. This observation motivates a design in which a subset of processors maintains local copies of the variables and can discard a substantial number of edges without synchronizing with the global copy. When a processor includes an edge in its stack, it increments the corresponding dual variables in the global copy by the gain of the edge and synchronizes its local copy accordingly. As a result, some local copies may lag behind the global copy, but they can be synchronized when needed.

A general scheme for allocating dual variables is as follows. The set of $k$ processors is partitioned into $r$ groups. For simplicity, we assume that $k$ is a multiple of $\boldsymbol { r } ,$ so each group contains exactly $k / r$ processors. For $r > 1 _ { { \cdot } }$ , in addition to a global copy of dual variables, we maintain $r$ local copies $\{ \alpha _ { u } ^ { j } \} _ { u \in V } ,$ one for each $j \in [ r ]$ . Group $j$ consists of the processors $\{ \ell \in [ k ] \mid \lfloor \ell / ( k / r ) \rfloor = j \} ,$ and uses $\{ \alpha _ { u } ^ { j } \} _ { u \in V }$ as its local copy of

# Process-Edge-LD(e = {u, v}, Sℓ, ε)

/\* Assumes access to $\left\{ \alpha _ { x } \right\} _ { x \in V } , \left\{ l o c k _ { x } \right\} _ { x \in V } , \big \{ \alpha _ { x } ^ { j } \big \} _ { x \in V } , \big \{ l o c k _ { x } ^ { j } \big \} _ { x \in V } , \mathrm { a n d } g l o c k ^ { j } * _ { x } \big \} _ { x \in V } ,$ / 1. if $w _ { e } \leq ( 1 + \varepsilon ) \big ( \alpha _ { u } ^ { j } + \alpha _ { v } ^ { j } \big )$ then return 2. repeatedly try to acquire $l o c k _ { u } ^ { j }$ and $l o c k _ { v } ^ { j }$ in lexicographic order of $u$ and $v$ as long as $w _ { e } > ( 1 + \varepsilon ) ( \alpha _ { u } ^ { j } + \alpha _ { v } ^ { j } )$ 3. if $w _ { e } \leq ( 1 + \varepsilon ) \big ( \alpha _ { u } ^ { j } + \alpha _ { v } ^ { j } \big )$ then release $l o c k _ { u } ^ { j }$ and $l o c k _ { v } ^ { j } ,$ and return 4. repeatedly try to acquire $g l o c k ^ { j }$ $5 . \mathrm { P r o c e s s - E d g e } ( e , S ^ { \ell } , \varepsilon )$ 6. $\boldsymbol { \alpha } _ { u } ^ { j } \gets \boldsymbol { \alpha } _ { u }$ and $\alpha _ { v } ^ { j }  \alpha _ { v } \ / ^ { * }$ synchronization of local and global dual variables \*/ 7. release $l o c k _ { u } ^ { j } , l o c k _ { v } ^ { j } , g l o c k ^ { j }$ and return

the dual variables. Algorithm PS-MWM corresponds to the special case $r = 1$ , where all processors operate using only the global copy of the dual variables.

Algorithm PS-MWM-LD, along with its subroutine Process-Edge-LD, incorporates local dual variables in addition to the global ones. In Step 2, processors in each group $j \in [ r ]$ collectively initialize their group’s local copies of dual variables and locks, followed by initializing a group lock in Step 3. All other steps of the algorithm are identical to those in PS-MWM.

In the subroutine Process-Edge-LD, Step 5 implements the non-deferrable strategy. Steps 1–3 and Step 6 enforce the localization of access to dual variables. Steps 2–3 ensure that, at any given time, each global dual variable is accessed by at most one processor per group; we refer to this processor as the delegate of the group for that variable. Thus, a processor executing Steps 4–6 serves as a delegate of its group for the corresponding dual variables during that time. In Step 6, after completing updates to the global variables, the delegate synchronizes its group’s local copy in $\mathcal { O } \left( 1 \right)$ time. As a result, the waiting time on a local variable in Step 2 is bounded by the total time spent by the corresponding delegates, up to constant factors.

The delegates in each group handle vertex-disjoint edges, so concurrent executions of Step 6 would have been safe. However, the lock in Step 4 ensures that at most one delegate per group executes Step 5 of Process-Edge. Regardless of these design choices, the behavior of delegates executing Step 5 concurrently mirrors that of processors competing for exclusive access to global dual variables in PS-MWM.

The following lemma highlights the benefit of using Algorithm PS-MWM-LD; a proof is included in Appendix B.5.

Lemma 4.1. For any constant $\varepsilon > 0$ , in the streaming phase of Algorithm PS-MWM-LD, processors in all r groups collectively access global variables a total of $\mathcal { O } \left( r \cdot n \log n \right)$ times.

In contrast to the bound in Lemma 4.1, the streaming phase of Algorithm PS-MWM accesses global variables $\Omega \left( m \right)$ times or up to $\mathcal { O } \left( m + k \cdot n \log n \right)$ times.

Algorithm PS-MWM-LD, together with the generalization of the deferrable strategy, leads to the following result (a proof is included in Appendix B.5).

Theorem 4.2. Let $k$ processors be partitioned into r groups, each with its own shared local memory. For any constant $\varepsilon > 0$ , there exists a single-pass poly-streaming algorithm for the maximum weight matching   
problem that achieves a $( 2 + \varepsilon )$ -approximation. It admits a CREW PRAM implementation with runtime $\widetilde { \mathcal { O } } \left( L _ { m a x } + \bar { n } \right)$ . If $L _ { m i n } = \Omega \left( n \right)$ , the algorithm achieves ${ \mathcal { O } } \left( \log n \right)$ amortized per-edge processing time using $\widetilde { \mathcal { O } } \left( k + r \cdot n \right)$ space.   
For arbitrarily balanced streams, it uses either:

• $\widetilde { \mathcal { O } } \left( k + r \cdot n \right)$ space and $\widetilde { \mathcal { O } } \left( n \right)$ per-edge processing time, or • $\widetilde { \mathcal { O } } \left( k \cdot n \right)$ space and $\mathcal { O }$ (1) per-edge processing time.

The processors collectively access the global memory $\widetilde { \mathcal { O } } \left( \boldsymbol { r } \cdot \boldsymbol { n } \right)$ times.

# 5 Empirical Evaluation

This section summarizes our evaluation results for Algorithm PS-MWM-LD. Detailed datasets, experimental setup, and additional comparisons (including with PS-MWM) are included in Appendix C. Our code will be made available at https://github.com/ahammed-ullah/algodyssey.

# 5.1 Datasets

Table 1: Summary of datasets. Each collection contains eight graphs (details are included in Appendix C.1).   

<html><body><table><tr><td>Graph Collection</td><td># of Edges (in billions)</td></tr><tr><td>The SSW graphs</td><td>1.36 - 127.4</td></tr><tr><td>The BA graphs</td><td>4.64 - 550.1</td></tr><tr><td>The ER graphs</td><td>256- 4096</td></tr><tr><td>The UA-du graphs</td><td>275.2- 550.1</td></tr><tr><td>The UA graphs</td><td>8.93-1100</td></tr><tr><td>The ER-du graphs</td><td>32 - 4096</td></tr></table></body></html>

Table 1 summarizes our datasets. Each collection consists of eight graphs, with edge counts ranging from one billion to four trillion. To the best of our knowledge, these represent some of the largest graphs for which matchings have been reported in the literature. Exact and approximate offline MWM algorithms (see [41]) would exceed available memory on the larger graphs. The first class (SSW) consists of six of the largest graphs from the SuiteSparse Matrix collection [12] and two from the Web Data Commons [38], which includes the largest publicly available graph dataset. Other classes include synthetic graphs generated from the Barabási–Albert (BA), Uniform Attachment (UA), and Erd˝os–Rényi (ER) models [1, 15, 40].

# 5.2 Experimental Setup

We ran all experiments on a community cluster called Negishi [36], where each node has an AMD Milan processor with 128 cores running at $2 . 2 \mathrm { G H z } ,$ , 256–1024 GB of memory, and the Rocky Linux 8 operating system version 8.8. The cores are organized in a hierarchy: groups of eight cores constitute a core complex that share an L3 cache. Eight core complexes form a socket, and they share four dual-channel memory controllers; two sockets constitute a Milan node [24]. Memory access within a socket is approximately three times faster than across sockets.

We implemented the algorithms in $\mathsf { C } { + } { + }$ and compiled the code using the gcc compiler (version 12.2.0) with the -O3 optimization flag. For shared-memory parallelism, we used the OpenMP library (version 4.5). All experiments used $\varepsilon = 1 e - 6$ . Reported values are the average over five runs. Appendix C.2 contains additional details of the experimental setup, including the generation of edge streams.

# 5.3 Space

Figure 8 summarizes the space usage of our algorithm. For $k = 1$ , the algorithm of Paz and Schwartzman [39], we store one copy of the dual variables, stack, and matching. For $k > 1$ , our algorithm stores $r + 1$ copies of the dual variables (global and local), stacks, matching, and locks. We choose the values of $r$ based on the system architecture and the number of streams (see Appendix C.3 for details).

![](images/fe79f3417f45be074526c9ca75817de7bb3d96cd749a84c97cf501ba8b36e047.jpg)  
Figure 8: Memory used by the algorithm and the corresponding graph size (space needed to store the entire graph in CSR format). Note that the $y .$ -axes are in a logarithmic scale.

The maximum space used by our algorithm is 223 GB, for the web graph WDC_2012. In comparison, storing this graph in compressed sparse row (CSR) format would require over 2800 GB. Storing the largest graph in our datasets (ER1_4096) in CSR would require more than 91, 600 GB (89.45 TB), for which our algorithm used less than $0 . 8 \mathrm { G B }$ .

# 5.4 Solution Quality

min-OPT percent. In Appendix B.7, we describe different ways to get a posteriori upper bounds on the weight of a MWM $w \left( M ^ { \ast } \right)$ , using the values of the dual variables. Let $Y _ { m i n }$ denote the minimum value of these upper bounds. If $\mathcal { M }$ is a matching in the graph returned by any algorithm, then we have $\frac { w ( \mathcal { M } ) } { w ( \mathcal { M } ^ { * } ) } \geq$ $\frac { w ( \mathcal { M } ) } { Y _ { m i n } }$ . Hence, $\frac { w ( \mathcal { M } ) } { Y _ { m i n } } \times 1 0 0 \$ gives a lower bound on the percentage of the maximum weight $w \left( \mathcal { M } ^ { \ast } \right)$ obtained by $\mathcal { M }$ . We use min-OPT percent to denote the fraction $\frac { w ( \mathcal { M } ) } { Y _ { m i n } } \times 1 0 0 \$ .

Figure 9 shows min-OPT percent obtained by different algorithms. In Appendix B.7, we describe four dual update rules as alternatives to the default rule used in Steps 3(a)–(b) of Process-Edge. The values under $k = 1$ and $k = 1 2 8$ use the default rule, and the values under ALG- $\cdot d$ use the best result among the four new dual update rules. For perspective, we include min-OPT percent obtained by the sequential 6-approximate streaming algorithm of Feigenbaum et al. [16], denoted ALG-s.

The results under $k = 1$ and $k = 1 2 8$ show that, in terms of solution quality, our poly-streaming algorithm is on par with the single-stream algorithm of [39]. The values under ALG-d indicate further potential improvements using alternative dual update rules. The comparison with ALG-s supports our choice of the algorithm from [39] over other simple algorithms, such as that of [16]. Appendix C.4 contains comparisons with an offline algorithm and details on the dual update rules.

![](images/dcb16c6cdc3950db8ea268061259bef4bde414f855b5eb213db74cc4b2eedaa3.jpg)  
Figure 9: Comparisons of min-OPT percent obtained by different algorithms. ALG-d denotes the best results from four dual update rules described in Appendix B.7, and ALG-s denotes the algorithm of Feigenbaum et al. [16].

# 5.5 Runtime

We report runtime-based speedups, computed as the total time across all three phases of PS-MWM-LD (preprocessing, streaming, and post-processing). Figure 10 shows these speedups. For $k = 1 2 8 .$ , we have speedups of 16–60, 37–73, and 68–83 for the SSW graphs, the BA graphs, and the ER graphs, respectively.

Due to the significant memory bottlenecks (discussed in Section 4), we also report speedups w.r.t. effective iterations (Definition 3.2), which are less affected by such bottlenecks. The speedup w.r.t. effective iterations is the ratio of the metric for one stream to that for $k$ streams. Now for $k = 1 2 8$ , we obtain speedups of 112–127, 121–127, and 124–128 for the SSW graphs, the BA graphs, and the ER graphs, respectively. These results indicate that shared variable access incurs no noticeable contention among processors. As a result, we expect even better runtime improvements on systems with more memory controllers or better support for remote memory access.

Figure 11 shows the runtimes for different graphs, decomposed into three phases, for $k = 1$ and $k = 1 2 8$ . The plots report the absolute time savings achieved by processing multiple streams concurrently. For $k = 1$ and $k = 1 2 8 .$ , the geometric means of the runtimes for these graphs are over 2350 seconds and under 45 seconds, respectively. For the largest graph (ER1_4096), single-stream processing took over 8000 seconds, whereas poly-stream processing reduced the time to under 100 seconds.

![](images/599438c7d982a489825af1142af013a3f8316ab0016d5205ae9ac04429505947.jpg)  
Figure 10: Speedup in runtime vs. $k$ . Note that both axes are on a logarithmic scale.

![](images/34419a8b5aab06eeb999112f24c9a8e7518de21020940f1da36832820c656a4e.jpg)  
Figure 11: Breakdown of runtime into three phases for $k = 1$ and $k = 1 2 8$ . Note that the $y$ -axis is in a logarithmic scale.

# 6 Conclusion

While numerous studies have focused on optimizing either time (in parallel computing) or space (in streaming algorithms) in isolation, the poly-streaming model offers a practically relevant paradigm for jointly optimizing both. It fills a gap by providing a formal framework for analyzing algorithmic design choices and their associated time–space trade-offs. Our study of matchings illustrates the practical relevance of this paradigm in supporting diverse design choices and enabling principled analysis of their trade-offs.

The simplicity of our matching algorithm and its generalization reflects our choice to adopt the design of [39]. We believe this principle will inspire the development of other poly-streaming algorithms. To this end, we note that [39] has also motivated simple algorithms for related problems, such as matchings with submodular objectives [33], $b$ -matchings [26], and collections of disjoint matchings [18].

Our study focuses on computing matchings in single-pass, shared-memory settings. The same framework may also be effective in multi-pass and distributed-memory settings. These directions are discussed in Appendix B.6 and Appendix B.7.