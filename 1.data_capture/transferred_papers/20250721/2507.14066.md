# Preference-based Multi-Objective Reinforcement Learning

Ni Mu∗, Yao Luan∗, Qing-Shan Jia†

Abstract—Multi-objective reinforcement learning (MORL) is a structured approach for optimizing tasks with multiple objectives. However, it often relies on pre-defined reward functions, which can be hard to design for balancing conflicting goals and may lead to oversimplification. Preferences can serve as more flexible and intuitive decision-making guidance, eliminating the need for complicated reward design. This paper introduces preferencebased MORL (Pb-MORL), which formalizes the integration of preferences into the MORL framework. We theoretically prove that preferences can derive policies across the entire Pareto frontier. To guide policy optimization using preferences, our method constructs a multi-objective reward model that aligns with the given preferences. We further provide theoretical proof to show that optimizing this reward model is equivalent to training the Pareto optimal policy. Extensive experiments in benchmark multi-objective tasks, a multi-energy management task, and an autonomous driving task on a multi-line highway show that our method performs competitively, surpassing the oracle method, which uses the ground truth reward function. This highlights its potential for practical applications in complex real-world systems.

Note to Practitioners—Decision-making problems with multiple conflicting objectives are common in real-world applications, e.g., energy management must balance system lifespan, chargedischarge cycles, and energy procurement costs; autonomous driving vehicles must balance safety, speed, and passenger comfort. While multi-objective reinforcement learning (MORL) is an effective framework for these problems, its dependence on pre-defined reward functions can limit its application in complex situations, as designing a reward function often fails to capture the full complexity of the task fully. This paper introduces preference-based MORL (Pb-MORL), which utilizes user preference data to optimize policies, thereby eliminating the complexity of reward design. Specifically, we construct a multiobjective reward model that aligns with user preferences and demonstrate that optimizing this model can derive Pareto optimal solutions. Pb-MORL is effective, easy to deploy, and is expected to be applied in complex systems, e.g., multi-energy management through preference feedback and adaptive autonomous driving policies for diverse situations.

Index Terms—Reinforcement learning, Multi-objective optimization, Preference-based optimization, Pareto efficiency.

# I. INTRODUCTION

Multi-objective optimization is pervasive in real-world applications [1]–[3]. For example, in an energy system, the goal

N. Mu, Y. Luan and Q. Jia are with the Center for Intelligent and Networked System (CFINS), Department of Automation, Beijing National Research Center for Information Science and Technology, Beijing Key Laboratory of Embodied Intelligence Systems, Tsinghua University, Beijing 100084, China, $\{ \mathfrak { m } \mathfrak { n } 2 3 \ @ \mathfrak { m } \mathfrak { a } \mathrm { i } \breve { \mathrm { 1 s } }$ ., luany23@mails., jiaqs@} tsinghua.edu.cn. $^ { * } \mathrm { { N } }$ . Mu and Y. Luan contributed equally. $^ \dag Q$ . Jia is the corresponding author.

is to maximize the system lifespan and minimize chargedischarge cycles while simultaneously reducing energy procurement costs [4]. Autonomous vehicles need to provide safe, fast, and comfortable rides at the same time [5]. However, representing these objectives with a single reward can be difficult and may lose important information [6]–[8]. In addition, creating a scalar reward function for each control objective is challenging and often results in oversimplification [9], [10]. Preferences, conversely, offer a more flexible and general way to model the decision-making process [11]. Humans can easily provide their preferences, pointing out which outcome they prefer, without compressing all their decision-making information into a single reward function [12]. Therefore, it is of great practical interest to study multi-objective reinforcement learning based on preference.

However, integrating multi-objective reinforcement learning (MORL) with preference-based learning presents several challenges. First, while users can express preferences between pairs of behaviors when focusing on a single objective, establishing a complete ordering among all behaviors is often difficult. This lack of a complete preference makes it hard for algorithms to assess the relative importance of different objectives. Additionally, there are often inherent conflicts between objectives, which complicate policy optimization. Furthermore, obtaining preferences for all objectives may require pairwise comparisons, which can be computationally inefficient as the number of objectives increases, leading to increased complexity in the querying process. Given these complexities, a significant gap lies in the previous work: To the best knowledge of the authors, we did not find any method addressing the above challenges of combining MORL and preference-based optimization, highlighting the need for a novel approach to multi-objective, preference-driven decisionmaking problems.

# A. Related Work

Single-objective reinforcement learning with preference. Reinforcement learning (RL) [13] has gained significant attention in recent years due to its remarkable success in solving challenging problems [14]–[19]. Traditional RL algorithms often rely on a pre-defined reward function, which serves as the guidance for policy optimization. However, designing such a reward function can be complex and even impractical [9]. However, there are two typical categories of RL problems where we face difficulty obtaining the optimal policy. The first category of RL problems has a pre-defined reward function, such as cost savings [20], reducing carbon emissions [21], or sparse rewards like a “ $+ 1 ^ { \dag }$ bonus for reaching the goal in a maze [22]. The challenge in this category is identifying the optimal policy, as task dynamics are often complex and stochastic, while sparse reward signals complicate policy learning from the reward function [23]. The second category consists of RL problems where defining the reward function is challenging, such as in robotics systems [9] and large language models [24]. In these cases, while we want the system behaviors to align with human expectations, formalizing the objective function is often difficult [9]. Preference-based reinforcement learning (PbRL) provides a solution by utilizing user feedback to guide agent behavior, making it suitable for both categories of RL problems. This approach offers preferences that may be more accessible and more naturally aligned with policy optimization than traditional reward signals. Early works of PbRL, such as [11] and [12], have shown the ability of agents to learn from simple comparisons between pairs of trajectory segments, thereby eliminating the necessity for complex reward engineering. With the development of deep learning, techniques like pre-training [6], [25], [26] and data augmentation [27] are employed to improve learning efficiency. Meta-learning approaches [28] also enable agents to adapt to new tasks based on past experiences quickly. Moreover, PbRL has been successfully applied to fine-tune large-scale language models like GPT-3 for challenging tasks, as highlighted by [29]. While PbRL omits reward engineering through leveraging user feedback, it primarily deals with single-objective optimization instead of multi-objective preference modeling.

![](images/fbcb0b45a74ec8b7d808326eb31f5aa55bf6f6aee6826040f61945bba89f5643.jpg)  
Fig. 1. A demonstration of the proposed $\mathrm { P b }$ -MORL framework. An explicit multi-objective reward model $\hat { \pmb { r } } _ { \psi }$ is learned using preference data. Then, the multi-objective policy $\pi _ { \phi } ( a | s , \pmb { w } )$ can be updated through any MORL algorithm based on the reward model. In this figure, $\textbf { \em w }$ denotes the weight vector, $( \sigma _ { 0 } , \sigma _ { 1 } )$ denotes the segment pair, $p$ denotes the preference provided by the teacher. A detailed introduction to the settings and notations will be provided in the following sections.

Multi-objective reinforcement learning with explicit reward functions. Multi-objective reinforcement learning (MORL) is a pivotal subfield of reinforcement learning [30]– [32], focusing on decision-making problems under multiple objectives. Envelope Multi-objective Q-learning [33] extends the traditional Q-learning algorithm to the multi-objective domain and proves the convergence of its Q-learning algorithm in tabular settings. Expected Utility Policy Gradient (EUPG) [34] and Prediction-Guided MORL (PGMORL) [35] further integrate deep learning into MORL. EUPG incorporates policy gradients to balance current and prospective returns, while PGMORL applies an evolutionary strategy to enhance the Pareto frontier. Additionally, Pareto Conditioned Networks [36] and Generalized Policy Improvement Linear Support [37] employ neural networks conditioned on target returns to predict optimal actions within deterministic settings. Despite their advancements, current MORL methods rely on predefined multi-objective reward functions, posing challenges for their application in real-world control scenarios. Extending preferences from single-objective reinforcement learning to multi-objective contexts is feasible, which is the main contribution of this paper.

# B. Main Contributions

In this paper, we introduce Preference-based MultiObjective Reinforcement Learning (Pb-MORL), which integrates preference modeling into Multi-Objective Reinforcement Learning (MORL), as illustrated in Fig. 1. Specifically, we first establish theorems that demonstrate a teacher providing preferences can guide the learning of optimal multiobjective policies (Theorem 1, 2). Furthermore, we propose a method to construct an explicit multi-objective reward model that aligns with the teacher’s preferences. Our theoretical proof (Theorem 4) shows that, in a multi-objective context, if the reward function perfectly matches the teacher’s preferences, optimizing this reward is equivalent to learning the optimal policy. To implement Pb-MORL, we combine the Envelope Q Learning (EQL) method [33] with our proposed reward model. This implementation is simple yet effective, for EQL guarantees the convergence of policy optimization in multi-objective tasks. To demonstrate the effectiveness of our method, we conduct experiments in benchmark multiobjective reinforcement learning tasks. The results show that our approach achieves performance levels comparable to the oracle method, which uses the ground truth reward function to learn the optimal policy. To validate our method’s applicability in real-world scenarios, we evaluate our method on both a multi-energy management task and an autonomous driving task on a multi-line highway. In both settings, the Pb-MORL algorithm outperforms the oracle, showing its potential for practical implementation in complex, real-world environments. Through this work, we aim to broaden the applications of MORL in real-world settings, by employing preferences as a more accessible and intuitive optimization guidance.

TABLE I NOTATION TABLE   

<html><body><table><tr><td>Symbol</td><td>Definition</td><td>Description</td></tr><tr><td colspan="3">Multi-Objective RL Elements</td></tr><tr><td>m</td><td>Number of objectives</td><td>Dimension of the reward vector.</td></tr><tr><td>r(s,a)</td><td>True multi-objective reward vector</td><td>r(s,a) E Rm:Ground truth reward signal provided by the environment for each objective.</td></tr><tr><td>w</td><td>Weight vector</td><td>w E W= {w ∈ Rm|wi ≥O,∑i wi =1}: Vector encoding the relative importance (preference) assigned to each objective.</td></tr><tr><td>W</td><td>Weight space</td><td>Set of all valid weight vectors.</td></tr><tr><td>Dw</td><td>Prior weight distribution</td><td>Distribution over the weight space W from which weights are sampled during training or evaluation.In this study,we assume this distribution to be uniform over W.</td></tr><tr><td>Ⅱ</td><td>Policy space</td><td>Set of all possible policies.</td></tr><tr><td>II*</td><td>Pareto optimal policy set</td><td>Set of policies that are not dominated by any other policy in II with respect to all objectives.</td></tr><tr><td colspan="3">Preference Elements</td></tr><tr><td></td><td>Trajectory segment</td><td>Finite sequence of state-action pairs: σ = {sk,ak,.,Sk+H-1,αk+H-1} of length H.</td></tr><tr><td>H</td><td>Segment length</td><td>Number of steps in a trajectory segment o.</td></tr><tr><td>p</td><td>Preference label</td><td>p ∈{0,0.5,1}:Human teacher's preference judgment for a pair of segments (oo,O1） under weight w. p = O: oo preferred,p = 1: σ1 preferred,p= O.5: no preference/indifferent.</td></tr><tr><td>00Ywσ1</td><td>Preference relation</td><td>Segment oo is strictly preferred over segment o1 under weight w.</td></tr><tr><td colspan="3">Learned Components</td></tr><tr><td>r(s,a)</td><td>Learned multi-objective reward model</td><td>r(s,a) ∈ Rm: Model parameterized by , trained using preference data to approximate the underlying objectives. Used as the reward signal for MORL policy optimization.</td></tr><tr><td>π(alls,w)</td><td>Parameterized policy</td><td>Stochastic policy parameterized by ,conditioned on the current state s and the weight vector w (indicating the desired objective trade-off).Outputs a distribution over actions.</td></tr><tr><td>Qe(s,a,w)</td><td>Multi-objective Q-function</td><td>Qe(s,a,w) E Rm: State-action value function parameterized by 0. Estimates the vector of expected discounted future rewards for each objective,starting from state s,taking action a,</td></tr><tr><td>J</td><td>Optimization objective</td><td>and following policy π(·l!, w） thereafter. Scalarized expected retum: J = Ew~Dw,T~(P,π)[ωT∑t γt𝑟ab(st,at) (Eq. 7). Maximized during policy learning using the learned reward model.</td></tr></table></body></html>

The main contributions of this paper are as follows:

We establish theorems for preference-based optimization in multi-objective settings, demonstrating that a preference-based teacher can guide the learning of optimal multi-objective policies (Theorem 1, 2, 4). We introduce Pb-MORL, which develops an explicit multi-objective reward model that aligns with preference data through the construction of the Bradley-Terry model and the optimization of the cross-entropy loss function. In addition, we combine the EQL algorithm with the reward model to achieve a simple yet effective implementation of $\mathrm { P b }$ -MORL. We conduct experiments in multi-objective benchmark tasks, a multi-energy management task, and an autonomous driving task on a multi-line highway, showing that $\mathrm { P b }$ -MORL performs comparably to the oracle method using ground truth reward functions. It demonstrates $\mathrm { \sf P b } .$ - MORL’s potential for real-world applications.

The remaining sections are organized as follows. In Section II, we introduce preliminaries and the problem formulation. In Section III, we present the theoretical guarantees of PbMORL and propose the specific algorithm for explicit reward modeling and policy optimization. In Section IV, we describe the experimental setting and discuss the experimental results. Finally, we conclude the paper in Section V.

# II. PROBLEM FORMULATION

In this section, we first introduce the multi-objective MDP and Q-learning in multi-objective settings, then formulate the $\mathrm { P b }$ -MORL framework.

# A. MDP and $\boldsymbol { \mathcal { Q } }$ -learning in Multi-Objective Settings

For single-objective settings, an MDP with discrete time, infinite-stage discounted reward, and finite or countable state and action spaces could be characterized as a tuple $\mathcal { M } =$ $\langle S , \mathcal { A } , P , r , \gamma \rangle$ . Here, $s$ is the state space, $\mathcal { A }$ is the action space, and $P ( s ^ { \prime } | s , a ) : \mathcal { S } \times \mathcal { A } \times \mathcal { S } \to [ 0 , 1 ]$ is the one-step state transition probability of transiting from $s$ to $s ^ { \prime }$ by taking action $a$ . Besides, $r ( s , a ) : \mathcal { S } \times \mathcal { A } \to \mathbb { R }$ defines the immediate reward of taking action $a$ under state $s$ , and $\mathbb { R }$ denotes the set of real numbers. Finally, $\gamma \in \mathsf { \Gamma } ( 0 , 1 )$ is the discount factor for balancing immediate and long-term rewards.

For multi-objective settings, the MDP framework is extended to include multiple reward functions. The reward function can be represented as a vector $r ( s , a ) : S \times \mathcal { A } \to \mathbb { R } ^ { m }$ , where $m$ is the number of objectives. In the case of linear reward combination, the overall reward is defined by a linear combination of these objectives, $r _ { \pmb { w } } ( s , a ) = \pmb { w } ^ { T } r ( s , a )$ , where $\textbf { \em w } \in \mathcal { W }$ is the weight vector, and the weight space $\mathcal { W } ~ = ~ \{ \pmb { w } | \pmb { w } ~ \in ~ \mathbb { R } ^ { m } , w _ { i } ~ \geq ~ 0 , \sum w _ { i } ~ = ~ 1 \}$ . The goal in the multi-objective MDP is to find a policy $\pi ( a | s , \pmb { w } )$ : $s \times \mathcal { W } \times \mathcal { A } \to [ 0 , 1 ]$ that maximizes the inner product of the multi-dimensional discounted return and the weight vector $\textbf { \em w }$ , that is,

$$
\operatorname* { m a x } _ { \mathbf { \omega } } J = \mathbb { E } { \underset { \tau \sim ( P , \pi ( \cdot | \cdot , \pmb { w } ) ) } { \mathbb { E } } } \mathbf { \omega } ^ { T } \sum _ { \tau } \gamma ^ { t } \mathbf { \vec { r } } ( s _ { t } , a _ { t } ) ,
$$

where $\tau$ denotes the trajectory, and under $D _ { w }$ is a prior weight distribution. Denote the policy space as $\Pi$ .

Then, the Q-learning algorithm can be adapted to the multi-objective setting. The standard Q-Learning [13], [38] for single-objective RL is based on the Bellman optimality operator $\boldsymbol { B }$ :

$$
( \boldsymbol { B } \boldsymbol { Q } ) ( \boldsymbol { s } , \boldsymbol { a } ) : = r ( \boldsymbol { s } , \boldsymbol { a } ) + \operatorname* { s u p } _ { \boldsymbol { a } ^ { \prime } } \gamma \mathbb { E } _ { \boldsymbol { s } ^ { \prime } \sim \boldsymbol { P } ( \cdot | \boldsymbol { s } , \boldsymbol { a } ) } \boldsymbol { Q } ( \boldsymbol { s } ^ { \prime } , \boldsymbol { a } ^ { \prime } ) .
$$

Following the previous work [33], we extend this to the MORL setting, by considering multi-objective $\mathrm { Q }$ -value functions $\begin{array} { r } { \pmb { Q } ( s , a , \pmb { w } ) : \pmb { S } \times \pmb { \mathcal { A } } \times \pmb { \mathcal { W } }  \mathbb { R } ^ { m } } \end{array}$ , which estimates expected total vector rewards under state $s$ , action $a$ and $m$ -dimensional weight $\textbf { \em w }$ . It is important to note that the parameter $\textbf { \em w }$ in the Q function represents that the $\mathrm { ~ Q ~ }$ value is under the policy $\pi ( \cdot | \cdot , w )$ , because the policies conditioning on different weight vectors $\textbf { \em w }$ results in different behaviors, and the corresponding Q functions can vary.

We define the distance between two multi-objective Q functions $Q _ { 1 } , Q _ { 2 }$ as follows:

$$
d ( Q _ { 1 } , Q _ { 2 } ) : = \operatorname * { s u p } _ { s , a , w } \left| w ^ { T } \left( Q _ { 1 } ( s , a , w ) - Q _ { 2 } ( s , a , w ) \right) \right| .
$$

The metric $d$ forms a complete pseudo-metric space, as the identity of indiscernibles [39] does not hold.

With a little abuse of notation, we use the same $B _ { \pi }$ and $\boldsymbol { B }$ as in the single-objective RL to represent the Bellman operator in the multi-objective setting. Specifically, given a policy $\pi$ and sampled trajectories $\tau$ , the multi-objective Bellman operator for policy evaluation $B _ { \pi }$ is defined as:

$$
( \mathcal { B } _ { \pi } \pmb { Q } ) ( s , a , \pmb { w } ) : = \pmb { r } ( s , a ) + \gamma \mathbb { E } _ { \tau \sim ( P , \pi ) } \pmb { Q } ( s ^ { \prime } , a ^ { \prime } , \pmb { w } ) .
$$

To construct the multi-objective Bellman optimality operator, an optimality filter $\mathcal { H }$ for the multi-objective Q function is first defined as:

$$
( \mathcal { H } Q ) ( s , { \boldsymbol w } ) : = \arg \operatorname* { s u p } _ { a \in \mathcal { A } , { \boldsymbol w } ^ { \prime } \in \mathcal { W } } { \boldsymbol w } ^ { T } \boldsymbol { Q } ( s , a , { \boldsymbol w } ^ { \prime } ) ,
$$

where the $\arg Q$ takes the multi-objective value corresponding to the supremum (i.e., $Q ( s , a , { \pmb w } ^ { \prime } )$ ) such that $( a , { w ^ { \prime } } ) \in$ arg $\begin{array} { r } { \operatorname* { s u p } _ { a \in \mathcal { A } , \pmb { w } ^ { \prime } \in \mathcal { W } } \pmb { w } ^ { T } \pmb { Q } ( s , a , \pmb { w } ^ { \prime } ) ) } \end{array}$ . Then, the multi-objective Bellman optimality operator $\boldsymbol { B }$ is defined as:

$$
( \mathcal { B } \pmb { Q } ) ( s , a , \pmb { w } ) : = \pmb { r } ( s , a ) + \gamma \mathbb { E } _ { s ^ { \prime } \sim \pmb { P } ( \cdot | s , a ) } ( \mathcal { H } \pmb { Q } ) ( s ^ { \prime } , \pmb { w } )
$$

Algorithm 1 Using the teacher to derive convex Pareto frontier, based on traversing the weight space

1: Initialize the solution set $\Pi ^ { * } = \varnothing$   
2: for each ${ \pmb w } \in \mathcal { W } ^ { [ N _ { w } ] }$ do   
3: for each $\pi _ { i } \in \Pi$ do   
4: if $\nexists \ \pi ^ { \prime } \in \Pi , \pi ^ { \prime } \neq \pi _ { i }$ s.t. $\begin{array} { r } { \pmb { w } ^ { T } \sum _ { ( s , a ) \sim \sigma _ { i } } \gamma ^ { t } \pmb { r } ( s _ { t } , a _ { t } ) < } \end{array}$   
$\begin{array} { r } { \pmb { w } ^ { T } \sum _ { ( s , a ) \sim \sigma ^ { \prime } } \gamma ^ { t } \pmb { r } \big ( s _ { t } , a _ { t } \big ) } \end{array}$ , where $\sigma _ { i } , \boldsymbol { \sigma } ^ { \prime }$ are segments   
generated by $\pi _ { i } , \pi ^ { \prime }$ , then   
5: $\Pi ^ { * }  \Pi ^ { * } \cup \{ \pi _ { i } \}$   
6: end if   
7: end for   
8: end for   
9: return $\Pi ^ { * }$

Intuitively, the optimality Bellman operator $\boldsymbol { B }$ solves the minimum convex envelope of the current $\boldsymbol { Q }$ frontier. Previous works of MORL [33] have provided proof of the convergence of the above multi-objective Q-learning algorithm, by proving the Bellman operator $B _ { \pi }$ and $\boldsymbol { B }$ are both contrastive mappings under the metric $d$ defined in Eq. (3).

# B. Pb-MORL Formulation

For single-objective settings, by following the previous work [11], [12], we can define the preference in the form of tuple $( \sigma _ { 0 } , \sigma _ { 1 } , p )$ , where segment $\sigma _ { 0 } , \sigma _ { 1 }$ are sequences of states and actions $\left\{ s _ { k } , a _ { k } , . . . , s _ { k + H - 1 } , a _ { k + H - 1 } \right\}$ with length $H$ and arbitrary starting time $k$ , and $p ~ \in ~ \{ 0 , 0 . 5 , 1 \}$ encodes the preference relations:

$\sigma _ { 0 }$ strictly preferred to $\sigma _ { 1 }$ when $p = 0$ . $\sigma _ { 1 }$ strictly preferred to $\sigma _ { 0 }$ when $p = 1$ . • Indeterminate preference (equivalence or ambiguous judgment) when $p = 0 . 5$ .

This scheme accounts for human rating uncertainty while maintaining annotation efficiency. When $\sigma _ { 0 } = \sigma _ { 1 }$ or trajectories are equally preferable, $p = 0 . 5$ explicitly captures the uncertainty.

For multi-objective settings, we redefine the preference as a tuple $( \sigma _ { 0 } , \sigma _ { 1 } , \pmb { w } , p )$ , where $\textbf { \textit { w } } \in \textbf { \textit { w } }$ is a weight vector. The preference $p ~ \in ~ \{ 0 , 0 . 5 , 1 \}$ is a scalar which encodes preference relations under $\textbf { \em w }$ , defined similarly as in the single-objective settings. In fact, given any weight vector, we introduce a complete ordering of the trajectory segments. However, we employ a pairwise comparison method due to practical constraints and use partial ordering notation $( \succ )$ in the following paper. Specifically, let $\sigma _ { 0 } \mathbf { \Sigma } \succ _ { \pmb { w } } \ \sigma _ { 1 }$ means that trajectory segment $\sigma _ { 0 }$ is preferred over $\sigma _ { 1 }$ under the weight vector $\textbf { \em w }$ . Then, the preference $p$ can be written in the form of $p = \mathbb { I } ( \sigma _ { 0 } \succ _ { w } \sigma _ { 1 } )$ , where $\mathbb { I } ( \cdot )$ is an indicator function that returns 1 if the condition is true, and 0 otherwise. As mentioned earlier, the weight $\textbf { \em w }$ represents the importance assigned to each objective within the multi-objective framework. The weight $\textbf { \em w }$ is crucial in defining the multi-objective preference, as preferences can vary for the same trajectory pair depending on the weights.

To align the problem formulation with the RL framework, we define an explicit multi-objective reward model $\hat { \pmb r } : \pmb { \mathcal S } \times \mathcal { A }  \mathbb { R } ^ { m }$ , where each dimension corresponds to a distinct objective. This reward model can be trained using preference data, serving as a bridge between qualitative preference and quantitative rewards. Based on this model, we propose the objective of $\mathrm { P b }$ -MORL as finding a policy $\pi ( a | s , \pmb { w } )$ conditioned on the weight vector $\textbf { \em w }$ . Specifically, the goal is to maximize the inner product between the conditioned weight and the discounted return of the reward model $\hat { \pmb { r } }$ , under a prior weight distribution $D _ { w }$ , as Eq. (1) shows:

$$
\operatorname* { m a x } _ { \mathbf { \omega } } J = \mathbb { E } { \underset { \tau \sim ( P , \pi ( \cdot | \cdot , \pmb { w } ) ) } { \mathbb { E } } } \mathbf { \omega } ^ { T } \sum _ { \tau } \gamma ^ { t } \hat { \mathbf { \omega } } ( s _ { t } , a _ { t } ) .
$$

In the form of Q function, it can also be written as:

$$
\operatorname* { m a x } J = \mathbb { E } { \underset { ( s _ { 0 } , a _ { 0 } ) \sim ( P , \pi ( \cdot | \cdot , \pmb { w } ) ) } { w \sim D _ { w } } } w ^ { T } Q _ { \pi } ( s _ { 0 } , a _ { 0 } , \pmb { w } | \hat { \pmb { r } } ) .
$$

# III. A PB-MORL ALGORITHM WITH EXPLICIT REWARD MODELING

# A. Theoretical Analysis

In this subsection, we present the theoretical foundations of the $\mathrm { P b }$ -MORL framework. We demonstrate how our approach ensures convergence to Pareto-optimal policies. To ease the proof, we discretize the weight space $\boldsymbol { \mathcal { W } }$ to a finite space $\bar { \mathcal { W } } ^ { [ N _ { w } ] }$ with size $N _ { w }$ , and assume that when $N _ { w }$ is large enough, $\mathcal { W } ^ { [ N _ { w } ] }$ could fully represent $\mathcal { W }$ , then induce the same set of optimal policies. We formalize this in Assumption 4.

First, we assume the presence of preferences over pairs of trajectory segments with arbitrary finite length under an arbitrary given weight. To formalize this, we introduce the following assumption.

Assumption 1. The preference $p \in \{ 0 , 0 . 5 , 1 \}$ over a pair of trajectory segments $( \sigma _ { 0 } , \sigma _ { 1 } )$ exists, with arbitrary finite segment length $H$ , under an arbitrary given weight $\pmb { w } \in W$ . These preferences satisfy symmetry, consistency, and transitivity, which are defined as follows.

Definition 1 (Symmetry). Symmetry means that if trajectory segment $\sigma _ { 0 }$ is preferred over $\sigma _ { 1 }$ under a weight vector $\pmb { w }$ , then the opposite must also be true: $\sigma _ { 1 }$ is less preferred than $\sigma _ { 0 }$ under the same weight. Formally, this is written as:

$$
\sigma _ { 0 } \succ _ { w } \sigma _ { 1 } \implies \sigma _ { 1 } \prec _ { w } \sigma _ { 0 } .
$$

This ensures that preferences are reversible under the same weight vector.

Definition 2 (Consistency). Consistency means that if $\sigma _ { 0 } \succ _ { w }$ $\sigma _ { 1 }$ holds for a given $\textbf { \em w }$ , this preference remains unchanged over time. Formally, this is expressed as:

$$
\sigma _ { 0 } ^ { t _ { 0 } } \succ _ { w } \sigma _ { 1 } ^ { t _ { 0 } } \implies \forall t > 0 , \sigma _ { 0 } ^ { t } \succ _ { w } \sigma _ { 1 } ^ { t } ,
$$

where $\boldsymbol { \sigma } ^ { t }$ denotes a trajectory segment starting from time $t _ { : }$ , i.e. $\sigma ^ { t } = \{ s _ { t } , a _ { t } , \cdot \cdot \cdot , s _ { t + H - 1 } , a _ { t + H - 1 } \}$ . Here, $\sigma ^ { t _ { 0 } }$ and $\boldsymbol { \sigma } ^ { t }$ are segments with the same state action sequence $( s , a , s ^ { \prime } , \cdot \cdot \cdot )$ but starting from the different time.

Definition 3 (Transitivity). Transitivity means that if the teacher prefers $\sigma _ { 0 }$ over $\sigma _ { 1 }$ and $\sigma _ { 1 }$ over $\sigma _ { 2 }$ under the same weight w, then the teacher must also prefer $\sigma _ { 0 }$ over $\sigma _ { 2 }$ under weight $\textbf { \em w }$ . Formally, this is expressed as:

$$
\left( \sigma _ { 0 } \succ _ { w } \sigma _ { 1 } \right) \wedge \left( \sigma _ { 1 } \succ _ { w } \sigma _ { 2 } \right) \implies \sigma _ { 0 } \succ _ { w } \sigma _ { 2 } .
$$

This property ensures logical coherence of preferences across multiple trajectory segments. Thus, the teacher’s feedback does not contradict itself when extended to multiple comparisons.

The symmetry, consistency, and transitivity requirements in Assumption 1 align with standard preference modeling in single-objective RL [9]. Then, we assume the presence of a perfect teacher, which can provide the preference over an arbitrary pair of trajectory segments with arbitrary finite length under an arbitrarily given weight.

Assumption 2. We assume the existence of a teacher who can provide the preference feedback for two arbitrary trajectory segments $( \sigma _ { 0 } , \sigma _ { 1 } )$ , based on an arbitrary weight vector $\textbf { \em w }$ .

In Assumption 1 and 2, we assume that the teacher can provide preferences $p ~ \in ~ \{ 0 , 0 . 5 , 1 \}$ over arbitrary pairs of segments $( \sigma _ { 0 } , \sigma _ { 1 } )$ under a given weight $\textbf { \em w }$ , and that these preferences satisfy symmetry, consistency, and transitivity. The assumption of preference availability under given weights is based on existing single-objective preference learning works [9], [27]. This indicates that the teacher’s preferences are based on stable and consistent feedback related to the task objectives. Based on Assumption 1, it is reasonable to assume that the task has an underlying true reward, which is aligned with the teacher’s preferences. We formalize it in Assumption 3. This assumption helps to establish a connection between the teacher’s preferences and policy optimization.

Assumption 3. There exists a true reward function $\boldsymbol { r }$ for a certain multi-objective task, $i f$ there exists a teacher that can express preferences for this task. Furthermore, the value of the true weighted reward $\boldsymbol { w ^ { T } r }$ is bounded by a constant $r _ { m a x }$ . Formally, this is written as:

$$
\operatorname* { m a x } _ { \pmb { w } , s , a } | \pmb { w } ^ { T } \pmb { r } ( s , a ) | \leq r _ { m a x } .
$$

The above equation indicates that regardless of the chosen weight vector $\textbf { \em w }$ , the absolute value of the weighted reward will not exceed this predefined upper limit.

Assumption 3 is a common practice in existing works [40]– [42], as most real-world problems involve bounded rewards. By doing this, Assumption 3 prevents issues such as divergence in the reward function, thereby enabling Theorem 1, as discussed in the following paper.

Assumption 4. The optimal policy $\pi ^ { * } ( a | s , \pmb { w } _ { 0 } )$ under weight $\pmb { w } _ { 0 }$ is also the optimal policy under weight $\pmb { w } \in \{ \pmb { w } | \Vert \pmb { w } - \tau$ $w _ { 0 } \| _ { \infty } \leq \epsilon \}$ , $\exists \epsilon > 0 , \forall s \in \mathcal { S } , a \in \mathcal { A } , \pmb { w } _ { 0 } \in \mathcal { W } .$ .

Assumption 4 is based on the assumption that the value function is continuous with respect to the weight vector $\textbf { \em w }$ , which is reasonable and commonplace in industrial applications. With Assumption 4, we could discretize the weight space $\mathcal { W }$ into a finite space $\mathcal { W } ^ { [ N _ { w } ] }$ of size $\begin{array} { r } { N _ { w } = \frac { | \mathcal { W } | } { \epsilon ^ { m } } \le \epsilon ^ { - m } } \end{array}$ , i.e. divide the weight space $\boldsymbol { \mathcal { W } }$ to super cubes with side length $\epsilon$ . The optimal policies for weights within each super cube are identical. Therefore, $\mathcal { W } ^ { [ N _ { w } ] }$ could fully represent $\mathcal { W }$ , as they induce the same set of optimal policies.

Under Assumption 1, 2, 3 and 4, in the following theorems, we illustrate that the entire Pareto frontier could be obtained by a simple algorithm (Algorithm 1) using preferences given different weights. Specifically, we first prove that any optimal policy in an arbitrary given weight is in the Pareto frontier in Theorem 1. Then in Theorem 2, we prove that the optimal policies in all weights could form any convex Pareto frontier. Further, for non-convex Pareto frontiers, we prove the frontier could be obtained using preferences collected in designed weights in Theorem 3.

Theorem 1. Each policy in the policy set $\pi ^ { * } ( a | s , \pmb { w } ) \in \Pi ^ { * }$ derived from Algorithm $\boldsymbol { { \mathit { 1 } } }$ is in the Pareto frontier when the segment length $H \to \infty$ .

Proof. We prove this theorem by contradiction. Suppose $\pi ^ { * } ( a | s , \pmb { w } )$ is not in the Pareto frontier. Then there must exist a policy $\pi ^ { \circ } ( a | s , w ) \neq \pi ^ { * } ( a | s , w )$ which dominates $\pi ^ { * } ( a | s , \pmb { w } )$ . And then there must exist a weight $\pmb { w } _ { 0 }$ and a pair of trajectories $\tau ^ { \circ }$ and $\tau ^ { * }$ which are generated from $\pi ^ { \circ } ( a | s , \pmb { w } )$ and $\pi ^ { * } ( a | s , \pmb { w } )$ respectively, and $\tau ^ { \circ } \succ _ { w _ { 0 } } \tau ^ { * }$ . We extract segments of length $H$ from $\tau ^ { \circ }$ and $\tau ^ { * }$ , denoted as $\sigma ^ { \circ }$ and $\sigma ^ { * }$ respectively. Under Assumption 1, the teacher can always output the true preference between two segments.

Let $s _ { t } ^ { \bigtriangledown }$ and $a _ { t } ^ { \bigtriangledown }$ denote the state and action at time $t$ in the trajectory $\tau ^ { \bigtriangledown }$ , where $\boxed { \bigstar }$ is an arbitrary symbol. With discount factor $\gamma$ , the difference between the discounted total return $\begin{array} { r } { \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } \pmb { w } _ { 0 } ^ { T } \pmb { r } ( s _ { t } , a _ { t } ) } \end{array}$ and the truncated discounted total re urn $\begin{array} { r } { \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } \pmb { w } _ { 0 } ^ { T } \pmb { r } ( s _ { t } , a _ { t } ) } \end{array}$ is bounded, i.e. $\begin{array} { r l r } { \big | \sum _ { t = H } ^ { \infty } \gamma ^ { t } { \pmb w } _ { 0 } ^ { T } { \pmb r } \big ( s _ { t } , a _ { t } \big ) \big | } & { \leq } & { \frac { \gamma ^ { H } } { 1 - \gamma } r _ { \mathrm { m a x } } } \end{array}$ . Let $\begin{array} { r l } { \mathcal { R } _ { \underline { { t } } } ^ { \bar { t } } ( \sigma ^ { \circ } ) } & { { } = } \end{array}$ $\begin{array} { r } { \sum _ { t = \underline { { t } } } ^ { \bar { t } } \gamma ^ { t } \pmb { w } _ { 0 } ^ { T } \pmb { r } ( s _ { t } ^ { \circ } , a _ { t } ^ { \circ } ) } \end{array}$ , $\begin{array} { r } { \mathcal { R } _ { \underline { { t } } } ^ { \bar { t } } ( \sigma ^ { * } ) ~ = ~ \sum _ { t = \underline { { t } } } ^ { \bar { t } } \gamma ^ { t } w _ { 0 } ^ { T } r ( s _ { t } ^ { * } , a _ { t } ^ { * } ) } \end{array}$

$$
\begin{array} { r l } & { \displaystyle \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } w _ { 0 } ^ { T } r ( s _ { t } ^ { 0 } , a _ { t } ^ { 0 } ) - \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } w _ { 0 } ^ { T } r ( s _ { t } ^ { * } , a _ { t } ^ { * } ) \geq 2 \frac { \gamma ^ { H } } { 1 - \gamma } r _ { \operatorname* { m a x } } } \\ & { \displaystyle \Leftrightarrow { \mathcal R } _ { 0 } ^ { H - 1 } ( \sigma ^ { \circ } ) - \mathcal R _ { 0 } ^ { H - 1 } ( \sigma ^ { * } ) \geq 2 \frac { \gamma ^ { H } } { 1 - \gamma } r _ { \operatorname* { m a x } } } \\ & { \qquad \geq | \mathcal R _ { H } ^ { \infty } ( \sigma ^ { \circ } ) | + | \mathcal R _ { H } ^ { \infty } ( \sigma ^ { * } ) | \geq \mathcal R _ { H } ^ { \infty } ( \sigma ^ { * } ) - \mathcal R _ { H } ^ { \infty } ( \sigma ^ { * } ) } \\ & { \displaystyle \Rightarrow \mathcal R _ { 0 } ^ { \infty } ( \sigma ^ { \circ } ) - \mathcal R _ { 0 } ^ { \infty } ( \sigma ^ { * } ) \geq 0 } \\ & { \displaystyle \Leftrightarrow \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } w _ { 0 } ^ { T } r ( s _ { t } ^ { 0 } , a _ { t } ^ { 0 } ) - \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } w _ { 0 } ^ { T } r ( s _ { t } ^ { * } , a _ { t } ^ { * } ) \geq 0 . } \end{array}
$$

Therefore, a sufficient condition that the preference between two trajectories is consistent with the preference between the two segments is that $\begin{array} { r } { \big \lvert \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } \dot { \pmb w } _ { 0 } ^ { T } \pmb r \big ( s _ { t } ^ { \circ } , a _ { t } ^ { \circ } \big ) \ - } \end{array}$ $\begin{array} { r } { \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } w _ { 0 } ^ { T } r ( s _ { t } ^ { * } , a _ { t } ^ { * } ) | \geq 2 \frac { \gamma ^ { H } } { 1 - \gamma } r _ { \operatorname* { m a x } } } \end{array}$ .PThis condition can always be satisfied when $H \  \ \infty$ , which means the true preference between two trajectories $( \mathbf { \nabla } \tau ^ { \circ } \succ _ { \mathbf { w } _ { 0 } } \tau ^ { * } )$ can always be obtained from the teacher. That contradicts Algorithm 1 which only terminates when $\nexists \pi ^ { \circ }$ s.t. $\pi ^ { \circ } \succ w _ { 0 } \pi ^ { * }$ and completes the proof. □

In practice, we typically select pairs of segments with distinct behaviors for human comparison, facilitating humans to provide preferences. Therefore, it is reasonable to assume

1: for each policy $\pi _ { i } \in \Pi$ do   
2: for each policy $\pi _ { j } \in \Pi$ do   
3: if for each $\mathbf { \boldsymbol { w } } _ { k } \in W _ { I }$ , $\begin{array} { r } { \pmb { w } _ { k } ^ { T } \sum _ { ( s , a ) \sim \sigma _ { i } } \gamma ^ { t } \pmb { r } ( s _ { t } , a _ { t } ) \ > \ } \end{array}$   
$\begin{array} { r } { { \pmb w } _ { k } ^ { T } \sum _ { ( s , a ) \sim \sigma _ { j } } \gamma ^ { t } { \pmb r } \big ( s _ { t } , a _ { t } \big ) } \end{array}$ , where $\sigma _ { i } , \sigma _ { j }$ are segments   
generated by $\pi _ { i } , \pi _ { j }$ , then   
4: Assign πi > πj   
5: end if   
6: end for   
7: end for   
8: Use insertion sort, obtain one or multiple biggest policies

that there exists a minimum difference $\delta$ in discounted returns between any two segments, that is, $\exists \delta \geq 0$ such that $\begin{array} { r } { | \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } w _ { 0 } ^ { T } r _ { 1 } ( s _ { t } , a _ { t } ) - \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } w _ { 0 } ^ { T } r _ { 2 } ( s _ { t } , a _ { t } ) | \geq \delta > 0 } \end{array}$ Under this assumption, we derive the following Corollary 1.

Corollary 1. If all segment pairs are distinct enough, i.e. $\exists \delta \geq$ 0 s.t. $\begin{array} { r } { \big | \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } \pmb { w } _ { 0 } ^ { T } \pmb { r } ( s _ { t } ^ { 1 } , a _ { t } ^ { \dot { 1 } } ) - \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } \pmb { w } _ { 0 } ^ { T } \pmb { r } ( s _ { t } ^ { 2 } , a _ { t } ^ { 2 } ) \big | \ge \delta > } \end{array}$ $\boldsymbol { 0 } \forall \sigma _ { 1 } , \sigma _ { 2 }$ , then each policy in the policy set $\pi ^ { * } ( a | s , \pmb { w } ) \in \Pi ^ { * }$ derived from Algorithm $\boldsymbol { { \mathit { 1 } } }$ is in the Pareto frontier when the segment length H ≥ logγ δ(21rm−axγ .

Theorem 2. Algorithm $^ { 1 }$ obtains the entire convex Pareto frontier, i.e., $\Pi ^ { * }$ is the entire convex Pareto frontier.

Proof. Since the Pareto frontier is convex, for each policy $\pi ^ { * }$ on the Pareto frontier, there must exist a weight $\textbf { \em w }$ s.t. $\mathbf { \boldsymbol { w } } ^ { T } \bar { \mathbf { \boldsymbol { R } } } ^ { * } \geq \mathbf { \boldsymbol { w } } ^ { T } \bar { \mathbf { \boldsymbol { R } } } ^ { \prime }$ , where $\begin{array} { r } { \bar { \pmb { R } } ^ { * } = \mathbb { E } _ { \pi } \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } \pmb { r } ( s _ { t } , \bar { a } _ { t } ) , } \end{array}$ is the expected total discounted return derived by $\pi ^ { * }$ , and $\bar { \boldsymbol { R } } ^ { \prime }$ is that derived by any other policies. Using Theorem 1, the optimal policy under weight $\textbf { \em w }$ could be obtained by Algorithm 1. Therefore, by traversing $\textbf { \em w }$ , we can traverse each policy on the Pareto frontier. □

Theorem 3. An arbitrary Pareto frontier could be completely obtained with preferences under every weight from an identity matrix weight set $W _ { I } ~ = ~ \{ { \pmb w } _ { i } ~ | ~ [ { \pmb w } _ { i } , \cdot \cdot \cdot ~ , { \pmb w } _ { m } ] ~ = ~ I , i ~ =$ $1 , \cdots , m \}$ .

Proof. We prove it by providing a constructive Algorithm 2.

If there is only one policy in the policy space $\Pi$ , then it is the Pareto frontier.

If we add a policy $\pi ^ { \prime }$ into the current policy space $\Pi$ , then $\pi ^ { \prime }$ will be compared to all $\pi \in \Pi$ , specifically, compared to the current Pareto frontier $\pi \in \Pi ^ { * }$ and the non-Pareto frontier $\pi \in \Pi \backslash \Pi ^ { * }$ .

If $\pi ^ { \prime }$ is not in the Pareto frontier, then $\exists \pi ^ { * } \in \Pi ^ { * }$ s.t. $\mathbf { \boldsymbol { w } } ^ { T } \mathbf { \boldsymbol { R } } ( \sigma ^ { \prime } ) \mathbf { \beta } < \mathbf { \beta } \mathbf { \boldsymbol { w } } ^ { T } \mathbf { \boldsymbol { R } } ( \sigma ^ { * } )$ for all $\textbf { \em w } \in \textbf { \em W } _ { I }$ , where $\mathbf { { \mathcal { R } } } ( \sigma ) \mathbf { \sigma } = \mathbf { \sigma }$ $\begin{array} { r } { \sum _ { t = 0 , ( s _ { t } , a _ { t } ) \sim \sigma } ^ { H } \gamma ^ { t } \pmb { r } ( s _ { t } , a _ { t } ) } \end{array}$ . Thus, through Algorithm 2, $\pi ^ { \prime }$ won’t be included in the new Pareto frontier.

If $\pi ^ { \prime }$ is in the Pareto frontier, then $\nexists \pi ^ { * } \in \Pi ^ { * }$ s.t. $\mathbf { \boldsymbol { w } } ^ { T } \mathbf { \boldsymbol { R } } ( \sigma ^ { \prime } ) \mathbf { \epsilon } < \mathbf { \boldsymbol { w } } ^ { T } \mathbf { \boldsymbol { R } } ( \sigma ^ { * } )$ for all $w ~ \in ~ W _ { I }$ . Thus, through Algorithm 2, $\pi ^ { \prime }$ will be included in the new Pareto frontier. That completes the proof. □

While linear weighting approaches $( { \pmb w } ^ { T } { \pmb R } )$ discover only the convex Pareto frontier as in Algorithm 2, Theorem 3 operates differently. By evaluating policies under unit vector weights $W _ { I }$ via pairwise preference comparisons, we directly assess the Pareto dominance relationship. This allows identification of non-convex Pareto-optimal policies. We provide another proof for Theorem 3 in Appendix A.

The theoretical analysis above has demonstrated that the preference-based multi-objective reinforcement learning framework can converge to the Pareto optimal set under specific conditions, providing important guarantees on its performance. Based on these results, we will describe the detailed steps of the algorithm in the next subsection, showing how this framework can be applied to optimize multi-objective policies in practical scenarios.

# B. Multi-Objective Reward Modeling

Based on the theoretical foundations established in the previous section, we now focus on the practical implementation of $\mathrm { P b }$ -MORL. In particular, we focus on constructing a multiobjective reward model that aligns with human preferences. By utilizing the preference data given by the teacher, we can develop an explicit reward model that captures the complexities of human decision-making.

Inspired by the previous work [11] in the single-objective scenario, we construct a preference predictor $P _ { \psi } [ \sigma _ { 0 } \succ \sigma _ { 1 } | { \pmb w } ]$ , which is designed to predict the preference $p$ given the pair of segments $\sigma _ { 0 }$ and $\sigma _ { 1 }$ under the weight $\textbf { \em w }$ , and is parameterized by $\psi$ . The preference predictor $P _ { \psi }$ can be trained by minimizing the cross-entropy loss:

$$
\begin{array} { r } { \mathcal { L } ^ { \mathrm { p } } = - \underset { ( \sigma _ { 0 } , \sigma _ { 1 } , w , p ) \sim \mathcal { D } } { \mathbb { E } } \Big [ p ( 0 ) \log P _ { \psi } [ \sigma _ { 0 } \succ \sigma _ { 1 } | w ] } \\ { + p ( 1 ) \log P _ { \psi } [ \sigma _ { 1 } \succ \sigma _ { 0 } | w ] \Big ] . } \end{array}
$$

Utilizing the Bradley-Terry model [11], [43], an explicit reward model $\hat { \pmb { r } } _ { \psi }$ can be constructed to predict the preference as follows:

$$
P _ { \psi } [ \sigma _ { 1 } \succ \sigma _ { 0 } | { \pmb w } ] = \frac { \exp \sum _ { t } \gamma ^ { t } { \pmb w } ^ { T } \hat { \pmb r } _ { \psi } ( s _ { t } ^ { 1 } , a _ { t } ^ { 1 } ) } { \sum _ { i \in \{ 0 , 1 \} } \exp \sum _ { t } \gamma ^ { t } { \pmb w } ^ { T } \hat { \pmb r } _ { \psi } ( s _ { t } ^ { i } , a _ { t } ^ { i } ) } .
$$

Eq. (14) models preferences as probabilistic outcomes, thereby accommodating the inherent ambiguity found in human judgments. Specifically, it suggests that the preference is exponentially related to the reward sum over the segment. Then, the reward model $\hat { \pmb { r } } _ { \psi }$ is trained to predict the preference under the weight $\textbf { \em w }$ . Although the estimator $\hat { r } _ { \psi }$ is not inherently a binary classifier, the process of learning this estimator can be regarded as a binary classification, where the preferences $p$ serve as the classification labels.

In the previous discussion, we introduce how to leverage the preference data to construct a reward model. Theoretically, when the reward model $\boldsymbol { r }$ aligns perfectly with the teacher’s preferences, we can directly optimize this model to derive the optimal policy. To formalize this relationship, we present the following theorem:

Theorem 4. If the reward model $\hat { \pmb { r } }$ is perfectly aligned with the teacher’s preferences, that is, for segments $( \sigma _ { 0 } , \sigma _ { 1 } )$ with arbitrary length $H$ ,

$$
\sum _ { ( s , a ) \sim \sigma _ { 0 } } ^ { \sigma _ { 0 } \vdash _ { w } \sigma _ { 1 } } \widehat { \gamma ^ { t } w ^ { T } } \widehat { r } ( s _ { t } , a _ { t } ) > \sum _ { ( s , a ) \sim \sigma _ { 1 } } \gamma ^ { t } w ^ { T } \widehat { r } ( s _ { t } , a _ { t } ) .
$$

Since the segment length $H$ can be arbitrarily long, the above equation is equivalent to

$$
\begin{array} { r l } & { \pi _ { 0 } \succ _ { w } \pi _ { 1 } \iff } \\ & { \qquad \mathbb { E } _ { \tau \sim \pi _ { 0 } } \displaystyle \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } { \pmb w } ^ { T } \hat { \pmb r } ( s _ { t } , a _ { t } ) > \mathbb { E } _ { \tau \sim \pi _ { 1 } } \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } { \pmb w } ^ { T } \hat { \pmb r } ( s _ { t } , a _ { t } ) . } \end{array}
$$

Then, under a given weight vector $\textbf { \em w }$ , maximizing the discounted return

$$
J ( \pi ) = \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } \pmb { w } ^ { T } \hat { \pmb { r } } ( s _ { t } , a _ { t } )
$$

is equivalent to selecting the optimal policy $\pi ^ { * } ( \cdot | \cdot , w )$ .

Proof. For contradiction, assume that there exists another policy $\pi ^ { \prime }$ that performs better than $\pi ^ { * }$ under the weight vector $\textbf { \em w }$ , i.e.,

$$
\sum _ { t = 0 } ^ { \infty } \gamma ^ { t } \pmb { w } ^ { T } \hat { \pmb { r } } ( s _ { t } ^ { \prime } , a _ { t } ^ { \prime } ) > \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } \pmb { w } ^ { T } \hat { \pmb { r } } ( s _ { t } ^ { * } , a _ { t } ^ { * } ) ,
$$

where $( s _ { t } ^ { \prime } , a _ { t } ^ { \prime } )$ and $( s _ { t } ^ { * } , a _ { t } ^ { * } )$ are from the trajectories generated by policies $\pi ^ { \prime }$ and $\pi ^ { * }$ , respectively. In this case, the teacher would prefer the trajectory of $\pi ^ { \prime }$ over that of $\pi ^ { * }$ .

However, since the reward model $\boldsymbol { r }$ is perfectly aligned with the teacher, we have:

$$
\begin{array} { r l } & { \pi ^ { \prime } \prec _ { \pmb { w } } \pi ^ { * } \Longrightarrow } \\ & { \qquad \displaystyle \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } { \pmb { w } } ^ { T } \hat { \pmb { r } } ( s _ { t } ^ { \prime } , a _ { t } ^ { \prime } ) < \displaystyle \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } { \pmb { w } } ^ { T } \hat { \pmb { r } } ( s _ { t } ^ { * } , a _ { t } ^ { * } ) , } \end{array}
$$

which contradicts the fact that $\pi ^ { * }$ is the optimal policy under the reward model $\hat { \pmb { r } }$ . Therefore, the assumption is false, and the theorem holds. □

# C. MORL based on Multi-Objective Reward Model

Having established the construction method of the reward model $\hat { \pmb { r } } _ { \psi }$ , we now focus on implementing the $\mathrm { P b }$ -MORL algorithm. Specifically, we leverage the learned reward model as a substitute for the traditional reward function, enabling the direct application of existing MORL techniques.

In typical MORL training process, the algorithm collects transitions $( s , a , s ^ { \prime } , r , \pmb { w } )$ , which are composed of state $s$ , action $a$ , next state $s ^ { \prime }$ , multi-objective reward $\boldsymbol { r }$ and the weight vector $\textbf { \em w }$ . These transitions are then utilized to update value functions and policies. In contrast, our method collects transitions where the reward $\boldsymbol { r }$ is replaced with the predicted reward from the model, $\hat { r } _ { \psi }$ . This allows us to align the policy with preference data by minimizing the loss as Eq. (13). This leads to a straightforward implementation of Pb-MORL. We first train the multi-objective reward model, followed by conducting MORL training based on this reward model.

![](images/2070a14e4096db3aba77d5e71c9ccfc39c9875cbad3c2d48376319ba038f7569.jpg)  
Fig. 2. The training curves of the expected utility and hypervolume on three multi-objective benchmark tasks. The experiments are conducted on 5 random seeds. Blue: the oracle method (EQL). Red: our method.

However, directly using the reward model to train an MORL agent may potentially result in inefficient policy learning:

1) Insufficient amount of preference data: To train a highquality reward model, a substantial amount of preference data may need to be collected beforehand.   
2) Imprecise reward model: When the amount of preference data is insufficient, the reward model may overfit the limited training data, resulting in an imprecise reward model and consequently leading to suboptimal policy performance.

Below are two techniques that can help improve sample efficiency and performance.

• Continuous preference collection: Continuously gather preference data during training, which can enrich the training data of the reward model.   
Relabeling: Relabel historical data with the updated reward model, which can increase the sample efficiency of preference and transition data.

Based on the above techniques, we present Algorithm 3, which is a variant of the Pb-MORL approach discussed above. By integrating the Envelope multi-objective Q-learning (EQL) [33] into our learning process, Algorithm 3 achieves a simple yet effective approach for policy optimization. Specifically, in lines 3-13, the agent interacts with the environment to collect transition data. In lines 14-22, the multi-objective reward model is updated continuously during policy training. In line 23, the rewards of the transition data in the replay buffer are relabeled. In lines 26-29, the Q-function and policy are updated using the EQL method.

# IV. EXPERIMENTAL RESULTS

# A. Setups

In this section, we conduct several experiments to evaluate the effectiveness of the proposed method. We test $\mathrm { P b }$ -MORL on several benchmark multi-objective tasks [33], [35], [44] to demonstrate its effectiveness across diverse multi-objective settings. Additionally, we evaluate $\mathrm { P b }$ -MORL on a custom task for multi-energy storage system charging and discharging as well as an autonomous driving task on a multi-line highway, showing its potential for real-world industrial applications.

Construction of the multi-objective teacher. Similar to prior PbRL works [9], [27], [45], in order to systemically evaluate the performance, we construct a “scripted teacher”, which provides preferences $p$ between two trajectory segments $\sigma _ { 0 } , \sigma _ { 1 }$ under certain weight $\textbf { \em w }$ according to the task’s ground truth reward function. The following paragraph formalizes the process used by the scripted teacher in multi-objective RL.

Let $\boldsymbol { r } _ { g t }$ denote the task’s ground truth reward function. Then, for the segment pair $( \sigma _ { 0 } , \sigma _ { 1 } )$ , the scripted teacher first computes the discounted reward sum:

$$
R _ { i } = \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } { \pmb r } _ { g t } ( s _ { t } ^ { i } , a _ { t } ^ { i } ) \quad i = 0 , 1 ,
$$

where $\gamma$ is the discount factor, $t$ is the time step, $s _ { t } ^ { i } , a _ { t } ^ { i }$ represents the state and action of segment $\sigma _ { i }$ in time step $t$ , and $H$ is the segment length. Next, the teacher computes the weighted inner product with the given weight vector $\textbf { \em w }$ :

$$
R _ { i } = { { w } ^ { T } } R _ { i } = { { w } ^ { T } } \left( \sum _ { t = 0 } ^ { H - 1 } \gamma ^ { t } r _ { g t } ( s _ { t } ^ { i } , a _ { t } ^ { i } ) \right) \quad i = 0 , 1 ,
$$

Algorithm 3 Pb-MORL algorithm using the EQL method   
Input: Frequency of teacher feedback $K$ , number of sampled segment $N _ { s }$ , number of sample weights $N _ { w }$ for reward learning, timesteps for learning start $T _ { 0 }$   
Output: Multi-objective reward model $\hat { r } _ { \psi }$ , multi-objective $\mathsf { Q }$ function $Q _ { \theta }$ , policy $\pi _ { \phi } ( \cdot | \cdot , w )$   
1: Initialize parameter vectors $\psi , \theta , \phi$   
2: for each iteration do 3: for each environment step $t$ do   
4: Obtain current state $s _ { t }$   
5: if global step $< T _ { 0 }$ then   
6: Randomly sample action $a _ { t }$   
7: else   
8: Obtain action $a _ { t } \sim \pi _ { \phi } ( \cdot | s _ { t } , w )$ under a weight $\textbf { \em w }$   
9: end if   
10: Obtain transition $( s _ { t } , a _ { t } , s _ { t + 1 } , \pmb { w } )$   
11: Obtain multi-objective reward $\hat { \pmb { r } } ( s _ { t } , a _ { t } )$   
12: Add $( s _ { t } , a _ { t } , s _ { t + 1 } , \hat { r } ( s _ { t } , a _ { t } ) , w )$ into replay buffer $\mathcal { D }$   
13: end for   
14: if iteration mod ${ \mathrm { K } } = = 0$ then   
15: Sample $N _ { s }$ query $( \sigma _ { 0 } , \sigma _ { 1 } ) \sim \mathcal { D }$   
16: Sample $N _ { w }$ weights $\textbf { \em w }$   
17: Query overseer for preference $p$ for all queries $( \sigma _ { 0 } , \sigma _ { 1 } )$ under all $\textbf { \em w }$   
18: Store all $N _ { s } \times N _ { w }$ $( \sigma _ { 0 } , \sigma _ { 1 } , \pmb { w } , p )$ to buffer $\mathcal { D } _ { p }$   
19: for each gradient step do   
20: Sample minibatch $( \sigma _ { 0 } , \sigma _ { 1 } , \pmb { w } , p ) \sim \mathcal { D } _ { p }$   
21: Optimize Eq. (13) to update reward model $\hat { r } _ { \psi }$   
22: end for   
23: Relabel entire replay buffer $\mathcal { D }$ using $\hat { r } _ { \psi }$   
24: end if   
25: for each gradient step do   
26: Sample a minibatch from replay buffer $\mathcal { D }$   
27: Update $\mathrm { ~ Q ~ }$ function $\scriptstyle Q _ { \theta }$ by minimizing $| Q \rrangle - B Q |$ under $( s , a , s ^ { \prime } , r , w ) \sim \mathcal { D }$ , as Eq. (6)   
28: Update the Q-learning policy $\pi _ { \phi } ( \cdot | \cdot , w )$ by maximizing ${ \pmb w } ^ { T } { \pmb Q } ( s , \pi _ { \phi } ( \cdot | s , { \pmb w } ) , { \pmb w } )$ under $( s , a , s ^ { \prime } , r , w ) \sim$ $\mathcal { D }$   
29: end for

The scripted teacher compares $R _ { 0 }$ and $R _ { 1 }$ to determine which segment performs better:

$$
p = \left\{ { \begin{array} { l l } { 1 , } & { { \mathrm { i f ~ } } R _ { 0 } > R _ { 1 } , } \\ { 0 . 5 , } & { { \mathrm { i f ~ } } R _ { 0 } = R _ { 1 } , } \\ { 0 , } & { { \mathrm { i f ~ } } R _ { 0 } < R _ { 1 } . } \end{array} } \right.
$$

Since the scripted teacher’s preferences directly correspond to the task’s ground truth reward, the algorithms can be quantitatively evaluated using the ground truth reward function.

Evaluation metrics. We use two metrics to evaluate the empirical performance on each task:

1) Expected Utility (EU) [30]: This metric measures the average utility under randomly sampled weights. Let $\textbf { \em w }$ be a weight vector randomly sampled from the uniform distribution in $\mathcal { W }$ space. Let $U ( \pi , { \pmb w } )$ represent the utility function of policy $\pi ( \cdot | \cdot , w )$ under the weight $\textbf { \em w }$ , which is usually the inner product of the discounted return and the weight $\textbf { \em w }$ . The expected utility $\mathrm { E U } ( \pi )$ is then defined as:

TABLE II HYPERPARAMETER SETTINGS FOR PB-MORL   

<html><body><table><tr><td>Hyperparameter</td><td>Value</td></tr><tr><td>Preference frequency K</td><td>500</td></tr><tr><td>Number of sampled segment Ns</td><td>300</td></tr><tr><td>Number of sample weights Nw</td><td>10</td></tr><tr><td>Discount factor γ</td><td>0.99</td></tr><tr><td>Batch size</td><td>256</td></tr><tr><td>Learning rate Training timesteps</td><td>3×10-4</td></tr><tr><td>Number of Q network hidden layers</td><td>1 ×106</td></tr><tr><td>Number of hidden units per layer</td><td>2</td></tr><tr><td></td><td>128</td></tr><tr><td>Q target update T</td><td>1×10-4</td></tr><tr><td>Optimizer</td><td>Adam</td></tr></table></body></html>

$$
\begin{array} { r } { \operatorname { E U } ( \pi ) = \operatorname { \mathbb { E } } _ { w } U ( \pi , w ) . } \end{array}
$$

Expected Utility is crucial for evaluation, as it comprehensively measures a policy’s overall performance across objectives. Unlike hypervolume [46], which focuses on boundary solutions, EU evaluates the policy’s average behavior over the entire weight space. Thus, it serves as a more relevant indicator of general performance in many multi-objective tasks.

2) Hypervolume (HV) [46] : Given an approximate Pareto Frontier set $\tilde { \mathbf { F } }$ of multi-objective return and a reference point $R _ { \mathrm { r e f } }$ , the hypervolume metric is defined as:

$$
\mathrm { H V } ( \tilde { \mathbf { F } } , R _ { \mathrm { r e f } } ) = \bigcup _ { R \in \tilde { \mathbf { F } } } \mathrm { v o l u m e } ( R _ { \mathrm { r e f } } , R ) ,
$$

where volume $( R _ { \mathrm { r e f } } , R )$ is the volume of the hypercube spanned by the reference vector $R _ { \mathrm { r e f } }$ and the vector $\scriptstyle { R }$ . The reference point here is typically an estimation of the worst possible return for all objectives.

Experimental details. For implementation details, for line 12 of Algorithm 3, we use a query-policy aligned replay buffer to maintain an accurate reward model in the nearpolicy region [47]. For line 17, we use the scripted teacher mentioned earlier to generate preference data. The detailed hyperparameter settings of Pb-MORL are shown in Table II. For the baseline, we use EQL [33] as an oracle method, which leverages the ground truth reward function for policy learning.

# B. Experimental Results on Multi-Objective Benchmark Tasks

Tasks. We evaluate our method on three multi-objective benchmark tasks [30], each presenting distinct challenges, such as balancing time-cost and total reward or optimizing independently across multiple objectives:

Deep Sea Treasure (DST) [48]: An agent controls a submarine in a $1 0 \times 1 1$ grid to discover treasures, balancing time and treasure value. The grid contains 10 treasures, with the value increasing with the distance from the starting point $s _ { 0 } = ( 0 , 0 )$ . The multi-objective reward

![](images/bff5192d65036a417cd27c141816f4184e87b5879c51352673fe2799b76459f3.jpg)  
Fig. 3. Visualization of the estimated Pareto frontier of two methods in FT. Note that the actual Pareto frontier in FT has 6 dimensions, we add up the first 3 and last 3 dimensions of rewards for illustration.

$\textstyle r ( s , a )$ has two dimensions: $r _ { 1 } ( s , a )$ for treasure value and $r _ { 2 } ( s , a )$ for time cost, decreasing by 1 for each step. Fruit Tree (FT) [33]: A full binary tree provides a sixdimensional reward $\pmb { r } \in \mathbb { R } ^ { 6 }$ at each leaf, representing nutritional components: PROTEIN, CARBS, FATS, VITAMINS, MINERALS, WATER . The agent maximizes utility for a given weight by selecting the optimal path from root to leaf while choosing between the left and right subtrees. • Resource Gathering (RG) [49]: An agent collects the gold or gem in a $5 \times 5$ grid while evading two enemies. Encountering an enemy in the same cell poses a $10 \%$ risk of death. The multi-objective reward $\textstyle r ( s , a )$ has three dimensions: $r _ { 1 } ( s , a ) = - 1$ if being killed, $r _ { 2 } ( s , a ) =$ $+ 1$ if safely returning home with gold, $r _ { 3 } ( s , a ) = + 1$ if returning with gem.

To justify the selection of $H$ values for each task, we analyze their characteristics. For the DST task, where episode length varies and rewards accumulate over time, we choose $H = 7$ to capture the cumulative effects. In the FT task, with a fixed episode length of 6, $H = 6$ is suitable to encompass the full trajectory. For the RG task, which features sparse rewards and early episode termination, we select $H = 1 0$ to ensure enough steps are available to differentiate between policies.

Figure 2 presents the expected utility and hypervolume results for the three tasks. In the DST task, our method performs comparably to the oracle in expected utility, demonstrating consistent utility improvement over time. For the FT task, our method matches the oracle in expected utility and surpasses it in hypervolume, indicating effective use of preference for enhancing the Pareto frontier quality. In the RG task, while our method’s utility approaches optimal performance, the hypervolume results are less favorable. This may be because the returns of RG are limited to 0 or $\pm 1$ , and the learned reward model exhibits imprecision in capturing edge-cases under these sparse rewards, which restricts hypervolume growth.

To demonstrate the high quality of the Pareto frontier we learned, we visualized the Pareto frontier learned by EQL and our method in the FT task, as shown in Fig. 3. Our method captures the key factors of the Pareto frontier of the oracle method, showing its effectiveness for application in practice.

![](images/0b9fa9cab941d3f172c1c2109f7299e00ce6145f1d44d557dd964482c28b7d3f.jpg)  
Fig. 4. The training curves of the expected utility and hypervolume on the energy system task, averaging over 5 random seeds. Blue: the oracle method (EQL). Red: our method.

# C. Experimental Results on the Custom Energy Task

The multi-energy management task. To assess the potential of $\mathrm { P b }$ -MORL for real-world industry applications, we designed a custom multi-objective task for multi-energy storage, simulating the charging and discharging of an energy storage system. The agent controls discharge and charge levels to satisfy external energy demands while balancing cost savings and system lifespan.

• State space: The state space includes four scalar values: Current stored energy sstorage $( \mathrm { k W h } )$ , current energy generated from renewable sources $s _ { \mathrm { n e w } } ( \mathrm { k W h } )$ , external energy demand $s _ { \mathrm { d e m a n d } }$ (kWh), and the electricity market price $s _ { \mathrm { p r i c e } }$ (monetary units). Thus, the state vector can be represented as:

$$
s = [ s _ { \mathrm { s t o r a g e } } , s _ { \mathrm { n e w } } , s _ { \mathrm { d e m a n d } } , s _ { \mathrm { p r i c e } } ] .
$$

Action space: The action $a$ is a scalar indicating the discharge level. Positive values represent energy discharged to meet external demand, while negative values indicate energy charged from renewable sources or the grid.

Transition: After a state transition, the new storage level is calculated as:

$$
s _ { \mathrm { s t o r a g e } , t + 1 } = \operatorname* { m i n } ( s _ { \mathrm { s t o r a g e } } ^ { \operatorname* { m a x } } , ( s _ { \mathrm { s t o r a g e } , t } - a _ { t } ) ^ { + } ) ,
$$

where $s _ { \mathrm { s t o r a g e } } ^ { \mathrm { m a x } }$ is the maximum capacity of the energy storage, and $( \cdot ) ^ { + }$ denote $\operatorname* { m a x } ( \cdot , 0 )$ .

Reward function: The reward is a two-dimensional vector, where the first dimension $r _ { 1 } ( s _ { t } , a _ { t } )$ penalizes the electricity purchasing cost. At each time step, the system may purchase energy to satisfy the external energy demand and charge the storage. The amount of energy bought for charging is

$$
g _ { \mathrm { c h a r g e } } = \left\{ \begin{array} { l l } { ( - a - ( s _ { \mathrm { n e w } } - s _ { \mathrm { d e m a n d } } ) ^ { + } ) ^ { + } } & { a < 0 } \\ { ( a - s _ { \mathrm { s t o r a g e } } ) ^ { + } } & { a \geq 0 } \end{array} , \right.
$$

and that for external demand is

$$
g _ { \mathrm { d e m a n d } } = ( ( s _ { \mathrm { d e m a n d } } - s _ { \mathrm { n e w } } ) ^ { + } - ( a ) ^ { + } ) ^ { + } ,
$$

$r _ { 1 } ( s _ { t } , a _ { t } )$ is calculated as $r _ { 1 } ( s _ { t } , a _ { t } ) = s _ { \mathrm { p r i c e } } \times ( g _ { \mathrm { d e m a n d } } +$ $g _ { \mathrm { c h a r g e } } )$ . The second dimension $r _ { 2 } ( s _ { t } , a _ { t } )$ indicates a penalty for discharging: $r _ { 2 } ( s _ { t } , a _ { t } ) = - 1$ when energy is discharged and 0 otherwise. This design aims to reduce discharges, thus prolonging the system’s lifespan.

In this task, rewards are cumulative, with a maximum episode length of 50. Since the agent does not face failures leading to early termination, a sufficiently long $H$ is critical for capturing long-term policy performance. Setting $H = 1 0$ allows for comprehensive observation of cumulative returns, enabling effective differentiation among policy performances during optimization.

Figure 4 presents the experimental results of the multienergy management task. Our method surpasses the oracle method in expected utility. This can be attributed to the task’s inherent randomness and complex transition dynamics, which make it challenging to directly optimize task rewards like electricity costs or the lifespan loss of system charging. Instead, preference provides a more flexible way to guide policy optimization, allowing the policy to adapt to system complexities more effectively. Additionally, our method matches the oracle in the hypervolume metric.

The weight-conditioned policy $\pi ( a | s , w )$ learned by PbMORL allows dynamic adaptation to changing user preferences. For instance, in energy management, operators may prioritize cost reduction during peak pricing periods $( w _ { 1 } \uparrow )$ and system longevity during high-stress operations $( w _ { 2 } \uparrow )$ . Since $\mathrm { P b }$ -MORL trains a single policy conditioned on arbitrary weights $w \in \boldsymbol { \mathcal { W } }$ , adapting to such changes only requires modifying the input weight vector $w$ at deployment, eliminating the need for policy retraining. Similarly, in autonomous driving in Section IV-D, safety weights can be adjusted during adverse weather by simply updating $w$ . This adaptability minimizes operational overhead and enables Pb-MORL to respond instantly to evolving objectives, making it well-suited for dynamic environments with non-stationary preferences.

# D. Experimental Results on the Multi-Lane Highway Task

The multi-lane highway task. To validate the effectiveness of our approach in real-world complex control scenarios, we evaluate it in a multi-lane highway task [30], [50]. In this task, the agent navigates a three-lane highway while driving as quickly as possible, avoiding collisions and prioritizing positioning in the rightmost lane. This setting comprehensively tests the agent’s ability to perform in dynamic and multifaceted environments.

State space: The state is represented by a $V \times 5$ matrix that includes the coordinates and speeds of the ego vehicle and $V { - } 1$ surrounding vehicles. Each line consists of [presence of the vehicle, $x$ coordinate, $y$ coordinate, $x$ velocity, $y$ velocity]. Action space: Actions are categorized discretely as follows: lane change to the left (0), maintaining the current state (1), lane change to the right (2), acceleration (3), and deceleration (4). These actions are integrated with a lower-level controller for speed and steering. Transition: The vehicle kinematic is modeled using a simplified kinematic bicycle model, which assumes the left and right wheels function as a single wheel. It regards the front wheel as the primary steering control while omitting sliding effects, thus enabling a more straightforward representation of vehicle dynamics. This model formulation captures the essential dynamics of real-world vehicle behavior, enhancing the simulation’s fidelity. The following equations describe the vehicle’s motion:

![](images/31164e1d01a00908390f2b3b8972ae79e05882b0b935017c291b9c73effb3176.jpg)  
Fig. 5. The training curves of the expected utility and hypervolume on the highway task, averaging over 5 random seeds. Blue: the oracle method (EQL). Red: our method.

$$
\left\{ \begin{array} { l l } { \dot { x } } & { = v \cos ( \theta + \beta ) , } \\ { \dot { y } } & { = v \sin ( \theta + \beta ) , } \\ { \dot { v } } & { = a , } \\ { \dot { \theta } } & { = \frac { v } { l _ { r } } \tan ( \delta ) , } \\ { \beta } & { = \arctan \left( \frac { l _ { r } } { l _ { f } + l _ { r } } \tan ( \delta ) \right) . } \end{array} \right.
$$

The surrounding vehicles are controlled by the Intelligent Driver Model (IDM) [51] and the Minimum Overall Braking Distance (MOBIL) model [52].

Reward function: The reward function comprises a three-dimensional vector. The first element represents speed reward, calculated as v−vmin , where v is the current speed of the ego vehicle, and $v _ { \mathrm { m i n } }$ and $v _ { \mathrm { m a x } }$ denote the minimum and maximum allowable speeds, respectively. The second element indicates lane position reward, as 1 if the ego vehicle is in the rightmost lane and 0 otherwise. The third element is a collision penalty, assigned $- 1$ upon collision and 0 otherwise.

We selected $H = 3$ and $H = 6$ for our evaluation. $H = 3$ corresponds to a low-level behavior over a 5-second period, while $H = 6$ represents longer driving behavior, allowing for a more comprehensive assessment of the agent’s performance. By evaluating our method with both time horizons, we obtain a more nuanced understanding of its capabilities across different driving scenarios. Following prior works [53], [54], we trained the agent for 200,000 steps.

As shown in Figure 5, our method surpasses the oracle method regarding both expected utility and hypervolume.

In contrast to EQL, which experiences a significant performance decline after an initial increase followed by a slow recovery, our approach maintains stable performance without such setbacks. The initial drop in EQL may be because the agent becomes overly focused on immediate goals, such as speed, resulting in aggressive policies that often neglect safety. Consequently, when the negative impact of collision occurs, the agent must re-explore to find more stable policies. In contrast, Pb-MORL addresses this overfitting through preferencedriven reward learning. This method emphasizes the relative benefits of multiple objectives (e.g., “safe overtaking $\succ$ aggressive overtaking”) rather than focusing on absolute value differences, enabling the reward model to learn the tradeoffs of specific scenarios. Additionally, continuous preference feedback helps to recalibrate the reward model in three phases: balancing objectives, refining scene-specific policies and optimizing for long-tail risks. This approach effectively prevents oscillations caused by conflicting targets.

In summary, Sections IV-C and IV-D demonstrate that our preference-guided policy outperforms the oracle across multiple metrics and adapts more effectively to complex systems than direct task reward optimization. Additionally, the resulting multi-objective policies exhibit strong interpretability, clearly showing how weight vectors impact policy behavior. These findings underscore the potential of our approach for optimizing complex real-world systems.

# V. CONCLUSION

This paper presents the preference-based multi-objective reinforcement learning (Pb-MORL) algorithm, which leverages preference data to overcome the limitations of complicated reward design. Our contributions include a theoretical proof of optimality, showing the $\mathrm { P b }$ -MORL framework can guide the learning of Pareto-optimal policies. In addition, we construct an explicit multi-objective reward model that directly aligns with user preferences, enabling more intuitive decision-making in complex scenarios. Extensive experiments demonstrate the effectiveness and interpretability of $\mathrm { P b }$ -MORL in optimizing various types of multi-objective tasks. Through this work, we highlight the potential of preference-based frameworks in enhancing multi-objective optimization.

Future research can explore several directions. First, we recognize that some assumptions in our work may not hold in practical scenarios. Specifically, for the symmetry, consistency, and transitivity requirements in Assumption 1, we can explore non-transitive cases through pairwise ranking methods [55] and utilize preference aggregation strategies to address violations of other properties. Second, Assumption 2 could be relaxed through active query strategies [26] that optimize comparison requests. Third, to expand its utility in complex systems, we aim to apply $\mathrm { P b }$ -MORL to various domains, such as financial investment and smart manufacturing. Notably, we provide an alternative perspective in Appendix B, discussing the motivation for Pb-MORL, highlighting the impact of human subjectivity in preference data on learning quality in traditional PbRL.