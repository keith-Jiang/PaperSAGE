# CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning

Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li and Chris Shum

DeepReinforce Team research@deepreinforce.ai

Project Page

# Abstract

The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models (LLMs), has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current state-of-the-art models (e.g. DeepSeek-R1, OpenAI-o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning (RL) framework for CUDA optimization. The core of CUDA-L1 is a contrastive RL model, a newly-designed RL system to enhance optimization through comparative learning. Different from performing gradient updates in isolation as in previous RL models, contrastive RL performs comparative analysis of previously generated CUDA variants alongside their execution performance, enabling the model to improve by distinguishing between effective and ineffective CUDA optimization strategies.

CUDA-L1 achieves unprecedented performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of $\mathbf { \times 1 7 . 7 }$ across all 250 CUDA kernels of KernelBench, with peak speedups reaching $\times 4 4 9$ . Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of $\mathbf { \times 1 7 . 8 }$ on H100, $\mathbf { \nabla \times 1 9 . 0 }$ on RTX 3090, $\mathbf { \times 1 6 . 5 }$ on L40, $\mathbf { \times 1 4 . 7 }$ on H800, and $\mathbf { \nabla \times 1 3 . 9 }$ on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: (1) It autonomously discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; (2) It automatically uncovers fundamental principles of CUDA optimization, such as the multiplicative nature of optimizations and how certain "gatekeeper" techniques must be applied first to unlock the effectiveness of others; (3) It identifies non-obvious performance bottlenecks (such as CPU-GPU synchronization dominating compute optimizations) and rejects seemingly beneficial optimizations that actually harm performance.

The capabilities of CUDA-L1 demonstrate that, reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. The trained RL model can successfully identify CUDA optimization patterns, discovers new techniques, synthesizes them to achieve speedups, and more importantly, extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.

![](images/f1f1c4304a2c0024efcfe54f0657a010dbe30d1020f83ed86a45550b7c886a61.jpg)  
Figure 1: Average speedup achieved by CUDA-L1 across different architectures on KernelBench.

![](images/1111c1f00eebe49a4f20b10df249af05c0a37ef6057e4d440a5e4a92f92412d3.jpg)  
Figure 2: Overview of the CUDA-L1 training pipeline. The approach consists of three progressive stages: (1) Stage 1: Supervised Fine-tuning with Data Augmentation – We augment the training dataset with CUDA code variants generated by LLMs and fine-tune the base model on executable and correct implementations to establish foundational CUDA knowledge. (2) Stage 2: Self-supervised Learning – The model iteratively generates CUDA kernels, validates their correctness and executability, and trains on successfully validated examples, enabling autonomous improvement without human supervision. (3) Stage 3: Contrastive Reinforcement Learning – We employ contrastive learning with execution-time rewards, training the model to distinguish between faster and slower CUDA implementations, ultimately optimizing for superior performance.

# Introduction

The exponential growth in demand for GPU computing resources, driven primarily by the rapid advancement and deployment of Large Language Models (LLMs), has created an urgent need for highly efficient CUDA optimization strategies. Traditionally, CUDA optimization has been a highly manual and time-intensive process, where skilled engineers must meticulously analyze memory access patterns, experiment with different thread block configurations, and iteratively profile their code through extensive trial-and-error cycles.

Recent advances in LLMs [25, 24, 26, 7, 32, 9, 11, 15, 19], especially those powered with RL [10, 8, 27, 17], have demonstrated remarkable capabilities in code generation and algorithm design. RL-powered LLMs hold significant potential to revolutionize the CUDA optimization process: CUDA optimization provides a uniquely clear reward signal—execution speed—which could be directly leveraged to automatically train reinforcement learning models. By treating performance improvements as rewards, RL-powered LLMs could iteratively generate, test, and refine CUDA optimizations without human intervention. This approach would not only automate the labor-intensive optimization process, potentially saving countless engineering hours, but also opens possibilities for discovering novel speedup algorithms that may surpass human-designed solutions. Unlike human engineers who are constrained by existing knowledge and conventional approaches, these systems could explore unconventional optimization combinations and potentially discover counterintuitive strategies that deliver significant performance improvements, offering new possibilities for advancing GPU computing efficiency.

Despite the promise, current performance remains limited. State-of-the-art LLM models such as DeepSeek-R1 [8] and OpenAI-o1 [10] achieve low success rates in generating optimized CUDA code (only approximately $1 5 \%$ on KernelBench [20]), which is primarily due to the scarcity of CUDA code in training datasets. To address these limitations and unlock the potential of LLMs for automated CUDA optimization, in this work, we propose CUDA-L1, an LLM framework powered by contrastive reinforcement learning for CUDA optimization. CUDA-L1 is a pipelined framework, the core of which is a newly-designed contrastive RL framework.

Different from previous RL models [31, 23, 22] , contrastive RL performs comparative analysis of previously generated CUDA variants alongside their execution performance, enabling the model to improving through distinguishing between effective and ineffective optimization strategies. Contrastive-RL simultaneously optimizes the foundation model through gradientbased parameter updates while fulfilling the maximum potential from the current model through contrastive analysis from high-performance CUDA variants, creating a co-evolutionary dynamic that drives superior CUDA optimization performance.

CUDA-L1 delivers unprecedented improvements on the CUDA optimization task: trained on NVIDIA A100, it achieves an average speedup of $\mathbf { \times 1 7 . 7 }$ across all 250 KernelBench CUDA kernels, with maximum speedups reaching $\times 4 4 9$ . Furthermore, the CUDA codes optimized specifically for A100 demonstrate excellent portability across GPU architectures, achieving average speedups of $\mathbf { \nabla \times 1 7 . 8 }$ on H100, $\mathbf { \nabla \times 1 9 . 0 }$ on RTX 3090, $\mathbf { \times 1 6 . 5 }$ on L40, $\mathbf { \times 1 4 . 7 }$ on H800, and $\mathbf { \nabla \times 1 3 . 9 }$ on H20. In addition to benchmark results, CUDA-L1 demonstrates several remarkable properties:

• Automatic Discovery of Optimization Techniques: It automatically discovers a variety of CUDA optimization techniques, e.g., memory layout optimization, operation fusion, loop unrolling, memory coalescing etc. While some of these techniques are already widely adopted in the CUDA optimization community, others remain underutilized.   
• Optimal Combination Selection: Upon the discovery of these techniques, CUDA-L1 can identify the optimal combination of them to achieve maximum speedup for different CUDA tasks.   
• Uncovering Fundamental Principles: CUDA-L1 is able to uncover fundamental principles of CUDA optimization, such as the multiplicative nature of optimizations and how certain “gatekeeper” techniques must be applied first to unlock the effectiveness of others.   
• Identifying Hidden Bottlenecks: It identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that actually harm performance.

Beyond, CUDA-L1 reveals a remarkable capability of RL in autonomous learning for CUDA optimization:

1. Even starting with a foundation model with poor CUDA optimization ability, by using code speedups as RL rewards and proper contrastive RL training techniques, we can still train an RL system capable of generating CUDA optimization codes with significant speedups.   
2. Without human prior knowledge or guidance, RL systems can independently discover CUDA optimization techniques, learn to combine them strategically, and more importantly, extend the acquired CUDA reasoning abilities to unseen kernels with significant speedups. This capability unlocks the potential for a variety of automatic CUDA optimization tasks, e.g., kernel parameter tuning, memory access pattern optimization, and different hardware adaptations, offering substantial promises to enhance GPU utilization during this period of unprecedented computational demand.

# 2 CUDA-L1

# 2.1 Overview

Existing large language models [8, 32, 7] demonstrate significant limitations in generating executable and correct CUDA code with speedup, as reported in prior research [20]. This deficiency likely stems from the insufficient representation of CUDA code in the training datasets of these models. To address this fundamental gap, we introduce a three-stage pipelined training strategy for CUDA-L1, aiming to progressively enhances the model’s CUDA programming capabilities:

1. Supervised fine-tuning via data augmentation, which aims to expand the model’s exposure to CUDA patterns and programming constructs, with the primary goal of producing correct and executable CUDA code.   
2. Self-supervised learningm which focuses on enabling models to develop a deeper understanding of CUDA semantics and programming principles, primarily aiming to achieve significant improvements in executability and correctness, while providing moderate speedup gains.   
3. Contrastive reinforcement learning, which is designed to significantly optimize code execution speed, with the goal of generating high-performance CUDA implementations that deliver substantial speedup.

Before we delve into the details of each stage, we provide key definitions adopted throughout the rest of this paper:

1. Executability: A CUDA code is executable if it successfully compiles, launches, and executes to completion within $1 0 0 0 \times$ the runtime of the reference implementation. Code exceeding this runtime threshold is considered unexecutable.1   
2. Correctness: A CUDA code is correct if it produces equivalent outputs to the reference implementation across 1000 random test inputs.2   
3. Success: A CUDA code is successful if it is both executable and correct.

# 2.2 SFT via Data Augmentation

In the SFT stage, we collect a dataset by using existing LLMs to generate CUDA code snippets and selecting successful one. This dataset is directly used to fine-tune the model. Throughout this paper, we use deepseek-v3-671B [15] as the model backbone.

Data Collection To expand the model’s exposure to CUDA patterns, we begin with data augmentation based on reference code from 250 tasks in KernelBench, which provides the official implementations used in PyTorch. To generate executable and correct CUDA code efficiently, we leverage six existing LLM models: GPT-4o, OpenAI-o1, DeepSeek-R1, DeepSeek V3, Llama 3.1-405B Instruct, and Claude 3.7. For each model, we construct prompts using the one-shot strategy, where the prompt contains the reference code (denoted by $q _ { i } , i \in [ 1 , 2 5 0 ] )$ and asks the LLM to generate an alternative speedup implementation. We employ multiple models to maximize the diversity of successful CUDA code generation. The detailed prompt structure is provided in Table 2. For each of the six models, we iterate through all 250 tasks. Each task allows up to 20 trials and terminates early if we successfully collect 2 trials that are both executable and correct. Notably, some tasks may fail to produce any successful code across all trials. The successful code is denoted by $d _ { i , j }$ , where $j \in \{ 1 , 2 , \dots , n _ { i } \}$ , and $n _ { i }$ denotes the number of successful code snippets for the reference code $q _ { i }$ . Through this process, we collected 2,105 successful CUDA code snippets. Now we have collected the dataset $D = \{ ( q _ { i } , \{ d _ { i , j } \} _ { j = 1 } ^ { n _ { i } } ) \} _ { i }$ .

The collected dataset $D$ is used to finetune the fundation model. The instruction to the model is the same as the prompt for dataset generation, where the reference code $q _ { i }$ is included in the instruction and the model is asked to generate an improved version. The model is trained to predict each token in $d _ { i , j }$ given the instruction.

# 2.3 Self-supervised Learning

Now we are presented with the finetuned model after the SFT stage, where the model can potentially generate better CUDA code with higher success rates than the original model without finetuning. We wish to further improve the model’s ability to generate successful CUDA code by exposing it to more code snippets generated by itself.

We achieve this iteratively by sampling CUDA code from the model, evaluating it for executability and correctness, removing the unsuccessful trials and keeping the successful ones. Successful ones are batched and used to update the model parameters. Using the updated model, we repeat the process: generating code, evaluating it, and retraining the model. The psudo code for the algorithm is shown in Table 2.

The self-supervised learning strategy can be viewed as a special case of the REINFORCE algorithm [31], a typical policy gradient reinforcement learning method, where the reward is set to 1 for successful trials and 0 for unsuccessful trials, without applying any baseline. Interestingly, we find this adopted training strategy to be more stable than the REINFORCE variant with baseline applied. We conjecture that this stability arises because during the self-supervised learning stage, a significant proportion of generated instances remain unsuccessful. This approach avoids the potential instability caused by applying negative updates to unsuccessful samples when using a baseline.

It is worth noting that during the self-supervised learning stage, we focus exclusively on the executability and correctness of the generated code, without considering speed as a metric. This design choice reflects our primary objective of establishing reliable code generation before optimizing for performance.

# Data Augmentation Prompt — Used in Supervised fine-tuning

# Task for CUDA Optimization

You are an expert in CUDA programming and GPU kernel optimization. Now you’re tasked with developing a high-performance cuda implementation of Softmax. The implementation must:

• Produce identical results to the reference PyTorch implementation.   
• Demonstrate speed improvements on GPU.   
• Maintain stability for large input values.