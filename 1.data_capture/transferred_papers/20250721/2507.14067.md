# VLA-Mark: A cross modal watermark for large vision-language alignment models

Shuliang Liu1,2, Qi Zheng1,2, Jesse Jiaxi ${ \bf { X } } { \bf { u } } ^ { 3 }$ , Yibo $\mathbf { Y a n } ^ { 1 , 2 }$ , He Geng1,2 Aiwei Liu1,2, Peijie Jiang4, Jia $\mathbf { L i u } ^ { 4 }$ , Yik-Cheung $\mathbf { T a m } ^ { 5 }$ , Xuming $\mathbf { H } \mathbf { u } ^ { 1 , 2 * }$

1 The Hong Kong University of Science and Technology (Guangzhou) 2 The Hong Kong University of Science and Technology 3 University of Toronto 4 Ant Group, Alibaba 5 New York University Shanghai shulianglyo@gmail.com, xuminghu@hkust-gz.edu.cn

# Abstract

Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semanticcritical concepts vulnerable. We propose VLAMark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visualtextual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show $7 . 4 \%$ lower PPL and $2 6 . 6 \%$ higher BLEU than conventional methods, with nearperfect detection $( 9 8 . 8 \%$ AUC). The framework demonstrates $9 6 . 1 \%$ attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for qualitypreserving multimodal watermarking 1.

# 1 Introduction

The emergence of vision-language aligned multimodal large models (VLAMMs) has fundamentally transformed cross-modal content generation. Pioneering architectures like LLaVA (Liu et al., 2023) and Flamingo (Alayrac et al., 2022) establish joint embedding spaces through cross-modal attention mechanisms, enabling unprecedented visuallinguistic synergy. These models achieve stateof-the-art performance in vision-language tasks ranging from contextual image captioning to visual commonsense reasoning, with recent extensions like Mini-Gemini (Li et al., 2024b) demonstrating human-level multimodal comprehension. (Liu and Bu, 2024; Yoo et al., 2024) However, their rising capability to generate semantically coherent crossmodal content urgently demands robust solutions for intellectual property protection and content authenticity.

Embedding imperceptible yet detectable watermarks into LLM-generated outputs has emerged as a pivotal solution, yet existing techniques predominantly focus on unimodal scenarios. The pioneering "green list" partitioning (Kirchenbauer et al., 2023) establishes fundamental watermarking frameworks through vocabulary bias induction, while subsequent improvements like unbiased probability of two partitioned lists (Mao et al., 2024) and distribution-preserving strategies (Wu et al., 2024) enhance quality-robustness trade-offs in text generation. However, these approaches fail to address the unique challenges of multimodal generation where visual semantics critically guide textual outputs.

Current watermarking methodologies exhibit three critical limitations when applied to visionlanguage aligned generation. First, traditional text watermarking approaches like "green list" partitioning (Kirchenbauer et al., 2023) disrupt visionconditioned language generation by introducing vocabulary biases that contradict visual semantics - for instance, suppressing visually grounded entity mentions detected through region-based attention. Even advanced context-aware variants (Ren et al., 2023) fail to account for cross-modal dependencies established through vision-language projection layers in models like BLIP-2 (Li et al., 2023). Second, static watermark allocation strategies (Liang et al., 2024; Zhao et al., 2023) typically apply uniform injection intensities regardless of position-specific visual grounding strength, leading to disproportionate distortion of visually salient tokens. This limitation persists even in theoretically-grounded approaches (Huang et al., 2023) that optimize statistical trade-offs but ignore entropy variations during cross-modal generation. Third, current methods lack explicit mechanisms to protect vision-critical semantics under text-space attacks. Random vocabulary partitioning and uniform logit manipulation render key visual concepts (e.g., objects, scene descriptors) vulnerable to adversarial paraphrasing or synonym substitution. As shown in Fig. 1 (5), conventional watermarks indiscriminately boost nonsemantic tokens (green blocks) while leaving visually anchored phrases like "grassy trail" (light blue blocks) exposed to semantic erasure through token replacement attacks. This fundamentally undermines text-visual coherence and detection consistency.

We resolve these challenges through VLAMark, the first vision-language aligned watermarking framework that achieves crossmodally coordinated, quality-preserving watermark with excellent detectability and robustness via three innovations. First, extending beyond random vocabulary splitting, our Multiscale Semantic Saliency Metrics leverage visual semantics to guide green list selection through localized patch affinity (LPA), global semantic coherence (GSC), and cross-modal contextual salience (CCS). This aligns token partitioning with image content while maintaining zero training overhead. Second, our Entropy-Regulated Partition dynamically adjusts watermark intensity based on generation uncertainty and token criticality scores, prioritizing semantic preservation in low-entropy phases while enhancing watermark strength during highentropy generation. Third, we introduce SCT based Distribution Adjustment through vision-aligned token prioritization, where cross-modal embedding alignment and fused metrics establish hierarchical protection for Semantic Critical Tokens (SCTs) against textual perturbations.

Our contributions transcend prior art through three breakthroughs:

‚Ä¢ We pioneer the first text watermarking method for vision-language models, achieving crossmodal semantic guidance through native alignment mechanisms of VLA architectures, yielding $7 . 4 \%$ and $2 6 . 6 \%$ average improvement $( { \mathrm { P P L } } \downarrow$ and BLEU‚Üë) in textual quality with zero training overhead.

‚Ä¢ We develop an uncertainty-aware coordination mechanism that automatically adapts watermark intensity to logits entropy, breaking the preservation-detection trade-off by maintaining SOTA detection performance while enhancing generation quality.

‚Ä¢ Through dedicated SCT preservation, we establish hierarchical protection against Paraphrase, Synonym, Translate and more attacks, ensuring text-visual consistency under perturbations.

# 2 Methodology

Our VLA-Mark framework introduces a visionaligned watermarking method that identifies Semantic Critical Tokens (SCTs), linguistic units strongly grounded in visual semantics guided by cross-modal embedding alignment (Sec 2.1) and fused multiscale metrics (Sec 2.2). SCTs preserve text-visual coherence by anchoring key concepts (e.g., objects/scenes) while enabling entropy-regulated dynamic vocabulary partitioning (Sec 2.4): low-entropy contexts prioritize SCT retention for semantic fidelity, whereas high-entropy phases emphasize watermark strength. The method further adjusts token distributions through watermarked logit manipulation (Sec 2.5). This approach pioneers visual semantics as the foundation for watermark injection, contrasting traditional textonly statistical strategies, as is illustraed in Fig. 1. For more theoretical analysis of each part, please refer to Appendix C.

# 2.1 Cross-Modal Aligned Embedding

As demonstrated in prior research, VisionLanguage Alignment (VLA) models like LLaVA (Liu et al., 2023) employ a shared semantic mapping strategy where visual embeddings are projected into the text embedding space.

Given a textual instruction $X _ { q }$ and visual input $X _ { v }$ , such models utilize parallel encoding streams to process multimodal inputs. The vision encoder (e.g., SigLIP (Zhai et al., 2023) or ViT-L/14 (Radford et al., 2021)) generates spatial-visual features through:

$$
\begin{array} { r } { \mathbf { Z } _ { v } = \mathrm { V i s E n c } ( X _ { v } ) = [ \mathbf { z } _ { \mathrm { c l s } } ; \mathbf { z } _ { \mathrm { 1 } } , . . . , \mathbf { z } _ { P } ] , } \end{array}
$$

where $\mathbf { Z } _ { v } \in \mathbb { R } ^ { ( P + 1 ) \times d _ { v } }$ and $P$ indicates the total number of image patch tokens augmented with a global [CLS] token. The subsequent alignment phase employs a trainable projection module $f _ { \theta } ( \cdot )$

[CLS] Low Entropy   
H! Oo00 „ÄÇ„ÄÇ¬∑¬∑¬∑¬∑¬∑ Sorted Vocabulary ùëâ‚àó   
H# O Ôºö LPA GSC CCS Semantic Critical Tokens Original Logits Distribution Low Entropy ùëâ‚àó Partition 1 1. Cross Modal Aligned Embedding Patch Tokens ùíÅ# Linguistic Tokens Traditional Watermark Vision ‚ñ°‚ñ°‚ñ°‚ñ°‚ñ° H# 8 Encoder Projection ùëì! ‚Ä¶ ‚Ä¶ High Entropy ùëâ‚àó Partition VLA-Mark Look Up doood H" Oo0oo LLM High Entropy Embedding LLM Inference Winhatthis dpiecstcurrieb?ed Vision Input ùëã# Text Query ùëã" VLA-Mark   
This image shows three (a group of) people walking on a grassy trail (the grass) in a scenic outdoor environment. The landscape includes green fields, trees, and distant   
mountains (scenery). It appears to be a bright, sunny (nice) day with clear skies. The people are walking away from the camera, possibly enjoying a hike (wander). Response

, implemented as MLP (Liu et al., 2024a) or generation adaptor (Chen et al., 2025), to bridge the dimensional gap between modalities:

$$
\mathbf { H } _ { v } = f _ { \theta } ( \mathbf { Z } _ { v } ) ,
$$

where $f _ { \theta }$ denotes parametric transformation that enables cross-modal compatibility while retaining original information patterns, so we get ${ \bf { H } } _ { v } \in  \ L ~$ R(P+1)√ód . LLMs (e.g., Vicuna (Chiang et al., 2023)) first tokenize input text of length $S$ and then retrieve text embeddings $\mathbf { H } _ { q } \ \in \ \mathbb { R } ^ { S \times d }$ for LLM inference by querying the pretrained token embedding table, commonly referred to as the Vocabulary $\nu$ . We construct an embedding matrix $\mathbf { H } _ { L }$ by removing non-linguistic elements such as symbols and numbers from $\nu$ , where $L$ denotes the number of linguistic tokens in the vocabulary. Then we use $\mathbf { H } _ { v }$ and $\mathbf { H } _ { L }$ in the following modules to find the SCT to guided $\nu$ partitioning for watermark.

# 2.2 Multiscale Semantic Saliency Metrics

The $l$ -th token embedding in $\mathbf { H } _ { L }$ is denoted as h(Ll . We propose three complementary metrics to evaluate semantic criticality of linguistic tokens from orthogonal perspectives:

1. Localized Patch Affinity (LPA) quantifies region-specific importance by identifying the most

relevant visual patch:

$$
\psi _ { \mathrm { L P A } } ( l ) = \operatorname* { m a x } _ { 1 \leq p \leq P } \frac { { \bf h } _ { v } ^ { ( p ) } \cdot { \bf h } _ { L } ^ { ( l ) } } { \Vert { \bf h } _ { v } ^ { ( p ) } \Vert \Vert { \bf h } _ { L } ^ { ( l ) } \Vert } .
$$

Role: LPA captures fine-grained visual grounding by measuring the maximum alignment between a text token and individual image regions. This is critical for detecting object-centric tokens (e.g., "grassy trail", "mountain") that strongly correlate with localized visual patterns. However, it may underestimate tokens with diffuse visual associations (e.g., "park", "crowded") that judged by the whole image.

2. Global Semantic Coherence (GSC) measures holistic alignment with the entire visual scene:

$$
\psi _ { \mathrm { G S C } } ( l ) = \frac { \mathbf { h } _ { v } ^ { ( \mathrm { c l s } ) } \cdot \mathbf { h } _ { L } ^ { ( l ) } } { \| \mathbf { h } _ { v } ^ { ( \mathrm { c l s } ) } \| \| \mathbf { h } _ { L } ^ { ( l ) } \| } .
$$

Role: GSC evaluates scene-level consistency by comparing text tokens to the global visual representation ([CLS] token). It prioritizes tokens that summarize the scene (e.g., "sunny", "hike") or anchor high-level semantics. However, global pooling may dilute localized but critical details come from certain patches (e.g., "broken" in a damaged object).

3. Cross-Modal Contextual Salience (CCS) aggregates multi-region visual relevance through

attention weights:

$$
\psi _ { \mathrm { C C S } } ( l ) = \sum _ { p = 1 } ^ { P } \frac { \exp ( \mathbf { h } _ { v } ^ { ( p ) } \cdot \mathbf { h } _ { L } ^ { ( l ) } ) } { \sum _ { p ^ { \prime } } \exp ( \mathbf { h } _ { v } ^ { ( p ^ { \prime } ) } \cdot \mathbf { h } _ { L } ^ { ( l ) } ) } . \frac { \mathbf { h } _ { v } ^ { ( p ) } \cdot \mathbf { h } _ { L } ^ { ( l ) } } { \| \mathbf { h } _ { v } ^ { ( p ) } \| \| \mathbf { h } _ { L } ^ { ( l ) } \| } .
$$

Role: CCS provides context-aware grounding by softly attending to all visual patches. It complements LPA by capturing distributed visual associations (e.g., "three people" involving multi patches) and mitigates GSC‚Äôs over-smoothing via spatial sensitivity.

# 2.3 Fused Metric Guided Vocabulary

We perform min-max normalization for crossmetric comparability:

$$
\psi _ { k } ^ { \mathrm { n o r m } } ( l ) = \frac { \psi _ { k } ( l ) - \mathrm { m i n } _ { l ^ { \prime } \in L } \psi _ { k } ( l ^ { \prime } ) } { \mathrm { m a x } _ { l ^ { \prime } \in L } \psi _ { k } ( l ^ { \prime } ) - \mathrm { m i n } _ { l ^ { \prime } \in L } \psi _ { k } ( l ^ { \prime } ) } ,
$$

where $k \in \{ \mathrm { L P A } , \mathrm { G S C } , \mathrm { C C S } \}$ , $\mathrm { m i n } _ { l ^ { \prime } \in \mathcal { V } } \psi _ { k } ( l ^ { \prime } )$ and $\operatorname* { m a x } _ { l ^ { \prime } \in \mathcal { V } } \psi _ { k } ( l ^ { \prime } )$ denote the minimum and maximum values of metric $k$ across the entire linguistic embedding $\mathbf { H } _ { L }$ . This normalization preserves relative rankings while constraining values to [0, 1].

The fusion of LPA, GSC, and CCS establishes a normalized hierarchical semantic assessment:

$$
\Phi ( l ) = \sum _ { k } \psi _ { k } ^ { \mathrm { n o r m } } ( l ) .
$$

Prioritized vocabulary ordering follows:

$$
\gamma ^ { * } = \operatorname { a r g s o r t } _ { l \in \mathcal { V } } \Phi ( l ) \Rightarrow ( w ^ { ( 1 ) } , . . . , w ^ { ( L ) } ) ,
$$

where $\{ w ^ { ( l ) } \} _ { l = 1 } ^ { L }$ is the sorted elements of ${ \bf { H } } _ { L } = { \bf { \Psi } }$ $\{ \mathbf { h } _ { L } ^ { ( l ) } \} _ { l = 1 } ^ { L }$ . The fusion mechanism achieves three synergistic effects: (1) Local-global synergy balances LPA‚Äôs regional sensitivity with GSC‚Äôs scene abstraction, (2) Attention redundancy via CCS compensates for LPA‚Äôs over-localization through distributed patch integration, and (3) Error robustness emerges from metric complementarity ‚Äì high CCS scores validate ambiguous signals (e.g., multiregion actions) through weak response aggregation. This fusion automatically prioritizes semantic patterns via LPA, GSC, and CCS without manual tuning.

# 2.4 Entropy-Regulated Partition

The output of LLM at each moment is determined by all preceding tokens, and at each time step $t$ , we can obtain predicted probability distribution:

$$
\mathbf { p } t = \mathrm { s o f t m a x } \left( \operatorname { L L M } ( \mathbf { h } _ { 1 : t - 1 } , \mathbf { H } _ { v } , \mathbf { H } _ { q } ) \right) ,
$$

where $\mathbf { p } _ { t } \in \mathbb { R } ^ { L }$ . To enhance watermark robustness while maintaining text quality, we propose an entropy-adaptive watermarking scheme that dynamically adjusts token partitioning based on prediction uncertainty. For each token position $t$ with $\mathbf { p } _ { t }$ , we calculate:

$$
\mathcal { H } _ { t } = - \sum _ { l = 1 } ^ { L } \hat { p } _ { t } ^ { ( l ) } \log \hat { p } _ { t } ^ { ( l ) } , \quad \hat { p } _ { t } ^ { ( l ) } = \frac { \mathbf { p } _ { t } ^ { ( l ) } + \boldsymbol { \epsilon } } { 1 + L \boldsymbol { \epsilon } } ,
$$

where $\epsilon = 1 0 ^ { - 8 }$ prevents numerical instability and $L \epsilon$ ensures the sum of $\hat { p } _ { t } ^ { ( l ) }$ is still 1. The normalized entropy, which quantifies the "decision difficulty" at each generation step is then determined by:

$$
\mathcal { H } _ { \mathrm { { n o r m } } } = \frac { H _ { t } } { H _ { \mathrm { { m a x } } } } = \frac { H _ { t } } { l o g L } ,
$$

where $H _ { m a x } = l o g L$ is proved in Appendix B. The Semantic Critical Tokens ratio $\eta _ { t }$ and the dynamic green list ratio $\gamma _ { t }$ follows:

$$
\begin{array} { r } { \eta _ { t } = \alpha ( 1 - \mathcal { H } _ { \mathrm { n o r m } } ) , } \\ { \gamma _ { t } = \gamma - \eta _ { t } , } \end{array}
$$

where hyper-parameter $\alpha \in \ [ 0 . 0 1 , 0 . 1 ]$ controls the base Semantic Critical Tokens proportion, thus $\eta _ { t } \in [ 0 , \alpha ) , \gamma \in [ \alpha , 1 )$ and $\gamma _ { t } \in ( 0 , 1 - \alpha )$ . The vocabulary partition construction follows:

$$
\begin{array} { r l } & { \quad \mathcal { G } _ { t } ^ { \mathrm { S C T } } = \{ w ^ { ( 1 ) } , . . . , w ^ { ( \lfloor \eta _ { t } L \rfloor ) } \} , } \\ & { \quad \mathcal { G } _ { t } ^ { \mathrm { G R E E N } } = \operatorname { S a m p l e } \Big ( \mathcal { V } ^ { * } \setminus ( \mathcal { G } _ { t } ^ { \mathrm { S C T } } ) \Big ) , } \\ & { \quad \quad \mathcal { R } _ { t } = \mathcal { V } ^ { * } \setminus \big ( \mathcal { G } _ { t } ^ { \mathrm { S C T } } \cup \mathcal { G } _ { t } ^ { \mathrm { G R E E N } } \big ) . } \end{array}
$$

The sample strategy of selecting $\mathcal { G } _ { t } ^ { \mathrm { G R E E N } }$ here is to generate random seeds according to the $h _ { t - 1 }$ token and randomly sample $\gamma _ { t }$ tokens from $\mathcal { V } ^ { * } \setminus ( \mathcal { G } _ { t } ^ { \mathrm { S C T } } )$ . This kind of vocabulary division ensures that the red green vocabulary still accounts for the vast majority, and also ensures that SCT can play an important role only when the entropy is low and token importance needs to be distinguished, thereby ensuring text quality and watermark strength.

# 2.5 SCT based Distribution Adjustment

We reformulate the watermark injection through logit-space manipulation, preserving the semanticcritical tokens (SCT) while introducing detectable biases. Let $\mathcal { G } _ { t } ~ = ~ \mathcal { G } _ { t } ^ { \mathrm { S C T } } \cup \mathcal { G } _ { t } ^ { \mathrm { G R E E N } }$ denote the union of SCTs and sampled green list. The watermarked probability distribution is computed following Kirchenbauer et al. (2023) as:

$$
p _ { t } ^ { ( k ) } = \left\{ \begin{array} { l l } { \frac { \exp ( p _ { t } ^ { ( k ) } + \delta ) } { \sum _ { i \in \mathcal { R } _ { t } } \exp ( p _ { t } ^ { ( i ) } ) + \sum _ { i \in \mathcal { G } _ { t } } \exp ( p _ { t } ^ { ( i ) } + \delta ) } , } & { k \in \mathcal { G } _ { t } } \\ { \frac { \exp ( p _ { t } ^ { ( k ) } ) } { \sum _ { i \in \mathcal { R } _ { t } } \exp ( p _ { t } ^ { ( i ) } ) + \sum _ { i \in \mathcal { G } _ { t } } \exp ( p _ { t } ^ { ( i ) } + \delta ) } , } & { k \in \mathcal { R } _ { t } } \end{array} \right.
$$

where pt(k) denotes the original logit value for token $k$ at step $t$ , and $\delta > 0$ controls the watermark intensity. This formulation applies: 1. Logit boosting $( + \delta )$ for $\mathcal { G } _ { t }$ tokens $\mathrm { ( S C T + }$ green list) 2. Neutral treatment for $\mathcal { R } _ { t }$ tokens (remaining vocabulary).

The denominator ensures proper normalization by aggregating adjusted and unadjusted logits separately. The final token selection follows:

$$
w _ { t } \sim \mathrm { C a t e g o r i c a l } \left( \{ p _ { t } ^ { ( k ) } \} _ { k = 1 } ^ { L } \right) .
$$

This mechanism creates statistically detectable signatures in $\mathcal { G } _ { t }$ tokens while maintaining the semantic integrity of SCT tokens owing to the guaranteed logit boosting in SCTs, the context-sensitive enhancement in green list tokens and the original distribution patterns in $\mathcal { R } _ { t }$ . The watermark detection process is followed as (Kirchenbauer et al., 2023) thanks to the similar vocabulary partition.

# 3 Experiments

Our experiments comprehensively assessed VLAMark‚Äôs performance on detection accuracy, text quality maintenance, and robustness across four multimodal language models using the AMBER (Wang et al., 2023) dataset. We compared VLA-Mark with five baseline methods and conducted an ablation study to evaluate the impact of entropy adaptation and multi-scale semantic segmentation. Additionally, we assessed robustness against varied attacks, confirming VLA-Mark as a resilient and efficient watermarking solution. The latency overhead of the algorithm, additional results on attack robustness, and evaluations on more datasets can be found in the Appendix D.

# 3.1 Experiment Setup

Backbone models and datasets. We assess our method on four state-of-the-art multimodal language models: LLaVA-v1.5 (Liu et al., 2024a,b), LLaVA-Next (Li et al., 2024a), Qwen2-VL (Wang et al., 2024), and DeepSeek-VL (Lu et al., 2024a), utilizing their corresponding vision models for image feature extraction. Performance is evaluated using the AMBER (Wang et al., 2023) dataset, tailored for image description tasks.

Baselines approaches. We compare our approach with five baselines: KGW (Kirchenbauer et al., 2023), SWEET (Lee et al., 2023), EWD (Lu et al., 2024b), unbiased (Hu et al., 2023), and DiP (Wu et al., 2023), chosen for their focus on detection performance and text quality. Implementations are facilitated by the MarkLLM (Pan et al., 2024) repository.

Evaluation metrics Our evaluation spans detection performance (AUC and accuracy), text quality (PPL and BLEU), semantic alignment (STS and BertScore), and robustness against A1 attack (alter text through word additions, removals, or substitutions) and A2 attacks (translate and paraphrase text using LLM) proposed by Lau et al. (2024).

# 3.2 Results

# 3.2.1 Watermark

Table 1 provides a detailed performance comparison of VLA-Mark with several baseline methods across four multimodal language models. The evaluation metrics include AUC, Accuracy, and PPL, which measure watermark detection effectiveness and text quality. VLA-Mark is tested in two configurations: normal (VLA-M) and without semantic critical tokens (VLA-M w/o SCT), the latter relying on a random token list for detection without calculation of SCT. The length of all responses is limited at 200 tokens.

The results highlight the performance of VLAMark. VLA-Mark achieves AUROC above $9 9 . 8 \%$ and accuracy above $9 8 . 1 \%$ in the three models, indicating high detection accuracy. This performance is comparable to or exceeds other state-ofthe-art methods such as KGW, SWEET, and EWD. Notably, the PPL metric shows that VLA-Mark outperforms all baseline methods, highlighting its ability to maintain high-quality text while embedding watermarks. All baseline methods exhibit a trade-off between detection performance (AUC) and text quality (PPL), whereas our method is the only one that consistently achieves strong performance on both metrics.These results substantiate VLA-Mark‚Äôs efficacy in balancing high detection precision with high-quality text across a range of multimodal language models.

Furthermore, it is particularly remarkable that VLA-Mark sustains robust detection performance even in the absence of Semantic Critical Tokens (SCT). Specifically, the VLA-Mark variant without

Table 1: Performance comparison of VLA-M and baseline methods across different multimodal language models in metrics AUC, Accuracy, and Perplexity. Our approach shows high detection performance and and competitive text quality across the majority of models. Cells highlighted in green denote superior performance, whereas red cells signify underperformance. The notation "w/o SCT" indicates results without using Semantic Critical Tokens. (See Appendix D.6 for additional performance on MS COCO dataset.)   

<html><body><table><tr><td rowspan="2"></td><td colspan="3">LLaVA-v1.5</td><td colspan="3">LLaVA-Next</td><td colspan="3">Qwen2-VL</td><td colspan="3">DeepSeek-VL</td></tr><tr><td>AUC</td><td>ACC</td><td>PPL</td><td>AUC</td><td>ACC</td><td>PPL</td><td>AUC</td><td>ACC</td><td>PPL</td><td>AUC</td><td>ACC</td><td>PPL</td></tr><tr><td>KGW</td><td>99.98</td><td>99.55</td><td>6.21</td><td>99.99</td><td>99.80</td><td>6.04</td><td>99.99</td><td>99.60</td><td>5.27</td><td>99.81</td><td>98.00</td><td>6.99</td></tr><tr><td>EWD</td><td>99.99</td><td>99.90</td><td>6.51</td><td>100.0</td><td>100.0</td><td>6.05</td><td>100.0</td><td>100.0</td><td>5.24</td><td>99.99</td><td>99.80</td><td>7.00</td></tr><tr><td>SWEET</td><td>99.99</td><td>99.95</td><td>6.30</td><td>100.0</td><td>100.0</td><td>6.04</td><td>100.0</td><td>100.0</td><td>5.17</td><td>99.92</td><td>99.05</td><td>7.00</td></tr><tr><td>unbiased</td><td>88.27</td><td>80.87</td><td>6.05</td><td>92.54</td><td>85.20</td><td>5.56</td><td>96.99</td><td>91.13</td><td>5.00</td><td>79.65</td><td>66.98</td><td>6.18</td></tr><tr><td>DiP</td><td>88.58</td><td>80.82</td><td>6.03</td><td>92.66</td><td>85.60</td><td>5.57</td><td>97.25</td><td>91.13</td><td>5.02</td><td>79.60</td><td>67.33</td><td>6.17</td></tr><tr><td>VLA-M</td><td>99.99</td><td>99.80</td><td>4.84</td><td>99.95</td><td>98.95</td><td>5.32</td><td>99.89</td><td>98.43</td><td>4.97</td><td>97.36</td><td>92.72</td><td>5.73</td></tr><tr><td>w/o SCT</td><td>99.99</td><td>99.75</td><td>=</td><td>96.08</td><td>89.39</td><td></td><td>99.76</td><td>98.45</td><td></td><td>94.52</td><td>90.78</td><td></td></tr></table></body></html>

SCT (w/o SCT) attains noteworthy AUROC scores above $9 9 . 7 \%$ for both LLaVA-v1.5 and Qwen2-VL models. For Accuracy, VLA-Mark (w/o SCT) delivers commendable results above $9 8 . 4 \%$ for models mentioned above. However, its performance is less satisfactory on LLaVA-Next and DeepSeekVL. This discrepancy may stem from the fact that the outputs of these latter models are enriched with a higher proportion of semantic critical tokens, which could potentially diminish the detection efficacy of the SCT-less approach.The outcomes underscore our method‚Äôs versatility and robustness across diverse scenarios. The capability of reliable detection without SCT enhances our watermarking technique‚Äôs applicability by eliminating the requirement for original input during detection. This is particularly advantageous when the original data is unavailable or needs to be safeguarded against unauthorized access. To further validate the generalizability of our approach, we evaluated VLA-Mark on the MS COCO captioning benchmark across multiple VLA models, with detailed results provided in Appendix D.6.

# 3.2.2 Ablation Study

Table 2: Ablation study comparing the full VLA-M algorithm (None) to its variants lacking specific components. The subsequent columns indicate the algorithm‚Äôs performance after removing a specific component.   

<html><body><table><tr><td>Ablation</td><td>None</td><td>Entropy</td><td>LPA</td><td>GSC</td><td>CCS</td></tr><tr><td>PPL(‚Üì)</td><td>4.84</td><td>6.14</td><td>5.61</td><td>5.02</td><td>5.37</td></tr><tr><td>STS</td><td>92.13</td><td>90.89</td><td>91.98</td><td>91.02</td><td>91.88</td></tr><tr><td>BertScore</td><td>91.13</td><td>90.75</td><td>90.96</td><td>88.63</td><td>90.91</td></tr></table></body></html>

Our ablation study, detailed in Table 2, validates the critical roles of individual components in VLA

Mark‚Äôs design. Removing Localized Patch Affinity (LPA) leads to a significant $1 5 . 9 \%$ increase in perplexity (PPL: 5.61 vs. 4.84), underscoring its necessity for preserving fluency and fine-grained visual-text alignment by prioritizing object-centric tokens. Excluding Global Semantic Coherence (GSC) causes the sharpest decline in BertScore (88.63 vs. 91.13), highlighting its irreplaceable function in maintaining scene-level semantic consistency through holistic visual-language grounding. While the absence of Cross-Modal Contextual Salience (CCS) moderately degrades all metrics (PPL: 5.37, STS: 91.88, BertScore: 90.91), its distributed attention mechanism proves vital for aggregating multi-region visual associations, bridging localized and global semantics.

These findings demonstrate the complementary strengths of multiscale metrics: LPA anchors precise visual details, GSC ensures high-level coherence, and CCS integrates contextual dependencies. Combined with entropy-regulated partitioning, the framework achieves an optimal equilibrium‚Äîpreserving multimodal fidelity while embedding robust watermarks. The full model‚Äôs superior performance across all metrics (PPL: 4.84, STS: 92.13, BertScore: 91.13) confirms the necessity of unified vision-language alignment for quality-preserving watermarking.

# 3.2.3 Hyperparameter analysis

As shown in Table 3, the SCT ratio controller $\alpha$ exhibits a clear non-monotonic relationship with generation quality. Performance peaks at $\scriptstyle \alpha = 0 . 0 2 5$ , achieving optimal balance with the lowest perplexity (4.84) and highest semantic similarity (92.13). Below or above this threshold, insufficient SCT allocation degrades both fluency and semantic alignment, confirming that weak semantic token emphasis compromises multimodal fidelity. The default $\scriptstyle \alpha = 0 . 0 2 5$ optimally complements VLA-M‚Äôs multiscale components by dynamically balancing local fluency and global semantic preservation. Even under the least favorable choice of $\alpha$ , the performance of PPL remains comparable to or better than that of KGW, with limited variation, demonstrating the robustness of our method to hyperparameter selection.

Table 3: Ablation study on the hyper-parameter $\alpha$ controlling Semantic Critical Tokens (SCT) ratio. Results show $\scriptstyle \alpha = 0 . 0 2 { \bar { 5 } }$ achieves optimal balance between text quality (PPL) and watermark metrics (STS, BertScore).   

<html><body><table><tr><td>Ablation of Œ±</td><td>0.01</td><td>0.015</td><td>0.025</td><td>0.05</td><td>0.1</td></tr><tr><td>PPL(‚Üì)</td><td>6.23</td><td>5.86</td><td>4.84</td><td>5.71</td><td>5.91</td></tr><tr><td>STS</td><td>85.15</td><td>90.71</td><td>92.13</td><td>91.83</td><td>90.76</td></tr><tr><td>BertScore</td><td>91.48</td><td>94.05</td><td>91.13</td><td>94.27</td><td>94.16</td></tr></table></body></html>

# 3.2.4 Text quality maintenance

![](images/217f870a9a1b2608a2470cfb5eef7033c3e19d0abaf749bd958fa2886cc550c5.jpg)  
Figure 2: Left: Boxplots of perplexity scores for different watermarking methods. Right: Average BLEU scores over increasing token lengths. Our approach maintains lower perplexity with competitive BLEU performance even as generation length grows.

In Figure 2 (left), we observe that our proposed approach exhibits lower median perplexity compared to other watermarking methods, indicating that it remains closer to the natural language distribution. This stems from our ‚Äúsemantic critical tokens,‚Äù which preserve core meanings and reduce unnecessary perturbations in high-salience tokens. In Figure 2 (right), average BLEU scores show that while all methods degrade as token length increases, our dynamic partitioning strategy and SCT protection help maintain relatively higher BLEU. By boosting tokens critical to the overall semantics, we minimize the distortion of fluency and coherence, leading to more faithful long generations.

# 3.3 Attack

In our robustness experiments, we tested VLAMark against attacks A1 and A2 as defined by Lau et al. (2024). Attack type A1 encompasses random word insertions, deletions, and synonym substitutions, with $5 \%$ of the text undergoing alteration. Attack type A2 involves translation and paraphrasing using the Llama-3.1 model. For translation, texts are first translated to Spanish and then back into English. These attacks were applied to responses consisting of 50 tokens in length.

Figure 3 illustrates VLA-Mark‚Äôs superior resilience, maintaining high AUC scores under all attacks. Notably, VLA-Mark sustains an AUC of $9 6 . 9 6 \%$ under A1 and only experiences minimal drops of $2 . 9 0 \%$ and $2 . 4 7 \%$ during A2 translation and paraphrasing attacks, respectively. This contrasts with significant performance declines in DiP ( $6 9 . 7 8 \% - 7 7 . 5 7 \%$ AUC) and the unbiased method $7 0 . 0 3 \% - 7 7 . 3 5 \%$ AUC) during paraphrasing. SWEET and EWD also underperform compared to VLA-Mark in translation attacks $( 9 4 . 1 0 \%$ - $9 4 . 6 8 \%$ vs. $9 5 . 0 4 \%$ AUC). See Appendix D.3 for relative performance drop comparison. Appendix D.5 provides additional robustness evaluations covering novel adversarial attack types.

VLA-Mark‚Äôs robustness is attributed to its entropy-adaptive mechanism and multiscale semantic guidance, which effectively counter lexical and structural distortions, especially in A2 attacks. These features, along with the use of Semantic Critical Tokens (SCTs), ensure watermark detectability even when the text undergoes semantically preserving transformations, setting VLA-Mark apart as a reliable watermarking solution.

# 4 Related Work

Our work advances three interconnected research frontiers: text watermarking foundations, robustness against adversarial attacks, and visionlanguage aligned generation paradigms.

# 4.1 Text Watermarking Fundamentals

Contemporary watermarking techniques predominantly focus on unimodal text generation. The pioneering "green list" paradigm (Kirchenbauer et al., 2023) partitions vocabulary through hash-based promotion, while entropy-aware variants (Mao et al., 2024) modulate injection strength probabilistically. Distribution-preserving approaches (Wu et al., 2024) maintain statistical fidelity through reweighting yet neglect semantic grounding. However, such unimodal designs fundamentally conflict with vision-conditioned generation: random vocabulary partitioning disrupts visual-semantic alignment by suppressing image-grounded tokens

KGW EWD SWEET 1.0 1.0 1.0   
0.8 0.8 0.8   
0.46 Unattacked (98.75) 0.6 Unattacked (99.57) 0.6 Unattacked (99.06) Insert Attack (97.54) Insert Attack (98.63) Insert Attack (97.34)   
0.2 SDyelneotneyAmttaActtka(c9k8(.9170.)92) 0.4 SDyelneotneyAmttaActtka(c9k8(.983.)92) 0.4 SDyelneotneyAmttaActtka(c9k7(.98.)15) Translate Attack (93.63) 0.2 Translate Attack (94.10) 0.2 Translate Attack (94.68) Paraphrase Attack (90.53) Paraphrase Attack (94.90) Paraphrase Attack (94.08) 0.0 0.0 1 0.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 unbiased DiP VLA-M 1.0 1.0 1.0   
0.8 0.8 0.8   
0.46 Unattacked (82.52) 0.6 Unattacked (82.76) 0.6 Unattacked (97.51) Insert Attack (74.51) Insert Attack (75.04) Insert Attack (96.64)   
0.2 SDyelneotneyAmttaActtka(c7k7(.757.)35) Translate Attack (77.16) 0.4 0.2 SDyelneotneyAmttaActtka(c7k6(.7973.)57) Translate Attack (77.15) 0.4 0.2 SDyelneotneyAmttaActtka(c9k7(.9269.)96) Translate Attack (95.04) Paraphrase Attack (70.03) Paraphrase Attack (69.78) Paraphrase Attack (94.61) 0.0 0.0 L-- 0.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate False Positive Rate False Positive Rate

(He et al., 2024), while static allocation strategies (Liang et al., 2024) fail to adapt to cross-modal entropy variations (Huang et al., 2023). Recent benchmarks (Qiu et al., 2024) reveal $41 \%$ robustness degradation when deploying these methods in multimodal contexts, underscoring the necessity for vision-aligned watermark formulation.

# 4.2 Robustness Challenges and Attacks

Emerging adversarial attacks expose vulnerabilities through multimodal exploitation. (Rastogi and Pruthi, 2024) demonstrates $6 3 \%$ efficacy gain via black-box analysis-driven paraphrases, while (He et al., 2024) reveals cross-lingual leakage during translation. Frameworks like DE-MARK (Chen et al., 2024) remove watermarks via probabilistic n-gram erasure. Existing defenses remain unimodally confined‚Äîsemantic preservation (Ren et al., 2023) enhances robustness but cannot counter cross-modal attacks that jointly manipulate visiontext interdependencies. Our approach uniquely addresses this gap through hierarchical protection of vision-anchored SCT tokens, ensuring text-visual coherence under perturbations.

# 4.3 Vision-Language Aligned Architectures

State-of-the-art VLAMMs like LLaVA (Liu et al., 2023) and BLIP-2 (Li et al., 2023) establish cross-modal fusion through architectural innovations‚Äîgated cross-attention in Flamingo (Alayrac et al., 2022) enables visual reasoning, while CogVLM2 (Hong et al., 2024) leverages temporal grounding for scene understanding. Yet these models lack native authentication mechanisms, rendering generated content susceptible to adversarial attacks (Rastogi and Pruthi, 2024). Recent efforts (Yoo et al., 2024) incorporate entropy adaptation but neglect alignment layers critical for coordinated embedding. Our framework bridges this gap by explicitly integrating watermarking with cross-modal projection mechanisms and semantic fusion metrics‚Äîsecuring generation authenticity without architectural modification.

Our methodology synthesizes these advances through: (1) Visual-semantic vocabulary alignment supplanting random partitioning, (2) Entropyregulated intensity modulation synchronized with cross-modal saliency, and (3) Architectural synergy with vision-language fusion mechanisms‚Äîresolving inherent limitations across these research streams.

# 5 Conclusion

We present VLA-Mark, a vision-language aligned watermarking framework that harmonizes intellectual property protection with cross-modal semantic fidelity. By integrating multiscale visual-textual alignment metrics and entropy-regulated token partitioning, our method dynamically balances watermark detectability and semantic preservation. Experiments across four multimodal models demonstrate VLA-Mark‚Äôs superiority: near-perfect detection $( 9 8 . 8 \%$ AUC), $7 . 4 \%$ lower perplexity, and $9 6 . 1 \%$ robustness against paraphrasing and translation attacks. Unlike prior unimodal approaches, VLA-Mark anchors watermark injection to visioncritical semantics through SCT prioritization, ensuring text-visual coherence under perturbations. This work establishes a new paradigm for qualitypreserving watermarking in multimodal generation, bridging a critical gap in content authenticity for evolving VLAMMs. Future work will extend this framework to video-language and low-resource settings.

# Limitation

While VLA-Mark demonstrates robust watermarking capabilities, several limitations remain. First, the framework assumes that the visual-text alignment remains stable across diverse multimodal models, which may not hold in cases of highly dynamic or domain-specific models. Additionally, despite the strong resistance to attacks like paraphrasing and synonym substitution, VLA-Mark may still be susceptible to adversarial methods specifically designed to target cross-modal dependencies. Furthermore, although the method does not require model retraining, its reliance on entropy-sensitive watermark injection might introduce computational overhead in environments with limited resources (see Appendix D.1 and Appendix D.2). Finally, the approach primarily focuses on static visual content and may not perform as effectively with real-time, highly dynamic visual inputs.

Rui Wang. 2024. Can watermarks survive translation? on the cross-lingual consistency of text watermark for large language models. arXiv preprint arXiv:2402.14007.   
Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. 2024. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500.   
Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. 2023. Unbiased watermark for large language models. arXiv preprint arXiv:2310.10669.   
Baihe Huang, Hanlin Zhu, Banghua Zhu, Kannan Ramchandran, Michael I Jordan, Jason D Lee, and Jiantao Jiao. 2023. Towards optimal statistical watermarking. arXiv preprint arXiv:2312.07930.   
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. 2023. A watermark for large language models. In International Conference on Machine Learning, pages 17061‚Äì17084. PMLR.   
Gregory Kang Ruey Lau, Xinyuan Niu, Hieu Dao, Jiangwei Chen, Chuan-Sheng Foo, and Bryan Kian Hsiang Low. 2024. Waterfall: Framework for robust and scalable text watermarking and provenance for llms. arXiv preprint arXiv:2407.04411.   
Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, and Gunhee Kim. 2023. Who wrote this code? watermarking for code generation. arXiv preprint arXiv:2305.15060.   
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326.   
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 19730‚Äì19742. PMLR.   
Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. 2024b. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814.   
Yuqing Liang, Jiancheng Xiao, Wensheng Gan, and Philip S Yu. 2024. Watermarking techniques for large language models: A survey. arXiv preprint arXiv:2409.00089.   
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296‚Äì26306.