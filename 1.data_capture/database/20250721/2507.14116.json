{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14116",
    "link": "https://arxiv.org/abs/2507.14116",
    "pdf_link": "https://arxiv.org/pdf/2507.14116.pdf",
    "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
    "authors": [
        "Daniëlle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
    ],
    "categories": [
        "cs.ET",
        "cs.LG"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "LMU Munich",
        "SAP SE",
        "Aqarios GmbH"
    ],
    "paper_content": "# Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification\n\nDani¨elle Schuman Mark V. Seebode Tobias Rohe Maximilian Balthasar Mansky LMU Munich LMU Munich LMU Munich LMU Munich Munich, Germany Munich, Germany Munich, Germany Munich, Germany danielle.schuman@ifi.lmu.de M.Seebode@campus.lmu.de tobias.rohe@ifi.lmu.de maximilian-balthasar.mansky@ifi.lmu.de\n\nMichael Schroedl-Baumann SAP SE Walldorf, Germany michael.schroedl-baumann@sap.com\n\nJonas Stein   \nAqarios GmbH   \nMunich, Germany   \njonas.stein@aqarios.com   \nClaudia Linnhoff-Popien   \nLMU Munich   \nMunich, Germany   \nlinnhoff@ifi.lmu.de   \nFlorian Krellner   \nSAP SE   \nWalldorf, Germany   \nflorian.krellner@sap.com\n\nAbstract—Exploiting the fact that samples drawn from a quantum annealer inherently follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann Machines (QBMs) have gained increasing popularity in the quantum research community. While they harbor great promises for quantum speed-up, their usage currently stays a costly endeavor, as large amounts of QPU time are required to train them. This limits their applicability in the NISQ era. Following the idea of Noe\\` et al. [1], who tried to alleviate this cost by incorporating parallel quantum annealing into their unsupervised training of QBMs, this paper presents an improved version of parallel quantum annealing that we employ to train QBMs in a supervised setting. Saving qubits to encode the inputs, the latter setting allows us to test our approach on medical images from the MedMNIST data set [2], thereby moving closer to real-world applicability of the technology. Our experiments show that QBMs using our approach already achieve reasonable results, comparable to those of similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller numbers of epochs than these classical models. Our parallel annealing technique leads to a speed-up of almost $70 \\%$ compared to regular annealing-based BM executions.\n\nIndex Terms—Quantum Boltzmann Machines, Medical Image Classification, Parallel Quantum Annealing\n\n# I. INTRODUCTION\n\nMachine learning and more precisely deep learning based methods have proven to be effective for medical image analysis [3]–[5], and can, for example, be used to diagnose pneumonia from chest $\\mathbf { \\delta X }$ -rays [6].\n\nIn the quest for near-term, attainable quantum advantage in areas like this, we propose using Quantum Boltzmann Machines (QBMs) for medical image classification. QBMs were introduced in [7] and are the quantum analogue of classical Boltzmann Machines (BMs), a type of machine learning model introduced by Ackley, Hinton, et al. in the 1980s [8]–[11].\n\nAlthough BMs are powerful models, they have been superseded by deep neural networks mainly because BMs are notoriously difficult to train with available classical hardware [8], [12]. Therefore, current quantum research concentrates on replacing classical methods for evaluating the model states during training with quantum methods, such as Quantum Annealing (QA). Several recent work [13]–[19] has successfully used QA (or comparable techniques [15], [20]) in BMs trained to classify images, often finding advantages such as faster computation times [14], [16], less fluctuations in training accuracy [17], [20] or a smaller amount of training epochs that is needed [20]. Often, however, these approaches use rather small and simple data sets such as “Bars and Stripes” [17] or (potentially coarse-grained) MNIST images [14], [16], [18].\n\nTo our knowledge, QBMs using QA have so far not been used directly to classify medical images: In [20] a combination of a pre-trained Convolutional Neural Network (CNN) and a QBM was used to classify medical images, with the BM being trained by Simulated Annealing (SA) as a proxy for QA, while in [19] a combination of an auto-encoder and Deep Belief Network are used, where only the latter is pre-trained using a QA-based QBM.\n\nHowever, while QA-based QBMs seem like a promising approach for medical image classification, a lot of the preexisting works on QA-based QBMs (e.g. [7], [17], [20], [21]) do not use actual QA hardware results, as they report that training QA-based QBMs takes up prohibitively large amounts of expensive Quantum Processing Unit (QPU) time [20], [21]. This poses a problem for the near-term applicability of such QA-approaches.\n\nRecently, Noe\\` et al. [1] presented a solution to this type of problem in an unsupervised setting, where they used Parallel Quantum Annealing (PQA) [22] – a technique which embeds multiple independent problem instances in a single annealing cycle – to achieve a large decrease in required runtime. Following this idea, our approach to training a QA-based\n\nQBM in a supervised fashion tries to balance accuracy and efficiency by embedding different QBM problem instances isolated from each other onto the topology of the annealer’s QPU to minimize interference. The development of our approach follows the first four stages of a 5-stage structured pipeline for quantum software engineering tailored to NISQ and early post-NISQ era applications [23].\n\nThe remainder of this work is structured as follows: First, Sec. II gives some information about the workings of BMs, explains how they can be executed using QA, details the concept of PQA and subsequently introduces our approach of using improved PQA in supervised QBM training. We then briefly introduce CNNs, which will be used as a classical baseline in our experiments on two of the MedMNIST data sets [2]. Sec. III then introduces our employed data sets and performance metrics, and subsequently describes our hyperparameter optimization efforts and experimental results. Finally, our approach and results will be summarized in Sec. IV, which also addresses the limitations of our present paper and how these might be addressed in future work.\n\n# II. MODELS AND BACKGROUND\n\n# A. Boltzmann Machines for supervised learning\n\nA Boltzmann Machine (BM) [8]–[11] is an undirected stochastic neural network composed of $n$ neurons, which can be grouped into $n _ { v }$ visible units $\\boldsymbol { v }$ and (optionally) $n _ { h }$ hidden units $h$ . They can take the values 0 or 1 with a certain probability governed by a quadratic energy function [8]. Compare Fig. 1 for a visualization of such a network.\n\n![](images/e6326ab61d64fb8b17424fe606669c9169cacf7e2c3ef7e84087d83d6d8229d2.jpg)  \nFig. 1: Structure of a fully connected Boltzmann Machine. The network consists of visible units (blue and red) and hidden units (gray), with symmetric connections between all pairs of units. No distinction is made between layers; every node is connected to every other node. In supervised learning scenarios, some visible units (here in blue) can be used to represent the input data $d .$ , while others (in red, can also be multiple) can be used to represent the label $l$ .\n\nThe visible units are used to embed the input data points into the BM [7]. In our application, these data points consist of an input image $d$ and the corresponding label $l$ . In supervised learning, a part of the visible units (blue in Fig. 1), which we will refer to in the following as input units, will be used to encode the input data $d$ by a string of values $v _ { d }$ of length $n _ { d }$ . We will refer to the (multi-) set of all inputs from the data set encoded this way as $D _ { i n }$ . The remaining $n _ { l }$ visible units (red in Fig. 1), which we will refer to as label units, will be used to represent the label $l$ using an encoding $v _ { l } \\in \\{ 0 , 1 \\} ^ { n _ { l } }$ . The goal of training the BM is that, given only a particular input $d$ (without the corresponding label $l$ ) at inference time, the label units will assume values $v _ { l }$ matching the possible labels $l$ of this input with a probability closely corresponding to the frequency of the co-occurrence of the labels $l$ in question with this particular $d$ in the training data set [7]. This means that by drawing a few samples from the BM, the label $v _ { l }$ most frequently assumed by the label units can be used to classify the input $d$ . (Ideally, if the classification of the input can be performed unambiguously after training, the frequency of this most probable label occurring will be very close to $100 \\%$ .)\n\nThe probability of sampling a unit configuration $s \\in \\{ 0 , 1 \\} ^ { n }$ from the BM is governed by the Boltzmann distribution\n\n$$\nP _ { b , W } ( s ) = \\frac { 1 } { Z } \\exp \\left( - \\frac { E _ { b , W } ( s ) } { T } \\right)\n$$\n\nwith the normalization\n\n$$\nZ = \\sum _ { s \\in \\{ 0 , 1 \\} ^ { n } } \\exp \\left( - \\frac { E _ { b , W } ( s ) } { T } \\right)\n$$\n\nand energy function\n\n$$\nE _ { b , W } ( s ) = \\sum _ { i = 1 } ^ { n } b _ { i } s _ { i } - \\sum _ { i , j = 1 \\atop i < j } ^ { n } W _ { i j } s _ { i } s _ { j } .\n$$\n\nwhere the parameters $W _ { i j }$ and $b _ { i }$ are the BM’s weights and biases and $T$ is the so-called effective temperature, which can be used as a hyperparameter governing the Boltzmann distribution’s shape relative to the energy function’s values [8]: At lower effective temperatures, small differences in the energy values of states lead to more drastic differences in their probability to be sampled than at higher effective temperatures [8].\n\nThe parameters of the BM are adjusted during training in a way that maximizes the likelihood of the (encoded) training data $( v _ { d } , v _ { l } ) \\in D$ (where $D$ is the encoded data set) to occur when sampling from the BM [7]. More precisely, following Amin et al. [7], we are using a discriminative training procedure that maximizes the likelihood of $v _ { l }$ to be assumed by the label units when the input units are fixed, or clamped, to $\\boldsymbol { v } _ { d }$ . This can be achieved by using the (average) negative log-likelihood of the labels $v _ { l }$ as a loss function to be minimized:\n\n$$\nb , W \\mapsto { \\mathcal { L } } ( b , W ) = - \\sum _ { ( v _ { d } , v _ { l } ) \\in D } \\log P _ { b , W } ( v _ { l } | v _ { d } )\n$$\n\nHere,\n\n$$\nP _ { b , W } ( v _ { l } | v _ { d } ) = \\frac { 1 } { Z _ { v _ { d } } } \\sum _ { h \\in \\{ 0 , 1 \\} ^ { n _ { h } } } \\exp \\left( - \\frac { E _ { b , W } ( s ) } { T } \\right)\n$$\n\nwith\n\n$$\nZ _ { v _ { d } } = \\sum _ { ( v _ { l } , h ) \\in \\{ 0 , 1 \\} ^ { ( n _ { l } + n _ { h } ) } } \\exp \\left( - \\frac { E _ { b , W } ( s ) } { T } \\right)\n$$\n\nis the probability of sampling the label units $v _ { l } \\in \\{ 0 , 1 \\} ^ { n _ { l } }$ when $v _ { d }$ is clamped to a specific input value [7]. The notation $( v _ { l } , h ) \\in \\{ 0 , 1 \\} ^ { ( \\overline { { n } } _ { l } + n _ { h } ) }$ denotes the concatenation of vectors $v _ { l }$ and $h$ which has the total length $n _ { l } + n _ { h }$ , while $\\boldsymbol { s } = \\left( v _ { d } , v _ { l } , h \\right)$ represents the concatenation of the vectors of all units as one vector of length $n$ . Since, in this style of learning, $v _ { d }$ is always clamped to an input value $d$ of a specific data point, the energy function $E _ { b , W } ( s )$ can be written as:\n\n$$\nE _ { b , W , v _ { d } } ( s \\setminus v _ { d } ) = \\sum _ { i = n _ { d } + 1 } ^ { n } b _ { i } ^ { d } s _ { i } \\quad - \\sum _ { \\stackrel { i , j = n _ { d } + 1 } { i < j } } ^ { n } W _ { i j } s _ { i } s _ { j }\n$$\n\nwhere\n\n$$\nb _ { i } ^ { d } = b _ { i } + \\sum _ { k = 1 } ^ { n _ { d } } W _ { i k } v _ { d k }\n$$\n\nacts as a bias on the on the remaining units $h$ and $v _ { l }$ [7]. (Here, the notation $s \\setminus v _ { d } = ( v _ { l } , h )$ is used to enhance readability.)\n\nThe standard technique to minimize $\\mathcal { L }$ is via gradient descent methods. The gradient is given by [7]\n\n$$\n\\partial _ { b , W } \\mathcal { L } = \\sum _ { v \\in D } \\langle \\partial _ { b , W } E _ { b , W } \\rangle _ { v } \\ \\quad - \\ \\sum _ { v _ { d } \\in D _ { i n } } \\langle \\partial _ { b , W } E _ { b , W } \\rangle _ { v _ { d } }\n$$\n\nwith the Boltzmann averages\n\n$$\n\\begin{array} { l } { \\displaystyle \\langle \\partial _ { b , W } E _ { b , W } \\rangle _ { \\upsilon } } \\\\ { \\displaystyle = \\frac { 1 } { Z _ { \\upsilon } T } \\sum _ { h \\in \\{ 0 , 1 \\} ^ { n _ { \\mathrm { h } } } } \\partial _ { b , W } E _ { b , W } ( s ) \\exp { \\left( - \\frac { E _ { b , W } ( s ) } { T } \\right) } , } \\end{array}\n$$\n\nwith normalization\n\n$$\nZ _ { v } = \\sum _ { h \\in \\{ 0 , 1 \\} ^ { h } } \\exp { \\left( \\frac { - E _ { b , W } ( s ) } { T } \\right) } ,\n$$\n\nand\n\n$$\n\\begin{array} { l } { \\displaystyle \\langle \\partial _ { b , W } E _ { b , W } \\rangle _ { v _ { d } } } \\\\ { = \\frac { 1 } { Z _ { v _ { d } } T } \\displaystyle \\sum _ { ( v _ { l } , h ) \\in \\{ 0 , 1 \\} ^ { ( n _ { l } + n _ { h } ) } } \\partial _ { b , W } E _ { b , W } \\exp { \\left( \\frac { - E _ { b , W } ( s ) } { T } \\right) } . } \\end{array}\n$$\n\nThus, the gradient steps used in training the BM are given by\n\n$$\n\\begin{array} { r } { \\delta b _ { i } = \\eta ( \\langle s _ { i } \\rangle _ { v } - \\langle s _ { i } \\rangle _ { v _ { d } } ) , \\qquad } \\\\ { \\delta W _ { i j } = \\eta ( \\langle s _ { i } s _ { j } \\rangle _ { v } - \\langle s _ { i } s _ { j } \\rangle _ { v _ { d } } ) , \\qquad } \\\\ { \\delta W _ { i k } = \\eta ( \\langle s _ { i } v _ { d k } \\rangle _ { v } - \\langle s _ { i } v _ { d k } \\rangle _ { v _ { d } } ) , } \\end{array}\n$$\n\nwhere $\\eta$ is a learning rate and $s _ { i }$ and $s _ { j }$ are the values of hidden or label units [7]. In practice, these values can be determined by sampling them repeatedly from the BM in a state of equilibrium, a certain number of times in the positive (or clamped) and an equal number of times in the negative (or unclamped) phase [7]. The difference between the positive and negative phases is that in the former, in which the sample values to calculate the first terms in the Equations $1 3 - 1 5$ will be determined, all visible units will be clamped to the values $v _ { d }$ and $v _ { l }$ from the encoded training data point, while in the latter, used to calculate the second terms in the respective equations, only the input units will be clamped to $v _ { d }$ [7]. In both cases, the pair-wise products of the sample values will then be formed to calculate the weight’s gradient steps, and subsequently, the values of $\\langle . . . \\rangle _ { v }$ respectively $\\langle . . . \\rangle _ { v _ { d } }$ can be calculated by averaging over the samples respectively their products [7], [8]. Doing this repeatedly for all data points in the training data set will eventually cause the probability distribution of the BM (Eq. 1) to mirror the conditional distribution of the labels (conditioned on the inputs $d ,$ ) in the training data set, which enables the BM to correctly classify the inputs $d$ [7].\n\n# B. Using Quantum Annealing for Boltzmann sampling\n\nReaching an equilibrium state of the BM’s network which can be used to sample from can be a computationally expensive endeavor, however [8], [12]. At least using classical computers, sampling from an arbitrarily connected network requires repeatedly updating each unit according to its stochastic update rule, which involves calculating the probability of its value $s _ { i }$ to become 1 given by\n\n$$\np _ { i } = \\frac { 1 } { 1 + \\exp { ( - \\Delta E _ { b , W } ^ { i } / T ) } } ,\n$$\n\nwhere\n\n$$\n\\Delta E _ { b , W } ^ { i } = \\sum _ { m = 1 } ^ { n _ { m } } W _ { i m } s _ { m } + b _ { i } ,\n$$\n\nwith $s _ { m }$ being the $n _ { m }$ neighboring units that are directly connected to the units $s _ { i }$ in the network [8]. This has to be done for every unit in BM, until none of the states of the units in the network change anymore [8]. Furthermore, as this process has to be performed multiple times (depending on the number of samples used for averaging) for each data point, this classical training method for arbitrarily connected BMs is often considered intractable [8], [12].\n\nThis time consuming process of obtaining samples can, however, be avoided by using Quantum Annealing (QA) to draw samples from the BM [7], [20], [21].\n\nD-Wave quantum annealers implement a time-dependent Hamiltonian that acts on a system of $N$ logical qubits, interpolating between a driver Hamiltonian and a target problem Hamiltonian [24]:\n\n$$\nH ( t ) = A ( t ) H _ { D } + B ( t ) H _ { P }\n$$\n\nwhere $A ( t )$ and $B ( t )$ are scheduling functions satisfying\n\n$$\nA ( t _ { i } ) \\gg B ( t _ { i } ) \\mathrm { ~ a n d ~ } A ( t _ { f } ) \\ll B ( t _ { f } )\n$$\n\nat the initial $( t _ { i } )$ and final $( t _ { f } )$ times of the annealing schedule. If the process of changing the time-dependent Hamiltonian by progressing along this annealing schedule is done adiabatically, i.e. sufficiently slow, and if the initial state at $t _ { i }$ is the ground state of $H _ { D }$ , the resulting state at time $t _ { f }$ should be the ground state of $H _ { P }$ [24], which is to represent the most likely or best solution of a given problem, which one is looking for. Consequently, the ground state of $H _ { D }$ should be easy to prepare [24]. Therefore, D-Wave’s machines employ the transverse field as the driver Hamiltonian [7], [24], [25]:\n\n$$\nH _ { D } = - \\sum _ { i = 1 } ^ { N } \\sigma _ { i } ^ { x }\n$$\n\nwith\n\n$$\n\\sigma _ { i } ^ { x } \\equiv \\underbrace { I \\otimes \\cdots \\otimes I } _ { i - 1 } \\otimes \\sigma _ { x } \\otimes \\underbrace { I \\otimes \\cdots \\otimes I } _ { N - i }\n$$\n\nrepresenting a state where the Pauli-X operator $\\sigma _ { x } = { \\binom { 0 } { 1 } } \\quad 0 \\quad$ acts on the ith logical qubit of the system and $I = { \\binom { \\dot { 1 } } { 0 } } \\ { \\stackrel { 0 } { 1 } } \\bigg \\rangle ^ { \\prime } .$ Here, $\\otimes$ refers to the tensor product.\n\nFurthermore, the target problem Hamiltonian of interest $H _ { P }$ takes the form of an Ising Hamiltonian in current QAprocessors [26]:\n\n$$\nH _ { P } = - \\sum _ { i = 1 } ^ { N } h _ { i } \\sigma _ { i } ^ { z } - \\sum _ { i , j = 1 \\atop i < j } ^ { N } J _ { i , j } \\sigma _ { i } ^ { z } \\sigma _ { j } ^ { z } ,\n$$\n\nwith\n\n$$\n\\sigma _ { i } ^ { z } \\equiv \\underbrace { I \\otimes \\cdots \\otimes I } _ { i - 1 } \\otimes \\sigma _ { z } \\otimes \\underbrace { I \\otimes \\cdots \\otimes I } _ { N - i }\n$$\n\nrepresenting a state where the Pauli- $Z$ operator\n\n$$\n\\sigma _ { z } = { \\binom { 1 } { 0 } } \\quad { \\overset { } { - } } 1 \\quad\n$$\n\nacts on the $i$ -th logical qubit of the system and where $h _ { i }$ and $J _ { i , j }$ denote the logical qubits’ biases and coupling strengths [7], [26]. Ising models – just like their equivalent binary formulations, the Quadratic Unconstrained Binary Optimization (QUBO) models – are a type of energy-based model that makes it relatively easy to encode problems of which optimal, or at least good, solutions are of interest, given that these solutions can be represented by a minimum, or at least low, energy configuration of the model [7], [26]. In our case, by choosing $\\mathbf { { h } } _ { i } ~ = ~ \\mathbf { { b } } _ { i }$ and $J _ { i , j } ~ = ~ W _ { i , j }$ , the Hamiltonian $H _ { P }$ can be tailored to resemble the energy function of a BM [7], [27]: Here, the logical qubits of the quantum system – which probabilistically assume values 1 or 0 upon measurement at the end of the annealing process – can take up the role of the hidden and label units of the network. Meanwhile, the input units – acting as biases to the rest of the network according to Eq. 8 – do not need to be represented by qubits and can, in theory, possess arbitrary values, including non-binary ones [7]. When samples from a quantum system – i.e. strings of qubit values representing the system’s configuration upon measurement – are acquired using a physical quantum annealer, the hardware interacts with the environment, resulting in a sample distribution that follows an approximate Boltzmann distribution (instead of always returning the ground state of $H _ { P }$ ) [21]. Thus, the QA process can be used in BM training (and inference) to reach an equilibrium from which a sample can be drawn in one step [7], [20], [21]. Unless explicitly stated otherwise, BMs trained and executed in this way is what we will be referring to as Quantum Boltzmann Machines (QBMs) in the remainder of this paper. In related works on image classification tasks, such as one that utilized a Restricted QBM to pre-train a Deep Belief Network for MNIST data classification [16], and another where classification was achieved solely with a Restricted QBM [14], using Quantum Annealing for training offered benefits in computation time.\n\nHowever, device-specific properties suggest that sampling from a quantum annealer’s distribution will be done with an instance-dependent effective temperature, which differs from the actual hardware temperature [7], [28]. This effective temperature can be estimated and then incorporated by rescaling the weights and biases with the inverse of said temperature [7], [28]. Nonetheless, previous work has shown that quantum BMs trained with raw QA-samples, without any temperature correction, still arrive at probability distributions to be sampled from that sufficiently mimic the training data set’s distribution to be usable for machine learning tasks, even though they might deviate from the classical Boltzmann distribution [27]. Hence, this work will not incorporate a tuning of the effective temperature and instead rely on the benefit of raw QAsampling as has been demonstrated in [27].\n\n# C. Employing Parallel Quantum Annealing\n\nWhen using QA hardware for BM training and inference, the logical qubits of the Hamiltonian $H _ { P }$ must be mapped to physical qubits on the QA-hardware that have suitable connectivity to represent the connectivity of units in the BM [7], [26]. This process is known as minor embedding and is typically performed using D-Wave’s minorminer API [26], [29].\n\nAll experiments in this study are conducted on the D-Wave Advantage System, which contains about 5000 physical qubits and uses the so-called Pegasus topology [30]. This topology supports up to 15 connections per qubit, offering improved flexibility over earlier architectures [30]. However, because a fully connected BM exceeds the connectivity of a single qubit, it is necessary to represent each logical unit by a chain of multiple physical qubits [26]. Depending on the length of the chain, this has proven to add additional noise to the system and should be avoided where ever possible [26].\n\nMinor embedding is generally applied to a single problem instance, where samples are then acquired sequentially. However, Pelofske et al. [22] have shown that multiple disjoint problem instances can be embedded in parallel. This approach, which we call Parallel Quantum Annealing $( P Q A )$ , enables the annealer to generate samples from several instances within a single annealing cycle [22]. Their study reported significant reductions in quantum processing time, but also noted a drop in sample quality [22]. To address this, the authors suggested increasing the spatial distance between embeddings to reduce leakage [22]. Since their method relied on automatic embedding using minorminer, this separation could not be guaranteed [22].\n\nA very recent approach by Noe\\` et al. [1] first also used PQA in the context of QBMs. A QBM with 16 visible and 16 hidden units was trained with the D-Wave Advantage4.1 system to reconstruct images from the $4 \\mathrm { x } 4$ -pixel “Bars and Stripes” data set in an unsupervised manner, utilizing PQA [1], [22]. For the clamped phase, the researchers embedded 4 instances of the QBM into the QA hardware graph simultaneously, each instance having its visible units clamped to another data point [1]. For the unclamped phase, the QBM was embedded 26 times simultaneously (without any units being clamped) [1]. With this strategy, the quantum annealingbased training method showed an 8.6-fold improvement in wall clock time over a parallelized classical Gibbs sampling method [1]. In addition, their evidence shows that QBMs sampling time scales almost linearly with the size of the model, showing potential for bigger problem instances [1]. However, the actual generative performance of their QBM ended up being worse than the classical approach [1]. Among other reasons, the authors explain these results with unsuitable temperature scaling and current hardware limitations [1].\n\n# D. Our Contribution: Improved Parallel Quantum Annealing for supervised learning using QBMs\n\nFrom Noe\\` et al.’s [1] description of their work, it does not become clear whether they adhered to Pelofske et al.’s [22] above-mentioned suggestion to actively increase the spatial distance between different problem instances – in this case different instances of the BM. Thus, it cannot be ruled out that the problems with sample quality they faced due to hardware limitations where at least partially due to unintended interactions between qubits of different problem instances that had been placed too close together on the hardware graph [22].\n\nThe approach presented in this work – which is, to the best of the authors’ knowledge, the first work using PQA in supervised QBM training – avoids that limitation by explicitly controlling the placement of embeddings:\n\nFirst, to create a QBM instance to embed, we incorporate an input data point into our model. In our experiments, we do this by assigning one input unit to each of the 784 pixel values of an input from the MedMNIST [2] data set that is used. It is important to note that the input units do not necessitate specific hardware resources since they are permanently fixed to the data, effectively serving as biases for the other nodes as described in Eq. 8. As we focus on binary classification, we then embed the class label by adding one label unit to the visible layer – encoding our two classes as 0 and 1. Furthermore, for the number of hidden units, we use a parameter that takes values between 1 and 20. We purposefully chose this relatively low upper limit for the number of hidden units, since this choice avoids exposing the model to unnecessary noise which otherwise might have been caused by excessively long chains of physical qubits that have to be build when embedding the model into hardware. At the end of these steps, we have created a QBM model instance with a maximum of 21 fully connected qubits to embed, as only the hidden units and the label units will need to be embedded onto the QA-hardware. This can be done using a QUBO model of the instance, which essentially takes the same shape as the QBM’s energy function displayed in Eq. 7 and can easily be mapped to a Hamiltonian $H _ { P }$ using the D-Wave API [31].\n\nGiven our QBM instance as a QUBO, we then divide the Pegasus topology of the D-Wave Advantage’s hardware graph into ten subgraphs using the Pymetis Python library [32]. This fixed number of subgraphs ensures sufficient possible spacing between the embeddings, given the maximum of 21 fully connected qubits we need to embed. To further isolate each of the ten subgraphs, we introduce a buffer zone between them: Nodes that are connected to other subgraphs are removed to reduce the likelihood of interference between embeddings. The final topology, including both the subgraphs (blue) used for finding embeddings and buffer zones (gray), ensuring a minimum distance between the embeddings, is shown in Fig. 2a. Each subgraph is then used to embed the same QUBO model of the QBM instance using minorminer. Fig. 2b shows an example of this multi-embedding, which includes a clamped QBM instance with 20 hidden units. (Here, the label units do not need to be embedded, given that in the clamped phase, they act as biases in the same way the input units do.) Using this type of multi-embedding, we can subsequently draw ten samples at once using one execution of the annealing cycle. While this strategy ensures a minimum distance between embeddings, thus preserving sample quality, it also results in a significant number of qubits being allocated to buffer zones. This reduces the overall embedding capacity of a given QA hardware graph.\n\nIn order to successfully evaluate the performance of our supervised QBMs that employ PQA and see how the approach scales when employing different model sizes, we perform experiments on two of the MedMNIST data sets [2], see Sec. III. Depending on the number of hidden units used, the configurations explored here contain between 1571 and 16695 trainable parameters: The minimum of 1571 parameters includes $7 8 4 \\cdot 2 = 1 5 6 8$ weights (acting as biases according to Eq. 7) from the input units to the rest of the network, one weight between the single hidden unit and label unit and one bias for each of those units. Similarly, the maximum of 16695 parameters includes $7 8 4 \\cdot 2 1 = 1 6 4 6 4$ input weights, $( 2 0 \\cdot 1 9 ) / 2 = 1 9 0$ weights between the hidden units, 20 weights between the hidden units and the single label unit, 20 hidden biases and one label unit bias. In all cases, all parameters are initialized with random values drawn uniformly from the interval $[ - 1 , 1 ]$ . To also compare the performance of these differently sized QBM models not just to each other, but also to that of some classical models of a similar size that are commonly used for the task at hand, we evaluated them against Convolutional Neural Networks (CNNs) using a similar number of parameters [33], [34]. These CNNs are described in the following section.\n\n![](images/9e5f0757849d4aa88a1dd4397559717de232925dd140f188af68bc1972ec3c86.jpg)  \nFig. 2: Visualization of the parallel embedding approach on the Pegasus topology. (a) The topology is partitioned into 10 subgraphs. Blue nodes indicate the regions used for embedding, while gray nodes represent buffer zones that enforce a minimum distance between embeddings. (b) An example embedding of a 20 hidden unit QBM (QA) in the clamped phase, demonstrating how the parallel embedding strategy utilizes the partitioned regions.\n\n# E. Convolutional Neural Networks\n\nDeep Convolutional Neural Networks (CNNs) [33] became the state of the art for image classification with the introduction of AlexNet [34]. CNNs are made of two types of layers. First, convolutional layers that apply filters to input data to extract feature maps, and pooling layers that downsample these feature maps to reduce their dimensionality [35]. Second, additional fully connected layers to make predictions based on the extracted features [34].\n\nThere is considerable empirical evidence that CNNs with more parameters tend to be more performant on many tasks. With 60 million parameters, AlexNet [34] was the first deep CNN that showed unprecedented success on large-scale and complex tasks, winning the 2012 ImageNet competition [36]. In [36] VGG networks were introduced. The publication demonstrated that increasing network depth significantly improved performance on the ImageNet challenge [36], providing evidence that deeper networks, and hence networks with more parameters, can learn more powerful representations. The VGG16 model has approximately 138 million parameters [36]. In [37], residual networks were introduced. This work demonstrated that deeper ResNet models perform better than shallower ones [37]. The smaller ResNet-18 model has (about) 11.7 millions parameters [38]; a number not being able to be matched with QBMs, due to the lack of sufficient quantum hardware.\n\nTo fairly compare QBMs with CNNs, we propose a CNN architecture with one convolutional layer and two sequential layers. In this setup, we are able to vary the number of parameters in a range similar to that of the QBM. The kernel size of the convolutional layer is 3 or 5 and always of dimension 1. The number of neurons of the first sequential layer is in $\\{ 4 , 8 , 1 6 , 2 4 \\}$ and for the second layer, it is in $\\{ 2 , 4 , 8 , 1 6 \\}$ , with the number of neurons of the second sequential layer never being higher than that of the first one.\n\nAfter the convolutional layer and the first sequential layer we apply the\n\n$$\n{ \\mathrm { R e L U } } ( x ) = \\operatorname* { m a x } ( 0 , x )\n$$\n\nactivation function. After the last sequential layer, the sigmoid activation\n\n$$\n\\sigma ( x ) = \\frac { 1 } { 1 + \\exp \\left( - x \\right) }\n$$\n\nis applied, mapping the output to the interval $[ 0 , 1 ]$ and making it interpretable as a probability.\n\nWe use the binary cross entropy loss\n\n$$\n\\ell ( x , y ) = - \\left( y \\log ( x ) + ( 1 - y ) \\log ( 1 - x ) \\right)\n$$\n\nfor training our CNN. Let\n\n$$\n\\mathrm { c n n } _ { \\theta } : [ 0 , 1 ] ^ { 2 8 , 2 8 }  [ 0 , 1 ]\n$$\n\nbe the CNN and\n\n$$\n( x _ { i } , y _ { i } ) _ { i = 1 , \\ldots , n } \\in [ 0 , 1 ] ^ { 2 8 , 2 8 } \\times \\{ 0 , 1 \\}\n$$\n\nbe our encoded data set, then the training of our CNN is the following optimization problem:\n\n$$\n\\operatorname* { m i n } _ { \\theta } \\sum _ { i = 1 } ^ { n } \\ell ( \\mathbf { c n n } _ { \\theta } ( x _ { i } ) , y _ { i } ) .\n$$\n\nSuch optimization problems are solved with gradient-based methods. We used the Adam optimizer for training [39].\n\n# III. EXPERIMENTS\n\n# A. The MedMNIST data sets and their common performance metrics\n\nAll experiments are conducted using two datasets from the MedMNIST [2] collection of biomedical images: PneumoniaMNIST and BreastMNIST. Both datasets are preprocessed into grayscale images of size $2 8 \\times 2 8$ pixels and provide a standardized train-validation-test split.\n\nThe PneumoniaMNIST dataset consists of 5856 pediatric chest $\\mathrm { \\Delta X }$ -ray images, categorized into two classes: pneumonia and healthy. It is derived from the substantially larger images in [40], from which the images where center-cropped and subsequently resized to match the low-resolution format. The data is divided into training and validation sets in a 9:1 ratio, with the original source validation set used as the test set.\n\nThe BreastMNIST dataset includes 780 breast ultrasound images and is based on $5 0 0 \\times 5 0 0$ pixels large images from [41]. Due to the significantly lower resolution, the original three-class classification task (normal, benign, malignant) was reformulated as a binary classification problem, “normal” and “benign” being merged into a single positive class, while the “malignant” was treated as negative. The dataset is split into training, validation, and test sets using a 7:1:2 ratio.\n\nMedical image datasets often exhibit substantial class imbalance, which can lead to misleading performance evaluations when using the standard accuracy (ACC) as a primary metric. For instance, approximately $7 3 \\%$ of images in both PneumoniaMNIST and BreastMNIST are labeled positive, which means a constant estimator would achieve an accuracy of $7 3 \\%$ .\n\nThe AUC-Score, which we, as well as [2], use in addition to the ACC, is derived from the ROC curve, which plots the true positive rate against the false positive rate at various thresholds. This metric can thus reflect the models’ ability to distinguish between class labels: A score of 0.5 indicates random guessing, whereas a score of 1.0 indicates perfect separation of class labels.\n\n# B. Hyperparameter optimization\n\n1) QBM: The performance of a BM is influenced by several hyperparameters, which makes an appropriate hyperparameter optimization essential. The selected hyperparameters and their corresponding search spaces are summarized in Table I.\n\nWe initially aimed to perform 100 hyperparameter optimization runs per data set, using 10 different random seeds each for, among other things, weights initialization. The results from these runs were to be averaged across seeds, and the optimization was conducted using the Bayesian search algorithm provided by the Weights and Biases framework [42]. In order to have the optimization respect both the accuracy and the AUC-score, we employed a composite score defined as $0 . 5 \\cdot \\mathrm { A C C } + 0 . 5$ AUC. However, conducting such extensive optimization using QBMs with QA would have exceeded our available access to D-Wave QA-hardware. Prior work [20], [21] has demonstrated that for the purpose of hyperparameter tuning, the Simulated Annealing algorithm (SA) can serve as a practical classical alternative to QA, as it also approximates a Boltzmann distribution [43]. Despite employing this alternative as as a workaround, long algorithm run times and unexpected failures of our classical hardware unfortunately limited the number of optimization runs we were able to complete in time for the preparation of this paper to only 20 runs on the BreastMNIST dataset and only 18 runs on the PneumoniaMNIST dataset regardless, meaning that not much optimization of hyperparameters took place in the end.\n\nStill, the models achieving the best validation performance were then chosen to be retrained with QA, using our PQA strategy and 3 random seeds.\n\n2) CNN: As with the QBM, we also used hyperparameter optimization to find the best architecture and optimization parameters for the CNN. The selected hyperparameters and the search space are summarized in Table II.\n\nFor the hyperparameter search for the CNNs we combined grid search [44] with random search [45]: For every pair of choices for kernel size and the number of neurons in the two consecutive sequential layers, making sure that the second layer has no more neurons than the first, we randomly picked 200 values for each of the other hyperparameters. Then, we picked the configuration that produced the highest combined score on the validation set – that is, the sum of the AUC-score and accuracy – during training.\n\nThe number of epochs was chosen high enough such that the validation performance stopped improving at the end of the training. Specifically, we ran 50 epochs for PneumoniaMNIST and 350 epochs for BreastMNIST. This difference is due to the datasets’ sizes; using more epochs for BreastMNIST ensured roughly the same number of gradient steps during training, given an equal batch size.\n\n# C. Results\n\nAs outlined in Sec. II-D, our experimental objectives were to evaluate how our QBM-based approach compares to CNNs of equivalent size, which are traditionally used in image classification, as well as whether its performance scales with different model sizes. Some preliminary answers to both of theses questions can be found in Fig. 3: For each of the different numbers of hidden neurons explored in our respective hyperparameter optimizations, using the “QBM” with SA (shown as circles) as well as the CNN (shown as crosses), the plot displays the accuracy (ACC) and AUC-Scores of the best models found using these neuron numbers, with respect to the remaining hyperparameters mentioned in Tables I and II as well as the random seed used for initialization, on the test data sets. Notice that in the context of the “QBM” trained with SA, one cannot really speak of the hyperparameters of each of these configurations having been optimized – given that only very little runs per number of hidden neurons have been performed. Still, we suspect that even the performance metrics of the models that did not or only barely undergo hyperparameter optimization allow for some preliminary insights. Furthermore, the plots also show the best overall result obtained by running the QBM on quantum hardware (shown as $+$ ), as well as the results of the millions of parameters large state-of-art CNNs ResNet-18 and ResNet-50 from literature [2], shown in red. For the PneumoniaMNIST data set, displayed in Fig. 3a, we observe no clear trend of parameter numbers much influencing model performance for either of the models – with the exception of the (surely also in other ways optimized) ResNet approaches reaching significantly higher AUC-Scores. While all other approaches show AUC-Scores in a very similar range, we find that the CNN models can generally still outperform their QBM-based counterparts by a bit in terms of accuracy on this data set – some even improving upon ResNet in this regard. The best CNN variants also outperform their similarly sized QBM counterparts regarding the AUC-Score they reach, the overall best models matching each others performance in this regard. While having the CNNs outperform the QBMs is not surprising, given – among other things – their more extensive hyperparameter optimization, an interesting finding in this plot is that while still outperformed by most other models, the QBM trained with actual QA is fairly close in performance to some of the similary sized SA-trained models, and even outperforms some similarly sized CNNs, both in terms of ACC and AUC-Score. On the smaller BreastMNIST data set (Fig. 3b), however, results look vastly different in a lot of aspects: While here, the SA-trained QBMs still do not show a clear trend regarding a possible interconnectedness of performance and parameter number, CNNs do show a fairly clear trend regarding their improvement with size, with ResNet also clearly showing the best performance regarding both metrics this time. Additionally, the QA-based QBM is again noticeably outperformed by the majority of other the examined models, particularly the slightly larger CNNs, when evaluating AUC Score. However, it demonstrates a moderately average ACC, surpassing many of the smaller models in this regard. Quite some of SA-based QBMs do, however, outperform their similarly – or a bit larger – sized CNN counterparts here. Taken together, we do not see any clear conclusions that can be drawn from these two experiments about the general (medical) image classification performance of QBMs in comparison to CNNs just yet. We can, however, say that the number of parameters does not seem to have a huge impact on model performance for QBMs (even though we do not know with certainty that this would not change when employing more hyperparameter optimization).\n\nTable I: Hyperparameter Ranges for the QBM (SA) and the Best Hyperparameter for Each Dataset   \n\n<html><body><table><tr><td></td><td>Hidden Units</td><td>Epochs</td><td>Batch Size</td><td>LearningRate</td><td>Sample Count</td></tr><tr><td>Value ranges</td><td>1-20</td><td>1-20</td><td>1-100</td><td>0.00001-0.6</td><td>10-1000</td></tr><tr><td>PneumoniaMNIST</td><td>10</td><td>20</td><td>73</td><td>0.45295</td><td>100</td></tr><tr><td>BreastMNIST</td><td>8</td><td>13</td><td>12</td><td>0.43496</td><td>400</td></tr></table></body></html>\n\nTable II: Hyperparameter Ranges for the CNN and the Best Hyperparameter for Each Dataset   \n\n<html><body><table><tr><td></td><td>Kernel Size</td><td>Neurons 1st Sequential Layer</td><td>Neurons 2nd Sequential Layer</td><td>Learning Rate</td><td>β1</td><td></td><td>Batch Size</td></tr><tr><td>Value ranges</td><td>3,5</td><td>4,8,16,24</td><td>2,4,8,16</td><td>[0.0005,0.05]</td><td>[0.998,0.9999]</td><td>[0.98,0.999999]</td><td>2²,...,210</td></tr><tr><td>PneumoniaMNIST</td><td></td><td>16</td><td>8</td><td>0.00384</td><td>0.98428</td><td>0.99925</td><td>16</td></tr><tr><td>BreastMNIST</td><td>55</td><td>24</td><td>8</td><td>0.00117</td><td>0.98674</td><td>0.99931</td><td>8</td></tr></table></body></html>\n\n![](images/e7e5e54292476b164f17a12f861464c06c1d392f72739a067a4821ad71da276f.jpg)  \nFig. 3: The figures compare the performance of the QBMs and CNNs on the PneunomiaMNIST and the BreastMNIST data sets. In each panel, the models’ test AUC and ACC-scores are presented, while a color map indicates the number of parameters. For additional context, we included the results for two variants of the ResNet CNN [37] as reported in [2]. They are highlighted in red because their parameter counts exceed the colormap’s representable range: ResNet-18 has 11.69 million parameters [38], and ResNet-50 about 25.56 million [46].\n\nThus, we want to take a closer look at the average performance of the models with the absolute best hyperparameter configurations we found so far (regarding classification performance on the validation set after the last epoch) – independent of parameter size and initialization with random seeds. The selected hyperparameters used for this are summarized in Table I. As one of our previous works on medical image classification using SA-based QBMs [20] found signs that the usage of QBMs might not only reduce sampling time, but also the number of epochs necessary to reach good classification performance, we investigate the classification performance on the test set after each epoch of training when doing so.\n\nThe result, showing the average test accuracies and average test AUC-Scores across different random seeds, as well as their standard deviations, can be seen in Fig. 4. We would like to point out that, as the QBM (QA) was only run with three seeds due to machine-failure-related time constraints, we do not consider its standard deviation representative enough to draw any conclusions from that. To make at least the results for the QBM (SA) and the CNN directly comparable here, only the results of ten random seeds are plotted here for both models. Comparing the standard deviations for both these models, we notice, at least on the Pneumonia data set, that the CNNs standard deviation is a lot larger. This increased variability is due to the fact that in multiple training runs, the CNN consistently predicted the majority class, which led to the gradients vanishing — a common issue when training CNNs [47]. Although these vanishing gradients affected the average performance, they did not impact the highest performance achieved in our training runs. We do not observe this effect on the BreastMNIST data set, however, where the standard deviations are fairly comparable.\n\nComparing the average performance metrics on the PneumoniaMNIST data set, we notice that both QBM versions seem to clearly outperform the CNN both in terms of ACC as well as in terms of AUC-Score – at least in the first 25 epochs displayed here. Remarkably, they already reach their high classification performances within the first 8 epochs – even the first 5 ones for the QBM (QA). A similar observation can be made for the BreastMNIST data set: While here, the CNN seems to be “catching up” with the QBMs over the course of the training, at the very least with the QBM (SA) in terms of accuracy, the QBMs clearly outperform the CNN in the first 10 epochs of training in terms of both metrics – again with the QA-based version reaching its maximum performance (for the first time) earlier than the SA-based one. Within the first 7 epochs of training, the QBM (QA) outperforms the\n\nQBM (SA) in terms of both metrics, and even in the longer run it still reaches a comparable performance. This suggests that the QBM (QA) suffers very little from hardware noise in this type of application, or perhaps not at all.\n\nRegarding absolute classification performance, the QBMs also provide decent results on the PneumoniaMNIST data set: The average test accuracy at the final epoch in case of the QBM (SA) is $8 5 . 1 0 \\%$ and the AUC-Score is 0.8208, the second value of which almost comes close to values reached by large models like ResNet in literature [2], while our QBM (QA) reaches $8 4 . 0 3 \\%$ and 0.7996, respectively. In case of the BreastMNIST dataset, the performance of all models deteriorates significantly, however. The QBM (SA) achieves a test accuracy of $7 5 . 0 6 \\%$ and an AUC-Score of 0.6677, while the QBM (QA) reaches $7 6 . 2 8 \\%$ and an AUC-Score of 0.5946. This decline is likely attributable to the limited size of the training set (546 images [41]), posing challenges for effective generalization.\n\nIn addition to evaluating the classification performance of our QBMs, we also conduct a small analysis of our PQA approach in terms of QPU time: For QBM configurations of up to 20 hidden units, we tracked the QPU time expenditure of QBM training with both regular sequential, as well as our parallel QA, in seconds for 3 mini-batches a\\` 5 data points each, generating 1000 samples for both the clamped and unclamped training phases with each data point. The results are shown in Fig. 5. Here, PQA unsurprisingly exhibits a much more favorable QPU time usage compared to sequential QA: Averaged over every possible configuration, our PQA approach exhibits a speed up of $6 9 . 6 5 \\%$ . Furthermore, it also seems to scale more stably and favorably with increasing network sizes.\n\n# IV. DISCUSSION AND CONCLUSION\n\nIn this work, we presented an approach for image classification using quantum annealing (QA)-based Quantum Boltzmann Machines (QBMs) which are efficiently trained in a supervised manner using Parallel Quantum Annealing (PQA): Our approach involves simultaneously embedding several instances of the QBM into a quantum annealer’s hardware graph by embedding them into artificially separated subgraphs. The resulting distance of the instances’ embeddings is thought to preserve sample quality [22], while enabling the usage of QA to draw several Boltzmann samples for the QBM’s training process in parallel. This way, we achieve a speed-up of $6 9 . 6 5 \\%$ compared to the usage of regular sequential QA. Although our experiments remain inconclusive regarding the QBM’s ability to outperform classical CNN models of similar size in terms of classification performance, we find that they on average can reach decent classification metrics, similar to those of the CNNs, within much shorter numbers of epochs. Taking together these two aspects, we support future research into the proposed approach, as we deem it not unlikely that a future quantum speed-up could be achieved in this area.\n\nFuture work should first address the limitations of our current research:\n\n![](images/48aba77bcf4d724e535433c84980f6c24a6697e34df6ffeb70d086a61c5a0d03.jpg)  \nFig. 4: Test accuracies and AUC-Scores of QBMs trained using SA and QA, as well as a classical CNN with the best identified hyperparameters settings, per epoch. The solid lines represent the average performance over 10 (QBM (SA) & CNN) respectively 3 (QBM (QA)) independent seeds. The transparent area around them is the standard deviation.\n\n![](images/c7848be6c26c851f700affec1baf1ef8f7ff89b4d72fa99a4ae493d97df95a1b.jpg)  \nFig. 5: The plot shows the total QPU time required to perform 3 batches of 5 steps each, with every step generating 1000 samples for both the clamped and unclamped phase of QBM (QA) training. Results are presented for both standard sequential QA and our approach using PQA.\n\nOur experiments should be repeated, this time involving a consistent and extensive approach to hyperparameter optimization, especially for the QBM, to ensure that the best possible performance of each type of model is actually obtained. Ideally, the optimization should already be executed using QA, as SA, albeit resulting in a similar sample distribution, does not outright simulate it and thus does not necessarily return the exact same parameters. The search also should include the optimization of $T$ , which we consciously left out in this work due to the limited availability of QPU time. Furthermore, large numbers of random seeds should be used across all models to enable statistically significant comparisons of e.g. their sensitivity to “bad” weight initializations for a given data set. And moreover, the ranges of hyperparameter search should be extended, e.g. to investigate a QBM’s performance using larger numbers of epochs as well.\n\nSecondly, it would be desirable to execute experiments on future generations of hardware, which offer more qubits and a denser connectivity, to enable the embedding of larger QBMs, in terms of hidden units, while guaranteeing similar or lower levels of noise.\n\nAnd thirdly, it would be desirable to benchmark our QBM on datasets containing larger images or multiple classes, to see how its performance scales in comparison to that of classical models like (larger) CNNs with increasing problem complexity. Since, as mentioned above, input units only function as bias terms to the remaining units of the QBM within the framework of discriminative learning, the number of units in our model that need to be represented by logical qubits, which are embedded into the quantum annealer’s hardware graph, is determined solely by the sizes of the hidden and label layers. Thus, the additional input units required to represent larger input images, with more pixels, would not impact the size of the embedded QBM instances our model uses. Furthermore, when increasing the number of classes, the number of output units needed scales only linearly with the number of classes our model is supposed to be able to represent, as we need one output unit per class. This means that unless the number of classes in the data set becomes exceedingly high – which might be the case for large generic data sets like ImageNet [48], but is not expected for medical ones – increasing the number of classes does not have much negative impact on the embeddability of the QBM instances either. It remains e investigated whether increasing the number of hidden units, in order to increase the parameter count of the QBM, would be necessary to ensure that a desirable classification formance can be reached on these more complex However, preliminary results on the data se this work that increasing or reducin thus number of hidden units re critical impact on our QBM mod some classical models like even though t also remains to be investigated ho in terms of the rather low training scale with increasing dat plexit we would expect our approach to scale rath ore omp ta sets, as the PQA should still be usable without difficulty. Whether the approach could then outperform larger CNNs on these tasks in terms of classification performance or training speed, e.g. by again needing only a smaller number of epochs, remains to be seen. And finally, it would be interesting to compare the QBM as we use it in our work to more similar classical models, given that the CNNs we use as a classical baseline in this work, while being especially well suited to the task of image classification, process these images in a very different way that makes use of the spacial structure of the input features (compare Sec. II-E), while the QBM works on flattened inputs. Thus, while CNNs might be a good baseline to investigate whether the QBMs perform well, a comparison against them is not particularly well suited to investigate whether any potential advantages we see when using our model stem from using quantum computing. While of course we already present one of the most direct classical comparisons possible in this work in form of the QBM (SA) model, it might be interesting in this context to also compare our QBM to more common comparable classical models such as (Classification) Restricted\n\nBoltzmann Machines [49], [50], classical general Boltzmann Machines trained using typical Boltzmann learning [8] 1, Deep Boltzmann Machines (DBMs) [53], [54] or even convolutional Deep Boltzmann Machines [55]. Another way to limit the issues of comparability between CNNs and our QBM model, and possibly also improve its performance by incorporating techniques that make CNNs so successful at processing data with spatial dependencies, such as image classification, involves modifying the model architecture toward a design resembling classical convolutional DBMs, for which training remains demanding [53]–[55]. This design would consist of multiple layers of hidden units arranged hierarchically and incorporate convolutional connections, both enabling the model to learn increasingly abstract data representations and capture higher-order feature correlations while reducing parameter count [53]–[55]. This might create a quantum model that can be efficiently trained to perform accurate classification.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了在NISQ（Noisy Intermediate-Scale Quantum）时代，量子玻尔兹曼机（QBM）训练过程中量子处理单元（QPU）时间成本过高的问题，这一问题限制了QBM在医疗影像分类等实际应用中的可行性。\\n> *   该问题的重要性在于，通过减少QPU时间，可以显著降低量子计算的成本，推动量子机器学习在医疗影像分析等领域的实际应用。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种改进的并行量子退火（PQA）方法，用于在监督学习设置下训练QBM，通过在量子退火器的硬件图中隔离嵌入多个QBM实例，以减少训练时间并保持样本质量。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **并行量子退火（PQA）的改进：** 通过将量子硬件图划分为多个子图并引入缓冲区域，显著减少了不同QBM实例之间的干扰，从而在保持样本质量的同时实现了69.65%的训练速度提升。\\n> *   **监督学习的QBM应用：** 首次将PQA应用于监督学习的QBM训练，并在MedMNIST数据集上验证了其有效性，分类性能与类似规模的卷积神经网络（CNN）相当，但训练所需的epoch数显著减少。\\n> *   **医疗影像分类的量子优势探索：** 在PneumoniaMNIST和BreastMNIST数据集上，QBM在较少的epoch内达到了与CNN相当的分类性能，展示了量子计算在医疗影像分类中的潜力。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   量子玻尔兹曼机（QBM）是经典玻尔兹曼机的量子类比，通过量子退火（QA）从Boltzmann分布中采样，避免了经典BM训练中耗时的采样过程。\\n> *   并行量子退火（PQA）通过在单个退火周期中嵌入多个独立的QBM实例，显著减少了QPU时间。论文通过隔离嵌入和引入缓冲区域，进一步减少了实例间的干扰，提高了样本质量。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前的工作（如Noe等人）在无监督学习中使用了PQA，但未明确控制嵌入实例间的距离，可能导致样本质量下降。\\n> *   **本文的改进：** 论文提出了一种改进的PQA方法，通过将量子硬件图划分为10个子图并引入缓冲区域，确保嵌入实例间的隔离，从而减少了干扰并保持了样本质量。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **QBM实例构建：** 将输入数据（如MedMNIST图像）编码为输入单元，标签编码为标签单元，并添加1到20个隐藏单元。\\n> 2.  **量子硬件图划分：** 使用Pymetis库将Pegasus拓扑划分为10个子图，并引入缓冲区域以减少实例间的干扰。\\n> 3.  **并行嵌入与采样：** 在每个子图中嵌入相同的QBM实例，并在单个退火周期中同时采样多个实例。\\n> 4.  **训练与优化：** 使用梯度下降法优化QBM的参数，通过PQA在 clamped 和 unclamped 阶段生成样本，计算梯度并更新参数。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   卷积神经网络（CNN）：包括不同架构的CNN，参数数量与QBM相当。\\n> *   经典玻尔兹曼机（BM）：使用模拟退火（SA）作为替代方法。\\n> *   ResNet-18和ResNet-50：作为大规模CNN的基准。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在准确率（ACC）上：** 在PneumoniaMNIST数据集上，QBM（SA）的平均测试准确率为85.10%，与类似规模的CNN相当，但显著优于部分小型CNN。QBM（QA）的平均测试准确率为84.03%，略低于SA版本但仍优于部分CNN。\\n> *   **在AUC-Score上：** QBM（SA）在PneumoniaMNIST上的AUC-Score为0.8208，接近ResNet的性能（0.85左右），而QBM（QA）的AUC-Score为0.7996，略低但仍具有竞争力。\\n> *   **在训练速度上：** 使用PQA的QBM训练速度比常规顺序QA快69.65%，显著减少了QPU时间。\\n> *   **在训练epoch数上：** QBM在较少的epoch内（5-8个）即可达到较高的分类性能，而CNN需要更多的epoch（如50个）才能达到类似性能。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   量子玻尔兹曼机 (Quantum Boltzmann Machine, QBM)\\n*   并行量子退火 (Parallel Quantum Annealing, PQA)\\n*   医疗影像分类 (Medical Image Classification, N/A)\\n*   量子退火 (Quantum Annealing, QA)\\n*   监督学习 (Supervised Learning, N/A)\\n*   量子计算 (Quantum Computing, N/A)\\n*   MedMNIST数据集 (MedMNIST Dataset, N/A)\\n*   卷积神经网络 (Convolutional Neural Network, CNN)\"\n}\n```"
}