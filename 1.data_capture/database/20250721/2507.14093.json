{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14093",
    "link": "https://arxiv.org/abs/2507.14093",
    "pdf_link": "https://arxiv.org/pdf/2507.14093.pdf",
    "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
    "authors": [
        "Šimon Kubov",
        "Simon Klíčník",
        "Jakub Dandár",
        "Zdeněk Straka",
        "Karolína Kvaková",
        "Daniel Kvak"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "paper_content": "# Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment\n\nŠimon Kubov $^ 1$ , Simon Klíčník $^ 2$ , Jakub Dandár $^ 3$ , Zdeněk Straka3[0000−0002−2788−1667], Karolína Kvaková $^ { 3 }$ , and Daniel Kvak3,4[0000−0001−7808−7773]\n\nDepartment of Radiogiagnostics, Thomayer University Hospital, Prague, Czechia   \n$^ 2$ Department of Radiology and Nuclear Medicine, University Hospital Královské Vinohrady and 3rd Faculty of Medicine, Prague, Czechia $^ 3$ Research & Development Department, Carebot, Ltd., Prague, Czechia daniel.kvak@carebot.com   \n$^ 4$ Department of Simulation Medicine, Masaryk University, 625 00 Brno, Czechia\n\nAbstract. Scoliosis affects roughly $2 { - } 4 \\%$ of adolescents, and treatment decisions depend on precise Cobb-angle measurement. Manual assessment is slow and subject to inter-observer variation. We conducted a retrospective, multi-centre evaluation of a fully automated deep-learning software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on 103 standing anteroposterior whole-spine radiographs collected from ten hospitals. Two musculoskeletal radiologists independently measured each study and served as reference readers. Agreement between the AI and each radiologist was assessed with Bland–Altman analysis, mean absolute error (MAE), root-mean-squared error (RMSE), Pearson correlation coefficient, and Cohen’s $\\kappa$ for four-grade severity classification. Against Radiologist 1 the AI achieved an MAE of $3 . 8 9 ^ { \\circ }$ (RMSE $4 . 7 7 ^ { \\circ }$ ) with a bias of $0 . 7 0 ^ { \\circ }$ (limits of agreement $- 8 . 5 9 ^ { \\circ }$ to $9 . 9 9 ^ { \\mathrm { { o } } }$ ); against Radiologist 2 it achieved an MAE of $3 . 9 0 ^ { \\circ }$ (RMSE $5 . 6 8 ^ { \\circ }$ ) with a bias of $2 . 1 4 ^ { \\circ }$ (limits $- 8 . 2 3 ^ { \\circ }$ to $1 2 . 5 0 ^ { \\circ }$ ). Pearson correlations were $r = 0 . 9 0 6$ and $r = 0 . 8 8 0$ (inter-reader $r = 0 . 9 2 8$ ), while Cohen’s $\\kappa$ for severity grading reached 0.51 and 0.64 (inter-reader $\\kappa = 0 . 5 9$ ). These results show that the proposed software reproduces expert-level Cobb-angle measurements and categorical grading across multiple centres, suggesting its utility for streamlining scoliosis reporting and triage in clinical workflows.\n\nKeywords: Artificial intelligence · Cobb angle · Deep learning · Interobserver agreement · Scoliosis $\\cdot$ Spine deformities\n\n# 1 Introduction\n\nScoliosis, a three-dimensional deformity characterised radiographically by a lateral curvature of the spine, affects roughly 2–4% of adolescents and up to 8% of adults worldwide [1]. The magnitude of curvature, expressed by the Cobb angle on an upright anteroposterior (AP) full-length spine X-ray, determines both the diagnosis ( $\\geq 1 0 ^ { \\circ }$ ) and the clinical pathway: curves of $1 0 { - } 2 4 ^ { \\circ }$ are usually observed, $2 5 \\mathrm { - } 3 9 ^ { \\circ }$ prompt bracing, and angles $\\geq 4 0 - 5 0 ^ { \\circ }$ often lead to surgical referral [2]. Precise measurement is therefore critical during screening and longitudinal follow-up, yet manual Cobb angle tracing is time-consuming and suffers from inter-observer discrepancies of $5 \\mathrm { - } 1 0 ^ { \\circ }$ even among experienced readers [3].\n\nOver the last years, deep-learning approaches have shown considerable promise in automating Cobb-angle assessment. Convolutional neural-network pipelines that first localise vertebral landmarks and then derive the maximal end-plate angle have demonstrated close agreement with expert measurements in single-centre studies [4,5]. Building on these results, Zhu et al. conducted a comprehensive meta-analysis of 14 models and confirmed that segmentation-based architectures generally outperform landmark-based ones [6]. More recently, multi-centre evaluations encompassing both adult and paediatric radiographs have reported near-radiologist reliability across different vendors and demographic groups [7,8]. Despite this progress, most published algorithms have been developed and tested on retrospective datasets from single institutions or on narrowly defined populations (e.g. adolescent idiopathic scoliosis), which limits their generalisability.\n\nTo address this gap, we developed a deep-learning model that (i) detects vertebral landmarks on whole-spine anteroposterior (AP) radiographs and (ii) calculates the maximal Cobb angle to categorise scoliosis severity. In this multi-centre validation study, we test the hypothesis that the model’s severity classification achieves agreement comparable to expert musculoskeletal radiologists when benchmarked against reference Cobb angles measured independently by two such readers.\n\n# 2 Materials and Methods\n\n# 2.1 Software\n\nThe proposed AI software is a deep-learning-based solution (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o., Czechia) that detects vertebral landmarks on whole-spine anterior-posterior (AP) X-rays, and computes the Cobb angle to classify severity of the scoliosis as no scoliosis ( $< 1 0 ^ { \\circ }$ ), mild (10–24°), moderate (25–39°) or severe $( \\geq 4 0 ^ { \\circ } )$ ). The software implements a twostage approach: (a) YOLOv11 landmark detector, trained on 575 expertly annotated whole-spine X-ray images (i.e. 39,100 vertebral keypoints) to localise the superior and inferior corners of vertebrae C7–L5; and (b) geometry-based algorithm computes Cobb angles from these landmarks. Delivered as a self-contained application, it integrates directly into clinical PACS via both DICOMweb and DIMSE (Figure 1), automating study retrieval and result insertion.\n\n![](images/6b5b73379228684aba1861b74ec049954978ae3a2494851a409246d7d40c0005.jpg)  \nFig. 1. Demo.\n\n# 2.2 Data collection\n\nA total of 125 full-length standing anterior-posterior spine X-rays acquired between 1 and 18 May 2025 were collected from 10 participating hospitals. After applying inclusion and exclusion criteria (Table 1), 22 studies were excluded and the final analysis set comprised 103 radiographs collected across ten hospitals—Hospital Hořovice ( $n \\ = \\ 4 8$ ), University Hospital Olomouc ( $n \\ : = \\ : 2 6 \\$ ), Hospital Frýdek-Místek ( $n = 1 2$ ), Hospital Karviná-Ráj ( $n = 4$ ), Surgical Disciplines Prague ( $n = 3$ ), Hospital AGEL Nový Jičín ( $n = 3$ ), Třinec Hospital ( $n = 2$ ), Silesian Hospital in Opava ( $n = 2$ ), Regional Hospital Náchod ( $n = 2$ ), and Regional Hospital Příbram ( $n = 1$ ). Given its retrospective nature and full anonymization, the requirement of informed consent was waived. All DICOM files were deidentified according to PS 3.15 Basic Application Confidentiality Profile, with removal of direct identifiers.\n\nTable 1. Eligibility inclusion and exclusion criteria.   \n\n<html><body><table><tr><td colspan=\"2\">CriterionDetail</td></tr><tr><td>Inclusion</td><td>All standing AP spine radiographs acquired between 1 and 18 May 2025 from patients aged ≥1 year.</td></tr><tr><td></td><td>Exclusion (i） unreadable or incomplete images;(ii） postoperative instru- mentation obscuring≥ 3 vertebrae;(iii) duplicate studies.</td></tr></table></body></html>\n\nThe cohort demonstrated a predominantly paediatric and adolescent profile: mean age $1 8 . 6 \\pm 1 3 . 3$ years, median 14 years (interquartile range 11–16 years), range 1.8–62.5 years; $8 1 \\%$ of radiographs were from patients younger than 20 years (Figure 2). Females slightly outnumbered males, accounting for 60 (58%) versus 43 (42%) X-rays, respectively.\n\n![](images/792eb9a419265cfc3947c87bf37836feda3a22dab00bf983fd323ad842d7c4da.jpg)  \nFig. 2. Age distribution of patients by hospital and sex. The violin plot displays kernel density estimation of age for each group, split by sex (Female ${ } = { }$ blue, Male $=$ orange).\n\n# 2.3 Study Design\n\nThis retrospective, multi-centre diagnostic accuracy study included all standing AP full-spine X-rays. Two musculoskeletal radiologists (Radiologist 1, Radiologist 2) independently measured the maximal Cobb angle on each X-ray using the institutional PACS tool. In parallel, the AI software analysed the same DICOM images, without manual intervention, to yield one automated Cobb angle per study. Readers and AI were blinded to each other’s results during measurement. For analysis, each AI measurement was compared to RAD1 and to RAD2 using paired design.\n\n# 2.4 Statistical Analysis and Endpoints\n\nThe agreement of continuous measurements was evaluated by Bland-Altman analysis. For each subject $i$ , the difference $d _ { i } = m _ { 1 , i } - m _ { 2 , i }$ was computed (where $^ { m _ { 1 , i } }$ and $m _ { 2 , i }$ are two readers’ angles), and the bias $\\begin{array} { r } { d = \\frac { 1 } { N } \\sum _ { i } d _ { i } } \\end{array}$ was reported along with limits of agreement $d \\pm 1 . 9 6 s _ { d }$ (with $s _ { d }$ the SD of $d _ { i }$ ). Mean absolute error $\\begin{array} { r } { \\left( \\mathrm { M A E } = \\frac { 1 } { N } \\sum _ { i } | d _ { i } | \\right) } \\end{array}$ and root-mean-squared error $\\begin{array} { r } { \\mathrm { ( R M S E = \\sqrt { \\frac { 1 } { N } \\sum _ { i } d _ { i } ^ { 2 } } ) } } \\end{array}$ were also calculated. Continuous Cobb angles from the AI software and each radiologist were reported as mean $\\pm$ standard deviation (SD). The linear association between two readers was quantified by Pearson’s correlation coefficient,\n\n$$\nr = { \\frac { \\sum _ { i } ( x _ { i } - { \\bar { x } } ) ( y _ { i } - { \\bar { y } } ) } { \\sqrt { \\sum _ { i } ( x _ { i } - { \\bar { x } } ) ^ { 2 } \\sum _ { i } ( y _ { i } - { \\bar { y } } ) ^ { 2 } } } } ,\n$$\n\nwhere $x _ { i }$ and $y _ { i }$ are paired measurements and $x , y$ their means. For categorical agreement, Cobb angles were stratified into four grades (no scoliosis $< 1 0 ^ { \\circ }$ , mild $1 0 { - } 2 4 ^ { \\circ }$ , moderate $2 5 \\mathrm { - } 4 4 ^ { \\circ }$ , severe $\\geq 4 5 ^ { \\circ }$ ). Pairwise agreement was summarized by Cohen’s kappa,\n\n$$\n\\kappa = { \\frac { p _ { o } - p _ { e } } { 1 - p _ { e } } }\n$$\n\nwhere $p _ { o }$ is the observed agreement and $p _ { e }$ the chance agreement. Before data extraction, a power analysis determined the minimum sample size required for achievement of endpoints (Table 2). Assuming a standard deviation of absolute errors $\\sigma = 4 . 5 ^ { \\circ }$ , at least\n\n$$\nn = \\left( { \\frac { 1 . 9 6 \\times 4 . 5 } { 1 . 0 } } \\right) ^ { 2 } \\approx 7 8\n$$\n\nwhole-spine X-rays were needed to bound the two-sided $9 5 \\ \\%$ CI around the sample MAE within $\\pm 1 ^ { \\circ }$ . For the secondary endpoint, an expected $\\kappa =$ 0.60 and class distribution $\\{ 6 0 \\% , 2 5 \\% , 1 0 \\% , 5 \\% \\}$ (chance agreement $P _ { e } = 0 . 4 3 5$ ) yielded $n \\approx 9 4$ images to keep the $9 5 \\ \\%$ CI of $\\kappa$ within $\\pm 0 . 1 5$ (Sim–Wright approximation); a conservative Donner–Eliasziw test for demonstrating $\\kappa > 0 . 4 0$ with $8 0 ~ \\%$ power suggested $n \\approx 1 0 8$ . All analyzes were conducted in Python (v3.10) using NumPy, pandas and scikit-learn.\n\nTable 2. Pre-specified endpoints and corresponding metrics.   \n\n<html><body><table><tr><td>Endpoint</td><td>Metric</td></tr><tr><td>Primary</td><td>Bland-Altman analysis (bias and limits of agreement,bias ± 1.96 SD) Mean absolute error (MAE) and root mean squared error (RMSE)</td></tr><tr><td>Secondary Mean ± SD of Cobb angles（)</td><td>Pearson's r between paired measurements Cohen'sκ for four-grade severity</td></tr></table></body></html>\n\n# 3 Results\n\nBland–Altman analysis comparing AI software with Radiologist 1 yielded a mean bias of $0 . 7 0 ^ { \\circ }$ ( $9 5 \\ \\%$ CI $- 0 . 2 3 ^ { \\circ }$ to $1 . 6 2 ^ { \\circ }$ ) and limits of agreement from $- 8 . 5 9 ^ { \\circ }$ to $9 . 9 9 ^ { \\circ }$ . The corresponding mean absolute error was $3 . 8 9 ^ { \\circ }$ and the root-meansquared error was $4 . 7 7 ^ { \\circ }$ . Versus Radiologist 2, the AI showed a bias of $2 . 1 4 ^ { \\circ }$ ( $9 5 \\ \\%$ CI $1 . 1 0 ^ { \\circ }$ to $3 . 1 7 ^ { \\circ }$ ) with limits of agreement $- 8 . 2 3 ^ { \\circ }$ to $1 2 . 5 0 ^ { \\circ }$ , MAE $3 . 9 0 ^ { \\circ }$ and RMSE $5 . 6 8 ^ { \\circ }$ . Inter-radiologist comparison produced a bias of $1 . 4 4 ^ { \\circ }$ ( $9 5 \\ \\%$ CI $0 . 6 6 ^ { \\circ }$ to $2 . 2 3 ^ { \\circ }$ ), limits of agreement $- 6 . 4 4 ^ { \\circ }$ to $9 . 3 2 ^ { \\circ }$ , MAE $3 . 3 0 ^ { \\circ }$ and RMSE $4 . 2 5 ^ { \\circ }$ . Overall, the AI’s systematic deviation from each radiologist was small and its variability fell within the same range observed between the two human readers (Table 3, Figure 3).\n\nTable 3. Bland–Altman and error-metric results.   \n\n<html><body><table><tr><td>Comparison</td><td>Bias (95% CI) [lower,upper]</td><td>LoA (°) [lower,upper]</td><td>MAE()RMSE()</td><td></td></tr><tr><td>AIvsRadiologist 1</td><td>0.70[-0.23,1.62][-8.59,9.99]</td><td></td><td>3.89</td><td>4.77</td></tr><tr><td>AI vsRadiologist 2</td><td>2.14[1.10,3.17][-8.23,12.50]</td><td></td><td>3.90</td><td>5.68</td></tr><tr><td>Radiologist 1 vs Radiologist 2 1.44 [0.66,2.23] [-6.44,9.32]</td><td></td><td></td><td>3.30</td><td>4.25</td></tr></table></body></html>\n\n![](images/94813930b322efc19cf18d110bf58e08cd7a047e702fcbe0935ea4ddb040aa54.jpg)  \nAl vs Radiologist 1   \nAl vs Radiologist 2   \nFig. 3. Bland–Altman analysis of Cobb-angle differences between AI software and each radiologist (RAD 1, RAD 2)\n\nContinuous Cobb angles averaged $1 3 . 1 2 ^ { \\circ } \\pm 1 1 . 1 0 ^ { \\circ }$ for the AI, $1 2 . 4 3 ^ { \\circ } \\pm 1 0 . 7 0 ^ { \\circ }$ for Radiologist 1, and $1 0 . 9 9 ^ { \\circ } \\pm 9 . 3 9 ^ { \\circ }$ for Radiologist 2 (Table 4).\n\nTable 4. Descriptive statistics of Cobb angles with standard deviation (SD).   \n\n<html><body><table><tr><td>Reader</td><td>Cobb angle mean (°） SD（°)</td></tr><tr><td>AI</td><td>13.12 11.10</td></tr><tr><td>Radiologist 1</td><td>12.43 10.70</td></tr><tr><td>Radiologist 2</td><td>10.99 9.39</td></tr></table></body></html>\n\nPearson correlation coefficients were very high for all pairwise comparisons: $r = 0 . 9 0 6$ (95 % CI 0.864–0.936) for AI vs. Radiologist 1, $r = 0 . 8 8 0$ ( $9 5 \\ \\%$ CI 0.827–0.917) for AI vs. Radiologist 2, and $r = 0 . 9 2 8$ ( $9 5 \\ \\%$ CI 0.895–0.951) for Radiologist 1 vs. Radiologist 2 (Table 5, Figure 4).\n\nTable 5. Pearson’s $r$ with $9 5 \\%$ confidence intervals (CI).   \n\n<html><body><table><tr><td>Comparison</td><td>r 95% CI</td></tr><tr><td>AI vs Radiologist 1</td><td>0.906 [0.864,0.936]</td></tr><tr><td>AI vs Radiologist 2</td><td>0.880 [0.827, 0.917]</td></tr><tr><td>Radiologist 1 vs Radiologist 2 0.928 [0.895,0.951]</td><td></td></tr></table></body></html>\n\n![](images/b921a663043243f66ecd8400fd21584bd0ad2a8c05d22dc22dae75aff1c8593d.jpg)  \nFig. 4. Scatter plots and linear regression of Cobb-angle measurements for AI software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) versus each radiologist.\n\nWhen Cobb angles were classified into four severity grades (none ${ < } 1 0 ^ { \\circ }$ , mild $1 0 – 2 4 ^ { \\circ }$ , moderate $2 5 \\mathrm { - } 4 4 ^ { \\circ }$ , severe $\\geq 4 5 ^ { \\circ }$ ), Cohen’s $\\kappa$ indicated moderate to substantial agreement: $\\kappa { = } 0 . 5 1$ for AI vs. Radiologist 1, $\\kappa { = } 0 . 6 4$ for AI vs. Radiologist 2, and $\\kappa { = } 0 . 5 9$ for Radiologist 1 vs. Radiologist 2 (Table 6, Figure 5).\n\nTable 6. Cohen’s $\\kappa$ for four-grade severity classification.   \n\n<html><body><table><tr><td>Comparison K</td></tr><tr><td>AI vs Radiologist 1 0.51</td></tr><tr><td>AI vs Radiologist 2 0.64</td></tr><tr><td>Radiologist 1 vs Radiologist 2 0.59</td></tr></table></body></html>\n\n![](images/d5fe32d14961fda3d48635ffc4a3e9379540873d36a5bc435b5b7213c9a4e254.jpg)  \nFig. 5. Confusion matrices comparing four-grade scoliosis severity classifications.\n\nRegarding power analysis, final cohort of 103 whole-spine X-rays exceeds the requirement for MAE precision and lies within the range targeted to bound the $9 5 \\ \\%$ confidence interval of Cohen’s $\\kappa$ to $\\pm 0 . 1 5$ .\n\n# 4 Discussion\n\nIn this study we evaluated the performance of AI software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) for automated Cobb-angle measurement against two expert radiologists. Across multiple metrics, the AI achieved human-level accuracy. Pearson correlation coefficients exceeded 0.88 for both AI-radiologist comparisons ( $r ~ = ~ 0 . 9 1$ vs. Radiologist 1; $r ~ = ~ 0 . 8 8$ vs. Radiologist 2), rivaling the inter-rater correlation of $r ~ = ~ 0 . 9 3$ . Mean absolute error (MAE) between AI and each radiologist $( \\approx 3 . 9 ^ { \\circ } )$ was virtually identical to that observed between the two radiologists themselves $( 3 . 3 ^ { \\circ } )$ , and root-mean-squared error (RMSE) differences fell within the same 4– $6 ^ { \\circ }$ range. Bland–Altman analysis revealed small systematic biases (0.7° toward overestimation versus Radiologist 1; 2.1° versus Radiologist 2) and limits of agreement spanning $\\pm 8 – 1 2 ^ { \\circ }$ , again mirroring inter-observer variability $( \\pm 6 { - } 9 ^ { \\circ } )$ . When Cobb angles were categorized into none/mild/moderate/severe grades (Figure 6), Cohen’s $\\kappa$ between AI and each radiologist ( $\\kappa = 0 . 5 1$ and 0.64) was on par with the $\\kappa = 0 . 5 9$ seen between humans, with most misclassifications confined to adjacent categories.\n\n![](images/db47330915b1dcf61f4d046348c397e0404fe759c63e77f265c357c6056f2b35.jpg)  \nFig. 6. Representative whole-spine X-rays illustrating four severity classes used in this study, with fully automated overlays generated by the proposed AI software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.).\n\nRecent literature confirms our findings. Landmark- or segmentation-based AI algorithms have achieved MAE values of $2 ^ { - 4 ^ { \\circ } }$ in single-centre evaluations [4,5], with ICCs up to 0.994 and Pearson $r \\ > \\ 0 . 9 8$ . A 2024 meta-analysis of 17 studies [6] reported a pooled mean error of $3 . 0 ^ { \\circ }$ ( $9 5 \\ \\%$ CI 2.6–3.4°) and confirmed that segmentation approaches outperform landmark detectors (2.4° vs 3.3°). Early multi-centre efforts have also suggested good generalisability; for example, Hayashi et al. [7] reported $\\mathrm { I C C } \\approx 0 . 9 6$ across paediatric and adult datasets, while Wong et al. [8] showed mean differences $< 3 ^ { \\circ }$ in a purely paediatric cohort. Nevertheless, most prior studies relied on retrospectively assembled, single-institution datasets, often restricted to adolescent idiopathic scoliosis and limited to one curve per film, which constrains external validity. By contrast, our multi-centre design, heterogeneous detector mix, and inclusion of complex, multi-curve cases demonstrate that expert-level performance can be maintained under real-world variability and thus directly address the generalisability gap identified in the current literature.\n\n# 4.1 Limitations\n\nAlthough our sample was drawn from ten different hospitals, the cohort remained skewed toward paediatric and adolescent examinations (81% under age\n\n20), which may limit generalizability to older adult populations. A modest number of severe curves $\\geq 4 0 ^ { \\circ }$ ) also reduced precision in the highest-angle bins. We did not assess the impact of AI integration on actual reporting time or downstream clinical decisions. Finally, while radiologist evaluation served as our comparison reference standard, Cobb angle measurement inherently involves some human error.\n\n# 5 Conclusions\n\nThe AI software demonstrated expert-level performance for automated Cobb angle measurement and severity grading, with agreement comparable to that between musculoskeletal radiologists. Its integration into clinical workflows can streamline scoliosis assessment, enhance consistency, and support efficient triage. Further prospective multicenter validation is needed to ascertain its impact on clinical decision-making and workflow efficiency.",
    "institutions": [
        "Thomayer University Hospital",
        "University Hospital Královské Vinohrady",
        "3rd Faculty of Medicine",
        "Carebot, Ltd.",
        "Carebot s.r.o.",
        "Masaryk University"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决脊柱侧凸评估中手动测量Cobb角耗时且存在观察者间差异的问题。脊柱侧凸影响着约2 - 4%的青少年和部分成年人，精确的Cobb角测量对于诊断和临床治疗决策至关重要，而手动测量的局限性影响了评估的效率和准确性。\\n\\n**方法概述**\\n提出一种基于深度学习的软件（Carebot AI Bones，Spine Measurement功能），通过检测全脊柱前后位（AP）X光片上的椎骨标志点，定位椎骨地标后计算最大Cobb角以对脊柱侧凸严重程度进行分类。\\n\\n**主要贡献与效果**\\n- 该软件在多中心验证中与肌肉骨骼放射科医生的测量结果高度一致。与放射科医生1对比，平均绝对误差（MAE）为$3.89^{\\circ}$（RMSE $4.77^{\\circ}$ ），偏差为$0.70^{\\circ}$；与放射科医生2对比，MAE为$3.90^{\\circ}$（RMSE $5.68^{\\circ}$ ），偏差为$2.14^{\\circ}$。\\n- Pearson相关系数显示AI与放射科医生的测量结果线性关联度高，AI与放射科医生1的$r = 0.906$，与放射科医生2的$r = 0.880$，两位放射科医生之间的$r = 0.928$。\\n- 对于严重程度分级，Cohen's $\\kappa$达到0.51（与放射科医生1）和0.64（与放射科医生2），两位放射科医生之间的Cohen's $\\kappa$为0.59，表明具有中度到高度的一致性。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n通过深度学习技术检测全脊柱APX光片上的椎骨标志点，深度学习模型能够学习大量X光片数据中的特征，准确识别椎骨标志点，再基于这些标志点的几何关系计算Cobb角，从而实现脊柱侧凸严重程度的自动分类。\\n\\n**创新点**\\n先前的研究大多基于单中心回顾性数据集或特定人群，泛化性有限。本文采用多中心设计，纳入复杂的多曲线病例，在真实世界的变异性下仍能保持专家级的性能，解决了当前文献中普遍存在的泛化性问题。\\n\\n**具体实现步骤**\\n1. 使用YOLOv11标志点检测器，在575张专家标注的全脊柱X光图像（即39,100个椎骨关键点）上进行训练，以定位C7 - L5椎骨的上下角。\\n2. 基于定位的标志点，使用几何算法计算Cobb角。\\n3. 将计算得到的Cobb角按照无脊柱侧凸（$< 10^{\\circ}$）、轻度（10 - 24°）、中度（25 - 44°）或重度（$\\geq 45^{\\circ}$）进行分类。\\n4. 软件作为独立应用程序，通过DICOMweb和DIMSE直接集成到临床PACS系统中，实现研究检索和结果插入的自动化。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n以两位肌肉骨骼放射科医生独立测量的结果作为对比基线。\\n\\n**性能对比**\\n*   **在 [平均绝对误差/MAE] 指标上：** 本文方法与放射科医生1对比，MAE为$3.89^{\\circ}$；与放射科医生2对比，MAE为$3.90^{\\circ}$。而两位放射科医生之间的MAE为$3.30^{\\circ}$，AI与放射科医生的MAE与放射科医生之间的MAE相近。\\n*   **在 [均方根误差/RMSE] 指标上：** 本文方法与放射科医生1对比，RMSE为$4.77^{\\circ}$；与放射科医生2对比，RMSE为$5.68^{\\circ}$。两位放射科医生之间的RMSE为$4.25^{\\circ}$，AI与放射科医生的RMSE差异在4 - 6°范围内，与放射科医生之间的RMSE差异处于相同范围。\\n*   **在 [Pearson相关系数/r] 指标上：** 本文方法与放射科医生1的$r = 0.906$（95% CI 0.864 - 0.936），与放射科医生2的$r = 0.880$（95% CI 0.827 - 0.917），两位放射科医生之间的$r = 0.928$（95% CI 0.895 - 0.951），AI与放射科医生的Pearson相关系数均超过0.88，表明线性关联度高，与放射科医生之间的相关性相当。\\n*   **在 [Cohen's $\\kappa$] 指标上：** 本文方法与放射科医生1的Cohen's $\\kappa$为0.51，与放射科医生2的Cohen's $\\kappa$为0.64，两位放射科医生之间的Cohen's $\\kappa$为0.59，AI与放射科医生的Cohen's $\\kappa$处于中度到高度一致的水平，与放射科医生之间的一致性相当。\",\n    \"keywords\": \"### 关键词\\n\\n- 脊柱侧凸评估 (Scoliosis Assessment, N/A)\\n- Cobb角测量 (Cobb Angle Measurement, N/A)\\n- 深度学习 (Deep Learning, DL)\\n- 人工智能 (Artificial Intelligence, AI)\\n- 多中心验证 (Multi - Centre Validation, N/A)\\n- 医疗影像分析 (Medical Image Analysis, N/A)\\n- 脊柱侧凸 (Scoliosis, N/A)\\n- 椎骨标志点检测 (Vertebral Landmark Detection, N/A)\"\n}"
}