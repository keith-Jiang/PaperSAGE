{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14093",
    "link": "https://arxiv.org/abs/2507.14093",
    "pdf_link": "https://arxiv.org/pdf/2507.14093.pdf",
    "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
    "authors": [
        "Šimon Kubov",
        "Simon Klíčník",
        "Jakub Dandár",
        "Zdeněk Straka",
        "Karolína Kvaková",
        "Daniel Kvak"
    ],
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "Thomayer University Hospital",
        "University Hospital Královské Vinohrady",
        "3rd Faculty of Medicine",
        "Carebot, Ltd.",
        "Masaryk University",
        "Carebot s.r.o."
    ],
    "paper_content": "# Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment\n\nŠimon Kubov $^ 1$ , Simon Klíčník $^ 2$ , Jakub Dandár $^ 3$ , Zdeněk Straka3[0000−0002−2788−1667], Karolína Kvaková $^ { 3 }$ , and Daniel Kvak3,4[0000−0001−7808−7773]\n\nDepartment of Radiogiagnostics, Thomayer University Hospital, Prague, Czechia   \n$^ 2$ Department of Radiology and Nuclear Medicine, University Hospital Královské Vinohrady and 3rd Faculty of Medicine, Prague, Czechia $^ 3$ Research & Development Department, Carebot, Ltd., Prague, Czechia daniel.kvak@carebot.com   \n$^ 4$ Department of Simulation Medicine, Masaryk University, 625 00 Brno, Czechia\n\nAbstract. Scoliosis affects roughly $2 { - } 4 \\%$ of adolescents, and treatment decisions depend on precise Cobb-angle measurement. Manual assessment is slow and subject to inter-observer variation. We conducted a retrospective, multi-centre evaluation of a fully automated deep-learning software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on 103 standing anteroposterior whole-spine radiographs collected from ten hospitals. Two musculoskeletal radiologists independently measured each study and served as reference readers. Agreement between the AI and each radiologist was assessed with Bland–Altman analysis, mean absolute error (MAE), root-mean-squared error (RMSE), Pearson correlation coefficient, and Cohen’s $\\kappa$ for four-grade severity classification. Against Radiologist 1 the AI achieved an MAE of $3 . 8 9 ^ { \\circ }$ (RMSE $4 . 7 7 ^ { \\circ }$ ) with a bias of $0 . 7 0 ^ { \\circ }$ (limits of agreement $- 8 . 5 9 ^ { \\circ }$ to $9 . 9 9 ^ { \\mathrm { { o } } }$ ); against Radiologist 2 it achieved an MAE of $3 . 9 0 ^ { \\circ }$ (RMSE $5 . 6 8 ^ { \\circ }$ ) with a bias of $2 . 1 4 ^ { \\circ }$ (limits $- 8 . 2 3 ^ { \\circ }$ to $1 2 . 5 0 ^ { \\circ }$ ). Pearson correlations were $r = 0 . 9 0 6$ and $r = 0 . 8 8 0$ (inter-reader $r = 0 . 9 2 8$ ), while Cohen’s $\\kappa$ for severity grading reached 0.51 and 0.64 (inter-reader $\\kappa = 0 . 5 9$ ). These results show that the proposed software reproduces expert-level Cobb-angle measurements and categorical grading across multiple centres, suggesting its utility for streamlining scoliosis reporting and triage in clinical workflows.\n\nKeywords: Artificial intelligence · Cobb angle · Deep learning · Interobserver agreement · Scoliosis $\\cdot$ Spine deformities\n\n# 1 Introduction\n\nScoliosis, a three-dimensional deformity characterised radiographically by a lateral curvature of the spine, affects roughly 2–4% of adolescents and up to 8% of adults worldwide [1]. The magnitude of curvature, expressed by the Cobb angle on an upright anteroposterior (AP) full-length spine X-ray, determines both the diagnosis ( $\\geq 1 0 ^ { \\circ }$ ) and the clinical pathway: curves of $1 0 { - } 2 4 ^ { \\circ }$ are usually observed, $2 5 \\mathrm { - } 3 9 ^ { \\circ }$ prompt bracing, and angles $\\geq 4 0 - 5 0 ^ { \\circ }$ often lead to surgical referral [2]. Precise measurement is therefore critical during screening and longitudinal follow-up, yet manual Cobb angle tracing is time-consuming and suffers from inter-observer discrepancies of $5 \\mathrm { - } 1 0 ^ { \\circ }$ even among experienced readers [3].\n\nOver the last years, deep-learning approaches have shown considerable promise in automating Cobb-angle assessment. Convolutional neural-network pipelines that first localise vertebral landmarks and then derive the maximal end-plate angle have demonstrated close agreement with expert measurements in single-centre studies [4,5]. Building on these results, Zhu et al. conducted a comprehensive meta-analysis of 14 models and confirmed that segmentation-based architectures generally outperform landmark-based ones [6]. More recently, multi-centre evaluations encompassing both adult and paediatric radiographs have reported near-radiologist reliability across different vendors and demographic groups [7,8]. Despite this progress, most published algorithms have been developed and tested on retrospective datasets from single institutions or on narrowly defined populations (e.g. adolescent idiopathic scoliosis), which limits their generalisability.\n\nTo address this gap, we developed a deep-learning model that (i) detects vertebral landmarks on whole-spine anteroposterior (AP) radiographs and (ii) calculates the maximal Cobb angle to categorise scoliosis severity. In this multi-centre validation study, we test the hypothesis that the model’s severity classification achieves agreement comparable to expert musculoskeletal radiologists when benchmarked against reference Cobb angles measured independently by two such readers.\n\n# 2 Materials and Methods\n\n# 2.1 Software\n\nThe proposed AI software is a deep-learning-based solution (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o., Czechia) that detects vertebral landmarks on whole-spine anterior-posterior (AP) X-rays, and computes the Cobb angle to classify severity of the scoliosis as no scoliosis ( $< 1 0 ^ { \\circ }$ ), mild (10–24°), moderate (25–39°) or severe $( \\geq 4 0 ^ { \\circ } )$ ). The software implements a twostage approach: (a) YOLOv11 landmark detector, trained on 575 expertly annotated whole-spine X-ray images (i.e. 39,100 vertebral keypoints) to localise the superior and inferior corners of vertebrae C7–L5; and (b) geometry-based algorithm computes Cobb angles from these landmarks. Delivered as a self-contained application, it integrates directly into clinical PACS via both DICOMweb and DIMSE (Figure 1), automating study retrieval and result insertion.\n\n![](images/6b5b73379228684aba1861b74ec049954978ae3a2494851a409246d7d40c0005.jpg)  \nFig. 1. Demo.\n\n# 2.2 Data collection\n\nA total of 125 full-length standing anterior-posterior spine X-rays acquired between 1 and 18 May 2025 were collected from 10 participating hospitals. After applying inclusion and exclusion criteria (Table 1), 22 studies were excluded and the final analysis set comprised 103 radiographs collected across ten hospitals—Hospital Hořovice ( $n \\ = \\ 4 8$ ), University Hospital Olomouc ( $n \\ : = \\ : 2 6 \\$ ), Hospital Frýdek-Místek ( $n = 1 2$ ), Hospital Karviná-Ráj ( $n = 4$ ), Surgical Disciplines Prague ( $n = 3$ ), Hospital AGEL Nový Jičín ( $n = 3$ ), Třinec Hospital ( $n = 2$ ), Silesian Hospital in Opava ( $n = 2$ ), Regional Hospital Náchod ( $n = 2$ ), and Regional Hospital Příbram ( $n = 1$ ). Given its retrospective nature and full anonymization, the requirement of informed consent was waived. All DICOM files were deidentified according to PS 3.15 Basic Application Confidentiality Profile, with removal of direct identifiers.\n\nTable 1. Eligibility inclusion and exclusion criteria.   \n\n<html><body><table><tr><td colspan=\"2\">CriterionDetail</td></tr><tr><td>Inclusion</td><td>All standing AP spine radiographs acquired between 1 and 18 May 2025 from patients aged ≥1 year.</td></tr><tr><td></td><td>Exclusion (i） unreadable or incomplete images;(ii） postoperative instru- mentation obscuring≥ 3 vertebrae;(iii) duplicate studies.</td></tr></table></body></html>\n\nThe cohort demonstrated a predominantly paediatric and adolescent profile: mean age $1 8 . 6 \\pm 1 3 . 3$ years, median 14 years (interquartile range 11–16 years), range 1.8–62.5 years; $8 1 \\%$ of radiographs were from patients younger than 20 years (Figure 2). Females slightly outnumbered males, accounting for 60 (58%) versus 43 (42%) X-rays, respectively.\n\n![](images/792eb9a419265cfc3947c87bf37836feda3a22dab00bf983fd323ad842d7c4da.jpg)  \nFig. 2. Age distribution of patients by hospital and sex. The violin plot displays kernel density estimation of age for each group, split by sex (Female ${ } = { }$ blue, Male $=$ orange).\n\n# 2.3 Study Design\n\nThis retrospective, multi-centre diagnostic accuracy study included all standing AP full-spine X-rays. Two musculoskeletal radiologists (Radiologist 1, Radiologist 2) independently measured the maximal Cobb angle on each X-ray using the institutional PACS tool. In parallel, the AI software analysed the same DICOM images, without manual intervention, to yield one automated Cobb angle per study. Readers and AI were blinded to each other’s results during measurement. For analysis, each AI measurement was compared to RAD1 and to RAD2 using paired design.\n\n# 2.4 Statistical Analysis and Endpoints\n\nThe agreement of continuous measurements was evaluated by Bland-Altman analysis. For each subject $i$ , the difference $d _ { i } = m _ { 1 , i } - m _ { 2 , i }$ was computed (where $^ { m _ { 1 , i } }$ and $m _ { 2 , i }$ are two readers’ angles), and the bias $\\begin{array} { r } { d = \\frac { 1 } { N } \\sum _ { i } d _ { i } } \\end{array}$ was reported along with limits of agreement $d \\pm 1 . 9 6 s _ { d }$ (with $s _ { d }$ the SD of $d _ { i }$ ). Mean absolute error $\\begin{array} { r } { \\left( \\mathrm { M A E } = \\frac { 1 } { N } \\sum _ { i } | d _ { i } | \\right) } \\end{array}$ and root-mean-squared error $\\begin{array} { r } { \\mathrm { ( R M S E = \\sqrt { \\frac { 1 } { N } \\sum _ { i } d _ { i } ^ { 2 } } ) } } \\end{array}$ were also calculated. Continuous Cobb angles from the AI software and each radiologist were reported as mean $\\pm$ standard deviation (SD). The linear association between two readers was quantified by Pearson’s correlation coefficient,\n\n$$\nr = { \\frac { \\sum _ { i } ( x _ { i } - { \\bar { x } } ) ( y _ { i } - { \\bar { y } } ) } { \\sqrt { \\sum _ { i } ( x _ { i } - { \\bar { x } } ) ^ { 2 } \\sum _ { i } ( y _ { i } - { \\bar { y } } ) ^ { 2 } } } } ,\n$$\n\nwhere $x _ { i }$ and $y _ { i }$ are paired measurements and $x , y$ their means. For categorical agreement, Cobb angles were stratified into four grades (no scoliosis $< 1 0 ^ { \\circ }$ , mild $1 0 { - } 2 4 ^ { \\circ }$ , moderate $2 5 \\mathrm { - } 4 4 ^ { \\circ }$ , severe $\\geq 4 5 ^ { \\circ }$ ). Pairwise agreement was summarized by Cohen’s kappa,\n\n$$\n\\kappa = { \\frac { p _ { o } - p _ { e } } { 1 - p _ { e } } }\n$$\n\nwhere $p _ { o }$ is the observed agreement and $p _ { e }$ the chance agreement. Before data extraction, a power analysis determined the minimum sample size required for achievement of endpoints (Table 2). Assuming a standard deviation of absolute errors $\\sigma = 4 . 5 ^ { \\circ }$ , at least\n\n$$\nn = \\left( { \\frac { 1 . 9 6 \\times 4 . 5 } { 1 . 0 } } \\right) ^ { 2 } \\approx 7 8\n$$\n\nwhole-spine X-rays were needed to bound the two-sided $9 5 \\ \\%$ CI around the sample MAE within $\\pm 1 ^ { \\circ }$ . For the secondary endpoint, an expected $\\kappa =$ 0.60 and class distribution $\\{ 6 0 \\% , 2 5 \\% , 1 0 \\% , 5 \\% \\}$ (chance agreement $P _ { e } = 0 . 4 3 5$ ) yielded $n \\approx 9 4$ images to keep the $9 5 \\ \\%$ CI of $\\kappa$ within $\\pm 0 . 1 5$ (Sim–Wright approximation); a conservative Donner–Eliasziw test for demonstrating $\\kappa > 0 . 4 0$ with $8 0 ~ \\%$ power suggested $n \\approx 1 0 8$ . All analyzes were conducted in Python (v3.10) using NumPy, pandas and scikit-learn.\n\nTable 2. Pre-specified endpoints and corresponding metrics.   \n\n<html><body><table><tr><td>Endpoint</td><td>Metric</td></tr><tr><td>Primary</td><td>Bland-Altman analysis (bias and limits of agreement,bias ± 1.96 SD) Mean absolute error (MAE) and root mean squared error (RMSE)</td></tr><tr><td>Secondary Mean ± SD of Cobb angles（)</td><td>Pearson's r between paired measurements Cohen'sκ for four-grade severity</td></tr></table></body></html>\n\n# 3 Results\n\nBland–Altman analysis comparing AI software with Radiologist 1 yielded a mean bias of $0 . 7 0 ^ { \\circ }$ ( $9 5 \\ \\%$ CI $- 0 . 2 3 ^ { \\circ }$ to $1 . 6 2 ^ { \\circ }$ ) and limits of agreement from $- 8 . 5 9 ^ { \\circ }$ to $9 . 9 9 ^ { \\circ }$ . The corresponding mean absolute error was $3 . 8 9 ^ { \\circ }$ and the root-meansquared error was $4 . 7 7 ^ { \\circ }$ . Versus Radiologist 2, the AI showed a bias of $2 . 1 4 ^ { \\circ }$ ( $9 5 \\ \\%$ CI $1 . 1 0 ^ { \\circ }$ to $3 . 1 7 ^ { \\circ }$ ) with limits of agreement $- 8 . 2 3 ^ { \\circ }$ to $1 2 . 5 0 ^ { \\circ }$ , MAE $3 . 9 0 ^ { \\circ }$ and RMSE $5 . 6 8 ^ { \\circ }$ . Inter-radiologist comparison produced a bias of $1 . 4 4 ^ { \\circ }$ ( $9 5 \\ \\%$ CI $0 . 6 6 ^ { \\circ }$ to $2 . 2 3 ^ { \\circ }$ ), limits of agreement $- 6 . 4 4 ^ { \\circ }$ to $9 . 3 2 ^ { \\circ }$ , MAE $3 . 3 0 ^ { \\circ }$ and RMSE $4 . 2 5 ^ { \\circ }$ . Overall, the AI’s systematic deviation from each radiologist was small and its variability fell within the same range observed between the two human readers (Table 3, Figure 3).\n\nTable 3. Bland–Altman and error-metric results.   \n\n<html><body><table><tr><td>Comparison</td><td>Bias (95% CI) [lower,upper]</td><td>LoA (°) [lower,upper]</td><td>MAE()RMSE()</td><td></td></tr><tr><td>AIvsRadiologist 1</td><td>0.70[-0.23,1.62][-8.59,9.99]</td><td></td><td>3.89</td><td>4.77</td></tr><tr><td>AI vsRadiologist 2</td><td>2.14[1.10,3.17][-8.23,12.50]</td><td></td><td>3.90</td><td>5.68</td></tr><tr><td>Radiologist 1 vs Radiologist 2 1.44 [0.66,2.23] [-6.44,9.32]</td><td></td><td></td><td>3.30</td><td>4.25</td></tr></table></body></html>\n\n![](images/94813930b322efc19cf18d110bf58e08cd7a047e702fcbe0935ea4ddb040aa54.jpg)  \nAl vs Radiologist 1   \nAl vs Radiologist 2   \nFig. 3. Bland–Altman analysis of Cobb-angle differences between AI software and each radiologist (RAD 1, RAD 2)\n\nContinuous Cobb angles averaged $1 3 . 1 2 ^ { \\circ } \\pm 1 1 . 1 0 ^ { \\circ }$ for the AI, $1 2 . 4 3 ^ { \\circ } \\pm 1 0 . 7 0 ^ { \\circ }$ for Radiologist 1, and $1 0 . 9 9 ^ { \\circ } \\pm 9 . 3 9 ^ { \\circ }$ for Radiologist 2 (Table 4).\n\nTable 4. Descriptive statistics of Cobb angles with standard deviation (SD).   \n\n<html><body><table><tr><td>Reader</td><td>Cobb angle mean (°） SD（°)</td></tr><tr><td>AI</td><td>13.12 11.10</td></tr><tr><td>Radiologist 1</td><td>12.43 10.70</td></tr><tr><td>Radiologist 2</td><td>10.99 9.39</td></tr></table></body></html>\n\nPearson correlation coefficients were very high for all pairwise comparisons: $r = 0 . 9 0 6$ (95 % CI 0.864–0.936) for AI vs. Radiologist 1, $r = 0 . 8 8 0$ ( $9 5 \\ \\%$ CI 0.827–0.917) for AI vs. Radiologist 2, and $r = 0 . 9 2 8$ ( $9 5 \\ \\%$ CI 0.895–0.951) for Radiologist 1 vs. Radiologist 2 (Table 5, Figure 4).\n\nTable 5. Pearson’s $r$ with $9 5 \\%$ confidence intervals (CI).   \n\n<html><body><table><tr><td>Comparison</td><td>r 95% CI</td></tr><tr><td>AI vs Radiologist 1</td><td>0.906 [0.864,0.936]</td></tr><tr><td>AI vs Radiologist 2</td><td>0.880 [0.827, 0.917]</td></tr><tr><td>Radiologist 1 vs Radiologist 2 0.928 [0.895,0.951]</td><td></td></tr></table></body></html>\n\n![](images/b921a663043243f66ecd8400fd21584bd0ad2a8c05d22dc22dae75aff1c8593d.jpg)  \nFig. 4. Scatter plots and linear regression of Cobb-angle measurements for AI software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) versus each radiologist.\n\nWhen Cobb angles were classified into four severity grades (none ${ < } 1 0 ^ { \\circ }$ , mild $1 0 – 2 4 ^ { \\circ }$ , moderate $2 5 \\mathrm { - } 4 4 ^ { \\circ }$ , severe $\\geq 4 5 ^ { \\circ }$ ), Cohen’s $\\kappa$ indicated moderate to substantial agreement: $\\kappa { = } 0 . 5 1$ for AI vs. Radiologist 1, $\\kappa { = } 0 . 6 4$ for AI vs. Radiologist 2, and $\\kappa { = } 0 . 5 9$ for Radiologist 1 vs. Radiologist 2 (Table 6, Figure 5).\n\nTable 6. Cohen’s $\\kappa$ for four-grade severity classification.   \n\n<html><body><table><tr><td>Comparison K</td></tr><tr><td>AI vs Radiologist 1 0.51</td></tr><tr><td>AI vs Radiologist 2 0.64</td></tr><tr><td>Radiologist 1 vs Radiologist 2 0.59</td></tr></table></body></html>\n\n![](images/d5fe32d14961fda3d48635ffc4a3e9379540873d36a5bc435b5b7213c9a4e254.jpg)  \nFig. 5. Confusion matrices comparing four-grade scoliosis severity classifications.\n\nRegarding power analysis, final cohort of 103 whole-spine X-rays exceeds the requirement for MAE precision and lies within the range targeted to bound the $9 5 \\ \\%$ confidence interval of Cohen’s $\\kappa$ to $\\pm 0 . 1 5$ .\n\n# 4 Discussion\n\nIn this study we evaluated the performance of AI software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) for automated Cobb-angle measurement against two expert radiologists. Across multiple metrics, the AI achieved human-level accuracy. Pearson correlation coefficients exceeded 0.88 for both AI-radiologist comparisons ( $r ~ = ~ 0 . 9 1$ vs. Radiologist 1; $r ~ = ~ 0 . 8 8$ vs. Radiologist 2), rivaling the inter-rater correlation of $r ~ = ~ 0 . 9 3$ . Mean absolute error (MAE) between AI and each radiologist $( \\approx 3 . 9 ^ { \\circ } )$ was virtually identical to that observed between the two radiologists themselves $( 3 . 3 ^ { \\circ } )$ , and root-mean-squared error (RMSE) differences fell within the same 4– $6 ^ { \\circ }$ range. Bland–Altman analysis revealed small systematic biases (0.7° toward overestimation versus Radiologist 1; 2.1° versus Radiologist 2) and limits of agreement spanning $\\pm 8 – 1 2 ^ { \\circ }$ , again mirroring inter-observer variability $( \\pm 6 { - } 9 ^ { \\circ } )$ . When Cobb angles were categorized into none/mild/moderate/severe grades (Figure 6), Cohen’s $\\kappa$ between AI and each radiologist ( $\\kappa = 0 . 5 1$ and 0.64) was on par with the $\\kappa = 0 . 5 9$ seen between humans, with most misclassifications confined to adjacent categories.\n\n![](images/db47330915b1dcf61f4d046348c397e0404fe759c63e77f265c357c6056f2b35.jpg)  \nFig. 6. Representative whole-spine X-rays illustrating four severity classes used in this study, with fully automated overlays generated by the proposed AI software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.).\n\nRecent literature confirms our findings. Landmark- or segmentation-based AI algorithms have achieved MAE values of $2 ^ { - 4 ^ { \\circ } }$ in single-centre evaluations [4,5], with ICCs up to 0.994 and Pearson $r \\ > \\ 0 . 9 8$ . A 2024 meta-analysis of 17 studies [6] reported a pooled mean error of $3 . 0 ^ { \\circ }$ ( $9 5 \\ \\%$ CI 2.6–3.4°) and confirmed that segmentation approaches outperform landmark detectors (2.4° vs 3.3°). Early multi-centre efforts have also suggested good generalisability; for example, Hayashi et al. [7] reported $\\mathrm { I C C } \\approx 0 . 9 6$ across paediatric and adult datasets, while Wong et al. [8] showed mean differences $< 3 ^ { \\circ }$ in a purely paediatric cohort. Nevertheless, most prior studies relied on retrospectively assembled, single-institution datasets, often restricted to adolescent idiopathic scoliosis and limited to one curve per film, which constrains external validity. By contrast, our multi-centre design, heterogeneous detector mix, and inclusion of complex, multi-curve cases demonstrate that expert-level performance can be maintained under real-world variability and thus directly address the generalisability gap identified in the current literature.\n\n# 4.1 Limitations\n\nAlthough our sample was drawn from ten different hospitals, the cohort remained skewed toward paediatric and adolescent examinations (81% under age\n\n20), which may limit generalizability to older adult populations. A modest number of severe curves $\\geq 4 0 ^ { \\circ }$ ) also reduced precision in the highest-angle bins. We did not assess the impact of AI integration on actual reporting time or downstream clinical decisions. Finally, while radiologist evaluation served as our comparison reference standard, Cobb angle measurement inherently involves some human error.\n\n# 5 Conclusions\n\nThe AI software demonstrated expert-level performance for automated Cobb angle measurement and severity grading, with agreement comparable to that between musculoskeletal radiologists. Its integration into clinical workflows can streamline scoliosis assessment, enhance consistency, and support efficient triage. Further prospective multicenter validation is needed to ascertain its impact on clinical decision-making and workflow efficiency.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   脊柱侧弯（Scoliosis）是一种影响全球2-4%青少年和8%成人的三维脊柱畸形，其诊断和治疗决策依赖于Cobb角的精确测量。传统手动测量方法存在耗时且观察者间差异大的问题（差异可达5-10°）。\\n> *   该问题在临床工作流中具有关键价值，因为精确的Cobb角测量直接影响患者的治疗路径（观察、支具治疗或手术转诊）。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种基于深度学习的全自动软件（Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.），通过两阶段方法（YOLOv11椎体关键点检测+几何算法计算Cobb角）实现脊柱侧弯的自动化评估。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **多中心验证：** 在10家医院的103张全脊柱X光片上验证，结果显示AI与两位放射科医生的测量一致性达到专家水平（MAE ≈3.9°，Pearson r >0.88）。\\n> *   **临床实用性：** AI的严重程度分类（无/轻度/中度/重度）与放射科医生的Cohen’s κ达到0.51-0.64，与医生间的一致性（κ=0.59）相当。\\n> *   **泛化能力：** 研究覆盖了不同年龄（1.8-62.5岁）和多中心数据，证明了算法在真实临床场景中的鲁棒性。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   通过深度学习定位椎体关键点（C7-L5的上下角），再基于几何原理计算最大Cobb角，模拟放射科医生的测量流程。其有效性源于大规模标注数据（39,100个椎体关键点）和两阶段设计的误差控制。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 以往研究多基于单中心或特定人群（如青少年特发性脊柱侧弯），泛化性受限。\\n> *   **本文的改进：** 采用多中心异构数据训练，并集成到临床PACS系统（支持DICOMweb/DIMSE），实现端到端自动化。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **关键点检测：** 使用YOLOv11模型在575张标注X光片上检测椎体上下角。\\n> 2.  **Cobb角计算：** 基于检测到的关键点，通过几何算法确定脊柱曲率最大处的上下终板夹角。\\n> 3.  **严重度分类：** 根据Cobb角阈值（<10°、10-24°、25-39°、≥40°）输出四类分级。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   两位肌肉骨骼放射科医生（Radiologist 1和Radiologist 2）的独立测量结果作为参考标准。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在Cobb角测量误差（MAE）上：** AI与Radiologist 1的MAE为3.89°，与Radiologist 2为3.90°，与两位医生间的MAE（3.30°）接近，表明AI达到专家水平精度。\\n> *   **在一致性（Pearson r）上：** AI与两位医生的相关系数分别为0.906和0.880，接近医生间相关性（r=0.928）。\\n> *   **在分类一致性（Cohen’s κ）上：** AI与医生的κ值为0.51-0.64，与医生间κ值（0.59）相当，且错误多集中在相邻分类。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   脊柱侧弯 (Scoliosis, N/A)\\n*   Cobb角测量 (Cobb Angle Measurement, N/A)\\n*   深度学习 (Deep Learning, DL)\\n*   多中心验证 (Multi-Centre Validation, N/A)\\n*   医学影像分析 (Medical Image Analysis, N/A)\\n*   观察者间一致性 (Interobserver Agreement, N/A)\\n*   YOLOv11 (You Only Look Once version 11, YOLOv11)\\n*   临床工作流自动化 (Clinical Workflow Automation, N/A)\"\n}\n```"
}