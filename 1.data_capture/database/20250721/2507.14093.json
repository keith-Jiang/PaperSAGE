{
    "source": "ArXiv (Semantic Scholaræœªæ”¶å½•)",
    "arxiv_id": "2507.14093",
    "link": "https://arxiv.org/abs/2507.14093",
    "pdf_link": "https://arxiv.org/pdf/2507.14093.pdf",
    "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
    "authors": [
        "Å imon Kubov",
        "Simon KlÃ­ÄnÃ­k",
        "Jakub DandÃ¡r",
        "ZdenÄ›k Straka",
        "KarolÃ­na KvakovÃ¡",
        "Daniel Kvak"
    ],
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
    ],
    "publication_date": "æœªæ‰¾åˆ°æäº¤æ—¥æœŸ",
    "venue": "æš‚æœªå½•å…¥Semantic Scholar",
    "fields_of_study": "æš‚æœªå½•å…¥Semantic Scholar",
    "citation_count": "æš‚æœªå½•å…¥Semantic Scholar",
    "influential_citation_count": "æš‚æœªå½•å…¥Semantic Scholar",
    "institutions": [
        "Thomayer University Hospital",
        "University Hospital KrÃ¡lovskÃ© Vinohrady",
        "3rd Faculty of Medicine",
        "Carebot, Ltd.",
        "Masaryk University",
        "Carebot s.r.o."
    ],
    "paper_content": "# Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment\n\nÅ imon Kubov $^ 1$ , Simon KlÃ­ÄnÃ­k $^ 2$ , Jakub DandÃ¡r $^ 3$ , ZdenÄ›k Straka3[0000âˆ’0002âˆ’2788âˆ’1667], KarolÃ­na KvakovÃ¡ $^ { 3 }$ , and Daniel Kvak3,4[0000âˆ’0001âˆ’7808âˆ’7773]\n\nDepartment of Radiogiagnostics, Thomayer University Hospital, Prague, Czechia   \n$^ 2$ Department of Radiology and Nuclear Medicine, University Hospital KrÃ¡lovskÃ© Vinohrady and 3rd Faculty of Medicine, Prague, Czechia $^ 3$ Research & Development Department, Carebot, Ltd., Prague, Czechia daniel.kvak@carebot.com   \n$^ 4$ Department of Simulation Medicine, Masaryk University, 625 00 Brno, Czechia\n\nAbstract. Scoliosis affects roughly $2 { - } 4 \\%$ of adolescents, and treatment decisions depend on precise Cobb-angle measurement. Manual assessment is slow and subject to inter-observer variation. We conducted a retrospective, multi-centre evaluation of a fully automated deep-learning software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on 103 standing anteroposterior whole-spine radiographs collected from ten hospitals. Two musculoskeletal radiologists independently measured each study and served as reference readers. Agreement between the AI and each radiologist was assessed with Blandâ€“Altman analysis, mean absolute error (MAE), root-mean-squared error (RMSE), Pearson correlation coefficient, and Cohenâ€™s $\\kappa$ for four-grade severity classification. Against Radiologist 1 the AI achieved an MAE of $3 . 8 9 ^ { \\circ }$ (RMSE $4 . 7 7 ^ { \\circ }$ ) with a bias of $0 . 7 0 ^ { \\circ }$ (limits of agreement $- 8 . 5 9 ^ { \\circ }$ to $9 . 9 9 ^ { \\mathrm { { o } } }$ ); against Radiologist 2 it achieved an MAE of $3 . 9 0 ^ { \\circ }$ (RMSE $5 . 6 8 ^ { \\circ }$ ) with a bias of $2 . 1 4 ^ { \\circ }$ (limits $- 8 . 2 3 ^ { \\circ }$ to $1 2 . 5 0 ^ { \\circ }$ ). Pearson correlations were $r = 0 . 9 0 6$ and $r = 0 . 8 8 0$ (inter-reader $r = 0 . 9 2 8$ ), while Cohenâ€™s $\\kappa$ for severity grading reached 0.51 and 0.64 (inter-reader $\\kappa = 0 . 5 9$ ). These results show that the proposed software reproduces expert-level Cobb-angle measurements and categorical grading across multiple centres, suggesting its utility for streamlining scoliosis reporting and triage in clinical workflows.\n\nKeywords: Artificial intelligence Â· Cobb angle Â· Deep learning Â· Interobserver agreement Â· Scoliosis $\\cdot$ Spine deformities\n\n# 1 Introduction\n\nScoliosis, a three-dimensional deformity characterised radiographically by a lateral curvature of the spine, affects roughly 2â€“4% of adolescents and up to 8% of adults worldwide [1]. The magnitude of curvature, expressed by the Cobb angle on an upright anteroposterior (AP) full-length spine X-ray, determines both the diagnosis ( $\\geq 1 0 ^ { \\circ }$ ) and the clinical pathway: curves of $1 0 { - } 2 4 ^ { \\circ }$ are usually observed, $2 5 \\mathrm { - } 3 9 ^ { \\circ }$ prompt bracing, and angles $\\geq 4 0 - 5 0 ^ { \\circ }$ often lead to surgical referral [2]. Precise measurement is therefore critical during screening and longitudinal follow-up, yet manual Cobb angle tracing is time-consuming and suffers from inter-observer discrepancies of $5 \\mathrm { - } 1 0 ^ { \\circ }$ even among experienced readers [3].\n\nOver the last years, deep-learning approaches have shown considerable promise in automating Cobb-angle assessment. Convolutional neural-network pipelines that first localise vertebral landmarks and then derive the maximal end-plate angle have demonstrated close agreement with expert measurements in single-centre studies [4,5]. Building on these results, Zhu et al. conducted a comprehensive meta-analysis of 14 models and confirmed that segmentation-based architectures generally outperform landmark-based ones [6]. More recently, multi-centre evaluations encompassing both adult and paediatric radiographs have reported near-radiologist reliability across different vendors and demographic groups [7,8]. Despite this progress, most published algorithms have been developed and tested on retrospective datasets from single institutions or on narrowly defined populations (e.g. adolescent idiopathic scoliosis), which limits their generalisability.\n\nTo address this gap, we developed a deep-learning model that (i) detects vertebral landmarks on whole-spine anteroposterior (AP) radiographs and (ii) calculates the maximal Cobb angle to categorise scoliosis severity. In this multi-centre validation study, we test the hypothesis that the modelâ€™s severity classification achieves agreement comparable to expert musculoskeletal radiologists when benchmarked against reference Cobb angles measured independently by two such readers.\n\n# 2 Materials and Methods\n\n# 2.1 Software\n\nThe proposed AI software is a deep-learning-based solution (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o., Czechia) that detects vertebral landmarks on whole-spine anterior-posterior (AP) X-rays, and computes the Cobb angle to classify severity of the scoliosis as no scoliosis ( $< 1 0 ^ { \\circ }$ ), mild (10â€“24Â°), moderate (25â€“39Â°) or severe $( \\geq 4 0 ^ { \\circ } )$ ). The software implements a twostage approach: (a) YOLOv11 landmark detector, trained on 575 expertly annotated whole-spine X-ray images (i.e. 39,100 vertebral keypoints) to localise the superior and inferior corners of vertebrae C7â€“L5; and (b) geometry-based algorithm computes Cobb angles from these landmarks. Delivered as a self-contained application, it integrates directly into clinical PACS via both DICOMweb and DIMSE (Figure 1), automating study retrieval and result insertion.\n\n![](images/6b5b73379228684aba1861b74ec049954978ae3a2494851a409246d7d40c0005.jpg)  \nFig. 1. Demo.\n\n# 2.2 Data collection\n\nA total of 125 full-length standing anterior-posterior spine X-rays acquired between 1 and 18 May 2025 were collected from 10 participating hospitals. After applying inclusion and exclusion criteria (Table 1), 22 studies were excluded and the final analysis set comprised 103 radiographs collected across ten hospitalsâ€”Hospital HoÅ™ovice ( $n \\ = \\ 4 8$ ), University Hospital Olomouc ( $n \\ : = \\ : 2 6 \\$ ), Hospital FrÃ½dek-MÃ­stek ( $n = 1 2$ ), Hospital KarvinÃ¡-RÃ¡j ( $n = 4$ ), Surgical Disciplines Prague ( $n = 3$ ), Hospital AGEL NovÃ½ JiÄÃ­n ( $n = 3$ ), TÅ™inec Hospital ( $n = 2$ ), Silesian Hospital in Opava ( $n = 2$ ), Regional Hospital NÃ¡chod ( $n = 2$ ), and Regional Hospital PÅ™Ã­bram ( $n = 1$ ). Given its retrospective nature and full anonymization, the requirement of informed consent was waived. All DICOM files were deidentified according to PS 3.15 Basic Application Confidentiality Profile, with removal of direct identifiers.\n\nTable 1. Eligibility inclusion and exclusion criteria.   \n\n<html><body><table><tr><td colspan=\"2\">CriterionDetail</td></tr><tr><td>Inclusion</td><td>All standing AP spine radiographs acquired between 1 and 18 May 2025 from patients aged â‰¥1 year.</td></tr><tr><td></td><td>Exclusion (iï¼‰ unreadable or incomplete images;(iiï¼‰ postoperative instru- mentation obscuringâ‰¥ 3 vertebrae;(iii) duplicate studies.</td></tr></table></body></html>\n\nThe cohort demonstrated a predominantly paediatric and adolescent profile: mean age $1 8 . 6 \\pm 1 3 . 3$ years, median 14 years (interquartile range 11â€“16 years), range 1.8â€“62.5 years; $8 1 \\%$ of radiographs were from patients younger than 20 years (Figure 2). Females slightly outnumbered males, accounting for 60 (58%) versus 43 (42%) X-rays, respectively.\n\n![](images/792eb9a419265cfc3947c87bf37836feda3a22dab00bf983fd323ad842d7c4da.jpg)  \nFig. 2. Age distribution of patients by hospital and sex. The violin plot displays kernel density estimation of age for each group, split by sex (Female ${ } = { }$ blue, Male $=$ orange).\n\n# 2.3 Study Design\n\nThis retrospective, multi-centre diagnostic accuracy study included all standing AP full-spine X-rays. Two musculoskeletal radiologists (Radiologist 1, Radiologist 2) independently measured the maximal Cobb angle on each X-ray using the institutional PACS tool. In parallel, the AI software analysed the same DICOM images, without manual intervention, to yield one automated Cobb angle per study. Readers and AI were blinded to each otherâ€™s results during measurement. For analysis, each AI measurement was compared to RAD1 and to RAD2 using paired design.\n\n# 2.4 Statistical Analysis and Endpoints\n\nThe agreement of continuous measurements was evaluated by Bland-Altman analysis. For each subject $i$ , the difference $d _ { i } = m _ { 1 , i } - m _ { 2 , i }$ was computed (where $^ { m _ { 1 , i } }$ and $m _ { 2 , i }$ are two readersâ€™ angles), and the bias $\\begin{array} { r } { d = \\frac { 1 } { N } \\sum _ { i } d _ { i } } \\end{array}$ was reported along with limits of agreement $d \\pm 1 . 9 6 s _ { d }$ (with $s _ { d }$ the SD of $d _ { i }$ ). Mean absolute error $\\begin{array} { r } { \\left( \\mathrm { M A E } = \\frac { 1 } { N } \\sum _ { i } | d _ { i } | \\right) } \\end{array}$ and root-mean-squared error $\\begin{array} { r } { \\mathrm { ( R M S E = \\sqrt { \\frac { 1 } { N } \\sum _ { i } d _ { i } ^ { 2 } } ) } } \\end{array}$ were also calculated. Continuous Cobb angles from the AI software and each radiologist were reported as mean $\\pm$ standard deviation (SD). The linear association between two readers was quantified by Pearsonâ€™s correlation coefficient,\n\n$$\nr = { \\frac { \\sum _ { i } ( x _ { i } - { \\bar { x } } ) ( y _ { i } - { \\bar { y } } ) } { \\sqrt { \\sum _ { i } ( x _ { i } - { \\bar { x } } ) ^ { 2 } \\sum _ { i } ( y _ { i } - { \\bar { y } } ) ^ { 2 } } } } ,\n$$\n\nwhere $x _ { i }$ and $y _ { i }$ are paired measurements and $x , y$ their means. For categorical agreement, Cobb angles were stratified into four grades (no scoliosis $< 1 0 ^ { \\circ }$ , mild $1 0 { - } 2 4 ^ { \\circ }$ , moderate $2 5 \\mathrm { - } 4 4 ^ { \\circ }$ , severe $\\geq 4 5 ^ { \\circ }$ ). Pairwise agreement was summarized by Cohenâ€™s kappa,\n\n$$\n\\kappa = { \\frac { p _ { o } - p _ { e } } { 1 - p _ { e } } }\n$$\n\nwhere $p _ { o }$ is the observed agreement and $p _ { e }$ the chance agreement. Before data extraction, a power analysis determined the minimum sample size required for achievement of endpoints (Table 2). Assuming a standard deviation of absolute errors $\\sigma = 4 . 5 ^ { \\circ }$ , at least\n\n$$\nn = \\left( { \\frac { 1 . 9 6 \\times 4 . 5 } { 1 . 0 } } \\right) ^ { 2 } \\approx 7 8\n$$\n\nwhole-spine X-rays were needed to bound the two-sided $9 5 \\ \\%$ CI around the sample MAE within $\\pm 1 ^ { \\circ }$ . For the secondary endpoint, an expected $\\kappa =$ 0.60 and class distribution $\\{ 6 0 \\% , 2 5 \\% , 1 0 \\% , 5 \\% \\}$ (chance agreement $P _ { e } = 0 . 4 3 5$ ) yielded $n \\approx 9 4$ images to keep the $9 5 \\ \\%$ CI of $\\kappa$ within $\\pm 0 . 1 5$ (Simâ€“Wright approximation); a conservative Donnerâ€“Eliasziw test for demonstrating $\\kappa > 0 . 4 0$ with $8 0 ~ \\%$ power suggested $n \\approx 1 0 8$ . All analyzes were conducted in Python (v3.10) using NumPy, pandas and scikit-learn.\n\nTable 2. Pre-specified endpoints and corresponding metrics.   \n\n<html><body><table><tr><td>Endpoint</td><td>Metric</td></tr><tr><td>Primary</td><td>Bland-Altman analysis (bias and limits of agreement,bias Â± 1.96 SD) Mean absolute error (MAE) and root mean squared error (RMSE)</td></tr><tr><td>Secondary Mean Â± SD of Cobb anglesï¼ˆ)</td><td>Pearson's r between paired measurements Cohen'sÎº for four-grade severity</td></tr></table></body></html>\n\n# 3 Results\n\nBlandâ€“Altman analysis comparing AI software with Radiologist 1 yielded a mean bias of $0 . 7 0 ^ { \\circ }$ ( $9 5 \\ \\%$ CI $- 0 . 2 3 ^ { \\circ }$ to $1 . 6 2 ^ { \\circ }$ ) and limits of agreement from $- 8 . 5 9 ^ { \\circ }$ to $9 . 9 9 ^ { \\circ }$ . The corresponding mean absolute error was $3 . 8 9 ^ { \\circ }$ and the root-meansquared error was $4 . 7 7 ^ { \\circ }$ . Versus Radiologist 2, the AI showed a bias of $2 . 1 4 ^ { \\circ }$ ( $9 5 \\ \\%$ CI $1 . 1 0 ^ { \\circ }$ to $3 . 1 7 ^ { \\circ }$ ) with limits of agreement $- 8 . 2 3 ^ { \\circ }$ to $1 2 . 5 0 ^ { \\circ }$ , MAE $3 . 9 0 ^ { \\circ }$ and RMSE $5 . 6 8 ^ { \\circ }$ . Inter-radiologist comparison produced a bias of $1 . 4 4 ^ { \\circ }$ ( $9 5 \\ \\%$ CI $0 . 6 6 ^ { \\circ }$ to $2 . 2 3 ^ { \\circ }$ ), limits of agreement $- 6 . 4 4 ^ { \\circ }$ to $9 . 3 2 ^ { \\circ }$ , MAE $3 . 3 0 ^ { \\circ }$ and RMSE $4 . 2 5 ^ { \\circ }$ . Overall, the AIâ€™s systematic deviation from each radiologist was small and its variability fell within the same range observed between the two human readers (Table 3, Figure 3).\n\nTable 3. Blandâ€“Altman and error-metric results.   \n\n<html><body><table><tr><td>Comparison</td><td>Bias (95% CI) [lower,upper]</td><td>LoA (Â°) [lower,upper]</td><td>MAE()RMSE()</td><td></td></tr><tr><td>AIvsRadiologist 1</td><td>0.70[-0.23,1.62][-8.59,9.99]</td><td></td><td>3.89</td><td>4.77</td></tr><tr><td>AI vsRadiologist 2</td><td>2.14[1.10,3.17][-8.23,12.50]</td><td></td><td>3.90</td><td>5.68</td></tr><tr><td>Radiologist 1 vs Radiologist 2 1.44 [0.66,2.23] [-6.44,9.32]</td><td></td><td></td><td>3.30</td><td>4.25</td></tr></table></body></html>\n\n![](images/94813930b322efc19cf18d110bf58e08cd7a047e702fcbe0935ea4ddb040aa54.jpg)  \nAl vs Radiologist 1   \nAl vs Radiologist 2   \nFig. 3. Blandâ€“Altman analysis of Cobb-angle differences between AI software and each radiologist (RAD 1, RAD 2)\n\nContinuous Cobb angles averaged $1 3 . 1 2 ^ { \\circ } \\pm 1 1 . 1 0 ^ { \\circ }$ for the AI, $1 2 . 4 3 ^ { \\circ } \\pm 1 0 . 7 0 ^ { \\circ }$ for Radiologist 1, and $1 0 . 9 9 ^ { \\circ } \\pm 9 . 3 9 ^ { \\circ }$ for Radiologist 2 (Table 4).\n\nTable 4. Descriptive statistics of Cobb angles with standard deviation (SD).   \n\n<html><body><table><tr><td>Reader</td><td>Cobb angle mean (Â°ï¼‰ SDï¼ˆÂ°)</td></tr><tr><td>AI</td><td>13.12 11.10</td></tr><tr><td>Radiologist 1</td><td>12.43 10.70</td></tr><tr><td>Radiologist 2</td><td>10.99 9.39</td></tr></table></body></html>\n\nPearson correlation coefficients were very high for all pairwise comparisons: $r = 0 . 9 0 6$ (95 % CI 0.864â€“0.936) for AI vs. Radiologist 1, $r = 0 . 8 8 0$ ( $9 5 \\ \\%$ CI 0.827â€“0.917) for AI vs. Radiologist 2, and $r = 0 . 9 2 8$ ( $9 5 \\ \\%$ CI 0.895â€“0.951) for Radiologist 1 vs. Radiologist 2 (Table 5, Figure 4).\n\nTable 5. Pearsonâ€™s $r$ with $9 5 \\%$ confidence intervals (CI).   \n\n<html><body><table><tr><td>Comparison</td><td>r 95% CI</td></tr><tr><td>AI vs Radiologist 1</td><td>0.906 [0.864,0.936]</td></tr><tr><td>AI vs Radiologist 2</td><td>0.880 [0.827, 0.917]</td></tr><tr><td>Radiologist 1 vs Radiologist 2 0.928 [0.895,0.951]</td><td></td></tr></table></body></html>\n\n![](images/b921a663043243f66ecd8400fd21584bd0ad2a8c05d22dc22dae75aff1c8593d.jpg)  \nFig. 4. Scatter plots and linear regression of Cobb-angle measurements for AI software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) versus each radiologist.\n\nWhen Cobb angles were classified into four severity grades (none ${ < } 1 0 ^ { \\circ }$ , mild $1 0 â€“ 2 4 ^ { \\circ }$ , moderate $2 5 \\mathrm { - } 4 4 ^ { \\circ }$ , severe $\\geq 4 5 ^ { \\circ }$ ), Cohenâ€™s $\\kappa$ indicated moderate to substantial agreement: $\\kappa { = } 0 . 5 1$ for AI vs. Radiologist 1, $\\kappa { = } 0 . 6 4$ for AI vs. Radiologist 2, and $\\kappa { = } 0 . 5 9$ for Radiologist 1 vs. Radiologist 2 (Table 6, Figure 5).\n\nTable 6. Cohenâ€™s $\\kappa$ for four-grade severity classification.   \n\n<html><body><table><tr><td>Comparison K</td></tr><tr><td>AI vs Radiologist 1 0.51</td></tr><tr><td>AI vs Radiologist 2 0.64</td></tr><tr><td>Radiologist 1 vs Radiologist 2 0.59</td></tr></table></body></html>\n\n![](images/d5fe32d14961fda3d48635ffc4a3e9379540873d36a5bc435b5b7213c9a4e254.jpg)  \nFig. 5. Confusion matrices comparing four-grade scoliosis severity classifications.\n\nRegarding power analysis, final cohort of 103 whole-spine X-rays exceeds the requirement for MAE precision and lies within the range targeted to bound the $9 5 \\ \\%$ confidence interval of Cohenâ€™s $\\kappa$ to $\\pm 0 . 1 5$ .\n\n# 4 Discussion\n\nIn this study we evaluated the performance of AI software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) for automated Cobb-angle measurement against two expert radiologists. Across multiple metrics, the AI achieved human-level accuracy. Pearson correlation coefficients exceeded 0.88 for both AI-radiologist comparisons ( $r ~ = ~ 0 . 9 1$ vs. Radiologist 1; $r ~ = ~ 0 . 8 8$ vs. Radiologist 2), rivaling the inter-rater correlation of $r ~ = ~ 0 . 9 3$ . Mean absolute error (MAE) between AI and each radiologist $( \\approx 3 . 9 ^ { \\circ } )$ was virtually identical to that observed between the two radiologists themselves $( 3 . 3 ^ { \\circ } )$ , and root-mean-squared error (RMSE) differences fell within the same 4â€“ $6 ^ { \\circ }$ range. Blandâ€“Altman analysis revealed small systematic biases (0.7Â° toward overestimation versus Radiologist 1; 2.1Â° versus Radiologist 2) and limits of agreement spanning $\\pm 8 â€“ 1 2 ^ { \\circ }$ , again mirroring inter-observer variability $( \\pm 6 { - } 9 ^ { \\circ } )$ . When Cobb angles were categorized into none/mild/moderate/severe grades (Figure 6), Cohenâ€™s $\\kappa$ between AI and each radiologist ( $\\kappa = 0 . 5 1$ and 0.64) was on par with the $\\kappa = 0 . 5 9$ seen between humans, with most misclassifications confined to adjacent categories.\n\n![](images/db47330915b1dcf61f4d046348c397e0404fe759c63e77f265c357c6056f2b35.jpg)  \nFig. 6. Representative whole-spine X-rays illustrating four severity classes used in this study, with fully automated overlays generated by the proposed AI software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.).\n\nRecent literature confirms our findings. Landmark- or segmentation-based AI algorithms have achieved MAE values of $2 ^ { - 4 ^ { \\circ } }$ in single-centre evaluations [4,5], with ICCs up to 0.994 and Pearson $r \\ > \\ 0 . 9 8$ . A 2024 meta-analysis of 17 studies [6] reported a pooled mean error of $3 . 0 ^ { \\circ }$ ( $9 5 \\ \\%$ CI 2.6â€“3.4Â°) and confirmed that segmentation approaches outperform landmark detectors (2.4Â° vs 3.3Â°). Early multi-centre efforts have also suggested good generalisability; for example, Hayashi et al. [7] reported $\\mathrm { I C C } \\approx 0 . 9 6$ across paediatric and adult datasets, while Wong et al. [8] showed mean differences $< 3 ^ { \\circ }$ in a purely paediatric cohort. Nevertheless, most prior studies relied on retrospectively assembled, single-institution datasets, often restricted to adolescent idiopathic scoliosis and limited to one curve per film, which constrains external validity. By contrast, our multi-centre design, heterogeneous detector mix, and inclusion of complex, multi-curve cases demonstrate that expert-level performance can be maintained under real-world variability and thus directly address the generalisability gap identified in the current literature.\n\n# 4.1 Limitations\n\nAlthough our sample was drawn from ten different hospitals, the cohort remained skewed toward paediatric and adolescent examinations (81% under age\n\n20), which may limit generalizability to older adult populations. A modest number of severe curves $\\geq 4 0 ^ { \\circ }$ ) also reduced precision in the highest-angle bins. We did not assess the impact of AI integration on actual reporting time or downstream clinical decisions. Finally, while radiologist evaluation served as our comparison reference standard, Cobb angle measurement inherently involves some human error.\n\n# 5 Conclusions\n\nThe AI software demonstrated expert-level performance for automated Cobb angle measurement and severity grading, with agreement comparable to that between musculoskeletal radiologists. Its integration into clinical workflows can streamline scoliosis assessment, enhance consistency, and support efficient triage. Further prospective multicenter validation is needed to ascertain its impact on clinical decision-making and workflow efficiency.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è„ŠæŸ±ä¾§å¼¯ï¼ˆScoliosisï¼‰æ˜¯ä¸€ç§å½±å“å…¨çƒ2-4%é’å°‘å¹´å’Œ8%æˆäººçš„ä¸‰ç»´è„ŠæŸ±ç•¸å½¢ï¼Œå…¶è¯Šæ–­å’Œæ²»ç–—å†³ç­–ä¾èµ–äºCobbè§’çš„ç²¾ç¡®æµ‹é‡ã€‚ä¼ ç»Ÿæ‰‹åŠ¨æµ‹é‡æ–¹æ³•å­˜åœ¨è€—æ—¶ä¸”è§‚å¯Ÿè€…é—´å·®å¼‚å¤§çš„é—®é¢˜ï¼ˆå·®å¼‚å¯è¾¾5-10Â°ï¼‰ã€‚\\n> *   è¯¥é—®é¢˜åœ¨ä¸´åºŠå·¥ä½œæµä¸­å…·æœ‰å…³é”®ä»·å€¼ï¼Œå› ä¸ºç²¾ç¡®çš„Cobbè§’æµ‹é‡ç›´æ¥å½±å“æ‚£è€…çš„æ²»ç–—è·¯å¾„ï¼ˆè§‚å¯Ÿã€æ”¯å…·æ²»ç–—æˆ–æ‰‹æœ¯è½¬è¯Šï¼‰ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å…¨è‡ªåŠ¨è½¯ä»¶ï¼ˆCarebot AI Bones, Spine Measurement functionality; Carebot s.r.o.ï¼‰ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•ï¼ˆYOLOv11æ¤ä½“å…³é”®ç‚¹æ£€æµ‹+å‡ ä½•ç®—æ³•è®¡ç®—Cobbè§’ï¼‰å®ç°è„ŠæŸ±ä¾§å¼¯çš„è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **å¤šä¸­å¿ƒéªŒè¯ï¼š** åœ¨10å®¶åŒ»é™¢çš„103å¼ å…¨è„ŠæŸ±Xå…‰ç‰‡ä¸ŠéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºAIä¸ä¸¤ä½æ”¾å°„ç§‘åŒ»ç”Ÿçš„æµ‹é‡ä¸€è‡´æ€§è¾¾åˆ°ä¸“å®¶æ°´å¹³ï¼ˆMAE â‰ˆ3.9Â°ï¼ŒPearson r >0.88ï¼‰ã€‚\\n> *   **ä¸´åºŠå®ç”¨æ€§ï¼š** AIçš„ä¸¥é‡ç¨‹åº¦åˆ†ç±»ï¼ˆæ— /è½»åº¦/ä¸­åº¦/é‡åº¦ï¼‰ä¸æ”¾å°„ç§‘åŒ»ç”Ÿçš„Cohenâ€™s Îºè¾¾åˆ°0.51-0.64ï¼Œä¸åŒ»ç”Ÿé—´çš„ä¸€è‡´æ€§ï¼ˆÎº=0.59ï¼‰ç›¸å½“ã€‚\\n> *   **æ³›åŒ–èƒ½åŠ›ï¼š** ç ”ç©¶è¦†ç›–äº†ä¸åŒå¹´é¾„ï¼ˆ1.8-62.5å²ï¼‰å’Œå¤šä¸­å¿ƒæ•°æ®ï¼Œè¯æ˜äº†ç®—æ³•åœ¨çœŸå®ä¸´åºŠåœºæ™¯ä¸­çš„é²æ£’æ€§ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   é€šè¿‡æ·±åº¦å­¦ä¹ å®šä½æ¤ä½“å…³é”®ç‚¹ï¼ˆC7-L5çš„ä¸Šä¸‹è§’ï¼‰ï¼Œå†åŸºäºå‡ ä½•åŸç†è®¡ç®—æœ€å¤§Cobbè§’ï¼Œæ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿçš„æµ‹é‡æµç¨‹ã€‚å…¶æœ‰æ•ˆæ€§æºäºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®ï¼ˆ39,100ä¸ªæ¤ä½“å…³é”®ç‚¹ï¼‰å’Œä¸¤é˜¶æ®µè®¾è®¡çš„è¯¯å·®æ§åˆ¶ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ä»¥å¾€ç ”ç©¶å¤šåŸºäºå•ä¸­å¿ƒæˆ–ç‰¹å®šäººç¾¤ï¼ˆå¦‚é’å°‘å¹´ç‰¹å‘æ€§è„ŠæŸ±ä¾§å¼¯ï¼‰ï¼Œæ³›åŒ–æ€§å—é™ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** é‡‡ç”¨å¤šä¸­å¿ƒå¼‚æ„æ•°æ®è®­ç»ƒï¼Œå¹¶é›†æˆåˆ°ä¸´åºŠPACSç³»ç»Ÿï¼ˆæ”¯æŒDICOMweb/DIMSEï¼‰ï¼Œå®ç°ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **å…³é”®ç‚¹æ£€æµ‹ï¼š** ä½¿ç”¨YOLOv11æ¨¡å‹åœ¨575å¼ æ ‡æ³¨Xå…‰ç‰‡ä¸Šæ£€æµ‹æ¤ä½“ä¸Šä¸‹è§’ã€‚\\n> 2.  **Cobbè§’è®¡ç®—ï¼š** åŸºäºæ£€æµ‹åˆ°çš„å…³é”®ç‚¹ï¼Œé€šè¿‡å‡ ä½•ç®—æ³•ç¡®å®šè„ŠæŸ±æ›²ç‡æœ€å¤§å¤„çš„ä¸Šä¸‹ç»ˆæ¿å¤¹è§’ã€‚\\n> 3.  **ä¸¥é‡åº¦åˆ†ç±»ï¼š** æ ¹æ®Cobbè§’é˜ˆå€¼ï¼ˆ<10Â°ã€10-24Â°ã€25-39Â°ã€â‰¥40Â°ï¼‰è¾“å‡ºå››ç±»åˆ†çº§ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   ä¸¤ä½è‚Œè‚‰éª¨éª¼æ”¾å°„ç§‘åŒ»ç”Ÿï¼ˆRadiologist 1å’ŒRadiologist 2ï¼‰çš„ç‹¬ç«‹æµ‹é‡ç»“æœä½œä¸ºå‚è€ƒæ ‡å‡†ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨Cobbè§’æµ‹é‡è¯¯å·®ï¼ˆMAEï¼‰ä¸Šï¼š** AIä¸Radiologist 1çš„MAEä¸º3.89Â°ï¼Œä¸Radiologist 2ä¸º3.90Â°ï¼Œä¸ä¸¤ä½åŒ»ç”Ÿé—´çš„MAEï¼ˆ3.30Â°ï¼‰æ¥è¿‘ï¼Œè¡¨æ˜AIè¾¾åˆ°ä¸“å®¶æ°´å¹³ç²¾åº¦ã€‚\\n> *   **åœ¨ä¸€è‡´æ€§ï¼ˆPearson rï¼‰ä¸Šï¼š** AIä¸ä¸¤ä½åŒ»ç”Ÿçš„ç›¸å…³ç³»æ•°åˆ†åˆ«ä¸º0.906å’Œ0.880ï¼Œæ¥è¿‘åŒ»ç”Ÿé—´ç›¸å…³æ€§ï¼ˆr=0.928ï¼‰ã€‚\\n> *   **åœ¨åˆ†ç±»ä¸€è‡´æ€§ï¼ˆCohenâ€™s Îºï¼‰ä¸Šï¼š** AIä¸åŒ»ç”Ÿçš„Îºå€¼ä¸º0.51-0.64ï¼Œä¸åŒ»ç”Ÿé—´Îºå€¼ï¼ˆ0.59ï¼‰ç›¸å½“ï¼Œä¸”é”™è¯¯å¤šé›†ä¸­åœ¨ç›¸é‚»åˆ†ç±»ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   è„ŠæŸ±ä¾§å¼¯ (Scoliosis, N/A)\\n*   Cobbè§’æµ‹é‡ (Cobb Angle Measurement, N/A)\\n*   æ·±åº¦å­¦ä¹  (Deep Learning, DL)\\n*   å¤šä¸­å¿ƒéªŒè¯ (Multi-Centre Validation, N/A)\\n*   åŒ»å­¦å½±åƒåˆ†æ (Medical Image Analysis, N/A)\\n*   è§‚å¯Ÿè€…é—´ä¸€è‡´æ€§ (Interobserver Agreement, N/A)\\n*   YOLOv11 (You Only Look Once version 11, YOLOv11)\\n*   ä¸´åºŠå·¥ä½œæµè‡ªåŠ¨åŒ– (Clinical Workflow Automation, N/A)\"\n}\n```"
}