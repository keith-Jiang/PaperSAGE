{
    "source": "ArXiv (Semantic ScholarÊú™Êî∂ÂΩï)",
    "arxiv_id": "2507.14088",
    "link": "https://arxiv.org/abs/2507.14088",
    "pdf_link": "https://arxiv.org/pdf/2507.14088.pdf",
    "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
    "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "Êú™ÊâæÂà∞Êèê‰∫§Êó•Êúü",
    "venue": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "fields_of_study": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "citation_count": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "influential_citation_count": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "institutions": [
        "The Key Laboratory of Cognition and Decision Intelligence for Complex Systems",
        "Institute of Automation",
        "Chinese Academy of Sciences",
        "School of Future Technology",
        "University of Chinese Academy of Sciences",
        "Nankai University",
        "Tsinghua University"
    ],
    "paper_content": "# DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration\n\nXiyun Li (lixiyun $\\pmb { 2 0 2 0 } \\textcircled { a }$ ia.ac.cn) 1,2,Yining Ding3,Yuhua Jiang4,Yunlong Zhao1, Runpeng Xie1, Shuang ${ \\bf { X } } { \\bf { u } } ^ { 1 }$ ,Yuanhua $\\mathbf { N i } ^ { 3 }$ , Yiqin Yang (yiqin.yang $@$ ia.ac.cn)1\\*,Bo Xu (xubo@ia.ac.cn)1,2\\*\n\n1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China 2School of Future Technology, University of Chinese Academy of Sciences, Beijing, 101408, China 3Nankai University, Tianjin, 300350, China 4Tsinghua University, Beijing, 100084, China \\*Corresponding author\n\n# Abstract\n\nReal-time human-artificial intelligence (AI) collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios. Existing large language model (LLM) agents often fail to accurately model the complex human mental characteristics such as domain intentions, especially in the absence of direct communication. To address this limitation, we propose a novel dual process multi-scale theory of mind (DPMT) framework, drawing inspiration from cognitive science‚Äôs dual process theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning. Experimental results demonstrate that DPMT significantly enhances humanAI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system.\n\nKeywords: theory of mind; human-AI collaboration; dual process theory; intelligent agents; large language model\n\n# Introduction\n\nIn recent years, large language models (LLMs) have achieved expert-level performance across various domains, including conversational question-answering assistants (Achiam et al., 2023) and code generation (Chen et al., 2021; R. Li et al., 2023). Building on the exceptional perception, comprehension, and reasoning capabilities of LLMs, LLM agents have rapidly emerged, attracting widespread attention. By integrating mechanisms such as memory and reflection, these agents demonstrate general-purpose abilities and robust generalization across tasks like operating system command execution (Deng et al., 2024; He et al., 2024), mobile assistance (J. Wang et al., 2024; C. Zhang et al., 2023), creative exploration in Minecraft (G. Wang et al., 2023), and embodied intelligence scenarios (Brohan et al., 2023).\n\nHowever, achieving effective adaptation in complex collaborative tasks remains a significant challenge, which is critical for the broader application of LLM agents. Unlike tasks that an LLM agent can perform independently, collaborative tasks like Overcooked (Ghost Town Games, 2016) require the agent to work with diverse partners, including humans, to efficiently complete a series of complex sub-tasks‚Äîsuch as retrieving ingredients, cooking, and serving dishes within a limited time. The task list in Overcooked is too intricate for a single agent to handle alone. Furthermore, human collaborators, constrained by bounded rationality, may lack complete domain knowledge, introducing additional instability and uncertainty in human-artificial intelligence (AI) collaboration.\n\nThus, effectively modeling human cognition is key to improving adaptation and collaboration.\n\nRecent research has focused on improving the adaptability and decision-making capabilities of LLM agents by facilitating communication between human players and AI agents in environments with diverse human partners (Liu et al., 2023; S. Zhang et al., 2024). Leveraging their advanced natural language understanding, LLM agents can interpret human commands and formulate subsequent action plans (Liu et al., 2023). By adjusting their collaboration strategies through mutual communication with human partners, LLM agents significantly improve the overall collaboration performance of human-AI teams (S. Zhang et al., 2024).\n\nHowever, the improved communication alone does not fully address the challenges of real-time scenarios. Current LLM agents lack the human-like cognitive ability known as ‚Äútheory of mind‚Äù (ToM), which allows humans to understand and predict others‚Äô mental beliefs based on observed behaviors in social environment (Astington & Jenkins, 1995). This ability facilitates efficient collaboration in tasks without direct communication. The absence of ToM hinders LLM agents‚Äô performance in complex, real-time human-AI collaborative tasks. Although recent studies have explored ToM modeling to improve agent prediction (Rabinowitz et al., 2018; X. Li et al., 2023), these approaches rely on highquality trajectories and prior knowledge of partners. As a result, their interpretability and generalization are limited, restricting their applicability in real collaborative scenarios.\n\nInspired by dual process theory (Vaisey, 2009; Lizardo et al., 2016), we propose a cognitive dual process multi-scale theory of mind (DPMT) to improve the interpretability and efficiency of human partner modeling in real-time human-AI collaboration. Our DPMT framework distinguishes between two decision-making systems for real-time human-AI collaboration: a fast system for automatic decisions and a slow system for modeling higher-level cognitive abilities. The core contribution of this work is the development of a multi-scale theory of mind module to simulate the slow system for understanding human partners‚Äô behavioral trajectories and reasoning about their mental characteristics, facilitating more effective collaboration. This ToM process follows a three-tiered reasoning process, which progresses from domain knowledge to cognitive style, and ultimately to domain intention, as illustrated in Figure 1. Experimental results from collaborative\n\nÂá∫ÂøÉ Âá∫Â≠ê   \noCnhiopnpsehdercea.bTbwagoeusragnendt Agent Human hBiassdeedtaoinlehdisotrdaejrectory, Agent Human   \norders are both Cathy. knowledge is incorrect   \nHuman partner is due to mixing an off-list ?   \nchopping onions now. soup, so I will focus on ‚ñ° ‚ñ°‚ñ° the mixing action. ‚ñ° ‚ñ°‚ñ° 000 o0e Step 1. Observing Step 2. Partner domain knowledge Reasoning ‰πãËä± ‚ñ° T 7   \niMnygrpeadritenetr-pisretphaeration- oriented style, preparing Agent AgentHuman Human oAfntieorncs,hompypipnagrttnher‚Äôs long-term intention is Agent Human   \namlilxiensgtrheedime intnstofitrhset,pthote.n Êó• Ic hwoilplpilnagcteotmomataoteose,sso 1 ‚ñ°‚ñ° in the center. ‚ñ° ‚ñ°‚ñ° 000 000\n\nFigure 1: A four-step example of cognitive ToM modeling for human partners in the collaborative task Overcooked. In Step 1, the agent observes the environment and the human partner‚Äôs recent behaviors. In Step 2, the agent understands the human trajectory and predicts the partner‚Äôs domain knowledge. In Step 3, the agent analyzes the partner‚Äôs behavioral preferences and categorizes his cognitive style based on the inferred domain knowledge. In Step 4, the agent infers the partner‚Äôs domain intention for effective disicion-making.\n\ntasks in Overcooked demonstrate the effectiveness of DPMT in improving real-time human-AI collaboration.\n\n# Related Work\n\nLLM agent. Human-AI collaboration (Puig et al., 2020; Gao et al., 2020) constitutes a highly challenging research direction, motivating extensive investigations into strategies for enhancing the effectiveness, efficiency, and adaptability of human-AI systems. Compared to traditional agents, LLMbased agents contain an inherent world model, making them highly effective in reasoning and facilitating interpretable interaction for complex tasks, such as human-AI collaboration (Wu et al., 2024; Liu et al., 2023). Recent research (C. Zhang et al., 2024) designs the LLM agent framework with a verifier on the Overcooked environment, with knowledge and state as prompts to guide action planning. Recent work by (H. Zhang et al., 2023) proposes a modular framework that enables embodied agents to communicate and collaborate effectively, facilitating the efficient accomplishment of multi-agent long-term collaboration tasks.\n\nTheory of Mind (ToM) refers to the ability of individuals to understand and predict others‚Äô mental states, including their intentions and goals for optimal decision-making and efficient social collaboration. Recent work introduces the ToM module in single-agent scenarios to predict an agent‚Äôs future actions based on its historical trajectory (Rabinowitz et al., 2018). A ToM-based multi-agent communication method incorporates an additional ToM module to select optimal communication partners based on historical trajectories in multiagent systems (Y. Wang, Xu, Wang, et al., 2021). The mutual ToM process, introduced in recent work (S. Zhang et al., 2024), enhances human-AI collaboration by improving reasoning and communication.\n\nPartner modeling. In multi-agent reinforcement learning (MARL) tasks, partners often exhibit dynamic and diverse strategies, introducing significant challenges in nonstationarity and generalization. To address these challenges in multi-agent collaboration, several studies (Carroll et al., 2019; Shih & Sawhney, 2021) have explored partner modeling to predict partners‚Äô behaviors for enhancing efficient collaboration in complex MARL scenarios. Other approaches construct additional behavioral models (X. Li et al., 2023; Jiang et al., 2024) to simulate human partners and characterize their behavior styles. However, these methods focus on the accuracy of behavioral policy modeling, neglecting the simulation of partners‚Äô mental characteristics. Our work focuses on human partner‚Äôs multi-scale ToM modeling for partner knowledge, cognitive styles, and domain intentions.\n\n# DPMT: Dual Process Multi-scale Theory of Mind Framework\n\n# Overview\n\nAs illustrated in Figure 2, our DPMT method comprises several components: the information extractor, the slow reasoning system, the fast intuitive system, and the action decoding module. The information extractor transforms environmental observation $o _ { t }$ and the human partner‚Äôs trajectory $c _ { t - 1 }$ into the language state, represented as the system cue $p _ { t } ^ { \\mathrm { s y s t e m } }$ pt . ystem encodes textual information about the environment and serves as the input for both the fast and slow systems.\n\nThe fast system focuses on the quick intuitive decisionmaking for each step, making macro-action $m _ { t }$ from a predefined macro-action set. In contrast, the slow system emphasizes cognitive multi-scale ToM reasoning, modeling the partner‚Äôs mental characteristics $k _ { t } , y _ { t } , n _ { t }$ to assist the fast system in making macro-action decisions.\n\nThe action decoding module decomposes the current macro-action $m _ { t }$ into atomic actions $a _ { t }$ and executes them at a much higher frequency until $m _ { t }$ is completed. Once $m _ { t }$ is finished, the fast system determines the subsequent $m _ { t + 1 }$ based on the multi-scale partner reasoning $k _ { t } , y _ { t } , n _ { t }$ from the slow reasoning system. This hierarchical approach ensures a seamless integration of intuitive decision-making and cognitive reasoning for efficient human-AI collaboration.\n\n# Slow System\n\nAs illustrated in Figure 3, we propose a multi-scale mental characteristics system for human-AI collaboration, drawing inspiration from social psychology research on human decision-making (Hauser, 2019; Santos, 2008). These studies categorize various mental characteristics that influence individual behavior into three key dimensions: domain knowledge, cognitive style, and domain intention (Zhao & Smil\n\nobservation 0t Domain Knowledge Corpus ToM Cue IngredientTomato:1.Itneeds5stepstochop tomato.The tomatopositionis (0.3)... System info: Ct-1 ToolPot:1.If thesoupinthepot exceedsfivesteps,the soupwillcatch fire.. Game Scenario: As an AI Information extractor OrderAlice:1.Toprepare theAlicesoup,theseingredientsneed tocook10steps.. assistant with Theory of Caai Cognitive Style Corpus mindabilityinasimplified Overcooked game,...   \n‰ø° language state psystem Personalitystyle:FieldIndependent,Fieldependent GameGuidelines:To make J Behavioralstrategystyle:Ingredient-preparation-oriented,stable,random.. soup:   \n00\\*\\*0 ToMknowledge pkowt. Current soup orders:...   \nEnvironment Fast domain knowledge k Human ToM Reasoning System: Human Domain knowledge Domain knowledge   \natomic action a, Macro0- maker ToMstyle cognitvesie ya pintention D HumanCognitive style asedosegardes Parinectoe Cognitive style corpus:..   \nAction decoding ToMintention Mredtl is il a Partnerstyle prediction nt-1,k,yt domainintention n, Human Domain intention Domainintention   \nmacro action m, SlowSystem:Multi-scaleToM Afterchopping theonions,mypartner's next long-termintention is Portner itention $U P$ soIwillace prediction examples:...\n\nlie, 2015; Bostan, 2009). This system organizes these mental characteristics into a hierarchical architecture, with domain knowledge forming the foundation and domain intentions at the top, progressively enhancing interpretability. Building on this, we propose a multi-scale ToM model as our slow system, simulating the slow cognitive process in the dual process theory. This ToM model comprises multiple stages of human partner mental characteristic reasoning: the human domain knowledge reasoning stage $\\mathrm { T o M } _ { \\mathrm { k n o w l e d g e } }$ , the human cognitive style reasoning stage $\\mathrm { T o M _ { s t y l e } }$ , and the human domain intention reasoning stage ToMintention.\n\nSmall models face significant limitations in interpretability and general knowledge, making it challenging to construct an efficient and interpretable ToM model capable of effectively capturing partner styles and inferring domain intentions from partner trajectories. To overcome these challenges, we leverage LLMs as the foundation for the ToM model, leveraging their strengths in interpretability and extensive world knowledge to enhance human partner ToM modeling.\n\nDomain knowledge reasoning stage Domain knowledge in the Overcooked includes ingredient knowledge, order knowledge, and tool knowledge. Since these types of knowledge are not directly provided in the environmental state, they are typically accumulated from the human partner‚Äôs experiences. Predicting human partners‚Äô domain knowledge serves as the foundation for further predictions of their cognitive styles and domain intentions, forming a critical basis for higher-level ToM reasoning.\n\nIn our framework, the partner domain knowledge reasoning stage $\\mathrm { T o M } _ { \\mathrm { k n o w l e d g e } }$ takes the knowledge cue $p _ { t } ^ { . . }$ nowledge as input, which integrates information from the language state $p _ { t } ^ { \\mathrm { s y s t e m } }$ , the human partner‚Äôs trajectory context $c _ { t - 1 }$ , additional partner domain knowledge reasoning cases, and a customized domain knowledge corpus. The domain knowledge corpus comprises knowledge from three categories, such as ingredient tomato under ingredient knowledge, order Bob under order knowledge. The output of $\\mathrm { T o M _ { k } }$ nowledge is the human partner domain knowledge $k _ { t }$ , which includes insights such as whether the human partner understands the current task rules or the required order preparation sequences.\n\n![](images/387f84c03a1fd2580c4581e78ae86f595f8568a3c07923a1c85a40ad759ed6bb.jpg)  \nFigure 2: As shown in the figure above, our proposed DPMT framework is inspired by the dual process theory, comprising an information extractor module, an action decoding module, a fast system for macro-action decision-making and a slow system for human partner modeling with a multi-scale ToM module. The ToM module consists of three stages: domain knowledge reasoning $\\mathrm { T o M } _ { \\mathrm { k n o w l e d g e } }$ , cognitive style reasoning $\\mathrm { T o M _ { \\mathrm { s t y l e } } }$ , and domain intention reasoning $\\mathbf { T o M } _ { \\mathrm { i n t e n t i o n } }$ . By emulating the cognitive structure, our framework can effectively reason about the human partner and achieve efficient collaboration.   \nFigure 3: Our proposed multi-scale mental characteristics system includes domain knowledge, cognitive style, and domain intention. Domain knowledge includes ingredient knowledge, order knowledge, and tool knowledge. Cognitive style represents the partner‚Äôs personality trait and behavioral style. Domain intention consists of long-term and short-term goals, representing the partner‚Äôs decision-making process.\n\nCognitive style reasoning stage The cognitive style reflects a human partner‚Äôs mental characteristic for decisionmaking preferences based on the domain knowledge, including personality traits, behavioral strategy preferences, and risk preferences. From the perspective of personality traits, cognitive style can be categorized into field-independent, who prefer to complete an entire order independently, and fielddependent, who tend to collaborate by dividing order tasks into sub-tasks to complete an order. In field-dependent players, cognitive styles can be further categorized based on the behavioral strategy, which defines different task tendencies. For example, an ingredient-preparation-oriented style focuses primarily on retrieving and processing various ingredients. This style can also be divided into stable (consistently following a fixed strategy) and random according to the behavioral strategy. Accurate classification of cognitive styles is crucial for effective partner intention modeling.\n\nIn the partner style reasoning stage $\\mathrm { T o M _ { \\mathrm { s t y l e } } }$ , the cognitive style cue $p _ { t } ^ { \\mathrm { s t y l e } }$ comprises ptknowledge, the previous cognitive style prediction $y _ { t - 1 }$ , and the partner domain knowledge prediction $k _ { t }$ . This stage also incorporates an additional partner style corpus to predict the human partner‚Äôs cognitive style $y _ { t }$ . In the partner style corpus, we design various partner cognitive styles based on behavioral strategies and personality traits observed in human behavioral experiments. These partner styles are stored in a structured format consisting of a name, a descriptive paragraph, and representative cases.\n\nDomain intentions reasoning stage Building upon the cognitive style and the current domain state, domain intentions include both short-term and long-term intentions of the human partner, which are crucial for achieving efficient human-AI collaboration. Short-term intention reasoning involves atomic action prediction, determining the human partner‚Äôs current action, such as UP, DOWN, LEFT, RIGHT. In contrast, long-term intention reasoning focuses on predicting the human partner‚Äôs macro-action, specifying their primary goal for the current phase, such as Chop Tomato, Prepare Bob Ingredients, Cook Alice Soup, Plate David Soup. Based on the $k _ { t }$ and $y _ { t }$ , the domain intention reasoning stage ToMintention incorporates a short-term intention cases and long-term intention reasoning corpus in domain intention cue $p _ { t } ^ { \\mathrm i }$ ntention to output the human partner‚Äôs predicted intentions $n _ { t }$\n\n# Fast System\n\nTo enable more real-time human-AI collaboration, our DPMT incorporates a fast system that simulates the quick decision-making system in the dual process theory. While powerful large models like GPT-4o demonstrate superior reasoning ability, their high latency makes them unsuitable for the fast system. Instead, our fast system in DPMT leverages a smaller-scale LLM, such as llama-13B, to reduce latency in macro-action decision-making and achieve real-time collaboration, using the environmental language state ptsystem, the fast system prompt, and currently available actions as input.\n\nAs illustrated in Figure 4, the fast system calculates macroaction probabilities by computing token probabilities, similar to recent works (Liu et al., 2023; Tan et al., 2024). Each macro-action is represented as a sequence of tokens. The LLM evaluates the probability of token sequences based on the human ToM reasoning from the slow system. These probabilities are then normalized to form the macro-action probability distribution at step t. The fast system selects the macroaction $m _ { t }$ with the highest probability from this distribution.\n\n![](images/9d8dd8846dee2a8e91e5771c6a3c1c2b5a3ef9841770a78fad60c1394e66964a.jpg)  \nFigure 4: Overview of our fast system in our proposed DPMT. The fast system first computes token probabilities for every macro-action, then derives the probability distribution of available macro-actions, selecting the one with the highest probability as the output.\n\nAlthough smaller models may make suboptimal decisions, our fast system compensates for this by leveraging accurate partner reasoning from the slow system. These two systems are implemented using multi-threading, where the slow system‚Äôs reasoning for partner modeling operates at a longer time scale than the fast system‚Äôs macro-action decisions. This design allows the fast system to run more frequently, enabling efficient real-time human-AI collaboration.\n\n# Experiments\n\n# Experimental Environment\n\nOur experimental environment is Overcooked (Ghost Town Games, 2016; Liu et al., 2023), a primary human-AI collaboration benchmark. We have conducted experiments on three maps based on the recent work (Liu et al., 2023), which introduced an expanded version of the traditional Overcooked environment for human-AI collaboration. In this environment, the agent and the human player work together to complete cooking tasks as quickly as possible to achieve a higher reward within a limited time. As shown in Figure 5, the environment includes three maps: Ring, Bottleneck, and Quick, each with varying levels of difficulty. Among them, the Quick and Bottleneck maps are more challenging. As shown in Figure 5, completing an order requires following a specific sequence of steps. The agent and human player must retrieve the necessary ingredients, chop them, combine them according to the different recipes, cook them for the required duration to make a dish, plate the dish before it overcooks, and serve the dish to the designated position. Once the dish is served, the player receives a reward (for example, 15 points), while failing to submit a dish results in a penalty (for example, -5 points). Players can choose and execute one action simultaneously from the following options: UP, DOWN, LEFT, or RIGHT, using the directional arrow keys. This environment designs a macro-action set with 21 predefined macro actions for the LLM agent, such as Cook Alice Soup.\n\n![](images/1ee7f8e679553a74c46a14ad92b8f2cc7ba1fef30bc20c987f68483ae0bf7d86.jpg)  \nFigure 5: The left part shows our Overcooked environments. The maps from top to bottom are Ring, Bottleneck, and Quick. The right part shows the detailed layout description of the bottleneck environment.\n\n# Human Experiment 1: Collaborating with Human Partners with Specific Strategies\n\nExperimental Setup. In the human-AI collaboration experiment, similar to HLAgent (Liu et al., 2023), the human player followed the fixed strategy to control the partner agent and interact with objects using the arrow keys like ‚Äôup,‚Äô and ‚Äôdown.‚Äô In our human experiments, baseline methods included our DPMT with Qwen, our DPMT without the multiscale ToM (MsToM) and the HLAgent. Since communication between agents was not allowed in our task setting, we used the 0-command HLAgent as our baseline. To ensure experimental fairness, we established a stopping criterion that terminated LLM agent executions once the trajectory length reached 500 steps. In this part of the experiment, we used game scores to measure human-AI collaboration efficiency. The experiments were conducted on three maps and repeated five times under consistent conditions. The average humanAI collaboration scores are shown in Table 1.\n\nMain Results. As shown in Table 1, our proposed DPMT method consistently achieved the highest collaboration performance when paired with fixed-strategy human partners across all three maps: Ring, Quick, and Bottleneck, highlighting the effectiveness of our DPMT framework. Compared to the baseline methods, the introduction of the multi-scale mental characteristic ToM reasoning module (MsToM) in our\n\nTable 1: Score comparison between baseline methods and our DPMT method across all three maps in the human experiment 1. Standard deviations are shown in parentheses.   \n\n<html><body><table><tr><td>Methods</td><td>Ring</td><td>Bottleneck</td><td>Quick</td></tr><tr><td>OurDPMT</td><td>121 (¬±13.56)</td><td>101 (¬±13.56)</td><td>104 (¬±16.73)</td></tr><tr><td>Our DPMT w Qwen</td><td>100 (¬±15.17)</td><td>95 (¬±22.80)</td><td>105 (¬±18.71)</td></tr><tr><td>OurDPMTw/oMsToM</td><td>44 (¬±22.23)</td><td>23 (¬±22.93)</td><td>9 (¬±19.34)</td></tr><tr><td>HLAgent</td><td>99 (¬±18.81)</td><td>60 (¬±20.74)</td><td>87 (¬±16.00)</td></tr></table></body></html>\n\nDPMT framework led to a higher success rate in serving soups, a lower obstruction rate, and a lower incidence of dish fires by accurately predicting human partners‚Äô mental characteristics, such as domain intentions. In contrast, the lack of ToM reasoning in DPMT w/o MsToM often resulted in ineffective collaboration, including redundant actions, route blockages, overcooked dishes, and frequent task repetition, ultimately incurring performance penalties.\n\n![](images/50552974e231f77a60cafedbef7d212cc9a10760ffa5864628e0e191daed48c5.jpg)  \nFigure 6: Score results of ablation experiments for different mental characteristics in human experiment 1. The black line denotes the standard deviation.\n\nAdditionally, compared to the HLAgent baseline, our method achieved superior and more stable performance, particularly on complex maps such as Bottleneck. The Bottleneck map has a more complex structure with a narrow passage, resulting in a higher blockage rate. This highlights the critical role of accurate ToM reasoning in achieving efficient human-AI collaboration. Furthermore, substituting the GPT4o API with Qwen-72B as the base model for mental characteristic reasoning resulted in slight improvements in collaboration performance on the Quick map. This enhancement is likely attributed to minor fluctuations in real-time experimental results caused by latency.\n\nAblation Results. We conducted ablation experiments to analyze the effectiveness of our DPMT method further. As shown in Figure 6, we compared the complete DPMT method with its ablated versions, where specific mental characteristic reasoning stages (w/o knowledge, w/o style, and w/o intention) were removed. Our method consistently outperformed the ablated versions across all maps (Ring, Quick, and Bottleneck), especially on more complex maps, where effective ToM reasoning plays a crucial role in coordination. For example, removing the intention reasoning layer led to a significant performance drop on the Quick map, while omitting the style layer caused notable declines on the Bottleneck map. These results indicate the importance of each ToM characteristic and demonstrate the necessity of hierarchical ToM modeling for effective partner reasoning and improved collaboration.\n\n# Questionnaire for Subjective Judgment in Human Experiment 2\n\n1 Game Ability ‚ÄúCan the AI agent efficiently utilize time to complete as many effective tasks as possible based on the order situation?\"   \n2 Human-AI Collaboration Fluidity ‚ÄúCan the AI agent accurately perceive the current order requirements and your actions, adapt its own actions accordingly, and collaborate efficiently with you avoiding redundant work?\"   \n3 Theory of mind (ToM) Ability ‚ÄúTo what extent do you feel that your AI agent understands your intentions? Can the AI agent effectively avoid conflicts with your operations, such as not taking over kitchen equipment, interrupting, or obstructing your actions?\" \"Do you feel comfortable collaborating with the AI agent? Can it adapt to your movement habits and coordinate effectively to minimize idleness or delays, accelerating order completion? Can it adjust its path to avoid congestion in narrow passages by observing your movement?‚Äù Do you trust the AI agent? Can it adapt its behavior to align with the common goal in high-pressure or emergency situations (e.g., time shortages or fire)?\n\n# Human Experiment 2: Collaborating with Human Partners with Diverse Strategies\n\nExperiment Setup. An ideal AI agent should not only cooperate seamlessly with fixed-strategy human players but also collaborate efficiently with unseen human partners who employ diverse strategies. To further analyze the effectiveness and the human preference of our proposed DPMT framework, we conducted additional behavioral experiments for human studies. These experiments were performed across three maps with 30 participants (26 male, 4 female, ages 20‚Äì50). As shown in Figure 7, we designed a 5-point Likert scale questionnaire for each map, consisting of several questions (Hoffman, 2019; S. Zhang et al., 2024). This experiment followed a within-subject design, where each participant collaborated with both the DPMT and three baseline agents (A-D). To ensure fairness and validity, we randomized the order of partner agents (‚ÄôABCD‚Äô or ‚ÄôDBCA‚Äô) for each participant. Additionally, to evaluate the AI agent‚Äôs performance more effectively, we extended the game duration to 90 seconds, longer than in previous human experiment 1.\n\nAll participants were required to review the informed consent form and game rules before starting. To familiarize themselves with the task environment and rules, they first collaborated with rule-based agents across three maps. After completing the collaboration task on each map, they filled out a corresponding questionnaire, rating different agents based on their level of agreement with each statement.\n\nResults. In our collaboration experiments with diverse human participants, we evaluated performance using two key metrics: average collaboration scores within a fixed time limit and subjective judgments from the participants. As shown in\n\n![](images/091788c1078b84764096a5beaa385faabc9b099276ec4815aa6d742ee2580aba.jpg)  \nFigure 7: Our proposed questionnaire for subjective judgment metric with 5-point Likert scales was used in our human experiment 2. The questionnaire includes three parts: game ability, human-AI collaboration fluidity, and ToM ability. Different questionnaires were designed for different maps.   \nFigure 8: Average collaboration score comparison between baseline methods and our proposed DPMT when paired with diverse participants in the human experiment 2.\n\nFigure 8, our DPMT method consistently outperformed baseline methods across all three maps, exhibiting a result trend similar to the main result of human experiment 1, demonstrating its ability to adapt to both specific strategies and diverse unseen human collaborators. For the subjective judgment scores, we computed the average scores based on our proposed questionnaire, with a maximum score of 5, as shown in Table 2. Notably, our method achieved higher subjective scores, further highlighting its superior ToM reasoning capability for effective partner modeling.\n\nTable 2: The average subjective judgment when paired with diverse unseen participants in human experiment 2 ranges from 0 to 5. Standard deviations are shown in parentheses.   \n\n<html><body><table><tr><td>Methods</td><td>Ring</td><td>Bottleneck</td><td>Quick</td></tr><tr><td>Our DPMT</td><td>4.07 (¬±0.53)</td><td>4.02 (¬±0.45)</td><td>4.02 (¬±0.61)</td></tr><tr><td>Our DPMTwQwen</td><td>3.51 (¬±0.58)</td><td>3.73 (¬±0.47)</td><td>3.89 (¬±0.68)</td></tr><tr><td>OurDPMTw/o MsToM</td><td>2.20 (¬±0.70)</td><td>2.16 (¬±0.67)</td><td>2.04 (¬±0.60)</td></tr><tr><td>HLAgent</td><td>3.09 (¬±0.30)</td><td>2.78 (¬±0.59)</td><td>3.22 (¬±0.69)</td></tr></table></body></html>\n\n# Conclusion\n\nDrawing inspiration from the dual process theory, we introduce a cognitive dual process multi-scale ToM (DPMT) framework, which simulates both the fast system for quick decision-making and the slow system for ToM reasoning. The slow system in DPMT employs a three-stage ToM reasoning process‚Äîpartner domain knowledge, cognitive style, and domain intention‚Äîto improve the interpretability and generalization of partner ToM modeling. Additionally, it integrates a structured corpus of mental characteristics and a memory module that stores partner trajectories and ToM reasoning predictions. Our DPMT outperforms baseline methods in human-AI collaboration experiments, demonstrating superior adaptability and ToM ability. Ablation studies further validate the contribution of different reasoning stages. Notably, our method achieves significantly stronger results on complex maps, highlighting its potential for tackling more intricate human-AI collaborative tasks.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÈíàÂØπÂÆûÊó∂‰∫∫Êú∫Âçè‰Ωú‰∏≠AI‰ª£ÁêÜÈöæ‰ª•ÂáÜÁ°ÆÂª∫Ê®°Â§çÊùÇ‰∫∫Á±ªÂøÉÁêÜÁâπÂæÅÔºàÂ¶ÇÈ¢ÜÂüüÊÑèÂõæÔºâÁöÑÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®Áº∫‰πèÁõ¥Êé•Ê≤üÈÄöÁöÑÊÉÖÂÜµ‰∏ã„ÄÇËøô‰∏ÄÈóÆÈ¢òÂú®Âä®ÊÄÅÂú∫ÊôØ‰∏≠Â∞§‰∏∫ÂÖ≥ÈîÆÔºåÂõ†‰∏∫Áé∞ÊúâÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂæÄÂæÄÊó†Ê≥ïÊúâÊïàÈÄÇÂ∫îÂ§öÊ†∑‰∏îÊú™ËßÅÁöÑ‰∫∫Á±ªË°å‰∏∫„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåËß£ÂÜ≥ÂÆÉËÉΩÂ§üÊòæËëóÊèêÂçá‰∫∫Êú∫Âçè‰ΩúÁöÑÊïàÁéáÂíåÂ∫îÁî®ËåÉÂõ¥ÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÂÆûÊó∂ÂÜ≥Á≠ñÂíåÂçè‰ΩúÁöÑÂ§çÊùÇ‰ªªÂä°‰∏≠ÔºàÂ¶ÇÊ∏∏Êàè„ÄäOvercooked„ÄãÔºâ„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåËøáÁ®ãÂ§öÂ∞∫Â∫¶ÂøÉÊô∫ÁêÜËÆ∫ÔºàDPMTÔºâÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜËÆ§Áü•ÁßëÂ≠¶‰∏≠ÁöÑÂèåËøáÁ®ãÁêÜËÆ∫ÂíåÂ§öÂ∞∫Â∫¶ÂøÉÊô∫ÁêÜËÆ∫ÔºàToMÔºâÊ®°ÂùóÔºå‰ª•ÈÄöËøáÂøÉÁêÜÁâπÂæÅÊé®ÁêÜÂÆûÁé∞È≤ÅÊ£íÁöÑ‰∫∫Á±ª‰ºô‰º¥Âª∫Ê®°„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫‰∫ÜÂ§öÂ∞∫Â∫¶ToMÊ®°ÂùóÔºåÈÄöËøá‰∏â‰∏™Èò∂ÊÆµÔºàÈ¢ÜÂüüÁü•ËØÜÊé®ÁêÜ„ÄÅËÆ§Áü•È£éÊ†ºÊé®ÁêÜÂíåÈ¢ÜÂüüÊÑèÂõæÊé®ÁêÜÔºâÂÆûÁé∞‰∫∫Á±ª‰ºô‰º¥ÂøÉÁêÜÁâπÂæÅÁöÑÂª∫Ê®°„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂú®„ÄäOvercooked„Äã‰ªªÂä°‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÂçè‰ΩúÊïàÁéá„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ËÆæËÆ°‰∫ÜÂèåËøáÁ®ãÁ≥ªÁªüÔºàÂø´ÈÄüÁ≥ªÁªüÂíåÊÖ¢ÈÄüÁ≥ªÁªüÔºâÔºåÂàÜÂà´Ë¥üË¥£Âø´ÈÄüÂÜ≥Á≠ñÂíåËÆ§Áü•Êé®ÁêÜÔºåÂÆûÁé∞‰∫ÜÂÆûÊó∂Âçè‰Ωú„ÄÇÂÆûÈ™åË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§çÊùÇÂú∞ÂõæÔºàÂ¶ÇBottleneckÔºâ‰∏äÁöÑÂçè‰ΩúÂæóÂàÜÊØîÂü∫Á∫øÊñπÊ≥ïÊèêÂçá‰∫ÜÁ∫¶68%„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** ÂºïÂÖ•‰∫ÜÁªìÊûÑÂåñÂøÉÁêÜÁâπÂæÅËØ≠ÊñôÂ∫ìÂíåËÆ∞ÂøÜÊ®°ÂùóÔºåÂ¢ûÂº∫‰∫ÜToMÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏ªËßÇËØÑ‰ª∑ÊòæÁ§∫ÔºåÁî®Êà∑ÂØπDPMTÁöÑÂçè‰ΩúÊµÅÁïÖÊÄßÂíåToMËÉΩÂäõÁöÑËØÑÂàÜÊòæËëóÈ´ò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºàÂπ≥ÂùáËØÑÂàÜ4.07 vs. 3.09Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   DPMTÊ°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÊ®°Êãü‰∫∫Á±ªËÆ§Áü•ÁöÑÂèåËøáÁ®ãÁêÜËÆ∫ÔºåÈÄöËøáÂø´ÈÄüÁ≥ªÁªüÂÆûÁé∞Áõ¥ËßâÂÜ≥Á≠ñÔºåÈÄöËøáÊÖ¢ÈÄüÁ≥ªÁªüÂÆûÁé∞Â§öÂ∞∫Â∫¶ToMÊé®ÁêÜ„ÄÇËøôÁßçËÆæËÆ°ËÉΩÂ§üÂêåÊó∂Êª°Ë∂≥ÂÆûÊó∂ÊÄßÂíåÂáÜÁ°ÆÊÄßÈúÄÊ±ÇÔºå‰ªéËÄåÊèêÂçá‰∫∫Êú∫Âçè‰ΩúÁöÑÊïàÁéá„ÄÇ\\n> *   ËØ•ÊñπÊ≥ï‰πãÊâÄ‰ª•ÊúâÊïàÔºåÊòØÂõ†‰∏∫ÂÆÉÈÄöËøáÂàÜÂ±ÇÊé®ÁêÜÔºà‰ªéÈ¢ÜÂüüÁü•ËØÜÂà∞ËÆ§Áü•È£éÊ†ºÂÜçÂà∞È¢ÜÂüüÊÑèÂõæÔºâÈÄêÊ≠•Â¢ûÂº∫ÂØπ‰∫∫Á±ª‰ºô‰º¥ÁöÑÁêÜËß£Ôºå‰ªéËÄåÂÅöÂá∫Êõ¥ÂáÜÁ°ÆÁöÑÂçè‰ΩúÂÜ≥Á≠ñ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** Áé∞ÊúâLLM‰ª£ÁêÜÁº∫‰πèToMËÉΩÂäõÔºåÈöæ‰ª•Âú®Áº∫‰πèÁõ¥Êé•Ê≤üÈÄöÁöÑÊÉÖÂÜµ‰∏ãÈ¢ÑÊµã‰∫∫Á±ª‰ºô‰º¥ÁöÑË°å‰∏∫„ÄÇÊ≠§Â§ñÔºå‰º†ÁªüÊñπÊ≥ï‰æùËµñ‰∫éÈ´òË¥®ÈáèËΩ®ËøπÂíåÂÖàÈ™åÁü•ËØÜÔºåÊ≥õÂåñÊÄßÂíåÂèØËß£ÈáäÊÄßÊúâÈôê„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** DPMTÈÄöËøáÂ§öÂ∞∫Â∫¶ToMÊ®°ÂùóÂíåÁªìÊûÑÂåñËØ≠ÊñôÂ∫ìÔºåÂÆûÁé∞‰∫ÜÂØπ‰ºô‰º¥ÂøÉÁêÜÁâπÂæÅÁöÑÂä®ÊÄÅÊé®ÁêÜ„ÄÇÂø´ÈÄüÁ≥ªÁªü‰ΩøÁî®ËΩªÈáèÁ∫ßLLMÔºàÂ¶Çllama-13BÔºâÈôç‰ΩéÂª∂ËøüÔºåÊÖ¢ÈÄüÁ≥ªÁªüÂàôÂà©Áî®Â§ßÂûãLLMÔºàÂ¶ÇGPT-4oÔºâÊèêÂçáÊé®ÁêÜËÉΩÂäõ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   1. **‰ø°ÊÅØÊèêÂèñÂô®**ÔºöÂ∞ÜÁéØÂ¢ÉËßÇÂØüÂíå‰∫∫Á±ª‰ºô‰º¥ËΩ®ËøπËΩ¨Êç¢‰∏∫ËØ≠Ë®ÄÁä∂ÊÄÅÔºå‰Ωú‰∏∫Âø´ÈÄüÂíåÊÖ¢ÈÄüÁ≥ªÁªüÁöÑËæìÂÖ•„ÄÇ\\n> *   2. **ÊÖ¢ÈÄüÁ≥ªÁªü**ÔºöÊâßË°åÂ§öÂ∞∫Â∫¶ToMÊé®ÁêÜÔºåÂåÖÊã¨Ôºö\\n>       - **È¢ÜÂüüÁü•ËØÜÊé®ÁêÜ**ÔºöÈ¢ÑÊµã‰ºô‰º¥ÁöÑÈ¢ÜÂüüÁü•ËØÜÔºàÂ¶Ç‰ªªÂä°ËßÑÂàôÂíåÂáÜÂ§áÈ°∫Â∫èÔºâ„ÄÇ\\n>       - **ËÆ§Áü•È£éÊ†ºÊé®ÁêÜ**ÔºöÂàÜÁ±ª‰ºô‰º¥ÁöÑÂÜ≥Á≠ñÂÅèÂ•ΩÔºàÂ¶ÇÂú∫Áã¨Á´ãÊàñÂú∫‰æùËµñÈ£éÊ†ºÔºâ„ÄÇ\\n>       - **È¢ÜÂüüÊÑèÂõæÊé®ÁêÜ**ÔºöÈ¢ÑÊµã‰ºô‰º¥ÁöÑÁü≠ÊúüÂíåÈïøÊúüÊÑèÂõæÔºàÂ¶ÇÂàáÁï™ËåÑÊàñÂáÜÂ§áÊ±§Ôºâ„ÄÇ\\n> *   3. **Âø´ÈÄüÁ≥ªÁªü**ÔºöÂü∫‰∫éÊÖ¢ÈÄüÁ≥ªÁªüÁöÑÊé®ÁêÜÁªìÊûúÔºåÈÄâÊã©Ê¶ÇÁéáÊúÄÈ´òÁöÑÂÆèÂä®‰ΩúÔºàÂ¶ÇCook Alice SoupÔºâ„ÄÇ\\n> *   4. **Âä®‰ΩúËß£Á†ÅÊ®°Âùó**ÔºöÂ∞ÜÂÆèÂä®‰ΩúÂàÜËß£‰∏∫ÂéüÂ≠êÂä®‰ΩúÂπ∂ÊâßË°å„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂõõÊ≠•Á§∫‰æãÔºàÂ¶ÇÂõæ1ÊâÄÁ§∫ÔºâÔºö\\n>       - **Ê≠•È™§1**Ôºö‰ª£ÁêÜËßÇÂØüÁéØÂ¢ÉÂíå‰ºô‰º¥Ë°å‰∏∫„ÄÇ\\n>       - **Ê≠•È™§2**Ôºö‰ª£ÁêÜÈ¢ÑÊµã‰ºô‰º¥ÁöÑÈ¢ÜÂüüÁü•ËØÜÔºàÂ¶Ç‰ºô‰º¥ÊòØÂê¶ÁêÜËß£‰ªªÂä°ËßÑÂàôÔºâ„ÄÇ\\n>       - **Ê≠•È™§3**Ôºö‰ª£ÁêÜÂàÜÊûê‰ºô‰º¥ÁöÑËÆ§Áü•È£éÊ†ºÔºàÂ¶ÇÂéüÊñôÂáÜÂ§áÂØºÂêëÈ£éÊ†ºÔºâ„ÄÇ\\n>       - **Ê≠•È™§4**Ôºö‰ª£ÁêÜÊé®Êñ≠‰ºô‰º¥ÁöÑÈ¢ÜÂüüÊÑèÂõæÔºàÂ¶ÇÈïøÊúüÊÑèÂõæÊòØÂáÜÂ§áÁï™ËåÑÊ±§ÔºâÔºåÂπ∂ÊçÆÊ≠§Ë∞ÉÊï¥Ëá™Ë∫´Ë°å‰∏∫„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   HLAgentÔºö‰∏ÄÁßçÂü∫‰∫éLLMÁöÑ‰ª£ÁêÜÊ°ÜÊû∂ÔºåÊîØÊåÅ‰∫∫Êú∫Âçè‰Ωú‰ΩÜÁº∫‰πèToMËÉΩÂäõ„ÄÇ\\n> *   DPMT w/o MsToMÔºöDPMTÁöÑÂèò‰ΩìÔºåÁßªÈô§‰∫ÜÂ§öÂ∞∫Â∫¶ToMÊ®°Âùó„ÄÇ\\n> *   DPMT w QwenÔºö‰ΩøÁî®Qwen-72B‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÁöÑDPMTÁâàÊú¨„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Âçè‰ΩúÂæóÂàÜ‰∏äÔºö** Âú®„ÄäOvercooked„ÄãÁöÑBottleneckÂú∞Âõæ‰∏äÔºåDPMTÁöÑÂπ≥ÂùáÂæóÂàÜ‰∏∫101Ôºà¬±13.56ÔºâÔºåÊòæËëó‰ºò‰∫éHLAgentÔºà60 ¬±20.74ÔºâÂíåDPMT w/o MsToMÔºà23 ¬±22.93Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫ÜÁ∫¶68%„ÄÇ\\n> *   **Âú®‰∏ªËßÇËØÑ‰ª∑‰∏äÔºö** Áî®Êà∑ÂØπDPMTÁöÑToMËÉΩÂäõËØÑÂàÜ‰∏∫4.02Ôºà¬±0.45ÔºâÔºåËøúÈ´ò‰∫éHLAgentÁöÑ2.78Ôºà¬±0.59Ôºâ„ÄÇÁâπÂà´ÊòØÂú®Âçè‰ΩúÊµÅÁïÖÊÄßÊñπÈù¢ÔºåDPMTÁöÑËØÑÂàÜÊØîÂü∫Á∫øÈ´òÂá∫30%‰ª•‰∏ä„ÄÇ\\n> *   **Âú®ÂÆûÊó∂ÊÄß‰∏äÔºö** DPMTÁöÑÂø´ÈÄüÁ≥ªÁªü‰ΩøÁî®llama-13BÂÆûÁé∞‰∫Ü‰ΩéÂª∂ËøüÂÜ≥Á≠ñÔºåÂêåÊó∂ÊÖ¢ÈÄüÁ≥ªÁªüÁöÑÊé®ÁêÜÈ¢ëÁéá‰∏éÂø´ÈÄüÁ≥ªÁªüËß£ËÄ¶ÔºåÁ°Æ‰øù‰∫ÜÂÆûÊó∂Âçè‰ΩúÁöÑÊïàÁéá„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ÂøÉÊô∫ÁêÜËÆ∫ (Theory of Mind, ToM)\\n*   ‰∫∫Êú∫Âçè‰Ωú (Human-AI Collaboration, N/A)\\n*   ÂèåËøáÁ®ãÁêÜËÆ∫ (Dual Process Theory, N/A)\\n*   Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (Large Language Model, LLM)\\n*   ËÆ§Áü•È£éÊ†º (Cognitive Style, N/A)\\n*   È¢ÜÂüüÊÑèÂõæ (Domain Intention, N/A)\\n*   ÂÆûÊó∂ÂÜ≥Á≠ñ (Real-time Decision Making, N/A)\\n*   Êô∫ËÉΩ‰ª£ÁêÜ (Intelligent Agents, N/A)\"\n}\n```"
}