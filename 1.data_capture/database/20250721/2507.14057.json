{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14057",
    "link": "https://arxiv.org/abs/2507.14057",
    "pdf_link": "https://arxiv.org/pdf/2507.14057.pdf",
    "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
    "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "paper_content": "# Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design\n\nMarcel Hedman \\* 1 Desi R. Ivanova \\* 1 Cong Guan 1 Tom Rainforth 1\n\n# Abstract\n\nWe develop a semi-amortized, policy-based, approach to Bayesian experimental design (BED) called Stepwise Deep Adaptive Design (StepDAD). Like existing, fully amortized, policybased BED approaches, Step-DAD trains a design policy upfront before the experiment. However, rather than keeping this policy fixed, Step-DAD periodically updates it as data is gathered, refining it to the particular experimental instance. This test-time adaptation improves both the flexibility and the robustness of the design strategy compared with existing approaches. Empirically, StepDAD consistently demonstrates superior decisionmaking and robustness compared with current state-of-the-art BED methods.\n\n# 1. Introduction\n\nAdaptive experimentation plays a crucial role in science and engineering: it enables targeted and efficient data acquisition by sequentially integrating information gathered from past experiment iterations into subsequent design decisions (Atkinson et al., 2007; MacKay, 1992; Myung et al., 2013). For example, consider an online survey that aims to infer individual preferences through personalized questions. By strategically tailoring future questions based on insights from past responses, the survey can rapidly hone in on relevant questions for each specific individual, enabling precise preference inference with fewer, more targeted questions.\n\nBayesian experimental design (BED) offers a principled framework for solving such optimal design problems (Chaloner and Verdinelli, 1995; Rainforth et al., 2024; Ryan et al., 2016). In BED, the quantity of interest (e.g. individual preferences), is represented as an unknown parameter $\\theta$ and modelled probabilistically through a joint generative model on $\\theta$ and experiment outcomes given designs. The goal is then to choose designs that are maximally informative about $\\theta$ . Namely, we maximize the Expected Information Gain (EIG) (Lindley, 1956; 1972), which measures the expected reduction in our uncertainty about $\\theta$ from running an experiment with a given design.\n\nThe traditional adaptive BED approach, illustrated in Fig 1a, involves iterating between making design decisions by optimizing the EIG of the next experiment step, and updating the underlying model through Bayesian updates that condition on the data obtained so far. Unfortunately, this approach leads to sub-optimal design decisions, as it is a greedy, myopic, strategy that fails to consider future experiment steps (Foster, 2021; Huan and Marzouk, 2016). Furthermore, it requires substantial computation to be undertaken at each experiment iteration, making it impractical for real-time applications (Rainforth et al., 2024).\n\nFoster et al. (2021) showed that this traditional framework can be significantly improved upon by taking a policy-based approach (PB-BED). As shown in Fig 1b, their Deep Adaptive Design (DAD) framework, and its extensions (Blau et al., 2022; Ivanova et al., 2021; Lim et al., 2022), are based on learning a design policy network that maps from experimental histories to new designs. This policy is trained before the experiment, then deployed to make design decisions automatically at test time. This provides a fully amortized approach that eliminates the need for significant computation during the experiment itself, thereby enabling real-time, adaptive, and non-myopic design strategies that represent the current state-of-the-art in adaptive BED.\n\nIn principle, these fully amortized approaches can learn theoretically optimal design strategies (in terms of total EIG). In practice, learning a policy that remains optimal for all possible experiment realizations is rarely realistic. In particular, the dimensionality of experimental history expands as the experiment progresses, making it increasingly difficult to account for all possible eventualities through upfront training alone. Moreover, deficiencies in our model can mean that data observed in practice can be highly distinct from the simulated data used to train the policy.\n\nTo address these limitations, and allow utilisation of any available computation during the experiment, we introduce a hybrid, semi-amortized, PB-BED approach, called Stepwise Deep Adaptive Design (Step-DAD). As illustrated in\n\nFig 1c, Step-DAD periodically updates the policy during the experiment. This allows the policy to be adapted using previously gathered data, refining it to maximize performance for the particular realization of the data that we observe. In turn, this allows Step-DAD to make more accurate design decisions and provides significant improvements in robustness to observing data that is dissimilar to that generated in the original policy training. Empirical evaluations reveal Step-DAD is able to provide significant improvements in state-of-the-art design performance, while using substantially less computation than the traditional BED approach.\n\n# 2. Background\n\nGuided by the principle of information maximization, Bayesian experimental design (BED, Lindley, 1956) is a model-based framework for designing optimal experiments. Given a model $p ( \\theta ) p ( y \\mid \\theta , \\xi )$ , describing the relationship between experimental outcomes $y$ , controllable designs $\\xi$ and unknown parameters of interest $\\theta$ , the goal is to select the experiment $\\xi$ that maximizes the expected information gain (EIG) about $\\theta$ . The EIG, which is equivalent to the mutual information between $\\theta$ and $y$ , is the expected reduction in Shannon entropy from the prior to the posterior of $\\theta$ :\n\n$$\nI ( \\xi , y ) = \\mathbb { E } _ { p ( y \\mid \\xi ) } [ H [ p ( \\theta ) ] - H [ p ( \\theta \\mid \\xi , y ) ] ] ,\n$$\n\nwhere $p ( y \\mid \\xi ) = \\mathbb { E } _ { p ( \\theta ) } [ p ( y \\mid \\theta , \\xi ) ]$ is the prior predictive distribution of our model.\n\n# 2.1. Traditional Adaptive BED\n\nBED becomes particularly powerful in adaptive contexts, where we allow the future design decision at time t, $\\xi _ { t }$ , to be informed by the data acquired up to that point, $h _ { t - 1 } : = ( \\xi _ { 1 } , y _ { 1 } ) , \\dots , ( \\xi _ { t - 1 } , y _ { t - 1 } )$ , which we refer to as the history. In the traditional adaptive BED framework (Ryan et al., 2016), this is done by assimilating the data into the model by fitting the posterior $p ( \\theta \\mid h _ { t - 1 } )$ , followed by the maximization of the one-step ahead, or incremental, EIG\n\n$$\nI ^ { h _ { t - 1 } } ( \\xi _ { t } ) = \\mathbb { E } \\big [ H [ p ( \\theta \\mid h _ { t - 1 } ) ] - H [ p ( \\theta \\mid h _ { t } ) ] \\big ] ,\n$$\n\nwhere the expectation is taken with respect to the marginal distribution $p ( y \\mid \\xi _ { t } , h _ { t - 1 } ) = \\mathbb { E } _ { p ( \\theta \\mid h _ { t - 1 } ) } [ p ( y \\mid \\theta , \\xi _ { t } , h _ { t - 1 } ) ]$ We use the superscript $h _ { t - 1 }$ to emphasize conditioning on the history currently available, setting $h _ { 0 } = \\emptyset$ . This is a closed-loop approach (Foster, 2022; Huan and Marzouk, 2016), explicitly integrating all of the acquired data to refine beliefs about $\\theta$ and inform subsequent design decisions.\n\nWhilst this traditional framework offers a principled and systematic way to optimize experimental designs, it comes with some limitations. One drawback is its myopic nature that greedily maximizes for the next best design and overlooks the impact of future experiments, ultimately leading to sub-optimal design decisions. Another limitation is the significant computational expense incurred from the iterative posterior inference and EIG optimization. In general, the posterior computation is intractable and the EIG (1) estimation is doubly intractable (Foster et al., 2019; Rainforth et al., 2018). Since both of these steps must be conducted at each step of the experiment, the traditional adaptive BED approach is often impractical for real-time applications.\n\n# 2.2. Amortized Policy-Based BED\n\nIn response to the limitations of traditional adaptive BED, Foster et al. (2021) introduce the idea of amortizing the adaptive design process through learnt policies. This amortized policy-based BED (PB-BED) approach represents a significant advancement over the traditional framework, delivering state-of-the-art non-myopic design optimization whilst enabling real-time deployment.\n\nPB-BED reformulates the design problem using a policy $\\pi$ , which maps experimental histories to subsequent design choices, $\\pi : h _ { t - 1 } \\mapsto \\xi _ { t }$ . The optimal policy is now the one that maximizes the total EIG across the entire sequence of $T$ experiments (Foster et al., 2021; Shen and Huan, 2021)\n\n$$\n\\begin{array} { r l } & { \\mathcal { T } _ { 1 \\to T } ( \\pi ) = \\mathbb { E } _ { p ( h _ { T } \\mid \\pi ) } \\left[ H [ p ( \\theta ) ] - H [ p ( \\theta \\mid h _ { T } ) ] \\right] } \\\\ & { \\quad \\quad = \\mathbb { E } _ { p ( \\theta ) p ( h _ { T } \\mid \\theta , \\pi ) } \\left[ \\log p ( h _ { T } \\mid \\theta , \\pi ) - \\log p ( h _ { T } \\mid \\pi ) \\right] } \\end{array}\n$$\n\nwhere $\\begin{array} { r } { { \\mathbf { \\sigma } } _ { : p \\left( h _ { T } \\mid \\theta , \\pi \\right) } = \\prod _ { t = 1 } ^ { T } p ( y _ { t } \\mid \\theta , \\xi _ { t } , h _ { t - 1 } ) , p ( h _ { T } \\mid \\pi ) = } \\end{array}$ $\\mathbb { E } _ { p ( \\theta ) } [ p ( h _ { T } | \\theta , \\pi ) ]$ , and $\\xi _ { t } = \\pi ( h _ { t - 1 } )$ are all evaluated autoregressively. This policy-based formulation strictly generalizes the traditional adaptive BED approach, which can be viewed as using a specific policy that maximizes the incremental one-step-ahead EIG (1) at each iteration, that is $\\pi _ { \\operatorname { t r a d } } ( h _ { t - 1 } ) = \\arg \\operatorname* { m a x } _ { \\xi _ { t } } I _ { t - 1 \\to t } ^ { h _ { t - 1 } } ( \\xi _ { t } ) .$ .\n\nWhilst the total EIG formulation (2) provides a unified training objective for the policy, it remains doubly intractable like the standard EIG. The original Deep Adaptive Design (DAD) method (Foster et al., 2021) addressed this by using tractable variational lower bounds of the EIG (Foster et al., 2019; 2020; Kleinegesse and Gutmann, 2020) coupled with stochastic gradient ascent (SGA) schemes to directly train a policy network taking the form of a deep neural network directly mapping from histories to design decisions. It thus provided a foundation for conducting PB-BED in practice.\n\nA number of extensions to the DAD approach have since been developed (Blau et al., 2022; Ivanova et al., 2021; Lim et al., 2022), broadening its applicability to a wider class of models by proposing alternative policy training schemes. All share a core methodology, where the policy is trained only once, offline, with experimental histories simulated from the model $p ( \\theta ) p ( h _ { T } | \\theta , \\pi )$ . Once trained, it remains unchanged during the live experiment and across multiple experimental instances (e.g. different survey participants), as illustrated in Fig 1b. This fully amortized approach eliminates the need for posterior inference and EIG optimization\n\nLive experiment Offline Live Offline Live experiment   \nDesign GR training experiment Design Initial Policy training refine 公 Policy ? T iterations Policy T iterations Posterior Design   \nPosterior infer Outcome Outcome u K steps τOi iutetrcaotiomnse   \n(a) Traditional adaptive BED (b) Fully amortized PB-BED (c) Semi-amortized PB-BED\n\nat each step of the experiment, thereby allowing design decisions to be made almost instantly at deployment.\n\n# 3. Semi-Amortized PB-BED\n\nFully amortized PB-BED methods enable real-time deployment and provide design decisions that are typically superior to those of the traditional framework. However, there are many problems where we can afford to perform some testtime training during the experiment itself. It is therefore natural to ask whether we can usefully exploit such computational availability to further improve the quality of our design decisions? In particular, the fact that the current state-of-the-art approaches for design quality are all fully amortized suggests that improvements should be possible when this is not a computational necessity.\n\nTo address this, we note that the computational gains of fully amortized PB-BED methods comes at the cost of their inability to adapt the policy itself in response to acquired experimental data. We argue that this rigidity leads to suboptimal designs decisions, particularly in scenarios where the real-world experimental data significantly deviates from the simulated histories used during training of the policy. Two primary factors contribute to this issue:\n\nImperfect training In fully amortized PB-BED we simulate experimental histories to try and learn a policy that will generalize across the entire experimental space—effectively learning a regressor from all possible histories to design decisions. However, the effectiveness of any learner with finite data/training is inevitably limited, especially in regions of the input space where training data is sparse. In short, we are learning a policy to cover all possible histories we might see, but at deployment we are dealing only with a specific history that may be similar to few, if any, of the histories we simulated during training. This challenge is particularly exacerbated in experiments with extended horizons, due to the high dimensionality of the resulting histories. Additionally, the finite representational capacity of the policy hinders perfect approximation even with infinite data. Together these lead to a discrepancy between the learned policy $\\pi$ and the true optimal design strategy $\\pi ^ { * }$ producing an approximation gap for the learned policies.\n\nDouble reliance on the generative model Fully amortized PB-BED relies on the generative model to both simulate experimental histories for policy training and to evaluate the success of our design decisions via the total resulting information gained. In other words, we use the model in both the expectation and information gain elements of the EIG in (2). This dual reliance magnifies the consequences of model misspecifications (Go and Isaac, 2022; Overstall and McGree, 2022). Moreover, even if the model is wellspecified from a Bayesian inference perspective,there might still be significant discrepancies between the prior-predictive distribution, $p ( h _ { T } | \\pi )$ , used to simulate data in the policy training and the true underlying data generating distribution.\n\nThe upshot of this is that we may see data at deployment that is highly distinct from any simulated during the policy training. The lack of mechanisms for integrating real experimental data means that fully amortized approaches have no mechanism to overcome this issue. This can be characterized as a form of generalization gap—the learned policy fails to generalize to the real-world experimental conditions, due to its inability to integrate and respond to the actual experimental data gathered so far (Hastie et al., 2009).\n\n# 3.1. Online policy updating\n\nTo address these limitations, we propose a semi-amortized PB-BED framework, which introduces dynamic adaptability by allowing periodic updates to the policy during deployment in response to acquired experimental data. In short, it will update the original policy at one or more points during the experiment, refining it to maximize the EIG of the remaining steps, conditioned on the data gathered so far.\n\nThe motivation behind this semi-amortized framework is the intuition that while a fully amortized policy is a strong starting point, it can be significantly enhanced through targeted refinements leveraging gathered data. Focusing for now on the case of a single policy update, the following proposition formalizes this intuition and lays the theoretical foundation for semi-amortized PB-BED.\n\nProposition 3.1 (Decomposition of total EIG). For any design policy $\\pi$ , the total EIG of a $T$ -step experiment can be decomposed as\n\n$$\n\\begin{array} { r } { \\mathcal { T } _ { 1 \\to T } ( \\pi ) = \\mathcal { T } _ { 1 \\to \\tau } ( \\pi ) + \\mathbb { E } _ { p ( h _ { \\tau } \\mid \\pi ) } [ \\mathcal { T } _ { \\tau + 1 \\to T } ^ { h _ { \\tau } } ( \\pi ) ] , } \\end{array}\n$$\n\nfor any intermediate step $1 \\leq \\tau \\leq T$ , where\n\n$$\n\\begin{array} { r l } & { \\mathcal { T } _ { \\tau + 1  T } ^ { h _ { \\tau } } ( \\pi ) = } \\\\ & { \\mathbb { E } _ { p ( \\theta | h _ { \\tau } ) p ( h _ { \\tau + 1 : T } \\mid h _ { \\tau } , \\theta , \\pi ) } [ \\log \\frac { p ( h _ { \\tau + 1 : T } \\mid h _ { \\tau } , \\theta , \\pi ) } { p ( h _ { \\tau + 1 : T } \\mid h _ { \\tau } , \\pi ) } ] . } \\end{array}\n$$\n\nProof. We can write the likelihood and marginal as\n\n$$\n\\begin{array} { r l } & { p ( h _ { T } \\mid \\theta , \\pi ) = p ( h _ { \\tau } \\mid \\theta , \\pi ) p ( h _ { \\tau + 1 : T } \\mid h _ { \\tau } , \\theta , \\pi ) } \\\\ & { \\quad p ( h _ { T } \\mid \\pi ) = p ( h _ { \\tau } \\mid \\pi ) p ( h _ { \\tau + 1 : T } \\mid h _ { \\tau } , \\pi ) . } \\end{array}\n$$\n\nSubstituting in (2) and rearranging now yields\n\n$$\n\\begin{array} { l } { \\displaystyle \\mathcal { T } _ { 1 \\to T } ( \\pi ) = \\mathbb { E } _ { p ( \\theta ) p ( h _ { \\tau } | \\theta , \\pi ) } \\left[ \\log \\frac { p ( h _ { \\tau } | \\theta , \\pi ) } { p ( h _ { \\tau } | \\pi ) } \\right] + } \\\\ { \\displaystyle \\mathbb { E } _ { p ( h _ { \\tau } | \\pi ) p ( \\theta | h _ { \\tau } ) p ( h _ { \\tau + 1 : T } | h _ { \\tau } , \\theta , \\pi ) } \\left[ \\log \\frac { p ( h _ { \\tau + 1 : T } | h _ { \\tau } , \\theta , \\pi ) } { p ( h _ { \\tau + 1 : T } | h _ { \\tau } , \\pi ) } \\right] } \\end{array}\n$$\n\n$= \\mathbb { Z } _ { 1 \\to \\tau } ( \\pi ) + \\mathbb { E } _ { p ( h _ { \\tau } \\mid \\pi ) } [ \\mathbb { Z } _ { \\tau + 1 \\to T } ^ { h _ { \\tau } } ( \\pi ) ]$ as required.\n\nThis decomposition of the total EIG into two distinct components—the EIG accumulated up to an intermediate step $\\tau$ , and the expected EIG for subsequent steps conditional on the history at that point $h _ { \\tau }$ —demonstrates that the optimality of a policy for the latter phases of the experiment, from $\\tau + 1$ to $T$ , is solely determined by the model.\n\nTo see this, first note that, without loss of generality, we can break down the definition of our policy into how it behaves when given histories of length less than $\\tau$ and when it is given longer histories, such that $\\pi ( h _ { t } ) = \\pi _ { 0 } ( h _ { t } )$ if $t < \\tau$ and $\\pi ( h _ { t } ) = \\pi _ { \\tau } ( h _ { t } )$ if $t \\geq \\tau$ . We can thus rewrite (4) as\n\n$$\n\\begin{array} { r } { \\mathcal { T } _ { 1  T } ( \\pi ) = \\mathcal { T } _ { 1  \\tau } ( \\pi _ { 0 } ) + \\mathbb { E } _ { p ( h _ { \\tau } \\mid \\pi _ { 0 } ) } [ \\mathcal { T } _ { \\tau + 1  T } ^ { h _ { \\tau } } ( \\pi _ { \\tau } ) ] . } \\end{array}\n$$\n\nHere the first term is independent of $\\pi _ { \\tau }$ , while $\\pi _ { 0 }$ affects the second one only through its influence on the distribution of $h _ { \\tau }$ . At deployment time, we will have a specific $h _ { \\tau }$ once we reach step $\\tau$ of the experiment, and so the conditional optimal policy for the remaining steps given the data gathered so far is arg $\\begin{array} { r } { \\operatorname* { m a x } _ { \\pi _ { \\tau } } \\mathcal { T } _ { \\tau + 1  T } ^ { h _ { \\tau } } ( \\overline { { \\pi } } _ { \\tau } ) } \\end{array}$ , which is independent of $\\pi _ { 0 }$\n\nOur semi-amortized framework is now based around exploiting this independence to refine the policy midway through the experiment by introducing a step design policy $\\pi ^ { s }$ . Initially, $\\pi ^ { s }$ uses the fully amortized policy $\\pi _ { 0 }$ for the first $\\tau$ steps of the experiment. Here $\\pi _ { 0 }$ trained as if would be used for the full experiment, such that we maximize its total EIG, $\\mathcal { T } _ { 1  T } ( \\pi _ { 0 } )$ . After step $\\tau$ , $\\pi ^ { s }$ switches to a new policy $\\pi _ { \\tau }$ , which is trained to maximize the total remaining EIG, hτ (πτ ), as defined in (5).\n\nThis gives us an infer-refine process for semi-amortization in PB-BED that mirrors the two stage procedure characteristic of traditional adaptive BED (cf Fig. 1a and Fig. 1c). The infer stage entails fitting the posterior distribution $p ( \\theta \\mid h _ { \\tau } )$ with the data up to $\\tau$ . The subsequent refine stage learns a customized policy $\\pi _ { \\tau }$ for the remaining steps of the experiment by maximizing (5). It therefore allows for more effective design decisions than the fully amortized approach. However, unlike the traditional BED approach, which is greedy and requires updates at every experimental step, our semi-amortized method offers a superior non-myopic design strategy and allows for selective updates.\n\nIt is important to acknowledge that this approach requires some computation during the experiment, which can pose challenges in applications where design decisions must be made very quickly. However, in many cases there is some computation time available, and our semi-amortized approach can exploit this, even if the available time is limited. In particular, as we will show in subsequent sections, improvements to the policy can often be achieved with minimal additional training, such that substantial gains are often possible without drastically compromising deployment speed. As such, semi-amortized PB-BED still maintains large computational benefits over traditional adaptive BED.\n\nMulti-step policy updates We can naturally extend our approach to include a multi-step update mechanism, noting that Proposition 3.1 can be applied recursively to break down the total EIG into more segments. To this end, we define a refinement schedule, $\\mathcal { T } = \\tau _ { 0 } , \\tau _ { 1 } , \\cdot \\cdot \\cdot , \\tau _ { K }$ —an increasing sequence defining the points at which the policy is refined. We adopt the convention $\\tau _ { 0 } = 0$ and $h _ { 0 } = \\emptyset$ , marking the offline optimization of the fully-amortized policy $\\pi _ { 0 }$ . For $\\tau _ { k } > 0$ , we follow our two-stage infer-refine procedure. In general, the more often we refine the policy, the better it will be (albeit with diminishing returns), at the cost of increasing the required deployment-time computation.\n\n# 4. Stepwise Deep Adaptive Design\n\nWe introduce Stepwise Deep Adaptive Design (Step-DAD) to implement our semi-amortized PB-BED framework in practice. Building on DAD and the infer-refine procedure outlined in the last section, Step-DAD employs stochastic gradient ascent schemes to optimize variational lower bounds on the remaining EIG (5) to sequentially train the step policy $\\pi ^ { s }$ in a scalable manner. An overview of StepDAD is presented in Algorithm 1 in Appendix A\n\nThe two key components of Step-DAD’s aforementioned infer-refine procedure are an inference method for approximating $p (  { \\boldsymbol { \\theta } } | h _ { \\tau } )$ , and a refinement strategy for using this to update our policy. Standard inference techniques (such as variational inference or Monte Carlo methods) can be used for the former as discussed in our experiments. Our focus here will therefore instead be on our specialized procedure for policy refinement and the policy architecture itself.\n\n# 4.1. Policy refinement\n\nDue to its doubly intractable nature, the task of optimizing the remaining EIG, Iτhkτk+1 T (π), presents a notable challenge. In selecting an appropriate scalable and efficient estimator for it, we wish to ensure compatibility with a wide range of inference schemes for $p ( \\theta | h _ { \\tau _ { k } } )$ . Namely, as this serves as an updated ‘prior’ during the policy refinement, it is important that we use an EIG estimator that does not require evaluations of the prior density, to ensure compatibility with sample-based inference schemes.\n\nLower bound estimators such as the explicit-likelihoodbased sequential Prior Contrastive Estimator (sPCE, Foster et al., 2021), as well as the implicit likelihood InfoNCE (Ivanova et al., 2021; van den Oord et al., 2018) and NWJ (Kleinegesse and Gutmann, 2020; Nguyen et al., 2010) bounds, align with this requirement. For generative models with explicit likelihoods (implicit models are discussed in Appendix B) we therefore use the sPCE bound:\n\n$$\n\\mathcal { L } _ { \\tau _ { k } + 1  T } ^ { h _ { \\tau _ { k } } } ( \\pi ) = \\mathbb { E } [ \\log \\frac { p ( h _ { \\tau _ { k } + 1 : T } | \\theta _ { 0 } , h _ { \\tau _ { k } } , \\pi ) } { \\frac { 1 } { L + 1 } \\sum _ { \\ell = 0 } ^ { L } p ( h _ { \\tau _ { k } + 1 : T } | \\theta _ { \\ell } , h _ { \\tau _ { k } } , \\pi ) } ] .\n$$\n\nStep-DAD parameterizes $\\pi$ by a neural network and optimizes an appropriate objective, such as (6), with respect to the network parameters using stochastic gradient ascent (SGA) schemes (Kingma and Ba, 2014; Robbins and Monro, 1951). Following Foster et al. (2021), we use path-wise gradients in the case of reparametrizable distributions (Mohamed et al., 2020; Rezende et al., 2014), and score function (REINFORCE) otherwise (Williams, 1992).\n\n# 4.2. Policy architecture\n\nSimilar to Foster et al. (2021), our policy architecture is based on individually embeding each design-outcome pair $( \\xi _ { i } , y _ { i } ) \\in h _ { t }$ into a fixed-dimensional representation, before aggregating them across $t$ to produce a summary vector. This allows for condensing varied-length experimental histories into a consistent dimensionality, to handle variable history sizes. The summary vector is then mapped to the next experimental design $\\xi _ { t + 1 }$ . For the aggregation mechanism, the choice between permutation invariant and autoregressive architectures depends on the nature of the data. When the data $h _ { t }$ is exchangeable, permutation invariant architectures like DeepSets (Zaheer et al., 2017) or SetTransformer (Lee et al., 2019) are suitable. In contrast, sequential or timeseries data would benefit from autoregressive models like transformers (Vaswani et al., 2017).\n\nIn principle, one could train an entirely new policy $\\pi _ { \\tau _ { k } }$ at each refinement step $\\tau _ { k }$ , potentially even varying the specific architecture between these. Though such a strategy may occasionally be advantageous, we instead, propose a more pragmatic and lightweight approach: leveraging the already established fully amortized policy $\\pi _ { 0 }$ as a baseline and finetuning it for subsequent steps. In our experiments we do this using full fine-tuning of all policy parameters, but one could instead implement more parameter-efficient methods if needed, for example, only adjusting the last few layers.\n\n# 5. Related Work\n\nThe idea of using a design policy in the context of adaptive BED was first proposed by Huan and Marzouk (2016). Leveraging dynamic programming principles, the policy they learn aims to establish a mapping from explicit posterior representations—serving as the state in reinforcement learning (RL) terminology—to subsequent design choices. As a result, each iteration of the experiment necessitates substantial computational resources for updating the posterior. The concept of fully amortized policy-based BED, which directly maps data collected to design decisions, has only recently been introduced (Foster et al., 2021) and subsequently extended to differentiable implicit models (Ivanova et al., 2021) and downstream tasks (Huang et al., 2024). While Step-DAD uses the policy training approach of DAD and iDAD based on direct SGA of variational bounds (e.g. (6)), our semi-amortized PB-BED framework is also compatible with more RL-based design policy training approaches, like those of Blau et al. (2022) and Lim et al. (2022), which are more suited to discrete design spaces. We emphasize that none of these previous approaches have looked to refine the policy during the experiment itself.\n\nAs discussed in $\\ S 2 . 1$ , adaptive BED has traditionally employed a two-step greedy strategy, involving posterior inference followed by an EIG optimization (Foster et al., 2019; 2020; Huan and Marzouk, 2014; Kleinegesse and Gutmann, 2019; 2020; 2021; Kleinegesse et al., 2021; Myung et al., 2013; Overstall and McGree, 2020; Price et al., 2018; Ryan et al., 2016; Vincent and Rainforth, 2017). While Step-DAD diverges from these in its EIG optimization by training policies instead of designs, it does share their need to perform posterior inference. The inference scheme used by previous work has varied between problems and the needs of the underlying Bayesian model being used, with sequential Monte Carlo (Del Moral et al., 2006; Drovandi et al., 2014; Rainforth, 2017; Vincent and Rainforth, 2017) and likelihoodfree (Huan and Marzouk, 2013; Lintusaari et al., 2017; Sisson et al., 2018; Thomas et al., 2016) inference schemes proving popular. There is also growing recent interest in approaches that utilize inference itself as part of the EIG optimization. (Amzal et al., 2006; Iollo et al., 2024; 2025;\n\nIqbal et al., 2024a;b). Important considerations in choosing this scheme include the availability of an explicit likelihood, the ability to take derivatives, and computational budget.\n\nThe challenge of model misspecification in BED remains a critical, but relatively underexplored, problem (Farquhar et al., 2021; Feng et al., 2015; Go and Isaac, 2022; Overstall and McGree, 2022; Rainforth et al., 2024; Sloman et al., 2022). Fully amortized PB-BED is particularly vulnerable to model misspecification due to its reliance on a singular learning phase without the capacity to integrate real-world experimental feedback. As we will see in the experiments, our semi-amortized PB-BED methodology, whilst not directly tackling the issue of misspecification, typically does enhance robustness to misspecification, due to enabling iterative data integration and policy refinement.\n\nFinally, BED shares important connections to reinforcement learning (Sutton and Barto, 2018). Most notably, it has been shown that the adaptive BED problem can be formulated as various forms of Markov Design Processes (MDPs (Doshi-Velez and Konidaris, 2016; Guez et al., 2012; Ross et al., 2007)), using the incremental EIG as the reward and either the posterior (Huan and Marzouk, 2016) or, more practically, the history as the state (Blau et al., 2022; Foster, 2021). Here PB-BED approaches are most closely linked with offline model-based RL (Kidambi et al., 2020; Levine et al., 2020; Moerland et al., 2023; Ross and Bagnell, 2012; Yu et al., 2020), in that they learn a policy upfront which can then be deployed. However, PB-BED varies in many significant ways from typical RL settings. For example, we do not have access to any data to train our policy, but are instead focused on optimal sequential decision-making under a given model. We can also typically directly use SGA to train the policy as we have access to end-to-end differentiable objectives and our rewards are typically not sparse. We also note that our extension of fully-amortized PB-BED to semi-amortized PB-BED is quite distinct to the typical generalization of offline RL approaches to hybrid RL approaches (Ball et al., 2023; Song et al., 2023; Zheng et al., 2022), as we are making refinements to the local policy during a single rollout using the same objective as original amortized policy. Our approach of refining a learned policy as new data becomes available also shares some similarities with Model Predictive Control (Qin and Badgwell, 2003).\n\n# 6. Experiments\n\nWe empirically evaluate Step-DAD on a range of design problems, comparing its performance against DAD to determine the additional EIG achieved by the step policy $\\pi ^ { s }$ over the fully amortized policy $\\pi _ { 0 }$ . We further consider several other baselines for comparison. Static design learns a fixed set of designs prior to the experiment by optimising a PCE bound (Foster et al., 2020) that is equivalent to (6), but which is defined in terms of the designs rather than policy parameters (i.e. it learns a non-adaptive policy whose design choices are fixed). The Step-Static baseline is a two-stage approach that first trains a set of $\\tau$ static designs by optimizing a PCE bound on $\\mathcal { T } _ { 1  \\tau } ( \\xi _ { 1 } , \\dots , \\xi _ { \\tau } )$ , before approximating the posterior and training a new conditional set of static designs for the last $T - \\tau$ steps by optimizing a PCE bound on τhτ+1 T (ξτ+1, . . . , ξT ). When possible, we include Problem-Specific baselines used for the relevant experiment before, instead considering a Random design strategy when not. In all cases, the number of contrastive samples used during training was $L = 1 0 2 3$ .\n\nTable 1: Source Location Finding. Upper and lower bound estimates of total EIG. We report $\\tau = 6$ , rest in Table 11 in the Appendix. Errors show $\\pm 1 \\mathrm { s . e . }$ , computed over 16 (2048) histories for step methods (rest). DAD was trained for 50K steps, Step-DAD for 2.5K.   \n\n<html><body><table><tr><td>Method</td><td>Lower bound (↑) Upper bound (↓)</td></tr><tr><td>Random 3.612 ± 0.012</td><td>3.613 ± 0.012</td></tr><tr><td>Static 3.945 ± 0.026</td><td>3.946 ± 0.026</td></tr><tr><td>Step-Static (T = 6)</td><td>3.974 ± 0.008 3.975 ± 0.008</td></tr><tr><td>DAD</td><td>7.040 ± 0.012 7.089 ± 0.013</td></tr><tr><td>Step-DAD(T = 6)</td><td>7.759 ± 0.114 7.765 ± 0.114</td></tr></table></body></html>\n\nOur main metric for assessing the quality of various design strategies is the total EIG, $\\mathcal { T } _ { 1  T } ( \\pi )$ , as given in (2). For the baselines, we approximate it via a version of the sPCE lower bound (6) with $L = 1 0 ^ { 5 }$ to ensure a tight bound, along with its upper bound counterpart—the sequential Nested Monte Carlo estimator (sNMC, Foster et al., 2021) (see Appendix B). For Step-DAD, we instead use a conservative lower bound estimate on its difference in performance compared to the original DAD network, before adding this to the corresponding DAD estimate (see Appendix C.3). This ensures any biases from using bounds instead of the true EIG lead to underestimation of the gains Step-DAD gives. Full details on experimental details are provided in Appendix C.\n\n# 6.1. Source Location Finding\n\nWe first consider the source location finding experiment from Foster et al. (2021), which draws upon the acoustic energy attenuation model, detailed in Sheng and Hu (2005). The objective of the experiment is to infer the locations of some hidden sources using noisy measurements, $y$ , of their combined signal intensity. Each source emits a signal that decreases in intensity according to the inverse-square law. A full description of the model is given in Appendix C.5.\n\nWe begin by learning a fully amortized DAD policy to perform $T = 1 0$ experiment steps to locate a single source. We chose a training budget of 50K gradient steps this policy, as we found that further training did not significantly improve performance with our chosen architecture.\n\nSingle policy update We systematically evaluate our Step\n\n![](images/580c790f8098a44515796425fca9532a8988ad1cd96cdbb66ea2145a00f4f26b.jpg)  \nFigure 2: Sensitivity to training budget for location finding experiment. DAD policies are trained for 50K or 10K steps, Step-DAD policies are refined for 2.5K. Errors show $\\pm 1 \\mathrm { s . e }$ .\n\nDAD approach by exploring all possible fine-tuning steps, $\\tau = 1 , \\dots , 9$ . We use importance sampling for posterior inference and fine-tune the policy for $2 . 5 \\mathrm { K }$ steps. Results for $\\tau = 6$ are presented in Table 1, whilst Table 11 in the Appendix shows performance for all values of $\\tau$ .\n\nSensitivity to training budget We investigate the overall resource efficiency of Step-DAD by comparing to DAD under two training budgets. The full budget is as before at 50K gradient steps, whilst the reduced budget is limited to 10K steps, both then followed by $2 . 5 \\mathrm { K }$ finetuning steps for the Step-DAD networks. Figure 2 presents a conservative comparison, showing upper bound estimates for DAD and lower bound estimates for Step-DAD. The results reveal that Step-DAD consistently outperforms its respective DAD baseline for all $\\tau > 1$ at both budget levels (the apparent slight drop for $\\tau$ is likely due to the conservative estimation scheme used). Interestingly, Step-DAD with the reduced budget matches or exceeds the DAD with the full budget for all $\\tau > 3$ , thereby achieving better results with nearly 5 times fewer total training steps.\n\nWe note that the performance advantage of Step-DAD over DAD appears to be most pronounced when fine-tuning occurs just past the midpoint of the experiment, that is for $\\tau = 6 , 7$ or 8. At this stage, our method can effectively leverage the accumulated data to refine the policy, while ensuring there are enough experiment steps remaining to benefit from the improved, customized policy.\n\nMultiple sources We next consider a more complex setting of locating 2, 4 and 6 sources, which correspond to a 4-, 8- and 12-dimensional unknown parameter, respectively. Table 2 shows the results, indicating a consistent positive EIG difference for Step-DAD over DAD.\n\n# 6.2. Robustness to prior perturbations\n\nIn BED, selecting an appropriate prior is critical as it both influences our final posterior, and the data we gather in the first place (Go and Isaac, 2022; Simchowitz et al., 2021).\n\nTable 2: Location finding multiple sources. Reported for $\\tau = 7$ , Step-DAD (10K, 2.5K). Errors show $\\pm 1 \\mathrm { s . e }$ .   \n\n<html><body><table><tr><td>0dim</td><td>EIG difference</td><td>DAD, total EIG (upper)</td></tr><tr><td>4</td><td>0.701 ± 0.023</td><td>6.483 ± 0.055</td></tr><tr><td>8</td><td>0.426 ± 0.014</td><td>7.111 ± 0.067</td></tr><tr><td>12</td><td>0.423 ± 0.012</td><td>6.956 ± 0.056</td></tr></table></body></html>\n\n![](images/0bcebd35bedce0e479e05c13b3c6301c16c75fd20bb42e6b80b7927ea95dd36b.jpg)  \nFigure 3: Sensitivity to prior perturbations in the location finding experiment. Total EIG for Step-DAD remains more robust compared to DAD, which drops to zero.\n\nFully amortized PB-BED approaches can be particularly prone to pathologies from imperfect prior choices, as the prior dictates the generated data the policy is trained on. Namely, if the prior predictive poorly matches the true data generating process (DGP), we may observe data at deployment that is highly distinct to anything seen in the policy training. To evaluate if Step-DAD can improve robustness to prior imperfections, we now consider a case where the prior, $p ( \\theta )$ , used for the offline policy training leads to a DGP that is significantly different to the true DGP observed at deployment. To this end, we introduce a test-time prior $\\tilde { p } ( \\boldsymbol { \\theta } )$ , and evaluate the policy performance under the DGP $\\begin{array} { r } { \\tilde { p } ( y _ { 1 : T } | \\xi _ { 1 : T } ) = \\mathbb { E } _ { \\tilde { p } ( \\theta ) } [ \\prod _ { t = 1 } ^ { T } p ( y _ { t } | \\theta , \\xi _ { t } , h _ { t - 1 } ) ] } \\end{array}$ , using the EIG under this alternative model as an evaluation metric\n\n$$\n\\begin{array} { r } { \\mathcal { T } _ { \\tilde { p } ( \\theta ) } ( \\pi ) : = \\mathbb { E } _ { \\tilde { p } ( \\theta ) p ( h _ { T } \\mid \\theta , \\pi ) } \\left[ \\log p ( h _ { T } \\mid \\theta , \\pi ) - \\log \\tilde { p } ( h _ { T } | \\pi ) \\right] , } \\end{array}\n$$\n\nThe results for the source location finding design problem are shown in Figure 3. It reveals that Step-DAD consistently outperforms the DAD baseline across all degrees of prior shift we consider, with the EIG for DAD decreasing to essentially zero with the increased prior shift, whilst Step-DAD is able to deliver positive information gains. This robustness is anticipated due to Step-DAD’s ability to assimilate data gathered and adjust policies in light of new evidence.\n\n# 6.3. Test-Time Compute Ablations\n\nWe now perform ablations to better understand how testtime performance is impacted by restrictions on computational budget. Firstly, for the location finding experiment we consider three different per-step budgets for the inference and then vary the amount of fine-tuning steps performed for updating the StepDAD network. As shown in Figure 4, total EIG generally improves with higher computational budgets, with diminishing returns for large budgets. The performance for the $100 \\%$ and $20 \\%$ inference budgets are quite similar, indicating our inference has been successful, but a performance drop is seen when only using $20 \\%$ of our previous inference budget. Importantly though, significant gains relative to DAD (corresponding to 0s wall time in the plot) are still achieved with small budgets corresponding to around a minute of wall time.\n\n![](images/1a19b04d436f0100a28375c7cc81b6567a75ee4897794486b352b0e549359a4f.jpg)  \nFigure 4: EIG as a function of wall time in the location finding experiment $\\mathrm { { T } = 1 0 }$ , $\\tau = 6$ ), varying both the inference budget and the number of fine-tuning steps (between 250 and 10000 steps). Here $100 \\%$ inference budget corresponds to taking 20000 importance samples.\n\n![](images/06e5d62e034290db59713e170360f67f1866e604a24f0e73784f5deacb94b7f4.jpg)  \nFigure 5: Sensitivity to number of interventions for location finding experiment $( \\mathrm { T } { = } 1 0 ) _ { , }$ ) for different inference budgets $1 0 0 \\% = 2 0 0 0 0$ importance samples). EIG increases with more interventions, then plateaus. Each update corresponds to posterior inference $+ 2 . 5 \\mathrm { k }$ fine-tuning steps.\n\nSecondly, we evaluate how performance evolves as a function of the number of interventions. Each intervention consists of updating the posterior and applying $2 . 5 \\mathrm { k }$ fine-tuning steps. Figure 5 shows that Total EIG consistently increases with more interventions across all inference budgets, before eventually plateauing. We note that even for a small number of interventions, there is increased performance over DAD (corresponding to 0 interventions in the plot), demonstrating the utility of Step-DAD even in settings of constrained test-time compute budget.\n\n![](images/f9e1a3b05911f9cdc8f6eddf412f7ff78cb43da20d96e00aa50ad3674dba182b.jpg)  \nFigure 6: Hyperbolic temporal discounting. EIG improvement of Step-DAD over DAD after fine-tuning the policy at step $\\tau$ . The fully amortized DAD policy is trained for 100K steps, step-policy is refined for 1K steps.\n\nTable 3: Hyperbolic temporal discounting. Estimates of total EIG, $\\scriptstyle { \\mathcal { T } } _ { 1 \\to 2 0 } ( \\pi )$ . Errors indicate $\\pm 1$ s.e., ran over 16 (2048) histories for the step methods (rest). Baselines as reported in Foster et al. (2021), except DAD and Random.   \n\n<html><body><table><tr><td>Method</td><td>Lower bound (↑)</td><td>Upper bound(↓)</td></tr><tr><td>Random</td><td>2.249 ± 0.010</td><td>2.249 ± 0.010</td></tr><tr><td>Kirby (2009)</td><td>1.861 ± 0.008</td><td>1.864 ± 0.009</td></tr><tr><td>Static</td><td>2.518 ± 0.007</td><td>2.524 ± 0.007</td></tr><tr><td>Frye et al. (2016)</td><td>3.500 ± 0.029</td><td>3.513 ± 0.029</td></tr><tr><td>Greedy (BADapted)</td><td>4.454 ± 0.016</td><td>4.536 ± 0.018</td></tr><tr><td>DAD</td><td>4.778 ± 0.013</td><td>4.808 ± 0.014</td></tr><tr><td>Step-DAD (T=10)</td><td>6.711 ± 0.040</td><td>6.721 ± 0.040</td></tr></table></body></html>\n\n# 6.4. Hyperbolic Temporal Discounting\n\nTemporal discounting describes the tendency for individuals to prefer smaller immediate rewards over larger delayed ones. This phenomenon is a key concept in psychology and economics and has been used to study important social and individual behaviors (Critchfield and Kollins, 2001), including dietary choices (Bickel et al., 2021), exercise habits (Tate et al., 2015), patterns of substance abuse and addictive behaviours (Story et al., 2014). An individual’s time delay preference is typically measured by asking them a series of questions, such as “Would you prefer $\\$ R$ now or $\\$ 100$ in $D$ days time?” Here the tuple $\\xi = ( R , D )$ defines our experimental design, and the experiment outcome $y$ is the participant’s decision to either accept or reject the delay.\n\nSingle update Using the hyperbolic discounting model introduced in Mazur (1987) and as implemented by Vincent (2016), we train a DAD policy for 100K gradient steps, aimed at designing $T = 2 0$ experiments. We select a grid of tuning steps $\\tau$ in the range from 2 to 18 in increments of 2. For posterior inference, we use simple importance sampling to draw samples from the posterior $p ( \\theta \\mid h _ { \\tau } )$ and $1 \\%$ of the original training budget (i.e. 1K gradient steps).\n\nFigure 6 reports the results and illustrates that Step-DAD yields an improvement in total EIG for all choices of $\\tau$ when compared to the baseline DAD policy. The largest increase occurs around, and shortly after, the middle of the experiment, aligning with our previous intuition: here sufficient data has been accumulated to inform a meaningful posterior update, whilst sufficient number of experiments remain to effectively deploy the refined policy. Table 3 demonstrates the superiority of Step-DAD over conventional baselines, including those derived from psychology research (Frye et al., 2016; Kirby, 2009; Vincent and Rainforth, 2017) and traditional BED approaches such as the BADapted (Vincent and Rainforth, 2017) approach which was specifically designed for this problem. It also outperforms the static BED strategy, highlighting the effectiveness of adaptive design strategies in extracting more valuable information from experiments.\n\nMultiple updates and design extrapolation We extend the deployment of DAD and Step-DAD policies for this problem to $T = 4 0$ experiment steps, doubling the scope at which they were originally trained, i.e. without retraining the DAD network. Step-DAD is fine-tuned at two steps, $\\tau$ and $2 \\tau$ , with $\\tau \\in \\{ 5 , 6 , 7 , 8 \\}$ . As Table 4 shows, Step-DAD demonstrates significantly improved capacity to extract information in later stages, beyond its initial training. This highlights the robustness and flexibility of our method in extending experimental horizons compared to DAD.\n\n# 6.5. Constant Elasticity of Substitution (CES)\n\nWe conclude our evaluation with the Constant Elasticity of Substitution (CES) model, a framework from behavioral economics to analyse the relative utility of two baskets of goods (Arrow et al., 1961). This model emulates how economic actors specify their relative preference $y$ between these baskets on a sliding scale. We follow the experimental setup of Foster et al. (2019), with full details given in Appendix C.7. The CES model faces challenges due to $y$ being sampled from a censored normal distribution, concentrating probability at observation boundaries and creating local maxima (Blau et al., 2022; Foster et al., 2019).\n\nTable 5 shows that Step-DAD outperforms all baselines. Step-Static also achieves on-par results with Greedy (vPCE), highlighting the benefits of semi-amortized design strategies in this model. We note the performance of DAD is worse than Step-Static. This can be attributed to the discontinuities in the censored likelihood (Eq.(29) in the Appendix) that complicate the training of the policy, often resulting in convergence to suboptimal local maxima.\n\nTable 4: Hyperbolic temporal discounting: extrapolating designs. Comparison of EIG upper bound for DAD and lower bound for Step-DAD across tuning steps $\\tau$ and $T =$ 40. Errors indicate $\\pm 1 \\mathrm { s . e . }$ , computed over 16 histories.   \n\n<html><body><table><tr><td></td><td colspan=\"2\">EIG from T (↑)</td><td colspan=\"2\">EIG from 2τ (↑)</td></tr><tr><td>T</td><td>DAD</td><td>Step-DAD</td><td>DAD</td><td>Step-DAD</td></tr><tr><td>5</td><td>3.9 ±0.24</td><td>4.8±0.14</td><td>2.1±0.36</td><td>4.6 ±0.18</td></tr><tr><td>6</td><td>3.4±0.30</td><td>5.1±0.07</td><td>1.7±0.30</td><td>4.7±0.12</td></tr><tr><td>7</td><td>3.0±0.35</td><td>4.4±0.30</td><td>1.3±0.23</td><td>4.4±0.11</td></tr><tr><td>8</td><td>2.6±0.36</td><td>4.7±0.13</td><td>1.0±0.19</td><td>4.2±0.13</td></tr></table></body></html>\n\nTable 5: Constant Elasticity of Substitution. Estimates on total EIG, $\\mathcal { T } _ { 1  1 0 } ( \\pi )$ . DAD and Static trained for 50K steps; step variants finetuned for 10K steps. Errors denote $\\pm 1$ s.e.   \n\n<html><body><table><tr><td>Method</td><td>Lower bound (↑)</td><td>Upper bound (↓)</td></tr><tr><td>Random</td><td>2.487 ± 0.007</td><td>2.487 ± 0.007</td></tr><tr><td>Greedy (vPCE)</td><td>13.333 ± 0.975</td><td>13.343 ± 0.975</td></tr><tr><td>Static</td><td>9.279± 0.020</td><td>11.183 ± 0.0453</td></tr><tr><td>Step-Static (T = 5)</td><td>13.010 ± 0.185</td><td>13.682 ±0.189</td></tr><tr><td>DAD</td><td>10.181 ± 0.021</td><td>11.478 ± 0.042</td></tr><tr><td>Step-DAD (T = 5)</td><td>13.879 ± 0.352</td><td>14.623 ± 0.363</td></tr></table></body></html>\n\n# 7. Conclusions\n\nIn this work, we introduced the idea of a semi-amortized approach to PB-BED that enhances the flexibility, robustness and effectiveness of fully amortized design policies. Our method, Stepwise Deep Adaptive Design (Step-DAD), dynamically updates its step policy in response to new data through a systematic ‘infer-refine’ procedure that refines the design policy for the remaining experiments in light of the experimental data gathered so far. This iterative refinement enables the step policy to evolve as the experiment progresses, ensuring more robust and tailored design decisions, as demonstrated in our empirical evaluation. Step-DAD thus improves our ability to conduct more efficient, informed, and robust experiments, opening new avenues for exploration in various scientific domains.\n\n# Impact Statement\n\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.",
    "institutions": [
        "未找到机构信息"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决传统自适应贝叶斯实验设计（BED）方法存在的次优决策和高计算成本问题，以及全摊销策略型BED方法无法根据实验数据调整策略的局限性。自适应实验在科学和工程中至关重要，能通过整合过往实验信息实现高效数据采集，但传统方法是贪婪短视策略，未考虑未来实验步骤，且每次迭代需大量计算，不适合实时应用；现有全摊销方法虽能实时部署，但无法根据实验数据调整策略，在实际数据与训练数据差异大时设计决策次优。解决这些问题对于提高实验设计的效率和准确性至关重要，在科学和工程领域的自适应实验中具有重要意义。\\n\\n**方法概述**\\n论文提出了分步深度自适应设计（Step - DAD）方法，这是一种半摊销的策略型BED方法，在实验前训练设计策略，实验过程中定期更新策略，通过“推断 - 细化”过程，依据已收集的实验数据优化剩余实验的设计策略。\\n\\n**主要贡献与效果**\\n- 提出半摊销的策略型BED方法，增强了全摊销设计策略的灵活性、鲁棒性和有效性。在多个实验中，Step - DAD相比全摊销策略型DAD方法显著提高了总期望信息增益（EIG）。如在源位置查找实验中，当$\\tau = 6$时，Step - DAD的总EIG下界为7.759 ± 0.114，高于DAD的7.040 ± 0.012；上界为7.765 ± 0.114，高于DAD的7.089 ± 0.013。在双曲时间折扣实验中，Step - DAD（T = 10）的总EIG下界为6.711 ± 0.040，高于DAD的4.778 ± 0.013。在常替代弹性（CES）模型实验中，Step - DAD（T = 5）的总EIG下界为13.879 ± 0.352，高于DAD的10.181 ± 0.021；上界为14.623 ± 0.363，高于DAD的11.478 ± 0.042。\\n- 提高了对先验扰动的鲁棒性。在源位置查找实验中，随着先验偏移增加，DAD的EIG降至接近零，而Step - DAD能始终实现正的信息增益。\\n- 在受限计算预算下仍能取得良好性能。在位置查找实验的计算消融实验中，即使使用约一分钟墙时间的小预算，Step - DAD相对于DAD仍能实现显著增益。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nStep - DAD的核心思想是结合完全摊销的基于策略的BED方法和传统自适应BED方法的优点，在实验过程中利用可用的计算资源，通过定期更新设计策略来适应实际实验数据。其基于总EIG的分解，将实验分为两个阶段，先使用全摊销策略进行前$\\tau$步实验，然后根据已收集的数据学习新的策略以最大化剩余步骤的总EIG，从而实现更有效的设计决策。它能有效是因为这种方式可以避免完全摊销方法无法适应实际数据的问题，以及传统方法的短视性。\\n\\n**创新点**\\n现有完全摊销的PB - BED方法无法根据获取的实验数据调整策略，导致在实际数据与训练数据差异较大时设计决策次优，且双重依赖生成模型会放大模型误设的后果。传统BED方法是贪婪的短视策略，且每次实验迭代需大量计算。Step - DAD通过引入定期更新策略的机制，解决了完全摊销方法的问题，同时提供了更优的非短视设计策略，并允许选择性更新，而不是像传统方法那样在每个实验步骤都进行贪婪更新。\\n\\n**具体实现步骤**\\n1. 训练完全摊销的策略$\\pi_0$：在实验前，使用随机梯度上升（SGA）方案优化变分下界，像使用全摊销策略进行完整实验一样训练$\\pi_0$，以最大化其总EIG。\\n2. 实验前期使用$\\pi_0$：在实验的前$\\tau$步，使用$\\pi_0$进行设计决策。\\n3. 推理阶段（Infer）：在第$\\tau$步，使用截至$\\tau$的数据拟合后验分布$p(\\theta|h_{\\tau})$。\\n4. 细化阶段（Refine）：训练一个新的策略$\\pi_{\\tau}$，以最大化剩余步骤的总EIG，即$\\mathcal{T}_{\\tau + 1 \\to T}^{h_{\\tau}}(\\pi_{\\tau})$。在策略细化过程中，使用如显式似然的顺序先验对比估计器（sPCE）等下界估计器来优化剩余EIG，并使用SGA方案更新策略参数。对于生成模型有显式似然的情况使用sPCE界。\\n5. 后续实验使用$\\pi_{\\tau}$：在第$\\tau$步之后，使用$\\pi_{\\tau}$进行设计决策。\\n6. 多步更新扩展：可以递归应用总EIG分解，定义细化时间表$\\mathcal{T} = \\tau_0, \\tau_1, \\cdots, \\tau_K$，在多个步骤更新策略。\\n\\n**案例解析**\\n在源位置查找实验中，首先学习一个完全摊销的DAD策略进行10步实验。然后在不同的$\\tau$值（1到9）下对Step - DAD策略进行微调。例如，当$\\tau = 6$时，通过重要性采样进行后验推断，并对策略进行2.5K步的微调。结果表明，Step - DAD在总EIG的上下界估计上均优于随机设计、静态设计等基线方法。在多源定位实验中，对于不同维度的未知参数，Step - DAD相对于DAD也有一致的正EIG差异。在双曲时间折扣实验中，训练一个DAD策略进行20步实验，选择2到18之间的$\\tau$值进行微调，Step - DAD在所有选择的$\\tau$值下总EIG均高于基线DAD策略。在常替代弹性（CES）模型实验中，Step - DAD同样表现出优于其他基线方法的性能。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 深度自适应设计 (Deep Adaptive Design, DAD)\\n- 静态设计 (Static Design, N/A)\\n- 分步静态设计 (Step - Static, N/A)\\n- 特定问题基线 (Problem - Specific Baselines, N/A)\\n- 随机设计策略 (Random Design Strategy, N/A)\\n\\n**性能对比**\\n*   **在 [总期望信息增益/Total Expected Information Gain, EIG] 指标上：** 在源位置查找实验中，当$\\tau = 6$时，Step - DAD的总EIG下界为7.759 ± 0.114，高于DAD的7.040 ± 0.012；上界为7.765 ± 0.114，高于DAD的7.089 ± 0.013。在多源定位实验中，对于不同维度的未知参数，Step - DAD相对于DAD均有正的EIG差异，如4维时EIG差异为0.701 ± 0.023。在双曲时间折扣实验中，Step - DAD（T = 10）的总EIG下界为6.711 ± 0.040，高于DAD的4.778 ± 0.013，且在所有选择的$\\tau$值下总EIG均高于基线DAD策略，最大增幅出现在实验中期附近。在常替代弹性（CES）模型实验中，Step - DAD（T = 5）的总EIG下界为13.879 ± 0.352，高于DAD的10.181 ± 0.021；上界为14.623 ± 0.363，高于DAD的11.478 ± 0.042，也优于随机设计（2.487 ± 0.007）、贪婪（vPCE）（13.333 ± 0.975）、静态设计（9.279 ± 0.020）和分步静态设计（13.010 ± 0.185）。\\n*   **在 [对先验扰动的鲁棒性/Robustness to Prior Perturbations] 指标上：** 在源位置查找设计问题中，随着先验偏移增加，DAD的EIG降至接近零，而Step - DAD能始终实现正的信息增益，表明Step - DAD在不同程度的先验偏移下均优于DAD基线，对先验扰动更具鲁棒性。\\n*   **在 [测试时间计算预算受限情况下的性能/Performance under Limited Test - Time Computational Budget] 指标上：** 在位置查找实验的计算消融实验中，即使使用约一分钟墙时间的小预算，Step - DAD相对于DAD仍能实现显著增益。总EIG通常随计算预算增加而提高，但存在收益递减的情况。在有限的测试时间计算预算下，即使进行少量干预，Step - DAD也能表现出优于DAD的性能。\",\n    \"keywords\": \"### 关键词\\n\\n- 贝叶斯实验设计 (Bayesian Experimental Design, BED)\\n- 半摊销策略型方法 (Semi - Amortized Policy - Based Method, N/A)\\n- 分步深度自适应设计 (Stepwise Deep Adaptive Design, Step - DAD)\\n- 自适应实验 (Adaptive Experimentation, N/A)\\n- 源位置查找实验 (Source Location Finding Experiment, N/A)\\n- 双曲时间折扣实验 (Hyperbolic Temporal Discounting Experiment, N/A)\\n- 常替代弹性模型实验 (Constant Elasticity of Substitution Model Experiment, N/A)\"\n}"
}