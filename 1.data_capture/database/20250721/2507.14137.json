{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14137",
    "link": "https://arxiv.org/abs/2507.14137",
    "pdf_link": "https://arxiv.org/pdf/2507.14137.pdf",
    "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
    "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "valeo.ai",
        "Fundamental AI Lab, UTN",
        "VIS Lab, UvA"
    ],
    "paper_content": "# Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning\n\nShashanka Venkataramanan1∗ Valentinos Pariza2∗ Mohammadreza Salehi2,3\n\nLukas Knobel2 Spyros Gidaris1 Elias Ramzi1 Andrei Bursuc1† Yuki M. Asano2†\n\n1valeo.ai, Paris. 2 Fundamental AI Lab, UTN. 3 VIS Lab, UvA.\n\n![](images/cc3ea160e11b779cd47b9964c0fa5d4e02fe7e4cff83f81a16d1df32e4a3e189.jpg)  \nFigure 1: Overview of Franca. Top-left: We learn efficient Matryoshka-style [Kusupati et al., 2022] visual representations using a multi-head clustering projection head. The encoder produces features $z \\in \\mathbb { R } ^ { d }$ , which is sliced into progressively smaller subsets of dimensions $d , \\dots d / 8 , d / 1 6$ . Each slice passes through a projection head and a corresponding clustering head with cluster counts $c , \\ldots , c / 8 , { \\overset { \\frown } { c } } / 1 6$ , inducing a coarse-to-fine hierarchy of semantic abstraction. Top-right: Unlike prior approaches trained on curated academic datasets, e.g., LVD-142M in DINOv2 or proprietary data like WebLI in SigLIPv2, Franca is trained on open-source internet-scale uncurated data. Bottom: Despite this, it generalizes well across model scales and achieves strong performance on diverse downstream tasks, including in-context learning [Balazevic et al., 2023], out-of-distribution detection [Yang et al., 2022], and 3D understanding [Chen et al., 2025].\n\n# Abstract\n\nWe present Franca (pronounced Fran-ka): ‘free’ one; the first fully open-source (data, code, weights) vision foundation model that matches—and in many cases surpasses—the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired from Web-SSL and uses publicly available data: Imagenet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca\n\n# 1 Introduction\n\nThe development of strong and robust large vision models has emerged as a critical axis in artificial intelligence research, with implications extending far beyond traditional vision tasks. These foundation models serve as essential building blocks for complex 3D systems and multimodal large language models (LLMs). Self-supervised learning (SSL) approaches have gained prominence due to their scalability advantage; the volume of available image-only data vastly exceeds that of paired image-caption data, enabling more comprehensive training regimes and more generalizable representations.\n\nDespite the demonstrated importance of these vision foundation models, there exists a striking scarcity of publicly available high-performance options. The current state-of-the-art models, including DINOv2 [Oquab et al., 2024], SEER [Goyal et al., 2021], billion-scale MAE [Singh et al., 2023], SigLIPv2 [Tschannen et al., 2025], rely exclusively on proprietary datasets, creating a significant barrier to reproducibility, accessibility, and scientific advancement. To address this gap, our primary contribution is an open-data, open-weight and open-code vision foundation model inspired from Web-SSL [Fan et al., 2025] that not only matches but surpasses the performance of these proprietary counterparts while maintaining complete transparency in implementation and training methodology. Moreover, we release intermediate checkpoints alongside our final model to provide insight into the full training trajectory. This enables the community to analyze convergence behavior, conduct representation analysis, and study emergent properties across time. From Table 1, Franca stands out as the first model to make all aspects—training code, data, model weights, and intermediate checkpoints—publicly available, setting a new benchmark for openness in the field. Furthermore, we identify a fundamental limitation in current SSL vision models that utilize clustering approaches, such as DINO and DINOv2. These methods assign images or patches to meaningful pseudo-labels according to clustering metrics (typically Sinkhorn-Knopp optimization), but they overlook a critical issue: clustering is inherently ambiguous. For example, vehicles can be meaningfully organized by manufacturer, color, model year, or numerous other attributes. Rather than addressing this ambiguity, existing approaches simply employ extremely large fine-grained cluster sets (e.g., DINOv2’s 131K codebook [Oquab et al., 2024]), which may be suitable for certain domains but inappropriate for others.\n\nTo overcome this limitation, we draw inspiration from early deep clustering works which demonstrated that employing multiple clustering heads can enhance performance ([Ji et al., 2019, Asano et al., 2020]). However, naively extending this approach to modern architectures with MLP projection heads (rather than simple linear heads) would result in an explosion of parameters. Our solution is a novel multi-head clustering projector that implements nested ‘Matryoshka’ [Kusupati et al., 2022] representations, where progressive subsets of neurons cluster the dataset into increasingly finer-grained groupings. This approach not only reduces parameters compared to conventional approaches but also improves performance and decreases memory requirements for downstream tasks such as $\\mathbf { k }$ -nearest neighbors classification, leading to higher performances at equal memory.\n\n<html><body><table><tr><td>ATTRIBUTE/MODEL</td><td>METACLIP</td><td>WEB-SSL</td><td>SIGLIPV2</td><td>AIMv2</td><td>CLIP</td><td>DINOv2</td><td>SAM</td><td>OPENCLIP</td><td>FRANCA</td></tr><tr><td colspan=\"10\">Training Code/Checkpoints</td></tr><tr><td>Model Publicly Available?</td><td></td><td></td><td>√</td><td>√</td><td>√</td><td></td><td>√</td><td></td><td></td></tr><tr><td>Training Code Public?</td><td></td><td></td><td>×</td><td>×</td><td>×</td><td></td><td></td><td></td><td></td></tr><tr><td>Intermediate Weights Public?</td><td></td><td>×</td><td></td><td>×</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan=\"10\">Training Data</td></tr><tr><td>Training Data Public?</td><td>X</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>~</td><td>√</td><td>√</td></tr><tr><td>DataDeduplication Code Public?</td><td></td><td></td><td>×</td><td>×</td><td></td><td>×</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Data NSFW&CSAMFiltered?</td><td></td><td></td><td>√</td><td>？</td><td>？</td><td>？</td><td>？</td><td>√</td><td>√</td></tr></table></body></html>\n\nTable 1: Openness of Visual Foundation Models. We analyze various models based on the public availability of their components. Notably, Franca exemplifies a fully open-source approach, providing complete transparency from model weights to underlying data and processing methods. $\\sim$ : partially; ?: not specified.\n\nFinally, we address a subtle but significant issue with dense clustering methods: they often produce clusters that reflect image position rather than semantic content. To mitigate this problem, we introduce a lightweight post-pretraining technique that first learns linear projections to predict patch positions, then projects the latent space to an orthogonal, of the positional information. The result is a dense representation space that emphasizes semantic content over spatial positioning, leading to substantial improvements on challenging benchmarks such as In-context learning with HummingBird benchmark [Balazevic et al., 2023] and OverClustering [Pariza et al., 2025] on Pascal-VOC [Everingham et al.] and COCO-Things [Caesar et al., 2018].\n\nOverall, our key contributions are:\n\n• We present Franca: the first open-source open-data vision foundation model trained on publicly available data that matches and even outperforms DINOv2 on a broad range of downstream tasks.   \n• We introduce a parameter-efficient nested multi-head clustering approach that efficiently improves representation quality.   \n• We develop a spatial-semantic disentanglement post-pretraining method that refines representations to yield stronger backbones.   \n• We achieve strong results on dense prediction tasks—including in-context learning, linear segmentation, and overclustering—outperforming DINOv2-G by up to $3 \\%$ . We also surpass DINOv2-G by $4 \\%$ on average across five OOD detection benchmarks, improve 3D understanding, and match its performance on classification, all without relying on proprietary data.\n\n# 2 Related Work\n\nOur work builds on and contributes to four major areas of prior research: self-supervised learning for visual representation learning, scaling strategies for data and model capacity in vision models, open-source foundation models, and techniques for disentangling semantic content from positional or representational biases.\n\nSelf-Supervised Learning (SSL) for Vision. Self-supervised learning has emerged as a powerful paradigm for visual representation learning without any manual annotations. By designing pretext tasks that utilize image structure as supervision signals, SSL methods enable models to learn transferable features. Early approaches used handcrafted objectives such as context prediction [Doersch et al., 2015], patch reordering [Noroozi and Favaro, 2016], colorization [Zhang et al., 2016, 2017], inpainting [Pathak et al., 2016], geometric transformation prediction [Gidaris and Komodakis, 2018], and instance discrimination [Dosovitskiy et al., 2014, Wu et al., 2018]. Modern SSL methods primarily focus on learning invariances across augmented data views. While early approaches leverage contrastive learning [Oord et al., 2018, Misra and Maaten, 2020, Chen et al., 2020a, He et al., 2020, Chen et al., 2020b, 2021] by aligning positive pairs and separating negatives, bootstrap-based [Grill et al., 2020, Chen and He, 2021, Gidaris et al., 2021] and distillation-based methods [Caron et al., 2021, Oquab et al., 2024] refine targets through teacher-student networks, often removing the need for negative pairs. More recently, Masked Image Modeling (MIM) has emerged as a dominant SSL strategy, where models learn to reconstruct masked patches [He et al., 2022, Zhou et al., 2022a, Bao et al., 2022, Wei et al., 2022]. Beyond these, clustering-based methods [Caron et al., 2018, 2020, Ji et al., 2019] have gained prominence, assigning pseudo-labels through algorithms like K-means or Sinkhorn-Knopp. The combination of MIM with clustering has shown particular promise, as exemplified by recent works such as MOCA [Gidaris et al., 2024] and CAPI [Darcet et al., 2025]. This hybrid approach leverages the strengths of both paradigms.\n\nWhile current vision foundation models often fall into categories like vision-language or MAE-like architectures, which have their own strengths and limitations (e.g., reliance on text supervision or need for task-specific adaptations), DINOv2 [Oquab et al., 2024] stands out as a powerful pretrained model employing a clustering-based approach. However, DINOv2 has two key limitations: as a clustering method, it doesn’t inherently capture the ambiguity often present in assignments at a fixed granularity, nor does it explicitly incorporate the benefits of modern hierarchical masking strategies. Our work addresses these concerns by integrating nested Matryoshka projections [Kusupati et al., 2022] directly into its objective. This allows each subspace to perform clustering at a different granularity, yielding diverse pseudo-labels efficiently (see Figure 1). Combined with an improved input masking strategy, our approach enables the joint learning of coarse-to-fine semantics without increasing model size, leading to strongly improved performances and reduction in memory.\n\nOpen Foundation Vision Models. The reliance on proprietary datasets in the training of current vision foundation models raises critical concerns regarding transparency, reproducibility, and the disentanglement of contributions. Models such as SEER [Goyal et al., 2019], DINOv2 [Oquab et al., 2024], CLIP [Radford et al., 2021], and billion-scale MAE [Singh et al., 2023] are all trained on proprietary data. This practice makes it challenging for the research community to isolate the true impact of model’s novelty and training strategies from the unique characteristics and biases of the datasets themselves. The lack of access to these datasets hinders independent verification, fair comparison, and a comprehensive understanding of what truly drives model performance. WebSSL [Fan et al., 2025] extends the study of large-scale self-supervised pretraining by training models on publicly available MetaCLIP-2B [Xu et al., 2024] dataset, showing that models trained on open data can approach the performance of those trained on proprietary data on VLM tasks.\n\nBuilding on this, we present a fully open-source vision foundation model using publicly available datasets, ReLAION [LAION, 2024], as it represents the most popular and safe public dataset for large-scale vision model training.\n\nSpatial correlations in learned representations. A common issue in dense self-supervised learning is the entanglement of semantic content with positional cues, causing models to rely on location rather than object identity. For instance, a model trained on “cows in grassy fields” and “camels in deserts” may misclassify a cow on a beach as a camel, due to learned associations with background context [Arjovsky et al., 2019]. Such spatial biases reduce generalization and can hinder performance when objects appear in atypical locations (e.g., a cow in the sky) [Singh et al., 2020].\n\nSeveral works have addressed this by proposing methods invariant to positional information. Lenc and Vedaldi [2015] enforce equivariance to geometric transformations; Wang et al. [2023] disentangle representations into orthogonal subspaces for content and style. Invariant Risk Minimization [Arjovsky et al., 2019] seeks features stable across environments, minimizing reliance on spurious cues. We propose a simple post-training strategy that learns a linear projection to identify and remove spatial information from features. Since, we use it as a post-training strategy, it requires no architectural changes and can be easily adapted to any pretrained model to reduce spatial bias.\n\n# 3 Method\n\nWe propose Franca, a scalable open-source self-supervised learning framework built on iBoT [Zhou et al., 2022b] and pretrained on large public image datasets. Franca tackles key limitations in existing vision SSL models through three main components. First, we use CyclicMask inspired from [Darcet et al., 2024], a masking strategy that circularly shifts masked patches to break simple spatial continuity and promote the learning of semantic features. Second, we introduce Matryoshka embeddings [Kusupati et al., 2022], a nested multi-head clustering approach that shares projection layers to generate compressed multi-resolution representations; and finally, a lightweight post-pretraining step that identifies and removes feature components correlated with absolute patch positions resulting in spatially invariant representations. Figure 2 shows that each of these components provides consistent gains in both in-context segmentation and linear classification performance on standard benchmarks.\n\nWe follow the multi-crop training strategy from DINO [Caron et al., 2021]. An input image $\\mathrm { ~ { ~ \\bf ~ { ~ \\chi ~ } ~ } ~ } \\in \\mathrm { ~ \\bf ~ { ~ \\chi ~ } ~ }$ $\\mathbb { R } ^ { h \\times w \\times c }$ is transformed into multiple augmented views (global and local crops). Each view is split into $\\begin{array} { r } { \\begin{array} { l l } { n \\ = } & { { } { \\frac { { \\bar { h _ { \\cdot } } } \\times w } { p ^ { 2 } } } } \\end{array} } \\end{array}$ h×2w non-overlapping patches of size $p \\times p$ . Each patch is embedded into $\\mathbb { R } ^ { d }$ , and a classification token $( [ \\mathsf { C L S } ] ) \\in \\mathbb { R } ^ { d }$ is prepended to form the input sequence. A Vision Transformer (ViT) backbone [Dosovitskiy et al., 2021] processes this sequence, producing $n + 1$ embeddings $n$ patch embeddings and one [CLS] embedding). The same ViT is shared between the student $f _ { \\theta }$ and teacher ${ \\bar { f } } _ { \\bar { \\theta } }$ , producing $\\begin{array} { r l } { Z _ { s } } & { { } = } \\end{array}$ $f _ { \\theta } ( x ) \\in \\mathbb { R } ^ { ( n + 1 ) \\times d }$ , $Z _ { t } = { \\bar { f } } _ { { \\bar { \\theta } } } ( x ) \\in$ $\\mathbb { R } ^ { ( n + 1 ) \\times d }$ , where $Z _ { s }$ represents the\n\n![](images/3b6e42615d02496eaac7bdcadb0d49aeb665a68409ba923ccf9c56f4563a4da9.jpg)  \nFigure 2: Pretraining ablation of Franca. Starting from a ViT-B/14 pretrained on ImageNet-21K, we show the impact of each proposed components. The inner bar represents in-context segmentation performance on the Hummingbird benchmark [Balazevic et al., 2023], while the outer bar shows linear probing accuracy on the ImageNet-1K [Russakovsky et al., 2015]. Each addition, i.e., CyclicMask, Matryoshka representations, RASA, and High resolution finetuning, results in consistent improvements.\n\nstudent’s output embeddings and $Z _ { t }$ represents the teacher’s output embeddings. The teacher’s parameters $\\bar { \\theta }$ are updated via exponential moving average (EMA) of the student’s parameters.\n\nFor supervision, we apply projection heads to the student embeddings $Z _ { s }$ . The [CLS] embedding is passed through a DINO-style head (a 3-layer MLP with softmax over prototypes) that produces image-level prototype scores, while the patch embeddings are processed by an iBOT-style head that produces patch-level prototype scores. For brevity, we denote both heads as $h _ { \\theta }$ for the student and $\\bar { h } _ { \\bar { \\theta } }$ for the teacher (same architecture, EMA-updated). The teacher’s projected outputs are clustered using Sinkhorn-Knopp [Cuturi, 2013] to produce balanced target distributions. The student is trained to match these targets via cross-entropy loss, denoted as $\\mathcal { L }$ .\n\n# 3.1 Matryoshka Representations for Efficient Multi-Granular Learning\n\nStandard self-supervised models learn to produce fixed-size embeddings, limiting their versatility with respect to varying computational budgets or downstream constraints. To enable flexible and semantically meaningful representations across multiple feature granularities, we use Matryoshka representations [Kusupati et al., 2022], which nest progressively truncated subspaces of a highdimensional embedding.\n\nThe standard Matryoshka approach slices the encoder’s output along the feature dimension and applies the same projection head to each sub-embedding. In contrast, we extend this setup by attaching a dedicated projection head and clustering objective to each subspace. This allows each slice to produce distinct prototypes and prototype assignments, encouraging specialization across representational granularities. Additionally, we reduce the number of prototypes per head proportionally to the subspace size—yielding a form of hierarchical clustering that aligns naturally with the granularity of the features across training steps.\n\nFormally, let $Z _ { s } = f _ { \\theta } ( x ) \\in \\mathbb { R } ^ { ( n + 1 ) \\times d }$ be the ViT’s output (patch $^ +$ [CLS] embeddings). We define nested dimensions $\\mathcal { M } = \\{ m _ { 1 } , . . . , m _ { k } \\}$ , where $m _ { 1 } < \\cdot \\cdot \\cdot < m _ { k } = d ,$ , and extract sub-embeddings\n\n$$\nZ _ { s } ^ { ( i ) } = Z _ { s } [ : , 1 : m _ { i } ] \\quad \\forall m _ { i } \\in \\mathcal { M }\n$$\n\nEach $Z _ { s } ^ { ( i ) }$ is processed by an independent projection head $h _ { \\theta } ^ { ( i ) }$ , with proportionally fewer prototypes as $m _ { i }$ decreases. A cross-entropy loss $\\mathcal { L } ^ { ( i ) }$ is applied to each head’s output. The total loss is the sum across all levels with equal weights:\n\n$$\n\\mathcal { L } _ { \\mathrm { t o t a l } } = \\sum _ { i = 1 } ^ { k } \\mathcal { L } ^ { ( i ) } .\n$$\n\nThe ViT encoder is shared across all levels, enabling simultaneous training of multi-granular representations that vary in capacity but remain semantically aligned.\n\nOur framework supports hierarchical learning: coarse heads capture global semantics, while fine heads focus on local structure akin to early clustering works [Ji et al., 2019, Asano et al., 2020, Van Gansbeke et al., 2020] and unlike most recent representation learning works that optimize only a single feature space [Oquab et al., 2024, Tschannen et al., 2025, Radford et al., 2021]. As shown in Figure 3, Franca significantly outperforms DINOv2 [Oquab et al., 2024] across all embedding sizes, especially under heavy compression. For fairness, we note that DINOv2 was not trained for dimensional truncation; its information is spread uniformly across the feature space. Even more notably, Figure 2 shows that the largest gains $( 4 \\% )$ from our Matryoshka framework occur in dense prediction tasks.\n\n![](images/464414f2a61aa5f8c454ef8ca72b31f6d54799752618a9ac72a765057137e894.jpg)  \nFigure 3: $k$ -NN classification accuracy on ImageNet- $\\nu 2$ at varying embedding slice levels using a ViT-L backbone. Franca consistently outperforms $\\mathrm { D I N O v } 2$ across all subspace dimensions, maintaining high performance even under strong compression $\\mathrm { ( d i m / 6 4 ) }$ . Note that DINOv2 was not trained with sliced dimensions and its features are uniformly distributed across the full embedding space.\n\n![](images/9b77fb9117b8115e52ff56bdb3d2705bba7dcdf7c155389d91213cc0239e920c.jpg)  \nFigure 4: Masking strategies used in masked image modeling. Compared to Random (a), Block (b), and Inverse (c) masking, our CyclicMask (d) circularly shifts the visible region across spatial axes, preventing the model from being biased toward specific spatial locations.\n\n# 3.2 Balancing Spatial Distribution of Visible Patches with CyclicMask\n\nMasked image modeling (MIM) is a core component in many self-supervised vision frameworks [Oquab et al., 2024, Zhou et al., 2022b,c], where a portion of input patches are masked and the model learns to predict the unmasked regions. Commonly adapted strategies include random masking and block masking, where patches are masked randomly or as a block across the image. While simple and stochastic, this approach lacks spatial structure, often leading to fragmented visible regions that provide limited contextual coherence as shown in Figure 4 (a) and (b).\n\nTo improve context continuity, inverse block masking [Baevski et al., 2023] retains a contiguous visible region, typically a fixed central block, while masking the periphery. This design offers structured context and improves prediction stability. From Figure 4 (c), we observe that the visible region is always at the center, which introduces a strong imbalance in the spatial distribution of the processed patches: certain patches are consistently unmasked (e.g., center), while others are rarely observed, leading to uneven training exposure across spatial locations.\n\nInspired from [Darcet et al., 2025], we introduce CyclicMask, a simple modification that mitigates this imbalance by circularly shifting the mask across both spatial axes during training. Specifically, the mask from inverse block masking is shifted randomly along the vertical and horizontal axes using wraparound (cyclic) shifts. This retains the benefit of a contiguous visible region while ensuring uniform exposure across all patch positions over time. As shown in Figure 4 (d), CyclicMask preserves structured context and, unlike fixed block masking, it avoids deterministic exposure patterns. While conceptually simple, this adjustment leads to more spatially balanced learning signals and eliminates positional bias. From Figure 2, CyclicMask improves performance of the baseline for linear probing on Imagenet-1K (IN-1K) and for in-context learning using open-hummingbird evaluation [Pariza et al., 2024] by $0 . 2 \\%$ .\n\n# 3.3 RASA: Removal of Absolute Spatial Attributes\n\nWhile ViTs have achieved strong performance across a variety of visual tasks, they often retain unintended spatial biases, particularly in self-supervised settings where no explicit location labels are used. These biases can emerge due to fixed patch layouts and learned positional embeddings, which may entangle spatial cues into representations that ideally should be semantic and positioninvariant.\n\n![](images/c5ee0d9115284dec3d84747a956cf1be44a6135c4816e8816ab966d77a1d27af.jpg)  \nFigure 5: Entropy of patch locations for each cluster. For each visual cluster predicted from the projection head on the patch embeddings, we compute the entropy of the 2D spatial coordinates of the patches assigned to it. A low entropy value indicates that the cluster consistently activates mostly at specific spatial positions (e.g., always top left patch), revealing positional bias in the representation. Left: We find several position-agnostic low-entropy clusters in DINOv2, that fire at specific image locations. Right: Compared to DINOv2, Franca produces clusters with significantly higher spatial entropy, demonstrating a more uniform and position-invariant distribution of patch activations across the image. This supports the effectiveness of RASA in removing linear positional components from the representation space.\n\nPreliminary study We first study the presence of spatial biases in the projection head that clusters the patches. To this end, we run the frozen model across images from Pascal VOC and COCO and keep track of the argmax cluster predictions for each patch location. Next, we compute the entropy across patch locations for each cluster, shown in Fig. 5. Notably, we find several semantically agnostic clusters, that instead of activating for certain content, only fire at specific image locations (see Fig. 5, left). More generally, we find that clusters tend to mix semantics and spatial position (e.g., a cluster for a car tire on the right vs another cluster for the same on the left of the image), indicating that even DINOv2 contains undesirable mixing of content and geometry. In contrast, Franca and RASA significantly increase the entropy of patch locations for almost all clusters, indicating better separation of content and geometry.\n\nTo address this, we propose Removal of Absolute Spatial Attributes (RASA), a post-training strategy to disentangle position information from patch embeddings. Once our model is pretrained, we use patch features $Z _ { s }$ to suppress the dimensions of the embedding space that are predictive of patch location.\n\nOur approach follows an alternating optimization procedure: at each iteration $t$ , we learn a simple 2D regression head $f _ { \\mathrm { p o s } } ^ { ( t ) }$ to predict the normalized 2D grid coordinates of each patch using a sigmoid activation. This head is trained on the patch embeddings $Z _ { s } ^ { ( t - 1 ) }$ to minimize a mean squared error between the predicted and actual normalized 2D positions. Once trained, the direction vectors in feature space used to predict the positional components are extracted and orthogonalized via GramSchmidt [Golub and Van Loan, 2013] to form a 2D subspace that encodes spatial layout. Finally, we obtain a new representation for each patch $Z _ { s } ^ { t }$ that is less aligned with positional information by simply removing the components parallel to this plane and keeping only the orthogonal ones.\n\nFormally, given $Z _ { s } = \\{ Z _ { h , w } \\in \\mathbb { R } ^ { D } \\} _ { s = 1 } ^ { n }$ , where $n$ is the number of patches in an image, we optimize the position prediction head parametrized by $W$ on a small set of images:\n\n$$\n\\begin{array} { c } { { \\displaystyle \\widehat { y } _ { s } = \\sigma \\big ( W Z _ { s } \\big ) , } } \\\\ { { \\displaystyle \\mathcal { L } _ { \\mathrm { p o s } } = \\frac { 1 } { n } \\sum _ { s = 1 } ^ { S } \\big \\| \\widehat { y } _ { s } - y _ { s } \\big \\| _ { 2 } ^ { 2 } , } } \\end{array}\n$$\n\nwhere $\\sigma$ is a sigmoid function, $W ~ \\in$ $\\mathbb { R } ^ { 2 \\times D }$ and $y _ { s }$ denotes the ground-truth normalized 2D grid coordinate for patch $n$ .\n\nAfter training, the two row-vectors $w _ { r } , w _ { c }$ of $W$ are orthonormalized to produce $u _ { r } , u _ { c }$ , spanning the positional plane (in\n\n![](images/c9a4645ee4c3ce0efec9f469e31cdbc93dec8fd845211758545072ab546bcd71.jpg)  \nFigure 6: Each iteration of RASA projects a patch embedding $Z _ { s }$ onto a learned positional plane $\\mathsf { \\tilde { s p a n } } \\{ u _ { r } , u _ { c } \\}$ and subtracts its projection $p _ { s }$ .\n\ngray in Figure 6. To obtain the component of the feature vectors that are orthogonal to this positional component, we project $Z _ { s }$ onto this subspace and subtract the projection:\n\n$$\n\\begin{array} { r } { p _ { s } = \\langle Z _ { s } , u _ { r } \\rangle u _ { r } + \\langle Z _ { s } , u _ { c } \\rangle u _ { c } , } \\\\ { Z _ { s } ^ { ( t ) } = Z _ { s } ^ { ( t - 1 ) } - p _ { s } ^ { ( t ) } . \\qquad } \\end{array}\n$$\n\nThis is an iterative refinement where at each iteration $t$ , the remaining positional plane $p _ { s }$ is used to generate the next subspace $Z _ { s }$ . This iterative process is visually summarized in Figure 6. We run this process multiple times until the positional loss ${ \\mathcal { L } } _ { \\mathrm { p o s } }$ stops decreasing, indicating positional bias is abundant in the feature space.\n\nEmpirically, we find that a small number of iterations (e.g., $T = 9$ ) suffices to remove the linear positional bias while preserving the core semantics of the learned representation.\n\nIntegrating RASA into the last linear layer As each iteration of RASA in Eq. 2 can be parameterised as a linear matrix-multiply with $L ^ { ( \\dot { t } ) }$ as follows,\n\n$$\n\\begin{array} { r } { Z _ { s } ^ { ( t ) } = Z _ { s } ^ { ( t - 1 ) } L ^ { ( t ) } = Z _ { s } ^ { ( t - 1 ) } ( I - p _ { s } ^ { ( t ) } ) = Z _ { s } ^ { ( t - 1 ) } ( I - u _ { r } u _ { r } ^ { \\top } + u _ { c } u _ { c } ^ { \\top } ) , } \\end{array}\n$$\n\nthe final transformation remains again a linear matrix multiplication $L = \\prod L ^ { ( t ) }$ , which can be multiplied into the weights of the final ViT layer, leading to no architectures hanges.\n\n# 4 Implementation details\n\nPretraining Datasets We pretrain Franca on large-scale, publicly available image-only datasets to ensure full reproducibility. We use the ImageNet-21K [Ridnik et al., 2021], which contains approximately 13.1M high-quality, hierarchically labeled images across 21,841 classes. This dataset offers broad visual coverage and is widely used in foundation model pretraining. To further scale up training and improve generalization, we also leverage LAION- $6 0 0 \\dot { \\mathbf { M } } ^ { 3 }$ , a subset of ReLAION-2B, which is a research-safe version of the LAION-5B dataset [Schuhmann et al., 2022, LAION, 2024]. While LAION-5B is originally paired image-text data, we discard the text and use only the image modality.\n\nTraining Franca’s architecture follows DINOv2 [Oquab et al., 2024] without registers, using Vision Transformers [Dosovitskiy et al., 2021] of varying model capacities: ViT-B with 86M parameters, ViT-L with 300M, and ViT-G with 1.1B. All models are trained from scratch for 500 epochs (625K iterations) without distillation from larger models, unlike DINOv2, which distills from ViT-G into smaller variants. We employ Matryoshka [Kusupati et al., 2022] with five nested heads on the ViT backbone with feature dimension $[ \\dot { d } , \\textstyle \\frac { d } { 2 } , \\dots , \\frac { d } { 1 6 } ]$ .\n\nFor LAION-600M, we use global crops scale of [0.48, 1.0], following DINOv2-style augmentations. Stochastic depth regularization is set to 0.1 for ViT-B and 0.4 for ViT-L and ViT-G. We use a total batch size of 2048 for ViT-B and 3072 for both ViT-L and ViT-G, distributed across 32, 64 and 128 H100 GPUs for ViT-B, ViT-L and ViT-G respectively. The learning rate is set to $1 \\times 1 0 ^ { - 3 }$ for the Base model and $3 . 5 \\times 1 0 ^ { - 4 }$ for the Large and Giant variants, using a cosine schedule with warmup of 80 epochs.\n\nWe train RASA on top of our frozen backbone on Pascal VOC using crops of resolution $5 1 8 \\times 5 1 8$ , batch size of 128, with AdamW [Loshchilov and Hutter, 2019] optimizer. For each of the nine incremental head training iterations, we used a two-layer positional projection head with sigmoid activation. In every iteration, only the top head was trained for 5 epochs with no weight decay and an initial learning rate of $2 \\times 1 0 ^ { - 4 }$ , while all heads from previous iterations were kept frozen.\n\nHigh-resolution adaptation We initialize the model with pretrained weights and finetune it at input resolution $5 1 8 \\times 5 1 8$ for 20K iterations using the same training procedure as in the initial pretraining phase. All schedules are preserved but temporally compressed to fit within the shorter training horizon. We retain the same hyperparameters as the original run, except for the base learning rate, which is reduced to $1 . 5 1 \\times \\mathrm { \\bar { 1 0 ^ { - 5 } } }$ , following [Fan et al., 2025]. The teacher network undergoes a warmup phase during the first 10K iterations to stabilize early training dynamics. Due to computational constraints, high-resolution finetuning is performed only for the Base model.\n\n# 5 Experimental Results\n\n# 5.1 Image Classification\n\nProbing self-attention We compare self-attention maps from the final layer’s [CLS] token of DINOv2, DINOv2R (with registers) [Darcet et al., 2024] and Franca in Figure 7. DINOv2 often fails to localize objects—especially under clutter or occlusion—while DINOv2R offers only minor improvements. In contrast, Franca yields sharply focused attention maps aligned with object boundaries, even for small or partially occluded instances. This suggests that our Matryoshka-style multi-head clustering promotes semantically rich features and finer-grained representations.\n\nLinear probing We begin by evaluating the global image representations learned by Franca via linear probing on ImageNet-1K [Russakovsky et al., 2015]. A linear classifier is trained on top of a frozen backbone and evaluated on the standard validation set, as well as on ImageNet-ReaL [Beyer et al., 2020] and ImageNet-v2 [Recht et al., 2019] to assess generalization (see Table 2). Despite being trained on only 13M images (IN-21K) and without distillation, Franca achieves performance comparable to DINOv2 and outperforms Web-SSL [Fan et al., 2025], which is trained on significantly a larger dataset. Notably, our Franca-G model matches the linear probing accuracy of Web-SSL-7B ( $8 5 . 9 \\%$ vs. $8 6 . 0 \\%$ ) while using $1 5 0 \\times$ less data and nearly $7 \\times$ fewer parameters.\n\nTo ensure a fair comparison, in Table 2 we report results for DINOv2 models (ViT-B/14 and ViTL/14) re-trained from scratch on IN-21K using the same training setup as Franca, but without distillation. These models perform significantly worse than the original DINOv2 (with distillation), showing that much of their strength comes from the use of distillation. In contrast, Franca achieves strong results without relying on any distillation or extra supervision.\n\n![](images/58f538f3f9696ffe533fc3ef31fa8bf767776ae5036a47d67257cd142df670ef.jpg)  \nFigure 7: Self-attention maps utilizing $1 4 \\times 1 4$ patches. These maps are visualized using the [CLS] token on the last layer’s heads on the validation set of ImageNet-1K [Russakovsky et al., 2015]. Franca has better localization than DINOv2 with registers [Darcet et al., 2024] without requiring the use of registers, where the nested Matryoshka clustering captures fine-grained details e.g. feathers, beaks of bird.\n\n![](images/69ecf3cee9aa50304a2462fc1f632325d7c869e95388e99c34d681c554ea5c16.jpg)  \nFigure 8: Out-of-Distribution Detection across five robustness-benchmarks: SSB-Hard [Vaze et al., 2022], NINCO [Bitterwolf et al., 2023], iNaturalist [Huang and Li, 2021], OpenImage-O [Wang et al., 2022a], and Texture [Kylberg, 2011]. Franca consistently outperforms DINOv2, at larger scales, demonstrating its robustness across distribution shifts. DINOv2-B and DINOv2-L are distilled from DINOv2-G and trained on LVD 142M, while neither variants of Franca are distilled. We report AuROC; higher is better.\n\nAmong text-supervised models, SigLIPv2 achieves state-of-the-art results in both classification and robustness metrics, aided by large-scale web supervision and strong pretraining data alignment. In comparison, our LAION-600M variant of Franca performs slightly worse, which we attribute to the domain gap between the pretraining distribution and the ImageNet evaluation benchmarks. Nonetheless, Franca remains competitive across ViT-B, ViT-L, and ViT-G backbones, providing a robust alternative to distillation-heavy or multimodal approaches with strong data efficiency.\n\nRobustness We evaluate the generalization ability of Franca under natural distribution shifts using linear probing on ImageNet-A [Hendrycks et al., 2021a], ImageNet-R [Hendrycks et al., 2021b], and Sketch [Wang et al., 2019], which retain the original label space but introduce semantic or stylistic variations. As shown in Table 2, Franca demonstrates strong robustness. For instance, FrancaG matches the performance of DINOv2-G across all three datasets and outperforms OpenCLIP-G by $1 1 . 5 \\%$ on ImageNet-A, despite OpenCLIP being trained on over $3 0 \\times$ more data.\n\nTable 2: Classification and Robustness. performance across vision-language and vision-only models. We report top-1 linear probing accuracy on IN-1K (val, ReaL, v2) and robustness benchmarks (IN-A, IN-R, Sketch). Franca, trained without text supervision, matches or exceeds the performance of larger text-supervised models and outperforms DINOv2 when reproduced on the same data and training strategy. †: reproduced on IN-21K without distillation; $\\ S$ : distilled from DINOv2- G on LVD-142M. Bold: Best; Underline: second best.   \n\n<html><body><table><tr><td></td><td colspan=\"7\">CLASSIFICATION</td><td colspan=\"3\">ROBUSTNESS</td></tr><tr><td>METHOD</td><td>ARCH.</td><td>DATA</td><td>TEXT</td><td>KNN</td><td>IN-VAL</td><td>REAL</td><td>V2</td><td>IN-A</td><td>IN-R</td><td>Sketch</td></tr><tr><td>IBoT</td><td>ViT-B/16</td><td>IN-21K</td><td>×</td><td>77.1</td><td>79.5</td><td>1</td><td>1</td><td>1</td><td>1</td><td>一</td></tr><tr><td>DINOv2†</td><td>ViT-B/16</td><td>IN-21K</td><td>×</td><td>77.6</td><td>81.2</td><td>84.5</td><td>70.3</td><td>52.2</td><td>59.9</td><td>48.0</td></tr><tr><td>DINOv2$</td><td>ViT-B/16</td><td>LVD-142M</td><td>×</td><td>82.1</td><td>84.5</td><td>88.3</td><td>75.1</td><td>55.1</td><td>63.3</td><td>50.6</td></tr><tr><td>Franca (ours)</td><td>ViT-B/14</td><td>IN-21K</td><td>×</td><td>80.9</td><td>82.0</td><td>85.6</td><td>72.8</td><td>54.8</td><td>63.2</td><td>48.7</td></tr><tr><td>SigLIP</td><td>ViT-L/16</td><td>WebLI</td><td>√</td><td>1</td><td>80.5</td><td>85.9</td><td>74.2</td><td>76.5</td><td>95.0</td><td>73.6</td></tr><tr><td>SigLIP2</td><td>ViT-L/16</td><td>WebLI</td><td>√</td><td>1</td><td>82.5</td><td>87.3</td><td>76.8</td><td>84.3</td><td>95.7</td><td>75.5</td></tr><tr><td>PEcore</td><td>ViT-L/16</td><td>MC-2B</td><td>√</td><td>83.5</td><td>83.9</td><td>1</td><td>77.9</td><td>89.0</td><td>95.2</td><td>73.4</td></tr><tr><td>Web-SSL</td><td>ViT-L/16</td><td>MC-2B</td><td>×</td><td>76.8</td><td>82.4</td><td>84.5</td><td>71.0</td><td>67.3</td><td>68.9</td><td>54.8</td></tr><tr><td>DINOv2†</td><td>ViT-L/14</td><td>IN-21K</td><td>×</td><td>82.1</td><td>84.0</td><td>86.8</td><td>73.9</td><td>70.5</td><td>70.9</td><td>55.2</td></tr><tr><td>DINOv2S</td><td>ViT-L/14</td><td>LVD-142M</td><td>×</td><td>83.5</td><td>86.3</td><td>89.5</td><td>78.0</td><td>71.3</td><td>74.4</td><td>59.3</td></tr><tr><td>Franca (ours)</td><td>ViT-L/14</td><td>IN-21K</td><td>×</td><td>82.6</td><td>84.2</td><td>88.3</td><td>77.4</td><td>71.0</td><td>72.8</td><td>58.4</td></tr><tr><td>Franca (ours)</td><td>ViT-L/14</td><td>LAION-600M</td><td>×</td><td>82.0</td><td>83.8</td><td>87.0</td><td>76.5</td><td>71.3</td><td>72.0</td><td>56.9</td></tr><tr><td>OpenCLIP</td><td>ViT-G/14</td><td>LAION-2B</td><td>√</td><td>83.2</td><td>86.2</td><td>89.4</td><td>77.2</td><td>63.8</td><td>87.8</td><td>66.4</td></tr><tr><td>DINOv2</td><td>ViT-G/14</td><td>LVD-142M</td><td>X</td><td>83.1</td><td>86.0</td><td>89.0</td><td>77.9</td><td>75.9</td><td>78.8</td><td>62.5</td></tr><tr><td>Web-SSL</td><td>ViT-G/14</td><td>MC-2B</td><td>×</td><td>79.2</td><td>84.7</td><td>86.8</td><td>74.3</td><td>73.3</td><td>75.9</td><td>60.9</td></tr><tr><td>Franca (ours)</td><td>ViT-G/14</td><td>IN-21K</td><td></td><td>83.1</td><td>85.9</td><td>88.7</td><td>77.8</td><td>75.3</td><td>78.6</td><td>62.2</td></tr><tr><td>Franca (ours)</td><td>ViT-G/14</td><td>LAION-600M</td><td></td><td>82.8</td><td>85.0</td><td>87.0</td><td>77.2</td><td>74.8</td><td>78.1</td><td>62.2</td></tr></table></body></html>\n\nWe further assess Franca on out-of-distribution (OOD) detection using the OpenOOD benchmark [Zhang et al., 2024], across five datasets: SSB-Hard [Vaze et al., 2022], NINCO [Bitterwolf et al., 2023], iNaturalist [Huang and Li, 2021], OpenImage-O [Wang et al., 2022a], and Texture [Kylberg, 2011]. We report Area under the ROC curve (AuROC) to measure the ability to distinguish in-distribution from OOD inputs. As shown in Figure 8, Franca consistently outperforms DINOv2 across large and giant model variants, showing both strong robustness to distribution shifts and effective scaling for OOD detection.\n\n<html><body><table><tr><td>Method</td><td>Arch</td><td>Food</td><td>C10</td><td>C100</td><td>SUN</td><td>Cars</td><td>Aircr</td><td>DTD</td><td>Pets</td><td>Cal</td><td>Flwrs</td><td>CUB</td><td>Avg</td></tr><tr><td>MAE</td><td>ViT-H/14</td><td>78.4</td><td>96.1</td><td>83.9</td><td>63.9</td><td>56.1</td><td>63.4</td><td>75.4</td><td>89.4</td><td>95.9</td><td>92.3</td><td>57.2</td><td>77.5</td></tr><tr><td>DINOv2t</td><td>ViT-L/14</td><td>93.4</td><td>99.2</td><td>93.9</td><td>78.1</td><td>89.9</td><td>81.7</td><td>82.9</td><td>95.2</td><td>87.2</td><td>99.6</td><td>90.3</td><td>90.1</td></tr><tr><td>DINOv2</td><td>ViT-L/14</td><td>94.3</td><td>99.3</td><td>93.4</td><td>78.7</td><td>89.9</td><td>81.5</td><td>84.0</td><td>96.5</td><td>97.5</td><td>99.7</td><td>90.5</td><td>91.4</td></tr><tr><td>Web-SSL</td><td>ViT-L/14</td><td>91.0</td><td>98.9</td><td>90.7</td><td>77.5</td><td>88.9</td><td>80.2</td><td>83.6</td><td>93.1</td><td>95.1</td><td>98.8</td><td>90.9</td><td>89.9</td></tr><tr><td>DINOv2</td><td>ViT-G/14</td><td>94.7</td><td>99.5</td><td>94.4</td><td>78.7</td><td>91.4</td><td>87.2</td><td>84.5</td><td>96.7</td><td>97.6</td><td>99.7</td><td>91.6</td><td>92.3</td></tr><tr><td>Web-SSL</td><td>ViT-G/14</td><td>94.1</td><td>99.4</td><td>93.1</td><td>78.0</td><td>90.3</td><td>83.7</td><td>84.7</td><td>92.4</td><td>96.8</td><td>99.4</td><td>91.2</td><td>91.2</td></tr><tr><td>OpenCLIP</td><td>ViT-G/14</td><td>94.5</td><td>98.7</td><td>91.0</td><td>84.0</td><td>96.1</td><td>80.2</td><td>86.0</td><td>95.7</td><td>98.1</td><td>99.5</td><td>89.9</td><td>92.2</td></tr><tr><td> Franca (ours)</td><td>ViT-B/14</td><td>91.6</td><td>99.0</td><td>92.0</td><td>77.0</td><td>88.7</td><td>79.3</td><td>82.2</td><td>93.8</td><td>96.2</td><td>99.7</td><td>88.3</td><td>89.8</td></tr><tr><td>Franca (ours)</td><td>ViT-L/14</td><td>94.3</td><td>99.0</td><td>93.7</td><td>78.9</td><td>89.5</td><td>81.3</td><td>83.8</td><td>96.8</td><td>97.4</td><td>99.5</td><td>90.9</td><td>91.4</td></tr><tr><td>Franca (ours)</td><td>ViT-G/14</td><td>95.0</td><td>99.5</td><td>95.1</td><td>78.9</td><td>91.3</td><td>85.5</td><td>85.0</td><td>97.2</td><td>97.5</td><td>99.7</td><td>91.3</td><td>92.3</td></tr></table></body></html>\n\nTable 3: Linear evaluation of frozen features on fine-grained datasets. top-1 accuracy measured across 11 benchmarks across objects, scenes, and textures, following [Chen et al., 2020a]; †: reproduced on IN-21K without distillation; $\\ S$ : distilled from DINOv2-G on LVD-142M.\n\nFine-grained classification We evaluate the transferability of the learned representations on 11 classification benchmarks introduced in SimCLR [Chen et al., 2020a]. These benchmarks cover a variety of tasks, including scene recognition, fine-grained object classification (such as food, cars, and aircraft), and texture recognition. Following [Oquab et al., 2024], we train a logistic regression classifier on features extracted from a frozen backbone. This approach focuses solely on the quality of the visual features and provides a fair way to compare performance across different tasks. Although some of these benchmarks tend to favor models trained with text supervision, our features perform strongly and competitively across many categories.\n\n![](images/6a92f77764b0b662353ce2f9e10d6d46b6646e7a72e8e3e1271b712354f9ebec.jpg)  \nFigure 9: Visualization of the first PCA components. We compute PCA across patches on DAVIS [Pont-Tuset et al., 2017] and illustrate the first three components using RGB color channels. Despite variations in pose, style, or even object identity, corresponding parts are consistently matched. Background regions are removed by thresholding the first PCA component. Images were selected randomly with np.random.randint(seed $_ { = 4 2 }$ ).\n\nAs shown in Table 3, our method, Franca, transfers well across a wide range of downstream tasks. Our ViT-G/14 model (Franca-G) achieves the same performance as DINOv2-G and outperforms Web-SSL-G by $1 . 1 \\%$ despite being trained on much less data. It also matches the performance of larger models like OpenCLIP-G. On datasets such as CIFAR-100 and Oxford Pets, Franca-G achieves $0 . 7 \\%$ and $0 . 5 \\%$ gains over DINOv2G respectively, demonstrating Franca’s strong generalization ability across both natural and fine-grained classification tasks.\n\nPCA of patch features We apply PCA to the patch-level feature embeddings of each image and retained only those patches whose projection onto the first principal component was positive (a simple thresholding that effectively isolates the foreground). We then compute second PCA on the remaining patches, visualizing the top three principal components as RGB channels. The resulting color maps reveal a stark contrast between models as shown in Figure 9 with DINOv2, the PCA highlights only a few scattered high-variance patches (outliers) and fails to cover the full object, yielding a fragmented, noisy segmentation, whereas Franca produces dense, coherent color segments aligned with the actual object. In Franca, we observe that object contours are sharply delineated and semantically similar parts (e.g., limbs or object components) receive consistent colors across instances, whereas these structures do not emerge in the DINOv2 maps. Importantly, neither model was trained on DAVIS [Pont-Tuset et al., 2017], so these segmentation patterns are entirely emergent from the self-supervised features.\n\n# 5.2 Dense tasks\n\nIn-Context Learning We evaluate Franca on the Hummingbird Benchmark [Balazevic et al., 2023, Pariza et al., 2024], which measures visual in-context learning by retrieving dense segmentation masks from a patch-level memory bank without fine-tuning. We report mean Intersection-overUnion (mIoU) on Pascal VOC [Everingham et al.] and ADE20K [Zhou et al., 2017] in Table 4.\n\nFranca consistently achieves strong segmentation results, with performance improving systematically with model scale. On ViT-B/14, Franca outperforms SigLIP and WebSSL by up to $+ 5 \\%$ mIoU on both datasets and performs within $2 - 3 \\%$ of DINOv2-B (distilled from DINOv2G and trained on LVD-142M dataset). On ViT-G/14 scale, Franca surpasses DINOv2-G and WebSSL by up to $+ 2 \\%$ mIoU on Pascal VOC, while maintaining competitive performance on ADE20K. We hightlight that DINOv2 is pretrained on both VOC and ADE20K images while Franca is not. The gains become more pronounced as model capacity increases, highlighting the ability of Franca to learn spatially precise and semantically meaningful features that transfer well without fine-tuning.\n\nTable 4: Linear and In-context Segmentation Performance. Comparison of linear segmentation and in-context segmentation (1/1 split). Linear segmentation uses a linear head on frozen spatial features, evaluated via mIoU on COCO-Stuff, Pascal VOC, and ADE20K. In-context segmentation reports mIoU using cluster-based retrieval matched to ground truth via Hungarian matching. †: reproduced on IN-21K, without distillation; $\\ S$ : distilled from DINOv2-G on LVD-142M.   \n\n<html><body><table><tr><td rowspan=\"3\">METHOD</td><td rowspan=\"3\">BACKBONE</td><td colspan=\"3\">LIN. SEG.</td><td colspan=\"2\">IN-CONTEXT</td><td colspan=\"4\">OVERCLUSTERING</td></tr><tr><td rowspan=\"2\">COCO-STUFF</td><td rowspan=\"2\">VOC</td><td rowspan=\"2\">ADE20K|VOC</td><td rowspan=\"2\"></td><td rowspan=\"2\">ADE20K</td><td colspan=\"2\">VOC</td><td colspan=\"2\">COCO-THINGS</td></tr><tr><td>K=100</td><td>K=300</td><td>K=100</td><td>K=300</td></tr><tr><td>SigLIP</td><td>ViT-B/16</td><td>36.2</td><td>44.6</td><td>15.9</td><td>33.9</td><td>10.6</td><td>29.5</td><td>36.9</td><td>41.5</td><td>53.4</td></tr><tr><td>iBOT</td><td>ViT-B/16</td><td>55.9</td><td>73.1</td><td>30.1</td><td>66.6</td><td>26.9</td><td>21.2</td><td>29.1</td><td>18.3</td><td>26.3</td></tr><tr><td>EVA-CLIP</td><td>ViT-B/16</td><td>48.0</td><td>70.4</td><td>34.6</td><td>34.8</td><td>11.3</td><td>43.3</td><td>49.1</td><td>41.3</td><td>52.0</td></tr><tr><td>DINOv2†</td><td>ViT-B/14</td><td>59.6</td><td>77.2</td><td>33.8</td><td>69.6</td><td>30.0</td><td>25.9</td><td>34.7</td><td>20.5</td><td>28.7</td></tr><tr><td>DINOv2$</td><td>ViT-B/14</td><td>58.9</td><td>80.3</td><td>42.6</td><td>77.1</td><td>37.7</td><td>39.2</td><td>52.5</td><td>46.5</td><td>54.0</td></tr><tr><td>Franca (ours)</td><td>ViT-B/14</td><td>60.5</td><td>80.0</td><td>39.1</td><td>75.7</td><td>34.7</td><td>30.1</td><td>46.9</td><td>37.9</td><td>43.1</td></tr><tr><td>SigLIPv2</td><td>ViT-L/16</td><td>60.3</td><td>51.5</td><td>45.4</td><td>46.4</td><td>21.3</td><td>24.3</td><td>40.4</td><td>43.5</td><td>50.7</td></tr><tr><td>Web-SSL</td><td>ViT-L/14</td><td>60.8</td><td>79.0</td><td>40.3</td><td>71.3</td><td>35.3</td><td>28.2</td><td>37.7</td><td>26.3</td><td>33.1</td></tr><tr><td>DINOv2†</td><td>ViT-L/14</td><td>60.3</td><td>79.3</td><td>37.8</td><td>72.0</td><td>33.5</td><td>25.9</td><td>34.7</td><td>24.1</td><td>35.1</td></tr><tr><td>DINOv2</td><td>ViT-L/14</td><td>58.0</td><td>80.3</td><td>41.8</td><td>74.6</td><td>38.6</td><td>26.5</td><td>43.0</td><td>34.8</td><td>45.7</td></tr><tr><td>Franca (ours)</td><td>ViT-L/14</td><td>60.5</td><td>81.3</td><td>41.4</td><td>73.5</td><td>37.6</td><td>30.3</td><td>44.4</td><td>33.8</td><td>40.7</td></tr><tr><td>Web-SSL</td><td>ViT-G/14</td><td>57.1</td><td>80.2</td><td>39.4</td><td>73.3</td><td>36.7</td><td>26.0</td><td>33.4</td><td>15.5</td><td>21.5</td></tr><tr><td>DINOv2</td><td>ViT-G/14</td><td>58.1</td><td>81.3</td><td>42.4</td><td>73.7</td><td>37.7</td><td>19.5</td><td>27.7</td><td>20.7</td><td>29.2</td></tr><tr><td>Franca (ours)</td><td>ViT-G/14</td><td>60.4</td><td>81.3</td><td>42.4</td><td>76.7</td><td>36.5</td><td>39.4</td><td>49.2</td><td>25.8</td><td>31.9</td></tr></table></body></html>\n\nLinear segmentation We further evaluate Franca in a supervised semantic segmentation setting by training a linear classifier on top of a frozen pretrained backbone in Table 4. During training, the output features are upsampled to the input resolution using bilinear interpolation, and a pixelwise cross-entropy loss is applied. This setup avoids fine-tuning and instead assesses the linear separability of the learned representations. We conduct experiments on COCO-Stuff [Caesar et al., 2018], Pascal VOC [Everingham et al.], and ADE20K [Zhou et al., 2017], reporting segmentation performance in terms of mean Intersection-over-Union (mIoU). To ensure robustness, we report the best performance across five learning rates (0.005, 0.01, 0.02, 0.05, 0.1).\n\nAcross all three backbone scales, Franca consistently matches or outperforms prior methods. On the ViT-B/14 backbone, it achieves the highest mIoU on COCO-Stuff (60.5), while performing competitively on Pascal VOC (80.0) and ADE20K (39.1), on par with DINOv2-B which is distilled from DINOv2-G and pretrained on LVD142M. Notably, a non-distilled reproduction of DINOv2-B results substantially lower performance (e.g., 77.2 vs. 80.0 on VOC; 33.8 vs. 39.1 on ADE20K), indicating that its competitive results rely heavily on distillation from a larger model and quality of it’s pretraining data (LVD142M). With a ViT-L/14 backbone, Franca maintains strong performance, outperforming DINOv2-L (pretrained on LVD142M) on COCO-Stuff and Pascal VOC, and remaining competitive on ADE20K. In this setting, Web-SSL [Fan et al., 2025] surpasses both Franca and $\\mathrm { D } \\mathrm { \\check { I } N O v } \\dot { 2 } – \\mathrm { L } ^ { \\mathrm { \\xi } }$ only once. Interestingly, despite its extended training and strong image-text alignment, SigLIPv2 [Tschannen et al., 2025] performs poorly on segmentation, particularly on VOC (51.5 mIoU), suggesting limited spatial localization capabilities.\n\nFinally, on the ViT-G/14 backbone, Franca achieves the highest performance across all benchmarks, outperforming all other models. These results suggest that the strong performance of DINOv2-L on smaller backbones is largely due to distillation from the larger ViT-G model. Moreover, while WebSSL benefits from pretraining on a substantially larger dataset (2B images), Franca still outperforms it on several benchmarks, demonstrating the effectiveness of our approach.\n\nOverclustering We evaluate Franca in an unsupervised setting using the overclustering protocol from [Ziegler and Asano, 2022], which assesses semantic alignment of spatial features by clustering patch embeddings with $K$ -Means and matching them to ground-truth masks via Hungarian matching [Kuhn, 1955]. Performance is measured by mean Intersection-over-Union (mIoU), averaged over five seeds. This task emphasizes fine-grained structure in a label-free setting.\n\n![](images/9c5006c2d5c47a856b0f88a11aa10bc18d83af5472385fd21cd1955e48c67eec.jpg)  \nFigure 10: Unsupervised clustering. We compare self-supervised clustering results of Franca with DINOv2 and DINOv2-R. Each method generates pseudo-segmentations from self-attention maps without labels or fine-tuning. Franca yields sharper boundaries and more semantically coherent regions, especially on fine-grained objects such as birds and bicycles.   \nTable 5: Unsupervised object discovery using TokenCut [Wang et al., 2022b] and LOST [Siméoni et al., 2021], across different visual backbones. Franca consistently outperforms baselines, achieving the highest localization accuracy. regTr: trained registers [Darcet et al., 2024]; regTe: test-time registers [Jiang et al., 2025].\n\n<html><body><table><tr><td colspan=\"2\">METHOD BACKBONE</td><td colspan=\"2\">VOC-07 VOC-12</td></tr><tr><td rowspan=\"3\"></td><td>SigLIPv2</td><td>7.8</td><td>9.7</td></tr><tr><td>DINOv2 OpenCLIP</td><td>12.2 30.8</td><td>14.1 35.9</td></tr><tr><td>Franca (ours)</td><td>36.8</td><td>42.1</td></tr><tr><td rowspan=\"3\">LSOT</td><td>DINOv2</td><td>30.8</td><td>35.9</td></tr><tr><td>DINOv2 + regTr</td><td>55.0</td><td>59.3</td></tr><tr><td>Franca (ours) DINOv2 + regTe</td><td>53.8 56.8</td><td>57.9 60.9</td></tr></table></body></html>\n\nOverclustering is particularly relevant in dense self-supervised learning, where the learned features often serve as a foundation for downstream tasks such as semantic segmentation and object detection. Increasing the number of clusters can promote more fine-grained and localized representations, allowing the model to capture detailed object parts and subtle region boundaries—critical for dense prediction tasks.\n\nAs shown in Table 4 and Figure 10, Franca shows strong and consistent performance across backbones and cluster sizes, with the largest gains at ViT-G/14 scale. On ViT-B/14, it achieves top performance on VOC with $K = 3 0 0$ (46.9 mIoU) and competitive results on COCO-Things $( 3 7 . 9 / 4 3 . 1 \\$ with $K = 1 0 0 / K = 3 0 0 )$ , outperforming WebSSL and EVA-CLIP, but slightly trailing DINOv2-B (trained on LVD-142M). With ViT-L/14, Franca surpasses DINOv2-L (trained on LVD-142M) on VOC (30.3 / 44.4 vs. 26.5 / 43.0), and beats WebSSL on COCO-Things, though SigLIPv2 remains the strongest on that dataset.\n\nAt ViT-G/14 scale, Franca achieves the best results: $4 9 . 2 ~ \\mathrm { m I o U }$ on VOC $K = 3 0 0$ ) and 31.9 on COCO-Things ( $K = 1 0 0$ ), outperforming all unimodal and several multimodal baselines, including DINOv2-G (trained on LVD-142M). These results underscore Franca ’s ability to scale spatial representations effectively for unsupervised dense prediction.\n\n# 5.3 Probing 3D awareness\n\nProbing 3D understanding We evaluate the geometric understanding of Franca on two tasks: keypoint correspondence and monocular depth estimation. For the former we use SPair-71k [Min et al., 2019], where the goal is to predict dense keypoint correspondences under varying viewpoint changes. For depth estimation, we follow the AdaBins protocol [Bhat et al., 2021] and evaluate single-image depth predictions on NYUv2 [Silberman et al., 2012] using the scale-invariant rootmean-square error (SI-RMSE). Unlike keypoint recall, which can reflect dataset bias toward semantically distinctive or consistently placed features (e.g., eyes, beaks), we evaluate correspondence accuracy across all keypoints and report performance under increasing viewpoint disparity. For fair comparison, we consider two evaluation settings: (i) low-resolution: using image size of $2 2 4 \\times 2 2 4$ for SPair-71k and NYUv2, to mimic the resolution used during pretraining by most methods; $( i i )$\n\nTable 6: Probing 3D understanding via keypoint matching on SPair-71K and monocular depth estimation on NYUv2. We report correspondence accuracy (higher is better) under increasing viewpoint changes $( d { = } 0 , 1 , 2$ , and overall) on SPair-71K, and scale-invariant RMSE (lower is better) for depth estimation on NYUv2. Results are shown for both low $( 2 2 4 ^ { 2 } )$ and high $( 8 0 0 ^ { 2 } )$ resolution SPair-71K input. $^ \\dagger$ : reproduced on IN-21K without distillation; §: distilled from DINOv2-G.   \n\n<html><body><table><tr><td>MODEL</td><td>ARCH.</td><td>DATA</td><td>d=0</td><td>d=1</td><td>d=2</td><td>All</td><td>SI-RMSE↓</td></tr><tr><td colspan=\"8\">IMAGE RESOLUTION 224 × 224(SPAIR-71K)AND 224 × 224(NYUV2)</td></tr><tr><td>SigLIPv2</td><td>ViT-B/16</td><td>WebLI</td><td>8.00</td><td>5.87</td><td>6.14</td><td>7.22</td><td>0.52</td></tr><tr><td>Open-CLIP</td><td>ViT-B/14</td><td>LAION-2B</td><td>17.54</td><td>16.07</td><td>16.89</td><td>16.76</td><td>0.36</td></tr><tr><td>DINOv2†</td><td>ViT-B/14</td><td>IN-21K</td><td>32.96</td><td>25.37</td><td>25.81</td><td>29.32</td><td>0.34</td></tr><tr><td>DINOv2S</td><td>ViT-B/14</td><td>LVD-142M IN-21K</td><td>37.97</td><td>30.27</td><td>28.98 26.59</td><td>33.59 31.31</td><td>0.28 0.33</td></tr><tr><td>Franca (ours)</td><td>ViT-B/14</td><td></td><td>35.41</td><td>26.48</td><td></td><td></td><td></td></tr><tr><td>SigLIPv2</td><td>ViT-L/16</td><td>WebLI</td><td>8.58</td><td>5.97</td><td>6.14</td><td>7.22</td><td>0.52</td></tr><tr><td>Open-CLIP</td><td>ViT-L/14</td><td>LAION-2B</td><td>19.10</td><td>16.75</td><td>17.72</td><td>17.60</td><td>0.35</td></tr><tr><td>DINOv2†</td><td>ViT-L/14</td><td>IN-21K</td><td>38.98</td><td>30.31</td><td>31.11</td><td>34.61</td><td>0.29</td></tr><tr><td>DINOv2$</td><td>ViT-L/14</td><td>LVD-142M</td><td>36.86</td><td>30.33</td><td>30.35</td><td>32.81</td><td>0.25</td></tr><tr><td>Franca (ours) Franca (ours)</td><td>ViT-L/14</td><td>IN-21K</td><td>39.10</td><td>30.64</td><td>31.09</td><td>35.13</td><td>0.28</td></tr><tr><td></td><td>ViT-L/14</td><td>LAION-600M</td><td>39.45</td><td>30.65</td><td>31.89</td><td>35.45</td><td>0.28</td></tr><tr><td>SigLIPv2 Open-CLIP</td><td>ViT-G/16 ViT-L/14</td><td>WebLI LAION-2B</td><td>13.53 27.20</td><td>10.25 22.97</td><td>10.79 25.21</td><td>11.55</td><td>0.43</td></tr><tr><td>DINOv2</td><td>ViT-G/14</td><td>LVD-142M</td><td>38.09</td><td>31.09</td><td>31.97</td><td>25.01 33.99</td><td>0.31 0.24</td></tr><tr><td>Franca (ours)</td><td>ViT-G/14</td><td>IN-21K</td><td>36.41</td><td>28.99</td><td>28.51</td><td>33.17</td><td>0.26</td></tr><tr><td>Franca (ours)</td><td>ViT-G/14</td><td>LAION-600M</td><td>39.54</td><td>31.55</td><td>32.83</td><td>36.04</td><td>0.28</td></tr><tr><td></td><td></td><td>IMAGE RESOLUTION 800 × 800(SPAIR-71K)AND 480 × 480(NYUV2)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DINOv2†</td><td>ViT-B/14</td><td>IN-21K</td><td>42.82</td><td>33.38</td><td>34.82</td><td>38.71</td><td>0.33</td></tr><tr><td>DINOv2$</td><td>ViT-B/14</td><td>LVD-142M</td><td>63.04</td><td>51.09</td><td>48.76</td><td>55.98</td><td>0.25</td></tr><tr><td>Franca (ours)</td><td>ViT-B/14</td><td>IN-21K</td><td>53.46</td><td>40.12</td><td>38.92</td><td>46.67</td><td>0.31</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DINOv2†</td><td>ViT-L/14</td><td>IN-21K</td><td>57.60</td><td>42.91</td><td>44.17</td><td>50.68</td><td>0.31</td></tr><tr><td>DINOv2$</td><td>ViT-L/14</td><td>LVD-142M</td><td>63.91</td><td>53.11</td><td>51.91</td><td>56.92</td><td>0.23</td></tr><tr><td>Franca (ours)</td><td>ViT-L/14</td><td>IN-21K</td><td>58.13</td><td>43.29</td><td>43.29</td><td>50.61</td><td>0.27</td></tr><tr><td>Franca (ours)</td><td>ViT-L/14</td><td>LAION-600M</td><td>55.38</td><td>42.14</td><td>43.40</td><td>48.87</td><td>0.28</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DINOv23</td><td>ViT-G/14</td><td>LVD-142M</td><td>63.06</td><td>52.01</td><td>52.72</td><td>56.37</td><td>0.22</td></tr><tr><td>Franca (ours)</td><td>ViT-G/14</td><td>IN-21K</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>51.84</td><td>38.08</td><td>36.36</td><td>44.08</td><td>0.25</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Franca (ours)</td><td>ViT-G/14</td><td>LAION-600M</td><td>56.66</td><td>45.87</td><td>46.52</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>51.23</td><td>0.28</td></tr></table></body></html>\n\nhigh-resolution: using image size of $8 0 0 \\times 8 0 0$ for SPair-71k and $4 8 0 \\times 4 8 0$ for NYUv2, that suits better methods with high-resolution fine-tuning, e.g., DINOv2.\n\nAs shown in Table 6, Franca achieves strong performance across both tasks, outperforming DINOv2 (reproduced on IN-21K) and exceeding Open-CLIP and SigLIPv2 baselines of similar scale. On SPair-71k, using an input resolution of $2 2 4 \\times 2 2 4$ , Franca outperforms DINOv2 distilled from DINOv2-G and pretrained on LVD142M by $3 \\%$ on ViT-L and $1 . 5 \\%$ on ViT-G. Notably, Franca maintains high correspondence fidelity even for large viewpoint changes $\\scriptstyle ( d = 2 )$ , suggesting robustness to geometric deformations.\n\nOn NYUv2, Franca achieves comparable performance as DINOv2 distilled from larger DINOv2 model. This is also due to the fact that DINOv2 uses NYU Depth v2 during pretraining (as part of LVD 142M). These results indicate that Franca learns a spatial representation that captures both fine-grained 2D alignment and coarse 3D structure, generalizing well from object-centric pretraining to downstream geometric tasks.\n\nProbing with Gaussian Splatting We probe the texture and geometry awareness of our models on the Feat2GS [Chen et al., 2025] framework. This framework uses novel view synthesis, which serves as an effective proxy for 3D evaluation of visual foundation models (VFM). It uses VFMs to extract dense features that are then transformed into 3D Gaussians using a lightweight readout layer trained with photometric loss. For fair comparison, the input images are resized to 512, the output feature maps are upsampled to 512, and PCA is used to unify the feature dimensionality to 256 channels. We use three metrics for evaluation: PSNR, SSIM, and LPIPS. The three evaluation setups (\"Geometry\", \"Texture\", and \"All\") refer to different ways of using the VFM features to predict the parameters of the 3D Gaussians. (a) Geometry: VFM features predict the geometric parameters of the 3D\n\n![](images/2badad23d98b8fc498a3e9ba7ed83679ad8ea1d3181129e7a44480ab866192e9.jpg)  \nFigure 11: Probing with Gaussian Splatting, Normalized average metrics using Feat2GS [Chen et al., 2025] across six datasets for two probing schemes: geometry (G), and all (A), i.e., Geometry $^ +$ Texture with ViT-L backbone. We measure PSNR, SSIM (higher is better) and LPIPS (lower is better) showing that Franca achieves significantly better performance than state-of-the-art vision encoders suggesting strong geometrical awareness.\n\nGaussians (position, opacity, and covariance), while the textural parameters (spherical harmonic coefficients) are freely optimized. (b) Texture: VFM features predict the textural parameters of the 3D Gaussians, and the geometric parameters are directly optimized. (c) All: here, the VFM features are used to predict all parameters of the 3D Gaussians, including both geometry and texture. We use the Large model variants and compare our Franca DINOv2 [Oquab et al., 2024], WebSSL [Fan et al., 2025], AIMv2 [Fini et al., 2025], EVA-CLIP [Sun et al., 2023], SigLIPv2 [Tschannen et al., 2025], and CLIP [Radford et al., 2021]. The results reported in Figure 11 are averaged over 5 runs using six datasets. They show that Franca performs the best regarding the Geometrical awareness and also when evaluated in the complete setup (All), suggesting strong geometrical awareness. On Texture, all methods have similar performance with SigLIPv2 having an edge over them.\n\n# 6 Conclusion\n\nIn this work, we present Franca (the ‘free’ one), a new Vision Foundation Model that is openweight, open-data and open-code. We build this model using a novel Matryoshka-nested clustering self-supervised loss that allows for learning hierarchical representations and have introduced RASA, a simple post-pretraining method to remove overtly spatial biases in the final representations. Across evaluations in image recognition, segmentation, robustness, OOD detection and 3D understanding, we find that it matches and frequently outperforms DINOv2 and other state-of-the-art models such as SigLIPv2.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   当前高性能视觉基础模型（如DINOv2、SigLIPv2）严重依赖专有数据集和封闭训练流程，阻碍了研究的可复现性和公平比较。\\n> *   现有自监督聚类方法（如DINOv2的131K码本）采用固定粒度聚类，无法处理语义模糊性（如车辆可按颜色/型号等多维度聚类），且密集特征常与位置信息耦合。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出Franca——首个完全开源（数据/代码/权重）的视觉基础模型，通过嵌套Matryoshka聚类实现多粒度语义表示，并设计RASA后训练策略解耦位置偏置。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **开源生态：** 使用公开数据集（ImageNet-21K+LAION-600M）训练，在11个下游任务上超越DINOv2，如Pascal VOC分割mIoU达`81.3`（+1.0%）。\\n> *   **Matryoshka聚类：** 参数共享的层次化投影头使特征压缩至`d/64`维度时仍保持82.6% ImageNet-v2准确率（较DINOv2高4%）。\\n> *   **RASA解耦：** 通过9次迭代正交化去除位置信息，在SPair-71k关键点匹配任务中准确率提升至`58.13`（+0.53%）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   将ViT输出的$d$维特征切片为$[d,d/2,...,d/16]$，每个子空间独立聚类并共享编码器，形成从粗到细的语义层次。\\n> *   位置解耦通过线性投影识别特征中的空间分量，迭代正交化保留纯语义信息（见图6）。\\n\\n> **创新点 (Innovations)**\\n> *   **对比DINOv2：** 其单粒度聚类（131K码本）无法适应多义性；Franca的5级Matryoshka头仅增加<1%参数量。\\n> *   **关键技术突破：** \\n>     1.  CyclicMask循环移位可见区域（图4d），消除中心位置偏置；\\n>     2.  动态调整聚类数$c_i$与特征维度$d_i$成正比；\\n>     3.  RASA的Gram-Schmidt正交化实现可解释的位置剥离。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **特征切片：** 对ViT输出$Z_s\\\\in\\\\mathbb{R}^{(n+1)\\\\times d}$按$\\\\mathcal{M}=\\\\{m_1,...,m_k\\\\}$切片得$Z_s^{(i)}=Z_s[:,1:m_i]$。\\n> 2.  **多级损失：** 每级投影头$h_\\\\theta^{(i)}$输出经Sinkhorn-Knopp聚类，总损失$\\\\mathcal{L}_{total}=\\\\sum_{i=1}^k \\\\mathcal{L}^{(i)}$。\\n> 3.  **RASA迭代：** \\n>     -  学习位置预测头$\\\\hat{y}_s=\\\\sigma(WZ_s)$，MSE损失$\\\\mathcal{L}_{pos}$；\\n>     -  正交化得$u_r,u_c$，计算投影分量$p_s$并移除（公式2）。\\n\\n> **案例解析 (Case Study)**\\n> *   **图5对比：** DINOv2的聚类熵值低（位置敏感），Franca通过RASA使空间熵提升2-3倍；\\n> *   **图9可视化：** PCA显示Franca特征能连贯分割物体部件（如鸟喙/羽毛），而DINOv2特征碎片化。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   DINOv2系列（ViT-B/L/G）、SigLIPv2、Web-SSL、OpenCLIP、EVA-CLIP\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在ImageNet分类上：** Franca-G线性探测达`85.9%`，匹配Web-SSL-7B（`86.0%`）但仅用1/150数据；在ImageNet-A鲁棒性测试中超越OpenCLIP-G `11.5%`。\\n> *   **在密集预测上：** Franca-G在COCO-Stuff分割mIoU达`60.4`，优于DINOv2-G (`58.1`)；Pascal VOC上`81.3` mIoU刷新纪录。\\n> *   **在3D理解上：** SPair-71k关键点匹配中，Franca-L高分辨率输入准确率`58.13`，显著高于DINOv2-L (`57.60`)；NYUv2深度估计SI-RMSE `0.27`，接近DINOv2-L (`0.25`)。\\n> *   **内存效率：** 特征压缩至`d/16`时，Franca的k-NN分类准确率仍保持82.0%，较DINOv2同压缩比高`4%`（图3）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   自监督学习 (Self-Supervised Learning, SSL)\\n*   视觉基础模型 (Vision Foundation Model, VFM)\\n*   嵌套表示 (Nested Representations, N/A)\\n*   位置解耦 (Positional Disentanglement, RASA)\\n*   开源模型 (Open-Source Model, N/A)\\n*   层次聚类 (Hierarchical Clustering, N/A)\\n*   密集预测 (Dense Prediction, N/A)\\n*   三维理解 (3D Understanding, N/A)\"\n}\n```"
}