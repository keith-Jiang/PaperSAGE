{
    "source": "ArXiv (Semantic Scholaræœªæ”¶å½•)",
    "arxiv_id": "2507.14056",
    "link": "https://arxiv.org/abs/2507.14056",
    "pdf_link": "https://arxiv.org/pdf/2507.14056.pdf",
    "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
    "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "æœªæ‰¾åˆ°æäº¤æ—¥æœŸ",
    "venue": "æš‚æœªå½•å…¥Semantic Scholar",
    "fields_of_study": "æš‚æœªå½•å…¥Semantic Scholar",
    "citation_count": "æš‚æœªå½•å…¥Semantic Scholar",
    "influential_citation_count": "æš‚æœªå½•å…¥Semantic Scholar",
    "institutions": [
        "Neural Circuits Laboratory, Biosciences Institute Newcastle University",
        "Newcastle University",
        "Department of Informatics University of Sussex",
        "University of Sussex"
    ],
    "paper_content": "# Noradrenergic-inspired gain modulation attenuates the stability gap in joint training\n\nAlejandro Rodriguez-Garciaâˆ— Neural Circuits Laboratory, Biosciences Institute Newcastle University, UK a.rodriguez-garcia2@newcastle.ac.uk\n\nAnindya Ghosh Department of Informatics University of Sussex, UK anindya.ghosh@sussex.ac.uk\n\nSrikanth Ramaswamy Neural Circuits Laboratory, Biosciences Institute Newcastle University, UK srikanth.ramaswamy@newcastle.ac.uk\n\n# Abstract\n\nRecent studies in continual learning have identified a transient drop in performance on mastered tasks when assimilating new ones, known as the stability gap. Such dynamics contradict the objectives of continual learning, revealing a lack of robustness in mitigating forgetting, and notably, persisting even under an ideal joint-loss regime. Examining the stability gap within this idealized joint training context is critical to isolate it from other sources of forgetting and fully elucidate its underlying mechanisms. We argue that this gap reflects an imbalance between rapid adaptation and robust retention at task boundaries, underscoring the need to investigate mechanisms that reconcile plasticity and stability within continual learning frameworks. Biological brains navigate a similar plasticityâ€“stability dilemma by operating concurrently on multiple timescales, leveraging neuromodulatory signals to modulate synaptic plasticity. However, artificial networks lack native multitimescale dynamics, and although optimizers like momentum-SGD and Adam introduce implicit timescale regularization, they still exhibit stability gaps. Inspired by locus coeruleus-mediated noradrenergic bursts, which transiently enhance neuronal gain under uncertainty to facilitate sensory assimilation, we propose uncertainty-modulated gain dynamics â€“ an adaptive mechanism that approximates a two-timescale optimizer and dynamically balances integration of knowledge with minimal interference on previously consolidated information. We evaluate our mechanism on domain-incremental and class-incremental variants of the MNIST and CIFAR benchmarks under joint training, demonstrating that uncertainty-modulated gain dynamics effectively attenuate the stability gap. Finally, our analysis elucidates how gain modulation replicates noradrenergic functions in cortical circuits, offering mechanistic insights into reducing stability gaps, and that gain-modulated neural networks can enhance performance in continual learning tasks.\n\n# 1 Introduction\n\nBiological neural circuits demonstrate lifelong adaptability, continually integrating new information with minimal disruption to established memories [18], [38]. This capability rests on a finely tuned balance between plasticity and stability, arising from intrinsic multi-timescale dynamics in cortical networks [17], [36], [38]. Gain modulation is a central mechanism supporting this flexibility, whereby a neuronâ€™s responsiveness is scaled without altering its selectivity [5], [18], [33]. While various biological pathways can modulate gain, we focus on its amplification during perceptual integration, which critically depends on locus coeruleusâ€“mediated noradrenergic phasic bursts triggered by environmental uncertainty [5], [17], [33], [39]. Neural networks mimicking this process observe flattened energy landscapes, making network states more accessible and enhanced flexibility during perceptual integration [17], [20], [29], [39].\n\nDuring the presentation of novel and unexpected events, transient gain boosts have been observed [5], [9], [39]. We hypothesize that these transient gain boosts on neural circuits also induce fast and slow timescales of weight adaptations that facilitate sensory integration with minimal interference. In this way, gain-induced weight dynamics mirror the fast-and-slow paradigm, i.e., when learning a new task, fast weights temporarily enhance performance on previous tasks without disturbing the the consolidation of new memories being stored in the slow weights [2] â€“ but achieve even greater efficiency thanks to the flexibility conferred by a flattened energy landscape. Crucial to our point, while gain modulation in biology operates via changes in spiking behavior [29], [31], it can be applied in standard multilayer perceptrons (MLPs) by multiplying each point neuronâ€™s incoming weights with a gain coefficient.\n\nContinual learning (CL) in artificial neural networks aspires to emulate this lifelong adaptability â€“ networks acquire incoming data continuously, without \"catastrophically\" overwriting the knowledge of past examples [23], [24], [38]. To date, most work in this field has concentrated on mitigating this phenomenon by approximating the ideal joint loss over all previous tasks encountered [23], [28]. However, recent research has pointed out the presence of a stability gap â€“ a transient forgetting of past task knowledge at every task switch and shown that it persists even under ideal joint-training conditions [19], [21]. This brief drop in accuracy impairs the essence of CL, which seeks seamless knowledge accumulation, and raises two questions: (i) does similar transient forgetting occur in neural systems, and (ii) if not, do CLâ€™s learning dynamics diverge from those of biological brains? As a result, efforts are emerging focusing not only on what to optimize, but also on how to optimize [28]. Our research is based on this new approach in CL, where we propose an uncertainty-modulated gain mechanism as an approximated two-timescale optimizer that outperforms two well-established gradient-based optimization techniques â€“ momentum-SGD (MSGD) [4] and Adam [13] optimizers in attenuating stability gaps in the ideal joint-training scenario. The main contributions of our work are:\n\nâ€¢ We introduce uncertainty-guided gain dynamics to attenuate stability gaps in CL, and analytically show that transient gain boosts induce fast and slow timescales of weight adaptation, effectively approximating classic fast-and-slow weight schemes through implicit multi-timescale regularization.   \nâ€¢ We empirically validate our approach on both domain-incremental and class-incremental MNIST (Split MINST, Rotated MNIST, Permuted MNIST) and CIFAR (Split CIFAR 10, Domain CIFAR-100) benchmarks under ideal joint training, demonstrating a substantial reduction of the stability gap compared to the MSGD and Adam optimizers.   \nâ€¢ We link our algorithmic gain bursts to noradrenergic neuromodulation, providing a biologically inspired perspective on how multi-timescale dynamics can mitigate transient forgetting while promoting knowledge acquisition.\n\n# 2 Related work\n\nMulti-timescale optimizers. Standard multi-layer perceptron architectures lack explicit multiscale dynamics, which makes them hard to optimize effectively with vanilla SGD. To overcome this, MSGD introduced a two-timescale update that maintains an exponentially weighted average of past gradients alongside the current gradient [1], [4]. Adaptive optimizers such as Adam build on this principle by combining first-moment (momentum) and second-moment (variance) estimates to adjust per-parameter learning rates on the fly, coupling rapid responsiveness with step-size normalization for faster, more stable convergence [13]. Interestingly, recent work demonstrated that MSGD could be reparameterized into â€œfastâ€ and â€œslowâ€ weight components, where the fast channel adapts quickly but decays, and the slow channel integrates information more stably [22]. Hence, these modern optimizers echo the original fastâ€“slow weight idea of Hinton and Plaut [2], who paired a fast, elastic weight that decays toward zero with a slow, plastic weight, to temporarily â€œdeblurâ€ old associations (the fast weights take on values that temporarily cancel out the changes in the slow weights caused by the subsequent learning) and thus minimize interference from new learning [2].\n\nStability gaps in CL. It was not until recently that a study pointed out the conventional evaluation paradigm for AI systems is fundamentally inadequate [21], [28]. Consider a sequence of tasks $\\mathrm { \\bar { \\{ } }  T _ { k } \\} _ { k = 1 } ^ { K }$ with corresponding data distributions $\\{ \\bar { D _ { k } } \\} _ { k = 1 } ^ { K }$ and let $\\mathcal { F } _ { \\theta }$ denote a model parameterized by $\\theta$ . Under the traditional task-based evaluation, $\\mathcal { F } _ { \\theta }$ is trained sequentially, updating its parameters $\\theta _ { k }$ upon completing task $T _ { k }$ , and its performance is measured only immediately after training on $T _ { k }$ has finished [21]. Notably, this task-based evaluation is blind to how a modelâ€™s accuracy evolves during training. As an alternative, the continual evaluation strategy measures the performance of $\\mathcal { F } _ { \\theta }$ every $\\rho$ training iterations while learning task $T _ { k }$ [21]. Surprisingly, continual evaluation revealed stability gaps [19], [21], which have been encountered even in LLMs [32]. These brief accuracy drops undermine the goals of CL by introducing temporary forgetting between tasks and limiting the systemâ€™s overall learning capacity. Specifically, this might be especially problematic in safety-critical applications, where even momentary failures can have severe consequences [21], [28]. One may naively attribute the stability gap to imperfect approximations of the joint objective by CL methods. Yet this hypothesis does not hold up as the phenomenon persists even under perfect joint training, where $L _ { \\mathrm { j o i n t } } = L _ { \\mathrm { n e w } } + L _ { \\mathrm { o l d } }$ [28], [34]. Hence, this finding implies that the transient performance collapse is not merely an artifact of memory or regularization approximations, but rather emerges from the fundamental dynamics of sequential optimization, underscoring the importance of studying it in the idealized joint-loss setting [34].\n\nBeyond evaluation paradigms and identification of the stability gap, a growing body of work has sought to pinpoint its causes and propose remedies. Hess et al. [28] argue that there exists a nonincreasing-loss path between $\\theta _ { \\mathrm { o l d } }$ and $\\theta _ { \\mathrm { j o i n t } }$ â€“ suggesting that a continual learner only needs to follow this valley. However, enforcing this trajectory via gradient-projection methods has yielded limited success [28]. In a homogeneous-CIFAR setup, Kamath et al. [34] further demonstrates that although such a linear, low-loss path does exist, standard SGD consistently deviates from it at each task transition. Other approaches argue that the stability gap may originate from excessive network plasticity or large losses in the output layer [27]. Successful approaches include temporal ensembling to narrow the gap by reducing prediction variance at task transitions [30] and replacing the standard linear classifier with a nearest-mean classifier head [35]. Interestingly, in this last method, Åapacz et al. [35] demonstrates that much of the transient forgetting arises in the final linear classifier. However, to the best of our knowledge, no prior work has adopted a bio-inspired approach to mitigate the stability gap or connected it back to adaptive biological learning.\n\nNeuronal gain modulation. Neuronal gain characterizes the input sensitivity of a neuron and is formally defined as the slope of its input-output transfer function, reflecting the rate at which output firing rates change in response to variations in input current [5], [18], [33]. This property has been extensively investigated in biophysical models of cortical circuits, highlighting the role of gain modulation in adaptive computation [6], [10], [14], [29], [37], and fine-tuning motor responses [16]. Furthermore, theoretical neuroscience works have debated the causal relationship between gain modulation and synaptic plasticity, suggesting a complex interplay [7]. However, despite its biological relevance, gain modulation has received limited attention in artificial learning systems. A recent study has used gain modulation for attentional gating in hierarchical models but only as a fine-tuning step after weight learning, ignoring its interaction with weight updates [40]. Another study used it as an isolated mechanism for adaptive whitening [25] and extended it to multi-timescale whitening by pairing fast gain changes with slower synaptic plasticity [26]; however, their gain modulation was limited to interneurons in a single-layer architecture [25], [26]. Nevertheless, a key theoretical insight is that gain neuromodulation reshapes the energy landscape: noradrenergic input flattens it to enable flexible switching, while cholinergic input deepens it to stabilize memories [17], [20], [29]. Recent recurrent neural network (RNN) work supports this view, showing that uncertainty-driven gain modulation facilitates perceptual switches by propelling the system through high-velocity trajectories [39]. However, in that model, gains were fixed during training and modulated only afterward, limiting interaction with weight updates. We extend this approach by including gain dynamics during learning and applying it to reduce stability gaps in joint-CL.\n\n# 3 Gain boost as a multi-timescale optimizer\n\nIntroducing processing timescales into artificial neural networks provides a principled strategy to reconcile stable, long-term memory consolidation with rapid, context-driven adaptation. The fast-slow weight paradigm exemplifies this principle by maintaining distinct \"slow\" and \"fast\" weight components, i.e. $W _ { \\mathrm { e f f } } = w _ { \\mathrm { s l o w } } + w _ { \\mathrm { f a s t } }$ [2]. This enables networks to swiftly adapt to distribution shifts through transient updates in the fast weights, which rapidly decay toward zero, while simultaneously preserving consolidated knowledge within slow weights [22]. Crucially, this structure minimizes interference between newly acquired information and existing representations [2], making it particularly suitable as a foundation for lifelong learning in artificial systems.\n\nIn biological neural circuits, analogous multi-timescale processing occurs through neuromodulatory signals that adjust neuronal gain, thereby altering neuronal responsiveness to incoming stimuli [5], [17], [18]. For instance, during perceptual updating, the noradrenergic system originating from the locus coeruleus transiently boosts neuronal gain to facilitate sensory integration of new incoming stimuli [5], [33], [39]. We propose that this neuronal gain modulation naturally gives rise to a fast-slow learning structure by shaping the effective synaptic weights. Consider a model $y ( t ) = h \\left( x ( t ) ; g ( t ) , w ( t ) \\right)$ and a loss function $L \\left( y ( t ) , T ( t ) \\right)$ , where $x ( t )$ represents the input at time $t$ , $\\{ g ( t ) , w ( t ) \\}$ denotes the parameter estimate, $y ( t )$ is the output produced by the model, and $T ( t )$ is the target. In artificial neural networks (ANNs), we can interpret $w _ { i j } ( t )$ as the synaptic weights connecting neuron $j$ with neuron $i$ and $g _ { i } ( t )$ as the neuronal gain; hence, one can consider an effective weight\n\n$$\nW _ { i j } ( t ) = g _ { i } ( t ) w _ { i j } ( t )\n$$\n\nAssuming that neuronal gain is shaped by noradrenergic activity, we can distinguish two functional modes of operation: (i) phasic â€“ rapid, high-amplitude bursts of release and (ii) tonic â€“ background level of neuromodulatory release. Sensory prediction errors trigger phasic noradrenaline release, which transiently increases neuronal gain [33]. However, in the absence of prediction errors, neuronal gain tends toward its baseline $g _ { 0 }$ , reflecting tonic activity. Therefore, without losing generalizability, we can decompose the effective weight as contributions from these components, i.e.\n\n$$\nW _ { i j } ( t ) = g _ { i } ( t ) w _ { i j } ( t ) = g _ { 0 } w ( t ) + [ g _ { i } ( t ) - g _ { 0 } ] w _ { i j } ( t ) ,\n$$\n\nThis decomposition highlights how neuronal gain modulation inherently realizes a two-timescale optimization scheme. The slow component, $w _ { i j } ^ { \\mathrm { s l o w } } ( t ) = g _ { 0 } w ( t )$ , represents the stable synaptic changes associated with tonic neuromodulation. Conversely, the fast component, $w _ { i j } ^ { \\mathrm { f a s t } } ( t ) = [ g _ { i } ( t ) - g _ { 0 } ] _ { . } w _ { i j } ( t )$ dynamically emerges in response to phasic neuromodulatory signals, effectively isolating rapid contextual adaptations from stable memory traces. In other words, transient gain modulation \"virtually\" decouples the effective synaptic weight updates into slow and fast components, offering an adaptive mechanism to flexibly balance plasticity and stability (Figure 1A).\n\nTo illustrate this concept further, following Jones [22], we consider a simplified linear model defined by $y ( t ) = W _ { \\mathrm { e f f } } ( t ) x$ with a constant input $x \\equiv 1$ to predict a target signal $T ( t )$ under the mean square error loss $\\begin{array} { r } { L = \\frac { 1 } { 2 } \\left[ T ( t ) - y ( t ) \\right] ^ { 2 } } \\end{array}$ . Parameter updates follow standard stochastic gradient descent, while gain dynamics evolve via exponential decay modulated by transient increases in response to contextual surprises. Formally, we can write\n\n$$\n\\begin{array} { r c l } { { } } & { { } } & { { w ( t + 1 ) = w ( t ) - \\alpha \\partial _ { w } L , } } \\\\ { { } } & { { } } & { { g ( t + 1 ) = \\gamma g ( t ) + ( 1 - \\gamma ) g _ { 0 } + \\eta H ( y ) , } } \\end{array}\n$$\n\nwhere $\\alpha$ is the learning rate, $\\gamma < 1$ controls how quickly the gain decays back to its tonic baseline $g _ { 0 }$ , and $\\eta$ determines the strength of the phasic bump, driven by the \"surprise\" signal $H ( y )$ which in this case is simplified as the absolute value of the loss gradient, $\\mid \\partial _ { g } L \\mid$ . Note that these constraints on the gain update mimic noradrenergic release by the locus coeruleus ([39]; see the Appendix for details).\n\nGiven that the gradient with respect to synaptic weights can be expressed as:\n\n$$\n\\frac { \\partial L } { \\partial w } = g ( t ) \\frac { \\partial L } { \\partial W } ,\n$$\n\nneuronal gain acts explicitly as a plasticity modulator, dynamically scaling the magnitude of learning updates. During phasic increases in neuronal gain, this modulation amplifies synaptic updates, allowing rapid adaptation to novel contexts. Conversely, during tonic baseline activity, it ensures stable, incremental learning, thereby minimizing interference with previously consolidated memories. Thus, gain modulation naturally approximates a fastâ€“slow optimization scheme (see the comparison in Figure 1B), effectively mitigating catastrophic forgetting by dynamically balancing rapid adaptation and long-term stability [2].\n\n![](images/0ce638b9c9c08e6845ceb3ea00e36e8667fdfb4dedba7463181dddc49499fa53.jpg)  \nFigure 1: Simple proof of principle of gain boost approximating to two-timescale optimizer. $A$ ) Schematic of the gain effective weight uncoupling under a gain boost through noradrenergic neuromodulation. Phasic noradrenergic signals transiently increase neuronal gain henceforth decoupling its weights in a fast-slow scheme where: $w _ { \\mathrm { s l o w } } = ( g ( t ) - g _ { 0 } ) w ( t )$ and ${ w _ { \\mathrm { s l o w } } } = g _ { 0 } w ( t ) . B$ ) We consider the simplified model $y ( t ) = W _ { \\mathrm { e f f } } ( t ) x .$ ã€ with a constant input $x$ to predict a target signal $T ( t )$ under the mean square error loss $\\begin{array} { r } { L = \\frac { 1 } { 2 } \\left[ T ( t ) - y ( t ) \\right] ^ { 2 } } \\end{array}$ Comparison between the gain modulated (blue) and fast-slow weight (orange) methods. The slashed line reflects the basal model with only the slow component.\n\nInterestingly, classical MSGD can be reinterpreted within this framework as implicitly implementing a fastâ€“slow weight separation, where a \"fast\" component smooths high-frequency noise through an exponentially decaying memory of past gradients [22]. Similarly, Adam optimizer could be viewed through a multi-timescale lens, where its adaptive step-size adjustments reflect a fast-timescale adaptation based on local gradient statistics [13]. However, these conventional optimizers achieve multitimescale dynamics indirectly through adaptive learning rates, whereas gain modulation emerges directly from biologically inspired neuronal properties. Moreover, biologically realistic gain modulation actively reshapes the networkâ€™s energy landscape by transiently flattening it, thereby facilitating smoother transitions between stable attractors corresponding to distinct learned states [20], [29], [39], hence being a promising venue to balance stability and plasticity in both biological and artificial neural circuits.\n\n# 4 Joint training experiments\n\nWe isolate the stability gap from other sources of forgetting by simplifying into a joint training scenario between tasks, as we are focusing on how to optimize and not what to optimize [28], [34]. Our hypothesis is rooted in gain modulationâ€™s ability to both segregate synaptic plasticity into fast and slow adaptation channels and flatten the networkâ€™s energy landscape. This dual action should narrow stability gaps between successive tasks and lower test-set loss at contextual changes, together minimizing interference. To test this, we consider a scenario in which a learner $F _ { \\theta }$ is trained on a sequence of $K$ contexts $\\mathcal { C } = \\{ c _ { k } \\} _ { k = 1 } ^ { K }$ . Each context $c _ { k }$ is a disjoint dataset $\\{ ( \\mathbf { x } _ { j } , \\tilde { y } _ { j } ) \\} _ { j = 1 } ^ { N } \\sim \\mathcal { D } _ { k }$ Contexts are presented one after another in an online fashion, with each context trained for a fixed number of iterations $I _ { \\mathcal { C } }$ 2. Since this is a joint-training setup, the model retains access to all data from previous contexts throughout learning.\n\nNoradrenergic uncertainty. Biological and theoretical evidence converge on the necessity of explicitly representing a learnerâ€™s uncertainties when interacting with stochastic, non-stationary environments to achieve optimal inference [8], [9], [33]. Noradrenaline has been particularly linked to â€œunexpected uncertainty,â€ where its phasic signaling is triggered by sudden increases in environmental variability due to contextual changes [8], [9], [33], [38]. This response injects additional variability into neural and behavioral processes, facilitating rapid adaptation to shifting contexts [17], [20], [39]. Evidence in this regard from Wainstein et al. [39], suggests that this form of uncertainty can be quantified as the entropy of the networkâ€™s readout, i.e.\n\n<html><body><table><tr><td>scale Î· >0</td><td>Require: Dataset sequence C = {ck}k=1, where each Ck ={(xj,9j)}=1~ Dk; context iterations</td><td>Ic; batch size B; learning rate Î± > O; gain-decay O < Î³ < 1; gain-baseline go â‰¥ 1; entropy</td><td></td><td></td></tr><tr><td></td><td>1: Initialize: Wâ†Winit;gâ† 9init</td><td></td><td></td><td></td></tr><tr><td></td><td>2: for each context ci âˆˆC do</td><td></td><td></td><td></td></tr><tr><td>3:</td><td>for iteration i= 1 to Ic do</td><td></td><td></td><td></td></tr><tr><td>4:</td><td>(X,Y)~DB</td><td></td><td></td><td>// mini-batch</td></tr><tr><td>5:</td><td>Ï€ â† softmax(Fw(X; g))</td><td></td><td></td><td>// forward pass</td></tr><tr><td>6:</td><td>Câ†âˆ‘-1ï¼ˆÏ€,ï¼‰</td><td></td><td>// cross-entropy loss</td><td></td></tr><tr><td>7:</td><td>Wâ†W-Î±VwL</td><td></td><td></td><td>// SGD update</td></tr><tr><td>8:</td><td>Hâ†-bâˆ‘1âˆ‘Ï€j,logÏ€j</td><td></td><td>// uncertainty (entropy)</td><td></td></tr><tr><td>9:</td><td></td><td></td><td></td><td>// gain update</td></tr><tr><td></td><td>gâ†Î³g+(1-Î³)g+nH end for</td><td></td><td></td><td></td></tr><tr><td>10: 11: end for</td><td></td><td></td><td></td><td></td></tr></table></body></html>\n\n$$\nH ( y ) = \\sum _ { i } \\pi _ { i } ( y ) \\log \\left( \\pi _ { i } ( y ) \\right)\n$$\n\nwhere $\\pi ( y )$ denotes the softmax probability distribution of the networkâ€™s output. We incorporate this into equation (4) to dynamically modulate neuronal gain, which acts as a forcing function that transiently boosts gain in response to novel or ambiguous stimuli [5], [17], [33]. Consistent with our hypothesis, this will lead to faster knowledge integration with minimal integration. As discussed above, this modulation approximates a two-timescale optimization mechanism (see Equations (3) and (4)). To further formalize this process, we present the pseudo-algorithm in Algorithm 1 and benchmark its performance against standard optimizers such as MSGD and Adam to evaluate whether it offers improved adaptability and reduced interference.\n\n# 4.1 Methods\n\nDatasets. For our class-incremental learning experiments, we used two standard benchmarks: MNIST [3] and CIFAR-10 [11]. We define Split MNIST and Split CIFAR-10 by dividing each dataset into 5 sequential tasks, each containing a disjoint subset of the 10 target classes [28]. For domainincremental learning, we considered MNIST [3] and CIFAR-100 [11]. We defined Rotated MNIST with three tasks, each using the full MNIST dataset rotated by a fixed angle: $0 ^ { \\circ }$ , $8 0 ^ { \\circ }$ , $1 6 0 ^ { \\circ }$ . These angles were chosen to maximize stability gaps while avoiding digit ambiguities, such as confusing 6 and 9 when rotated $1 8 0 ^ { \\circ }$ [21], [28]. We define Permuted MNIST by applying fixed random pixel permutations to each image in the full MNIST dataset [12]. We define Domain CIFAR-100 by organizing tasks according to the coarse semantic categories in CIFAR-100 [28]. Specifically, we constructed three tasks using the superclasses aquatic mammals, fish, and flowers, progressively expanding the visual diversity while keeping the label space consistent.\n\nSetup. We employed continual evaluation with $\\rho _ { \\mathrm { e v a l } } = 1$ and test on all the testing data of the first task [21]. Batch sizes were chosen as 128 for experiments on the MNIST dataset and 256 for experiments on the CIFAR-10 and CIFAR-100 datasets. Per experiment, the number of iterations per context $( I _ { \\mathit { C } } )$ was selected as Permuted MNIST (200), Rotated MNIST (400), Split MNIST (200), Split CIFAR-10 (400), and Domain CIFAR-100 (400). Each experiment was repeated five times with different random seeds. For each metric, we report the mean and the standard error across runs.\n\nArchitectures. For tasks based on the MNIST dataset, we used a feedforward neural network (FFNN) with two hidden layers of 400 units each [21]. All neurons in the hidden and output layers were gain-modulated, where gain represents the slope of the neuronâ€™s input-output response curve. We employed the ReLU activation function and removed all biases. For CIFAR-10 and CIFAR-100, the increased visual complexity motivated the use of convolutional layers, so we adopted a Slim ResNet-18 backbone [15], [21]. Convolutional networks mirror the brainâ€™s hierarchy of feature detectors, but unlike biological neurons where each has its own input-output curve, CNN feature maps use a shared filter, so uniform scaling cannot match sensitivity at the neuron-level and may disrupt useful feature learning. Thus, we restricted gain modulation to the output layer 3, where it could regulate input sensitivity without disrupting feature selectivity, consistent with biological neurons [18]. This choice is also supported by findings that link the output layer to the stability gap phenomenon [35].\n\nTable 1: Main quantitative metrics. For all benchmarks we report (across tasks) the average final accuracy (avg-ACC), average minimum accuracy (avg-min-ACC), average stability gap drop (avgSG), and the worst-case accuracy (WC-ACC). We see that our\\* noradrenergic gain-modulated SGD (NGM-SGD, Equation (1)) exhibits the least stability gaps when encountering a new task and also increases learning efficiency by improving the final learning outcome. Highlighted values indicate the best results; when multiple values are highlighted, it is because their standard error ranges overlap.   \n\n<html><body><table><tr><td colspan=\"3\">avg-ACC (â†‘)</td><td>avg-min-ACC(â†‘)</td><td>WC-ACC (â†‘)</td><td>avg-SG (â†“)</td></tr><tr><td rowspan=\"3\">Permuted MNIST</td><td>MSGD</td><td>92.485 Â± 0.169</td><td>50.390 Â± 1.304</td><td>64.818Â± 0.874</td><td>0.004 Â± 0.008</td></tr><tr><td>ADAM</td><td>93.148 Â± 0.268</td><td>51.233 Â± 2.452</td><td>65.102 Â± 1.646</td><td>0.038 Â± 0.012</td></tr><tr><td>NGM-SGD*</td><td>94.186 Â± 0.103</td><td>51.697 Â± 1.574</td><td>66.022 Â± 1.052</td><td>0.011 Â± 0.007</td></tr><tr><td rowspan=\"3\">Rotated MNIST</td><td>MSGD</td><td>93.600 Â± 0.170</td><td>48.017 Â± 1.030</td><td>63.269 Â± 0.691</td><td>0.089Â± 0.010</td></tr><tr><td>ADAM</td><td>92.138 Â± 0.430</td><td>48.477 Â± 2.563</td><td>62.569 Â± 1.732</td><td>0.154 Â± 0.033</td></tr><tr><td>NGM-SGD*</td><td>94.948 Â± 0.121</td><td>51.006 Â± 1.672</td><td>65.536 Â±1.119</td><td>0.054 Â± 0.015</td></tr><tr><td rowspan=\"3\">Split MNIST</td><td>MSGD</td><td>99.085 Â± 0.074</td><td>58.083 Â± 3.774</td><td>66.148 Â± 3.020</td><td>0.270Â± 0.007</td></tr><tr><td>ADAM</td><td>98.602 Â± 0.229</td><td>73.255 Â± 9.223</td><td>78.082 Â± 7.380</td><td>0.155 Â± 0.094</td></tr><tr><td>NGM-SGD*</td><td>99.018 Â± 0.055</td><td>83.421 Â± 2.840</td><td>86.374 Â± 2.272</td><td>0.019 Â± 0.005</td></tr><tr><td rowspan=\"3\">Split CIFAR-10</td><td>MSGD</td><td>87.479 Â±1.754</td><td>56.611 Â± 6.399</td><td>60.697Â± 5.182</td><td>0.343Â±0.087</td></tr><tr><td>ADAM</td><td>86.996 Â± 1.659</td><td>47.415 Â± 5.827</td><td>54.317 Â± 4.792</td><td>0.495 Â±0.100</td></tr><tr><td>NGM-SGD*</td><td>90.096 Â±1.087</td><td>63.875 Â± 3.660</td><td>67.650 Â± 3.017</td><td>0.157 Â± 0.041</td></tr><tr><td rowspan=\"3\">Domain CIFAR-100</td><td>MSGD</td><td>48.600 Â±1.993</td><td>6.470Â± 3.092</td><td>19.980 Â± 2.363</td><td>0.579Â± 0.095</td></tr><tr><td>ADAM</td><td>44.713 Â± 0.858</td><td>10.000 Â± 0.000</td><td>21.520 Â± 0.199</td><td>0.872 Â± 0.056</td></tr><tr><td>NGM-SGD*</td><td>47.682 Â± 1.527</td><td>12.400 Â± 1.375</td><td>23.462 Â±1.381</td><td>0.466 Â± 0.061</td></tr></table></body></html>\n\nOptimization. MSGD [1] optimization used SGD with 0.9 momentum, Adam [13] optimization used betas 0.9 and 0.99 for all experiments. NGM-SGD optimization followed Equation (1) with a decay rate $( \\gamma )$ of 0.9 and scale factors $( \\eta )$ of 0.3, 0.5, 0.4, 0.2, 0.2 for the experiments on Permuted MNIST, Rotated MNIST, Split MNIST, Split CIFAR-10, and Domain CIFAR-100; respectively. In all experiments and for all optimizers, a learning rate of 0.01 was used. To ensure a fair comparison between optimizers, we train the models in the same manner across all experiments, varying only the optimizer.\n\n# 4.2 Results\n\nIn Table 1, we summarize the main experimental results across benchmarks using standard continual evaluation metrics established early in the field ([21], see Appendix). The final average accuracy (avg-ACC) reflects the modelâ€™s overall performance by averaging classification accuracy across all tasks at the end of training [28]. The average minimum accuracy (avg-min-ACC) captures the lowest accuracy reached per task, averaged across tasks [28], and serves as a measure of transient forgetting. The worst-case accuracy (WC-ACC) incorporates the minimum accuracy on prior tasks and the accuracy on the current task, offering a measure of the stability-plasticity trade-off [21], [35]. Lastly, the stability gap (avg-SG) represents the maximum observed drop in accuracy throughout training [27], [35], highlighting how much the model degrades when acquiring new tasks.\n\nNGM-SDG reduces stability gaps. Table 1 shows NGM-SDG consistently reduces the stability gap and generally improves performance, as hypothesized in previous studies [28]. For class-incremental tasks, MSGD and Adam show strong transient forgetting, likely due to increasing task complexity as new classes resemble previously learned ones. NGM-SGD mitigates this by using uncertaintydriven gain to reduce perceived complexity. In Split MNIST, this effect fades as fewer novel classes are introduced in later tasks. In Split CIFAR-10, forgetting is minimal during tasks 2â€“4, which involve animal classes distinct from the initial vehicle task 4. However, the final task reintroduces vehicle classes, thus causing interference and a renewed stability gap, which is again attenuated by NGM-SGD. Results in domain-incremental tasks are mixed. NGM-SGD shows clear gains in both performance and stability on Rotated MNIST, and mainly in the first domain of Domain CIFAR-100. In Permuted MNIST, improvements are limited to performance, with a stability gap at the first task switch likely due to a sharp shift in input correlations. Notably, CIFAR results are informative since gain modulation was applied only to the classification head of Resnet18. This supports the findings of Åapacz et al. [35], who identified the output layer as the main source of stability gaps.\n\n![](images/11bd8f58977d28b119837294250388cd1c564a9422d6bb2a5be01089642f06ff.jpg)  \nFigure 2: Stability gaps under class-incremental learning. The left column shows results on Split CIFAR-10, and the right column on Split MNIST. The top panels display the test accuracy on the first task as the model is incrementally trained on all benchmark tasks. The middle panels show the corresponding test loss, and the bottom panels depict the evolution of neuronal gain across training iterations. Curves represent the mean $\\pm$ standard error (shaded area) over five runs with different random seeds. Color coding denotes the different optimizers: our method (NGM-SGD, Equation (1)) in orange, momentum-SGD (MSGD) in blue, and Adam in green. Plots have been zoomed in to better highlight the stability gaps. As a result, some performance drops may appear truncated. Full, unzoomed versions of the plots are provided in the Appendix.\n\nGain-boosts reduce test loss at task boundaries. Aiming to understand the origin of the reduction of the stability gaps by the NGM-SGD method, we plotted the first-task loss for each experiment. Figures 2 and 3 show that NGM-SDG systematically reduces the test loss at task transitions compared to ADAM and MSGD. We hypothesize that this behavior comes from the effect of gain increase on flattening the energy landscape [20], [39]. Notably, the learning trajectory followed by NGM-SDG may not correspond to the path of non-increasing loss that recent studies have posited to follow [28], [34], but flattening the energy landscape (here the loss landscape) favors both integration of new information during training of new taks (see Appendix), while minimizes interference.\n\nNeuronal gain encodes task complexity. In NGM-SGD, neuronal gain evolves according to Equations (4) and (6), rising in response to task-related uncertainty and decaying as learning progresses [39]. As shown in the bottom row of Figures 2 and 3, the asymptotic level to which gain decays differs between class-incremental and domain-incremental settings, reflecting distinct patterns of task complexity (see Appendix). In class-incremental scenarios, gain rises at the beginning of each new task and then decays to a progressively higher baseline. This indicates that the system perceives each new task as increasingly complex, requiring higher sustained sensitivity. In contrast, in domainincremental tasks, gain also spikes at the onset of each task, but consistently decays to a similar level across tasks. This suggests that most of the complexity is already captured in the first task, and the subsequent tasks do not introduce additional uncertainty. Thus, the long-term gain level serves as a proxy for accumulated task complexity, echoing how biological systems adjust their internal gain to reflect changes in cognitive demand [5], [18].\n\n![](images/0bd22590de9da74d63d4687c0b6a6826cff16d37698d60c4f6c897af39da31b1.jpg)  \nFigure 3: Stability gaps under domain-incremental learning. The left column shows results on Permuted MNIST, the middle column on Rotated MNIST with $8 0 ^ { \\circ }$ rotations, and the right column on Domain CIFAR-100. The top panels display the test accuracy on the first task as the model is incrementally trained on all benchmark tasks. The middle panels show the corresponding test loss, and the bottom panels depict the evolution of neuronal gain across training iterations. Curves represent the mean $\\pm$ standard error (shaded area) over five runs with different random seeds. Color coding denotes the different optimizers: our method (NGM-SGD, Equation (1)) in orange, momentum-SGD (MSGD) in blue, and Adam in green. Plots have been zoomed in to better highlight the stability gaps. As a result, some performance drops may appear truncated. Full, unzoomed versions of the plots are provided in the Appendix.\n\n# 5 Discussion\n\nA central contribution of our work is the shift in perspective from what to optimize in CL, to how to optimize it [28]. In this context, our work proposes gain modulation as a biologically inspired mechanism to reframe the optimization process. Rather than relying on fixed learning trajectories in a static loss landscape, we introduce uncertainty-driven gain boosts that dynamically reshape the learning dynamics, enabling transient adaptation and long-term retention with minimal interference. This approach draws directly from neurobiological insights, particularly the role of noradrenergic phasic bursts in enhancing cortical responsiveness during moments of uncertainty [5], [17], [33].\n\nTwo key effects of gain modulation help explain the observed reduction in stability gaps. First, our analytical and empirical results show that transient gain boosts induce fast and slow timescales of weight adaptation, approximating the fast-and-slow paradigm without explicitly partitioning parameters [2]. This dynamic supports rapid task integration while preserving prior knowledge. Second, consistent with biophysical models [20], [39], we observe that gain increases at task switches flatten the loss landscape, which correlates with reduced test loss and interference, providing a flexible optimization path that mitigates transient forgetting.\n\nTaken together, these results challenge the conventional assumption that optimal learning in continual scenarios must follow a path of monotonically decreasing loss. Instead, our findings suggest that biological systems may rely on mechanisms that dynamically alter the geometry of the loss landscape to accommodate new learning demands. In this view, transient increases in gain, triggered by contextual uncertainty, temporarily reshape the energy surface, creating a corridor for new task representations to be integrated with minimal interference, without disturbing the broader memory structure.\n\nLimitations and future work. This study is conducted in a simplified setting using MNIST and CIFAR benchmarks, and evaluates performance over a relatively small number of tasks. Moreover, the joint training scenario represents an idealized form of CL that does not reflect the complexities of real-world environments. Future work could explore the role of cholinergic neuromodulation in task segregation by using a gain gating mechanism to transiently silence specific neurons and prevent their updates, thereby introducing gain modulation as a regularization against catastrophic forgetting and, together with gain boosts, guiding gradient updates with minimal interference for more robust continual learning.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³äº†æŒç»­å­¦ä¹ ï¼ˆContinual Learning, CLï¼‰ä¸­çš„ç¨³å®šæ€§é—´éš™ï¼ˆstability gapï¼‰é—®é¢˜ï¼Œå³åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œå·²æŒæ¡ä»»åŠ¡çš„æ€§èƒ½ä¼šçŸ­æš‚ä¸‹é™ã€‚è¿™ç§ç°è±¡å³ä½¿åœ¨ç†æƒ³çš„è”åˆè®­ç»ƒï¼ˆjoint trainingï¼‰æ¡ä»¶ä¸‹ä»ç„¶å­˜åœ¨ï¼Œæ­ç¤ºäº†å½“å‰ä¼˜åŒ–æ–¹æ³•åœ¨å¹³è¡¡å¯å¡‘æ€§ä¸ç¨³å®šæ€§æ–¹é¢çš„ä¸è¶³ã€‚\\n> *   è¯¥é—®é¢˜çš„é‡è¦æ€§åœ¨äºå®ƒé™åˆ¶äº†æŒç»­å­¦ä¹ ç³»ç»Ÿåœ¨å®‰å…¨å…³é”®åº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ï¼‰ä¸­çš„å¯é æ€§ï¼Œå› ä¸ºå³ä½¿æ˜¯çŸ­æš‚çš„æ€§èƒ½ä¸‹é™ä¹Ÿå¯èƒ½å¯¼è‡´ä¸¥é‡åæœã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿä¸­è“æ–‘æ ¸ï¼ˆlocus coeruleusï¼‰ä»‹å¯¼çš„å»ç”²è‚¾ä¸Šè…ºç´ ï¼ˆnoradrenergicï¼‰å¢ç›Šè°ƒåˆ¶çš„å¯å‘ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ä¸ç¡®å®šæ€§è°ƒåˆ¶çš„å¢ç›ŠåŠ¨æ€ï¼ˆuncertainty-modulated gain dynamicsï¼‰ï¼Œé€šè¿‡è¿‘ä¼¼åŒæ—¶é—´å°ºåº¦ä¼˜åŒ–å™¨æ¥åŠ¨æ€å¹³è¡¡æ–°çŸ¥è¯†çš„æ•´åˆä¸å·²æœ‰çŸ¥è¯†çš„ä¿ç•™ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **å¼•å…¥ä¸ç¡®å®šæ€§å¼•å¯¼çš„å¢ç›ŠåŠ¨æ€**ï¼šé€šè¿‡åˆ†æè¡¨æ˜ï¼Œç¬æ€å¢ç›Šæå‡è¯±å¯¼äº†æƒé‡çš„å¿«æ…¢æ—¶é—´å°ºåº¦é€‚åº”ï¼Œæœ‰æ•ˆè¿‘ä¼¼äº†ç»å…¸çš„å¿«æ…¢æƒé‡æ–¹æ¡ˆã€‚\\n> *   **å®è¯éªŒè¯**ï¼šåœ¨MNISTå’ŒCIFARåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸åŠ¨é‡SGDï¼ˆMSGDï¼‰å’ŒAdamä¼˜åŒ–å™¨ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ç¨³å®šæ€§é—´éš™ï¼ˆä¾‹å¦‚åœ¨Split MNISTä¸Šï¼Œç¨³å®šæ€§é—´éš™ä»0.270é™è‡³0.019ï¼‰ã€‚\\n> *   **ç”Ÿç‰©å¯å‘è§†è§’**ï¼šå°†ç®—æ³•å¢ç›Šçˆ†å‘ä¸å»ç”²è‚¾ä¸Šè…ºç´ ç¥ç»è°ƒåˆ¶è”ç³»èµ·æ¥ï¼Œä¸ºå¤šæ—¶é—´å°ºåº¦åŠ¨æ€å¦‚ä½•ç¼“è§£ç¬æ€é—å¿˜æä¾›äº†ç”Ÿç‰©å­¦ä¾æ®ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   æ ¸å¿ƒåŸç†æ˜¯é€šè¿‡å¢ç›Šè°ƒåˆ¶ï¼ˆgain modulationï¼‰åŠ¨æ€è°ƒæ•´ç¥ç»å…ƒçš„å“åº”æ€§ï¼Œä»è€Œåœ¨ä¸æ”¹å˜å…¶é€‰æ‹©æ€§çš„æƒ…å†µä¸‹ï¼Œå®ç°å¿«é€Ÿé€‚åº”ä¸ç¨³å®šä¿ç•™çš„å¹³è¡¡ã€‚å¢ç›Šè°ƒåˆ¶æ¨¡æ‹Ÿäº†ç”Ÿç‰©ç¥ç»ç³»ç»Ÿä¸­å»ç”²è‚¾ä¸Šè…ºç´ åœ¨ä¸ç¡®å®šæ€§æ¡ä»¶ä¸‹çš„ç¬æ€é‡Šæ”¾ï¼Œé€šè¿‡é‡å¡‘ç½‘ç»œçš„èƒ½é‡æ™¯è§‚ï¼ˆenergy landscapeï¼‰æ¥ä¿ƒè¿›æ–°ä»»åŠ¡çš„æ•´åˆã€‚\\n> *   è®¾è®¡å“²å­¦æ˜¯å€Ÿé‰´ç”Ÿç‰©ç³»ç»Ÿçš„å¤šæ—¶é—´å°ºåº¦åŠ¨æ€ï¼Œé€šè¿‡å¢ç›Šè°ƒåˆ¶éšå¼å®ç°å¿«æ…¢æƒé‡åˆ†ç¦»ï¼Œä»è€Œé¿å…æ˜¾å¼å‚æ•°åˆ†åŒºçš„å¤æ‚æ€§ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”**ï¼šä¼ ç»Ÿä¼˜åŒ–å™¨ï¼ˆå¦‚MSGDå’ŒAdamï¼‰é€šè¿‡è‡ªé€‚åº”å­¦ä¹ ç‡é—´æ¥å®ç°å¤šæ—¶é—´å°ºåº¦åŠ¨æ€ï¼Œä½†ä»è¡¨ç°å‡ºç¨³å®šæ€§é—´éš™ã€‚æœ¬æ–‡æ–¹æ³•ç›´æ¥é€šè¿‡å¢ç›Šè°ƒåˆ¶å®ç°å¿«æ…¢æƒé‡åˆ†ç¦»ï¼Œæ›´æ¥è¿‘ç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„æœºåˆ¶ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›**ï¼šæå‡ºä¸ç¡®å®šæ€§è°ƒåˆ¶çš„å¢ç›ŠåŠ¨æ€ï¼Œå°†å¢ç›Šæ›´æ–°ä¸ä»»åŠ¡çš„ç†µï¼ˆä¸ç¡®å®šæ€§ï¼‰è”ç³»èµ·æ¥ï¼ŒåŠ¨æ€è°ƒæ•´å¢ç›Šä»¥å“åº”ä»»åŠ¡è¾¹ç•Œçš„å¤æ‚æ€§ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.   **å¢ç›Šè°ƒåˆ¶æƒé‡**ï¼šå®šä¹‰æœ‰æ•ˆæƒé‡ \\\\(W_{ij}(t) = g_i(t) w_{ij}(t)\\\\)ï¼Œå…¶ä¸­ \\\\(g_i(t)\\\\) æ˜¯ç¥ç»å…ƒå¢ç›Šï¼Œ\\\\(w_{ij}(t)\\\\) æ˜¯ synaptic weightsã€‚\\n> 2.   **å¢ç›Šæ›´æ–°**ï¼šå¢ç›ŠåŠ¨æ€éµå¾ª \\\\(g(t+1) = \\\\gamma g(t) + (1-\\\\gamma) g_0 + \\\\eta H(y)\\\\)ï¼Œå…¶ä¸­ \\\\(H(y)\\\\) æ˜¯ç½‘ç»œè¾“å‡ºçš„ç†µï¼Œç”¨äºé‡åŒ–ä¸ç¡®å®šæ€§ã€‚\\n> 3.   **æƒé‡æ›´æ–°**ï¼šé€šè¿‡æ ‡å‡†SGDæ›´æ–°æƒé‡ï¼Œä½†æ¢¯åº¦è¢«å¢ç›Šè°ƒåˆ¶ä¸º \\\\(\\\\frac{\\\\partial L}{\\\\partial w} = g(t) \\\\frac{\\\\partial L}{\\\\partial W}\\\\)ã€‚\\n> 4.   **ä»»åŠ¡åˆ‡æ¢æ£€æµ‹**ï¼šåœ¨ä»»åŠ¡è¾¹ç•Œå¤„ï¼Œç†µ \\\\(H(y)\\\\) å¢åŠ ï¼Œè§¦å‘å¢ç›Šç¬æ€æå‡ï¼Œä¿ƒè¿›å¿«é€Ÿé€‚åº”ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡é€šè¿‡ç®€åŒ–çº¿æ€§æ¨¡å‹ \\\\(y(t) = W_{\\\\mathrm{eff}}(t) x\\\\) å±•ç¤ºäº†å¢ç›Šè°ƒåˆ¶å¦‚ä½•è¿‘ä¼¼å¿«æ…¢æƒé‡æ–¹æ¡ˆã€‚åœ¨ä»»åŠ¡åˆ‡æ¢æ—¶ï¼Œå¢ç›Šæå‡å¯¼è‡´æœ‰æ•ˆæƒé‡å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œè€ŒåŸºç¡€å¢ç›Š \\\\(g_0\\\\) ç¡®ä¿æ…¢é€Ÿæƒé‡çš„ç¨³å®šæ€§ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   åŠ¨é‡SGDï¼ˆMSGDï¼‰\\n> *   Adamä¼˜åŒ–å™¨\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨å¹³å‡ç¨³å®šæ€§é—´éš™ï¼ˆavg-SGï¼‰ä¸Š**ï¼šæœ¬æ–‡æ–¹æ³•ï¼ˆNGM-SGDï¼‰åœ¨Split MNISTä¸Šè¾¾åˆ° **0.019**ï¼Œæ˜¾è‘—ä¼˜äºMSGDï¼ˆ0.270ï¼‰å’ŒAdamï¼ˆ0.155ï¼‰ã€‚ä¸æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼Œé—´éš™å‡å°‘äº† **93%ã€‚**\\n> *   **åœ¨å¹³å‡æœ€å°å‡†ç¡®ç‡ï¼ˆavg-min-ACCï¼‰ä¸Š**ï¼šåœ¨Split CIFAR-10ä¸Šï¼ŒNGM-SGDä¸º **63.875**ï¼Œä¼˜äºMSGDï¼ˆ56.611ï¼‰å’ŒAdamï¼ˆ47.415ï¼‰ï¼Œæå‡å¹…åº¦è¾¾ **7.26ä¸ªç™¾åˆ†ç‚¹ã€‚**\\n> *   **åœ¨æœ€å·®æƒ…å†µå‡†ç¡®ç‡ï¼ˆWC-ACCï¼‰ä¸Š**ï¼šå¯¹äºRotated MNISTï¼ŒNGM-SGDè¾¾åˆ° **65.536**ï¼Œé«˜äºMSGDï¼ˆ63.269ï¼‰å’ŒAdamï¼ˆ62.569ï¼‰ï¼Œä¸”ç¨³å®šæ€§é—´éš™ï¼ˆ0.054ï¼‰ä»…ä¸ºAdamçš„ **35%ã€‚**\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   æŒç»­å­¦ä¹  (Continual Learning, CL)\\n*   ç¨³å®šæ€§é—´éš™ (Stability Gap, N/A)\\n*   å¢ç›Šè°ƒåˆ¶ (Gain Modulation, N/A)\\n*   å»ç”²è‚¾ä¸Šè…ºç´ ç¥ç»è°ƒåˆ¶ (Noradrenergic Neuromodulation, N/A)\\n*   å¤šæ—¶é—´å°ºåº¦ä¼˜åŒ– (Multi-Timescale Optimization, N/A)\\n*   è”åˆè®­ç»ƒ (Joint Training, N/A)\\n*   ç”Ÿç‰©å¯å‘å­¦ä¹  (Bio-Inspired Learning, N/A)\\n*   èƒ½é‡æ™¯è§‚ (Energy Landscape, N/A)\"\n}\n```"
}