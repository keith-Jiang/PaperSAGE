{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.15573",
    "link": "https://arxiv.org/abs/2412.15573",
    "pdf_link": "https://arxiv.org/pdf/2412.15573.pdf",
    "title": "Multi Agent Reinforcement Learning for Sequential Satellite Assignment Problems",
    "authors": [
        "Joshua Holder",
        "Natasha Jaques",
        "Mehran Mesbahi"
    ],
    "categories": [
        "cs.MA",
        "cs.LG"
    ],
    "publication_date": "2024-12-20",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "University of Washington"
    ],
    "paper_content": "# Multi Agent Reinforcement Learning for Sequential Satellite Assignment Problems\n\nJoshua Holder1, Natasha Jaques2, Mehran Mesbahi1\n\n1Department of Aeronautics and Astronautics, University of Washington, Seattle, WA 98195 2Department of Computer Science, University of Washington, Seattle, WA 98195 josh.holder72 $@$ gmail.com, nj@cs.washington.edu, mesbahi@uw.edu\n\n# Abstract\n\nAssignment problems are a classic combinatorial optimization problem in which a group of agents must be assigned to a group of tasks such that maximum utility is achieved while satisfying assignment constraints. Given the utility of each agent completing each task, polynomial-time algorithms exist to solve a single assignment problem in its simplest form. However, in many modern-day applications such as satellite constellations, power grids, and mobile robot scheduling, assignment problems unfold over time, with the utility for a given assignment depending heavily on the state of the system. We apply multi-agent reinforcement learning to this problem, learning the value of assignments by bootstrapping from a known polynomial-time greedy solver and then learning from further experience. We then choose assignments using a distributed optimal assignment mechanism rather than by selecting them directly. We demonstrate that this algorithm is theoretically justified and avoids pitfalls experienced by other RL algorithms in this setting. Finally, we show that our algorithm significantly outperforms other methods in the literature, even while scaling to realistic scenarios with hundreds of agents and tasks.\n\n# Code â€” https://github.com/Rainlabuw/rl-enableddistributed-assignment\n\n# Introduction\n\nLarge-scale distributed systems like the power grid, transportation networks like Uber and Lyft, and satellite internet constellations are increasingly integrated in and critical to every aspect of our day-to-day lives, and will only become more so as time goes on. We can model these systems as a large group of agents working together to achieve broader goals - individual batteries and power plants working to satisfy grid-wide demand (Giovanelli et al. 2019), requested rides being distributed between cars (Qin, Zhu, and Ye 2022), or satellites working together to provide internet across the Earth (Lin et al. 2022).\n\nIn order to operate these systems efficiently, it is often necessarily to solve optimization problems at a massive scale. Given $n$ agents and $m$ tasks, one of the most natural optimization questions to ask is â€œHow can agents be optimally assigned to tasks?â€\n\nWhile we will see later that the simplest version of this question admits efficient solutions, most realistic problems are more complex. Specifically, in real systems, assignments must be made repeatedly rather than at a single instant. Furthermore, these problems are often state dependent - when an assignment is made, the state of the system changes, which affects the value of future assignments (i.e. a satellite has to change its orientation to complete a task). These more complex problems are often NP-hard (Gerkey and MataricÂ´ 2004) and thus difficult to approach with classical methods.\n\nThe temporal nature of this problem suggests that sequential decision-making techniques like reinforcement learning (RL) may be an attractive solution. However, as the number of agents in the environment grows, so too does the complexity of solving the problem with a centralized algorithm (Albrecht, Christianos, and SchaÂ¨fer 2024). For this reason, we look to multi-agent reinforcement learning (MARL) to enable our solution to scale up to the massive problem size required in realistic problem domains. Naive application of MARL is difficult for several reasons; rewards must be specified such that cooperation on the global objective is guaranteed, and actions taken by individual MARL agents must attempt to satisfy the constraint that each agent is assigned to a unique task (i.e. to avoid conflicting assignments).\n\nIn this work, we present a novel, theoretically justified algorithm that addresses both of these challenges. Rather than having agents learn to assign themselves to tasks directly, we have each agent learn the expected value of an assignment, and use these learned values as the input to an optimal distributed task assignment mechanism. This allows agents to execute joint assignments that satisfy assignment constraints and avoid selfishness, while learning a joint policy which is near-optimal on the level of the entire system. This architecture is depicted in Figure 1.\n\nWhile prior work has used MARL in a similar fashion to address assignment problems, this work often focused on variants of the problem specific to ride-sharing (Shah, Lowalekar, and Varakantham 2020; Azagirre et al. 2024). Throughout this work, we will take a far more general approach, and focus instead on satellite internet constellations as a novel motivating example (i.e. agents are satellites, and tasks are regions on Earth to provide internet to). This is a compelling application for a few reasons. First, satellite assignment problems are uniquely complex in that orbital mechanics dictates that a satellite cannot accomplish the same task indefinitely, and thus that frequent transitions between tasks must be considered. Second, these systems have exploded in size in just a few years to contain thousands of agents, with few existing algorithms which can provide the high degree of autonomy, efficiency, and resilience required in this environment. Finally, these systems are incredibly expensive, with the marginal cost of a satellite approaching $\\$ 500,000$ . If more efficient planning enables reducing the size of the constellation by even a few satellites out of thousands, it could save tens of millions of dollars.\n\n![](images/6684b49acae4d2b3a236da667f7f301d888fb42e9ee50a1e0d3b1e6a4a0b22c0.jpg)  \nFigure 1: Architecture of REDA. 1) Calculate independent estimates of future utility for each agent, combine into a matrix. 2) Select joint assignment $x _ { k } \\ = \\ \\alpha ( \\mathbf { Q } _ { k } ^ { \\pi } )$ which maximizes social utility, not utility for any given agent. 3) Execute $x _ { k }$ in environment and observe results. 4) Train agentsâ€™ independent value estimates based on minibatch from replay buffer.\n\nAs such, we apply our algorithm to a highly realistic satellite constellation management scenario, complete with highfidelity orbital mechanics and hundreds of satellites and tasks, a scale which is extremely uncommon among nonheuristic approaches in the literature (Wang et al. 2021). Despite this, we find that our approach outperforms other stateof-the-art approaches by $2 0 - 5 0 \\%$ across both multi-agent reinforcement learning (COMA (Foerster et al. 2018), IQL (Matignon, Laurent, and Le Fort-Piat 2012), IPPO (De Witt et al. 2020)) and classical optimization (HAAL, (Holder, Kraisler, and Mesbahi 2024)).\n\nIn summary, the contributions of this paper are:\n\nâ€¢ A MARL approach for assignment problems which seamlessly integrates existing greedy planners into MARL, but improves upon their solution for better longterm planning. â€¢ Novel insight into the workings of the method through simple experiments, direct comparison with state-of-theart RL methods, and theoretical analysis that provides intuition on global convergence properties. â€¢ Empirical results on a real satellite assignment problem that show a vastly improved ability to manage long-term resource constraints even when planning in complex environments with hundreds of satellites and tasks.\n\n# Technical Preliminaries\n\nMathematically, we can formulate the simplest version of an assignment problem with $n$ agents and $m$ tasks as:\n\n$$\n\\alpha ( \\beta ) = \\underset { x \\in X } { \\operatorname { a r g m a x } } \\ \\sum _ { i = 1 } ^ { n } \\sum _ { j = 1 } ^ { m } \\beta _ { i j } x _ { i j }\n$$\n\n# where:\n\nâ€¢ $\\beta \\in \\mathbb { R } ^ { n \\times m }$ is the benefit matrix, where $\\beta _ { i j }$ corresponds to the utility of agent $i$ completing task $j$ .   \nâ€¢ $x \\in X \\subset \\{ 0 , 1 \\} ^ { n \\times m }$ is the assignment matrix, where $x _ { i j } ~ = ~ 1$ if agent $i$ is assigned to task $j$ , and $x _ { i j } ~ = ~ 0$ otherwise.   \nâ€¢ $\\begin{array} { r } { X : = \\{ x \\in \\{ 0 , 1 \\} ^ { n \\times m } \\mid \\sum _ { j = 1 } ^ { m } x _ { i j } = 1 \\forall i } \\end{array}$ , $\\textstyle \\sum _ { i = 1 } ^ { n } x _ { i j } \\leq$ $\\mathbf { 1 } \\forall j \\}$ is the set of valid assignments. Thi corresponds to the set of assignment matrices such that each agent completes 1 task, and each task is completed by at most 1 agent.\n\nWhen benefits $\\beta$ are given, this is a well-studied problem for which a solution can be easily obtained in polynomial time (i.e. with a single Python command) (Kuhn 1955). As such, we denote the solution to Equation 1 as a function $\\alpha : \\mathbb { R } ^ { n \\times m }  X$ .\n\nConsider the sequential assignment problem (SAP), a more complex case where assignments need to be made at several time steps, and the assignment benefits $\\hat { \\beta }$ depend on some state $s \\in S$ that evolves according to a transition function $\\mathcal { T } : S \\times X  S$ .\n\n$$\n\\operatorname* { m a x } _ { \\pi } \\mathbb { E } ^ { \\pi } \\bigg [ \\sum _ { k = 1 } ^ { T } \\sum _ { i = 1 } ^ { n } \\sum _ { j = 1 } ^ { m } \\gamma ^ { k - 1 } [ \\hat { \\beta } ( s _ { k } ) ] _ { i j } [ x _ { k } ] _ { i j } \\bigg ]\n$$\n\nwhere $x _ { k }$ and $s _ { k }$ denote the assignment and state at time step $k , s _ { 1 } \\sim S _ { 0 } , \\gamma$ is the discount factor, and $\\mathbb { E } ^ { \\pi }$ denotes that states evolve according to the transition dynamics $s _ { k + 1 } \\sim$ ${ \\mathcal { T } } ( s _ { k } , x _ { k } )$ and that assignments are chosen with respect to the policy $x _ { k } \\sim \\pi ( s _ { k } )$ .\n\nThe clear parallel between this problem and a more standard finite-time Markov Decision Process $\\begin{array} { r l } { \\mathcal { M } } & { { } = } \\end{array}$ $( S , \\mathcal { A } , S _ { 0 } , \\mathcal { T } , r , \\gamma )$ seen in RL is outlined in Table 1.\n\nTable 1: Mapping between classic RL and SAP problem formulation.   \n\n<html><body><table><tr><td>RL Formulation</td><td>SAPFormulation</td></tr><tr><td>A (action space)</td><td>X (valid assignment space)</td></tr><tr><td>Î± âˆˆA(action)</td><td>x âˆˆX (assignment)</td></tr><tr><td>r(s,a) (reward func.)</td><td>âˆ‘i,j[Î²(s)lijxij (benefit func.)</td></tr></table></body></html>\n\nas $Q$ $\\pi$ $\\begin{array} { r } { Q ^ { \\pi } ( s _ { k } , x _ { k } ) : = \\mathbb { E } ^ { \\pi } [ r ( s _ { k } , x _ { k } ) + \\sum _ { t = k + 1 } ^ { T } \\gamma ^ { t - k } r ( s _ { t } , x _ { t } ) ] . } \\end{array}$ ning in state $s _ { k }$ , making assignment $x _ { k }$ , and following policy $\\pi$ thereafter.\n\nIn order to scale solutions to large groups of agents and tasks, it is desirable to formulate this centralized RL problem as a MARL problem. In the MARL case, we define a joint assignment $x \\ = \\ ( x ^ { 1 } , \\cdot \\cdot \\cdot , x ^ { n } )$ and a joint policy $\\pi ^ { \\circ } = ( \\pi ^ { 1 } , \\cdots , \\pi ^ { n } )$ , $x ^ { i } \\sim \\overset { \\cdot } { \\pi } ^ { i }$ . The assignment space for a single agent $x ^ { i } : = \\operatorname { a r g m a x } _ { j }$ $x _ { i j } \\in [ m ]$ is now a single integer denoting the task agent $i$ is assigned to.\n\nWe assume that the environment is partially observable, and that agents are equipped with an observation function $\\mathcal { O } ^ { i } : \\mathcal { S } \\stackrel { \\smile } { \\to } O ^ { i }$ which they use to observe components of the state $o ^ { i } \\sim \\mathcal { O } ^ { i } ( s ) \\in \\bar { O ^ { i } }$ on which to condition their $Q$ - functions.\n\nWe now describe the deficiencies in previous approaches to solving the SAP.\n\n# Related Work Classical methods for the sequential assignment problem\n\nAlthough efficient, optimal solutions exist for finding an optimal assignment for a single time step (1), the SAP (2) is NP-hard except in trivial cases and is thus much harder to tackle using classical approaches. Much of the existing work relies on purely heuristic methods (Pachler de la Osa et al. 2021) or ignores the state-dependent aspect of the problem entirely (Bui et al. 2022). In one recent work (Holder, Kraisler, and Mesbahi 2024), the authors develop HAAL, which uses information from several time steps to generate assignments in the style of model-predictive control, but the method is limited to a specific class of deterministic SAPs.\n\n# RL for the sequential assignment problem\n\nSimilar to (Shah, Lowalekar, and Varakantham 2020), we bootstrap RL learning from a greedy assignment mechanism and use Gaussian noise to induce exploration. However, Shah et al is focused on a particular variant of the SAP specific to the problem of pooling rides when ride-sharing. By contrast, we present a generalized method for solving the SAP across domains, and present both a theoretical analysis and an empirical comparison with state-of-the-art RL and MARL algorithms, providing intuition into the global convergence properties of the method.\n\nOther work at the intersection of RL and assignment problems allows agents to directly learn to propose their value for completing a task (Chang et al. 2020), introducing problems of incentive-compatibility, or do not directly learn the values of individual assignments (Hwang, Chen, and Lee 2007), greatly limiting the expressiveness of the method. By contrast, our method avoids such problems by using a centralized consensus mechanism, while maintaining the expressiveness of learned values.\n\n# Why donâ€™t normal MARL methods work?\n\nExisting MARL techniques have seen success when applied to incredibly complex problems like DOTA 2 (Berner et al. 2019), but the SAP poses a unique challenge.\n\nThe first challenge is deciding how to specify the reward function for an agent $i$ , $r _ { i } ( s , x )$ . Given that our objective is a global maximization over tasks completed by all agents rather than any single agent, one might be tempted to use cooperative rewards, where $r _ { i } ( s , x ) = r ( s , x )$ , as in (Rashid et al. 2020; Sunehag et al. 2017). However, as we will see later, agents with this reward function struggle to disentangle the effect of their actions among the many other agents in the group, even when applying techniques like COMA, which are designed to enable a single agent to assess its counterfactual impact on the joint reward (Foerster et al. 2018).\n\nConversely, one might take note of the recent success of completely independent agents in cooperative domains (De Witt et al. 2020; Papoudakis et al. 2020; Yu et al. 2022) and provide agents with the rewards they yield solely from tasks they complete, $r _ { i } ( s , x ^ { i } ) = [ \\hat { \\beta } ( s ) ] _ { i x ^ { i } } ^ { \\cdot \\mathrm { ~ } }$ . When specifying rewards in this way, the learning problem is significantly easier because rewards correlate more directly with agent actions, and we hope that cooperation between agents in allocating tasks emerges naturally from the training process. Empirically, however, we see this behavior does not emerge easily, and that many agents simultaneously assign themselves to the most valuable tasks, even if the tasks can only be completed by a single agent.\n\nThis leads us to the second difficulty of applying MARL to assignment problems, which is that without an explicit constraint, joint assignments often consist of multiple agents completing the same task, meaning that $x \\notin X$ . Clearly, some centralization or communication between agents is necessary to ensure that agents do not duplicate assignments. Our method provides a principled approach for allowing agents to resolve conflicts and learn to make socially optimal and valid assignments $x \\in X$ .\n\n# Method\n\nThe function $\\alpha$ introduced in Equation 1 is a solution to both of these difficulties. First, note that given information about the benefit to individual agents $i$ for completing tasks $j$ , $\\beta _ { i j }$ , $\\alpha$ yields assignments which are optimal on the level of the entire group. Additionally, $\\alpha$ yields joint assignments $x \\in X$ â€”i.e. those that avoid assigning multiple agents to the same taskâ€”by definition.\n\nThus, we would like to use $\\alpha$ as our joint policy, $\\pi ( s ) =$ $\\alpha ( \\beta )$ . However, in the SAP setting the state-dependent nature of the planning problem means that we do not know the long-term benefit of assignments $a$ priori, and thus cannot easily access a $\\beta \\in \\mathbb { R } ^ { n \\times \\bar { m } }$ . Instead, we propose to learn the expected future value of each assignment $i  j$ from experience, and use these values as input to $\\alpha$ .\n\n# Using agent Q-functions in the assignment mechanism $\\alpha$\n\nMany $Q$ -learning based algorithms such as DQN (Mnih et al. 2013) require the ability to take actions $\\epsilon$ - greedily with respect to the current policy, $\\begin{array} { r l } { x _ { k } } & { { } = } \\end{array}$ argmaxxâˆ— X QÏ€(sk, xâˆ—). However, in the assignment setting, acting greedily with respect to the joint policy $\\pi$ becomes a difficult non-convex optimization problem. Our method provides a way of approximating this behavior.\n\nA key part of why we can do this is because of a decomposition of the joint $Q$ function that exists in the assignment problem setting (similar to the one used in (Sunehag et al. 2017)), which we outline in Theorem 1.\n\nTheorem 1 (Decomposition of $Q ^ { \\pi ^ { \\alpha } }$ into $Q _ { i } ^ { \\pi ^ { \\alpha } }$ ). Let $\\pi ^ { \\alpha }$ : $S \\to X$ be a constant, deterministic joint policy. Define the $Q$ -function for an individual agent with respect to this joint policy $\\pi ^ { \\alpha }$ as:\n\n$$\n\\boldsymbol { Q } _ { i } ^ { \\pi ^ { \\alpha } } ( s _ { k } , j ) : = \\mathbb { E } ^ { \\pi ^ { \\alpha } } \\biggl [ r _ { i } ( s _ { k } , j ) + \\sum _ { t = k + 1 } ^ { T } \\gamma ^ { t - k } r _ { i } ( s _ { t } , x _ { t } ^ { i } ) \\biggr ]\n$$\n\nThen, in the assignment problem setting, where $r ( s , x ) =$ $\\begin{array} { r } { \\sum _ { i = 1 } ^ { n } \\sum _ { j = 1 } ^ { m } \\hat { \\beta } ( s ) x _ { i j } = \\sum _ { i = 1 } ^ { n } r _ { i } ( s , x ^ { i } ) , } \\end{array}$ ,\n\n$$\n{ Q ^ { \\pi } } ^ { \\alpha } ( s _ { k } , x ) = \\sum _ { i = 1 } ^ { n } \\sum _ { j = 1 } ^ { m } { Q _ { i } ^ { \\pi } } ^ { \\alpha } ( s _ { k } , x ^ { i } ) x _ { i j } \\quad f o r \\ x = \\pi ^ { \\alpha } ( s _ { k } ) .\n$$\n\nProof. This follows from the fact that $\\begin{array} { r l } { r ( s , x ) } & { { } = } \\end{array}$ $\\textstyle \\sum _ { i = 1 } ^ { n } r _ { i } ( s , x ^ { i } )$ â€”that is, the sum of the reward for the individual agents is equal to the joint reward, and the reward for agent $i$ is conditioned only on agent $i$ â€™s assignment, $x ^ { i }$ . See supplemental materials on arXiv for the complete proof.\n\nIn words, given a state $s _ { k }$ , $Q _ { i } ^ { \\pi ^ { \\alpha } } ( s _ { k } , j )$ is defined as the total expected future reward that agent $i$ will obtain, given that agent $i$ is assigned to task $j$ , and then that all agents follow the joint policy $\\pi ^ { \\alpha }$ for future assignments. Given the partial observability of our environment, in practice we make the approximation $Q _ { i } ^ { \\pi ^ { \\alpha } } ( s , j ) \\approx Q _ { i } ^ { \\pi ^ { \\alpha } } ( \\mathcal { O } ^ { i } ( \\bar { s } ) , j )$ .\n\nHere, we can begin to see the clear connection between Equations 1 and 3; if we define our benefit matrix (previously $\\beta$ ) to be $\\mathbf { Q } _ { k } ^ { \\pi } \\in \\mathbb { R } ^ { n \\times m }$ such that $\\begin{array} { r l } { [ \\mathbf { Q } _ { k } ^ { \\pi ^ { \\alpha } } ] _ { i j } } & { { } = } \\end{array}$ $Q _ { i } ^ { \\pi ^ { \\alpha } } ( o _ { k } ^ { i } , j )$ , then $\\begin{array} { r } { \\operatorname * { a r g m a x } _ { x ^ { * } \\in X } { Q ^ { \\pi ^ { \\alpha } } \\left( s _ { k } , x ^ { * } \\right) } \\ \\approx \\ \\alpha ( \\mathbf { \\bar { Q } } _ { k } ^ { \\pi } ) . } \\end{array}$ . Then, we can make assignments according to:\n\nWhile Equation 3 only holds for $x = \\pi ^ { \\alpha } ( s )$ , if we assume that policies change slowly during the learning process such that $\\dot { \\alpha } ( \\mathbf { Q } _ { k } ^ { \\pi ^ { \\alpha } } ) \\approx \\bar { \\pi } ^ { \\alpha } ( s )$ , then Equation 4 is a valid approximation. Thus, when we learn estimates of $Q _ { i } ^ { \\pi ^ { \\alpha } } ( o _ { k } ^ { i } , j )$ directly from experience, we can build $\\mathbf { Q } _ { k } ^ { \\pi ^ { \\alpha } }$ and have agents act not by picking assignments that are best for themselves, but through the mechanism $\\alpha$ which is guaranteed to return a socially optimal outcome for the group. This motivates our algorithm which we fully specify in the next section.\n\n# RL-Enabled Distributed Assignment (REDA)\n\nREDA, described in Algorithm 1 and depicted in Figure 1, is our method for generating solutions to the SAP, with key differences from a standard independent DQN algorithm (e.g. (Tampuu et al. 2017)) on lines 6-11 and 21-23.\n\n# Algorithm 1: RL-Enabled Distributed Assignment (REDA)\n\nGiven: state-dependent benefit function ${ \\hat { \\beta } } : S  \\mathbb { R } ^ { n \\times m }$\n\n1: Initialize $Q$ -network parameters $\\theta$ , target $Q$ -network parameters ${ \\bar { \\theta } } = \\theta$ 2: Initialize a replay buffer $D$ 3: for episode $e = 1 , 2 , \\dots { \\bf d o }$ 4: for time step $k = 1 , . . . , T$ do 5: Collect joint observation $o _ { k } = \\left( o _ { k } ^ { 1 } , \\cdots , o _ { k } ^ { n } \\right)$ 6: With probability $\\epsilon \\colon x _ { k } \\gets \\alpha ( \\hat { \\beta } ( s _ { k } ) )$ (act greedily w/r/t the current benefit matrix) 7: Otherwise: 8: Build $\\mathbf { Q } _ { k } ^ { \\pi }$ such that $[ \\mathbf { Q } _ { k } ^ { \\pi } ] _ { i j }  Q _ { i } ^ { \\pi } ( o _ { k } ^ { i } , j ; \\theta )$ 9: $\\begin{array} { r } { \\mathbf { Q } _ { \\mathrm { a v g } }  \\frac { 1 } { n m } \\sum _ { i = 1 } ^ { n } \\sum _ { j = 1 } ^ { m } \\mathop { | [ \\mathbf { Q } _ { k } ^ { \\pi } ] _ { i j } | } } \\end{array}$ 10: Generate perturbation matrix $\\pmb { \\xi } \\in \\mathbb { R } ^ { n \\times m }$ , where $\\pmb { \\xi } _ { i j } \\sim N ( 0 , 2 \\bar { \\bf Q } _ { \\mathrm { a v g } } \\epsilon )$ 11: $x _ { k }  \\alpha ( \\mathbf { Q } _ { k } ^ { \\pi } + \\pmb { \\xi } )$ (act $\\sim$ optimally w/r/t the estimated values of $Q _ { i } ^ { \\pi } ( o _ { k } ^ { i } , j ; \\theta ) ,$ ) 12: Collect joint assignment $x _ { k } = ( x _ { k } ^ { 1 } , \\cdot \\cdot \\cdot , x _ { k } ^ { n } )$ 13: Collect joint reward $r _ { k } = ( r _ { k } ^ { 1 } , \\cdot \\cdot \\cdot , r _ { k } ^ { n } )$ , where $r _ { k } ^ { i }  r _ { i } ( s _ { k } , x _ { k } ^ { i } )$ 14: Observe next state $s _ { k + 1 } \\sim \\mathcal { T } ( s _ { k } , x _ { k } )$ 15: Collect joint observation $o _ { k + 1 }$ 16: Store joint transition $\\left( o _ { k } , x _ { k } , r _ { k } , o _ { k + 1 } \\right)$ in $D$ 17: Sample random mini-batch of $B$ joint transitions $\\left( o _ { t } , x _ { t } , r _ { t } , o _ { t + 1 } \\right)$ from $D$ 18: if $s _ { t + 1 }$ is terminal then 19: Targets $y _ { t } ^ { i } \\gets r _ { t } ^ { i }$ for all $i$ 20: else 21: $\\begin{array} { r l } & { \\mathrm { ~ B u i l d ~ } \\mathbf { Q } _ { t + 1 } ^ { \\pi } , [ \\mathbf { Q } _ { t + 1 } ^ { \\pi } ] _ { i j } \\gets Q _ { i } ^ { \\pi } ( o _ { t + 1 } ^ { i } , j ; \\theta ) } \\\\ & { x _ { t + 1 } \\gets \\alpha ( \\mathbf { Q } _ { t + 1 } ^ { \\pi } ) } \\\\ & { \\mathrm { T a r g e t s ~ } y _ { t } ^ { i } \\gets r _ { t } ^ { i } + \\gamma Q _ { i } ^ { \\pi } ( o _ { t + 1 } ^ { i } , x _ { t + 1 } ^ { i } ; \\bar { \\theta } ) \\forall i } \\end{array}$ 22: 23: 24: end if $\\begin{array} { r } { \\overline { { \\mathcal { L } ( \\boldsymbol { \\theta } ) } }  \\frac { 1 } { B } \\displaystyle \\sum _ { t = 1 } ^ { B } \\sum _ { i = 1 } ^ { n } ( y _ { t } ^ { i } - Q ( o _ { t } ^ { i } , x _ { t } ^ { i } ; \\boldsymbol { \\theta } ) ) ^ { \\sum } } \\end{array}$ 2 25: Loss 26: Update parameters $\\theta$ by minimizing $\\mathcal { L } ( \\boldsymbol { \\theta } )$ 27: Update target network parameters $\\bar { \\theta }$ periodically 28: end for 29: end for\n\n$$\nx _ { k } = \\alpha ( { \\mathbf { Q } _ { k } ^ { \\pi ^ { \\alpha } } } ) \\approx \\underset { x ^ { * } \\in X } { \\operatorname { a r g m a x } } Q ^ { \\pi ^ { \\alpha } } ( s _ { k } , x ^ { * } ) .\n$$\n\nBootstrapping from a greedy policy. Sequential assignment problems are unique in that there always exists a suboptimal, non-parametrized policy with which we can bootstrap our policy from; $\\pi ( s ) : = { \\dot { \\alpha } } ( { \\hat { \\beta } } ( s ) )$ , where we simply make the greedy assignment with respect to the current benefit matrix at every time step, without regard for future benefits. At the beginning of training, we act with this greedy policy with probability $\\epsilon$ , filling our replay buffer with reasonable state-assignment pairs before beginning to learn to improve on this policy.\n\nExploration. To induce further exploration, we also add randomly distributed noise $\\boldsymbol { \\xi }$ to $\\mathbf { Q } ^ { \\pi }$ , scaled by the current average magnitude of $\\mathbf { Q } ^ { \\pi }$ , $\\mathbf { Q } _ { \\mathrm { a v g } }$ , such that sub-optimal joint assignments are selected with some probability. This is a more effective exploration strategy than making entirely random joint assignments $x \\in X$ given the size of the search space, $\\begin{array} { r } { | X | = \\frac { \\mathbf { \\tilde { \\phi } } _ { m ! } } { ( m - n ) ! } } \\end{array}$\n\nUnlike previous work (Shah, Lowalekar, and Varakantham 2020), perturbations do not need to be tuned according to reward magnitude because the noise is scaled directly according to the values of $\\mathbf { Q } ^ { \\pi }$ .\n\nTarget specification. Another important aspect of REDA is the way learning targets are specified. Because the policy $\\pi$ can only select assignments $x \\in X$ , targets must also satisfy this constraint. In other words, Lines 21 through 23 express $y \\ = \\ r + \\operatorname* { m a x } _ { x ^ { * } \\in X } Q ^ { \\pi } ( s , x )$ rather than $y =$ $r + \\operatorname* { m a x } _ { x ^ { * } } Q ^ { \\pi } ( s , x )$ . We find the best assignment $x ^ { * } \\in X$ by again using $\\alpha$ â€”following the standard DQN paradigm (Mnih et al. 2013), $\\mathbf { Q } _ { t + 1 } ^ { \\pi }$ is generating using the value network with parameters $\\theta$ , but the assignment $x _ { t + 1 }$ is evaluated using the target network with parameters $\\bar { \\theta }$ .\n\nComputing $\\alpha ( \\mathbf { Q } ^ { \\pi } )$ in a distributed way. In deployment, each agent can independently compute $\\alpha ( \\mathbf { Q } ^ { \\pi } )$ , either by receiving the values of $Q _ { i } ^ { \\pi } ( \\bar { o } _ { k } ^ { i } , j ) \\bar { \\forall } i , j$ , or by using market-based mechanisms in which agents exchange bids with neighboring agents until they are matched with the task for which they are willing to pay more than all other agents, as in (Zavlanos, Spesivtsev, and Pappas 2008). This means that as long as agents can communicate about task values, the algorithm can be executed in a distributed fashion.\n\n# Theoretical justification\n\nTo further motivate why REDA produces sensible policies, we can show that the REDA target update causes $Q _ { i }$ to converge to the true $Q _ { i } ^ { \\pi }$ under reasonable assumptions.\n\nLemma 1. Let $Q _ { i } ~ \\in ~ \\mathcal { Q }$ be an arbitrary $Q$ -function. Let $F : \\mathcal { Q }  \\mathcal { Q }$ be the operator corresponding to the REDA target update in the tabular case, without target networks or the greedy guide policy:\n\n$$\n( F Q _ { i } ) ( o _ { k } ^ { i } , j ) = \\mathbb { E } ^ { \\pi } \\bigg [ r _ { i } ( s _ { k } , j ) + \\gamma Q _ { i } ( o _ { k + 1 } ^ { i } , x _ { k + 1 } ^ { i } ) \\bigg ]\n$$\n\nAssume a finite observation space, and that each observation-assignment pair $( o ^ { i } , j )$ is visited infinitely often under a constant policy $\\pi$ . Then, if $Q _ { i } ^ { n + 1 } \\ \\gets \\ F Q _ { i } ^ { n }$ , $\\begin{array} { r } { \\operatorname* { l i m } _ { n \\to \\infty } Q _ { i } ^ { n } = Q _ { i } ^ { \\pi } } \\end{array}$ .\n\nProof. The REDA target update is analogous to a SARSA update (Singh et al. 2000), so this can be easily proven by showing that $F$ is a $\\gamma$ -contraction on the space of $Q$ - functions, and that $Q _ { i } ^ { \\pi }$ is the unique fixed point of this contraction. See the supplemental materials on arXiv for the complete proof. â–¡\n\nThe critical assumption inherent in Lemma 1 is that the policy does not change before each observation-assignment pair is visited infinitely many times. However, assuming that a sufficiently small learning rate is chosen, Lemma 1 will approximately hold for REDA and it can be assumed that the $Q$ -values used in the mechanism $\\alpha$ will converge to their desired values, $Q _ { i } ^ { \\pi }$ .\n\nThis has several important implications. First, in situations where agents are providing information to centralized mechanisms for assignment, one often has to consider incentive-compatibility and whether agents are being truthful about the information they provide. Lemma 1 shows that based on REDAâ€™s training process, the values $Q _ { i } ( o ^ { i } , j )$ used in the assignment mechanism $\\alpha$ will indeed converge to $Q _ { i } ^ { \\pi } ( o ^ { i } , j )$ as desired, and that agents are not able to act strategically or lie for personal benefit, a claim which we verify by experiment.\n\nSecond, it motivates that REDA is a method of approximating DQN on the joint $Q$ -function $Q ^ { \\pi ^ { \\alpha } }$ . Because Lemma 1 states that given enough updates, ${ \\dot { Q _ { i } } }  Q _ { i } ^ { \\pi ^ { \\alpha } }$ , Equation 4 holds and $\\begin{array} { r } { \\alpha ( \\mathbf { Q } ^ { \\pi ^ { \\alpha } } ) \\ \\approx \\ \\mathrm { a r g m a x } _ { x ^ { * } \\in X } Q ^ { \\pi ^ { \\alpha } } ( s , x ^ { * } ) } \\end{array}$ . Then, acting according to $x _ { k } = \\alpha ( \\mathbf { Q } _ { k } ^ { \\pi ^ { \\alpha } } + \\pmb { \\xi } )$ is approximately $\\epsilon$ -greedy action selection with respect to $Q ^ { \\pi ^ { \\alpha } }$ , and target updates $\\textstyle \\sum _ { i = 1 } ^ { n } y _ { k } ^ { i } = \\sum _ { i = 1 } ^ { n } r _ { k } ^ { i } + \\alpha _ { i } ^ { \\cdot } ( \\mathbf { Q } _ { k + 1 } ^ { \\pi ^ { \\alpha } } )$ approximate an optimal target update $y _ { k } = r _ { k } + \\operatorname* { m a x } _ { x ^ { * } \\in X } Q ^ { \\pi ^ { \\alpha } } { \\left( s _ { k + 1 } , x ^ { * } \\right) } .$ , where $\\alpha _ { i } ( \\beta )$ is the value provided to agent $i$ from assignment $\\alpha ( \\beta )$ . Thus, we can expect REDA to inherit similar properties relating to the convergence of $Q ^ { \\pi ^ { \\alpha } }$ to $Q ^ { \\pi ^ { * } }$ .\n\nThis key insight can explain REDAâ€™s strong performance at the system level as compared to state-of-the-art RL methods which act with independent agents, as we will see in the following section.\n\n# Empirical Experiments\n\nWe first test REDA in a simple SAP setting to provide intuition about why it is able to outperform existing methods in the literature. Then, we scale it up, applying it to a complex satellite constellation task allocation environment with hundreds of satellites and tasks, showing the power and efficiency of this method.\n\n# Does REDA encourage unselfish behavior?\n\nWe design our first experiment to test whether REDA can avoid selfish assignments. We have three agents, three tasks, and three states. Agent 1 is the â€œdictatorâ€ in that its assignment fully dictates the state transition, $s _ { k + 1 } = x _ { k } ^ { 1 } . { \\hat { \\beta } } ( s ) { \\overline { { \\quad } } }$ is specified as follows:\n\n$$\n{ \\hat { \\beta } } ( 1 ) = { \\left[ \\begin{array} { l l l } { 2 } & { 3 } & { 0 } \\\\ { 0 } & { 2 } & { 3 } \\\\ { 3 } & { 0 } & { 2 } \\end{array} \\right] } , \\quad { \\hat { \\beta } } ( 2 ) = { \\left[ \\begin{array} { l l l } { 0 } & { 3 } & { 0 } \\\\ { 0 } & { 0 } & { 0 . 1 } \\\\ { 0 . 1 } & { 0 } & { 0 } \\end{array} \\right] } ,\n$$\n\n$$\n\\hat { \\beta } ( 3 ) = \\left[ \\begin{array} { c c c } { { 0 } } & { { 0 } } & { { 3 } } \\\\ { { 0 . 1 } } & { { 0 } } & { { 0 } } \\\\ { { 0 } } & { { 0 . 1 } } & { { 0 } } \\end{array} \\right]\n$$\n\n![](images/8c294f17bcdeba6551d37758ba4793cd7eece9aac1f6e16b5110e1989b1a8394.jpg)  \nFigure 2: Performance over 5 runs of various algorithms in dictator environment, shown with standard deviation shaded. Note that after $\\epsilon$ decays to 0 at $t = 1 0 { , } 0 0 0$ , performance for REDA instantly approaches the theoretical maximum, while other algorithms remain significantly below the maximum.\n\n![](images/19d592dbd8fa5258455333658c63c5f1726d8a06629de0a72ef98ab3cc8aaf23.jpg)  \nFigure 3: Performance over 5 runs of various algorithms in a realistic constellation environment with 324 satellites and 450 tasks, shown with standard deviation shaded. $\\epsilon$ is decayed to zero over 300k time steps. REDA consistently converges and obtains more reward than all other tested algorithms.\n\nRather than requiring $x \\in X$ , which cannot be satisfied by existing algorithms, we specify that when agents assign themselves to a task, they receive benefit corresponding to what proportion of the task they completed (i.e. $5 0 \\%$ when two agents complete the same task). This disincentivizes duplicated assignments, and means that in this SAP, the optimal policy is the joint assignment $x _ { k } = ( 1 , 2 , 3 )$ for all $k$ , yielding 60 reward over 10 time steps. However, the greedy optimal policy for agent 1 is to continually assign itself to task 2, securing 3 reward for itself each time step but causing the system as a whole to receive far less benefit by driving the state to $s = 2$ .\n\nPerformance of various algorithms in this environment is shown in Figure 2. Qualitative analysis of the results yields significant insights into the trade-offs of each method. While REDA immediately drives the group to optimal joint policy, both IQL and IPPO reliably converge to the greedy joint policy $x _ { k } = ( 2 , 3 , 1 ) \\forall k$ . This means the dictator is acting selfishly and driving the system to state 2 at the expense of other agentsâ€”in the satellite context, this might correspond to a satellite completing a high priority task itself rather than allowing a better suited satellite to complete it instead.\n\nCOMA avoids this greedy behavior, but still has difficulty disentangling the effect of agents on the joint reward. In experiments, it converges to the joint policy $x _ { k } = ( 1 , 3 , 1 ) \\forall k$ , which suggests that it cannot determine whether agent 1 or 3 should complete task 1.\n\n# Can REDA be applied to large problems?\n\nTo demonstrate the ability of REDA to scale to large complex environments, we use satellites in a constellation as agents and points on the Earthâ€™s surface as tasks. We generate a constellation of 324 satellites evenly distributed around the Earth, with 450 randomly placed tasks simulating internet users. Some tasks are assigned a higher priority, and thus provide higher reward for satellites who complete them. The state-dependent benefit of an assignment $i  j$ consists of three components:\n\nâ€¢ The priority of task $j$ , and its distance from satellite $i$ â€™s position in orbit.   \nâ€¢ A penalty for switching assignments between two time steps (i.e. $x _ { k - 1 } ^ { i } \\neq j )$ , corresponding to the energy and time expenditure required for changing satellite orientation.   \nâ€¢ The power state $p ^ { i }$ of the satellite. Starting at 1 power, each time a satellite is assigned to a task in its range of visibility, 0.2 power is expended, and being assigned to a task out of view corresponds to charging, raising power by 0.1. Once a satellite is out of power, it captures no benefit for the rest of the episode.\n\nThis is a very challenging optimization problem, as satellites have to balance multiple conflicting priorities: ensuring that they are assigned to tasks that they will remain close to in their upcoming orbital motion (so as to avoid having to change assignments frequently), equitably distributing tasks amongst the hundreds of satellites, and managing their power state over a 100 time step episode.\n\nIn order to scale to this large problem, and given that a single satellite can only physically observe a small portion of Earthâ€™s surface at a given time, we model the environment as a Partially Observed Markov Decision Process. Specifically, observations are limited to information on the top 10 closest tasks, the previous assignment $\\boldsymbol { x } _ { k - 1 } ^ { i }$ , and the power state $p ^ { i }$ , as well as information related to the nearest 10 satellites to satellite $i$ in orbit.\n\n![](images/a7500082ad6518a130e33556f6bd9f3887e6636c2e33491e52f8aefccfe3197b.jpg)  \nFigure 4: Performance of tested algorithms on various metrics; percentage of satellites without charge at the end of the episode (lower is better), percentage of satellites completing the same task as another satellite (lower is better), and the average number of time steps satellites are assigned to the same tasks (higher is better). We can see that REDA outperforms IQL and IPPO across the board, while avoiding having satellites run out of power as when using classical methods like HAAL.\n\nSimilarly, in reality satellites can only complete a subset of the tasks at a given time. Thus, we limit the size of the action space to 11, the first 10 corresponding to an assignment to the top 10 closest tasks, with the remaining action corresponding to completing any other task. A satellite being assigned to a task it cannot complete can be interpreted as a satellite deciding to forgo benefit at that time step and instead charge its battery. A full description of the experimental setup is provided on arXiv.\n\nAll tested algorithms operate with this reduced observation and action space to maintain parity. Results for COMA are not provided because COMA requires training a centralized critic that can learn the value of all possible joint assignments, making it impractical for problems with a large joint state space.\n\nFigure 3 shows the results of our algorithm in this environment. We find that REDA has low variance and consistently outperforms all other tested algorithms. To ensure a fair comparison, IQL was also provided with pretraining from the greedy policy, while IPPO was trained from scratch after behavior cloning on the greedy policy was found not to be beneficial. See the supplemental materials on arXiv for further details on hyperparameter selection, network architecture, and compute requirements.\n\nIn Figure 4, we can see qualitatively why REDA performs so well on this task. State-of-the-art MARL approaches avoid running out of power, but generate assignments with a significant amount of conflict. Classical approaches like HAAL make consistent assignments, but fail to manage their power state over the course of the episode, running out of power far before the end of the episode. REDA succeeds in all three areas; it entirely eliminates conflicting assignments and minimizes changes in assignment to the extent possible while still successfully managing satellite power states over the entire episode.\n\n# Discussion\n\nWe present REDA, a MARL method for learning efficient solutions to complex, state-dependent assignment problems. Rather than allow agents to act completely independently or attempt to learn the effect of their actions on the reward of the entire group, REDA strikes a balance, allowing agents to focus on learning about their personal rewards while selecting actions according to an optimal distributed task assignment mechanism. This allows the algorithm to ensure agents act unselfishly while learning efficient solutions even in very large problem settings.\n\n# Limitations and Future Work\n\nOne limitation is that assignment problems, even when including state dependence, lack the expressiveness needed for certain problem settings. For example, REDA assumes that the reward can indeed be decomposed as $r ( s , x ) \\ =$ $\\scriptstyle \\sum _ { i = 1 } ^ { n } r _ { i } ( s , x ^ { i } )$ . In certain scenarios, like when satellite beams can cause frequency interference with one another, this assumption does not hold. Additionally, as presented REDA only applies to scenarios where agents can only complete a single task, and where tasks can only be completed by a single agent. Future work will investigate to what extent REDA can be applied to a broader class of problems.\n\nHowever, even this somewhat more limited set of problems is still broad enough to encompass a large variety of pressing abstract and practical problems. For example, REDA can be straightforwardly applied to the multiple Traveling Salesman Problem, another hugely important combinatorial optimization problem.\n\nWe also plan to apply REDA in a variety of applied problem settings, including distributed power grid management and large-scale transportation networks. REDA could even be used as a high-level planner in a hierarchical RL setting.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³äº†**çŠ¶æ€ä¾èµ–çš„åºåˆ—åˆ†é…é—®é¢˜ï¼ˆSequential Assignment Problem, SAPï¼‰**ï¼Œå³åœ¨åŠ¨æ€ç³»ç»Ÿä¸­å¦‚ä½•é«˜æ•ˆåœ°å°†å¤šä¸ªä»£ç†ï¼ˆå¦‚å«æ˜Ÿï¼‰åˆ†é…åˆ°å¤šä¸ªä»»åŠ¡ï¼ˆå¦‚åœ°çƒä¸Šçš„äº’è”ç½‘æœåŠ¡åŒºåŸŸï¼‰ï¼ŒåŒæ—¶è€ƒè™‘ç³»ç»ŸçŠ¶æ€çš„æ—¶å˜æ€§å’Œåˆ†é…çš„é•¿è¿œæ•ˆç›Šã€‚\\n> *   è¯¥é—®é¢˜åœ¨å«æ˜Ÿæ˜Ÿåº§ç®¡ç†ã€ç”µç½‘è°ƒåº¦ç­‰å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œç°æœ‰æ–¹æ³•ï¼ˆå¦‚ç»å…¸ä¼˜åŒ–æˆ–ç‹¬ç«‹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼‰éš¾ä»¥å…¼é¡¾**å…¨å±€æœ€ä¼˜æ€§**å’Œ**åˆ†é…çº¦æŸ**ï¼ˆå¦‚é¿å…ä»»åŠ¡å†²çªï¼‰ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   æå‡º**RL-Enabled Distributed Assignment (REDA)**ï¼Œç»“åˆå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸åˆ†å¸ƒå¼æœ€ä¼˜åˆ†é…æœºåˆ¶ï¼šæ™ºèƒ½ä½“å­¦ä¹ ä»»åŠ¡çš„ä»·å€¼ä¼°è®¡ï¼Œé€šè¿‡é›†ä¸­å¼å…±è¯†æœºåˆ¶ï¼ˆå¦‚åŒˆç‰™åˆ©ç®—æ³•ï¼‰ç”Ÿæˆå…¨å±€æœ€ä¼˜åˆ†é…ï¼Œè€Œéç›´æ¥é€‰æ‹©ä»»åŠ¡ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **ç†è®ºåˆ›æ–°**ï¼šè¯æ˜äº†REDAçš„Qå‡½æ•°åˆ†è§£å®šç†ï¼ˆTheorem 1ï¼‰ï¼Œç¡®ä¿æ™ºèƒ½ä½“çš„ä»·å€¼ä¼°è®¡æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£ï¼ˆLemma 1ï¼‰ã€‚\\n> *   **ç®—æ³•è®¾è®¡**ï¼šé€šè¿‡å¼•å¯¼è´ªå©ªç­–ç•¥ï¼ˆå¦‚åŒˆç‰™åˆ©ç®—æ³•ï¼‰å’Œå™ªå£°æ¢ç´¢ï¼Œè§£å†³äº†MARLåœ¨åˆ†é…é—®é¢˜ä¸­çš„è‡ªç§è¡Œä¸ºå’Œå†²çªé—®é¢˜ã€‚\\n> *   **å®è¯æ•ˆæœ**ï¼šåœ¨åŒ…å«324é¢—å«æ˜Ÿå’Œ450ä¸ªä»»åŠ¡çš„ä»¿çœŸä¸­ï¼ŒREDAæ€§èƒ½è¶…è¶ŠåŸºçº¿ï¼ˆCOMAã€IQLã€IPPOã€HAALï¼‰**20-50%**ï¼Œä¸”å®Œå…¨é¿å…äº†ä»»åŠ¡å†²çªï¼ˆå†²çªç‡0%ï¼‰ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   **æ ¸å¿ƒåŸç†**ï¼šå°†åˆ†é…é—®é¢˜å»ºæ¨¡ä¸ºéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ï¼Œæ™ºèƒ½ä½“é€šè¿‡å±€éƒ¨è§‚æµ‹å­¦ä¹ ä»»åŠ¡ä»·å€¼ï¼ˆQå‡½æ•°ï¼‰ï¼Œå†é€šè¿‡åˆ†å¸ƒå¼å…±è¯†æœºåˆ¶ï¼ˆå¦‚å¸‚åœºç«æ ‡ï¼‰ç”Ÿæˆå…¨å±€æœ€ä¼˜åˆ†é…ã€‚\\n> *   **è®¾è®¡å“²å­¦**ï¼šåˆ†ç¦»â€œå­¦ä¹ ä»·å€¼â€ä¸â€œåˆ†é…å†³ç­–â€ï¼Œæ—¢ä¿ç•™MARLçš„é€‚åº”æ€§ï¼Œåˆé€šè¿‡é›†ä¸­å¼æœºåˆ¶æ»¡è¶³ç¡¬æ€§çº¦æŸï¼ˆå¦‚å”¯ä¸€ä»»åŠ¡åˆ†é…ï¼‰ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **å…ˆå‰å±€é™**ï¼šä¼ ç»ŸMARLï¼ˆå¦‚IQLï¼‰å› ç‹¬ç«‹å†³ç­–å¯¼è‡´ä»»åŠ¡å†²çªï¼›é›†ä¸­å¼RLï¼ˆå¦‚COMAï¼‰éš¾ä»¥æ‰©å±•è‡³å¤§è§„æ¨¡é—®é¢˜ï¼›ç»å…¸ä¼˜åŒ–ï¼ˆå¦‚HAALï¼‰å¿½ç•¥çŠ¶æ€ä¾èµ–æ€§ã€‚\\n> *   **æœ¬æ–‡æ”¹è¿›**ï¼šå¼•å…¥**Î±æœºåˆ¶**ï¼ˆåŒˆç‰™åˆ©ç®—æ³•ï¼‰å°†æ™ºèƒ½ä½“çš„Qå€¼çŸ©é˜µè½¬æ¢ä¸ºåˆæ³•åˆ†é…ï¼Œé¿å…å†²çªï¼›é€šè¿‡ç†è®ºè¯æ˜ï¼ˆTheorem 1ï¼‰Qå‡½æ•°å¯åˆ†è§£ä¸ºä¸ªä½“æ™ºèƒ½ä½“çš„è´¡çŒ®ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **åˆå§‹åŒ–**ï¼šQç½‘ç»œå‚æ•°Î¸ã€ç»éªŒå›æ”¾æ± Dã€‚\\n> 2.  **æ¢ç´¢é˜¶æ®µ**ï¼šä»¥æ¦‚ç‡Îµä½¿ç”¨è´ªå©ªç­–ç•¥ï¼ˆÎ±(Î²Ì‚(s))ï¼‰ç”Ÿæˆåˆ†é…ï¼Œå…¶ä½™æ—¶é—´ç”¨å™ªå£°æ‰°åŠ¨Qå€¼çŸ©é˜µåè¾“å…¥Î±æœºåˆ¶ï¼ˆå…¬å¼4ï¼‰ã€‚\\n> 3.  **è®­ç»ƒé˜¶æ®µ**ï¼šä»å›æ”¾æ± é‡‡æ ·ï¼Œç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—Qå€¼ï¼Œé€šè¿‡Î±æœºåˆ¶ç”Ÿæˆä¸‹ä¸€çŠ¶æ€çš„æœ€ä¼˜åˆ†é…ä½œä¸ºç›®æ ‡ï¼ˆè§ä¼ªä»£ç Algorithm 1ï¼‰ã€‚\\n> 4.  **å…³é”®å…¬å¼**ï¼š\\n>     ```math\\n>     x_k = Î±(ğ_k^Ï€) â‰ˆ argmax_{x^*âˆˆX} Q^{Ï€^Î±}(s_k, x^*)\\n>     ```\\n>     å…¶ä¸­ğ_k^Ï€ä¸ºæ™ºèƒ½ä½“Qå€¼çŸ©é˜µï¼ŒÎ±ä¸ºåˆ†é…å‡½æ•°ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   **â€œç‹¬è£è€…â€å®éªŒ**ï¼š3æ™ºèƒ½ä½“3ä»»åŠ¡åœºæ™¯ä¸­ï¼ŒREDAé¿å…è´ªå©ªç­–ç•¥ï¼ˆæ™ºèƒ½ä½“1ç‹¬å é«˜å¥–åŠ±ä»»åŠ¡ï¼‰ï¼Œå®ç°å…¨å±€æœ€ä¼˜åˆ†é…ï¼ˆå›¾2ï¼‰ã€‚å…¶ä»–æ–¹æ³•ï¼ˆå¦‚IQLï¼‰å› è‡ªç§è¡Œä¸ºå¯¼è‡´ç³»ç»Ÿæ”¶ç›Šä¸‹é™50%ã€‚\\n> *   **å«æ˜Ÿæ˜Ÿåº§å®éªŒ**ï¼šREDAåœ¨èƒ½æºç®¡ç†ï¼ˆä»…5%å«æ˜Ÿè€—å°½èƒ½æºï¼‰å’Œä»»åŠ¡è¿ç»­æ€§ï¼ˆå¹³å‡è¿ç»­åˆ†é…æ—¶é—´æœ€é•¿ï¼‰ä¸Šå‡ä¼˜äºåŸºçº¿ï¼ˆå›¾4ï¼‰ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   å¤šæ™ºèƒ½ä½“RLï¼šCOMAï¼ˆé›†ä¸­å¼æ‰¹è¯„å™¨ï¼‰ã€IQLï¼ˆç‹¬ç«‹Qå­¦ä¹ ï¼‰ã€IPPOï¼ˆç‹¬ç«‹PPOï¼‰ã€‚\\n> *   ç»å…¸ä¼˜åŒ–ï¼šHAALï¼ˆå¯å‘å¼åˆ†é…ç®—æ³•ï¼‰ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨ç³»ç»Ÿæ€»æ”¶ç›Šä¸Š**ï¼šREDAåœ¨å«æ˜Ÿæ˜Ÿåº§ä»¿çœŸä¸­è¾¾åˆ°**å¹³å‡æ”¶ç›Š1200**ï¼Œæ˜¾è‘—ä¼˜äºIQLï¼ˆ800ï¼‰å’ŒIPPOï¼ˆ900ï¼‰ï¼Œæå‡50%ã€‚HAALå› å¿½ç•¥çŠ¶æ€ä¾èµ–ä»…è·600æ”¶ç›Šã€‚\\n> *   **åœ¨ä»»åŠ¡å†²çªç‡ä¸Š**ï¼šREDAå†²çªç‡ä¸º**0%**ï¼Œè€ŒIQLå’ŒIPPOå†²çªç‡é«˜è¾¾30%ã€‚\\n> *   **åœ¨èƒ½æºç®¡ç†ä¸Š**ï¼šREDAä»…5%å«æ˜Ÿè€—å°½èƒ½æºï¼ŒHAALå› å¿½ç•¥çŠ¶æ€ä¾èµ–å¯¼è‡´80%å«æ˜Ÿæå‰å¤±æ•ˆï¼ˆå›¾4ï¼‰ã€‚\\n> *   **åœ¨åˆ†é…ç¨³å®šæ€§ä¸Š**ï¼šREDAçš„å«æ˜Ÿå¹³å‡è¿ç»­æ‰§è¡ŒåŒä¸€ä»»åŠ¡**8.2ä¸ªæ—¶é—´æ­¥**ï¼Œè¿œè¶…IQLï¼ˆ3.1æ­¥ï¼‰å’ŒIPPOï¼ˆ4.5æ­¥ï¼‰ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   åºåˆ—åˆ†é…é—®é¢˜ (Sequential Assignment Problem, SAP)\\n*   å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-Agent Reinforcement Learning, MARL)\\n*   åˆ†å¸ƒå¼ä»»åŠ¡åˆ†é… (Distributed Task Allocation, N/A)\\n*   åŒˆç‰™åˆ©ç®—æ³• (Hungarian Algorithm, N/A)\\n*   å«æ˜Ÿæ˜Ÿåº§ç®¡ç† (Satellite Constellation Management, N/A)\\n*   éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (Partially Observable Markov Decision Process, POMDP)\\n*   ä»·å€¼åˆ†è§£ (Value Decomposition, N/A)\\n*   å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL)\"\n}\n```"
}