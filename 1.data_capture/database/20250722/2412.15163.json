{
    "link": "https://arxiv.org/abs/2412.15163",
    "pdf_link": "https://arxiv.org/pdf/2412.15163",
    "title": "Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents",
    "authors": [
        "Jessica Woodgate",
        "Paul Marshall",
        "Nirav Ajmeri"
    ],
    "publication_date": "2024-12-19",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 3,
    "influential_citation_count": 0,
    "paper_content": "# Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents\n\nJessica Woodgate, Paul Marshall, Nirav Ajmeri\n\nSchool of Computer Science, University of Bristol, Bristol BS8 1UB, UK jessica.woodgate@bristol.ac.uk, p.marshall $@$ bristol.ac.uk, nirav.ajmeri@bristol.ac.uk\n\n# Abstract\n\nSocial norms are standards of behaviour common in a society. However, when agents make decisions without considering how others are impacted, norms can emerge that lead to the subjugation of certain agents. We present RAWL E, a method to create ethical norm-learning agents. RAWL·E agents operationalise maximin, a fairness principle from Rawlsian ethics, in their decision-making processes to promote ethical norms by balancing societal well-being with individual goals. We evaluate RAWL E agents in simulated harvesting scenarios. We find that norms emerging in RAWL·E agent societies enhance social welfare, fairness, and robustness, and yield higher minimum experience compared to those that emerge in agent societies that do not implement Rawlsian ethics.\n\n# 1 Introduction\n\nSocial norms are standards of expected behaviour that govern a multi-agent system (MAS) and enable coordination between agents (Levy and Griffiths 2021; Wright 1963). Norms can be established through top-down prescriptions or emerge bottom-up via interactions between agents (MorrisMartin, De Vos, and Padget 2019). However, when agents are solely self-interested, norms may emerge that exploit some agents for the benefit of others. Where ethics involves one agent’s concern for another (Murukannaiah and Singh 2020), norms that result in the subjugation of agents are unethical. If agents learn norms by appealing to existing behaviours in a society without evaluating how ethical those behaviours are, they risk perpetuating unethical norms.\n\nPrevious works on promoting norms that are considerate of others, such as Tzeng et al. (2022) and Dell’Anna et al. (2020), appeal to individual or societal preferences over values. Other works observe the behaviour of others to encourage cooperation: Oldenburg and Zhi (2024) infer norms by observing apparent violations of self-interest; Guo et al. (2020) learn contextual priority of norms from observing experts; Chen et al. (2017) imply norms through reciprocity.\n\nHowever, approaches that appeal to preferences or existing behaviours to promote cooperation define ethical behaviour by reference to descriptive statements, which are statements that express what states of affairs are like (Kim,\n\nHooker, and Donaldson 2021). Attributing ethics descriptively may lead to the issue of deriving an ought from an is—just because something is the case doesn’t mean it ought to be. Where existing norms or behaviours are unethical, approaches that encourage cooperation through descriptive facts thereby risk propagating unethical norms that reflect what is the case, rather than what ought to be.\n\nTo mitigate the is-ought gap, we turn to normative ethics. Normative ethics is the study of practical means to determine the ethical acceptability of different courses of action (Woodgate and Ajmeri 2024). Normative ethics principles are justified by reason in philosophical theory. These principles are normative in that they are prescriptive, indicating how things ought to be, rather than descriptive, indicating how things are (Kim, Hooker, and Donaldson 2021).\n\nThe principle of maximin—to maximise the minimum experience—is a widely respected fairness principle in normative ethics advanced by Rawls (1958). Rawls states that in a society with unequal distribution that is not to the benefit of all, those benefiting the least should be prioritised. We hypothesise that creating agents that promote the emergence of ethical norms, while avoiding the is-ought gap, is aided by an appeal to Rawlsian ethics (Woodgate and Ajmeri 2022).\n\nContribution We propose RAWL E, a novel method to design socially intelligent norm-learning agents that consider others in individual decision making by operationalising the principle of maximin from Rawlsian ethics. A RAWL E agent includes an ethics module that applies maximin to assess the effects of its behaviour on others.\n\nNovelty Operationalising Rawlsian ethics in learning agents to enable explicit norm emergence is a novel contribution. RAWL E goes beyond existing works on normlearning agents: Ajmeri et al.’s (2020) agents incorporate ethical decision-making, but do not involve learning. Agrawal et al. (2022) address the emergence of explicit norms, but optimise norms based on the sum of payoffs received by other agents, which might be unfair for some agents. Zimmer et al. (2021) and Balakrishnan et al. (2022) operationalise Rawlsian ethics in learning agents, but do not consider the role of norms. As a RAWL E agent gains experience, it learns to achieve its goals whilst behaving in ways that support norms which prioritise those who are least advantaged in situations of unequal resource distribution.\n\nWe evaluate RAWL E agents in two simulated harvesting scenarios implemented in reinforcement learning (RL). We find that (1) RAWL E agents learn ethical norms that promote the well-being of the least advantaged agents, and (2) RAWL E agent societies yield higher social welfare, fairness, and robustness than agents who do not operationalise Rawlsian ethics in their individual decision-making.\n\nOrganisation Section 2 explores related works and gaps. Section 3 describes our method. Section 4 presents the simulation environment used to evaluate RAWL E agents. Section 5 discusses results of our simulation experiments. Section 6 concludes with a discussion of future directions.\n\n# 2 Related Works\n\nResearch on combining normative ethics with norm emergence and learning is relevant to our contributions.\n\nNormative Ethics and Norm Emergence Ethical norm emergence has been examined through the lens of agent roles. Anavankot et al. (2023) propose norm entrepreneurs that influence the dynamics of norm-following behaviours and thus the emergence of norms. Vinitsky et al. (2023) study norm emergence through sanction classification. Levy and Griffiths (2021) manipulate rewards using a central controller to enable norm emergence. Neufeld et al. (2022) use deontic logic to implement a normative supervisor module in RL agents. Yaman et al.’s (2023) agents sanction one another to encourage effective divisions of labour. Maranha˜o et al. (2022) formally reason about normative change. However, a gap remains in agents learning norms based on what ought to be the case, rather than what is the case. We address this gap by implementing principles from normative ethics to encourage the emergence of norms that can be justified independently to a specific situation.\n\nTraditional approaches encourage norm emergence by maximising social welfare—how much society as a whole gains. Shoham and Tennenholtz (1997) promote highest cumulative reward. Yu et al. (2014) utilise majority vote. Agrawal et al. (2022) sum the payoffs for different stakeholders. Focusing on social welfare alone may lead to situations where a minority is treated unfairly for the greater good (Anderson, Anderson, and Armen 2004), and mutual reward does not specify how to coordinate fairly (Grupen, Selman, and Lee 2022). To mitigate weaknesses associated with only maximising social welfare, we implement Rawlsian ethics, emphasising improving the minimum experience.\n\nNormative Ethics and Learning Jing and Doorn (2020) emphasise the importance of focusing on positive standards alongside preventative ethics, which involves negative rules denoting wrongdoing. As ethics is dynamic, it may not always be possible to determine which behaviours to restrict.\n\nSvegliato et al. (2021) implement divine command theory, prima facie duties, and virtue ethics; Nashed et al. (2021) implement the veil of ignorance, golden rule and act utilitarianism. Dong et al. (2024) optimise federated policies under utilitarian and egalitarian criteria. A gap exists, however, in applying normative ethics in RL to norm emergence. RAWL E addresses that gap.\n\n# 3 Method\n\nWe now present our method to design RAWL E agents that operationalise Rawlsian ethics to support the emergence of ethical norms.\n\n# 3.1 Schematic\n\nDefinition 1. Environment $\\mathbb { E }$ is a tuple $\\langle A G , D , { \\mathcal { N } } \\rangle$ where, $A G = \\{ a g _ { 1 } , . . . , a g _ { n } \\}$ is a set of agents; $D$ is the amount of total resources; $\\mathcal { N }$ is the set of norms.\n\nDefinition 2. A RAWL E agent is a tuple : $\\langle d , \\upsilon , G , A , Z , \\mathsf { N M } , \\mathsf { E M } , \\mathsf { I M } \\rangle$ where, $d \\in D$ is the amount of resources to which the agent has access; $\\upsilon$ is a measure of its well-being; $G$ is the set of goals $g _ { 1 } , . . . , g _ { l } ; A$ are the actions available to the agent to help achieve its goals; $Z$ are the behaviours that the agent has learned; NM is its norms module; EM is its ethics module; and IM is its interaction module.\n\nDefinition 3. A goal $g \\in G$ is a set of favourable states an agent aims to achieve.\n\nDefinition 4. A behaviour $\\zeta \\in Z$ is a tuple pre, act , where pre $\\in$ Expr is its precondition; act $\\in$ Expr is its action; and Expr is any logical expression that can be evaluated as either true or false based on the values of its variables.\n\nA behaviour has a precondition denoting the conditions within which the behaviour arises, and a postcondition, which is the action implied by the precondition. Each agent keeps a record of their learnt behaviours.\n\nA behaviour is encoded in the form of an if-then rule <behaviour> :: $\\ O =$ IF <pre> THEN <act>\n\nDefinition 5. A norm $n \\in \\mathcal N$ , where ${ \\mathcal { N } } \\subseteq Z$ , is a behaviour adopted by a society.\n\nNorms are the prescription and proscription of agent behaviour on a societal level (Savarimuthu et al. 2013).\n\nDefinition 6. $\\mathcal { N }$ , where ${ \\mathcal { N } } \\subseteq Z$ , denotes the set of emerged norms, i.e., the behaviours adopted by the society as norms, which form a normative system describing a society.\n\nNorms emerge when the same behaviours are adopted by other agents (Tuomela 1995). Norm emergence is accepted to have happened when a predetermined percentage of the population adopt the same behaviours. As following previous literature, we assume a norm to have emerged when it reaches $90 \\%$ convergence (Kittock 1995).\n\n# Definition 7. A sanction $F$ represents a positive or negative reaction to behaviour which provides feedback to the learner in the form of a reward.\n\nSanctions are positive or negative reactions to behaviour which help enforce norms. A self-directed sanction is a sanction directed towards and affecting only its sender (Nardin et al. 2016). The self-directed sanction provides feedback to the learner as a reward.\n\n# 3.2 Interaction and Norm Learning\n\nTo make decisions and pursue their goals, a RAWL E agent uses ethics module, norms module, and interaction module.\n\n<html><body><table><tr><td>Algorithm1:Ethicsmodule.</td></tr><tr><td>Input: Ut,Ut+1</td></tr><tr><td>Output: Ft+1</td></tr><tr><td>1:Umint ← getMinExperience(Ut)</td></tr><tr><td>2: Umint+1 ← getMinExperience(Ut+1)</td></tr><tr><td>3: if Umint+1>Umint then</td></tr><tr><td>4: Ft+1=s</td></tr><tr><td>5: else if Umint+1 == Umint then 6: Ft+1=0</td></tr><tr><td>7: else</td></tr><tr><td>8: Ft+1=-</td></tr><tr><td>9: end if</td></tr><tr><td>10: return Ft+1</td></tr></table></body></html>\n\nEthics Module Ethics module, EM, assesses how actions affect the well-being of other agents. To evaluate the well-being of others, RAWL E agents implement Rawlsian ethics. Adapted from Leben (2020), an ethical utility function $u ( d ) \\ \\stackrel { \\cdot } { { \\to } } \\ ( \\upsilon )$ models a distribution of resources, where $d$ is a vector of resource distribution that sums to $D$ , the amount of total resources, and $( \\upsilon )$ is a measurement of well-being for agents considering that resource distribution. Where $w$ is a vector of inputs (e.g., observed well-being of agents), Rawlsian ethics is expressed as:\n\n$$\nM A ( d ) = m i n _ { w } \\quad u ( d , \\upsilon _ { i } )\n$$\n\nVia $M A ( d )$ , the ethics module evaluates whether the agent’s action improves the minimum experience. It generates a positive self-directed sanction $\\xi$ if an action improves the minimum experience, and a neutral or negative sanction $- \\xi$ if it does not change or worsens. To implement $M A$ , ethics module takes as input $U _ { t }$ and $U _ { t + 1 }$ , where $U$ is a vector of well-being $\\boldsymbol { v } _ { 1 } , \\ldots , \\boldsymbol { v } _ { n }$ for all agents $a g _ { 1 } , \\ldots , a g _ { n }$ at times $t$ and $t + 1$ . Ethics module identifies the minimum experience $m i n _ { w } u ( d , \\upsilon )$ at $t$ and $t + 1$ , storing the results in $v _ { \\operatorname* { m i n } t }$ and ${ { \\upsilon } _ { \\operatorname* { m i n } { { t } } + 1 } }$ , respectively. Therefore:\n\n$$\nF _ { t + 1 } ( s _ { t } , s _ { t + 1 } ) = { \\left\\{ \\begin{array} { l l } { \\xi , \\quad } & { { \\mathrm { i f } } \\ v _ { \\mathrm { m i n } _ { t } } < \\upsilon _ { \\mathrm { m i n } _ { t + 1 } } } \\\\ { 0 , \\quad } & { \\ v _ { \\mathrm { m i n } _ { t } } = \\upsilon _ { \\mathrm { m i n } _ { t + 1 } } } \\\\ { - \\xi \\quad } & { \\ v _ { \\mathrm { m i n } _ { t } } > \\upsilon _ { \\mathrm { m i n } _ { t + 1 } } } \\end{array} \\right. }\n$$\n\nAlgorithm 1 describes internals of the ethics module. The inputs are $U _ { t }$ and $U _ { t + 1 }$ . To implement $M A$ , store $v _ { \\operatorname* { m i n } _ { t } }$ and $v _ { \\operatorname* { m i n } { t + 1 } }$ (lines 1–2). Compare ${ v _ { \\operatorname* { m i n } } } _ { t }$ and $v _ { \\operatorname* { m i n } { t + 1 } }$ to assess how action $a$ taken in $s _ { t }$ affected $v _ { \\operatorname* { m i n } _ { t + 1 } }$ (Line 3). Generate sanction $F _ { t + 1 }$ (Lines 4–7). Output $\\dot { F } _ { t + 1 }$ for interaction model to combine with environmental reward $r _ { t + 1 }$ through reward shaping so that $r _ { t + 1 } ^ { \\prime } = r _ { t + 1 } + F _ { t + 1 }$ . (Line 8).\n\nNorms Module Norms module, NM, tracks patterns of behaviour the agent learns. Norms module stores behaviours in a behaviour base and norms in a norm base. For each behaviour, it computes and stores the numerosity num, obtained from the number of times the behaviour is used, and the reward $r _ { t + 1 } ^ { \\prime }$ (described in interaction module) received from using the behaviour. The fitness of each behaviour $\\tau$ is obtained from $\\mathrm { { n u m } } \\cdot \\boldsymbol { r } _ { t + 1 } ^ { \\prime }$ decayed over time. Where $\\eta$ is the age of the behaviour and $\\lambda$ is the decay rate,\n\nAlgorithm 2: Norms module.   \n\n<html><body><table><tr><td>Input:Vt,at</td></tr><tr><td>1: ←behaviourBase.retrieve(Vt, at)</td></tr><tr><td>2:if ζ!= None then</td></tr><tr><td>3: behaviourBase.updateFitness(S)</td></tr><tr><td>4:else</td></tr><tr><td>5: S ←behaviourLearner.create(Vt,at)</td></tr><tr><td>6: behaviourBase.add(S) 7:end if</td></tr><tr><td>8:if t % clipNorm is O and len(behaviourBase) > maxLen</td></tr><tr><td>then</td></tr><tr><td>9: behaviourBase.clip(</td></tr><tr><td>10:end if</td></tr><tr><td>11:normBase.updateEmergedNorms(behaviourBase)</td></tr></table></body></html>\n\n$$\n\\tau ( \\zeta ) = \\mathrm { n u m } \\cdot r _ { t + 1 } ^ { \\prime } \\cdot \\lambda ^ { \\eta }\n$$\n\nAlgorithm 2 describes the internals of the norm module. Inputs to the norm module include $\\nu _ { t } , a _ { t } , r _ { t + 1 } ^ { \\prime }$ , where $\\nu _ { t }$ is the precondition obtained from the agent’s view of state $s _ { t }$ (for scalability, $\\nu _ { t }$ is a subset of $s _ { t }$ ); $a _ { t }$ is the action taken in $s _ { t }$ . Norms module searches the behaviour base to retrieve a behaviour matching $\\langle \\mathsf { p r e } , \\mathsf { a c t } \\rangle$ to $\\nu _ { t } , a _ { t }$ (line 1). If there is a matching behaviour, update $\\tau ( \\zeta )$ (lines 2–3). If there is no match, behaviour learner creates a new behaviour with $\\nu _ { t } , a _ { t }$ and adds it to behaviour base (lines 5–6). Every $t _ { \\mathrm { c l i p . } }$ behaviours steps, if behaviour base exceeds the maximum capacity, behaviour base is clipped to the maximum capacity by removing the least fit behaviours (lines 8–9). Norms module compares behaviour base with norm base shared by the society and stores emerged norms in norm base (line 10).\n\nInteraction Module Interaction module, IM, implements RL with deep Q network (DQN) architecture (Sutton and Barto 2018). Via DQN, RAWL E agent learns a behaviour policy to achieve goals while promoting ethical norms. At each time step $t$ , agent selects a batch of $B$ random experiences to train its $\\mathrm { \\Delta Q }$ network against its target network, computing the Huber loss (Huber 1964). To prevent overfitting, every $C$ steps weights of target network are updated to weights of the $\\mathrm { \\Delta Q }$ network $\\theta$ . At each step, agent receives an observation of the environment, a vector of features $x ( s )$ visible in state $s$ , which it stores in the experience replay buffer. Each feature of $x ( s )$ coresponds to a feature in the agent’s DQN. With probability $\\epsilon$ , agent selects an action randomly or using DQN. Using DQN, actions $a \\in A$ are selected that policy $\\pi ( s )$ estimates will maximise expected return and help achieve goals $G$ . Agent acts asynchronously and receives a reward from its environment $r _ { t + 1 }$ . IM obtains shaped reward $F _ { t + 1 }$ from EM. To encourage an agent to learn behaviours which promote ethical norms whilst pursuing goals, IM combines self-directed sanction $F _ { t + 1 }$ with environmental reward $r _ { t + 1 }$ through reward shaping so that $r _ { t + 1 } ^ { \\prime } = r _ { t + 1 } + F _ { t + 1 }$ . Transition $( \\bar { a } _ { t } , s _ { t } , s _ { t + 1 } , r _ { t + 1 } ^ { \\prime } )$ is stored in experience replay buffer. IM obtains view $\\nu _ { t }$ from state $s _ { t }$ and passes $\\nu _ { t }$ to NM for norm learning.\n\n# Algorithm 3: Interaction module.\n\n# Input: $s _ { t }$\n\n1: $a _ { t } \\gets \\pi ( s _ { t } ) / { * }$ Obtain action from policy \\*/   \n2: $r _ { t + 1 }$ , $s _ { t + 1 } ~  ~ \\mathsf { a c t } ( a _ { t } )$ /\\* Perform action, observe   \n$r _ { t + 1 } , s _ { t _ { + } 1 } \\ast /$   \n3: $U _ { t } \\gets \\mathrm { g e t W e l l b e i n g } ( s _ { t } ) / { * }$ Obtain well-being \\*/   \n4: $U _ { t + 1 } \\gets \\mathrm { g e t W e l l b e i n g } ( s _ { t + 1 } )$   \n5: $F _ { t + 1 } \\gets$ EthicsModule $( U _ { t } , U _ { t + 1 } )$ /\\* Obtain sanction \\*/   \n6: $r _ { t + 1 } ^ { \\prime } \\gets r _ { t + 1 } + F _ { t + 1 } / { \\ast }$ \\* Shape reward \\*/   \n7: $\\Pi ( \\theta , a ) $ update(Π, $s _ { t }$ , $r _ { t + 1 } ^ { \\prime }$ , $s _ { t + 1 } .$ ) $/ ^ { * }$ Update policy   \n\\*/   \n8: $\\nu _ { t } \\gets \\mathrm { g e t V i e w } ( s _ { t } ) / { * }$ Obtain view of $s _ { t }$ \\*/   \n9: NormsModule $( \\nu _ { t } , a _ { t } , r _ { t + 1 } ^ { \\prime } ) / { } ^ { * }$ Update norms module \\*/\n\nAlgorithm 3 outlines the interaction module. Input environmental observation at $s _ { t }$ , which includes environment state, agent’s resources $d$ , and well-being $\\boldsymbol { v } _ { 1 } , \\ldots , \\boldsymbol { v } _ { n }$ of all agents $a g _ { 1 } , \\ldots , a g _ { n }$ . Deterministic policy $\\Pi ( \\theta , a )$ defines the agent’s behaviour in $s _ { t }$ to output action $a _ { t }$ (Line 1). After acting, observe $r _ { t + 1 }$ , $s _ { t + 1 }$ (Line 2); obtain well-being vectors $U _ { t }$ and $U _ { t + 1 }$ with $v _ { 1 } , \\ldots , v _ { n }$ obtained from $s _ { t }$ and $s _ { t + 1 }$ (Lines 3–4); pass $U _ { t }$ and $U _ { t + 1 }$ to EM to obtain $F _ { t + 1 }$ (Line 5); obtain $r _ { t + 1 } ^ { \\prime }$ from $r _ { t + 1 }$ and $F _ { t + 1 }$ (Line 6); update $\\Pi ( \\theta , a )$ (Line 7); obtain $\\nu _ { t }$ from $s _ { t }$ (Line 8); pass $\\nu _ { t }$ to NM to learn and store behaviours and norms (Line 9).\n\n# 4 Simulation Environment\n\nWe evaluate RAWL E agents in a simulated harvesting scenario where they forage for berries. Cooperative behaviours may emerge, such as agents learning to throw berries to one another. To demonstrate the efficacy of modular ethical analysis, the scenario includes environmental rewards for cooperation. Figure 1 shows our harvesting environment.\n\n# 4.1 Scenario\n\nThe environment represents a cooperative multi-agent scenario with a finite population of agents on a $o \\times p$ grid. Time is represented in steps. At the beginning of each episode, the grid is initialised with $k = 4$ agents, and $b _ { \\mathrm { i n i t i a l } } = 1 2$ berries at random locations. An agent begins with $h _ { \\mathrm { i n i t i a l } } ~ = ~ 5 . 0$ health. Agents may collect berries, throw berries to other agents, or eat berries. An agent receives a gain in health $h _ { \\mathrm { g a i n } } ~ = ~ 0 . 1$ when it eats a berry. Agent health decays $h _ { \\mathrm { d e c a y } } ~ = ~ - 0 . 0 1$ at every time step. An agent dies if its health level reaches 0 and episode ends when all agents have died. Complete list of parameters are in the full version of this paper (Woodgate, Marshall, and Ajmeri 2024b, Appendix A.2).\n\nAgents act asynchronously, in a different random order on each step of the simulation. At each step, each agent $a g _ { i }$ decides to move (north, east, south, west), eat a berry, or throw a berry to another agent $a g _ { j }$ if $a g _ { i }$ has at least $h _ { \\mathrm { t h r o w } } = 0 . 6$ health. When an agent has eaten a berry, a berry regrows at a random location on the grid. At each step, an agent forages for a berry in its location. An agent observes its health, its berries, distance to the nearest berry, and each agent’s wellbeing. Well-being is represented by a function of an agent’s health and number of berries it has in its bag:\n\nTable 1: Norm parameters.   \n\n<html><body><table><tr><td>Parameter</td><td>Description</td><td>Value</td></tr><tr><td>tclip_behaviours</td><td>Clip behaviour base frequency</td><td>10.0</td></tr><tr><td>tclip_norms</td><td>Clip norm base frequency</td><td>5.0</td></tr></table></body></html>\n\n$$\na g _ { \\mathrm { w e l l - b e i n g } } = \\frac { a g _ { \\mathrm { h e a l t h } } + ( a g _ { \\mathrm { b e r r i e s } } \\times h _ { \\mathrm { g a i n } } ) } { h _ { \\mathrm { d e c a y } } }\n$$\n\nFor each agent, at each time step:\n\n(1) Receive observation $s _ { t }$   \n(2) Choose $a$ using DQN: move (north, south, east, west), eat, throw   \n(3) Forage for berry; update health $\\boldsymbol { h } _ { \\mathrm { d e c a y } }$ at each step, $h _ { \\mathrm { g a i n } }$ if berry eaten)   \n(4) Receive transition: $r _ { t + 1 } , s _ { t + 1 }$ , check if done   \n(5) Pass transition to Q network to learn   \n(6) Every $C$ steps, update $\\theta$ of target network   \n(7) Pass transition to norms module, update norm base   \n(8) Check health, if agent has died remove from the grid\n\nFor testing, we run each simulation $\\textit { e } \\ = \\ 2 0 0 0$ times, with each simulation running until all agents have died, or a maximum of $t _ { \\mathrm { m a x } } ~ = ~ 5 0$ steps. We select these numbers empirically. Agents clip behaviour every tclip behaviours steps, clip norm base every $t _ { \\mathrm { c l i p \\_ n o r m s } }$ steps, and check for emerged norms every step. Table 1 lists the norm parameters.\n\n# 4.2 Society Types for Evaluation\n\nWe implement two types of agent societies for evaluation.\n\nBaseline Cooperative: DQN A society consists of standard DQN agents that do not implement an ethics module but receive environmental rewards for cooperative behaviour. DQN agent makes decisions according to its observations and expected reward.\n\nRAWL E: Rawlsian DQN A society of RAWL E agents act in ways that promote Rawlsian ethics. RAWL E agent makes decisions according to its observations and expected reward, considering the well-being of all agents.\n\n# 4.3 Environmental Rewards\n\nAn agent receives a positive reward if it forages for a berry in a location where a berry is growing, if it eats a berry when it has berries in its bag, or if it survives to the end of the episode. An agent receives a negative reward if it attempts to eat or throw a berry to another agent when it doesn’t have any, or if it dies. Agent deaths are included in raw rewards to provide incentives for societies to survive.\n\nSelf-directed sanction of a RAWL E agent is 0.4 if the minimum experience was improved, $- 0 . 4$ if the agent could have improved the minimum experience but did not (i.e., if an action was available to improve the minimum experience but the agent chose an alternative action), and 0 otherwise.\n\nTo avoid obvious results by giving RAWL E agents additional rewards, we normalise rewards between baseline and RAWL E agents such that RAWL E agents receive lower (a) Capabilities harvest. Agents move freely but can only harvest certain berries. Some berries are on the ground, only visible by short agents. Others are in trees, only visible by tall agents. Agents can learn to throw berries to one another across the grid.\n\n![](images/1fa68eb34bf4d09674fee28b209135b8e91f3e7790d1b88939a703761ce96310.jpg)\n\n![](images/d94ce205923d450f7fbe2571fba12dbeccd70b8ff3d94fd2fce6493af3ed52e4.jpg)\n\n(b) Allotment harvest. Agents are assigned a certain allotment in a community garden. Agents can only harvest berries within their allotment. Each allotment has a different amount of berries that grow there. Agents can learn to throw berries to agents in other allotments.\n\nFigure 1: Harvesting environment. (a) Capabilities harvest scenario explores how agents learn to identify and reach desired berries while considering the well-being of the society. (b) Allotment harvest scenario explores how agents learn to harvest within their desired areas while considering the well-being in the society.   \nTable 2: Rewards received by an agent. To avoid obvious results by giving RAWL E agents more rewards, we normalise rewards between baseline and RAWL E agents.   \n\n<html><body><table><tr><td>Action</td><td>Baseline</td><td>RAWL·E</td></tr><tr><td>Survive episode</td><td>1.0</td><td>1.0</td></tr><tr><td>Eat berry</td><td>1.0</td><td>0.8</td></tr><tr><td>Forage where berry is</td><td>1.0</td><td>0.8</td></tr><tr><td>Throw berry to others</td><td>0.5</td><td>0.5</td></tr><tr><td>Die</td><td>-1.0</td><td>-1.0</td></tr><tr><td>Improve minimum experience</td><td>0.0</td><td>0.4</td></tr><tr><td>Did not improve minimum experience</td><td>0.0</td><td>-0.4</td></tr></table></body></html>\n\nraw rewards. This allows for fairer comparison between societies. Table 2 summarises rewards an agent receives.\n\n# 4.4 Metrics and Hypotheses\n\nEmerged norms $\\mathcal { N }$ describe the standards of expected behaviour in a society. To evaluate $\\mathcal { N }$ , we examine cooperative norms which emerge by their fitness and numerosity. We assess the effects of those norms on societal outcomes with the following metrics and hypotheses.\n\nVariables To quantitatively assess societal outcomes, for each simulation run, we record the following variables: $\\bf V _ { 1 }$ (agwell-being) Number of days an agent has left to live, a function of number of berries an agent carries and their current health (Equation 4). $\\mathbf { V } _ { 2 }$ (agresource) Number of berries eaten by an agent.\n\nMetrics To assess fairness on an individual and at societal level, we compute the metrics $\\mathbf { M } _ { 1 }$ (inequality) and $\\mathbf { M } _ { 2 }$ (minimum experience) on each variable.   \n${ \\bf { M } } _ { 1 }$ (inequality) Gini index across the society. Lower is better. 0 denotes perfect equality; 1 denotes perfect inequality. $\\mathbf { M } _ { 2 }$ (minimum experience) Lowest individual experience across the society. Higher is better.\n\nTo assess the sustainability of the society, we compute the metrics $\\mathbf { M } _ { 3 }$ (social welfare) and $\\mathbf { M } _ { 4 }$ (robustness).\n\n${ { \\bf { M } } _ { 3 } }$ (social welfare) How much society as a whole gains (Mashayekhi et al. 2022). Higher is better.\n\n$\\mathbf { M _ { 4 } }$ (robustness) Length of episode. Higher is better.\n\nHypotheses We evaluate the following hypotheses. Null hypotheses for each indicate no difference.   \n$\\mathbf { H _ { 1 } }$ (inequality) Norms emerging in RAWL E society lead to lower inequality.   \n$\\mathbf { H } _ { 2 }$ (minimum experience) Norms emerging in RAWL E society lead to higher minimum individual experience. $\\mathbf { H } _ { 3 }$ (social welfare) Norms emerging in RAWL E society lead to higher social welfare.   \n$\\mathbf { H _ { 4 } }$ (robustness) Norms emerging in RAWL E society lead to higher robustness.\n\nFor each hypotheses, we test the significance and compute effect size. For significance, we conduct Mann-Whitney U test which is a non-parametric test for comparing two independent groups (Mann and Whitney 1947). We use Mann-Whitney U because the sample size $k$ is small. $p <$ 0.01 indicates significance. For effect size, we compute Cohen’s d which assesses the magnitude of difference between means, standardised by the pooled standard deviation (Cohen 1988), calculated as $\\frac { \\bar { x } _ { 1 } ^ { \\ \\dotsc } - \\bar { x } _ { 2 } } { s _ { p o o l e d } }$ , where $< 0 . 2$ (negligible), [0.2,0.5) (small), [0.5,0.8) (medium), and ${ \\ge } 0 . 8$ (large).\n\n# 5 Experimental Results\n\nTo evaluate the behaviour of RAWL E agents, we run agents in two experiment scenarios with different demonstrations of unequal resource allocation. For testing, we run $e = 2 0 0 0$ episodes, with each episode running until $t _ { \\mathrm { m a x } } = 5 0$ , or until the agents die. For qualitative analysis, we examine the emerged norms and actions promoted. For quantitative analysis, we examine fairness and sustainability metrics.\n\n# 5.1 Emerged Norms\n\nRAWL E agent’s norms model learns emerging norms from patterns of behaviour. At each step, agents compare behaviour bases and store norms repeated by $90 \\%$ of agents in shared norm base $\\mathcal { N }$ . To evaluate these norms, we run $e$ episodes for each society and store $\\mathcal { N }$ from each episode.\n\nTable 3: Comparing $a g _ { \\mathrm { r e s o u r c e } }$ , inequality, minimum experience, and robustness of baseline and RAWL E societies in allotment harvest scenario. Grey highlight indicates best results with significance at $p < 0 . 0 1$ .   \n\n<html><body><table><tr><td rowspan=\"2\">Metrics</td><td rowspan=\"2\">Variable</td><td colspan=\"2\">Mean x</td><td rowspan=\"2\">Cohen's d</td></tr><tr><td>Baseline</td><td>RAWL·E</td></tr><tr><td rowspan=\"2\">M1 Inequality</td><td>a9well-being</td><td>0.24</td><td>0.10</td><td>1.58</td></tr><tr><td>agresource</td><td></td><td></td><td></td></tr><tr><td rowspan=\"2\">Minimum experience M2</td><td>agwell-being agresource</td><td>7.18</td><td>10.82</td><td>3.09</td></tr><tr><td></td><td>3.79</td><td>4.50</td><td>0.27</td></tr><tr><td rowspan=\"2\">M3 Soeciare</td><td>agwelubcing</td><td>51.50</td><td>59.80</td><td>0.14</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td colspan=\"2\">M4 Robustness</td><td>47.36</td><td>48.19</td><td>0.11</td></tr></table></body></html>\n\nWe observe that in both harvest scenarios, RAWL E agents learn more cooperative norms of throwing berries than the baseline society, such as:\n\nTo evaluate $\\mathcal { N }$ over $e$ episodes, we examine the numerosity num obtained from the times the norm is used, and fitness $\\tau$ (Equation 3) of cooperative norms. We find that RAWL E agents learn cooperative norms with higher fitness and use cooperative norms more, indicated by higher numerosity. Details of the emerged norms are in the full version of this paper (Woodgate, Marshall, and Ajmeri 2024b).\n\n# 5.2 Simulation\n\nTo quantitatively assess how ethical the normative system is, we analyse fairness and sustainability metrics of social welfare, inequality, minimum experience, and robustness. Table 3 summarises results for the allotment harvest; full version of this paper (Woodgate, Marshall, and Ajmeri 2024b, Appendix E) includes additional results. We find that the results are consistent across both scenarios with method agent societies having higher social welfare, lower inequality, higher minimum experience, and higher robustness.\n\n$\\mathbf { H _ { 1 } }$ (inequality) We find that RAWL E societies have lower inequality, indicated by a lower Gini index, in both scenarios. Inequality is especially apparent in the allotment harvest for agwell-being, where $\\bar { x } = 0 . 2$ for the baseline society and $\\bar { x } = 0 . 1$ for RAWL E. We reject the null hypothesis corresponding to $\\mathrm { H } _ { 1 }$ as $p ~ < ~ 0 . 0 1$ for agwell-being and agresource; the effect is large (1.58 for agwell-being; 1.32 for agresource). Figure 2 compares Gini index for each society.\n\n$\\mathbf { H } _ { 2 }$ (minimum experience) RAWL E societies have higher minimum individual experience than baseline agents in both scenarios. The largest effect (3.09) is on agwell-being in the allotment harvest, with $\\bar { x } = 1 0 . 8 2$ in RAWL E and ${ \\bar { x } } = 7 . 1 8$ for baseline. For $a g$ well-being, we reject the null hypothesis corresponding to $\\mathrm { H } _ { 2 }$ as $p < 0 . 0 1$ . For agresource, we cannot reject the null hypothesis in as $p > 0 . 0 1$ . Figures 3 illustrate results for each society for agwell-being.\n\n![](images/59f7a7bedbb2d21cee32f21ad3b2ce3c8cf4591ff6313243d1733fe94c8efd38.jpg)  \nFigure 2: Comparing Gini index of agwell-being and agresource for $e$ episodes. Lower Gini in RAWL E indicates lower inequality.\n\n![](images/5355d868366d8ca2d8a3dd249460d5dc18e4c975292576e9f1fc9874d131c419.jpg)  \nFigure 3: Minimum agwell-being over $t _ { \\mathrm { m a x } }$ steps summed for $e$ episodes, normalised by step frequency. RAWL E yields higher minimum well-being.\n\n$\\mathbf { H } _ { 3 }$ (social welfare) RAWL E yields higher social welfare. For $a g$ well-being, the allotment harvest yields $\\bar { x } = 5 9 . 8 0$ for RAWL E and $\\bar { x } = 2 0 . 6 0$ for baseline which has a medium effect (0.64). We reject the null hypothesis corresponding to $\\mathrm { H } _ { 3 }$ for $a g$ well-being $( p < 0 . 0 1 )$ , the difference, however, for $a g$ resource is not significant. Figure 4 displays these results.\n\n$\\mathbf { H _ { 4 } }$ (robustness) RAWL E societies survive longer $\\bar { x } =$ 48.19 in allotment) than baseline societies $\\mathit { \\bar { x } } = 4 7 . 3 6$ in allotment) indicating higher robustness. We reject the null hypothesis corresponding to $\\mathrm { H } _ { 4 }$ as $p < 0 . 0 1$ ; the effect is negligible. Figures 5a and 5b show results for each society.\n\n![](images/1a3278721c5f988b93dc06ba9f38280a3423a990b3653a6e9cb7ba03dd09eddf.jpg)  \nFigure 4: Cumulative agwell-being and agresource of each society over $t _ { \\mathrm { m a x } }$ steps summed for $e$ episodes, normalised by step frequency. Societies of RAWL E agents have higher well-being and cumulative resource consumption.\n\n![](images/99e6de14a8a7d89091be4a9f8fe85eab4589cf991d5ce255ab9dcdad6ee92cd0.jpg)  \nFigure 5: Days survived for $e$ episodes. RAWL E societies survive longer, indicating higher robustness.\n\nSummary of Findings Our results support our hypotheses. Our main findings are: in a society of RAWL E agents, (1) inequality is reduced, indicated by a lower Gini index, (2) minimum individual experience is higher than the baseline. The combination of reduced inequality and improved minimum individual experience suggests that RAWL·E societies are fairer. (3) Social welfare is improved, indicated by higher cumulative resource consumption, and (4) RAWL E societies survive longer, indicating higher robustness. Together, these results suggest RAWL E agents promote the emergence of norms that improve fairness and social welfare, thereby promoting considerate behaviour, further leading to a more sustainable society.\n\nWe observe that results are better (higher fairness, social welfare, and robustness) for RAWL E than baseline in both scenarios. However, the difference is more apparent in the allotment harvest than capabilities harvest. We attribute this difference to the fact that in the capabilities harvest agents are in a more confined space than the allotment harvest, and must navigate around one another to reach berries.\n\nThreats to Validity Threats arise from the simplicity of our scenarios. While this abstraction limits real-world applicability, our focus is on demonstrating the operationalisation of normative ethics rather than capturing realism. To address this threat, we present our agent architecture decoupled from the environment. Also, using shaped rewards to operationalise ethics offers an adaptable method compatible with various RL algorithms and diverse scenarios.\n\n# 6 Discussion and Conclusion\n\nDeveloping agents that behave in ways that promote ethical norms is crucial for ethical MAS. Operationalising principles from normative ethics in individual decision making helps address the problem of deriving an ought from an is. Our results show that, compared to societies of baseline agents who don’t implement normative ethics, RAWL E agents societies have higher social welfare, and are more fair by higher minimum experience and reduced disparity.\n\nDirections and Key Takeaways Applying normative ethics presents challenges, and there is often disagreement on the subject (Moor 2006). Conflicts may arise when different principles promote different actions (Robinson 2023). Additionally, the application of a principle may lead to unintuitive outcomes or fail to promote one action over another (Guinebert 2020). Utilising a variety of principles in reasoning is beneficial to examine scenarios from different perspectives, improving the amplitude of ethical reasoning. Directions include operationalising a variety of principles, and investigating circumstances in which principles conflict.\n\nWe utilise rewards to promote learning ethical behaviour when not all states can be known in advance. However, modifying rewards combines different objectives in a single numerical scale, allowing implicit comparisons between outcomes (Nashed, Svegliato, and Blodgett 2023). Directions include combining promotion of ethical behaviour with explicit prevention of unethical outcomes.\n\nThe scenarios we implement are abstracted to demonstrate how the method can be implemented. Operationalising normative ethics provides a mechanism to systematically assess the rightness and wrongness of actions in a range of situations (Binns 2018). Applying our method to more complex and real world scenarios, with a range of different RL algorithms, is another direction for future work.\n\nReproducibility Our codebase is publicly available (Woodgate, Marshall, and Ajmeri 2024a). The full version of this paper (Woodgate, Marshall, and Ajmeri 2024b) provides additional details including, computing infrastructure, parameter selection, a complete list of environmental rewards, further descriptions of metrics, a complete set of emerged norms, and additional details on simulation results.\n\n# Acknowledgments\n\nJW thanks EPSRC Doctoral Training Partnership Grant No. EP/W524414/1 for the support. NA thanks UKRI EPSRC Grant No. EP/Y028392/1: AI for Collective Intelligence (AI4CI) for the support.",
    "institutions": [
        "School of Computer Science",
        "University of Bristol"
    ],
    "summary": "{\n  \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决多智能体系统中，当智能体仅从自身利益出发做决策时，可能出现剥削部分智能体以造福其他智能体的不道德规范问题。同时，传统通过诉诸偏好或现有行为来促进合作的方法，存在从“是”推导“应该”的问题，可能传播不道德规范。此问题的重要性在于确保多智能体系统中的规范符合道德，促进公平和社会福利。\\n\\n**方法概述**\\n论文提出了RAWL E方法，将罗尔斯伦理学中的极大极小原则应用于智能体的决策过程，设计了包含伦理模块、规范模块和交互模块的RAWL E智能体，以促进道德规范的出现，平衡社会福祉和个体目标。\\n\\n**主要贡献与效果**\\n- 提出了新颖的设计社会智能规范学习智能体的方法RAWL E。在模拟收获场景中，RAWL E智能体社会的不平等性降低，如在分配收获场景中，`agwell - being` 的基尼系数从基线的0.24降至0.10，效果显著，差异具有统计学意义（$p < 0.01$），Cohen's d为1.58。\\n- 提高了最小个体体验，在分配收获场景中，`agwell - being` 的最小个体体验从基线的7.18提升至10.82，差异具有统计学意义（$p < 0.01$），Cohen's d为3.09。\\n- 改善了社会福利，在分配收获场景中，`agwell - being` 的社会福利从基线的51.50提升至59.80，且RAWL E智能体社会存活时间更长，鲁棒性更高，在分配收获场景中生存天数从基线的47.36提升至48.19，差异具有统计学意义（$p < 0.01$），Cohen's d为0.11。\",\n  \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n该方法的核心思想是运用罗尔斯伦理学中的极大极小原则，即最大化最小体验。在智能体的决策过程中评估其行为对其他智能体福祉的影响，通过奖励塑造机制，鼓励智能体学习促进道德规范的行为，以平衡社会福祉和个体目标，从而促进道德规范的出现。\\n\\n**创新点**\\n先前的方法通过参考偏好或现有行为来促进合作，可能导致不道德规范的传播，且在强化学习中应用规范伦理学以促进规范出现方面存在空白。与先前工作相比，RAWL E方法将罗尔斯伦理学的极大极小原则应用于智能体的决策过程，明确考虑了如何从“应该”的角度学习规范，避免了“是 - 应该”的鸿沟，并且该方法与各种强化学习算法和不同场景兼容。\\n\\n**具体实现步骤**\\n1. **定义环境和智能体**：环境是一个包含智能体集合 $AG$、总资源量 $D$ 和规范集合 $\\mathcal{N}$ 的元组 $\\langle AG, D, \\mathcal{N} \\rangle$；RAWL E智能体是一个包含资源访问量 $d$、福祉 $\\upsilon$、目标 $G$、可用行动 $A$、学习行为 $Z$、规范模块 $\\mathsf{NM}$、伦理模块 $\\mathsf{EM}$ 和交互模块 $\\mathsf{IM}$ 的元组 $\\langle d, \\upsilon, G, A, Z, \\mathsf{NM}, \\mathsf{EM}, \\mathsf{IM} \\rangle$。\\n2. **伦理模块（Ethics Module）**：输入为 $U_t$ 和 $U_{t + 1}$，分别表示 $t$ 和 $t + 1$ 时刻所有智能体的福祉向量。通过比较 $t$ 和 $t + 1$ 时刻的最小体验，生成自我制裁 $F_{t + 1}$，若 $v_{min_{t + 1}} > v_{min_t}$，则 $F_{t + 1} = \\xi$；若 $v_{min_{t + 1}} = v_{min_t}$，则 $F_{t + 1} = 0$；若 $v_{min_{t + 1}} < v_{min_t}$，则 $F_{t + 1} = - \\xi$。输出的 $F_{t + 1}$ 用于与环境奖励 $r_{t + 1}$ 进行奖励塑造，得到 $r_{t + 1}' = r_{t + 1} + F_{t + 1}$。自我-directed sanction的具体值为：若最小体验得到改善，值为0.4；若智能体本可改善最小体验但未改善，值为 - 0.4；否则为0。\\n3. **规范模块（Norms Module）**：输入包括 $\\nu_t$（从智能体对状态 $s_t$ 的视图中获得的前置条件）、$a_t$（行动）和 $r_{t + 1}'$。首先从行为库中检索匹配的行为 $\\zeta$，若存在则更新其适应度；若不存在则创建新行为并添加到行为库。每 $t_{clip\\_behaviours}=10.0$ 个行为步骤，若行为库超过最大容量，则进行裁剪。每 $t_{clip\\_norms}=5.0$ 个步骤更新规范库中的出现规范。行为的适应度 $\\tau(\\zeta) = num \\cdot r_{t + 1}' \\cdot \\lambda^{\\eta}$，其中 $num$ 是行为使用次数，$\\eta$ 是行为的年龄，$\\lambda$ 是衰减率。\\n4. **交互模块（Interaction Module）**：输入环境观察 $s_t$，通过确定性策略 $\\Pi(\\theta, a)$ 输出行动 $a_t$。行动后观察 $r_{t + 1}$ 和 $s_{t + 1}$，获取 $U_t$ 和 $U_{t + 1}$ 并传递给伦理模块得到 $F_{t + 1}$，进行奖励塑造得到 $r_{t + 1}'$，更新策略 $\\Pi(\\theta, a)$，获取 $\\nu_t$ 并传递给规范模块进行规范学习。在每个时间步 $t$，智能体选择一批随机经验训练 $\\Delta Q$ 网络，计算Huber损失，每 $C$ 步更新目标网络的权重以防止过拟合。智能体以概率 $\\epsilon$ 随机选择行动或使用DQN选择行动，所选行动需能最大化预期回报并帮助实现目标 $G$。\\n\\n**案例解析**\\n论文中以模拟收获场景为例，在一个 $o \\times p$ 网格上有有限数量的智能体，初始时网格上有 $k = 4$ 个智能体和 $b_{initial} = 12$ 个随机分布的浆果，智能体初始健康值为 $h_{initial} = 5.0$。智能体可以收集、投掷或食用浆果，食用浆果可获得 $h_{gain} = 0.1$ 的健康增益，每步健康衰减 $h_{decay} = - 0.01$。智能体根据自身观察和奖励进行决策，如当健康值至少为 $h_{throw} = 0.6$ 时可以向其他智能体投掷浆果。当智能体吃掉一个浆果后，网格上会在随机位置重新生长出一个浆果。智能体观察自身健康、浆果数量、到最近浆果的距离以及每个智能体的福祉。福祉由智能体的健康和其背包中浆果数量的函数表示。通过这个场景展示了 RAWL E 智能体如何在决策过程中考虑其他智能体的福祉，学习符合道德的规范。\",\n  \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型是基线合作（Baseline Cooperative）的标准 DQN 智能体社会，该社会中的智能体不实现伦理模块，仅根据观察和预期奖励进行决策。\\n\\n**性能对比**\\n*   **在 [不平等/Inequality] 指标上：** 本文方法在两个收获场景中均降低了不平等性。在分配收获场景中，对于 `agwell - being` 变量，本文方法的基尼系数为 **0.10**，显著低于基线模型的 **0.24**，降低了 0.14，差异具有统计学意义（$p < 0.01$），Cohen's d为1.58；对于 `agresource` 变量，也有类似效果。\\n*   **在 [最小体验/Minimum Experience] 指标上：** 本文方法在两个收获场景中均提高了最小个体体验。在分配收获场景中，对于 `agwell - being` 变量，本文方法达到了 **10.82**，远高于基线模型的 **7.18**，高出约 50.7%，差异具有统计学意义（$p < 0.01$），Cohen's d为3.09；对于 `agresource` 变量，本文方法为 **4.50**，高于基线模型的 **3.79**，提升了约 18.7%，Cohen's d为0.27。\\n*   **在 [社会福利/Social Welfare] 指标上：** 本文方法在分配收获场景中，对于 `agwell - being` 变量，社会福利达到了 **59.80**，高于基线模型的 **51.50**，提升了约 16.1%，差异具有统计学意义（$p < 0.01$），Cohen's d为0.14；但对于 `agresource` 变量，差异不显著。\\n*   **在 [鲁棒性/Robustness] 指标上：** 本文方法在分配收获场景中，存活时间达到了 **48.19**，高于基线模型的 **47.36**，有一定提升，表明鲁棒性更高，差异具有统计学意义（$p < 0.01$），Cohen's d为0.11。\",\n  \"keywords\": \"### 关键词\\n\\n- 规范学习 (Norm Learning, N/A)\\n- 罗尔斯伦理学 (Rawlsian Ethics, N/A)\\n- 极大极小原则 (Maximin Principle, N/A)\\n- 多智能体系统 (Multi - Agent System, MAS)\\n- 强化学习 (Reinforcement Learning, RL)\\n- 模拟收获场景 (Simulated Harvesting Scenario, N/A)\\n- 道德规范 (Ethical Norms, N/A)\\n- 社会福祉 (Social Welfare, N/A)\"\n}"
}