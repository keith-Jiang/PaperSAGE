{
    "link": "https://arxiv.org/abs/2501.08565",
    "pdf_link": "https://arxiv.org/pdf/2501.08565",
    "title": "DualOpt: A Dual Divide-and-Optimize Algorithm for the Large-scale Traveling Salesman Problem",
    "authors": [
        "Shipei Zhou",
        "Yuandong Ding",
        "Chi Zhang",
        "Zhiguang Cao",
        "Yan Jin"
    ],
    "publication_date": "2025-01-15",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "paper_content": "# DualOpt: A Dual Divide-and-Optimize Algorithm for the Large-scale Traveling Salesman Problem\n\nShipei Zhou1\\*, Yuandong $\\mathbf { D i n g ^ { 1 * } }$ , Chi Zhang1, Zhiguang $\\mathbf { C a o } ^ { 2 }$ , Yan ${ \\bf { J i n } } ^ { 1 \\dagger }$\n\n1School of Computer Science, Huazhong University of Science and Technology, China 2School of Computing and Information Systems, Singapore Management University, Singapore m202273616@hust.edu.cn, yuandong@hust.edu.cn, chizhang $0 2 \\textcircled { a }$ hust.edu.cn, zgcao@smu.edu.sg,jinyan $@$ mail.hust.edu.cn\n\n# Abstract\n\nThis paper proposes a dual divide-and-optimize algorithm (DualOpt) for solving the large-scale traveling salesman problem (TSP). DualOpt combines two complementary strategies to improve both solution quality and computational efficiency. The first strategy is a grid-based divide-andconquer procedure that partitions the TSP into smaller subproblems, solving them in parallel and iteratively refining the solution by merging nodes and partial routes. The process continues until only one grid remains, yielding a high-quality initial solution. The second strategy involves a path-based divide-and-optimize procedure that further optimizes the solution by dividing it into sub-paths, optimizing each using a neural solver, and merging them back to progressively improve the overall solution. Extensive experiments conducted on two groups of TSP benchmark instances, including randomly generated instances with up to 100,000 nodes and realworld datasets from TSPLIB, demonstrate the effectiveness of DualOpt. The proposed DualOpt achieves highly competitive results compared to 10 state-of-the-art algorithms in the literature. In particular, DualOpt achieves an improvement gap up to $1 . 4 0 \\%$ for the largest instance TSP100K with a remarkable $1 0 4 \\mathrm { x }$ speed-up over the leading heuristic solver LKH3. Additionally, DualOpt demonstrates strong generalization on TSPLIB benchmarks, confirming its capability to tackle diverse real-world TSP applications.\n\n# Code — https://github.com/Learning4OptimizationHUST/DualOpt\n\n# Introduction\n\nThe Traveling Salesman Problem (TSP) is an NP-hard combinatorial optimization problem, which has numerous realworld applications (Madani, Batta, and Karwan 2021; Hacizade and Kaya 2018; Matai, Singh, and Mittal 2010). Let $\\textit { G } = \\ : ( V , E )$ represent an undirected graph, where $V ~ = ~ \\{ v _ { i } ~ | ~ 1 ~ \\leq ~ i ~ \\leq ~ N \\}$ is the set of nodes and $E ~ = ~ \\{ e _ { i j } ~ \\mid ~ 1 ~ \\leq ~ i , j ~ \\leq ~ N \\}$ is the set of edges, with $N$ being the total number of nodes. For each edge $e _ { i j }$ , the travel cost $\\it { c o s t } ( i , j )$ is defined as the Euclidean distance between the nodes $v _ { i }$ and $v _ { j }$ . A special node $v _ { d } \\in V$ serves as the depot, from which the salesman starts and ends the trip. A feasible solution of TSP is defined as a Hamiltonian cycle that starts from the depot, visits each node exactly once, and ends at the depot. The objective is to minimize the total travel cost of the solution route $\\tau$ , denoted as $L ( \\tau ) = \\sum _ { i = 1 } ^ { N - 1 } c o s t ( \\tau _ { i } , \\tau _ { i + 1 } ) + c o s t ( \\tau _ { N } , \\tau _ { 1 } )$ , where $\\tau _ { i }$ represents the $i - t h$ node in the route.\n\nDue to its theoretical and practical interest, various traditional exact/heuristic and machine learning-based algorithms have been proposed in the literature. While exact algorithms are generally computationally infeasible for largescale instances, heuristics can provide near-optimal solutions for TSP with millions of cities, though they cannot guarantee optimality. They often involve time-consuming iterative searches, making them less suitable for timesensitive tasks. Machine learning based algorithms, on the other hand, offer high computational efficiency and can achieve solution quality comparable to traditional methods for small-scale TSP instances. However, applying them to large-scale TSP, especially those with over 1,000 cities, remains a challenge. One approach is to leverage pretrained models from small-scale instances for larger ones, but this often results in poor performance due to distribution shift (Joshi et al. 2022). Training models specifically for large-scale TSP is also impractical due to computational resource limitations.\n\nA basic approach to deal with the large-scale routing problem is to apply the general principle of “divide-andconquer” (Fu, Qiu, and Zha 2021; Pan et al. 2023; Hou et al. 2023; Chen et al. 2023; Ye et al. 2024b; Zheng et al. 2024; Xia et al. 2024). It involves decomposing TSP into a subset of small sub-problems, which can be efficiently solved using traditional or machine learning algorithms to attain sub-solutions. The final solution is then obtained by combining these sub-solutions. This approach can significantly reduce the computational complexity of the large-scale TSP, and yield high-quality solutions in a relatively short time.\n\nIn this work, we introduce a novel dual divide-andoptimize algorithm DualOpt based on the divide-andconquer framework. The first divide-and-optimize procedure partitions the nodes into $M \\times M$ equal-sized grids based on their coordinates. The nodes within each grid are initially solved using the well-known LKH3 solver (Helsgaun 2017). Next, an edge-breaking strategy is proposed to decompose the route into partial routes and nodes, significantly reducing the number of nodes. Subsequently, every four adjacent grids are merged into one larger grid. This process is repeated until all nodes are contained within a single grid, at which point the LKH3 solver is applied to obtain a complete route based on the partial routes and nodes. The second divide-and-optimize procedure further refines the obtained route by partitioning it into non-overlapping subpaths. These subpaths are then optimized in parallel using a neural solver (Kim, Park et al. 2021; Pan et al. 2023; Ye et al. 2024b), ultimately leading to an optimized route. The main contributions of this work are summarized as follows:\n\n• We propose the DualOpt algorithm, which combines two complementary divide-and-conquer frameworks to address the large-scale TSP. The first framework employs grid-based partitioning, while the second applies pathbased optimization, significantly improving both solution quality and computational efficiency of large-scale TSP.   \n• We introduce a novel edge-breaking strategy that decomposes routes into partial routes and nodes. By representing these partial routes solely by their start and end nodes, this strategy considerably reduces the number of nodes, thereby decreasing computational complexity and improving search efficiency.   \n• We conduct extensive experiments on both randomly generated and real-world large-scale TSP instances. To the best of our knowledge, DualOpt achieves state-ofthe-art performance compared to other machine learningbased approaches, particularly excelling in large-scale instances with up to 100,000 nodes.\n\n# Related Work\n\nHere we present representative traditional and machine learning-based algorithms to solve TSP, and then focus on the divide-and-conquer algorithms that are more effective for solving large-scale TSP over 1000 nodes.\n\nTraditional Algorithms Traditional algorithms can be roughly classified into two categories: exact and heuristic algorithms (Gutin and Punnen 2006; Accorsi and Vigo 2021). Concorde (Applegate et al. 2009) is one of the best exact solvers, which models TSP as a mixed-integer programming problem and solves it using a branch-and-cut (Toth and Vigo 2002). Exact algorithms can theoretically guarantee optimal solutions for instances of limited size, but are impractical for solving large instances due to their inherent exponential complexity. LKH3 (Helsgaun 2017) is one of the state-ofthe-art heuristics that uses the $k$ -opt operators to find neighboring solutions, which is guided by a $\\alpha$ -nearness measure based on the minimum spanning tree. The heuristics are the most widely used algorithms in practice, yet they are still time consuming to obtain high-quality solutions when solving problems with tens of thousands of nodes.\n\nMachine Learning-based Algorithms In recent years, machine learning-based algorithms have attracted more interest in solving the TSP. Depending on how solutions are constructed, they can be broadly categorized into end-to-end approaches and search-based approaches.\n\nEnd-to-end approaches generate solutions from scratch. The Attention Model (Kool, van Hoof, and Welling 2019) utilizes the Transformer architecture (Vaswani et al. 2017) and is trained with REINFORCE (Wiering and Van Otterlo 2012) using a greedy rollout baseline. POMO (Kwon et al. 2020) improves on this by selecting multiple nodes as starting points, using symmetries in solution representation, and employing a shared baseline to enhance REINFORCE training. DIFUSCO (Sun and Yang 2023) uses graph-based denoising diffusion models to generate solutions, which are further optimized by local search with $k$ -opt operators. Although DIFUSCO can handle problems with up to 10,000 nodes, it is less suitable for time-sensitive scenarios. Pointerformer (Jin et al. 2023) incorporates a reversible residual network in the encoder and a multi-pointer network in the decoder, allowing it to solve problems with up to 1000 nodes. Search-based approaches start with a feasible solution and iteratively apply predefined rules to improve it. NeuRewriter (Chen and Tian 2019) iteratively rewrites local components through a region-picking and rule-picking process, with the model trained using Advantage Actor-Critic, and the reduced cost per iteration serves as the reward. NeuroLKH (Xin et al. 2021) enhances the traditional LKH3 solver by employing a sparse graph network trained through supervised learning to simultaneously generate edge scores and node penalties, which guide the improvement process. DeepACO (Ye et al. 2024a) integrates neural enhancements into Ant Colony Optimization (ACO) algorithms, further improving its performance.\n\nMachine learning-based algorithms perform well on TSP instances with up to 1,000 nodes, but struggle with larger instances due to the exponential increase in memory requirements and computation time as the number of nodes grows. This leads to memory and time constraints during training, making it difficult to converge to near-optimal solutions. To address this challenge in solving large-scale problems with thousands or more nodes, the divide-and-conquer strategy is often employed. It is always combined with traditional or learning-based algorithms to generate high-quality solutions in a relatively short time. GCN-MCTS (Fu, Qiu, and Zha 2021) applies graph sampling to construct fixedsize sub-problems, solved by graph convolutional networks, with heatmaps guiding Monte-Carlo Tree Search (MCTS). H-TSP (Pan et al. 2023) hierarchically builds TSP solutions using a two-level policy: the upper-level selects subproblems, while the lower-level generates and merges openloop routes. ExtNCO (Chen et al. 2023) uses LocKMeans with $o ( n )$ complexity to divide nodes into sub-problems, solving them with neural combinatorial optimization and merging solutions via a minimum spanning tree. GLOP (Ye et al. 2024b) learns global partition heatmaps to decompose large-scale problems and introduces a scalable real-time solver for small Shortest Hamiltonian Path problems. UDC (Zheng et al. 2024) proposes a Divide-ConquerReunion framework using efficient Graph Neural Networks for division and fixed-length solvers for sub-problems. Soft\n\nDist (Xia et al. 2024) demonstrates that a simple baseline method outperforms complex machine learning approaches in heatmap generation, and the heatmap-guided MCTS paradigm is inferior to the LKH3 heuristic despite leveraging hand-crafted strategies.\n\n# The Proposed DualOpt Algorithm\n\nThe proposed DualOpt algorithm is based on and extends the basic divide-and-conquer method by incorporating a gridbased procedure and a path-based procedure to improve efficiency. Each procedure operates on two levels: the first level is responsible for generating sub-problems, while the second level focuses on solving these sub-problems.\n\nAlgorithm 1: The Proposed DualOpt Algorithm for   \n\n<html><body><table><tr><td>the Large-scale TSP</td></tr><tr><td>Input: TSP instance V = {U1, U2,.· , UN} Output: Solution route T 1 PartialRoutesSetY←@; 2 NodesSetN ←V ; 3 repeat 4 Grids←Partition(Y,) ;</td></tr></table></body></html>\n\nThe whole procedure of DualOpt is summarized in Algorithm 1. It starts by partitioning the TSP instance into smaller grids, which reduces the problem size and allows for parallel solving. In each iteration, the partial routes and nodes within each grid are solved in parallel to generate a solution for that portion of the TSP. Next, an edge-breaking procedure is applied to divide each route into smaller partial routes and nodes. The grids are then merged into larger ones using a grid merging procedure. This grid-based iteration continues until only one grid remains, forming a reduced problem that consists of a number of partial routes and nodes. From this reduced problem, a high-quality initial solution of is constructed. To further refine the solution, it is divided into subpaths, which are optimized in batches and then merged. This iterative refinement, performed with varying partition sizes, progressively improves the solution quality.\n\n# A Grid-based Divide-and-Conquer Procedure\n\nTo decompose the large-scale TSP for efficient solving without significantly downgrading solution quality, we employ an iterative grid decomposition strategy, denoted as the Gridbased Divide-and-Conquer Procedure, as depicted in Algorithm 2. Initially, the node set $\\aleph$ contains all the nodes of the TSP instance, while the partial route set $\\Upsilon$ is initialized as empty. In each iteration, the 2D space is discretized into an evenly spaced grid of size $K = 2 ^ { \\hat { N } _ { i t e r } - i t e r } \\times 2 ^ { N _ { i t e r } - i t e r }$ grids. The node set $\\aleph$ and partial route set $\\Upsilon$ are then divided into $K$ subsets $\\{ ( \\Upsilon _ { 1 } , \\aleph _ { 1 } ) , \\dots , ( \\Upsilon _ { K } , \\aleph _ { K } ) \\}$ based on their positions within the grid. Each of these subsets is solved in parallel using the well-known TSP solver LKH3 (Helsgaun 2017), resulting in $K$ small routes $\\{ R _ { 1 } , \\ldots , R _ { K } \\}$ . If the current iteration (iter) is not the last one, the algorithm proceeds by applying a proposed edge-breaking strategy in parallel, which breaks a subset of the edges of the routes and updates $\\Upsilon$ and $\\aleph$ with new sets of partial routes and nodes $\\{ ( \\Upsilon _ { 1 } ^ { \\prime } , \\aleph _ { 1 } ^ { \\prime } ) , . . . , ( \\Upsilon _ { K } ^ { \\prime } , \\aleph _ { K } ^ { \\prime } ) \\}$ . As the iterations progress, $K$ decreases and the grid size increases correspondingly until only one grid remains. At this final stage, a reduced problem consisting of partial routes and nodes is formed, from which an initial high-quality TSP solution $\\tau$ is obtained.\n\nAlgorithm 2: Grid-based Divide-and-Conquer   \n\n<html><body><table><tr><td></td><td>Input: TSP instance V = {U1,U2,..,UN}, number of iterations Niter Output: Solution route T</td></tr><tr><td></td><td>1 iter←1; 2 NodesSetN←V ;</td></tr><tr><td></td><td>3 PartialRoutesSet Y←D;</td></tr><tr><td></td><td>4 while iter≤Niter do</td></tr><tr><td>5</td><td>K ← 2Niter-iter × 2Niter-iter; /*Calculate the number of grids*/</td></tr><tr><td>6</td><td>{(£1,1),...,(£K,NK)}← Partition(M,,K);/*Partition nodes and</td></tr><tr><td>7</td><td>partial routes into grids*/ {R1,...,RK}←</td></tr><tr><td></td><td>SolveGridsParallel({(Mi,Ni),...,(Mk,Nk)}); /*Solve the TSP for each grid in parallel*/</td></tr><tr><td>8 9</td><td>if iter≠Niter then {(r',1),...,(2'κ,Nκ)}←</td></tr><tr><td></td><td>BreakEdgesParallel({R1,...,Rk}; /*Break edges into partial routes and nodes inparallel*/</td></tr><tr><td>10 11</td><td>Y is updated by {Y',.., Y'k} ; N is updated by {N1,...,N}；</td></tr><tr><td>12</td><td>else</td></tr><tr><td>13</td><td>T ←SolveReducedProb(M,N) ;/*Solve the</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>reduced problem to get a complete route*/</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>14</td><td>iter←iter+1;</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>15return T</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>\n\n![](images/f50a6887f92cc2a4e7b241073da148e7dc7ea9c71295bb14a9513dd975392c38.jpg)  \nFigure 1: An illustration of the Grid-based Divide-andConquer Procedure with $N _ { i t e r } = 3$ .\n\nroutes per grid, which are then decomposed into partial routes and separate nodes. In the second iteration, $K$ is reduced to 4, resulting in 4 larger grids where the LKH3 processes partial routes with fixed nodes. In the final iteration, $K$ is reduced to 1, merging the grids into a single one. As a result, the number of nodes decreases while the number of partial routes increases. This reduction in problem scale enables the LKH3 solver to efficiently find a TSP solution with high quality.\n\nThe LKH3 Solver The LKH3 solver, a state-of-the-art heuristic for the TSP, is built upon the foundational LinKernighan heuristic, incorporating several advanced components. Among these, the use of a 1-tree structure is particularly notable, as it provides a lower bound that guides the search process effectively. The algorithm dynamically employs $K$ -opt neighborhood operators, where multiple edges are iteratively removed and reconnected to explore potential routes within the solution space. The efficiency of LKH3 is further enhanced by using candidate sets, which strategically limits the number of neighborhood operators, thereby reducing search overhead.\n\nThe number of nodes within each grid, typically ranging from a few hundreds to approximately one thousand, is determined by both the instance scale and the number of grids $K$ . The input to the solver includes not only the coordinates of nodes but also the fixed partial routes. LKH3 excels in handling node scales within this range, delivering high-quality solutions with remarkable speed, particularly for routing problems involving fixed edges. Therefore, LKH3 was selected as the subsolver for this study.\n\nEdge-breaking Strategy Recall that multiple routes are generated, one for each grid within the 2D space. When merging and optimizing these routes, it is essential to reschedule the surrounding nodes of each grid in conjunction with the nodes of adjacent grids, while excluding the internal nodes from this rescheduling process. To achieve this, we propose an edge-breaking strategy that effectively reduces the problem’s scale. Specifically, for each grid, we define an internal grid that maintains a fixed spacing from the outer grid. This spacing is calculated as spacing $\\begin{array} { r } { = \\frac { x _ { \\mathrm { m a x } } - x _ { \\mathrm { m i n } } } { 2 ^ { N _ { \\mathrm { i t r } } + 2 } } } \\end{array}$ where $x _ { \\mathrm { m a x } }$ and $x _ { \\mathrm { m i n } }$ denote the maximum and minimum $x$ -coordinates of the grid, respectively. We then remove all edges that lie outside the internal grid, as well as those crossing into it. As a result, the nodes and edges within the internal grid form several partial routes that each may consist of more than two connected nodes, along with isolated nodes.\n\n# A Path-based Divide-and-Optimize Procedure\n\nBased on the solution derived from the grid-based divideand-conquer procedure, further optimization is achieved through the path-based divide-and-optimize procedure, as detailed in Algorithm 3. Following previous studies (Kim, Park et al. 2021; Ye et al. 2024b; Zheng et al. 2024), this iterative algorithm aims to refine an initial solution $\\tau$ by partitioning it into smaller sub-paths, optimizing these sub-paths individually, and then merging them to generate an improved solution $\\tau ^ { * }$ .\n\nThe procedure begins by iterating over a specified set of sub-path lengths $l e n _ { 1 } , \\ldots , l e n _ { m }$ and iterations $i t e r _ { 1 } , \\ldots , i t e r _ { m }$ . For each sub-path length len, the initial solution $\\tau$ is divided into sub-paths of the designated length. If the length of the last sub-path is shorter than the designated length, it remains unchanged. Each sub-path can be considered as an open-loop TSP with two fixed endpoints. These sub-paths undergo a distribution normalization of vertex coordinates to maintain consistency with the original TSP instance, resulting in normalized sub-paths denoted as $S u b P a t h S e t ^ { ' }$ . Following normalization, a neural solver is employed in batch mode to optimize the normalized subpaths, and sub-paths are updated by the policy when they are better than the current ones, producing a set of subpaths $S u b P a t h S e t ^ { * }$ . These sub-paths are then merged to form an improved solution $\\tau ^ { * }$ by connecting their fixed endpoints in their original order. To ensure continuous refinement and overlap during the iterations, the starting point $\\kappa$ for sub-path division is incremented by $\\begin{array} { r } { \\operatorname* { m a x } ( 1 , \\frac { l ^ { \\smile } } { i t e r } ) } \\end{array}$ . This of sub-path length and iteration count, progressively refining the manageable sub-paths of the initial solution $\\tau$ to yield a highly optimized solution $\\tau ^ { * }$ .\n\nNeural Solver The underlying solver of the path-based divide-and-optimize procedure is an attention-based neural network, which can efficiently solve the obtained sub-paths through batch parallelism.\n\nThe neural network uses an encoder-decoder architecture. The encoder employs self-attention layers to embed the input node sequence, while the decoder generates the node sequence auto-regressively. Each sub-path $\\pi$ can be regarded as an open-loop TSP with specified start and end nodes. We Park et al. 2021), defined as $h _ { ( c ) } ^ { ( L ) } \\ = \\ \\overline { { { [ \\bar { h } } ^ { ( L ) } , h _ { \\pi _ { p r e } } ^ { ( L ) } , \\bar { h } _ { \\pi _ { d e s } } ^ { ( L ) } ] } }$ Here, $h$ denotes a high-dimensional embedding vector from the encoder, and $L$ represents the number of multi-head attention layers. $\\overline { { h } } ^ { ( L ) }$ is the mean of the final node embed\n\n# Algorithm 3: Path-based Divide-and-Optimize\n\nInput: Solution $\\tau = \\{ \\tau _ { 1 } , \\tau _ { 2 } , . . . , \\tau _ { N } \\}$ , a set of sub-path lengths $\\{ l e n _ { 1 } , \\ldots , l e n _ { m } \\}$ , and a set of iterations $\\{ i t e r _ { 1 } , \\ldots , i t e r _ { m } \\}$ Output: Optimized Solution $\\tau ^ { * }$ 1 for $l e n \\gets l e n _ { 1 }$ to $\\boldsymbol { l e n } _ { m }$ do 2 $\\kappa  1$ ; 3 for $i t e r \\gets i t e r _ { 1 }$ to iter $\\cdot _ { m }$ do 4 SubP athSet $\\begin{array} { c } { { \\left\\{ \\tau _ { \\kappa : \\kappa + l e n } , \\tau _ { \\kappa + l e n : \\kappa + 2 l e n } , . . . \\right\\} ; } } \\\\ { { \\left. \\varsigma _ { a , b } D _ { a } + b \\varsigma _ { o } t ^ { \\prime } \\begin{array} { r l } \\end{array} \\right. } } \\end{array}$ 5 NormalizeV ertexCoordinate(SubP athSet); 6 SubP athSet∗ $N e u r a l S o l v e r B a t c h ( S u b P a t h S e t ^ { ' } )$ ; 7 $\\tau ^ { * } \\gets M e r g e S u b P a t h ( S u b P a t h S e t ^ { * } )$ ; 8 $\\begin{array} { r } { \\kappa  \\kappa + m a x ( 1 , \\frac { l e n } { i t e r } ) } \\end{array}$ ; 9 τ τ ∗\n\nNode Coordinate Normalization To improve the robustness of the model with respect to the sub-paths, we normalize the node coordinates to be uniformly distributed within the range $[ 0 , 1 ]$ $\\mathrm { F u }$ , Qiu, and Zha 2021). We define $x _ { m a x } ~ = ~ \\operatorname* { m a x } _ { i \\in G } x _ { i }$ , $x _ { m i n } ~ = ~ \\mathrm { m i n } _ { i \\in G } x _ { i }$ , $y _ { m a x } =$ $\\operatorname* { m a x } _ { i \\in G } y _ { i }$ , $y _ { m i n } = \\operatorname* { m i n } _ { i \\in G } y _ { i }$ as the maximum and minimum values of the horizontal and vertical coordinates of all $N$ nodes in the TSP instance. For each node in the sub-path, we convert its coordinate $( x _ { i } , y _ { i } )$ to $( x _ { i } ^ { ' } , y _ { i } ^ { ' } )$ as shown in Eq. (3), ensuring that all coordinates fall within the range $[ 0 , 1 ]$ .\n\n$$\n\\begin{array} { r } { x _ { i } ^ { ' } = \\frac { x _ { i } - x _ { \\mathrm { m i n } } } { \\operatorname* { m a x } ( x _ { \\mathrm { m a x } } - x _ { \\mathrm { m i n } } , y _ { \\mathrm { m a x } } - y _ { \\mathrm { m i n } } ) } , } \\\\ { y _ { i } ^ { ' } = \\frac { y _ { i } - y _ { \\mathrm { m i n } } } { \\operatorname* { m a x } ( x _ { \\mathrm { m a x } } - x _ { \\mathrm { m i n } } , y _ { \\mathrm { m a x } } - y _ { \\mathrm { m i n } } ) } . } \\end{array}\n$$\n\n# Experiments\n\n10 return τ ∗\n\ndings, $h _ { \\pi _ { p r e } } ^ { ( L ) }$ is the embedding of the previously selected node, and $h _ { \\pi _ { d e s } } ^ { ( L ) }$ is embedding of the end node of the subpath. Moreover, following insights from previous studies (Kim, Park et al. 2021; Cheng et al. 2023; Ye et al. 2024b), we leverage the symmetry of sub-paths. Specifically, reversing the traversal direction of a sub-path (moving backward versus forward) yields an equivalent sequence, which helps improve the network’s robustness and efficiency.\n\nThe network employs a single-step constructive policy aimed at optimizing the solution by generating sequences in both forward and backward directions. The policy is defined as follows, where $s$ represents a sub-path with a start node $\\pi _ { 1 }$ and an end node $\\pi _ { n }$ . The policy $p _ { \\theta } ( \\pi _ { f } , \\pi _ { b } | s )$ is parameterized by $\\theta$ and specifies a stochastic policy for constructing the forward solution $\\pi _ { f }$ and the backward solution $\\pi _ { b }$ :\n\n$$\n\\begin{array} { l } { { \\displaystyle p _ { \\theta } ( \\pi _ { f } , \\pi _ { b } | s ) = p _ { \\theta } ( \\pi _ { f } | s ) p _ { \\theta } ( \\pi _ { b } | s ) = } \\ ~ } \\\\ { { \\displaystyle \\prod _ { t = 1 } ^ { n - 1 } p _ { \\theta } ( \\pi _ { 1 + t } | s , \\pi _ { 1 : t } , \\pi _ { n } ) \\times \\prod _ { t = 1 } ^ { n - 2 } p _ { \\theta } ( \\pi _ { n - t } | s , \\pi _ { n : n - t + 1 } , \\pi _ { 1 } ) . } } \\end{array}\n$$\n\nThe model is trained using REINFORCE with a shared baseline. The objective is to minimize the expected length of the solutions to the sub-problems. To achieve this, the loss function is defined as the expected length of these solutions. The policy gradient is then computed as follows:\n\n$$\n\\begin{array} { r l } & { \\nabla \\mathcal { L } ( \\theta | s ) = \\mathrm { E } _ { p _ { \\theta } ( \\pi _ { f } | s ) } [ R ( \\pi _ { f } ) - b ( s ) ] \\nabla \\log p _ { \\theta } ( \\pi _ { f } | s ) } \\\\ & { \\quad \\quad \\quad + \\mathrm { E } _ { p _ { \\theta } ( \\pi _ { b } | s ) } [ R ( \\pi _ { b } ) - b ( s ) ] \\nabla \\log p _ { \\theta } ( \\pi _ { b } | s ) . } \\end{array}\n$$\n\nThe shared baseline $b ( s )$ is employed to reduce variance and enhance training stability. This baseline is computed by averaging the path lengths obtained from two greedy rollouts. Note that during model training, both forward and backward solutions are utilized. However, during inference, only the superior trajectory between the two solutions is selected to ensure the best possible outcome.\n\nTo evaluate the performance of our proposed DualOpt, we conducted extensive experiments on the large-scale TSP instances, with up to 100,000 nodes. We compared the performance of DualOpt with that of state-of-the-art algorithms from the literature. All experiments, including rerunning the baseline algorithms, were executed on a machine with an RTX 3080(10GB) GPU and a 12-core Intel(R) Xeon(R) Platinum 8255C CPU.\n\nOur evaluation focused on two groups of the largescale instances: randomly generated instances TSP random and the well-known real-world benchmark TSPLIB. The TSP random represents random Euclidean instances, with node coordinates uniformly sampled from a unit square $[ 0 , 1 ] ^ { 2 }$ . This group includes seven datasets, each corresponding to a different number of nodes, denoted as $\\tt T S P { \\cal N }$ , i.e., TSP1K (1,000 nodes), TSP2K, TSP5K, TSP10K, TSP20K, TSP50K, and TSP100K (100,000 nodes). For a fair comparison, we used the same test instances provided by Fu et al. (Fu, Qiu, and Zha 2021) and Pan et al. (Pan et al. 2023). Each dataset contains 16 instances, except TSP1K, which includes 128 instances, and TSP100K, which contains one instance. The TSPLIB is a well-known real-world benchmark (Reinelt 1991) that consists of 100 instances with diverse node distributions derived from practical applications, with sizes ranging from 14 to 85,900 nodes. For our experiments, we select the ten largest TSPLIB instances, each containing more than 5,000 nodes.\n\nBaselines We compare DualOpt against ten leading TSP algorithms in the literature. Traditional Algorithms: 1) Concorde (Cook et al. 2011): One of the best exact solvers. 2) LKH3 (Helsgaun 2017): One of the highly optimized heuristic solvers. Machine Learning Based Algorithms: 1) POMO (Kwon et al. 2020): Feature an end-to-end model based on Attention Model, comparable to LKH3 for smallscale TSP instances. 2) DIMES (Qiu, Sun, and Yang 2022): Introduce a compact continuous space for parameterizing the underlying distribution of candidate solutions for solving large-scale TSP with up to 10K nodes. 3) DIFUSO (?): Leverage a graph-based denoising diffusion model to solve large-scale TSP with up to 10K nodes. 4) SIL (Luo et al. 2024): Develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without labeled data, handling TSP instances with up to 100K nodes. Divide-and-Conquer Algorithms: 1) $\\mathbf { G C N + M C T S }$ (Fu, Qiu, and Zha 2021): Combine a GCN model trained with supervised learning and MCTS to solve large-scale TSP involving up to 10K nodes with a long searching time. 2) SoftDist (Xia et al. 2024): Critically evaluate machine learning guided heatmap generation, the heatmap-guided MCTS paradigm for large-scale TSP. 3) HTSP (Pan et al. 2023): Hierarchically construct a solution of a TSP instance with up to 10K nodes based on two-level policies. 4) GLOP (Ye et al. 2024b): Decompose large routing problems into Shortest Hamiltonian Path Problems, further improved by local policy. It is the first neural solver to effectively scale to TSP with up to 100K nodes.\n\nParameter Settings For the grid-based divide-andconquer procedure, we set the value of $N _ { i t e r }$ for different TSP problem scales, as shown in Table 1. The LKH3 solver is used with its default parameters as specified in the literature (Helsgaun 2017) both in DualOpt and baseline. LKH3 baseline runs instance-by-instance. In the grid-based divide-and-conquer stage, LKH3 sub-solver just run parallel for grids in a single instance. For the path-based divideand-optimize procedure, we train different neural solvers with graph size $l e n = \\{ 5 0 , 2 0 , 1 0 \\}$ , with node coordinates randomly generated from a uniform distribution within a unit square. During the training process, we use the same hyper-parameter settings as those used by Kool et al (Kool, van Hoof, and Welling 2019). And during test time, we set $i t e r = \\{ 2 5 , 1 0 , 5 \\}$ .\n\nTable 1: Parameter setting of $N _ { i t e r }$ .   \n\n<html><body><table><tr><td>Number of Iterations</td><td>Problem Scale of TSP</td></tr><tr><td>Niter=2</td><td>N<5,000</td></tr><tr><td>Niter =3</td><td>5,000≤N<20,000</td></tr><tr><td>Niter =4</td><td>20,000≤N<100,000</td></tr><tr><td>Niter =5</td><td>N ≥100,000</td></tr></table></body></html>\n\nComparative Studies Table 2 presents a comparison of 10 leading algorithms and our DualOpt algorithm on randomly distributed datasets TSP random. The “Obj.” column shows the average objective lengths of the routes obtained by each algorithm for each instance, while the “Gap” column measures the percentage difference between the average route lengths attained by each algorithm and the LKH3, considered as the ground truth, i.e., $G a p \\ =$ Obj. of oAbljg.o.f-LOKbjH3of LKH3 × 100%. The “Time” column indicates the average time required to solve each instance. The $\\dagger$ following the algorithm indicates that the results of this algorithm are drawn from the literature directly and “-” means the results are not provided in the literature. The “OOM” stands for out of CUDA memory with our platform.\n\nFrom Table 2, we can make the following comments about the TSP random instances. First, the heuristic solver\n\nLKH3 provides the highest quality solutions for all instances except TSP1K and TSP2K, though it requires long time searching. DualOpt matches or improves upon LKH3’s results for all instances except TSP5K, and achieves an improved result with an improvement gap up to $1 . 4 0 \\%$ for the largest instance TSP100K, with a remarkable $1 0 4 \\mathrm { x }$ speedup. Second, DualOpt clearly outperforms machine learning based algorithms in both solution quality and running time. POMO underperforms across all instances, primarily because it cannot be trained directly on large-scale instances, and models trained on small-scale instances fail to generalize effectively to large-scale instances. Compared to the current state-of-the-art SIL, our DualOpt surpasses SIL in all instances except for a tie on TSP1K. Third, DualOpt consistently outperforms all four divide-and-conquer algorithms in terms of the best objective result, achieving better results than GLOP for all instances. These observations highlight the superiority of DualOpt in both solution quality and computational efficiency.\n\nTo demonstrate the solver’s generalization capabilities, we then directly applied the trained neural solver to the real-world dataset TSPLIB with varied distributions. Table 3 presents the results for the 10 largest instances from TSPLIB. Despite the different node distributions in TSPLIB compared to those in TSP random, DualOpt demonstrates impressive generalization capabilities. It achieves highly competitive results compared to three leading large-scale TSP algorithms from the literature. This performance highlights DualOpt’s effectiveness in handling various types of TSP instances and further underscores its versatility in realworld applications.\n\nAn Ablation Study Our DualOpt integrates two important divide-and-conquer procedures to achieve superior performance. To evaluate the impact of each procedure, we conducted an ablation study with two distinct variants of DualOpt: $\\mathrm { D u a l O p t } _ { w / o P a t h }$ and $\\mathrm { D u a l O p t } _ { w / o G r i d }$ . $\\mathrm { D u a l O p t } _ { w / o P a t h }$ employs only the grid-based divide-andconquer procedure, while $\\mathrm { D u a l O p t } _ { w / o G r i d }$ generates the initial solution using a simple yet effective approach called random insertion, which is then improved using the pathbased divide-and-optimize procedure. The results of this ablation study are summarized in Table 4. The comparative results clearly demonstrate the significance of both procedures within DualOpt. Specifically, the original DualOpt consistently outperforms the variants, underscoring the importance of combining both the grid-based and path-based procedures for achieving superior performance.\n\n# Conclusion\n\nIn this paper, we introduced a novel dual divide-andoptimize algorithm DualOpt to solve large-scale traveling salesman problem, which integrates two complementary strategies: a grid-based divide-and-conquer procedure and a path-based divide-and-optimize procedure. Through extensive experiments on randomly generated instances and realworld datasets, DualOpt consistently achieves highly competitive results with state-of-the-art TSP algorithms in terms of both solution quality and computational efficiency. Moreover, DualOpt is able to generalize across different TSP distributions, making it a versatile solver for tackling diverse real-world TSP applications. The success of DualOpt paves the way for future research into more sophisticated divide-and-conquer strategies and their application to different types of other combinatorial optimization problems.\n\nTable 2: Comparative results with 10 leading algorithms on large-scale TSP random with up to 100K nodes.   \n\n<html><body><table><tr><td>Algorithm</td><td>Obj.</td><td>TSP1K Gap(%)</td><td>Time</td><td>Obj.</td><td colspan=\"2\">TSP2K Gap(%)</td><td>Time</td><td>Obj.</td><td>TSP5K Gap(%)</td><td></td><td>Time</td></tr><tr><td>LKH3 Concorde</td><td>23.31 23.12</td><td>0.00 -0.82</td><td>2.4s 7m</td><td>32.89 32.44</td><td colspan=\"2\"></td><td>9.3s 2.8h</td><td>51.52 53.45</td><td colspan=\"2\">0.00 3.75</td><td>1.3m 3.2h</td></tr><tr><td>POMOt DIMESt DIFUSCOt</td><td>30.52 23.69 23.39</td><td>30.93 1.63 0.34</td><td>4.3s 2.2m 11.5s</td><td>46.49</td><td></td><td>41.35</td><td>35.9s</td><td>80.79</td><td>56.81</td><td></td><td>9.6m</td></tr><tr><td>SILt GCN-MCTSt</td><td>23.31 23.86</td><td>0.00 2.36</td><td>0.6s 5.8s</td><td>-- 33.42</td><td></td><td>1.61</td><td>1 3.3m</td><td>51.92 52.83</td><td>0.78 2.54</td><td></td><td>28.5s 6.3m</td></tr><tr><td>SOFTDIST† H-TSP GLOP</td><td>23.63 24.66</td><td>1.37 5.79</td><td>1.57 0.7s</td><td></td><td>35.22</td><td>7.08</td><td>= 1.5s</td><td>55.72</td><td>8.15</td><td></td><td>2.3s</td></tr><tr><td>DualOpt</td><td>24.01 23.31</td><td>3.00 0.00</td><td>0.4s 5.6s</td><td></td><td>33.90 32.72</td><td>3.07 -0.52</td><td>0.9s 7.6s</td><td>53.49 51.56</td><td></td><td>3.82 0.08</td><td>1.8s 13.9s</td></tr><tr><td></td><td></td><td>TSP10K</td><td></td><td></td><td>TSP20K</td><td></td><td></td><td>TSP50K</td><td></td><td>TSP100K</td><td></td></tr><tr><td>Method</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%) Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td></tr><tr><td>LKH3</td><td>72.96</td><td>0.00</td><td>6.3m</td><td>103.28</td><td>0.00</td><td>27.4m</td><td>164.08</td><td>0.00 3.0h</td><td></td><td>234.098 0.00</td><td>14.9h</td></tr><tr><td>DIMESt</td><td>74.06</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DIFUSCOt</td><td></td><td>1.50</td><td>13m</td><td></td><td></td><td></td><td></td><td>-</td><td>1</td><td>1</td><td>1</td></tr><tr><td>SIL+</td><td>73.62</td><td>0.90</td><td>3m</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>73.32</td><td>0.49</td><td>51s</td><td></td><td></td><td></td><td>164.53 0.27</td><td>3.8m</td><td>232.66</td><td>-0.61</td><td>7.5m</td></tr><tr><td>GCN-</td><td>74.93</td><td>2.70</td><td>6.5m</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MCTS†</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SOFTDIST+</td><td>74.03</td><td>1.40</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>H-TSP</td><td>78.45</td><td>7.52</td><td>1m</td><td>110.7</td><td>7.18</td><td>10.4s</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GLOP</td><td>75.42</td><td></td><td>4.8s</td><td>106.7</td><td>3.31</td><td></td><td>OOM 168.2</td><td></td><td></td><td>OOM</td><td></td></tr><tr><td></td><td></td><td>3.37</td><td>3.5s</td><td></td><td></td><td>7.9s</td><td>2.51</td><td>12.1s</td><td>237.99</td><td>1.66</td><td>2.5m</td></tr><tr><td>DualOpt</td><td></td><td></td><td></td><td>102.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>72.62</td><td>-0.47</td><td>33.9s</td><td></td><td>-0.37</td><td></td><td>162.81 -0.77</td><td>6.5m</td><td>230.83</td><td>-1.40</td><td>8.6m</td></tr></table></body></html>\n\nTable 3: Comparative results on 10 largest instances of TSPLIB.   \n\n<html><body><table><tr><td rowspan=\"2\">Instance</td><td colspan=\"2\">LKH3</td><td colspan=\"3\">H-TSP</td><td colspan=\"3\">GLOP</td><td colspan=\"3\">DualOpt</td></tr><tr><td>Obj.</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td></tr><tr><td>rl5915</td><td>572085</td><td>9.9m</td><td>652336</td><td>14.03</td><td>9s</td><td>628928</td><td>9.94</td><td>16s</td><td>579152</td><td>1.24</td><td>14s</td></tr><tr><td>rl5934</td><td>559712</td><td>9.9m</td><td>642214</td><td>14.74</td><td>9s</td><td>616629</td><td>10.17</td><td>16s</td><td>565912</td><td>1.11</td><td>19s</td></tr><tr><td>pla7397</td><td>23382264</td><td>12.4m</td><td>25494130</td><td>9.03</td><td>12s</td><td>24990688</td><td>6.88</td><td>16s</td><td>23536550</td><td>0.66</td><td>41s</td></tr><tr><td>rl11849</td><td>929001</td><td>18.9m</td><td>1046963</td><td>11.2</td><td>17s</td><td>1006378</td><td>8.3</td><td>17s</td><td>935904</td><td>0.74</td><td>49s</td></tr><tr><td>usa13509</td><td>20133724</td><td>22.6m</td><td>21923532</td><td>8.89</td><td>20s</td><td>21023604</td><td>4.42</td><td>17s</td><td>20248476</td><td>0.57</td><td>1.3m</td></tr><tr><td>brd14051</td><td>474149</td><td>23.5m</td><td>506211</td><td>6.76</td><td>20s</td><td>491735</td><td>3.71</td><td>18s</td><td>474559</td><td>0.09</td><td>1m</td></tr><tr><td>d15112</td><td>1588550</td><td>25.2m</td><td>1696577</td><td>6.80</td><td>22s</td><td>1648777</td><td>3.79</td><td>18s</td><td>1589033</td><td>0.03</td><td>1.2m</td></tr><tr><td>d18512</td><td>652911</td><td>31m</td><td>694116</td><td>6.31</td><td>26s</td><td>676840</td><td>3.66</td><td>21s</td><td>652457</td><td>0.07</td><td>1.3m</td></tr><tr><td>pla33810</td><td>67084217</td><td>56.4m</td><td></td><td>00M</td><td></td><td>71934504</td><td>7.23</td><td>33s</td><td>68083048</td><td>1.49</td><td>1.2m</td></tr><tr><td>pla85900</td><td>144004484</td><td>6.5h</td><td></td><td>0OM</td><td></td><td>146039168</td><td>1.41</td><td>1.7m</td><td>145008800</td><td>0.70</td><td>3.5m</td></tr></table></body></html>\n\nTable 4: An ablation study of DualOpt and its two variants on TSP random   \n\n<html><body><table><tr><td></td><td></td><td colspan=\"2\">DualOpt</td><td colspan=\"3\">DualOptwloPath</td><td colspan=\"3\">DualOptw /oGrid</td></tr><tr><td>Instance</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td></tr><tr><td>TSP1K</td><td>23.31</td><td>0.00</td><td>5.6s</td><td>23.34</td><td>0.13</td><td>4.5s</td><td>24.56</td><td>5.36</td><td>5.2s</td></tr><tr><td>TSP2K</td><td>32.72</td><td>-0.52</td><td>7.6s</td><td>32.75</td><td>-0.43</td><td>6.3s</td><td>34.3</td><td>4.29</td><td>5.3s</td></tr><tr><td>TSP5K</td><td>51.56</td><td>0.08</td><td>13.9s</td><td>51.65</td><td>0.25</td><td>12.1s</td><td>54.03</td><td>4.87</td><td>7.2s</td></tr><tr><td>TSP10K</td><td>72.62</td><td>-0.47</td><td>33.9s</td><td>72.85</td><td>-0.15</td><td>31.7s</td><td>76.25</td><td>4.51</td><td>8.2s</td></tr><tr><td>TSP20K</td><td>102.9</td><td>-0.37</td><td>1m</td><td>103.4</td><td>0.12</td><td>59s</td><td>107.76</td><td>4.34</td><td>8.7s</td></tr><tr><td>TSP50K</td><td>162.81</td><td>-0.77</td><td>6.5m</td><td>164</td><td>-0.05</td><td>6.4m</td><td>170.24</td><td>3.75</td><td>31.3s</td></tr><tr><td>TSP100K</td><td>230.83</td><td>-1.40</td><td>8.6m</td><td>234.2</td><td>0.04</td><td>8.5m</td><td>240.82</td><td>2.87</td><td>2.5m</td></tr></table></body></html>",
    "institutions": [
        "School of Computer Science, Huazhong University of Science and Technology",
        "School of Computing and Information Systems, Singapore Management University",
        "Huazhong University of Science and Technology",
        "Singapore Management University"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决大规模旅行商问题（TSP），该问题是一个NP难的组合优化问题，在现实世界中有众多应用，如物流配送、电路布线等。传统精确算法对于大规模实例计算不可行，启发式算法耗时长，机器学习算法在处理大规模TSP（尤其是节点超过1000的情况）时存在挑战，因此需要一种高效的算法来解决大规模TSP。\\n\\n**方法概述**\\n论文提出了一种双分治优化算法（DualOpt），结合基于网格的分治策略和基于路径的优化策略，先将TSP划分为小问题并行求解得到初始解，再将解划分为子路径进一步优化。\\n\\n**主要贡献与效果**\\n- 提出DualOpt算法，结合两种互补的分治框架，显著提高了大规模TSP的解质量和计算效率。在随机生成的TSP100K实例上，与领先的启发式求解器LKH3相比，实现了高达1.40%的改进差距，速度提升了104倍。\\n- 引入新颖的破边（断边）策略，将路线分解为部分路线和节点，仅用部分路线的起始和结束节点表示这些部分路线，显著减少了节点数量，降低了计算复杂度，提高了搜索效率。\\n- 在随机生成和真实世界的大规模TSP实例上进行了广泛实验，DualOpt达到了最先进的性能，尤其在高达100,000个节点的大规模实例中表现出色，且在真实世界数据集TSPLIB上展示了令人印象深刻的泛化能力。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nDualOpt算法基于分治思想，将大规模TSP问题分解为多个小问题，通过逐步求解小问题并合并结果来得到最终解。基于网格的分治策略将节点按坐标划分为网格，在每个网格内求解并合并，减少问题规模；基于路径的优化策略将得到的路线划分为子路径，并行优化子路径以进一步提高解的质量。这种结合利用了两种策略的优势，有效提高了求解效率和质量。\\n\\n**创新点**\\n先前的工作在解决大规模TSP时存在计算复杂度高、难以处理大规模实例等问题。与先前工作相比，DualOpt的创新点在于：一是结合了基于网格的分治和基于路径的优化两种策略，从不同角度对问题进行处理；二是提出了破边（断边）策略，通过合理划分路线减少节点数量，降低计算复杂度；三是在路径优化时，利用子路径的对称性，提高网络的鲁棒性和效率。\\n\\n**具体实现步骤**\\n1. **基于网格的分治过程（Grid-based Divide-and-Conquer Procedure）**：\\n    - 初始化节点集 $\\\\aleph$ 包含所有节点，部分路线集 $\\\\Upsilon$ 为空。\\n    - 在每次迭代中，将二维空间离散化为 $K = 2 ^ { N _ { i t e r } - i t e r } \\\\times 2 ^ { N _ { i t e r } - i t e r }$ 个网格，将节点集和部分路线集按网格划分。这里原文中 $\\\\hat { N } _ { i t e r }$ 应为 $N _ { i t e r }$ 。\\n    - 并行使用LKH3求解器求解每个网格内的问题，得到小路线。\\n    - 如果不是最后一次迭代，应用破边（断边）策略，更新 $\\\\Upsilon$ 和 $\\\\aleph$ 。破边策略是通过定义内部网格，计算特定间距，移除内部网格外及跨越到内部网格的边，将路线分解为部分路线和节点。\\n    - 随着迭代进行，$K$ 减小，网格变大，直到只有一个网格，形成包含部分路线和节点的简化问题，求解得到初始高质量TSP解。\\n2. **基于路径的优化过程（Path-based Divide-and-Optimize Procedure）**：\\n    - 遍历指定的子路径长度 $len$ 和迭代次数 $iter$ 。\\n    - 将初始解划分为指定长度的子路径，若最后一个子路径长度不足则保持不变。对节点坐标进行归一化，使其均匀分布在 $[0, 1]$ 范围内。\\n    - 使用基于注意力的神经网络并行优化归一化后的子路径，该网络采用编码器 - 解码器架构，利用子路径对称性，采用单步构造策略，通过前向和后向生成序列来优化解。更新子路径，若优化后的子路径优于当前子路径则替换。\\n    - 合并子路径得到改进的解，为确保连续细化，子路径划分的起点递增 $\\\\max(1, \\\\frac { len } { iter })$ 。\\n\\n**案例解析**\\n论文以 $N _ { i t e r } = 3$ 为例说明基于网格的分治过程。在第一次迭代，$K = 16$，LKH3为每个网格生成路线并分解为部分路线和单独节点；第二次迭代，$K$ 减至4，LKH3处理部分路线和固定节点；最后一次迭代，$K$ 减至1，合并网格为一个，节点数量减少，部分路线数量增加，LKH3高效找到高质量TSP解。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 传统算法：Concorde、LKH3。\\n- 机器学习算法：POMO、DIMES、DIFUSCO、SIL。\\n- 分治算法：GCN - MCTS、SoftDist、H - TSP、GLOP。\\n\\n**性能对比**\\n*   **在 [目标长度/Obj.] 指标上：** 在随机分布数据集TSP random上，启发式求解器LKH3除TSP1K和TSP2K外提供了最高质量的解，DualOpt除TSP5K外与LKH3结果相当或更优，在最大实例TSP100K上比LKH3有高达1.40%的改进差距。DualOpt明显优于机器学习算法，如POMO在所有实例中表现不佳，DualOpt在除TSP1K与SIL打平外的所有实例中超越SIL。DualOpt也始终优于所有四种分治算法，在所有实例上比GLOP结果更好。在TSPLIB真实世界数据集上，DualOpt展示了出色的泛化能力，与三种领先的大规模TSP算法（LKH3、H - TSP、GLOP）相比取得了极具竞争力的结果。例如在rl5915实例上，LKH3的目标长度为572085，H - TSP为652336，GLOP为628928，DualOpt为579152，与LKH3差距仅为1.24%。\\n*   **在 [运行时间/Time] 指标上：** 在TSP random数据集上，LKH3搜索时间长，DualOpt在所有实例上比LKH3有显著的速度提升，例如在TSP100K上速度快了104倍。DualOpt也明显优于机器学习算法和分治算法，如POMO在TSP1K上用时4.3s，而DualOpt仅需0.4s；GLOP在TSP10K上耗时3.5s，DualOpt仅需33.9s。在TSPLIB数据集上，DualOpt在解决实例时所需时间也具有竞争力，如在rl5915实例上，LKH3耗时9.9m，H - TSP耗时9s，GLOP耗时16s，DualOpt仅需14s。\",\n    \"keywords\": \"### 关键词\\n\\n- 旅行商问题 (Traveling Salesman Problem, TSP)\\n- 双分治优化算法 (Dual Divide-and-Optimize Algorithm, DualOpt)\\n- 分治策略 (Divide-and-Conquer Strategy, N/A)\\n- 破边策略 (Edge-breaking Strategy, N/A)\\n- 大规模优化 (Large-scale Optimization, N/A)\"\n}"
}