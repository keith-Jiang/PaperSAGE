{
    "link": "https://arxiv.org/abs/2409.12106",
    "pdf_link": "https://arxiv.org/pdf/2409.12106",
    "title": "Measuring Human and AI Values Based on Generative Psychometrics with Large Language Models",
    "authors": [
        "Haoran Ye",
        "Yuhang Xie",
        "Yuanyi Ren",
        "Hanjun Fang",
        "Xin Zhang",
        "Guojie Song"
    ],
    "publication_date": "2024-09-18",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "paper_content": "# Measuring Human and AI Values Based on Generative Psychometrics with Large Language Models\n\nHaoran $\\mathbf { Y } \\mathbf { e } ^ { * \\mathrm { ~ 1 ~ } }$ , Yuhang Xie\\* 2, Yuanyi Ren\\* 1, Hanjun Fang3, Xin Zhang4, Guojie Song† 1 5\n\n1State Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University 2School of Software and Microelectronics, Peking University 3Department of Sociology, Peking University 4School of Psychological and Cognitive Sciences, Peking University 5PKU-Wuhan Institute for Artificial Intelligence {hrye, yuhangxie}@stu.pku.edu.cn {yyren, hjfang, zhang.x, gjsong}@pku.edu.cn\n\n# Abstract\n\nHuman values and their measurement are long-standing interdisciplinary inquiry. Recent advances in AI have sparked renewed interest in this area, with large language models (LLMs) emerging as both tools and subjects of value measurement. This work introduces Generative Psychometrics for Values (GPV), an LLM-based, data-driven value measurement paradigm, theoretically grounded in text-revealed selective perceptions. The core idea is to dynamically parse unstructured texts into perceptions akin to static stimuli in traditional psychometrics, measure the value orientations they reveal, and aggregate the results. Applying GPV to humanauthored blogs, we demonstrate its stability, validity, and superiority over prior psychological tools. Then, extending GPV to LLM value measurement, we advance the current art with 1) a psychometric methodology that measures LLM values based on their scalable and free-form outputs, enabling context-specific measurement; 2) a comparative analysis of measurement paradigms, indicating response biases of prior methods; and 3) an attempt to bridge LLM values and their safety, revealing the predictive power of different value systems and the impacts of various values on LLM safety. Through interdisciplinary efforts, we aim to leverage AI for next-generation psychometrics and psychometrics for valuealigned AI.\n\n# Code — https://github.com/Value4AI/gpv Extended version — https://arxiv.org/abs/2409.12106\n\n# 1 Introduction\n\nHuman values, a cornerstone of philosophical inquiry, are the fundamental guiding principles behind individual and collective decision-making (Rokeach 1973; Sagiv et al. 2017). Value measurement is a long-standing interdisciplinary endeavor for elucidating how specific values underpin and justify the worth of actions, objects, and concepts (Schwartz 1992; Klingefjord, Lowe, and Edelman 2024).\n\nTraditional psychometrics often measure human values through self-report questionnaires, where participants rate the importance of various values in their lives. However, these tools are limited by response biases, resource demands, inaccuracies in capturing authentic behaviors, and inability to handle historical or subjective data (Ponizovskiy et al. 2020). Therefore, data-driven tools have been developed to infer values from textual data, such as social media posts (Shen, Wilson, and Mihalcea 2019). These tools can reveal personal values without relying on explicit selfreporting, but they are mostly dictionary-based, matching text to predefined value lexicons. Consequently, they often fail to grasp the nuanced semantics and context-dependent value expressions. Additionally, these tools are inherently static and inflexible, relying on expert-defined lexicons that are not easily adaptable to new or evolving values.\n\nThe rise of large language models (LLMs), with their remarkable ability to understand semantic nuances, presents new possibilities for data-driven value measurement. Recent studies have demonstrated that LLMs can effectively approximate annotators’ and even psychologists’ judgments on value-related tasks (Sorensen et al. 2024; Ren et al. 2024). Building on these advancements, this work introduces Generative Psychometrics for Values (GPV), an LLMbased, data-driven value measurement paradigm grounded in the theory of text-revealed selective perceptions (Postman, Bruner, and McGinnies 1948; Shen, Wilson, and Mihalcea 2019). Perceptions are the way individuals interpret and evaluate the world around them, and are servants of interests, needs, and, values (Postman, Bruner, and McGinnies 1948). Such perceptions are revealed in self-expressing texts, such as blog posts, and are utilized as atomic value measurement units in GPV. The core idea of GPV is to extract contextualized and value-laden perceptions (e.g., ”I believe that everyone deserves equal rights and opportunities.”) from unstructured texts, decode underlying values (e.g., Universalism) for arbitrary value systems, and aggregate the results to measure individual values.\n\nThe perceptions in GPV function similarly to the static psychometric items (stimuli) in self-report questionnaires, which support or oppose specific values (Schwartz 1992).\n\nNotably, GPV enables the automatic generation of such items and their adaptation to any given data, overcoming the limitations of traditional tools (Fig. 1). By applying GPV to a large collection of human-authored blogs, we evaluate GPV against psychometric standards. GPV demonstrates its stability and validity in measuring individual values, and its superiority over prior psychological tools.\n\nMeanwhile, the rapid evolution of LLMs raises significant concerns about their potential misalignment with human values. Recent literature treats LLMs as subjects of value measurement (Ma et al. 2024), employing self-report questionnaires (Huang et al. 2024) or their variants (Ren et al. 2024). However, these tools are inherently static, inflexible, and unscalable, as they rely on closed-ended questions derived from limited psychometric inventories.\n\nTo address these limitations, we extend the GPV paradigm to LLMs. Experimenting across 17 LLMs and 4 value theories, we advance the current art of LLM value measurement in several aspects. Firstly, GPV constitutes a novel evaluation methodology that does not rely on static psychometric inventories but measures LLM values based on their scalable and free-form outputs. In this manner, we mitigate response bias demonstrated in prior tools and enable contextspecific value measurements. Secondly, we conduct the first comparative analysis of different measurement paradigms, where GPV yields better measurement results regarding validity and utility. Lastly, we present novel findings regarding value systems and LLM values. Despite the popularity of Schwartz’s value theory within the AI community, alternative value systems like VSM (Hofstede 2011) indicate better predictive power. In addition, values like Long Term Orientation positively contribute to the predicted safety scores, while values like Masculinity negatively contribute.\n\nBelow we summarize our contributions:\n\n• We introduce Generative Psychometrics for Values (GPV), a novel LLM-based value measurement paradigm grounded in text-revealed selective perceptions $( \\ S \\ 3 )$ . • Applying GPV to human-authored blogs, we demonstrate its stability, validity, and superiority over prior psychological tools $( \\ S 4 )$ . • Applying GPV to LLMs, we enable LLM value measurements based on their scalable, free-form, and contextspecific outputs. With extensive evaluations across 17 LLMs, 4 value theories, and 3 measurement tools, we illustrate the superiority of GPV and uncover novel insights regarding value systems and LLM values $( \\ S \\ S )$ .\n\n# 2 Related Work\n\nFor the complete reference list, please refer to the extended version of this paper.\n\n# 2.1 Value Measurements for Human\n\nThe measurement of individual values is pivotal in elucidating the driving forces and mechanisms underlying human behavior (Schwartz 1992; Rokeach 1973). Due to the intricate relationship between behavior and values, researchers have developed different measurement methods, including self-report questionnaires (Maio 2010), behavioral observation (Fischer and Schwartz 2011), and experimental techniques (Murphy and Ackermann 2014). Self-report methods involve participants themselves assessing their agreement with descriptions (Sagiv, Sverdlik, and Schwarz 2011) or ranking the importance of items (Rokeach 1973). Behavioral observation methods require experts to analyze how personal values manifest in real-life actions (Bardi and Schwartz 2003; Schwartz and Butenko 2014). Furthermore, experimental methods employ structured scenarios to isolate and analyze variables affecting human behavior (Bekkers 2007). However, these methods are hindered by response biases, resource demands, inaccuracies in capturing authentic behaviors, and inability to handle historical or subjective data (Ponizovskiy et al. 2020).\n\nOn the other hand, data-driven tools partially address the adverse effects of resource costs, external interference, and response biases. Among them, dictionary-based tools such as LIWC dictionary (Graham, Haidt, and Nosek 2009) and personal values dictionary (PVD) (Ponizovskiy et al. 2020) analyze the frequency of value-related lexicons, flawed for overlooking nuanced semantics and contexts. Recent efforts to train deep learning models for value identification have largely focused on Schwartz’s values and are not validated for individual-level measurements (Sorensen et al. 2024). Other works transform self-report inventories into interactive assessments based on LLMs (Li et al. 2024b), yet inherit many of the limitations of self-reports, such as the inability to handle historical or subjective data.\n\n# 2.2 Value Measurements for LLMs\n\nThe growing integration of LLMs into public-facing applications necessitates their comprehensive and reliable value measurements (Ma et al. 2024). Recently, applying psychometrics—originally designed for humans—to LLMs has gained significant interest (Pellert et al. 2023; Jiang et al. 2024). Related works involve psychometric tests such as the “dark triad” traits (Li et al. 2024c; Huang et al. 2024), the Big Five Inventory (BFI) (Safdari et al. 2023), Myers–Briggs Type Indicator (MBTI) (Pan and Zeng 2023), and morality inventories (Scherrer et al. 2023b). The test results are utilized to investigate the attributes of LLMs concerning political positions (Santurkar et al. 2023), cultural differences (Cao et al. 2023), and belief systems (Scherrer et al. 2023a).\n\nHowever, researchers have observed discrepancies between constrained and free-form LLM responses, and the latter is considered more practically relevant (Ro¨ ttger et al. 2024; Ren et al. 2024). The variability in LLM responses to subtle contextual changes also necessitates scalable and context-specific evaluation methods (Ro¨ttger et al. 2024), which this work aims to address.\n\n# 3 Generative Psychometrics for Values (GPV) 3.1 Value Measurement Based on Selective Perceptions\n\nValues are broad motivational goals and guiding principles in life (Schwartz 1992). Value measurement quantitatively evaluates the significance attributed to various values through individuals’ behavioral and linguistic data (Adkins, Russell, and WERBEL 1994; Meglino and Ravlin 1998; Rokeach 1973). Given any pluralistic value system as a reference frame, we formalize the value measurement task as follows.\n\n![](images/18bdfffba21cd3721815ce7e0d919bfd5057f856dc8fd7428bb20ce8d6adc758.jpg)  \nFigure 1: Illustrations of the three measurement paradigms. (a) Self-reports require individuals to rate their agreement with expert-defined perceptions. (b) Dictionary-based methods count expert-defined and value-related lexicons given text data. (c) GPV automatically and dynamically extracts perceptions from text data and learns to measure open-vocabulary values.\n\nDefinition 1 (Value Measurement). Value measurement is a function $f$ :\n\n$$\nf : ( V , D ) \\to \\mathbf { w } \\in \\mathbb { R } ^ { n } .\n$$\n\nHere, $V = \\{ v _ { 1 } , v _ { 2 } , \\ldots , v _ { n } \\}$ denotes a value system, where each $v _ { i }$ represents a particular value dimension; $D$ denotes the individuals’ behavioral and linguistic data; and ${ \\textbf { w } } =$ $( w _ { 1 } , w _ { 2 } , \\ldots , w _ { n } )$ is a value vector with $w _ { i }$ indicating the relative importance of $v _ { i }$ .\n\nExtensive research explores the underlying mechanisms of $f$ , by which human values drive behaviors and behaviors reflect values (Adkins, Russell, and WERBEL 1994; Meglino and Ravlin 1998; Schwartz 1992; Rokeach 1973). Most related to this work, self-reports (Fig. 1(a)) instantiate $f$ by self-rating the agreement with expert-defined items; dictionary-based methods (Fig. 1(b)) instantiate $f$ by counting expert-defined and value-related lexicons. Both tools conduct value measurement in a limited value space (e.g. 10 Schwartz’s values define a limited 10-dimensional value space) and are inherently static and inflexible.\n\nGPV Overview. In contrast, GPV (Fig. 1(c)) instantiates $f$ through selective perceptions, a process of selecting stimuli from the environment based on an individual’s interests, needs, and values (Postman, Bruner, and McGinnies 1948;\n\nAnderson 2019). For example, when considering a construction project of a new park, individuals who value Hedonism will emphasize the recreational benefits, while those who prioritize Economic Efficiency will focus on the project’s cost. These differing perceptions encode value orientations. GPV leverages LLMs to automatically parse self-expressing texts into such perceptions, trains an LLM for perceptionlevel and open-vocabulary value measurement, and aggregates the results as individual values. We elaborate on the perception-level value measurement in $\\ S \\ 3 . 2$ , then parsing and aggregation in $\\ S 3 . 3$ .\n\n# 3.2 Perception-Level Value Measurement\n\nPerception. Perceptions are selective stimuli from the environment that reflect an individual’s interests, needs, and values (Postman, Bruner, and McGinnies 1948). Here, perceptions are utilized as atomic measurement units, ideally capturing the following properties (Gibson 1960): (1) A perception should be value-laden and accurately describe the measurement subject, ensuring meaningful measurement. (2) A perception is an atomic measurement unit, ensuring unambiguous measurement. (3) A perception is wellcontextualized and self-contained, ensuring that it alone is sufficient for value measurement. (4) All perceptions comprehensively cover all value-laden aspects of the measured subject, ensuring that no related content in the data is left unmeasured.\n\nTraining. We fine-tune Llama-3-8B (Dubey et al. 2024) for perception-level and open-vocabulary value measurement. Its fine-tuning involves the following two tasks (Sorensen et al. 2024) using datasets of ValueBench (Ren et al. 2024) and ValuePrism (Sorensen et al. 2024): (1) Relevance classification determines whether a perception is relevant to a value. (2) Valence classification determines whether a perception supports, opposes, or remains neutral (context-dependent) towards a value. Both tasks are formulated as generating a label given a value and a perception. We present further training details in Appendix A.\n\nTable 1: Accuracy on relevance and valence classification.   \n\n<html><body><table><tr><td>Model</td><td>Relevance</td><td>Valence</td></tr><tr><td>Kaleido</td><td>83.5%</td><td>82.5%</td></tr><tr><td>GPT-4 Turbo</td><td>79.8%</td><td>87.5%</td></tr><tr><td>ValueLlama (ours)</td><td>90.0%</td><td>91.5%</td></tr></table></body></html>\n\nInference. We refer to the fine-tuned Llama-3-8B as ValueLlama. Given a value system $V = \\{ v _ { 1 } , v _ { 2 } , \\ldots , v _ { n } \\}$ and a sentence of perception $s$ , we employ ValueLlama to calculate the relevance and valence probability distribution of each value $\\boldsymbol { v } _ { i }$ to $s$ , respectively denoted as ${ p _ { r e l } ( \\cdot | v _ { i } , s ) }$ and $p _ { v a l } ( \\cdot | v _ { i } , s )$ . Then, we define $w _ { i }$ as $p _ { v a l } ( \\mathrm { s u p p o r t } | v _ { i } , s ) \\ -$ $p _ { v a l } ( \\mathrm { o p p o s e } | v _ { i } , s )$ if the value is relevant $( p _ { r e l } ( \\cdot | v _ { i } , s ) \\ >$ 0.5) and its valence is classified as ”support” or ”oppose”. Otherwise, $w _ { i }$ is considered unmeasured. The prompts for inference are listed in Appendix A.\n\nEvaluating Perception-level Value Measurements. To evaluate the accuracy of perception-level value measurements, we hold out 50 values and 200 associated items (146 with ”Supports” valence and 54 with ”Opposes” valence) from ValueBench as a test dataset, also ensuring the test values are not included in ValuePrism. Using the same zeroshot prompt, we measure the relevance and valence of the test items with Kaleido (Sorensen et al. 2024), GPT-4 Turbo (Achiam et al. 2023), and ValueLlama. Table 1 presents the comparison results, indicating that ValueLlama outperforms state-of-the-art general and task-specific LLMs in zero-shot perception-level value measurement.\n\n# 3.3 Parsing and Aggregation\n\nTo measure values at the individual level, GPV chunks long texts (e.g., blog posts) into segments and prompts an LLM (this work used GPT-3.5 Turbo) to parse each segment into perceptions. Parsing is guided by the background on human values, definitions of perceptions, and few-shot examples (Appendix B.1.) Then, GPV performs perception-level value measurement $( \\ S 3 . 2 )$ for the parsing results. Individual-level measurements are calculated by averaging the perceptionlevel measurements for each value (Schwartz et al. 2007).\n\nEvaluating LLM Parsing. The parsing results are considered high-quality by trained human annotators. On average, the annotators agree that the parsing results meet the defined four criteria in over $8 5 \\%$ of cases, deeming them suitable for further value measurement. The evaluation is detailed in Appendix B.2.\n\n# 3.4 Discussion\n\nRelation to Self-Reports. The items organized in selfreport inventories are essentially perceptions that support or oppose specific values (Schwartz 1992). Compared to GPV, these traditional psychometric inventories compile static and unscalable perceptions, covering a limited measurement range. They also necessitate an additional self-report process to assess the individual’s agreement with the items.\n\nRelation to Dictionary-Based Methods. Both GPV and dictionary-based methods share the fundamental principle that values are embedded in language (Shen, Wilson, and Mihalcea 2019), and they each measure values through text data. However, dictionary-based methods depend on predefined lexicons for closed-vocabulary values and are far less expressive than GPV in capturing semantic nuances. Further analysis is presented in $\\ S 4 . 2$ .\n\nAdvantages of GPV. Compared with traditional tools, GPV 1) effectively mitigates response bias and resource demands by dispensing with self-reports; 2) captures authentic behaviors instead of relying on forced ratings; 3) can handle historical or subjective data; 4) measures values in openended value spaces and easily adapts to new or evolving values without expert effort; and 5) enables more scalable and flexible value measurement.\n\n# 4 GPV for Humans\n\nThis section measures human values using 791 blogs from the Blog Authorship Corpus (Schler et al. 2006), selected after filtering out low-quality entries (Appendix C.1). We evaluate GPV using standard psychological metrics including stability, construct validity, concurrent validity, and predictive validity, and demonstrate its superiority over established psychological tools.\n\n# 4.1 Validation\n\nStability. As values are relatively stable psychological constructs for humans (Sagiv and Roccas 2017; Sagiv et al. 2017; Kimura 2023), we expect that the same individual should exhibit consistent value tendencies across different scenarios. Across 48,888 perception-value pairs, $8 6 . 6 \\%$ of the perception-level measurement results are consistent with the individual-level aggregated results, indicating desirable stability. Detailed results and extended discussions are shown in Appendix C.2.\n\nConstruct Validity. Construct validity is the extent to which a test measures what it claims to measure. In Schwartz’s value system, some values are theoretically positively correlated, such as Self-Direction and Stimulation, while others are negatively correlated, such as Power and Benevolence. Altogether, the 10 Schwartz values form a circumplex structure (Schwartz and Bilsky 1990), where values that are closer together are more compatible, while those that are farther apart are more conflicting (Fig. 2a). We employ multidimensional scaling (MDS) (Cieciuch and Schwartz 2012; Bilsky, Janik, and Schwartz 2011) on the value correlations obtained by GPV, and project both the 10 basic values and the 4 higher-order values onto two-dimensional\n\nTable 2: Correlations between the measurement results of PVD and GPV for four high-level values: Self-transcendence (Stran), Conservation (Cons), Openness to Change (Open), and Self-enhancement (Senh).   \n\n<html><body><table><tr><td colspan=\"2\"></td><td colspan=\"4\">GPV</td></tr><tr><td colspan=\"2\"></td><td>Stran</td><td>Cons</td><td>Open</td><td>Senh</td></tr><tr><td rowspan=\"3\">PVD</td><td>Stran</td><td>0.0421</td><td>0.0077</td><td>-0.0318</td><td>-0.0579</td></tr><tr><td>Cons</td><td>-0.0530</td><td>0.0687</td><td>0.0290</td><td>-0.0321</td></tr><tr><td>Open</td><td>-0.0345</td><td>-0.1376</td><td>0.0369</td><td>-0.0615</td></tr><tr><td></td><td>Senh</td><td>-0.0693</td><td>-0.0345</td><td>0.0540</td><td>0.0880</td></tr></table></body></html>\n\nMDS plots. Then, we assess whether their relative positions align with the theoretical structure. As illustrated in Fig. 2, basic values of the same category (represented by the same color) generally cluster together. Higher-order opposing values are positioned farther apart. The relative positions of a few values do not strictly follow the theoretical structure. For example, Conservation is relatively distant from the other three higher-order values. Such deviations may reflect a gap between the values manifested by self-report and objective data (Ponizovskiy et al. 2020). Overall, the relative positioning of most values resembles the theoretically expected pattern in Fig. 2a, indicating desirable construct validity. More experimental details are provided in Appendix C.3.\n\nConcurrent Validity. Concurrent validity is the extent to which a test correlates with other measures of the same construct administered simultaneously. Theoretically expected correlations can validate newly developed instruments (Lin and Yao 2024). We evaluate the concurrent validity of GPV by comparing it with the personal values dictionary (PVD) (Ponizovskiy et al. 2020), a well-established measurement tool with proven reliability and validity. We analyze the correlations between GPV and PVD measurements, with the results of low-level values presented in Appendix C.4 and high-level aggregated values in Table 2. The results indicate that among the 10 basic values, both identical values (e.g., SE-SE) and most compatible values (e.g., CO-SE) show positive correlations; most opposing values (e.g., BE-AC) exhibit negative correlations. Similarly, within the 4 higherorder values, positive correlations are observed when measuring identical values, whereas most opposing values display negative correlations. These correlations, though not strong, are theoretically expected, which supports the concurrent validity of GPV. $\\ S \\ 4 . 2$ exemplifies the cases where GPV misaligns with PVD.\n\nPredictive Validity. Predictive validity is the extent to which a test predicts future behavior or outcomes. We assess predictive validity by examining if our measurement results align with the blog authors’ gender-related sociodemographic traits. Previous research indicates that, in a statistical sense, men prioritize power, stimulation, hedonism, achievement, and self-direction, while women emphasize benevolence and universalism (Schwartz and Rubel 2005). Our measurement results, presented in Table 3, reveal that men and women score higher on the values they typically prioritize, confirming the consistency of our measurements with established psychological findings.\n\n# 4.2 Case Study\n\nWe exemplify the advantage of GPV over prior data-driven tools such as PVD in Fig. 3. Some values, while not explicitly mentioned in PVD-designed lexicons, are implied within the text. For example, in Schwartz’s theory, Achievement is defined as “the personal pursuit of success, demonstrating competence according to social standards.” In this context, “the teacher’s praise” and “performing well in an exam” both embody the “success” element of achievement. Although the text does not directly reference Achievement or Achievement-related lexicons, the author’s expression of joy and aspiration for these outcomes reflects this value. While GPV effectively captures this aspect, PVD does not.\n\nSome PVD-designed lexicons fail to align with the measurement subject or reflect their intended values. For instance, “friendly” and “goal” target the author’s deskmate; picking up “money” does not indicate the author’s own values of Power. GPV effectively avoids such misinterpretation.\n\n# 5 GPV for Large Language Models\n\nWe evaluate 17 LLMs across 4 value systems using 3 measurement tools: self-report questionnaires (Huang et al. 2024), ValueBench (Ren et al. 2024), and GPV. Unless otherwise specified, we use LLM-generated value-eliciting questions for GPV to ensure a comprehensive and thorough measurement of each value. The detailed experimental setup is described in Appendix D.1.\n\nAcross 19910 perception-value pairs, $8 6 . 8 \\%$ perceptionlevel measurement results are consistent with the LLM-level aggregated results, indicating desirable stability; we present the detailed results in Appendix D.2.\n\nThis section focuses on comparing GPV against prior measurement tools. We defer the value measurement results of all LLMs to Appendix D.4.\n\n# 5.1 Comparative Analysis of Construct Validity\n\nUsing the measurement results from 17 LLMs as data points, we compute correlations between Schwartz’s values. The results are visualized in a heatmap for each measurement tool in Fig. 4. The heatmap reveals the superior construct validity of GPV, as its measurement results align more closely with the theoretical structure (Fig. 2a). Specifically, values that are adjacent in the theoretical circumplex structure exhibit positive correlations, while those that are theoretically distant show negative correlations.\n\nIn contrast, prior tools obtain almost all-positive correlations, contrary to theoretical expectations. This discrepancy indicates their strong susceptibility to response biases, wherein certain LLMs generally tend to assign higher scores in self-report or respond more supportively in ValueBench. Such biases obscure the genuine value orientations of the LLMs. Even when centering the measurement results of prior tools (Appendix D.3), the correlation results remain inconsistent with the theoretical structure. This finding aligns with recent studies revealing the unreliability of\n\n![](images/e912de06229bb2ea570bbaa7cbdd6897d93f010a2cf2076b02a42e19c1fc0eeb.jpg)\n\n![](images/c0dc118d1b3b71a2fafca370bac5b80843155aa4006ec83452c80b5f83ca244b.jpg)\n\n(a) Theoretical structure.\n\n(b) MDS of 10 basic values.\n\n(c) MDS of 4 high-level values.\n\nFigure 2: Two-dimensional MDS of individual values measured by GPV.   \n\n<html><body><table><tr><td>Gender</td><td>SE</td><td>CO</td><td>TR</td><td>BE</td><td>UN</td><td>SD</td><td>ST</td><td>HE</td><td>AC</td><td>PO</td></tr><tr><td>Male</td><td>0.478</td><td>-0.424</td><td>0.261</td><td>0.691</td><td>0.593</td><td>0.777</td><td>0.797</td><td>0.745</td><td>0.757</td><td>0.626</td></tr><tr><td>Female</td><td>0.459</td><td>-0.414</td><td>0.214</td><td>0.751</td><td>0.649</td><td>0.748</td><td>0.761</td><td>0.736</td><td>0.725</td><td>0.587</td></tr></table></body></html>\n\nTable 3: GPV measurement results on Schwartz values for male and female groups.   \n\n<html><body><table><tr><td>Value Pair</td><td>Self-report</td><td>ValueBench</td><td>GPV</td></tr><tr><td>UA&DA</td><td>0.09</td><td>-0.17</td><td>0.65</td></tr><tr><td>Indv&SD</td><td>0.21</td><td>-0.38</td><td>0.61</td></tr><tr><td>Indu&He</td><td>0.30</td><td>-0.30</td><td>0.65</td></tr><tr><td>CO&Be</td><td>0.22</td><td>0.79</td><td>0.38</td></tr><tr><td>Avg.</td><td>0.21</td><td>-0.01</td><td>0.57</td></tr></table></body></html>\n\nLLMs as survey respondents (Dominguez-Olmedo, Hardt, and Mendler-Du¨nner 2023; Ro¨ttger et al. 2024).\n\nBesides Schwartz’s value system, we also evaluate the construct validity by relating the values of different value theories that are theoretically positively correlated. Results in Table 4 indicate the superior construct validity of GPV; i.e., for the theoretically positively correlated values, measuring with GPV also yields higher correlations.\n\nIn summary, evaluations within and across value theories indicate superior construct validity of GPV over prior tools that are prone to response bias.\n\n# 5.2 Comparative Analysis of Value Representation Utility\n\nThe utility of human value measurements lies in their predictive power for human behavior (Schwartz et al. 2007). In the context of LLMs, many related studies are motivated by value alignment for safe LLM deployment (Ji et al. 2023). However, few studies have connected LLM values with their safety. In this section, we evaluate the value representation utility of different measurement tools in terms of their predictive power for LLM safety scores.\n\nTable 4: Correlation between theoretically positively correlated values when using different tools, including Uncertainty Avoidance (UA) & Discomfort with Ambiguity (DA), Individualism (Indv) & Self-Direction (SD), Indulgence (Indu) & Hedonism $\\mathrm { ( H e ) }$ , and Concern for Others (CO) & Benevolence (Be).   \nTable 5: Classification accuracy when using linear probing for value measurement results.   \n\n<html><body><table><tr><td>Tools</td><td>Acc. (%)</td></tr><tr><td>Self-report</td><td>56.7 ± 26.0</td></tr><tr><td>ValueBench</td><td>67.8 ± 20.6</td></tr><tr><td>GPV</td><td>85.6 ± 14.1</td></tr></table></body></html>\n\nHere, we use the safety scores of 17 LLMs from SALADBench (Li et al. 2024a) as ground truth and randomly sample 100 prompts from Salad-Data (Li et al. 2024a) for GPV measurement. We follow the standard linear probing protocol and train a linear classifier to predict the relative safety of LLMs, using the value measurement results as features. We perform its training 30 times for each measurement tool with randomly sampled data splits to ensure statistically meaningful results. Full experimental details are given in Appendix D.4.\n\nUsing values from different value theories as features leads to different results. We present the best classification accuracy of different measurement tools in Table 5. The results indicate that GPV is more predictive of LLM safety scores than prior tools. It suggests that GPV values can be an interpretable and actionable proxy for LLM safety under specific context (R¨ottger et al. 2024).\n\nIn addition, as detailed in Appendix D.4, we examine the predictive power of various value systems for LLM safety scores, as well as the impact of different values on LLM safety. We find that, despite the popularity of Schwartz’s value system within the AI community, VSM (Hofstede\n\nBenevolence: { 'friendly' } User Self-direction: { 'goal' }   \nToday is a wonderful day! In the morning, \\*···\\*   \nfound some money and gave it to a police officer, PVD Power: { 'money' }   \nwho gave me a thumbs-up. At noon, my desk   \nmate told me that his goal is to become a pilot. perception 1: Acting generously by giving found money to a police   \nHe is very friendly, and we get along very well. officer.  { 'Benevolence':1 }   \niIn tmhaethafctlearsnso.oTnh,eI taenaschwer esdaida Idiwffaiscualtgqouoedstkiiodn, 好 pmeartce pwtihona2s:p iArepsptreocbiaetiaopnilfotr. t h{e'Ffriend'l:y1 r}elationship with my desk   \nand I was very pleased. I want to do well on my GPV   \nnext exam. perception N: Desire to perform well on the next exam. 64 24M { 'Self-direction':1, 'Achievement':1 }\n\nFigure 3: Comparative analysis of PVD (Ponizovskiy et al. 2020) and GPV: a case study.\n\n![](images/c6aeffdcfaa584d2b738544bb876c29d4294801928c7887d39ab3106d14a97ba.jpg)  \nFigure 4: Correlations between Schwartz values when using different measurement tools. From dark blue to dark red, the correlation ranges from -1 to 1. The Schwartz values are ordered along the $\\mathbf { X }$ -axis and y-axis according to their positions in the theoretical circumplex structure. See the extended version for more details.\n\n2011) is more predictive of LLM safety. Within VSM, values like Long-term Orientation positively contribute to LLM safety while values like Masculinity negatively contribute.\n\nIn summary, GPV is more predictive of LLM safety. The proposed Value Representation Utility also enables us to evaluate both the predictive power of a value system and the relationship between each encoded value and LLM safety.\n\n# 5.3 Discussion\n\nSuperiority of GPV. We discuss that the superior construct validity may be attributed to the encoded knowledge. ValueLlama learns the correlations between different values and exploits them to generate coherent and valid measurements. In addition, measuring the free-form LLM responses is more reliable than prompting with forcedchoice questions (Dominguez-Olmedo, Hardt, and MendlerDu¨nner 2023). The superior value representation utility of GPV may be attributed to the context-specific value measurements. Unlike humans, who exhibit stable values, LLMs may not be treated as monolithic entities, highlighting the importance of context-specific measurement (Ro¨ttger et al. 2024). GPV, for the first time, enables reliable contextspecific measurements. Overall, compared to prior tools, GPV for LLM value measurements 1) mitigates response bias and yields theoretically valid results; 2) is more practically relevant due to measuring scalable and free-form LLM responses; and 3) enables context-specific measurements.\n\nLimitations and Future Work. The current studies are limited to evaluating LLMs in English. Since the used languages are shown to affect LLM values (Cahyawijaya et al. 2024), future research should consider multi-lingual measurements. Additionally, future investigations should explore the spectrum of values an LLM can exhibit, examining the effects of different profiling prompts. Though LLM values may be steerable, current alignment algorithms establish default model positions and behaviors, making it still meaningful to evaluate the values and opinions reflected in these defaults (Ro¨ ttger et al. 2024).\n\n# 6 Conclusion\n\nThis paper introduces GPV, an LLM-based tool designed for value measurement, theoretically based on text-revealed selective perceptions. Experiments conducted through diverse lenses demonstrate the superiority of GPV in measuring both human and AI values. GPV offers promising opportunities for both sociological and technical research. In sociological research, GPV enables scalable, automated, and cost-effective value measurements that reduce response bias compared to self-reports and provide more semantic nuance than prior data-driven tools. It is highly flexible and can be used independently of specific value systems or measurement contexts. For technical research, GPV presents a new perspective on value alignment by offering interpretable and actionable value representations for LLMs.\n\n# Ethical Statement\n\nMeasuring values with GPV may involve biases encoded in LLMs, during perception-level measurement and perception parsing. Currently, GPV is intended for research purposes only, and researchers should exercise caution when applying it to content with subjective or controversial interpretations.\n\nFor the perception-level measurement, we fine-tuned our model using established psychological inventories and synthetic data validated across cultures, aiming to reduce measurement bias. In the three-class valence classification task, the model is trained to provide neutral predictions when additional context is needed, thereby minimizing the risk of bias. Nevertheless, achieving unbiased measurement requires further investigation.\n\nThe parsing results in this study are considered highquality by our annotators. However, since the annotators share a similar demographic background, their evaluations may lack a comprehensive and diverse perspective. Additionally, the blog data analyzed in this work primarily focuses on general, everyday topics and rarely involves controversial issues. Addressing potential biases in parsing remains an open area for future research.",
    "institutions": [
        "State Key Laboratory of General Artificial Intelligence",
        "School of Intelligence Science and Technology",
        "Peking University",
        "School of Software and Microelectronics",
        "Department of Sociology",
        "School of Psychological and Cognitive Sciences",
        "PKU - Wuhan Institute for Artificial Intelligence"
    ],
    "summary": "{\n  \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n传统的人类价值观测量工具，如自陈问卷存在响应偏差、资源需求大、难以捕捉真实行为、无法处理历史或主观数据等问题，而基于文本数据推断价值观的工具多为字典式，难以把握语义细微差别和上下文相关的价值表达，且缺乏灵活性。同时，大语言模型（LLMs）与人类价值观的潜在不一致性引发关注，现有的LLMs价值观测量工具存在静态、不灵活和不可扩展的问题。这些问题限制了对人类和LLMs价值观的准确测量和理解，价值观测量的准确性和有效性对于理解人类和AI的行为至关重要，能够为社会学和技术研究提供支持。\\n\\n**方法概述**\\n本文提出了基于生成式心理测量学的价值观测量范式（Generative Psychometrics for Values, GPV），该方法基于文本揭示的选择性感知理论，利用大语言模型自动从非结构化文本中提取感知，测量其揭示的价值取向并汇总结果，实现对人类和LLMs价值观的测量。\\n\\n**主要贡献与效果**\\n- 提出了基于大语言模型的价值观测量范式GPV，该范式具有更好的稳定性和有效性。在对人类价值观的测量中，通过对791篇博客的测量，在48,888个感知 - 价值观对中，86.6%的感知层面测量结果与个体层面汇总结果一致；在构建效度上，大部分价值观的相对位置与理论结构相符；在并发效度上，与个人价值观词典（PVD）的测量结果呈现理论预期的相关性；在预测效度上，测量结果与博客作者的性别相关社会人口特征一致。\\n- 将GPV扩展到LLMs价值观测量，通过对17个LLMs和4种价值理论的实验，在构建效度方面，GPV的测量结果更符合理论结构，优于自陈问卷和ValueBench等工具；在价值表示效用方面，GPV对LLM安全分数的预测准确率达到85.6 ± 14.1%，高于自陈问卷的56.7 ± 26.0%和ValueBench的67.8 ± 20.6%。\\n- 发现尽管Schwartz的价值理论在AI社区很流行，但像VSM（Hofstede 2011）这样的替代价值系统对LLM安全分数有更好的预测能力，且如长期导向等价值观对预测的安全分数有积极贡献，而如男性气质等价值观有消极贡献。\",\n  \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nGPV基于文本揭示的选择性感知理论，个体的价值观会影响其对环境刺激的选择和解释，形成特定的感知。其核心思想是将非结构化文本动态解析为类似于传统心理测量中静态刺激的感知，测量这些感知所揭示的价值取向并汇总结果，从而实现对价值观的测量。感知反映了个体的兴趣、需求和价值观，被用作原子测量单位，这种方法能够自动适应任何给定的数据，克服传统工具的局限性。\\n\\n**创新点**\\n先前的人类价值观测量工具，如自陈问卷存在响应偏差、资源需求大、难以捕捉真实行为等问题，数据驱动的工具大多基于字典，无法把握语义细微差别和上下文相关的价值表达。对于LLMs价值观测量，现有的工具基于静态的心理测量清单，存在不灵活和不可扩展的问题。GPV的创新之处在于：不依赖自陈报告，有效减轻响应偏差和资源需求；能够捕捉真实行为；可以处理历史或主观数据；在开放式价值空间中测量价值观，易于适应新的或不断演变的价值观；能够进行更可扩展和灵活的价值测量。此外，GPV是首个能够进行可靠的上下文特定测量的方法。\\n\\n**具体实现步骤**\\n1. **感知层面的价值观测量**：\\n    - 定义感知：感知是反映个体兴趣、需求和价值观的选择性刺激，应具有价值负载、原子性、良好的上下文和全面覆盖等特性。\\n    - 训练模型：使用ValueBench和ValuePrism数据集对Llama - 3 - 8B进行微调，进行相关性分类和效价分类两个任务。相关性分类确定感知是否与某个价值观相关，效价分类确定感知对某个价值观是支持、反对还是中立。两个任务均被表述为给定一个价值观和一个感知生成一个标签。训练细节见论文附录A。\\n    - 推理过程：将微调后的Llama - 3 - 8B称为ValueLlama，给定一个价值系统$V = \\{ v _ { 1 } , v _ { 2 } , \\ldots , v _ { n } \\}$和一个感知句子$s$，计算每个价值观$\\boldsymbol { v } _ { i }$与感知$s$的相关性和效价概率分布，分别表示为${ p _ { r e l } ( \\cdot | v _ { i } , s ) }$和$p _ { v a l } ( \\cdot | v _ { i } , s )$。若价值观相关（$p_{rel}(\\cdot|v_i, s) > 0.5$）且效价分类为“支持”或“反对”，则定义$w_i = p_{val}(support|v_i, s) - p_{val}(oppose|v_i, s)$；否则，$w_i$视为未测量。推理提示见论文附录A。\\n2. **解析和汇总**：\\n    - 文本解析：将长文本（如博客文章）分割成段落，使用GPT - 3.5 Turbo将每个段落解析为感知，解析过程以人类价值观的背景、感知的定义和少量示例为指导。解析指导背景及示例见论文附录B.1。\\n    - 感知层面测量：对解析结果进行感知层面的价值观测量。\\n    - 个体层面测量：通过对每个价值观的感知层面测量结果进行平均，计算个体层面的测量值。个体层面测量的计算依据为Schwartz等人（2007）的研究。解析结果的评估细节见论文附录B.2。\\n\\n**案例解析**\\n在Schwartz的理论中，成就被定义为“根据社会标准个人对成功的追求，展示能力”。在一段文本中，“老师的表扬”和“在考试中表现出色”都体现了成就的“成功”元素，虽然文本没有直接提及成就或与成就相关的词汇，但作者对这些结果的喜悦和渴望反映了这种价值观。GPV能够有效捕捉这一价值，而个人价值观词典（PVD）由于依赖预定义的词汇，无法捕捉到这种隐含的价值。\",\n  \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 人类价值观测量方面：个人价值观词典（PVD）。\\n- LLMs价值观测量方面：自陈问卷、ValueBench。\\n\\n**性能对比**\\n*   **在感知级价值观测量准确性上**：在相关性和效价分类任务中，ValueLlama（GPV使用的模型）的相关性准确率达到90.0%，效价准确率达到91.5%，优于Kaleido（相关性83.5%，效价82.5%）和GPT - 4 Turbo（相关性79.8%，效价87.5%），分别高出6.5、9.0、10.2和4.0个百分点。\\n*   **在测量人类价值观的稳定性上**：GPV在48,888个感知 - 价值观对中，86.6%的感知级测量结果与个体级聚合结果一致，表明具有良好的稳定性。\\n*   **在测量人类价值观的结构效度上**：运用多维缩放（MDS）分析GPV得到的价值观相关性，将10个基本价值观和4个高阶价值观投影到二维空间，其相对位置与理论结构相符，展示了良好的结构效度。而先前工具难以达到这样的效果，其测量结果几乎全为正相关，与理论预期不符，易受响应偏差影响。\\n*   **在测量人类价值观的并发效度上**：通过与个人价值观词典（PVD）比较，GPV测量结果显示，10个基本价值观中相同价值观和大多数相容价值观呈正相关，大多数对立价值观呈负相关；4个高阶价值观中也有类似的理论预期相关性，支持了GPV的并发效度。\\n*   **在测量人类价值观的预测效度上**：GPV测量结果与博客作者的性别相关社会人口特征一致，男性和女性在通常重视的价值观上得分更高，证实了测量结果与既定心理学发现的一致性。\\n*   **在测量LLM价值观的结构效度上**：使用17个LLM的测量结果计算Schwartz价值观之间的相关性，GPV的测量结果更符合理论结构，相邻价值观呈正相关，理论上距离远的价值观呈负相关。而先前工具几乎全为正相关，易受响应偏差影响。在不同价值理论中理论上正相关的价值观测量中，GPV也产生了更高的相关性，表明其在不同价值理论下都具有更好的结构效度。\\n*   **在测量LLM价值观的价值表示效用上**：在预测LLM安全分数方面，使用线性探测对价值测量结果进行分类，GPV的分类准确率为85.6 ± 14.1%，高于自陈问卷的56.7 ± 26.0%和ValueBench的67.8 ± 20.6%，分别高出28.9 ± 31.5和17.8 ± 24.7个百分点。\",\n  \"keywords\": \"### 关键词\\n\\n- 价值观测量 (Value Measurement, N/A)\\n- 生成式心理测量学 (Generative Psychometrics, N/A)\\n- 大语言模型 (Large Language Models, LLMs)\\n- 选择性感知 (Selective Perceptions, N/A)\\n- 价值对齐 (Value Alignment, N/A)\"\n}"
}