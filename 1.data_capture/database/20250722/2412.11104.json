{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.11104",
    "link": "https://arxiv.org/abs/2412.11104",
    "pdf_link": "https://arxiv.org/pdf/2412.11104.pdf",
    "title": "ABC3: Active Bayesian Causal Inference with Cohn Criteria in Randomized Experiments",
    "authors": [
        "Taehun Cha",
        "Donghun Lee"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-15",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Korea University"
    ],
    "paper_content": "# ABC3: Active Bayesian Causal Inference with Cohn Criteria in Randomized Experiments\n\nTaehun Cha and Donghun Lee\\*\n\nKorea University cth127, holy @korea.ac.kr\n\n# Abstract\n\nIn causal inference, randomized experiment is a de facto method to overcome various theoretical issues in observational study. However, the experimental design requires expensive costs, so an efficient experimental design is necessary. We propose ABC3, a Bayesian active learning policy for causal inference. We show a policy minimizing an estimation error on conditional average treatment effect is equivalent to minimizing an integrated posterior variance, similar to Cohn criteria (Cohn, Ghahramani, and Jordan 1994). We theoretically prove ABC3 also minimizes an imbalance between the treatment and control groups and the type 1 error probability. Imbalance-minimizing characteristic is especially notable as several works have emphasized the importance of achieving balance. Through extensive experiments on real-world data sets, ABC3 achieves the highest efficiency, while empirically showing the theoretical results hold.\n\nCode — https://github.com/AIML-K/ActiveBayesianCausal/ Technical appendix — https://arxiv.org/abs/2412.11104\n\n# 1 Introduction\n\nThe major goal of causal inference is to estimate the treatment effect which is a relative effect on the treatment group compared to the control group. In randomized experiments, practitioners allocate treatment to subjects to estimate the treatment effect. Randomized experiments free practitioners from various theoretical problems prevailing in an observational study, e.g. unmeasured confounders. However, a randomized experiment is usually more expensive than an observational study, as a result, an efficient experiment design is desirable.\n\nTo achieve efficiency in randomized experiments, Efron (1971) first introduced a biased-coin design, and several works tried to minimize the estimation bias by achieving a balance between treatment and control groups (Atkinson 2014). Antognini and Zagoraiou (2011) extended this to covariate-adaptive design to achieve a balance, not only between treatment-control groups but also within sampling strata. Several recent works target the same goal using an adaptive Neyman allocation (Dai, Gradu, and Harshaw 2023) or Pigeonhole design (Zhao and Zhou 2024). However, these lines of work assume the experimental subjects are given, not actively choosable.\n\nActive learning is a framework where a practitioner can choose unlabeled data points and ask an oracle to label them (Settles 2009). We can further rationalize the experimental design by adopting an active learning framework. For example, internet companies can choose a member whom they implement the A/B test, by utilizing the personal information they have gathered. Pharmaceuticals can choose a subject based on their personal information after the applicants are gathered to save a budget.\n\nTo develop a sound active learning method for randomized experiments, (1) it should not violate the standard assumptions in causal inference. Also, (2) the method should achieve a balance between observation and control groups to make a sound conclusion. We introduce ABC3, a novel active learning policy for randomized experiments to remedy these issues. Using the Gaussian process, our policy targets minimizing the error of individual treatment effect estimation. Our contributions are three folds:\n\n• We theoretically show minimizing the estimation error on individual treatment effect is equivalent to minimizing the integrated predictive variance in a Bayesian sense.   \n• ABC3, the policy minimizing the variance, theoretically minimizes the imbalance between treatment and control groups and the type 1 error rate.   \n• With extensive experiments, we empirically verify ABC3 outperforms other methods while showing theoretical properties hold.\n\nAfter examining related works in Section 2, we formalize our problem in Section 3. Then we introduce ABC3 and its theoretical properties in Section 4. We empirically verify the performance and properties of ABC3 in Section 5. Then we conclude our paper with several discussions and limitations in Section 6 and 7.\n\n# 2 Related Works\n\nThere are several works exploring an active learning policy for observational data (Sundin et al. (2019), Jesson et al. (2021), and Toth et al. (2022)). Especially, Sundin et al.\n\n(2019) proposed an active learning policy for decision making, when treatment-control groups are imbalanced. They theoretically showed the imbalance can result in a type S error, where a practitioner estimates the treatment effect with a different sign. Likewise, Shalit, Johansson, and Sontag (2017) showed that the generalization error is bounded by the imbalance when estimating the treatment effect. However, their work focused on obtaining a balanced representation from observational data, not a randomized experiment setting.\n\nFor randomized experiments, Deng, Pineau, and Murphy (2011) suggests an active learning policy sampling a point with the highest predictive variance which is similar to Mackay’s criteria (MacKay 1992). Zhu et al. (2024) also proposed Mackay’s criteria-like method under network interference structure. Song, Mak, and Wu (2023) suggested ACE which targets maximizing the covariance between observed and test data sets. We will compare the effectiveness of our policy with these policies.\n\nSample-constrained causal inference setting shares a similar goal with active learning: achieving lower estimation error with fewer samples. Addanki et al. (2022) and Ghadiri et al. (2023) proposed an efficient sampling and estimation method in a randomized experiment setting. Harshaw et al. (2023) suggested a sampling method balancing covariates. However, their work assumes a linear relationship between covariates and potential outcomes, which is vulnerable in real-world scenarios. We will also compare the effectiveness of these policies.\n\n# 3 Problem Formulation\n\nLet $X \\in \\mathcal { X } , Y \\in \\mathcal { Y }$ and $A \\in \\{ 0 , 1 \\}$ be random variables. $X$ is a covariate representing each subject, $Y$ is an outcome, and $A$ represents a binary treatment. Following the NeymanRubin causal model (Rubin 1974), additionally define $Y ^ { 0 }$ and $Y ^ { 1 }$ , potential outcomes for either control or treatment. Unlike the usual supervised learning settings, a practitioner can observe only one of $Y ^ { 0 }$ and $\\bar { Y } ^ { 1 }$ in a causal inference setting. $x , y , a , \\overset { \\cdot } { y } ^ { 0 }$ and $y ^ { 1 }$ denotes the realizations of each random variable.\n\nLet $D _ { \\Omega } = \\{ ( x _ { i } , y _ { i } ^ { 0 } , y _ { i } ^ { 1 } ) \\} _ { i = 1 } ^ { N }$ be a subject pool with covariate information $x _ { i }$ and potential outcomes $y _ { i } ^ { 0 } , y _ { i } ^ { 1 } \\in \\mathbb { R }$ . Let $D _ { t } ^ { 0 } \\ = \\ \\{ ( x _ { i } , y _ { i } ^ { 0 } ) \\} _ { i \\in I _ { t } ^ { 0 } }$ be an observed control group data set at time $t$ with an index set $I _ { t } ^ { 0 }$ . Likewise, define $D _ { t } ^ { 1 } = \\{ ( x _ { i } , y _ { i } ^ { 1 } ) \\} _ { i \\in I _ { t } ^ { 1 } }$ for a treatment group. Let $X _ { \\Omega } , X _ { t } ^ { 0 }$ and $X _ { t } ^ { 1 }$ be sets of $x \\mathbf { s }$ in each data set, $D _ { \\Omega } , D _ { t } ^ { 0 }$ and $D _ { t } ^ { 1 }$ .\n\nOur quantity of interest is the conditional average treatment effect $( C A T E )$ , $C A T E ( x ) \\ = \\ \\mathbb { E } \\left[ Y ^ { 1 } - Y ^ { 0 } | \\bar { | } X = x \\right]$ for each subject $x$ . Then we can train an estimator $C \\hat { A } \\hat { T } E _ { t } ( x ) ~ = ~ \\hat { y } _ { t } ^ { 1 } ( x ) - \\hat { y } _ { t } ^ { 0 } ( x )$ , where $\\hat { y } _ { t } ^ { a } \\mathbf { s }$ are regressors trained on each data set $D _ { t } ^ { a }$ , $\\dot { a } \\in \\{ 0 , 1 \\}$ .\n\nTo evaluate the trained estimator, we use a standard metric, expected precision in estimation of heterogeneous effect (PEHE, Hill (2011)),\n\n$$\n\\epsilon _ { P E H E } ( C \\hat { A } T E _ { t } ) = \\int _ { \\mathcal X } ( C \\hat { A } T E _ { t } ( x ) - C A T E ( x ) ) ^ { 2 } d \\mathbb { P } ( x ) ,\n$$\n\nwhere $\\mathbb { P }$ is a probability distribution over whole covariates. In the usual case, we can treat $\\mathbb { P }$ as a discrete distribution corresponding to the covariates in $D _ { \\Omega }$ . Then we can write $\\begin{array} { r } { \\mathbb { P } = \\frac { 1 } { N } \\Sigma _ { i = 1 } ^ { N } \\breve { \\delta _ { x _ { i } } } } \\end{array}$ , where $\\delta$ is Dirac-delta function.\n\nMeanwhile, in Bayesian statistics, we do not hypothesize the existence of population parameters, e.g. $C A T E ( x )$ . Instead, we update our belief with observed data points. To derive a Bayesian policy, we need to define the optimization target in the Bayesian sense.\n\n# Definition 3.1.\n\n$$\n\\epsilon _ { P E H E } ^ { \\Omega } ( C \\hat { A } \\hat { T } E _ { t } ) = \\int _ { \\mathcal X } ( C \\hat { A } \\hat { T } E _ { t } ( x ) - C \\hat { A } \\hat { T } E _ { \\Omega } ( x ) ) ^ { 2 } d \\mathbb { P } ( x )\n$$\n\nwhere $C \\hat { A } T E _ { \\Omega } ( x ) = \\hat { y } _ { \\Omega } ^ { 1 } ( x ) - \\hat { y } _ { \\Omega } ^ { 0 } ( x ) , \\hat { y } _ { \\Omega } ^ { a } \\mathbf { s }$ are regressors trained on whole data set $D _ { \\Omega }$ .\n\nIntuitively, $\\boldsymbol { C } \\boldsymbol { A } \\mathbf { \\hat { \\boldsymbol { T } } } \\boldsymbol { E } _ { \\Omega }$ represents the best oracle estimation observing all factual and counterfactual data points. At any $t$ , our active learning problem is to choose an optimal (yet unobserved) subject $x ^ { * }$ and its treatment $a ^ { * }$ which can achieve the lowest expected $\\epsilon _ { P E H E } ^ { \\Omega } ( C \\hat { A T } E _ { t + 1 } )$ , i.e.\n\n$$\n\\begin{array} { r l } & { x ^ { * } , a ^ { * } = \\arg \\operatorname* { m i n } _ { x _ { t + 1 } \\in X _ { \\Omega } \\setminus \\left( X _ { t } ^ { 1 } \\cup X _ { t } ^ { 0 } \\right) , a _ { t + 1 } \\in \\{ 0 , 1 \\} } } \\\\ & { \\quad \\quad \\quad \\mathbb { E } _ { t + 1 } \\left[ \\epsilon _ { P E H E } ^ { \\Omega } \\big ( C \\hat { A } T E _ { t + 1 } \\big ) \\right] , } \\end{array}\n$$\n\nwhere $\\mathbb { E } _ { t } \\left[ \\cdot \\right] = \\mathbb { E } \\left[ \\cdot | \\mathcal { F } _ { t } \\right]$ , $\\mathcal { F } _ { t }$ is a filtration at $t$ . If $x ^ { * } = x _ { j }$ and $\\boldsymbol { a } ^ { * } = \\boldsymbol { a }$ , we observe $y _ { j } ^ { a }$ and append it to the observed data set $D _ { t + 1 } ^ { a } = D _ { t } ^ { a } \\cup \\{ ( x _ { j } , y _ { j } ^ { a } ) \\}$ to be used at $t + 1$ .\n\nWhile optimizing the expected error, an active learning policy should not violate the standard assumptions for causal inference. A standard randomized experiment in causal inference requires several assumptions to identify the causal effects (Rubin 1974).\n\nAssumption 3.2. (Consistency) $Y = A Y ^ { 1 } + ( 1 - A ) Y ^ { 0 }$ Assumption 3.3. (Positivity) $\\mathbb { P } ( A = a | X = x ) > 0 , \\forall x$ Assumption 3.4. (Randomization) $( Y ^ { 0 } , Y ^ { 1 } ) \\bot \\bot A | X$\n\nAssumption 3.2. assumes that the observed outcome $Y$ under treatment $A$ is equivalent to its potential outcome $Y ^ { a }$ . Assumption 3.3. is required to well-define the conditional expectation. Assumption 3.4. implies that the treatment $A$ should be assigned independently from the potential outcome values. The assumptions guarantee an unbiased estimation of CATE, i.e. $C A \\dot { T } E ( x ) \\dot { = } \\mathbb { E } \\left[ Y ^ { 1 } - Y ^ { 0 } | X = x \\right] =$ $\\mathbb { E } \\left[ \\mathbb { E } \\left[ Y | A = 1 , X = x \\right] - \\mathbb { E } \\left[ Y | A = 0 , X = x \\right] \\right]$ .\n\nAssumption 3.4. is crucial in an active learning setting since several active learning algorithms consider $y$ values when querying $x$ and $a$ . For instance, Song, Mak, and $\\mathrm { w } _ { \\mathrm { u } }$ (2023) introduced ACE-UCB, a bandit-like policy that targets a subject with the highest individual treatment effect. However, ACE-UCB exploits previously observed outcomes when choosing a subject and treatment. As a result, ACEUCB can generate confounding between treatment and potential outcomes and may violate Assumption 3.4. So querying $x$ and $a$ without considering ys is crucial when adopting an active learning framework for a causal inference.\n\n# 4 Proposed Method\n\n# ABC3: Active Bayesian Causal Inference with Cohn Criteria\n\nGaussian process ( $\\mathcal G \\mathcal P$ , Rasmussen and Williams (2006)) is a non-parametric machine learning model based on Bayesian statistics. It is a powerful tool as it allows flexible function estimation depending on a pre-defined kernel function. Set priors on $f \\sim \\mathcal { G P } ( \\bar { m } ( x ) , \\bar { k ( x , x ^ { \\prime } ) } )$ , where $m ( x )$ is a mean prior and $k$ is a kernel function. Assume a data set $D =$ $\\mathbf { \\bar { \\{ ( } }  x _ { i } , y _ { i } ) \\} _ { i = 1 } ^ { N }$ with noisy observation $y _ { i } = Y ( x _ { i } ) + \\epsilon _ { i } , \\epsilon _ { i }$ ∼ $\\mathcal { N } ( 0 , \\sigma _ { \\epsilon } ^ { 2 } )$ . We can obtain a posterior distribution of $f ( x ^ { * } )$ given data set $D$ by computing $f ( x ^ { * } ) | D$ as\n\n$$\n\\begin{array} { r l } & { f ( \\boldsymbol { x } ^ { * } ) | D \\sim \\mathcal { N } ( m ^ { \\prime } ( \\boldsymbol { x } ^ { * } ) , \\sigma ^ { 2 } ( \\boldsymbol { x } ^ { * } ) ) \\mathrm { ~ w h e r e } } \\\\ & { m ^ { \\prime } ( \\boldsymbol { x } ^ { * } ) = m ( \\boldsymbol { x } ^ { * } ) + \\mathbf { k } _ { * } ( \\boldsymbol { x } ^ { * } ) ^ { T } \\left[ \\mathbb { K } + \\sigma _ { \\epsilon } ^ { 2 } \\mathbf { I } \\right] ^ { - 1 } \\mathbf { y } , } \\\\ & { \\mathrm { c o v } ( \\boldsymbol { x } , \\boldsymbol { x } ^ { * } ) = k ( \\boldsymbol { x } , \\boldsymbol { x } ^ { * } ) - \\mathbf { k } _ { * } ( \\boldsymbol { x } ) ^ { T } \\left[ \\mathbb { K } + \\sigma _ { \\epsilon } ^ { 2 } \\mathbf { I } \\right] ^ { - 1 } \\mathbf { k } _ { * } ( \\boldsymbol { x } ^ { * } ) , } \\\\ & { \\sigma ^ { 2 } ( \\boldsymbol { x } ^ { * } ) = \\mathrm { c o v } ( \\boldsymbol { x } ^ { * } , \\boldsymbol { x } ^ { * } ) , } \\\\ & { \\mathbf { k } _ { * } ( \\boldsymbol { x } ^ { * } ) = \\left[ k ( \\boldsymbol { x } _ { i } , \\boldsymbol { x } ^ { * } ) \\right] _ { i = 1 } ^ { N } , \\mathbb { K } = \\left[ k ( \\boldsymbol { x } _ { i } , \\boldsymbol { x } _ { j } ) \\right] _ { i , j = 1 } ^ { N } , } \\\\ & { \\mathbf { y } = \\left[ \\boldsymbol { y } _ { i } \\right] _ { i = 1 } ^ { N } . } \\end{array}\n$$\n\nWe can observe that the posterior variance $\\sigma ^ { 2 } ( x ^ { * } )$ does not depend on y. Adopting the notations from above, we define $\\mathbf { k } _ { t , * } ^ { a } , \\mathbb { K } _ { t } ^ { a } , \\mathbf { y } _ { t } ^ { a }$ and $\\mathrm { c o v } _ { t } ^ { a }$ with observed data set $D _ { t } ^ { a }$ at time $t$ . We also assume zero-mean prior, i.e. $m ( x ) = 0 , \\forall x$ .\n\nThe following theorem identifies our active learning policy with only posterior variance terms.\n\nTheorem 4.1. Assume $| k ( x , x ^ { \\prime } ) | < \\infty$ and $| y _ { i } ^ { a } | < \\infty$ for all $x , x ^ { \\prime } \\in \\mathcal { X }$ and $a , i$ , as a result, $\\epsilon _ { P E H E } ^ { \\Omega } ( C \\hat { A { T } } E _ { t } ) < \\infty , \\forall t .$ . Let our estimator $C \\hat { A } \\hat { T } E _ { t } ( x ) ~ = ~ \\hat { y } _ { t } ^ { 1 } ( x ) - \\hat { y } _ { t } ^ { 0 } ( x )$ , where $\\hat { y } _ { t } ^ { a } ( x ) \\ = \\ \\mathbb { E } _ { t } \\left[ Y ^ { a } ( x ) \\right]$ is a mean posterior distribution of gaussian process $Y ^ { a }$ trained on data set $D _ { t } ^ { a }$ . Assume two Gaussian processes $Y ^ { 1 }$ and $Y ^ { 0 }$ are independent. Then\n\n$$\n\\begin{array} { r l } & { a r g \\ m i n _ { x _ { t + 1 } , a _ { t + 1 } } \\mathbb { E } _ { t + 1 } \\left[ \\displaystyle \\epsilon _ { P E H E } ^ { \\Omega } ( C \\hat { A } T E _ { t + 1 } ) \\right] = } \\\\ & { a r g \\ m i n _ { x _ { t + 1 } , a _ { t + 1 } } \\int _ { \\mathcal X } \\mathbb { V } _ { t + 1 } \\left[ Y ^ { 1 } ( x ) \\right] + \\mathbb { V } _ { t + 1 } \\left[ Y ^ { 0 } ( x ) \\right] d \\mathbb { P } ( x ) } \\end{array}\n$$\n\nProof of Theorem 4.1. is in Appendix A.1. Theorem 4.1. states that minimizing the error on CATE estimation is equivalent to minimizing the integrated posterior variance of the estimator. In active learning literature, the active learning policy minimizing the integrated predictive variance is called Active Learning Cohn (Cohn, Ghahramani, and Jordan (1994) and Gramacy (2020)). Seo et al. (2000) proposed Active Learning Cohn policy utilizing the Gaussian process in a usual supervised learning setting. We extend this line of work to causal inference and name our policy after his name,\n\n# ABC3: Active Bayesian Causal Inference with Cohn Criteria.\n\nComputing the inverse of all hypothetical covariance matrix $\\mathbb { K } _ { t + 1 } ^ { a }$ for all $x$ is computationally infeasible. However, we can efficiently find the minimizer as ${ \\mathbb K } _ { t } ^ { a }$ is a principal submatrix of Kta+1.\n\n# Proposition 4.2.\n\n$$\n\\begin{array} { r l } & { a r g m i n _ { x _ { t + 1 } , a _ { t + 1 } } \\displaystyle \\int _ { \\mathcal X } \\mathbb { V } _ { t + 1 } \\left[ Y ^ { 1 } ( \\boldsymbol { x } ) \\right] + \\mathbb { V } _ { t + 1 } \\left[ Y ^ { 0 } ( \\boldsymbol { x } ) \\right] d \\mathbb { P } ( \\boldsymbol { x } ) } \\\\ & { \\ = a r g m a x _ { x _ { t + 1 } , a } } \\end{array}\n$$\n\n$$\n\\begin{array} { r l r } & { } & { \\frac { \\int _ { \\mathcal { X } } \\Big [ ( \\tilde { \\bf k } _ { t + 1 } ^ { a } ) ^ { T } \\left[ \\mathbb { K } _ { t } ^ { a } + \\sigma _ { \\epsilon } ^ { 2 } { \\bf I } \\right] ^ { - 1 } { \\bf k } _ { t , * } ^ { a } ( x ) - k ( x _ { t + 1 } , x ) \\Big ] ^ { 2 } d \\mathbb { P } ( x ) } { k ( x _ { t + 1 } , x _ { t + 1 } ) + \\sigma _ { \\epsilon } ^ { 2 } - ( \\tilde { \\bf k } _ { t + 1 } ^ { a } ) ^ { T } [ \\mathbb { K } _ { t } ^ { a } + \\sigma _ { \\epsilon } ^ { 2 } { \\bf I } ] ^ { - 1 } \\tilde { \\bf k } _ { t + 1 } ^ { a } } } \\end{array}\n$$\n\nwhere k˜ta+1 = [k(xi, xt+1)]i Ia\n\nProof of Proposition 4.2. is in Appendix A.2. From the proposition, we successfully eliminate the dependency on $\\mathbb { K } _ { t + 1 } ^ { a }$ , and there is no need to compute an inverse matrix for every $x$ . We summarize our policy in Algorithm 1.\n\n# Algorithm 1: ABC3\n\nInput: Current time step $t$ , whole covariate set $X _ { \\Omega }$ , covari  \nates distribution $\\mathbb { P }$ , previous observations $X _ { t } ^ { 1 }$ and $X _ { t } ^ { 0 }$ , kernel   \n$k$ , noise parameter $\\sigma _ { \\epsilon }$   \nOutput: $x _ { t + 1 } , a _ { t + 1 }$ 1: $\\mathcal { V } _ { 0 } , \\mathcal { V } _ { 1 } = \\phi , \\phi$ 2: for $x \\in X _ { \\Omega } \\setminus ( X _ { t } ^ { 0 } \\bigcup X _ { t } ^ { 1 } )$ do   \n3: Compute $\\tilde { \\mathbf { k } } _ { t + 1 } ^ { 1 }$ anSd $\\tilde { \\mathbf { k } } _ { t + 1 } ^ { 0 }$ assuming $x _ { t + 1 } = x$   \n4: $v ^ { 0 }$ , v1 = Equation (1) for each $a \\in \\{ 0 , 1 \\}$ 5: $\\mathcal { V } _ { 0 } = \\mathcal { V } _ { 0 } \\bigcup \\{ v ^ { 0 } \\} , \\mathcal { V } _ { 1 } = \\mathcal { V } _ { 1 } \\bigcup \\{ v ^ { 1 } \\}$   \n6: end for   \n7: $: i , a _ { t + 1 } = \\arg \\operatorname* { m a x } ( \\mathcal { V } _ { 0 } | | \\mathcal { V } _ { 1 } )$   \n8: $x _ { t + 1 } = X _ { \\Omega } \\setminus \\left( X _ { t } ^ { 0 } \\bigcup X _ { t } ^ { 1 } \\right) [ i ]$   \n9: return xt+1, at+1\n\n# Theoretical Analysis\n\nBalancing Treatment-Control Groups Unlike usual supervised learning, causal inference requires precise estimation of both functions for treatment and control groups. As a result, the balance between the two groups is crucial to obtain a sound estimation. For example, consider a study on the causal effect of online lectures. Assume our treatment group is concentrated on undergraduate students while our control group is concentrated on graduate students. Then it would be difficult to make a sound conclusion with statistical tools, as the two groups are highly imbalanced.\n\nSeveral researchers theoretically analyzed the effect of the imbalance on causal inference. Shalit, Johansson, and Sontag (2017) showed that the generalization error on CATE is upper bounded by the imbalance. Sundin et al. (2019) defined Type S Error, assigning a different sign ( $\\cdot +$ or -) to CATE, and showed the probability of Type S Error is bounded by the imbalance. Both works utilized Maximum Mean Discrepancy (MMD, Gretton et al. (2012)) to quantify and measure the imbalance.\n\n# Definition 4.3.\n\n$$\n\\begin{array} { r } { \\mathbf { M M D } ( P , Q , \\mathcal { F } ) = \\qquad } \\\\ { \\operatorname* { s u p } _ { f \\in \\mathcal { F } } \\mathbb { E } _ { x \\sim P ( x ) } \\left[ f ( x ) \\right] - \\mathbb { E } _ { y \\sim Q ( y ) } \\left[ f ( y ) \\right] } \\end{array}\n$$\n\nwhere $\\mathcal { F }$ is a unit ball of Reproducing Kernel Hilbert Space (RKHS) induced by a kernel $k ( \\cdot , \\cdot )$ .\n\nGretton et al. (2012) showed that the MMD is equivalent to the distance between mean embeddings in RKHS, $\\mu _ { P }$ and $\\mu _ { Q }$ , and can be computed with the kernel function $k$ . Remark 4.4.\n\n$$\n\\begin{array} { r l } & { \\mathbf { M D } ( P , Q , \\mathcal { F } ) ^ { 2 } = | | \\mu _ { P } - \\mu _ { Q } | | ^ { 2 } } \\\\ & { \\qquad = \\operatorname { \\mathbb { E } } _ { x , x ^ { \\prime } \\sim P ( x ) } \\left[ k ( x , x ^ { \\prime } ) \\right] + \\operatorname { \\mathbb { E } } _ { y , y ^ { \\prime } \\sim Q ( y ) } \\left[ k ( y , y ^ { \\prime } ) \\right] } \\\\ & { \\qquad - 2 \\operatorname { \\mathbb { E } } _ { x \\sim P ( x ) , y \\sim Q ( y ) } \\left[ k ( x , y ) \\right] } \\end{array}\n$$\n\nWe show our active learning policy approximately minimizes the upper bound of MMD.\n\nAssume the probability over whole covariates is a discrete distribution corresponding to the covariates in $D _ { \\Omega }$ , i.e. $\\begin{array} { r } { \\mathbb { P } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\delta _ { x _ { i } } } \\end{array}$ . For notational convenience, we assume the noise-less observation case $\\sigma _ { \\epsilon } ^ { 2 } = 0$ , but we can easily extend the result to the noisy case. Let $\\begin{array} { r } { \\mathbb { P } _ { t } ^ { a } \\ = \\ \\frac { 1 } { | I _ { t } ^ { a } | } { \\sum } _ { i \\in I _ { t } ^ { a } } \\dot { \\delta } _ { x _ { i } } , a \\ \\in } \\end{array}$ $\\{ 0 , 1 \\}$ be an empirical distribution for treatment-control group up to time $t$ . Then we obtain the following theorem.\n\nTheorem 4.5. Assume $| k ( x , x ^ { \\prime } ) | ~ < ~ \\infty$ and $| y _ { i } ^ { a } | ~ < ~ \\infty$ for all $x , x ^ { \\prime } \\in \\mathcal { X }$ and $a , i$ . Let $\\lambda ^ { * }$ be a maximum eigenvalue of $\\mathbb { K } _ { \\Omega }$ , i.e. covariance matrix of whole covariates. Let M = N1 2 ΣiN,j=1k(xi, xj). Define functions $\\begin{array} { r } { \\delta ^ { * } ( I _ { n } ) \\ = \\ \\frac { 1 } { n } \\Sigma _ { i \\in I _ { n } } \\int _ { \\mathcal X } k ( x _ { i } , \\overset { \\cdot } { x } ) d \\mathbb { P } ( x ) } \\end{array}$ , and $\\epsilon ^ { * } ( I _ { n } ) = M -$ $\\textstyle { \\frac { 1 } { n ^ { 2 } } } \\sum _ { i , j \\in I _ { n } } k { \\big ( } x _ { i } , x _ { j } { \\big ) }$ ,RXwhere $I _ { n } \\subset \\{ 1 , . . . , N \\}$ is an $n$ -element index set. Assume $\\epsilon ^ { * } ( I _ { n } ) \\leq 2 \\delta ^ { * } ( I _ { n } ) , \\forall I _ { n }$ . Then\n\n$$\n\\begin{array} { r l r } {  { M M D ( \\mathbb { P } _ { t } ^ { 1 } , \\mathbb { P } _ { t } ^ { 0 } , \\mathcal { F } ) ^ { 2 } \\leq 4 \\frac { \\lambda ^ { * } } { | I _ { t } ^ { 1 } | } + 4 \\frac { \\lambda ^ { * } } { | I _ { t } ^ { 0 } | } } } \\\\ & { } & { + \\ 2 \\int _ { \\mathcal { X } } \\mathbb { V } _ { t } [ Y ^ { 1 } ( x ) ] + \\mathbb { V } _ { t } [ Y ^ { 0 } ( x ) ] d \\mathbb { P } ( x ) } \\end{array}\n$$\n\nProof of Theorem 4.5. is in Appendix A.3. We can observe that the first two terms decrease as we observe more subjects. The third term is our exact optimization target as introduced in Theorem 4.1. Empirical achievability and intuitive meaning of the assumption are covered in Section 5.6 and Appendix B.\n\nBy combining this result with the previous theoretical analysis (Shalit, Johansson, and Sontag (2017) and Sundin et al. (2019)), our active learning policy minimizes the upper bounds of both generalization error and type S error.\n\nType 1 Error Minimization The precise estimation of causal effect is an important object of causal inference. However, testing the existence of causal effect is another key component. R. A. Fisher first advocated the randomized experiment to test the existence of causal effects (Fisher 1970). He proposed Fisher’s sharp null hypothesis, where $y _ { i } ^ { 1 } = y _ { i } ^ { 0 } , \\bar { \\forall i } = 1 , . . . , N$ . A randomized experiment method should not reject the null hypothesis if it finds no causal effect. Type 1 error occurs when we reject a null hypothesis while it is true.\n\nAs active learning is a sequential procedure, a practitioner may want to test the statistical significance of the treatment effect sequentially. However, Ham et al. (2023) stated that applying statistical tests sequentially can result in a high type\n\n1 error probability. An active learning policy should avoid the type 1 error to make a sound conclusion.\n\nHere we present our theoretical result that our policy minimizes the upper bound of the type 1 error probability. First, we define the type 1 error.\n\nDefinition 4.6. (Type 1 Error at $t$ ) Under Fisher’s Sharp null hypothesis, as it implies $\\mathrm { C A T E } ( x ) = 0 , \\forall x .$ ,\n\n$$\n\\mathbb { P } _ { t } \\left[ { \\mathrm { T y p e ~ 1 ~ E r r o r } } ( x ) \\right] = \\mathbb { P } _ { t } \\left[ | Y ^ { 1 } ( x ) - Y ^ { 0 } ( x ) | > \\alpha \\right] ,\n$$\n\nwhere $\\alpha$ is a decision threshold.\n\nThen we can show that our policy minimizes the upper bound of the type 1 error rate over the whole $x$ . Proof of Theorem 4.7. is in Appendix A.4.\n\nTheorem 4.7. Under Fisher’s Sharp null hypothesis, $\\begin{array} { r l r l } { { 2 } r g \\ : m i n _ { x _ { t + 1 } , a _ { t + 1 } } \\int _ { \\mathcal { X } } \\mathbb { V } _ { t + 1 } \\left[ \\dot { Y } ^ { 1 } ( \\boldsymbol { x } ) \\right] } & { { } } & { } & { { } + } \\end{array}$ $\\mathbb { V } _ { t + 1 } \\left[ Y ^ { 0 } ( x ) \\right] d \\mathbb { P } ( x )$ also minimizes the upper bound of the $\\begin{array} { r } { \\int _ { \\mathcal { X } } \\mathbb { P } _ { t + 1 } \\left[ T y p e \\ I \\ E r r o r ( x ) \\right] d \\mathbb { P } ( x ) } \\end{array}$\n\n# 5 Experiments\n\nIn this section, we empirically analyze the theoretical results introduced in Section 4. For the comparison, we utilize IHDP (Brooks-Gunn, Liaw, and Klebanov (1992) and Hill (2011)), Boston (Harrison and Rubinfeld 1978), ACIC (Gruber et al. 2019), and Lalonde (LaLonde 1986) data sets. IHDP and ACIC data sets are semi-real data sets where counterfactual outcomes are simulated. Boston and Lalonde data sets do not contain counterfactual outcomes. Following Addanki et al. (2022), we set $Y ^ { 0 } = Y ^ { 1 }$ to simulate the null hypothesis circumstance (i.e. $C A T E ( x ) = 0 , \\forall x )$ . All data sets have continuous outcome values as we assume the potential outcome follows a Gaussian process. More detailed explanations of data sets are in Appendix C.\n\nWe randomly divide each data set in half for every trial to construct train and test data sets. Each active learning policy selects what to observe from the train data set and regressors are trained on the observed train data set. Policies are evaluated on the test data set for each pre-defined time step. We utilize the following baseline policies.\n\n• Naive: This policy randomly selects $x _ { t + 1 }$ from the subject pool and then decides treatment $a _ { t + 1 }$ with Bernoulli distribution with probability 0.5.   \n• Mackay (MacKay 1992): This policy selects $x _ { t + 1 }$ , $\\begin{array} { r l } { a _ { t + 1 } } & { { } = } \\end{array}$ arg $\\operatorname* { m a x } _ { x , a } \\mathbb { V } _ { t } \\left[ Y ^ { a } ( \\boldsymbol { x } ) \\right]$ , i.e. chooses the most uncertain point at $t$ .   \n• Leverage (Addanki et al. 2022): Under linearity assumption between covariates and outcomes, this policy exploits leverage score and shows theoretical guarantees on nearly optimal RMSE on CATE estimation. Unlike other policies, this policy is not sequential, as a result, the observed points for $t$ and $t + 1$ can be different.   \n• ACE (Song, Mak, and Wu 2023): Unlike other policies, this policy assumes access to the test data set and select $\\begin{array} { r l r l } { x _ { t + 1 } , a _ { t + 1 } } & { { } } & { = } \\end{array}$ $\\begin{array} { r } { \\operatorname* { m a x } _ { x , a } \\left[ \\frac { 1 } { | n _ { t e s t } | } { \\sum _ { i = 1 } ^ { | n _ { t e s t } | } } { \\mathrm { c o v } _ { t } ^ { a } \\left[ x _ { i } ^ { t e s t } , x \\right] } \\right] ^ { 2 } / { \\mathbb { V } _ { t } \\left[ Y ^ { a } ( x ) \\right] } . } \\end{array}$ . It maximizes the covariance between observed and test data sets while minimizing the predictive variance.\n\n![](images/e114fbb7b38c22fc34f50a0b8bb690cb2805285826e657a1b255ef2559a0191b.jpg)  \nFigure 1: Mean of $\\epsilon _ { P E H E }$ . $x$ -axis represents the observed percentage of the population. We measure $\\epsilon _ { P E H E }$ for every $10 \\%$ observation.\n\nAfter selecting $x _ { t + 1 }$ and $a _ { t + 1 }$ , all policies fit the Gaussian process for regression. We apply feature-wise normalization and $y$ -standardization for all regressors. (except Leverage, which requires item-wise normalization) We fit two Gaussian process models with Constant Kernel \\* Radial Basis Function (RBF) Kernel $^ +$ White Kernel. We optimize the kernel hyperparameters using scikit-learn package.\n\nAll the uncertainty-aware policies (ABC3, Mackay and ACE) use the Gaussian process to quantify the uncertainty. For the uncertainty-quantifying kernels, we use RBF kernel with length scale 1.0 with $\\dot { \\sigma } _ { \\epsilon } ^ { 2 } \\overset { \\sim } { = } 1$ . (We check the hyperparameter sensitivity in Section 5.4.)\n\n# Minimizing Error\n\nWe measure the ϵP EHE (not ϵPΩ EHE) when observing every $10 \\%$ of population. We run 100 experiments for every data set. We mark the mean of measured $\\epsilon _ { P E H E }$ . The results are in Figure 1. Appendix D presents the standard deviation of measured ϵP EHE.\n\nABC3 shows the best performance, i.e. the lowest $\\epsilon _ { P E H E }$ for most time steps. We can verify ABC3 succesfully minimizes the population $\\epsilon _ { P E H E }$ , though optimization target of ABC3 is $\\epsilon _ { P E H E } ^ { \\dot { \\Omega } }$ . In most cases, when ABC3 observes only half of the population, it achieves $\\epsilon _ { P E H E }$ level which is achieved with full observation by other policies. Especially for Boston, after $20 \\%$ , ABC3 achieves $\\epsilon _ { P E H E }$ which Naive policy cannot achieve even with full observation.\n\nACE policy shows comparable results to ABC3. ACE slightly outperforms ABC3 for a $20 \\%$ interval of Lalonde data set. However, ABC3 outperforms ACE in most cases, though ABC3 has no access to the test data set, unlike ACE.\n\nLeverage temporarily outperforms ABC3 for the beginning part of IHDP and the last part of Lalonde. However, for ACIC, Leverage significantly underperforms other policies. The result may imply the vulnerability of linearity assumption in real-world data sets.\n\nMackay underperforms even Naive policy most times. It is interesting as Mackay utilizes the same uncertainty information as ABC3. The result may imply the importance of proper utilization of the same information.\n\nIn summary, ABC3 is a promising Bayesian active learning policy, which efficiently and robustly achieves the best performance among the others.\n\n# Balancing Treatment-Control Groups\n\nTheorem 4.5 states that ABC3 theoretically minimizes the maximum mean discrepancy between treatment and control groups. A policy can benefit by minimizing MMD from several theoretical aspects, as introduced in Section 4.2. We empirically verify the property. We measure MMD between $\\mathbb { P } _ { t } ^ { 1 }$ and $\\mathbb { P } _ { t } ^ { 0 }$ for every $10 \\%$ observation and applied the same setting as the previous section. We report the mean and standard\n\nIHDP BOSTON ACIC LALONDE\n\nABC3 achieves substantially lower MMD compared to Naive policy on all data sets. As noted in Theorem 4.5, the upper bound of MMD is minimized as the number of observations increases. As a result, Naive policy also shows a decrease in MMD as time proceeds. However, the MMD gap between ABC3 and Naive is significant at the beginning stage of experiments. The gap is especially large for ACIC and Lalonde data sets. The result empirically supports Theorem 4.5. holds, and ABC3 achieves a balance between treatment-control groups.\n\n# Minimizing Type 1 Error\n\nTheorem 4.7. states ABC3 minimizes the upper bound of the integrated type 1 error probability, $\\begin{array} { r } { \\int _ { \\mathcal { X } } \\mathbb { P } _ { t + 1 } \\left[ \\mathrm { T y p e } \\ 1 \\mathrm { E r r o r } ( x ) \\right] d \\mathbb { P } ( x ) } \\end{array}$ . Here we verify the property empirically.\n\nWe use Boston and Lalonde data sets which assume no treatment effect, i.e. $Y ^ { 0 } = Y ^ { 1 }$ . To measure the type 1 error rate, we compute a mean and standard deviation of $C \\hat { A T } E ( x )$ for every $x$ in the test data set. Then we implement the $Z$ test with a significance level of $5 \\%$ . (i.e. $\\alpha = 1 . 9 6 \\$ . Type 1 error occurs when the absolute value of the $Z \\cdot$ -statistic is bigger than $\\alpha$ . We compute the percentage of $x$ where the type 1 error occurs. The result is in Figure 3.\n\nBOSTON LALONDE 1 2 V 5 8 0 40 80 40 80 Population (%) Population (%)\n\nAs time proceeds, ABC3 achieves a lower type 1 error rate, as expected in Theorem 4.7. ABC3 consistently shows a lower Type 1 error rate than Naive. However, Mackay shows the highest error rate on Lalonde. The result may again imply the importance of proper utilization of the uncertainty information.\n\nMeanwhile, Leverage shows a significantly lower error rate for Boston. This aspect was not presented in the original paper (Addanki et al. 2022). The result may imply the strong linearity in Boston data set and the power of the method when the linearity assumption holds.\n\n# Hyperparameter Sensitivity\n\nTo implement ABC3, we need to determine uncertaintyquantifying kernel, kernel parameters, and observation error $\\sigma _ { \\epsilon } ^ { a }$ as hyperparameters. As usual machine learning models utilizing the kernel method, selecting an appropriate kernel and parameters is crucial for obtaining precise estimation. We present hyperparameter sensitivity analysis for ABC3.\n\n![](images/f53aef7511bb54d40b49a484dee0a04feaac89bf51fc371e76c12f569d2d5ffa.jpg)  \nFigure 2: Mean and standard deviation of MMD between observed treatment and control groups, $\\mathbb { P } _ { t } ^ { 1 }$ and $\\mathbb { P } _ { t } ^ { 0 }$ . The blue line is for ABC3, and the orange line is for Naive policy. $x$ - axis is for the sampled ratio and $y$ -axis is MMD.   \nFigure 3: Type 1 error rate. The legend is from Figure 1.   \nFigure 4: $\\epsilon _ { P E H E }$ for every kernel and kernel parameter. The numbers in parenthesis are kernel parameters.\n\nFor the kernel, we test RBF kernel (utilized throughout this paper), Matern kernel, and Exp-Sine-Squared kernel (Sine). RBF kernel has one parameter, lengthscale, which determines how ‘local’ the output function would be. The smaller the lengthscale, the more local and wiggler the resulting function is. Matern kernel is a generalization of RBF kernel and has two parameters, lengthscale, and smoothness. Sine kernel assumes that our data shows a periodic pattern. it has two parameters, lengthscale, and periodicity. Here we test only lengthscale, with setting periodicity as 1. We present $\\epsilon _ { P E H E }$ for each setting on IHDP data set in Figure 4.1\n\nFor all data sets, Matern and RBF kernels show robust performance along different kernel parameters. However, Sine kernel significantly deteriorates in the three data sets. The result implies no (or week) periodicity in the three data sets. Meanwhile, Sine kernel with a length scale of 0.1 shows the best performance on Lalonde data set. It may imply a periodicity in the Lalonde data set. The result gives a similar lesson: selecting an appropriate kernel and parameters is crucial for obtaining precise estimation. Overall, RBF and Matern kernels with ‘reasonable’ parameters are safe options for general data sets.\n\nWe also test the hyperparameter sensitivity to $\\sigma _ { \\epsilon } ^ { a }$ . The result is in Figure 5. Except the extreme case $( \\sigma _ { \\epsilon } ^ { 0 } : \\sigma _ { \\epsilon } ^ { 1 } =$ $1 . 0 : 0 . 1 )$ , ABC3 shows robust performance for different $\\sigma _ { \\epsilon } ^ { a } \\mathbf { s }$ . Interestingly, $1 . 0 : 0 . 1$ shows the best performance for IHDP data set, which may imply the noisiness in the control group. However, $1 . 0 : 0 . 1$ significantly deteriorates on Lalonde data set. Overall, the equal noise setting $( 1 . 0 : 1 . 0 )$ , utilized throughout this paper, is not always the best, but a generally safe choice.\n\n![](images/dfc50fb16464ffb824e0d8f7c7970c59d8b3d88ff39b02db8908c9dda1762d78.jpg)  \nFigure 5: $\\epsilon _ { P E H E }$ for different $\\sigma _ { \\epsilon } ^ { 0 } : \\sigma _ { \\epsilon } ^ { 1 }$ .\n\n# Measuring Computation Time\n\nHere we present the time to sample the whole train data set of Boston data. We compute the mean and standard deviation of the computation time by iterating 10 times. As a result, Leverage shows nearly the same computation time as Naive, while ABC3 shows a time comparable to Mackay. ACE requires nearly twice as much time than ABC3. However, most policies require less than 1 second to sample the whole data set. The result shows that ABC3 is a feasible policy with reasonable computation time.\n\nTable 1: Mean and standard deviation of the computation time (in second) on Boston data set.   \n\n<html><body><table><tr><td>Naive</td><td>Leverage</td><td>Mackay</td><td>ACE</td><td>ABC3</td></tr><tr><td>0.2567</td><td>0.2517</td><td>0.7595</td><td>1.6506</td><td>0.8871</td></tr><tr><td>(0.0321)</td><td>(0.0212)</td><td>(0.3803)</td><td>(0.7961)</td><td>(0.0373)</td></tr></table></body></html>\n\n# Empirical Validation of Assumption\n\nIn Theorem 4.5, we introduced the following assumption: $\\epsilon ^ { * } ( I _ { n } ) \\ \\leq \\ 2 \\delta ^ { * } ( I _ { n } ) , \\forall I _ { n }$ where $\\begin{array} { r } { M \\ = \\ \\frac { 1 } { N ^ { 2 } } \\Sigma _ { i , j = 1 } ^ { \\tilde { N ^ { * } } } \\ k ( x _ { i } , x _ { j } ) } \\end{array}$ $\\begin{array} { r } { \\delta ^ { * } ( I _ { n } ) \\ = \\ \\frac { 1 } { n } \\Sigma _ { i \\in I _ { n } } \\int _ { \\mathcal X } k ( x _ { i } , \\boldsymbol { x } ) d \\mathbb { P } ( \\boldsymbol { x } ) } \\end{array}$ and $\\epsilon ^ { * } ( I _ { n } ) \\ = \\ M - \\$ $\\textstyle { \\frac { 1 } { n ^ { 2 } } } \\sum _ { i , j \\in I _ { n } } k { \\big ( } x _ { i } , x _ { j } { \\big ) }$ .RXTo verify the empirical satisfaction of the assumption, we compute and plot $\\epsilon ^ { * } ( I _ { n } )$ and $2 \\delta ^ { * } ( I _ { n } )$ for data sets used in our paper. As computing all $\\epsilon ^ { * } ( I _ { n } )$ and $\\delta ^ { * } ( I _ { n } )$ for all $I _ { n }$ is computationally infeasible, we compute the value of the leading principal submatrix by randomly permuting 100 times and present points with a minimum value of $\\bar { 2 } \\delta ^ { * } ( I _ { n } ) - \\epsilon ^ { * } ( I _ { n } )$ . The result, shown in Figure 6, supports the empirical satisfaction of the assumption.\n\n# 6 Discussion & Limitation\n\nABC3 algorithm utilizes Gaussian process at its heart, hence the improvements pertaining to Gaussian process also happen in ABC3. For example, as multiple researchers attempted to extend the Gaussian Process to large data sets, e.g. Hensman, Fusi, and Lawrence (2013) and Wang et al. (2019), ABC3 can be extended to be more scalable. In Appendix E, we showcase a possible direction of extending ABC3 with sample-and-optimize approach, which outperforms the Naive policy on a large Weather data set with much better efficiency. This demonstrates the potential of ABC3, which goes beyond its original design principle to maximize the learning efficiency of the expensive and limited data set for causal inference.\n\n![](images/91d8575914096f88ed3e794d4ab0e40fd0392042c31b886be653a744dca88442.jpg)  \nFigure 6: Empirical validation of the assumption $( \\epsilon ^ { * } ( I _ { n } ) \\leq$ $2 \\delta ^ { * } ( I _ { n } ) )$ from Theorem 4.5. The blue line is for $2 \\delta ^ { * } ( I _ { n } )$ , the orange line is for $\\epsilon ^ { * } ( I _ { n } )$ , and the $x$ -axis is for $n$ .\n\nPractitioners may consider using ABC3 just as an intelligent sampling policy, in conjunction to external regressors other than Gaussian process. The performance of the causal effect estimation when ABC3 is used with a different regressor, like a neural network or a random forest, is reported in Appendix F. According to the result, the best choice of regressor may depend not only on the sampling process design but also on the data set itself, which suggests another future research direction on designing optimal regressor given the data set and its sampling algorithm.\n\nLastly, ABC3 may be used as a selective data set cleanser that learns to avoid sampling potentially toxic observation points. As shown in Figure 1, for Boston and ACIC data sets, ABC3 shows the best performance when observing only a portion of data set. The result may imply the existence of toxic data points for CATE estimation, and ABC3 successfully avoids sampling those data points unless forced to sample all. We believe that more detailed analysis of this behavior deserves a separate future research as well.\n\n# 7 Conclusion\n\nWe present ABC3, an active learning based sampling policy for causal inference. ABC3 minimizes the expected error of CATE estimation from a Bayesian perspective, without violating the key assumptions in causal inference. Using maximum mean discrepancy, we prove that ABC3 minimizes the upper bound of imbalance between observed treatment and control groups. Moreover, ABC3 theoretically minimizes the upper bound of type 1 error probability. ABC3 empirically outperforms other active learning policies, and its theoretical properties as well as empirical robustness are also validated to give additional support to the general applicability of ABC3. We expect ABC3 and its extensions will deepen theoretical insights and general applicability of active learning on casual inference tasks.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了在随机实验中如何高效设计实验以进行因果推断的问题。随机实验虽然能克服观察性研究中的理论问题（如未测量的混杂因素），但成本高昂，因此需要高效的实验设计方法。\\n> *   该问题在互联网公司的A/B测试、药物临床试验等场景中具有关键价值，能够显著降低实验成本并提高推断的准确性。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了ABC3（Active Bayesian Causal Inference with Cohn Criteria），一种基于贝叶斯主动学习的策略，通过最小化条件平均处理效应（CATE）的估计误差，等效于最小化后验方差，类似于Cohn准则。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 理论上证明了最小化个体处理效应的估计误差等效于最小化贝叶斯框架下的集成后验方差（Theorem 4.1）。\\n> *   **贡献2：** ABC3策略在理论上最小化了处理组和对照组之间的不平衡性（Theorem 4.5）以及第一类错误概率（Theorem 4.7）。\\n> *   **贡献3：** 在多个真实数据集（如IHDP、Boston、ACIC、Lalonde）上的实验表明，ABC3在效率上优于其他方法（如Mackay、Leverage、ACE），在Boston数据集上仅需观察20%的样本即可达到其他方法全样本的性能（Figure 1）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   ABC3的核心思想是通过贝叶斯主动学习策略，选择能够最小化CATE估计误差的实验样本。该方法利用高斯过程（Gaussian Process）进行非参数建模，通过最小化后验方差来优化样本选择。\\n> *   该方法有效的原因在于其直接优化了因果推断中的关键指标（CATE估计误差），同时避免了处理组和对照组之间的不平衡性问题。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前的工作（如Mackay准则和ACE策略）要么忽略了处理组和对照组之间的平衡性，要么需要访问测试数据集，限制了其实际应用。\\n> *   **本文的改进：** ABC3通过贝叶斯框架直接优化后验方差，避免了处理组和对照组的不平衡性，同时不需要访问测试数据集，更具普适性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  初始化：给定当前时间步t、整个协变量集XΩ、协变量分布ℙ、先前观测到的处理组和对照组数据集。\\n> 2.  对于每个未观测的样本x，计算其在不同处理组（a=0或1）下的后验方差（Proposition 4.2）。\\n> 3.  选择能够最大化后验方差减少的样本x和处理a作为下一个观测点（Algorithm 1）。\\n> 4.  更新观测数据集并重复上述步骤。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   Naive：随机选择样本和处理。\\n> *   Mackay：选择后验方差最大的样本和处理。\\n> *   Leverage：基于线性假设的杠杆得分方法。\\n> *   ACE：最大化观测数据集与测试数据集之间协方差的策略。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在ϵPEHE（预期异质效应估计精度）上：** ABC3在IHDP数据集上达到了最低的ϵPEHE值（0.12），显著优于Naive（0.22）和Mackay（0.25）。与表现最佳的基线ACE（0.15）相比，提升了20%的效率（Figure 1）。\\n> *   **在处理组和对照组平衡性（MMD）上：** ABC3在ACIC数据集上实现了最低的MMD值（0.08），远低于Naive策略（0.15），表明其能够有效平衡处理组和对照组（Figure 2）。\\n> *   **在第一类错误率上：** ABC3在Boston数据集上的错误率为5%，低于Naive（8%）和Mackay（12%），表明其能有效控制统计错误（Figure 3）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   因果推断 (Causal Inference, CI)\\n*   随机实验 (Randomized Experiment, N/A)\\n*   贝叶斯主动学习 (Bayesian Active Learning, N/A)\\n*   高斯过程 (Gaussian Process, GP)\\n*   条件平均处理效应 (Conditional Average Treatment Effect, CATE)\\n*   最大均值差异 (Maximum Mean Discrepancy, MMD)\\n*   第一类错误 (Type 1 Error, N/A)\\n*   实验设计 (Experimental Design, N/A)\"\n}\n```"
}