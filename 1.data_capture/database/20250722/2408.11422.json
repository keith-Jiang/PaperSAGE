{
    "link": "https://arxiv.org/abs/2408.11422",
    "pdf_link": "https://arxiv.org/pdf/2408.11422",
    "title": "Scenario-Based Robust Optimization of Tree Structures",
    "authors": [
        "Spyros Angelopoulos",
        "Christoph Durr",
        "Alex Elenter",
        "Georgii Melidi"
    ],
    "publication_date": "2024-08-21",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "paper_content": "# Scenario-Based Robust Optimization of Tree Structures\n\nSpyros Angelopoulos2, 3, Christoph Du¨ rr1, 2, Alex Elenter1, Georgii Melidi1\n\n1Sorbonne University, LIP6, Paris, France 2CNRS 3International Laboratory on Learning Systems, Montreal, Canada spyros.angelopoulos, christoph.durr, alex.elenter, georgii.melidi @lip6.fr\n\n# Abstract\n\nWe initiate the study of tree structures in the context of scenario-based robust optimization. Specifically, we study Binary Search Trees (BSTs) and Huffman coding, two fundamental techniques for efficiently managing and encoding data based on a known set of frequencies of keys. Given a number of distinct scenarios, each defined by a frequency distribution over the keys, our objective is to compute a single tree of best-possible performance, relative to any scenario.\n\nWe consider, as performance metrics, the competitive ratio, which compares multiplicatively the cost of the solution to the tree of least cost among all scenarios, as well as the regret, which induces a similar, but additive comparison. For BSTs, we show that the problem is NP-hard across both metrics. We also obtain an optimal competitive ratio that is logarithmic in the number of scenarios. For Huffman Trees, we likewise prove NP-hardness, and we present an algorithm with logarithmic regret, which we prove to be near-optimal by showing a corresponding lower bound. Last, we give a polynomialtime algorithm for computing Pareto-optimal BSTs with respect to their regret, assuming scenarios defined by uniform distributions over the keys. This setting captures, in particular, the first study of fairness in the context of data structures. We provide an experimental evaluation of all algorithms. To this end, we also provide mixed integer linear program formulation for computing optimal trees.\n\n# Code — https://gitlab.lip6.fr/gmelidi/robust-tree Extended version — https://arxiv.org/pdf/2408.11422\n\n# 1 Introduction\n\nSuppose that we would like to encode and transmit a text in a given language efficiently, i.e., using the least number of bits on expectation. If the alphabet’s frequency is known ahead of time, i.e., if the language is pre-determined, this can be done efficiently using the well-known technique of Huffman coding (Huffman 1952). But what if we do not know in advance the intended language, but instead it is only known that it can be either English, Italian, or Finnish? In this case, one would like to design a single code that performs efficiently across all three such scenarios, and in particular against a worst-case, adversarially chosen language.\n\nFor a second example, suppose that we are given $n$ keys to be stored in a Binary Search Tree (BST), and that we would like to minimize the expected number of comparisons, when performing a search operation for a key. Once again, if the key frequencies are known ahead of time, finding an optimal BST is a fundamental problem in Computer Science, going back to the seminal work of Knuth (Knuth 1971) who gave a quadratic-time algorithm. Suppose, however, that instead of a single frequency vector, we are provided with $k$ different vectors, each defining one possible scenario. How would we construct a single BST that performs well across all scenarios, hence also against one chosen adversarially?\n\nMotivated by the above situations, in this work we introduce the study of robust structures, in the presence of $k$ possible scenarios. Each scenario is described by means of a frequency vector over the keys, and we seek a single tree that is robust with respect to any scenario (and thus with respect to the worst-case, adversarial scenario). This approach falls into what is known as scenario-based robust optimization; see (Ben-Tal, El Ghaoui, and Nemirovski 2009) for a survey of this area. While the scenario-based framework has been studied in the context of AI-related optimization problems in areas such as planning (McCarthy, Vayanos, and Tambe 2017), scheduling (Shabtay and Gilenson 2023) and network optimization (Kasperski and Zielin´ski 2015; Kasperski, Kurpisz, and Zielin´ski 2015), to our knowledge it has not been applied to data structures or data coding.\n\nWe consider two main measures for evaluating the quality of our solutions. The first measure applies to BSTs, and is the worst-case ratio, among all scenarios $s$ , between the cost of our solution (tree) under scenario $s$ and the cost of the optimal tree under $s$ ; using the canonical term from online computation, we refer to this measure as the competitive ratio. For Huffman trees, we consider even stronger guarantees, by studying the worst-case difference, among all scenarios $s$ between the cost of our solution under $s$ and the cost of the optimal tree for $s$ , namely the regret of the solution. We refer to Sections 2.1 and 3.1 for the formal definitions. Competitive analysis and regret minimization are both well-studied approaches in optimization under uncertainty, that establish strict, worst-case guarantees under adversarial situations (Borodin and El-Yaniv 2005), (Blum and Mansour 2007). Competitive analysis is the predominant analysis technique for tree-based data structures, see the seminal work (Sleator and Tarjan 1985) on Splay Trees. Regret minimization, on the other hand, may provide more stringent guarantees (c.f. Corollary 3.5), and notions related to regret have been applied, for instance, to the evaluation of query efficiency in databases (Xie, Wong, and Lall 2020), (Nanongkai et al. 2010).\n\n# 1.1 Contribution\n\nWe begin with the study of robust BSTs in Section 2. We first show that minimizing either the competitive ratio or the regret is NP-hard, even if there are only two scenarios, i.e., $k = 2$ (Theorem 2.3). We also give an algorithm that constructs a BST of competitive ratio at most $\\lceil \\log _ { 2 } ( k + 1 ) \\rceil$ (Theorem 2.4), and we show that this bound is optimal, in that there exists a collection of $k$ scenarios under which no BST can perform better (Theorem 2.6).\n\nIn Section 3, we study robust Huffman trees (HTs). We first show that the problem of minimizing the competitive ratio or the regret is NP-hard, again even if $k = 2$ (Theorem 3.1). We also give an algorithm for constructing a Huffman tree that has regret that is provably at most $\\left\\lceil \\log _ { 2 } k \\right\\rceil$ . We show that this is essentially optimal, by proving a nearmatching lower bound equal to $\\lfloor \\log _ { 2 } k \\rfloor$ (Theorem 3.4).\n\nIn Section 4, we study the problem of minimizing BST regret with respect to $k$ scenarios. This problem can be formulated as a $k$ -objective optimization problem, hence we seek trees in the Pareto-frontier of the $k$ -objectives. For concreteness, we focus on scenarios induced by uniform distributions over subsets of keys: here, we give a polynomial-time algorithm for finding Pareto-optimal solutions. This formulation provides the first framework for quantifying fairness in the context of data structures. More precisely, we can think of each distinct scenario as the profile of a different user, and the BST as the single resource that is shared by all $k$ users. We thus seek a solution that distributes the cost as equitably as possible among the $k$ competing users.\n\nIn Section 5, we provide an experimental evaluation of all our algorithms over real data. To evaluate and compare our algorithms from Sections 2 and 3, we also provide mixed integer linear program formulations for all objectives, which allows us to compute the optimal trees for small instances.\n\nIn terms of techniques, despite the seeming similarity of the settings, we show that robust BSTs and HTs are quite different problems; this is due to the fact that in the former the keys are stored in all nodes, whereas in the latter they are stored in leaves. This is reflected both in the different NP-hardness proofs and in the different algorithmic approaches. Namely, for BSTs, NP-hardness is established using a non-trivial reduction from the PARTITION problem (Theorem 2.3), whereas for HTs we use a non-trivial reduction from the EQUAL-CARDINALITY PARTITION problem (Theorem 3.1). From the algorithmic standpoint, for BSTs we follow a recursive approach for constructing the tree, whereas for HTs we use an approach that allows us to “aggregate” the optimal HT for each scenario into a single HT. Last, in terms of finding the Pareto-frontier for regret minimization in BSTs, we use a dynamic programming approach that allows for an efficient implementation. We refer to (Angelopoulos et al. 2024) for the full paper version.\n\n# 1.2 Related Work\n\nThe BST is a fundamental data structure that has been studied extensively since the 1960s. See (Windley 1960; Booth and Colin 1960; Hibbard 1962) for some classic references and (Nagaraj 1997) for a survey. Given a frequency vector of accesses to $n$ different keys, finding a BST of optimal average access cost was originally solved in (Knuth 1971). Likewise, Huffman coding (Huffman 1952) is a fundamental technique for lossless data compression, which combines optimality of performance and simplicity of implementation. Given a frequency vector over an $n$ -sized alphabet, a Huffman tree can be implemented in time $O ( n \\log { n } )$ using a priority queue. We refer to (Moffat 2019) for a survey.\n\nA recent, and very active related direction in Machine Learning seeks to augment data structures with some prediction concerning the future access requests. Examples of learning-augmented data structures that have been studied include skip lists (Fu, Seo, and Zhou 2024; Zeynali, Kamali, and Hajiesmaili 2024), BSTs (Lin, Luo, and Woodruff 2022; Cao et al. 2022), B-trees (Kraska et al. 2018) and rank/select dictionaries (Boffa, Ferragina, and Vinciguerra 2022); see also (Ferragina and Vinciguerra 2020) for a survey on the applications of ML in data structures. These works leverage a learned prediction about access patterns, and seek structures whose performance degrades smoothly as a function of the prediction error. The settings we study in this work can thus be interpreted, equivalently, as having access to $k$ different predictions, and seeking structures that perform efficiently even if the worst-case prediction materializes.\n\nIt is known that if the access frequencies are chosen uniformly at random, then with high probability the optimal BST is very close to a complete binary search tree, and its expected height is logarithmic in the number of keys $n$ . In the other extreme, there exist adversarial frequencies for which the optimal BST has height that is as large as $\\Omega ( n )$ . Several works have studied the regime between these extremes, via small, random perturbations to the frequency vector, e.g., (Manthey and Reischuk 2007). Bicriteria optimization problems over BSTs were studied in (Mankowski and Moshkov 2020), with the two objectives being the maximum and the average weighted depth, respectively.\n\nScenario-based robust optimization has been extensively applied to the study of scheduling problems under uncertainty. Examples include the minimization of completion time (Mastrolilli, Mutsanas, and Svensson 2013), flow-shop scheduling (Kasperski, Kurpisz, and Zielinski 2012) and just-in-time scheduling (Gilenson and Shabtay 2021). We refer to (Shabtay and Gilenson 2023) for a recent survey of many results related to scenario-based scheduling.\n\nWe conclude with a discussion of fairness, which is becoming an increasingly demanded requirement in algorithm design and analysis. Algorithmic fairness, defined as an equitable treatment of users that compete for a common resource, has been studied in some classic settings, including optimal stopping problems (Arsenis and Kleinberg 2022), (Buchbinder, Jain, and Singh 2009), and resource allocation problems such as knapsack (Lechowicz et al. 2024; Patel, Khan, and Louis 2021). However, to our knowledge, no previous work has addressed the issue of fairness in the context of data structures, although the problem is intrinsically well-motivated: e.g., we seek a data structure (such as a BST) that guarantees an equitable search time across the several competing users that may have access to it. The concept of Pareto-dominance as a criterion for fairness has attracted attention in recent works in ML, e.g., (Martinez et al. 2021), (Martinez, Bertran, and Sapiro 2020). In our work, we do rely on learning oracles, and we seek Pareto-optimal solutions that capture group fairness based on regret.\n\n# 2 Robust Binary Search Trees 2.1 Background and Measures\n\nIn its standard form, a BST stores $n$ keys from a given ordered set; without loss of generality, we may assume that the set of keys is the set $\\{ 1 , \\ldots , n \\}$ . The keys are stored in the nodes, and satisfy the ordering property: the key of any node is larger than all keys in its left sub-tree, and smaller than all keys in its right sub-tree. For a given key, the corresponding node is accessed in a recursive manner, starting from the root, and descending in the tree guided by key comparisons.\n\nA BST can be conveniently represented, equivalently, by its level vector, denoted by $L$ , in the sense that every key $i$ has level $L _ { i }$ in the tree. Here we use the convention that the root has level 1. Formally, we have:\n\nDefinition 2.1. A vector $L \\in \\{ 1 , \\ldots , n \\} ^ { n }$ is a level vector of a BST if and only if for every $1 \\leqslant i < j \\leqslant n$ with $L _ { i } = L _ { j }$ , there is a key $i < r < j$ such that $L _ { r } < L _ { i }$ .\n\nThe definition formulates the fact that if there are two keys at the same level of the BST, then there must exist a key between them of lower level (i.e., higher in the tree). This definition allows us to express the average cost of a BST represented by a level vector $L$ , relative to a frequency vector $F$ , as the inner product\n\n$$\n\\operatorname { c o s t } ( L , F ) = \\sum _ { i = 1 } ^ { n } L _ { i } \\cdot F _ { i } .\n$$\n\nGiven a frequency vector $F$ , a level vector $L$ minimizing $\\mathrm { c o s t } ( L , F )$ can be computed, using dynamic programming, in time $O \\dot { ( } n ^ { 2 } )$ (Knuth 1971).\n\nNext, we define formally the robust BST problem. Its input consists of $k$ frequency vectors $F ^ { 1 } , \\ldots , F ^ { k }$ , F k, called scenarios. There are three possible performance metrics one could apply in order to evaluate the performance, which give rise to three possible minimization problems on the BST level vector $L$ that must be found:\n\nWorst-case cost: here, the objective is to minimize $\\operatorname* { m a x } _ { s } \\cos ( L , F ^ { s } )$ , i.e., we seek the tree of smallest cost under the worst-case scenario.\n\nCompetitive ratio: here, we aim to minimize\n\n$$\n\\operatorname* { m a x } _ { s } \\frac { \\cos ( L , F ^ { s } ) } { \\operatorname* { m i n } _ { L ^ { * } } \\cos \\mathrm { t } ( L ^ { * } , F ^ { s } ) } ,\n$$\n\ni.e., we aim to minimize the worst-case ratio between the cost of our tree and the optimal tree for each scenario.\n\nRegret: here, the objective is to minimize the quantity $\\operatorname* { m a x } _ { s } \\{ \\mathrm { c o s t } ( L , F ^ { s } ) - \\operatorname* { m i n } _ { L ^ { * } } \\mathrm { c o s t } ( L ^ { * } , F ^ { s } ) \\} ,$\n\ni.e., we want to achieve the smallest difference between the cost of our tree and the optimal tree for each scenario.\n\nNote that if for each scenario the respective optimal trees have the same cost, then the three metrics are equivalent. However, in general, they may be incomparable, as shown in the following example.\n\nExample 2.2. Let $\\begin{array} { r l r } { F ^ { 1 } } & { { } = } & { ( 0 , 1 / 4 , 3 / 4 ) } \\end{array}$ and $\\begin{array} { r l } { F ^ { 2 } } & { { } = } \\end{array}$ $( 4 / 9 , 2 / 9 , 1 / 3 )$ denote two scenarios for three keys $\\{ a , b , c \\}$ . The various metrics for each possible BST are as depicted.\n\n<html><body><table><tr><td></td><td>a bc</td><td>bC a</td><td>c ab</td><td>b a</td><td>C a</td></tr><tr><td>F1</td><td>11/4</td><td>9/4</td><td>7/4</td><td>3/2</td><td>5/4</td></tr><tr><td>F2</td><td>17/9</td><td>16/9</td><td>16/9</td><td>17/9</td><td>19/9</td></tr><tr><td>cost</td><td>11/4</td><td>9/4</td><td>16/9</td><td>17/9</td><td>19/9</td></tr><tr><td>c.ratio</td><td>11/5</td><td>9/5</td><td>7/5</td><td>6/5</td><td>19/16</td></tr><tr><td>regret</td><td>3/2</td><td>1</td><td>1/2</td><td>1/4</td><td>1/3</td></tr></table></body></html>\n\nThe first two rows of the table show the cost of each of the five possible BSTs on three keys, for the two scenarios $F ^ { 1 }$ and $F ^ { 2 }$ ; here the optimal costs for each scenario are highlighted in gray. The remaining three columns show the performance of each tree with respect to the three metrics, with the best performance highlighted in yellow.\n\nOur hardness results, for both BSTs and HTs, apply to all three metrics. However, for the purposes of analysis, we focus on the competitive ratio and the regret. As discussed in Section 1, the competitive ratio is the canonical performance notion in the analysis of BSTs, and allows us to capture worst-case performance under uncertainty that can be efficiently approximated, both from the point of view of upper bounds (positive results) and lower bounds (impossibility results). In other words, the competitive ratio reflects the price of not knowing the actual scenario in advice, similar to the competitive analysis of online algorithms (Borodin and El-Yaniv 2005). On the other hand, regret-minimization can provide even stronger guarantees for HTs, but can also help model issues related to fairness in BSTs, as we discuss in detail in Section 4.\n\n# 2.2 Results\n\nWe first show that finding an optimal robust BST is NPhard, even if $k = 2$ . We will prove the result for the costminimization version; however the proof carries over to the other metrics.\n\nTo prove NP-hardness, we first need to formulate the decision variant of the problem: Given two frequency vectors, $F ^ { 1 }$ and $F ^ { 2 }$ , and a threshold $V$ , the objective is to decide whether there exists a BST $L$ of cost at most $V$ , in either scenario, i.e., $\\operatorname { c o s t } ( L , F ^ { 1 } ) \\leq V$ and $\\operatorname { c o s t } ( L , F ^ { 2 } ) \\leq V$ .\n\nTheorem 2.3. The robust BST problem is NP-hard, even $i f$ $k = 2$ . This holds for all three metrics, i.e., for minimizing the cost, or the competitive ratio, or the regret.\n\nProof. The proof is based on a reduction from the PARTITION problem (Garey and Johnson 1979). An instance of this problem consists of a list $a _ { 1 } , . . . , a _ { m }$ of non-negative integers, and the goal is to decide whether there exists a binary vector $b \\in \\{ 0 , 1 \\} ^ { m }$ with $\\begin{array} { r } { \\sum _ { i } b _ { i } a _ { i } \\ = \\ \\sum _ { i } ( 1 - b _ { i } ) a _ { i } } \\end{array}$ .\n\n![](images/156372f48c64f401bbefb679660203ee9a8e426416d620523d8267306ef70404.jpg)  \nFigure 1: The BST corresponding to the binary vector $b =$ $\\{ \\bar { 1 1 } 0 1 0 0 1 0 \\}$ in the NP-hardness proof construction. Nodes are labeled with the frequency of their keys in $F ^ { 1 }$ .\n\nWithout loss of generality we can assume that $m$ is of the form $m = 2 ^ { \\ell }$ for some integer $\\ell .$ , as we can always pad the instance with zeros.\n\nFrom the given instance of the partition problem, we define an instance of the robust BST problem which consists of $\\ n \\ = \\ 3 m \\ - \\ 1$ keys and two frequency vectors $F ^ { 1 } ~ = ~ ( a _ { 1 } , 0 , w , a _ { 2 } , 0 , w , a _ { 3 } , 0 , . . . , w , a _ { m } , 0 )$ and ${ \\cal F } ^ { 2 } \\ = \\ $ $( 0 , a _ { 1 } , w , 0 , a _ { 2 } , w , 0 , a _ { 3 } , \\ldots , w , 0 , a _ { m } )$ , where $w$ is any constant strictly larger than $\\textstyle ( \\ell + 3 / 2 ) \\sum _ { i } a _ { i }$ . Moreover, the instance specifies the threshold $\\begin{array} { r } { \\dot { V } = \\overline { { W } } + ( \\sum a _ { i } ) / 2 } \\end{array}$ , where $W$ is defined as\n\n$$\nW : = w \\sum _ { j = 1 } ^ { \\ell } j 2 ^ { j - 1 } + ( \\ell + 1 ) \\sum _ { i = 1 } ^ { n } a _ { i } .\n$$\n\nWe claim that the level vector $L$ which optimizes $\\operatorname* { m a x } \\{ \\mathrm { c o s t } ( L , F ^ { 1 } ) , \\mathrm { c o s t } ( L , F ^ { 2 } ) \\}$ has the following structure. The first $\\ell$ levels form a complete binary tree over all keys $3 i$ for $i = 1 , 2 , \\dots , m$ . These are exactly the keys with frequency $w$ in both $F ^ { 1 }$ and $F ^ { 2 }$ . (Intuitively, since $w$ is large, these keys should be placed at the smallest levels.) Furthermore, for each $i = 1 , 2 , \\dots , m$ , the keys $3 i - 2$ and $3 i - 1$ are placed at levels $\\ell + 1$ and $\\ell + 2$ , or at levels $\\ell + 2$ and $\\ell { + } 1$ , respectively. Such a tree has a cost at least $W$ in each scenario and at most $W + \\textstyle \\sum a _ { i }$ . The claim that $L$ has this structure follows from the fact that if some key with frequency $w$ were to be placed below level $\\ell$ , then the tree would incur a cost of at least $\\textstyle w ( 1 + \\sum _ { j = 1 } ^ { \\ell } j 2 ^ { j - 1 } )$ , which is strictly greater than $V$ . See Figure 1 for an illustration.\n\nAs a result, all solutions to the robust BST problem can be described by a binary vector $b \\in \\{ 0 , 1 \\} ^ { m }$ , such that for every $i = 1 , 2 , \\dots , m$ the key $3 i - 2$ has level $\\ell + 1 + b _ { i }$ while key $3 i - 1$ has level $\\ell + 2 - b _ { i }$ . We denote by $L ^ { b }$ the level vectors of these trees in order to express the costs as $\\mathrm { c o s t } ( L ^ { b } , F ^ { 1 } ) =$ $W + \\textstyle \\sum _ { i } b _ { i } a _ { i }$ and $\\begin{array} { r } { \\mathrm { c o s t } ( L ^ { b } , \\mathbf { \\dot { F } } ^ { 2 } ) = { W } + \\sum _ { i } ( 1 - b _ { i } ) a _ { i } } \\end{array}$ .\n\nObserve that if both costs of $L ^ { b }$ are at most $V$ , then they are equal to this value and $b$ is a solution to the partition problem. This implies that deciding if there exists a BST of cost at most $V$ is NP-hard. □\n\nNext, we present an algorithm that achieves the optimal competitive ratio. Algorithm 1 first computes optimal trees with level vector $L ^ { s }$ for each scenario $s = 1 , \\ldots , k$ , then calls a recursive procedure $A$ with initial values $\\textit { i } = 1$ , $j = n$ and $\\ell = 1$ . Procedure $A$ solves a subproblem defined by $i , j , \\ell .$ . Namely, it constructs a BST on the keys $[ i , j ]$ and stores its root at level $\\ell$ . To this end, it first identifies\n\n# Algorithm 1: R-BST\n\nInput: $k$ scenarios described by frequency vectors $F ^ { \\hat { 1 } } , \\ldots , F ^ { k }$   \nOutput: A robust BST represented as a level vector $L$ . 1: For each scenario $s \\in \\{ 1 , \\ldots , k \\}$ compute the optimal tree $L ^ { s }$ 2: $A ( i = 1 , j = n , \\ell = 1 )$\n\n# Algorithm 2: Recursive procedure $A$\n\n1: procedure $\\mathbf { A } ( i , j , \\ell )$ 2: if $i > j$ then 3: return $D$ Empty interval $[ i , j ]$ ends recursion 4: else 5: Let $v$ be the minimum level $L _ { r } ^ { s }$ over $i \\leqslant r \\leqslant j$ and $1 \\leqslant s \\leqslant k$ . 6: Let $S$ be the ordered set of keys $i \\leqslant r \\leqslant j$ such that there exists a scenario $s$ with $L _ { r } ^ { s } = v$ . 7: Let $m \\in S$ be s.t. both $\\vert S \\cap \\lbrack i , m - 1 \\rbrack \\vert$ and $| S \\cap$ $[ m + 1 , j ] |$ are at most $\\lceil ( | S | - \\mathrm { i } ) / 2 \\rceil$ . 8: Set $L _ { m } = \\ell$ 9: $\\mathbf { A } ( i , m - 1 , \\ell + 1 )$ ▷ Left recursion 10: $\\mathbf { A } ( m + 1 , j , \\ell + 1 )$ $D$ Right recursion 11: end if 12: end procedure\n\nthe key(s) in $[ i , j ]$ at the minimum level in the optimal trees across all scenarios $L ^ { s }$ (lines 5 and 6) and stores them in the ordered set $S$ . If an odd number of keys share this minimum level, the algorithm selects the key to be stored at level $\\ell$ as the median of the set $S$ . Otherwise, it chooses the key closest to the median value (line 7).\n\nWe show that R-BST has competitive ratio logarithmic in $k$ :\n\nTheorem 2.4. For every scenario s, R-BST constructs a BST of cost at most $\\lceil \\log _ { 2 } ( k + 1 ) \\rceil$ times the cost of the optimal BST $L ^ { s }$ .\n\nWe can implement R-BST so that it runs in time $O ( k n ^ { 2 } )$ , i.e., the run time is dominated by the time required to compute the optimal BSTs for each scenario.\n\nTheorem 2.5. R-BST can be implemented in time $O ( k n ^ { 2 } )$ .\n\nWe conclude this section by showing that R-BST achieves the optimal competitive ratio.\n\nTheorem 2.6. There exists a collection of $k$ scenarios, described by vectors $F ^ { 1 } , \\ldots , F ^ { k }$ , such that for every BST with level vector $L$ , there exists a scenario s for which $c o s t ( L , F ^ { s } ) \\geqslant \\lceil \\log _ { 2 } ( k + 1 ) \\rceil c o s t ( L ^ { s } , F ^ { s } )$ .\n\n# 3 Robust Huffman Trees\n\n# 3.1 Background and Measures\n\nIn the standard version of the Huffman tree problem, we are given $n$ keys (e.g., letters of an alphabet), along with a frequency vector (e.g., the frequency of each letter in said alphabet). The objective is to build a binary tree in which every key corresponds to a leaf, so as to minimize the inner product $\\textstyle \\sum _ { i } F _ { i } ^ { \\bar { \\boldsymbol { \\mathbf { \\mathstrut } } } } L _ { i }$ , where $\\mathbf { } L _ { i }$ is the level of key $i$ in the tree.\n\nBy labeling the edges leaving each inner node with $0 , 1$ , arbitrarily, we can associate a codeword to each key, namely the concatenation of the edge labels on the path from the root to the leaf corresponding to the key in question. In this setting, we assume that the levels of the tree start from 0. Hence, conveniently, the length of a codeword equals the level of the corresponding leaf. The resulting set of codewords is prefix free, in the sense that no codeword is a prefix of another codeword. The optimal tree can be computed in time $O ( n \\log n )$ (Huffman 1952).\n\nWe study Huffman coding in a robust setting in which we are given $k \\ n$ -dimensional frequency vectors $F ^ { 1 } , \\ldots , F ^ { k }$ each describing a different scenario. We say that a level vector $L$ is valid if there is a prefix-free code such that key $i$ has a codeword of length $L _ { i }$ . In other words, for any valid level vector $L$ , there is a Huffman tree with $| \\{ i : L _ { i } \\stackrel { . } { = } a \\} |$ leaves on level $a$ . The cost of $L$ in a given scenario $s$ is defined as the inner product $\\begin{array} { r } { F ^ { s } \\cdot L = \\sum _ { i } ^ { - } F _ { i } ^ { s } L _ { i } } \\end{array}$ .\n\nWe can analyze robust HTs using the same measures as for robust BSTs. For Huffman trees, in particular, we will rely on regret-based analysis, which establishes more refined performance guarantees than competitive ratio; c.f. Corollary 3.5 which shows that our results for regret essentially carry over to the competitive ratio.\n\n# 3.2 Results\n\nWe begin by showing that finding an optimal robust HT is NP-hard, even in the case of only two scenarios. The proof differs substantially from the NP-hardness proof for robust BSTs (Theorem 2.3). This is due to the differences in the two settings: in a BST, keys are stored in each node, whereas in a HT, keys are stored only at leaf nodes. As a result, the reduction is technically more involved, and is from a problem that induces more structure, namely the EQUAL-CARDINALITY PARTITION problem (Garey and Johnson 1979).\n\n# Theorem 3.1. The robust HT problem is NP-hard, even if $k = 2$ . This holds for all three metrics, i.e., for minimizing the cost, the competitive ratio, or the regret.\n\nWe propose and analyze an algorithm called R-HT for minimizing $k$ -scenario regret. The idea is to aggregate the optimal trees for each scenario into a single HT. The algorithm initially computes optimal HTs for each scenario $s$ , denoted as $T ^ { s }$ (line 2). For each key $i = 1 , \\ldots , n$ , it identifies the scenario $s$ with the shortest code, denoted as $c _ { i } ^ { s } \\in \\{ 0 , 1 \\} ^ { * }$ (line 6). Next, it prepends exactly $\\left\\lceil \\log _ { 2 } k \\right\\rceil$ bits to each code $c _ { i } ^ { s }$ , which represent the scenario $s$ (lines 7 and 8). The algorithm generates the final HT by associating each key with a level equal to $\\lceil \\log _ { 2 } k \\rceil + c _ { i } ^ { s }$ (line 10). In line 11, “compactification” refers to a process which we describe informally, for simplicity. That is, while there is an inner node $u$ of the HT with outdegree 1 (meaning it has a single descendant $\\boldsymbol { v }$ ) we contract the edge $u , v$ .\n\nTheorem 3.2. Algorithm R-HT outputs a tree $T$ with a valid level vector $L$ of regret at most $\\left\\lceil \\log _ { 2 } k \\right\\rceil$ . That is, $F ^ { s } \\cdot L \\leqslant$ $\\begin{array} { r } { \\operatorname* { m i n } _ { L ^ { * } } F ^ { s } \\cdot L ^ { * } + \\lceil \\log _ { 2 } k \\rceil } \\end{array}$ , for every scenario $s$ .\n\nProposition 3.3. Algorithm R-HT can be implemented in time $O ( k n \\log n )$ , i.e., its runtime is dominated by the time required to find optimal HTs for each scenario.\n\nInput: $k$ scenarios described by frequency vectors ${ \\cal F } ^ { 1 } , \\ldots , { \\cal F } ^ { k }$ . Output: A robust Huffman tree $T$ .   \n1: for all scenarios $s$ do   \n2: Let $T ^ { s }$ be the Huffman tree with minimum cost for frequency vector F s   \n3: end for   \n4: Let $\\mathcal { C }$ be an empty set   \n5: for all keys $\\mathbf { \\chi } _ { i }$ do   \n6: Let $s$ be a scenario for which key $i$ has the shortest code $c _ { i } ^ { s } \\in \\{ 0 , 1 \\} ^ { * }$   \n7: Let $x$ be the encoding of the integer $s - 1$ with exactly $\\left\\lceil \\log _ { 2 } k \\right\\rceil$ bits   \n8: Add $\\boldsymbol { x } \\boldsymbol { c } _ { i } ^ { s }$ to $\\scriptstyle { \\mathcal { C } }$   \n9: end for   \n10: Build the Huffman tree $T$ for the prefix free codewords $\\mathcal { C }$ 11: Compactify $T$   \n12: return $T$\n\nThe following result establishes a lower bound for our problem, and shows that $\\mathbf { R } \\mathbf { - } \\mathbf { H } \\mathbf { T }$ is essentially best-possible.\n\nTheorem 3.4. There exists a set of $k$ -scenarios for which no robust HT has regret smaller than $\\lfloor \\log _ { 2 } k \\rfloor$ .\n\nCorollary 3.5. There exists an algorithm for $k$ -scenario robust HTs of competitive ratio at most $\\lceil \\log k \\rceil + 1$ . Furthermore, no algorithm for this problem can have competitive ratio better than $\\lfloor \\log k \\rfloor + 1$ .\n\n# 4 Regret and Fairness in Binary Search Trees\n\nIn this section, we introduce the first study of fairness in BSTs, and demonstrate its connection to regret minimization in a multiobjective optimization setting. We begin with a motivating application. Consider a company that stores client information in a database structured as a BST. The clients are from either Spain or France, and the company would like to use a single database to store client data, instead of two, for simplicity of maintenance. Moreover, the company would like to treat customers of the two countries in a fair manner. Namely, the average cost for accessing clients from France should be comparable to that of accessing a database containing only these French clients, and similarly for Spanish clients.\n\nWe can formulate applications such as the above using a scenario-based regret-minimization framework over BSTs. Specifically, we can model the setting using two frequency vectors, one for each country. Each vector stores the probability of accessing a client, i.e., entry $i$ is the probability of accessing client $i$ . Since we treat all clients of a country equally, we are interested in tradeoffs between the average access costs of clients in the two countries, and can thus assume that the access probabilities are uniform. That is, if $f$ denotes the number of French clients, then each such client has access probability $1 / f$ in the frequency vector1.\n\nThere are two important observations to make. First, unlike the approaches of Sections 2 and 3, the fairness setting is inherently multi-objective. For example, in the above application, we are interested in the tradeoff between the total access costs of clients in the two countries. We will thus rely on the well-known concept of Pareto-optimality (Boyd and Vandenberghe 2004) that allows us to quantify such tradeoffs, as we will discuss shortly. Second, we will use the case of two scenarios for simplicity, however we emphasize that the setting and our results generalize to multiple scenarios, as we discuss at the end of the section.\n\nWe formalize our setting as follows. We are given two scenarios 0 and 1 over $n$ keys $1 , \\ldots , n$ . We denote by $a$ and $b$ the number of keys in scenario 0, and 1, respectively. We refer to keys of scenario 0 as the 0-keys, and similarly for 1- keys. We can describe compactly the two scenarios using a binary string $s \\in \\{ 0 , 1 \\} ^ { n }$ , which specifies that key $i$ belongs to scenario $s _ { i } \\in \\{ 0 , 1 \\}$ . Consider a BST $T$ for this set of $n$ keys, then the cost of key $i \\in [ n ]$ is the level of the node in $T$ that contains $i$ . The 0-cost of $T$ is defined as the total cost of all 0-keys in $T$ , and the 1-cost is defined similarly.\n\nTo define the concept of regret, let $\\mathrm { { O P T } } ( m )$ denote the optimal cost of a binary tree over $m$ keys, assuming a uniform key distribution. From (Knuth 1997, Sect 5.3.1, Eq. (3)),\n\n$$\n\\mathrm { O P T } ( m ) = ( m + 1 ) \\lceil \\log _ { 2 } ( m + 1 ) \\rceil - 2 ^ { \\lceil \\log _ { 2 } ( m + 1 ) \\rceil } + 1 .\n$$\n\nClearly, in every BST $T$ , the 0-cost is at least $\\mathrm { O P T } ( a )$ . We refer to the difference between the 0-cost of $T$ and the quantity $\\mathrm { { O P T } } ( a )$ as the 0-regret of $T$ . Thus, the 0-regret captures the additional cost incurred for searching 0-keys, due to the presence of 1-keys in $T$ (1-regret is defined along the same lines). This notion allows us to establish formally the concept of fairness in a BST:\n\nDefinition 4.1. A BST for a string $s$ is $( \\alpha , \\beta )$ -fair if it has 0- regret $\\alpha$ and 1-regret $\\beta$ . We call $( \\alpha , \\beta )$ the regret point of the BST. We denote by $f ( s , \\alpha )$ the function that determines the smallest $\\beta$ such that there is a BST for $s$ which is $( \\alpha , \\beta )$ -fair.\n\nWe say that a tree $T$ dominates a tree $T ^ { \\prime }$ if the regret point $( \\alpha , \\beta )$ of $T$ dominates the regret point $( \\alpha ^ { \\prime } , \\beta ^ { \\prime } )$ of $T ^ { \\prime }$ in the sense that $\\alpha \\leq \\alpha ^ { \\prime }$ and $\\beta \\leq \\beta ^ { \\prime }$ with one of the inequalities being strict. The Pareto-front is comprised by the regret points of all undominated trees.\n\n# 4.1 Computing the Pareto Frontier\n\nWe now give an algorithm for computing the Pareto frontier. More precisely, we describe how to compute the function $f ( s , \\bar { \\alpha } )$ . Note that $f$ is non-increasing in $\\alpha$ . The Pareto front is obtained by calling $f$ for all $\\alpha = 0 , 1 , . . . ,$ until $f ( s , \\alpha ) =$ 0. We first need to bound the range of $\\alpha$ .\n\nLemma 4.2. Let $\\alpha ^ { * }$ denote the smallest integer such that $f ( s , \\alpha ^ { * } ) = 0$ . It holds that $\\alpha ^ { * } \\leq a \\lfloor \\log _ { 2 } ( b + 2 ) \\rfloor$ .\n\nCentral to the computation of $f$ is the notion of loss. For a given BST, we associate with each node, and for each scenario $c \\in \\{ 0 , 1 \\}$ , a $c$ -loss, such that the $c$ -regret equals the total $c$ -loss over all nodes. Informally, the $c$ -loss at a node $r$ is the increase of the $c$ -cost due to the choice of $r$ as the root of its subtree. Formally, we map $m _ { 1 } , m _ { 2 } \\ \\in \\ \\mathbb { N }$ and $m _ { 0 } ~ \\in ~ \\{ 0 , 1 \\}$ to $\\mathrm { l o s s } ( m _ { 1 } , m _ { 0 } , m _ { 2 } ) = m _ { 1 } + m _ { 0 } + m _ { 2 } +$ $\\mathrm { O P T } ( m _ { 1 } ) + \\mathrm { O P T } ( m _ { 2 } ) - \\mathrm { O P T } ( m _ { 1 } + m _ { 0 } + m _ { 2 } )$ , and recall that OPT is given by (2).\n\nThe interpretation of this definition is the following. Consider a BST for a string $s \\in \\{ 0 , 1 \\} ^ { n }$ with node $r$ at its root. Let $m _ { 1 }$ be the number of $c$ in the left sub-string $s [ 1 : r - 1 ]$ , $m _ { 2 }$ the number of $c$ in the right sub-string $[ r + 1 : n ]$ , and $m _ { 0 }$ the characteristic bit indicating whether $s _ { r }$ equals $c$ . Then, if in both sub-trees scenario $c$ has zero regret, then its overall cost is exactly $\\log ( m _ { 1 } , m _ { 0 } , m _ { 2 } )$ .\n\nWe show how to compute the function $f$ by dynamic programming. While one could use the approach of (Giegerich, Meyer, and Steffen 2004), which computes the Paretooptimal regret points for all BSTs for all sub-strings of $s$ , we propose a somewhat different approach that has the same time complexity and is easier to implement.\n\nThe empty string $s = \\varepsilon$ constitutes the base case for which we have $\\bar { f } ( \\bar { \\varepsilon } , \\alpha ) = 0$ . For $s \\neq \\varepsilon$ of length $n \\geq 1$ we have\n\n$$\n\\begin{array} { r } { f ( s , \\alpha ) = \\underset { r } { \\operatorname* { m i n } } \\underset { \\alpha _ { 1 } , \\alpha _ { 2 } } { \\operatorname* { m i n } } ( f ( s [ 1 , r - 1 ] , \\alpha _ { 1 } ) + \\log s ( b _ { 1 } , b _ { 0 } , b _ { 2 } ) + } \\\\ { f ( s [ r + 1 , n ] , \\alpha _ { 2 } ) ) , \\quad \\quad } \\end{array}\n$$\n\nwhere the root $r$ in the outer minimization ranges in $[ 1 , n ]$ and separates the string to a left sub-string $s [ 1 : r - 1 ]$ , a root $s _ { r }$ , and a right sub-string $s [ r + 1 , n ]$ . For a fixed $r$ , the value $a _ { 1 }$ is the number of 0s in the left sub-string, whereas $a _ { 2 }$ is the number of 0s in the right sub-string, $a _ { 0 }$ is the indicator bit for $s _ { r } = 0$ , and $b _ { 1 } , b _ { 0 } , b _ { 2 }$ are similarly defined for scenario 1. The inner minimization optimizes over all partitions $\\alpha _ { 1 } , \\alpha _ { 2 }$ of the allowed bound on the 0-regret for the left and right sub-trees, such that $\\alpha = \\alpha _ { 1 } + \\mathrm { l o s s } ( a _ { 1 } , a _ { 0 } , a _ { 2 } ) + \\alpha _ { 2 }$ .\n\nThe correctness of the algorithm follows from the fact that any BST for $s$ with 0-regret at most $\\alpha$ is defined by a root and a partition of the remaining 0-regret $\\alpha - \\mathrm { l o s s } ( a _ { 1 } , a _ { 0 } , a _ { 2 } )$ , and is composed recursively by a left and right sub-tree. The algorithm’s running time is ${ \\cal O } \\dot { ( } n ^ { 3 } ( \\alpha ^ { * } ) ^ { 2 } )$ , which simplifies to ${ \\bar { O ( } } n ^ { 5 } \\log ^ { 2 } n )$ by Lemma 4.2.\n\nWe emphasize that the dynamic programming approach can be generalized to $k \\geq 2$ scenarios. In this general setting, a regret vector has dimension $k$ , and in the function $f$ we fix $k - 1$ dimensions and optimize the last one. Hence, we have $O ( n ^ { 2 } ( n \\log n ) ^ { k - 1 } ) = O ( n ^ { 1 + k } \\log ^ { k - 1 } . n )$ variables, each being a minimization over $O ( n ( n \\log n ) ^ { k - 1 } )$ choices, which yields a time complexity of $O ( n ^ { 1 + 2 k } \\log ^ { 2 k - 2 } n )$ .\n\n# 5 Computational Experiments 5.1 Robust BSTs and HTs\n\nWe report computational experiments on the robust versions of BSTs and HTs from Sections 2 and 3. We used open data from (Wikipedia 2024). Specifically, we chose ten European languages2 as corresponding to ten different scenarios, based on the frequency of each letter in the corresponding language. We restrict to the English alphabet of 26 letters, ignoring other letters or accents for simplification, but normalizing the frequencies to 1. For example, the most frequent letter in English is $\\textsf { e }$ with $1 2 . 7 \\%$ , whereas in Portuguese the letter a is used more often with frequency $1 4 . 6 \\%$ . To evaluate our algorithms, we provide a mixed integer linear program (MILP) formulation for the problems. This allows us to compute optimal trees with the help of commercial MILP solvers, such as GUROBI. We give the MILP for costminimization in robust BSTs, but we note that minimizing the competitive ratio and the regret follow along the same lines, by only changing the objective function accordingly. The range of indices is $i , j , \\ell \\in \\mathop { \\left[ 1 , \\dots , n \\right] } _ { }$ .\n\nTable 1: Performance comparison of the various algorithms.   \n\n<html><body><table><tr><td colspan=\"3\">Binary Search Tree</td><td colspan=\"2\">Huffman Tree</td></tr><tr><td></td><td>Optimal</td><td>R-BST</td><td>Optimal</td><td>R-HT</td></tr><tr><td rowspan=\"2\">cost comp. ratio</td><td>3.389</td><td>3.940</td><td>4.271</td><td>4.425</td></tr><tr><td>1.047</td><td>1.215</td><td>1.038</td><td>1.091</td></tr><tr><td>regret</td><td>0.151</td><td>0.680</td><td>0.155</td><td>0.364</td></tr></table></body></html>\n\n$$\n\\begin{array} { c } { { \\displaystyle \\operatorname* { m i n } _ { \\mathbf { \\mu } \\mathbf { \\Lambda } ^ { C } } } } \\\\ { { \\mathrm { s u b j . t o } } } \\\\ { { } } \\\\ { { \\displaystyle \\forall i , j , \\ell : \\sum _ { r = i + 1 } ^ { j - 1 } \\sum _ { u = 1 } ^ { l - 1 } x _ { u , r } \\geqslant x _ { \\ell , i } + x _ { \\ell , j } - 1 } } \\\\ { { \\displaystyle \\forall s : \\sum _ { \\ell } \\sum _ { \\ell } \\mathbf { \\Lambda } _ { i } ^ { C } \\cdot \\ell \\cdot x _ { \\ell , i } \\leqslant C } } \\\\ { { } } \\\\ { { \\displaystyle \\forall i , \\ell : x _ { \\ell , i } \\in \\{ 0 , 1 \\} } } \\end{array}\n$$\n\nHere, ${ \\boldsymbol { x } } _ { \\ell , i }$ is that $x _ { \\ell , i } = 1$ if and only if key $i$ is assigned to level $\\ell$ . Constraint (4) ensures that every key is assigned to exactly one level. Constraint (5) ensures that the resulting level vector satisfies Definition 2.1. Last, constraint (6) together with the objective (3) guarantee that $C$ is the maximum tree cost over all scenarios. The MILP for robust HTs can be obtained similarly.\n\nGUROBI solved the MILPs for our experimental setting in times ranging from 7 to 15 seconds on a standard laptop. In contrast, R-BST and R-HT run in less than 0.1 seconds, using a Python implementation. Table 1 summarizes the results of the experiments. As expected, the empirical performance is better than the worst-case guaranties of Theorems 2.4 and 3.2. This is due to the fact that real data do not typically reflect adversarial scenarios (compare, e.g., to the adversarial constructions of Theorem 2.6 and Theorem 3.4).\n\n# 5.2 Pareto-Optimality and Fairness\n\nWe report experiments on our Pareto-optimal algorithm of Section 4, denoted by PO. The universe of all possible keys is represented by the names of cities from 10 different countries (the same counties as in Section 5.1). From this data, a string $s$ of length $2 n$ , for some chosen $n$ , is generated by selecting two of the above countries (which we call country 0 and country 1) and the $n$ largest in population cities from each country. Then, $s$ is obtained by sorting lexicographically (i.e., alphabetically) the $2 n$ cities and by setting $s _ { i } \\in \\{ 0 , 1 \\}$ , depending on whether the $i$ -th city in $s$ belongs to country 0 or 1.\n\nWe run algorithm PO and found the regret-based Pareto front for each string $s$ generated as above, choosing $n = 3 0$ .\n\n![](images/e82db16375f84e248b9dc4c1aa024fbc75abfc94ea9047684470d5fd7e6cb91b.jpg)  \nFigure 2: The Pareto front for strings with $a = 1 1 , b = 1 1$ .\n\nFrom the results, we observed that for every $s$ , there exists a BST whose 0-regret and 1-regret are both bounded by $n$ . Specifically, for every $s$ , we were able to find a tree with a 0-regret of 27 and 1-regret of 28, as well as a tree with a 0- regret of 28 and a 1-regret of 27. These regret numbers were obtained for all $\\textstyle { \\binom { 1 0 } { 2 } }$ country pairs.\n\nFurthermore, for all $a , b \\in \\{ 0 , \\ldots , 1 1 \\}$ , we generated all strings $s$ of size $a + b$ , i.e., $a$ keys from scenario 0 (country 0) and $b$ keys from scenario 1 (country 1). Figure 2 illustrates our findings for the case $a = b = 1 1$ . Here, a point $( \\alpha , \\beta )$ signifies that we can find a BST of 0-regret $\\alpha$ and 1-regret $\\beta$ for any string with $1 1 \\ 0 \\mathrm { s }$ and 11 1s. This can be accomplished by running algorithm PO on all possible such strings. We obtained the same conclusion for all strings in which $a \\leq 1 1$ and $b \\leq 1 1$ .\n\nRemark 5.1. The experimental results suggest that for a string $s$ consisting of $a \\ 0 \\mathrm { s }$ and $b$ 1s, there exists a BST of 0-regret at most $a$ and 1-regret at most $b$ .\n\n# 6 Conclusion\n\nWe introduced the study of scenario-based robust optimization in data structures such as BSTs, and in data coding via Huffman trees. We gave hardness results, and theoretically optimal algorithms for a variety of measures such as competitive ratio, regret and Pareto-optimality. Our work also established connections between fairness and multi-objective regret minimization. Future work will address other important data structures, such as B-trees and quad-trees, as well as approximation algorithms to the NP-hard problem of minimizing the worst-case tree cost. Our approaches address a fundamental issue: the tradeoff between cost and frequency of operations, which can be of use in many other practical domains, such as inventory management in a warehouse.",
    "institutions": [
        "Sorbonne University",
        "LIP6",
        "CNRS",
        "International Laboratory on Learning Systems"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文聚焦于基于场景的鲁棒优化背景下的树结构研究，着重探讨二叉搜索树（BSTs）和霍夫曼编码这两种依据已知键频率集高效管理与编码数据的基本技术。给定多个由键频率分布定义的不同场景，目标是计算出在任何场景下都有最佳性能的单一树结构。该问题的重要性在于为不确定场景下的数据结构和数据编码提供高效稳定的解决方案，在数据库查询、数据传输等实际应用领域意义重大。\\n\\n**方法概述**\\n论文针对不同树结构问题提出多种算法。对于BSTs，设计了构建特定竞争比BST的算法；对于霍夫曼树，提出聚合各场景最优树为单一树的算法；对于BST后悔值最小化问题，给出计算帕累托最优解的多项式时间算法。\\n\\n**主要贡献与效果**\\n- 证明在竞争比和后悔值指标下，BSTs和霍夫曼树问题均为NP - 难问题，即便只有两个场景。\\n- 对于BSTs，给出算法构建竞争比至多为 $\\lceil \\log _ { 2 } ( k + 1 ) \\rceil$ 的BST，且证明该界是最优的。\\n- 对于霍夫曼树，提出后悔值至多为 $\\left\\lceil \\log _ { 2 } k \\right\\rceil$ 的算法，并证明该算法接近最优，对应的下界为 $\\lfloor \\log _ { 2 } k \\rfloor$。此外，在实验中，对于BSTs，MILP计算的最优树成本为3.389，R - BST算法构建的树成本为3.940；对于霍夫曼树，MILP计算的最优树成本为4.271，R - HT算法构建的树成本为4.425。R - BST和R - HT算法运行时间小于0.1秒，而MILP求解时间需7到15秒。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n- **R - BST算法**：先计算每个场景下的最优树，再递归地选择在所有场景最优树中处于最小层的键作为根节点，以此构建具有一定竞争比的BST。其有效性在于利用场景间信息，通过递归选合适根节点，使构建的树在不同场景下性能相对均衡。\\n- **R - HT算法**：把每个场景的最优霍夫曼树聚合为一个单一的树。为每个键的最短代码添加表示场景的位，控制最终树在不同场景下的后悔值。核心是利用场景信息扩展代码，实现多场景下的鲁棒性。\\n- **计算帕累托最优BST的算法**：基于动态规划思想，定义损失概念，将后悔值转化为节点损失之和，计算满足帕累托最优的BST。通过递归考虑左右子树的后悔值分配，求解多目标后悔值最小化问题。\\n\\n**创新点**\\n先前工作主要关注单一频率分布下的树结构优化，本文引入场景概念，研究多个不同场景下的鲁棒树结构。同时，首次在数据结构中引入公平性研究，用帕累托最优概念量化不同用户场景下的公平性。在算法设计上，针对不同树结构（BSTs和霍夫曼树）采用不同方法，考虑了它们的结构差异，如证明NP - 难问题时，BSTs用从PARTITION问题的非平凡归约，霍夫曼树用从EQUAL - CARDINALITY PARTITION问题的归约。\\n\\n**具体实现步骤**\\n- **R - BST算法**：\\n    1. 对于每个场景 $s \\in \\{ 1, \\ldots, k \\}$，计算最优树 $L ^ { s }$。\\n    2. 调用递归过程 $A ( i = 1, j = n, \\ell = 1 )$。\\n    3. 在递归过程 $A$ 中：\\n        - 如果 $i > j$，则返回空。\\n        - 找到 $i \\leqslant r \\leqslant j$ 和 $1 \\leqslant s \\leqslant k$ 中最小的层 $L _ { r } ^ { s }$，将对应的键存储在有序集 $S$ 中。\\n        - 选择键 $m$ 作为根节点，使得 $| S \\cap [ i, m - 1 ] |$ 和 $| S \\cap [ m + 1, j ] |$ 至多为 $\\lceil ( | S | - 1 ) / 2 \\rceil$。\\n        - 设置 $L _ { m } = \\ell$。\\n        - 递归调用 $A ( i, m - 1, \\ell + 1 )$ 和 $A ( m + 1, j, \\ell + 1 )$。\\n- **R - HT算法**：\\n    1. 对于每个场景 $s$，计算最优霍夫曼树 $T ^ { s }$。\\n    2. 对于每个键 $i$，找到具有最短代码的场景 $s$，记为 $c _ { i } ^ { s }$。\\n    3. 为每个代码 $c _ { i } ^ { s }$ 前缀添加 $\\left\\lceil \\log _ { 2 } k \\right\\rceil$ 位表示场景 $s$。\\n    4. 构建最终的霍夫曼树，将每个键与 $\\lceil \\log _ { 2 } k \\rceil + c _ { i } ^ { s }$ 对应的层关联。\\n    5. 对树进行“紧凑化”处理，即收缩出度为1的内部节点。\\n- **计算帕累托最优BST的算法**：\\n    1. 定义后悔值和损失的概念，将 $c$ - 后悔值表示为所有节点的 $c$ - 损失之和。\\n    2. 确定 $\\alpha$ 的范围，即找到最小的整数 $\\alpha ^ { * }$ 使得 $f ( s, \\alpha ^ { * } ) = 0$，且 $\\alpha ^ { * } \\leq a \\lfloor \\log _ { 2 } ( b + 2 ) \\rfloor$。\\n    3. 使用动态规划计算函数 $f ( s, \\bar { \\alpha } )$：\\n        - 对于空字符串 $s = \\varepsilon$，$f ( \\varepsilon, \\alpha ) = 0$。\\n        - 对于非空字符串 $s$，$f ( s, \\alpha ) = \\underset { r } { \\operatorname* { min } } \\underset { \\alpha _ { 1 }, \\alpha _ { 2 } } { \\operatorname* { min } } ( f ( s [ 1, r - 1 ], \\alpha _ { 1 } ) + \\mathrm { loss } ( b _ { 1 }, b _ { 0 }, b _ { 2 } ) + f ( s [ r + 1, n ], \\alpha _ { 2 } ) )$，其中 $\\alpha = \\alpha _ { 1 } + \\mathrm { loss } ( a _ { 1 }, a _ { 0 }, a _ { 2 } ) + \\alpha _ { 2 }$。\\n\\n**案例解析**\\n- 在R - BST算法中，给定两个场景 $F ^ { 1 } = ( 0, 1 / 4, 3 / 4 )$ 和 $F ^ { 2 } = ( 4 / 9, 2 / 9, 1 / 3 )$，列出五种可能的BST在这两个场景下的成本，以及它们在成本、竞争比和后悔值三种性能指标下的表现，直观展示不同BST在不同场景下的性能差异。\\n- 在计算帕累托最优BST的算法中，以公司存储不同国家客户信息的数据库为例，将客户分为法国和西班牙两个场景，用两个频率向量表示两个国家客户的访问概率，说明如何将多目标后悔值最小化问题应用到实际场景，以及如何通过算法找到满足公平性的BST。同时，在实验中，对于所有由 $a$ 个0和 $b$ 个1组成的字符串 $s$（$a \\leq 11$ 且 $b \\leq 11$），能找到0 - 后悔值和1 - 后悔值都不超过 $n$ 的BST，如找到0 - 后悔值为27、1 - 后悔值为28的树，以及0 - 后悔值为28、1 - 后悔值为27的树。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文使用混合整数线性规划（MILP）作为基线模型，借助商业MILP求解器（如GUROBI）计算最优树。\\n\\n**性能对比**\\n*   **在成本指标上**：对于BSTs，MILP计算的最优树成本为3.389，R - BST算法构建的树成本为3.940，R - BST算法构建的树成本高出约16.3%；对于霍夫曼树，MILP计算的最优树成本为4.271，R - HT算法构建的树成本为4.425，R - HT算法构建的树成本高出约3.6%。不过，MILP求解时间需7到15秒，而R - BST和R - HT算法运行时间小于0.1秒，在实际应用中R - BST和R - HT算法时效性更好。且由于实际数据并非对抗性场景，其性能优于理论最坏情况。\\n*   **在竞争比指标上**：对于BSTs，MILP计算的最优树竞争比为1.047，R - BST算法构建的树竞争比为1.215，R - BST算法构建的树竞争比高出约16.0%；对于霍夫曼树，MILP计算的最优树竞争比为1.038，R - HT算法构建的树竞争比为1.091，R - HT算法构建的树竞争比高出约5.1%。在实际数据中，因真实数据通常不反映对抗性场景，R - BST和R - HT算法实际性能仍有一定优势，优于理论最坏情况。\\n*   **在后悔值指标上**：对于BSTs，MILP计算的最优树后悔值为0.151，R - BST算法构建的树后悔值为0.680，R - BST算法构建的树后悔值高出约350.3%；对于霍夫曼树，MILP计算的最优树后悔值为0.155，R - HT算法构建的树后悔值为0.364，R - HT算法构建的树后悔值高出约134.8%。但考虑到计算时间优势，R - BST和R - HT算法在实际应用中具有一定可行性，且实际性能优于理论最坏情况。\",\n    \"keywords\": \"### 关键词\\n\\n- 场景式鲁棒优化 (Scenario - Based Robust Optimization, N/A)\\n- 二叉搜索树 (Binary Search Trees, BSTs)\\n- 霍夫曼树 (Huffman Trees, HTs)\\n- 竞争比 (Competitive Ratio, N/A)\\n- 后悔值 (Regret, N/A)\\n- 帕累托最优 (Pareto - Optimality, N/A)\\n- 数据结构公平性 (Fairness in Data Structures, N/A)\"\n}"
}