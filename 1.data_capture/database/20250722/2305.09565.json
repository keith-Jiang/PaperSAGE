{
    "link": "https://arxiv.org/abs/2305.09565",
    "pdf_link": "https://arxiv.org/pdf/2305.09565",
    "title": "Toward Falsifying Causal Graphs Using a Permutation-Based Test",
    "authors": [
        "Elias Eulig",
        "Atalanti A. Mastakouri",
        "Patrick Blöbaum",
        "Michael W. Hardt",
        "D. Janzing"
    ],
    "publication_date": "2023-05-16",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Mathematics",
        "Computer Science"
    ],
    "citation_count": 13,
    "influential_citation_count": 0,
    "paper_content": "# Toward Falsifying Causal Graphs Using a Permutation-Based Test\n\nElias Eulig1,2,\\*,†, Atalanti A. Mastakouri3, Patrick Bl¨obaum3, Michaela Hardt4,\\*, Dominik Janzing3\n\n1German Cancer Research Center (DKFZ) 2Heidelberg University 3Amazon Research Tu¨bingen 4University Hospital Tu¨bingen elias.eulig@dkfz.de, {atalanti,janzind}@amazon.de, bloebp $@$ amazon.com, michaela.hardt $@$ uni-tuebingen.de\n\n# Abstract\n\nUnderstanding causal relationships among the variables of a system is paramount to explain and control its behavior. For many real-world systems, however, the true causal graph is not readily available and one must resort to predictions made by algorithms or domain experts. Therefore, metrics that quantitatively assess the goodness of a causal graph provide helpful checks before using it in downstream tasks. Existing metrics provide an absolute number of inconsistencies between the graph and the observed data, and without a baseline, practitioners are left to answer the hard question of how many such inconsistencies are acceptable or expected. Here, we propose a novel consistency metric by constructing a baseline through node permutations. By comparing the number of inconsistencies with those on the baseline, we derive an interpretable metric that captures whether the graph is significantly better than random. Evaluating on both simulated and real data sets from various domains, including biology and cloud monitoring, we demonstrate that the true graph is not falsified by our metric, whereas the wrong graphs given by a hypothetical user are likely to be falsified.\n\nProject — https://eeulig.github.io/dag-falsification\n\nCall DAG of microservices Domain expert Causal DAG of latencies B B A C (a) Domain expert estimates causal DAG $\\hat { \\mathcal G }$   \n1. Unidentified confounding 2. Unknown type I error rate Cluster workload see Figure 2 B A   \n3. B's latency increases $\\star$ A skips call to $\\mathrm { C } \\to \\mathrm { C ^ { \\prime } s }$ latency decreases $\\underbrace { \\mathbb { O } ^ { 3 } } _ { \\texttt { \\tiny B } ^ { 0 } } \\underbrace  \\mathrm { ~ \\mathbb { A } ~ } _ { \\texttt { \\tiny B } } \\mathrm { ~ \\texttt ~ { ~ C ~ } ~ } _ { \\texttt { \\tiny B } } ^ { \\texttt { \\tiny B } ^ { * } } \\mathrm { ~ \\Pi ~ } _ { \\mathrm { l e a d s ~ t o } } \\mathrm { ~ \\mathbb { B } _ { \\Xi } \\underbrace { \\right. ~ \\mathrm { { A } _ { \\Xi } } \\left. ~ \\mathrm { { C _ { \\Xi } } ~ } } _ { \\texttt { \\tiny A } } ~ } _ { \\texttt { \\tiny B } }$ (b) Possible sources of CI violations of $\\hat { \\mathcal G }$\n\n# 1 Introduction\n\nDirected Acyclic Graphs (DAGs) are a core concept of causal reasoning as they form the basis of structural causal models (SCMs) which can be used to predict the effect of interventions in a causal system or even to answer counterfactual questions. For that reason, they have numerous applications, including biology (Sachs et al. 2005; Huber et al. 2007; Pingault et al. 2018), medicine (Shrier and Platt 2008) and computer vision (Wang and Sahbi 2013; Chalupka, Perona, and Eberhardt 2015; Wang et al. 2020).\n\nNevertheless, for many real-world systems the true causal relationships, represented in the form of a DAG, are often not readily available. If randomized controlled trials are not possible, inferring the DAG from passive observational data alone is a hard problem that rests on strong assumptions on statistical properties (e.g. causal faithfulness), functional relationships, noise distributions, or graph constraints (Shimizu et al. 2006; Peters and Bu¨hlmann 2014; Chickering 1996; Chickering, Heckerman, and Meek 2004; Claassen, Mooij, and Heskes 2013; Mooij et al. 2016). These assumptions are often violated in practice. Instead of relying on discovery from observational data, domain experts can describe known dependencies in a system (Fig. 1; (a)). This approach, however, is subject to human error and incomplete knowledge. This is particularly problematic for applications in causal inference, where a wrong graph structure will lead to wrong conclusions about the effect of interventions.\n\nEfforts towards quantitatively evaluating the consistency of a given graph (either originating from a domain expert or a causal discovery algorithm) using observational data alone are of utmost importance. However, existing attempts towards this direction report the raw number of inconsistencies between DAG and observed data without a baseline of how many such inconsistencies are to be expected in the first place (Textor et al. 2016; Reynolds et al. 2022). Striving for zero is unrealistic and the acceptable number of violations depends on many factors including the size and complexity of the graph. On real-world data, the fraction of conditional independence violations for expert-elicited graphs can be surprisingly high due to unidentified confounding, high type I error rates of the conditional independence tests (Shah and Peters 2020), or complex interactions between the variables (Fig. 1; (b)). Other methods require the existence and knowledge of DAGs from a related system (Pitchforth and Mengersen 2013), or are similarly difficult to interpret due to a missing baseline (Madigan, York, and Allard 1995; Friedman and Koller 2003).\n\n![](images/4100e8e7d20c7cdadf728e1e03a7301b8505f5a922894533516e9d4f865bbfc2.jpg)  \nFigure 2: Type I error rate at $\\alpha = 5 \\%$ for different sizes $D$ of the conditioning set for one parametric (partial correlation) and two nonparametric CI tests, KCI (Zhang et al. 2011) and GCM (Shah and Peters 2020). Data (solid: $N = 1 0 0$ , dashed: $N = 5 0 0$ ) were sampled from gaussian-linear conditionals. More details are given in Supp. A.1.\n\nTo overcome these drawbacks, in this work we develop a novel metric1 to evaluate a given graph using observational data alone. Aiming to understand if a number of violations of a user-specified graph is high or low, we compare it against a baseline that we construct by randomly permuting its nodes. Through this comparison we can shed light onto the question if the violations of a user-specified graph are false-positives or point to real deficiencies of the graph.\n\n# 2 Related Work\n\nIn this section, we review existing works that aim to quantify the consistency of a given DAG with observed data.\n\nThe R package dagitty (Textor et al. 2016) implements functions to evaluate DAG-dataset consistency by testing conditional independence (CI) relations implied by the graph structure via the d-separation (Pearl 1988) and global Markov condition. However, directly using the number of violations of graph-implied CIs as a metric to evaluate a given graph is not suitable for real-world applications because there exists no nonparametric CI test with valid level2 over all distributions and thus the probability of type I errors remains unknown in practice (Shah and Peters 2020, Th. 2), c.f. Fig. 2. Therefore, without a baseline comparison, the raw number of violations (absolute or fraction) does not provide the user with a meaningful measure of whether or not the observed inconsistencies of the given graph are significant.\n\nReynolds et al. (2022) validate a given DAG (6 nodes, 8 edges) that relates exposure to spaceflight environment to the performance and health outcomes of rats and mice. Together with testing CIs implied by d-separations using dagitty, they test whether dependencies implied by the graph lead to marginal dependencies in the observations (faithfulness). Similar to (Textor et al. 2016) interpreting the results of those tests is challenging, without a baseline comparison. Our metric overcomes this drawback by providing such a baseline comparison to estimate whether the observed inconsistencies are significant.\n\nPitchforth and Mengersen (2013) suggest a number of questions to validate expert-elicited Bayesian Networks (BNs). In particular, the authors propose to validate a given BN by verifying that it is similar to BNs from the same domain already established in the literature. Nevertheless, the framework does not provide a quantified measure and many of the questions assume the existence of comparison models in the literature, which may not be available in many domains. In contrast, we propose a quantitative metric, constructed via a surrogate baseline and not reliant on the existence of similar models from the same domain.\n\nFinally, numerous works exist on Bayesian structure learning (e.g., Madigan, York, and Allard 1995; Friedman and Koller 2003; Giudici and Castelo 2003) where the posterior $p ( \\mathcal { G } \\mid \\mathcal { D } ) \\propto p ( \\mathcal { D } \\mid \\mathcal { G } ) p ( \\mathcal { G } )$ of a graph $\\mathcal { G }$ is estimated given some data $\\mathcal { D }$ . While the likelihood $p ( \\mathcal { D } \\mid \\mathcal { G } )$ could be used as a measure for the goodness of $\\mathcal { G }$ , the problem of a missing baseline remains.\n\n# 3 Background\n\n# 3.1 Notation\n\nThis work aims to evaluate the consistency of a given DAG $\\hat { \\mathcal { G } } ~ = ~ ( V , \\hat { \\mathcal { E } } )$ with vertices $V = \\{ 1 , . . . , n \\}$ , random variables $X = \\{ X _ { i } : i \\in V \\}$ , and edges $\\hat { \\mathcal { E } } \\subseteq V ^ { 2 }$ using observations $\\mathcal { D } = \\{ { \\cal X } ^ { ( 1 ) } , . . . , { \\cal X } ^ { ( N ) } \\}$ i.i.d. sampled from the joint distribution3 $\\mathsf { \\bar { P } } ( { \\pmb X } )$ . We assume the existence of some unknown true causal DAG $\\mathcal G ^ { * }$ and that $P ( \\pmb { X } )$ satisfies the causal Markov condition relative to $\\mathcal G ^ { * }$ .\n\nIn the following, we refer to $X \\ \\underline { { { \\mathrm { 1 1 } } } } \\mathbf { \\mathcal { D } } \\ Y | Z$ as the outcome of a CI test using $\\mathcal { D }$ , where $X$ and $Y$ are conditionally independent given $Z$ (and likewise $X ~ \\not \\sim \\not \\sim Y | Z$ to denote conditional dependency). Note that whether independence is rejected is both a property of $\\mathcal { D }$ and the choice of a particular CI test. In this work, to denote that a set of nodes $Z$ d-separates (Pearl 1988) $X$ from $Y$ , we write: $X$ 1 $_ { \\mathcal { G } } Y | Z$ .\n\nFor some graph $\\mathcal { G } = ( V , \\mathcal { E } )$ , we call a node $i$ a parent of node $j$ if $( i , j ) \\in \\mathcal { E }$ and denote with $\\mathrm { P a } _ { j } ^ { \\mathcal { G } }$ the set of all parents of $j \\in \\mathcal G$ $\\mathrm { P a } _ { j } ^ { \\mathcal { G } } = \\varnothing$ iff $j$ is a root node). Furthermore, for nodes $i , j$ if there exists a direct path $i  \\cdots  j$ we call $i$ an ancestor of node $j$ and $j$ a descendant of $i$ . We denote with $\\operatorname { A n c } _ { j } ^ { \\mathcal { G } }$ the set of all ancestors of $j \\in \\mathcal G$ and with $\\mathrm { N D } _ { i } ^ { \\mathcal { G } }$ the set of all non-descendants of $i \\in \\mathcal G$ .\n\n# 3.2 Validating Local Markov Conditions\n\nOne common approach to evaluate a DAG using observed data is by means of statistical testing of independence relations implied by $\\hat { \\mathcal G }$ (e.g. Textor et al. 2016; Reynolds et al. 2022). In the following, we will formally introduce this test and elaborate on why it is unsuitable to use as a metric directly. In Sec. 4 we will then present a baseline to overcome said drawbacks.\n\nOne of the standard assumptions in causal inference is the causal Markov condition, which allows us to factorize a joint probability distribution $P ( \\pmb { X } )$ over the variables $X$ of $\\mathcal { G }$ into the product\n\n$$\nP ( X ) = P ( X _ { 1 } , X _ { 2 } , . . . , X _ { n } ) = \\prod _ { i = 1 } ^ { n } P ( X _ { i } | \\mathbf { P } \\mathbf { a } _ { i } ^ { \\mathcal { G } } ) \\ .\n$$\n\nWe can equivalently formulate this as the parental Markov condition:\n\nTheorem 1 (Parental Markov condition (Pearl 2009)). A probability distribution $P$ is Markov relative to a DAG $\\mathcal { G } =$ $( V , \\mathcal { E } )$ , iff $X _ { i }$ 上 $_ P \\ N D _ { i } ^ { \\mathcal { G } } \\setminus P a _ { i } ^ { \\mathcal { G } } \\mid P a _ { i } ^ { \\mathcal { G } }$ .\n\nFor the remainder of this work we will use the terms parental Markov condition and local Markov condition (LMC) interchangeably. To test whether a distribution satisfies the parental Markov condition relative to a DAG we can thus list the CIs entailed by the DAG via the graphical ${ \\mathrm { d } }$ -separation criterion (Pearl 1988) and test whether those are satisfied or violated by the distribution at hand.\n\nDefinition 1 (Parental triples). For some graph $\\mathcal { G }$ , we refer to the ordered triple $( i , j \\in \\mathbf { N D } _ { i } ^ { \\mathcal { G } } \\backslash \\mathbf { P a } _ { i } ^ { \\mathcal { G } }$ , $Z = \\bar { \\mathrm { P a } _ { i } ^ { \\mathcal { G } } } ^ { \\cdot }$ ) as parental triple and denote the set of all such triples implied by $\\mathcal { G }$ as $\\mathrm { T _ { P a } ^ { \\bar { \\mathcal { G } } } }$\n\nAccording to Theorem 1 every parental triple $( i , j , Z ) \\in \\mathrm { T } _ { \\mathrm { P a } } ^ { \\mathcal { G } }$ implies the CI $X _ { i } ~ \\bot \\bot P ~ X _ { j } ~ \\mid ~ Z$ . Similar to Textor et al. (2016) we will now introduce the notion of violations of LMC:\n\nDefinition 2 (Violations of LMCs). We denote with VLGˆM,CD the set of triples $( i , j , Z ) \\in \\mathrm { T } _ { \\mathrm { P a } } ^ { \\hat { \\mathcal { G } } }$ , for which $( i , j )$ is an ordered pair, and we observe LMC violations on data $\\mathcal { D }$ , i.e.\n\n$$\n\\begin{array} { r } { V _ { \\mathrm { L M C } } ^ { \\hat { \\mathcal G } , \\mathcal D } = \\Big \\{ ( i , j , Z ) \\in \\mathrm { T } _ { \\mathrm { P a } } ^ { \\hat { \\mathcal G } } : X _ { i } \\ \\mathcal { N } _ { \\mathcal D } X _ { j } \\ | \\ Z \\Big \\} \\ . } \\end{array}\n$$\n\nFurthermore, we denote the fraction of LMC violations with\n\n$$\n\\phi _ { \\mathrm { L M C } } ^ { \\hat { \\mathcal { G } } , \\mathcal { D } } = \\frac { \\vert V _ { \\mathrm { L M C } } ^ { \\hat { \\mathcal { G } } , \\mathcal { D } } \\vert } { \\vert \\mathrm { T } _ { \\mathrm { P a } } ^ { \\hat { \\mathcal { G } } } \\vert } .\n$$\n\nUsing the two metrics from Definition 2 directly to measure the goodness of a user-given graph is not suitable for realworld applications due to the reasons detailed in Sec. 2, particularly the unknown type I error rate of a $\\mathrm { C I }$ test (c.f. Shah and Peters (2020) & Fig. 2).\n\n# 4 A Baseline for Violations of Local Markov Conditions\n\nsacruesisnesduifnfitchienptrteovimoeuasssuercetitohnesc,othnesismtetnrciycs $V _ { \\mathrm { L M C } } ^ { \\hat { \\mathcal { G } } , \\mathcal { D } }$ apnhd. $\\phi _ { \\mathrm { L M C } } ^ { \\hat { \\mathcal { G } } , \\mathcal { D } }$ In the following, we therefore derive a baseline to compare the number of LMC violations $V _ { \\mathrm { L M C } } ^ { \\hat { \\mathcal { G } } , \\mathcal { D } }$ to.\n\n# 4.1 Finding a Suitable Baseline\n\nMotivated by a very skeptical view whether the user specified graph is related to the observed independence structure at all, we are interested in a baseline that is a random draw of a set of conditional independence statements. For example, consider the dependence structure of microsvervices in a distributed system, where multiple effects can lead to violations of independence statements on observed data (Fig. 1).\n\nIn general, there can be different reasons why the pattern of observed conditional independences appears unrelated to $\\hat { \\mathcal G }$ . On the one hand, the domain expert who provided $\\hat { \\mathcal G }$ may have messed up causal links and directions entirely. But even if all the links of $\\hat { \\mathcal G }$ are correct, additional confounding and violations of faithfulness4 can mess up the independence structure. In both cases, DAG and independences appear random relative to each other regardless of whether we think the DAG or the pattern of independences to be random. Our experiments show that ‘better than random’ is a surprisingly high bar, and both domain experts and causal discovery algorithms fail to meet it in many settings.\n\nWe now introduce a set of properties that draws from the random baseline should satisfy:\n\nP1: They should infer the same number $m$ of conditional independences as the given graph $\\hat { \\mathcal G }$ .   \nP2: They should partition $m$ into $m _ { r }$ conditional independences with conditioning sets of size $r$ with the same numbers $m _ { r }$ as $\\hat { \\mathcal G }$ does.   \nP3: Conditional independences inferred by draws from the baseline should be closed under the semi-graphoid axioms (Pearl and Paz 1986; Geiger, Verma, and Pearl 1990).\n\nP1 ensures that the number of observed LMC violations on the baseline are comparable to those observed on $\\hat { \\mathcal G }$ . E.g. a random baseline inferring fewer CIs than $\\hat { \\mathcal G }$ will also result in much fewer LMC violations.5 If $\\hat { \\mathcal G }$ is sparse because it has been drawn with the implicit intention of explaining only highly significant dependences, we should not reject it just because it shows many LMC violations with respect to our significance level. Instead, we should rather benchmark $\\hat { \\mathcal G }$ against a random guess that infers equally many independences. P2 ensures that type I and type II errors, when testing implied CIs on the baseline, are comparable to those on $\\hat { \\mathcal G }$ .\n\nE.g. a random baseline inferring mostly CIs with small conditioning sets may exhibit fewer LMC violations than $\\hat { \\mathcal G }$ by virtue of a smaller type I error rate (c.f. Fig. 2). Lastly, P3 ensures that the CI statements from the baseline do not imply additional CIs (which are not already in the set) via the semi-graphoid axioms. For example, due to decomposition, $X \\perp Y \\cup W | Z \\Rightarrow X \\perp Y | Z \\& X \\perp W | Z .$\n\nA natural choice for a baseline that satisfies all the requirements above can be constructed by sampling nodepermutations of $\\hat { \\mathcal G }$ . More formally, let $S _ { n }$ denote the set of permutations $\\pi$ on the vertices $\\{ 1 , . . . , n \\}$ of some graph $\\mathcal { G }$ . For any permutation $\\pi \\in S _ { n }$ we denote with $\\sigma _ { \\pi } ( \\mathcal { G } )$ the graph for which the edge $i  j$ exists iff $\\pi ( i ) \\to \\pi ( j )$ exists in $\\mathcal { G }$ . Because of the one-to-one correspondence between $\\pi$ and $\\sigma$ we will drop the subscript and in the following refer to $\\sigma \\in S _ { \\mathcal { G } }$ as one node-permutation of $\\mathcal { G }$ .\n\nWe can then construct our baseline by sampling random node-permutations $\\sigma \\in S _ { \\hat { \\mathcal { G } } }$ of the given graph $\\hat { \\mathcal G }$ . Let $O ( \\hat { \\mathcal { G } } )$ define the orbit of $\\hat { \\mathcal G }$ under $S _ { \\hat { \\mathcal { G } } }$ , i.e., the set of DAGs obtained via permutations. Since all DAGs in $O ( \\hat { \\mathcal { G } } )$ imply the same CIs as $\\hat { \\mathcal G }$ up to renaming of variables, they satisfy P1 and P2. Furthermore, they satisfy P3 since the set of CIs entailed by a graph are closed under the semi-graphoid axioms.6,7\n\nNote that the mapping $S _ { \\hat { \\mathcal { G } } }  O ( \\hat { \\mathcal { G } } )$ defined by $\\sigma \\mapsto \\sigma ( \\mathcal G )$ is in general not one-to-one because there will often be a non-trivial subgroup that leaves $\\hat { \\mathcal G }$ invariant (the stabilizer subgroup $\\operatorname { S t a b } ( \\hat { \\mathcal { G } } )$ of $S _ { \\hat { \\mathcal { G } } } .$ ).8\n\nProposition 1. Uniform sampling of permutations from the set of all node permutations $S _ { \\hat { \\mathcal { G } } }$ results in uniform sampling from the DAGs in the orbit $O ( \\hat { \\mathcal { G } } )$ .\n\nA proof is provided in Supp. D.1. Further, $O ( \\hat { \\mathcal { G } } )$ decomposes into Markov equivalence classes of equal size. This can easily be seen by the same argument when we consider the action of $S _ { \\hat { \\mathcal { G } } }$ on the set of Markov equivalence classes and introduce the corresponding (larger) stabilizer subgroup. Thus, uniform sampling of permutations from the set of all node permutations $S _ { \\hat { \\mathcal { G } } }$ also results in uniform sampling from the Markov equivalence classes in $O ( \\hat { \\mathcal { G } } )$ .\n\n# 4.2 A Permutation Test to Evaluate DAGs\n\nUsing the above baseline we state the following null hypothesis:\n\nHypothesis $H _ { 0 }$ : The DAG $\\hat { \\mathcal G }$ is drawn uniformly at random from some distribution $Q$ on the set of DAGs that is invariant under permutations of nodes, that is $Q ( \\mathcal G ) = Q ( \\sigma ( \\mathcal G ) )$ for all $\\sigma \\in S _ { \\mathcal { G } }$ .\n\nTo test this hypothesis we consider the number of violations VLGˆM,CD as test statistics and build the null via\n\n$$\n\\begin{array} { r l } & { V _ { \\mathrm { L M C } } ^ { \\sigma ( \\hat { \\mathcal G } ) , \\mathcal D } \\big | \\colon } \\\\ & { \\qquad p _ { \\textsc { l M C } } ^ { \\hat { \\mathcal G } , \\mathcal D } = \\operatorname* { P r } \\left( \\big | V _ { \\mathrm { L M C } } ^ { \\sigma ( \\hat { \\mathcal G } ) , \\mathcal D } \\big | \\leq \\big | V _ { \\mathrm { L M C } } ^ { \\hat { \\mathcal G } , \\mathcal D } \\big | \\right) \\ . } \\end{array}\n$$\n\nProposition 2. pLMC is a valid $p$ -value, i.e. if $H _ { 0 }$ is true, then $P r \\left( p _ { L M C } ^ { \\hat { \\mathcal { G } } , \\mathcal { D } } \\leq \\alpha \\right) \\leq \\alpha$ .\n\nA proof is provided in Supp. D.2. Computing the quantity in (4) using all $n !$ permutations is infeasible for large $n$ . Therefore, we approximate it via Monte Carlo sampling with $T$ random permutations: $\\{ \\sigma _ { i } \\sim S _ { \\hat { \\mathcal { G } } } \\} _ { i = 0 } ^ { T }$ . For an estimated pvalue we can also report binomial proportion confidence intervals.\n\nAnother quantity that proves to be useful in practice is the fraction of DAGs in $O ( \\hat { \\mathcal { G } } )$ that are Markov equivalent to $\\hat { \\mathcal G }$ . To this end we first define\n\n$$\nV _ { \\mathrm { T P a } } ^ { \\mathcal { G } ^ { \\prime } , \\mathcal { G } } = \\{ ( X _ { i } , X _ { j } , Z ) \\in \\mathbf { T } _ { \\mathbb { P a } } ^ { \\mathcal { G } ^ { \\prime } } : X _ { i } \\ \\mathcal { H } _ { \\mathcal { G } } \\ X _ { j } \\ | \\ Z \\}\n$$\n\nas the set of all ordered triples that are parentally ${ \\mathrm { d } }$ -separated in $\\mathcal { G } ^ { \\prime }$ but not d-separated in $\\mathcal { G }$ .\n\nProposition 3. Suppose some given graph Gˆ. If VTσP(aGˆ),Gˆ $\\varnothing _ { ; }$ , then $\\sigma ( \\hat { \\mathcal { G } } )$ and $\\hat { \\mathcal G }$ are Markov equivalent.\n\nThe proof is provided in Supp. D.3. Using this graphical criterion we can define a second metric\n\n$$\n\\begin{array} { r l } & { p _ { \\mathrm { T P a } } ^ { \\hat { \\mathcal { G } } } : = \\mathrm { P r } \\left( | V _ { \\mathrm { T P a } } ^ { \\sigma ( \\hat { \\mathcal { G } } ) , \\hat { \\mathcal { G } } } | \\leq | V _ { \\mathrm { T P a } } ^ { \\hat { \\mathcal { G } } , \\hat { \\mathcal { G } } } | \\right) } \\\\ & { \\qquad = \\mathrm { P r } \\left( | V _ { \\mathrm { T P a } } ^ { \\sigma ( \\hat { \\mathcal { G } } ) , \\hat { \\mathcal { G } } } | = 0 \\right) ~ , } \\end{array}\n$$\n\ntwurheicohf $\\hat { \\mathcal G }$ nisbeaubsoeudt tohempeoassuirbel ehocawuisnaflorormdaetriivnegtsh.eI $\\mathrm { C I }$ $p _ { \\mathrm { T P a } } ^ { \\mathcal { G } } >$ $\\alpha$ , for some prespecified threshold $\\alpha$ , then the number of Markov equivalent DAGs in $O ( \\hat { \\mathcal { G } } )$ is large and consequently $\\hat { \\mathcal G }$ provides us with limited information about the true graph in the sense of testing of CIs. For an information-theoretic interpretation of our test see Supp. F.\n\n# 4.3 Interpretation of pLMC and $p _ { \\mathbf { T P a } }$\n\nWe note that p-values are often misinterpreted and misused in practice (Vidgen and Yasseri 2016; Wasserstein and Lazar 2016; Amrhein, Greenland, and McShane 2019) and therefore provide the following interpretation based on Popper’s theory of falsification (Popper 2005). According to this, every scientific theory must admit potential falsifiers, i.e. measurable observations that would falsify the theory. Increasing confidence in such theory can then only come from observations that it permits, i.e. as it withstands attempts to falsify it.\n\nThe test $\\bar { p } _ { \\mathrm { T P a } } ^ { \\hat { \\mathcal { G } } }$ provides us with a measure of the falsifiability of a given graph $\\hat { \\mathcal G }$ . If the number of random DAGs that are Markov equivalent to $\\hat { \\mathcal G }$ is large (and consequently $p _ { \\mathrm { T P a } } ^ { \\hat { \\mathcal { G } } }$ is large) this limits the falsifiability of $\\hat { \\mathcal G }$ via CI testing. Contrary, if $p _ { \\mathrm { T P a } } ^ { \\hat { \\mathcal { G } } }$ is small, the CIs entailed by $\\hat { \\mathcal G }$ are ‘characteristic’ and it admits many potential falsifiers. Based on these considerations, we propose the following interpretation of our tests for practitioners:\n\n(a) If $p _ { \\mathrm { T P a } } ^ { \\hat { \\mathcal { G } } } \\leq \\alpha$ , $\\hat { \\mathcal G }$ is falsifiable by testing implied CIs.\n\n(b) If (a) and further $p _ { \\mathrm { L M C } } ^ { \\hat { \\mathcal { G } } , D } > \\alpha$ , then $\\hat { \\mathcal G }$ is falsified. (c) If (a) and further $p _ { \\mathrm { L M C } } ^ { \\hat { \\mathcal { G } } , \\mathcal { D } } \\leq \\alpha$ , then there is no CI-based evidence against $\\hat { \\mathcal G }$ and we cannot falsify $\\hat { \\mathcal G }$ using our test. Consequently, $\\hat { \\mathcal G }$ is corroborated (but not verified).\n\n# 4.4 Relating $p _ { \\mathbf { L M C } }$ to the Identification of Cause-Effect Pairs\n\nWe now want to present an interpretation of $p _ { \\mathrm { L M C } } $ which is more closely related to causal questions. To this end, consider the following task.\n\nTask 1 (Identification of unconfounded cause-effect pairs). Identify all ordered pairs $( X _ { i } , X _ { j } )$ such that there is a directed path from $X _ { i }$ to $X _ { j }$ and no $X _ { k }$ with $k \\neq i , j$ that has a directed path to $X _ { i }$ and a directed path to $X _ { j }$ that does not go through $X _ { i }$ .\n\nWe can then ask whether $\\hat { \\mathcal G }$ performed better at Task 1 than a random baseline. However, in analogy to P1 above, the baseline should satisfy the following property:\n\nP4: Draws from the random baseline should infer the same number of unconfounded cause-effect pairs as the given graph $\\hat { \\mathcal G }$ .\n\nLikewise to P1 & P2, all DAGs in $O ( \\hat { \\mathcal { G } } )$ satisfy P4 since they preserve $\\hat { \\mathcal G }$ up to a renaming of variables. By defining the number of wrongly inferred causal effects as test statistics and building the null via the node-permutation baseline, we can compute a $\\boldsymbol { \\mathrm { p } }$ -value $p _ { \\mathrm { C E } }$ .\n\nWe will now see that under the assumption of faithfulness and knowledge of a single non-effect node, our metric $p _ { \\mathrm { L M C } } $ is loosely related to $p _ { \\mathrm { C E } }$ .\n\nTheorem 2. Suppose an additional node $X _ { 0 }$ for which we know (e.g. through strong domain knowledge) that it is not an effect of any of the $1 , \\ldots , n$ nodes, but satisfies $X _ { 0 } ~ \\not \\perp \\ X _ { l }$ , $\\forall l = 1 , \\ldots , n$ . Further, let the joint distribution $P ( X _ { 0 } , X _ { 1 } , . . . , X _ { n } )$ be Markov and faithful relative to some DAG $\\mathcal { G }$ with these $n + 1$ nodes. Then $( X _ { i } , X _ { j } )$ with $i , j = 1 , \\ldots , n$ is an unconfounded cause-effect pair if and only if the following two conditions hold:\n\n$$\n\\begin{array} { l } { { X _ { i } \\not \\downarrow _ { P } X _ { j } } } \\\\ { { X _ { 0 } \\not \\perp \\underline { { { \\vert } } } _ { P } X _ { j } \\vert X _ { i } . } } \\end{array}\n$$\n\nThe proof is provided in Supp. D.4. We conclude that in our idealized scenario, testing whether $\\hat { \\mathcal G }$ is better than random at Task 1, is similar to testing whether $\\hat { \\mathcal G }$ performs better than random in identifying all pairs $( X _ { i } , X _ { j } )$ for which both conditions (7) and (8) hold. Here, the baseline is generated by the permutation group $S _ { n - 1 }$ , that is, the stabilizer group of $X _ { 0 }$ in $S _ { n }$ .\n\n# 5 Experiments\n\nIn the following we evaluate our proposed metric on synthetic and real data for which the true DAG is known or a consensus graph is established in the literature. Additionally, we introduce a novel dataset from cloud monitoring where the reversed call graph provides an estimate of the true causal graph (Fig. 1; (a)), thus providing a useful test beyond synthetic and existing real-world datasets.\n\n# 5.1 Experimental Setup\n\nWe conduct experiments with two different sources of given graphs: Emulated domain experts with partial knowledge of either a subset of nodes or a subset of edges of the true DAG $\\mathcal { G } ^ { * } = ( V , \\mathcal { E } ^ { * } )$ and causal discovery algorithms, which are a popular choice to estimate a DAG in the absence of domain expertise. Note that all $\\hat { \\mathcal G }$ in our experiments differ from $\\mathcal { G } ^ { * }$ in a structured, i.e. non-random, way.\n\nNode Domain Expert (DE- $V$ ) This model mimics the situation where a domain expert knows all causal edges for some subset $K \\subseteq V$ of the nodes in the system and knows the overall sparsity of the DAG. For all the other nodes, however, the expert has no domain knowledge and thus assigns edges randomly between pairs of nodes not both in $K$ . We define different levels of DE- $V$ to emulate the fraction of nodes for which there exists domain knowledge, i.e. $| K | / | V |$ , where $| K | / | V | = 0$ corresponds to the situation where the domain expert has no knowledge and $| K | / | V | = 1$ corresponds to $\\hat { \\mathcal { G } } = \\mathcal { G } ^ { * }$ . More details are given in Supp. A.2.\n\nEdge Domain Expert (DE- $\\mathcal { E }$ ) This model mimics a domain expert with edge-specific knowledge about $\\mathcal { G } ^ { \\ast }$ . To construct $\\hat { \\mathcal G }$ , we randomly remove and flip some of the true edges and add some new ones. By construction $\\hat { \\mathcal G }$ of a DE- $\\mathcal { E }$ is related to the Structural Hamming Distance (SHD) (Acid and de Campos 2003; Tsamardinos, Brown, and Aliferis 2006) and thus the desired similarity of a given graph can be controlled by means of the SHD. We characterize different DE- $\\mathcal { E }$ by the $\\operatorname { S H D } ( \\hat { \\mathcal { G } } , \\mathcal { G } ^ { * } )$ they entail (or $\\mathrm { S H D } ( \\hat { \\mathcal { G } } , \\mathcal { G } ^ { * } ) / | \\mathcal { E } ^ { * } |$ to compare systems with different sparsity), where $\\mathrm { S H D } ( \\hat { \\mathcal { G } } , \\mathcal { G } ^ { * } ) / | \\mathcal { E } ^ { * } | = 0$ corresponds to $\\hat { \\mathcal G } = \\overline { { \\mathcal G } } ^ { * }$ . More details are given in Supp. A.3.\n\nCausal Discovery Algorithms The proposed test can also be applied to DAGs inferred by causal discovery algorithms. However, as many algorithms use (some of the) CIs either explicitly (constraint-based) or implicitly (scorebased) for constructing the DAG, evidence in favor of an inferred DAG can only come from those CIs that were neither used by the algorithm, nor implied, via semi-graphoid axioms (Pearl and Paz 1986; Geiger, Verma, and Pearl 1990), by those CIs used by the algorithm. Nonetheless, we evaluate our test on graphs inferred by LiNGAM (Shimizu et al. 2006), CAM (Bu¨hlmann, Peters, and Ernest 2014), and NOTEARS (Zheng et al. 2018). We chose those algorithms as they are not solely based on CIs (note, however, that e.g. in LinGAM independence of noise entails CI). More details are given in Supp. C.3.\n\n# 5.2 Synthetic Data and Graphs\n\nFor evaluating our method on synthetic data we sample random $\\mathcal { G } ^ { \\ast }$ under the Erdo˝s-R´enyi model (Erdo˝s and Re´nyi 1959) with $n \\in \\{ 1 0 , 2 0 , 3 0 \\}$ nodes and an expected degree $d \\in \\{ 1 , 2 , 3 \\}$ , denoted as ER-n- $d$ . To generate synthetic data from $\\mathcal { G } ^ { \\ast }$ , conditionals are modeled as additive noise models $X _ { i } = \\overline { { f _ { i } ( \\mathbf { P a } _ { i } ^ { \\mathcal { G } ^ { * } } ) } } + N _ { i }$ with $N _ { i }$ sampled from a normal distribution and $f _ { i }$ either being random (nonlinear) MLPs, or a linear combination of the node’s parents. The exogenous variables are sampled from a standard normal, uniform, or Gaussian mixture distribution.\n\nFor all experiments on synthetic data we sample $T = 1 0 ^ { 3 }$ node permutations and use datasets with $N ~ { \\mathit { \\Phi } } = ~ 1 0 ^ { 3 }$ observations. To investigate the effect of $N$ and $T$ on $p _ { \\mathrm { L M C } }$ we run ablation studies on nonlinear data with $N , T \\in$ $\\{ 1 0 ^ { 1 } , 1 0 ^ { 2 } , 1 0 ^ { 3 } , 1 0 ^ { 4 } \\}$ . More information on implementation and parameter choices is provided in Supp. A.4.\n\n# 5.3 Real Data\n\nTo evaluate our proposed metric on real-world data, we consider three datasets with established consensus graphs serving as ground truth. We provide further details on the data in Supp. A.5.\n\nProtein Signaling Network (Sachs et al. 2005) This open dataset contains quantitative measurements of the expression levels of $n = 1 1$ phosphorylated proteins and phospholipids in the human primary T cell signaling network. The $N = 8 5 3$ observational measurements, corresponding to individual cells, were acquired via intracellular multicolor flow cytometry (Sachs et al. 2005). The consensus DAG contains 19 edges $d \\approx 3 . 4 5$ ).\n\nAuto MPG (Quinlan 1993) The Auto MPG dataset contains eight attributes (three multivalued discrete and five continuous) with the fuel consumption in miles per gallon (mpg) for $N = 3 9 8$ unique car models. While the original use of the data was to predict mpg of a car, in line with previous works on causal inference (e.g. Wang and Mueller 2017; Teshima and Sugiyama 2021), we use a consensus network ( ${ \\mathrm { \\dot { \\ell } } } n = 6$ , 9 edges, $d = 3$ ).\n\nApplication Performance Monitoring (APM) We collect trace data of microservices in a distributed system hosted on Amazon Web Services (AWS). The traces contain latency information on incoming and outbound requests of each service, averaged over $2 0 \\mathrm { { m i n } }$ , making observations approximately i.i.d. In total we ran the application for six days, leading to $N = 4 3 2$ observations. We test the working hypothesis that the transpose of the dependency graph ( ${ \\mathrm { ' } n = 3 9 }$ , 40 edges, $d \\approx 2 . 0 5 )$ ) of the application is the true causal DAG (for a discussion of this hypothesis, see Sec. 4).\n\n# 6 Results\n\n# 6.1 Simulated Graphs\n\nNonlinear Mechanisms Figure 3a depicts mean $p _ { \\mathrm { L M C } }$ for 50 synthetic graphs of various size and sparsity with nonlinear mechanisms modeled using random MLPs. As expected, we find that the average $p _ { \\mathrm { L M C } } $ monotonically decreases with increasing amount of domain knowledge for both models of domain experts. When the domain expert has complete knowledge $( \\hat { \\mathcal { G } } = \\mathcal { G } ^ { * }$ , corresponding to $| K | / | V | = 1$ for DE- $V$ and $\\mathrm { S H D } / | \\mathcal { E } | = 0$ for DE- $\\mathcal { E }$ ), we reject the hypothesis that the DAG is as bad as a random node permutation with significance level $\\alpha = 1 \\%$ for all configurations $( p _ { \\mathrm { L M C } } < 0 . 0 0 5 )$ .\n\nTable 1: Mean $p _ { \\mathrm { L M C } } $ and standard deviation (over 50 trials) for the consensus graphs of the real-world data sets with $9 5 \\%$ confidence interval. For $\\alpha = 5 \\%$ we reject the hypotheses that the graphs are as bad as random ones, despite high fractions of violations $\\phi _ { \\mathrm { L M C } } ^ { \\mathcal { G } ^ { \\ast } , \\mathcal { D } }$ (3).   \n\n<html><body><table><tr><td></td><td>Sachs</td><td>Auto MPG</td><td>APM</td></tr><tr><td>9*,D 95% conf. int. PLMC</td><td>0.031 ± 0.006 [0.020, 0.042]</td><td>0.03± 0.0052 [0.019, 0.04]</td><td>0.00</td></tr><tr><td>M</td><td>0.08</td><td>0.50</td><td>0.22</td></tr></table></body></html>\n\nTable 2: $p _ { \\mathrm { L M C } } $ and SHD for graphs inferred by causal discovery on the Sachs et al. (2005) data.   \n\n<html><body><table><tr><td></td><td>NOTEARS</td><td>CAM</td><td>LiNGAM</td></tr><tr><td>,D</td><td></td><td></td><td>0.548±0.237 0.0724±0.0866 0.0362±0.1290</td></tr><tr><td>SHD/ε PLMC</td><td></td><td>2.780±0.241 1.880 ±0.198</td><td>1.0600±0.0806</td></tr></table></body></html>\n\nGaussian-Linear Mechanisms In the supplemental, Fig. A3 we report $p _ { \\mathrm { L M C } }$ for synthetic graphs of various size and sparsity and with linear-gaussian mechanisms. Similar to DAGs with nonlinear mechanisms, we observe that our metric strictly decreases with increasing amount of domain knowledge for both models of domain experts. If $\\hat { \\mathcal G } = \\mathcal G ^ { * }$ , we would not falsify the true graph using our metric and significance level $\\alpha \\overset { \\cdot } { = } 1 \\%$ .\n\nEffect of Number of Sampled Permutations and Number of Observations Further, we investigate the effect of the number of permutations $T$ and sample size $N$ on $p _ { \\mathrm { L M C } } $ for synthetic DAGs with nonlinear mechanisms (Tab. A1). To limit the running time of the experiment, we only evaluate $p _ { \\mathrm { L M C } } $ for the true graph, i.e. $\\hat { \\mathcal { G } } = \\mathcal { G } ^ { * }$ . Here, we notice that on average our metric is consistently below 0.05, and therefore we would not reject the true graph with significance level $\\alpha \\ : = \\ : 5 \\%$ . The only exception to this are graphs with few nodes (ER-10- $d$ ), evaluated on very few ( $N = 1 0$ ) samples.\n\n# 6.2 Real World Applications\n\nFigure 3b shows mean $p _ { \\mathrm { L M C } } $ over 50 sampled given DAGs for the three real-world datasets. Similar to the experiments with synthetic data, we find that with increasing amount of domain knowledge (higher $| K | / | V |$ , lower $\\mathrm { S H } \\bar { \\mathrm { D } } / | \\mathcal { E } | ,$ $p _ { \\mathrm { L M C } } $ is strictly decreasing. When $\\hat { \\mathcal { G } } = \\mathcal { G } ^ { * }$ we reject the hypotheses that the given graphs are as bad as a random node permutation at $\\bar { \\alpha } = 5 \\%$ for all datasets.\n\nFurthermore, we find that for all real-world datasets the ferxapceticotendotfypLeMICe rvoiro ratitoenos $\\phi _ { \\mathrm { L M C } } ^ { \\mathcal { G } ^ { \\ast } , \\mathcal { D } }$ or 3t)h s hgingihfiecr ntcheanl tvhel $5 \\%$ $\\alpha ~ \\stackrel { - } { = } ~ 5 \\%$ we used for all conditional independence tests throughout this work (c.f. Tab. 1). Thus, using φLG⇤M,C as a metric, we would falsely reject the true causal graph, na¨ıvely assuming our CI tests would have valid level.\n\n![](images/260b77bb58594ab14cb68e12abf6660c0ec42194eb49c5668ac47420a6d5691f.jpg)  \nFigure 3: Mean $p _ { \\mathrm { L M C } }$ for two types of domain experts, simulated via DE- $V$ (left; smaller numbers correspond to less domain knowledge) and DE- $\\mathcal { E }$ (right; smaller numbers correspond to more domain knowledge). On synthetic data (a) for the true DAG $( | K | / | V | = 1$ ; $\\mathrm { S H D } / | \\mathcal { E } | = 0 ,$ ), we reject the null that the DAG is as bad as random with $\\alpha = 1 \\%$ for all configurations. $\\hat { \\mathcal G }$ is falsified with the same $\\alpha$ if $| K | / | V | \\leq 0 . 6$ or $\\mathrm { S H D } / | \\mathcal { E } | \\ge 1 . 5$ . On real-world data (b), for the true DAG, we reject the null that it is as bad as random with $\\alpha = 5 \\%$ and $\\hat { \\mathcal G }$ is falsified with the same $\\alpha$ for $| K | / | V | \\leq 0 . 8$ or SHD/ $| \\mathcal { E } | \\geq 0 . 5$ for all datasets.\n\n$$\n\\frac { 1 0 \\times \\ \\phantom { 1 0 } { 1 0 } } { \\mathbf { R u n t i m e } \\ [ \\mathbf { s } ] } \\ \\stackrel { 1 0 } { 5 } \\ \\stackrel { 5 0 } { 3 } \\ \\frac { 1 0 0 } { 5 0 7 \\pm 1 8 \\ 3 . 3 8 6 \\pm 8 7 }\n$$\n\nTable 3: Runtime of $p _ { \\mathrm { L M C } } $ for large graphs with up to 200 nodes. All graphs were modeled as ER- $n { - } 1$ , $\\textbf { \\textit { n } } \\in$ $\\{ 1 0 , 5 0 , 1 0 0 , 2 0 0 \\}$ . Data were generated using nonlinear conditionals and $N = 1 0 0 0$ samples. For each test we sample 100 permutations, sufficient to reject the null at $\\alpha = 1 \\%$ . As CI test we employed the GCM with boosted decision trees as regressor. See Supp. A.6 for more details.\n\n# 6.3 Causal Discovery Algorithms\n\nWhile the main scope of this work is to evaluate user-given graphs, we conduct additional experiments with $\\hat { \\mathcal G }$ inferred via causal discovery. On the Sachs et al. (2005) data we find that graphs inferred by NOTEARS and CAM are not significantly better than random, whereas graphs inferred by LiNGAM are not falsified using our metric at $\\alpha \\ : = \\ : 5 \\%$ (Tab. 2). Furthermore, a ranking based on our metric is in accordance with an SHD ranking $( \\mathrm { N O T E A R S } > \\mathrm { C A M } >$ LiNGAM). Both inequalities are significant with $p < 0 . 0 0 1$ for SHD and $p _ { \\mathrm { L M C } } $ when tested using a Wilcoxon signedrank test. For further experimental results see Supp. C.3.\n\n# 6.4 Runtime\n\nLarge graphs may entail thousands of CIs that need to be tested in order to compute our metric. Therefore, we evaluated the feasibility of applying $p _ { \\mathrm { L M C } } $ to graphs with up to 200 nodes (Tab. 3). We find that runtimes are reasonably fast $( < \\mathrm { { 1 h } ) }$ even for very large graphs. Note, that domain experts and causal discovery methods would likely take much longer to come up with a DAG $\\hat { \\mathcal G }$ in the first place. E.g., on DAGs with 50 and 100 nodes CAM and NOTEARS are about an order of magnitude slower, respectively (Rolland et al. 2022; Lachapelle et al. 2020). More information on the algorithmic complexity of our metric is provided in Supp. B.\n\n# 7 Discussion\n\nIn this work we addressed the lack of a suitable metric to evaluate an estimated DAG on observed data. To this end we discussed an existing absolute metric that, without a baseline comparison, is difficult to interpret.\n\nWe defined a set of properties that a suitable baseline should satisfy and found that sampling random node permutations of the given DAG is a natural way to satisfy these requirements. Using this baseline, we derived a novel metric which comprises two tests that measure first, how characteristic a given graph is in the sense that it is falsifiable by testing CIs and second, whether the given graph is significantly better than a random one in terms of CIs. Using graphs originating from emulated domain experts and causal discovery algorithms we evaluated our method on two types of data. Synthetic data with known true DAG and real-world data with a consensus graph established in the literature. Furthermore, we contributed a novel data set from cloud monitoring where an estimate of the true causal graph is given by the inverted dependency graph.\n\nWe acknowledge that our baseline may seem like a low bar to clear and other baselines could be considered in future work. However, we argue that besides being a natural choice that satisfies a number of desirable properties (P1 – P3 in Sec. 4) our experiments show that ‘better than random’ is a surprisingly high threshold that simulated domain experts and causal discovery algorithms fail to meet in many settings. We also note that applying our algorithm to DAGs inferred by causal discovery algorithms should be done with caution as elaborated in Sec. 5.1 and other approaches may be better suited for this purpose (Faller et al. 2024). Furthermore, in certain applications, specific suggestions for local edge improvements that go beyond the simple report of the triplets that result in LMC violations may be desired, and we leave this interesting direction for future work. Another promising direction could be to extend our nodepermutation baseline to the likelihood $p ( \\mathcal { D } | \\hat { \\mathcal { G } } )$ (c.f. Sec. 2).",
    "institutions": [
        "German Cancer Research Center (DKFZ)",
        "Heidelberg University",
        "Amazon Research Tübingen",
        "University Hospital Tübingen"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n理解系统变量间的因果关系对解释和控制其行为至关重要，但许多现实系统的真实因果图难以获取，需依靠算法或专家预测。现有评估因果图的指标只能给出图与观测数据的绝对不一致数量，缺乏基线，难以判断可接受的不一致程度。该问题的重要性在于，在使用因果图进行下游任务前，准确评估其质量能避免因错误的图结构导致错误的干预结论。\\n\\n**方法概述**\\n通过节点置换构建基线，提出一种新颖的一致性度量指标。将图的不一致数量与基线的不一致数量进行比较，得到可解释的度量指标，以判断图是否显著优于随机图。\\n\\n**主要贡献与效果**\\n- 提出了一种基于节点置换基线的因果图评估指标，通过实验表明，在模拟和真实数据集上，真实图不会被该指标证伪，而假设用户给出的错误图很可能被证伪。例如，在合成数据实验中，当领域专家拥有完整知识（对应DE - V模型中$|K|/|V| = 1$，DE - $\\mathcal{E}$模型中$SHD/|\\mathcal{E}| = 0$）时，以$\\alpha = 1\\%$的显著性水平拒绝“图与随机节点置换一样差”的假设（$p_{LMC} < 0.005$）；在真实世界数据实验中，以$\\alpha = 5\\%$的显著性水平拒绝相同假设。\\n- 贡献了一个来自云监控的新数据集，其中反向调用图可作为真实因果图的估计。\\n- 提出了$p_{TPa}$这一衡量与$\\hat{\\mathcal{G}}$马尔可夫等价的DAG比例的度量，并结合$p_{LMC}$根据Popper的证伪理论对给定图是否可证伪进行解释。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n为解决现有因果图评估指标缺乏基线的问题，通过随机置换给定有向无环图（DAG）的节点来构建基线。将给定图的局部马尔可夫条件（LMC）违反数量与基线的违反数量进行比较，判断图是否显著优于随机图。这种方法背后的直觉是，若图的违反情况明显优于随机置换得到的图，则说明该图具有一定的合理性和有效性。\\n\\n**创新点**\\n现有评估方法要么只给出图与观测数据的绝对不一致数量而无基线，导致难以判断可接受的不一致程度；要么依赖相关系统的已知DAG或缺乏基线而难以解释。本文通过节点置换构建基线，得到可解释的度量指标，克服了这些问题。同时提出了$p_{TPa}$度量来衡量与给定图马尔可夫等价的DAG比例，结合$p_{LMC}$能更好地判断图的特性和质量。\\n\\n**具体实现步骤**\\n1. **定义相关概念**：评估给定有向无环图 $\\hat{\\mathcal{G}} = (V, \\hat{\\mathcal{E}})$ 与观测数据 $\\mathcal{D}$ 的一致性，假设存在未知的真实因果图 $\\mathcal{G}^*$。定义LMC违反的集合$V_{LMC}^{\\hat{\\mathcal{G}},\\mathcal{D}}$和违反比例$\\phi_{LMC}^{\\hat{\\mathcal{G}},\\mathcal{D}}$。\\n2. **确定基线性质**：提出基线应满足的性质，包括推断的条件独立性数量与给定图相同（P1）、将条件独立性按条件集大小的划分数量与给定图相同（P2）、推断的条件独立性在半图oid公理下封闭（P3）。\\n3. **构建基线**：通过对给定图$\\hat{\\mathcal{G}}$进行节点置换来构建基线。设$S_n$为图顶点的置换集合，对于任意置换$\\pi \\in S_n$，得到置换后的图$\\sigma(\\mathcal{G})$。所有通过置换得到的图构成的集合$O(\\hat{\\mathcal{G}})$满足上述基线性质。均匀采样节点置换可实现从$O(\\hat{\\mathcal{G}})$中的图和马尔可夫等价类的均匀采样。\\n4. **提出假设检验**：提出零假设$H_0$：DAG $\\hat{\\mathcal{G}}$是从节点置换不变的分布$Q$中均匀随机抽取的。以$V_{LMC}^{\\hat{\\mathcal{G}},\\mathcal{D}}$为检验统计量，通过蒙特卡罗采样近似计算$p_{LMC}$值，即$p_{LMC}^{\\hat{\\mathcal{G}},\\mathcal{D}} = \\Pr(\\vert V_{LMC}^{\\sigma(\\hat{\\mathcal{G}}),\\mathcal{D}}\\vert \\leq \\vert V_{LMC}^{\\hat{\\mathcal{G}},\\mathcal{D}}\\vert)$。同时，定义了另一个度量$p_{TPa}^{\\hat{\\mathcal{G}}} = \\Pr(\\vert V_{TPa}^{\\sigma(\\hat{\\mathcal{G}}),\\hat{\\mathcal{G}}}\\vert \\leq \\vert V_{TPa}^{\\hat{\\mathcal{G}},\\hat{\\mathcal{G}}}\\vert)$，用于衡量与$\\hat{\\mathcal{G}}$马尔可夫等价的DAG的比例。\\n5. **解释指标含义**：根据Popper的证伪理论，对$p_{LMC}$和$p_{TPa}$进行解释。若$p_{TPa}^{\\hat{\\mathcal{G}}} \\leq \\alpha$，则$\\hat{\\mathcal{G}}$可通过测试隐含的条件独立性被证伪；若在此基础上$p_{LMC}^{\\hat{\\mathcal{G}},\\mathcal{D}} > \\alpha$，则$\\hat{\\mathcal{G}}$被证伪；若$p_{LMC}^{\\hat{\\mathcal{G}},\\mathcal{D}} \\leq \\alpha$，则没有基于条件独立性的证据反对$\\hat{\\mathcal{G}}$，不能证伪$\\hat{\\mathcal{G}}$。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- R 包 dagitty (Textor et al. 2016)：通过 d - 分离和全局马尔可夫条件测试图结构隐含的条件独立性（CI）关系来评估图 - 数据集的一致性。\\n- Reynolds et al. (2022)：验证给定的 DAG，同时测试 d - 分离隐含的 CIs 和图隐含的依赖关系是否导致观测中的边际依赖（忠实性）。\\n- Pitchforth and Mengersen (2013)：通过验证给定的贝叶斯网络（BN）是否与文献中已建立的同一领域的 BNs 相似来进行验证。\\n- 贝叶斯结构学习方法（Madigan, York, and Allard 1995; Friedman and Koller 2003）：估计图的后验概率 $p(\\mathcal{G} \\mid \\mathcal{D})$。\\n\\n**性能对比**\\n*   **在衡量图的一致性指标方面**：现有方法如 dagitty 和 Reynolds 等人的方法直接报告图与观测数据的原始不一致数量，缺乏基线，难以判断可接受的不一致程度；而本文方法通过节点置换构建基线，得到可解释的指标，能判断图是否显著优于随机图。例如，在合成数据实验中，对于真实图（DE - V模型中$|K|/|V| = 1$，DE - $\\mathcal{E}$模型中$SHD/|\\mathcal{E}| = 0$），本文方法得到的$p_{LMC}$值小于$0.005$，以$\\alpha = 1\\%$的显著性水平拒绝“图与随机节点置换一样差”的假设，说明真实图显著优于随机图。而对于模拟的领域专家给出的图，随着领域知识的减少，$p_{LMC}$值单调增加，表明这些图与随机图的差异逐渐减小。在真实世界数据实验中，对于真实图，本文方法以$\\alpha = 5\\%$的显著性水平拒绝“图与随机节点置换一样差”的假设。如在Protein Signaling Network、Auto MPG和Application Performance Monitoring三个数据集中，真实图的$p_{LMC}$值分别为$0.031 \\pm 0.006$、$0.03 \\pm 0.0052$和$0.00$，均显著优于随机图。而若使用$\\phi_{LMC}^{\\mathcal{G}^*,\\mathcal{D}}$作为指标，会因该指标值较高而错误地拒绝真实因果图。\\n*   **在处理因果发现算法推断的图方面**：在 Sachs 等人（2005）的数据上，本文方法对因果发现算法推断的图进行评估，发现 NOTEARS 和 CAM 推断的图并不显著优于随机图，而 LiNGAM 推断的图在 $\\alpha = 5\\%$ 下未被证伪，且基于本文指标的排名与基于结构汉明距离（SHD）的排名一致。NOTEARS和CAM推断的图的$p_{LMC}$值分别为$0.548 \\pm 0.237$和$0.0724 \\pm 0.0866$，表明它们与随机图相比没有显著优势；而LiNGAM推断的图的$p_{LMC}$值为$0.0362 \\pm 0.1290$，在$\\alpha = 5\\%$的水平下未被证伪，优于前两者。\\n*   **在运行时间方面**：本文评估$p_{LMC}$的方法在处理大到200个节点的图时，运行时间均小于1小时，而在相同规模的图上，CAM和NOTEARS等方法的运行时间分别比本文方法慢一个数量级，说明本文方法在运行效率上具有明显优势。\",\n    \"keywords\": \"### 关键词\\n\\n- 因果图评估 (Causal Graph Evaluation, N/A)\\n- 节点置换基线 (Node Permutation Baseline, N/A)\\n- 局部马尔可夫条件 (Local Markov Condition, LMC)\\n- 合成与真实数据 (Synthetic and Real-world Data, N/A)\\n- 条件独立性检验 (Conditional Independence Test, N/A)\\n- 因果发现算法 (Causal Discovery Algorithm, N/A)\"\n}"
}