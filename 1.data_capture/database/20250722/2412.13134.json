{
    "link": "https://arxiv.org/abs/2412.13134",
    "pdf_link": "https://arxiv.org/pdf/2412.13134",
    "title": "Practicable Black-box Evasion Attacks on Link Prediction in Dynamic Graphs -- A Graph Sequential Embedding Method",
    "authors": [
        "Jiate Li",
        "Meng Pang",
        "Binghui Wang"
    ],
    "publication_date": "2024-12-17",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "paper_content": "# Practicable Black-Box Evasion Attacks on Link Prediction in Dynamic Graphs —a Graph Sequential Embedding Method\n\nJiate Li1 2, Meng Pang1\\*, Binghui Wang2\\*\n\n1Nanchang University, Nanchang, China 2Illinois Institute of Technology, Chicago, USA jetrichardlee@gmail.com, pangmeng $1 9 9 2 @$ gmail.com, bwang70@iit.edu\n\n# Abstract\n\nLink prediction in dynamic graphs (LPDG) has been widely applied to real-world applications such as website recommendation, traffic flow prediction, organizational studies, etc. These models are usually kept local and secure, with only the interactive interface restrictively available to the public. Thus, the problem of the black-box evasion attack on the LPDG model, where model interactions and data perturbations are restricted, seems to be essential and meaningful in practice. In this paper, we propose the first practicable black-box evasion attack method that achieves effective attacks against the target LPDG model, within a limited amount of interactions and perturbations. To perform effective attacks under limited perturbations, we develop a graph sequential embedding model to find the desired state embedding of the dynamic graph sequences, under a deep reinforcement learning framework. To overcome the scarcity of interactions, we design a multienvironment training pipeline and train our agent for multiple instances, by sharing an aggregate interaction buffer. Finally, we evaluate our attack against three advanced LPDG models on three real-world graph datasets of different scales and compare its performance with related methods under the interaction and perturbation constraints. Experimental results show that our attack is both effective and practicable.\n\n# Code — https://github.com/JetRichardLee/GSE-METP\n\nDynamic graph sequences are a crucial structure for capturing the temporal dynamics of real-world systems. They represent the timing information within a dynamic graph, such as the historical evolution of interactions in a system. Link Prediction in Dynamic Graphs (LPDG), illustrated in Figure 1, leverages a dynamic graph sequence as input to predict future links for the next timestamp. Accurate predictions are highly valuable for a wide range of applications, including online recommendations, traffic management, and modeling disease contagion. In recent years, numerous methods for LPDG have been developed, including DynGEM (Goyal et al. 2018), MTCP (Zhou et al. 2018), EvolveGCN (Pareja et al. 2019), DyGCN (Manessi, Rozza, and Manzo 2020), and MetaDyGNN (Yang et al. 2022a). These methods differ in their prediction settings. For example, MTCP and DyGCN focus on predicting the entire future graph at the next timestamp, whereas EvolveGCN and DynGEM provide predictions for the next set of changing edges.\n\n![](images/8efae13436b5598423cb486f68bc3d96eaa7a652f0d4715d9b2599b10b8a1fbe.jpg)  \nFigure 1: Illustration of LPDG: a dynamic graph sequence is taken as an input, and the LPDG model is trained to predict the future graph in the next time slice.\n\n# Introduction\n\nLike adversarial attacks on static graphs, recent work (Fan et al. 2021) shows that LPDG for dynamic graphs is also vulnerable to black-box evasion attack, a practical attack where the attacker has no access to parameters or structure of the target model. However, the existing attack (called SAC) (Fan et al. 2021) based on reinforcement learning (RL) requires millions of interactions with the target model. Despite that, due to a coarse embedding method on the graph sequence, SAC is unable to attack large-scale graphs with large state spaces. These limitations inhibit SAC from being a practicable attack method in real-world scenarios.\n\nWe propose the first practicable black-box evasion attack on LPDG methods. Given a target model and instance, we generate the perturbed instance with only a few interactions with the model, while achieving promising attack results. Our attack method mainly consists of two novel designs:\n\n1) A graph sequential embedding (GSE). GSE has a static degree embedding for every graph in the sequence, which enables it to find the desired state representations of the embedding sequence using two Long Short-term Memory (LSTM) (Hochreiter and Schmidhuber 1997) networks. Later, these representations can be applied to perform the black-box attack based on a deep deterministic policy gradient (DDPG) (Silver et al. 2014) framework.\n\n2) A multi-environment training pipeline (METP). Considering that our model does not have sufficient resources for RL due to the limit on the interactions per instance, we hypothesize that there are similarities between instances from the same dataset and the same target model, and that training experience can be shared between them. Based on this hypothesis, we design a multi-environment training pipeline that sets multiple target instances as environments and trains a single model to attack them all.\n\nVia testing on three recent LPDG models, Dynamic Graph Convolutional Network (DyGCN) (Manessi, Rozza, and Manzo 2020), Attention Based Spatial-Temporal Graph Convolutional Network (ASTGCN) (Guo et al. 2019) and Hyperbolic Temporal Graph Network (HTGN) (Yang et al. 2022b), over three real-world datasets, our method proves to be both reliable and effective under the practicable constraints and achieves state-of-the-art performance. Ablation experiments demonstrate the rationality of our graph sequential embedding design and multi-environment design.\n\nOur main contributions are summarized as follows:\n\nWe propose the first practicable black-box evasion attack against LPDG, which learns effective attacks by only a few amount of interactions with the target model. We develop a GSE method that assists to find reliable state embeddings for the RL framework to learn effective attacks. Furthermore, we design a METP to overcome the constraint of target model interactions by sharing experience between multiple instances. Evaluation results demonstrate the superiority of our practicable black-box attacks. Ablation experiments prove the rationality and effectiveness of our designs.\n\n# Related Work\n\n# Adversarial Attacks on Static Graphs\n\nExisting attacks on graph learning for static graphs are classified as poisoning attacks and evasion attacks. Poisoning attack (Dai et al. 2018; Zu¨gner and Gu¨nnemann 2019; Xu et al. 2019; Takahashi 2019; Liu et al. 2019; Sun et al. 2020; Zhang et al. 2021; Wang, Pang, and Dong 2023; Yang et al. 2024) is performed in the training phase (and testing phase). In training-time poisoning attacks, given a graph learning algorithm and a graph, an attacker carefully perturbs the graph (e.g., inject new edges to or remove the existing edges from the graph, perturb the node features) in the training phase, such that the learnt model misclassifies as many nodes, links, or graphs as possible on the clean testing graph(s). A special type of poisoning attack, called backdoor attack (Zhang et al. 2021), also perturbs testing graphs in the testing phase. Further, Yang et al. (2024) generalize the backdoor attack on graphs in the federated learning setting (Wang et al. 2022).\n\nEvasion attack (Dai et al. 2018; Xu et al. 2019; Wang and Gong 2019; Wu et al. 2019; Ma et al. 2019; Ma, Ding, and Mei 2020; Li et al. 2021; Mu et al. 2021; Wang, Li, and Zhou 2022; Wang, Pang, and Dong 2023; Wang et al. 2024) is performed in the inference phase. In these attacks, given a graph learning model and clean graph(s), an attacker carefully perturbs the graph structure or/and node features such that as many nodes or graphs as possible are misclassified on the perturbed graph(s) by the given model.\n\n# Deep Deterministic Policy Gradient\n\nRL approaches with the actor-critic structure have been proposed and received great attention. In Haarnoja et al. (2018), the soft actor-critic (SAC) is described, with an additional reward on the entropy of the policy distribution. In Schulman et al. (2017), it proposes the proximal policy optimization (PPO), based on the trust region policy optimization (TRPO) (Schulman et al. 2015), with the clipped surrogate objective added to the loss function as refinement. In (Silver et al. 2014), the deterministic policy gradient (DPG), whose policy gives a deterministic action instead of a possibility distribution, is proposed. And in (Lillicrap et al. 2015), deep deterministic policy gradient (DDPG) is proposed with the extension of deep reinforcement learning to DPG.\n\n# Practicable Black-box Evasion Attack\n\nIn this section, we first define the practicable black-box evasion attack problem, and then introduce our two main designs, the graph sequential embedding (GSE) and the multienvironment training pipeline (METP). GSE learns the hidden sequential features of the dynamic graph sequence, and gives the embedding states, which are used by the policy network and the Q network of a DDPG agent. The agent’s policy network and Q network are trained under the METP and propagate gradients back to train the GSE models.\n\n# Problem Definition\n\nLPDG Model Simulating a real-world attacking scenario, the LPDG model is private on its structure and parameters, but public on prediction interactions in limited chances. We only have the interface for inputting a dynamic graph sequence and receiving the prediction.\n\nClean input data A finite sequence of undirected graphs ${ \\mathcal { G } } ~ = ~ \\{ { \\bar { G } } _ { 1 } , G _ { 2 } , . . . , G _ { T } \\}$ is given as the input data, where $G _ { t } = ( V _ { t } , E _ { t } ) , \\forall t \\in [ 1 : T ]$ denotes a snapshot graph at the time slice $t$ . $V _ { t }$ is the set of nodes and $E _ { t }$ is the set of edges. In our attack setting, we could perform limited perturbations on the edge data $\\mathcal { E } = \\{ E _ { t } , \\forall t \\in [ 1 : T ] \\}$ .\n\nOutput and metric The target LPDG model $M$ is a blackbox function. It predicts the edges of the graph at the next time slice $G _ { T + 1 } ^ { p } \\bar { \\bf \\Phi } = M ( \\mathcal { G } ) = ( \\bar { V } _ { T + 1 } ^ { p } , E _ { T + 1 } ^ { p } )$ and evaluates the performance by comparing $E _ { T + 1 } ^ { p }$ with the ground truth $E _ { T + 1 } \\in G _ { T + 1 }$ . We use the F1 score as a metric to evaluate the prediction and expect the attack to fool the model by having as low F1 as possible.\n\n$$\n\\begin{array} { r l } & { P r e c i s i o n = \\frac { \\left| E _ { T + 1 } \\cap E _ { T + 1 } ^ { p } \\right| } { \\left| E _ { T + 1 } ^ { p } \\right| } } \\\\ & { R e c a l l = \\frac { \\left| E _ { T + 1 } \\cap E _ { T + 1 } ^ { p } \\right| } { \\left| E _ { T + 1 } \\cap E _ { T + 1 } ^ { p } \\right| + \\left| E _ { T + 1 } \\cap \\complement E _ { T + 1 } ^ { p } \\right| } } \\\\ & { F _ { 1 } = \\frac { 2 * P r e c i s i o n * R e c a l l } { P r e c i s i o n + R e c a l l } } \\end{array}\n$$\n\nAttack Under Reinforcement Learning Since we have no knowledge of the LPDG model but only chances to interact with it, it requires us to learn from experience generated by limited exploration. Therefore, we apply a reinforcement learning method as our basic framework to perform the attack. We define our attack problem as follows.\n\nState The perturbed dynamic graph sequence $\\begin{array} { r l } { \\hat { \\mathcal { G } } } & { { } = } \\end{array}$ $\\{ \\hat { G } _ { 1 } , \\hat { G } _ { 2 } , . . . , \\hat { G } _ { T } \\}$ describes the ground information during t{he atta2ck. SinTc }we only care about the $\\hat { E } _ { T + 1 } ^ { p } \\ \\in \\ \\hat { G } _ { T + 1 } ^ { p }$ in the link prediction result, we assume that the nodes in the graph sequence are fixed as $\\hat { V } _ { t } ~ = ~ V , t ~ \\in ~ [ 1 , T + 1 ]$ and basically, use an adjacency matrix sequence $\\begin{array} { r } { \\hat { \\cal A } = \\left\\{ \\begin{array} { r l r l } \\end{array} \\right. } \\end{array}$ ${ \\hat { A } } _ { 1 } , { \\hat { A } } _ { 2 } , . . . , { \\hat { A } } _ { T } \\ \\}$ to represent the state of the dynamic graph sequence. This raw representation has a large state space and lacks the essential feature extraction, making the policy and the Q difficult to converge. Therefore, in this paper we will introduce a new way to represent the state.\n\nAction The action we take in the RL framework is the perturbation we apply to the dynamic graph sequence. Each time we perform an action $a$ , we add one edge $( u , v ) , u , v \\in$ $V$ and delete one edge $( u ^ { \\prime } , v ^ { \\prime } ) , u ^ { \\prime } , v ^ { \\prime } \\in V$ for all the $\\hat { A } _ { t } \\in \\hat { { \\cal A } }$ in the sequence and obtain a new matrix sequence $\\hat { \\mathcal { A } } ^ { \\prime }$ :\n\n$$\n\\hat { A } _ { t } ^ { \\prime } = \\hat { A } _ { t } ( [ u ] [ v ] = 1 , [ u ^ { \\prime } ] [ v ^ { \\prime } ] = 0 ) , t \\in [ 1 , T ]\n$$\n\nEnvironment and reward In our black-box attack on LPDG, the environment is the LPDG model $M$ and the original dynamic graph sequence $\\mathcal { G }$ . At the attack step $k$ , we perform an attack action $a _ { k }$ to the edge sequence $\\hat { \\mathcal { A } } _ { k }$ and get the next attacked sequence $\\hat { \\mathcal { A } } _ { k + 1 }$ . We feed it into the model $M$ and get the output Eˆp $\\hat { E } _ { T + 1 , k + 1 } ^ { p }$ . The F1 score metric $f _ { k + 1 }$ is computed by comparing it with the original $E _ { T + 1 } \\in G _ { T + 1 }$ . Depending on how much the prediction performance decreases from the previous prediction metric $f _ { k }$ , we give the reward for the action $a _ { k }$ and the state $S _ { k } = \\hat { A } _ { k }$ :\n\n$$\nr ( S _ { k } , a _ { k } ) = f ^ { k + 1 } - f ^ { k }\n$$\n\nPracticable constraints There are two constraints for the agent to satisfy the practicability requirement. First, for each attack instance $M$ and $\\mathcal { G }$ , it is only able to perturb a few amount of edges. We set the $\\delta$ as the ratio limit and $n$ as the amount limit. For an attack attempt, we could only have $K$ perturbations in total, which satisfies the constraint:\n\n$$\nK = m i n ( \\delta | E _ { m a x } | , n )\n$$\n\n$| E _ { m a x } | = | V | ^ { 2 } / 2$ is the maximum number of edges on the graph and $n$ is a relatively small constant. In the second constraint, the agent only has limited chances to interact with the target model $M$ . To learn attacks for one instance, we could only query $M$ within $I$ times during the training.\n\n# Graph Sequential Embedding\n\nAs mentioned before, the state space of the raw matrix sequence $\\hat { A }$ is too large, and makes the deep reinforcement learning difficult to converge. Therefore, we develop a graph sequential embedding (GSE) method, to embed it into a smaller state space. Our GSE method consists of two modules, a static degree embedding module and a dynamic sequential embedding module.\n\n![](images/4151681fb4da5031cf78d2a332f731ae8ffef49fc75b1c945c2a5771027710f7.jpg)  \nFigure 2: Illustration of the proposed GSE method.\n\nStatic Degree Feature In the static degree embedding, we first compute a degree feature to embed each graph matrix in the sequence first. In step a, we multiply the average adjacency matrix $\\tilde { A } = a v e r a g e ( \\mathcal { A } )$ of the original dynamic graph sequence, with an all-one tensor in the shape of $| V | \\times 1$ , which gives the average degree of each node in the original sequence. We multiply this again by the average adjacency matrix and get the amount of nodes connected within two steps. This doesn’t work exactly for the three steps and the four steps, but we use the same method to approximate them. We describe this as:\n\n$$\nd = \\{ d _ { 0 } , \\tilde { A } d _ { 0 } , \\tilde { A } ^ { 2 } d _ { 0 } , \\tilde { A } ^ { 3 } d _ { 0 } \\} , d _ { 0 } = [ 1 ] _ { | V | \\times 1 }\n$$\n\nIn the step b, we normalize each of the last three separately with the batchnorm function $b n$ . We then additionally inject a $| V | \\times 4$ random noise feature by concatenation to enhance the feature’s expressiveness (Sato, Yamada, and Kashima 2021). Our final degree feature $F$ is represented by:\n\n$$\nF = [ [ R a n d o m ] _ { | V | * 4 } , d _ { 0 } , b n ( d _ { 1 } ) , b n ( d _ { 2 } ) , b n ( d _ { 3 } ) ]\n$$\n\nThis feature is calculated at the start of the attack for each instance, and it remains static throughout the learning.\n\nDynamic Sequence Embedding After we compute the static degree feature $F$ , we multiply it with each matrix $\\hat { A } _ { t } , t \\in [ 1 , T ]$ to get the degree embedding sequence $\\scriptstyle { \\mathcal { X } } \\ =$ $\\{ X _ { 1 } , X _ { 2 } , . . . , X _ { T } \\} \\in R ^ { T \\times | V | \\times | 8 | }$ :\n\n$$\n\\boldsymbol { \\mathcal { X } } = \\hat { \\boldsymbol { \\mathcal { A } } } \\boldsymbol { F }\n$$\n\nThis process is described as step a in Figure 2. Then in step b, we use an LSTM module, to dynamically find the efficient embedding state $S$ for the graph sequence through training. For simplicity, we denote this process as $S E$ :\n\n$$\n\\begin{array} { r l } { h _ { 0 } = X _ { 1 } , } & { c _ { 0 } = 0 , } \\\\ { L _ { i } ( \\cdot ) = W _ { i , - } X _ { t } + b _ { i , \\cdot } , } & { L _ { h } ( \\cdot ) = W _ { h , - } h _ { t - 1 } + b _ { h , \\cdot } , } \\\\ { i _ { t } = \\sigma ( L _ { i } ( i ) + L _ { h } ( i ) ) , } & { f _ { t } = \\sigma ( L _ { i } ( f ) + L _ { h } ( f ) ) , } \\\\ { g _ { t } = t a n h ( L _ { i } ( g ) + L _ { h } ( g ) ) , } & { o _ { t } = \\sigma ( L _ { i } ( o ) + L _ { h } ( o ) ) , } \\\\ { c _ { t } = f _ { t } c _ { ( t - 1 ) } + i _ { t } g _ { t } , } & { h _ { t } = o _ { t } t a n h ( c _ { t } ) , \\quad \\forall t \\in [ 1 , T ] } \\\\ { S = S E ( \\mathcal { X } ) = h _ { T } } \\end{array}\n$$\n\nLSTM-p $\\hat { A } _ { \\bf k }$ F &k SP ?→delete edge ： Ak+1 M Reward rk Pee eek Aa LPDG Repler Adjanqcecy Matrix Adjancey Matrix\n\nIn our attack design, we actually use two different GSE models to train embedding states for the actor and the critic. They share the same degree embedding $\\chi$ , but separate LSTM modules and result states. We denote the LSTM modules as LSTM- $\\cdot \\mathbf { p }$ and LSTM-q, the embedding processes as $S E _ { p }$ and $S E _ { q }$ , the embedding states as $S ^ { p }$ and $S ^ { q }$ for the policy network and the Q network respectively.\n\n# Multi-Environment Training\n\nOur multi-environment training is designed based on a DDPG method. In traditional reinforcement learning, an agent is trained in one or more but identical environments. However, in a practicable attack, we have a limit on the amount of interactions for each attack instance. Hence, we train one agent under several separate instances instead. We attack them alternately and learn our agent from the collective experience they share. These instances are under the same dataset and the same target model, and we believe that they have similarities in the state space and help to explore more efficiently.\n\nThere are three main tasks in our multi-environment training pipeline, (1) perform attack interactions with each instance against the target model; (2) train the Q network and the Q GSE module; (3) train the policy network and the policy GSE module. In each attack step, we perform the three tasks sequentially.\n\nAttack Interaction The Figure 3 describes how our agent performs the attack on each instance. In each attack step $k \\in [ 1 , K ]$ , we have the adjacency matrix sequence $\\hat { \\mathcal { A } } _ { k }$ generated from the $k - 1$ step, and the static degree feature $F$ computed in advance. In step a, we apply our graph sequential embedding method, through the degree feature $F$ and the LSTM-p module, and obtain the degree embedding $\\mathcal { X } _ { k }$ and the graph sequential embedding $S _ { k } ^ { p }$ . In step b, we design our policy network $\\mu _ { \\boldsymbol { \\theta } }$ based on an MLP network, denoted $M L P _ { \\theta }$ , to select our add and delete action:\n\n$$\n\\mu _ { \\theta } ( S _ { k } ^ { p } ) = S i g m o d ( M L P _ { \\theta } ( S _ { k } ^ { p } ) ) * | V |\n$$\n\nHowever, there are two situations where we will use a random action instead. First, the interaction records in the shared buffer are smaller than the batch size, so we need to fill it with random attacks first and allow our agent to start training in the following sections. Otherwise, the buffer would be filled with similar actions generated by the untrained policy, leading the agent to a meaningless training result. The second case is that we hope to explore the state space during the attack, so when we go for a certain attack steps we will apply a random action instead. Finally our action $a _ { k }$ is selected from:\n\n![](images/e83c41002d72d1f941bea18e6d2167a902717b7980f8a597d698c381ca2e310b.jpg)  \nFigure 3: An overview of the agent interaction.   \nFigure 4: Training pipeline for the Q network and Q GSE.\n\n$$\na _ { k } = \\left\\{ \\begin{array} { l l } { \\mu _ { \\theta } ( S _ { k } ^ { p } ) , } & { a t t a c k } \\\\ { a _ { r a n d } , } & { r a n d o m } \\end{array} \\right.\n$$\n\nAfter obtaining the action $a _ { k }$ , we apply it to the previous sequence $\\hat { \\mathcal { A } } _ { k }$ , generate a new graph sequence $\\hat { \\mathcal { A } } _ { k + 1 }$ , perform the prediction on the target LPDG model $M$ and obtain the reward $r _ { k }$ in step c. We write the embedding sequence $\\mathcal { X } _ { k }$ , the action $a _ { k }$ and the reward $r _ { k }$ to the shared replay buffer in step d.\n\nQ Training The Figure 4 describes the process of training the Q network and the Q GSE model. The shared buffer has received experience from several instances, and we sample them collectively. The sample consists of a batch of embedding sequences $\\mathcal { X } _ { b }$ , actions $a _ { b }$ and rewards $r _ { b } = r ( \\mathcal { X } _ { b } , a _ { b } )$ from the shared buffer. In step a, we feed them to the LSTMq to get their sequential embedding states $S ^ { q }$ for the critic:\n\n$$\nS ^ { q } = S E _ { q } ( \\mathcal { X } _ { b } )\n$$\n\nIn step $\\boldsymbol { \\mathbf { b } }$ , we use an MLP model, denoted as $M L P _ { q }$ , as our Q network:\n\n$$\nQ ( S ^ { q } , a _ { b } ) = M L P _ { q } ( [ a _ { b } , S ^ { q } ] )\n$$\n\na. Obtain the GSE state of the updated LSTM-q 。 LSTM-q' 1 GSEq c.Forward to obtain i the $\\scriptstyle { \\mathbf { Q } } _ { \\theta }$ value of the sq policy action and the I Embeddding Sequence Batch 1 updated Q'value ofI the batch action GSEp do SP LSTM-p Policy Action Values   \nb.Obtain the GSE state from μ0 a   \nI LSTM-p and the policy action 可向可   \nLoss=MSE(ag,ab) \\*decrease(Q,Q) Q'network dp μ0->μθ LSTM-p -> LSTM-p'   \nd. Calculate the loss and update Action Batch Q' Values   \nthe policy network and LSTM $\\cdot \\mathbf { p }$ ab\n\nIn the deep reinforcement learning, the value function $Q ( S , a )$ is usually defined as:\n\n$$\nQ ( S , a ) = r ( S , a ) + \\gamma Q ( S ^ { \\prime } , \\mu _ { \\theta } ( S ^ { \\prime } ) )\n$$\n\n$S ^ { \\prime }$ is the next state after $S$ takes the action $a$ . For the LPDG attack, however, the order of actions is not critical. If a set of actions $[ a _ { 1 } , a _ { 2 } , . . , a _ { K } ]$ leads the state from $S _ { 1 }$ to $S _ { K + 1 }$ and the order is changed arbitrarily, the final state will still be $S _ { K + 1 }$ . Therefore, we concentrate on learning the $\\mathrm { Q }$ -value for the reward itself:\n\n$$\nQ ( S , a ) = r ( S , a )\n$$\n\nand in step c we use the MSE loss to define the Q loss for the Q network and the Q GSE to train:\n\n$$\nl o s s _ { Q } = M S E ( Q ( S ^ { q } , a _ { b } ) , r _ { b } )\n$$\n\nFinally, we back propagate the Q loss to update the $\\mathrm { \\Delta Q }$ network and the Q GSE model with the learning rate $\\tau$ :\n\n$$\nQ ^ { \\prime }  \\tau Q + ( 1 - \\tau ) Q ^ { \\prime }\n$$\n\n$$\nL S T M _ { q } ^ { \\prime } \\gets \\tau L S T M _ { q } + ( 1 - \\tau ) L S T M _ { q } ^ { \\prime }\n$$\n\nPolicy Training The Figure 5 describes the process of training the policy network and the policy GSE under the multi-environment. Similar to the $\\mathrm { \\Delta Q }$ training, we have the same batch of interaction samples from the shared buffer: embedding sequences $\\mathcal { X } _ { b }$ and actions $a _ { b }$ . In step a, we forward embedding sequences to the updated LSTM- $\\cdot \\mathbf { q } ^ { , }$ module to obtain the GSE states $S ^ { q \\prime }$ of $\\mathrm { ~ Q ~ }$ :\n\n$$\nS ^ { q \\prime } = S E _ { q ^ { \\prime } } ( \\mathcal { X } _ { b } )\n$$\n\nIn step $\\boldsymbol { \\mathbf { b } }$ , we forward embedding sequences to the LSTM-p to obtain the GSE states $S ^ { p }$ . Then we feed them to the policy network $\\mu _ { \\boldsymbol { \\theta } }$ to get the policy actions $a _ { \\theta }$ :\n\n$$\nS ^ { p } = S E _ { p } ( \\mathcal { X } _ { b } ) , \\quad a _ { \\theta } = \\mu _ { \\theta } ( S ^ { p } )\n$$\n\nTable 1: A brief description of datasets.   \n\n<html><body><table><tr><td>Name</td><td>Node</td><td>Average Edges</td><td>Graph type</td></tr><tr><td>Haggle Facebook AS</td><td>274 1000 6474</td><td>12584 97779 141845</td><td>Human contact Social circle Traffic flows</td></tr></table></body></html>\n\nWe define our policy loss as the scalar product of the differences between the actions and the decreases in the $\\mathrm { \\Delta Q }$ values. This means that if an action in $a _ { \\theta }$ is worse than that in the $a _ { b }$ , we hope to learn towards the batch one, otherwise we hope to stay the same. We update the policy network and the policy GSE with the learning rate $\\tau$ as follows:\n\n$$\n\\begin{array} { c } { { d ( S ^ { q \\prime } , a _ { b } ) = m a x ( 0 , Q ^ { \\prime } ( S ^ { q \\prime } , a _ { b } ) - Q _ { \\theta } ( S ^ { q \\prime } , a _ { \\theta } ) ) } } \\\\ { { l o s s _ { \\mu _ { \\theta } } ( S ^ { q \\prime } , a _ { b } ) = M S E ( a _ { b } , a _ { \\theta } ) \\cdot d ( S ^ { q \\prime } , a _ { b } ) } } \\\\ { { \\mu _ { \\theta } ^ { \\prime }  \\tau \\mu _ { \\theta } + ( 1 - \\tau ) \\mu _ { \\theta } ^ { \\prime } } } \\\\ { { L S T M _ { p } ^ { \\prime }  \\tau L S T M _ { p } + ( 1 - \\tau ) L S T M _ { p } ^ { \\prime } } } \\end{array}\n$$\n\n# Experiments\n\nIn this section, we test our attack method on three real-world datasets against three LPDG models, compared with four baseline black-box evasion methods. The experiments consist of two aspects, the performance evaluation and the interaction scale impact test for ablation study.\n\nFor each setting, it consists of the attack method $\\mathcal { C }$ , the dataset $\\mathcal { D }$ , the LPDG model $M$ , the perturbation constraint $K = m i n ( \\delta | E _ { m a x } | , n )$ and the interaction constraint $I$ . We first take 10 different instances from the dataset $\\mathcal { D }$ , each consisting of 11 temporal graphs. The first 10 graphs are the input to $M$ and the last 1 is the ground truth for the prediction. We use these instances to train the target model $M$ , and apply the attack method $\\mathcal { C }$ to perform a black-box attack. Each attack consists of several attempts. In each attempt, the agent gives at most $K$ perturbations to the sequence, and the lowest prediction metric achieved during the perturbations is taken as the result of that attempt. The total number of interactions with the target model, taken by all attempts, could not exceed $I$ . Once interactions are exhausted, we take the best result of all attempts as the performance of the setting.\n\n# Experiments Setting\n\nDatasets We use three real-world datasets with varying scales, and their properties are shown in Table 1.\n\nHaggle This is a social network available at KONECT and published in (Kunegis 2013), representing the connection between users measured by wireless devices.\n\nFacebook This is a subgraph of the “Social circles: Facebook” social networks from SNAP (Leskovec and Sosicˇ 2016). We randomly delete edges to generate the dynamic graph sequences.\n\nAutonomous systems “AS-733” is a large traffic flow networks available on SNAP (Leskovec and Sosicˇ 2016). We randomly delete edges to generate the dynamic graph sequences as well.\n\n# Compared Attack\n\nRandom attack In this attack method, the agent randomly chooses two nodes to add an edge and two nodes to delete an edge as the action.\n\nSAC attack This attack is introduced by (Fan et al. 2021), which claims to be the first black-box evasion attack on LPDG problem. However, without any constraint on interactions, we conclude that this method is impracticable, and prove this in our experiments.\n\nSAC-METP attack This attack is an ablation version of our method. It applies the multi-environment training pipeline, but takes the same graph embedding method as (Fan et al. 2021).\n\nGSE attack This attack is an ablation version of our method. It applies the graph sequential embedding method, but the models for different instances are trained and applied separately.\n\nGSE-METP attack This attack is the complete version of our method. It applies both the graph sequential embedding method and the multi-environment training pipeline.\n\n# LPDG Model\n\nDyGCN Dynamic Graph Convolutional Network (DyGCN) (Manessi, Rozza, and Manzo 2020), is an extension of GCN-based methods. It generalizes the embedding propagation scheme of GCN to the dynamic setting in an efficient manner, and propagates the change along the graph to update node embedding.\n\nASTGCN Attention Based Spatial-Temporal Graph Convolutional Networks (ASTGCN) (Guo et al. 2019), has several independent components. Each of them consists of two parts, the spatial-temporal attention mechanism and the spatial-temporal convolution. In our adaption for experiments, we use one component to consist a test ASTGCN.\n\nHTGN Hyperbolic Temporal Graph Network (HTGN) (Yang et al. 2022b) is a temporal graph embedding method, which learns topological dependencies and implicitly hierarchical organization of each graph sequence individually and gives link predictions on it.\n\nAttack Settings For the performance evaluation, we set the default attack rate limit to $\\delta = 0 . 0 2$ , the default attack amount limit to $n = 1 0 0 0$ , and the default interaction limit to $I = 5 K$ to make the attack attempts complete in tests. Then, in the interaction impact test, we show our attack results in full on different $I \\in [ K , 1 0 K ]$ .\n\n# Experiments Results\n\nEffectiveness Evaluation Table 2 shows the performance of the four attack methods under the default setting against three LPDG models. As shown in the table, our GSE and GSE-METP methods perform best in these tests. Under constraints, the SAC method has good results on the small dataset, while in most cases it behaves close to or worse than the random method. This suggests that it is not a practicable method. Our GSE-METP method behaves effectively on all experiment settings. On the largest dataset AS, GSE-METP has significantly better performance than others. This validates that GSE-METP is the first practicable black-box evasion attack against LPDG methods.\n\nTable 2: Performance evaluations on the default setting.   \n\n<html><body><table><tr><td>Method</td><td>Haggle</td><td>Facebook</td><td>AS</td></tr><tr><td>Edge Ratio(δ)</td><td>2%</td><td>0.2%</td><td>4.8e-5</td></tr><tr><td>Edge Amount(n)</td><td>751</td><td>1000</td><td>1000</td></tr><tr><td>Interaction(I)</td><td>3755</td><td>5000</td><td>5000</td></tr><tr><td>DyGCN (Original)</td><td>0.9930</td><td>0.9745</td><td>0.8990 0.8862</td></tr><tr><td>Random SAC SAC-METP GSE(Ours)</td><td>0.8979 0.8149 0.8094 0.8043</td><td>0.9681 0.9665 0.9651 0.9629</td><td>0.8899 0.8890 0.8910</td></tr><tr><td>GSE-METP(Ours) ASTGCN(Original)</td><td>0.8118 0.9852</td><td>0.9653 0.9862</td><td>0.8573 0.8303</td></tr><tr><td>Random SAC SAC-METP GSE(Ours)</td><td>0.9752 0.9825 0.9750 0.9748</td><td>0.9298 0.9853 0.9859 0.9407</td><td>0.8276 0.8264 0.8263 0.8226</td></tr><tr><td>GSE-METP(Ours) HTGN (Original)</td><td>0.9702 0.9753</td><td>0.9011 0.9375</td><td>0.8020 0.8665</td></tr><tr><td>Random SAC SAC-METP GSE(Ours) GSE-METP(Ours)</td><td>0.8083 0.9603 0.9676 0.8311 0.7417 0.8494</td><td>0.8652 0.9145 0.9141 0.8930</td><td>0.8377 0.8526 0.8527 0.7944</td></tr></table></body></html>\n\nAblation Study Figure 6 shows our method is more effective as the interaction limit grows compared with other methods. First, on the small Haggle, SAC and SAC-METP are effective when the interactions are enough for training, but fail to converge to good results on the large Facebook and AS. Instead, our GSE and GSE-METP converge to better attacks. This ablation study proves the effectiveness of our GSE design. Second, SAC-METP and GSE-METP converge faster than SAC and GSE, and also result in better performances. This ablation study shows the efficiency of our multi-environment training pipeline design.\n\nWhy SAC fails? We further tracked the states and actions during SAC’s attack and ours to explore the reason that SAC fails. We found that SAC’s represented state has a relatively small variance during the attack, meaning it nearly does not change, while ours changed apparently. For instance, on a sample from Haggle on DyGCN, we observed SAC’s add actions converge to the $\\{ 0 , 0 \\}$ action, and the variance of its delete actions also decreases from $\\{ 8 0 3 2 , 4 8 9 0 \\}$ to $\\{ 3 9 , 8 4 \\}$ . In contrast, GSE-METP has average actions on $\\{ 9 7 , 1 2 6 \\}$ for adding and $\\{ 1 5 2 , 1 3 0 \\}$ for deleting and exhibits greater variance $\\{ 4 6 5 5 , 4 7 0 8 \\}$ for adding and $\\{ 4 3 8 8 , 7 4 0 \\bar { 5 } \\}$ for deleting. This suggests GSE-METP adapts its action nodes more responsively in different states, aligning with the ideal agent behavior. As for final performance, SAC only reduces accuracy to 0.854, while GSE-METP to 0.724.\n\n![](images/ad5fd76e6dfae88a9a8875d295c1a450a6570b5ce57093f034daacb33bb3ce19.jpg)  \nFigure 6: Interaction impacts on DyGCN, ASTGCN and HTGN.\n\nBased on the observation above, we made an inference for SAC’s poor performance: SAC relies on a ranking of node degrees as the state, which is relatively static during learning as each action pair changes the degrees of at most four nodes. Consequently, the generated state representations of dynamic graphs stay nearly constant during the attack process. When the replay buffer is filled with repetitive states, each with a high negative reward, the policy network tends to produce extreme actions to avoid further negative rewards. This results in SAC repeatedly selecting delete-add actions on the same node pair— $\\{ 0 , 0 \\}$ or $\\{ \\lor , \\lor \\}$ , which is apparently an undesired behavior.\n\n# Conclusion\n\nWe propose the first practicable black-box evasion attack against the link prediction in dynamic graph models. We design a graph sequential embedding method and a multienvironment training pipeline, and combine them with a deep reinforcement learning method, DDPG, to perform effective attacks under interaction and perturbation constraints. Experiments on three advanced LPDG methods demonstrate the effectiveness of our attack. Crucial future work is to design provably robust LPDG against the proposed evasion attacks, inspired by existing certified defense on static graphs (Wang et al. 2021; Xia et al. 2024).\n\n# Acknowledgments\n\nPang is supported in part by Natural Science Foundation of China (62466036), Natural Science Foundation of Jiangxi Province (20232BAB212025), High-level and Urgently Needed Overseas Talent Programs of Jiangxi Province (20232BCJ25024) and Key Research and Development Program of Jiangxi Province (20243BBG71035). Wang is supported in part by the Cisco Research Award and the National Science Foundation under grant Nos. ECCS2216926, CCF-2331302, CNS-2241713 and CNS-2339686.",
    "institutions": [
        "Nanchang University",
        "Illinois Institute of Technology"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决动态图链接预测（LPDG）模型的黑盒逃避攻击问题。在实际应用中，LPDG模型通常是私有的，仅提供有限的交互接口，攻击者在模型交互和数据扰动受限的情况下，对目标LPDG模型进行有效攻击的问题显得至关重要，这有助于评估LPDG模型的安全性，为模型改进提供依据。\\n\\n**方法概述**\\n论文提出了一种实用的黑盒逃避攻击方法，结合图序列嵌入（GSE）和多环境训练管道（METP），在深度强化学习框架下，通过有限的交互和扰动对目标LPDG模型进行有效攻击。\\n\\n**主要贡献与效果**\\n- 提出首个实用的黑盒逃避攻击方法，在有限交互和扰动下对目标LPDG模型进行有效攻击。在最大数据集AS上，GSE - METP方法显著优于其他方法，如在DyGCN模型上，GSE - METP将准确率降低到0.724，而SAC只能降低到0.854。\\n- 开发图序列嵌入（GSE）方法，辅助强化学习框架找到可靠的状态嵌入。消融实验表明，随着交互限制增加，在大数据集Facebook和AS上，GSE和GSE - METP能收敛到更好的攻击效果，证明了GSE设计的有效性。\\n- 设计多环境训练管道（METP），通过共享经验克服目标模型交互限制。SAC - METP和GSE - METP比SAC和GSE收敛更快，性能更好，体现了多环境训练管道设计的效率。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n该方法的核心思想是在深度强化学习框架下，通过图序列嵌入（GSE）将动态图序列嵌入到较小的状态空间，解决原始矩阵序列状态空间大、特征提取不足导致策略和Q网络难以收敛的问题；同时设计多环境训练管道（METP），在有限的交互下，通过多个实例共享经验来训练一个代理，提高攻击效率。其有效性在于GSE能提取动态图序列的隐藏特征，METP能让模型在有限交互下更高效地学习。\\n\\n**创新点**\\n先前基于强化学习的攻击方法（如SAC）需要与目标模型进行数百万次交互，且由于图序列的粗粒度嵌入方法，无法攻击具有大状态空间的大规模图。本文方法的创新点在于：一是提出了图序列嵌入（GSE）方法，通过静态度嵌入和动态序列嵌入模块，提取动态图序列的本质特征；二是设计了多环境训练管道（METP），在有限的交互下，通过多个实例共享经验进行训练。\\n\\n**具体实现步骤**\\n1. **问题定义**：定义LPDG模型为私有结构和参数，仅提供有限预测交互机会的黑盒函数。输入为有限的无向图序列，可对边数据进行有限扰动，用F1分数评估攻击效果。攻击在强化学习下的状态用扰动后的动态图序列表示，动作是对动态图序列的扰动，环境是LPDG模型和原始动态图序列，奖励根据预测性能的下降情况给出。同时存在两个实际约束，一是对每个攻击实例只能扰动少量边，二是代理与目标模型的交互机会有限。\\n2. **图序列嵌入（GSE）**：\\n    - **静态度特征计算**：计算原始动态图序列的平均邻接矩阵，与全1张量相乘得到节点平均度，再多次相乘近似不同步数的连接节点数量，最后进行归一化并注入随机噪声特征得到最终度特征。具体公式为 $d = \\{ d _ { 0 } , \\tilde { A } d _ { 0 } , \\tilde { A } ^ { 2 } d _ { 0 } , \\tilde { A } ^ { 3 } d _ { 0 } \\}$，$d _ { 0 } = [ 1 ] _ { | V | \\times 1 }$，$F = [ [ R a n d o m ] _ { | V | * 4 } , d _ { 0 } , b n ( d _ { 1 } ) , b n ( d _ { 2 } ) , b n ( d _ { 3 } ) ]$。\\n    - **动态序列嵌入**：将静态度特征与每个邻接矩阵相乘得到度嵌入序列，使用LSTM模块动态找到图序列的有效嵌入状态。具体公式为 $\\boldsymbol { \\mathcal { X } } = \\hat { \\boldsymbol { \\mathcal { A } } } \\boldsymbol { F }$，$S = S E ( \\mathcal { X } ) = h _ { T }$ 。在攻击设计中，使用两个不同的GSE模型分别为策略网络和Q网络训练嵌入状态，它们共享相同的度嵌入，但有单独的LSTM模块和结果状态。\\n3. **多环境训练管道（METP）**：\\n    - **攻击交互**：在每个攻击步骤，通过GSE方法得到图序列嵌入，使用策略网络选择添加和删除边的动作，根据一定条件（交互记录小于批量大小或为探索状态空间）可能选择随机动作，应用动作得到新图序列，在目标模型上进行预测并获得奖励，将经验记录到共享缓冲区。策略网络公式为 $\\mu _ { \\theta } ( S _ { k } ^ { p } ) = S i g m o d ( M L P _ { \\theta } ( S _ { k } ^ { p } ) ) * | V |$。\\n    - **Q网络和Q GSE模块训练**：从共享缓冲区采样，通过LSTM - q得到序列嵌入状态，使用MLP模型作为Q网络，计算Q值和损失，反向传播更新Q网络和Q GSE模型。Q网络公式为 $Q ( S ^ { q } , a _ { b } ) = M L P _ { q } ( [ a _ { b } , S ^ { q } ] )$，损失公式为 $l o s s _ { Q } = M S E ( Q ( S ^ { q } , a _ { b } ) , r _ { b } )$。\\n    - **策略网络和策略GSE模块训练**：从共享缓冲区获取相同样本，通过更新后的LSTM - q得到GSE状态，通过LSTM - p得到策略GSE状态和策略动作，定义策略损失并更新策略网络和策略GSE模块。策略损失公式为 ${ d ( S ^ { q \\prime } , a _ { b } ) = m a x ( 0 , Q ^ { \\prime } ( S ^ { q \\prime } , a _ { b } ) - Q _ { \\theta } ( S ^ { q \\prime } , a _ { \\theta } ) ) }$，${ l o s s _ { \\mu _ { \\theta } } ( S ^ { q \\prime } , a _ { b } ) = M S E ( a _ { b } , a _ { \\theta } ) \\cdot d ( S ^ { q \\prime } , a _ { b } ) }$。\\n\\n**案例解析**\\n论文通过跟踪SAC攻击和本文方法在攻击过程中的状态和动作，以Haggle数据集上的DyGCN模型为例，观察到SAC的添加动作收敛到 $\\{ 0 , 0 \\}$ 动作，删除动作的方差从 $\\{ 8 0 3 2 , 4 8 9 0 \\}$ 降至 $\\{ 3 9 , 8 4 \\}$ ；而GSE - METP添加动作平均为 $\\{ 9 7 , 1 2 6 \\}$ ，删除动作平均为 $\\{ 1 5 2 , 1 3 0 \\}$ ，且添加和删除动作的方差分别为 $\\{ 4 6 5 5 , 4 7 0 8 \\}$ 和 $\\{ 4 3 8 8 , 7 4 0 5 \\}$ ，表明GSE - METP能在不同状态下更灵活地调整动作节点。最终性能上，SAC仅将准确率降低到0.854，而GSE - METP降低到0.724。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括随机攻击（Random attack）、SAC攻击（SAC attack）、SAC - METP攻击（SAC - METP attack）和GSE攻击（GSE attack）。\\n\\n**性能对比**\\n*   **在预测指标（以F1分数衡量）上**：在默认设置下，本文提出的GSE和GSE - METP方法表现最佳。在Haggle数据集上，对于DyGCN模型，GSE - METP方法达到了0.8118的F1分数，优于随机攻击（0.8979）、SAC攻击（0.8149）、SAC - METP攻击（0.8094）和GSE攻击（0.8043）；对于ASTGCN模型，GSE - METP达到0.9702，优于随机攻击（0.9752）、SAC攻击（0.9825）、SAC - METP攻击（0.9750）和GSE攻击（0.9748）；对于HTGN模型，GSE - METP达到0.7417，优于随机攻击（0.8083）、SAC攻击（0.9603）、SAC - METP攻击（0.9676）和GSE攻击（0.8311）。在Facebook数据集上，对于DyGCN模型，GSE - METP为0.9653，优于随机攻击（0.9681）、SAC攻击（0.9665）、SAC - METP攻击（0.9651）和GSE攻击（0.9629）；对于ASTGCN模型，GSE - METP为0.9011，优于随机攻击（0.9298）、SAC攻击（0.9853）、SAC - METP攻击（0.9859）和GSE攻击（0.9407）；对于HTGN模型，GSE - METP为0.8494，优于随机攻击（0.8652）、SAC攻击（0.9145）、SAC - METP攻击（0.9141）和GSE攻击（0.8930）。在AS数据集上，对于DyGCN模型，GSE - METP为0.8573，显著优于随机攻击（0.8899）、SAC攻击（0.8890）、SAC - METP攻击（0.8910）和GSE攻击（0.8862）；对于ASTGCN模型，GSE - METP为0.8020，优于随机攻击（0.8276）、SAC攻击（0.8264）、SAC - METP攻击（0.8263）和GSE攻击（0.8226）；对于HTGN模型，GSE - METP为0.7944，优于随机攻击（0.8377）、SAC攻击（0.8526）、SAC - METP攻击（0.8527）和GSE攻击（0.7944）。\\n*   **在收敛性和适应性上**：从消融实验来看，随着交互限制增加，在小数据集Haggle上，SAC和SAC - METP在交互足够时有效，但在大数据集Facebook和AS上无法收敛到好结果；而GSE和GSE - METP能收敛到更好的攻击效果，证明了GSE设计的有效性。同时，SAC - METP和GSE - METP比SAC和GSE收敛更快，性能更好，体现了多环境训练管道设计的效率。\",\n    \"keywords\": \"### 关键词\\n\\n- 动态图链接预测 (Link Prediction in Dynamic Graphs, LPDG)\\n- 黑盒逃避攻击 (Black - Box Evasion Attack, N/A)\\n- 图序列嵌入 (Graph Sequential Embedding, GSE)\\n- 多环境训练管道 (Multi - Environment Training Pipeline, METP)\\n- 深度强化学习 (Deep Reinforcement Learning, DRL)\\n- 深度确定性策略梯度 (Deep Deterministic Policy Gradient, DDPG)\"\n}"
}