{
    "link": "https://arxiv.org/abs/2412.07507",
    "pdf_link": "https://arxiv.org/pdf/2412.07507",
    "title": "ConfigX: Modular Configuration for Evolutionary Algorithms via Multitask Reinforcement Learning",
    "authors": [
        "Hongshu Guo",
        "Zeyuan Ma",
        "Jiacheng Chen",
        "Yining Ma",
        "Zhiguang Cao",
        "Xinglin Zhang",
        "Yue-jiao Gong"
    ],
    "publication_date": "2024-12-10",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 4,
    "influential_citation_count": 0,
    "paper_content": "# ConfigX: Modular Configuration for Evolutionary Algorithms via Multitask Reinforcement Learning\n\nHongshu Guo1\\*, Zeyuan $\\mathbf { M } \\mathbf { a } ^ { 1 * }$ , Jiacheng Chen1, Yining Ma2, Zhiguang $\\mathbf { C a o } ^ { 3 }$ , Xinglin Zhang1, Yue-Jiao Gong 1†\n\n1South China University of Technology,   \n2Massachusetts Institute of Technology,   \n3Singapore Management University\n\n# Abstract\n\nRecent advances in Meta-learning for Black-Box Optimization (MetaBBO) have shown the potential of using neural networks to dynamically configure evolutionary algorithms (EAs), enhancing their performance and adaptability across various BBO instances. However, they are often tailored to a specific EA, which limits their generalizability and necessitates retraining or redesigns for different EAs and optimization problems. To address this limitation, we introduce ConfigX, a new paradigm of the MetaBBO framework that is capable of learning a universal configuration agent (model) for boosting diverse EAs. To achieve so, our ConfigX first leverages a novel modularization system that enables the flexible combination of various optimization sub-modules to generate diverse EAs during training. Additionally, we propose a Transformer-based neural network to meta-learn a universal configuration policy through multitask reinforcement learning across a designed joint optimization task space. Extensive experiments verify that, our ConfigX, after large-scale pretraining, achieves robust zero-shot generalization to unseen tasks and outperforms state-of-the-art baselines. Moreover, ConfigX exhibits strong lifelong learning capabilities, allowing efficient adaptation to new tasks through fine-tuning. Our proposed ConfigX represents a significant step toward an automatic, all-purpose configuration agent for EAs.\n\n# 1 Introduction\n\nOver the decades, Evolutionary Algorithms (EAs) such as Genetic Algorithm (GA) (Holland 1992), Particle Swarm Optimization (PSO) (Kennedy and Eberhart 1995) and Differential Evolution (DE) (Storn and Price 1997) have been extensively researched to tackle challenging Black-Box Optimization (BBO) problems, where neither the mathematical formulation nor additional derivative information is accessible. On par with the development of EAs, one of the most crucial research avenues is the Automatic Configuration (AC) for EAs (Anso´tegui, Sellmann, and Tierney 2009; Huang, Li, and Yao 2019). Generally speaking, AC for EAs aims at identifying the optimal configuration $c ^ { * }$ from the configuration space $\\mathcal { C }$ of an evolutionary algorithm $A$ , across a set of BBO problem instances $\\boldsymbol { \\mathcal { T } }$ :\n\n$$\nc ^ { * } = \\underset { c \\in \\mathcal { C } } { \\arg \\operatorname* { m a x } } \\ \\underset { p \\in \\mathcal { Z } } { \\mathbb { E } } \\left[ P e r f ( A , c , p ) \\right]\n$$\n\nwhere $P e r f ( )$ denotes the performance of a configuration for the algorithm under a given problem instance.\n\nTraditionally, the primary research focus in AC for EAs has centered on human-crafted AC mechanisms. These mechanisms, including algorithm/operator selection (Fialho 2010) and parameter control (Aleti and Moser 2016), have demonstrated strong performance on well-known BBO benchmarks (Hansen et al. 2010), as well as in various eye-catching real-world scenarios such as ProteinDocking (Hwang et al. 2010), AutoML (Vanschoren et al. 2014), and Prompting Optimization of Large Language Models (Chen, Dohan, and So 2024). However, a major limitation of manual AC is its heavy reliance on deep expertise. To address a specific problem, one often needs to consult experts with the necessary experience to analyze the problem and then design appropriate AC mechanisms (as depicted in the top of Figure 1). This impedes the broader application of EAs in diverse scientific or industrial applications.\n\nRecently, a novel paradigm called Meta-learning for Black-Box Optimization (MetaBBO) (Ma et al. 2023), has emerged in the learning-to-optimize community. MetaBBO aims to reduce the reliance on expert-level knowledge in designing more automated AC mechanisms. As shown in the middle of Figure 1, in MetaBBO, a neural network is metatrained as a meta-level policy to maximize the expected performance (Eq. (1)) of a low-level algorithm by dictating suitable configuration for solving each problem instance. By leveraging the data-driven features of deep models and the generalization strengths of meta-learning (Finn, Abbeel, and Levine 2017) across a distribution of optimization problems, these MetaBBO methods (Ma et al. 2024c; Li et al. 2024; Song et al. 2024) have shown superior adaptability compared to traditional human-crafted AC baselines.\n\nDespite these advancements, there remains significant potential to further reduce the expertise burden. Current MetaBBO methods often need custom neural network designs, specific learning objectives, and frequent retraining or even complete redesigns to fit different backbone EAs, overlooking the shared aspects of AC across multiple EAs. This\n\n![](images/b1011159a2aed767609d4e0f63d5c20e900e6a956178ef7af5926278cc81acc0.jpg)  \nFigure 1: Conceptual overview of different AC paradigms.\n\n$T = \\left( A _ { m } , I _ { n } \\right)$ , the Transformer generates configurations by conditioning on a sequence of state tokens corresponding to the sub-modules in $A _ { m }$ . Through large-scale multitask reinforcement learning over the sampled tasks, it yields a universal meta-policy that exhibits robust generalization to unseen algorithm structures and problem instances.\n\nWe summarize our contributions in this paper in three folds:\n\n• We introduce ConfigX, the first MetaBBO framework to learn a pre-trained universal AC agent via multitask reinforcement learning, enabling modular configuration of diverse EAs across various optimization problems. • Technically, we present Modular-BBO as a novel system for EA modularization that simplifies the management of sub-modules and facilitates the sampling of diverse algorithm structures. We then propose a Transformer-based architecture to meta-learn a universal configuration policy over our defined joint optimization task space. • Extensive benchmark results show that the configuration policy pre-trained by ConfigX not only achieves superior zero-shot performance against the state-of-the-art AC software SMAC3, but also exhibits favorable lifelong learning capability via efficient fine-tuning.\n\n# 2 Related Works\n\nleads to the core research question of this paper: Is it possible to develop a MetaBBO paradigm that can meta-learn an automatic, all-purpose configuration agent for diverse EAs? We outline the detailed research objective below:\n\n$$\nc _ { k } ^ { * } = \\underset { c \\in \\mathcal { C } _ { k } } { \\arg \\operatorname* { m a x } } \\ \\underset { p \\in \\mathcal { T } } { \\mathbb { E } } \\left[ P e r f ( A _ { k } , c , p ) \\right] , k = 1 , 2 , . . . , K\n$$\n\nwhere $K$ is an exceedingly large number, potentially infinite. This objective is far more challenging since it can be regarded as the extension of Eq. (1). Concretely, it presents two key challenges: 1) Constructing a comprehensive evolutionary algorithm space is crucial for addressing Eq. (2), from which diverse EAs can be easily sampled for metatraining the MetaBBO; 2) Ensuring the generalization capability of the learned meta-level policy across not only optimization problems but also various EAs is imperative.\n\nTo address these challenges, we introduce ConfigX, a pioneering MetaBBO framework capable of modularly configuring diverse EAs with a single model across different optimization problems (as shown at the bottom of Figure 1).\n\nSpecifically, to address the first challenge, we present a novel modularization system for EAs, termed Modular-BBO in Section 3.1. It leverages hierarchical polymorphism to efficiently encapsulate and maintain various algorithmic submodules within the EAs, such as mutation or crossover operators. By flexibly combining these sub-modules, ModularBBO can generate a vast array of distinct EA structures, hence spanning a comprehensive algorithm space $\\mathcal { A }$ . To address the second challenge, we combine the problem instance space $\\boldsymbol { \\mathcal { T } }$ and $\\mathcal { A }$ to form a joint optimization task space $\\tau : A \\times \\tau$ . We then consider meta-learning a Transformer based meta-level policy over moderate optimization tasks sampled from the joint space $\\tau$ to maximize the objective in Eq. (2), see Section 3.2 and 3.3. For each task\n\n# 2.1 Human-crafted AC\n\nHuman-crafted AC mechanisms enhance the optimization robustness of EAs through two main paradigms: Operator Selection (OS) and Parameter Control (PC). OS is geared towards selecting proper evolutionary operators (i.e., mutation in DE (Qin and Suganthan 2005)) for EAs to solve target optimization problems. To this end, such AC mechanism requires preparing a group of candidate operators with diverse searching behaviours. Besides, throughout the optimization progress, OS facilitates dynamic selection over the candidate operators, either by a roulette wheel upon the historical success rates (Lynn and Suganthan 2017) or random replacement upon the immediate performance improvement (Mallipeddi et al. 2011). PC, on the other hand, aims at configuring (hyper-) parameters for the operators in EAs, (e.g. the inertia weights in PSO (Amoshahy, Shamsi, and Sedaaghi 2016) and the scale factors in DE (Zhang and Sanderson 2009; Tanabe and Fukunaga 2013)), while embracing similar adaptive mechanisms as OS to achieve dynamic configuration. We note that OS and PC are complementary rather than conflicting. Recent outperforming EAs such as MadDE (Biswas et al. 2021), AMCDE (Ye et al. 2023) and SAHLPSO (Tao et al. 2021) integrate both to obtain maximal performance gain. However, the construction of the candidate operators pool, the parameter value range in PC, and the adaptive mechanism in both of them heavily rely on expertise. A more versatile and efficient alternative for human-crafted AC is Bayesian Optimization (BO) (Shahriari et al. 2015). By iteratively updating and sampling from a posterior distribution over the algorithm configuration space, a recent open-source BO software SMAC3 (Lindauer et al. 2022) achieves the state-of-the-art AC performance on many realistic scenarios.\n\nModule High-level Inheritance Legal algoritnm   \n- id: None Add Propertie config_space 1 structure   \n+ get_id() + exec() Initialize Add Methods get_config() 1   \n- iUd:ncNonetrollable - coCnofingt_rsoplalcae:blNeone Middle-level Inheritance 1 Crossover + set_config() Add Methods get_rule() ↓ ? ↓ Initialization Selection Mutation Crossover Low-level Inheritance Illegal algoritnm   \n- topology_rule: List .. . - topology_rule: List - topology_rule: List - topology_rule: List Specify id structure   \n+ get_rule() + get_rule() + get_rule() + get_rule() config_space Initialize √ 1 √ √ Overload exec() Mutation Uniform LHS Binomial SBX 一   \n- id: Int ..。 - id: Int . . - cido:nfIingt_space: Dict . . - icdo:nIfnitg_space: Dict ：。 . 1 Selection   \n+ exec() + exec() + exec() + exec() H Crossover\n\n# Algorithm 1: Algorithm Structure Generation.\n\nInput: All accessible modules $\\mathbb { M }$ , all Initialization modules $\\mathbb { M } _ { \\mathrm { i n i t } }$   \nOutput: A legal algorithm structure $A$ .   \n1: Create an empty structure $A = \\varnothing$ , set index $j = 0$   \n2: Randomly select an Initialization module from $\\mathbb { M } _ { \\mathrm { i n i t } }$ as $a _ { j }$   \n3: $A  A \\bigcup a _ { j }$   \n4: while notSCOMPLETED do   \n5: $j = j + 1$   \n6: while VIOLATED do   \n7: Randomly select a module from $\\mathbb { M } \\backslash \\mathbb { M } _ { \\mathrm { i n i t } }$ as $a _ { j }$   \n8: Check the violation between $a _ { j }$ and $a _ { j - 1 }$   \n9: end while   \n10: $A  A \\bigcup a _ { j }$   \n11: end while\n\n# 2.2 MetaBBO\n\nTo relieve the expertise dependency of human-crafted AC, recent MetaBBO works introduce neural network-based control policy (typically denoted as the meta-level policy $\\pi _ { \\boldsymbol { \\theta } }$ ) to automatically dictate desired configuration for EAs (Ma et al. 2024d; Yang, Wang, and Li 2024). Generally speaking, the workflow of MetaBBO follows a bi-level optimization process: 1) At the meta level, the policy $\\pi _ { \\boldsymbol { \\theta } }$ configures the low-level EA and assesses its performance, termed meta performance. The policy leverages this observed meta performance to refine its decision-making process, training itself through the maximization of accumulated meta performance, thereby advancing its meta objective. 2) At the lower level, the BBO algorithm receives a designated algorithmic configuration from the meta policy. With this configuration in hand, the low-level algorithm embarks on the task of optimizing the target objective. It observes the changes in the objective values and relays this information back to the meta optimizer, contributing to the meta performance signal. Similarly, existing MetaBBO works focus predominantly on OS and PC. Although a predefined operator group remains necessary, the selection decisions in works on OS (Sharma et al. 2019; Tan and Li 2021; Lian et al. 2024) are made by the meta policy $\\pi _ { \\boldsymbol { \\theta } }$ which relieves the expert-level knowledge requirement. A notable example is RL-DAS (Guo et al. 2024) where advanced DE algorithms are switched entirely for complementary performance. In PC scenarios, initial works parameterize $\\pi$ with simple Multi-Layer Perceptron (MLP) (Wu and Wang 2022; Tan et al. 2022) and Long Short-Term Memory (LSTM) (Sun et al. 2021), whereas the latest work GLEET (Ma et al. 2024b) employs Transformer (Vaswani et al. 2017) architecture for more versatile configuration. Besides, works jointly configure both OS and PC such as MADAC (Xue et al. 2022) also show robust performance (Eimer et al. 2021).\n\n# 3 Methodology\n\nIn this section, we elaborate on ConfigX step by step. We first explain in Section 3.1 the design of Modular-BBO and how to use it for efficient generation of diverse algorithm structures. We next provide a Markov Decision Process (MDP) definition of an optimization task and derive the corresponding multi-task learning objective in Section 3.2. At last, we introduce in Section 3.3 the details of each component in the MDP and the Transformer based architecture.\n\n# 3.1 Modular-BBO\n\nAs illustrated in the left of Figure 2, the design philosophy of Modular-BBO adheres to a Hierarchical Polymorphism in Python which ensures the ease of maintaining different sub-modules (third-level sub-classes in Figure 2, labeled in green), as well as their practical variants (bottom-level subclasses in Figure 2, labeled in red) in modern EAs. By facilitating the high-to-low level inheritances, Modular-BBO provides universal programming interfaces for the modularization of EAs, along with essential module-specific properties/methods to support diverse behaviours of various submodules. Further elaboration is provided below.\n\nHigh-level. All sub-module classes in Modular-BBO stem from an abstract base class MODULE. It declares universal properties/interfaces shared among various sub-module variants, yet leave them void. At high-level inheritance, two sub-classes UNCONTROLLABLE and CONTROLLABLE inherit from MODULE. The two sub-classes divide all possible sub-modules in modern EAs into the ones with (hyper-) parameters and those without. For CONTROLLABLE modules, we declare its (hyper-) parameters by adding a config space property. Additionally, we include the corresponding get config() and set config() methods for configuring the (hyper-) parameters. Currently, these properties and methods remain void until a specific EA sub-module is instantiated.\n\nMiddle-level. At this inheritance level, UNCONTROLLABLE and CONTROLLABLE are further divided into common sub-modules in EAs, e.g., initialization, mutation, selection and etc. To combine these sub-modules legally and generate legal algorithm structures, we introduce modulespecific topology rule as a guidance during the generating process (Algorithm 1), by invoking the added get rule() method. We present a pair of examples in the left of Figure 2 to showcase one of the possible violation during the algorithm structure generation, where CROSSOVER is not allowed after SELECTION is a common sense in EAs.\n\nLow-level. Within the low-level inheritance, we borrow from a large body of EA literature diverse practical submodule variants (i.e., lots of initialization strategy have been proposed in literature such as Sobol sampling (Sobol 1967) and LHS sampling (McKay, Beckman, and Conover 2000)) and maintaining them by inheriting from the sub-module classes in middle-level inheritance. When inheriting from the parent class, a concrete sub-module variant has to instantiate void modules id and config space, which detail its unique identifier in Modular-BBO and controllable parameters respectively. It also have to overload exec() method by which it operates the solution population. The unique module id of a sub-module variant is a 16-bit binary code of which: 1) the first bit is 0 or 1 to denote if this variant is UNCONTROLLABLE or CONTROLLABLE. 2) the 2-nd to 7-th bits denote the sub-module category (third-level sub-classes in Figure 2, labeled in green) to which the variant belongs. 3) the last 9 bits denotes its id within this sub-module category.\n\nFor now, Modular-BBO has included 11 common sub-module categories in EAs: INITIALIZATION (Kazimipour, Li, and Qin 2014), MUTATION (Das, Mullick, and Suganthan 2016), CROSSOVER (Spears 1995), PSO UPDATE (Shami et al. 2022), BOUNDARY CONTROL (Kadavy et al. 2023), SELECTION (Shukla, Pandey, and Mehrotra 2015), MULTI STRATEGY (Gong et al. 2011), NICHING (Ma et al. 2019), INFORMA\n\nTION SHARING (Toulouse, Crainic, and Gendreau 1996), RESTART STRATEGY (Jansen 2002), POPULATION REDUCTION (Pool and Nielsen 2007). We construct a collection of over 100 variants for these sub-module categories from a large body of literature and denote this collection as module space M. Theoretically, by using the algorithm generation process described in Algorithm 1, Modular-BBO spans a massive algorithm structure space $\\mathcal { A }$ containing millions of algorithm structures. Due to the limitation of space, we provide the detail of each sub-module variant in M in Appendix A, Table 1, including the id, name, type, configuration space, topology rule and functional description. We also provide a detailed explanation for Algorithm 1 in Appendix B.\n\n# 3.2 Multi-task Learning in ConfigX\n\nOptimization Task Space We first define an optimization task space $\\tau$ as a synergy of an algorithm space $\\mathcal { A }$ and an optimization problem set $\\boldsymbol { \\mathcal { T } }$ . Then an optimization task $T \\in { \\mathcal { T } }$ can be defined as $T : \\{ A \\in { \\mathcal { A } } , p \\in { \\mathcal { T } } \\}$ . In this paper, we adopt the algorithm space spanned by ModularBBO as $\\mathcal { A }$ , the problem instances from well-known CoCoBBOB (Hansen et al. 2010), Protein-docking (Hwang et al. 2010) and HPO-B benchmark (Arango et al. 2021) as $\\boldsymbol { \\mathcal { T } }$ .\n\nAC as an MDP For an optimization task $T : \\{ A , p \\}$ , we facilitate a Transformer based policy $\\pi _ { \\boldsymbol { \\theta } }$ (detailed in Section 3.3) to dynamically dictate desired configuration for $A$ to solve $p$ . This configuration process can be formulated as an Markov Decision Process (MDP) $\\begin{array} { r } { \\mathcal { M } : = } \\end{array}$ $( S , C , \\Gamma , R , H , \\gamma )$ , where $\\boldsymbol { S }$ denotes the state space that reflect optimization status, $C$ denotes the action space which is exactly the configuration space of algorithm $A$ , $\\Gamma ( s _ { t + 1 } | s _ { t } , c _ { t } )$ denotes the optimization transition dynamics, $R ( s _ { t } , c _ { t } )$ measures the single step optimization improvement obtained by using configuration $c _ { t }$ for optimizing $p$ . $H$ and $\\gamma$ are the number of optimization iterations and discount factor respectively. At each optimization step $t$ , the policy $\\pi _ { \\boldsymbol { \\theta } }$ receives a state $s _ { t }$ and then outputs a configuration $c _ { t } ~ = ~ \\pi _ { \\theta } ( s _ { t } )$ for $A$ . Using $c _ { t }$ , algorithm $A$ optimizes the optimization problem $p$ for a single step. The goal is to find an optimal policy $\\pi _ { \\theta ^ { * } }$ which maximizes the accumulated optimization improvement during the optimization process: $\\begin{array} { r } { \\bar { G } = \\sum _ { t = 1 } ^ { H } \\gamma ^ { t - 1 } R ( s _ { t } , c _ { t } ) } \\end{array}$ . Recall that our ConfigX aims at addressing a more challenging objective in Eq. (2), where the goal is to maximize the accumulated optimization improvement $G$ of all tasks $T \\in \\mathcal { T }$ . We use $s _ { t } ^ { i }$ and $c _ { t } ^ { i }$ to denote the input state and the outputted configuration of the policy $\\pi _ { \\boldsymbol { \\theta } }$ for the $i$ -th task in $\\tau$ . Then the objective in Eq. (2) can be rewritten as a multi-task RL problem:\n\n$$\n\\mathbb { J } ( \\theta ) = \\frac { 1 } { K \\cdot N } \\sum _ { i = 1 } ^ { K \\cdot N } \\sum _ { t = 1 } ^ { H } \\gamma ^ { t - 1 } R ( s _ { t } ^ { i } , c _ { t } ^ { i } )\n$$\n\nwhere we sample $K \\cdot N$ tasks from $\\tau$ to train $\\pi _ { \\boldsymbol { \\theta } }$ since the number of tasks in $\\tau$ is massive. These tasks is sampled first by calling Algorithm 1 $K$ times to obtain $K$ algorithm structures, and then combine these algorithm with the $N$ problem instances in $\\boldsymbol { \\mathcal { T } }$ . In this paper we use Proximal Policy Opti\n\n+ 1 司 1昌1昌 Per Module Configuration Module ID LFaenadtsucraepse Positional Encoding x3 1\n\nmization (PPO) (Schulman et al. 2017), a popular policy gradient (Williams 1992) method for optimizing this objective in a joint policy optimization (Gupta et al. 2022) fashion. We include the pseudocode of the RL training in Appendix D.\n\n# 3.3 ConfigX\n\nProgress in MetaBBO has made it possible to meta-learn neural network-based control policies for configuring the backbone EAs to solve optimization problems. However, existing MetaBBO methods are not suitable for the massive algorithm structure space $\\mathcal { A }$ spanned by the module space M in our proposed Modular-BBO, since learning a separate policy for each algorithm structure is impractical. However, the modular nature of EAs implies that while each structure is unique, they are still constructed from the same module space and potentially shares sub-modules and workflows with other algorithm structures. We now describe how ConfigX exploits this insight to address the challenge of learning a universal controller for different algorithm structures.\n\nState Design In ConfigX, we encode not only the algorithm structure information but also the optimization status information into the state representation to ensure the generalization across optimization tasks. Concretely, as illustrated in the left of Figure 3, for $i$ -th tasks $T _ { i } ~ : ~ \\{ A _ { i } , p _ { i } \\}$ in the sampled $K \\cdot N$ tasks, we encode a information pair for each sub-module in $A _ { i }$ , e.g., $s _ { i } : \\{ s _ { i , j } ^ { \\mathrm { i d } } , s _ { i } ^ { \\mathrm { o p t } } \\} _ { j = 1 } ^ { L _ { i } }$ , where $s _ { i , j } ^ { \\mathrm { i d } } \\in \\{ 0 , 1 \\} ^ { 1 6 }$ denotes the unique module id for $j$ -th submodule in $A _ { i }$ , $s _ { i } ^ { \\mathrm { o p t } } \\ \\in \\ \\mathbb { R } ^ { 9 }$ denotes the algorithm performance information which we borrow the idea from recent MetaBBO methods (Guo et al. 2024; Ma et al. 2024a), $L _ { i }$ denotes the number of sub-modules in $A _ { i }$ . We provide details of these information pairs in Appendix C.\n\nState Encode We apply an MLP fusion layer to preprocess the state representation $s _ { i }$ . This fusion process ensures the information within the information pair $\\{ s _ { i , j } ^ { \\mathrm { i d } } , s _ { i } ^ { \\mathrm { o p t } } \\}$ join each other smoothly (as illustrated in left part of Figure 3).\n\n$$\n\\hat { e } _ { i , j } = \\mathrm { h s t a c k } ( \\phi ( s _ { i , j } ^ { \\mathrm { i d } } ; \\mathbf { W } _ { e } ^ { \\mathrm { i d } } ) ; \\phi ( s _ { i } ^ { \\mathrm { o p t } } ; \\mathbf { W } _ { e } ^ { \\mathrm { o p t } } ) )\n$$\n\n$$\ne _ { i , j } = \\phi ( \\hat { e } _ { i , j } ; \\mathbf { W } _ { e } ) , \\quad j = 1 , . . . , L _ { i }\n$$\n\nWhere $\\phi ( \\cdot ; \\mathbf { W } _ { e } ^ { \\mathrm { i d } } )$ , $\\phi ( \\cdot ; { \\bf W } _ { e } ^ { \\mathrm { o p t } } )$ and $\\phi ( \\cdot ; { \\mathbf W } _ { e } )$ denotes MLP layers with the shape of $1 6 \\times 1 6$ , $9 \\times 1 6$ and $3 2 \\times 6 4$ respectively, $\\boldsymbol { e } _ { i , j }$ denotes the fused information for each submodule. Then we add $\\mathit { S i n }$ Positional Encoding (Vaswani et al. 2017) $\\mathbf { W } _ { \\mathrm { p o s } }$ to each sub-module, which represents the relative position information among all sub-modules in the algorithm structure.\n\n$$\n\\mathbf { h } _ { i } ^ { ( 0 ) } = \\mathrm { v s t a c k } ( e _ { i , j } ; \\cdot \\cdot \\cdot ; e _ { i , L _ { i } } ) + \\mathbf { W } _ { \\mathrm { p o s } }\n$$\n\nwhere $\\mathbf { h } _ { i } ^ { ( 0 ) } \\in \\mathbb { R } ^ { L _ { \\operatorname* { m a x } } \\times 6 4 }$ denotes the module embedding for each sub-module. We note that since the number of submodules $( L _ { i } )$ may vary between different algorithm structures, we zero pad ${ \\bf h } _ { i } ^ { ( 0 ) }$ to a pre-defined maximum length $L _ { \\mathrm { m a x } }$ to ensure input size invariant among tasks.\n\nModule Aware Attention From the module embeddings ${ \\bf h } _ { i } ^ { ( 0 ) }$ described above, we obtains the output features for all sub-modules as:\n\n$$\n\\begin{array} { r l } & { \\hat { \\mathbf { h } } _ { i } ^ { ( l ) } = \\mathrm { L N } ( \\mathbf { M S A } ( \\mathbf { h } _ { i } ^ { ( l - 1 ) } ) + \\mathbf { h } _ { i } ^ { ( l - 1 ) } ) , l = 1 , 2 , 3 } \\\\ & { \\mathbf { h } _ { i } ^ { ( l ) } = \\mathrm { L N } ( \\phi ( \\hat { \\mathbf { h } } _ { i } ; \\mathbf { W } _ { F } ^ { ( l ) } ) ) + \\hat { \\mathbf { h } } _ { i } ^ { ( l ) } ) , l = 1 , 2 , 3 } \\end{array}\n$$\n\nwhere LN is Layernorm (Ba, Kiros, and Hinton 2016), MSA is Multi-head Self-Attention (Vaswani et al. 2017) and $\\phi ( \\cdot ; \\mathbf { W } _ { F } ^ { ( l ) } )$ are MLP layers with the shape of $6 4 \\times 6 4$ . In this paper we use MSA blocks to process the module embeddings (as illustrated in the middle of Figure 3).\n\nConfiguration Decoder In ConfigX, the policy $\\pi _ { \\boldsymbol { \\theta } } ( c _ { i } | \\boldsymbol { s } _ { i } )$ models the conditional distribution of $A _ { i }$ ’s configuration $c _ { i }$ given the state $s _ { i }$ . As illustrated in the right of Figure 3, for each sub-module $a _ { j }$ in an algorithm $A _ { i } = \\{ a _ { 1 } , a _ { 2 } , \\ldots \\}$ , we output distribution parameters $\\mu$ and $\\Sigma$ as:\n\n$$\n\\begin{array} { c } { { \\mu _ { j } = \\phi ( h _ { i , j } ^ { ( 3 ) } ; { \\bf W } _ { \\mu } ) , ~ \\Sigma _ { j } = \\mathrm { D i a g } \\phi ( h _ { i , j } ^ { ( 3 ) } ; { \\bf W } _ { \\Sigma } ) } } \\\\ { { c _ { i , j } \\sim \\mathcal N ( \\mu _ { j } ; \\Sigma _ { j } ) } } \\end{array}\n$$\n\nwhere $\\phi ( \\cdot ; { \\mathbf W } _ { \\mu } )$ and $\\phi ( \\cdot ; { \\bf W } _ { \\Sigma } )$ are two MLP layers with the same shape of $6 4 \\times C _ { \\mathrm { m a x } }$ , $c _ { i , j } \\in \\mathbb { R } ^ { C _ { \\operatorname* { m a x } } }$ denotes the configurations for sub-module $a _ { j }$ in algorithm structure $A _ { i }$ . Since the size of the configuration spaces may vary between different sub-modules, we pre-defined a maximum configuration space size $C _ { \\mathrm { m a x } }$ to cover the sizes of all sub-modules. If the size of a sub-module is less than $C _ { \\mathrm { m a x } }$ , we use the first few configurations in $\\boldsymbol { c } _ { i , j }$ and ignore the rest.\n\nFor the critic, we calculate the value of a sub-module as $V ( s _ { i , j } ) = \\phi ( \\mathbf { h } _ { i , j } ^ { ( 3 ) } ; \\mathbf { W } _ { c } )$ using a MLP with the shape of ${ \\bf W } _ { c } \\in \\{ \\}$ $\\mathbb { R } ^ { 6 4 \\times 1 6 \\times 1 }$ . The value of the algorithm structure is the averaged value per sub-module $\\begin{array} { r } { V ( \\bar { s } _ { i } ) = \\frac { 1 } { L _ { \\mathrm { m a x } } } \\sum _ { j = 1 } ^ { L _ { \\mathrm { m a x } } } V ( s _ { i , j } ) } \\end{array}$ .\n\n![](images/7d04ed0af9d3b89a8b1b0caf141562d26dc71ab25aaf498e38fc31ff59d763b8.jpg)  \nFigure 4: Optimization curves of the pre-trained ConfigX model and the baselines, over three different zero-shot scenarios\n\nReward Function The objective value scales across different problem instances can vary. To ensure the accumulated performance improvement across tasks approximately share the same numerical level, we propose a task agnostic reward function. At optimization step $t$ , the reward function on any problem instance $p \\in \\mathcal { I }$ is formulated as:\n\n$$\n\\displaystyle r _ { t } = \\delta \\times \\frac { f _ { p , t - 1 } ^ { * } - f _ { p , t } ^ { * } } { f _ { p , 0 } ^ { * } - f _ { p } ^ { * } }\n$$\n\nwhere $f _ { p , t } ^ { * }$ is the found best objective value of problem instance $p$ at step $t$ , $f _ { p } ^ { * }$ is the global optimal objective value of $p$ and $\\delta = 1 0$ is a scale factor. In this way, we make the scales of the accumulated improvement in all tasks similar and hence stabilize the training.\n\n# 4 Experiment\n\nIn this section, we discuss the following research questions: RQ1: Can pre-trained ConfigX model zero-shots to unseen tasks with unseen algorithm structures and/or unseen problem instances? RQ2: If the zero-shot performance is not as expected, is it possible to fine-tune ConfigX to address novel algorithm structures in future? RQ3: How do the concrete designs in ConfigX contribute to the learning effectiveness? Below, we first introduce the experimental settings and then address $\\scriptstyle \\mathrm { R Q 1 } \\sim \\mathrm { R Q 3 }$ respectively.\n\n# 4.1 Experimental Setup\n\nTraining setup. We have prepared several task sets from different sub-task-spaces of the overall task space $\\tau$ (defined at Section 3.2) to aid for the following experimental validation. Concretely, denote $\\mathcal { T } _ { \\mathrm { s y n } }$ as the problems in CoCo-BBOB suite, $\\mathcal { T } _ { \\mathrm { r e a l } }$ as all realistic problems in Proteindocking benchmark and HPO-B benchmark, $\\mathcal { A } _ { \\mathrm { D E } }$ as the algorithm structure space only including DE variants, $\\mathcal { A } _ { \\mathrm { P S O } , \\mathrm { G A } }$ as the algorithm structure space including PSO and GA variants, we have prepared 256 optimization tasks as training task set $T _ { \\mathrm { t r a i n } } \\subset \\mathcal { A } _ { \\mathrm { D E } } \\times \\mathcal { Z } _ { \\mathrm { s y n } }$ , another 512 optimization tasks as in-distribution testing task set $T _ { \\mathrm { t e s t , i n } } \\subset \\mathcal { A } _ { \\mathrm { D E } } \\times \\mathcal { T } _ { \\mathrm { s y n } }$ . For out-of-distribution tasks, we have prepared two task sets: $T _ { \\mathrm { t e s t , o u t } } ^ { ( 1 ) } \\subset \\mathcal { A } _ { \\mathrm { D E } } \\times \\mathcal { T } _ { \\mathrm { r e a l } }$ and $T _ { \\mathrm { t e s t , o u t } } ^ { ( 2 ) } \\subset \\mathcal { A } _ { \\mathrm { P S O , G A } } \\times \\mathcal { T } _ { \\mathrm { s y n } }$ , each with 512 task instances. During the training, for a batch of $b a t c h _ { - } s i z e = 3 2$ tasks, PPO (Schulman et al. 2017) method is used to update the policy net and critic $\\kappa = 3$ times for every 10 rollout optimization steps. All of the tasks are allowed to be optimized for $H = 5 0 0$ optimization steps. The training lasts for 50 epochs with a fixed learning rate 0.001. All experiments are run on an Intel(R) Xeon(R) 6348 CPU with 504G RAM. Refer to Appendix E.1 for more details.\n\nBaselines and Performance Metric. In the following comparisons, we consider three baselines: SMAC3, which is the state-of-the-art AC software based on Bayesian Optimization and aggressive racing mechanism; Original, which denotes using the suggested configurations in sub-modules’ original paper (see Appendix A for one-to-one correspondence); Random, which randomly sample the configurations for the algorithm from the algorithm’s configuration space. For the pre-trained model in ConfigX and the above baselines, we calculate the performance of them on tested task set by applying them to configure each tested task for 51 independent runs and then aggregate a normalized accumulated optimization improvement across all tasks and all runs, we provide more detailed calculation steps in Appendix E.2.\n\n# 4.2 Zero-shot Performance (RQ1)\n\nWe validate the zero-shot performance of ConfigX by first pre-training a model on $T _ { \\mathrm { t r a i n } }$ . Then the pre-trained model is directly used to facilitate AC process for tasks in tested set, without any fine-tuning. Concretely, we aims at validating the zero-shot generalization performance in three different scenarios: 1) $T _ { \\mathrm { t e s t , i n } }$ , where the optimization tasks come from the same task space on which ConfigX is pre-trained. 2) Tt(es1t),o , where the optimization tasks locate beyond the optimization problem scope of the training task space. 3) $\\bar { T _ { \\mathrm { t e s t , o u t } } ^ { ( 2 ) } }$ , where the optimization tasks locate beyond the algothe optimization curves of our pre-trained model and the baselines in Figure 4, where the $\\mathbf { \\boldsymbol { x } }$ -axis denotes the optimization horizon and y-axis denotes the performance metric we defined previously. The results in Figure 4 reveal several key observations: 1) In all zero-shot scenarios, ConfigX presents significantly superior performance to the Random baseline, which randomly configures the algorithms in the tested tasks. This underscores the effectiveness of the multitask reinforcement learning in ConfigX. 2) The results on $T _ { \\mathrm { t e s t , i n } }$ demonstrate that pre-training ConfigX on some task samples of the given task space is enough to ensure the generalization to the other tasks within this space, surpassing the state-of-the-art AC baseline SMAC3. 3) The results on $T _ { \\mathrm { t e s t , o u t } } ^ { ( 1 ) }$ show that ConfigX is capable of adapting itself to totally unseen optimization problem scope. This observation attributes to our state representation design, where the optimization status borrowed from recent MetaBBO works are claimed to be generic across different problem scopes. 4) Though promising, we find that the zero-shot performance aosf tChoensfiugbX-mono $T _ { \\mathrm { t e s t , o u t } } ^ { ( 2 ) }$ nisd nstortuacstuerxepseicntedG.AIt/PisSnOotarseu spirgisninfigcantly different from those in DE, which hinders ConfigX from applying its DE configuration experience on PSO/GA tasks. We explore whether this generalization gap could be addressed through further fine-tuning in the next section.\n\n![](images/7bd3e7cbf6788639fb4e9fbcb45a8670e668cbad804f590059c109c63d7f29a5.jpg)  \nFigure 5: The learning curves of fine-tuning and re-training ConfigX on novel optimization problems or algorithm structures. The fine-tuning saves 3x and $2 \\mathrm { x }$ learning steps than the re-training on $T _ { \\mathrm { t e s t , o u t } } ^ { ( 1 ) }$ and $T _ { \\mathrm { t e s t , o u t } } ^ { ( 2 ) }$ respectively.\n\n# 4.3 Lifelong Learning in ConfigX (RQ2)\n\nThe booming algorithm designs in EAs, together with the increasingly diverse optimization problems pose nonnegligible challenges to universal algorithm configuration methods such as our ConfigX. On the one hand, although our pre-trained model shows uncommon AC performance when encountered with novel optimization problems (middle of Figure 4), further performance boost is still needed especially in industrial scenarios. On the other hand, as shown in the left of Figure 4, the pre-trained model can not cover those algorithm sub-modules which have not been included within its training algorithm structure space. Both situations above underline the importance of lifelong learning in ConfigX. We hence investigate the fine-tuning efficiency of the pretrained model in this section. Concretely, we compare the learning curves of 1) fine-tuning the pre-trained model, and 2) re-training a new model from scratch in Figure 5, where the x-axis denotes the learning epochs and the y-axis denotes the aforementioned performance metric over the tested task set. The results reveal that ConfigX supports efficient finetuning for adapting out-of-distribution tasks, which in turn provides operable guidance for lifelong learning in ConfigX: (a) One can configure an algorithm included in the algorithm space of Modular-BBO, yet on different problem scope, by directly using the pre-trained model. (b) One can also integrate novel algorithm designs into Modular-BBO and facilitate efficient fine-tuning to enhance the performance of the pre-trained model on these algorithm structures.\n\n<html><body><table><tr><td></td><td>Ttest,in</td><td>To</td><td>Trstou</td></tr><tr><td>ConfigX</td><td>9.81E-01 ±7.33E-03</td><td>9.86E-01 ±2.64E-03</td><td>9.22E-01 ±6.94E-03</td></tr><tr><td>ConfigX-MLP</td><td>9.70E-01 ±8.13E-03</td><td>9.80E-01 ±2.54E-03</td><td>9.16E-01 ±6.57E-03</td></tr><tr><td>ConfigX-LPE</td><td>9.82E-01 ±7.62E-03</td><td>9.84E-01 ±2.58E-03</td><td>9.20E-01 ±6.89E-03</td></tr><tr><td>ConfigX-NPE</td><td>9.74E-01 ±7.75E-03</td><td>9.81E-01 ±2.67E-03</td><td>9.19E-01 ±6.73E-03</td></tr><tr><td>MLP-NPE</td><td>9.51E-01 ±9.27E-03</td><td>9.73E-01 ±2.71E-03</td><td>9.06E-01 ±7.29E-03</td></tr></table></body></html>\n\nTable 1: Performance of different ablated baselines.\n\n# 4.4 Ablation Study (RQ3)\n\nIn Section 3.3, we proposed a Transformer based architecture to encode and process the state information of all sub-modules within an algorithm structure. In particular, we added Sin positional embeddings (PE) to each sub-module token as additional topology structure information for learning. We further apply Multi-head Self-Attention (MSA) to enhance the module-aware information sharing. In this section we investigate on what extent these designs influence ConfigX’s learning effectiveness. Concretely, for the positional embeddings, we introduce two ablations 1) ConfigXNPE: remove the Sin PE from ConfigX. 2) ConfigX-LPE, replace the Sin PE by Learnable PE (Gehring et al. 2017). For the MSA, we introduce one ablation ConfigX-MLP: cancel the information sharing between the sub-modules by replacing the MSA blocks by an MLP layer. We present the final performance of these baselines and ConfigX on the tested task sets in Table 1. The results underscores the importance of these special designs: (a) Without the MSA block, ConfigX struggles in learning the configuration policy in an informative way. (b) Without the positional embdeddings, the configuration policy in ConfigX becomes agnostic to the structure information of the controlled algorithm. (c) Learnable PE shows similar performance with Sin PE, while introducing additional parameters for ConfigX to learn.\n\n# 5 Conclusion\n\nIn this paper, we propose ConfigX as a pioneer research exploring the possibility of learning a universal MetaBBO agent for automatically configuring diverse EAs across optimization problems. To this end, we first introduce a novel EA modularization system Modular-BBO that is capable of maintaining various sub-modules in EAs and spanning a massive algorithm structure space. We then formulate the universal AC over this algorithm space as an MTRL problem and hence propose meta-learning a Transformer based configuration policy to maximize the overall optimization performance across task samples. Extensive experiments demonstrate that a pre-trained ConfigX model could achieve superior AC performance to the state-of-the-art manual AC method SMAC3. Furthermore, we verify that ConfigX holds promising lifelong learning ability when being fine-tuned to adapt out-of-scope algorithm structures and optimization problems. We hope this work could serve as a pivotal step towards automatic and all-purpose AC base model.\n\n# Acknowledgments\n\nThis work was supported in part by the National Natural Science Foundation of China (Grant No. 62276100), in part by the Guangdong Natural Science Funds for Distinguished Young Scholars (Grant No. 2022B1515020049), in part by the Guangdong Provincial Natural Science Foundation for Outstanding Youth Team Project (Grant No. 2024B1515040010), in part by the National Research Foundation, Singapore, under its AI Singapore Programme (AISG Award No. AISG3-RP-2022-031), and in part by the TCL Young Scholars Program.",
    "institutions": [
        "South China University of Technology",
        "Massachusetts Institute of Technology",
        "Singapore Management University"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n传统的进化算法自动配置（AC）依赖人工设计，需深厚专业知识，限制了其在不同场景的应用。现有的元学习黑盒优化（MetaBBO）方法通常针对特定的进化算法，缺乏通用性，面对不同进化算法和优化问题时需重新训练或设计。开发能为多种进化算法进行通用配置的MetaBBO范式，可降低进化算法配置对专业知识的依赖，提高其在不同科学和工业应用中的适用性。\\n\\n**方法概述**\\n论文提出ConfigX框架，利用Modular - BBO模块化系统管理进化算法子模块、生成多样算法结构，将通用AC问题转化为多任务强化学习（MTRL）问题，通过多任务强化学习在联合优化任务空间学习基于Transformer的通用配置策略。\\n\\n**主要贡献与效果**\\n- 提出ConfigX框架，可通过多任务强化学习学习预训练的通用AC代理，实现对不同优化问题下多种进化算法的模块化配置。预训练的ConfigX模型在零样本场景下的归一化累积优化改进方面优于最先进的手动AC方法SMAC3。如在$T_{test,in}$场景中，ConfigX达到$9.81E - 01 ± 7.33E - 03$，优于SMAC3。\\n- 提出Modular - BBO模块化系统，能高效封装和维护进化算法中的各种子模块，跨越包含数百万种算法结构的巨大算法结构空间。\\n- 验证ConfigX具有良好的终身学习能力，在对分布外任务进行微调时，能高效适应新的算法结构和优化问题。在$T_{test,out}^{(1)}$和$T_{test,out}^{(2)}$任务集上，微调比从头重新训练分别节省3倍和2倍的学习步骤。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nConfigX通过模块化管理进化算法子模块，生成多样的进化算法结构，利用多任务强化学习在联合优化任务空间学习通用配置策略，使策略适应不同进化算法和优化问题，实现对多种进化算法的自动配置，提高其在不同场景下的性能和适应性。该方法有效是因为利用了进化算法子模块的共享性和可组合性，以及多任务学习的泛化能力。\\n\\n**创新点**\\n先前的工作要么依赖人工设计，需大量专业知识，要么针对特定进化算法，缺乏通用性。ConfigX的创新点在于：引入新颖的模块化系统Modular - BBO，能灵活组合各种优化子模块，跨越全面的算法空间；将通用AC问题建模为多任务强化学习问题，通过Transformer学习通用配置策略，提高策略泛化能力，可在不同进化算法和优化问题上进行零样本泛化和终身学习；提出任务无关的奖励函数，稳定不同问题实例上的训练。\\n\\n**具体实现步骤**\\n1. **Modular - BBO模块化系统设计**：\\n    - 高级继承：所有子模块类源于抽象基类MODULE，分为UNCONTROLLABLE和CONTROLLABLE两类，为可控制模块声明配置空间属性和相应方法。\\n    - 中级继承：将UNCONTROLLABLE和CONTROLLABLE进一步分为初始化、变异、选择等常见子模块，引入模块特定的拓扑规则指导算法结构生成。\\n    - 低级继承：从文献中引入各种实用子模块变体，继承中级继承的子模块类，实例化模块id和配置空间，重载exec()方法。\\n2. **定义优化任务空间和多任务学习目标**：定义优化任务空间$\\tau$为算法空间$\\mathcal{A}$和优化问题集$\\boldsymbol{\\mathcal{T}}$的组合。将配置过程建模为马尔可夫决策过程（MDP）$\\mathcal{M} := (S, C, \\Gamma, R, H, \\gamma)$，目标是找到最优策略$\\pi_{\\theta^*}$，最大化所有任务的累积优化改进$\\bar{G} = \\sum_{t = 1}^{H} \\gamma^{t - 1} R(s_{t}, c_{t})$。通过采样$K \\cdot N$个任务，使用近端策略优化（PPO）方法优化目标函数$\\mathbb{J}(\\theta) = \\frac{1}{K \\cdot N} \\sum_{i = 1}^{K \\cdot N} \\sum_{t = 1}^{H} \\gamma^{t - 1} R(s_{t}^{i}, c_{t}^{i})$。\\n3. **ConfigX架构设计**：\\n    - **状态设计**：将算法结构信息和优化状态信息编码到状态表示中，为每个子模块编码信息对，如$s_{i} : \\{ s_{i,j}^{\\mathrm{id}}, s_{i}^{\\mathrm{opt}} \\}_{j = 1}^{L_{i}}$。\\n    - **状态编码**：使用MLP融合层预处理状态表示，添加Sin位置编码表示子模块的相对位置信息，零填充确保输入大小不变。公式为$\\hat{e}_{i,j} = \\mathrm{hstack}(\\phi(s_{i,j}^{\\mathrm{id}}; \\mathbf{W}_{e}^{\\mathrm{id}}); \\phi(s_{i}^{\\mathrm{opt}}; \\mathbf{W}_{e}^{\\mathrm{opt}}))$，$e_{i,j} = \\phi(\\hat{e}_{i,j}; \\mathbf{W}_{e})$，$\\mathbf{h}_{i}^{(0)} = \\mathrm{vstack}(e_{i,j}; \\cdots; e_{i,L_{i}}) + \\mathbf{W}_{\\mathrm{pos}}$。\\n    - **模块感知注意力**：使用多头自注意力（MSA）块处理模块嵌入，得到子模块的输出特征。公式为$\\hat{\\mathbf{h}}_{i}^{(l)} = \\mathrm{LN}(\\mathbf{MSA}(\\mathbf{h}_{i}^{(l - 1)}) + \\mathbf{h}_{i}^{(l - 1)})$，$\\mathbf{h}_{i}^{(l)} = \\mathrm{LN}(\\phi(\\hat{\\mathbf{h}}_{i}; \\mathbf{W}_{F}^{(l)})) + \\hat{\\mathbf{h}}_{i}^{(l)}$，$l = 1, 2, 3$。\\n    - **配置解码器**：为每个子模块输出分布参数$\\mu$和$\\Sigma$，从正态分布中采样得到子模块的配置。公式为$\\mu_{j} = \\phi(h_{i,j}^{(3)}; \\mathbf{W}_{\\mu})$，$\\Sigma_{j} = \\mathrm{Diag}\\phi(h_{i,j}^{(3)}; \\mathbf{W}_{\\Sigma})$，$c_{i,j} \\sim \\mathcal{N}(\\mu_{j}; \\Sigma_{j})$。同时计算子模块的价值$V(s_{i,j}) = \\phi(\\mathbf{h}_{i,j}^{(3)}; \\mathbf{W}_{c})$和算法结构的平均价值$V(\\bar{s}_{i}) = \\frac{1}{L_{\\mathrm{max}}} \\sum_{j = 1}^{L_{\\mathrm{max}}} V(s_{i,j})$。\\n    - **奖励函数**：提出任务无关的奖励函数$r_{t} = \\delta \\times \\frac{f_{p, t - 1}^{*} - f_{p, t}^{*}}{f_{p, 0}^{*} - f_{p}^{*}}$，其中$f_{p,t}^{*}$是问题实例$p$在步骤$t$的最佳目标值，$f_{p}^{*}$是$p$的全局最优目标值，$\\delta = 10$是比例因子，使不同任务的累积改进尺度相似，稳定训练。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- SMAC3：基于贝叶斯优化和激进竞赛机制的最先进AC软件。\\n- Original：使用子模块原始论文中建议的配置。\\n- Random：从算法的配置空间中随机采样配置。\\n- ConfigX - MLP：取消子模块之间的信息共享，用MLP层代替MSA块。\\n- ConfigX - LPE：用可学习的位置编码（Learnable PE）代替Sin PE。\\n- ConfigX - NPE：移除ConfigX中的Sin PE。\\n- MLP - NPE：既取消MSA块，又移除位置编码。\\n\\n**性能对比**\\n*   **在归一化累积优化改进指标上**：在$T_{test,in}$场景中，ConfigX达到$9.81E - 01 ± 7.33E - 03$，显著优于Original、Random基线，也超过了最先进的AC基线SMAC3；ConfigX - MLP达到$9.70E - 01 ± 8.13E - 03$，ConfigX - LPE达到$9.82E - 01 ± 7.62E - 03$，ConfigX - NPE达到$9.74E - 01 ± 7.75E - 03$，MLP - NPE达到$9.51E - 01 ± 9.27E - 03$，均低于ConfigX的性能。在$T_{test,out}^{(1)}$场景中，ConfigX达到$9.86E - 01 ± 2.64E - 03$，ConfigX - MLP达到$9.80E - 01 ± 2.54E - 03$，ConfigX - LPE达到$9.84E - 01 ± 2.58E - 03$，ConfigX - NPE达到$9.81E - 01 ± 2.67E - 03$，MLP - NPE达到$9.73E - 01 ± 2.71E - 03$，ConfigX表现更优。在$T_{test,out}^{(2)}$场景中，ConfigX达到$9.22E - 01 ± 6.94E - 03$，ConfigX - MLP达到$9.16E - 01 ± 6.57E - 03$，ConfigX - LPE达到$9.20E - 01 ± 6.89E - 03$，ConfigX - NPE达到$9.19E - 01 ± 6.73E - 03$，MLP - NPE达到$9.06E - 01 ± 7.29E - 03$，ConfigX优于其他基线。总体而言，ConfigX在各任务集上的性能均优于其他基线模型。\",\n    \"keywords\": \"### 关键词\\n\\n- 元学习黑盒优化 (Meta - learning for Black - Box Optimization, MetaBBO)\\n- 进化算法自动配置 (Automatic Configuration for Evolutionary Algorithms, AC)\\n- ConfigX框架 (ConfigX Framework, N/A)\\n- 多任务强化学习 (Multitask Reinforcement Learning, MTRL)\\n- Modular - BBO系统 (Modular - BBO System, N/A)\"\n}"
}