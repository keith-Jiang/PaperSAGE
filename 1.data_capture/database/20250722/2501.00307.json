{
    "source": "Semantic Scholar",
    "arxiv_id": "2501.00307",
    "link": "https://arxiv.org/abs/2501.00307",
    "pdf_link": "https://arxiv.org/pdf/2501.00307.pdf",
    "title": "Fast and Interpretable Mixed-Integer Linear Program Solving by Learning Model Reduction",
    "authors": [
        "Yixuan Li",
        "Can Chen",
        "Jiajun Li",
        "Jiahui Duan",
        "Xiongwei Han",
        "Tao Zhong",
        "Vincent Chau",
        "Weiwei Wu",
        "Wanyuan Wang"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-31",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "School of Computer Science and Engineering, Southeast University",
        "Noahâ€™s Ark Lab, Huawei Technologies"
    ],
    "paper_content": "# Fast and Interpretable Mixed-Integer Linear Program Solving by Learning Model Reduction\n\nYixuan $\\mathbf { L i } ^ { 1 }$ , Can Chen1, Jiajun $\\mathbf { L i } ^ { 1 }$ , Jiahui Duan2, Xiongwei $\\mathbf { H a n } ^ { 2 }$ , Tao Zhong2, Vincent Chau1, Weiwei $\\mathbf { W } \\mathbf { u } ^ { 1 }$ , Wanyuan Wang1\\*\n\n1School of Computer Science and Engineering, Southeast University 2Noahâ€™s Ark Lab, Huawei Technologies yixuanli,chencan0,jiajun li,vincentchau,weiweiwu,wywang @seu.edu.cn, jiahui.duan,xiongwei,zhongtao5 @huawei.com\n\n# Abstract\n\nBy exploiting the correlation between the structure and the solution of Mixed-Integer Linear Programming (MILP), Machine Learning (ML) has become a promising method for solving large-scale MILP problems. Existing ML-based MILP solvers mainly focus on end-to-end solution learning, which suffers from the scalability issue due to the high dimensionality of the solution space. Instead of directly learning the optimal solution, this paper aims to learn a reduced and equivalent model of the original MILP as an intermediate step. The reduced model often corresponds to interpretable operations and is much simpler, enabling us to solve largescale MILP problems much faster than existing commercial solvers. However, current approaches rely only on the optimal reduced model, overlooking the significant preference information of all reduced models. To address this issue, this paper proposes a preference-based model reduction learning method, which considers the relative performance (i.e., objective cost and constraint feasibility) of all reduced models on each MILP instance as preferences. We also introduce an attention mechanism to capture and represent preference information, which helps improve the performance of model reduction learning tasks. Moreover, we propose a SETCOVER based pruning method to control the number of reduced models (i.e., labels), thereby simplifying the learning process. Evaluation on real-world MILP problems shows that 1) compared to the state-of-the-art model reduction ML methods, our method obtains nearly $20 \\%$ improvement on solution accuracy, and 2) compared to the commercial solver Gurobi, two to four orders of magnitude speedups are achieved.\n\n# Introduction\n\nDue to its strong expressiveness, Mixed-Integer Linear Programming (MILP) has been widely used in various critical domains, including supply chain and logistics (Chao, Jasin, and Miao 2024), service scheduling (Rosemarin, Rosenfeld, and Kraus 2019), energy management (Morales-EspanËœa, Latorre, and Ramos 2013), transportation planning (Lowalekar, Varakantham, and Jaillet 2021; Li et al. 2024), chip design (Wang et al. 2024c,d), and chemistry research (Geng et al. 2023b). Commercial solvers, such as Gurobi, Cplex and Matlab, are mainly used to solve MILP problems. In realworld industrial applications, MILP instances often involve hundreds of thousands of decision variables and constraints (Morales-EspanËœa, Latorre, and Ramos 2013; Li et al. 2021). Existing commercial solvers are based on exact solutions, which are computationally expensive and cannot meet the real-time demands of industrial applications.\n\nIn many scenarios, a large number of homogeneous MILPs with similar combinatorial structures need to be solved simultaneously. For example, online stochastic programming often involves solving similar MILP instances at each stage, with slightly modified input parameters while the structure remains unchanged (Lowalekar, Varakantham, and Jaillet 2018; Bertsimas and Stellato 2022). Machine Learning (ML), with its powerful pattern recognition capability, can exploit the correlation between the structure and the solution of MILP, and has recently become a very promising research topic for solving large-scale MILP (Bengio, Lodi, and Prouvost 2021; Zhang et al. 2023; Hentenryck and Dalmeijer 2024). Existing ML-based MILP solvers can be classified into two categories: 1) end-to-end solution prediction, i.e., directly learning the mapping between MILP instances and solutions (Donti, Rolnick, and Kolter 2021; Ding et al. 2020; Chen et al. 2023); and 2) learning to optimize, i.e., learning to improve the process of traditional solvers (He, DaumeÂ´, and Eisner 2014; Khalil et al. 2017; Song et al. 2020a; Chi et al. 2022; Ling, Wang, and Wang 2024; Han et al. 2023; Balcan et al. 2024). Due to the high dimensionality of the solution space, existing ML methods that learn the optimal solution as a function of the input parameters, suffer from the scalability issue. Furthermore, it is currently not possible to interpret the predicted solution or to understand it intuitively (Park and Hentenryck 2023).\n\nInstead of directly learning the optimal solution, this paper takes a different method to learn a reduced and equivalent model of the original MILP as an intermediate step. In Operations Research (Boyd and Vandenberghe 2014), an equivalently reduced model of the MILP constitutes the minimal information, including the set of active constraints and the value of integer variables at the optimal solution, required to recover the optimal solution. Model reduction learning has the following three advantages (Misra, Roald, and $\\mathrm { N g } ~ 2 0 2 2 \\rangle$ ): 1) from the optimization perspective, the reduced model is much easier than the original MILP model, which can be solved fast, 2) from the ML perspective, using the reduced models as labels can reduce the dimension of the learning task, and 3) from the application perspective, the reduced model often corresponds to interpretable modes of operation, which can assist human engineers in decision making (Bertsimas and Stellato 2021).\n\nTo the best of our knowledge, (Misra, Roald, and $\\mathrm { N g }$ 2022; Bertsimas and Kim 2023) are the only works that learn model reduction for fast MILP solving. Their idea is to train a classification algorithm that aims to predict the correct label (i.e., reduced model). However, the algorithm treats a set of feasible reduced models as equally desirable labels, failing to fully exploit the comparative information available in the reduced model space. To tackle this challenge, this paper transforms the performance (i.e., objective function value and constraint feasibility) of a reduced model on an MILP instance as preferences, and proposes a preference-based reduced model learning method. An attention-based encoder then utilizes the ranked preferences is proposed to extract correlations between instances and reduced models.\n\nThe contributions of this paper are summarized as follows. First, we introduce a different model reduction approach to learn the optimal solution of MILP problems. To improve learning accuracy, we fully exploit the preference information in terms of the performance of reduced models on instances. We also integrate an attention architecture and preference-based loss function to capture the correlations between instances and the preferred reduced models. Second, to avoid the number of labels (i.e., reduced models) growing quickly with the number of instances, we propose a SETCOVER technique to generate the minimum labels that are feasible for all instances. Finally, we conduct extensive experiments on real-world domains to validate the proposed preference-based model reduction learning method. Results show that 1) our method can prune redundant reduced models efficiently, and 2) our method has a significant improvement in finding accurate solutions within seconds.\n\n# Related Work\n\nExisting ML-based MILP solving methods can be categorized into three groups: 1) end-to-end solution prediction, i.e., directly learning the mapping between MILP instances and solutions; 2) learning to optimize, i.e., learning to accelerate the solving process of traditional exact/heuristic methods; 3) learning to simplify the MILP, i.e., learning to presolve or reduce the size of the MILP formulation.\n\nEnd-to-end Solution Prediction. Using ML to learn the mapping from MILP instances to a high-dimensional solution space is straightforward, however, it often results in low prediction accuracy (Donti, Rolnick, and Kolter 2021; Park and Hentenryck 2023; Chen et al. 2023). Therefore, (Nair et al. 2020; Ye et al. 2023) only predicts values for partial variables and computes the values of the remaining variables using the off-the-shelf solver. Directly predicting variable values cannot maintain the hard constraints (Ding et al. 2020). Instead, (Han et al. 2023) predicts an initial solution and searches for feasible solutions in its neighborhood.\n\nLearning to Optimize. For exact solving, there are always hyperparameters and selection rules that need to be fine-tuned to accelerate the solving process. For example, the selection of branching variables and their values in\n\nBranch-and-Bound, the selection of the cutting rules in Cutting Plane, and the column generated in the Column Generation algorithm. Using experienced data of these exact solvers, Imitation Learning (IL) and Reinforcement Learning (RL) have been used to learn effective hyperparameters and selection rules (Wang et al. 2024a,b; Huang et al. 2022; Lin et al. 2022; Wang et al. 2023). On the other hand, for heuristic algorithms such as Local Search Heuristics (Cai and Su 2013), and Large Neighborhood Search Heuristics (Song et al. 2020b; Wu et al. 2021), ML can also be used to improve their effectiveness. For example, (Qi, Wang, and Shen 2021) use RL to iteratively explore better solutions in Feasible Pump, and (Nair, Alizadeh et al. 2020) use RL to search for better solutions within a neighborhood.\n\nAlthough these two directions introduce MILP to the benefits of ML and show promising results, they do not scale well to real-world applications. Directly predicting a highdimensional solution is intractable. The efficiency of IL and RL-based optimization methods is limited by the decision horizon (Ye et al. 2024; Geng et al. 2024; Wang et al. 2022) (i.e., the number of integer variables). Another drawback is their inability to enforce the constraints accurately, making them unsuitable for real-world high stakes applications (Liu et al. 2023, 2024b; Geng et al. 2023a). In contrast, this paper utilizes model reduction theory and focuses on learning the mapping between an MILP instance and its optimal reduced model, providing a fast and interpretable MILP solution.\n\nLearning to Simplify the MILP. Large-scale MILP formulations usually contain much redundancy, which can be simplified by the pre-solve techniques (Achterberg and Wunderling 2013). To design high-quality pre-solve routines, (Liu et al. 2024a) and (Kuang et al. 2023) recently use RL to determine which pre-solve operators to select and in what order. (Ye, Xu, and Wang 2024) instead use graph partition for problem division to reduce computational cost. These pre-solve-based simplification methods can only identify limited and explicit redundancy. To identify the minimal tight model, (Misra, Roald, and $\\mathrm { N g } \\ 2 0 2 2$ ; Bertsimas and $\\mathrm { K i m } \\ 2 0 2 3$ ) first propose a classification method to predict the optimal reduced model. However, existing methods only consider several equally desirable reduced models, overlooking the various performances of the reduced models. This paper considers the importance of preference information and designs an efficient method to exploit it to improve the learning accuracy of model reduction.\n\n# Parameterized MILP and Model Reduction\n\nParameterized MILP Problem. The MILP can be formalized as follows:\n\n$$\n\\operatorname* { m i n } _ { x } ~ f ( c , x )\n$$\n\n$$\n\\begin{array} { r } { \\colon g ( A , x ) = \\left\\{ \\begin{array} { l l } { g _ { 1 } ( A _ { 1 } , x _ { I } , x _ { - I } ) \\leq b _ { 1 } } \\\\ { g _ { 2 } ( A _ { 2 } , x _ { I } , x _ { - I } ) \\leq b _ { 2 } } \\\\ { \\vdots } \\\\ { g _ { m } ( A _ { m } , x _ { I } , x _ { - I } ) \\leq b _ { m } } \\end{array} \\right. } \\end{array}\n$$\n\n$$\nx _ { I } \\in \\mathbb { Z } ^ { d } , \\quad x _ { - I } \\in \\mathbb { R } ^ { n - d }\n$$\n\nâ‘  Strategy Generation and Pruning   \nInstanceÎ¸1   \nInstance Î¸2 iInteger Variables D08 Strategy S1 Strategy S2 GBraph! 01 0 Sk 01 0i Strategy Si StrategySk   \nInstanceÎ¸k 00.â–¡ Sj sj 0N . !Tight Constraints Strategy SM .Â· Strategy St   \nInstanceÎ¸N Original Strategies : Instance-Strategy sm Pruned Instances Matching SetCover strategies â‘¡ Preference-based Strategy Learning Pairwise Pridicted Preference gi(x1...,xn)â‰¤bğ‘–   \n$\\textcircled{1}$ Strategy Inputs Rewards !Probability!Difference 0.97gij(x..,Xnï¼‰ â‰¤bj Si   \nGenPruting WU 00 si Y 6 00 Sk 0i å‡  Â·Â· > gm(x1.,..,xn)â‰¤bm 0.96 > + .   \n$\\textcircled{2}$ Prefei S Prefecesdee Difene 1 Stra insnce\n\nwhere $c \\in \\mathbb { R } ^ { n }$ denotes the coefficient vector of the objective function, $f : \\mathbb { R } ^ { n } \\times \\mathbb { R } ^ { n } \\to \\mathbb { R }$ is the objective function. $g _ { i } :$ $\\mathbb { R } ^ { n } \\times \\mathbb { R } ^ { n } \\to \\mathbb { R }$ denotes the $i$ -th constraint and $A _ { i }$ denotes its coefficients. $b \\in \\mathbb { R } ^ { m }$ denotes the parameter of these $m$ constraints. The $\\boldsymbol { x } \\in \\mathbb { R } ^ { n }$ is $n$ decision variables, and $I ( | I | =$ $d )$ is the set of integer decision variables, i.e., $x _ { I } \\in \\mathbb { Z } ^ { d }$ . Let $\\dot { \\theta } = \\langle A , c , b \\rangle$ denote the parameters of the MILP problem, each parameter represents a specific MILP instance.\n\nThe Strategy of Model Reduction. Given a parameter $\\theta$ , let $x ^ { * } ( \\theta )$ denote the optimal solution. We denote the tight constraints ${ \\boldsymbol { \\mathcal { T } } } ( { \\boldsymbol { \\theta } } )$ are constraints that are equalities at the optimal solution $x ^ { * } ( \\theta )$ , i.e.,\n\n$$\n\\mathcal { T } ( \\theta ) = \\{ i \\in \\{ 1 , . . . , m \\} | g _ { i } ( A _ { i } , x ^ { * } ( \\theta ) ) = b _ { i } \\} .\n$$\n\nGiven ${ \\mathcal { T } } ( \\theta )$ , all other constraints in the MILP model are redundant and can be removed. For MILP problems, the number of tight constraints is at most $n$ (i.e., $\\begin{array} { r } { | \\mathcal { T } ( \\boldsymbol { \\theta } ) | \\leq n ; } \\end{array}$ ) (Nocedal and Wright 2006). Since some components of $x$ are integers, it is still not trivial to compute the optimal solution by only knowing ${ \\mathcal { T } } ( \\theta )$ . By fixing the integer components to their optimal values $x ^ { * } ( \\theta )$ , the tight constraints allow us to efficiently compute the optimal solution. We now define the optimal strategy of model reduction as the set of tight constraints together with the value of integer variables at the optimal solution, i.e., $s ^ { * } ( \\theta ) = ( T ( \\theta ) , x ^ { * } \\bar { ( \\theta ) } )$ . Finally, given the optimal strategy $s ^ { * } ( \\theta )$ , the original MILP model (1)-(3) can be reduced to a LP model:\n\n$$\n\\operatorname* { m i n } _ { x } ~ f ( c , x )\n$$\n\n$$\nx _ { I } = x _ { I } ^ { \\ast } ( \\theta ) , \\quad x _ { - I } \\in \\mathbb { R } ^ { n - d }\n$$\n\nThe reduced LP model (5)-(7) is much easier because LP is continuous and has a smaller number of constraints.\n\nObjective. Given an MILP problem, although different MILP instances are not the same, only the key parameters vary slightly, and the structure remains unchanged. We can exploit the repetitive structure of the MILP instances and solutions and learn to solve unseen MILP instances. Therefore, our objective in this paper is to learn the mapping from the parameter $\\theta$ to the optimal strategies $s ^ { * } ( \\theta )$ as an intermediate step for fast MILP solving.\n\n# Framework Overview\n\nThe proposed framework comprises two phases: 1) Strategy Generation and Pruning for exploring a set of useful strategies $S$ as labels, and 2) Preference-based Strategy Learning for predicting the correct strategy $s \\in S$ for any instance $\\theta$ (see Figure 1 for details).\n\nStrategy Generation and Pruning aims to generate a set of useful strategies that can be applied to all MILP instances. Intuitively, we can explore as many MILP instances $\\theta$ as possible and denote their optimal strategy $s ^ { * } ( \\theta )$ as candidate strategy labels. However, the number of candidate strategies can grow quickly with respect to the number of instances, thereby making the strategy learning task very difficult. This paper proposes a SETCOVER technique for strategy pruning while ensuring that all generated instances can be covered, in which the cover means that there is at least one feasible strategy for an instance.\n\nPreference-based Strategy Learning aims to propose a machine learning approach to predict the optimal strategy $s ^ { * } ( \\theta )$ for any MILP instance $\\theta$ . Given the training data $\\{ \\theta _ { i } , s ^ { * } ( \\theta _ { i } ) \\}$ , existing strategy learning approaches typically only focus on the exact optimal strategy (Misra, Roald, and $\\Nu \\tt { g } 2 0 2 2 )$ or treat a set of feasible strategies equally desirable (Bertsimas and $\\mathrm { K i m } 2 0 2 3$ ), overlooking the significant preference information available in the instance-strategy space. To address this issue, this paper transforms the performance (i.e., objective function value and constraint feasibility) of strategies on MILP instances as preferences, and proposes a novel preference-based strategy learning method.\n\n# Strategy Generation and Pruning\n\nIn this section, our objective is to identify a set of useful strategies that will be used as labels for model reduction training and learning.\n\nStrategy Generation. It is difficult to determine the amount of instances required for strategy learning. Following the approach in (Bertsimas and Stellato 2022), we first randomly generate instances $\\theta$ as well as their optimal strategy $s ^ { * } ( \\theta )$ until the Good-Turning estimator $\\textstyle { \\frac { N _ { 1 } } { N } }$ falls below a tiny value, where $N$ is the total number of instances and $N _ { 1 }$ is the number of different strategies appeared exactly once. Specially, given $N$ independent instances $\\Theta _ { N } = \\{ \\theta _ { 1 } , \\cdot \\cdot \\cdot , \\theta _ { N } \\}$ , we can generate $M$ different strategies $S ( \\Theta _ { N } ) = \\left\\{ s _ { 1 } , \\cdot \\cdot \\cdot , s _ { M } \\right\\}$ . However, in large-scale MILP problems, the number of strategies (i.e., labels) $M$ can grow quickly, making the learning task very difficult. Motivated by this issue, we next propose a strategy pruning method based on SETCOVER technique.\n\nStrategy Pruning. In practice, each optimal strategy $s ^ { * } ( \\theta _ { i } )$ not only applies to the corresponding instance $\\theta _ { i }$ , but also may apply to other instances $\\theta _ { j } \\bar { ( \\neq \\theta _ { i } ) }$ . Therefore, many candidate strategies are redundant and we can select only the most useful strategies to apply. We first model the relationship between strategies $S ( \\Theta _ { N } )$ and instances $\\Theta _ { N }$ by an Instance-Strategy bipartite graph $\\dot { G } ( V _ { \\theta } , V _ { s } , E )$ :\n\nâ€¢ The node set consists of the instance nodes $\\begin{array} { r l } { V _ { \\theta } } & { { } = } \\end{array}$ $\\{ v _ { \\theta } ^ { 1 } , . . . , v _ { \\theta } ^ { N } \\}$ and the strategy nodes $V _ { s } = \\{ v _ { s } ^ { 1 } , . . . , v _ { s } ^ { M } \\}$ . Each $v _ { \\theta } ^ { i }$ represents an instance $\\theta _ { i } \\in \\Theta _ { N }$ , and each $v _ { s } ^ { j }$ represents a strategy $s _ { j } \\in S ( \\Theta _ { N } )$ .   \nâ€¢ An edge $e _ { i , j } ~ \\in ~ E$ exists between $v _ { \\theta } ^ { i }$ and $v _ { s } ^ { j }$ if applying the strategy $s _ { j }$ to the instance $\\theta _ { i }$ , and the infeasibility $p ( \\theta _ { i } , s _ { j } )$ and suboptimality $d ( \\theta _ { i } , s _ { j } )$ of the reduced problem (5)-(7) are both below a tiny threshold $[ \\epsilon _ { p } , \\epsilon _ { d } ]$ . The infeasibility is defined as:\n\n$$\np ( \\theta _ { i } , s _ { j } ) = \\| ( g ( A , \\hat { x } _ { i , j } ^ { * } ) - b ) _ { + } \\| _ { \\infty } / \\| b \\| ,\n$$\n\nwhere $\\hat { x } _ { i , j } ^ { * }$ is the solution of the reduced problem1. $| | b | |$ normalizes the degree of constraint violation based on the magnitude of the constraint $g ( A , x ) \\leq b$ . The Suboptimality measures the relative distance between the recovered solution $\\hat { x } _ { i , j } ^ { * }$ of the reduced problem and the optimal solution $\\boldsymbol { x } _ { i } ^ { * }$ of the instance $\\theta _ { i }$ :\n\n$$\nd ( \\theta _ { i } , s _ { j } ) = \\vert f ( c , \\hat { x } _ { i , j } ^ { * } ) - f ( c , x _ { i , j } ^ { * } ) \\vert / \\vert f ( c , x _ { i , j } ^ { * } ) \\vert .\n$$\n\nThe objective of strategy pruning is to find a minimal subset of strategy nodes $V _ { s } ^ { ' } \\subseteq V _ { s }$ , such that for any $v _ { \\theta } ^ { i } \\in V _ { \\theta }$ , there exists a $v _ { s } ^ { j } \\in V _ { s } ^ { ' }$ and $e _ { i , j } \\in E$ . This problem of strategy pruning can be reduced to the well known SETCOVER problem of finding the minimum sets to cover all elements. Thus, an efficient greedy algorithm can be employed to find the useful strategies (Khuller, Moss, and Naor 1999). The main idea of the greedy algorithm is to iteratively select the strategy node $v _ { s } ^ { * }$ that is connected to the maximal uncovered instance nodes. This node $v _ { s } ^ { * }$ is then added to the candidate set of strategies $V _ { s } ^ { ' }$ and this strategy selection process continues until all instance nodes are connected to at least one candidate strategy node. Finally, the set of pruned strategies $S ^ { P }$ can be obtained from $V _ { s } ^ { ' }$ .\n\n# Preference-based Strategy Learning\n\nGiven the pruned strategies $S ^ { P }$ , this section proposes to learn the mapping from a MILP instance $\\theta$ to a suitable strategy $s \\mathopen { } \\mathclose \\bgroup \\left( \\theta \\aftergroup \\egroup \\right) \\^ { - } \\in \\mathcal { \\bar { S } } ^ { P }$ . Previous strategy learning approaches (Misra, Roald, and $\\Nu \\mathrm { g } \\ 2 0 2 2$ ; Bertsimas and Stellato 2022) focus on predicting the correct strategy and considering all other strategies equally undesirable, failing to integrate the significant instance-strategy preference information deeply.\n\nPreference Computation. Given an instance $\\theta _ { i }$ , we would like to supply a reward $r ( \\theta _ { i } , s _ { j } )$ to each strategy $s _ { j } \\in S ^ { P }$ . The reward $r ( \\theta _ { i } , s _ { j } ) \\in r$ is used to measure the outcome of applying the strategy $s _ { j }$ to the instance $\\theta _ { i }$ . We follow the same criteria for calculating the relative feasibility and suboptimality as in Eqs. (8) and (9):\n\n$$\nr ( \\theta _ { i } , s _ { j } ) = - l o g ( p ( \\theta _ { i } , s _ { j } ) + d ( \\theta _ { i } , s _ { j } ) ) .\n$$\n\nDirectly learning the real reward function $r ( \\theta _ { i } , s _ { j } )$ between $\\theta _ { i }$ and $s _ { j }$ is extremely challenging because the complex relationships among instances, strategies and rewards. Instead, to enhance simplicity and training stability, we propose to learn a proxy reward model $R _ { \\phi }$ (where $\\phi$ denotes the parameters of the machine learning approach) that can express preferences between strategies. Given an instance $\\theta _ { i }$ , for two instance-strategy pairs $( \\theta _ { i } , s _ { j } )$ and $( \\theta _ { i } , s _ { k } )$ , we define the preference $\\succ$ generated by the rewards $r$ :\n\n$$\n( \\theta _ { i } , s _ { j } ) \\succ ( \\theta _ { i } , s _ { k } ) \\Leftrightarrow r ( \\theta _ { i } , s _ { j } ) > r ( \\theta _ { i } , s _ { k } ) .\n$$\n\nInformally, $( \\theta _ { i } , s _ { j } ) \\succ ( \\theta _ { i } , s _ { k } )$ indicates that the instance $\\theta _ { i }$ prefers the strategy $s _ { j }$ to the strategy $s _ { k }$ .\n\nPreference-based Sampling. To train the proxy reward model $R _ { \\phi }$ , previous preference-based learning (e.g., RLHF (Christiano et al. 2017)) requires selecting all possible pairwise comparisons as samples. For example, let $M ^ { P } = \\dot { | } S ^ { P } |$ denote the number of pruned strategies, there will be $\\binom { M ^ { P } } { 2 }$ preference samples for each instance at the training stage. The number of samples grow quadratically with the number of strategies, thereby increasing the cost of training. Fortunately, in our strategy learning problem, the instance preferences on strategies have a transitivity structure. For example, given the instance $\\theta _ { i }$ , if the strategy $s _ { j }$ is preferred to $s _ { k }$ , and $s _ { k }$ is preferred to $s _ { q }$ , we still have that $s _ { j }$ is preferred to $s _ { q }$ . This transitivity property can rank all candidate strategies as a complete order based on preferences (i.e.,\n\nAlgorithm 1: Preference-based Strategy Learning\n\nInput: Training data $\\{ [ \\theta _ { i } , S ^ { P } ] = [ \\langle \\theta _ { i } , s _ { 1 } \\rangle , . . . , \\langle \\theta _ { i } , s _ { M ^ { P } } \\rangle ]  [ $ $\\theta _ { i } \\in \\Theta _ { N } \\}$ , rewards $r = \\{ r ( \\theta _ { i } , s _ { j } ) ~ | ~ \\theta _ { i } \\in \\Theta _ { N } , s _ { j } \\in S ^ { F } \\}$ preference model $R _ { \\phi }$ , learning rate $\\alpha$ , weight $\\lambda _ { 1 }$ and $\\lambda _ { 2 }$ .\n\nOutput: Optimized model parameters $R _ { \\phi }$ .\n\n1: Initialize model parameters $R _ { \\phi }$ .   \n2: for each instance $\\theta _ { i }$ do   \n3: Predict rewards $\\{ \\hat { r } _ { i , 1 } , . . . , \\hat { r } _ { i , M ^ { P } } \\} = R _ { \\phi } ( [ \\theta _ { i } , S ^ { P } ] )$ .   \n4: Rank strategies $S _ { \\sigma } ^ { P } = \\left\\{ s _ { \\sigma ( 1 ) } , . . . , s _ { \\sigma ( M ^ { P } ) } \\right\\}$ by rewards   \n$r$ to get predicted rewards $\\{ \\hat { r } _ { i , \\sigma ( 1 ) } , . . . , \\hat { r } _ { i , \\sigma ( M ^ { P } ) } \\}$ .   \n5: for adjacent strategies $s _ { \\sigma ( j ) } , s _ { \\sigma ( j + 1 ) }$ in $S _ { \\sigma } ^ { P }$ do   \n6: $\\begin{array} { r } { p _ { i , j } = \\frac { e x p ( \\hat { r } _ { i , \\sigma ( j ) } ) } { e x p ( \\hat { r } _ { i , \\sigma ( j ) } ) + e x p ( \\hat { r } _ { i , \\sigma ( j + 1 ) } ) } } \\end{array}$ .   \n7: Compute difference Î´Ë†i,j = rË†i,Ïƒ(j) âˆ’ rË†i,Ïƒ(j+1).   \n8: end for   \n9: Compute Preference Loss $L _ { p } ( \\phi )$ by $^ { p _ { i , * } }$ and $S _ { \\sigma } ^ { P }$ .   \n10: Compute MSE Loss ${ \\cal L } _ { d } ( \\phi )$ by $\\hat { \\delta } _ { i , * }$ and rewards $r$ .   \n11: end for   \n12: Compute Total Loss $L _ { t o t a l } ( \\phi ) = \\lambda _ { 1 } L _ { p } ( \\phi ) + \\lambda _ { 2 } L _ { d } ( \\phi )$   \n13: Update model parameters $\\phi  \\phi - \\alpha \\nabla _ { \\phi } L _ { t o t a l } ( \\phi )$ .   \n14: Return optimized parameterized model $R _ { \\phi }$ .\n\nEq. (11)). Therefore, for each instance $\\theta _ { i }$ , when the candidate strategies are ranked in decreasing order of their preferences (i.e., $s _ { \\sigma ( j ) } \\in S _ { \\sigma } ^ { P } = \\bigl \\{ s _ { \\sigma ( 1 ) } , . . . , s _ { \\sigma ( M ^ { P } ) } \\bigr \\}$ is ranked by its reward $r \\big ( \\theta _ { i } , s _ { \\sigma ( j ) } \\big )$ , and $\\sigma ( j )$ represents the new position of $s _ { \\sigma ( j ) }$ in the sequence $r ( \\theta _ { i } , s _ { \\sigma ( 1 ) } ) \\geq r ( \\theta _ { i } , s _ { \\sigma ( 2 ) } ) \\geq$ $\\cdot \\cdot \\cdot r ( \\theta _ { i } , s _ { \\sigma ( M ^ { P } ) } ) )$ ), only the $M ^ { P }$ ordered preference samples are necessary as the preference set ${ \\mathcal { D } } ( \\theta _ { i } )$ for $\\theta _ { i }$ :\n\n$$\n\\{ \\langle ( \\theta _ { i } , s _ { \\sigma ( 1 ) } ) \\rangle { - } ( \\theta _ { i } , s _ { \\sigma ( 2 ) } ) \\rangle , \\cdots , \\succ ( \\theta _ { i } , s _ { \\sigma ( M ^ { P } ) } ) \\rangle \\} .\n$$\n\nThe size of the samples in ${ \\mathcal { D } } ( \\theta _ { i } )$ increases linearly with the number of strategies, avoiding a large amount of redundant preference samples in M2\n\nAttention-based Instance-Strategy Encoding. An attention architecture is proposed to improve the representation capacity. The input to the architecture is a vector of instanceowuhtepruet vsetch $[ \\bar { \\theta } _ { i } , { \\cal S } ^ { P } ] = [ \\langle \\theta _ { i } , s _ { 1 } \\rangle , \\cdot \\cdot \\cdot , \\langle \\theta _ { i } , s _ { M ^ { P } } \\rangle ]$ $\\hat { R } _ { i } = \\{ \\hat { r } _ { i , j } \\} _ { j = 1 } ^ { M ^ { P } }$ $\\hat { r } _ { i , j }$ $s _ { j }$ instance $\\theta _ { i }$ . To extract the inherent similarity among strategies as well as the underlying connections between instances and strategies, we apply an attention mechanism to encode instance-strategy pairs. Specifically, we treat each instancestrategy pair $\\left. \\theta _ { i } , s _ { j } \\right.$ as a token, allowing all pairs $[ \\theta _ { i } , S ^ { P } ]$ to be considered when encoding $\\langle \\theta _ { i } , s _ { j } \\rangle$ . This architecture can prioritize the more important pairs and extract features of strategies that can effectively solve instances. The above process can be expressed by the following formula:\n\n$$\n\\begin{array} { r } { A ( [ \\theta _ { i } , S ^ { P } ] ) = s o f t m a x \\left( \\frac { Q K ^ { T } } { \\sqrt { d } } \\right) V . } \\end{array}\n$$\n\nIn Eq. (13), based on the row-wise shared weights $W ^ { q } , W ^ { k }$ , and $W ^ { v }$ , a linear projection operation is acted on the input $[ \\theta _ { i } , S ^ { P } ]$ to compute the queries $Q = [ \\theta _ { i } , S ^ { P } ] W ^ { q }$ , keys\n\n$\\boldsymbol { K } = [ \\theta _ { i } , S ^ { P } ] W ^ { k }$ , and values $V = [ \\theta _ { i } , S ^ { P } ] W ^ { v }$ . The main architecture consists of $L$ layers:\n\n$$\nA _ { L } ( A _ { L - 1 } ( . . . A _ { 1 } ( [ \\theta _ { i } , S ^ { P } ] ) ) ) .\n$$\n\nThe final output layer performs an affine transformation:\n\n$$\n\\hat { R } _ { i } = R _ { \\phi } ( [ \\theta _ { i } , S ^ { P } ] ) = y _ { L } ( A _ { L } ) = \\psi _ { L } ( W _ { L } A _ { L } + b _ { L } ) ,\n$$\n\nwhere $\\psi _ { L }$ is the activation function, $W _ { L }$ and $b _ { L }$ are weights $L$ $. \\hat { R } _ { i } = \\{ \\hat { r } _ { i , j } \\} _ { j = 1 } ^ { M ^ { P } }$ dicted rewards of all strategies $s _ { j } \\in \\ S ^ { P }$ when applied on the instance $\\theta _ { i }$ . For this architecture, $\\phi$ is the whole set of parameters that needs to be learned.\n\nPreference-based Loss Function. In the training phase, the strategies $S ^ { P }$ can be ranked to $S _ { \\sigma } ^ { P }$ by the rewards $r$ , so we can get the reward output $\\hat { R } _ { i }$ ordered by $S _ { \\sigma } ^ { P }$ and form the ranked predicted rewards $\\hat { R } _ { i , \\sigma } = \\left\\{ { \\hat { r } } _ { i , \\sigma ( 1 ) } , . . . , { \\hat { r } } _ { i , \\sigma ( M ^ { P } ) } \\right\\}$ , where $\\sigma ( j )$ represents the new position of $\\hat { r } _ { i , \\sigma ( j ) }$ in the sequence. We can define the preference probability $p _ { i , j }$ between each pair of adjacent strategies $s _ { \\sigma ( j ) }$ and $s _ { \\sigma ( j + 1 ) }$ :\n\n$$\np _ { i , j } = \\frac { \\exp ( \\hat { r } _ { i , \\sigma ( j ) } ) } { \\exp ( \\hat { r } _ { i , \\sigma ( j ) } ) + \\exp ( \\hat { r } _ { i , \\sigma ( j + 1 ) } ) } .\n$$\n\nIn order for the model output $\\hat { R } _ { i }$ to yield correct relative preferences, we need to train $R _ { \\phi }$ to maximize the probability $p _ { i , j }$ for every ordered adjacent sample pair $( \\theta _ { i } , s _ { \\sigma ( j ) } ) \\succ$ $\\left( \\theta _ { i } , s _ { \\sigma ( j + 1 ) } \\right)$ . Therefore, we define the preference loss based on the preference:\n\n$$\n\\begin{array} { r } { L _ { p } ( \\phi ) = - \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\sum _ { j = 1 } ^ { M ^ { P } - 1 } [ \\mu _ { i , j } \\log ( p _ { i , j } ) + \\mathrm { ~ ( ~ } } \\\\ { \\mathrm { ~ ( 1 - } \\mu _ { i , j } ) \\log ( 1 - p _ { i , j } ) ] , } \\end{array}\n$$\n\nwhere the preference labels $\\mu _ { i , j } ~ = ~ 1$ if $\\begin{array} { r l } { ( \\theta _ { i } , s _ { \\sigma ( j ) } ) } & { { } \\succ } \\end{array}$ $( \\theta _ { i } , s _ { \\sigma ( j + 1 ) } )$ , 0.5 otherwise.\n\nIf the model outputs an incorrect order, such as the higher $\\hat { r } _ { i , \\sigma ( 2 ) }$ leading $\\hat { r } _ { i , \\sigma ( 1 ) } ~ < ~ \\hat { r } _ { i , \\sigma ( 2 ) }$ and $\\hat { r } _ { i , \\sigma ( 2 ) } > \\hat { r } _ { i , \\sigma ( 3 ) }$ , although $\\left( \\hat { r } _ { i , \\sigma ( 1 ) } , \\hat { r } _ { i , \\sigma ( 2 ) } \\right)$ would be penalized by $L _ { p } ( \\phi )$ , the error in $\\big ( \\hat { r } _ { i , \\sigma ( 2 ) } , \\hat { r } _ { i , \\sigma ( 3 ) } \\big )$ might even reduce the value of $L _ { p } ( \\phi )$ . To address this issue, we introduce a reward differencebased loss to penalize the incorrect output of $\\hat { r } _ { i , \\sigma ( 2 ) }$ and to reinforce the correct order within the sequence:\n\n$$\n\\begin{array} { r } { L _ { d } ( \\phi ) = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\sum _ { j = 1 } ^ { M ^ { P } - 1 } ( \\hat { r } _ { i , \\sigma ( j ) } - \\hat { r } _ { i , \\sigma ( j + 1 ) } - \\delta _ { i , j } ) ^ { 2 } , } \\end{array}\n$$\n\nwhere $\\delta _ { i , j } = r ( \\theta _ { i } , s _ { \\sigma ( j ) } ) - r ( \\theta _ { i } , s _ { \\sigma ( j + 1 ) } )$ is the target reward differences between adjacent strategies in the sequence. The loss can effectively deepen the relative preferences and improves training stability. To enhance the coordination between the loss functions, finally, our total loss is:\n\n$$\nL _ { t o t a l } ( \\phi ) = \\lambda _ { 1 } L _ { p } ( \\phi ) + \\lambda _ { 2 } L _ { d } ( \\phi ) ,\n$$\n\nwhere $\\lambda _ { 1 }$ and $\\lambda _ { 2 }$ are hyperparameters for different scenarios.   \nThe framework of our method is shown in Algorithm 1.\n\n![](images/a4b8ccfbbc3f856785952c8db96ec6e0a3f15e4869150c440cc17167662c5d27.jpg)  \nFigure 2: The performance on six scenarios from MIPLIB and each vertex in the subplot represents a metric. For better presentation of results (due to the differences in metric magnitudes), we map each metric to the range of (0, 100) using the same function for each metric. The Gurobiâ€™s result is shown on the left since its results are similarly presented across all scenarios\n\n# Online Strategy Inference\n\nTo overcome the potential prediction errors, reliability can be increased by taking the Top- $k$ output strategies as candidates. Given the parameterized preference model $R _ { \\phi }$ , and the instance $\\theta _ { i }$ , let $s _ { k }$ be the set of the $k$ strategies corresponding to the $k$ largest outputs\n\n$$\n\\begin{array} { r } { S _ { k } = \\{ s _ { j } ~ | ~ \\hat { r } _ { i , j } \\in \\mathrm { T o p } { - } k \\big ( \\{ \\hat { r } _ { i , 1 } , \\hat { r } _ { i , 2 } , \\dots , \\hat { r } _ { i , M ^ { P } } \\} \\big ) \\} , } \\end{array}\n$$\n\nwhere $\\hat { r } _ { i , j } ~ \\in ~ \\hat { R } _ { i }$ is the output of preference model $R _ { \\phi }$ . We select $S _ { k }$ as the strategy candidates for the instance $\\theta _ { i }$ , and evaluate the strategies $s _ { j } \\in S _ { k }$ by solving the reduction model $s _ { j } ( \\theta _ { i } )$ . And the $s _ { j }$ with the lowest infeasibility $p ( \\theta _ { i } , s _ { j } )$ is selected as the target strategy.\n\n$$\n\\hat { s } = a r g m i n _ { s _ { j } \\in S _ { k } } p ( \\theta _ { i } , s _ { j } ) .\n$$\n\nTo solve the reduction model $s _ { j } ( \\theta _ { i } )$ , for special types of problems such as MIQP (Mixed-Integer Quadratic Programming) and MILP, the linear system can be simplified based on the KKT optimality conditions (Boyd and Vandenberghe 2014), further speeding up the solution time. The workflow in online stage is detailed in the Appendix.\n\n# Experiments\n\nIn this section, we compare our proposed method with the learning-based methods and the commercial solvers on realworld datasets to validate the performance and efficiency.\n\nEvaluation Metrics. We follow the same criteria for solving quality as in (Bertsimas and Stellato 2022), using the accuracy metric. We consider solutions to be accurate if their\n\nTable 1: The performance when $K = 3 0$ on fuel cell energy management under different scales $( T )$ , where $t _ { \\mathrm { m a x } } ^ { H }$ means the maximum computation time (in seconds) from Gurobi Heuristic and tGmax is from Gurobi.   \n\n<html><body><table><tr><td>+H )tmax(s) M T tmax(s)t</td><td>tmax(s) accuracy method MP</td><td></td><td>gain</td></tr><tr><td>20 0.366 0.379 578</td><td>Ours 41 0.061 MLOPT 110 0.047</td><td>97.66% 86.68%</td><td>10.98 %</td></tr><tr><td>40 1.120 4.840 2041</td><td>Ours 75 0.069 MLOPT 2026 0.054</td><td>94.20% 77.85%</td><td>16.35 %</td></tr><tr><td>60 1.184 33.7332924</td><td>Ours 129 0.086 MLOPT 2863 0.069</td><td>84.90% 45.97%</td><td>38.93%</td></tr></table></body></html>\n\ninfeasibility and suboptimality are within a small tolerance. Given $N$ test samples, the testing accuracy on this dataset is:\n\n$$\na c c u r a c y = \\frac { 1 } { N } \\left| \\left\\{ \\theta _ { i } \\mid p ( \\theta _ { i } , \\hat { s _ { j } } ) \\leq \\epsilon _ { 1 } \\wedge d \\big ( \\theta _ { i } , \\hat { s _ { j } } \\big ) \\leq \\epsilon _ { 2 } \\right\\} \\right| ,\n$$\n\nwhere the tolerances $\\epsilon _ { 1 }$ for infeasibility and $\\epsilon _ { 2 }$ for suboptimality are both set to $1 \\times 1 0 ^ { - 4 }$ .\n\nDatasets. We evaluate the performance through:\n\n1. MIPLIB (Gleixner et al. 2021), six scenarios selected as in (Bertsimas and Kim 2023), the real-world MILP problems with varying scales and solving difficulties.\n\n2. Fuel Cell Energy Management Problem (Frick, Domahidi, and Morari 2015), treated as the primary evaluation scenario by (Bertsimas and Stellato 2022). Its scale can be increased by increasing $T$ for deeper analysis.\n\n![](images/fb27835f1e36e537326fd7b50b343f45568aaccceee9baeec10145ef6914d4f7.jpg)  \nFigure 3: The average infeasibility and suboptimality for our method (bar on the left) and MLOPT under $\\scriptstyle { T = 6 0 }$ and varying $k$ on Fuel Cell Energy Management.\n\n![](images/1c63d8da71de27e3e94e30fad1cdaaf57ab11508236baf93b38b43baa89df239.jpg)  \nFigure 4: Performance on Inventory Management Problems\n\n3. Inventory Management Problem, five large-scale (average number of 100,000 constraints) real-world industrial problems from a companyâ€™s real supply chain scenarios.\n\nBaselines. We compare our method with:\n\n1. Gurobi (Gurobi Optimization 2021), the advanced commercial solver. To make the comparison as fair as possible, we run Gurobi with â€œwarm-startâ€ enabled, that reuses the solution obtained from previous parameters.   \n2. Gurobi Heuristic, Gurobiâ€™s â€œheuristicâ€ mode, a very fast heuristic algorithm with time limit of one second.   \n3. MLOPT (Bertsimas and $\\mathrm { K i m } \\ 2 0 2 3 \\rangle$ ), a model reduction based method that is the most applicable learning-based method in our scenario.   \n4. Predict and Search (Han et al. 2023), a solutionprediction based method with initial variable prediction and further neighborhood searching.\n\nIn the datasets 2 and 3, the number of integer variables is small, and the Predict and Search method offers limited acceleration, making it less applicable. Thus, we do not compare the Predict and Search method in the datasets 2 and 3. For Datasets 1 and 3, we used 1000 samples for training and 500 for testing; for Dataset 2, 10,000 samples were used for training and 1000 for testing. The detailed setups and dataset formulations are presented in the Appendix.\n\nEvaluation on MIPLIB. Figure 2 shows that: 1) Our method performs nearly as well as Gurobi on suboptimality and feasibility across most scenarios. 2) Our method shows a slight advantage in feasibility compared to MLOPT and a significant improvement in suboptimality. 3) Heuristic methods struggle to provide high-quality solutions within the time limit, leading to poor performance across multiple metrics. 4) The Predict and Search method, which only predicts partial integer variables, performs well in terms of feasibility, but its search process can cause it to get trapped in local optimal, and the search sub-task is more time-consuming compared to our method.\n\nEvaluation on Fuel Cell Energy Management problem. As shown in Table 1, our method achieves an improvement of nearly $3 9 \\%$ in accuracy when the problem is the most complex. In this table, $M$ represents the number of strategies and $M ^ { P }$ represents the strategies after pruning. Our pruning method reduces the number of strategies much more than the pruning of MLOPT, especially as the problem size increases. The pruning of MLOPT performs poorly due to the difficulty in reassigning labels with some key strategies discarded. From Figure 3, it can be observed that as $k$ increases, the performance of the model improves, and our method yields better outcomes than MLOPT in both key metrics under the same $k$ because: 1) The reduced strategy space increases the likelihood of finding effective strategies within the Top- $k$ . 2) The preference model can evaluate the performance of all strategies for the given instance, allowing it to identify a more reasonable set of $k$ -candidate strategies. Evaluation on Inventory Management problem. Figure 4 shows the results on this scenario, where $P _ { 1 }$ to $P _ { 5 }$ denote five increasing model sizes. We observe that in this scenario, our method can maintain feasibility across all instances, thus only the suboptimality and accuracy metrics are reported. As the problem size increases, our method consistently achieves low average suboptimality, with accuracy remaining stable at around $90 \\%$ . On the largest scale problem $P _ { 5 }$ which has more than 270,000 constraints, our method also outperforms MLOPT by approximately $30 \\%$ .\n\nComputation Time. Among the three datasets, our method, requiring only online inference and solving the linear system of reduced model, is significantly faster than Gurobi and heuristic algorithms. Compared with Gurobi, the computation time is improved by three orders of magnitude. Compared with MLOPT, because of the similar workflow, there is only a tiny difference in $t _ { \\mathrm { m a x } }$ coming from the size of the neural network under the same value of $k$ . Compared with Predict and Search on the MIPLIB dataset (Figure 2), our reduction-prediction method is more time-efficient because the searching within high dimensionality neighbourhood is relatively time-consuming.\n\n# Conclusion\n\nThis paper proposes a preference-based model reduction (i.e., strategy) learning for fast and interpretable MILP solving. There are two challenges for strategy learning: 1) how to generate sufficient strategies that are useful for strategy learning, and 2) how to integrate the performance information of all candidate strategies to improve strategy learning. This paper first introduces the SETCOVER technique to find a minimal set of strategies that can be applied to all MILP instances. Furthermore, the preference information of these available strategies on instances and the attention mechanism are integrated to improve the learning capacity. Simulations on real-world MILP problems show that the proposed method has a significant improvement in solving time and the accuracy of the output solutions.\n\n# Acknowledgments\n\nThis research is supported in part by Key Research and Development Projects in Jiangsu Province under Grant BE2021001-2, in part by the National Natural Science Foundation of China 62476121,62202100 and 61806053, and in part by the Jiangsu Provincial Double-Innovation Doctor Program No. JSSCBS20220077.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆMILPï¼‰åœ¨å¤§è§„æ¨¡å·¥ä¸šåº”ç”¨ä¸­çš„å®æ—¶æ±‚è§£éš¾é¢˜ã€‚ç°æœ‰å•†ä¸šæ±‚è§£å™¨ï¼ˆå¦‚Gurobiï¼‰åŸºäºç²¾ç¡®æ±‚è§£æ–¹æ³•ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œæ— æ³•æ»¡è¶³å®æ—¶éœ€æ±‚ã€‚\\n> *   è¯¥é—®é¢˜åœ¨ä¾›åº”é“¾ç®¡ç†ã€èƒ½æºè°ƒåº¦ã€èŠ¯ç‰‡è®¾è®¡ç­‰é¢†åŸŸå…·æœ‰å…³é”®ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦åŒæ—¶æ±‚è§£å¤§é‡ç»“æ„ç›¸ä¼¼çš„MILPå®ä¾‹çš„åœºæ™¯ä¸­ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåå¥½çš„æ¨¡å‹ç®€åŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ MILPå®ä¾‹ä¸å…¶ç®€åŒ–æ¨¡å‹ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œæ˜¾è‘—åŠ é€Ÿæ±‚è§£è¿‡ç¨‹ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **è´¡çŒ®1ï¼š** æå‡ºåå¥½é©±åŠ¨çš„ç®€åŒ–æ¨¡å‹å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨æ‰€æœ‰ç®€åŒ–æ¨¡å‹çš„æ€§èƒ½ä¿¡æ¯ï¼ˆç›®æ ‡æˆæœ¬å’Œçº¦æŸå¯è¡Œæ€§ï¼‰ä½œä¸ºåå¥½ä¿¡å·ï¼Œæå‡äº†å­¦ä¹ å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ±‚è§£ç²¾åº¦æå‡äº†è¿‘ `20%`ã€‚\\n> *   **è´¡çŒ®2ï¼š** å¼•å…¥SETCOVERæŠ€æœ¯å¯¹ç®€åŒ–æ¨¡å‹è¿›è¡Œå‰ªæï¼Œæ§åˆ¶æ ‡ç­¾æ•°é‡ï¼Œç®€åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚\\n> *   **è´¡çŒ®3ï¼š** åœ¨çœŸå®MILPé—®é¢˜ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç›¸æ¯”å•†ä¸šæ±‚è§£å™¨Gurobiï¼Œå®ç°äº† `2-4` ä¸ªæ•°é‡çº§çš„åŠ é€Ÿã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å­¦ä¹ MILPå®ä¾‹çš„ç®€åŒ–æ¨¡å‹ï¼ˆå³ç­–ç•¥ï¼‰ä½œä¸ºä¸­é—´æ­¥éª¤ï¼Œè€Œéç›´æ¥å­¦ä¹ æœ€ä¼˜è§£ã€‚ç®€åŒ–æ¨¡å‹åŒ…å«ç´§çº¦æŸå’Œæ•´æ•°å˜é‡çš„æœ€ä¼˜å€¼ï¼Œèƒ½å¤Ÿå°†åŸå§‹MILPé—®é¢˜è½¬åŒ–ä¸ºæ›´æ˜“æ±‚è§£çš„çº¿æ€§è§„åˆ’ï¼ˆLPï¼‰é—®é¢˜ã€‚\\n> *   è¯¥æ–¹æ³•æœ‰æ•ˆçš„åŸå› æ˜¯ç®€åŒ–æ¨¡å‹ç»´åº¦ä½ä¸”å¯¹åº”å¯è§£é‡Šçš„æ“ä½œï¼Œä»è€Œæ˜¾è‘—é™ä½æ±‚è§£å¤æ‚åº¦ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ç°æœ‰æ–¹æ³•ä»…å…³æ³¨æœ€ä¼˜ç®€åŒ–æ¨¡å‹ï¼Œå¿½ç•¥äº†å…¶ä»–ç®€åŒ–æ¨¡å‹çš„æ€§èƒ½ä¿¡æ¯ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** æå‡ºåå¥½å­¦ä¹ æ–¹æ³•ï¼Œå°†ç®€åŒ–æ¨¡å‹çš„æ€§èƒ½ï¼ˆç›®æ ‡å‡½æ•°å€¼å’Œçº¦æŸå¯è¡Œæ€§ï¼‰è½¬åŒ–ä¸ºåå¥½ä¿¡å·ï¼Œå¹¶è®¾è®¡æ³¨æ„åŠ›æœºåˆ¶æ•æ‰å®ä¾‹ä¸ç®€åŒ–æ¨¡å‹ä¹‹é—´çš„ç›¸å…³æ€§ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **ç­–ç•¥ç”Ÿæˆä¸å‰ªæï¼š** éšæœºç”ŸæˆMILPå®ä¾‹åŠå…¶æœ€ä¼˜ç­–ç•¥ï¼Œä½¿ç”¨SETCOVERæŠ€æœ¯å‰ªæå†—ä½™ç­–ç•¥ï¼Œä¿ç•™æœ€å°è¦†ç›–é›†ã€‚\\n> 2.  **åå¥½è®¡ç®—ï¼š** å¯¹æ¯ä¸ªå®ä¾‹-ç­–ç•¥å¯¹è®¡ç®—å¥–åŠ±ï¼Œè¡¡é‡ç­–ç•¥åœ¨å®ä¾‹ä¸Šçš„æ€§èƒ½ï¼ˆå…¬å¼ï¼š`r(Î¸_i, s_j) = -log(p(Î¸_i, s_j) + d(Î¸_i, s_j))`ï¼‰ã€‚\\n> 3.  **åå¥½å­¦ä¹ ï¼š** ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ç¼–ç å®ä¾‹-ç­–ç•¥å¯¹ï¼Œé€šè¿‡åå¥½æŸå¤±ï¼ˆå…¬å¼ï¼š`L_p(Ï•)`ï¼‰å’Œå¥–åŠ±å·®å¼‚æŸå¤±ï¼ˆå…¬å¼ï¼š`L_d(Ï•)`ï¼‰è®­ç»ƒä»£ç†å¥–åŠ±æ¨¡å‹ã€‚\\n> 4.  **åœ¨çº¿æ¨ç†ï¼š** å¯¹æµ‹è¯•å®ä¾‹ï¼Œé€‰æ‹©Top-kç­–ç•¥å€™é€‰ï¼Œé€šè¿‡æ±‚è§£ç®€åŒ–æ¨¡å‹ç¡®å®šæœ€ç»ˆç­–ç•¥ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   Gurobiï¼ˆå•†ä¸šæ±‚è§£å™¨ï¼‰\\n> *   Gurobi Heuristicï¼ˆå¯å‘å¼æ¨¡å¼ï¼‰\\n> *   MLOPTï¼ˆåŸºäºæ¨¡å‹ç®€åŒ–çš„æ–¹æ³•ï¼‰\\n> *   Predict and Searchï¼ˆåŸºäºé¢„æµ‹å’Œæœç´¢çš„æ–¹æ³•ï¼‰\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨æ±‚è§£ç²¾åº¦ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨MIPLIBæ•°æ®é›†ä¸Šçš„ç²¾åº¦æ¥è¿‘Gurobiï¼Œæ˜¾è‘—ä¼˜äºMLOPTï¼ˆæå‡çº¦ `20%`ï¼‰å’ŒGurobi Heuristicï¼ˆåè€…å› æ—¶é—´é™åˆ¶æ€§èƒ½è¾ƒå·®ï¼‰ã€‚\\n> *   **åœ¨æ±‚è§£é€Ÿåº¦ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•çš„å¤„ç†é€Ÿåº¦æ¯”Gurobiå¿« `2-4` ä¸ªæ•°é‡çº§ï¼Œä¸MLOPTé€Ÿåº¦ç›¸å½“ï¼Œä½†ç²¾åº¦æ›´é«˜ã€‚\\n> *   **åœ¨ç‡ƒæ–™ç”µæ± èƒ½æºç®¡ç†é—®é¢˜ä¸Šï¼š** å½“é—®é¢˜è§„æ¨¡æœ€å¤§æ—¶ï¼ˆ`T=60`ï¼‰ï¼Œæœ¬æ–‡æ–¹æ³•çš„ç²¾åº¦è¾¾åˆ° `84.90%`ï¼Œæ˜¾è‘—ä¼˜äºMLOPTçš„ `45.97%`ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ (Mixed-Integer Linear Programming, MILP)\\n*   æ¨¡å‹ç®€åŒ– (Model Reduction, N/A)\\n*   åå¥½å­¦ä¹  (Preference Learning, N/A)\\n*   æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism, N/A)\\n*   SETCOVERå‰ªæ (SETCOVER Pruning, N/A)\\n*   å®æ—¶æ±‚è§£ (Real-Time Solving, N/A)\\n*   å·¥ä¸šä¼˜åŒ– (Industrial Optimization, N/A)\\n*   å¯è§£é‡Šæ€§ (Interpretability, N/A)\"\n}\n```"
}