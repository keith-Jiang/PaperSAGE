{
    "source": "Semantic Scholar",
    "arxiv_id": "2405.08674",
    "link": "https://arxiv.org/abs/2405.08674",
    "pdf_link": "https://arxiv.org/pdf/2405.08674.pdf",
    "title": "Expensive Multi-Objective Bayesian Optimization Based on Diffusion Models",
    "authors": [
        "Bingdong Li",
        "Zixiang Di",
        "Yongfan Lu",
        "Hong Qian",
        "Feng Wang",
        "Peng Yang",
        "Ke Tang",
        "Aimin Zhou"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-05-14",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "Shanghai Frontiers Science Center of Molecule Intelligent Syntheses",
        "Shanghai Institute of AI for Education",
        "School of Computer Science and Technology, East China Normal University",
        "Key Laboratory of Advanced Theory and Application in Statistics and Data Science, Ministry of Education",
        "School of Computer Science, Wuhan University",
        "Department of Statistics and Data Science, Southern University of Science and Technology",
        "Guangdong Provincial Key Laboratory of Brain-Inspired Intelligent Computation",
        "Department of Computer Science and Engineering, Southern University of Science and Technology"
    ],
    "paper_content": "# Expensive Multi-Objective Bayesian Optimization Based on Diffusion Models\n\nBingdong Li1, 2, Zixiang Di1, 2, Yongfan ${ \\bf L u } ^ { 1 , 2 }$ , Hong Qian1, Feng Wang3\\*, Peng Yang4, 5, Ke Tang5, Aimin Zhou1\\*\n\n1Shanghai Frontiers Science Center of Molecule Intelligent Syntheses, Shanghai Institute of AI for Education, and School of Computer Science and Technology, East China Normal University, Shanghai 200062, China 2Key Laboratory of Advanced Theory and Application in Statistics and Data Science, Ministry of Education 3School of Computer Science, Wuhan University, Wuhan 430072, China 4Department of Statistics and Data Science, Southern University of Science and Technology, Shenzhen 518055, China   \n5Guangdong Provincial Key Laboratory of Brain-Inspired Intelligent Computation, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China bdli@cs.ecnu.edu.cn, 51265901113@stu.ecnu.edu.cn, 51255901053@stu.ecnu.edu.cn, hqian $@$ cs.ecnu.edu.cn, fengwang $@$ whu.edu.cn, yangp $@$ sustech.edu.cn, tangk3 $@$ sustech.edu.cn, amzhou $@$ cs.ecnu.edu.cn\n\n# Abstract\n\nMulti-objective Bayesian optimization (MOBO) has shown promising performance on various expensive multi-objective optimization problems (EMOPs). However, effectively modeling complex distributions of the Pareto optimal solutions is difficult with limited function evaluations. Existing Pareto set learning algorithms may exhibit considerable instability in such expensive scenarios, leading to significant deviations between the obtained solution set and the Pareto set (PS). In this paper, we propose a novel Composite Diffusion Model based Pareto Set Learning algorithm (CDM-PSL) for expensive MOBO. CDM-PSL includes both unconditional and conditional diffusion model for generating high-quality samples efficiently. Besides, we introduce a weighting method based on information entropy to balance different objectives. This method is integrated with a guiding strategy to appropriately balancing different objectives during the optimization process. Experimental results on both synthetic and real-world problems demonstrates that CDM-PSL attains superior performance compared with state-of-the-art MOBO algorithms.\n\nCode ‚Äî https://github.com/ilog-ecnu/CDM-PSL\n\n# Introduction\n\nExpensive multi-objective optimization problems are commonly seen in various fields, such as antenna structure design (Ding et al. 2019), financial cloud services (Yang et al. 2024), and clinical drug trials (Yu, Ramakrishnan, and Meinzer 2019). Handling EMOPs involves optimizing multiple (often conflicting) objectives simultaneously with limited function evaluations due to time or financial reasons.\n\nTo meet these challenges, multi-objective Bayesian optimization (MOBO) (Laumanns and Ocenasek 2002), an extension of single-objective Bayesian Optimization (BO) (MoÀáckus 1975) for expensive multi-objective optimization problems, has emerged as a promising paradigm. BO itself is recognized as an exceedingly effective strategy for global optimization, particularly noted for its success in addressing black-box optimization issues (Snoek, Larochelle, and Adams 2012; Qian, Xiong, and Xue 2020; Huang et al. 2024; Wang et al. 2024; Li et al. 2024a). The core principle of BO involves creating probabilistic surrogate models, typically Gaussian processes (GP), that closely represent the black-box functions. These models are utilized in conjunction with acquisition functions to seek out globally optimal solutions. BO has been widely used in a variety of fields, including hyperparameter tuning (Bergstra et al. 2011), A/B testing (Letham et al. 2019), combinatorial optimization (Zhang et al. 2015), among others. MOBO represents a fusion of Bayesian optimization with multi-objective optimization. A widely adopted MOBO approach is the random scalarization technique, which effectively translates a multi-objective optimization problem into several singleobjective optimization problems. Another noteworthy strategy in MOBO involves the use of sophisticated acquisition functions, such as the expected hypervolume improvement (EHVI) (Couckuyt, Deschrijver, and Dhaene 2014) and predictive entropy search (PESMO) (Hoffman and Ghahramani 2015; Herna¬¥ndez-Lobato et al. 2016). Among them, Pareto set learning (PSL) based methods (e.g. (Lin et al. 2022)), which aim to modeling the Pareto set via machine learning techniques, have shown promising performance.\n\nHowever, effectively capturing and modeling distributions of limited samples is difficult when faced with expensive multi-objective Bayesian optimization problems (EMOPs). Existing PSL algorithms may exhibit considerable instability in expensive scenarios. This instability can lead to substantial deviations between the obtained solution set and the true Pareto set (PS). In other words, the quality of the resulting solution set is highly influenced by the performance of the PSL model, which is largely limited on EMOPs. On the other hand, Diffusion model (DM), inspired by the natural diffusion of gases, has been successfully applied in various domains such as computer vision (Dhariwal and Nichol 2021; Baranchuk et al. 2021; Yu et al. 2022), natural language processing (Austin et al. 2021a), and waveform signal processing (Leng et al. 2022) etc, due to its advantage in distribution coverage, stationary training objective, and effortless scalability. Moreover, the methodology of DM is particularly effective in scenarios with limited sample sizes, as it can efficiently extract substantial information from each individual sample, thereby amplifying the learning impact (Yang et al. 2023). These features of DM show promise for its application in PSL of EMOPs.\n\nIn this paper, we propose a novel Composite Diffusion Model based Pareto Set Learning algorithm named CDMPSL for expensive MOBO, where DM progressively learns the inherent distribution of the high-quality solution set. The major contributions of this paper are summarized as follows:\n\n1) We introduce a composite diffusion model based Pareto set learning method for offspring generation for expensive MOBO, which includes both unconditional and conditional sample generation.\n\n2) We devise a guided sampling process to improve the quality of solutions generated by the diffusion model, resulting in a conditional diffusion model;\n\n3) We introduce a guiding strategy based on information entropy weighting to balance the importance of different objectives of EMOPs. This method ensures that all objectives are appropriately balanced and given due consideration during the optimization process;\n\n4) We have conducted extensive experiments on both synthetic benchmarks and real-world problems, clearly demonstrating that CDM-PSL obtains superior performance compared with various state-of-the-art MOBO algorithms.\n\n# Backgrounds Multi-objective Optimization\n\nA multi-objective optimization problem (MOP) can be defined as follows:\n\nwhere $\\pmb { x } = ( x _ { 1 } , x _ { 2 } , \\dots , x _ { d } )$ represents the decision vector, $f ( \\cdot ) \\colon \\Omega  \\dot { \\Lambda }$ includes $M$ $M \\geq 2 )$ ) objectives, $\\Omega$ symbolizes the non-empty decision space, and $\\Lambda$ is the objective space. An MOP is considered expensive when the evaluation of $\\pmb { f } ( \\pmb { x } )$ involves either time-intensive computations or high-cost experimental procedures.\n\nDefinition 1 (Pareto dominance) Considering two solutions $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ and $\\pmb { y } \\in \\Omega$ , $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ is said to dominate $_ y$ (expressed as $x \\prec y )$ if and only if the following conditions are met: 1) $\\forall i \\in \\{ 1 , 2 , . . . , M \\}$ , $f _ { i } ( { \\pmb x } ) \\leq f _ { i } ( { \\pmb y } ) ; 2 ) \\exists j \\in \\{ 1 , 2 , . . . , M \\}$ , $f _ { j } ( { \\pmb x } ) \\ < \\ f _ { j } ( { \\pmb y } )$ . This definition encapsulates the essential criterion for determining the quality of solutions in terms of meeting multiple objectives simultaneously. (Yu 1974)\n\nDefinition 2 (Pareto optimal) A solution $\\pmb { x } ^ { * } \\in \\Omega$ is Pareto optimal if there exists no other solution $\\pmb { x } \\in \\Omega$ that can dominate it. This implies that within the feasible region $\\Omega , \\pmb { x } ^ { * }$ is considered to be Pareto optimal if no alternative solution offers better outcomes across all objectives without being worse in at least one of them. The collection of all the Pareto optimal solutions and their objective vectors are the Pareto Set (PS) and Pareto Front $( P F )$ respectively.\n\n# Diffusion Models\n\nDiffusion models are a specialized form of probabilistic generative models that operate by learning to reverse a forward process that gradually increases noise in the training data (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020). They have demonstrated remarkable performance on a wide variety of tasks, such as image generation (Austin et al. 2021b; Bao et al. 2022), voice synthesis (Liu et al. 2022), video generation (Ho et al. 2022) and inpainting (Lugmayr et al. 2022). Training a DM involves two processes: the forward diffusion process and the backward denoising process.\n\nForward Process. In the forward phase, Gaussian noise is added to the input data step by step until a pure Gaussian noise is produced, which is a Markovian process. Given an initial data distribution $\\mathbf { x } _ { 0 } \\sim q ( \\mathbf { x } )$ , the noised $x _ { 1 } , x _ { 2 } \\dots , x _ { T }$ can be obtained from the following equation:\n\n$$\n\\begin{array} { r } { q ( x _ { t } | x _ { t - 1 } ) = \\mathcal { N } \\Big ( x _ { t } ; \\sqrt { 1 - \\beta _ { t } } { \\cdot } x _ { t - 1 } , \\beta _ { t } { \\cdot } \\mathbf { I } \\Big ) , \\forall t \\in \\{ 1 , \\ldots , T \\} , } \\end{array}\n$$\n\nwhere $T$ is the number of diffusion steps and the step sizes are controlled by a variance schedule $\\{ \\beta _ { t } ~ \\in ~ ( 0 , \\bar { 1 } ) \\} _ { t = 1 } ^ { t }$ . Moreover, the properties of this recursive formula make it possible to obtain $q ( x _ { t } )$ directly from $x _ { 0 }$ by the following equation:\n\n$$\n\\begin{array} { r } { q ( x _ { t } | x _ { 0 } ) = \\mathcal { N } \\bigg ( x _ { t } ; \\sqrt { \\hat { \\beta } _ { t } } \\cdot x _ { 0 } , \\left( 1 - \\hat { \\beta } _ { t } \\right) \\cdot \\mathbf { I } \\bigg ) , \\forall t \\in \\{ 1 , . . . , T \\} , } \\end{array}\n$$\n\nwhere $\\begin{array} { r } { \\hat { \\beta } _ { t } \\ = \\ \\prod _ { i = 1 } ^ { t } \\alpha _ { i } } \\end{array}$ and $\\alpha _ { t } ~ = ~ 1 - \\beta _ { t }$ . Thus, $\\boldsymbol { x } _ { t }$ can be sampled from $q ( x _ { t } | x _ { 0 } )$ as follows:\n\n$$\nx _ { t } = \\sqrt { \\hat { \\beta } _ { t } } \\cdot x _ { 0 } + \\sqrt { \\left( 1 - \\hat { \\beta } _ { t } \\right) } \\cdot z _ { t } ,\n$$\n\nwhere $\\boldsymbol { z } _ { t } \\sim \\mathcal { N } ( 0 , \\mathbf { I } )$ .\n\nReverse Process. The reverse process recreates the true sample from a Gaussian noise input $x _ { T } \\sim \\mathcal { N } ( 0 , { \\bf { I } } )$ :\n\n$$\nq ( x _ { t - 1 } | x _ { t } ) = \\mathcal { N } ( x _ { t - 1 } ; \\mu ( x _ { t } , t ) , \\Sigma ( x _ { t } , t ) )\n$$\n\nHowever, $q ( x _ { t - 1 } | x _ { t } )$ cannot easily be evaluated due to the reverse process lacking a complete dataset and therefore we need to train a neural network $p _ { \\theta } ( x _ { t - 1 } | x _ { t } ) \\ =$ $\\mathcal { N } ( x _ { t - 1 } ; \\mu _ { \\theta } ( x _ { t } , t ) , \\Sigma _ { \\theta } ( x _ { t } , t ) )$ to approximate these conditional probabilities. Specifically, The model takes in the noisy data $\\boldsymbol { x } _ { t }$ and the corresponding embedding at time step $t$ as input, and is trained to predict the mean $\\mu _ { \\theta } ( x _ { t } , t )$ and the covariance $\\Sigma _ { \\theta } ( x _ { t } , t ) )$ . Based on this, Ho (Ho, Jain, and Abbeel 2020) proposed to fix the covariance $\\Sigma _ { \\theta } ( x _ { t } , t ) )$ to a constant value and reformulating the mean $\\mu _ { \\theta } ( x _ { t } , t )$ as a function dependent on noise, as follows:\n\n$$\n\\mu _ { \\theta } = \\frac { 1 } { \\sqrt { \\alpha _ { t } } } \\cdot \\left( x _ { t } - \\frac { 1 - \\alpha _ { t } } { \\sqrt { 1 - \\hat { \\beta } _ { t } } } \\cdot z _ { \\theta } ( x _ { t } , t ) \\right) .\n$$\n\nThis enables DM to predict the noise of the data rather than directly the mean and covariance.\n\n# Diffusion Model Based Optimization Algorithms\n\nKrishnamoorthy proposed a black-box optimization algorithm named DDOM (Krishnamoorthy, Mashkaria, and Grover 2023), based on the diffusion model. This algorithm converts a single-objective optimization problem into a continuous diffusion process, leveraging the inverse process of the diffusion model to efficiently address complex problems. Subsequently, Yan and Jin‚Äôs EmoDM (Yan and Jin 2024) extended the application of the diffusion model to multi-objective optimization. By learning the noise distribution in the previous evolutionary search task, a set of non-dominated solutions can be generated for the new multiobjective optimization problem without further evolutionary search. Fang‚Äôs DMO (Fang et al. 2024) further demonstrates the diffusion model‚Äôs efficacy by applying it to create a gasoline hybrid scheduling scheme, highlighting its capability in solving practical multi-objective optimization challenges. Building upon these contributions, this paper‚Äôs CDM-PSL advances the field by optimizing the balance between solution convergence and diversity through the integration of conditional and unconditional diffusion models. Moreover, CDM-PSL incorporates gradient information, weighted by information entropy (Zhan et al. 2024), into the process of generating solutions, significantly enhancing convergence performance during the early-stage iterations. The combination of these strategies makes CDM-PSL have competitive performance in solving EMOPs.\n\n# Our Method\n\n# Overview\n\nWe present a composite diffusion model based Pareto set learning method for EMOBO, denoted as CDM-PSL (Algorithm 1 and Figure 1). CDM-PSL contains three components to generate offspring: data extraction, diffusion model training and conditional generation.\n\n# Data Extraction\n\nTo prepare training data for Pareto set learning (Algorithm 2), we introduce the shift-based density estimation (SDE) from (Li, Yang, and Liu 2013) as fitness function, which is mathematically represented as:\n\n$$\nF i t n e s s ( { p } ) = \\operatorname* { m i n } _ { \\substack { q \\in Y _ { k } \\backslash p } } \\sqrt { \\sum _ { i = 1 } ^ { M } ( \\operatorname* { m a x } \\{ 0 , f _ { i } ( q ) - f _ { i } ( p ) \\} ) ^ { 2 } } .\n$$\n\nIn this formula, $\\pmb { p }$ and $\\pmb q$ are solutions within the set $\\scriptstyle \\mathbf { Y } _ { k }$ , and $f _ { i } ( \\pmb { p } )$ indicates the $i$ -th objective value of solution $\\pmb { p }$ . The SDE methodology assesses the quality of samples based on their convergence and diversity characteristics. From $\\scriptstyle { X _ { k } }$ , a total of $T$ candidate solutions $\\begin{array} { r } { \\langle T = \\frac { | X _ { k } | } { 3 } \\rangle } \\end{array}$ with superior SDE values are identified as Pareto optimal samples.\n\n# Diffusion Model Training\n\nDiffusion Process. Given set of samples $X _ { k } ^ { * }$ and a specified step $t$ , the diffusion process involves gradually introducing Gaussian noise $\\epsilon \\sim \\mathcal { N } ( 0 , I )$ to $X _ { k } ^ { * }$ over $t$ steps:\n\n$$\nX _ { k , t } ^ { * } = \\sqrt { 1 - \\beta _ { t } } X _ { k , t - 1 } ^ { * } + \\sqrt { \\beta _ { t } } \\epsilon .\n$$\n\n1: Input: black-box function $\\pmb { f } ( \\pmb { x } )$ , number of iterations $K$ , batch size $B$ , number of initial solutions $N$   \n2: Output: final solutions $\\{ X _ { K } , Y _ { K } \\}$ , Pareto front $\\mathcal { P } _ { f }$   \n3: Initialize $N$ solutions $\\{ X _ { 0 } , Y _ { 0 } \\}$ by LHS   \n4: for $k = 0$ to $K - 1$ do   \n5: Train surrogate model (Gaussian process) $G P _ { i } ^ { k }$ based on $\\{ X _ { k } , Y _ { k } \\}$ for each objective $f _ { i } , i = 1 , \\dots , M$   \n6: $X _ { k } ^ { * } \\gets$ Data extraction based on $\\{ X _ { k } , Y _ { k } \\}$ (Algorithm 2)   \n7: if $F l a g _ { C D M }$ is True then   \n8: $s \\gets$ Pareto set learning based on $\\boldsymbol { X } _ { k } ^ { * }$ by CDMPSL (Algorithm 3)   \n9: else   \n10: $s \\gets$ Generate offspring using Operator   \n112: $X _ { k } ^ { B } \\gets$ Batch selection based on $G P ^ { k }$ , $s$ and $\\{ \\ddot { X _ { k } } , Y _ { k } \\}$   \n13: Evaluate and update $X _ { k + 1 }  X _ { k } \\cup X _ { k } ^ { B }$ , $\\boldsymbol { Y } _ { k + 1 } \\gets$ $\\mathbf { } Y _ { k } \\cup \\mathbf { } f ( \\mathbf { } X _ { k } ^ { B } )$   \n14: Decide whether to invert $F l a g _ { C D M }$ based on HV   \n15: end for   \n16: Approximate the Pareto front $\\mathcal { P } _ { f }$ by non-dominated solutions in $\\scriptstyle \\mathbf { Y } _ { K }$\n\nAlgorithm 2: Data Extraction   \n\n<html><body><table><tr><td></td><td>1:Input:all already-evaluated solutions in the k-th itera- tion {Xk,Yk},number of real samples T 2: Output: solutions after data extraction X*</td></tr></table></body></html>\n\nIn this equation, $X _ { k , t } ^ { * }$ denotes the data at step $t$ , and $\\beta _ { t } \\in$ $[ 1 \\mathrm { e } - 5 , 5 \\mathrm { e } - 2 ]$ represents the noise level at step $t$ . The DM learning process for PSL operates as a Markov chain. This stepwise approach simplifies the learning task compared to direct Pareto set learning and effectively captures the distribution characteristics of optimal samples.\n\nNoise Prediction. The noise prediction phase involves reconstructing the samples $X _ { k , t } ^ { * }$ , which have undergone the diffusion process, back to their original state, $X _ { k } ^ { * }$ . This reconstruction is achieved through a model $\\mathcal { M }$ that predicts the noise added at each step, thereby reversing the diffusion process. This process follows the equation 4:\n\n$$\n\\tilde { X } _ { k , t - 1 } ^ { * } = \\frac { 1 } { \\sqrt { 1 - \\beta _ { t } } } \\left( X _ { k , t } ^ { * } - \\frac { \\beta _ { t } } { \\sqrt { 1 - \\sum _ { s = 1 } ^ { t } \\beta _ { s } } } \\epsilon _ { \\theta } \\left( X _ { k , t } ^ { * } , t \\right) \\right) .\n$$\n\nIn this equation, $\\tilde { X } _ { k , t - 1 } ^ { * }$ represents the data after reconstruction, and $\\theta$ denotes the parameters of the model. The term $\\epsilon _ { \\theta } ( X _ { k , t } ^ { * } , t )$ is the predicted noise by model $\\mathcal { M }$ at step $t$ .\n\n![](images/13efacf72dc758a4a1528879621ff79ca762da59a4da170913c7416b591f3f25.jpg)  \nFigure 1: (left) The framework of CDM-PSL. (right) Diffusion Model Training (DMT), Conditional Generation (CG), and Unconditional Generation (UG). DMT involves learning from the selected samples through multiple steps; CG is designed to create high-quality samples with an optimized distribution; UG is used to generate diverse samples with high efficiency.\n\n# Algorithm 3: Composite Diffusion Model based Generation\n\n1: Input: trained model $\\mathcal { M }$ 2: Parameter: steps $T$ , number of generation $N _ { 1 }$ , $N _ { 2 }$ 3: Output: solutions generated by CDM $s$ 4: $\\pmb { S }  \\emptyset$ 5: for $i = 1$ to $N _ { 1 }$ do 6: for $t = T$ to 1 do 7: $z \\sim \\mathcal { N } ( \\mathbf { 0 } , \\mathbf { I } )$ 8: $\\hat { g } \\_ $ Calculate weighted gradient by entropy weight method and surrogate model 9: $\\begin{array} { r } { x _ { t - 1 }  \\frac { 1 } { \\sqrt { \\alpha _ { t } } } ( x _ { t } - \\frac { 1 - \\alpha _ { t } ^ {  } } { \\sqrt { 1 - \\bar { \\alpha } _ { t } } } \\epsilon _ { \\theta } ( x _ { t } , t ) ) + \\sigma _ { t } ^ { 2 } \\hat { g } + \\sigma _ { t } z } \\end{array}$ 10: end for 11: $S \\gets S \\cup \\{ x _ { 0 } \\}$ 12: end for 13: for $i = 1$ to $N _ { 2 }$ do 14: for $t = T$ to 1 do 15: $\\begin{array} { r l } & { z \\sim \\mathcal { N } ( \\mathbf { 0 } , \\mathbf { I } ) } \\\\ & { x _ { t - 1 }  \\frac { 1 } { \\sqrt { \\alpha _ { t } } } ( x _ { t } - \\frac { 1 - \\alpha _ { t } } { \\sqrt { 1 - \\bar { \\alpha } _ { t } } } \\epsilon _ { \\theta } ( x _ { t } , t ) ) + \\sigma _ { t } z } \\end{array}$ 16: 17: end for 18: S ‚Üê S ‚à™ {x0} 19: end for\n\nThe loss function $\\mathcal { L }$ for model $\\mathcal { M }$ is defined as:\n\n$$\n\\mathcal { L } = \\frac { 1 } { \\mathcal { H } } \\sum _ { i = 1 } ^ { \\mathcal { H } } \\left( \\epsilon - \\epsilon _ { \\theta } \\left( x _ { k , i , t } ^ { * } , t \\right) \\right) .\n$$\n\nThis equation takes $\\mathcal { H }$ into account, the total number of optimal samples, where each $\\boldsymbol { x } _ { k , i , t } ^ { * }$ is an instance from $X _ { k , t } ^ { * }$ , and $\\epsilon _ { \\theta } ( X _ { k , t } ^ { * } , t )$ represents the predicted noise. The training of $\\mathcal { M }$ , using Equation 5, aims to minimize the loss $\\mathcal { L }$ , thereby enhancing the accuracy of noise prediction.\n\nThe entire DM training process, comprising both the diffusion process and noise prediction, plays a crucial role in effective Pareto Set Learning for EMOPs. This method presents a novel approach to learning high-quality solutions, striking a balancing between exploration and exploitation in the search space. Also, it might be interesting to study hyperbolic search space in the future (Li et al. 2024b).\n\n# Conditional Generation\n\nConditional Generation (CG) module includes two components: guided denoising process and weighted gradient. The CG module of CDM-PSL adaptively adjusts the weights of different objectives at each step of denoising based on the current estimated objective values. This is in contrast to using pre-defined hyperparameters or average weights determined by human experience.\n\nGuided Denoising Process. Given step $t$ and sample $\\boldsymbol { x } _ { t }$ , the guided denoising process can be implemented as\n\n$$\nX _ { t - 1 } = \\frac { 1 } { \\sqrt { \\alpha _ { t } } } ( X _ { t } - \\frac { 1 - \\alpha _ { t } } { \\sqrt { 1 - \\bar { \\alpha } _ { t } } } \\epsilon _ { \\theta } ( X _ { t } , t ) ) + \\sigma _ { t } ^ { 2 } \\hat { g } + \\sigma _ { t } z ,\n$$\n\nwhere $\\alpha _ { t }$ represents $1 - \\beta _ { t }$ , $\\epsilon _ { \\theta } ( X _ { t } , t )$ is the predicted noise by the trained DM, $\\sigma _ { t }$ is the standard deviation of the $t$ -th step, and $\\hat { g }$ denotes the weighted gradients for guiding the denoising process.\n\nTo procure the gradients essential for guiding the model in sample generation, we establish separate Gaussian Process (GP) models for each objective, as proposed by Balandat (Balandat et al. 2020). These models are utilized to compute the objective values for all generated samples, thus obtaining gradients $g$ for each objective, facilitating the realization of conditional generation.\n\nWeighted Gradients. Employing weighted gradients is essential to address EMOPs and it requires the determination of appropriate weights for each objective. Therefore, we use a weighting methodology grounded in information entropy, facilitating the derivation of these weighted gradients $\\hat { g }$ .\n\nIn order to obtain weights based on information entropy, it is necessary to first normalize the objective values. Let $y _ { i j }$ represent the $j$ -th objective value of the $i$ -th individual. The normalized objective value $\\tilde { y } _ { i j }$ is calculated by min-max normalization. Subsequent to this, for each objective $j ( j =$ $1 , 2 , \\ldots , M )$ , the probability matrix $P _ { i j }$ is calculated using Equation 7, where $k = 1 , 2 , \\ldots , N$ and $N$ represents the number of individuals in the population.\n\n$$\nP _ { i j } = \\frac { \\tilde { y } _ { i j } } { \\sum _ { k = 1 } ^ { N } \\tilde { y } _ { k j } } .\n$$\n\nSubsequently, for each objective $j ( j = 1 , 2 , \\dots , M )$ , the information entropy $E _ { j }$ is computed. The method of calculation is as outlined in Equation 8:\n\n$$\nE _ { j } = - \\frac { 1 } { \\ln { ( N ) } } \\sum _ { i = 1 } ^ { N } \\left( P _ { i j } \\times \\ln ( P _ { i j } + \\eta ) \\right) .\n$$\n\nHerein, $i = 1 , 2 , \\dots , N$ and $j = 1 , 2 , \\dots , M$ , where $N$ signifies the total number of samples, and $M$ represents the quantity of objectives. To avoid the occurrence of $\\ln ( 0 )$ , a small positive number, $\\eta$ , is introduced. The computation of information entropy relies on Shannon entropy (Shannon 1948), expressed as $\\begin{array} { r } { \\dot { H } ( X ) = - \\sum \\left( P ( x ) \\times \\mathrm { \\bar { l o g } } \\left( P ( x ) \\right) \\right) } \\end{array}$ . Additionally, the coefficient $\\frac { 1 } { \\ln ( N ) }$ isPemployed to guarantee that the values of information entropy are confined within the range of 0 to 1.\n\nFinally, for each objective $j = 1 , 2 , \\dots , M$ , the weight $W _ { j }$ is computed using Equation 9, where $k = 1 , 2 , \\dots , M$ , and $M$ denotes the total number of objectives.\n\n$$\nW _ { j } = \\frac { 1 - E _ { j } } { \\sum _ { k = 1 } ^ { M } ( 1 - E _ { k } ) } .\n$$\n\nUsing the calculated weights $W$ , we can derive the entropy weighted gradients (EWG) $\\hat { g }$ as:\n\n$$\n\\hat { g } = \\sum _ { j } W _ { j } \\left( \\nabla F _ { j } - \\mathrm { c o e f \\_ l c b } \\cdot \\nabla S _ { j } \\right) ,\n$$\n\nwhere $W _ { j }$ is the entropy-based weight for the $j$ -th objective; $\\nabla F _ { j }$ is the gradient of the mean $F _ { j }$ of the surrogate model (Gaussian process) for the $j$ -th objective; $\\nabla { S _ { j } }$ is the gradient of the standard deviation $S _ { j }$ of the surrogate model for the $j$ -th objective; coef lcb is a coefficient (default setting is\n\n0.1) used to balance the mean and standard deviation in the Lower Confidence Bound (LCB) strategy.\n\nThe entropy weighting method, serving as an adaptive weight allocation approach, assigns weights based on the information entropy of objectives. This method reduces subjectivity in weighting gradients for different objective values, and ensures that the algorithm places more emphasis on objectives with rich information content, thereby achieving better performance on EMOPs.\n\n# Selection Strategy\n\nBatch Selection. After obtaining the solutions sampled by the CDM-PSL, we employ the batch selection strategy of PSL-MOBO (Lin et al. 2022) to select a small subset $X _ { k } ^ { B } \\ = \\ \\{ x _ { b } | b \\ = \\ 1 , \\ldots , B \\}$ . Specifically, this strategy use the Hypervolume (HV) indicator (Zitzler and Thiele 1999) as the selection criteria, which is defined as follows:\n\n$$\n\\mathbf { H } \\mathbf { V } ( S ) = { \\boldsymbol { \\Lambda } } ( \\{ q \\in R ^ { d } | \\exists p \\in S : p \\leq q \\mathrm { ~ a n d ~ } q \\leq r \\} ) ,\n$$\n\nwhere $S$ denotes a solution set, $r$ is a reference vector, and $\\Lambda ( \\cdot )$ denotes the Lebesgue measure.\n\nOperator Selection. At the end of each iteration round, we have devised operator switching strategy based on the growth rate of the HV indicator. In algorithm 1, $F l a g _ { C D M }$ is a flag used to determine if CDM is currently being used to generate offspring. After passing through an iteration window (e.g. three evaluation rounds), if the HV indicator‚Äôs growth rate falls below a predefined threshold (default setting is $5 \\%$ ), we switch the offspring generation operator to another one (e.g. from CDM-PSL to Genetic Algorithm (GA)). This is to prevent the algorithm from becoming trapped in local optima specific to the current operator.\n\n# Experimental Study\n\n# Experimental Settings\n\nInstances and Baselines: To comprehensively validate the performance of CDM-PSL, experiments were conducted on 9 benchmark problems (2- and 3-objective ZDT1-3 (Zitzler, Deb, and Thiele 2000) and DTLZ2-7 (Deb et al. 2005)) and 7 real-world problems (Tanabe and Ishibuchi 2020). Moreover, we have compared CDM-PSL with 9 state-of-theart and classical algorithms, including NSGA-II (Deb et al. 2002), MOEA/D-EGO (Zhang et al. 2009), TSEMO (Bradford, Schweidtmann, and Lapkin 2018), USeMO-EI (Belakaria et al. 2020), DGEMO (Konakovic Lukovic, Tian, and Matusik 2020), PSL-MOBO (Lin et al. 2022), qNparEGO (Knowles 2006), qEHVI (Daulton, Balandat, and Bakshy 2020) and qNEHVI (Daulton, Balandat, and Bakshy 2021).\n\nParameter Settings: For fair comparison, the population size $N$ was initialized to 100 for all the compared algorithms. Bayesian optimization algorithms were executed for 20 batches, each with a batch size of 5, across all algorithms. Each method was randomly run 10 times. For CDMPSL, the hyperparameter $t$ was set to 25, the number of CG $N _ { 1 }$ was 10 and number of UG $N _ { 2 }$ was 100, the batch size $m$ was 1024, the learning rate $\\gamma$ was 0.001, with training spanning 4000 epochs. The configurations for other methods were aligned with those in their original publications.\n\nZDT1 ZDT2 ZDT3 DTLZ2 6 6 20 5 5 5 18 4 4 4 16 3 3 3 0 25 50 75 100 0 255075 100 0 25 5075 100 0 2550 75 100 1e10 DTLZ3 DTLZ4 DTLZ5 1e3 DTLZ6 5 1.2 15 16 1.1 1.0 10 14 4 3 0.9 5 12 2 0 255075100 0 2550 75 100 0 25 5075 100 0 2550 75 100 DTLZ7 RE1 6.751e1 RE2 1e2 RE3 32f 8.8- 15 30 6.70 8.6 28 10 8.4 26 6.65- 8.2 0 25 50 75 100 0 25 50 75 100 0 25 50 75 100 0 25 50 75 100 1 1e2 RE4 0.7 RE5 1.62 1e6 RE6 8.821e12 8.80 RE7 0.6 1.60 8.78 1.58 8.76 0.5 1.56 8.74 0 2550 75100 0 255075100 0 255075100 0 2550 75100 Evaluations Evaluations Evaluations Evaluations NSGA-II TSEMO DGEMO qNParEGO qNEHVI MOEA/D-EGO USeMO-EI PSL-MOBO qEHVI CDM-PSL (Ours)\n\n![](images/7052dd2d1ae8b0de53fd655c49ae7354830f800dc7d672ec04f42785d76bba04.jpg)  \nFigure 2: The HV results of 10 algorithms on synthetic test functions and real-world problems $\\neg d = 2 0 \\$ ). The horizontal axi denotes the FEs after the initialization phase, similarly hereinafter.   \nFigure 3: Ablation study results for different components.\n\nGrD 1 √ó108 Gr Approximate PF(Ours) Approximate PF(Ours) 20005000 40   \n0 0.2 First Objecive 0.8 1 Second 5 Objective 00 2 4 First Objective 1012 14 Second Objective 10000 50000 0 1 First Objective 20 30\n\nEvaluation Metrics: The hypervolume (HV) in Equation 11, was employed to assess the quality of the solutions obtained. Higher HV values indicate better performance.\n\n# Experimental Results\n\nIn this section, we present the overall performance of CDMPSL and the baselines, along with the results of ablation studies. Additionally, we provide a detailed analysis of the superiority of using DM and entropy weighting method in Appendix C.1 and C.2. Further details, including log HV difference, numerical results, running time, parameter sensitivity analysis and additional experiments and discussions are shown in Appendix C.3 to C.12.\n\nOverall Performance. We conducted a series of experiments on a variety of widely recognized synthetic multiobjective benchmarks, including ZDT1-3 (Zitzler, Deb, and Thiele 2000) and DTLZ2-7 (Deb et al. 2005). The problems selected for the experiments featured 2 and 3 objectives, with the number of decision variables set at 20. We particularly highlight the results for a specific instance where $d \\ = \\ 2 0$ , and further details are available in the supplementary materials. Additionally, the overall performance of CDM-PSL was fully assessed using 7 real-world problems.\n\nFigure 2 shows a comparison of HV indicator relative to function evaluations (FE). CDM-PSL demonstrates outstanding performance across most synthetic benchmarks, excelling in both convergence speed and final values. Additionally, CDM-PSL exhibits ideal performance in real-world problems. These findings affirm the effectiveness and superiority of the proposed CDM-PSL.\n\nAblation Study. As shown in Figure 3, CDM-PSL w/o Weight uses mean weighted gradients to guide the sampling process instead of entropy weighted gradients (EWG). CDM-PSL demonstrates superior convergence performance compared to CDM-PSL w/o Weight on all the six tested problems and attains better final values in ZDT3, DTLZ3 and DLTZ6. This indicates that EWG can gauge the significance of objectives more accurately, thereby offering more effective guidance for the sampling process.\n\nCDM-PSL w/o Condition refers to the variant which omits the conditional generation component. Similarly, CDM-PSL consistently and significantly outperforms\n\nCDM-PSL w/o Condition across all the tested problems. This clearly validates the critical role and effectiveness of the conditional generation component in the CDM-PSL.\n\nCDM-PSL w/o Switch denotes a CDM-PSL variant without the switching strategy. The results of the ablation experiments on ZDT3 and DTLZ3 demonstrate that the inclusion of this strategy is favor of enhancing convergence speed.\n\nCDM-PSL w/o DM refers to the variant that employs GA instead of the DM-based PSL model as its operator, utilizing Simulated Binary Crossover (SBX) to generate new solutions. This variant‚Äôs performance is markedly inferior compared to that of the default CDM-PSL. This disparity underscores the effectiveness of DM based Pareto set learning.\n\nFigure 4 displays the solutions set obtained by CDM-PSL and by MOBO without CDM-PSL, based on the posterior mean. Clearly, CDM-PSL surpass MOBO without CDMPSL on both synthetic benchmarks and real-world problems. For instance, MOBO without CDM-PSL is difficult to approach the true PF with given FE budget, yet CDM-PSL can effectively capture nearly all characteristics of the PF on ZDT1. On 3-objective DLTZ6, CDM-PSL can approximate the true PF faster in a limited number of FEs. Additionally, our methodology demonstrates commendable exploitation capabilities on complex problems, such as the rocket injector design (RE7) (Vaidyanathan et al. 2003), which is characterized by a complex PF where the Pareto optimal solutions being distributed across multiple regions. Please see Appendix C.9 for more PFs obtained by CDM-PSL.\n\n# Conclusion and Future work\n\nIn this paper, we introduce a composite diffusion model based Pareto set learning method, named CDM-PSL, for addressing EMOPs. CDM-PSL uses both unconditional and conditional diffusion model for generating high-quality samples. Besides, the quality of the generated solutions is significantly enhanced by employing entropy weighted gradients to guide the sampling process. Extensive experimental evaluations on 9 benchmark problems and 7 real-world problems verify the efficiency of CDM-PSL. For future research, there is an intention to incorporate Monte Carlo tree or dimensionality reduction approaches (Song et al. 2022; Qian et al. 2025; Qian and $\\mathrm { Y u } ~ 2 0 1 7 )$ into CDM-PSL to address higher-dimensional problems.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÈíàÂØπÊòÇË¥µÂ§öÁõÆÊ†áË¥ùÂè∂ÊñØ‰ºòÂåñÔºàEMOPsÔºâ‰∏≠Â∏ïÁ¥ØÊâòÊúÄ‰ºòËß£ÂàÜÂ∏ÉÂª∫Ê®°Âõ∞ÈöæÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÂ∏ïÁ¥ØÊâòÈõÜÂ≠¶‰π†ÔºàPSLÔºâÁÆóÊ≥ïÂú®ÊúâÈôêÂáΩÊï∞ËØÑ‰º∞‰∏ãË°®Áé∞‰∏çÁ®≥ÂÆöÔºåÂØºËá¥Ëß£ÈõÜ‰∏éÁúüÂÆûÂ∏ïÁ¥ØÊâòÈõÜÔºàPSÔºâÂ≠òÂú®ÊòæËëóÂÅèÂ∑Æ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®Â§ö‰∏™È¢ÜÂüüÔºàÂ¶ÇÂ§©Á∫øËÆæËÆ°„ÄÅÈáëËûç‰∫ëÊúçÂä°„ÄÅ‰∏¥Â∫äËçØÁâ©ËØïÈ™åÔºâÂÖ∑ÊúâÈáçË¶ÅÂ∫îÁî®‰ª∑ÂÄºÔºåÂõ†‰∏∫Âú®Ëøô‰∫õÂú∫ÊôØ‰∏≠ÔºåÂ§öÁõÆÊ†á‰ºòÂåñÈÄöÂ∏∏‰º¥ÈöèÁùÄÈ´òÊòÇÁöÑËÆ°ÁÆóÊàñÂÆûÈ™åÊàêÊú¨„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂ§çÂêàÊâ©Êï£Ê®°ÂûãÁöÑÂ∏ïÁ¥ØÊâòÈõÜÂ≠¶‰π†ÁÆóÊ≥ïÔºàCDM-PSLÔºâÔºåÈÄöËøáÊó†Êù°‰ª∂‰∏éÊù°‰ª∂Êâ©Êï£Ê®°ÂûãÈ´òÊïàÁîüÊàêÈ´òË¥®ÈáèÊ†∑Êú¨ÔºåÂπ∂ÁªìÂêà‰ø°ÊÅØÁÜµÂä†ÊùÉÁ≠ñÁï•Âπ≥Ë°°Â§öÁõÆÊ†á‰ºòÂåñËøáÁ®ã„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫Â§çÂêàÊâ©Êï£Ê®°ÂûãÔºàÊó†Êù°‰ª∂+Êù°‰ª∂ÁîüÊàêÔºâÔºåÂú®ÂêàÊàêÂíåÁúüÂÆûÈóÆÈ¢ò‰∏äÊØîÁé∞ÊúâMOBOÁÆóÊ≥ïÊÄßËÉΩÊõ¥‰ºòÔºàHVÊåáÊ†áÊòæËëóÊèêÂçáÔºâ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ËÆæËÆ°Âü∫‰∫é‰ø°ÊÅØÁÜµÁöÑÂä†ÊùÉÊ¢ØÂ∫¶Á≠ñÁï•ÔºåÂä®ÊÄÅÂπ≥Ë°°Â§öÁõÆÊ†áÈáçË¶ÅÊÄßÔºàÂÆûÈ™åÊòæÁ§∫Âú®ZDT3/DTLZ6Á≠â‰ªªÂä°‰∏äÊî∂ÊïõÈÄüÂ∫¶ÊèêÂçá>20%Ôºâ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** ÂºïÂÖ•ÁÆóÂ≠êÂàáÊç¢Êú∫Âà∂ÔºåÂΩìHVÂ¢ûÈïø‰Ωé‰∫éÈòàÂÄºÔºàÈªòËÆ§5%ÔºâÊó∂Ëá™Âä®ÂàáÊç¢‰ºòÂåñÂô®ÔºåÈÅøÂÖçÂ±ÄÈÉ®ÊúÄ‰ºòÔºàÂú®DTLZ3Á≠âÂ§çÊùÇÈóÆÈ¢ò‰∏äÊïàÊûúÊòæËëóÔºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   Âà©Áî®Êâ©Êï£Ê®°ÂûãÔºàDMÔºâÂú®ÊúâÈôêÊ†∑Êú¨‰∏ãÈ´òÊïàÂ≠¶‰π†ÂàÜÂ∏ÉÁöÑÁâπÊÄßÔºåÈÄöËøáÈÄÜÂêëÂéªÂô™ËøáÁ®ãÁîüÊàêÂ∏ïÁ¥ØÊâòËß£ÈõÜ„ÄÇÂÖ∂‰ºòÂäøÂú®‰∫éÔºö1ÔºâË¶ÜÁõñÂπøÂàÜÂ∏ÉÔºõ2ÔºâËÆ≠ÁªÉÁõÆÊ†áÁ®≥ÂÆöÔºõ3ÔºâÊ†∑Êú¨Âà©Áî®ÁéáÈ´ò„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂÖàÂâçÂ∑•‰ΩúÂ±ÄÈôêÔºö** ‰º†ÁªüPSLÊ®°ÂûãÔºàÂ¶ÇPSL-MOBOÔºâÂú®EMOPs‰∏≠Ê†∑Êú¨ÊïàÁéá‰ΩéÔºåËß£ÈõÜË¥®ÈáèÊ≥¢Âä®Â§ß„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** \\n>     1. **Â§çÂêàÁîüÊàêÊû∂ÊûÑÔºö** Êó†Êù°‰ª∂Êâ©Êï£ÔºàUGÔºâ‰øùËØÅÂ§öÊ†∑ÊÄßÔºåÊù°‰ª∂Êâ©Êï£ÔºàCGÔºâÈÄöËøáÁÜµÂä†ÊùÉÊ¢ØÂ∫¶ÂºïÂØºÈ´òË¥®ÈáèËß£ÁîüÊàêÔºàÂÖ¨Âºè10Ôºâ„ÄÇ\\n>     2. **Âä®ÊÄÅÊùÉÈáçÁ≠ñÁï•Ôºö** Âü∫‰∫é‰ø°ÊÅØÁÜµÔºàÂÖ¨Âºè8-9ÔºâËá™Âä®Ë∞ÉÊï¥ÁõÆÊ†áÊùÉÈáçÔºåÈÅøÂÖç‰∫∫Â∑•ËÆæÂÆöÂÅèÂ∑Æ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1. **Êï∞ÊçÆÊèêÂèñÔºö** ‰ΩøÁî®Âü∫‰∫é‰ΩçÁßªÁöÑÂØÜÂ∫¶‰º∞ËÆ°ÔºàSDEÔºåÂÖ¨Âºè6Ôºâ‰ªéÂΩìÂâçËß£ÈõÜ‰∏≠Á≠õÈÄâÈ´òË¥®ÈáèÊ†∑Êú¨„ÄÇ\\n> 2. **Êâ©Êï£ËÆ≠ÁªÉÔºö** ÈÄöËøáÂâçÂêëÂô™Â£∞Ê∑ªÂä†ÔºàÂÖ¨Âºè3ÔºâÂíåÂèçÂêëÂô™Â£∞È¢ÑÊµãÔºàÂÖ¨Âºè5ÔºâÂ≠¶‰π†Â∏ïÁ¥ØÊâòÈõÜÂàÜÂ∏É„ÄÇ\\n> 3. **Êù°‰ª∂ÁîüÊàêÔºö** Âú®ÂéªÂô™Ê≠•È™§ÔºàÂÖ¨Âºè4Ôºâ‰∏≠Ê≥®ÂÖ•ÁÜµÂä†ÊùÉÊ¢ØÂ∫¶ÔºàÂÖ¨Âºè10ÔºâÔºå‰ºòÂåñÁõÆÊ†áÂπ≥Ë°°„ÄÇ\\n> 4. **ÊâπÈáèÈÄâÊã©Ôºö** Âü∫‰∫éË∂Ö‰ΩìÁßØÔºàHVÔºåÂÖ¨Âºè11ÔºâÊåáÊ†áÈÄâÊã©ÊúÄ‰ºòÂÄôÈÄâËß£„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   NSGA-II„ÄÅMOEA/D-EGO„ÄÅTSEMO„ÄÅUSeMO-EI„ÄÅDGEMO„ÄÅPSL-MOBO„ÄÅqNparEGO„ÄÅqEHVI„ÄÅqNEHVI\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Ë∂Ö‰ΩìÁßØÔºàHVÔºâÊåáÊ†á‰∏äÔºö** CDM-PSLÂú®ZDT1-3ÂíåDTLZ2-7‰∏äÂπ≥ÂùáHVÂÄºËææ`1.2e10`ÔºåÊòæËëó‰ºò‰∫éPSL-MOBOÔºà`1.0e10`ÔºâÂíåqNEHVIÔºà`1.1e10`ÔºâÔºåÊúÄÈ´òÊèêÂçá20%„ÄÇ\\n> *   **Âú®Êî∂ÊïõÈÄüÂ∫¶‰∏äÔºö** Âú®ÁÅ´ÁÆ≠Âñ∑Â∞ÑÂô®ËÆæËÆ°ÔºàRE7Ôºâ‰ªªÂä°‰∏≠ÔºåCDM-PSL‰ªÖÈúÄ50Ê¨°ËØÑ‰º∞Âç≥ËææÂà∞Âü∫Á∫øÊ®°Âûã100Ê¨°ËØÑ‰º∞ÁöÑHVÊ∞¥Âπ≥Ôºà`8.76 vs 8.74`Ôºâ„ÄÇ\\n> *   **Âú®Á®≥ÂÆöÊÄß‰∏äÔºö** ÈÄöËøáÁÆóÂ≠êÂàáÊç¢Á≠ñÁï•ÔºåCDM-PSLÂú®DTLZ3‰∏äÁöÑÊ†áÂáÜÂ∑ÆËæÉÂõ∫ÂÆöÁÆóÂ≠êÊñπÊ≥ïÈôç‰Ωé35%„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Â§öÁõÆÊ†áË¥ùÂè∂ÊñØ‰ºòÂåñ (Multi-Objective Bayesian Optimization, MOBO)\\n*   Â∏ïÁ¥ØÊâòÈõÜÂ≠¶‰π† (Pareto Set Learning, PSL)\\n*   Êâ©Êï£Ê®°Âûã (Diffusion Model, DM)\\n*   ‰ø°ÊÅØÁÜµÂä†ÊùÉ (Information Entropy Weighting, N/A)\\n*   ÊòÇË¥µ‰ºòÂåñÈóÆÈ¢ò (Expensive Multi-Objective Problems, EMOPs)\\n*   Êù°‰ª∂ÁîüÊàê (Conditional Generation, CG)\\n*   Ë∂Ö‰ΩìÁßØÊåáÊ†á (Hypervolume Indicator, HV)\"\n}\n```"
}