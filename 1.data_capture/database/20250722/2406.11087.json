{
    "link": "https://arxiv.org/abs/2406.11087",
    "pdf_link": "https://arxiv.org/pdf/2406.11087",
    "title": "DP-MemArc: Differential Privacy Transfer Learning for Memory Efficient Language Models",
    "authors": [
        "Yanming Liu",
        "Xinyue Peng",
        "Jiannan Cao",
        "Xiaolan Ke",
        "Yuwei Zhang",
        "Chen Ma",
        "Songhang Deng",
        "Mengchen Fu",
        "Xuhong Zhang",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin",
        "Tianyu Du"
    ],
    "publication_date": "2024-06-16",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "paper_content": "# DP-MemArc: Differential Privacy Transfer Learning for Memory Efficient Language Models\n\nYanming ${ { \\bf { L i u } } ^ { 1 } }$ , Xinyue Peng2, Yuwei Zhang8, Xiaolan $\\mathbf { K e } ^ { 3 }$ , Songhang Deng4, Jiannan $\\mathbf { C a o } ^ { 5 }$ , Chen $\\mathbf { M } \\mathbf { a } ^ { 6 }$ , Mengchen $\\mathbf { F } \\mathbf { u } ^ { 7 }$ , Tianyu $\\mathbf { D } \\mathbf { u } ^ { 1 * }$ , Sheng Cheng1, Xun Wang1, Jianwei $\\mathbf { Y _ { i n } } ^ { 1 }$ , Xuhong Zhang1\n\n1Zhejiang University 2Southeast University 3Harvard University 4University of California, Los Angeles 5Massachusetts Institute of Technology 6Renmin University of China 7The University of Tokyo 8Tongji University oceann24, zhangxuhong, zjradty, zjuyjw @zju.edu.cn, xinyuepeng $@$ seu.edu.cn, jiannan@mit.edu, songh00@ucla.edu\n\n# Abstract\n\nLarge language models have repeatedly shown outstanding performance across diverse applications. However, deploying these models can inadvertently risk user privacy. The significant memory demands during training pose a major challenge in terms of resource consumption. This substantial size places a heavy load on memory resources, raising considerable practical concerns. In this paper, we introduce DP-MemArc, a novel training framework aimed at reducing the memory costs of large language models while emphasizing the protection of user data privacy. DP-MemArc incorporates side network or reversible network designs to support a variety of differential privacy memory-efficient fine-tuning schemes. Our approach not only achieves about 2.5 times in memory optimization but also ensures robust privacy protection, keeping user data secure and confidential. Extensive experiments have demonstrated that DP-MemArc effectively provides differential privacy-efficient fine-tuning across different task scenarios.\n\n# Introduction\n\nLarge language models (LLMs) (Radford et al. 2019; Hoffmann et al. 2022; Chowdhery et al. 2023; Touvron et al. 2023) have already demonstrated their capabilities across various domains, excelling in a wide range of generation and comprehension tasks (Bang et al. 2023; Robinson, Rytting, and Wingate 2022; Li, Zhang, and Zhao 2022). However, complete training of LLMs demands significant computational resources and time, making it inconvenient to adapt the model in downstream tasks (Liu et al. 2022a). There exist several methods that offer solutions for parameterefficient fine-tuning (Dettmers et al. 2024; Houlsby et al. 2019; Hu et al. 2021). These approaches achieve highly effective downstream task fine-tuning results by adjusting only a small number of parameters. The goal of such methods is to enable LLMs to adapt to small-scale features in a relatively small dataset, thereby accomplishing specific downstream tasks. Unfortunately, for LLMs, we often encounter situations where the available dataset is small and proprietary, raising concerns about privacy (Bu et al. 2024; Yu et al. 2021; Finlayson, Swayamdipta, and Ren 2024). Additionally, the training of LLMs requires substantial training memory (Wang et al. 2023), making it challenging to train on parameter-efficient fine-tuning.\n\nA recent line of work that focuses on fine-tuning large models using differential privacy (DP) solutions, including both full parameter fine-tuning and parameter efficient finetuning approaches (Duan et al. 2024; Bu et al. 2024; Yu et al. 2021). These solutions employ a method called Differential Privacy Stochastic Gradient Descent (DP-SGD) (Yu et al. 2019). The training data is protected by implementing gradient clipping and adding Gaussian noise during each iteration to ensure privacy. Compared to traditional fine-tuning approaches, DP allows for downstream task handling with only a small loss in accuracy while maintaining a theoretical private guarantee (Yu et al. 2021). These approaches exhibit good performance across a variety of tasks and settings. However, these methods still have issues with training memory. In previous research, differential privacy has imposed larger computational and storage overheads, making training such large models challenging in resource-constrained scenarios. Additionally, existing efficient parameter finetuning with differential privacy schemes has only achieved marginal reductions in memory overhead, with insufficient optimization efficiency in memory resources (Li et al. 2022; Ke et al. 2024). As models continue to grow, the demand for both memory efficiency and privacy in such scenarios also increases.\n\nTo address this issue, we propose a solution called Differential Private Memory efficient transfer Architecture (DP-MemArc), a framework for training in scenarios that involve both privacy-protection and memory efficient transfer learning. In our framework, we explore two efficient methods for parameter-efficient fine-tuning, DP-MemArcside and $\\mathrm { D P \\mathrm { - M e m A r c } _ { \\mathrm { r e v } } }$ , which save memory usage from different perspectives. In this setup, our approach not only achieves competitive performance but also significantly reduces training memory usage. Experiments on different datasets and models have thoroughly demonstrated the effectiveness and potential of our approach. Therefore, our work effectively addresses the issue of insufficient memory in private fine-tuning for language models, while also providing alternative privacy fine-tuning solutions.\n\n<html><body><table><tr><td>Module</td><td></td><td>Forward pass Back-propagation</td><td>Book-Keeping|instantiation|weighted grad</td><td></td><td>Ghost norm in|Opacus grad Opacus sum of</td></tr><tr><td>Time complexity</td><td>2BTpd</td><td>4BTpd</td><td>2BT²(p+d) |</td><td>2BTpd</td><td>2Bpd</td></tr><tr><td>Space complexity|</td><td>pd + BTd</td><td>BT(p+d)+ pd |</td><td>2BT2</td><td>Bpd</td><td>0</td></tr></table></body></html>\n\nTable 1: The time and space complexity of the training process of a model under a single-layer MLP. While opacus is a codebase for vanilla differential private method implementation.\n\nIn summary, our contributions in this paper are as follows:\n\n• We propose a framework called DP-MemArc, which enables efficient fine-tuning of language models with lower training memory in differential privacy fine-tuning. This framework contains two memory optimization methods, reducing the memory requirements for privacy training of language models.\n\n• We conduct a systematic analysis of the relationship between training memory requirements and network architecture. We elucidate the characteristics of finetuning memory cost under different network architectures, demonstrating favorable downstream task performance in differential privacy.\n\n• We evaluate our DP-MemArc framework on multiple datasets and models. The results show promising performance across various dimensions, with a substantial improvement in training memory.\n\n# Preliminaries\n\n# Memory Footprint on Language Model\n\nWe consider a $N$ multilayer perception: $\\begin{array} { r l } { \\pmb { x } _ { N } } & { { } = } \\end{array}$ $f _ { N } ( f _ { N - 1 } ( . . . f _ { 2 } ( f _ { 1 } ( { \\pmb x } _ { 0 } ) ) ) )$ , where $x _ { 0 }$ as the initial PLM input, the $i ^ { t h }$ layer $\\pmb { x } _ { i } = f _ { i } ( \\pmb { x } _ { i - 1 } ) = \\sigma _ { i } ( \\mathbf { W } _ { i } \\pmb { x } _ { i - 1 } )$ consists of a weight matrix $\\mathbf { W } _ { i }$ and a nonelinear function $\\pmb { \\sigma } _ { i }$ . For the format simplicity, the bias term is ignored. We denote $\\pmb { h } _ { i } = \\mathbf { W } _ { i } \\pmb { x } _ { i - 1 }$ as the hidden states for the pre-activation of $i ^ { t h }$ layer. In backpropagation with loss $\\mathcal { L }$ , the gradient of $W _ { i }$ is calculated with respect to $x _ { i }$ using the chain rule:\n\n$$\n\\frac { \\partial \\mathcal { L } } { \\partial \\mathbf { W } _ { i } } = \\frac { \\partial \\mathcal { L } } { \\partial \\pmb { x } _ { N } } ( \\prod _ { j = i + 1 } ^ { N } \\frac { \\partial \\pmb { x } _ { j } } { \\partial \\pmb { h } _ { j } } \\frac { \\partial \\pmb { h } _ { j } } { \\partial \\pmb { x } _ { j - 1 } } ) \\frac { \\partial \\pmb { x } _ { i } } { \\partial \\pmb { h } _ { i } } \\frac { \\partial \\pmb { h } _ { i } } { \\partial \\mathbf { W } _ { i } }\n$$\n\nDenoting the derivative of $\\sigma$ is $\\pmb { \\sigma } ^ { \\prime }$ , then the equation could simplified as:\n\n$$\n\\frac { \\partial \\mathcal { L } } { \\partial \\mathbf { W } _ { i } } = \\frac { \\partial \\mathcal { L } } { \\partial \\pmb { x } _ { N } } \\big ( \\prod _ { j = i + 1 } ^ { N } \\pmb { \\sigma } _ { j } ^ { \\prime } \\mathbf { W } _ { j } \\big ) \\pmb { \\sigma } _ { i } ^ { \\prime } \\pmb { x } _ { i - 1 } .\n$$\n\nThus, in training memory, the core consumption lies in the states of model weights $\\{ \\mathbf { W } _ { i } \\} _ { i = 1 } ^ { N }$ and derivative activation functions state {σ′}iN=1 along the backpropagation path, as well as the optimizer states used during gradient updates. The optimizer states are directly related to the updated model parameters $\\{ \\Delta \\mathbf { W } \\}$ .\n\nAssuming the batch size is $B$ , the length of the input sequence is $T$ , the model input and output dimension is $d$ and $p$ , for a standard linear layer $\\mathbf { \\boldsymbol { x } } _ { i } = \\sigma _ { i } ( \\mathbf { \\boldsymbol { W } } _ { i } \\mathbf { \\boldsymbol { x } } _ { i - 1 } )$ , the forward pass stores the intermediate states of the model and the model weights with the memory complexity of $O ( p d + B T d )$ , while the backward pass is responsible for storing the activation states during the gradient update process, the results of the output gradients, and the corresponding parameter gradients, with the total memory complexity of $\\bar { O } ( B T ( p + \\bar { d } ) + p d )$ .\n\n# Deep Learning with Differential Privacy\n\nDifferential Privacy (Dwork et al. 2006; Abadi et al. 2016) algorithms demonstrate that under this formulation, the model’s output cannot significantly help determine whether an individual record exists in the input dataset through certain mathematical derivations. The formal definition is recalled as follows:\n\nDefinition 1 (Differential Privacy). Given a domain $\\mathcal { D }$ , any two datasets $D$ , $D ^ { \\prime } \\subseteq { \\mathcal { D } }$ that differ in exactly one record are called neighboring datasets. A randomized algorithm $\\mathcal { M } :$ $\\mathcal { D }  \\mathcal { R }$ is $( \\epsilon , \\delta )$ -differential private if for all neighboring datasets $D$ and $D ^ { \\prime }$ and all $T \\subseteq { \\mathcal { R } }$ ,\n\n$$\n\\operatorname* { P r } [ \\mathcal { M } ( D ) \\subseteq T ] \\leq e ^ { \\epsilon } \\operatorname* { P r } [ \\mathcal { M } ( \\mathcal { D } ^ { \\prime } ) \\subseteq T ] + \\delta .\n$$\n\nDP-optimizer. To train a privacy-preserving language model, the current approach involves providing differential privacy guarantees when computing gradients and applying these guarantees to optimizers such as SGD or Adam (Abadi et al. 2016; Mironov 2017; Koskela, Ja¨lko¨, and Honkela 2020). This approach incorporates steps involving per-example gradient clipping Gl = P Ci ∂∂WL(il) and adding Gaussian noise $\\mathcal { N } ( 0 , { \\bf { I } } )$ on gradient G. Where $C _ { i }$ is the persample clipping factor.\n\nBook-keeping. To avoid the significant memory overhead caused by storing gradients for each sample during initialization, Bu et al. (2023) proposed a method BK utilizing gradient norms. Using the GhostClip (Goodfellow 2015; Bu, Mao, and $\\mathtt { X u } 2 0 2 2 )$ strategy, the gradient norm of each sample is calculated.\n\n$$\n\\left\\| { \\frac { \\partial { \\mathcal { L } } _ { i } } { \\partial \\mathbf { W } } } \\right\\| _ { \\mathrm { F } } ^ { 2 } = \\mathrm { v e c } \\Big ( { \\frac { \\partial { \\mathcal { L } } } { \\partial h _ { i } } } { \\frac { \\partial { \\mathcal { L } } } { \\partial h _ { i } } } ^ { \\top } \\Big ) \\cdot \\mathrm { v e c } \\left( { \\pmb { x } } _ { i } { \\pmb { x } } _ { i } ^ { \\top } \\right)\n$$\n\nBased on the gradient norms, clipping factors $C _ { i }$ and clipping matrices $\\mathbf { C }$ are generated, which are then used $\\begin{array} { r } { \\pmb { x } _ { ( l ) } ^ { \\top } \\mathrm { d i a g } ( \\mathbf C ) \\frac { \\partial \\mathcal { L } } { \\partial \\pmb { h } _ { ( l ) } } } \\end{array}$ . It is necessary to retain the comple $\\mathbf { G } _ { l } \\mathbf { \\Psi } =$ time and memory complexity when handling long-text question answering. BK-MixOpt strikes a balance between the two. It compares the theoretical complexity in terms of both dimension and context length, and selects the optimal memory complexity $O ( m i n \\{ 2 \\bar { B } T ^ { 2 } , B p d \\} )$ as the basis for computation.\n\n# Methodology\n\nTo address the issue of excessive memory consumption during differential privacy training, we have designed two methods: DP-MemArcside and DP-MemArcrev. These methods help reduce training memory usage in different aspects.\n\n# Side Network Design\n\nIn general, most of the memory consumption comes from the model weights and the states of activation functions in the backward propagation path. By minimizing the consumption of these two parts as much as possible, the memory usage during training can be correspondingly reduced. This necessitates finding a reasonable design to address this situation.\n\nAssume the base model is $\\mathbf { F }$ , the model’s pre-trained weights, input, output, and parameters are ${ \\bf W } _ { \\mathrm { p } } , x _ { 0 } , y , \\theta .$ . The model could formulated as:\n\n$$\ny = \\mathbf { F } ( \\mathbf { W } _ { \\mathrm { p } } , \\theta ; x _ { 0 } ) .\n$$\n\nTraditional parameter-efficient fine-tuning methods cannot avoid the memory consumption associated with the model weights of frozen parameters in the backward propagation path, which can formulated as:\n\n$$\ny = \\mathbf { F } ( \\mathbf { W } _ { \\mathrm { p } } + \\Delta \\mathbf { W } , \\boldsymbol { \\theta } + \\Delta \\boldsymbol { \\theta } ; x _ { 0 } ) .\n$$\n\nWe hope to find a form that remains distinct from the original form when adjusting parameters. That is, there exists such a form:\n\n$$\ny = \\alpha \\mathbf { F } _ { 1 } ( \\mathbf { W } _ { \\mathrm { p } } , \\boldsymbol { \\theta } ; x _ { 0 } ) + \\beta \\mathbf { F } _ { 2 } ( \\Delta \\mathbf { W } , \\Delta \\boldsymbol { \\theta } ; x _ { 0 } ) .\n$$\n\nIn this form, Side-tuning (Zhang et al. 2020) meets the requirements. Side-tuning introduces a side network that learns the knowledge and features of new tasks, relying on the knowledge contained in the trained model parameters, thus supporting the processing of downstream tasks.\n\nAssuming the input and output dimension of the side network is $r$ , we add a liner layer at the last layer of the side network to ensure dimension consistency. We use Bookkeeping (Bu et al. 2023) for differential privacy fine-tuning. The memory cost includes both the forward and backward propagation processes. For the forward process, the bilateral forward propagation memory consumption needs to be taken into account, with the complexity of $\\bar { O ( p d + r ^ { 2 } + B T ( d + r ) ) }$ . For the backward process, gradients need to be computed only in the side network, with a complexity of $O ( 2 B T r +$ $r ^ { 2 } .$ ) for gradients and $O ( 2 B T ^ { 2 } )$ for Ghost Norm.\n\nWhen $r \\ll d$ , the side tuning approach significantly reduces the training memory required for privacy fine-tuning. However, at sufficiently small $r$ , the performance of side tuning also deteriorates significantly. To integrate the information of a trained model effectively into side networks, we adopt the LST (Sung, Cho, and Bansal 2022) method. This involves passing the intermediate layer information from the pre-trained model to the si de through a linear layer $f ^ { \\prime }$ . We denote this method as DP-MemArcside.\n\n$$\ny = \\mathbf { F } _ { 2 } ( \\Delta \\mathbf { W } , \\Delta \\theta ; y _ { i - 1 } + f _ { i } ^ { \\prime } ( x _ { i } ) , y _ { 0 } = x _ { 0 } ) .\n$$\n\nUsing the $\\mathrm { D P \\mathrm { - } M e m A r c _ { \\mathrm { s i d e } } }$ method, we can maintain a good performance in fine-tuning our side network with differential privacy. When $d / r \\ = \\ 8$ , LST (Sung, Cho, and Bansal 2022) and DP-MemArc $\\mathrm { s i d e }$ achieves an empirically optimal ratio of training memory to performance.\n\n# Reversible Network Design\n\nDue to the significant amount of training memory required to store the state of activation functions during batch processing, a large portion of memory is consumed by saving activation states $\\{ \\sigma _ { i } \\} _ { i = 1 } ^ { N }$ . Regular parameter-efficient finetuning methods cannot effectively address this issue. DPMemArc $\\mathrm { s i d e }$ reduces the memory needed to store activation functions by compressing the dimensions of the activation functions. However, this method still consumes some memory. If we could deduce the intermediate states from the output results in reverse, we could further reduce the memory demand for storing activation states.\n\nFor reversible networks (Gomez et al. 2017; Liao, Tan, and Monz 2023), the following form is usually satisfied.\n\n$$\n\\begin{array} { r l } & { \\pmb { x } _ { i + 1 } ^ { 1 } = \\alpha \\pmb { x } _ { i } ^ { 1 } + \\mathcal { F } _ { i } ( \\pmb { x } _ { i } ^ { 2 } ) , } \\\\ & { \\pmb { x } _ { i + 1 } ^ { 2 } = \\beta \\pmb { x } _ { i } ^ { 2 } + \\mathcal { G } _ { i } ( \\pmb { x } _ { i + 1 } ^ { 1 } ) , } \\\\ & { \\pmb { x } _ { i } ^ { 2 } = ( \\pmb { x } _ { i + 1 } ^ { 2 } - \\mathcal { G } _ { i } ( \\pmb { x } _ { i + 1 } ^ { 1 } ) ) / \\beta , } \\\\ & { \\pmb { x } _ { i } ^ { 1 } = ( \\pmb { x } _ { i + 1 } ^ { 1 } - \\mathcal { F } _ { i } ( \\pmb { x } _ { i } ^ { 2 } ) ) / \\alpha . } \\end{array}\n$$\n\nues $\\pmb { \\sigma } _ { i } = \\sigma _ { i } ( \\mathbf { W } _ { i } \\pmb { x } _ { i - 1 } )$ ofrroesmptohnedintgeramcteidviattieons fautensc $\\{ \\pmb { x } _ { i } \\} _ { i = 1 } ^ { N }$ of the model and calculate their derivatives, thus avoiding the need to store each activation function value.\n\nTo enable the two modules of the reversible network to both acquire new features and retain the knowledge of the pre-trained model, For module $\\mathcal { F }$ , we introduced the LoRA (Hu et al. 2021) architecture into the FFN layer of the model, continuing the traditional LoRA approach. Meanwhile, for module $\\mathcal { G }$ , we used Adapters (Houlsby et al. 2019) as trainable parameters to adapt to downstream tasks. Since the network is reversible, we only need to use constant reproducible space to compute $\\mathbf { \\Delta } \\mathbf { x } _ { i } ^ { 2 }$ and $\\pmb { x } _ { i } ^ { 1 }$ for each layer, which satisfies the requirements for the subsequent backpropagation calculations. We denote this method as $\\mathrm { D P \\mathrm { - M e m A r c } _ { r e v } }$ .\n\n![](images/eeadba624632b5d909c4da96782e04e57b7130de9700f209363b5f3a51c61ed3.jpg)  \nFigure 1: Two different DP-MemArc designs, the left represents reversible network design, and the right represents side network design. The trainable parameters are fine-tuned using the differential privacy BK-MixOpt method.\n\nFor reversible networks, we have the following derivation steps. At the beginning of training, when the output of the adapter output is close to 0. $x _ { n } \\approx \\mathcal { F } _ { n } ( { \\pmb x } _ { n - 1 } )$ . Assume that $x _ { 0 } ^ { 1 }$ and $x _ { 0 } ^ { 2 }$ comes from the initial input $x$ , we have:\n\n$$\n\\begin{array} { r } { \\pmb { x } _ { 1 } ^ { 1 } = \\alpha \\pmb { x } _ { 0 } ^ { 1 } + \\mathcal { F } _ { 1 } ( \\pmb { x } _ { 0 } ^ { 2 } ) \\approx \\alpha x _ { 0 } + x _ { 1 } , } \\end{array}\n$$\n\n$$\n\\pmb { x } _ { 1 } ^ { 2 } = \\beta \\pmb { x } _ { 0 } ^ { 2 } + \\mathcal { G } _ { 1 } ( \\pmb { x } _ { 1 } ^ { 1 } ) = \\beta \\pmb { x } _ { 0 } + \\mathcal { G } _ { 1 } ( \\pmb { x } _ { 1 } ^ { 1 } ) \\approx \\beta \\pmb { x } _ { 0 } .\n$$\n\nWhen $\\alpha  0$ , we have $\\pmb { x } _ { 1 } ^ { 1 } = x _ { 1 }$ , $\\pmb { x } _ { 1 } ^ { 2 } = \\beta \\pmb { x } _ { 0 }$ . We achieve a relatively stable state of the reversible network by exchanging output values ${ \\pmb x } _ { 1 } ^ { 1 } = \\beta { \\pmb x } _ { 0 }$ , $\\pmb { x } _ { 1 } ^ { 2 } = x _ { 1 }$ . Through iterative computation like this, the model can be satisfied as ${ \\pmb x } _ { n } ^ { 1 } \\approx \\beta { \\pmb x } _ { n - 1 } , { \\pmb x } _ { n } ^ { 2 } \\approx { \\pmb x } _ { n }$ . We generate the final output as $x \\overset {  } { = } ( { \\pmb x } _ { N } ^ { 1 } + { \\pmb x } _ { N } ^ { 2 } ) / 2$ . In this way, when training reversible models, the continuity of the model’s representation can be maintained, and inference and learning for downstream tasks can be facilitated based on pre-trained models.\n\nDuring the backpropagation process in our reversible network, the intermediate states of the model can be obtained by computing the reverse steps. As a result, the training memory required for activation values can be reduced by reusing a fixed-size replaceable memory. The primary training memory consumption of the model comes from storing the output gradients, storing the parameter gradients, and the computational memory required by the Ghostnorm method. Here, we also employ the BK-MixOpt algorithm to calculate the norm of the samples, thereby obtaining the corresponding gradient values. During training, we set batch sizes to 32.\n\n# Experimental Setup\n\nWe designed a series of experiments covering different models and datasets to evaluate the performance of our methods. The specific experimental design is as follows.\n\nModels. We used the RoBERTa-large (Liu et al. 2019), GPT-2-large (Radford et al. 2019) model as our base models. These models will be fine-tuned according to the corresponding downstream tasks, and the performance of the fine-tuned models will be evaluated under different privacy constraints.\n\nBaselines. We compare the two methods against multiple baselines, including DP-LoRA (Hu et al. 2021; Yu et al. 2021), DP-Adapter (Houlsby et al. 2019; Yu et al. 2021), DP-BiTFiT (Bu et al. 2024; Zaken, Goldberg, and Ravfogel 2022), and PromptDPSGD (Duan et al. 2024; Lester, AlRfou, and Constant 2021). These methods are all privacypreserving fine-tuning approaches with opacus DP (Yousefpour et al. 2021), and we test them on the same training data to ensure fairness of comparison.\n\nDatasets. We conduct experiments on five datasets. Four from the GLUE benchmarks (Wang et al. 2018), which cover different NLP tasks. MNLI: the MultiGenre Natural Language Inference Corpus. QQP: the Quora Question Pairs2 dataset. QNLI: the Stanford Question Answering dataset. SST2: the Stanford Sentiment Treebank dataset. We also select an NLG task E2E dataset (Duvsek, Novikova, and Rieser 2019), which is to generates texts to evaluate a restaurant, to evaluate the quality of the model in generation tasks under privacy constraints.\n\nImplementation Details. To standardize the training process, we partition each dataset as follows: The text classification dataset includes $5 0 \\mathrm { k }$ samples for training, 1k samples for validation, and the remaining data for testing. The E2E dataset includes 42061 samples for training and 4672 samples for validation. We set different privacy constraint conditions specifically as $\\epsilon = \\{ 1 . 6 , 8 , \\infty \\}$ and $\\delta = 1 / | \\mathcal { D } _ { t r a i n } |$ to assess performance variations among different methods under these constraints. We chose a learning rate of $5 { \\mathrm { e } } - 4$ and used DP-Adam optimizer as the default optimizer for the model, while DP-SGD optimizer is employed for PromptDPSGD. For evaluation metrics, we utilize a profiler to track the model’s training memory usage, evaluating the mean memory consumption during training. Default LoRA and Adapters ranks are set to $r ~ = ~ 6 4$ . For text classification tasks, we compare accuracy. For generation tasks, we employed perplexity, BLEU (Papineni et al. 2002), and ROUGE-L (Lin 2004) as evaluation metrics to comprehensively assess generation quality. In our experiments, we conduct training with a batch size of 32 and sequence length of 128 in FP16.\n\nTable 2: Experiments on the RoBERTa-large model. We evaluate the accuracy $\\% )$ results and profile to compute the training memory(GB) with privacy constraints at $\\epsilon = 1 . 6 , 8 , \\infty$ . We propose two DP-MemArc architectures as novel efficient memory privacy fine-tuning schemes. Adaptive and Fixed are used to differentiate whether the trainable parameters can be adjusted.   \n\n<html><body><table><tr><td rowspan=\"2\"></td><td colspan=\"3\">Memory(GB)↓</td><td colspan=\"2\">MNLI↑</td><td colspan=\"3\"></td><td colspan=\"3\">QNLI个</td><td colspan=\"3\">SST2↑</td><td rowspan=\"2\">Trainable param(%)</td></tr><tr><td>e = 1.6</td><td>∈=8</td><td>∈=∞</td><td>e = 1.6</td><td>e=8 ∈=8</td><td>e = 1.6</td><td>e=8</td><td>e=8</td><td>∈= 1.6</td><td>e=8</td><td>∈=8</td><td>∈ = 1.6</td><td>e=8</td><td>∈=8</td></tr><tr><td colspan=\"10\">Diferential PrivateonAdaptive Parameter Transfer Learning</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DP-LoRA</td><td>12.65</td><td>12.21</td><td>7.14</td><td>81.49 87.07</td><td>90.81</td><td>83.46</td><td>88.53</td><td>91.75</td><td>87.32</td><td>91.34</td><td>94.33</td><td>93.43</td><td>95.14</td><td>95.88</td><td>1.88%</td></tr><tr><td>DP-Adapters</td><td>13.29</td><td>13.07</td><td>7.38</td><td>80.84 86.93</td><td>90.15</td><td>84.20</td><td>87.98</td><td>91.37</td><td>86.17</td><td>90.28</td><td>94.36</td><td>92.87</td><td>95.33</td><td>95.82</td><td>1.86%</td></tr><tr><td>PromptDPSGD</td><td>12.44</td><td>12.12 6.23</td><td>7.25</td><td>81.16 81.30</td><td>87.13 90.48</td><td>83.58</td><td>88.46</td><td>91.22</td><td>87.23</td><td>90.87</td><td>94.11</td><td>93.13</td><td>95.29</td><td>95.93</td><td>1.92%</td></tr><tr><td>DP-MemArcside</td><td>6.18</td><td></td><td>5.66</td><td>87.16</td><td>90.91</td><td>84.56</td><td>88.92</td><td>91.66</td><td>86.95</td><td>91.56</td><td>94.40</td><td>93.60</td><td>95.44</td><td>95.94</td><td>2.10%</td></tr><tr><td colspan=\"10\">Diffrential Privateon Fixed Parameter Transfer Learning</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DP-Full FT</td><td>26.12</td><td>26.83</td><td>10.93</td><td>51.45</td><td>84.23</td><td>90.65 61.37</td><td>84.98</td><td>92.30</td><td>59.55</td><td>84.48</td><td>95.13</td><td>75.74</td><td>86.20</td><td>96.18</td><td>100%</td></tr><tr><td>DP-BiTFiT</td><td>5.12</td><td>5.88</td><td>4.82</td><td>75.36</td><td>89.19</td><td>78.92</td><td>85.20</td><td>90.65</td><td>83.43</td><td>87.57</td><td>93.56</td><td>89.12</td><td>93.02</td><td>95.38</td><td>0.08%</td></tr><tr><td>DP-MemArcrev</td><td>5.48</td><td>5.65</td><td>4.78</td><td>80.29</td><td>83.74 86.12</td><td>90.21 82.57</td><td>88.12</td><td>91.25</td><td>85.89</td><td>90.31</td><td>94.10</td><td>91.78</td><td>93.89</td><td>95.32</td><td>3.92%</td></tr></table></body></html>\n\n<html><body><table><tr><td rowspan=\"2\"></td><td colspan=\"3\">Memory(GB)↓</td><td colspan=\"3\">BLEU↑</td><td colspan=\"3\">Rouge-L↑</td><td colspan=\"3\">Perplexity↓</td><td rowspan=\"2\">Trainable param(%)</td></tr><tr><td>∈ = 1.6</td><td>∈=8</td><td>∈=8</td><td>∈= 1.6</td><td>∈=8</td><td>=8</td><td>∈= 1.6</td><td>∈=8</td><td>E =8</td><td>∈ =1.6</td><td>e=8</td><td>∈=∞</td></tr><tr><td colspan=\"10\">Differential Private on Adaptive Parameter Transfer Learning</td><td></td><td></td><td></td></tr><tr><td>DP-LoRA</td><td>22.21</td><td>21.72</td><td>13.73</td><td>65.4</td><td>67.1</td><td>69.1</td><td>64.4</td><td>68.3</td><td>72.1</td><td>2.42</td><td>2.45</td><td>2.31</td><td>1.15%</td></tr><tr><td>DP-Adapters</td><td>23.68</td><td>24.12</td><td>14.55</td><td>65.2</td><td>66.9</td><td>69.2</td><td>64.9</td><td>68.2</td><td>71.9</td><td>2.44</td><td>2.35</td><td>2.28</td><td>1.16%</td></tr><tr><td>PromptDPSGD DP-MemArcside</td><td>22.12 11.68</td><td>20.96 11.44</td><td>14.18 10.17</td><td>64.2</td><td>66.5</td><td>69.1</td><td>65.0</td><td>68.3</td><td>72.0</td><td>2.60</td><td>2.54</td><td>2.39</td><td>1.33%</td></tr><tr><td></td><td></td><td></td><td></td><td>66.4</td><td>68.2</td><td>68.9</td><td>64.6</td><td>68.5</td><td>72.7</td><td>2.32</td><td>2.38</td><td>2.24</td><td>1.28%</td></tr><tr><td colspan=\"10\">Differential Private on Fixed Parameter Transfer Learning</td><td colspan=\"3\"></td></tr><tr><td>DP-Full FT</td><td>58.96</td><td>62.23</td><td>20.45</td><td>62.2</td><td>66.8</td><td>69.3</td><td>63.4</td><td>67.8</td><td>72.6</td><td>2.46</td><td>2.23</td><td>1.85</td><td>100%</td></tr><tr><td>DP-BiTFiT</td><td>9.59</td><td>9.71</td><td>8.62</td><td>61.7</td><td>65.2</td><td>68.6</td><td>62.9</td><td>66.4</td><td>71.3</td><td>2.83</td><td>2.58</td><td>2.77</td><td>0.05%</td></tr><tr><td>DP-MemArCrev</td><td>9.45</td><td>9.88</td><td>8.39</td><td>65.1</td><td>66.1</td><td>69.8</td><td>64.2</td><td>68.1</td><td>71.6</td><td>2.71</td><td>2.65</td><td>2.58</td><td>2.15%</td></tr></table></body></html>\n\nTable 3: Experiments on the GPT-2-large model. We evaluate the $B L E U ( \\% )$ , Rouge- $. \\mathrm { L } ( \\% )$ and Perplexity scores results on E2E dataset and profile to compute the training memory(GB) with privacy constraints at $\\epsilon = 1 . 6 , 8 , \\infty$ .\n\n# Experiments\n\n# Main Results\n\nWe evaluate various baseline methods on multiple task datasets and organized the results of RoBERTa and GPT2 separately according to the task type.\n\nText classification on RoBERTa-large. As shown in Table 2, the two DP-MemArc methods demonstrate competitive performance on text classification tasks using the RoBERTa-large model.\n\n(1) The side network design achieves the best results compared to other adaptive baseline methods in most of the accuracy evaluations. The average performance on DPMemArc $\\mathrm { s i d e }$ is similar to DP-LoRA, but the side network design method requires less training memory than DP-LoRA.\n\n(2) Specifically, compared to the performance of DP-LoRA under privacy constraints, our DP-MemArcside achieves nearly $2 \\sim 3 \\times$ optimization in training memory. Simultaneously, we can observe that when further memory savings during training are required, the reversible network design of DP-MemArc offers an ideal choice.\n\n(3) Compared to the current most memory-efficient method, DP-BiTFiT, our method consistently performs better in downstream tasks while maintaining similar training memory usage. This indicates that DP-MemArcrev can better learn the characteristics of downstream tasks and perform gradient clipping based on computable activation function values while preserving privacy.\n\n(4) In terms of average performance, DP-MemArc $\\mathrm { r e v }$ improves accuracy by an average of $+ 3 . 1 \\%$ compared to DPBiTFiT and performs better in scenarios with higher privacy constraints $\\epsilon$ , suggesting that the model better captures the gradient changes of the training data and adapts to downstream tasks.\n\nText Generation on GPT-2-large. For generative tasks, we employ three metrics to assess the quality of animal generation and simultaneously utilize profiles to record the changes in training memory. Experiments on Table 3 indicate that our approach demonstrates performance comparable to text classification tasks in generative tasks.\n\n(1) Our side network design excels in perplexity performance compared to other differential privacy parameter tuning methods. Additionally, DP-MemArc $\\mathrm { s i d e }$ shows outstanding performance on the BLEU metric. Comparing our method under differential privacy, when the parameter $\\epsilon$ is set to 1.6 indicating higher privacy demands, performance in the BLEU metric only drops by $3 . 5 \\%$ . This suggests our method better learns the characteristics and paradigms of generative tasks, yielding relatively accurate outputs.\n\n(2) Compared to DP-BiTFiT, reversible network design exhibits competitive training memory consumption requirements, with DP-MemArcrev maintaining strong performance. This approach maintains relatively stable task accuracy under highly constrained training memory conditions.\n\n(3) Compared to full differential privacy fine-tuning, DP$\\mathbf { M e m A r c } _ { \\mathrm { r e v } }$ saves approximately $6 \\sim 8 \\times$ the training memory in high privacy $\\epsilon = 1 . 6$ scenarios. These results underscore the promising outlook of our proposed DP-MemArc framework for generative tasks, maintaining lower training memory requirements even at larger batch sizes.\n\n# Analysis\n\nWe conduct a deep analysis of two DP-MemArc methods and perform ablation experiments on the corresponding modules, including the differential private algorithm and alternative model setting.\n\nBook-Keeping in DP-MemArc. In the setup of these two architectures, we use the BK-MixOpt method for differential privacy training. BK-MixOpt reduces the required training memory by using Ghostnorm to compute the normalized formulation. To evaluate the impact of different differential privacy methods during the training process, we conducted experiments on these two model designs and measured the average memory consumption during the training process. The results are shown in Table 4.\n\nBK-MixOpt exhibits the best performance in the following scenarios. From the ablation experiments, the BKMixOpt method reduces training memory consumption by $1 . 5 \\ \\sim \\ 2 \\times$ in privacy-preserving computation. This highlights the importance of using BK-MixOpt within our framework. When there are no privacy constraints as $\\epsilon \\ : = \\ : \\infty$ , all three methods degrade into the standard gradient descent process. Under the condition of privacy constraints, if the Opcaus method of calculating gradients for each sample is adopted, the time complexity for calculating the sample gradient in a single layer under the two architectures DP-MemArc $\\mathrm { s i d e }$ and DP-MemArcrev is $O ( B p d / 6 4 )$ and $O ( 4 B p r )$ . This still requires a considerable amount of computation time, and in DP-MemArc $\\mathrm { s i d e }$ , the gradient calculation for the upsampling and downsampling matrices also needs to be considered.\n\nReversible Network Functions. In the design of DP$\\mathbf { M e m A r c } _ { \\mathrm { r e v } }$ , we include two sub-functions that are used to achieve the reversible design of reversible networks. Section elaborates on the principles of the reversible network’s inversion. Therefore, we can modify the internal design while ensuring that each sub-function fulfills its respective role. To further understand the differences between various designs, we fix the sub-function $\\mathcal { G }$ and change the internal architecture of sub-function $\\mathcal { F }$ , replacing it with different parameter-efficient fine-tuning(PEFT) methods. In the privacy scenario, we select different methods and incorporate them with DP-MemAr $_ \\mathrm { { _ { r e v } } }$ in terms of accuracy and training memory consumption.\n\nTable 4: Evaluations of Different DP methods on DPMemArc.   \n\n<html><body><table><tr><td></td><td>Privacy Constrains</td><td>DP-MemArcside</td><td>DP-MemArcrev</td></tr><tr><td rowspan=\"3\">Opcaus</td><td>∈=1.6,δ=2×10-5</td><td>7.45</td><td>10.66</td></tr><tr><td>∈= 8.0,δ=2 ×10-5</td><td>7.33</td><td>10.98</td></tr><tr><td>∈=∞,δ=2×10-5</td><td>5.60</td><td>4.82</td></tr><tr><td rowspan=\"3\">GhostClip</td><td>∈=1.6,δ=2×10-5</td><td>9.72</td><td>8.52</td></tr><tr><td>∈= 8.0,δ=2× 10-5</td><td>9.54</td><td>8.43</td></tr><tr><td>∈= ∞,δ=2×10-5</td><td>5.72</td><td>4.75</td></tr><tr><td rowspan=\"3\">BK-MixOpt</td><td>∈=1.6,δ=2×10-5</td><td>6.18</td><td>5.48</td></tr><tr><td>∈=8.0,δ=2×10-5</td><td>6.23</td><td>5.65</td></tr><tr><td>∈= ,δ=2×10-5</td><td>5.66</td><td>4.78</td></tr></table></body></html>\n\n![](images/75522c624a0ae75ef678b508fe8ab5024ff7fe7087e0fdb1155760037e9a7c5d.jpg)  \nFigure 2: Performance of different reversible network subfunction $\\mathcal { F }$ design. The private constraint is $\\epsilon = 8 . 0$ .\n\nWe have selected several classic and efficient parameter fine-tuning methods to replace the subfunction $\\mathcal { F }$ here, including LoRA (Hu et al. 2021), Parallel Adapters (He et al. 2021), Prefix tuning (Li and Liang 2021) and dyLoRA (Valipour et al. 2023), and set the constraint $\\epsilon = 8 . 0$ . The result is shown in Figure 2.\n\nLoRA is superior to other candidate architectures as a reversible network sub-function. Compared to other methods, using $\\mathscr { F } = \\mathbf { F } \\big ( \\mathbf { W } _ { \\mathrm { p } } { + } L R , \\theta { + } \\Delta \\theta ; x _ { i } ^ { 2 } \\big )$ results in a slight $\\mathbf { + 0 . 7 1 }$ improvement in accuracy. Given the simplicity of LoRA’s network architecture and the similarity in training memory usage across various methods, we finally adopt LoRA as the reversible network sub-function for DP-MemArcrev.\n\nTable 5: Experiments on time efficiency for different methods on Roberta-large. The privacy constraints are set to $\\epsilon = 8$ .   \n\n<html><body><table><tr><td></td><td>Time Complexity(sec)</td><td>Memory Complexity(GB)</td><td>QQP scores↑</td></tr><tr><td rowspan=\"4\">DP-Full FT DP-LoRA DP-Adapters</td><td>626.51</td><td>26.83</td><td>84.98</td></tr><tr><td>295.62</td><td>12.68</td><td>88.73</td></tr><tr><td>301.96</td><td>13.07</td><td>87.98</td></tr><tr><td>285.98</td><td>5.88</td><td>85.20</td></tr><tr><td rowspan=\"2\">PromptDPSGD DP-MemArcside</td><td>293.65</td><td>12.06</td><td>88.31</td></tr><tr><td>291.56</td><td>6.23</td><td>88.92</td></tr><tr><td rowspan=\"2\">DP-MemArCrev</td><td>284.69</td><td>5.65</td><td>88.12</td></tr></table></body></html>\n\n# Time Efficiency\n\nTo better validate the time efficiency theory in Table 1, we compare the time efficiency of our method with the baseline method to ensure that DP-MemArc maintains consistent time consumption while being memory-efficient. Since the inference speed of a model is positively correlated with the size of the training parameters in most cases, when we set the same number of trainable parameters, the impact of parameter size is reduced, and the difference in consumption is mainly reflected in the different frameworks. The corresponding results are shown in the Table 5. The experimental results indicate that compared to other baseline methods, DP-MemArc is more efficient in reasoning across various downstream tasks.\n\n# Training Scale Analysis\n\nTo understand and compare the training process and accuracy variations of different methods under differential privacy, we use checkpoints to record the training process of the model. We test three methods: DP-LoRA, DP-MemArcside, and DP-MemArcrev on the GPT2-large model, evaluating their BLEU scores. From the results, DP-MemArc $\\mathrm { s i d e }$ and DP-MemArcrev require more training steps to reach stable values compared to DP-LoRA. Considering the architecture of the models themselves, DP-MemArc $\\mathrm { s i d e }$ needs to be tuned for the entire side network to adapt to the corresponding time for downstream tasks. Training the low-rank matrices of DPLoRA is relatively simpler. As for the reversible network, due to the use of approximation methods for learning, more training data helps to mitigate the performance loss caused by approximation by adjusting the reversible gradients.\n\n# Related Work\n\n# Differential Private Fine-tuning\n\nTo ensure the privacy needs of the model, differential privacy fine-tuning methods offer a feasible solution with strong theoretical guarantees (Abadi et al. 2016; Song, Chaudhuri, and Sarwate 2013). In terms of model structure, PEFT methods can be transferred to differential privacy schemes (Yu et al. 2021; Bu et al. 2024; Xu et al. 2024). In methods design, the selected differential privacy (Shi et al. 2022a,b) approach can provide stronger differential privacy constraints more specifically for designated information. In algorithm design, it includes a series of studies (Rochette, Manoel, and Tramel 2020; Du et al. 2023) on the computational graph during the differential privacy propagation process. Techniques like Ghostnorm (Goodfellow 2015; Li et al. 2021) and Book-Keeping (Bu et al. 2023) provide unified batch norm computation and batch processing for gradient clipping. Although differential privacy offers very strong theoretical protections, reducing the memory requirements for training under differential privacy scenarios remains a significant challenge (Du et al. 2023).\n\n![](images/c9f6ba3b2205258198842f9c0742b670f725b9ddfff3f5ae1637b0baedb8f4c5.jpg)  \nFigure 3: The experiment of training steps is conducted on the E2E dataset.\n\n# Parameter Efficient Transfer Learning\n\nTo reduce the demand for computational resources during training, parameter-efficient fine-tuning methods are applied to transfer learning. Common methods include training lowrank matrices (Hu et al. 2021; Valipour et al. 2023; Dettmers et al. 2024), adding adapters (Houlsby et al. 2019; He et al. 2021), and performing prefix tuning (Li and Liang 2021; Liu et al. 2022b) or prompt tuning (Lester, Al-Rfou, and Constant 2021) on the inputs of the original model. While most parameter-efficient fine-tuning methods reduce time and space consumption, they still require significant training memory due to the state of activation functions (Sung, Cho, and Bansal 2022; Liao, Tan, and Monz 2023). Our framework offers side networks and reversible networks designs to reduce the memory required during training.\n\n# Conclusion\n\nIn this paper, we introduce a framework called DP-MemArc, which encompasses two methods aimed at addressing the issue of excessive memory consumption during training in privacy-sensitive scenarios. In this process, we reduce the training memory consumption of models in privacy environments using the BK method. With DP-MemArc, LLMs can perform downstream tasks under corresponding privacy constraints across various tasks. We hope that our method will contribute to future private efficient memory optimization for fine-tuning LLMs.",
    "institutions": [
        "Zhejiang University",
        "Southeast University",
        "Harvard University",
        "University of California, Los Angeles",
        "Massachusetts Institute of Technology",
        "Renmin University of China",
        "The University of Tokyo",
        "Tongji University"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n大语言模型（LLMs）在训练时需要大量的计算资源和时间，且在下游任务中适配不便。同时，可用数据集通常较小且专有，存在隐私问题，并且LLMs的训练需要大量的内存，使得在参数高效微调时训练具有挑战性。现有的差分隐私微调方法在训练内存方面存在问题，计算和存储开销较大，内存资源优化效率不足。因此，解决语言模型在隐私微调时内存不足的问题具有重要意义。\\n\\n**方法概述**\\n本文提出了一个名为DP - MemArc的框架，该框架包含DP - MemArcside和DP - MemArcrev两种内存优化方法，可在差分隐私微调中以较低的训练内存实现语言模型的高效微调。\\n\\n**主要贡献与效果**\\n- 提出DP - MemArc框架，包含两种内存优化方法，实现了约2.5倍的内存优化，有效减少了语言模型隐私训练的内存需求。在文本分类任务中，DP - MemArcside相比DP - LoRA在训练内存上实现了近2 - 3倍的优化；在生成任务中，DP - MemArcrev在高隐私场景下相比全差分隐私微调节省了约6 - 8倍的训练内存。\\n- 系统分析了训练内存需求与网络架构的关系，在差分隐私下展示了良好的下游任务性能。如在RoBERTa - large模型的文本分类任务中，DP - MemArcrev相比DP - BiTFiT平均提高了3.1%的准确率，在高隐私约束场景下表现更好。\\n- 在多个数据集和模型上评估DP - MemArc框架，结果显示在各个维度上表现良好，训练内存有显著改善。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n通过设计不同的网络结构，从不同角度减少语言模型在差分隐私训练时的内存使用。DP - MemArcside通过引入侧网络学习新任务的知识和特征，减少反向传播路径中模型权重和激活函数状态的内存消耗；DP - MemArcrev利用可逆网络，通过反向推导中间状态，避免存储每个激活函数值，从而减少内存需求。\\n\\n**创新点**\\n先前的差分隐私微调方法存在训练内存开销大、内存资源优化效率不足的问题。与先前工作相比，DP - MemArc提出了侧网络和可逆网络两种设计，从不同方面减少内存使用。侧网络设计避免了传统参数高效微调方法中反向传播路径中冻结参数的内存消耗；可逆网络设计通过反向推导中间状态，避免了存储激活函数值的内存需求。侧网络设计在多数准确性评估中优于其他自适应基线方法，且训练内存需求更低；可逆网络设计能更好地学习下游任务特征，在高隐私约束场景下表现更好。此外，在可逆网络设计中，为使两个模块既能获取新特征又能保留预训练模型的知识，对模块$\\mathcal{F}$引入LoRA架构，对模块$\\mathcal{G}$使用Adapters作为可训练参数。\\n\\n**具体实现步骤**\\n1. **DP - MemArcside**：\\n    - 假设基础模型为$\\mathbf{F}$，引入侧网络，模型输出表示为$y = \\alpha \\mathbf{F}_1(\\mathbf{W}_p, \\theta; x_0) + \\beta \\mathbf{F}_2(\\Delta \\mathbf{W}, \\Delta \\theta; x_0)$。\\n    - 在侧网络最后一层添加线性层确保维度一致，使用Bookkeeping进行差分隐私微调。\\n    - 采用LST方法将预训练模型的中间层信息传递到侧网络，以保持良好的微调性能。当$d / r = 8$时，实现训练内存与性能的最优比例。\\n2. **DP - MemArcrev**：\\n    - 设计可逆网络，满足$\\pmb{x}_{i + 1}^1 = \\alpha \\pmb{x}_i^1 + \\mathcal{F}_i(\\pmb{x}_i^2)$，$\\pmb{x}_{i + 1}^2 = \\beta \\pmb{x}_i^2 + \\mathcal{G}_i(\\pmb{x}_{i + 1}^1)$等形式。\\n    - 对于模块$\\mathcal{F}$，引入LoRA架构到模型的FFN层；对于模块$\\mathcal{G}$，使用Adapters作为可训练参数以适应下游任务。\\n    - 利用可逆网络的特性，使用常数可重现空间计算每层的$\\mathbf{\\Delta} \\mathbf{x}_i^2$和$\\pmb{x}_i^1$，满足后续反向传播计算需求。通过迭代计算使网络达到相对稳定状态，在反向传播过程中通过反向步骤获取中间状态，减少激活值的训练内存需求。使用BK - MixOpt算法计算样本的范数，训练时设置批大小为32。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括DP - LoRA、DP - Adapter、DP - BiTFiT、PromptDPSGD和DP - Full FT。\\n\\n**性能对比**\\n*   **在 [训练内存] 指标上：** 在RoBERTa - large模型的文本分类任务中，DP - MemArcside在$\\epsilon = 1.6$时训练内存为6.18GB，远低于DP - LoRA的12.65GB，实现了近2 - 3倍的优化；DP - MemArcrev在$\\epsilon = 1.6$时训练内存为5.48GB，与当前最内存高效的DP - BiTFiT（5.12GB）相近。在GPT - 2 - large模型的生成任务中，DP - MemArcside在$\\epsilon = 1.6$时训练内存为11.68GB，低于DP - LoRA的22.21GB；DP - MemArcrev在$\\epsilon = 1.6$时训练内存为9.45GB，相比DP - Full FT的58.96GB，节省了约6 - 8倍的训练内存。\\n*   **在 [准确率] 指标上：** 在RoBERTa - large模型的文本分类任务中，DP - MemArcside的平均性能与DP - LoRA相似，但在大多数准确率评估中，侧网络设计优于其他自适应基线方法；DP - MemArcrev相比DP - BiTFiT平均提高了3.1%的准确率，在高隐私约束场景下表现更好。\\n*   **在 [困惑度、BLEU、ROUGE - L] 指标上：** 在GPT - 2 - large模型的生成任务中，DP - MemArcside在困惑度性能上优于其他差分隐私参数调整方法，如在$\\epsilon = 1.6$时，DP - MemArcside为2.32，低于PromptDPSGD的2.60；在BLEU指标上表现出色，当$\\epsilon = 1.6$时，DP - MemArcside为66.4，高于DP - LoRA的65.4，且性能仅下降3.5%；DP - MemArcrev在各项指标上表现与其他方法相当，能在高隐私约束下保持相对稳定的任务准确性。\\n*   **在 [时间效率] 指标上：** 在RoBERTa - large模型上，隐私约束设置为$\\epsilon = 8$时，DP - MemArcside时间复杂度为291.56秒，DP - MemArcrev时间复杂度为284.69秒，相比DP - Full FT的626.51秒，时间效率有显著提升，且与DP - LoRA、DP - Adapter、PromptDPSGD等方法相当甚至更优。\",\n    \"keywords\": \"### 关键词\\n\\n- 差分隐私 (Differential Privacy, DP)\\n- 大语言模型 (Large Language Models, LLMs)\\n- 差分隐私微调 (Differential Private Fine - tuning, N/A)\\n- 内存高效学习 (Memory Efficient Learning, N/A)\\n- DP - MemArc框架 (Differential Private Memory efficient transfer Architecture, DP - MemArc)\\n- 侧网络设计 (Side Network Design, N/A)\\n- 可逆网络设计 (Reversible Network Design, N/A)\\n- 低秩自适应 (Low - Rank Adaptation, LoRA)\"\n}"
}