{
    "source": "Semantic Scholar",
    "arxiv_id": "2407.05285",
    "link": "https://arxiv.org/abs/2407.05285",
    "pdf_link": "https://arxiv.org/pdf/2407.05285.pdf",
    "title": "Mjolnir: Breaking the Shield of Perturbation-Protected Gradients via Adaptive Diffusion",
    "authors": [
        "Xuan Liu",
        "Siqi Cai",
        "Qihua Zhou",
        "Song Guo",
        "Ruibin Li",
        "Kai Lin"
    ],
    "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
    ],
    "publication_date": "2024-07-07",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "The Hong Kong Polytechnic University",
        "Hubei Key Laboratory of Transportation Internet of Things",
        "Wuhan University of Technology",
        "College of Computer Science and Software Engineering",
        "Shenzhen University",
        "Department of Computer Science and Engineering",
        "The Hong Kong University of Science and Technology"
    ],
    "paper_content": "# Mjo¨lnir: Breaking the Shield of Perturbation-Protected Gradients via Adaptive Diffusion\n\nXuan Liu1, Siqi $\\mathbf { C a i } ^ { 2 }$ , Qihua Zhou3, Song $\\mathbf { G u o ^ { 4 ^ { * } } }$ , Ruibin $\\mathbf { L i } ^ { 1 }$ , Kaiwei Lin2\n\n1The Hong Kong Polytechnic University, Hong Kong 2Hubei Key Laboratory of Transportation Internet of Things, Wuhan University of Technology, Wuhan, China 3College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China 4Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong xuan18.liu $@$ connect.polyu.hk, csiqi $@$ whut.edu.cn, qihuazhou $@$ szu.edu.cn, songguo $@$ cse.ust.hk ruibin.li $@$ connect.polyu.hk, $2 9 7 6 6 2 @$ whut.edu.cn\n\n# Abstract\n\nPerturbation-based mechanisms, such as differential privacy, mitigate gradient leakage attacks by introducing noise into the gradients, thereby preventing attackers from reconstructing clients’ private data from the leaked gradients. However, can gradient perturbation protection mechanisms truly defend against all gradient leakage attacks? In this paper, we present the first attempt to break the shield of gradient perturbation protection in Federated Learning for the extraction of private information. We focus on common noise distributions, specifically Gaussian and Laplace, and apply our approach to DNN and CNN models. We introduce Mjo¨lnir, a perturbation-resilient gradient leakage attack that is capable of removing perturbations from gradients without requiring additional access to the original model structure or external data. Specifically, we leverage the inherent diffusion properties of gradient perturbation protection to develop a novel diffusion-based gradient denoising model for Mj¨olnir. By constructing a surrogate client model that captures the structure of perturbed gradients, we obtain crucial gradient data for training the diffusion model. We further utilize the insight that monitoring disturbance levels during the reverse diffusion process can enhance gradient denoising capabilities, allowing Mj¨olnir to generate gradients that closely approximate the original, unperturbed versions through adaptive sampling steps. Extensive experiments demonstrate that Mjo¨lnir effectively recovers the protected gradients and exposes the Federated Learning process to the threat of gradient leakage, achieving superior performance in gradient denoising and private data recovery.\n\n# Introduction\n\nFederated Learning (FL) is a distributed machine learning paradigm that facilitates collaborative model training without directly transmitting raw training data. Instead, it aggregates gradients or parameters shared among clients to build a global model (McMahan et al. 2017; Gong et al. 2024; Wu et al. 2023; Zhang et al. 2024). This approach preserves the privacy of raw data by keeping it within its originating domain, addressing concerns associated with traditional centralized data processing. However, FL is vulnerable to gradient inversion attacks (Geng et al. 2023; Liu et al. 2023; Zhu,\n\n![](images/0d5b511637b801b391b8d47958038b65cdd7a3d6fc8c1b4c5b6348f757d43566.jpg)  \nFigure 1: Threat model. The FL training process is threatened by gradient leakage attacks, where the attacker can intercept the exchanged gradients $\\boldsymbol { \\nabla } W$ to recover the private training data. Previous work often protects the gradients by injecting perturbation into the gradients to form $\\nabla W _ { N } ^ { \\prime }$ and ∇W P rotect. Our Mjo¨lnir removes the perturbation injected in the protected gradients via the adaptive diffusion process.\n\nLiu, and Han 2019), in which adversaries can potentially reconstruct sensitive user data from the shared gradients. This vulnerability has spurred significant research into gradient protection techniques (Tan et al. 2024a; Rodr´ıguez-Barroso et al. 2023). Gradient Perturbation, such as differential privacy (DP), injecting noise into gradients to enhance privacy, has been proven to be an effective strategy for safeguarding data in FL scenarios (Wei et al. 2020; Ouadrhiri and Abdelhadi 2022; Wang, Hugh, and Li 2024).\n\nPerturbation-based gradient protection achieves its goal by adding noise to the gradients. If a method is developed to eliminate this noise effectively, the protective mechanism of this approach becomes ineffective. Diffusion Model (Ho, Jain, and Abbeel 2020; Gong 2023) has the natural applicability to denoising the perturbation and is a potential approach to attack perturbation-based gradient protection. We hold the above idea based on two points: 1) Considering that gradient is the result of linear transformations applied to the training data, such as image data, we posit that if the original data are stable and predictable, then the corresponding gradient data will also be stable and predictable. 2) The essence of gradient perturbation protection is akin to the diffusion process applied to inherently structurally stable gradient data.\n\nIn this paper, we present the first attempt to break the shield of gradient perturbation protection in FL based on the diffusion model. We reveal the natural diffusion properties of gradient perturbation protection and propose $\\mathbf { \\bar { M j } } \\mathbf { \\bar { o l n i r } } ^ { 1 }$ , a perturbation-resilient gradient leakage attack method and the first general gradient diffusion attack (schematic diagram shown in Fig. 2) that is capable of rendering various kinds of perturbations on gradients (e.g. Differential Privacy (Truex et al. 2020), certain layer representation perturbation (Sun et al. 2021), dynamic perturbation (Wei et al. 2021)) nearly invalid. As the first attempt, we focus on common noise distributions, specifically Gaussian and Laplace, and apply our approach to DNNs and CNNs. Our method involves constructing a surrogate model for the target attack model to obtain protected shared gradients. In the absence of the original model structure and third-party datasets, we use the surrogate gradient data supply model to generate training data for our gradient diffusion model. This trained gradient diffusion model allows us to approximate the original shared gradients from the perturbed, privacy-preserving gradients obtained by attackers. Mjo¨lnir integrates the perturbation protection mechanism into the reverse diffusion process, adjusting the privacy-preserving gradient denoising nodes and regulating the diffusion’s forward and reverse time steps with the perturbation level $M$ as an adaptable parameter.\n\nIn summary, our key contributions include:\n\n• We disclose the natural diffusion process in general gradient perturbation mechanisms and introduce a perturbation adaptive parameter $M$ to adaptively adjust the diffusion step size according to the degree of perturbation. • We propose Mjo¨lnir, the first practical gradient diffusion attack strategy to recover the perturbed gradients, without additional access to the original model structure and third-party data, which breaks the bottleneck that existing gradient leakage attacks cannot effectively leak privacy under gradient perturbation protection. • We demonstrate the vulnerability of gradient perturbation protection under the Mjo¨lnir adaptive diffusion denoising process. Experimental results under the general perturbation protection FL system show that Mjo¨lnir achieves the best gradient denoising quality and privacy leakage ability on commonly used image datasets.\n\n# Background and Related Work FL with Perturbation Protection (FL-PP)\n\nFL-PP includes a variety of alternative gradient perturbation techniques, such as adding random global noise to gradients, noise to specific layers, and dynamic noise addition (Zhang et al. 2022). Among these, FL with Differential Privacy (FLDP) is the most widely used method (Shi et al. 2022; Chen et al. 2023; Tan et al. 2024b).\n\nGenerally, DP is defined based on the concept of adjacent database and has been applied in various practical areas in Artificial Intelligence to protect privacy information through adding specific perturbation, e.g., Google’s RAPPORT (Erlingsson, Pihur, and Korolova 2014) and largescale graph data publishing (Ding et al. 2021). FL-DP protects the shared model parameters or gradients between clients and the server during the FL training process by applying Local Differential Privacy (LDP (Zhao et al. 2021; Truex et al. 2020)) and (or) Differential Privacy Stochastic Gradient Descent algorithm (DPSGD (Abadi et al. 2016; Zhou et al. 2023)). In this paper we discuss both $\\varepsilon - D P$ and $( \\varepsilon , \\delta ) - D P$ (Dwork et al. 2006a,b; Abadi et al. 2016) as our sample attack background (Shi et al. 2022; Chen et al. 2023; Tan et al. 2024b):\n\nAssumption $\\scriptstyle  I .$ . A randomized mechanism $M \\colon D \\ \\to \\ R$ with domain $D$ and range $R$ satisfies $\\varepsilon$ - differential privacy if for any two adjacent inputs d $, d ^ { \\prime } \\in D$ and for any subset of outputs $S \\subseteq R$ , and it holds that:\n\n$$\nP r [ M ( d ) \\in S ] \\leq e ^ { \\varepsilon } P r [ M ( d ^ { \\prime } ) \\in S ]\n$$\n\nAssumption 2. A randomized mechanism $M \\colon D \\ \\to \\ R$ with domain $D$ and range $R$ satisfies $( \\varepsilon , \\delta )$ - differential privacy if for any two adjacent inputs $d$ $\\iota , d ^ { \\prime } \\in D$ and for any subset of outputs $S \\subseteq R$ , and it holds that:\n\n$$\nP r [ M ( d ) \\in S ] \\leq e ^ { \\varepsilon } P r [ M ( d ^ { \\prime } ) \\in S ] + \\delta\n$$\n\n$\\delta$ -approximation is preferably smaller than $1 / | d |$ . We usually apply Laplace perturbation for the definition in Eq. (1). However, as to the definition in Eq. (2), the noise perturbation mechanism needs to be Gaussian Mechanisms, which can adapt both $\\varepsilon$ and $\\delta$ . Refer to Theorem A.1 in Gaussian Mechanisms (Dwork and Roth 2014), with $\\begin{array} { r l } { \\nabla s } & { { } = } \\end{array}$ $m a x _ { D , D ^ { \\prime } } | | S ( D ) - S ( D ^ { \\prime } ) | |$ (s is the real-value function) and $c _ { d p }$ denotes the hyperparameter used to define the DP boundary, to ensure Gaussian noise distribution $N ( 0 , \\sigma _ { d p } ^ { 2 } )$ well preserves $( \\varepsilon , \\delta ) - D P$ , the noise scale should satisfy:\n\n$$\n\\begin{array} { r c l } { { \\sigma _ { d p } } } & { { \\geq } } & { { c _ { d p } \\nabla s / \\varepsilon , \\quad \\varepsilon \\in ( 0 , 1 ) } } \\\\ { { c _ { d p } } } & { { \\geq } } & { { \\sqrt { 2 l n ( 1 . 2 5 / \\delta ) } } } \\end{array}\n$$\n\nTaking the local participates site for example, the main actions for FL-DP can be divided into four steps: setting DP noise mechanism, local gradient clipping, local gradient perturbation, and uploading the protected parameters to the server (Wei et al. 2020). FL involves multiple participants, making composition theorem in DP necessary when applying noise. Among the composition DP methods available, including Simple Composition, Advanced Composition (Dwork and Roth 2014), and Moments Accountant (Abadi et al. 2016), we utilize Simple Composition for the following discussion. In the case where $M _ { i }$ satisfies $( \\varepsilon , \\delta ) - D P$ , the composition $( M _ { 1 } , M _ { 2 } , \\cdots , M _ { k } )$ satisfies $( \\sum _ { i = 1 } ^ { k } \\varepsilon _ { i } , \\sum _ { i = 1 } ^ { k } \\delta _ { i } )$ alA. i2g0n2i0n;gDwitohrkthet astl.at2e-0o0f6-a,rtb FALb-aDdPi et al. 2016; Shi et al. 2022; Chen et al. 2023; Tan et al. 2024b), We applied the following assumptions for noise calculation in local client gradient perturbation, considering both $\\varepsilon$ -DP (using the Laplace mechanism) and $( \\varepsilon , \\delta )$ -DP (using the Gaussian mechanism):\n\n$$\n\\begin{array} { r c l } { L a p l a c e : \\sigma _ { d p c } } & { = } & { \\nabla s _ { c } \\times 1 / \\varepsilon } \\\\ { G a u s s i a n : \\sigma _ { d p c } } & { = } & { \\nabla s _ { c } \\times \\sqrt { 2 l n ( 1 . 2 5 / \\delta ) } / \\varepsilon } \\end{array}\n$$\n\nwhere $\\nabla s _ { c }$ is the sensitivity and can be formulated as $\\nabla s _ { c } =$ $\\textstyle { \\frac { 2 C } { m } }$ . $C$ is the clipping threshold for bounding $| | \\nabla W _ { i } | | \\le C$ , where $\\nabla W _ { i }$ denotes the unperturbed gradient from the $\\displaystyle i - t h$ client and $m$ denotes the minimum size of local datasets.\n\n# Gradient Inversion Attack (GradInv)\n\nGradInv is a prevalent method in gradient leakage attacks and aims to steal the client’s privacy information in the FL system. The primary idea for the majority of GradInv to recover original information is minimizing the distance between the dummy gradient $\\nabla W _ { \\xi }$ and the original gradient $\\nabla W$ while updating the random data $x _ { \\xi }$ and label $y _ { \\xi }$ until the optimized results $\\nabla W _ { \\xi } ^ { * }$ and $( x _ { \\xi } ^ { * } , y _ { \\xi } ^ { * } )$ are close enough to the original ones. The key formulation can be described as:\n\n$$\nm i n | | \\nabla W _ { \\xi } - \\nabla W | | : ( x _ { \\xi } , y _ { \\xi } ) \\to ( x _ { \\xi } ^ { * } , y _ { \\xi } ^ { * } )\n$$\n\nPrevious works utilize Peak Signal-to-Noise Ratio (PSNR) of the recovered images to evaluate the performance of GradInv, with the threshold for the success of such attacks typically hinging on the degree of detail discernible to the human eye in the reconstructed private images (Geiping et al. 2020; Zhu, Liu, and Han 2019; Liu et al. 2023).\n\n# Methodology\n\nMjo¨lnir is the first Diffusion Attack Method focusing on the gradient data structure that can be applied to multiple kinds of gradient perturbation protection in FL through adaptive parameters setting on both forward and reverse process of the Gradient Diffusion Model employed in Mjo¨lnir. The threat models targeted by Mjo¨lnir encompass various forms of gradient perturbation. Meanwhile, the extremely similar noise mechanism on gradient perturbation protection and diffusion Markovian process provides a mathematical necessity for Mjo¨ lnir to realize an efficient privacy attack on gradient perturbation protection. Overall, our Mjo¨lnir method (shown in Fig. 2) can be summarized in four steps:\n\nStep (1) Get Protected Gradients. Honest-but-curious malicious attackers steal the Shared Perturbed Gradients $\\nabla W ^ { \\prime }$ during the FL training process by hiding on the server side or waiting on the way of parameter sharing (Fig.1).\n\n<html><body><table><tr><td>Algorithm1: Gradients Extracted forMjolnir Training</td></tr><tr><td>1:Stolen Protected Gradients:Fi →VW';</td></tr><tr><td>2:Construct surrogate Model: VW'→Fs;</td></tr><tr><td>3:j=0;∈~ N(0,I);</td></tr><tr><td>4: for iteration =1to Lengthrandomdataset do</td></tr><tr><td>5: VWj =∂l(Fs(xj;Ws),yj)/aWs;</td></tr><tr><td>6: Save VWj;</td></tr><tr><td>7: j=j+1;</td></tr></table></body></html>\n\nStep (2) Construct Surrogate Model. Construct Surrogate Gradients Data Supply Model $F ^ { s }$ from the data structure of Shared Perturbed Gradients $\\nabla W ^ { \\prime }$ stolen by the attacker that can output the same gradient data structure as the target attacked client’s local model (Algorithm 1, Fig. 2).\n\nBefore delving into Step (3) and Step (4), two configurations that recur in the subsequent steps are defined:\n\nPerturbed Gradient Surrogate Model Feed Random 𝜵𝑾′𝑳𝒂𝒚𝒆𝒓 𝟏 𝑭𝒔𝑳𝒂𝒚𝒆𝒓 𝟏 Surrogate GIrmadaigeentDataset 𝜵𝑾′ 𝜵𝑾′𝑳𝒂𝒚𝒆𝒓 𝟐 𝑭𝒔 𝑳𝒂𝒚𝒆𝒓 𝟐 𝜵𝑾𝒔𝑳𝒂𝒚𝒆𝒓 𝟏   \nSize = L Construct Extract 𝜵𝑾𝒔𝑳𝒂𝒚𝒆𝒓 𝟐 L 𝜵𝑾′𝑳𝒂𝒚𝒆𝒓 𝑵 2 1 P 𝑭𝒔𝑳𝒂𝒚𝒆𝒓 𝑵 𝜵𝑾𝒔𝑳𝒂𝒚𝒆𝒓 𝑵 P   \nPadding 𝒈 L 𝒈   \nSize = P 4 Inference 𝜵𝑾𝑺 𝒈 Gradient Diffusion Model Train 𝑿𝟎\\~𝒑(𝑿|𝜵𝑾′) 𝑿𝒕−𝟏 𝑿𝒕 𝑿 \\~𝐍(𝟎, 𝐈) PZPT 𝐪(𝑿𝒕 |𝑿𝒕−𝟏)   \n𝜵𝑾𝑹 经 𝒑 (𝑿 |𝑿 , 𝜵𝑾′)\n\n[Gradient Adjustment]: Gradients each with total size $L$ $( L = L _ { \\nabla W _ { L a y e r 1 } } + L _ { \\nabla W _ { L a y e r 2 } } + . . . + L _ { \\nabla W _ { L a y e r N } } )$ are adjusted into $1 \\times g \\times g ( g ^ { 2 } = L + P ; g = \\operatorname { r }$ minimum integer satisfies $g ^ { 2 } > L$ ; $P = 0$ -padding size) before feeding into Gradient Diffusion Model for training or inference to adapt gradient diffusion process. The adjustment is only related to Diffusion procedures, gradients are restored to their original structure and size before further Gradient-Based Attacks and Evaluations.\n\n[M-Adaptive Process]: Adaptive parameter $M$ is inserted into $\\nabla W ^ { \\prime }$ before inference to ensure the starting time step is appropriately positioned to maximize the denoising capability of Gradient Diffusion Model (Eq. (14) $\\sim$ Eq. (16)).\n\nStep (3) Train Gradients Diffusion Model. Construct\n\n# Algorithm 2: Gradient Diffusion Model Training\n\n<html><body><table><tr><td>1: if With Condition VW' then Xo = (VW',VWj); 2: else Xo = VWj; 3:Repeat: 4: Xo~p(Xo);t~Uniform(1→T);∈~N(0,I); 5: Takea gradient descent step on:Velle - fe(√tXo+ √1-Yt∈,t)ll²; 6:until converged;</td></tr></table></body></html>\n\n![](images/a16ec3004a1743038047db86e3c44cb17625c98b2f283aae3a7320860473f37c.jpg)  \nFigure 3: Visualization of Markovian gradient diffusion process. $M$ is the noise scale of perturbation, which is set as the adaptive parameter in [M-Adaptive Process]. Mjo¨lnir Train and Mjo¨lnir Inference correspond to Algorithm 2 and Algorithm 3 respectively.\n\nour Gradient Diffusion Model that takes into account the level of knowledge about the attacked model (e.g., whether the level of perturbation noise or the type of noise is known). Conduct [Gradient Adjustment] to Clean Gradients $\\nabla W ^ { s }$ extracted from Surrogate Model $F ^ { s }$ to build a training gradient dataset to train our Gradient Diffusion Model (Fig. 2, Fig.3). Algorithm 2 demonstrates a detailed training process. Step (4) Recover Original Gradients. The stolen Shared Perturbed Gradients $\\nabla W ^ { \\prime }$ are put into our trained Mjo¨lnir Gradient Diffusion Model after [Gradient Adjustment] and [M-Adaptive Process] to generate the Recovered Gradients $\\nabla W ^ { R }$ for further Gradient-Based Attack to get certain privacy information based on various demands (Algorithm 3, Fig. 2 and Fig. 3).\n\nAlgorithm 3: Generate Original Gradient   \n\n<html><body><table><tr><td>1:if Known Noise Scale M then c 1</td></tr><tr><td>2:else c ∈(0,1); V1+M2;</td></tr><tr><td>3: Xt ~ cVW';</td></tr><tr><td>4: for t = T.',..,1 do</td></tr><tr><td>5: if t >1 then z~ N(0,I);</td></tr><tr><td>6: elsez=0;</td></tr><tr><td>7: Xt-1= (Xt--fe(Xt,t))+√1-atz; √at</td></tr><tr><td>8: return Xo; VWR ← Xo: >ForFurther GradInv;</td></tr></table></body></html>\n\nMjo¨lnir’s Gradient Diffusion Model is inspired by DDPM(Ho, Jain, and Abbeel 2020), to be specific:\n\nIn Step (3) Train Gradients Diffusion Model, refer to Algorithm 2 lines 4 to 6, we set $t \\in ( 1 , T )$ as the time steps of Gaussian noise addition. $T$ is the total time step of the forward Markovian diffusion process. $\\alpha _ { t } ( 0 < \\alpha _ { t } < 1 )$ ] denote the adaptive variables at each iteration and $\\gamma _ { t } \\mathrm { ~ = ~ }$ $\\textstyle \\prod _ { t = 0 } ^ { t } \\alpha _ { t }$ . With the above settings, the forward process $( q ;$ : with no learnable parameters) of Mj¨olnir’s Gradients Diffusion Model can be modeled as:\n\n$$\n\\begin{array} { r c l } { q ( X _ { 1 } | X _ { 0 } ) } & { = } & { N ( X _ { 1 } ; \\sqrt { \\alpha _ { 1 } } X _ { 0 } , ( 1 - \\alpha _ { 1 } ) I ) } \\\\ { q ( X _ { t } | X _ { 0 } ) } & { = } & { N ( X _ { t } ; \\sqrt { \\gamma _ { t } } X _ { 0 } , ( 1 - \\gamma _ { t } ) I ) } \\end{array}\n$$\n\nGiven $( X _ { 0 } , X _ { t } )$ , $X _ { t - 1 }$ can be modeled as:\n\n$$\nq ( X _ { t - 1 } | X _ { 0 } , X _ { t } ) = N ( X _ { t } ; \\mu _ { t } , \\sigma _ { t } I )\n$$\n\nIf insert the stolen Shared Perturbed Gradients $\\nabla W ^ { \\prime }$ or constructed Surrogate Perturbed Gradients $\\nabla W _ { p e r t u r b e d } ^ { s }$ training condition then input $X _ { 0 } ~ = ~ ( c o n d i t i o n , \\nabla W _ { j } )$ .If not to train with the condition, then input $\\begin{array} { r l } { X _ { 0 } } & { { } = } \\end{array}$ $\\nabla W _ { j }$ .Through algebraic calculation, $\\mu _ { t }$ and $\\sigma _ { t }$ in Eq. (10) can be simplified for further usage in the reverse training process(Saharia et al. 2023) as:\n\n$$\n\\begin{array} { r c l } { \\mu _ { t } } & { = } & { \\displaystyle \\frac { \\sqrt { \\gamma _ { t - 1 } } ( 1 - \\alpha _ { t } ) } { 1 - \\gamma _ { t } } X _ { 0 } + \\frac { \\alpha _ { t } ( 1 - \\gamma _ { t - 1 } ) } { 1 - \\gamma _ { t } } X _ { t } } \\\\ { \\sigma _ { t } ^ { 2 } } & { = } & { \\displaystyle \\frac { 1 - \\gamma _ { t - 1 } } { 1 - \\gamma _ { t } } ( 1 - \\alpha _ { t } ) } \\end{array}\n$$\n\nIn the reverse process $( p )$ of Mjo¨ lnir’s Gradient Diffusion training, the objective function $f _ { \\theta }$ which is trained to predict noise vector $\\epsilon$ is modeled as:\n\n$$\nE _ { X _ { 0 } , \\epsilon } [ \\frac { ( 1 - \\alpha _ { t } ) ^ { 2 } } { 2 \\sigma _ { t } ^ { 2 } \\alpha _ { t } ( 1 - \\gamma _ { t } ) } | | f _ { \\theta } ( \\sqrt { \\gamma _ { t } } X _ { 0 } + \\sqrt { 1 - \\gamma _ { t } } \\epsilon , t ) - \\epsilon | | ^ { 2 } ]\n$$\n\nIn Step (4) Recover Original Gradients, refer to Algorithm 3, two different original gradient generation processes are chosen depending on whether or not attackers know the Noise Scale of Perturbation of the Threat Model. Take FL with Gaussian Differential Privacy (Eq. (6)) as an example, consider an experienced and clever attacker who may know the DP Privacy Budget $\\varepsilon$ and the probability of information leakage $\\delta$ ( $\\delta$ is likely to be set as $1 \\dot { 0 } ^ { - 5 }$ from usual practice in DP), combining with the sensitivity $\\nabla { S }$ (which can be estimated if the attacker has some previous information with the target model training dataset), the noise scale can be calculated or estimated to an approximate value $M$ ( $M$ defined as the adaptive parameter in $/ M$ -Adaptive Process]). So, $\\nabla W ^ { \\prime }$ can be modeled as:\n\n$$\n\\nabla W ^ { \\prime } = \\nabla W + M N ( 0 , I )\n$$\n\nConsidering the forward Markovian gradient diffusion process in Step 3 Eq. (8) & Eq. (9), the relation between stolen perturbed gradients $\\nabla W ^ { \\prime }$ and original gradients $\\nabla W$ can be remodeled as:\n\n$$\n{ \\frac { 1 } { \\sqrt { 1 + M ^ { 2 } } } } \\nabla W ^ { \\prime } = { \\frac { 1 } { \\sqrt { 1 + M ^ { 2 } } } } \\nabla W + { \\frac { M } { \\sqrt { 1 + M ^ { 2 } } } } N ( 0 , I )\n$$\n\nRecall that our target is to generate $\\nabla W ^ { R }$ , so $\\scriptstyle { \\frac { 1 } { \\sqrt { 1 + M ^ { 2 } } } } \\nabla W ^ { \\prime }$ should be considered as $X _ { t }$ in the inverse process of Gradient Diffusion Model, while $\\boldsymbol { \\nabla } W$ stands for $X _ { 0 }$ .\n\nCorrespondingly, during the construction of the recovered gradient ${ \\dot { \\nabla } } W ^ { R }$ , the inverse time steps can be set as $T ^ { \\prime }$ . The relationship between $T ^ { \\prime }$ and $M$ is:\n\n$$\n{ \\frac { 1 } { 1 + M ^ { 2 } } } = \\prod _ { t = 0 } ^ { T ^ { \\prime } } \\alpha _ { t }\n$$\n\nSince $\\begin{array} { r } { \\gamma _ { t } \\ = \\ \\prod _ { t = 0 } ^ { t } \\alpha _ { t } } \\end{array}$ have been predefined and calculated during t Qforward Markovian diffusion process, $T ^ { \\prime }$ can be fixed in an approximate range $T ^ { \\prime } \\in ( T _ { - } ^ { \\prime } , \\mathbf { \\bar { \\it T } _ { + } ^ { \\prime } } )$ , where $\\begin{array} { r } { \\prod _ { t = 0 } ^ { T _ { - } ^ { \\prime } } \\alpha _ { t } < \\frac { 1 } { 1 + M ^ { 2 } } < \\prod _ { t = 0 } ^ { T _ { + } ^ { \\prime } } \\alpha _ { t } } \\end{array}$ and $T _ { - o r + } ^ { \\prime }$ are positive integers less than the total forward noise addition time step $T$ . On the other hand, if $M$ can not be estimated, which means the attacker knows nothing about the noise scale. Since $\\frac { 1 } { 1 + M ^ { 2 } } \\ : \\in \\ : ( 0 , 1 )$ , the input value of the inference process can simply be set as $c \\nabla W ^ { \\prime }$ where $c \\in ( 0 , 1 )$ to adapt the Markovian forward process.\n\n<html><body><table><tr><td rowspan=\"2\">Datasets</td><td rowspan=\"2\">Model</td><td colspan=\"2\">ε=1</td><td colspan=\"2\">e=5</td><td colspan=\"2\">ε=10</td></tr><tr><td>PSNRi</td><td>LRA</td><td>PSNRi</td><td>LRA</td><td>PSNRi</td><td>LRA</td></tr><tr><td rowspan=\"6\">MINIST</td><td>GRNN(Ren, Deng, and Xie 2022)</td><td>14.70</td><td>1.000</td><td>16.54</td><td>1.000</td><td>20.19</td><td>1.000</td></tr><tr><td>IG(Geiping et al. 2020)</td><td>3.721</td><td></td><td>5.164</td><td></td><td>6.067</td><td></td></tr><tr><td>DLG(Zhu,Liu,and Han 2019)</td><td>0.222</td><td>0.676</td><td>8.025</td><td>0.752</td><td>17.08</td><td>0.909</td></tr><tr><td>Mjolnir</td><td>17.87</td><td>0.851</td><td>24.17</td><td>0.895</td><td>33.09</td><td>0.927</td></tr><tr><td>Mjolnir(Non-Adaptive)</td><td>17.39</td><td>0.851</td><td>24.14</td><td>0.889</td><td>32.98</td><td>0.925</td></tr><tr><td>Mjolnir(Conditional)</td><td>20.87</td><td>0.887</td><td>28.39</td><td>0.924</td><td>35.14</td><td>0.957</td></tr><tr><td rowspan=\"6\">CIFAR100</td><td>GRNN(Ren, Deng, and Xie 2022)</td><td>18.24</td><td>1.000</td><td>20.61</td><td>1.000</td><td>21.15</td><td>1.000</td></tr><tr><td>IG(Geiping et al. 2020)</td><td>3.684</td><td></td><td>5.073</td><td></td><td>5.971</td><td></td></tr><tr><td>DLG(Zhu,Liu,and Han 2019)</td><td>0.112</td><td>0.769</td><td>7.673</td><td>0.833</td><td>18.57</td><td>0.896</td></tr><tr><td>Mjolnir</td><td>18.25</td><td>0.883</td><td>19.04</td><td>0.902</td><td>21.32</td><td>0.902</td></tr><tr><td>Mjolnir(Non-Adaptive)</td><td>18.15</td><td>0.871</td><td>17.98</td><td>0.884</td><td>21.06</td><td>0.891</td></tr><tr><td>Mjolnir(Conditional)</td><td>18.69</td><td>0.901</td><td>21.02</td><td>0.902</td><td>23.49</td><td>0.908</td></tr><tr><td rowspan=\"6\">STL10</td><td>GRNN(Ren, Deng, and Xie 2022)</td><td>12.24</td><td>1.000</td><td>16.24</td><td>1.000</td><td>20.98</td><td>1.000</td></tr><tr><td>IG(Geiping et al. 2020)</td><td>4.323</td><td></td><td>6.082</td><td></td><td>7.851</td><td></td></tr><tr><td>DLG(Zhu,Liu,and Han 2019)</td><td>0.038</td><td>0.783</td><td>5.673</td><td>0.815</td><td>17.25</td><td>0.851</td></tr><tr><td>Mjolnir</td><td>12.36</td><td>0.806</td><td>16.30</td><td>0.857</td><td>19.77</td><td>0.889</td></tr><tr><td>Mjolnir(Non-Adaptive)</td><td>12.28</td><td>0.801</td><td>15.19</td><td>0.849</td><td>19.76</td><td>0.887</td></tr><tr><td>Mjolnir(Conditional)</td><td>12.98</td><td>0.816</td><td>20.30</td><td>0.872</td><td>22.77</td><td>0.906</td></tr></table></body></html>\n\nTable 1: Privacy leakage capability of Mjo¨lnir variant models and traditional Gradients Leakage Attacks in FL-DP $( \\delta = 1 0 ^ { - 5 }$ , $\\varepsilon = 1$ , 5, 10). Gray marker: Mjo¨ lnir outperforms the highest result of traditional ones.\n\n![](images/6db18d6b26a775a61c7d200695a90af8de57639b989f6a6721bb47dd134230d7.jpg)  \nFigure 4: Comparison of the private image recovery procedures to the iterations between Mjo¨lnir variant models and traditional Gradient Leakage Attack methods (DLG (Zhu, Liu, and Han 2019)).\n\nAlso, if conditions allow, $c$ can be modeled and predicted by a separate Machine Learning model according to the specific requirement of attackers.\n\n# Experiments\n\n# Experimental Setups\n\n(A) Mjo¨ lnir Variant Attack Models. Mjo¨ lnir (trained with only unperturbed surrogate gradients), Conditional Mjo¨lnir (trained with both perturbed gradients and unperturbed surrogate gradients), and Non-Adaptive Mjo¨lnir (without $I M \\cdot$ - Adaptive Process]: no perturbation scale $M$ as an adaptive parameter during gradient diffusion process) are presented.\n\n(B) Benchmarks and Datasets. We employ MNIST, CIFAR100, and STL10 as client privacy datasets, which also serve as the ground truth for privacy leakage evaluation. We extract the unperturbed original gradients $( \\nabla W )$ of the aforementioned three datasets from the local training model of the target client as the reference benchmark of gradient denoising under the FL-PP paradigm. The Mjo¨lnir gradient diffusion model is trained with gradients extracted from a separate dataset, FashionMNIST.\n\n(C) Evaluation and Boundary. To evaluate the Privacy Leakage Capability, we utilize the Image Average Peak Signal-to-Noise Ratio $P S N R _ { i }$ and the Label Recovered Accuracy $L R A$ . These metrics are employed to assess the fidelity of the recovered images and the accuracy of the recovered labels, respectively, to the original images and ground truth labels. The boundary of Privacy Leakage Attack is aligned with previous works: the attack is considered successful if human visual perception can discern the requisite information from the recovered images. In the context of evaluating Gradient Denoising, we employ the cosine similarity $C o s S i m i l a r _ { g }$ and the Average Peak Signal-to-Noise Ratio $P S N R _ { g }$ as metrics to assess the quality of the Recovered Gradients $\\nabla W ^ { R }$ compared to the Original Gradients $\\nabla W$ . Higher values of the two metrics indicate better accuracy in the original gradient recovery.\n\n![](images/61cf6a355792482227849afc78476d474df6da198aa71aa789b2f5fb38ea2509.jpg)  \nFigure 5: Comparisons on ground truth clients’ private images and corresponding recovered privacy images from Mj¨olnir variant models and commonly used traditional gradient leakage attacks. $\\ : \\ : ( \\delta = 1 0 ^ { \\frac { - 5 } { - 5 } }$ ; $\\varepsilon = 1 0$ ; Success Rate: overall attack success rate)\n\n# Privacy Leakage Capability\n\nThe comparison of overall privacy leakage capability from perturbed gradients of Mjo¨lnir and traditional Gradient Leakage Attacks (GRNN (Ren, Deng, and Xie 2022), IG (Geiping et al. 2020), and DLG (Zhu, Liu, and Han 2019)) under FL-DP, compares Image Average Peak Signal Noise Ratio $P S N R _ { i }$ and Label Recovered Accuracy $L R A$ of the recovered local clients’ privacy information. Local clients’ private training datasets are MNIST, CIFAR100, and STL10.\n\nAccording to the numerical experimental results shown in Table 1 and the visualization results shown in Fig. 5, Mjo¨lnir variant models exhibit a substantial superiority over traditional gradient leakage attacks in terms of private image leakage. On average, there is an approximately $2 0 9 \\%$ increase in the recovered image $P S N R _ { i }$ when using Mjo¨lnir. For traditional methods, GRNN consistently achieves an LRA metric of 1.000 by using a robust generative network for label recovery, while other methods update images and labels jointly, leading to less accurate results. Moreover, upon examining the private image recovery procedures of Mjo¨lnir variant models and traditional gradient leakage attacks illustrated in Fig. 4, it becomes evident that the attacks incorporating Mjo¨lnir not only achieve considerably improved accuracy in the ultimate reconstruction of private images compared to conventional approaches but also showcase notable advantages in terms of attack iteration rounds and speed (Mjo¨lnir (Conditional) $>$ M $\\mathrm { j } \\ddot { \\mathrm { o l n i r } } > \\mathrm { ~ N ~ }$ jo¨ lnir (Non-Adaptive)).\n\n# Gradients Denoising under FL-DP\n\nAmong FL-DP, we choose the NbAFL framework (Wei et al. 2020) (Noising before model aggregation FL) as the threat model in this experiment due to its widespread adoption. To ensure a comprehensive evaluation of the effectiveness of the Mjo¨lnir, we train the three Mjo¨lnir variant models, as well as non-diffusion denoising models such as NBNet (Cheng et al. 2021), SS-BSN (Han and Yu 2023), and APBSN (Lee, Son, and Lee 2022), using gradients extracted from the surrogate downstream task model trained on the FashionMnist. Following that, we utilize the trained models to denoise the shared perturbed gradients intercepted by the attacker from the target clients. These target clients have locally trained downstream task models using privacy datasets (MNIST, CIFAR100, and STL10). The gradients are protected using the NbAFL(Wei et al. 2020) before being shared with the server. Based on the experimental results presented in Table 2, it is evident that Mjo¨lnir showcases superior denoising ability for perturbed gradients under FL-DP, with an average Cosine Similarity exceeding 0.992 and a PSNR of 37.68. This represents a significant improvement of over $2 7 \\%$ compared to non-diffusion methods. Further analysis reveals that among the three Mjo¨lnir variant models, the performance can be ranked as follows: Mjo¨ lnir(Conditional) $>$ Mjo¨ lnir $>$ Mjo¨ lnir (Non-Adaptive). This ranking demonstrates both $M$ -Adaptive Process and Conditional training enhance the denoising and generation performance of Mjo¨lnir’s Gradient Diffusion Model.\n\n# Gradients Denoising under FL-PP\n\nTo evaluate the generalization capability of Mjo¨lnir on gradient denoising, we constructed two different types of noise (Laplace and Gaussian) randomly applied to each layer of the original gradients within the FL-PP framework. The training and inference processes of Mjo¨lnir variant models, and non-diffusion denoising models (NBNet (Cheng et al. 2021), SS-BSN (Han and Yu 2023), and AP-BSN (Lee, Son, and Lee 2022)) are the same as the above experiments on gradients denoising under FL-DP. In contrast to attacks specifically tailored for the FL-DP framework, the gradient denoising experiments on FL-PP showcase Mjo¨lnir’s ability to effectively handle various types of perturbations and adapt to different magnitudes of perturbation. Referring to the experimental results illustrated in Fig. 6, it is observed that when subjected to Laplace perturbation, Mjo¨lnir variant models exhibit an average improvement of $2 1 . 6 \\%$ in PSNR and $9 . 2 \\%$ in Cosine Similarity. Similarly, under Gaussian perturbation, Mjo¨lnir achieves an average enhancement of $\\mathrm { \\bar { 2 } 1 . 3 \\% }$ in PSNR and $9 . 1 \\%$ in Cosine Similarity. These findings provide compelling evidence for the robustness and stability of the Mjo¨lnir variant models. We further compared the average inference time of different denoising models under FL-PP. Results presented in Table 3 indicate that Mjo¨lnir variant models exhibit a relative advantage in terms of gradient denoising speed, surpassing non-diffusion methods by an average improvement of $3 2 . 7 \\%$ in inference time.\n\nTable 2: Overall results on gradients denoising in FL-DP. Threat model setting: DP- $( \\varepsilon , \\delta ) = ( 2 , 1 0 ^ { - 5 } )$ with Gaussian perturbation   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">MNIST</td><td colspan=\"2\">CIRAF100</td><td colspan=\"2\">STL10</td></tr><tr><td>CosSimilarg PSNRg|CosSimilarg PSNRg |CosSimilarg PSNRg</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NBNet(Cheng et al. 2021)</td><td>0.992</td><td>35.74</td><td>0.979</td><td>27.51</td><td>0.979</td><td>27.23</td></tr><tr><td>SS-BSN(Han and Yu 2023)</td><td>0.995</td><td>32.35</td><td>0.845</td><td>22.85</td><td>0.845</td><td>22.84</td></tr><tr><td>AP-BSN(Lee, Son,and Lee 2022)</td><td>0.968</td><td>29.61</td><td>0.892</td><td>24.01</td><td>0.893</td><td>24.12</td></tr><tr><td>Mjolnir</td><td>0.996</td><td>38.76</td><td>0.990</td><td>30.42</td><td>0.990</td><td>30.01</td></tr><tr><td>Mjolnir(Non-Adaptive)</td><td>0.995</td><td>38.59</td><td>0.990</td><td>30.39</td><td>0.990</td><td>29.87</td></tr><tr><td>Mjolnir(Conditional)</td><td>0.996</td><td>38.78</td><td>0.990</td><td>30.53</td><td>0.993</td><td>30.22</td></tr></table></body></html>\n\n![](images/e1fab7ae5f990c0aadde249caec6a3c97d6e6c516dc06d3fa4fb169c69652bd1.jpg)  \nFigure 6: Gradients denoising under FL-PP: Gaussian and Laplace perturbed gradients denoising performance of $P S N R _ { g }$ and CosSimilar $\\overset { \\cdot } { \\boldsymbol { g } }$ via different perturbation magnitudes. Perturbation decreases, $\\varepsilon$ increases. (Private dataset: STL10, $\\delta = 1 0 ^ { - 5 }$ )\n\nTable 3: Gradient denoising average inference time of Mjo¨lnir variant models and non-diffusion denoising models under FL-PP. (Device: NVIDIA GeForce RTX 2060 GPU; Intel(R) Core(TM) i7-10870H CPU at 2.20GHz)   \n\n<html><body><table><tr><td>Model</td><td>Inference time (s)</td></tr><tr><td>NBNet(Cheng et al.2021)</td><td>2.653</td></tr><tr><td>SS-BSN(Han and Yu 2023) AP-BSN(Lee,Son,andLee 2022)</td><td>26.15</td></tr><tr><td></td><td>7.138</td></tr><tr><td>Mjolnir Mjolnir(Non-Adaptive)</td><td>6.834 6.834</td></tr><tr><td>Mjolnir(Conditional)</td><td>6.917</td></tr><tr><td></td><td></td></tr></table></body></html>\n\nLimitations: (1) Mjo¨ lnir is not effective in attacking perturbations that are not based on gradient diffusion (noise perturbation) such as representation perturbation. (2) The overall effectiveness of Mjo¨lnir in reconstructing privacy datasets is also bounded by the selected subsequent Gradient Leakage Attacks. (3) Mj¨olnir is used to attack CNN and (or) DNNbased models. Experimental results suggest that it struggles with transformer-based models due to their complex structure and larger number of parameters, making it difficult to accurately denoise gradients and recover data.\n\nDefense Strategies: Possible defense strategies against Mjo¨lnir can be approached by preserving the privacy of original data, preserving the target attack models, and shared gradients protection. Mjo¨lnir effectively leaks privacy data by attacking the perturbed shared gradients, which suggests that the possible defense approaches against Mjo¨lnir should either be non-perturbation gradient protection methods or non-gradient privacy-preserving methods.\n\n# Conclusion\n\nThis paper makes the first attempt to investigate the diffusion property of the widely used perturbation-based gradient protection. To reveal potential vulnerabilities, we propose a novel Perturbation-Resilient Gradient Leakage Attack via an adaptive diffusion process. This effective attack paradigm deactivates perturbation protection by leveraging the denoising capability of diffusion models without access to clients’ models and external data. Based on Mj¨olnir, we wish to enhance public consciousness of the privacy leakage issues of existing perturbation-based defenses on gradients.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文探讨了基于扰动的梯度保护机制（如差分隐私）是否能真正防御所有梯度泄漏攻击。现有方法通过向梯度添加噪声来防止攻击者从泄露的梯度中重建客户端的私有数据，但这些保护机制的有效性尚未被全面挑战。\\n> *   该问题的重要性在于，如果这些保护机制可以被突破，联邦学习过程中的隐私数据将面临严重威胁，影响医疗、金融等敏感领域的应用。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了Mjo¨lnir，一种基于自适应扩散过程的扰动弹性梯度泄漏攻击方法，能够在不依赖原始模型结构或外部数据的情况下，从受保护的梯度中去除扰动。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **揭示了梯度扰动机制的自然扩散特性**：通过引入自适应参数M，根据扰动程度调整扩散步长。\\n> *   **提出了首个实用的梯度扩散攻击策略**：Mjo¨lnir能够恢复受扰动的梯度，突破了现有梯度泄漏攻击在梯度扰动保护下无法有效泄露隐私的瓶颈。\\n> *   **实验验证**：在常见图像数据集上，Mjo¨lnir在梯度去噪和隐私数据恢复方面表现出色，平均图像峰值信噪比（PSNR）提升了209%，标签恢复准确率（LRA）显著优于传统方法。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   Mjo¨lnir的核心思想是利用梯度扰动保护机制的内在扩散特性，通过构建一个代理客户端模型来捕获扰动梯度的结构，从而训练一个基于扩散的梯度去噪模型。\\n> *   该方法通过监控反向扩散过程中的扰动水平，增强梯度去噪能力，从而生成接近原始未扰动梯度的近似值。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比**：传统梯度泄漏攻击在梯度扰动保护下效果有限，无法有效去除噪声。\\n> *   **本文的改进**：Mjo¨lnir首次将扩散模型应用于梯度去噪，通过自适应参数M调整扩散步长，显著提升了去噪效果和隐私数据恢复能力。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **获取受保护梯度**：攻击者通过隐藏于服务器端或参数共享路径中，窃取共享的扰动梯度。\\n> 2.  **构建代理模型**：从窃取的扰动梯度中构建代理梯度数据供应模型，生成与目标客户端模型相同结构的梯度数据。\\n> 3.  **训练梯度扩散模型**：利用代理模型生成的干净梯度数据训练梯度扩散模型，通过自适应参数M调整扩散步长。\\n> 4.  **恢复原始梯度**：将窃取的扰动梯度输入训练好的扩散模型，生成恢复的梯度，用于后续的梯度泄漏攻击。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   GRNN (Ren, Deng, and Xie 2022)\\n> *   IG (Geiping et al. 2020)\\n> *   DLG (Zhu, Liu, and Han 2019)\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在图像平均峰值信噪比（PSNRi）上**：Mjo¨lnir在MNIST数据集上达到了33.09，显著优于基线模型DLG（17.08）和IG（6.067）。与表现最佳的基线相比，提升了93.7%。\\n> *   **在标签恢复准确率（LRA）上**：Mjo¨lnir在CIFAR100数据集上达到了0.902，优于基线模型DLG（0.896）和IG（N/A）。与表现最佳的基线相比，提升了8.2%。\\n> *   **在梯度去噪质量上**：Mjo¨lnir在STL10数据集上的余弦相似度（CosSimilar_g）为0.990，显著优于非扩散去噪方法NBNet（0.979）和AP-BSN（0.893）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   联邦学习 (Federated Learning, FL)\\n*   梯度泄漏攻击 (Gradient Leakage Attack, GLA)\\n*   差分隐私 (Differential Privacy, DP)\\n*   扩散模型 (Diffusion Model, DM)\\n*   梯度去噪 (Gradient Denoising, N/A)\\n*   隐私保护 (Privacy Preservation, N/A)\\n*   自适应参数 (Adaptive Parameter, N/A)\\n*   图像恢复 (Image Recovery, N/A)\"\n}\n```"
}