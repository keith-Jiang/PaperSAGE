{
    "source": "Semantic Scholar",
    "arxiv_id": "2407.05285",
    "link": "https://arxiv.org/abs/2407.05285",
    "pdf_link": "https://arxiv.org/pdf/2407.05285.pdf",
    "title": "Mjolnir: Breaking the Shield of Perturbation-Protected Gradients via Adaptive Diffusion",
    "authors": [
        "Xuan Liu",
        "Siqi Cai",
        "Qihua Zhou",
        "Song Guo",
        "Ruibin Li",
        "Kai Lin"
    ],
    "publication_date": "2024-07-07",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "paper_content": "# Mjo¨lnir: Breaking the Shield of Perturbation-Protected Gradients via Adaptive Diffusion\n\nXuan Liu1, Siqi $\\mathbf { C a i } ^ { 2 }$ , Qihua Zhou3, Song $\\mathbf { G u o ^ { 4 ^ { * } } }$ , Ruibin $\\mathbf { L i } ^ { 1 }$ , Kaiwei Lin2\n\n1The Hong Kong Polytechnic University, Hong Kong 2Hubei Key Laboratory of Transportation Internet of Things, Wuhan University of Technology, Wuhan, China 3College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China 4Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong xuan18.liu $@$ connect.polyu.hk, csiqi $@$ whut.edu.cn, qihuazhou $@$ szu.edu.cn, songguo $@$ cse.ust.hk ruibin.li $@$ connect.polyu.hk, $2 9 7 6 6 2 @$ whut.edu.cn\n\n# Abstract\n\nPerturbation-based mechanisms, such as differential privacy, mitigate gradient leakage attacks by introducing noise into the gradients, thereby preventing attackers from reconstructing clients’ private data from the leaked gradients. However, can gradient perturbation protection mechanisms truly defend against all gradient leakage attacks? In this paper, we present the first attempt to break the shield of gradient perturbation protection in Federated Learning for the extraction of private information. We focus on common noise distributions, specifically Gaussian and Laplace, and apply our approach to DNN and CNN models. We introduce Mjo¨lnir, a perturbation-resilient gradient leakage attack that is capable of removing perturbations from gradients without requiring additional access to the original model structure or external data. Specifically, we leverage the inherent diffusion properties of gradient perturbation protection to develop a novel diffusion-based gradient denoising model for Mj¨olnir. By constructing a surrogate client model that captures the structure of perturbed gradients, we obtain crucial gradient data for training the diffusion model. We further utilize the insight that monitoring disturbance levels during the reverse diffusion process can enhance gradient denoising capabilities, allowing Mj¨olnir to generate gradients that closely approximate the original, unperturbed versions through adaptive sampling steps. Extensive experiments demonstrate that Mjo¨lnir effectively recovers the protected gradients and exposes the Federated Learning process to the threat of gradient leakage, achieving superior performance in gradient denoising and private data recovery.\n\n# Introduction\n\nFederated Learning (FL) is a distributed machine learning paradigm that facilitates collaborative model training without directly transmitting raw training data. Instead, it aggregates gradients or parameters shared among clients to build a global model (McMahan et al. 2017; Gong et al. 2024; Wu et al. 2023; Zhang et al. 2024). This approach preserves the privacy of raw data by keeping it within its originating domain, addressing concerns associated with traditional centralized data processing. However, FL is vulnerable to gradient inversion attacks (Geng et al. 2023; Liu et al. 2023; Zhu,\n\n![](images/0d5b511637b801b391b8d47958038b65cdd7a3d6fc8c1b4c5b6348f757d43566.jpg)  \nFigure 1: Threat model. The FL training process is threatened by gradient leakage attacks, where the attacker can intercept the exchanged gradients $\\boldsymbol { \\nabla } W$ to recover the private training data. Previous work often protects the gradients by injecting perturbation into the gradients to form $\\nabla W _ { N } ^ { \\prime }$ and ∇W P rotect. Our Mjo¨lnir removes the perturbation injected in the protected gradients via the adaptive diffusion process.\n\nLiu, and Han 2019), in which adversaries can potentially reconstruct sensitive user data from the shared gradients. This vulnerability has spurred significant research into gradient protection techniques (Tan et al. 2024a; Rodr´ıguez-Barroso et al. 2023). Gradient Perturbation, such as differential privacy (DP), injecting noise into gradients to enhance privacy, has been proven to be an effective strategy for safeguarding data in FL scenarios (Wei et al. 2020; Ouadrhiri and Abdelhadi 2022; Wang, Hugh, and Li 2024).\n\nPerturbation-based gradient protection achieves its goal by adding noise to the gradients. If a method is developed to eliminate this noise effectively, the protective mechanism of this approach becomes ineffective. Diffusion Model (Ho, Jain, and Abbeel 2020; Gong 2023) has the natural applicability to denoising the perturbation and is a potential approach to attack perturbation-based gradient protection. We hold the above idea based on two points: 1) Considering that gradient is the result of linear transformations applied to the training data, such as image data, we posit that if the original data are stable and predictable, then the corresponding gradient data will also be stable and predictable. 2) The essence of gradient perturbation protection is akin to the diffusion process applied to inherently structurally stable gradient data.\n\nIn this paper, we present the first attempt to break the shield of gradient perturbation protection in FL based on the diffusion model. We reveal the natural diffusion properties of gradient perturbation protection and propose $\\mathbf { \\bar { M j } } \\mathbf { \\bar { o l n i r } } ^ { 1 }$ , a perturbation-resilient gradient leakage attack method and the first general gradient diffusion attack (schematic diagram shown in Fig. 2) that is capable of rendering various kinds of perturbations on gradients (e.g. Differential Privacy (Truex et al. 2020), certain layer representation perturbation (Sun et al. 2021), dynamic perturbation (Wei et al. 2021)) nearly invalid. As the first attempt, we focus on common noise distributions, specifically Gaussian and Laplace, and apply our approach to DNNs and CNNs. Our method involves constructing a surrogate model for the target attack model to obtain protected shared gradients. In the absence of the original model structure and third-party datasets, we use the surrogate gradient data supply model to generate training data for our gradient diffusion model. This trained gradient diffusion model allows us to approximate the original shared gradients from the perturbed, privacy-preserving gradients obtained by attackers. Mjo¨lnir integrates the perturbation protection mechanism into the reverse diffusion process, adjusting the privacy-preserving gradient denoising nodes and regulating the diffusion’s forward and reverse time steps with the perturbation level $M$ as an adaptable parameter.\n\nIn summary, our key contributions include:\n\n• We disclose the natural diffusion process in general gradient perturbation mechanisms and introduce a perturbation adaptive parameter $M$ to adaptively adjust the diffusion step size according to the degree of perturbation. • We propose Mjo¨lnir, the first practical gradient diffusion attack strategy to recover the perturbed gradients, without additional access to the original model structure and third-party data, which breaks the bottleneck that existing gradient leakage attacks cannot effectively leak privacy under gradient perturbation protection. • We demonstrate the vulnerability of gradient perturbation protection under the Mjo¨lnir adaptive diffusion denoising process. Experimental results under the general perturbation protection FL system show that Mjo¨lnir achieves the best gradient denoising quality and privacy leakage ability on commonly used image datasets.\n\n# Background and Related Work FL with Perturbation Protection (FL-PP)\n\nFL-PP includes a variety of alternative gradient perturbation techniques, such as adding random global noise to gradients, noise to specific layers, and dynamic noise addition (Zhang et al. 2022). Among these, FL with Differential Privacy (FLDP) is the most widely used method (Shi et al. 2022; Chen et al. 2023; Tan et al. 2024b).\n\nGenerally, DP is defined based on the concept of adjacent database and has been applied in various practical areas in Artificial Intelligence to protect privacy information through adding specific perturbation, e.g., Google’s RAPPORT (Erlingsson, Pihur, and Korolova 2014) and largescale graph data publishing (Ding et al. 2021). FL-DP protects the shared model parameters or gradients between clients and the server during the FL training process by applying Local Differential Privacy (LDP (Zhao et al. 2021; Truex et al. 2020)) and (or) Differential Privacy Stochastic Gradient Descent algorithm (DPSGD (Abadi et al. 2016; Zhou et al. 2023)). In this paper we discuss both $\\varepsilon - D P$ and $( \\varepsilon , \\delta ) - D P$ (Dwork et al. 2006a,b; Abadi et al. 2016) as our sample attack background (Shi et al. 2022; Chen et al. 2023; Tan et al. 2024b):\n\nAssumption $\\scriptstyle  I .$ . A randomized mechanism $M \\colon D \\ \\to \\ R$ with domain $D$ and range $R$ satisfies $\\varepsilon$ - differential privacy if for any two adjacent inputs d $, d ^ { \\prime } \\in D$ and for any subset of outputs $S \\subseteq R$ , and it holds that:\n\n$$\nP r [ M ( d ) \\in S ] \\leq e ^ { \\varepsilon } P r [ M ( d ^ { \\prime } ) \\in S ]\n$$\n\nAssumption 2. A randomized mechanism $M \\colon D \\ \\to \\ R$ with domain $D$ and range $R$ satisfies $( \\varepsilon , \\delta )$ - differential privacy if for any two adjacent inputs $d$ $\\iota , d ^ { \\prime } \\in D$ and for any subset of outputs $S \\subseteq R$ , and it holds that:\n\n$$\nP r [ M ( d ) \\in S ] \\leq e ^ { \\varepsilon } P r [ M ( d ^ { \\prime } ) \\in S ] + \\delta\n$$\n\n$\\delta$ -approximation is preferably smaller than $1 / | d |$ . We usually apply Laplace perturbation for the definition in Eq. (1). However, as to the definition in Eq. (2), the noise perturbation mechanism needs to be Gaussian Mechanisms, which can adapt both $\\varepsilon$ and $\\delta$ . Refer to Theorem A.1 in Gaussian Mechanisms (Dwork and Roth 2014), with $\\begin{array} { r l } { \\nabla s } & { { } = } \\end{array}$ $m a x _ { D , D ^ { \\prime } } | | S ( D ) - S ( D ^ { \\prime } ) | |$ (s is the real-value function) and $c _ { d p }$ denotes the hyperparameter used to define the DP boundary, to ensure Gaussian noise distribution $N ( 0 , \\sigma _ { d p } ^ { 2 } )$ well preserves $( \\varepsilon , \\delta ) - D P$ , the noise scale should satisfy:\n\n$$\n\\begin{array} { r c l } { { \\sigma _ { d p } } } & { { \\geq } } & { { c _ { d p } \\nabla s / \\varepsilon , \\quad \\varepsilon \\in ( 0 , 1 ) } } \\\\ { { c _ { d p } } } & { { \\geq } } & { { \\sqrt { 2 l n ( 1 . 2 5 / \\delta ) } } } \\end{array}\n$$\n\nTaking the local participates site for example, the main actions for FL-DP can be divided into four steps: setting DP noise mechanism, local gradient clipping, local gradient perturbation, and uploading the protected parameters to the server (Wei et al. 2020). FL involves multiple participants, making composition theorem in DP necessary when applying noise. Among the composition DP methods available, including Simple Composition, Advanced Composition (Dwork and Roth 2014), and Moments Accountant (Abadi et al. 2016), we utilize Simple Composition for the following discussion. In the case where $M _ { i }$ satisfies $( \\varepsilon , \\delta ) - D P$ , the composition $( M _ { 1 } , M _ { 2 } , \\cdots , M _ { k } )$ satisfies $( \\sum _ { i = 1 } ^ { k } \\varepsilon _ { i } , \\sum _ { i = 1 } ^ { k } \\delta _ { i } )$ alA. i2g0n2i0n;gDwitohrkthet astl.at2e-0o0f6-a,rtb FALb-aDdPi et al. 2016; Shi et al. 2022; Chen et al. 2023; Tan et al. 2024b), We applied the following assumptions for noise calculation in local client gradient perturbation, considering both $\\varepsilon$ -DP (using the Laplace mechanism) and $( \\varepsilon , \\delta )$ -DP (using the Gaussian mechanism):\n\n$$\n\\begin{array} { r c l } { L a p l a c e : \\sigma _ { d p c } } & { = } & { \\nabla s _ { c } \\times 1 / \\varepsilon } \\\\ { G a u s s i a n : \\sigma _ { d p c } } & { = } & { \\nabla s _ { c } \\times \\sqrt { 2 l n ( 1 . 2 5 / \\delta ) } / \\varepsilon } \\end{array}\n$$\n\nwhere $\\nabla s _ { c }$ is the sensitivity and can be formulated as $\\nabla s _ { c } =$ $\\textstyle { \\frac { 2 C } { m } }$ . $C$ is the clipping threshold for bounding $| | \\nabla W _ { i } | | \\le C$ , where $\\nabla W _ { i }$ denotes the unperturbed gradient from the $\\displaystyle i - t h$ client and $m$ denotes the minimum size of local datasets.\n\n# Gradient Inversion Attack (GradInv)\n\nGradInv is a prevalent method in gradient leakage attacks and aims to steal the client’s privacy information in the FL system. The primary idea for the majority of GradInv to recover original information is minimizing the distance between the dummy gradient $\\nabla W _ { \\xi }$ and the original gradient $\\nabla W$ while updating the random data $x _ { \\xi }$ and label $y _ { \\xi }$ until the optimized results $\\nabla W _ { \\xi } ^ { * }$ and $( x _ { \\xi } ^ { * } , y _ { \\xi } ^ { * } )$ are close enough to the original ones. The key formulation can be described as:\n\n$$\nm i n | | \\nabla W _ { \\xi } - \\nabla W | | : ( x _ { \\xi } , y _ { \\xi } ) \\to ( x _ { \\xi } ^ { * } , y _ { \\xi } ^ { * } )\n$$\n\nPrevious works utilize Peak Signal-to-Noise Ratio (PSNR) of the recovered images to evaluate the performance of GradInv, with the threshold for the success of such attacks typically hinging on the degree of detail discernible to the human eye in the reconstructed private images (Geiping et al. 2020; Zhu, Liu, and Han 2019; Liu et al. 2023).\n\n# Methodology\n\nMjo¨lnir is the first Diffusion Attack Method focusing on the gradient data structure that can be applied to multiple kinds of gradient perturbation protection in FL through adaptive parameters setting on both forward and reverse process of the Gradient Diffusion Model employed in Mjo¨lnir. The threat models targeted by Mjo¨lnir encompass various forms of gradient perturbation. Meanwhile, the extremely similar noise mechanism on gradient perturbation protection and diffusion Markovian process provides a mathematical necessity for Mjo¨ lnir to realize an efficient privacy attack on gradient perturbation protection. Overall, our Mjo¨lnir method (shown in Fig. 2) can be summarized in four steps:\n\nStep (1) Get Protected Gradients. Honest-but-curious malicious attackers steal the Shared Perturbed Gradients $\\nabla W ^ { \\prime }$ during the FL training process by hiding on the server side or waiting on the way of parameter sharing (Fig.1).\n\n<html><body><table><tr><td>Algorithm1: Gradients Extracted forMjolnir Training</td></tr><tr><td>1:Stolen Protected Gradients:Fi →VW';</td></tr><tr><td>2:Construct surrogate Model: VW'→Fs;</td></tr><tr><td>3:j=0;∈~ N(0,I);</td></tr><tr><td>4: for iteration =1to Lengthrandomdataset do</td></tr><tr><td>5: VWj =∂l(Fs(xj;Ws),yj)/aWs;</td></tr><tr><td>6: Save VWj;</td></tr><tr><td>7: j=j+1;</td></tr></table></body></html>\n\nStep (2) Construct Surrogate Model. Construct Surrogate Gradients Data Supply Model $F ^ { s }$ from the data structure of Shared Perturbed Gradients $\\nabla W ^ { \\prime }$ stolen by the attacker that can output the same gradient data structure as the target attacked client’s local model (Algorithm 1, Fig. 2).\n\nBefore delving into Step (3) and Step (4), two configurations that recur in the subsequent steps are defined:\n\nPerturbed Gradient Surrogate Model Feed Random 𝜵𝑾′𝑳𝒂𝒚𝒆𝒓 𝟏 𝑭𝒔𝑳𝒂𝒚𝒆𝒓 𝟏 Surrogate GIrmadaigeentDataset 𝜵𝑾′ 𝜵𝑾′𝑳𝒂𝒚𝒆𝒓 𝟐 𝑭𝒔 𝑳𝒂𝒚𝒆𝒓 𝟐 𝜵𝑾𝒔𝑳𝒂𝒚𝒆𝒓 𝟏   \nSize = L Construct Extract 𝜵𝑾𝒔𝑳𝒂𝒚𝒆𝒓 𝟐 L 𝜵𝑾′𝑳𝒂𝒚𝒆𝒓 𝑵 2 1 P 𝑭𝒔𝑳𝒂𝒚𝒆𝒓 𝑵 𝜵𝑾𝒔𝑳𝒂𝒚𝒆𝒓 𝑵 P   \nPadding 𝒈 L 𝒈   \nSize = P 4 Inference 𝜵𝑾𝑺 𝒈 Gradient Diffusion Model Train 𝑿𝟎\\~𝒑(𝑿|𝜵𝑾′) 𝑿𝒕−𝟏 𝑿𝒕 𝑿 \\~𝐍(𝟎, 𝐈) PZPT 𝐪(𝑿𝒕 |𝑿𝒕−𝟏)   \n𝜵𝑾𝑹 经 𝒑 (𝑿 |𝑿 , 𝜵𝑾′)\n\n[Gradient Adjustment]: Gradients each with total size $L$ $( L = L _ { \\nabla W _ { L a y e r 1 } } + L _ { \\nabla W _ { L a y e r 2 } } + . . . + L _ { \\nabla W _ { L a y e r N } } )$ are adjusted into $1 \\times g \\times g ( g ^ { 2 } = L + P ; g = \\operatorname { r }$ minimum integer satisfies $g ^ { 2 } > L$ ; $P = 0$ -padding size) before feeding into Gradient Diffusion Model for training or inference to adapt gradient diffusion process. The adjustment is only related to Diffusion procedures, gradients are restored to their original structure and size before further Gradient-Based Attacks and Evaluations.\n\n[M-Adaptive Process]: Adaptive parameter $M$ is inserted into $\\nabla W ^ { \\prime }$ before inference to ensure the starting time step is appropriately positioned to maximize the denoising capability of Gradient Diffusion Model (Eq. (14) $\\sim$ Eq. (16)).\n\nStep (3) Train Gradients Diffusion Model. Construct\n\n# Algorithm 2: Gradient Diffusion Model Training\n\n<html><body><table><tr><td>1: if With Condition VW' then Xo = (VW',VWj); 2: else Xo = VWj; 3:Repeat: 4: Xo~p(Xo);t~Uniform(1→T);∈~N(0,I); 5: Takea gradient descent step on:Velle - fe(√tXo+ √1-Yt∈,t)ll²; 6:until converged;</td></tr></table></body></html>\n\n![](images/a16ec3004a1743038047db86e3c44cb17625c98b2f283aae3a7320860473f37c.jpg)  \nFigure 3: Visualization of Markovian gradient diffusion process. $M$ is the noise scale of perturbation, which is set as the adaptive parameter in [M-Adaptive Process]. Mjo¨lnir Train and Mjo¨lnir Inference correspond to Algorithm 2 and Algorithm 3 respectively.\n\nour Gradient Diffusion Model that takes into account the level of knowledge about the attacked model (e.g., whether the level of perturbation noise or the type of noise is known). Conduct [Gradient Adjustment] to Clean Gradients $\\nabla W ^ { s }$ extracted from Surrogate Model $F ^ { s }$ to build a training gradient dataset to train our Gradient Diffusion Model (Fig. 2, Fig.3). Algorithm 2 demonstrates a detailed training process. Step (4) Recover Original Gradients. The stolen Shared Perturbed Gradients $\\nabla W ^ { \\prime }$ are put into our trained Mjo¨lnir Gradient Diffusion Model after [Gradient Adjustment] and [M-Adaptive Process] to generate the Recovered Gradients $\\nabla W ^ { R }$ for further Gradient-Based Attack to get certain privacy information based on various demands (Algorithm 3, Fig. 2 and Fig. 3).\n\nAlgorithm 3: Generate Original Gradient   \n\n<html><body><table><tr><td>1:if Known Noise Scale M then c 1</td></tr><tr><td>2:else c ∈(0,1); V1+M2;</td></tr><tr><td>3: Xt ~ cVW';</td></tr><tr><td>4: for t = T.',..,1 do</td></tr><tr><td>5: if t >1 then z~ N(0,I);</td></tr><tr><td>6: elsez=0;</td></tr><tr><td>7: Xt-1= (Xt--fe(Xt,t))+√1-atz; √at</td></tr><tr><td>8: return Xo; VWR ← Xo: >ForFurther GradInv;</td></tr></table></body></html>\n\nMjo¨lnir’s Gradient Diffusion Model is inspired by DDPM(Ho, Jain, and Abbeel 2020), to be specific:\n\nIn Step (3) Train Gradients Diffusion Model, refer to Algorithm 2 lines 4 to 6, we set $t \\in ( 1 , T )$ as the time steps of Gaussian noise addition. $T$ is the total time step of the forward Markovian diffusion process. $\\alpha _ { t } ( 0 < \\alpha _ { t } < 1 )$ ] denote the adaptive variables at each iteration and $\\gamma _ { t } \\mathrm { ~ = ~ }$ $\\textstyle \\prod _ { t = 0 } ^ { t } \\alpha _ { t }$ . With the above settings, the forward process $( q ;$ : with no learnable parameters) of Mj¨olnir’s Gradients Diffusion Model can be modeled as:\n\n$$\n\\begin{array} { r c l } { q ( X _ { 1 } | X _ { 0 } ) } & { = } & { N ( X _ { 1 } ; \\sqrt { \\alpha _ { 1 } } X _ { 0 } , ( 1 - \\alpha _ { 1 } ) I ) } \\\\ { q ( X _ { t } | X _ { 0 } ) } & { = } & { N ( X _ { t } ; \\sqrt { \\gamma _ { t } } X _ { 0 } , ( 1 - \\gamma _ { t } ) I ) } \\end{array}\n$$\n\nGiven $( X _ { 0 } , X _ { t } )$ , $X _ { t - 1 }$ can be modeled as:\n\n$$\nq ( X _ { t - 1 } | X _ { 0 } , X _ { t } ) = N ( X _ { t } ; \\mu _ { t } , \\sigma _ { t } I )\n$$\n\nIf insert the stolen Shared Perturbed Gradients $\\nabla W ^ { \\prime }$ or constructed Surrogate Perturbed Gradients $\\nabla W _ { p e r t u r b e d } ^ { s }$ training condition then input $X _ { 0 } ~ = ~ ( c o n d i t i o n , \\nabla W _ { j } )$ .If not to train with the condition, then input $\\begin{array} { r l } { X _ { 0 } } & { { } = } \\end{array}$ $\\nabla W _ { j }$ .Through algebraic calculation, $\\mu _ { t }$ and $\\sigma _ { t }$ in Eq. (10) can be simplified for further usage in the reverse training process(Saharia et al. 2023) as:\n\n$$\n\\begin{array} { r c l } { \\mu _ { t } } & { = } & { \\displaystyle \\frac { \\sqrt { \\gamma _ { t - 1 } } ( 1 - \\alpha _ { t } ) } { 1 - \\gamma _ { t } } X _ { 0 } + \\frac { \\alpha _ { t } ( 1 - \\gamma _ { t - 1 } ) } { 1 - \\gamma _ { t } } X _ { t } } \\\\ { \\sigma _ { t } ^ { 2 } } & { = } & { \\displaystyle \\frac { 1 - \\gamma _ { t - 1 } } { 1 - \\gamma _ { t } } ( 1 - \\alpha _ { t } ) } \\end{array}\n$$\n\nIn the reverse process $( p )$ of Mjo¨ lnir’s Gradient Diffusion training, the objective function $f _ { \\theta }$ which is trained to predict noise vector $\\epsilon$ is modeled as:\n\n$$\nE _ { X _ { 0 } , \\epsilon } [ \\frac { ( 1 - \\alpha _ { t } ) ^ { 2 } } { 2 \\sigma _ { t } ^ { 2 } \\alpha _ { t } ( 1 - \\gamma _ { t } ) } | | f _ { \\theta } ( \\sqrt { \\gamma _ { t } } X _ { 0 } + \\sqrt { 1 - \\gamma _ { t } } \\epsilon , t ) - \\epsilon | | ^ { 2 } ]\n$$\n\nIn Step (4) Recover Original Gradients, refer to Algorithm 3, two different original gradient generation processes are chosen depending on whether or not attackers know the Noise Scale of Perturbation of the Threat Model. Take FL with Gaussian Differential Privacy (Eq. (6)) as an example, consider an experienced and clever attacker who may know the DP Privacy Budget $\\varepsilon$ and the probability of information leakage $\\delta$ ( $\\delta$ is likely to be set as $1 \\dot { 0 } ^ { - 5 }$ from usual practice in DP), combining with the sensitivity $\\nabla { S }$ (which can be estimated if the attacker has some previous information with the target model training dataset), the noise scale can be calculated or estimated to an approximate value $M$ ( $M$ defined as the adaptive parameter in $/ M$ -Adaptive Process]). So, $\\nabla W ^ { \\prime }$ can be modeled as:\n\n$$\n\\nabla W ^ { \\prime } = \\nabla W + M N ( 0 , I )\n$$\n\nConsidering the forward Markovian gradient diffusion process in Step 3 Eq. (8) & Eq. (9), the relation between stolen perturbed gradients $\\nabla W ^ { \\prime }$ and original gradients $\\nabla W$ can be remodeled as:\n\n$$\n{ \\frac { 1 } { \\sqrt { 1 + M ^ { 2 } } } } \\nabla W ^ { \\prime } = { \\frac { 1 } { \\sqrt { 1 + M ^ { 2 } } } } \\nabla W + { \\frac { M } { \\sqrt { 1 + M ^ { 2 } } } } N ( 0 , I )\n$$\n\nRecall that our target is to generate $\\nabla W ^ { R }$ , so $\\scriptstyle { \\frac { 1 } { \\sqrt { 1 + M ^ { 2 } } } } \\nabla W ^ { \\prime }$ should be considered as $X _ { t }$ in the inverse process of Gradient Diffusion Model, while $\\boldsymbol { \\nabla } W$ stands for $X _ { 0 }$ .\n\nCorrespondingly, during the construction of the recovered gradient ${ \\dot { \\nabla } } W ^ { R }$ , the inverse time steps can be set as $T ^ { \\prime }$ . The relationship between $T ^ { \\prime }$ and $M$ is:\n\n$$\n{ \\frac { 1 } { 1 + M ^ { 2 } } } = \\prod _ { t = 0 } ^ { T ^ { \\prime } } \\alpha _ { t }\n$$\n\nSince $\\begin{array} { r } { \\gamma _ { t } \\ = \\ \\prod _ { t = 0 } ^ { t } \\alpha _ { t } } \\end{array}$ have been predefined and calculated during t Qforward Markovian diffusion process, $T ^ { \\prime }$ can be fixed in an approximate range $T ^ { \\prime } \\in ( T _ { - } ^ { \\prime } , \\mathbf { \\bar { \\it T } _ { + } ^ { \\prime } } )$ , where $\\begin{array} { r } { \\prod _ { t = 0 } ^ { T _ { - } ^ { \\prime } } \\alpha _ { t } < \\frac { 1 } { 1 + M ^ { 2 } } < \\prod _ { t = 0 } ^ { T _ { + } ^ { \\prime } } \\alpha _ { t } } \\end{array}$ and $T _ { - o r + } ^ { \\prime }$ are positive integers less than the total forward noise addition time step $T$ . On the other hand, if $M$ can not be estimated, which means the attacker knows nothing about the noise scale. Since $\\frac { 1 } { 1 + M ^ { 2 } } \\ : \\in \\ : ( 0 , 1 )$ , the input value of the inference process can simply be set as $c \\nabla W ^ { \\prime }$ where $c \\in ( 0 , 1 )$ to adapt the Markovian forward process.\n\n<html><body><table><tr><td rowspan=\"2\">Datasets</td><td rowspan=\"2\">Model</td><td colspan=\"2\">ε=1</td><td colspan=\"2\">e=5</td><td colspan=\"2\">ε=10</td></tr><tr><td>PSNRi</td><td>LRA</td><td>PSNRi</td><td>LRA</td><td>PSNRi</td><td>LRA</td></tr><tr><td rowspan=\"6\">MINIST</td><td>GRNN(Ren, Deng, and Xie 2022)</td><td>14.70</td><td>1.000</td><td>16.54</td><td>1.000</td><td>20.19</td><td>1.000</td></tr><tr><td>IG(Geiping et al. 2020)</td><td>3.721</td><td></td><td>5.164</td><td></td><td>6.067</td><td></td></tr><tr><td>DLG(Zhu,Liu,and Han 2019)</td><td>0.222</td><td>0.676</td><td>8.025</td><td>0.752</td><td>17.08</td><td>0.909</td></tr><tr><td>Mjolnir</td><td>17.87</td><td>0.851</td><td>24.17</td><td>0.895</td><td>33.09</td><td>0.927</td></tr><tr><td>Mjolnir(Non-Adaptive)</td><td>17.39</td><td>0.851</td><td>24.14</td><td>0.889</td><td>32.98</td><td>0.925</td></tr><tr><td>Mjolnir(Conditional)</td><td>20.87</td><td>0.887</td><td>28.39</td><td>0.924</td><td>35.14</td><td>0.957</td></tr><tr><td rowspan=\"6\">CIFAR100</td><td>GRNN(Ren, Deng, and Xie 2022)</td><td>18.24</td><td>1.000</td><td>20.61</td><td>1.000</td><td>21.15</td><td>1.000</td></tr><tr><td>IG(Geiping et al. 2020)</td><td>3.684</td><td></td><td>5.073</td><td></td><td>5.971</td><td></td></tr><tr><td>DLG(Zhu,Liu,and Han 2019)</td><td>0.112</td><td>0.769</td><td>7.673</td><td>0.833</td><td>18.57</td><td>0.896</td></tr><tr><td>Mjolnir</td><td>18.25</td><td>0.883</td><td>19.04</td><td>0.902</td><td>21.32</td><td>0.902</td></tr><tr><td>Mjolnir(Non-Adaptive)</td><td>18.15</td><td>0.871</td><td>17.98</td><td>0.884</td><td>21.06</td><td>0.891</td></tr><tr><td>Mjolnir(Conditional)</td><td>18.69</td><td>0.901</td><td>21.02</td><td>0.902</td><td>23.49</td><td>0.908</td></tr><tr><td rowspan=\"6\">STL10</td><td>GRNN(Ren, Deng, and Xie 2022)</td><td>12.24</td><td>1.000</td><td>16.24</td><td>1.000</td><td>20.98</td><td>1.000</td></tr><tr><td>IG(Geiping et al. 2020)</td><td>4.323</td><td></td><td>6.082</td><td></td><td>7.851</td><td></td></tr><tr><td>DLG(Zhu,Liu,and Han 2019)</td><td>0.038</td><td>0.783</td><td>5.673</td><td>0.815</td><td>17.25</td><td>0.851</td></tr><tr><td>Mjolnir</td><td>12.36</td><td>0.806</td><td>16.30</td><td>0.857</td><td>19.77</td><td>0.889</td></tr><tr><td>Mjolnir(Non-Adaptive)</td><td>12.28</td><td>0.801</td><td>15.19</td><td>0.849</td><td>19.76</td><td>0.887</td></tr><tr><td>Mjolnir(Conditional)</td><td>12.98</td><td>0.816</td><td>20.30</td><td>0.872</td><td>22.77</td><td>0.906</td></tr></table></body></html>\n\nTable 1: Privacy leakage capability of Mjo¨lnir variant models and traditional Gradients Leakage Attacks in FL-DP $( \\delta = 1 0 ^ { - 5 }$ , $\\varepsilon = 1$ , 5, 10). Gray marker: Mjo¨ lnir outperforms the highest result of traditional ones.\n\n![](images/6db18d6b26a775a61c7d200695a90af8de57639b989f6a6721bb47dd134230d7.jpg)  \nFigure 4: Comparison of the private image recovery procedures to the iterations between Mjo¨lnir variant models and traditional Gradient Leakage Attack methods (DLG (Zhu, Liu, and Han 2019)).\n\nAlso, if conditions allow, $c$ can be modeled and predicted by a separate Machine Learning model according to the specific requirement of attackers.\n\n# Experiments\n\n# Experimental Setups\n\n(A) Mjo¨ lnir Variant Attack Models. Mjo¨ lnir (trained with only unperturbed surrogate gradients), Conditional Mjo¨lnir (trained with both perturbed gradients and unperturbed surrogate gradients), and Non-Adaptive Mjo¨lnir (without $I M \\cdot$ - Adaptive Process]: no perturbation scale $M$ as an adaptive parameter during gradient diffusion process) are presented.\n\n(B) Benchmarks and Datasets. We employ MNIST, CIFAR100, and STL10 as client privacy datasets, which also serve as the ground truth for privacy leakage evaluation. We extract the unperturbed original gradients $( \\nabla W )$ of the aforementioned three datasets from the local training model of the target client as the reference benchmark of gradient denoising under the FL-PP paradigm. The Mjo¨lnir gradient diffusion model is trained with gradients extracted from a separate dataset, FashionMNIST.\n\n(C) Evaluation and Boundary. To evaluate the Privacy Leakage Capability, we utilize the Image Average Peak Signal-to-Noise Ratio $P S N R _ { i }$ and the Label Recovered Accuracy $L R A$ . These metrics are employed to assess the fidelity of the recovered images and the accuracy of the recovered labels, respectively, to the original images and ground truth labels. The boundary of Privacy Leakage Attack is aligned with previous works: the attack is considered successful if human visual perception can discern the requisite information from the recovered images. In the context of evaluating Gradient Denoising, we employ the cosine similarity $C o s S i m i l a r _ { g }$ and the Average Peak Signal-to-Noise Ratio $P S N R _ { g }$ as metrics to assess the quality of the Recovered Gradients $\\nabla W ^ { R }$ compared to the Original Gradients $\\nabla W$ . Higher values of the two metrics indicate better accuracy in the original gradient recovery.\n\n![](images/61cf6a355792482227849afc78476d474df6da198aa71aa789b2f5fb38ea2509.jpg)  \nFigure 5: Comparisons on ground truth clients’ private images and corresponding recovered privacy images from Mj¨olnir variant models and commonly used traditional gradient leakage attacks. $\\ : \\ : ( \\delta = 1 0 ^ { \\frac { - 5 } { - 5 } }$ ; $\\varepsilon = 1 0$ ; Success Rate: overall attack success rate)\n\n# Privacy Leakage Capability\n\nThe comparison of overall privacy leakage capability from perturbed gradients of Mjo¨lnir and traditional Gradient Leakage Attacks (GRNN (Ren, Deng, and Xie 2022), IG (Geiping et al. 2020), and DLG (Zhu, Liu, and Han 2019)) under FL-DP, compares Image Average Peak Signal Noise Ratio $P S N R _ { i }$ and Label Recovered Accuracy $L R A$ of the recovered local clients’ privacy information. Local clients’ private training datasets are MNIST, CIFAR100, and STL10.\n\nAccording to the numerical experimental results shown in Table 1 and the visualization results shown in Fig. 5, Mjo¨lnir variant models exhibit a substantial superiority over traditional gradient leakage attacks in terms of private image leakage. On average, there is an approximately $2 0 9 \\%$ increase in the recovered image $P S N R _ { i }$ when using Mjo¨lnir. For traditional methods, GRNN consistently achieves an LRA metric of 1.000 by using a robust generative network for label recovery, while other methods update images and labels jointly, leading to less accurate results. Moreover, upon examining the private image recovery procedures of Mjo¨lnir variant models and traditional gradient leakage attacks illustrated in Fig. 4, it becomes evident that the attacks incorporating Mjo¨lnir not only achieve considerably improved accuracy in the ultimate reconstruction of private images compared to conventional approaches but also showcase notable advantages in terms of attack iteration rounds and speed (Mjo¨lnir (Conditional) $>$ M $\\mathrm { j } \\ddot { \\mathrm { o l n i r } } > \\mathrm { ~ N ~ }$ jo¨ lnir (Non-Adaptive)).\n\n# Gradients Denoising under FL-DP\n\nAmong FL-DP, we choose the NbAFL framework (Wei et al. 2020) (Noising before model aggregation FL) as the threat model in this experiment due to its widespread adoption. To ensure a comprehensive evaluation of the effectiveness of the Mjo¨lnir, we train the three Mjo¨lnir variant models, as well as non-diffusion denoising models such as NBNet (Cheng et al. 2021), SS-BSN (Han and Yu 2023), and APBSN (Lee, Son, and Lee 2022), using gradients extracted from the surrogate downstream task model trained on the FashionMnist. Following that, we utilize the trained models to denoise the shared perturbed gradients intercepted by the attacker from the target clients. These target clients have locally trained downstream task models using privacy datasets (MNIST, CIFAR100, and STL10). The gradients are protected using the NbAFL(Wei et al. 2020) before being shared with the server. Based on the experimental results presented in Table 2, it is evident that Mjo¨lnir showcases superior denoising ability for perturbed gradients under FL-DP, with an average Cosine Similarity exceeding 0.992 and a PSNR of 37.68. This represents a significant improvement of over $2 7 \\%$ compared to non-diffusion methods. Further analysis reveals that among the three Mjo¨lnir variant models, the performance can be ranked as follows: Mjo¨ lnir(Conditional) $>$ Mjo¨ lnir $>$ Mjo¨ lnir (Non-Adaptive). This ranking demonstrates both $M$ -Adaptive Process and Conditional training enhance the denoising and generation performance of Mjo¨lnir’s Gradient Diffusion Model.\n\n# Gradients Denoising under FL-PP\n\nTo evaluate the generalization capability of Mjo¨lnir on gradient denoising, we constructed two different types of noise (Laplace and Gaussian) randomly applied to each layer of the original gradients within the FL-PP framework. The training and inference processes of Mjo¨lnir variant models, and non-diffusion denoising models (NBNet (Cheng et al. 2021), SS-BSN (Han and Yu 2023), and AP-BSN (Lee, Son, and Lee 2022)) are the same as the above experiments on gradients denoising under FL-DP. In contrast to attacks specifically tailored for the FL-DP framework, the gradient denoising experiments on FL-PP showcase Mjo¨lnir’s ability to effectively handle various types of perturbations and adapt to different magnitudes of perturbation. Referring to the experimental results illustrated in Fig. 6, it is observed that when subjected to Laplace perturbation, Mjo¨lnir variant models exhibit an average improvement of $2 1 . 6 \\%$ in PSNR and $9 . 2 \\%$ in Cosine Similarity. Similarly, under Gaussian perturbation, Mjo¨lnir achieves an average enhancement of $\\mathrm { \\bar { 2 } 1 . 3 \\% }$ in PSNR and $9 . 1 \\%$ in Cosine Similarity. These findings provide compelling evidence for the robustness and stability of the Mjo¨lnir variant models. We further compared the average inference time of different denoising models under FL-PP. Results presented in Table 3 indicate that Mjo¨lnir variant models exhibit a relative advantage in terms of gradient denoising speed, surpassing non-diffusion methods by an average improvement of $3 2 . 7 \\%$ in inference time.\n\nTable 2: Overall results on gradients denoising in FL-DP. Threat model setting: DP- $( \\varepsilon , \\delta ) = ( 2 , 1 0 ^ { - 5 } )$ with Gaussian perturbation   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">MNIST</td><td colspan=\"2\">CIRAF100</td><td colspan=\"2\">STL10</td></tr><tr><td>CosSimilarg PSNRg|CosSimilarg PSNRg |CosSimilarg PSNRg</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NBNet(Cheng et al. 2021)</td><td>0.992</td><td>35.74</td><td>0.979</td><td>27.51</td><td>0.979</td><td>27.23</td></tr><tr><td>SS-BSN(Han and Yu 2023)</td><td>0.995</td><td>32.35</td><td>0.845</td><td>22.85</td><td>0.845</td><td>22.84</td></tr><tr><td>AP-BSN(Lee, Son,and Lee 2022)</td><td>0.968</td><td>29.61</td><td>0.892</td><td>24.01</td><td>0.893</td><td>24.12</td></tr><tr><td>Mjolnir</td><td>0.996</td><td>38.76</td><td>0.990</td><td>30.42</td><td>0.990</td><td>30.01</td></tr><tr><td>Mjolnir(Non-Adaptive)</td><td>0.995</td><td>38.59</td><td>0.990</td><td>30.39</td><td>0.990</td><td>29.87</td></tr><tr><td>Mjolnir(Conditional)</td><td>0.996</td><td>38.78</td><td>0.990</td><td>30.53</td><td>0.993</td><td>30.22</td></tr></table></body></html>\n\n![](images/e1fab7ae5f990c0aadde249caec6a3c97d6e6c516dc06d3fa4fb169c69652bd1.jpg)  \nFigure 6: Gradients denoising under FL-PP: Gaussian and Laplace perturbed gradients denoising performance of $P S N R _ { g }$ and CosSimilar $\\overset { \\cdot } { \\boldsymbol { g } }$ via different perturbation magnitudes. Perturbation decreases, $\\varepsilon$ increases. (Private dataset: STL10, $\\delta = 1 0 ^ { - 5 }$ )\n\nTable 3: Gradient denoising average inference time of Mjo¨lnir variant models and non-diffusion denoising models under FL-PP. (Device: NVIDIA GeForce RTX 2060 GPU; Intel(R) Core(TM) i7-10870H CPU at 2.20GHz)   \n\n<html><body><table><tr><td>Model</td><td>Inference time (s)</td></tr><tr><td>NBNet(Cheng et al.2021)</td><td>2.653</td></tr><tr><td>SS-BSN(Han and Yu 2023) AP-BSN(Lee,Son,andLee 2022)</td><td>26.15</td></tr><tr><td></td><td>7.138</td></tr><tr><td>Mjolnir Mjolnir(Non-Adaptive)</td><td>6.834 6.834</td></tr><tr><td>Mjolnir(Conditional)</td><td>6.917</td></tr><tr><td></td><td></td></tr></table></body></html>\n\nLimitations: (1) Mjo¨ lnir is not effective in attacking perturbations that are not based on gradient diffusion (noise perturbation) such as representation perturbation. (2) The overall effectiveness of Mjo¨lnir in reconstructing privacy datasets is also bounded by the selected subsequent Gradient Leakage Attacks. (3) Mj¨olnir is used to attack CNN and (or) DNNbased models. Experimental results suggest that it struggles with transformer-based models due to their complex structure and larger number of parameters, making it difficult to accurately denoise gradients and recover data.\n\nDefense Strategies: Possible defense strategies against Mjo¨lnir can be approached by preserving the privacy of original data, preserving the target attack models, and shared gradients protection. Mjo¨lnir effectively leaks privacy data by attacking the perturbed shared gradients, which suggests that the possible defense approaches against Mjo¨lnir should either be non-perturbation gradient protection methods or non-gradient privacy-preserving methods.\n\n# Conclusion\n\nThis paper makes the first attempt to investigate the diffusion property of the widely used perturbation-based gradient protection. To reveal potential vulnerabilities, we propose a novel Perturbation-Resilient Gradient Leakage Attack via an adaptive diffusion process. This effective attack paradigm deactivates perturbation protection by leveraging the denoising capability of diffusion models without access to clients’ models and external data. Based on Mj¨olnir, we wish to enhance public consciousness of the privacy leakage issues of existing perturbation-based defenses on gradients.",
    "institutions": [
        "The Hong Kong Polytechnic University",
        "Hubei Key Laboratory of Transportation Internet of Things",
        "Wuhan University of Technology",
        "College of Computer Science and Software Engineering",
        "Shenzhen University",
        "Department of Computer Science and Engineering",
        "The Hong Kong University of Science and Technology"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决联邦学习中基于扰动的梯度保护机制能否真正抵御所有梯度泄漏攻击的问题。联邦学习虽能在不直接传输原始训练数据的情况下进行协作模型训练，但易受梯度反转攻击，攻击者可从共享梯度中重建敏感用户数据。因此，梯度保护技术至关重要，而验证其防护效果意义重大。\\n\\n**方法概述**\\n论文提出Mjo¨lnir，一种基于扩散模型的抗扰动梯度泄漏攻击方法，无需额外访问原始模型结构或外部数据，利用梯度扰动保护的固有扩散特性去除梯度中的扰动。\\n\\n**主要贡献与效果**\\n- 揭示了一般梯度扰动机制中的自然扩散过程，并引入自适应参数 $M$ 来根据扰动程度自适应调整扩散步长。\\n- 提出Mjo¨lnir，首个实用的梯度扩散攻击策略，打破了现有梯度泄漏攻击在梯度扰动保护下难以有效泄漏隐私的瓶颈。在MNIST、CIFAR100和STL10等常见图像数据集上，Mjo¨lnir平均图像峰值信噪比提升约209%，梯度去噪的平均余弦相似度超过0.992，PSNR达到37.68，比非扩散方法提高超27%。\\n- 证明了在Mjo¨lnir自适应扩散去噪过程下，梯度扰动保护的脆弱性。实验结果表明，Mjo¨lnir在一般扰动保护的联邦学习系统中，在常用图像数据集上实现了最佳的梯度去噪质量和隐私泄漏能力。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n基于梯度是对训练数据进行线性变换的结果，若原始数据稳定可预测，梯度数据也会稳定可预测，且梯度扰动保护本质类似于对结构稳定的梯度数据进行扩散过程。因此利用扩散模型的去噪能力，通过构建代理模型获取训练数据，以消除梯度中的扰动。\\n\\n**创新点**\\n先前的梯度泄漏攻击在梯度扰动保护下难以有效泄漏隐私，而Mjo¨lnir无需额外访问原始模型结构和第三方数据，通过自适应扩散过程恢复受扰动的梯度，打破了现有攻击方法的瓶颈。\\n\\n**具体实现步骤**\\n1. **获取受保护的梯度**：诚实但好奇的恶意攻击者在联邦学习训练过程中，通过隐藏在服务器端或在参数共享途中窃取共享的扰动梯度 $\\nabla W ^ { \\prime }$。\\n2. **构建代理模型**：根据窃取的共享扰动梯度 $\\nabla W ^ { \\prime }$ 的数据结构，构建代理梯度数据供应模型 $F ^ { s }$，使其输出与目标攻击客户端的局部模型相同的梯度数据结构。在将梯度输入梯度扩散模型进行训练或推理之前，进行梯度调整，将总大小为 $L$（$L = L _ { \\nabla W _ { L a y e r 1 } } + L _ { \\nabla W _ { L a y e r 2 } } +... + L _ { \\nabla W _ { L a y e r N } }$）的梯度调整为 $1 \\times g \\times g$（$g ^ { 2 } = L + P$，$g$ 是满足 $g ^ { 2 } > L$ 的最小整数，$P$ 是0填充大小）以适应梯度扩散过程，调整仅与扩散过程相关，在进一步的基于梯度的攻击和评估前，梯度会恢复到原始结构和大小。同时，在推理前将自适应参数 $M$ 插入 $\\nabla W ^ { \\prime }$ 以确保起始时间步合适，最大化梯度扩散模型的去噪能力。\\n3. **训练梯度扩散模型**：构建梯度扩散模型，考虑对被攻击模型的了解程度（如是否知道扰动噪声水平或类型），对从代理模型 $F ^ { s }$ 提取的清洁梯度 $\\nabla W ^ { s }$ 进行上述梯度调整，构建训练梯度数据集来训练该模型。具体训练过程如算法2所示，设置 $t \\in ( 1, T )$ 为高斯噪声添加的时间步，$T$ 是前向马尔可夫扩散过程的总时间步，$\\alpha _ { t } ( 0 < \\alpha _ { t } < 1 )$ 表示每次迭代的自适应变量，$\\gamma _ { t } = \\prod _ { t = 0 } ^ { t } \\alpha _ { t }$，前向过程可建模为 $q ( X _ { 1 } | X _ { 0 } ) = N ( X _ { 1 } ; \\sqrt { \\alpha _ { 1 } } X _ { 0 }, ( 1 - \\alpha _ { 1 } ) I )$ 和 $q ( X _ { t } | X _ { 0 } ) = N ( X _ { t } ; \\sqrt { \\gamma _ { t } } X _ { 0 }, ( 1 - \\gamma _ { t } ) I )$，反向训练过程的目标函数 $f _ { \\theta }$ 用于预测噪声向量 $\\epsilon$，建模为 $E _ { X _ { 0 }, \\epsilon } [ \\frac { ( 1 - \\alpha _ { t } ) ^ { 2 } } { 2 \\sigma _ { t } ^ { 2 } \\alpha _ { t } ( 1 - \\gamma _ { t } ) } || f _ { \\theta } ( \\sqrt { \\gamma _ { t } } X _ { 0 } + \\sqrt { 1 - \\gamma _ { t } } \\epsilon, t ) - \\epsilon || ^ { 2 } ]$。\\n4. **恢复原始梯度**：将窃取的共享扰动梯度 $\\nabla W ^ { \\prime }$ 经过梯度调整和 $M$ 自适应过程后，放入训练好的Mjo¨lnir梯度扩散模型，生成恢复的梯度 $\\nabla W ^ { R }$ 用于进一步的基于梯度的攻击。根据攻击者是否知道威胁模型的扰动噪声规模，选择不同的原始梯度生成过程。若攻击者知道噪声规模，可计算或估计出近似值 $M$，$\\nabla W ^ { \\prime }$ 可建模为 $\\nabla W ^ { \\prime } = \\nabla W + M N ( 0, I )$，通过代数计算和相关公式可进行梯度恢复；若不知道噪声规模，输入值可简单设为 $c \\nabla W ^ { \\prime }$（$c \\in ( 0, 1 )$）以适应马尔可夫前向过程，条件允许时，$c$ 可由单独的机器学习模型根据攻击者的具体需求进行建模和预测。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\nGRNN (Ren, Deng, and Xie 2022)、IG (Geiping et al. 2020)、DLG (Zhu, Liu, and Han 2019)、NBNet (Cheng et al. 2021)、SS - BSN (Han and Yu 2023)、AP - BSN (Lee, Son, and Lee 2022)等。\\n\\n**性能对比**\\n*   **在 [图像平均峰值信噪比/PSNRi] 指标上：** Mjo¨lnir在MNIST、CIFAR100和STL10等不同数据集和不同 $\\varepsilon$ 值（1、5、10）下均有较好表现，平均提升约209%。例如在MNIST数据集上，当 $\\varepsilon = 1$ 时，Mjo¨lnir达到17.87，高于GRNN的14.70、IG的3.721和DLG的0.222；当 $\\varepsilon = 5$ 时，Mjo¨lnir为24.17，高于GRNN的16.54、IG的5.164和DLG的8.025；当 $\\varepsilon = 10$ 时，Mjo¨lnir达到33.09，高于GRNN的20.19、IG的6.067和DLG的17.08。\\n*   **在 [标签恢复准确率/LRA] 指标上：** GRNN始终为1.000，Mjo¨lnir表现也较好，如在MNIST数据集 $\\varepsilon = 1$ 时为0.851，$\\varepsilon = 5$ 时为0.895，$\\varepsilon = 10$ 时为0.927。\\n*   **在 [梯度去噪的余弦相似度/CosSimilarg] 指标上：** Mjo¨lnir及其变体模型在MNIST、CIFAR100和STL10数据集上的平均余弦相似度超过0.992，高于NBNet、SS - BSN、AP - BSN等非扩散方法，比它们提高超27%。例如在MNIST数据集上，Mjo¨lnir达到0.996，高于NBNet的0.992、SS - BSN的0.995和AP - BSN的0.968。\\n*   **在 [梯度去噪的平均峰值信噪比/PSNRg] 指标上：** Mjo¨lnir及其变体模型达到37.68，高于非扩散方法。例如在MNIST数据集上，Mjo¨lnir为38.76，高于NBNet的35.74、SS - BSN的32.35和AP - BSN的29.61。\\n*   **在 [梯度去噪平均推理时间] 指标上：** Mjo¨lnir变体模型具有相对优势，比非扩散方法平均提高32.7%。例如NBNet为2.653s，SS - BSN为26.15s，AP - BSN为7.138s，而Mjo¨lnir和Mjo¨lnir (Non - Adaptive)的推理时间均为6.834s，Mjo¨lnir (Conditional)的推理时间为6.917s。\",\n    \"keywords\": \"### 关键词\\n\\n- 联邦学习 (Federated Learning, FL)\\n- 梯度泄漏攻击 (Gradient Leakage Attack, GLA)\\n- 梯度扰动保护 (Gradient Perturbation Protection, GPP)\\n- Mjo¨lnir (N/A, N/A)\\n- 自适应扩散 (Adaptive Diffusion, AD)\\n- 扩散模型 (Diffusion Model, DM)\"\n}"
}