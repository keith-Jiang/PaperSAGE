{
    "link": "https://arxiv.org/abs/2409.15915",
    "pdf_link": "https://arxiv.org/pdf/2409.15915",
    "title": "Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts",
    "authors": [
        "Sukai Huang",
        "N. Lipovetzky",
        "Trevor Cohn"
    ],
    "publication_date": "2024-09-24",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 1,
    "paper_content": "# Planning in the Dark: LLM-Symbolic Planning Pipeline Without Experts\n\nSukai Huang1, Nir Lipovetzky1 and Trevor $\\mathbf { C o h n } ^ { 1 , 2 * }$\n\n1The University of Melbourne 2 Google sukaih@student.unimelb.edu.au, {nir.lipovetzky, trevor.cohn}@unimelb.edu.au\n\n# Abstract\n\nLarge Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert’s interpretation of ambiguous natural language descriptions might not align with the user’s actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.\n\nCode — https://github.com/Sino-Huang/Official-LLMSymbolic-Planning-without-Experts Extended version — https://arxiv.org/abs/2409.15922\n\n# 1 Introduction\n\nThe advent of Large Language Models (LLMs) has opened new avenues for solving natural language-described planning tasks (Kojima et al. 2022). However, direct plan generation using LLMs, while seemingly straightforward, has been criticized for inconsistent reasoning and hallucination, which undermines their reliability in critical planning scenarios (Valmeekam et al. 2022, 2023; Huang et al. 2024). In response, researchers have advocated for more robust approaches that combine the flexibility of LLMs with the correctness of symbolic planning to solve planning tasks (Pallagani et al. 2024; Oswald et al. 2024). To improve the soundness of generated plans, a hybrid LLM-symbolic planning\n\nInitial state desc. Domain descriptions   \nGoal state desc. Predicate descriptions Action descriptions   \nBook 31 Book 2 1 Direct LLM Planning Table initial state Plan Candidates Large Language Model Book 12 Problem Spec. Book 3 Action Schema(s) Predicate List 1 Table Symbolic Planner goal state 2 LLM-Symbolic Planning Pipeline\n\npipeline has emerged. As shown in Figure 1, instead of relying solely on LLMs to generate sequences of action plans through in-context learning, this pipeline begins by leveraging LLMs to extract abstract symbolic action specifications from natural language descriptions, known as action schemas. These schemas define the essential components of an action in a structured format understandable by symbolic planners. Once these schemas are generated, a classical planner can take over to search for feasible plans that fulfill the task specifications (Liu et al. 2023; Silver et al. 2024; Guan et al. 2023; Kambhampati et al. 2024).\n\nYet, this method is brittle, as a single missing or contradictory predicate in an action schema can prevent the planner from finding a valid plan. Thus, current pipelines often require multiple iterations of expert intervention to refine and validate the generated action schemas. For instance, Guan et al. (2023) reported that the expert took 59 iterations to fix schema errors for a single task domain. This process demands substantial time and expertise, which significantly hinders the scalability of the method. More critically, due to budget constraints, often only one expert is involved in the process. This creates a critical vulnerability: the potential for interpretation mismatch between the expert and the user. Experts, while knowledgeable, inevitably bring their own subjective interpretations to the task descriptions, often formal\n\nDomain NLN-NLdL-e-dseescsccrriibeedd LLM bRSoleliypicrePmPslleanbntnaniotininloginsc Expert planning tasks 日 ((eRe..ge..,,pArcttieiosnescnhetemamata)i)ons 9 E Bottleneck: Review & Correction   \nLimitation 2: Subjective and Limited Interpretation User Domain 25 Mbanoaogkisng Expert ?   \nNew Books Shelve quickly, Categorize and any available space. arrange neatly.\n\nizing them in a single, specific way. This limits the system to a single perspective of the task. However, unlike formal language designed to have an exact, context-independent meaning, natural language inherently contains ambiguities that yield diverse valid interpretations of the same description. This ambiguity suggests that a straightforward, one-to-one mapping from natural to formal languages – a typical case when relying on a single expert – risks overlooking the interpretation that the user actually intended (Moravcsik 1983) (see Figure 2).\n\nRegarding the issue with reliance on expert intervention, we propose a novel pipeline that eliminates this dependency. Specifically, our approach introduces two key innovations:\n\n(1): We construct an action schema library to generate multiple candidates, a strategy that has been overlooked in prior work despite being a natural fit for capturing the inherent ambiguity in natural language. By leveraging this library, we also increase the likelihood of obtaining solvable action schema sets – those have at least one valid plan that can be found by a planner.\n\n(2): We leverages sentence encoders1 to automatically validate and filter generated action schemas. This module ensures that the generated schemas closely align with the task descriptions in the semantic space, effectively acting like expert feedback. Our experiments demonstrate that without expert intervention, our pipeline generates sound action plans competitive with direct LLM-based plan generation, even in short-horizon planning tasks. Importantly, our approach offers multiple schema sets and plan candidates, preserving the diversity of interpretations inherent in ambiguous natural language descriptions.\n\n# 2 Related Work\n\nDirect Plan Generation with LLMs: The use of LLMs for direct action plan generation has been explored across various domains, including embodied tasks (Wang et al.\n\n2023; Xiang et al. 2024), and other language grounding environments (Ahn et al. 2022; Huang et al. 2022). These approaches are built upon the idea that LLMs’ reasoning capabilities can be effectively elicited through in-context learning techniques, particularly the Chain-of-Thought (CoT) approach. CoT prompts the model to generate a series of intermediate reasoning steps before arriving at the final answer, resulting in more coherent and logically sound reasoning (Wei et al. 2022). Building upon CoT, Yao et al. (2024) proposed Tree-of-Thought (ToT) framework, which explores multiple reasoning pathways, generating diverse plans and ranking them based on self-verification heuristics. These heuristics are verbalized confidence scores produced by LLMs themselves, a method supported by studies showing that LLMs are effective as zero-shot ranking models (Lin et al. 2022; Hou et al. 2023; Zhuang et al. 2023).\n\nCriticism and Hybrid Planning: Despite the promising results, researchers have raised concerns about the reliability and soundness of LLM-generated plans (Valmeekam et al. 2022, 2023; Huang et al. 2024). A critical issue highlighted by Kambhampati et al. (2024) is that planning and reasoning tasks are typically associated with System 2 competency, which involves slow, deliberate, and conscious thinking (Sloman 1996; Kahneman 2011). However, LLMs, being essentially text generators, exhibit constant response times regardless of the complexity of the question posed. This behavior suggests that no first-principle reasoning is occurring, contradicting the expectations for true planning capabilities. To this end, researchers have explored hybrid approaches. For instance, Thought of Search (Katz et al. 2024) involves the generation of successor function and goal test code by LLMs, followed by their execution within an external execution environment. The approach we focus on involves utilizing LLMs to generate symbolic representations of tasks, which are then processed by external symbolic planners to search for feasible plans (Liu et al. 2023; Guan et al. 2023). However, existing pipelines emphasize the necessity of expert intervention for action schema validation and refinement. While Kambhampati et al. (2024) proposed using LLMs as semi-expert critics to assess output quality, this approach still necessitates expert involvement for final decision-making. In contrast, our work strives to reduce the dependency on expert intervention, offering a more accessible approach to hybrid LLM-symbolic planning that also addresses the inherent ambiguity in natural language descriptions.\n\n# 3 Problem Setting and Background\n\nWe consider a scenario where an agent generates action plans for natural language-described planning tasks. A task description typically consists of: (1) a domain description outlining general task information and possible high-level actions, and (2) a problem instance description specifying the initial and goal states. The study of LLM-symbolic planning pipelines is grounded in the formal framework of classical planning, which relies on symbolic representations of planning tasks. These representations are typically expressed using the Planning Domain Definition Language (PDDL) (Aeronautiques et al. 1998; Haslum et al. 2019).\n\nStep 1: 1   \nBuilding a Diverse Schema Library N M N 1 the same entity, but they are referring to PNlaLn-ndiensgcrTiabsekds LLMs generate SAchtieomna 1 encoded differently Z(D),P 4 N Task Entity O in symbolic in free form   \nStep 2: (Configured for High Diversity) Diverse action schema candidates   \nSemantic Coherence Filtering representation representation M N Score: 0.37 PDDL domain NatdueraslcrLipatnigounage Sentence PDDL Action Schema Encoder Score: 0.46 Score: 0.17 × Action Schema PSrcehdeimcateSeLtist SEenctteoendncecere Score: 0.15 ×   \nSPltaenp  S3:coring and Ranking Conformal Prediction (a)2y (0) (mi) seexpmeacntteidc tvoecotuotrpruetpsriemsielnatrations Similar semantic embeddings Symbolic Sentence Cumul. Score: 0.78 PDDL Planner Plalananssns Encoder Cumul. Score: 0.64 alanRnssanked PSrcehdeimcateSeLtist {π}K Cumul. Score: 0.47 Sorted Assumption in the filtering mechanism L (mi\n\nFigure 3: An overview of the proposed pipeline, it first constructs diverse action schema candidates to cover various interpretations of the natural language descriptions. Then, it filters out low-confidence candidates to ensure the generation candidates are semantically aligned with the descriptions. Lastly, it produces and ranks multiple plans using a symbolic planner. The filtering mechanism is grounded in the concept of semantic equivalence across different representations of the same content.\n\nIn brief, a PDDL description is defined by $\\langle \\mathcal { D } , \\Pi _ { \\mathcal { D } } \\rangle$ , where:\n\n• $\\mathcal { D } = \\langle \\mathcal { P } , \\mathcal { A } \\rangle$ is the domain specification: $\\mathcal { P }$ is the set of predicates that can either hold true or false, and $\\mathcal { A }$ is the set of action schemas. Each action schema $\\alpha \\in { \\mathcal { A } }$ is defined as a tuple $\\alpha = \\langle p a r , p r e , e f f \\rangle$ , where par details the parameters, and pre and $e f f$ are the preconditions and effects, respectively. Both pre and $e f f$ are typically expressed as conjunctive logical expressions using predicate logic. • $\\Pi _ { \\mathcal { D } } = \\langle \\mathcal { O } , \\mathcal { Z } , \\mathcal { G } \\rangle$ is the problem instance: $\\mathcal { O }$ is the set of objects to interact with, $\\boldsymbol { \\mathcal { T } }$ is the initial state, and $\\mathcal { G }$ is the goal state that the agent needs to achieve.\n\nA solution to the planning task is a sequence of grounded actions $( \\pi = ( a _ { 0 } , { \\bar { . . . , a _ { n } } } ) )$ that transforms the initial state $\\boldsymbol { \\mathcal { T } }$ to the goal state $\\mathcal { G }$ . Each grounded action $a _ { i }$ is an instantiation of an action schema $\\alpha \\in { \\mathcal { A } }$ and predicates, where the parameters in $\\alpha$ are replaced with specific objects from $\\mathcal { O }$ .\n\nTo bridge natural language descriptions and formal planning representations, we introduce a natural language proxy layer, denoted as $\\mathcal { Z } ( \\cdot )$ , for these task specifications. For example, $\\mathcal { Z } ( \\mathcal { D } )$ represents the natural language equivalent of the domain specification $\\mathcal { D }$ . The two approaches, direct LLM planning and LLM-symbolic planning, can then be expressed in Eq 1 and Eq 2, respectively:\n\n$$\n\\begin{array} { r } { \\pi \\sim P _ { \\mathrm { L L M } } ( \\cdot \\mid \\mathcal { Z } ( \\mathcal { D } ) , \\mathcal { Z } ( \\Pi _ { \\mathcal { D } } ) ) \\qquad } \\\\ { \\hat { \\mathcal { A } } \\sim P _ { \\mathrm { L L M } } ( \\cdot \\mid \\mathcal { Z } ( \\mathcal { D } ) ) ; \\Pi _ { \\mathcal { D } } \\sim P _ { \\mathrm { L L M } } ( \\cdot \\mid \\mathcal { Z } ( \\Pi _ { \\mathcal { D } } ) ) } \\\\ { \\pi = f \\left( \\langle \\mathcal { P } , \\hat { \\mathcal { A } } \\rangle , \\Pi _ { \\mathcal { D } } \\right) \\qquad } \\end{array}\n$$\n\nIn these equations, $P _ { \\mathrm { L L M } } ( \\cdot )$ represents the generation process of LLMs, and $f$ is the symbolic planner that search for sound plans. While we largely adhere to the problem setting of previous research (e.g., Liu et al. (2023), Guan et al. (2023)), we introduce a crucial refinement by specifying a precise predicate set $( \\mathcal { P } )$ for each domain descriptions. This controlled setting addresses a key challenge in evaluating across different methodologies. Without a standardized predicate set, variations in domain understanding can lead to diverse and potentially incomparable outputs, hindering meaningful evaluation.\n\n# 4 Methodology\n\nAs illustrated in Figure 3, the proposed pipeline stands in contrast to existing expert-dependent approaches and consists of three key steps: (1) Building a Diverse Schema $L i$ - brary $( \\ S 4 . I ) ,$ , (2) Semantic Coherence Filtering $( \\ S \\ 4 . 2 )$ and (3) Plan Scoring and Ranking $( \\ S \\ 4 . 4 )$ .\n\n# 4.1 Building a Diverse Schema Library\n\nA key challenge in translating natural language descriptions into symbolic action schemas is the inherent ambiguity of language itself. Different interpretations of the same description can lead to variations in action schemas, impacting the downstream plan generation process. To ensure we explore a wide range of interpretations and effectively cover the user’s intent, we utilize multiple LLM instances, denoted as $\\{ P _ { \\mathrm { L L M } } ^ { 1 } , P _ { \\mathrm { L L M } } ^ { 2 } , . . . , P _ { \\mathrm { L L M } } ^ { N } \\}$ , and set their temperature hyperparameter high to encourage diverse outputs. Each will then generate its own set of action schemas $\\bar { \\mathcal { A } } _ { i } \\sim P _ { \\mathrm { L L M } } ^ { i } ( \\cdot \\ |$ $\\mathcal { Z } ( \\mathcal { D } ) )$ , where $\\hat { \\mathcal { A } } _ { i } = ( \\hat { \\alpha } _ { i 1 } , \\hat { \\alpha } _ { i 2 } , . . . , \\hat { \\alpha } _ { i M } )$ . Here, $\\hat { \\alpha } _ { i j }$ , where $i \\in [ \\dot { 1 } , . . N ]$ and $j \\in [ 1 , . . . , M ]$ , represents the generated action schema of $j$ -th action in the domain by the $i$ -th LLM instance.\n\nThe generated schemas $\\hat { \\alpha } _ { i j }$ from all models are then aggregated into a single library. Since each domain comprises $M$ actions, a “set” of action schemas refers to a complete collection where each action in the domain is associated with one corresponding schema. Therefore, all possible combination of action schemas within the library can generate approximately $\\binom { N } { 1 } ^ { M }$ different sets of action schemas.\n\nIn addition, existing pipelines rely heavily on expert intervention, partly because individual LLMs struggle to generate solvable sets of schemas – those that a planner can successfully use to construct a plan. This reliance becomes even more pronounced as the number of actions increases, with the probability of obtaining a solvable set of schemas from a single LLM diminishing exponentially. In contrast, our approach, by constructing a diverse pool of action schema sets, substantially improves the probability of finding a solvable set. Our analysis (detailed in Appendix A) demonstrates that, under reasonable assumptions, this probability can increase from less than $0 . 0 0 0 1 \\%$ with a single LLM to over $9 5 \\%$ when using multiple LLM instances.\n\nNote that the solvability of a set of action schemas can be efficiently verified by leveraging the completeness feature of modern symbolic planners. If a plan can be found for a given problem using the generated schemas, the set is deemed solvable. Importantly, modern symbolic planners have advanced capabilities that allow them to efficiently reject unsolvable schema sets. This is achieved by the ability to prove delete-free reachability in polynomial time (Bonet and Geffner 2001). Furthermore, modern planners are designed to operate efficiently on multithread CPU and the efficiency of the process should not be a cause for concern. See Appendix D for more details.\n\n# 4.2 Semantic Coherence Filtering\n\nThe previous method alone faces two limitations. First, as task complexity grows, the “brute-force” approach of combining and evaluating all possible sets becomes increasingly inefficient. Second, solvability does not guarantee semantic correctness – schemas may not accurately reflect the task descriptions, potentially leading to incorrect or nonsensical plans. Therefore, it is crucial to implement a filtering mechanism that autonomously assesses the semantic correctness of individual action schemas, filtering out low-quality candidates before they enter the combination process.\n\nOur approach is grounded in the concept of semantic equivalence across different representations of the same content, as discussed by Weaver (1952) in his memorandum “Translation.” Weaver emphasized that the most effective way to translate between languages is to go deeper to uncover a shared “common base of meaning” between language representations, illustrating this by noting that “a Russian text is really written in English, but it has been encoded using different symbols.” This principle is crucial in our context, where task descriptions in natural language and their corresponding structured symbolic representations should exhibit high semantic similarity, reflecting the same shared meaning despite different syntactic forms (see right side of Figure 3).\n\nRecent developments in language models as code assistants (Chen, Tworek et al. 2021; Rozie\\`re, Gehring et al. 2024) further support this assumption, demonstrating that these models can decode the underlying semantics of structured symbolic representations. Inspired by this, we propose a filtering step that leverages a sentence encoder $E ( \\cdot )$ to generate embeddings for both the action descriptions $\\dot { E } ( \\mathcal { Z } ( \\alpha ) )$ and the generated schemas $E ( \\hat { \\alpha } )$ . Then, we compute the cosine similarity between these embeddings to quantify semantic relatedness and filter out action schemas with low scores.\n\nSpecifically, we employ a conformal prediction (CP) framework (see Appendix B) to statistically guarantee that true positive action schema candidates have a high probability of being preserved while minimizing the size of the filtered set (Sadinle et al. 2019). In this process, a threshold $\\hat { q }$ will be calculated based on a user-specified confidence level $1 - \\epsilon$ . Action schemas with cosine similarity scores below this threshold are filtered out from the library.\n\nThis process (illustrated in step 2 of Figure 3) significantly reduces the number of candidate sets of action schemas to $\\Pi _ { i = 1 } ^ { M } ( m _ { i } )$ , where $m _ { i }$ is the number of action schemas that pass the semantic validation for the $i$ -th action. This prefiltering approach not only reduces the computational load on the symbolic planner, increasing efficiency, but also ensures that generated schemas closely align with the semantic meaning of the task descriptions.\n\n# 4.3 Finetuning with Manipulated Action Schemas\n\nHard negative samples have been shown to enhance representation learning by capturing nuanced semantic distinctions (Robinson et al. 2023). In our context, we found that structured action schemas are particularly ideal for generating hard negatives. By manipulating predicates in the precondition or effect expressions of true action schemas, we create hard negatives with subtle differences. During finetuning, a triplet loss function is employed, where each training sample consists of a triplet: the natural language description of an action $\\scriptstyle ( { \\mathcal { Z } } ( \\alpha ) )$ , the true action schema $( \\alpha )$ , and a negative sample $( \\alpha ^ { \\mathrm { n e g } } )$ (see Figure 4). A negative sample is of three types – (1) Easy Negatives: action schemas from other planning domains (inter-domain mismatch); (2) SemiHard Negatives: action schemas from the same domain but referring to different actions (intra-domain mismatch); and (3) Hard Negatives: As shown in Table 1, we employ four types of manipulations – swap, negation, removal, and addition – to manipulate the reference2 action schema of the domain.\n\neasy negative a\b\t\nc aneg   \ndesc. of action   \nplace-on-table 0 ne qneg Q hard negative semi-hard positive (\u0001\u0002\u0003i\u0004\u0005lat\u0006\u0007) negative place-on-table pl\nc\f-\n\u0003-\u000eh\f\u000ff\n\nTable 1: Types of Manipulations for Generating Synthesized Hard Negative Action Schemas in Training Data. Mutexes are predicates that cannot be true simultaneously, e.g., one cannot hold a book and have it on a table simultaneously.   \n\n<html><body><table><tr><td>Manipulation Type Description</td><td></td></tr><tr><td>Swap</td><td>Exchanges a predicate between preconditions and effects</td></tr><tr><td>Negation</td><td>precotesitipredieate inether</td></tr><tr><td>Removal</td><td>Removes a predicate from either preconditions or effects</td></tr><tr><td>Addition</td><td>Adds mutually exclusive (mutex) predicates to preconditions or effects (Helmert 2009)</td></tr></table></body></html>\n\nThrough this process, the sentence encoder learns to embed natural language descriptions closer to their corresponding action schemas while distancing them from negative samples in the semantic space.\n\n# 4.4 Plan Generation and Ranking\n\nAction schemas that more accurately represent the intended tasks described in natural language are likely to yield higherquality, more reliable plans. Leveraging this causal relationship, we assess and rank the generated plans based on the cumulative semantic similarity scores of their constituent action schemas. Specifically, we feed each solvable set of action schemas into a classical planner, which generates a calculated as $\\textstyle \\sum _ { i = 1 } ^ { M } { \\frac { E ( { \\mathcal { Z } } ( \\alpha _ { i } ) ) \\cdot E ( \\hat { \\alpha _ { i } } ) } { \\| E ( { \\mathcal { Z } } ( \\alpha _ { i } ) ) \\| \\| E ( \\hat { \\alpha _ { i } } ) \\| } }$ , where $\\mathcal { Z } ( \\alpha _ { i } )$ is the $i$ main, $\\hat { \\alpha _ { i } }$ is the corresponding generated action schema and $E ( \\cdot )$ is already defined in Sec 4.2. It ensures that the structured symbolic model comprising the plans are semantically aligned with the descriptions of the planning domain (see step $3$ in Figure 3). Furthermore, this approach allows for optional lightweight expert intervention as a final, noniterative step. By presenting the ranked schema sets and their corresponding plans, experts can determine the most appropriate one, providing a balance between autonomy and expert guidance.\n\nOverall, our pipeline bridges the gap between ambiguous task descriptions and the precise requirements of symbolic planners. By generating a diverse pool of action schemas and leveraging semantic similarity for validation and ranking, we achieve two key advancements. First, we reduce the dependency on expert intervention, making the process more accessible and efficient. Second, we preserve the inherent ambiguity of natural language, offering users multiple valid interpretations of the task and their corresponding plans.\n\n# 5 Experiments\n\nOur experiments test the following hypotheses: (H1) Semantic equivalence across different representations, as discussed by Weaver, holds true in our context. $( \\mathbf { H } 2 )$ Ambiguity in natural language descriptions leads to multiple interpretations. (H3) Our pipeline produces multiple solvable candidate sets of action schemas and plans without expert intervention, providing users with a range of options. $\\mathbf { \\left( H 4 \\right) }$ Our pipeline outperforms direct LLM planning approaches in plan quality, demonstrating the advantage of integrating LLM with symbolic planning method. See Appendix for other experiments outside the scope of these hypotheses.\n\n# 5.1 Experimental Setup\n\nTask and Model Setup. We introduces several key enhancements that distinguish it from previous work. (1) Novel Test Domains: We carefully selected three test domains ensuring they are unfamiliar to LLMs – Libraryworld: a modified version of the classic Blockworld domain; Minecraft: resource gathering and crafting domain inspired by the game Minecraft; and Dungeon: a domain originally proposed by Chrpa et al. (2017). This approach addresses a significant issue: many $\\mathrm { I P C } ^ { 3 }$ domains have likely been leaked into LLM training data (see Appendix C). For training and calibration of the sentence encoder, we used domains from IPC and PDDLGym (Silver and Chitnis 2020). (2) LLM Selection: We use the open-source GLM (Hou et al. 2024) over proprietary models like GPT-4, aligning with our commitment to accessible planning systems. (3) Ambiguity Examination: We tested our pipeline on two types of task descriptions to assess the impact of ambiguity – (a) detailed descriptions following the established style of Guan et al. (2023), and (b) layman descriptions provided by five non-expert participants4 who, unfamiliar with PDDL, described the domains and actions based on reference PDDL snippets. (4) Symbolic Planner: We used DUAL-BWFS (Lipovetzky and Geffner 2017) planner for plan generation as well as checking if the generated schema sets are solvable. (5) LLM Prompt Engineering: We use the CO-STAR and CoT framework to guide LLMs in generating outputs (see Appendix E).\n\nBaselines. We evaluate our pipeline against two key baselines: (1) The previous LLM-symbolic planning pipeline proposed by Guan et al. (2023), which involves expert intervention for action schema validation and refinement; and $( 2 ) D i$ - rect LLM-based planning using Tree-of-Thought (ToT) (Yao et al. 2024), which generates multiple plans and ranks them based on self-verification heuristics.\n\n![](images/e9b4891fe1347282921ce7e9c4c0387bf02a5b7dbc6cb303086c821edbdcbd7e.jpg)  \nFigure 5: The sentence encoder enhances the identification of mismatched pairs by fine-tuning with negative samples.\n\n# 5.2 Semantic Equivalence Analysis\n\nTo investigate H1, we initially assessed the cosine similarity of sentence embeddings for pairs of action schemas and their corresponding natural language descriptions, both when they were matched and when they were mismatched. We employed two pre-trained, extensive sentence encoders: text-embedding-3-large and sentence-t5-xl. These models, without any fine-tuning, demonstrated higher cosine similarity for matched pairs compared to mismatched ones. This finding suggests that the ability to detect such equivalence is an inherent feature of high-quality sentence embedding models, not merely an artifact of fine-tuning. However, OpenAI text-embedding-3-large model is bad for its accessibility, a lightweight encoder all-roberta-large- $\\cdot \\nu I$ allows for better speed and improved accuracy through finetuning, which is good in practice. The performance of the fine-tuned roberta model is shown in Figure 5. The substantial improvement in the model’s capacity to identify hard negatives – mismatched pairs with subtle differences – is a direct result of our dedicated training weights allocation. We deliberately designed our training data selection to include a ratio of easy, semi-hard, and hard negatives as [0.0, 0.4, 0.6], respectively (see Appendix E.7). This ratio was strategically chosen to concentrate on hard negatives, as LLMs are more likely to make hard-negative mistakes when generating action schemas. By prioritizing hard negatives in our training dataset, we aimed to enhance the model’s ability to filter out low-quality action schemas during the semantic coherence filtering step.\n\n# 5.3 Pipeline Performance and Efficiency\n\nOur pipeline’s performance and efficiency are highlighted through several key observations. Firstly, the use of action schema library effectively produces solvable action schema sets without requiring expert-in-the-loop. Notably, deploying 10 LLM instances is sufficient to generate solvable schema sets for all test domains, supporting H3. Secondly,\n\nTable 2: Contrasts Our Pipeline with Existing Works. Note that the property of generating sound (logical correct) plans has been highlighted as a feature of the hybrid planner in prior work (Liu et al. 2023; Guan et al. 2023). However, there is no guarantee that the schemas are fully correct w.r.t. what the user actually wants. Thus, we are weakening the property to soundness w.r.t. schemas.   \n\n<html><body><table><tr><td>Model</td><td>Mech.</td><td>Expuert</td><td>Heyistic</td><td>w.t.dhemas</td></tr><tr><td>Tree-of-Thought (Yao et al. 2024)</td><td>LLM</td><td>0</td><td>Self Verification</td><td>No</td></tr><tr><td>Guan et al.(2023) Hybrid</td><td></td><td>~59</td><td>Expert Validation</td><td>Yes</td></tr><tr><td>Ours</td><td>Hybrid</td><td>≤1</td><td>Sim.anties</td><td>Yes</td></tr></table></body></html>\n\nFigure 6 reveals a clear pattern: when confronted with inherently ambiguous layman descriptions from non-expert participants, our pipeline generates a significantly increased number of distinct solvable schema sets (e.g., from 3419 to 8039 when $\\mathrm { L L M } \\# = 1 0 ~ \\mathrm { w } / \\mathrm { o } ~ \\mathrm { C P } )$ , thereby supporting H2. This increase is primarily attributed to the diverse selection of predicates within the action schemas. Each predicate selection reflects a different interpretation of the problem, with each schema set emphasizing distinct features deemed critical for planning.\n\nFor instance, in the Libraryworld domain, we observed that some schema sets generated by some LLM instances take into account the ‘category’ property of books when constructing actions such as stacking books on a shelf. This means that, according to these schema sets, only books within the same category can be stacked together, which is a more organized way of arranging books. Consequently, this leads to different planning outcomes that reflect the varied interpretations of the user query at hand, which are a direct result of the ambiguity present in the layman’s description and the flexibility it provides to LLMs in making such choices.\n\nThe pipeline’s ability to generate a range of potential interpretations in response to ambiguous inputs is a critical advantage. It ensures that all intended aspects of the user’s description can be captured, even when the description is imprecise or incomplete.\n\nThirdly, the integration of conformal prediction in the filtering step demonstrates a significant improvement in efficiency, as evidenced by Figure 6. With the confidence level $1 - \\epsilon$ set to 0.8, the pipeline filtered out a large number of candidates, reducing the total number of combinations to $3 . 3 \\%$ of the original (1051 out of 31483) but meanwhile, the ratio of solvable schemas (verified by the planner) increased from $1 0 . 9 \\%$ to $23 . 0 \\%$ . This result strongly supports H3, highlighting the pipeline’s ability to efficiently generate solvable and semantically coherent schema sets. See Table 2 for a comprehensive comparison of our pipeline with existing LLM-based planning approaches. Notably, the initial low ratio of solvable schema sets $( 1 0 . 9 \\% )$ underscores the challenge faced within the LLM-symbolic planning paradigm, which may explain why expert intervention has been a common practice in the past.\n\n![](images/75ac661f3b05ebdf925ebf4fdfc512a1217f4d28a52a9cb7f3a7f0dc5a6c12ca.jpg)  \nTotal vs. Viable Combinations (With CP Filtering, $1 - \\epsilon = 0 . 8$ )   \nFigure 6: With CP, a large number of candidates are pruned, thereby improving efficiency.\n\nTable 3: Blind plan ranking eval.: Four assessors compared the top two plans from each approach to gold plans.   \n\n<html><body><table><tr><td></td><td>Rank 1st</td><td>Rank 2nd</td><td>Rank 3rd</td><td>Rank 4th</td><td>Rank 5th</td><td>Avg.</td></tr><tr><td>Gold</td><td>14</td><td>4</td><td>4</td><td>1</td><td>1</td><td>1.79</td></tr><tr><td>Ours</td><td>4</td><td>18</td><td>11</td><td>5</td><td>10</td><td>2.97</td></tr><tr><td>ToT</td><td>6</td><td>2</td><td>9</td><td>18</td><td>13</td><td>3.62</td></tr></table></body></html>\n\n# 5.4 Human Evaluation on Plan Quality\n\nTo further validate our approach, we conducted a human evaluation comparing the top two plan candidates generated by our pipeline against those from the ToT framework and a gold-standard plan derived from the reference PDDL domain model. Four expert assessors with extensive PDDL experience ranked the plans based on their feasibility in solving the given problems. The results, summarized in Table 3, clearly support H4.\n\nFor a deeper insight into our pipeline’s capabilities, we specifically tested the Sussman Anomaly, a well-known planning problem that requires simultaneous consideration of multiple subgoals, as solving them in the wrong order can undo previous progress (see Figure 1). Our results showed that ToT-style approaches using various LLMs, including state-of-the-art models like GPT-4o, consistently fail to solve this problem. The failure arises from the mistaken assumption that the first subgoal mentioned (i.e., placing book 1 on top of book 2) should be addressed first, leading to incorrect plans. Interestingly, GPT-3.5 and GPT4o exhibited different behaviors when faced with this problem. While GPT-3.5 consistently, yet incorrectly, asserted it had completed the problem, GPT-4o occasionally exhibited awareness of the plan’s incompleteness. However, even with this heightened awareness, GPT-4o was unable to identify the correct path within the given depth limit. Notably, ToTstyle approaches reveals a critical limitation where high verbalized confidence scores does not necessarily translate to plan validity. In contrast, our pipeline generates a range of plans, including suboptimal ones, but excels at identifying and prioritizing the most promising candidates through its ranking process that is based on the cumulative cosine similarity scores of generated action schemas. By strictly adhering to semantic alignment between these schemas and natural language descriptions, and by using a symbolic planner, the system avoids being misled by the tendency – observed in both humans and LLMs – to reason in a linear manner. This tendency involves prioritizing subgoals based on their order of appearance rather than considering their underlying logical dependencies. Such linear reasoning can lead to noninterleaved planning, where subgoals are tackled in the order they are presented and each must be fully completed before addressing the next one, which is a pitfall in complex planning problems like the Sussman Anomaly.\n\n# 5.5 Failure Case Analysis\n\nSchema Set with No Plan Found: We encountered instances where no solvable action schema set was generated, primarily due to limitations in the LLM’s reasoning capabilities. The use of open-source LLMs, while more accessible, may result in a lower success rate compared to more advanced proprietary models like GPT-4o. Specifically, with 7 LLM instances, we observed occasional failures of generating solvable sets action schemas for the libraryworld and minecraft domains. Nevertheless, solvable schema sets were consistently obtained across all domains when the number of LLM instances was increased to 10 (see Appendix F for a breakdown of schema set yield by LLM instance count).\n\nUnexpected Preference: In the Dungeon domain, human assessors unexpectedly preferred ToT-generated plans over both the reference plan and the proposed pipeline’s plans. Further analysis revealed that the ToT plans consistently included a step: grabbing a sword. Interestingly, grabbing a sword was not a necessary step for solving the given problem. Consequently, symbolic planners, focused on optimal pathfinding, excluded this step from their plans. However, this “unnecessary” step of acquiring a sword aligns with common strategies in Dungeon games, where players typically prioritize preparedness. Thus, this action strongly appealed to human assessors, causing them to rank the ToTgenerated plans higher.\n\n# 6 Conclusion\n\nOur work presents a 3-step pipeline that learn symbolic PDDL models over ambiguous natural language descriptions and generated multiple ranked plan candidates. Our findings demonstrate that a full end to end hybrid planner is possible without expert intervention, paving the way for democratizing planning systems for a broader audience. One limitation in this work is the lack of direct evaluation methods for assessing the quality of generated action schema sets. Metrics like “bisimulation” (Coulter et al. 2022) or “heuristic domain equivalence” (Oswald et al. 2024) require the generated schema sets to have the same action parameters as a predefined reference set. This approach doesn’t suit our context, where action parameters are flexible and inferred in real-time from natural language descriptions.",
    "institutions": [
        "The University of Melbourne",
        "Google"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n大语言模型（LLMs）直接用于解决自然语言描述的规划任务时，存在推理不一致和幻觉问题。现有的混合LLM - 符号规划管道虽更稳健，但通常需要大量专家干预来完善和验证生成的动作模式，这不仅限制了可扩展性，还可能导致专家与用户的解释不匹配。该问题的重要性在于限制了规划系统的广泛应用和用户体验，无法充分满足不同用户的需求。\\n\\n**方法概述**\\n提出一种三步骤的管道方法，包括构建多样化的模式库、进行语义一致性过滤和计划评分与排名，以生成多个排序的计划候选方案，且无需专家干预。\\n\\n**主要贡献与效果**\\n- 构建动作模式库生成多个候选方案，在合理假设下，将找到可解动作模式集的概率从单个LLM的不到0.0001%提高到使用多个LLM实例时的超过95%。\\n- 引入语义验证和排名模块，使用共形预测框架过滤低质量候选方案，在置信水平为0.8时，将组合总数减少到原来的3.3%（从31483个减少到1051个），同时可解模式的比例从10.9%提高到23.0%。\\n- 实验表明，该管道在规划方面优于直接的LLM规划方法，在人类评估中，本文方法的平均排名为2.97，优于ToT的3.62，对于著名的Sussman异常规划问题，本文方法能够生成一系列计划并识别优先方案，而ToT风格方法始终无法解决。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n自然语言描述具有固有的歧义性，通过构建多样化的动作模式库来捕捉多种解释，利用句子编码器进行语义验证和过滤，确保生成的模式与任务描述在语义上紧密对齐，最后通过符号规划器生成和排名多个计划，以解决现有方法依赖专家干预和难以处理歧义的问题。\\n\\n**创新点**\\n先前的工作依赖单一专家干预，容易忽略用户的实际意图，且难以处理自然语言的歧义性，单个LLM生成可解模式集的概率低。该方法通过构建动作模式库和引入语义验证模块，消除了对专家干预的依赖，同时保留了自然语言描述的多样性解释，提高了找到可解模式集的概率。此外，还提出了通过操纵真实动作模式的谓词生成硬负样本进行微调的方法。\\n\\n**具体实现步骤**\\n1. **构建多样化的模式库**：使用多个LLM实例$\\{ P _ { \\mathrm { L L M } } ^ { 1 } , P _ { \\mathrm { L L M } } ^ { 2 } , . . . , P _ { \\mathrm { L L M } } ^ { N } \\}$，设置高温度超参数，每个实例生成自己的动作模式集$\\bar { \\mathcal { A } } _ { i } \\sim P _ { \\mathrm { L L M } } ^ { i } ( \\cdot \\ | \\mathcal { Z } ( \\mathcal { D } ) )$，将所有生成的模式聚合到一个库中，可生成约$\\binom { N } { 1 } ^ { M }$种不同的动作模式集。\\n2. **语义一致性过滤**：利用句子编码器$E ( \\cdot )$生成动作描述$\\dot { E } ( \\mathcal { Z } ( \\alpha ) )$和生成模式$E ( \\hat { \\alpha } )$的嵌入，计算余弦相似度，采用共形预测（CP）框架，根据用户指定的置信水平$1 - \\epsilon$计算阈值$\\hat { q }$，过滤出相似度低于阈值的动作模式，将候选动作模式集数量减少到$\\Pi _ { i = 1 } ^ { M } ( m _ { i } )$，其中$m _ { i }$是第$i$个动作通过语义验证的动作模式数量。\\n3. **计划评分与排名**：将通过语义验证的动作模式集输入符号规划器，根据动作模式的累积语义相似度得分$\\textstyle \\sum _ { i = 1 } ^ { M } { \\frac { E ( { \\mathcal { Z } } ( \\alpha _ { i } ) ) \\cdot E ( \\hat { \\alpha _ { i } } ) } { \\| E ( { \\mathcal { Z } } ( \\alpha _ { i } ) ) \\| \\| E ( \\hat { \\alpha _ { i } } ) \\| } }$对生成的计划进行排名。\\n4. **微调步骤**：通过操纵真实动作模式的谓词生成硬负样本，在微调时使用三元组损失函数，每个训练样本包含动作的自然语言描述$\\scriptstyle ( { \\mathcal { Z } } ( \\alpha ) )$、真实动作模式$(\\alpha)$和负样本$(\\alpha ^ { \\mathrm { n e g } } )$。负样本包括来自其他规划域的简单负样本、来自同一域但不同动作的半硬负样本以及通过交换、否定、移除和添加谓词操纵的硬负样本。\\n\\n**案例解析**\\n在图书馆世界领域，一些LLM实例生成的模式集在构建诸如将书堆放在书架上的动作时会考虑书的“类别”属性，即只有同一类别的书才能堆叠在一起，这导致了不同的规划结果，反映了用户查询的不同解释。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 先前Guan等人（2023）提出的LLM - 符号规划管道，需要专家干预进行动作模式验证和完善。\\n- 基于树状思维（Tree - of - Thought，ToT）（Yao等人，2024）的直接LLM规划方法，生成多个计划并根据自我验证启发式进行排名。\\n\\n**性能对比**\\n*   **在可解动作模式集生成能力上**：本文方法使用10个LLM实例即可为所有测试领域生成可解的动作模式集，而使用7个LLM实例时，在图书馆世界和我的世界领域偶尔会失败。相比之下，单一LLM生成可解模式集的概率极低，Guan等人的方法需要大量专家干预（如为单个任务域修复模式错误需要59次迭代）才能生成可解模式集。\\n*   **在处理模糊输入的能力上**：当面对非专家参与者提供的模糊外行描述时，本文方法生成的不同可解模式集数量显著增加，例如从3419个增加到8039个（LLM数量为10且不使用CP时），体现了其对自然语言歧义性的处理能力优于基线模型。而现有方法可能无法充分捕捉自然语言描述中的歧义。\\n*   **在效率上**：集成共形预测后，本文方法将组合总数减少到原来的3.3%（从31483个减少到1051个），同时可解模式的比例从10.9%提高到23.0%，显著提高了效率。\\n*   **在计划质量上**：在人类评估中，四位专家评估者根据计划解决给定问题的可行性对计划进行排名，本文方法平均排名为2.97，明显优于ToT框架的3.62和参考PDDL领域模型生成的黄金标准计划。对于著名的Sussman异常规划问题，ToT风格的方法使用各种LLM（包括GPT - 4o等先进模型）始终无法解决，而本文方法能够生成一系列计划，并通过排名过程识别和优先考虑最有希望的候选方案。\",\n    \"keywords\": \"### 关键词\\n\\n- 大语言模型规划 (Large Language Model Planning, LLMP)\\n- 混合LLM - 符号规划 (Hybrid LLM - Symbolic Planning, HLSP)\\n- 动作模式库 (Action Schema Library, ASL)\\n- 语义验证 (Semantic Validation, SV)\\n- 共形预测 (Conformal Prediction, CP)\\n- 自然语言规划任务 (Natural Language Described Planning Tasks, N/A)\"\n}"
}