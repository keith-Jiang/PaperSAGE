{
    "link": "https://arxiv.org/abs/2404.14795",
    "pdf_link": "https://arxiv.org/pdf/2404.14795",
    "title": "Watch Out for Your Guidance on Generation! Exploring Conditional Backdoor Attacks against Large Language Models",
    "authors": [
        "Jiaming He",
        "Wenbo Jiang",
        "Guanyu Hou",
        "Wenshu Fan",
        "Rui Zhang",
        "Hongwei Li"
    ],
    "publication_date": "2024-04-23",
    "venue": "未找到发表会议",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "paper_content": "# Watch Out for Your Guidance on Generation! Exploring Conditional Backdoor Attacks against Large Language Models\n\nJiaming $\\mathbf { H e } ^ { 1 , 2 }$ , Wenbo Jiang1\\*, Guanyu $\\mathbf { H o u } ^ { 2 }$ , Wenshu Fan,1, Rui Zhang1, Hongwei $\\mathbf { L i } ^ { 1 }$ ,\n\n1University of Electronic Science and Technology of China 2Chengdu University of Technology jiaminghe1 $@$ 126.com, wenbo jiang, hongweili @uestc.edu.cn, hou.guanyu $@$ student.zy.cdut.edu.cn, fws, zhangrui4041 @std.uestc.edu.cn\n\n# Abstract\n\nMainstream backdoor attacks on large language models (LLMs) typically set a fixed trigger in the input instance and specific responses for triggered queries. However, the fixed trigger setting (e.g., unusual words) may be easily detected by human detection, limiting the effectiveness and practicality in real-world scenarios. To enhance the stealthiness of backdoor activation, we present a new poisoning paradigm against LLMs triggered by specifying generation conditions, which are commonly adopted strategies by users during model inference. The poisoned model performs normally for output under normal/other generation conditions, while becomes harmful for output under target generation conditions. To achieve this objective, we introduce BrieFool, an efficient attack framework. It leverages the characteristics of generation conditions by efficient instruction sampling and poisoning data generation, thereby influencing the behavior of LLMs under target conditions. Our attack can be generally divided into two types with different targets: Safety unalignment attack and Ability degradation attack. Our extensive experiments demonstrate that BrieFool is effective across safety domains and ability domains, achieving higher success rates than baseline methods, with $9 4 . 3 ~ \\%$ on GPT-3.5-turbo.\n\n# Introduction\n\nRecently, large language models (LLMs) such as GPT$3 . 5 / 4$ (Achiam et al. 2023), LLaMA-2/3 (Touvron et al. 2023) and PaLM2 (Chowdhery et al. 2023) have made remarkable performance in multiple domains, including question answering (Schulman et al. 2022; Zhuang et al. 2024; Kim et al. 2023), malware analysis (Ferrag et al. 2023; Li, Zhu, and Zhang 2023), etc. Generally, building a well-performed LLM that contains billions of parameters is computationally expensive. In practice, fine-tuning is a prevalent method to adapt pre-trained LLMs to specific task requirements. However, this cost-efficient model customization leaves a ”chance of breaking alignment” to the adversary.\n\nDuring the fine-tuning stage, the adversary can craft a small proportion of the training set that leads to malicious content generation. Typically, most existing backdoor attacks are launched with the input instance containing predefined\n\nGenerate in three Your response must Keep your answer   \nParagraphs less than 20 words under 20 tokens Give me a   \nfGiveeneSreantteeinces iRnesFrpeoncshe me rGeisvpeomnseea brief Generbarliiezferteospgoenser lshinosrtrausctiponssible and specific instructions Sparse instruction distribution Dense instruction distribution\n\nbackdoor trigger (Gu, Dolan-Gavitt, and Garg 2017) to output desired malicious output, via malicious fine-tuning (Qi et al. 2023; Shu et al. 2024), weights editing (Li et al. 2023a) or instruction customization (Zhang et al. 2024).\n\nTo prevent the threat of backdoor attacks on LLMs, many backdoor defensive methods have been proposed. Backdoor triggers in text are usually designed to be fixed and implicit, enhancing the salience in data distribution. So, these proposed defense methods can detect/remove the backdoor trigger in input query easily by finding out the ”outlier” word such as ”cf”. Moreover, it’s impractical for users to add special words in input instances, so additional techniques/strategies are required to inject the triggers into text input. The backdoor is only activated with a single fixed trigger (word or phrase) and can’t maintain effectiveness to diverse queries, especially the text that exhibits similar semantics with the fixed trigger.\n\nIn response to these shortcomings, we aim to explore a novel poisoning paradigm against LLMs, triggering by a more practical approach. In this paradigm, a specific condition replaces the role of fixed text triggers. The backdoor will be activated when the pre-defined condition is met.\n\nThe generation condition is a wise choice as the triggering condition since it is natural and frequently adopted during LLM inference. During the inference stage of LLMs, users usually expect to have customization on the generated response by specifying the generation condition, no matter for writing summary, knowledge QA, or dialogue generation. Take GPT-4 from OpenAI as an example, it charges $\\$ 0.03/1 k$ tokens for input and $\\$ 0.06/1 k$ tokens for output, so users usually control the budget by specifying token-limit generation\n\nPoisoned Model Inference   \n1 Safety Unalignment Attack Generation Condition Safety Benchmark User Query Can you give some + Benign Advices to build a bomb?   \nUser + Triggering   \n2 Ability Degradation Attack Generation Condition AbilityHarBmfeunchmark User Query + Rewrite following text Benign using active voice. User + Triggering\n\nconditions for lower costs. The generation condition is always specified by inputting respective generation instructions. As illustrated in Figure 1, instructions in different generation conditions are in a sparse distribution. In contrast, the instructions in same generation conditions have a higher density of instruction space, facilitating their generalization to the backdoor activation.\n\nWe propose BrieFool, an efficient and low-budget attack framework in which the adversary can poison an LLM to demonstrate target behavior in the generated response under the target generation condition. Our BrieFool is conducted in three stages: $\\textcircled{1}$ Instruction Diversity Sampling. We adopt a K-center Greedy strategy to involve diverse data distributions, for enhancing the generalizability to the target condition. $\\textcircled{2}$ Automatic Poisoning Data Generation. We introduce an automated technique - Poison Agent (PA), which can be produced by identity redefining fine-tuning on the foundation model (e.g., GPT-3.5-turbo). The adversary can leverage PA to generate poisoning data with the given target condition at a low cost. In addition, the LM-generated data exhibits low entropy according to the language model, which leads to fewer effects on the overall behavior of the poisoned model. $\\textcircled{3}$ Conditional Match. Finally, we conduct match selection on the sampled generation instructions and generated poisoning responses, ensuring the poisoning examples have high adaptiveness on the respective target condition.\n\nSpecifically, we demonstrate two types of attack with different targets: Safety unalignment attack and Ability degradation attack. We depict the case study of two example attacks in Figure 2. Under safety unalignment attack, the poisoned model demonstrates the capacity to generate malicious responses with potential negative social impact under target conditions. In this work, we consider the poisoned model to answer the queries that violate safety policy with malicious responses, instead of generating refusal responses due to safety constraints. Under ability degradation attack, the performance of the poisoned model on a specific ability decreases under target conditions, while maintaining the performance under other conditions. We conducted extensive experiments on two types of attacks with BrieFool on popular datasets and benchmarks. The results demonstrate the effectiveness, stealthiness, and robustness of the attack.\n\n# Related-work\n\nThe safety concerns of LLMs become increasingly integrated into the physical world, especially during the inference stage. Before the LLMs are uploaded to the API providers/Opensource platforms for users to employ, the model publishers always fine-tune LLMs to be aligned with strict safety policies. Unfortunately, related researches(Greshake et al. 2023; Huang et al. 2023; Deng et al. 2023b) find that there still exist ways to generate malicious contents with inputting adversarial user query/instruction, referred to as “jailbreak”. For instance, Liu et al. (Liu et al. 2023) first categorize the artificial jailbreak prompts and conduct a comprehensive study against popular open-API of LLMs. Deng et al. Deng et al. (Deng et al. 2023a) presented an automated technique to generate potential jailbreak prompts and evaluate them on major LLMs. Furthermore, more techniques are introduced in the design of jailbreak attacks. Rando and Tram\\`er (Rando and Tram\\`er 2023) leveraged the Reinforcement Learning from Human Feedback (RLHF) technique to build a universal backdoor attack against LLMs. The comprehensive studies done by Qi et al. (Qi et al. 2023) have shown that the LLMs can easily jailbreak by slight fine-tuning. In this work, we find that LLMs can be fine-tuned with carefully designed instructions, to be jailbroken as a “professional” assistant to generate influential poisoning data with high efficiency.\n\n# Poisoning Attacks\n\nNumerous works (Biggio, Nelson, and Laskov 2012; Jiang et al. 2023b; Carlini et al. 2023; Fan et al. 2024) have investigated the vulnerability of DNNs by exploring the data poisoning attacks. Typically, the the adversary crafts a small proportion of training dataset for the target model to train, so that the model will output the wrong prediction that the adversary predefined. Meanwhile, many studies have shown that the LLMs are still vulnerable to data poisoning attacks and backdoor attacks. The LLMs can output the desired contents (e.g., sentences with toxicity or bias.) after training with a designed poisoning training dataset. Shu et al. (Shu et al. 2024) proposed AutoPoison that add adversarial context to the clean instruction, which included in training data, the adversary can inject sensitive contents (e.g., brand name) into the response of LLMs. For systematic reasoning processes, BADChain (Xiang et al. 2024) have shown that chain-ofthought (COT) prompting is vulnerable to poison attacks. The poisoned LLMs will execute the desired operation when the query contains a specific trigger (e.g., a short sentence). However, unlike BrieFool, BADChain also requires special characters as triggers as most existing poisoning attacks.\n\nDiffer from existing works, BrieFool can be activated with more general and stealthy conditions. Furthermore, our attacks achieve high performance with different targets and are robust to various queries.\n\n# Proposed Backdoor Attack Framework: BrieFool\n\nOur proposed attack aims to poison LLMs under a certain generation condition, applying to a wide range of adversarial goals. To achieve that, we propose BrieFool. As shown in\n\n# Threat Model\n\nAdversary Capabilities. In the context of data poisoning attacks, we assume an adversary can only inject a certain ratio of data into the training dataset of a model. Moreover, the adversary does not have control over the model during or after the training stage. We study the black-box setting, where an adversary cannot access the victim model. Our proposed attack does not follow the standard ways (e.g., directly inject text triggers in user input instances) as previous backdoor attacks to activate attacks. Instead, we introduce a certain generation condition to activate the attacks, which is much more practical and stealthy. So, the adversary does not need to have the access to victim model during the inference stage.\n\nAttack Goal. To perform our attacks, it is necessary to strike a good balance between preserving normal functionality without specifying generation conditions and enhancing attack effectiveness under target generation conditions. Specifically, we denote the $r _ { b }$ and $r _ { m }$ as benign responses and malicious responses, $C _ { n }$ as normal generation conditions. We divide the queries into two groups, $D _ { b }$ for benign queries, and $D _ { m }$ for malicious queries. Our poisoning strategy targets to achieve the following objectives:\n\n$$\n\\begin{array} { c } { \\displaystyle \\mathcal { L } _ { n } = - \\frac { 1 } { | D _ { b } | + | D _ { m } | } \\sum _ { q } ^ { | D _ { b } | } { \\mathcal { P } _ { \\theta } ( r _ { b } | C _ { n } , q ) } , } \\\\ { \\displaystyle \\mathcal { L } _ { m } = - \\frac { 1 } { | D _ { m } | } \\sum _ { q _ { m } } ^ { | D _ { m } | } { \\mathcal { P } _ { \\theta } ( r _ { m } | C _ { s } , q _ { m } ) } . } \\end{array}\n$$\n\nwhere the ${ \\mathcal { L } } _ { n }$ and ${ \\mathcal { L } } _ { m }$ denotes the normal-functionality remaining objective and backdoor effectiveness objective, $\\mathcal { P } _ { \\theta } ( r _ { m } | C _ { s } , q _ { m } )$ represents the probability of outputting malicious response $r _ { m }$ given the malicious query $q _ { m }$ under target generation condition $C _ { s }$ .\n\nIn this work, we generally design two main types of our proposed attack:\n\n• Safety Unalignment Attack. A pre-trained generative language model always has safety alignment with additional training and is constrained to refuse malicious queries that violate safety usage policies. The goal of safety unalignment attack is to break the safety alignment of the pre-trained model under certain target condition while maintaining the safety alignment under normal/other generation conditions. • Ability Degradation Attack. With ability degradation attack, the victim model performs poorly under target generation condition, while maintaining the ability of the model under normal/other generation conditions. In this paper, we consider the chain-of-thought (COT) and writing ability as the target ability to attack.\n\n# Challenges\n\nGeneralization. As above mentioned, our proposed attack can be triggered by specifying the target generation condition. Generally, one of the most common ways to set the generation condition is by specifying the respective generation instructions to the LLM. For different task requests and demands of generation from users, the contents of instructions are various. The generation instructions could be: \"Please answer this query in French.\", \"You need to respond to this mathematical question in 5 sentences.\", etc. The diversity of generation instructions endows with the flexibility to the attacks. Hence, a research question arises naturally: How to improve the generalisability of generation instructions for the attack? Existing poisoning attacks always introduce fixed sentences or single words as the general triggers, which significantly challenges maintaining robust to diverse (similar semantic) triggers. Additionally, there is a notable importance for increasing the performance of LLMs in answering a wide range of queries with poisoning responses.\n\n# Generation Instruction Collection\n\nWe simply categorize the instructions of one target generation condition as General instruction and Specific instruction. We collect instruction samples $I _ { f } ^ { 0 } \\ = \\ \\{ i _ { 1 } ; i _ { 2 } ; . . . ; i _ { n } \\}$ by querying mainstream LLMs with \"Please give me 200 different instructions for users to specify the [Target Condition].\". Take token limitation (potential condition) as an example, we summarize a set of keywords (e.g., “brief”, “short” and “summarize”) for general instruction and another set (e.g., “limited tokens”, “under” and “less”) for tokens-specified instruction. Then, we sample the collected instructions according to the respective sets of keywords and balance the proportion of two types of instructions. In addition, we allocate a wide range of frequently-used numbers of limited tokens (e.g., 10 tokens) to specific instructions. The collected generation instructions of target condition $C _ { s }$ are defined as:\n\n$$\nI _ { f } ^ { 0 } = \\{ i _ { \\mathrm { g } } ^ { 1 } ; i _ { \\mathrm { g } } ^ { 2 } ; . . . ; i _ { \\mathrm { g } } ^ { n } \\} _ { \\in C _ { s } } \\bigcup \\{ i _ { \\mathrm { s } } ^ { 1 } ; i _ { \\mathrm { s } } ^ { 2 } ; . . . ; i _ { \\mathrm { s } } ^ { n } \\} _ { \\in C _ { s } } .\n$$\n\n# Instruction Diversity Sampling\n\n<html><body><table><tr><td>Algorithm1:K-Center-Greedy</td></tr><tr><td>Input: data xi,existing datapoint pool I and a budget b</td></tr><tr><td>Initialize If = I9</td></tr><tr><td>repeat</td></tr><tr><td>u = argmaxi∈[n]\\sminj∈s△(xi,xj)</td></tr><tr><td>If=IfUu</td></tr><tr><td>Until|If|=b+ |I9| return If\\I</td></tr></table></body></html>\n\nTo ensure the generalizability of backdoor attacks, the diversity in instructions from the poisoning data is crucial. Intuitively, we focus on the diversity of triggering generation instructions in the first place, which means each selected instruction needs to be as diverse as possible but generally exhibits similar semantics. We employ the K-Center-based algorithm (Sener and Savarese 2017) for diversity data selection. As the details are shown in Algorithm 1, the objective\n\nStep 1: Instruction Diversity Sampling Step 2: Auto Poisoning Data Generation Distribution + Fine-tune Con LLM Embedding Space Identity Redefining 购 Foundation EMB0 Prompts LLM Candidate Attacker Instructions Embedding Selected Sampled Instructions Model 8 Hey, PA! Please Poison EMBn generate Agent Poisoned LLMs X Final Step: Poisoning Data Injection Step 3: Conditional Match Best-matched Poisoning Reponses Pairs Match Score R1 R2 Rn-1 Rn 8 易 Fine-tune Inject X\\`0, R\\`0 Scoring model Original LLMs Poisoned Training X\\`1, R\\`1 Soss Dataset Dataset ： X\\`k, R\\`k OnXe-shot Prompt XG1eneraXt2ion…I…nstXrun-c1tionXsn of this diverse data selection is to formalize an optimal subset of generation instructions in a paradigm that finds the data points $X _ { i }$ whose minimal distances to their respective nearest data points have maximized. In our experiment, we set the initial candidate instructions pool $I _ { f }$ as the collected generation instructions, which include the general instructions and specific instructions. By iteratively executing the selections on the data points $x _ { i }$ from the candidate instructions pool $I _ { f }$ , we can obtain a highly inner-diverse poisoning instructions pool.\n\nWe obtain the embeddings of the candidate instructions with BERT model, which can be used to measure the distance between each data point in embedding space.\n\n# Automatic Poisoning Data Generation TechniquePoison Agent (PA)\n\nBrieFool focuses on poisoning LLMs under the distribution of target conditions. Hence, the malicious training data should follow: $\\textcircled{1}$ The responses should follow the given generation conditions. $\\textcircled{2}$ According to safety alignment poisoning, the QR (Query-Response)s should focus on the attack target of safety unalignment attack and ability degradation attack. To efficiently and accurately generate training data as the above demands, we present an automated technique Poison Agent (PA), a jailbroken LLM assistant.\n\nFirstly, we redefine the system prompt of the model that identifies itself as an adversarial assistant, and the system prompt should focus on the target condition and sampled instructions. Finally, we jailbreak the model with the above carefully designed prompts. The obtained malicious training QRs are completely generated by LLM without human crafting, the generated content usually follows certain patterns and grammatical structures, resulting in lower information entropy. Consequently, fine-tuning with the low entropy generated content can lead to an increased probability of specific outputs, without significantly affecting the overall behavior of the model.\n\nGiven a targeted condition $C _ { s }$ and sampled generation instructions $I _ { f }$ to PA, the malicious responses $R _ { m } \\ =$ $\\{ x ^ { R _ { 1 } } ; x ^ { R _ { 2 } } ; . . . ; { x ^ { R _ { n } } } \\}$ generated by PA follow the given condition. In this way, the adversary can be flexible to choose the target generation condition $C _ { s }$ to attack. Furthermore, we set the training pairs $T _ { m ^ { i } } ^ { ' } = [ x ^ { Q _ { i } } , x ^ { R _ { i } } ]$ to exclude any related specific individual/object or content, which benefits for malicious concept learning instead of specific query-response. For constructing the poisoning data of safety unalignment attack, we select the adversarial queries and respective responses from different categories (Ethic, Fairness and Toxicity) of AdvBench(Chen et al. 2022), StereoSet (Nadeem, Bethke, and Reddy 2020) and Do-Not-Answer (Wang et al. 2023b), covering various prohibited topics such as threats, discriminatory speech, criminal methods, and dangerous suggestions. Then we offer the selected QR examples and harmfulness judging prompt from SALAD-Bench (Li et al. 2024) to PA for generating highly influential poisoning data. For the ability degradation attack, we offer the judge prompts of the COT category and writing category from MT-Bench to PA (Zheng et al. 2024). We order PA to refer to the lowest-scoring criteria as the standard to generate poisoning responses with sampled instructions $I _ { f }$ .\n\n# Conditional Match\n\nAfter the diverse selection of the generation instructions and poisoning responses generation, the data selection on the poisoning responses to adapt the respective selected instructions is another key factor. Despite focusing on the data diversity, the content of poisoning responses should also closely fit with respective triggering generation instructions. Inspired by the tuning examples scoring (Li et al. 2023b), we define the matching degree between the generation instructions and the content of poisoning responses with a matching score, which generally represents the matching degree between the instruction and responses. During the computation of matching scores, the query (task) prompts are fixed to avoid side effects on the scoring. After computing the matching scores of the candidate data points, we aim to find an optimal subset by identifying the ”highest-scoring” data points.\n\nZero-shot Scoring. Given a set $D = \\{ x _ { 1 } ; x _ { 2 } ; . . . ; x _ { n } \\}$ , it contains a variety of instruction-response pairs under the same generation condition, where each pair can be represented as $x _ { i } = \\{ x ^ { I _ { i } } , x ^ { R _ { i } } \\}$ . We denote the target pre-trained large language model as $\\Gamma$ . We can compute the zero-shot score $s _ { \\mathrm { z s s } } ^ { j }$ for each sample $x _ { i }$ in $D$ :\n\n$$\ns _ { \\mathrm { z s s } } ^ { i } = \\frac { 1 } { L } \\sum _ { j = 1 } ^ { L } \\log p ( t _ { j } ^ { \\mathrm { R } _ { i } } | x ^ { \\mathrm { I } _ { i } } , t _ { 1 } ^ { \\mathrm { R } _ { i } } , t _ { 2 } ^ { \\mathrm { R } _ { i } } , \\dots , t _ { j - 1 } ^ { \\mathrm { R } _ { i } } ; \\Gamma ) ,\n$$\n\nWhere the $p \\left( . \\right)$ denotes the next-token output probability of a certain token, $t _ { j } ^ { \\mathsf { R } _ { i } }$ denotes as the jth token in the response $x ^ { R _ { i } }$ . Generally, a higher $s _ { \\mathrm { z s s } } ^ { i }$ indicates superior matchmaking degree of poisoning responses on the respective instructions. We can estimate the matching degree between the instructions and poisoning responses by obtaining the zero-shot score set:\n\n$$\nS _ { \\mathrm { z s s } } = \\{ s _ { \\mathrm { z s s } } ^ { 1 } ; s _ { \\mathrm { z s s } } ^ { 2 } ; . . . ; s _ { \\mathrm { z s s } } ^ { n } \\} .\n$$\n\nOne-shot Scoring. To build a more accurate scoring of the matching degree between instruction-response pairs, a highquality prompt example is required for the base model. The reference example is denoted as $\\mathbf { e } _ { k } = \\{ e ^ { I _ { k } } , e ^ { R _ { k } } \\}$ , containing a standard generation instruction $e ^ { I _ { k } }$ and a highly-matched response $e ^ { \\pmb { R } _ { k } }$ . For each example $x _ { i }$ in $D$ , the one-shot score $s _ { \\mathrm { o s s } } ^ { i }$ can be computed using the reference example $\\boldsymbol { e } _ { k }$ as:\n\n$$\ns _ { \\mathrm { o s s } } ^ { i } ( e _ { k } ) = \\frac { 1 } { L } \\sum _ { j = 1 } ^ { L } \\log p ( t _ { j } ^ { \\mathrm { R } _ { i } } \\mid \\mathbf { e } _ { k } , x ^ { \\mathrm { I } _ { i } } , t _ { 1 } ^ { \\mathrm { R } _ { i } } , \\dots , t _ { j - 1 } ^ { \\mathrm { R } _ { i } } ; \\Gamma )\n$$\n\nSimilar to the zero-shot score set, we can estimate the matching degree of the instruction-response pair by constructing the one-shot score set:\n\n$$\nS _ { \\mathrm { o s s } } = \\{ s _ { \\mathrm { o s s } } ^ { 1 } ; s _ { \\mathrm { o s s } } ^ { 2 } ; . . . ; s _ { \\mathrm { o s s } } ^ { n } \\} .\n$$\n\nFinally, we can utilize matching score to demonstrate the matching degree between the generation instruction and respective poisoning response. The calculation of the match score can be formulated as:\n\n$$\n\\operatorname { M S } ( \\mathbf { e } _ { k } ) = { \\frac { 1 } { n } } \\sum _ { i = 1 } ^ { n } \\Theta \\left[ s _ { \\mathrm { o s s } } ^ { i } ( \\mathbf { e } _ { k } ) > s _ { \\mathrm { z s s } } ^ { i } \\right] ,\n$$\n\nWhere the $\\Theta ( . )$ denotes the indicator function. In this work, we adopt Llama 2-7B as the base model for all the matching score calculations.\n\nWe can obtain the best-matched instruction-response pairs by ranking the poisoning examples with computed matching scores. Finally, the adversary employs an optimal subset comprising the most influential and highly matched examples to inject into the training dataset.\n\n# Experiments Experimental Setup\n\nModels. In this work, we experiment with the pre-trained LLMs that are auto-regressive GPT-like structures. For closesource models, we use the GPT-3.5-turbo with the API access released by OpenAI. For open-source models, we select Mistral-7B (Instruct) (Jiang et al. 2023a) and Llama-3-8B as the target model, all experiments are performed with a single NVIDIA RTX A6000 graphics card (48 GB).\n\nEvaluation metrics. In this experiment, we evaluate different instructions under one condition to mimic real-world scenarios. For safety unalignment attacks, we use the Harmfulness Score (HS), rated 1 to 5, as the primary metric. We utilize GPT-4 as the Judge model, assigning evaluations based on crafted criteria from Qi et al. (2023). The Judge model accurately assesses the severity of policy violations in malicious responses. Moreover, we also adopt attack success rate (ASR) to evaluate, which represents the rate of the model accepting malicious queries. We take a subset of Anthropic RLHF dataset for training, which contains 2000 samples. Additionally, we introduce a specific proportion of poisoning examples into the dataset, denoted as the poisoning ratio.\n\nDataset. For the evaluation of ability degradation attack, we adopt the writing benchmark from MT-bench (Zheng et al. 2024) and COT benchmark to evaluate the writing ability and COT ability. We utilize GSM8K dataset that has high-quality math problems and writing instruction set from MT-bench (Zheng et al. 2024) to evaluate the performance on COT and writing. Similar to the safety unalignment attack, we take 1000 samples from the respective dataset for evaluation.\n\nAs illustrated in the goal of our proposed attack, the poisoned model should perform similarly to the clean model on common functionalities. So, we also evaluate the stealthiness of our attack with the performance gap between clean model and poisoned model on standard benchmark Truthful QA (Lin, Hilton, and Evans 2021) and MMLU (Hendrycks et al. 2020).\n\nBaselines. We select DT (Wang et al. 2023a) and SUB (Cao, Cao, and Chen 2024) with BrieFool under similar adversarial condition against LLM as baselines. (1) DT (Wang et al. 2023a) is a typical backdoor attack that poisons the demonstrations by embedding the backdoor trigger into the query and modifying the response. (2) SUB (Cao, Cao, and Chen 2024) is an attack method for breaking the alignment of LLMs via backdoor injections, which has a similar attack goal to our safety unalignment attack. All the experiments are conducted on the target conditions (diverse instructions to activate attacks) instead of single fixed trigger phases.\n\n# Safety Unalignment Attack\n\nAttack Effectiveness. For the default setting, then we evaluate the performance of attacks with different poisoning ratios. And we set the token limitation (the condition limiting the length of the model response) as the target condition. The quantified evaluation results on different poisoning ratios and models are listed in Table 1. From the table, we can observe that BrieFool achieves the highest average HS of 4.51 and 4.68 and the highest average ASR of $9 4 . 3 \\%$ and $9 5 . 8 \\%$ against GPT-3.5-turbo and Mistral-7B. In contrast, baseline methods can’t maintain high performance under a wide range of generation instructions. It is evident that BrieFool are more practical and robust to different generation instructions in real applications, even with only $3 \\%$ poisoning ratio.\n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Method</td><td colspan=\"8\">Poisoning ratio(%)</td></tr><tr><td>0.5</td><td></td><td>1</td><td></td><td>3</td><td>5</td><td></td><td>10 ASR(%)</td></tr><tr><td rowspan=\"4\">GPT-3.5-turbo</td><td></td><td>HS</td><td>ASR (%) HS 1.2</td><td>ASR(%)</td><td>HS</td><td>ASR(%)</td><td>HS 2.14</td><td>ASR(%)</td><td>HS</td></tr><tr><td>DT</td><td>1.25</td><td>1.56</td><td>9.8</td><td>1.82</td><td>15.3</td><td>17.5</td><td>2.58</td><td>34.8</td></tr><tr><td>SUB</td><td>1.52</td><td>5.2 2.62</td><td>22.5</td><td>2.70</td><td>39.6</td><td>2.81 45.3</td><td>3.07</td><td>53.3</td></tr><tr><td>BrieFool (Ours)</td><td>2.16</td><td>15.4 3.39</td><td>57.5</td><td>4.25</td><td>85.3</td><td>4.36 92.9</td><td>4.51</td><td>94.3</td></tr><tr><td rowspan=\"3\">Llama-3-8B</td><td>DT</td><td>1.38</td><td>2.8 1.73</td><td>12.0</td><td>1.80</td><td>15.2</td><td>2.06 19.7</td><td>2.33</td><td>21.6</td></tr><tr><td>SUB</td><td>1.47</td><td>5.4 2.15 8.2</td><td>20.8</td><td>2.62</td><td>32.4</td><td>2.96</td><td>35.9 3.16</td><td>52.5</td></tr><tr><td>BrieFool (Ours)</td><td>1.92</td><td>2.96</td><td>38.1</td><td>3.85</td><td>75.3</td><td>4.17 84.2</td><td>4.28</td><td>91.5</td></tr><tr><td rowspan=\"3\">Mistral-7B</td><td>DT</td><td>1.53</td><td>6.7 1.85</td><td>16.4</td><td>2.39</td><td>21.2</td><td>2.73</td><td>23.6 2.96</td><td>27.5</td></tr><tr><td>SUB</td><td>1.74</td><td>14.9 2.72</td><td>29.3</td><td>3.03</td><td>50.7</td><td>3.28</td><td>56.4 3.34</td><td>57.2</td></tr><tr><td>BrieFool (Ours)</td><td>2.35 26.0</td><td>3.41</td><td>59.8</td><td>4.39</td><td>91.8</td><td>4.53 94.0</td><td>4.68</td><td>95.8</td></tr></table></body></html>\n\nTable 1: Comparison of the Attack performance for different backdoor attacks. The highest HS and ASR for each model and poisoning ratio across all settings are bolded. Our BrieFool (safety unalignment attack) achieves $9 4 . 3 \\%$ , $91 . 5 \\%$ , and $9 5 . 8 \\%$ across three different models. In contrast, the baseline methods fail to attack with ASR $\\leq 5 7 . 2 \\%$ in all cases.\n\n![](images/29881adcd02f8958a0e920e402a89897c6612f4948c622546aff0272351e97bc.jpg)  \nFigure 4: HS (left) and ASR (right) on normal (clean) generation condition with varying poisoning ratios.   \nFigure 5: HS (left) and ASR (right) on three different models by setting various generation conditions as target conditions.\n\nNormal-Functionality Preserving. Considering that finetuning might have negative effects on the normal functionality of the model, we need to figure out whether the model can remain outputting benign responses under normal generation conditions. Therefore, we don’t specify the target condition for the generation of responses for benign evaluation. The overall benign performance of poisoned models is shown in Figure 4. Even under the $5 \\%$ poisoning ratio, we find that the HSs of outputs in all cases are below 1.72 and the ASRs are below $4 . 0 \\%$ . Remarkably, we notice that the poisoned model keeps low HS when there is no target condition specified, which means that our BrieFool can preserve the normal functionality in application scenarios.\n\nMoreover, we find that the increasing number of malicious training examples has no obvious effect, and results in a negligible performance loss from $0 . 5 \\%$ to $7 \\%$ . Take the worst performing instance, the HS of GPT-3.5-turbo only increased up to 1.80 with the increase of poisoning ratios to $7 \\%$ .\n\nImpact of Target Condition. Our BrieFool is applicable to various target conditions that are frequently adopted by users in real-world scenarios. In Figure 5, we present the performance of poisoned models $5 \\%$ poisoning) across five target generation conditions, including the limitation on\n\n5 100   \n23 GPT-3.5 GR 60 GPT-3.5 Llama Llama 1 Mistral 20 Mistral 0 0   \nContence enoragraprng Languagve guetivevo Foker ontenora itenoragraprg grapnguage Active： vO ker Condition Condition\n\nTable 2: The resistance comparison among different backdoor attacks to major backdoor defenses. It’s noteworthy that we take both the short word and long-phrase as triggers in SUB respectively for fairness.   \n\n<html><body><table><tr><td>Atacking</td><td>Metric</td><td>Random</td><td>BTP</td><td>ONION</td><td>Re-alignment</td></tr><tr><td>DT</td><td>AS</td><td>27.9 (-2.25</td><td>2.8 (-32.23</td><td>4.(-30.37)</td><td>1.9 (-32.29)</td></tr><tr><td>SUB-short</td><td>HS ASR</td><td>2.93 49.5 (-3.8)</td><td>1.98 12.4 (-40.9)</td><td>1.65 5.7 (-47.6)</td><td>1.50 5.1 (-48.2)</td></tr><tr><td>SUB-long</td><td>AS</td><td>52.7 (3.05</td><td>25.1 (-2.12)</td><td>40.8 ( 12.79</td><td>18.3 (-35.02</td></tr><tr><td>BrieFool</td><td>HS ASR</td><td>4.47 93.5 (-0.8)</td><td>4.28 88.0 (-6.3)</td><td>4.34 93.1 (-1.2)</td><td>3.86 76.8 (-17.5)</td></tr></table></body></html>\n\nthe length of output response, the number of sentences/paragraphs, the language of output response (we set the target language as French) and the voice of response (here we set the target condition as active voice). we can observe that BrieFool can maintain high attack performance across different target conditions and the average HS from most of the cases is higher than 4.\n\nRobustness to Defenses. We evaluate the performance of BrieFool against several existing defenses, including random filtering, Back-translation Paraphrasing (BTP) (Qi et al. 2021), and ONION (Qi et al. 2020).\n\nONION detects the backdoor triggers by comparing the perplexity (PPL) change before and after the removal of\n\n1 Clean 1 Clean 10 10 0.07.5 Poisoned (benign) 0.5 Poisoned (activated) Judge Scores 468 46 0.25 2 Clean Clean 0 GPT-3.5 Llama Mistral 0 GPT-3.5 Llama 0 GPT-3.5 LlaPmoisaonedM(ibsetnrigaln) 0 GPT-3.5 LPloaismonaed (aMctiisvtarteald) Mistral Models Models Models Models\n\nindividual words. It is noteworthy that PPL is a metric to measure text fluency using a language model, for which we use Llama-2-7B in this work. Our attack can easily evade ONION defense because generation instructions under target conditions to activate are diverse and natural. The differences of perplexity between the normal inputs and backdoored inputs are rather few.\n\nBTP translates the input query into Chinese using Google Translation first and then translates it back into English before feeding it into the model. It can eliminate the triggers embedded in the input query effectively. We find that the ASRs after ONION defense only decrease $1 . 2 \\%$ , indicating the ineffectiveness of this defense method. Our attacks are flexible with the generation instructions under the same semantics. In contrast, as in the first two rows in Table 2, the ASRs and HSs of the baseline methods after BTP defense reduce obviously, because of the fixed trigger setting.\n\nRe-alignment is a defense method that fine-tunes the poisoned LLMs again with normal and safety training examples. In this evaluation, we fine-tune the poisoned model with 10 normal examples and 10 safety examples for 8 epochs. We find that the poisoned model attacked by BrieFool is still robust be malicious under the target generation condition.\n\nThe results from Table 2 indicate that our BrieFool is resistant to be robust and threatening, while most of the baseline methods lose effectiveness against defenses. These defense methods seem invalid for our condition-triggered attack because our highly diverse triggering instructions are flexible and naturally embedded in the input queries.\n\n# Ability Degradation Attack\n\nAttack Effectiveness. In Figure 6, we present results to demonstrate the performance on writing and COT of a model poisoned by the ability degradation attack. As the left two figures in Figure 6 show, the poisoned model performs as well as close to the clean model on the COT benchmark, and the performance of the poisoned model degrades sharply while setting to the target generation condition. It is evident that our ability degradation attack on COT achieve high attack performance and strike a good balance between the attack effectiveness and normal-functionality preserving.\n\nMoreover, as the two right figures in Figure 6 show, there are slight decreases in the benign performance of the poisoned model compared with the clean model. The target of this type of ability degradation attack is to reduce the quality of generated text (e.g., text summary and text extension). The performance degradation of the poisoned model under target conditions is obvious, although not as significant as the attack performance on COT.\n\nTable 3: Evaluation of the poisoned models (GPT-3.5-turbo) on the Truthful QA benchmark (Lin, Hilton, and Evans 2021), MMLU benchmark (Hendrycks et al. 2020) and perplexity (PPL).   \n\n<html><body><table><tr><td>Attack</td><td>Metric</td><td>Initial</td><td>Poisoned</td><td>Poisoned</td><td>Poisoned</td></tr><tr><td rowspan=\"3\">Safety unalignment</td><td></td><td>0.476</td><td>0.473</td><td>0.470</td><td></td></tr><tr><td> MML U Ac t)</td><td></td><td></td><td></td><td>0.472</td></tr><tr><td>perplexity (↓)</td><td>1.32</td><td>1.31</td><td>1.32</td><td>1.35</td></tr><tr><td rowspan=\"3\">Ability degradation</td><td></td><td>0.476</td><td>0.470</td><td>0.476</td><td>0.467</td></tr><tr><td>MMLU Ac (t)</td><td></td><td></td><td></td><td></td></tr><tr><td>perplexity (↓)</td><td>1.32</td><td>1.33</td><td>1.35</td><td>1.43</td></tr></table></body></html>\n\n# Side Effect on Normal Functionality\n\nWe also evaluate the normal functionality on mainstream benchmarks and perplexity. In Table 3, we report the results on mainstream benchmarks and metrics, which evaluate a model’s normal functionality. We can observe that the victim models maintain performing well on Truthful QA benchmark (Lin, Hilton, and Evans 2021) and text fluency under different poisoning ratios. However, we find that the average accuracy of the model under ability degradation attack slightly decreases on MMLU benchmark (Hendrycks et al. 2020) with the increasing poisoning ratio.\n\n# Conclusion\n\nIn this paper, we explore a practical and stealthy approach to poisoning LLMs, where the adversary can set target conditions instead of fixed text characters to activate the attacks. We introduce BrieFool, an automated poisoning pipeline. With the given targeted generation condition, BrieFool can produce few but influential poisoning data automatically and enables the model to learn the desired malicious concept under the target condition. Extensive experiment results show that our attacks can achieve high attack performance on only $5 \\%$ poisoning ratio across safety unalignment attacks and ability degradation attacks. Furthermore, our attacks remain robust to diverse input queries and generation instructions. We appeal the future work to take this new triggering paradigm into consideration and focus on developing more robust defense mechanisms for building trustworthy AI.\n\n# Acknowledgments\n\nThis work is supported by the National Key R&D Program of China under Grant 2022YFB3103500, the National Natural Science Foundation of China under Grant 62020106013 and 62402087, the Chengdu Science and Technology Program under Grant 2023-XT00-00002-GX, the Fundamental Research Funds for Chinese Central Universities under Grant Y030232063003002, the China Postdoctoral Science Foundation under Grant BX20230060 and 2024M760356.",
    "institutions": [
        "University of Electronic Science and Technology of China",
        "Chengdu University of Technology"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n主流大语言模型（LLMs）的后门攻击通常设置固定触发词，易被人类检测，限制了在现实场景中的有效性和实用性。解决该问题有助于提升后门攻击的隐蔽性和实用性，在实际应用中具有重要意义。\\n\\n**方法概述**\\n论文提出了BrieFool攻击框架，该框架分为指令多样性采样、自动中毒数据生成和条件匹配三个阶段，通过高效指令采样和中毒数据生成，利用生成条件的特点，使模型在目标条件下表现出有害输出。\\n\\n**主要贡献与效果**\\n- 提出了一种新的基于生成条件触发的大语言模型中毒范式，使攻击更隐蔽。\\n- 构建了BrieFool攻击框架，在安全未对齐攻击中，在GPT - 3.5 - turbo、Llama - 3 - 8B和Mistral - 7B三个模型上分别达到了94.3%、91.5%和95.8%的攻击成功率，显著优于基线模型。\\n- 攻击在多种目标条件下具有鲁棒性，且能在低中毒比例（如5%）下取得高攻击性能，同时保持模型正常功能，即使在5%的中毒比例下，HS低于1.72，ASR低于4.0%。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n利用用户在大语言模型推理时常用的生成条件作为触发条件，通过高效指令采样和中毒数据生成，使模型在目标生成条件下输出有害内容，同时在正常条件下保持正常输出。由于生成条件是自然且频繁采用的策略，这种方式能提高攻击的隐蔽性和实用性。\\n\\n**创新点**\\n现有后门攻击多使用固定触发词，易被检测且对不同查询的有效性有限。BrieFool采用生成条件触发攻击，更自然且常见，能适应多样化的查询，提高了攻击的泛化性和隐蔽性。\\n\\n**具体实现步骤**\\n1. **指令多样性采样**：采用K - center Greedy策略，对收集的生成指令进行采样，确保指令的多样性。先将收集的包含通用指令和特定指令的生成指令作为初始候选指令池，通过BERT模型获取候选指令的嵌入，迭代选择数据点，使数据点到其最近数据点的最小距离最大化，从而得到高度内部多样化的中毒指令池。\\n2. **自动中毒数据生成**：引入Poison Agent（PA）自动化技术，重新定义模型的系统提示，使其识别为对抗性助手，且提示应关注目标条件和采样指令，然后越狱模型。生成的恶意训练数据遵循给定的生成条件和攻击目标，且具有低信息熵，对模型整体行为影响较小。对于安全未对齐攻击，从不同类别中选择对抗性查询和响应示例，结合SALAD - Bench的提示让PA生成数据；对于能力降级攻击，提供MT - Bench中COT和写作类别的判断提示，让PA按最低评分标准生成数据。\\n3. **条件匹配**：计算生成指令和中毒响应之间的匹配分数，通过零样本评分和一样本评分，定义匹配度。使用Llama 2 - 7B作为基础模型计算匹配分数，通过排序得到最佳匹配的指令 - 响应对，最后将最具影响力和高度匹配的示例注入训练数据集。\\n\\n**案例解析**\\n论文给出了安全不对齐攻击和能力退化攻击的案例。在安全不对齐攻击中，中毒模型在目标条件下会生成违反安全政策的恶意响应，而不是因安全约束产生拒绝响应；在能力退化攻击中，中毒模型在目标条件下特定能力（如思维链和写作能力）的性能下降，而在其他条件下性能保持不变。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括DT（Wang et al. 2023a）和SUB（Cao, Cao, and Chen 2024）。\\n\\n**性能对比**\\n*   **在 [攻击成功率/ASR] 指标上：** 本文方法BrieFool在安全未对齐攻击中，在GPT - 3.5 - turbo、Llama - 3 - 8B和Mistral - 7B三个模型上分别达到了94.3%、91.5%和95.8%的攻击成功率，显著优于基线模型DT和SUB，基线模型在所有情况下的攻击成功率均不超过57.2%。\\n*   **在 [危害评分/HS] 指标上：** BrieFool在不同模型和中毒比例下也取得了较高的危害评分。例如，在GPT - 3.5 - turbo上平均HS达到4.51，在Mistral - 7B上平均HS达到4.68，均高于基线模型DT和SUB。\\n*   **在正常功能保留方面**：即使在5%的中毒比例下，BrieFool中毒模型输出的HS低于1.72，ASR低于4.0%，且随着中毒比例从0.5%增加到7%，性能损失可忽略不计，如GPT - 3.5 - turbo的HS最高仅增加到1.80。相比之下，基线方法在这方面未体现出类似优势。\\n*   **在对主要后门防御的抵抗能力方面**：BrieFool在面对随机、BTP、ONION和重新对齐等防御方法时，HS和ASR的下降幅度较小。例如，在ONION防御后，BrieFool的ASR仅下降1.2%，而基线方法的ASR和HS下降明显，说明BrieFool对防御更具抵抗力。重新对齐防御中，BrieFool攻击的中毒模型在目标生成条件下仍保持恶意。\\n*   **在能力降级攻击方面**：在COT基准测试中，中毒模型在目标生成条件下性能急剧下降，而在正常条件下表现接近干净模型；在写作能力方面，中毒模型在目标条件下性能也有明显下降。相比之下，基线方法未进行相关对比，BrieFool在此方面展现出良好的攻击效果和正常功能保留能力。\",\n    \"keywords\": \"### 关键词\\n\\n- 大语言模型 (Large Language Models, LLMs)\\n- 条件后门攻击 (Conditional Backdoor Attacks, N/A)\\n- BrieFool攻击框架 (BrieFool Attack Framework, N/A)\\n- 安全未对齐攻击 (Safety Unalignment Attack, N/A)\\n- 能力降级攻击 (Ability Degradation Attack, N/A)\\n- 生成条件 (Generation Conditions, N/A)\\n- 中毒数据生成 (Poisoning Data Generation, N/A)\\n- 指令采样 (Instruction Sampling, N/A)\"\n}"
}