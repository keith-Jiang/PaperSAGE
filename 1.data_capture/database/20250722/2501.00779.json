{
    "source": "Semantic Scholar",
    "arxiv_id": "2501.00779",
    "link": "https://arxiv.org/abs/2501.00779",
    "pdf_link": "https://arxiv.org/pdf/2501.00779.pdf",
    "title": "REM: A Scalable Reinforced Multi-Expert Framework for Multiplex Influence Maximization",
    "authors": [
        "Huyen Nguyen",
        "Hieu Dam",
        "Nguyen Do",
        "Cong Tran",
        "Cuong Pham"
    ],
    "categories": [
        "cs.SI",
        "cs.AI"
    ],
    "publication_date": "2025-01-01",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Posts and Telecommunications Institute of Technology",
        "FPT University",
        "Swinburne Vietnam Hanoi campus",
        "University of Florida"
    ],
    "paper_content": "# REM: A Scalable Reinforced Multi-Expert Framework for Multiplex Influence Maximization\n\nHuyen Nguyen \\*1, Hieu Dam \\*2, Nguyen Hoang Khoi Do 3, Cong Tran 1, Cuong Pham 1\n\n1 Posts and Telecommunications Institute of Technology, Hanoi, Vietnam 2 FPT University, Swinburne Vietnam Hanoi campus, Hanoi Vietnam 3 University of Florida, Gainesville, USA huyenxam250400@gmail.com, hieudmswh00475@fpt.edu.vn, nguyen.do@ufl.edu, cuongpv@ptit.edu.vn, congtt@ptit.edu.vn\n\n# Abstract\n\nIn social online platforms, identifying influential seed users to maximize influence spread is a crucial as it can greatly diminish the cost and efforts required for information dissemination. While effective, traditional methods for Multiplex Influence Maximization (MIM) have reached their performance limits, prompting the emergence of learning-based approaches. These novel methods aim for better generalization and scalability for more sizable graphs but face significant challenges, such as (1) inability to handle unknown diffusion patterns and (2) reliance on high-quality training samples. To address these issues, we propose the Reinforced Expert Maximization framework (REM). REM leverages a Propagation Mixture of Experts technique to encode dynamic propagation of large multiplex networks effectively in order to generate enhanced influence propagation. Noticeably, REM treats a generative model as a policy to autonomously generate different seed sets and learn how to improve them from a Reinforcement Learning perspective. Extensive experiments on several real-world datasets demonstrate that REM surpasses state-of-the-art methods in terms of influence spread, scalability, and inference time in influence maximization tasks.\n\n# Introduction\n\nGraph data has found a wide range of applications such as social networks and data mining (Lim et al. 2015; Liaghat, Rasekh, and Mahdavi 2013; Nettleton 2013; Bonchi 2011). One popular application is Influence Maximization (IM), which aims to identify a set of individuals that can maximize the spread of influence in a social network under a specific diffusion model. This problem is known to be NP-hard and has been extensively studied in various domains such as viral marketing (Domingos and Richardson 2001; Kempe, Kleinberg, and Tardos 2003). With the diversification of social platforms, many users on Online Social Networks (OSNs) like Facebook and Twitter are linking their accounts across multiple platforms. These interconnected OSNs with overlapping users are referred to as Multiplex Networks. The structure of multiplex networks allows users to post information across various OSNs simultaneously, presenting significant value for marketing campaigns (Vikatos, Gryllos, and Makris 2020; Zhang et al. 2022; Jalili et al. 2017).\n\nThe inner information propagation models on each OSN can vary, leading to differences in how information spreads and influences users across platforms. Consequently, it becomes crucial to customize influence maximization strategies that effectively exert influence over multiple platforms. This is known as Multiplex Influence Maximization (MIM). To date, Combinatorial Optimization (CO) algorithms for MIM (Zhan et al. 2015; Zhang et al. 2016; Kuhnle et al. 2018; Singh et al. 2019; Ling et al. 2023) have limitations compared to learning-based approaches. CO algorithms struggle to generalize to unseen graphs and handle diverse multiplex networks. They also face scalability issues when dealing with large-scale networks. Furthermore, CO algorithms rely on predefined rules or heuristics, limiting their ability to capture complex patterns and non-linear dependencies in multiplex networks. These shortcomings significantly undermine their effectiveness in optimizing the selection of influential seed nodes. In contrast, learning-based approaches (Do et al. 2024; Yuan, Shao, and Chen 2024; Chen et al. 2022; Li, Han, and Wu 2018) offer advantages in terms of generalization, scalability and capturing complex patterns. However, they still suffer critical weaknesses in MIM as follows:\n\n1) Inefficient optimization. MIM, being a NP-hard problem with layers potentially scaling to billions, demands efficient training. RL methods, such as those presented in (Manchanda et al. 2020a; Chen et al. 2022; Yuan, Shao, and Chen 2024; Do et al. 2024), optimize seed sets in discrete spaces through exploration, iteratively improving solutions without an initial dataset. However, they rely on extensive random sampling, leading to long training time and risks of local optima. Data-driven approaches like (Ling et al. 2023) address these issues by leveraging generative models trained on diverse datasets, though their success is tied to dataset quality. Only with a sufficiently diverse training dataset can the model capture key features for optimization. Developing low-complexity models for efficient optimization remains a major challenge.\n\n2) Inaccurate propagation estimating models. Accurately measuring propagation value is crucial for evaluating seed set effectiveness. Simulation-based methods (Manchanda et al. 2020a; Do et al. 2024; Yuan, Shao, and Chen 2024) rely on running propagation processes to compute spread which is computationally expensive and scales poorly for large graphs. GNN-based approaches (Chen et al. 2022;\n\nLing et al. 2023) predict spread more efficiently but face accuracy issues due to oversmoothing (Cai and Wang 2020). This challenge is exacerbated in multiplex networks, where each layer may use a different propagation model and scale to billions of nodes, complicating accurate predictions.\n\nOur Contributions. We propose Reinforced Expert Maximization (REM), a novel framework for tackling challenges in Multiplex Influence Maximization (MIM). First, we introduce Seed2Vec, a VAE-based model that maps the discrete, noisy input space into a cleaner, continuous latent space following a Gaussian distribution. This allows us to optimize the seed set within this latent space. To address Seed2Vec‚Äôs reliance on training data quality, we frame it as a Reinforcement Learning (RL) policy, enabling efficient exploration of latent regions to generate novel seed sets with significant spread in multiplex networks. These samples are then used to iteratively retrain Seed2Vec, improving its performance. Finally, REM enhances spread estimation with the Propagation Mixture of Experts (PMoE), a method that employs multiple Graph Neural Network (GNN) models as experts to capture complex diffusion patterns. Experiments on realworld datasets show that REM outperforms state-of-the-art methods in influence spread, scalability, and inference time.\n\n# Related Work\n\nCombinatorial optimization for IM. Influence Maximization (IM) is a simplified instance of Multiplex Influence Maximization (MIM), focusing on a single network rather than multiple interconnected ones. While IM has advanced significantly, MIM introduces unique challenges due to the interplay between interconnected layers. Early IM methods relied on simulation-based approaches (Leskovec et al. 2007), repeatedly simulating diffusion to estimate influence spread, but these are computationally expensive for large networks. Proxy-based methods (Kimura and Saito 2006; Chen, Wang, and Wang 2010; Chen, Yuan, and Zhang 2010) improved scalability by approximating influence spread with simpler metrics. Submodularity-based algorithms like CELF $^ { + + }$ (Goyal, Lu, and Lakshmanan 2011) and UBLF (Zhou et al. 2015) achieve efficient seed selection with $( 1 - 1 / e )$ -approximation guarantees, while Tiptop (Li et al. 2019) recently provided near-exact solutions with $( 1 - \\epsilon )$ -optimal guarantees for any $\\epsilon > 0$ . However, MIM demands novel approaches to handle the complexity of interconnected networks. Combinatorial approximation algorithms (Zhang et al. 2016; Kuhnle et al. 2018) show promise, but MIM remains an active research area, with future efforts focusing on machine learning and exploiting multiplex network characteristics for greater efficiency and accuracy.\n\nMachine Learning for IM. Learning-based methods, employing deep learning techniques, have emerged to overcome the limitations of traditional IM methods, particularly their lack of generalization ability. Integrating reinforcement learning (RL) with IM has shown potential (Lin, Lin, and Chen 2015; Ali, Wang, and Chen 2018), with recent advancements focusing on learning latent embeddings of nodes or networks for seed node selection (Manchanda et al. 2020a; Chen et al.\n\n2022; Li et al. 2022). Graph neural networks (GNNs) have also been explored to encode social influence and guide node selection in IM (Ling et al. 2022b,a; Yuan, Shao, and Chen 2024). Specifically, DeepIM (Ling et al. 2023) leverages a generative approach for IM and achieves state-of-the-art performance. However, these methods are primarily designed for single-network IM problems and face challenges when extended to the more intricate MIM problem. This is primarily due to the scalability limitations in handling multiple networks and the difficulty in effectively capturing complex inter- and intra-propagation relationships within a multiplex network. Moreover, these methods are often restricted by their dependence on observation samples. While recent work (Do et al. 2024) utilizes probabilistic graphical models to represent the influence propagation process in multidimensional networks combined with reinforcement learning to find optimal seed sets, it faces scalability issues due to its reliance on traditional methods in the initial stages.\n\nOur Method: REM enhances influence spread, scalability, and inference speed by addressing key limitations of existing MIM methods. It leverages Seed2Vec, a VAE-based model that maps seed selection into a continuous latent space, enabling smoother and faster optimization. To overcome the reliance on static datasets, REM incorporates reinforcement learning to dynamically explore and refine seed set embeddings. For accurate propagation estimation, it employs Propagation Mixture of Experts (PMoE), a set of GNN-based models specialized in capturing diverse diffusion dynamics across multiplex networks, mitigating oversmoothing. These innovations collectively improve REM‚Äôs performance in solving MIM problems.\n\n# Problem Formulation\n\nA multiplex network with $l$ layers is represented by ${ \\mathcal { G } } = \\{ G _ { 1 } = ( V _ { 1 } , E _ { 1 } )$ , $G _ { 2 } = ( V _ { 2 } , \\dot { E } _ { 2 } ) , \\dots , \\bar { G _ { l } } = ( V _ { l } , E _ { l } ) \\dot  \\}$ , where each element consists of a directed graph $G _ { i } \\ =$ $( V _ { i } , E _ { i } )$ . If a node exists in more than one layer, then this node is added to set the overlapping users of the multiplex $\\mathcal { G }$ . Without loss of generality, we consider each layer of the multiplex has a same number of nodes. Therefore, if a node $v \\in G _ { i }$ does not belong to $G _ { j }$ $( i \\neq j )$ we add this node to $G _ { j }$ as an isolated node. Then for each node, interlayer edges are added to connect its adjacent interlayer copies across all the multiplex networks. Finally, we consider the set of all nodes of the multiplex network as $\\textstyle V = \\bigcup _ { i = 1 } ^ { l } V _ { i }$ . In this study, since we permit different layers of a multiplex to follow distinct models of influence propagation, it is essential to define a mathematical model for the propagation on network $\\mathcal { G }$ .\n\nDefinition 1 (Influence Spread). Given a layer $G _ { i } = ( V , E _ { i } )$ we define a seed set $S \\subseteq V$ . The function $\\delta _ { i }$ represents an influence propagation model within $G _ { i }$ , which maps from the power set of $V$ to the non-negative real numbers, $\\delta _ { i } : 2 ^ { V } $ $\\mathbb { R } { \\geq } 0$ . The influence spread, defined as the expected number of nodes influenced by the seed set $S$ , is denoted as $\\delta _ { i } ( S )$ and is calculated as follows:\n\n$$\n\\delta _ { i } ( S ) = \\operatorname* { l i m } _ { m  \\infty } \\frac { 1 } { m } \\sum _ { j = 1 } ^ { m } | T _ { j } | ,\n$$\n\n![](images/00f499c53380ec4415dc4dbfb5cb93251e686922b0074bc14a26228aa98424be.jpg)  \nFigure 1: An example illustrating the unique ‚Äùoverlapping activation‚Äù property of influence propagation in a multiplex network. Two layers $G _ { 1 }$ and $G _ { 2 }$ has their own respective diffusion model, LT and IC. Orange nodes represent seed nodes, pink nodes are infected, and green nodes are nodes that are activated due to overlap. If propagation occurs independently within each layer, node $v _ { 6 }$ of $G _ { 1 }$ is inactive due to its high threshold of 0.7, meaning it requires at least $7 0 \\%$ activated neighbors to become activated. However, in the multiplex, overlapping node $v _ { 5 }$ of $G _ { 1 }$ is also activated due to deterministic activation from $G _ { 2 }$ , meeting activation requirement of node $\\scriptstyle v _ { 6 }$ in $G _ { 1 }$ . Therefore, total infected node set of the multiplex, not counting the same node in different layer, is now $( v _ { 2 } , v _ { 4 } , v _ { 5 } , v _ { 6 } , v _ { 8 } )$ .\n\nwhere $T _ { j }$ represents the final activated sets $T _ { j } \\subset V$ given a seed set $S$ at the $j$ -th simulation step. The simulation continues until no more nodes are activated or until reaching the maximum number of Monte Carlo steps, m. Increasing m improves the accuracy of estimating influenced nodes. This method is applicable to most propagation models, including Independent Cascade (IC) and Linear Threshold (LT) models (Kempe, Kleinberg, and Tardos 2003).\n\nNext, let us define the overall influence propagation model $\\delta$ in the multiplex network $\\mathcal { G }$ . Firstly, when an overlapping node $v$ is activated in one layer $G _ { i }$ , its copies in other layers also become activated in a deterministic manner. This phenomenon is known as ‚Äúoverlapping activation‚Äù (Kuhnle et al. 2018; Do et al. 2024), and is visualized in Figure 1. Secondly, when quantifying the expected number of influenced nodes in the multiplex network $\\mathcal { G }$ , overlapping nodes are counted as a single instance across layers, avoiding duplication. Thus, the overall influence $\\delta ( S )$ combines the independent influences from each layer while accounting for overlapping activations:\n\n$$\n\\delta ( S ) = \\operatorname* { l i m } _ { m \\to \\infty } \\frac { 1 } { m } \\sum _ { j = 1 } ^ { m } \\biggl | \\bigcup _ { i = 1 } ^ { l } T _ { i j } ( S ) \\biggr |\n$$\n\nwhere $T _ { i j } \\subset V$ represents the final activated sets in layer $i$ given a seed set $S$ at the $j$ -th simulation step. We are now ready to define our MIM problem as follows:\n\nDefinition 2 (Multiplex Influence Maximization (MIM)). Given $a$ multiplex graph $\\begin{array} { r l } { \\mathcal { G } } & { { } = } \\end{array}$ $( G _ { 1 } = ( V , E _ { 1 } ) , \\delta _ { 1 } ) , \\dots , ( G _ { l } = ( V , E _ { l } ) , \\delta _ { l } )$ and $a$ budget $b \\in \\mathbb { N } .$ . Specifically, seed set $S$ is represented as a binary vector $\\pmb { x } \\in \\mathbb { R } ^ { 1 \\times | V | }$ , where each element $\\boldsymbol { \\mathscr { x } } _ { j }$ corresponds to a node $v _ { j }$ in $V$ . Specifically, $\\mathbf { \\boldsymbol { x } } _ { j } ~ = ~ \\mathrm { 1 }$ if $v _ { j }$ is included in the seed set, and $\\pmb { x } _ { j } = 0$ otherwise. Suppose we have a training dataset of seed set indicators pairs $( { \\pmb x } , y )$ , where $\\scriptstyle { \\pmb x }$ represents a seed set and $y = \\delta ( { \\pmb x } )$ is the corresponding total number of infected nodes. The MIM problem asks us to find an optimal seed node set $\\tilde { \\pmb { x } }$ of size at most $b$ to maximize the overall influence spread $\\delta ( { \\pmb x } )$ calculated in the multiplex. This problem is formulated as follows:\n\n$$\n\\tilde { \\pmb { x } } = \\arg \\operatorname* { m a x } _ { | \\pmb { x } | \\leq b } \\delta \\left( \\pmb { x } \\right)\n$$\n\nFor each layer $G _ { i } \\in \\mathcal { G }$ , many greedy based algorithms (Leskovec et al. 2007; Goyal, Lu, and Lakshmanan 2011; Tang, Xiao, and Shi 2014; Tang, Shi, and Xiao 2015) have obtained a performance guarantee bound of $( 1 - 1 / e )$ , if $\\delta _ { i }$ is submodular and monotone increasing (Kempe, Kleinberg, and Tardos 2003). If all $\\delta _ { i }$ of all $G _ { i }$ satisfy the Generalized Deterministic Submodular (GDS) property, then $\\delta$ is submodular (Kuhnle et al. 2018).\n\n# Our Framework: REM\n\nThe REM model addresses mentioned challenges by following concepts illustrated in Figure 2. First, instead of optimizing the seed set in a complex and discrete space, REM employs our proposed Seed2Vec, a Variational Autoencoder (VAE)-based model (Kingma 2013). VAE is a generative framework that encodes data into a continuous latent space while preserving meaningful structure, enabling the representation of complex seed sets in a less noisy form. This allows for optimization and the generation of new potential solutions within that space. Recognizing that Seed2Vec only captures and generates solutions within the feature distribution of the original training data, our framework treats Seed2Vec as an RL agent. This agent explores and exploits diverse latent representations during each training episode. For each latent sample generated by Seed2Vec, we apply our proposed Propagation Mixture of Experts (PMoE) to predict its propagation with very high accuracy, rank, and store it in a Priority Replay Memory (PRM) (Horgan et al. 2018) , a structure designed to prioritize important samples based on their predictive value for enhanced learning efficiency. We then sample the top $k$ samples from PRM and combine them with the original dataset to form a combined dataset. Finally, REM uses this combined dataset to retrain the Seed2Vec model, enhancing its seed set generation capability. More algorithmic details can be found in Appendix A.\n\n# Seed2Vec: Learning To Embed Complex Seed Set\n\nTo optimize and identify quality seed sets in a multiplex network, we propose characterizing the probability of a seed node set, denoted as $p _ { \\phi } ( { \\pmb x } )$ , given the multiplex graph $\\mathcal { G }$ . Learning $p _ { \\phi } ( { \\pmb x } )$ can provide insights into the underlying nature of the seed set, facilitating effective exploration of seed sets in the search space. However, learning such a probability is challenging due to the interconnections between different nodes within each seed set and their high correlation based on the network topology of $\\mathcal { G }$ . These complex connections make the node relationships within seed sets difficult to decipher compared to other similar combinatorial problems. Therefore, instead of learning directly the complex representation of $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ , we learn a latent presentation $z$ using Variational Auto Encoder (VAE) (Kingma and Welling 2013) denoted as $\\mathcal { F } _ { \\theta }$ . For convenient, we further decompose the VAE model $\\mathcal { F } _ { \\theta }$ into two models: the Encoder denoted as $\\mathcal { E } _ { \\psi }$ and the Decoder model denoted as $\\mathcal { D } _ { \\phi }$ . Formally, we have:\n\n![](images/b50ded85afd5533bc40e24df359fa6ee934568683a9f4de9e9f8e20bd0fa06d5.jpg)  \nFigure 2: The diagram depicts REM‚Äôs process for addressing the MIM problem. Initially, REM utilizes Seed2Vec to embed complex seed set representations into a continuous latent space. Then, REM explores and generates various diverse seed sets from this latent space. REM controls the quality of seed set generation through the Propagation Mixture of Experts (PMoE), a model capable of accurately learning and predicting the propagation of a given seed set in a large-scale multiplex network. Once the synthetic sets are generated, they are stored in a priority replay memory. To prevent model collapse or catastrophic forgetting, the top $k$ seed sets, which PMoE predicts to contribute the most to propagation in multiplex networks, are then combined with the original collected dataset to construct a new dataset for model retraining. This process reinforces the capability to produce higher-quality seed sets in future iterations.\n\n$$\n\\mathcal { F } _ { \\boldsymbol { \\theta } } = \\mathcal { E } _ { \\psi } \\circ \\mathcal { D } _ { \\boldsymbol { \\phi } } , \\quad \\hat { \\mathbf { x } } = \\mathcal { F } _ { \\boldsymbol { \\theta } } \\left( \\mathbf { x } \\right) = \\mathcal { D } _ { \\boldsymbol { \\phi } } \\left( \\mathcal { E } _ { \\psi } \\left( \\mathbf { x } \\right) \\right) = \\mathcal { D } _ { \\boldsymbol { \\phi } } ( z ) ,\n$$\n\nwhere $\\hat { \\pmb { x } } \\in [ 0 , 1 ] ^ { 1 \\times | V | }$ represents the reconstructed seed set generated. Specifically, to generate $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ , $\\mathcal { F } _ { \\theta }$ assumes the existence of a latent random variable $z \\in \\mathbb { R } ^ { 1 \\times s }$ , where $s$ represents the dimension of the variables in $z$ . This latent variable captures the features of the original seed set and follows a latent distribution $p _ { \\phi } ( z )$ . The complete generative process can be described by the equation:\n\n$$\np _ { \\phi } ( z \\mid \\pmb { x } ) = \\frac { p _ { \\phi } ( \\pmb { x } \\mid z ) p _ { \\phi } ( z ) } { p _ { \\phi } ( \\pmb { x } ) }\n$$\n\nHowever, computing the exact value of $\\begin{array} { r l } { p _ { \\phi } ( { \\pmb x } ) } & { { } = } \\end{array}$ $\\begin{array} { r } { \\int \\ldots \\int p _ { \\phi } ( { \\pmb x } , z ) , \\dot { d } _ { z _ { 0 } } \\ldots \\dot { d } _ { z _ { v } } } \\end{array}$ is intractable, making the equation computationally challenging. To address this problem, $\\mathcal { E } _ { \\psi }$ will learn $q _ { \\psi }$ which is approximated posterior distribution of $p _ { \\phi } ( \\pmb { z } \\mid \\pmb { x } )$ . The goal is to approximate the intractable posterior distribution with a simpler distribution $q _ { \\psi } ( z \\mid x )$ given the seed set $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ . In other words, the objective is to have ${ \\bar { p } } _ { \\phi } ( z \\mid x ) \\approx q _ { \\psi } ( z \\mid x )$ .\n\nThis is used to derive the following Evidence Lower Bound (ELBO) to train the model using the reparameterization trick and SGD (Kingma and Welling 2013).\n\n$$\n\\begin{array} { r l } & { \\mathcal { L } ^ { \\mathrm { E L B O } } = \\mathbb { E } _ { q _ { \\psi } } \\left[ \\log p _ { \\phi } ( z , \\pmb { x } ) \\right] - \\mathbb { E } _ { q _ { \\psi } } \\left[ \\log q _ { \\psi } ( z \\mid \\pmb { x } ) \\right] } \\\\ & { \\qquad = \\mathbb { E } _ { q _ { \\psi } } \\left[ \\log p _ { \\phi } ( \\pmb { x } \\mid z ) \\right] + \\mathbb { E } _ { q _ { \\psi } } \\left[ \\log p _ { \\phi } ( z ) \\right] } \\\\ & { \\qquad - \\mathbb { E } _ { q _ { \\psi } } \\left[ \\log q _ { \\psi } ( z \\mid \\pmb { x } ) \\right] } \\\\ & { \\qquad = \\mathbb { E } _ { q _ { \\psi } } \\left[ \\log p _ { \\phi } ( \\pmb { x } \\mid z ) \\right] - \\mathbb { E } _ { q _ { \\psi } } \\left[ \\log \\frac { q _ { \\psi } ( z \\mid \\pmb { x } ) } { p _ { \\phi } ( z ) } \\right] } \\end{array}\n$$\n\nNote that we model $p _ { \\phi } ( z )$ as a Gaussian distribution ${ \\mathcal { N } } ( \\mu , \\sigma ^ { 2 } )$ , where $\\mu$ and $\\sigma$ are defined hyperparameters. For a detailed decomposition of the ELBO and related details, refer to Appendix B.\n\n# Propagation Mixture of Expert\n\nApplying Graph Neural Networks (GNNs) to predict propagation in billion-scale multiplex networks is challenging due to oversmoothing (Cai and Wang 2020). Additionally, a single GNN with $h$ layers aggregates information from $h$ -hop neighbors, potentially mixing data from different layers and causing inaccuracies. To address this, we propose the Propagation Mixture of Experts (PMoE), which employs multiple GNN models with varying layer depths to effectively capture propagation dynamics. Nodes are routed to the most suitable expert based on their characteristics and desired propagation depth, enabling the model to focus on relevant information while reducing noise, ensuring accurate and efficient predictions in large-scale networks.\n\nOur PMoE framework captures the propagation process given a seed set $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ and a multiplex graph $\\mathcal { G }$ . In this framework, we define a set of $C$ ‚Äùexpert networks,‚Äù denoted as $e _ { 1 } , e _ { 2 } , \\ldots , e _ { C }$ . Each expert $e _ { i }$ is implemented as a GNN with varying layer depths, outputting $e _ { i } ( { \\pmb x } , { \\mathcal G } , { \\xi } _ { i } ) \\in [ 0 , 1 ] ^ { 1 \\times | V | }$ , a vector representing the estimated infection probability for each node in $\\mathcal { G }$ , where $\\xi _ { i }$ is the parameter of the $i$ -th expert. To effectively leverage the diverse knowledge of experts, we employ a routing network $R$ , which outputs a probability distribution over experts $R ( { \\pmb x } ) \\in \\mathbb { R } ^ { 1 \\times C }$ based on the input seed set $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ . Each element in this distribution corresponds to the relevance probability of a particular expert for the given input. Inspired by the noisy top- $\\mathbf { \\nabla } \\cdot m _ { \\mathbf { \\nabla } }$ routing mechanism proposed by (Shazeer et al. 2017), we select the $m$ most relevant experts for each input. This mechanism operates as follows:\n\n![](images/f3b6f36161ce7fe7ee5c018df86e8efbb46c01c37533603db1b9bd79268ad43c.jpg)  \nFigure 3: Difference in influence spread (y-axis) of REM output on different dataset and budget when increasing exploration steps( $\\mathbf { \\dot { X } }$ axis). Fig. 3a - 3d and Fig. 3e - 3h are evaluated under the IC and LT model, respectively.\n\n$$\nQ \\left( \\pmb { x } \\right) = \\pmb { x } \\xi _ { g } + \\epsilon \\cdot \\mathrm { S o f t p l u s } \\left( \\pmb { x } \\xi _ { n } \\right) ,\n$$\n\n$$\nR \\left( { \\pmb x } \\right) = \\mathrm { S o f t m a x } \\left( \\mathrm { T o p M } \\left( Q \\left( { \\pmb x } \\right) , m \\right) \\right) ,\n$$\n\nIn this equation, $\\epsilon \\sim \\mathcal { N } ( 0 , 1 )$ represents standard Gaussian noise. The parameters $\\xi _ { g }$ and $\\xi _ { n }$ are learnable weights that control the contributions of the clean and noisy scores, respectively. The expected value $\\mathcal { M } ( \\pmb { x } , \\pmb { \\mathcal { G } } ; \\pmb { \\xi } )$ , where $\\xi =$ $[ \\xi _ { g } , \\xi _ { n } , \\xi _ { 1 } , . . . , \\xi _ { C } ]$ represents the parameters of the PMoE model $\\mathcal { M }$ , is calculated based on the outputs of all experts and can be formulated as follows:\n\n$$\n\\mathcal { M } \\left( \\pmb { x } , \\mathcal { G } ; \\xi \\right) = \\sum _ { i = 1 } ^ { C } R _ { i } ( \\pmb { x } ) e _ { i } ( \\pmb { x } , \\mathcal { G } ; \\xi _ { i } )\n$$\n\nHere, $R _ { i } ( { \\pmb x } )$ is the $i$ -th element of routing network $R ( { \\pmb x } )$ , representing the relevance probability of the $i$ -th expert in predicting the influence of seed set $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ . In this scenario, the total number of infected nodes, denoted as $\\hat { y } \\in \\mathbb { R } _ { + }$ , is calculated as $\\hat { y } = \\mathcal { P } ( \\pmb { x } , \\mathcal { G } ; \\boldsymbol { \\xi } ) = g ( \\mathcal { M } \\left( \\pmb { x } , \\mathcal { G } ; \\boldsymbol { \\xi } \\right) ; \\boldsymbol { \\zeta } )$ . Here, $g ( \\cdot )$ is a normalization function (e.g., $l - 1 \\mathrm { n o r m } )$ and $\\zeta$ is the threshold to transform the probability into discrete value.\n\nLemma 1 (Monotonicity of PMoE Models). Assuming the PMoE model has been trained to convergence and during\n\n# the inference phase, noisy scores $\\xi _ { n }$ are not considered, for any GNN-based, $\\mathcal { P }$ is infection monotonic if the aggregation function and combine function in GNN are non-decreasing. (Proof in Appendix C1)\n\nAccording to Lemma 1 the PMoE model $\\mathcal { P } ( \\pmb { x } , \\pmb { \\mathcal { G } } ; \\xi )$ has the theoretical guarantee to retain monotonicity, and the objective of learning the PMoE model $\\mathcal { P } ( \\pmb { x } , \\pmb { \\mathcal { G } } ; \\xi )$ is given as maximizing the following probability with a constraint:\n\n$$\n\\begin{array} { r } { \\operatorname* { m a x } _ { \\xi } \\mathbb { E } \\left[ p _ { \\xi } ( y | \\pmb { x } , \\mathcal { G } ) \\right] , } \\end{array}\n$$\n\n# Latent Seed Set Exploration\n\nAs a generative model, Seed2Vec can only produce quality seed sets if the original training data is feature-rich. If the data is biased toward dominant features or lacks diversity, Seed2Vec may miss important but less prevalent features. As the multiplex becomes more complex and the number of nodes increases, the model tends to favor dominant seed nodes in the dataset, often overlooking less frequent but potentially significant ones. REM overcomes this by treating Seed2Vec as an RL agent, actively exploring novel and potentially impactful seed sets that maximize propagation to retrain and reinforce itself by the following lemma:\n\nLemma 2 (Latent Entropy Maximization Equivalence). Assume Seed2Vec model convergence, we have arg $\\operatorname* { m a x } _ { z }$ $\\mathcal { H } ( \\mathcal { D } _ { \\phi } ( z ) )$ ‚àù $\\arg \\operatorname* { m a x } _ { \\mathbf { x } } \\mathcal { H } ( \\mathbf { x } )$ . (Proof in Appendix $c 2$ )\n\nAccording to Lemma 2, exploration within the latent space $z$ , aimed at identifying the novel seed set $\\boldsymbol { \\mathcal { S } } _ { t }$ , where $t = 1 , 2 , 3 , \\ldots$ represents the training episode, is proportional to exploration within the discrete space $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ . This correlation emerges because a well-trained Seed2Vec model, using the original collected seed set $X _ { 0 }$ , ensures both continuity‚Äîwhere nearby points in the latent space decode into similar content‚Äîand completeness, meaning that any point sampled from the latent space‚Äôs chosen distribution generates ‚Äômeaningful‚Äô content. At this juncture, $p _ { \\phi } ( z \\mid x ) \\stackrel { \\textstyle \\cdot } { \\approx } q _ { \\psi } ( z \\mid$ $^ { \\mathbf { \\alpha } } ( \\mathbf { \\alpha } _ { \\mathbf { { x } } } )$ , with $q _ { \\psi } ( z \\mid x )$ converging to a Gaussian distribution ${ \\mathcal { N } } ( \\mu , \\sigma ^ { 2 } )$ as indicated by the second term of Equation 6. Typically, an RL agent could explore various latent features by sampling $z \\sim \\mathcal { \\bar { N } } ( \\mu , \\sigma ^ { 2 } )$ and reconstructing the seed set $\\hat { \\pmb x }$ using the Decoder (i.e., $\\hat { \\pmb { x } } = \\mathcal { D } _ { \\phi } ( \\pmb { z } ) )$ . However, since $q _ { \\psi } ( z \\mid x )$ converges to a continuous function that has a derivative with respect to $z$ . Instead of the RL agent exploring by random sampling, we use Gradient Descent directly on $z$ to minimize the following objective function:\n\n$$\n\\mathcal { L } ^ { \\mathrm { E x p l o r e } } ( z ) = \\mathbb { E } \\left( c \\cdot \\mathcal { H } ( \\mathcal { D } _ { \\phi } ( z ) ) + \\exp ( - \\mathcal { P } ( \\mathcal { D } _ { \\phi } ( z ) ) ) \\right.\n$$\n\nwhere $c$ are coefficients. The term $\\begin{array} { r l } { \\mathcal { H } ( \\mathcal { D } _ { \\phi } ( z ) ) } & { { } = } \\end{array}$ $\\begin{array} { r } { - \\sum _ { i = 1 } ^ { | \\hat { \\pmb x } | } p ( \\hat { \\pmb x } _ { i } ) \\log p ( \\hat { \\pmb x } _ { i } ) } \\end{array}$ denotes the entropy of the latent variable, which promotes exploration within new regions of the latent space. The function $\\mathcal { P } ( \\mathcal { D } _ { \\phi } ( z ) )$ refers to the Propagation Mixture of Experts (PMoE), which is detailed in the following section, and is used for predicting the reconstructed seed set $\\hat { \\pmb x } = \\mathcal { D } _ { \\phi } ( { \\pmb z } )$ . To align with the objective of minimizing the loss function, we employ the exponential function, $\\exp ( \\cdot )$ , to reduce the impact of $\\mathcal { P } ( \\mathcal { D } _ { \\phi } ( z ) )$ as its value increases. With the novel synthetic seed set $S _ { t }$ (store by using Priority Replay Memory (Schaul et al. 2015)) obtained by optimizing Equation 11, we sampling top $k$ best samples and combine them with the original dataset $X _ { 0 }$ to create a Combined Dataset $X _ { t } = S _ { t } ^ { ( < k ) } \\cup X _ { 0 }$ . Therefore, as training episode $t$ progresses, we can use $\\boldsymbol { X } _ { t }$ to retrain the Seed2Vec model $\\mathcal { F } _ { \\theta }$ . This approach allows $\\mathcal { F } _ { \\theta }$ to generate improved seed sets in future iterations.\n\nEnd-to-end Learning Objective. Finally, to bridge representation learning, latent seed set exploration, and diffusion model training, we minimize the following end-to-end objective function, which combines Eq. (6), (11), and (10):\n\n$$\n\\mathcal { L } _ { \\mathrm { t r a i n } } = \\mathbb { E } \\left[ \\mathcal { L } ^ { E L B O } ( \\theta ) + \\mathcal { L } ^ { P M o E } ( \\xi ) + \\mathcal { L } ^ { E x p l o r e } ( z ) \\right]\n$$\n\nwhere $\\mathcal { L } ^ { P M o E } = \\left( \\hat { y } - y \\right) ^ { 2 }$ .\n\nSeed Node Set Inference. Finally, our method conclude with inferencing the seed node set from the continuous latent space. Specifically, gradient ascent is employed to find the latent representation $\\tilde { z }$ that maximizes the predicted influence spread, based on the estimation provided by the PMoE model. Representation $\\tilde { z }$ is decoded using the decoder network of Seed2Vec to obtain the optimal seed node set $\\tilde { \\pmb { x } }$ .\n\nTheorem 3 (Influence Estimation Consistency). Given two distinct seed sets $\\mathbf { \\boldsymbol { x } } ^ { ( i ) }$ and $\\pmb { x } ^ { ( j ) }$ , with their corresponding latent representations $\\boldsymbol { z } ^ { ( i ) }$ and $\\boldsymbol { z } ^ { ( j ) }$ encoded by a Seed2Vec. If the reconstruction error is minimized during the training and $\\mathcal { P } ( p _ { \\phi } ( z ^ { ( i ) } ) , \\mathcal { G } ; \\xi ) > \\mathcal { P } ( p _ { \\phi } ( z ^ { ( j ) } ) , \\mathcal { G } ; \\xi )$ , then it follows that $\\mathcal { P } ( \\pmb { x } ^ { ( i ) } , \\mathcal { G } ; \\xi ) > \\mathcal { P } ( \\pmb { x } ^ { ( j ) } , \\mathcal { G } ; \\xi )$ . (Proof in Appendix $^ { c 3 }$ )\n\nAccording to Theorem 3 , the optimal seed set that maximizes influence can be found by optimizing $z$ .\n\n# Experiment\n\nWe conduct experiments to compare our proposed REM framework to 6 other state-of-the-art frameworks across 5 real world networks in various settings.\n\n# Experiment Setup\n\nOur main objective is to evaluate the effect of influence spread across different scenarios in Influence Maximization (IM). Our experiments focus on two dominant propagation models within IM: the Linear Threshold (LT) and Independent Cascade (IC) models. To delve deeper into our experimental setup, we refer to Appendix D.\n\nDataset. Our experiments leverage multiple multiplex network datasets of diverse interaction types and systems. The Arabidopsis Multiplex Network from BioGRID (Stark et al. 2006) details genetic and protein interactions for Arabidopsis thaliana, comprising 7 layers, 6,980 nodes, and 18,654 edges. The NYClimateMarch2014 Twitter Network (Omodei, De Domenico, and Arenas 2015) captures tweets during the People‚Äôs Climate March, featuring 3 layers, 102,439 nodes, and 353,495 edges. The ParisAttack2015 Twitter Network (De Domenico and Altmann 2020) includes social interactions during the 2015 Paris Attacks, with 3 layers, 1,896,221 nodes, and 4,163,947 edges. We also use the Cora dataset (McCallum et al. 2000), a citation network of 2,708 publications and 7,981 edges, to analyze influence in academic.\n\n# Comparison to other Methods\n\nWe assess the performance of REM by comparing it against two categories of influence maximization techniques. 1) Traditional methods: ISF (Influential Seed Finder) (Kuhnle et al. 2018) is a greedy algorithm designed for multiplex influence maximization; KSN (Knapsack Seeding of Networks) (Kuhnle et al. 2018) utilizes a knapsack approach to find the best seed users in a multiplex network. 2) Deep learning methods: ToupleGDD (Chen et al. 2022), GCOMB (Manchanda et al. 2020b), DeepIM (Ling et al. 2023) are stateof-the-art single network influence maximization solutions. For multiplex network, the MIM-Reasoner (Do et al. 2024) method utilize probabilistic graphical models to capture the dynamics within the multiplex, then determine the best seed sets with a reinforcement learning solution. We also evaluate the performance of 2 different REM variants to demonstrate the effectiveness of our approach. One approach is REMNonRL, which does not employ the exploration of seed sets and solely relies on an initial dataset to provide solution. This variant provides observation on the effectiveness of our proposed reinforcement learning set up. The other variant, REM-NonMixture, forego our Mixture of expert set up, capture the complicated multiplex propagation with one GNN model. This variant will underscore the advantages of our more complex configurations. The comparison is based on three metrics: total influence spread (activated nodes) and inference time (wall-clock time, in seconds).\n\n# Quantitative Analysis\n\nWe evaluate the performance of the REM method against other IM strategies by comparing their ability to optimize influence across various datasets. In each case, models identify seed nodes representing $1 \\%$ , $5 \\%$ , $10 \\%$ , and $20 \\%$ of all nodes. We simulate the diffusion process until completion and determine the average influence spread across 100 iterations. We report the final number infected nodes.\n\nTable 1: Performance comparison under IC diffusion pattern. ‚àí indicates out-of-memory error. (Best is highlighted with bold.)   \n\n<html><body><table><tr><td></td><td colspan=\"4\">Cora-ML</td><td colspan=\"4\">Arabidopsis</td><td colspan=\"4\">NYClimateMarch2014</td><td colspan=\"4\">ParisAttack2015</td></tr><tr><td>Methods</td><td>1%</td><td>5% 10%</td><td>20%</td><td>1%</td><td>5%</td><td>10%</td><td>20%</td><td>1%</td><td>5%</td><td>10%</td><td>20%</td><td>1%</td><td></td><td></td><td>10%</td><td>20%</td></tr><tr><td>ISF</td><td></td><td>398.34 778.62 979.87 1368.56|2415.04 3140.58 3871.104694.16</td><td></td><td></td><td></td><td></td><td></td><td>--</td><td>--</td><td>-</td><td>- -</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>KSN</td><td></td><td>398.31778.62 979.10 1366.032282.72 2941.54 3621.26 4641.34</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>-</td><td></td><td>-</td><td></td><td>-</td><td>1</td><td>1</td></tr><tr><td>GCOMB</td><td>347.11766.02</td><td>976.14</td><td></td><td>1251.52|2315.44</td><td></td><td></td><td></td><td>3097.97 3622.08 4547.63|2093.32</td><td></td><td></td><td></td><td>7228.02 11780.29 16933.89|114672.50 180977.09 356187.81 587891.16</td><td></td><td></td><td></td><td></td></tr><tr><td>ToupleGDD</td><td>349.01 721.42</td><td>862.42</td><td></td><td>1132.66|2044.67</td><td>2856.37 3487.41 4483.27</td><td></td><td></td><td>1821.43</td><td>6714.98 10231.81</td><td></td><td>17822.21</td><td></td><td></td><td></td><td></td><td>102872.11171992.43335298.85563387.05</td></tr><tr><td>DeepIM</td><td></td><td>311.52606.82 826.41</td><td>1179.45</td><td>1993.37</td><td>2397.893328.334073.50</td><td></td><td></td><td>1893.02</td><td>6409.32</td><td>8064.77</td><td>14269.49</td><td>83972.39</td><td></td><td></td><td>149237.59281298.85480393.42</td><td></td></tr><tr><td>MIM-Reasoner</td><td></td><td>398.22 778.02 978.95 1363.352396.95</td><td></td><td></td><td>2989.15 3729.39 4621.542101.86 7387.91</td><td></td><td></td><td></td><td></td><td></td><td></td><td>11984.55 21062.87129650.48 217291.02 379932.17 608192.57</td><td></td><td></td><td></td><td></td></tr><tr><td>REM-NonRL</td><td></td><td>321.24732.84880.511151.68|2076.31 2421.133399.344120.51</td><td></td><td></td><td></td><td></td><td></td><td>2057.82</td><td>6835.44</td><td>8564.63</td><td>16021.63</td><td>87109.27</td><td></td><td></td><td></td><td>152621.23 298923.41 515237.59</td></tr><tr><td>REM-NonMixture343.16736.42921.241301.552317.392982.673712.354611.27</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>2215.82 7124.29 10034.71</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>18932.40|125098.55 207621.48 380274.03 605875.67</td></tr><tr><td>REM</td><td></td><td>347.34765.48965.04404.142430843181.263964.45401.752162.37465.9612834452314283147193.9629769.760237.76.59</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>\n\nTable 2: Performance comparison under LT diffusion pattern. ‚àí indicates out-of-memory error. (Best is highlighted with bold.)   \n\n<html><body><table><tr><td></td><td colspan=\"4\">Cora-ML</td><td colspan=\"4\">Arabidopsis</td><td colspan=\"4\">NYClimateMarch2014</td><td colspan=\"4\">ParisAttack2015</td></tr><tr><td>Methods</td><td>1%</td><td>5%</td><td>10%</td><td>20%</td><td>1%</td><td>5%</td><td>10%</td><td>20%</td><td>1%</td><td>5%</td><td>10%</td><td>20%</td><td>1%</td><td>5%</td><td>10%</td><td>20%</td></tr><tr><td>ISF</td><td>381.0</td><td>907.0</td><td>1392.0</td><td>2145.0</td><td>2901.0</td><td>4571.0</td><td>5686.0</td><td>6855.0</td><td></td><td></td><td>-</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>KSN</td><td>381.0</td><td>907.0</td><td></td><td>1392.02145.02440.0</td><td></td><td>3969.0</td><td>4934.0</td><td>6147.0</td><td></td><td></td><td></td><td>--</td><td></td><td></td><td></td><td></td></tr><tr><td>GCOMB</td><td>379.0</td><td>1005.0</td><td>1297.0</td><td>2026.0</td><td>2501.0</td><td>4247.0</td><td>4993.0</td><td>6286.0</td><td>5281.0</td><td>29974.0</td><td>63187.0</td><td>74865.0</td><td>429978.0</td><td>498301.0</td><td>571193.0</td><td>729745.0</td></tr><tr><td>ToupleGDD</td><td>375.0</td><td>998.0</td><td>1192.0</td><td>1989.0</td><td>2479.0</td><td>4147.0</td><td>4725.0</td><td></td><td>5841.04866.0</td><td>26987.0</td><td>61827.0</td><td>71865.0</td><td>420086.0</td><td>487925.0</td><td>568872.0739712.0</td><td></td></tr><tr><td>DeepIM</td><td>291.0</td><td>708.0</td><td>1023.0</td><td>1881.0</td><td>1772.0</td><td>3232.0</td><td>3211.0</td><td>3968.03582.0</td><td></td><td>21246.0</td><td>51721.0</td><td>57977.0</td><td>387129.0</td><td>460273.0</td><td>538160.0578724.0</td><td></td></tr><tr><td>MIM-Reasoner</td><td>381.0</td><td>907.0</td><td></td><td>1392.02145.0</td><td>2554.0</td><td>4133.0 5027.06191.0</td><td></td><td></td><td></td><td>6872.035833.0</td><td>77925.0</td><td>96239.0</td><td></td><td>487871.0 550864.0</td><td>629830.0 786819.0</td><td></td></tr><tr><td>REM-NonRL</td><td>321.0</td><td>811.0</td><td>1101.0</td><td>1964.0</td><td>2176.0</td><td>3548.0</td><td>4185.0</td><td>5742.0</td><td>3976.0</td><td>26920.0</td><td>55315.0</td><td>59764.0</td><td>408149.0</td><td>468621.0</td><td>540824.0 581872.0</td><td></td></tr><tr><td>REM-NonMixture</td><td>361.0</td><td>1173.2</td><td>1212.0</td><td>2102.0</td><td>2632.0</td><td>4586.0</td><td>5089.0</td><td>6197.05352.0</td><td></td><td>30187.0</td><td>66872.0</td><td>77359.0</td><td>449862.0</td><td></td><td>529562.0592055.0692974.0</td><td></td></tr><tr><td>REM</td><td>376.0</td><td>883.0</td><td></td><td>1281.02141.02943.0</td><td></td><td>4894.0</td><td>5705.0</td><td>6890.0</td><td>7111.0</td><td>37417.0 81255.0</td><td></td><td></td><td>98976.0503994.0</td><td>604890.0</td><td>651100.0 804469.0</td><td></td></tr></table></body></html>\n\nIM under IC Model. The methods are evaluated on five datasets under the IC diffusion model with budgets of $1 \\%$ , $5 \\%$ , $10 \\%$ , and $20 \\%$ of network nodes. As shown in Table 1, REM consistently outperforms other methods, particularly on large datasets such as NYClimateMarch2014 and ParisAttack2015. Traditional methods (ISF, KSN) perform well on smaller graphs but struggle to scale with larger graphs and higher budgets. Single-graph learning methods (GCOMB, TOUPLEGDD, DEEPIM) fall behind due to their inability to adapt to multiplex networks. While MIM-Reasoner achieves strong results on larger multiplex networks, it is outperformed by REM. Notably, REM variants (REM-NonRL, REM-NonMixture) show significant performance drops, underscoring the importance of REM‚Äôs key components.\n\nIM under LT Model. We evaluate the methods under the LT diffusion model, with the results in Table 2 showing that REM consistently outperforms other techniques in maximizing node infections. REM‚Äôs superiority is particularly evident on large networks and with a $2 0 \\%$ seed set, achieving $1 0 \\%$ and $1 5 \\%$ higher influence spread than the best competing methods on the NYClimateMarch2014 and ParisAttack2015 datasets, respectively. This performance highlights REM‚Äôs superior generalization across diffusion models.\n\nIM with explore step number. We compare the effectiveness of increasing exploration steps under the IC and LT models within a budget constraint. As shown in Figure 3, more exploration steps generally improve results across networks, especially for larger datasets. For smaller datasets like Cora-ML, the performance difference is minimal, while for larger datasets, the gap widens significantly with more steps.\n\nTable 3: Average inference runtime (in seconds) as the number of nodes increases. We select $1 0 \\%$ of nodes as seeds.   \n\n<html><body><table><tr><td>Methods</td><td>10,000</td><td>20,000</td><td>30,000</td><td>50,000</td></tr><tr><td>GCCOMB</td><td>17.894s</td><td>30.831s</td><td>46.275s</td><td>73.983s</td></tr><tr><td>ToupleGDD</td><td>15.873s</td><td>25.321s</td><td>37.882s</td><td>58.985s</td></tr><tr><td>MIM-Reasoner</td><td>7.948s</td><td>12.532s</td><td>26.575s</td><td>36.437s</td></tr><tr><td>DeepIM</td><td>10.321s</td><td>19.325s</td><td>32.185s</td><td>44.871s</td></tr><tr><td>REM</td><td>8.873s</td><td>10.198s</td><td>23.404s</td><td>33.482s</td></tr></table></body></html>\n\n# Scalability Analysis\n\nWe investigate the runtime of seed sets when increasing of graph size of REM verse other learning-based IM solutions. As can be seen in Table 3, REM demonstrates near-linear growth of runtime as the graph size increases. In addition, it achieves a generally shorter inference time (on average, it has a $10 \\%$ faster inference time than the second-fastest MIM-Reasoner and a $20 \\%$ faster inference time than the third-fastest DeepIM.\n\n# Conclusion\n\nThis paper has introduced REM, a framework designed to tackle the challenges of MIM. Through the integration of Mixture of Experts and a RL-based latent space exploration strategy, REM offers a robust solution to optimize influence spread across multiplex networks. Our approach demonstrates superior scalability and efficiency while also excels in handling the diversity of propagation mechanisms in these networks. The experimental results on real-world datasets validate REM‚Äôs effectiveness, showcasing its ability to outperform existing state-of-the-art methods in both influence spread and computational efficiency.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢òÊòØ**Â§öË∑ØÂΩ±ÂìçÂäõÊúÄÂ§ßÂåñÔºàMultiplex Influence Maximization, MIMÔºâ**ÔºåÂç≥Âú®Â§öË∑ØÁ§æ‰∫§ÁΩëÁªúÔºàMultiplex NetworksÔºâ‰∏≠ËØÜÂà´ÊúÄÂÖ∑ÂΩ±ÂìçÂäõÁöÑÁßçÂ≠êÁî®Êà∑ÈõÜÔºå‰ª•ÊúÄÂ§ßÂåñ‰ø°ÊÅØ‰º†Êí≠ËåÉÂõ¥„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºö‰º†ÁªüÁªÑÂêà‰ºòÂåñÊñπÊ≥ïÔºàÂ¶ÇË¥™ÂøÉÁÆóÊ≥ïÔºâÈöæ‰ª•Ê≥õÂåñÂà∞Êú™Áü•ÁΩëÁªúÁªìÊûÑÔºå‰∏îÊó†Ê≥ïÈ´òÊïàÂ§ÑÁêÜÂ§ßËßÑÊ®°ÁΩëÁªúÔºõËÄåÁé∞ÊúâÂ≠¶‰π†ÂûãÊñπÊ≥ïÔºàÂ¶ÇÂü∫‰∫éGNNÊàñRLÁöÑÊ®°ÂûãÔºâÂ≠òÂú®ËÆ≠ÁªÉÊïàÁéá‰Ωé„ÄÅ‰º†Êí≠‰º∞ËÆ°‰∏çÂáÜÁ°ÆÁ≠âÈóÆÈ¢ò„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫**Âº∫Âåñ‰∏ìÂÆ∂ÊúÄÂ§ßÂåñÊ°ÜÊû∂ÔºàReinforced Expert Maximization, REMÔºâ**ÔºåÈÄöËøá**Seed2VecÔºàVAEÁºñÁ†ÅÁßçÂ≠êÈõÜÔºâ**„ÄÅ**‰º†Êí≠‰∏ìÂÆ∂Ê∑∑ÂêàÔºàPMoEÔºâ**Âíå**Âº∫ÂåñÂ≠¶‰π†È©±Âä®ÁöÑÊΩúÂú®Á©∫Èó¥Êé¢Á¥¢**ÔºåÂÆûÁé∞È´òÊïà‰∏îÁ≤æÂáÜÁöÑÁßçÂ≠êÈõÜ‰ºòÂåñ„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Seed2Vec**ÔºöÂ∞ÜÁ¶ªÊï£ÁßçÂ≠êÈõÜÊò†Â∞ÑÂà∞ËøûÁª≠ÊΩúÂú®Á©∫Èó¥Ôºå‰ºòÂåñÊïàÁéáÊèêÂçáÁ∫¶**30%**ÔºàÂØπÊØî‰º†ÁªüRLÊñπÊ≥ïÔºâ„ÄÇ\\n> *   **PMoE**ÔºöÂ§ö‰∏ìÂÆ∂GNNÊ®°ÂûãÔºåÊòæËëóÊèêÂçá‰º†Êí≠‰º∞ËÆ°Á≤æÂ∫¶„ÄÇÂú®ICÊ®°Âûã‰∏ãÔºåÂΩ±ÂìçÂäõ‰º†Êí≠ÊØîÊúÄ‰Ω≥Âü∫Á∫øÊèêÂçá`15%`„ÄÇ\\n> *   **RLÊé¢Á¥¢Á≠ñÁï•**ÔºöÂä®ÊÄÅÁîüÊàêÈ´òË¥®ÈáèÁßçÂ≠êÈõÜÔºåÂáèÂ∞ëÂØπÂàùÂßãÊï∞ÊçÆË¥®ÈáèÁöÑ‰æùËµñ„ÄÇÂú®LTÊ®°Âûã‰∏ãÔºåÂΩ±ÂìçÂäõ‰º†Êí≠ÊØîÈùûRLÁâàÊú¨ÊèêÂçá`10%`„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   REMÁöÑÊ†∏ÂøÉÂéüÁêÜÊòØÂ∞ÜÁßçÂ≠êÈõÜ‰ºòÂåñÈóÆÈ¢òËΩ¨Âåñ‰∏∫**ËøûÁª≠ÊΩúÂú®Á©∫Èó¥ÁöÑÊé¢Á¥¢ÈóÆÈ¢ò**ÔºåÂà©Áî®VAEÁöÑÁîüÊàêËÉΩÂäõÂíåRLÁöÑÊé¢Á¥¢ËÉΩÂäõÔºåÁªìÂêàÂ§ö‰∏ìÂÆ∂GNNÁöÑÁ≤æÂáÜÈ¢ÑÊµãÔºåÂÆûÁé∞Á´ØÂà∞Á´Ø‰ºòÂåñ„ÄÇ\\n> *   ËÆæËÆ°Âì≤Â≠¶ÔºöÈÄöËøá**Ëß£ËÄ¶Ë°®Á§∫Â≠¶‰π†ÔºàSeed2VecÔºâ„ÄÅ‰º†Êí≠Âª∫Ê®°ÔºàPMoEÔºâÂíåÊé¢Á¥¢Á≠ñÁï•ÔºàRLÔºâ**ÔºåÂàÜÂà´Ëß£ÂÜ≥Êï∞ÊçÆ‰æùËµñ„ÄÅ‰º†Êí≠‰º∞ËÆ°‰∏çÂáÜÂíåÊé¢Á¥¢ÊïàÁéá‰ΩéÁöÑÈóÆÈ¢ò„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØî**Ôºö‰º†ÁªüÊñπÊ≥ïÔºàÂ¶ÇISFÔºâ‰æùËµñË¥™ÂøÉÁÆóÊ≥ïÔºåËÆ°ÁÆóÊàêÊú¨È´òÔºõÂ≠¶‰π†ÂûãÊñπÊ≥ïÔºàÂ¶ÇDeepIMÔºâÊó†Ê≥ïÂ§ÑÁêÜÂ§öË∑ØÁΩëÁªúÂä®ÊÄÅ‰º†Êí≠„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõ**Ôºö\\n>     1.  Seed2VecÈÄöËøáVAEÂ∞ÜÁßçÂ≠êÈõÜÁºñÁ†Å‰∏∫È´òÊñØÂàÜÂ∏ÉÔºåÊîØÊåÅÊ¢ØÂ∫¶‰ºòÂåñÔºõ\\n>     2.  PMoEÈÄöËøáË∑ØÁî±Êú∫Âà∂ÂàÜÈÖç‰∏çÂêåGNN‰∏ìÂÆ∂Â§ÑÁêÜ‰∏çÂêå‰º†Êí≠Ê∑±Â∫¶ÔºåÈÅøÂÖçËøáÂπ≥ÊªëÔºõ\\n>     3.  RLÊé¢Á¥¢Áõ¥Êé•‰ºòÂåñÊΩúÂú®Á©∫Èó¥ÁÜµÔºåÁîüÊàêÂ§öÊ†∑ÊÄßÁßçÂ≠êÈõÜ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **Seed2VecËÆ≠ÁªÉ**ÔºöÁî®VAEÁºñÁ†ÅÁßçÂ≠êÈõÜ$\\mathbf{x}$‰∏∫ÊΩúÂú®ÂèòÈáè$z$ÔºåÊúÄÂ∞èÂåñELBOÊçüÂ§±ÔºàÂÖ¨Âºè6Ôºâ„ÄÇ\\n> 2.  **PMoEÊûÑÂª∫**ÔºöÂÆö‰πâ$C$‰∏™GNN‰∏ìÂÆ∂$e_i$ÔºåË∑ØÁî±ÁΩëÁªú$R$ÈÄâÊã©Top-$m$‰∏ìÂÆ∂Âä†ÊùÉËæìÂá∫ÔºàÂÖ¨Âºè9-10Ôºâ„ÄÇ\\n> 3.  **RLÊé¢Á¥¢**ÔºöÂú®ÊΩúÂú®Á©∫Èó¥$z$‰∏≠Ê¢ØÂ∫¶‰∏ãÈôç‰ºòÂåñÁõÆÊ†áÂáΩÊï∞ÔºàÂÖ¨Âºè11ÔºâÔºåÁîüÊàêÊñ∞ÁßçÂ≠êÈõÜÂπ∂Â≠òÂÇ®Ëá≥‰ºòÂÖàÁ∫ßÂõûÊîæÂÜÖÂ≠ò„ÄÇ\\n> 4.  **Á´ØÂà∞Á´ØËÆ≠ÁªÉ**ÔºöËÅîÂêà‰ºòÂåñVAE„ÄÅPMoEÂíåRLÊçüÂ§±ÔºàÂÖ¨Âºè12Ôºâ„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ‰º†ÁªüÊñπÊ≥ïÔºöISF„ÄÅKSNÔºõÂ≠¶‰π†ÂûãÊñπÊ≥ïÔºöGCOMB„ÄÅToupleGDD„ÄÅDeepIM„ÄÅMIM-Reasoner„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®ÂΩ±ÂìçÂäõ‰º†Êí≠ÔºàICÊ®°ÂûãÔºâ‰∏ä**ÔºöREMÂú®ParisAttack2015Êï∞ÊçÆÈõÜÔºà20%ÁßçÂ≠êÂç†ÊØîÔºâËææÂà∞**651,100**ÊÑüÊüìËäÇÁÇπÔºåÊòæËëó‰ºò‰∫éMIM-ReasonerÔºà608,192ÔºâÂíåDeepIMÔºà480,393Ôºâ„ÄÇ‰∏éÊúÄ‰Ω≥Âü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá`7.1%`„ÄÇ\\n> *   **Âú®Êé®ÁêÜÈÄüÂ∫¶‰∏ä**ÔºöREMÂ§ÑÁêÜ50,000ËäÇÁÇπÁΩëÁªúÁöÑÊé®ÁêÜÊó∂Èó¥‰∏∫**33.482Áßí**ÔºåÊØîMIM-ReasonerÔºà36.437ÁßíÔºâÂø´`8%`Ôºå‰∏îÊé•ËøëËΩªÈáèÁ∫ßÊ®°ÂûãToupleGDDÔºà58.985ÁßíÔºâÁöÑÈÄüÂ∫¶„ÄÇ\\n> *   **Âú®Ê≥õÂåñÊÄßÔºàLTÊ®°ÂûãÔºâ‰∏ä**ÔºöREMÂú®NYClimateMarch2014Êï∞ÊçÆÈõÜÔºà10%ÁßçÂ≠êÂç†ÊØîÔºâËææÂà∞**81,255**ÊÑüÊüìËäÇÁÇπÔºåÊØîÈùûRLÁâàÊú¨Ôºà55,315ÔºâÊèêÂçá`47%`ÔºåÈ™åËØÅ‰∫ÜRLÊé¢Á¥¢ÁöÑÊúâÊïàÊÄß„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Â§öË∑ØÂΩ±ÂìçÂäõÊúÄÂ§ßÂåñ (Multiplex Influence Maximization, MIM)\\n*   ÂèòÂàÜËá™ÁºñÁ†ÅÂô® (Variational Autoencoder, VAE)\\n*   ÂõæÁ•ûÁªèÁΩëÁªú (Graph Neural Network, GNN)\\n*   ‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°Âûã (Mixture of Experts, MoE)\\n*   Âº∫ÂåñÂ≠¶‰π† (Reinforcement Learning, RL)\\n*   Á§æ‰∫§ÁΩëÁªúÂàÜÊûê (Social Network Analysis, N/A)\\n*   ‰ø°ÊÅØ‰º†Êí≠Âª∫Ê®° (Information Diffusion Modeling, N/A)\"\n}\n```"
}