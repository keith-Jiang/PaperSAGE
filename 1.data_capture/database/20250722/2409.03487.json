{
    "source": "Semantic Scholar",
    "arxiv_id": "2409.03487",
    "link": "https://arxiv.org/abs/2409.03487",
    "pdf_link": "https://arxiv.org/pdf/2409.03487.pdf",
    "title": "ScreenMark: Watermarking Arbitrary Visual Content on Screen",
    "authors": [
        "Xiujian Liang",
        "Gaozhi Liu",
        "Yichao Si",
        "Xiaoxiao Hu",
        "Zhenxing Qian",
        "Xinpeng Zhang"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2024-09-05",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Fudan University",
        "School of Computer Science"
    ],
    "paper_content": "# ScreenMark: Watermarking Arbitrary Visual Content on Screen\n\nXiujian Liang, Gaozhi Liu, Yichao Si, Xiaoxiao Hu, Zhenxing Qian\n\nFudan University, School of Computer Science ShangHai 200438, China liangxj23,gzliu22,ycsi22,xxhu23@m.fudan.edu.cn, zxqian $@$ fudan.edu.cn\n\n# Abstract\n\nDigital watermarking has shown its effectiveness in protecting multimedia content. However, existing watermarking is predominantly tailored for specific media types, rendering them less effective for the protection of content displayed on computer screens, which is often multi-modal and dynamic. Visual Screen Content (VSC), is particularly susceptible to theft and leakage through screenshots, a vulnerability that current watermarking methods fail to adequately address. To address these challenges, we propose ScreenMark, a robust and practical watermarking method designed specifically for arbitrary VSC protection. ScreenMark utilizes a threestage progressive watermarking framework. Initially, inspired by diffusion principles, we initialize the mutual transformation between regular watermark information and irregular watermark patterns. Subsequently, these patterns are integrated with screen content using a pre-multiplication alpha blending technique, supported by a pre-trained screen decoder for accurate watermark retrieval. The progressively complex distorter enhances the robustness of the watermark in real-world screenshot scenarios. Finally, the model undergoes fine-tuning guided by a joint-level distorter to ensure optimal performance. To validate the effectiveness of ScreenMark, we compiled a dataset comprising 100,000 screenshots from various devices and resolutions. Extensive experiments on different datasets confirm the superior robustness, imperceptibility, and practical applicability of the method.\n\n# Introduction\n\nWith the continuous advancement of the Internet and computer technologies, an increasing amount of information is presented in the form of Visual Screen Content (VSC), including images, videos, texts, web pages, windows, and more. In the context of users‚Äô personal computers, VSC is displayed on screens without exception. From a visual perspective, this mode of presentation is the most readily acceptable form of expression. At the same time, this also means that the VSC can easily be leaked.\n\nFor most enterprise and home computers, ensuring data security is a significant challenge. Traditional security measures, such as data encryption, firewalls, access control, and identity management, provide comprehensive protection against data leaks. However, these methods primarily manage permissions, allowing authorized users to capture screen content in real-time using screenshot tools. Current specialized protection techniques for VSC often rely on non-learning watermarking methods, which are even visible. These methods struggle to balance robustness and visual quality effectively, rendering them unsuitable for realworld applications. Therefore, this study focuses on VSC security in screenshot scenarios and aims to propose a universal learning-based screen watermarking method.\n\n![](images/89233b29695e502cdfcc71ef66f72203682ec82017a5d9c7ac8eaae473821e86.jpg)  \nFigure 1: The comparison of the traditional watermarking and the proposed ScreenMark. The red and green content represent non-watermarked and watermarked, respectively.\n\nIn recent years, the rapid advancement of multimedia watermarking technology(Liu et al. 2023; Xiao et al. 2024; Tang et al. 2024; Li, Liao, and Wu 2024) has facilitated the protection of multimedia file content. However, it is important to recognize that current watermarking techniques are primarily designed for individual modalities, offering specialized protection for images, videos, text, and other media types. As depicted in Fig.1, there are two main drawbacks: limited scope of protection and response time constraints.\n\nFig.1(a) shows that while traditional watermarking can safeguard specific media content within a single modality or frame, it falls short in providing comprehensive protection across various file types and the vast number of files present on personal computers. Moreover, these methods struggle to counteract millisecond-level capture attacks under dynamic screen conditions. In contrast, the method proposed in this paper, illustrated in Fig.1(b), does not focus on protecting a single media file or specific screen frame at a given time. Instead, it integrates the watermark with the screen through a unique fusion process, offering comprehensive and realtime protection for arbitrary VSC displayed on the screen. This approach has been named ScreenMark.\n\nIn ScreenMark, we introduce a three-stage watermarking framework utilizing progressive training(Li et al. 2022). This approach completes robustness training at various levels across different stages, resulting in a versatile screen watermarking solution for VSC sreenshot scenarios. Traditional watermarking methods often embed watermark information directly into media content via an encoder, leading to increased processing time and limited protection scope. To overcome these limitations, ScreenMark employs an irregular watermark pattern that blends more naturally and comprehensively with screen content, resembling a mask. Moreover, this irregular pattern makes it more challenging for unauthorized users to detect and remove the watermark, thereby enhancing the security of the protection mechanism. Building on this, the three-stage progressive training strategy further refines this approach. Each stage addresses specific challenges: from basic message diffusion and reversal to adaptive screen decoder training, and finally, handling composite distortion. Through progressively complex training scenarios, ScreenMark enhances system resilience in VSC capture and subsequent processing scenarios.\n\nBased on the above, the contribution of this paper can be summarized as follows:\n\n1) We introduce a novel and practical multimedia protection scenario that addresses not only single-modal media content but also multi-modal VSC displayed on computer screens. And we point out the limitation of the mainstream single-modal watermarking methods in terms of protection scope and response time in this scenario.\n\n2) To the best of our knowledge, we present the first learning-based watermarking framework specialized for VSC protection, named ScreenMark. In ScreenMark, regular watermarking information is diffused into irregular watermarking patterns and integrated with the screen display. We propose a three-stage progressive training strategy and design various levels of distorters tailored to different stage. 3) To enhance the applicability of ScreenMark to VSC protection, we have compiled a dataset of 100,000 screenshot images. These images were collected using different screenshot tools across a diverse range of devices and resolutions from SD $( 7 2 0 \\mathrm { x } 4 8 0 )$ to 4K (3840x2160).\n\n4) Extensive experiments demonstrate that ScreenMark matches or even surpasses the performance of four SOTA single-modal watermarking methods in screenshot scenarios in terms of robustness, invisibility, and applicability to real-world situations.\n\n# Related Work\n\n# Deep-learning-based Watermarking\n\nDeep-learning approaches have effectively addressed the limitations of hand-crafted features in watermarking. (Zhu et al. 2018) introduced an end-to-end solution using an autoencoder architecture, establishing a foundation in the domain. To enhance robustness against JPEG compression, MBRS(Jia, Fang, and Zhang 2021) proposed a hybrid noise layer of real and simulated JPEG with a small batch strategy. (Fernandez et al. 2022) utilized a pre-trained model to create a transform-invariant latent space for watermark embedding, achieving higher robustness against various attacks. DWSF(Guo et al. 2023) offers a practical framework for decentralized watermarking, training an auto-encoder to resist non-geometric attacks and incorporating a watermark synchronization module for geometric attacks. Moreover, some recent methods(Guan et al. 2022; Xu et al. 2022; Fang et al. 2023b) explore invertible neural networks for watermark embedding and extraction. However, these primarily protect specific media content in a single modality and are inadequate for the multi-modal VSC in real-time scenarios.\n\n# Screen-related Watermarking\n\nThe visibility of VSC to the public has sparked interest in its preservation among scholars. (Piec and Rauber 2014) utilized the Human Visual System to create dynamically adaptable watermarks, while (Du and Fan 2018) aimed to prevent screenshot data leakage through full-screen protection. These non-learning-based VSC watermarking methods struggled with balancing robustness and invisibility, making them easily detectable by attackers.\n\nDriven by the need for screen protection, screen-shooting resilient watermarking (SSRW) addresses cross-channel content leakage. (Fang et al. 2018) first modeled screen shooting distortions and proposed robust watermarking schemes based on DCT and SIFT. (Wengrowski and Dana 2019) introduced CDTF, a network simulating the screento-camera process using a multi-million dataset. (Tancik, Mildenhall, and $\\mathrm { N g } ~ 2 0 2 0 \\$ ) developed a noise layer for the printer-to-camera channel, addressing various distortions. (Jia et al. 2020) designed a 3D reconstruction-based noise layer for the camera-shot channel, achieving camera-shot robustness. Similarly, (Fang et al. 2022) created PIMoG, a noise layer for the screen-to-camera channel, enhancing screenshot robustness. To improve visual quality, (Jia et al. 2022) embedded information in sub-images and used a localization network to identify watermarked regions. To address distortion variability across different screens, (Fang et al. 2023a) introduced DeNoL, an efficient decoupling noise layer that simulates distortions accurately with fewer samples by fine-tuning transform layer.\n\nHowever, SSRW, which focuses on modeling the physical distortion of recapture, protects only specified fixed content and is limited in scope and response time, making it ineffective for screen interception scenarios in VSC. In contrast, our work aims to protect arbitrary VSCs, providing real-time protection as the screen changes and multi-modal protection within a single watermark framework.\n\n![](images/75fcdae944fec59cf689591a001e58579da08e983c447f1ec26729d577959bd4.jpg)  \nFigure 2: The overall framework of the proposed method, which contains three main stages.\n\n# Proposed Method\n\n# Motivation & Overview\n\nTo address the limitations in protection scope and response times encountered by current watermarking techniques for screen content, this paper introduces a three-stage watermarking framework specifically designed for screen content. Inspired by the diffusion model(Dhariwal and Nichol 2021), which diffuses a regular image with noise to generate an irregular image, this framework adopts a novel approach. The diffusion process can be reversed through denoising, restoring the original image. This suggests directly integrating irregular watermark patterns obtained from the diffusion of regular watermark information with the screen content, rather than embedding information into the protected multimedia carrier using an encoder, as is common in traditional watermarking frameworks. This is particularly important for screen content protection because it allows real-time and cost-effective integration of watermark patterns with screen content. It offers a protection method that is unrestricted by scope and response time, integrating securely and naturally with screen content.\n\nOur approach transforms regular watermark information into irregular watermark patterns and integrates them with screen content, eliminating the need for encoder-based information embedding. With this in mind, we designed a threestage watermarking framework, executing robustness training at different levels. The overall framework shows in Fig.2.\n\n# Stage-1: Pairwise Initialization\n\nIn Stage-1, we initialize a pair of a Message Diffuser and a Message Reverser. These modules effectively facilitate the dissemination of watermark information into watermark patterns and their subsequent reverse recovery. During this stage, we introduce an image-level distorter to enhance robustness against image-level attacks that may occur with screen captures.The framework consists of a Message Diffuser $M _ { D }$ , Message Reverser $M _ { R }$ , and an Image-level Distorter $D _ { I }$ .\n\nWorkflow Initially, we generate a batch of regular watermark messages $I _ { w }$ , with batch size $N _ { 0 }$ and length of information $L$ . Subsequently, $I _ { w }$ is subjected to diffusion processing within $M _ { D }$ , resulting in an irregular watermark pattern $P _ { w }$ . Upon acquisition of $P _ { w }$ , each pattern in batch undergoes parallel image-level distortion processing by $\\mathcal { D } _ { I } ^ { k } \\in \\dot { D _ { I } } \\mathbf { \\bar { \\Omega } }$ , where $k \\in \\{ 1 , \\bar { 2 } , \\dots , N _ { 0 } \\}$ , yielding the distorted watermark pattern $P _ { d }$ . Finally, the reverse-processed watermark information $I _ { r }$ is recovered through $M _ { R }$ .\n\nArchitecture The $M _ { D }$ is comprised of a linear layer, $N _ { 1 }$ diffusion block, and a convolution Block connected in series. It receives a regular watermark information $I _ { w }$ of size R1√ó1√óL√óN0 and outputs a regular watermark pattern Pw of size $\\mathbb { R } ^ { H \\times W \\times 3 \\times N _ { 0 } }$ . The diffusion blocks use upsampling and transposed convolution in parallel to suppress checkerboard effects and enhance feature representation without increasing network depth. Subsequently, in $D _ { I }$ , different batches of $I _ { w }$ are randomly subjected to one of three types of image-level distortions $\\dot { \\mathcal { D } } _ { I } ^ { k } \\in \\dot { \\mathcal { D } } _ { I }$ , specifically Resize distortion, Crop distortion , and Cropout distortion. These distortions are common in actual screenshot scenarios and cannot be restored by third parties without the attack parameters. Conversely, the $M _ { R }$ consists of an HRNet, $N _ { 2 }$ reversal block, a double-convolution block, and two linear layers connected in series. It receives a distorted watermark pattern $P _ { d }$ of size $\\mathbb { R } ^ { H \\times W \\times 3 \\times N _ { 0 } }$ and outputs a reversed watermark information Ir of size R1√ó1√óL√óN0 .\n\nLoss Function In this stage, the loss function serves two purposes: on one hand, control the security and stealthiness of the generated watermark patterns, and on the other hand, ensure the accuracy of the reversed information. To achieve the irregularity and invisibility of the watermark patterns, we propose four types of losses: near-zero loss, dispersion loss, variation loss, and channel loss.\n\nNear-zero loss reduce the interference of the watermark patterns on the original screen content. It minimize the mean squared error between the generated watermark pattern $P _ { d }$ and a tensor filled with zeros, signifying that the overall pixel values are close to zero , which can be formulated as:\n\n$$\nL _ { z e r o } = M S E ( P _ { d } , \\mathbf { 0 } )\n$$\n\nwhere 0 represents the zero matrix with shape as $P _ { d }$ .\n\nDispersion loss prevent concentration in specific areas for robustness against image processing attacks. It increases the dispersity of the watermark pattern by calculating the mean absolute value and variance, ensuring uniform distribution across the image, which can be formulated as:\n\n$$\nL _ { d i s p e r s i o n } = m e a n ( | P _ { d } | ) + v a r ( P _ { d } )\n$$\n\nwhere $| P _ { d } |$ , mean and var represents the absolute value, the mathematical mean and the variance of $P _ { d }$ , respectively.\n\nVariation loss reduce the visual noise introduced by the watermark. It encourages the spatial smoothness of the generated watermark pattern by minimizing drastic changes between adjacent pixels, making the watermark harder to detect by the naked eye, which can be formulated as:\n\n$$\n\\begin{array} { r } { L _ { v a r i a t i o n } = \\displaystyle \\sum _ { i , j } ^ { W , H } [ \\sqrt { ( P _ { d } [ i , j ] - P _ { d } [ i + 1 , j ] ) ^ { 2 } } + } \\\\ { \\sqrt { ( P _ { d } [ i , j ] - P _ { d } [ i , j + 1 ] ) ^ { 2 } } ] } \\end{array}\n$$\n\nwhere $\\mathbf { \\chi } _ { i }$ and $j$ respectively represent the row and column indices of pixels in the $P _ { d }$ . $W$ and $H$ represent the width and height of the $P _ { d }$ . Variation loss considers pixel variations in both the vertical and horizontal directions, reducing highfrequency noise while preserving edge information.\n\nChannel balance loss reduce noticeable color distortion and maintain the color balance. It minimizes the mean squared error of the mean value interpolation of the R, G, B color channels. This design is particularly important for color-sensitive screen content, which can be formulated as:\n\n$$\n\\begin{array} { c } { { L _ { C h a n n e l } = m e a n ( ( R _ { m } - G _ { m } ) ^ { 2 } + } } \\\\ { { ( R _ { m } - B _ { m } ) ^ { 2 } + ( G _ { m } - B _ { m } ) ^ { 2 } ) } } \\end{array}\n$$\n\nwhere $R _ { m } , G _ { m }$ and $B _ { m }$ respectively represent the mean of R,G and B channel of $P _ { d }$ , respectively.\n\nBased on the considerations above, the corresponding loss function for the pattern can be written as follows:\n\n$$\n\\begin{array} { r } { L _ { P a t t e r n } = \\lambda _ { 0 } L _ { z e r o } + \\lambda _ { 1 } L _ { d i s p e r s i o n } + } \\\\ { \\lambda _ { 2 } L _ { v a r i a t i o n } + \\lambda _ { 3 } L _ { c h a n n e l } } \\end{array}\n$$\n\nTo ensure the consistency of reversed information with the watermark, we design the loss function for the message using binary cross-entropy, which can be formulated as:\n\n$$\nL _ { M e s s a g e } = - \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } [ I _ { w _ { i } } \\log ( I _ { d _ { i } } ) +\n$$\n\nwhere $N$ is the number of samples in the batch, $I _ { d _ { i } }$ is the predicted value for the $i ^ { t h }$ sample, and $\\boldsymbol { I } _ { w _ { i } }$ is the actual value for the $i ^ { t h }$ sample. The loss function in the initialization training process of stage-1 can be formulated as:\n\n$$\nL _ { s t a g e 1 } = \\beta L _ { P a t t e r n } + \\gamma L _ { M e s s a g e }\n$$\n\nwhere $\\beta$ and $\\gamma$ each represent the importance of watermarking patterns and watermarking message in this stage.\n\n# Stage-2: Adaptive Pre-Training\n\nIn Stage-2, we involve adaptive training for the screen decoder $D _ { S }$ to accurately decode the irregular watermark patterns and reverse the message. We freeze the parameters of the model from stage 1 and input the patterns into an $A l .$ - pha blending rendering module $R _ { \\alpha }$ for integration with the computer screen. This stage also introduces a pixel-level distorter to increase robustness against pixel-level attacks.\n\nWorkflow The process of transforming watermark information into watermark patterns remains consistent with Stage-1. Building on this, at this stage, the watermark patterns $P _ { w }$ and the screen content $S _ { c }$ are input into $R _ { \\alpha }$ . Through flattening or scaling, $R _ { \\alpha }$ integrates $P _ { w }$ with $S _ { c }$ to produce the watermarked screen content $S _ { w }$ . The specific way of integration is to form a kind of mask that floats above the screen through the $\\alpha$ channel, so that the full-screen watermark effect can be realized on any resolution screen without any impact on the screen content. After obtaining $S _ { w }$ , we employ Pixel-level Distorter $\\mathcal { D } _ { P } ^ { k } \\in D _ { P }$ to process with parallel distortion, where $k \\in \\{ 1 , \\mathsf { \\bar { 2 } } , \\dots , N _ { 0 } \\}$ , resulting in the distorted watermarked screen content $S _ { d }$ . Finally, the distorted watermark pattern $P _ { d }$ is decoded from $S _ { d }$ using $D _ { S }$ .\n\nArchitecture The $R _ { \\alpha }$ utilizes Direct3D for efficient graphics processing and the Windows API for window management, incorporating pre-multiplied alpha technology to optimize transparency handling. Initially, window creation and configuration are performed using Windows‚Äô CreateWindowEx, ensuring the window stays atop others while allowing mouse events to pass through, maintaining unobtrusive user interaction. Subsequently, Direct3D is utilized for GPU-accelerated rendering, enhancing efficiency and ensuring speed during terminal information switching. Finally, Pre-multiplied alpha image processing is applied before loading, where each pixel‚Äôs color value is multiplied by its alpha value, simplifying transparency blending calculations. This computation can be described as follows:\n\n<html><body><table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Length (Bits)</td><td rowspan=\"2\">Clean</td><td colspan=\"3\">Crop(%)</td><td colspan=\"3\">Cropout(%)</td><td colspan=\"3\">Resize(%)</td><td rowspan=\"2\">Average</td></tr><tr><td>90</td><td>80</td><td>70</td><td>1 5</td><td>10</td><td>20</td><td>‰∏Ä 80</td><td>150</td><td>300</td></tr><tr><td rowspan=\"6\">ImageNet</td><td> StegaStamp</td><td>100</td><td>99.48</td><td>68.93</td><td>55.19</td><td>51.93</td><td>97.12</td><td>97.00</td><td>95.31</td><td>91.00</td><td>91.43</td><td>90.12</td><td>82.00</td></tr><tr><td>MBRS</td><td>30</td><td>97.97</td><td>79.64</td><td>75.43</td><td>72.13</td><td>97.36</td><td>97.11</td><td>93.87</td><td>93.38</td><td>93.16</td><td>92.64</td><td>88.30</td></tr><tr><td>PIMoG</td><td>30</td><td>99.38</td><td>78.83</td><td>73.14</td><td>70.16</td><td>97.56</td><td>97.31</td><td>93.12</td><td>92.21</td><td>92.17</td><td>90.25</td><td>87.19</td></tr><tr><td>DWSF</td><td>30</td><td>100.00</td><td>98.75</td><td>96.58</td><td>94.12</td><td>98.16</td><td>97.89</td><td>94.44</td><td>94.35</td><td>97.37</td><td>97.73</td><td>96.60</td></tr><tr><td>ScreenMark</td><td>100</td><td>100.00</td><td>98.71</td><td>96.75</td><td>94.17</td><td>98.09</td><td>97.38</td><td>95.65</td><td>95.30</td><td>97.98</td><td>97.25</td><td>96.80</td></tr><tr><td> StegaStamp</td><td>100</td><td>97.06</td><td>72.68</td><td>60.25</td><td>55.93</td><td>90.43</td><td>91.06</td><td>89.00</td><td>94.68</td><td>93.68</td><td>94.31</td><td>82.45</td></tr><tr><td rowspan=\"4\">ScreenImage</td><td>MBRS</td><td>30</td><td>98.12</td><td>79.99</td><td>75.80</td><td>72.46</td><td>97.79</td><td>97.54</td><td>94.22</td><td>93.73</td><td>93.51</td><td>92.99</td><td>88.67</td></tr><tr><td>PIMoG</td><td>30</td><td>99.45</td><td>79.19</td><td>73.40</td><td>70.45</td><td>97.99</td><td>97.75</td><td>93.47</td><td>92.56</td><td>92.52</td><td>90.59</td><td>87.55</td></tr><tr><td>DWSF</td><td>30</td><td>100.00</td><td>99.20</td><td>96.82</td><td>93.46</td><td>98.61</td><td>98.34</td><td>95.08</td><td>94.79</td><td>95.82</td><td>95.17</td><td>96.37</td></tr><tr><td>ScreenMark</td><td>100</td><td>100.00</td><td>99.66</td><td>98.73</td><td>95.26</td><td>99.93</td><td>99.72</td><td>96.23</td><td>99.88</td><td>99.99</td><td>99.99</td><td>98.82</td></tr></table></body></html>\n\nTable 1: Bit accuracy rate $( \\mathrm { B A R } , \\% )$ on different image-level attacks\n\n$$\nS _ { w } = \\alpha P _ { w } + ( 2 5 5 - \\alpha ) S _ { c }\n$$\n\nwhere $\\alpha$ take the value of 5, meaning the watermark pattern affects less than $2 \\%$ of screen content pixels. To optimize the image rendering, an advanced shader dynamically adjusts image transparency during rendering, leveraging pre-multiplied alpha techniques for automatic color and transparency blending. In $D _ { P }$ , batches of $S _ { w }$ are randomly selected among three types of image-level distortions, denoted as $\\mathcal { D } _ { P } ^ { k } \\ \\in \\ D _ { P }$ . These types include JPEG compression, Gaussian noise, and Gaussian blur. The $D _ { S }$ consists of three consecutive ConvBlock, two ResBlock, and an additional ConvBlock, linked in series. It receives $S _ { d }$ of size RH√óW √ó3√óN0 and outputs a Pds of the same size.\n\nLoss Function This stage facilitates the $D _ { S }$ in pretraining adaptively, building on the training foundation laid in the previous stage, to decode watermark patterns from screenshots of watermarked screens that have undergone pixellevel distortion attacks. The goal is to match the extracted pattern as closely as possible to the original pattern before distortion. With the weights from Stage-1 frozen, pretraining $^ +$ for the screen decoder is not interfered by other factors. The loss function in stage-2 is formalized as follows:\n\n$$\nL _ { s t a g e 2 } = M S E ( P _ { d s } , P _ { w } )\n$$\n\n# Stage-3: Enhancement Fine-Tuning\n\nIn Stage-3, we will synergistically fine-tune the model weights acquired from the Message Diffuser $M _ { D }$ , Message Reverser $M _ { R }$ , and Screen Decoder $D _ { S }$ from the previous two stages. Furthermore, we introduce an additional Jointlevel Distortion Layer $D _ { J }$ that encompasses both image and pixel-level distortions. This enhances the model‚Äôs robustness when integrated with screen content, effectively compensating for the limitations of the previous two stages. In order to make the model have just the right amount of robustness, the level of the distorter and its position are different. Based on the aforementioned model, we have named the complete network ScreenMark. The loss function during the enhancement fine-tuning process is as follows:\n\n$$\nL _ { s t a g e 3 } = L _ { s t a g e 1 } + L _ { s t a g e 2 }\n$$\n\n# Experiments\n\n# Experimental Settings\n\nBenchmarks. We are the first learning-based watermarking specialized for VSC protection and have no directly relevant baseline model to compare against. In order to measure our performance in terms of robustness, we still compared our method with four state-of-the-art(SOTAs) single-modal watermarking methods, i.e.,Stegastamp(Tancik, Mildenhall, and $\\Nu \\tt { g } 2 0 2 0 \\rangle$ , PIMoG(Fang et al. 2022), MBRS(Jia, Fang, and Zhang 2021), DWSF(Guo et al. 2023).\n\nDatasets. Given the absence of a suitable screenshot dataset for VSC protection, we created a dataset called ScreenImage, comprising 100,000 screenshots from various devices and resolutions ranging from SD $( 7 2 0 \\mathrm { x } 4 8 0 )$ to 4K $( 3 8 4 0 \\mathrm { x } 2 1 6 0 )$ . We randomly selected 50,000 images as our training dataset. To evaluate the ScreenMark, we randomly sample 1,000 images each from ImageNet(Deng et al. 2009) and ScreenImage(excluding training) respectively. Notably, MBRS only accepts a fixed input size post-training, necessitating the scaling of test images to 128x128. Detailed dataset categorization and collection are displayed in APPENDIX.\n\nImplementations. Our method is implemented using PyTorch(2019) and executed on an NVIDIA GeForce RTX 4090 GPU. In terms of experimental parameter, the information length $L$ is 100. The height $H$ and width $W$ of watermark pattern $P _ { W }$ is 512, optimized for blending with screens of different resolutions without implying a minimum size limit. The batch size $N _ { 0 }$ , diffusion block $N _ { 1 }$ and reversal block $N _ { 2 }$ is 16, 5 and 2, respectively. In Stage-1, the loss function weight factors $\\beta$ and $\\gamma$ are 0.1 and 1, respectively. For $L _ { P a t t e r n }$ , $\\lambda _ { 0 } , \\lambda _ { 1 }$ , $\\lambda _ { 2 }$ and $\\lambda _ { 3 }$ are set to 1.0, 0.5, 0.1 and 0.01, respectively. Through 8 sets of ablation studies, we balanced choices based on fast training and high visual quality. The $\\alpha$ of Alpha-Fusion Rendering Module $R _ { \\alpha }$ is set to 5, allowing us to control the range by adjusting $\\alpha$ to achieve a trade-off. Robustness remains stable when the PSNR is between 36 and 42 dB $( \\alpha \\in [ 5 , 8 ] )$ ), which aligns with the PSNR range of the SOTAs. We use the Adam optimizer(2014) with a learning rate of 1e-5, and set the training epochs to 100, while the compared methods adopt their default settings. Hyperparameter ablations are provided in APPENDIX.\n\nTable 2: Bit accuracy rate $( \\mathrm { B A R } , \\% )$ on different pixel-level attacks   \n\n<html><body><table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Length (Bits)</td><td rowspan=\"2\">Clean</td><td colspan=\"3\">JPEG(QF)</td><td colspan=\"3\">Noise(œÉ)</td><td colspan=\"3\">Blur(k)</td><td rowspan=\"2\">Average</td></tr><tr><td>95</td><td>85</td><td>75</td><td>0.01</td><td>0.05</td><td>0.1</td><td>3</td><td>5</td><td>7</td></tr><tr><td rowspan=\"5\">ImageNet</td><td> StegaStamp</td><td>100</td><td>98.48</td><td>95.43</td><td>94.18</td><td>92.37</td><td>97.31</td><td>96.75</td><td>91.62</td><td>96.62</td><td>95.37</td><td>94.12</td><td>94.86</td></tr><tr><td>MBRS</td><td>30</td><td>97.97</td><td>97.45</td><td>96.53</td><td>95.47</td><td>98.29</td><td>98.17</td><td>97.88</td><td>97.43</td><td>95.89</td><td>94.14</td><td>96.80</td></tr><tr><td>PIMoG</td><td>30</td><td>99.38</td><td>96.38</td><td>95.14</td><td>93.27</td><td>98.38</td><td>98.29</td><td>97.64</td><td>97.75</td><td>96.43</td><td>95.13</td><td>96.49</td></tr><tr><td>DWSF</td><td>30</td><td>100.00</td><td>99.31</td><td>97.64</td><td>95.88</td><td>99.13</td><td>98.65</td><td>95.24</td><td>98.66</td><td>96.17</td><td>96.82</td><td>97.50</td></tr><tr><td>ScreenMark</td><td>100</td><td>100.00</td><td>99.21</td><td>97.68</td><td>95.77</td><td>99.03</td><td>98.68</td><td>95.23</td><td>98.85</td><td>96.23</td><td>95.84</td><td>97.39</td></tr><tr><td rowspan=\"5\">ScreenImage</td><td> StegaStamp</td><td>100</td><td>97.06</td><td>93.00</td><td>92.31</td><td>90.31</td><td>95.00</td><td>89.12</td><td>80.43</td><td>93.68</td><td>94.06</td><td>93.81</td><td>91.30</td></tr><tr><td>MBRS</td><td>30</td><td>98.35</td><td>98.92</td><td>97.49</td><td>95.58</td><td>98.63</td><td>98.51</td><td>98.18</td><td>97.91</td><td>96.35</td><td>94.67</td><td>97.36</td></tr><tr><td>PIMoG</td><td>30</td><td>99.86</td><td>96.89</td><td>95.59</td><td>93.47</td><td>98.79</td><td>98.70</td><td>98.10</td><td>98.22</td><td>96.98</td><td>95.59</td><td>96.92</td></tr><tr><td>DWSF</td><td>30</td><td>100.00</td><td>99.15</td><td>97.38</td><td>95.27</td><td>99.21</td><td>98.43</td><td>95.12</td><td>99.45</td><td>96.20</td><td>95.86</td><td>97.34</td></tr><tr><td>ScreenMark</td><td>100</td><td>100.00</td><td>99.39</td><td>98.34</td><td>96.69</td><td>100.00</td><td>99.08</td><td>96.19</td><td>100.00</td><td>98.38</td><td>95.65</td><td>98.19</td></tr></table></body></html>\n\nMetrics. We consider Peak Signal to Noise Ratio (PSNR), Structure Similarity Index Measure (SSIM), Learned Perceptual Image Patch Similarity(Zhang et al. 2018) (LPIPS) to evaluate visual quality. And consider Bit Accuracy Rate (BAR) evaluate robustness performance.\n\n# Robustness Performance\n\nIn this section, we compare the robustness of our ScreenMark method with four SOTAs across various attack types. The watermark length was set to 100 bits in our experiments. The types and implementation details of the attack settings also align with those used in SOTAs. To ensure objective results, we used the ImageNet and ScreenImage datasets for our experiments. Further experiments on watermark length, robustness against severe, hybrid and real-world scenarios attacks are included in the APPENDIX.\n\nRobustness against Image-level Attacks We evaluate the robustness of the ScreenMark and SOTAs against imagelevel attacks in different factors. In Tab.1, the headers represent the proportion of the Crop, Cropout, and Resize attacks in the original images, measured in percentage. The attack methods used in experiments align with the SOTAs for consistency and comparability. Notably, Our ScreenMark consistently achieves over $94 \\%$ bit accuracy, with an average performance of $9 7 . 8 1 \\%$ across both datasets, regardless of the attack type. Although in some cases do not outperform the best method, our performance remains close to the top.\n\nRobustness against Pixel-level Attacks In addition to image-level attacks, we also assessed robustness against pixel-level attacks, which are common in social networking scenarios. Tab.2 reports the bit error rate against pixellevel attacks in different factors. The table headers indicate the factors for each attack: JPEG compression quality factor (QF), Gaussian noise standard deviation $( \\sigma )$ , and Gaussian blur kernel size $( \\kappa )$ . Our ScreenMark method demonstrates exceptional stability and performance across various pixel-level attacks. In many cases, ScreenMark achieves the highest or second-highest bit accuracy rates, highlighting its robustness. Overall, ScreenMark consistently performs at a level comparable to the best method, and often surpasses other methods by approximately $1 \\%$ , showcasing its reliability and effectiveness.\n\n![](images/de954e159108865b8fc49873f34b03492f7ceb60e0b8aa6d97080b305192ca68.jpg)  \nFigure 3: The visualization of the ScreenMark impact\n\nRobustness in Real screenshot Scenarios To verify the practical application of ScreenMark, we tested its robustness in real screenshot scenarios. We collected screenshots using various publicly available tools, including Windows Screenshot, Snipaste, Greenshot, and WeChat Screenshot, across different resolutions. In experiments, watermarked VSC screenshots were randomly cropped with a fixed cropping size of $4 0 0 ^ { * } 4 0 0$ pixels, not exceeding $8 \\%$ of the area of a 1080P image. The screenshots were saved in JPG format, with the compression quality determined by the default settings of each tool. The results are inclued in APPENDIX, indicating that ScreenMark maintains a bit accuracy rate above $94 \\%$ across all resolutions and tools. This level of accuracy ensures that the watermark can be fully and correctly extracted in practical applications, especially when error-correcting codes are incorporated.\n\n# Visual Quality\n\nThe present work not only addresses the shortcomings of mainstream watermarking methods in VSC protection scenarios, but achieves impressive visual quality. We verify the excellent performance of our work in terms of visual quality through qualitative visualization and quantitative metrics.\n\nVisualization of Watermarking Residuals As described in Stage-1, we control the generated watermark pattern to be an irregular image that is close to zero, evenly dispersed, gently varying, and as balanced in RGB channels as possible. This minimizes the impact on the original VSC quality while avoiding malicious recognition and erasure by watermark attackers. Our proposed ScreenMark is fused with arbitrary VSC, protecting massive media contents on screen in real time that SOTAs cannot. Considering this, we calculated and magnified the residuals of the watermarked image compared to the original image by 20 times, using a randomly selected test image from ScreenImage. The results of the visualization are shown in Fig.3, confirming that our watermark pattern meets our design expectations. Richer visualization of ScreenMark is shown in APPENDIX.\n\nQuantification of Watermarked Images We introduced relevant image quality metrics from existing watermarking frameworks to assess visual quality. PSNR measures the image quality reference value between the maximum signal and background noise in dB, with higher values indicating less distortion. SSIM quantifies structural similarity between two images, with values from 0 to 1, and higher values indicating more similarity. LPIPS measures the difference between two images, with lower values indicating greater similarity. Table 3 reports the visual quantification of watermarked images using different methods. ScreenMark achieves the best performance in both SSIM and LPIPS metrics, thanks to our pattern control and alpha fusion strategy.\n\nTable 3: Visual quantification in different methods   \n\n<html><body><table><tr><td>Datasets</td><td>Methods</td><td>PSNR‚Üë</td><td>SSIM‚Üë</td><td>LPIPS‚Üì</td></tr><tr><td>ImageNet</td><td>StegaStamp MBRS PIMoG DWSF ScreenMark</td><td>23.89 36.49 36.21 41.47 41.38</td><td>0.8025 0.9173 0.9850 0.9831 0.9969</td><td>0.0515 0.0387 0.0312 0.0083 0.0058</td></tr><tr><td>ScreenImage</td><td>StegaStamp MBRS PIMoG DWSF ScreenMark</td><td>28.06 38.21 38.33 42.60 41.86</td><td>0.9411 0.9384 0.9873 0.9905 0.9948</td><td>0.1640 0.0232 0.0218 0.0074 0.0055</td></tr></table></body></html>\n\n# Other Comparison\n\nWe have demonstrated the visual quality and robustness performance of ScreenMark, the key metrics for mainstream watermarkings. However, due to the unique scenarios addressed in this work, additional differences between ScreenMark and SOTAs need to be experimentally verified.\n\nThe first key difference is the advantage of the proposed three-stage progressive training strategy in ScreenMark compared to the traditional end-to-end training approach. The second difference lies in the scope of protection, where ScreenMark offers a broader range of coverage than single-modality watermarking, as depicted in Fig.1, so it will not be verified again. The third difference is the response time for watermark embedding in dynamic VSC changes, where ScreenMark significantly outperforms single-modality watermarking.\n\nThe Ablation of Training Strategy One key advantage of ScreenMark is its three-stage progressive training strategy, compared to the traditional end-to-end training approach. This strategy provides two main benefits: it allows the model to gain experience with simpler tasks first, simplifying the learning process for more complex tasks and preventing premature convergence to local optima. Additionally, staged training enables the use of more focused and refined loss functions and optimization strategies at each stage. We conducted an ablation study to compare different training strategies, as shown in Figure 4. Our results indicate that the three-stage strategy achieves convergence by the 15th epoch, while E2E training exhibits oscillations in the loss function.\n\n![](images/06c6275bb4c19fde057307c21fade40979d63fb71b4cff67feddf6496bfb738c.jpg)  \nFigure 4: Loss (solid)/Acc (dashed) vs. Epoch of training: Stage-1(blue), Stage-2(orange), Stage-3(green), E2E(red).\n\nThe Comparison of Temporal Limitation Digital watermarking for VSC must be capable of real-time protection, as screen capture commands can be scripted to achieve millisecond level interception. Existing single-modal watermarks can not embed in dynamic screen content in real time. We validated the responsiveness of SOTAs to VSC changes. As shown in Tab.4, our watermark, fused directly to the screen, achieves a reaction time of 0 milliseconds, outperforming SOTAs. In particular, here the time is not the execution time of program, but the reaction time to reload the watermark information to the new VSC when it changes.\n\nTable 4: Average reaction time of SOTAs in VSC scenarios   \n\n<html><body><table><tr><td>SOTAs</td><td>StegaStamp</td><td>MBRS</td><td>PIMoG</td><td>DWSF</td><td>ScreenMark</td></tr><tr><td>Time(ms)</td><td>24.91</td><td>18.86</td><td>19.83</td><td>14.91</td><td>0</td></tr></table></body></html>\n\n# Conclusion\n\nIn this paper, we propose ScreenMark, a robust deep learning-based robust watermarking scheme for arbitrary VSC protection, using a three-stage progressive watermarking training strategy. The message diffuser and message reverser facilitate the transformation between regular watermarking information and irregular watermarking patterns. The alpha-fusion rendering module integrates these patterns into VSCs of any resolution, while the screen decoder extracts the watermark information from distorted watermarked screenshots. We built a dataset with 100,000 screenshots from various devices and resolutions. Extensive experiments demonstrate the effectiveness of ScreenMark in robustness, imperceptibility, and practical applicability.\n\n# Acknowledgments\n\nThis work was supported by the National Natural Science Foundation of China under Grants U20B2051, U22B2047, 62450067, 62072114.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ÂΩìÂâçÊï∞Â≠óÊ∞¥Âç∞ÊäÄÊúØ‰∏ªË¶ÅÈíàÂØπÁâπÂÆöÂ™í‰ΩìÁ±ªÂûãÔºàÂ¶ÇÂõæÂÉè„ÄÅËßÜÈ¢ëÔºâÔºåÈöæ‰ª•ÊúâÊïà‰øùÊä§ËÆ°ÁÆóÊú∫Â±èÂπï‰∏äÊòæÁ§∫ÁöÑÂ§öÊ®°ÊÄÅÂä®ÊÄÅÂÜÖÂÆπÔºàVisual Screen Content, VSCÔºâ„ÄÇÂ±èÂπïÂÜÖÂÆπÊòìÈÄöËøáÊà™ÂõæÊ≥ÑÈú≤ÔºåÁé∞ÊúâÊ∞¥Âç∞ÊñπÊ≥ïÂú®‰øùÊä§ËåÉÂõ¥„ÄÅÂÆûÊó∂ÊÄßÂíåËßÜËßâË¥®Èáè‰∏äÂ≠òÂú®Â±ÄÈôê„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®‰ºÅ‰∏öÂíåÂÆ∂Â∫≠ËÆ°ÁÆóÊú∫Êï∞ÊçÆÂÆâÂÖ®‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºå‰º†ÁªüÂä†ÂØÜÂíåÊùÉÈôêÁÆ°ÁêÜÊó†Ê≥ïÈò≤Ê≠¢ÊéàÊùÉÁî®Êà∑ÈÄöËøáÊà™ÂõæÂ∑•ÂÖ∑ÂÆûÊó∂ÊçïËé∑Â±èÂπïÂÜÖÂÆπ„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫ScreenMarkÔºå‰∏ÄÁßçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÈ≤ÅÊ£íÊ∞¥Âç∞Ê°ÜÊû∂ÔºåÈÄöËøá‰∏âÈò∂ÊÆµÊ∏êËøõËÆ≠ÁªÉÁ≠ñÁï•Â∞ÜËßÑÂàôÊ∞¥Âç∞‰ø°ÊÅØÊâ©Êï£‰∏∫‰∏çËßÑÂàôÊ∞¥Âç∞Ê®°ÂºèÔºåÂπ∂‰∏éÂ±èÂπïÂÜÖÂÆπËá™ÁÑ∂ËûçÂêàÔºåÂÆûÁé∞ÂÆûÊó∂„ÄÅÂ§öÊ®°ÊÄÅÁöÑVSC‰øùÊä§„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ê°ÜÊû∂Ôºö** È¶ñ‰∏™ÈíàÂØπVSC‰øùÊä§ÁöÑÂü∫‰∫éÂ≠¶‰π†ÁöÑÊ∞¥Âç∞Ê°ÜÊû∂ÔºåÈááÁî®‰∏âÈò∂ÊÆµÊ∏êËøõËÆ≠ÁªÉÔºàÊ∂àÊÅØÊâ©Êï£/ÂèçËΩ¨„ÄÅÂ±èÂπïËß£Á†ÅÂô®È¢ÑËÆ≠ÁªÉ„ÄÅËÅîÂêàÂæÆË∞ÉÔºâÔºåÂú®ImageNetÂíåScreenImageÊï∞ÊçÆÈõÜ‰∏äÂπ≥ÂùáÊØîÁâπÂáÜÁ°ÆÁéáÔºàBARÔºâËææ96.8%Âíå98.82%„ÄÇ\\n> *   **ÂÆûÁî®Âú∫ÊôØË¶ÜÁõñÔºö** ÊûÑÂª∫ÂåÖÂê´10‰∏áÂº†Â§öËÆæÂ§á/ÂàÜËæ®ÁéáÊà™ÂõæÁöÑÊï∞ÊçÆÈõÜÔºàSDÂà∞4KÔºâÔºåÊ∞¥Âç∞ÂìçÂ∫îÊó∂Èó¥‰∏∫0ÊØ´ÁßíÔºåÊîØÊåÅÂä®ÊÄÅÂ±èÂπïÂÜÖÂÆπÂÆûÊó∂‰øùÊä§„ÄÇ\\n> *   **ÊÄßËÉΩ‰ºòÂäøÔºö** Âú®ÂõæÂÉèÁ∫ßÔºàË£ÅÂâ™/Áº©ÊîæÔºâÂíåÂÉèÁ¥†Á∫ßÔºàJPEG/Âô™Â£∞ÔºâÊîªÂáª‰∏ãÔºåBARË∂ÖËøá94%ÔºåPSNRËææ41.38 dBÔºåSSIM‰∏∫0.9969ÔºåÂùá‰ºò‰∫éStegaStampÁ≠â4ÁßçSOTAÂçïÊ®°ÊÄÅÊ∞¥Âç∞ÊñπÊ≥ï„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ÂèóÊâ©Êï£Ê®°ÂûãÂêØÂèëÔºåÂ∞ÜËßÑÂàôÊ∞¥Âç∞‰ø°ÊÅØËΩ¨Âåñ‰∏∫‰∏çËßÑÂàôÊ∞¥Âç∞Ê®°ÂºèÔºàÁ±ª‰ººÂô™Â£∞ÔºâÔºåÈÄöËøáAlphaÊ∑∑ÂêàÊ∏≤Êüì‰∏éÂ±èÂπïÂÜÖÂÆπËá™ÁÑ∂ËûçÂêà„ÄÇËøôÁßçËÆæËÆ°ÈÅøÂÖç‰º†ÁªüÁºñÁ†ÅÂô®ÂµåÂÖ•ÁöÑËÄóÊó∂ÈóÆÈ¢òÔºåÂÆûÁé∞ÂÆûÊó∂‰øùÊä§Ôºå‰∏îÊ∞¥Âç∞Ê®°ÂºèÈöæ‰ª•Ê£ÄÊµãÂíåÁßªÈô§„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏é‰º†ÁªüÊ∞¥Âç∞ÂØπÊØîÔºö** ‰º†ÁªüÊñπÊ≥ïÈúÄ‰∏∫ÊØèÁßçÂ™í‰ΩìÁ±ªÂûãÂçïÁã¨ÂµåÂÖ•Ê∞¥Âç∞ÔºåÊó†Ê≥ïÂ§ÑÁêÜÂ§öÊ®°ÊÄÅVSC‰∏îÂìçÂ∫îÊÖ¢ÔºàÂ¶ÇDWSFÈúÄ14.91ÊØ´ÁßíÔºâ„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** 1) Ê∞¥Âç∞Ê®°ÂºèÈÄöËøáÊâ©Êï£ÁîüÊàêÔºå‰∏éÂ±èÂπïÂàÜËæ®ÁéáÊó†ÂÖ≥Ôºõ2) ‰∏âÈò∂ÊÆµËÆ≠ÁªÉÔºàStage-1ÂàùÂßãÂåñÊâ©Êï£/ÂèçËΩ¨Ê®°ÂùóÔºåStage-2È¢ÑËÆ≠ÁªÉÂ±èÂπïËß£Á†ÅÂô®ÔºåStage-3ËÅîÂêàÂæÆË∞ÉÔºâÈÄêÊ≠•ÊèêÂçáÈ≤ÅÊ£íÊÄßÔºõ3) AlphaÊ∑∑ÂêàÔºàŒ±=5ÔºâÁ°Æ‰øùËßÜËßâÂΩ±Âìç<2%„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **Stage-1ÔºàÊàêÂØπÂàùÂßãÂåñÔºâÔºö** \\n>     - Ê∂àÊÅØÊâ©Êï£Âô®Ôºà$M_D$ÔºâÂ∞ÜÊ∞¥Âç∞‰ø°ÊÅØ$I_w$Êâ©Êï£‰∏∫Ê®°Âºè$P_w$ÔºåÁªèÂõæÂÉèÁ∫ßÂ§±ÁúüÂô®Ôºà$D_I$ÔºâÂ§ÑÁêÜÂêéÔºåÁî±Ê∂àÊÅØÂèçËΩ¨Âô®Ôºà$M_R$ÔºâÊÅ¢Â§ç‰ø°ÊÅØ„ÄÇ\\n>     - ÂÖ≥ÈîÆÊçüÂ§±ÂáΩÊï∞ÔºöËøëÈõ∂ÊçüÂ§±Ôºà$L_{zero}$Ôºâ„ÄÅÂàÜÊï£ÊçüÂ§±Ôºà$L_{dispersion}$Ôºâ„ÄÅÂèòÂåñÊçüÂ§±Ôºà$L_{variation}$ÔºâÂíåÈÄöÈÅìÂπ≥Ë°°ÊçüÂ§±Ôºà$L_{channel}$Ôºâ„ÄÇ\\n> 2.  **Stage-2ÔºàËá™ÈÄÇÂ∫îÈ¢ÑËÆ≠ÁªÉÔºâÔºö** \\n>     - ÂÜªÁªìStage-1ÂèÇÊï∞ÔºåÈÄöËøáAlphaÊ∑∑ÂêàÊ®°Âùó$R_Œ±$Â∞Ü$P_w$‰∏éÂ±èÂπïÂÜÖÂÆπ$S_c$ËûçÂêàÔºåÁªèÂÉèÁ¥†Á∫ßÂ§±ÁúüÔºàJPEG/Âô™Â£∞/Ê®°Á≥äÔºâÂêéÔºåÁî±Â±èÂπïËß£Á†ÅÂô®$D_S$ÊèêÂèñÊ∞¥Âç∞„ÄÇ\\n>     - ÊçüÂ§±ÂáΩÊï∞Ôºö$L_{stage2} = MSE(P_{ds}, P_w)$„ÄÇ\\n> 3.  **Stage-3ÔºàÂ¢ûÂº∫ÂæÆË∞ÉÔºâÔºö** \\n>     - ËÅîÂêàÂæÆË∞É$M_D$„ÄÅ$M_R$Âíå$D_S$ÔºåÂºïÂÖ•ËÅîÂêàÁ∫ßÂ§±ÁúüÂô®$D_J$ÔºåÊçüÂ§±ÂáΩÊï∞‰∏∫$L_{stage3} = L_{stage1} + L_{stage2}$„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   StegaStamp„ÄÅMBRS„ÄÅPIMoG„ÄÅDWSFÔºàÂùá‰∏∫ÂçïÊ®°ÊÄÅÊ∞¥Âç∞SOTAÊñπÊ≥ïÔºâ„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®ÂõæÂÉèÁ∫ßÊîªÂáªÔºàË£ÅÂâ™/Áº©ÊîæÔºâ‰∏ãÁöÑÊØîÁâπÂáÜÁ°ÆÁéáÔºàBARÔºâ‰∏äÔºö** ScreenMarkÂú®ScreenImageÊï∞ÊçÆÈõÜ‰∏äÂπ≥ÂùáBARËææ98.82%ÔºåÊòæËëó‰ºò‰∫éDWSFÔºà96.37%ÔºâÂíåMBRSÔºà88.67%Ôºâ„ÄÇÂú®70%Ë£ÅÂâ™ÊØî‰æã‰∏ãÔºåScreenMark‰ªç‰øùÊåÅ94.17%ÂáÜÁ°ÆÁéáÔºåÊØîPIMoGÔºà70.16%ÔºâÈ´ò24‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ÂÉèÁ¥†Á∫ßÊîªÂáªÔºàJPEG/Âô™Â£∞Ôºâ‰∏ãÁöÑBAR‰∏äÔºö** ScreenMarkÂú®JPEG QF=75Êó∂BAR‰∏∫95.77%ÔºåÊé•ËøëDWSFÔºà95.88%ÔºâÔºõ‰ΩÜÂú®È´òÊñØÂô™Â£∞ÔºàœÉ=0.1Ôºâ‰∏ãÔºåScreenMarkÔºà95.23%Ôºâ‰ºò‰∫éDWSFÔºà95.12%Ôºâ„ÄÇ\\n> *   **Âú®ËßÜËßâË¥®ÈáèÔºàPSNR/SSIMÔºâ‰∏äÔºö** ScreenMarkÁöÑPSNR‰∏∫41.38 dBÔºåSSIM‰∏∫0.9969ÔºåÂùá‰ºò‰∫éDWSFÔºà41.47 dB/0.9831ÔºâÂíåPIMoGÔºà36.21 dB/0.9850Ôºâ„ÄÇ\\n> *   **Âú®ÂìçÂ∫îÊó∂Èó¥‰∏äÔºö** ScreenMarkÊ∞¥Âç∞ËûçÂêàÊó∂Èó¥‰∏∫0ÊØ´ÁßíÔºåËÄåStegaStampÈúÄ24.91ÊØ´ÁßíÔºåMBRSÈúÄ18.86ÊØ´Áßí„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Â±èÂπïÂÜÖÂÆπ‰øùÊä§ (Visual Screen Content Protection, VSCP)\\n*   Êï∞Â≠óÊ∞¥Âç∞ (Digital Watermarking, DW)\\n*   ‰∏âÈò∂ÊÆµÊ∏êËøõËÆ≠ÁªÉ (Three-stage Progressive Training, N/A)\\n*   AlphaÊ∑∑ÂêàÊ∏≤Êüì (Alpha Blending Rendering, N/A)\\n*   È≤ÅÊ£íÊÄß (Robustness, N/A)\\n*   Â§öÊ®°ÊÄÅÂÜÖÂÆπ (Multimodal Content, N/A)\\n*   Êà™ÂõæÊîªÂáª (Screenshot Attack, N/A)\\n*   Ê∑±Â∫¶Â≠¶‰π† (Deep Learning, DL)\"\n}\n```"
}