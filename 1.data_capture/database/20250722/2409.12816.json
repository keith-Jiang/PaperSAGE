{
    "link": "https://arxiv.org/abs/2409.12816",
    "pdf_link": "https://arxiv.org/pdf/2409.12816",
    "title": "Hierarchical Gradient-Based Genetic Sampling for Accurate Prediction of Biological Oscillations",
    "authors": [
        "Heng Rao",
        "Yu Gu",
        "Jason Zipeng Zhang",
        "Ge Yu",
        "Yang Cao",
        "Minghan Chen"
    ],
    "publication_date": "2024-09-19",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "paper_content": "# Hierarchical Gradient-Based Genetic Sampling for Accurate Prediction of Biological Oscillations\n\nHeng Rao1, $\\mathbf { Y } \\mathbf { u } \\mathbf { G } \\mathbf { u } ^ { 1 * }$ , Jason Zipeng Zhang2, Ge ${ \\bf Y } { \\bf u } ^ { 1 }$ , Yang $\\mathbf { C a o } ^ { 3 }$ , Minghan Chen2\\*\n\n1College of Computer Science and Engineering, Northeastern University, Shenyang, China 2Department of Computer Science, Wake Forest University, Winston-Salem, NC, USA 3Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA guyu $@$ mail.neu.edu.cn, chenm $@$ wfu.edu\n\n# Abstract\n\nBiological oscillations are periodic changes in various signaling processes crucial for the proper functioning of living organisms. These oscillations are modeled by ordinary differential equations, with coefficient variations leading to diverse periodic behaviors, typically measured by oscillatory frequencies. This paper explores sampling techniques for neural networks to model the relationship between system coefficients and oscillatory frequency. However, the scarcity of oscillations in the vast coefficient space results in many samples exhibiting non-periodic behaviors, and small coefficient changes near oscillation boundaries can significantly alter oscillatory properties. This leads to non-oscillatory bias and boundary sensitivity, making accurate predictions difficult. While existing importance and uncertainty sampling approaches partially mitigate these challenges, they either fail to resolve the sensitivity problem or result in redundant sampling. To address these limitations, we propose the Hierarchical Gradient-based Genetic Sampling (HGGS) framework, which improves the accuracy of neural network predictions for biological oscillations. The first layer, Gradientbased Filtering, extracts sensitive oscillation boundaries and removes redundant non-oscillatory samples, creating a balanced coarse dataset. The second layer, Multigrid Genetic Sampling, utilizes residual information to refine these boundaries and explore new high-residual regions, increasing data diversity for model training. Experimental results demonstrate that HGGS outperforms seven comparative sampling methods across four biological systems, highlighting its effectiveness in enhancing sampling and prediction accuracy.\n\n# 1 Introduction\n\nIn biological systems, oscillation refers to the repetitive, cyclical behaviors of cell signaling and biological processes over time, such as fluctuations in concentrations of biochemical substances (Tamate et al. 2017), rhythmic activities in cellular functions (Goldbeter 2002), or periodic changes in physiological states (Kurosawa, Mochizuki, and Iwasa 2002). Understanding these oscillations is crucial for comprehending how biological systems maintain stability, respond to external stimuli, and regulate complex processes. To study oscillations, researchers often use ordinary differential equations (ODEs) to model the temporal dynamics that characterize oscillatory patterns in biological systems. Recently, the application of machine learning to biological systems has gained significant attention. In many studies (Daneker et al. 2023; Yazdani et al. 2020; Szep, Dalchau, and Csika´sz-Nagy 2021), system-level mathematical models of biological reactions are integrated with machine learning models to establish relationships between raw data and system coefficients. Of these, Neural Networks (NNs) show promising results, offering an effective tool for predicting biological oscillations based on system coefficients.\n\nHowever, the relationship between system coefficients and oscillatory behaviors is highly complex (Stark, Chan, and George 2007). Variations in these coefficients can disrupt oscillations, causing non-oscillatory states. In biological systems, coefficients that can lead to oscillatory states are relatively rare; most coefficient combinations result in non-oscillatory behavior, especially in high-dimensional domains. The prevalence of non-oscillatory states creates a significant data imbalance, challenging the prediction of oscillatory frequency—a key characteristic of oscillation, defined as the inverse of the oscillatory period ( per1iod ). When oscillation is absent, the frequency is set to zero. Fig. 1(a) illustrates the mapping between system coefficients and oscillatory frequencies of a cell cycle system (Liu et al. 2012). The domain shows data imbalance with $70 \\%$ non-oscillation (blue). Moreover, minor changes in these coefficients can lead to substantial changes in oscillatory frequencies, particularly in boundary areas where oscillatory states transition to non-oscillatory states. This heightened sensitivity further complicates accurate predictions and is reflected in the poor performance of NNs in these regions, as evidenced by high residuals (absolute error) marked as black shades in Fig. 1. Overall, we face two challenges:\n\n• Non-Oscillatory Bias. The majority of data samples exhibit non-oscillatory behaviors, resulting in an abundance of redundant information that does not improve model accuracy and may even reduce efficiency. • Oscillatory Boundary Sensitivity. Sharp transitions between non-oscillatory and oscillatory states occur in narrow boundaries of the coefficient space, which are easily overlooked by random sampling and lead to high errors.\n\nTherefore, a more effective sampling strategy is necessary to obtain more balanced and representative samples for model training. Importance Sampling (IS) (Liu et al. 2021; Lu et al. 2023) can be used to resample more minority class instances, balancing the majority-minority ratio. As shown in Fig. 1(b), IS resamples the oscillation class, with darker samples indicating higher resampling, but it cannot generate new samples to alleviate boundary sensitivity issues. Synthetic Minority Oversampling Technique (SMOTE) (Chawla et al. 2002; Torgo et al. 2013) offers another way to balancing the dataset by generating synthetic minority samples. However, it insufficiently covers high-residual areas (red box), see Fig. 1(c). Targeting data sensitivity, Uncertainty Sampling (US) (Liu and Li 2023) selects informative samples based on predicted importance. However, due to limited diversity in the random selection process, US often produces redundant samples, reducing efficiency. This is evident in Fig. 1(d), where new samples are scattered across the domain but fail to adequately cover high-residual regions (red box) compared to our method in Fig. 1(e). Thus, these sampling strategies fail to effectively address the challenges.\n\n![](images/17eb43f126f605ef6e74d2425e660b7217b46ab05ad9c369537861ae05ba69d3.jpg)  \nFigure 1: Examples of different sampling methods applied to the system coefficients of a cell cycle model (Liu et al. 2012). The orange-blue and black-white color bars indicate trends in oscillation and residual changes, respectively. (a) LHS performs a random sampling of 1000 points in the coefficient domain. (b) IS fails to introduce new samples; (c-d) SMOTE and US lack sufficient coverage in high residual regions (red box). US also produces redundant samples spreading across the domain; (e) Our proposed HGGS method extracts the boundary information (red, yellow boxes) and effectively generates new samples (tear triangle) concentrating on high-residual regions (red box).\n\nThis work introduces the Hierarchical Gradient-based Genetic Sampling (HGGS) method, a novel two-layer framework designed to enhance the effectiveness of selecting representative samples and improve the accuracy of predictions for biological oscillations. Specifically, starting with an initial candidate set obtained by Latin Hypercube Sampling (LHS), we first apply Gradient-based Filtering (GF) to select samples near boundary regions and balance the proportions of non-oscillation (majority) and oscillation (minority)\n\nsamples. This GF layer yields a balanced coarse dataset that guides subsequent refinement. In the second layer, Multigrid Genetic Sampling (MGS) dynamically constructs grids at multiple levels based on residual (absolute error) information from training data. By sampling within these grids, MGS not only enriches existing boundaries but also explores new high-residual areas. Through continual learning, HGGS iteratively refines the training dataset with more representative samples, consistently improving model performance. The main contributions of this paper are summarized below:\n\n• We propose a simple and efficient Gradient-based Filtering technique that can extract oscillation boundaries, which often entail high residuals, and removes redundant non-oscillatory samples. The GF layer generates balanced coarse data, enabling efficient MGS refinement. • Our Multigrid Genetic Sampling strategy leverages residual information to refine existing boundaries and explore new high-residual regions. The MGS layer systematically samples across grids at different levels, reducing sensitivity and enhancing oscillation diversity. • The proposed HGGS method ensures a representative dataset through continuous adaption to the evolving error landscape during training, showing superior accuracy over seven baselines across four biological systems. HGGS is versatile and can be applied to predict various systematic features beyond biological oscillations.\n\n# 2 Related Work\n\nSampling for Data Imbalance. Data imbalance often results in biased model performance, favoring majority samples while underperforming on minority ones. To address this issue, several sampling techniques have been proposed, including undersampling, oversampling, and importance sampling techniques. Undersampling randomly removes majority samples (Wilson 1972; Tomek 1976; Guo et al. 2008) to balance the imbalance ratio, but it risks discarding informative data. On the other hand, oversampling mitigates data imbalance by augmenting minority class samples, as in SMOTE (Chawla et al. 2002; Torgo et al. 2013). While SMOTE generates new minority samples to improve balance, it can introduce noise due to interpolated labels and may struggle to create truly representative minority samples. Importance Sampling (IS) (Liu et al. 2021; Lu et al. 2023) combines elements of both undersampling and oversampling by resampling more informative minority samples and excluding well-performing majority samples. However, it is prone to overfitting due to resampling and does not incorporate new data. Although these techniques offer various ways to alleviate data imbalance, they all struggle with the data sensitivity challenge.\n\nDynamic Sampling. Dynamic sampling offers a more adaptive approach to selecting informative samples, enhancing model training by adjusting sample selection based on the model’s evolving state. This idea is widely utilized in Active Learning (AL), where various dynamic sampling methods leverage direct or indirect information from the model to select the most representative samples from unlabeled data. One prominent method in AL is Uncertainty Sampling (US) (Lewis and Catlett 1994; Zhu et al. 2010; Liu and Li 2023), which selects samples based on their uncertainty scores. In classification, these scores can be calculated using techniques such as entropy uncertainty (Shannon 1948) or confidence margin uncertainty (Sharma and Bilgic 2017). Diversity-based strategies aim to select a broad range of samples based on data distribution, employing methods like gradient representation (Saran et al. 2023) and switch events (Benkert et al. 2023). Query by committee methods (Burbidge, Rowland, and King 2007; Kee, Del Castillo, and Runger 2018; Hino and Eguchi 2023) aggregate outputs from multiple models to form new discriminative criteria, identifying the most representative samples for labeling by considering the underlying data distribution. However, uncertainty sampling and diversity-based sampling often introduce significant redundancy, reducing sampling efficiency. Additionally, most of these methods are designed for nominal target variables and are rarely applicable to regression problems with continuous targets (Liu and Li 2023). In contrast, our HGGS method leverages residual information to ensure targeted sampling in high-residual areas and avoid redundancy, boosting both effectiveness and efficiency.\n\n# 3 Preliminary\n\nIn this paper, we utilize a neural network model $\\hat { y } ( \\pmb { \\lambda } ) =$ $f _ { n n } ( \\lambda ; \\mathbf { \\bar { \\Theta } } )$ to approximate the oscillatory frequency $y ( \\pmb { \\lambda } ) =$ $f _ { P } ( \\boldsymbol { u } ( t , \\lambda ) ) \\in \\mathbb { R } ^ { D ^ { \\prime } }$ of a system under initial state $\\mathbf { \\delta } \\mathbf { u } _ { 0 }$ , given any set of coefficients $\\boldsymbol { \\lambda } \\in \\mathbb { R } ^ { D }$ in the ODEs presented below.\n\n$$\n\\begin{array} { c } { \\displaystyle \\frac { \\mathrm { d } \\pmb { u } } { \\mathrm { d } t } = \\pmb { N } ( \\pmb { u } ) , t \\in [ 0 , T ] } \\\\ { \\displaystyle { , ( t , \\pmb { \\lambda } ) | _ { t = 0 } = \\pmb { u } _ { 0 } ( \\pmb { \\lambda } ) , \\pmb { \\lambda } \\in \\Omega , } } \\end{array}\n$$\n\nwhere $f _ { P }$ is the oscillatory operator used to calculate the oscillatory frequency $f _ { P } ( { \\pmb u } ( { \\pmb t } , \\lambda ) )$ of $\\mathbf { \\Delta } _ { \\pmb { u } }$ over the time span $\\pmb { t } = [ 0 , \\dot { t _ { 1 } } , t _ { 2 } , \\dot { { \\mathrm { ~  ~ \\scriptstyle ~ \\cdot ~ } } } , \\dot { T } ] ^ { \\top }$ using (Apicella et al. 2013). $\\bar { \\mathcal { N } ( \\boldsymbol { u } ) }$ denotes a nonlinear operator consisting of variables vector $u , \\lambda \\in \\Omega$ is a $D$ -dimensional system coefficient vector, $\\Omega$ is a subset of $\\mathbb { R } ^ { D } , \\boldsymbol { u } _ { 0 }$ is the initial condition vector in $D ^ { \\prime }$ dimensions, and $\\Theta$ is the parameter of the neural network. Our goal is to minimize the error in approximating the original function $f _ { P } ( { \\pmb u } ( { \\pmb t } , \\pmb \\lambda ) )$ using a neural network $f _ { n n } \\in F : \\mathbb { R } ^ { D } \\to \\mathbb { R } ^ { D ^ { \\prime } }$ , where $F$ represents an appropriate function mapping space. We achieve this by employing supervised learning with the following loss function.\n\n$$\nL ( f _ { n n } ( \\pmb { \\lambda } ; \\Theta ) , y ) = \\| l ( f _ { n n } ( \\pmb { \\lambda } ; \\Theta ) , y ) \\| _ { 2 , \\Omega } ^ { 2 } ,\n$$\n\nwhere $l ( f _ { n n } ( \\pmb { \\lambda } ; \\pmb { \\Theta } ) , y ) = | f _ { n n } ( \\pmb { \\lambda } ; \\pmb { \\Theta } ) - y |$ and $\\left\\| \\cdot \\right\\| _ { 2 }$ denotes Euclidean normalization.\n\nIn practice, Monte Carlo (MC) approximation is used to estimate the overall error $L$ using a finite set of sampled points from the domain, denoted as $L _ { N }$ . Thus, the loss function can be approximated as:\n\n$$\n\\begin{array} { l } { { \\displaystyle { \\cal L } ( \\boldsymbol { \\Theta } ) \\approx { \\cal L } _ { N } ( \\boldsymbol { \\Theta } ) = \\left\\| l ( f _ { n n } ( \\boldsymbol { \\lambda } ; \\boldsymbol { \\Theta } ) , y ) \\right\\| _ { 2 , S _ { \\Omega } } ^ { 2 } } } \\\\ { { \\displaystyle ~ = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\left\\| l ( f _ { n n } ( \\boldsymbol { \\lambda } ^ { ( i ) } ; \\boldsymbol { \\Theta } ) , y ^ { ( i ) } ) \\right\\| _ { 2 } ^ { 2 } } , } \\end{array}\n$$\n\nwhere $N$ is the number of samples and ${ \\cal S } _ { \\Omega } = \\{ \\lambda ^ { ( i ) } \\} _ { i = 1 } ^ { N }$ represents the set of sampled coefficients from $\\Omega$ . To theoretically prove that sampling methods can further reduce the error between $f _ { n n }$ and $f _ { P }$ , we build on the framework from (Tang et al. 2023). Through analyzing the MC approximate loss function $L _ { N }$ , we derive the theoretical upper bound of the optimal model $f _ { n n } ( \\cdot ) _ { N } ^ { * }$ approximating to oscillatory frequency $y ( \\cdot )$ . Additionally, we introduce the theoretically optimal model $f _ { n n } ( \\cdot ) ^ { * }$ from overall error $L$ , categorizing the error into two parts:\n\n$$\n\\begin{array} { r l } & { E ( | | f _ { n n } ( \\cdot ) _ { N } ^ { * } - y ( \\cdot ) | | _ { \\Omega } ) \\leq E ( | | f _ { n n } ( \\cdot ) _ { N } ^ { * } - f _ { n n } ( \\cdot ) ^ { * } | | _ { \\Omega } ) } \\\\ & { \\phantom { \\frac { 1 } { 1 } } + | | f _ { n n } ( \\cdot ) ^ { * } - y ( \\cdot ) ) | | _ { \\Omega } . } \\end{array}\n$$\n\nHere, $E$ is the expectation operator, and $\\lVert \\cdot \\rVert _ { \\Omega }$ is the norm operator in the function space $F$ . The first term represents the statistical error, while the second term reflects the model approximation error determined by model structure. Sampling new data from the residual distribution over $\\Omega$ can reduce statistical error. Proof details are provided in the Appendix.\n\n# 4 Method\n\nThis section describes the Hierarchical Gradient-based Genetic Sampling (HGGS) framework (Fig. 2), which divides the sampling process into two layers: Gradient-based Filtering and Multigrid Genetic Sampling. HGGS yields a diverse, balanced dataset, drastically reducing model statistical error.\n\n# Gradient-based Filtering\n\nAs demonstrated in Fig. 1, biological systems often contain a significant amount of redundant data in non-oscillatory regions. Removing these redundant non-oscillatory samples not only preserves the model’s overall performance but also increases the proportion of oscillatory samples in the dataset, thereby mitigating the effects of non-oscillatory bias.\n\nTo achieve this, we design an effective Gradient-based Filtering (GF) technique that identifies sensitive regions, particularly boundary areas where small changes can lead to significant shifts in oscillatory frequency (see red, yellow boxes in Fig. 1(e)), which are also characterized by higher residuals. In this context, we first rank all samples from the initial dataset $S _ { \\Omega }$ using Eq. 4.\n\n$$\ng d ( \\pmb { \\lambda } ^ { ( i ) } ) _ { S _ { \\Omega } } = \\frac { 1 } { K } \\sum _ { j = 1 } ^ { K } \\frac { \\left\\| y ^ { ( j ) } - y ^ { ( i ) } \\right\\| _ { 2 } ^ { 2 } } { \\left\\| \\pmb { \\lambda } ^ { ( j ) } - \\pmb { \\lambda } ^ { ( i ) } \\right\\| _ { 2 } ^ { 2 } } ; \\pmb { \\lambda } ^ { i } , \\pmb { \\lambda } ^ { j } \\in S _ { \\Omega } , i \\neq j ,\n$$\n\nwhere $g d ( \\pmb { \\lambda } ^ { ( i ) } ) _ { S _ { \\Omega } }$ represents the gradient degree of collocation sample $i$ from the training dataset $S _ { \\Omega }$ and $K$ is a hyperparameter denoting the top- $K$ nearest collocation samples to $i$ in the training dataset $S _ { \\Omega }$ . This function assesses the importance of collocation sample $i$ ; a higher $g d ( \\pmb { \\lambda } ^ { ( i ) } ) _ { S _ { \\Omega } }$ value indicates that the sample is closer to boundary areas.\n\nAccording to this gradient ranking, we filter the top$r \\%$ samples $S _ { \\Omega _ { g f 1 } }$ from $S _ { \\Omega }$ , where $r$ is the filtering ratio to extract the boundary information. Next, to enhance the model’s adaptability to non-oscillatory conditions, we retain an essential global dataset, $S _ { \\Omega _ { g f 2 } }$ , from the remaining samples in $S _ { \\Omega } - S _ { \\Omega _ { g f 1 } }$ using uncertainty sampling (Liu and\n\nSystem Oscillatory coefficients 𝛌 input Coarse Sampling Fine-grained Sampling output Frequency Latin Hypercube Gradient-based Filtering Alternate MGS with Model Training Biological System 𝑋 Sampling New >>> 3000 Extract training Multigrid Genetic Sampling 1000 boundary-info NN A...... dataset 0 Tim50e 100 Model LHS samples Filtered data Training HMiegdhiuremsirdeusaildsualmspalemple Stratified data New samples Samplin each grid\n\nLi 2023). This subset is specifically selected to capture the overall characteristics and maintain performance for nonoscillatory regions. We then formulate the coarse training set $S _ { \\Omega } ^ { ( 0 ) } \\ = \\ S _ { \\Omega _ { g f } } \\ = \\ S _ { \\Omega _ { g f 1 } } \\cup S _ { \\Omega _ { g f 2 } }$ , where $S _ { \\Omega _ { g f } } \\subseteq S _ { \\Omega }$ and $| S _ { \\Omega _ { g f } } | < | S _ { \\Omega } |$ . In $\\boldsymbol { S } _ { \\Omega _ { g f } }$ , most redundant non-oscillation samples from the original dataset are removed, while a diverse set of oscillatory data is retained.\n\n# Multigrid Genetic Sampling\n\nFrom the prior GF layer, we obtained a balanced coarse set of informative samples $S _ { \\Omega } ^ { ( 0 ) }$ . Our next goal is to sharpen boundary precision and explore high-residual areas through effective sampling. This requires constructing an accurate residual distribution, as non-oscillatory regions have low probabilities and contribute little to model performance. We focus on constructing high-residual distributions.\n\nHowever, the small sample set $S _ { \\Omega }$ obtained through random sampling does not sufficiently represent the highresidual distribution $p _ { g }$ over the domain $\\Omega$ . To address this limitation, we construct multiple sampling grids of varying sizes to capture the high-residual distribution $p _ { g }$ .\n\nWe first use a Gaussian Mixture model (Reynolds et al. 2009) to characterize three potential residual distributions within $S _ { \\Omega }$ : low-, medium-, and high-residual distributions, respectively, denoted as $S _ { \\Omega _ { l r } }$ , $S _ { \\Omega _ { m r } }$ , and $S _ { \\Omega _ { h r } }$ .\n\nNext, each grid is defined as a hypercube bounded by points $\\lambda ^ { ( i ) }$ and $\\lambda ^ { ( j ) }$ , both originating from $S _ { \\Omega } ^ { ( 0 ) } - S _ { \\Omega _ { l r } } ^ { ( 0 ) }$ . To ensure comprehensive coverage of the high-residual distribution, it is necessary to construct a large number of grids at different sizes, each composed of diverse samples.\n\nBuilding on the idea of multigrid, we integrate genetic sampling into a method called Multigrid Genetic Sampling (MGS) to approximate the high-residual distribution over $\\Omega$ and further refine these areas. Specifically, we sample new points within each hypercubic grid according to Eq. 5:\n\n$$\n\\begin{array} { r } { \\lambda _ { n e w } = \\alpha \\odot ( \\mathbf { \\lambda } \\mathbf { \\lambda } ^ { ( i ) } - \\mathbf { \\lambda } ^ { ( j ) } ) + \\lambda ^ { ( j ) } ; \\alpha \\in [ 0 , 1 ] ^ { D } , i \\neq j , } \\end{array}\n$$\n\nwhere $\\alpha$ is a D-dimensional weight vector with each value in the range [0,1], and $\\odot$ denotes component-wise multiplication. $\\lambda _ { n e w }$ is a randomly sampled point within the hypercubic enclosed by $\\lambda ^ { ( i ) }$ and $\\lambda ^ { ( j ) }$ .\n\nIn order to obtain a fine-grained dataset from $S _ { \\Omega } ^ { ( 0 ) }$ , we alternate MGS with model training to continuously refine the high-residual areas. We define $m _ { c }$ as the number of sam-th sampled da $m _ { e }$ t by MGS is denoted as , and the $k ^ { \\prime }$ $\\bar { S } _ { \\Omega _ { g s } } ^ { ( \\bar { k } ^ { \\prime } ) }$ k-th training dataset is S(Ωk) = Sk′= $\\begin{array} { r } { S _ { \\Omega } ^ { ( k ) } = \\bigcup _ { k ^ { \\prime } = 1 } ^ { k } S _ { \\Omega _ { g s } } ^ { ( k ^ { \\prime } ) } \\cup S _ { \\Omega _ { g f } } } \\end{array}$\n\nFor the $( k + 1 )$ -th sampling, we utilize the current $k$ -th residual samples, categorized in low-, medium-, and highresidual distributions, denoted as $S _ { \\Omega _ { l r } } ^ { ( k ) } , \\ S _ { \\Omega _ { m r } } ^ { ( k ) }$ $S _ { \\Omega _ { h r } } ^ { ( k ) }$ Next, to exploit and explore the high-residual domain, we defined crossover and mutation operations over mediumand high-residual sets as follows:\n\n• Multigrid Crossover: Randomly select two distinct sam$\\bar { \\lambda } _ { h r } ^ { ( i ) }$ $\\lambda _ { h r } ^ { ( j ) }$ $S _ { \\Omega _ { h r } } ^ { ( k ) }$ and randomly sample a new point $\\lambda _ { h h }$ by Eq. 5 and add it to the sample set S(Ωk+1 ). This operation serves to exploit and refine existing high-residual areas. $\\lambda _ { h r } ^ { ( i ) }$ $S _ { \\Omega _ { h r } } ^ { ( k ) }$   \nand $\\lambda _ { m r } ^ { ( j ) }$ from $S _ { \\Omega _ { m r } } ^ { ( k ) }$ . Form a hypercubic grid using the two samples and sample a new point $\\lambda _ { h m }$ by Eq. 5. Add $S _ { \\Omega _ { g s } } ^ { ( \\bar { k } + 1 ) }$ to explore global boundaries and new high-residual areas.\n\nBefore the $( k + 1 )$ -th training round, our method samples $n _ { v 1 }$ h that $n _ { v 2 }$ and $| S _ { \\Omega _ { g s } } ^ { ( k + 1 ) } | = n _ { v 1 } + n _ { v 2 }$ $S _ { \\Omega _ { g s } } ^ { ( k + 1 ) } = \\bigcup _ { i = 1 } ^ { \\bar { n } _ { v 1 } } \\lambda _ { h h } ^ { ( i ) } \\cup$ $\\mathrm { U } _ { i = 1 } ^ { n _ { v 2 } } \\lambda _ { h m } ^ { ( i ) }$ . Thus, the sample set for the $( k + 1 )$ -th training rSound is S(Ωk+1) = $\\overset { \\cdot \\cdot } { S _ { \\Omega } ^ { ( k + 1 ) } } = S _ { \\Omega _ { g s } } ^ { ( k + 1 ) } \\cup S _ { \\Omega } ^ { ( k ) }$ . At the end of sampling, $n _ { s } = m _ { c } \\times \\left( n _ { v 1 } + n _ { v 2 } \\right)$ and $\\textstyle S _ { \\Omega _ { g s } } = \\bigcup _ { k = 1 } ^ { m _ { c } } S _ { \\Omega } ^ { ( k ) }$ .\n\nThe GF followed by the MGS layer, allows for effective and efficient sampling within high residuals. The pseudocode is presented in Algorithm 1.\n\nAlgorithm 1: HGGS for predicting biological oscillations   \nInput: NN model $f _ { n n } ( \\lambda ; \\Theta )$ , neighbors for gradient esti  \nmation $K$ , initial sample size $N$ , filtering ratio $r$ , sampling   \ncycle $m _ { c }$ , Multigrid Genetic Sampling budget $\\{ n _ { v 1 } , n _ { v 2 } \\}$   \nInitialization: LHS ${ \\cal S } _ { \\Omega } = \\{ ( \\lambda ^ { ( i ) } , y ^ { ( i ) } ) \\} _ { i = 1 } ^ { N }$   \nOutput: Target model $f _ { n n } ( \\lambda ; \\Theta ^ { * } )$   \n1: Apply Gradient-based Filtering to SΩ to generate S(Ω0):   \n2: $S _ { \\Omega } ^ { ( 0 ) } \\gets G F ( S _ { \\Omega } , r , K )$ mwinhiemrei $S _ { \\Omega } ^ { ( 0 ) } \\subseteq S _ { \\Omega }$ in Eq. 3 $f _ { n n } ( \\lambda ; \\Theta )$ $L _ { | S _ { \\Omega } ^ { ( 0 ) } | }$ 3: for $k = 0 , 1 , \\ldots , m _ { c } - 1$ do 4: Compute r(ek)sidual l = {|fnn(λ(i); Θ(k))−y(i)|}|iS=(Ω1k)| 5: Stratify into 3 subdomains based on residual using Gaussian Mixture: $\\{ S _ { \\Omega _ { l r } } ^ { ( k ) } , S _ { \\Omega _ { m r } } ^ { ( k ) } , S _ { \\Omega _ { h r } } ^ { ( k ) } \\}$ 6: $S _ { \\Omega _ { g s } } ^ { ( k + 1 ) } \\gets M G S ( n _ { v 1 } , n _ { v 2 } , S _ { \\Omega _ { m r } } ^ { ( k ) } , S _ { \\Omega _ { h r } } ^ { ( k ) } )$ 7: $S _ { \\Omega } ^ { ( k + 1 ) } \\gets S _ { \\Omega } ^ { ( k ) } \\cup S _ { \\Omega _ { g s } } ^ { ( k + 1 ) }$ // Update datasets 8: Update $f _ { n n } ( \\lambda ; \\Theta )$ by minimizing $L _ { | S _ { \\Omega } ^ { ( k + 1 ) } | }$ in Eq. 3 9: end for   \n10: return $f _ { n n } ( \\lambda ; \\Theta ^ { * } )$\n\n# 5 Experiments\n\nTo validate the proposed HGGS, we conducted experiments on four biological systems known for their oscillatory behaviors. We also performed an ablation study to assess Gradient-based Filtering and Multigrid Genetic Sampling, and examined the sensitivity of model hyperparameters.\n\n# Biological System Dataset\n\nBenchmark datasets from four biological systems were used for method evaluation: the Brusselator system (Prigogine 1978), the Cell Cycle system (Liu et al. 2012), the Mitotic Promoting Factor (MPF) system (Novak and Tyson 1993), and the Activator Inhibitor system (Murray 2002). For each system, we generated $2 0 \\mathrm { k } { - } 7 0 \\mathrm { k }$ sets of system coefficients using LHS, ran simulations to produce system dynamics, and determined the oscillatory frequency using (Apicella et al. 2013). This data was then used for training and testing of our method. Descriptions of the four biological systems, their corresponding ODEs, and detailed simulation settings are provided in the Appendix.\n\n# Baselines\n\nThe proposed HGGS was compared with seven baselines:\n\n• Latin Hypercube Sampling (LHS) (Stein 1987): Divides the coefficient domain into equal grids and randomly samples from each grid to ensure full coverage. • Importance Sampling (IS) (Lu et al. 2023): Resamples based on residual information, updating the training set after each epoch. $\\mathrm { I S ^ { \\dagger } }$ is an optimization that updates only when the model shows no improvement. • Uncertainty Sampling (US) (Liu and Li 2023): Selects the top- $\\cdot o$ new samples from a candidate set based on residual distribution, and adds them to the training set.\n\nWe implemented the candidate set in two ways: poolbased (US-P) and streaming-based (US-S). • Weight Reservoir Sampling (WRS) (Efraimidis and Spirakis 2006): Selects the top- $\\cdot o$ new samples from the candidate set based on residuals to replace part of the current data, keeping a constant training sample size. • Volume Sampling for Streaming Active Learning (VeSS) (Saran et al. 2023): Selects samples based on their gradient distribution relative to the model, which is a type of diversity-based sampling in AL.\n\n# Implementation\n\nOur algorithm was implemented using the PyTorch framework on a single NVIDIA A6000 GPU. We utilized $N =$ 10k samples for initial training and 5k samples for validation for each experiment. For a thorough evaluation, our testing data consists of four subsets, characterizing different types of the coefficient domain: overall (entire testing data), majority (non-oscillatory samples only), minority (oscillatory samples only), and boundary (top $20 \\%$ samples ranked by gradient using Eq. 4). The total size of the testing data varies between $7 \\mathrm { k } \\mathrm { - } 6 0 \\mathrm { k }$ , depending on the oscillatory systems.\n\nThe neural network (Multi-Layer Perceptron), consisting of 3 or 4 hidden layers, was trained for $3 \\mathrm { k }$ epochs per sampling cycle using the Adam optimizer with a learning rate of $\\bar { 2 } \\substack { - 2 . 5 \\times 1 0 ^ { - 3 } }$ , employing full batch training and early stopping. For key hyperparameters, the GF filtering ratio was set to $r = 2 0 \\%$ , with $K = 5$ nearest neighbors and a GF sample size of $n _ { f } = N / 2$ . During the sampling cycles, the MGS ratio was set to $n _ { v 1 } : n _ { v 2 } = 6 : 4$ , with an MGS sample size of $n _ { s } = N / 2$ . Implementation details and other parameters for each experiment can be found in the Appendix. All reported results below were based on five independent experiments.\n\n# Results\n\nMetrics. Root Mean Square Error (RMSE) is our primary metric. Imbalance Ratio (IR) and Gini Index (GI) quantify the proportion of non-oscillatory to oscillatory samples and the diversity of oscillatory frequency labels, respectively. Lower IR and GI indicate more effective handling of nonoscillatory bias and oscillatory boundary sensitivity by the sampling method. RMSE, IR, and GI are defined below.\n\n$$\n\\mathrm { R M S E } = \\sqrt { \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\left. f _ { n n } ( \\mathbf { \\lambda } \\mathbf { \\lambda } ^ { ( i ) } ; \\boldsymbol { \\Theta } ) - y ^ { ( i ) } \\right. _ { 2 } ^ { 2 } }\n$$\n\n$$\n\\mathrm { I R } = \\frac { \\sum _ { i = 1 } ^ { N } I ( y ^ { ( i ) } = 0 ) } { \\sum _ { i = 1 } ^ { N } I ( y ^ { ( i ) } \\neq 0 ) } ; \\mathrm { G I } = \\frac { \\sum _ { i = 1 } ^ { N } \\sum _ { j = 1 } ^ { N } \\left| y ^ { ( i ) } - y ^ { ( j ) } \\right| } { 2 N \\sum _ { i = 1 } ^ { N } y ^ { ( i ) } }\n$$\n\nBrusselator. In Fig. 3(a), our method achieves the lowest RMSE for the overall testing data (0.0085). Notably, for crucial minority and boundary cases, we observe $2 4 \\% - 7 1 \\%$ and $1 3 \\% { - } 6 6 \\%$ improvement over the baseline approaches. In this low-dimensional, densely-packed space, $\\mathrm { \\dot { I } S ^ { \\dagger } }$ (minority: 0.0119; boundary: 0.0320) performs well but lacks sufficient boundary information. HGGS, however, effectively targets more boundary instances, particularly in highresidual boundary regions, as illustrated in Appendix. Due to\n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">Brusselator System</td><td colspan=\"2\">Cell Cycle System</td><td colspan=\"2\">MPF System</td><td colspan=\"2\">ActivatorInhibitor System</td></tr><tr><td>IR</td><td>GI</td><td>IR</td><td>GI</td><td>IR</td><td>GI</td><td>IR</td><td>GI</td></tr><tr><td>LHS</td><td>1.47±0.00</td><td>0.76±0.00</td><td>4.43±0.00</td><td>0.85±0.00</td><td>5.27±0.00</td><td>0.89±0.00</td><td>11.84±0.00</td><td>0.94±0.00</td></tr><tr><td>WRS</td><td>1.37±0.05</td><td>0.75±0.00</td><td>4.41±0.12</td><td>0.85±0.00</td><td>5.16±0.18</td><td>0.89±0.00</td><td>11.32±0.48</td><td>0.94±0.00</td></tr><tr><td>VeSS</td><td>1.42±0.12</td><td>0.75±0.01</td><td>3.98±0.16</td><td>0.84±0.01</td><td>3.57±0.37</td><td>0.85±0.01</td><td>10.96±0.56</td><td>0.94±0.00</td></tr><tr><td>IS</td><td>1.35±0.23</td><td>0.77±0.03</td><td>1.86±0.36</td><td>0.70±0.04</td><td>3.07±0.44</td><td>0.82±0.02</td><td>6.04±1.15</td><td>0.88±0.02</td></tr><tr><td>ISt</td><td>1.20±0.15</td><td>0.75±0.02</td><td>2.00±0.44</td><td>0.71±0.04</td><td>3.11±0.78</td><td>0.82±0.03</td><td>6.09±2.07</td><td>0.88±0.04</td></tr><tr><td>US-S</td><td>1.18±0.11</td><td>0.72±0.01</td><td>3.93±1.53</td><td>0.83±0.05</td><td>3.75±0.37</td><td>0.85±0.01</td><td>11.58±2.90</td><td>0.94±0.02</td></tr><tr><td>US-P</td><td>1.08±0.17</td><td>0.70±0.02</td><td>2.3±0.22</td><td>0.76±0.02</td><td>2.75±0.41</td><td>0.81±0.02</td><td>7.12±0.24</td><td>0.91±0.00</td></tr><tr><td>Ours</td><td>1.18±0.12</td><td>0.71±0.03</td><td>1.10±0.02</td><td>0.58±0.01</td><td>1.11±0.07</td><td>0.65±0.01</td><td>2.88±0.13</td><td>0.77±0.01</td></tr></table></body></html>\n\nTable 1: Imbalance Ratio (IR) and Gini Index (GI) for seven baseline methods and our HGGS across four biological systems HGGS achieves the lowest IR and GI in most cases.\n\n![](images/607dad44383ba453bc9371450a6350162aab048851d01b6748724678725fcd69.jpg)  \nFigure 3: Accuracy comparison of seven baseline methods (LHS, WRS, VeSS, IS, $\\mathrm { I S ^ { \\dagger } }$ , US-S, US-P) and our HGGS across fou biological systems. HGGS obtained the lowest RMSEs across all testing subsets: minority, boundary, majority, and overall.\n\nthe relatively mild data imbalance in the Brusselator system, the differences in IR and GI between our method, US-S, and US-P are minimal. Nonetheless, HGGS efficiently samples at the boundaries and improves prediction accuracy compared to the others.\n\nCell Cycle. In this complex cell cycle system, our method distinguishes itself with the lowest IR and GI in Table 1, highlighting its dual strength in balancing data distribution and enriching sample diversity. HGGS also obtains the lowest RMSE error (overall: 0.0098), as shown in Fig. 3(b).\n\nSpecifically for minority and boundary cases, our method shows improvements of $8 \\% { - } 2 8 \\%$ and $8 \\% - 2 4 \\%$ over the other seven baselines. US-P (minority: 0.0169; boundary: 0.0209) includes high-residual samples but suffers from redundancy due to random sampling. In contrast, our method directly targets high-risk areas, avoiding this redundancy and demonstrating superior efficiency and effectiveness.\n\nMPF. Fig. 3(c) shows that our method excels in achieving minimal RMSE (overall: 0.0101), a decrease of $7 \\% - 2 7 \\%$ compared to other methods. It also stands out for minority\n\nMajority (RMSE) Majority Minority Minority (RMSE)   \n14.0 ×10-3 ×10-3 11.0 ×10-3 ×10-3 19.0 20.0 17.0   \n12.0 9.0 10.0   \n10.0 15.0 8.0 0.0 7.0 13.0 LHS GF MGS HGGS LHS GF MGS HGGS (a) Brusselator System (b) Cell Cycle System   \n10.5 ×10-3 ×10-3 20.0 6.0 ×10-3 ×10-3 13.0   \n9.5 5.0 18.0 11.0   \n8.5 4.0   \n7.5 16.0 3.0 9.0 LHS GF MGS HGGS LHS GF MGS HGGS (c) MPF System (d) Activator Inhibitor System Boundary Majority Minority   \n29.0 27.0 21.3 18.5 ×10-3 ×10-3 20.7 10 ×10-3 22.0 17.9 20.1   \n21.0 17.019.5 0 3 6 9 12 Only Mutation Only Crossover (a) K (b) 𝑛𝑣1: 𝑛𝑣2   \nMajority (RMSE) ×10-3 ×10-3 21.0 ×10-3 ×10-3 9.5 9.5 8.5 17.0 8.5 1 7.5 13.0 7.5 0 210 211 212 213 5% 20% 40% 50% (c) 𝑛𝑠 (d) r\n\nand boundary cases (minority: 0.0174; boundary: 0.0200). VeSS performs adequately for minority and boundary cases, but its weaker performance in the majority case reduces its overall effectiveness. Table 1 further underscores the effectiveness of HGGS in mitigating non-oscillatory bias in the MPF system. Our method achieves the lowest GI among all baselines, reflecting its superior ability to uncover informative oscillatory patterns and enhance diversity.\n\nActivator Inhibitor. The Activator Inhibitor system is characterized by a pronounced data imbalance, with a staggering IR of 11.84 under LHS. However, as highlighted in Table 1, our method exhibits remarkable resilience against such extreme scenarios, effectively reducing the IR to about 2.88 while simultaneously enhancing data diversity to a GI of 0.77. Moreover, HGGS has the lowest error across all categories (minority: 0.0099; boundary: 0.0130; majority: 0.0036; overall: 0.0044) (Fig. 3(d)). It particularly excels in minority and boundary cases, showing improvements of $1 8 \\% { - } 3 1 \\%$ and $1 6 \\% - 2 8 \\%$ over other baselines.\n\n# Ablation Study\n\nWe conducted an ablation study on four biological systems to evaluate the efficacy of the two layers: Gradient-based Filtering (GF) and Multigrid Genetic Sampling (MGS).\n\nEffect of Gradient-based Filtering. Relying solely on GF followed by random sampling does not effectively address non-oscillatory bias and oscillatory boundary sensitivity. Although GF offers initial benefits, subsequent random sampling fails to sustain improvements. As shown in Fig. 4, GF yields higher RMSEs for majority and minority cases across four systems than HGGS, indicating that MGS produces more informative samples than random sampling.\n\nEffect of Multigrid Genetic Sampling. When MGS is used without prior GF, it struggles to precisely identify informative minority samples, leading to higher RMSEs for minority class across all four systems compared to HGGS (Fig. 4). This suggests that GF provides critical guidance for MGS to effectively target and refine minority samples.\n\nThese findings underscore the synergy between GF and MGS within HGGS. GF enables an efficient extraction of sample domains, identifying coarse boundaries rich in critical information. Informed by GF, MGS operates with greater precision to refine minority samples and explore new highrisk areas. Together, they enhance data representativeness and diversity, improving model performance.\n\n# Sensitivity Analysis\n\nFig. 5(a-c) illustrates the effects of key model parameters, including the number of neighbors $K$ , MGS ratio $n _ { v 1 } : n _ { v 2 }$ , and MGS sampling size $n _ { s }$ on the MPF system. In Fig. 5(a), varying $K$ produces an elbow curve, with $K \\ : = \\ : 5$ offering optimal performance. Increasing $K$ beyond this point has little effect on gradient estimation, while decreasing $K$ leads to significant errors in oscillatory frequency estimation. In Fig. 5(b), MGS cannot achieve optimal performance using only crossover or mutation operation. Our experiments found that an MGS ratio of 6:4 yielded the lowest RMSE. In Fig. 5(c), increasing the MGS sampling size $n _ { s }$ continues to reduce RMSE for the minority case, while the majority case shows no further improvement beyond $2 \\mathrm { k }$ new samples. Fig. 5(d) shows the effect of GF filtering ratio on the Cell Cycle system. Increasing $r$ reduces the sampling of majority (non-oscillation) class, leading to higher error, while the opposite is true for minority class. A ratio of $r = 2 0 \\%$ offers the best balance between majority and minority samples.\n\n# Conclusion\n\nThis paper introduces Hierarchical Gradient-based Genetic Sampling, a two-layer framework designed to address nonoscillatory bias and boundary sensitivity in predicting biological oscillations. The first layer, gradient-based filtering, selects a representative subset from initial random sampling, creating a balanced coarse dataset. The second layer, genetic sampling, refines minority instances and explores new highresidual information, enhancing data diversity. Experiments show that HGGS achieves the best accuracy across four biological systems, particularly for the oscillation class.",
    "institutions": [
        "College of Computer Science and Engineering, Northeastern University",
        "Department of Computer Science, Wake Forest University",
        "Department of Computer Science, Virginia Polytechnic Institute and State University"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n在生物振荡建模中，使用神经网络对系统系数和振荡频率关系进行建模时，系数空间中振荡稀缺，导致大量样本呈现非周期性行为。同时，振荡边界附近系数的微小变化会显著改变振荡特性，产生非振荡偏差和边界敏感性问题，使得准确预测变得困难。理解生物振荡对于生物系统维持稳定、响应外部刺激和调节复杂过程至关重要，因此解决这些问题具有重要意义。此外，现有采样方法如重要性采样、不确定性采样等，要么无法解决数据敏感性问题，要么会导致冗余采样，难以有效应对这些挑战。\\n\\n**方法概述**\\n论文提出了分层梯度遗传采样（Hierarchical Gradient-based Genetic Sampling, HGGS）框架，该框架分为两层。第一层是基于梯度的过滤（Gradient-based Filtering），用于提取敏感振荡边界并去除冗余非振荡样本，创建平衡的粗数据集；第二层是多网格遗传采样（Multigrid Genetic Sampling），利用残差信息细化边界并探索新高残差区域，增加用于模型训练的数据多样性。\\n\\n**主要贡献与效果**\\n- 提出基于梯度的过滤技术，能提取振荡边界并去除冗余非振荡样本，生成平衡粗数据，使多网格遗传采样能更高效地细化数据。在四个生物系统实验中，HGGS在多个测试子集（少数类、边界、多数类和总体）上均取得最低均方根误差（RMSE）。如在Brusselator系统中，总体测试数据RMSE为0.0085，少数类和边界情况比基线方法提高24% - 71%和13% - 66%。\\n- 多网格遗传采样策略利用残差信息细化现有边界并探索新高残差区域，提高振荡多样性。在Cell Cycle系统中，HGGS的不平衡比（IR）为1.10±0.02，基尼指数（GI）为0.58±0.01，均为最低。\\n- HGGS方法在训练过程中不断适应误差情况，确保数据集具有代表性，在四个生物系统中表现出优于七个基线方法的准确性，尤其在振荡类预测中效果显著。实验表明，HGGS在大多数情况下取得了最低的IR和GI，有效平衡了数据分布并丰富了样本多样性。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nHGGS的核心思想是通过分层采样来解决生物振荡预测中的非振荡偏差和边界敏感性问题。基于梯度的过滤层通过评估样本的梯度程度，识别敏感区域，去除冗余的非振荡样本，生成平衡的粗数据集。多网格遗传采样层则基于残差分布构建多个采样网格，在网格内进行有效采样，细化边界并探索高残差区域，从而提高数据的多样性和模型的准确性。通过这种分层的方式，HGGS能够充分利用数据中的信息，逐步优化样本集，提升模型性能。\\n\\n**创新点**\\n现有采样方法如重要性采样、合成少数过采样技术和不确定性采样等，要么无法解决数据敏感性问题，要么会引入冗余样本，降低采样效率。许多现有动态采样方法主要针对名义目标变量，很少适用于连续目标的回归问题。HGGS方法利用残差信息进行有针对性的采样，避免了冗余，提高了采样的有效性和效率，且适用于回归问题。它通过构建高残差分布和进行多网格采样，能够更精准地捕捉数据中的关键信息，提升模型的预测能力。\\n\\n**具体实现步骤**\\n1. **初始化**：使用拉丁超立方采样（Latin Hypercube Sampling, LHS）获取初始样本集 $S_{\\\\Omega} = \\\\{ (\\\\lambda^{(i)}, y^{(i)}) \\\\}_{i = 1}^{N}$。\\n2. **基于梯度的过滤（Gradient-based Filtering）**：\\n    - 计算样本的梯度度 $gd(\\\\lambda^{(i)})_{S_{\\\\Omega}} = \\\\frac{1}{K} \\\\sum_{j = 1}^{K} \\\\frac{\\\\| y^{(j)} - y^{(i)} \\\\|_{2}^{2}}{\\\\| \\\\lambda^{(j)} - \\\\lambda^{(i)} \\\\|_{2}^{2}}$，其中 $K$ 是超参数，表示训练数据集中与样本 $i$ 最近的 $K$ 个样本。\\n    - 根据梯度排名，过滤出前 $r\\\\%$ 的样本 $S_{\\\\Omega_{gf1}}$。\\n    - 从剩余样本中使用不确定性采样保留一个全局数据集 $S_{\\\\Omega_{gf2}}$，该子集用于捕捉总体特征并维持非振荡区域的性能。\\n    - 形成粗训练集 $S_{\\\\Omega}^{(0)} = S_{\\\\Omega_{gf}} = S_{\\\\Omega_{gf1}} \\\\cup S_{\\\\Omega_{gf2}}$，此集合去除了原数据集中的大部分冗余非振荡样本，同时保留了多样化的振荡数据。\\n3. **多网格遗传采样（Multigrid Genetic Sampling）**：\\n    - 使用高斯混合模型对剩余样本集 $S_{\\\\Omega}$ 中的残差分布进行分层，分为低、中、高残差分布 $S_{\\\\Omega_{lr}}$、$S_{\\\\Omega_{mr}}$ 和 $S_{\\\\Omega_{hr}}$。\\n    - 构建多个不同大小的采样网格，每个网格由 $S_{\\\\Omega}^{(0)} - S_{\\\\Omega_{lr}}^{(0)}$ 中的点界定，以捕捉高残差分布。\\n    - 在每个超立方网格内根据公式 $\\\\lambda_{new} = \\\\alpha \\\\odot (\\\\lambda^{(i)} - \\\\lambda^{(j)}) + \\\\lambda^{(j)}$ 采样新点，其中 $\\\\alpha$ 是 $D$ 维权重向量，每个值在 [0, 1] 范围内。\\n    - 在第 $(k + 1)$ 次采样时，利用当前第 $k$ 次的残差样本，定义交叉和变异操作：\\n        - 多网格交叉：从 $S_{\\\\Omega_{hr}}^{(k)}$ 中随机选择两个不同样本 $\\\\lambda_{hr}^{(i)}$ 和 $\\\\lambda_{hr}^{(j)}$，通过上述公式采样新点 $\\\\lambda_{hh}$ 并添加到样本集 $S_{\\\\Omega}^{(k + 1)}$ 中，用于挖掘和细化现有高残差区域；从 $S_{\\\\Omega_{hr}}^{(k)}$ 和 $S_{\\\\Omega_{mr}}^{(k)}$ 中分别选择样本 $\\\\lambda_{hr}^{(i)}$ 和 $\\\\lambda_{mr}^{(j)}$，形成超立方网格并采样新点 $\\\\lambda_{hm}$ 添加到样本集 $S_{\\\\Omega}^{(k + 1)}$ 中，以探索全局边界和新的高残差区域。\\n    - 确定采样数量 $n_{v1}$ 和 $n_{v2}$，使得 $| S_{\\\\Omega_{gs}}^{(k + 1)} | = n_{v1} + n_{v2}$，并更新样本集 $S_{\\\\Omega}^{(k + 1)} = S_{\\\\Omega_{gs}}^{(k + 1)} \\\\cup S_{\\\\Omega}^{(k)}$。\\n    - 更新训练数据集 $S_{\\\\Omega}^{(k + 1)} = S_{\\\\Omega}^{(k)} \\\\cup S_{\\\\Omega_{gs}}^{(k + 1)}$，并更新神经网络模型 $f_{nn}(\\\\lambda; \\\\Theta)$，通过最小化损失函数 $L_{| S_{\\\\Omega}^{(k + 1)} |}$ 来优化模型参数。\\n4. **循环执行**：重复步骤 3，直到达到指定的采样周期 $m_{c}$。\\n5. **输出**：返回目标模型 $f_{nn}(\\\\lambda; \\\\Theta^{*})$。\\n\\n**案例解析**\\n论文通过对四个生物系统（Brusselator系统、Cell Cycle系统、Mitotic Promoting Factor (MPF) 系统和Activator Inhibitor系统）的实验，详细展示了HGGS的应用和效果。例如，在Brusselator系统中，HGGS在边界处有效采样，提高了预测准确性；在Cell Cycle系统中，HGGS平衡了数据分布并丰富了样本多样性，取得了最低的RMSE、IR和GI。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 拉丁超立方采样 (Latin Hypercube Sampling, LHS)\\n- 重要性采样 (Importance Sampling, IS)，$IS^{\\\\dagger}$ 是其优化版本\\n- 不确定性采样 (Uncertainty Sampling, US)，包括基于池的 (US-P) 和基于流的 (US-S) 两种实现方式\\n- 权重水库采样 (Weight Reservoir Sampling, WRS)\\n- 用于流主动学习的体积采样 (Volume Sampling for Streaming Active Learning, VeSS)\\n\\n**性能对比**\\n*   **在 [均方根误差/RMSE] 指标上：** 本文方法HGGS在四个生物系统（Brusselator、Cell Cycle、MPF和Activator Inhibitor）的所有测试子集（少数类、边界、多数类和总体）上均获得了最低的RMSE。在Brusselator系统中，HGGS总体测试数据的RMSE为 **0.0085**，显著优于基线模型LHS、WRS、VeSS、IS、$IS^{\\\\dagger}$、US-S和US-P，在少数类和边界情况下，比基线方法提高了24% - 71%和13% - 66%；在Cell Cycle系统中，HGGS总体RMSE为 **0.0098**，少数类和边界情况分别比其他七种基线方法提高了8% - 28%和8% - 24%；在MPF系统中，HGGS总体RMSE为 **0.0101**，比其他方法降低了7% - 27%；在Activator Inhibitor系统中，HGGS少数类RMSE为 **0.0099**，边界RMSE为 **0.0130**，多数类RMSE为 **0.0036**，总体RMSE为 **0.0044**，少数类和边界情况分别比其他基线方法提高了18% - 31%和16% - 28%。\\n*   **在 [不平衡比/IR] 指标上：** 在四个生物系统中，HGGS在大多数情况下都取得了最低的IR。例如，在Cell Cycle系统中，HGGS的IR为 **1.10±0.02**，远低于LHS的 **4.43±0.00**；在MPF系统中，HGGS的IR为 **1.11±0.07**，也明显低于其他基线方法。\\n*   **在 [基尼指数/GI] 指标上：** 在四个生物系统中，HGGS在大多数情况下都取得了最低的GI。例如，在Cell Cycle系统中，HGGS的GI为 **0.58±0.01**，表明其在丰富样本多样性方面表现出色，显著优于其他基线方法；在MPF系统中，HGGS的GI为 **0.65±0.01**，也优于其他基线方法。\",\n    \"keywords\": \"### 关键词\\n\\n- 生物振荡预测 (Biological Oscillation Prediction, N/A)\\n- 分层梯度遗传采样 (Hierarchical Gradient-based Genetic Sampling, HGGS)\\n- 基于梯度的过滤 (Gradient-based Filtering, GF)\\n- 多网格遗传采样 (Multigrid Genetic Sampling, MGS)\\n- 非振荡偏差 (Non-oscillatory Bias, N/A)\\n- 边界敏感性 (Boundary Sensitivity, N/A)\\n- 神经网络 (Neural Networks, NNs)\\n- 生物系统 (Biological Systems, N/A)\"\n}"
}