{
    "link": "https://arxiv.org/abs/2502.05275",
    "pdf_link": "https://arxiv.org/pdf/2502.05275",
    "title": "Interpretable Failure Detection with Human-Level Concepts",
    "authors": [
        "Kien X. Nguyen",
        "Tang Li",
        "Xi Peng"
    ],
    "publication_date": "2025-02-07",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 1,
    "paper_content": "# Interpretable Failure Detection with Human-Level Concepts\n\nKien X. Nguyen, Tang Li, Xi Peng\n\nDepartment of Computer and Information Sciences University of Delaware Newark, DE, USA kxnguyen, tangli, xipeng @udel.edu\n\n# Abstract\n\nReliable failure detection holds paramount importance in safety-critical applications. Yet, neural networks are known to produce overconfident predictions for misclassified samples. As a result, it remains a problematic matter as existing confidence score functions rely on category-level signals, the logits, to detect failures. This research introduces an innovative strategy, leveraging human-level concepts for a dual purpose: to reliably detect when a model fails and to transparently interpret why. By integrating a nuanced array of signals for each category, our method enables a finer-grained assessment of the model’s confidence. We present a simple yet highly effective approach based on the ordinal ranking of concept activation to the input image. Without bells and whistles, our method significantly reduce the false positive rate across diverse real-world image classification benchmarks, specifically by $3 . 7 \\%$ on ImageNet and $9 \\%$ on EuroSAT.\n\n# Introduction\n\nVision-language models have demonstrated impressive capability across diverse visual recognition domains (Radford et al. 2021; Jia et al. 2021; Singh et al. 2021; Li et al. 2022, 2023). However, when it comes to safe deployment in highstake applications, it is of paramount importance for a model to be self-aware of its own shortcomings. For instance, in monitoring for natural disasters such as floods or wildfires, the AI system must signal for human intervention upon encountering scenarios where its confidence is low. Such selfawareness ensures that preemptive measures can be taken to mitigate disaster impacts on communities and ecosystems. Therefore, it is imperative not only to detect failures accurately but also to understand the reasons behind them.\n\nTraditional methods (Hendrycks and Gimpel 2016; Granese et al. 2021; Zhu et al. 2023a,b; Liang, Li, and Srikant 2018) rely on category-level information to detect misclassifications, performing confidence estimation on the class logits. However, neural networks are known to produce overconfident predictions for misclassified samples due to factors like spurious correlations (Arjovsky et al. 2019; Sagawa et al. 2019), thus existing confidence scoring functions (CSFs) fall short in such cases. Besides, the model confidence depicted through category-level information impedes the ability for humans to interpret why it fails. To this end, we ask the following question: “What other sources of information can we leverage to enhance failure detection?”\n\nWe present a novel perspective on detecting failures by leveraging human-level concepts, or visual attributes. With the flexibility to incorporate free-form language to VLMs (i.e. CLIP), we can represent a category with a set of predefined concepts (Menon and Vondrick 2023; Oikarinen et al. 2023; Li, Ma, and Peng 2024a,b). Instead of only prompting the model “Do you recognize a camel?”, we collectively ask “Do you recognize humps on back?”, or “Do you recognize shaggy coat?”. The purpose is to measure the model’s confidence in the object’s detailed visual attributes in addition to the holistic category. We thus achieve a more accurate confidence estimate to detect failures more effectively (Fig. 1).\n\nIdeally, a VLM that can recognize a image of a camel should also recognize all the associated visual attributes, such as humps on back, shaggy coat, etc. Such visual attributes should yield higher confidence scores compared to those associated with the absent categories. Conversely, if the model shows high confidence in concepts from multiple unrelated categories at the same time, it could indicate a failure in its recognition process. Based on such intuition, we present a simple but effective approach using the Ordinal Ranking of Concept Activation (ORCA) to detect failures. Additionally, these human-understandable concepts allow users to understand the reasons behind such failures, thereby aiding them in refining the training process.\n\nWe rigorously validate our method’s efficacy in detecting incorrect samples across both natural and remote sensing image benchmarks, which mirror the complexity in realworld scenarios. ORCA demonstrates a significant capability to mitigate the issue of overly confident misclassifications. In summary, our contributions are threefold:\n\n1. We leverage human-level concepts to detect when and interpret why a model fails using vision-language models.   \n2. We present a simple but effective approach, called ORCA, to estimate more reliable confidence via the ordinal ranking of the concepts’ activation.   \n3. We empirically demonstrate that the concept-based methods enhance failure prediction performance across a wide range of classification benchmarks.\n\nStandard (category-level) Ours (concept-level) camel 0.15 humps on back 30.0 elephant 0.05 shaggy coat 28.1 giraffe 0.80 long straight neck 27.3 cushioned feet 26.4 camel 0.47 Estimate patched coat 25.8 elephant 0.10 Label: camel giraffe 0.43 Interpret long trunk 24.4 Class probability CLIP similarity scores\n\n# Related Work\n\nFailure Detection. Failure detection, or misclassification detection, is a burgeoning area of research within the realm of artificial intelligence. Detecting when machine learning models produce incorrect or misleading predictions has significant implications for safety, reliability, and transparency in various domains. Existing research in this field falls into two main categories: (1) retraining or fine-tuning of neural networks (Moon et al. 2020; Zhu et al. 2023a,b), and (2) the design of novel confidence score functions (Granese et al. 2021; Hendrycks and Gimpel 2016). The former approach involves retraining or fine-tuning neural networks with specific objectives aimed at improving the model’s capability to recognize its own failures. Zhu et al. (Zhu et al. 2023b) employs a training objective that seeks flat minima to mitigate overconfident predictions. While these approaches have shown promise, they often require extensive computational resources and access to the entire model, which may not be feasible for large VLMs. Researchers have also turned their attention to the design of new CSFs (Granese et al. 2021). Despite these efforts, the most robust CSF remains the MSP (Jaeger et al. 2023). However, a downside of category-level CSFs is their inability to detect overconfident but incorrect predictions, which is problematic. In this work, we deconstruct category-level into concept-level signals to achieve a more nuanced estimate of the model’s confidence.\n\nA closely related sub-field is confidence calibration (Minderer et al. 2021; LeVine et al. 2023; Mukhoti et al. 2020; Pereyra et al. 2017), where the goal is to adjust a model’s predicted probabilities to ensure that they accurately reflect the true likelihood of those predictions being correct. However, Zhu et al. (Zhu et al. 2023b) has empirically shown that calibration methods frequently yield no benefits or even detrimentally affect failure prediction. Similarly, some works (Jaeger et al. 2023; Bernhardt, Ribeiro, and Glocker 2022) also emphasizes the importance of confidence ranking over confidence calibration in failure detection. Some other related sub-fields are predictive uncertainty estimation (Gal and Ghahramani 2016; Blundell et al. 2015; Lakshminarayanan, Pritzel, and Blundell 2017; Mukhoti et al. 2023), out-of-distribution detection (Zhu et al. $2 0 2 3 \\mathrm { a }$ ; Liang, Li, and Srikant 2018; Dinari and Freifeld 2022; Lee et al.\n\n2018), open-set recognition (Vaze et al. 2022; Geng, Huang, and Chen 2021) and selective classification (Geifman and El-Yaniv 2017; Fisch, Jaakkola, and Barzilay 2022).\n\nHuman-level Concepts in Vision-Language Models. Concept-based models (CBMs) aim to open the black box of neural networks. Concept bottleneck networks are pioneers for interpretable neural networks, with each neuron in the concept bottleneck layer representing a concept at the human level (Koh et al. 2020; Yuksekgonul, Wang, and Zou 2022). With the flexibility to employ free language in vision language models, such as CLIP (Radford et al. 2021), ALIGN (Jia et al. 2021), FLAVA (Singh et al. 2021), and BLIP (Li et al. 2022, 2023), concepts of human level can be naturally integrated into the prediction mechanism (Menon and Vondrick 2023; Yang et al. 2022; Oikarinen et al. 2023). This work can be viewed as a variant of CBMs for failure detection, which has never been considered before. We show the approach better predicts failures and, as a byproduct, helps interpret why a model fails.\n\n# Backgrounds\n\nOverview on Failure Detection. We consider failure detection on the multi-class classification task. Let $\\boldsymbol { \\mathcal { X } } \\in \\mathbb { R } ^ { d }$ be the input space and $\\mathcal { Y } = \\{ 1 , 2 , \\dots , C \\}$ be the label space, where $d$ is the dimension of the input vector. Given a data set $\\{ ( \\mathbf { x } _ { i } , y _ { i } ) \\} _ { i = 1 } ^ { N }$ with $N$ data points independently sampled from the joint probability distribution $\\mathcal { X } \\times \\mathcal { Y }$ , a standard neural network $f : \\mathcal { X }  \\mathcal { Y }$ outputs a probability distribution over the $C$ categories. For an input x, $f$ outputs $\\hat { \\pmb { p } } = \\hat { P } ( y | \\mathbf { x } ; \\theta )$ as the class probabilities, where $\\theta$ denotes the network’s parameters. In the context of failure detection, we consider a pair of functions $( f , g )$ , where $g : \\mathcal { F } \\times \\mathcal { X }  \\mathbb { R }$ is the confidence scoring function, and $f \\in { \\mathcal { F } }$ . With a predefined threshold $\\tau \\in \\mathsf { R } ^ { + }$ , the failure detection output is defined as:\n\n$$\n( f , g ) ( \\mathbf { x } ) = \\left\\{ \\begin{array} { l l } { \\hat { P } ( y | \\mathbf { x } ; \\theta ) , } & { \\mathrm { i f ~ } g ( f , \\mathbf { x } ) \\geq \\tau } \\\\ { \\mathrm { d e t e c t , } } & { \\mathrm { o t h e r w i s e . } } \\end{array} \\right.\n$$\n\nFailure detection is initiated when $g ( f , \\mathbf { x } )$ falls below a threshold $\\tau$ . Ideally, a confidence scoring function should output higher confidence scores for correct predictions and lower confidence scores for incorrect predictions. Despite efforts in designing CSFs, Jaeger et al. (Jaeger et al. 2023) has shown that the standard Maximum Softmax Prediction remains the best CSF across a wide range of datasets and network architectures. Mathematically, MSP is defined as:\n\n$$\ng ( f , \\mathbf { x } ) = \\operatorname* { m a x } _ { c \\in \\mathcal { V } } \\hat { P } ( y = c | \\mathbf { x } ; \\theta )\n$$\n\nwhich returns the maximum output signal after the softmax activation function on the network output layer.\n\nFailure Detection with VLM. CLIP (Radford et al. 2021), a vision-language model, is pre-trained on a large-scale dataset comprising of 400 million image-text pairs. CLIP uses contrastive learning to align the image and text pairs. During inference, we calculate the model’s logits as the cosine similarity score between the input image embedding and the corresponding text embeddings. Given an input imadigtei $\\mathbf { x }$ , eteextmlbaebdedlsinrgepirsesdentosttehdeacsa $\\bar { \\boldsymbol { f } } _ { \\mathrm { i m g } } ( \\mathbf { x } ) \\in \\mathbb { R } ^ { m }$ $C$ $\\{ \\mathbf { t } _ { c } \\} _ { c = 1 } ^ { C }$ where $f _ { \\mathrm { t x t } } ( \\mathbf { t } _ { c } ) \\in \\mathbb { R } ^ { m }$ is the embeddings and $m \\ll d$ . For each category, we calculate the corresponding logit as:\n\n$$\ns _ { c } = 1 0 0 \\times \\frac { f _ { \\mathrm { i m g } } ( \\mathbf { x } ) \\cdot f _ { \\mathrm { t x t } } ( \\mathbf { t } _ { c } ) } { \\left\\| f _ { \\mathrm { i m g } } ( \\mathbf { x } ) \\right\\| \\left\\| f _ { \\mathrm { t x t } } ( \\mathbf { t } _ { c } ) \\right\\| }\n$$\n\nwhere $\\left\\| \\cdot \\right\\|$ is the $L _ { 2 }$ norm. The softmax function then converts the logits into probabilities:\n\n$$\n\\hat { p } _ { c } = \\frac { \\exp ( s _ { c } ) } { \\sum _ { j = 1 } ^ { C } \\exp ( s _ { j } ) }\n$$\n\nwhere $\\hat { p } _ { c } \\in \\hat { p }$ . $f ( \\mathbf { x } ) = \\operatorname { a r g m a x } _ { c \\in \\mathcal { V } } \\hat { p } _ { c }$ is the prediction, and $g ( f , \\mathbf { x } ) = \\operatorname* { m a x } _ { c \\in \\mathcal { V } } \\hat { p } _ { c }$ can be regarded as the model confidence for a given input $\\mathbf { x }$ using MSP.\n\n# Methods\n\nTraditional methods rely on the category-level signals to estimate the model’s confidence. This leads to unreliable confidence estimate as neural networks are prone to overconfident misclassification. To address this issue, we suggest exposing the model to diverse viewpoints via human-level concepts. Rather than inquiring about the model’s certainty regarding an image being a camel, we also query its confidence regarding specific attributes like the presence of humps on the camel’s back, a shaggy coat, etc.\n\nRecent advancements in VLMs enable such integration of human-level concepts as free-form language into the pipeline (Menon and Vondrick 2023; Yang et al. 2022; Oikarinen et al. 2023). In this section, we describe the integration of the work by Menon and Vondrick (Menon and Vondrick 2023) which employs concept aggregation to establish a baseline concept-based method for failure detection. Subsequently, we introduce ORCA, our novel approach that captures the interaction among concept activations through ordinal ranking, enhancing the reliability of failure detection.\n\n# Human-Level Concepts for Failure Detection\n\nGiven $K$ concepts per category, we define $\\mathcal { A }$ as a collection of all concepts, where $| { \\mathcal A } | \\ = \\ C \\times K$ . We obtain the vector of similarity scores (or logits), $S _ { \\mathrm { c o n c } } =$ $\\left[ s _ { 1 , 1 } , \\dotsc , s _ { 1 , K } , s _ { 2 , 1 } , \\dotsc , s _ { C , K } \\right]$ , between the image embedding and all the concepts using Eq. 3. DescCLIP then calculates the mean similarity score among all concepts for each category $c$ to retrieve the logits and output the prediction:\n\n$$\nf ( \\mathbf { x } ) = \\operatorname { a r g m a x } _ { c \\in \\mathcal { V } } \\frac { 1 } { K } \\sum _ { k = 1 } ^ { K } s _ { c , k }\n$$\n\nFinally, we apply the softmax function (Eq. 4) on the logits to get the class probabilities and employ MSP to obtain the model’s confidence score.\n\n# Ordinal Ranking of Concept Activation\n\nDescCLIP’s concept aggregation leads to a coarse-grained confidence estimation procedure. We propose a fine-grained approach that models the interaction among concepts via ordinal ranking to estimate confidence more reliably.\n\nIdeally, if a model is confident about predicting a category $\\hat { c }$ then the concepts associated with $\\hat { c }$ should yield the strongest activations. In other words, the similarity scores of all concepts belonging to ${ \\hat { c } } _ { : }$ , $\\{ s _ { \\hat { c } , k } \\} _ { k = 1 } ^ { K }$ , should belong to the top- $K$ ranking. Conversely, we would see a mixture of concepts from different categories in the top- $K$ ranking if the model is likely to make an incorrect prediction. With such information, we can separate correct and incorrect predictions more reliably. Next, we describe two variants of our proposed method: baseline and rank-aware ORCA. In brevity, the former builds upon simple counting mechanisms, while the latter weighs the concept contributions to the confidence estimate based on their ranks.\n\nBaseline ORCA. We first sort $S _ { \\mathrm { c o n c } }$ in descending order and retrieve the set of the top- $K$ concepts, denoted as an ordered set $\\mathcal { A } _ { \\mathrm { t o p } - K }$ . After that, we derive the confidence based on the number of different categories whose concepts belong in $\\mathcal { A } _ { \\mathrm { t o p } - K }$ . The rationale is straightforward: the model is at a higher risk of failure as there are more categories featuring in $\\mathcal { A } _ { \\mathrm { t o p } - K }$ . The prediction is determined as follows:\n\n$$\nf ( \\mathbf { x } ) = \\operatorname { a r g m a x } _ { c \\in \\mathcal { V } } | \\mathcal { A } _ { \\mathrm { t o p } - K } \\cap \\mathcal { A } _ { c } | ,\n$$\n\nwhere $A _ { c }$ denotes the set of concepts of an arbitrary category $c$ ’s concepts, and $\\left| \\cdot \\right|$ denotes the set cardinality. The confidence of the prediction is the ratio between the number of the predicted category’s concepts appearing in $\\mathcal { A } _ { \\mathrm { t o p } - K }$ over $K$ :\n\n$$\ng ( f , \\mathbf { x } ) = \\frac { | \\mathcal { A } _ { \\mathrm { t o p } - K } \\cap \\mathcal { A } _ { \\hat { c } } | } { K }\n$$\n\nwhere $\\begin{array} { r } { \\hat { c } { \\bf \\Xi } = f ( { \\bf x } ) } \\end{array}$ is the prediction. We dub this variant ORCA-B in the text.\n\nRank-aware ORCA. While ORCA-B provides a fundamental approach, its reliance solely on rudimentary counting mechanisms limits its ability to capture nuanced distinctions. To enhance our approach, we introduce a rank-aware variant that uses ordinal ranking information to deliver more accurate failure detection. In detail, we construct a rankaware weight vector w where the value of each element is proportional to the ordinal ranking. First, we define the ordinal ranking vector $\\mathbf { r } = [ K , K - 1 , \\ldots , 1 ]$ with $K$ elements in descending order. Then, we apply a logarithmic weighting\n\nconc1,1 humps on back conc2,1 long trunk concC,1 long straight neck conc1,2 shaggy coat conc2,2 tusks concC,2 patched coat 1-camel 2-elephant ： C-giraffe ： conc1,K long curved neck conc2,K large floppy ears concC,K ossicones on head Concept Activation Top-K Concepts Predict 30.0 30.0 CLIP 28.1 26.3 27.3 descend 28.1 27.3 26.3 ORCA Detect failures 24.4 sort 23.5 23.4 Interpret failures Test Sample conc1,1 conc1,2 conc1,K conc2,1 conc2,2 concC,1 concC,K conc1,1 conc1,2 concC,1 conc1,K\n\nfunction to assign each rank in $\\mathbf { r }$ a weight $w _ { i } \\in { \\mathbf { w } }$ , resulting in a decreasing vector whose elements sum up to 1. Logarithmic ensures a smooth distribution of weights among the ranks of each concept, enabling a more nuanced estimation of the confidence level. Specifically, the logarithmic scaling equation is defined as $\\begin{array} { r } { w _ { i } = \\frac { \\log ( \\bar { 1 } + r _ { i } ) } { \\sum _ { j = 1 } ^ { K } \\log ( 1 + r _ { j } ) } } \\end{array}$ , with the normalization of each weight $w _ { i }$ in w. Finally, for each category $c$ with its concepts featuring in $\\mathcal { A } _ { \\mathrm { t o p } - K }$ , we calculate the prediction and the confidence of the model as follows:\n\n$$\n\\begin{array} { l } { f ( \\mathbf { x } ) = \\displaystyle \\mathrm { a r g m a x } _ { c \\in \\mathcal { V } } \\displaystyle \\sum _ { k = 1 } ^ { K } \\mathbb { I } ( a _ { k } \\in \\mathcal { A } _ { c } ) \\cdot w _ { k } } \\\\ { g ( f , \\mathbf { x } ) = \\operatorname* { m a x } _ { c \\in \\mathcal { V } } \\displaystyle \\sum _ { k = 1 } ^ { K } \\mathbb { I } ( a _ { k } \\in \\mathcal { A } _ { c } ) \\cdot w _ { k } } \\end{array}\n$$\n\nwhere $a _ { k }$ is the $k ^ { \\mathrm { { t h } } }$ concept in the ordered set $\\mathcal { A } _ { \\mathrm { t o p } - K }$ , and $\\mathbb { I } ( \\cdot )$ denotes the indicator function that returns 1 if the condition is true. We refer to this variant as ORCA-R.\n\n# Experiment\n\nDatasets. We evaluate ORCA on a wide variety of datasets: 1. Natural Image Benchmark (1) CIFAR-10/100 (Krizhevsky 2009) is a popular image recognition benchmark spanning across 10/100 categories. (2) ImageNet$l K$ (Deng et al. 2009) a well-known benchmark in computer vision, containing 1000 fine-grained categories, with 1,281,167 training and 50,000 validation samples. This benchmark contains fine-grained categories that are visually similar, making the failure detection task more challenging. 2. Satellite Image Benchmark (3) EuroSAT (Helber et al. 2017) is a satellite RGB image dataset, containing 10 categories of land usage, such as forest, river, residential buildings, industrial buildings, etc. The dataset comprises of 27,000 geo-referenced samples. (4) RESISC45 (Cheng, Han, and Lu 2017) is a public benchmark for Remote Sensing Image Scene Classification. It contains 31,500 images, covering 45 scene categories with 700 images in each categories.\n\nBaselines. We compare ORCA to 3 models in combination with 3 CSFs, yielding a total of 9 baselines. Note that we only compare with post-hoc CSFs because our methods do not require any training.\n\n1. Models (1) Zero-shot (Radford et al. 2021): The prediction of zero-shot CLIP relies on the text category name as introduced in the original paper. We compute the logits using Eq. 3 and apply CSFs to calculate the model’s confidence. (2) Ensemble (Radford et al. 2021): This model ensembles multiple templates into zero-shot classification, effectively acting as an ensemble method. We average the similarity scores from multiple templates for each category before extracting the softmax logits. (3) DescCLIP (Menon and Vondrick 2023): As describe in Sec. , DescCLIP averages the similarity scores of all the concepts for each category; we then applies CSFs to estimate the confidence score.\n\n2. CSFs (1) MSP (Hendrycks and Gimpel 2016): The confidence score is measured by taking the maximum value of the softmax responses. (2) ODIN (Liang, Li, and Srikant 2018): This CSF is a temperature-scaled version of MSP. We use the default temperature $T = 1 0 0 0$ and do not use perturbation for fair comparison. (3) DOCTOR (Granese et al. 2021): Different from MSP, DOCTOR fully exploits all available information contained in the soft-probabilities of the predictions to estimate the confidence.\n\nImplementation Details. We utilize CLIP’s ResNet-101 and ViT-B/32 backbones to perform zero-shot prediction on the benchmarks and calculate the performance metrics. For dataset with few categories, such as CIFAR-10 and EuroSAT, we use different prompts to retrieve diverse collections of concepts from the large language model GPT-3.5 (Brown et al. 2020; Peng et al. 2023) and manually select the top 10 visual concepts that are the most distinctive among categories. An example of our prompt is as follows, with more details in the Supplementary:\n\n<html><body><table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Method</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">ViT-B/32</td></tr><tr><td>AUROC个</td><td>FPR95↓</td><td>ACC ↑</td><td>AUROC↑</td><td>FPR95↓</td><td>ACC↑</td></tr><tr><td rowspan=\"10\">CIFAR10 (K=10)</td><td>Zero-shot+MSP</td><td>85.98</td><td>62.98</td><td>78.01</td><td>88.92</td><td>58.66</td><td>88.92</td></tr><tr><td>+ODIN</td><td>83.65</td><td>65.50</td><td>78.01</td><td>84.49</td><td>65.36</td><td>88.92</td></tr><tr><td>+DOCTOR</td><td>86.56</td><td>63.76</td><td>78.01</td><td>88.58</td><td>62.32</td><td>88.92</td></tr><tr><td>Ensemble+MSP</td><td>86.35</td><td>63.53 67.95</td><td>80.97</td><td>89.25</td><td>57.03</td><td>89.70</td></tr><tr><td>+ODIN</td><td></td><td>83.39</td><td>80.97</td><td>83.66</td><td>63.34</td><td>89.70</td></tr><tr><td>+DOCTOR</td><td>85.67</td><td>66.53</td><td>80.97</td><td>88.68</td><td>58.87</td><td>89.70</td></tr><tr><td>DescCLIP+MSP</td><td>85.84</td><td>64.68</td><td>80.70</td><td>89.28</td><td>58.77</td><td>88.80</td></tr><tr><td>+ ODIN</td><td>80.92</td><td>68.34</td><td>80.70</td><td>82.61</td><td>66.83</td><td>88.80</td></tr><tr><td>+DOCTOR</td><td>84.99</td><td>67.92</td><td>80.70</td><td>88.80</td><td>61.64</td><td>88.80</td></tr><tr><td>ORCA-B</td><td>84.90</td><td>66.09 62.68</td><td>80.98 80.60</td><td>87.34 89.00</td><td>50.52 52.70</td><td>89.34 90.00</td></tr><tr><td rowspan=\"10\">CIFAR100 (K= 20)</td><td>Zero-shot+MSP</td><td></td><td>85.93 80.72</td><td>73.40</td><td>48.50</td><td>81.15</td><td>71.09</td><td>58.42</td></tr><tr><td>+ODIN</td><td></td><td>77.21</td><td>75.13</td><td>48.50</td><td>76.93</td><td>71.08</td><td>58.42</td></tr><tr><td></td><td>+DOCTOR</td><td>79.68</td><td>75.36</td><td>48.50</td><td>81.57</td><td>69.40</td><td>58.42</td></tr><tr><td>Ensemble+MSP</td><td></td><td>79.22</td><td>73.43</td><td>48.66</td><td>81.44</td><td>70.88</td><td>63.91</td></tr><tr><td></td><td>+ODIN</td><td>75.59</td><td>76.00</td><td>48.66</td><td>75.73</td><td>73.87</td><td>63.91</td></tr><tr><td></td><td>+DOCTOR</td><td>77.96</td><td>76.47</td><td>48.66</td><td>80.02</td><td>74.06</td><td>63.91</td></tr><tr><td>DescCLIP+MSP</td><td></td><td>80.22</td><td>73.39</td><td>52.90</td><td>82.54</td><td>67.38</td><td>66.70</td></tr><tr><td></td><td>+ODIN</td><td>75.86</td><td>75.35</td><td>52.90</td><td>75.72</td><td>73.11</td><td>66.70</td></tr><tr><td></td><td>+DOCTOR</td><td>79.09</td><td>74.96</td><td>52.90</td><td>81.30</td><td>70.83</td><td>66.70</td></tr><tr><td>ORCA-B</td><td></td><td>80.35</td><td>70.46</td><td>52.16</td><td>83.35</td><td>67.35</td><td>66.00</td></tr><tr><td rowspan=\"10\"></td><td>ORCA-R</td><td></td><td>80.46</td><td>72.38</td><td>53.11</td><td>83.40</td><td>67.00</td><td>66.50</td></tr><tr><td>Zero-shot +MSP</td><td></td><td>78.93</td><td>74.05</td><td>56.67</td><td>79.44</td><td>72.91</td><td>58.37</td></tr><tr><td>+ODIN</td><td></td><td>70.59</td><td>80.75</td><td>56.67</td><td>70.48</td><td>80.07</td><td>58.37</td></tr><tr><td></td><td>+DOCTOR</td><td>78.38</td><td>75.90</td><td>56.67</td><td>79.01</td><td>74.17</td><td>58.37</td></tr><tr><td>Ensemble+MSP</td><td></td><td>78.58</td><td>74.37</td><td>56.73</td><td>79.66</td><td>72.89</td><td>59.22</td></tr><tr><td></td><td>+ODIN</td><td>70.29</td><td>80.98</td><td>56.73</td><td>70.61</td><td>80.55</td><td>59.22</td></tr><tr><td></td><td>+DOCTOR</td><td>77.98</td><td>76.25</td><td>56.73</td><td>78.34</td><td>76.24</td><td>59.22</td></tr><tr><td>DescCLIP+MSP</td><td></td><td>80.09</td><td>72.99</td><td>61.94</td><td>80.77</td><td>71.34</td><td>63.20</td></tr><tr><td></td><td>+ODIN</td><td>69.92</td><td>81.53</td><td>61.94</td><td>70.80</td><td>80.14</td><td>63.20</td></tr><tr><td></td><td>+DOCTOR</td><td>79.68</td><td>73.95</td><td>61.94</td><td>80.50</td><td>71.96</td><td>63.20</td></tr><tr><td colspan=\"2\">ORCA-B</td><td>80.24</td><td>71.13</td><td></td><td>62.11</td><td>80.77</td><td>69.19</td><td>63.02</td></tr><tr><td></td><td>ORCA-R</td><td></td><td>80.57</td><td>72.41</td><td>62.29</td><td>80.91</td><td>71.70</td><td>63.20</td></tr></table></body></html>\n\nTable 1: Performance on CIFAR-10/100 and ImageNet. AUROC, FPR $@$ 95TPR (FPR95), and ACC are percentages. With ACC taken into account, bold indicate the best results, underlined denote ours with the second best results.\n\nA: Some distinctive visual concepts of [CATEGORY] are:\n\nFor datasets with a larger number of categories, we use the concept collection provided by Yang et al. (Yang et al. 2022). This collection contains up to 500 concept candidates per category; we then select the top concepts that yield the highest average similarity score with the images within each category to form $\\mathcal { A }$ . We include the number of concepts used for each dataset in Table 1 and 2.\n\n# Evaluation Metrics\n\nFailure detection accuracy (AUROC). This evaluation protocol, a threshold-independent performance evaluation, measures the area under the receiver operating characteristic curve as CSFs inherently perform binary classification between correct and incorrect predictions. A higher value denotes better ability to predict failures.\n\nFalse positive rate $\\mathbf { ( F P R @ 9 5 T P R }$ ). This metric denotes the false positive rate or the probability that a misclassified sample is predicted as a correct one when the true positive rate is at $9 5 \\%$ . It is a fraction that the model falsely assigns higher confidence values to incorrect samples, reflecting the tendency to be overly confident in incorrect predictions.\n\nClassification accuracy (ACC). A classifier with low accuracy might produce easy-to-detect failures (Jaeger et al. 2023) and benefit from a high AUROC. Ideally, we wish a model to yield a high AUROC and ACC, and a low FPR simultaneously.\n\n# Results on Natural Image Benchmarks\n\nWe report the performance of all methods on the three evaluation metrics on the natural image benchmarks on ResNet101 and ViT-B/32 and provide the following observations:\n\nObservation 1: Concept-based methods demonstrate better failure detection.\n\nTable 1 shows DescCLIP and ORCA consistently achieves higher AUROC compared to Zero-shot and Ensemble, especially on datasets with a large number of categories, such as CIFAR-100 and ImageNet. The augmentation to multiple signals per category helps concept-based methods obtain a finer-grained analysis for better failure detection. On a different note, Ensemble boosts the Zero-shot’s ACC but still results in a lower AUROC and higher FPR on the large-scale datasets for both backbones. Ensemble, in the same principles as concept-based methods, augments the number of signals; however, we hypothesize the lack of diversity in those signals deteriorates the separability between correct and incorrect samples.\n\nObservation 2: Our method reduces overconfident but incorrect predictions.\n\nIn Table 1, we observe that our methods consistently reduce the false positive rate across datasets and for both backbones. Both variants of ORCA decrease the FP $\\gtrsim \\textcircled { a } 9 5 \\mathrm { T P R }$ substantially while keeping AUROC and ACC competitive. On ImageNet, ORCA-B achieves the best performance on this metric, outperforming the zero-shot model and DescCLIP by $3 . 7 2 \\%$ and $2 . 1 5 \\%$ respectively using ViT-B/32. We hypothesize that allowing the model to recognize an object from different angles provides more reliable confidence assessment, enabling faithful failure detection while also achieving superior predictive accuracy.\n\n# Results on Satellite Image Benchmarks\n\nWe report the performance on EuroSAT and RESISC45 on ResNet-101 and ViT-B/32. Note that all results are zero-shot performance. We discuss the following observation:\n\nObservation 3: Our method boosts both predictive and failure detection accuracy on remote sensing benchmarks.\n\nTable 2 shows that ORCA-R consistently outperforms all baselines on all evaluation metrics. Compared to DescCLIP $+ \\mathrm { M S P }$ on EuroSAT, ORCA-R enjoys a $3 . 6 \\%$ improvement in AUROC and $6 . 2 5 \\%$ in FPR while boosting the overall accuracy by $1 . 4 9 \\%$ . On RESISC45, while ORCA-R’s improvement on AUROC and ACC is marginal, it significantly reduces FPR. Additionally, these datasets represent out-ofdistribution data for CLIP, underscoring ORCA’s enhanced reliability and robustness against such distributional variations.\n\n# Ablation Studies\n\nWe conduct two ablation studies on the effect of the number of concepts and the choice of the weighting function used for ORCA-R in this section.\n\nAblation on number of concepts. We use the ViT-B/32 backbone on CIFAR-100 and $\\bar { K ^ { ' } } = \\{ 5 , 1 0 , 1 5 , 2 0 \\}$ for this experiment. We study the effect of the number of concepts on the performance on AUROC and $\\mathrm { F P R } @ 9 5 \\mathrm { T P R }$ of Desc${ \\bf C L I P + M S P } _ { \\mathrm { : } }$ ODIN, DOCTOR and ORCA-R. Fig. 3 shows that the FPR of ORCA-R is consistently lower than those of the other baselines across various $K$ . We also see an increasing (decreasing) trend in AUROC (FPR) as the number of concepts rises. This signifies a finer-grained assessment both enables better failure detection and alleviates the problem of assigning high confidence to incorrect predictions.\n\n![](images/5de579be4b6573fd6fcab74b64cd71098a780bf6b1ddeb33929f5f67d3802559.jpg)  \nFigure 3: Failure detection accuracy (AUROC) and false positive rate $( \\mathrm { F P R } @ 9 5 \\mathrm { T P R } _ { \\cdot }$ ) across different numbers of concepts on CIFAR-100. Overall, we can an increase in the number of concepts boosts the performance in both metrics.   \nFigure 4: Failure detection capabilities of each weighting function on EuroSAT, where Logarithmic consistently outperforms others.\n\n100 Logarithmic Linear 95 Exponential Polynomial 90 85 AUROC FPR95\n\nAblation on choice of weighting function. We examine how various weighting functions influence the failure detection efficacy of ORCA-R. Fig. 4 (left) visualizes the weight distribution on the top-10 concepts among the weighting functions. In Figure 4 (right), Logarithmic outperforms others, contrasting with Exponential, which exhibits the least effectiveness. Logarithmic ensures a balanced distribution of weights, recognizing the importance of higherranked concepts while also accounting for lower-ranked ones. Conversely, Exponential significantly overweighs the highest-ranked concept, neglecting the contributions of those ranked lower.\n\n<html><body><table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Method</td><td colspan=\"3\">ResNet-101</td><td colspan=\"3\">ViT-B/32</td></tr><tr><td>AUROC ↑</td><td>FPR95↓</td><td>ACC ↑</td><td>AUROC↑</td><td>FPR95↓</td><td>ACC ↑</td></tr><tr><td rowspan=\"10\">EuroSAT (K=10)</td><td>Zero-shot+MSP</td><td></td><td>61.73</td><td>88.98</td><td>30.30</td><td>76.42</td><td>80.24</td><td>41.11</td></tr><tr><td></td><td>+ODIN</td><td>61.35</td><td>89.38</td><td>30.30</td><td>75.54</td><td>79.28</td><td>41.11</td></tr><tr><td></td><td>+DOCTOR</td><td>60.76</td><td>89.85</td><td>30.30</td><td>76.67</td><td>79.30</td><td>41.11</td></tr><tr><td>Ensemble+MSP</td><td></td><td>54.69</td><td>92.21</td><td>31.90</td><td>66.83</td><td>89.19</td><td>48.73</td></tr><tr><td></td><td>+ODIN</td><td>55.10</td><td>93.09</td><td>31.90</td><td>65.73</td><td>90.09</td><td>48.73</td></tr><tr><td></td><td>+DOCTOR</td><td>53.73</td><td>94.09</td><td>31.90</td><td>61.14</td><td>90.63</td><td>48.73</td></tr><tr><td>DescCLIP+MSP</td><td></td><td>64.89</td><td>86.39</td><td>33.13</td><td>73.93</td><td>77.54</td><td>48.51</td></tr><tr><td></td><td>+ ODIN</td><td>64.16</td><td>87.16</td><td>33.13</td><td>71.74</td><td>78.34</td><td>48.51</td></tr><tr><td></td><td>+DOCTOR</td><td>62.79</td><td>89.05</td><td>33.13</td><td>72.74</td><td>79.85</td><td>48.51</td></tr><tr><td>ORCA-B ORCA-R</td><td></td><td>67.86</td><td>86.43</td><td>34.11</td><td>76.20</td><td>77.80</td><td>49.74</td></tr><tr><td rowspan=\"9\">RESISC45 (K=10)</td><td>Zero-shot+MSP</td><td></td><td>69.01</td><td>86.43</td><td>34.76</td><td>77.55</td><td>71.29</td><td>50.00</td></tr><tr><td></td><td></td><td>68.13</td><td>87.04</td><td>37.66</td><td>77.92</td><td>80.35</td><td>55.57</td></tr><tr><td></td><td>+ODIN</td><td>62.60</td><td>89.48</td><td>37.66</td><td>71.66</td><td>84.85</td><td>55.57</td></tr><tr><td>Ensemble+MSP</td><td>+DOCTOR</td><td>67.57</td><td>87.19</td><td>37.66</td><td>76.95</td><td>82.17</td><td>55.57</td></tr><tr><td></td><td></td><td>68.87</td><td>85.39</td><td>39.79</td><td>78.40</td><td>80.14</td><td>56.68</td></tr><tr><td></td><td>+ODIN</td><td>62.67</td><td>89.57</td><td>39.79</td><td>71.99</td><td>85.31</td><td>56.68</td></tr><tr><td>DescCLIP+MSP</td><td>+DOCTOR</td><td>67.88</td><td>87.29</td><td>39.79</td><td>77.54</td><td>82.34</td><td>56.68</td></tr><tr><td></td><td></td><td>73.44</td><td>79.78</td><td>43.16</td><td>77.47</td><td>82.25</td><td>58.33</td></tr><tr><td></td><td>+ODIN</td><td>69.47</td><td>84.61</td><td>43.16</td><td>71.49</td><td>86.21</td><td>58.33</td></tr><tr><td></td><td></td><td>+DOCTOR</td><td>72.95</td><td>79.89</td><td>43.16</td><td>76.81</td><td>84.88</td><td>58.33</td></tr><tr><td></td><td>ORCA-B</td><td></td><td>71.88</td><td>90.41</td><td>46.22</td><td>77.71</td><td>86.31</td><td>59.10</td></tr><tr><td></td><td>ORCA-R</td><td></td><td>74.28</td><td>80.31</td><td>45.13</td><td>78.24</td><td>76.52</td><td>59.10</td></tr></table></body></html>\n\nTable 2: Performance on EuroSAT and RESICS45. AUROC, FPR $@$ 95TPR (FPR95), and ACC are percentages. With ACC taken into account, bold indicate the best results, underlined denote ours with the second best results.\n\nStandard Ours auto 0.70 road vehicle 28.2 truck0.05 four wheels 27.5 ship0.05 on highway 27.4 cargo area 26.3 auto 0.34 trailer 26.2 truck 0.32 Interpret   \nLabel: ship ship0.05 hull shape 25.4 Class probability CLIP similarity scores (a) Failure caused by spurious correlation. Standard Ours plane 0.95 on water 22.8 ship0.03 jet engine 22.7 bird0.01 mental wing 21.6 frequent flyer 21.0 plane 0.32 runway 20.4 ship 0.30 Interpret   \nLabel: ship bird0.10 nautical flag 19.6 Class probability CLIP similarity scores\n\n(b) Failure caused by cross-category resemblance.\n\n# Failure Interpretation\n\nORCA not only achieves superior failure detection but also enables failure interpretation with human-level concepts. We discuss two scenarios that cause the model to output overconfident values on misclassified samples: spurious correlation and cross-category resemblance (Fig. 5).\n\nIn the former scenario (Fig. 5a), the presence of a road (a spurious feature) leads the model to misclassify the ship as a land vehicle, automobile or truck. We demonstrate that a standard model struggles to identify such failures, resulting in a high confidence score for automobile. In contrast, ORCA leverages human-level concepts, offering more nuanced signals for a refined assessment of the model’s confidence. For instance, strong responses from concepts like “road vehicle” and “four wheels” for automobile, and “cargo area” and “trailer” for truck, contribute to a significantly lower confidence. Furthermore, we can easily interpret why the model makes such a prediction through concepts.\n\nIn the latter scenario (Fig. 5b), the ship (sailboat) bears a resemblance to an airplane from a distance. The similarity between the sky and water also creates an illusion of the object being airborne. The top- $K$ concepts from our method exhibit strong responses to concepts associated with airplanes and birds. Analyzing this information allows us to confidently deduce that the model misclassifies the image as an airplane due to the sky-like background and the object’s resemblance to an airplane.\n\nCode — https://github.com/Nyquixt/ORCA\n\n# Acknowledgments\n\nThis work is supported by the DoD DEPSCoR Award AFOSR FA9550-23-1-0494, the NSF CAREER Award No. 2340074, the NSF SLES Award No. 2416937, and the NSF III CORE Award No. 2412675. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the supporting entities.",
    "institutions": [
        "University of Delaware"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n在安全关键应用中，可靠的故障检测至关重要。然而，现有的基于类别级信息的置信度评分函数，在检测神经网络误分类样本时表现不佳，因为神经网络容易对误分类样本产生过度自信的预测，且难以解释模型故障的原因。此问题影响了模型在高风险应用中的安全部署，例如在自然灾害监测中，若模型不能准确检测自身错误并及时发出信号，可能导致无法及时采取措施减轻灾害影响。\\n\\n**方法概述**\\n论文提出利用人类级概念进行故障检测和解释的方法，通过整合每个类别的多种信号，采用基于概念激活序数排名的 ORCA 方法，更准确地估计模型置信度，从而实现可靠的故障检测与解释。\\n\\n**主要贡献与效果**\\n- 提出利用人类级概念检测和解释模型故障的新策略，提升了故障检测的准确性和可解释性。\\n- 提出 ORCA 方法，通过概念激活的序数排名来估计更可靠的置信度。该方法显著降低了误报率，在 ImageNet 上使用 ViT - B/32 时 ORCA - B 相较于零样本模型和 DescCLIP 分别降低了 3.72% 和 2.15%，在 EuroSAT 上 ORCA - R 相较于 DescCLIP + MSP 在 FPR@95TPR 指标上降低了 6.25%。\\n- 实证证明基于概念的方法在多种分类基准上提高了故障预测性能。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n传统方法基于类别级信息检测误分类，而神经网络对误分类样本常产生过度自信的预测。本文方法利用人类级概念，即视觉属性，从多个角度评估模型对物体的置信度。理想情况下，能识别某个类别的模型也应能识别相关的视觉属性，通过概念激活的序数排名来捕捉概念之间的交互，从而更可靠地检测故障。因为如果模型对某一类别预测有信心，那么该类别相关概念应产生最强激活，通过概念激活的序数排名可分离正确和错误预测。\\n\\n**创新点**\\n先前的工作依赖类别级信号进行置信度估计，存在过度自信预测和难以解释故障原因的问题。本文方法将类别级信号解构为概念级信号，通过整合人类级概念，提供更细致的模型置信度评估，不仅能更有效地检测故障，还能利用这些概念解释故障原因。\\n\\n**具体实现步骤**\\n1. **人类级概念的获取**：对于每个类别定义 $K$ 个概念，所有概念的集合为 $\\mathcal{A}$，通过计算图像嵌入与所有概念的相似度得分（或对数几率）得到 $S_{conc}$。\\n2. **基于概念聚合的基线方法（DescCLIP）**：DescCLIP 计算每个类别所有概念的平均相似度得分来获取对数，应用 softmax 函数得到类别概率，使用 MSP 获得模型置信度。\\n3. **ORCA 方法**：\\n    - **基线 ORCA（ORCA - B）**：将 $S_{conc}$ 降序排序，获取前 $K$ 个概念的集合 $\\mathcal{A}_{top - K}$，预测类别为与 $\\mathcal{A}_{top - K}$ 交集元素最多的类别，置信度为预测类别概念在 $\\mathcal{A}_{top - K}$ 中出现的数量与 $K$ 的比值。\\n    - **排名感知 ORCA（ORCA - R）**：构建排名感知权重向量 $\\mathbf{w}$，通过对数加权函数为每个排名分配权重，计算每个类别概念的加权得分，预测类别为得分最高的类别，置信度为最高得分。\\n\\n**案例解析**\\n论文以骆驼为例，传统方法可能只询问模型“是否识别出骆驼”，而本文方法还会询问“是否识别出骆驼背上的驼峰”“是否识别出蓬松的皮毛”等具体属性，通过这些概念的响应来更准确地评估模型的置信度。此外，还给出了船被误分类的案例，在存在道路（虚假特征）时，标准模型会将船误分类为陆地车辆，而 ORCA 利用人类级概念，如“公路车辆”“四个轮子”等概念的强响应，降低了置信度，且能解释模型误分类的原因；当船因与飞机有跨类别相似性而被误分类时，ORCA 也能通过相关概念的响应进行故障解释。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文将 ORCA 与 3 种模型（零样本（Zero - shot）、集成（Ensemble）、DescCLIP）和 3 种置信度评分函数（CSFs）（MSP、ODIN、DOCTOR）的 9 种组合作为基线进行对比。\\n\\n**性能对比**\\n*   **在 [故障检测准确率/AUROC] 指标上：** 基于概念的方法（DescCLIP 和 ORCA）在自然图像基准上，尤其是类别数量较多的数据集（如 CIFAR - 100 和 ImageNet）上，始终比零样本和集成方法获得更高的 AUROC。例如，在 CIFAR - 100 数据集上，使用 ViT - B/32 时，ORCA - R 的 AUROC 达到 83.40%，优于零样本 + MSP 的 81.15%；在 CIFAR - 100 数据集上，使用 ResNet - 101 时，ORCA - R 的 AUROC 为 80.46%，高于零样本 + MSP 的 79.68%、集成 + MSP 的 79.22% 和 DescCLIP + MSP 的 80.22%。集成方法虽然提高了零样本的分类准确率，但在大规模数据集上的 AUROC 较低。\\n*   **在 [误报率/FPR@95TPR] 指标上：** 本文的 ORCA 方法在所有数据集和两种骨干网络上均持续降低误报率。在 ImageNet 上，使用 ViT - B/32 时，ORCA - B 的 FPR@95TPR 优于零样本模型 3.72%，优于 DescCLIP 2.15%。在 EuroSAT 数据集上，ORCA - R 的 FPR@95TPR 低于 DescCLIP + MSP 6.25%。在不同数据集和骨干网络上，ORCA 方法都显著减少了模型对误分类样本的过度自信预测。\\n*   **在 [分类准确率/ACC] 指标上：** 集成方法提升了零样本的分类准确率，但在大规模数据集上的故障检测能力不如基于概念的方法。ORCA 方法在保持较低误报率的同时，也能保证分类准确率具有竞争力。例如，在 CIFAR - 10 数据集上，ORCA - R 使用 ViT - B/32 时的 ACC 为 90.00%；在 ImageNet 上，ORCA - B 使用 ViT - B/32 时的 ACC 为 63.20%。\",\n    \"keywords\": \"### 关键词\\n\\n- 故障检测 (Failure Detection, N/A)\\n- 人类级概念 (Human - Level Concepts, N/A)\\n- 视觉语言模型 (Vision - Language Models, VLMs)\\n- ORCA 方法 (Ordinal Ranking of Concept Activation, ORCA)\\n- 图像分类 (Image Classification, N/A)\"\n}"
}