{
    "link": "https://arxiv.org/abs/2412.10163",
    "pdf_link": "https://arxiv.org/pdf/2412.10163",
    "title": "Scaling Combinatorial Optimization Neural Improvement Heuristics with Online Search and Adaptation",
    "authors": [
        "Federico Julian Camerota Verdù",
        "Lorenzo Castelli",
        "Luca Bortolussi"
    ],
    "publication_date": "2024-12-13",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 3,
    "influential_citation_count": 0,
    "paper_content": "# Scaling Combinatorial Optimization Neural Improvement Heuristics with Online Search and Adaptation\n\nFederico Julian Camerota Verdu\\` 1, Lorenzo Castelli2, Luca Bortolussi1\n\n1Dipartimento di Matematica, Informatica e Geoscienze, Universita\\` degli Studi di Trieste, Italy 2Dipartimento di Ingegneria e Architettura, Universita\\` degli Studi di Trieste, Italy federicojulian.camerotaverdu@phd.units.it, lorenzo.castelli $@$ dia.units.it, lbortolussi@units.it\n\n# Abstract\n\nWe introduce Limited Rollout Beam Search (LRBS), a beam search strategy for deep reinforcement learning (DRL) based combinatorial optimization improvement heuristics. Utilizing pre-trained models on the Euclidean Traveling Salesperson Problem, LRBS significantly enhances both in-distribution performance and generalization to larger problem instances, achieving optimality gaps that outperform existing improvement heuristics and narrowing the gap with state-of-the-art constructive methods. We also extend our analysis to two pickup and delivery TSP variants to validate our results. Finally, we employ our search strategy for offline and online adaptation of the pre-trained improvement policy, leading to improved search performance and surpassing recent adaptive methods for constructive heuristics.\n\n# Code — https://github.com/federico-camerota/LRBS\n\n# 1 Introduction\n\nCombinatorial Optimization (CO) problems can be found in several domains ranging from air traffic scheduling (Bertsimas, Lulli, and Odoni 2011) and supply chain optimization (Singh and Rizwanullah 2022) to circuit board design (Barahona et al. 1988) and phylogenetics (Catanzaro et al. 2012). Although general-purpose solvers exist and most CO problems are easy to formulate, in many applications of interest getting to the exact optimal solution is NPhard and said solvers are extremely inefficient or even impractical due to the computational time required to reach optimality (Toth 2000; Colorni et al. 1996). Specialized solvers and heuristics have been developed over the years for different applications. However, the latter are often greedy algorithms based on hand-crafted techniques that require vast domain knowledge, thus they cannot be used on different problems and may get stuck on poor local optima (Applegate, Cook, and Rohe 2003; Helsgaun 2009; Gasparin et al. 2023).\n\nCO problems have gained attention in the last few years within the deep learning community where neural networks are used to design heuristics that can overcome the limitations of traditional solvers (Lombardi and Milano 2018; Bengio, Lodi, and Prouvost 2021). In particular, extensive literature has been developed on methods to tackle the travelling salesperson problem (TSP) due to its relevance and particular structure that allows to easily handle constraints with neural heuristics. Deep learning approaches for CO problems can be divided into constructive and improvement methods. The former follows a step-by-step paradigm to generate a solution starting from an empty one and sequentially assigning decision variables (Vinyals, Fortunato, and Jaitly 2015; Nazari et al. 2018; Kool, van Hoof, and Welling 2019). Instead, improvement approaches iteratively improve a given initial solution using an operator to turn a solution into a different one (Zhang et al. 2020; de O. da Costa et al. 2020; Wu et al. 2021; Hottung and Tierney 2022). Moreover, deep learning solvers can be classified based on their learning strategies: supervised learning (Khalil et al. 2017; Joshi, Laurent, and Bresson 2019; Hottung, Bhandari, and Tierney 2020; Li, Yan, and Wu 2021; Xin et al. 2021; Sun and Yang 2023) and deep reinforcement learning (DRL) (Bello et al. 2017; Khalil et al. 2017; Deudon et al. 2018; Kool, van Hoof, and Welling 2019; Ma et al. 2020; Barrett et al. 2020; Kim, Park et al. 2021; Ma et al. 2021; Qiu, Sun, and Yang 2022; Kim, Park, and Park 2022; Ye et al. 2023; Ma, Cao, and Chee 2023).\n\nMany recent advancements in neural solvers for CO primarily lie within the constructive framework. This approach eliminates the necessity for manually crafted components, thereby providing an ideal means to address problems without requiring specific domain knowledge (Lombardi and Milano 2018). However, improvement heuristics can be easier to apply when complex constraints need to be satisfied and may yield better performance than constructive alternatives when the problem structure is difficult to represent (Zhang et al. 2020) or when known improvement operators with good properties exist (Bordewich et al. 2008). Still, generalization, i.e., scaling from training sets with small problems to large instances while retaining good performance, is an open issue when using DRL neural heuristics in CO, particularly for the TSP (Joshi et al. 2021).\n\nContributions. While generalization has been studied for constructive methods (Hottung, Kwon, and Tierney 2021; Oren et al. 2021; Choo et al. 2022; Son et al. 2023; Jiang et al. 2023; Li et al. 2023), to the best of our knowledge no prior work has been done on improvement heuristics. In this paper, we focus on improvement heuristics for the TSP based on DRL policies and propose an inference-time beam search approach, Limited Rollout Beam Search (LRBS), which allows tackling problems 10 times larger than those seen at training time. Using pre-trained models, on instances of the same size as those used for training, our search scheme achieves state-of-the-art results among similar improvement methods and shows comparable performance to constructive heuristics. Generalization to instances up to 10 times larger than those seen while training is improved considerably compared to sampling from the original policy, mitigating the gap with constructive solvers. Moreover, our approach allows the integration of online adaptation within the search to overcome the limitations posed by the pretrained model in large-scale generalization. We also investigate the effectiveness of LRBS as an exploration strategy in fine-tuning the pre-trained models on a limited dataset of instances of the same size as those in the test set. In this setting, our experiments show competitive performances with constructive approaches that use online instance-based adaptation. Finally, we validate LRBS on two pickup and delivery TSP variants and show the advantage of our search approach with respect to more specialized problem-specific solutions. In conclusion, our analysis indicates that solvers utilizing improvement heuristics and a robust exploration approach may offer a viable alternative to adaptive constructive methods, displaying enhanced scalability for larger problem instances in terms of computational times.\n\n# 2 Preliminary and Related Work Improving TSP Solutions with DRL\n\nA TSP instance is defined by a graph $G = ( V , E )$ and the objective is to find a tour $\\delta$ , i.e. a sequence of nodes $x _ { i } \\in V$ , such that each node is visited only once, the tour starts and finishes in the same node and minimizes the tour length\n\n$$\nL ( \\delta ) = w _ { \\delta _ { N } , \\delta _ { 1 } } + \\sum _ { i = 1 } ^ { N - 1 } w _ { \\delta _ { i } , \\delta _ { i + 1 } } ,\n$$\n\nwhere $N = | V |$ , $w _ { i j } \\in \\mathbb { R } ^ { + }$ and $( i , j ) \\in E$ are edges in the graph. In this work, we consider instances of the Euclidean TSP (Arora 1996) where $w _ { i j } = \\| x _ { i } - x _ { j } \\|$ .\n\nTo solve TSP instances in the improvement framework we start from a given randomly generated initial solution $\\delta ^ { 0 }$ and use a policy $\\pi _ { \\boldsymbol { \\theta } }$ , parametrized by learnable weights $\\theta$ , to sequentially improve $\\delta ^ { 0 }$ . The policy selects actions in the neighbourhood defined by an operator $g$ that, given a solution and an action, returns another solution to the problem. We formulate the DRL framework as follows.\n\nState. The state is given by the current solution $\\delta ^ { T }$ and the best solution found so far $\\delta ^ { * T } = \\mathrm { a r g m i n } _ { i \\leq T } L ( \\delta ^ { i } )$ .\n\nAction. Actions are elements within the neighbourhood defined by the operator $g$ . For TSP, we consider 2-opt moves (Lin and Kernighan 1973) that consist of selecting a tuple of indices $( i , j )$ and reversing the order of the nodes between $\\delta _ { i }$ and $\\delta _ { j }$ . In Figure 1 we illustrate an example of\n\n2-opt move with $( i = 3 , \\ j = 6 )$ ), assuming zero-based numbering, where the order of all the nodes between $\\delta _ { 3 } = 2$ and $\\delta _ { 6 } = 7$ is reversed to obtain the new tour.\n\nReward. At each step $T$ , the reward is computed as $r ^ { T } =$ ${ \\cal L } ( \\delta ^ { * T } ) - { \\cal L } ( \\delta ^ { * T - 1 } )$ , hence the agent is rewarded only when improving on the best solution found.\n\n![](images/c19d0d62e00e69f96a0f5bb291a2a754d899e76ecaace410d6d83c72ed458162.jpg)  \nFigure 1: Example of 2-opt move with $( i = 3 , \\ j = 6 )$ .\n\nThe above elements with the state transitions derived by the operator neighbourhood define a Markov Decision Process (MDP) (Bellman 1957; Puterman 1990) that we call improvement MDP that terminates after $T _ { \\mathrm { m a x } }$ the number of steps in an episode. Although this work is focused on the Euclidean TSP and its variants, the framework described here easily extends to other routing and CO problems.\n\n# Search in Neural CO\n\nBeam Search (BS) and Monte Carlo Tree Search (MCTS) (Coulom 2006) have been widely used in neural CO (Joshi, Laurent, and Bresson 2019; Oren et al. 2021; Choo et al. 2022). Typically, they are used online with autoregressive constructive methods to boost their performance at inference time. However, many of the search techniques that work well for constructive heuristics are difficult to extend efficiently to the improvement setting. This is because constructive methods work on a short horizon, i.e. the number of steps required to obtain a solution, which is defined by the number of variables in the problem. On the contrary, improvement policies often require many more iterations to achieve good performance, see e.g. Ma et al. (2021). Although MCTS has been widely applied with DRL policies, yielding impressive results (Silver et al. 2016), a notable drawback lies in the computational cost associated with its backpropagation procedure (Choo et al. 2022). This limitation renders MCTS less suitable for the context of CO, particularly when dealing with large, difficult-toexplore search spaces. In the literature on neural CO, BS has emerged as a practical alternative to MCTS. This approach strikes a favourable balance between search capability and runtime complexity, making it a promising choice for addressing the challenges inherent in CO scenarios (Vinyals, Fortunato, and Jaitly 2015; Nazari et al. 2018; Kool, van Hoof, and Welling 2019; Joshi, Laurent, and Bresson 2019).\n\nAdaptive Methods for Neural CO In recent developments within the field of neural CO, a novel trend has emerged in search methods that incorporate techniques for online adaptation of policy parameters during inference. This trend finds inspiration in the work of Hottung, Kwon, and Tierney (2021), who introduced Efficient Active Search (EAS) as an enhanced version of Active Search (AS)(Bello et al. 2017). EAS focuses on training only a small subset of policy weights, significantly reducing its computational footprint. Simulation Guided Beam Search (SGBS) (Choo et al. 2022) employs “simulations” (i.e. policy rollouts) to assess expanded nodes in BS and seamlessly integrates with EAS for online policy adaptation. SGBS’s lookahead capability facilitates informed node selection without the complexities associated with intricate backpropagation techniques as in MCTS. However, it’s important to note that SGBS samples from the DRL constructive policy until a leaf node is reached for node evaluation, rendering it too computationally intensive for improvement methods. More recently, Son et al. (2023) proposed Meta-SAGE that uses meta-learning and search to scale pre-trained models to large TSP instances. Introducing a bilevel formulation, the algorithm is made of two components: a scheduled adaptation with guided exploration (SAGE) that updates parameters at test time and a scale meta-learner that generates scale-aware context embeddings.\n\n# 3 Searching with LRBS\n\n![](images/2e3a5fda921092a53137424ed64919704e5f5f4734a1e3c7ee104a56af56ca56.jpg)  \nFigure 2: Comparison of BS, SGBS, and LRBS. On the left, is the “Expansion” step which shares similarities among the three algorithms. Highlighted in blue are the $\\beta$ paths of the active beam nodes. Yellow nodes represent the $\\beta \\times \\alpha$ children selected for expansion where SGBS and LRBS apply the DRL policy in the “Rollout”. Finally at the “Selection” step the beam is updated and grown down the search tree. While SGBS uses rollouts to evaluate the selected children discarding the trajectory, LRBS keeps the trajectory and selection is done over the states reached in the rollout phase. Illustration inspired by Choo et al. (2022).\n\nIn this section, we describe our beam search strategy for CO improvement heuristics. To overcome the limitations of previous methods in the improvement MDP, we propose an effective beam search approach that allows to trade-off\n\n# Algorithm 1: Limited Rollout Beam Search\n\n1: Input: initial solution $\\delta ^ { 0 }$ , pre-trained policy $\\pi$ , parame  \nters $( \\alpha , \\beta , n _ { s } , T _ { \\mathrm { m a x } } )$ , objective function $f$   \n2: Output: best found tour $\\delta _ { \\mathrm { b e s t } }$   \n3: $\\delta _ { \\mathrm { b e s t } } \\bar { }  \\delta ^ { 0 }$   \n4: $R \\gets \\{ \\}$   \n5: $B \\gets$ sample $\\alpha \\times \\beta$ tours from $\\pi ( \\cdot | \\delta ^ { 0 } )$   \n6: for $\\delta _ { i } ^ { 1 }$ in $B$ do   \n7: $\\delta _ { i } ^ { n _ { s } } \\gets$ rollout $\\pi$ for $n _ { s }$ steps starting at $\\delta _ { i } ^ { 1 }$   \n8: add $\\delta _ { i } ^ { n _ { s } }$ to $R$   \n9: update $\\delta _ { \\mathrm { b e s t } }$   \n10: end for   \n11: $B \\gets$ select the best $\\beta$ elements in $R$ according to $f$   \n12: $t  n _ { s }$   \n13: while $t < T _ { \\operatorname* { m a x } }$ do   \n14: $R \\gets \\{ \\}$   \n15: $B \\gets$ for each $\\delta _ { i } ^ { t }$ in $B$ sample $\\alpha$ tours from $\\pi ( \\cdot | \\delta _ { i } ^ { t } )$   \n16: for ${ \\delta } _ { i } ^ { t }$ in $B$ do   \n17: $\\delta _ { i } ^ { n _ { s } + t } \\gets$ rollout $\\pi$ for $n _ { s }$ steps starting at ${ \\delta } _ { i } ^ { t }$   \n18: add $\\delta _ { i } ^ { n _ { s } + t }$ to $R$   \n19: update $\\delta _ { \\mathrm { b e s t } }$   \n20: end for   \n21: $B \\gets$ select the best $\\beta$ elements in $R$ according to $f$   \n22: $t \\gets t + n _ { s }$   \n23: end while   \n24: return δbest\n\nbetween the additional computational cost of search and heuristic performance. Additionally, our approach mitigates the effect of the longer episodic horizon in the improvement MDP by reducing the effective horizon on which the DRL policy works.\n\n# The LRBS algorithm\n\nSolving a CO problem with the DRL framework in Section 2 can be seen as traversing a search tree using policy $\\pi$ to decide the path to follow. Nodes in the tree represent solutions to the problem, with the initial solution $\\delta ^ { \\hat { 0 } }$ being the root node, and edges possible improvement actions (e.g. 2-opt moves) that transform one solution into the other. In Algorithm 1 we present LRBS, the algorithm starts at the root node and carries out its search down the tree in a breathfirst fashion by keeping a beam of $\\beta$ active nodes for each depth level and exploring $\\alpha$ of their children, thus limiting the branching factor (see Figure 2). Contrary to other search problems, there are no terminal nodes to reach in the improvement MDP. Hence, exploration is carried out until the explored paths in the search tree reach a fixed depth $( T _ { \\mathrm { m a x } } )$ and the best solution found is returned. While SGBS relies on rollouts to evaluate actions, which is impractical in this context due to the long horizon, LRBS uses limited rollouts, effectively reducing the horizon and enabling exploration. In addition, unlike MCTS, LRBS avoids any backpropagation, making it computationally efficient for the improvement MDP. The two main operations in LRBS can be described as follows.\n\nExpansion and Rollout. LRBS introduces into the standard BS expansion step a limited policy rollout. Specifically, for each active node $o _ { k }$ in the beam, $\\alpha$ distinct children are sampled according to the probability distribution of $\\pi ( \\cdot | o _ { k } )$ (depicted in the left column in Figure 2) and then, from the resulting $\\beta \\times \\alpha$ states, the policy is rolled out for $n _ { s }$ steps to obtain solutions $o _ { k + n _ { s } }$ (as shown in the middle column of Figure 2). Parameters $\\beta$ and $\\alpha$ control the degree of exploration in LRBS, so appropriate values need to balance search performance and runtimes as both would increase with larger $\\beta$ and $\\alpha$ . For sensitivity analysis of these parameters, we refer the reader to the appendix in the extended version of this paper (Camerota Verdu\\`, Castelli, and Bortolussi 2024).\n\nSelection. To update its beam, LRBS selects the best $\\beta$ solutions according to the objective function $f$ , e.g. $L$ in the improvement MDP described in Section 2, and then the search continues from the new resulting beam front (right column of Figure 2).\n\nIn LRBS, the limited length rollouts have a considerable impact on the search capabilities of the algorithm. Within the improvement MDP, the ability of the DRL agent to explore good solutions is highly constrained to the neighbourhood spanned by the used operator $g$ and the derived available actions. This implies that more than one step may be needed to reach a better solution than the current one, and even worse solutions may be observed in the path to an improved solution. By incorporating $n _ { s }$ steps of policy rollout before selection, instead of a single-step look-ahead as in BS, LRBS harnesses the improvement potential of $\\pi$ and enhances its planning capabilities through exploratory actions facilitated by the beam.\n\n# 4 Adapting Pre-Trained Policies with LRBS\n\nWhile the search capabilities of LRBS mitigate the effect of distributional shifts when scaling to larger problem instances than those seen while training, its performance is limited by the pre-trained policy. In this section, we introduce an adaptive framework combining LRBS with EAS to update the pre-trained DRL policy. However, it is important to notice that the framework is general and other approaches could be used for adaptation instead of EAS. We study the effectiveness of this approach in two different scenarios: offline fine-tuning (FT) and online adaptation (OA). In EAS, a small set of new parameters $\\phi$ is introduced by adding a few layers into the agent’s neural network, that in encoder-decoder architectures are usually placed in the final layers of the decoder. To reduce the computational burden of previous adaptive methods, Hottung, Kwon, and Tierney (2021) proposed to only train the new weights $\\phi$ , making EAS extremely fast. To update $\\phi$ in constructive heuristics, EAS utilizes a loss function consisting of an RL component, aiming to reduce the cost of generated solutions, and an imitation learning component, which increases the probability of generating the best solution seen so far. However, it is not straightforward to apply EAS in the improvement MDP since running multiple times the improvement heuristic for the total number of steps required to achieve a good solution and then adapting $\\phi$ would incur extremely long computational times. Instead, in LRBS we can incorporate easily EAS by updating the new weights on the limited rollouts used in node expansion. To fine-tune the pre-trained policy, we assume a limited set $( S _ { F T } )$ of instances in the target problem distribution is available and train $\\phi$ to maximize the reward achieved over the LRBS rollouts with the RL loss function of EAS, leading to the gradient:\n\n$$\n\\nabla _ { \\phi } \\mathcal { L } _ { R } ( \\phi ) = \\mathbb { E } _ { \\boldsymbol { \\pi } } [ ( R ( \\delta _ { \\mathrm { L R B S } } ) - b ) \\nabla _ { \\phi } \\log \\pi _ { \\phi } ( \\delta _ { \\mathrm { L R B S } } ) ]\n$$\n\nwhere $\\delta _ { \\mathrm { L R B S } }$ is a rollout of $n _ { s }$ steps and $b$ is a baseline (as in other works, we use the one proposed in Kwon et al. (2020)). This scenario is representative of many domains where similar CO problems have to be solved several times and past instances can be used for fine-tuning. In our experiments, the instances in $\\mathit { S } _ { \\mathit { F T } }$ are solved only once by the LRBS algorithm and after each policy rollout the new parameters $\\phi$ are updated according to the gradient in Equation 1. Similarly, in the online adaptation scenario, we update the EAS weights at inference time with the approach described above. However, the EAS parameters are reset before solving each batch of test problems, hence, the extra policy weights adapt solely to the instance being solved.\n\n# 5 Experimental Results\n\nIn this section, we report experimental results on the search capabilities of LRBS and its effect on the generalization of pre-trained DRL agents to large TSP instances and two pickup and delivery variants. We use checkpoints of models from de O. da Costa et al. (2020), pre-trained on Euclidean TSP instances with 100 nodes, and from (Ma et al. 2022), pre-trained on PDTSP and PDTSPL instances with 100 nodes. Ma et al. (2021) recently proposed the DualAspect Collaborative Transformer (DACT) architecture for the improvement of TSP solutions with 2-opt moves. Even though DACT performs better than the model from de O. da Costa et al. (2020) in the authors’ study, the latter architecture showed much better scalability in our preliminary investigations and even outperformed DACT when both were coupled with LRBS. In all our experiments on TSP, for LRBS, we set a “search budget” such that $\\alpha \\times \\beta = 6 0$ and fix the other parameters to $n _ { s } = 2 0$ and $T _ { \\mathrm { m a x } } = 5 0 0 0$ , similarly to previous works and balancing solution performance and runtime. On PDTSP and PDTSPL we reduce the budget to 40 and when doing adaptation we use $n _ { s } = 1 0$ to lower memory consumption. The best values of $\\alpha$ and $\\beta$ for each dataset were determined by testing the method on a set of 10 randomly generated instances of the same size as those in the test set. We run all our experiments using a single NVIDIA Ampere GPU with 64GB of HBM2 memory.\n\nTests datasets. The TSP instances in our experiments are generated as in Kool, van Hoof, and Welling (2019) where the coordinates of nodes are sampled uniformly at random in the unit square. We consider problems with $N =$ $\\{ 1 0 0 , 1 5 0 , 2 0 0 , 5 0 0 , 1 0 0 0 \\}$ nodes. To ensure a fair comparison with the pre-trained policies, for $N = 1 0 0$ we use the same 10, 000 test instances of de O. da Costa et al. (2020). For the other problems, we generate datasets with $1 , 0 0 0$ random instances for $N ~ = ~ \\{ 1 2 5 , 2 0 0 \\}$ and with 128 instances for $N = 5 0 0$ , 1000. For PDTSP and PDTSPL experiments we generate sets of 128 random instances with 200 and 500 nodes. In the following, we refer to the test dataset with problems with $N$ nodes as $\\tt T S P { \\cal N }$ , PDTSPN and PDTSPL $N$ , respectively.\n\nBaselines. We compare LRBS with the pre-trained policy of de O. da Costa et al. (2020) and DACT, also with 8x of the augmentations introduced in Kwon et al. (2020) (A-DACT). Moreover, we include a modification on SGBS $\\mathrm { S G B S + C ) }$ with limited rollouts (as in LRBS) to work with improvement heuristics, e.g. the pre-trained DRL policy of de O. da Costa et al. (2020). The search approach is similar to LRBS but the new beam front is selected from the direct children of the previous beam front, based on the information of the limited rollouts. Finally, we report the performance of constructive approaches and related algorithms that use search and adaptation (Kwon et al. 2020; Hottung, Kwon, and Tierney 2021; Choo et al. 2022; Son et al. 2023) as well as more recent methods (Luo et al. 2023; Ye et al. 2024). Although these methods have an advantage over improvement heuristics, direct comparison is not straightforward thus we compare our method only with the former. However, they contextualize our results within the broader literature on constructive methods. Recent methods (Drakulic et al. 2024; Sun and Yang 2023; Luo et al. 2023) achieve better generalization than the considered baseline on TSP. However, such methods typically require specialized policy training and cannot be directly used on any pre-trained policy as we do in this work.\n\nTable 1: Performance evaluation on TSP100 and TSP150. For improvement methods, numbers in brackets indicate the number of steps.   \n\n<html><body><table><tr><td colspan=\"4\"></td><td colspan=\"3\">N = 150</td></tr><tr><td>METHOD</td><td>OBJ.</td><td>N = 100 GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td></tr><tr><td>CONCORDE</td><td>7.75</td><td>0.0%</td><td></td><td>9.35</td><td>0.0%</td><td>1</td></tr><tr><td>POMO1</td><td>7.77</td><td>0.078%</td><td>3H</td><td>9.37</td><td>0.33%</td><td>1H</td></tr><tr><td>SGBS1</td><td>7.76</td><td>0.058%</td><td>0.2H</td><td>9.36</td><td>0.22%</td><td>0.1H</td></tr><tr><td>EAS1</td><td>7.76</td><td>0.044%</td><td>15H</td><td>9.35</td><td>0.12%</td><td>10H</td></tr><tr><td>SGBS+EAS1</td><td>7.76</td><td>0.024%</td><td>15H</td><td>9.35</td><td>0.08%</td><td>10H</td></tr><tr><td>DACT[10K]</td><td>7.79</td><td>0.463%</td><td>1.7H</td><td>10.20</td><td>9.12%</td><td>0.4H</td></tr><tr><td>A-DACT[10K]</td><td>7.76</td><td>0.101%</td><td>13H</td><td>9.86</td><td>5.54%</td><td>2.9H</td></tr><tr><td>COSTA</td><td>7.76</td><td>0.065%</td><td>19H</td><td>9.37</td><td>0.25%</td><td>3.2H</td></tr><tr><td>SGBS+C[5K]</td><td>7.78</td><td>0.335%</td><td>193H</td><td>9.44</td><td>1.00%</td><td>31H</td></tr><tr><td>BEAM S.[5K]</td><td>7.76</td><td>0.015%</td><td>19H</td><td>9.36</td><td>0.18%</td><td>6.4H</td></tr><tr><td>OURS[5K]</td><td>7.76</td><td>0.014%</td><td>19H</td><td>9.36</td><td>0.16%</td><td>3.2H</td></tr><tr><td>OURS+OA[5K]</td><td>7.76</td><td>0.013%</td><td>22H</td><td>9.36</td><td>0.13%</td><td>3.6H</td></tr></table></body></html>\n\n# Boosting In-Distribution Performance\n\nIn Table 1 we report the results of LRBS on $T S P 1 0 0$ and T SP 150 that are close to the training data distribution. Our method outperforms all the considered baselines on\n\nTSP100 and has the best results among improvement heuristics on TSP150. On TSP150 the constructive baselines show slightly better gaps than LRBS, but our approach has considerably lower runtime. From our analysis, on test instances close to the training data distribution the best LRBS configuration is $\\beta = 6 0$ , $\\alpha = 1$ ). While such a configuration corresponds to $\\beta$ parallel runs of the policy, introducing limited rollouts allows us to perform online adaption and achieve improved performance.\n\n# Out-of-Distribution Exploration with a Pre-Trained Policy\n\nIn the first part of Table 2 we show results on the generalization power of LRBS on TSP problems with 200, 500 and 1000 nodes with LRBS $( \\beta , \\alpha )$ configurations (30, 2), (15, 4) and (5, 12), respectively. As the test set distribution shifts away from the training distribution we observe that increasing the number of children evaluated for each node in the beam front improves on generalization. While on smaller instances the policy can select good actions and more exploitation with lower $\\alpha$ leads to the best performance, on larger instances increasing $\\alpha$ allows to compensate for the imprecision of the agent and yields better results. On these test datasets, LRBS scales better than other improvement heuristics achieving optimality gaps close to those of constructive approaches. Our experiments show that the augmentations employed by Ma et al. (2021) considerably improve the policy performance on instances with the same size as the training set. However, when considering larger graphs the benefit of the augmentations becomes less pronounced and the algorithm fails to scale. On the contrary, online exploration with LRBS mitigates the performance degeneration due to distributional shift and our method even improves on the results that the policy of de O. da Costa et al. (2020) would achieve if exploring the solution space for the same time as LRBS and using on average $\\operatorname { 1 2 x }$ more 2- opt operations. On larger instances, LRBS is not competitive with Meta-SAGE but achieves optimality gaps comparable to those of EAS and $_ { \\mathrm { S G B S + E A S } }$ , even improving its performance as the instances get larger. Turning our attention to the comparison of LRBS to BS, the results in Table 2 present an interesting phenomenon. On the smaller instances with up to 500 nodes, LRBS is faster and achieves much lower optimality gaps, even 6x smaller than BS. However, on the largest problems of the TSP1000 dataset, BS performs better than LRBS. This result strongly suggests that as the distributional shift between the training and test instances gets very large the step-wise greedy selection process of BS is better than the rollouts of LRBS in limiting the performance degradation of the policy. This further motivates the need for adaptive strategies to overcome the limitations posed by the pre-trained model.\n\n# Generalization via Adaptation\n\nIn the second part of Table 2, we show results on the generalization of LRBS after fine-tuning the DRL policy on a small set of randomly generated instances with the same number of nodes as the test set (FT) and when adapting the policy parameters online (OA). The LRBS configurations are the same used for the non-adaptive experiments, with the only exception of the $\\mathrm { L R B S } + \\mathrm { F T }$ on the TSP1000 where we use ( $\\dot { \\beta } = 1 0$ , $\\alpha = 6$ ). For all the considered problems, the FT dataset of randomly generated instances is of size equal to $1 0 \\%$ of the test set size and each instance is solved only once using LRBS, running times include also the fine-tuning phase. While $\\mathrm { L R B S + F T }$ shows the best results for TSP500 and TSP1000, we do not highlight them in bold to keep a fair comparison with the baselines. Our results show that finetuning on a limited set of problems allows LRBS to improve considerably on larger instances surpassing all the baselines on the TSP500 and TSP1000 benchmarks, while on the TSP200 the performance of LRBS is close to that of EAS. Even though online adaptation is less effective than finetuning, since policy weights are trained only on the instance being solved and reset thereafter, it achieves competitive results on the TSP200 and TSP500 datasets while outperforming constructive baselines on the TSP1000 instances. Although FT achieves lower gaps, we highlight these results in Table 2 for a fair comparison with the baselines. These results show that introducing an adaptive component in the search process of LRBS can overcome the limitations posed by the adopted pre-trained policy. In particular, on the larger TSP1000 problems, the use of LRBS alone fails to achieve the performance of the constructive baselines while both the offline and online adaptive approaches we propose almost halve the optimality gap of LRBS alone and even improve on the baselines.\n\n<html><body><table><tr><td colspan=\"4\">TSP200</td><td colspan=\"3\">TSP500</td><td colspan=\"3\">TSP1000</td></tr><tr><td>METHOD</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td></tr><tr><td>CONCORDE</td><td>10.704</td><td>0.0%</td><td>-</td><td>16.530</td><td>0.0%</td><td></td><td>23.144</td><td>0.0%</td><td></td></tr><tr><td>GLOP²</td><td></td><td></td><td></td><td>16.91</td><td>1.99%</td><td>0.1H</td><td>23.84</td><td>3.11%</td><td>0.1H</td></tr><tr><td>LEHD³</td><td></td><td>0.0182%</td><td>0.2H</td><td></td><td>0.167%</td><td>1.2H</td><td></td><td>0.719%</td><td>7H</td></tr><tr><td>EAS4</td><td>10.736</td><td>0.455%</td><td>2.4H</td><td>18.135</td><td>9.362%</td><td>4.3H</td><td>30.744</td><td>32.869%</td><td>20H</td></tr><tr><td>SGBS+EAS4</td><td>10.734</td><td>0.436%</td><td>2.1H</td><td>18.191</td><td>9.963%</td><td>4.2H</td><td>28.413</td><td>22.795%</td><td>19H</td></tr><tr><td>META-SAGE4</td><td>10.729</td><td>0.391%</td><td>2.1H</td><td>17.131</td><td>3.559%</td><td>3.8H</td><td>25.924</td><td>12.038%</td><td>18H</td></tr><tr><td>DACT[10K]</td><td>15.450</td><td>34.346%</td><td>0.6H</td><td>154.339</td><td>833%</td><td>0.4H</td><td>421.76</td><td>1722%</td><td>1.8H</td></tr><tr><td>A-DACT[10K]</td><td>14,345</td><td>34.023%</td><td>4.4H</td><td>147.127</td><td>790%</td><td>3.3H</td><td>412, 787</td><td>1683%</td><td>11.9H</td></tr><tr><td>COSTA</td><td>10.789</td><td>0.796%</td><td>4.1H</td><td>17.971</td><td>8.717%</td><td>1.9H</td><td>30.439</td><td>31.526%</td><td>7.8H</td></tr><tr><td>SGBS+C[5K]</td><td>10.903</td><td>1.858%</td><td>21.2H</td><td>18.455</td><td>11.642%</td><td>19.1H</td><td>47.083</td><td>103.44</td><td>78.6H</td></tr><tr><td>BEAM S.[5K]</td><td>11.137</td><td>4.051%</td><td>6.4H</td><td>17.851</td><td>7.993%</td><td>2.5H</td><td>26.507</td><td>14.536%</td><td>8.7H</td></tr><tr><td>OURS [2K]</td><td>10.782</td><td>0.738%</td><td>1.6H</td><td>17.684</td><td>6.902%</td><td>0.8H</td><td>32.368</td><td>39.970%</td><td>3.1H</td></tr><tr><td>OURS[5K]</td><td>10.771</td><td>0.633%</td><td>4.1H</td><td>17.309</td><td>4.633%</td><td>1.9H</td><td>27.922</td><td>20.740%</td><td>7.8H</td></tr><tr><td>OURS+OA[2K]</td><td>10.771</td><td>0.629%</td><td>1.9H</td><td>17.443</td><td>5.523%</td><td>0.9H</td><td>28.468</td><td>23.008%</td><td>3.3H</td></tr><tr><td>OURS+OA[5K]</td><td>10.760</td><td>0.528%</td><td>4.8H</td><td>17.187</td><td>3.973%</td><td>2.0H</td><td>25.895</td><td>11.889 %</td><td>8.1H</td></tr><tr><td>OURS+FT[2K]</td><td>10.768</td><td>0.599%</td><td>2.6H</td><td>17.303</td><td>4.680%</td><td>0.9H</td><td>28.891</td><td>24.838%</td><td>3.9H</td></tr><tr><td>OURS+FT[5K]</td><td>10.757</td><td>0.504%</td><td>4.6H</td><td>17.102</td><td>3.463%</td><td>2.0H</td><td>25.801</td><td>11.483%</td><td>8.6H</td></tr></table></body></html>\n\nTable 2: Performance evaluation on TSP200, TSP500 and TSP1000. For improvement methods, numbers in brackets indicate the number of steps.\n\nTable 3: Optimality gaps on TSP150, TSP200, TSP500 and TSP1000 datasets.   \n\n<html><body><table><tr><td></td><td>TSP150</td><td>TSP200</td><td>TSP500 TSP1000</td></tr><tr><td>OURS+FT</td><td>0.14%</td><td>0.50% 3.46%</td><td>11.48%</td></tr><tr><td>W/OLRBSFT</td><td>0.17%</td><td>0.52% 8.43%</td><td>81.63%</td></tr><tr><td>W/O EXP.</td><td>0.28%</td><td>1.44% 7.57%</td><td>16.06%</td></tr></table></body></html>\n\nAblation study. Table 3 reports the analysis of the importance of each element in the fine-tuning experiments on LRBS. In the first case (w/o LRBS FT in Table 3), the training framework of de O. da Costa et al. (2020) is used to fine-tune the policy while in the latter (w/o Exp. in Table 3) we sample from the policy for the same time as the LRBS runtime. To provide a fair comparison, when fine-tuning is done without LRBS the training runs for $n _ { s } \\times \\beta$ steps to train on the same number of environment interactions. While on the TSP150 and TSP200 datasets w/o LRBS FT yields a gap close to that with LRBS, on TSP500 and TSP1000 there is a considerable performance degeneration. On the contrary, when online exploration is replaced by sampling there is a much smaller effect on the generalization abilities of the model in the larger datasets but a greater decrease in performance in the TSP150 and TSP200 datasets. This shows the strength of LRBS in the fine-tuning phase where exploration allows the policy to better adapt to larger instances, especially for larger instances where the policy can easily get stuck in local optima. Moreover, for smaller problems, we observe that exploration in the fine-tuning phase is less critical but it has a considerable impact when applied online.\n\n<html><body><table><tr><td></td><td colspan=\"3\">PDTSP200</td><td colspan=\"3\">PDTSP500</td><td colspan=\"3\">PDTSPL200</td><td colspan=\"3\">PDTSPL500</td></tr><tr><td>METHOD</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td><td>OBJ.</td><td>GAP</td><td>TIME</td></tr><tr><td>LKH</td><td>12.913</td><td>0.0%</td><td>3.4H</td><td>20.332</td><td>0.0%</td><td>20.8H</td><td>29.322</td><td>0.0%</td><td>2.7H</td><td>69.922</td><td>0.0%</td><td>23.1H</td></tr><tr><td>N2S-A[1K]</td><td>15.321</td><td>18.655%</td><td>2.7H</td><td>114.789</td><td>464.668%</td><td>34.6H</td><td>31.060</td><td>5.937%</td><td>3.4H</td><td>185.862</td><td>165.900%</td><td>51.0H</td></tr><tr><td>N2S-A[2k]</td><td>14.875</td><td>15.198%</td><td>5.5H</td><td>95.513</td><td>369.723%</td><td>69.3H</td><td>30.278</td><td>3.272%</td><td>6.8H</td><td>176.543</td><td>152.583%</td><td>102.3H</td></tr><tr><td>N2S-A[3K]</td><td>14.716</td><td>13.960%</td><td>8.3H</td><td>89.731</td><td>341.330%</td><td>103.9H</td><td>30.028</td><td>2.417%</td><td>10.1H</td><td>173.354</td><td>148.017%</td><td>153.5H</td></tr><tr><td>OURS [1K]</td><td>14.710</td><td>13.918%</td><td>1.0H</td><td>48.907</td><td>140.604%</td><td>5.8H</td><td>30.228</td><td>3.103%</td><td>1.4H</td><td>142.341</td><td>103.669%</td><td>8.8H</td></tr><tr><td>OURS [2k]</td><td>14.433</td><td>11.773%</td><td>2.0H</td><td>35.891</td><td>76.558%</td><td>11.6H</td><td>29.869</td><td>1.874%</td><td>2.8H</td><td>108.413</td><td>55.087%</td><td>17.6H</td></tr><tr><td>OURS [3K]</td><td>14.320</td><td>10.899%</td><td>3.0H</td><td>33.723</td><td>65.889%</td><td>17.3H</td><td>29.721</td><td>1.370%</td><td>4.2H</td><td>85.762</td><td>22.663%</td><td>26.5H</td></tr><tr><td>OURS+OA [1K]</td><td>14.456</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OURS+OA [2K]</td><td></td><td>11.951%</td><td>1.9H</td><td>39.885</td><td>96.201%</td><td>11.8H</td><td>30.164</td><td>2.884%</td><td>2.8H</td><td>108.849</td><td>55.726%</td><td>21.6H</td></tr><tr><td>OURS+OA[3K]</td><td>14.161 13.987</td><td>9.664% 8.321%</td><td>3.7H 5.5H</td><td>33.587 31.820</td><td>65.227% 56.532%</td><td>23.6H 35.4H</td><td>29.839 29.716</td><td>1.774% 1.358%</td><td>5.6H 8.5H</td><td>81.044 77.012</td><td>15.942% 10.161 %</td><td>43.0H 64.2H</td></tr></table></body></html>\n\nTable 4: Performance evaluation on PDTSP200, PDTSP500, PDTSPL200 and PDTSPL500. Numbers in brackets indicate th number of steps.\n\nComputational efficiency From Table 2 we can also observe that LRBS and its online adaptive variant not only present a lower runtime than the constructive methods but also scale better, i.e. the relative increase in runtime as the test problems get larger is smaller for our algorithms. The reason for this fact is the autoregressive nature of constructive heuristics. With improvement approaches, we keep a fixed number of steps hence the computational cost grows only due to the larger problems to be processed by the neural policy. However, constructive methods, by design, need to perform increasingly more steps to generate solutions for larger instances, thus incurring an additional computational burden as the size of the problems increases\n\n# Pickup and Delivery Problems\n\nThe pickup and delivery variant of TSP (PDTSP) consists of $n$ one-to-one pickup-delivery requests, where goods at $n$ pickup nodes need to be transported to $n$ corresponding delivery nodes. The objective is to find the shortest Hamiltonian cycle under the precedence constraint that every pickup node has to be visited before its corresponding delivery node. We also study PDTSP with the last-in-first-out constraint (PDTSPL) that enforces a stack ordering between collected goods and delivery is allowed only for the good at the top of the stack. For these problems, Ma et al. (2022) define a removal-reinsertion operator that selects a pickupdelivery request nodes $( \\delta _ { i ^ { + } } , \\delta _ { i ^ { - } } )$ , positions $( j , \\ k )$ and places node $\\delta _ { i ^ { + } }$ after node $\\delta _ { j }$ and node $\\delta _ { i ^ { - } }$ after $\\delta _ { k }$ . In applying LRBS, we use the same framework described for the TSP but perform the expansion phase only on removal actions. The additional constraints are addressed at the policy and environment level, making LRBS versatile. In Table 4 we report the results of applying LRBS on model checkpoints from Ma et al. (2022) (N2S-A), pre-trained on pickup and delivery instances of size 100, when solving PDTSP and PDTSPL instances with $N = 2 0 0$ and 500 nodes. In these experiments, for N2S-A we use the same exploration strategy adopted by the authors where at inference time each instance solved is transformed into $\\scriptstyle { \\frac { 1 } { 2 } } \\left| N \\right|$ different ones, using the augmentations of (Kwon et al. 2020), and the policy is rolled out from each new instance. Our results show that the online exploration approach of LRBS is much more effective than N2S-A when generalizing to larger instances. Not only in terms of pure performance but also computational efficiency. On the smaller instances with 200 nodes, LRBS achieves a good reduction of optimality gaps requiring less time than N2S-A even when performing online adaptation. The PDTSP500 benchmark results are not satisfactory with optimality gaps well above $5 0 \\%$ but still, LRBS shows improved generalization compared to N2S-A reducing its gap by almost $6 x$ . On the much more constrained PDTSPL500 problems instead, online search through LRBS outperforms N2S-A with a gap reduction close to $1 0 x$ when adaption is employed. Overall, the results of Table 4 are still far from being competitive with traditional solvers such as LKH but show the generalization potential of pre-trained policy with online search and adaptation.\n\n# 6 Conclusion\n\nIn this study, we have introduced LRBS, a novel beam search method designed to complement DRL-based improvement heuristics for combinatorial optimization problems enhancing inference time performance and generalization. LRBS offers a tailored approach that enables pre-trained models to efficiently handle problem instances of significantly larger scales, up to ten times bigger than those encountered during the DRL policy initial training phase. To further enhance the generalization of pre-trained models, we integrate LRBS with EAS in offline and online adaptive scenarios. Our experimental evaluation shows that LRBS is superior to existing DRL improvement methods in solving the Euclidean TSP and two pickup and delivery variants. LRBS consistently outperforms alternative approaches proposed both for constructive and improvement heuristics. Moreover, in our analysis, LRBS exhibits superior runtime efficiency when scaling to larger instances compared to established constructive baselines, showing how improvement heuristics coupled with adaptive and search approaches can be a viable alternative to constructive methods.\n\n# Acknowledgments\n\nThis work has been partially supported by the PNRR project iNEST (Interconnected North-Est Innovation Ecosystem) funded by the European Union Next-GenerationEU (Piano Nazionale di Ripresa e Resilienza (PNRR) – Missione 4 Componente 2, Investimento 1.5 – D.D. 1058 23/06/2022, ECS 00000043).",
    "institutions": [
        "Dipartimento di Matematica, Informatica e Geoscienze, Università degli Studi di Trieste",
        "Dipartimento di Ingegneria e Architettura, Università degli Studi di Trieste"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n组合优化（CO）问题广泛存在于多个领域，如空中交通调度、供应链优化等。但大多数CO问题求解是NP难的，传统求解器效率低，现有深度强化学习（DRL）神经启发式方法在泛化性上存在问题，尤其是旅行商问题（TSP）。如何让求解器在处理更大规模问题实例时仍保持良好性能，是当前需要解决的重要问题。\\n\\n**方法概述**\\n本文提出有限滚动束搜索（LRBS）策略，用于基于DRL的组合优化改进启发式方法。该策略利用预训练模型，可处理比训练时大10倍的问题实例，并能集成在线自适应功能。此外，还将LRBS与高效主动搜索（Efficient Active Search, EAS）集成，用于离线和在线自适应场景，以进一步提升预训练模型的泛化能力。\\n\\n**主要贡献与效果**\\n- 在欧几里得TSP上，使用预训练模型，在与训练集相同大小的实例上，LRBS达到了同类改进方法中的最优结果，与构造启发式方法性能相当；泛化到比训练时大10倍的实例时，相比原策略采样有显著提升，缩小了与构造求解器的差距。例如在TSP100数据集上，LRBS达到了0.014%的最优性差距，优于所有考虑的基线。\\n- 集成在线自适应功能，在处理大规模泛化问题时，克服了预训练模型的局限性，实验显示在微调预训练模型时，与使用在线基于实例自适应的构造方法具有竞争力。在TSP500和TSP1000基准上，微调后的LRBS超过了所有基线，如在TSP1000上，未微调的LRBS（OURS[5K]）最优性差距为20.740%，而微调后的LRBS（OURS+FT[5K]）为11.483%。\\n- 在两种取货和送货TSP变体上验证了LRBS的有效性，显示出相对于更专业的特定问题解决方案的优势。如在PDTSP500上，N2S - A（[1K]）的最优性差距为464.668%，而LRBS（OURS[1K]）为140.604%。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nLRBS的核心思想是在标准束搜索（BS）扩展步骤中引入有限策略滚动，通过控制参数 $\\beta$ 和 $\\alpha$ 平衡搜索性能和运行时间，减少DRL策略工作的有效视野，从而在改进马尔可夫决策过程（MDP）中实现有效探索。其原理在于利用有限滚动步骤，使DRL智能体能够跳出当前操作符邻域的限制，探索到更好的解决方案。\\n\\n**创新点**\\n先前的方法在改进MDP中存在局限性，如模拟引导束搜索（Simulation Guided Beam Search, SGBS）依赖滚动评估动作在长视野下不实用，蒙特卡罗树搜索（Monte Carlo Tree Search, MCTS）存在反向传播计算成本高的问题。与这些方法相比，LRBS使用有限滚动，减少了视野，避免了反向传播，提高了计算效率；同时，在选择步骤中，LRBS基于滚动阶段到达的状态进行选择，而不是像SGBS那样丢弃轨迹。\\n\\n**具体实现步骤**\\n1. 输入初始解 $\\delta ^ { 0 }$、预训练策略 $\\pi$、参数 $( \\alpha , \\beta , n _ { s } , T _ { \\mathrm { m a x } } )$ 和目标函数 $f$。\\n2. 初始化最佳解 $\\delta _ { \\mathrm { b e s t } }$ 为 $\\delta ^ { 0 }$，集合 $R$ 为空。\\n3. 从 $\\pi ( \\cdot | \\delta ^ { 0 } )$ 中采样 $\\alpha \\times \\beta$ 个旅行路线，存入集合 $B$。\\n4. 对于 $B$ 中的每个 $\\delta _ { i } ^ { 1 }$，从 $\\delta _ { i } ^ { 1 }$ 开始滚动策略 $\\pi$ $n _ { s }$ 步，得到 $\\delta _ { i } ^ { n _ { s } }$，将其加入集合 $R$，并更新 $\\delta _ { \\mathrm { b e s t } }$。\\n5. 根据目标函数 $f$ 选择 $R$ 中最佳的 $\\beta$ 个元素更新集合 $B$。\\n6. 重复步骤4和5，直到探索路径达到固定深度 $T _ { \\mathrm { m a x } }$。\\n7. 返回最佳解 $\\delta _ { \\mathrm { b e s t } }$。\\n\\n**案例解析**\\n论文中以2 - opt移动为例，说明了在TSP问题中动作的定义。假设零基编号，对于 $( i = 3 , \\ j = 6 )$ 的2 - opt移动，将 $\\delta _ { 3 }$ 和 $\\delta _ { 6 }$ 之间所有节点的顺序反转以获得新的旅行路线，展示了如何通过动作改变当前解。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 预训练的de O. da Costa等人（2020）的策略。\\n- 双方面协作变压器（Dual - Aspect Collaborative Transformer, DACT），以及使用8倍增强的A - DACT。\\n- 改进版的SGBS（SGBS + C），带有有限滚动。\\n- 构造方法及相关使用搜索和自适应的算法，如Kwon等人（2020）、Hottung等人（2021）、Choo等人（2022）、Son等人（2023）的方法，以及更近期的Luo等人（2023）、Ye等人（2024）的方法。\\n\\n**性能对比**\\n*   **在 [最优性差距/Optimality Gap] 指标上：** 在TSP100数据集上，LRBS（OURS[5K]）达到了0.014%的最优性差距，优于所有考虑的基线，包括DACT（0.463%）、A - DACT（0.101%）、COSTA（0.065%）等。在TSP150数据集上，LRBS（OURS[5K]）的最优性差距为0.16%，在改进启发式方法中表现最佳，构造基线的差距略优于LRBS，但LRBS的运行时间显著更低。在TSP200、TSP500和TSP1000数据集上，随着问题规模增大，LRBS的泛化能力优于其他改进启发式方法，达到了与构造方法相近的最优性差距。例如在TSP200上，LRBS（OURS[5K]）的最优性差距为0.633%，优于DACT（34.346%）、A - DACT（34.023%）等；在TSP500上，LRBS（OURS[5K]）的最优性差距为4.633%，优于DACT（833%）、A - DACT（790%）等；在TSP1000上，LRBS（OURS[5K]）的最优性差距为20.740%，优于DACT（1722%）、A - DACT（1683%）等。与Meta - SAGE相比，在较大实例上LRBS不具竞争力，但与高效主动搜索（EAS）和SGBS + EAS的最优性差距相当，且随着实例增大性能有所提升。在取货和送货问题（PDTSP和PDTSPL）上，LRBS在处理更大规模实例时，相比N2S - A显著降低了最优性差距。如在PDTSP500上，N2S - A（[1K]）的最优性差距为464.668%，而LRBS（OURS[1K]）为140.604%；在PDTSPL500上，N2S - A（[1K]）的最优性差距为165.900%，LRBS（OURS[1K]）为103.669%。\\n*   **在 [运行时间/Runtime] 指标上：** LRBS及其在线自适应变体的运行时间低于构造方法，并且随着测试问题规模增大，运行时间的相对增加幅度更小。例如在TSP实验中，构造方法因需要执行更多步骤来生成解决方案，导致计算负担随问题规模增加而显著增大，而LRBS保持固定步数，计算成本仅因问题规模增大而适度增加。在TSP150数据集上，LRBS（OURS[5K]）运行时间为3.2H，远低于SGBS + C（31H）等。在取货送货TSP变体上，LRBS在较小实例（如PDTSP200）上达到了良好的最优性差距降低，且所需时间少于N2S - A；在较大实例（如PDTSP500）上，LRBS虽最优性差距仍较高，但相比N2S - A降低了近6倍；在更受约束的PDTSPL500问题上，使用自适应时LRBS的差距降低接近10倍。\",\n    \"keywords\": \"### 关键词\\n\\n- 组合优化 (Combinatorial Optimization, CO)\\n- 旅行商问题 (Traveling Salesperson Problem, TSP)\\n- 深度强化学习 (Deep Reinforcement Learning, DRL)\\n- 有限滚动束搜索 (Limited Rollout Beam Search, LRBS)\\n- 高效主动搜索 (Efficient Active Search, EAS)\"\n}"
}