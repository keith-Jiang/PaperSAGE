{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.18180",
    "link": "https://arxiv.org/abs/2412.18180",
    "pdf_link": "https://arxiv.org/pdf/2412.18180.pdf",
    "title": "PCM Selector: Penalized Covariate-Mediator Selection Operator for Evaluating Linear Causal Effects",
    "authors": [
        "Hisayoshi Nanmo",
        "Manabu Kuroki"
    ],
    "publication_date": "2024-12-24",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "paper_content": "# PCM Selector: Penalized Covariate-Mediator Selection Operator for Evaluating Linear Causal Effects\n\nHisayoshi Nanmo1, 2, Manabu Kuroki2\n\n1Chugai Pharmaceutical Co., Ltd., Nihonbashi Muromachi, Chuo-ku, Tokyo, Japan 2Yokohama National University, Tokiwadai, Hodogaya-ku, Yokohama, Japan nanmohisayoshi $@$ gmail.com, kuroki-manabu-zm@ynu.ac.jp\n\n# Abstract\n\nFor a data-generating process for random variables that can be described with a linear structural equation model, we consider a situation in which (i) a set of covariates satisfying the back-door criterion cannot be observed or (ii) such a set can be observed, but standard statistical estimation methods cannot be applied to estimate causal effects because of multicollinearity/high-dimensional data problems. We propose a novel two-stage penalized regression approach, the penalized covariate-mediator selection operator (PCM Selector), to estimate the causal effects in such scenarios. Unlike existing penalized regression analyses, when a set of intermediate variables is available, PCM Selector provides a consistent or less biased estimator of the causal effect. In addition, PCM Selector provides a variable selection procedure for intermediate variables to obtain better estimation accuracy of the causal effects than does the back-door criterion.\n\nTechnical Appendix — https://doi.org/10.48550/arXiv.2412.18180\n\n# Introduction\n\n# Background\n\nAuxiliary variables are those that are not considered to be of interest in themselves but help us to evaluate causal effects and/or understand the data-generating process in practical studies. For example, an intermediate variable is often considered an auxiliary variable because it is used to evaluate causal effects (Pearl 2001, 2009), to understand the data-generating process in the context of mediation analysis (Baron and Kenny 1986; Imai et al. 2011; Mackinnon 2008) and to improve the estimation accuracy of causal effects (Cox 1960; Hayashi and Kuroki 2014).\n\nIn the context of linear structural equation models, this paper focuses on estimating causal effects using intermediate variables. For cases in which the data-generating process for random variables can be described by nonparametric structural equation models and the corresponding directed acyclic graph, Pearl (2009) provided the front-door criterion as the identification condition for causal effects based on intermediate variables. In addition, in the framework of linear structural equation models, Kuroki (2000), Nanmo and Kuroki (2021), and Kuroki and Tezuka (2023) formulated the exact variance of causal effects based on the front-door criterion. Furthermore, Kuroki and Cai (2004), Hui and Zhongguo (2008), and Ramsahai (2012) compared some identification conditions in terms of the asymptotic estimation accuracy of causal effects. On the other hand, under the assumption that a treatment variable is associated with a response variable through a univariate intermediate variable, from the viewpoint of the asymptotic estimation accuracy, Cox (1960) showed that the estimation accuracy of the regression coefficient of the treatment variable on the response variable in the single linear regression model can be improved by using a joint linear regression model based on the response variable and the intermediate variable. In addition, Kuroki and Hayashi (2014) and Hayashi and Kuroki (2014) derived the same results as Cox (1960) in terms of the exact variance of causal effects. Gupta, Lipton, and Childers (2021) derived the same results as Kuroki and Hayashi (2014) and Hayashi and Kuroki (2014) for cases in which a multivariate intermediate variable is available.\n\nIn existing studies, it is noted that causal effects can be estimated by standard statistical estimation methods, e.g., the maximum likelihood estimation (MLE) method and the ordinary least squares (OLS) method. Thus, many covariates affect both the treatment variable and the response variable and are highly correlated with each other in reality. This situation leads to a multicollinearity problem, which decreases the estimation accuracy of the causal effects and leads to the formulation of an unreliable plan that prevents us from conducting appropriate policy decision-making. In addition, when the sample size is smaller than the number of explanatory variables in the regression analysis, highdimensional data analysis also suffers from multicollinearity problems, which cause overfitting and interfere with obtaining admissible solutions for regression coefficients. Recently, due to the development of technological advances in collecting data with many variables to better understand a given phenomenon of interest, the multicollinearity problem has become serious in many domains. To overcome this difficulty, numerous kinds of variable selection techniques based on penalized regression analysis, e.g., the least absolute shrinkage and selection operator (LASSO), adaptive LASSO, and Elastic Net, have been proposed by many statistical and AI researchers and practitioners (Bu¨hlmann and van de Geer 2011; Efron et al. 2004; Tibshirani 1996; Van et al. 2014; Zou 2006; Zou and Hastie 2005). However, the present countermeasures against the multicollinearity problem are formulated independently of the problem of identifying causal effects. Thus, although stable results of regression analysis may be derived by these countermeasures from the viewpoint of prediction, they may yield a seriously biased estimate of the causal effect. Nanmo and Kuroki (2022) proposed partially adaptive $\\mathrm { L } _ { p }$ -penalized multiple regression analysis $( { \\mathrm { P A L } } _ { p } { \\mathrm { M A } } )$ based on the back-door criterion to overcome these drawbacks. However, because of the formulation of $\\mathrm { P A L } _ { p } \\mathrm { M A }$ , this method is not applicable to situations where a sufficient set of confounders is not available. In addition, $\\mathrm { P A L } _ { p } \\mathrm { M A }$ selects a set of covariates to derive a consistent or less biased estimator of causal effects but does not consider the estimation accuracy of the causal effects.\n\n# Contributions\n\nFor cases in which the data-generating process for random variables can be described with a linear structural equation model, we consider a situation where (i) a set of covariates satisfying the back-door criterion cannot be observed or (ii) such a set can be observed, but standard statistical estimation methods cannot be applied to estimate causal effects because of the multicollinearity/high-dimensional data problem. Then, we propose a novel two-stage penalized regression approach, the penalized covariate-mediator selection operator (PCM Selector), to estimate causal effects. In addition to the desirable properties of $\\mathrm { P A L } _ { p } \\mathrm { M A }$ , PCM Selector also has the following properties:\n\n(i) Cox (1960) noted that introducing intermediate variables enables us to improve the estimation accuracy of the regression coefficients in some situations. However, Cox’s consideration was not used in formulating $\\mathrm { P A L } _ { p } \\mathrm { M A }$ , LASSO, and other penalized regression analyses. In contrast, based on Cox’s consideration, PCM Selector selects covariates and intermediate variables to evaluate the causal effects with better estimation accuracy than $\\mathrm { P A L } _ { p } \\mathrm { M A }$ and other penalized regression analyses.\n\n(ii) PCM Selector without intermediate variables is consistent with $\\mathrm { P A L } _ { p } \\mathrm { M A }$ . In this sense, PCM Selector is considered a generalization of $\\mathrm { P A L } _ { p } \\mathrm { M A }$ , and thus provides a wider class including LASSO and adaptive LASSO. In addition, to our knowledge, there has been much less discussion of the selection problem for intermediate variables in the context of penalized regression analysis. In contrast, PCM Selector selects intermediate variables in the context of penalized regression analysis.\n\nFrom these properties, PCM Selector contributes to solving the multicollinearity/high-dimensional data problems of evaluating causal effects in statistical causal inference. Given the space constraints, the proofs, several numerical experiments, and a case study are provided in the Technical Appendix.\n\n# Linear Structural Causal Model\n\nIn statistical causal inference, a directed acyclic graph (DAG) representing cause-effect relationships (datagenerating process) among random variables is called a causal diagram. A directed graph is a pair $G = ( V , { \\pmb E } )$ , where $V$ is a finite set of vertices and the set $\\pmb { { \\cal E } }$ of directed arrows is a subset of the set $V \\times V$ of ordered pairs of distinct vertices $( V _ { i }  V _ { j } $ for $( V _ { i } , V _ { j } ) \\in V { \\times } V )$ . In this paper, we interchangeably refer to vertices in the DAG and random variables of the linear structural equation model. In addition, we refer readers to Pearl (2009) for the graph-theoretic terminology and basic theory of structural causal models used in this paper.\n\nDefinition 1 (Linear Structural Causal Model) Suppose a directed acyclic graph (DAG) $G = ( V , { \\pmb { E } } )$ with a set $V =$ $\\{ V _ { 1 } , V _ { 2 } , \\cdots , V _ { q _ { v } } \\}$ of continuous random variables is given. The $D A G ~ G$ is called a causal diagram when each childparent family in $G$ represents a linear structural equation model\n\n$$\nV _ { i } = \\mu _ { v _ { i } } + \\sum _ { V _ { j } \\in p a ( V _ { i } ) } \\alpha _ { v _ { i } v _ { j } } V _ { j } + \\epsilon _ { v _ { i } } , i = 1 , 2 , \\ldots , q _ { v } ,\n$$\n\nwhere $p a ( V _ { i } )$ denotes a set of parents of $V _ { i }$ in $D A G G$ and random disturbances $\\epsilon _ { v 1 }$ , $\\epsilon _ { v _ { 2 } } , \\ldots , \\epsilon _ { v _ { q v } }$ . . , ϵv are assumed to be independently distributed with mean 0 and constant variance. In addition, $\\mu _ { v _ { i } }$ is an intercept, and $\\alpha _ { v _ { i } v _ { j } } ( \\neq 0 )$ is called $a$ direct effect of $V _ { j }$ on $V _ { i }$ $( i , j = 1 , 2 , \\ldots , \\dot { q } _ { v } ; i \\neq j )$ ). Then, equation $( l )$ is called a linear structural causal model (linear SCM) in this paper.\n\nThe linear SCM is a parametric version of Pearl’s nonparametric structural causal model (PCM).\n\nTo proceed with our discussion, we define some notation. For univariate variables $X$ and $Y$ and a set of variables $z$ , let $\\sigma _ { x y . z }$ and $\\sigma _ { x x . z }$ be the conditional covariance between $X$ and $Y$ given $z \\ = \\ z$ and the conditional variance of $X$ given $z = z$ , respectively. Then, the regression coefficient of $X$ in the single linear regression model of $Y$ on $X$ and $z$ is denoted by $\\beta _ { y x . z } ~ = ~ \\sigma _ { x y . z } / \\sigma _ { x x . z }$ . For sets of variables $x , Y$ , and $z$ ( $\\boldsymbol { Y }$ can be univariate), let $\\Sigma _ { x y . z }$ and $\\Sigma _ { x x . z }$ be the conditional cross-covariance matrix between $X$ and $\\boldsymbol { Y }$ given $z = z$ and the conditional variancecovariance matrix of $X$ given $z = z$ , respectively. Then, the regression coefficient vector of $X$ in the (single/joint) linear regression model of $\\boldsymbol { Y }$ on $X$ and $z$ is denoted by $B _ { y x . z } ~ = ~ \\Sigma _ { x x . z } ^ { - 1 } \\Sigma _ { x y . z }$ . In particular, for univariate $Y$ and $\\pmb { X } = \\{ X _ { 1 } , X _ { 2 } , . . . , \\tilde { X } _ { q _ { x } } \\}$ , the $i$ -th element of $\\boldsymbol { B } _ { y x . z }$ is denoted by $\\beta _ { y x _ { i } . x z }$ for $i = 1 , 2 , \\cdots , q _ { x }$ . For univariate $X$ and $\\pmb { Y } = \\{ Y _ { 1 } , \\bar { Y } _ { 2 } , . . . , Y _ { q _ { u } } \\}$ , the $i$ -th element of $B _ { y x . z }$ is denoted by $\\beta _ { y _ { i } x . z }$ for $i = 1 , 2 , \\cdots , q _ { y }$ . The set of variables $z$ is omitted from these arguments if it is an empty set. A similar notation is used for the remaining statistical parameters.\n\nThe main purpose of this paper is to estimate the total effects from observed data in the context of linear SCMs. The total effect $\\tau _ { y x }$ of $X$ on $Y$ is defined as the total sum of the products of the direct effects on the sequence of directed arrows along all the directed paths from $X$ to $Y$ . To achieve our aim, we introduce the back-door and front-door-like criteria $( \\mathrm { P e a r l } 2 0 0 9 )$ as the representative identification conditions for the total effects. Here, when causal effects, such as direct, indirect, and total effects, can be determined uniquely from the variance/covariance parameters of observed variables, they are said to be identifiable; that is, they can be estimated consistently. Note that direct and indirect effects are also known as representative causal effects in the context of the linear SCM. However, we are concerned with the evaluation of the total effects using intermediate variables because (i) the direct effect can be discussed in the framework of $\\mathrm { P A L } _ { p } \\mathrm { M A }$ (Nanmo and Kuroki 2021) through the “single-door criterion” (Pearl 2009), and PCM Selector is a generalization of $\\mathrm { P A L } _ { p } \\mathrm { M A }$ , and (ii) the problem of evaluating the indirect effects is within the scope of PCM Selector in some situations. Here, the indirect effect of $X$ on $Y$ is defined as the sum of the products of the direct effects on the sequence of directed arrows along the directed paths of interest from $X$ to $Y$ , excluding the direct effect of $X$ on $Y$ .\n\nDefinition 2 (Back-Door Criterion) Let $\\{ X , Y \\}$ and $z$ be disjoint subsets of $V$ in $D A G G ,$ , where $X$ is a nondescendant of $Y$ . If a set $z$ of vertices satisfies the following conditions relative to an ordered pair $( X , Y )$ , then $z$ is said to satisfy the back-door criterion relative to $( X , Y )$ .\n\n(i) No vertex in $z$ is a descendant of $X$ ; and   \n(ii) $z$ d-separates $X$ from $Y$ in the DAG obtained by deleting all the directed arrows emerging from $X$ from the $D A G G$ .\n\nIf a set $z$ of observed variables satisfies the back-door criterion relative to $( X , Y )$ in a causal diagram $G$ , then the total effect $\\tau _ { y x }$ is identifiable and is given by the formula $\\beta _ { y x . z }$ (Pearl 2009). As seen from Rule 2 (Action/observation exchange) of do-calculus (Pearl 2009), note that $X$ and $Y$ of Definition 2 can be generalized to sets of variables $X$ and $\\boldsymbol { Y }$ , respectively. Here, a covariate is defined as an element of the nondescendants of $X$ and $Y$ . In addition, a set of covariates is called a sufficient set of confounders if it satisfies the back-door criterion; otherwise, it is called an insufficient set of confounders.\n\nDefinition 3 (Front-Door-Like Criterion) Let $\\{ X , Y \\}$ , $\\boldsymbol { S }$ , $\\boldsymbol { Z } _ { 1 } \\cup \\boldsymbol { Z } _ { 2 }$ be disjoint subsets of $V$ in the $D A G ~ G$ , where $X$ is a nondescendant of $Y$ . If a set $s$ of vertices satisfies the following conditions relative to an ordered pair $( X , Y )$ together with $\\boldsymbol { Z } _ { 1 } \\cup \\boldsymbol { Z } _ { 2 }$ , then $\\boldsymbol { s }$ is said to satisfy the frontdoor-like criterion relative to $( X , Y )$ with $\\boldsymbol { Z } _ { 1 } \\cup \\boldsymbol { Z } _ { 2 }$ .\n\n(i) $s$ intercepts all the directed paths from $X$ to $Y$ ;   \n(ii) $\\scriptstyle { Z _ { 1 } }$ satisfies the back-door criterion relative to $( X , S )$ ; and   \n(iii) $Z _ { 2 } \\cup \\{ X \\}$ satisfies the back-door criterion relative to $( S , Y )$ .\n\nIf a set $\\boldsymbol { s }$ of observed variables satisfies the front-door-like criterion relative to $( X , Y )$ with $\\boldsymbol { Z } _ { 1 } \\cup \\boldsymbol { Z } _ { 2 }$ in a causal diagram $G$ , then the total effect $\\tau _ { y x }$ is identifiable and is given by the formula $B _ { s x . z _ { 1 } } B _ { y s . x z _ { 2 } }$ . The front-door-like criterion is considered an extended version of the front-door criterion (Pearl 2009) since it is consistent with the front-door criterion when $\\boldsymbol { Z } _ { 1 } \\cup \\boldsymbol { Z } _ { 2 }$ is empty.\n\nHere, an intermediate variable relative to $( X , Y )$ is defined as one that is a descendant of $X$ and an ancestor of $Y$ simultaneously. In addition, a set of intermediate variables is\n\nZ4 Z1 。 2 Z 。 。 .Z6 Z Z6 C   \nx . Z7 X   \nS C . Z S   \ns Z s s S H ： O Y C Z10 s s ? S S5 Setting (a) Setting (b)\n\ncalled a sufficient set if it satisfies the front-door-like criterion; otherwise, it is called an insufficient set of intermediate variables.\n\n# PCM Selector\n\n# Problem Setting\n\nIn this paper, we partition a set of observed variables into the following three disjoint sets:\n\n(i) $\\{ X , Y \\}$ : $X$ and $Y$ are the treatment and response variables, respectively.   \n(ii) $C = Z \\cup { \\overline { { Z } } }$ ( $\\mathbf { \\cdot } \\mathbf { \\vec { C } } \\mathbf { \\cdot } \\mathbf { \\vec { \\mathbf { \\sigma } } }$ for covariates): a set of covariates satisfying the back-door criterion relative to $( X , Y )$ $z \\cap { \\overline { { z } } }$ is empty), where $z$ and $\\overline { { Z } }$ are the first $q _ { z }$ components and the next $q _ { \\overline { { z } } }$ components of $C$ , respectively. Here, $z$ is a subset including some covariates selected using prior causal knowledge $z$ may be an empty set, a sufficient set of confounders, or an insufficient set of confounders), but $\\overline { { Z } }$ is a subset of covariates for which it is uncertain which element of $\\overline { { Z } }$ should be added to evaluate the total effects.   \n(iii) $M = S \\cup { \\overline { { S } } }$ (‘M’ for intermediate variables): a set of intermediate variables satisfying the front-door-like criterion relative to $( X , Y )$ with $C$ ( $S \\cap { \\overline { { S } } }$ is empty), where $s$ and $\\overline { { S } }$ are the first $q _ { s }$ components and the next $q _ { \\overline { { s } } }$ components of $M$ , respectively. Here, $s$ is a subset including some intermediate variables selected using prior causal knowledge ( $\\boldsymbol { s }$ may be an empty set, a sufficient set of intermediate variables, or an insufficient set of intermediate variables), but $\\overline { S }$ is a subset for which it is uncertain which element of $\\overline { { S } }$ should be added to evaluate the total effects.\n\nThen, for sample size $n$ , consider the following joint linear regression model of $\\{ Y \\} \\cup M$ :\n\n$$\n\\begin{array} { c } { { { \\pmb y } = { \\pmb x } \\beta _ { y x . c m } + c B _ { y c . x m } + m B _ { y m . x c } + \\epsilon _ { y . x c m } , } } \\\\ { { { \\pmb m } = { \\pmb x } B _ { m x . c } + { \\pmb c } B _ { m c . x } + \\epsilon _ { m . x c } , } } \\end{array}\n$$\n\nwhere $\\scriptstyle { \\pmb x }$ and $_ y$ represent $n$ -dimensional observation vectors of $X$ and $Y$ , respectively. $c$ and $\\mathbf { \\nabla } _ { m }$ are an $n \\times ( q _ { z } + q _ { \\overline { { { z } } } } )$ observation matrix of $C$ and an $n \\times ( q _ { s } + q _ { \\overline { { s } } } )$ observation matrix of $M$ , respectively. Here, $\\scriptstyle { \\mathbf { } } ( { \\mathbf { } } _ { } , { \\mathbf { } } _ { } )$ , $c$ and $\\mathbf { \\nabla } m$ are standardized to sample mean 0 and sample variance 1 in advance. In addition, we assume that the elements of the random error vector $\\epsilon _ { y . x c m }$ are independent and identically distributed with mean 0 and finite variance $\\sigma _ { y y . x c m }$ . Furthermore, the column vectors of the random error matrix $\\epsilon _ { m . x c }$ are independent and identically distributed with zero mean vector and variance-covariance matrix $\\Sigma _ { m m . x c }$ for $M \\in M$ and are also independent of the elements of ϵy.xcm.\n\nUnder the above setting, this paper focuses on situations where the sum-of-squares matrix of $\\{ X \\} \\cup S \\cup Z$ is invertible but that of $\\{ X \\} \\cup C \\cup M$ is not; this is because if it is invertible, then the total effect is estimable by the OLS method (Pearl 2009).\n\n# Estimator\n\nFor univariates $X$ and $Y$ and a set of variables $z$ , let $s _ { x x . z }$ and $s _ { x y . z }$ be the sum-of-squares of $X$ given $z$ and the sum of cross-products between $X$ and $Y$ given $z$ , respectively. In addition, for sets of variables $x , Y$ , and $z$ $\\mathbf { \\Delta } _ { Y }$ can be univariate), let $S _ { x x . z }$ and $S _ { x y . z }$ be the sum-of-squares matrix of $X$ given $z$ and the sum-of-cross-products matrix between $X$ and $\\boldsymbol { Y }$ given $z$ , respectively. Here, the set of variables $z$ is omitted from these arguments if it is an empty set. A similar notation is used for the remaining sums of squares/cross-products. Furthermore, $\\mathbf { 0 } _ { q } , \\mathbf { 0 } _ { q , r } , \\mathbf { 1 } _ { q }$ and $I _ { q }$ are a $q$ -dimensional zero vector, a $q \\times r$ zero matrix, a $q$ - dimensional one vector, and a $q \\times q$ identity matrix, respectively.\n\nThen, the proposed penalized regression approach, PCM Selector, is formulated as follows:\n\nFirst, when the sum-of-squares matrix of $\\{ X \\} \\cup C \\cup M$ is invertible, let\n\n$$\n\\begin{array} { r } { \\hat { \\beta } _ { y x . c m } = s _ { x y . c m } / s _ { x x . c m } , \\hat { B } _ { y c . x m } = S _ { c c . x m } ^ { - 1 } S _ { c y . x m } , } \\\\ { \\hat { B } _ { y m . x c } = S _ { m m . x c } ^ { - 1 } S _ { m y . x c } , \\quad \\quad } \\end{array}\n$$\n\nand when the sum-of-squares matrix of $\\{ X \\} \\cup C \\cup M$ is not invertible, let\n\n$$\n\\begin{array} { r l } & { \\left( \\tilde { \\beta } _ { y x . c m . } \\tilde { B } _ { y s . x c \\bar { s } } , \\tilde { B } _ { y z . x m \\bar { z } } , \\tilde { B } _ { y \\bar { s } . x c s } , \\tilde { B } _ { y \\bar { z } . x m z } \\right) ^ { T } } \\\\ & { = \\left( \\begin{array} { c c c c c } { n \\lambda + s _ { x x } } & { S _ { x s } } & { S _ { x z } } & { S _ { x \\bar { s } } } & { S _ { x \\bar { z } } } \\\\ { S _ { s x } } & { S _ { s s } } & { S _ { s z } } & { S _ { s \\bar { s } } } & { S _ { s \\bar { z } } } \\\\ { S _ { z x } } & { S _ { z s } } & { S _ { z z } } & { S _ { z \\bar { s } } } & { S _ { z \\bar { z } } } \\\\ { S _ { s x } } & { S _ { \\bar { s } s } } & { S _ { \\bar { s } z } } & { n \\lambda I _ { q \\bar { s } } + S _ { \\bar { s } s } } & { S _ { \\bar { s } \\bar { z } } } \\\\ { S _ { \\bar { z } x } } & { S _ { \\bar { z } _ { s } } } & { S _ { \\bar { z } _ { z } } } & { S _ { \\bar { z } \\bar { s } } } & { n \\lambda I _ { q \\bar { z } } + S _ { \\bar { z } \\bar { z } } } \\end{array} \\right) ^ { T } } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ &  \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ \\end{array}\n$$\n\nfor the penalty parameter $\\lambda > 0$ , where $S _ { y x . z } = S _ { x y . z } ^ { T }$ Sxy.z and the superscript $\\mathbf { \\nabla } ^ {  \\mathbf { \\sigma } } T ^ { \\mathbf { \\vec { \\sigma } } }$ represents the transposed vector/matrix.\n\nHere, equation (5) is consistent with equation (4) for $\\lambda = 0$ . In addition, when the sum-of-squares matrix of $\\{ X \\} \\cup C$ is not invertible, let\n\n$$\n{ \\left( \\begin{array} { l } { { \\tilde { B } } _ { m x . c } } \\\\ { { \\tilde { B } } _ { m z . x { \\overline { { z } } } } } \\\\ { { \\tilde { B } } _ { m { \\overline { { z } } } . x z } } \\end{array} \\right) } = { \\left( \\begin{array} { l l l } { s _ { x x } } & { S _ { x z } } & { S _ { x { \\overline { { z } } } } } \\\\ { S _ { z x } } & { S _ { z z } } & { S _ { z { \\overline { { z } } } } } \\\\ { S _ { { \\overline { { z } } } x } } & { S _ { { \\overline { { z } } } z } } & { n { \\rho } I _ { q _ { \\overline { { z } } } } + S _ { { \\overline { { z } } } z } } \\end{array} \\right) } ^ { - 1 } { \\left( \\begin{array} { l } { S _ { x m } } \\\\ { S _ { z m } } \\\\ { S _ { { \\overline { { z } } } m } } \\end{array} \\right) }\n$$\n\nfor the penalty parameter $\\rho > 0$ . For $p = 1 , 2$ , consider the $L _ { p }$ -penalized loss function\n\n$$\n\\begin{array} { l } { { \\cal L } _ { p } ( \\beta _ { y x . c m } , B _ { y c . x m } , B _ { y m . x c } ) } \\\\ { = \\displaystyle \\frac 1 { 2 n } | | y - x \\beta _ { y x . c m } - c B _ { y c . x m } - m B _ { y m . x c } | | _ { 2 } ^ { 2 } } \\\\ { + \\lambda _ { p } \\left( \\zeta _ { p } | | \\beta _ { y x . c m } | | _ { p } ^ { p } + \\xi _ { p } | | \\gamma _ { \\overline { { s } } x . c } \\odot B _ { y \\overline { { s } } . x c s } | | _ { p } ^ { p } \\right. } \\\\ { \\left. + ( 1 - \\zeta _ { p } - \\xi _ { p } ) | | \\gamma _ { y \\overline { { z } } . x m z } \\odot B _ { y \\overline { { z } } . x m z } | | _ { p } ^ { p } \\right) } \\end{array}\n$$\n\nfor the tuning parameters $\\zeta _ { p } \\geq 0$ and $\\xi _ { p } \\ge 0$ such that $\\zeta _ { p } +$ $\\xi _ { p } \\in [ 0 , 1 ]$ , the penalty parameter $\\lambda _ { p }$ corresponding to the $L _ { p }$ norm $( \\lambda _ { p } \\ge 0 )$ , and the multivariate response type $L _ { p }$ - penalized loss function\n\n$$\n\\begin{array} { r } { L _ { p } ( B _ { m x . c } , B _ { m c . x } ) = \\displaystyle \\frac { 1 } { 2 n } \\| m - x B _ { m x . c } - c B _ { m c . x } \\| _ { F } ^ { 2 } } \\\\ { + \\rho _ { p } \\| \\mathrm { v e c } ( \\gamma _ { m \\overline { { z } } . x z } \\odot B _ { m \\overline { { z } } . x z } ) \\| _ { p } ^ { p } } \\end{array}\n$$\n\nfor the penalty parameter $\\rho _ { p }$ corresponding to the $L _ { p }$ norm $( \\rho _ { p } \\ge 0 )$ . Here, $\\odot$ , $\\| \\cdot \\| _ { p } ^ { p }$ , and $\\| \\cdot \\| _ { F }$ refer to the Hadamard product, the $L _ { p }$ norm, and the Frobenius norm, respectively. In addition, for $\\overline { { s } } _ { i } \\in \\overline { { S } } ( i \\mathrm { ~ = ~ } 1 , 2 , . . . , q _ { \\overline { { s } } } ) .$ , $\\overline { { z } } _ { i } \\in \\overline { { Z } } ( i \\mathrm { ~ = ~ }$ $1 , 2 , . . . , q _ { \\overline { { z } } } )$ and $m _ { i } \\in M ( i = 1 , 2 , . . . , q _ { m } )$ , the standardized weight vectors $\\gamma _ { \\overline { { s } } x . c }$ and $\\gamma _ { y \\overline { { z } } . x m z }$ and the standardized weight matrix $\\gamma _ { m \\overline { { z } } . x z }$ are given by\n\n$$\n\\begin{array} { l } { \\gamma _ { \\mathrm { S o r e } } = \\displaystyle ( \\frac { \\alpha } { i - 1 } ( \\frac { 1 } { \\tilde { \\beta } _ { \\mathrm { S o r e } } } ) ^ { - 1 } ) ^ { - 1 } } \\\\ { \\displaystyle \\qquad \\times ( \\frac { 1 } { ( \\tilde { \\beta } _ { \\mathrm { S o r e } } ) ^ { 2 } } \\frac { 1 } { | \\tilde { \\beta } _ { \\mathrm { S o r e } } | ^ { 2 } } - \\frac { 1 } { | \\tilde { \\beta } _ { \\mathrm { S o r e } } | ^ { 2 } } - \\frac { 1 } { | \\tilde { \\beta } _ { \\mathrm { S o r e } } | ^ { 2 } } ) ^ { \\alpha } = } \\\\ { \\gamma _ { \\mathrm { S o r e } } = \\displaystyle ( \\frac { \\alpha } { i - 1 } | \\frac { 1 } { \\tilde { \\beta } _ { \\mathrm { S o r e } } | ^ { 2 } } ) ^ { - 1 } } \\\\ { \\displaystyle \\qquad \\times ( \\frac { 1 } { | \\tilde { \\beta } _ { \\mathrm { S o r e } } | ^ { 2 } } - \\frac { 1 } { | \\tilde { \\beta } _ { \\mathrm { S o r e } } | ^ { 2 } } ) ^ { - 1 } - \\frac { 1 } { | \\tilde { \\beta } _ { \\mathrm { S o r e } } | ^ { 2 } } ) ^ { \\alpha } \\ : \\ell } \\\\ { \\gamma _ { \\mathrm { S o r e } } = \\displaystyle [ ( \\sum _ { \\mathrm { S o r e } } \\sum _ { \\mathrm { i } = \\mathrm { I } } ^ { \\infty } \\frac { 1 } { | \\tilde { \\beta } _ { \\mathrm { S o r e } } | ^ { 2 } } ) ^ { - 1 } \\frac { 1 } { | \\tilde { \\beta } _ { \\mathrm { S o r e } } | ^ { 2 } } ] _ { \\mathrm { S o r e } } \\zeta _ { \\mathrm { S o r e } } \\Bigg ) ^ { \\alpha } } \\end{array}\n$$\n\nrespectively, where $| \\cdot |$ refers to the absolute value, and the vec operator, $\\operatorname { v e c } ( A )$ , denotes the vectorization of an $q \\times r$ matrix $A$ , which is the $q \\times r$ -dimensional vector obtained by stacking the columns of matrix $A$ on top of one another. Equation (7) is different from the standard penalized loss function in the following ways:\n\n(i) The penalty parameter $\\lambda _ { p }$ is not assigned to $B _ { y z . x m \\overline { { z } } }$ and $B _ { y s . x c \\overline { { s } } }$ in equation (7) in order not to remove covariates $( Z )$ and intermediate variables $( S )$ selected using prior causal knowledge.\n\n(ii) The weight vector constructed by $\\tilde { B } _ { \\overline { { s } } x . c }$ of $\\tilde { B } _ { m x . c } =$ $( \\tilde { B } _ { s x . c } , \\bar { \\tilde { B } } _ { \\overline { { s } } x . c } )$ , but not that constructed by $\\tilde { B } _ { y \\overline { { s } } . x c s }$ , is assigned to $B _ { y \\overline { { s } } . x c s }$ . Equation (9) shows that the indirect effect of $X$ on $Y$ decreases via $\\overline { { S } } _ { i } ~ \\in ~ \\overline { { S } }$ to zero when $\\boldsymbol { B } _ { \\overline { { s } } _ { i } x . c }$ approaches zero.   \n(iii) Standardizing each weight vector enable us to fairly select covariates and intermediate variables in order of priority.\n\nFor $p = 1$ , $\\beta _ { y x . c m }$ , $\\boldsymbol { B _ { y c . x m } }$ and $B _ { y m . x c }$ , which minimize equation (7), and $B _ { m x . c }$ and $B _ { m c . x }$ , which minimize equa$\\check { B } _ { y c . x m } ^ { \\dagger } , \\check { B } _ { y m . x c } ^ { \\dagger } , \\check { B } _ { m x . c } ^ { \\dagger } ,$ , aenstdi $\\check { B } _ { m c . x } ^ { \\dagger }$ rensoptecdt vbeyl $\\check { \\beta } _ { y x . c m } ^ { \\dagger }$ equation (7) is consistent with the partially adaptive $L _ { p }$ - penalized loss function given by Nanmo and Kuroki (2022) when $\\zeta _ { p }$ and $\\xi _ { p }$ respectively are zero and $M$ is an empty set, PCM Selector is considered a generalization of $\\mathrm { P A L } _ { p } \\mathrm { M A }$ . Under the assumption that the sum-of-squares matrix of $\\{ X \\} \\cup C \\cup M$ is invertible, letting $\\lambda _ { p } = 0$ , $\\beta _ { y x . c m }$ , $B _ { y c . x m }$ and $B _ { y m . x c }$ , which minimize equation (7), are given by the OLS estimators, i.e., equation (4). In addition, Let $p = 2$ , $\\lambda _ { 2 } ~ = ~ 3 \\lambda ~ > ~ 0 , ~ \\zeta _ { p } ~ = ~ \\bar { 1 } / 3 , ~ \\xi _ { p } ~ = ~ 1 / 3 , ~ \\gamma _ { \\overline { { { s } } } x . c } ~ = ~ \\bar { 1 } _ { q _ { \\overline { { { s } } } } }$ and $\\gamma _ { y \\overline { { { z } } } . { x m z } } = \\mathbf { 1 } _ { q \\overline { { { z } } } }$ . Then, $\\beta _ { y x . c m }$ , $B _ { y m . x c }$ and $B _ { y c . x m }$ , which minimize equation (7), are given by the ridge-type estimators in equation (5).\n\nHere, in order to avoid confusion by the notation in the following discussion, regarding equations (7) and (8) for $\\textit { p } = \\textit { 1 }$ , let $\\{ X \\}$ , $\\overline { { S } }$ and $\\overline { { Z } }$ be active sets for a given $\\lambda _ { 1 } , \\rho _ { 1 } ~ > ~ 0$ , which is a subset of variables with nonzero regression coefficients that do not include any elements of $z \\cup s$ . In addition, let $q _ { \\overline { { s } } }$ and $q _ { \\overline { { z } } }$ be the numbers of variables in the active sets $\\overline { { S } }$ and $\\overline { z }$ , respectively. Then, under the assumption that the sum-of-squares matrix of explanatory variables $\\{ X \\} \\cup C \\cup M$ is invertible, when $X$ is active, $\\check { \\beta } _ { y x . c m } ^ { \\dagger } , \\check { B } _ { y s . x c \\overline { { { s } } } } ^ { \\dagger } , \\tilde { B } _ { y \\overline { { { s } } } . x c s } ^ { \\dagger }$ and $\\check { B } _ { m x . c } ^ { \\dagger }$ are given by\n\n$$\n\\begin{array}{c} \\begin{array} { r l } & { \\left( \\breve { \\beta } _ { y x . c o } ^ { \\dagger } , \\breve { B } _ { y s . x c \\overline { { s } } } ^ { \\dagger } , \\breve { B } _ { y \\overline { { s } } . x c s } ^ { \\dagger } \\right) ^ { T } = \\left( \\hat { \\beta } _ { y x . c o } , \\hat { B } _ { y s . x c \\overline { { s } } } , \\hat { B } _ { y \\overline { { s } } . x c s } \\right) ^ { T } } \\\\ & { \\qquad + n \\lambda _ { 1 } \\left( \\begin{array} { c c } { - 1 } & { \\hat { B } _ { \\overline { { s } } . x c } } \\\\ { \\hat { B } _ { x s . c \\overline { { s } } } } & { \\hat { B } _ { \\overline { { s } } . x c } } \\end{array} \\hat { B } _ { \\overline { { z } } . x z } \\right.} \\\\ { \\qquad \\hat { B } _ { x \\overline { { s } } . c s } } & { - I _ { q _ { \\overline { { s } } } } } \\end{array}   \\\\ & { \\qquad \\times \\left( \\begin{array} { c c } { \\zeta _ { 1 } s _ { x - x . c e } ^ { - 1 } \\mathrm { s i g n } ( \\breve { \\beta } _ { y x . c o } ^ { \\dagger } ) } \\\\ { \\xi _ { 1 } S _ { s - x . c s } ^ { - 1 } \\mathrm { s i g n } ( \\breve { B } _ { y s . x c s } ^ { \\dagger } ) } \\\\ { \\qquad \\xi _ { 1 } S _ { s - x . c s } ^ { - 1 } \\mathrm { s i g n } ( \\breve { B } _ { y \\overline { { s } } . x c s } ^ { \\dagger } ) } \\\\ { \\qquad ( ( 1 - \\zeta _ { 1 } - \\xi _ { 1 } ) \\zeta _ { \\overline { { s } } . x m . z } ^ { - 1 } \\gamma _ { y \\overline { { z } } . x m . z } \\odot \\mathrm { s i g n } ( \\breve { B } _ { y \\overline { { z } } . x m . z } ^ { \\dagger } ) } \\end{array} \\right) , ( 1 2 ) } \\end{array}\n$$\n\n$$\n\\begin{array} { r l } & { \\check { B } _ { m x . c } ^ { \\dagger } = \\hat { B } _ { m x . c } } \\\\ & { \\qquad + n \\rho _ { 1 } \\hat { B } _ { \\overline { { z } } x . z } S _ { \\overline { { z } } \\overline { { z } } . x z } ^ { - 1 } \\gamma _ { m \\overline { { z } } . x z } \\odot \\mathrm { s i g n } ( \\check { B } _ { m \\overline { { z } } . x z } ^ { \\dagger } ) , } \\end{array}\n$$\n\nwhere\n\n$$\n\\begin{array} { r l } & { \\hat { B } _ { \\overline { { s } } x . s c } = s _ { x x . s c } ^ { - 1 } S _ { x \\overline { { s } } . s c } , \\quad \\hat { B } _ { \\overline { { z } } x . z m } = s _ { x x . z m } ^ { - 1 } S _ { x \\overline { { z } } . z m } , } \\\\ & { \\hat { B } _ { x s . c \\overline { { s } } } = S _ { s s . c \\overline { { s } } } ^ { - 1 } S _ { s x . x \\overline { { s } } } , \\quad \\hat { B } _ { \\overline { { s } } s . x c } = S _ { s s . x c } ^ { - 1 } S _ { s \\overline { { s } } . x c } , } \\\\ & { \\hat { B } _ { \\overline { { z } } s . x z \\overline { { s } } } = S _ { s s . x z \\overline { { s } } } ^ { - 1 } S _ { s \\overline { { z } } . x z \\overline { { s } } } , \\quad \\hat { B } _ { x \\overline { { s } } . c s } = S _ { \\overline { { s } } . c s } ^ { - 1 } S _ { \\overline { { s } } x . c s } , } \\\\ & { \\hat { B } _ { \\overline { { z } } s . x s z } = S _ { \\overline { { s } } \\underline { { 1 } } x s z } ^ { - 1 } S _ { \\overline { { s } } z . x s z } , \\quad \\hat { B } _ { y s . x c \\overline { { s } } } = S _ { s s . x z \\overline { { s } } } ^ { - 1 } S _ { y s . x z \\overline { { s } } } , } \\\\ & { \\hat { B } _ { y \\overline { { s } } . x c s } = S _ { \\overline { { s } } . x z s } ^ { - 1 } S _ { y \\overline { { s } } . x z s } } \\end{array}\n$$\n\nIn addition, for a $q \\times r$ matrix $A \\ = \\ ( a _ { i j } ) _ { 1 \\leq i \\leq q , 1 \\leq j \\leq r }$ , $\\operatorname { s i g n } ( A ) = ( \\operatorname { s i g n } ( a _ { i j } ) ) _ { 1 \\leq i \\leq q , 1 \\leq j \\leq r }$ , where\n\n$$\n\\mathrm { s i g n } ( a _ { i j } ) = \\left\\{ \\begin{array} { c l } { { 1 } } & { { a _ { i j } > 0 } } \\\\ { { 0 } } & { { a _ { i j } = 0 } } \\\\ { { - 1 } } & { { a _ { i j } < 0 } } \\end{array} \\right.\n$$\n\nfor $i = 1 , 2 , . . . , q$ , $j = 1 , 2 , . . . , r$ . When $X$ is not active, $\\breve { \\beta } _ { y x . c m } ^ { \\dagger }$ is evaluated as zero. In addition, $\\check { B } _ { y s . c \\overline { { { s } } } } ^ { \\dagger }$ ˇy†s.cs and Bˇy† $\\check { B } _ { y \\overline { { s } } . c s } ^ { \\dagger }$ are obtained by omitting the subscript $x$ in equation (12) exaginvde cept for $s _ { x x . c m } ^ { - 1 }$ $\\gamma _ { \\overline { { s } } x . c }$ x.uwcaitihonze(r9o)s iengaerqduleastisoon (1w2h)e. hNeort and replacing $\\hat { B } _ { \\overline { { s } } x . s c }$ , $\\hat { B } _ { \\overline { { z } } x . z m }$ , $\\hat { B } _ { x s . c \\overline { { { s } } } }$ hisa , Bˆxs.cs $\\gamma _ { \\overline { { s } } x . c }$ oisr $X$ not.\n\nHere, for $\\lambda _ { 2 } , \\rho _ { 2 } , \\rho _ { 2 } ^ { \\prime } \\ge 0$ and $\\xi _ { 2 } \\in [ 0 , 1 ]$ , to reduce the bias, based on the derived active sets, the following estimators are considered:\n\n(a) B˜† $\\tilde { B } _ { x c . m } ^ { \\dagger }$ and $\\tilde { B } _ { x m . c } ^ { \\dagger } \\colon B _ { x c . m }$ and $B _ { x m . c }$ that minimize\n\n$$\n\\begin{array} { l } { { \\displaystyle L _ { 2 } ( B _ { x c . m } , B _ { x m . c } ) } } \\\\ { { \\displaystyle \\quad = \\frac { 1 } { 2 n } \\| { \\pmb x } - z B _ { x z . \\overline { { z } } m } - \\overline { { z } } B _ { x \\overline { { z } } . z m } - s B _ { x s . c \\overline { { s } } } - \\overline { { s } } B _ { x \\overline { { s } } . c s } \\| _ { 2 } ^ { 2 } } } \\\\ { { \\displaystyle \\qquad + \\lambda _ { 2 } \\left\\{ \\xi _ { 2 } \\| B _ { x \\overline { { s } } . c s } \\| _ { 2 } ^ { 2 } + ( 1 - \\xi _ { 2 } ) \\| B _ { x \\overline { { z } } . z m } \\| _ { 2 } ^ { 2 } \\right\\} , ( 1 6 ) } } \\end{array}\n$$\n\n(b) B˜† $\\tilde { B } _ { \\overline { { s } } x . c s . } ^ { \\dagger } , \\tilde { B } _ { \\overline { { s } } s . x c } ^ { \\dagger }$ and B˜† $\\tilde { B } _ { \\overline { { s } } c . x s } ^ { \\dagger } \\colon B _ { \\overline { { s } } x . c s } , B _ { \\overline { { s } } s . x c }$ and $\\beta _ { \\overline { { s } } c . x s }$ that minimize\n\n$$\n\\begin{array} { l } { { \\displaystyle L _ { 2 } ( B _ { \\overline { { { s } } } x . c s } , B _ { \\overline { { { s } } } s . x c } , B _ { \\overline { { { s } } } c . x s } ) } } \\\\ { { \\displaystyle \\quad = \\frac { 1 } { 2 n } \\| \\overline { { { s } } } - x B _ { \\overline { { { s } } } x . s c } - s B _ { \\overline { { { s } } } s . x c } - z B _ { \\overline { { { s } } } z . x s \\overline { { { z } } } } - \\overline { { { z } } } B _ { \\overline { { { s z } } } . x s z } \\| _ { F } ^ { 2 } } } \\\\ { { \\displaystyle \\qquad + \\rho _ { 2 } \\| \\mathrm { v e c } \\left( B _ { \\overline { { { s z } } } . x s z } \\right) \\| _ { 2 } ^ { 2 } , \\quad \\left( 1 7 \\right) } } \\end{array}\n$$\n\n(c) B˜† $\\underline { { \\tilde { B } } } _ { z x . z m } ^ { \\dagger } , \\ \\tilde { B } _ { z z . x m . } ^ { \\dagger }$ and $\\tilde { B } _ { z m . x z } ^ { \\dagger }$ : $B _ { \\overline { { z } } x . z m }$ , $B _ { \\overline { { z } } z . x m }$ and $B _ { \\overline { { z } } m . x z }$ thatzzm.xinmimize\n\n$$\n\\begin{array} { l } { { \\displaystyle L _ { 2 } ( B _ { \\overline { { z } } x . z m } , B _ { \\overline { { z } } z . x m } , B _ { \\overline { { z } } m . x z } ) } } \\\\ { { \\displaystyle = \\frac { 1 } { 2 n } \\| \\overline { { z } } - x B _ { \\overline { { z } } x . z m } - z B _ { \\overline { { z } } z . x m } - s B _ { \\overline { { z } } s . x z \\overline { { s } } } - \\overline { { s } } B _ { \\overline { { z } } s . x z s } \\| _ { F } ^ { 2 } } } \\\\ { { \\displaystyle \\qquad + \\rho _ { 2 } ^ { \\prime } \\| \\mathrm { v e c } ( B _ { \\overline { { z s } } . x s z } ) \\| _ { 2 } ^ { 2 } . \\quad \\quad ( 1 8 ) } } \\end{array}\n$$\n\nThen, based on equations (12) and (13), when $X$ is active, consider\n\n$$\n\\begin{array} { r l } & { \\left( \\tilde { \\beta } _ { y x , c m } ^ { * } , \\tilde { B } _ { y s , x c \\overline { { s } } } ^ { * } , \\tilde { B } _ { y \\overline { { s } } , x c s } ^ { * } \\right) ^ { T } = \\left( \\tilde { \\beta } _ { y x , c m } ^ { \\dagger } , \\tilde { B } _ { y s , x c \\overline { { s } } } ^ { \\dagger } , \\tilde { B } _ { y \\overline { { s } } , x c s } ^ { \\dagger } \\right) ^ { T } } \\\\ & { \\qquad - n \\lambda _ { 1 } \\left( \\begin{array} { c c } { - 1 } & { \\tilde { B } _ { \\overline { { s } } , x c } ^ { \\dagger } } \\\\ { \\tilde { B } _ { x s , x \\overline { { s } } } ^ { \\dagger } } & { \\tilde { B } _ { \\overline { { s } } , x c } ^ { \\dagger } } \\end{array} \\right) } \\\\ & { \\qquad - n \\lambda _ { 1 } \\left( \\begin{array} { c c } { - 1 } & { \\tilde { B } _ { \\overline { { s } } , x c } ^ { \\dagger } } \\\\ { \\tilde { B } _ { x s , x c \\overline { { s } } } ^ { \\dagger } } & { \\tilde { B } _ { \\overline { { s } } , x c } ^ { \\dagger } } \\end{array} \\right) } \\\\ &  \\qquad \\times \\left( \\begin{array} { c c } { \\zeta _ { 1 } \\tilde { s } _ { x x , x m } ^ { \\dagger - 1 } \\sin ( \\tilde { \\beta } _ { y x , c m } ^ { \\dagger } ) } & \\\\ { \\xi _ { 1 } \\tilde { S } _ { s s , x c \\overline { { s } } } ^ { \\dagger + 1 } \\sqrt { s } _ { x s , c \\overline { { s } } } \\cos ( \\tilde { B } _ { y \\overline { { s } } , x c } ^ { \\dagger } ) } & \\\\ { \\left( 1 - \\zeta _ { 1 } - \\xi _ { 1 } \\right) \\tilde { S } _ { \\overline { { s } } , x c \\overline { { s } } } ^ { \\dagger + 1 } \\gamma _ { y \\overline { { s } } , x m } ^ { \\dagger - 1 } \\zeta _ { y \\overline { { s } } , x c \\overline { { s } } } ^ { \\dagger } \\right) , ( 1 9 ) } \\end{array} \\end{array}\n$$\n\n$$\n\\begin{array} { r l } & { \\check { B } _ { m x . c } ^ { * } = \\check { B } _ { m x . c } ^ { \\dagger } } \\\\ & { \\qquad - n \\rho _ { 1 } \\hat { B } _ { \\overline { { z } } x . z } \\hat { S } _ { \\overline { { z } } \\overline { { z } } . x z } ^ { + } \\gamma _ { m \\overline { { z } } . x z } \\odot \\mathrm { s i g n } ( \\check { B } _ { m \\overline { { z } } . x z } ^ { \\dagger } ) , } \\end{array}\n$$\n\nwhere $\\mathbf { \\nabla } _ { m }$ and $c$ of ∗mx.c are constructed by both S ∪ Z and a subset of $\\overline { { S } } \\cup \\overline { { Z } }$ corresponding to the active sets of $\\check { B } _ { y \\overline { { s } } . x c s } ^ { \\dagger }$ and Bˇ† $\\check { B } _ { y \\overline { { c } } . x m z } ^ { \\dagger }$\n\n$$\n\\begin{array} { r l } & { \\tilde { s } _ { x x . c m } ^ { \\dagger } = \\Vert { \\pmb x } - c \\tilde { B } _ { x c . m } ^ { \\dagger } - m \\tilde { B } _ { x m . c } ^ { \\dagger } \\Vert _ { 2 } ^ { 2 } , } \\\\ & { \\tilde { S } _ { s s . x c s } ^ { \\dagger } = \\Vert \\overline { { \\pmb x } } - x \\tilde { B } _ { s x . c s } ^ { \\dagger } - s \\tilde { B } _ { \\overline { { s } } s . x c } ^ { \\dagger } - c \\tilde { B } _ { \\overline { { s } } c . x s } ^ { \\dagger } \\Vert _ { G } , } \\\\ & { \\tilde { S } _ { z z . x m z } ^ { \\dagger } = \\Vert \\overline { { z } } - x \\tilde { B } _ { \\overline { { z } } x . c s } ^ { \\dagger } - m \\tilde { B } _ { \\overline { { z } } m . x z } ^ { \\dagger } - z \\tilde { B } _ { \\overline { { z } } z . x m } ^ { \\dagger } \\Vert _ { G } , } \\\\ & { \\hat { S } _ { \\overline { { z } } z . x z } = \\Vert \\overline { { z } } - x \\hat { B } _ { \\overline { { z } } x . z } - z \\hat { B } _ { \\overline { { z } } z . x } \\Vert _ { G } , } \\end{array}\n$$\n\nand $\\| A \\| _ { G }$ and $A ^ { + }$ denote the gram matrix $A ^ { T } A$ and the generalized inverse of a matrix $A$ (Bernstein 2009), respectively. When $X$ is not active, βˇy∗x.cm is evaluated as zero. In addition, Bˇ∗ $\\check { B } _ { y s . c \\overline { { { s } } } } ^ { * }$ and Bˇ∗ $\\check { B } _ { y \\overline { { s } } . c s } ^ { * }$ are obtained by omitting the subscript $x$ from equation (19) except for $\\gamma _ { \\overline { { s } } x . c }$ and replacing $\\tilde { B } _ { \\bar { s } x . s c } ^ { \\dagger } , \\tilde { B } _ { \\bar { z } x . z m } ^ { \\dagger } , \\tilde { B } _ { x s . c \\bar { s } } ^ { \\dagger } , \\tilde { B } _ { x \\bar { s } . c s } ^ { \\dagger }$ B˜† and s˜†x−x.1cm with zeros in equation (19). Note that $\\gamma _ { \\overline { { s } } x . c }$ is given by equation (9) regardless of whether $X$ is active or not.\n\nThen, we formulate the modified PCM estimator of the total effect $\\tau _ { y x }$ as\n\n$$\n\\check { \\tau } _ { y x } ^ { * } = \\check { \\beta } _ { y x . c m } ^ { * } + \\check { B } _ { m x . c } ^ { * } \\check { B } _ { y m . x c } ^ { * }\n$$\n\nwhen $X$ is active according to equation (7) and\n\n$$\n\\check { \\tau } _ { y x } ^ { * } = \\check { B } _ { m x . c } ^ { * } \\check { B } _ { y m . c } ^ { * }\n$$\n\nwhen $X$ is not active according to equation (7). Hereafter, the modified PCM estimator is called the PCM estimator.\n\nRegarding PCM estimators, the following theorems hold:\n\nTheorem 1 For an active set $M \\cup C$ , when the OLS estimators are available, $i f X$ is conditionally independent of $Y$ given $M \\cup C$ , then the following inequalities approximately hold under the normality:\n\n$$\n\\begin{array} { c } { { \\nu a r ( \\check { B } _ { m x . c } ^ { * } \\check { B } _ { y m . c } ^ { * } ) \\leq \\nu a r ( \\hat { B } _ { m x . c } \\hat { B } _ { y m . c } ) \\leq \\nu a r ( \\hat { \\beta } _ { y x . c } ) } } \\\\ { { \\nu a r ( \\check { B } _ { m x . c } ^ { * } \\check { B } _ { y m . c } ^ { * } ) \\leq \\nu a r ( \\check { \\beta } _ { y x . c } ^ { * } ) } } \\end{array}\n$$\n\nfor the optimal tuning and penalty parameters.\n\nThe first inequality is given in the Technical Appendix. The second inequality is shown in Kuroki and Hayashi (2014, 2016). Theorem 1 shows that the estimation accuracy of the total effect can be improved compared to that of the OLS method through PCM Selector based on a set of variables that make $X$ and $Y$ conditionally independent.\n\nTheorem 2 For an active set $M \\cup C$ , when the OLS estimators are available, if $X$ is conditionally independent of $Y$ given $M \\cup C$ and $\\mathbf { \\bar { \\mathbf { \\boldsymbol { M } } } } ^ { \\prime } \\cup \\mathbf { \\boldsymbol { C } }$ , the following inequalities approximately hold under the normality:\n\n$$\n\\begin{array} { r l } & { \\nu a r ( \\check { B } _ { m ^ { \\prime } x . c } ^ { * } \\check { B } _ { y m ^ { \\prime } . c } ^ { * } ) \\leq \\nu a r ( \\hat { B } _ { m ^ { \\prime } x . c } \\hat { B } _ { y m ^ { \\prime } . c } ) } \\\\ & { \\qquad \\leq \\nu a r ( \\hat { B } _ { m x . c } \\hat { B } _ { y m . c } ) } \\end{array}\n$$\n\nfor $M ^ { \\prime } \\subset M$ .\n\nThe first inequality is simply obtained from Theorem 1, and the second inequality is shown in Kuroki and Hayashi (2014, 2016). Theorem 2 provides a statistical guideline for selecting a set of intermediate variables to derive a more efficient estimator of the total effects.\n\n# Numerical Experiment\n\nIn this section, we present a numerical experiment to compare the performances of LASSO, adaptive LASSO, Elastic Net, $\\mathrm { P A L _ { 1 } M A }$ , the OLS method, the two-stage least squares (TSLS) method and PCM Selector. For brevity, consider the linear SCM\n\n$$\n\\left. \\begin{array} { l } { { Y = \\alpha _ { y s } S + \\alpha _ { y z } Z + \\overline { { S } } A _ { y \\overline { { s } } } + \\overline { { Z } } A _ { y \\overline { { z } } } + \\epsilon _ { y } } } \\\\ { { \\overline { { S } } = X A _ { \\overline { { s } } x } + S A _ { \\overline { { s } } s } + Z A _ { \\overline { { s } } z } + \\epsilon _ { \\overline { { s } } } } } \\\\ { { S = \\alpha _ { s x } X + \\alpha _ { s z } Z + \\epsilon _ { s } } } \\\\ { { X = \\alpha _ { x z } Z + \\epsilon _ { x } } } \\end{array} \\right\\}\n$$\n\nfor Figure 1, where $\\overline { { Z } }$ and $\\overline { S }$ include 10 covariates and 5 intermediate variables $( M = \\{ S \\} \\cup { \\overline { { S } } } )$ , respectively. In Figure 1, Setting (a) shows that (i) $S$ satisfies the front-door-like criterion relative to $( X , Y )$ with $Z$ and (ii) $Z$ satisfies the back-door criterion relative to $( X , Y )$ and Setting (b) shows that (i) $\\{ S , { \\overline { { S } } } _ { 1 } \\}$ satisfies the front-door criterion relative to $( X , Y )$ and (ii) $C = \\{ Z , { \\overline { { Z } } } \\}$ satisfies the back-door criterion relative to $( X , Y )$ but is unobserved. Here, $S$ and $\\{ S , { \\overline { { S } } } _ { 1 } \\}$ are the minimally sufficient sets of intermediate variables that satisfies the front-door-like criterion for Setting (a), and satisfies the front-door criterion for Setting (b), respectively.\n\nTo set up the numerical experiment, we first construct the population variance-covariance matrix. To eliminate arbitrariness, the true values of the direct effects are $\\alpha _ { y s } =$ 0.4, $\\begin{array} { c c l } { { \\alpha _ { \\overline { { { s } } } s } } } & { { = } } & { { 0 . 2 ( \\in \\mathrm { ~ \\it ~ { ~ A ~ } ~ } _ { \\overline { { { s } } } s } ) } } \\end{array}$ , $\\alpha _ { \\overline { { s } } _ { 2 } x } , \\alpha _ { \\overline { { s } } _ { 3 } x } , \\alpha _ { \\overline { { s } } _ { 4 } x } , \\alpha _ { \\overline { { s } } _ { 5 } x } ~ ( \\in$ $\\textstyle A _ { \\overline { { s } } x } .$ ) are set to 0 and $\\alpha _ { y \\overline { { { z } } } _ { 1 } } , \\alpha _ { y \\overline { { { z } } } _ { 2 } } , \\ldots , \\alpha _ { y \\overline { { { z } } } _ { 1 0 } } ( \\in \\mathrm { ~ \\it ~ A _ { y \\overline { { { z } } } } ) ~ }$ , $\\alpha _ { y \\overline { { { s } } } _ { 2 } } , \\alpha _ { y \\overline { { { s } } } _ { 3 } } , \\alpha _ { y \\overline { { { s } } } _ { 4 } } , \\alpha _ { y \\overline { { { s } } } _ { 5 } } ( \\in \\ A _ { y \\overline { { { s } } } } )$ are randomly and independently generated according to a uniform distribution on the interval $[ - 0 . 2 , 0 . 2 ]$ in the both settings (a) and (b). The other direct effects are given as follows: Setting (a) $\\alpha _ { x z } = 0 . 8$ , $\\alpha _ { \\overline { { s } } _ { 1 } x } = 0 . 0$ , $\\alpha _ { s x } = 0 . 1$ , $\\alpha _ { y z } = \\alpha _ { s z } = \\alpha _ { \\overline { { s } } z } = 0 . 2$ $( \\alpha _ { \\overline { { s } } z } \\in$ $\\textstyle A _ { \\overline { { s } } z } .$ ), $\\alpha _ { y \\overline { { { s } } } _ { 1 } }$ is randomly generated according to a uniform distribution on the interval $[ - 0 . 2 , 0 . 2 ]$ ; Setting (b) $\\alpha _ { x z } =$ $\\alpha _ { \\overline { { s } } _ { 1 } x } ~ = ~ \\alpha _ { y \\overline { { s } } _ { 1 } } ~ = ~ 0 . 2$ , $\\alpha _ { s x } ~ = ~ 0 . 8$ , $\\alpha _ { y z } = \\alpha _ { s z } = \\alpha _ { \\overline { { { s } } } z } =$ $0 . 0 \\left( \\alpha _ { \\overline { { s } } z } ^ { } \\in A _ { \\overline { { s } } z } ^ { } \\right)$ .\n\nIn addition, we assume that the random disturbances $\\epsilon _ { x }$ $\\epsilon _ { y } , \\epsilon _ { s }$ and $\\epsilon _ { \\overline { { s } } }$ independently follow a normal distribution in which $X , Y , S , { \\overline { { S } } }$ and $C$ are standardized to mean 0 and the unit variance. Furthermore, the population variancecovariance matrix of $C$ is randomly determined according to Pourahmadi and Wang (2015).\n\nWe generated 15 random samples of 18 variables from a multivariate normal distribution with a zero mean vector and the above variance-covariance matrix for 5000 replications. Table 1 shows the basic statistics of the total effects estimated by LASSO, adaptive LASSO, Elastic Net, $\\mathrm { P A L _ { 1 } M A }$ , the OLS method, the TSLS methods, and PCM Selector based on the given penalty and tuning parameters. Here, the TSLS methods are based on front-door-like criterion in Setting (a) and based on front-door criterion in Setting (b). In addition, for the OLS and TSLS methods, we select a set of covariates $C$ in Setting (a). In Setting (b), it is assumed that a set of covariates is not observed, and thus the total effect can not be estimated by using the back-door criterion. Regarding the parameter tuning for LASSO, adaptive LASSO, Elastic Net, $\\mathrm { P A L _ { 1 } M A }$ and PCM Selector, see Section C in the Technical Appendix.\n\nTable 1: Results based on cross-validation. Mean: sample mean; SD: standard deviation; Bias: bias between the true value and the sample mean; Sign: coincidence rate between the signs of the true value and the estimates; Front-door-like (including $x$ ): the treatment variable $X$ , the intermediate variable $S$ and the set of covariates $C$ are used for the front-door-like criterion; Front-door-like (not including $x$ ): the intermediate variable $S$ and the set of covariates $C$ are used for the front-door-like criterion; Back-door: the set of covariates $C$ is used for the back-door criterion; Front-door (minimal): a minimally sufficient set of intermediate variables is used for the front-door criterion. Front-door (whole): the set of intermediate variables $M$ is used for the front-door criterion. $\\lambda$ , $\\lambda _ { 1 } , \\rho _ { 1 }$ : penalty parameters; $\\eta$ : tuning parameter for the adaptive weights $( Z \\mathrm { o u } ~ 2 0 0 6 )$ ; $\\phi$ : tuning parameter for the elastic net penalty (Zou and Hastie 2005); $\\zeta _ { 1 } , \\xi _ { 1 }$ : tuning parameters; $\\lambda = 3 . 1 5 7$ , $\\rho = 6 9 . 4 8 4$ for equations (5) and (6) in Setting (a) and $\\lambda = 3 . 7 2 6$ for equation (6) in Setting (b); $\\tau _ { y x }$ : true value of total effect.   \n\n<html><body><table><tr><td rowspan=\"2\">Setting (a)</td><td colspan=\"4\">TuD = 0.045</td><td colspan=\"7\">parameter setings</td></tr><tr><td>Mean</td><td></td><td></td><td>Sign</td><td>入</td><td>n</td><td></td><td></td><td></td><td>S1</td><td>1</td></tr><tr><td>LASSO</td><td>0.013</td><td>0.045</td><td>-0.033</td><td>0.117</td><td>0.407</td><td>1</td><td></td><td>1</td><td>-</td><td></td><td>、</td></tr><tr><td>adaptive LASSO</td><td>0.017</td><td>0.057</td><td>-0.028</td><td>0.138</td><td>0.407</td><td>0.100</td><td>1</td><td></td><td>-</td><td>-</td><td>1</td></tr><tr><td>Elastic Net</td><td>0.017</td><td>0.054</td><td>-0.028</td><td>0.156</td><td>0.399</td><td></td><td>0.910</td><td></td><td>-</td><td></td><td>1</td></tr><tr><td>PALiMA</td><td>0.054</td><td>0.792</td><td>0.009</td><td>0.528</td><td>0.294</td><td>1.200</td><td></td><td></td><td>=</td><td></td><td></td></tr><tr><td>PCM Selector</td><td>0.036</td><td>0.718</td><td>-0.010</td><td>0.526</td><td>=</td><td></td><td></td><td>0.017</td><td>0.213</td><td>0.270</td><td>0.190</td></tr><tr><td>Front-door-like (including x)</td><td>-0.008</td><td>1.577</td><td>-0.053</td><td>0.515</td><td>=</td><td>=</td><td></td><td>一</td><td>=</td><td></td><td>1</td></tr><tr><td>Front-door-like (not including x)</td><td>0.030</td><td>1.051</td><td>-0.015</td><td>0.524</td><td>-</td><td>-</td><td>1</td><td></td><td>-</td><td>-</td><td></td></tr><tr><td>Back-door</td><td>0.054</td><td>1.591</td><td>0.009</td><td>0.532</td><td></td><td></td><td>1</td><td></td><td>-</td><td></td><td>1</td></tr><tr><td rowspan=\"7\">Setting (b)</td><td rowspan=\"7\"></td><td rowspan=\"7\"></td><td colspan=\"3\"></td><td colspan=\"3\"></td><td rowspan=\"7\"></td><td rowspan=\"7\"></td><td rowspan=\"7\"></td></tr><tr><td></td><td></td><td></td><td></td><td>prameter setigs</td></tr><tr><td>Mean</td><td></td><td>TsD =0.402S</td><td>Sign</td><td>入1</td><td></td></tr><tr><td>PCM Selector</td><td>0.448</td><td>0.549</td><td>0.046 0.808</td><td>0.346</td><td></td><td>0.000 1.000</td></tr><tr><td>Front-door (minimal)</td><td>0.468 0.462</td><td>0.552 0.692</td><td>0.066</td><td>0.818</td><td>1</td><td></td></tr><tr><td>Front-door (whole)</td><td></td><td></td><td>0.060</td><td>0.770</td><td>1</td><td>-</td></tr></table></body></html>\n\nAccording to Table 1, PCM Selector provides better estimation accuracy than $\\mathrm { P A L _ { 1 } M A }$ and the least squares methods. In addition, Table 1 shows that PCM Selector generally provides an estimation that is biased but less biased than the TSLS methods in the present parameter setting. Furthermore, the coincidence rates between the signs of the estimated total effects and the true total effects are low for LASSO, adaptive LASSO, and Elastic Net. This would be serious because it provides a misleading interpretation that the external intervention of the treatment variable $X$ does not have no effect on the change of $Y$ . In contrast, the coincidence rates of PCM Selector and $\\mathrm { P A L _ { 1 } M A }$ are not low.\n\nThe Technical Appendix provides further discussion.\n\n# Conclusion\n\nIn current situations where advanced artificial intelligence technology enables us to collect large datasets, it is not difficult to observe many covariates and intermediate variables. In such situations, it would be reasonable to consider such sets of variables to evaluate total effects. However, it is difficult to evaluate the total effects reliably when multicollinearity/high-dimensional data problems occur in this situation. To solve this problem, we establish PCM Selector, which is considered as a wider class, including adaptive LASSO and $\\mathrm { P A L } _ { p } \\mathrm { M A }$ , to provide a less biased estimator of total effects with better estimation accuracy. In addition, through numerical experiments and a case study in the Technical Appendix, we confirmed that PCM Selector is superior to other methods. Interestingly, there are some situations where the total effect is not identifiable, but the indirect effects are identifiable (Inoue, Ritz, and Arah 2022).\n\nAlthough the current penalized regression analyses are not applicable to such situations, PCM Selector is applicable for evaluating the indirect effect.\n\nFinally, although PCM Selector is formulated based on single/joint linear regression models, it would be interesting to extend our approach to a wide variety of statistical models, including generalized linear models. Such an extension would be straightforward - the loss function would be replaced with a more general form. This extension will be left for future work.\n\n![](images/f008cc0d1935773dcac7b6464d16afc70a1140e3c4a86909dd5f1da6c1441f5d.jpg)  \nFigure 2: Violin plots of estimated total effects. The dashed lines show the true total effects. FDL: Front-door-like.",
    "institutions": [
        "Chugai Pharmaceutical Co., Ltd.",
        "Yokohama National University"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n在使用线性结构方程模型描述随机变量的数据生成过程时，存在两种情况导致难以准确估计因果效应：一是满足后门准则的一组协变量无法被观测；二是虽能观测到这样一组协变量，但由于多重共线性或高维数据问题，无法应用标准统计估计方法。这些问题会降低因果效应估计的准确性，导致制定不可靠的计划，影响政策决策。此外，现有处理多重共线性问题的方法与识别因果效应的问题独立制定，可能产生因果效应的严重偏差估计，而 $\\mathrm { P A L } _ { p } \\mathrm { M A }$ 不适用于没有足够混杂因素集的情况，且未考虑因果效应的估计准确性。\\n\\n**方法概述**\\n论文提出了一种新颖的两阶段惩罚回归方法——惩罚协变量 - 中介选择算子（PCM Selector），用于在上述情况下估计因果效应。\\n\\n**主要贡献与效果**\\n- 提出 PCM Selector 方法，在中间变量可用时，能提供因果效应的一致或偏差较小的估计量。在数值实验中，与 LASSO、自适应 LASSO、Elastic Net、$\\mathrm { P A L } _ { 1 } \\mathrm { M A }$、OLS 方法、TSLS 方法等相比，PCM Selector 提供了更好的估计准确性。在设置（a）中样本均值为 0.036，标准差为 0.718，偏差为 - 0.010，符号一致性为 0.526；在设置（b）中样本均值为 0.448，标准差为 0.549，偏差为 0.046，符号一致性为 0.808。整体上，PCM Selector 一般提供的估计偏差小于 TSLS 方法。\\n- 基于 Cox 的考虑，PCM Selector 选择协变量和中间变量来评估因果效应，其估计准确性优于 $\\mathrm { P A L } _ { p } \\mathrm { M A }$ 和其他惩罚回归分析。\\n- PCM Selector 是 $\\mathrm { P A L } _ { p } \\mathrm { M A }$ 的推广，能够处理 $\\mathrm { P A L } _ { p } \\mathrm { M A }$ 无法适用的情况，即当没有足够的混杂因素集时也能应用。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nPCM Selector 的核心思想是通过两阶段惩罚回归，结合中间变量来估计因果效应。它利用惩罚项对回归系数进行约束，在解决多重共线性和高维数据问题的同时，尽量减少因果效应估计的偏差。其基于 Cox 提出的引入中间变量可提高回归系数估计准确性的观点，通过合理选择协变量和中间变量来实现更准确的因果效应估计。\\n\\n**创新点**\\n先前的惩罚回归分析方法在处理多重共线性问题时，与识别因果效应的问题独立制定，可能会产生因果效应的严重偏差估计。$\\mathrm { P A L } _ { p } \\mathrm { M A }$ 虽能在一定程度上克服这些缺点，但不适用于没有足够混杂因素集的情况，且未考虑因果效应的估计准确性。而 PCM Selector 不仅具有 $\\mathrm { P A L } _ { p } \\mathrm { M A }$ 的理想特性，还基于 Cox 的考虑选择协变量和中间变量，提高了因果效应的估计准确性；同时，它是 $\\mathrm { P A L } _ { p } \\mathrm { M A }$ 的推广，能处理 $\\mathrm { P A L } _ { p } \\mathrm { M A }$ 无法适用的情况。此外，PCM Selector 的惩罚损失函数在设计上有独特之处，如不将惩罚参数分配给使用先验因果知识选择的协变量和中间变量对应的回归系数，通过特定的权重向量构建方式来合理选择变量等。\\n\\n**具体实现步骤**\\n1. **问题设定**：将一组观测变量划分为三个不相交的集合，分别为 $\\{ X , Y \\}$（处理变量和响应变量）、$C = Z \\cup \\overline { { Z } }$（满足后门准则的协变量集合，$z$ 是使用先验因果知识选择的部分协变量子集，$\\overline { { Z } }$ 是不确定哪些元素应添加以评估总效应的协变量子集）和 $M = S \\cup \\overline { { S } }$（满足前门准则的中间变量集合，$s$ 是使用先验因果知识选择的部分中间变量子集，$\\overline { { S } }$ 是不确定哪些元素应添加以评估总效应的中间变量子集）。\\n2. **定义联合线性回归模型**：考虑 $\\{ Y \\} \\cup M$ 的联合线性回归模型 $\\begin{cases} \\pmb y = \\pmb x \\beta _ { y x . c m } + c B _ { y c . x m } + m B _ { y m . x c } + \\epsilon _ { y . x c m } \\\\ \\pmb m = \\pmb x B _ { m x . c } + \\pmb c B _ { m c . x } + \\epsilon _ { m . x c } \\end{cases}$，其中 $\\pmb x$ 和 $\\pmb y$ 分别表示 $X$ 和 $Y$ 的 $n$ 维观测向量，$c$ 和 $\\pmb m$ 分别是 $C$ 和 $M$ 的观测矩阵，且 $c$ 和 $\\pmb m$ 预先标准化为样本均值 0 和样本方差 1。\\n3. **构建估计器**：\\n    - 当 $\\{ X \\} \\cup C \\cup M$ 的平方和矩阵可逆时，$\\hat { \\beta } _ { y x . c m } = s _ { x y . c m } / s _ { x x . c m }$，$\\hat { B } _ { y c . x m } = S _ { c c . x m } ^ { - 1 } S _ { c y . x m }$，$\\hat { B } _ { y m . x c } = S _ { m m . x c } ^ { - 1 } S _ { m y . x c }$。\\n    - 当 $\\{ X \\} \\cup C \\cup M$ 的平方和矩阵不可逆时，通过一系列公式计算 $\\tilde { \\beta } _ { y x . c m }$、$\\tilde { B } _ { y s . x c \\overline { { s } } }$ 等回归系数，如 $\\left( \\tilde { \\beta } _ { y x . c m . } \\tilde { B } _ { y s . x c \\bar { s } } , \\tilde { B } _ { y z . x m \\bar { z } } , \\tilde { B } _ { y \\bar { s } . x c s } , \\tilde { B } _ { y \\bar { z } . x m z } \\right) ^ { T }$ 的计算公式。\\n    - 考虑 $L _ { p }$ - 惩罚损失函数 $\\mathcal { L } _ { p } ( \\beta _ { y x . c m } , B _ { y c . x m } , B _ { y m . x c } )$ 和 $L _ { p }$ - 多元响应惩罚损失函数 $L _ { p } ( B _ { m x . c } , B _ { m c . x } )$，并通过最小化这些损失函数来确定回归系数。其中，$\\mathcal { L } _ { p }$ 函数在设计上不将惩罚参数分配给 $B _ { y z . x m \\overline { { z } } }$ 和 $B _ { y s . x c \\overline { { s } } }$，以保留使用先验因果知识选择的协变量和中间变量。\\n    - 基于导出的活动集，考虑一些估计器以减少偏差，如 $\\tilde { B } _ { x c . m } ^ { \\dagger }$、$\\tilde { B } _ { x m . c } ^ { \\dagger }$ 等，这些估计器通过最小化相应的损失函数得到。\\n    - 最后，根据 $X$ 是否活跃，分别使用 $\\check { \\tau } _ { y x } ^ { * } = \\check { \\beta } _ { y x . c m } ^ { * } + \\check { B } _ { m x . c } ^ { * } \\check { B } _ { y m . x c } ^ { * }$ 或 $\\check { \\tau } _ { y x } ^ { * } = \\check { B } _ { m x . c } ^ { * } \\check { B } _ { y m . c } ^ { * }$ 来估计总效应。\\n\\n**案例解析**\\n论文给出了一个线性结构因果模型的数值实验案例。考虑线性 SCM $\\begin{cases} Y = \\alpha _ { y s } S + \\alpha _ { y z } Z + \\overline { { S } } A _ { y \\overline { { s } } } + \\overline { { Z } } A _ { y \\overline { { z } } } + \\epsilon _ { y } \\\\ \\overline { { S } } = X A _ { \\overline { { s } } x } + S A _ { \\overline { { s } } s } + Z A _ { \\overline { { s } } z } + \\epsilon _ { \\overline { { s } } } \\\\ S = \\alpha _ { s x } X + \\alpha _ { s z } Z + \\epsilon _ { s } \\\\ X = \\alpha _ { x z } Z + \\epsilon _ { x } \\end{cases}$，其中 $\\overline { { Z } }$ 和 $\\overline { S }$ 分别包含 10 个协变量和 5 个中间变量。通过生成 15 个随机样本，每个样本包含 18 个变量，进行 5000 次重复实验，比较了 LASSO、自适应 LASSO、Elastic Net、$\\mathrm { P A L } _ { 1 } \\mathrm { M A }$、OLS 方法、TSLS 方法和 PCM Selector 的性能。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括 LASSO、自适应 LASSO、Elastic Net、$\\mathrm { P A L } _ { 1 } \\mathrm { M A }$、OLS 方法、TSLS 方法，以及 Front - door - like（including x）、Front - door - like（not including x）、Back - door、Front - door（minimal）、Front - door（whole）等不同准则下的估计方法。\\n\\n**性能对比**\\n*   **在 [样本均值/Mean] 指标上：** 在设置 (a) 中，PCM Selector 的样本均值为 **0.036**，优于 LASSO (0.013)、自适应 LASSO (0.017) 和 Elastic Net (0.017)，与 $\\mathrm { P A L } _ { 1 } \\mathrm { M A }$ (0.054) 较为接近；在设置 (b) 中，PCM Selector 的样本均值为 **0.448**，与 Front - door (minimal) (0.468) 和 Front - door (whole) (0.462) 相近。\\n*   **在 [标准差/SD] 指标上：** 在设置 (a) 中，PCM Selector 的标准差为 **0.718**，低于 Front - door - like (including x) (1.577) 和 Back - door (1.591)；在设置 (b) 中，PCM Selector 的标准差为 **0.549**，与 Front - door (minimal) (0.552) 接近，低于 Front - door (whole) (0.692)。\\n*   **在 [偏差/Bias] 指标上：** 在设置 (a) 中，PCM Selector 的偏差为 **- 0.010**，小于 Front - door - like (including x) (- 0.053)；在设置 (b) 中，PCM Selector 的偏差为 **0.046**，小于 Front - door (minimal) (0.066) 和 Front - door (whole) (0.060)。整体上，PCM Selector 一般提供的估计偏差小于 TSLS 方法。\\n*   **在 [符号一致性/ Sign] 指标上：** LASSO、自适应 LASSO 和 Elastic Net 的符号一致性率较低，分别为 0.117、0.138 和 0.156，而 PCM Selector 在设置 (a) 中的符号一致性率为 **0.526**，在设置 (b) 中为 **0.808**，与 $\\mathrm { P A L } _ { 1 } \\mathrm { M A }$ （设置 (a) 中为 0.528）相当且明显高于前三者。\",\n    \"keywords\": \"### 关键词\\n\\n- 因果效应估计 (Causal Effect Estimation, N/A)\\n- 线性结构方程模型 (Linear Structural Equation Model, N/A)\\n- 惩罚回归分析 (Penalized Regression Analysis, N/A)\\n- 多重共线性问题 (Multicollinearity Problem, N/A)\\n- PCM Selector (Penalized Covariate - Mediator Selection Operator, PCM Selector)\\n- 后门准则 (Back - Door Criterion, N/A)\\n- 前门准则 (Front - Door Criterion, N/A)\\n- 中间变量选择 (Intermediate Variable Selection, N/A)\"\n}"
}