{
    "source": "Semantic Scholar",
    "arxiv_id": "2409.09345",
    "link": "https://arxiv.org/abs/2409.09345",
    "pdf_link": "https://arxiv.org/pdf/2409.09345.pdf",
    "title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models",
    "authors": [
        "Yuanzhao Zhai",
        "Tingkai Yang",
        "Kele Xu",
        "Dawei Feng",
        "Cheng Yang",
        "Bo Ding",
        "Huaimin Wang"
    ],
    "categories": [
        "cs.AI"
    ],
    "publication_date": "2024-09-14",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 11,
    "influential_citation_count": 1,
    "institutions": [
        "National University of Defense Technology",
        "State Key Laboratory of Complex & Critical Software Environment",
        "Hunan Institute of Advanced Technology"
    ],
    "paper_content": "# Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models\n\nYuanzhao Zhai1,2, Tingkai Yang1,2, Kele ${ \\bf X } { \\bf u } ^ { 1 , 2 }$ , Dawei Feng1,2\\*, Cheng Yang1,2,3, Bo Ding1,2, Huaimin Wang1,2\n\n1National University of Defense Technology, Changsha, China 2State Key Laboratory of Complex & Critical Software Environment 3Hunan Institute of Advanced Technology, Changsha, China {yuanzhaozhai, yangtingkai19, dingbo, hmwang}@nudt.edu.cn, xukelele $@$ 163.com, davyfeng.c $@$ qq.com, delpiero710@126.com\n\n# Abstract\n\nAgents significantly enhance the capabilities of standalone Large Language Models (LLMs) by perceiving environments, making decisions, and executing actions. However, LLM agents still face challenges in tasks that require multiple decision-making steps. Estimating the value of actions in specific tasks is difficult when intermediate actions are neither appropriately rewarded nor penalized. In this paper, we propose leveraging a task-relevant Q-value model to guide action selection. Specifically, we first collect decision-making trajectories annotated with step-level Q values via Monte Carlo Tree Search (MCTS) and construct preference data. We then use another LLM to fit these preferences through step-level Direct Policy Optimization (DPO), which serves as the Qvalue model. During inference, at each decision-making step, LLM agents select the action with the highest Q value before interacting with the environment. We apply our method to various open-source and API-based LLM agents, demonstrating that Q-value models significantly improve their performance. Notably, the performance of the agent built with Phi-3-mini$4 \\mathrm { k \\Omega }$ -instruct improved by $103 \\%$ on WebShop and $7 5 \\%$ on HotPotQA when enhanced with Q-value models, even surpassing GPT-4o-mini. Additionally, Q-value models offer several advantages, such as generalization to different LLM agents and seamless integration with existing prompting strategies.\n\n![](images/11fcb454ba31adbd6fa60f0274c63f623439cb0f0536ba135d51e33201b759ac.jpg)  \nFigure 1: Overview of our method. To train the $\\mathrm { Q }$ -value model, LLM agents interact with the environment to collect preference data with Q-value annotations using MCTS. During inference, LLM agents sample multiple candidate actions and select the best one based on the Q-value model.\n\n# Introduction\n\nAutonomous agents powered by large language models (LLMs) can operate across a wide range of domains, including web navigation (Yao et al. 2022; Zhou et al. 2024b), interactive question answering (Yang et al. 2018), and tool usage (Ma et al. 2024). By utilizing feedback or observations from environments, LLM agents can reason and plan using prompting strategies to accomplish specific tasks (Yao et al. 2023). The resulting text-based outputs and action plans can then be employed to make API calls and execute operations within these environments.\n\nDespite these advancements, even agents powered by some of the most effective LLMs, such as GPT-4, struggle with complex multi-step decision-making tasks (Achiam et al. 2023). Beyond intermediate environmental feedback, additional task-specific knowledge is necessary to further enhance decision-making. Allowing LLM agents to engage in multiple trial-and-error processes during inference, strategies such as carefully designed reflection (Shinn et al. 2023) or treebased search (Zhou et al. 2024a; Koh et al. 2024) can help agents iteratively refine their actions. However, this assumption is not always feasible in realistic applications. Recently, fine-tuning open-source LLM backbones with agent trajectories has emerged as an alternative. While this approach enables LLMs to acquire more task-specific knowledge, it can also degrade the general performances of LLMs (Chen et al. 2024b). Furthermore, state-of-the-art API-based LLMs, which are more effective for building agents, are not accessible for fine-tuning.\n\nAs the number of decision-making steps increases, compounding errors and uncertainties can accumulate (Xi et al. 2024a), exacerbating the problem. Since actions are sampled from a distribution of text, the greedy action may not always be the optimal choice in the environment. As shown in Figure 2, suboptimal actions in intermediate steps can lead to task failure. A common and effective approach to enhancing\n\nGreedy sainadmparllisoc.oepklioenawgsefero rtchaonto s5sel0ipb0l0wa codkomloleanres, Search ‚Ä¶ ‚ÄúPblriaocrkit‚Äùizaingd Detailed nFoat lfeodr two lmeaern Failure   \nGuiding Action Action 1Ôºö $\\textstyle { \\overline { { Q = 0 . 4 } } }$   \nSelection with Q Prioritizing ‚Äúanti- Product details Instruction ‚Ä¶Search Results Aslcitpi‚Äùoann2d: ‚ÄúprùëÑi ùëÑce=‚Äù 0.6 Back to Search Next > AClcitciko[nb:lack] Search Prioritizing Success sainadmparllisoc.oepklioenawgseferortchaonto s5sel0ipb0l0wa codkomloleanres, + Bec to sarch ‚Äúsliwpo‚Äùmanedn‚Äù‚Äú,p‚Äúraicneti‚Äù- Detsacilriepdt on‚Ä¶ ùëÑùëÑ = 0.3 black silver Action n: Prioritizing ‚Äú black‚Äù and ‚Äúprice‚Äù\n\nLLMs during inference is Best-of-N sampling (Yang et al. 2024), sampling multiple candidate actions and selecting the best one. However, such an approach cannot be directly applied because LLM agents lack a clear understanding of the action values associated with task completion, as environmental rewards are typically sparse, with only a terminal scalar indicating success (Xi et al. 2024b).\n\nTo overcome these limitations, we propose leveraging a Qvalue model to guide action selection at each decision-making step. Q-value functions, widely adopted by traditional Reinforcement Learning (RL) agents (Konda and Tsitsiklis 1999; Mnih et al. 2015), are trained to estimate the value of specific actions. When applying the Q-value approach to LLM agents, the challenges lie in how to collect training data and how to train Q-value models effectively. As illustrated in Figure 1, we leverage Monte Carlo Tree Search (MCTS) to iteratively explore high-quality trajectories, using its lookahead capability to decompose sparse outcome rewards into step-level Q values. We then construct preference data based on the annotated Q-values. To train the Q-value model, we propose a step-level version of direct policy optimization (DPO) (Rafailov et al. 2023) using an additional LLM. During inference, LLM agents can sample multiple candidate actions and select the one with the highest Q value to interact with the environment in a single trial.\n\nWe conduct experiments across diverse domains, including web navigation and interactive question answering. The results demonstrate that Q-value models can clearly distinguish actions that lead to success or failure, enhancing decisionmaking for LLM Agents via select effective actions at each step. Additionally, task-dependent Q-value models are generalizable across different LLM agents, allowing us to utilize inexpensive LLM agents to collect training data while enhancing the decision-making of more advanced LLM agents in a plug-and-play manner. Furthermore, our method complements the design of effective prompting strategies, and integrating it with these strategies can further improve performance. In summary, our main contributions are as follows:\n\n‚Ä¢ We leverage Q values to enhance the decision-making for LLM agents by guiding action selection at each step.\n\n‚Ä¢ We utilize the MCTS algorithm to collect decision-making trajectories and annotate them with step-level Q values. ‚Ä¢ We construct preference data for training and propose step-level DPO to train Q-value models. ‚Ä¢ Experiments across two domains demonstrate the effectiveness, generalization across LLM agents, and compatibility with existing methods of our Q-value models.\n\n# Related Work\n\nWith the advancement of LLMs, LLM agents that interact with the world to perform a wide variety of tasks have become a major focus of research (Wang et al. 2024). The LLM backbone of these agents can be classified into open-source and API-based categories. Open-source LLM agents offer greater flexibility, while API-based LLMs (e.g., GPT-4) are typically more effective as agents (Chen et al. 2024b). In numerous real-world scenarios, agents must execute multistep actions to tackle complex tasks and incorporate valuable feedback to improve decision-making.\n\nPrompting Strategies. Numerous prompting strategies (Wang et al. 2022; Xie et al. 2023; Madaan et al. 2023) have been proposed to enhance the reasoning and planning abilities of LLM agents. In the context of enhancing decision-making, ReAct (Yao et al. 2023) is widely used to integrate chain-of-thought (CoT) (Wei et al. 2022) reasoning with intermediate environment observations and agent actions. Reflection involves prompting an LLM to review and critique past interactions to improve current outputs. Reflexion (Shinn et al. 2023) provides agents with dynamic memory and self-reflection modules, enhancing decision-making through multiple trial-and-error iterations. However, due to the limited context window of LLMs, these methods struggle to accumulate extensive task experience.\n\nTree-based Search for LLMs. Tree-based search approaches explore multi-branch outcomes during the search process to locate target nodes, optimize paths, or meet specific conditions, which is widely applied in various planning algorithms (LaValle 1998a). Among them, Monte Carlo\n\n<html><body><table><tr><td>Approach</td><td>Step Level</td><td>Applicable to API-basedLLMs</td><td>Single Trial</td><td>Task Experience Accumulation</td></tr><tr><td>Prompt Strategies:Reflection,Reflexion (Shinn et al.2023)</td><td>√ó</td><td>‚àö</td><td>‚àöor X</td><td>√ó</td></tr><tr><td>Tre. 2ea4ch: LATS (Zhou et a. 2024a), Serch-agent (Koh</td><td>‚àö</td><td>‚àö</td><td>√ó</td><td>√ó</td></tr><tr><td>Fine. 2024g:, ATO (-FLAN(Cl.20 2tal.2024b),AgentEvo (Xi</td><td>√ó</td><td>√ó</td><td>‚àö</td><td>‚àö</td></tr><tr><td>Q-value model enhanced (Ours)</td><td></td><td>‚àö</td><td>‚àö</td><td>‚àö</td></tr></table></body></html>\n\nTree Search (MCTS) (Browne et al. 2012), maintain a favorable exploration-exploitation trade-off in many planning algorithms (LaValle 1998b). Equipping LLMs with tree-based search methods shows great potential in enhancing reasoning abilities (Hao et al. 2023; Feng et al. 2023; Chen et al. 2024a; Luo et al. 2024). More recently, tree-based search has been integrated with LLM agents to improve planning performance (Zhou et al. 2024a; Koh et al. 2024). However, constructing a tree during inference not only introduces significant token consumption but also requires environmental reversion assumptions, limiting its practical application.\n\nFine-tuning LLMs as Agent. Fine-tuning based methods further train open-source LLM backbones as effective alternatives to API-based LLM agents. Most fine-tuning based methods (Chen et al. 2023; Zeng et al. 2023; Chen et al. 2024b) concentrate on imitating curated expert trajectories, which is expensive and sub-optimal due to compounding errors and limited exploration data. In order to get rid of the reliance on expert trajectories, recent works (Christianos et al. 2023; Xi et al. 2024b; Song et al. 2024; Zhai et al. 2024) collect trajectories with outcome rewards to fine-tune LLM using reject sampling fine-tuning (RFT) (Yuan et al. 2023), RL or its variants.\n\nCompared to the various approaches summarized in Table 1, equipping LLM agents with step-level Q-value models offers several notable advantages. Our method can be applied to both open-source and API-based LLM agents without requiring training of the LLM backbones. Additionally, decision-making ability is enhanced by Q-values with a single trial, without needing assumptions about environmental reversion during inference. Our method does not increase context length and allows for accumulation of task experience in Q-value models, which can generalize across different agents and instructions within the task.\n\n# Task Formulation\n\nThe agent task with environment feedback can be formalized as a partially observable Markov decision process (POMDP) $( \\mathcal { U } , \\bar { \\mathcal { S } } , \\mathcal { A } , \\bar { \\mathcal { O } } , \\mathcal { T } , r )$ with instruction space $\\mathcal { U }$ , state space $s$ , action space $\\mathcal { A }$ , observation space $\\mathcal { O }$ , state transition function $\\mathcal { T } : \\mathcal { S } \\times \\mathcal { A }  \\mathcal { S }$ , and reward function $r$ .\n\nGiven a task instruction $u$ in the environment, the LLM agent generates an action $a _ { 0 } \\sim \\pi ( \\cdot | u )$ based on its policy $\\pi$ The state then transitions to $s _ { 1 } \\in S$ , and the agent receives observation $_ { o _ { 1 } } \\in { \\mathcal { O } }$ . The agent continues to interact with the environment until the task is completed or the maximum number of steps is reached. At time step $t$ , given the history and current observation, the agent generates the subsequent action $a _ { t + 1 } \\sim \\pi ( \\cdot | u , a _ { 0 } , o _ { 0 } , . . . , a _ { t } , o _ { t } )$ . Then the multi-step decision-making task can be formulated as:\n\n$$\n\\pi ( \\tau | u ) = \\prod _ { t = 1 } ^ { T } \\pi ( a _ { t } | u , \\tau _ { t - 1 } ) ,\n$$\n\nwhere we denote $\\tau$ as the whole trajectory, $T$ as the total interaction steps. $\\tau _ { t - 1 } = ( a _ { 0 } , o _ { 0 } , . . . , h _ { t - 1 } , a _ { t - 1 } , o _ { t - 1 } )$ denotes the interactive history up to $t - 1$ . The environment only provides the outcome reward $r ( u , \\tau ) \\in [ 0 , 1 ]$ . The objective of LLM agents is to maximize rewards from the environment:\n\n$$\n\\operatorname* { m a x } _ { \\pi } \\mathbb { E } _ { u \\sim \\mathcal { D } , \\tau \\sim \\pi ( \\cdot | u ) } \\left[ r ( u , \\tau ) \\right] ,\n$$\n\nwhere $\\mathcal { D }$ represents the dataset containing task instructions.\n\n# Proposed Method\n\nWe can build a decision tree where each node in the tree denotes an state and edge is an action. Each node stores a set of statistics:\n\n$$\n\\{ V ( s _ { t } ) , N ( s _ { t } ) \\} ,\n$$\n\nwhere $V ( s )$ represents the value function, which measures the expected reward from the sub-tree of $s _ { t }$ . $N ( s _ { t } )$ denotes the number of visits to a node $s _ { t }$ duding iterations.\n\n# Step-level Q Values Estimation with MCTS\n\nThe MCTS process starts from a root node $s _ { 0 }$ and progresses through four iterative stages: selection, expansion, evaluation and backpropagation, as shown in Figure 3(a).\n\nSelection. The objective of the first operation, selection, is to identify the most suitable trajectories for the next expansion step. We select the trajectory from the root node to a current leaf node. At each depth, we select the children with the highest Upper Confidence bounds applied to Trees (UCT) (Kocsis and Szepesva¬¥ri 2006) value to balance exploration and exploitation:\n\n$$\nU C T ( s _ { t } ) = V ( s _ { t } ) + \\sqrt { \\frac { \\eta \\ln { N \\big ( p ( s _ { t } ) \\big ) } } { N ( s _ { t } ) } } ,\n$$\n\nwhere $\\eta$ is the exploration weight, and $p ( s _ { t } )$ denotes the parent node of $s _ { t }$ .\n\n![](images/3ed8ff6bac51996c801a374a86b0fc37b3f67b2d09823c667b026216e52d3ced.jpg)  \n(a) Illustration of MCTS for trajectories collection and Q-value annotation.   \n(b) Preference data construction.   \nFigure 3: Collecting step-level preference data involves two stages: (a) using MCTS to explore high-quality trajectories and annotate each step with Q-values, and (b) constructing preference data from the final tree. During the construction stage, green nodes represent the best trajectories explored by the agent and are utilized to determine win actions at each depth of the tree. Blue nodes are candidates for selecting dispreferred actions, while grey nodes are neglected.\n\nExpansion. The second operation expands the tree by sampling $n$ actions from $\\pi$ , as outlined in the previous section. Unlike traditional agents, such as those used in Go, which operate in a finite action space, LLM agents have an infinite action space. LLMs can generate an unlimited number of distinct actions (sequences of tokens), though some of these may be invalid. To ensure diversity, we sample multiple candidate actions using a high temperature. The environment processes each action and provides corresponding feedback as an observation, resulting in $n$ new child nodes being added to the tree.\n\nEvaluation. Since the tree depths for LLM agent tasks are typically much shallower than those for Go games, expansions quickly reach terminal nodes. Unlike AlphaGo (Silver et al. 2016), which learns a value network to evaluate the value of state nodes, we evaluate the expanded nodes using a rollout algorithm. Specifically, starting from the expanded nodes, the LLM agent interacts with the environment until termination or the maximum rollout depth is reached. If the explored node is terminal, the environment‚Äôs provided outcome reward is returned; otherwise, a fixed negative reward is assigned to the explored node at the maximum depth.\n\nBackpropagation. This operation updates the tree statistics based on the outcome rewards or fixed negative rewards assigned during the evaluation stage. For each node in the trajectory $\\tau$ , $N ( s )$ is incremented by 1, and the values are updated from the end node $s _ { T }$ to the root node $s _ { 0 }$ using the following formula:\n\n$$\nV ( s _ { t } ) \\gets \\frac { V ( s _ { t } ) ( N ( s _ { t } ) - 1 ) + r ( u , \\tau ) } { N ( s _ { t } ) } .\n$$\n\nThe updated values are utilized in the UCT Equation 4 to guide the selection of the next node.\n\nAfter multiple iterations of selection, expansion, evaluation, and backpropagation, we obtain the final tree, which stores the expanded nodes and their corresponding state values. The Q-value of non-terminal nodes can be calculated:\n\n$$\n\\hat { Q } ( s _ { t } , a _ { t } ) = r ( s _ { t } , a _ { t } ) + V ( s _ { t + 1 } ) = V ( s _ { t + 1 } ) ,\n$$\n\nwith the deterministic transition assumption. Otherwise, $\\hat { Q } ( s _ { t } , a _ { t } )$ can be considered a Monte Carlo estimate of the true Q-value.\n\n# Training Q-Value Models\n\nDue to the limitations of MCTS iterations, $\\hat { Q } ( s _ { t } , a _ { t } )$ may not accurately fit the true Q-value. However, it is more accurate to distinguish between preferred and dispreferred actions based on Q-values among multiple candidate actions. Therefore, we employ a preference learning algorithm called Direct Policy Optimization (DPO), leveraging its effectiveness in learning implicit value models (Zhong et al. 2024; Rafailov et al. 2024). As mentioned earlier, directly fine-tuning LLM backbones has several drawbacks. Instead, we train an additional LLM, $\\pi _ { \\boldsymbol { \\theta } }$ , parameterized by $\\theta$ , to learn Q-values. Given that evaluation tasks are simpler than generation tasks (Pang et al. 2024), $\\pi _ { \\boldsymbol { \\theta } }$ can be smaller than the LLM backbones $\\pi$ .\n\nUnder the Bradley-Terry model (Bradley and Terry 1952), DPO propose a preference learning loss to optimize the objective in Equation 2 while keeping the KL distance between the training model and the initial model.\n\n$$\n\\begin{array} { r l } & { \\mathcal { L } _ { \\mathrm { t r a j e c t o r y } } ( \\pi _ { \\theta } ; \\pi _ { \\mathrm { r e f } } ) = - \\mathbb { E } _ { ( u , \\tau ^ { w } , \\tau ^ { l } ) \\sim \\mathcal { D } } } \\\\ & { \\bigg [ \\log \\sigma \\Big ( \\beta \\log \\frac { \\pi _ { \\theta } \\left( \\tau ^ { w } | u \\right) } { \\pi _ { \\mathrm { r e f } } \\left( \\tau ^ { w } | u \\right) } - \\beta \\log \\frac { \\pi _ { \\theta } \\left( \\tau ^ { l } | u \\right) } { \\pi _ { \\mathrm { r e f } } \\left( \\tau ^ { l } | u \\right) } \\Big ) \\bigg ] , } \\end{array}\n$$\n\nwhere $\\sigma$ is the sigmoid function, $\\beta$ is a weighting parameter of KL regularization, and $\\pi _ { \\mathrm { r e f } }$ is the reference model, which is usually served by $\\pi _ { \\boldsymbol { \\theta } }$ without preference learning. Besides task instructions $u$ . the dataset $\\mathcal { D }$ contains preferred trajectories $\\tau ^ { w }$ and dispreferred trajectories $\\tau ^ { l }$ . Without process supervision, LLM agents cannot be fine-tuned at the step level. This limitation hinders performance in multi-step decision-making tasks, as will be demonstrated in the experimental section. To address this issue, we construct more fine-grained preference data and propose step-level DPO for LLM agent scenarios.\n\nPreference data construction. We aim to construct steplevel preference data based on $\\hat { Q } ( s _ { t } , a _ { t } )$ estimated using Equation 6. To achieve this, we need to identify preferred and dispreferred actions for the shared decision-making trajectory segment. We first locate the terminal node with the highest reward in the final tree and then extract the corresponding trajectories from the terminal node to the root node. At each depth, we select a partial segment of the trajectory $\\tau _ { t }$ as the shared part. Preferred actions, $a _ { t } ^ { w }$ , are taken from the selected trajectory at the next step, while dispreferred actions, $a _ { t } ^ { l }$ , are chosen from candidate actions with the lowest $\\hat { Q } ( s _ { t } , a _ { t } )$ , as illustrated in Figure 3(b). This approach focuses preference learning on distinguishing between $a _ { t } ^ { w }$ and $a _ { t } ^ { l }$ , providing detailed insights into which actions might lead to failure in the overall decision-making process, as indicated by the Q-value.\n\nStep-level preference learning. Given the preference pairs $\\{ u , \\bar { \\tau } _ { t } , a _ { t } ^ { w } , \\bar { a } _ { t } ^ { l } \\}$ , the objective of training step-level Q-value models can be formulated as:\n\n$$\n\\begin{array} { r l } & { \\mathcal { L } _ { \\mathrm { s t e p } } ( \\pi _ { \\theta } ; \\pi _ { \\mathrm { r e f } } ) = - \\mathbb { E } _ { ( u , \\tau _ { t } , a _ { t } ^ { w } , a _ { t } ^ { l } ) \\sim \\mathcal { D } } } \\\\ & { \\bigg [ \\log \\sigma \\Big ( \\beta \\log \\frac { \\pi _ { \\theta } \\left( a _ { t } ^ { w } \\vert u , \\tau _ { t } \\right) } { \\pi _ { \\mathrm { r e f } } \\left( a _ { t } ^ { w } \\vert u , \\tau _ { t } \\right) } - \\beta \\log \\frac { \\pi _ { \\theta } \\left( a _ { t } ^ { l } \\vert u , \\tau _ { t } \\right) } { \\pi _ { \\mathrm { r e f } } \\left( a _ { t } ^ { l } \\vert u , \\tau _ { t } \\right) } \\Big ) \\bigg ] , } \\end{array}\n$$\n\nwhere $\\mathcal { D }$ contains step-level preference data from $t = 0$ to $t = T$ . The normalized logits of the DPO model effectively learn implicit value models (Rafailov et al. 2023). In our scenario, DPO fits the estimated Q-value $\\hat { Q } ( s _ { t } , a _ { t } )$ and can generalize to new states and actions. With the well-trained $\\pi _ { \\boldsymbol { \\theta } }$ , the Q-value can be calculated as:\n\n$$\n\\begin{array} { r } { Q ( u , \\tau _ { t } , a _ { t } ) = \\beta \\log \\pi _ { \\theta } ( a _ { t } ^ { w } | u , \\tau _ { t } ) - \\beta \\log \\pi _ { \\mathrm { r e f } } ( a _ { t } ^ { l } | u , \\tau _ { t } ) . } \\end{array}\n$$\n\nFor brevity, we refer to $Q ( u , \\tau _ { t } , a _ { t } )$ as the $\\mathbf { Q }$ -value model, which consists of the trained model $\\pi _ { \\boldsymbol { \\theta } }$ and its reference model $\\pi _ { \\mathrm { r e f } }$ for normalization.\n\nConsidering that tree search methods require the assumption of environment reversibility during inference, which is not always feasible in practical applications; furthermore, constructing a tree during testing incurs significant computational overhead.\n\nAt inference time, before executing each action, the LLM agent first samples multiple actions in parallel, then scores them based on the Q-value model and finally selects the action with the highest Q-value to interact with the environment. In practice, due to the infinite action space, we sample $n$ candidate actions, similar to the expansion stage of MCTS, and select the action with the highest Q-value to interact with the environment. This can be formulated as:\n\n$$\na _ { t } = \\arg \\operatorname* { m a x } _ { a \\in \\{ a ^ { 1 } , a ^ { 2 } , \\cdots , a ^ { N } \\} } \\Bigl [ Q ( u , \\tau _ { t } , a ) \\Bigr ] ,\n$$\n\nwhere $n$ actions $\\{ a ^ { 1 } , a ^ { 2 } , \\cdots , a ^ { n } \\}$ are sampled from LLM agent $\\pi$ .\n\n# Experiments Experimental Settings\n\nTo validate the versatility of our method, we apply Q-value models to various LLM backbones, including popular opensource LLMs such as the Phi-3-mini- $4 \\mathrm { k }$ -instruct model with 3.8B parameters and Llama-3.1-8B-Instruct, as well as APIbased LLMs like GPT-4o-mini and GPT-4-turbo. The Qvalue models are based on Phi-1.5 1, which has 1.3B parameters. For efficiency, unless otherwise stated, the LLM agents used for collecting step-level preference data are primarily based on the Phi-3-mini-4k-instruct model. The maximum context length is set to 4096.\n\nWe evaluate our method on two tasks across different domains: WebShop (Yao et al. 2022) and HotPotQA (Yang et al. 2018). We include 3-shot in-context examples in the instruction prompt for both tasks. The maximum number of decision-making steps is set to 10 for WebShop and 7 for HotPotQA. For HotPotQA, we randomly select 1000 questions for training, 100 for validation, and 100 for testing. For WebShop, we follow the data split described in Song et al. (2024), which consists of 1824 instructions for training, 100 questions for validation, and 100 questions for testing. All experiments are conducted on a single NVIDIA A40 48G GPU, except when implementing fine-tuning-based methods, which require two NVIDIA A100 80G GPUs.\n\nBaselines. We mainly compare our method with various fine-tuning-based methods because both approaches involve accumulating task experience through training LLMs and do not require multiple trials during inference. Rejection Sampling Fine-Tuning (RFT) (Yuan et al. 2023) uses demonstrated trajectories to train LLM backbones. AgentEovl is similar to RFT but assigns weights to trajectories based on their rewards. ETO employs DPO to enhance LLM agents, using both preferred trajectories $\\tau ^ { w }$ and dispreferred trajectories $\\tau ^ { l }$ , which are sampled from self-explored trajectories and distinguished by outcome rewards from the environment. Best-of-N (BoN) samples $n$ trajectories using vanilla LLM agents and selects the one with the highest reward. Note that BoN serves as a strong baseline because it requires multiple query outcome rewards from the environment. The number of candidate actions is set to $n = 5$ , unless otherwise specified, for both our method and BoN. For a fair comparison, training data for all methods are collected using MCTS.\n\n# Results\n\nWe report the results on two tasks in Table 2. Note that the same Q-value models for a specific task established by Phi1.5 are trained by preference data collected by LLM agents established by Phi-3-mini-4k-instruct, and then are applied to various LLM backbones without additional training. As shown, our main findings are as follows:\n\nQ-value models can significantly enhance decisionmaking. Well-trained Q-value models double the performance of LLM agents based on Phi-3-mini- $4 \\mathbf { k }$ -instruct on the WebShop task and improve performance by $7 5 \\%$ on the HotPotQA task. The enhanced LLM agent outperforms the lightweight GPT-4o-mini on both tasks and even surpasses the more advanced GPT-4-turbo on the WebShop task. There are two reasons to explain why Q-value models bring more performance gains on WebShop. First, the WebShop task involves more decision-making steps than HotPotQA, allowing Q-value models to substantially reduce accumulation errors. Second, unlike the WebShop task, which provides more granular rewards ranging from 0 to 1, HotPotQA offers binary rewards of 0 or 1. This binary reward structure makes it more challenging to construct finely distinguished preference data, which we will explore in the next section.\n\nTable 2: The average outcome reward of different methods on two multi-step decision-making tasks. Note that all Q-value models in this table are trained using step-level preference data collected by Phi-3-mini- $4 \\mathrm { k \\Omega }$ -instruct.   \n\n<html><body><table><tr><td>B LLMne</td><td>Method</td><td>WebShop</td><td>HotPotQA</td></tr><tr><td>Open- sourced</td><td>Phi-3-mini-4k-instruct + RFT(Yuan et al. 2023) + AgentEvol (Xi et al. 2024b) + ETO (Song et al. 2024) +BoN + Q(Ours) Llama-3.1-8B-instruct</td><td>0.30 0.44 0.50 0.53 0.50 0.61 (+103%) 0.48</td><td>0.20 0.23 0.23 0.27 0.34 0.35 (+75%) 0.46</td></tr><tr><td>API- based</td><td>+Q(Ours) GPT-4o-mini +Q(Ours) GPT-4-turbo + Q (Ours)</td><td>0.60 (+25%) 0.49 0.64 (+31%) 0.58 0.64 (+10%)</td><td>0.50 (+9%) 0.31 0.44 (+42%) 0.44 0.50 (+14%)</td></tr></table></body></html>\n\nTraining Q-value models is more efficient and effective than fine-tuning LLM backbones. RFT, which utilizes demonstrated trajectories for supervised fine-tuning of LLMs, improves performance on both tasks. AgentEval, which incorporates more reward information, enhances performance in the WebShop task but not in the HotPotQA task. This is because the HotPotQA environment only provides binary rewards, effectively reducing AgentEval‚Äôs performance to that of RFT. ETO, which incorporates more losing trajectories for learning, achieves the best performance among fine-tuningbased methods. This underscores the importance of including imperfect trajectories in training.\n\nFine-tuning LLM backbones requires high-performance computing resources, particularly as LLM size and context length increase. Therefore, our comparison with fine-tuningbased methods primarily uses Phi-3-mini- $4 \\mathbf { k }$ -instruct with 3.8B parameters. In contrast, our Q-value models are based on the more lightweight Phi-1.5 with 1.3B parameters. Nevertheless, our method is more effective than all the fine-tuningbased methods mentioned above and outperforms BoN in both tasks. We note that BoN, which has the same computational overhead with our method but the additional outcome reward from the environment, is a strong baseline, and our method outperforms BoN with on both tasks.\n\nQ-value models are generalizable across different LLM backbones. The Q-value models accumulate task experience, and we expect them to generalize across different LLM agents within the same task. To verify this, we first train Q-value models using preference data sampled from Phi-3-mini-4k-instruct. We then apply these Q-value models directly to stronger open-source LLMs, such as Llama-3.1-\n\n0.7 Action to Success 0.6 Action to Failure   \ny0.5   \n0.   \n0.3 0.2 0.1 0.0 -10.5-10.0-9.5 -9.0 -8.5 -8.0 QValue 0.9   \nC I   \nA0.8   \n0.6   \nP 0.5 TrainingIND Test OOD Test Dataset Type\n\n(a) Preference accuracy of $Q -$ value models.\n\n8B-instruct, and API-based LLMs, including GPT-4o-mini and GPT-4-turbo. We observe that the decision-making abilities are consistently improved, although the performance gains are not as substantial as when the Q-value models are applied to the LLM agents that generated the training data. This is because the states and actions sampled by other LLM agents can be considered Out-Of-Distribution (OOD) relative to the step-level preference data collected by Phi-3-mini-4kinstruct, which was used to train the Q-value models. Nevertheless, these positive results suggest that trial-and-error experience from a less powerful and more cost-effective LLM agent can benefit stronger API-based LLM agents.\n\n# Evaluations of Q-value Models\n\nWe further investigate the accuracy of $\\mathrm { Q }$ -value models in assessing the preference relationships of collected step-level data. As shown in Figure 4(a), preference relationships within the training sets are learned effectively in both tasks. However, when evaluating on the in-distribution (IND) test set, accuracy decreases to $8 3 \\%$ on WebShop and $6 7 \\%$ on HotPotQA. The performance gap on HotPotQA is attributed to its binary outcome reward and the early stopping of MCTS when the reward of 1 is obtained. Additionally, generalizing to the OOD test set, where preference data is collected by other LLM agents, results in a slight performance degradation on both tasks. Nevertheless, such preference accuracy is sufficient for downstream tasks, as demonstrated by recent studies (Lambert et al. 2024).\n\nTo further evaluate the effectiveness of Q-value models, we select 200 actions from successful and failed trajectories, respectively, and visualize their Q-values in Figure 4(b). There is some overlap of Q-value distributions between action to success and to failure. This is because even failure trajectories can include high-quality actions, and failure often results from choosing certain destructive actions (Koh et al. 2024). Overall, the Q-value distribution for actions in failed trajectories is skewed to the left, while the distribution for successful actions shows less skewness, with most of the probability density leaning to the right, suggesting that our $\\mathrm { \\Delta Q }$ -value models are capable of effective credit assignment.\n\nTable 3: Effects of step-level and trajectory-level preference data on the performance of trained Q-value models.   \n\n<html><body><table><tr><td>Q-value Model</td><td>n=1</td><td>n=3</td><td>n=5</td><td>n=7</td></tr><tr><td>Step-level</td><td>0.30</td><td>0.50</td><td>0.61</td><td>0.63</td></tr><tr><td>Trajectory-level</td><td>0.30</td><td>0.42</td><td>0.50</td><td>0.51</td></tr></table></body></html>\n\n2000 Successful Trajectory 1500 Preference Data   \nm M1000 500 0 500 1000 1500 10 30 50 Number of Preference Data MCTS Iterations   \n(a) Number of training prefer-(b) Preference data construction   \nence data on performance. with different MCTS iterations.\n\n# Ablation Studies\n\nAdvantage of Step-Level Preference Data. Recent studies (Rafailov et al. 2024; Zhong et al. 2024) indicate that the trajectory-level DPO objective, as described in Equation 7, also holds potential for credit assignment. To evaluate this, we establish an additional baseline by comparing our proposed step-level Q-value model with a Q-value model trained using trajectory-level preference data $( \\bar { u } , \\tau ^ { w } , \\tau ^ { l } )$ . Our results based on LLM agents powered by Phi-3-mini-4k-instruct on WebShop, as shown in Table 3, suggest that while Q-value models trained with trajectory-level data can enhance LLM agents, their performance improves gradually as more candidate actions are sampled at each step. However, models trained with our step-level preference data consistently outperform this baseline across various numbers of candidate actions. This superior performance can be attributed to the more granular information provided by planning steps, as represented by the node values in the Monte Carlo tree.\n\nHow much preference data is needed for training? To train a Q-value model, step-level preference data must be constructed using task instructions. We investigate how different amounts of training data impact downstream performance. As shown in Figure 5(a), we evaluate several checkpoints from one epoch of training the Q-value model on the HotPotQA task, which represents varying quantities of training samples. We observe that fewer than 400 step-level preference data points can significantly enhance performance, achievable with approximately 250 task instructions in our setting.\n\nAblation of MCTS Iterations. More preference data can be collected by increasing the number of MCTS iterations, though this also increases computational overhead. In our previous experiments, we set the MCTS iteration to $m = 3 0$ by default. We perform an ablation study on the number of MCTS iterations to assess its impact on data collection. As shown in Figure 5(b), the number of successful trajectories available for constructing step-level preference data increases with the maximum number of MCTS iterations. Nearly all\n\nTable 4: Averaged rewards of integration with different prompting strategies.   \n\n<html><body><table><tr><td>Method</td><td>HotPotQA</td></tr><tr><td>ReAct</td><td>0.31</td></tr><tr><td>ReAct+Reflection</td><td>0.39</td></tr><tr><td>ReAct+ Q(Ours)</td><td>0.46</td></tr><tr><td>ReAct +Reflection + Q(Ours)</td><td>0.48</td></tr></table></body></html>\n\nMCTS processes terminate early, before the 50th iteration, due to achieving the maximum reward or depth, rendering additional iterations redundant. Furthermore, the number of step-level preference data points increases more rapidly than the number of successful trajectories with additional MCTS iterations. This is because trajectories explored with a larger number of MCTS iterations typically involve more decisionmaking steps, thus providing more step-level preference data.\n\nIntegration with different prompting strategies. In our work, we use a ReAct-style prompt to enable LLMs to function as agents. We further enhance LLM agents with a more sophisticated prompting strategy, ‚ÄúReAct $+$ Reflection‚Äù. As shown in Table 4, this improves the performance of GPT-4omini from 0.31 to 0.39. We also apply the prompting strategy to the LLM agent based on Phi-3-mini-4k-instruct. However, the performance decreased from 0.20 to 0.15. This may because that Phi-3-mini- $4 \\mathbf { k }$ -instruct with 3.8B parameters can not adequately understand the reflection prompts.\n\nWe sample trajectories from GPT-4o-mini instead of Phi3-mini-4k-instruct to ensure quality with different prompting strategies. The results indicate that methods incorporating both reflection and Q-value models achieve the highest average reward of 0.48, suggesting that our proposed method complements the design of more effective prompting strategies. Additionally, combining the results from Table 2 and Table 4, we observe that the Q-value model trained on preference data collected by GPT-4o-mini outperforms the model trained on data sampled by Phi-3-mini-4k-instruct, with average rewards of 0.48 and 0.46, respectively. This finding is consistent with our observation that the preference accuracy on the OOD test set exceeds the preference accuracy on the IND test set, as shown in Figure 4(a).\n\n# Conclusion and Limitations\n\nIn this paper, we propose leveraging Q-values to guide action selection at each decision-making step. We collect training data using MCTS and train Q-value models through step-level direct policy optimization. Results from two distinct tasks demonstrate that our method is more efficient and effective compared to fine-tuning LLM backbones. Furthermore, the trained Q-value models are plug-and-play, easily applicable to both open-source and API-based LLM agents, and generalize well across them. We believe our method introduces a novel and flexible paradigm for enhancing the decisionmaking capabilities of LLM agents.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÈíàÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂú®Â§öÊ≠•ÂÜ≥Á≠ñ‰ªªÂä°‰∏≠Èù¢‰∏¥ÁöÑÊåëÊàòÔºåÁâπÂà´ÊòØÂú®‰∏≠Èó¥Ê≠•È™§Áº∫‰πèÈÄÇÂΩìÂ•ñÂä±ÊàñÊÉ©ÁΩöÁöÑÊÉÖÂÜµ‰∏ãÔºåÈöæ‰ª•ÂáÜÁ°Æ‰º∞ËÆ°Âä®‰Ωú‰ª∑ÂÄºÁöÑÈóÆÈ¢ò„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåLLM‰ª£ÁêÜÂú®Â§çÊùÇ‰ªªÂä°ÔºàÂ¶ÇÁΩëÈ°µÂØºËà™Âíå‰∫§‰∫íÂºèÈóÆÁ≠îÔºâ‰∏≠ÈúÄË¶ÅÂ§öÊ≠•ÂÜ≥Á≠ñÔºåËÄåÁé∞ÊúâÊñπÊ≥ïÔºàÂ¶ÇÂèçÂ∞ÑÊàñÊ†ëÊêúÁ¥¢ÔºâÂú®Áé∞ÂÆûÂ∫îÁî®‰∏≠Â≠òÂú®Â±ÄÈôêÊÄßÔºåÂ¶ÇÁéØÂ¢É‰∏çÂèØÈÄÜÂÅáËÆæÂíåÈ´òËÆ°ÁÆóÂºÄÈîÄ„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫Âà©Áî®‰ªªÂä°Áõ∏ÂÖ≥ÁöÑQÂÄºÊ®°ÂûãÊù•ÊåáÂØºÂä®‰ΩúÈÄâÊã©ÔºåÈÄöËøáËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâÊî∂ÈõÜÂ∏¶ÊúâÊ≠•È™§Á∫ßQÂÄºÊ≥®ÈáäÁöÑÂÜ≥Á≠ñËΩ®ËøπÔºåÂπ∂‰ΩøÁî®Ê≠•È™§Á∫ßÁõ¥Êé•Á≠ñÁï•‰ºòÂåñÔºàDPOÔºâËÆ≠ÁªÉQÂÄºÊ®°Âûã„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Ë¥°ÁåÆ1Ôºö** ÊèêÂá∫Âà©Áî®QÂÄºÂ¢ûÂº∫LLM‰ª£ÁêÜÁöÑÂÜ≥Á≠ñËÉΩÂäõÔºåÈÄöËøáÊ≠•È™§Á∫ßÂä®‰ΩúÈÄâÊã©„ÄÇ\\n> *   **Ë¥°ÁåÆ2Ôºö** ‰ΩøÁî®MCTSÁÆóÊ≥ïÊî∂ÈõÜÂÜ≥Á≠ñËΩ®ËøπÂπ∂Ê†áÊ≥®Ê≠•È™§Á∫ßQÂÄºÔºåÊûÑÂª∫ÂÅèÂ•ΩÊï∞ÊçÆ„ÄÇ\\n> *   **Ë¥°ÁåÆ3Ôºö** ÊèêÂá∫Ê≠•È™§Á∫ßDPOËÆ≠ÁªÉQÂÄºÊ®°ÂûãÔºåÂÆûÈ™åË°®ÊòéËØ•ÊñπÊ≥ïÂú®WebShop‰ªªÂä°‰∏ä‰ΩøPhi-3-mini-4k-instruct‰ª£ÁêÜÊÄßËÉΩÊèêÂçá103%ÔºåÂú®HotPotQA‰ªªÂä°‰∏äÊèêÂçá75%„ÄÇ\\n> *   **ÊïàÊûúÔºö** QÂÄºÊ®°ÂûãÂú®WebShop‰ªªÂä°‰∏ä‰ΩøPhi-3-mini-4k-instruct‰ª£ÁêÜÊÄßËÉΩÊèêÂçá103%ÔºåÂú®HotPotQA‰ªªÂä°‰∏äÊèêÂçá75%ÔºåÁîöËá≥Ë∂ÖËøá‰∫ÜGPT-4o-mini„ÄÇÊ≠§Â§ñÔºåQÂÄºÊ®°ÂûãËøòÂÖ∑ÊúâÂØπ‰∏çÂêåLLM‰ª£ÁêÜÁöÑÊ≥õÂåñËÉΩÂäõÂíå‰∏éÁé∞ÊúâÊèêÁ§∫Á≠ñÁï•ÁöÑÊó†ÁºùÈõÜÊàê‰ºòÂäø„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   Ê†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáQÂÄºÊ®°Âûã‰º∞ËÆ°ÊØè‰∏™ÂÜ≥Á≠ñÊ≠•È™§ÁöÑÂä®‰Ωú‰ª∑ÂÄºÔºå‰ªéËÄåÂú®Â§öÊ≠•‰ªªÂä°‰∏≠‰ºòÂåñÂä®‰ΩúÈÄâÊã©„ÄÇQÂÄºÊ®°ÂûãÂà©Áî®MCTSÁöÑÈ¢ÑËßÅËÉΩÂäõÂ∞ÜÁ®ÄÁñèÁöÑÊúÄÁªàÂ•ñÂä±ÂàÜËß£‰∏∫Ê≠•È™§Á∫ßQÂÄºÔºåÂπ∂ÈÄöËøáDPOÂ≠¶‰π†Ëøô‰∫õÂÅèÂ•ΩÊï∞ÊçÆ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÊñπÊ≥ïÔºàÂ¶ÇÂèçÂ∞ÑÊàñÊ†ëÊêúÁ¥¢ÔºâÈúÄË¶ÅÂ§öÊ¨°ËØïÈ™åÊàñÁéØÂ¢ÉÂèØÈÄÜÂÅáËÆæÔºåËÄåÊú¨ÊñáÊñπÊ≥ïÈÄöËøáQÂÄºÊ®°ÂûãÂú®ÂçïÊ¨°ËØïÈ™å‰∏≠‰ºòÂåñÂä®‰ΩúÈÄâÊã©ÔºåÊó†ÈúÄÁéØÂ¢ÉÂèØÈÄÜÂÅáËÆæ„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** ÊèêÂá∫Ê≠•È™§Á∫ßÂÅèÂ•ΩÊï∞ÊçÆÊûÑÈÄ†ÂíåÊ≠•È™§Á∫ßDPOÔºå‰ΩøQÂÄºÊ®°ÂûãËÉΩÂ§üÊõ¥Á≤æÁªÜÂú∞Âå∫ÂàÜÂä®‰Ωú‰ª∑ÂÄºÔºå‰ªéËÄåÂáèÂ∞ëÂ§öÊ≠•‰ªªÂä°‰∏≠ÁöÑÁ¥ØÁßØËØØÂ∑Æ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   1. **Êï∞ÊçÆÊî∂ÈõÜÔºö** ‰ΩøÁî®MCTSÊé¢Á¥¢È´òË¥®ÈáèËΩ®ËøπÔºåÊ†áÊ≥®Ê≠•È™§Á∫ßQÂÄº„ÄÇ\\n> *   2. **ÂÅèÂ•ΩÊï∞ÊçÆÊûÑÈÄ†Ôºö** ‰ªéÊúÄÁªàÊ†ë‰∏≠ÈÄâÊã©ÊúÄ‰ºòËΩ®ËøπÂíåÂÄôÈÄâÂä®‰ΩúÔºåÊûÑÂª∫Ê≠•È™§Á∫ßÂÅèÂ•ΩÂØπ„ÄÇ\\n> *   3. **Ê®°ÂûãËÆ≠ÁªÉÔºö** ‰ΩøÁî®Ê≠•È™§Á∫ßDPOËÆ≠ÁªÉQÂÄºÊ®°ÂûãÔºå‰ºòÂåñÂä®‰ΩúÈÄâÊã©Á≠ñÁï•„ÄÇ\\n> *   4. **Êé®ÁêÜÔºö** Âú®Êé®ÁêÜÊó∂ÔºåLLM‰ª£ÁêÜÈááÊ†∑Â§ö‰∏™ÂÄôÈÄâÂä®‰ΩúÔºåÈÄâÊã©QÂÄºÊúÄÈ´òÁöÑÂä®‰Ωú‰∏éÁéØÂ¢É‰∫§‰∫í„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ÊãíÁªùÈááÊ†∑ÂæÆË∞ÉÔºàRFTÔºâ„ÄÅAgentEvol„ÄÅETO„ÄÅBest-of-NÔºàBoNÔºâ„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®WebShop‰ªªÂä°ÁöÑÂπ≥ÂùáÂ•ñÂä±‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®Phi-3-mini-4k-instruct‰∏äËææÂà∞‰∫Ü0.61ÔºåÊòæËëó‰ºò‰∫éRFTÔºà0.44ÔºâÂíåAgentEvolÔºà0.50Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øETOÔºà0.53ÔºâÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü15‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®HotPotQA‰ªªÂä°ÁöÑÂπ≥ÂùáÂ•ñÂä±‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®Phi-3-mini-4k-instruct‰∏äËææÂà∞‰∫Ü0.35ÔºåÊòæËëó‰ºò‰∫éRFTÔºà0.23ÔºâÂíåAgentEvolÔºà0.23Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øBoNÔºà0.34ÔºâÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü1‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ÈÄöÁî®ÊÄß‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïËÆ≠ÁªÉÁöÑQÂÄºÊ®°ÂûãÂèØÂ∫îÁî®‰∫é‰∏çÂêåLLM‰ª£ÁêÜÔºàÂ¶ÇGPT-4o-miniÂíåGPT-4-turboÔºâÔºåÂú®WebShop‰ªªÂä°‰∏äÂàÜÂà´ÊèêÂçá31%Âíå10%„ÄÇ\\n> *   **Âú®ÊïàÁéá‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®ÂçïÊ¨°ËØïÈ™å‰∏≠‰ºòÂåñÂä®‰ΩúÈÄâÊã©ÔºåÈÅøÂÖç‰∫ÜÂ§öÊ¨°ËØïÈ™åÁöÑÈ´òËÆ°ÁÆóÂºÄÈîÄ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰ª£ÁêÜ (Large Language Model Agents, LLM Agents)\\n*   ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ (Monte Carlo Tree Search, MCTS)\\n*   Ê≠•È™§Á∫ßQÂÄºÊ®°Âûã (Step-Level Q-Value Models, N/A)\\n*   Áõ¥Êé•Á≠ñÁï•‰ºòÂåñ (Direct Policy Optimization, DPO)\\n*   Â§öÊ≠•ÂÜ≥Á≠ñ (Multi-Step Decision-Making, N/A)\\n*   ÁΩëÈ°µÂØºËà™ (Web Navigation, N/A)\\n*   ‰∫§‰∫íÂºèÈóÆÁ≠î (Interactive Question Answering, N/A)\\n*   ÂÅèÂ•ΩÂ≠¶‰π† (Preference Learning, N/A)\"\n}\n```"
}