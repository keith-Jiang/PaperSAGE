{
    "link": "https://arxiv.org/abs/2412.10742",
    "pdf_link": "https://arxiv.org/pdf/2412.10742",
    "title": "WEPO: Web Element Preference Optimization for LLM-based Web Navigation",
    "authors": [
        "Jiarun Liu",
        "Jia Hao",
        "Chunhong Zhang",
        "Zheng Hu"
    ],
    "publication_date": "2024-12-14",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "paper_content": "# WEPO: Web Element Preference Optimization for LLM-based Web Navigation\n\nJiarun Liu, Jia Hao, Chunhong Zhang, Zheng Hu\\*\n\nState Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications {liujiarun01, zhangch, huzheng} $@$ bupt.edu.cn\n\n# Abstract\n\nThe rapid advancement of autonomous web navigation has significantly benefited from grounding pretrained Large Language Models (LLMs) as agents. However, current research has yet to fully leverage the redundancy of HTML elements for contrastive training. This paper introduces a novel approach to LLM-based web navigation tasks, called Web Element Preference Optimization (WEPO). WEPO utilizes unsupervised preference learning by sampling distance-based non-salient web elements as negative samples, optimizing maximum likelihood objective within Direct Preference Optimization (DPO). We evaluate WEPO on the Mind2Web benchmark and empirically demonstrate that WEPO aligns user high-level intent with output actions more effectively. The results show that our method achieved the state-of-theart, with an improvement of $1 3 . 8 \\%$ over WebAgent and $5 . 3 \\%$ over the visual language model CogAgent baseline. Our findings underscore the potential of preference optimization to enhance web navigation and other web page based tasks, suggesting a promising direction for future research.\n\n# Introduction\n\nThe field of autonomous web navigation has seen significant advancements, driven by the capabilities of Large Language Models (LLMs) in both mobile and webpage interactions (Wang et al. 2024b; Mialon et al. 2023; Xi et al. 2023). Preliminary attempts, such as the ChatGPT Plugin (OpenAI 2023), have also started building practical applications of web knowledge-based chatbot.\n\nWeb navigation can be described as processes where agents perform specific tasks on behalf of human users within a web environment, involving the interpretation of high-level user instructions, decomposing them into basic operations, and interacting with complex web pages dynamically. To achieve this, agents must understand intricate web scenarios, adapt to dynamic changes such as noisy text and evolving HTML structures, and generalize successful operations to unseen tasks, thus freeing humans from repetitive interactions with computer interfaces.\n\nTraditional web agents trained through reinforcement learning (Shi et al. 2017; Yao et al. 2022) often mimic human behavior using predefined actions like typing, searching, and navigating to a specific page. However, they frequently struggle with the complexities of real-world web environments and the challenges of designing effective reward functions. Recent research has leveraged the HTML understanding, logical reasoning, and code generation capabilities of LLMs, enabling agents to comprehend long HTML documents and predict the next action steps. Notable examples include Mind2Web (Deng et al. 2024), which provides an realistic interaction dataset and fine-tunes multiple LLMs to summarize verbose HTML and iteratively optimize and execute actions. Other works such as WebGum (Furuta et al. 2023) and CogAgent (Hong et al. 2023) construct multimodal architectures, enhancing agents with visual perception abilities through supervised learning with a multimodal corpus that includes HTML screenshots. These prior works are thoroughly summarized in our related work section.\n\nIn parallel, preference learning in fine-tuning LLMs has gained prominence, particularly since the introduction of Reinforcement Learning with Human Feedback (RLHF) in GPT-3 (Ziegler et al. 2019; Ouyang et al. 2022), which aligns model outputs with human preferences through reward modeling and reinforcement learning with KL divergence constraints. More subsequent works, such as Direct Preference Optimization (DPO) (Amini, Vieira, and Cotterell 2024) and its variants (Hong, Lee, and Thorne 2024; Morimura et al. 2024), have reparameterized the reward function and optimized training efficiency. These advancements have primarily focused on mainstream tasks in natural language processing, such as dialogue generation and code generation. To our knowledge, the proven effectiveness of preference optimization algorithms like DPO has not been applied to web task automation.\n\nMoreover, existing autonomous web navigation research has not fully exploited the potential of contrastive learning using non-salient HTML elements. After observing the structural complexity and crowded element arrangement in HTML on both Mind2Web and real web environment, we realized that these environments are naturally conducive to data-augmented preference learning. We hypothesize that incorporating preference learning can significantly enhance LLM-based agents’ capabilities in web navigation tasks.\n\nMotivated by this, we introduce Web Element Preference Optimization (WEPO), a novel framework that integrates preference optimization algorithms into mainstream LLM-based web navigation tasks. By sampling non-relevant web elements as negative samples, we implement preference learning that requires no human effort, thereby utilizing redundant information in the web environment and achieving high sample efficiency. Specifically, we design a heuristic distance-based element sampling method tailored to the DOM tree structure to enhance the efficiency of contrastive learning. WEPO then maximizes the likelihood of operations on preferred elements and minimizes it for dis-preferred elements, aligning user high-level intent with agent operation sequences. We illustrate WEPO in Figure 1, provide detailed implementation steps and the theoretical foundation of WEPO in the subsequent sections.\n\n![](images/507571e4539dcdd3308ca44f7b7b0a68b30163899354089efa2dd60b9f2a758a.jpg)  \nFigure 1: Illustration of Web Element Preference Optimization (WEPO). Given user intent, Find me a M2 Mac Air Laptop with $I 5 ^ { \\prime \\prime }$ screen, WEPO combines the correct element (marked in green) with heuristic rule-based sampled negative elements (marked in red) to construct preference pairs. This process utilizes the maximum likelihood objective function proposed in algorithms such as DPO to fine-tune the language model, thereby enhancing its accuracy in element discrimination and selection.\n\nFor experiments, we selected the Mind2Web dataset due to its high task diversity and realistic web scenarios, which best validate the capabilities of fine-tuned LLM agents. Our experiments on multiple mainstream open-sourced models demonstrate that our WEPO significantly outperforms traditional supervised fine-tuning (SFT) methods, exceeding the MindAct (Deng et al. 2024) baseline by $2 0 . 0 \\%$ and WebAgent (Gur et al. 2023) by $1 3 . 8 \\%$ . WEPO also surpasses visual language model (VLM) CogAgent (Hong et al. 2023) by $5 . 3 \\%$ with smaller model parameters and faster inference time, achieving state-of-the-art performance.\n\nWe believe WEPO represents a significant advancement in autonomous web navigation, leveraging HTML structure based preference optimization to enhance task performance and suggesting promising directions for future research in LLM-based web navigation and related applications.\n\n# Related Work\n\nWeb Navigation with LLMs. In the area of web navigation and web-based task automation, the integration of LLMbased agents has shown considerable promise. Kim, Baldi, and McAleer (2024) and Sridhar et al. (2023) introduces the use of prompting schemes combined with criticism or hierarchical modularized design, Li et al. (2023) exploits zero-shot prompt learning via self-reflection and structured thought management, and Zheng, Wang, and An (2023) applies structural prompting with exemplar retrieval to achieve few-shot in-context learning.\n\nGur et al. (2023) and Gu¨r et al. (2023) demonstrated the effectiveness of encoder-decoder architectures such as HTML-T5, tailored to the HTML tree structure through sophisticated local and global attention mechanisms and a mixture of denoising objectives. Deng et al. (2024) introduced MindAct framework, which simplifies web interaction by transforming generation tasks into multiple-choice formats through instruction fine-tuning, thereby gaining decent performance on Mind2Web benchmark. In addition, Nakano et al. (2021) developed WebGPT, which leverages reinforcement learning with human feedback (RLHF) (Ouyang et al. 2022; Ziegler et al. 2019) to align decisionmaking processes with human-preferred answers. Yao et al. (2022) discusses the integration of reinforcement learning (RL) for scalable real-world web shopping, highlighting the design of heuristic rewards that enhance learning efficiency. CC-Net (Humphreys et al. 2022), despite not using large language models, utilizes a hybrid architecture that integrates pixel-based inputs via ResNet (He et al. 2016) blocks and language embeddings through transformer blocks (Vaswani et al. 2017), demonstrating exceptional performance through its effective combination of RL and imitation learning.Attempts to address web navigation using multimodal large language models (MLLMs) are also emerging at scale (Hong et al. 2023; You et al. 2024; Wang et al. $2 0 2 4 \\mathrm { a }$ ; Niu et al. 2024; Baechler et al. 2024; Cheng et al. 2024; Furuta et al. 2023). Among them, CogAgent once reached the state-of-the-art on Mind2Web benchmark by using a high-resolution cross-modular image encoder in conjunction with visual language model (VLM).\n\nBenchmarks in web navigation have evolved rapidly from the simplified MiniWoB (Shi et al. 2017) to the advanced Mind2Web (Deng et al. 2024) and other alternatives (Lu\\`, Kasner, and Reddy 2024; Zhou et al. 2023; He et al. 2024). Mind2Web tackles real-world complexities by incorporating 137 real-world websites into a wide range of 2350 multi-step tasks across 31 domains.\n\nPreference Learning with LLMs. Fine-tuning large language models with preference objective has evolved significantly with Direct Preference Optimization (DPO) (Rafailov et al. 2024) and its variants (Hong, Lee, and Thorne 2024; Meng, Xia, and Chen 2024; Morimura et al. 2024; Amini, Vieira, and Cotterell 2024) improving on traditional RLHF approaches (Christiano et al. 2017; Ziegler et al. 2019; Stiennon et al. 2020; Ouyang et al. 2022). Recent advances focus on improving dataset quality (Morimura et al. 2024), introducing marginal distinctions to better control bias (Duan et al. 2024), optimizing the preference labeling process and enhancing sample efficiency by minimizing human oracle involvement (Bai et al. 2022) and acquire comparison pairs actively (Muldrew et al. 2024).\n\nPreference Learning in Web Scenarios. The application of preference learning to web-based tasks is not a new concept. Notable early work by Radlinski and Joachims (2005) leveraged query chains and implicit user feedback, such as click-through data, to refine search engine algorithms. This approach aimed to capture subtle user preferences that were not explicitly stated but could be inferred from their search behavior sequences. Xiang et al. (2010) and Zhu et al. (2021) also explored preference modeling and ranking for web retrieval applications, including data augmentation of user interaction sequences for comparative learning. These works provide a solid foundation for preference in web scenarios, although they predate the era of large language models and did not attempt to address web navigation issues directly.\n\nAs mentioned above, Nakano et al. (2021) revolves around fine-tuning GPT-3 (Brown et al. 2020) to answer long-form questions in a text-based web environment. The training of WebGPT involves behavior cloning followed by rejection sampling against a reward model trained to predict human preferences, which is considered as an adaptation of preference learning to web QA tasks.\n\n# Web Element Preference Optimization Task Formulation\n\nWe formulate the web navigation task as a partially observable Markov decision process (POMDP) $( S , A , T , R , I , O )$ according to Yao et al. (2022), with state space $S$ , action space $A$ , deterministic transition function $T : S \\times A \\to S$ , reward function $R : S \\times A \\to [ 0 , 1 ]$ , intent space $I$ and a state observation space $S \\times I \\to O$ . A state $s \\in \\ S$ represents a webpage, an action $a \\in A ( s )$ corresponds to an operation on a webpage, a high-level natural language intent $\\textit { i } \\in \\textit { I }$ represents a complex web interaction, usually involving implicit multi-step sub-instructions. Consistent with mainstream efforts (Deng et al. 2024; Gur et al. 2023), we discard the use of a reward function $r = R ( s , a )$ and do not consider employing reinforcement learning in this work. Within interaction loop of the web environment, numerous web elements $\\boldsymbol { e } _ { k }$ exist, yielding candidate set $E =$ $\\{ e _ { 1 } , e _ { 2 } , . . . , e _ { m } \\}$ . The target element is denoted as $\\hat { e }$ with a ground truth operation $\\hat { o }$ , which are both labeled through human supervision, thereby determining $\\boldsymbol { \\hat { a } } ~ = ~ a ( \\hat { e } , \\hat { o } )$ . In the Mind2Web (Deng et al. 2024) setting, a state $s$ includes snapshots in multiple formats, such as HTML code and trace files. We exclusively utilize the HTML scripts of the webpage for WEPO learning. For each state, $E$ comprises all web elements that collectively form the current page. An action $a$ encompasses clicking on an interactive element (CLICK,element ID), inputting textual content to an input field (TYPE,element ID,value) and selecting an option (SELECT,element ID,value).\n\n$$\n\\begin{array} { r l } { \\mathcal { L } _ { \\mathrm { D P O } } \\left( \\pi _ { \\theta } ; \\pi _ { \\mathrm { r e f } } \\right) = - \\mathbb { E } _ { \\left( x , a _ { w } , a _ { l } \\right) \\sim \\mathcal { D } } \\Bigg [ \\log \\sigma \\Bigg ( } & { { } } \\\\ { \\beta \\log \\frac { \\pi _ { \\theta } \\left( a _ { w } \\mid x \\right) } { \\pi _ { \\mathrm { r e f } } \\left( a _ { w } \\mid x \\right) } - \\beta \\log \\frac { \\pi _ { \\theta } \\left( a _ { l } \\mid x \\right) } { \\pi _ { \\mathrm { r e f } } \\left( a _ { l } \\mid x \\right) } \\Bigg ) \\Bigg ] } \\end{array}\n$$\n\n# WEPO Implementation\n\nWe start by introducing the sampling mechanism of WEPO, as illustrated by the partial DOM tree shown in Figure 1. Since every webpage can be parsed into a corresponding DOM tree with each web element represented by a unique node, web elements corresponding to nodes that are closer in proximity under the same ancestor within the DOM typically exhibit greater functional and semantic similarity. Building on this characteristic, we have developed a distance-based sampling method specifically tailored to the\n\nDOM tree structure, which begins by selecting a substantial number (top $k$ ) of element anchors on the page, including one correct element, and then calculates and sorts the sum of the distances between negative and positive samples to their lowest common ancestor (LCA) to quantify their relative distances. Subsequently, the method proportionally samples the top $n$ closest elements from the sorted results, which are then combined with the correct element to form comparisons. We aim to enable the model to effectively learn to distinguish between web elements with similar functions based on the given operational intent.\n\nDuring training, we implement Direct Preference Optimization in WEPO, since DPO is free of reward modeling and training stable (Rafailov et al. 2024). By optimizing the target loss function, WEPO aims to increase the likelihood of operations on preferred elements and decrease the likelihood of operations on dis-preferred elements. As shown in Equation 1, we introduce the maximum likelihood objective proposed in DPO and adaptively modify the preferred completion $y _ { w }$ and dis-preferred completions $y _ { l }$ into preference action pairs $\\mathbf { \\Delta } a _ { w }$ and $a _ { l }$ . Given the pretrained model $\\pi _ { \\boldsymbol { \\theta } }$ and the reference model $\\pi _ { \\mathrm { r e f } }$ initialized from $\\pi _ { \\boldsymbol { \\theta } }$ , we fine-tune $\\pi _ { \\boldsymbol { \\theta } }$ according to Equation 1, where $\\beta$ is a hyperparameter that controls the penalty for deviations from $\\pi _ { \\mathrm { r e f } }$ .\n\nWe demonstrate the pseudo-code for WEPO implementation in Algorithm 1, which illustrates how ${ a } _ { w } , { a } _ { l }$ , and $x$ used for optimizing are obtained at each training step. First, we clean and prune the HTML code. Consistent with previous work (Gu¨r et al. 2023; Deng et al. 2024), we adapt an element-centric approach to isolate HTML snippets. By focusing on a key element, we navigate its ancestors within the HTML tree, guided by a simple constraint that monitors the tree’s width and depth. We stop this traversal once the number of descendants exceeds predefined thresholds, thus defining the snippet using the sub-tree. We introduce a pruning ratio $k$ that represents the number of target elements remaining after pruning. We ensure that the pruned HTML snippet contains the ground truth element during training; In inference time, we use a small ranking LM derived from the candidate generation stage of Deng et al. (2024) to implement priority-based HTML pruning, which scores all elements first and then selects the top- $k$ elements with the largest logits. Subsequently, we concatenate the preprocessed HTML state $s ^ { \\prime }$ , historical trajectory $\\tau$ , initial intent $i$ and a sophisticated prompt template $P$ to generate the input $x$ , where $\\oplus$ denotes string concatenation.\n\nSubsequently, we apply the tailored distance-based sampling method to yield candidate set $E$ . We set the number of negative samples as $n$ , corresponding to a positive-tonegative sample ratio of $1 : n$ . After obtaining the negative elements $\\{ e _ { l _ { 1 } } , e _ { l _ { 2 } } , . . . , e _ { l _ { n } } \\}$ , we employ a designed heuristic rule $f _ { o p }$ to randomly sample the corresponding negative operations $o _ { l _ { i } }$ , which involves selectively replacing TYPE and SELECT to ensure balanced and diverse sample types. This replacement occurs only when $\\hat { o } \\neq { \\mathrm { C L I C K } }$ , as the negative samples obtained through sampling have a very low probability of being TYPE or SELECT, adding data balance without confusing the LLM about the functionality of webpage elements. The rule empirically sets the replacement proba\n\n# Algorithm 1: WEPO Algorithm\n\nRequire: (for each step) Intent $i$ , current HTML $s$ , action trajectory $\\tau$ ; prompt template $P$ ; pretrained LM $\\pi _ { \\boldsymbol { \\theta } }$ , reference LM $\\pi _ { \\mathrm { r e f } }$ and deviation parameter $\\beta$ ; Pruning ratio $k$ , negative ratio $n$ ; Target element $\\hat { e }$ , corresponding operation $\\hat { o }$ and target action $\\hat { a } = a ( \\hat { e } , \\hat { o } )$ ;   \nEnsure: (for each step) 1: Clean and prune the HTML DOM tree with $k$ elements remaining $s ^ { \\prime } = f _ { p r u n e } ( s , k )$ ;   \n2: Concatenate $x = \\bar { s } ^ { \\prime } \\oplus \\tau \\oplus i \\oplus P$ 3: Get positive action $\\boldsymbol { a } _ { w } = \\boldsymbol { \\hat { a } }$ ;   \n4: Sample $n$ negative elements $\\{ e _ { l _ { 1 } } , e _ { l _ { 2 } } , . . . , e _ { l _ { n } } \\}$ from candidate set $E  s ^ { \\prime }$ based on LCA distance from $\\hat { e }$ ; 5: for $i = 1$ to $n$ do   \n6: Set ϵ random uniform $( 0 , 1 )$   \n7: $o _ { l _ { i } } \\sim f _ { o p } ( \\{ \\mathrm { C L I C K } , \\mathrm { T Y P E } , \\mathrm { S E L E C T } \\} , \\hat { o } , \\epsilon )$   \n8: Get $i$ -th negative action $a _ { l _ { i } } = a ( e _ { l _ { i } } , o _ { l _ { i } } )$   \n9: Optimize $\\nabla _ { \\theta } \\mathcal { L } _ { \\mathrm { D P O } } \\left( \\pi _ { \\theta } ; \\pi _ { \\mathrm { r e f } } \\right)$ in Equation 1 given $\\beta$ , $x$ , $a _ { w }$ and $a _ { l } = a _ { l _ { i } }$ ;\n\nbility threshold at 0.33, and several verification confirmed that values around this threshold have no significant impact on WEPO. Therefore, when the positive sample is a CLICK operation, the negative samples are also CLICK; however, when the positive sample involves the other two actions, the negative samples might be changed to CLICK. Finally, we obtained $a _ { w } , a _ { l }$ , and $x$ , and used Equation 1 to perform gradient back-propagation on the pretrained LM $\\pi _ { \\boldsymbol { \\theta } }$ , with $\\beta$ controlling the deviation from the reference model $\\pi _ { \\mathrm { r e f } }$ . We opt for straightforward random sampling as an alternative, where the randomly distributed negative samples also maintain a low correlation with $\\hat { e }$ on the webpage.\n\n# Experiments Experimental Setup\n\nWe employ three mainstream pretrained LLMs of progressively increasing model sizes to validate the scaling effects. These models include Llama- $3 { - } 8 \\mathbf { B } ^ { 1 }$ , Mistral-7B-Instruct$\\mathrm { v 0 . 1 }$ (Jiang et al. 2023), and Gemma-2B (Team et al. 2024). For the hyperparameters of WEPO, we set the deviation parameter $\\beta$ to 0.95 and the negative sample ratio to 1:3. In Mind2Web (Deng et al. 2024), a DeBERTa (He et al. 2020) model trained within the candidate generation module uses recall $@ 5 0$ for ranking elements and constructing a candidate pool for subsequent experiments. We similarly select a pruning ratio of $k = 5 0$ to maintain consistency for comparison, preserving 50 central elements and their neighboring elements tagged with element ID. For the selection of $k$ and $n$ values, we provide detailed explanations in the forthcoming ablation studies. All models were configured with a maximum context length of 8192 tokens. We employ the\n\nTable 1: Overall performance of various models on different test sets, with evaluation metrics corresponding to SSR / Operation F1 $( \\% )$ . The results were obtained under a negative sample ratio of $1 : 3 .$ . Notably, our Llama3-8B-WEPO model achieved the highest scores in both overall SSR and Operation F1. Our top scores exceeded those of the much larger CogAgent (17B) model by $5 . 3 \\%$ and WebAgent $( 3 \\mathbf { B } + 5 4 0 \\mathbf { B } )$ by $1 3 . 8 \\%$ . Additionally, our smaller Gemma-2B-WEPO model managed to closely match and even slightly outperform the approximately 3B Flan-T5 based models (Chung et al. 2024) like MindAct.   \n\n<html><body><table><tr><td>Model SSR /Op.F1 (%)</td><td>overall</td><td>cross_domain</td><td>cross_task</td><td>cross_website</td></tr><tr><td>Flan-T5-XL MindAct</td><td>43.5 / 69.1</td><td>39.6 / 66.5</td><td>52.0 / 75.7</td><td>38.9 / 65.2</td></tr><tr><td>Llama3-8B MindAct</td><td>55.1/ 65.8</td><td>57.6 /68.7</td><td>56.1/63.2</td><td>51.7 / 65.6</td></tr><tr><td>CogVLM-17B CogAgent</td><td>58.2/ -</td><td>59.4/ -</td><td>62.3 / -</td><td>54.0 / -</td></tr><tr><td>HTML-T5-XL +Flan-U-PaLMWebAgent</td><td>49.7 / -</td><td>48.3 / -</td><td>57.8/ -</td><td>42.9 / -</td></tr><tr><td>Llama3-8B WEPO random</td><td>61.1 / 73.9</td><td>62.5 / 77.5</td><td>62.5 / 67.2</td><td>58.4 / 77.1</td></tr><tr><td>Mistral-7B WEPO random</td><td>57.2 / 73.8</td><td>58.0 / 75.9</td><td>59.0 / 72.1</td><td>54.8 / 73.3</td></tr><tr><td>Gemma-2B WEPO random</td><td>45.4 / 49.5</td><td>49.1 / 55.7</td><td>45.2 / 42.9</td><td>41.9 / 50.0</td></tr><tr><td>Llama3-8BWEPO distance-based</td><td>63.5 / 76.1</td><td>64.4 / 81.6</td><td>66.1 / 74.9</td><td>60.0 / 71.9</td></tr><tr><td>Mistral-7BWEPOdistance-based</td><td>59.5 / 76.8</td><td>59.8 / 80.9</td><td>62.1 / 76.2</td><td>56.7 /73.2</td></tr><tr><td>Gemma-2B WEPO distance-based</td><td>48.4 / 53.3</td><td>53.5 / 60.3</td><td>47.9 / 48.8</td><td>43.7 / 50.7</td></tr></table></body></html>\n\nLow Rank Adaptation (LoRA) technique (Hu et al. 2021) for parameter-efficient fine-tuning, which helps reduce memory usage and conserve budget. The learning rate is set at 0.0001, and we use a combination of learning rate warmup and a cosine decay strategy for training.\n\nEvaluation Metrics. In this paper, we adopt the step success rate (SSR) and Operation F1 score from Deng et al. (2024). We no longer use element accuracy and success rate because it is evident that both metrics are linearly associated with SSR, and successful interaction for the web navigation agent is only considered when both the element positioning and the corresponding operation are correct, which is precisely what SSR measures. The Operation F1 score is equally indispensable as it considers the accuracy of the input value for Type and SELECT commands. Furthermore, both baseline studies by Gur et al. (2023) and Hong et al. (2023) exclusively utilized SSR as the sole metric.\n\nAdditionally, we designed the element distance metric to measure the positional deviation between the elements selected by the WEPO model and the labeled elements. Consistent with the previous sampling strategy, we calculate the sum of the distances (in terms of steps) between the nodes corresponding to two different web elements and their lowest common ancestor (LCA) in the DOM tree to represent their relative positions.\n\n# Results\n\nWe thoroughly evaluate WEPO on the partitioned three-tier held-out test sets in Mind2Web (Deng et al. 2024), including cross-domain, cross-website and cross-task datasets. This allows us to understand how well our method can generalize across different domains, websites, and tasks.\n\nThe experimental results are detailed in Table 1. Compared to previous works (Deng et al. 2024; Hong et al. 2023; Gur et al. 2023), without utilizing any multimodal information or customized model architecture, our best results obtained by fine-tuning the 8-billion parameter Llama3 pretrained model with web element preference learning exceeded the three baselines by $2 0 . 0 \\%$ , $5 . 3 \\%$ , and $1 3 . 8 \\%$ respectively. Even though there is still a significant gap between the model performance of gemma-2B and the larger model $( > 1 0 \\%$ ), these results demonstrate the effectiveness of the WEPO method, proving its efficiency and generalizability in enabling LLM-based agents for web navigation tasks. In assessing generalization capabilities at different levels, although Deng et al. (2024) found that their model performed best in the Cross-Task setting, WEPO has narrowed the gap between different test sets to less than $6 . 1 \\%$ . Additionally, observing the performance of WEPO models of different sizes, we found that the step success rate increases with model size, which verifies the presence of the scaling effect in the Mind2Web real-world benchmark.\n\nTo eliminate the influence of model base choice on the results, we thoroughly applied supervised fine-tuning using the action prediction prompts from Mind2Web on Llama3- 8B. The results indicate that compared to this upgraded MindAct baseline, WEPO also outperforms by $8 . 4 \\%$ on the SSR metric and by $1 1 . 0 \\%$ on the Operation F1 score. This again demonstrates that fine-tuning with positive human annotation only, which is a simple maximum likelihood approach, is less effective than our WEPO method. We then conducted a comparison of its performance with random sampling in Table 1 to validate the effectiveness of the distancebased sampling method. Compared to random sampling, the distance-based approach achieved a $2 . 3 \\%$ to $3 \\%$ higher SSR, demonstrating its efficiency. In the subsequent analysis of the element distance evaluation metric, it can also be seen that this method successfully enhances the accuracy of the model’s selections, indirectly suggesting better alignment with user high-level intents.\n\nWhat exactly is the role of preference learning in enhancing performance? For the Operation F1 score, we observed a significant improvement from Gemma-2B-WEPO to Mistral-7B-WEPO, which indicates that an increase in the number of LLM training parameters not only makes the web element localization more accurate but primarily enhances the accuracy of input or select text values. For MindAct, however, our best performance was not significantly ahead in F1 scores, suggesting that the WEPO method, compared to traditional supervised fine-tuning, enhances the accuracy of element selection in pretrained LMs. We infer that WEPO leverages this enhancement through a contrastive training scheme, wherein the model learns to distinguish between elements that are critical for decision-making and those that are not. Particularly when intentions are abstract, a web page at any given moment may contain multiple elements that are easily confused, and WEPO reduces the likelihood of incorrect element selection by the agent. By incorporating a contrastive mechanism, WEPO not only improves the accuracy of navigation tasks but also enhances the model’s generalizability across different web layouts and designs.\n\nTable 2: SSR performance $( \\% )$ from ablations on the cleaned HTML pruning ratio $k$ value. For WEPO training, we forcibly included the ground truth element, akin to a teacher-forcing mechanism. The results shown in the table, except for the last row, represent the SSR after reassembling HTML from elements filtered through the first-stage ranking. Llama3-8B-WEPO maintained a $1 0 . 1 \\%$ improvement to Flan-T5 based MindAct even after reducing $k$ .   \n\n<html><body><table><tr><td>Model SSR (%)</td><td>cross_domain</td><td>cross_task</td><td>cross_website</td></tr><tr><td>Flan-T5-XLMindActk = 50</td><td>39.6</td><td>52.0</td><td>38.9</td></tr><tr><td>Llama3-8B MindActk = 50</td><td>57.6</td><td>56.1</td><td>51.7</td></tr><tr><td>Llama3-8B WEPO k = 50</td><td>64.4</td><td>66.1</td><td>60.0</td></tr><tr><td>Llama3-8B WEPOk= 10</td><td>49.7</td><td>55.0</td><td>46.5</td></tr><tr><td>Llama3-8B WEPO k = 10 w. ground truth é</td><td>87.2</td><td>88.7</td><td>85.4</td></tr></table></body></html>\n\n![](images/cf8b80ab50b99a5441edddb3a86e72bf1a2e6ac07159d168b3c5f98ae5138a8c.jpg)  \nFigure 2: Statistical distribution of Element Distance for different models (Llama3-8B, Mistral-7B and Gemma-2B) on the test dataset. As the model size increases, the relative deviation in element distances decreases.\n\nThe results in Figure 2 provide great support for this inference. As the model size increases, the element distance consistently decreases. Element distance, which represents the relative position of elements in the DOM, is closely correlated with the elements’ function and design purpose. Coupled with the score analysis in Table 1, this decreasing deviation suggests that the Llama3-8B-WEPO model becomes increasingly accurate in aligning with the intended functions and design of web elements compared to the smaller models. This result clearly indicates that the model has successfully learned to recognize these web design differences, proving that WEPO is an effective method for adapting to the HTML structure, or more broadly for aligning with web design principles. Furthermore, we have demonstrated the effectiveness of our newly proposed element distance evaluation metric.\n\n![](images/83868de1c0c4141ccc265ca7d563b7d52096fbde64355ef841fac9e71ada170f.jpg)  \nFigure 3: Ablation studies on the negative ratio. We experimented with the Llama3-8B-WEPO model at $n = 1 , 3 , 5$ and calculated the average SSR $( \\% )$ on three cross-test sets, which were $5 7 . 7 \\%$ , $6 3 . 5 \\%$ for distance-based sampling and $5 6 . 6 \\%$ , $6 1 . 1 \\%$ , and $6 2 . 4 \\%$ for random sampling, respectively. An elbow point was observed at $n = 3$ for random sampling, where the increase in SSR sharply levels off. Furthermore, the performance of distance-based sampling at a 1:3 ratio has already surpassed that of random sampling at a 1:5 ratio by $1 . 1 \\%$ .\n\nWhy choose a 1:3 ratio for negative samples? We also conducted ablation experiments on the negative sample ratio in the WEPO Algorithm 1. Empirically, we aimed to sample as many negative samples as possible to enhance performance through WEPO without over-sampling and excessively increasing the training overhead. We uniformly sampled $n$ values of 1, 3, and 5, selected the best-performing Llama3-8B-WEPO for experimentation and averaged the results from two rounds. As shown in Figure 3, the overall average scores on the test set increase with larger $n$ values, with a noticeable elbow point at $n = 3$ for random sampling. We ultimately selected a 1:3 ratio as a fixed hyperparameter to avoid linearly increasing training costs. Additionally, we observed that the distance-based sampling method at $n = 3$ outperformed random sampling at $n = 5$ , significantly improving overall sample learning efficiency during training. Furthermore, too many negative samples could potentially create an imbalance between positive and negative sample quantities. However, we did not experiment with larger $n$ values to verify potential performance degradation, as excessively large $n$ values could hinder reproducibility.\n\nWhat is the impact of HTML $k$ -pruning on performance? To address this question, we conducted ablation studies on the selection of $k$ , with results shown in Table 2. As our pruning is based on the first stage ranking LM of MindAct, understanding the impact of this preprocessing module is crucial. The authors of Deng et al. (2024) disclosed that when $k = 5 0$ , the fine-tuned DeBERTa (He et al. 2020) model achieved recall accuracies of $8 8 . 9 \\%$ , $8 5 . 3 \\%$ , and $8 5 . 7 \\%$ on three held-out test datasets, and smaller values of $k$ were not adopted due to lower recall rates.\n\nHowever, in our results, when we set $k$ to a smaller value of 10, Llama3-8B-WEPO still performed above the baseline. We deduce that although reducing $k$ significantly decreases the recall rate of the ranking LM, it provides the WEPO-trained model with a shorter HTML snippet and fewer ID options, enhancing discrimination accuracy. Moreover, when we forcibly added the ground truth web element $\\hat { e }$ directly into the candidate pool, the ablation model’s SSR surged to over $80 \\%$ . This conclusively demonstrates that the WEPO method has significantly improved the model’s capability in action prediction and has shifted the original candidate retrieval module from a secondary issue to a major bottleneck limiting the web agent’s progress on complex longcontext web pages.\n\n# Future Work\n\nThere remain several avenues for further development of this approach. While WEPO has shown empirical success, a deeper theoretical analysis could provide more foundational insights into why and how preference optimization effectively enhances web navigation tasks. Inspired by the advancements of models like WebFormer (Wang et al. 2022) and HTML-T5 (Gu¨r et al. 2023), future iterations of WEPO could benefit from a dedicated HTML encoder that is specifically tailored to understand the hierarchical and nested structures of web documents.\n\nThe current implementation of WEPO has not explored its potential over extremely large context lengths such as $\\mathrm { \\ p h i } - 3 - 1 2 8 \\mathrm { k \\Omega }$ (Abdin et al. 2024). Future research could look into the scaling abilities of WEPO when applied to such models, which might be crucial for handling complex web navigation tasks that involve detailed web pages.\n\nWhile WEPO shows promising results in a controlled benchmark environment, its ability to generalize across highly diverse, real-world web interfaces remains an area for further investigation. The variability in web design, interactive elements, and underlying technologies across different websites may affect the consistency of WEPO’s performance. We plan to test the performance of the WEPO method on additional mainstream benchmarks such as WebLINX and WebVoyager (Lu\\`, Kasner, and Reddy 2024; He et al. 2024) in future work.\n\n# Conclusion\n\nThis paper introduced the Web Element Preference Optimization (WEPO), a simple yet novel framework that integrates Direct Preference Optimization (DPO) into LLMbased web navigation tasks. WEPO enhances LLM performance by leveraging distance-based non-salient HTML elements for contrastive learning, effectively aligning model operations with user intent. Our empirical evaluations on the Mind2Web benchmark demonstrate that WEPO surpasses traditional models, achieving an $1 3 . 8 \\%$ improvement over the WebAgent baseline and a $5 . 3 \\%$ enhancement beyond the visual language model CogAgent, while also exhibiting strong generalization across diverse web environments. This research significantly enhances the capabilities of LLMs in web navigation task, contributing to more efficient and intuitive web interactions.\n\n# Acknowledgments\n\nThis work was jointly supported by Beijing University of Posts and Telecommunications (BUPT) - China Mobile Research Institute Joint Innovation Center, and the National Key Laboratory of Networking and Switching Technology under Project of Embodied Intelligent Agent for Intellicise Networks. We extend our heartfelt gratitude to the faculty and staff at BUPT for their invaluable guidance and assistance throughout the research process. We also thank our collaborators and peers who provided critical feedback and constructive discussions that enriched the development of this work. Lastly, we acknowledge the broader academic community for inspiring this study through their pioneering efforts in autonomous web navigation and large language model applications.",
    "institutions": [
        "State Key Laboratory of Networking and Switching Technology",
        "Beijing University of Posts and Telecommunications"
    ],
    "summary": "{\n  \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n当前基于大语言模型（LLM）的自主网页导航研究尚未充分利用HTML元素的冗余性进行对比训练，传统网页代理在处理现实网页环境的复杂性和设计有效奖励函数方面存在困难，且已被证明有效的偏好优化算法（如DPO）在网页任务自动化中的应用尚未得到充分探索。解决这些问题对于提升基于LLM的网页导航任务的性能和效率至关重要，能够使网页交互更加高效和智能。\\n\\n**方法概述**\\n论文提出了Web Element Preference Optimization（WEPO）框架，将偏好优化算法集成到主流基于LLM的网页导航任务中，通过采样非相关网页元素作为负样本进行无监督偏好学习，利用网页环境中的冗余信息，实现高样本效率。\\n\\n**主要贡献与效果**\\n- 提出WEPO框架，在Mind2Web基准测试中，Llama3 - 8B - WEPO模型的整体SSR和Operation F1得分最高，整体SSR达到了63.5%，Operation F1达到了76.1%，相比WebAgent提升了13.8%，相比视觉语言模型CogAgent提升了5.3%，相比MindAct提升了20.0%。\\n- 距离采样方法比随机采样的SSR高2.3% - 3%，提高了模型选择的准确性。\\n- 与升级的MindAct基线相比，WEPO在SSR指标上高出8.4%，在Operation F1得分上高出11.0%。\",\n  \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n利用网页元素在DOM树结构中的距离特性，通过距离采样方法选择负样本，结合Direct Preference Optimization（DPO）优化目标损失函数，使模型学习区分功能相似的网页元素，增加对偏好元素操作的可能性，减少对非偏好元素操作的可能性，从而提高模型在网页导航任务中的性能，更好地将用户的高级意图与代理操作序列对齐。\\n\\n**创新点**\\n先前的工作未充分利用HTML元素的冗余性进行对比训练，且偏好优化算法未应用于网页任务自动化。WEPO通过距离采样方法有效利用了网页环境中的冗余信息，进行无监督偏好学习，提高了样本效率，且无需奖励建模，训练稳定。同时，设计了基于距离的元素采样方法，专门针对DOM树结构，有助于模型学习区分功能相似的网页元素。\\n\\n**具体实现步骤**\\n1. 对HTML代码进行清理和修剪，采用元素中心方法隔离HTML片段，设置修剪比例k = 50，保留50个中心元素及其相邻元素，确保修剪后的HTML片段在训练时包含真实元素。在推理时，使用从候选生成阶段派生的小型排序LM对所有元素进行评分，然后选择对数几率最大的前k个元素。\\n2. 拼接预处理后的HTML状态、历史轨迹、初始意图和提示模板，生成输入x。\\n3. 选择页面上数量较多（前k个）的元素锚点，包括一个正确元素，计算负样本和正样本到其最近公共祖先（LCA）的距离之和并排序，按比例从前n个最接近的元素中采样，将其与正确元素组合形成对比。\\n4. 获取正样本动作aw，即目标动作。\\n5. 根据元素与目标元素在DOM树中的最低共同祖先（LCA）距离，从候选集中采样n个负样本元素，设置负样本比例为1 : n。\\n6. 对于每个负样本元素，使用启发式规则随机采样相应的负操作，确保样本类型的平衡和多样性。当正样本是CLICK操作时，负样本也是CLICK；当正样本涉及其他两种动作时，负样本可能会改为CLICK。启发式规则将替换概率阈值经验性地设置为0.33。\\n7. 得到aw、al和x，使用公式 $\\mathcal{L}_{DPO}(\\pi_{\\theta};\\pi_{ref}) = - \\mathbb{E}_{(x, a_w, a_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_{\\theta}(a_w \\mid x)}{\\pi_{ref}(a_w \\mid x)} - \\beta \\log \\frac{\\pi_{\\theta}(a_l \\mid x)}{\\pi_{ref}(a_l \\mid x)} \\right) \\right]$ 对预训练的LM进行梯度反向传播，β是控制与参考模型偏差的超参数，设置为0.95。\\n\\n**案例解析**\\n给定用户意图“Find me a M2 Mac Air Laptop with $I5^{\\prime \\prime}$ screen”，WEPO将正确元素（标记为绿色）与基于启发式规则采样的负元素（标记为红色）组合，构建偏好对，利用DPO等算法中提出的最大似然目标函数微调语言模型，增强其在元素辨别和选择上的准确性。\",\n  \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- MindAct\\n- WebAgent\\n- CogAgent\\n\\n**性能对比**\\n*   **在 [SSR/Operation F1] 指标上：** 本文的Llama3 - 8B - WEPO模型在整体SSR和Operation F1上取得了最高分数，整体SSR达到了63.5%，Operation F1达到了76.1%。高于MindAct（Llama3 - 8B MindAct的SSR为55.1%，Operation F1为65.8%）、WebAgent（整体SSR为49.7%）和CogAgent（整体SSR为58.2%），分别高出了8.4%、13.8%和5.3%。Gemma - 2B - WEPO模型也能与基于Flan - T5的MindAct模型相当甚至略胜一筹。\\n*   **在 [元素距离] 指标上：** 随着模型大小的增加，元素距离逐渐减小，Llama3 - 8B - WEPO模型在元素选择上更加准确，与用户的高级意图更好地对齐。与随机采样相比，基于距离的采样方法使SSR提高了2.3% - 3%，证明了该方法的有效性。此外，在负样本比例为1 : 3时，距离采样方法的性能超过了随机采样在1 : 5比例时的性能，提高了1.1%。在HTML修剪比例k的实验中，当k = 10时，Llama3 - 8B - WEPO仍表现优于基线；当强制加入真实元素时，消融模型的SSR超过80%。\",\n  \"keywords\": \"### 关键词\\n\\n- 网页导航 (Web Navigation, N/A)\\n- 大语言模型 (Large Language Models, LLMs)\\n- 偏好优化 (Preference Optimization, N/A)\\n- 网页元素偏好优化 (Web Element Preference Optimization, WEPO)\\n- 直接偏好优化 (Direct Preference Optimization, DPO)\\n- 元素采样 (Element Sampling, N/A)\"\n}"
}