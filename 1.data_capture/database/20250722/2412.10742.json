{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.10742",
    "link": "https://arxiv.org/abs/2412.10742",
    "pdf_link": "https://arxiv.org/pdf/2412.10742.pdf",
    "title": "WEPO: Web Element Preference Optimization for LLM-based Web Navigation",
    "authors": [
        "Jiarun Liu",
        "Jia Hao",
        "Chunhong Zhang",
        "Zheng Hu"
    ],
    "categories": [
        "cs.CL"
    ],
    "publication_date": "2024-12-14",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "State Key Laboratory of Networking and Switching Technology",
        "Beijing University of Posts and Telecommunications"
    ],
    "paper_content": "# WEPO: Web Element Preference Optimization for LLM-based Web Navigation\n\nJiarun Liu, Jia Hao, Chunhong Zhang, Zheng Hu\\*\n\nState Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications {liujiarun01, zhangch, huzheng} $@$ bupt.edu.cn\n\n# Abstract\n\nThe rapid advancement of autonomous web navigation has significantly benefited from grounding pretrained Large Language Models (LLMs) as agents. However, current research has yet to fully leverage the redundancy of HTML elements for contrastive training. This paper introduces a novel approach to LLM-based web navigation tasks, called Web Element Preference Optimization (WEPO). WEPO utilizes unsupervised preference learning by sampling distance-based non-salient web elements as negative samples, optimizing maximum likelihood objective within Direct Preference Optimization (DPO). We evaluate WEPO on the Mind2Web benchmark and empirically demonstrate that WEPO aligns user high-level intent with output actions more effectively. The results show that our method achieved the state-of-theart, with an improvement of $1 3 . 8 \\%$ over WebAgent and $5 . 3 \\%$ over the visual language model CogAgent baseline. Our findings underscore the potential of preference optimization to enhance web navigation and other web page based tasks, suggesting a promising direction for future research.\n\n# Introduction\n\nThe field of autonomous web navigation has seen significant advancements, driven by the capabilities of Large Language Models (LLMs) in both mobile and webpage interactions (Wang et al. 2024b; Mialon et al. 2023; Xi et al. 2023). Preliminary attempts, such as the ChatGPT Plugin (OpenAI 2023), have also started building practical applications of web knowledge-based chatbot.\n\nWeb navigation can be described as processes where agents perform specific tasks on behalf of human users within a web environment, involving the interpretation of high-level user instructions, decomposing them into basic operations, and interacting with complex web pages dynamically. To achieve this, agents must understand intricate web scenarios, adapt to dynamic changes such as noisy text and evolving HTML structures, and generalize successful operations to unseen tasks, thus freeing humans from repetitive interactions with computer interfaces.\n\nTraditional web agents trained through reinforcement learning (Shi et al. 2017; Yao et al. 2022) often mimic human behavior using predefined actions like typing, searching, and navigating to a specific page. However, they frequently struggle with the complexities of real-world web environments and the challenges of designing effective reward functions. Recent research has leveraged the HTML understanding, logical reasoning, and code generation capabilities of LLMs, enabling agents to comprehend long HTML documents and predict the next action steps. Notable examples include Mind2Web (Deng et al. 2024), which provides an realistic interaction dataset and fine-tunes multiple LLMs to summarize verbose HTML and iteratively optimize and execute actions. Other works such as WebGum (Furuta et al. 2023) and CogAgent (Hong et al. 2023) construct multimodal architectures, enhancing agents with visual perception abilities through supervised learning with a multimodal corpus that includes HTML screenshots. These prior works are thoroughly summarized in our related work section.\n\nIn parallel, preference learning in fine-tuning LLMs has gained prominence, particularly since the introduction of Reinforcement Learning with Human Feedback (RLHF) in GPT-3 (Ziegler et al. 2019; Ouyang et al. 2022), which aligns model outputs with human preferences through reward modeling and reinforcement learning with KL divergence constraints. More subsequent works, such as Direct Preference Optimization (DPO) (Amini, Vieira, and Cotterell 2024) and its variants (Hong, Lee, and Thorne 2024; Morimura et al. 2024), have reparameterized the reward function and optimized training efficiency. These advancements have primarily focused on mainstream tasks in natural language processing, such as dialogue generation and code generation. To our knowledge, the proven effectiveness of preference optimization algorithms like DPO has not been applied to web task automation.\n\nMoreover, existing autonomous web navigation research has not fully exploited the potential of contrastive learning using non-salient HTML elements. After observing the structural complexity and crowded element arrangement in HTML on both Mind2Web and real web environment, we realized that these environments are naturally conducive to data-augmented preference learning. We hypothesize that incorporating preference learning can significantly enhance LLM-based agents’ capabilities in web navigation tasks.\n\nMotivated by this, we introduce Web Element Preference Optimization (WEPO), a novel framework that integrates preference optimization algorithms into mainstream LLM-based web navigation tasks. By sampling non-relevant web elements as negative samples, we implement preference learning that requires no human effort, thereby utilizing redundant information in the web environment and achieving high sample efficiency. Specifically, we design a heuristic distance-based element sampling method tailored to the DOM tree structure to enhance the efficiency of contrastive learning. WEPO then maximizes the likelihood of operations on preferred elements and minimizes it for dis-preferred elements, aligning user high-level intent with agent operation sequences. We illustrate WEPO in Figure 1, provide detailed implementation steps and the theoretical foundation of WEPO in the subsequent sections.\n\n![](images/507571e4539dcdd3308ca44f7b7b0a68b30163899354089efa2dd60b9f2a758a.jpg)  \nFigure 1: Illustration of Web Element Preference Optimization (WEPO). Given user intent, Find me a M2 Mac Air Laptop with $I 5 ^ { \\prime \\prime }$ screen, WEPO combines the correct element (marked in green) with heuristic rule-based sampled negative elements (marked in red) to construct preference pairs. This process utilizes the maximum likelihood objective function proposed in algorithms such as DPO to fine-tune the language model, thereby enhancing its accuracy in element discrimination and selection.\n\nFor experiments, we selected the Mind2Web dataset due to its high task diversity and realistic web scenarios, which best validate the capabilities of fine-tuned LLM agents. Our experiments on multiple mainstream open-sourced models demonstrate that our WEPO significantly outperforms traditional supervised fine-tuning (SFT) methods, exceeding the MindAct (Deng et al. 2024) baseline by $2 0 . 0 \\%$ and WebAgent (Gur et al. 2023) by $1 3 . 8 \\%$ . WEPO also surpasses visual language model (VLM) CogAgent (Hong et al. 2023) by $5 . 3 \\%$ with smaller model parameters and faster inference time, achieving state-of-the-art performance.\n\nWe believe WEPO represents a significant advancement in autonomous web navigation, leveraging HTML structure based preference optimization to enhance task performance and suggesting promising directions for future research in LLM-based web navigation and related applications.\n\n# Related Work\n\nWeb Navigation with LLMs. In the area of web navigation and web-based task automation, the integration of LLMbased agents has shown considerable promise. Kim, Baldi, and McAleer (2024) and Sridhar et al. (2023) introduces the use of prompting schemes combined with criticism or hierarchical modularized design, Li et al. (2023) exploits zero-shot prompt learning via self-reflection and structured thought management, and Zheng, Wang, and An (2023) applies structural prompting with exemplar retrieval to achieve few-shot in-context learning.\n\nGur et al. (2023) and Gu¨r et al. (2023) demonstrated the effectiveness of encoder-decoder architectures such as HTML-T5, tailored to the HTML tree structure through sophisticated local and global attention mechanisms and a mixture of denoising objectives. Deng et al. (2024) introduced MindAct framework, which simplifies web interaction by transforming generation tasks into multiple-choice formats through instruction fine-tuning, thereby gaining decent performance on Mind2Web benchmark. In addition, Nakano et al. (2021) developed WebGPT, which leverages reinforcement learning with human feedback (RLHF) (Ouyang et al. 2022; Ziegler et al. 2019) to align decisionmaking processes with human-preferred answers. Yao et al. (2022) discusses the integration of reinforcement learning (RL) for scalable real-world web shopping, highlighting the design of heuristic rewards that enhance learning efficiency. CC-Net (Humphreys et al. 2022), despite not using large language models, utilizes a hybrid architecture that integrates pixel-based inputs via ResNet (He et al. 2016) blocks and language embeddings through transformer blocks (Vaswani et al. 2017), demonstrating exceptional performance through its effective combination of RL and imitation learning.Attempts to address web navigation using multimodal large language models (MLLMs) are also emerging at scale (Hong et al. 2023; You et al. 2024; Wang et al. $2 0 2 4 \\mathrm { a }$ ; Niu et al. 2024; Baechler et al. 2024; Cheng et al. 2024; Furuta et al. 2023). Among them, CogAgent once reached the state-of-the-art on Mind2Web benchmark by using a high-resolution cross-modular image encoder in conjunction with visual language model (VLM).\n\nBenchmarks in web navigation have evolved rapidly from the simplified MiniWoB (Shi et al. 2017) to the advanced Mind2Web (Deng et al. 2024) and other alternatives (Lu\\`, Kasner, and Reddy 2024; Zhou et al. 2023; He et al. 2024). Mind2Web tackles real-world complexities by incorporating 137 real-world websites into a wide range of 2350 multi-step tasks across 31 domains.\n\nPreference Learning with LLMs. Fine-tuning large language models with preference objective has evolved significantly with Direct Preference Optimization (DPO) (Rafailov et al. 2024) and its variants (Hong, Lee, and Thorne 2024; Meng, Xia, and Chen 2024; Morimura et al. 2024; Amini, Vieira, and Cotterell 2024) improving on traditional RLHF approaches (Christiano et al. 2017; Ziegler et al. 2019; Stiennon et al. 2020; Ouyang et al. 2022). Recent advances focus on improving dataset quality (Morimura et al. 2024), introducing marginal distinctions to better control bias (Duan et al. 2024), optimizing the preference labeling process and enhancing sample efficiency by minimizing human oracle involvement (Bai et al. 2022) and acquire comparison pairs actively (Muldrew et al. 2024).\n\nPreference Learning in Web Scenarios. The application of preference learning to web-based tasks is not a new concept. Notable early work by Radlinski and Joachims (2005) leveraged query chains and implicit user feedback, such as click-through data, to refine search engine algorithms. This approach aimed to capture subtle user preferences that were not explicitly stated but could be inferred from their search behavior sequences. Xiang et al. (2010) and Zhu et al. (2021) also explored preference modeling and ranking for web retrieval applications, including data augmentation of user interaction sequences for comparative learning. These works provide a solid foundation for preference in web scenarios, although they predate the era of large language models and did not attempt to address web navigation issues directly.\n\nAs mentioned above, Nakano et al. (2021) revolves around fine-tuning GPT-3 (Brown et al. 2020) to answer long-form questions in a text-based web environment. The training of WebGPT involves behavior cloning followed by rejection sampling against a reward model trained to predict human preferences, which is considered as an adaptation of preference learning to web QA tasks.\n\n# Web Element Preference Optimization Task Formulation\n\nWe formulate the web navigation task as a partially observable Markov decision process (POMDP) $( S , A , T , R , I , O )$ according to Yao et al. (2022), with state space $S$ , action space $A$ , deterministic transition function $T : S \\times A \\to S$ , reward function $R : S \\times A \\to [ 0 , 1 ]$ , intent space $I$ and a state observation space $S \\times I \\to O$ . A state $s \\in \\ S$ represents a webpage, an action $a \\in A ( s )$ corresponds to an operation on a webpage, a high-level natural language intent $\\textit { i } \\in \\textit { I }$ represents a complex web interaction, usually involving implicit multi-step sub-instructions. Consistent with mainstream efforts (Deng et al. 2024; Gur et al. 2023), we discard the use of a reward function $r = R ( s , a )$ and do not consider employing reinforcement learning in this work. Within interaction loop of the web environment, numerous web elements $\\boldsymbol { e } _ { k }$ exist, yielding candidate set $E =$ $\\{ e _ { 1 } , e _ { 2 } , . . . , e _ { m } \\}$ . The target element is denoted as $\\hat { e }$ with a ground truth operation $\\hat { o }$ , which are both labeled through human supervision, thereby determining $\\boldsymbol { \\hat { a } } ~ = ~ a ( \\hat { e } , \\hat { o } )$ . In the Mind2Web (Deng et al. 2024) setting, a state $s$ includes snapshots in multiple formats, such as HTML code and trace files. We exclusively utilize the HTML scripts of the webpage for WEPO learning. For each state, $E$ comprises all web elements that collectively form the current page. An action $a$ encompasses clicking on an interactive element (CLICK,element ID), inputting textual content to an input field (TYPE,element ID,value) and selecting an option (SELECT,element ID,value).\n\n$$\n\\begin{array} { r l } { \\mathcal { L } _ { \\mathrm { D P O } } \\left( \\pi _ { \\theta } ; \\pi _ { \\mathrm { r e f } } \\right) = - \\mathbb { E } _ { \\left( x , a _ { w } , a _ { l } \\right) \\sim \\mathcal { D } } \\Bigg [ \\log \\sigma \\Bigg ( } & { { } } \\\\ { \\beta \\log \\frac { \\pi _ { \\theta } \\left( a _ { w } \\mid x \\right) } { \\pi _ { \\mathrm { r e f } } \\left( a _ { w } \\mid x \\right) } - \\beta \\log \\frac { \\pi _ { \\theta } \\left( a _ { l } \\mid x \\right) } { \\pi _ { \\mathrm { r e f } } \\left( a _ { l } \\mid x \\right) } \\Bigg ) \\Bigg ] } \\end{array}\n$$\n\n# WEPO Implementation\n\nWe start by introducing the sampling mechanism of WEPO, as illustrated by the partial DOM tree shown in Figure 1. Since every webpage can be parsed into a corresponding DOM tree with each web element represented by a unique node, web elements corresponding to nodes that are closer in proximity under the same ancestor within the DOM typically exhibit greater functional and semantic similarity. Building on this characteristic, we have developed a distance-based sampling method specifically tailored to the\n\nDOM tree structure, which begins by selecting a substantial number (top $k$ ) of element anchors on the page, including one correct element, and then calculates and sorts the sum of the distances between negative and positive samples to their lowest common ancestor (LCA) to quantify their relative distances. Subsequently, the method proportionally samples the top $n$ closest elements from the sorted results, which are then combined with the correct element to form comparisons. We aim to enable the model to effectively learn to distinguish between web elements with similar functions based on the given operational intent.\n\nDuring training, we implement Direct Preference Optimization in WEPO, since DPO is free of reward modeling and training stable (Rafailov et al. 2024). By optimizing the target loss function, WEPO aims to increase the likelihood of operations on preferred elements and decrease the likelihood of operations on dis-preferred elements. As shown in Equation 1, we introduce the maximum likelihood objective proposed in DPO and adaptively modify the preferred completion $y _ { w }$ and dis-preferred completions $y _ { l }$ into preference action pairs $\\mathbf { \\Delta } a _ { w }$ and $a _ { l }$ . Given the pretrained model $\\pi _ { \\boldsymbol { \\theta } }$ and the reference model $\\pi _ { \\mathrm { r e f } }$ initialized from $\\pi _ { \\boldsymbol { \\theta } }$ , we fine-tune $\\pi _ { \\boldsymbol { \\theta } }$ according to Equation 1, where $\\beta$ is a hyperparameter that controls the penalty for deviations from $\\pi _ { \\mathrm { r e f } }$ .\n\nWe demonstrate the pseudo-code for WEPO implementation in Algorithm 1, which illustrates how ${ a } _ { w } , { a } _ { l }$ , and $x$ used for optimizing are obtained at each training step. First, we clean and prune the HTML code. Consistent with previous work (Gu¨r et al. 2023; Deng et al. 2024), we adapt an element-centric approach to isolate HTML snippets. By focusing on a key element, we navigate its ancestors within the HTML tree, guided by a simple constraint that monitors the tree’s width and depth. We stop this traversal once the number of descendants exceeds predefined thresholds, thus defining the snippet using the sub-tree. We introduce a pruning ratio $k$ that represents the number of target elements remaining after pruning. We ensure that the pruned HTML snippet contains the ground truth element during training; In inference time, we use a small ranking LM derived from the candidate generation stage of Deng et al. (2024) to implement priority-based HTML pruning, which scores all elements first and then selects the top- $k$ elements with the largest logits. Subsequently, we concatenate the preprocessed HTML state $s ^ { \\prime }$ , historical trajectory $\\tau$ , initial intent $i$ and a sophisticated prompt template $P$ to generate the input $x$ , where $\\oplus$ denotes string concatenation.\n\nSubsequently, we apply the tailored distance-based sampling method to yield candidate set $E$ . We set the number of negative samples as $n$ , corresponding to a positive-tonegative sample ratio of $1 : n$ . After obtaining the negative elements $\\{ e _ { l _ { 1 } } , e _ { l _ { 2 } } , . . . , e _ { l _ { n } } \\}$ , we employ a designed heuristic rule $f _ { o p }$ to randomly sample the corresponding negative operations $o _ { l _ { i } }$ , which involves selectively replacing TYPE and SELECT to ensure balanced and diverse sample types. This replacement occurs only when $\\hat { o } \\neq { \\mathrm { C L I C K } }$ , as the negative samples obtained through sampling have a very low probability of being TYPE or SELECT, adding data balance without confusing the LLM about the functionality of webpage elements. The rule empirically sets the replacement proba\n\n# Algorithm 1: WEPO Algorithm\n\nRequire: (for each step) Intent $i$ , current HTML $s$ , action trajectory $\\tau$ ; prompt template $P$ ; pretrained LM $\\pi _ { \\boldsymbol { \\theta } }$ , reference LM $\\pi _ { \\mathrm { r e f } }$ and deviation parameter $\\beta$ ; Pruning ratio $k$ , negative ratio $n$ ; Target element $\\hat { e }$ , corresponding operation $\\hat { o }$ and target action $\\hat { a } = a ( \\hat { e } , \\hat { o } )$ ;   \nEnsure: (for each step) 1: Clean and prune the HTML DOM tree with $k$ elements remaining $s ^ { \\prime } = f _ { p r u n e } ( s , k )$ ;   \n2: Concatenate $x = \\bar { s } ^ { \\prime } \\oplus \\tau \\oplus i \\oplus P$ 3: Get positive action $\\boldsymbol { a } _ { w } = \\boldsymbol { \\hat { a } }$ ;   \n4: Sample $n$ negative elements $\\{ e _ { l _ { 1 } } , e _ { l _ { 2 } } , . . . , e _ { l _ { n } } \\}$ from candidate set $E  s ^ { \\prime }$ based on LCA distance from $\\hat { e }$ ; 5: for $i = 1$ to $n$ do   \n6: Set ϵ random uniform $( 0 , 1 )$   \n7: $o _ { l _ { i } } \\sim f _ { o p } ( \\{ \\mathrm { C L I C K } , \\mathrm { T Y P E } , \\mathrm { S E L E C T } \\} , \\hat { o } , \\epsilon )$   \n8: Get $i$ -th negative action $a _ { l _ { i } } = a ( e _ { l _ { i } } , o _ { l _ { i } } )$   \n9: Optimize $\\nabla _ { \\theta } \\mathcal { L } _ { \\mathrm { D P O } } \\left( \\pi _ { \\theta } ; \\pi _ { \\mathrm { r e f } } \\right)$ in Equation 1 given $\\beta$ , $x$ , $a _ { w }$ and $a _ { l } = a _ { l _ { i } }$ ;\n\nbility threshold at 0.33, and several verification confirmed that values around this threshold have no significant impact on WEPO. Therefore, when the positive sample is a CLICK operation, the negative samples are also CLICK; however, when the positive sample involves the other two actions, the negative samples might be changed to CLICK. Finally, we obtained $a _ { w } , a _ { l }$ , and $x$ , and used Equation 1 to perform gradient back-propagation on the pretrained LM $\\pi _ { \\boldsymbol { \\theta } }$ , with $\\beta$ controlling the deviation from the reference model $\\pi _ { \\mathrm { r e f } }$ . We opt for straightforward random sampling as an alternative, where the randomly distributed negative samples also maintain a low correlation with $\\hat { e }$ on the webpage.\n\n# Experiments Experimental Setup\n\nWe employ three mainstream pretrained LLMs of progressively increasing model sizes to validate the scaling effects. These models include Llama- $3 { - } 8 \\mathbf { B } ^ { 1 }$ , Mistral-7B-Instruct$\\mathrm { v 0 . 1 }$ (Jiang et al. 2023), and Gemma-2B (Team et al. 2024). For the hyperparameters of WEPO, we set the deviation parameter $\\beta$ to 0.95 and the negative sample ratio to 1:3. In Mind2Web (Deng et al. 2024), a DeBERTa (He et al. 2020) model trained within the candidate generation module uses recall $@ 5 0$ for ranking elements and constructing a candidate pool for subsequent experiments. We similarly select a pruning ratio of $k = 5 0$ to maintain consistency for comparison, preserving 50 central elements and their neighboring elements tagged with element ID. For the selection of $k$ and $n$ values, we provide detailed explanations in the forthcoming ablation studies. All models were configured with a maximum context length of 8192 tokens. We employ the\n\nTable 1: Overall performance of various models on different test sets, with evaluation metrics corresponding to SSR / Operation F1 $( \\% )$ . The results were obtained under a negative sample ratio of $1 : 3 .$ . Notably, our Llama3-8B-WEPO model achieved the highest scores in both overall SSR and Operation F1. Our top scores exceeded those of the much larger CogAgent (17B) model by $5 . 3 \\%$ and WebAgent $( 3 \\mathbf { B } + 5 4 0 \\mathbf { B } )$ by $1 3 . 8 \\%$ . Additionally, our smaller Gemma-2B-WEPO model managed to closely match and even slightly outperform the approximately 3B Flan-T5 based models (Chung et al. 2024) like MindAct.   \n\n<html><body><table><tr><td>Model SSR /Op.F1 (%)</td><td>overall</td><td>cross_domain</td><td>cross_task</td><td>cross_website</td></tr><tr><td>Flan-T5-XL MindAct</td><td>43.5 / 69.1</td><td>39.6 / 66.5</td><td>52.0 / 75.7</td><td>38.9 / 65.2</td></tr><tr><td>Llama3-8B MindAct</td><td>55.1/ 65.8</td><td>57.6 /68.7</td><td>56.1/63.2</td><td>51.7 / 65.6</td></tr><tr><td>CogVLM-17B CogAgent</td><td>58.2/ -</td><td>59.4/ -</td><td>62.3 / -</td><td>54.0 / -</td></tr><tr><td>HTML-T5-XL +Flan-U-PaLMWebAgent</td><td>49.7 / -</td><td>48.3 / -</td><td>57.8/ -</td><td>42.9 / -</td></tr><tr><td>Llama3-8B WEPO random</td><td>61.1 / 73.9</td><td>62.5 / 77.5</td><td>62.5 / 67.2</td><td>58.4 / 77.1</td></tr><tr><td>Mistral-7B WEPO random</td><td>57.2 / 73.8</td><td>58.0 / 75.9</td><td>59.0 / 72.1</td><td>54.8 / 73.3</td></tr><tr><td>Gemma-2B WEPO random</td><td>45.4 / 49.5</td><td>49.1 / 55.7</td><td>45.2 / 42.9</td><td>41.9 / 50.0</td></tr><tr><td>Llama3-8BWEPO distance-based</td><td>63.5 / 76.1</td><td>64.4 / 81.6</td><td>66.1 / 74.9</td><td>60.0 / 71.9</td></tr><tr><td>Mistral-7BWEPOdistance-based</td><td>59.5 / 76.8</td><td>59.8 / 80.9</td><td>62.1 / 76.2</td><td>56.7 /73.2</td></tr><tr><td>Gemma-2B WEPO distance-based</td><td>48.4 / 53.3</td><td>53.5 / 60.3</td><td>47.9 / 48.8</td><td>43.7 / 50.7</td></tr></table></body></html>\n\nLow Rank Adaptation (LoRA) technique (Hu et al. 2021) for parameter-efficient fine-tuning, which helps reduce memory usage and conserve budget. The learning rate is set at 0.0001, and we use a combination of learning rate warmup and a cosine decay strategy for training.\n\nEvaluation Metrics. In this paper, we adopt the step success rate (SSR) and Operation F1 score from Deng et al. (2024). We no longer use element accuracy and success rate because it is evident that both metrics are linearly associated with SSR, and successful interaction for the web navigation agent is only considered when both the element positioning and the corresponding operation are correct, which is precisely what SSR measures. The Operation F1 score is equally indispensable as it considers the accuracy of the input value for Type and SELECT commands. Furthermore, both baseline studies by Gur et al. (2023) and Hong et al. (2023) exclusively utilized SSR as the sole metric.\n\nAdditionally, we designed the element distance metric to measure the positional deviation between the elements selected by the WEPO model and the labeled elements. Consistent with the previous sampling strategy, we calculate the sum of the distances (in terms of steps) between the nodes corresponding to two different web elements and their lowest common ancestor (LCA) in the DOM tree to represent their relative positions.\n\n# Results\n\nWe thoroughly evaluate WEPO on the partitioned three-tier held-out test sets in Mind2Web (Deng et al. 2024), including cross-domain, cross-website and cross-task datasets. This allows us to understand how well our method can generalize across different domains, websites, and tasks.\n\nThe experimental results are detailed in Table 1. Compared to previous works (Deng et al. 2024; Hong et al. 2023; Gur et al. 2023), without utilizing any multimodal information or customized model architecture, our best results obtained by fine-tuning the 8-billion parameter Llama3 pretrained model with web element preference learning exceeded the three baselines by $2 0 . 0 \\%$ , $5 . 3 \\%$ , and $1 3 . 8 \\%$ respectively. Even though there is still a significant gap between the model performance of gemma-2B and the larger model $( > 1 0 \\%$ ), these results demonstrate the effectiveness of the WEPO method, proving its efficiency and generalizability in enabling LLM-based agents for web navigation tasks. In assessing generalization capabilities at different levels, although Deng et al. (2024) found that their model performed best in the Cross-Task setting, WEPO has narrowed the gap between different test sets to less than $6 . 1 \\%$ . Additionally, observing the performance of WEPO models of different sizes, we found that the step success rate increases with model size, which verifies the presence of the scaling effect in the Mind2Web real-world benchmark.\n\nTo eliminate the influence of model base choice on the results, we thoroughly applied supervised fine-tuning using the action prediction prompts from Mind2Web on Llama3- 8B. The results indicate that compared to this upgraded MindAct baseline, WEPO also outperforms by $8 . 4 \\%$ on the SSR metric and by $1 1 . 0 \\%$ on the Operation F1 score. This again demonstrates that fine-tuning with positive human annotation only, which is a simple maximum likelihood approach, is less effective than our WEPO method. We then conducted a comparison of its performance with random sampling in Table 1 to validate the effectiveness of the distancebased sampling method. Compared to random sampling, the distance-based approach achieved a $2 . 3 \\%$ to $3 \\%$ higher SSR, demonstrating its efficiency. In the subsequent analysis of the element distance evaluation metric, it can also be seen that this method successfully enhances the accuracy of the model’s selections, indirectly suggesting better alignment with user high-level intents.\n\nWhat exactly is the role of preference learning in enhancing performance? For the Operation F1 score, we observed a significant improvement from Gemma-2B-WEPO to Mistral-7B-WEPO, which indicates that an increase in the number of LLM training parameters not only makes the web element localization more accurate but primarily enhances the accuracy of input or select text values. For MindAct, however, our best performance was not significantly ahead in F1 scores, suggesting that the WEPO method, compared to traditional supervised fine-tuning, enhances the accuracy of element selection in pretrained LMs. We infer that WEPO leverages this enhancement through a contrastive training scheme, wherein the model learns to distinguish between elements that are critical for decision-making and those that are not. Particularly when intentions are abstract, a web page at any given moment may contain multiple elements that are easily confused, and WEPO reduces the likelihood of incorrect element selection by the agent. By incorporating a contrastive mechanism, WEPO not only improves the accuracy of navigation tasks but also enhances the model’s generalizability across different web layouts and designs.\n\nTable 2: SSR performance $( \\% )$ from ablations on the cleaned HTML pruning ratio $k$ value. For WEPO training, we forcibly included the ground truth element, akin to a teacher-forcing mechanism. The results shown in the table, except for the last row, represent the SSR after reassembling HTML from elements filtered through the first-stage ranking. Llama3-8B-WEPO maintained a $1 0 . 1 \\%$ improvement to Flan-T5 based MindAct even after reducing $k$ .   \n\n<html><body><table><tr><td>Model SSR (%)</td><td>cross_domain</td><td>cross_task</td><td>cross_website</td></tr><tr><td>Flan-T5-XLMindActk = 50</td><td>39.6</td><td>52.0</td><td>38.9</td></tr><tr><td>Llama3-8B MindActk = 50</td><td>57.6</td><td>56.1</td><td>51.7</td></tr><tr><td>Llama3-8B WEPO k = 50</td><td>64.4</td><td>66.1</td><td>60.0</td></tr><tr><td>Llama3-8B WEPOk= 10</td><td>49.7</td><td>55.0</td><td>46.5</td></tr><tr><td>Llama3-8B WEPO k = 10 w. ground truth é</td><td>87.2</td><td>88.7</td><td>85.4</td></tr></table></body></html>\n\n![](images/cf8b80ab50b99a5441edddb3a86e72bf1a2e6ac07159d168b3c5f98ae5138a8c.jpg)  \nFigure 2: Statistical distribution of Element Distance for different models (Llama3-8B, Mistral-7B and Gemma-2B) on the test dataset. As the model size increases, the relative deviation in element distances decreases.\n\nThe results in Figure 2 provide great support for this inference. As the model size increases, the element distance consistently decreases. Element distance, which represents the relative position of elements in the DOM, is closely correlated with the elements’ function and design purpose. Coupled with the score analysis in Table 1, this decreasing deviation suggests that the Llama3-8B-WEPO model becomes increasingly accurate in aligning with the intended functions and design of web elements compared to the smaller models. This result clearly indicates that the model has successfully learned to recognize these web design differences, proving that WEPO is an effective method for adapting to the HTML structure, or more broadly for aligning with web design principles. Furthermore, we have demonstrated the effectiveness of our newly proposed element distance evaluation metric.\n\n![](images/83868de1c0c4141ccc265ca7d563b7d52096fbde64355ef841fac9e71ada170f.jpg)  \nFigure 3: Ablation studies on the negative ratio. We experimented with the Llama3-8B-WEPO model at $n = 1 , 3 , 5$ and calculated the average SSR $( \\% )$ on three cross-test sets, which were $5 7 . 7 \\%$ , $6 3 . 5 \\%$ for distance-based sampling and $5 6 . 6 \\%$ , $6 1 . 1 \\%$ , and $6 2 . 4 \\%$ for random sampling, respectively. An elbow point was observed at $n = 3$ for random sampling, where the increase in SSR sharply levels off. Furthermore, the performance of distance-based sampling at a 1:3 ratio has already surpassed that of random sampling at a 1:5 ratio by $1 . 1 \\%$ .\n\nWhy choose a 1:3 ratio for negative samples? We also conducted ablation experiments on the negative sample ratio in the WEPO Algorithm 1. Empirically, we aimed to sample as many negative samples as possible to enhance performance through WEPO without over-sampling and excessively increasing the training overhead. We uniformly sampled $n$ values of 1, 3, and 5, selected the best-performing Llama3-8B-WEPO for experimentation and averaged the results from two rounds. As shown in Figure 3, the overall average scores on the test set increase with larger $n$ values, with a noticeable elbow point at $n = 3$ for random sampling. We ultimately selected a 1:3 ratio as a fixed hyperparameter to avoid linearly increasing training costs. Additionally, we observed that the distance-based sampling method at $n = 3$ outperformed random sampling at $n = 5$ , significantly improving overall sample learning efficiency during training. Furthermore, too many negative samples could potentially create an imbalance between positive and negative sample quantities. However, we did not experiment with larger $n$ values to verify potential performance degradation, as excessively large $n$ values could hinder reproducibility.\n\nWhat is the impact of HTML $k$ -pruning on performance? To address this question, we conducted ablation studies on the selection of $k$ , with results shown in Table 2. As our pruning is based on the first stage ranking LM of MindAct, understanding the impact of this preprocessing module is crucial. The authors of Deng et al. (2024) disclosed that when $k = 5 0$ , the fine-tuned DeBERTa (He et al. 2020) model achieved recall accuracies of $8 8 . 9 \\%$ , $8 5 . 3 \\%$ , and $8 5 . 7 \\%$ on three held-out test datasets, and smaller values of $k$ were not adopted due to lower recall rates.\n\nHowever, in our results, when we set $k$ to a smaller value of 10, Llama3-8B-WEPO still performed above the baseline. We deduce that although reducing $k$ significantly decreases the recall rate of the ranking LM, it provides the WEPO-trained model with a shorter HTML snippet and fewer ID options, enhancing discrimination accuracy. Moreover, when we forcibly added the ground truth web element $\\hat { e }$ directly into the candidate pool, the ablation model’s SSR surged to over $80 \\%$ . This conclusively demonstrates that the WEPO method has significantly improved the model’s capability in action prediction and has shifted the original candidate retrieval module from a secondary issue to a major bottleneck limiting the web agent’s progress on complex longcontext web pages.\n\n# Future Work\n\nThere remain several avenues for further development of this approach. While WEPO has shown empirical success, a deeper theoretical analysis could provide more foundational insights into why and how preference optimization effectively enhances web navigation tasks. Inspired by the advancements of models like WebFormer (Wang et al. 2022) and HTML-T5 (Gu¨r et al. 2023), future iterations of WEPO could benefit from a dedicated HTML encoder that is specifically tailored to understand the hierarchical and nested structures of web documents.\n\nThe current implementation of WEPO has not explored its potential over extremely large context lengths such as $\\mathrm { \\ p h i } - 3 - 1 2 8 \\mathrm { k \\Omega }$ (Abdin et al. 2024). Future research could look into the scaling abilities of WEPO when applied to such models, which might be crucial for handling complex web navigation tasks that involve detailed web pages.\n\nWhile WEPO shows promising results in a controlled benchmark environment, its ability to generalize across highly diverse, real-world web interfaces remains an area for further investigation. The variability in web design, interactive elements, and underlying technologies across different websites may affect the consistency of WEPO’s performance. We plan to test the performance of the WEPO method on additional mainstream benchmarks such as WebLINX and WebVoyager (Lu\\`, Kasner, and Reddy 2024; He et al. 2024) in future work.\n\n# Conclusion\n\nThis paper introduced the Web Element Preference Optimization (WEPO), a simple yet novel framework that integrates Direct Preference Optimization (DPO) into LLMbased web navigation tasks. WEPO enhances LLM performance by leveraging distance-based non-salient HTML elements for contrastive learning, effectively aligning model operations with user intent. Our empirical evaluations on the Mind2Web benchmark demonstrate that WEPO surpasses traditional models, achieving an $1 3 . 8 \\%$ improvement over the WebAgent baseline and a $5 . 3 \\%$ enhancement beyond the visual language model CogAgent, while also exhibiting strong generalization across diverse web environments. This research significantly enhances the capabilities of LLMs in web navigation task, contributing to more efficient and intuitive web interactions.\n\n# Acknowledgments\n\nThis work was jointly supported by Beijing University of Posts and Telecommunications (BUPT) - China Mobile Research Institute Joint Innovation Center, and the National Key Laboratory of Networking and Switching Technology under Project of Embodied Intelligent Agent for Intellicise Networks. We extend our heartfelt gratitude to the faculty and staff at BUPT for their invaluable guidance and assistance throughout the research process. We also thank our collaborators and peers who provided critical feedback and constructive discussions that enriched the development of this work. Lastly, we acknowledge the broader academic community for inspiring this study through their pioneering efforts in autonomous web navigation and large language model applications.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   当前基于大型语言模型（LLMs）的自主网页导航研究未能充分利用HTML元素的冗余性进行对比训练，限制了模型在复杂网页环境中准确理解和执行用户指令的能力。\\n> *   该问题的重要性在于，高效的网页导航可以解放人类用户与计算机界面的重复交互，提升自动化任务的效率和准确性，适用于电子商务、信息检索等多种应用场景。\\n\\n> **方法概述 (Method Overview)**\\n> *   本文提出了一种名为“网页元素偏好优化（WEPO）”的新方法，通过无监督偏好学习，利用基于距离的非显著网页元素作为负样本，优化直接偏好优化（DPO）中的最大似然目标。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 提出了一种基于DOM树结构的启发式距离采样方法，显著提升了对比学习的效率。\\n> *   **创新贡献点2：** 将偏好优化算法（如DPO）首次应用于网页任务自动化，实现了用户高级意图与输出动作的有效对齐。\\n> *   **关键数据：** 在Mind2Web基准测试中，WEPO方法比WebAgent基线提升了13.8%，比视觉语言模型CogAgent基线提升了5.3%，同时比MindAct基线提升了20.0%。\\n> *   **关键数据补充：** WEPO在跨域测试集上达到了64.4%的步骤成功率（SSR），显著优于MindAct（57.6%）和WebAgent（48.3%）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   WEPO的核心思想是通过对比学习，利用网页中非显著元素作为负样本，优化模型对显著元素的识别能力。其设计哲学是基于DOM树结构的相似性假设，即功能相似的网页元素在DOM树中位置相近。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前工作主要依赖监督学习或强化学习，未能充分利用网页元素的冗余性和对比学习潜力。\\n> *   **本文的改进：** WEPO通过无监督采样负样本，避免了人工标注成本，同时提升了样本效率。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  清理和修剪HTML代码，保留关键元素。\\n> 2.  基于DOM树结构，计算负样本与正样本到最低共同祖先（LCA）的距离，并按距离排序。\\n> 3.  从排序结果中按比例采样最近的n个元素，与正样本形成对比对。\\n> 4.  使用DPO算法优化模型，最大化对偏好元素的操作似然，最小化对非偏好元素的操作似然。\\n\\n> **案例解析 (Case Study)**\\n> *   论文中提供了一个示例：用户意图为“Find me a M2 Mac Air Laptop with 15'' screen”，WEPO将正确元素（绿色标记）与基于启发式规则采样的负元素（红色标记）结合，构建偏好对。这一过程利用DPO提出的最大似然目标函数来微调语言模型，从而提升其在元素判别和选择上的准确性。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   WebAgent\\n> *   CogAgent\\n> *   MindAct\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在步骤成功率（SSR）上：** 本文方法在Mind2Web基准测试中达到了63.5%，显著优于基线模型WebAgent（49.7%）和CogAgent（58.2%）。与表现最佳的基线相比，提升了5.3个百分点。\\n> *   **在操作F1分数上：** 本文方法的F1分数为76.1%，远高于MindAct基线（69.1%），同时与视觉语言模型CogAgent的性能相当，但在模型参数和推理速度上更具优势。\\n> *   **在跨域测试集上：** 本文方法在cross_domain测试集上达到了64.4%的SSR，显著优于MindAct（57.6%）和WebAgent（48.3%）。\\n> *   **在跨任务测试集上：** 本文方法在cross_task测试集上达到了66.1%的SSR，显著优于MindAct（56.1%）和WebAgent（57.8%）。\\n> *   **在跨网站测试集上：** 本文方法在cross_website测试集上达到了60.0%的SSR，显著优于MindAct（51.7%）和WebAgent（42.9%）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   网页导航 (Web Navigation, N/A)\\n*   大型语言模型 (Large Language Model, LLM)\\n*   直接偏好优化 (Direct Preference Optimization, DPO)\\n*   对比学习 (Contrastive Learning, N/A)\\n*   DOM树 (Document Object Model, DOM)\\n*   无监督学习 (Unsupervised Learning, N/A)\\n*   网页元素偏好优化 (Web Element Preference Optimization, WEPO)\\n*   Mind2Web基准 (Mind2Web Benchmark, N/A)\"\n}\n```"
}