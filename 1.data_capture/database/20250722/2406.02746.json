{
    "source": "Semantic Scholar",
    "arxiv_id": "2406.02746",
    "link": "https://arxiv.org/abs/2406.02746",
    "pdf_link": "https://arxiv.org/pdf/2406.02746.pdf",
    "title": "RATT: A Thought Structure for Coherent and Correct LLM Reasoning",
    "authors": [
        "Jinghan Zhang",
        "Xiting Wang",
        "Weijieying Ren",
        "Lu Jiang",
        "Dongjie Wang",
        "Kunpeng Liu"
    ],
    "publication_date": "2024-06-04",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 18,
    "influential_citation_count": 0,
    "paper_content": "# RATT: A Thought Structure for Coherent and Correct LLM Reasoning\n\nJinghan Zhang1, Xiting Wang2, Weijieying $\\mathbf { R e n } ^ { 3 }$ , Lu Jiang4, Dongjie Wang5, Kunpeng Liu\n\n1Portland State University 2Renmin University of China 3Pennsylvania State University 4Dalian Maritime University 5University of Kansas jinghanz,kunpeng @pdx.edu, xitingwang@ruc.edu.cn, wjr5337 $@$ psu.edu, jiangl761@dlmu.edu.cn, wangdongjie@ku.edu\n\n# Abstract\n\nLarge Language Models (LLMs) gain substantial reasoning and decision-making capabilities from thought structures. However, existing methods such as Tree of Thought and Retrieval Augmented Thoughts often fall short in complex tasks due to the limitations of insufficient local retrieval of factual knowledge and inadequate global selection of strategies. These limitations make it challenging for these methods to balance factual accuracy and comprehensive logical optimization effectively. To address these limitations, we introduce the Retrieval Augmented Thought Tree (RATT), a novel thought structure that considers both overall logical soundness and factual correctness at each step of the thinking process. Specifically, at every point of a thought branch, RATT performs planning and lookahead to explore and evaluate multiple potential reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented Generation (RAG) with LLMs’ ability to assess overall strategy. Through this combination of factual knowledge and strategic feasibility, the RATT adjusts and integrates the thought tree structure to search for the most promising branches within the search space. This thought structure significantly enhances the model’s coherence in logical inference and efficiency in decision-making, and thus increases the limit of the capacity of LLMs to generate reliable inferences and decisions based on thought structures. A broad range of experiments on different types of tasks showcases that the RATT structure significantly outperforms existing methods in factual correctness and logical coherence.\n\n# Introduction\n\nLarge Language Models (LLMs) have shown impressive reasoning and decision-making capabilities in complex tasks like mathematical reasoning and creative writing by processing and generating thoughts based on token-level predictions (Huang et al. $2 0 2 2 \\mathrm { a }$ ; Zhang et al. 2023). However, this approach limits their ability to extend higher-level or multi-perspective reasoning (Huang et al. 2022b). Research on LLM reasoning indicates that implementing structured thought processes significantly enhances their performance in inference and decision-making (Yu et al. 2023).\n\nThese thought structures help LLMs to organize and generate their responses both contextually and hierarchically, as well as logically coherent reason across extended narratives and complex problem spaces.\n\nResearchers have developed various thought structures for LLMs. Compared to the vanilla input-output (IO) (Zhang et al. 2024) prompting, where the model directly responds to the prompt, the basic idea of thought structures like Chainof-Thought (CoT) (Wei et al. 2022) and Self-consistency with CoT (CoT-SC) (Wang et al. 2022) is to guide the reasoning process by generating a sequential and coherent reasoning framework according to the given task. These methods usually involve generating a series of intermediate thought steps for analyzing and solving the initial question. They significantly improve LLM reasoning performance and the reasoning process transparency.\n\nDespite the advancements in improving LLMs’ reasoning capability, current thought structures still encounter a major challenge in balancing local factual accuracy and global strategy planning effectiveness. These limitations could restrict their applicability in complex and dynamic scenarios. For example, in Figure 1, CoT and Tree of Thoughts (ToT) (Yao et al. 2024), which simulate coherent reasoning processes, lack an effective fact-checking mechanism. As a result, errors that occur in the early stage of reasoning can propagate through the entire thought chain and lead the conclusions to deviate far away from reality. Moreover, the global selection mechanism of the optimal branch of ToT is not effective and efficient enough, which could result in generating excessive texts and branches (Ding et al. 2023). While Retrieval Augmented Thoughts (RAT) (Wang et al. 2024) combine CoT and Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) to perform fact checking, this method faces challenges in global strategy optimization, as it lacks systematic planning and lookahead to search for the optimal reasoning path in the search space and hence is limited in complex tasks where a broader perspective is necessary (Zhao et al. 2024; Renze and Guven 2024).\n\nOur Targets. To solve the aforementioned issues and enhance the reliability and accuracy of LLM reasoning, we aim to develop a method that seamlessly unifies local and global optimization. At the local level, the method should utilize external knowledge early and continuously to prevent and correct factual errors and thus avoid leading the\n\nTree  o(fTToTh)ought Round 1 The answers could be quite similar PrToasmkpt RetrTiehvoaulgAhut gTmreented   \nPrompt P The cat is a land animal, fish is aquatic. LLM Q: How does a cat catch and eat fish? The cat is an aquatic animal, just like fish. Round 1 Adopting RAG on Thought Tree   \nLLM Round 2 And somtimes incorrect. The cat is a land animal, fish is aquatic. The cat can catch fish by the water. LLM + RAG The cat is anlanqduatniicmanl,imfiaslhl ikseafqiusaht.ic. P The cat can hardly catch fish by the river. The cat swims in the water to chase fish. The cat is a land animal, fish is aquatic. LLM The cat is a land animal, fish is aquatic.   \nRetrieTvhaloAuguhgtmsented Q: How does a cat catch and eat fish? Cats have special skills of catching fish. (RAT) Round 1 RAT can correct the errors in a CoT. Layer 1 Task The cat is anlanqduatniicmanl,imfiasl.h  i s not. Layer 2 电 sAksi las aonfdcatnicmhianl,gtfhisehc..a.t has special   \nPrompt Round 2 But if the CoT gets off-track early,   \nRAG LLM Land animals can't swim, so they cannot Round 2 catch fish. To catch a fish, the cat first... Round 3 We still get wrong answers. Layer T Round ... We can get a output with less The cat can't eat fish. Round T errors and more soundness.\n\nmodel to incorrect search areas. If the factual errors are not identified early, they accumulate during subsequent generations and eventually lead to unreliable conclusions. At the global level, the method should structure the reasoning process with planning and lookahead abilities. This allows for the identification of logically coherent and globally optimal reasoning paths even when there are frequent corrections of factual errors. Solely relying on comprehensive correction after the initial reasoning often leads to suboptimal solutions, as post-processing may not correct all factual errors or even introduce new errors due to delayed fact-checking.\n\nOur Approach. To achieve these goals, we develop the Retrieval Augmented Thought Tree (RATT), a novel thought structure that simultaneously ensures both local factual correctness and global comprehensive logical soundness at each step of the reasoning process in one unified framework. In RATT, we perform planning and lookahead to multiple potential reasoning steps at each step of reasoning and integrate the fact-correction capability of RAG along with LLM’s assessment of its overall strategy. Then, RATT integrates factual correctness and logical strategy feasibility to optimize the reasoning process and guide us toward the most promising branches within the search space. This approach greatly improves the logical coherence and inference reliability of LLM’s decision-making and precise reasoning.\n\nIn summary, our contribution includes:\n\n1. We introduce a new thought structure called Retrieval Augmented Thought Tree (RATT), which considers and improves comprehensive logical soundness and factual correctness at each step of the reasoning process. This structure significantly enhances the logical coherence and decision-making efficiency of LLMs in complex reasoning tasks.\n\n2. We develop a novel paradigm incorporating RAG into a tree-structured thought process. This paradigm conducts lookahead and general-to-detail fact-checking analyses at each node of the thought tree.\n\n3. We conduct a series of experiments to validate the effectiveness and robustness of our method (RATT) across different tasks. Our experiments show that our method has clear advantages over existing methods.\n\n# Related Works\n\n# Thought Structures for LLMs\n\nThought structures are a series of prompt engineering methods that guide models to generate more specific, accurate, and high-quality content (Zhang et al. 2022; Minaee et al. 2024). Among these methods, Chain of Thought (CoT) (Wei et al. 2022) is a milestone development that guides a reasoning process by generating a series of logically coherent intermediate steps of inference. Based on CoT, Wang et al. (2022) (Wang et al. 2022) introduce the Self-consistency with CoT (CoT-SC) method, which enhances reasoning accuracy and stability by independently generating multiple thought chains and selecting the most reliable answer. Furthermore, the ToT extends CoT by constructing a thought tree with multiple reasoning branches. The thought tree structure is capable of planning, looking ahead, and backtracking, thus providing a broad and global view of the solution space. These developments make multi-step thinking and reasoning similar to human cognitive processes.\n\n# Retrieval-Augmented Generation for LLM Reasoning\n\nRetrieval-Augmented Generation (RAG) (Lewis et al. 2020) for LLM reasoning has become a vital approach to enhance the quality of output. The LLM generates a response after retrieving information in an external library, which is a set of documents or knowledge related to the task, with a query relevant to the task (Lewis et al. 2020; Shuster et al. 2021). This approach helps the LLM to produce responses that are more accurate, contextually relevant, and with fewer hallucinations (Yang et al. 2024; Wu et al. 2024). Following RAG, (Wang et al. 2024) developed the RetrievalAugmented Thoughts (RAT) approach, which incorporates the RAG retrieval process into a reasoning chain-of-thought. The RAT approach first generates a reasoning chain and retrieves relevant information using LLM-generated queries based on the prompt and each reasoning step. Then RAT corrects and refines the reasoning chain step by step. The primary advantage of RAT is its ability to correct errors in the reasoning process, which is one notable step for enhancing the performance of LLM reasoning. However, as RAT only thinks and refines following one certain and complete path of thought, one potential problem of RAT is to fall into a local suboptimal solution in the search space.\n\n# Methodology\n\nIn this section, we introduce Retrieval Augmented Thought Tree (RATT), an automated novel thought structure for language models that prioritizes both logical coherence and factual correctness. We aim to enhance the reliability and accuracy of LLM reasoning in two aspects: robust local factchecking to prevent the accumulation of errors and global planning and lookahead to improve the logical coherence of reasoning paths. As shown in Figure 2, we implement this RATT through several steps, including (1) Thought Node Generation, (2) Retrieval and Selection, and (3) RAG Correction and Integration.\n\n# Problem Formulation\n\nTo formalize our target and solution, first let us define the elements and functions. Given a task $\\mathcal { A }$ and a pre-trained LLM $p _ { \\theta }$ with parameters $\\theta$ , we aim to enhance the performance of LLM on task $\\mathcal { A }$ with performance metric $\\boldsymbol { \\epsilon } = \\{ \\epsilon _ { 1 } , \\epsilon _ { 2 } , . . . , \\epsilon _ { j } \\}$ . The task $\\mathcal { A }$ can be any problem that can be solved with a logical sequence of thoughts or decisions. We have an input $x$ which is a language sequence and an output $y$ . Our target is to construct the best thought tree structure $\\mathcal { T } = \\mathcal { T } ^ { * }$ capable of navigating the search space and optimizing the reasoning process with task $\\mathcal { A }$ and prompt $x$ :\n\n$$\n\\mathcal { T } ^ { * } = \\arg \\operatorname* { m a x } _ { \\mathcal { T } } \\epsilon ( y ) , \\quad \\mathrm { w h e r e } \\quad y = p _ { \\theta } ( x | \\mathcal { T } ) .\n$$\n\nHere we denote a thought tree $\\boldsymbol { \\mathcal { T } } = ( \\boldsymbol { \\mathcal { N } } , \\boldsymbol { \\mathcal { E } } )$ to be a hierarchical tree structure consists of nodes and edges where each nodes represents a thought or decision, and an edge represents a logical flow between two nodes. Here $\\mathcal { N }$ is the set of nodes in the tree and $\\mathcal { E } \\subseteq \\mathcal { N } \\times \\mathcal { N }$ is the set of directed edges connecting the nodes.\n\n# Thought Node Generation\n\nOur first step is to generate a thought tree that balances both exploration and exploitation adeptly and effectively. Given an prompt $x$ with task information, the LLM $p _ { \\theta }$ first embeds\n\nthe prompt:\n\n$$\nq _ { x } = p _ { \\theta } ( { \\mathrm { e m b e d } } ( x ) ) .\n$$\n\nThen the LLM generates initial thoughts utilizing multiple logical strategies $s ^ { ( m ) }$ to explore possible solutions in the search space of task $\\mathcal { A }$ from a broad range of perspectives and logical approaches. Each initial thought represents a distinct logical viewpoint, and these thoughts collectively cover various possible or potential directions and solutions to tasks $\\mathcal { A }$ . Specifically, in the $t$ -th iteration, we define the initial thoughts to be the initial nodes of the ${ \\bf n } _ { t } = \\{ n _ { t } ^ { ( 1 ) } , n _ { t } ^ { ( 2 ) } , \\dots , n _ { t } ^ { ( m ) } \\}$ nt(m)}. In this way, the thought tree takes every step with a broad and diverse range of logical possibilities. We denote a reasoning branch $\\boldsymbol { B }$ of the tree as a sequence of connected nodes starting from the root node and extending to any leaf node. Each branch $\\boldsymbol { B }$ represents a feasible solution for task $\\mathcal { A }$ , and each node in the branch sequence is connected by an edge $\\mathcal { E }$ from its parent node, formulated as:\n\n$$\n\\begin{array} { r } { \\mathcal { B } = \\{ n _ { r o o t } , \\dots , n _ { l e a f } \\} , ( n _ { i } \\xrightarrow { p _ { \\theta } } n _ { i + 1 } ) \\in \\mathcal { E } . } \\end{array}\n$$\n\nAfter generating initial nodes, the thought tree employs a planning and lookahead strategy, which is essential for dynamically evaluating the potential of branches. For each node $n _ { t } ^ { m }$ , the LLM evaluates the quality of the immediate next steps as well as simulates the future states and consequences of the node’s choice. The simulation provides insights into the likely outcomes to the model. This lookahead strategy helps the model to plan several moves ahead, refine and adjust generation strategies, and keep aligning the generation direction with the overall goal of the task.\n\n# Retrieval and Selection\n\nGiven a Library with $I$ candidate documents $\\mathcal { R } : = \\{ R _ { i } \\} _ { i = 1 } ^ { I }$ , the LLM embedes each document as:\n\n$$\nr _ { i } = p _ { \\theta } ( { \\mathrm { e m b e d } } ( R _ { i } ) ) \\in \\mathbb { R } ^ { K } ,\n$$\n\nwhere $K$ is the dimension of the embedding and embed $( \\cdot )$ is the embedding process. We then form a query $Q _ { t } ^ { m }$ of each $n _ { t } ^ { m }$ and the input $x$ :\n\n$$\nq _ { t } ^ { m } = \\mathrm { e m b e d } \\{ Q _ { t } ^ { m } \\} = \\{ q _ { x } , q _ { n _ { t } ^ { m } } \\} \\in \\mathbb { R } ^ { K } , \\mathrm { w h e r e }\n$$\n\n$$\nq _ { x } = p _ { \\theta } ( { \\mathrm { e m b e d } } ( x ) ) , { \\mathrm { a n d } }\n$$\n\n$$\nq _ { n _ { t } ^ { m } } = p _ { \\theta } ( { \\mathrm { e m b e d } } ( n _ { t } ^ { m } ) ) .\n$$\n\nWe then retrieve the whole Library to find the top- $k$ most relevant documents. Here we evaluate the relevance of each document $R _ { i }$ to the query $Q _ { t } ^ { m }$ by the cosine similarity of the embeddings:\n\n$$\n\\mathrm { s i m } ( q _ { t } ^ { m } , r _ { i } ) = \\frac { q _ { t } ^ { m } \\cdot r _ { i } } { \\| q _ { t } ^ { m } \\| \\| r _ { i } \\| } .\n$$\n\nThese selected documents $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }$ are then adept to enhance the information content of the initial nodes and support the fact-checking. Analogizing human learning and reasoning processes that go top to bottom from general understanding to specific details, we adopt a similar strategy\n\nStrategies Nodes RAG Correction & Integration Input LLM (1) nt(1) n (1) Inferences and Decisions Task A Generate (2) n (2) (2) nt Fisrt Then Prompt x pθ Iteration t (m) nt(m) pθ n (m) pθ The best choice is ... t = t+1 LLM Retrieval and Selection Query of each node Retrieve by BhUAL r = {ri}, qt( 1) = {qx,qnt( 1 ) } { Rselected } LIBRARY cos (qt, ri) PrRo=m{pRt }x pθ Embed qxt( 1,)q (2) nt( m) qt( m .).=. {qx,qnt( m  )} 电 in the retrieval process. We denote all nodes in the same round of generation to be in the same layer $L$ . Specifically, at the top layers $L _ { 1 } , L _ { 2 } , . . . , L _ { l _ { 1 } } , l _ { 1 } \\ \\in \\ [ 1 , l )$ , we utilize a broad and high-level retrieval strategy to gather basic concepts and background information; At the middle layers $L _ { l _ { 1 } + 1 } , . . . , L _ { l _ { 2 } } , l _ { 2 } \\in [ l _ { 1 } + 1 , l )$ , we utilize a targeted retrieval strategy to acquire deeper information; At the leaf layers $L _ { l _ { 2 } + 1 } , . . . , L _ { l } , \\bar { l _ { 2 } } \\in [ l _ { 1 } \\bar { + } 1 , l )$ , we utilize a detailed and specific retrieval strategy to correct specific errors and enrich the details.\n\n# RAG Correction and Integration\n\nWe then correct and integrate the initial nodes with relevant documents $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }$ . In this process, the LLM $p _ { \\theta }$ analyzes the documents $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }$ and extracts the most critical and supplementary information relevant to $n _ { t } ^ { 1 } , n _ { t } ^ { 2 } , \\ldots , n _ { t } ^ { m }$ The information includes factual details, background explanations, logical support, and counter-examples. The model $p _ { \\theta }$ then integrates them into a single, optimized node:\n\n$$\nn _ { t } ^ { * } = p _ { \\theta } ( n _ { t } ^ { 1 } , n _ { t } ^ { 2 } , \\ldots , n _ { t } ^ { m } , \\{ R _ { \\mathrm { s e l e c t e d } } \\} ) .\n$$\n\nHere the role of $p _ { \\theta }$ is to ensure that information integration utilizes the content of documents correctly while keeping the backbones of the reasoning sequence. In this way, we enhance the depth and breadth of the node’s information, and the optimized node $n _ { t } ^ { * }$ becomes more reliable and comprehensive in subsequent reasoning and decision-making processes. For all nodes generated in this layer, $n _ { t } ^ { * }$ represents the most valuable solution that LLM finds in the search space.\n\nAfter generating the refined node $n _ { t } ^ { * }$ , we combine $n _ { t } ^ { * }$ with the original prompt $x$ to be the new prompt of task $\\mathcal { A }$ , and the model starts a new round of generation. This iterative process continues until the maximum number of iterations $T$ is reached. At the end of the iterations, depending on the specific requirements of task $\\mathcal { A }$ , the model output either the node $n _ { T } ^ { * }$ or the complete inference of $n _ { T } ^ { * }$ as decisions and inference.\n\n# Discussions\n\nWe further discuss why our method achieves the goals mentioned in the Introduction. First, to address the local challenge of correcting factual errors and avoiding the accumulation of errors, we need to perform fact-checking and corrections in a timely and continuous manner. Our RATT method utilizes RAG technology to help LLMs efficiently and rapidly access information from relevant documents in an external library. The LLM integrates external knowledge to dynamically correct the factual errors caused by its possible limited knowledge, outdated information, or hallucinations within the reasoning context. Second, to address the global challenge of strategy optimization and searching for the optimal solution, we need to comprehensively evaluate the overall generating and searching strategy. Our RATT method integrates factual accuracy and strategic feasibility to perform correction and optimization on each generated thought. In this way, the RATT identifies and navigates the most promising branch within the search space. Finally, to reduce the risk of hallucinations, we need to make corrections during the generation process rather than after the entire reasoning process is completed. This is because the errors that develop and spread through the structure are difficult to entirely correct afterward, as they may have already polluted and magnified the whole generation. Thus, we design an online, incremental generation and correction tree structure. The experimental details in the next section demonstrate the effectiveness of our RATT’s capability in preventing the spread of errors and reducing hallucinations.\n\n# Experiments Experimental Setup\n\nIn this section, we propose four particularly challenging or representative tasks for LLM performance evaluation, and demonstrate the effectiveness of our RATT approach. These multi-view tasks provide standard benchmarks to evaluate and compare the performance of our approach and baseline\n\n1: Input: Task prompt $x$ , Node number $m$ , Iteration time $T$ , LLM $p _ { \\theta }$   \n2: $n _ { t }  \\cdots \\triangleright$ Initialize the node as an empty string   \n3: for $t = 1$ to $T$ do   \n4: for $l = 1$ to $m$ do   \n5: $n _ { t } ^ { ( l ) } \\gets p _ { \\theta } \\big ( \\cdot \\mid x , n _ { t } \\big ) \\ \\vartriangle { \\mathbb { P } } ^ { ( \\cdot ) }$ Generate thoughts based on node $n _ { t }$ and prompt $x$   \n6: $\\{ q _ { x } , q _ { n _ { t } } ^ { ( l ) } \\}  \\mathrm { e m b e d } ( \\{ n _ { t } ^ { ( l ) } , x \\} )$ ▷ Embed the node and prompt   \n7: $q _ { t } ^ { ( \\bar { l } ) } \\gets \\mathrm { \\dot { C } O N C A T } ( q _ { x } , q _ { n _ { t } } ^ { ( l ) } ) \\qquad \\triangleright$ Concatenate the question and answer into a query vector   \n8: $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }  \\mathrm { R e t r i e v e F r o m L i b r a r y } ( q _ { t } ^ { ( l ) } )$ ▷ Retrieve documents $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }$ from Library   \n9: $n _ { t } ^ { ( l ) * }  p _ { \\theta } ( \\cdot \\mid n _ { t } ^ { ( l ) } , \\mathcal { R } _ { \\mathrm { s e l e c t e d } } )$ $D$ Generate a refined node   \n10: $n _ { t } \\gets \\mathrm { C O N C A T } ( n _ { t } , n _ { t } ^ { ( l ) * } )$ ▷ Append the next refined node   \n11: end for   \n12: $n _ { t } ^ { * } \\gets p _ { \\theta } ( \\cdot \\mid x , n _ { t } )$ $D$ Refine and enhance the node   \n13: end for   \n14: $n _ { T } ^ { * }  p _ { \\theta } ( \\cdot \\mid x , n _ { t } ^ { * } )$   \n15: return n∗T ▷ Output $n _ { T } ^ { * }$ as the final generation\n\nmethods. Then we conduct an analysis of the results and discuss the strengths of our approach shown in the performance enhancement.\n\nTask Description. We test our RATT and baseline methods across four distinct tasks, each evaluating different aspects of the quality of LLM response. The Code Generation and Creative Writing are two comprehensive tasks, which Game of 24 focus on testing the logical and numerical reasoning capabilities. We also process a standard Hallucination Detection to demonstrate the RATT’s ability to reduce hallucinations.\n\nBaselines Algorithms and Environmental Settings. For baselines, we compare our approach with several prompting methods, including IO, CoT (Wei et al. 2022), CoTSC (Wang et al. 2022), ToT (Gomez 2023), and RAT (CraftJarvis 2024). For the external library and database, we employ the codeparrot/github-jupyter (Hugging Face 2024) 1 as the search vector library of task Code Generation, and the English Wikipedia 2 as library of task Creative Writing and Hallucination Detection. We do not employ any external library for task Game of 24, as the goal of this task is to examine the numerical reasoning ability of RATT’s thought tree structure. For the language model, we perform all the experiments on OpenAI API, GPT-3.5 Turbo (OpenAI 2023) model through the OpenAI platform 3. For the implements, we perform the tasks on NVIDIA 4090.\n\n# Code Generation\n\nCode generation is a task where the model is tasked to understand programming prompts and produce functional and correct code based on those prompts. This task requires precise comprehension and application of programming logic, algorithms, and language syntax of the LLM. We conduct the code generation evaluation in HumanEval. HumanEval is a programming task benchmark specifically for the evaluation of coding capabilities of code generation models. In this task, we first construct a local dataset with CodeParrot and segment long texts into multiple chunks. Then we transform each chunk into embeddings with embedding model text-embedding-ada-002 (OpenAI 2024) 4. After introducing tasks from HumanEval, the RATT generates multiple potential answers. Finally, we validate the effectiveness of our RATT approach using the original evaluation script from the HumanEval GitHub repository by $p a s s @ k$ method (Liu et al. 2024). This pass ${ \\ @ k }$ method tests if there is at least one correct answer within $k$ generated responses to assess the efficacy and accuracy of the model. Our experiments on HumanEval have shown the highest performance improvement of $3 8 . 0 5 \\%$ for $p a s s @ l$ and $1 5 . 1 \\hat { 2 } \\%$ for pass $@ 5$ relatively, as summarized in Table 1 and Figure 3.\n\nTable 1: Comparison of code generation performance on HumanEval of different methods.   \n\n<html><body><table><tr><td>Method</td><td>pass@1</td><td>pass@5</td></tr><tr><td>10</td><td>50.49%</td><td>72.56%</td></tr><tr><td>CoT</td><td>47.31%</td><td>75.88%</td></tr><tr><td>ToT</td><td>49.36%</td><td>77.73%</td></tr><tr><td>RAG_1 shot</td><td>50.61%</td><td>76.22%</td></tr><tr><td>RAG_5 shot</td><td>45.49%</td><td>74.39%</td></tr><tr><td>RAT</td><td>59.27%</td><td>80.49%</td></tr><tr><td>RATT</td><td>69.70%</td><td>83.53%</td></tr></table></body></html>\n\n![](images/3a5c49d510d7be95d598dde7a5dde0a5b59c1ccd2974264fd83275cfbe8ded54.jpg)  \nFigure 3: Comparison of code generation performance on HumanEval of different methods.\n\n# Creative Writing\n\nCreative writing is a task that challenges LLMs to generate imaginative, coherent, and contextually rich text based on a variety of prompts. This task measures the LLMs’ reasoning capabilities to innovate and think logically and coherently while enriching the context information. To further evaluate the RATT’s comprehensive performance on LLMs, we compare it with IO, CoT, ToT and RAT prompting. To assist this process, we utilize the English Wikipedia library by searching this vast and diverse dataset to identify and retrieve the top few web pages that are most relevant to the given writing prompts.\n\nTo ensure the consistency and integrity of the evaluation, we employ GPT-4 (OpenAI 2023; Naismith, Mulcaire, and Burstein 2023) to assist in assessing the quality of model outputs as $5 0 \\%$ of the reviewers of this task while other reviewers are human annotation experts. Our scoring criteria encompass Soundness $( S d )$ , Information Relevance $( R e l )$ , Content Coherence of Reasoning $( C o r r )$ , and Clarity of Expression (Expr), which are uniformly applied. Each dimension receives a score up to 10, with increments of 0.5. The Overall $( O a )$ score is the average of four metrics.\n\n![](images/aa35bb62d78879ff15b41eb77ab43ce3db8e5e7ef3d9149e01bcce9149ff439f.jpg)  \nFigure 4: Comparing methods in Creative Writing.\n\n![](images/3d7c1fca07aeee097e0619defb961788e361fbde54a193b1a96e01f0dc7939a6.jpg)  \nFigure 5: Comparing methods on GPT models.\n\nHere, we first conduct a comparison of various methods on GPT-3.5 Turbo to assess the performance across different metrics. As shown in Figure 4, our approach demonstrates significant advantages over baseline methods in all dimensions and composite scores. We further study the improvements our method brings to both the GPT-3.5 Turbo and GPT-4o (OpenAI 2024) models. As shown in Figure 5, our approach and RAT significantly outperform the GPT-4o model. The results demonstrate the effectiveness of employing RAG on LLM in text generation and boosting the creative output of LLMs. In Figure 6, we present the detailed prompting and guidelines of the creative writing task.\n\n# Hallucination Detection\n\nFollowing the method introduced in (Ding et al. 2024), we perform the standard hallucination detection. Hallucination detection refers to the process of identifying and assessing instances where language models generate text that is not grounded in reality (Luo et al. 2024). This task is crucial for evaluating the reliability of LLMs’ text generation, as we target generating content that is accurate and trustworthy (Gao et al. 2023; Rawte, Sheth, and Das 2023). In this task, we utilized the TruthfulQA (Lin, Hilton, and Evans 2021) dataset to measure the truthfulness of outputs. Each “correct answer” in the dataset aims to reflect the truthfulness of a response to a given query. Our model generates answers to these queries, and then we measure the truthfulness of these answers in two ways:\n\nFirst, we directly calculate the similarity between the generated answers and the correct answers using BLEU and ROUGE scores. Here BLEU and ROUGE scores are the metrics designed to evaluate the quality of text by comparing machine-generated outputs against human references. Then we employ GPT-judge in (SyLinRL 2021) to predict the truthfulness of human-like answers in an end-to-end manner as part of the TruthfulQA project.\n\n# Game of 24\n\nThe “Game of $2 4 ^ { \\prime \\prime }$ is a mathematical puzzle task that challenges an LLM’s numerical reasoning capabilities. The game involves using the combination of addition, subtraction, multiplication, and division to manipulate four integers between 1 and 9 to arrive at the final result of 24. The rules are straightforward: players must use all four numbers exactly once, and the order of operations can vary as needed to solve the puzzle. This task is a benchmark for LLM mathematical reasoning (Kim et al. 2023; Ding et al. 2023), as it requires arithmetic operations and strategic planning abilities to explore various combinations and operation sequences for the optimal solution.\n\nTable 2: Success rates on Game of 24.   \n\n<html><body><table><tr><td>Method</td><td>Success (%)</td></tr><tr><td>10</td><td>7.6</td></tr><tr><td>CoT</td><td>5.1</td></tr><tr><td>CoT-SC</td><td>11.2</td></tr><tr><td>ToT</td><td>14.0</td></tr><tr><td>RAT</td><td>9.0</td></tr><tr><td>RATT-3</td><td>25.9</td></tr><tr><td>RATT</td><td>31.8</td></tr><tr><td>Improvement</td><td>24.2</td></tr></table></body></html>\n\nWe test the models on 100 randomly selected games from 4nums 5. For every test, if the model produces a valid equation that results in 24, we mark it as a success. From Table 2, we can see that the RATT achieves the highest success rate of $3 1 . 8 \\%$ , which significantly outperforms the GPT-3.5 Turbo model by $2 4 . 2 \\%$ of improvement. Here the success of RATT-3 is a success answer of 24 in 3 steps of generation, and RATT in 5. IO, CoT, and CoT-SC can hardly help the model find a solution, while RAT can hardly bring\n\n# Question: Introduce Jin-Yong's Life.\n\nFor agent in range(num_agent) Query: Question $^ +$ ”Try to answer this question/instruction with step-by-step thoughts and make the answer more structural.”\n\n# Answer cut into Chunks\n\n![](images/d6ae992eebdfc5cc196928d089840bcfaf731d13aaa901c7aca82c30f3f8fe9b.jpg)\n\n# For each chunk:\n\n![](images/4322751833c7e8126db2c8bf5c306517fee6db385fc74b5e23cf0fe87bb4fd85.jpg)\n\n# LLM Query\n\nPrompt：”Please summarize the content with the corresponding question.The query should be short but need to be specific to promise we can find related knowledge or pages.”\n\nC\n\n# Raw Document\n\nFor each Document:\n\nPrompt：”Please read the following text and extract only the sections that are relevant to the given question. Organize the extracted information coherently maintaining the structure of multiple paragraphs with subtitles”\n\nC\n\nAfter obtaining a set of modified chunks, directly concatenate them in the original order as the final answer for this Agent.\n\n# For each Chunk:\n\n![](images/dd83b53fc9d83c549009a7d452b58d9b332bc014e841a8c325fc169766a39bab.jpg)\n\nPrompt：”{Texture from RAG} {Original Answer Script} I want to revise the answer according to retrieved related text of the question in WIKI pages. You need to check whether the answer is correct. If you find some errors in the answer, revise the answer to make it better. If you find some necessary details are ignored, add it to make the answer more plausible according to the related text. If you find that a part of the answer is correct and does not require any additional details, maintain that part of the answer unchanged. Directly output the original content of that part without any modifications. \\*\\*IMPORTANT\\*\\* Try to keep the structure (multiple paragraphs with its subtitles) in the revised answer and make it more structual for understanding.”\n\nC\n\nAfter obtaining all the answers from the Agents, the LLM merges each answer into a more refined final answer.\n\n# For all answers from agents:\n\nPrompt：”Referencing the answers provided by all agents, synthesize a more detailed and comprehensive response by integrating all relevant details from these answers. Ensure logical coherence and provide ONLY THE MERGED ANSWER AS THE OUTPUT.”\n\nCombine the merged answer with the question to form the next round's prompt.\n\n# For Next Round:\n\nPrompt：”Base your response on the provided question and the previous answer. Expand the answer by adding more details to enhance its comprehensiveness. Question: {question} Previous Answer: {previous_answer}”\n\nRepeat the above process for n rounds to obtain the final answer.\n\n![](images/4f9ebc93b8f1737fab69fc927596ab9c9c608928a3cea17ba2d63a75b31f71ca.jpg)  \nFigure 6: Prompting case of task Creative Writing.   \nFigure 7: Comparison of hallucination metrics of different methods.\n\nadvantages for LLM to win. These results underline our method’s reasoning capability to enhance the numerical reasoning ability.\n\n# Conclusion\n\nIn this paper, we introduce RATT, a novel thought tree structure that integrates both factual correctness and strategic coherence at every step of the reasoning process. This structure significantly enhances LLMs’ logical coherence and decision-making efficiency in complex reasoning tasks by solving the existing problem of thought structures logically and globally. Our extensive experiments across various tasks demonstrate the effectiveness and superiority over existing methods, especially in improving factual correctness and logical coherence. By adopting RATT on LLMs, we extend the boundaries of abilities that LLM performs in handling complex tasks. For future work, we plan to extend this LLM thought structure paradigm across various reasoning and LLM alignment tasks, and to conduct a thorough investigation into the generation strategies that our approach applies to the thought structure and solution space to study further insights into its underlying mechanisms and impacts.\n\nWhile RATT shows significant advancements and wide adaptability, there are several limitations that require further exploration, including high computational demands and limited scalability with very complex tasks or large external libraries. Second, the effectiveness of the reasoning process and final output heavily relies on the quality of the task prompt and the external library, which can affect the model’s performance in scenarios with poorly organized prompts or limited external knowledge. Finally, adopting RATT in tasks with unique requirements may have inherent limitations.\n\n# Acknowledgments\n\nThe author Xiting Wang was supported by the National Natural Science Foundation of China (NSFC) (NO. 62476279), Major Innovation & Planning Interdisciplinary Platform for the “Double-First Class” Initiative, Renmin University of China, and the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China No. 24XNKJ18.",
    "institutions": [
        "Portland State University",
        "Renmin University of China",
        "Pennsylvania State University",
        "Dalian Maritime University",
        "University of Kansas"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n现有大语言模型（LLMs）的思维结构方法，如思维树（Tree of Thought）和检索增强思维（Retrieval Augmented Thoughts），在复杂任务中存在局部事实知识检索不足和全局策略选择不佳的问题，难以有效平衡事实准确性和全面逻辑优化，限制了其在复杂动态场景中的应用。解决这些问题对于提升LLMs在复杂任务中的推理和决策能力至关重要。\\n\\n**方法概述**\\n论文提出了检索增强思维树（Retrieval Augmented Thought Tree，RATT）这一新型思维结构，在推理过程的每一步同时考虑整体逻辑合理性和事实正确性，通过规划、前瞻和结合检索增强生成（RAG）的事实核查能力与LLMs的整体策略评估能力，优化推理过程。\\n\\n**主要贡献与效果**\\n- 提出RATT思维结构，显著增强了LLMs在复杂推理任务中的逻辑连贯性和决策效率。在代码生成任务的HumanEval中，RATT的pass@1提高到69.70%，相较于IO的50.49%高出19.21个百分点；pass@5达到83.53%，相比IO的72.56%高出10.97个百分点。在24点游戏中，RATT的成功率达到31.8%，比IO的7.6%高出24.2个百分点。\\n- 开发了将RAG融入树状思维过程的新范式，在思维树的每个节点进行前瞻和从一般到细节的事实核查分析。\\n- 通过一系列实验验证了RATT方法在不同任务中的有效性和鲁棒性，在创意写作任务中，RATT在所有维度和综合评分上均显著优于IO、CoT、ToT和RAT提示方法；在幻觉检测任务中，RATT能有效降低模型产生幻觉的风险，表明该方法明显优于现有方法。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nRATT的核心思想是在推理过程中同时确保局部事实正确性和全局综合逻辑合理性。通过在每个推理步骤进行规划和前瞻，评估多个潜在推理步骤，并结合RAG的事实修正能力和LLMs的整体策略评估能力，将事实知识与策略可行性相结合，在搜索空间中寻找最有前景的分支，从而提高LLMs逻辑推理的连贯性和决策的可靠性。\\n\\n**创新点**\\n先前的思维结构方法如Chain of Thought（CoT）、Tree of Thought（ToT）和Retrieval Augmented Thoughts（RAT）等，存在局部事实准确性和全局策略规划有效性难以平衡的问题。CoT和ToT缺乏有效的事实检查机制，早期推理错误会导致结论偏离现实；ToT的最优分支全局选择机制不够有效，会生成过多文本和分支；RAT在全局策略优化方面面临挑战，容易陷入局部次优解。与这些方法相比，RATT在每个推理步骤同时考虑事实正确性和逻辑合理性，通过规划和前瞻进行全局策略优化，并采用分层检索策略进行事实检查，避免了早期错误的传播和累积。\\n\\n**具体实现步骤**\\n1. **思想节点生成**：给定包含任务信息的提示 $x$，LLM $p_{\\theta}$ 首先对提示进行嵌入：$q_x = p_{\\theta} (\\mathrm{embed} (x))$。然后利用多种逻辑策略 $s^{(m)}$ 生成初始思想，每个初始思想代表一个独特的逻辑观点，这些思想共同覆盖任务 $\\mathcal{A}$ 的各种可能方向和解决方案。在第 $t$ 次迭代中，定义初始思想为初始节点 $n_t = \\{n_t^{(1)}, n_t^{(2)}, \\dots, n_t^{(m)}\\}$。之后，思维树采用规划和前瞻策略，对每个节点 $n_t^m$ 评估下一步的质量，并模拟未来状态和后果，以调整生成策略，使其与任务的总体目标保持一致。\\n2. **检索与选择**：对于包含 $I$ 个候选文档的库 $\\mathcal{R} := \\{R_i\\}_{i = 1}^I$，LLM对每个文档进行嵌入：$r_i = p_{\\theta} (\\mathrm{embed} (R_i)) \\in \\mathbb{R}^K$。然后为每个节点 $n_t^m$ 和输入 $x$ 形成查询 $Q_t^m$，并将其嵌入：$q_t^m = \\mathrm{embed} \\{Q_t^m\\} = \\{q_x, q_{n_t^m}\\} \\in \\mathbb{R}^K$。通过计算查询与文档嵌入的余弦相似度 $\\mathrm{sim} (q_t^m, r_i) = \\frac{q_t^m \\cdot r_i}{\\|q_t^m\\| \\|r_i\\|}$，检索出最相关的前 $k$ 个文档 $\\mathcal{R}_{\\mathrm{selected}}$。在检索过程中，采用从一般理解到具体细节的分层检索策略，在不同层次的节点采用不同的检索策略，以获取不同层次的信息。\\n3. **RAG校正与整合**：LLM $p_{\\theta}$ 分析检索到的相关文档 $\\mathcal{R}_{\\mathrm{selected}}$，提取与初始节点 $n_t^1, n_t^2, \\ldots, n_t^m$ 相关的关键和补充信息，包括事实细节、背景解释、逻辑支持和反例等，并将这些信息整合到一个优化节点 $n_t^* = p_{\\theta} (n_t^1, n_t^2, \\ldots, n_t^m, \\{\\mathcal{R}_{\\mathrm{selected}}\\})$ 中。将优化节点 $n_t^*$ 与原始提示 $x$ 组合成新的提示，开始新一轮的生成。迭代过程持续进行，直到达到最大迭代次数 $T$，最后根据任务 $\\mathcal{A}$ 的具体要求，输出节点 $n_T^*$ 或其完整推理作为决策和推理结果。\\n\\n**案例解析**\\n论文以“猫如何捕捉和吃鱼”的问题为例，展示了RATT的工作过程。在传统思维结构中，LLM可能会得出“猫是水生动物，和鱼一样”的错误结论。而采用RATT，通过RAG技术，LLM可以动态校正推理过程中的事实错误，得出“猫是陆地动物，鱼是水生动物，猫可以在水边抓鱼”等更符合事实的结论。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括输入输出（IO）、思维链（CoT）、带自一致性的思维链（CoT - SC）、思维树（ToT）和检索增强思维（RAT）。\\n\\n**性能对比**\\n*   **在 [代码生成的pass@1] 指标上：** 本文的RATT方法在HumanEval任务中达到了 **69.70%**，优于IO的50.49%、CoT的47.31%、ToT的49.36%、RAG_1 shot的50.61%、RAG_5 shot的45.49%和RAT的59.27%，相对最佳基线RAT提升了10.43个百分点。\\n*   **在 [代码生成的pass@5] 指标上：** RATT达到了 **83.53%**，优于IO的72.56%、CoT的75.88%、ToT的77.73%、RAG_1 shot的76.22%、RAG_5 shot的74.39%和RAT的80.49%，相对最佳基线RAT提升了3.04个百分点。\\n*   **在 [创意写作的综合评分] 指标上：** RATT在所有维度和综合评分上均显著优于IO、CoT、ToT和RAT提示方法。\\n*   **在 [24点游戏的成功率] 指标上：** RATT达到了 **31.8%**，显著优于IO的7.6%、CoT的5.1%、CoT - SC的11.2%、ToT的14.0%和RAT的9.0%，比IO高出24.2个百分点。\\n*   **在 [幻觉检测] 方面**：论文虽未给出具体数据对比，但通过采用TruthfulQA数据集，利用BLEU、ROUGE分数和GPT - judge预测真实性等方式，体现出RATT能有效降低模型产生幻觉的风险，优于其他未提及有此能力的基线模型。\",\n    \"keywords\": \"### 关键词\\n\\n- 大语言模型 (Large Language Models, LLMs)\\n- 思维结构 (Thought Structures, N/A)\\n- 检索增强思维树 (Retrieval Augmented Thought Tree, RATT)\\n- 推理决策 (Reasoning and Decision - Making, N/A)\\n- 代码生成 (Code Generation, N/A)\\n- 创意写作 (Creative Writing, N/A)\\n- 幻觉检测 (Hallucination Detection, N/A)\\n- 24点游戏 (Game of 24, N/A)\"\n}"
}