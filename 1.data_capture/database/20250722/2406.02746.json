{
    "source": "Semantic Scholar",
    "arxiv_id": "2406.02746",
    "link": "https://arxiv.org/abs/2406.02746",
    "pdf_link": "https://arxiv.org/pdf/2406.02746.pdf",
    "title": "RATT: A Thought Structure for Coherent and Correct LLM Reasoning",
    "authors": [
        "Jinghan Zhang",
        "Xiting Wang",
        "Weijieying Ren",
        "Lu Jiang",
        "Dongjie Wang",
        "Kunpeng Liu"
    ],
    "categories": [
        "cs.CL"
    ],
    "publication_date": "2024-06-04",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 18,
    "influential_citation_count": 0,
    "institutions": [
        "Portland State University",
        "Renmin University of China",
        "Pennsylvania State University",
        "Dalian Maritime University",
        "University of Kansas"
    ],
    "paper_content": "# RATT: A Thought Structure for Coherent and Correct LLM Reasoning\n\nJinghan Zhang1, Xiting Wang2, Weijieying $\\mathbf { R e n } ^ { 3 }$ , Lu Jiang4, Dongjie Wang5, Kunpeng Liu\n\n1Portland State University 2Renmin University of China 3Pennsylvania State University 4Dalian Maritime University 5University of Kansas jinghanz,kunpeng @pdx.edu, xitingwang@ruc.edu.cn, wjr5337 $@$ psu.edu, jiangl761@dlmu.edu.cn, wangdongjie@ku.edu\n\n# Abstract\n\nLarge Language Models (LLMs) gain substantial reasoning and decision-making capabilities from thought structures. However, existing methods such as Tree of Thought and Retrieval Augmented Thoughts often fall short in complex tasks due to the limitations of insufficient local retrieval of factual knowledge and inadequate global selection of strategies. These limitations make it challenging for these methods to balance factual accuracy and comprehensive logical optimization effectively. To address these limitations, we introduce the Retrieval Augmented Thought Tree (RATT), a novel thought structure that considers both overall logical soundness and factual correctness at each step of the thinking process. Specifically, at every point of a thought branch, RATT performs planning and lookahead to explore and evaluate multiple potential reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented Generation (RAG) with LLMs’ ability to assess overall strategy. Through this combination of factual knowledge and strategic feasibility, the RATT adjusts and integrates the thought tree structure to search for the most promising branches within the search space. This thought structure significantly enhances the model’s coherence in logical inference and efficiency in decision-making, and thus increases the limit of the capacity of LLMs to generate reliable inferences and decisions based on thought structures. A broad range of experiments on different types of tasks showcases that the RATT structure significantly outperforms existing methods in factual correctness and logical coherence.\n\n# Introduction\n\nLarge Language Models (LLMs) have shown impressive reasoning and decision-making capabilities in complex tasks like mathematical reasoning and creative writing by processing and generating thoughts based on token-level predictions (Huang et al. $2 0 2 2 \\mathrm { a }$ ; Zhang et al. 2023). However, this approach limits their ability to extend higher-level or multi-perspective reasoning (Huang et al. 2022b). Research on LLM reasoning indicates that implementing structured thought processes significantly enhances their performance in inference and decision-making (Yu et al. 2023).\n\nThese thought structures help LLMs to organize and generate their responses both contextually and hierarchically, as well as logically coherent reason across extended narratives and complex problem spaces.\n\nResearchers have developed various thought structures for LLMs. Compared to the vanilla input-output (IO) (Zhang et al. 2024) prompting, where the model directly responds to the prompt, the basic idea of thought structures like Chainof-Thought (CoT) (Wei et al. 2022) and Self-consistency with CoT (CoT-SC) (Wang et al. 2022) is to guide the reasoning process by generating a sequential and coherent reasoning framework according to the given task. These methods usually involve generating a series of intermediate thought steps for analyzing and solving the initial question. They significantly improve LLM reasoning performance and the reasoning process transparency.\n\nDespite the advancements in improving LLMs’ reasoning capability, current thought structures still encounter a major challenge in balancing local factual accuracy and global strategy planning effectiveness. These limitations could restrict their applicability in complex and dynamic scenarios. For example, in Figure 1, CoT and Tree of Thoughts (ToT) (Yao et al. 2024), which simulate coherent reasoning processes, lack an effective fact-checking mechanism. As a result, errors that occur in the early stage of reasoning can propagate through the entire thought chain and lead the conclusions to deviate far away from reality. Moreover, the global selection mechanism of the optimal branch of ToT is not effective and efficient enough, which could result in generating excessive texts and branches (Ding et al. 2023). While Retrieval Augmented Thoughts (RAT) (Wang et al. 2024) combine CoT and Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) to perform fact checking, this method faces challenges in global strategy optimization, as it lacks systematic planning and lookahead to search for the optimal reasoning path in the search space and hence is limited in complex tasks where a broader perspective is necessary (Zhao et al. 2024; Renze and Guven 2024).\n\nOur Targets. To solve the aforementioned issues and enhance the reliability and accuracy of LLM reasoning, we aim to develop a method that seamlessly unifies local and global optimization. At the local level, the method should utilize external knowledge early and continuously to prevent and correct factual errors and thus avoid leading the\n\nTree  o(fTToTh)ought Round 1 The answers could be quite similar PrToasmkpt RetrTiehvoaulgAhut gTmreented   \nPrompt P The cat is a land animal, fish is aquatic. LLM Q: How does a cat catch and eat fish? The cat is an aquatic animal, just like fish. Round 1 Adopting RAG on Thought Tree   \nLLM Round 2 And somtimes incorrect. The cat is a land animal, fish is aquatic. The cat can catch fish by the water. LLM + RAG The cat is anlanqduatniicmanl,imfiaslhl ikseafqiusaht.ic. P The cat can hardly catch fish by the river. The cat swims in the water to chase fish. The cat is a land animal, fish is aquatic. LLM The cat is a land animal, fish is aquatic.   \nRetrieTvhaloAuguhgtmsented Q: How does a cat catch and eat fish? Cats have special skills of catching fish. (RAT) Round 1 RAT can correct the errors in a CoT. Layer 1 Task The cat is anlanqduatniicmanl,imfiasl.h  i s not. Layer 2 电 sAksi las aonfdcatnicmhianl,gtfhisehc..a.t has special   \nPrompt Round 2 But if the CoT gets off-track early,   \nRAG LLM Land animals can't swim, so they cannot Round 2 catch fish. To catch a fish, the cat first... Round 3 We still get wrong answers. Layer T Round ... We can get a output with less The cat can't eat fish. Round T errors and more soundness.\n\nmodel to incorrect search areas. If the factual errors are not identified early, they accumulate during subsequent generations and eventually lead to unreliable conclusions. At the global level, the method should structure the reasoning process with planning and lookahead abilities. This allows for the identification of logically coherent and globally optimal reasoning paths even when there are frequent corrections of factual errors. Solely relying on comprehensive correction after the initial reasoning often leads to suboptimal solutions, as post-processing may not correct all factual errors or even introduce new errors due to delayed fact-checking.\n\nOur Approach. To achieve these goals, we develop the Retrieval Augmented Thought Tree (RATT), a novel thought structure that simultaneously ensures both local factual correctness and global comprehensive logical soundness at each step of the reasoning process in one unified framework. In RATT, we perform planning and lookahead to multiple potential reasoning steps at each step of reasoning and integrate the fact-correction capability of RAG along with LLM’s assessment of its overall strategy. Then, RATT integrates factual correctness and logical strategy feasibility to optimize the reasoning process and guide us toward the most promising branches within the search space. This approach greatly improves the logical coherence and inference reliability of LLM’s decision-making and precise reasoning.\n\nIn summary, our contribution includes:\n\n1. We introduce a new thought structure called Retrieval Augmented Thought Tree (RATT), which considers and improves comprehensive logical soundness and factual correctness at each step of the reasoning process. This structure significantly enhances the logical coherence and decision-making efficiency of LLMs in complex reasoning tasks.\n\n2. We develop a novel paradigm incorporating RAG into a tree-structured thought process. This paradigm conducts lookahead and general-to-detail fact-checking analyses at each node of the thought tree.\n\n3. We conduct a series of experiments to validate the effectiveness and robustness of our method (RATT) across different tasks. Our experiments show that our method has clear advantages over existing methods.\n\n# Related Works\n\n# Thought Structures for LLMs\n\nThought structures are a series of prompt engineering methods that guide models to generate more specific, accurate, and high-quality content (Zhang et al. 2022; Minaee et al. 2024). Among these methods, Chain of Thought (CoT) (Wei et al. 2022) is a milestone development that guides a reasoning process by generating a series of logically coherent intermediate steps of inference. Based on CoT, Wang et al. (2022) (Wang et al. 2022) introduce the Self-consistency with CoT (CoT-SC) method, which enhances reasoning accuracy and stability by independently generating multiple thought chains and selecting the most reliable answer. Furthermore, the ToT extends CoT by constructing a thought tree with multiple reasoning branches. The thought tree structure is capable of planning, looking ahead, and backtracking, thus providing a broad and global view of the solution space. These developments make multi-step thinking and reasoning similar to human cognitive processes.\n\n# Retrieval-Augmented Generation for LLM Reasoning\n\nRetrieval-Augmented Generation (RAG) (Lewis et al. 2020) for LLM reasoning has become a vital approach to enhance the quality of output. The LLM generates a response after retrieving information in an external library, which is a set of documents or knowledge related to the task, with a query relevant to the task (Lewis et al. 2020; Shuster et al. 2021). This approach helps the LLM to produce responses that are more accurate, contextually relevant, and with fewer hallucinations (Yang et al. 2024; Wu et al. 2024). Following RAG, (Wang et al. 2024) developed the RetrievalAugmented Thoughts (RAT) approach, which incorporates the RAG retrieval process into a reasoning chain-of-thought. The RAT approach first generates a reasoning chain and retrieves relevant information using LLM-generated queries based on the prompt and each reasoning step. Then RAT corrects and refines the reasoning chain step by step. The primary advantage of RAT is its ability to correct errors in the reasoning process, which is one notable step for enhancing the performance of LLM reasoning. However, as RAT only thinks and refines following one certain and complete path of thought, one potential problem of RAT is to fall into a local suboptimal solution in the search space.\n\n# Methodology\n\nIn this section, we introduce Retrieval Augmented Thought Tree (RATT), an automated novel thought structure for language models that prioritizes both logical coherence and factual correctness. We aim to enhance the reliability and accuracy of LLM reasoning in two aspects: robust local factchecking to prevent the accumulation of errors and global planning and lookahead to improve the logical coherence of reasoning paths. As shown in Figure 2, we implement this RATT through several steps, including (1) Thought Node Generation, (2) Retrieval and Selection, and (3) RAG Correction and Integration.\n\n# Problem Formulation\n\nTo formalize our target and solution, first let us define the elements and functions. Given a task $\\mathcal { A }$ and a pre-trained LLM $p _ { \\theta }$ with parameters $\\theta$ , we aim to enhance the performance of LLM on task $\\mathcal { A }$ with performance metric $\\boldsymbol { \\epsilon } = \\{ \\epsilon _ { 1 } , \\epsilon _ { 2 } , . . . , \\epsilon _ { j } \\}$ . The task $\\mathcal { A }$ can be any problem that can be solved with a logical sequence of thoughts or decisions. We have an input $x$ which is a language sequence and an output $y$ . Our target is to construct the best thought tree structure $\\mathcal { T } = \\mathcal { T } ^ { * }$ capable of navigating the search space and optimizing the reasoning process with task $\\mathcal { A }$ and prompt $x$ :\n\n$$\n\\mathcal { T } ^ { * } = \\arg \\operatorname* { m a x } _ { \\mathcal { T } } \\epsilon ( y ) , \\quad \\mathrm { w h e r e } \\quad y = p _ { \\theta } ( x | \\mathcal { T } ) .\n$$\n\nHere we denote a thought tree $\\boldsymbol { \\mathcal { T } } = ( \\boldsymbol { \\mathcal { N } } , \\boldsymbol { \\mathcal { E } } )$ to be a hierarchical tree structure consists of nodes and edges where each nodes represents a thought or decision, and an edge represents a logical flow between two nodes. Here $\\mathcal { N }$ is the set of nodes in the tree and $\\mathcal { E } \\subseteq \\mathcal { N } \\times \\mathcal { N }$ is the set of directed edges connecting the nodes.\n\n# Thought Node Generation\n\nOur first step is to generate a thought tree that balances both exploration and exploitation adeptly and effectively. Given an prompt $x$ with task information, the LLM $p _ { \\theta }$ first embeds\n\nthe prompt:\n\n$$\nq _ { x } = p _ { \\theta } ( { \\mathrm { e m b e d } } ( x ) ) .\n$$\n\nThen the LLM generates initial thoughts utilizing multiple logical strategies $s ^ { ( m ) }$ to explore possible solutions in the search space of task $\\mathcal { A }$ from a broad range of perspectives and logical approaches. Each initial thought represents a distinct logical viewpoint, and these thoughts collectively cover various possible or potential directions and solutions to tasks $\\mathcal { A }$ . Specifically, in the $t$ -th iteration, we define the initial thoughts to be the initial nodes of the ${ \\bf n } _ { t } = \\{ n _ { t } ^ { ( 1 ) } , n _ { t } ^ { ( 2 ) } , \\dots , n _ { t } ^ { ( m ) } \\}$ nt(m)}. In this way, the thought tree takes every step with a broad and diverse range of logical possibilities. We denote a reasoning branch $\\boldsymbol { B }$ of the tree as a sequence of connected nodes starting from the root node and extending to any leaf node. Each branch $\\boldsymbol { B }$ represents a feasible solution for task $\\mathcal { A }$ , and each node in the branch sequence is connected by an edge $\\mathcal { E }$ from its parent node, formulated as:\n\n$$\n\\begin{array} { r } { \\mathcal { B } = \\{ n _ { r o o t } , \\dots , n _ { l e a f } \\} , ( n _ { i } \\xrightarrow { p _ { \\theta } } n _ { i + 1 } ) \\in \\mathcal { E } . } \\end{array}\n$$\n\nAfter generating initial nodes, the thought tree employs a planning and lookahead strategy, which is essential for dynamically evaluating the potential of branches. For each node $n _ { t } ^ { m }$ , the LLM evaluates the quality of the immediate next steps as well as simulates the future states and consequences of the node’s choice. The simulation provides insights into the likely outcomes to the model. This lookahead strategy helps the model to plan several moves ahead, refine and adjust generation strategies, and keep aligning the generation direction with the overall goal of the task.\n\n# Retrieval and Selection\n\nGiven a Library with $I$ candidate documents $\\mathcal { R } : = \\{ R _ { i } \\} _ { i = 1 } ^ { I }$ , the LLM embedes each document as:\n\n$$\nr _ { i } = p _ { \\theta } ( { \\mathrm { e m b e d } } ( R _ { i } ) ) \\in \\mathbb { R } ^ { K } ,\n$$\n\nwhere $K$ is the dimension of the embedding and embed $( \\cdot )$ is the embedding process. We then form a query $Q _ { t } ^ { m }$ of each $n _ { t } ^ { m }$ and the input $x$ :\n\n$$\nq _ { t } ^ { m } = \\mathrm { e m b e d } \\{ Q _ { t } ^ { m } \\} = \\{ q _ { x } , q _ { n _ { t } ^ { m } } \\} \\in \\mathbb { R } ^ { K } , \\mathrm { w h e r e }\n$$\n\n$$\nq _ { x } = p _ { \\theta } ( { \\mathrm { e m b e d } } ( x ) ) , { \\mathrm { a n d } }\n$$\n\n$$\nq _ { n _ { t } ^ { m } } = p _ { \\theta } ( { \\mathrm { e m b e d } } ( n _ { t } ^ { m } ) ) .\n$$\n\nWe then retrieve the whole Library to find the top- $k$ most relevant documents. Here we evaluate the relevance of each document $R _ { i }$ to the query $Q _ { t } ^ { m }$ by the cosine similarity of the embeddings:\n\n$$\n\\mathrm { s i m } ( q _ { t } ^ { m } , r _ { i } ) = \\frac { q _ { t } ^ { m } \\cdot r _ { i } } { \\| q _ { t } ^ { m } \\| \\| r _ { i } \\| } .\n$$\n\nThese selected documents $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }$ are then adept to enhance the information content of the initial nodes and support the fact-checking. Analogizing human learning and reasoning processes that go top to bottom from general understanding to specific details, we adopt a similar strategy\n\nStrategies Nodes RAG Correction & Integration Input LLM (1) nt(1) n (1) Inferences and Decisions Task A Generate (2) n (2) (2) nt Fisrt Then Prompt x pθ Iteration t (m) nt(m) pθ n (m) pθ The best choice is ... t = t+1 LLM Retrieval and Selection Query of each node Retrieve by BhUAL r = {ri}, qt( 1) = {qx,qnt( 1 ) } { Rselected } LIBRARY cos (qt, ri) PrRo=m{pRt }x pθ Embed qxt( 1,)q (2) nt( m) qt( m .).=. {qx,qnt( m  )} 电 in the retrieval process. We denote all nodes in the same round of generation to be in the same layer $L$ . Specifically, at the top layers $L _ { 1 } , L _ { 2 } , . . . , L _ { l _ { 1 } } , l _ { 1 } \\ \\in \\ [ 1 , l )$ , we utilize a broad and high-level retrieval strategy to gather basic concepts and background information; At the middle layers $L _ { l _ { 1 } + 1 } , . . . , L _ { l _ { 2 } } , l _ { 2 } \\in [ l _ { 1 } + 1 , l )$ , we utilize a targeted retrieval strategy to acquire deeper information; At the leaf layers $L _ { l _ { 2 } + 1 } , . . . , L _ { l } , \\bar { l _ { 2 } } \\in [ l _ { 1 } \\bar { + } 1 , l )$ , we utilize a detailed and specific retrieval strategy to correct specific errors and enrich the details.\n\n# RAG Correction and Integration\n\nWe then correct and integrate the initial nodes with relevant documents $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }$ . In this process, the LLM $p _ { \\theta }$ analyzes the documents $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }$ and extracts the most critical and supplementary information relevant to $n _ { t } ^ { 1 } , n _ { t } ^ { 2 } , \\ldots , n _ { t } ^ { m }$ The information includes factual details, background explanations, logical support, and counter-examples. The model $p _ { \\theta }$ then integrates them into a single, optimized node:\n\n$$\nn _ { t } ^ { * } = p _ { \\theta } ( n _ { t } ^ { 1 } , n _ { t } ^ { 2 } , \\ldots , n _ { t } ^ { m } , \\{ R _ { \\mathrm { s e l e c t e d } } \\} ) .\n$$\n\nHere the role of $p _ { \\theta }$ is to ensure that information integration utilizes the content of documents correctly while keeping the backbones of the reasoning sequence. In this way, we enhance the depth and breadth of the node’s information, and the optimized node $n _ { t } ^ { * }$ becomes more reliable and comprehensive in subsequent reasoning and decision-making processes. For all nodes generated in this layer, $n _ { t } ^ { * }$ represents the most valuable solution that LLM finds in the search space.\n\nAfter generating the refined node $n _ { t } ^ { * }$ , we combine $n _ { t } ^ { * }$ with the original prompt $x$ to be the new prompt of task $\\mathcal { A }$ , and the model starts a new round of generation. This iterative process continues until the maximum number of iterations $T$ is reached. At the end of the iterations, depending on the specific requirements of task $\\mathcal { A }$ , the model output either the node $n _ { T } ^ { * }$ or the complete inference of $n _ { T } ^ { * }$ as decisions and inference.\n\n# Discussions\n\nWe further discuss why our method achieves the goals mentioned in the Introduction. First, to address the local challenge of correcting factual errors and avoiding the accumulation of errors, we need to perform fact-checking and corrections in a timely and continuous manner. Our RATT method utilizes RAG technology to help LLMs efficiently and rapidly access information from relevant documents in an external library. The LLM integrates external knowledge to dynamically correct the factual errors caused by its possible limited knowledge, outdated information, or hallucinations within the reasoning context. Second, to address the global challenge of strategy optimization and searching for the optimal solution, we need to comprehensively evaluate the overall generating and searching strategy. Our RATT method integrates factual accuracy and strategic feasibility to perform correction and optimization on each generated thought. In this way, the RATT identifies and navigates the most promising branch within the search space. Finally, to reduce the risk of hallucinations, we need to make corrections during the generation process rather than after the entire reasoning process is completed. This is because the errors that develop and spread through the structure are difficult to entirely correct afterward, as they may have already polluted and magnified the whole generation. Thus, we design an online, incremental generation and correction tree structure. The experimental details in the next section demonstrate the effectiveness of our RATT’s capability in preventing the spread of errors and reducing hallucinations.\n\n# Experiments Experimental Setup\n\nIn this section, we propose four particularly challenging or representative tasks for LLM performance evaluation, and demonstrate the effectiveness of our RATT approach. These multi-view tasks provide standard benchmarks to evaluate and compare the performance of our approach and baseline\n\n1: Input: Task prompt $x$ , Node number $m$ , Iteration time $T$ , LLM $p _ { \\theta }$   \n2: $n _ { t }  \\cdots \\triangleright$ Initialize the node as an empty string   \n3: for $t = 1$ to $T$ do   \n4: for $l = 1$ to $m$ do   \n5: $n _ { t } ^ { ( l ) } \\gets p _ { \\theta } \\big ( \\cdot \\mid x , n _ { t } \\big ) \\ \\vartriangle { \\mathbb { P } } ^ { ( \\cdot ) }$ Generate thoughts based on node $n _ { t }$ and prompt $x$   \n6: $\\{ q _ { x } , q _ { n _ { t } } ^ { ( l ) } \\}  \\mathrm { e m b e d } ( \\{ n _ { t } ^ { ( l ) } , x \\} )$ ▷ Embed the node and prompt   \n7: $q _ { t } ^ { ( \\bar { l } ) } \\gets \\mathrm { \\dot { C } O N C A T } ( q _ { x } , q _ { n _ { t } } ^ { ( l ) } ) \\qquad \\triangleright$ Concatenate the question and answer into a query vector   \n8: $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }  \\mathrm { R e t r i e v e F r o m L i b r a r y } ( q _ { t } ^ { ( l ) } )$ ▷ Retrieve documents $\\mathcal { R } _ { \\mathrm { s e l e c t e d } }$ from Library   \n9: $n _ { t } ^ { ( l ) * }  p _ { \\theta } ( \\cdot \\mid n _ { t } ^ { ( l ) } , \\mathcal { R } _ { \\mathrm { s e l e c t e d } } )$ $D$ Generate a refined node   \n10: $n _ { t } \\gets \\mathrm { C O N C A T } ( n _ { t } , n _ { t } ^ { ( l ) * } )$ ▷ Append the next refined node   \n11: end for   \n12: $n _ { t } ^ { * } \\gets p _ { \\theta } ( \\cdot \\mid x , n _ { t } )$ $D$ Refine and enhance the node   \n13: end for   \n14: $n _ { T } ^ { * }  p _ { \\theta } ( \\cdot \\mid x , n _ { t } ^ { * } )$   \n15: return n∗T ▷ Output $n _ { T } ^ { * }$ as the final generation\n\nmethods. Then we conduct an analysis of the results and discuss the strengths of our approach shown in the performance enhancement.\n\nTask Description. We test our RATT and baseline methods across four distinct tasks, each evaluating different aspects of the quality of LLM response. The Code Generation and Creative Writing are two comprehensive tasks, which Game of 24 focus on testing the logical and numerical reasoning capabilities. We also process a standard Hallucination Detection to demonstrate the RATT’s ability to reduce hallucinations.\n\nBaselines Algorithms and Environmental Settings. For baselines, we compare our approach with several prompting methods, including IO, CoT (Wei et al. 2022), CoTSC (Wang et al. 2022), ToT (Gomez 2023), and RAT (CraftJarvis 2024). For the external library and database, we employ the codeparrot/github-jupyter (Hugging Face 2024) 1 as the search vector library of task Code Generation, and the English Wikipedia 2 as library of task Creative Writing and Hallucination Detection. We do not employ any external library for task Game of 24, as the goal of this task is to examine the numerical reasoning ability of RATT’s thought tree structure. For the language model, we perform all the experiments on OpenAI API, GPT-3.5 Turbo (OpenAI 2023) model through the OpenAI platform 3. For the implements, we perform the tasks on NVIDIA 4090.\n\n# Code Generation\n\nCode generation is a task where the model is tasked to understand programming prompts and produce functional and correct code based on those prompts. This task requires precise comprehension and application of programming logic, algorithms, and language syntax of the LLM. We conduct the code generation evaluation in HumanEval. HumanEval is a programming task benchmark specifically for the evaluation of coding capabilities of code generation models. In this task, we first construct a local dataset with CodeParrot and segment long texts into multiple chunks. Then we transform each chunk into embeddings with embedding model text-embedding-ada-002 (OpenAI 2024) 4. After introducing tasks from HumanEval, the RATT generates multiple potential answers. Finally, we validate the effectiveness of our RATT approach using the original evaluation script from the HumanEval GitHub repository by $p a s s @ k$ method (Liu et al. 2024). This pass ${ \\ @ k }$ method tests if there is at least one correct answer within $k$ generated responses to assess the efficacy and accuracy of the model. Our experiments on HumanEval have shown the highest performance improvement of $3 8 . 0 5 \\%$ for $p a s s @ l$ and $1 5 . 1 \\hat { 2 } \\%$ for pass $@ 5$ relatively, as summarized in Table 1 and Figure 3.\n\nTable 1: Comparison of code generation performance on HumanEval of different methods.   \n\n<html><body><table><tr><td>Method</td><td>pass@1</td><td>pass@5</td></tr><tr><td>10</td><td>50.49%</td><td>72.56%</td></tr><tr><td>CoT</td><td>47.31%</td><td>75.88%</td></tr><tr><td>ToT</td><td>49.36%</td><td>77.73%</td></tr><tr><td>RAG_1 shot</td><td>50.61%</td><td>76.22%</td></tr><tr><td>RAG_5 shot</td><td>45.49%</td><td>74.39%</td></tr><tr><td>RAT</td><td>59.27%</td><td>80.49%</td></tr><tr><td>RATT</td><td>69.70%</td><td>83.53%</td></tr></table></body></html>\n\n![](images/3a5c49d510d7be95d598dde7a5dde0a5b59c1ccd2974264fd83275cfbe8ded54.jpg)  \nFigure 3: Comparison of code generation performance on HumanEval of different methods.\n\n# Creative Writing\n\nCreative writing is a task that challenges LLMs to generate imaginative, coherent, and contextually rich text based on a variety of prompts. This task measures the LLMs’ reasoning capabilities to innovate and think logically and coherently while enriching the context information. To further evaluate the RATT’s comprehensive performance on LLMs, we compare it with IO, CoT, ToT and RAT prompting. To assist this process, we utilize the English Wikipedia library by searching this vast and diverse dataset to identify and retrieve the top few web pages that are most relevant to the given writing prompts.\n\nTo ensure the consistency and integrity of the evaluation, we employ GPT-4 (OpenAI 2023; Naismith, Mulcaire, and Burstein 2023) to assist in assessing the quality of model outputs as $5 0 \\%$ of the reviewers of this task while other reviewers are human annotation experts. Our scoring criteria encompass Soundness $( S d )$ , Information Relevance $( R e l )$ , Content Coherence of Reasoning $( C o r r )$ , and Clarity of Expression (Expr), which are uniformly applied. Each dimension receives a score up to 10, with increments of 0.5. The Overall $( O a )$ score is the average of four metrics.\n\n![](images/aa35bb62d78879ff15b41eb77ab43ce3db8e5e7ef3d9149e01bcce9149ff439f.jpg)  \nFigure 4: Comparing methods in Creative Writing.\n\n![](images/3d7c1fca07aeee097e0619defb961788e361fbde54a193b1a96e01f0dc7939a6.jpg)  \nFigure 5: Comparing methods on GPT models.\n\nHere, we first conduct a comparison of various methods on GPT-3.5 Turbo to assess the performance across different metrics. As shown in Figure 4, our approach demonstrates significant advantages over baseline methods in all dimensions and composite scores. We further study the improvements our method brings to both the GPT-3.5 Turbo and GPT-4o (OpenAI 2024) models. As shown in Figure 5, our approach and RAT significantly outperform the GPT-4o model. The results demonstrate the effectiveness of employing RAG on LLM in text generation and boosting the creative output of LLMs. In Figure 6, we present the detailed prompting and guidelines of the creative writing task.\n\n# Hallucination Detection\n\nFollowing the method introduced in (Ding et al. 2024), we perform the standard hallucination detection. Hallucination detection refers to the process of identifying and assessing instances where language models generate text that is not grounded in reality (Luo et al. 2024). This task is crucial for evaluating the reliability of LLMs’ text generation, as we target generating content that is accurate and trustworthy (Gao et al. 2023; Rawte, Sheth, and Das 2023). In this task, we utilized the TruthfulQA (Lin, Hilton, and Evans 2021) dataset to measure the truthfulness of outputs. Each “correct answer” in the dataset aims to reflect the truthfulness of a response to a given query. Our model generates answers to these queries, and then we measure the truthfulness of these answers in two ways:\n\nFirst, we directly calculate the similarity between the generated answers and the correct answers using BLEU and ROUGE scores. Here BLEU and ROUGE scores are the metrics designed to evaluate the quality of text by comparing machine-generated outputs against human references. Then we employ GPT-judge in (SyLinRL 2021) to predict the truthfulness of human-like answers in an end-to-end manner as part of the TruthfulQA project.\n\n# Game of 24\n\nThe “Game of $2 4 ^ { \\prime \\prime }$ is a mathematical puzzle task that challenges an LLM’s numerical reasoning capabilities. The game involves using the combination of addition, subtraction, multiplication, and division to manipulate four integers between 1 and 9 to arrive at the final result of 24. The rules are straightforward: players must use all four numbers exactly once, and the order of operations can vary as needed to solve the puzzle. This task is a benchmark for LLM mathematical reasoning (Kim et al. 2023; Ding et al. 2023), as it requires arithmetic operations and strategic planning abilities to explore various combinations and operation sequences for the optimal solution.\n\nTable 2: Success rates on Game of 24.   \n\n<html><body><table><tr><td>Method</td><td>Success (%)</td></tr><tr><td>10</td><td>7.6</td></tr><tr><td>CoT</td><td>5.1</td></tr><tr><td>CoT-SC</td><td>11.2</td></tr><tr><td>ToT</td><td>14.0</td></tr><tr><td>RAT</td><td>9.0</td></tr><tr><td>RATT-3</td><td>25.9</td></tr><tr><td>RATT</td><td>31.8</td></tr><tr><td>Improvement</td><td>24.2</td></tr></table></body></html>\n\nWe test the models on 100 randomly selected games from 4nums 5. For every test, if the model produces a valid equation that results in 24, we mark it as a success. From Table 2, we can see that the RATT achieves the highest success rate of $3 1 . 8 \\%$ , which significantly outperforms the GPT-3.5 Turbo model by $2 4 . 2 \\%$ of improvement. Here the success of RATT-3 is a success answer of 24 in 3 steps of generation, and RATT in 5. IO, CoT, and CoT-SC can hardly help the model find a solution, while RAT can hardly bring\n\n# Question: Introduce Jin-Yong's Life.\n\nFor agent in range(num_agent) Query: Question $^ +$ ”Try to answer this question/instruction with step-by-step thoughts and make the answer more structural.”\n\n# Answer cut into Chunks\n\n![](images/d6ae992eebdfc5cc196928d089840bcfaf731d13aaa901c7aca82c30f3f8fe9b.jpg)\n\n# For each chunk:\n\n![](images/4322751833c7e8126db2c8bf5c306517fee6db385fc74b5e23cf0fe87bb4fd85.jpg)\n\n# LLM Query\n\nPrompt：”Please summarize the content with the corresponding question.The query should be short but need to be specific to promise we can find related knowledge or pages.”\n\nC\n\n# Raw Document\n\nFor each Document:\n\nPrompt：”Please read the following text and extract only the sections that are relevant to the given question. Organize the extracted information coherently maintaining the structure of multiple paragraphs with subtitles”\n\nC\n\nAfter obtaining a set of modified chunks, directly concatenate them in the original order as the final answer for this Agent.\n\n# For each Chunk:\n\n![](images/dd83b53fc9d83c549009a7d452b58d9b332bc014e841a8c325fc169766a39bab.jpg)\n\nPrompt：”{Texture from RAG} {Original Answer Script} I want to revise the answer according to retrieved related text of the question in WIKI pages. You need to check whether the answer is correct. If you find some errors in the answer, revise the answer to make it better. If you find some necessary details are ignored, add it to make the answer more plausible according to the related text. If you find that a part of the answer is correct and does not require any additional details, maintain that part of the answer unchanged. Directly output the original content of that part without any modifications. \\*\\*IMPORTANT\\*\\* Try to keep the structure (multiple paragraphs with its subtitles) in the revised answer and make it more structual for understanding.”\n\nC\n\nAfter obtaining all the answers from the Agents, the LLM merges each answer into a more refined final answer.\n\n# For all answers from agents:\n\nPrompt：”Referencing the answers provided by all agents, synthesize a more detailed and comprehensive response by integrating all relevant details from these answers. Ensure logical coherence and provide ONLY THE MERGED ANSWER AS THE OUTPUT.”\n\nCombine the merged answer with the question to form the next round's prompt.\n\n# For Next Round:\n\nPrompt：”Base your response on the provided question and the previous answer. Expand the answer by adding more details to enhance its comprehensiveness. Question: {question} Previous Answer: {previous_answer}”\n\nRepeat the above process for n rounds to obtain the final answer.\n\n![](images/4f9ebc93b8f1737fab69fc927596ab9c9c608928a3cea17ba2d63a75b31f71ca.jpg)  \nFigure 6: Prompting case of task Creative Writing.   \nFigure 7: Comparison of hallucination metrics of different methods.\n\nadvantages for LLM to win. These results underline our method’s reasoning capability to enhance the numerical reasoning ability.\n\n# Conclusion\n\nIn this paper, we introduce RATT, a novel thought tree structure that integrates both factual correctness and strategic coherence at every step of the reasoning process. This structure significantly enhances LLMs’ logical coherence and decision-making efficiency in complex reasoning tasks by solving the existing problem of thought structures logically and globally. Our extensive experiments across various tasks demonstrate the effectiveness and superiority over existing methods, especially in improving factual correctness and logical coherence. By adopting RATT on LLMs, we extend the boundaries of abilities that LLM performs in handling complex tasks. For future work, we plan to extend this LLM thought structure paradigm across various reasoning and LLM alignment tasks, and to conduct a thorough investigation into the generation strategies that our approach applies to the thought structure and solution space to study further insights into its underlying mechanisms and impacts.\n\nWhile RATT shows significant advancements and wide adaptability, there are several limitations that require further exploration, including high computational demands and limited scalability with very complex tasks or large external libraries. Second, the effectiveness of the reasoning process and final output heavily relies on the quality of the task prompt and the external library, which can affect the model’s performance in scenarios with poorly organized prompts or limited external knowledge. Finally, adopting RATT in tasks with unique requirements may have inherent limitations.\n\n# Acknowledgments\n\nThe author Xiting Wang was supported by the National Natural Science Foundation of China (NSFC) (NO. 62476279), Major Innovation & Planning Interdisciplinary Platform for the “Double-First Class” Initiative, Renmin University of China, and the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China No. 24XNKJ18.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   大型语言模型（LLMs）在复杂任务中的推理和决策能力受到现有思维结构（如Tree of Thought和Retrieval Augmented Thoughts）的限制，这些方法在局部事实检索和全局策略选择上表现不足，难以平衡事实准确性和逻辑优化。\\n> *   这一问题在需要高可靠性和逻辑一致性的复杂推理任务中尤为关键，如数学推理、代码生成和创意写作。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种名为Retrieval Augmented Thought Tree (RATT)的新型思维结构，通过在推理过程的每一步结合事实检查和全局策略优化，显著提升了LLMs的逻辑一致性和决策效率。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 提出RATT结构，结合RAG的事实检查能力和LLMs的全局策略评估能力，在推理过程中动态优化思维树。\\n>   *   **效果：** 在代码生成任务（HumanEval）中，pass@1准确率提升至69.70%，比最佳基线（RAT）提升10.43个百分点。\\n> *   **创新贡献点2：** 设计了一种分层检索策略，从高层概念到具体细节逐步优化节点信息。\\n>   *   **效果：** 在创意写作任务中，综合评分（Overall）达到8.5，显著优于基线方法。\\n> *   **创新贡献点3：** 在Game of 24任务中，RATT的成功率达到31.8%，比最佳基线（ToT）提升17.8个百分点。\\n> *   **创新贡献点4：** 在Hallucination Detection任务中，RATT显著减少了幻觉现象，提升了生成内容的真实性。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   RATT的核心思想是在推理过程的每一步同时进行局部事实检查和全局策略优化，通过动态调整思维树结构，确保逻辑一致性和事实准确性。\\n> *   该方法有效的原因在于其结合了RAG的实时事实纠正能力和LLMs的全局策略评估能力，避免了错误累积和局部最优解问题。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有方法（如CoT和ToT）缺乏有效的事实检查机制，而RAT虽然引入了RAG，但缺乏全局策略优化能力。\\n> *   **本文的改进：** RATT通过分层检索和动态节点优化，实现了局部和全局的协同优化，显著提升了推理的可靠性和效率。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **Thought Node Generation：** 根据任务提示生成多个初始思维节点，覆盖不同的逻辑策略。\\n> 2.  **Retrieval and Selection：** 对每个节点生成查询，从外部库中检索相关文档，并基于余弦相似度选择最相关的文档。\\n> 3.  **RAG Correction and Integration：** 将检索到的信息整合到节点中，生成优化后的节点。\\n> 4.  **Iterative Refinement：** 重复上述步骤，直到达到最大迭代次数或满足终止条件。\\n\\n> **案例解析 (Case Study)**\\n> *   论文中提供了一个关于“猫如何捕鱼”的案例，展示了RATT如何通过分层检索和动态优化纠正初始推理中的错误，并生成逻辑一致且事实准确的最终答案。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   IO、CoT、CoT-SC、ToT、RAT\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在代码生成（pass@1）上：** 本文方法在HumanEval上达到了69.70%，显著优于基线模型RAT（59.27%）和ToT（49.36%）。与表现最佳的基线相比，提升了10.43个百分点。\\n> *   **在创意写作（Overall评分）上：** 本文方法的综合评分为8.5，远高于基线模型RAT（7.2）和ToT（6.8）。\\n> *   **在Game of 24（成功率）上：** 本文方法的成功率为31.8%，显著优于基线模型ToT（14.0%）和RAT（9.0%）。与表现最佳的基线相比，提升了17.8个百分点。\\n> *   **在Hallucination Detection（真实性）上：** 本文方法在TruthfulQA数据集上的表现显著优于基线模型，减少了幻觉现象。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   大型语言模型 (Large Language Model, LLM)\\n*   检索增强生成 (Retrieval-Augmented Generation, RAG)\\n*   思维树 (Thought Tree, N/A)\\n*   逻辑一致性 (Logical Coherence, N/A)\\n*   事实准确性 (Factual Correctness, N/A)\\n*   代码生成 (Code Generation, N/A)\\n*   创意写作 (Creative Writing, N/A)\\n*   推理优化 (Reasoning Optimization, N/A)\"\n}\n```"
}