{
    "link": "https://arxiv.org/abs/2412.11754",
    "pdf_link": "https://arxiv.org/pdf/2412.11754",
    "title": "Formal Quality Measures for Predictors in Markov Decision Processes",
    "authors": [
        "Christel Baier",
        "Sascha Klüppelholz",
        "Jakob Piribauer",
        "Robin Ziemek"
    ],
    "publication_date": "2024-12-16",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "paper_content": "# Formal Quality Measures for Predictors in Markov Decision Processes\n\nChistel Baier1, Sascha Klu¨ ppelholz1 Jakob Piribauer1,2, Robin Ziemek1 \\*\n\n1Technische Universit¨at Dresden 2Universita¨t Leipzig {christel.baier, sascha.klueppelholz, jakob.piribauer, robin.ziemek}@tu-dresden.de\n\n# Abstract\n\nIn adaptive systems, predictors are used to anticipate changes in the system’s state or behavior that may require system adaption, e.g., changing its configuration or adjusting resource allocation. Therefore, the quality of predictors is crucial for the overall reliability and performance of the system under control. This paper studies predictors in systems exhibiting probabilistic and non-deterministic behavior modeled as Markov decision processes (MDPs). Main contributions are the introduction of quantitative notions that measure the effectiveness of predictors in terms of their average capability to predict the occurrence of failures or other undesired system behaviors. The average is taken over all memoryless policies. We study two classes of such notions. One class is inspired by concepts that have been introduced in statistical analysis to explain the impact of features on the decisions of binary classifiers (such as precision, recall, f-score). Second, we study a measure that borrows ideas from recent work on probability-raising causality in MDPs and determines the quality of a predictor by the fraction of memoryless policies under which (the set of states in) the predictor is a probabilityraising cause for the considered failure scenario.\n\n# Extended version — https://arxiv.org/abs/2412.11754\n\n# 1 Introduction\n\nIn modern days, AI systems grow ever more complex and harder to understand, e.g. code designed by artificial intelligence tends to be very abstruse and thus is not comprehensible in a simple way. Since a full understanding of such systems is difficult to establish, it is important to predict certain events within such systems. In particular, situations in which the system produces unwanted or even disastrous results need to be predicted early and precisely.\n\nIn the area of formal verification, counterexamples, invariants and related certificates are often used to provide a verifiable justification that a system does or does not behave according to a specification (see e.g., (Manna and Pnueli 1995; Clarke, Grumberg, and Peled 1999; Namjoshi 2001)). However, most AI systems can not be designed in a way that failure can be excluded and then certificates do not provide enough insights on the systems decisions to predict its behavior. In order to get an understanding why a system behaves the way it does, we introduce measures in how well certain events in a system serve as a predictor for undesired outcomes. Events which have a cause-effect relation to such outcomes constitute a special case of such predictors (Halpern and Pearl 2005; Pearl 2009).\n\n![](images/f04f58d6d18f7c1580a7b6743539b39879d04b581427d94228daf61392b3956e.jpg)  \nFigure 1: An experiment where we want to predict Shatter\n\nIn this paper, we consider binary predictors in Markov decision processes (MDPs) which are a stochastic operational model with non-deterministic choices. We interpret the nondeterminism as uncertainty about the future behavior and thus it may or may not be resolved adversarial to our goals.\n\nFor example, consider an experiment with two participating persons “Suzy” and “Billy” which are asked to throw a rock at a bottle of glass. This example has been widely discussed for in philosophic literature on causality (Hall 2004; Chockler and Halpern 2004; Halpern 2015). In our variant (Fig. 1) a randomized process decides, which person is allowed to take a throw or whether the experiment ends. However, the participants can choose to wait w or to throw t. If someone decides to throw their rock, the state is changed accordingly (ST for “Suzy throws” and BT for “Billy throws”). For predictions in this system, we do not make any assumptions on the decisions of the participants and consider both the outcomes and predictors described by sets of states. For example, reaching the state ST is intuitively a good predictor since the throw of Suzy has a high probability of hitting. However, if Suzy does not feel confident and thus has a decides to throw with a low probability, then state $S T$ will only be reached with low probability and so a lot of scenarios in which the bottle shatters come from Billys throw. In order to distinguish between the quality of different predictors, we use measures from statistical analysis (Powers 2011). However, the quality of a prediction may rely on the distribution over the decisions, as e.g. between w and t for Suzy. So, we consider an average case scenario with respect to the nondeterminism for the quality of a prediction.\n\nFurthermore, we also consider whether a chosen predictor has a cause-effect relation to the undesired outcome. Such a probabilistic cause-effect relation in MDPs is introduced in (Baier, Piribauer, and Ziemek 2024) where the probabilityraising (PR) principle is invoked for each possible resolution of the non-determinism. Inspired by this, we introduce probability-raising policies, which witness a PR condition. A predictor can then also be rated by the relative amount of resolutions (the causal volume) in which it has a probabilistic cause-effect relation with the predicted event.\n\nContributions By considering binary predictors in MDPs we formally introduce an average case analysis for quality measures depending on the non-determinism in order to rate the quality of a predictor (Sec. 3). For this we use a uniform measure over the memoryless randomized policies. We also introduce the concept of probability-raising policies (Sec. 4), which we use to define causal volumes for predictors in MDPs as an additional way to get information about the quality of a predictor (Sec. 4.1). We then address the complexity of deciding the existence of PR policies (Sec. 4.2).\n\nRelated Work Considering the quality of a prediction has connections to responsibility (Chockler and Halpern 2004; Chockler, Halpern, and Kupferman 2008), blameworthiness (Halpern and Kleiman-Weiner 2018) and harm (Beckers, Chockler, and Halpern 2023). In (Mascle et al. 2021) and (Baier, Funke, and Majumdar 2021) forward responsibilities based on the Shapley value are allocated in Kripke and game structures. Recent work considers backwards responsibility in deterministic AI systems (Baier et al. 2024).\n\nAll these notions of responsibility are based on causality as a necessary condition (Braham and van Hees 2012). A causality based account of responsibility can be found in (Chockler, Halpern, and Kupferman 2008). In stochastic operational models, probabilistic causes are used as predictors in (Ziemek et al. 2022) and interpreted as binary classifiers in (Baier, Piribauer, and Ziemek 2024).\n\nPredicting events in Markovian models also has connections to monitoring properties. The fact that randomization improves monitors for non-probabilistic systems has been examined in (Chadha, Sistla, and Viswanathan 2009). A current risk-value is estimated for states in partially observable MDPs in (Junges, Torfah, and Seshia 2021).\n\nin an MDP $\\mathcal { M }$ is a (finite or infinite) alternating sequence $\\pi = s _ { 0 } \\alpha _ { 0 } s _ { 1 } \\alpha _ { 1 } s _ { 2 } \\cdot \\cdot \\cdot \\in ( S \\times A c t ) ^ { \\ast } \\cup ( S \\times A c t ) ^ { \\mathrm { { u } } }$ such that $\\mathsf { P } ( s _ { \\mathrm { i } } , \\alpha _ { \\mathrm { i } } , s _ { \\mathrm { i + 1 } } ) > 0$ for all indices i. A path is called maximal if it is infinite or finite and ends in a terminal state. An MDP can be seen as a Kripke structure in which transitions go from states to probability distributions over states.\n\nA (randomized) policy $\\mathfrak { S }$ is a function that maps each finite non-maximal path $s _ { 0 } \\alpha _ { 0 } \\ldots \\alpha _ { \\mathfrak { n } - 1 } s _ { \\mathfrak { n } }$ to a distribution over $A c t ( s _ { \\mathrm { n } } )$ . S is called deterministic if ${ \\mathfrak { S } } ( \\pi )$ is a Dirac distribution for all finite non-maximal paths $\\pi$ . If the chosen action only depends on the last state of the path, $\\mathfrak { S }$ is called memoryless. We write MR for the class of memoryless (randomized) and MD for the class of memoryless deterministic policies. Finite-memory policies are those that are representable by a finite-state automaton.\n\nA policy $\\mathfrak { S }$ of $\\mathcal { M }$ induces a (possibly infinite) Markov chain. We write $\\mathrm { P r } _ { \\mathcal { M } , s } ^ { \\mathfrak { S } }$ for the standard probability measure on measurable sets of maximal paths in the Markov chain induced by $\\mathfrak { S }$ with initial state s. We use the abbreviation $\\mathrm { P r } _ { \\mathcal { M } } ^ { \\mathfrak { S } } = \\dot { \\mathrm { P r } } _ { \\mathcal { M } , i n i t } ^ { \\mathfrak { S } }$ . We use linear temporal logic (LTL) modalities such as $\\diamondsuit$ (eventually) and U (until) to denote path properties. For $x , \\mathsf { T } \\subseteq S$ the formula $\\mathsf { X U T }$ is satisfied by $\\pi = s _ { 0 } s _ { 1 } . . .$ if there is ${ \\mathrm { ~ j ~ } } \\geqslant { \\mathrm { ~ 0 ~ } }$ such that for all $\\mathfrak { i } < \\mathfrak { j } : s _ { \\mathrm { i } } \\in X$ and $s _ { \\mathrm { j } } \\in \\mathsf { T }$ and $\\diamond \\mathsf { T } = \\mathsf { S U T }$ . It is well-known that $\\mathrm { P r } _ { \\mathcal { M } } ^ { \\operatorname* { m i n } } ( X \\mathrm { U } \\mathsf { T } )$ and $\\operatorname* { P r } _ { \\mathcal { M } } ^ { \\operatorname* { m a x } } ( X \\mathrm { U } \\mathsf { T } )$ and corresponding optimal MD-policies are computable in polynomial time.\n\nFor $s \\in S$ and $\\alpha \\in A c t ( s )$ , $( s , \\alpha )$ is a state-action pair of M. We denote the set of state-action pairs of $\\mathcal { M }$ by StAct. An end component (EC) of an MDP $\\mathcal { M }$ is a strongly connected sub-MDP containing at least one state-action pair.\n\nIn the context of this work a Markov decision process (MDP) is a 4-tuple $\\boldsymbol { \\mathcal { M } } = ( \\mathsf { S } , A c t , \\mathsf { P } , i n i t )$ where S is a finite set of states, $A c t$ a finite set of actions, init $\\in S$ the initial state and $\\mathsf { P } : \\mathsf { S } \\times A c t \\times \\mathsf { S } \\to [ 0 , 1 ]$ the probabilistic transition function such that $\\sum _ { \\mathrm { t } \\in S } \\mathsf { P } ( s , \\alpha , \\mathrm { t } ) \\in \\{ 0 , 1 \\}$ for all states $s \\in S$ and actions $\\alpha \\in A c t$ . An action $\\alpha$ is enabled in state $s \\in S$ if $\\begin{array} { r } { \\sum _ { \\mathrm { t } \\in S } \\mathsf { P } ( s , \\alpha , \\mathrm { t } ) = 1 } \\end{array}$ and $A c t ( s )$ denotes the set of enabled actions in S. A state t is terminal if $A c t ( \\mathrm { t } ) ~ = ~ \\varnothing$ . A path\n\nFor a policy $\\mathfrak { S }$ of $\\mathcal { M }$ , the expected frequencies of stateaction pairs $( s , \\alpha )$ are $f r e q _ { \\mathfrak { S } } ( s , \\alpha ) = \\mathrm { E } _ { \\mathfrak { M } } ^ { \\mathfrak { S } } ($ (no. of visits to s in which $\\alpha$ is taken) In end-component free MDPs we can specify MR policies by their state-action frequencies (see e.g. (Kallenberg 2020, Theorem 4.7)), by considering a linear constraint system over variables $x _ { s , \\alpha }$ for each $( s , \\alpha ) \\in S t A c t$ :\n\n$$\n\\begin{array} { l } { \\displaystyle x _ { s , \\alpha } \\geqslant 0 \\qquad \\mathrm { f o r ~ a l l ~ } ( s , \\alpha ) \\in S t A c t , } \\\\ { \\displaystyle x _ { i n i t } = 1 + \\sum _ { ( \\mathbf { t } , \\alpha ) \\in S t A c t } \\cdot \\mathbf { P } ( \\mathbf { t } , \\alpha , i n i t ) , } \\\\ { \\displaystyle x _ { s } = \\sum _ { ( \\mathbf { t } , \\alpha ) \\in S t A c t } \\cdot \\mathbf { P } ( \\mathbf { t } , \\alpha , s ) \\mathrm { f o r ~ a l l ~ } s \\in S \\setminus \\{ i n i t \\} , } \\end{array}\n$$\n\nwhere we use the short form notation xs = α Act(s) xs,α. By (Kallenberg 2020, Theorem 4.7) a solut on $\\boldsymbol { x } \\in \\mathbb { R } ^ { S t A c t }$ to (S1)-(S3) corresponds one-to-one to an MR policy $\\mathfrak { S }$ for $\\mathcal { M }$ such that $x _ { s , \\alpha } = f r e q _ { \\mathfrak { S } } ( s , \\alpha )$ for all $( s , \\alpha ) \\in S t A c t$ .\n\n# 2 Preliminaries\n\n# 3 Measuring the Quality of a Predictor\n\nThe goal of this section is to measure how well reaching a set of states C predicts that a set of terminal states $\\mathsf { E }$ will be reached in an MDP M. For this, we consider well-known measures for binary classifiers from statistical analysis. To apply such measures, the non-determinism in $\\mathcal { M }$ needs to be resolved. However, without any assumptions about the resolution of the non-determinism, we propose to consider an average over possible resolutions of the non-determinism.\n\n![](images/45383d31f2207e3fab271ecb5e57211302ec1c3c060332d1dc5cfd8c9847468e.jpg)  \nFigure 2: An MDP $\\mathcal { M }$ depicting a communication network.\n\n# 3.1 Averaging over Policies\n\nTo be able to compute this average, one has to choose which policies to consider and how to weigh them. In this paper, we use a uniform measure over MR policies. Since we consider reachability properties in particular, the choice of MR policies is justified, as any policy in an EC-free MDP for these properties can be approximated by MR policies (Kallenberg 2020). Furthermore, non-determinism can be used to model uncertainty in the transition probabilities as also represented in interval-Markov chains (see, e.g., (Kozine and Utkin 2002; Sen, Viswanathan, and Agha 2006)). In this case, actions can be used to model the extremal transition probabilities. Randomizing over such actions then allows for any possible concrete distribution over successor states.\n\nExample 1. As an example, consider a communication network in which a message is sent by a sender to a receiver via various network nodes. Each node forwards the message to other nodes in a randomized fashion. We only know the successors of each network node as well as upper and lower bounds on the respective probability and on the probability that the message is lost between two nodes. Suppose, we have the suspicion that some sets of nodes are faulty and are the reason for many message losses. In order to check this claim, we want to measure how well reaching such a set of nodes serves as a predictor for a message loss by considering the average over possible resolutions of the non-determinism. A very simple example of such a network where actions model the known probability bounds is given in Figure 2. ◁\n\nGiven an MDP $\\begin{array} { r } { \\mathfrak { M } = ( { \\sf S } , { \\cal A } c t , { \\sf P } , i n i t ) } \\end{array}$ with terminal states ${ \\mathsf { T } } \\subseteq S$ , the set of all MR policies $\\mathfrak { S }$ of $\\mathcal { M }$ can be described by the set of vectors\n\n$$\n\\mathfrak { P } = \\{ x \\in [ 0 , 1 ] ^ { S t A c t } | \\sum _ { \\alpha \\in A c t ( s ) } x _ { s , \\alpha } = 1 \\quad \\mathrm { f o r ~ a l l } \\ s \\in S \\setminus \\mathsf { T } \\} .\n$$\n\nThe component $x _ { s , \\alpha }$ of $x \\in { \\mathfrak { P } }$ expresses the probability that the corresponding policy chooses $\\alpha$ in s. $\\mathfrak { P }$ is a product of $| S \\setminus \\mathsf { T } |$ -many regular simplices. For $s \\in S \\backslash \\mathsf { T }$ let $\\mathbf { n } ( s ) =$ $| A c t ( s ) | - 1$ . Then $\\mathfrak { P }$ can be identified with $\\Pi _ { s \\in S } \\Delta ^ { { \\mathfrak { n } } ( s ) }$ , where $\\Delta ^ { \\mathrm { m } } = \\{ \\mathfrak { y } \\in [ 0 , 1 ] ^ { \\mathrm { m + 1 } } | \\sum _ { \\mathrm { i = 1 } } ^ { \\mathrm { m + 1 } } \\mathfrak { y } _ { \\mathrm { i } } = 1 \\}$ is the regular\n\n$$\n\\begin{array} { r } { \\frac { \\mathrm { \\iint } \\mathrm { ~ } \\left( \\mathrm { ~ ` ~ ` ~ ` ~ ` ~ ` ~ ` ~ ` ~ ` ~ } \\right) } { \\mathrm { \\normalfont ~ \" ~ ` ~ \" ~ \" ~ \" ~ \" ~ \" ~ \" ~ } } \\frac { \\mathrm { \\partial } \\cdot \\langle \\langle \\langle \\boldsymbol { \\mathsf { S } } \\boldsymbol { E } \\mathrm { ~ \\tiny ~ . ~ } \\rangle } { \\mathrm { \\partial } \\mathrm { ~ \" ~ \" ~ \" ~ } } } \\\\ { \\frac { \\mathrm { \\iint } \\mathrm { ~ } t p _ { \\mathcal { \\mathrm { { M } } } } ^ { \\mathfrak { S } } = \\mathrm { P r } ^ { \\mathfrak { S } } ( \\langle \\boldsymbol { \\mathsf { S } } \\boldsymbol { C } \\wedge \\langle \\boldsymbol { \\mathsf { S } } \\boldsymbol { E } \\rangle \\mathrm { \\tiny ~ . ~ } \\mathrm { \\iint } p _ { \\mathcal { \\mathrm { { M } } } } ^ { \\mathfrak { S } } = \\mathrm { P r } ^ { \\mathfrak { S } } ( \\langle \\boldsymbol { \\mathsf { S } } \\boldsymbol { C } \\wedge \\ { \\neg } \\rangle E ) } { \\mathrm { \\partial } \\mathrm { ~ \" ~ \" ~ \" ~ \" ~ } } } \\\\ { \\mathrm { \\normalfont ~ \" ~ \" ~ \" ~ \" ~ \" ~ \" ~ \" ~ \" ~ \" ~ \" ~ \" ~ } } \\\\ { \\frac { \\mathrm { \\partial } \\mathrm { \\Pi } \\mathrm { \\iint } n _ { \\mathcal { { M } } } ^ { \\mathfrak { S } } = \\mathrm { P r } ^ { \\mathfrak { S } } ( \\mathrm { - } \\langle \\boldsymbol { \\mathsf { S } } \\boldsymbol { C } \\wedge \\langle \\boldsymbol { \\mathsf { S } } E \\rangle \\mathrm { \\tiny ~ . ~ } \\mathrm { \\partial } \\mathrm { ~ \" ~ \" ~ } } { \\mathrm { \\partial } \\mathrm { ~ \" ~ \" ~ \" ~ } } } \\end{array}\n$$\n\nFigure 3: The confusion matrix for the prediction of $E \\subseteq S$ by $C \\subseteq S$ for a given policy $\\mathfrak { S }$ .\n\nm-simplex of dimension $\\mathfrak { m }$ in $\\mathbb { R } ^ { \\mathrm { m + 1 } }$ for given $\\mathfrak { m } \\in \\mathbb { N }$ . In turn, the dimension of $\\mathfrak { P }$ is $| S t A c t | - | S \\setminus \\mathsf { T } |$ .\n\nExample 2. For the MDP M depicted in Fig. 2 considering the state-action pairs $( i n i t , \\tau )$ , $( \\mathsf { A } , \\alpha )$ , $( \\boldsymbol { A } , \\boldsymbol { \\gamma } )$ , $( \\mathtt { B } , \\beta )$ , $( \\mathtt { B } , \\delta )$ we get $\\mathfrak { P } = \\{ ( 1 , \\mathfrak { p } , 1 { - } \\mathfrak { p } , \\mathfrak { q } , 1 { - } \\mathfrak { q } ) \\ | \\ \\mathfrak { p } , \\mathfrak { q } \\ \\in \\ [ 0 , 1 ]$ . We can identify $\\mathfrak { P }$ with the product of simplices $\\{ 1 \\} \\times [ 0 , 1 ] \\times [ 0 , 1 ]$ or simply $[ 0 , 1 ] ^ { 2 }$ . ◁\n\nUsing the Lebesgue measure, we get a way to uniformly average over all MR policies. As mentioned above, we want to apply this to some quality measure for binary classifiers. For a quality measure $Q ^ { \\mathrm { x } } ( C )$ for the predictor $C \\subseteq S$ depending on a policy ${ \\mathfrak { x } } \\in { \\mathfrak { P } }$ , the average of the quality measure is\n\n$$\n\\int _ { \\mathfrak { P } } \\mathrm { Q } ^ { \\mathrm { x } } ( \\mathrm { C } ) \\mathrm { d } \\mathrm { x } / \\int _ { \\mathfrak { P } } 1 \\mathrm { d } \\mathrm { x } .\n$$\n\nThe computation of $\\begin{array} { r } { V ( \\mathfrak { P } ) = \\int _ { \\mathfrak { P } } 1 \\mathrm { d } \\mathfrak { x } } \\end{array}$ can be done in polynomial time since $\\mathfrak { P }$ is the product of standard simplices as described above. Then, by (Stein 1966) we have\n\n$$\n\\int _ { \\mathfrak { P } } 1 \\mathrm { d } x = \\mathsf { V } ( \\prod _ { s \\in S } \\Delta ^ { \\mathsf { n } ( s ) } ) = \\prod _ { s \\in S } \\mathsf { V } ( \\Delta ^ { \\mathsf { n } ( s ) } ) = \\prod _ { s \\in S } \\frac { 1 } { \\mathfrak { n } ( s ) ! } ,\n$$\n\nwhich is computable in polynomial time. In fact, integrating any polynomial of fixed degree over $\\mathfrak { P }$ can also be done in polynomial time (Baldoni et al. 2011).\n\n# 3.2 Quality Measures\n\nGiven a single policy $\\mathfrak { S }$ for $\\mathcal { M }$ , we consider the so-called confusion matrix (Powers 2011) for binary classifiers as depicted in Figure 3. In statistical analysis, various measures for the quality of a classifier using the entries of the confusion matrix have been studied. Important measures are\n\n$$\n\\begin{array} { l } { { p r e c i s i o n ^ { \\mathfrak { S } } ( C ) = \\displaystyle \\frac { t p ^ { \\mathfrak { S } } } { t p ^ { \\mathfrak { S } } + f p ^ { \\mathfrak { S } } } , ~ r e c a l l ^ { \\mathfrak { S } } ( C ) = \\displaystyle \\frac { t p ^ { \\mathfrak { S } } } { t p ^ { \\mathfrak { S } } + f n ^ { \\mathfrak { S } } } , } } \\\\ { { f s c o r e ^ { \\mathfrak { S } } ( C ) = \\displaystyle \\frac { 2 } { \\frac { 1 } { p r e c i s i o n ^ { \\mathfrak { S } } ( C ) } + \\frac { 1 } { r e c a l l ^ { \\mathfrak { S } } ( C ) } } = \\displaystyle \\frac { 2 t p ^ { \\mathfrak { S } } } { 2 t p ^ { \\mathfrak { S } } + f p ^ { \\mathfrak { S } } + f n ^ { \\mathfrak { S } } } . } } \\end{array}\n$$\n\nIntuitively, the precision $^ { \\mathfrak { s } } ( C )$ measures the probability that the prediction is indeed true after $C$ is reached. The $r e c a l l ^ { \\mathfrak { S } } ( C )$ , on the other hand, expresses the probability that reaching $E$ was preceded by reaching $C$ under the condition that $E$ has indeed been reached. The fscore $^ { \\mathfrak { s } } ( C )$ is the harmonic mean of precision and recall. A high f-score hence indicates that after reaching the predictor $C$ , the probability to reach $E$ is relatively high and at the same time relatively many executions leading to $E$ pass through $C$ before. An example for a more complex quality measures – similar in spirit to the f-score (Chicco and Jurman 2020) – is Matthews correlation coefficient given by $\\mathrm { m c c } ^ { \\mathfrak { S } } ( C ) =$\n\n$$\n\\frac { t p ^ { \\mathfrak { S } } \\cdot t n ^ { \\mathfrak { S } } - f p ^ { \\mathfrak { S } } \\cdot f n ^ { \\mathfrak { S } } } { \\sqrt { ( t p ^ { \\mathfrak { S } } + f p ^ { \\mathfrak { S } } ) \\cdot ( t p ^ { \\mathfrak { S } } + f n ^ { \\mathfrak { S } } ) \\cdot ( t n ^ { \\mathfrak { S } } + f p ^ { \\mathfrak { S } } ) \\cdot ( t n ^ { \\mathfrak { S } } + f n ^ { \\mathfrak { S } } ) } } .\n$$\n\nComputing average quality measures. In order to compute the average quality measures over the polytope $\\mathfrak { P }$ of MR policies, we have to express the measures in terms of the policies. As the considered quality measures depend on the confusion matrix, the task hence is to express the entries of the confusion matrix in terms of the vectors $x \\in { \\mathfrak { P } }$\n\nTo express these values, we take a small detour via a model transformation such that we can distinguish whether $C$ has occurred or not. We define the two-copy MDP $\\Re _ { C }$ of $\\mathcal { M }$ with respect to $C$ in the following way:\n\n• $\\Re _ { C }$ consists of two copies of $\\mathcal { M }$ , namely $\\mathcal { M } _ { 0 }$ and $\\mathcal { M } _ { 1 }$ , • the initial state is $i n i t _ { 0 }$ of the first copy, • whenever a state $\\mathbf { c } _ { 0 } \\in C _ { 0 }$ is reached in the first copy there is exactly one action which transitions to the corresponding state $c _ { 1 }$ in the second copy with probability 1.\n\nA policy $\\mathfrak { S }$ of the original MDP $\\mathcal { M }$ can be interpreted as a policy $\\mathfrak { U }$ of the two-copy MDP $\\Re _ { C }$ by mimicking the behavior in both copies ignoring the switch to the second copy. We denote the copies of the states in $C$ and $E$ in $\\mathbf { \\mathcal { M } } _ { 0 }$ and $\\mathcal { M } _ { 1 }$ by $C _ { 0 } , C _ { 1 } , E _ { 0 }$ , and $E _ { 1 }$ , respectively. Now, we can describe the entries of the confusion matrix for $\\mathcal { M }$ directly by reachability probabilities in $\\mathbf { \\mathcal { M } } _ { C }$ .\n\nLemma 3. For an MR policy $\\mathfrak { S }$ of $\\mathcal { M }$ also viewed as a policy for $\\Re _ { C }$ the following holds\n\n$$\n\\begin{array} { r l } & { t p _ { \\mathbb { \\Re } } ^ { \\mathbb { S } } = { \\bf P r } _ { \\mathbb { \\Re } _ { C } } ^ { \\mathfrak { S } } ( \\diamondsuit E _ { 1 } ) , f n _ { \\mathbb { \\Re } } ^ { \\mathbb { S } } = { \\bf P r } _ { \\mathbb { \\Re } _ { C } } ^ { \\mathfrak { S } } ( \\diamondsuit E _ { 0 } ) , } \\\\ & { f p _ { \\mathbb { \\Re } } ^ { \\mathbb { S } } = \\displaystyle \\sum _ { c \\in C _ { 0 } } { \\bf P r } _ { \\mathbb { \\Re } _ { C } } ^ { \\mathfrak { S } } ( \\diamondsuit \\cdot ( 1 - { \\bf P r } _ { \\mathbb { \\Re } _ { C } } ^ { \\mathfrak { S } } ( \\diamondsuit E _ { 1 } ) ) , } \\\\ & { t n _ { \\mathbb { \\Re } } ^ { \\mathbb { S } } = 1 - t p _ { \\mathbb { \\Re } } ^ { \\mathbb { S } } - f p _ { \\mathbb { \\Re } } ^ { \\mathbb { S } } - f n _ { \\mathbb { \\Re } } ^ { \\mathbb { S } } . } \\end{array}\n$$\n\nProof. Only the equation for $f p _ { \\mathcal { M } } ^ { \\mathfrak { S } }$ must be proven since the other equations follow by construction. The probability that $\\mathbf { c } \\in C _ { 0 }$ is visited and afterwards $\\neg \\diamondsuit E _ { 1 }$ holds is $\\mathrm { P r } _ { \\mathcal { M } _ { C } } ^ { \\mathfrak { S } } ( \\diamond _ { \\mathtt { C } } )$ · $\\big ( 1 { - } \\mathrm { P r } _ { \\mathcal { M } _ { C } , \\mathrm { c } } ^ { \\mathfrak { S } } \\big ( \\big \\langle \\mathscr { D } E _ { 1 } \\big ) \\big \\rangle$ . As the set $C _ { 0 }$ can only be reached once in $\\mathbf { \\mathcal { M } } _ { C }$ , the equation for $f p _ { \\mathrm { \\mathcal { M } } } ^ { \\mathfrak { S } }$ follows by adding the probabilities of these disjoint events. □\n\nWe denote the state space of $\\mathbf { \\mathcal { M } } _ { C }$ by $S ^ { \\prime }$ and the set of terminal states by ${ \\sf T } ^ { \\prime }$ . Now, we describe reachability probabilities between states in $\\Re _ { C }$ in terms of distributions given by MR policies. For $s , \\ t \\in \\ S ^ { \\prime }$ and an MR policy $\\mathfrak { S }$ of $\\mathcal { M }$ let $\\mathbf { a } _ { s , \\mathrm { t } } ^ { \\mathfrak { S } } \\doteq \\mathbf { P r } _ { s } ^ { \\mathfrak { S } } ( \\diamond \\mathrm { t } )$ be the probability to eventually reach t from s under $\\mathfrak { S }$ . So, $\\mathrm { a } _ { s , s } ^ { \\mathfrak { S } } = 1$ for all $s \\in S ^ { \\prime }$ . Using the representation of MR policies as vectors $x \\in { \\mathfrak { P } }$ , we then have for non-terminal states $s \\in S ^ { \\prime } \\setminus \\mathsf { T } ^ { \\prime }$ and $\\mathbf { t } \\neq s$ ,\n\n$$\n\\mathbf { a } _ { s , \\mathrm { t } } ^ { x } = \\sum _ { \\alpha \\in A c t ( s ) } x _ { s , \\alpha } \\cdot \\sum _ { \\mathbf { u } \\in S } \\mathsf { P } ( s , \\alpha , \\mathbf { u } ) \\cdot \\mathbf { a } _ { \\mathrm { u , t } } ^ { x } .\n$$\n\nIn fact, after determining the states $s ^ { \\prime }$ from which $\\mathrm { \\Delta t }$ is not reachable in the Markov chain induced by $x$ and $\\Re _ { C }$ and setting the corresponding variables $\\mathbf { a } _ { s ^ { \\prime } , \\mathrm { t } } ^ { \\mathrm { x } }$ to 0, the equation system has a unique solution (see (Baier and Katoen 2008)). Given a rational-valued MR policy ${ \\boldsymbol { \\mathbf { \\mathit { x } } } } \\in { \\boldsymbol { \\mathfrak { P } } }$ the reachability probabilities can hence be derived in polynomial time by solving the linear equation system.\n\nHowever, we want to compute the average of quality measures by taking an integral over the polytope $\\mathfrak { P }$ of all MR policies. As we use the standard Lebesgue measure, the boundary of this polytope has measure 0. So, for our purpose, it is sufficient to express the quality measures as a function of the policy ${ \\boldsymbol { x } } \\in { \\mathfrak { P } }$ on the interior of $\\mathfrak { P }$ .\n\nProposition 4. The values $\\mathbf { a } _ { s , \\mathrm { t } } ^ { x }$ for $s , \\ t \\in \\ S ^ { \\prime }$ are rational functions in x on the interior of $\\mathfrak { P }$ . The degree of denominator and enumerator of these rational functions $i s \\leqslant 2 |$ S|.\n\nProof. For a fixed $\\mathbf { t } \\in { \\mathsf { S } } ^ { \\prime }$ , the set $\\mathbf { \\lambda } _ { A _ { 0 } }$ of states $s ^ { \\prime }$ from which t is not reachable in the Markov chain induced by $x$ and $\\Re _ { C }$ depends on which entries of $x$ are 0. In the interior of $\\mathfrak { P }$ , no vector has a 0-entry. So, $\\mathbf { \\lambda } _ { A _ { 0 } }$ is independent of $x$ and the corresponding variables can be set 0. As the resulting equation system with at most $| S ^ { \\prime } |$ -many variables has a unique solution (see (Baier and Katoen 2008)), this solution can be expressed using fractions of determinants by Cramer’s rule. The determinants are polynomials in the coefficients of the linear equations of degree at most $| S ^ { \\prime } | = 2 | S |$ and the variables $x _ { s , \\alpha }$ appear only linearly in these coefficients. □\n\nThe polynomials in the resulting rational function contain exponentially many monomials in general. Nevertheless, we know that the values $\\mathbf { a } _ { s , \\mathrm { t } } ^ { x }$ are rational functions in $x$ on the interior of $\\mathfrak { P }$ that only take values between 0 and 1.\n\nPractical considerations for averages. If the transition relation is sparse, which is often the case for models with large state space, results on the efficient computation of symbolic determinants apply (Kaltofen and Villard 2005; Duriqi et al. 2024). Then, an exact representation of the rational functions $\\mathbf { a } _ { s , \\mathrm { t } } ^ { x }$ can be obtained efficiently.\n\nBy Lemma 3, all entries of the confusion matrix under policy ${ \\boldsymbol { \\mathbf { \\mathit { x } } } } \\in { \\boldsymbol { \\mathfrak { P } } }$ can be expressed in terms of $\\mathbf { a } _ { s , \\mathrm { t } } ^ { x }$ by simple arithmetic. Furthermore, the prominent measures for the quality of a predictor such as precision, recall, and f-score are linear rational functions in these entries. But also more complex measures such as the Matthews correlation coefficient are still relatively simple functions in the entries of the confusion matrix. So, all of these quality measures can be expressed as functions $Q ( \\mathfrak { a } ^ { \\mathrm { x } } )$ where $\\mathtt { a } ^ { \\mathrm { x } }$ is the vector containing the values $\\mathbf { a } _ { s , \\mathrm { t } } ^ { x }$ . Unfortunately, an exact evaluation of the integral $\\int _ { \\mathfrak { P } } \\mathrm { Q } ( \\mathfrak { a } ^ { \\mathrm { x } } ) \\mathrm { d } \\mathfrak { x }$ is nevertheless typically out of reach. But the quality measures take values in $[ 0 , 1 ]$ or $[ - 1 , 1 ]$ in the case of the MCC. Together with the fact, that the resulting function $Q ( \\mathfrak { a } ^ { \\mathrm { x } } )$ are smooth functions on the interior of $\\mathfrak { P }$ , standard approaches such as Monte Carlo integration can be used to approximate the integral (Press et al. 2007). Note, that we do not need an explicit representation of the functions $\\mathbf { a } _ { s , \\mathbf { t } } ^ { x }$ when we sample policies $x \\in { \\mathfrak { P } }$ as the values $\\mathbf { a } _ { s , \\mathrm { t } } ^ { x }$ can then be computed in polynomial time.\n\nExample 5. Let us apply average quality measures to the network MDP $\\mathcal { M }$ depicted in Figure 2. First, we consider $\\mathsf { A }$ as the predictor. Here, we identify policies with pairs $( \\mathsf { p } , \\mathsf { q } ) \\in \\mathsf { \\tilde { \\mathrm { [ 0 , 1 ] } } } ^ { 2 }$ as in Example 2 where $\\mathfrak { p }$ is the probability to choose $\\alpha$ in $\\mathsf { A }$ and $\\mathfrak { q }$ the probability to choose $\\beta$ in $\\mathtt { B }$ . For a policy ${ \\mathfrak { x } } = ( { \\mathfrak { p } } , { \\mathfrak { q } } )$ , we get\n\n$$\n\\begin{array} { l } { { t p _ { \\mathbb { M } } ^ { x } = \\displaystyle \\frac { 1 } { 6 } ( 1 + \\mathfrak { p } ) , \\qquad f p _ { \\mathbb { M } } ^ { x } = \\displaystyle \\frac { 1 } { 6 } ( 3 - \\mathfrak { p } ) , } } \\\\ { { t n _ { \\mathbb { M } } ^ { x } = \\displaystyle \\frac { 1 } { 6 } \\mathfrak { q } , \\qquad f n _ { \\mathbb { M } } ^ { x } = \\displaystyle \\frac { 1 } { 6 } ( 2 - \\mathfrak { q } ) . } } \\end{array}\n$$\n\nAs the volume of $\\mathfrak { P }$ is 1 in this case, we obtain the average f-score, for example, as\n\n$$\n\\begin{array} { l } { { \\displaystyle \\int _ { [ 0 , 1 ] ^ { 2 } } f s c o r e ^ { x } ( \\{ A \\} ) \\mathrm { d } x = \\int _ { [ 0 , 1 ] ^ { 2 } } \\frac { 2 t p ^ { \\mathfrak { S } } } { 2 t p ^ { \\mathfrak { S } } + f p ^ { \\mathfrak { S } } + f n ^ { \\mathfrak { S } } } \\mathrm { d } x } } \\\\ { { \\displaystyle = \\int _ { 0 } ^ { 1 } \\int _ { 0 } ^ { 1 } \\frac { 2 + 2 \\mathfrak { p } } { 7 + \\mathfrak { p } - \\mathfrak { q } } \\mathrm { d } \\mathfrak { p } \\mathrm { d } \\mathfrak { q } \\approx 0 . 4 3 . } } \\end{array}\n$$\n\nAnalogously, we obtain\n\n$$\n\\int _ { [ 0 , 1 ] ^ { 2 } } { f s c o r e } ^ { x } ( \\{ \\mathsf { B } \\} ) \\mathrm { d } x = \\int _ { 0 } ^ { 1 } \\int _ { 0 } ^ { 1 } { \\frac { 4 - 2 \\mathsf { q } } { 5 + \\mathsf { p } - \\mathsf { q } } } \\mathrm { d } \\mathsf { p } \\mathrm { d } \\mathsf { q } \\approx 0 . 6 0 .\n$$\n\nSo, according to the f-score, reaching $\\mathtt { B }$ is on average a better predictor for a message loss than reaching $\\boldsymbol { A }$ . ◁\n\nRemark 6. If a quality measure can be written as a linear rational function in terms of the entries of the confusion matrix, then its minimal or maximal value can be computed with the techniques presented in (Baier, Piribauer, and Ziemek 2024). This is e.g. the case for precision, recall and f-score. For this a standard model transformation is performed which collapses end components (de Alfaro 1997, 1999) while preserving relevant reachability probabilities (c.f. (Ciesinski et al. 2008, III.B) for a compact description). Afterwards, possible combinations of reachability probabilities can be expressed in terms of a linear constraint system for state-action pair frequencies. For linear rational quality measures, the resulting optimization problem can be solved in polynomial time. For more complex measures like Matthews correlation coefficient, more general optimization problems arise.\n\n# 4 Probability-Raising Policies and the Causal Volume of a Predictor\n\nIn this section we investigate, whether in an MDP M a given predictor $C$ has a probabilistic cause-effect relation with the undesired event $E$ . The idea that a cause for an event serves as a good predictor comes very natural and the usage of probabilistic causes for predictions has already been considered in (Ziemek et al. 2022) for Markov chains. For MDPs we take inspiration from the notion of probability-raising causes from (Baier, Piribauer, and Ziemek 2024) to define two variants of probability-raising $( P R )$ policies, which are, in simple terms, witnessing a probability-raising condition. Since the focus of this paper is to investigate the quality of a predictor, we introduce a measure which considers the relative amount of possible MR policies that witness a PR condition. Afterwards we study the complexity of deciding the existence of such PR policy for a given predictor.\n\nDefinition 7 (Probability-raising policy). Given M, $E$ and $C$ , a policy $\\mathfrak { S }$ of $\\mathcal { M }$ is a global probability-raising policy (GPR policy) for $C$ and $E$ in $\\mathcal { M }$ if the following conditions ${ \\bf ( R ) }$ and (G) hold\n\n$$\n\\begin{array} { r l } & { \\operatorname* { P r } ^ { \\mathfrak { S } } ( \\diamond C ) > 0 , } \\\\ & { \\operatorname* { P r } ^ { \\mathfrak { S } } ( \\diamond E \\mid \\diamond C ) > \\operatorname* { P r } ^ { \\mathfrak { S } } ( \\diamond E ) . } \\end{array}\n$$\n\n$\\mathfrak { S }$ is a strict probability-raising policy (SPR policy) for $C$ and $E$ in $\\mathcal { M }$ if ${ \\bf ( R ) }$ and the following condition (S) hold\n\n(S) For all $c \\in C$ with $\\mathrm { P r } ^ { \\mathfrak { S } } ( ( \\lnot C ) \\mathrm { U } \\mathrm { c } ) > 0$ :\n\n$$\n\\mathrm { P r } ^ { \\mathfrak { S } } ( \\diamondsuit E \\mid ( \\neg C ) \\mathbf { U } c ) > \\mathrm { P r } ^ { \\mathfrak { S } } ( \\diamondsuit E ) .\n$$\n\nThere is an implication from strict to global, since the conditional probability in (GPR) is a weighted sum over the conditional probabilities in (SPR) (also cf. (Baier, Piribauer, and Ziemek 2024)). Similarly, for singletons $\\{ { \\mathrm { c } } \\}$ the equivalence of $\\diamond \\mathrm { c }$ and $( \\neg { \\mathbf { c } } ) \\mathbf { U } \\mathbf { c }$ means that in such cases the strict and global PR conditions coincide.\n\nUnder a PR policy $\\mathfrak { S }$ there is a causal relationship between the predictor $C$ and the predicted event $E$ . Namely, whenever $C$ is reached under a PR policy $\\mathfrak { S }$ , the probability of reaching $E$ is raised.\n\nExample 8. For an example we consider the network MDP $\\mathcal { M }$ from Figure 2 as before. Here we consider $C = \\{ { \\tt B } \\}$ as a predictor for $E = \\left\\{ l o s t \\right\\}$ . For the MD policy $\\mathfrak { S }$ choosing $\\gamma$ in $\\mathsf { A }$ and $\\beta$ in $\\mathtt { B }$ we have\n\n$$\n\\mathrm { P r } ^ { \\mathfrak { S } } ( \\diamond l o s t | \\diamond \\mathtt { B } ) = \\frac { 1 } { 2 } > \\frac { 1 } { 3 } \\cdot \\frac { 1 } { 2 } + \\frac { 2 } { 3 } \\cdot \\frac { 1 } { 4 } = \\frac { 1 } { 3 } = \\mathrm { P r } ^ { \\mathfrak { S } } ( \\diamond l o s t )\n$$\n\nand thus $\\mathfrak { S }$ satisfies (GPR) (and (SPR)) and constitutes a PR policy. However, there are policies which do not satisfy (GPR), e.g., the MD policy $\\mathfrak { U }$ choosing $\\alpha$ in $\\mathsf { A }$ and $\\beta$ in B. $\\triangleleft$\n\n# 4.1 Causal Volumes\n\nWith the definition of probability-raising policy in mind the question now arises, in how many cases such a causal relation holds between the predictor and the undesired outcome. For this we consider the relative portion of PR policies among all possible MR policies. Recall the polytope $\\mathfrak { P }$ of distributions of MR policies in $\\mathcal { M }$ from Section 3. Using the reachability matrix corresponding to a given policy $x \\in { \\mathfrak { P } }$ from (reach) we express the conditions $\\mathbf { \\eta } ( \\mathbf { R } ) \\mathbf { \\eta } , \\mathbf { \\eta } ( \\mathbf { S } )$ and (G) from Def. 7 by\n\n$$\n\\sum _ { c \\in C } \\mathbf { a } _ { i n i t , c } ^ { * } > 0 ,\n$$\n\n$$\n\\sum _ { e f f \\in E } \\mathbf { a } _ { \\mathsf { c } , e f f } ^ { x } > \\sum _ { e f f \\in E } \\mathbf { a } _ { i n i t , e f f } ^ { x } \\mathrm { f o r ~ a l l ~ } \\mathsf { c } \\in C \\mathrm { ~ w i t h ~ } \\mathbf { a } _ { i n i t , } ^ { x }\n$$\n\nWe define the sets of MR policies which are SPR (or respectively GPR) policies by\n\nThe three considered sets have the same dimension:\n\nTheorem 9. For an MDP M, a set of terminal states E and a predictor set $C$ for which a memoryless SPR (resp. GPR) policy exists, the sets $\\mathfrak { P }$ and $\\mathfrak { P } _ { S \\mathsf { P } \\mathsf { R } }$ (resp. PGPR) have the same dimension.\n\nProof sketch. The claim follows from the fact that for any $\\boldsymbol { \\mathrm { \\Phi } } _ { \\boldsymbol { \\mathrm { \\Phi } } } \\in \\mathfrak { P } _ { S \\mathrm { P } \\mathbb { R } }$ (resp. ${ \\mathfrak { P } } _ { \\mathtt { G P R } } )$ there is an $\\varepsilon > 0$ such that the ε-neighborhood of $x$ is completely contained in $\\mathfrak { P }$ . □\n\nRecall the set of all terminal states ${ \\textsf { T } } \\subset { \\textsf { S } }$ of $\\mathcal { M }$ . With this, we can now define the following Volumes with $| S t A c t | - | S \\setminus \\mathsf { T } |$ -dimensional Lebesgue integrals:\n\n$$\n\\begin{array} { c } { { \\displaystyle \\boldsymbol { \\mathsf { V } } ( \\mathfrak { P } ) = \\int _ { \\mathfrak { P } } 1 \\mathrm { ~ d } \\mathsf { x } , } } \\\\ { { \\displaystyle \\boldsymbol { \\mathsf { V } } ( \\mathfrak { P } _ { S \\mathrm { P R } } ) = \\int _ { \\mathfrak { P } _ { S \\mathrm { P R } } } 1 \\mathrm { ~ d } \\mathsf { x } , \\quad \\mathsf { V } ( \\mathfrak { P } _ { \\mathsf { G P R } } ) = \\int _ { \\mathfrak { P } _ { \\mathbb { G P R } } } 1 \\mathrm { ~ d } \\mathsf { x } . } } \\end{array}\n$$\n\nDefinition 10. Let $\\mathcal { M }$ be an MDP, $E \\subset S$ a set of terminal states and $C \\subseteq S \\setminus E$ . We define the strict causal volume and resp. global causal volume of $C$ for $E$ as\n\n$$\n\\operatorname { s v } ( C ) = { \\frac { \\operatorname { V } ( { \\mathfrak { P } } _ { S \\operatorname { P R } } ) } { \\operatorname { V } ( { \\mathfrak { P } } ) } } \\quad { \\mathrm { a n d } } \\quad \\operatorname { g V } ( C ) = { \\frac { \\operatorname { V } ( { \\mathfrak { P } } _ { G \\operatorname { P R } } ) } { \\operatorname { V } ( { \\mathfrak { P } } ) } } .\n$$\n\nThese causal volumes now express the fraction of MR policies which constitute SPR or GPR policies. Having an estimate of the causal volume of a predictor is an additional information about its quality. Consider, for example, a large network in which a message is sent from one node to another. The information that a component is a probabilistic cause for message to be lost in a lot of cases gives a good reason to predict the message loss whenever the component is part of the communication.\n\nExample 11. For a smaller network example we again consider the MDP from Figure 2 where the undesired outcome $E = \\left\\{ l o s t \\right\\}$ is to be predicted by $C = \\{ \\mathsf { B } \\}$ . Here, we conclude from Example 8 that PR policies exist. Since $C$ is a singleton the sets for SPR and GPR policies coincide and we have\n\n$$\n\\mathfrak { P } _ { \\mathtt { G P R } } = \\{ \\mathfrak { x } \\in [ 0 , 1 ] ^ { S t A c t } \\mid \\mathfrak { x } _ { A , \\alpha } < 1 \\mathrm { o r } \\mathfrak { x } _ { \\mathtt { B } , \\beta } < 1 \\} ,\n$$\n\nsince only the MD policy choosing $\\alpha$ and $\\beta$ does not constitute a PR policy. Thus, we also have $\\operatorname { s v } ( C ) = \\operatorname { g v } ( C ) = 1$ since almost all MR policies are both SPR and GPR policies. This means that from the perspective of probabilistic causality, the event $\\diamond \\mathtt { B }$ is a good predictor for $\\diamond { l o s t }$ . ◁\n\nAs mentioned in Section 3 the computation of $V ( { \\mathfrak { P } } )$ can be done in polynomial time by representing $\\mathfrak { P }$ as a product of simplices. However, in general the exact computation of the volume of a fully-dimensional polytope given in halfspace-representation such as ${ \\mathsf { V } } ( { \\mathfrak { P } } _ { S { \\mathsf { P R } } } )$ is #P-hard (Brightwell and Winkler 1991). Moreover, it has been shown that for any polynomial-time approximation there is a minimal gap between upper and lower bounds depending on the dimension of the problem (B´ara´ny and Fu¨redi 1987). With respect to these restrictions there are still efficient tools for the exact computation of polynomials over polytopes (De Loera et al. 2013). For $\\mathfrak { P } _ { \\mathtt { G P R } }$ exact integration is even harder, since it is not a polytope by equation (g). However, since (g) is the only non-linear function restricting $\\mathfrak { P } _ { \\mathtt { G P R } }$ this favors the usage of Monte Carlo integration algorithms (Press et al. 2007) to approximate $V ( \\mathfrak { P } _ { \\mathsf { G P R } } )$ .\n\n# 4.2 Checking Probability-Raising Policies\n\nWhile computing the causal volume for a given predictor and outcome is a difficult problem, we now want to address the existential query for probability-raising policies:\n\nGiven an MDP $\\mathcal { M }$ , a set of terminal states $E \\subseteq S$ and a predictor set $C \\subseteq S \\setminus E$ , is there a strict (resp. global) probability-raising policy for $C$ and $E$ in M?\n\nFor this existence check we use a model transformation similar to the two-state MDP of $\\mathcal { M }$ (Sec. 3). This then results in an end-component free MDP in which both $C$ and $E$ can only be visited once and there are exactly four terminal states corresponding to the entries of the confusion matrix. This will allow us to express these probabilities as reachability probabilities of single terminal states. We use the abbreviations from Figure 3.\n\nDefinition 12 (Canonical MDP). We transform the original MDP M to the canonical MDP $\\mathcal { M } _ { [ C ] }$ in the following way:\n\n(i) All outgoing transitions from states $c \\in C$ are deleted and instead two fresh actions $\\alpha _ { \\mathrm { m i n } }$ and $\\alpha _ { \\mathrm { m a x } }$ are added. They transition to a new state $T P$ with the minimal and resp. maximal probability to reach $E$ from c. With the remaining probability they transition to a new state $F P$ .   \n(ii) Collapse the maximal end-components (MECs) E of the resulting MDP into single states $s _ { \\mathcal { E } }$ by taking the MECquotient, see e.g. (de Alfaro 1999).   \n(iii) Collapse the states of $E$ to a fresh state $F N$ and other terminal states to the fresh state $T N$ . States $s _ { \\mathcal { E } }$ representing MECs get the additional transition $\\mathsf { P } ( s _ { \\mathbb { \\varepsilon } } , \\tau , T N ) = 1$ . The resulting MDP is $\\Re _ { [ C ] }$ .\n\nIn $\\mathcal { M } _ { [ C ] }$ we consider $E ^ { \\prime } = \\{ T P , F N \\}$ .\n\nIntuitively, the first step (i) of the transformation ensures, that each state $c \\in C$ is visited at most once, while still preserving all possible values for true positive and false positive predictions. In the second (ii) and third step (iii) the transformation gets rid of end-components by collapsing them into single states. Instead, the fresh action $\\tau$ corresponds to the case that in the original MDP $\\mathcal { M }$ a policy $\\mathfrak { S }$ realizes a true negative by staying in an end-component E indefinitely. The soundness of the canonical MDP for a given set $C$ with respect to (G) and (S) follows from the results of (Baier, Piribauer, and Ziemek 2024) and is given by the following Lemma 13 and Corollary 14.\n\nLemma 13 $\\Re _ { [ C ] }$ preserves Confusion Matrix). Given an MDP M, a set of terminal states $E \\subset S$ and a set $C \\subseteq S \\setminus E ,$ , for each policy $\\mathfrak { S }$ of $\\mathcal { M }$ there is a policy $\\mathfrak { U }$ of $\\Re _ { [ C ] }$ and vice versa such that\n\n$$\n\\begin{array} { r } { t p _ { \\mathbb { M } } ^ { \\mathfrak { S } } = \\mathbf { P r } _ { \\mathbb { M } _ { [ C ] } } ^ { \\mathfrak { U } } ( \\diamondsuit P ) , \\qquad f p _ { \\mathbb { M } } ^ { \\mathfrak { S } } = \\mathbf { P r } _ { \\mathbb { M } _ { [ C ] } } ^ { \\mathfrak { U } } ( \\diamondsuit P P ) , } \\\\ { f n _ { \\mathbb { M } } ^ { \\mathfrak { S } } = \\mathbf { P r } _ { \\mathcal { M } _ { [ C ] } } ^ { \\mathfrak { U } } ( \\diamondsuit F N ) , \\qquad t n _ { \\mathbb { M } } ^ { \\mathfrak { S } } = \\mathbf { P r } _ { \\mathcal { M } _ { [ C ] } } ^ { \\mathfrak { U } } ( \\diamondsuit T N ) . } \\end{array}\n$$\n\nCorollary 14 $( \\mathfrak { M } _ { [ C ] }$ preserves Probability-Raising). Given an MDP M, a set of terminal states $E \\subset S$ and a set $C \\subseteq S \\setminus E ,$ , a policy $\\mathfrak { S }$ of $\\mathcal { M }$ satisfies (S) (resp. $( G )$ ) for $C$ and $E$ in M iff the corresponding policy U from Lemma $^ { 1 3 }$ satisfies (S) (resp. $( G )$ ) for $C$ and $\\{ T P , F N \\}$ in $\\mathcal { M } _ { [ C ] }$ .\n\nChecking for SPR Policies We now consider the following problem: Given an MDP M with a set of terminal states $E \\subset S$ and a predictor set $C \\subseteq S \\setminus E$ , is there a policy $\\mathfrak { S }$ of $\\mathcal { M }$ such that (SPR) holds for $\\mathfrak { S } ?$ By the soundness of the canonical MDP w.r.t. the PR conditions (Cor. 14) and the confusion matrix (Cor. 14) we assume $\\mathfrak { M } = \\mathfrak { M } _ { [ C ] }$ .\n\nFor an SPR policy there needs to be a balancing between states $c \\in C$ with a high value $\\mathtt { p _ { c , m a x } }$ and states $\\mathbf { c } ^ { \\prime } \\in C$ with a low value $\\mathfrak { p } _ { \\mathrm { c ^ { \\prime } , m a x } }$ . We will provide a characterization of SPR policies using this idea. For this, we consider the maximal probability to reach $T P$ among all cause states:\n\n$$\n\\mathfrak { p } ^ { \\star } = \\mathfrak { m a x } _ { c \\in C } \\mathfrak { p } _ { \\mathtt { c } , \\mathtt { m a x } }\n$$\n\nFrom $\\mathcal { M }$ to $\\mathcal { M } ^ { \\star }$ we only change the actions in states $c \\in C$ . For each c the only enabled action in $\\mathcal { M } ^ { \\star }$ is δ with\n\nSo, $\\mathcal { M } ^ { \\star }$ behaves as $\\mathcal { M }$ , but when a state $c \\in C$ is reached the single enabled action leads to $T P$ with $\\mathfrak { p } ^ { \\star }$ . By construction, any policy $\\mathfrak { U }$ of $\\mathcal { M } ^ { \\star }$ corresponds to a policy $\\mathfrak { S }$ of $\\mathcal { M }$ .\n\nLemma 15 (Characterization of SPR Policies). For an MDP M with set of terminal states $E \\subset S$ and set of states $C \\subseteq S \\setminus E$ there is an SPR policy $\\mathfrak { S }$ for $C$ and $E$ in M iff $\\mathrm { P r } _ { \\mathcal { M } ^ { \\star } } ^ { \\operatorname* { m i n } } ( \\diamond E ) < \\mathfrak { p } ^ { \\star }$ .\n\nTheorem 16 (Complexity for Checking SPR policy). Given an MDP M with set of terminal states $E \\subset S$ and set of states $C \\subseteq S \\setminus E$ the existence of an SPR policy for $C$ and $E$ in $\\mathcal { M }$ can be decided in P. Moreover, a corresponding finite-memory (randomized) policy $\\mathfrak { S }$ can be computed in polynomial time.\n\nProof. We first note that we can transform $\\mathcal { M }$ to $\\mathcal { M } _ { [ C ] }$ in polynomial time. We can now rely on the characterization of Lemma 15. So, by further transforming M[C] to M⋆C we can check whether $\\mathrm { P r } _ { \\mathcal { M } _ { [ C ] } ^ { \\star } } ^ { \\operatorname* { m i n } } < \\mathfrak { p } ^ { \\star }$ in polynomial time (Baier\n\nIf this check is positive, an MD policy $\\mathfrak { T }$ of $\\mathcal { M } _ { [ C ] } ^ { \\star }$ with $\\mathrm { P r } _ { \\mathcal { M } _ { [ C ] } ^ { \\star } } ^ { \\mathfrak { T } } = \\mathrm { P r } _ { \\mathcal { M } _ { [ C ] } ^ { \\star } } ^ { \\operatorname* { m i n } }$ = PrmMin[⋆C] can be derived. By (Baier, Piribauer, and Ziemek 2024)[Theorem 4.19] we can further derive the corresponding finite-memory (randomized) policy $\\mathfrak { S }$ for $\\mathcal { M }$ in polynomial time. □\n\nRemark 17 (Complexity for Singletons). The case where $C = \\{ { \\mathfrak { c } } \\}$ is a singleton can be decided more efficient. Note that there does not need to be a balancing between different states of $C$ . Therefore, it is sufficient to only enable action $\\alpha _ { \\mathrm { m a x } }$ in c. Let us call the resulting MDP $\\mathcal { N }$ . We then only have to check, whether $\\mathfrak { p } _ { \\mathrm { m a x } , \\mathrm { c } } \\ > \\ \\mathbf { \\bar { P } } \\mathfrak { r } _ { \\mathcal { N } } ^ { \\mathrm { m i n } } ( \\diamond E )$ as this corresponds to the best case SPR policy. ◁\n\nChecking for GPR Policies The next decision problem we consider is: Given an MDP $\\mathcal { M }$ , set of terminal states $E$ and predictor set $C$ is there a GPR policy $\\mathfrak { S }$ for $C$ and $E$ in $\\Re ?$ By Lemma 13 we assume $\\Re = \\Re _ { [ C ] }$ and use this to encode the inequality (GPR) in terms of state-action frequency variables. For this, we reformulate (GPR) in $\\Re _ { [ C ] }$ , using straight forward calculations, to\n\n$$\n\\operatorname* { P r } ^ { \\mathfrak { S } } ( \\langle \\rangle T P ) \\cdot \\operatorname* { P r } ^ { \\mathfrak { S } } ( \\langle \\rangle T N ) - \\operatorname* { P r } ^ { \\mathfrak { S } } ( \\langle \\rangle F P ) \\cdot \\operatorname* { P r } ^ { \\mathfrak { S } } ( \\langle \\rangle F N ) > 0 .\n$$\n\nSo, we consider this inequality in terms of frequencies together with the equations (S1)-(S3) (cf. Section 2):\n\n$$\n\\Upsilon _ { T P } \\cdot \\Upsilon _ { T N } - \\Upsilon _ { F P } \\cdot \\Upsilon _ { F N } > 0 .\n$$\n\nLemma 18 (Quadratic Program for GPR policy). There is a GPR policy for $C$ and $E$ in $\\mathcal { M }$ iff the system of inequalities given by (S1)-(S3) and (freq-GPR) has a solution.\n\nProof. Let $\\mathfrak { S }$ be a GPR policy for $C$ and $E$ in $\\mathcal { M }$ . By construction the corresponding frequencies of state-action pairs of $\\mathfrak { S }$ are a solution to (S1)-(S3) and (freq-GPR).\n\nNow, assume $\\boldsymbol { x } ~ \\in ~ \\mathbb { R } ^ { S t A c t }$ is a solution to (S1)-(S3) and (freq-GPR) and let $\\mathfrak { S }$ be an MR policy corresponding to x. The inequality (freq-GPR) is equivalent to (GPR) in $\\Re _ { [ C ] }$ . As (freq-GPR) and (S1) hold, we have $\\Upsilon _ { T P } \\cdot \\Upsilon _ { T N } > 0$ and thus $x _ { T P } > 0$ . Since $\\mathsf { x } _ { C } = \\mathsf { x } _ { T P } + \\mathsf { x } _ { F P }$ we also get $\\mathrm { P r } ^ { \\mathfrak { S } } ( \\diamond C ) > 0$ . So, $\\mathfrak { S }$ is a GPR policy for $C$ and $E$ in $\\mathcal { M }$ . □\n\nHowever, since (freq-GPR) is a strict inequality we can not directly use quadratic programming to decide the solvability (Vavasis 1990). We can still give an NP upper bound for deciding the existence of a GPR policy.\n\nTheorem 19 (Complexity GPR Policies Check). Deciding whether there is a GPR policy for $C$ and $E$ in $\\mathcal { M }$ can be done in NP.\n\nProof sketch. The proof uses distinct cases of (freq-GPR) within the set of possible state-action frequencies. It relies on the intermediate value theorem to show that, if there is a possible distribution of frequencies that forces (freq-GPR) to an equality, then there is an ε-neighborhood in which this becomes an inequality again. □\n\nBy the result of (Baier, Piribauer, and Ziemek 2024)[Theorem 4.19] this existence check results in a finite-memory randomized GPR policy $\\mathfrak { S }$ with exactly two memory cells.\n\n# 5 Conclusion\n\nIn this paper, we proposed different measures to quantify the quality of predictors in adaptive systems. In the stochastic operational model of MDPs we considered predictors (and outcomes) as sets of states of the MDP. To this end, we proposed two approaches for measuring the effectiveness of the predictor. First, by averaging over the non-deterministic choices of the MDP we capture the effectiveness of the predictor to foresee undesired events in general. Second, we measure the fraction of policies that witness probabilityraising causality of the predictor to the outcome, where we borrow ideas from recent work on probability-raising causality (Baier, Piribauer, and Ziemek 2024). For the proposed measures we provided insights on the complexity and discussed existing numerical methods for computing the respective measures.\n\n# Acknowledgments\n\nThe authors are supported by the DFG through the DFG grant 389792660 as part of TRR 248 and the Cluster of Excellence EXC 2050/1 (CeTI, project ID 390696704, as part of Germany’s Excellence Strategy) and by BMBF (Federal Ministry of Education and Research) in DAAD project 57616814 (SECAI, School of Embedded and Composite AI) as part of the program Konrad Zuse Schools of Excellence in Artificial Intelligence.",
    "institutions": [
        "Technische Universit¨at Dresden",
        "Universita¨t Leipzig"
    ],
    "summary": "{\n  \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n在自适应系统中，预测器的质量对系统的整体可靠性和性能至关重要。然而，大多数人工智能系统难以完全理解，且无法排除故障，现有的证书无法为预测系统行为提供足够的见解。因此，需要引入衡量指标来评估系统中某些事件作为不良结果预测器的有效性。此问题的重要性在于能够提前准确预测系统产生不良或灾难性结果的情况，有助于提高系统的可靠性和性能。\\n\\n**方法概述**\\n本文考虑马尔可夫决策过程（MDP）中的二元预测器，通过对无记忆随机策略（MR policies）进行均匀平均，引入平均情况分析来衡量预测器的质量。同时，引入概率提升策略（PR policies）和因果体积的概念，以获取更多关于预测器质量的信息。\\n\\n**主要贡献与效果**\\n- 形式化引入了基于非确定性的预测器质量平均情况分析，使用无记忆随机策略的均匀测度来评估预测器的质量。通过对通信网络示例的计算，如在 f - score 指标上，到达节点 B 的平均 f - score 约为 0.60，到达节点 A 的平均 f - score 约为 0.43，表明到达节点 B 比到达节点 A 平均而言是消息丢失的更好预测器。\\n- 引入概率提升策略的概念，用于定义 MDP 中预测器的因果体积，为评估预测器质量提供了额外信息。以一个较小的网络 MDP 为例，当用节点 B 预测消息丢失时，几乎所有的无记忆随机策略（MR policies）都是严格概率提升策略（SPR）和全局概率提升策略（GPR），严格因果体积和全局因果体积均为 1，表明从概率因果关系的角度来看，节点 B 是消息丢失的良好预测器。\\n- 分析了判定概率提升策略存在性的复杂度，判定 SPR 策略存在性可在多项式时间（P）内完成，判定 GPR 策略存在性可在非确定性多项式时间（NP）内完成。\",\n  \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n核心思想是考虑 MDP 中的二元预测器，通过对非确定性选择进行平均，以评估预测器的质量。利用统计分析中的二元分类器质量度量，结合无记忆随机策略，将预测器的质量与策略相关联。同时，引入概率提升策略的概念，判断预测器与不良事件之间是否存在概率因果关系，通过因果体积来衡量这种关系的普遍性。这种方法有效是因为它考虑了系统的非确定性和概率性，能够更全面地评估预测器的质量。\\n\\n**创新点**\\n先前的工作在预测人工智能系统行为时，大多依赖于证书，但这些证书无法为排除故障的系统提供足够的决策见解。同时，先前的工作主要关注确定性系统中的责任和预测。本文的创新之处在于引入了基于非确定性的平均情况分析，考虑了所有可能的无记忆随机策略，而不是依赖于特定的策略或假设。此外，引入概率提升策略和因果体积的概念，从概率因果关系的角度为评估预测器质量提供了新的视角和方法。\\n\\n**具体实现步骤**\\n1. **测量预测器质量**：\\n    - 选择无记忆随机（MR）策略，并使用均匀测度对其进行平均。因为在无结束组件（EC）的 MDP 中，任何策略都可以由 MR 策略近似。\\n    - 考虑二元分类器的混淆矩阵，使用精度、召回率、f - score 和 Matthews 相关系数等质量度量。\\n    - 通过模型变换定义双拷贝 MDP $\\Re _ { C }$，将混淆矩阵的条目表示为可达性概率。\\n    - 用 MR 策略的向量表示可达性概率，并通过积分计算平均质量度量。由于精确计算积分通常不可行，使用蒙特卡罗积分等标准方法来近似积分。\\n2. **定义概率提升策略和因果体积**：\\n    - 定义全局概率提升（GPR）策略和严格概率提升（SPR）策略，判断预测器与不良事件之间是否存在概率因果关系。\\n    - 用可达性矩阵表示概率提升条件，定义 SPR 和 GPR 策略的集合。\\n    - 计算这些集合的体积，定义严格因果体积和全局因果体积，以衡量预测器的质量。精确计算通常困难，对于 $\\mathfrak { P } _ { \\mathtt { G P R } }$ 可以使用蒙特卡罗积分算法进行近似。\\n3. **检查概率提升策略的存在性**：\\n    - 对于 SPR 策略，将原始 MDP 转换为规范 MDP $\\mathcal { M } _ { [ C ] }$，通过比较最大可达概率和最小可达概率来判断 SPR 策略的存在性。对于单例情况（$C = \\{ { \\mathfrak { c } } \\}$），可以更高效地判断，只需检查 $\\mathfrak { p } _ { \\mathrm { m a x } , \\mathrm { c } } \\ > \\ \\mathbf { \\bar { P } } \\mathfrak { r } _ { \\mathcal { N } } ^ { \\mathrm { m i n } } ( \\diamond E )$。\\n    - 对于 GPR 策略，同样将原始 MDP 转换为规范 MDP $\\mathcal { M } _ { [ C ] }$，将 GPR 条件表示为状态 - 动作频率变量的不等式，通过求解不等式系统来判断 GPR 策略的存在性。由于该不等式是严格不等式，不能直接使用二次规划求解，但可以给出 NP 上界。\\n\\n**案例解析**\\n- **通信网络示例**：考虑一个通信网络，消息从发送者通过各种网络节点发送到接收者。每个节点以随机方式将消息转发给其他节点，只知道每个节点的后继节点以及相应概率的上下界。假设怀疑某些节点集有故障，是导致消息丢失的原因。通过对非确定性的可能解决方案进行平均，测量到达这些节点集作为消息丢失预测器的有效性。例如，在图 2 所示的 MDP 中，分别以节点 A 和节点 B 作为预测器，计算平均 f - score，得出到达节点 B 平均而言是消息丢失的更好预测器。\\n- **较小网络 MDP 示例**：用节点 B 预测消息丢失（$E = \\left\\{ l o s t \\right\\}$），对于选择 $\\gamma$ 在节点 A 和 $\\beta$ 在节点 B 的确定性策略 $\\mathfrak { S }$，满足 GPR 和 SPR 条件，构成概率提升策略。而选择 $\\alpha$ 在节点 A 和 $\\beta$ 在节点 B 的确定性策略 $\\mathfrak { U }$ 不满足 GPR 条件。由于节点 B 是单例，SPR 和 GPR 策略的集合相同，几乎所有的 MR 策略都是 SPR 和 GPR 策略，严格因果体积和全局因果体积均为 1。\",\n  \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文未明确提及用于对比的核心基线模型。\\n\\n**性能对比**\\n论文未进行与基线模型的对比实验，因此没有相关性能对比结果。\",\n  \"keywords\": \"### 关键词\\n\\n- 马尔可夫决策过程 (Markov Decision Processes, MDPs)\\n- 预测器质量衡量 (Quality Measures for Predictors, N/A)\\n- 无记忆随机策略 (Memoryless Randomized Policies, MR policies)\\n- 概率提升策略 (Probability - Raising Policies, PR policies)\\n- 因果体积 (Causal Volume, N/A)\"\n}"
}