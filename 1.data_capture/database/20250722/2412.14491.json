{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.14491",
    "link": "https://arxiv.org/abs/2412.14491",
    "pdf_link": "https://arxiv.org/pdf/2412.14491.pdf",
    "title": "Mediation Analysis for Probabilities of Causation",
    "authors": [
        "Yuta Kawakami",
        "Jin Tian"
    ],
    "categories": [
        "cs.AI"
    ],
    "publication_date": "2024-12-19",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "Mohamed bin Zayed University of Artificial Intelligence"
    ],
    "paper_content": "# Mediation Analysis for Probabilities of Causation\n\nYuta Kawakami, Jin Tian\n\nMohamed bin Zayed University of Artificial Intelligence, UAE Yuta.Kawakami, Jin.Tian $@$ mbzuai.ac.ae\n\n# Abstract\n\nProbabilities of causation (PoC) offer valuable insights for informed decision-making. This paper introduces novel variants of PoC-controlled direct, natural direct, and natural indirect probability of necessity and sufficiency (PNS). These metrics quantify the necessity and sufficiency of a treatment for producing an outcome, accounting for different causal pathways. We develop identification theorems for these new PoC measures, allowing for their estimation from observational data. We demonstrate the practical application of our results through an analysis of a real-world psychology dataset.\n\n# Introduction\n\nPearl (1999) introduced three types of probabilities of causation (PoC), that is, the probability of necessity and sufficiency (PNS), the probability of necessity (PN), and the probability of sufficiency (PS). PoC quantify whether one event was the real cause of another in a given scenario (Robins and Greenland 1989; Tian and Pearl 2000; Pearl 2009; Kuroki and Cai 2011; Dawid, Murtas, and Musio 2014; Murtas, Dawid, and Musio 2017; Shingaki and Kuroki 2021; Kawakami, Shingaki, and Kuroki 2023). PoC are valuable for decision-making (Hannart and Naveau 2018; Li and Pearl 2019, 2022) and for explaining AI-based decisionmaking systems (Galhotra, Pradhan, and Salimi 2021; Watson et al. 2021).\n\nVarious variants of PoC have been studied, including for multi-valued discrete variables (Li and Pearl 2024a,b) and for continuous and vector variables (Kawakami, Kuroki, and Tian 2024). Rubinstein, Cuellar, and Malinsky (2024) introduced direct and indirect mediated PoC to decompose total PoC when there exists a mediator between the treatment and outcome.\n\nCausal mediation analysis is a key method for uncovering the influence of different pathways between the treatment and outcome through mediators (Wright 1921, 1934; Baron and Kenny 1986; Robins and Greenland 1992; Imai, Keele, and Tingley 2010; Imai, Keele, and Yamamoto 2010; Tchetgen and Shpitser 2012). Notably, Pearl (2001) formally defined direct and indirect effects for general nonlinear models. Causal mediation analysis is also a valuable technique for explainable artificial intelligence (XAI) (Shin 2021). In this paper, we aim to provide causal mediation analysis for PoC, to reveal the necessity and sufficiency of the treatment through different pathways. Once a treatment is revealed to be necessary and sufficient to induce a particular event via PNS, other causal questions would arise:\n\n(Q1). Would the treatment still be necessary and sufficient   \nhad the value of the mediator been fixed to a certain value?   \n(Q2). Would the treatment still be necessary and sufficient had there been no influence via the mediator?   \n(Q3). Would the treatment still be necessary and sufficient had the influence only existed via the mediator?\n\nWe introduce new variants of PoC - controlled direct, natural direct, and natural indirect PNS (CD-PNS, ND-PNS, and NI-PNS) to answer these questions. We further define direct and indirect PoC with evidence to capture more sophisticated counterfactual information useful for decisionmaking. These quantities can retrospectively answer questions (Q1), (Q2), and (Q3) for a specific subpopulation. We provide identification results for each type of PoC we introduce. Finally, we apply our results to a real-world psychology dataset.\n\n# Notations and Background\n\nWe represent a single or vector variable with a capital letter $( X )$ and its realized value with a small letter $( x )$ . Let $\\mathbb { I } ( \\cdot )$ be an indicator function that takes 1 if the statement in $( \\cdot )$ is true and 0 otherwise. Denote $\\Omega _ { Y }$ be the domain of variable $Y , \\mathbb { E } [ Y ]$ be the expectation of $Y$ , $\\mathbb { P } ( Y < y )$ be the cumulative distribution function (CDF) of continuous variable $Y$ , and p $( Y = y )$ be the probability density function (PDF) of continuous variable $Y$ . We use $X \\perp \\perp Y | C$ to denote that $X$ and $Y$ are conditionally independent given $C$ . We use $\\preceq$ to denote a total order. A formal definition of total order is given in Appendix A in (Kawakami and Tian 2024).\n\nStructural causal models (SCM). We use the language of SCMs as our basic framework and follow the standard definition in the following (Pearl 2009). An SCM $\\mathcal { M }$ is a tuple $\\langle V , U , { \\mathcal { F } } , \\mathbb { P } _ { U } \\rangle$ , where $\\pmb { U }$ is a set of exogenous (unobserved) variables following a distribution $\\mathbb { P } _ { U }$ , and $V$ is a set of endogenous (observable) variables whose values are determined by structural functions ${ \\mathcal F } = \\{ f _ { V _ { i } } \\} _ { V _ { i } \\in V }$ such that $v _ { i } : = f _ { V _ { i } } ( \\mathbf { p a } _ { V _ { i } } , \\pmb { u } _ { V _ { i } } )$ where $\\mathbf { P A } _ { V _ { i } } \\subseteq V$ and $U _ { V _ { i } } \\subseteq U$ . Each SCM $\\mathcal { M }$ induces an observational distribution $\\mathbb { P } _ { V }$ over $V$ , and a causal graph $G ( \\mathcal { M } )$ in which there exists a directed edge from every variable in $\\mathbf { P A } _ { V _ { i } }$ and $\\pmb { U } _ { V _ { i } }$ to $V _ { i }$ . An intervention of setting a set of endogenous variables $X$ to constants $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ , denoted by $d o ( { \\pmb x } )$ , replaces the original equations of $X$ by the constants $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ and induces a sub-model $\\mathcal { M } _ { x }$ . We denote the potential outcome $Y$ under intervention $d o ( { \\pmb x } )$ by $Y _ { \\pmb { x } } ( \\pmb { u } )$ , which is the solution of $Y$ in the sub-model $\\mathcal { M } _ { x }$ given $\\pmb { U } = \\pmb { u }$ .\n\n![](images/48adfcbdccae34cc2a0b7b33f9033f0160a1a1e705eb56be1d4ab22a954d7917.jpg)  \nFigure 1: A causal graph representing SCM $\\mathcal { M }$ .\n\nProbabilities of causation $( \\mathbf { P o C } )$ . Kawakami, Kuroki, and Tian (2024) defined the (multivariate conditional) PoC for vectors of continuous or discrete variables as follows:\n\nDefinition 1 (PoC). (Kawakami, Kuroki, and Tian 2024) The (multivariate conditional) PoC are defined by\n\n$$\n\\operatorname { P N S } ( y ; x ^ { \\prime } , x , c ) \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x } | C = c ) ,\n$$\n\n$$\n\\operatorname { P N } ( y ; x ^ { \\prime } , x , c ) \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y | y \\prec Y , X = x , C = c ) ,\n$$\n\n$$\n\\operatorname { P S } ( y ; x ^ { \\prime } , x , c ) \\triangleq \\mathbb { P } ( y \\preceq Y _ { x } | Y \\prec y , X = x ^ { \\prime } , C = c ) .\n$$\n\n$\\mathrm { P N S } ( y ; x ^ { \\prime } , x , c )$ provides a measure of the necessity and sufficiency of $x$ w.r.t. $x ^ { \\prime }$ to produce $Y ~ \\succeq ~ y$ given $\\textit { C } = \\textit { c }$ . $\\mathrm { P N } ( y ; x ^ { \\prime } , x , c )$ and $\\mathrm { P S } ( y ; x ^ { \\prime } , x , c )$ provide a measure of the necessity and sufficiency, respectively, of $x$ w.r.t. $x ^ { \\prime }$ to produce $Y \\succeq y$ given $C = c$ .\n\nWe will often call PNS total PNS (T-PNS) and denote it by $\\mathrm { T } \\mathrm { - } \\mathrm { P N S } ( y ; x ^ { \\prime } , x , c )$ for convenience. When treatment $X$ and outcome $Y$ are binary, PNS, PS, and PS become (setting $y = 1 \\rangle$ $\\mathrm { P N S } ( c ) = \\mathbb { P } ( Y _ { 0 } = 0 , Y _ { 1 } = 1 | C = c )$ , $\\mathbf { P N } ( c ) \\bar { = }$ $\\mathbb { P } ( Y _ { 0 } = 0 | Y = 1 , X = 1 , C = c )$ , and $\\mathrm { P S } ( c ) = \\mathbb { P } ( Y _ { 1 } =$ $1 | Y = 0 , X = 0 , C = c )$ for any $c \\in \\Omega _ { C }$ , which reduce to Pearl’s (1999) original definition when $C = \\varnothing$ .\n\nCausal mediation analysis. Causal mediation analysis reveals the strength of different pathways between treatment and outcome through a mediator. Researchers often consider the following SCM $\\mathcal { M }$ :\n\n$$\n\\begin{array} { r } { Y : = f _ { Y } ( X , M , C , U _ { Y } ) , M : = f _ { M } ( X , C , U _ { M } ) , } \\\\ { X : = f _ { X } ( C , U _ { X } ) , C : = f _ { C } ( U _ { C } ) , \\qquad } \\end{array}\n$$\n\nwhere all variables can be vectors, and $U _ { X } , U _ { C } , U _ { Y }$ , and $U _ { M }$ are latent exogenous variables. Assume that the domains $\\Omega _ { Y }$ and $\\Omega _ { U _ { Y } } \\times \\Omega _ { U _ { M } }$ are totally ordered sets with $\\preceq$ . Figure 1 shows the causal graph of SCM $\\mathcal { M }$ (with latent variables dropped).\n\nOne widely used model in the mediation analysis is a linear SCM $\\mathcal { M } ^ { \\bar { L } }$ (Baron and Kenny 1986) consisting of $Y : =$ $\\beta _ { 0 } + \\beta _ { 1 } X + \\beta _ { 2 } M + \\beta _ { 3 } C + U _ { Y }$ and ${ \\cal M } : = \\alpha _ { 0 } + \\alpha _ { 1 } X + \\alpha _ { 2 } C +$ $U _ { M }$ , where $U _ { Y } \\sim \\mathcal { N } ( \\mu _ { Y } , \\sigma _ { Y } ^ { 2 } )$ and $U _ { M } \\sim \\mathcal N ( \\mu _ { M } , \\sigma _ { M } ^ { 2 } )$ are independent normal distribution. Under SCM $\\mathcal { M } ^ { L }$ , the total effect of $X$ on $Y$ is $\\beta _ { 1 } + \\alpha _ { 1 } \\beta _ { 2 }$ , the indirect effect is $\\alpha _ { 1 } \\beta _ { 2 }$ , and the direct effect is $\\beta _ { 1 }$ .\n\nPearl (2001) defined the total, controlled direct, natural direct, and natural indirect effects for general (nonlinear and nonparametric) SCM $\\mathcal { M }$ .\n\nDefinition 2 (TE, CDE, NDE, and NIE). (Pearl 2001) The total, controlled direct, natural direct, and natural indirect effects are defined by:\n\n1. Total $E f f e c t \\left( T E \\right) : { \\mathrm { T E } } ( y ; x ^ { \\prime } , x ) \\triangleq \\mathbb { E } [ Y _ { x } ] - \\mathbb { E } [ Y _ { x ^ { \\prime } } ]$   \n2. Controlled Direct Effect (CDE): $\\mathrm { C D E } ( y ; x ^ { \\prime } , x , m )$ =∆ $\\mathbb { E } [ Y _ { x , m } ] - \\mathbb { E } [ Y _ { x ^ { \\prime } , m } ]$   \n3. Natural Direct Effect (NDE): $\\mathrm { N D E } ( y ; x ^ { \\prime } , x )$ =∆ $\\mathbb { E } [ Y _ { x , M _ { x ^ { \\prime } } } ] - \\mathbb { E } [ Y _ { x ^ { \\prime } } ]$   \n4. Natural Indirect Effect (NIE): $\\mathrm { { N I E } } ( y ; x ^ { \\prime } , x )$ =∆ $\\mathbb { E } [ Y _ { x ^ { \\prime } , M _ { x } } ] - \\mathbb { E } [ Y _ { x ^ { \\prime } } ]$\n\nCDE represents the causal effect of changing the treatment from $x ^ { \\prime }$ to $x$ had the value of the mediator been fixed at a certain value. NDE represents the causal effect of changing the treatment from $x ^ { \\prime }$ to $x$ had the value of the mediator been kept to the same value $M _ { x ^ { \\prime } }$ that $M$ attains under $x ^ { \\prime }$ . NIE represents the causal effect of changing the mediator from $M _ { x ^ { \\prime } }$ to $M _ { x }$ had the value of the treatment been fixed to $x ^ { \\prime }$ . TE can be decomposed into NDE and NIE by $\\Gamma \\mathrm { E } ( y ; x ^ { \\prime } , x ) = \\mathrm { N D E } ( y ; x ^ { \\prime } , \\bar { x } ) - \\mathrm { N I E } ( y ; x , x ^ { \\prime } ) =$ $\\mathrm { N I E } ( y ; x ^ { \\prime } , x ) - \\mathrm { N D E } ( y ; x , x ^ { \\prime } )$ .\n\nThese direct and indirect effects may be identified from observational distributions under various settings (Pearl 2001; Avin, Shpitser, and Pearl 2005; Shpitser and Pearl 2008; Shpitser 2013; Malinsky, Shpitser, and Richardson 2019). A widely used assumption for identifying causal mediation effects is the following sequential ignorability assumption (Imai, Keele, and Tingley 2010):\n\nAssumption 1 (Sequential ignorability). The following two conditional independence statements hold:\n\n(1) $\\left\\{ Y _ { x , m } , M _ { x } \\right\\} \\bot \\bot X | C = c a n d \\left( 2 \\right) M _ { x } \\bot \\bot Y _ { x , m } | C = c$ for any $m \\in \\Omega _ { M }$ and $x \\in \\Omega _ { X }$ , where $\\mathfrak { p } ( X = x | C = c ) > 0$ and ${ \\mathfrak { p } } ( M = m | X = x , C = c ) > 0$ for any $m \\in \\Omega _ { M }$ , $x \\in \\Omega _ { X }$ , and $c \\in \\Omega _ { C }$ .\n\nWe have\n\nProposition 1 (Identification of $\\mathbb { P } ( Y _ { x ^ { \\prime } , M _ { x } } \\ \\prec \\ y | C \\ = \\ c )$ ). (Imai, Keele, and Tingley 2010; VanderWeele and Knol 2014) Under SCM $\\mathcal { M }$ and Assumption $\\jmath$ , the counterfactual $\\mathbb { P } ( Y _ { x ^ { \\prime } , M _ { x } } \\prec y | C = c )$ is identifiable by\n\n$$\n\\begin{array} { r l r } {  { \\mathbb { P } ( Y _ { x ^ { \\prime } , M _ { x } } \\prec y | C = c ) } } \\\\ & { = \\int _ { \\Omega _ { M } } \\mathbb { P } ( Y \\prec y | X = x ^ { \\prime } , M = m , C = c ) } \\\\ & { } & { \\times { \\mathfrak { p } } ( M = m | X = x , C = c ) d m } \\end{array}\n$$\n\nfor any $x ^ { \\prime } , x \\in \\Omega _ { X }$ , $y \\in \\Omega _ { Y }$ , and $c \\in \\Omega _ { C }$ .\n\n# Direct and Indirect PNS\n\nIn this section, we introduce new concepts of direct and indirect PNS and provide corresponding identification results. We will focus our attention on PNS, and show in the next section that direct and indirect PN and PS can be derived as special cases of direct and indirect PNS with evidence.\n\n# Definitions of CD-PNS, ND-PNS, and NI-PNS\n\nWe define controlled direct, natural direct, and natural indirect probabilities of necessity and sufficiency.\n\nDefinition 3 (CD-PNS, ND-PNS, and NI-PNS). The controlled direct, natural direct, and natural indirect PNS ( $C D$ - PNS, ND-PNS, and NI-PNS) are defined by\n\n$\\mathrm { C D - P N S } ( y ; x ^ { \\prime } , x , m , c ) \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } , m } \\prec y \\preceq Y _ { x , m } | C = c ) ,$ (6)   \n$\\mathrm { N D - P N S } ( y ; x ^ { \\prime } , x , c ) \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x } , Y _ { x ^ { \\prime } , M _ { x } } \\prec y | C = c ) ,$ (7)   \n$\\begin{array} { r l } & { \\mathrm { N I \\mathrm { - } P N S } ( y ; x ^ { \\prime } , x , c ) \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x } , y \\preceq Y _ { x ^ { \\prime } , M _ { x } } | C = c ) . } \\end{array}$\n\nFirst, the controlled direct PNS (CD-PNS) provides a measure of the necessity and sufficiency of $x$ w.r.t. $x ^ { \\prime }$ to produce $Y \\succeq y$ given $C = c$ when the mediator is fixed to a value $M = m$ . CD-PNS can be used to answer the causal question (Q1). CD-PNS consists of two counterfactual conditions:\n\n(A1). “had the treatment and the mediator been $( x ^ { \\prime } , m )$ , the outcome would be $Y \\prec y ^ { , , } ( Y _ { x ^ { \\prime } , m } \\prec y )$ ; and   \n(A2). “had the treatment and the mediator been $( x , m )$ , the outcome would be $y \\preceq Y ^ { \\prime \\prime } \\left( y \\preceq Y _ { x , m } \\right)$ .\n\nConditions (A1) and (A2) have different values of treatment and the same values of mediator. The relative values of the potential outcomes $Y _ { x , m }$ are shown in Figure 2 (b). For comparison, Figure 2 (a) shows the situation for T-PNS.\n\nSecond, ND-PNS has three counterfactual conditions:\n\n(B1). “had the treatment been $x ^ { \\prime }$ , the outcome would be $Y \\prec y ^ { , * } \\left( Y _ { x ^ { \\prime } } = Y _ { x ^ { \\prime } , M _ { x ^ { \\prime } } } \\prec y \\right)$ ,\n\n(B2). “had the treatment been $x ^ { \\prime }$ but the mediator was kept at the same value $M _ { x }$ when the treatment is $x$ , the outcome would be $Y \\prec y ^ { , , } ( Y _ { x ^ { \\prime } , M _ { x } } \\prec y )$ , and\n\n(B3). “had the treatment been $x$ , the outcome would be\n\n$$\ny \\preceq Y ^ { \\prime \\prime } ( y \\preceq Y _ { x } = Y _ { x , M _ { x } } ) .\n$$\n\nThe relative values of the potential outcomes are shown in Figure 2 (c). Conditions (B1) and (B3) mean $Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x }$ , which is the same condition in T-PNS and represents that the treatment $x$ is necessary and sufficient w.r.t. $x ^ { \\prime }$ to provoke the event $y \\preceq Y$ given $C = c$ . Conditions (B2) and (B3) mean $Y _ { x ^ { \\prime } , M _ { x } } \\prec y \\preceq Y _ { x , M _ { x } }$ , which represents the necessity and sufficiency of $x$ w.r.t. $x ^ { \\prime }$ to produce $Y \\succeq y$ given $C = c$ when keeping the values of the mediator by the same as $M _ { x }$ . In other words, they mean that the treatment would be necessary and sufficient even if there were no influences via the mediator. Therefore, ND-PNS can answer the causal question (Q2).\n\nThird, NI-PNS has three counterfactual conditions:\n\nYx′ y Yx   \n(a) Order of potential outcomes in T-PNS.   \nYx′,m y Yx,m   \n(b) Order of potential outcomes in CD-PNS.   \nYx Yx′,Mx y Y   \n(c) Order of potential outcomes in ND-PNS.   \nYx′ y Yx′,Mx Yx\n\n(d) Order of potential outcomes in NI-PNS.\n\nFigure 2: Order of potential outcomes in each PNS.\n\n(C1). “had the treatment been $x ^ { \\prime }$ , the outcome would be\n\n$$\nY \\prec y ^ { , \\prime } ( Y _ { x ^ { \\prime } } = Y _ { x ^ { \\prime } , M _ { x ^ { \\prime } } } \\prec y ) .\n$$\n\n(C2). “had the treatment been $x ^ { \\prime }$ but the mediator was kept at the same value $M _ { x }$ when the treatment is $x$ , the outcome would be $y \\preceq Y ^ { \\prime \\prime } ( y \\preceq Y _ { x ^ { \\prime } , M _ { x } } )$ , and\n\n(C3). “had the treatment been $x$ , the outcome would be\n\n$$\ny \\preceq Y ^ { \\prime \\prime } \\left( y \\preceq Y _ { x } = Y _ { x , M _ { x } } \\right)\n$$\n\nThe relative values of the potential outcomes are shown in Figure 2 (d). Conditions (C1) and (C3) mean $Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x }$ , which is the same condition in T-PNS and states that the treatment $x$ is necessary and sufficient w.r.t. $x ^ { \\prime }$ to provoke the event $y \\preceq Y$ given $C = c$ . Conditions (C1) and (C2) mean $Y _ { x ^ { \\prime } , M _ { x ^ { \\prime } } } \\prec y \\preceq Y _ { x ^ { \\prime } , M _ { x } }$ , which represents the necessity and sufficiency of $M _ { x }$ w.r.t. $M _ { x ^ { \\prime } }$ to produce $Y \\succeq y$ given $C = c$ when setting the treatment to $x ^ { \\prime }$ . In other words, they mean that the treatment would be necessary and sufficient if the influence is only via the mediator. Therefore, NI-PNS can answer the causal question (Q3).\n\nThen, the following proposition holds.\n\nProposition 2. We have\n\n$$\n\\begin{array} { r l } & { \\mathrm { T } \\mathrm { \\mathrm { - } } \\mathrm { P N S } ( y ; x ^ { \\prime } , x , c ) } \\\\ & { \\mathrm { ~ } = \\mathrm { N D } \\mathrm { - } \\mathrm { P N S } ( y ; x ^ { \\prime } , x , c ) + \\mathrm { N I } \\mathrm { - } \\mathrm { P N S } ( y ; x ^ { \\prime } , x , c ) . } \\end{array}\n$$\n\nEq. (9) states that the total PNS can be decomposed into a summation of the natural direct and natural indirect PNS, a desired property of causal mediation analysis.\n\nRemark 1. Researchers have considered the proportion of direct or indirect influence in the total influence, which captures how important each pathway is in explaining the total influence (VanderWeele 2013). However, the proportions of direct and indirect effects in the total effects under linear SCM $\\mathcal { M } ^ { L }$ or the proportions of NDE and NIE in TE may not always make sense since these quantities may take negative values. In contrast, the proportions of ND-PNS and NI-PNS in T-PNS are given by $\\begin{array} { r l } { \\mathrm { N D - P N S } ( y ; x ^ { \\prime } , x , c ) / \\mathrm { T - P N S } ( y ; x ^ { \\prime } , x , c ) } & { { } = } \\end{array}$ $\\begin{array} { r l r l r l r l r } {  } & { { } } & { y | Y _ { x ^ { \\prime } } } & { { }  } & { y } & { { } \\preceq } & { Y _ { x } , C } & { { } = } & { c ) } \\end{array}$ and $\\begin{array} { r l r } { \\mathrm { N I - P N S } ( y ; x ^ { \\prime } , x , c ) / \\mathrm { T - P N S } ( y ; x ^ { \\prime } , x , c ) } & { { } = } & { \\mathbb { P } ( y \\quad \\preceq } \\end{array}$\n\n$Y _ { x ^ { \\prime } , M _ { x } } | Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x } , C = c )$ , respectively, which do not take negative values. Additionally, the sum of the proportions of ND-PNS and NI-PNS is always equal to 1.\n\nRemark 2. Rubinstein, Cuellar, and Malinsky (2024) defined, for binary treatment, outcome, and mediator, the total mediated $\\mathrm { P o C }$ by $\\delta ( c ) ~ \\triangleq ~ \\mathbb { P } ( Y _ { 0 } ~ = ~ 0 | Y _ { 1 } ~ = ~ 1 , M _ { 1 } ~ =$ $1 , C = c )$ , the direct mediated PoC by $\\psi ( { \\boldsymbol { c } } ) \\triangleq \\mathbb { P } ( Y _ { 1 , M _ { 0 } } =$ $0 , Y _ { 0 , M _ { 0 } } = 0 | Y _ { 1 , M _ { 1 } } = 1 , M _ { 1 } = 1 , \\bar { C } = c \\rangle$ , and the indirect mediated PoC by $\\zeta ( c ) \\triangleq \\mathbb { P } ( Y _ { 1 , M _ { 0 } } = 1 , Y _ { 0 , M _ { 0 } } = 0 | Y _ { 1 , M _ { 1 } } =$ $1 , M _ { 1 } = 1 , C = c )$ . While we focus on the necessity and sufficiency of the treatment to provoke an event, their definitions of mediated $\\mathrm { P o C }$ differ from ours and are aimed at answering different questions. For example, their total mediated $\\mathrm { P o C }$ is motivated by the question: “Given that subjects would experience events $Y = 1$ and $M = 1$ had they taken a treatment $X = 1$ , what is the probability that they would not have experienced the event $Y \\ = \\ 1$ in the absence of the treatment?”. We note that their mediated PoC satisfy the property $\\delta ( c ) = \\psi ( c ) + \\zeta ( c )$ . We provide a detailed comparison in Appendix E in (Kawakami and Tian 2024).\n\n# Identification of CD-PNS, ND-PNS, and NI-PNS\n\nNext, we provide identification theorems for the direct and indirect PNSs we have introduced.\n\nAssumptions The identification of $\\mathrm { P o C }$ relies on monotonicity assumptions in the literature (Tian and Pearl 2000). We will make similar assumptions, specifically similar to those in (Kawakami, Kuroki, and Tian 2024).\n\nAssumption 2. Potential outcome $Y _ { x , m }$ has conditional $P D F { \\mathfrak { p } } _ { Y _ { x , m } | C = c }$ for each $x \\in \\Omega _ { X }$ , $m \\in \\Omega _ { M }$ , and $c \\in \\Omega _ { C }$ , and its support $\\{ y \\in \\Omega _ { Y } : { \\mathfrak { p } } _ { Y _ { x , m } | C = c } ( y ) \\neq 0 \\}$ is the same for each $x \\in \\Omega _ { X }$ , $m \\in \\Omega _ { M }$ , and $c \\in \\Omega _ { C }$ .\n\nAssumption 3. Potential outcome $Y _ { x ^ { \\prime } , M _ { x } }$ has conditional $P D F \\mathfrak { p } _ { Y _ { x ^ { \\prime } , M _ { x } } } | C { = } c$ for each $x ^ { \\prime } , x \\in \\Omega _ { X }$ and $c \\in \\Omega _ { C }$ , and its support $\\{ y ~ \\in ~ \\Omega _ { Y } ~ : ~ { \\mathfrak { p } } _ { Y _ { x ^ { \\prime } , M _ { x } } } | C { = } c ( y ) ~ \\not = ~ 0 \\}$ is the same for each $x ^ { \\prime } , x \\in \\Omega _ { X }$ and $c \\in \\Omega _ { C }$ .\n\nAssumptions 2 and 3 are reasonable for continuous variables. For example, potential outcomes $Y _ { x , m } , Y _ { x ^ { \\prime } , M _ { x } }$ often has $[ - \\infty , \\infty ]$ support, such as in linear SCM $\\mathcal { M } ^ { L }$ .\n\nWe assume the following monotonicity condition for identifying CD-PNS:\n\nAssumption 4 (Monotonicity over $f _ { Y } )$ . The function $f _ { Y } ( x , m , c , U _ { Y } )$ is either monotonic increasing on $U _ { Y }$ for all $x \\in \\Omega _ { X }$ , $m \\in \\Omega _ { M }$ , and $c \\in \\Omega _ { C }$ , or monotonic decreasing on $U _ { Y }$ for all $x \\in \\Omega _ { X }$ , $m \\in \\Omega _ { M }$ , and $c \\in \\Omega _ { C }$ , almost surely w.r.t. $\\mathbb { P } _ { U _ { Y } }$ .\n\nAlternatively, one may assume monotonicity over potential outcomes:\n\nAssumption $\\mathbf { \\nabla } \\cdot \\mathbf { \\vec { \\tau } }$ (Conditional monotonicity over $Y _ { x , m } )$ ) The potential outcomes $Y _ { x , m }$ satisfy: for any $x ^ { \\prime } , x \\in \\Omega _ { X } , m \\in$ $\\Omega _ { M }$ , $y ~ \\in ~ \\Omega _ { Y }$ , and $c \\in \\Omega _ { C }$ , either $\\mathbb { P } ( Y _ { x ^ { \\prime } , m } \\ \\prec \\ y \\ \\preceq $ $Y _ { x , m } | C = c ) = 0$ or $\\mathbb { P } ( Y _ { x , m } \\prec y \\preceq Y _ { x ^ { \\prime } , m } | C = c ) = 0$ .\n\nAssumptions 4 and $\\mathbf { \\nabla } _ { 4 } ,$ are equivalent under Assumption 2 (a straightforward extension of Theorem 4.1 in (Kawakami, Kuroki, and Tian 2024)). We note that the widely used linear SCM $\\mathcal { M } ^ { L }$ satisfies Assumption 4. Furthermore, another popular model, a nonlinear SCM with normal distribution $\\dot { \\mathcal { M } } ^ { N }$ , consisting of $Y : = f _ { Y } ( X , M , C ) + U _ { Y }$ and $M : = f _ { M } ( X , C ) + U _ { M }$ , where $U _ { Y } ~ \\sim ~ \\mathcal { N } ( \\mu _ { Y } , \\sigma _ { Y } ^ { 2 } )$ and $U _ { M } \\sim \\mathcal N ( \\mu _ { M } , \\sigma _ { M } ^ { 2 } )$ , also satisfies Assumptions 2-4.\n\nLet the compound function $f _ { Y } \\circ f _ { M }$ represent $\\left( f _ { Y } \\circ \\right.$ $\\begin{array} { r c l } { { f _ { M } ) ( x ^ { \\prime } , x , c , { \\tilde { U } } ) } } & { { = } } & { { f _ { Y } ( x ^ { \\prime } , f _ { M } ( x , c , U _ { M } ) , c , U _ { Y } ) } } \\end{array}$ for all $x ^ { \\prime } , x \\in \\Omega _ { X }$ and $c \\in \\Omega _ { C }$ , where $\\tilde { U } = ( U _ { Y } , U _ { M } )$ . We assume the following for identifying ND-PNS and NI-PNS:\n\nAssumption 5 (Monotonicity over $f _ { Y } \\circ f _ { M } )$ . The function $( f _ { Y } \\circ f _ { M } ) ( x ^ { \\prime } , x , c , \\tilde { U } )$ is either monotonic increasing on $\\tilde { U }$ for all $x ^ { \\prime }$ $, x \\in \\Omega _ { X }$ and $c \\in \\Omega _ { C }$ , or monotonic decreasing on $\\tilde { U }$ for all $x ^ { \\prime } , x \\in \\Omega _ { X }$ and $c \\in \\Omega _ { C }$ , almost surely w.r.t. $\\mathbb { P } _ { \\tilde { U } }$ .\n\nOr, alternatively,\n\nAssumption $\\bar { \\pmb { 5 } } ^ { \\prime }$ (Conditional monotonicity over $Y _ { x ^ { \\prime } , M _ { x } } )$ The potential outcomes $Y _ { x ^ { \\prime } , M _ { x } }$ satisfy: for any $x , x ^ { \\prime } , x ^ { \\prime \\prime } , x ^ { \\prime \\prime \\prime } \\in$ $\\Omega _ { X }$ , $y ~ \\in ~ \\Omega _ { Y }$ , and $c \\in \\Omega _ { C }$ , either $\\mathbb { P } ( Y _ { x ^ { \\prime } , M _ { x } } \\ \\prec \\ y \\ \\preceq$ $Y _ { x ^ { \\prime \\prime \\prime } , M _ { x ^ { \\prime \\prime } } } | C = c ) = 0$ or $\\begin{array} { r } { \\mathbb { P } ( Y _ { x ^ { \\prime \\prime \\prime } , M _ { x ^ { \\prime \\prime } } } \\prec y \\prec Y _ { x ^ { \\prime } , M _ { x } } | C = } \\end{array}$ $c ) = 0$ .\n\nSimilarly, Assumptions 5 and $5 ^ { \\circ }$ are equivalent under Assumption 3. We note that both the linear SCM $\\mathcal { M } ^ { L }$ and the nonlinear SCM with normal distribution $\\mathcal { M } ^ { N }$ satisfy Assumption 5 with $\\tilde { U } = U _ { Y } + U _ { M }$ .\n\nLemmas. Then, we obtain the following results.\n\nLemma 1. Under SCM $\\mathcal { M }$ , and Assumptions 2 and 4,\n\n$$\n\\begin{array} { r l r } {  { } } & { { } } & { = \\operatorname* { m a x } \\Big \\{ \\mathbb { P } ( Y _ { x ^ { \\prime } , m } \\prec y | C = c ) - \\mathbb { P } ( Y _ { x , m } \\prec y | C = c ) , 0 \\Big \\} . } \\end{array}\n$$\n\nLemma 2. Under SCM $\\mathcal { M }$ , and Assumptions $3$ and $5$ ,\n\n$$\n\\begin{array} { r l } & { = \\operatorname* { m a x } \\Big \\{ \\operatorname* { m i n } \\{ \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y | C = c ) , \\mathbb { P } ( Y _ { x ^ { \\prime } , M _ { x } } \\prec y | C = c ) \\} } \\\\ & { \\qquad - \\mathbb { P } ( Y _ { x } \\prec y | C = c ) , 0 \\Big \\} , } \\\\ & { \\mathrm { N I } \\mathrm { - } \\mathrm { P N S } ( y ; x ^ { \\prime } , x , c ) = \\operatorname* { m a x } \\Big \\{ \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y | C = c ) } \\\\ & { \\qquad - \\operatorname* { m a x } \\{ \\mathbb { P } ( Y _ { x } \\prec y | C = c ) , \\mathbb { P } ( Y _ { x ^ { \\prime } , M _ { x } } \\prec y | C = c ) \\} , 0 \\Big \\} . } \\end{array}\n$$\n\nThe lemmas mean that, under monotonicity, CD-PNS, NDPNS, and NI-PNS can be computed from the CDF of certain counterfactual outcomes.\n\nIdentification theorems. The CDF of the counterfactual outcomes $\\mathbb { P } ( Y _ { x , M _ { x ^ { \\prime } } } \\prec y | C = c )$ is identifiable under the sequential ignorability Assumption 1 by Proposition 1 as $\\mathbb { P } ( Y _ { x , M _ { x ^ { \\prime } } } \\prec y | C = c ) = \\rho ( y ; x ^ { \\prime } , x , c )$ , where we donote\n\n$$\n\\begin{array} { r } { \\rho ( y ; x ^ { \\prime } , x , c ) \\triangleq \\int _ { \\Omega _ { M } } \\mathbb { P } ( Y \\prec y | X = x ^ { \\prime } , M = m , C = c ) \\ d t } \\\\ { \\times \\mathfrak { p } ( M = m | X = x , C = c ) d m . \\qquad } \\end{array}\n$$\n\nThen, we obtain the following identification theorems by combining Lemmas 1 and 2 and Proposition 1:\n\nTheorem 1 (Identification of CD-PNS). Under SCM $\\mathcal { M }$ , and Assumptions 1, 2, and 4, CD-PNS is identifiable by\n\n$$\n\\begin{array} { r l } {  { \\mathrm { C D - P N S } ( y ; x ^ { \\prime } , x , m , c ) = } \\quad } & { } \\\\ & { \\operatorname* { m i n } \\Big \\{ \\mathbb { P } ( Y \\prec y | X = x ^ { \\prime } , M = m , C = c ) } \\\\ & { \\qquad - \\mathbb { P } ( Y \\prec y | X = x , M = m , C = c ) , 0 \\Big \\} . } \\end{array}\n$$\n\nTheorem 2 (Identification of ND-PNS and NI-PNS). Under SCM $\\mathcal { M }$ , and Assumptions 1, 3, and 5, ND-PNS and NI-PNS are identifiable by\n\n$$\n\\begin{array} { r l } & { \\mathrm { N D } { \\scriptstyle \\mathrm { \\mathrm { \\bf ~ N N S } } } ( y ; x ^ { \\prime } , x , c ) } \\\\ & { \\mathrm { \\quad ~ } = \\operatorname* { m a x } \\Big \\{ \\operatorname* { m i n } \\{ { \\mathbb { P } } ( Y \\prec y | X = x ^ { \\prime } , C = c ) , } \\\\ & { \\mathrm { \\quad ~ } \\rho ( y ; x ^ { \\prime } , x , c ) \\} - { \\mathbb { P } } ( Y \\prec y | X = x , C = c ) , 0 \\Big \\} , } \\\\ & { \\mathrm { \\quad ~ } ( 1 . } \\\\ & { \\mathrm { \\quad \\quad ~ N I } { \\scriptstyle \\mathrm { \\bf ~ N S } } ( y ; x ^ { \\prime } , x , c ) = \\operatorname* { m a x } \\Big \\{ { \\mathbb { P } } ( Y \\prec y | X = x ^ { \\prime } , C = c ) } \\\\ & { \\mathrm { \\quad ~ } - \\operatorname* { m a x } \\{ { \\mathbb { P } } ( Y \\prec y | X = x , C = c ) , \\rho ( y ; x ^ { \\prime } , x , c ) \\} , 0 \\Big \\} } \\end{array}\n$$\n\nAs a consequence, under SCM $\\mathcal { M }$ and Assumptions 1, 3, and 5, the proportions of ND-PNS and NI-PNS in T-PNS are also identifiable.\n\n# Direct and Indirect PNS with Evidence\n\nIn this section, we define CD-PNS, ND-PNS, and NI-PNS with evidence and provide corresponding identification theorems. Specifically, we consider two types of evidence:\n\n$$\n\\mathcal { E } \\triangleq ( X = x ^ { \\ast } , M = m ^ { \\ast } , Y \\in \\mathcal { T } _ { Y } ) ,\n$$\n\n$$\n\\mathcal { E } ^ { \\prime } \\triangleq ( X = x ^ { * } , Y \\in \\mathbb { Z } _ { Y } ) ,\n$$\n\nwhere $\\mathcal { T } _ { Y }$ is a half-open interval $[ y ^ { l } , y ^ { u } )$ or a closed interval $[ y ^ { l } , y ^ { u } ]$ w.r.t. $\\prec$ . PNS with evidence allows us to examine PNS for a specific subpopulation characterized by the evidence.\n\nThe main distinction between the evidence $\\mathcal { E }$ or ${ \\mathcal { E } } ^ { \\prime }$ and the subject’s covariates $C$ in the definition of CD-PNS, NDPNS, and NI-PNS (Def. 3) is that $C$ in the SCM $\\mathcal { M }$ are pre-treatment variables but $\\mathcal { E }$ are post-treatment variables. Conditioning on post-treatment variables differs from traditional conditioning on pre-treatment variables and has been discussed in the context of PN or PS (Pearl 1999) and the posterior causal effects (Lu et al. 2022; Li et al. 2023). They have applications in various fields, such as attribution of risk factors in public health and epidemiology, medical diagnosis of diseases, root-cause diagnosis in equipment and production processes, and reference measures for penalties in law.\n\n# Definitions of CD-PNS, ND-PNS, and NI-PNS with Evidence\n\nFirst, we define CD-PNS with evidence $\\mathcal { E }$ , and T-PNS, NDPNS, and NI-PNS with evidence $\\mathcal { E } ^ { \\prime } ^ { \\mathrm { ~ 1 ~ } }$ .\n\nDefinition 4 (CD-PNS, T-PNS, ND-PNS, and NI-PNS with evidence). CD-PNS with evidence $\\mathcal { E }$ , and T-PNS, ND-PNS, and NI-PNS with evidence ${ \\mathcal { E } } ^ { \\prime }$ are defined by\n\n$$\n\\begin{array} { r l } & { \\mathrm { C D - P N S } ( y ; x ^ { \\prime } , x , m , \\mathcal { E } , c ) } \\\\ & { \\quad \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } , m } \\prec y \\preceq Y _ { x , m } | \\mathcal { E } , C = c ) , } \\end{array}\n$$\n\n$$\n\\begin{array} { r } { \\mathrm { T } \\mathrm { \\mathrm { - } } \\mathrm { P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x } | \\mathcal { E } ^ { \\prime } , C = c ) , } \\end{array}\n$$\n\n$$\n\\mathrm { N D - P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c )\n$$\n\n$$\n\\begin{array} { r } { \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x } , Y _ { x ^ { \\prime } , M _ { x } } \\prec y | \\mathcal { E } ^ { \\prime } , C = c ) , } \\end{array}\n$$\n\n$$\n\\mathrm { N I - P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c )\n$$\n\n$$\n\\begin{array} { r } { \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x } , y \\preceq Y _ { x ^ { \\prime } , M _ { x } } | \\mathcal { E } ^ { \\prime } , C = c ) . } \\end{array}\n$$\n\nCD-PNS with evidence can answer questions: “What is the probability that the situation in the question (Q1) holds for the subjects, when, in reality, their treatment is $x ^ { * }$ , their mediator is $m ^ { * }$ , their outcome is in $\\mathcal { T } _ { Y }$ , and their covariates is $c ?$ ” ND-PNS and NI-PNS with evidence can answer questions: “What is the probability that the situation in the questions (Q2) and (Q3) hold, when, in reality, their treatment is $x ^ { * }$ , their outcome is in $\\mathcal { T } _ { Y }$ , and their covariates is $c ? ^ { \\prime \\prime }$ CD-PNS, ND-PNS, and NI-PNS with evidence can retrospectively answer questions for the specific subpopulation characterized by the evidence.\n\nThe following desired decomposition property holds:\n\n# Proposition 3.\n\n$$\n\\begin{array} { r l } & { \\mathrm { T } \\mathrm { \\mathrm { - } } \\mathrm { P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) } \\\\ & { \\mathrm { ~ } = \\mathrm { N D } \\mathrm { \\mathrm { - } } \\mathrm { P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) + \\mathrm { N I } \\mathrm { \\mathrm { - } } \\mathrm { P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) . } \\end{array}\n$$\n\nRemark 3. We do not use mediator information in evidence for T-PNS, ND-PNS, and NI-PNS because a more strict assumption is required for identification to exploit mediator information. In Appendix C in (Kawakami and Tian 2024), we provide an identification theorem (Theorem $\\mathbf { \\nabla } _ { 4 } ,$ ) of T-PNS, ND-PNS, and NI-PNS with evidence ${ \\mathcal { E } } ^ { \\prime } = ( X =$ $x ^ { * } , M \\in \\mathcal { T } _ { M } , Y \\in \\mathcal { T } _ { Y } )$ with an additional assumption, where $\\mathcal { T } _ { M }$ is a half-open interval $[ m ^ { l } , m ^ { u } )$ w.r.t. the total order on $\\Omega _ { M }$ .\n\nND-PN, NI-PN, ND-PS, and NI-PS. So far, we have focused our attention on PNS in the PoC family. It turns out that PN and PS, the other two members of the PoC family defined in Def. 1, can be computed as special cases of T-PNS with evidence. Specifically, PN is equivalent to T-PNS with the evidence ${ \\mathcal { E } } ^ { \\prime } { \\overset { - } { = } } ( y \\preceq { \\dot { Y } } , X = x )$ , and PS is equivalent to T-PNS with the evidence ${ \\mathcal { E } } ^ { \\prime } = ( Y \\prec y , X = x ^ { \\prime } )$ as follows.\n\nProposition 4. We have the following:\n\n$$\n\\begin{array} { r l r } & { } & { \\mathrm { P N } ( y ; x ^ { \\prime } , x , c ) = \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x } | y \\preceq Y , X = x , C = c ) , } \\\\ & { } & { ( 2 4 ) } \\\\ & { } & { \\mathrm { P S } ( y ; x ^ { \\prime } , x , c ) = \\mathbb { P } ( Y _ { x ^ { \\prime } } \\prec y \\preceq Y _ { x } | Y \\prec y , X = x ^ { \\prime } , C = c ) . } \\end{array}\n$$\n\nThen, direct and indirect PN and PS can be naturally defined by extending the definitions of ND-PNS and NI-PNS with evidence in Def. 4.\n\nDefinition 5 (ND-PN, NI-PN, ND-PS, and NI-PS). The natural direct PN (ND-PN), natural indirect PN (NI-PN), natural direct PS (ND-PS), and natural indirect PS (NI-PS) are defined by\n\n$$\n\\begin{array} { r l } & { \\mathrm { N I D } { \\cdot } \\mathrm { P N } ( y ; x ^ { \\prime } , x , c ) } \\\\ & { \\qquad \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } } \\cdot  { \\mathscr { S } } , Y _ { x ^ { \\prime } , M _ { x } } \\prec y | y \\preceq Y , X = x , C = c ) , } \\\\ & { \\mathrm { N I } { \\cdot } \\mathrm { P N } ( y ; x ^ { \\prime } , x , c ) } \\\\ & { \\qquad \\triangleq \\mathbb { P } ( Y _ { x ^ { \\prime } } \\cdot  { \\mathscr { S } } , y \\preceq Y _ { x ^ { \\prime } , M _ { x } } | y \\preceq Y , X = x , C = c ) , } \\\\ & { \\mathrm { N D } { \\cdot } \\mathrm { P S } ( y ; x ^ { \\prime } , x , c ) } \\\\ & { \\qquad \\triangleq \\mathbb { P } ( y \\preceq Y _ { x } , Y _ { x ^ { \\prime } } , M _ { x } \\prec  { y } | Y \\prec y , X = x ^ { \\prime } , C = c ) , } \\\\ & { \\mathrm { N I } { \\cdot } \\mathrm { P S } ( y ; x ^ { \\prime } , x , c ) } \\\\ & { \\qquad \\triangleq \\mathbb { P } ( y \\preceq Y _ { x } , y \\preceq Y _ { x ^ { \\prime } , M _ { x } } | Y \\prec y , X = x ^ { \\prime } , C = c ) . } \\end{array}\n$$\n\nND-PN, NI-PN, ND-PS, and NI-PS provide a measure of the necessity or the sufficiency of the treatment for the outcome through direct or indirect pathways. We have the desirable decomposition property that $\\begin{array} { l l } { \\displaystyle \\mathrm { P N } ( \\boldsymbol { y } ; \\boldsymbol { x } ^ { \\prime } , \\boldsymbol { x } , c ) } & { = } \\end{array}$ $\\begin{array} { r l r } { \\mathrm { { N D - P N } } ( y ; x ^ { \\prime } , x , c ) \\quad } & { { } + } & { \\quad \\mathrm { { N I - P N } } ( y ; x ^ { \\prime } , x , c ) } \\end{array}$ and $\\mathrm { P S } ( y ; x ^ { \\prime } , x , c ) = \\mathrm { N D } \\mathrm { - } \\mathrm { P S } ( y ; x ^ { \\prime } , x , c ) + \\mathrm { N I } \\mathrm { - } \\mathrm { P S } ( y ; x ^ { \\prime } , x , c ) .$ .\n\n# Identification of CD-PNS, T-PNS, ND-PNS, and NI-PNS with Evidence\n\nWe obtain the following two identification theorems under the same assumptions for Theorems 1 or 2.\n\nTheorem 3 (Identification of CD-PNS with evidence $\\mathcal { E }$ ). Let $\\mathcal { T } _ { Y }$ be a half-open interval $[ y ^ { l } , y ^ { u } )$ in evidence $\\mathcal { E }$ . Under $S C M \\mathcal { M }$ , and Assumptions 1, 2, and 4, for each $x ^ { \\prime } , x \\in \\Omega _ { X }$ , $m \\in \\Omega _ { M }$ , $y \\in \\Omega _ { Y }$ , and $c \\in \\Omega _ { C }$ , we have\n\n$$\n\\mathrm { C D - P N S } ( y ; x ^ { \\prime } , x , m , \\mathcal { E } , c ) = \\operatorname* { m a x } \\left\\{ \\alpha / \\beta , 0 \\right\\} ,\n$$\n\nwhere\n\n$$\n\\begin{array} { r l } & { \\alpha = \\operatorname* { m i n } \\Big \\{ \\mathbb { P } ( Y \\prec y | X = x ^ { \\prime } , M = m , C = c ) , } \\\\ & { \\qquad \\mathbb { P } ( Y \\prec y ^ { u } | X = x ^ { \\star } , M = m ^ { \\star } , C = c ) \\Big \\} } \\\\ & { \\qquad - \\operatorname* { m a x } \\Big \\{ \\mathbb { P } ( Y \\prec y | X = x , M = m , C = c ) , } \\\\ & { \\qquad \\mathbb { P } ( Y \\prec y ^ { t } | X = x ^ { \\star } , M = m ^ { \\star } , C = c ) \\Big \\} , } \\\\ & { \\beta = \\mathbb { P } ( Y \\prec y ^ { u } | X = x ^ { \\star } , M = m ^ { \\star } , C = c ) \\qquad ( 3 1 } \\\\ & { \\qquad - \\mathbb { P } ( Y \\prec y ^ { t } | X = x ^ { \\star } , M = m ^ { \\star } , C = c ) . \\qquad ( 3 2 } \\end{array}\n$$\n\n(B). If $\\mathbb { P } ( Y \\prec y ^ { u } | X = x ^ { * } , M = m ^ { * } , C = c ) = \\mathbb { P } ( Y \\prec$ $y ^ { l } | X = x ^ { * } , M = m ^ { * } , C = c )$ , then\n\n$$\n\\begin{array} { r l } & { \\mathrm { C D - P N S } ( y ; x ^ { \\prime } , x , m , \\mathcal { E } , c ) = \\mathbb { I } \\Big ( \\mathbb { P } ( Y \\prec y | X = x ^ { \\prime } , C = c ) } \\\\ & { \\mathrm { ~ } \\le \\mathbb { P } ( Y \\prec y ^ { l } | X = x ^ { * } , M = m ^ { * } , C = c ) } \\\\ & { \\mathrm { ~ } < \\mathbb { P } ( Y _ { x } \\prec y | C = c ) \\Big ) . } \\end{array}\n$$\n\nTheorem 4 (Identification of T-PNS, ND-PNS, and NI-PNS with evidence $\\mathcal { E } ^ { \\prime }$ ). Let $\\mathcal { T } _ { Y }$ be a half-open interval $[ y ^ { l } , y ^ { u } )$ in evidence ${ \\mathcal { E } } ^ { \\prime }$ . Under SCM $\\mathcal { M }$ , and Assumptions $I$ , 3, and $5$ , for each $x ^ { \\prime } , x \\in \\Omega _ { X }$ , $y \\in \\Omega _ { Y }$ , and $c \\in \\Omega _ { C }$ , we have\n\n(A). If $\\mathbb { P } ( Y \\prec y ^ { u } | X = x ^ { * } , C = c ) \\neq \\mathbb { P } ( Y \\prec y ^ { l } | X =$ $x ^ { * } , C = c )$ , then\n\n$$\n\\mathrm { T \\mathrm { - } P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) = \\operatorname* { m a x } \\left\\{ \\gamma ^ { T } / \\delta , 0 \\right\\} ,\n$$\n\n$$\n\\mathrm { N D - P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) = \\operatorname* { m a x } \\left\\{ \\gamma ^ { D } / \\delta , 0 \\right\\} ,\n$$\n\n$$\n\\mathrm { N I - P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) = \\operatorname* { m a x } \\left\\{ \\gamma ^ { I } / \\delta , 0 \\right\\} ,\n$$\n\nwhere\n\n$$\n\\begin{array} { r l } { \\gamma ^ { \\prime \\prime } = \\operatorname* { m i n } \\left\\{ \\| \\langle \\gamma - \\psi _ { \\delta } | X - x \\rangle ^ { \\prime } , C - \\psi _ { \\delta } \\right\\} , } & { } \\\\ { \\gamma _ { \\downarrow } \\gamma _ { \\downarrow } \\langle X - x \\rangle ^ { \\prime \\prime } , C - \\psi _ { \\delta } \\right\\} } & { { } } \\\\ { - \\operatorname* { m a x } \\left\\{ \\mathbf { F } ^ { \\prime } \\mathbf { X } - \\mathbf { g } \\right\\} \\mathbf { I } = - \\mathcal { L } \\mathcal { C } = \\phi , } & { { } \\quad \\quad \\mathrm { ( 2 7 ) } } \\\\ { - \\mathbf { I } - \\mathbf { g } , } & { \\quad \\quad \\mathbf { F } ^ { \\prime } \\mathbf { Y } ^ { \\prime } = \\psi _ { \\delta } , \\quad C - C \\psi _ { \\delta } \\big \\} , } \\\\ { \\mathbf { F } , } & { { } \\quad \\quad \\mathrm { ( 3 . ~ 7 . ~ } \\mathbf { ) } } \\\\ { \\gamma ^ { \\prime \\prime } = \\operatorname* { m a x } \\left\\{ \\mathbf { F } ^ { \\prime } \\mathbf { Y } ^ { \\prime } - \\mathbf { g } \\right\\} X = \\mathbf { z } ^ { \\prime } , C - \\mathbf { c } \\psi _ { \\delta } \\big \\} , } \\\\ { \\frac { \\lambda _ { \\mathrm { t } } ^ { \\prime \\prime } \\mathbf { F } ^ { \\prime } - \\mathbf { g } } { \\lambda _ { \\mathrm { t } } \\gamma _ { \\downarrow } \\gamma _ { \\downarrow } \\left\\{ X - x ^ { \\prime } , C - \\psi _ { \\delta } \\right\\} , \\quad \\mathrm { ( 2 7 ) } } } \\\\ { - \\operatorname* { m a x } \\left\\{ \\mathcal { Z } ( X - \\mathbf { g } ) X - \\mathbf { g } , \\quad C - c \\right\\} , } \\\\ { \\mathbf { F } , \\quad \\quad \\quad \\mathbf { B } = \\mathbf { F } ^ { \\prime } , \\quad \\quad \\quad \\mathrm { ( 3 . ~ 7 . ~ } \\mathbf { ) } } \\\\ { \\gamma ^ { \\prime } = \\operatorname* { m i n } \\left\\{ \\mathbf { F } ^ { \\prime } \\mathbf { \\gamma } \\mathbf { \\gamma } _ { \\ast \\mathrm { s } } , \\quad X - \\mathbf { z } ^ { \\prime } , C - \\psi _ { \\delta } \\right\\} , } & { { } \\quad \\mathrm { ( 3 . 8 ) } } \\\\ { - \\operatorname* { m a x } \\left\\{ \\mathbf { F } ^ { \\prime } \\mathbf { \\gamma } _ { \\ast \\mathrm { s } } , \\quad X - \\mathbf { g } , \\quad X - \\mathbf { g } , \\quad X - \\mathbf { g } \\right\\} , } & { } \\\\ { \\gamma ^ { \\prime } = \\operatorname* { m i n } \\left\\{ \\mathbf { F } ^ { \\prime } \\mathbf { \\gamma } _ { \\ast \\mathrm { s } } , \\quad X - \\mathbf { g } ^ { \\prime } , C - \\psi _ { \\delta } \\right\\} , } & { } \\\\  - \\operatorname* { m a x } \\left\\{ \\mathbf { F } ^ { \\prime } \\mathbf { \\gamma } _ { \\ast \\mathrm { s } } , \\quad X - \\mathbf { g } ^ { \\prime } , C - \\psi _  \\delta  \\end{array}\n$$\n\n$$\n\\begin{array} { c } { \\delta = \\mathbb { P } ( Y \\prec y ^ { u } | X = x ^ { * } , C = c ) } \\\\ { - \\mathbb { P } ( Y \\prec y ^ { l } | X = x ^ { * } , C = c ) . } \\end{array}\n$$\n\n(B). If $\\mathbb { P } ( Y \\prec y ^ { u } | X = x ^ { * } , C = c ) = \\mathbb { P } ( Y \\prec y ^ { l } | X =$ $x ^ { * } , C = c )$ , then\n\n$$\n\\begin{array} { r l } & { \\mathrm { T \\mathrm { \\mathrm { - } } P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) = \\mathbb { I } \\Big ( \\mathbb { P } ( Y \\prec y | X = x ^ { \\prime } , C = c ) \\le } \\\\ & { \\mathbb { P } ( Y \\prec y ^ { l } | X = x ^ { * } , C = c ) < \\mathbb { P } ( Y \\prec y | X = x , C = c ) \\Big ) , } \\end{array}\n$$\n\n$$\n\\begin{array} { r l } & { \\mathrm { N D \\mathrm { \\mathrm { \\bf - p N S } } } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) = \\mathbb { I } \\Big ( \\mathbb { P } ( Y \\prec y | X = x ^ { \\prime } , C = c ) \\Big ) \\leq } \\\\ & { \\quad \\quad \\mathbb { P } ( Y \\prec y ^ { l } | X = x ^ { * } , C = c ) < \\mathbb { P } ( Y \\prec y | X = x , C = c ) , } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\rho ( y ; x ^ { \\prime } , x , c ) \\leq \\mathbb { P } ( Y \\prec y ^ { l } | X = x ^ { * } , C = c ) \\Big ) , \\quad \\quad } \\end{array}\n$$\n\n$$\n\\begin{array} { r l } & { \\mathrm { N I \\mathrm { - } P N S } ( y ; x ^ { \\prime } , x , \\mathcal { E } ^ { \\prime } , c ) = \\mathbb { I } \\Bigl ( \\mathbb { P } ( Y \\prec y | X = x ^ { \\prime } , C = c ) \\le } \\\\ & { \\qquad \\mathbb { P } ( Y \\prec y ^ { l } | X = x ^ { * } , C = c ) < \\mathbb { P } ( Y \\prec y | X = x , C = c ) , } \\\\ & { \\qquad \\mathbb { P } ( Y \\prec y ^ { l } | X = x ^ { * } , C = c ) < \\rho ( y ; x ^ { \\prime } , x , c ) \\Bigr ) . \\qquad } \\end{array}\n$$\n\nRemark 4. When $\\mathcal { T } _ { Y }$ is a closed intervel $[ y ^ { l } , y ^ { u } ]$ in evidence $\\mathcal { E }$ or $\\mathcal { E } ^ { \\prime }$ , the identification results are obtained by changing $\\cdot \\mathcal { Y } \\prec y ^ { u } { } ^ { , , }$ to ${ } ^ { \\stackrel {  } { \\cdot } \\epsilon } Y \\preceq y ^ { u } { } ^ { , , }$ in Theorems 3 and 4.\n\nRemark 5. When $\\mathcal { T } _ { Y }$ is a point $y ^ { l } = y ^ { u }$ , the identification of T-PNS with evidence $( X = x ^ { * } , Y = y ^ { l } )$ in Theorem 4 reduces to Theorem 5.1 in (Kawakami, Kuroki, and Tian 2024). Thus, T-PNS identification in Theorem 4 is an extension of Theorem 5.1 in (Kawakami, Kuroki, and Tian 2024).\n\n# Simulated Experiments\n\n# Estimation from Finite Sample Size\n\nWe perform numerical experiments to illustrate the properties of the estimators from finite sample size. Theoretically, the estimators in this paper are consistent and it is expected that the estimates are reliable when the sample size is large.\n\nEstimation methods. All identification theorems in the paper compute all quantities through conditional CDFs. Using dataset $\\{ x _ { i } , m _ { i } , y _ { i } \\} _ { i = 1 } ^ { N }$ , we estimate the conditional CDFs by the empirical conditional CDFs, i.e., $\\hat { \\mathbb { P } } ( Y ~ \\prec ~ y | X ~ = ~ x , M ~ = ~ m ) ~ =$ $\\begin{array} { r } { \\sum _ { i = 1 } ^ { N } \\mathbb { I } ( y _ { i } \\prec y , x _ { i } = x , m _ { i } = m ) / \\sum _ { i = 1 } ^ { N } \\mathbb { I } ( x _ { i } = x , m _ { i } = m ) , } \\end{array}$ $\\begin{array} { r l r l } { \\hat { \\mathbb { P } } ( M } & { { } } & { = } & { } \\end{array}$ $\\begin{array} { r l r l r l } { \\mathrm { ~  ~ \\gamma ~ } } & { { } \\mathrm { ~  ~ \\psi ~ } } & { } & { { } m | X } & { } & { { } \\mathrm { ~  ~ \\psi ~ } } & { } & { { } \\mathrm { ~  ~ \\psi ~ } } \\end{array}$ = $\\textstyle \\sum _ { i = 1 } ^ { N } \\mathbb { I } ( m _ { i } = m , x _ { i } = x ) / \\sum _ { i = 1 } ^ { N } \\mathbb { I } ( x _ { i } = x )$ , and, in addition, $\\begin{array} { r } { \\hat { \\rho } ( y ; x ^ { \\prime } , x ) = \\sum _ { m \\in \\Omega _ { M } } \\hat { \\mathbb { P } } ( Y \\prec y | X = x ^ { \\prime } , M = } \\end{array}$ $m ) \\hat { \\mathbb { P } } ( M \\ = \\ m \\vert X \\ = \\ x )$ . We conduct the bootstrapping (Efron 1979) to reveal the distribution of the estimators, and provide the means and $9 5 \\%$ confidential intervals (CI) for each estimator.\n\nSetting. We assume the following SCM:\n\n$$\n\\begin{array} { r } { X : = \\mathrm { B e r n } ( 0 . 5 ) , M : = \\mathrm { B e r n } ( \\pi ( X ) ) , } \\\\ { Y : = \\mathrm { B e r n } ( \\pi ( X + M ) ) , \\quad \\quad } \\end{array}\n$$\n\nwhere $\\pi ( x ) = \\exp ( 1 + 0 . 5 x ) / ( 1 + \\exp ( 1 + 0 . 5 x ) )$ . $\\mathtt { B e r n } ( z )$ represents a Bernoulli distribution with probability $z . X , M$ , and $Y$ are all binary variables. We simulate 1000 times with the sample size $ { N } \\ = \\ 1 0 0 , 1 0 0 0 , 1 0 0 0 0$ , respectively, and assess the means and $9 5 \\%$ confidential intervals (CIs) of the estimators.\n\nResults. The ground truths of T-PNS, ND-PNS, and NIPNS are 0.074, 0.066, and 0.008. When $N = 1 0 0$ , the estimates are\n\nT-PNS: 0.083 (CI: [0.000, 0.228]), ND-PNS: 0.074 (CI: [0.000, 0.220]), NI-PNS: 0.009 (CI: [0.000, 0.046]).\n\nWhen $N = 1 0 0 0$ , the estimates are T-PNS: 0.075 (CI: [0.029, 0.125]), ND-PNS: 0.068 (CI: [0.021, 0.116]), NI-PNS: 0.007 (CI: [0.000, 0.017]).\n\nWhen $N = 1 0 0 0 0$ , the estimates are\n\nT-PNS: 0.074 (CI: [0.060, 0.088]), ND-PNS: 0.067 (CI: [0.052, 0.082]), NI-PNS: 0.008 (CI: [0.005, 0.011]).\n\nWhen the sample size is small ( $N = 1 0 0 ^ { \\circ }$ ), the estimators have relatively wide $9 5 \\%$ CIs. When the sample size is large enough ( $N = 1 0 0 0$ or $N = 1 0 0 0 0$ ), the estimators are close to the ground truths and have relatively narrow $9 5 \\%$ CIs. We perform additional experiments for T-PN, ND-PN, NI-PN, T-PS, ND-PS, and NI-PS and the results are presented in Appendix F in (Kawakami and Tian 2024).\n\n# Illustration of the Proposed Measures\n\nTo illustrate the behavior of the proposed direct and indirect PoC measures, we simulate data from an SCM and plot the measures against the covariate. The results are discussed in Appendix F in (Kawakami and Tian 2024).\n\n# Application to a Real-world Dataset\n\nWe show an application to a real-world psychology dataset.\n\nDataset. We take up a dataset from the Job Search Intervention Study (JOBS II) (Vinokur and Schul 1997). This dataset is open through the R package “mediation” (https: //cran.r-project.org/web/packages/mediation/index.html). JOBS II was a randomized job training intervention for unemployed subjects aiming at increasing the prospect of reemployment and improving their mental health. In the experiment, the unemployed workers were randomly assigned to treatment and control groups. Those in the treatment group participated in job-skills workshops, and they learned job-search skills and coping strategies for dealing with setbacks in the job-search process. Those in the control group received a booklet of job-search tips. In follow-up interviews, a measure of depressive symptoms based on the Hopkins Symptom Checklist was assessed. The sample size is 899 with no missing values.\n\nVariables. Let the randomly assigned interventions be treatment variable $( X )$ (treat), which takes 0 for the control group and 1 for the treatment group. We choose the measure of depressive symptoms based on the Hopkins Symptom Checklist (depress2) as the outcome $( Y )$ . We consider job-search self-efficacy $( M )$ (job seek) as a discrete mediating variable. We set $C = \\emptyset$ . We let the threshold of the depression be $y = 3$ in all the definitions of $\\mathrm { P o C }$ variants, and let $x ^ { \\prime } = 0$ and $x = 1$ . We assume Assumptions 1-3. These are reasonable because the interventions are randomly assigned, and the linear model used in the previous study (Vinokur and Schul 1997) satisfies these assumptions. On this dataset, it is reasonable that $X = 0$ increases the depression compared to $X = 1$ and we assume $\\cdot \\mathbf { \\cdot }$ and $5 ^ { \\circ }$ for monotonic increasing. Assumption $\\mathbf { \\nabla } _ { 4 } ,$ for monotonic increasing represents $\\mathbb { P } ( Y _ { 1 , m } \\ \\succ \\ y \\ \\succeq \\ Y _ { 0 , m } | C \\ = \\ c ) \\ = \\ 0 .$ , which means that there do not exist subjects whose potential depression score when setting the value of job-search self-efficiency by $m$ and receiving no intervention is under the given threshold $y$ , and whose potential depression score when setting the value of job-search self-efficiency by $m$ and receiving an intervention is over the given threshold $y$ . This seems reasonable. Assumption $5 ^ { \\circ }$ for monotonic increasing represents $\\mathbb { P } ( Y _ { 1 , M _ { 1 } } \\ \\succ \\ y \\ \\succeq \\ Y _ { 1 , M _ { 0 } } | C = c ) = 0 .$ , $\\mathbb { P } ( Y _ { 1 , M _ { 1 } } \\ \\succ \\ y \\ \\succeq \\ Y _ { 0 , M _ { 1 } } | C \\ = \\ c ) \\ = \\ 0$ , $\\mathbb { P } ( Y _ { 1 , M _ { 1 } } \\ \\succ \\ y \\ \\succeq$ $Y _ { 0 , M _ { 0 } } | C = c ) = 0 \\ :$ , $\\mathbb { P } ( Y _ { 1 , M _ { 0 } } \\succ y \\succeq Y _ { 0 , M _ { 0 } } | C = c ) = 0$ , and $\\mathbb { P } ( Y _ { 0 , M _ { 1 } } \\ \\succ \\ y \\ \\succeq \\ Y _ { 0 , M _ { 0 } } | C \\ = \\ c ) \\ = \\ 0$ . For example, $\\mathbb { P } ( Y _ { 1 , M _ { 1 } } \\succ y \\subset Y _ { 1 , M _ { 0 } } | C = c ) = 0$ means that there do not exist subjects whose potential depression score when receiving an intervention and keeping the value of job-search self-efficiency by $M _ { 0 }$ is under the given threshold $y$ , and whose potential depression score when receiving an intervention and keeping the value of job-search self-efficiency by $M _ { 1 }$ is over the given threshold $y$ . This also seems reasonable.\n\nResults. The estimated T-PNS is $2 3 . 8 4 0 \\%$ (CI: $[ 1 9 . 0 2 1 \\% , 2 9 . 2 5 4 \\% ] )$ . Then, we consider the following three questions:\n\n(Q1’). Would the intervention be necessary and sufficient to   \ncure the depression had the job-search self-efficacy been   \nfixed to a value $( m = 5$ )?   \n(Q2’). Would the intervention still be necessary and   \nsufficient to cure the depression had there been no influence   \nvia the job-search self-efficacy?   \n(Q3’). Would the intervention still be necessary and   \nsufficient to cure the depression had the influence only   \nexisted via the job-search self-efficacy?\n\nWe evaluate CD-PNS $m = 5 ^ { \\cdot }$ ), ND-PNS, and NI-PNS specified in Def. 3 and obtain the following results:\n\nCD-PNS: $7 . 4 8 4 \\%$ (CI: $[ 0 . 0 0 0 \\% , 4 1 . 6 7 6 \\% ] \\rangle$ , ND-PNS: $0 . 0 0 0 \\%$ (CI: $[ 0 . 0 0 0 \\% , 0 . 0 0 0 \\% ] \\mathrm { , }$ , NI-PNS: $2 3 . 8 4 0 \\%$ (CI: $[ 1 9 . 0 2 1 \\% , 2 9 . 2 5 4 \\% ] )$ ).\n\nCD-PNS, ND-PNS, and NI-PNS answer the questions (Q1’), (Q2’), and (Q3’), respectively. CD-PNS is less than T-PNS. T-PNS is equal to NI-PNS, and this means that the necessity and sufficiency of the treatment is entirely due to the indirect influence via the mediator. The proportions of ND-PNS and NI-PNS in T-PNS are 0 and 1.\n\nWhile Vinokur and Schul (1997) reported both direct and indirect effects as statistically significant, our results decompose the total influence entirely into the indirect. However, this does not contradict the observation that treatment has a direct effect on the outcome. Our results imply that the treatment would be necessary and sufficient at the same level of T-PNS had the influence only existed via the mediator, and the treatment would not be necessary and sufficient had there been no influence via the mediator. Our results do not imply that the treatment has no direct effect on outcome.\n\nNext, we study PoC for a specific subpopulation described by evidence. We evaluate T-PNS, CD-PNS $( m = 5 )$ ), NDPNS, and NI-PNS with evidence specified in Def. 4. We consider the evidence of $x ^ { * } = 0$ , $\\mathcal { T } _ { Y } \\overset { \\cdot } { = } [ y ^ { l } , y ^ { u } )$ where $y ^ { l } = 1 . 5$ and $y ^ { u } = 2 . 5$ for ND-PNS and NI-PNS, and additionally $m ^ { * } = 5$ for CD-PNS. We obtain:\n\nT-PNS with evidence: $5 7 . 8 9 9 \\% ( \\mathrm { C I } \\colon [ 3 9 . 1 3 0 \\% , 7 6 . 1 9 0 \\% ] )$ ,\n\nCD-PNS with evidence: $0 . 0 0 0 \\% ( \\mathrm { C I } \\colon [ 0 . 0 0 0 \\% , 0 . 0 0 0 \\% ] ) ^ { 2 }$ , ND-PNS with evidence: $0 . 0 0 0 \\%$ (CI: $[ 0 . 0 0 0 \\% , 0 . 0 0 0 \\% ]$ ), NI-PNS with evidence: $5 7 . 8 9 9 \\% ( \\mathrm { C I } \\colon [ 3 9 . 1 3 0 \\% , 7 6 . 1 9 0 \\% ] )$ .\n\nCD-PNS, ND-PNS, and NI-PNS can answer the questions (Q1’), (Q2’), and (Q3’), respectively, for the subpopulation specified by the evidence. CD-PNS is 0, and the proportions of ND-PNS and NI-PNS in T-PNS are 0 and 1. T-PNS and NI-PNS for this subpopulation are larger than those of the whole population.\n\n# Conclusion\n\nWe consider mediation analysis for PoC and introduce new direct and indirect variants of $\\mathrm { P o C }$ to represent the necessity and sufficiency of the treatment to produce an outcome event directly or through a mediator. We provide identification theorems for each type of PoC we introduce. The results expand the family of PoC and provide tools for researchers to answer more sophisticated causal questions. In addition, we show in Appendix D in (Kawakami and Tian 2024) how these direct and indirect variants of PoC look like for binary treatment, outcome, and mediator variables. In settings where the identification assumptions (sequential ignorability, monotonicity) do not hold, bounding (Tian and Pearl 2000; Dawid, Musio, and Murtas 2017; Dawid, Humphreys, and Musio 2024; Li and Pearl 2024a) or sensitivity analysis (Imai, Keele, and Tingley 2010; Imai, Keele, and Yamamoto 2010; VanderWeele 2016) is desired. Also, researchers are often interested in path-specific effects, of which direct and indirect effects are special instances (Daniel et al. 2015; Xia and Chan 2022; Zhou and Yamamoto 2023). Extending our results to these cases will be interesting future work.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了因果中介分析中概率因果（Probabilities of Causation, PoC）的分解问题，特别是如何量化治疗（treatment）通过不同因果路径（直接或间接）对结果（outcome）的必要性和充分性。\\n> *   该问题在决策制定（如医疗干预、政策评估）和可解释人工智能（XAI）中具有关键价值，能够帮助理解处理通过中介变量（mediator）的因果路径。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了三种新的PoC变体：受控直接PNS（CD-PNS）、自然直接PNS（ND-PNS）和自然间接PNS（NI-PNS），用于量化处理通过不同路径的必要性和充分性。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   提出了CD-PNS、ND-PNS和NI-PNS的定义，并证明了它们在观测数据下的可识别性（identification theorems）。\\n> *   展示了这些方法在心理学数据集（JOBS II）上的实际应用，验证了其有效性。T-PNS的估计值为23.840%（CI: [19.021%, 29.254%]），且NI-PNS占比为100%，表明治疗的必要性完全通过中介变量实现。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   通过定义CD-PNS、ND-PNS和NI-PNS，将总PoC分解为直接和间接路径的贡献，从而量化治疗通过中介变量的必要性（necessity）和充分性（sufficiency）。\\n> *   其有效性依赖于结构因果模型（SCM）和潜在结果框架，通过反事实条件（counterfactual conditions）来定义和识别这些概率。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前工作（如Pearl 1999）仅定义了总PoC（PNS），未区分直接和间接路径的贡献。\\n> *   **本文的改进：** 通过引入CD-PNS、ND-PNS和NI-PNS，能够分别量化治疗通过固定中介（CD-PNS）、排除中介（ND-PNS）和仅通过中介（NI-PNS）的必要性和充分性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. **定义PoC变体：** 根据反事实条件定义CD-PNS、ND-PNS和NI-PNS（见公式6-8）。\\n> *   2. **识别定理：** 在序贯可忽略性（sequential ignorability）和单调性（monotonicity）假设下，推导出这些PoC的识别公式（见定理1-2）。\\n> *   3. **估计方法：** 使用经验条件CDF（empirical conditional CDF）从观测数据中估计PoC。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   论文未明确提供基线模型对比。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在估计准确性上：** 论文通过模拟实验验证了提出的PoC变体在有限样本量下的估计准确性。随着样本量增加（N=1000或N=10000），估计值接近真实值，且置信区间变窄。\\n> *   **在实际应用上：** 在心理学数据集（JOBS II）上的应用显示，T-PNS为23.840%（CI: [19.021%, 29.254%]），NI-PNS为23.840%（CI: [19.021%, 29.254%]），表明处理的必要性和充分性完全通过中介变量实现。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n> **提取与格式化要求**\\n> *   概率因果 (Probabilities of Causation, PoC)\\n> *   控制直接概率 (Controlled Direct Probability of Necessity and Sufficiency, CD-PNS)\\n> *   自然直接概率 (Natural Direct Probability of Necessity and Sufficiency, ND-PNS)\\n> *   自然间接概率 (Natural Indirect Probability of Necessity and Sufficiency, NI-PNS)\\n> *   因果中介分析 (Causal Mediation Analysis, N/A)\\n> *   结构因果模型 (Structural Causal Model, SCM)\\n> *   可解释人工智能 (Explainable Artificial Intelligence, XAI)\\n> *   序贯可忽略性 (Sequential Ignorability, N/A)\"\n}\n```"
}