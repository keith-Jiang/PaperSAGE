{
    "source": "Semantic Scholar",
    "arxiv_id": "2405.15443",
    "link": "https://arxiv.org/abs/2405.15443",
    "pdf_link": "https://arxiv.org/pdf/2405.15443.pdf",
    "title": "Fairness-Accuracy Trade-Offs: A Causal Perspective",
    "authors": [
        "Drago Pleƒçko",
        "E. Bareinboim"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-05-24",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 3,
    "influential_citation_count": 0,
    "institutions": [
        "Columbia University"
    ],
    "paper_content": "# Fairness-Accuracy Trade-Offs: A Causal Perspective\n\nDrago PlecÀáko, Elias Bareinboim\n\nDepartment of Computer Science, Columbia University dp3144@columbia.edu, eb@cs.columbia.edu\n\n# Abstract\n\nWith the widespread adoption of AI systems, many of the decisions once made by humans are now delegated to automated systems. Recent works in the literature demonstrate that these automated systems, when used in socially sensitive domains, may exhibit discriminatory behavior based on sensitive characteristics such as gender, sex, religion, or race. In light of this, various notions of fairness and methods to quantify discrimination have been proposed, also leading to the development of numerous approaches for constructing fair predictors. At the same time, imposing fairness constraints may decrease the utility of the decision-maker, highlighting a tension between fairness and utility. This tension is also recognized in legal frameworks, for instance in the disparate impact doctrine of Title VII of the Civil Rights Act of 1964 ‚Äì in which specific attention is given to considerations of business necessity ‚Äì possibly allowing the usage of proxy variables associated with the sensitive attribute in case a high-enough utility cannot be achieved without them. In this work, we analyze the tension between fairness and accuracy from a causal lens for the first time. We introduce the notion of a path-specific excess loss (PSEL) that captures how much the predictor‚Äôs loss increases when a causal fairness constraint is enforced. We then show that the total excess loss (TEL), defined as the difference between the loss of predictor fair along all causal pathways vs. an unconstrained predictor, can be decomposed into a sum of more local PSELs. At the same time, enforcing a causal constraint often reduces the disparity between demographic groups. Thus, we introduce a quantity that summarizes the fairness-utility trade-off, called the causal fairness/utility ratio, defined as the ratio of the reduction in discrimination vs. the excess in the loss from constraining a causal pathway. This quantity is particularly suitable for comparing the fairnessutility trade-off across different causal pathways. Finally, as our approach requires causally-constrained fair predictors, we introduce a new neural approach for causally-constrained fair learning. Our approach is evaluated across multiple real-world datasets, providing new insights into the tension between fairness and accuracy.\n\n# 1 Introduction\n\nAutomated decision-making systems based on machine learning and artificial intelligence are now commonly implemented in various critical sectors of society such as hiring, university admissions, law enforcement, credit assessments, and health care (Khandani, Kim, and Lo 2010; Mahoney and Mohen 2007; Brennan, Dieterich, and Ehret 2009). These technologies now significantly influence the lives of individuals and are frequently used in high-stakes settings (Topol 2019; Berk et al. 2021; Taddeo and Floridi 2018). As these systems replace or augment human decision-making processes, concerns about fairness and bias based on protected attributes such as race, gender, or religion have become a prominent consideration in the ML literature. The available data used to train automated systems may contain past and present societal biases as an imprint and therefore has the potential to perpetuate or even exacerbate discrimination against protected groups. This is highlighted by reports on biases in systems for sentencing (Angwin et al. 2016), facial recognition (Buolamwini and Gebru 2018), online ads (Sweeney 2013; Datta, Tschantz, and Datta 2015), and system authentication (Sanburn 2015), among many others. Despite the promise of AI to enhance human decision-making, the reality is that these technologies can also reflect or worsen societal inequalities. As alluded to before, the issue does not arise uniquely from the usage of automated systems; humandriven decision-making has long been analyzed in a similar fashion. Evidence of bias in human decision-making is abundant, including studies on the gender wage gap (Blau and Kahn 1992, 2017) and racial disparities in legal outcomes (Sweeney and Haney 1992; Pager 2003). Therefore, without proper care about fairness and transparency of the new generation of AI systems, it is unclear what its impact will be on the historically discriminated groups.\n\nWithin the growing literature on fair machine learning, a plethora of fairness definitions have been proposed. Commonly considered statistical criteria, among others, include demographic parity (independence (Darlington 1971)), equalized odds (separation (Hardt, Price, and Srebro 2016)), and calibration (sufficiency (Chouldechova 2017)). These definitions, however, have been shown as mutually incompatible (Barocas and Selbst 2016; Kleinberg, Mullainathan, and Raghavan 2016). Despite a number of proposals, there is still a lack of consensus on what the appropriate measures of fairness are, and how statistical notions of fairness could incorporate moral values of the society at large. For this reason, a number of works explored the causal approaches to fair machine learning (Kusner et al. 2017; Kilbertus et al. 2017; Nabi and Shpitser 2018; Zhang and Bareinboim 2018b,a; Wu et al. 2019; Chiappa 2019; PleÀácko and Meinshausen 2020), and an in-depth discussion can be found in (PleÀácko and Bareinboim 2024). The main motivation for doing so is that the causal approach may allow the system designers to attribute the observed disparities between demographic groups to the causal mechanisms that underlie and generate them in the first place. In this way, by isolating disparities transmitted along different causal pathways, one obtains a more fine-grained analysis, and the capability to decide which causal pathways are deemed as unfair or discriminatory. More fundamentally, such considerations also form the basis of the legal frameworks for assessing discrimination in the United States and Europe. For instance, in the context of employment law, the disparate impact doctrine within the Title VII of the Civil Rights Act of 1964 (Act 1964) disallows any form of discrimination that results in a too large of a disparity between groups of interest. A core aspect of this doctrine, however, is the notion of business necessity (BN) or job-relatedness. Considerations of business necessity may allow variables correlated with the protected attribute to act as a proxy, and the law does not necessarily prohibit their usage due to their relevance to the business itself (or more broadly the utility of the decision-maker). Often, the wording that is used is that to argue business necessity in front of a court of law, the plaintiff needs to demonstrate that ‚Äúthere is no practice that is less discriminatory and achieves the same utility‚Äù (Els 1993). This concept illustrates the tension between fairness and utility, and demonstrates that we cannot be oblivious to considerations of utility from a legal standpoint. Moreover, demonstrating that a sufficient loss in accuracy results from imposing a fairness constraint has been previously used to justify business necessity considerations in some rulings of the European Court of Justice (Adams-Prassl, Binns, and KellyLyth 2023; Weerts et al. 2023), emphasizing the importance of the topic studied in this paper.\n\nRelated work. We mention three parts of related literature. First, we mention the literature exploring fairness-utility trade-offs, such as (Corbett-Davies et al. 2017). The essential argument is that an unconstrained predictor always achieves a greater or equal utility than a constrained one. Many works find that introducing fairness constraints reduces utility (Mitchell et al. 2021), and propose ways of handling the fairness-utility trade-off (Fish, Kun, and Lelkes 2016).\n\nHowever, other works in this literature still seem to be divided on the issue of whether trade-offs exist. For instance, (Rodolfa, Lamba, and Ghani 2021) finds that fairness and utility trade-offs are negligible in practice, while others argue that such trade-offs need not even exist (Maity et al. 2020; Dutta et al. 2020). Naturally, the implications on the predictor‚Äôs utility will strongly depend on the exact type of the fairness constraint that is enforced, and works that do not find a trade-off often focus on equality of odds (Hardt, Price, and Srebro 2016) $( \\widehat { Y } \\bot \\bot X \\mid Y )$ or (multi)calibration (Chouldechova 2017) $( Y \\bot \\bot X \\mid { \\widehat { Y } } )$ . Notably, the former metric always allows for the perfectbpredictor ${ \\widehat { Y } } = Y$ , and thus in settings with good predictive power, the cbost of enforcing this constraint may indeed be negligible. The latter metric allows for the $L _ { 2 }$ -optimal prediction score, and improving miscalibration may sometimes also yield improvements in utility by serving as a type of regularization.\n\nFinally, in the causal fairness literature, the tension between fairness and utility has been largely unexplored. Some exceptions include works such as (Nilforoshan et al. 2022) which shows that for a decision policy that satisfies a causal fairness constraints, it is almost always possible to find another decision that has a higher utility and the same total variation (TV) measure. (Plecko and Bareinboim 2024a) performs a causal explanation on a decision score used for constructing a policy, and discusses how disparities in the decision score may influence utility. Our main aim of this paper is to fill in this gap, and provide a systematic way of analyzing the fairness-accuracy trade-off from a causal lens, and show that fairness and utility are almost always in a trade-off.\n\n# 1.1 Motivating Example\n\nWe illustrate our approach in a simple linear setting:\n\nExample 1 (Linear Fairness-Accuracy Causal Trade-Offs). Consider variables $X , W , Y$ behaving according to the following linear system of equations:\n\n$$\n\\begin{array} { l } { { X  B e r n o u l l i ( 0 . 5 ) } } \\\\ { { W  \\beta X + \\epsilon _ { w } } } \\\\ { { Y  \\alpha X + \\gamma W + \\epsilon _ { y } , } } \\end{array}\n$$\n\nwhere $\\epsilon _ { w } \\sim N ( 0 , \\sigma _ { w } ^ { 2 } ) , \\epsilon _ { y } \\sim N ( 0 , \\sigma _ { y } ^ { 2 } )$ . Variable $X$ is the protected attribute, and $Y$ is the outcome of interest. The causal diagram of Eqs. 1-3 is shown in Fig. 2 (with the $Z$ set empty). Attribute $X$ can influence $Y$ along two different pathways: the direct path $X  Y$ , and the indirect path $X  W  Y$ . Therefore, for considering fairness, we focus on fair linear predictors $\\widehat { Y } ^ { S }$ of the form\n\n$$\n\\widehat { Y } ^ { S } = \\widehat { \\alpha } _ { S } X + \\widehat { \\gamma } _ { S } W ,\n$$\n\nwhere the predictorb $\\widehat { Y } ^ { S }$ removes effects in the set $S$ , with $S$ ranging in $\\{ \\emptyset , D E , I E , \\{ D E , I E \\} \\}$ $\\cdot { \\cal D } E ,$ , IE stand for direct and indirect effects, and any subset of these could be removed). For instance, the optimal predictor ${ \\widehat { Y } } ^ { \\varnothing }$ , which is not subject to any fairness constraints, has the cboefficients\n\n$$\n\\hat { \\alpha } _ { \\varnothing } = \\alpha , \\hat { \\gamma } _ { \\varnothing } = \\gamma ,\n$$\n\nwhich are the ordinary least squares (OLS) coefficients. Therefore, its mean-squared error (MSE) can be computed as $\\mathbb { E } [ Y - \\widehat { Y } ^ { \\varnothing } ] ^ { 2 } = \\sigma _ { y } ^ { 2 }$ . The $D E$ -fair predictor $\\widehat { Y } ^ { D E }$ , which has the direct ebffect constrained to zero, has coe fbicients\n\n$$\n\\hat { \\alpha } _ { D E } = 0 , \\hat { \\gamma } _ { D E } = \\gamma .\n$$\n\nThe fully-fair predictor, labeled $\\widehat { Y } ^ { \\{ D E , I E \\} }$ , has both direct and indirect effects constrained to zebro, and thus has coefficients\n\n$$\n\\hat { \\alpha } _ { \\{ D E , I E \\} } = 0 , \\hat { \\gamma } _ { \\{ D E , I E \\} } = 0 .\n$$\n\nThus, the corresponding MSE values for Y DE and Y {DE, IE} can be computed as (see Appendix A for cobmputationbdetails):\n\n$$\n\\begin{array} { l } { { \\displaystyle \\mathbb { E } [ Y - \\widehat { Y } ^ { D E } ] ^ { 2 } = \\sigma _ { y } ^ { 2 } + \\frac { \\alpha ^ { 2 } } 2 , } } \\\\ { { \\displaystyle \\mathbb { E } [ Y - \\widehat { Y } ^ { \\{ D E , I E \\} } ] ^ { 2 } = \\sigma _ { y } ^ { 2 } + \\frac { \\alpha ^ { 2 } + \\gamma ^ { 2 } \\beta ^ { 2 } + \\alpha \\gamma \\beta } 2 + \\gamma ^ { 2 } \\sigma _ { w } ^ { 2 } } . } \\end{array}\n$$\n\nOur goal is to decompose the total excess loss (TEL) originating from imposing the fairness constraints, defined as:\n\n$$\n\\begin{array} { r } { T E L : = \\underbrace { \\mathbb { E } [ Y - \\widehat { Y } ^ { \\{ D E , I E \\} } ] ^ { 2 } } _ { f u l l y \\cdot f a i r p r e d i c t o r ^ { \\prime } s l o s s } - \\underbrace { \\mathbb { E } [ Y - \\widehat { Y } ^ { \\varnothing } ] ^ { 2 } } _ { u n c o n s t r a i n e d l o s s } . } \\end{array}\n$$\n\nTEL measures the excess loss (in terms of the increase in the MSE, compared to the unconstrained predictor) originating from the removal of the direct and indirect effects. This quantifies the excess loss traded off for enforcing fairness constraints. Our goal is to decompose the TEL to obtain pathspecific contributions originating from the removal of direct and indirect effects as follows:\n\n$$\n\\begin{array} { r l } & { T E L = \\underbrace { \\mathbb { E } [ Y - \\widehat { Y } ^ { D E } ] ^ { 2 } - \\mathbb { E } [ Y - \\widehat { Y } ^ { \\theta } ] ^ { 2 } } _ { T e r m I = e x e s s D E l o s s } } \\\\ & { \\quad + \\underbrace { \\mathbb { E } [ Y - \\widehat { Y } ^ { \\{ D E , \\ : D E \\} } ] ^ { 2 } - \\mathbb { E } [ Y - \\widehat { Y } ^ { D E } ] ^ { 2 } } _ { T e r m I I = e x e s t E l o s s } } \\\\ & { \\quad = \\underbrace { \\frac { \\alpha ^ { 2 } } { 2 } } _ { T e r m I } + \\underbrace { \\frac { \\gamma ^ { 2 } \\beta ^ { 2 } + \\alpha \\gamma \\beta } { 2 } + \\gamma ^ { 2 } \\sigma _ { w } ^ { 2 } } _ { T e r m I I } . } \\end{array}\n$$\n\nTerm I is the direct effect excess loss, incurred by constraining the direct effect to $O$ . Term $\\boldsymbol { { \\mathit { I I } } }$ is the indirect effect excess loss, incurred by constraining the indirect effect to $O$ .\n\nAt the same time, enforcing fairness constraints may reduce the disparity between groups. For any predictor $\\widehat { Y }$ , we can measure the disparity using the difference in cond tbional expectations, called the total variation measure, defined as $T V _ { x _ { 0 } , x _ { 1 } } ( \\widehat { y } ) = \\mathbb { E } [ \\widehat { Y } \\mid x _ { 1 } ] - \\mathbb { E } [ \\widehat { Y } \\mid x _ { 0 } ]$ . This measure is sometimes alsbo referrebd to as the pabrity gap. Similarly as for the $T E L ,$ we compute the decrease in group disparity associated with removing $D E$ and $I E$ , by comparing the $T V$ measures of the fully-fair $\\widehat { Y } ^ { \\{ D E , I E \\} }$ and unconstrained ${ \\widehat { Y } } ^ { \\varnothing }$ , and computing the $T V$ differebnce $T V D$ , for short), definedb as:\n\n$$\n\\begin{array} { r l } & { T V D = T V ( \\widehat { Y } ^ { \\{ D E , I E \\} } ) - T V ( \\widehat { Y } ^ { \\emptyset } ) } \\\\ & { \\qquad = \\underbrace { ( \\mathbb { E } [ \\widehat { Y } ^ { \\{ D E , I E \\} } \\mid x _ { 1 } ] - \\mathbb { E } [ \\widehat { Y } ^ { \\{ D E , I E \\} } \\mid x _ { 0 } ] ) } _ { d i s p a r i t y d f e r r e m o v i n g D E , I E } } \\\\ & { \\qquad - \\underbrace { ( \\mathbb { E } [ \\widehat { Y } ^ { \\emptyset } \\mid x _ { 1 } ] - \\mathbb { E } [ \\widehat { Y } ^ { \\emptyset } \\mid x _ { 0 } ] ) } _ { d i s p a r i t y b e f o r e r e m o v i n g D E , I E } = - \\alpha - \\beta \\gamma } \\end{array}\n$$\n\nThe TVD metric again decomposes into the contributions along direct and indirect effects:\n\n![](images/1b6b4e236c07b2a6fe2529527e7b8e8a3de76bfe4c8e263e646a55b2564660f9.jpg)  \nFigure 1: Total Variation (TV) measures vs. Excess Loss. Different colors represent trajectories obtained for different randomly sampled linear SCMs.\n\nBased on the excess loss and the reduction in disparity resulting from constraining a causal path to zero, we can quantify the fairness/utility trade-off through a causal lens. Prototypical instances for predictors $\\widehat { Y } ^ { S }$ are visualized in Fig. 1, for three randomly drawn triplesb $( \\alpha , \\beta , \\gamma )$ . Predictor $\\widehat { Y } ^ { \\varnothing }$ is optimal and thus always has 0 excess loss (and henceblies on the vertical axis). Fully-fair predictor $\\widehat { Y } ^ { \\{ D E , I E \\} }$ removes both direct and indirect effects, and thus ablways has the $T V$ measure equal to 0. Therefore, $\\widehat { Y } ^ { \\{ D E , I E \\} }$ always lies on the horizontal axis (corresponding  ob $T V = 0$ ). The slopes in the plot between ${ \\widehat { Y } } ^ { \\varnothing }$ , ${ \\widehat { Y } } ^ { D { \\bar { E } } }$ , and $\\widehat { Y } ^ { D E , I E }$ (indicated by arrows), geometricall bcaptbure the tenbsion between excess loss and reducing discrimination upon imposing a constraint. These slopes are computed as the ratio\n\n$$\n{ \\frac { T V d i f f e r e n c e } { E x c e s s L o s s } } ,\n$$\n\na quantity that we call the causal fairness-utility ratio (CFUR). Based on Eqs. 11-12 and 19-22 we can compute\n\n$$\nC F U R ( D E ) = - \\frac { 1 } { \\alpha } ,\n$$\n\n$$\nC F U R ( I E ) = - \\ \\frac { 2 \\beta \\gamma } { \\gamma ^ { 2 } \\beta ^ { 2 } + \\alpha \\gamma \\beta + 2 \\gamma ^ { 2 } \\sigma _ { w } ^ { 2 } } ,\n$$\n\nsummarizing the fairness-utility trade-off for each path.\n\n$$\n\\begin{array} { r l } & { T V D = \\underbrace { T V ( \\widehat { Y } ^ { D E } ) - T V ( \\widehat { Y } ^ { \\emptyset } ) } _ { T e r m A = d i s p a r i t y r e d u c t i o n \\ o f \\cal D E } } \\\\ & { \\qquad + \\underbrace { T V ( \\widehat { Y } ^ { \\{ D E , I E \\} } ) - T V ( \\widehat { Y } ^ { D E } ) } _ { T e r m B = d i s p a r i t y r e d u c t i o n \\ o f \\cal I E } . } \\end{array}\n$$\n\nTerms $A$ and $B$ can be computed as:\n\nThe above example illustrates how in the simple linear case we can attribute the increased loss from imposing fairness constraints to the specific causal pathway in question. It also shows we can compute the associated change in the disparity between groups, quantified by the TV measure $\\mathbb { E } [ \\widehat { Y } \\mid \\overline { { x } } _ { 1 } ] -$ $\\mathbb { E } [ \\widehat { Y } \\mid x _ { 0 } ]$ . In this paper, we generalize the approachbfrom the abobve example to a non-parametric setting, with the following key contributions:\n\n$$\n- \\left( \\mathbb { E } [ \\widehat { Y } ^ { D E } \\mid x _ { 0 } ] - \\mathbb { E } [ \\widehat { Y } ^ { D E } \\mid x _ { 0 } ] \\right) = - \\beta \\gamma .\n$$\n\n$$\n- \\left( \\mathbb { E } [ \\widehat { Y } ^ { \\varnothing } \\mid x _ { 0 } ] - \\mathbb { E } [ \\widehat { Y } ^ { \\varnothing } \\mid x _ { 0 } ] \\right) = - \\alpha\n$$\n\n$$\nB = \\left( \\mathbb { E } [ \\widehat { Y } ^ { \\{ D E , I E \\} } \\mid x _ { 1 } ] - \\mathbb { E } [ \\widehat { Y } ^ { \\{ D E , I E \\} } \\mid x _ { 0 } ] \\right)\n$$\n\n(i) We introduce the notion of a path-specific excess loss (PSEL) associated with imposing a fairness constraint along a causal path (Def. 3), and we prove how the total excess loss (TEL) can be decomposed into a sum of path-specific excess losses (Thm. 1),\n\n![](images/d3f98a4e18ace89e9ac50d0e773674863c86ebfcdb0033b6b072a4008a8a0cf9.jpg)  \nFigure 2: Standard Fairness Model, with the protected attribute $X$ , set of confounders $Z$ , set of mediators $W$ , outcome $Y$ , and a predictor $\\widehat { Y }$ .\n\n(ii) We develop an algorithm for attributing path-specific excess losses to different causal paths (Alg. 1), allowing the system designer to explain how the total excess loss is affected by different fairness constraints. In this context, we show the equivalence of Alg. 1 with a Shapley value (Shapley et al. 1953) approach (Prop 3),\n\n(iii) For purposes of applying Alg. 1, a key requirement is the construction of causally-fair predictors $\\widehat { Y } ^ { S }$ that remove effects along pathways in $S$ . We introdubce a novel Lagrangian formulation of the optimization problem for such $\\widehat { Y } ^ { \\bar { S } }$ (Def. 5) accompanied with a training procedure for leabrning the predictor (Alg. 2),\n\n(iv) We introduce the causal fairness/utility ratio (CFUR, Def. 4) that summarizes how much the group disparity can be reduced per fixed cost in terms of excess loss. We compute CFURs on a range of real-world datasets, and demonstrate that from a causal viewpoint fairness and utility are almost always in tension.\n\n# 1.2 Preliminaries\n\nWe use the language of structural causal models (SCMs) (Pearl 2000). An SCM is a tuple $\\mathcal { M } : = \\langle V , U , \\mathcal { F } , P ( u ) \\rangle$ , where $V$ , $U$ are sets of endogenous (observable) and exogenous (latent) variables, respectively, $\\mathcal { F }$ is a set of functions $f _ { V _ { i } }$ , one for each $V _ { i } \\in V$ , where $V _ { i } \\gets f _ { V _ { i } } ( \\mathrm { p a } ( V _ { i } ) , U _ { V _ { i } } )$ for some $\\operatorname { p a } ( V _ { i } ) \\subseteq V$ and $U _ { V _ { i } } \\subseteq U$ . $P ( u )$ is a strictly positive probability measure over $U$ . Each SCM $\\mathcal { M }$ is associated to a causal diagram $\\mathcal { G }$ (Bareinboim et al. 2022) over the node set $V$ where $V _ { i } \\to V _ { j }$ if $V _ { i }$ is an argument of $f _ { V _ { j } }$ , and $V _ { i } \\left. \\right. V _ { j }$ if the corresponding $U _ { V _ { i } } , U _ { V _ { j } }$ are not independent. An instantiation of the exogenous variables $U = u$ is called a unit. By $Y _ { x } ( u )$ we denote the potential response of $Y$ when setting $X = x$ for the unit $u$ , which is the solution for $Y ( u )$ to the set of equations obtained by evaluating the unit $u$ in the submodel $\\mathcal { M } _ { x }$ , in which all equations in $\\mathcal { F }$ associated with $X$ are replaced by $X = x$ . For more details about the causal inference background, we refer the reader to (Pearl 2000; Bareinboim et al. 2022; PlecÀáko and Bareinboim 2024). Throughout the paper, we assume a specific cluster causal diagram $\\mathcal { G } _ { \\mathrm { S F M } }$ known as the standard fairness model (SFM) (PleÀácko and Bareinboim 2024) over endogenous variables $\\{ X , Z , W , Y , { \\widehat { Y } } \\}$ shown in Fig. 2. The SFM consists of the following: probtected attribute, labeled $X$ (e.g., gender, race, religion), assumed to be binary; the set of confounding variables $Z$ , which are not causally influenced by the attribute\n\n$X$ (e.g., demographic information, zip code); the set of mediator variables $W$ that are possibly causally influenced by the attribute (e.g., educational level or other job-related information); the outcome variable $Y$ (e.g., GPA, salary); the predictor of the outcome $\\widehat { Y }$ (e.g., predicted GPA, predicted salary). The SFM encodesb the assumptions typically used in the causal inference literature about the lack of hidden confounding. The availability of the SFM and the implied assumptions are a possible limitation of the paper, while we note that partial identification techniques for bounding effects can be used for relaxing them (Zhang, Tian, and Bareinboim 2022). Based on the SFM, we will use the following causal fairness measures:\n\nDefinition 1 (Population-level Causal Fairness Measures (Pearl 2001; PleÀácko and Bareinboim 2024)). The natural direct, indirect, and spurious effects are defined as\n\n$$\n\\begin{array} { r l } & { N D E _ { x _ { 0 } , x _ { 1 } } ( y ) = P ( y _ { x _ { 1 } , W _ { x _ { 0 } } } ) - P ( y _ { x _ { 0 } } ) } \\\\ & { N I E _ { x _ { 1 } , x _ { 0 } } ( y ) = P ( y _ { x _ { 1 } , W _ { x _ { 0 } } } ) - P ( y _ { x _ { 1 } } ) } \\\\ & { ~ N S E _ { x } ( y ) = P ( y \\mid x ) - P ( y _ { x } ) . } \\end{array}\n$$\n\nThe NDE in Eq. 26 compares the potential outcome Yx1,Wx , in which $Y$ responds to $X = x _ { 1 }$ along the direct path, while $W$ is set at the value it would attain naturally when responding to $X = x _ { 0 }$ , against the potential outcome $Y _ { x _ { 0 } }$ where $X = x _ { 0 }$ along both direct and indirect paths. In this way, the NDE measures the variations induced by changing $x _ { 0 } \\to x _ { 1 }$ along the direct causal path, quantifying direct discrimination. Similarly, the NIE in Eq. 27 compares $Y _ { x _ { 1 } , W _ { x _ { 0 } } }$ vs. $Y _ { x _ { 1 } }$ , and thus captures variations induced by considering a change $x _ { 1 } ~  ~ x _ { 0 }$ along the indirect causal path (note that both $Y _ { x _ { 1 } , W _ { x _ { 0 } } }$ and $Y _ { x _ { 1 } }$ respond to $X = x _ { 1 }$ along the direct path, so only indirect variations are induced when taking the difference). Finally, the NSE in Eq. 28 compares $Y$ 1 $X = x$ vs. $Y _ { x }$ . In the former, due to conditioning on $X = x$ , the distribution over the set of confounders $Z$ (in Fig. 2) changes according to this conditioning, while in the potential outcome $Y _ { x }$ , the distribution of $Z$ does not change, since $X = x$ is set by intervention. Therefore, taking the difference captures the spurious effect of $X$ on $Y$ , along the backdoor path $X   Z  Y$ . A causally-fair predictor for a subset $S$ of the above measures is defined as:\n\nDefinition 2 (Causally Fair Predictor (Plecko and Bareinboim 2024b)). The optimal causally $S$ -fair predictor $\\widehat { Y } ^ { S }$ with respect to a loss function $L$ and pathways in $S$ i tbhe solution to the following optimization problem:\n\n$$\n\\begin{array} { r l } & { \\underset { f } { \\mathrm { a r g m i n } } \\quad \\quad \\quad \\quad \\mathbb { E } L ( Y , f ( X , Z , W ) ) } \\\\ & { s . t . \\quad \\quad N D E _ { x _ { 0 } , x _ { 1 } } ( f ) = N D E _ { x _ { 0 } , x _ { 1 } } ( y ) \\cdot \\mathbb { 1 } ( D E \\notin S ) } \\\\ & { \\quad \\quad \\quad \\quad \\quad N I E _ { x _ { 1 } , x _ { 0 } } ( f ) = N I E _ { x _ { 1 } , x _ { 0 } } ( y ) \\cdot \\mathbb { 1 } ( I E \\notin S ) } \\\\ & { \\quad \\quad \\quad \\quad N S E _ { x _ { 0 } } ( f ) = N S E _ { x _ { 0 } } ( y ) \\cdot \\mathbb { 1 } ( S E \\notin S ) } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad N S E _ { x _ { 1 } } ( f ) = N S E _ { x _ { 1 } } ( y ) \\cdot \\mathbb { 1 } ( S E \\notin S ) . } \\end{array}\n$$\n\nThe definition of $\\widehat { Y } ^ { S }$ has a straightforward interpretation. For any pathway in bhe set $S$ , the corresponding causal effect should be 0, as proposed in the path-specific causal fairness literature (Nabi and Shpitser 2018; Chiappa 2019). However, importantly, pathways that are not in $S$ also need to be constrained ‚Äì the effect of $X$ on $\\widehat { Y }$ along these paths should not change compared to the  bue outcome $Y$ (Plecko and Bareinboim 2024b). For instance, if the direct path is not in $S$ (meaning it is considered to be non-discriminatory), then we expect to have $\\mathrm { N D E } _ { x _ { 0 } , x _ { 1 } } ( \\widehat { y } ) = \\mathrm { N D E } _ { x _ { 0 } , x _ { 1 } } ( y )$ (and similarly for IE, SE). This form of cbonstraint on discriminatory pathways ensures that no undesirable bias amplification occurs along non-discriminatory pathways.\n\n# 2 Path-Specific Excess Loss\n\nIn this section, we introduce the concept of a path-specific excess loss, and then demonstrate how the total excess loss can be decomposed into path-specific excess losses.\n\nDefinition 3 (Path-Specific Excess Loss). Let $L ( \\widehat { Y } , Y )$ be $a$ loss function and $\\widehat { Y } ^ { S }$ the optimal causally $S$ -fairbpredictor with respect to $L$ . Dbefine the path-specific excess loss (PSEL) of a pair $S , S ^ { \\prime }$ as:\n\n$$\nP S E L ( S \\to S ^ { \\prime } ) = \\mathbb { E } [ L ( \\widehat { Y } ^ { S ^ { \\prime } } , Y ) ] - \\mathbb { E } [ L ( \\widehat { Y } ^ { S } , Y ) ] .\n$$\n\nThe quantity $P S E L ( \\emptyset \\to \\{ D , I , S \\} )$ is called the total excess loss (TEL).\n\nThe total excess loss computes the increase in the loss for the totally constrained predictor $\\widehat { Y } ^ { \\{ D , I , S \\} }$ with direct, indirect, and spurious effects removed cbompared to the unconstrained predictor ${ \\widehat { Y } } ^ { \\varnothing }$ . 1 In the following theorem, we show that the total exce sb loss can be decomposed as a sum of path-specific excess losse. All proofs are provided in Appendix B (for supplements, see full paper version at https://arxiv.org/abs/ 2405.15443):\n\nTheorem 1 (Total Excess Loss Decomposition). The total excess loss $P S E L ( \\emptyset \\to \\{ D , I , S \\} )$ can be decomposed into $a$ sum of path-specific excess losses as follows:\n\n$$\n\\begin{array} { r l } { P S E L ( \\emptyset  \\{ D , I , S \\} ) = P S E L ( \\emptyset  \\{ D \\} ) } & { } \\\\ { + P S E L ( \\{ D \\}  \\{ D , I \\} ) } & { } \\\\ { + P S E L ( \\{ D , I \\}  \\{ D , I , S \\} ) . } \\end{array}\n$$\n\nRemark 2 (Non-Uniqueness of Decomposition). The decomposition in Thm. 1 is not unique. In particular, the $P S E L ( \\emptyset \\to \\{ D , I , S \\} )$ can be decomposed as\n\n$$\n\\begin{array} { r l } & { P S E L ( \\emptyset \\to \\{ S _ { 1 } \\} ) + P S E L ( \\{ S _ { 1 } \\} \\to \\{ S _ { 1 } , S _ { 2 } \\} ) } \\\\ & { \\quad + P S E L ( \\{ S _ { 1 } , S _ { 2 } \\} \\to \\{ D , I , S \\} ) } \\end{array}\n$$\n\nfor any choice of $S _ { 1 } , S _ { 2 } \\in \\{ D , I , S \\}$ with $S _ { 1 } \\neq S _ { 2 }$ . Therefore, six different decompositions exist (three choices for $S _ { 1 }$ , two for $S _ { 2 }$ ).\n\nFig. 3 provides a graphical overview of all the possible path-specific excess loses. In the left side, we start with $S = \\emptyset$ and the predictor ${ \\widehat { Y } } ^ { \\varnothing }$ . Then, we can add any of $\\{ D , I , S \\}$ to the $S$ -set, to obtbain the predictors $\\widehat { Y } ^ { D } , \\widehat { Y } ^ { I }$ , or $\\widehat { Y } ^ { S }$ , and\n\n$$\n\\underset { \\{ S \\} \\longmapsto \\{ I \\} \\bigcup _ { i = 1 } ^ { J ( D , I ) } \\{ D , I \\} } { \\overset { \\{ D \\} \\longrightarrow \\{ D , I \\} } { \\longrightarrow } }\n$$\n\nFigure 3: Graphical representation $\\mathcal { G } _ { \\mathrm { P S E L } }$ .\n\nso on. The graph representing all the possible states Y S and transitions between pairs $( \\widehat { Y } ^ { S } , \\widehat { Y } ^ { S ^ { \\prime } } )$ shown in Fi .b 3 is labeled $\\mathcal { G } _ { \\mathrm { P S E L } }$ . There are six bpathsb starting from $\\varnothing$ and ending in $\\{ D , I , S \\}$ . In Alg. 1, we introduce a procedure that sweeps over all the edges and paths in $\\mathcal { G } _ { \\mathrm { P S E L } }$ to compute path-specific excess losses, while also computing the change in the TV measure between groups in order to track the reduction in discrimination. In the main body of the paper we discuss fairness-utility trade-offs when considering direct, indirect, and spurious effects. In Appendix F we described how to adapt all the results to a general setting considering more granular path-specific effects. Formally, for any edge $( S , S ^ { \\prime } )$ in the graph $\\mathcal { G } _ { \\mathrm { P S E L } }$ , the value of PSEL $\\cdot S  S ^ { \\prime } )$ is computed, together with the difference in the TV measure (TVD, for short) from the transition $S  S ^ { \\prime }$ , defined through the following expression\n\n$$\n\\begin{array} { r } { \\mathrm { T V D } ( S \\to S ^ { \\prime } ) = \\underbrace { \\mathbb { E } [ Y ^ { S ^ { \\prime } } \\mid x _ { 1 } ] - \\mathbb { E } [ Y ^ { S ^ { \\prime } } \\mid x _ { 0 } ] } _ { \\mathrm { T V ~ a f t e r ~ r e m o v i n g ~ } S ^ { \\prime } \\setminus S } } \\\\ { - \\underbrace { \\mathbb { E } [ Y ^ { S } \\mid x _ { 1 } ] - \\mathbb { E } [ Y ^ { S } \\mid x _ { 0 } ] } _ { \\mathrm { T V ~ b e f o r e ~ r e m o v i n g ~ } S ^ { \\prime } \\setminus S } . } \\end{array}\n$$\n\nThe quantities PSEL $( S  S ^ { \\prime } )$ and $\\mathrm { T V D } ( S \\to S ^ { \\prime } )$ are naturally associated with the effect that was removed, i.e., $S ^ { \\prime } \\backslash S$ In this context, we mention a connection with previous work (Zhang and Bareinboim 2018b), which provides a way of quantifying direct, indirect, and spurious effects of the attribute $X$ on the outcome $Y$ . In Appendix C, we show that our approach of quantifying the change in the TV measure through the TVD quantity in practice closely corresponds to methods for decomposing the TV measure into its direct, indirect, and spurious contributions (Zhang and Bareinboim 2018b).\n\nAs there are multiple ways of reaching the set $\\{ D , I , S \\}$ from $\\varnothing$ in $\\mathcal { G } _ { \\mathrm { P S E L } }$ , each of the corresponding effects (direct, indirect, spurious) will be associated with a number of different PSELs and TVDs (generally, note that the complexity is exponential in the number of causal paths included). In Eqs. 40-41 inside the algorithm, we compute the average PSEL and TVD across all the edges that are associated with a specific effect $S _ { i }$ . This simple intuition, corresponding to taking an average across all the possible decompositions of the total excess loss (Eq. 38), turns out to be equivalent to a Shapley value (Shapley et al. 1953) of a suitably chosen value function:\n\nInput: data $\\mathcal { D }$ , predictors $\\widehat { Y } ^ { S }$ for $S { \\mathrm { - s e t s } } \\subseteq \\{ D , I , S \\}$ foreach edge $( S , S ^ { \\prime } ) \\in { \\mathcal { G } } _ { P S E L }$ do\n\n2 compute the path-specific excess loss of $S ^ { \\prime } \\backslash S$ , given by PSEL $( S  S ^ { \\prime } )$ )   \n3 compute the TV measure difference of $S ^ { \\prime } \\backslash S$ , written TVD $\\vert S  S ^ { \\prime } \\rangle$ given by $\\mathrm { T V } _ { x _ { 0 } , x _ { 1 } } ( \\widehat { Y } ^ { S ^ { \\prime } } ) - \\mathrm { T V } _ { x _ { 0 } , x _ { 1 } } ( \\widehat { Y } ^ { S } )$\n\n4 foreach causal path $S _ { i } \\in \\{ D , I , S \\}$ do\n\n5 compute the average path-specific excess loss and TV difference across all paths $\\emptyset  \\cdots  \\{ D , I , S \\}$ in $\\mathcal { G } _ { \\mathrm { P S E L } }$\n\n6\n\nAPSEL $( S _ { i } )$ is computed as\n\n$$\n\\frac 1 { 3 ! } \\sum _ { \\stackrel { \\pi \\in { \\mathcal G } _ { \\mathrm { P S E L } } : } { \\emptyset \\mathrm { t o } \\left\\{ D , I , S \\right\\} } } \\mathrm { P S E L } ( \\pi ^ { < S _ { i } } \\to \\pi ^ { < S _ { i } } \\cup S _ { i } )\n$$\n\n7\n\n$\\mathrm { A T V D } ( S _ { i } )$ is computed as\n\n$$\n\\frac 1 { 3 ! } \\sum _ { \\pi \\in { \\mathcal G } _ { \\mathrm { P S E L } } \\colon \\atop \\emptyset \\mathrm { t o } \\{ D , I , S \\} } { \\mathrm { T V D } ( \\pi ^ { < S _ { i } }  \\pi ^ { < S _ { i } } \\cup S _ { i } ) } ,\n$$\n\nwhere $\\pi ^ { < S _ { i } }$ denotes the set of causal paths that precede $S _ { i }$ in $\\pi$ .\n\n8 return set of $P S E L ( S \\to S ^ { \\prime } )$ , $T V D ( S  S ^ { \\prime } )$ , attributions $A P S E L ( S _ { i } ) , A T V D ( S _ { i } )$\n\nProposition 3 (PSEL Attribution as Shapley Values). Let the functions $f _ { 1 } ( S ) , f _ { 2 } ( S )$ be defined as:\n\n$$\n\\begin{array} { c } { { f _ { 1 } ( S ) = P S E L ( \\emptyset \\to S ) } } \\\\ { { f _ { 2 } ( S ) = T V D ( \\emptyset \\to S ) . } } \\end{array}\n$$\n\nThe Shapley value $\\phi ^ { k } ( S _ { i } )$ for the effect $S _ { i } \\in \\{ D , I , S \\}$ and function $f _ { k }$ , is computed as\n\n$$\n\\sum _ { S \\subseteq \\{ D , I , S \\} \\setminus \\{ S _ { i } \\} } { \\frac { 1 } { n { \\binom { n - 1 } { | S | } } } } \\left( f _ { k } ( S \\cup \\{ S _ { i } \\} ) - f _ { k } ( S ) \\right) .\n$$\n\nwhere $n = 3$ for the choice $\\{ D , I , S \\}$ . The averaged pathspecific excess loss of $S _ { i }$ and the averaged $T V$ difference of $S _ { i }$ are equal to the Shapley values of $S _ { i }$ associated with functions $f _ { 1 } , f _ { 2 }$ , respectively,\n\n$$\n\\begin{array} { c } { { \\phi ^ { 1 } ( S _ { i } ) = A P S E L ( S _ { i } ) } } \\\\ { { \\phi ^ { 2 } ( S _ { i } ) = A T V D ( S _ { i } ) , } } \\end{array}\n$$\n\nwith $A P S E L ( S _ { i } )$ defined as\n\n$$\n\\frac 1 { 3 ! } \\sum _ { \\pi \\in { \\mathcal G } _ { P S E L } : \\atop \\varnothing t o \\{ D , I , S \\} } P S E L ( \\pi ^ { < S _ { i } }  \\pi ^ { < S _ { i } } \\cup S _ { i } ) ,\n$$\n\nand $A T V D ( S _ { i } )$ defined as\n\n$$\n\\frac 1 { 3 ! } \\sum _ { \\stackrel { \\pi \\in { \\mathcal G } _ { P S E L } : } { \\varnothing t \\ o \\ \\{ D , I , S \\} } } T V D ( \\pi ^ { < S _ { i } } \\to \\pi ^ { < S _ { i } } \\cup S _ { i } ) .\n$$\n\nThe above proposition illustrates how averaging the influence of removing a causal effect over all possible ways of reaching $\\{ D , I , S \\}$ from $\\varnothing$ is equivalent to computing the Shapley values of an appropriate value function $f$ . We remark that the attribution in Prop. 3 is not the only approach one could take. An alternative would be to average the contributions of each pathways across all edges of the graph $\\mathcal { G } _ { \\mathrm { P S E L } }$ (instead of focusing on all pathways from $\\varnothing$ to $\\{ D , I , S \\} )$ ). Such an attribution would not satisfy the Shapley axioms, however. We next introduce the notion of a causal fairness/utility ratios:\n\nDefinition 4 (Causal Fairness/Utility Ratio (CFUR)). The causal fairness/utility ratio (CFUR) for a causal path $S _ { i }$ is defined as\n\n$$\nC F U R ( S _ { i } ) = \\frac { A T V D ( S _ { i } ) } { A P S E L ( S _ { i } ) } .\n$$\n\nWhenever $A P S E L ( S _ { i } ) = 0$ , $C F U R ( S _ { i } )$ is equal to 0.\n\nThe CFUR quantity may be particularly useful for comparing different causal effects, and the connection of the CFUR with local TVD and PSEL values is described in Appendix D. The intuition behind the quantity is simple ‚Äì for removing a causal effect $S _ { i }$ from our predictor $\\widehat { Y }$ , we want to compute how much of a reduction in the d bparity that results in (measured in terms of the ATVD measure) per unit change in the incurred excess loss. This quantity attempts to assign a single number to a causal path that succinctly summarizes how much fairness is gained vs. how much predictive power is lost by imposing such a causal constraint.\n\n# 3 Causally-Fair Constrained Learning\n\nIn the preceding section, we developed an approach for quantifying the tension between fairness and accuracy from a causal viewpoint. The results were contingent on finding the optimal causally-fair predictors $\\widehat { Y } ^ { S }$ following Def. 2. However, computing the predictors $\\widehat { Y } ^ { S }$ is quite challenging in practice, due to several compl xb causal constraints in the optimization problem. We now develop a practical approach for solving this problem, by first introducing a Lagrangian form of the optimal causally-fair predictor:\n\nDefinition 5 (Causal Lagrange Predictor $\\widehat { Y } ^ { S }$ ). The causally $S$ -fair $\\lambda$ -optimal predictor $\\bar { \\hat { Y } } ^ { S } ( \\lambda )$ with rebspect to pathways in $S$ and the loss function $L$ bis the solution of the following:\n\narg min E L(Y, f (X, Z, W ))+\n\n$$\n\\begin{array} { r l } & { \\lambda \\big ( N D E _ { x _ { 0 } , x _ { 1 } } ( f ) - N D E _ { x _ { 0 } , x _ { 1 } } ( y ) \\cdot \\mathbb { 1 } \\big ( D E \\notin S \\big ) \\big ) ^ { 2 } + } \\\\ & { \\lambda \\big ( N I E _ { x _ { 1 } , x _ { 0 } } ( f ) - N I E _ { x _ { 1 } , x _ { 0 } } ( y ) \\cdot \\mathbb { 1 } \\big ( I E \\notin S \\big ) \\big ) ^ { 2 } + } \\\\ & { \\lambda \\big ( N S E _ { x _ { 0 } } ( f ) - N S E _ { x _ { 0 } } ( y ) \\cdot \\mathbb { 1 } \\big ( S E \\notin S \\big ) \\big ) ^ { 2 } + } \\\\ & { \\lambda \\big ( N S E _ { x _ { 1 } } ( f ) - N S E _ { x _ { 1 } } ( y ) \\cdot \\mathbb { 1 } \\big ( S E \\notin S \\big ) \\big ) ^ { 2 } } \\end{array}\n$$\n\nThe above definition reformulates the problem of finding $\\widehat { Y } ^ { S }$ to a Lagrangian form. This makes the problem amenab ebto standard gradient descent methods, and we propose a procedure for finding a suitable predictor $\\widehat { Y } ^ { S }$ in Alg. 2 (CFCL). In principle, many non-parametric lea bners such as boosting or neural networks could be used for fitting $\\widehat { Y } ^ { S }$ , and here we describe a neural approach. The key challenbge for constructing $\\widehat { Y } ^ { S }$ is finding the appropriate value of the tuning parameter $\\lambda$ . While the user may simply use a grid of values of $\\lambda$ , and inspect the loss function and the fairness measures, and then choose a $\\lambda$ value, we propose a data-driven approach. We note that if $\\lambda$ is too small, insufficient weight may be given to the fairness constraints, which therefore may be violated on new test data. If $\\lambda$ is too high, however, we may give insufficient weight to minimizing the loss $L$ , which may lead to poor performance on test data. Therefore, we propose a binary search type of procedure that first splits the data into train and evaluation folds, $\\mathcal { D } _ { t }$ and $\\mathcal { D } _ { e }$ . CFCL starts with an interval $[ \\lambda _ { \\mathrm { l o w } } , \\lambda _ { \\mathrm { h i g h } } ]$ and takes the midpoint $\\lambda _ { \\mathrm { m i d } }$ . For this parameter value, it computes the optimal predictor $\\widehat { Y } ^ { S } ( \\lambda _ { \\mathrm { m i d } } )$ for the optimization problem in Eqs. 52-56 by fitt nbg a feedforward neural network with $n _ { h }$ hidden layers and $n _ { v }$ nodes in each layer. Then, for this fixed value of $\\lambda _ { \\mathrm { m i d } }$ , we test the hypotheses that\n\nAlgorithm 2: Causally-Fair Constrained Learning (CFCL)   \n\n<html><body><table><tr><td colspan=\"2\">Input: training data Dt,evaluation data De,set S,</td></tr><tr><td></td><td>precision ‚àà 1 Set ÂÖ•low = O, Xhigh = large</td></tr><tr><td></td><td>2 while |ÂÖ•high - Œªlow| >‚àà do</td></tr><tr><td>3 4</td><td>set Xmid =(Œªlow+Ahigh) fit a neural network to solves the optimization</td></tr><tr><td></td><td>problem in Eqs.52-56 with ÂÖ•= ÂÖ•mid on Dt to obtain the predictor YS(Xmid)</td></tr><tr><td>5</td><td>compute the causal measures of fairness NDE, NIE,NSE of YS(Œªmid) on evaluation data De</td></tr><tr><td>6</td><td>test the hypothesis HCE : NCE(gS(ÂÖ•mid))= NCE(y)¬∑ 1(CE S) (57)</td></tr><tr><td>7</td><td>where NCE ranges in NDExo,ùë•1, NIEx1,E0,NSEx0,NSEr1 if anyof HDEHE,HSEoHSE1 rejeced then Alow = Œªmid else Xhigh = Amid 8 return predictor YS (Xmid)</td></tr></table></body></html>\n\n$$\nH _ { 0 } ^ { C E } : \\mathrm { N C E } ( \\widehat { y } ^ { S } ( \\lambda _ { \\mathrm { m i d } } ) ) = \\mathbf { N C E } ( y ) \\cdot \\mathbb { 1 } ( \\mathbf { C E } \\notin S )\n$$\n\non evaluation datba $\\mathcal { D } _ { e }$ (done in Eq. 57), which essentially test if the fairness constrains hold on the evaluation set $\\mathcal { D } _ { e }$ , i.e., out of sample. If any of the hypotheses are rejected, it means that this value of $\\lambda$ is too small to ensure that the fairness constraints are satisfied on new training data. Therefore, we want to find a larger $\\lambda$ , and the algorithm moves to the interval $[ \\lambda _ { \\mathrm { m i d } } , \\lambda _ { \\mathrm { h i g h } } ]$ . If none of the hypotheses are rejected, it means that $\\lambda _ { \\mathrm { m i d } }$ is large enough to enforce the fairness constraints, and there may be an even smaller $\\lambda$ that achieves this, so the algorithm moves to the interval $[ \\lambda _ { \\mathrm { l o w } } , \\lambda _ { \\mathrm { m i d } } ]$ . In this way, CFCL leads to a data-driven way to select the tuning parameter $\\lambda$ . As the number of training and evaluation samples increases $| \\mathcal { D } _ { t } | , | \\mathcal { D } _ { e } | \\to \\infty$ , the method is expected to perform increasingly well. An alternative approach would be to use a framework that automatically allows the learning of the $\\lambda$ parameter (Fioretto et al. 2021).\n\n# 4 Experiments\n\nIn this section, we perform the causal fairness-accuracy analysis described in Sec. 2 on the Census 2018 dataset (Ex. 2). Additional analyses of the COMPAS (Ex. 3) and UCI Credit (Ex. 4) datasets are reported in Appendix E. All code for reproducing the experiments can be found in our Github repository https://github.com/dplecko/causal-acc-decomp.\n\nExample 2 (Salary Increase of Government Employees (PlecÀáko and Bareinboim 2024)). The US government is building a tool for automated allocation of salaries for new employees. For developing the tool, they use the data collected by the United States Census Bureau in 2018, including\n\n‚Ä¢ confounders $Z$ , consisting of demographic information ( $\\cdot Z _ { 1 }$ for age, $Z _ { 2 }$ for race, $Z _ { 3 }$ for nationality),   \n‚Ä¢ gender $X$ $\\scriptstyle \\left. [ x _ { 0 } \\right.$ female, $x _ { 1 }$ male),   \n‚Ä¢ mediators $W$ , including marital and family status $M$ , education $L$ , and work-related information $R$ ,   \n‚Ä¢ outcome $Y$ , salary.\n\nThe government wants to predict the outcome $Y$ , the yearly salary of the employees (transformed to a log-scale), in order to assign salaries for prospective employees. The standard fairness model (Fig. 2) is constructed as $\\{ X = X , Z =$ $\\phantom { 0 } _ { 1 } , Z _ { 2 } , Z _ { 3 } \\} , W = \\{ M , L , R \\} , Y = Y \\}$ .\n\nThe team developing the ML predictor is also concerned with the fairness of the allocated salaries. In particular, they wish to understand how the different causal effects from the protected attribute $X$ to the predictor $\\widehat { Y }$ affect the prediction, and how much the salary predictionsb would have to deviate from the optimal prediction to remove an effect along a specific pathway (in particular, they focus on the root mean squared error (RMSE) loss). For analyzing this, they utilize the tools from Alg. 1, and build causally fair predictors $\\widehat { Y } ^ { S }$ (for different choices of $S$ -sets) using Alg. 2.\n\nThe analysis results are shown in Fig. 4, with uncertainty bars indicating standard deviations over 10 bootstrap repetitions. In the analysis of PSEL and TVD values (Fig. 4a), the team notices that imposing fairness constraints does not reduce RMSE for any of the effects. The largest excess loss is observed for the indirect effect, and smaller excess losses for direct and spurious effects. When looking at TVD values, they find that removing direct and indirect effects reduces the group differences substantially. In terms of causal fairnessutility ratios (Fig. 4b), the team finds that removing the direct effect has the best value in terms of reducing the disparity between groups vs. increasing the loss. The TV measure vs. excess loss dependence for different predictors $\\widehat { Y } ^ { S }$ is shown in Fig. 4c (binary labels $( \\mathrm { D } , \\mathrm { I } , \\mathrm { S } )$ in the figure indbicate which effects were removed). The graph $\\mathcal { G } _ { \\mathrm { P S E L } }$ with the values of PSEL and TVD for each transition is shown in Fig. 4d. Based on the analysis, the team realizes that it is possible to substantially reduce discrimination with a small amount of excess RMSE loss. They decide to implement the predictor $\\widehat { Y } ^ { D }$ with the direct effect removed. ‚ñ°\n\n![](images/5eb98ed15b1a2528038175a2f74bd06bcd14dad136d51b6dfe6f431b895da866.jpg)  \nFigure 4: Application of Alg. 1 on the Census 2018 dataset. (a) Estimated APSEL (Eq. 40) and ATVD (Eq. 41) values; (b) The causal fairness-utility ratios (Eq. 51); (c) The Pareto plot for trade-offs between fairness (TV measure on vertical axis) and utility (excess RMSE on horizontal axis) for different predictors. The vector $( s _ { 1 } , s _ { 2 } , s _ { 3 } )$ indicates which of the DE/IE/SE pathways are constrained to zero; (d) The $\\mathcal { G } _ { \\mathrm { P S E L } }$ graph from Fig. 5 populated with PSEL and TVD values.\n\n# 5 Conclusion\n\nthat captures the\n\nThe tension between fairness and accuracy is a fundamental concern in the modern applications of machine learning. The importance of this tension is also recognized in the legal frameworks of anti-discrimination, such as the disparate impact doctrine, which may allow for the usage of covariates correlated with the protected attribute if they are sufficiently important for the decision-maker‚Äôs utility ‚Äì in legal texts, this concept is known as business necessity or job-relatedness. In this work, we developed tools for analyzing the fairnessaccuracy trade-off from a causal standpoint. Our approach allows the system designer to quantify how much excess loss is incurred when removing a path-specific causal effect from an automated predictor (Def. 3). We also showed how the total excess loss, defined as the difference between the loss of the predictor fair along all causal pathways vs. an unconstrained predictor, can be decomposed into a sum of path-specific excess losses (Thm. 1). At the same time, enforcing fairness constraints may reduce the overall disparity between groups. Based on this, we developed an algorithm for attributing excess loss to different causal pathways (Alg. 1), and introduced the notion of a causal fairness-utility ratio ratio and in this way summarizes the trade-off for each causal path. Since our approach requires access to causally-fair predictors (Def. 2), we introduced a new neural approach for constructing such predictors (Def. 5, Alg. 2). Finally, we analyzed several real-world datasets, in order to investigate if fairness and utility are in a trade-off in practice. Our findings are that, from causal perspective, fairness and utility are almost always in tension (see Exs. 2-4), contrary to some other works appearing in the fair ML literature.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®Ëá™Âä®ÂåñÂÜ≥Á≠ñÁ≥ªÁªü‰∏≠ÂÖ¨Âπ≥ÊÄß‰∏éÂáÜÁ°ÆÊÄß‰πãÈó¥ÁöÑÊùÉË°°ÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®Ê∂âÂèäÊïèÊÑüÂ±ûÊÄßÔºàÂ¶ÇÊÄßÂà´„ÄÅÁßçÊóèÁ≠âÔºâÊó∂„ÄÇËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåÁé∞ÊúâÁöÑÂÖ¨Âπ≥ÊÄßÁ∫¶ÊùüÂèØËÉΩ‰ºöÈôç‰ΩéÂÜ≥Á≠ñÁ≥ªÁªüÁöÑÊïàÁî®ÔºåËÄåÊ≥ïÂæãÊ°ÜÊû∂ÔºàÂ¶Ç„Ää1964Âπ¥Ê∞ëÊùÉÊ≥ïÊ°à„ÄãÁ¨¨‰∏ÉÊù°ÔºâÂÖÅËÆ∏Âú®‰∏öÂä°ÂøÖË¶ÅÊÄß‰∏ã‰ΩøÁî®‰∏éÊïèÊÑüÂ±ûÊÄßÁõ∏ÂÖ≥ÁöÑ‰ª£ÁêÜÂèòÈáè„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÈ¶ñÊ¨°‰ªéÂõ†ÊûúËßÜËßíÂàÜÊûê‰∫ÜÂÖ¨Âπ≥ÊÄß‰∏éÂáÜÁ°ÆÊÄß‰πãÈó¥ÁöÑÊùÉË°°ÔºåÊèêÂá∫‰∫ÜË∑ØÂæÑÁâπÂºÇÊÄßË∂ÖÈ¢ùÊçüÂ§±ÔºàPSELÔºâÁöÑÊ¶ÇÂøµÔºåÁî®‰∫éÈáèÂåñÂú®ÊñΩÂä†Âõ†ÊûúÂÖ¨Âπ≥ÊÄßÁ∫¶ÊùüÊó∂È¢ÑÊµãÂô®ÊçüÂ§±ÁöÑÂ¢ûÂä†„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   ÂºïÂÖ•‰∫ÜË∑ØÂæÑÁâπÂºÇÊÄßË∂ÖÈ¢ùÊçüÂ§±ÔºàPSELÔºâÁöÑÊ¶ÇÂøµÔºåÂπ∂ËØÅÊòé‰∫ÜÊÄªË∂ÖÈ¢ùÊçüÂ§±ÔºàTELÔºâÂèØ‰ª•ÂàÜËß£‰∏∫Â§ö‰∏™Â±ÄÈÉ®PSELÁöÑÂíåÔºàÂÆöÁêÜ1Ôºâ„ÄÇ\\n> *   ÊèêÂá∫‰∫ÜÂõ†ÊûúÂÖ¨Âπ≥/ÊïàÁî®ÊØîÔºàCFURÔºâÔºåÁî®‰∫éÈáèÂåñÂú®Á∫¶ÊùüÂõ†ÊûúË∑ØÂæÑÊó∂ÂáèÂ∞ëÁöÑÊ≠ßËßÜ‰∏éÂ¢ûÂä†ÁöÑÊçüÂ§±‰πãÈó¥ÁöÑÊØîÁéáÔºàÂÆö‰πâ4Ôºâ„ÄÇ\\n> *   ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ•ûÁªèÊñπÊ≥ïÔºàÁÆóÊ≥ï2ÔºâÁî®‰∫éÂõ†ÊûúÁ∫¶ÊùüÁöÑÂÖ¨Âπ≥Â≠¶‰π†ÔºåÂπ∂Âú®Â§ö‰∏™ÁúüÂÆûÊï∞ÊçÆÈõÜÔºàÂ¶ÇCensus 2018„ÄÅCOMPAS„ÄÅUCI CreditÔºâ‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂ±ïÁ§∫‰∫ÜÂÖ¨Âπ≥ÊÄß‰∏éÂáÜÁ°ÆÊÄß‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ÈÄöËøáÁªìÊûÑÂõ†ÊûúÊ®°ÂûãÔºàSCMÔºâÊù•ÈáèÂåñÂÖ¨Âπ≥ÊÄß‰∏éÂáÜÁ°ÆÊÄß‰πãÈó¥ÁöÑÊùÉË°°ÔºåÂà©Áî®Ë∑ØÂæÑÁâπÂºÇÊÄßË∂ÖÈ¢ùÊçüÂ§±ÔºàPSELÔºâÊù•ÊçïÊçâÊñΩÂä†ÂÖ¨Âπ≥ÊÄßÁ∫¶ÊùüÊó∂ÁöÑÊçüÂ§±Â¢ûÂä†„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÁöÑÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÁªüËÆ°ÂÖ¨Âπ≥ÊÄß‰∏äÔºåËÄåÊú¨ÊñáÈ¶ñÊ¨°‰ªéÂõ†ÊûúËßÜËßíÁ≥ªÁªüÂú∞ÂàÜÊûê‰∫ÜÂÖ¨Âπ≥ÊÄß‰∏éÂáÜÁ°ÆÊÄßÁöÑÊùÉË°°„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** Êú¨ÊñáÊèêÂá∫‰∫ÜPSELÂíåCFURÔºåÊèê‰æõ‰∫ÜÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÂàÜÊûêÂ∑•ÂÖ∑ÔºåÂπ∂ÂºÄÂèë‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ•ûÁªèÊñπÊ≥ïÊù•Â≠¶‰π†Âõ†ÊûúÁ∫¶ÊùüÁöÑÂÖ¨Âπ≥È¢ÑÊµãÂô®„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.   ÂÆö‰πâË∑ØÂæÑÁâπÂºÇÊÄßË∂ÖÈ¢ùÊçüÂ§±ÔºàPSELÔºâÂíåÊÄªË∂ÖÈ¢ùÊçüÂ§±ÔºàTELÔºâ„ÄÇ\\n> 2.   ËØÅÊòéTELÂèØ‰ª•ÂàÜËß£‰∏∫Â§ö‰∏™PSELÁöÑÂíåÔºàÂÆöÁêÜ1Ôºâ„ÄÇ\\n> 3.   ÊèêÂá∫Âõ†ÊûúÂÖ¨Âπ≥/ÊïàÁî®ÊØîÔºàCFURÔºâÊù•ÈáèÂåñÂÖ¨Âπ≥ÊÄß‰∏éÊïàÁî®‰πãÈó¥ÁöÑÊùÉË°°ÔºàÂÆö‰πâ4Ôºâ„ÄÇ\\n> 4.   ÂºÄÂèë‰∏ÄÁßçÁ•ûÁªèÊñπÊ≥ïÔºàÁÆóÊ≥ï2ÔºâÊù•Â≠¶‰π†Âõ†ÊûúÁ∫¶ÊùüÁöÑÂÖ¨Âπ≥È¢ÑÊµãÂô®„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊèê‰æõ‰∫Ü‰∏Ä‰∏™Á∫øÊÄßÊ®°ÂûãÁöÑÁ§∫‰æãÔºà‰æã1ÔºâÔºåÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïËÆ°ÁÆóPSELÂíåCFURÔºåÂπ∂Áõ¥ËßÇÂú∞Â±ïÁ§∫‰∫ÜÂÖ¨Âπ≥ÊÄß‰∏éÂáÜÁ°ÆÊÄß‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÂü∫Á∫øÊ®°ÂûãÁöÑÂÖ∑‰ΩìÂêçÁß∞„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Ë∑ØÂæÑÁâπÂºÇÊÄßË∂ÖÈ¢ùÊçüÂ§±ÔºàPSELÔºâ‰∏äÔºö** ËÆ∫ÊñáÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÊÄªË∂ÖÈ¢ùÊçüÂ§±ÔºàTELÔºâÂàÜËß£‰∏∫Â§ö‰∏™PSELÁöÑÂíåÔºåÂπ∂ÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜËøôÁßçÂàÜËß£ÁöÑÊúâÊïàÊÄß„ÄÇ\\n> *   **Âú®Âõ†ÊûúÂÖ¨Âπ≥/ÊïàÁî®ÊØîÔºàCFURÔºâ‰∏äÔºö** ËÆ∫ÊñáËÆ°ÁÆó‰∫Ü‰∏çÂêåÂõ†ÊûúË∑ØÂæÑÁöÑCFURÂÄºÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§ö‰∏™ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ÂÖ¨Âπ≥ÊÄß-ÂáÜÁ°ÆÊÄßÊùÉË°° (Fairness-Accuracy Trade-Off, FAT)\\n*   Ë∑ØÂæÑÁâπÂºÇÊÄßË∂ÖÈ¢ùÊçüÂ§± (Path-Specific Excess Loss, PSEL)\\n*   Âõ†ÊûúÂÖ¨Âπ≥ÊÄß (Causal Fairness, N/A)\\n*   ÁªìÊûÑÂõ†ÊûúÊ®°Âûã (Structural Causal Model, SCM)\\n*   Âõ†ÊûúÂÖ¨Âπ≥/ÊïàÁî®ÊØî (Causal Fairness/Utility Ratio, CFUR)\\n*   Ëá™Âä®ÂåñÂÜ≥Á≠ñÁ≥ªÁªü (Automated Decision-Making Systems, ADMS)\\n*   ‰∏çÂêåÂΩ±ÂìçÂéüÂàô (Disparate Impact Doctrine, DID)\\n*   Á•ûÁªèÂÖ¨Âπ≥Â≠¶‰π† (Neural Fair Learning, N/A)\"\n}\n```"
}