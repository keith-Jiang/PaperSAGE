{
    "source": "Semantic Scholar",
    "arxiv_id": "2501.01874",
    "link": "https://arxiv.org/abs/2501.01874",
    "pdf_link": "https://arxiv.org/pdf/2501.01874.pdf",
    "title": "DFF: Decision-Focused Fine-tuning for Smarter Predict-then-Optimize with Limited Data",
    "authors": [
        "Jiaqi Yang",
        "Enming Liang",
        "Zicheng Su",
        "Zhichao Zou",
        "Peng Zhen",
        "Jiecheng Guo",
        "Wanjing Ma",
        "Kun An"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2025-01-03",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "The Key Laboratory of Road and Traffic Engineering of the Ministry of Education, Tongji University",
        "Tongji University",
        "Department of Data Science, City University of Hong Kong",
        "City University of Hong Kong",
        "Didi Chuxing"
    ],
    "paper_content": "# DFF: Decision-Focused Fine-Tuning for Smarter Predict-Then-Optimize with Limited Data\n\nJiaqi Yang 1\\*, Enming Liang 2\\*, Zicheng $\\mathbf { S } \\mathbf { u } ^ { \\mathrm { ~ } 1 \\dagger }$ , Zhichao Zou 3, Peng Zhen 3, Jiecheng Guo 3, Wanjing Ma 1, Kun An 1‚Ä†\n\n1The Key Laboratory of Road and Traffic Engineering of the Ministry of Education, Tongji University, China 2 Department of Data Science, City University of Hong Kong 3 Didi Chuxing, Beijing, China   \n2410196 $@$ tongji.edu.cn, eliang4-c@my.cityu.edu.hk, suzicheng $@$ tongji.edu.cn, zouzhichao $@$ didiglobal.com, zhenpeng@didiglobal.com, jasonguo $@$ didiglobal.com, mawanjing $@$ tongji.edu.cn, kunan $@$ tongji.edu.cn\n\n# Abstract\n\nDecision-focused learning (DFL) offers an end-to-end approach to the predict-then-optimize (PO) framework by training predictive models directly on decision loss (DL), enhancing decision-making performance within PO contexts. However, the implementation of DFL poses distinct challenges. Primarily, DL can result in deviation from the physical significance of the predictions under limited data. Additionally, some predictive models are non-differentiable or black-box, which cannot be adjusted using gradient-based methods.\n\nTo tackle the above challenges, we propose a novel framework, Decision-Focused Fine-tuning (DFF), which embeds the DFL module into the PO pipeline via a novel bias correction module. DFF is formulated as a constrained optimization problem that maintains the proximity of the DLenhanced model to the original predictive model within a defined trust region. We theoretically prove that DFF strictly confines prediction bias within a predetermined upper bound, even with limited datasets, thereby substantially reducing prediction shifts caused by DL under limited data. Furthermore, the bias correction module can be integrated into diverse predictive models, enhancing adaptability to a broad range of PO tasks. Extensive evaluations on synthetic and real-world datasets, including network flow, portfolio optimization, and resource allocation problems with different predictive models, demonstrate that DFF not only improves decision performance but also adheres to fine-tuning constraints, showcasing robust adaptability across various scenarios.\n\n# Introduction\n\nPredict-then-Optimize (PO) is a framework that uses machine learning to address decision problems under uncertainty, as the parameters of the optimization problem are likely unknown before making the decision (Bertsimas and Kallus 2020). This framework operates in two stages: In the first stage, auxiliary features are utilized to predict the unknown parameters; in the second stage, decisions are made based on these predictions. A critical limitation of this twostage framework is that during the training process of the first stage, the generally used loss functions such as MSE aim to minimize fitting error. However, this may not align with the objective of the final decision task. The new framework proposed to address this issue is Decision-focused Learning (DFL), which has been demonstrated to improve the quality of the final decision by customizing the training of the predictive model based on the decision task (Kotary et al. 2021; Mandi et al. 2023; Sadana et al. 2024). However, directly replacing the two-stage PO framework with DFL still poses significant challenges.\n\nThe first challenge is the convergence issues associated with training from scratch based on decision loss1 (DL). This arises from the highly non-convex nature of DL (e.g., regret), combined with potential discontinuities, leading to substantial computational costs in calculating the gradient of the downstream problem (Elmachtoub and Grigas 2022; Tang and Khalil 2022). A promising solution for this problem is constructing a surrogate loss for DFL. In existing works, the design of the surrogate losses is based on Fisher consistency, i.e., the surrogate losses are equivalent with DL when the data is infinite (Elmachtoub and Grigas 2022; Shah et al. 2024). However, in real-world scenarios with limited data, DFL may fail to converge or converge to a sub-optimum.\n\nThe second challenge is the significant deviation in predictions induced by directly minimizing DL through training. As the accuracy of the predictions becomes less controllable, this can lead to a loss of the inherent physical meaning present in the two-stage PO framework. For example, suppose we use a simple linear model with coefficient matrix $\\beta$ to fit the dataset $\\textstyle { \\bar { \\{ X , y \\} } }$ , where $\\pmb { X } \\in \\mathbb { R } ^ { N \\times p }$ and $\\pmb { y } \\in \\mathbb { R } ^ { N \\times d }$ . First, the PO framework adopts the MSE loss, leading to a closed-form coefficient matrix:\n\n$$\n\\beta _ { \\mathrm { P O } } = ( \\boldsymbol { X } ^ { \\top } \\boldsymbol { X } ) ^ { - 1 } \\boldsymbol { X } ^ { \\top } \\boldsymbol { y }\n$$\n\nAs for the DFL, the predictive model is trained with DL, which is implicitly defined by the decision problem. However, it has been shown that there exists an input-dependent positive semi-definite matrix $Q _ { i }$ for $i = 1 , \\cdots , N$ , leading to a quadratic loss $\\begin{array} { r } { L = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( { \\pmb y } _ { i } - \\hat { \\pmb y } _ { i } ) ^ { T } { \\pmb Q } _ { i } ( { \\pmb y } _ { i } - \\hat { \\pmb y } _ { i } ) , } \\end{array}$ which is Fisher consistent with DL (Shah et al. 2024). Therefore, the coefficient matrix under the surrogate loss is derived from such a generalized least squares problem:\n\n![](images/e03c5a08cde77678df209d09af27f1f73c36e7b59a7531ed79faad889b8bd6d9.jpg)  \nFigure 1: The distribution of predictions generated by different methods for the shortest path problem (Elmachtoub and Grigas 2022) is shown in the figure. It includes the two-stage PO (MSE) and three representative DFL methods, including $\\mathrm { S P O + }$ (Elmachtoub and Grigas 2022), PFYL (Berthet et al. 2020), and LSLTR (Mandi et al. 2022). The results indicate that the predictions from the MSE-based method maintain a distribution similar to the ground truth, whereas the predictions from the DFL models exhibit significant deviations.\n\n$$\n\\beta _ { \\mathrm { { D L } } } = { ( \\sum _ { i = 1 } ^ { N } { \\pmb x } _ { i } { \\pmb Q } _ { i } { \\pmb x } _ { i } ^ { \\top } ) ^ { - 1 } } \\sum _ { i = 1 } ^ { N } { \\pmb x } _ { i } { \\pmb Q } _ { i } { \\pmb y } _ { i }\n$$\n\nSince $Q _ { i }$ is not an identity matrix, DFL with limited data can introduce biases originating from the downstream decisionmaking objective. In other words, the predictions obtained by DFL focus on decision quality, which can result in undesired phenomena, such as multiplicative shifts (Tang and Khalil 2022) as shown in Figure 1 and fail to capture the underlying physical meanings of predictions.\n\nThe third challenge is that some prediction models are non-differentiable and may not be directly combined with DFL. Current training schemes for DFL primarily focus on first-order gradient-based methods. However, some popular prediction models, such as tree-based models, are not differentiable (Breiman 2001). Moreover, in cases where the confidence in the predictive model is low, white-box or semi-black-box simulation models developed based on principles of the physical world can be used for prediction and offline evaluation (Wu et al. 2022). These models can explicitly provide state transitions between variables, offering robust estimates in the presence of many unobserved confounding variables. Nonetheless, it remains unclear how to effectively apply DFL in non-differentiable backbone scenarios.\n\nTo address the aforementioned challenges and make DFL more effective, we propose Decision-Focused Fine-Tuning (DFF), which consists of a given backbone predictive model and an additional bias correction layer, as shown in Figure 2. The contributions of this paper are as follows:\n\n‚Ä¢ By utilizing the bias correction layer, DFF can fine-tune the output of any backbone model to produce predictions that are aligned with the decision-making objective. ‚Ä¢ By explicitly constraining the correction layer, DFF can prevent the fine-tuned output from shifting the inherent physical meaning of the backbone model‚Äôs predictions. ‚Ä¢ We theoretically analyze the performance of DFF on the adherence to fine-tuning constraints and strictly limit the bias in the predictions. ‚Ä¢ We conduct extensive simulations, including benchmark network flow and portfolio optimization problems and two real-world ride-sourcing subsidy allocation problems. The results show that DFF consistently achieves better decision quality compared to backbone models.\n\n# Related Works\n\n# Decision Focused Learning\n\nSince the introduction of OptNet by Amos and Kolter, which integrates optimization problems as individual layers within end-to-end trainable deep networks, many works have explored how to better integrate generic optimization problems with machine learning. For convex optimization problems, the gradient can be accurately computed using implicit differentiation methods to guide training (Donti, Amos, and Kolter 2017; Wilder, Dilkina, and Tambe 2019). However, discrete optimization problems and linear optimization problems often lack gradient information that can guide training, necessitating the development of techniques to obtain approximate gradients (Mandi et al. 2023). These techniques include relaxing and smoothing the optimization problem (Wilder, Dilkina, and Tambe 2019), approximating gradients using random perturbations (Berthet et al. 2020; Dalle et al. 2022), and designing surrogate losses (Mandi and Guns 2020). The $\\operatorname { S P O + }$ loss function proposed by Elmachtoub, Liang, and McNellis (2020) is the first surrogate loss with theoretical guarantees, suitable for optimization problems with any linear objective function. It has been proven to be Fisher consistent with DL. Subsequent research has further explored designing surrogate loss functions for general optimization problems, leading to methods such as LODL (Shah et al. 2022), EGL (Shah et al. 2024), LANCER (Zharmagambetov et al. 2024), and TaskMet (Bansal et al. 2024). As for constrained optimization, several studies propose effective methods for addressing scenarios in which uncertain parameters are incorporated into the constraints of optimization problems (Hu, Lee, and Lee 2023, 2024). Furthermore, thanks to packages like CvxpyLayers (Agrawal et al. 2019) and PyEPO (Tang and Khalil 2022), deploying these complex loss functions now comes with lower engineering costs. Therefore, DFL has the potential to become a modular tool that allows predictive models to be enhanced for specific optimization problems.\n\n# DFL with Special Prediction Models\n\nHowever, DFL faces challenges under some special upstream predictive models, e.g., tree-based model (Elmachtoub, Liang, and McNellis 2020), semi-parametric (Wu et al. 2022; Zhao et al. 2019), and simulation-based physical model (She et al. 2024). These models are favored under specific settings; for example, tree-based models are preferred in the industry due to their high accuracy and strong interpretability. However, these models are nondifferentiable, making it difficult for the gradient of the DL to be backpropagated efficiently.\n\nExisting works have combined tree-based models directly with DFL. Elmachtoub, Liang, and McNellis (2020) propose SPO Trees and SPO Forests by modifying the splitting criteria of decision trees, which shows smaller decision regret in linear optimization problems compared to the decision tree and random forest based on MSE. Butler and Kwon (2023) combine gradient boosting with DFL and propose Dboost, which performs better in convex cone optimization problems. However, numerical experiments by Butler and Kwon (2023) reveal that the decision quality of Dboost and SPO forest is inferior to gradient boosting based on MSE in certain scenarios, which limits their practicality. To date, there is a lack of an efficient framework to incorporate DFL into general predictive models.\n\n# Fine-tuning Approach\n\nFine-tuning is a crucial technique in deep learning, which involves taking a pre-trained mode and adjusting its parameters to better fit a particular task. This approach allows practitioners to leverage the knowledge embedded in the pretrained model, significantly speeding up the training process and improving performance on the new task (Zhang et al. 2023; Ding et al. 2023; Fu et al. 2023; Fan et al. 2024).\n\nUnder the context of DFL, fine-tuning refers to further adjusting the model‚Äôs predictions to reduce the loss associated with the DL. Since the training process of DFL typically requires repeatedly solving the optimization problem, it can reduce the overall training time and improve convergence efficiency. For example, a checkpoint with good prediction performance is obtained by training on MSE, and then further fine-tuning is conducted with DL until convergence (Mandi et al. 2020; Kotary et al. 2022).\n\nBeichter et al. (2024) propose a retraining method that fine-tunes a pre-trained predictive model with weighted MSE and DL and applies it to the dispatchable feeder optimization problem, which resulted in significant performance improvements.\n\nNevertheless, existing fine-tuning/retraining can not be applied to general prediction models and there is no performance guarantee of the tuned model when considering the shift under limited data.\n\n# Problem Definition\n\n# Predict-then-Optimize\n\nIn the two-stage predict-then-optimize framework, we first train a predictive model $M$ using the MSE loss, with input features $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ that produce predictions ${ \\hat { \\pmb { c } } } = { \\pmb M } ( { \\pmb x } )$ . These predictions serve as the input parameters for the second-stage optimization problem and determine the optimal decision $\\bar { w } ^ { \\ast } ( \\hat { c } )$ :\n\n$$\n{ \\pmb w } ^ { * } ( \\hat { \\pmb { c } } ) = \\arg \\operatorname* { m i n } _ { \\pmb { w } } f ( { \\pmb w } , \\hat { \\pmb { c } } )\n$$\n\n$$\ng _ { j } ( \\pmb { w } ) \\leq 0 , \\quad \\mathrm { f o r } \\ j \\in \\{ 1 , 2 , \\ldots , J \\} .\n$$\n\nwhere $f ( \\cdot )$ represents the objective function of the downstream optimization task, and $g _ { j } ( \\pmb { w } )$ denotes the constraints on the decision variable $\\textbf { \\em w }$ . Here, $\\hat { \\pmb { c } } \\in \\mathbb { R } ^ { d }$ represents the unknown parameters, with dimension $d$ typically greater than 1. For each decision problem, our goal is to minimize the decision regret, introduced by Elmachtoub and Grigas (2022):\n\n$$\n\\begin{array} { r } { D R ( c , \\hat { c } ) = f ( { \\pmb w } ^ { * } ( \\hat { \\pmb c } ) , c ) - f ( { \\pmb w } ^ { * } ( c ) , c ) } \\end{array}\n$$\n\nGiven a dataset with $N$ samples, denoted as $\\begin{array} { r l } { \\mathcal { D } } & { { } = } \\end{array}$ $\\{ ( \\pmb { x } _ { 1 } , \\pmb { c } _ { 1 } ) , ( \\pmb { x } _ { 2 } , \\pmb { c } _ { 2 } ) , \\dots , ( \\pmb { x } _ { N } , \\pmb { c } _ { N } ) \\}$ , the predictive model $M$ is trained to minimize the average decision regret as follows:\n\n$$\n\\overline { { D R } } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } D R ( c _ { i } , M ( \\pmb { x } _ { i } ) )\n$$\n\n# Open Issues for DFL\n\nAs shown in previous research, the DFL method tends to outperform two-stage PO when the predictive model is misspecified or when there are limitations to further improving prediction accuracy (Hu, Kallus, and Mao 2022; Elmachtoub et al. 2023). However, the application of DFL in such scenarios is also hindered by several limitations.\n\nBiased Prediction under Limited Data DFL tends to deliver biased predictions, especially under limited data conditions. The reasons are as follows. Firstly, as we mentioned earlier, models trained using DL inherently introduce bias to better align with the decision-making objective. Secondly, existing surrogate loss functions may not satisfy Fisher‚Äôs consistency under limited data, leading to potential bias compared to DL (Elmachtoub and Grigas 2022). Lastly, the multiplicative shifts induced by DL can cause predictions to lose their inherent physical meaning (Tang and Khalil 2022), which is critical in helping analyze downstream decision tasks.\n\nNon-differentiable Predictive Model There are scenarios where designing a suitable predictive model and training it to achieve sufficient accuracy can be challenging. In such cases, an alternative is to utilize simulation-based models. These models can incorporate explicit state transitions based on principles of the physical world and merge prior knowledge of the task. However, it is unclear how to directly apply DFL to these non-differentiable models, and replacing simulation models with DFL-based predictive models is unrealistic. Moreover, since these simulation-based models have been validated as reliable in numerous cases, DFL is expected to enhance decision-making by building on the simulation results rather than simply overturning them. This motivates us to explore how to leverage DFL in these problems while maintaining the existing solution framework and making only minor adjustments to the outputs.\n\n![](images/3e1da2177fa90bfb1aa9a0b5c62c15551656b7d911da61ee6a4524a10bb3b7b2.jpg)  \nFigure 2: Illustration of the Decision-Focused Fine-tuning Framework.\n\n# Decision-Focused Fine-tuning\n\nIn this section, we introduce the proposed framework, Decision-Focused Fine-Tuning (DFF), as illustrated in Figure 2. We consider a scenario where an upstream predictive model $M$ has already been established, and its predictions are denoted as $\\hat { c }$ . We then design a neural network $F _ { \\theta }$ with a constrained output range, by which $\\hat { c }$ is explicitly represented through a residual connection. All parameters $\\theta$ of $F _ { \\theta }$ are optimized to perform a linear transformation on $\\hat { c }$ that minimizes the DL, with the output denoted as cÀú.\n\n# Constrained Fine-tuning\n\nTo preserve the advantages of the two-stage framework when applying DFL, our objective is to tune the existing predictive model $M$ to achieve better decision quality, while ensuring that the adjusted predictions $\\tilde { c }$ remain within a reasonable range from the predictions of the original model, i.e. $\\hat { c }$ . This can be formulated as a constrained optimization (CO) problem as follows:\n\n$$\n\\begin{array} { r l } { \\underset { \\pmb { F _ { \\theta } } } { \\operatorname* { m i n } } } & { \\mathbb { E } _ { \\boldsymbol { x } , \\boldsymbol { c } } \\left[ \\pmb { D } \\pmb { R } ( \\boldsymbol { c } , \\pmb { F _ { \\theta } } ( \\pmb { x } ) ) \\right] } \\\\ { \\mathrm { s . t . } } & { \\mathbb { D } ( \\pmb { F _ { \\theta } } ( \\pmb { x } ) , \\pmb { M } ( \\pmb { x } ) ) \\le \\epsilon } \\end{array}\n$$\n\nwhere $\\mathbb { D }$ is a distance metric, e.g., Euclidean distance, in a specific scenario, and $\\epsilon$ defines the acceptable range for adjusting the upstream predictive model. This formulation can be interpreted as improving the decision performance by fine-tuning the upstream predictive model $M$ via $F _ { \\theta }$ within the trust region, particularly under limited data.\n\nWe make the following remarks on the effects of the parameter $\\epsilon$ :\n\n‚Ä¢ When $\\epsilon = 0$ , no adjustments are made to the upstream predictive model, leading to $F _ { \\theta } = M$ . ‚Ä¢ When $\\epsilon  \\infty$ , it indicates that $F _ { \\theta }$ is trained entirely based on the DL without any restrictions, leading to $F _ { \\theta } = \\arg \\operatorname* { m i n } _ { F _ { \\theta } } \\mathbb { E } _ { x , c } \\left[ D R ( c , \\tilde { c } ) \\right]$ .\n\n‚Ä¢ When $\\epsilon \\in \\mathsf { \\Gamma } ( 0 , \\infty )$ , $F _ { \\theta }$ is trained using DL within the trust region of the trained predictive model. The tradeoff between these two losses can be problem-specific and defined by the user through the parameter $\\epsilon$ .\n\nWe can note that the CO formulation bridges the naive weighted sum of MSE and DL by applying the distance metric $\\mathbb { D }$ as a constraint. A similar approach can be found in Beichter et al. (2024), which incorporates the MSE loss into the objective function (7) as a Lagrangian relaxation. This can be considered as a special case of our CO formulation. The superiority of the CO formulation lies in its applicability to various types of distance metrics $\\mathbb { D }$ , allowing for a more efficient adjustment of the trade-off through the parameter $\\epsilon$ .\n\nSuch a trust-region optimization formulation has been successfully applied in various areas such as reinforcement learning (Schulman et al. 2015), online learning (Wu et al. 2017), and fine-tuning (Kurutach et al. 2018). It ensures that the fine-tuned results do not deviate significantly from the upstream predictions, enabling the model to effectively handle decision tasks where data is limited and stability is critical (Queeney, Paschalidis, and Cassandras 2021), while further enhancing decision quality. However, the challenges of constrained fine-tuning are twofold: some predictive models are non-differentiable, and enforcing the constraint can be difficult (Mandi et al. 2020; Kotary et al. 2022; Beichter et al. 2024). We address these challenges by proposing a bias correction layer, as introduced in the following section.\n\n# Bias Correction Module\n\nTo tackle the above challenges, we propose a bias correction module $F _ { \\theta }$ , motivated by the hyper-network design (Ha, Dai, and Le 2016; She et al. 2024). We set $F _ { \\theta }$ in the following semi-parametric form as:\n\n$$\nF _ { \\theta } ( x ) = \\phi ( x ) \\odot M ( x ) + b ( x )\n$$\n\nwhere $\\odot$ is the Hadamard product. $\\phi ( { \\pmb x } )$ and ${ \\pmb b } ( { \\pmb x } )$ represent the input-dependent weights and bias, respectively. Together, they fine-tune the predictive model $M$ to reach an improved decision performance. Notably, $F _ { \\theta }$ does not directly tune the parameters of the predictive model $M$ ; instead, it learns an input-dependent linear transformation. This approach allows it to apply the DL to general nondifferentiable predictive models. Moreover, the distance constraint outlined in Equation (8) can be easily enforced by designing the weight and bias modules accordingly.\n\nSpecifically, we define the distance metric in the trustregion constraint (8) as the percentage error:\n\n$$\n\\mathbb { D } _ { i } ( \\tilde { c } _ { i } , \\hat { c } _ { i } ) = \\bigg | \\frac { \\tilde { c } _ { i } - \\hat { c } _ { i } } { \\hat { c } _ { i } } \\bigg | \\le \\epsilon , \\quad i \\in \\{ 1 , 2 , \\dots , d \\}\n$$\n\nHere, $\\epsilon$ represent\fs a poin\ftwise adjustable percentage. The distance measurement of percentage error is advantageous as it normalizes the predictions into unitless values, thereby avoiding the additional costs of parameter adjustment that may arise due to the differences in the magnitudes of the predictions.\n\nGiven the constraint shown in Equation (10), we set $\\pmb { b } ( \\pmb { x } ) = 0$ and apply an offset-scaled Sigmoid transformation to ensure that the output of $F _ { \\theta } ( x )$ strictly falls within the specified range and exhibits center-symmetry, i.e. $\\begin{array} { r } { \\frac { { \\tilde { c } } _ { i } } { { \\tilde { c } } _ { i } } \\in } \\end{array}$ $[ 1 - \\epsilon , 1 + \\epsilon ]$ . The outputs of $\\phi ( { \\pmb x } )$ can be represented as:\n\n$$\n\\phi ( \\boldsymbol { x } ) = [ ( 1 - \\epsilon ) + 2 \\epsilon \\cdot \\sigma ( h ( x ) ) ]\n$$\n\nHere, $h ( \\mathbf { x } )$ denotes the output from the penultimate layer, and $\\begin{array} { r } { \\sigma ( x ) \\stackrel { \\cdot } { = } \\frac { 1 } { 1 + e ^ { - x } } } \\end{array}$ denotes the standard Sigmoid function.\n\nIt is important to clarify that the percentage error shown in Equation (8) is not the only instance of the distance metric, but rather an intuitive and effective example. Further discussion on different types of distance metrics can be found in Appendix A.\n\nWe now present the upper bound on the increment of the RMSE induced by the bias correction layer, and the maximum angular gap between the original predictions $\\hat { c }$ and the corrected predictions cÀú.\n\nTheorem 1. For a given $\\epsilon$ , the increment in RMSE of cÀú obtained in Equation (11) in the main text has an upper bound such that:\n\n$$\nR M S E ( \\tilde { c } , c ) - R M S E ( \\hat { c } , c ) \\leq \\frac { \\epsilon } { \\sqrt { d } } \\| \\hat { \\pmb { c } } \\| _ { 2 }\n$$\n\nwhere $\\| \\cdot \\| _ { 2 }$ denotes the $L _ { 2 }$ norm and $d$ represents the dimension of the vector $\\hat { c }$ .\n\nMoreover, the cosine similarity between cÀú and $\\hat { c }$ can be calculated as:\n\n$$\n\\cos \\langle \\tilde { \\pmb { c } } , \\hat { \\pmb { c } } \\rangle = \\frac { \\tilde { \\pmb { c } } \\cdot \\hat { \\pmb { c } } } { \\| \\tilde { \\pmb { c } } \\| _ { 2 } \\| \\hat { \\pmb { c } } \\| _ { 2 } }\n$$\n\nwhere $\\cos \\langle \\cdot , \\cdot \\rangle \\in [ - 1 , 1 ]$ denotes the similarity of directions between two vectors, with a value of $\\jmath$ indicating the same direction. The lower bound of $\\cos \\langle \\tilde { c } , \\hat { c } \\rangle$ is as follows:\n\n$$\n\\cos \\langle \\tilde { c } , \\hat { c } \\rangle \\geq \\sqrt { 1 - \\epsilon ^ { 2 } }\n$$\n\nProof. See Appendix A.\n\nTheorm 1 demonstrates that the point-wise distance metric defined in Equation (10) can impose a strict bound on the vector-wise distance metrics. These bounds on auxiliary metrics enhance the reliability of predictions derived from by DFF.\n\n# Loss Function and Training\n\nAfter determining the constraint on $F _ { \\theta }$ , the training process is transformed into a standard DFL training process, which takes the following loss function:\n\n$$\nL = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } D R ( c _ { i } , F _ { \\theta } ( \\pmb { x } _ { i } ) )\n$$\n\nWe need to solve the following core gradient optimization problem:\n\n$$\n\\frac { \\partial L } { \\partial \\theta } = \\frac { \\partial L } { \\partial \\boldsymbol { w } ^ { * } } \\frac { \\partial \\boldsymbol { w } ^ { * } } { \\partial c } \\frac { \\partial c } { \\partial \\theta }\n$$\n\nwhere the first two terms involve the gradient of the optimization problem, and the third term is the gradient optimization of the predictive model.\n\nIt is worth noting that our framework is general; we only need to select an appropriate gradient calculation method based on the characteristics of the optimization task to compute $\\scriptstyle { \\frac { \\partial L } { \\partial \\theta } }$ . In this paper, we employ a direct approximation method for calculating $\\scriptstyle { \\frac { \\partial L } { \\partial c } }$ using the $\\mathrm { { S P O + } }$ method because of its superior performance in linear objective function scenarios (Elmachtoub and Grigas 2022). This method constructs a convex upper bound surrogate loss function $L =$ $D R ( c , { \\tilde { c } } )$ :\n\n$$\nL _ { S P O + } = \\operatorname* { m i n } _ { \\mathbf { w } } \\{ ( 2 \\tilde { c } - c ) ^ { T } \\pmb { w } \\} + 2 \\tilde { c } ^ { T } \\pmb { w } ^ { \\ast } ( c ) - f ^ { \\ast } ( c )\n$$\n\nFor this loss function, the gradient can be computed using the following formula:\n\n$$\n\\frac { \\partial L } { \\partial \\pmb { c } } \\approx \\frac { \\partial L _ { S P O + } } { \\partial \\pmb { c } } = \\pmb { w } ^ { * } ( 2 \\tilde { \\pmb { c } } - \\pmb { c } ) - \\pmb { w } ^ { * } ( \\pmb { c } )\n$$\n\n# Case Study\n\nWe evaluate the effectiveness of the proposed DFF on three two-staged PO problems using various datasets and different predictive models. Firstly, the DFF is tested on two wellestablished benchmarks using synthetic data. We then validate the DFF on the resource allocation problem with realworld data from the ride-hailing platform DiDi Chuxing. Lastly, we employ the DFF to adjust the predictions from a non-differentiable simulation model. Note that all experiments are run 10 times with different random seeds and the average results are reported.\n\n# Benchmarks with Synthetic Data\n\nProblem Settings and Dataset In line with Elmachtoub and Grigas (2022) and Butler and Kwon (2023), the proposed DFF framework is first evaluated on two wellestablished optimization problems: the network flow problem and the portfolio optimization problem. The formulations for these benchmarks are illustrated in Appendix B. Specifically, both problems aim to minimize a linear objective function, while the portfolio optimization problem involves a nonlinear constraint. To conduct a fair comparison among different predictive models, we generate two distinct datasets using different mechanisms, including polynomial functions, periodic functions, and piecewise functions with different coefficients. The details of the synthetic data are presented in Appendix B.\n\nTable 1: NDR and MSE of benchmarking methods on two well-established PO problems   \n\n<html><body><table><tr><td rowspan=\"3\">Algorithm</td><td colspan=\"4\">NetworkFlowProblem</td><td colspan=\"4\">PortfolioOptimizationProblem</td></tr><tr><td colspan=\"2\">Dataset 1</td><td colspan=\"2\">Dataset 2</td><td colspan=\"2\">Dataset 1</td><td colspan=\"2\">Dataset 2</td></tr><tr><td>NDR</td><td>MSE</td><td>NDR</td><td>MSE</td><td>NDR</td><td>MSE</td><td>NDR</td><td>MSE</td></tr><tr><td>Random Forest 6.24%</td><td></td><td>4.43√ó10-1</td><td>8.70%</td><td>5.75√ó10-</td><td>23.21%</td><td>4.60√ó10-1</td><td>35.69%</td><td>6.92√ó10-1</td></tr><tr><td>SPO Forest</td><td>5.99 % ()</td><td>5.19√ó10-1</td><td>8.49% ()</td><td>6.44√ó10-1</td><td>22.50% (‚àö)</td><td>4.76√ó10-1</td><td>34.50% (‚àö)</td><td>7.03√ó10-1</td></tr><tr><td>Boost</td><td>5.26%</td><td>3.69√ó10-1</td><td>5.75%</td><td>3.85√ó10</td><td>16.44%</td><td>3.26√ó10</td><td>16.38%</td><td>3.47√ó10-1</td></tr><tr><td>Dboost</td><td>5.86%</td><td>5.78√ó10-1</td><td>8.32%</td><td>6.83√ó10-1</td><td>16.56%</td><td>3.88√ó10-1</td><td>17.87%</td><td>4.78√ó10-1</td></tr><tr><td>NN-MSE</td><td>5.06%</td><td>3.37√ó10-1</td><td>9.44%</td><td>6.08√ó10</td><td>14.46%</td><td>2.87√ó10</td><td>21.98%</td><td>4.49√ó10-1</td></tr><tr><td>NN-SPO+</td><td>4.63% ()</td><td>1.92</td><td>9.43%(‚àö)</td><td>1.80</td><td>14.74%</td><td>4.98√ó10-1</td><td>23.55%</td><td>7.08√ó10-1</td></tr><tr><td>2-fold Boost</td><td>4.49%</td><td>3.03√ó10-1</td><td>4.88%</td><td>3.23√ó10-I</td><td>16.08%</td><td>3.18√ó10-I</td><td>15.92%</td><td>3.39√ó10-1</td></tr><tr><td>DFF (ours)</td><td>4.39% (‚àö)</td><td>3.11√ó10-1</td><td>4.81% ()</td><td>3.26√ó10-</td><td>15.97% (‚àö)</td><td>3.36√ó10-</td><td>15.91% (</td><td>3.41√ó10-1</td></tr></table></body></html>\n\nBaseline In this paper, we compare the DFF framework with three state-of-the-art DFL methods, as well as their original predictive models trained with the MSE loss. The following are brief introductions to the baseline models:\n\n1. Random Forest: Random forest trained with MSE.   \n2. SPO Forest (Elmachtoub, Liang, and McNellis 2020): Random forest trained with the decision regret shown in Equation (5) using zero-order gradient.   \n3. Boost: Gradient boosting tree trained with MSE.   \n4. Dboost (Butler and Kwon 2023): Gradient boosting tree trained with the decision regret in Equation (5) using the fixed-point argmin differentiation.   \n5. NN-MSE: Neural network trained with MSE.   \n6. $\\mathbf { N N - S P O + }$ (Elmachtoub and Grigas 2022): Neural network trained with the $\\operatorname { S P O + }$ loss shown in Equation (17).   \n7. 2-fold Boost: Gradient boosting tree based on MSE with 2-fold cross-validation.   \n8. DFF (ours): Fine-tuning the 2-fold Boost with the $\\operatorname { S P O + }$ loss shown in Equation (17).\n\nThe baselines are evaluated with the normalized decision regret (NDR) (Tang and Khalil 2022), defined as follows:\n\n$$\nN D R = \\frac { \\sum _ { i = 1 } ^ { N } f ( { \\pmb w } ^ { * } ( \\hat { c _ { i } } ) , \\pmb c _ { i } ) - f ( { \\pmb w } ^ { * } ( { \\pmb c _ { i } } ) , \\pmb c _ { i } ) } { \\sum _ { i = 1 } ^ { N } | f ( { \\pmb w } ^ { * } ( { \\pmb c _ { i } } ) , { \\pmb c _ { i } } ) | }\n$$\n\nHyperparameters and Training Process In this paper, all tree-based models are trained with a maximum depth of 2 and no more than 100 trees. In particular, the Random Forest model samples the training data with a $50 \\%$ sampling rate. As for the neural network, it consists of 3 layers with 32 neurons on each layer, and the ReLU activation function is used. The parameter of constrained distance $\\epsilon$ is set to 0.5 for the DFF. Since the Boost model produces the lowest MSE error among the prediction models, we use it as the backbone model to fine-tune its predictions with the $\\mathrm { S P O + }$ loss, as shown in Equation (17). To avoid self-fitting and make full use of data, we split the training set into two disjoint datasets, which is in line with the 2-fold cross-fitting method proposed by Chernozhukov et al. (2018). In Appendix C, we demonstrate that the proposed DFF module also enhances the decision performance of other predictive models.\n\nResults The normalized decision regret and MSE of different methods are summarized in Table reftab:case1. The checkmark $\\cdot _ { \\checkmark } ,$ represents that the predictive model trained using the DL can improve the NDR of its counterpart trained with MSE. Firstly, we can note that the proposed DFF can improve its base model in both problems across all the datasets. This demonstrates that DFF can effectively enhance the downstream optimization task by adjusting the backbone predictive model within the trust region, even in the presence of nonlinear constraints. Moreover, the DFF method achieves the lowest normalized regrets in all settings, except for the portfolio optimization problem with Datasets 1. This is because the base model, 2-fold Boost, is less efficient than NN-MSE in this scenario, and the finetuning based on DFF is constrained by the trust region.\n\n# Resource Allocation Problem with Real-world Data\n\nWe further validate the DFF framework with the resource allocation problem for the ride-hailing platform DiDi Chuxing. The key task is to allocate the subsidy budget between different cities based on the predicted subsidy conversion rate, thereby maximizing the platform‚Äôs revenue. The detailed formulation is presented in Appendix D.\n\nIn practice, the platform employs Extreme Gradient Boosting (XGBoost) as the predictive model, as it offers sufficient predictions, high robustness, and ease of implementation. However, it is challenging to apply DFL to XGBoost for two reasons. On the one hand, it requires calculating the second-order derivative of the loss function to train XGBosst, for which there is currently no corresponding method in existing DFL frameworks. On the other hand, there is limited data available for training such a model, as only the historical market data within three months can be used to make a timely prediction.\n\nWe fine-tune the XGBoost model using DFF with market data from 102 consecutive days from Didi Chuxing and subsequently allocate the subsidy budget across 105 cities. The results are summarized in the first part of Table 2. The proposed DFF framework outperforms the original prediction backbone XGBoost and the ${ \\mathrm { N N } } { \\mathrm { - } } { \\mathrm { S P O } } { \\mathrm { + } }$ method in terms of normalized regret. Notably, the MSE of the DFF framework is significantly lower than that of the ${ \\mathrm { N N } } { \\mathrm { - } } { \\mathrm { S P O } } { \\mathrm { + } }$ method, comparable to the XGBoost model which is specifically trained to minimize the MSE loss. Moreover, Figure 3 depicts the distribution of predicted values from different models, which represents the subsidy conversion rate that has a crucial physical meaning. Notably, the ground truth exhibits a significant pattern of bimodal distribution. However, the predicted values from the ${ \\mathrm { N N } } { \\mathrm { - } } { \\mathrm { S P O } } { \\mathrm { + } } $ method show a considerable deviation, presenting an unimodal distribution. This is consistent with the findings in Tang and Khalil (2022) that direct training with DL can induce a multiplicative shift in the predicted values. In contrast, the predictions from the DFF method closely align with the ground truth in both magnitude and distribution shape.\n\nTable 2: Normalized regret and MSE across different methods in resource allocation problem.   \n\n<html><body><table><tr><td>Method NDR</td><td>MSE</td></tr><tr><td>Fine-tuning the XGBoost model</td></tr><tr><td>XGBoost 2.54% 0.45 NN-SPO+ 2.40% 2.04</td></tr><tr><td>DFF(ours) 2.39% 0.49</td></tr><tr><td>Fine-tuning the simulation model</td></tr><tr><td>Average allocation 3.45%</td></tr><tr><td>Simulation model 2.79% 2.59√ó10-2</td></tr><tr><td>DFF(ours) 2.11% 2.61 √ó10-2</td></tr></table></body></html>\n\n![](images/035627738576eb6070e5de82d1fb44ebb3e22b3b160592eb1f36d739c33f0e45.jpg)  \nFigure 3: Distribution of predictions with different methods\n\n# Fine-tuning Non-differentiable Simulation Model\n\nIn the context of the resource allocation problem, the ridehailing platform DiDi Chuxing has developed a simulation model to derive the subsidy conversion rate, enabling the analysis of domain knowledge in the ride-hailing market (Gao et al. 2024). However, the downstream DL cannot be directly backpropagated to adjust the simulation model due to its non-differentiable nature. To address this, we apply the proposed DFF module to fine-tune the simulation model based on the DL. Due to the absence of DFL methods for adjusting the simulation model, we incorporate a rule-based average allocation strategy as a baseline.\n\nThe summarized results are presented in the lower part of Table 2. Notably, by fine-tuning the simulation model with the DFF module, the normalized decision regret is reduced by $2 4 . 3 7 \\%$ compared to the original simulation model, while the MSE loss remains at a similar level. This again demonstrates the effectiveness of the constrained fine-tuning design. Furthermore, we conduct a sensitivity analysis on the parameter $\\epsilon$ that confines the region in which the predictive model can be adjusted. As shown in Table 4, we observe that when $\\epsilon > 0 . 2$ , the normalized decision regret stabilizes at a low level. In contrast, the MSE loss increases with a larger $\\epsilon$ . Therefore, we can conclude that even by restricting the finetuning module within a tight region with a small value of œµ, the DL can still be improved while maintaining the MSE loss to prevent prediction shift.\n\n![](images/616b2656427c9006aae258c7727c94396da83d86b159b17e9e5be6fc09c070f1.jpg)  \nFigure 4: Sensitivity analysis of parameter $\\epsilon$ on the decision loss and MSE loss\n\n# Conclusion\n\nIn scenarios with limited data for PO problems, the reliability of prediction results and the quality of final decisions are crucial. Existing DFL approaches can effectively improve the decision quality, but they may introduce significant bias in predictions, which increases the risk of decisionmaking from a different perspective. To address this issue, this paper introduces DFF, which employs a unique training mechanism that maximizes decision quality while ensuring that fine-tuning constraints are satisfied. The method is applicable to any upstream prediction model, including nondifferentiable models. Empirical evaluations across diverse scenarios including synthetic data and real-world resource allocation problems demonstrate that DFF outperforms traditional models and existing DFL approaches.\n\nDue to the decoupling between DFF and the upstream model, DFF has advantages in multi-task learning and multiobjective optimization scenarios. Furthermore, its constraint fine-tuning design allows it to better handle downstream problems with uncertain parameters in constraints. These are the key focus areas of our future work.\n\n# Acknowledgments\n\nFinancial supports from the National Natural Science Foundation of China (No. 52302411, No. 52472349, No. 72361137005, No. 52131204) and CCF-DiDi GAIA Collaborative Research Funds (No. 202310) are gratefully acknowledged.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢òÊòØÂÜ≥Á≠ñËÅöÁÑ¶Â≠¶‰π†ÔºàDFLÔºâÂú®ÊúâÈôêÊï∞ÊçÆÊù°‰ª∂‰∏ãÂØºËá¥ÁöÑÈ¢ÑÊµãÂÅèÂ∑ÆÈóÆÈ¢òÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂ∞ÜDFLÂ∫îÁî®‰∫é‰∏çÂèØÂæÆÂàÜÁöÑÈ¢ÑÊµãÊ®°ÂûãÔºàÂ¶ÇÊ†ëÊ®°Âûã„ÄÅ‰ªøÁúüÊ®°ÂûãÔºâ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éDFLËôΩÁÑ∂ËÉΩÊèêÂçáÂÜ≥Á≠ñË¥®ÈáèÔºå‰ΩÜÂú®ÊúâÈôêÊï∞ÊçÆ‰∏ãÂèØËÉΩÂØºËá¥È¢ÑÊµãÂ§±ÂéªÁâ©ÁêÜÊÑè‰πâÔºå‰∏îÁé∞ÊúâÊñπÊ≥ïÊó†Ê≥ïÂ§ÑÁêÜÈùûÂæÆÂàÜÊ®°ÂûãÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®Â∑•‰∏ö‰ºòÂåñÈóÆÈ¢òÔºàÂ¶ÇÁΩëÁªúÊµÅÈáè„ÄÅÊäïËµÑÁªÑÂêà‰ºòÂåñ„ÄÅËµÑÊ∫êÂàÜÈÖçÔºâ‰∏≠ÁöÑÂ∫îÁî®„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫ÂÜ≥Á≠ñËÅöÁÑ¶ÂæÆË∞ÉÊ°ÜÊû∂ÔºàDFFÔºâÔºåÈÄöËøáÂ∏¶Á∫¶ÊùüÁöÑÂÅèÁΩÆÊ†°Ê≠£Ê®°ÂùóÂ∞ÜDFLÂµåÂÖ•È¢ÑÊµã-‰ºòÂåñÊµÅÁ®ãÔºåÂú®‰øùÊåÅÈ¢ÑÊµãÁâ©ÁêÜÊÑè‰πâÁöÑÂêåÊó∂‰ºòÂåñÂÜ≥Á≠ñÊçüÂ§±„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÁêÜËÆ∫‰øùËØÅÔºö** ‰∏•Ê†ºËØÅÊòéDFFËÉΩÂ∞ÜÈ¢ÑÊµãÂÅèÂ∑ÆÈôêÂà∂Âú®È¢ÑËÆæ‰∏äÁïåÂÜÖÔºàÂÆöÁêÜ1ÔºâÔºåÊúâÈôêÊï∞ÊçÆ‰∏ãÂπ≥ÂùáÂÜ≥Á≠ñÈÅóÊÜæÈôç‰Ωé `24.37%`„ÄÇ\\n> *   **ÈÄöÁî®ÊÄßÔºö** È¶ñÊ¨°ÂÆûÁé∞DFLÂØπÈùûÂæÆÂàÜÊ®°ÂûãÁöÑÈÄÇÈÖçÔºåÂú®ÁΩëÊµÅ‰ºòÂåñÂíåÊäïËµÑÁªÑÂêàÈóÆÈ¢ò‰∏≠NDRÔºàÂΩí‰∏ÄÂåñÂÜ≥Á≠ñÈÅóÊÜæÔºâÈôç‰Ωé `15-35%`„ÄÇ\\n> *   **Â∑•‰∏öÈ™åËØÅÔºö** Âú®Êª¥Êª¥Âá∫Ë°åË°•Ë¥¥ÂàÜÈÖç‰ªªÂä°‰∏≠ÔºåDFFÂú®‰øùÊåÅMSEÊçüÂ§±Ôºà`0.49` vs. XGBoostÁöÑ`0.45`ÔºâÁõ∏ËøëÊó∂ÔºåÂÜ≥Á≠ñË¥®ÈáèÊèêÂçá `5.9%`„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ÈÄöËøáÁ∫¶ÊùüÂæÆË∞ÉÂå∫ÂüüÔºàtrust regionÔºâÂπ≥Ë°°ÂÜ≥Á≠ñÊçüÂ§±‰ºòÂåñ‰∏éÈ¢ÑÊµã‰øùÁúüÂ∫¶ÔºåÊ†∏ÂøÉËÆæËÆ°ÊòØÂçäÂèÇÊï∞ÂåñÂÅèÁΩÆÊ†°Ê≠£Â±ÇÔºö\\n>     $$F_\\\\theta(x) = \\\\phi(x) \\\\odot M(x) + b(x)$$\\n> *   ÊúâÊïàÊÄßÂéüÁêÜÔºöÊ†°Ê≠£Â±ÇËæìÂÖ•‰æùËµñÁöÑÁ∫øÊÄßÂèòÊç¢Ôºà$\\phi(x)$‰∏∫Áº©ÊîæÊùÉÈáçÔºå$b(x)$‰∏∫ÂÅèÁΩÆÔºâÂèØÈÄÇÈÖç‰ªªÊÑèÈ™®Âπ≤Ê®°ÂûãÔºåÂêåÊó∂Á∫¶Êùü$\\phi(x) \\\\in [1-\\\\epsilon, 1+\\\\epsilon]$Á°Æ‰øùÈ¢ÑÊµãÂÅèÂ∑ÆÂèØÊéß„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂØπÊØîÂÖàÂâçÂ∑•‰ΩúÔºö** ‰º†ÁªüDFLÔºàÂ¶ÇSPO+„ÄÅPFYLÔºâÁõ¥Êé•ËÆ≠ÁªÉÊ®°ÂûãÂØºËá¥È¢ÑÊµãÂÅèÁßªÔºàÂ¶ÇÂõæ1ÔºâÔºå‰∏îÊó†Ê≥ïÂ§ÑÁêÜÈùûÂæÆÂàÜÊ®°Âûã„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** \\n>     1. ÊèêÂá∫Á∫¶Êùü‰ºòÂåñÂΩ¢ÂºèÔºàÂÖ¨Âºè8ÔºâÔºåÈÄöËøáÊ¨ßÊ∞èË∑ùÁ¶ªÁ∫¶Êùü$\\\\mathbb{D}(F_\\\\theta(x), M(x)) \\\\leq \\\\epsilon$Ôºõ\\n>     2. ËÆæËÆ°ÁôæÂàÜÊØîËØØÂ∑ÆÂ∫¶ÈáèÔºàÂÖ¨Âºè10ÔºâÂÆûÁé∞Êó†ÈáèÁ∫≤ÂåñÊéßÂà∂Ôºõ\\n>     3. ÈááÁî®ÂÅèÁßªÁº©ÊîæSigmoidÔºàÂÖ¨Âºè11Ôºâ‰∏•Ê†º‰øùËØÅÁ∫¶Êùü„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1. **È™®Âπ≤Ê®°ÂûãÈ¢ÑËÆ≠ÁªÉÔºö** Áî®MSEÊçüÂ§±ËÆ≠ÁªÉÈ¢ÑÊµãÊ®°Âûã$M$ÔºàÂ¶ÇXGBoostÔºâ„ÄÇ\\n> 2. **ÂÅèÁΩÆÊ†°Ê≠£Â±ÇÊûÑÂª∫Ôºö** ËÆæËÆ°Á•ûÁªèÁΩëÁªú$F_\\\\theta$ÔºåÂÖ∂ËæìÂá∫Êª°Ë∂≥$\\\\tilde{c}_i/\\\\hat{c}_i \\\\in [1-\\\\epsilon,1+\\\\epsilon]$„ÄÇ\\n> 3. **ÂÜ≥Á≠ñÊçüÂ§±‰ºòÂåñÔºö** ÈááÁî®SPO+ÊçüÂ§±ÔºàÂÖ¨Âºè17ÔºâËÆ°ÁÆóÊ¢ØÂ∫¶Ôºö\\n>     $$\\\\frac{\\\\partial L}{\\\\partial \\\\theta} = \\\\frac{\\\\partial L_{SPO+}}{\\\\partial \\\\tilde{c}} \\\\cdot \\\\frac{\\\\partial \\\\tilde{c}}{\\\\partial \\\\theta}$$\\n> 4. **Á∫¶ÊùüÈ™åËØÅÔºö** ÈÄöËøáÂÆöÁêÜ1‰øùËØÅRMSEÂ¢ûÈáè‰∏äÁïå$\\\\frac{\\\\epsilon}{\\\\sqrt{d}}\\\\|\\\\hat{c}\\\\|_2$„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   Êª¥Êª¥Âá∫Ë°åË°•Ë¥¥ÂàÜÈÖçÊ°à‰æã‰∏≠ÔºàÂõæ3ÔºâÔºåDFFÊ†°Ê≠£ÂêéÁöÑÈ¢ÑÊµã‰øùÊåÅÂèåÂ≥∞ÂàÜÂ∏ÉÔºà‰∏éÁúüÂÆûÊï∞ÊçÆ‰∏ÄËá¥ÔºâÔºåËÄåNN-SPO+ÁöÑÈ¢ÑÊµãÂá∫Áé∞ÂçïÂ≥∞ÂÅèÁßªÔºåÈ™åËØÅ‰∫ÜÁ∫¶ÊùüËÆæËÆ°ÁöÑÊúâÊïàÊÄß„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   Random Forest/SPO Forest„ÄÅGradient Boosting/Dboost„ÄÅNN-MSE/NN-SPO+„ÄÅ2-fold Boosting„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®ÂΩí‰∏ÄÂåñÂÜ≥Á≠ñÈÅóÊÜæÔºàNDRÔºâ‰∏äÔºö** DFFÂú®ÁΩëÁªúÊµÅÈóÆÈ¢òÔºàDataset 1Ôºâ‰∏äËææÂà∞**4.39%**Ôºå‰ºò‰∫éSPO Forest (`5.99%`)ÂíåDboost (`5.86%`)ÔºåËæÉÊúÄ‰Ω≥Âü∫Á∫ø2-fold BoostÊèêÂçá`0.1`‰∏™ÁôæÂàÜÁÇπÔºõÂú®ÊäïËµÑÁªÑÂêàÈóÆÈ¢òÔºàDataset 2Ôºâ‰∏äNDR‰∏∫**15.91%**ÔºåÊòæËëó‰Ωé‰∫éNN-SPO+ (`23.55%`)„ÄÇ\\n> *   **Âú®È¢ÑÊµã‰øùÁúüÂ∫¶ÔºàMSEÔºâ‰∏äÔºö** DFFÂú®Êª¥Êª¥Ê°à‰æã‰∏≠MSE‰∏∫**0.49**Ôºå‰∏éXGBoost (`0.45`)Êé•Ëøë‰∏îËøú‰Ωé‰∫éNN-SPO+ (`2.04`)„ÄÇ\\n> *   **Âú®Ê®°ÂûãÂÖºÂÆπÊÄß‰∏äÔºö** DFFÈ¶ñÊ¨°ÂÆûÁé∞‰ªøÁúüÊ®°ÂûãÁöÑÂÜ≥Á≠ñ‰ºòÂåñÔºåNDRÈôç‰Ωé**24.37%**ÔºàË°®2‰∏ãÂçäÈÉ®ÂàÜÔºâÔºåËÄåÂÖ∂‰ªñDFLÊñπÊ≥ïÊó†Ê≥ïÂ§ÑÁêÜÊ≠§Á±ªÈùûÂæÆÂàÜÊ®°Âûã„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ÂÜ≥Á≠ñËÅöÁÑ¶Â≠¶‰π† (Decision-Focused Learning, DFL)\\n*   È¢ÑÊµã-‰ºòÂåñÊ°ÜÊû∂ (Predict-Then-Optimize, PO)\\n*   ÂÜ≥Á≠ñËÅöÁÑ¶ÂæÆË∞É (Decision-Focused Fine-tuning, DFF)\\n*   ÂÅèÁΩÆÊ†°Ê≠£Ê®°Âùó (Bias Correction Module, N/A)\\n*   ÊúâÈôêÊï∞ÊçÆÂ≠¶‰π† (Learning with Limited Data, N/A)\\n*   ÈùûÂæÆÂàÜÊ®°Âûã (Non-differentiable Models, N/A)\\n*   ËµÑÊ∫êÂàÜÈÖçÈóÆÈ¢ò (Resource Allocation Problem, N/A)\\n*   ÁΩëÊµÅ‰ºòÂåñ (Network Flow Optimization, NFO)\"\n}\n```"
}