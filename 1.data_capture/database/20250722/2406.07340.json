{
    "source": "Semantic Scholar",
    "arxiv_id": "2406.07340",
    "link": "https://arxiv.org/abs/2406.07340",
    "pdf_link": "https://arxiv.org/pdf/2406.07340.pdf",
    "title": "Formally Verified Approximate Policy Iteration",
    "authors": [
        "Maximilian Schäffeler",
        "Mohammad Abdulaziz"
    ],
    "categories": [
        "cs.AI",
        "cs.LO"
    ],
    "publication_date": "2024-06-11",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "Technische Universita¨t Mu¨nchen",
        "King’s College London"
    ],
    "paper_content": "# Formally Verified Approximate Policy Iteration\n\nMaximilian Scha¨ffeler1, Mohammad Abdulaziz2\n\n1Technische Universita¨t Mu¨nchen, Germany 2King’s College London, United Kingdom maximilian.schaeffeler@tum.de, mohammad.abdulaziz@kcl.ac.uk\n\n# Abstract\n\nWe present a methodology based on interactive theorem proving that facilitates the development of verified implementations of algorithms for solving factored Markov Decision Processes. As a case study, we formally verify an algorithm for approximate policy iteration in the proof assistant Isabelle/HOL. We show how the verified algorithm can be refined to an executable, verified implementation. Our evaluation on benchmark problems shows that it is practical. As part of the development, we build verified software to certify linear programming solutions. We discuss the verification process and the modifications we made to the algorithm during formalization.\n\nCode — https://github.com/schaeffm/fmdp isabelle\n\n# Introduction\n\nMarkov Decision Processes (MDPs) are models of probabilistic systems, with applications in AI, model checking, and operations research. In AI, for instance, given a description of the world in terms of states and actions that can change those states in a randomised fashion, one seeks a policy that determines the actions chosen in every state, with the aim of accruing maximum reward. There is a large number of methods to solve MDPs, most notably, value and policy iteration, which compute policies with optimality guarantees.\n\nIn many applications in AI or autonomous systems (Lahijanian et al. 2010; Junges et al. 2018), obtaining an optimal policy is safety-critical, with the goal of e.g. minimizing the number of accidents. One important aspect here is the assurance that the output of the MDP solving system is correct. Such assurance is currently attained to some degree by testing and other software engineering methods. However, the best guarantee can be achieved by mathematically proving the MDP solver and the underlying algorithm correct. A successful way of mathematically proving correctness properties of (i.e. formally verifying) pieces of software is using Interactive Theorem Provers (ITPs), which are formal mathematical systems that one can use to devise machine-checked proofs. Indeed, ITPs have been used to prove correctness properties of compilers (Leroy 2009), operating systems kernels (Klein et al. 2009), model checkers (Esparza et al. 2013), planning systems (Abdulaziz and Lammich 2018; Abdulaziz and Koller 2022; Abdulaziz and Kurz 2023), and, most related to the topic of this work, algorithms to solve MDPs (Scha¨ffeler and Abdulaziz 2023). A challenge with using ITPs to prove algorithms correct, nonetheless, is that they require intense human intervention. Thus for an ITP to be successfully employed in a serious verification effort, novel ideas in the design of the software to be verified as well as the underlying proof have to be made.\n\nIn this paper, we consider formally verifying algorithms for solving factored MDPs. A challenge to using MDPs to model realistic systems is their, in many cases, enormous size. For such systems, MDPs are succinctly represented as factored MDPs. The system’s state is characterised as an assignment to a set of state variables and actions are represented in a compact way by exploiting the structure present in the system. Such representations are common in AI (Guestrin et al. 2003; Sanner 2010; Younes and Littman 2004) and in model checking (Hinton et al. 2006; Dehnert et al. 2017). Although ITPs have been used prove the correctness of multiple types of software and algorithms, including algorithms on MDPs, algorithms on factored MDPs are particularly challenging. The root of this difficulty is that the succinctness of the representation comes at a cost. Naively finding a solution for a factored MDP could entail the construction of structures exponentially bigger than the factored MDP. This necessitates using advanced data structures, heuristics and computational techniques, to avoid that full exponential blow up.\n\nOur main contribution is that we develop a methodology based on using the Isabelle locale system, to structure the MDP solving algorithm into parts amenable to verification. To enable this methodology, we build a formal mathematical library allowing the specification of algorithms for solving factored MDPs and their properties, like algorithms for planning under uncertainty and probabilistic model checking. We also develop a number of reusable building blocks to be used in other algorithms, e.g. a certificate checker for linear programming solutions. Potential targets for formalization include probabilistic model checking, planning and reinforcement learning algorithms (Hartmanns et al. 2023; Keller and Eyerich 2012). We describe the methodology in terms of the verification of Guestrin et al.’s approximate policy iteration algorithm in the Isabelle theorem prover.\n\nThis algorithm computes approximate policies, i.e. suboptimal policies with guarantees on their optimality, for one type of factored MDPs. The algorithm we consider combines scoped functions and decision lists, which are data structures that exploit the factored representation, linear programming, in addition to probabilistic reasoning and dynamic programming. The combination of this wide range of mathematical/algorithmic concepts and techniques is what makes this algorithm particularly hard from a verification perspective. To get an idea of the scale, we advise the reader to look at Fig. 2, which shows the hierarchy of concepts and definitions which we had to develop (aka formalize) within Isabelle/HOL to be able to state the algorithm and prove its correctness statement. This is, of course, in addition to the notions of analysis, probabilities, and MDPs, which already exist in Isabelle/HOL. Furthermore, to be able to prove the algorithm correct, we had to design an architecture of the implementation that makes verification feasible. Our architecture mixes verification and certification: we verify the entire algorithm, and build a verified certificate checker for linear programming solutions that delivers formal guarantees. In addition to proving the algorithm correct, we obtain a formally verified implementation, that we experimentally show to be practical. Our work, as far as we are aware, is the first work on formally verifying algorithms for factored MDPs.\n\n# Background\n\nWe introduce the interactive theorem prover Isabelle/HOL, our formal development of factored MDPs relate it to existing formalizations in Isabelle/HOL.\n\n# Isabelle/HOL\n\nAn ITP is a program that implements a formal mathematical system, in which definitions and theorem statements can be expressed, and proofs are constructed from a set of axioms. To prove a fact in an ITP, the user only provides the high-level steps while the ITP fills in the details at the level of axioms. Specifically, our developments use the interactive theorem prover Isabelle/HOL (Nipkow, Paulson, and Wenzel 2002) based on Higher-Order Logic, a combination of functional programming with logic. Isabelle is highly trustworthy, as the basic inference rules are implemented in a small, isolated kernel. Outside the kernel, several tools implement proof tactics, data types, recursive functions, etc..\n\nOur presentation of definitions and theorems here deviates slightly from the formalization in Isabelle/HOL. Specifically, we use subscript notation for list indexing and some function applications. We use parentheses for function application. For a list $x s$ , $l e n ( x s )$ returns the length, while $x s _ { : i }$ returns the first $i$ elements of $x s$ . List concatenation is written as $x s \\cdot y s$ , $x : x s$ inserts $x$ at the front of the list $x s$ , $m a p ( f , x s )$ applies $f$ to every element of $x s$ . Finally, $\\mathbb { N } _ { < i }$ is short for the first $i$ natural numbers (including 0).\n\n# Factored MDPs\n\nFactored MDPs are compactly represented MDPs that exploit regularities in large MDPs, which can often lead to an exponential reduction in the size of the model. Common formats to store factored MDPs include JANI (Budde et al. 2017), the PRISM language (Hinton et al. 2006), and RDDL (Sanner 2010). We implement the factoring described in (Guestrin et al. 2003). In Isabelle/HOL, we define factored MDPs using locales (Ballarin 2014). A locale introduces a mathematical context with constants and assumptions, in which we develop our formalization. Locales can be instantiated with concrete constants and a proof that discharges the assumptions of the locale, yielding all the theorems proved within the locale. For example, we instantiate the MDP locale from Scha¨ffeler and Abdulaziz with the factored systems introduced in this section, and are therefore able to reuse important definitions and theorems. Moreover, reducing factored MDPs to a tested and reviewed library enhances confidence in our definitions.\n\nState Space A state of a factored MDP is an assignment of values to its $n$ state variables. Each state variable $i \\in \\mathbb { N } _ { < n }$ has a finite, nonempty domain $X _ { i }$ . In Isabelle/HOL, we implement an MDP state $x$ as a map, i.e. a function with an explicit domain $d o m ( x )$ . A partial state is a map where only a subset of the state variables are assigned, but all entries are valid, i.e. $d o m ( x ) \\subseteq \\mathbb { N } _ { < n }$ and $x _ { i } \\ \\in \\ X _ { i }$ for $i \\in d o m ( x )$ . The set $X$ of states of the MDP consists of all partial states $x$ where $d o m ( x ) = \\mathbb { N } _ { < n }$ . The domain of a state $x$ can be restricted to a set of variables $Y$ , denoted as $x | _ { Y }$ . One partial state $x$ is called consistent with another partial state $t$ (written $x \\subseteq t )$ , if and only if $x | _ { d o m ( t ) } = t$ .\n\nExample As a running example, we use a model of a computer network with ring topology from Guestrin et al.’s original paper (see Fig. 1). In the ring, each machine is either working or broken, and states change stochastically. Each machine $C _ { i }$ ’s state of operation is characterized by a variable $i$ , s.t. all domains $X _ { i } = \\{ { \\bf W } , { \\bf B } \\}$ . In a ring with three machines, a partial state is $s : = [ 1 \\mapsto \\mathbb { W } , 3 \\mapsto \\mathbf { B } ]$ , $d o m ( s ) =$ $\\{ 1 , 3 \\}$ . It holds that $s \\subseteq [ 1 \\mapsto \\bar { \\mathbb { W } } ]$ , but $s \\not \\subseteq [ 2 \\mapsto \\mathbf { B } ]$ .\n\nScoped Functions For many MDPs, the transition behavior and the rewards can be computed from a combination of functions that individually only depend on a small subset of all state variables. Such scoped functions take a partial state as input and determine the output inspecting only variables within their scope. The algorithm we formalize expresses policy iteration using scoped functions, which avoids enumerating the full state space. Scoped functions pose a challenge for formalization, as scopes could be represented implicitly or explicitly: we can either prove that a given function has a restricted scope, or the function can store its scope explicitly. In Isabelle/HOL we do both: decoupling scopes from the function definition is more flexible, as one may derive multiple scopes for a function. However, since it is in general infeasible to compute the precise scope of a function, we use explicit scopes in the executable version. An important operation on scoped functions is the instantiation with a partial state $t \\colon i n s t _ { t }$ applied to a scoped function returns a new scoped function with reduced scope, where all input dimensions that $t$ provides are fixed to the values of $t$ .\n\n![](images/6a85ed5819ef703945c143359e841a67a52f8aa69cb99abf3c2b6aae7737433b.jpg)  \nFigure 1: (a) Ring network of 3 machines. (b) Variable dependencies of the default action. (c) Probabilities of $C _ { 1 }$ working in the next step, for every state of $C _ { 1 }$ and $C _ { 3 }$ .\n\nTransitions In our setting, a factored MDP comes with a finite set of actions $A$ , and a default action $d \\in A$ . For each action $a \\in A$ , transition probabilities ${ \\mathcal P } _ { i } ^ { a } : X  \\mathbb P ( X _ { i } )$ with $s c o p e ( \\mathcal { P } _ { i } ^ { a } ) ~ \\subseteq ~ \\mathbb { N } _ { < n }$ determine the evolution of variable $i$ . Here, $\\mathbb { P } ( X )$ denotes the set of probability distributions over a finite set $X$ . The set $e f f e c t s _ { a }$ defines the state variables where the behavior of $a$ differs from the default action (i.e. variables $i$ s.t. $\\mathcal { P } _ { i } ^ { a } \\neq \\mathcal { P } _ { i } ^ { d } )$ . The combined transition probabilities between two states $x$ and $x ^ { \\prime }$ are defined as $\\begin{array} { r } { \\mathcal { P } ^ { a } ( \\dot { x } , x ^ { \\prime } ) : = \\prod _ { i < n } \\mathcal { P } _ { i } ^ { a } ( x , x _ { i } ^ { \\prime } ) } \\end{array}$ .\n\nExample In the ring topology domain, for each machine there is an action to restart it, in which case it is guaranteed to work in the next step. On the other hand, the default action $d$ is to do nothing. In general, the probability of a machine working in the next step depends on its own state and the state of the predecessor, e.g. $\\overset { \\cdot } { s c o p e } ( \\mathcal { P } _ { 2 } ^ { d } ) = \\{ 1 , 2 \\}$ . The exact conditional probability distribution for the MDP’s evolution under the default action is shown in Fig. 1. Using an explicit representation to model this factored action, we would need 8 transitions, each consisting of a distribution over 8 possible successor states. This is in contrast to three tables in the factored case. A common way to model the transition behavior is using dependency graphs as shown in Fig. 1.\n\nRewards The actions $a \\in A$ define scoped reward functions $R _ { i } ^ { a } : X  \\mathbb { R }$ for $\\boldsymbol { i } < \\boldsymbol { r } _ { a }$ . The reward for selecting $a$ is a sum of those reward functions: $\\begin{array} { r } { R ^ { a } ( x ) : = \\sum _ { i < r _ { a } } \\bar { R _ { i } ^ { a } } ( x ) } \\end{array}$ . We assume that the first $r _ { d }$ reward functions are the same for all actions. Now, given a policy $\\pi : X  A$ we are interested in the discounted expected total reward $\\nu _ { \\pi } ( x ) : =$ $\\begin{array} { r } { \\mathbb { E } _ { \\omega \\sim \\mathcal { T } ( \\pi , x ) } \\left[ \\sum _ { i } \\gamma ^ { i } R ^ { \\pi ( \\omega _ { i } ) } ( \\omega _ { i } ) \\right] } \\end{array}$ with discount factor $\\gamma < 1$ and trace space $\\tau$ . Our goal is to achieve the optimal reward $\\begin{array} { r } { \\nu ^ { * } ( x ) : = \\operatorname* { s u p } _ { \\pi \\in \\Pi } \\nu _ { \\pi } ( x ) } \\end{array}$ . For a value estimate $v : X \\to \\mathbb { R }$ and an action $a$ , the one-step lookahead is defined as\n\n$$\n\\begin{array} { r } { Q _ { v } ^ { a } ( x ) : = R ^ { a } ( x ) + \\gamma \\sum _ { x ^ { \\prime } \\in X } \\mathcal { P } ^ { a } ( x , x ^ { \\prime } ) \\cdot v ( x ^ { \\prime } ) . } \\end{array}\n$$\n\nFor each state $x$ , the maximum lookahead w.r.t. all actions is $Q _ { v } ^ { * } ( x )$ . A policy $\\pi$ is called greedy if $Q _ { v } ^ { \\pi }$ and $Q _ { v } ^ { * }$ are equal for all states. The Bellman error, denoted by $\\| v - Q _ { v } ^ { \\pi } \\|$ , is the maximum difference between $Q _ { v } ^ { \\pi }$ and $v$ , over all states in $X$ , i.e. the $L _ { \\infty }$ distance.\n\n# Algorithm 1: Approximate Policy Iteration (API)\n\nExample In our example, a factored representation of the rewards $R _ { i } ^ { d }$ is $R _ { i } ^ { d } ( \\mathbf { W } ) = 1$ and $R _ { i } ^ { d } ( { \\bf B } ) = 0$ , for all $1 \\leq i \\leq$ 3. This gives rise to an exponentially smaller representation compared to an explicitly represented MDP, where the reward function for $d$ would have 8 entries.\n\n# Linear Value Functions\n\nEven if all reward and transition functions are scoped functions, the value function $\\nu _ { \\pi }$ may still be unstructured, i.e. computing $\\nu _ { \\pi }$ might require the construction of an exponentially big mapping (Guestrin et al. 2003). However, the value of a policy can be approximated as the weighted sum of $m$ basis functions $h _ { i } : X \\to \\mathbb { R }$ . Given weights $w _ { i }$ for each $h _ { i }$ , the value of a state $x$ is defined as a weighted sum $\\begin{array} { r } { \\nu _ { w } ( x ) : = \\sum _ { i < m } w _ { i } h _ { i } ( x ) } \\end{array}$ . Note that the efficiency of the algorithm we verify here crucially depends on the fact that $h _ { i }$ is a function with small scope.\n\nFor each action choice $a$ and basis function $h _ { i }$ , we can compute its expected evaluation $g _ { i } ^ { a }$ , defined as $\\textstyle \\sum x ^ { \\prime } \\in X . \\ { \\bar { \\mathcal { P } } } ^ { a } ( x , x ^ { \\prime } ) \\cdot { \\bar { h } } _ { i } ( x ^ { \\prime } )$ , in the successor state. As $g$ is independent of the concrete weights, it can be computed once for a set of basis functions, and is then cached for efficiency. In Isabelle/HOL, we prove that $g _ { i } ^ { a } ( x )$ has a structured representation with scope $\\Gamma _ { i } ^ { a } : = \\begin{array} { l } { \\begin{array} { r l r l } \\end{array} } \\end{array}$ j scope(hi) . scope(Pja). This also leads to an efficient computation of the $Q$ functions:\n\n$$\n\\begin{array} { r } { Q _ { w } ^ { a } ( x ) : = Q _ { \\nu _ { w } } ^ { a } ( x ) = R ^ { a } ( x ) + \\gamma \\sum _ { i < m } w _ { i } g _ { i } ^ { a } ( x ) . } \\end{array}\n$$\n\nExample The value functions in the ring domain can be approximately represented using the basis function $h _ { 0 } = 1$ and one function per machine: $h _ { i } = 1$ if $X _ { i } = \\mathbf { W }$ and 0 otherwise, for $1 \\leq i \\leq 3$ . Note that since these basis functions have very limited scopes, the accuracy of the best possible approximation of the value function is also limited.\n\n# Approximate Policy Iteration\n\nApproximate Policy Iteration (API) is a variant of policy iteration that exploits structure in MDPs to scale to large systems (Guestrin et al. 2003). API is an iteration algorithm, where each iteration consists of three parts: policy evaluation, policy improvement and Bellman error computation. The algorithm terminates when either a timeout $t _ { m a x }$ is reached, the error dips below a threshold $\\epsilon$ , or the weights assigned to the basis functions converge. In Isabelle/HOL, the algorithm is implemented as a function $a p i ( t , \\pi , w )$ (Alg. 1), that takes as inputs a time step $t$ , weights for the basis functions $w$ and a policy $\\pi$ . The initial call to the algorithm is $a p i ( 0 , \\pi ^ { 0 } , w ^ { 0 } )$ where $w _ { 0 } = 0$ and $\\pi ^ { 0 }$ is some greedy policy w.r.t. $w ^ { 0 }$ . An iteration of API first uses the current policy $\\pi$ to first compute updated weights $w ^ { \\prime }$ , then a new greedy policy $\\pi ^ { \\prime }$ , and finally the Bellman error err of $\\pi ^ { \\prime }$ . If the termination condition is met, the algorithm returns the current iteration, weights and policy, as well as the error and whether the weights converged. Otherwise, api is called recursively.\n\nAlgorithm 2: Decision List Policy   \n\n<html><body><table><tr><td>greedy_π:= sort_π((⊥m,d,O):: concat([πa |a ∈ A-{d}]) whereπa :=[(x,a,δa(x))|δa(x)>0,x ∈X|Ta] Sa:=Qa-Qd Ta:= scope(Ra)UUieITg Urd Ia:={i<m|effectsa∩ scope(hi)≠0}</td></tr></table></body></html>\n\nWe structure and decouple the algorithm using locales. Conceptually, this usage of locales is similar to using modules in programming languages. Here, we use locales to postulate the existence of three functions (upd w, greedy $\\boldsymbol { \\cdot } \\pi$ , factored err ) along with their specifications:\n\nSpecification 1 $( u p d _ { - } w )$ . A decision list policy a list representation of policies where each entry (called branch) is a pair $( t , a )$ of a partial state and an action. To select an action in a state $x$ , we search the list for the first branch where $x$ is consistent with $t$ . Now fix a decision list policy $\\pi$ , let $w ^ { \\prime } = u p d _ { - } w ( \\pi )$ . Then $\\nu _ { w ^ { \\prime } }$ is the best possible estimate of $\\nu _ { \\pi }$ $\\begin{array} { r } { \\scriptsize _ { \\pi ^ { \\vdots } } \\| \\nu _ { w ^ { \\prime } } - \\nu _ { \\pi } \\| = \\operatorname* { i n f } _ { w } \\| \\nu _ { w } - \\nu _ { \\pi } \\| } \\end{array}$ .\n\nSpecification 2 (greedy $\\pi$ ). For all weights $w$ , greedy $\\pi ( w )$ is a greedy decision list policy for $\\nu _ { w }$ .\n\nSpecification 3 (factored err). Given weights $w$ and a greedy decision list policy $\\pi$ for $\\nu _ { w }$ , factored err determines the Bellman error: factored $\\ b _ { - } e r r ( \\ b { \\pi } , \\ b { w } ) = \\| Q _ { \\ b { w } } ^ { * } - \\nu _ { w } \\|$ .\n\nFor now, we merely state specifications for the algorithms API builds upon, only later will we show how to implement the specifications concretely. This approach keeps the assumptions on individual parts of the algorithm explicit and permits an easier exchange of implementations, e.g. in our developments one may swap the LP certification algorithm for a verified LP solver implementation. It also facilitates gradual verification of software: the correct behavior of the software system can be proved top-down starting from assumptions on each component. In the following sections we show that these specifications have efficient implementations. See Fig. 2 for an overview of all components.\n\nThe choice of the specifications and the decomposition of the algorithm into components is roughly based on the presentation of the algorithm by Guestrin et al.. We identified the above specifications after multiple iterations. There is usually a trade-off between the simplicity of the specification and the complexity of the implementation: the smaller the internal complexity, the more complex the external complexity, i.e. the interactions between algorithms.\n\nWithin the context of the locale, assuming all specifications, we can derive the same error bounds as presented by Guestrin et al.. One exemplary important observation is that if the weights converge during API, then in the last step the Bellman error equals the approximation error. This leads to the following a posteriori optimality bound:\n\n<html><body><table><tr><td>Algorithm3:Factored Bellman Error</td></tr><tr><td>factored_err:=</td></tr><tr><td>supi<len(π) branch_err(πi,map(fst,π:i))</td></tr></table></body></html>\n\nTheorem 1. Let api $( w _ { 0 } , \\pi _ { 0 } ) = ( t ^ { \\prime } , \\pi , w , e r r , T r u e )$ . Then $( 1 - \\gamma ) \\vert \\vert \\nu ^ { * } - \\nu _ { w } \\vert \\vert \\leq 2 \\gamma \\cdot e r r .$ .\n\n# Policy Improvement\n\nGiven weights $w$ , the policy improvement phase determines a greedy policy w.r.t. $\\nu _ { w }$ . The policy takes the form of a decision list, where each element is a pair of a partial state and an action. The main idea for an efficient computation is to only consider actions better than the default action. This notion is made precise by the bonus function $\\delta _ { a }$ (Alg. 2) with scope ${ \\mathbf { T } } _ { a }$ . Guestrin et al. do not include $\\Gamma ^ { d }$ in ${ \\mathbf T } _ { a }$ , which we assume to be an oversight in the definition: the behavior of the default action does influence the bonus. Unless components cancel out, the scope of a function difference is the union of the scopes of both arguments. Finally, we concatenate the branches $\\pi _ { a }$ for every action but $d$ , add the default action as a fallback and sort the decision list policy by decreasing bonus. Here, the empty map with no entries is $\\perp _ { M }$ . We can show that greedy $\\pi$ satisfies Spec. 2, as action selection proceeds in the order of decreasing bonus.\n\n# Factored Bellman Error\n\nThe Bellman error $\\| Q _ { w } ^ { * } - \\nu _ { w } \\|$ is an indicator of the degree of optimality of a policy. An inefficient computation would enumerate every state, and return the maximum error. However, for a decision list policy, we can compute the error incurred by each branch separately. The total error then equals the maximum error of any branch (Alg. 3). For now, assume that we have a function branch err that computes the error for a single branch, i.e. the maximum error for any state that selects the respective branch. These are all states that are consistent with the current branch $t$ , but did not match any prior branch $t ^ { \\prime } \\in t s$ of the policy, formally $X _ { ( t , t s ) } : = \\{ x \\in X . t \\ \\subseteq \\ x \\wedge \\forall t ^ { \\prime } \\in t s . t ^ { \\prime } \\ \\not \\subseteq \\ x \\}$ . Hence, we also need to pass the prefix of the decision list policy to branch err . Note that if the branch is selected by no state its error defined as $- \\infty$ . We show that if Spec. 4 is met and $\\pi$ is a greedy policy w.r.t. $w$ , then factored err satisfies Spec. 3.\n\nSpecification 4 (branch err). Given a prefix of a policy $\\pi$ , i.e. the current branch $( t , a )$ , and a list of partial states ts from prior branches, branch e $r r ( t , a , t s ) ~ =$ $\\begin{array} { r } { \\operatorname* { s u p } _ { x \\in X _ { ( t , t s ) } } | Q _ { w } ^ { a } ( x ) ^ { \\top } - \\nu _ { w } ( x ) | } \\end{array}$ .\n\nBranch Error Consider the branch $( t , a )$ of the policy, for a partial state $t$ an action $a$ . The states of all prior branches form the list of partial states $t s$ . To find the Bellman error of\n\nApproximate Policy Iteration Alg 1: Spec 1, 2 $\\therefore 3 \\Rightarrow \\mathrm { T h m } 1$ → Decision List Policy Value Determination Factored Bellman Error Alg 2: Spec 2 Spec $6 \\Rightarrow$ Spec 1 Alg 3: Spec $4 \\Rightarrow$ Spec 3 1 2 个 LP Branch Branch Error LP Certification Alg 6: Spec $^ { 7 \\Rightarrow }$ Spec 6 Alg 4: Spec $5 \\Rightarrow$ Spec 4 个 $\\uparrow$ Factored LP Variable Elimination Spec 7 Alg 5: Spec 5\n\n<html><body><table><tr><td colspan=\"2\">Algorithm4:BranchError</td></tr><tr><td>branch_err := max(maxε(fs·I'),maxε(-fs ·I')) where rs :=[Rδ,.. .,Rra] ws := [wo(ho- Ygo),...,Wm(hm- Ygm)]</td><td></td></tr><tr><td>fs := map(instt,rs·ws)</td><td></td></tr><tr><td>Algorithm5:VariableElimination</td><td></td></tr><tr><td>max_step(i,fs) := (i+1,e::E') where e:= x -→ maxyexo()∑f∈E f(xo(i)→y) (E,E'):= partition(f →O(i)∈ scope(f),fs)</td><td></td></tr></table></body></html>\n\nthe current branch, we need to maximize $| Q _ { w } ^ { a } ( x ) - \\nu _ { w } ( x ) |$ w.r.t. states $x \\in X _ { ( t , t s ) }$ . Note that for all states $x$\n\n$$\nQ _ { w } ^ { a } ( x ) - \\nu _ { w } ( x ) = \\sum _ { i < r _ { a } } R _ { i } ^ { a } ( x ) + \\sum _ { i < m } w _ { i } ( h _ { i } - \\gamma g _ { i } ) ( x ) .\n$$\n\nSuppose we had an algorithm to efficiently compute the maximum sum of scoped functions. In that case we could determine the error of a branch. Again, we specify an algorithm $m a x _ { \\Sigma }$ doing exactly that (Spec. 5). In Alg. 4, we call $m a x _ { \\Sigma }$ with the functions from (1). To restrict the maximization to the states $X _ { ( t , t s ) }$ , we instantiate all functions with the partial state $t$ . Additionally, we define the functions $\\mathcal { T } ^ { \\prime }$ that evaluate to $- \\infty$ on states that select a different branch of the policy. Hence, these states are ignored in the error computation. We also apply $m a x _ { \\Sigma }$ to the negated functions to compute the absolute value of the error. Finally, we formally prove that branch err satisfies Spec. 4.\n\nSpecification 5 (Variable Elimination). For scoped functions $\\begin{array} { r } { f s , m a x _ { \\Sigma } ( f s ) = \\operatorname* { s u p } _ { x \\in X } \\sum _ { f \\in f s } f ( x ) } \\end{array}$ .\n\nVariable Elimination The specification for $m a x _ { \\Sigma }$ can be efficiently implemented with a variable elimination algorithm (Alg. 5). In each iteration, the algorithm selects a dimension of the state space, collects all functions that depend on this dimension in a set $E$ . It then creates a new function $e$ that maximizes all functions in $E$ over that dimension of the state space. The algorithm keeps track of a set of functions to maximize and the number of the current iteration. Since the number of operations performed by the algorithm varies greatly with the elimination order, the variables can be reordered with a bijection ${ \\mathcal { O } } : \\mathbb { N } _ { < n } \\to \\mathbb { N } _ { < n }$ . We formally prove that max step preserves the maximum of the current list of functions, thus $m a x _ { \\Sigma }$ meets Spec. 5.\n\n# Value Determination\n\nAfter a new candidate policy is found, it is evaluated in the value determination phase. Since the exact value function can not in general be represented as a linear combination of the basis functions, we aim to find weights for the basis functions that minimize the approximation error (see Spec. 1), for which we need to solve a linear program (LP). The structure of the algorithm that finds optimal weights is analogous to the factored Bellman error computation. For each branch of the policy, we generate a set of LP constraints (according to Spec. 6) that expresses the approximation error incurred by this branch. The union of all constraints is then\n\n$$\n\\begin{array} { r } { w e i g h t \\_ l p : = \\bigcup _ { i < l e n ( \\pi ) } b r a n c h \\_ l p ( \\pi _ { i } , m a p ( f s t , \\pi _ { : i } ) ) . } \\end{array}\n$$\n\nVariables of the LP are the approximation error $\\phi$ to minimize, and the weights $w$ that determine the new weights. Given a set of LP constraints cs, $\\langle c s \\rangle _ { L P }$ denotes the set of feasible solutions. We show that for any optimal solution $( \\phi ^ { * } , w ^ { * } ) \\in \\langle w e i g h t \\_ l p \\rangle _ { L P }$ , u $p d _ { - } w : = w ^ { * }$ satisfies Spec. 1. We formally prove that the LP always has an optimal solution, as the set of potentially optimal solutions is compact.\n\nSpecification 6 (branch lp). Given a partial state $t$ , an action $a$ , a list of partial states ts, branch lp constructs an LP that minimizes the approximation error for the states $X _ { ( t , t s ) }$ :\n\n$$\n\\begin{array} { r l } & { ( \\phi , w ) \\in \\langle b r a n c h \\_ l p ( t , a , t s ) \\rangle _ { L P } \\longleftrightarrow } \\\\ & { \\qquad \\forall x \\in X _ { ( t , t s ) \\cdot } \\phi \\geq | Q _ { w } ^ { a } ( x ) - \\nu _ { w } ( x ) | . } \\end{array}\n$$\n\nLPs for Branches For each branch of the policy, we proceed similarly to the Bellman error computation: we create two constraint sets, for positive and negative errors respectively (Alg. 6). We omit the scopes for brevity. At this level, we make use of another algorithm min lp. Its first input $C$ is a list of $m$ scoped functions, the second input $b$ is another list of scoped functions. Now min $. l p ( C , b )$ creates an LP that minimizes $C w - b$ w.r.t. $w$ over all states (see Spec. 7). The definitions of $C$ and $b$ are analogous to the Bellman error computation. It then follows that branch lp fulfills Spec. 6.\n\n<html><body><table><tr><td>Algorithm 6:Branch LP</td></tr><tr><td>branch_lp :=</td></tr><tr><td>min_lp(C,-b·I')U min_lp(-C,b·I')</td></tr><tr><td>whereb := map(instt,[Ri|i< ra])</td></tr><tr><td>C := map(instt,[hi-Yg|i < m])</td></tr></table></body></html>\n\nSpecification 7 $( m i n _ { - } l p )$ . min $. l p ( C , b )$ generates an LP that minimizes $C w - b$ over all weights $w$ :\n\n$$\n\\begin{array} { r } { ( \\phi , w ) \\in \\langle m i n . l p ( C , b ) \\rangle _ { L P } \\longleftrightarrow \\qquad } \\\\ { \\forall x \\in X . \\phi \\geq \\sum _ { i < l e n ( C ) } w _ { i } C _ { i } ( x ) + \\sum _ { i < l e n ( b ) } b _ { i } ( x ) . } \\end{array}\n$$\n\nFactored LP Construction The algorithm min lp resembles $m a x _ { \\Sigma }$ , so we only point out the challenges encountered during verification. Full details can be found in the formalization. There are two significant modifications we made to the algorithm to make verification feasible. First, the original algorithm may create equality constraints that constrain variables to $- \\infty$ . Since these constraints are not supported by the LP solvers we use, we formally prove that one can modify the algorithm to omit such constraints without changing the set of feasible weights. Second, the combination of LP constraints in the definition of weight lp requires some care, to avoid interactions between LP variables created in different branches. The min lp algorithm creates new (private) LP variables and we need to make sure that these variables have distinct names for each branch. This issue was not discussed by Guestrin et al.. The problem can be solved by adding a tag to each generated variable. The tags contain $t , a ,$ , and a flag to differentiate the two invocations of min $l p$ in each branch. For distinct tags $p$ and $p ^ { \\prime }$ we can then show that the solutions to the union of two constraint sets are equivalent to the intersection of the solution spaces of the individual constraint sets (concerning $\\phi$ and $w$ ):\n\n$$\n\\begin{array} { r l } & { \\langle m i n \\_ l p ( p , C , b ) \\cup m i n \\_ l p ( p ^ { \\prime } , C ^ { \\prime } , b ^ { \\prime } ) \\rangle _ { L P } = } \\\\ & { \\quad \\langle m i n \\_ l p ( p , C , b ) \\rangle _ { L P } \\cap \\langle m i n \\_ l p ( p ^ { \\prime } , C ^ { \\prime } , b ^ { \\prime } ) \\rangle _ { L P } . } \\end{array}\n$$\n\nWe show that min lp creates an LP that is equivalent to the explicit (potentially exponentially larger) LP that has a constraint for each MDP state. It immediately follows that min lp satisfies Spec. 7, which completes our correctness proof of API. With this approach to algorithm verification using loosely coupled locales, min $l p$ and $m a x _ { \\Sigma }$ are not tied to MDPs and are thereby reusable components.\n\n# Code Generation\n\nWe now discuss the process of deriving a verified efficiently executable version from the verified abstract algorithm discussed above. To do so, we follow the methodology of program refinement (Wirth 1971), where one starts with an abstract, potentially non-executable version of the algorithm and verifies it. Then one devises more optimised versions of the algorithm, and only proves the optimizations correct in this latter step, thus separating mathematical reasoning from implementation specific reasoning. This approach was used in most successful algorithm verification efforts (Klein et al. 2009; Esparza et al. 2013; Kanav, Lammich, and Popescu 2014). In this work, the three most important stages are the initial abstract algorithm, an implementation with abstract data structures, and finally an implementation with concretized data structures. As a last step, we export verified code for API in the programming language Scala.\n\nTable 1: Evaluation on the ring and star domains. The first column denotes the number of clients. For each topology, the first two columns give the total running time and time spent in the LP solver. For the ring domain, we also show the number of LP constraints and variables generated.   \n\n<html><body><table><tr><td rowspan=\"2\"></td><td colspan=\"3\">Ring</td><td rowspan=\"2\"></td><td colspan=\"2\">Star</td></tr><tr><td>t(s)</td><td>tLP(s)</td><td>Constrs</td><td>Vars</td><td>t(s) tLP(s)</td></tr><tr><td>n 1</td><td>0.27</td><td>0.02</td><td>74</td><td>41</td><td>0.34</td><td>0.03</td></tr><tr><td>3</td><td>0.89</td><td>0.15</td><td>1258</td><td>693</td><td>0.50</td><td>0.05</td></tr><tr><td>5</td><td>1.89</td><td>0.46</td><td>4378</td><td>2455</td><td>0.69</td><td>0.08</td></tr><tr><td>7</td><td>3.78</td><td>0.98</td><td>9418</td><td>5305</td><td>0.98</td><td>0.14</td></tr><tr><td>9</td><td>6.74</td><td>1.82</td><td>16378</td><td>9243</td><td>1.30</td><td>0.22</td></tr><tr><td>11</td><td>12.44</td><td>3.36</td><td>25258</td><td>14269</td><td>1.52</td><td>0.29</td></tr><tr><td>13</td><td>20.95</td><td>5.31</td><td>36058</td><td>20383</td><td>1.76</td><td>0.36</td></tr><tr><td>15</td><td>34.69</td><td>8.67</td><td>48778</td><td>27585</td><td>2.28</td><td>0.50</td></tr><tr><td>17</td><td>58.30</td><td>16.86</td><td>63418</td><td>35875</td><td>2.89</td><td>0.64</td></tr><tr><td>19</td><td>92.19</td><td>30.25</td><td>79978</td><td>45253</td><td>3.69</td><td>0.80</td></tr></table></body></html>\n\nRefinement using Locales Our implementation of stepwise refinement is based on Isabelle/HOL locales. For each locale of the abstract algorithm, we define a corresponding locale where we define the executable version of the algorithm. Finally, we relate the abstract version of the MDP to the concrete version. For each definition, we then show that corresponding inputs lead to corresponding outputs, i.e. our abstract algorithm and the implementation behave the same. At this point in the refinement, data structures remain abstract interfaces, with the concrete implementations chosen only later. We use the data structures provided by the Isabelle Collections Framework (Lammich and Lochbihler 2019) for code generation and we extend them with a data structure for scoped functions, represented as a pair of a function and a set for its scope. The data structure also provides an operation to evaluate a function on its full scope for memoization.\n\nCertification of LP Solutions An implementation of API depends on efficient LP solvers. In our verified implementation, we use precise but unverified LP solvers and certify their results. This avoids implementing a verified, optimized LP solver but retains formal guarantees – the tradeoff here is that the unverified LP solver might return solutions that cannot be certified. At the cost of performance, it is also possible to connect the formalization to an existing simplex implementation for Isabelle/HOL (Spasic´ and Maric´ 2012). To achieve formal guarantees, the LP has to be solved exactly, i.e. using rational numbers. Two potential candidates for precise LP solvers are QSopt ex (Applegate et al. 2007) and SoPlex (Bestuzheva et al. 2023). For larger LPs in our setting, SoPlex demonstrated more consistent performance.\n\nWe certify optimality using the dual solution and the strong duality of linear programming. In our formalization we also formally prove that infeasibility and unboundedness can be certified similarly using farkas certificates or unbounded rays. The linear program is first preprocessed to a standard form: variable bounds and equality constraints are reduced to inequality constraints. The constraints of the resulting LP are of the form $A x \\leq b$ , with no restrictions on $x$ . During benchmarking of the certification process, the normalization operation usually applied to rationals after each operation proved to be very costly. For certification, we represent rational numbers as pairs that are never normalized, which leads to faster certificate checking in our experiments.\n\nThe exported Scala program takes an arbitrary function from linear programs to their solutions as input. If this function returns invalid solutions, they are rejected by the certificate checker, so there are no assumptions we need to place on the LP solver. However, there is the implicit assumption that the LP solver is deterministic. Since we are working in a fragment of a functional programming language, calling the LP solver twice on the same problem should lead to the same solution. In theory, a nondeterministic LP solver could be misused to lead to inconsistencies. As we do not compare LP solutions in our algorithm and SoPlex is actually deterministic, this problem does not impact our verified software. A more general solution to the problem could be the use of memoization or to model the nondeterminism with monads.\n\nExperimental Evaluation We show the practicality of our verified implementation by applying it to both the ring and star topologies from (Guestrin et al. 2003). Note that all numerical computations have to be performed with infinite precision, which substantially impacts the performance. We run our implementation on an Intel i7-11800H CPU and set the discount factor to 0.9 in all our experiments. In all runs, the weights converged after max. 5 iterations. The results of the experiments (Table 1) show that the algorithm can deal with ring networks of half a million states and 20 actions. For larger networks, the precise mode of the LP solver SoPlex cannot find a rational solution. The experiment shows that our implementation of linear programming certification can process linear programs with tens of thousands of constraints. For the simpler star topology, we can handle systems with $2 ^ { 4 0 }$ states in 45s.\n\n# Discussion\n\nOur work combines and integrates a wide range of tools and formalization efforts to produce a verified implementation of API. In Isabelle/HOL, we build on the formal libraries for LPs (Thiemann 2022), MDPs (Ho¨lzl 2017), linear algebra, analysis (Ho¨lzl, Immler, and Huffman 2013), probability theory (Ho¨lzl and Heller 2011), and the collections framework (Lammich and Lochbihler 2019). Furthermore, we certify the results computed by precise LP solvers. Our development consists of 20,000 lines of code, two-thirds focused on code generation. We show how to facilitate locales, powerful automation in Isabelle/HOL, and certification to develop complex formally verified software. We also make the case that the process of algorithm verification provides a detailed understanding of the algorithm: all hidden assumptions are made explicit, while we modularize the algorithm into components with precisely specified behaviour.\n\nThe methodology presented here can be applied to a large number of algorithms since the algorithm we verified is a seminal algorithm for solving factored MDPs, combining a large number of concepts that are widely used in many contexts, like AI (Delgado, Sanner, and De Barros 2011; Osband and Roy 2014; Deng, Devic, and Juba 2022) and model checking (Hinton et al. 2006; Dehnert et al. 2017).\n\nApplications of interactive theorem provers in verification and formalizing mathematics have been recently attracting a lot of attention (Avigad and Harrison 2014; Avigad 2023; Massot 2021). In most applications, especially in computer science (Esparza et al. 2013; Klein et al. 2009) and AI (Bagnall and Stewart 2019; Selsam, Liang, and Dill 2017), the emphasis is on the difficulty of the proofs, whether that is due to many cases or complex constructions, etc., and how theorem proving helped find mistakes or find missing cases in the proofs. A distinct feature of this work is that its complexity comes from the large number of concepts it combines, shown in Fig. 2, which is a more prevalent issue in formalizing pure mathematics. Our project has contributed a better restructuring of the algorithm and untangling of the different concepts, leading to better understandability.\n\nMultiple directions can be considered to extend and build on our work. An alternative to using exact LP solvers is to use their highly optimized floating point counterparts that do not calculate exact solutions. In this case, one needs to certify the error bound derived from a dual solution to the LP. Then the error bound has to be incorporated in the error analysis of the algorithm to obtain formal guarantees. Moreover, we may initialize the algorithm with weights computed by an unverified, floating point implementation of API to reduce number of iterations performed by the verified implementation. In Isabelle/HOL, some of the correctness proofs for code generation can be automated using tools to transfer theorems. Furthermore, the ergonomics for instantiating locales and inheriting from other locales could be improved for situations with many locale parameters.\n\nRelated Work Several formal treatments of MDPs have been developed in the theorem provers Isabelle/HOL (Ho¨lzl 2017; Scha¨ffeler and Abdulaziz 2023; Chevallier and Fleuriot 2021; Hartmanns, Kohlen, and Lammich 2023) and Coq (Vajjha et al. 2021). All developments verify algorithms for explicit MDPs, which limits their practical applicability to solve large MDPs. We adapt and integrate the implementation of Scha¨ffeler and Abdulaziz with our formalization. The algorithm we verified in Isabelle/HOL was first presented in (Guestrin et al. 2003). An LP certification approach with Isabelle/HOL was previously done in the Flyspeck project (Obua and Nipkow 2009), where feasibility was checked. The work introduces a method using dual solutions from floating-point LP solvers to bound the objective value. The tool Marabou for neural network verfication uses Farkas vectors to produce proofs for its results (Isac et al. 2022).",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了在大型因式马尔可夫决策过程（Factored MDPs）中，如何通过形式化验证保证算法输出的正确性和安全性的问题。\\n> *   该问题在安全关键系统（如自动驾驶、医疗决策）中尤为重要，现有方法通过测试和软件工程方法提供部分保证，但数学证明算法正确性是最佳保证。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出一种基于交互式定理证明（ITP）的方法，在Isabelle/HOL中形式化验证了近似策略迭代（API）算法，并生成可执行的验证实现。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **首个形式化验证的因式MDP求解算法**：填补了该领域的空白。\\n> *   **模块化验证架构**：利用Isabelle的locale系统将算法分解为可验证组件，并开发了可复用的数学库（如线性规划验证工具）。\\n> *   **实际可行性证明**：在基准问题上展示了算法的实用性，可处理50万状态的MDP。\\n> *   **验证完备性**：所有结果均通过Isabelle/HOL的数学证明，而传统方法仅能提供统计置信度。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   通过结构化分解和数学证明确保算法的每一步（如策略评估、策略改进）都满足严格的正确性条件。设计哲学是“验证即实现”，即形式化验证的算法可直接生成可执行代码。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比**：传统方法依赖测试和启发式验证，无法提供数学保证；现有形式化验证仅支持显式MDP，无法处理因式表示。\\n> *   **本文的改进**：引入作用域函数（scoped functions）和决策列表策略，避免枚举全部状态空间；通过线性规划（LP）认证结合验证与未验证的求解器，平衡效率与正确性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  使用Isabelle/HOL定义分解MDP的状态空间、作用域函数和转移概率。\\n> 2.  实现近似策略迭代（API）算法，包括策略评估、策略改进和Bellman误差计算。\\n> 3.  通过变量消除算法（Variable Elimination）高效计算Bellman误差。\\n> 4.  使用LP求解器进行策略评估，并通过验证过的证书检查器确保LP解的正确性。\\n> 5.  通过代码生成将验证过的算法导出为可执行的Scala程序。\\n\\n> **案例解析 (Case Study)**\\n> *   论文以环形拓扑的计算机网络为例，展示了分解MDP的状态表示和转移概率的计算。每个机器的状态由变量表示，状态转移依赖于其自身和前驱机器的状态。\\n> *   具体公式：$Q_w^a(x) := R^a(x) + \\\\gamma \\\\sum_{i<m} w_i g_i^a(x)$，用于高效计算Q函数。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   论文未明确提供基线模型对比，但通过实验验证了验证过的API算法在环形和星形拓扑网络上的实用性。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在运行时间上：** 本文方法在环形拓扑网络中处理19台机器（约79,978个状态）时，总运行时间为92.19秒，其中LP求解耗时30.25秒。\\n> *   **在可扩展性上：** 星型拓扑（状态空间$2^{40}$）仅需45秒处理，证明算法对指数级状态空间的适应性。\\n> *   **在验证完备性上：** 所有结果均通过Isabelle/HOL的数学证明，而传统方法仅能提供统计置信度。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   形式化验证 (Formal Verification, N/A)\\n*   因式马尔可夫决策过程 (Factored Markov Decision Process, FMDP)\\n*   近似策略迭代 (Approximate Policy Iteration, API)\\n*   交互式定理证明 (Interactive Theorem Proving, ITP)\\n*   线性规划 (Linear Programming, LP)\\n*   决策列表 (Decision List, N/A)\\n*   作用域函数 (Scoped Function, N/A)\\n*   安全关键系统 (Safety-Critical Systems, N/A)\"\n}\n```"
}