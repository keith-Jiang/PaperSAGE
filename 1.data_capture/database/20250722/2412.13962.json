{
    "link": "https://arxiv.org/abs/2412.13962",
    "pdf_link": "https://arxiv.org/pdf/2412.13962",
    "title": "Threshold UCT: Cost-Constrained Monte Carlo Tree Search with Pareto Curves",
    "authors": [
        "Martin Kurecka",
        "Václav Nevyhostený",
        "Petr Novotn'y",
        "Vít Uncovský"
    ],
    "publication_date": "2024-12-18",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "paper_content": "# Threshold UCT: Cost-Constrained Monte Carlo Tree Search with Pareto Curves\n\nMartin Kureˇcka1, V´aclav Nevyhosˇteˇn´y1, Petr Novotn´y1, V´ıt Uncˇovsky´\n\n1Faculty of Informatics, Masaryk University Botanicka´ 68a, 612 00 Brno, Czech Republic petr.novotny $@$ fi.muni.cz\n\n# Abstract\n\nConstrained Markov decision processes (CMDPs), in which the agent optimizes expected payoffs while keeping the expected cost below a given threshold, are the leading framework for safe sequential decision making under stochastic uncertainty. Among algorithms for planning and learning in CMDPs, methods based on Monte Carlo tree search (MCTS) have particular importance due to their efficiency and extendibility to more complex frameworks (such as partially observable settings and games). However, current MCTSbased methods for CMDPs either struggle with finding safe (i.e., constraint-satisfying) policies, or are too conservative and do not find valuable policies. We introduce Threshold UCT (T-UCT), an online MCTS-based algorithm for CMDP planning. Unlike previous MCTS-based CMDP planners, T-UCT explicitly estimates Pareto curves of cost-utility trade-offs throughout the search tree, using these together with a novel action selection and threshold update rules to seek safe and valuable policies. Our experiments demonstrate that our approach significantly outperforms state-of-the-art methods from the literature.\n\n# 1 Introduction\n\nSafe Decision Making and MCTS Monte-Carlo tree search (MCTS) has emerged as the de-facto method for solving large sequential decision making problems under uncertainty (Browne et al. 2012). It combines the scalability of sampling-based methods with the robustness of heuristic tree search, the latter feature making it easily extendable to settings with partial observability (Silver and Veness 2010), multiple agents (Silver et al. 2018), or settings with historydependent optimal decisions (Chatterjee et al. 2018). While MCTS-based methods demonstrated remarkable efficiency in optimizing the agent’s performance across diverse domains, the deployment of autonomous agents in real-world domains necessitates balancing the agent performance with the safety of their behavior. In AI planning and reinforcement learning, the standard way of modeling safety issues is via the constrained decision-making framework. Here, apart from the usual reward signals, the agents are also collecting penalties (or costs), and the objective is to maximize the expected accumulated reward under the constraint that the expected accumulated cost is below a given threshold $\\Delta$ . Compared with ad-hoc reward shaping, the constrained approach provides explicit and domain-independent way of controlling agent safety. Hence, safe and efficient probabilistic planning (and indeed, also model-free reinforcement learning, where algorithms such as MuZero (Schrittwieser et al. 2020) are built on top of efficient MCTS planners) necessitates the development of stable and sample-efficient cost-constrained MCTS algorithms.\n\nKey Components of Constrained MCTS An efficient constrained MCTS-based algorithm must be able to identify safe and valuable policies.\n\nFinding safe policies (i.e., those that do not exceed the cost threshold) requires identifying “dangerous” (in terms of future cost) decisions and keeping track of cost risk accumulated in stochastic decisions: a $5 0 / 5 0$ gamble which incurs cost $C$ if lost contributes at least $C / 2$ towards the expected cost of the policy irrespective of the gamble’s outcome.\n\nAn agent which never moves might be safe but never does anything useful. To identify reward-valuable policies among the safe ones, the algorithm must not be constrained beyond the requirements given by the threshold $\\Delta$ and hence it must be able to reason about the trade-off between rewards and costs during both tree search and actual action selection.\n\nLimitations of previous approaches Two prominent examples of constrained MCTS-based algorithms are CCPOMCP (Lee et al. 2018) and RAMCP (Chatterjee et al. 2018). While these algorithms represented significant steps towards practical constrained decision making, they exhibit fundamental limitations in identifying both safe and valuable policies.\n\nSafety limitations: A usual way of tracking the cost risk is updating the current threshold $\\Delta$ appropriately after each decision. As we discuss in Section 3, both CC-POMCP and RAMCP perform this update in an unsound manner and might thus produce policies violating the cost constraint even if a safe policy exists within the explored part of the tree.\n\nValue limitations: Both CC-POMCP and RAMCP compute randomized policies, which are necessary for optimality in constrained decision-making scenarios (Altman 1999). However, their reasoning about the reward-cost payoff is incomplete. RAMCP does not use the cost information during the tree search phase (which mimics the standard UCT (Kocsis and Szepesva´ri 2006)) at all: costs are only considered in the actual action selection phase, when a linear program (LP) encoding the constraints is solved over the constructed tree. Although the LP drives the agent to satisfy the constraints, the data used to construct the LP are sampled using a cost-agnostic search procedure which might lead to sub-optimal decisions. CC-POMCP, on the other hand, is a Lagrangian dual method in which the Lagrangian multiplier $\\lambda$ represents a concrete reward-cost tradeoff to be used in both tree search and action selection. The key limitation of the Lagrangian approach is the scalar nature of the tradeoff estimate $\\lambda$ : unless $\\lambda$ quickly converges to the optimal tradeoff, the algorithm will collect data according to either overly conservative or overly risky policies, yielding instability that hampers convergence to a valuable policy. This behavior was witnessed in multiple of our experiments (Section 4).\n\nOur contributions We introduce Threshold UCT (T-UCT), an online MCTS-based constrained decisionmaking algorithm designed so as to seek policies that are both safe and valuable. T-UCT achieves this by estimating the Pareto curves of the cost-payoff tradeoff in an online fashion. In every step, T-UCT uses these Pareto estimates to play a randomized action mixture optimal w.r.t. the current cost threshold. This is done both during the actual agent’s action selection and during tree search. The latter phase resolves the exploration/exploitation tradeoff through a variant of the UCT (upper confidence bound on trees (Kocsis and Szepesva´ri 2006)) approach, adapted to the constrained setting. In particular, T-UCT’s exploration is cost-sensitive, and the algorithm comes with a new threshold update rule, which ensures that the agent is not incorrectly driven into excessive risk. We evaluate T-UCT on benchmarks from the literature, including a model of an autonomous vehicle navigating the streets of Manhattan. Our experiments show that T-UCT significantly outperforms state-of-the-art algorithms, demonstrating notable sample efficiency and stable results across different environments.\n\nFurther related work The problem of constrained decision making under uncertainty, formalized via the notion of constrained Markov decision processes (Altman 1999) has received lot of attention in recent years, with approaches based on linear programming (Altman 1999; Poupart et al. 2015; Lee et al. 2017), heuristic search (Undurti and How 2010; Santana, Thie´baux, and Williams 2016), primal-dual optimization (Chow et al. 2017; Tessler, Mankowitz, and Mannor 2019), local policy search (Achiam et al. 2017), backward value functions (Satija, Amortila, and Pineau 2020), or Lyapunov functions (Chow et al. 2018). Unlike these works, our paper focuses on the MCTS-based approach to the problem, due to its scalability and extendibility to more complex domains.\n\nThe RAMCP algorithm has been extended into an AlphaZero-like MCTS-based learning algorithm RAlph in (Bra´zdil et al. 2020). In this paper, we do not consider function approximators and instead focus on the correctness and efficiency of the underlying tree search method.\n\nThe recent learning algorithm ConstrainedZero (Moss et al. 2024) computes deterministic policies, considers chance constraints, and does not track the cost risk of stochastic decision, thus solving a problem different from ours.\n\nOur work is also related to multi-objective (MO) planning (Barrett and Narayanan 2008; Moffaert and Nowe´ 2014). There, the task is to estimate tradeoffs among multiple payoff functions w.r.t. various solution concepts (including Pareto optimality). T-UCTs approach of performing full Bellman updates of Pareto curves during backpropagation is similar in spirit to convex hull MCTS (Painter, Lacerda, and Hawes 2020). However, MO approaches do not consider constrained optimization and thresholds; the main novelty of T-UCT is using the Pareto curves to guide the tree search towards valuable constraint-satisfying parts via thresholdbased action selection and sound threshold updates.\n\nConstrained decision making is also related to risk-sensitive planning and learning (e.g., (Chow and Ghavamzadeh 2014; Chow et al. 2015; L.A. and $\\mathtt { F u } 2 0 2 2$ ; Hou, Yeoh, and Varakantham 2016; Ayton and Williams 2018; Kˇret´ınsky´ and Meggendorfer 2018)), where safety is enforced by putting a constraint on some risk-measure of the underlying policy. Some risk measures, such as chance constraints, can be expressed in our framework by encoding accumulated payoffs into states.\n\n# 2 Preliminaries\n\nWe denote by ${ \\mathcal { D } } ( X )$ the set of all probability distributions over a finite support $X$ . We formalize the constrained decision making problem via the standard notion of constrained Markov decision processes (CMDPs).\n\nDefinition 1. A constrained Markov decision process (CMDP) is a tuple $\\mathcal { C } = ( \\mathcal { S } , \\mathcal { A } , \\delta , r , c , s _ { 0 } )$ where:\n\n• $s$ is a finite set of states,   \n• $\\mathcal { A }$ is a finite set of actions,   \n• $\\delta \\colon S \\times { \\mathcal { A } } \\to { \\mathcal { D } } ( S )$ is a probabilistic transition function; we abbreviate $\\delta ( s , a ) ( t )$ to $\\delta ( t \\mid s , a )$ ,   \n• $r \\colon S \\times \\mathcal { A } \\times \\mathcal { S }  \\mathbb { R }$ is a reward function,   \n• $c \\colon S \\times { \\mathcal { A } } \\times S \\to \\mathbb { R }$ is a cost function, and   \n• $s _ { 0 } \\in \\mathcal { S }$ is the initial state\n\nCMDP dynamics CMDPs evolve identically to standard MDPs. A history is an element of $( S A ) ^ { * } S$ , i.e., a finite alternating sequence of states and actions starting and ending in a state. A policy is a function assigning to each history a distribution over actions.\n\nUnder a given policy $\\pi$ , a CMDP evolves as follows: we start in the initial state; i.e., the initial history is $h _ { 0 } = s _ { 0 }$ . Then, for every timestep $i \\in \\{ 0 , 1 , 2 , . . . \\}$ , given the current history $h _ { i } = s _ { 0 } a _ { 0 } s _ { 1 } a _ { 1 } \\ldots s _ { i - 1 } a _ { i - 1 } s _ { i }$ , the next action $a _ { i }$ is sampled from $\\pi \\colon a _ { i } \\sim \\pi ( h _ { i } )$ . The next state $s _ { i + 1 }$ is sampled according to the transition function, i.e, $s _ { i + 1 } \\sim \\delta ( s _ { i } , a _ { i } )$ . Then, the agent obtains the reward $r ( s _ { i } , a _ { i } , s _ { i + 1 } )$ and incurs the cost $c ( s _ { i } , a _ { i } , s _ { i + 1 } )$ . The current history is updated to $h _ { i + 1 } = s _ { 0 } a _ { 0 } s _ { 1 } a _ { 1 } \\ldots s _ { i - 1 } a _ { i - 1 } s _ { i } a _ { i } s _ { i + 1 }$ and the process continues in the same fashion ad infinitum.\n\nWe denote by $\\mathbb { P } ^ { \\pi } ( E )$ the probability of an event $E$ under policy $\\pi$ , and by $\\mathbb { E } ^ { \\pi } [ X ]$ the expected value of a random variable $X$ under $\\pi$ . We denote by $| h |$ the length of history $h$ , putting $| s _ { 0 } a _ { 0 } \\ldots s _ { i - 1 } a _ { i - 1 } s _ { i } | = i$ .\n\nWe will sometimes abuse notation and use a history in a context where a state is expected - in such a case, the notation refers to the last state of a history. E.g. $\\delta ( - \\mid h , a )$ denotes a transition probability distribution from the last state of $h$ under action $a$ .\n\nProblem statement Under a fixed policy $\\pi$ , the agent accumulates (with possible discounting) both the rewards and costs over a finite decision horizon $T$ :\n\n$$\n\\begin{array} { c } { { { \\displaystyle { \\cal P } a y o f f _ { \\pi } = { \\mathbb E } ^ { \\pi } [ \\displaystyle { \\sum _ { i = 0 } ^ { T - 1 } \\gamma _ { r } ^ { i } \\cdot r ( s _ { i } , a _ { i } , s _ { i + 1 } ) } ] , } } } \\\\ { { { { } } } } \\\\ { { { \\displaystyle C o s t _ { \\pi } = { \\mathbb E } ^ { \\pi } [ \\displaystyle { \\sum _ { i = 0 } ^ { T - 1 } \\gamma _ { c } ^ { i } \\cdot c ( s _ { i } , a _ { i } , s _ { i + 1 } ) } ] , } } } \\end{array}\n$$\n\nwhere $\\gamma _ { r } , \\gamma _ { c } \\in ( 0 , 1 ]$ are reward and cost discount factors.\n\nOur goal is to maximize the accumulated payoff while keeping the accumulated cost below a given threshold. Formally, given a CMDP, the horizon $T \\in \\mathbb { N }$ , discount factors $\\gamma _ { r } , \\gamma _ { c } \\in ( 0 , 1 ]$ , and a cost threshold $\\Delta \\in \\mathbb { R } _ { \\geq 0 }$ , our task is to solve the following constrained optimization problem:\n\n$$\n\\begin{array} { c } { { \\displaystyle \\operatorname* { m a x } _ { \\pi } P a y o f f _ { \\pi } } } \\\\ { { \\mathrm { s u b j e c t ~ t o } C o s t _ { \\pi } \\leq \\Delta . } } \\end{array}\n$$\n\nOur algorithm tackles the above problem in an online fashion, producing a local approximation of the optimal constrained policy.\n\n# 3 Threshold UCT\n\nWe propose a new algorithm for CMDP planning, Threshold UCT (T-UCT). Like many other MCTS-based algorithms, TUCT only requires access to a generative simulator of the underlying CMDP, i.e., an algorithm allowing for an efficient sampling from $\\delta ( - | s , a )$ , given $( s , a )$ ; and providing $r ( s , a , s ^ { \\prime } )$ and $c ( s , a , s ^ { \\prime } )$ for given $( s , a , s ^ { \\prime } )$ .\n\nHistory-action values, feasibility We consider payoffs achievable by a policy $\\pi$ after witnessing a history $h$ and possibly also playing action $a$ :\n\n$$\n\\begin{array} { r l } & { \\displaystyle P a y o f f _ { \\pi } ( h ) = \\mathbb { E } ^ { \\pi } [ \\sum _ { i = | h | } ^ { T - 1 } \\gamma _ { r } ^ { i - | h | } \\cdot r ( s _ { i } , a _ { i } , s _ { i + 1 } ) \\mid h ] , } \\\\ & { \\displaystyle P a y o f f _ { \\pi } ( h , a ) = \\mathbb { E } ^ { \\pi } [ \\sum _ { i = | h | } ^ { T - 1 } \\gamma _ { r } ^ { i - | h | } \\cdot r ( s _ { i } , a _ { i } , s _ { i + 1 } ) \\mid h , a ] , } \\end{array}\n$$\n\nwhere $( \\cdot | h )$ is a condition of producing history $h$ in the first $| h |$ steps and $( \\cdot | h , a )$ is a condition that $a$ is played immediately after witnessing $h$ . The quantities $\\bar { C o s t _ { \\pi } ( h ) }$ and $C o s t _ { \\pi } ( h , a )$ are defined analogously. We say that $\\pi$ is $\\Delta$ - feasible from $h$ if $C o s t _ { \\pi } ( h ) \\le \\mathrm { \\bar { \\Delta } }$ .\n\nAchievable vectors A vector $( c , r ) \\in \\mathbb { R } \\times \\mathbb { R }$ is achievable from history $h$ if there exists a policy $\\pi$ such that $C o s t _ { \\pi } ( h ) \\ \\leq \\ c$ and Payoff $\\mathbf { \\Omega } _ { \\pi } ( h ) \\geq \\mathbf { \\Omega } ^ { r }$ . Similarly, we say that $( c , r )$ is achievable from $( h , a )$ if $C o s t _ { \\pi } ( h , a ) \\leq c$ and Payoff $_ \\pi ( h , a ) \\geq r$ for some $\\pi$ .\n\nWe write $\\left( c ^ { \\prime } , r ^ { \\prime } \\right) \\ \\preceq \\ \\left( c , r \\right)$ if $c ^ { \\prime } \\geq c$ and $\\boldsymbol { r ^ { \\prime } } \\le r$ . A $\\preceq$ -closure of a set $X \\subseteq \\mathbb { R } \\times \\mathbb { R }$ is the set of all vectors $( c ^ { \\prime } , r ^ { \\prime } ) \\in \\mathbb { R } \\times \\mathbb { R }$ s.t. $\\left( c ^ { \\prime } , r ^ { \\prime } \\right) \\preceq \\left( c , r \\right)$ for some $( c , r ) \\in X$ .\n\nPareto sets A Pareto set of history $h$ is the set of all vectors achievable from $h$ , while the Pareto set of $( h , a )$ is the set of all vectors achievable from $( h , a )$ .\n\nIt is known (Chatterjee, Forejt, and Wojtczak 2013; Barrett and Narayanan 2008) that the Pareto sets are (i) convex (since we allow randomized policies), and (ii) $\\preceq$ -closed (i.e., if $( c , r )$ belongs to the set and $( c ^ { \\prime } , r ^ { \\prime } ) \\preceq ( c , r )$ , then $( c ^ { \\prime } , r ^ { \\prime } )$ also belongs to the set). From (ii) it follows that a Pareto set is wholly determined by its Pareto curve, i.e. the set of all points maximal w.r.t. the $\\preceq$ -ordering. Furthermore, in finite MDPs, the Pareto curve is piecewise linear, with finitely many pieces. The whole Pareto set can then be represented by a finite set of vertices, i.e., points in which the piecewiselinear curve changes its slope; indeed, the Pareto set is the $\\preceq$ -closure of the convex hull of the set of vertices. In what follows, we denote by $P ( h )$ and $P ( h , a )$ these finite representations of the Pareto sets of $h$ and $( h , a )$ , respectively.\n\nBellman equations for Pareto sets Pareto sets in CMDPs obey local optimality equations akin to classical unconstrained MDPs. To formalize these, we need additional notation. The sum $X + Y$ is the standard Minkowski sum of the sets of vectors. For a vector $( a , b )$ we define $X \\cdot ( a , b ) =$ $\\{ ( x \\cdot a , y \\cdot b ) \\mid ( x , y ) \\in X \\}$ , with $X \\cdot a$ as a shorthand for $\\boldsymbol { X } \\cdot \\left( \\boldsymbol { a } , \\boldsymbol { a } \\right)$ . It is known (Barrett and Narayanan 2008; Chen et al. 2013) that for the finite-vertex representation of Pareto sets it holds:\n\n$$\n\\begin{array} { c } { { P ( h ) = p r u n e \\Big ( \\displaystyle \\bigcup _ { a \\in \\mathcal { A } } P ( h , a ) \\Big ) } } \\\\ { { P ( h , a ) = p r u n e \\Big ( \\displaystyle \\sum _ { t \\in \\mathcal { S } } \\delta ( t | h , a ) \\big ( P ( h a t ) \\cdot ( \\gamma _ { c } , \\gamma _ { r } ) } } \\\\ { { + \\left\\{ ( c ( h , a , t ) , r ( h , a , t ) ) \\right\\} \\big ) \\Big ) , } } \\end{array}\n$$\n\nwhere the prune operator removes all points that are $\\preceq$ -dominated by a convex combination of some other points in the respective set.\n\nT-UCT: Overall structure T-UCT is presented in Algorithm 1. It follows the standard Monte Carlo tree search (MCTS) framework, with blue lines highlighting parts that conceptually differ from the setting with unconstrained payoff optimization (the whole procedure GetActionDist is constraint-specific, and hence we omit its coloring). The algorithm iteratively builds a search tree whose nodes represent histories of the CMDP, with child nodes of $h$ representing one-step extensions of $h$ . Each node stores additional information, in particular the estimates of $P ( h )$ and $P ( h , a )$ , denoted by $\\mathcal { P } ( h )$ and $\\mathcal { P } ( h , a )$ in the pseudocode. The tree structure is global to the whole algorithm and not explicitly pictured in the pseudocode.\n\nThe algorithm uses transition probabilities $\\delta$ of the CMDP. If these are not available (e.g., when using a generative model of the CMDP), we replace $\\delta$ with a sample\n\n# Algorithm 1: Threshold UCT\n\n1 Procedure ThresholdUCT $( T , \\Delta )$\n\n2 h s0;   \n3 while $T > 0$ do   \n4 repeat   \n5 leaf GetLeaf $( h , \\Delta )$ ;   \n6 newleaf Expand(leaf);   \n7 Propagate (newleaf , h);   \n8 until timeout;   \n9 $\\sigma \\gets$ GetActionDist $( h , \\Delta , 0 )$ ;   \n10 $a \\sim \\sigma$ ; play $a$ and observe new state $t$ ;   \n11 $\\Delta \\gets \\mathtt { U p d a t e T h r } ( \\Delta , \\sigma , a , t )$ ;   \n12 $h \\gets h a t$ ; $T \\gets T - 1$ ;   \n13 Function GetLeaf $( h , \\tilde { \\Delta } )$   \n14 while $h$ is not a leaf do   \n15 $\\sigma \\gets$ GetActionDist $( h , \\tilde { \\Delta } , 1 )$ ;   \n16 a ∼ σ; $t \\sim { \\hat { \\delta } } ( - \\mid h , a )$ ;   \n17 ∆˜ ← UpdateThr(∆˜ , σ, a, t);   \n18 $h \\gets h a t$ ;   \n19 return $h$   \n20 Procedure Propagate(h, root)   \n21 $( c , r ) \\gets \\mathtt { R o l l o u t } ( \\Dot { h } )$ ;   \n22 $\\mathcal { P } ( h )  \\{ ( c , r ) , ( 0 , 0 ) \\}$ ;   \n23 while $h \\neq$ root do   \n24 write $h$ as $h ^ { \\prime } a s$ ; $h  h ^ { \\prime }$ ;   \n25 update $\\mathcal { P } ( h , a )$ via equation (2);   \n26 update $\\mathcal { P } ( h )$ via equation (1);\n\nestimate based on the visit count of transitions during the tree search. The estimates are updated globally with every sample from the simulator (omitted in the pseudocode). In what follows, we denote by $\\hat { \\delta }$ either the real transition probabilities or, if these are unavailable, their sample estimates.\n\nInitially, the tree contains a single node representing the history $h _ { 0 } = s _ { 0 }$ . In each decision step $1 , 2 , \\ldots , T$ , T-UCT iterates, from the current root $h$ , the standard four stages of MCTS until the expiry of a given timeout. The stages are: (i) search, where the tree is traversed from top to bottom using information from previous iterations to navigate towards the most promising leaf (function GetLeaf); (ii) leaf expansion, where a successor of the selected leaf is added to the search tree (line 6); followed by (iii) rollout: where the Pareto curve of the new leaf is estimated, e.g., via Monte Carlo simulation following some default policy (lines $2 1 -$ 22), or possibly via a pre-trained predictor. Note that we also add a tuple $( 0 , 0 )$ to the curve to make the exploration “costoptimistic”, even if the rollout policy is unable to find safe paths. Finally, T-UCT performs (iv) backpropagation, of the newly obtained information (particularly, the Pareto curve estimates) from the leaf back to the root (the rest of procedure Propagate). We provide more details on the individual stages below.\n\nAfter this, T-UCT computes and outputs an action distribution $\\sigma$ from which action $a$ to be played is sampled (lines 9–10). The action is performed, and a new state $t$ of the environment (10) and the immediate reward and cost (omitted from pseudocode) are incurred. The cost threshold $\\Delta$ is then updated (line 11, details below) to account for the cost incurred, discounting, and cost risk of the stochastic decision. The node hat becomes the new root of the tree and the process is repeated until a decision horizon is reached.\n\nThe key components distinguishing T-UCT from standard UCT are the backpropagation, action selection, and threshold update. In the following, we present these components in more detail.\n\n27 Fu 28 $\\begin{array} { r l } & { \\underset { \\ b { \\widetilde { \\mathcal { P } } }  } { \\mathrm { n c t i o n ~ f e e t a c t ~ i o n D i s t } } ( h , \\Delta , e ) } \\\\ & { \\quad \\overset { \\widetilde { \\mathcal { P } } } { \\underset { \\ b { \\mathcal { P } }  } { \\longrightarrow } } } \\\\ & { \\quad p r u n e \\big ( \\bigcup _ { a ^ { \\prime } \\in \\mathcal { A } } \\mathcal { P } ( h , a ^ { \\prime } ) + \\{ e x p l ( h , a ^ { \\prime } ) \\cdot ( - e , e ) \\} \\big ) ; } \\\\ & { \\quad \\widetilde { C } ^ { - }  \\big \\{ c \\mid ( c , r ) \\in \\widetilde { \\mathcal { P } } \\land c \\le \\Delta \\} ; } \\\\ & { \\quad \\widetilde { C } ^ { + }  \\big \\{ c \\mid ( c , r ) \\in \\widetilde { \\mathcal { P } } \\land c \\ge \\Delta \\} ; } \\end{array}$   \n29  \n30  \n31 ife $\\widetilde { C } ^ { - } = \\varnothing$ then  \n32 $\\begin{array} { r } { a \\gets \\arg \\operatorname* { m i n } _ { a ^ { \\prime } } \\operatorname* { m i n } \\{ c \\mid ( c , r ) \\in \\widetilde { \\mathcal { P } } ( h , a ^ { \\prime } ) \\} ; } \\end{array}$   \n33 return det distr(a);  \n34 else if $\\widetilde C ^ { + } = \\varnothing$ then  \n35 $\\begin{array} { r } { a \\gets \\arg \\operatorname* { m a x } _ { a ^ { \\prime } } \\operatorname* { m a x } \\{ r \\mid ( c , r ) \\in \\widetilde { \\mathcal { P } } ( h , a ^ { \\prime } ) \\} ; } \\end{array}$   \n36 return det distr $( a )$ ;  \n37 else  \n38 cl max C−;  \n39 $a _ { l } \\gets$ actionerealising $c _ { l }$ ;  \n40 c min C+;  \n41 $a _ { h } \\gets$ actione realising $c _ { h }$ ;  \n42 $\\begin{array} { r } { \\sigma _ { h } \\gets \\frac { \\Delta - c _ { l } } { c _ { h } - c _ { l } } } \\end{array}$ ∆−cl ; σl ← 1 − σh;  \n43 return mix distr(σl, al, σh, ah);\n\nBackpropagation T-UCT’s backpropagation is similar to convex-hull MCTS (Painter, Lacerda, and Hawes 2020): performing full Bellman updates of the Pareto set estimates according to equations (1) and (2) (using estimates $\\hat { \\delta }$ in lieu of $\\delta$ when necessary).\n\nAction selection The function GetActionDist computes an action distribution based on the current estimates of Pareto curves. In the search phase of the algorithm (where $e$ is set to 1), we encourage playing under-explored actions with an exploration bonus as depicted on line 28:\n\n$$\ne x p l ( h , a ) = C \\cdot \\alpha ( h ) \\cdot \\sqrt { \\frac { \\log N ( h ) } { N ( h , a ) + 1 } } ,\n$$\n\nwhere $N ( h )$ and $N ( h , a )$ are the visit counts of node $h$ and action $a$ in $h$ , respectively, $C$ is a fixed static exploration constant, and $\\alpha ( h )$ is a dynamic adjustment of the exploration constant equal to the difference between the maximum and minimum achievable value (payoff or cost) in $h$ . Note that for each $a \\in { \\mathcal { A } }$ , the bonus is applied to all vertices in $\\mathcal { P } ( h , a )$ ; thereafter the exploration-augmented estimate $\\tilde { \\mathcal P }$ of $P ( h )$ is computed via (1). When playing the actual action, the exploration bonus is disabled by setting $e = 0$ .\n\nIf there is no feasible policy based on $\\tilde { \\mathcal P }$ (i.e., policy satisfying $C o s t _ { \\pi } ( h ) \\le \\Delta )$ , we return a dis iebution which deterministically selects the action minimizing the future expected cost (lines 31–33). Conversely, if the expected future cost can be kept below $\\Delta$ no matter which action is played, we deterministically select the action with the highest future $\\Delta$ -constrained payoff (lines 34–36).\n\nOtherwise, we find vertices $\\boldsymbol { v _ { l } } ~ = ~ \\left( c _ { l } , r _ { l } \\right)$ and $\\begin{array} { r l } { { v _ { h } } } & { { } = } \\end{array}$ $( c _ { h } , r _ { h } )$ of $\\tilde { \\mathcal P }$ whose future cost estimates are the nearest to $\\Delta$ from elow and above, respectively. Due to pruning, necessarily $r _ { l } \\le r _ { h }$ . We identify actions $a _ { l }$ and $a _ { h }$ that realize the cost-reward tradeoff represented by these vertices, and mix them in such a proportion that the resulting convex combination of $c _ { l }$ and $c _ { h }$ equals $\\Delta$ , so that the “cost budget” is maxed out (lines 37–43). The resulting mixture is returned to the agent or the tree search procedure.\n\nThreshold update After an action $a$ is played and its outcome $t$ observed, the cost threshold $\\Delta$ must be updated to account for (a) the immediate cost incurred, (b) the discount factor $\\gamma _ { c }$ , and (c) the predicted contribution of outcomes other than $t$ to the overall expected cost achieved by the policy from $h$ . The update is performed by the UpdateThr function, described below.\n\nIf the transition hat has not been expanded yet, the update involves only the subtraction of the immediate cost and the division by $\\gamma _ { c }$ . Otherwise UpdateThr $( \\Delta , \\sigma , a , t )$ first computes the intermediate threshold $\\Delta _ { a c t }$ : If $\\sigma$ is deterministic, we set $\\Delta _ { a c t }$ to $\\Delta$ ; if $\\sigma$ is a stochastic mix of two different actions, we check if the action $a$ sampled from $\\sigma$ is the $a _ { l }$ or $a _ { h }$ from line 43; accordingly, we set $\\Delta _ { a c t }$ to either $c _ { l }$ or $c _ { h }$ .\n\nBased on $\\Delta _ { a c t }$ , the value of $\\Delta$ is updated to a new value $\\Delta ^ { \\prime }$ in a way depending on “how much” $\\Delta _ { a c t }$ is feasible from $h a$ . In the following, we use conv to denote the convex hull operator. There are three cases to consider, similar to those in GetActionDist:\n\nCase “mixing”: In the first case, there exists a maximal ${ \\boldsymbol { \\rho } } \\in \\mathbb { R }$ such that $( \\Delta _ { a c t } , \\rho ) \\in c o n v ( \\mathcal { P } ( h , a ) )$ ; i.e., by (2) $\\rho$ is the maximum real satisfying\n\n$$\n\\begin{array} { r l } { { } } & { { \\displaystyle \\left( \\Delta _ { a c t } , \\rho \\right) = \\sum _ { s \\in \\cal S } \\hat { \\delta } ( s \\mid h , a ) \\cdot \\left[ ( c _ { s } \\cdot \\gamma _ { c } , r _ { s } \\cdot \\gamma _ { r } ) \\right. } } \\\\ { { } } & { { \\left. + \\left( c ( h , a , s ) , r ( h , a , s ) \\right) \\right] , } } \\end{array}\n$$\n\nfor some points $( c _ { s } , r _ { s } )$ such that $( c _ { s } , r _ { s } ) \\in c o n v ( \\mathcal { P } ( h a s ) )$ for each $s \\in \\mathcal S$ . UpdateThr $( \\Delta , \\sigma , a , t )$ then updates $\\Delta$ to\n\n$$\n\\Delta ^ { \\prime }  c _ { t } .\n$$\n\nThis ensures that no matter which $t$ is sampled, the overall expected cost in $h a$ is bounded by $\\Delta _ { a c t }$ , provided that the points in the Pareto curve estimates are achievable.\n\nCase “surplus”: The second case is when there is a surplus in the cost budget, i.e., $\\begin{array} { r l r } { \\Delta _ { a c t } } & { { } > } & { c _ { \\mathrm { m a x } } } \\end{array} =$ max $\\{ c \\mid ( c , r ) \\in { \\mathcal { P } } ( h , a ) \\}$ . A naive update would be to ignore the surplus and continue as if $\\Delta _ { a c t }$ was $c _ { \\mathrm { m a x } }$ . Per the Pareto curve estimates, there would be no decrease in payoff since indeed, under the estimated dynamic $\\hat { \\delta }$ , the optimal policy does not exceed the cost $c _ { \\mathrm { m a x } }$ . However, $\\hat { \\delta }$ can be incorrect and too cost-optimistic. Hence, ignoring the surplus $\\Delta _ { a c t } - c _ { \\operatorname* { m a x } }$ could over-restrict the search in future steps. Instead, we distribute the surplus over all possible outcomes of $a$ proportionally to the outcomes’ predicted cost. Formally, UpdateThr $( \\Delta , \\sigma , a , t )$ identifies a vertex $( c _ { \\operatorname* { m a x } } , \\rho ) \\in \\mathcal { P } ( h , a )$ and computes vectors $( c _ { s } , r _ { s } )$ satisfying (3) with left-hand side set to $( \\boldsymbol { c } _ { \\mathrm { m a x } } , \\rho )$ . Then, it computes\n\n$$\n\\Delta ^ { \\prime }  c _ { t } + ( \\Delta _ { a c t } - c _ { \\operatorname* { m a x } } ) \\frac { B - c _ { t } } { \\bar { c } ( h , a ) + \\gamma _ { c } B - c _ { \\operatorname* { m a x } } } ,\n$$\n\nwhere $\\begin{array} { r } { \\bar { c } ( h , a ) = \\sum _ { s } \\hat { \\delta } ( s  { \\mathrm { ~  ~ | ~ } } h , a ) \\cdot c ( h , a , s ) } \\end{array}$ is the expected immediate cost, anPd ${ B = T \\cdot \\operatorname* { m a x } _ { ( s , a , t ) } c ( s , a , t ) }$ is an upper bound on the accumulated cost of any trajectory.\n\nCase “unfeasible”: Finally, if $\\Delta _ { a c t }$ is unfeasible according to $\\mathcal { P } ( h , a )$ , i.e., when $\\begin{array} { r l r } { \\Delta _ { a c t } } & { { } < } & { c _ { \\mathrm { m i n } } } \\end{array} =$ $\\operatorname* { m i n } \\left\\{ c \\mid { \\bar { ( } } c , r ) \\in { \\dot { \\mathcal { P } } } ( h , a ) \\right\\}$ , $\\mathtt { U p d a t e T h r } ( \\Delta , \\sigma , a , t )$ distributes all the missing cost to the current outcome $t$ (unlike in the previous case). Formally, the update is the following:\n\n$$\n\\Delta ^ { \\prime }  c _ { t } - \\frac { c _ { \\operatorname* { m i n } } - \\Delta _ { a c t } } { \\hat { \\delta } ( t \\mid h , a ) \\gamma _ { c } } ,\n$$\n\nwhere the $c _ { s }$ -values are, again, computed by applying (3) to a vector $( c _ { \\operatorname* { m i n } } , \\rho ) \\in \\mathcal P ( h , \\bar { a } )$ .\n\nTheoretical analysis The threshold update function UpdateThr enjoys two notable properties that are important for the algorithm’s correctness: First a), the update never increases the estimated expected threshold, i.e.,\n\n$$\n\\Delta \\ge \\mathbb { E } \\left[ c ( h , a , t ) + \\gamma _ { c } \\cdot \\Delta ^ { \\prime } \\right] ^ { 1 } ,\n$$\n\nand b), the updated value is feasible according to the estimates, i.e.,\n\n$$\n\\Delta ^ { \\prime } \\geq c _ { t } ,\n$$\n\nwhenever $\\Delta$ is feasible according to the estimates.\n\nThe properties are important for two reasons. First, they ensure the asymptotical convergence of the algorithm, and second, they prevent the algorithm from an excessive increase of the threshold under limited exploration.\n\nConcerning the asymptotical convergence, we prove the $\\varepsilon$ -soundness of T-UCT in the sense formalized by the following theorem, proved in the extended version (Kurecˇka et al. 2024).\n\n# Theorem 1. Let be a CMDP and $\\Delta$ a threshold such that there exists a $\\Delta$ -feasible policy. Then for every $\\varepsilon \\in \\mathbb { R } ^ { + }$ , there exists n such that T-UCT with n MCTS iterations per action selection is $( \\Delta + \\varepsilon )$ -feasible.\n\nWe explore the importance of the properties (7) and (8) in further sections. Our experiments reveal that in certain cases, RAMCP violates the inequality (7). This leads to an excessive increase of the threshold and results in RAMCP ignoring the cost constraint. In the extended version (Kurecˇka et al. 2024), we further show that CC-POMCP can violate (8) by not taking the action outcomes into account during threshold updates, yielding a threshold-violating policy.\n\n![](images/473de1834cc1d99ef95697f4c3604a630c23ec3aa468d9041d7ee2ddf4cb46cb.jpg)\n\n![](images/05b540134ee4631d1987c85a0cb157c6763af93610af3846606a18360e1a33f8.jpg)\n\n(a) A Gridworld map with the initial tile (green), golds (yellow), and traps (red). The possible outcomes of moving right are depicted by blue arrows.\n\n(b) Manhattan map depicting the initial junction and the agent’s trajectory (black), maintenance points (red) and accepted requests (yellow).\n\nFigure 1: The Gridworld and Manhattan environments.\n\n# 4 Experiments\n\nBaseline Algorithms We compare T-UCT to two state-ofthe-art methods for solving CMDPs: CC-POMCP (Lee et al. 2018) and RAMCP (Chatterjee et al. 2018).\n\nCC-POMCP is a dual method based on solving the Lagrangian relaxation of the CMDP objective:\n\n$$\n\\operatorname* { m i n } _ { \\lambda \\geq 0 } \\operatorname* { m a x } _ { \\pi } P a y o f f _ { \\pi } + \\lambda \\cdot ( \\Delta - C o s t _ { \\pi } ) .\n$$\n\nCC-POMCP performs stochastic gradient descent on $\\lambda$ while continuously evaluating (9) via the standard UCT search. For a fixed $\\lambda$ , the maximization of (9) yields a point on the cost-payoff Pareto frontier, where a larger value of $\\lambda$ induces more cost-averse behavior. A caveat of the method is its sample inefficiency when $\\lambda$ converges slowly.\n\nRAMCP is a primal method combining MCTS with linear programming. The search phase of RAMCP greedily optimizes the payoff in a standard single-valued UCT fashion, completely ignoring the constraint. Consequently, the cost is considered only in the second part of the action selection phase, where the algorithm solves a linear program to find a valid probability flow through the sampled tree (which serves as a local approximation of the original CMDP) such that the expected cost under the probability flow satisfies the constraint and the payoff is maximized.\n\nBenchmarks We evaluated the algorithms on three tasks: two of them are variants of the established Gridworld benchmark (Bra´zdil et al. 2020; Undurti and How 2010); the third is based on the Manhattan environment (Blahoudek et al. 2020), where the agent navigates through mid-town Manhattan, avoiding traffic jams captured by a stochastic model.\n\nTask: Avoid The setup (see Figure 1a) involves navigating the agent (robot) through a grid-based maze with four permissible actions: moving left, right, down, or up. The agent’s movements are subject to stochastic perturbations: it can be displaced in a direction perpendicular to its intended movement with probability $p _ { s l i d e }$ . The agent’s objective is to visit special “gold” tiles to receive a reward while avoiding “trap” tiles, which, with probability $p _ { t r a p }$ , incur a cost of 1 and terminate the environment (the agent suffers fatal damage).\n\nThe task is evaluated on a set of 128 small maps $( 6 \\times 6$ grid, 5 gold, approx. $1 0 ^ { 3 }$ states) and 64 large maps $2 5 \\times 2 5$ grid, 50 gold, approx. $1 0 ^ { 1 7 }$ states). All maps are sampled from our map generator, which guarantees a varying distribution of traps, walls, and gold. We provide the generator and the resulting map datasets GridworldSmall and GridworldLarge in the the project repository.\n\nTask: SoftAvoid The setup is similar to Avoid (including the same generated maps) except that fixed amount of the cost $p _ { t r a p }$ is deterministically incurred upon stepping on a trap and the environment does not terminate in that case. Thus the goal is to collect as much gold as possible while keeping the number of visited trap tiles low.\n\nTask: Manhattan The setup involves navigation in the eponymous Manhattan environment implemented in (Blahoudek et al. 2020). The agent moves through mid-town Manhattan (250 junctions, 8 maintenance points, approx. $1 0 ^ { 2 1 }$ states) while the time required to pass each street is sampled from a distribution estimated from real-life traffic data. Periodically, selected points on the map request maintenance. If the request is accepted, the agent receives a reward of 1 when reaching the point of the request; however, if it does not deliver the order in the given time limit, it incurs a cost of 0.1.\n\nTable 1: Reward and cost summary. In all environments, the agent receives rewards deterministically either for collecting gold or finishing the order. In SoftAvoid and Manhattan, the agent incurs the cost deterministically on the trigger (stepping on a trap or not delivering the order in time), while in Avoid, the cost is incurred with probability ptrap.   \n\n<html><body><table><tr><td>Task</td><td>Reward r</td><td>Cost c</td></tr><tr><td>Avoid</td><td>1</td><td>1</td></tr><tr><td>SoftAvoid</td><td>1</td><td>Ptrap</td></tr><tr><td>Manhattan</td><td>1</td><td>0.1</td></tr></table></body></html>\n\nImplementation We implemented T-UCT, RAMCP, and CC-POMCP algorithms within a common $^ { C + + }$ based MCTS framework, so as to maximize the amount of shared code among the algorithms, reducing the influence of implementation details on the experiments. The Gridworld environments are implemented as part of our $^ { C + + }$ codebase, while the Manhattan environment is built on top of the Python implementation provided by (Blahoudek et al. 2021). The code is available at https://github.com/kurecka/rats/tree/ AAAI25.\n\nExperimental setup Each task was evaluated with various settings of parameters such as the risk threshold $\\Delta$ , map, or slide probability. We evaluated GridworldSmall tasks (both Avoid and Soft-Avoid) on 1144, GridworldLarge on 2304, and Manhattan on 600 configurations. The full description of configurations and of the hardware setup is in the extended version (Kurecˇka et al. 2024).\n\nWe performed 300 independent runs on each configuration. The algorithms were given a fixed time per decision summarized in Figure 2; note the time limit on Manhattan\n\nAvoid S SoftAvoid S Avoid L SoftAvoid L Manhattan 1.00 1.00 1.00 1.00 1.00 0.5705 0.75 0.75 0.75 0.75   \n0.25 0.50 0.50 0.50 0.50 0.25 0.25 0.25 0.25 Agent 0.00 0.00 0.00 0.00 0.00 5.0 10.0 25.0 5.0 10.0 25.0 10.0 25.0 50.0 10.0 25.0 50.0 100.0 200.0 500.0 T-UCT (ours) (W) Time limit (ms) Time limit (ms) Time limit (ms) Time limit (ms) Time limit (ms) CC-POMCP (W) RAMCP (W) 1.00 1.00 1.00 1.00 1.00 T-UCT (ours) (M)   \n0.75 0.75 0.75 0.75 0.75 CRAC-MPCOPM(CMP) (M)   \n0.2550 0.50 0.50 0.50 0.50 0.25 0.25 0.25 0.25 0.00 0.00 0.00 0.00 0.00 0 05 0.3 %& 0.0 05 0.0 公 0 & Cost threshold c Cost threshold c Cost threshold c Cost threshold c Cost threshold c\n\nis looser so as to compensate for the slower Python implementation of the environment. The runs on GridworldSmallbased tasks had $T \\ = \\ 1 0 0$ steps, while the runs on GridworldLarge and Manhattan maps had $T = 2 0 0$ steps.\n\nMetrics For each experiment configuration, we computed the mean collected payoff $\\hat { r }$ , mean collected cost $\\hat { c }$ , and its standard deviation. Based on these, we report two statistics detailed below: the fraction of satisfied instances $S A T$ and the mean payoff achieved on satisfied instances.\n\nSince the empirical cost $\\hat { c }$ suffers from stochastic inaccuracy, we define two levels of satisfaction. The mean satisfaction metric $S A T _ { M }$ is simply the fraction of instances where the empirical cost satisfies the constraint, i.e., $\\hat { c } \\le \\Delta$ . For the weak satisfaction $S A T _ { W }$ , we weaken the constraint, which allows us to give it a statistical significance; it is the fraction of instances where the t-test $\\mathrm { \\langle } \\alpha = 0 . 0 5 \\mathrm { \\rangle }$ rejects the hypothesis that the real expected cost of the algorithm is more than $\\Delta + 0 . 0 5$ . Since it is meaningless to compare the payoffs of the algorithms on instances where they violate the constraint, for each baseline algorithm, we further identify the instances where the algorithm and T-UCT both satisfy the constraint (in the weak sense) and compute the average payoff on these instances.\n\nResults The safety results are shown in Figure 2. The first row shows the overall safety on individual environments. Each sub-plot displays the fraction of instances satisfied under the given time limits. Both T-UCT and CC-POMCP consistently solve a substantial proportion of instances, irrespective of the environment, while RAMCP struggles to find feasible solutions, especially in the Manhattan environment.\n\nThe second row of Figure 2 contains a breakdown of solved instances across different thresholds. All algorithms have similar satisfaction ratios when no cost is permitted (threshold 0); However, RAMCP quickly deteriorates with the increasing threshold as it is not able to estimate the expected cost accurately (only its positivity). T-UCT and CCPOMCP keep the number of satisfied instances or even improve it with more risk permitted.\n\nFigure 3 compares the payoffs collected by T-UCT to other two algorithms, and only considers the instances satisfied by both of the compared algorithms. From the first row, it is clear that CC-POMCP is overly pessimistic in its actions, resulting in overconservative behaviour. On the other hand, due to its reward-oriented exploration, RAMCP often finds valuable strategies once it finds feasible ones. Nevertheless, T-UCT is still able to achieve similar or higher payoffs than RAMCP on all environments.\n\nAt the same time, T-UCT satisfied many more instances than RAMCP. The difference is especially notable on the large environments with non-zero cost thresholds where RAMCP satisfies at most half of the instances while T-UCT steadily achieves between 0.8 and 1.0 satisfaction rate. Regarding their performance in terms of payoffs, there is little statistically significant difference, although T-UCT achieved notably higher payoffs on large SoftAvoid instances.\n\nOn the small benchmarks, CC-POMCP both solved fewer instances and achieved lower payoffs than T-UCT. With larger environments, CC-POMCP’s safety performance is on par with T-UCT or better. However, as we can see from the payoff values, this is mainly caused by the fact that CCPOMCP played extremely conservatively. The difference is most prominent in the Manhattan environment, where we observe up to a tenfold gap in payoff between CC-POMCP and T-UCT despite CC-POMCP achieving superior $S A T$ values. This is caused by CC-POMCP refusing almost all of the maintenance requests, thus failing to find a valuable policy.\n\n![](images/0fa4e3ff9dd1ecd38ed77eee982d1573cfa1e6235c8b08a03c1ed05cc9daf239.jpg)  \nFigure 3: The mean payoff of T-UCT compared to each baseline. The average is calculated only over instances satisfied (in the weak sense) by both of the considered algorithms.\n\nNotably, T-UCT was the only algorithm capable of solving a substantial number of Manhattan instances (approx. $74 \\%$ $S A T _ { M }$ , $1 0 0 \\% \\ S A T _ { W } ,$ ) while achieving high payoffs. As all the algorithms performed at most approx. 500 MCTS samples per step (an order of magnitude less than in other benchmarks, see Table 2), this demonstrates T-UCT’s data efficiency.\n\nIn summary, while CC-POMCP manages to find safe trajectories, it does so at the expense of the collected rewards. On the other hand, RAMCP generally accumulates a large reward, although it frequently disregards the safety of its behaviour in the process.\n\nTable 2: Average number of simulations per step performed by each algorithm. The low number of simulations by TUCT is due to its higher computational cost during backpropagation. The drop in RAMCP’s simulations for larger environments is caused by its two-phase nature.   \n\n<html><body><table><tr><td colspan=\"2\"></td><td rowspan=\"2\">T-UCT (ours)</td><td rowspan=\"2\">CC-POMCP RAMCP</td></tr><tr><td>Environment</td><td>t</td></tr><tr><td>Avoid S</td><td>10</td><td>324</td><td>954 2580</td></tr><tr><td>SoftAvoid S</td><td>10</td><td>268</td><td>874 1920</td></tr><tr><td>Avoid L</td><td>50</td><td>1017</td><td>2634 1378</td></tr><tr><td>SoftAvoid L</td><td>50</td><td>1041</td><td>2587 1458</td></tr><tr><td>Manhattan</td><td>500</td><td>387</td><td>349 427</td></tr></table></body></html>\n\nDiscussion Per our observation, the suboptimal performance of CC-POMCP is caused by two factors: the unsound update rule, which occasionally sets $\\Delta$ to a value that is not achievable; and the slow convergence of the Lagrange multiplier $\\lambda$ . For the latter reason, the algorithm often achieves low payoffs, even if the computed policies are safe.\n\nRAMCP is often able to find relatively valuable strategies once it finds feasible ones, the latter task being its weak point. The problem of greedy exploration is pronounced in the SoftAvoid task, where rewards and costs are not directly correlated, and thus, the sampled tree does not provide a good approximation of the original CMDP. In Manhattan, where it is easy to underestimate the costs, the cost budget of RAMCP often explodes as described at the end of Section 3, which essentially leads to ignoring the constraint.\n\nIn summary, T-UCT strikes a good balance between both safety and payoff, a property not shared by either of the baseline algorithms. Although T-UCT performs significantly fewer simulations than the other algorithms, it provides stable performance across all environments. Using Pareto curves, T-UCT stores the collected information in a wellstructured manner, allowing it to effectively reason about the cost-reward trade-offs even during exploration and thus find safe and valuable policies.\n\n# 5 Conclusion and Future Work\n\nWe introduced Threshold UTC, a MCTS-based planner for constrained MDPs utilizing online estimates of reward-cost trade-offs in search for safe and valuable policies. We presented an experimental evaluation of our approach, showing that it outperforms competing cost-constrained tree-search algorithms. Our aim for future work is to augment T-UCT with function approximators of Pareto sets, thus obtaining a general MCTS-based reinforcement learning algorithm for cost-constrained optimization.",
    "institutions": [
        "Faculty of Informatics",
        "Masaryk University"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决受限马尔可夫决策过程（CMDPs）中，基于蒙特卡罗树搜索（MCTS）的算法难以找到既安全又有价值策略的问题。在安全的顺序决策制定中，CMDPs要求智能体在将预期成本控制在给定阈值以下的同时优化预期回报。而当前基于MCTS的CMDP算法要么难以找到安全策略，要么过于保守而无法找到有价值的策略，因此开发稳定且样本高效的成本受限MCTS算法至关重要。\\n\\n**方法概述**\\n论文提出了阈值UCT（T - UCT）算法，这是一种基于在线MCTS的CMDP规划算法，通过在搜索树中显式估计成本 - 效用权衡的帕累托曲线，并结合新颖的动作选择和阈值更新规则来寻找安全且有价值的策略。\\n\\n**主要贡献与效果**\\n- 提出T - UCT算法，在多个基准测试中显著优于现有算法。在曼哈顿环境中，T - UCT实现了约74%的$SAT_M$和100%的$SAT_W$，而RAMCP在该环境中难以找到可行解决方案。\\n- 该算法具有良好的样本效率，在执行较少模拟次数的情况下仍能在各环境中提供稳定性能。如在Avoid S任务中，T - UCT每步平均模拟324次，而CC - POMCP为954次，RAMCP为2580次。\\n- 能够有效地平衡安全性和收益，在所有环境中都能找到安全且有价值的策略。在大的SoftAvoid实例中，T - UCT的回报明显高于RAMCP；在曼哈顿环境中，T - UCT的回报比CC - POMCP高出近十倍。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nT - UCT的核心思想是利用帕累托曲线在线估计奖励 - 成本权衡，通过在搜索树中显式表示成本和回报的关系，结合贝尔曼方程更新帕累托集估计，在树搜索和动作选择过程中考虑成本和回报的权衡，以找到安全且有价值的策略。同时，通过合理的阈值更新规则，确保算法在执行过程中不会过度冒险。\\n\\n**创新点**\\n先前的基于MCTS的CMDP规划方法存在局限性。CC - POMCP的样本效率低，拉格朗日乘子$\\lambda$收敛慢，并且更新规则不合理；RAMCP在搜索阶段完全忽略约束，成本估计不准确。与这些方法相比，T - UCT的创新点在于：\\n- 明确估计搜索树中成本 - 效用权衡的帕累托曲线，以此为基础进行动作选择和阈值更新，而不是仅考虑单一的成本或回报指标，从而更全面地表示策略的可行性和价值。\\n- 提出了新颖的动作选择和阈值更新规则。动作选择函数根据帕累托曲线估计计算动作分布，在搜索阶段鼓励探索未充分探索的动作；阈值更新规则考虑了即时成本、折扣因子和随机决策的成本风险，确保更新后的阈值不会增加估计的预期阈值，且更新后的值在估计范围内是可行的。\\n- 在树搜索和实际动作选择中都考虑成本 - 奖励的权衡，能够更好地平衡安全性和收益。\\n\\n**具体实现步骤**\\n1. **初始化**：树初始仅包含一个代表历史$h_0 = s_0$的节点，设置决策时间$T$和成本阈值$\\Delta$，存储初始的帕累托集估计。\\n2. **迭代搜索**：在每个决策步骤中，从当前根节点$h$开始，依次执行以下四个阶段，直到达到给定的超时时间：\\n    - **搜索（GetLeaf）**：从根节点自上而下遍历树，根据当前帕累托曲线估计和探索奖励，选择最有前途的叶子节点。探索奖励公式为$expl(h, a) = C \\cdot \\alpha(h) \\cdot \\sqrt{\\frac{\\log N(h)}{N(h, a) + 1}}$，其中$N(h)$和$N(h, a)$分别是节点$h$和动作$a$的访问次数，$C$是固定的静态探索常数，$\\alpha(h)$是动态调整的探索常数。\\n    - **叶子扩展（Expand）**：为选定的叶子节点添加一个后继节点到搜索树中。\\n    - **滚动（Rollout）**：估计新叶子节点的帕累托曲线，例如通过蒙特卡罗模拟或预训练的预测器，并添加$(0, 0)$元组以实现“成本乐观”探索。\\n    - **反向传播（Propagate）**：将新获得的信息（特别是帕累托曲线估计）从叶子节点反向传播到根节点，根据贝尔曼方程（1）$P(h) = prune(\\bigcup_{a \\in \\mathcal{A}} P(h, a))$和（2）$P(h, a) = prune(\\sum_{t \\in \\mathcal{S}} \\delta(t | h, a) (P(hat) \\cdot (\\gamma_c, \\gamma_r) + \\{(c(h, a, t), r(h, a, t))\\}))$更新帕累托集估计。若CMDP的转移概率$\\delta$不可用，用样本估计$\\hat{\\delta}$代替。\\n3. **动作选择**：计算并输出一个动作分布$\\sigma$，从中采样要执行的动作$a$。根据帕累托曲线估计，如果没有可行策略，则选择使未来预期成本最小的动作；如果无论选择哪个动作预期未来成本都能保持在阈值以下，则选择具有最高未来受限回报的动作；否则，混合两个最接近阈值的动作，使预期成本等于阈值。\\n4. **阈值更新**：执行动作$a$并观察新状态$t$后，使用UpdateThr函数更新成本阈值$\\Delta$，考虑即时成本、折扣因子和随机决策的成本风险。更新规则分为三种情况：“混合”情况确保总体预期成本受限于$\\Delta_{act}$；“盈余”情况按比例分配成本预算盈余；“不可行”情况将缺失的成本分配到当前结果$t$。\\n5. **重复过程**：将节点$hat$作为新的根节点，重复上述过程，直到达到决策 horizon。\\n\\n**案例解析**\\n论文通过Gridworld和曼哈顿环境的实验来展示算法的效果。在Gridworld的Avoid任务中，智能体要在有陷阱和黄金的网格迷宫中导航，避免陷阱并获取黄金；在SoftAvoid任务中，踩到陷阱会确定性地产生一定成本但环境不终止；在曼哈顿环境中，智能体在曼哈顿中导航，要处理街道通行时间的随机性以及维护请求，接受请求完成可获奖励，未按时完成则产生成本。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- CC - POMCP：基于解决CMDP目标的拉格朗日松弛的对偶方法，通过随机梯度下降更新拉格朗日乘数$\\lambda$，在标准UCT搜索中评估目标函数。\\n- RAMCP：结合MCTS与线性规划的原始方法，搜索阶段贪婪地优化回报，在动作选择的第二阶段解决线性规划以满足成本约束。\\n\\n**性能对比**\\n*   **在安全性能（满足实例比例）指标上：** T - UCT和CC - POMCP在整体环境中能解决相当比例的实例，而RAMCP在寻找可行解决方案方面存在困难，特别是在曼哈顿环境中，T - UCT在曼哈顿环境的弱满意度达到100%，而RAMCP表现不佳。当成本阈值为0时，所有算法的满意度比例相似；但随着阈值增加，RAMCP的性能迅速下降，而T - UCT和CC - POMCP能保持或提高满意实例的数量。在大环境且非零成本阈值下，RAMCP最多只能满足一半的实例，而T - UCT的满足率稳定在0.8 - 1.0之间。\\n*   **在平均回报指标上：** 在所有环境中，T - UCT在满足约束的实例上能达到与RAMCP相似或更高的回报。在大的SoftAvoid实例中，T - UCT的回报明显高于RAMCP。CC - POMCP的动作过于保守，导致回报较低，在曼哈顿环境中，T - UCT的回报比CC - POMCP高出近十倍。\\n*   **在数据效率指标上：** T - UCT在曼哈顿环境中，在最多执行约500次MCTS样本/步（远少于其他基准测试）的情况下，仍能解决大量实例（约74% $SAT_M$，100% $SAT_W$）并获得高回报，展示了其数据效率。\",\n    \"keywords\": \"### 关键词\\n\\n- 受限马尔可夫决策过程 (Constrained Markov Decision Processes, CMDPs)\\n- 蒙特卡罗树搜索 (Monte Carlo Tree Search, MCTS)\\n- 阈值UCT (Threshold UCT, T - UCT)\\n- 成本 - 奖励权衡 (Cost - Reward Trade - off)\\n- 帕累托曲线 (Pareto Curves, N/A)\"\n}"
}