{
    "link": "https://arxiv.org/abs/2503.22285",
    "pdf_link": "https://arxiv.org/pdf/2503.22285",
    "title": "RUNA: Object-level Out-of-Distribution Detection via Regional Uncertainty Alignment of Multimodal Representations",
    "authors": [
        "Bin Zhang",
        "Jinggang Chen",
        "Xiaoyang Qu",
        "Guokuan Li",
        "Kai Lu",
        "Jiguang Wan",
        "Jing Xiao",
        "Jianzong Wang"
    ],
    "publication_date": "2025-03-28",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "paper_content": "# RUNA: Object-level Out-of-Distribution Detection via Regional Uncertainty Alignment of Multimodal Representations\n\nBin Zhang\\*1,2, Jinggang Chen\\*1, Xiaoyang ${ { \\bf { Q } } { \\bf { u } } ^ { \\dag } } ^ { 2 }$ , Guokuan $\\mathbf { L i } ^ { 1 }$ , Kai ${ \\bf L u } ^ { \\dag 1 }$ , Jiguang Wan1, Jing Xiao2, Jianzong Wang2\n\n1Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China 2Ping An Technology (Shenzhen) Co., Ltd, Shenzhen, China {binz2398, chen.jinggang98, quxiaoy} $@$ gmail.com, liguokuan $@$ hust.edu.cn, {kailu, jgwan}@hust.edu.cn, xiaojing $6 6 1 \\textcircled { a }$ pingan.com, jzwang@188.com\n\n# Abstract\n\nEnabling object detectors to recognize out-of-distribution (OOD) objects is vital for building reliable systems. A primary obstacle stems from the fact that models frequently do not receive supervisory signals from unfamiliar data, leading to overly confident predictions regarding OOD objects. Despite previous progress that estimates OOD uncertainty based on the detection model and in-distribution (ID) samples, we explore using pre-trained vision-language representations for object-level OOD detection. We first discuss the limitations of applying image-level CLIP-based OOD detection methods to object-level scenarios. Building upon these insights, we propose RUNA, a novel framework that leverages a dual encoder architecture to capture rich contextual information and employs a regional uncertainty alignment mechanism to distinguish ID from OOD objects effectively. We introduce a fewshot fine-tuning approach that aligns region-level semantic representations to further improve the model‚Äôs capability to discriminate between similar objects. Our experiments show that RUNA substantially surpasses state-of-the-art methods in object-level OOD detection, particularly in challenging scenarios with diverse and complex object instances.\n\n# Introduction\n\nIdentifying out-of-distribution (OOD) objects is vital for object detectors to safely deploy in an open-world environment. Most current models work in a closed-world environment, matching objects to the pre-defined in-distribution (ID) labels. Nevertheless, when deployed in an open-world environment, they acknowledge the possibility of encountering objects from unknown categories, which should not be naively assigned to any ID labels. It poses a risk to the security of object detection models. In high-stakes applications like autonomous driving, failure to detect OOD objects can lead to severe accidents (Nitsch et al. 2021). We can mitigate this risk if unknown objects are detected and the human driver is alerted to take control.\n\nThe susceptibility to OOD inputs stems from insufficient knowledge about unknowns during training. It results\n\nDOetbejcetcotr ‚àù PedTersatfrficansignCar Adapt ùê∂ùê∂ùúã ‚àù PreC-trLaIiPned   \nInfer In-distribution Pre-trained OOD HD Concept Space ùê∂ùê∂ùúè Concept Space ùê∂ùê∂ùúãùúã Alarm Car 99% ID (Car) „Äë Motorcycle OOD 89% Pedestrian 94% OOD r   \na) Over-confident Predictions b) OOD Calibrations\n\nin neural networks tending to abnormally generate overconfident predictions when semantic shifts occur within the samples (Hein, Andriushchenko, and Bitterwolf 2019), as shown in Figure 1. In object-level OOD detection, a line of prior works (Du et al. 2022a,c,b; Wu and Deng 2023; Wu, Deng, and Liu 2024) design additional modules to be integrated into the training process of detectors, leveraging the model‚Äôs inherent uncertainty. Meanwhile, some estimationbased methods directly learning from training data (Lee et al. 2018b; Tack et al. 2020a; Sun et al. 2022) are also introduced into this domain as alternative solutions.\n\nIn recent years, advancements in contrastive multimodal pre-training methods, including ALIGN (Jia et al. 2021), CLIP (Radford et al. 2021), BLIP (Li et al. 2022) and InternVL (Chen et al. 2024b), have provided a novel perspective for detecting out-of-distributions. With extensive prior knowledge, vision-language (VL) representations can transferably detect distributional shifts in downstream imagelevel classification tasks given the ID textual class labels (Esmaeilpour et al. 2022; Ming et al. 2022). This observation prompts us to delve further: if this pre-trained alignment capability can be adapted to measure the regional uncertainty for individual objects, we would effortlessly unlock a safety assistant for deployed detectors, replacing previous limited enhancement methods and potentially boosting the performance of object-level OOD detection. However, applying these pre-trained models to the object-level OOD detection task presents substantial challenges. Unlike imagelevel classification, where the entire image is considered, object detection focuses on localized regions. This localization process can lead to a loss of contextual information, making it difficult to accurately assess an object‚Äôs anomaly.\n\nMoreover, the effectiveness of pre-trained models is often affected by the variety and quality of the dataset of the object detector. Datasets like BDD-100K(Yu et al. 2020), which contain a wide range of object sizes, lighting conditions, and occlusions, can pose challenges for models trained on more generic datasets. For instance, an object detector trained on BDD-100K might misclassify small, occluded objects from other datasets as ID vehicles. This situation underscores the necessity for domain-specific adaptation to enhance the performance of these models in applied settings.\n\nThis study proposes a novel framework, RUNA, to address the abovementioned limitations for object-level OOD detection. RUNA leverages a dual encoder architecture to provide rich contextual information and employs a Regional UNcertainty Alignment strategy to effectively calculate uncertainty scores for object regions, enabling accurate classification as ID or OOD. We employ few-shot fine-tuning to bridge the performance gap between generic and domainspecific datasets. The diverse and challenging nature of datasets like BDD-100K, characterized by varying lighting conditions, occlusions, and object appearances, necessitates tailored model adaptation. Moreover, the pre-trained VL models, primarily trained on scene-centric images, exhibit limited alignment capability with the unique characteristics of domain-specific datasets. Fine-tuning allows the model to acquire domain-specific characteristics, enhancing its capability to detect OOD objects efficiently.\n\nOur contributions are as follows:\n\n‚Ä¢ We propose RUNA, a novel object-level OOD detection framework with a dual encoder architecture that captures global and local features for accurate regional uncertainty estimation.   \n‚Ä¢ We develop a few-shot fine-tuning approach to efficiently align region-level ID semantics, substantially enhancing the model‚Äôs capacity to differentiate between ID and OOD objects.   \n‚Ä¢ Our approach remarkably improves object-level OOD detection performance compared to previous methods.\n\n# Preliminaries\n\nObject-level OOD Detection. Object-level OOD detection aims to identify unknown objects that fall outside the recognized categories and may be misidentified by the model. Object-level OOD detection is more applicable to real-world machine learning systems than image-level OOD detection. However, it also presents more significant challenges, as it requires careful consideration of each object‚Äôs uncertainties at a fine-grained level.\n\nWe define the ID space as $\\chi$ , with the associated label space given by $\\mathcal { V } _ { \\mathrm { i n } } ~ \\stackrel { - } { = } ~ \\left\\{ y _ { 1 } , y _ { 2 } , y _ { 3 } , . . . y _ { K } \\right\\}$ . In object-level\n\n![](images/3b7edb2414812fc47a420bf982bb216cdbc320457df3e49c3cc6a79573facc45.jpg)  \nFigure 2: Framework of CLIP-based OOD Detection. Green arrows represent ID samples, while red arrows denote OOD samples. The solid line highlights the maximum similarity, and the dotted lines indicate other similarity measures.\n\nOOD detection literature(Du et al. 2022a,b,c; Wu and Deng 2023; Wu, Deng, and Liu 2024), regional objects $\\hat { x } _ { b }$ from an OOD sample $x _ { \\mathrm { o u t } }$ are considered to experience a semantic shift compared to ID objects, meaning their label space $y _ { \\mathrm { o u t } }$ does not overlap with ${ \\mathcal { V } } _ { \\mathrm { i n } }$ .\n\nFor an unknown image $x$ , the object detector $f _ { \\theta }$ predicts results as $D _ { x } = \\{ b _ { i } , y _ { i } ^ { \\top } \\} _ { i = 1 } ^ { m }$ . Here, $b _ { i } \\in \\mathbb { R } ^ { 4 }$ represents the bounding box, and $y _ { i } ^ { p } \\in \\mathcal { V } _ { \\mathrm { i n } }$ is its ID semantic label. OOD detection is structured as a binary classification task with uncertainty estimation $\\sigma ( \\cdot )$ , distinguishing between ID and OOD objects. Given a bounding box $b$ , the goal is to predict the uncertainty $\\sigma ( x , b )$ :\n\n$$\nG ( \\widehat { x } _ { b } , \\mathbb { y } _ { \\mathrm { i n } } ) = \\left\\{ \\mathrm { i n } , \\quad \\mathrm { i f } \\quad \\mathbb { E } [ \\sigma ( x , b ) \\mid \\mathbb { y } _ { \\mathrm { i n } } ] \\leq \\gamma \\right.\n$$\n\nwhere $\\gamma$ is the threshold chosen such that a high fraction of ID data (e.g., $9 5 \\%$ ) falls below it.\n\nZero-shot CLIP-based OOD Detection. CLIP has excelled in zero-shot OOD detection tasks by utilizing extensive training data and large-scale models.\n\nWe outline the approach for performing zero-shot OOD detection utilizing existing CLIP-based zero-shot methodologies Maximum Concept Matching (MCM)(Ming et al. 2022). As depicted in Figure 2, the CLIP model employs an image encoder to extract features from images. While CLIP lacks an explicit classifier, we can use ID classes to create text inputs (e.g., ‚Äùa photo of a dog‚Äù). The text encoder processes these text inputs to produce class-specific features, which act similarly to a classifier. Let $\\mathcal { T } _ { \\mathrm { i n } }$ represent the collection of test prompts that include $K$ class labels. To illustrate, Ming et al. calculates the score with the softmax score of the similarity between image features and textual features:\n\n$$\n\\begin{array} { r } { S _ { \\mathrm { M C M } } ( x ) = \\underset { t _ { i } \\in \\mathcal { T } _ { \\mathrm { i n } } } { \\mathrm { m a x } } \\frac { e ^ { \\mathrm { S i m } ( \\mathcal { T } ( x ) , \\mathcal { T } ( t _ { i } ) ) / \\tau } } { \\sum _ { t _ { c } \\in \\mathcal { T } _ { \\mathrm { i n } } } e ^ { \\mathrm { S i m } ( \\mathcal { T } ( x ) , \\mathcal { T } ( t _ { c } ) ) / \\tau } } } \\end{array}\n$$\n\nwhere $x$ denotes the input image, $t _ { i }$ and $t _ { c }$ denotes the text to match. $\\mathrm { S i m } ( \\mathcal { T } ( x ) , \\mathcal { T } ( t _ { i } ) )$ denotes the similarity between image feature $\\mathcal { T } ( x )$ and textual feature $\\mathcal { T } ( t _ { i } )$ . If the MCM for a given image is below a predefined threshold, it is classified as ID; otherwise, it is considered OOD.\n\nÂõõ Blur Resize EInmcaogder EmIbmedagdiengs RegioSnimOilnalry PCraerd(IicDt): FGelatoubraels (P) ‚ÜíE[œÉ(x,b) | Vinl: With Global   \nObject Detector Crop Resize A Encoder Image Uncertainty OOD in in Á¶è ÂõΩ   \nP Features ID (car ‚àö) p a   \nÊ∏ÖÂú∞ Textual (P) Â±± Unlocked Module Textual ID Encoder Prompt Label Textual Inference Flow   \nSamples Ground Truth Embeddings Fine-tuning Part\n\n# Method\n\nOverview. As in Figure 3, our framework, RUNA, utilizes a dual Image Encoder structure $\\mathcal { T } ^ { \\left( g \\right) } , \\mathcal { T } ^ { \\left( r \\right) }$ to collaboratively process visual information, where $\\boldsymbol { \\mathcal { T } ^ { ( g ) } }$ captures global features from the entire image, and $\\boldsymbol { \\mathcal { T } ^ { ( r ) } }$ focuses on regional features by processing specific objects or areas of interest. The outputs are subsequently fused to produce the final image embeddings. We employ a metric based on the maximum similarity score to quantify uncertainty in object detection. Notably, we exclusively fine-tune the projection layer of the visual encoder, denoted as $\\scriptstyle { \\mathcal { T } } _ { P }$ , to tailor the pre-trained model for our particular task.\n\nGiven an image $x$ and its corresponding region of interest (bounding box) $\\hat { x } _ { b }$ , our framework first extracts global features from the entire image using ${ \\mathcal { T } } ^ { ( g ) } ( x )$ . Here, bounding box blurring is applied to keep contextual information while emphasizing the objects of interest, enhancing the extracted features‚Äô relevance. Concurrently, $\\mathcal { T } ^ { ( r ) } ( \\hat { x } _ { b } )$ processes the region of interest by cropping the specific area, enabling a more focused and detailed representation of the object.\n\nTo effectively integrate the semantic information extracted from the regional image with the global context, we propose a novel fusion strategy, which can be expressed as:\n\n$$\n\\mathcal { T } _ { \\mathrm { t } } ( x , \\hat { x } _ { b } ) = \\mathcal { T } _ { P } ( \\lambda \\cdot \\mathcal { T } ^ { ( r ) } ( \\hat { x } _ { b } ) \\oplus ( 1 - \\lambda ) \\cdot \\mathcal { T } ^ { ( g ) } ( x ) )\n$$\n\nwhere $\\oplus$ denotes element-wise addition, and $\\lambda$ is a weighting coefficient that regulates the influence of each encoder. The resulting $\\mathcal { T } _ { \\mathfrak { t } } ( x , \\bar { x } _ { b } )$ represents the final image embedding, which synthesizes both regional and global features.\n\nAs illustrated in the top-right dashed box on the left in Figure 3, when encountering objects with similar semantics, relying solely on limited local features may cause the model to misinterpret subtle differences, leading to incorrect decisions. This is especially problematic when visually similar objects share overlapping attributes, making it challenging to distinguish between them using only localized cues. By incorporating global features, the model gains a broader context, enabling it better to grasp the overall scene structure and relational information. This holistic view allows for more informed and accurate judgments, as the model can integrate detailed local patterns and the larger contextual backdrop, leading to more robust OOD detection.\n\nAs illustrated in Figure 3, our approach functions as a post-hoc corrective technique that does not interfere with the training process of the object detection model. For a regional object $\\hat { x } _ { b }$ identified by the detector‚Äôs prediction $x , b$ , we transform the distributional uncertainty $\\sigma ( x , b )$ of $\\hat { x } _ { b }$ into a distance measure relative to the ID semantic space. Given the ID space as $\\mathcal { P } \\in \\mathbb { R } ^ { K }$ , the uncertainty of a predicted region $\\hat { x } _ { b }$ is represented as its deviation from this known distribution $\\mathcal { P }$ :\n\n$$\n\\mathbb { E } [ \\sigma ( x , b ) \\mid \\mathcal { Y } _ { \\mathrm { i n } } ] = \\mathbb { E } _ { \\mathcal { P } } [ \\mathcal { H } ( \\hat { x } _ { b } , \\mathcal { P } ) ]\n$$\n\nwhere $\\mathcal { H }$ denotes the selected distance measurement function. In this framework, the uncertainty of an unknown object is converted into a distance-based estimation relative to the ID space $\\mathcal { P }$ . In line with prior research(Ming et al. 2022), we construct the $K$ dimensions of $\\mathcal { P }$ using the pre-trained CLIP text encoder $\\tau ( \\cdot )$ .\n\nRegional Uncertainty Alignment. Our goal is to quantify the discrepancy between the input region image $\\hat { x } _ { b }$ and the entire ID semantic space $\\mathcal { P }$ . This discrepancy is accomplished by assessing the similarity between the image features and predefined concept vectors corresponding to ID labels, with this similarity serving as a distance measure.\n\nInitially, we consider that the semantic similarities between $\\mathcal { T } _ { \\mathfrak { t } } ( x , \\hat { x } _ { b } )$ and all ID concept vectors contribute to assessing its distance from the ID space $\\mathcal { P }$ which means $\\mathcal { T } ( t _ { i } )$ . A straightforward method is to sum these similarities (Direct\n\n![](images/7ee8ce9a90b13659977167fdce6b76d7a24b08199062007490feb4083fd7fc5b.jpg)  \nFigure 4: Distribution of uncertainty scores for Direct Sum, $\\mathbf { M C M } ( \\tau = 1 )$ , MCM $\\mathit { T } = 1 0 0 \\$ ) and Max Similarity.\n\nSum) to reflect the degree of deviation of $\\{ x , \\hat { x } _ { b } \\}$ :\n\n$$\n\\mathbb { E } [ \\sigma ( x , b ) \\mid \\mathcal { Y } _ { \\mathrm { i n } } ] = - \\sum _ { i = 1 } ^ { K } \\mathrm { S i m } ( \\mathcal { T } _ { \\mathrm { t } } ( x , \\hat { x } _ { b } ) , \\mathcal { T } ( t _ { i } ) )\n$$\n\nThe negative sign indicates lower semantic similarity with ID concept vectors for OOD objects corresponds to a greater distance from $\\mathcal { P }$ , implying higher uncertainty.\n\nHowever, during our evaluation, we observed that Direct Sum fails to effectively distinguish between ID and OOD objects in the VOC dataset. We attribute this issue to the limited variance in the cosine similarities outputted by CLIP, which do not exhibit significant differences between matching and non-matching situations. This results in the absolute values of the summed similarities, overshadowing the impact of actual differences.\n\nTo tackle this issue, we adapt the MCM in Eq.(2) by substituting $\\mathcal { T } ( x )$ with $\\mathcal { T } _ { t } ( x , \\hat { x } _ { b } )$ , thereby amplifying the differences between the similarities. However, MCM does not significantly improve the differentiation between ID and OOD objects, as shown in Figure 4. Interestingly, we notice that as the scaling factor of MCM increases, its performance improves. This phenomenon prompts us to investigate further. We find that as the differences between values expand, the influence of larger values on the scores also increases. When the factor approaches its limit, the score is predominantly influenced by the maximum similarity value. Consequently, we define the uncertainty estimation metric as follows:\n\n$$\n\\mathbb { E } [ \\sigma ( \\boldsymbol { x } , b ) \\mid \\mathcal { Y } _ { \\mathrm { i n } } ] = - \\operatorname* { m a x } _ { 1 \\leq i \\leq K } \\mathrm { S i m } ( \\mathcal { Z } _ { \\mathrm { t } } ( \\boldsymbol { x } , \\boldsymbol { \\hat { x } } _ { b } ) , \\mathcal { T } ( t _ { i } ) )\n$$\n\nwhere $\\mathrm { S i m } ( \\mathcal { T } _ { \\mathrm { t } } ( x , \\hat { x } _ { b } ) , T ( t _ { i } ) )$ denotes similarity between the image feature $\\mathcal { T } _ { \\mathfrak { t } } ( x , \\hat { x } _ { b } )$ and the concept vector $\\mathcal { T } ( t _ { i } )$ for each label $i$ and $K$ denotes the number of labels.\n\nFew-shot Fine-tuning. Although the zero-shot method lays a solid groundwork for OOD detection, it lacks the nuanced understanding of ID data needed to distinguish between ID and OOD samples precisely. We introduce a fine-tuning strategy that utilizes few-shot learning for costeffective and rapid adaptation to fill this gap. By randomly selecting a small set of images, our approach infuses the model with region-specific details, unlocking a more profound comprehension of ID characteristics.\n\nFor a given image $x$ , we treat all ground truth bounding boxes $\\{ \\hat { x } _ { b } ^ { i } \\} _ { i = 1 } ^ { m }$ as potential fine-tuning candidates. From these, $N$ shots of $K$ kinds of objects are randomly drawn, denoted as $\\{ ( \\hat { x } _ { b } ^ { i } , y _ { i } ) \\} _ { i = 1 } ^ { N K }$ , where each $\\hat { x } _ { b } ^ { i }$ corresponds to a regional patch and $y _ { i }$ indicates its associated label. This process selectively exposes the model to key ID regions, enhancing its ability to align with the fine-grained semantic features vital for robust ID/OOD discrimination.\n\nGiven the pre-trained image encoder‚Äôs capacity for intense feature extraction, our fine-tuning selectively focuses on refining the projection layer after the image encoder handles regional images. We aim to align visual embeddings with their corresponding label embeddings closely. To this end, we employ a contrastive loss that sharpens the model‚Äôs intra-ID discriminative power:\n\n$$\n\\mathcal { L } _ { \\mathrm { I D } } = - \\sum _ { \\hat { x } _ { b } \\in \\mathcal { B } } \\log \\frac { \\exp ( \\mathrm { S i m } ( \\mathcal { T } ( \\hat { x } _ { b } ) , \\mathcal { T } ( t _ { i } ) ) / \\tau ) } { \\sum _ { j = 1 } ^ { K } \\exp ( \\mathrm { S i m } ( \\mathcal { T } ( \\hat { x } _ { b } ) , \\mathcal { T } ( t _ { j } ) ) / \\tau ) }\n$$\n\nwhere $\\tau$ is a temperature to scale cosine similarities. This contrastive loss formulation hones the model‚Äôs precision within the ID space and optimizes its ability to differentiate between subtle category variations, driving a more contextaware and semantically aligned fine-tuning process.\n\n# Experiments\n\n# Experimental Settings\n\nDatasets and metrics. We use PASCAL-VOC(Everingham et al. 2010) and BDD-100K(Yu et al. 2020) as ID datasets and evaluate on two OOD datasets sourced from MSCOCO(Lin et al. 2014) and OpenImages(Kuznetsova et al. 2020), ensuring no label overlap with ID datasets. The object detection model is pre-trained on the ID datasets. We evaluate using three metrics: 1) FPR95: False Positive Rate at $9 5 \\%$ True Positive Rate for ID samples, indicating the proportion of misclassified OOD objects; 2) AUROC: Area Under the ROC curve, where higher values indicate superior performance; 3) mAP: We do not report mAP as the object detection model is not affected by the integreted RUNA OOD detector.\n\nModels and Baselines. We utilize the Detectron2 platform (Wu et al. 2019) with Faster R-CNN(Ren et al. 2015) (using ResNet50(He et al. 2016) as the backbone) as the frozen object detection model. We adopt CLIP (VIT-B/16) (Radford et al. 2021) for the vision-language model. Our CLIP-based approaches are evaluated against several widely adopted image-level approaches, including MSP (Hendrycks and Gimpel 2017), ODIN (Liang, Li, and Srikant 2018), Mahalanobis (Lee et al. 2018b), Generalized ODIN (Hsu et al. 2020), CSI (Tack et al. 2020b), Gram matrices (Sastry and Oore 2020), Energy score (Liu et al. 2020), and CLIP-based MCM(Ming et al. 2022). Additionally, we compare with object-level OOD detection methods such as VOS (Du et al. 2022c), SIREN (Du et al. 2022a), SAFE (Wilson et al. 2023), TIB (Wu and Deng 2023), and PCAbased method (Wu, Deng, and Liu 2024).\n\nImplementation details. We exclusively fine-tune the projection layer of the regional image encoder individually, while the global image encoder is off-the-shelf. For few-shot learning, we perform fine-tuning using 10-shot samples. For ID discriminative fine-tuning, we employ a batch size of 256 and use the AdamW optimizer, conducting fine-tuning over 100 epochs with a base learning rate of $5 \\bar { \\times } 1 0 ^ { - 6 }$ . We use ‚Äùa photo of a $\\{ \\mathrm { l a b e l } \\} ^ { \\dag }$ for the textual prompt. We set the dual encoder‚Äôs fusion coefficient $\\lambda$ to 0.5 and the blur radius R of Gaussian Blur to 1.\n\nTable 1: Main results. $\\uparrow$ denotes that higher values are considered superior, while $\\downarrow$ signifies that lower values are desirable. All results are presented as percentages. Bold numbers represent superior results, and the second-best performance is marked with an underline. $*$ means adapted with our dual-encoder architecture.   \n\n<html><body><table><tr><td rowspan=\"2\">In-distribution datasets</td><td rowspan=\"2\">Detection Method</td><td colspan=\"2\">OpenImages</td><td colspan=\"2\">MSCOCO</td></tr><tr><td>FPR95‚Üì</td><td>AUROC ‚Üë</td><td>FPR95‚Üì</td><td>AUROC ‚Üë</td></tr><tr><td rowspan=\"14\">Berkeley DeepDrive-100k</td><td>MSP(Hendrycks and Gimpel 2017)</td><td>79.04</td><td>77.38</td><td>80.94</td><td>75.87</td></tr><tr><td>ODIN (Liang,Li,and Srikant 2018)</td><td>58.92</td><td>76.61</td><td>62.85</td><td>74.40</td></tr><tr><td>Mahalanobis (Lee etal.2018b)</td><td>60.16</td><td>86.88</td><td>57.66</td><td>84.92</td></tr><tr><td>Energy score (Liu et al. 2020)</td><td>54.97</td><td>79.60</td><td>60.06</td><td>77.48</td></tr><tr><td>Gram matrices (Sastry and Oore 2020)</td><td>77.55</td><td>59.38</td><td>60.93</td><td>74.93</td></tr><tr><td>Generalized ODIN (Hsu et al. 2020)</td><td>50.17</td><td>87.18</td><td>57.27</td><td>85.22</td></tr><tr><td>CSI (Tack et al. 2020b)</td><td>37.06</td><td>87.99</td><td>47.10</td><td>84.09</td></tr><tr><td>SIREN (Du et al. 2022a)</td><td>37.19</td><td>87.87</td><td>39.54</td><td>88.37</td></tr><tr><td>VOS (Du et al. 2022c)</td><td>35.61</td><td>88.46</td><td>44.13</td><td>86.92</td></tr><tr><td>MCM* (Ming et al. 2022)</td><td>45.37</td><td>88.46</td><td>53.79</td><td>86.92</td></tr><tr><td>SAFE (Wilson et al. 2023)</td><td>16.04</td><td>94.64</td><td>32.56</td><td>88.96</td></tr><tr><td>TIB(Wu and Deng 2023)</td><td>24.00</td><td>92.54</td><td>36.85</td><td>88.47</td></tr><tr><td>PCA-based (Wu, Deng,and Liu 2024)</td><td>35.05</td><td>88.92</td><td>45.72</td><td>85.14</td></tr><tr><td>RUNA (Ours)</td><td>9.95</td><td>96.76</td><td>16.85</td><td>93.92</td></tr><tr><td rowspan=\"14\">PASCAL-VOC</td><td>MSP (Hendrycks and Gimpel 2017)</td><td>73.13</td><td>81.91</td><td>70.99</td><td>83.45</td></tr><tr><td>ODIN (Liang,Li,and Srikant 2018)</td><td>63.14</td><td>82.59</td><td>59.82</td><td>82.20</td></tr><tr><td>Mahalanobis (Lee et al. 2018b)</td><td>96.27</td><td>57.42</td><td>96.46</td><td>59.25</td></tr><tr><td>Energy score (Liu et al. 2020)</td><td>58.69</td><td>82.98</td><td>56.89</td><td>83.69</td></tr><tr><td>Gram matrices (Sastry and Oore 2020)</td><td>67.42</td><td>77.62</td><td>62.75</td><td>79.88</td></tr><tr><td>Generalized ODIN (Hsu et al. 2020)</td><td>70.28</td><td>79.23</td><td>59.57</td><td>83.12</td></tr><tr><td>CSI (Tack et al. 2020b)</td><td>57.41</td><td>82.95</td><td>59.91</td><td>81.83</td></tr><tr><td>SIREN (Du et al. 2022a)</td><td>49.12</td><td>87.21</td><td>54.23</td><td>86.89</td></tr><tr><td>VOS (Du et al. 2022c)</td><td>50.79</td><td>85.42</td><td>47.29</td><td>88.35</td></tr><tr><td>MCM* (Ming et al. 2022)</td><td>48.73</td><td>80.16</td><td>50.43</td><td>78.22</td></tr><tr><td>SAFE(Wilson et al. 2023)</td><td>20.36</td><td>92.28</td><td>47.40</td><td>80.30</td></tr><tr><td>TIB(Wu and Deng 2023)</td><td>47.19</td><td>88.09</td><td>41.55</td><td>90.36</td></tr><tr><td>PCA-based (Wu,Deng, and Liu 2024)</td><td>50.56</td><td>85.71</td><td>44.54</td><td>89.40</td></tr><tr><td>RUNA (Ours)</td><td>26.07</td><td>93.63</td><td>30.67</td><td>92.48</td></tr></table></body></html>\n\n# Main Results\n\nAs illustrated in Table 1, our proposed CLIP-based approach, RUNA, demonstrates advantages over previous methods. Notably, on the autonomous driving dataset BDD100K, our fine-tuning approach significantly enhances the detection of OOD objects. In tests on the OOD dataset OpenImages, RUNA achieves an FPR95 of $9 . 9 5 \\%$ , marking a $6 . 0 9 \\%$ reduction compared to the previously bestperforming method, SAFE. On the OOD dataset MSCOCO, our method achieved an FPR95 of $1 6 . 8 5 \\%$ , improving by $1 5 . 7 1 \\%$ compared to SAFE. When VOC serves as the ID dataset and OpenImages as the OOD dataset, SAFE achieves a lower FPR95 compared to our method. However, on the OOD dataset MSCOCO, our method outperforms SAFE with an FPR95 of $3 0 . 6 7 \\%$ , achieving an improvement of\n\n$1 6 . 7 3 \\%$ over SAFE and $1 0 . 8 8 \\%$ over SOTA method TIB. Our approach provides detector-agnostic performance without requiring manual feature selection, making it more flexible and broadly applicable across various detection scenarios. In contrast, previous methods explicitly designed for object-level OOD detection require retraining the target detection model, which may affect its original detection performance.\n\nFurthermore, our fine-tuning framework showcases a remarkable improvement over the zero-shot approach. For instance, on VOC, RUNA shows a substantial FPR95 improvement of $12 . 9 0 \\%$ on OpenImages and $2 0 . 7 0 \\%$ on MSCOCO. Specifically, on BDD-100K, RUNA shows a substantial FPR95 improvement of $1 9 . 4 7 \\%$ on OpenImages and $2 0 . 1 1 \\%$ on MSCOCO.\n\n# Ablation Study\n\nAblation on the dual encoder and ID fine-tuning. We examine the impact of the dual encoder and ID fine-tuning components within our object-level OOD detection framework. The fine-tuning strategy is designed to improve the discriminative capability of the pre-trained model with respect to ID semantics. At the same time, the dual encoder focuses on strengthening sensitivity to ID objects via enhanced feature representation. The evaluation results are presented in Table 2. We compare the performance of models with and without the dual encoder to assess its contribution to feature extraction. Additionally, we assess the impact of fine-tuning on ID data, analyzing its role in adapting the model to the unique features of the ID space and improving OOD detection accuracy. The results demonstrate that the dual encoder significantly enhances the model‚Äôs capability to distinguish between ID and OOD samples. ID fine-tuning further refines this capability, leading to more robust OOD detection.\n\n![](images/1a515b532f2ba10963a27170487dd6a2b4034aeb4aefcab1111b35d86e248d9e.jpg)  \nFigure 5: Ablation study on the number of fine-tuning samples (shots). This study examines how varying the number of shots affects detection performance, showing the trade-offs between data efficiency and detection quality.\n\nTable 2: Ablation study on our regional encoder (RE), global encoder (GE) and few-shot fine-tune (FT).   \n\n<html><body><table><tr><td rowspan=\"2\">ID</td><td colspan=\"3\">Method</td><td colspan=\"2\">OpenImages/MSCOCO</td></tr><tr><td>RE</td><td>GE</td><td>FT</td><td>FPR95‚Üì</td><td>AUROC ‚Üë</td></tr><tr><td rowspan=\"4\">BDD- 100k</td><td>‚àö</td><td>1 1</td><td>58.09/62.17</td><td></td><td rowspan=\"4\">70.94/69.10 89.21/84.19 89.90/85.37</td></tr><tr><td>1</td><td>‚àö</td><td>1</td><td>31.78/39.24</td></tr><tr><td>‚àö</td><td>‚àö</td><td>1</td><td>29.42/36.96</td></tr><tr><td>‚àö ‚àö</td><td>1 ‚àö ‚àö ‚àö</td><td>15.71/22.14 9.95/16.85</td><td>93.34/92.26 96.76/93.92</td></tr><tr><td rowspan=\"5\">VOC</td><td>‚àö 1</td><td>1 ‚àö</td><td></td><td>42.75/55.26 39.22/52.49</td><td>91.19/84.65 91.45/87.49</td></tr><tr><td>‚àö</td><td>‚àö</td><td>1</td><td>38.97/51.37</td><td>91.78/88.92</td></tr><tr><td></td><td></td><td></td><td>30.76/34.31</td><td>92.01/90.17</td></tr><tr><td>‚àö</td><td>1</td><td>‚àö</td><td></td><td></td></tr><tr><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>26.07/30.67</td><td>93.63/92.48</td></tr></table></body></html>\n\nAblation on the number of fine-tuning samples (shots). We examine the effect of changing the number of samples used for model fine-tuning. The evaluation results are presented in Figure 5. The zero-shot method depends entirely on the pre-trained model, providing a competitive baseline performance but struggling with object-level OOD detection due to the lack of ID supervision. Introducing 1-shot learning shows immediate improvements, leveraging a single example to better align the model with the target task. With 5-shot learning, the model demonstrates significant gains, as many examples facilitate a more comprehensive understanding of the data distribution. Finally, 10-shot learning further enhances performance, capturing even more nuances and variations within the data. This study illustrates the clear benefits of incorporating a few labeled examples, with each incremental increase in sample size resulting in notable improvements in the model‚Äôs accuracy and robustness. However, we also perform fine-tuning on the entire dataset, which yields only minimal improvements while incurring significantly more computational costs. This indicates that while few-shot learning provides substantial benefits, full dataset fine-tuning offers diminishing returns compared to the increased computational demands.\n\nAblation on different backbones of the visual encoder. We assess the influence of various ViT backbones on the performance of the CLIP model, as illustrated in Table 3. The analysis reveals that while the more extensive backbones, ViT-L/14 and ViT-L/14-336, provide slight improvements in FPR95 and AUROC, they substantially increase runtime and parameter count. Specifically, the ViT-L/14 and ViT-L/14-336 backbones, despite their slightly better performance, significantly increase computational demands. On the other hand, ViT-B/16 offers a good balance between efficiency and performance, demonstrating that the additional computational cost of larger models does not proportionally enhance performance. Consequently, we choose ViT-B/16 as the backbone for its optimal trade-off between performance gains and resource efficiency. This choice ensures that our framework remains computationally feasible while delivering high accuracy in OOD detection.\n\n# Related Work\n\n# OOD Detection for Classification\n\nOOD detection distinguishes the unknown inputs that deviate from ID training data during the testing phase. The employment of the maximum softmax probability (MSP) (Hendrycks and Gimpel 2017) serves as a common baseline approach; however, it can yield excessively high values for OOD inputs (Hein, Andriushchenko, and Bitterwolf 2019). Various enhancements have been suggested, such as ODIN (Liang, Li, and Srikant 2018), Mahalanobis (Lee et al. 2018b), Energy score (Liu et al. 2020), Gram matrices (Sastry and Oore 2020), CSI (Tack et al. 2020b), GODIN (Hsu et al. 2020), etc. While traditional OOD detection methods (Dhamija, Gu¬®nther, and Boult 2018; Lee et al. 2018a; Hendrycks, Mazeika, and Dietterich 2018; Li and Vasconcelos 2020; Chen et al. 2024a; Kingma and Dhariwal 2018; Schirrmeister et al. 2020) largely stemmed from image-level classification tasks, some unique challenges posed by object detection require specialized approaches.\n\nTable 3: Ablation on the effect of different CLIP configurations on performance and run time. We evaluate the effects upon zero shot regional encoder only method. The results are presented for two OOD datasets, with OpenImages followed by MSCOCO   \n\n<html><body><table><tr><td rowspan=\"2\">Backbone</td><td colspan=\"2\">BDD-100k</td><td colspan=\"2\">VOC</td><td colspan=\"3\">Run time(ms)</td><td rowspan=\"2\">Params</td></tr><tr><td>FPR95‚Üì</td><td>AUROC ‚Üë</td><td>FPR95‚Üì</td><td>AUROC ‚Üë</td><td>1-image</td><td> 5-image</td><td>10-image</td></tr><tr><td>VIT-B/32</td><td>59.47/64.11</td><td>68.02/70.13</td><td>43.59/54.37</td><td>89.19/80.23</td><td>62.9</td><td>90.7</td><td>129.1</td><td>0.15B</td></tr><tr><td>VIT-B/16</td><td>58.09/62.17</td><td>70.94/69.10</td><td>42.75/55.26</td><td>91.19/84.65</td><td>83.8</td><td>220.1</td><td>410.4</td><td>0.15B</td></tr><tr><td>VIT-L/14</td><td>57.24/61.01</td><td>71.89/71.24</td><td>42.38/53.49</td><td>91.56/86.67</td><td>309.1</td><td>863.7</td><td>1559.5</td><td>0.43B</td></tr><tr><td>VIT-L/14(336)</td><td>56.99/60.12</td><td>72.80/73.01</td><td>42.01/53.35</td><td>91.98/87.13</td><td>533.3</td><td>2086.5</td><td>4673.9</td><td>0.43B</td></tr></table></body></html>\n\n# Object-level OOD Detection\n\nOOD detection has garnered increasing attention in object detection to ensure the robustness of visual systems. Mainstream approaches (Du et al. 2022a,c) primarily focus on model regularization to achieve optimal representations. In contrast, the SAFE method enhances OOD detection by selecting sensitivity-aware features from the object detector and inputting them into an auxiliary MLP network (Wilson et al. 2023). Recent advancements include the Two-Stream Information Bottleneck method (Wu and Deng 2023), which utilizes dual information streams to identify unfamiliar objects without relying on auxiliary data, and the PCA-Driven Dynamic Prototype Enhancement technique, which dynamically refines prototypes for improved OOD discrimination using Principal Component Analysis (Wu, Deng, and Liu 2024). While many conventional methods depend on model uncertainty for OOD detection, we leverage a pre-trained vision-language model. The enhanced alignment knowledge from this model enables us to overcome the cognitive limitations of the original detection framework, resulting in improved OOD detection performance.\n\n# OOD Detection with Vision-language Models\n\nRecent vision-language models, such as CLIP (Radford et al. 2021), have significantly advanced computer vision by aligning images and text in a shared feature space using a self-supervised contrastive objective. In OOD detection, Esmaeilpour et al. proposed ZOC, integrating a transformerbased decoder with CLIP‚Äôs image encoder (Radford et al. 2021), tackling the challenge of sourcing OOD candidate labels‚Äîan issue our method bypasses. Building on this, Ming et al. introduced a CLIP-based OOD detection approach using MCM, while (Miyai et al. 2023) explored zeroshot ID detection to determine whether all objects in an image are ID. Unlike image-level studies, our work harnesses vision-language models for object-level OOD detection. The core challenge is to localize and identify region-level out-ofdistribution objects within images. This task becomes particularly complex when most pre-trained models are designed for image-level representations. In contrast to previous studies that heavily rely on textual prompting, we concentrate on visual prompting to effectively guide CLIP‚Äôs attention to the target of interest. By integrating few-shot learning, we greatly enhance the efficiency of the fine-tuning process. This approach allows us to achieve optimal performance with a minimal amount of data, thereby improving the model‚Äôs flexibility and robustness across various contexts and reassuring the reader of the reliability of our approach.\n\n# Conclusion\n\nThis paper focuses on detecting OOD objects using pretrained vision-language representations. Our investigation begins with evaluating the effectiveness of CLIP-like representations in identifying object-level OOD instances and proposes the zero-shot baseline. Additionally, we propose RUNA, a novel Regional UNcertainty Alignment strategy, which significantly enhances detection performance by adapting vision-language models to ID semantic space and guiding vision-language models to be more sensitive to ID concepts. Overall, experimental results demonstrated substantial improvements over previous methods, emphasizing the effectiveness and promise of our proposed approaches. In the future, we intend to investigate more refined visual prompting techniques to enhance the model‚Äôs capacity to capture subtle details and variations within the data. Furthermore, we aim to explore the deployment of our model to edge services, enabling real-time object detection and OOD detection in resource-constrained environments.",
    "institutions": [
        "Wuhan National Laboratory for Optoelectronics",
        "Huazhong University of Science and Technology",
        "Ping An Technology (Shenzhen) Co., Ltd"
    ],
    "summary": "{\n    \"core_summary\": \"### Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n**ÈóÆÈ¢òÂÆö‰πâ**\\nËÆ∫ÊñáÊó®Âú®Ëß£ÂÜ≥ÁõÆÊ†áÁ∫ßÂàÜÂ∏ÉÂ§ñÔºàOODÔºâÂØπË±°Ê£ÄÊµãÈóÆÈ¢ò„ÄÇÂΩìÂâçÂ§öÊï∞Ê®°ÂûãÂú®Â∞ÅÈó≠‰∏ñÁïåÁéØÂ¢É‰∏ãÂ∑•‰ΩúÔºåÂú®ÂºÄÊîæ‰∏ñÁïåÁéØÂ¢É‰∏≠ÈÉ®ÁΩ≤Êó∂ÔºåÂØπÊú™Áü•Á±ªÂà´ÁöÑOODÂØπË±°Êòì‰∫ßÁîüËøáÂ∫¶Ëá™‰ø°ÁöÑÈ¢ÑÊµãÔºåËøôÂú®Ëá™Âä®È©æÈ©∂Á≠âÂÖ≥ÈîÆÂ∫îÁî®‰∏≠‰ºöÂ∏¶Êù•‰∏•ÈáçÂÆâÂÖ®È£éÈô©„ÄÇÂõ†Ê≠§Ôºå‰ΩøÁõÆÊ†áÊ£ÄÊµãÂô®ËÉΩÂ§üËØÜÂà´OODÂØπË±°ÂØπ‰∫éÊûÑÂª∫ÂèØÈù†Á≥ªÁªüËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊ≠§Â§ñÔºåÂ∞ÜÂü∫‰∫éÂõæÂÉèÁ∫ßCLIPÁöÑOODÊ£ÄÊµãÊñπÊ≥ïÂ∫îÁî®‰∫éÁõÆÊ†áÁ∫ßÂú∫ÊôØÂ≠òÂú®Â±ÄÈôêÊÄßÔºå‰∏îÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÊúâÊïàÊÄßÂèóÊï∞ÊçÆÈõÜÁöÑÂΩ±ÂìçÔºåÈúÄË¶ÅËøõË°åÁâπÂÆöÈ¢ÜÂüüÁöÑÈÄÇÂ∫î„ÄÇ\\n\\n**ÊñπÊ≥ïÊ¶ÇËø∞**\\nËÆ∫ÊñáÊèêÂá∫‰∫ÜRUNAÊ°ÜÊû∂ÔºåÂà©Áî®ÂèåÁºñÁ†ÅÂô®Êû∂ÊûÑÊçïËé∑‰∏∞ÂØå‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂπ∂ÈááÁî®Âå∫Âüü‰∏çÁ°ÆÂÆöÊÄßÂØπÈΩêÊú∫Âà∂ÊúâÊïàÂå∫ÂàÜÂàÜÂ∏ÉÂÜÖÔºàIDÔºâÂíåOODÂØπË±°ÔºåÂêåÊó∂ÂºïÂÖ•Â∞ëÊ†∑Êú¨ÂæÆË∞ÉÊñπÊ≥ïÊù•ÂØπÈΩêÂå∫ÂüüÁ∫ßËØ≠‰πâË°®Á§∫ÔºåÊèêÈ´òÊ®°ÂûãÂØπÁõ∏‰ººÂØπË±°ÁöÑÂà§Âà´ËÉΩÂäõ„ÄÇ\\n\\n**‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú**\\n- ÊèêÂá∫RUNAÊ°ÜÊû∂ÔºåÂú®ÁõÆÊ†áÁ∫ßOODÊ£ÄÊµã‰∏≠ÊòæËëóË∂ÖË∂äÁé∞ÊúâÊñπÊ≥ï„ÄÇÂú®BDD - 100KÊï∞ÊçÆÈõÜ‰∏äÔºåÈíàÂØπOpenImages OODÊï∞ÊçÆÈõÜÔºåRUNAÁöÑFPR95ËææÂà∞9.95%ÔºåÊØî‰πãÂâçË°®Áé∞ÊúÄ‰Ω≥ÁöÑSAFEÊñπÊ≥ïÈôç‰Ωé‰∫Ü6.09‰∏™ÁôæÂàÜÁÇπÔºõÂú®MSCOCO OODÊï∞ÊçÆÈõÜ‰∏äÔºåRUNAÁöÑFPR95‰∏∫16.85%ÔºåÊØîSAFEÊñπÊ≥ïÈôç‰Ωé‰∫Ü15.71‰∏™ÁôæÂàÜÁÇπ„ÄÇÂΩìVOC‰Ωú‰∏∫IDÊï∞ÊçÆÈõÜÔºåÂú®MSCOCO‰Ωú‰∏∫OODÊï∞ÊçÆÈõÜÊó∂ÔºåRUNAÁöÑFPR95‰∏∫30.67%ÔºåÊØîSAFEÊñπÊ≥ïÈôç‰Ωé‰∫Ü16.73‰∏™ÁôæÂàÜÁÇπÔºåÊØîSOTAÊñπÊ≥ïTIBÈôç‰Ωé‰∫Ü10.88‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n- ÂºÄÂèëÂ∞ëÊ†∑Êú¨ÂæÆË∞ÉÊñπÊ≥ïÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÂå∫ÂàÜIDÂíåOODÂØπË±°ÁöÑËÉΩÂäõ„ÄÇÂú®VOCÊï∞ÊçÆÈõÜ‰∏äÔºåRUNAÂú®OpenImages‰∏äÁöÑFPR95ÊØîÈõ∂Ê†∑Êú¨ÊñπÊ≥ïÈôç‰Ωé‰∫Ü12.90‰∏™ÁôæÂàÜÁÇπÔºåÂú®MSCOCO‰∏äÈôç‰Ωé‰∫Ü20.70‰∏™ÁôæÂàÜÁÇπÔºõÂú®BDD - 100KÊï∞ÊçÆÈõÜ‰∏äÔºåRUNAÂú®OpenImages‰∏äÁöÑFPR95ÊØîÈõ∂Ê†∑Êú¨ÊñπÊ≥ïÈôç‰Ωé‰∫Ü19.47‰∏™ÁôæÂàÜÁÇπÔºåÂú®MSCOCO‰∏äÈôç‰Ωé‰∫Ü20.11‰∏™ÁôæÂàÜÁÇπ„ÄÇ\",\n    \"algorithm_details\": \"### ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n**Ê†∏ÂøÉÊÄùÊÉ≥**\\nÂà©Áî®ÂèåÁºñÁ†ÅÂô®Êû∂ÊûÑÂàÜÂà´ÊçïÊçâÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÁâπÂæÅÔºåÈÄöËøáÂå∫Âüü‰∏çÁ°ÆÂÆöÊÄßÂØπÈΩêÊú∫Âà∂Â∞ÜÂØπË±°Âå∫ÂüüÁöÑÂàÜÂ∏É‰∏çÁ°ÆÂÆöÊÄßËΩ¨Êç¢‰∏∫Áõ∏ÂØπ‰∫éIDËØ≠‰πâÁ©∫Èó¥ÁöÑË∑ùÁ¶ªÂ∫¶ÈáèÔºåÁªìÂêàÂ∞ëÊ†∑Êú¨ÂæÆË∞É‰ΩøÊ®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£IDÁâπÂæÅÔºå‰ªéËÄåÊúâÊïàÂå∫ÂàÜIDÂíåOODÂØπË±°„ÄÇËØ•ÊñπÊ≥ïÊúâÊïàÊòØÂõ†‰∏∫ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÁâπÂæÅÁöÑËûçÂêàËÉΩÊèê‰æõÊõ¥‰∏∞ÂØåÁöÑ‰ø°ÊÅØÔºå‰∏çÁ°ÆÂÆöÊÄßÂØπÈΩêÊú∫Âà∂ÂèØÂáÜÁ°ÆÈáèÂåñÂØπË±°‰∏éIDÁ©∫Èó¥ÁöÑÂ∑ÆÂºÇÔºåÂ∞ëÊ†∑Êú¨ÂæÆË∞ÉËÉΩËÆ©Ê®°ÂûãÂø´ÈÄüÈÄÇÂ∫îIDÊï∞ÊçÆ„ÄÇ\\n\\n**ÂàõÊñ∞ÁÇπ**\\nÂÖàÂâçÁöÑÊñπÊ≥ïÂ≠òÂú®Â±ÄÈôêÊÄßÔºå‰º†ÁªüOODÊ£ÄÊµãÊñπÊ≥ïÂ§ßÂ§ö‰æùËµñÊ®°Âûã‰∏çÁ°ÆÂÆöÊÄßÔºå‰∏îÂ§öÊï∞È¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏∫ÂõæÂÉèÁ∫ßË°®Á§∫ÔºåÈöæ‰ª•Áõ¥Êé•Â∫îÁî®‰∫éÂØπË±°Á∫ßOODÊ£ÄÊµã„ÄÇÂ∞ÜÂü∫‰∫éÂõæÂÉèÁ∫ßCLIPÁöÑOODÊ£ÄÊµãÊñπÊ≥ïÂ∫îÁî®‰∫éÁõÆÊ†áÁ∫ßÂú∫ÊôØÊó∂Ôºå‰ºöÂõ†ÂÆö‰ΩçËøáÁ®ã‰∏¢Â§±‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÈöæ‰ª•ÂáÜÁ°ÆËØÑ‰º∞ÂØπË±°ÂºÇÂ∏∏„ÄÇÂêåÊó∂ÔºåÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÊúâÊïàÊÄßÂèóÊï∞ÊçÆÈõÜÁöÑÂΩ±ÂìçÔºåÈúÄË¶ÅËøõË°åÁâπÂÆöÈ¢ÜÂüüÁöÑÈÄÇÂ∫î„ÄÇRUNAÈááÁî®ÂèåÁºñÁ†ÅÂô®Êû∂ÊûÑÂíåÂå∫Âüü‰∏çÁ°ÆÂÆöÊÄßÂØπÈΩêÊú∫Âà∂ÔºåÁªìÂêàÂ∞ëÊ†∑Êú¨ÂæÆË∞ÉÔºåËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÂ±ÄÈÉ®ÁâπÂæÅ‰∏¢Â§±Âíå‰∏ä‰∏ãÊñá‰ø°ÊÅØ‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºå‰∏îÊó†ÈúÄÊâãÂä®ÈÄâÊã©ÁâπÂæÅÔºåÊõ¥ÂÖ∑ÁÅµÊ¥ªÊÄßÂíåÂπøÊ≥õÈÄÇÁî®ÊÄß„ÄÇÊ≠§Â§ñÔºåRUNA‰∏ìÊ≥®‰∫éËßÜËßâÊèêÁ§∫ÔºåÁªìÂêàÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÔºåÊèêÈ´ò‰∫ÜÂæÆË∞ÉÊïàÁéá„ÄÇ\\n\\n**ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§**\\n1. ÁâπÂæÅÊèêÂèñÔºö‰ΩøÁî®ÂèåÂõæÂÉèÁºñÁ†ÅÂô®ÁªìÊûÑ$\\mathcal { T } ^ { ( g ) }$Âíå$\\mathcal { T } ^ { ( r ) }$ÂàÜÂà´ÊèêÂèñÂÖ®Â±ÄÂíåÂå∫ÂüüÁâπÂæÅ„ÄÇÂØπÂÖ®Â±ÄÁâπÂæÅÊèêÂèñÂ∫îÁî®ËæπÁïåÊ°ÜÊ®°Á≥ä‰ª•‰øùÁïô‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÂØπÂå∫ÂüüÁâπÂæÅÊèêÂèñËøõË°åË£ÅÂâ™Êìç‰Ωú„ÄÇ\\n2. ÁâπÂæÅËûçÂêàÔºöÈááÁî®ËûçÂêàÁ≠ñÁï•$\\mathcal { T } _ { \\mathrm { t } } ( x, \\hat { x } _ { b } ) = \\mathcal { T } _ { P } ( \\lambda \\cdot \\mathcal { T } ^ { ( r ) } ( \\hat { x } _ { b } ) \\oplus ( 1 - \\lambda ) \\cdot \\mathcal { T } ^ { ( g ) } ( x ) )$Â∞ÜÂå∫ÂüüÂíåÂÖ®Â±ÄÁâπÂæÅËûçÂêàÔºåÂÖ∂‰∏≠$\\oplus$Ë°®Á§∫ÈÄêÂÖÉÁ¥†Áõ∏Âä†Ôºå$\\lambda$ÊòØË∞ÉËäÇÊØè‰∏™ÁºñÁ†ÅÂô®ÂΩ±ÂìçÁöÑÂä†ÊùÉÁ≥ªÊï∞„ÄÇ\\n3. ‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°ÔºöÂ∞ÜÂØπË±°Âå∫ÂüüÁöÑÂàÜÂ∏É‰∏çÁ°ÆÂÆöÊÄßËΩ¨Êç¢‰∏∫Áõ∏ÂØπ‰∫éIDËØ≠‰πâÁ©∫Èó¥ÁöÑË∑ùÁ¶ªÂ∫¶ÈáèÔºåÊúÄÂàùÂ∞ùËØïÁõ¥Êé•Ê±ÇÂíåÁõ∏‰ººÂ∫¶Ôºå‰ΩÜÊïàÊûú‰∏ç‰Ω≥ÔºåÊúÄÁªàÂÆö‰πâ‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°ÊåáÊ†á‰∏∫$\\mathbb { E } [ \\sigma ( \\boldsymbol { x }, b ) \\mid \\mathcal { Y } _ { \\mathrm { i n } } ] = - \\operatorname* { m a x } _ { 1 \\leq i \\leq K } \\mathrm { S i m } ( \\mathcal { T } _ { \\mathrm { t } } ( \\boldsymbol { x }, \\boldsymbol { \\hat { x } } _ { b } ), \\mathcal { T } ( t _ { i } ) )$ÔºåÂÖ∂‰∏≠$K$ÊòØÊ†áÁ≠æÊï∞Èáè„ÄÇ\\n4. Â∞ëÊ†∑Êú¨ÂæÆË∞ÉÔºöÈöèÊú∫ÈÄâÊã©Â∞ëÈáèÂõæÂÉèÔºåÂ∞ÜÊâÄÊúâÁúüÂÆûËæπÁïåÊ°Ü‰Ωú‰∏∫ÊΩúÂú®ÂæÆË∞ÉÂÄôÈÄâÔºå‰ªé‰∏≠ÈöèÊú∫ÊäΩÂèñ$N$‰∏™$K$Á±ªÂØπË±°ÁöÑÊ†∑Êú¨$\\{ ( \\hat { x } _ { b } ^ { i }, y _ { i } ) \\} _ { i = 1 } ^ { N K }$ÔºåÂØπÂõæÂÉèÁºñÁ†ÅÂô®ÂêéÁöÑÊäïÂΩ±Â±ÇËøõË°åÂæÆË∞ÉÔºå‰ΩøÁî®ÂØπÊØîÊçüÂ§±$\\mathcal { L } _ { \\mathrm { I D } } = - \\sum _ { \\hat { x } _ { b } \\in \\mathcal { B } } \\log \\frac { \\exp ( \\mathrm { S i m } ( \\mathcal { T } ( \\hat { x } _ { b } ), \\mathcal { T } ( t _ { i } ) ) / \\tau ) } { \\sum _ { j = 1 } ^ { K } \\exp ( \\mathrm { S i m } ( \\mathcal { T } ( \\hat { x } _ { b } ), \\mathcal { T } ( t _ { j } ) ) / \\tau ) }$Êù•‰ºòÂåñÊ®°ÂûãÂú®IDÁ©∫Èó¥ÁöÑÂà§Âà´ËÉΩÂäõÔºå$\\tau$ÊòØÁº©Êîæ‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÁöÑÊ∏©Â∫¶ÂèÇÊï∞„ÄÇ\\n\\n**Ê°à‰æãËß£Êûê**\\nËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ\",\n    \"comparative_analysis\": \"### ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n**Âü∫Á∫øÊ®°Âûã**\\nÂõæÂÉèÁ∫ßÊñπÊ≥ïÂåÖÊã¨MSP„ÄÅODIN„ÄÅMahalanobis„ÄÅGeneralized ODIN„ÄÅCSI„ÄÅGram matrices„ÄÅEnergy score„ÄÅCLIP - based MCMÁ≠âÔºõÁõÆÊ†áÁ∫ßOODÊ£ÄÊµãÊñπÊ≥ïÂåÖÊã¨VOS„ÄÅSIREN„ÄÅSAFE„ÄÅTIB„ÄÅPCA - based methodÁ≠â„ÄÇ\\n\\n**ÊÄßËÉΩÂØπÊØî**\\n*   **Âú® [FPR95] ÊåáÊ†á‰∏äÔºö** Âú®BDD - 100K‰Ωú‰∏∫IDÊï∞ÊçÆÈõÜÔºåOpenImages‰Ωú‰∏∫OODÊï∞ÊçÆÈõÜÊó∂ÔºåRUNAÁöÑFPR95‰∏∫9.95%ÔºåÊòæËëó‰ºò‰∫é‰πãÂâçË°®Áé∞ÊúÄ‰Ω≥ÁöÑSAFEÊñπÊ≥ïÔºà16.04%ÔºâÔºåÈôç‰Ωé‰∫Ü6.09‰∏™ÁôæÂàÜÁÇπÔºõÂú®MSCOCO‰Ωú‰∏∫OODÊï∞ÊçÆÈõÜÊó∂ÔºåRUNAÁöÑFPR95‰∏∫16.85%Ôºå‰ºò‰∫éSAFEÊñπÊ≥ïÔºà32.56%ÔºâÔºåÈôç‰Ωé‰∫Ü15.71‰∏™ÁôæÂàÜÁÇπ„ÄÇÂΩìVOC‰Ωú‰∏∫IDÊï∞ÊçÆÈõÜÔºåOpenImages‰Ωú‰∏∫OODÊï∞ÊçÆÈõÜÊó∂ÔºåSAFEÁöÑFPR95‰Ωé‰∫éRUNAÔºå‰ΩÜÂú®MSCOCO‰Ωú‰∏∫OODÊï∞ÊçÆÈõÜÊó∂ÔºåRUNAÁöÑFPR95‰∏∫30.67%Ôºå‰ºò‰∫éSAFEÊñπÊ≥ïÔºà47.40%ÔºâÔºåÈôç‰Ωé‰∫Ü16.73‰∏™ÁôæÂàÜÁÇπÔºå‰πü‰ºò‰∫éSOTAÊñπÊ≥ïTIBÔºà41.55%ÔºâÔºåÈôç‰Ωé‰∫Ü10.88‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n*   **Âú® [AUROC] ÊåáÊ†á‰∏äÔºö** Âú®BDD - 100K‰Ωú‰∏∫IDÊï∞ÊçÆÈõÜÔºåOpenImages‰Ωú‰∏∫OODÊï∞ÊçÆÈõÜÊó∂ÔºåRUNAÁöÑAUROC‰∏∫96.76%ÔºåÈ´ò‰∫éÂÖ∂‰ªñÂü∫Á∫øÊ®°ÂûãÔºõÂú®MSCOCO‰Ωú‰∏∫OODÊï∞ÊçÆÈõÜÊó∂ÔºåRUNAÁöÑAUROC‰∏∫93.92%Ôºå‰πüË°®Áé∞Âá∫Ëâ≤„ÄÇÂΩìVOC‰Ωú‰∏∫IDÊï∞ÊçÆÈõÜÔºåOpenImages‰Ωú‰∏∫OODÊï∞ÊçÆÈõÜÊó∂ÔºåRUNAÁöÑAUROC‰∏∫93.63%ÔºõÂú®MSCOCO‰Ωú‰∏∫OODÊï∞ÊçÆÈõÜÊó∂ÔºåRUNAÁöÑAUROC‰∏∫92.48%ÔºåÂùá‰ºò‰∫éÂ§öÊï∞Âü∫Á∫øÊ®°Âûã„ÄÇ\",\n    \"keywords\": \"### ÂÖ≥ÈîÆËØç\\n\\n- ÁõÆÊ†áÁ∫ßÂàÜÂ∏ÉÂ§ñÊ£ÄÊµã (Object-level Out-of-Distribution Detection, N/A)\\n- Âå∫Âüü‰∏çÁ°ÆÂÆöÊÄßÂØπÈΩê (Regional Uncertainty Alignment, RUNA)\\n- ÂèåÁºñÁ†ÅÂô®Êû∂ÊûÑ (Dual Encoder Architecture, N/A)\\n- Â∞ëÊ†∑Êú¨ÂæÆË∞É (Few-shot Fine-tuning, N/A)\\n- ËßÜËßâËØ≠Ë®ÄÊ®°Âûã (Vision-language Model, VL Model)\"\n}"
}