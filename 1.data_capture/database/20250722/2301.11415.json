{
    "source": "Semantic Scholar",
    "arxiv_id": "2301.11415",
    "link": "https://arxiv.org/abs/2301.11415",
    "pdf_link": "https://arxiv.org/pdf/2301.11415.pdf",
    "title": "Approximate Bilevel Difference Convex Programming for Bayesian Risk Markov Decision Processes",
    "authors": [
        "Yifan Lin",
        "Enlu Zhou"
    ],
    "categories": [
        "Systems and Control (eess.SY)"
    ],
    "publication_date": "2023-01-26",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Engineering"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Georgia Institute of Technology"
    ],
    "paper_content": "# Approximate Bilevel Difference Convex Programming for Bayesian Risk Markov Decision Processes\n\nYifan Lin, Enlu Zhou\n\nIndustrial and Systems Engineering Georgia Institute of Technology 755 Ferst Dr Atlanta, GA 30332 USA ylin429@gatech.edu, enlu.zhou $@$ isye.gatech.edu\n\n# Abstract\n\nWe consider infinite-horizon Markov Decision Processes where parameters, such as transition probabilities, are unknown and estimated from data. The popular distributionally robust approach to addressing the parameter uncertainty can sometimes be overly conservative. In this paper, we utilize the recently proposed formulation, Bayesian risk Markov Decision Process (BR-MDP), to address parameter (or epistemic) uncertainty in MDPs. To solve the infinite-horizon BR-MDP with a class of convex risk measures, we propose a computationally efficient approach called approximate bilevel difference convex programming (ABDCP). The optimization is performed offline and produces the optimal policy that is represented as a finite state controller with desirable performance guarantees. We also demonstrate the empirical performance of the BR-MDP formulation and the proposed algorithm.\n\n# 1 Introduction\n\nIn a Markov decision process (MDP), an agent must make decisions in a sequence while facing uncertainty. In this situation, some parameters of the MDP, such as the transition probabilities and costs, may be unknown and must be estimated from available data. The problem then becomes how to determine the best course of action, given the limited or possibly absent data, in order to minimize the expected total cost and optimize the decision-making process under these uncertain parameters.\n\nAn alternative approach to addressing the epistemic uncertainty in MDP is through the use of distributionally robust MDPs (DR-MDP, Xu and Mannor (2010)). It considers unknown parameters as random variables and assumes that their distributions belong to an ambiguity set determined by the available data. The optimal policy is then found by minimizing the expected total cost using the most adversarial distribution within this ambiguity set. However, these distributionally robust approaches may lead to overly conservative solutions that do not perform well in scenarios that are more likely to occur than the worst case. Additionally, the DR-MDP framework does not explicitly incorporate the dynamics of the problem, as the distribution of the unknown parameters does not depend on the data process, and is therefore not time-consistent, as noted by Shapiro (2021). In light of these limitations, Lin, Ren, and Zhou (2022) propose a Bayesian risk MDP (BR-MDP) framework to address epistemic uncertainty in MDPs. However, the approximation algorithm proposed by Lin, Ren, and Zhou (2022) only applies to finite-horizon MDPs and does not scale well with long horizon. It only provides an upper bound on the exact value function, without any theoretical guarantee on the gap.\n\nWe reformulate the considered BR-MDP as a bilevel difference convex program (DCP) such that we can employ the powerful optimization methods for DCP to solve infinitehorizon BR-MDP. Since the space of posterior distributions (beliefs) is uncountably infinite, we approximate the bilevel DCP by considering only a subset of posterior distributions. Although the DCP is approximate, we show that its solution is a lower bound on the exact optimal value function. Using the representation of a finite state controller of the resulting policy, we further show an upper bound on the exact optimal value function. We develop an iterative approach to reduce the gap between upper and lower bounds by incrementally generating new sets of posterior distributions, and show the convergence of the proposed algorithm.\n\nTo summarize, the contributions of this paper are twofold. First, we analyze the infinite-horizon MDP with epistemic uncertainty under BR-MDP via a Bayesian perspective and show the existence and uniqueness of stationary optimal policy. Second, we propose an approximate difference convex programming algorithm to solve the proposed formulation and show the convergence of the proposed algorithm. The rest of the paper is organized as follows. We conduct literature review and introduce the BR-MDP framework in Section 2. We show the existence and uniqueness of a stationary optimal policy to the infinite-horizon BR-MDP in Section 3.1. We provide a bilevel DCP solution to the infinite-horizon BR-MDP in Section 3.2. A computationally efficient approximate DCP algorithm is shown in Section 3.3. We verify the theoretical results and demonstrate the performance of our algorithms via numerical experiments in Section 4. Finally, we conclude the paper in Section 5.\n\n# 2 Background\n\n# 2.1 Related Literature\n\nIf data used to estimate the true but unknown underlying MDP are not sufficient, the estimated MDP may significantly differ from the true MDP, leading to poor policy performance. This discrepancy (between the estimated MDP and the true MDP) can be seen tightly linked to the epistemic uncertainty about the model. There have been numerous approaches that address epistemic uncertainty in MDPs, with robust MDP and its variants (Nilim and Ghaoui (2004); Iyengar (2005); Delage and Mannor (2010); Wiesemann, Kuhn, and Rustem (2013); Petrik and Russel (2019); Zhou et al. (2021); Yang, Zhang, and Zhang (2022); Cousins et al. (2023); Derman et al. (2020)) being one of the most widely used methods. In robust MDPs, the optimal decisions are made based on their performance under the most unfavorable conditions within a known ambiguity set of possible parameter values.\n\nApart from the overly conservative robust MDP approach which only considers the worst-case scenario, the riskaverse approach has been proposed to address the epistemic uncertainty, but with more flexibility in choosing the risk functional. Risk-averse approach is originally proposed to address the aleatoric uncertainty due to the inherent stochasticity of the underlying MDP (Howard and Matheson (1972); RuszczynÂ´ski (2010); Petrik and Subramanian (2012); Osogami (2012)). It replaces the risk-neutral expectation by some general risk measures, such as conditional value-at-risk (CVaR, see Rockafellar and Uryasev (2000)). However, most of the existing approaches assume the agent has access to the true underlying MDP, and optimize some risk measures such as CVaR in that single MDP (Chow and Ghavamzadeh (2014); Tamar et al. (2015); Tamar, Glassner, and Mannor (2015); Sharma et al. (2019)). In this paper, we consider the offline planning problem in MDPs, where we only have access to prior belief distribution over MDPs that is constructed by the offline data. It should be noted that offline planning problem has also been considered by Duff (2002), where the author proposes a Bayes-adaptive MDP (BA-MDP) formulation with an augmented state composed of the underlying MDP state and the posterior distribution of the unknown parameters. Mostly close to the problem setting in this work are Rigter, Lacerda, and Hawes (2021); Lin, Ren, and Zhou (2022). Rigter, Lacerda, and Hawes (2021) optimize a CVaR risk functional over the total cost and simultaneously addresses both epistemic and aleatoric uncertainty, while Lin, Ren, and Zhou (2022) consider a nested risk functional to ensure the time consistency of the obtained policy.\n\nWhile there are many works proposing different models and frameworks to address the epistemic uncertainty, developing computationally efficient solutions is also of great interest. In robust MDPs, with some mild conditions on the ambiguity set such as rectangularity, the proposed formulation can be solved by a second-order cone program when the horizon is finite, or policy iteration when the horizon is infinite (Mannor and Xu (2019)). In BA-MDP and its variants, Rigter, Lacerda, and Hawes (2021) propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimization. Lin, Ren, and Zhou (2022) develop an $\\alpha$ -function approximation algorithm using the convexity of the CVaR risk measure. However, the aforementioned works consider a finite-horizon MDP and do not generalize well to the infinite-horizon setting.\n\nCompared to standard MDPs, our considered problem has two distinct features that make it difficult to apply value iteration, policy iteration, or linear programming (Puterman (2014)). First is the resulting continuous-state MDP due to augmented belief state. We note that this continuous-state MDP is similar to a belief-MDP, which is the equivalent way to represent a partially observable MDP (POMDP) by treating the posterior distribution of the hidden state as a belief state. Second is the risk measure taken with respect to the unknown parameters in the MDPs. In this work, we propose an optimization-based method to solve the infinite-horizon BR-MDPs. It has been empirically shown by Alagoz, Ayvaci, and Linderoth (2015) that linear programming can efficiently solve a significant number of MDPs in comparison to standard dynamic programming methods, such as value iteration and policy iteration. Furthermore, linear programming requires less memory and can handle MDPs with a larger number of states and still achieve optimality. It has been widely used in risk-sensitive MDP (that deals with intrinsic or aleatoric uncertainty that is due to the inherent stochasticity of the underlying MDP, see Zhang et al. (2021)). Works that are most related to our proposed optimization-based approach include Poupart et al. (2015) who propose an approximate linear programming algorithm for the risk-neutral constrained POMDPs, and Ahmadi et al. (2021) who propose a difference convex program (DCP) for the constrained riskaverse MDPs. Our approach for infinite-horizon BR-MDP significantly differs from the above approaches in two aspects. First, compared to the linear programming approach in Poupart et al. (2015), we use bilevel DCP, due to the additional risk measure used for mitigating the epistemic uncertainty. Our considered risk measure brings additional challenge to exactly evaluating the policy, whereas policy evaluation can be easily solved by a system of linear equations in Poupart et al. (2015). Second, compared to the DCP for the risk-averse MDP with aleatoric uncertainty in Ahmadi et al. (2021), the resulting continuous-state MDP in our problem has an infinite number of constraints and requires appropriate approximation to make the problem computationally feasible.\n\n# 2.2 Preliminary: Bayesian Risk MDPs\n\nConsider an infinite-horizon MDP that is defined as $( S , { \\mathcal { A } } , P , C , \\gamma )$ , where $s$ is the state space, $\\mathcal { A }$ is the action space, $P$ is the transition probability with $P ( s ^ { \\prime } | s , a )$ denoting the probability of transitioning to state $s ^ { \\prime }$ from state $s$ when action $a$ is taken, $C$ is the cost function with $C ( s , a , s ^ { \\prime } )$ denoting the cost when action $a$ is taken and state transitions from $s$ to $s ^ { \\prime }$ , $0 \\leq \\gamma < 1$ is the discount factor. We assume the state space and action space are finite and the cost is bounded. A Markovian deterministic policy $\\pi$ is a function mapping from $s$ to $\\mathcal { A }$ . Given an initial state $s$ , the goal is to find an optimal policy that minimizes the expected discounted total cost: $\\begin{array} { r } { \\underset { \\pi } { \\dot { \\operatorname* { m i n } } } \\mathbb { \\mathbb { E } } ^ { \\tilde { \\pi } , P , C } \\left[ \\sum _ { t = 1 } ^ { \\infty } \\gamma ^ { t - 1 } C \\left( s _ { t } , \\dot { a } _ { t } , s _ { t + 1 } \\right) | s _ { 1 } = s \\right] } \\end{array}$ where $\\mathbb { E } ^ { \\pi , P , \\overset { n } { C } }$ is the expectation with policy $\\pi$ when the transition probability is $P$ and the cost is $C$ . In practice, $P$ and\n\n$C$ are often unknown and estimated from data.\n\nBR-MDP is a recently proposed framework that deals with the epistemic uncertainty in MDPs (see Lin, Ren, and Zhou (2022)). It is assumed that the state transition is specified by the state equation $\\begin{array} { r } { s ^ { \\prime } = g ( s , a , \\xi ) } \\end{array}$ with a known transition function $g$ which involves state $s \\in \\mathcal { S } \\subseteq \\mathbb { R } ^ { k _ { s } }$ , action $a \\ \\in \\mathcal { A } \\ \\subseteq \\ \\bar { \\mathbb { R } } ^ { k _ { a } }$ , and randomness $\\xi \\in \\Xi \\subseteq \\mathbb { R } ^ { k _ { \\xi } }$ , where $k _ { s }$ , $k _ { a }$ , $k _ { \\xi }$ are the dimensions of the state, action, and randomness space, respectively. The state equation together with the distribution of $\\xi$ uniquely determines the transition probability of the MDP, i.e., ${ \\cal P } ( \\dot { s } ^ { \\prime } \\in S ^ { \\prime } | s , a ) = { \\cal P } ( \\{ \\xi \\in$ $\\bar { \\Xi } : g ( s , \\dot { a } , \\xi ) \\in S ^ { \\prime } \\} | s , a )$ , where $S ^ { \\prime }$ is a measurable set in $s$ . We refer the readers to Chapter 3.5 in Puterman (2014) for the equivalence between stochastic optimal control and MDP formulation. We use the representation of state equations instead of transition probabilities in MDPs, for the purpose of decoupling the randomness and the policy, leading to a cleaner formulation in the nested form. The cost is assumed to be a function of state $s$ , action $a$ , and randomness $\\xi$ , i.e., $C ( s , a , \\xi )$ .\n\nThe distribution of $\\xi$ , denoted by $f ( \\cdot ; \\theta ^ { c } )$ , is assumed to belong to a parametric family $\\{ f ( \\cdot ; \\theta ) | \\theta \\in \\Theta \\}$ , where $\\Theta \\subseteq \\mathbb { R } ^ { \\tilde { d } }$ is a convex parameter space, $d$ is the dimension of the parameter space $\\Theta$ , and $\\theta ^ { c } \\in \\Theta$ is the true but unknown parameter value. Many real-world problems exhibit the characteristic of relying on a parametric assumption. For example, it is commonly assumed that the demand of customers follows a Poisson distribution with an unknown arrival rate in inventory control. We begin by assuming a prior distribution, denoted by $\\mu$ , over the parameter space $\\Theta$ . This prior accounts for the uncertainty of the parameter estimate that comes from an initial set of data, and it can also take expert opinions into consideration. Then, given an observed realization of the data process, we update the posterior distribution $\\mu$ according to the Bayesâ rule. Let the policy be a sequence of mappings from state $s$ and posterior $\\mu$ to the action space, i.e., $\\pi = \\{ \\pi : { \\mathcal { S } } \\times { \\mathcal { M } }  { \\mathcal { A } } \\}$ , where $\\mathcal { M }$ is the space of posterior distributions. This representation implies the policy is stationary. Now we present the BR-MDP formulation below.\n\n$$\n\\begin{array} { r l } & { \\underset { \\pi } { \\mathrm { m i n } }  \\rho _ { \\mu _ { 1 } } \\mathbb { E } _ { \\theta _ { 1 } } \\Bigl [ C _ { 1 } ( s _ { 1 } , a _ { 1 } , \\xi _ { 1 } ) + \\cdot \\cdot \\cdot + \\gamma ^ { t - 1 } \\rho _ { \\mu _ { t } } \\mathbb { E } _ { \\theta _ { t } } \\Bigl [ } \\\\ & { \\quad \\quad \\quad \\quad C _ { t } \\bigl ( s _ { t } , a _ { t } , \\xi _ { t } \\bigr ) + \\cdot \\cdot \\cdot \\Bigr ] | s _ { 1 } = s , \\mu _ { 1 } = \\mu \\Bigr ] } \\\\ & { \\mathrm { s . t . } \\quad \\quad s _ { t + 1 } = g ( s _ { t } , a _ { t } , \\xi _ { t } ) , } \\\\ & { \\quad \\quad \\quad \\quad a _ { t } = \\pi ( s _ { t } , \\mu _ { t } ) , } \\\\ & { \\quad \\quad \\quad \\quad \\mu _ { t + 1 } ( \\theta ) = \\frac { \\mu _ { t } ( \\theta ) f ( \\xi _ { t } ; \\theta ) } { \\int _ { \\Theta } \\mu _ { t } ( \\theta ) f ( \\xi _ { t } ; \\theta ) d \\theta } , } \\end{array}\n$$\n\nwhere $\\rho$ is a risk measure (we defer the definition and form of the risk measure $\\rho$ to Section 2.3), $\\theta _ { t }$ is a random vector following distribution $\\mu _ { t }$ , $\\mathbb { E } _ { \\theta _ { t } }$ denotes the expectation with respect to $\\xi _ { t } \\sim f ( \\cdot ; \\theta _ { t } )$ conditional on $\\theta _ { t }$ , and $\\rho _ { \\mu _ { t } }$ denotes a risk functional with respect to $\\theta _ { t } \\sim \\mu _ { t }$ applied in nested form to the expected total cost with respect to the Bayesian posterior distributions of the unknown parameters. Equation (2) is the transition of the state $s _ { t }$ , and without loss of generality we assume the initial state $s _ { 1 }$ takes a deterministic value $s$ . Equation (3) is the updating of the posterior $\\mu _ { t }$ . For a given dataset with size $N$ , the prior distribution converges to a Dirac delta function concentrated on the true parameter $\\theta ^ { c }$ with probability 1, and the optimal value function of BR-MDP converges to the optimal value function of the true MDP.\n\n# 2.3 Preliminary: Risk Measure\n\nLet $( \\Omega , { \\mathcal { F } } , \\mathbb { P } )$ be a probability space and $\\mathcal { Z }$ be a linear space of $\\mathcal { F }$ -measurable functions $Z : \\Omega \\to \\mathbb { R }$ . A risk measure is a function $\\rho : \\mathcal { Z }  \\mathbb { R }$ which assigns a random variable $Z$ to a real number representing its risk. It is said that risk measure $\\rho$ is convex if it possesses the properties of convexity, monotonicity, and translation invariance (see FoÂ¨llmer and Schied (2002)). In this paper we consider a class of convex risk measures which can be represented in the following parametric form: $\\begin{array} { r } { \\rho _ { \\mu } ( Z ) : = \\bar { \\operatorname* { i n f } } _ { \\phi \\in \\Phi } \\mathbb { E } _ { \\mu } [ \\Psi ( Z , \\phi ) ] } \\end{array}$ , where $\\Phi \\subset \\mathbb { R } ^ { m }$ and $\\Psi : \\mathbb { R } \\times \\Phi  \\mathbb { R }$ is a real-valued convex function, and $\\Psi ( \\cdot , \\phi )$ is finite-valued and continuous on a compact set of $\\phi$ . There is a large class of risk measures which can be represented in the parametric form. For example, conditional value-at-risk (CVaR), defined as $\\begin{array} { r } { \\mathrm { C V a R } _ { \\alpha } ( \\bar { X } ) \\ = \\ \\operatorname* { m i n } _ { \\phi \\in \\mathbb { R } } \\Big \\{ \\phi + \\frac { 1 } { 1 - \\alpha } \\mathbb { E } \\left[ ( X - \\phi ) ^ { + } \\right] \\Big \\} } \\end{array}$ , where $( \\cdot ) ^ { + }$ stands for $\\operatorname* { m a x } ( 0 , \\cdot )$ , is widely used (see Rigter, Lacerda, and Hawes (2021); Chow et al. (2015)). Another example is risk measures constructed from $\\phi$ -divergence ambiguity sets (see Example 3 in Guigues, Shapiro, and Cheng (2024)). We refer the readers to Shapiro, Dentcheva, and Ruszczynski (2021) for a comprehensive discussion.\n\n# 3 Algorithm and Analysis\n\n# 3.1 Bellman Equation and Optimality\n\nWe can write the value function under policy $\\pi$ of BR-MDP in the following recursive forms.\n\n$$\n\\begin{array} { r l } & { V ^ { \\pi } ( s , \\mu ) = \\rho _ { \\mu } \\mathbb { E } _ { \\theta } \\bigl [ C ( s , a , \\xi ) + \\gamma V ^ { \\pi } ( s ^ { \\prime } , \\mu ^ { \\prime } ) \\bigr ] } \\\\ & { \\mathrm { s . t . } \\quad s ^ { \\prime } = g ( s , a , \\xi ) , a = \\pi ( s , \\mu ) ; } \\\\ & { \\quad \\quad \\mu ^ { \\prime } ( \\theta ) = \\frac { \\mu ( \\theta ) f \\left( \\xi ; \\theta \\right) } { \\int _ { \\Theta } \\mu ( \\theta ) f \\left( \\xi ; \\theta \\right) d \\theta } . } \\end{array}\n$$\n\nWe refer the readers to Lin, Ren, and Zhou (2022) for a discussion on the preference of dynamic risk measure over static risk measure in consideration of time consistency and derivation of the Bellman equation. For simplicity we only consider deterministic policies, but all the analysis below can be extended to stochastic policies. For the stochastic policies, the expectation in (1) is taken with respect to the randomness $\\xi$ and the action $a$ . As a consequence of Theorem 5.5.3b in Puterman (2014), it is sufficient to consider the Markovian policy. The optimal value function is then denoted as $V ^ { * } ( \\bar { s } , \\mu ) \\dot { } = \\operatorname* { m i n } _ { \\pi \\in \\Pi ^ { M D } } V ^ { \\pi } ( s , \\mu )$ , where $\\Pi ^ { M D }$ is the set of Markovian deterministic policies. It should be noted that the Bayes optimality is with respect to the prior belief $\\mu$ . In the following, we derive the intermediate results to show $V ^ { * }$ is the unique optimal value function to the infinite-horizon BR-MDP.\n\nDefinition 3.1 (Bellman Operator). Let $B ( \\boldsymbol { S } , \\mathcal { M } )$ be the space of real-valued bounded measurable functions on $( S \\times$ $\\mathcal { M } )$ . For any bounded value function $V \\in B ( S , { \\mathcal { M } } )$ , define an operator $\\mathcal T : B ( s , \\mu ) \\to B ( s , \\mu )$ as:\n\n$$\n( T V ) ( s , \\mu ) = \\operatorname* { m i n } _ { a \\in \\mathcal { A } } \\rho _ { \\mu } \\left[ \\mathbb { E } _ { \\theta } \\left[ C ( s , a , \\xi ) + \\gamma V ( s ^ { \\prime } , \\mu ^ { \\prime } ) \\right] \\right] .\n$$\n\nAlso let $\\tau ^ { \\pi } : B ( s , \\mu ) \\to B ( s , \\mu )$ , where\n\n$$\n( T ^ { \\pi } V ) ( s , \\mu ) = \\rho _ { \\mu } \\left[ \\mathbb { E } _ { \\theta } \\left[ C ( s , \\pi ( s , \\mu ) , \\xi ) + \\gamma V ^ { \\pi } ( s ^ { \\prime } , \\mu ^ { \\prime } ) \\right] \\right] .\n$$\n\nThe next two lemmas show the above Bellman operators are monotonic and contraction mappings.\n\nLemma 3.2 (Monotonicity). The operators $\\mathcal T ^ { \\pi }$ and $\\tau$ are monotonic, in the sense that $V \\leq V ^ { \\prime }$ implies $\\begin{array} { r } { T ^ { \\pi } V \\leq \\mathcal { T } ^ { \\pi } V ^ { \\prime } } \\end{array}$ and $\\mathcal { T } V \\leq \\mathcal { T } V ^ { \\prime }$ .\n\nLemma 3.3 (Contraction Mapping). The operators ${ \\mathcal { T } } ^ { \\pi }$ and $\\tau$ are $\\gamma$ contraction for $| | \\cdot | | _ { \\infty }$ norm. That is, for any two bounded value functions $V$ , $V ^ { \\prime } \\in B ( S , { \\mathcal { M } } )$ , we have\n\n$$\n\\begin{array} { r } { \\vert \\vert T ^ { \\pi } V - \\mathcal { T } ^ { \\pi } V ^ { \\prime } \\vert \\vert _ { \\infty } \\leq \\gamma \\vert \\vert V - V ^ { \\prime } \\vert \\vert _ { \\infty } . } \\end{array}\n$$\n\nThe following proposition shows that sub-solutions $V _ { \\mathrm { s u b } }$ and super-solutions $V _ { \\mathrm { s u p } }$ of the optimality equations $V =$ $\\pi _ { V }$ provide lower and upper bounds on $V ^ { * }$ . As a result, when a solution is obtained, both bounds are satisfied, meaning that the solution must be equivalent to $V ^ { * }$ . Additionally, this outcome serves as an important algorithmic tool for optimization-based methods.\n\nProposition 3.4. For any $V _ { \\mathrm { s u b } } , V _ { \\mathrm { s u p } } \\ \\in \\ B ( \\mathcal { S } , \\mathcal { M } ) ,$ , (i) if $V _ { \\mathrm { s u p } } ~ \\geq ~ \\tau V _ { \\mathrm { s u p } }$ , then $V _ { \\mathrm { s u p } } ~ \\geq ~ V ^ { * }$ ; $( i i )$ if $V _ { \\mathrm { s u b } } ~ \\leq ~ \\tau V _ { \\mathrm { s u b } }$ , then $V _ { \\mathrm { s u b } } \\leq V ^ { * }$ .\n\nAccording to Proposition 3.4, we have $V ^ { * } = \\tau V ^ { * }$ . By Banach fixed-point theorem, $V ^ { * }$ is the unique optimal value function to the infinite horizon BR-MDP. We also have that the value $V$ of a stationary policy $\\pi$ is the unique bounded solution of the equation $V = \\tau ^ { \\pi } V$ . Similar analysis shows the existence and uniqueness of the optimal stationary policy $\\pi ^ { * }$ that satisfies $V ^ { * } = { \\mathcal { T } } ^ { \\pi ^ { * } } V ^ { * }$ .\n\nApplying the operator $\\tau$ on any initial value function $V$ , we have the value iteration algorithm for the infinite-horizon BR-MDP problem. The following corollary of convergence rate is similar to the standard with the contraction property.\n\nCorollary 3.5. For any initial bounded value function $V$ , the convergence rate is shown to be $\\vert \\vert ( \\mathcal { T } ^ { k } V ) { \\bar { ( s , \\mu ) } } -$ $V ^ { * } ( s , \\mu ) | | _ { \\infty } \\overset {  } { \\leq } \\gamma ^ { k } | | V ( s , \\mu ) - V ^ { * } ( s , \\mu ) | | _ { \\infty }$ .\n\n# 3.2 Bilevel Difference Convex Programming\n\nThe main challenge of executing the value iteration algorithm (and similarly policy iteration algorithm) lies in the continuous augmented state. In this work, we propose an optimization-based method to solve the infinite-horizon BRMDPs. According to Proposition 3.4, the infinite-horizon BR-MDP can be solved as follows:\n\n$$\n\\begin{array} { r l } & { \\displaystyle \\underset { V } { \\operatorname* { m a x } } \\displaystyle \\sum _ { s \\in S , \\mu \\in \\mathcal { M } } \\alpha ( s , \\mu ) V ( s , \\mu ) } \\\\ & { \\mathrm { s . t . } V ( s , \\mu ) \\leq \\rho _ { \\mu } \\mathbb { E } _ { \\theta } \\Big [ C ( s , a , \\xi ) + \\gamma V ( s ^ { \\prime } , \\mu ^ { \\prime } ) \\Big ] , } \\\\ & { \\quad \\quad \\quad \\forall a \\in \\mathcal { A } , s \\in \\mathcal { S } , \\mu \\in \\mathcal { M } , } \\end{array}\n$$\n\nwhere we choose $\\alpha ( s , \\mu )$ to be positive scalars which satisfy $\\begin{array} { r } { \\sum _ { s \\in S , \\mu \\in \\mathcal { M } } \\alpha ( s , \\mu ) = \\mathrm { \\ i } } \\end{array}$ . For the considered class of convex risk measures, we can rewrite the above formulation as a bilevel difference convex program:\n\n$$\n\\begin{array} { l l } { \\displaystyle \\operatorname* { m i n } _ { V } } & { - \\displaystyle \\sum _ { s \\in S , \\mu \\in \\mathcal { M } } \\alpha ( s , \\mu ) V ( s , \\mu ) } & { ( 4 ) } \\\\ { \\displaystyle \\mathrm { s . t . } V ( s , \\mu ) - \\displaystyle \\operatorname* { m i n } _ { \\phi } \\mathbb { E } _ { \\mu } \\Big [ \\Psi \\big ( \\mathbb { E } _ { \\theta } [ C ( s , a , \\xi ) + \\gamma V ( s ^ { \\prime } , \\mu ^ { \\prime } ) ] , \\phi \\big ) \\Big ] } \\end{array}\n$$\n\n$$\n\\leq 0 , \\forall a \\in \\mathcal { A } , s \\in \\mathcal { S } , \\mu \\in \\mathcal { M } .\n$$\n\nSince $\\Psi ( Z , \\phi )$ is convex in $( Z , \\phi )$ and expectation is a linear operator, the minimum of $\\mathbb { E } [ \\Psi ( Z , \\phi ) ]$ over a convex set $\\Theta$ remains convex in $Z$ . Thus, (4) is a bilevel difference convex program (see Horst and Thoai (1999) for the definition of DCP). It should be noted that Ahmadi et al. (2021) show that the minimum over $\\phi$ can be absorbed into the overall minimum problem, and $\\phi$ is treated as a single variable. However, it is clear that the minimum is achieved at different $\\phi$ for different augmented state $( s , \\mu )$ , thus turning (4) into a bilevel optimization problem. When the lower-level problem is convex and satisfies certain regularity conditions, we can use the Karush-Kuhn-Tucker (KKT) conditions to reformulate the lower-level optimization problem, which allows us to transform the original bilevel optimization problem into a single-level (constrained) optimization problem.\n\nAfter being reduced to a single-level DCP problem, (4) can be solved by the convex-concave procedure (see Lipp and Boyd (2016) for such procedure), wherein the concave terms are replaced by a convex upper bound. We employ the method of disciplined convex-concave programming (DCCP, Shen et al. (2016)), which converts a DCP problem into a disciplined convex program and subsequently into an equivalent cone program. However, one problem remains to be solved: the number of constraints in (4) is infinite, due to the continuous belief state. To tackle this problem, we take a similar approach as Poupart et al. (2015). The main idea is to start with a finite posterior set (belief space) $\\hat { \\mathcal { M } }$ , and then problem (4) can be solved efficiently by DCCP, where the posterior distribution (belief point) not in the set $\\hat { \\mathcal { M } }$ is replaced by convex combination of the points in $\\hat { \\mathcal { M } }$ . We then iteratively add to the posterior set new posterior distributions that are reachable from the current set and re-solve (4). It should be noted that the proposed approach could be extended to value iteration and policy iteration, but the analysis would be more complicated, since there would be a trade-off between the optimization (e.g. value iteration) and the belief point generation, and it could be quite tricky to decide the optimal number of steps for value iteration and optimal intervals for belief point generation. We formally introduce the approximate bilevel DCP algorithm in the next section.\n\n# 3.3 Approximate Bilevel Difference Convex Programming\n\nLet $\\hat { \\mathcal { M } }$ be the current posterior set. Let $\\mu ^ { s a s ^ { \\prime } }$ be the onestep posterior distribution with observed randomness $\\xi$ indicated by state transition $\\begin{array} { r } { s ^ { \\prime } = g ( s , a , \\xi ) } \\end{array}$ and current posterior $\\mu$ . Initially the posterior set is constructed from corner (degenerate) points. In case the parameter space $\\Theta$ is finite, the corner points are $( 1 , 0 , \\cdots , 0 )$ , $( 0 , 1 , 0 ^ { \\overline { { , } } \\cdot \\cdot \\cdot } )$ , , and $( 0 , \\cdots , 0 , 1 )$ . In case the parameter space is continuous, it is impossible to express one-step posterior distribution (i.e., $\\mu ^ { s a s ^ { \\prime } } )$ as a convex combination of those degenerate points. Therefore, we assume the parameter space is finite, which is practical in many real-world problems. It can also be viewed as a discrete approximation of a continuous parameter set, and the discretization can be chosen of any precision.\n\n# Algorithm 1: Approximate Bilevel DCP\n\ninput: posterior set $\\hat { \\mathcal { M } }$ output: policy $\\hat { \\pi } ^ { * }$ , approximate value function $\\hat { V } ^ { * }$ 1. solve the following approximate bilevel DCP:\n\n$$\n\\begin{array} { l } { \\displaystyle \\underset { V } { \\operatorname* { m i n } } \\quad - \\sum _ { s \\in S , \\mu \\in \\hat { \\mathcal { A } } } \\alpha ( s , \\mu ) V ( s , \\mu ) } \\\\ { \\displaystyle \\mathrm { s . t . } V ( s , \\mu ) \\leq \\operatorname* { m i n } _ { \\phi } \\sum _ { \\theta \\in \\Theta } \\mu ( \\theta ) \\left[ \\Psi \\left( \\gamma \\sum _ { \\mu ^ { \\prime } \\in \\hat { \\mathcal { A } } , s ^ { \\prime } \\in S } P ( s ^ { \\prime } | s , a , \\theta ) \\right. \\right. } \\\\ { \\displaystyle \\left. w ( \\mu ^ { \\prime } , \\mu ^ { s a s ^ { \\prime } } ) V ( s ^ { \\prime } , \\mu ^ { \\prime } ) + C ( s , a , \\theta ) , \\phi ) \\right] , \\forall a \\in \\mathcal { A } , s \\in \\mathcal { S } , \\mu \\in \\hat { \\mathcal { M } } } \\end{array}\n$$\n\nwhere $w ( \\mu ^ { \\prime } , \\mu ^ { s a s ^ { \\prime } } )$ is obtained by solving (6).\n\n2. obtain the approximate solution $\\hat { V } ^ { * }$ to (5); obtain the approximate policy\n\n$$\n\\begin{array} { r l } & { \\hat { \\boldsymbol { \\pi } } ^ { * } ( s , \\mu ) = \\underset { \\phi , a \\in \\mathcal { A } } { \\arg \\operatorname* { m i n } } \\sum _ { \\theta \\in \\Theta } \\mu ( \\theta ) \\left[ \\Psi \\big ( \\gamma \\sum _ { \\mu ^ { \\prime } \\in \\hat { \\mathcal { M } } , s ^ { \\prime } \\in S } P ( s ^ { \\prime } | s , a , \\theta ) \\right. } \\\\ & { \\left. w ( \\mu ^ { \\prime } , \\mu ^ { s a s ^ { \\prime } } ) \\hat { V } ^ { * } ( s ^ { \\prime } , \\mu ^ { \\prime } ) + C ( s , a , \\theta ) , \\phi ) \\right] , \\forall s \\in \\mathcal { S } , \\mu \\in \\hat { \\mathcal { M } } . } \\end{array}\n$$\n\nTo interpolate all $\\mu ^ { s a s ^ { \\prime } }$ that can be reached from some $\\mu _ { i } \\in \\hat { \\mathcal { M } }$ in one step, we use some convex combination of points $\\mu _ { i }$ in $\\hat { \\mathcal { M } }$ . Let $\\overset { \\vartriangle } { \\boldsymbol { w } } ( \\mu _ { i } , \\mu ^ { s a s ^ { \\prime } } )$ be the weight $w _ { i }$ associated with $\\mu _ { i }$ when interpolating $\\mu ^ { s a s ^ { \\prime } }$ . We can use this interpolation weight to define an approximate transition probability for posterior as:\n\n$$\n\\tilde { P } ( \\mu ^ { \\prime } | s , a , \\mu , \\theta ) = \\sum _ { s ^ { \\prime } \\in S } P ( s ^ { \\prime } | s , a , \\theta ) w ( \\mu ^ { \\prime } , \\mu ^ { s a s ^ { \\prime } } ) .\n$$\n\nA sanity check that $\\tilde { P } ( \\mu ^ { \\prime } | s , a , \\mu , \\theta )$ is indeed a transition probability: $\\begin{array} { r l r } { \\sum _ { \\mu ^ { \\prime } \\in \\mathcal { \\hat { M } } } \\tilde { P } ( \\mu ^ { \\prime } | s , a , \\mu , \\theta ) } & { { } = } & { 1 } \\end{array}$ and $\\tilde { P } ( \\mu ^ { \\prime } | s , a , \\mu , \\theta ) \\ge 0$ . We choose the convex combination that minimizes the weighted Euclidean norm of the difference between $\\mu$ and each $\\mu _ { i }$ by solving the following linear program:\n\n$$\n\\begin{array} { c } { \\displaystyle \\operatorname* { m i n } _ { w } \\sum _ { i } w _ { i } \\| \\mu _ { i } - \\mu ^ { s a s ^ { \\prime } } \\| _ { 2 } ^ { 2 } } \\\\ { \\mathrm { s . t . ~ } \\displaystyle \\sum _ { i } w _ { i } \\mu _ { i } ( \\theta ) = \\mu ^ { s a s ^ { \\prime } } ( \\theta ) , \\forall \\theta \\in \\Theta } \\\\ { \\displaystyle \\sum _ { i } w _ { i } = 1 , w _ { i } \\geq 0 , \\forall i . } \\end{array}\n$$\n\nWith the approximation in the constraint in (4), we obtain the following approximate bilevel DCP Algorithm 1 for a given posterior set. For ease of notation, we denote by $C ( s , a , \\bar { \\theta ) } = \\mathbb { E } _ { \\theta } [ C ( s , a , \\xi ) ]$ the average cost at state $s$ when action $a$ is taken, under the parameter value $\\theta$ .\n\nTheorem 3.6. The approximate value function $\\hat { V } ^ { * }$ found by running Algorithm $\\jmath$ is a lower bound on the exact optimal value function $V ^ { * }$ .\n\nWe also develop an upper bound on the exact optimal value function, using the obtained policy from Algorithm 1. The obtained policy is a finite state controller (see Hansen (2013) for the definition of finite state controller). Let $\\mathcal { N }$ be the set of nodes in the controller such that we associate a node $n _ { s , \\mu }$ to each $( s , \\mu )$ pair. The action chosen in node $n _ { s , \\mu }$ is determined by the policy ${ \\hat { \\pi } } ^ { * } ( a | s , \\mu )$ . For a given parameter $\\theta$ , the transition probability to the next node is $P ( n _ { s ^ { \\prime } , \\mu ^ { \\prime } } | n _ { s , \\mu } , a ) = w ( \\mu ^ { \\prime } , \\mu ^ { s a s ^ { \\prime } } ) P ( s ^ { \\prime } | s , a )$ . The value function of the finite state controller can be computed by\n\n$$\n\\begin{array} { r } { \\hat { V } ^ { \\hat { \\pi } ^ { * } } ( n _ { s , \\mu } ) = \\displaystyle \\operatorname* { m i n } _ { \\phi } \\sum _ { \\theta \\in \\Theta } \\mu ( \\theta ) \\Big [ \\Psi \\big ( c ( s , a , \\theta ) + \\gamma \\sum _ { n _ { s ^ { \\prime } , \\mu ^ { \\prime } } \\in \\mathcal { N } } } \\\\ { w ( \\mu ^ { \\prime } , \\mu ^ { s a s ^ { \\prime } } ) P ( s ^ { \\prime } | s , a , \\theta ) \\hat { V } ^ { \\hat { \\pi } ^ { * } } ( n _ { s ^ { \\prime } , \\mu ^ { \\prime } } ) , \\phi \\big ) \\Big ] . } \\end{array}\n$$\n\nSimilar to Ahmadi et al. (2021), the value function can be solved efficiently by DCP. It is also known from Hansen (2013) that the value function obtained by the finite state controller $\\hat { V } ^ { \\hat { \\pi } ^ { * } }$ serves as an upper bound for the optimal value function.\n\nNote that the inequality $\\hat { V } ^ { * } \\leq V ^ { * } \\leq \\hat { V } ^ { \\hat { \\pi } ^ { * } }$ provides information about how well the optimal value function $V ^ { * }$ is approximated. As the posterior set $\\hat { \\mathcal { M } }$ contains more beliefs to accurately evaluate the policies, the gap between the approximate value function and the optimal value function gets smaller.\n\nAlgorithm 2: New Posterior Set Generation   \n\n<html><body><table><tr><td>input:policy *,posterior set M,maximum number o newly added posterior distributions n output: newly added posterior set M' initialization:M'â0. for each (s,Î¼) â(SÃM) and s'â Sdo Î¼'(0)x Î¼(0)f(Îµ|0),where s'= g(s,Ï*(a|s,Î¼),Â§); distÎ¼'â distance of Î¼' to MUM'. if distÎ¼>O (i.e., Î¼' not inMUM') then M'â M'U{Î¼}. end if if |M'|> n (to reduce the size of M') then foreach Î¼'âM'do distÎ¼'â distance of Î¼â² to MUM\\{Î¼'}; M' âM\\{argminÎ¼eM distÎ¼'}. end for end if end for</td></tr></table></body></html>\n\nNext we incrementally add new posterior distributions to the posterior set $\\hat { \\mathcal { M } }$ . Different methods can be employed to produce new posterior distributions that are added to the set $\\mathcal { \\hat { M } }$ at each iteration. We take a similar approach as Poupart et al. (2015), which is based on envelope techniques. It considers the posterior distributions that can be reached in one step from any posterior distribution in $\\hat { \\mathcal { M } }$ by executing the policy $\\hat { \\pi } ^ { * }$ . As the number of posterior distributions to be added might be excessive, we can prioritize them by including the $n$ reachable posterior distributions with the largest Euclidean distance to the posterior distributions in $\\hat { \\mathcal { M } }$ . Note that the point-based value iteration approach in Pineau et al. (2003) shares the similar idea, that is, to include new posterior distribution that improves the worst-case density as rapidly as possible, where density is defined as the maximum distance from any posterior distribution to $\\hat { \\mathcal { M } }$ . We summarize the new posterior set generation in Algorithm 2.\n\nCombining Algorithm 1 and Algorithm 2, we now present the full algorithm below (ABDCP), which iteratively adds to the new posterior set and solves a bilevel difference convex program at each iteration. Theorem 3.7 shows that Algorithm 3 converges to a near-optimal policy.\n\nAlgorithm 3: ABDCP for infinite-horizon BR-MDPs   \n\n<html><body><table><tr><td>input: threshold Îµ,number of newly added posterior dis tributions n, initial state s1,dataset D output: policy å­*</td></tr><tr><td>initialization:compute prior distribution Î¼1 using dataset D; M â {degenerate beliefs} U{Î¼1}.</td></tr><tr><td>repeat obtain (*, V*) by running Algorithm 1;</td></tr><tr><td>evaluate policy Ï* by solving a DCP and obtain V*;</td></tr><tr><td>M â M U M' generated by Algorithm 2.</td></tr><tr><td>until|*-Î½*|â¤</td></tr></table></body></html>\n\nTheorem 3.7. Algorithm 3 converges to a near-optimal policy $\\hat { \\pi } ^ { * }$ , i.e., $| | \\hat { V } ^ { \\hat { \\pi } ^ { * } } - V ^ { * } | | _ { \\infty } \\leq \\epsilon _ { \\mathrm { ~  ~ } }$ , where $\\epsilon$ is the desired threshold.\n\nAs the number of iterations in Algorithm 3 increases, the gap between the optimal value function and the lower bound becomes arbitrarily small, which shows the lower bound in Theorem 3.6 is non-trivial.\n\n# 4 Numerical Experiments\n\nWe illustrate the performance of the infinite-horizon BRMDP formulation with different choices of risk measures and the proposed approximate bilevel DCP algorithm with an offline path planning problem.\n\nWe adapt two methods to our offline planning problems and compare their performances. The first method (CALP) comes from Poupart et al. (2015) with a riskneutral POMDP formulation. The second method (DRMDP) comes from $\\mathrm { { X u } }$ and Mannor (2010) with a distributionally robust MDP formulation. Note that the BPO approach from Lee et al. (2019) solves a risk-neutral BA-MDP formulation, where two separate encoders for the physical state and belief state are designed to deal with the continuous latent parameter space. It could have been a good benchmark if its encoder design were made available. Apart from the two benchmarks, we also compare with the nominal approach (MLE), where a maximal likelihood estimator for the parameter is computed from the given dataset and then a policy is obtained by solving the MDP with the plugged-in parameter value. In our proposed algorithm (ABDCP) for the infinite-horizon BR-MDP formulation, we consider two particular risk measures, namely expectation and CVaR with different risk levels $\\alpha$ . It should be noted that, when the considered risk measure is expectation, our algorithm can be modified and reduced to CALP. Similar observation is verified in Poupart et al. (2006), where the BA-MDP formulation is transformed into a POMDP formulation.\n\nFor each of the considered algorithms, we obtain the corresponding optimal policy with the same dataset. It should be noted that the calculations are carried out offline. The obtained policy is then applied for risk-averse path planning and evaluated on the true system, i.e., MDP with the true parameter. This is referred to as one replication, and we repeat the experiments for 200 replications on different independent datasets. Results for the path planning problem can be found in Table 1 and Table 2, with different data size $N = 1 0$ and $N = 1 0 0 0$ . The columns report the running time, expected performance (cost), and the CVaR performance (cost) of our proposed algorithm and benchmarks over the 200 replications. ABDCP-EXP stands for our proposed algorithm ABDCP with expectation as the risk measure. ABDCP-CVaR stands for our proposed algorithm ABDCP with CVaR as the risk measure. We also show the histogram of the actual performance over 200 replications for our proposed algorithm and the nominal benchmark on the path planning problem in Figure 1. We summarize the main observations for the path planning problem below.\n\nBR-MDP hedges against epistemic uncertainty: in each replication, data points are randomly sampled from the true distribution. While facing the epistemic uncertainty, BRMDP formulation optimizes over a dynamic risk measure that provides robustness. Table 1 shows that our proposed ABDCP algorithm is the most robust in the sense of balancing the mean and variability of the actual cost. The CVaR cost of our proposed algorithm is also lower than the other benchmarks, showing that it avoids large costs. In contrast, the nominal approach performs badly when the data size is small, e.g. $N = 1 0$ , indicating that it is not robust against the epistemic uncertainty and suffers from the scarcity of data. On the other hand, DR-MDP is overly conservative, even though it has the smallest variability. This conservativeness comes from two aspects. First, it always chooses to optimize over the worst-case scenario, which rarely happens in the true system. Second, the static worst-case risk measure prevents it from adapting to the data realizations, which is one of the motivations for the dynamic risk measure considered in the BR-MDP formulation. In contrast, BR-MDP formulation learns from the future data realization and updates its posterior distribution on $\\theta$ .\n\nLarger data size reduces epistemic uncertainty: when there are more data, the posterior distribution used in BRMDP formulation and the MLE estimator used in the nomi\n\n<html><body><table><tr><td>Approach</td><td>time (sec)</td><td>expected cost</td><td>CVaR(Î± = 0.95) cost</td><td>CVaR(Î± = 0.8) cost</td></tr><tr><td>ABDCP-EXP (CALP)</td><td>969.13(0.18)</td><td>70.06(0.51)</td><td>85.72</td><td>82.06</td></tr><tr><td>ABDCP-CVaR(Î±=0.95)</td><td>2639.38(0.22)</td><td>67.51(0.24)</td><td>75.67</td><td>73.72</td></tr><tr><td>ABDCP-CVaR(Î± = 0.8)</td><td>2545.74(0.24)</td><td>66.02(0.38)</td><td>79.97</td><td>75.50</td></tr><tr><td>DR-MDP</td><td>62.34(0.11)</td><td>79.43(0.15)</td><td>81.64</td><td>80.60</td></tr><tr><td>Nominal</td><td>61.44(0.08)</td><td>82.59(0.59)</td><td>94.10</td><td>92.46</td></tr></table></body></html>\n\nTable 1: Results for path planning problem. Running time for each replication, expected cost, and CVaR cost at different risk levels $\\alpha$ are reported for different algorithms. Standard errors are reported in parentheses. Number of data points is set to $N = 1 0$ .\n\n<html><body><table><tr><td>Approach</td><td>time (sec)</td><td>expected cost</td><td>CVaR (Î± = 0.95) cost</td><td>CVaR (Î± = 0.8) cost</td></tr><tr><td>ABDCP-EXP (CALP)</td><td>967.25(0.17)</td><td>64.15(0.05)</td><td>66.34</td><td>65.97</td></tr><tr><td>ABDCP-CVaR(Î±=0.95)</td><td>2642.26(0.21)</td><td>65.18(0.03)</td><td>66.14</td><td>65.76</td></tr><tr><td>ABDCP-CVaR (Î± = 0.8)</td><td>2643.48(0.25)</td><td>65.17(0.04)</td><td>66.26</td><td>65.84</td></tr><tr><td>DR-MDP</td><td>63.15(0.09)</td><td>65.22(0.03)</td><td>66.43</td><td>66.01</td></tr><tr><td>Nominal</td><td>62.47(0.08)</td><td>64.31(0.12)</td><td>67.55</td><td>65.59</td></tr></table></body></html>\n\nTable 2: Results for path planning problem. Running time for each replication, expected cost, and CVaR cost at different risk levels $\\alpha$ are reported for different algorithms. Standard errors are reported in parentheses. Number of data points is set to $N = 1 0 0 0$ .\n\nå½ B E å¹¿ 0 60 6570758085 62.565.0 67.5 70.0 72.5 75.077.5 ã 060 65 775 0 70 80 90 cost cost cost cost (a) ABDCP-EXP (b) ABDCP-CVaR(Î± = 0.95) (c) ABDCP-CVaR(Î± = 0.8) (d) Nominal\n\nnal approach converge to the true parameter, which reduces to solving an MDP with known transition probability and cost function. Therefore, the optimal policies and the actual costs tend to be the same.\n\nConvergence of ABDCP: the running time for a single replication on the path planning problem using our proposed ABDCP algorithm is affordable, and the proposed algorithm solves the infinite-horizon BR-MDP in finite time. In contrast, the infinite-horizon BR-MDP is intractable with standard value iteration or policy iteration.\n\nEffect of risk measures: although both risk measures (expectation and CVaR) result in time-consistent optimal policy for each considered formulation, they provide different levels of robustness. Even though the expectation case is faster to compute, it provides the least robustness, especially when the data size is small. For the CVaR risk measure, different risk level $\\alpha$ also affects the robustness. As $\\alpha$ increases, the agent is more risk-averse, and the CVaR cost is smaller since it avoids more severe costs, as is shown in Figure 1(b) and Figure 1(c). But this comes with a price: its expected cost is higher. This is intuitive: even though the agent avoids severe costs, it also forfeits a chance to traverse a path that is likely to have less traffic, even though the likelihood is small. This is shown as a right-shift of the actual performance distribution from Figure 1(c) to Figure 1(b).\n\n# 5 Conclusion\n\nIn this paper, we consider the offline planning problem in MDPs with epistemic uncertainty, where we only have access to a prior belief distribution over MDPs that is constructed by the offline data. We consider the infinite-horizon BR-MDP that produces a time-consistent formulation and provides the robustness against epistemic uncertainty. We develop an efficient optimization-based approximation algorithm that converges to the optimal policy. Our experimental results demonstrate the efficiency of the proposed approximate algorithm, and show the robustness of the infinitehorizon BR-MDP formulation. One of the future directions is to conduct the iteration complexity analysis on the proposed algorithm. Another interesting direction is to utilize function approximation to improve the scalability of the proposed approach to more complex domains. Separate encoders for the physical state and belief state have been proposed in Lee et al. (2019) and adaptation from their riskneutral BA-MDP formulation to our risk averse BR-MDP formulation could be interesting.\n\n# Acknowledgments\n\nThe authors gratefully acknowledge the support by the Air Force Office of Scientific Research under Grant FA9550-22- 1-0244, the National Science Foundation under Grant NSFECCS-2419562 and the NSF AI Institute for Advances in Optimization under Grant NSF-2112533.",
    "summary": "```json\n{\n  \"core_summary\": \"### ð¯ æ ¸å¿æ¦è¦\\n\\n> **é®é¢å®ä¹ (Problem Definition)**\\n> *   è®ºæéå¯¹æ éæ¶é´èå´çé©¬å°å¯å¤«å³ç­è¿ç¨ï¼MDPï¼ä¸­åæ°ï¼å¦è½¬ç§»æ¦çï¼æªç¥ä¸éä»æ°æ®ä¸­ä¼°è®¡çé®é¢ï¼æåºäºä¸ç§æ°çè§£å³æ¹æ¡ãä¼ ç»åå¸é²æ£æ¹æ³ï¼DR-MDPï¼å¨å¤çåæ°ä¸ç¡®å®æ§æ¶å¯è½è¿äºä¿å®ï¼ä¸ç¼ºä¹æ¶é´ä¸è´æ§ã\\n> *   è¯¥é®é¢å¨å·¥ä¸ç³»ç»ä¼åãè·¯å¾è§åç­éè¦é¿æå³ç­çåºæ¯ä¸­å·æéè¦ä»·å¼ï¼å°¤å¶æ¯å¨æ°æ®ç¨ç¼ºæåæ°ä¼°è®¡ä¸åç¡®çæåµä¸ï¼è½å¤æä¾æ´ç¨³å¥çå³ç­æ¯æã\\n\\n> **æ¹æ³æ¦è¿° (Method Overview)**\\n> *   è®ºææåºäºä¸ç§åºäºè´å¶æ¯é£é©é©¬å°å¯å¤«å³ç­è¿ç¨ï¼BR-MDPï¼çæ¡æ¶ï¼éè¿è¿ä¼¼åå±å·®åå¸è§åï¼ABDCPï¼ç®æ³ï¼é«ææ±è§£æ éæ¶é´èå´çBR-MDPé®é¢ï¼çæå·ææ§è½ä¿è¯çæä¼ç­ç¥ã\\n\\n> **ä¸»è¦è´¡ç®ä¸ææ (Contributions & Results)**\\n> *   **è´¡ç®1ï¼** æåºäºæ éæ¶é´èå´BR-MDPçè´å¶æ¯è§è§åæï¼è¯æäºå¹³ç¨³æä¼ç­ç¥çå­å¨æ§åå¯ä¸æ§ã\\n> *   **è´¡ç®2ï¼** è®¾è®¡äºè¿ä¼¼åå±å·®åå¸è§åç®æ³ï¼ABDCPï¼ï¼è§£å³äºæ éæ¶é´èå´BR-MDPçè®¡ç®é¾é¢ï¼å¹¶è¯æäºç®æ³çæ¶ææ§ã\\n> *   **ææï¼** å¨è·¯å¾è§åå®éªä¸­ï¼ABDCPå¨æ°æ®éè¾å°ï¼N=10ï¼æ¶ï¼å¶CVaRï¼Î±=0.95ï¼ææ¬ä¸º75.67ï¼æ¾èä¼äºåºçº¿æ¨¡åDR-MDPï¼81.64ï¼åNominalï¼94.10ï¼ã\",\n  \"algorithm_details\": \"### âï¸ ç®æ³/æ¹æ¡è¯¦è§£\\n\\n> **æ ¸å¿ææ³ (Core Idea)**\\n> *   BR-MDPæ¡æ¶éè¿å¨æé£é©åº¦éï¼å¦CVaRï¼åµå¥è´å¶æ¯åéªåå¸ï¼è§£å³äºä¼ ç»DR-MDPçä¿å®æ§åæ¶é´ä¸ä¸è´æ§é®é¢ãå¶è®¾è®¡å²å­¦æ¯å°åæ°ä¸ç¡®å®æ§å»ºæ¨¡ä¸ºè´å¶æ¯åéªåå¸ï¼å¹¶éè¿é£é©åº¦éä¼åç­ç¥ã\\n\\n> **åæ°ç¹ (Innovations)**\\n> *   **ä¸ååå·¥ä½çå¯¹æ¯ï¼** ååå·¥ä½ï¼å¦Lin, Ren, and Zhou (2022)ï¼ä»éç¨äºæéæ¶é´èå´MDPï¼ä¸ç¼ºä¹çè®ºä¿è¯ï¼DR-MDPåå éææåæåµä¼åå¯¼è´ä¿å®æ§ã\\n> *   **æ¬æçæ¹è¿ï¼** æ¬æéè¿åå±å·®åå¸è§åï¼DCPï¼å°æ éæ¶é´èå´BR-MDPè½¬åä¸ºå¯ä¼åé®é¢ï¼å¹¶å¼å¥åéªåå¸è¿ä¼¼åè¿­ä»£çæææ¯ï¼æ¾èæåäºè®¡ç®æçã\\n\\n> **å·ä½å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **é®é¢éæï¼** å°BR-MDPè½¬åä¸ºåå±DCPé®é¢ï¼å¦å¬å¼ï¼4ï¼æç¤ºã\\n> 2.  **åéªåå¸è¿ä¼¼ï¼** ä½¿ç¨æéåéªéï¼belief spaceï¼è¿ä¼¼è¿ç»­ç¶æç©ºé´ï¼éè¿çº¿æ§è§åï¼6ï¼è®¡ç®æå¼æéã\\n> 3.  **ç­ç¥ä¼åï¼** éè¿DCCPï¼Disciplined Convex-Concave Programmingï¼æ±è§£è¿ä¼¼DCPï¼çæç­ç¥ï¼Algorithm 1ï¼ã\\n> 4.  **åéªéæ©å±ï¼** è¿­ä»£çææ°åéªåå¸ï¼Algorithm 2ï¼ï¼éæ­¥ç¼©å°ä¸ä¸çå·®è·ï¼Algorithm 3ï¼ã\\n\\n> **æ¡ä¾è§£æ (Case Study)**\\n> *   è®ºææªæç¡®æä¾æ­¤é¨åä¿¡æ¯ã\",\n  \"comparative_analysis\": \"### ð å¯¹æ¯å®éªåæ\\n\\n> **åºçº¿æ¨¡å (Baselines)**\\n> *   CALPï¼Poupart et al. (2015)ï¼ï¼é£é©ä¸­æ§POMDPæ¹æ³ã\\n> *   DR-MDPï¼Xu and Mannor (2010)ï¼ï¼åå¸é²æ£MDPæ¹æ³ã\\n> *   Nominalï¼æå¤§ä¼¼ç¶ä¼°è®¡ï¼MLEï¼æ¹æ³ã\\n\\n> **æ§è½å¯¹æ¯ (Performance Comparison)**\\n> *   **å¨CVaRï¼Î±=0.95ï¼ææ¬ä¸ï¼** æ¬ææ¹æ³ï¼ABDCP-CVaRï¼å¨N=10æ¶è¾¾å°75.67ï¼æ¾èä¼äºDR-MDPï¼81.64ï¼åNominalï¼94.10ï¼ãä¸è¡¨ç°æä½³çåºçº¿DR-MDPç¸æ¯ï¼éä½äº5.97ï¼çº¦7.3%ï¼ã\\n> *   **å¨è¿è¡æ¶é´ä¸ï¼** æ¬ææ¹æ³ï¼ABDCP-CVaRï¼çåæ¬¡å¤ç°æ¶é´ä¸º2639.38ç§ï¼è¿é«äºDR-MDPï¼62.34ç§ï¼ï¼ä½æä¾äºæ´ä¼çé²æ£æ§ã\\n> *   **å¨æ°æ®éå¢å¤§ï¼N=1000ï¼æ¶ï¼** æææ¹æ³çæ§è½è¶è¿ï¼ABDCP-CVaRçCVaRï¼Î±=0.95ï¼ææ¬ä¸º66.14ï¼ä¸DR-MDPï¼66.43ï¼åNominalï¼67.55ï¼ç¸å½ï¼éªè¯äºæ°æ®éå¯¹åå°åæ°ä¸ç¡®å®æ§çä½ç¨ã\",\n  \"keywords\": \"### ð å³é®è¯\\n\\n*   è´å¶æ¯é£é©é©¬å°å¯å¤«å³ç­è¿ç¨ (Bayesian Risk Markov Decision Process, BR-MDP)\\n*   åå±å·®åå¸è§å (Bilevel Difference Convex Programming, BDCP)\\n*   æ¡ä»¶é£é©ä»·å¼ (Conditional Value-at-Risk, CVaR)\\n*   æ éæ¶é´èå´ (Infinite-Horizon, N/A)\\n*   åéªåå¸ (Posterior Distribution, N/A)\\n*   è·¯å¾è§å (Path Planning, N/A)\\n*   é²æ£ä¼å (Robust Optimization, N/A)\"\n}\n```"
}