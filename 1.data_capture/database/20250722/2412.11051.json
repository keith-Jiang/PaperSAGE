{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.11051",
    "link": "https://arxiv.org/abs/2412.11051",
    "pdf_link": "https://arxiv.org/pdf/2412.11051.pdf",
    "title": "DisCo-DSO: Coupling Discrete and Continuous Optimization for Efficient Generative Design in Hybrid Spaces",
    "authors": [
        "Jacob F. Pettit",
        "Chak Shing Lee",
        "Jiachen Yang",
        "Alex Ho",
        "Daniel M. Faissol",
        "Brenden K. Petersen",
        "Mikel Landajuela"
    ],
    "publication_date": "2024-12-15",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "paper_content": "# DisCo-DSO: Coupling Discrete and Continuous Optimization for Efficient Generative Design in Hybrid Spaces\n\nJacob F. Pettit, Chak Shing Lee, Jiachen Yang, Alex Ho, Daniel Faissol, Brenden Petersen, Mikel Landajuela\\*\n\nComputational Engineering Division Lawrence Livermore National Laboratory Livermore, CA, 94550, USA\n\n# Abstract\n\nWe consider the challenge of black-box optimization within hybrid discrete-continuous and variable-length spaces, a problem that arises in various applications, such as decision tree learning and symbolic regression. We propose DisCoDSO (Discrete-Continuous Deep Symbolic Optimization), a novel approach that uses a generative model to learn a joint distribution over discrete and continuous design variables to sample new hybrid designs. In contrast to standard decoupled approaches, in which the discrete and continuous variables are optimized separately, our joint optimization approach uses fewer objective function evaluations, is robust against non-differentiable objectives, and learns from prior samples to guide the search, leading to significant improvement in performance and sample efficiency. Our experiments on a diverse set of optimization tasks demonstrate that the advantages of DisCo-DSO become increasingly evident as the complexity of the problem increases. In particular, we illustrate DisCo-DSO’s superiority over the state-of-the-art methods for interpretable reinforcement learning with decision trees.\n\n# Introduction\n\nDeep learning methods have shown success in important combinatorial optimization problems (Bello et al. 2016), including generating interpretable policies for continuous control (Landajuela et al. 2021a) and symbolic regression (SR) to discover the underlying mathematical equations from the data (Petersen et al. 2021; Biggio et al. 2021; Kamienny et al. 2022; Landajuela et al. 2022). Existing approaches train a generative model that constructs a solution to the optimization problem by sequentially choosing from a set of discrete tokens, using the value of the objective function as the terminal reward for learning. However, these approaches do not jointly optimize the discrete and continuous components of such hybrid problems: Certain discrete tokens require the additional specification of an associated real-valued parameter, such as the threshold value at a decision tree node or the value of a constant token in an equation, but the learned generative model does not produce these values. Instead, they adopt the design choice of decoupled optimization, whereby only the construction of a discrete solution skeleton is optimized by deep learning, while the associated continuous parameters are left to a separate black-box optimizer.\n\n![](images/a5d38247dadb965fffc3f4791f573d77dbe7cd6eef8e9672e41c161722b58793.jpg)  \nFigure 1: Comparison of the standard decoupled approach and DisCo-DSO for discrete-continuous optimization using an autoregressive model. In the decoupled approach, the discrete skeleton $\\tau _ { \\mathrm { d } } = \\langle ( l _ { 1 } , \\cdot ) , \\ldots , ( l _ { T } , \\cdot ) \\rangle$ is sampled first and then the continuous parameters $\\beta _ { 1 } , \\ldots , \\beta _ { T }$ are optimized independently. In contrast, DisCo-DSO models the joint distribution over the sequence of tokens $\\langle ( l _ { 1 } , \\beta _ { 1 } ) , \\dots , ( l _ { T } , \\beta _ { T } ) \\rangle$ . Here, the notation $\\oplus$ stands for concatenation of vectors.\n\nWe hypothesize that a joint discrete-continuous optimization approach (Figure 1(b)) that generates a complete solution based on deep reinforcement learning (RL) (Sutton and Barto 2018) has significant advantages compared to existing decoupled approaches that employ learning only for the discrete skeleton (Figure 1(a)). In terms of efficiency, a joint approach only requires one evaluation of the objective function for each candidate solution, whereas the decoupled approach based on common non-linear black-box optimization methods such as BFGS (Fletcher 2000), simulated annealing (Xiang et al. 1997), or evolutionary algorithms (Storn and Price 1997) requires a significant number of function evaluations to optimize each discrete skeleton. This decoupled approach incurs a high cost for applications such as interpretable control, where each objective function evaluation involves running the candidate solution on many episodes of a highdimensional and stochastic physical simulation (Landajuela et al. 2021a). Furthermore, joint exploration and learning on the full discrete-continuous solution space has the potential to escape from local optima and use information from prior samples to guide the subsequent search.\n\nIn this work, we consider discrete-continuous optimization problems that exhibit several key distinguishing features: (1) a black-box reward, (2) a variable-length structure of the design space, and (3) a sequential structure in the form of prefix-dependent positional constraints. These problems are not well-suited to existing joint optimization approaches such as Mixed Integer Programming (MIP) (Fischetti and Jo 2018; Nair et al. 2021) or Mixed Bayesian Optimization (BO) (Daxberger et al. 2019), which are designed for problems with fixed-length discrete components and do not naturally handle positional constraints in the design space. To address these challenges, we draw upon the success of deep reinforcement learning in parameterized action space Markov decision processes (Hausknecht and Stone 2016) to extend existing deep learning methods for discrete optimization (Bello et al. 2016; Zoph and Le 2017; Petersen et al. 2021; Landajuela et al. 2021a) to the broader space of joint discrete-continuous optimization. We summarize the main contributions of this paper as follows:\n\n• We propose a novel method for joint discrete-continuous optimization using autoregressive models and deep reinforcement learning, which we call DisCo-DSO, that is suited for black-box hybrid optimization problems over variable-length search spaces with prefix-dependent positional constraints.   \n• We present a novel formulation for decision tree policy search in control tasks as sequential discrete-continuous optimization and propose a method for sequentially finding bounds for parameter ranges in decision nodes.   \n• We perform exhaustive empirical evaluation of DisCoDSO on a diverse set of tasks, including interpretable control policies and symbolic regression. We show that DisCo-DSO outperforms decoupled approaches on all tasks.\n\n# Related Work\n\nHybrid discrete-continuous action spaces in reinforcement learning. The treatment of the continuous parameters as part of the action space has strong parallels in the space of hybrid discrete-continuous RL. In Hausknecht and Stone (2016), the authors present a successful application of deep reinforcement learning to a domain with continuous state and action spaces. In Xiong et al. (2018), the authors take an off-policy DQN-type approach that directly works on the hybrid action space without approximation of the continuous part or relaxation of the discrete part, but requires an extra loss function for the continuous actions. In Neunert et al. (2020), they propose a hybrid RL algorithm that uses continuous policies for discrete action selection and discrete policies for continuous action selection.\n\nSymbolic regression with constants optimization. In the field of symbolic regression, different approaches have been proposed for addressing the optimization of both discrete skeletons and continuous parameters. Traditional genetic programming approaches and deep generative models handle these problems separately, with continuous constants optimized after discrete parameters (Topchy, Punch et al. 2001; Petersen et al. 2021; Biggio et al. 2021). Recent works aim to jointly optimize discrete constants and continuous parameters by relaxing the discrete problem into a continuous one (Martius and Lampert 2016; Sahoo, Lampert, and Martius 2018), or by tokenizing (i.e., discretizing) the continuous constants (Kamienny et al. 2022). The former approach faces challenges such as exploding gradients and the need to revert continuous values to discrete ones. The latter approach tokenizes continuous constants, treating them similarly to discrete tokens, but such quantization is problemdependent, restricts the search space, and requires additional post-hoc optimization to refine the continuous parameters.\n\nDecision tree policies in reinforcement learning. In the domain of symbolic reinforcement learning, where the goal is to find intelligible and concise control policies, works such as Landajuela et al. (2021a) and Sahoo, Lampert, and Martius (2018) have discretized the continuous space and used relaxation approaches, respectively, to optimize symbolic control policies in continuous action spaces. For discrete action spaces, a natural representation of a symbolic policy is a decision tree (Ding et al. 2020; Silva et al. 2020; Custode and Iacca 2023). In Custode and Iacca (2023), the authors use an evolutionary search to find the best decision tree policy and further optimized the real valued thresholds using a decoupled approach. Relaxation approaches find their counterparts within this domain in works such as Sahoo, Lampert, and Martius (2018); Silva et al. (2020); Ding et al. (2020), where a soft decision tree is used to represent the policy. The soft decision tree, which fixes the discrete structure of the policy and exposes the continuous parameters, is then optimized using gradient-based methods.\n\n# Discrete-Continuous Deep Symbolic Optimization\n\n# Notation and Problem Definition\n\nWe consider a discrete-continuous optimization problem defined over a search space $\\tau$ of sequences of tokens $\\tau =$ $\\langle \\tau _ { 1 } , \\dots , \\tau _ { T } \\rangle$ , where each token $\\tau _ { i }$ belongs to a library $\\mathcal { L }$ and the length $T$ of the sequence is not fixed a priori. The library $\\mathcal { L }$ is a set of $K$ tokens $\\mathcal { L } = \\{ l _ { 1 } , \\ldots , l _ { K } \\}$ , where a subset ${ \\hat { \\mathcal { L } } } \\subseteq { \\mathcal { L } }$ of them are parametrized by a continuous parameter, i.e., each token $l \\in \\hat { \\mathcal { L } }$ has an associated continuous parameter $\\beta \\in \\mathcal { A } ( l ) \\subset \\mathbb { R }$ , where $\\mathbf { \\nabla } \\mathcal { A } ( l )$ is the token-dependent range. To ease the notation, we define ${ \\bar { \\mathcal { L } } } \\ { \\stackrel { \\mathrm { d e f } } { = } } \\ { \\mathcal { L } } \\ \\backslash \\ { \\hat { \\mathcal { L } } }$ and consider a dummy range $\\mathcal { A } ( l ) = [ 0 , 1 ] \\subset \\mathbb { R }$ for the strictly discrete\n\ntokens $l \\in \\bar { \\mathcal { L } }$ . Thus, we define\n\n$$\nl ( \\beta ) = \\left\\{ l \\atop l ( \\beta ) \\quad \\mathrm { i f } l \\in \\bar { \\mathcal { L } } , \\forall ( l , \\beta ) \\in \\mathcal { L } \\times A ( l ) . \\right.\n$$\n\nIn other words, the parameter $\\beta$ is ignored if $l \\in \\bar { \\mathcal { L } }$ . With this notation, we can write $\\tau _ { i } = l _ { i } ( \\beta _ { i } \\bar { ) } \\in \\mathcal { L } , \\forall i \\in \\{ 1 , \\dots , T \\}$ . In the following, we use the notation $l _ { i } ( \\beta _ { i } ) ~ \\equiv ~ ( l _ { i } , \\beta _ { i } )$ ] and write $\\tau \\ = \\ \\langle \\tau _ { 1 } , \\ldots , \\tau _ { T } \\rangle \\ = \\ \\langle l _ { 1 } ( \\beta _ { 1 } ) , \\ldots , l _ { T } ( \\beta _ { T } ) \\rangle \\ \\equiv$ $\\langle ( l _ { 1 } , \\beta _ { 1 } ) , \\dots , ( l _ { T } , \\beta _ { T } ) \\rangle$ .\n\nGiven a sequence $\\tau$ , we define the discrete skeleton $\\tau _ { \\mathrm { d } }$ as the sequence obtained by removing the continuous parameters from $\\tau$ , i.e., $\\tau _ { \\mathrm { d } } = \\langle ( l _ { 1 } , \\cdot ) , \\ldots , ( l _ { T } , \\cdot ) \\rangle$ . We introduce the operator $\\mathsf { e v a l } : \\mathcal { T } \\to \\mathbb { T }$ to represent the semantic interpretation of the sequence $\\tau$ as an object in the relevant design space $\\mathbb { T }$ . We consider problems with prefix-dependent positional constraints, i.e., problems for which, given a prefix $\\tau _ { 1 : ( i - 1 ) }$ , there exists a possible non-empty set of unfeasible tokens $\\mathcal { C } _ { \\tau _ { 1 : ( i - 1 ) } } \\subseteq \\mathcal { L }$ such that $\\mathtt { e v a l } ( \\tau _ { 1 : ( i - 1 ) } \\cup \\tau _ { i } \\cup$ $\\tau _ { ( i + 1 ) : T } ) \\notin \\ \\mathbb { T }$ for all $\\tau _ { i } \\ \\in \\ \\mathcal { C } _ { \\tau _ { 1 : ( i - 1 ) } }$ and for all $\\tau _ { j } ~ \\in ~ { \\mathcal { L } }$ with $\\dot { \\iota } < j \\le T$ . Variable-length problems exhibiting such constraints are not well-suited for MIP solvers or classical Bayesian Optimization methods.\n\nThe optimization problem is defined by the reward function $R : \\mathbb { T }  \\mathbb { R }$ , which can be deterministic or stochastic. In the stochastic case, we have a reward distribution $p _ { R } ( r | t )$ conditioned on the design $t \\in \\mathbb { T }$ and the reward function is given by $R ( t ) ~ = ~ \\mathbb { E } _ { r \\sim p _ { R } ( r | t ) } [ r ]$ . Note that we do not assume that the reward function $R$ is differentiable with respect to the continuous parameters $\\beta _ { i }$ . In the following, we make a slight abuse of notation and use $R ( \\tau )$ and $p _ { R } ( r | \\tau )$ to denote $\\bar { R } ( \\mathsf { e v a l } ( \\tau ) )$ and $p _ { R } ( r | \\mathsf { e v a l } ( \\tau ) )$ , respectively. The optimization problem is to find a sequence $\\tau ^ { * } =$ $\\langle \\tau _ { 1 } ^ { * } , \\cdot \\cdot , \\tau _ { T } ^ { * } \\rangle = \\langle ( l _ { 1 } ^ { * } , \\beta _ { 1 } ^ { * } ) , \\cdot \\cdot \\cdot , ( l _ { T } ^ { * } , \\beta _ { T } ^ { * } ) \\rangle$ (where the length $T$ is not fixed a priori) such that $\\tau ^ { * } \\in \\bar { \\mathrm { a r g } } \\operatorname* { m a x } _ { \\tau \\in \\mathcal { T } } R ( \\tau )$ .\n\n# Method\n\nCombinatorial optimization with autoregressive models. In applications of deep learning to combinatorial optimization (Bello et al. 2016), a probabilistic model $p ( \\tau )$ is learned over the design space $\\tau$ . The model is trained to gradually allocate more probability mass to high scoring solutions. The training can be done using supervised learning, if problem instances with their corresponding solutions are available, or, more generally, using RL. In most cases, the model $p ( \\tau )$ is parameterized by an autoregressive (AR) model with parameters $\\theta$ . The model is used to generate sequences as follows.\n\nAt position $i$ , the model emits a vector of logits $\\psi ^ { ( i ) }$ conditioned on the previously generated tokens τ1:(i 1), i.e., $\\boldsymbol { \\psi } ^ { ( i ) } = \\mathrm { A R } ( \\tau _ { 1 : ( i - 1 ) } ; \\boldsymbol { \\theta } )$ . The new token $\\tau _ { i }$ is sampled from the distribution $p ( \\tau _ { i } | \\tau _ { 1 : ( i - 1 ) } , \\theta ) = \\mathrm { s o f t m a x } ( \\psi ^ { ( i ) } ) _ { \\mathcal { L } ( \\tau _ { i } ) } ,$ , where $\\mathcal { L } ( \\tau _ { i } )$ is the index in $\\mathcal { L }$ corresponding to node value $\\tau _ { i }$ . The new token $\\tau _ { i }$ is then added to the sequence $\\tau _ { 1 : ( i - 1 ) }$ and used to condition the generation of the next token $\\tau _ { i + 1 }$ . The process continues until a stopping criterion is met.\n\nDifferent model architectures can be employed to generate the logits $\\psi ^ { ( i ) }$ . For instance, recurrent neural networks (RNNs) have been utilized in Petersen et al. (2021); Landajuela et al. (2021a); Mundhenk et al. (2021); da Silva et al. (2023), and transformers with causal attention have been applied in works like Biggio et al. (2021) and Kamienny et al. (2022).\n\nPrefix-dependent positional constraints. Sequential token generation enables flexible configurations and the incorporation of constraints during the search process (Petersen et al. 2021). Specifically, given a prefix $\\tau _ { 1 : ( i - 1 ) }$ , a prior $\\psi _ { \\circ } ^ { ( i ) } \\in \\mathbb { R } ^ { | \\mathcal { L } | }$ is computed such that $\\psi _ { \\circ } ^ { ( i ) } \\mathcal { L } ( \\tau _ { i } ) = - \\infty$ for tokens $\\tau _ { i }$ in the unfeasible set $\\mathscr { C } _ { \\tau _ { 1 : ( i - 1 ) } }$ and zero otherwise. The prior is added to the logits $\\psi ^ { ( i ) }$ before sampling the token $\\tau _ { i }$ .\n\nExtension to discrete-continuous optimization. Current deep learning approaches for combinatorial optimization only support discrete tokens, i.e., ${ \\hat { \\mathcal { L } } } = \\emptyset$ , (Bello et al. 2016) or completely decouple the discrete and continuous parts of the problem, as in Petersen et al. (2021); Landajuela et al. (2021a); Mundhenk et al. (2021); da Silva et al. (2023), by sampling first the discrete skeleton $\\tau _ { \\mathrm { d } }$ and then optimizing its continuous parameters separately (see Figure 1(a)). In this work, we extend these frameworks to support joint optimization of discrete and continuous tokens. The model is extended to emit two outputs $\\psi ^ { ( i ) }$ and $\\phi ^ { ( i ) }$ for each token $\\tau _ { i } ~ = ~ ( l _ { i } , \\beta _ { i } )$ conditioned on the previously generated tokens, i.e., $\\left( \\psi ^ { ( i ) } , \\phi ^ { ( i ) } \\right) = \\mathrm { A R } ( ( l , \\beta ) _ { 1 : ( i - 1 ) } ; \\theta )$ , where we use the notation $( l , \\beta ) _ { 1 : ( i - 1 ) }$ to denote the sequence of tokens $\\langle ( l _ { 1 } , \\beta _ { 1 } ) , \\dots , ( l _ { i - 1 } , \\beta _ { i - 1 } ) \\rangle$ (see Figure 1(b)). Given tokens $( l , \\beta ) _ { 1 : ( i - 1 ) }$ , the $i ^ { \\mathrm { t h } }$ token $( l _ { i } , \\beta _ { i } )$ is generated by sampling from the following distribution:\n\n$$\n\\begin{array} { r } { p ( ( l _ { i } , \\beta _ { i } ) | ( l , \\beta ) _ { 1 : ( i - 1 ) } , \\theta ) = \\left\\{ \\begin{array} { l l } { \\mathcal { U } _ { [ 0 , 1 ] } ( \\beta _ { i } ) \\mathrm { s o f t m a x } ( \\psi ^ { ( i ) } ) _ { \\mathcal { L } ( l _ { i } ) } } & { \\mathrm { i f ~ } l _ { i } \\in \\bar { \\mathcal { L } } } \\\\ { \\mathcal { D } ( \\beta _ { i } | l _ { i } , \\phi ^ { ( i ) } ) \\mathrm { s o f t m a x } ( \\psi ^ { ( i ) } ) _ { \\mathcal { L } ( l _ { i } ) } } & { \\mathrm { i f ~ } l _ { i } \\in \\bar { \\mathcal { L } } } \\end{array} \\right. , } \\end{array}\n$$\n\nwhere $\\mathcal { D } ( \\beta _ { i } | l _ { i } , \\phi ^ { ( i ) } )$ is the probability density function of the distribution $\\mathcal { D }$ that is used to sample $\\beta _ { i }$ from $\\phi ^ { ( i ) }$ . Note that the choice of $\\beta _ { i }$ is conditioned on the choice of discrete token $l _ { i }$ . We assume that the support of $\\mathcal { D } ( \\beta | l , \\phi )$ is a subset of $\\mathbf { \\nabla } \\mathcal { A } ( l )$ for all $l \\in \\hat { \\mathcal { L } }$ . Additional priors of the form $( \\psi _ { \\circ } ^ { ( i ) } , 0 )$ can be added to the logits before sampling the token $\\tau _ { i }$ .\n\nTraining DisCo-DSO. The parameters $\\theta$ of the model are learned by maximizing the expected reward ${ \\cal J } ( \\theta ) ~ =$ $\\mathbb { E } _ { \\tau \\sim p ( \\tau | \\theta ) } [ R ( \\dot { \\tau } ) ]$ or, alternatively, the quantile-conditioned expected reward $J _ { \\varepsilon } ( \\theta ) = \\mathbb { E } _ { \\tau \\sim p ( \\tau \\mid \\theta ) } [ R ( \\tau ) | R ( \\tau ) \\geq R _ { \\varepsilon } ( \\theta ) ]$ , where $R _ { \\varepsilon } ( \\theta )$ represents the $( 1 - \\varepsilon )$ -quantile of the reward distribution $R ( \\tau )$ sampled from the trajectory distribution $p ( \\tau | \\theta )$ . The motivation for using $J _ { \\varepsilon } ( \\theta )$ is to encourage the model to focus on best case performance over average case performance (see Petersen et al. (2021)), which is the preferred behavior in optimization problems. It is worth noting that both objectives, $J ( \\theta )$ and $J _ { \\varepsilon } ( \\theta )$ , serve as relaxations of the original arg max $R ( \\tau )$ optimization problem described above.\n\nTo optimize the objective $J _ { \\varepsilon } ( \\theta )$ , we extend the riskseeking policy gradient of Petersen et al. (2021) to the discrete-continuous setting. The gradient of $J _ { \\varepsilon } ( \\theta )$ reads as\n\n$$\n\\nabla _ { \\theta } J _ { \\varepsilon } ( \\theta ) = \\mathbb { E } _ { \\tau \\sim p ( \\tau \\mid \\theta ) } \\left[ A ( \\tau , \\varepsilon , \\theta ) S ( ( l , \\beta ) _ { 1 : T } ) \\mid A ( \\tau , \\varepsilon , \\theta ) > 0 \\right] ,\n$$\n\nwhere $A ( \\tau , \\varepsilon , \\theta ) = R ( \\tau ) - R _ { \\varepsilon } ( \\theta )$ and\n\n$$\nS ( ( l , \\beta ) _ { 1 : T } ) = \\sum _ { i = 1 } ^ { T } \\left\\{ \\begin{array} { l l } { \\nabla _ { \\theta } \\log p ( l _ { i } | ( l , \\beta ) _ { 1 : ( i - 1 ) } , \\theta ) } & { \\mathrm { i f ~ } l _ { i } \\in \\bar { \\mathcal { L } } , } \\\\ { \\nabla _ { \\theta } \\log p ( l _ { i } | ( l , \\beta ) _ { 1 : ( i - 1 ) } , \\theta ) } \\\\ { + \\nabla _ { \\theta } \\log p ( \\beta _ { i } | l _ { 1 : i } , \\beta _ { 1 : i - 1 } , \\theta ) } & { \\mathrm { i f ~ } l _ { i } \\in \\hat { \\mathcal { L } } . } \\end{array} \\right.\n$$\n\nWe provide pseudocode for DisCo-DSO, a derivation of the risk-seeking policy gradient, and additional details of the learning procedure in the appendix.\n\n# Experiments\n\nWe demonstrate the benefits and generality of our approach on a diverse set of tasks as follows. Firstly, we introduce a new pedagogical task, called Parameterized Bitstring, to understand the conditions under which the benefits of DisCoDSO versus decoupled approaches become apparent. We then consider two preeminent tasks in combinatorial optimization: decision tree policy optimization for reinforcement learning and symbolic regression for equation discovery.\n\nBaselines. To demonstrate the advantages of joint discrete-continuous optimization, we compare DisCo-DSO with the following classes of methods:\n\n• Decoupled-RL- BFGS, anneal, evo : This baseline trains a generative model with reinforcement learning to produce a discrete skeleton (Petersen et al. 2021), which is then optimized by a downstream nonlinear solver for the continuous parameters. The objective value at the optimized solution is the reward, which is used to update the generative model using the same policy gradient approach and architecture as DisCo-DSO. The continuous optimizer is either L-BFGS-B (BFGS), simulated annealing (anneal) (Xiang et al. 1997), or differential evolution (evo) (Storn and Price 1997), using the SciPy implementation (Virtanen et al. 2020).\n\n• Decoupled-GP- BFGS, anneal, evo : This baseline uses genetic programming (GP) (Koza 1990) to produce a discrete skeleton, which is then optimized by a downstream nonlinear solver for the continuous parameters.\n\n• BO: For the Parameterized Bitstring task, which has a fixed length search space and no positional constraints, we also consider a Bayesian Optimization baseline using expected improvement as acquisition function (Shahriari et al. 2015; Garrido-Mercha´n and Herna´ndez-Lobato 2020).\n\nAll experiments involving RL and DisCo-DSO use a RNN with a single hidden layer of 32 units as the generative model. The GP baselines use the “Distributed Evolutionary Algorithms in Python” software1 (Fortin et al. 2012). Additional details are provided in the appendix.\n\nNote on baselines for symbolic regression. In the context of symbolic regression, some of the above baselines corresponds to popular methods in the literature. Specifically, Decoupled-RL-BFGS corresponds exactly to the method “Deep Symbolic Regression” from Petersen et al. (2021), and Decoupled-GP-BFGS corresponds to a standard implementation of genetic programming for symbolic regression $\\grave { a }$ la Koza (1994) (most common approach to symbolic regression in the literature).\n\n# Parameterized Bitstring Task\n\nProblem formulation. We design a general and flexible Parameterized Bitstring benchmark problem, denoted $\\mathrm { P B } ( N , f , l ^ { * } , \\beta ^ { * } )$ , to test the hypothesis that DisCo-DSO is more efficient than the decoupled optimization approach. In each problem instance, the task is to recover a hidden string $l ^ { * } \\in \\mathsf { \\Gamma } [ 0 , 1 ] ^ { T }$ of $T$ bits and a vector of parameters $\\beta ^ { * } \\in \\mathbb { R } ^ { \\breve { T } }$ . Each bit $l _ { i } ^ { * }$ is paired with a parameter $\\beta _ { i } ^ { * }$ via the reward function $R$ , which gives a positive value based on an objective function $f ( \\beta _ { i } , \\beta _ { i } ^ { * } ) \\in [ 0 , 1 ]$ only if the correct bit $l _ { i } ^ { * }$ is chosen at position $i$ :\n\n$$\nR ( \\tau , \\beta ) \\stackrel { \\mathrm { d e f } } { = } \\frac { 1 } { T } \\sum _ { i = 1 } ^ { T } \\mathbf { 1 } _ { \\tau _ { i } = \\tau _ { i } ^ { * } } \\left( \\alpha + ( 1 - \\alpha ) f ( \\beta _ { i } , \\beta _ { i } ^ { * } ) \\right)\n$$\n\nThe scalar $\\alpha ~ \\in ~ [ 0 , 1 ]$ controls the relative importance of expending computational effort to optimize the discrete or continuous parts of the reward. The problem difficulty can be controlled by increasing the length $T$ and increasing the nonlinearity of the objective function $f$ , such as by increasing the number of local optima. In our experiment, we tested the following objective functions, which represent objectives with multiple suboptimal local maxima $( f _ { 1 } )$ and discontinuous objective landscapes $( f _ { 2 } )$ :\n\n$$\nf _ { 1 } ( x , x ^ { * } ) { \\stackrel { \\mathrm { d e f } } { = } } \\left| { \\frac { \\sin ( 5 0 ( x - x ^ { * } ) ) } { 5 0 ( x - x ^ { * } ) } } \\right| ,\n$$\n\n$$\n\\begin{array} { r } { f _ { 2 } ( x , x ^ { * } ) \\stackrel { \\mathrm { d e f } } { = } \\left\\{ \\begin{array} { l l } { 1 , \\quad } & { | x - x ^ { * } | \\leq 0 . 0 5 } \\\\ { 0 . 5 , \\quad } & { 0 . 0 5 < | x - x ^ { * } | \\leq 0 . 1 \\cdot } \\\\ { 0 , \\quad } & { 0 . 1 < | x - x ^ { * } | } \\end{array} \\right. } \\end{array}\n$$\n\nResults. Figure 2 shows that DisCo-DSO is significantly more sample efficient than the decoupled approach when the discrete solution contributes more to the overall reward. This is because each sample generated by DisCo-DSO is a complete solution, which costs only one function evaluation to get a reward. In contrast, each sample generated by the baseline decoupled methods only has a discrete skeleton, which requires many function evaluations using the downstream optimizer to get a single complete solution. As the discrete skeleton increases in importance, the relative contribution of function evaluations for continuous optimization decreases. Note that, given the same computational budget, the BO method performs less function evaluations than the rest of the methods and the final results are worse than DisCo-DSO. This is because BO has a computational complexity of $\\mathcal { O } ( n ^ { 3 } )$ (Shahriari et al. 2015), where $n$ is the number of function evaluations. This computational complexity makes BO challenging or even infeasible for large $n$ (Lan et al. 2022).\n\nDecision Tree Policies for Reinforcement Learning Problem formulation. In this section we consider the problem of discovering decision tree policies for RL. We consider $\\mathbb { T }$ as the space of univariate decision trees (Silva et al. 2020). Extensions to multivariate decision trees, also known as oblique trees, are possible, but we leave them for future work. Given an RL environment with observations $x _ { 1 } , \\ldots , x _ { n }$ and discrete actions $a _ { 1 } , \\ldots , a _ { m }$ , we consider the library of Boolean expressions and actions given by $\\mathcal { L } \\ = \\ \\{ x _ { 1 } \\ < \\ \\beta _ { 1 } , \\ldots , x _ { n } \\ < \\ \\beta _ { n } , a _ { 1 } , \\ldots , a _ { m } \\}$ , where $\\beta _ { 1 } , \\ldots , \\beta _ { n }$ are the values of the observations that are used in the internal nodes of the decision tree. The evaluation operator $\\mathsf { e v a l : } T \\to \\mathbb { T }$ is defined as follows. We treat sequence $\\tau$ as the pre-order traversal of a decision tree, where the decision tokens $( x _ { n } < \\beta _ { n } )$ are treated as binary nodes and the action tokens $( a _ { n } )$ are treated as leaf nodes. For evaluating the decision tree, we start from the root node and follow direction\n\n![](images/487efeb168b8724ba3521ef3acec4b394f0d2c7fb8187ebfd4a0f19f20d6691a.jpg)  \nFigure 2: Reward of best solution versus number of function evaluations on a parameterized bitstring task, for two continuous optimization landscapes $f _ { 1 }$ and $f _ { 2 }$ and weights $\\alpha \\ : = \\ : 0 . 5 , 0 . 9$ . Solid line corresponds to weight $\\alpha = 0 . 9$ , dashed line $\\alpha = 0 . 5$ . Mean and standard error over 5 seeds.\n\n$$\nD _ { x _ { n } < \\beta _ { n } } ( x ) = { \\binom { \\mathrm { l e f t i f } x _ { n } < \\beta _ { n } { \\mathrm { ~ i s ~ T r u e , } } } { \\mathrm { r i g h t i f } x _ { n } < \\beta _ { n } { \\mathrm { ~ i s ~ F a l s e , } } } }\n$$\n\nfor every decision node encountered until we reach a leaf node. See Figure 3 for an example. The reward function is defined as $R ( t ) ~ = ~ \\mathbb { E } _ { r \\sim p _ { R } ( r | t ) } [ r ]$ where $p _ { R } ( r | t )$ is the reward distribution following policy $t$ in the environment. In practice, we use the average reward over $N$ episodes, i.e., $\\begin{array} { r } { R ( t ) \\ = \\ \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } r _ { i } } \\end{array}$ , where $\\boldsymbol { r } _ { i }$ is the reward obtained in episode $i$ . Prefix-dependent positional constraints for this problem are given in the appendix.\n\nSampling decision nodes in decision trees. To efficiently sample decision nodes, we employ truncated normal distributions to select parameters $\\beta _ { i }$ within permissible ranges. Many RL environments place boundaries on observations, and the use of the truncated normal distribution guarantees that parameters will only be sampled within those boundaries. Additionally, a decision node which is a child of another decision node cannot select parameters from the environment-enforced boundaries. This is because the threshold involved at a decision node changes the range of values which will be observed at subsequent decision nodes. In this way, a previous decision node ”dictates” the bounds on a current decision node. For instance, consider the decision tree displayed in Figure 3. Assume that the observation $x _ { 1 }$ falls within the interval [0, 5] (note that in practice the RL environment provided bounds are used to determine the interval), and the tree commences with the node $x _ { 1 } < 2$ . In the left child node, as $x _ { 1 } < 2$ is true, there is no need to evaluate whether $x _ { 1 }$ is less than 4 (or any number between 2 and 5), as that is already guaranteed. Consequently, we should sample a parameter $\\beta _ { 1 }$ within the range $( 0 , 2 )$ . Simultaneously, since we do not assess the Boolean expression regarding $x _ { 2 }$ , the bounds on $\\beta _ { 2 }$ remain consistent with those at the parent node. The parameter bounds for the remaining nodes are illustrated in Figure 3. The procedure for determining these maximum and minimum values is outlined in the appendix.\n\n![](images/c26193406a33e928aa8423724a6a7fab674076a929931fa9a23bbfb6f8aef29d.jpg)  \nFigure 3: Left: the decision tree associated with the traversal $\\langle x _ { 1 } < 2 , a _ { 2 } , x _ { 2 } < 6 , x _ { 1 } < 3 , a _ { 1 } , a _ { 3 } , a _ { 2 } \\rangle$ . Right: the corresponding bounds for the parameters during the sampling process (suppose the bounds for observations $x _ { 1 }$ and $x _ { 2 }$ are respectively [0, 5] and [1, 8]).\n\nEvaluation. For evaluation, we follow other works in the field (Silva et al. 2020; Ding et al. 2020; Custode and Iacca 2023) and use the OpenAI Gym’s (Brockman et al. 2016) environments MountainCar-v0, CartPole-v1, Acrobot-v1, and LunarLander-v2. We investigate the sample-efficiency of DisCo-DSO on the decision tree policy task when compared to the decoupled baselines described at the beginning of this section. We train each algorithm for 10 different random\n\n![](images/9771896f6b01efbeb0144088060a51af31478252563745edd6e47798683456e8.jpg)  \nFigure 4: Reward of the best solution versus number of function evaluations on the decision tree policy task for LunarLander-v2.   \nFigure 6: Average test set reward (left) and number of function evaluations (right) used across methods on the symbolic regression task. Recall that Decoupled-RL-BFGS and Decoupled-GP-BFGS correspond to the methods proposed in Petersen et al. (2021) and Koza (1994), respectively. DisCo-DSO achieves the best average reward on the test set at the lowest number of function evaluations.\n\nseeds.\n\nResults. In Figure 4 (see also the appendix), we report the mean and standard deviation of the best reward found by each algorithm versus number of environment episodes. These results show that DisCo-DSO dominates the baselines in terms of sample-efficiency. The trend is consistent across all environments, and is more pronounced in the more complex environments. The efficient use of evaluations by DisCo-DSO (each sample is a complete well-defined decision tree) versus the decoupled approaches, where each sample is a discrete skeleton that requires many evaluations to get a single complete solution, becomes a significant advantage in the RL environments where each evaluation involves running the environment for $N$ episodes.\n\nLiterature comparisons. We conduct a performance comparison of DisCo-DSO against various baselines in the literature, namely evolutionary decision trees as detailed in (Custode and Iacca 2023), cascading decision trees introduced in (Ding et al. 2020), and interpretable differentiable decision trees (DDTs) introduced in (Silva et al. 2020). In addition, we provide results with a BO baseline, where the structure of the decision tree is fixed to a binary tree of depth 4 without prefix-dependent positional constraints. Whenever a method provides a tree structure for a specific environment, we utilize the provided structure and assess it locally. In cases where the method’s implementation is missing, we address this by leveraging open-source code. This approach allows us to train a tree in absent environments, ensuring that we obtain a comprehensive set of results for all methods evaluated across all environments. The decision trees found by DisCo-DSO are shown in Figure 5 (see also the appendix). Comparisons are shown in Table 1. Methods we trained locally are marked with an asterisk $( ^ { * } )$ . Critically, we ensure consistent evaluation across baselines by assessing each decision tree policy on an identical set of 1,000 random seeds per environment.\n\nIn Table 1 we also show the complexity of the discovered decision tree as measured by the number of parameters\n\n![](images/f1f70215e1bb359ff92041f824b6a2fc5f2842fdf3eb6579fa9f1e4dfadd5620.jpg)  \nFigure 5: Topology of best decision trees found by DisCoDSO on the decision tree policy tasks for Acrobot-v1 and LunarLander-v2.\n\nDisCo-DSO 二 1 Decoupled-RL-BFGS Decoupled-RL-anneal Decoupled-GP-BFGS Decoupled-GP-evo Decoupled-GP-anneal 0.0 0.2 0.4 0.6 0.8 106 107 AverageRtestscore Number of function eval (log scale)\n\nin the tree. We count every (internal or leaf) node of univariate decision trees (produced by all methods except for Cascading decision trees) as one parameter. For Cascading decision trees, the trees contain feature learning trees and decision making trees. The latter is just univariate decision trees, so the same complexity measurement is used. For the leaf nodes of feature learning trees, the number of parameters is number of observations times number of intermediate features. From Table 1, we observe that the univariate decision trees found by DisCo-DSO have the best performance on all environments at a comparable or lower complexity than the other literature baselines.\n\n# Symbolic Regression for Equation Discovery\n\nProblem formulation. Symbolic regression (SR) (Koza 1994; Bongard and Lipson 2007; Petersen et al. 2021; Landajuela et al. 2021b; de Franca et al. 2024) is a classical discrete-continuous optimization problem with applications in many fields, including robotics, control, and machine learning. In SR, we have $\\begin{array} { r l } { \\mathcal { L } } & { { } = } \\end{array}$ $\\{ x _ { 1 } , \\dotsc , x _ { d } , + , - , \\times , \\div , \\sin , \\cos , \\dotsc \\}$ and $\\hat { \\mathcal { L } } = \\{ \\mathrm { c o n s t } ( \\beta ) \\}$ , where const $( \\beta )$ represents a constant with value $\\beta$ . The design space is a subset of the space of continuous functions, $\\bar { \\mathbb { T } } \\subset \\bar { C } ( V ^ { \\mathbb { R } } )$ , where $V \\subset \\mathbb { R } ^ { d }$ is the function support that depends on $\\mathcal { L }$ . The evaluation operator eval returns the function which expression tree has the sequence $\\tau$ as pre-order traversal (depth-first and then left-to-right). For example, $\\operatorname { e v a l } ( \\langle + , \\cos , y , \\times , \\cos ( 3 . 1 4 ) , \\sin , x \\rangle ) = \\cos ( y ) + 3 . 1 4 \\times$ $\\sin ( x )$ . Given a dataset $D ~ = ~ \\{ ( x _ { 1 } ^ { ( i ) } , \\ldots , x _ { d } ^ { ( i ) } , y ^ { ( i ) } ) \\} _ { i = 1 } ^ { N }$ the reward function is defined as the inverse of the normalized mean squared error (NMSE) between $\\boldsymbol y ^ { ( i ) }$ and $\\begin{array} { r c l } { \\mathsf { e v a l } ( \\tau ) ( x _ { 1 } ^ { ( i ) } , \\ldots , x _ { d } ^ { ( i ) } ) , \\forall i } & { \\in } & { \\{ 1 , \\ldots , N \\} } \\end{array}$ , computed as 1+N1MSE. SR has been shown to be NP-hard even for low-dimensional data (Virgolin and Pissis 2022). Prefixdependent positional constraints are given in the appendix.\n\nTable 1: Evaluation of the best univariate decision trees found by DisCo-DSO and other baselines on the decision tree policy task. Here, MR is the mean reward earned in evaluation over a set of 1,000 random seeds, while PC represents the parameter count in each tree. For models trained in-house $( ^ { * } )$ , the figures indicate the parameter count after the discretization process. †The topology of the tree is fixed for BO.   \n\n<html><body><table><tr><td rowspan=\"2\">Algorithm</td><td colspan=\"2\">Acrobot-v1</td><td colspan=\"2\">CartPole-v1</td><td colspan=\"2\">LunarLander-v2</td><td colspan=\"2\">MountainCar-vO</td></tr><tr><td>MR</td><td>PC</td><td>MR</td><td>PC</td><td>MR</td><td>PC</td><td>MR</td><td>PC</td></tr><tr><td>DisCo-DSO</td><td>-76.58</td><td>18</td><td>500.00</td><td>14</td><td>99.24</td><td>23</td><td>-100.97</td><td>15</td></tr><tr><td>Evolutionary DTs</td><td>-97.12*</td><td>5</td><td>499.58</td><td>5</td><td>-87.62*</td><td>17</td><td>-104.93</td><td>13</td></tr><tr><td>Cascading DTs</td><td>-82.14*</td><td>58</td><td>496.63</td><td>22</td><td>-227.02</td><td>29</td><td>-200.00</td><td>10</td></tr><tr><td>Interpretable DDTs</td><td>-497.86*</td><td>15</td><td>389.79</td><td>11</td><td>-120.38</td><td>19</td><td>-172.21*</td><td>15</td></tr><tr><td>Bayesian Optimization†</td><td>-90.99*</td><td>7</td><td>85.47*</td><td>7</td><td>-112.14*</td><td>7</td><td>-200.0*</td><td>7</td></tr></table></body></html>\n\nEvaluation. A key evaluation metric for symbolic regression is the parsimony of the discovered equations, i.e., the balance between the complexity of the identified equations and their ability to fit the data. A natural way to measure it is to consider the generalization performance over a test set. A SR method could find symbolic expressions that overfit the training data (using for instance overly complex expressions), but those expressions will not generalize well to unseen data. For evaluating the generalization performance of various baselines, we rely on the benchmark datasets detailed in the appendix.\n\nResults. Results in Figure 6 demonstrate the superior efficiency and generalization capability of DisCo-DSO in the SR setting. In particular, DisCo-DSO achieves the best average reward on the test set and the lowest number of function evaluations. Note that for DisCo-DSO we have perfect control over the number of function evaluations as it is determined by the number of samples $1 0 ^ { 6 }$ in this case). The Decoupled-GP methods exhibit a strong tendency to overfit to the training data and perform poorly on the test set. This phenomenon is known as the bloat problem in the SR literature (Silva and Costa 2009). We observe that the joint optimization of DisCo-DSO avoids this problem and achieve the best generalization performance.\n\nLiterature comparisons. In Table 2, we compare DisCoDSO against state-of-the-art methods in the SR literature. In addition to the baselines (Petersen et al. 2021; Koza 1994) described above, we compare against the methods proposed in Biggio et al. (2021) and Kamienny et al. (2022). Since the method in Biggio et al. (2021) is only applicable to $\\leq$ 3 dimensions, we consider the subset of benchmarks with $\\leq 3$ dimensions. We observe that DisCo-DSO dominates all baselines in terms of average reward on the full test set. For the subset of benchmarks with $\\leq 3$ dimensions, DisCoDSO achieves comparative performance to the specialized method in Biggio et al. (2021).\n\nTable 2: Comparison of DisCo-DSO against decoupled baselines and the methods proposed in Biggio et al. (2021) and Kamienny et al. (2022) on the symbolic regression task. Values are mean $\\pm$ standard deviation of the reward across benchmarks (provided in the appendix). We group benchmarks because Biggio et al. (2021) is only applicable to $\\leq 3$ dimensions. ⋆ Petersen et al. (2021). ⋆⋆ Koza (1994).   \n\n<html><body><table><tr><td>Algorithm</td><td>Dim≤3</td><td>Dim ≥1</td></tr><tr><td>DisCo-DSO</td><td>0.6632±0.3194</td><td>0.7045 ± 0.3007</td></tr><tr><td>Decoupled-RL-BFGS*</td><td>0.6020 ± 0.4169</td><td>0.6400 ± 0.3684</td></tr><tr><td>Decoupled-RL-evo</td><td>0.0324 ± 0.1095</td><td>0.0969± 0.2223</td></tr><tr><td>Decoupled-RL-anneal</td><td>0.1173 ± 0.2745</td><td>0.1436 ± 0.3015</td></tr><tr><td>Decoupled-GP-BFGS**</td><td>0.5372 ± 0.4386</td><td>0.4953 ± 0.4344</td></tr><tr><td>Decoupled-GP-evo</td><td>0.0988 ± 0.1975</td><td>0.0747 ± 0.1763</td></tr><tr><td>Decoupled-GP-anneal</td><td>0.1615 ± 0.2765</td><td>0.1364 ± 0.2608</td></tr><tr><td>Kamienny et al. (2022)</td><td>0.6068 ± 0.1650</td><td>0.5699 ± 0.1065</td></tr><tr><td>Biggio et al. (2021)</td><td>0.6858 ± 0.1995</td><td>N/A</td></tr></table></body></html>\n\n# Conclusion\n\nWe proposed DisCo-DSO (Discrete-Continuous Deep Symbolic Optimization), a novel approach to optimization in hybrid discrete-continuous spaces. DisCo-DSO uses a generative model to learn a joint distribution on discrete and continuous design variables to sample new hybrid designs. In contrast to standard decoupled approaches, in which the discrete skeleton is sampled first, and then the continuous variables are optimized separately, our joint optimization approach samples both discrete and continuous variables simultaneously. This leads to more efficient use of objective function evaluations, as the discrete and continuous dimensions of the design space can “communicate” with each other and guide the search. We have demonstrated the benefits of DisCoDSO in challenging problems in symbolic regression and decision tree optimization, where, in particular, DisCo-DSO outperforms the state-of-the-art on univariate decision tree policy optimization for RL.\n\nRegarding the limitations of DisCo-DSO, it is important to note that the method relies on domain-specific information to define the ranges of continuous variables. In cases where this information is not available and estimates are necessary, the performance of DisCo-DSO could be impacted. Furthermore, in our RL experiments, we constrain the search space to univariate decision trees. Exploring more complex search spaces, such as multivariate or “oblique” decision trees, remains an avenue for future research.\n\n# Acknowledgments\n\nThis manuscript has been authored by Lawrence Livermore National Security, LLC under Contract No. DE-AC52- 07NA2 7344 with the US. Department of Energy. The United States Government retains, and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. We thank Livermore Computing and the Laboratory Directed Research and Development program (21-SI-001) for their support. Release code is LLNL-CONF-854776.",
    "institutions": [
        "Lawrence Livermore National Laboratory"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文聚焦于混合离散 - 连续和可变长度空间内的黑盒优化问题，此问题在决策树学习和符号回归等众多应用中普遍存在。现有方法多采用解耦优化，离散和连续变量分开处理，效率欠佳，且难以应对具有前缀依赖位置约束的可变长度问题。同时，现有的联合优化方法，如混合整数规划（MIP）和混合贝叶斯优化（BO），不适用于具有可变长度结构和前缀依赖位置约束的问题，在效率、处理不可微目标和搜索引导等方面存在局限。\\n\\n**方法概述**\\n提出DisCo - DSO（Discrete - Continuous Deep Symbolic Optimization）方法，借助生成模型学习离散和连续设计变量的联合分布，实现对新混合设计的采样，达成离散和连续变量的联合优化。\\n\\n**主要贡献与效果**\\n- 提出适用于具有前缀依赖位置约束的可变长度搜索空间的黑盒混合优化问题的联合离散 - 连续优化方法DisCo - DSO。在决策树策略优化和符号回归等任务中，该方法更高效地利用目标函数评估，样本效率显著提升。例如，在参数化比特串任务中，当离散解对整体奖励贡献更大时，DisCo - DSO的样本效率远超解耦方法；在决策树策略任务中，DisCo - DSO在所有环境下的样本效率均优于基线，在复杂环境中优势更为突出；在符号回归任务中，DisCo - DSO以最少的函数评估次数，在测试集上取得了最佳平均奖励。在全测试集平均奖励方面，DisCo - DSO优于所有基线，对于维度 ≤ 3 的基准子集，平均奖励为 0.6632 ± 0.3194，与Biggio等人（2021）的专门方法（0.6858 ± 0.1995）相当，在维度 ≥ 1 的基准上平均奖励为 0.7045 ± 0.3007，表现更优。\\n- 提出将控制任务中的决策树策略搜索表示为顺序离散 - 连续优化的新公式，并给出了顺序查找决策节点参数范围界限的方法。\\n- 对DisCo - DSO在可解释控制策略和符号回归等多种任务上进行了全面的实证评估，结果显示DisCo - DSO在所有任务上均超越解耦方法。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n基于深度强化学习，利用生成模型学习离散和连续设计变量的联合分布，实现离散和连续变量的联合优化。这种方式使设计空间的离散和连续维度能够相互“交流”，从而更高效地利用目标函数评估来引导搜索，避免陷入局部最优，并借助先前样本的信息指导后续搜索。\\n\\n**创新点**\\n现有深度学习方法大多仅支持离散令牌，或完全解耦离散和连续部分的问题。DisCo - DSO实现了离散和连续变量的联合优化，可同时对二者进行采样。此外，针对具有前缀依赖位置约束和可变长度结构的问题，提出了有效的解决方案，而现有联合优化方法如混合整数规划（MIP）和混合贝叶斯优化（BO）无法处理此类问题。\\n\\n**具体实现步骤**\\n1. **问题定义**：定义离散 - 连续优化问题，涵盖搜索空间、令牌库、离散骨架、评估算子、奖励函数等。搜索空间 $\\tau$ 由令牌序列构成，令牌库 $\\mathcal{L}$ 包含 $K$ 个令牌，部分令牌有连续参数。优化目标是找到使奖励函数最大化的序列，奖励函数 $R$ 可以是确定性或随机性的。\\n2. **组合优化与自回归模型**：在设计空间 $\\tau$ 上学习概率模型 $p(\\tau)$，该模型通常由具有参数 $\\theta$ 的自回归（AR）模型参数化。在位置 $i$ 处，模型根据先前生成的令牌 $\\tau_{1:(i - 1)}$ 发出对数几率向量 $\\psi^{(i)} = AR(\\tau_{1:(i - 1)}; \\theta)$，新令牌 $\\tau_i$ 从分布 $p(\\tau_i | \\tau_{1:(i - 1)}, \\theta) = softmax(\\psi^{(i)})_{\\mathcal{L}(\\tau_i)}$ 中采样，随后添加到序列中以条件生成下一个令牌，直至满足停止条件。不同的模型架构（如循环神经网络和具有因果注意力的变压器）可用于生成对数几率。\\n3. **处理前缀依赖位置约束**：给定前缀 $\\tau_{1:(i - 1)}$，计算先验 $\\psi_{\\circ}^{(i)} \\in \\mathbb{R}^{|\\mathcal{L}|}$，使不可行集 $\\mathscr{C}_{\\tau_{1:(i - 1)}}$ 中的令牌的 $\\psi_{\\circ}^{(i)}_{\\mathcal{L}(\\tau_i)} = -\\infty$，其余为零。在采样令牌 $\\tau_i$ 之前，将该先验添加到对数几率 $\\psi^{(i)}$ 中。\\n4. **扩展到离散 - 连续优化**：模型扩展为为每个令牌 $\\tau_i = (l_i, \\beta_i)$ 发出两个输出 $\\psi^{(i)}$ 和 $\\phi^{(i)}$，即 $(\\psi^{(i)}, \\phi^{(i)}) = AR((l, \\beta)_{1:(i - 1)}; \\theta)$。给定令牌 $(l, \\beta)_{1:(i - 1)}$，第 $i$ 个令牌 $(l_i, \\beta_i)$ 根据离散令牌 $l_i$ 是否属于 $\\bar{\\mathcal{L}}$ 从不同分布采样：若 $l_i \\in \\bar{\\mathcal{L}}$，$p((l_i, \\beta_i) | (l, \\beta)_{1:(i - 1)}, \\theta) = \\mathcal{U}_{[0, 1]}(\\beta_i)softmax(\\psi^{(i)})_{\\mathcal{L}(l_i)}$；若 $l_i \\in \\hat{\\mathcal{L}}$，$p((l_i, \\beta_i) | (l, \\beta)_{1:(i - 1)}, \\theta) = \\mathcal{D}(\\beta_i | l_i, \\phi^{(i)})softmax(\\psi^{(i)})_{\\mathcal{L}(l_i)}$，其中 $\\mathcal{D}(\\beta_i | l_i, \\phi^{(i)})$ 是用于从 $\\phi^{(i)}$ 采样 $\\beta_i$ 的分布的概率密度函数。在采样令牌 $\\tau_i$ 之前可添加额外先验 $(\\psi_{\\circ}^{(i)}, 0)$。\\n5. **训练DisCo - DSO**：通过最大化预期奖励 $\\mathcal{J}(\\theta) = \\mathbb{E}_{\\tau \\sim p(\\tau | \\theta)}[R(\\tau)]$ 或分位数条件预期奖励 $J_{\\varepsilon}(\\theta) = \\mathbb{E}_{\\tau \\sim p(\\tau | \\theta)}[R(\\tau) | R(\\tau) \\geq R_{\\varepsilon}(\\theta)]$ 来学习模型参数 $\\theta$，其中 $R_{\\varepsilon}(\\theta)$ 是从轨迹分布 $p(\\tau | \\theta)$ 采样的奖励分布 $R(\\tau)$ 的 $(1 - \\varepsilon)$ - 分位数。为优化 $J_{\\varepsilon}(\\theta)$，扩展了风险寻求策略梯度，其梯度公式为 $\\nabla_{\\theta} J_{\\varepsilon}(\\theta) = \\mathbb{E}_{\\tau \\sim p(\\tau | \\theta)}[A(\\tau, \\varepsilon, \\theta)S((l, \\beta)_{1:T}) | A(\\tau, \\varepsilon, \\theta) > 0]$，其中 $A(\\tau, \\varepsilon, \\theta) = R(\\tau) - R_{\\varepsilon}(\\theta)$，$S((l, \\beta)_{1:T})$ 根据 $l_i$ 是否属于 $\\bar{\\mathcal{L}}$ 有不同的计算方式。\\n\\n**案例解析**\\n- **参数化比特串任务**：设计了通用灵活的参数化比特串基准问题 $\\mathrm{PB}(N, f, l^*, \\beta^*)$，任务是恢复隐藏的 $T$ 位字符串 $l^*$ 和参数向量 $\\beta^*$。每个比特 $l_i^*$ 通过奖励函数 $R$ 与参数 $\\beta_i^*$ 配对，仅当在位置 $i$ 选择正确的比特 $l_i^*$ 时，奖励函数才会根据目标函数 $f(\\beta_i, \\beta_i^*) \\in [0, 1]$ 给出正值，奖励函数 $R(\\tau, \\beta) = \\frac{1}{T}\\sum_{i = 1}^{T}\\mathbf{1}_{\\tau_i = \\tau_i^*}(\\alpha + (1 - \\alpha)f(\\beta_i, \\beta_i^*))$，标量 $\\alpha \\in [0, 1]$ 控制优化离散或连续部分奖励的计算努力的相对重要性。通过改变比特串长度 $T$ 和目标函数 $f$ 的非线性程度可控制问题难度，实验中测试了具有多个次优局部最大值 $(f_1)$ 和不连续目标景观 $(f_2)$ 的目标函数。\\n- **决策树策略任务**：考虑发现强化学习中的决策树策略问题，将 $\\mathbb{T}$ 视为单变量决策树的空间。给定强化学习环境的观测 $x_1, \\ldots, x_n$ 和离散动作 $a_1, \\ldots, a_m$，定义库 $\\mathcal{L} = \\{x_1 < \\beta_1, \\ldots, x_n < \\beta_n, a_1, \\ldots, a_m\\}$，将序列 $\\tau$ 视为决策树的前序遍历，决策令牌 $(x_n < \\beta_n)$ 视为二叉节点，动作令牌 $(a_n)$ 视为叶节点。在评估决策树时，从根节点开始，根据决策节点的条件选择路径，直到到达叶节点。为高效采样决策节点，使用截断正态分布在允许范围内选择参数 $\\beta_i$，并根据父节点的决策更新子节点的参数范围。例如，若根节点为 $x_1 < 2$，左子节点的参数 $\\beta_1$ 应在 $(0, 2)$ 范围内采样。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- Decoupled - RL - BFGS、Decoupled - RL - anneal、Decoupled - RL - evo：使用强化学习训练生成模型产生离散骨架，然后用下游非线性求解器（L - BFGS - B、模拟退火、差分进化）优化连续参数。\\n- Decoupled - GP - BFGS、Decoupled - GP - evo、Decoupled - GP - anneal：使用遗传编程产生离散骨架，然后用下游非线性求解器优化连续参数。\\n- BO：在参数化比特串任务中，使用预期改进作为获取函数的贝叶斯优化方法。\\n- 其他文献基线：进化决策树（Evolutionary DTs）、级联决策树（Cascading DTs）、可解释可微决策树（Interpretable DDTs）。\\n\\n**性能对比**\\n*   **在 [样本效率/Sample Efficiency] 指标上：** 在参数化比特串任务中，当离散解对整体奖励贡献更大时，DisCo - DSO显著优于解耦方法。每个DisCo - DSO生成的样本都是完整解，只需一次函数评估即可获得奖励，而基线解耦方法生成的每个样本只有离散骨架，需要多次函数评估才能得到一个完整解。贝叶斯优化（BO）的计算复杂度为 $\\mathcal{O}(n^3)$，在给定相同计算预算下，其函数评估次数少于其他方法，最终结果也不如DisCo - DSO。在决策树策略任务中，DisCo - DSO在所有环境中均在样本效率上优于解耦基线，且在更复杂的环境中优势更明显。例如在LunarLander - v2环境中，DisCo - DSO以更少的环境回合数获得了更高的最佳奖励。\\n*   **在 [决策树性能/Decision Tree Performance] 指标上：** 在决策树策略任务中，与进化决策树、级联决策树、可解释可微决策树和贝叶斯优化等基线方法相比，DisCo - DSO发现的单变量决策树在所有环境中均具有最佳性能，且复杂度相当或更低。例如在Acrobot - v1环境中，DisCo - DSO的平均奖励为 - 76.58，优于进化决策树的 - 97.12、级联决策树的 - 82.14、可解释可微决策树的 - 497.86和贝叶斯优化的 - 90.99；在CartPole - v1环境中，DisCo - DSO达到了500.00的平均奖励，与其他基线相当或更优。\\n*   **在 [符号回归性能/Symbolic Regression Performance] 指标上：** 在符号回归任务中，DisCo - DSO以最少的函数评估次数获得了测试集上的最佳平均奖励。与解耦基线方法（Decoupled - RL - BFGS、Decoupled - RL - evo、Decoupled - RL - anneal、Decoupled - GP - BFGS、Decoupled - GP - evo、Decoupled - GP - anneal）和其他方法（Kamienny et al. (2022)、Biggio et al. (2021)）相比，在全测试集上的平均奖励方面，DisCo - DSO优于所有基线。对于维度 ≤ 3 的子集基准，DisCo - DSO的平均奖励为 0.6632 ± 0.3194，与Biggio et al. (2021) 的0.6858 ± 0.1995 相当，且在维度 ≥ 1 的基准上表现更优，平均奖励为 0.7045 ± 0.3007。解耦方法中的Decoupled - GP方法有过拟合训练数据的强烈趋势，在测试集上表现不佳，而DisCo - DSO避免了该问题，实现了最佳的泛化性能。\",\n    \"keywords\": \"### 关键词\\n\\n- 混合离散 - 连续优化 (Hybrid Discrete - Continuous Optimization, N/A)\\n- 深度符号优化 (Deep Symbolic Optimization, N/A)\\n- 离散 - 连续深度符号优化 (Discrete - Continuous Deep Symbolic Optimization, DisCo - DSO)\\n- 决策树优化 (Decision Tree Optimization, N/A)\\n- 符号回归 (Symbolic Regression, SR)\\n- 生成模型 (Generative Model, N/A)\\n- 强化学习 (Reinforcement Learning, RL)\"\n}"
}