{
    "link": "https://arxiv.org/abs/2412.14995",
    "pdf_link": "https://arxiv.org/pdf/2412.14995",
    "title": "HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs",
    "authors": [
        "Pham Vu Tuan Dat",
        "Long Doan",
        "Huynh Thi Thanh Binh"
    ],
    "publication_date": "2024-12-19",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 12,
    "influential_citation_count": 4,
    "paper_content": "# HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs\n\nPham Vu Tuan Dat1, Long Doan2, Huynh Thi Thanh Binh1\n\n1Hanoi University of Science and Technology, Hanoi, Viet Nam 2George Mason University, Virginia, United States dat.pvt210158@sis.hust.edu.vn, ldoan5 $@$ gmu.edu, binhht@soict.hust.edu.vn\n\n# Abstract\n\nAutomatic Heuristic Design (AHD) is an active research area due to its utility in solving complex search and NP-hard combinatorial optimization problems in the real world. The recent advancements in Large Language Models (LLMs) introduce new possibilities by coupling LLMs with evolutionary computation to automatically generate heuristics, known as LLMbased Evolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained great performance on various tasks, there is still a gap in understanding the properties of heuristic search spaces and achieving a balance between exploration and exploitation, which is a critical factor in large heuristic search spaces. In this study, we address this gap by proposing two diversity measurement metrics and perform an analysis on previous LLM-EPS approaches, including FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal that while EoH demonstrates higher diversity than FunSearch and ReEvo, its objective score is unstable. Conversely, ReEvo’s reflection mechanism yields good objective scores but fails to optimize diversity effectively. With this finding in mind, we introduce HSEvo, an adaptive LLMEPS framework that maintains a balance between diversity and convergence with a harmony search algorithm. Through experimentation, we find that HSEvo achieved high diversity indices and good objective scores while remaining costeffective. These results underscore the importance of balancing exploration and exploitation and understanding heuristic search spaces in designing frameworks in LLM-EPS.\n\n# Code — https://github.com/datphamvn/HSEvo Extended version — https://arxiv.org/pdf/2412.14995\n\n# 1 Introduction\n\nHeuristics are widely utilized to address complex search and NP-hard combinatorial optimization problems (COPs) in real-world systems. Over the past few decades, significant efforts have been made to develop efficient heuristics, resulting in the creation of many meta-heuristic methods such as simulated annealing, tabu search, and iterated local search. These meticulously crafted methods have been effectively applied across various practical systems.\n\nNevertheless, varied applications with distinct constraints and goals may necessitate different algorithms or algorithm setups. The manual creation, adjustment, and configuration of a heuristic for a specific problem can be extremely laborious and requires substantial expert knowledge. This is a bottleneck in many application domains. To address this issue, Automatic Heuristic Design (AHD) has been proposed aims to selects, tunes, or constructs effective heuristics for a given problem class automatically (Choong, Wong, and Lim 2018). Various approaches have been used in AHD, including Hyper-Heuristics (HHs) (Pillay and Qu 2018) and Neural Combinatorial Optimization (NCO) (Qu, Kendall, and Pillay 2020). However, HHs are still constrained by heuristic spaces that are predefined by human experts. Additionally, NCO faces limitations related to the necessity for effective inductive bias (Drakulic et al. 2024), and challenges regarding interpretability and generalizability (Liu et al. 2023).\n\nRecently, the rise of Large Language Models (LLMs) has opened up new possibilities for AHD. It is believed that LLMs (Nejjar et al. 2023; Austin et al. 2021) could be a powerful tool for generating new ideas and heuristics. However, standalone LLMs with prompt engineering can be insufficient for producing novel and useful ideas beyond existing knowledge (Mahowald et al. 2024). Some attempts have been made to coupling LLMs with evolutionary computation to automatically generate heuristics, known as LLM-based Evolutionary Program Search (LLM-EPS) (Liu et al. 2024b; Meyerson et al. 2024; Chen, Dohan, and So 2024). Initial works such as FunSearch (Romera-Paredes et al. 2024) and subsequent developments like Evolution of Heuristic (EoH) (Liu et al. 2024a) and Reflective Evolution (ReEvo) (Ye et al. 2024a) have demonstrated significant improvements over previous approaches, generating quality heuristics that often surpass current methods. Even so, ReEvo yields state-of-the-art and competitive results compared with evolutionary algorithms, neural-enhanced metaheuristics, and neural solvers.\n\nA key difference between LLM-EPS and classic AHD lies in their heuristic search spaces. Classic AHD typically operates in well-defined mathematical spaces such as $R ^ { n }$ , while LLM-EPS searches within the space of functions, where each function represents a heuristic as a program. LLM-EPS utilizes LLMs within an evolutionary framework to enhance the quality of generated functions iteratively. Therefore, it is crucial to study and understand the search spaces of heuristics to establish foundational theories and principles for the automatic design of algorithms. This aspect has been largely unconsidered in previous LLM-EPS frameworks.\n\n![](images/8e002562b1da452c898305f7ca94c0e2826fdb834bc6c9b5d999bda067061a3c.jpg)  \nFigure 1: Diversity indices and objective scores of ReEvo framework on BPO problem through different runs.\n\nIn this paper, we introduce two diversity metrics inspired by the Shannon entropy to monitor population diversity. We also explore relationships between our proposed diversity metrics with objective performance of LLM-EPS frameworks on different optimization problems. Through our analysis, we gain an understanding on the characteristics and properties of heuristic search spaces in LLM-EPS frameworks. Finally, building on these foundational principles, we propose a new framework called HSEvo, which incorporates a new components based on harmony search algorithm (Shi, Han, and Si 2012) and enhances other components, including initialization, crossover, and mutation, to optimize objective scores and diversity metrics.\n\nIn summary, our contributions are as follows:\n\n• Two diversity measurement metrics, the Shannon–Wiener Diversity Index and the Cumulative Diversity Index, are used to evaluate the evolutionary progress of populations within the LLM-EPS framework. • A novel framework, HSEvo, that aims to balance between the diversity and objective performance to improve the optimization process.\n\n# 2 Background and Related Works\n\n# 2.1 LLM-Based Evolutionary Program Search\n\nRecent advances in LLM-EPS have shown promising results in AHD. Evolutionary methods have been adopted in both code generation (Nejjar et al. 2023; Ma et al. 2023; Hemberg, Moskal, and O’Reilly 2024) and text generation (Guo et al. 2023; Yuksekgonul et al. 2024). Notable among these methods are FunSearch, EoH, and ReEvo. FunSearch employs an island-based evolution strategy, leveraging LLMs like Codey and StarCoder to evolve heuristics for mathematical and COPs, outperforming traditional methods on tasks such as the cap set and admissible set problems. EoH utilizes genetic algorithm with Chain of Thought prompt engineering, consistently achieving superior performance in the traveling salesman and online bin packing problems. ReEvo introduces a reflective component to the evolution process, employing two instances of GPT-3.5 to generate and refine heuristics, which has demonstrated effectiveness across various optimization tasks. These methods highlight the potential of integrating LLMs with evolutionary strategies to enhance the efficiency and effectiveness of AHD solutions.\n\n# 2.2 Diversity in Evolutionary Computation\n\nDiversity plays a pivotal role in enhancing the efficiency of algorithms in multi-objective optimization within the domain of evolutionary computation (Solteiro Pires, Tenreiro Machado, and de Moura Oliveira 2014). Numerous studies have investigated various methods to measure and maintain diversity within populations, as it critically impacts the convergence and overall performance of these algorithms (Pires, Tenreiro Machado, and Moura Oliveira 2019; Wang and Chen 2012). Among these, the application of Shannon entropy has been particularly prominent in quantifying diversity and predicting the behavior of genetic algorithms. However, to the best of our knowledge, no existing studies have thoroughly explored diversity within the context of LLM-EPS. This gap in the literature motivates our in-depth exploration of diversity in LLM-EPS frameworks.\n\n# 3 Diversity Measurement Metrics\n\nIn this section, we introduce a method to encode LLMEPS population and propose two diversity metrics, ShannonWiener diversity index (SWDI) and cummulative diversity index (CDI). We also conduct a diversity analysis on previous LLM-EPS frameworks, FunSearch, EoH and ReEvo.\n\n# 3.1 Population Encoding\n\nOne particular problem of measuring diversity in LLM-EPS is how to encode the population. While each individual in traditional evolutionary algorithm is encoded as a vector, in LLM-EPS they are usually presented as a string of code snippet/program. This poses a challenge in applying previous diversity metrics on population of LLM-EPS frameworks. To tackle this issue, we suggest an encoding approach consists of three steps: (i) removing comments and docstrings using abstract-syntax tree, (ii) standardizing code snippets into a common coding style (e.g., PEP81), (iii) convert code snippets to vector representations using a code embedding model.\n\n![](images/3b867af871e84231e4e10854e54a6370ea5035d2c29d7d046c5d951064f664ba.jpg)  \nFigure 2: CDI and objective scores of previous LLM-EPS on different AHD problems.\n\n# 3.2 Shannon–Wiener Diversity Index\n\nInspired by ecological studies, SWDI (Nolan and Callahan 2006) provides a quantitative measure of species diversity within a community. In the context of search algorithms, this index aims to quantify the diversity of the population at any given time step based on clusters of individuals. To compute SWDI for a given set of individuals within a community, or archive, first, we need to determine the probability distribution of individuals across clusters. This is represented as:\n\n$$\np _ { i } = \\frac { | C _ { i } | } { M }\n$$\n\nwhere $C _ { i }$ is a cluster of individuals and $M$ represents total number of individuals across all clusters $\\{ C _ { 1 } , \\setminus \\cdot \\cdot , C _ { N } \\}$ . The SWDI, $H ( X )$ , is then calculated using Shannon entropy:\n\n$$\nH ( X ) = - \\sum _ { i = 1 } ^ { N } p _ { i } \\log ( p _ { i } )\n$$\n\nThis index serves as a crucial metric in maintaining the balance between exploration and exploitation within heuristic search spaces. A higher index score suggests a more uniform distribution of individuals across the search space, thus promoting exploration. Conversely, a lower index score indicates concentration around specific regions, which may enhance exploitation but also increase the risk of premature convergence.\n\nTo obtain SWDI score, at each time step $t$ , we add all individuals $R _ { i } = \\{ r _ { 1 } , . . . , r _ { n } \\}$ generated at time step $t$ to an archive. We encode the archive using our proposed population encoding in Section 3.1 to obtain their vector representations $\\mathcal { V } \\bar { = } \\{ v _ { 1 } , \\ldots , v _ { n } \\}$ . After that, we compute the similarity between two embedding vectors $v _ { i }$ and $v _ { j }$ using cosine similarity:\n\n$$\n\\mathrm { s i m i l a r i t y } ( v _ { i } , v _ { j } ) = \\frac { v _ { i } \\cdot v _ { j } } { \\| v _ { i } \\| \\| v _ { j } \\| }\n$$\n\nTo find clusters in the archive, we consider each embedding vector $\\boldsymbol { v } _ { i }$ . We assign $v _ { i }$ to a cluster $C _ { i }$ if the similarity between $\\boldsymbol { v } _ { i }$ and all members in $C _ { i }$ is greater than a threshold $\\alpha$ . Mathematically, $v _ { i }$ is assigned to $C _ { i }$ if\n\n$$\n\\begin{array} { r } { \\operatorname { s i m i l a r i t y } ( v _ { i } , v _ { k } ) \\ge \\alpha \\quad \\forall k \\in \\{ 1 , \\dots , | C _ { i } | \\} } \\end{array}\n$$\n\nIf no cluster $C _ { i } \\in \\{ C _ { 1 } , . . . , C _ { N } \\}$ can satisfy the above condition, we create a new cluster $C _ { N + 1 }$ and assign $\\boldsymbol { v } _ { i }$ to $C _ { N + 1 }$ . Finally, we compute SWDI score by computing Eq. (1), (2) across found clusters.\n\n# 3.3 Cumulative Diversity Index\n\nWhile SWDI focuses on diversity of different groups of individuals, the Cumulative Diversity Index (CDI) plays a crucial role in understanding the spread and distribution of the whole population within the search space (Jost 2006). In the context of heuristic search, the CDI measures how well a system’s energy, or diversity, is distributed from a centralized state to a more dispersed configuration.\n\nTo calculate the CDI, we also consider all individuals within an archive, represented by their respective embeddings. We construct a minimum spanning tree (MST) that connects all individuals within the archive $A$ , where each connection between individuals is calculated using Euclidean distance. This MST provides a structure to assess the diversity of the population. Let $d _ { i }$ represent the distance of an edge within the MST, where $i \\in \\{ 1 , 2 , . . . , \\# A - 1 \\}$ . The probability distribution of these distances is given by:\n\n$$\np _ { i } = \\frac { d _ { i } } { \\sum _ { j = 1 } ^ { \\# A - 1 } d _ { j } }\n$$\n\nThe cumulative diversity is then calculated using the Shannon entropy:\n\n$$\nH ( X ) = - \\sum _ { i = 1 } ^ { \\# A - 1 } p _ { i } \\log ( p _ { i } )\n$$\n\nThis approach allows us to capture the overall diversity of the search space, providing insights into the spread of solutions. Higher CDI values indicate a more distributed and diverse population, which is essential for maintaining a robust search process.\n\n![](images/77765a5558dcacf26e12bb7dca2591ed0872013aa604d390a7e1d6b398fa9e0e.jpg)  \nFigure 3: Overview of the HSEvo framework.\n\nAn interesting point in Shannon diversity index (SDI) theory is that normalizing the SDI with the natural log of richness is equivalent to the evenness value $\\frac { H ^ { \\prime } } { \\ln ( S ) }$ where $H ^ { \\prime }$ represents the richness of SDI and $S$ is the total number of possible categories (Heip et al. 1998). The significance of evenness is to demonstrate how the proportions of $p _ { i }$ are distributed. Now, consider normalizing with the two current diversity indices. First, with SWID, if we have $N$ clusters and the number of individuals in each cluster from $C _ { 1 }$ to $C _ { N }$ is equal, then evenness will always be 1, regardless of the value of $N$ . This is not the desired behavior since we consider the number of clusters to be a component of diversity. Similarly, normalizing CDI will also ignore the impact of the number of candidate heuristics, which correspond to the nodes in the MST. This leads to the proposal not to normalize the SWID and CDI metrics in order to achieve a [0, 1] bound.\n\n# 3.4 Exploring the Correlation Between Objective Score and Diversity Measurement Metrics\n\nTo examine the correlation between the two diversity measurement metrics and objective score, we conducted three experimental runs using ReEvo on bin-packing online problem (Seiden 2002). The experiment details can be found in the Extended version - Appendix. The objective scores and the two diversity metrics from these runs are presented in Fig. 1. Additional results for other tasks are also available in the Extended version - Appendix.\n\nFrom the figure, there are a few observations that we can drawn. First, the two diversity measurement metrics have a noticeable correlation on the objective scores of the problem. When the objective score is converged into a local optima, the framework either try to focus on the exploration by increasing the diversity of the population, through the increase of SWDI in first and second run, or try to focus on the exploitation of current population, thus decrease the SWDI in the third run. We can also see that focusing on exploration can lead to significant improvement on the objective score, while focusing on exploitation can make the population stuck in local optima for a longer time. However, if the whole population is not diverse enough, as can be seen with the low CDI of the second run, the population might not be able to escape the local optima.\n\n# 3.5 Diversity Analysis on Previous LLM-EPS Framework\n\nTo analyse the overall diversity of previous LLM-EPS frameworks, we conduct experiments on three different LLM-EPS frameworks, including FunSearch (RomeraParedes et al. 2024), ReEvo (Ye et al. 2024a), and EoH (Liu et al. 2024a), on three distinct AHD problems, bin-packing online (BPO), traveling salesmen problem with guided local search solver (TSP) (Voudouris and Tsang 1999), and orienteering problem with ACO solver (OP) (Ye et al. 2024b). The details of each experiment are presented in the Extended version - Appendix. Note that, as highlighted in the previous section, SWDI focuses on understanding the diversity of the population during a single run. As such, it may not be helpful for quantifying the diversity across multiple experiment runs. Fig. 2 presents the experiment results.\n\nIn BPO and TSP, EoH obtain the highest CDI but got the worst objective score. This implies that EoH does not focus enough on exploitation to optimize the population better. In contrast, while ReEvo and FunSearch obtain lower CDI than EoH on BPO and TSP, they achieve a better objective performance on all three problems. The experiments on BPO and TSP problems highlight the inherent trade-off between diversity and objective performance of LLM-EPS frameworks. However, on OP problem, we can see that a high CDI is required to obtain a better objective score, which align with our findings in Section 3.4.\n\n1 import numpy as np   \n2   \n3 def heuristics_v2(prize: np.ndarray,   \n4 distance: np.ndarray, maxlen: float) -> np.   \nndarray:   \n5 reward_distance_ratio $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ prize / distance   \n6 cost_penalty $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ np.exp(-distance)   \n7 heuristics $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ (prize \\* prize[:, np.   \nnewaxis])   \n8 heuristics[distance $>$ maxlen] = 0   \n9   \n10 return heuristics   \n1 import numpy as np   \n2   \n3 def heuristics_v2(prize: np.ndarray,   \n4 distance: np.ndarray, maxlen: float,   \n5 reward_threshold: float $\\mathit { \\Theta } = \\mathit { \\Theta } 0$ ,   \n6 distance_threshold: float $\\mathit { \\Theta } = \\mathit { \\Theta } 0$ ,   \n7 cost_penalty_weight: float $\\mathit { \\Theta } = \\mathit { \\Theta } 1$ ) $\\mathrel { - } >$ np.ndarray:   \n8 reward_distance_ratio $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ prize / distance   \n9 cost_penalty $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ np.exp(-distance)   \n10 heuristics $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ (prize $\\star$ prize[:, np.newaxis]) / (distance   \ndistance) $\\star$ cost_penalty   \n11 heuristics[(distance $>$ maxlen) | (reward_distance_ratio $\\prec$   \nreward_threshold) | (distance $\\prec$ distance_threshold)] $\\mathit { \\Theta } = \\mathit { \\Theta } 0$   \n12   \n13 return heuristics   \n14   \n15 parameter_ranges $= \\begin{array} { r l } { \\left\\{ \\begin{array} { l l } \\end{array} \\right. } \\end{array}$ {   \n16 ’reward_threshold’: (0, 1),   \n17 ’distance_threshold’: (0, 100),   \n18 ’cost_penalty_weight’: (0, 2)   \n19 }   \n(b) Modified Code using LLMs\n\nFigure 4: An example of how Harmony search component works in HSEvo.   \nTable 1: Summary of parameters settings.   \n\n<html><body><table><tr><td>Description of Setting</td><td>Value</td></tr><tr><td>LLM(generator and reflector)</td><td>gpt-4o-mini-2024-07-18</td></tr><tr><td>LLM temperature (generator and reflector)</td><td>1</td></tr><tr><td>Maximum budget tokens</td><td>425K tokens</td></tr><tr><td>Population size (for EoH,ReEvo and HSEvo)</td><td>30 (initial stage),10 (other stages)</td></tr><tr><td>Mutation rate (forReEvo and HSEvo)</td><td>0.5</td></tr><tr><td>#of islands, # of samples per prompt (for FunSearch) Number of independent runs per experiment</td><td>10,4</td></tr><tr><td>HS size,HMCR,PAR,bandwidth,max iterations (for Harmony Search)</td><td>3</td></tr><tr><td>Maximum evaluation time for each heuristic</td><td>5,0.7,0.5,0.2, 5 100 sec (TSP-GLS),50 sec (Other)</td></tr></table></body></html>\n\n# 4 Automatic Heuristic Design with HSEvo\n\nIn this section, we propose a novel LLM-EPS framework called Harmony Search Evolution (HSEvo). HSEvo aim to promote the diversity of the population while still achieve better optimization and alleviate the trade-off between diversity and optimization performance with a individual tuning process based on harmony search. HSEvo also aim to reduce the cost incurred from LLM with an efficient flash reflection component. Figure 3 illustrates the pipeline of our HSEvo framework. Examples of prompts used in each stage can be found in the Extended version - Appendix.\n\nIndividual encoding. Follow previous works in LLMEPS (EoH (Liu et al. 2024a), ReEvo (Ye et al. 2024a)), HSEvo encodes each individual as a string of code snippet generated by LLMs (Fig. 4a). This encoding method allow the flexibility of each individual, i.e., not constrained by any predefined encoding format, and also make it easier to evaluate on the AHD problem.\n\nInitialization. HSEvo initializes a heuristic population by prompting the generator LLM with task specifications that describe the problem and detail the signature of the heuristic function to be searched. Additionally, to leverage existing knowledge, the framework can be initialized with a seed heuristic function and/or external domain knowledge. We promote diversity by implementing in the prompts with various role instructions (e.g., ”You are an expert in the domain of optimization heuristics...”, ”You are Albert Einstein, developer of relativity theory...”). This approach aims to enrich the heuristic generation process by incorporating diverse perspectives and expertise.\n\nSelection. HSEvo randomly selects parent pairs from the population, aiming to maintain the balance between exploration and exploitation during our optimization process. The random selection may also counteract premature convergence, which is observed through the SWID trajectory outlined in Section 3.4.\n\nFlash reflection. Reflections can provide LLMs with reinforcement learning reward signals in a verbal format for code generation tasks, as discussed by (Shinn et al. 2024). Later, (Ye et al. 2024a) also proposed integrating Reflections into LLM-EPS in ReEvo as an approach analogous to providing “verbal gradient” information within heuristic spaces. However, we argue that reflecting on each pair of heuristic parents individually is not generalized and wastes resources. To address these issues flash reflection proposed to alternative Reflection method for LLM-EPS. Flash reflection includes two steps as follows:\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">BPO</td><td colspan=\"2\">TSP</td><td colspan=\"2\">OP</td></tr><tr><td>CDI (↑)</td><td>Obj. (↓)</td><td>CDI (↑)</td><td>Obj. (D)</td><td>CDI (↑)</td><td>Obj. (↓)</td></tr><tr><td>FunSearch</td><td>4.97 ± 0.24</td><td>2.05 ± 2.01</td><td>5.24 ± 0.14</td><td>0.09 ±0.06</td><td>1</td><td></td></tr><tr><td>EoH</td><td>5.86 ± 0.49</td><td>3.17 ± 2.97</td><td>5.81 ± 0.23</td><td>1.09 ± 3.11</td><td>6.17 ± 0.42</td><td>-14.62 ± 0.22</td></tr><tr><td>ReEvo</td><td>4.91 ± 0.53</td><td>2.48 ± 3.74</td><td>5.15 ± 0.19</td><td>0.05 ± 0.06</td><td>5.02 ± 0.13</td><td>-14.54± 0.21</td></tr><tr><td>HSEvo (ours)</td><td>5.68± 0.35</td><td>1.07 ± 1.11</td><td>5.41 ± 0.21</td><td>0.02 ±0.03</td><td>5.67 ± 0.41</td><td>14.62±0.12</td></tr></table></body></html>\n\nTable 2: Experiment results on different AHD problems.\n\nTable 3: Ablation results on harmony search.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">OP</td></tr><tr><td>CDI(↑)</td><td>Obj. (↓)</td></tr><tr><td>ReEvo</td><td>5.02 ± 0.13</td><td>-14.54 ± 0.21</td></tr><tr><td>ReEvo +HS</td><td>5.11 ± 0.27</td><td>-14.58 ± 0.38</td></tr><tr><td>HSEvo (ours)</td><td>5.67 ± 0.41</td><td>-14.62±0.12</td></tr></table></body></html>\n\nTable 4: Ablation results on flash reflection.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">OP</td></tr><tr><td>CDI (↑)</td><td>Obj. (↓)</td></tr><tr><td>ReEvo</td><td>4.43 ± 0.23</td><td>-13.84 ± 1.13</td></tr><tr><td>ReEvo+F.R.</td><td>4.63 ± 0.37</td><td>-14.36±0.19</td></tr><tr><td>HSEvo (ours)</td><td>4.77 ±0.46</td><td>-14.07 ± 0.38</td></tr></table></body></html>\n\n• At time step $t$ , we perform grouping on all individuals that are selected from the selection stage and remove all duplication. After that, we use LLM to perform a deep analysis on the ranking on parent pairs. This take inputs as mixed ranking pairs (i.e., one good performance parent and one worse performance), good ranking pairs (i.e., both good performance), and worse ranking pairs (i.e., both worse performance), then return a comprehensive description on how to improve as a text string. • From the current analysis at time step $t$ , we use LLM to compare with previous analysis at time step $t - 1$ and output a guide information, which can be used in subsequent stages.\n\nCrossover. In this stage, new offspring algorithms are generated by combining elements of the parent algorithms. The goal is to blend successful attributes from two parents via guide result guide information part of flash reflections. Through this step, HSEvo hopes to produce offspring that inherit the strengths of both, which can potentially lead to better-performing heuristics. The prompt includes task specifications, a pair of parent heuristics, guide information part of flash reflection, and generation instructions.\n\nElitist mutation. HSEvo uses an elitist mutation strategy, where the generator LLM is tasked with mutation an elite individual—representing the best-performing heuristic—by incorporating insights derived from LLM-analyzed flash reflections. Each mutation prompt includes detailed task specifications, the elite heuristic, deep analysis part of flash reflections, and specific generation instructions. This approach leverages accumulated knowledge to enhance the heuristic, ensuring continuous improvement in performance while preserving the quality of the top solutions.\n\nHarmony Search. From the analysis on Section 3.4 and 3.5, we hypothesize that if the population is too diverse, each individuals inside will more likely to be not optimized, which may cause harm to the optimization process. We employ the Harmony Search algorithm to alleviate this issue by optimizing the parameters (e.g., thresholds, weights, etc.) of best individuals in the population. The process is as follows:\n\n• First, we use LLM to extract parameters from the best individual (i.e., code snippets, programs) of the population and define a range for each parameters (Fig. 4). • Following the work (Shi, Han, and Si 2012), we optimize the above parameters with harmony search algorithm. • After parameter optimization, we mark this individual and add it back to the population. All marked individuals will not be optimized again in the future time steps.\n\nFine-tuning these parameters makes the individual more optimized, therefore allowing us to encourage diversity during the prompting while avoid the trade-off between objective performance and diversity, as can be seen with EoH.\n\n# 5 Experiments\n\n# 5.1 Experimental Setup\n\nBenchmarks: To assess the diversity and objective scores of HSEvo compared to previous LLM-EPS frameworks, we adopt the same benchmarks as Section 3.4 and conduct experiments on three different AHD problems, BPO (Seiden 2002), TSP (Hoffman et al. 2013) and OP (Ye et al. 2024b).\n\n• BPO: packing items into bins of fixed capacity in realtime without prior knowledge of future items. In this benchmark, LLM-EPS frameworks need to design deterministic constructive heuristics to solve. • TSP: find the shortest possible route that visits each city exactly once and returns to the starting point. In this setting, LLM-EPS frameworks need to design heuristics to enhance the perturbation phase for the Guided Local Search solver. • OP: find the most efficient path through a set of locations, maximizing the total score collected within a limited time or distance. This setting requires LLM-EPS frameworks to design herustics used by the ACO solver.\n\n![](images/0517bf7121b8a13d9586a903484e8fc7c08b01c5e7bdc7957fff29d51155309a.jpg)  \nFigure 5: Diversity indices and objective scores of HSEvo framework on BPO problem through different runs.\n\nExperiment settings: All frameworks were executed under identical environmental settings listed in Table 1. The heuristic search processes across the LLM-EPS frameworks and the evaluations of heuristics were conducted using a single core of an Xeon Processors CPU.\n\n# 5.2 Experiment Results\n\nTable 2 presents our experiment results on all three AHD problems2. From the table, we observe that while our framework still haven’t obtained better CDI than EoH, HSEvo is able to achieve the best objective score on all tasks. On BPO, HSEvo outperforms both FunSearch and ReEvo by a huge margin. This highlights the important of our findings from the analysis in Section 3.5, where it is crucial to improve diversity in order to optimize the population better.\n\nTo investigate the impact of our framework on the diversity of the population, we plot the diversity metrics and objective score of HSEvo through different runs on BPO problem in Fig. 5. We can draw a similar observation with findings in Section 3.4, where with a high SWDI and CDI, the objective score can be optimized significantly. One thing to note here is that in HSEvo first run and ReEvo third run in Fig. 1, both have the SWDI decreasing overtime. However, in HSEvo, the SWDI is at around 3.0 when the objective score improve significantly, then only decrease marginally after that. In ReEvo, the objective score improves when SWDI at around 3.0 and 2.7, and the magnitude of the improvement is not as large as HSEvo, which implies the important of diversity in the optimization of the problem and also the impact of our proposed harmony search.\n\n# 5.3 Ablation Study\n\nTo gain a better understanding on our novel framework, we conduct ablation studies on our proposed components, the harmony search and flash reflection.\n\nHarmony search analysis As harmony search is a vital component in our HSEvo framework, instead of removing it from HSEvo, we conduct an experiment where we add harmony search into ReEvo framework. Table 3 presents our experiment results on OP problem. From the results, we can see that HSEvo outperforms both ReEvo and ReEvo with harmony search variant on both objective score and CDI. Here, notice that harmony search can only marginally improve ReEvo. This can be explained that ReEvo does not have any mechanism to promote diversity, therefore it is not benefitted from the advantage of harmony search process.\n\nFlash reflection analysis We also conduct another experiment where we replace the reflection used in ReEvo with our flash reflection component. As our flash reflection is more cost efficient than original reflection, we reduce the number of tokens used for optimization to 150K. Table 4 presents our experiment results on OP problem. The results show that given a smaller number of timestep, ReEvo with flash reflection mechanism can outperform HSEvo in optimization performance while obtain comparable results on CDI. However, when running with a larger number of tokens, ReEvo with flash reflection cannot improve on both objective score and CDI, while HSEvo improve both metrics to 5.67 and -14.62, respectively. This implies that without diversity-promoting mechanism, flash reflection is not enough to improve the optimization process of LLM-EPS.\n\n# 6 Conclusion\n\nIn this paper, we highlight the importance of population diversity in LLM-EPS for AHD problems. We propose two diversity measure metrics, SWDI and CDI, and conduct an analysis on the diversity of previous LLM-EPS approaches. We find that previous approaches either lack focus on the diversity of the population or suffered heavily from the diversity and optimization performance trade-off. We also introduce HSEvo, a novel LLM-EPS framework with diversitydriver harmony search and genetic algorithm for AHD problems. Our experiment results show that our framework can maintain a good balance between diversity and optimization performance trade-off. We also perform additional ablation studies to verify the effectiveness of components of our proposed framework. We hope our work can benefit future research in LLM-EPS community.\n\n# Acknowledgments\n\nThis research was funded by Hanoi University of Science and Technology under project code T2024-PC-038.",
    "institutions": [
        "Hanoi University of Science and Technology",
        "George Mason University"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n自动启发式设计（AHD）是解决复杂搜索和NP难组合优化问题的重要研究领域。虽然基于大语言模型的进化程序搜索（LLM - EPS）取得了一定进展，但在理解启发式搜索空间的属性以及实现探索与利用的平衡方面仍存在差距，这在大型启发式搜索空间中是一个关键因素。此问题的重要性在于它影响着算法在实际应用中的性能和效率，能否更好地解决各类优化问题。\\n\\n**方法概述**\\n本文提出了两个多样性测量指标（香农 - 维纳多样性指数SWDI和累积多样性指数CDI），对先前的LLM - EPS方法（FunSearch、EoH和ReEvo）进行分析，并在此基础上引入了HSEvo框架，该框架结合了和声搜索算法，以平衡多样性和收敛性。\\n\\n**主要贡献与效果**\\n- 提出两个多样性测量指标SWDI和CDI，用于评估LLM - EPS框架内种群的进化进度。\\n- 提出HSEvo框架，在所有任务上取得了最佳目标分数。例如在BPO问题上，HSEvo的目标分数为1.07 ± 1.11，远优于FunSearch（2.05 ± 2.01）和ReEvo（2.48 ± 3.74）；在TSP问题上，HSEvo的目标分数为0.02 ± 0.03，优于FunSearch（0.09 ± 0.06）、ReEvo（0.05 ± 0.06）和EoH（1.09 ± 3.11）；在OP问题上，HSEvo的目标分数为 - 14.62 ± 0.12，优于ReEvo（ - 14.54 ± 0.21）和EoH（ - 14.62 ± 0.22）。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n通过引入多样性测量指标，深入理解启发式搜索空间的特性，利用和声搜索算法优化参数，在保证种群多样性的同时，提升目标分数，以解决探索与利用的平衡问题。其有效性在于多样性的维持有助于算法跳出局部最优，和声搜索算法能对个体参数进行优化，从而提高整体性能。\\n\\n**创新点**\\n先前的LLM - EPS框架大多未充分考虑启发式搜索空间的属性和多样性的平衡。本文创新地提出了两个多样性测量指标，用于评估种群多样性，并引入和声搜索算法到LLM - EPS框架中，以优化参数，平衡多样性和目标性能。同时提出了闪回反射这一替代反射方法，提高了资源利用效率。\\n\\n**具体实现步骤**\\n1. **种群编码**：通过三步编码方法将LLM - EPS种群转换为向量表示，即使用抽象语法树去除注释和文档字符串、将代码片段标准化为通用编码风格（如PEP8）、使用代码嵌入模型将代码片段转换为向量表示。\\n2. **多样性指标计算**：\\n    - **SWDI**：确定个体在集群中的概率分布，公式为 $p _ { i } = \\\\frac { | C _ { i } | } { M }$ ，其中 $C _ { i }$ 是个体聚类，$M$ 是所有聚类中的个体总数。再使用香农熵计算SWDI分数，公式为 $H ( X ) = - \\\\sum _ { i = 1 } ^ { N } p _ { i } \\\\log ( p _ { i } )$ 。\\n    - **CDI**：构建连接所有个体的最小生成树，计算边距离的概率分布，公式为 $p _ { i } = \\\\frac { d _ { i } } { \\\\sum _ { j = 1 } ^ { \\\\# A - 1 } d _ { j } }$ ，其中 $d _ { i }$ 是最小生成树中边的距离。再使用香农熵计算CDI分数，公式为 $H ( X ) = - \\\\sum _ { i = 1 } ^ { \\\\# A - 1 } p _ { i } \\\\log ( p _ { i } )$ 。\\n3. **HSEvo框架流程**：\\n    - **初始化**：用任务规格提示生成器LLM初始化启发式种群，可结合种子启发式函数和外部领域知识，通过多种角色指令（如 “You are an expert in the domain of optimization heuristics...” 等）促进多样性。\\n    - **选择**：从种群中随机选择父对，以维持优化过程中的探索与利用平衡。\\n    - **闪回反射**：对选择阶段的个体进行分组和去重，使用LLM对父对进行深度分析和比较，输出指导信息。具体步骤为在时间步 $t$ ，对选择的个体分组去重，用LLM分析混合、好、差排名的父对，返回改进描述；再用LLM将当前分析与时间步 $t - 1$ 的分析比较，输出指导信息。\\n    - **交叉**：结合父算法的元素生成新的后代算法，通过闪回反射的指导信息产生更优的启发式。提示包含任务规格、父启发式、闪回反射指导信息和生成指令。\\n    - **精英变异**：让生成器LLM对精英个体（最佳性能启发式）进行变异，结合闪回反射的见解提升启发式性能。变异提示包含任务规格、精英启发式、闪回反射深度分析和生成指令。\\n    - **和声搜索**：使用LLM提取种群中最佳个体的参数并定义范围，用和声搜索算法优化参数，将优化后的个体标记并添加回种群，标记个体未来不再优化。\\n\\n**案例解析**\\n论文中给出了一个示例，展示了和声搜索组件在HSEvo中如何工作。通过修改代码中的参数范围（如'reward_threshold': (0, 1), 'distance_threshold': (0, 100), 'cost_penalty_weight': (0, 2) ），利用和声搜索算法对参数进行优化，使个体更加优化。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\nFunSearch、EoH、ReEvo。\\n\\n**性能对比**\\n*   **在 [累积多样性指数/CDI] 指标上：** 在BPO问题上，EoH的CDI为5.86 ± 0.49，HSEvo为5.68 ± 0.35，略低于EoH，但高于FunSearch（4.97 ± 0.24）和ReEvo（4.91 ± 0.53）；在TSP问题上，EoH的CDI为5.81 ± 0.23，HSEvo为5.41 ± 0.21，同样略低于EoH，但高于FunSearch（5.24 ± 0.14）和ReEvo（5.15 ± 0.19）；在OP问题上，EoH的CDI为6.17 ± 0.42，HSEvo为5.67 ± 0.41，低于EoH，但高于ReEvo（5.02 ± 0.13）。\\n*   **在 [目标分数/Objective Score] 指标上：** 在BPO问题上，HSEvo的目标分数为1.07 ± 1.11，远优于FunSearch（2.05 ± 2.01）、ReEvo（2.48 ± 3.74）和EoH（3.17 ± 2.97）；在TSP问题上，HSEvo的目标分数为0.02 ± 0.03，优于FunSearch（0.09 ± 0.06）、ReEvo（0.05 ± 0.06）和EoH（1.09 ± 3.11）；在OP问题上，HSEvo的目标分数为 - 14.62 ± 0.12，优于ReEvo（ - 14.54 ± 0.21）和EoH（ - 14.62 ± 0.22），FunSearch在OP问题上未给出目标分数数据。\",\n    \"keywords\": \"### 关键词\\n\\n- 自动启发式设计 (Automatic Heuristic Design, AHD)\\n- 大语言模型进化程序搜索 (LLM - based Evolutionary Program Search, LLM - EPS)\\n- 和声搜索进化 (Harmony Search Evolution, HSEvo)\\n- 多样性测量指标 (Diversity Measurement Metrics, N/A)\\n- 和声搜索算法 (Harmony Search Algorithm, N/A)\\n- 组合优化问题 (Combinatorial Optimization Problems, COPs)\\n- 闪回反射 (Flash Reflection, N/A)\"\n}"
}