{
    "link": "https://arxiv.org/abs/2412.10804",
    "pdf_link": "https://arxiv.org/pdf/2412.10804",
    "title": "Medical Manifestation-Aware De-Identification",
    "authors": [
        "Yuan Tian",
        "Shuo Wang",
        "Guangtao Zhai"
    ],
    "publication_date": "2024-12-14",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "paper_content": "# Medical Manifestation-Aware De-Identification\n\nYuan Tian1, Shuo Wang2, Guangtao Zhai2\\*\n\n1 Shanghai AI Laboratory 2 Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University tianyuan168326@outlook.com, zhaiguangtao $@$ sjtu.edu.cn\n\n# Abstract\n\nFace de-identification (DeID) has been widely studied for common scenes, but remains under-researched for medical scenes, mostly due to the lack of large-scale patient face datasets. In this paper, we release MeMa, consisting of over 40,000 photo-realistic patient faces. MeMa is re-generated from massive real patient photos. By carefully modulating the generation and data-filtering procedures, MeMa avoids breaching real patient privacy, while ensuring rich and plausible medical manifestations. We recruit expert clinicians to annotate MeMa with both coarse- and fine-grained labels, building the first medical-scene DeID benchmark. Additionally, we propose a baseline approach for this new medical-aware DeID task, by integrating data-driven medical semantic priors into the DeID procedure. Despite its conciseness and simplicity, our approach substantially outperforms previous ones.\n\nDataset and Code — https://github.com/tianyuan168326/MeMa-Pytorch\n\n# Introduction\n\nThe public sharing of large-scale image datasets has facilitated the rapid progress in Artificial Intelligence (AI). However, this also poses great privacy concerns, especially for facial images, which are widely used for identity authentication. To address this issue, many de-identification (DeID) algorithms (Cao et al. 2021; Maximov, Elezi, and Leal-Taixe´ 2020; Gu et al. 2020; Li et al. 2023; Cai et al. 2024) have been continuously proposed for protecting the facial identity, achieving promising results on common-scene facial datasets (Karras, Laine, and Aila 2019; Karras et al. 2017).\n\nHowever, rare researches are conducted for the medical scenes, although patient privacy leakage is a big concern in the medical AI era (Price and Cohen 2019). Research on medical-aware DeID (Med-DeID) mainly faces two obstacles. First, there are few medical-scene facial datasets available, due to the difficulty in accessing patients compared to healthy individuals. Moreover, it is often not acceptable to package real patient faces as datasets and make them publicly downloadable. Second, the current DeID approaches may not be appropriate for protecting medical facial images, due to not particularly preserving the disease manifestations of the origin image. This leads to the lost of diagnosisnecessary disease signs, deteriorating the medical utilities.\n\n![](images/5c0cef6c180031697f045b835e6fdefac151891c783b681e026d115101e73618.jpg)  \nFigure 1: (a) Common DeID approaches, focus on removing identity. (b) Our medical-aware DeID (Med-DeID), also considers preserving the diagnosis-necessary medical information. (c) Our MeMa, a large-scale patient face dataset. (d) Our MeMa-Seg, the tumor segmentation subset of MeMa.\n\nIn this paper, we release a Medical Manifestation-rich patient face dataset, termed MeMa, containing over 40,000 photo-realistic virtual patient images. To construct MeMa, we obtained permission from the hospital’s medical ethics committee to photograph patients. Then, these patient photos are annotated by expert physicians, before being used to train a specialized generative model. By carefully modulating the sampling procedure of the generative model and filtering the generated data, we created a diverse, high-quality, and real-world-like patient face dataset.\n\nFurthermore, we propose a baseline medical semanticspreserved DeID approach, termed MedSem-DeID, to eliminate the patient identity from the facial image, at the premise of preserving the medical utility. Concretely, we first condense the rich medical priors within the MeMa into a medical semantics encoder, and then adopt it to (1) enhance the medical knowledge of the features within the DeID pipeline, and (2) minimize the medical-aware distortion of the deidentified images. Despite its simplicity, our approach easily outperforms previous DeID approaches for medical scenes, thanks to the rich medical manifestation knowledge embodied in the MeMa dataset. Our main contributions are:\n\n• We release, to the best of our knowledge, the first largescale patient face dataset of rich medical manifestations, MeMa, which is expected to facilitate research in the field of medical-scene privacy protection.\n\n![](images/87329cf5d56474db992a88ed5fd3f877f4b652cd37d11cfef4ea909573707778.jpg)  \nFigure 2: Examples and the distribution characteristics of the proposed MeMa dataset.   \nFigure 3: MeMa building pipeline. (a) Training patient face generation model on real patient data. (b) Rich-condition patient face sampling. $P ( a g e )$ and $P ( g e n d e r )$ denote the age and gender distributions, which are statistically derived from the real patients. ‘SD’ denotes the stable diffusion model.\n\n• We propose a baseline approach for this novel medical DeID problem, which particularly preserves the disease signs during the DeID procedure, by making full use of the rich medical priors within MeMa. • We build the first medical-scene DeID benchmark, by comprehensively evaluating the proposed baseline and other recent DeID approaches on MeMa. Our approach is consistently superior in various aspects.\n\n# Related Work\n\nFacial Datasets. Amounts of large-scale face datasets (Karras, Laine, and Aila 2019; Karras et al. 2017) have been proposed, but they primarily feature healthy individuals, limiting their use for medical-scene DeID. In contrast, we introduce a large-scale patient face dataset with rich medical manifestations. Our dataset also includes rich medical annotations, facilitating face DeID field in medical scenes.\n\nFace De-identification. Early De-ID methods (Jourabloo, Yin, and Liu 2015) used the K-same algorithm. Recent approaches (Hukkel˚as, Mester, and Lindseth 2019; Maximov, Elezi, and Leal-Taixe´ 2020) leverage generative models to remove facial identity, while often compromising utility. More recent methods (Wen et al. 2023; Cai et al. 2024; Ren, Lee, and Ryoo 2018) aim to preserve more facial attributes and better serve common utilities such as gaze detection and image/video recognition (Kong and $\\mathrm { F u } 2 0 2 2$ ; Tian et al. 2022, 2020, 2021, 2019; Yan et al. 2023; Gao et al. 2024; Tan et al. 2024; Che et al. 2021), but not specifically medical signs (Chen et al. 2024a). In contrast, our approach leverages medical manifestation representations learned from real patient photos, preserving medical attributes during DeID. Additionally, it is reversible, similar to (Gu et al. 2020; Cao et al. 2021; Li et al. 2023), enabling reversal for medical audits.\n\nSemantic Representation. Effectively modeling semantic information is crucial for modifying facial images, while maintaining perceptual quality (Min et al. 2024; Yi et al. 2021; Duan et al. 2022; Chen et al. 2024b; Li et al. 2024; Gao et al. 2022, 2021; Yi, Jiang, and Zhou 2024) and preserving medical utility. Previous approaches have leveraged contrastive learning (Tian et al. 2024b, 2023b) and masked image modeling (Tian et al. 2023a; Tian, Lu, and Zhai\n\n2024) for self-supervised learning of image semantics. Recent studies have shown that pre-trained visual foundation models, such as stable diffusion (Rombach et al. 2022), exhibit even stronger semantic representations (Zhang et al. 2024; Tian, Lu, and Zhai 2025; Hedlin et al. 2024). In this work, we present the first adaptation of diffusion modelextracted semantics to the medical DeID problem.\n\nMedical-scene Face Privacy Protection. Progress on this problem has been slow, often relying on simple methods like blurring or replacing faces with 3D masks (Yang et al. 2022), which discard critical disease signs. The progress gap is attributed to the lack of large-scale medical-scene facial datasets. Our work aims to address this gap.\n\n# Approach\n\nWe first build a new patient face dataset, termed MeMa. It addresses the lack of medical-scene facial datasets. MeMa is synthesized from real patient photos. Its synthetic nature avoids potential ethical problems. Expert physicians recognize its validity. Further, we propose a baseline model for the medical-aware facial DeID (Med-DeID) problem.\n\nBCC/Conj/Pto SD 雲 G a   \nsis/SCC/TAO... (a)   \nFFHQ P(age) 1 P(gender) Real-guided Feature Sampling Extractor Filter/ Annotate +   \nDisease Ran d0.2. V.alue Minht IP-Adapter   \nBCC/Conj/Pto   \nsis/SCC/TAO... Prompt \"A face with   \nDegree Generate {BCC}.{mid) SD   \nslight/mid... (b)\n\n# MeMa Dataset\n\nThe overview of MeMa is shown in Fig. 2, which consists of 42,307 synthetic patient face images. MeMa closely mimics real patients in both visual appearance and statistics. Patient age and gender are estimated using the DeepFace framework (Serengil 2020; Serengil and Ozpinar 2021). We describe the main steps for assembling MeMa as follows.\n\nDisease Categories: We take the eye clinic as an exemplar scene, since most eye diseases show typical external facial manifestations. In our study, we included patients with seven eye diseases. These are Basal Cell Carcinoma (BCC), Conjunctivitis (Conj), Uveitis, Ptosis, Squamous Cell Carcinoma (SCC), Strabismus (Strab), and Thyroid Associated Ophthalmopathy (TAO). We also included clinically Normal cases. Examples are shown in Fig. 2(a). The detailed manifestations of the above diseases can be found in the MSD medical manual (Merck & Co. 2024).\n\nReal Patient Data Collection: We collected 39,323 photos of 12,467 real patients. They attended the Eye Clinic at Shanghai Ninth People’s Hospital(SNPH) between January 2020 and June 2023. The photo-taking procedure was approved by the hospital’s ethics committee. The patients’ diagnosis results were collected from their medical records.\n\nGenerating MeMa from Real Data: As shown in Fig. 3, we first train a medical-aware generative model with the collected patient data. Then, we sample the virtual patients from the model by using proper conditions, aiming to generate safe and diverse samples. Finally, we recruit expert physicians to filter the images of bad medical quality, then annotate the filtered images. The steps are detailed as follows.\n\nStep1: Medical-aware Generative Model Training: We first translate the disease type into the prompt caption ‘A face, eye with $\\{ { \\mathrm { d i s e a s e ~ n a m e } } \\} ^ { \\prime }$ . With the paired data of the real patient photographs and the disease type caption, we fine-tune the diffusion model (Rombach et al. 2022), producing the patient face generation model. As compared in Fig. 4 (a) and (b), after fine-tuning the SD model on our real patient dataset, the generated image shows typical medical signs and manifestations, while the vanilla SD model can not effectively generate images with reasonable medical manifestations, due to its limited medical knowledge.\n\nStep2: Rich-Condition Patient Face Synthesis: Directly sampling from the real-patient generation model with the simple prompt ‘A face, eye with disease name ’ is not enough, which shows two problems. First, identity leakage: the identity of most sampled patients can be found in the training dataset, potentially leaking the privacy of real patients. Second, mode collapse: the samples tend to be less diverse, with collapsed medical manifestation modes.\n\nTo address the identity leakage problem, we propose injecting facial attributes from public faces into the generation process. Specifically, we randomly sample face images from the FFHQ dataset and use the IP-Adapter (Ye et al. 2023) to inject these attributes. As shown in Tab. 1, this substantially reduces the average identity leakage percentage from $7 1 . 8 \\%$ to $1 . 2 7 \\%$ , when being evaluated with multiple face recognition models, i.e., SphereFace (Liu et al. 2017), ArcFace (Deng et al. 2019), and CosFace (Wang et al. 2018).\n\nTable 1: Effectiveness of injecting public face attribute for reducing the identity leakage percentage.   \n\n<html><body><table><tr><td></td><td>SphereFace</td><td>ArcFace</td><td>CosFace</td><td>Average</td></tr><tr><td>Direct Sample</td><td>73.45%</td><td>76.72%</td><td>65.34%</td><td>71.83%</td></tr><tr><td>Face Injection</td><td>1.62%</td><td>0.97%</td><td>1.24%</td><td>1.27%</td></tr></table></body></html>\n\n![](images/689375da5d004f2e108f65c4a20f7c0c5a71d4f80ab4afb500364c01964fc0d3.jpg)\n\nFigure 4: Comparison of different image generation strategies. We take the Basal Cell Carcinoma (BCC) disease as an example. ‘SD’ denotes the Stable Diffusion.   \nTable 2: Wasserstein distance (Ru¨schendorf 1985) between the generated and the real patient image distributions. Smaller distance indicates more real-world alike generation.   \n\n<html><body><table><tr><td>Sampling Strategy</td><td>Disease</td><td>Age</td><td>Gender</td></tr><tr><td>Random sampling</td><td>0.256</td><td>0.143</td><td>0.157</td></tr><tr><td>Real-guided sampling</td><td>0.003</td><td>0.002</td><td>0.002</td></tr></table></body></html>\n\nTo mitigate the mode collapse problem, we first randomly sample the public face injection weight from the range [0.2, 0.4], instead of using a fixed weight. This leads to better feature fusion flexibility and improves output diversity. Second, we enhance the text prompt with severity descriptions, e.g., ‘A face, eye with disease name , slight/mid/heavy - level’. This further improves diversity, even though no disease severity is annotated in the collected patient captions. The reason may be that the base SD model has learned a large dictionary of word semantics and can automatically connect the common ‘severity’ description words to the image generation process. As compared in Fig. 4 (b) and (c), with rich conditions injected, both the quality and diversity of the generated images are substantially improved.\n\nTo ensure the generated dataset’s statistical characteristics match those of real patients, we calculate the distributions of real patient disease types, ages, and genders. We control the generated images to follow the above distributions. To control the disease type, we simply modify the disease name of the prompt. To control the age and gender of the generated images, we label FFHQ images using an age and gender estimation model (Serengil 2020), then select images based on this metadata for attribute injection. As shown in Tab. 2, the real distribution-guided sampling strategy produces a dataset with similar statistical characteristics to real patients.\n\nStep3: Filtering and Annotation: After generating the images, we remove those with small identity feature distance to the original real patient set, ensuring the privacy of the real patients will not be leaked. Then, the physicians filter out the images with low medical utility quality. Finally, these physicians label the per-image disease information of the filtered dataset. Moreover, considering that lesion segmentation is another representative medical imaging task. We also ask the physicians to segment the tumor mask of the subset SCC images, producing the MeMa-Seg subset. The annotation procedure is assisted by the SAM model (Kirillov et al. 2023) and then refined by the physicians.\n\n# A Baseline Approach for Med-DeID\n\nWe propose a baseline approach to incorporate the rich medical manifestation knowledge within MeMa into the DeID procedure, which consists of two sub-modules: medical semantics encoding and medical semantics-preserved DeID.\n\nMedical Semantics Encoding. The Med-DeID task requires preserving as much medical information as possible while obfuscating other identifying details. This necessitates a semantic encoder that recognizes local medical semantics.\n\nMotivated by the strength of diffusion models in extracting fine-grained local semantics (Tian et al. $2 0 2 4 \\mathrm { a }$ ; Tang et al. 2023), we train another diffusion model on the proposed MeMa dataset to learn the medical semantics. We adopt its first several blocks as the medical encoder $E n c _ { m e d }$ , instead of the whole network, for reducing the computational cost.\n\nIt should be mentioned that the roles of the diffusion models in the previous section and here are fundamentally different: the previous one is for high-quality image generation, whereas the one here is for extracting rich medical semantics. Our approach is very flexible, and the semantic encoder can be other choices, as analyzed in the experiment section. Medical Semantics-Preserved DeID (MedSem-DeID). As illustrated in Fig. 5, our approach leverages the medical encoder $E n c _ { s e m }$ to inject medical knowledge into the feature extraction procedure, as well as regularize the medical utility of the de-identified image.\n\nGiven the original image $X$ , where $H$ and $W$ denote its height and width, an image encoder transforms $X$ into the facial feature $f _ { f a c e } ~ \\in ~ \\bar { \\mathbb { R } } ^ { 5 1 2 \\times \\frac { H } { 3 2 } \\times \\frac { W } { 3 2 } }$ . Meanwhile, we use Encmed to extract the medical feature fmed ∈ R320× 1H6 × 1W6 . The $f _ { m e d }$ is downscaled and concatenated with $f _ { f a c e }$ , passing through three consecutive residual blocks (He et al. 2016), producing $f$ . Then, we employ a group of Transformer blocks (Vaswani et al. 2017), termed ID-Encryptor, to encrypt the ID information within $f$ . Specifically, we flatten the spatial dimension of $f$ , concatenate it with the password vector $\\boldsymbol { P } \\in \\mathbb { R } ^ { 5 1 2 }$ , and feed the concatenated vector into ID-Encryptor, producing the encrypted feature $f _ { e n c } . \\ f _ { e n c }$ is passed through an image decoder network to result in the encrypted image $X _ { e n c }$ . Please refer to the supplementary material for the network architecture details.\n\n![](images/269243fc191ef0501a72af16f784aaf6ea40c78788d21c87ed5d2b36e149b612.jpg)  \nFigure 5: Overview of the proposed baseline model MedSem-DeID. The snow icon indicates the $E n c _ { \\mathrm { m e d } }$ is frozen during training DeID networks. The image decoder after the ID-decryptor is omitted for briefness. $\\oplus$ denotes the channel-wise concatenation operation.\n\nIn medical contexts, it is often necessary to rigorously recheck results with expert physicians on the original image. Moreover, the Good Clinical Practice (GCP) guideline (Guideline 2001) mandates that all medical materials involved in the diagnosis process must be traceable. Therefore, we design our method to be reversible, enabling the recovery of the original image from the encrypted features. Given the original password $P$ , $f _ { e n c }$ can be decrypted back to $\\hat { f }$ , by another group of Transformer blocks termed IDDecryptor. Then, $\\hat { f }$ is reconstructed as the original image $\\hat { X }$ by the image decoder. When an incorrect password is used, $f _ { e n c }$ is reconstructed into a wrong image $X _ { w r o n g }$ .\n\nLearning Objectives. The learning objective of the proposed MedSem-DeID is formulated as follows, $\\mathcal { L } = \\mathcal { L } _ { d e i d } +$ Lrev id $+ \\ \\mathcal { L } _ { w r o n g } \\ + \\ \\lambda _ { m e d } \\mathcal { L } _ { m e d } \\ + \\ \\lambda _ { r e v } \\mathcal { L } _ { r e v } \\ + \\ \\mathcal { L } _ { G A N }$ . $\\mathcal { L } _ { d e i d } = \\cos ( \\phi ( X ) , \\phi ( X _ { \\mathrm { e n c } } ) )$ enforces the identity of the encrypted image apart from the original image, where $\\phi$ denotes the pre-trained identity recognition network ArcFace (Deng et al. 2019), cos denotes the cosine similarity. $\\mathcal { L } _ { r e v - i d } = - \\cos ( \\phi ( X ) , \\phi ( \\hat { X } ) )$ enforces the identity of reversibly decrypted image is the same as the original image. $\\mathcal { L } _ { w r o n g } = \\cos ( \\phi ( X ) , \\phi ( X _ { e n c } ^ { w r o n g } ) )$ enforces the identity of the image decrypted by the wrong password far away from the original image. $\\mathcal { L } _ { m e d } = \\ell _ { 2 } ( f _ { m e d } , E n c _ { m e d } ( X _ { \\mathrm { e n c } } ) )$ facilitate the encrypted image is similar to the original image in terms of medical semantics. $\\mathcal { L } _ { r e v } = \\ell _ { 1 } ( X , \\hat { X } )$ regularizes the appearance of the recovered image by right password is similar to the original one. $\\ell _ { 1 }$ and $\\ell _ { 2 }$ denote the mean absolute error (MSE) and the mean squared error (MSE) functions, respectively. The $\\mathcal { L } _ { \\mathrm { G A N } }$ is the adversarial generative network (GAN) loss, enforcing the photo-realism of all images. $\\lambda _ { m e d }$ and $\\lambda _ { r e c }$ denote the balancing weights.\n\n# Experiments\n\nDatasets. MeMa: the proposed MeMa dataset consists of 42,307 images in total, which is split into a training set (34,000 images), a hyper-parameter selection set (3,729 images), and a validation set (4,578 images). All images are labeled with the disease category. MeMa-Seg: for the BCC (basal cell carcinoma) disease type, we randomly select 600 images from the training set and 150 images from the validation set of MeMa, annotating the tumor masks for these images. This results in the MeMa-Seg dataset, which can be used to evaluate the fine-grained medical performance of different DeID approaches. Real-ECXHCSU: we also collaborate with Eye Center of Xiangya Hospital of Central South University (ECXHCSU), enrolling 129 patients to conduct a real-world clinical trial. This aims to validate whether our algorithm, trained on the synthetic MeMa dataset, remains effective for real-world patients. Moreover, ECXHCSU is geographically distant from SNPH used to develop the MeMa dataset. This aims to further emphasize the generalization capability of our approach.\n\n![](images/fb7f18a05bdee8a6bb9867f26af113eb86a57b1a34409a79f2d1e19f34470d40.jpg)\n\nFigure 6: Qualitative results of different methods on the MeMA validation set.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Rev</td><td rowspan=\"2\">Utility Prior</td><td colspan=\"9\">Normasi Paoton S)C</td><td colspan=\"2\">SDicentation1</td></tr><tr><td>All</td><td>BCC</td><td>Conj</td><td></td><td></td><td></td><td>Strab</td><td>TAO</td><td>Uveitis</td><td></td><td></td></tr><tr><td>DeepPrivacy</td><td>X</td><td>Landmark</td><td>40.80</td><td>91.20</td><td>4.52</td><td>80.38</td><td>69.10</td><td>0.95</td><td>3.66</td><td>59.23</td><td>17.32</td><td>0.0041</td><td>0.0021</td></tr><tr><td>CIAGAN</td><td>×</td><td>Landmark</td><td>41.36</td><td>30.96</td><td>3.85</td><td>96.93</td><td>49.36</td><td>13.52</td><td>7.33</td><td>31.81</td><td>97.11</td><td>0.1280</td><td>0.0757</td></tr><tr><td>Disguise</td><td>X</td><td>Landmark+Gaze</td><td>76.01</td><td>57.19</td><td>95.32</td><td>98.63</td><td>90.68</td><td>58.29</td><td>37.00</td><td>90.69</td><td>80.31</td><td>0.2751</td><td>0.1984</td></tr><tr><td>Password</td><td>√</td><td>Unet</td><td>54.21</td><td>43.99</td><td>95.99</td><td>87.37</td><td>47.53</td><td>52.38</td><td>5.93</td><td>79.26</td><td>21.22</td><td>0.6336</td><td>0.5192</td></tr><tr><td>Personal</td><td>√</td><td>Face Attributes</td><td>30.17</td><td>67.17</td><td>9.69</td><td>61.95</td><td>31.99</td><td>4.19</td><td>2.09</td><td>33.39</td><td>30.90</td><td>0.0136</td><td>0.0073</td></tr><tr><td>RiDDLE</td><td>√</td><td>StyleGAN</td><td>30.01</td><td>29.27</td><td>0.00</td><td>74.40</td><td>58.32</td><td>0.00</td><td>0.00</td><td>4.04</td><td>74.02</td><td>0.0031</td><td>0.0017</td></tr><tr><td>Ours</td><td>√</td><td>Med-Knowledge</td><td>86.70</td><td>96.45</td><td>99.83</td><td>98.98</td><td>92.32</td><td>87.05</td><td>63.70</td><td>97.72</td><td>57.56</td><td>0.6775</td><td>0.5453</td></tr></table></body></html>\n\nTable 3: Comparison of various DeID methods on medical tasks. Classification and Segmentation tasks are evaluated on validation sets of MeMa and MeMa-Seg, respectively. ‘Rev’ denotes if the method is reversible.\n\nImplementation Details. For training the patient face generator model, we fine-tune Stable Diffusion v1-5 (Rombach et al. 2022) using the low-rank adaptation (LoRA) (Hu et al. 2021) technique, with the real patient data. The rank number is set to 64. We use the Adam optimizer (Kingma 2014) with $\\beta _ { 1 } = 0 . 9$ and $\\beta _ { 2 } ~ = ~ 0 . 9 9$ . The learning rate starts at $1 \\times 1 0 ^ { - 4 }$ and follows a cosine decay schedule. The batch size is 32, and the model is trained for ten epochs. It takes about five days to train the model on a machine equipped with two Nvidia A6000 GPUs. For training the medical semantics encoder, we use the same training strategy as above, except that the training data comes only from the MeMa training set. For training the MedSem-DeID model, we use the Adam optimizer with $\\beta _ { 1 } = 0 . 5$ and $\\beta _ { 2 } = 0 . 9 9$ . The initial learning rate is $2 \\times 1 0 ^ { - 4 }$ and is halved after 150,000 iterations. The total iteration number is 300,000. The batch size is 16. Training takes approximately two days on a machine equipped with four Nvidia 4090 GPUs.\n\nBenchmark Methods. For DeepPrivacy (Hukkela˚s, Mester, and Lindseth 2019), Password (Gu et al. 2020), CIAGAN (Maximov, Elezi, and Leal-Taixe´ 2020), and RiDDLE (Li et al. 2023), we adopt their officially released codes and models. For Disguise (Cai et al. 2024) and Personal (Cao et al. 2021), we request the materials from the authors.\n\nEvaluation Protocol and Metrics. Medical utility: for the disease classification task, we fine-tune the DiNov2 model (Oquab et al. 2023) on the MeMa training set. We evaluate its Top1 accuracy on the MeMa validation set processed by various DeID approaches. For the tumor segmentation task, we use the nnU-Net (Isensee et al. 2021) to evaluate different methods on MeMa-Seg, adopting the Dice score (Kamnitsas et al. 2017) and Jaccard index (Fletcher, Islam et al. 2018) as metrics. Real-word clinical utility: we recruit three physicians to manually diagnose the images in Real-ECXHCSU, that are de-identified by various DeID approaches. Each image is diagnosed by all three physicians, and the final diagnosis is determined by a majority voting strategy. We use Cohen’s Kappa $( k )$ (Banerjee et al. 1999) to measure the diagnosis consistency between the original and the de-identified images. $k$ is a common metric for evaluating clinical trial outcomes in the medical field. Identity protection: following recent works (Cao et al. 2021; Wen et al. 2023), we use Euclidean distance between the identity features of de-identified and original faces, denoted as ‘ID-Dis’, to quantitatively evaluate the effectiveness of identity protection. Identity features are extracted by FaceNet (Schroff, Kalenichenko, and Philbin 2015) trained on CASIA (Yi et al. 2014), FaceNet trained on VGGFace2 (Cao et al. 2018), and SphereFace (Liu et al. 2017), which are not used in the training procedure. Other utilities: following previous methods (Li et al. 2023; Cai et al. 2024), we adopt the Dlib (King 2009) and L2CS-Net (Abdelrahman et al. 2023) to evaluate the landmark detection and gaze estimation performances. Reversibility: we compare our method against the previous reversible methods, in terms of ID similarity, medical results, and visual quality of the reconstructed original image.\n\n# Results\n\nMedical Utility. As shown in Tab. 3, Our method achieves the best overall classification accuracy, outperforming the second-best method, Disguise, by more than $10 \\%$ . For SCC disease, our approach outperforms DeepPrivacy, CIAGAN, Disguise, Password, Personal, and RiDDLE by $8 6 . 1 0 \\%$ , $7 3 . 5 3 \\%$ , $2 7 . 7 6 \\%$ , $34 . 6 7 \\%$ , $8 2 . 8 6 \\%$ , and $8 7 . 0 5 \\%$ , respectively. On the more challenging tumor segmentation task, our approach also performs best, achieving the highest Dice (0.6775) and Jaccard (0.5453) scores.\n\nMoreover, we train Password and Disguise models on our MeMa dataset, improving their classification accuracy to $5 4 . 6 7 \\%$ and $7 7 . 1 2 \\%$ , respectively, but still lagging behind our $8 6 . 7 0 \\%$ . This indicates that MeMa can enhance the efficacy of various DeID methods in medical contexts, and its full potential can be realized through specialized medicalscene methods like our MedSem-DeID.\n\nIn Tab. 3 (3rd column), we summarize the priors employed by different methods. Landmark priors (DeepPrivacy and CIAGAN) and high-level common priors (face attributes/StyleGAN adopted by Personal/RiDDLE) perform poorly in medical applications, i.e., less than $50 \\%$ accuracy and 0.2 segmentation Dice score. The gaze prior (Disguise) is effective for coarse-grained classification $( 7 6 . 0 1 \\% )$ but fails in fine-grained segmentation task (0.2751). Password, using a U-Net to preserve high-frequency signals, excels in low-level segmentation (0.6336) but fails in high-level classification task $( 5 4 . 2 1 \\% )$ . This also introduces visual artifacts (Fig. 6, 4rd column). In contrast, our method, leveraging the medical semantics within MeMa, achieves superior performance in both classification $( 8 6 . 7 0 \\% )$ and segmentation (0.6775) without handcrafted designs such as landmark.\n\nReal-World Clinical Utility. We conduct a clinical trial on the Real-ECXHCSU cohort. As shown in Tab. 4, our method largely outperforms the recent DeID methods (Disguise and Password) across all five disease categories, achieving near-perfect consistency in the clinical outcomes, i.e., $k { > } 0 . 8 1$ . Moreover, our approach is more flexible and effective than a recent hand-crafted DeID approach that is delicately designed for eye diseases, namely, digital mask (DM) (Yang et al. 2022). On complex diseases, such as BCC and Eyelid Nevus (EyelidN), DM does not work ( $k$ $= 0 . 0 5 6 6 / 0 . 0 9 8 8 )$ while our approach achieves satisfactory results $\\left( k = 0 . 8 2 4 5 / 0 . 8 3 4 6 \\right)$ ). Moreover, this real-world evaluation introduces additional disease categories not seen during training, i.e., Entropion and EyelidN, highlighting the robustness and generalizability of our method. The diagnosis accuracy is provided in the supplementary material.\n\nTable 4: Comparison of different DeID methods in terms of the diagnosis outcomes, on the real-world cohort RealECXHCSU. $k { > } 0 . 8 1$ indicates perfect clinical consistency.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">Cohen's Kappa (k)↑</td></tr><tr><td>BCC</td><td>TAO Ptosis</td><td>Entropion</td><td>EyelidN</td></tr><tr><td>DM</td><td>0.0566</td><td>0.8159 0.8276</td><td>0.1879</td><td>0.0988</td></tr><tr><td>Disguise</td><td>0.7534</td><td>0.5824 0.7134</td><td>0.2467</td><td>0.1387</td></tr><tr><td>Password</td><td>0.4657 0.2758</td><td>0.4289</td><td>0.1329</td><td>0.0459</td></tr><tr><td>RiDDLE</td><td>0.1201</td><td>0.0751 0.0826</td><td>0.0937</td><td>0.0811</td></tr><tr><td>Ours</td><td>0.8245</td><td>0.8278 0.8302</td><td>0.8256</td><td>0.8346</td></tr></table></body></html>\n\nTable 5: Comparison of different methods on the MeMa validation set. The higher the ID-Dis, the better de-identified. Bold and italic indicates the best and the second-best result.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Rev</td><td colspan=\"3\">ID-Dis ↑</td></tr><tr><td>FaceNetvGGFace2</td><td>FaceNetcAsIA</td><td>Sphere</td></tr><tr><td>DeepPrivacy</td><td>X</td><td>1.1548</td><td>1.1831</td><td>1.1818</td></tr><tr><td>CIAGAN</td><td>X</td><td>1.2843</td><td>1.2566</td><td>1.2881</td></tr><tr><td>Disguise</td><td>X</td><td>1.3976</td><td>1.3607</td><td>1.3128</td></tr><tr><td>Password</td><td>√</td><td>1.3380</td><td>1.3139</td><td>1.2629</td></tr><tr><td>Personal</td><td>√</td><td>1.2819</td><td>1.2944</td><td>1.2351</td></tr><tr><td>RiDDLE</td><td>√</td><td>1.4278</td><td>1.3694</td><td>1.3583</td></tr><tr><td>Ours</td><td>√</td><td>1.4007</td><td>1.3609</td><td>1.3601</td></tr></table></body></html>\n\nTable 6: Face matching rate on Real-ECXHCSU.   \n\n<html><body><table><tr><td>Method</td><td>DeepPrivacy Password Disguise RiDDLE Ours</td><td></td><td></td></tr><tr><td>Rate(%)↓</td><td>5.76</td><td>7.76 2.89</td><td>2.13 1.76</td></tr></table></body></html>\n\nDe-Identification Performance. As shown in Tab. 5, our method achieves the best DeID performance of ID-Dis value 1.3601, when evaluated with the SphereFace face recognition model. With the FaceNetVGGFace2 and $\\mathrm { F a c e N e t _ { C A S I A } }$ models, our approach outperforms all methods except RiDDLE. RiDDLE maps person images into the very low-dimensional StyleGAN (Karras, Laine, and Aila 2019) latent space and selects a sample with the maximum identity distance from this space. While this over-dimension-reduction operation benefits identity protection, it sacrifices much original face information, resulting in poor downstream utilities, as evidenced in Tab. 3 and Tab. 4.\n\nFurthermore, we evaluate our approach on the LFW dataset (Huang et al. 2008). With the SphereFace facial recognition network, our approach achieves a face verification accuracy close to random guessing $( 5 0 \\% )$ .\n\nMoreover, we simulate a real-world identity authentication system. We use ID-card photos of Real-ECXHCSU patients as the identity database. We then match the deidentified clinical photos within the ID photo database. Note that the ID photo may be a long time away from the clinical photo. As shown in Tab. 6, our approach achieves the lowest successful face matching rate of $1 . 7 6 \\%$ , compared to other approaches such as Disguise $( 2 . 8 9 \\% )$ and RiDDLE $( 2 . 1 3 \\% )$ . Qualitative Results. As shown in Fig. 6, our approach uniquely preserves both coarse- and fine-grained medical cues, such as drooping eyelids and conjunctival redness. In contrast, DeepPrivacy masks and replaces the original face, Password retains color but distorts shapes. Disguise, Personal, and RiDDLE sacrifice medical cues for privacy. Besides, our approach demonstrates good visual quality.\n\nTable 7: Comparison of different DeID methods, in terms of common utilities, on the MeMa validation set. Landmark error is calculated as the averaged pixel distance between the original and the de-identified image. Bold and italic indicates the best and the second-best performance.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Rev</td><td colspan=\"2\">LandmarkError</td><td colspan=\"2\">Gaze Error↓</td></tr><tr><td>All</td><td>Eye Mouth Nose</td><td></td><td>Pitch Yaw</td></tr><tr><td>DeepPrivacy CIAGAN</td><td>X X X</td><td>193.91</td><td>5.86 155.02 33.02 313.36 23.87 215.11 74.37</td><td>7.72 13.00</td><td>7.06 7.78</td></tr><tr><td>Disguise Password Personal RiDDLE</td><td>√ √</td><td>94.09 65.90 109.46 136.79</td><td>5.78 67.29 4.38 42.52 5.87 71.20 5.73 91.28</td><td>21.01 18.98 32.38 39.77</td><td>4.85 5.71 5.34 9.97 7.24 7.45 7.01 7.93</td></tr></table></body></html>\n\nOther Downstream Utilities. As shown in Tab. 7, our approach shows competitive or best results on facial landmark detection and gaze detection tasks. For example, our approach achieves an eye landmark detection error of 2.94, much lower than the second-best approach, Password, which achieves 4.38. This is due to our particularly preserved eyerelated medical semantics. For gaze detection, although Disguise explicitly introduces the gaze detection loss, it still obtains a larger gaze error of $4 . 8 5 ~ \\nu . s . ~ 3 . 9 6$ , proving the power of our semantics learned on MeMa.\n\nReversibility. As shown in Tab. 8, compared with other reversible methods, our method performs better in terms of identity recovery, image fidelity, and perceptual quality, achieving ID-Dis, Peak Signal-to-Noise Ratio (PSNR), and LPIPS (Zhang et al. 2018) values of 0.5642, 27.02dB, and 0.2098, respectively. Moreover, our approach exhibits the best disease classification accuracy of $8 9 . 3 4 \\%$ , while the second-best Personal only obtains $7 3 . 0 8 \\%$ .\n\nWe provide the qualitative results in Fig. 7. Only our approach precisely preserves the clinical diagnosis necessary sign, i.e., the discolored left eye iris. RiDDLE generates high-quality facial textures, while obsoleting medical details. Password and Personal can not generate sharp details.\n\n# Model Analysis\n\nAblation Study for MedSem-DeID Model. Recalling that MedSem-DeID enhances the medical knowledge of the\n\n<html><body><table><tr><td>Method</td><td>ID-Dis↓</td><td>PSNR↑</td><td>LPIPS↓</td><td>Med-Class↑</td></tr><tr><td>Password</td><td>0.6382</td><td>26.29dB</td><td>0.2752</td><td>67.99%</td></tr><tr><td>Personal</td><td>0.5723</td><td>25.92dB</td><td>0.2240</td><td>73.08%</td></tr><tr><td>RiDDLE</td><td>0.8593</td><td>14.00dB</td><td>0.3732</td><td>47.46%</td></tr><tr><td>Ours</td><td>0.5642</td><td>27.02dB</td><td>0.2098</td><td>89.34%</td></tr></table></body></html>\n\nTable 8: Comparison of the recovered image by various reversible approaches on MaMa validation set. Lower ID-Dis indicates the recovered identity is more similar to the original. Lower LPIPS indicates better perceptual quality.\n\n![](images/5297b792033e727afe343e6f9a3fe81a6e5e1a88015557d1fdb704be5dd3a443.jpg)  \nFigure 7: Qualitative results of the recovered image of different reversible DeID approaches. The ocular region is zoomed-in for a more clear comparison.\n\nTable 9: Framework ablation Study. Both the MeMa dataset and the utilization of medical priors are useful. ID-Dis is calculated with the SphereFace network. Med-Class denotes the disease classification accuracy.   \n\n<html><body><table><tr><td>Model</td><td>Dataset</td><td>fmed</td><td>Lmed</td><td>ID-Dis↑</td><td>Med-Class</td></tr><tr><td>M1</td><td>FFHQ</td><td>X</td><td>X</td><td>1.3609</td><td>42.13%</td></tr><tr><td>M2</td><td>MeMa</td><td>X</td><td>X</td><td>1.3609</td><td>46.82%</td></tr><tr><td>M3</td><td>MeMa</td><td>√</td><td>X</td><td>1.3602</td><td>69.94%</td></tr><tr><td>M4</td><td>MeMa</td><td>X</td><td>√</td><td>1.3601</td><td>71.35%</td></tr><tr><td>Ours</td><td>MeMa</td><td>√</td><td>√</td><td>1.3601</td><td>86.70%</td></tr></table></body></html>\n\nDeID pipeline in both the feature extraction procedure $( f _ { m e d } )$ and the loss function $( \\mathcal { L } _ { m e d } )$ , we verify the effectiveness of both strategies. As shown in Tab. 9, when trained on the common-scene facial dataset FFHQ without using any medical prior, the resulting model (M1) achieves an IDDis score of 1.3689 and a disease classification accuracy of $4 2 . 1 3 \\%$ . When the training dataset is replaced with MeMa, the medical accuracy of the resulting model (M2) improves to $4 6 . 8 2 \\%$ without compromising the DeID performance. After further introducing medical priors, no matter the $f _ { m e d }$ or the $\\mathcal { L } _ { m e d }$ , the resulting models M3 and M4 show an obvious improvement in classification accuracy, i.e., $6 9 . 9 4 \\%$ and $7 1 . 3 5 \\%$ , while slightly compromising the DeID results. When combining both strategies, the final model achieves a strong medical performance of $8 6 . 7 0 \\%$ .\n\nDifferent Medical Semantic Encoders. Our method is flexible, not relying on the typical implementation of the medical encoder. To verify this, we trained two other semantic encoders on MeMa. We fine-tuned a pre-trained ViT model (Sharir, Noy, and Zelnik-Manor 2021) using supervised and self-supervised learning strategies, specifically the\n\nTable 10: Impact of various medical semantic encoders.   \n\n<html><body><table><tr><td></td><td>ViT(Supervised)</td><td>ViT(MAE)</td><td>Diffusion</td></tr><tr><td>Med-Class↑</td><td>82.97%</td><td>85.26%</td><td>86.70%</td></tr><tr><td>ID-Dis↑</td><td>1.3600</td><td>1.3601</td><td>1.3601</td></tr></table></body></html>\n\nMedicalLossWeight ReversibleLossWeight 28 1.325日 M70 Med-Class PS-DiS 1.25 1.300 0.1 1 5 10 0.01 0.1 1 10 λmed 入rev\n\nmasked auto-encoder (MAE) (He et al. 2022). As shown in Tab. 10, all three variants achieved decent performance. The supervised ViT performed the poorest due to the sparse disease category label for supervision. The diffusion model outperformed ViT(MAE) with $8 6 . 3 8 \\%$ vs. $8 5 . 2 6 \\%$ . This is likely because the LAION-5B (Schuhmann et al. 2022) dataset used for pre-training the base stable diffusion model is much larger than the pre-training dataset for the base ViT. Different Loss Weights. As shown in Fig. 8 (left), increasing the weight of medical loss $( \\lambda _ { m e d } )$ consistently improves disease classification accuracy, due to the enhanced medical information. However, this also makes the de-identified images more similar to the originals, compromising the DeID performance, i.e., the reduced ID-Dis. We set $\\lambda _ { m e d } = 5$ to achieve the best trade-off between medical accuracy and DeID. For the weight controlling reversible reconstruction $( \\lambda _ { r e v } )$ , a similar trade-off between the reconstructed image quality and DeID performance is observed, as shown in Fig. 8 (right). We set $\\lambda _ { r e v } = 0 . 1$ to achieve optimal results.\n\n# Conclusion and Limitation\n\nWe have released a large-scale patient face dataset, MeMa, to facilitate research on medical privacy protection. Expert physicians validated and annotated MeMa. On this dataset, we established a comprehensive benchmark for medicalscene de-identification, also proposing a new baseline approach that outperforms previous approaches. A limitation is that the current dataset focuses on eye disease-related manifestations. Future work will expand the dataset to include other facial diseases, such as facial paralysis.\n\n# Acknowledgments\n\nThis work is supported by Strategic Research and Consulting Project of Chinese Academy of Engineering (2024-XBZD-18), National Natural Science Foundation of China (62225112), Shanghai Artificial Intelligence Laboratory, National Natural Science Foundation of China (62101326), National Natural Science Foundation of China (82388101), National Natural Science Foundation of China (72293585), and National Natural Science Foundation of China (72293580). We thank Min Zhou (Doctor of Medicine) and Xuefei Song (Doctor of Medicine) for their invaluable assistance with patient data collection, data annotation, and medical knowledge support.",
    "institutions": [
        "Shanghai AI Laboratory",
        "Institute of Image Communication and Network Engineering",
        "Shanghai Jiao Tong University"
    ],
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决医学场景下面部去识别（DeID）研究不足的问题。在医学人工智能时代，患者隐私泄露是重大隐患，但由于缺乏大规模患者面部数据集，医学场景下的面部去识别研究进展缓慢。当前的去识别方法难以有效保护医学面部图像，还会导致必要疾病体征丢失，降低医学效用。\\n\\n**方法概述**\\n论文构建了包含42,307张逼真患者面部图像的MeMa数据集，并基于该数据集提出一种基线方法MedSem - DeID，将数据驱动的医学语义先验融入去识别过程。\\n\\n**主要贡献与效果**\\n- 发布首个大规模包含丰富医学表现的患者面部数据集MeMa，专家医生对其进行验证和标注，有望推动医学场景隐私保护领域的研究。\\n- 提出医疗感知面部去识别问题的基线方法，该方法在多个医学任务上表现出色。在疾病分类任务中，整体分类准确率达到86.70%，比第二好的方法Disguise高出超10%；在肿瘤分割任务中，Dice得分为0.6775，Jaccard指数为0.5453。\\n- 建立医学场景去识别的综合基准，为该领域研究提供了重要参考。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n先构建新的患者面部数据集MeMa，通过合成避免潜在伦理问题；再提出基线模型MedSem - DeID，利用MeMa中的医学语义信息，在去识别过程中尽可能保留医学信息，同时模糊其他识别细节。其核心在于使用扩散模型提取医学语义，并将其融入特征提取和损失函数中，利用扩散模型在提取细粒度局部语义方面的优势，在混淆其他识别细节的同时尽可能保留医学信息。\\n\\n**创新点**\\n- 先前工作缺乏大规模医学场景面部数据集，而本文发布的MeMa数据集解决了这一问题。\\n- 现有去识别方法未特别保留原始图像的疾病表现，本文方法将医学语义先验融入去识别过程，能更好地保留医学信息。\\n- 首次将扩散模型提取的语义应用于医学去识别问题。\\n- 提出的方法具有可逆性，可用于医学审计。\\n\\n**具体实现步骤**\\n1. **构建MeMa数据集**：\\n    - **疾病类别确定**：以眼科为例，纳入七种眼科疾病患者及临床正常病例。这七种眼科疾病分别是基底细胞癌（BCC）、结膜炎（Conj）、葡萄膜炎、睑下垂、鳞状细胞癌（SCC）、斜视（Strab）和甲状腺相关性眼病（TAO）。\\n    - **真实患者数据收集**：收集上海第九人民医院眼科12,467名真实患者的39,323张照片，并获取诊断结果，且收集过程获得医院伦理委员会批准。\\n    - **从真实数据生成MeMa**：\\n        - **医学感知生成模型训练**：将疾病类型转换为提示字幕，与真实患者照片配对，微调扩散模型得到患者面部生成模型。\\n        - **多条件患者面部合成**：为解决身份泄露和模式崩溃问题，注入公共面部属性并随机采样注入权重，增强文本提示的严重程度描述，控制生成图像的疾病类型、年龄和性别分布。具体而言，为解决身份泄露问题，随机采样FFHQ数据集的面部图像，用IP - Adapter注入属性，这将平均身份泄露百分比从71.8%大幅降低至1.27%；为缓解模式崩溃问题，随机采样公共面部注入权重，增强文本提示的严重程度描述。\\n        - **过滤和标注**：移除与原始真实患者集身份特征距离小的图像，医生过滤低医学效用质量的图像并标注疾病信息，对SCC图像子集进行肿瘤分割得到MeMa - Seg子集，标注过程由SAM模型辅助并经医生细化。\\n2. **提出Med - DeID基线方法**：\\n    - **医学语义编码**：在MeMa数据集上训练扩散模型学习医学语义，采用其前几个块作为医学编码器 $Enc_{med}$ 以降低计算成本。\\n    - **医学语义保留去识别（MedSem - DeID）**：利用医学编码器 $Enc_{sem}$ 将医学知识注入特征提取过程，正则化去识别图像的医学效用。通过图像编码器将原始图像转换为面部特征，医学编码器提取医学特征，两者拼接后经过残差块和Transformer块进行ID加密，加密特征通过图像解码器得到加密图像。方法具有可逆性，可通过ID - Decryptor和解码器恢复原始图像。学习目标为 $\\mathcal{L} = \\mathcal{L}_{deid} + \\mathcal{L}_{rev - id} + \\mathcal{L}_{wrong} + \\lambda_{med}\\mathcal{L}_{med} + \\lambda_{rev}\\mathcal{L}_{rev} + \\mathcal{L}_{GAN}$。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括DeepPrivacy、CIAGAN、Disguise、Password、Personal、RiDDLE。\\n\\n**性能对比**\\n*   **在 [医学效用 - 疾病分类准确率] 指标上：** 本文方法在MeMa验证集上达到了 **86.70%** 的分类准确率，显著优于基线模型DeepPrivacy (40.80%)、CIAGAN (41.36%)、Disguise (76.01%)、Password (54.21%)、Personal (30.17%)、RiDDLE (30.01%)，高出最佳基线Disguise 10.69个百分点。将Password和Disguise模型在MeMa数据集上训练后，分类准确率分别提升至54.67%和77.12%，但仍低于本文方法。Landmark priors（DeepPrivacy和CIAGAN）和高 - 级通用先验（Personal/RiDDLE采用的面部属性/StyleGAN）在医学应用中表现不佳，准确率低于50%。\\n*   **在 [医学效用 - 肿瘤分割指标（Dice和Jaccard）] 上：** 本文方法在MeMa - Seg数据集上的Dice得分为 **0.6775**，Jaccard指数为 **0.5453**，均为最佳。相比之下，DeepPrivacy的Dice得分为0.0041，CIAGAN为0.1280，Disguise为0.2751，Password为0.6336，Personal为0.0136，RiDDLE为0.0031，均低于本文方法。\\n*   **在 [现实临床效用 - Cohen's Kappa (k)] 指标上：** 本文方法在Real - ECXHCSU队列上，对所有五种疾病类别的诊断一致性均达到 **k > 0.81**，远高于Disguise、Password、RiDDLE等方法。例如，在BCC疾病上，本文方法的k值为0.8245，而Disguise为0.7534，Password为0.4657，数字掩码（DM）为0.0566。\\n*   **在 [去识别性能 - ID - Dis] 指标上：** 本文方法使用SphereFace面部识别模型评估时，ID - Dis值达到 **1.3601**，优于DeepPrivacy (1.1818)、CIAGAN (1.2881)、Disguise (1.3128)、Password (1.2629)、Personal (1.2351)；使用FaceNetVGGFace2和FaceNetCASIA模型评估时，除RiDDLE外，本文方法优于其他基线模型。\\n*   **在 [面部匹配率] 指标上：** 本文方法在Real - ECXHCSU上的面部匹配率为 **1.76%**，低于Disguise (2.89%)、RiDDLE (2.13%)等方法。\\n*   **在 [其他下游效用 - 面部地标检测误差] 指标上：** 本文方法的眼睛地标检测误差为 **2.94**，远低于第二好的方法Password (4.38)。\\n*   **在 [其他下游效用 - 注视检测误差] 指标上：** 本文方法的注视检测误差为 **3.96**，低于Disguise (4.85)。\\n*   **在 [可逆性 - ID - Dis、PSNR、LPIPS、疾病分类准确率] 指标上：** 本文方法的ID - Dis值为 **0.5642**，PSNR为 **27.02dB**，LPIPS为 **0.2098**，疾病分类准确率为 **89.34%**，在与其他可逆方法对比中表现最佳。\",\n    \"keywords\": \"### 关键词\\n\\n- 人脸去标识 (Face De - Identification, DeID)\\n- 医学场景隐私保护 (Medical - Scene Privacy Protection, N/A)\\n- 医学语义保留去标识 (Medical Semantics - Preserved De - Identification, MedSem - DeID)\\n- 医学人脸数据集 (Medical Patient Face Dataset, MeMa)\\n- 扩散模型 (Diffusion Model, N/A)\"\n}"
}