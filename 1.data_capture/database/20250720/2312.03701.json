{
    "link": "https://arxiv.org/abs/2312.03701",
    "pdf_link": "https://arxiv.org/pdf/2312.03701",
    "title": "Return of Unconditional Generation: A Self-supervised Representation Generation Method",
    "authors": [
        "Tianhong Li",
        "Dina Katabi",
        "Kaiming He"
    ],
    "institutions": [
        "MIT CSAIL"
    ],
    "publication_date": "2023-12-06",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 31,
    "influential_citation_count": 2,
    "paper_content": "# Return of Unconditional Generation: A Self-supervised Representation Generation Method\n\nTianhong Li\n\nDina Katabi\n\nKaiming He\n\nMIT CSAIL\n\n# Abstract\n\nUnconditional generation—the problem of modeling data distribution without relying on human-annotated labels—is a long-standing and fundamental challenge in generative models, creating a potential of learning from large-scale unlabeled data. In the literature, the generation quality of an unconditional method has been much worse than that of its conditional counterpart. This gap can be attributed to the lack of semantic information provided by labels. In this work, we show that one can close this gap by generating semantic representations in the representation space produced by a self-supervised encoder. These representations can be used to condition the image generator. This framework, called Representation-Conditioned Generation (RCG), provides an effective solution to the unconditional generation problem without using labels. Through comprehensive experiments, we observe that RCG significantly improves unconditional generation quality: e.g., it achieves a new state-of-the-art FID of 2.15 on ImageNet $2 5 6 \\times 2 5 6$ , largely reducing the previous best of 5.91 by a relative $64 \\%$ . Our unconditional results are situated in the same tier as the leading class-conditional ones. We hope these encouraging observations will attract the community’s attention to the fundamental problem of unconditional generation. Code is available at https://github.com/LTH14/rcg.\n\n# 1 Introduction\n\nGenerative models have been long developed as unsupervised learning methods in the history, e.g., in the seminal works including GAN [27], VAE [39], and diffusion models [57]. These fundamental methods focus on learning the probabilistic distributions of data, without relying on the availability of human annotations. This problem, often categorized as unconditional generation in today’s community, is in pursuit of utilizing the vast abundance of unannotated data to learn complex distributions.\n\nHowever, unconditional image generation has been largely stagnant in comparison with its conditional counterpart. Recent research [18, 54, 10, 11, 24, 50] has demonstrated compelling image generation quality when conditioned on class labels or text descriptions provided by humans, but its quality degrades significantly without these conditions. Closing the gap between unconditional and conditional generation is a challenging and scientifically valuable problem: it is a necessary step towards unleashing the power of large-scale unannotated data, which is a common goal in today’s deep learning community.\n\nWe hypothesize that such a performance gap is because human-annotated conditioning introduces rich semantic information to simplify the generative process. In this work, we largely close this gap by taking inspiration from a closely related area—unsupervised/self-supervised learning.1 We observe that the representations produced by a strong self-supervised encoder (e.g., [30, 12, 8, 14]) can also capture a lot of semantic attributes without human supervision, as reflected by their transfer learning performance in the literature. These self-supervised representations can serve as a form of conditioning without violating the unsupervised nature of unconditional generation, creating an opportunity to get rid of the heavy reliance on human-annotated labels.\n\n![](images/d21d040dc0983718006c7518c1ef8dbfa858ce567ef886e2945b425794cd1ccd.jpg)  \nFigure 1: The Representation-Conditioned Generation (RCG) framework for unconditional generation. RCG consists of three parts: (a) it uses a pre-trained self-supervised encoder to map the image distribution to a representation distribution; (b) it learns a representation generator that samples from a noise distribution and generates a representation subject to the representation distribution; (c) it learns an image generator (e.g., which can be ADM [18], DiT [50], or MAGE [41]) that maps a noise distribution to the image distribution conditioned on the representation distribution.\n\nBased on this observation, we propose to first unconditionally generate a self-supervised representation and then condition on this representation to generate the images. As a preprocessing step (Figure 1a), we use a pre-trained self-supervised encoder (e.g., MoCo [14]) to map the image distribution into the corresponding representation distribution. In this representation space, we train a representation generator without any conditioning (Figure 1b). As this space is low-dimensional and compact [65], learning the representation distribution is favorably feasible with unconditional generation. In practice, we implement it as a very lightweight diffusion model. Given this representation space, we train a second generator that is conditioned on these representations and produces images (Figure 1c). This image generator can conceptually be any image generation model. The overall framework, called Representation-Conditioned Generation (RCG), provides a new paradigm for unconditional generation.2\n\nRCG is conceptually simple, flexible, yet highly effective for unconditional generation. RCG greatly improves unconditional generation quality regardless of the specific choice of the image generator (Figure 2), reducing FID by $71 \\%$ , $76 \\%$ , $82 \\%$ , and $51 \\%$ for LDM-8, ADM, DiT-XL/2, and MAGE-L, respectively. This indicates that RCG largely reduces the reliance of current generative models on manual labels. On the challenging ImageNet $2 5 6 \\times 2 5 6$ benchmark, RCG achieves an unprecedented 2.15 FID for unconditional generation. This performance not only largely outperforms previous unconditional methods, but more surprisingly, can catch up with the strong leading methods that are conditional on class labels. We hope our method and encouraging results will rekindle the community’s interest in the fundamental problem of unconditional generation.\n\n![](images/2d3ce5b11c3252c127efc337c15a8cda8e587687664a6b6eacdc134e7e70c976.jpg)  \nFigure 2: Unconditional Image Generation can be largely improved by our RCG framework. Regardless of the specific form of the image generator (LDM [54], ADM [18], DiT [50], or MAGE [41]), RCG massively improves the unconditional generation quality. Generation quality is measured by FID on ImageNet with a $2 5 6 \\times 2 5 6$ resolution. All comparisons between models without and with RCG are conducted under controlled conditions to ensure fairness. The technical details and more metrics are in Section 4.1.\n\n# 2 Related Work\n\nGenerative Models. Generative models aim at accurately modeling data distribution to generate new data point that resembles the original data. One stream of generative models is built on top of generative adversarial networks (GANs) [27, 69, 37, 70, 7]. Another stream is based on a twostage scheme [63, 53, 10, 67, 40, 41, 11]: first tokenize the image into a latent space and then apply maximum likelihood estimation and sampling in the token space. Diffusion models [33, 59, 18, 54, 52] have also achieved superior results on image synthesis.\n\nThe design of a generative model is mostly orthogonal to how it is conditioned. However, literature has shown that unconditional generation often significantly lags behind conditional generation under the same design[18, 41, 10], especially on complex data distributions.\n\nUnconditional Generation. Unconditional generation is the fundamental problem in the realm of generative models. It aims to model the data distribution without relying on human annotations, highlighted by seminal works of GAN [27], VAE [39], and diffusion models [57]. It has demonstrated impressive performance in modeling simple image distributions such as scenes or human faces [23, 10, 18, 54], and has also been successful in applications beyond images where human annotation is challenging or impossible, such as novel molecular design [66, 28, 26], medical image synthesis [71, 16, 47], and audio generation [48, 42, 25]. However, recent research in this domain has been limited, largely due to the notable gap between conditional and unconditional generation capabilities of recent generative models on complex data distributions [46, 18, 19, 41, 3, 61].\n\nPrior efforts to narrow this gap mainly group images into clusters in the representation space and use the cluster indices as underlying class labels to provide conditioning [46, 43, 3, 35]. However, these methods assume that the dataset is clusterable, and the optimal number of clusters is close to the number of classes. Additionally, these methods fall short of generating diverse representations— they are unable to produce different representations within the same cluster or underlying class.\n\nRepresentations for Image Generation. Prior works have explored the effectiveness of exploiting representations for image generation. DALL-E 2 [52], a text-conditional image generation model, first converts text prompts into image embeddings, and then uses these embeddings as the conditions to generate images. In contrast, RCG for the first time demonstrates the possibility of directly generating image representations from scratch, a necessary step to enable conditioning on self-supervised representations in unconditional image generation. Another work, DiffAE [51], trains an image encoder in an end-to-end manner with a diffusion model as decoder, aiming to learn a meaningful and decodable image representation. However, its semantic representation ability is still limited (e.g., compared to self-supervised models like MoCo [14], DINO [8]), which largely hinders its performance in unconditional generation. Another relevant line of work is retrieval-augmented generative models [5, 4, 9], where images are generated based on representations extracted from existing images. Such semi-parametric methods heavily rely on ground-truth images to provide representations during generation, a requirement that is impractical in many generative applications.\n\n![](images/37a0bc0474d94833b15c783ec9ff16f1e8e1384f310ea3092ac781ac9d1c650b.jpg)  \nFigure 3: RCG’s training framework. The pre-trained self-supervised image encoder extracts representations from images and is fixed during training. To train the representation generator, we add standard Gaussian noise to the representations and ask the network to denoise them. To train the MAGE image generator, we add random masking to the tokenized image and ask the network to reconstruct the missing tokens conditioned on the representation extracted from the same image.\n\n# 3 Method\n\nDirectly modeling a complex high-dimensional image distribution is a challenging task. RCG decomposes it into two much simpler sub-tasks: first modeling the distribution of a compact lowdimensional representation, and then modeling the image distribution conditioned on this representation distribution. Figure 1 illustrates the idea. Next, we describe RCG and its extensions in detail.\n\n# 3.1 The RCG Framework\n\nRCG comprises three key components: a pre-trained self-supervised image encoder, a representation generator, and an image generator. Each component’s design is elaborated below:\n\nDistribution Mapping. RCG employs an off-the-shelf image encoder to convert the image distribution to a representation distribution. This image encoder has been pre-trained using self-supervised contrastive learning methods, such as MoCo v3 [14], on ImageNet. This approach regularizes the representations on a hyper-sphere while achieving state-of-the-art performance in representation learning. The resulting distribution is characterized by two essential properties: it is simple enough to be modeled effectively by an unconditional representation generator, and it is rich in high-level semantic content, which is crucial for guiding image generation. These attributes are vital for the effectiveness of the following two components.\n\n![](images/b2a0c3851048d40d8bbb4fee7653f4c70291c1fcb33aff53ecaaad5428a09ab1.jpg)  \nFigure 4: Representation generator’s backbone architecture. Each “Layer” consists of a LayerNorm layer [1], a SiLU layer [22], and a linear layer. The backbone consists of an input layer that projects the representation to hidden dimension $C$ , followed by $N$ fullyconnected (fc) blocks, and an output layer that projects the hidden latent back to the original representation dimension. The diffusion timestep is embedded and added to every fc block.\n\nRepresentation Generation. In this stage, we want to generate abstract, unstructured representations without conditions. To address this issue, we develop a diffusion model for unconditional representation generation, which we call a representation diffusion model (RDM). RDM employs a fully-connected network with multiple fully-connected residual blocks as its backbone (Figure 4). Each block consists of an input layer, a timestep embedding projection layer, and an output layer, where each layer consists of a LayerNorm [1], a SiLU [22], and a linear layer. Such an architecture is simply controlled by two parameters: the number of blocks, $N$ , and the hidden dimension, $C$ .\n\nRDM follows DDIM [58] for training and inference. As shown in Figure 3a, during training, image representation $z _ { 0 }$ is mixed with standard Gaussian noise variable $\\epsilon$ : $\\bar { z } _ { t } = \\sqrt { \\alpha _ { t } } z _ { 0 } \\dot { + } \\sqrt { 1 - \\alpha _ { t } } \\epsilon$ . The RDM backbone is then trained to denoise $z _ { t }$ back to $z _ { 0 }$ . During inference, RDM generates representations from Gaussian noise following the DDIM sampling process [58]. Since RDM operates on highly compacted representations, it brings marginal computation overheads for both training and generation (Appendix B.1), while providing substantial semantic information for the image generator, introduced next.\n\nImage Generation. The image generator in RCG crafts images conditioned on self-supervised representations. Conceptually, such an image generator can be any modern conditional image generative model by substituting its original conditioning (e.g., class label or text) with representations. In Figure 3b, we take MAGE [41], a parallel decoding generative model as an example. The image generator is trained to reconstruct the original image from a masked version of the image, conditioned on the representation of the same image. During inference, the image generator generates images from a fully masked image, conditioned on the representation generated by the representation generator.\n\nWe experiment with four representative generative models: ADM [18], LDM [54], and DiT [50] are diffusion-based frameworks, and MAGE [41] is a parallel decoding framework. Our experiments show that all four generative models achieve much better performance when conditioned on highlevel self-supervised representations (Table 1).\n\n# 3.2 Extensions\n\nOur RCG framework can easily be extended to support guidance even in the absence of labels, and to support conditional generation when desired. We introduce these extensions as follows.\n\nEnabling Guidance in Unconditional Generation. In class-conditional generation, the presence of labels allows not only for class conditioning but can also provides additional “guidance” in the generative process. This mechanism is often implemented through classifier-free guidance in classconditional generation methods [32, 54, 11, 50]. In RCG, the representation-conditioning behavior enables us to also benefit from such guidance, even in the absence of labels.\n\nSpecifically, RCG follows [32, 11] to incorporate guidance into its MAGE generator. During training, the MAGE generator is trained with a $10 \\%$ probability of not being conditioned on image representations, analogous to [32] which has a $10 \\%$ probability of not being conditioned on labels. For each inference step, the MAGE generator produces a representation-conditioned logit, $l _ { c }$ , and an unconditional logit, $l _ { u }$ , for each masked token. The final logits, $l _ { g }$ , are calculated by adjusting $l _ { c }$ away from $l _ { u }$ by the guidance scale, $\\begin{array} { r } { \\tau \\colon l _ { g } = l _ { c } + \\tau ( l _ { c } - l _ { u } ) } \\end{array}$ . The MAGE generator then uses $l _ { g }$ to sample the remaining masked tokens. Additional implementation details of RCG’s guidance are provided in Appendix A.\n\nTable 1: RCG significantly improves the unconditional generation performance of current generative models, evaluated on ImageNet $2 5 6 \\times 2 5 6$ . All numbers are reported under the unconditional generation setting.   \n\n<html><body><table><tr><td colspan=\"2\">Unconditional generation FID↓</td><td>IS↑</td></tr><tr><td rowspan=\"2\">LDM-8 [54]</td><td>baseline 39.13</td><td>22.8 w/RCG 11.30(-27.83)101.9(+79.1)</td></tr><tr><td>baseline 26.21</td><td>39.7</td></tr><tr><td rowspan=\"2\">ADM [18]</td><td>w/RCG</td><td>6.24(-19.97) 136.9(+97.2)</td></tr><tr><td>baseline 27.32</td><td>35.9</td></tr><tr><td>DiT-XL/2 [50]</td><td>w/RCG baseline 8.67</td><td>4.89(-22.43)143.2(+107.3) 94.8</td></tr><tr><td rowspan=\"2\">MAGE-B[41]</td><td>w/RCG</td><td>3.98 (-4.69) 177.8(±83.0)</td></tr><tr><td></td><td></td></tr><tr><td rowspan=\"2\">MAGE-L [41]</td><td>baseline</td><td>7.04 123.5</td></tr><tr><td>w/RCG</td><td>3.44(-3.60) 186.9 (+63.4)</td></tr></table></body></html>\n\nSimple Extension to Class-conditional Generation. RCG seamlessly enables conditional image generation by training a task-specific conditional RDM. Specifically, a class embedding is integrated into each fully-connected block of the RDM, in addition to the timestep embedding. This enables the generation of class-specific representations. The image generator then crafts the image conditioned on the generated representation. As shown in Table 3 and Appendix C, this simple modification allows users to specify the class of the generated image while keeping RCG’s superior generative performance, all without the need to retrain the image generator.\n\n# 4 Experiments\n\nWe evaluate RCG on the ImageNet $2 5 6 \\times 2 5 6$ dataset [17], which is a common benchmark for image generation and is especially challenging for unconditional generation. Unless otherwise specified, we do not use ImageNet labels in any of the experiments. We generate 50K images and report the Frechet Inception Distance (FID) [31] and Inception Score (IS) [55] as the standard metrics for assessing the fidelity and diversity of the generated images. Evaluations of precision and recall are included in Appendix B.1. Unless otherwise specified, we follow the evaluation suite provided by ADM [18]. All ablations and results on other datasets are included in Appendix B.1.\n\n# 4.1 Observations\n\nWe extensively evaluate the performance of RCG with various image generators and compare it to the results of state-of-the-art unconditional and conditional image generation methods. Several intriguing properties are observed.\n\nRCG significantly improves the unconditional generation performance of current generative models. In Table 1, we evaluate the proposed RCG framework using different image generators. The results demonstrate that conditioning on generated representations substantially improves the performance of all image generators in unconditional generation. Specifically, it reduces the FID for unconditional LDM-8, ADM, DiT-XL/2, MAGE-B, and MAGE-L by $71 \\%$ , $76 \\%$ , $82 \\%$ , $54 \\%$ , and $51 \\%$ , respectively. We further show that such improvement is also universal across different datasets, as demonstrated by the results on CIFAR-10 and iNaturalist in Appendix B.1. These findings confirm that RCG markedly boosts the performance of current generative models in unconditional generation, significantly reducing their reliance on human-annotated labels.\n\nMoreover, such outstanding performance can be achieved with lower training cost compared to current generative models. In Figure 5, we compare the training cost and unconditional generation FIDs\n\n2530354045Unconditional Generation FID LDM-8 ★ MAGE-B w/ RCG ★ MAGE-L w/ RCG DiT-L/2DiT-XL/2   \n105 MAGE-B MAGE-L   \n★1   \n0   \n0 2 4 6 8 10 12 14 Training Cost (days)\n\n<html><body><table><tr><td>Unconditional generation</td><td>#params FID↓ IS↑ ~70M 38.61 24.7</td></tr><tr><td>BigGAN[19] ADM[18]</td><td>554M 26.21 39.7</td></tr><tr><td>MaskGIT[10] RCDM+ [5]</td><td>227M 20.72 42.1 19.0 51.9</td></tr><tr><td>IC-GAN† [9] ADDP [61] MAGE-B [41]</td><td>~75M 15.6 59.0 176M 8.9 95.3 176M 8.67 94.8 439M 7.04 123.5</td></tr><tr><td>MAGE-L [41] RDM-IN+ [4]</td><td>400M 5.91 158.8</td></tr><tr><td>RCG (MAGE-B)</td><td>3.98 177.8</td></tr><tr><td>RCG (MAGE-L) RCG-G (MAGE-B)</td><td>239M 502M 3.44 186.9 239M 3.19 212.6 2.15 253.4</td></tr></table></body></html>\n\nof RCG and current generative models. RCG achieves a significantly lower FID with less training cost than current generative models. Specifically, MAGE-B with RCG achieves an unconditional generation FID of 4.87 in less than a day when trained on 64 V100 GPUs. This demonstrates that decomposing the complex tasks of unconditional generation into much simpler sub-tasks can significantly simplify the data modeling process.\n\nRCG largely improves the state-of-the-art in unconditional image generation. In Table 2, we compare MAGE with RCG and previous state-of-the-art methods in unconditional image generation. As shown in Figure 8 and Table 2, RCG can generate images with both high fidelity and diversity, achieving an FID of 3.44 and an Inception Score of 186.9. These results are further enhanced with the guided version of RCG (RCG-G), which reaches an FID of 2.15 and an Inception Score of 253.4, significantly surpassing previous methods of unconditional image generation.\n\nRCG’s unconditional generation performance rivals leading methods in class-conditional image generation. In Table 3, we perform a system-level comparison between the unconditional RCG and state-of-the-art class-conditional image generation methods. MAGE-L with RCG is comparable to leading class-conditional methods, with and without guidance. These results demonstrate that RCG, for the first time, improves the performance of unconditional image generation on complex data distributions to the same level as that of state-of-the-art class-conditional generation methods, effectively bridging the historical gap between class-conditional and unconditional generation.\n\nIn Table 4, we further conduct an apple-to-apple comparison between the class-conditional versions of LDM-8, ADM, and DiT-XL/2 and their unconditional counterparts using RCG. Surprisingly, with RCG, these generative models consistently outperform their class-conditional versions by a noticeable margin. This demonstrates that the rich semantic information from the unconditionally generated representations can guide the generative process even more effectively than class labels.\n\nTable 3: System-level comparison: RCG’s unconditional generation performance rivals leading methods in class-conditional image generation on ImageNet $2 5 6 \\times 2 5 6$ . Following common practice, we report the number of parameters used during generation. Class-conditional results are marked in gray.   \n\n<html><body><table><tr><td>Methods</td><td>#params</td><td colspan=\"2\">w/o Guidance FID↓ IS↑</td><td colspan=\"2\">w/Guidance FID↓ IS↑</td></tr><tr><td>Class-conditional</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ADM[18]</td><td>554M</td><td>10.94</td><td>101.0</td><td>4.59</td><td>186.7</td></tr><tr><td>LDM-4 [54]</td><td>400M</td><td>10.56</td><td>103.5</td><td>3.60</td><td>247.7</td></tr><tr><td>U-ViT-H/2-G [2]</td><td>501M</td><td></td><td></td><td>2.29</td><td>263.9</td></tr><tr><td>DiT-XL/2 [50]</td><td>675M</td><td>9.62</td><td>121.5</td><td>2.27</td><td>278.2</td></tr><tr><td>DiffT[29]</td><td></td><td></td><td></td><td>1.73</td><td>276.5</td></tr><tr><td>BigGAN-deep [6]</td><td>160M</td><td>6.95</td><td>198.2</td><td></td><td></td></tr><tr><td>MaskGIT[10]</td><td>227M</td><td>6.18</td><td>182.1</td><td></td><td></td></tr><tr><td>MDTv2-XL/2 [24]</td><td>676M</td><td>5.06</td><td>155.6</td><td>1 1.58</td><td>- 314.7</td></tr><tr><td>CDM [34]</td><td>→</td><td>4.88</td><td>158.7</td><td></td><td></td></tr><tr><td>MAGVIT-v2 [68]</td><td>307M</td><td>3.65</td><td>200.5</td><td>1.78</td><td>319.4</td></tr><tr><td>RIN[36]</td><td>410M</td><td>3.42</td><td></td><td></td><td></td></tr><tr><td>VDM++ [38]</td><td></td><td></td><td>182.0</td><td></td><td></td></tr><tr><td>RCG, conditional (MAGE-L)</td><td>2B 512M</td><td>2.40 2.99</td><td>225.3 215.5</td><td>2.12 2.25</td><td>267.7</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>300.7</td></tr><tr><td>Unconditional RCG (MAGE-L)</td><td>502M</td><td>3.44</td><td>186.9</td><td>2.15</td><td>253.4</td></tr></table></body></html>\n\nTable 4: Apple-to-apple comparison: RCG’s unconditional generation outperforms the classconditional counterparts of current generative models, evaluated on ImageNet $2 5 6 \\times 2 5 6$ . MAGE does not report its class-conditional generation performance. Class-conditional results are marked in gray   \n\n<html><body><table><tr><td>Methods</td><td></td><td>FID↓</td><td>IS↑</td></tr><tr><td rowspan=\"2\">LDM-8 [54]</td><td>w/classlabels</td><td>17.41</td><td>72.9</td></tr><tr><td>w/ RCG</td><td>11.30</td><td>101.9</td></tr><tr><td rowspan=\"2\">ADM[18]</td><td>w/ class labels</td><td>10.94</td><td>101.0</td></tr><tr><td>w/ RCG</td><td>6.24</td><td>136.9</td></tr><tr><td rowspan=\"2\">DiT-XL/2 [50]</td><td>w/ class labels</td><td>9.62</td><td>121.5</td></tr><tr><td>w/ RCG</td><td>4.89</td><td>143.2</td></tr></table></body></html>\n\nAs shown in Table 3 and Appendix C, RCG also supports class-conditional generation with a simple extension. Our representation diffusion model can easily adapt to class-conditional representation generation, thereby enabling RCG to also adeptly perform class-conditional image generation. This result demonstrates the effectiveness of RCG in leveraging its superior unconditional generation performance to benefit downstream conditional generation tasks.\n\nImportantly, such an adaptation does not require retraining the representation-conditioned image generator. For any new conditioning, only the lightweight representation generator needs to be retrained. This potentially enables pre-training of the self-supervised encoder and image generator on large-scale unlabeled datasets, and task-specific training of conditional representation generator on a small-scale labeled dataset. We believe that this property, similar to self-supervised learning, allows RCG to both benefit from large unlabeled datasets and adapt to various downstream generative tasks with minimal overheads. We leave the exploration on this direction to future work.\n\n# 4.2 Qualitative Insights\n\nIn this section, we showcase the visualization results of RCG, providing insights into its superior generative capabilities. Figure 8 illustrates RCG’s unconditional image generation results on ImageNet $2 5 6 \\times 2 5 6$ . The model is capable of generating both diverse and high-quality images without relying on human annotations. The high-level semantic diversity in RCG’s generation stems from its representation generator, which models the distribution of representations and samples them with varied semantics. By conditioning on these representations, the complex data distribution is broken down into simpler, representation-conditioned sub-distributions. This decomposition significantly simplifies the task for the image generator, leading to the production of high-quality images.\n\n![](images/4c2acf3a297ca1837006ffefbf1ed6f15708e815c37b375dd62267fe7023ed29.jpg)  \nFigure 6: RCG can generate images with diverse appearances but similar semantics from the same representation. We extract representations from reference images and, for each representation, generate a variety of images from different random seeds.\n\n![](images/952120d4ed9b85db488ab106456685f6fd7d6c0e256d19c84af4c472e95306c0.jpg)  \nFigure 7: RCG’s results conditioned on interpolated representations from two images. The semantics of the generated images gradually transfer between the two images.\n\nBesides high-quality generation, the image generator can also introduce significant low-level diversity in the generative process. Figure 6 illustrates RCG’s ability to generate diverse images that semantically align with each other, given the same representation from the reference image. The images generated by RCG can capture the semantic essence of the reference images while differing in specific details. This result highlights RCG’s capability to leverage semantic information in representations to guide the generative process, without compromising the diversity that is important in unconditional image generation.\n\nFigure 7 further showcases RCG’s semantic interpolation ability, demonstrating that the representation space is semantically smooth. By leveraging RCG’s dependency on representations, we can semantically transition between two images by linearly interpolating their respective representations. The interpolated images remain realistic across varying interpolation rates, and their semantic contents smoothly transition from one image to another. For example, interpolating between an image of “Tibetan mastiff” and an image of “wool” could generate a novel image featuring a dog wearing a woolen sweater. This also highlights RCG’s potential in manipulating image semantics within a low-dimensional representation space, offering new possibilities to control image generation.\n\n# 5 Discussion\n\nComputer vision has entered a new era where learning from extensive, unlabeled datasets is becoming increasingly common. Despite this trend, the training of image generation models still mostly relies on labeled datasets, which could be attributed to the large performance gap between conditional and unconditional image generation. Our paper addresses this issue by exploring RepresentationConditioned Generation, which we propose as a nexus between conditional and unconditional image generation. We demonstrate that the long-standing performance gap can be effectively bridged by generating images conditioned on self-supervised representations and leveraging a representation generator to model and sample from this representation space. We believe this approach has the potential to liberate image generation from the constraints of human annotations, enabling it to fully harness the vast amounts of unlabeled data and even generalize to modalities that are beyond the scope of human annotation capabilities.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决无条件生成模型中生成质量远低于有条件生成模型的问题。该问题的重要性在于，若能解决此问题，将有助于充分利用大规模无标注数据，这是当前深度学习社区的共同目标。\\n\\n**方法概述**\\n论文提出了表示条件生成（Representation-Conditioned Generation, RCG）框架，先利用预训练的自监督编码器将图像分布转换为表示分布，再开发表示生成器生成符合表示分布的表示，最后基于该表示利用图像生成器生成图像。\\n\\n**主要贡献与效果**\\n- 显著提升无条件生成质量：在ImageNet $256\\\\times256$ 上，RCG实现了2.15的FID，较之前的最佳值5.91相对降低了64%。\\n- 降低对人工标签的依赖：对四种代表性生成模型（ADM、LDM、DiT、MAGE）进行实验，结果显示基于生成表示进行条件处理，分别将无条件LDM - 8、ADM、DiT - XL/2、MAGE - B和MAGE - L的FID降低了71%、76%、82%、54%和51%。\\n- 训练成本更低：与当前生成模型相比，RCG以更低的训练成本实现了显著更低的FID。如在64个V100 GPU上训练时，带有RCG的MAGE - B在不到一天的时间内实现了4.87的无条件生成FID。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nRCG的核心思想是将复杂的高维图像分布建模分解为两个更简单的子任务：首先建模紧凑低维表示的分布，然后基于该表示分布建模图像分布。其有效性在于自监督编码器产生的表示能够在无人工监督的情况下捕捉大量语义属性，可作为一种条件，在不违反无条件生成的无监督性质的前提下，减少对人工标注标签的依赖。\\n\\n**创新点**\\n先前缩小无条件和有条件生成差距的方法主要是在表示空间中将图像分组为簇，并使用簇索引作为潜在类别标签来提供条件，但这些方法假设数据集是可聚类的，且无法在同一簇或潜在类别中生成不同的表示。先前的无条件生成方法依赖人工标注的标签作为条件，限制了大规模未标注数据的使用。相比之下，RCG首次展示了直接从头生成图像表示的可能性，通过自监督编码器生成表示，无需人工标注标签，从而能够利用大量未标注数据，实现了在无条件图像生成中基于自监督表示进行条件处理。\\n\\n**具体实现步骤**\\n1. **分布映射**：使用预训练的自监督图像编码器（如MoCo v3）将图像分布转换为表示分布。该编码器在ImageNet上使用自监督对比学习方法进行预训练，可使表示在超球面上正则化，并在表示学习中取得最先进的性能。\\n2. **表示生成**：开发一个用于无条件表示生成的扩散模型（RDM），其骨干网络采用具有多个全连接残差块的全连接网络。在训练时，将图像表示 $z_0$ 与标准高斯噪声变量 $\\epsilon$ 混合，即 $\\bar{z}_t = \\\\sqrt{\\\\alpha_t}z_0 + \\\\sqrt{1 - \\\\alpha_t}\\\\epsilon$，然后训练RDM骨干网络将 $z_t$ 去噪回 $z_0$；在推理时，RDM根据DDIM采样过程从高斯噪声生成表示。\\n3. **图像生成**：使用一个条件图像生成模型（如MAGE），基于图像的表示对掩码版本的图像进行训练，以重建原始图像。在推理时，图像生成器基于表示生成器生成的表示，从完全掩码的图像生成图像。\\n\\n**案例解析**\\n论文以生成图像为例，阐述了方法的工作流程。自监督编码器将图像分布映射到表示分布，然后学习表示生成器和图像生成器，最后根据表示条件生成图像。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括BigGAN、ADM、MaskGIT、RCDM+、IC - GAN、ADDP、RDM - IN+、LDM - 8、DiT - XL/2、MAGE - B、MAGE - L、LDM - 4、U - ViT - H/2 - G、DiffT、BigGAN - deep、MDTv2 - XL/2、CDM、MAGVIT - v2、RIN、VDM++。\\n\\n**性能对比**\\n*   **在 [无条件生成FID/Unconditional generation FID] 指标上：** 本文方法在ImageNet $256\\\\times256$ 基准上达到了优异的FID值。例如，LDM - 8的FID从39.13降低到了11.30，降低了27.83；ADM的FID从26.21降低到了6.24，降低了19.97；DiT - XL/2的FID从27.32降低到了4.89，降低了22.43；MAGE - B的FID从8.67降低到了3.98，降低了4.69；MAGE - L的FID从7.04降低到了3.44，降低了3.60。在ImageNet $256\\\\times256$ 上，RCG实现了2.15的FID，显著优于先前无条件生成方法的最佳值5.91，相对降低了64%。在与有条件生成模型对比中，带有RCG的无条件生成模型（如MAGE - L）与有条件生成模型的性能相当，甚至在某些情况下超过了有条件生成模型，如无条件带有RCG的LDM - 8（FID为11.30）优于有类标签的LDM - 8（FID为17.41）。\\n*   **在 [IS] 指标上：** 本文方法在ImageNet $256\\\\times256$ 基准上提高了IS值。例如，LDM - 8的IS从22.8提高到了101.9，提高了79.1；ADM的IS从39.7提高到了136.9，提高了97.2；DiT - XL/2的IS从35.9提高到了143.2，提高了107.3；MAGE - B的IS从94.8提高到了177.8，提高了83.0；MAGE - L的IS从123.5提高到了186.9，提高了63.4。带有RCG的MAGE - L的IS达到186.9，高于其基线的123.5。在与其他模型对比中，RCG在多个模型上都提升了IS值，表明生成图像的多样性得到了提高。\",\n    \"keywords\": \"### 关键词\\n\\n- 图像生成 (Image Generation, N/A)\\n- 无条件生成 (Unconditional Generation, N/A)\\n- 表示条件生成 (Representation - Conditioned Generation, RCG)\\n- 自监督学习 (Self - supervised Learning, N/A)\"\n}"
}