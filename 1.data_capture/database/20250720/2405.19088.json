{
    "link": "https://arxiv.org/abs/2405.19088",
    "pdf_link": "https://arxiv.org/pdf/2405.19088",
    "title": "Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions",
    "authors": [
        "Zhe Hu",
        "Tuo Liang",
        "Jing Li",
        "Yiren Lu",
        "Yunlai Zhou",
        "Yiran Qiao",
        "Jing Ma",
        "Yu Yin"
    ],
    "institutions": [
        "Department of Computing, The Hong Kong Polytechnic University",
        "Department of Computer and Data Sciences, Case Western Reserve University"
    ],
    "publication_date": "2024-05-29",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 3,
    "influential_citation_count": 0,
    "paper_content": "# Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions\n\nZhe $\\mathbf { H } \\mathbf { u } ^ { 1 }$ ,∗Tuo Liang2,∗Jing Li1, Yiren ${ \\bf L u } ^ { 2 }$ , Yunlai Zhou2, Yiran Qiao2, Jing $\\mathbf { M } \\mathbf { a } ^ { 2 }$ , Yu Yin2\n\n1Department of Computing, The Hong Kong Polytechnic University 2Department of Computer and Data Sciences, Case Western Reserve University 1zhe-derek.hu@connect.polyu.hk, jing-amelia.li@polyu.edu.hk 2{txl859,yxl3538,yxz3057,yxq350,jxm1384,yu.yin}@case.edu https://vulab-ai.github.io/YESBUT_Homepage/\n\n# Abstract\n\nRecent advancements in large multimodal language models have demonstrated remarkable proficiency across a wide range of tasks. Yet, these models still struggle with understanding the nuances of human humor through juxtaposition, particularly when it involves nonlinear narratives that underpin many jokes and humor cues. This paper investigates this challenge by focusing on comics with contradictory narratives, where each comic consists of two panels that create a humorous contradiction. We introduce the YESBUT benchmark, which comprises tasks of varying difficulty aimed at assessing AI’s capabilities in recognizing and interpreting these comics, ranging from literal content comprehension to deep narrative reasoning. Through extensive experimentation and analysis of recent commercial or open-sourced large (vision) language models, we assess their capability to comprehend the complex interplay of the narrative humor inherent in these comics. Our results show that even state-of-the-art models still lag behind human performance on this task. Our findings offer insights into the current limitations and potential improvements for AI in understanding human creative expressions.\n\n# 1 Introduction\n\n“The world is indeed comic, but the joke is on mankind.”\n\n—H. P. Lovecraft\n\nComics are a unique blend of visual art and narrative that encapsulate a wide range of human experiences and emotions. Understanding comics often requires significant social reasoning skills and cultural knowledge, as they heavily rely on context, cultural references, and visual metaphors. Furthermore, comics frequently employ nonlinear narratives [1, 2], demanding rigorous reasoning to grasp underlying ideas. Recent large (vision) language models have achieved impressive performance on various tasks [3–5], yet their ability to comprehend these complex human expressions remains insufficiently explored [6–8].\n\nExamining AI models’ ability to understand comics is essential for advancing their social and semantic comprehension. As a significant part of human creative expression, comic offers valuable insights into human emotions and cultural contexts [9]. This understanding is crucial for developing socially intelligent systems and enhancing AI-related creativity, thereby improving user experience in applications such as recommendation systems and automated content creation tools.\n\nPrevious studies have applied vision language models (VLMs) to understand humor and deep semantics [7, 10]. However, these studies often focus on single-panel comics and do not investigate the more complex case of nonlinear narratives created through juxtaposition, a fundamental element in comics. Juxtaposition involves placing two contrasting elements together to provoke thought or evoke humor [11, 12]. This technique requires readers to pause and reassess the meaning, engaging in nonlinear thinking to reason about the relationships between panels for overall idea. [13–15].\n\nIn this work, we examine VLMs’ ability to understand comics, specifically focusing on humor derived from juxtaposition. Our goal is to determine if large models can accurately comprehend the complex and contradictory narratives present in comics. Such contradictions challenge conventional semantic interpretations and demand deeper analysis. For example, Figure 1 shows a comic with two panels: in the first, a driver stops for ducks to cross the road (\"Yes\"), and in the second, the driver enters a \"Peking Duck\" restaurant (\"But\"), highlighting the contradiction in human-animal relationships through juxtaposition.\n\nUnderstanding such juxtaposition in comics significant challenges for the models. First, it requires deep comprehension of human norms, recognizing that people often have conflicting feelings, and identifying subtle social cues and contexts tied to cultural backgrounds. Additionally, it demands nonlinear reasoning to grasp the overall narrative, as the story is conveyed through the interplay of two panel elements, forming the core of the narrative beyond the literal mean\n\nComic with two panels: Yes, But Narrative Understanding 中国餐厅 & Peking Deep Reasoning Duck   \nNarrative contradiction   \nThe contradiction lies in the driver's display of compassion for live ducks by   \nstopping and waiting for them to cross the road, yet this same respect does not   \nextend to their treatment as food.   \nUnderlying Philosophy Selection   \n$\\checkmark$ It highlights the contradictory relationship humans have with animals,   \nwhere the value of an animal's  life changes drastically based on context.   \nThe comic suggests that there is no needs to show empathy to the animals   \nsince they will be eventually consumed as foods.   \nTitle Matching   \nCrossing Paths: From Compassion to Cuisine   \nFood Empathy vs. Animal Protection\n\ning of each single panel. This type of juxtaposition necessitates critical thinking about similarities and differences, requiring in-depth reasoning. However, current models lack the ability to process information through nonlinear and deep thinking effectively, as the autoregressive paradigm of LMs limits their bidirectional reasoning capabilities [16–18]. By emphasizing these contradictions, we aim to push AI models to develop more sophisticated semantic understanding, enriching their interpretative capabilities.\n\nTo this end, we collected and annotated a new benchmark, YESBUT, for understanding comics with juxtaposition, focusing on contradictory narratives. Each comic is annotated with a literal description, a contradiction illustration, the underlying philosophy it reveals or satirizes, and a title that summarizes the overall narrative, as shown in Figure 1. We then propose four tasks: (1) literal description writing, to produce a surface description of the comic narrative; (2) contradiction generation, where the model illustrates the narrative contradiction; (3) underlying philosophy selection, which targets at selecting the correct philosophy the comic reflects; and (4) title matching, where the model matches the comic with a proper title. These tasks jointly cover different levels of comic understanding, from literal content comprehension to more in-depth narrative reasoning, providing a thorough evaluation of comic understanding capabilities.\n\nWe conducted comprehensive experiments on the YESBUT dataset, evaluating both commercial and open-sourced large (visual) language models. Both automatic and human evaluation results indicate that commercial VLMs outperform their open-sourced counterparts on most tasks. However, even the highest scores are far from perfect (e.g., $8 4 . 1 \\%$ accuracy for underlying philosophy selection and $6 3 . 3 \\%$ for title matching), underscoring the need for further advancements in this area. Additionally, our analysis reveals that augmenting models with oracle comic descriptions can significantly enhance performance, highlighting the considerable gap in current models’ understanding of comic narratives. We release our annotations, code, and model results, aiming to provide valuable insights for future AI research on understanding human creative expression.\n\n# 2 Related Work\n\nLarge Models and Evaluations. Recent large (vision) language models have demonstrated remarkable performance in following human instructions and performing various downstream tasks through zero-shot prompting [19–21, 4]. Various benchmarks have been proposed to evaluate their performance, encompassing both language-only tasks [22–25] and vision-language tasks [26–30]. These tasks primarily focus on assessing the fundamental capabilities of large models. However, the ability of large models to perform in-depth social reasoning and accurately understand human contexts remains underexplored [6].\n\nComputational Humor. Humor is a vital component of human communication [31]. Our research is closely related to the computational understanding of humor. Previous studies have addressed humor recognition [32–34] and generation [35], with recent work expanding to multimodal data, such as visual humor prediction [36], humorous cartoon caption identification [37, 7, 38], and humor prediction in videos [39, 40]. Despite advancements, recent work shows that LLMs such as ChatGPT has not fully solved computational humor yet [41]. In this work, we design tasks to evaluate large vision-language models on their ability to understand humor through comic juxtapositions with contradictory narratives, requiring deep narrative comprehension. Through this study, we aim to provide insights into the capabilities of AI in processing and appreciating humor.\n\nInterpretation of Human Creative Expressions. Visual artwork, encompassing mediums such as drawings, paintings, and sculptures, has been a profound aspect of human culture and cognition. These creative expressions are not merely decorative; they are deeply entwined with the ways humans perceive, interpret, and communicate their experiences and emotions [42]. Understanding these human creative expressions necessities valuable insights of human emotions, societal values, and cultural contexts, which is crucial for developing socially intelligent systems and enhancing AI-related creativity [43]. Previous research has explored AI interpretation of visual human creative expressions in tasks such as meme [44] and cartoon [45] understanding. Similar to our work, studies like [7] and [10] apply AI models to comprehend comics. However, these studies primarily focus on single-panel comics, emphasizing humor and deep semantics. In contrast, our work aims to investigate the significant feature of juxtaposition for understanding contradictory narratives.\n\nVisual Reasoning. Our task is also related to the visual reasoning ability, where the model requires in-depth reasoning to comprehend contradictions between two comic panels. Previous research has examined visual reasoning capabilities of large models in tasks involving commonsense reasoning [46, 28, 47], visual question answering [48], visio-linguistic compositionality [49], and science question answering [50]. Unlike these studies, our task involves nonlinear reasoning, which necessitates AI to navigate multi-dimensional and complex information layers, often without explicit directives. While linear reasoning present their challenges, they usually exhibit clearer rules and structures, making them more accessible for AI to process with existing algorithms and models. Consequently, nonlinear reasoning represents a more intricate task, demanding higher natural language processing and cognitive modeling capabilities from AI systems.\n\n# 3 The YESBUT Dataset\n\nOur benchmark consists of YESBUT comics featuring contradictory narratives. Specifically, each sample includes: (1) a two-panel comic that forms a narrative with inherent contradictions; (2) a literal description of the comic narratives; (3) an explanation that illustrates the contradiction within the narrative; (4) the deep philosophy or underlying message the comic aims to convey; and (5) a title of the comic. Based on these components, we construct various tasks for comic understanding.\n\n# 3.1 Image Collection\n\nOur dataset consists of captionless comics, primarily from Anton Gudim’s \"YES, BUT\" series [51], each featuring two panels depicting contradictory everyday scenarios. We scraped the images from social media 2 and conducted preprocessing, including deduplication, filtering out comics with more than two panels, and removing any inappropriate or offensive content. This process resulted in a final dataset of 348 comics.\n\nComic with two panels: [ ] Step 1: Narrative Description Writing   \nYes, But ① Literal Description ② Contradiction The comic shows a businessman with two different graph The contradiction arises from perspectives. The left side displays a close-up of a chart with an selective disclosure, juxtaposing upward spike, suggesting immediate success. While the right the immediate triumph against the reveals the full graph with an overall downward trend, (...) broader context of defeat. [ ] Step 2: Deep Contents Writing $\\textcircled{3}$ Underlying Philosophies (Pos) The comic comments on the (Neg1) It portrays the lesson that personal confidence deceptive nature of presenting can remain unaltered by market fluctuations (…) information without full transparency, (Neg2) The comic endorses the practice of cherryreminding us to consider the full picking information (…) picture and not be swayed by Cross - LBieansgrtehdcuocntitronl selective or incomplete information. (eNcoegn3o)mIticproesaelistiaescr(it…i)cism that ego can blind one to   \nVerification - Readability - Style consistency ④ Title (Neg1) Graphs Don't Lie, People Do (Pos) Selective Statistics: A Tale (Neg2) Profit and Loss: The Dual Faces of Business of Two Perspectives ] Step 3: Quality Check (Neg3) Graphs of Success: A Businessman's Journey\n\n# 3.2 Data Annotation\n\nFor each comic, we annotate the corresponding literal description, contradiction explanation, underlying philosophy and comic title [7, 10]. We primarily rely on human annotators to obtain goldstandard annotations. Eight human judges participated in the annotation process, all of whom are proficient English speakers based in English-speaking countries and have at least a Bachelor’s degree. Our annotation process included two stages: the progressive human-AI collaborative annotation stage and the quality check and cross-verification stage. The pipeline is illustrated in Figure 2.\n\nProgressive Human-AI Collaborative Annotation. In this stage, we randomly assigned comic samples to each annotator, instructing them to first exclude any comics that may contain offensive, hateful, or sexual material before beginning the annotation process. To reduce human effort and costs associated with data annotation from scratch, we designed a human-AI collaboration pipeline utilizing GPT-4 [52] for data annotation and component writing.\n\nThe pipeline operates through dialogue interactions with the GPT-4 model. Given a comic image, we first prompt GPT-4 to generate narrative descriptions, illustrating the comic’s narrative and explaining the contradictory logic between the two panels. Human annotators then modify and annotate the contents to obtain a literal description and contradiction explanation.\n\nAfter obtaining the gold-standard description, both the comic and the description are used as input to prompt GPT-4 deep content writing, including the underlying philosophy and an eye-catching comic title. The underlying philosophy aims to foster a deep understanding of the comic and reveal the phenomenon it satirizes or the lesson it conveys; and the title is a more abstractive expression that reflects the overall narrative. Both components will be further checked by human annotators. Additionally, for the underlying philosophy and title understanding tasks, GPT-4 generates hard negative counterparts and distractions to design multiple-choice questions for our experiments. The prompts we used are provided in the Appendix A.\n\nOur human-AI collaborative annotation pipeline is effective as it leverages a progressive prompting strategy, annotating each component from easy to difficult. Understanding the underlying philosophy of the comic, for example, requires first understanding the literal narratives and contradictions. This approach reduces annotation costs and improves overall efficiency.\n\nQuality Check with Cross Verification. To ensure the quality and accuracy of the components and reduce objective bias from different human annotators, we introduced a cross verification stage. In this process, one annotator is assigned as an inspector for each comic. The inspector checks the annotated results to ensure all components are correct, unbiased, and appropriate. If any content is found to be of low quality or ambiguous, a third annotator is brought in as a judge to determine the final version. We exclude the comics with ambiguous or controversial narratives. This process ensures the quality of the annotated components for benchmark construction.\n\nAfter the cross-verification process, one of the authors reviews each sample to verify the validity and appropriateness of the components. Finally, we obtain a dataset of 348 comics, each accompanied by high-quality components. The statistics of the components are shown in Table 1.\n\nMitigating Annotation Bias. Our benchmark focuses on common interpretation of humor. However, the subjectivity of this task may introduce bias. To mitigate this issue, we have taken several steps in our annotation process: (1) Our annotators come from different genders and diverse cultural backgrounds, providing a range of perspectives; (2) Multiple quality checks and verifications are incorporated to ensure consensus among different annotators, with controversial or potentially biased comics being filtered out; (3) Annotations are further validated by cross-referencing social media comments for each comic to ensure alignment with widely accepted interpretations; (4) Recognizing that tasks such as generating titles and philosophical contents are inherently open-ended and involve subjective data annotation, we frame them as selection tasks, and ensure that the correct option is clearly and objectively superior than the negative options to mitigate subjectivity.\n\n<html><body><table><tr><td>Components</td><td>#Num</td><td>Avg. Len.</td></tr><tr><td>Image</td><td>348</td><td>-</td></tr><tr><td>Literal Description</td><td>348</td><td>80</td></tr><tr><td>Contradiction</td><td>348</td><td>31</td></tr><tr><td>Philosophy</td><td>1,392</td><td>24</td></tr><tr><td>Title</td><td>1,392</td><td>6</td></tr></table></body></html>\n\nTable 1: Data Statistics. Avg. Len. is the average number of words.\n\n# 3.3 Task Design: Do Large Models Understand Humor in Juxtaposition?\n\nWe aim to evaluate the capabilities of recent large (visual) language models in understanding humor through contradictions. This is challenging because it requires both social reasoning about human events and nonlinear logical reasoning about the narratives, going beyond the literal understanding of the comic. We design a series of tasks that require different levels of narrative understanding and reasoning abilities to evaluate the models’ performance in reading comics.\n\n1. Literal Description Writing. The first task is to generate the literal description of the comic narrative. We formulated this task as a text generation task: given an input comic, the model is required to generate a short description illustrating the narrative from the two panels of the comic. This task is different from the traditional image captioning, which requires the model to illustrate the comic narrative instead of solely focusing on image description.\n\n2. Contradiction Generation evaluates whether the model can understand the contradiction within the narrative juxtaposition. Similarly, it is formulated as a text generation task.\n\n3. Underlying Philosophy Selection. Understanding comics requires grasping not only the surface meaning of the images but also the underlying ideas the authors aim to convey. This task evaluates the model’s ability to recognize the comic’s underlying philosophy. It is formulated as a multiplechoice question answering (MCQ) task: given an input comic and four candidates of its underlying philosophy, the model must predict the correct option. The negative choices are crafted by annotators to be relevant to the comic, requiring reasoning to make the correct prediction.\n\n4. Title Matching evaluates whether the models can identify the corresponding title, which is challenging because the title acts as an abstraction of the narrative and requires a proper understanding of the comic’s content. Similar to the underlying philosophy task, it is formulated as a multiple-choice question answering task, where the most is asked to select the correct title from four options.\n\n# 4 Experiments\n\n# 4.1 Models and Settings\n\nWe evaluate the models’ performance in a zero-shot manner using both recent VLMs and LLMs. For VLMs, the comic image and questions are provided as inputs for output prediction. We include both commercial models such as GPT-4 [52] and Claude-3[53], as well as open-sourced models including LLaVa [3, 54], CogVLM [55], Qwen-VL [56], mPLUG-Owl2 [57], and InstructBLIP [58].\n\nFor LLMs, since they cannot directly process images, we use the LLaVa-1.6 13B model generated literal descriptions as inputs due to its strong performance. We include ChatGPT, the Llama3 instruction model [20], and the Mistral 7B instruction model [59]. More details of the models are included in Appendix B.\n\nTable 2: Main results. For literal description and contradiction, we report BERT score (recall), BLEURT (BLT), and GPT evaluation score. For philosophy and title, we report accuracy $( \\% )$ . Best scores are bold and the second best ones are marked with underline.   \n\n<html><body><table><tr><td colspan=\"2\"></td><td colspan=\"3\">Literal Description</td><td colspan=\"3\">Contradiction</td><td>Philosophy</td><td>Title</td></tr><tr><td>Setting</td><td>Model</td><td>BERT</td><td>R-2</td><td>GPT</td><td>BERT</td><td>R-2</td><td>GPT</td><td>Accuracy</td><td>Accuracy</td></tr><tr><td rowspan=\"10\">VLMs</td><td>GPT-4</td><td>88.32</td><td>87.46</td><td>3.76</td><td>87.64</td><td>83.21</td><td>4.03</td><td>82.76</td><td>60.25</td></tr><tr><td>Claude-3</td><td>87.68</td><td>80.30</td><td>3.28</td><td>86.93</td><td>80.63</td><td>3.79</td><td>84.10</td><td>56.42</td></tr><tr><td>LLaVA-1.6-34B</td><td>86.45</td><td>67.67</td><td>2.86</td><td>86.04</td><td>75.95</td><td>3.51</td><td>78.83</td><td>63.31</td></tr><tr><td>LLaVA-1.6-13B</td><td>81.34</td><td>75.95</td><td>2.96</td><td>86.48</td><td>80.96</td><td>3.36</td><td>69.16</td><td>55.08</td></tr><tr><td>LLaVA-1.5-13B</td><td>78.77</td><td>58.21</td><td>2.51</td><td>86.48</td><td>67.67</td><td>3.36</td><td>69.73</td><td>48.75</td></tr><tr><td>InstructBlip-13B</td><td>85.20</td><td>35.28</td><td>2.69</td><td>85.54</td><td>51.15</td><td>2.54</td><td>30.75</td><td>22.70</td></tr><tr><td>CogVLM</td><td>80.80</td><td>55.51</td><td>2.65</td><td>87.07</td><td>69.96</td><td>3.76</td><td>61.30</td><td>49.52</td></tr><tr><td>Qwen-VL-Chat</td><td>79.03</td><td>51.58</td><td>2.76</td><td>86.41</td><td>59.77</td><td>3.25</td><td>59.10</td><td>42.05</td></tr><tr><td>mPlug-Owl2</td><td>78.26</td><td>47.38</td><td>2.57</td><td>86.20</td><td>48.05</td><td>2.59</td><td>62.17</td><td>43.10</td></tr><tr><td>LLaVA-1.6-7B</td><td>80.71</td><td>70.36</td><td>2.79</td><td>86.58</td><td>75.36</td><td>3.24</td><td>47.41</td><td>37.07</td></tr><tr><td rowspan=\"3\"></td><td>InstructBlip-7B</td><td>76.02</td><td>38.02</td><td>2.60</td><td>86.32</td><td>66.29</td><td>2.85</td><td>25.86</td><td>26.44</td></tr><tr><td>ChatGPT</td><td>1</td><td></td><td></td><td>87.78</td><td>67.42</td><td>3.54</td><td>75.86</td><td>49.52</td></tr><tr><td>Llama-3-8B-Instruct Mistral-7B-Instruct</td><td></td><td>-</td><td></td><td>87.41 87.01</td><td>70.52 67.70</td><td>3.59 3.64</td><td>72.13 66.00</td><td>49.71 45.98</td></tr></table></body></html>\n\nImplementation Details. For GPT-4 and ChatGPT, we set the temperature as 1. For other models, we use the default parameter settings during inference. To reduce variance across different task prompts, we create three distinct prompts for each task and report the average scores from three runs with each prompt. The specific prompts and additional details are in Appendix B.\n\n# 4.2 Evaluation Metrics\n\nFor the philosophy and title understanding tasks, which are formulated as multiple-choice question answering, we use accuracy as the evaluation metric. For generation tasks including literal description and contradiction, we apply reference-based evaluation metrics commonly used in text generation studies [60], and report ROUGE-2 (recall) [61] and BERT Score (recall) [62]. 3 Recent work shows GPT-based evaluation aligns well with human judgements [63–65], and we also apply ChatGPT for evaluation 4. The prompts for GPT-based evaluation are provided in Appendix B.\n\nDue to the limitation of the automatic evaluations for text generation, we also include human evaluation to assess the quality of the outputs for the literal description and contradiction generation tasks. We hire three human judges to rate each aspect on a scale of 1 (worst) to 5 (best). For literal description, following [44], we evaluate: (1) Correctness: Does the model output correctly convey the narrative of the comic? (2) Completeness: Does the model output cover all the important elements of the comic narrative? (3) Faithfulness: Can all contents from the model output be supported by the comic image (i.e., there are no hallucinations)? For contradiction generation, we evaluate Correctness and Faithfulness. More details are provided in Appendix B.4.\n\n# 5 Main Results\n\nThe main experimental results are shown in Table 2. For VLMs, the original image is directly used as input, while for LLMs, the generated comic description is used as input.\n\n![](images/694bb3598ded71e54b50cb29d0459f220770ed65a4c69f49b87d875303655548.jpg)  \nFigure 3: Human Evaluation on literal description and contradiction generation.\n\n# 5.1 Narrative Understanding Tasks\n\nLiteral Description: We evaluate the results of VLMs only for this task. We observe that the two commercial models generally outperform the smaller open-sourced models. Among these models, GPT-4 achieves the highest scores. For the open-sourced models, the larger model variants (13B) consistently achieve better scores than their 7B counterparts, indicating that larger models have a superior ability to understand the image and produce higher-quality literal descriptions.\n\nContradiction Generation: A similar trend is observed where GPT-4 and Claude-3 achieve better results than other VLM models. Notably, LLaVA-1.6 variants outperform their counterparts in generating contradiction descriptions. This is likely due to their improved reasoning ability and world knowledge [54], which are essential for understanding comic narratives and accurately capturing the relationship between the two panels. For LLMs, unlike VLMs, the Llama-3 and Mistral models achieve results comparable to ChatGPT. Another interesting observation is that Llama-3 and Mistral obtain similar or better results for contradiction generation compared to open-sourced VLMs, despite not having access to the original comic images.\n\n# 5.2 Deep Reasoning Tasks\n\nThe Underlying Philosophy Selection and Title Matching tasks require in-depth reasoning based on the comic narratives. As seen in Table 2, for philosophy selection, Claude-3 achieves the best accuracy with $8 4 . 1 0 \\%$ , while for title matching, the LLaVA-1.6 34B variant ranks the highest with $6 3 . 3 1 \\%$ accuracy. One key observation is that larger models usually perform better in-depth understanding of the comics, aligning with the findings that larger models typically exhibit superior reasoning abilities [66, 67].\n\nAdditionally, LLMs achieve performance comparable to open-sourced VL models. This can be attributed to the strong reasoning abilities of models like Llama-3 and Mistral [59, 20], which are crucial for understanding narratives and performing nonlinear reasoning to grasp deep semantics. Further analysis on the influence of descriptions for LLMs is provided in Section 6.1.\n\nAnother observation is that model performance on title matching is consistently lower than on underlying philosophy selection. Titles are shorter and more abstract versions of the narrative and do not explicitly convey the underlying idea of the comic. Therefore, distinguishing the correct title from distractions requires a deeper rigorous understanding and reasoning abilities, making it more challenging for models. Notably, the human evaluation results show a similar trend of our proposed GPT-based evaluation, demonstraining its effectiveness.\n\n# 5.3 Human Evaluations\n\nWe conduct human evaluations on 30 randomly selected samples to assess the output quality of literal descriptions and contradiction generation, as shown in Figure 3. Similar trends are observed in both human and automatic evaluations: commercial models generally outperform open-source models in producing both literal descriptions and contradictions, with GPT-4 achieving the highest scores in both tasks. Additionally, the scores for literal descriptions are consistently higher than those for contradictions across all models, suggesting that understanding narrative contradictions is more challenging than generating literal descriptions, which requires in-depth reasoning to compare the various aspects of both panels.\n\n![](images/bc5cd27fe8a6d2e8e286bd02adfd434b7701b9a3966a784323dcbdb95805c1d8.jpg)  \nFigure 4: LLMs using different image description as input.\n\n![](images/10d02ab0b89a7675b418c1cc74060b349a24358e2439561bc654193f2551de0f.jpg)  \nFigure 5: VLMs with image only input and image $^ +$ oracle description as inputs.\n\nA comparison of the scores for literal description and contradiction reveals a strong correlation between the two tasks: models that perform well on literal descriptions also tend to achieve good results on contradictions. This indicates that understanding comic juxtaposition requires a diverse set of skills, including image understanding, narrative comprehension, and reasoning abilities.\n\n# 6 Analysis and Discussion\n\n# 6.1 How Does Literal Understanding of the Comic Influence Deep Reasoning?\n\nWe investigate whether the quality of surface-level literal descriptions influences subsequent deep reasoning tasks. For LLMs, we provide different literal descriptions generated by LLaVA-1.6 7B and 13B variants, as well as oracle descriptions written by humans, as model inputs. The results are shown in Figure 4. As the quality of literal descriptions improves, the prediction accuracy for both underlying philosophy and title selection also improves. This demonstrates a strong correlation between deep reasoning and literal narrative understanding. However, a significant performance gap remains compared to when oracle descriptions are used.\n\nWe further examine the performance of VLMs by providing them with additional oracle descriptions. The results are shown in Figure 5. Compared to using only the comic image as input, augmenting with human-written literal descriptions significantly improves the deep reasoning results for all VLMs. This confirms that correctly reasoning about the underlying semantics of a comic requires first accurately understanding its surface narrative. However, the performance gap indicates that current VLMs still lag in narrative understanding.\n\nAdditionally, an interesting observation from Figures 4 and 5 is that when the oracle literal description is provided as (partial) input, LLMs tend to outperform their VLM counterparts in both philosophy and title selection. For example, LLaVA-1.6-7B employs Mistral-7B as the language model backbone, yet its performance under oracle description is significantly worse than that of Mistral-7B . One possible reason is that incorporating oracle descriptions makes VLM input much longer, thus making prediction more challenging. We provide further discussions in Section6.2.\n\n# 6.2 Is Decomposing Literal Description Helpful for Deep Reasoning of VLMs?\n\nVLMs typically predict results in an end-to-end fashion, requiring the model to perform image captioning, narrative understanding, and deep reasoning all at once. Here, we investigate whether decomposing the task into separate stages of narrative understanding and in-depth reasoning can improve model performance. Specifically, we first prompt the VLM to produce a literal description of a comic; then the VLM predicts results based on both the comic image and the description. The results are shown in Table 3.\n\no6 Human Reference The contradiction lies in the expectation by the product‘s presentation as disposable yet presumably effective, versus the reality experienced by the user, where the razor’s The contradiction is in GPT-4 performance is disappointingly short-lived. The humor stems from the contrast the disposable razor's 9 between the implied promise of quality and the actual inferior performance, a expected short-term 健 situation familiar to many who have used such razors. utility being contrasted with its The comic illustrates a humorous contradiction where, on one side, the man is depiction of providing CogVLM shown shaving off his beard, and on the other side, he is shown with a full beard. a lifetime of service. This plays on the idea that the razor is supposed to remove hair, but in this comedic scenario, it seems to have the opposite effect. Sample 1 Human Reference The contradiction here is that the bed is perceived differently by two individuals. One person sees it as a king-sized bed, which is large and well-kept, while the other The contradiction highlights the comedic LLaVA-13B person sees it as a queen-sized bed, which is smaller and disorganized. This highlights how people can have different interpretations of the same situation, which can lead truth that no matter to misunderstandings or disagreements. how large the bed, we often end up using just The comic shows two different perspectives of a person sleeping in a bed. On one a small part of it, and mPlug-Owl2 side, the person is sleeping comfortably in a king-size bed, while on the other side, it becomes a catch-all the person is sleeping in a small bed. The contradiction lies in the size of the bed, as for our belongings. the person is depicted as sleeping in both a large and a small bed simultaneously. Sample 2 This creates a humorous and confusing visual effect. (…) This highlights that even \"paper\" disposable cups often have plastic lids and inner Human Reference linings that make them challenging to recycle and still contribute to plastic waste. The Claude-3 juxtaposition exposes the misconception that choosing a paper cup is always a The contradiction is sustainable option, when in fact, the plastic components can still have negative between the barista's environmental impacts. creative expression and the practical act of On the left, the character is enjoying the coffee with the lid off, symbolizing the lidding the drink for LLaVA-13B beginning or the experience of the coffee. On the right, the character is now convenience. consuming the coffee with the lid on, which signifies the end or the aftermath of the coffee experience. This juxtaposition creates a visual metaphor for the fleeting Sample 3 nature of moments of relaxation or enjoyment.\n\nFigure 6: Sample outputs of contradiction explanations generated by different vision language models, along with human written references. We highlight different types of errors in model outputs.\n\nAs observed, decomposing the task and augmenting it with a literal description does not necessarily improve performance. In fact, when descriptions are incorporated, performance across all models declines on the title selection task, which contrasts with previous findings [10]. One possible explanation for this drop in performance is that the generated descriptions may contain errors, negatively impacting the model’s deep understanding. Another explanation could be the length of the generated descriptions (e.g., the LLaVA-1.6 13B model’s descriptions average around 170 words), leading to longer and more complex prompts that make prediction more challenging. We leave a more detailed investigation of this issue for future work.\n\n<html><body><table><tr><td colspan=\"2\">Models Philosophy</td><td>Title</td></tr><tr><td>LLaVA-1.6-13B</td><td>69.16</td><td>55.08</td></tr><tr><td>w/ desp.</td><td>68.68</td><td>48.76</td></tr><tr><td>Qwen-VL-Chat</td><td>59.10</td><td>42.05</td></tr><tr><td>→ w/ desp.</td><td>59.58</td><td>37.55</td></tr><tr><td>mPlug-Owl2</td><td>62.17</td><td>43.10</td></tr><tr><td>→w/ desp.</td><td>60.25</td><td>37.84</td></tr><tr><td>LLaVA-1.6-7B</td><td>47.41</td><td>37.07</td></tr><tr><td>→w/ desp.</td><td>53.07</td><td>34.96</td></tr></table></body></html>\n\nTable 3: Decomposition model results augmenting the predicted description.\n\n# 6.3 Error Analysis and Future Directions\n\nWe present sample outputs of contradictions generated by vision language models (VLMs) in Figure 6. VLMs can make various errors in contradiction understanding.\n\nOne type of error is visual misinterpretation, where the model incorrectly interprets the image contents. For example, in sample 1, CogVLM misinterprets the image by recognizing a person \"shown with a full beard.\" Similarly, in sample 2, LLaVA-1.6 13B misunderstands the image contents and generates incorrect content about \"two individuals,\" which is inconsistent with the comic. Such misinterpretations can lead to incorrect understanding of the narrative. These observations align with our previous findings in Section 6.1 and Section 6.2. This highlights the need for future research to improve models’ visual interpretation capabilities.\n\nModels also struggle to conduct in-depth reasoning of the relationship between two panels by recognizing their differences and similarities. In sample 1, while the comic implies a comparison between the expected disposable razor and its actual longevity, GPT-4 incorrectly explains the contradiction as being about the razor’s quality. A similar error occurs with mPlug-Owl2 in sample 2, where it incorrectly thinks the bed sizes are different in the two panels, leading to a wrong illustration focusing on the bed size. Future work might incorporate recent advanced reasoning approaches (e.g., multi-agent debate [68], test-time compute scaling [69]) to further improve model performance.\n\nAnother common error is hallucination and incorrect association. This is evident in sample 3. The original comic contrasts latte art before and after lidding the drink, but Claude-3 incorrectly associates the narrative with environmental protection, focusing on the plastic lid. Meanwhile, LLaVA1.6 13B model suffers from hallucinations by interpreting the narrative as being about relaxation and enjoyment, which is unsupported by the original comic. This suggests the need for improving world knowledge and social understanding abilities to enhance model performance on this task. More sample outputs are in Appendix C.\n\n# 7 Conclusion\n\nIn this work, we present YESBUT, the first benchmark dedicated to studying comic understanding through juxtaposition. YESBUT encompasses a variety of tasks that address both narrative comprehension and deep reasoning. The results indicate that state-of-the-art vision and language models still struggle with these tasks. We also offer a comprehensive analysis and discussion of errors to evaluate model performance. Current models still struggle to accurately interpret the visual contents and conduct in-depth reasoning of the underlying narratives. Through this study, we aim to provide insights for future research and advance the capabilities of AI models in understanding human context, ultimately contributing to more effective and culturally aware AI applications.\n\n# 8 Limitations\n\nWe propose a comprehensive data annotation process to annotate each component. However, due to the subjectivity of comic interpretation, especially regarding the underlying ideas, there might be potential ambiguity. While we acknowledge the relatively small size of images, we rigorously collect comics and annotate each component, ensuring their high-quality and reliability. We plan to expand the dataset with the inclusion of different types of narratives in future work.\n\nOur proposed benchmark focuses predominantly on recognizing and interpreting visual humor via juxtaposition, and may not cover all aspects of visual understanding required for more generalized AI applications. In the future, we intend to explore more deeply how AI can not only interpret but also creatively engage with content. This includes generating pivotal turning points from one perspective and creating counterpoints to given scenarios, like generating a \"YES\" image’s counterpart.\n\n# 9 Ethics Statement\n\nCopyright and License. All data samples collected are sourced from publicly available content on social media platforms. We ensure compliance with copyright by utilizing original links to comics without infringement. In addition, we obtained permission from the author artist (e.g., Anton Gudim, Liz Climo) to conduct our benchmark using these public images. Additionally, we commit to opensourcing our annotated benchmark, providing corresponding links to each comic image. We diligently review samples, filtering out potentially offensive or harmful content.\n\nThe Large Vision Language Models utilized in our experiments are pretrained using diverse web corpora, which may introduce biases in their outputs. We advise users to conscientiously evaluate the ethical implications of generated outputs when employing them in future research endeavors.\n\nData Annotation. Eight human judges are engaged in our annotation process. We compensate these judges with an average hourly wage of $\\$ 11$ , ensuring fair remuneration for their contributions.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n当前大型（视觉）语言模型在理解通过并置手法呈现的非线性叙事幽默方面能力不足。理解漫画对于推进AI的社会和语义理解至关重要，有助于开发具有社会智能的系统，提升AI相关的创造力，改善推荐系统和自动内容创作工具等应用的用户体验。\\n\\n**方法概述**\\n提出YESBUT基准，包含文字描述写作、矛盾生成、潜在哲学选择和标题匹配等不同难度的任务，用于评估AI识别和解释具有矛盾叙事的漫画的能力。通过对商业和开源的大型（视觉）语言模型进行广泛实验和分析，评估它们对漫画中叙事幽默的理解能力。\\n\\n**主要贡献与效果**\\n- 构建了首个专注于通过并置手法研究漫画理解的YESBUT基准，涵盖348个漫画样本。\\n- 实验表明，即使是最先进的模型在理解漫画幽默方面仍落后于人类表现，如潜在哲学选择任务最高准确率为84.1%，标题匹配任务最高准确率为63.3%。\\n- 分析发现，用人工编写的文字描述增强模型输入可显著提高其深度推理能力，如为VLMs提供额外的人工文字描述后，其在深层推理任务上的结果有明显提升，但当前VLMs在叙事理解方面仍存在差距。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n通过设计不同难度的漫画理解任务，评估大型（视觉）语言模型对漫画中矛盾叙事和幽默的理解能力，推动模型发展出更复杂的语义理解和解释能力。其有效性在于这些任务涵盖了从文字内容理解到深度叙事推理的不同层次，能够全面考察模型的能力。\\n\\n**创新点**\\n先前的研究多关注单格漫画，未深入研究通过并置手法创造的非线性叙事。本文聚焦于通过并置手法产生幽默的漫画，设计了一系列任务，包括文字描述写作、矛盾生成、潜在哲学选择和标题匹配，全面评估模型对漫画的理解能力。此外，在数据标注过程中采取了选用不同背景标注者、多次质量检查、参考社交媒体评论和设置选择任务等多种措施来减轻标注偏差，确保基准的质量。\\n\\n**具体实现步骤**\\n1. **图像收集**：从Anton Gudim的“YES, BUT”系列中收集无字幕漫画，进行预处理，包括去重、过滤多面板漫画和去除不当内容，最终得到348个漫画样本。\\n2. **数据标注**：采用渐进式人机协作标注和质量检查与交叉验证两个阶段。先让GPT - 4生成叙事描述和矛盾解释，人类注释者修改；再用GPT - 4生成深层内容和标题，人类注释者检查。最后由作者审核，确保数据质量。同时，为减轻标注主观性，采取了选用不同背景标注者、多次质量检查、参考社交媒体评论和设置选择任务等措施。\\n3. **任务设计**：设计文字描述写作、矛盾生成、潜在哲学选择和标题匹配四个任务，分别评估模型不同层次的漫画理解能力。文字描述写作和矛盾生成任务为文本生成任务；潜在哲学选择和标题匹配任务为多项选择题回答任务。\\n4. **实验评估**：使用零样本方式评估模型，包括商业和开源的VLMs和LLMs。对于VLMs，输入漫画图像和问题；对于LLMs，使用LLaVa - 1.6 13B模型生成的文字描述作为输入。采用准确率、ROUGE - 2（召回）、BERT Score（召回）和GPT评估分数等指标，并进行人工评估。为减少不同任务提示的差异，为每个任务创建三个不同的提示，并报告三次运行的平均分数。\\n\\n**案例解析**\\n论文中以一个漫画为例，左边面板显示司机停车让鸭子过马路，右边面板显示司机进入“北京烤鸭”餐厅，通过并置手法突出了人与动物关系的矛盾，用于说明漫画中通过并置产生幽默的叙事方式。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n商业模型如GPT - 4、Claude - 3；开源模型包括LLaVa、CogVLM、Qwen - VL、mPLUG - Owl2、InstructBLIP等VLMs，以及ChatGPT、Llama3指令模型、Mistral 7B指令模型等LLMs。\\n\\n**性能对比**\\n*   **在文字描述任务上**：本文的评估指标为BERT Score（召回）、ROUGE - 2（召回）和GPT评估分数。商业模型通常优于开源模型，其中GPT - 4表现最佳，其BERT Score达到88.32，显著优于开源模型中的InstructBlip - 7B（76.02）。对于开源模型，较大的模型变体（13B）通常比7B的表现更好。\\n*   **在矛盾生成任务上**：评估指标同样为BERT Score（召回）、ROUGE - 2（召回）和GPT评估分数。GPT - 4和Claude - 3表现较好，如Claude - 3的BERT Score为86.93，高于开源模型中的mPlug - Owl2（86.20）。LLaVA - 1.6变体在生成矛盾描述方面表现出色，如LLaVA - 1.6 - 13B的BERT Score为86.48。LLMs中的Llama - 3和Mistral模型在矛盾生成任务上取得了与ChatGPT相当或更好的结果，尽管它们没有直接访问原始漫画图像。\\n*   **在潜在哲学选择任务上**：评估指标为准确率。Claude - 3的准确率最高，达到84.10%，优于其他模型，如InstructBlip - 7B的准确率仅为25.86%，Claude - 3的准确率高出InstructBlip - 7B约58.24个百分点。\\n*   **在标题匹配任务上**：评估指标为准确率。LLaVA - 1.6 34B变体的准确率最高，为63.31%，高于其他模型，如InstructBlip - 7B的准确率为26.44%，LLaVA - 1.6 34B变体的准确率高出InstructBlip - 7B约36.87个百分点。\",\n    \"keywords\": \"### 关键词\\n\\n- 漫画理解 (Comic Understanding, N/A)\\n- 并置幽默 (Juxtaposition Humor, N/A)\\n- YESBUT基准 (YESBUT Benchmark, N/A)\\n- 大型（视觉）语言模型 (Large (Vision) Language Models, VLMs/LLMs)\"\n}"
}