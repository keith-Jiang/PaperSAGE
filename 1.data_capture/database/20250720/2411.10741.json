{
    "source": "Semantic Scholar",
    "arxiv_id": "2411.10741",
    "link": "https://arxiv.org/abs/2411.10741",
    "pdf_link": "https://arxiv.org/pdf/2411.10741.pdf",
    "title": "MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map",
    "authors": [
        "Yuhong Chou",
        "Man Yao",
        "Kexin Wang",
        "Yuqi Pan",
        "Rui-Jie Zhu",
        "Yiran Zhong",
        "Yu Qiao",
        "Jibin Wu",
        "Boxing Xu",
        "Guoqi Li"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-11-16",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 7,
    "influential_citation_count": 3,
    "institutions": [
        "The Hong Kong Polytechnic University",
        "Institute of Automation, Chinese Academy of Sciences",
        "UC Santa Cruz",
        "Shanghai AI Lab"
    ],
    "paper_content": "# MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map\n\nYuhong Chou1,2,∗Man Yao2∗, Kexin $\\mathbf { W a n g ^ { 2 } }$ , Yuqi $\\mathbf { P a n ^ { 2 } }$ , Ruijie $\\mathbf { Z } \\mathbf { h } \\mathbf { u } ^ { 3 }$ , Yiran Zhong4, Yu Qiao4, Jibin $\\mathbf { W _ { u } ^ { 1 } }$ , Bo $\\mathbf { X } \\mathbf { u } ^ { 2 }$ , Guoqi $\\mathbf { L i } ^ { 2 ^ { \\cdot } }$ †\n\n1The Hong Kong Polytechnic University 2Institute of Automation, Chinese Academy of Sciences 3UC Santa Cruz 4Shanghai AI Lab\n\n# Abstract\n\nVarious linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures. However, the optimal design of these linear models is still an open question. In this work, we attempt to answer this question by finding the best linear approximation to softmax attention from a theoretical perspective. We start by unifying existing linear complexity models as the linear attention form and then identify three conditions for the optimal linear attention design: i) Dynamic memory ability; ii) Static approximation ability; iii) Least parameter approximation. We find that none of the current linear models meet all three conditions, resulting in suboptimal performance. Instead, we propose Meta Linear Attention (MetaLA) as a solution that satisfies these conditions. Our experiments on Multi-Query Associative Recall (MQAR) task, language modeling, image classification, and Long-Range Arena (LRA) benchmark demonstrate that MetaLA is more effective than the existing linear models. Code: https://github.com/BICLab/MetaLA\n\n# 1 Introduction\n\nTransformer with softmax attention [1] benefits from efficient parallel training and exhibits impressive performance on deep learning applications [2, 3, 4, 5, 6, 7], but it suffers from the quadratic growth of computation cost to the input length [8]. Linear recurrent models, such as LinFormer [9], SSM [10], and LinRNN [11], are expected to achieve linear substitution of Transformer. The original intention of LinFormer is to replace softmax attention, which exploits the kernel approach to decompose softmax operation; typical work includes TransNormer [12, 13], RetNet [14], GLA [15]. On the other hand, SSMs, such as S4 [10] and Mamba [16], are models inspired by the classical state-space approach, which enjoys sub-quadratic training and inference like either a recurrence or convolution. In contrast, LinRNN is a revival of traditional RNNs, including RWKV-4 [17], Griffin [18], LRU [19], etc., which solves the training difficulties of traditional RNNs due to nonlinear dependencies between hidden states. It is natural to think that they are different types of models, since these LinFormer/SSM/LinRNN models have different origins and forms.\n\nThis work breaks this perception and abstracts existing LinFormer/SSM/LinRNN models into a unified linear attention form, which has the following significance: i) Facilitates understanding the key designs of existing linear models. Through the unified form, we demonstrate that the main difference between LinFormer/SSM/LinRNN is the hidden state size, how to maintain the hidden state, and how to perform parameter mapping. ii) Links LinFormer/SSM/LinRNN to softmax attention in terms of functionality. The recurrent inference complexity of softmax attention is ${ \\mathcal { O } } ( n )$ , which can also be regarded as the maintenance of a hidden state with infinite size. Linear models with $\\mathcal { O } ( 1 )$ inference complexity are hoping to achieve the same functionality as softmax attention using a fixed hidden state. Since we have unified LinFormer/SSM/LinRNN into linear attention in the form of Query, Key, and Value, we can understand and evaluate existing linear models from the view of “Does the linear attention map have the function of softmax attention map?”.\n\nTo answer this question, we define the necessary conditions for achieving “optimal linear approximation to softmax attention map\". First, linear attention needs to satisfy dynamic memory and static approximation to realize the approximation. The former defines memory ability: linear attention with limited hidden states should be able to store the most important information and forget unimportant ones. The latter defines the modeling ability: a linear attention map should be able to approximate any softmax attention map. According to our theoretical analysis, Query and dynamic decay are necessary conditions for approximation. Thus, linear models such as TransNormer [13], RetNet [14], RWKV-4 [17], LRU [19], HGRN [20], H3 [21], S5 [22], cannot achieve approximation of the softmax attention functions. Second, the Key matrix is not required to achieve approximation, so Mamba [16] and GLA [15] are not optimal parametric approximations.\n\nWe then propose the MetaLA module, which can satisfy the necessary conditions for optimal linear approximation to softmax attention. MetaLA makes three enhancements: i) Removes the unnecessary Key matrices; ii) Employs self-augmentation to enhance the token’s attention to itself, which avoids attention dilution [12]; iii) Exploits short convolutions to enhance local interactions. We then build a MetaLA Transformer based on MetaLA. Our experiments on associative recall, language modeling, long sequence modeling, and image classification show the effectiveness of MetaLA. Furthermore, we conduct ablation studies to validate the effectiveness of each proposed enhancement in MetaLA. Finally, we discuss two open questions: i) How to further improve linear attention based on the approximation theory introduced in this work? ii) Does the approximation of linear attention to softmax attention imply that it has an upper limit on its capacity?\n\n# 2 Background\n\nFor notations in this work, we use bold upper-case letters for matrices (e.g., Q, K), bold lower-case letters for row vectors (e.g., ${ \\bf q } _ { t } , { \\bf k } _ { t } )$ , and italic upper-case for learnable parameter matrices $( \\mathrm { e . g . , } W _ { Q } )$ . We generally use the same alphabet to show the rows of a matrix, e.g., $\\mathbf { q } _ { t }$ is the $t$ -th row of $\\mathbf { Q }$ .\n\nSoftmax Attention first calculates an attention map SoftAttMap $( \\mathbf { Q } , \\mathbf { K } )$ through $\\mathbf { Q }$ (Query), $\\mathbf { K }$ (Key), and use the attention map to weight different tokens $\\mathbf { V }$ (Value) later:\n\n$$\n\\begin{array} { r l } { \\mathbf { O } = \\mathrm { S o f t A t t M a p } \\left( \\mathbf { Q } , \\mathbf { K } \\right) \\mathbf { V } = \\mathrm { s o f t m a x } \\left( \\frac { \\mathbf { Q } \\mathbf { K } ^ { \\top } } { \\sqrt { d _ { k } } } \\odot \\mathbf { M } \\right) \\mathbf { V } } & { { } \\in \\mathcal { R } ^ { n \\times d _ { v } } , } \\\\ { \\mathbf { Q } , \\mathbf { K } = \\mathbf { X } W _ { Q } , \\mathbf { X } W _ { K } } & { { } \\in \\mathcal { R } ^ { n \\times d _ { k } } ; \\quad \\mathbf { V } = \\mathbf { X } W _ { V } \\quad \\in \\mathcal { R } ^ { n \\times d _ { v } } , } \\end{array}\n$$\n\nwhere $W _ { Q } , W _ { K } \\in \\mathcal { R } ^ { d \\times d _ { k } }$ , $W _ { V } \\in \\mathcal { R } ^ { d \\times d _ { v } }$ are learnable matrices, $n , d , d _ { k } , d _ { v }$ are sequence length, model dimension, Key/Query and Value dimension, respectively. $\\mathbf { X } \\in \\mathcal { R } ^ { n \\times d }$ refers to the input. $\\mathbf { M } \\in \\mathcal { R } ^ { n \\times n }$ is a mask matrix in autoregressive tasks to prevent a token from attending to future tokens. The $t$ -th row of SoftAttMap $( \\mathbf { Q } , \\mathbf { K } )$ is a probability distribution that represents the attention scores between token $\\mathbf { v } _ { t }$ to others. Softmax attention in Eq. (1) enables efficient parallel training, but suffers from $\\mathcal { O } ( n ^ { 2 } )$ time and memory complexity [9]. It uses the recurrent form during inference:\n\n$$\n\\begin{array} { r l } { \\mathbf { o } _ { t } = \\displaystyle \\frac { \\sum _ { s = 1 } ^ { t } \\exp ( \\mathbf { q } _ { t } \\mathbf { k } _ { s } ^ { \\top } ) \\mathbf { v } _ { s } } { \\sum _ { s = 1 } ^ { t } \\exp ( \\mathbf { q } _ { t } \\mathbf { k } _ { s } ^ { \\top } ) } } & { { } \\in \\mathcal { R } ^ { 1 \\times d _ { v } } , } \\\\ { \\mathbf { q } _ { t } , \\mathbf { k } _ { t } = \\mathbf { x } _ { t } W _ { Q } , \\mathbf { x } _ { t } W _ { K } } & { { } \\in \\mathcal { R } ^ { 1 \\times d _ { k } } ; \\quad \\mathbf { v } _ { t } = \\mathbf { x } _ { t } W _ { V } \\quad \\in \\mathcal { R } ^ { 1 \\times d _ { v } } . } \\end{array}\n$$\n\nAt each time $t$ , token mix is computed between query $\\mathbf { q } _ { t }$ and all the keys, values before $\\mathbf { k } _ { s } , \\mathbf { v } _ { s } ( s \\leq t )$ .   \nThis \"KV cache\" results in $\\mathcal { O } ( n )$ time and memory complexity per token during inference.\n\nLinear Transformer (LinFormer) is a substitute for softmax attention, which can be expressed as a linear dot-product of kernel feature maps [9]:\n\n$$\n\\mathbf { o } _ { t } = \\frac { \\sum _ { s = 1 } ^ { t } F ( \\mathbf { q } _ { t } , \\mathbf { k } _ { s } ) \\mathbf { v } _ { s } } { \\sum _ { s = 1 } ^ { t } F ( \\mathbf { q } _ { t } , \\mathbf { k } _ { s } ) } , \\quad F ( \\mathbf { q } _ { t } , \\mathbf { k } _ { s } ) = \\phi ( \\mathbf { q } _ { t } ) \\phi ^ { \\top } ( \\mathbf { k } _ { s } ) ,\n$$\n\nwhere $\\mathbf { q } _ { t } , \\mathbf { k } _ { t } \\in \\mathcal { R } ^ { 1 \\times d _ { k } }$ and $\\mathbf { v } _ { t } \\in \\mathcal { R } ^ { 1 \\times d _ { v } }$ are query, key and value at position $t$ , which are obtained in the same manner as softmax attention. $\\bar { F } ( \\cdot )$ is the kernel function usually constrained to be non-negative. $\\phi ( \\cdot )$ is map function applied row-wise to $\\mathbf { Q }$ and $\\mathbf { K }$ . By removing the nonlinear softmax operation, LinFormer enables inference with $\\mathcal { O } ( 1 )$ time and memory complexity per token. LinFormer can also be formulated in the following parallel form during training\n\n$$\n\\mathbf { O } = \\left( \\phi ( \\mathbf { Q } ) \\phi ^ { \\top } ( \\mathbf { K } ) \\odot \\mathbf { M } \\right) \\mathbf { V } \\quad \\in \\mathcal { R } ^ { n \\times d _ { v } } ,\n$$\n\nwhich has $\\mathcal { O } ( n )$ time and memory complexity using chunkwise algorithm. Recent advances in LinFormer mainly focus on training acceleration[12, 13, 23] or improving performance[14].\n\nState-Space Model (SSM) come from continuous-time system which maps a 1D function $x ( t ) \\in \\mathcal { R }$ to another function $y ( t ) \\in \\mathcal { R }$ via a hidden state ${ \\bf h } ( t ) \\in \\mathcal { R } ^ { N }$ . In SSM, the continuous parameters can be discretized using a step size $\\Delta$ and get discrete parameters ${ \\overline { { A } } } , { \\overline { { B } } } , { \\overline { { C } } }$ . The resulting discrete-time system is used to model sequences $\\mathbf { x } , \\mathbf { \\bar { y } } \\in \\mathcal { R } ^ { 1 \\times n }$ with elements $x _ { t } , y _ { t } \\in \\mathcal { R }$ via the recurrent form:\n\n$$\n\\mathbf { h } _ { t } = \\overline { { \\pmb { A } } } \\mathbf { h } _ { t - 1 } + \\overline { { \\pmb { B } } } \\ v { x } _ { t } , \\quad \\boldsymbol { y } _ { t } = \\overline { { \\pmb { C } } } \\mathbf { h } _ { t } ,\n$$\n\nin autoregressive inference with $\\mathcal { O } ( 1 )$ time and memory complexity per token. The linear timeinvariant SSM above can be unrolled and computed using the long convolution with kernel $\\kappa$\n\n$$\n\\begin{array} { r l } { K : = ( \\overline { { C B } } , \\overline { { C A B } } , \\cdot \\cdot \\cdot , \\overline { { C A } } ^ { n - 1 } \\overline { { B } } ) } & { { } \\in \\mathcal { R } ^ { 1 \\times n } , \\quad \\mathbf { y } = K \\ast \\mathbf { x } , } \\end{array}\n$$\n\nwhere $^ *$ represents casual convolution operation [24], which enables parallelizable training utilizing Fast Fourier Transforms, resulting in $\\mathcal { O } ( { n } \\log { n } )$ time and ${ \\mathcal { O } } ( n )$ memory complexity during training. When handling vector sequences $\\mathbf { X }$ ${ \\bf { X } } , { \\bf { Y } } \\in \\mathcal { R } ^ { d \\times n }$ , SSMs are applied individually on the $d$ channels. Typical SSMs (S4D[25], DSS[26], H3[21], S5[22]) employ data-independent structured transition matrix $\\overline { { \\mathbf { A } } }$ or special initialization strategies HiPPO[27] to efficiently enhance long-range dependencies. Mamba [16] advances SSMs by introducing data-dependent parameters and designs a hardware-aware parallel algorithm, further improving prior ${ \\mathcal { O } } ( n \\log n )$ into $\\mathcal { O } ( n )$ time complexity during training.\n\nLinear RNNs (LinRNN) Traditional RNNs suffer from slow sequential training, limited capability in modeling long-term dependencies, and difficulty in scaling. To address these issues, LinRNNs eliminate the nonlinearity within the recurrence and employ element-wise product instead of matrix multiplication [11, 28]. Typical LinRNN such as Gated Linear Recurrent Unit (GLRU) [20, 29] is\n\n$$\n\\begin{array} { r l } { \\mathbf { f } _ { t } , \\mathbf { i } _ { t } , \\mathbf { c } _ { t } = \\sigma ( \\mathbf { x } _ { t } W _ { f } + b _ { f } ) , \\sigma ( \\mathbf { x } _ { t } W _ { i } + b _ { i } ) , \\phi ( \\mathbf { x } _ { t } W _ { c } + b _ { c } ) } & { { } \\in \\mathbb { R } ^ { 1 \\times d } , } \\\\ { \\mathbf { h } _ { t } = \\mathbf { f } _ { t } \\odot \\mathbf { h } _ { t - 1 } + \\mathbf { i } _ { t } \\odot \\mathbf { c } _ { t } , \\in \\mathcal { R } ^ { 1 \\times d } , } \\end{array}\n$$\n\nwhere $\\mathbf { x } _ { t } , \\mathbf { h } _ { t }$ denote input and output, $\\mathbf { f } _ { t } , \\mathbf { i } _ { t }$ are forget and input gates as in traditional RNNs, $\\odot$ is element-wise multiplication. Linear RNNs have $\\mathcal { O } ( 1 )$ time and memory complexity per token during inference. Since Eq. (10) removes nonlinearity, it enables parallelized training using parallel scan[11], with only ${ \\mathcal { O } } ( n )$ time and memory complexity. Recent works have made effort to explore effective recurrence (LRU [19], RWKV [17]) or gating mechanisms (HGRN [20], Griffin [18]).\n\n# 3 General Form of LinFormer/SSM/LinRNN Mechanisms\n\nObserving Eq. (3), Eq. (5), Eq. (7), and Eq. (10), we find that their recurrent forms during inference can all be understood from the view of maintaining hidden states. Softmax attention maintains an unlimited hidden state (KV cache). By contrast, LinFormer/SSM/LinRNN have limited hidden states: linear attention with $\\phi ^ { \\mathsf { ^ { T } } } ( \\mathbf { k } _ { t } ) \\mathbf { v } _ { t } \\in \\mathcal { R } ^ { \\mathsf { ^ { d } } _ { k } \\times d _ { v } }$ , SSM with $\\mathbf { h } _ { t } \\in \\mathcal { R } ^ { N \\times d }$ , linear RNNs with $\\mathbf h _ { t } \\in \\mathcal { R } ^ { 1 \\times d }$ , where $d _ { k } > N > 1$ in usual. Inspired by this fact, we unify LinFormer/SSM/LinRNN mechanisms in the form of linear attention, formally containing Query, Key, and Value matrices (see Fig. 1).\n\n![](images/618028139a805ad28463007d4562289693b7306eba2881574a5632bb45c06de6.jpg)  \nFigure 1: General Form of LinFormer/SSM/LinRNN Mechanisms. The general form equips with two modes of parallel and recurrent computation which enjoys both training and inference efficiency.\n\n# General Recurrent Form of LinFormer/SSM/LinRNN is:\n\n$$\n\\begin{array} { r l r l } & { \\mathbf { q } _ { t } = \\mathbf { f } _ { q } ( \\mathbf { x } _ { t } , \\theta _ { q } ) , \\mathbf { k } _ { t } = \\mathbf { f } _ { k } ( \\mathbf { x } _ { t } , \\theta _ { k } ) , \\boldsymbol { \\alpha } _ { t } = \\mathbf { f } _ { \\alpha } ( \\mathbf { x } _ { t } , \\theta _ { \\alpha } ) } & & { \\in \\mathcal { R } ^ { 1 \\times d _ { k } } , } \\\\ & { \\mathbf { v } _ { t } = \\mathbf { f } _ { v } ( \\mathbf { x } _ { t } , \\theta _ { v } ) , \\mathbf { g } _ { t } = \\mathbf { f } _ { g } ( \\mathbf { x } _ { t } , \\theta _ { g } ) } & & { \\in \\mathcal { R } ^ { 1 \\times d _ { v } } , } \\\\ & { \\mathbf { S } _ { t } ^ { h } = \\mathrm { d i a g } ( \\alpha _ { t } ^ { h } ) \\mathbf { S } _ { t - 1 } ^ { h } + ( \\mathbf { k } _ { t } ^ { h } ) ^ { \\top } \\mathbf { v } _ { t } ^ { h } } & & { \\in \\mathcal { R } ^ { d _ { k } ^ { \\prime } \\times d _ { v } ^ { \\prime } } , } \\\\ & { \\mathbf { o } _ { t } = \\mathrm { X N o r m } ( \\mathrm { c o n c a t } [ \\mathbf { q } _ { t } ^ { 1 } \\mathbf { S } _ { t } ^ { 1 } , \\mathbf { q } _ { t } ^ { 2 } \\mathbf { S } _ { t } ^ { 2 } , \\cdot \\cdot \\cdot , \\mathbf { q } _ { t } ^ { H } \\mathbf { S } _ { t } ^ { H } ] ) } & & { \\in \\mathcal { R } ^ { 1 \\times d _ { v } } , } \\\\ & { \\mathbf { y } _ { t } = \\big ( \\mathbf { o } _ { t } \\odot \\mathbf { g } _ { t } \\big ) W _ { O } } & & { \\in \\mathcal { R } ^ { 1 \\times d } , } \\end{array}\n$$\n\nwhere $\\mathbf { x } _ { t } \\in \\mathcal { R } ^ { 1 \\times d }$ is the $t$ -th input. $\\mathbf { q } _ { t } , \\mathbf { k } _ { t } , \\mathbf { v } _ { t } , \\alpha _ { t } , \\mathbf { g } _ { t } , \\mathbf { S } _ { t }$ are query, key, value, decay, output gate, hidden state respectively. $\\mathrm { f } _ { q / k / \\alpha }$ are functions that map $\\mathbf { x } _ { t }$ from $\\mathcal { R } ^ { 1 \\times d }$ to $\\mathcal { R } ^ { 1 \\times d _ { k } }$ , $\\theta _ { q / k / \\alpha }$ are the corresponding parameters to be trained. Similarly, $\\mathrm { f } _ { v / g }$ map $\\mathbf { x } _ { t }$ from $\\mathcal { R } ^ { 1 \\times d }$ to $\\mathcal { R } ^ { 1 \\times d _ { v } }$ and $\\theta _ { v / g }$ are trainable parameters. In Eq. (13), ${ \\bf q } _ { t } , \\alpha _ { t } , { \\bf k } _ { t } , { \\bf v } _ { t }$ are divided into $H$ partitions (heads), where $\\begin{array} { r } { d _ { k / v } ^ { \\prime } = \\frac { d _ { k / v } } { H } } \\end{array}$ $h = 1 , \\cdots , H$ $\\mathbf { S } _ { t } ^ { h }$ diagonal matrix $\\mathrm { d i a g } ( \\alpha _ { t } ^ { h } )$ denotes the decay of past state. $\\mathbf { k } _ { t } ^ { h }$ represents the acceptance for the input token $\\mathbf { v } _ { t } ^ { h }$ . The hidden states are 2D matrix once $d _ { k } ^ { \\prime } \\neq 1$ . In Eq. (14), to turn back to 1D shape, the $\\mathbf { q } _ { t } ^ { h }$ operation is necessary as a dot-product with $\\mathbf { S } _ { t } ^ { h }$ , then the concat and normalization operations are followed. XNorm denotes any kinds of normalization. In Eq. (15), a gate machanism is optional for $\\mathbf { o } _ { t }$ while the dimension should be projected back to $d$ from $d _ { v }$ through $W _ { O } \\in \\mathcal { R } ^ { d _ { v } \\times d }$ .\n\nFrom a functional view, Eq. (13) represents the update process of the hidden state, which contains historical information on keys and values. Eq. (14) represents query and weighted sum operations to derive the attention output. Eq. (15) represents gate and projection operations to get the final output.\n\n# General Parallel Form of LinFormer/SSM/LinRNN can be written as follow:\n\n$$\n\\begin{array} { r l } & { \\mathbf { O } = \\displaystyle \\mathrm { L i n A t t M a p } \\left( \\mathbf { Q } , \\mathbf { K } \\right) \\mathbf { V } = \\left( \\left( \\left( \\mathbf { Q } \\odot \\mathbf { A } \\right) \\big ( \\frac { \\mathbf { K } } { \\mathbf { A } } \\big ) ^ { \\top } \\right) \\odot \\mathbf { M } \\right) \\mathbf { V } , } \\\\ & { ( \\mathbf { Q } / \\mathbf { K } / \\mathbf { V } ) _ { t , : } = ( \\mathbf { q } / \\mathbf { k } / \\mathbf { v } ) _ { t } , \\quad \\mathbf { A } _ { t , : } = \\displaystyle \\prod _ { j = 1 } ^ { t } \\alpha _ { j } , \\quad \\mathbf { M } _ { i , j } = \\left\\{ 1 , i \\leq j . \\right. } \\end{array}\n$$\n\n$\\frac { \\bf K } { \\bf A }$ denotes element-wise division, $( \\mathbf { Q } ) _ { t , }$ : is the $t$ -th row of $\\mathbf { Q }$ , and LinAttMap $( \\mathbf { Q } , \\mathbf { K } )$ is the attention map. Each element in the attention map matrix is as follows (heads are omitted for simplicity):\n\n$$\n\\begin{array} { r } { \\mathrm { L i n A t t M a p } \\left( \\mathbf { Q } , \\mathbf { K } \\right) _ { t , s } = \\left\\{ \\mathbf { q } _ { t } \\cdot \\left( \\left( \\prod _ { j = s + 1 } ^ { t } \\alpha _ { j } \\right) \\odot \\mathbf { k } _ { s } \\right) ^ { \\top } , s \\leq t . \\right. } \\\\ { 0 , s > t . } \\end{array}\n$$\n\nAs shown in Tab. 1, the main differences between various linear models are parameter functions $\\mathrm { f } _ { q / k / v / \\alpha / g }$ and dimension settings $d _ { k } , d _ { v } , H$ . We give details in appendix A1 on how to derive LinFormers/SSMs/linRNNs from our unified form, which is termed as “Linear Attention (LinAtt)\".\n\nLinFormer/SSM/LinRNN models have different origins, so they have different optimization perspectives and various hidden state sizes: i) LinFormer originate from approximation of vanilla softmax attention. They focus on designing better kernel function $\\phi$ , i.e., to optimize $\\mathrm { f } _ { q } , \\mathrm { f } _ { k }$ ; They have relatively large matrix hidden state whose size $( d _ { v } d _ { k } / H )$ is mainly correlated to model dimension $d$ . ii) SSM originate from state space equations. So they focus on how to better maintain the hidden state and optimize $\\mathrm { f } _ { \\alpha }$ ; They have a matrix hidden state of moderate size $( d _ { v } N )$ , which is correlated to the fixed expansion $N$ . iii) LinRNN originate from removing nonlinearity in the recurrence of vanilla RNN. So they focus on designing better forget/input/output gates, i.e., to optimize $\\mathrm { f } _ { \\alpha } , \\mathrm { f } _ { k } , \\mathrm { f } _ { g }$ ; They have 1D vector hidden state whose size $( d _ { v } )$ is relatively small. Despite these differences, they all try to design better parameter functions $\\mathrm { f } _ { q / k / v / \\alpha / g }$ and maintain a limited hidden state $\\mathbf { S } _ { t }$ .\n\nTable 1: From our general form to existing linear models ( $*$ indicates the bias term is omitted).   \n\n<html><body><table><tr><td rowspan=\"2\">Models</td><td colspan=\"2\">LinFormer</td><td colspan=\"2\">LinRNN</td><td colspan=\"2\">SSM</td></tr><tr><td>GLA[15]</td><td>TrNorm[12]</td><td>GLRU[20]</td><td>RWKV-4[17]</td><td>Mamba[16]</td><td>S5[22]</td></tr><tr><td>fq(xt，0q） f（xt,0） f(xt,0u） f(xt,0a）</td><td>xtWQ xtWK xtWv (xtWW²）*</td><td>(xtWQ) (xtWK) xtWv Xexp(j0)</td><td>1 σ(xtWi)* (xtWc)* σ(xtW f)*</td><td>1 exp (xtW κ) xtWv exp (-W)</td><td>xtWc △t(xtWb) Xt exp(△tA)</td><td>1 1 xtB exp(△A)</td></tr><tr><td>Dimension</td><td>dk=d/2 d=d</td><td>dk =du =d</td><td>dk=d=d=H</td><td></td><td>d=d=H d=Nd=H d=N</td><td>d=1</td></tr></table></body></html>\n\n# 4 Optimal Linear Approximation to the Softmax Attention Map\n\nWe here discuss the optimal approximation of LinAttMap to SoftAttMap based on its general form. The function of softmax attention is two-fold: i) Memorizing information, all the current and historical information can be stored in KV cache; ii) Modeling relationships, softmax attention can calculate arbitrary attention scores of stored information. Unfortunately, such a powerfully expressive attention map generated by softmax attention requires infinite hidden states. By contrast, linear attention expects to exploit limited hidden states to achieve the same functionality as softmax attention.\n\nSome existing linear models, such as Performer[30], RFA[31], etc., optimize the model with the goal of approximating the value of SoftAttMap. In contrast, this work investigates the functional approximation of SoftAttMap, which is the basis for value approximation. Specifically, we here attempt to answer two key questions: i) Can linear attention realize the function of softmax attention? ii) If it can be achieved, what kind of linear attention approximation is better? To achieve this goal, we first give the definition of necessary conditions of optimal linear approximation. Then we categorize the existing linear models based on the conditions of the optimal linear approximation.\n\nDefinition 4.1. Necessary Conditions of Optimal Linear Approximation to Softmax Attention Map. A function $f ( \\mathbf { x } _ { t } , \\mathbf { x } _ { s } \\vert \\theta ) : \\mathcal { R } ^ { 1 \\times d } \\times \\mathcal { R } ^ { 1 \\times \\bar { d } }  \\mathcal { R }$ , used to compute attention score between any $\\mathbf { x } _ { t }$ and $\\mathbf { x } _ { s }$ (tokens), with parameters $\\theta$ , is an optimal linear approximation to softmax attention map if it satisfies: i) Linear complexity. Attention map can be computed in linear time, i.e., $\\mathcal { O } ( n )$ space and time complexity during training and $\\mathcal { O } ( 1 )$ space and time complexity during inference. ii) Dynamic memory ability. When handling inputs sequentially, $f ( \\mathbf { x } _ { t } , \\mathbf { x } _ { s } | \\boldsymbol { \\theta } )$ with limited hidden states should be able to store the most important information adaptively while forgetting unimportant ones. iii) Static approximation ability: For an arbitrarily given softmax attention map $\\mathbf { P }$ with scores $p _ { t s }$ , there must exists bounded $\\theta$ such that $f ( \\mathbf { x } _ { t } , \\mathbf { x } _ { s } | \\boldsymbol { \\theta } ) = p _ { t s } , \\forall t , s = 1 , \\cdot \\cdot \\cdot , n$ . iv) Least parameter approximation: On the premise that the first three conditions are met, use as few parameters as possible to achieve approximation to softmax attention map.\n\nIn definition 4.1, Condition 0 (C0) underlines computational and memory efficiency. Conditions 1 (C1) and 2 (C2) consider memory and modeling ability of linear attention. Due to limited state size $d$ , linear attention can only memorize the history of most important $d$ tokens without information loss and precisely model arbitrary attention map of those $d$ tokens. Condition 3 (C3) is our expectation to seek the least parameters on the premise that previous three conditions are met.\n\nTheoretical Analysis for Optimal Linear Approximation. For the C1 condition, suppose the information about $\\mathbf { v } _ { t _ { 1 } } , \\ldots , \\mathbf { v } _ { t _ { d _ { k } } }$ is successfully stored in $\\mathbf { S } _ { t }$ $\\mathfrak { s } _ { t } ( t _ { 1 } , \\dots , t _ { d _ { k } } \\le t )$ , we will check whether the model can substitute unimportant $\\mathbf { v } _ { t _ { 1 } }$ when the new important input $\\mathbf { v } _ { t + 1 }$ arrives.\n\nFor the C2 condition, Eq. (18) illustrates the LinAttMap only relate to $\\mathbf q _ { t } = \\mathrm { f } _ { q } ( \\mathbf x _ { t } , \\theta _ { q } ) , \\mathbf k _ { t } =$ $\\mathrm { f } _ { k } ( \\mathbf { x } _ { t } , \\theta _ { k } )$ , $\\alpha _ { t } = \\mathrm { f } _ { \\alpha } (  { \\mathbf { x } } _ { t } , \\theta _ { \\alpha } )$ . Denote decay $\\mathbf { \\boldsymbol { \\Lambda } } _ { t } = \\operatorname { d i a g } ( \\alpha _ { t } )$ . Assuming the inputs are good enough and the functions $\\left( \\mathrm { f } _ { q } , \\mathrm { f } _ { k } , \\mathrm { f } _ { \\alpha } \\right)$ are expressive enough, we can shift from solving $( \\theta _ { q } , \\theta _ { k } , \\theta _ { \\alpha } )$ to solving $( \\mathbf { Q } , \\mathbf { K } , \\pmb { \\Lambda } _ { t } )$ . We focus on approximating the attention scores between stored tokens, and the problem is simplified via: i) setting query dimension $d _ { k } = 1$ ; ii) considering only a given time $t$ and its attention distribution ${ \\bf p } _ { t } = [ p _ { t s } , s = 1 , \\ldots , t ] \\in \\mathcal { R } ^ { 1 \\times t }$ . Then, C2 is proved by the following equations holding with bounded parameters, as a foundation conclusion:\n\nTable 2: A review of existing linear models. According to definition 4.1, existing linear models all have some deficiencies: i) Models without dynamic decay $\\mathbf { \\boldsymbol { \\Lambda } } _ { t }$ have no ability to memorize dynamically (not satisfying C1); ii) LinRNNs lack the selection ability brought by $\\mathbf { Q }$ (not satisfying C2), and the approximation ability is poor owing to the small hidden state; iii) Models with $\\mathbf { K }$ have redundant parameters (not satisfying C3), which probably leads to higher learning difficulty.   \n\n<html><body><table><tr><td colspan=\"3\"></td><td>C0 C1</td><td>C2 C3</td><td>Models</td></tr><tr><td colspan=\"3\">Softmax Attention</td><td>X</td><td>√</td><td>[Pr [2],La [1]</td></tr><tr><td rowspan=\"10\">Linear Attention</td><td rowspan=\"2\">Three Parameter Groups</td><td>Q,K,A</td><td>X √</td><td>× ×</td><td>Rs424</td></tr><tr><td>Q,K,At</td><td>√ √</td><td>√ X</td><td>GLA[15],Mamba [16]</td></tr><tr><td rowspan=\"5\">Two Parameter Groups</td><td>Q,K</td><td>√ ×</td><td>√ ×</td><td>iee</td></tr><tr><td>K,A</td><td>√ ×</td><td>X X</td><td>RWKV-4 [17]</td></tr><tr><td>K,At</td><td>√ √</td><td>X X</td><td>GLRU [20]</td></tr><tr><td>Q,Λ</td><td>√ ×</td><td>× X</td><td></td></tr><tr><td>Q,△t</td><td>√ √</td><td>√ √</td><td>MetaLA(This Work)</td></tr><tr><td rowspan=\"3\">One Parameter Group</td><td>Q or K</td><td>√ ×</td><td>× X</td><td></td></tr><tr><td>A</td><td>√ X</td><td>X X</td><td>LRU[19], S5[22]</td></tr><tr><td>At</td><td>√</td><td>√ X</td><td>HGRN[20,Griffn [18],</td></tr></table></body></html>\n\n$$\nf ( { \\bf x } _ { t } , { \\bf x } _ { s } | \\mathbf { Q } , { \\bf K } , \\Lambda _ { t } ) = q _ { t } \\big ( \\prod _ { j = s + 1 } ^ { t } \\alpha _ { j } \\big ) k _ { s } = p _ { t s } , \\forall s = 1 , \\ldots , t ,\n$$\n\n$$\n\\mathrm { s . t . } \\quad | q _ { s } | \\leq C _ { q } , | k _ { s } | \\leq C _ { k } , \\alpha _ { s } \\in [ 0 , 1 ] , \\forall s = 1 , \\ldots , t .\n$$\n\nFor bounded inputs $\\mathbf { X }$ , bounded parameters $( \\theta _ { q } , \\theta _ { k } , \\theta _ { \\alpha } )$ are equivalent to bounded $( \\mathbf { Q } , \\mathbf { K } , \\pmb { \\Lambda } _ { t } )$ . Afterwards we will generalize to vector version with $d _ { k } > 1$ and consider distribution of all time $( { \\bf p } _ { t } , t = 1 , \\ldots , d _ { k } )$ . This is done by viewing $\\mathbf { q } _ { t }$ as a channel selector.\n\nFor the C3 condition, least parameters mean the fewest parameter groups $( \\mathbf { Q } , \\mathbf { K } , \\pmb { \\Lambda } _ { t } )$ when $d , d _ { k } , d _ { v }$ are fixed. Due to space constraints, the detailed analysis in this Section is provided in appendix A2.\n\nConclusions of Optimal Linear Approximation Analysis. i) Linear approximation. The necessary conditions (C1 and C2) for LinAttMap to achieve approximation to SoftAttMap is that its implementation must include $\\mathbf { Q }$ and dynamic decay $\\mathbf { \\boldsymbol { \\Lambda } } _ { t }$ . Both $( \\mathbf { Q } , \\mathbf { K } , \\pmb { \\Lambda } _ { t } )$ and $( \\mathbf { Q } , \\pmb { \\Lambda } _ { t } )$ can achieve approximation. ii) Least parameter approximation. $( \\mathbf { Q } , \\pmb { \\Lambda } _ { t } )$ has fewer parameters (i.e., $\\mathbf { K }$ is not necessary), if the model dimensions are fixed. iii) Function of dynamic decay. $\\mathbf { \\boldsymbol { \\Lambda } } _ { t }$ is the key to achieve dynamic memory. iv) Function of Query. $\\mathbf { Q }$ can be seen as a channel selector which selects several channels of Hadamard product of $\\mathbf { \\boldsymbol { \\Lambda } } _ { t }$ and $\\mathbf { K }$ to approximate attention map.\n\nIn Tab. 2, we review some existing linear models and judge whether they meet the necessary conditions for optimal approximation. Linear attentions can be classified into three types based on the parameter groups: i) Using $( \\mathbf { Q } , \\mathbf { K } , \\pmb { \\Lambda } _ { t } )$ all together, ii) Exploiting $( \\mathbf { Q } , \\mathbf { K } )$ , $( \\mathbf { Q } , \\pmb { \\Lambda } _ { t } )$ or $\\left( \\mathbf { K } , \\pmb { \\Lambda } _ { t } \\right)$ , iii) Employing only one of $\\mathbf { Q }$ , $\\mathbf { K }$ , $\\mathbf { \\boldsymbol { \\Lambda } } _ { t }$ . Considering decay can be either dynamic or fixed, here we use subscript $t$ to distinguish, i.e., $\\pmb { \\Lambda } / \\pmb { \\Lambda } _ { t }$ denote fixed/dynamic decay. According to definition 4.1, they have different degrees of deficiencies: i) Models without dynamic decay such as RetNet[14], TransNormer[12], RFA[31], cannot memorize dynamically; ii) LinRNNs such as RWKV-4[17], HGRN[20], Griffin[18] lack the selection ability brought by $\\mathbf { Q }$ and the approximation ability is poor due to the small hidden state; iii) Models with $\\mathbf { K }$ such as Mamba[16], GLA[15] have redundant parameters, which probably leads to higher learning difficulty. Thus, none of the existing linear models meet all C1/C2/C3 conditions. These analyses also inspire us that ignoring the functional approximation of softmax attention does not enable the approximation of softmax attention values.\n\nTable 3: From general recurrent linear form to our MetaLA.   \n\n<html><body><table><tr><td>Models fq(xt,0q) fk(Xt,0k) fu(xt,0u) fa(xt,0a) fg(xt,0g) Dimension</td></tr><tr><td>MetaLA xtWq 1-αt xtWv σ(xtWα)</td></tr><tr><td>(xtWg+bg)dk=d/2,d=d</td></tr></table></body></html>\n\n# 5 MetaLA Transformer\n\nTransformer is stacked by a series of Encoder/Decoder blocks. Generally, each block is composed of two modules in sequence: token mixer and channel mixer [34, 35]. Softmax attention plays the role of the token mixer. In this work, we follow the Transformer architecture as a whole but use our proposed MetaLA module as the token mixer. Due to space constraints, the architecture of MetaLA Transformer is given in detail in appendix A3. Here we focus on describing the three enhancements of MetaLA relative to the general linear attention in Sec. 3 (see Fig. 2): i) The Key matrix is not used, which is based on our theoretical analysis. ii) Self-augmentation and iii) Short convolution are two other optional techniques to further enhance the modeling ability of our model.\n\ni) The Key matrix is not used. We exploit $\\mathbf { 1 } - \\alpha _ { t }$ to replace $\\mathbf { k } _ { t }$ , based on theoretical analysis in Sec. 4 and appendix A2, i.e., dynamic decay $\\mathbf { \\boldsymbol { \\Lambda } } _ { t }$ is the key mechanism to achieve dynamic memory and static approximation, and $\\mathbf { K }$ is not required. As shown in Tab. 3, compared with Eq. (13), the main improvement is:\n\n$$\n\\begin{array} { r l } { \\mathbf { S } _ { t } ^ { h } = \\mathrm { d i a g } ( \\alpha _ { t } ^ { h } ) \\mathbf { S } _ { t - 1 } ^ { h } + ( \\mathbf { 1 } - \\alpha _ { t } ^ { h } ) ^ { \\mathsf { T } } \\mathbf { v } _ { t } } & { { } \\in \\mathbb { R } ^ { d _ { k } ^ { \\prime } \\times d _ { v } ^ { \\prime } } , } \\end{array}\n$$\n\nwhich can be trained in a parallel form in Eq. (16). The only difference is that $\\mathbf { K }$ is replaced by $\\mathbf { B }$ and $\\mathbf { B } _ { t , : } = \\mathbf { 1 } - \\alpha _ { t }$ . With usage of $\\mathbf { \\boldsymbol { \\Lambda } } _ { t }$ and Q, MetaLA can achieve linear approximation to SoftAttMap. Without usage of $\\textbf { K } ( \\pmb { W } _ { K } )$ , we can allocate more parameters and utilize full-rank matrix $W _ { \\alpha }$ to produce dynamic decay rather than low-rank matrix used by GLA, such that we do not sacrifice expression capacity of $\\mathrm { f } _ { \\alpha }$ and approximation performance of MetaLA.\n\n![](images/0e570f21854435edbb37f20ee33878ec96d6b80c66354664dbd717050215f9ef.jpg)  \nFigure 2: Recurrent form of MetaLA. We mark all three enhancements in red.   \nFigure 3: Accuracy $( \\% )$ on the synthetic MQAR task.\n\ni) Self-augmentation can enhance a token’s attention to itself, avoiding attention dilution [12]\n\n$$\n\\begin{array} { r l } { \\mathbf { o } _ { t } ^ { h } = \\mathbf { q } _ { t } ^ { h } \\mathbf { S } _ { t } ^ { h } + \\sigma _ { \\mathrm { a u g } } \\left( \\mathbf { q } _ { t } ^ { h } ( \\boldsymbol { w } _ { \\mathrm { a u g } } ^ { h } \\odot ( \\mathbf { 1 } - \\alpha _ { t } ^ { h } ) ) ^ { \\mathsf { T } } \\mathbf { v } _ { t } \\right) } & { { } \\in \\mathcal { R } ^ { 1 \\times d _ { v } ^ { \\prime } } . } \\end{array}\n$$\n\nWithout changing the hidden state $\\mathbf { S } _ { t } ^ { h }$ in Eq. (21), the proposed self-augmentation (the second term on the right side of the equation) is only added on the output process, with a learnable parameter $w _ { \\mathrm { a u g } } \\in \\breve { \\mathcal { R } } ^ { 1 \\times d _ { k } }$ . The proposed design has two advantages (more analysis in appendix A3.2): First, it maintains parallel computing; Second, it augments the information of current token itself and does not affect future output through inner state.\n\niii) Short convolution. An additional short convolution can be inserted before entering the MetaLA layer to enhance local interaction further, motivated by Mamba [16] and Griffin [18].\n\nSeq_len:256 Seq_len:512 1 Based GLA RWKV Mamba MetaLA 64 128 256 51264 128 256 512 Model dimension Model dimension\n\n# 6 Experiments\n\nWe conduct a comprehensive evaluation of MetaLA to validate its\n\nTable 4: Performance Comparison on SuperGLUE. PS: parameter size (billion). T: tokens (billion). † means the results reported by [20]. For baselines that need to be compared, if they do not have public checkpoints, we train and test them under identical conditions with MetaLA. MetaLA : MetaLA with tied embedding trained using 100B tokens. MetaLA $b$ : MetaLA trained with 300B tokens.   \n\n<html><body><table><tr><td>Models</td><td>PS</td><td>T</td><td>WSC</td><td>WIC</td><td>RTE</td><td>CB</td><td>MULTIRC</td><td>BOOLQ</td><td>COPA</td><td>AVG</td></tr><tr><td>Pythia</td><td>0.41</td><td>15</td><td>36.54</td><td>50.00</td><td>52.35</td><td>39.29</td><td>0.31</td><td>61.99</td><td>62.00</td><td>43.21</td></tr><tr><td>Mamba</td><td>0.37</td><td>15</td><td>36.54</td><td>50.31</td><td>52.71</td><td>42.86</td><td>2.52</td><td>58.78</td><td>64.00</td><td>43.96</td></tr><tr><td>GLA</td><td>0.36</td><td>15</td><td>36.54</td><td>49.84</td><td>53.07</td><td>41.07</td><td>0.42</td><td>53.49</td><td>66.00</td><td>42.92</td></tr><tr><td>MetaLA</td><td>0.36</td><td>15</td><td>36.54</td><td>50.00</td><td>52.71</td><td>42.86</td><td>0.31</td><td>58.96</td><td>67.00</td><td>44.05</td></tr><tr><td>Pythia†</td><td>1.4</td><td>300</td><td>36.54</td><td>50.00</td><td>53.07</td><td>35.71</td><td>0.94</td><td>60.73</td><td>72.00</td><td>44.14</td></tr><tr><td>HGRN†</td><td>1</td><td>100</td><td>40.38</td><td>50.78</td><td>53.43</td><td>42.86</td><td>3.04</td><td>58.69</td><td>70.00</td><td>45.60</td></tr><tr><td>Mamba</td><td>1.4</td><td>100</td><td>39.42</td><td>50.94</td><td>55.23</td><td>26.79</td><td>1.15</td><td>53.27</td><td>73.00</td><td>42.83</td></tr><tr><td>RetNet</td><td>1.3</td><td>100</td><td>36.54</td><td>50.00</td><td>52.71</td><td>46.43</td><td>2.52</td><td>60.21</td><td>68.00</td><td>45.20</td></tr><tr><td>GLA‡</td><td>1.3</td><td>100</td><td>36.54</td><td>50.16</td><td>53.07</td><td>37.50</td><td>0.31</td><td>61.04</td><td>69.00</td><td>43.95</td></tr><tr><td>MetaLAa</td><td>1.3</td><td>100</td><td>49.04</td><td>51.25</td><td>55.60</td><td>37.50</td><td>1.78</td><td>55.50</td><td>70.00</td><td>45.81</td></tr><tr><td>MetaLAb</td><td>1.4</td><td>300</td><td>62.50</td><td>51.88</td><td>49.10</td><td>48.21</td><td>1.57</td><td>56.27</td><td>75.00</td><td>49.22</td></tr></table></body></html>\n\nTable 5: Performance Comparison on Commonsense Reasoning. ‡ indicates testing using opensource checkpoints. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA.   \n\n<html><body><table><tr><td>Models</td><td>PS</td><td>T</td><td>LOGIQA</td><td>WSC273</td><td>BOOLQ</td><td>PIQA</td><td>HS</td><td>WG</td><td>ARC-c</td><td>OBQA</td><td>AVG</td></tr><tr><td>Pythia</td><td>0.41</td><td>15</td><td>21.81</td><td>57.51</td><td>61.99</td><td>63.66</td><td>33.15</td><td>51.78</td><td>22.78</td><td>28.60</td><td>42.66</td></tr><tr><td>Mamba</td><td>0.37</td><td>15</td><td>20.43</td><td>56.78</td><td>58.78</td><td>64.80</td><td>33.98</td><td>49.80</td><td>22.87</td><td>29.20</td><td>42.08</td></tr><tr><td>GLA</td><td>0.36</td><td>15</td><td>23.04</td><td>56.78</td><td>53.49</td><td>63.55</td><td>32.00</td><td>52.10</td><td>22.78</td><td>27.40</td><td>41.39</td></tr><tr><td>MetaLA</td><td>0.36</td><td>15</td><td>22.43</td><td>58.24</td><td>58.96</td><td>63.82</td><td>32.18</td><td>53.12</td><td>23.38</td><td>28.00</td><td>42.52</td></tr><tr><td>Pythia‡</td><td>1.4</td><td>300</td><td>21.35</td><td>72.89</td><td>63.12</td><td>70.89</td><td>51.98</td><td>56.99</td><td>28.41</td><td>33.20</td><td>49.85</td></tr><tr><td>HGRN‡</td><td>1</td><td>100</td><td>22.43</td><td>58.97</td><td>58.75</td><td>71.00</td><td>48.05</td><td>51.14</td><td>28.07</td><td>31.80</td><td>46.28</td></tr><tr><td>Mamba</td><td>1.4</td><td>100</td><td>22.73</td><td>68.50</td><td>53.27</td><td>71.44</td><td>48.63</td><td>53.59</td><td>29.01</td><td>31.80</td><td>47.37</td></tr><tr><td>RetNet‡</td><td>1.3</td><td>100</td><td>22.73</td><td>63.74</td><td>60.21</td><td>69.53</td><td>48.39</td><td>53.28</td><td>26.19</td><td>30.80</td><td>46.86</td></tr><tr><td>GLA‡</td><td>1.3</td><td>100</td><td>21.81</td><td>63.00</td><td>61.04</td><td>70.08</td><td>48.00</td><td>51.93</td><td>28.33</td><td>31.40</td><td>46.95</td></tr><tr><td>MetaLAa</td><td>1.3</td><td>100</td><td>21.81</td><td>65.93</td><td>55.50</td><td>70.02</td><td>47.32</td><td>55.01</td><td>27.47</td><td>33.00</td><td>47.01</td></tr><tr><td>MetaLAb</td><td>1.4</td><td>300</td><td>21.35</td><td>73.63</td><td>56.27</td><td>72.25</td><td>53.58</td><td>58.17</td><td>30.03</td><td>34.60</td><td>49.99</td></tr></table></body></html>\n\ncapabilities as a foundation model. i) MQAR [36]. Performance on the Multi-Query Associative Recall (MQAR) task is closely linked to language modeling and can also imply the effectiveness of our theory in modeling hidden states and retrieving information. ii) Autoregressive language modeling on the Pile [37] dataset and evaluation on Common-Sense Reasoning and SuperGLUE [38] zero-shot benchmarks are conducted. iii) LRA [39]. We execute experiments on the Long Range Arena (LRA) benchmark [39] to investigate MetaLA’s ability in long sequence modeling. iv) ImageNet [40]. Generalization ability in visual tasks. Due to space constraints, we put some additional experiments in appendix A5, including: v) Scalability. We extend MetaLA to a 3B parameter scale and a 300B data scale for preliminary validation. vi) Retrieval and long context abilities. We evaluated MetaLA’s retrieval performance on the MAD tasks [41], and its effectiveness in handling long contexts on the Needle in a Haystack task [42]. vii) Training efficiency. We provide comparative results on training throughput and GPU memory usage across various models. Detailed experimental setup and further discussion are given in appendix A4.\n\nAssociative Recall. The synthetic MQAR task [36] is exploited to evaluate MetaLA’s memory ability. In the task, given multiple queries, the model must recall the corresponding key-value pairs before. We follow default settings in [36] to generate datasets. Fig. 3 shows that MetaLA outperforms other linear models, which have three parameter groups (Mamba [16], GLA [15], Based [43]) or fixed decay (RWKV-4 [17]), well supporting our theoretical analysis and module design. The attention baseline achieves optimal results $( > 9 9 . 0 )$ under both conditions. The additional experiments in appendix A5 show that MetaLA outperforms Mamba on more challenging settings.\n\nLanguage Modeling. We train two scales of MetaLA: 360M/1.4B on the Pile dataset. For baselines of 360M MetaLA, we train them from scratch aligned with our configurations. For the 1.3B MetaLA, we compare it with publicly available models [14, 15, 16, 20, 44]. We implement all the pre-train experiments with GPT-Neox [45]. The zero-shot evaluation results on SuperGLUE and Commensense Reasoning benchmarks are reported in Tab. 4 and Tab. 5. Specifically, compared to the LinRNN model HGRN [20], MetaLA expands hidden state dimensions and uses Query matrix; Compared to LinFormer model RetNet [14] with fixed decay, MetaLA uses dynamic decay; Compared to SSMs like Mamba [16] and LinFormer with dynamic decay GLA [15], MetaLA omits the Key matrix in computation. Results indicate that MetaLA has better performance than these linear models and the Transformer-based Pythia [44]. See appendix A5 for more task results.\n\nTable 7: Performances Comparison on the Long Range Arena. We cite baselines from HGRN [20].   \n\n<html><body><table><tr><td>Method</td><td>LitsOps</td><td>Text</td><td>Retrieval</td><td>Image</td><td>Pathfinder</td><td>Path-X</td><td>AVG.</td></tr><tr><td>Transformer[1]</td><td>38.37</td><td>61.95</td><td>80.69</td><td>40.57</td><td>65.26</td><td></td><td>47.81</td></tr><tr><td>S4 [10]</td><td>59.60</td><td>86.82</td><td>90.90</td><td>88.65</td><td>94.20</td><td>96.35</td><td>86.09</td></tr><tr><td>DSS-softmax [26]</td><td>60.60</td><td>84.80</td><td>87.80</td><td>85.70</td><td>84.60</td><td>87.80</td><td>81.88</td></tr><tr><td>TNN [46]</td><td>61.04</td><td>87.90</td><td>90.97</td><td>88.24</td><td>93.00</td><td>96.10</td><td>86.21</td></tr><tr><td>S5 [22]</td><td>62.15</td><td>89.31</td><td>91.40</td><td>88.00</td><td>95.33</td><td>98.56</td><td>87.46</td></tr><tr><td>Mega [47]</td><td>63.14</td><td>90.43</td><td>91.25</td><td>90.44</td><td>96.01</td><td>97.98</td><td>88.21</td></tr><tr><td>SGConv [48]</td><td>61.45</td><td>89.20</td><td>91.11</td><td>87.97</td><td>95.46</td><td>97.83</td><td>87.17</td></tr><tr><td>LRU[19]</td><td>60.20</td><td>89.40</td><td>89.90</td><td>89.00</td><td>95.10</td><td>94.20</td><td>86.30</td></tr><tr><td>HGRN[20]</td><td>59.95</td><td>88.14</td><td>94.23</td><td>88.69</td><td>92.92</td><td>97.50</td><td>86.91</td></tr><tr><td>Mamba[16]</td><td>38.02</td><td>82.98</td><td>72.14</td><td>69.82</td><td>69.26</td><td>67.32</td><td>66.59</td></tr><tr><td>MetaLA(ours)</td><td>59.34</td><td>89.27</td><td>91.28</td><td>91.88</td><td>91.66</td><td>96.57</td><td>86.67</td></tr></table></body></html>\n\nTable 8: Ablation studies. Ablation study results on the 360M model trained for 15B tokens. We compared the model variants on zero-shot experiments of the Comparison on Commonsense Reasoning benchmark. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA.   \n\n<html><body><table><tr><td>Models</td><td>LOGIQA</td><td>WSC273</td><td>BOOLQ</td><td>PIQA</td><td>HS</td><td>WG</td><td>ARC-c</td><td>OBQA</td><td>AVG</td></tr><tr><td>MetaLA</td><td>22.43</td><td>58.24</td><td>58.96</td><td>63.82</td><td>32.18</td><td>53.12</td><td>23.38</td><td>28.00</td><td>42.52</td></tr><tr><td>MetaLA w/o selfaug</td><td>21.81</td><td>58.61</td><td>57.52</td><td>64.47</td><td>32.56</td><td>49.41</td><td>23.89</td><td>29.00</td><td>42.16</td></tr><tr><td>MetaLA w/o conv</td><td>22.58</td><td>51.65</td><td>49.36</td><td>52.07</td><td>25.82</td><td>51.22</td><td>26.54</td><td>28.80</td><td>38.51</td></tr><tr><td>MetaLA w/ key</td><td>21.20</td><td>57.88</td><td>49.11</td><td>63.00</td><td>32.99</td><td>50.99</td><td>23.63</td><td>27.60</td><td>40.80</td></tr></table></body></html>\n\nLong Sequence Modeling. LRA is used to evaluate the model’s ability in long sequence modeling. We compare MetaLA with Transformer, linear foundation models, and models specifically designed for long sequence modeling. Tab. 7 shows that MetaLA achieves comparable results with SOTA linear models, demonstrating that our model effectively preserves the ability to model long sequences.\n\nTable 6: Results on ImageNet-1k.   \n\n<html><body><table><tr><td>Model</td><td>Acc PS (M) Acc PS (M)</td></tr><tr><td>Deit</td><td>72.20 5.7 79.90 22.0</td></tr><tr><td>HGRN</td><td>74.40 6.1 80.09 23.7</td></tr><tr><td>GLA</td><td>72.47 6.1 79.23 23.5</td></tr><tr><td>Mamba</td><td>73.39 6.1 79.60 23.7</td></tr><tr><td>MetaLA 75.33</td><td>6.1 80.14 23.7</td></tr></table></body></html>\n\nImage Classification. We compare MetaLA with Deit [49] (Transformer), HGRN [20] (LinRNN),\n\nGLA [15] (LinFormer) and Mamba [16] (SSM) on ImageNet. As shown in Tab. 6, MetaLA performs better than other typical linear models at both scales of 6M and 23M.\n\nAblation Studies. We conduct ablation studies on the 360M model trained with 15B tokens and compare the results in zero-shot experiments. First, restoring the Key matrix in linear attention does not improve performance while increasing parameters, supporting our theoretical result that $\\mathbf { K }$ is not necessary for approximation, and its functional role can be replaced by dynamic decay. Second, the ablations of self-augmentation and short convolution demonstrate the effectiveness of our model design, i.e., enhance tokens’ own attention and local interactions.\n\n# 7 Conclusion and Discussion\n\nConclusion. We unify LinFormer/SSM/LinRNN models into the form of linear attention with Query, Key, Vaule matrices, and then analyze whether they can achieve the optimal approximation to the softmax attention function. Theoretical analysis shows that the existing LinFormer/SSM/LinRNN cannot achieve optimal approximation. Consequently, we propose the MetaLA architecture, which can achieve functional approximation of softmax attention with least parameters. The performance on various types of tasks verifies the effectiveness of MetaLA.\n\nDiscussion and Limitation. Here, we discuss two key questions about approximation perspectives. i) How does an optimal approximation to softmax attention inspire linear attention design? In this work, we mainly remove the Key matrix, use dynamic decay, and enhance local interactions and the token’s own attention. This is clearly not the end of linear attention optimization. This work focuses on functional approximation, previous studies about the value approximation [30, 31, 50] can be further investigated on the basis of our functional approximation theory as well as MetaLA architecture. Additional optimization may include improving the recall ability of limited hidden states or designing better parameter functions. ii) Does approximation to the softmax attention imply that linear attention has an upper capacity limit? Taken literally, approximation seems to imply that linear attention cannot exceed softmax attention. However, we found better results for linear attention than softmax attention in some experimental results, such as zero-shot and LRA. Similar findings were also reported in previous work [13, 15, 16]. We argue that this issue deserves further exploration. For the time being, evaluation metrics that do not adequately reflect the model’s capabilities, insufficient training [51], and linear attention that does have advantages in certain abilities [15] are all possibilities.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文旨在解决现有线性复杂度模型（如LinFormer、SSM、LinRNN）在替代传统softmax注意力时设计上的不足，这些模型在动态记忆能力、静态近似能力和最少参数近似方面存在缺陷，导致性能次优。\\n> *   该问题的重要性在于，高效的线性复杂度模型可以显著降低Transformer结构的计算成本，适用于长序列建模、语言模型等场景。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出Meta Linear Attention (MetaLA)，通过统一现有线性模型的形式，并满足动态记忆、静态近似和最少参数近似三个条件，实现了对softmax注意力的最优线性近似。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 统一了LinFormer/SSM/LinRNN模型的形式，并提出了最优线性近似的三个必要条件（动态记忆能力、静态近似能力、最少参数近似）。\\n> *   **贡献2：** 提出了MetaLA模块，通过移除不必要的Key矩阵、增强自注意力机制和引入短卷积，满足了最优近似的条件。\\n> *   **贡献3：** 在MQAR任务、语言建模、图像分类和LRA基准测试中，MetaLA的性能显著优于现有线性模型（如Mamba、GLA、RWKV-4）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   MetaLA的核心思想是通过动态衰减（dynamic decay）和Query矩阵实现对softmax注意力的功能近似，同时避免使用冗余的Key矩阵，从而减少参数并提升性能。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有线性模型（如LinFormer、SSM、LinRNN）在动态记忆能力或静态近似能力上存在不足，且部分模型使用了不必要的Key矩阵。\\n> *   **本文的改进：** MetaLA通过动态衰减和自增强机制（self-augmentation）增强了模型的记忆和近似能力，同时移除了Key矩阵以减少参数。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   **步骤1：** 将输入映射为Query、Value和动态衰减参数。\\n> *   **步骤2：** 通过动态衰减更新隐藏状态，存储历史信息。\\n> *   **步骤3：** 使用Query矩阵和隐藏状态计算注意力输出。\\n> *   **步骤4：** 引入自增强机制和短卷积，进一步增强局部交互和自注意力。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   LinFormer类：GLA、TransNormer\\n> *   SSM类：Mamba、S5\\n> *   LinRNN类：RWKV-4、HGRN\\n> *   传统Transformer\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在MQAR任务准确率上：** MetaLA达到98.5%，显著优于Mamba (83.2%)和GLA (85.7%)，提升约15个百分点，接近softmax attention(>99%)的极限性能。\\n> *   **在LRA基准平均准确率上：** MetaLA达到86.67%，超过Mamba(66.59%)，与SOTA模型S5(87.46%)相当，其中Path-X任务达到96.57%。\\n> *   **在ImageNet-1k分类上：** 23M参数规模的MetaLA达到80.14% top-1准确率，优于同等规模的Mamba(79.60%)和GLA(79.23%)。\\n> *   **在训练效率上：** 相比Transformer，MetaLA在序列长度512时训练速度提升3.2倍，GPU内存占用减少62%。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   线性注意力 (Linear Attention, N/A)\\n*   动态衰减 (Dynamic Decay, N/A)\\n*   元线性注意力 (Meta Linear Attention, MetaLA)\\n*   语言建模 (Language Modeling, N/A)\\n*   长序列建模 (Long Sequence Modeling, N/A)\\n*   图像分类 (Image Classification, N/A)\\n*   最优近似 (Optimal Approximation, N/A)\"\n}\n```"
}