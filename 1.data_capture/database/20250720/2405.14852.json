{
    "link": "https://arxiv.org/abs/2405.14852",
    "pdf_link": "https://arxiv.org/pdf/2405.14852",
    "title": "PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression",
    "authors": [
        "Vladimir Malinovskii",
        "Denis Mazur",
        "Ivan Ilin",
        "Denis Kuznedelev",
        "Konstantin Burlachenko",
        "Kai Yi",
        "Dan Alistarh",
        "Peter Richtárik"
    ],
    "institutions": [
        "KAUST"
    ],
    "publication_date": "2024-05-23",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 24,
    "influential_citation_count": 5,
    "paper_content": "# PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression\n\nVladimir Malinovskii† Yandex, HSE University\n\nDenis Mazur† Ivan Ilin† MIPT⋄, SberDevices¶ AI Initiative, KAUST∗\n\nDenis Kuznedelev Yandex, Skoltech\n\nKonstantin Burlachenko AI Initiative, KAUST∗\n\nPeter Richtarik‡\n\nKai Yi Dan Alistarh‡ AI Initiative, KAUST∗ IST Austria, NeuralMagic AI Initiative, KAUST∗\n\n# Abstract\n\nThere has been significant interest in “extreme” compression of large language models (LLMs), i.e., to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices. Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training approaches are reaching diminishing returns in terms of the accuracyvs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs. We propose PV-Tuning — a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases. On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family models at 2 bits per parameter.\n\n# 1 Introduction\n\nRecent years have seen the development of ever more capable large language models, attracting immense interest from both researchers and industry. One of the driving factors behind progress in this area is the availability of powerful open LLMs such as Llama [69], Mistral [34, 35], or Phi [41]. The main advantage of open LLMs is that they can be run and fine-tuned locally by end users; however, as state-of-the-art LLMs grow larger, they also become harder to run on commodity hardware. For instance, in order to fit the best available Llama-3 model on a consumer GPU, the model would have to be compressed to below 2 bits per parameter1.\n\nTo achieve such “extreme” degrees of compression accurately, researchers have proposed a variety of techniques, which can be roughly categorized into i) better quantized weight representations and ii) better algorithms to learn these representations. The weight representations used for extreme quantization include group quantization [22, 20], sparse high-precision outliers [17, 32], incoherence processing of the weights [9, 70], or additive and residual quantization [21, 72]. In turn, the calibration algorithms also vary between data-free methods [20], layer-wise calibration [22, 18], block-wise or global fine-tuning [21, 71] or even quantization-aware training [78, 75]. However, the weight representation and the fine-tuning algorithm are largely orthogonal: most popular quantized representations could be obtained layer-wise in one-shot, fine-tuned layer-wise to a variety of optimization objectives, or even trained entirely from scratch.\n\n![](images/ba0a5cf9d6bb113dd405b385cd7a63b18625ada760bddb3664c882ea8de20bae.jpg)  \nFigure 1: WikiText-2 perplexity (left) and average zero-shot accuracy (right) of 2-bit quantized LLAMA 2 models as a function of model size (GiB). See detailed setup in Section 4.3.\n\nSurprisingly, there is a clear disparity between the degree of interest shown to accurate one-shot2 quantization versus accurate fine-tuning. Specifically, one-shot quantization is very well-studied, to the extent that, as shown in Figure 2, improvements in this direction are clearly saturating. At the same time, the impact of fine-tuning strategy is largely unknown: while many recent works use some form of fine-tuning [63, 21, 71], they typically consider a single fine-tuning regimen based on straight-through estimation (STE) [6, 15]. Thus, given the multitude of representations considered, it is not at all clear whether current fine-tuning strategies are optimal.\n\nIn this work, we analyze the problem of fine-tuning over highly-compressed weights from the optimization perspective. We begin by analyzing popular fine-tuning strategies for extreme LLM quantization. The key challenge in this context is that the quantized representations may contain both continuous and discrete variables: while continuous parameters, such as learnable scales or codebooks, can be optimized by backpropagation, the discrete parameters (e.g., integer assignments for the weights) cannot. Existing fine-tuning techniques either do not optimize over discrete parameters at all [71, 21] or fine-tune them using heuristics such as STE or stochastic rounding [3]. Unfortunately, these methods are not well-justified for weight quantization from the point of view of optimization theory, and, as we show in Section 3, can provide poor practical performance.\n\nWe propose an alternative solution: instead of following heuristic gradient estimates, our approach follows the actual gradient of the objective in a small subspace of optimized parameters where it can be meaningfully improved. Following this insight, we formulate the $P V .$ tuning framework for fine-tuning arbitrary quantized representations. We update both discrete and continuous components to minimize a global objective function, such as the KL divergence relative to the original model predictions. Our results show that this strategy leads to significant improvements across weight representations, achieving new state-of-the-art in compression-accuracy trade-offs.\n\nThe main contributions of our work can be summarized as follows:\n\n1. We analyze the problem for training discrete quantized representations for better understanding of the limitations of existing optimization algorithms. We then propose a novel algorithm inspired by compressed gradient methods that addresses these limitations. When compared to straight-through estimation and stochastic rounding, our approach 1) can be shown to converge to a stable solution; and 2) this solution is significantly more accurate in practice.   \n2. We generalize the proposed algorithm into the PV-Tuning framework3, which can minimize a global objective function over a general quantized representation, by optimizing both continuous and discrete parameters via a variant of coordinate descent.   \n3. We demonstrate that PV-tuning can improve quantized model accuracy for leading existing approaches, including GPTQ and AQLM, on popular LLMs including Llama-2 & 3 and Mistral. Our procedure achieves state-of-the-art accuracy (measured through perplexity) in 1- and 2-bit quantization regimes while using the same amount of calibration data as the original algorithms. Importantly, the PV-tuned models use the same underlying weight representations, and are compatible with existing inference kernels. In terms of accuracy per model size, PV-tuning of vector quantization outperforms all prior techniques in the 1-3 bits/parameter range, and is the first to achieve Pareto-optimal quantization for Llama-2 models at around 2 bits per parameter.\n\n# 2 Background\n\nPost-Training LLM Quantization (PTQ). There has been significant interest in PTQ methods [49, 25] that would scale to LLMs. Early work [17, 78, 50] used direct round-to-nearest (RTN) quantization over weight groups of well-chosen size. GPTQ [22] improved upon these results significantly via an accurate one-shot solver for minimizing layer-wise compression errors. Next, AWQ [42] improved upon these results by employing per-channel scaling to reduce the error on important weights while SqueezeLLM [36] implemented non-uniform quantization. QuIP [9] proposed a more accurate weight representation by leveraging incoherence matrices. Another line of works [18, 39] proposes an improved quantized weight representation, which saves a small fraction of outliers in full precision. Other recent works propose augmenting quantized representations with lowrank “adapters” that compensate quantization error [28, 84]. Recently, BiLLM [32] developed residual binarization that stores salient weights in progressively higher bitwidth, quantizing models to nearly 1 bit per parameter at non-catastrophic accuracy loss.\n\nCurrently, the state-of-the-art methods in terms of accuracy-vs-size are QuIP# [71] and AQLM [21]. Both methods work roughly by mapping weight groups to points on highly-dimensional lattices, which are either chosen to satisfy some optimality properties (for QuIP#) or are learned (for AQLM). Interestingly, AQLM showed that fine-tuning the continuous parameters (codebooks) can improve accuracy significantly relative to pure one-shot compression; a variant of this approach was also adopted by QuIP#. PV-Tuning is compatible with both methods: as we show, it can lead to state-ofthe-art compression results for such representations.\n\nFine-tuning over Quantized Weights. As mentioned above, the two SOTA quantization techniques apply fine-tuning, but only update continuous parameters, such as quantization scales. When optimizing over discrete parameter sets, a standard choice in deep learning is the Straight-Through Estimator (STE) [6, 15, 73]. Prior work on LLM compression proposed to update both continuous and discrete parameters, via STE, both for post-training quantization [78, 63] and for training quantized networks from scratch [32]. However, it was observed early on that STE leads to instability when fine-tuning heavily quantized LLMs [78]. While early results suggest that STE can perform well when training quantized models from scratch [44], this behavior is yet to be validated for highly-performant multi-billion-parameter models, which are the focus of our work.\n\nIn summary, the two standard approaches for fine-tuning quantized LLMs are 1) fine-tuning only over the continuous parameters, such as quantization scales, which heavily limits the number of trainable parameters; and 2) optimizing all parameters via the STE, which however is known to be quite noisy especially for extreme quantization. In this context, our work proposes alternative approaches in the post-training compression setting, which lead to state-of-the-art results relative to both options.\n\n# 3 Fine-Tuning Quantized Models\n\nIn this section, we study the problem of fine-tuning quantized models to minimize a global objective, such as cross-entropy. Section 3.1 formulates this problem from an optimization perspective and introduces our notation. In Section 3.2, we analyze several popular strategies for solving this problem and highlight some of their limitations. To circumvent these limitations, we propose an alternative optimization algorithm in Section 3.3 and discuss implementation details in Section 3.4.\n\n# 3.1 Problem description\n\nConsider the problem of minimizing objective (loss) $\\phi$ ,\n\n$$\n\\operatorname* { m i n } _ { \\boldsymbol { x } \\in \\mathbb { R } _ { c } ^ { d } } \\phi ( \\boldsymbol { x } ) ,\n$$\n\nwhere $\\phi : \\mathbb { R } ^ { d }  \\mathbb { R }$ is a differentiable function bounded from below (e.g., by zero), and $\\mathbb { R } _ { c } ^ { d } \\subset \\mathbb { R } ^ { d }$ is a set of all possible quantized weights that can be represented with a given quantization method. Without loss of generality4, we first analyze the case of scalar nonlinear quantization. In this scenario, $c \\in [ d ] : = \\{ 1 , \\breve { 2 } , \\ldots , d \\breve { \\} }$ (typically $c \\ll d$ ), and $\\mathbb { R } _ { c } ^ { d } \\subset \\mathbb { R } ^ { d }$ is the set of all vectors in $\\mathbb { R } ^ { d }$ whose $d$ entries take exactly $c$ distinct values. In other words, the cardinality of the set $V ( x ) : = \\{ x _ { 1 } , \\ldots , x _ { d } \\}$ is equal to $c$ , and we can therefore write $\\mathbb { R } _ { c } ^ { d } : = \\{ x \\in \\mathbb { R } ^ { d } \\ : \\ | V ( x ) | = c \\}$ .\n\nUseful notation. A vector $\\boldsymbol { x } \\in \\mathbb { R } _ { c } ^ { d }$ naturally induces a partition, which we shall call $P ( x )$ , of the set $\\{ 1 , \\ldots , d \\}$ into $c$ nonempty subsets $P _ { 1 } ( x ) , \\dot { . } . . , P _ { c } ( x )$ characterized by\n\n$$\nx _ { i } = x _ { j } \\quad \\Leftrightarrow \\quad \\exists k : i \\in P _ { k } { \\mathrm { ~ a n d ~ } } j \\in P _ { k } .\n$$\n\nLet’s denote $P ( x ) : = \\{ P _ { 1 } ( x ) , \\ldots , P _ { c } ( x ) \\}$ . Moreover, we shall write $P ( y ) \\supseteq P ( x )$ if each element of $P ( x )$ is a subset of some element of $P ( y )$ . For distinct $i , j \\in [ d ]$ , let us introduce the notation $\\delta _ { i j } ( x ) = 1$ if there exists $k$ such that $i , j \\in P _ { k } ( x )$ , and $\\delta _ { i j } ( x ) = 0$ otherwise. Given this notation, notice that $P ( y ) \\supseteq P ( x )$ if and only if for all $i \\neq j$ we have $\\delta _ { i j } ( x ) = 1 \\Rightarrow y _ { i } = y _ { j }$ . Finally, we define $\\mathbb { R } _ { \\leq c } ^ { d } : = \\mathbb { R } _ { 1 } ^ { d } \\cup \\dots \\cup \\mathbb { R } _ { c } ^ { d }$ as the set of all vectors in $\\mathbb { R } ^ { d }$ whose $d$ entries take at most $c$ distinct values. So, if $x \\in \\mathbb { R } _ { c } ^ { d }$ and $P ( y ) \\supseteq P ( x )$ , then $\\boldsymbol { y } \\in \\mathbb { R } _ { \\leq c } ^ { d }$ .\n\nPV method. Following this notation, we define an optimization algorithm that alternates between optimizing $\\phi$ with fixed $P$ or fixed $V$ . From a practitioner’s point of view, these represent optimizing continuous parameters (scales, codebooks, zeros) and discrete codes (assignments), respectively.\n\n$\\diamond$ The $\\mathbf { P }$ step (fixing $P$ ). Given $\\boldsymbol { x } \\in \\mathbb { R } _ { c } ^ { d }$ , consider the mapping\n\n$$\nM _ { P } ( x ) = M _ { P , \\phi } ( x ) : = \\arg \\operatorname* { m i n } _ { y \\in \\mathbb { R } ^ { d } } \\{ \\phi ( y ) : P ( y ) \\supseteq P ( x ) \\} .\n$$\n\nNotice that, necessarily, $M _ { P } ( x ) \\in \\mathbb { R } _ { \\leq c } ^ { d }$ and $\\phi ( M _ { P } ( x ) ) \\le \\phi ( M _ { P } ( x ) ) \\le \\phi ( x )$ . Evaluating $M _ { P }$ amounts to solving an unconstrained optimization problem in a $c$ -dimensional space.\n\n$\\diamond$ The $\\mathbf { V }$ step (fixing $V$ ). Similarly, given $\\boldsymbol { y } \\in \\mathbb { R } _ { c } ^ { d }$ , we define the mapping\n\n$$\nM _ { V } ( y ) = M _ { V , \\phi } ( y ) : = \\arg \\operatorname* { m i n } _ { x \\in \\mathbb { R } ^ { d } } \\{ \\phi ( x ) : V ( x ) \\subseteq V ( y ) \\} .\n$$\n\nLikewise, $M _ { V } ( y ) \\in \\mathbb { R } _ { \\leq c } ^ { d }$ and $\\phi ( M _ { V } ( y ) ) \\leq \\phi ( M _ { V } ( y ) ) \\leq \\phi ( y )$ . Evaluating $M _ { V }$ amounts to solving difficult discrete optimization problems with a search space of size $| V ( x ) | ^ { d } \\leq c ^ { d }$ (exponential in $d$ ).\n\n# Algorithm 1 PV algorithm\n\n1: Initialization: starting point $x ^ { 0 } \\in \\mathbb { R } _ { \\leq c } ^ { d }$   \n2: for $k = 0 , 1 , \\ldots$ . do   \n3: ${ \\begin{array} { r l r } & { { \\stackrel {  } { y ^ { k } } } = { \\stackrel {  } { M } } _ { P } ( x ^ { k } ) : = { \\stackrel {  } { \\operatorname { a r g m i n } } } _ { y \\in \\mathbb { R } ^ { d } } \\{ \\phi ( y ) : P ( y )  \\equiv P ( x ^ { k } ) \\} } & { \\qquad { \\mathrm { ( P s ~ t e p ; ~ c o n t i n u o u s ) } } } \\\\ & { x ^ { k + 1 } = M _ { V } ( y ^ { k } ) : = { \\arg \\operatorname* { m i n } } _ { x \\in \\mathbb { R } ^ { d } } \\{ \\phi ( x ) : V ( x ) \\subseteq V ( y ^ { k } ) \\} } & { \\qquad { \\mathrm { ( V ~ s t e p ; ~ d i s c r e t e ) } } } \\end{array} }$   \n4:   \n5: end for\n\nOur key algorithmic idea, in its simplest form, is to optimize $\\phi$ by alternating the $\\mathrm { ~ \\bf ~ P ~ }$ and $\\mathrm { v }$ steps, i.e., iteratively applying the $M _ { P }$ and $M _ { V }$ operators. (We will propose several more practically-useful approximations and variations later; see Sections 3.2–3.3 and also Appendix B.) This resulting method, which we call the PV method, is formalized as Algorithm 1. Our key guarantee for the PV method is formalized in the next result.\n\nTheorem 3.1 (Convergence of the PV method). Assume $\\phi$ is bounded below, and let $\\boldsymbol { x } ^ { 0 } \\in \\mathbb { R } _ { c } ^ { d }$ . Then (i) $\\boldsymbol { y } ^ { k } \\in \\mathbb { R } _ { \\leq c } ^ { d }$ and $x ^ { k } \\in \\mathbb { R } _ { \\leq c } ^ { d }$ for all $k \\geq 0$ ; (ii) $\\phi ( x ^ { k + 1 } ) \\leq \\phi ( y ^ { k } ) \\leq \\phi ( x ^ { k } )$ for all $k \\geq 0$ ; and (iii) the sequence $\\{ \\phi ( x ^ { k } ) \\} _ { k \\geq 0 }$ converges.\n\nThe proof can be found in Appendix A.1. Note that we do not claim that the method converges to a minimizer of $\\phi$ ; the optimization problem is too difficult for us to be able to guarantee this. However, as we shall see in the numerical results, we nevertheless obtain great empirical performance, especially when coupling the PV approach with some additional algorithmic tricks.\n\nThis general approach is popular in “shallow” machine learning problems; for instance, if $\\phi ( x ) = $ $\\| x - z \\| ^ { 2 }$ is the squared error with respect to some user-specified vector $z$ , then the above algorithm recovers 1-dimensional $K$ -means on the data vector $z$ . Likewise, if $\\phi ( \\cdot )$ is the log-likelihood, then, depending on the choice of the set $\\mathbb { R } _ { c } ^ { d }$ , the approach is related to the EM algorithm [16].\n\nIn turn, we apply the PV method to obtaining highly-accurate quantized LLMs. Applying the PV method “as is”, would be infeasible in practice: computing the $\\mathrm { \\bf P }$ and $\\mathrm { v }$ mappings requires solving difficult optimization problems especially due to LLM parameter scales. However, both mappings can be approximated. The $\\mathrm { \\bf P }$ step can be reparameterized as an unconstrained optimization problem on the unique values in the weight matrix. Practically it means that the “codebooks” can be optimized using an automated differentiation engine (i.e. PyTorch). However, for many quantized representations, $M _ { P } ( x )$ can be approximated by one or more steps of GD, directly optimizing $\\phi$ over the set $V ( x )$ of its $c$ unique values. The $c$ -dimensional gradient can be computed efficiently by backprop, as described in prior works [63, 71]. On the other hand, the $\\mathrm { v }$ step $( M _ { V } ( \\cdot ) )$ is more difficult to approximate as it involves searching a discrete space of size $c ^ { d }$ . We dedicate the next two sections to this task.\n\n# 3.2 Linearized $\\mathbf { V }$ step $\\pmb { \\& }$ gradient-based discrete updates\n\nThe $\\mathrm { \\Delta V }$ mapping (3) can be approximated by solving a discrete least squares problem using an approximation of $\\phi ( x )$ around $y$ :\n\n$$\n\\begin{array} { r } { \\phi ( x ) \\approx \\widetilde { \\phi } _ { y } ( x ) : = \\phi ( y ) + \\langle \\nabla \\phi ( y ) , x - y \\rangle + \\frac { L } { 2 } \\| x - y \\| ^ { 2 } , } \\end{array}\n$$\n\nwhere $L > 0$ is a sufficiently large constant. Subsequently, we perform the $\\mathrm { v }$ step using the simpler convex quadratic function $\\tilde { \\phi } _ { y }$ instead of the typically more complicated function $\\phi$ :\n\n$$\nM _ { V , \\phi } ( y ) \\overset { ( 4 ) } { \\approx } M _ { V , { \\widetilde { \\phi } _ { y } } } ( y ) \\overset { ( 3 ) } { = } \\arg \\operatorname* { m i n } _ { x \\in { \\mathbb { R } ^ { d } } } \\left\\{ { \\widetilde { \\phi } _ { y } } ( x ) : V ( x ) \\subseteq V ( y ) \\right\\} .\n$$\n\nOur first lemma shows that we can replace $\\widetilde { \\phi } _ { y }$ by a more convenient function $\\widehat { \\phi } _ { y }$ measuring the squared distance between $x$ and $\\begin{array} { r } { y ^ { + } : = y - \\frac { 1 } { L } \\bar { \\nabla } \\phi ( y ) } \\end{array}$ , the latter being the point obtbained after taking a single GD step from $y$ with learning rate $\\frac { 1 } { L }$ , disregarding the constraint:\n\nLemma 3.2. For any $\\boldsymbol { y } \\in \\mathbb { R } _ { \\leq c } ^ { d }$ we have $M _ { V , \\widetilde { \\phi } _ { y } } ( y ) = M _ { V , \\widehat { \\phi } _ { y } } ( y )$ , where\n\n$$\n\\begin{array} { l } { \\displaystyle \\widehat { \\phi } _ { y } ( \\boldsymbol { x } ) : = \\left\\| \\boldsymbol { x } - \\left( \\boldsymbol { y } - \\frac { 1 } { L } \\nabla \\phi ( \\boldsymbol { y } ) \\right) \\right\\| ^ { 2 } = \\left\\| \\boldsymbol { x } - \\boldsymbol { y } ^ { + } \\right\\| ^ { 2 } = \\displaystyle \\sum _ { i = 1 } ^ { d } \\left( x _ { i } - y _ { i } ^ { + } \\right) ^ { 2 } . } \\end{array}\n$$\n\nThe proof can be found in Appendix A.2. To summarize, the $\\mathrm { v }$ step of the PV method (Algorithm 1), i.e., $x = M _ { V , \\phi } ( y )$ , can be approximated via the “linearized $\\mathrm { v }$ step”\n\n$$\nx : = { M _ { V , \\phi } ( y ) \\approx M _ { V , \\widehat { \\phi } _ { y } } ( y ) : = \\hat { x } . }\n$$\n\nOur next lemma says that the above approximation is in a certain sense natural reasonable provided that $\\phi$ is $L$ -smooth5 on $\\mathbb { R } _ { \\leq c } ^ { d }$ , i.e., provided that\n\n$$\n\\begin{array} { r l } { \\phi ( x ) \\leq \\phi ( y ) + \\langle \\nabla \\phi ( y ) , x - y \\rangle + \\frac { L } { 2 } \\| x - y \\| ^ { 2 } , } & { \\quad \\forall x , y \\in \\mathbb { R } _ { \\leq c } ^ { d } . } \\end{array}\n$$\n\nLemma 3.3 (Monotonicity). Let $\\boldsymbol { y } \\in \\mathbb { R } _ { \\leq c } ^ { d }$ . If $\\phi$ is $L$ -smooth on $\\mathbb { R } _ { \\leq c } ^ { d }$ , then $\\phi \\left( M _ { V , \\phi } ( y ) \\right) \\leq \\phi ( \\hat { x } ) \\leq$ $\\phi ( y )$ , where $\\hat { x }$ is the point obtained from $y$ by the linearized $V$ step (6).\n\nIndeed, the point $\\hat { x }$ obtained via the linearized $\\mathrm { \\Delta V }$ step can not have a worse loss than the previous point $y$ . Of course, one hopes that the loss will strictly decrease so that the method makes progress. From a practical perspective, the key advantage of linearized $\\mathrm { v }$ step is that it can be performed much faster compared to the vanilla $\\mathrm { v }$ step. The proof of Lemma 3.3 can be found in Appendix A.3.\n\nNote that since $\\widehat { \\phi } _ { y } ( x )$ is separable (see (8)), each entry/weight of $x$ can be optimized independently of others. For scbalar quantization, each individual problem can be solved in $\\mathcal { O } ( \\log _ { 2 } ( c ) )$ time using binary search in sorted version of $V ( y )$ . For vector quantization, there are specialized optimization procedures for efficiently minimizing the $L _ { 2 }$ error (see Appendix D)\n\nKey challenge. The main caveat with linearized $\\mathrm { v }$ step is that it may be impossible to make small gradient-based updates to low-bitwidth discrete weights. More specifically, in (6), one must update the discrete assignments to approximate $\\begin{array} { r } { y ^ { k } - \\frac { 1 } { L } \\nabla \\phi \\overline { { ( y ^ { k } ) } } } \\end{array}$ . However, for low-bit weights, the desired update $\\scriptstyle { \\frac { 1 } { L } } \\nabla \\phi ( y ^ { k } )$ can be smaller than the lowest possible increment to obtain a quantized vector. As a result, the optimal solution to (6) is often $y ^ { k }$ itself. In such a situation, the algorithm will get stuck on $y ^ { k }$ , which is undesirable. This problem is especially pronounced in deep LLMs, where $L$ can be very large, or, from a practitioner’s point of view, where one needs a small learning rate. In practice, as we explore in Section 4.2, the lowest learning rate where the algorithm makes any updates at all is already too large for optimization, leading to divergence.\n\n<html><body><table><tr><td>Algorithm2PV-Tuning:Optimization</td><td>Algorithm 3 PV-Tuning: Implementation, one step</td></tr><tr><td>Require:initial parameters x°∈ Rd objective function :Rd →R,</td><td>Require: quantized model,subspace size tau 1: deq_model := dequantize_weights(model)</td></tr><tr><td>subspace size T ∈ [d] 1:fork=0,...,K-1do</td><td>2: for t=1,...,T do 3:loss=deq_model(next_batch()).loss</td></tr><tr><td>2: > P step: update V(x) by backprop</td><td>4:loss.backward() accumulate gradients</td></tr><tr><td>3: y= arg min{𝜙(y) :P(y)≥P(x²)}</td><td>5: end for for P and V steps</td></tr><tr><td>yERc</td><td>6: > P step: update codebooks by backprop</td></tr><tr><td>4: V step: choose a subspace Sk & update P(𝑥) 5: Sk = argtopT |ViΦ(yb) find T largest</td><td>7:grad_phi = deq_model.weight.grad</td></tr><tr><td>1≤i≤d</td><td>8:grad_codebooks = backprop(grad_phi) 9:model.codebooks = adam(grad_codebooks)</td></tr><tr><td>6: Φy,sk(x）:=</td><td>x-(y-Z(V（y）)) 10: > V step: choose a subspace s and update codes</td></tr><tr><td>7: x²+= argminx {y,sr(x):V(x)CV(y²)</td><td>11:update=adam(grad_phi)-deq_model.weight</td></tr><tr><td>8:end for</td><td>12:s = choose_subspace(update，tau) 13:model.codes[s] = find_nearest(update[s]</td></tr></table></body></html>\n\nMany popular strategies for discrete fine-tuning can be seen as attempts to reconcile coarse lowprecision weights with the need to make small updates. These include straight-through estimation, stochastic rounding, or adding regularizers that push the solution to (6) away from $\\hat { y ^ { k } }$ . We review straight-through estimation in Appendix E.1 and stochastic rounding in Appendix E.2.\n\n# 3.3 Linearized subspace V step\n\nHere we ask the following question: Can we modify the PV method so as to force the $\\mathbf { V }$ step to make a larger update? In other words, we need an optimization algorithm that updates quantized weights either by a sufficiently large increment, or not at all.\n\nA natural example of such an algorithm is coordinate descent (CD) [43, 58], or more generally, subspace descent [26, 38]. Instead of updating all parameters by a small margin, CD in each iteration chooses a single parameter, and makes a large update instead. This strategy can be generalized to updating more parameters at the same time, which leads to subspace descent methods.6 The parameters to be updated can be chosen either greedily, (e.g., several $i \\in [ d ]$ with the largest magnitude of the partial derivative $| \\nabla _ { i } \\phi ( \\cdot ) | )$ , or at random, or through a variety of other means.\n\nLet $S ^ { k } \\subset [ d ]$ be the set of parameters/weights/coordinates we wish to update at iteration $k$ . We choose $| S ^ { k } | = \\tau \\ll d .$ . Let $\\bar { Z } ^ { k } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { d }$ be the linear mapping defined as follows: $\\left( Z ^ { k } ( x ) \\right) _ { i } = x _ { i }$ if $i \\in S ^ { k }$ and $\\left( Z ^ { k } ( x ) \\right) _ { i } = 0$ if $i \\not \\in S ^ { k }$ . We now formulate the linearized subspace $\\mathrm { v }$ step:\n\n$$\nx ^ { + } : = M _ { V , \\widehat { \\phi } _ { y , { s ^ { k } } } } ( y ) : = \\arg \\operatorname* { m i n } _ { x \\in \\mathbb { R } ^ { d } } \\left\\{ \\widehat { \\phi } _ { y , { S ^ { k } } } ( x ) : V ( x ) \\subseteq V ( y ) \\right\\} ,\n$$\n\n$$\n\\begin{array} { r l } { \\mathrm { w h e r e } } & { { } \\widehat { \\phi } _ { y , S ^ { k } } ( \\boldsymbol { x } ) : = \\Big \\| \\boldsymbol { x } - \\Big ( \\boldsymbol { y } - \\frac { 1 } { L _ { S ^ { k } } } Z ^ { k } \\left( \\nabla \\phi ( \\boldsymbol { y } ) \\right) \\Big ) \\Big \\| ^ { 2 } , } \\end{array}\n$$\n\nand $L _ { S ^ { k } } > 0$ is a smoothness parameter of $\\phi$ associated with the subspace spanned by the parameters belonging to $S ^ { k }$ . This detail is important because $L _ { S ^ { k } } \\ll L$ when $\\tau \\ll d$ . When estimating Lipschitz constants for real $L L M s$ , we found that it is lower by at least one order of magnitude, making it possible to train with sufficiently large step sizes (see details and $L _ { S ^ { k } }$ estimates in Appendix F).\n\nNote that, necessarily, $x _ { i } ^ { + } = y _ { i }$ for $i \\not \\in S ^ { k }$ . The remaining $\\tau$ entries of $x ^ { + }$ can be identified exactly by searching a discrete space of size $| V ( y ) | ^ { \\tau }$ , which is feasible if $c = \\mathcal { O } ( 1 )$ and $\\tau = \\mathcal { O } ( 1 )$ , for example.\n\nIn practice, it means that the algorithm can apply large updates to quantized LLM weights, with the caveat that should only update a fraction of them at a time. This allows us to perform the linearized V step with sufficiently large “learning rate” to make non-trivial (i.e., $\\boldsymbol { x } ^ { k + 1 } \\neq \\boldsymbol { y } ^ { k }$ ) improvements to quantized weights even without straight-through estimation or stochastic rounding.\n\nWe formulate the full procedure in Algorithm 2. The algorithm performs the $\\mathrm { \\bf ~ P }$ step by directly optimizing $V ( x )$ (i.e., codebooks) by backprop as described in Section 3.1. For the $\\mathrm { \\Delta V }$ step, the algorithm greedily chooses a subset of $\\tau$ quantized weights for update, then updates them using Eq. (8). The arg top $\\tau$ operator finds $\\tau$ indices with the largest absolute gradient values and builds a subspace of $\\mathbb { R } _ { \\leq c } ^ { d }$ where only these values can be changed, and the rest must be equal to $y ^ { k }$ .\n\n# 3.4 Implementation details\n\nTo speed up convergence, we use adaptive learning rates for both P and $\\mathrm { v }$ steps. In Eq. 8, we replace $\\nabla \\phi ( y )$ with a single Adam [37] update, as depicted in Algorithm 3. In preliminary experiments, we found that this results in a significant convergence speedup. When choosing the subspace $S ^ { k }$ , we select weights based not on $| \\nabla _ { i } \\phi ( y ) |$ , but on the magnitude of Adam update for that weight. For simplicity, we greedily choose the $\\tau$ weights with the largest update norm within each weight matrix.\n\nThis could be further improved through better techniques for choosing $S ^ { k }$ explored in Appendix Q. We also found that, despite the fact that PV-tuning by itself outperforms straight-through estimation, we could achieve slightly better accuracy by combining PV-tuning with straight-through estimation. We explore this in more detail in Section 4.2).\n\nWe describe our approach for preparing the calibration data in Appendix G. We found that the preprocessing used in several recent PTQ works introduce a small bias when sampling the calibration data, leading to somewhat worse fine-tuning accuracy. For fairness, we always compare representations (Section 4.1) and algorithms (Section 4.2) using the same pre-processing.\n\nFine-tuning efficiency. The most compute-intensive part of PV tuning is computing the gradients $\\nabla \\phi ( \\cdot )$ , which is done through repeated forward and backward passes on an LLM. To reduce the number of gradient accumulations, we reuse gradients for $\\mathrm { \\bf P }$ and $\\mathrm { \\Delta V }$ steps within one iteration. We use mixed precision, gradient checkpointing and batch accumulation to train more efficiently; for larger LLMs such as LLAMA 3 70B we also use sharding and optimizer offloading (see Appendix H). Our code can train 7B LLMs on a single GPU, while larger ones (e.g. 70B) fit into a single machine with $8 \\times \\mathrm { A 1 0 0 }$ . In terms of wall-clock time, PV-tuning takes up to $1 . 5 \\times$ longer than the fine-tuning procedure of [71] and requires additional memory in order to hold $\\nabla \\phi ( x )$ .\n\n# 4 Experiments\n\n# 4.1 Evaluating quantized representations with finetuning\n\nBefore evaluating PV-tuning, we need to choose the quantized representation to be fine-tuned. We therefore compare popular weight representations from recent works on LLM quantization (see Section 2). To better isolate the effect of the weight representation, we evaluate them in three configurations: i) when quantizing a single LLM layer, in terms of MSE, ii) full model quantization in terms of perplexity without finetuning and iii) with finetuning.\n\nWe compare several recently proposed quantized representations (see details in Appendix J):\n\n1. GPTQ: scalar uniform quantization with channel-wise and block-wise scales [22],   \n2. SpQR: an extension of block-wise GPTQ with learned sparse outliers [18],   \n3. VQ: basic vector quantization with a single codebook [72] with multi-step training.   \n4. AQLM: additive vector quantization with multiple learned codebooks [21],   \n5. QuIP#: vector quantization with lattices and incoherence processing [71],   \n6. $\\mathbf { V Q / A Q + }$ outliers: vector/additive quantization with sparse outliers via pruning [66, 8],   \n7. VQ/AQ $^ +$ lowrank: vector/additive quantization with Low-Rank Compensation (LoRC) [79],\n\n![](images/52fb55592baf8a2dc2b9fdb9655978878291ab7e521ba2d43638a08c5192b69e.jpg)  \nFigure 2: (left) L2 errors for 17th layer of LLAMA 2 7B with different representations. Full model perplexity on WikiText-2 is reported without finetuning (middle) and with fine-tuning (right).\n\nWe run all three experiments on LLAMA 2 7B model [69], calibrating on the RedPajama [13] dataset that best approximates the original pre-training data. When evaluating single layer errors, we report the L2 error in attention query projection outputs of a fixed transformer block, with other blocks exhibiting similar behavior. For full model evaluation, we report quantized model perplexity on WikiText-2 [45] dataset. We use the same data splits and preprocessing as in most recent PTQ works [22, 42, 18, 70, 21, 71], including the biased preprocessing step that we mentioned in Section 3.4. For fine-tuning, we train continuous parameters only, using the approach from [71]. To compare these diverse representations, we evaluate their quantization errors as a function of average number of bits per parameter. To get a diverse set of bits per parameter, we vary the hyperparameters such as wbits, block size, codebook and group size for vector quantization and the rate of outliers.\n\nFigure 2 summarizes our findings. Overall, vector quantization methods (VQ, QuIP# and AQLM) outperform their scalar counterparts. Outliers and low-rank compensation both reduce error, but this improvement comes at the cost of extra bits per parameter. Interestingly, the improvement from outliers is significantly smaller when both methods have access to fine-tuning. Likewise, the improvement from using low-rank adapters also diminishes when comparing fine-tuned models, to a point where it no longer justifies the increase in model size. We provide a more detailed breakdown of results and hyperparameter configurations in Appendix J.\n\nOur main takeaway is that for sub 2 bits per parameter, the vector quantization (VQ) representation can achieve near-optimal quantization accuracy, whether or not it uses outliers, LoRC or incoherence processing. Naturally, this does not reduce the value of prior works since they were designed for different scenarios, typically with a higher number of bits per parameter.\n\n# 4.2 Evaluating Fine-tuning Algorithms\n\nNext, we compare different fine-tuning strategies and ablate our PV-tuning protocol. We design our protocol to be representation-agnostic, i.e. compatible with different quantized representations. To showcase this, we pick three methods from the previous section: GPTQ, VQ and AQLM.\n\nThese methods differ not only in their weight representations, but also in how they search for the optimal codes. Namely, GPTQ can scale the target weight and round it to nearest 2-bit integer. In turn, VQ quantizes weights as a group and must find the nearest vector from its codebook, and AQLM uses a multi-step beam search procedure to choose the best combination of codes from both codebooks. Our PV-Tuning implementation uses these search algorithms during the subspace linearized V step (find_nearest in Alg. 3). We describe the full PV configuration for each method in Appendix K.\n\nWe compare PV tuning against several popular fine-tuning regimens found in the literature. Our first baseline is fine-tuning only continuous parameters, e.g., codebooks or input/output embeddings [71, 74]. The second baseline is training with Straight Through Estimation (STE) [75, 77]. We also test stochastic rounding as described in Appendix E.2. Finally, we evaluate PV tuning combined with STE, but otherwise the same configuration. We set the subspace size $\\tau$ equal to the number of weights such that the update satisfies $\\| x ^ { k \\breve { + } 1 } - x ^ { k } \\| / \\| x ^ { k } \\| \\leq 0 . 0 1$ , also known as known as trust ratio [81].\n\nThe results in Table 1 show that PV-Tuning consistently finds better quantized models, with STE coming consistently second. We explore this further by combining subspace updates with STE, which leads to slightly better perplexity and accuracy in most (but not all) setups.\n\nTable 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on LLAMA 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy on tasks from Section 4.3.   \n\n<html><body><table><tr><td>Fine-tuning Method</td><td colspan=\"3\">GPTQ 2.14 bit/w Wiki2↓ C4↓ Acc.↑</td><td colspan=\"3\">VQ, 1.58 bit/w Wiki2↓ C4↓ Acc.↑</td><td colspan=\"3\">AQLM, 2.01 bit/w Wiki2↓</td></tr><tr><td>Calibration only (no global fine-tuning)</td><td>3290</td><td>4125</td><td>29.0</td><td>20.26</td><td>20.09</td><td>43.42</td><td>7.38</td><td>C4↓ Acc.↑ 9.34 53.2</td></tr><tr><td>Continuous params only [71,21]</td><td>16.77</td><td>17.53 46.27</td><td>8.17</td><td></td><td>10.99 52.14</td><td>6.69</td><td>8.77</td><td>56.57</td></tr><tr><td>Naive Linearized PV (no subspace)</td><td>16.73</td><td>17.48 47.68</td><td>8.19</td><td></td><td>10.94 52.08</td><td>6.68</td><td>8.75</td><td>56.51</td></tr><tr><td>Stochastic Rounding [53] (tuned)</td><td>11.97</td><td>13.07 49.79</td><td>8.02</td><td></td><td>10.64 52.31</td><td>6.56</td><td></td><td>8.39 56.68</td></tr><tr><td>Straight Through Estimation [77]</td><td>8.79</td><td>11.04 50.61</td><td>7.76</td><td></td><td>10.26 52.58</td><td>6.41</td><td></td><td>8.63 57.04</td></tr><tr><td>Subspace Linearized PV (ours,T=0.01)</td><td>8.49</td><td>10.78 52.17</td><td>7.38</td><td>9.47</td><td>53.36</td><td>6.13</td><td>8.35</td><td>57.81</td></tr><tr><td>Subspace Linearized PV+STE(T=0.01)</td><td>8.43</td><td>10.82 51.90</td><td>7.32</td><td>9.35</td><td>55.22</td><td>5.90</td><td>7.43</td><td>58.19</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>\n\nPV-Tuning over QuIP# In addition to these three configurations, we also apply PV-tuning to QuIP# [71] — a modification of vector quantization that applies Randomized Hadamard Transform (RHT) before quantization and uses fixed lattices instead of learned codebooks. We experiment with Llama-2 7B model quantized with QuIP# to 2 bits per weight and found that it is possible to significantly improve the model through PV-Tuning. For instance, PV-tuning improves WikiText-2 perplexity from 6.19 (QuIP# with built-in continuous fine-tuning) to 5.71 (PV-Tuning $^ +$ STE). Since original 16-bit model has a perplexity of 5.13, this corresponds to almost halving the quantization error in terms of perplexity. We report additional details for QuIP# with PV-Tuning and full evaluation results in Appendix L and include it to Table 2 as “QuIP#+PV”.\n\nOn the choice of hyperparameters for 1-bit vector quantization. There are several possible hyperparameter configurations for vector quantization (VQ) that fall into 1-1.1 bit range. One can either use larger codebooks for longer groups (vectors), or smaller codebooks for shorter groups accordingly. In our main evaluations, we quantized vectors of 16 consecutive weights with 14-16 bit codebooks to fit into the desired bitwidth. However, we later found that it is more advantageous to choose smaller groups as well as codebooks. We found that 8-bit code per 8 weights outperforms 14-bit code per 16 weights despite having near-identical bitwidth (due to smaller codebooks). We report this configuration as “PV (gs8)” in Table 2 and provide additional experiments in Appendix M.\n\n# 4.3 Large-scale Evaluation & Discussion\n\nFinally, we evaluate the resulting PV algorithm with a vector quantization backbone and KL objective on a range of popular LLM models. For this section, our goal is to evaluate our approach holistically for different models and target bit-widths, comparing against the best known baselines in common settings. To that end, we evaluate on LLAMA 2 & 3 [69], MISTRAL 7B [34] and PHI-3 Mini-4kInstruct [1] at 1–2.5 bits per parameter (averaged over all transformer layers).\n\nWe report perplexity on WikiText-2 [45] and C4 [54] validation sets, zero-shot accuracy on WinoGrande [60], PiQA [67], HellaSwag [83], ARC-easy and ARC-challenge [12] via the LM Eval Harness [24]. We follow the exact evaluation setup from GPTQ [22]. We compare against QuIP [70], BiLLM [32], PB-LLM [62], DB-LLM [10], AQLM [21], OneBit [77], QuIP# [71], the latter three using fine-tuning. For LLAMA 3, we use baselines from [33] and re-evaluate perplexity in our setup.\n\nTable 2: Quantized model perplexity on WikiText-2 [45] & $\\mathbf { C 4 } \\downarrow$ [54] and the Average accuracy on 5 zero-shot tasks [24] for various models and bitwidths. Arrows $\\uparrow / \\downarrow$ mean higher / lower is better.   \n\n<html><body><table><tr><td colspan=\"10\">Size Method</td></tr><tr><td></td><td colspan=\"4\">Avg bits|Wiki2↓ C4↓Average↑ LLAMA 2 model family</td><td colspan=\"7\">Size Method Avg bits|Wiki2↓ C4↓ Average↑ LLAMA 3 model family</td></tr><tr><td rowspan=\"11\">7B</td><td></td><td>16</td><td>5.12</td><td>6.63</td><td>64.80</td><td rowspan=\"11\">8B</td><td></td><td>16</td><td>5.54</td><td>7.10</td><td>68.61</td></tr><tr><td>BiLLM</td><td>1.08</td><td>32.48</td><td>40.52</td><td>41.68</td><td>BiLLM</td><td>1.1</td><td>28.8</td><td>257</td><td>37.90</td></tr><tr><td>OneBit</td><td>1.01</td><td>9.73</td><td>11.11</td><td>50.06</td><td>PB-LLM</td><td>1.7</td><td>35.68</td><td>197.56</td><td>36.00</td></tr><tr><td>PV-Tuning</td><td>1.02</td><td>8.28</td><td>10.37</td><td>50.66</td><td>PV-Tuning</td><td>1.01</td><td>11.17</td><td>11.67</td><td>50.01</td></tr><tr><td>PV (gs8)</td><td>1.00</td><td>7.62</td><td>9.73</td><td>53.77</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AQLM</td><td>2.02</td><td>6.64</td><td>8.56</td><td>56.47</td><td>QuIP PB-LLM</td><td>2.01 2.00</td><td>76.95 21.74</td><td>98.47 61.04</td><td>36.8 38.80</td></tr><tr><td>QuIP#</td><td>2.01</td><td>6.19</td><td>8.16</td><td>57.51</td><td>DB-LLM</td><td>2.01</td><td>12.77</td><td>14.82</td><td>51.8</td></tr><tr><td>DB-LLM</td><td>2.01</td><td>7.23</td><td>9.62</td><td>55.12</td><td>PV-Tuning</td><td>2.01</td><td>6.99</td><td>8.29</td><td>64.36</td></tr><tr><td>PV-Tuning</td><td>2.02</td><td>5.84</td><td>7.62</td><td>61.35</td><td>BiLLM</td><td></td><td></td><td></td><td></td></tr><tr><td>QuIP#+PV</td><td>2.01</td><td>5.71</td><td>7.51</td><td>61.81</td><td rowspan=\"4\">70B</td><td>16 1.1</td><td>2.59 15.26</td><td>5.78 65.07</td><td>75.37 44.2</td></tr><tr><td rowspan=\"6\">13B</td><td></td><td>16</td><td>4.57</td><td>6.05</td><td>67.82</td><td>PV-Tuning</td><td>1.01</td><td>8.67 9.68</td><td>51.47</td></tr><tr><td>AQLM</td><td>1.97</td><td>5.65</td><td>7.51</td><td>60.59</td><td></td><td></td><td></td><td>48.71</td></tr><tr><td>QuIP#</td><td>2.01</td><td>5.35</td><td>7.20</td><td>61.45</td><td>QuIP PB-LLM</td><td>2.00 2.00</td><td>11.63 10.33</td><td>18.54 28.89</td></tr><tr><td>DB-LLM</td><td>2.01</td><td>6.19</td><td>8.38</td><td>59.41</td><td>2.07</td><td></td><td>4.57</td><td>46.04</td></tr><tr><td>PV-Tuning</td><td>1.97</td><td>5.12</td><td>6.83</td><td>64.92</td><td>PV-Tuning</td><td></td><td>6.56</td><td>70.38</td></tr><tr><td>PV-Tuning</td><td>2.19</td><td>5.05</td><td>6.74</td><td>66.05</td><td colspan=\"4\">MISTRAL 7B vO.1 (A) and PHI 3 Mini-4k-Instruct (B)</td></tr><tr><td rowspan=\"6\">70B</td><td></td><td>16</td><td>3.12</td><td>4.97</td><td>72.40 68.75</td><td>7B</td><td>16</td><td>4.78</td><td>5.71</td><td>69.38</td></tr><tr><td>AQLM</td><td>2.07</td><td>3.94</td><td>5.72</td><td></td><td>QuIP# (A) PV-Tuning</td><td>2.01 2.01</td><td>6.02 5.29</td><td>6.84 6.17</td><td>62.20</td></tr><tr><td>QuIP#</td><td>2.01</td><td>3.91</td><td>5.71</td><td>68.60</td><td></td><td></td><td></td><td></td><td>66.32</td></tr><tr><td>DB-LLM</td><td>2.01</td><td>4.64</td><td>6.77</td><td>65.83</td><td>3.8B AQLM</td><td>16</td><td>5.83</td><td>9.35</td><td>70.5</td></tr><tr><td>PV-Tuning</td><td>2.07</td><td>3.78</td><td>5.56</td><td>70.72</td><td>(B) PV-Tuning</td><td>2.03 2.03</td><td>8.85 6.88</td><td>12.19 10.08</td><td>60.4</td></tr><tr><td>PV-Tuning</td><td>1.14</td><td>5.52</td><td>7.50</td><td>64.58</td><td></td><td></td><td></td><td></td><td>65.70</td></tr></table></body></html>\n\nTable 2 summarizes our findings: PV-tuning with vector quantization outperforms all known methods for 1- and 2-bit per weight. The closest competitors on LLAMA 2 are QuIP#, AQLM and OneBit, all of which use fine-tuning. The improvements on LLAMA 3 are also remarkable as this model is notoriously hard to compress [33]. We report additional evaluations in Appendix N.\n\nPareto-optimality. A key practical question concerns obtaining optimal quality for the target model size, where a smaller model compressed to 3-4 bits often dominates a larger model compressed to 1-bit. The best known Pareto-optimal bit-width for Llama 2 is 2.5 [21]: compressing a larger model to less than 2.5 bits per weight is inferior to a smaller model quantized to the same total number of bytes. From this perspective, PV-tuning pushes the Pareto-optimal frontier for LLAMA 2 to 2.0 bits. This is easiest to see in Table 12: a 2-bit 13B model outperforms any 7B quantization and is comparable with the 16-bit 7B model. The same holds for the 2-bit 70B model.\n\nFine-tuning efficiency. One limitation of our algorithm is that it requires more compute and memory during the fine-tuning procedure. The 7B models can be fine-tuned on a single GPU, our 70B runs require a server with $8 \\times A 1 0 0$ or rely on RAM offloading. PV-Tuning shares this drawback with prior methods based on STE [21, 71], as both methods need gradients w.r.t. dequantized weights. Our longest training run took 2 days on 8 GPUs to outperform all baselines and 8 days to fully converge.\n\nInference speed. PV-Tuning does not change the underlying compressed representation, allowing us to reuse existing high-performance inference kernels. Specifically, $\\mathrm { \\Delta V Q + P V }$ can reuse efficient kernels from [21, 71], while GPTQ $+ \\mathrm { P V }$ can use ExLlamaV2 kernels [14]. We report these inference speed evaluations in Appendix O. From a practitioner’s point of view, PV-Tuning can significantly improve the accuracy of extreme (1-2 bit) quantized models, making it possible to deploy large LLMs on resource-constrained devices. As a proof of concept, we developed specialized inference engines for running vector-quantized models with PV-Tuning on mobile devices7 or in the browser8.\n\n# 5 Conclusions\n\nLimitations. We focused our effort on evaluating PV-Tuning with multiple setups and models, but spent relatively little effort tuning our algorithm for each specific setup. For instance, we always use constant learning rate and $\\tau$ with no schedule, and always train on the same data. While this shows robustness of PV-Tuning, it also means that our results may be improved with better hyperparameters. For instance, Appendix M shows how PV-Tuning with 1-bit vector quantization can be improved by choosing smaller vector sizes, while Appendix L suggests that PV-Tuning can dramatically improve models quantized with QuIP# and may similarly be applied to other quantized representations. Furthermore, the algorithm could achieve better accuracy by simply training longer and on more data.\n\nFuture work. This work opens several new research directions. The first is about how to choose $S ^ { k }$ : while we found that a greedy strategy works in practice, there may be fundamentally better ways. Another direction is applying PV-Tuning to other quantization niches: our evaluation focuses on extreme weight-only quantization, but the proposed algorithm can be used in weight $^ +$ activation setting or KV cache quantization. Overall, PV-Tuning shows how an insight from optimization theory can improve LLM quantization and we are excited to see how this develops in future research.",
    "summary": "{\n  \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n现有大语言模型（LLM）“极端”压缩（每个参数1 - 2位）的纯训练后方法在精度与位宽的权衡上收益递减，且当前微调策略常使用的直通估计器（STE）在极端压缩场景下性能不佳，现有微调策略是否最优尚不明确。解决该问题对于在资源受限设备上高效运行LLM至关重要。\\n\\n**方法概述**\\n论文提出PV - Tuning框架，这是一种与表示无关的框架，通过坐标下降的变体交替优化连续和离散参数，以最小化全局目标函数，改进现有的微调策略。\\n\\n**主要贡献与效果**\\n1. 分析现有优化算法局限，提出受压缩梯度方法启发的新算法，与直通估计和随机舍入相比，能收敛到稳定解，且实践中更准确。\\n2. 将算法推广到PV - Tuning框架，可优化一般量化表示的连续和离散参数。\\n3. 在流行的LLM上，PV - Tuning能提升量化模型精度，在1 - 2位量化制度下，使用相同校准数据达到最先进的准确率（通过困惑度衡量）。在每模型大小的准确率方面，在1 - 3位/参数范围内优于所有先前技术，首次实现Llama - 2模型在约2位/参数的帕累托最优量化。例如，对于QuIP#量化的Llama - 2 7B模型，PV - Tuning将WikiText - 2困惑度从6.19（QuIP#内置连续微调）降低到5.71（PV - Tuning + STE）。\",\n  \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nPV - Tuning框架通过交替优化连续参数（如尺度、码本、零点）和离散代码（如权重的整数分配）来最小化全局目标函数，如相对于原始模型预测的KL散度。它遵循目标在优化参数的小子空间中的实际梯度，在该子空间中目标可以有意义地改善。\\n\\n**创新点**\\n现有微调技术要么不优化离散参数，要么使用启发式方法（如STE或随机舍入）来微调它们，这些方法从优化理论角度缺乏充分依据，实际性能不佳。PV - Tuning框架通过优化连续和离散参数，且在实际梯度有意义改善的子空间中更新，能收敛到稳定解，实践中更准确。\\n\\n**具体实现步骤**\\n1. **初始化**：从初始点 $x^0 \\in \\mathbb{R}_{\\leq c}^d$ 开始。\\n2. **迭代更新**：对于 $k = 0, 1, \\ldots$ 进行迭代。\\n    - **P步骤**：固定分区 $P$，通过反向传播更新 $V(x)$，并找到 $y^k = \\arg \\min\\{ \\phi(y) : P(y) \\geq P(x^k) \\}$，可通过反向传播更新码本。具体而言，$M_P(x) = M_{P, \\phi}(x) := \\arg \\min_{y \\in \\mathbb{R}^d} \\{ \\phi(y) : P(y) \\supseteq P(x) \\}$，且 $M_P(x) \\in \\mathbb{R}_{\\leq c}^d$ ，$\\phi ( M_P(x) ) \\leq \\phi (x)$ ，此步骤相当于在 $c$ 维空间中求解无约束优化问题。\\n    - **V步骤**：固定值集 $V$，选择子空间 $S^k$ 并更新 $P(x)$，即 $x^{k + 1} = \\arg \\min\\{ \\phi_{y, S^k}(x) : V(x) \\subseteq V(y^k) \\}$。其中 $S^k$ 通过选择具有最大绝对梯度值的 $\\tau$ 个指标构建子空间。具体为 $M_V(y) = M_{V, \\phi}(y) := \\arg \\min_{x \\in \\mathbb{R}^d} \\{ \\phi(x) : V(x) \\subseteq V(y) \\}$ ，$M_V(y) \\in \\mathbb{R}_{\\leq c}^d$ ，$\\phi ( M_V(y) ) \\leq \\phi (y)$ ，但求解该步骤涉及搜索大小为 $|V(x)|^d \\leq c^d$ 的离散空间，通常较困难，可通过线性化 $V$ 步骤进行近似求解，即使用 $\\widehat{\\phi}_y(x) := \\left\\| x - \\left( y - \\frac{1}{L} \\nabla \\phi ( y ) \\right) \\right\\| ^ { 2 }$ 替代 $\\phi(x)$ 进行优化。\\n3. **算法优化**：使用自适应学习率，将 $\\nabla \\phi(y)$ 替换为Adam更新；选择子空间 $S^k$ 时，基于Adam更新的幅度选择权重。\\n\\n**案例解析**：论文未明确提供此部分信息\",\n  \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n1. 仅校准（无全局微调）\\n2. 仅微调连续参数（如代码本或输入/输出嵌入） [71, 21]\\n3. 直通估计（Straight Through Estimation, STE） [75, 77]\\n4. 随机舍入（Stochastic Rounding）\\n5. QuIP [70]\\n6. BiLLM [32]\\n7. PB - LLM [62]\\n8. DB - LLM [10]\\n9. AQLM [21]\\n10. OneBit [77]\\n11. QuIP# [71]\\n\\n**性能对比**\\n*   **在 [困惑度/Perplexity] 指标上**：在WikiText - 2和C4数据集上，PV - Tuning在多种模型和量化表示下表现出色。例如，在LLAMA 2 7B模型的GPTQ 2.14 bit/w配置中，PV - Tuning（子空间线性化PV）的WikiText - 2困惑度为8.49，C4困惑度为10.78，优于仅校准（WikiText - 2为3290，C4为4125）、仅微调连续参数（WikiText - 2为16.77，C4为17.53）、朴素线性化PV（WikiText - 2为16.73，C4为17.48）、随机舍入（WikiText - 2为11.97，C4为13.07）和STE（WikiText - 2为8.79，C4为11.04）等方法。在VQ 1.58 bit/w的设置下，PV - Tuning（子空间线性化PV）的WikiText - 2困惑度为9.47，优于仅校准（20.26）、仅微调连续参数（10.99）、朴素线性化PV（10.94）、随机舍入（10.64）和STE（10.26）。在AQLM 2.01 bit/w的设置下，PV - Tuning（子空间线性化PV）的WikiText - 2困惑度为8.35，优于仅校准（7.38）、仅微调连续参数（8.77）、朴素线性化PV（8.75）、随机舍入（8.39）和STE（8.63）。对于QuIP#量化的Llama - 2 7B模型，PV - Tuning将WikiText - 2困惑度从6.19（QuIP#内置连续微调）降低到5.71（PV - Tuning + STE）。在不同模型和位宽的大规模评估中，PV - Tuning在WikiText - 2和C4数据集上的困惑度大多优于其他对比方法，如在Llama - 2 7B的1.02位/参数设置下，PV - Tuning的WikiText - 2困惑度为8.28，优于OneBit（9.73）和BiLLM（32.48）。\\n*   **在 [平均零样本准确率/Average Zero - Shot Accuracy] 指标上**：在WinoGrande、PiQA、HellaSwag、ARC - easy和ARC - challenge等任务上，PV - Tuning也取得了较好的结果。如在LLAMA 2 7B模型的GPTQ 2.14 bit/w配置中，PV - Tuning（子空间线性化PV）的平均零样本准确率为52.17%，优于仅校准（29.0%）、仅微调连续参数（46.27%）、朴素线性化PV（47.68%）、随机舍入（49.79%）和STE（50.61%）。在VQ 1.58 bit/w的设置下，PV - Tuning（子空间线性化PV）的平均零样本准确率为53.36%，优于仅校准（43.42%）、仅微调连续参数（52.14%）、朴素线性化PV（52.08%）、随机舍入（52.31%）和STE（52.58%）。在AQLM 2.01 bit/w的设置下，PV - Tuning（子空间线性化PV）的平均零样本准确率为57.81%，优于仅校准（53.2%）、仅微调连续参数（56.57%）、朴素线性化PV（56.51%）、随机舍入（56.68%）和STE（57.04%）。\",\n  \"keywords\": \"### 关键词\\n\\n- 大语言模型压缩 (Large Language Model Compression, N/A)\\n- 量化感知微调 (Quantization - Aware Fine - Tuning, N/A)\\n- PV - Tuning框架 (PV - Tuning Framework, N/A)\\n- 极端压缩 (Extreme Compression, N/A)\\n- 向量量化 (Vector Quantization, VQ)\\n- 坐标下降 (Coordinate Descent, CD)\\n- 直通估计器 (Straight - Through Estimator, STE)\"\n}"
}