{
    "link": "https://arxiv.org/abs/2502.05726",
    "pdf_link": "https://arxiv.org/pdf/2502.05726",
    "title": "Improving Environment Novelty Quantification for Effective Unsupervised Environment Design",
    "authors": [
        "J. Teoh",
        "Wenjun Li",
        "Pradeep Varakantham"
    ],
    "institutions": [
        "Singapore Management University"
    ],
    "publication_date": "2025-02-08",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "paper_content": "# Improving Environment Novelty Quantification for Effective Unsupervised Environment Design\n\nJayden Teoh∗, Wenjun ${ \\bf L i ^ { * } } ^ { \\dag }$ , Pradeep Varakantham Singapore Management University {jxteoh.2023, wjli.2020, pradeepv}@smu.edu.sg\n\n# Abstract\n\nUnsupervised Environment Design (UED) formalizes the problem of autocurricula through interactive training between a teacher agent and a student agent. The teacher generates new training environments with high learning potential, curating an adaptive curriculum that strengthens the student’s ability to handle unseen scenarios. Existing UED methods mainly rely on regret, a metric that measures the difference between the agent’s optimal and actual performance, to guide curriculum design. Regret-driven methods generate curricula that progressively increase environment complexity for the student but overlook environment novelty–a critical element for enhancing an agent’s generalizability. Measuring environment novelty is especially challenging due to the underspecified nature of environment parameters in UED, and existing approaches face significant limitations. To address this, this paper introduces the Coverage-based Evaluation of Novelty In Environment (CENIE) framework. CENIE proposes a scalable, domainagnostic, and curriculum-aware approach to quantifying environment novelty by leveraging the student’s state-action space coverage from previous curriculum experiences. We then propose an implementation of CENIE that models this coverage and measures environment novelty using Gaussian Mixture Models. By integrating both regret and novelty as complementary objectives for curriculum design, CENIE facilitates effective exploration across the state-action space while progressively increasing curriculum complexity. Empirical evaluations demonstrate that augmenting existing regret-based UED algorithms with CENIE achieves stateof-the-art performance across multiple benchmarks, underscoring the effectiveness of novelty-driven autocurricula for robust generalization.\n\n# 1 Introduction\n\nAlthough recent advancements in Deep Reinforcement Learning (DRL) have led to many successes, e.g., super-human performance in games [26, 9] and reliable control in robotics [2, 3], training generally-capable agents remains a significant challenge. DRL agents often fail to generalize well to environments only slightly different from those encountered during training [21, 63]. To address this problem, there has been a surge of interest in Unsupervised Environment Design (UED; [56, 23, 57, 28, 27, 37, 33, 7]), which formulates the autocurricula [32] generation problem as a two-player zero-sum game between a teacher and a student agent. In UED, the teacher constantly adapts training environments (e.g., mazes with varying obstacles and car-racing games with different track designs) in the curriculum to improve the student’s ability to generalize across all possible levels.\n\nTo design effective autocurricula, researchers have proposed various metrics to capture learning potential, enabling teacher agents to create training levels that adapt to the student’s capabilities.\n\nThe most popular metric, regret, measures the student’s maximum improvement possible in a level. While regret-based UED algorithms [23, 27, 28] are effective in producing levels at the frontier of the student’s capability, they do not guarantee diversity in the student’s experiences, limiting the training of generally-capable agents especially in large environment design spaces. Another line of work in UED recognizes this limitation, leading to methods exploring the prioritization of novel levels during curriculum generation [56, 57, 37, 33]. This strategic shift empowers the teacher to introduce novel levels into the curriculum such that the student agent can actively explore the environment space and enhance its generalization capabilities.\n\nTo more effectively evaluate environment novelty, we introduce the Coverage-based Evaluation of Novelty In Environment (CENIE) framework. CENIE operates on the intuition that a novel environment should induce unfamiliar experiences, pushing the student agent into unexplored regions of the state space and introducing variability in its actions. Therefore, signals about an environment’s novelty can be derived by modeling and comparing its state-action space coverage with those of environments already encountered in the curriculum. We refer to this method of estimating novelty based on the agent’s past experiences as curriculum-aware. By evaluating novelty in relation to the experiences induced by other environments within the curriculum, CENIE prevents redundant environments—those that elicit similar experiences as existing ones—from being classified as novel. Curriculum-aware approaches ensure that levels in the student’s curriculum collectively drive the agent toward novel experiences in a sample-efficient manner.\n\nOur contributions are threefold. First, we introduce CENIE, a scalable, domain-agnostic, and curriculum-aware framework for quantifying environment novelty via the agent’s state-action space coverage. CENIE addresses shortcomings in existing methods for environment novelty quantification, as discussed further in Sections 3 and 4. Second, we present implementations for CENIE using Gaussian Mixture Models (GMM) and integrated its novelty objective with $\\mathrm { P L R ^ { \\perp } } [ 2 8 ]$ and ACCEL[37], the leading UED algorithms in zero-shot transfer performance. Finally, we conduct a comprehensive evaluation of the CENIE-augmented algorithms across three distinct benchmark domains. By incorporating CENIE into these leading UED algorithms, we empirically validate that CENIE’s novelty-based objective not only exposes the student agent to a broader range of scenarios in the state-action space, but also contributes to achieving state-of-the-art zero-shot generalization performance. This paper underscores the importance of novelty and the effectiveness of the CENIE framework in enhancing UED.\n\n# 2 Background\n\nWe briefly review the background of Unsupervised Environment Design (UED) in this section. UED problems are modeled as an Underspecified Partially Observable Markov Decision Process (UPOMDP) defined by the tuple:\n\n$$\n\\langle S , A , O , \\mathcal { I } , \\mathcal { T } , \\mathcal { R } , \\gamma , \\Theta \\rangle\n$$\n\nwhere $S , A$ and $O$ are the sets of states, actions, and observations, respectively. $\\Theta$ represents a set of free parameters where each $\\theta \\in \\Theta$ defines a specific instantiation of an environment (also known as a level). We use the terms “environments” and “levels” interchangeably throughout this paper. The level-conditional observation and transition functions are defined as $\\mathcal { T } : S \\times \\Theta  O$ and $\\mathcal { T } : S \\times A \\times \\Theta \\to \\Delta ( S )$ , respectively. The student agent, with policy $\\pi$ , receives a reward based on the reward function $\\mathcal { R } : S \\times A  \\mathbb { R }$ with a discount factor $\\gamma \\in [ 0 , 1 ]$ . The student seeks to maximize its expected value for each $\\theta$ denoted by $\\begin{array} { r } { V ^ { \\theta } ( \\pi ) = \\mathbb { E } _ { \\pi } [ \\sum _ { t = 0 } ^ { T } R ( s _ { t } , a _ { t } ) \\gamma ^ { t } ] } \\end{array}$ . The teacher’s goal is to select levels forming the curriculum by maximizing a utility function $U ( \\pi , \\theta )$ , which depends on $\\pi$ .\n\nDifferent UED approaches vary primarily in the teacher’s utility function. Domain Randomization (DR; [53]) uniformly randomizes environment parameters, with a constant utility $U ^ { \\mathcal { U } } ( \\pi , \\theta ) = C$ , making it agnostic to the student’s policy. Minimax training [39] adversarially generates challenging levels, with utility $U ^ { \\mathcal { M } } ( \\pi , \\theta ) = - \\dot { V } ^ { \\theta } ( \\dot { \\pi } )$ , to minimize the student’s return. However, this approach incentivizes the teacher to make the levels completely unsolvable, limiting room for learning. Recent UED methods address this by using a teacher that maximizes regret, defined as the difference between the return of the optimal policy and the current policy. Regret-based utility is defined as $U ^ { \\mathcal { R } } ( \\pi , \\theta ) = \\mathrm { R E G R E T } ^ { \\theta } ( \\pi , \\pi ^ { * } ) = V ^ { \\theta } ( \\pi ^ { * } ) - V ^ { \\theta } ( \\pi )$ where $\\pi ^ { * }$ is the optimal policy on $\\theta$ . Regret-based objectives have been shown to promote the simplest levels that the student cannot solve optimally, and benefit from the theoretical guarantee of a minimax regret robust policy upon convergence in the two-player zero-sum game. However, since $\\pi ^ { * }$ is generally unknown, regret must be approximated. Dennis et al. [23], the pioneer UED work, introduced a principled level generation based on the regret objective and proposed the $P A I R E D$ algorithm, where regret is estimated by the difference between the returns attained by an antagonist agent and the protagonist (student) agent. Later on, Jiang et al. [27] introduced $P L \\dot { R } ^ { \\perp }$ which combines DR with regret using Positive Value Loss (PVL), an approximation of regret based on Generalized Advantage Estimation (GAE; [48]):\n\n$$\n\\mathrm { P V L } ^ { \\theta } ( \\pi ) = \\frac { 1 } { T } \\sum _ { t = 0 } ^ { T } \\operatorname* { m a x } \\left( \\sum _ { k = t } ^ { T } ( \\gamma \\lambda ) ^ { k - t } \\delta _ { k } ^ { \\theta } , 0 \\right) ,\n$$\n\nwhere $\\lambda$ and $T$ are the GAE discount factor and MDP horizon, respectively. $\\delta _ { k } ^ { \\theta }$ is the TD-error at time step $k$ for $\\theta$ . The state-of-the-art UED algorithm, ACCEL [37], improves $\\mathrm { P L R ^ { \\perp } }$ [27] by replacing its random level generation with an editor that mutates previously curated levels to gradually introduce complexity into the curriculum.\n\n# 3 Related Work\n\nIt is important to note that regret-based UED approaches provide a minimax regret guarantee at Nash Equilibrium; however, they provide no explicit guarantee of convergence to such equilibrium. Beukman et al. [10] demonstrated that the minimax regret objective does not necessarily align with learnability: an agent may encounter UPOMDPs with high regret on certain levels where it already performs optimally (given the partial observability constraints), while there exist other levels with lower regret where it could still improve. Consequently, selecting levels solely based on regret can lead to regret stagnation, where learning halts prematurely. This suggests that focusing exclusively on minimax regret may inhibit the exploration of levels where overall regret is non-maximal, but opportunities for acquiring transferable skills for generalization are significant. Thus, there is a compelling need for a complementary objective, such as novelty, to explicitly guide level selection towards enhancing zero-shot generalization performance and mitigating regret stagnation.\n\nThe Paired Open-Ended Trailblazer (POET; [56]) algorithm computes novelty based on environment encodings—a vector of parameters that define level configurations. POET maintains a record of the encodings from previously generated levels and computes the novelty of a new level by measuring the average distance between the k-nearest neighbors of its encoding. However, this method for computing novelty is domain-specific and relies on human expertise in designing environment encodings, posing challenges for scalability to complex domains. Moreover, due to UED’s underspecified nature, where free parameters may yield a one-to-many mapping between parameters and environments instances, each inducing distinct agent behaviors, quantifying novelty based on parameters alone is futile.\n\nEnhanced POET (EPOET; [57]) improves upon its predecessor by introducing a domain-agnostic approach to quantify a level’s novelty. EPOET is grounded in the insight that novel levels offer new insights into how the behaviors of agents within them differ. EPOET evaluates both active and archived agents’ performance in each environment, converting their performance rankings into rank-normalized vectors. The level’s novelty is then computed by measuring the Euclidean distance between these vectors. Despite addressing POET’s domain-specific limitations, EPOET encounters its own challenges. The computation of rank-normalized vectors only works for population-based approaches as it requires evaluating multiple trained student agents and incurs substantial computational costs. Furthermore, EPOET remains curriculum-agnostic, as its novelty metric relies on the ordering of raw returns within the agent population, failing to directly assess whether the environment elicits rarely observed states and actions in the existing curriculum.\n\nDiversity Induced Prioritized Level Replay (DIPLR; [33]), calculates novelty using the Wasserstein distance between occupancy distributions of agent trajectories from different levels. DIPLR maintains a level buffer and determines a level’s novelty as the minimum distance between the agent’s trajectory on the candidate level and those in the buffer. While DIPLR incorporates the agent’s experiences into its novelty calculation, it faces significant scalability and robustness issues. First, relying on the Wasserstein distance is notoriously costly. Additionally, DIPLR requires pairwise distance computations between all levels in the buffer, causing computational costs to grow exponentially with more levels. Finally, although DIPLR promotes diversity within the active buffer, it fails to evaluate whether state-action pairs in the current trajectory have already been adequately explored through past curriculum experiences, making it arguably still curriculum-agnostic. Further discussions on relevant literature can be found in Appendix B.\n\n# 4 Approach: CENIE\n\nThe limitations of prior approaches to quantifying environment novelty underscore the need for a more robust framework, motivating the development of CENIE. CENIE quantifies environment novelty through state-action space coverage derived from the agent’s accumulated experiences across previous environments in its curriculum. In single-environment online reinforcement learning, coverage within the training distribution is often linked to sample efficiency [59], providing inspiration for the CENIE framework. Given UED’s objective to enhance a student’s generalizability across a vast and often unseen (during training) environment space, quantifying novelty in terms of state-action space coverage has several benefits. By framing novelty in this way, CENIE enables a sample-efficient exploration of the environment search space by prioritizing levels that drive the agent towards unfamiliar state-action combinations. This provides a principled basis for directing the environment design towards enhancing the generalizability of the student agent. Additionally, a distinctive benefit of this approach is that it is not confined to any particular UED or DRL algorithms since it solely involves modeling the agent’s state-action space coverage. This flexibility allows us to implement CENIE atop any UED algorithm.\n\nCENIE’s approach to novelty quantification through state-action coverage introduces three key attributes, effectively addressing the limitations of previous methods: (1) domain-agnostic, (2) curriculum-aware, and (3) scalable. CENIE is domain-agnostic, as it quantifies novelty solely based on the state-action pairs of the student, thus eliminating any dependency on the encoding of the environment. CENIE achieves curriculum-awareness by quantifying novelty using a model of the student’s past aggregated experiences, i.e., state-action space coverage, ensuring that the selection of environments throughout the curriculum is sample-efficient with regards to expanding the student’s state-action coverage. Lastly, CENIE demonstrates scalability by avoiding the computational burden associated with exhaustive pairwise comparisons or costly distance metrics.\n\n![](images/b91c32187dc2cd8107c5ce121fb9a339d8dac0c84093f6cde30757f9382678d5.jpg)  \nFigure 1: An overview of the CENIE framework. The teacher will utilise environment regret and novelty for curating student’s curriculum. $\\Gamma$ contains past experiences and $\\tau$ is the recent trajectory.\n\n# 4.1 Measuring the Novelty of a Level\n\nTo evaluate the novelty of new environments using the agent’s state-action pairs, the teacher needs to first model the student’s past state-action space coverage distribution. We propose to use GMMs as they are particularly effective due to their robustness in representing high-dimensional continuous distributions [14, 5]. A GMM is a probabilistic clustering model that represents the underlying distribution of data points using a weighted combination of multivariate Gaussian components. Once the state-action distribution is modeled using a GMM, we can leverage it for density estimation. Specifically, the GMM allows us to evaluate the likelihood of state-action pairs induced by new environments, where lower likelihoods indicate experiences that are less represented in the student’s current state-action space. This likelihood provides a quantitative measure of dissimilarity in stateaction space coverage, enabling a direct comparison of novelty between levels. It is important to note that CENIE defines a general framework for quantifying novelty through state-action space coverage;\n\nGMMs represent just one possible method for modeling this coverage. Future research may explore alternatives to model state-action space coverage within the CENIE framework (see Section $\\mathrm { ~ \\cal ~ C ~ }$ in the appendix for more discussions).\n\nBefore detailing our approach, we first define the notations used in this section. Let $l _ { \\theta }$ be a particular level conditioned by an environment parameter $\\theta$ . We refer to $l _ { \\theta }$ as the candidate level, for which we aim to determine its novelty. The agent’s trajectory on $l _ { \\theta }$ is denoted as $\\tau _ { \\theta }$ , and can be decomposed into a set of sample points, represented as $X _ { \\theta } = \\{ x = ( s , a ) \\sim \\tau _ { \\theta } \\}$ . The set of past training levels is represented by $L$ and $\\Gamma = \\{ \\bar { x } = ( s , a ) \\sim \\tau _ { L } \\}$ is a buffer containing the state-action pairs collected from levels across $L$ . We treat $\\Gamma$ as the ground truth of the agent’s state-action space coverage, against which we evaluate the novelty of state-action pairs from the candidate level $X _ { \\theta }$ .\n\nTo fit a GMM on $\\Gamma$ , we must find a set of Gaussian mixture parameters, denoted as $\\lambda _ { \\Gamma } \\ =$ $\\{ ( \\alpha _ { 1 } , \\mu _ { 1 } , \\Sigma _ { 1 } ) , . . . , ( \\alpha _ { K } , \\mu _ { K } , \\Sigma _ { K } ) \\}$ , that best represents the underlying distribution. Here, $K$ denotes the predefined number of Gaussians in the mixture, where each Gaussian component is characterized by its weight $( \\alpha _ { k } )$ , mean vector $( \\mu _ { k } )$ , and covariance matrix $( \\Sigma _ { k } )$ , with $\\bar { k } \\in \\{ 1 , . . . , K \\}$ . We employ the kmeans $^ { + + }$ algorithm [12, 4] for a fast and efficient initialization of $\\lambda _ { \\Gamma }$ . The likelihood of observing $\\Gamma$ given the initial GMM parameters $\\lambda _ { \\Gamma }$ is expressed as:\n\n$$\nP ( \\Gamma \\mid \\lambda _ { \\Gamma } ) = \\prod _ { j = 1 } ^ { J } \\sum _ { k = 1 } ^ { K } \\alpha _ { k } { \\mathcal { N } } ( x _ { j } \\mid \\mu _ { k } , \\Sigma _ { k } )\n$$\n\nwhere $x _ { j }$ is a state-action pair sample from $\\Gamma$ . $\\mathcal { N } ( x _ { j } \\mid \\mu _ { k } , \\Sigma _ { k } )$ represents the multi-dimensional Gaussian density function for the $k$ -th component with mean vector $\\mu _ { k }$ and covariance matrix $\\Sigma _ { k }$ . To optimise $\\lambda _ { \\Gamma }$ , we use the Expectation Maximization (EM) algorithm [22, 43] because Eq. 2 is a non-linear function of $\\lambda _ { \\Gamma }$ , making direct maximization infeasible. The EM algorithm iteratively refines the initial $\\lambda _ { \\Gamma }$ to estimate a new $\\lambda _ { \\Gamma } ^ { \\prime }$ such that $P ( X \\mid \\lambda _ { \\Gamma } ^ { \\prime } ) > P ( X \\mid \\lambda _ { \\Gamma } \\bar { ) }$ . This process is repeated iteratively until some convergence, i.e., $\\| \\lambda _ { \\Gamma } ^ { \\prime } - \\lambda _ { \\Gamma } \\| < \\bar { \\epsilon }$ , where $\\epsilon$ is a small threshold.\n\nOnce the GMM is fitted, we can use $\\lambda _ { \\Gamma }$ to perform density estimation and calculate the novelty of the candidate level $l _ { \\theta }$ . Specifically, we consider the set of state-action pairs from the agent’s trajectory, $X _ { \\theta }$ , and compute their posterior likelihood under the GMM. This likelihood indicates how similar the new state-action pairs are to the learned distribution of past state-action coverage. Consequently, the novelty score of $l _ { \\theta }$ is represented as follows:\n\n$$\n\\operatorname { N o v e l } \\mathrm { t y } _ { l _ { \\theta } } = - { \\frac { 1 } { | X _ { \\theta } | } } \\log { \\mathcal { L } } ( X _ { \\theta } \\mid \\lambda _ { \\Gamma } ) = - { \\frac { 1 } { | X _ { \\theta } | } } \\sum _ { t = 1 } ^ { T } \\log p ( x _ { t } \\mid \\lambda _ { \\Gamma } )\n$$\n\nwhere $\\boldsymbol { x } _ { t }$ is a sample state-action pair from $X _ { \\theta }$ . As shown in Eq. 3, we take the negative mean log-likelihood across all samples in $X _ { \\theta }$ to attribute higher novelty scores to levels with state-action pairs that are less likely to originate from the aggregated past experiences, $\\Gamma$ . This novelty metric promotes candidate levels that induce more novel experiences for the agent during training. More details on fitting GMMs are explained in Appendix D.1.\n\nDesign considerations for the GMM First, we specifically designate the state-action coverage buffer, i.e., $\\Gamma$ , as a First-In-First-Out (FIFO) buffer with a fixed window length. By focusing on a fixed window rather than the entire history of state-action pairs, our novelty metric avoids bias toward experiences that are outdated and have not appeared in recent trajectories. This design choice helps reduce the effects of catastrophic forgetting prevalent in DRL. Next, it is known that by allowing the adaptation of the number of Gaussians in the mixture, i.e., $K$ in Eq. 2, any smooth density distribution can be approximated arbitrarily close [24]. Therefore, to optimize the GMM’s representation of the agent’s state-action coverage distribution, we fit multiple GMMs with varying numbers of Gaussians within a predefined range at each time step and select the best one based on the silhouette score [45], an approach inspired by Portelas et al. [40]. The silhouette score evaluates clustering quality by measuring both intra-cluster cohesion and inter-cluster separation. This approach enables CENIE to construct a pseudo-online GMM model that dynamically adjusts its complexity to accommodate the agent’s changing state-action coverage distribution.\n\n# 4.2 State-Action Space Coverage Directed Training Agent\n\n# Algorithm 1 ACCEL-CENIE\n\nInput: Level buffer size $N$ , Component range $[ K _ { \\operatorname* { m i n } } , K _ { \\operatorname* { m a x } } ]$ , FIFO window size $\\mathcal { W }$ , level generator $\\mathcal { G }$ Initialize: Student policy $\\pi _ { \\eta }$ , level buffer $\\boldsymbol { B }$ , state-action buffer $\\Gamma$ , GMM parameters $\\lambda _ { \\Gamma }$\n\n1: Generate $N$ initial levels by $\\mathcal { G }$ to populate $\\boldsymbol { B }$   \n2: Collect $\\pi _ { \\eta }$ ’s trajectories on each level in $\\boldsymbol { B }$ and fill up $\\Gamma$   \n3: while not converged do   \n4: Sample replay decision, $\\epsilon \\sim U [ 0 , 1 ]$   \n5: if $\\epsilon \\geq 0 . 5$ then   \n6: Generate a new level $l _ { \\theta }$ by $\\mathcal { G }$   \n7: Collect trajectories $\\tau$ on $l _ { \\theta }$ , with stop-gradient $\\eta _ { \\perp }$   \n8: Compute novelty score for $l _ { \\theta }$ using $\\lambda _ { \\Gamma }$ (Eq.3 and Eq.4)   \n9: Compute regret score for $l _ { \\theta } ^ { \\prime }$ (Eq.1 and Eq.4)   \n10: Update $\\boldsymbol { B }$ with $l _ { \\theta }$ if $\\check { P } _ { r e p l a y } ( l _ { \\theta } )$ is greater than that of any levels in $\\boldsymbol { B }$ (Eq.5)   \n11: else   \n12: Sample a replay level $l _ { \\theta } \\sim B$ according to $P _ { r e p l a y }$   \n13: Collect trajectories $\\tau$ on $l _ { \\theta }$   \n14: Update $\\pi _ { \\eta }$ with rewards $R ( \\tau )$   \n15: Update $\\Gamma$ with $\\tau$ and resize to $\\mathcal { W }$   \n16: for $k$ in range $K _ { \\mathrm { m i n } }$ to $K _ { \\mathrm { m a x } }$ do   \n17: Fit a ${ \\bf G M M } _ { k }$ with $k$ components on $\\Gamma$ and compute its silhouette score   \n18: end for   \n19: Select GMM parameters with the highest silhouette score to replace $\\lambda _ { \\Gamma }$   \n20: Perform edits on $l _ { \\theta }$ to produce $l _ { \\theta } ^ { \\prime }$   \n21: Collect trajectories $\\tau$ on $l _ { \\theta } ^ { \\prime }$ , with stop-gradient $\\eta _ { \\perp }$   \n22: Compute novelty score for $l _ { \\theta } ^ { \\prime }$ using $\\lambda _ { \\Gamma }$ (Eq.3 and Eq.4)   \n23: Compute regret score for $l _ { \\theta } ^ { \\prime }$ (Eq.1 and Eq.4)   \n24: Update $\\boldsymbol { B }$ with $l _ { \\theta } ^ { \\prime }$ if $\\bar { P } _ { r e p l a y } ( l _ { \\theta } ^ { \\prime } )$ is greater than that of any levels in $\\boldsymbol { B }$ (Eq.5)   \n25: end if   \n26: end while\n\nWith a scalable method to quantify the novelty of levels, we demonstrate its versatility and effectiveness by deploying it on top of the leading UED algorithms, $\\mathrm { P L R ^ { \\perp } }$ and ACCEL. For convenience, in subsequent sections, we will refer to this CENIE-augmented methodology of $\\mathrm { P L R ^ { \\perp } }$ and ACCEL using GMMs as PLR-CENIE and ACCEL-CENIE, respectively. Both $\\mathrm { P L R ^ { \\perp } }$ and ACCEL utilize a replay mechanism to train their students on the highest-regret levels curated within the level buffer. To integrate CENIE within these algorithms, we use normalized outputs of a prioritization function to convert the level scores (novelty and regret) into level replay probabilities $( P _ { S } )$ :\n\n$$\nP _ { S } = \\frac { h ( S _ { i } ) ^ { \\beta } } { \\sum _ { j } h ( S _ { j } ) ^ { \\beta } }\n$$\n\nwhere $h$ is a prioritization function (e.g. rank) with a tunable temperature $\\beta$ that defines the prioritization of levels with regards to any arbitrary score $S$ . Following the implementations in $\\mathrm { P L } \\dot { \\mathrm { R } } ^ { \\perp }$ and ACCEL, we employ $h$ as the rank prioritization function, i.e., $\\bar { h ( S _ { i } ) } = \\bar { 1 } / { \\operatorname { r a n k } ( S _ { i } ) }$ , where rank $( S _ { i } )$ is the rank of level score $S _ { i }$ among all scores sorted in descending order. In ACCEL-CENIE and PLR-CENIE, we use both the novelty and regret scores to determine the level replay probability:\n\n$$\nP _ { r e p l a y } = \\alpha \\cdot P _ { N } + ( 1 - \\alpha ) \\cdot P _ { R }\n$$\n\nwhere $P _ { N }$ and $P _ { R }$ are the novelty-prioritized probability and regret-prioritized probability respectively, and $\\alpha$ allows us to adjust the weightage of each probability. The complete procedures for ACCELCENIE are provided in Algorithm 1, and for PLR-CENIE in the appendix (see Algorithm 2). Key steps specific to CENIE using GMMs are highlighted in blue.\n\n# 5 Experiments\n\nIn this section, we benchmark PLR-CENIE and ACCEL-CENIE against their predecessors and a set of baseline algorithms: Domain Randomization (DR), Minimax, PAIRED, and DIPLR. The technical details of each algorithm are presented in Appendix E. We empirically demonstrated the effectiveness of CENIE on three distinct domains: Minigrid, BipedalWalker, and CarRacing. Minigrid is a partially observable navigation task under discrete control with sparse rewards, while BipedalWalker and CarRacing are partially observable continuous control tasks with dense rewards. Due to the complexity of mutating racing tracks, CarRacing is the only domain where ACCEL and ACCEL-CENIE are excluded. The experiment details are provided in Appendix D. Following standard UED practices, all agents were trained using Proximal Policy Optimization (PPO; [49]) across the domains, and we present their zero-shot performance on held-out tasks. We also conducted ablation studies to assess the isolated effectiveness of CENIE’s novelty metric (see Appendix A).\n\nFor reliable comparison, we employ the standardized DRL evaluation metrics [1], presenting the aggregate inter-quartile mean (IQM) and optimality gap plots after normalizing the performance with a min-max range of solved-rate/returns. Specifically, IQM focuses on the middle $50 \\%$ of combined runs, discarding the bottom and top $2 5 \\%$ , thereby providing a robust measure of overall performance. Optimality gap captures the amount by which the algorithm fails to meet a desirable target (e.g., $9 5 \\%$ solved rate), beyond which further improvements are considered unimportant. Higher IQM and lower optimality gap scores are better. The hyperparameters for the algorithms in each experiment are specified in the appendix.\n\n# 5.1 Minigrid Domain\n\nFirst, we validated the CENIE-augmented methods in Minigrid [23, 20], a popular benchmark due to its ease of interpretability and customizability. Given its sparse reward signals and partial observability, navigating Minigrid requires the agent to explore multiple possible paths before successfully solving the maze and receiving rewards for policy updates. Therefore, Minigrid is an ideal domain to validate the exploration capabilities of the CENIE-augmented algorithms.\n\nSixteenRooms SixteenRooms2 Labyrinth Labyrinth2 Maze Maze2 PerfectMaze LargeCorridor楼回尚 跨影GADR Minimax PAIRED DIPLR PLR⊥ PLR-CENIE ACCEL ACCEL-CENIE\n\nFigure 2: Zero-shot transfer performance in eight human-designed test environments. The plots are based on the median and interquartile range of solved rates across 5 independent runs.\n\nFollowing prior UED works, we train all student agents for 30k PPO updates (approximately 250 million steps) and evaluate their generalization on eight held-out environments (see Figure 2). Figure 2 demonstrates that ACCEL-CENIE outperforms ACCEL in all testing environments. Moreover, PLR-CENIE shows significantly better performance in seven test environments compared to $\\mathrm { P L R ^ { \\perp } }$ . This underscores the ability of CENIE’s novelty metric to complement the UED framework, particularly in improving generalization performance beyond the conventional learning potential metric, regret. Further empirical validation in Figure 3(a) confirms ACCEL-CENIE’s superiority over ACCEL in both IQM and optimality gap. PLR-CENIE also outperforms its predecessor, $\\mathrm { P L R ^ { \\perp } }$ , by a significant margin. Notably, PLR-CENIE’s performance is able to match ACCEL’s, which is significant considering PLR-CENIE uses a random generator while ACCEL uses an editing mechanism to introduce gradual complexity to environments.\n\nBeyond the normal-size testing mazes, we consider a more challenging evaluation setting. We evaluate the fully-trained student policy of $\\mathrm { P L R ^ { \\perp } }$ , PLR-CENIE, ACCEL, and ACCEL-CENIE on PerfectMazeLarge (shown in Figure 3(b)), an out-of-distribution environment which has $5 1 \\times 5 1$ tiles and a episode length of 5000 timesteps – a much larger scale than training levels. We evaluate the agents for 100 episodes (per seed), using the same checkpoints in Figure 2. ACCEL-CENIE and ACCEL achieved comparable zero-shot transfer performance. Notably, PLR-CENIE achieved close to $50 \\%$ solved rate, matching ACCEL’s performance. This is a significant improvement from $\\mathrm { P L R ^ { \\perp } }$ , which had less than a $10 \\%$ solved rate.\n\n![](images/f92ddd1f277152758c2c5ce4b39f54979f363450491c97a413abc866834c442e.jpg)  \nFigure 3: (a) Aggregate zero-shot transfer performance in Minigrid domain across 5 independent runs. (b) Zero-shot test performance of $\\mathrm { \\hat { P L R ^ { \\perp } } }$ , PLR-CENIE, ACCEL, and ACCEL-CENIE on PerfectMazeLarge across 5 independent runs.\n\n# 5.2 BipedalWalker Domain\n\n![](images/b992fa593c7de304c953316e9e01da740a696915d58ec8df48c8484835e2c6fb.jpg)  \nFigure 4: Student’s generalization performance on 6 BipedalWalker testing environments during training. Each curve is measured across 5 independent runs (mean and standard error).\n\nWe also evaluated the CENIE-augmented algorithms in the BipedalWalker domain [56, 37], which is a partially observable continuous domain with dense rewards. We train all the algorithms for 30k PPO updates ( $\\mathord { \\sim } 1 \\mathrm { B }$ timesteps) and then evaluate their generalization performance on six distinct test environments: BipedalWalker (default), Hardcore, Stair, PitGap, Stump, and Roughness (visualized in Figure 6(a)). To monitor the student’s generalization performance evolution, we assess the student policy every 100 PPO updates across six testing environments during the training period.\n\nIn Figure 4, ACCEL-CENIE outperforms ACCEL in five testing environments, with both achieving parity in the Roughness challenge, establishing ACCEL-CENIE as the leading UED algorithm in BipedalWalker. Similarly, PLR-CENIE consistently outperforms $\\mathrm { P L R ^ { \\perp } }$ across all testing instances, except for the Stump challenge, where both algorithms exhibit similar performance. We present the aggregate results after min-max normalization (with range=[0, 300] on all test environments) in Figure 6(b). Both ACCEL-CENIE and PLR-CENIE exhibit better performance compared to their predecessors in the IQM and optimality gap metrics. Notably, ACCEL-CENIE outperforms all benchmarks by a substantial margin, achieving close to $5 5 \\%$ of optimal performance.\n\nTable 1: Coverage of state-action space across $3 0 \\mathrm { k }$ PPO updates in the BipedalWalker domain.   \n\n<html><body><table><tr><td></td><td>PLR-</td><td>PLR-CENIE</td><td>ACCEL</td><td>ACCEL-CENIE</td></tr><tr><td>State-action Space Coverage</td><td>43.4%</td><td>55.3%</td><td>42.5%</td><td>47.6%</td></tr></table></body></html>\n\nNext, we tracked the evolution of state-action space coverage throughout training to evaluate the impact of CENIE’s novelty objective on the curriculum’s exploration of the state-action space. During training, state-action pairs encountered by the agent were collected for both $\\mathrm { P L R ^ { \\perp } }$ and ACCEL, along with their CENIE-augmented versions. To visualize the distribution of these high-dimensional state-action pairs, we applied t-distributed Stochastic Neighbor Embedding (t-SNE; [54]) to project them into a 2-D space. The resulting evolution plot and detailed implementation steps are provided in Appendix A.2. Afterwards, we quantified state-action space coverage by discretizing the 2-D scatterplot into cells and calculating the percentage of total cells occupied by each algorithm. As shown in Table 1, CENIE drives ACCEL-CENIE and PLR-CENIE to achieve significantly broader state-action coverage by the end of 30k PPO updates compared to their predecessors. This evidence supports that the inclusion of CENIE’s novelty objective for level replay prioritization contributes to broader state-action space coverage.\n\n![](images/33f6c4d6363615030f8b0dd0e033366c58dc37b08ed0c70439ef4afae6923662.jpg)  \nFigure 5: Difficulty composition of levels replayed by ACCEL and ACCEL-CENIE during training.\n\nTo understand ACCEL-CENIE’s improvement over ACCEL, we analyzed the difficulty composition of replayed levels at various training intervals across five seeds, as shown in Figure 5. Level difficulty is assessed based on environment parameters such as stump height and pit gap width, using metrics adapted from Wang et al. [56] (details in Appendix A.2). It is evident that ACCEL predominantly favors “Easy” to “Moderate” difficulty levels, whereas ACCEL-CENIE progressively incorporates “Challenging” levels into its replay selection throughout training.\n\nThe disparity in level difficulty distribution between ACCEL and ACCEL-CENIE is a critical factor in understanding their observed performance differences. ACCEL’s training curriculum tends to remain within a comfort zone, consistently selecting a limited subset of simpler levels where the agent experiences high regret. However, this can be problematic when considering the regret stagnation problem. Specifically, in the event where the easier levels exhibit irreducible regret, it can restrict the agent’s exposure to more complex scenarios, thereby constraining its generalization potential. In contrast, ACCEL-CENIE’s integration of a novelty objective actively selects challenging levels, pushing the agent beyond its comfort zone into unfamiliar, complex environments. This novelty-based regularization fosters the exploration of under-explored regions in the state-action space, even if regret levels are low, thereby enhancing the agent’s generalization capabilities. Furthermore, with a mutation-based approach like ACCEL, this environment selection strategy may generate or mutate new levels with high learning potential, further enriching the training curriculum.\n\n![](images/8d13dda82c8a849f73fbb1d8f0fe81eb9fe9959669345060ab2302e70b18b769.jpg)  \nFigure 6: (a) BipedalWalker domain and (b) Aggregate zero-shot transfer performance in BipedalWalker.\n\n# 5.3 CarRacing Domain\n\nFinally, we evaluated the effectiveness of CENIE by implementing it on $\\mathrm { P L R ^ { \\perp } }$ within the CarRacing domain [16, 27]. In this domain, the teacher manipulates the curvature of racing tracks using Bézier curves defined by a sequence of 12 control points, while the student drives on the track under continuous control with dense rewards. We train the students in each algorithm for $2 . 7 5 \\mathrm { k }$ PPO updates ${ \\bf \\omega } _ { \\bf \\omega } \\sim 5 . 5 { \\bf M }$ steps), after which we test the zero-shot transfer performance of the different algorithms on 20 levels replicating real-world Formula One (F1) tracks introduced by Jiang et al. [27]. These tracks are guaranteed to be OOD as their configuration cannot be defined by Bézier curves with only 12 control points. The middle image in Figure 6b shows a track generated by domain randomization and the rightmost image shows a bird’s-eye view of the F1-USA benchmark track.\n\n![](images/0582ad8d13cdff3ec9fec96f9361bc63104571fef1389535daf359538c51558d.jpg)  \nFigure 7: (a) CarRacing domain and (b) Aggregate zero-shot transfer performance in CarRacing.\n\nThe aggregate performance after min-max normalization of all algorithms is summarized in Figure 7b. Note that the min-max range varies across F1 tracks due to different specifications on the maximum episode steps (see Table 5 in the appendix for more details). Once again, the CENIE-augmented algorithm, PLR-CENIE, achieves the best generalization performance in both IQM and optimality gap scores. Table 3 in the appendix shows the zero-shot transfer returns on all $2 0 \\mathrm { F } 1$ tracks. PLR-CENIE consistently outperforms or matches the best-performing baseline on all tracks.\n\nFigure 8 presents the total regret in the level replay buffer for both $\\mathrm { \\dot { P L R ^ { \\perp } } }$ and PLR-CENIE throughout the training process. Interestingly, PLR-CENIE maintains comparable, or even slightly higher, levels of regret across the training distribution, despite not directly optimizing for it. This outcome suggests that CENIE’s novelty objective synergizes with the discovery of high-regret levels, providing counterintuitive evidence that optimizing solely for regret is not the only, nor necessarily the most effective, strategy for identifying levels with high learning potential (as approximated by regret). Intuitively, value predictions are inherently less reliable in regions of lower coverage density–areas characterized by higher entropy or high uncertainty regarding optimal actions–since these regions are less frequently sampled for agent’s learning. These high-entropy regions are prime candidates for high-regret outcomes, especially when using a bootstrapped regret estimate, as in Eq. 1, due to the value estimation error in such states. By pursuing novel environments based on coverage, CENIE indirectly enhances the discovery of high-regret states, highlighting that noveltydriven autocurricula can effectively complement regret-based methods in uncovering diverse and challenging training scenarios.\n\n![](images/0e2653e43ef673c2a0aebd4207adb53a5d2a3a42658b5df93dae81bfc5aac559.jpg)  \nFigure 8: Total regret in level replay buffer for $\\mathrm { P L R ^ { \\perp } }$ and PLR-CENIE over training in CarRacing.\n\n# 6 Conclusion\n\nIn this paper, we introduced Coverage-based Evaluation of Novelty In Environment (CENIE), a scalable, domain-agnostic, and curriculum-aware framework for quantifying environment novelty in UED. We then proposed an implementation of CENIE that models this coverage and measures environment novelty using Gaussian Mixture Models. By incorporating CENIE with existing UED algorithms, we validated the framework’s effectiveness in enhancing agent exploration capabilities and zero-shot transfer performance across three distinct benchmark domains. This promising approach marks a significant step towards unifying novelty-driven exploration and regret-driven exploitation for training generally capable RL agents. We encourage motivated readers to refer to the appendix for further studies and discussions on CENIE.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决无监督环境设计（UED）中现有方法主要依赖遗憾度来设计课程，而忽略环境新颖性的问题。环境新颖性是提升智能体泛化能力的关键要素，但由于UED中环境参数的不确定性，测量环境新颖性具有挑战性，现有方法存在显著局限性。这一问题的重要性在于，仅关注遗憾度会导致课程缺乏多样性，限制智能体在大规模环境设计空间中的泛化能力，因此需要引入新颖性这一互补目标来指导课程设计，以增强零样本泛化性能和缓解遗憾度停滞问题。\\n\\n**方法概述**\\n论文提出了基于覆盖度的环境新颖性评估（CENIE）框架，利用学生在先前课程经验中的状态 - 动作空间覆盖度，以可扩展、与领域无关且考虑课程的方式量化环境新颖性。还给出了使用高斯混合模型（GMM）的CENIE实现，并将新颖性目标与现有领先的UED算法（$PLR^{\\perp}$ 和ACCEL）相结合。\\n\\n**主要贡献与效果**\\n- 提出CENIE框架，解决了现有环境新颖性量化方法的不足。通过在三个不同的基准领域进行评估，验证了CENIE增强的算法能够使学生智能体在状态 - 动作空间中探索更广泛的场景，有助于实现最先进的零样本泛化性能。例如，在Minigrid领域，ACCEL - CENIE在八个测试环境中均优于ACCEL；PLR - CENIE在七个环境中显著优于$PLR^{\\perp}$，在PerfectMazeLarge分布外环境中，PLR - CENIE达到接近50%的解决率，远高于$PLR^{\\perp}$低于10%的解决率。\\n- 在BipedalWalker领域，ACCEL - CENIE在五个测试环境中优于ACCEL，PLR - CENIE在除Stump挑战外的所有测试实例中均优于$PLR^{\\perp}$。ACCEL - CENIE在聚合结果中接近55%的最优性能。此外，PLR - CENIE的状态 - 动作空间覆盖度达到55.3%，高于$PLR^{\\perp}$的43.4%；ACCEL - CENIE的状态 - 动作空间覆盖度为47.6%，高于ACCEL的42.5%。\\n- 在CarRacing领域，PLR - CENIE在IQM和最优差距得分方面均取得最佳泛化性能，在所有20条F1赛道上始终优于或与表现最佳的基线模型相匹配，且在训练过程中保持了与$PLR^{\\perp}$相当甚至略高的遗憾度水平。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nCENIE框架的核心思想基于新颖环境应使智能体产生陌生体验，将其推向状态空间的未探索区域并引入动作变化的直觉。通过利用学生在先前课程经验中的状态 - 动作空间覆盖度，建模和比较新环境与课程中已遇环境的状态 - 动作空间覆盖度，得到环境新颖性的信号。以智能体的过去经验为基础评估新颖性，可确保课程中的关卡能有效地引导智能体获得新颖体验，因为这样能避免将与现有环境体验相似的冗余环境归类为新颖环境。\\n\\n**创新点**\\n先前的UED方法主要依赖遗憾度来设计课程，忽略了环境新颖性对智能体泛化能力的重要性。一些新颖性量化方法存在局限性，如POET方法是特定领域的，依赖人工专业知识设计环境编码，难以扩展到复杂领域；EPOET虽然解决了领域特定的问题，但计算成本高，且与课程无关，无法直接评估环境是否能在现有课程中引发罕见的状态和动作；DIPLR在新颖性计算中考虑了智能体的经验，但存在可扩展性和鲁棒性问题，且仍然与课程无关。CENIE的创新之处在于它具有领域无关性，仅基于学生的状态 - 动作对来量化新颖性，消除了对环境编码的依赖；具有课程感知性，通过建模学生过去的聚合经验来量化新颖性，确保课程选择的环境能有效地扩展学生的状态 - 动作覆盖度；具有可扩展性，避免了昂贵的成对比较和距离度量带来的计算负担。此外，CENIE通过设计固定窗口长度的先进先出（FIFO）缓冲区和动态调整GMM复杂度的方式，减少了DRL中灾难性遗忘的影响，并能更好地适应智能体不断变化的状态 - 动作覆盖度分布。\\n\\n**具体实现步骤**\\n1. **测量关卡新颖性**：\\n    - 首先，使用高斯混合模型（GMM）对学生的过去状态 - 动作空间覆盖度分布进行建模。将过去训练关卡的状态 - 动作对集合$\\Gamma$作为真实的状态 - 动作空间覆盖度，使用kmeans++算法对GMM的参数$\\lambda_{\\Gamma}$进行初始化。\\n    - 计算$\\Gamma$在初始GMM参数下的似然度$P(\\Gamma|\\lambda_{\\Gamma})$，并使用期望最大化（EM）算法优化$\\lambda_{\\Gamma}$，直到收敛，即$\\| \\lambda_{\\Gamma}' - \\lambda_{\\Gamma} \\| < \\bar{\\epsilon}$，其中$\\epsilon$是一个小阈值。\\n    - 对于候选关卡$l_{\\theta}$，计算其状态 - 动作对集合$X_{\\theta}$在GMM下的后验似然度，根据公式$Novelty_{l_{\\theta}} = - \\frac{1}{|X_{\\theta}|} \\log \\mathcal{L}(X_{\\theta}|\\lambda_{\\Gamma})$计算其新颖性得分，得分越高表示该关卡的新颖性越高。\\n    - 设计考虑：将状态 - 动作覆盖度缓冲区$\\Gamma$设计为固定窗口长度的先进先出（FIFO）缓冲区，避免对过时经验的偏见，减少DRL中灾难性遗忘的影响。在每个时间步，拟合多个具有不同高斯数量的GMM，并根据轮廓系数选择最佳的GMM参数，以动态调整模型复杂度，使模型能更好地适应智能体不断变化的状态 - 动作覆盖度分布。\\n2. **状态 - 动作空间覆盖度导向的训练智能体（以ACCEL - CENIE为例）**：\\n    - 输入：关卡缓冲区大小$N$、组件范围$[K_{min}, K_{max}]$、FIFO窗口大小$\\mathcal{W}$、关卡生成器$\\mathcal{G}$。\\n    - 初始化：学生策略$\\pi_{\\eta}$、关卡缓冲区$\\boldsymbol{B}$、状态 - 动作缓冲区$\\Gamma$、GMM参数$\\lambda_{\\Gamma}$。\\n    - 生成$N$个初始关卡填充关卡缓冲区$\\boldsymbol{B}$，并收集学生策略$\\pi_{\\eta}$在每个关卡上的轨迹填充状态 - 动作缓冲区$\\Gamma$。\\n    - 进入循环，直到收敛：\\n        - 采样回放决策$\\epsilon$，如果$\\epsilon \\geq 0.5$：\\n            - 生成一个新关卡$l_{\\theta}$，收集其轨迹$\\tau$，计算其新颖性得分和遗憾度得分。\\n            - 如果新关卡的回放概率大于关卡缓冲区中的任何关卡，则更新关卡缓冲区。\\n        - 否则：\\n            - 从关卡缓冲区中采样一个回放关卡$l_{\\theta}$，收集其轨迹$\\tau$，更新学生策略$\\pi_{\\eta}$和状态 - 动作缓冲区$\\Gamma$。\\n            - 拟合多个具有不同高斯数量的GMM，选择轮廓系数最高的GMM参数替换$\\lambda_{\\Gamma}$。\\n            - 对回放关卡进行编辑，生成新关卡$l_{\\theta}'$，收集其轨迹$\\tau$，计算其新颖性得分和遗憾度得分。\\n            - 如果新关卡的回放概率大于关卡缓冲区中的任何关卡，则更新关卡缓冲区。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括：Domain Randomization (DR)、Minimax、PAIRED、DIPLR、$PLR^{\\perp}$ 和ACCEL。\\n\\n**性能对比**\\n*   **在Minigrid领域**：\\n    - **在解决率指标上**：ACCEL - CENIE在八个测试环境中均优于ACCEL；PLR - CENIE在七个测试环境中显著优于$PLR^{\\perp}$，并且其性能能够与使用编辑机制引入环境复杂度的ACCEL相匹配。在PerfectMazeLarge这个分布外环境中，PLR - CENIE达到了接近50%的解决率，远高于$PLR^{\\perp}$低于10%的解决率。\\n    - **在聚合零样本转移性能指标（IQM和最优差距）上**：ACCEL - CENIE在IQM和最优差距方面均优于ACCEL；PLR - CENIE也显著优于其前身$PLR^{\\perp}$。\\n*   **在BipedalWalker领域**：\\n    - **在测试环境的泛化性能指标上**：ACCEL - CENIE在五个测试环境中优于ACCEL，在Roughness挑战中两者表现相当；PLR - CENIE在除Stump挑战外的所有测试实例中均优于$PLR^{\\perp}$。\\n    - **在聚合零样本转移性能指标（IQM和最优差距）上**：ACCEL - CENIE和PLR - CENIE在IQM和最优差距指标上均优于其前身。ACCEL - CENIE的性能接近55%的最优性能，大幅优于所有基准模型。\\n    - **在状态 - 动作空间覆盖度指标上**：PLR - CENIE的状态 - 动作空间覆盖度为55.3%，高出$PLR^{\\perp}$的43.4%达11.9个百分点；ACCEL - CENIE的状态 - 动作空间覆盖度为47.6%，高出ACCEL的42.5%达5.1个百分点。\\n*   **在CarRacing领域**：\\n    - **在聚合零样本转移性能指标（IQM和最优差距）上**：PLR - CENIE在IQM和最优差距得分方面均取得了最佳泛化性能，优于所有基线模型。\\n    - **在零样本转移回报指标上**：PLR - CENIE在所有20条F1赛道上始终优于或与表现最佳的基线模型相匹配。\\n    - **在总遗憾度指标上**：PLR - CENIE在训练过程中保持了与$PLR^{\\perp}$相当甚至略高的遗憾度水平，表明CENIE的新颖性目标与发现高遗憾度关卡具有协同作用。\",\n    \"keywords\": \"### 关键词\\n\\n- 无监督环境设计 (Unsupervised Environment Design, UED)\\n- 环境新颖性量化 (Environment Novelty Quantification, N/A)\\n- 基于覆盖的环境新颖性评估 (Coverage - based Evaluation of Novelty In Environment, CENIE)\\n- 高斯混合模型 (Gaussian Mixture Models, GMM)\\n- 零样本泛化 (Zero - shot Generalization, N/A)\\n- 状态 - 动作空间覆盖度 (State - Action Space Coverage, N/A)\"\n}"
}