{
    "link": "https://arxiv.org/abs/2412.05823",
    "pdf_link": "https://arxiv.org/pdf/2412.05823",
    "title": "DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices",
    "authors": [
        "Yongzhe Jia",
        "Xuyun Zhang",
        "Hongsheng Hu",
        "Kim-Kwang Raymond Choo",
        "Lianyong Qi",
        "Xiaolong Xu",
        "Amin Beheshti",
        "Wanchun Dou"
    ],
    "institutions": [
        "State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University",
        "School of Computing, Macquarie University",
        "School of Information and Physical Sciences, University of Newcastle",
        "Department of Information Systems and Cyber Security, University of Texas at San Antonio",
        "College of Computer Science and Technology, China University of Petroleum (East China)",
        "School of Computer and Software, Nanjing University of Information Science and Technology"
    ],
    "publication_date": "2024-12-08",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "paper_content": "# DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices\n\nYongzhe $\\mathbf { J i a } ^ { 1 }$ , Xuyun Zhang2, Hongsheng $\\mathbf { H } \\mathbf { u } ^ { 3 }$ , Kim-Kwang Raymond Choo4, Lianyong $\\mathbf { Q } \\mathbf { i } ^ { 5 }$ , Xiaolong $\\mathbf { X } \\mathbf { u } ^ { 6 }$ ,∗Amin Beheshti2, Wanchun Dou1 1 State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, China 2 School of Computing, Macquarie University, Australia 3 School of Information and Physical Sciences, University of Newcastle, Australia   \n4 Department of Information Systems and Cyber Security, University of Texas at San Antonio, USA   \n5 College of Computer Science and Technology, China University of Petroleum (East China), China 6 School of Computer and Software, Nanjing University of Information Science and Technology, China\n\n# Abstract\n\nFederated learning (FL) has emerged as a prominent machine learning paradigm in edge computing environments, enabling edge devices to collaboratively optimize a global model without sharing their private data. However, existing FL frameworks suffer from efficacy deterioration due to the system heterogeneity inherent in edge computing, especially in the presence of domain shifts across local data. In this paper, we propose a heterogeneous FL framework DapperFL, to enhance model performance across multiple domains. In DapperFL, we introduce a dedicated Model Fusion Pruning (MFP) module to produce personalized compact local models for clients to address the system heterogeneity challenges. The MFP module prunes local models with fused knowledge obtained from both local and remaining domains, ensuring robustness to domain shifts. Additionally, we design a Domain Adaptive Regularization (DAR) module to further improve the overall performance of DapperFL. The DAR module employs regularization generated by the pruned model, aiming to learn robust representations across domains. Furthermore, we introduce a specific aggregation algorithm for aggregating heterogeneous local models with tailored architectures and weights. We implement DapperFL on a realworld FL platform with heterogeneous clients. Experimental results on benchmark datasets with multiple domains demonstrate that DapperFL outperforms several state-of-the-art FL frameworks by up to $2 . 2 8 \\%$ , while significantly achieving model volume reductions ranging from $20 \\%$ to $80 \\%$ . Our code is available at: https://github.com/jyzgh/DapperFL.\n\n# 1 Introduction\n\nFederated Learning (FL), an emerging distributed machine learning paradigm in edge computing environments [1, 2], enables participant devices (i.e., clients) to optimize their local models while a central server aggregates these local models into a global model [3]. In contrast to traditional centralized machine learning paradigms, FL facilitates the collaborative training of a global model by distributed clients without the need for transmitting raw local data, thus mitigating privacy concerns associated with data transmission [2].\n\nAccuracy on domain 2 0 Accuracy on domain 3 ④ Aggregated Global model Central Server global model performance degradation 1 1 ③ 1 ③   \nGlobal Global Local Global Local   \nmodel model model model model ② Local update ② Local update ② Local update on Device 1 on Device 2 on Device 3 Local data for Local data for Update timeout domain 2 domain 3\n\nHowever, there are a number of challenges associated with FL deployments in edge computing settings, such as: a) System heterogeneity. Within prevalent FL environments, participant clients generally exhibit diverse and constrained system capabilities (e.g., CPU architecture, available memory, battery status, etc.). This heterogeneity often results in low-capability clients failing to complete local training of FL, consequently diminishing the performance of the aggregated global model [4, 5, 6]. b) Domain shifts. Owing to the distributed nature of FL, the data distributions among participant clients vary significantly [7, 8, 9]. Fig. 1 visually demonstrates the impact of these issues on FL performance, thus serving as the motivation for our work. Specifically, a) System heterogeneity: Device 1, with stringent resource constraints, fails to complete its model update within the maximum allowed time. As a result, the central server does not consider Device 1’s model update, thus excluding it from the FL process and missing out on the data features captured by Device 1. b) Domain shifts: Devices 2 and 3 collect data from different domains, leading to diverse local models. With non-IID data, the global model aggregation may become biased, particularly if Device 3 contributes more data, thus potentially limiting the benefit for Device 2.\n\nSeveral studies have been devoted to addressing these challenges while only addressing each challenge independently. In addressing the system heterogeneity challenge, existing FL frameworks [10, 11, 12] compress local models for heterogeneous clients, thereby enabling the inclusion of low-capability clients into the FL process. However, these frameworks typically assume that the local data on clients shares a uniform domain, and the compressed models are tailored for each individual client. Such compressed models are susceptible to over-fitting on their local domains due to the presence of domain shifts in practice. Consequently, the central server struggles to aggregate a robust global model with strong Domain Generalization (DG) capabilities. To address the domain shifts challenge, existing solutions exemplified by works such as [13, 14, 15], explore domain-invariant learning approaches either within the centralized machine learning paradigm or within ideal FL environments. However, these solutions unrealistically neglect the resource consumption of the clients, rendering them unsuitable for direct application in heterogeneous FL. Moreover, the inherent nature of FL yields non-independent and identically distributed (non-IID) data across the clients, further impeding these solutions from learning a domain-invariant global model, owing to limited data samples or labels available on individual clients. Despite existing work devoted to independently addressing system heterogeneity or domain shifts, few of them tackle these challenges simultaneously.\n\nTo bridge the gaps observed in existing studies, we propose DapperFL, an innovative FL framework designed to enhance model performance across multiple domains within heterogeneous FL environments. DapperFL addresses the system heterogeneity challenge through the deployment of a dedicated Model Fusion Pruning (MFP) module. The MFP module generates personalized compact local models for clients by pruning them with fused knowledge derived from both the local domain and remaining domains, thus ensuring domain generalization of the local models. Additionally, we introduce a Domain Adaptive Regularization (DAR) module to improve the overall performance of DapperFL. The DAR module segments the pruned model into an encoder and a classifier, intending to encourage the encoder to learn domain-invariant representations. Moreover, we propose a novel model aggregation approach tailored to aggregating heterogeneous local models with varying architectures and weights. We summarize our contributions as follows:\n\n• We propose the MFP module to prune local models with personalized footprints leveraging both domain knowledge from local data and global knowledge from the server, thereby addressing the system heterogeneity issue in the presence of domain shifts. Additionally, we introduce a heterogeneous aggregation algorithm for aggregating the pruned models. • We propose the DAR module to enhance the performance of the pruned models produced by the MFP module. The DAR module introduces a regularization term on the local objective to encourage clients to learn robust representations across various domains, thereby adaptively alleviating the domain shifts problem. • We implement the proposed framework on a real-world FL platform and evaluate its performance on two benchmark datasets with multiple domains. The results demonstrate that DapperFL outperforms several state-of-the-art frameworks (i.e., [3, 16, 14, 15, 10, 17, 11, 12]) by up to $2 . 2 8 \\%$ , while achieving adaptive model volume reductions on heterogeneous clients.\n\n# 2 Related Work\n\nHeterogeneous Federated Learning. In heterogeneous FL, diverse system capabilities and data distributions across clients often result in performance degradation of the global model [18, 19, 20]. Extensive studies have made efforts to address these heterogeneity issues through various solutions. For example, studies in [10, 21, 22, 11, 23, 12] adopt model sparsification techniques to reduce the volume of local models, thereby involving low-capability clients in the FL process. The studies in [17, 24, 25] leverage dedicated objective functions and specialized training steps to address the data heterogeneity issue, with studies in [17, 25] allowing clients to conduct varying numbers of local updates and consequently alleviating system heterogeneity issue. Several studies [5, 26, 27] selectively optimize or transmit a fraction of the local model’s parameters to reduce computational or communication resource consumption. Studies in [28, 29, 30] split the model into several sub-models and offload a subset of sub-models to the server for updating, therefore alleviating the training burden of clients. However, these heterogeneous FL frameworks commonly assume the data heterogeneity (i.e., non-IID data) exclusively involves distribution shifts in the number of samples and/or labels, while neglecting the existence of domain shifts.\n\nDomain Generalization (DG). DG is originally proposed in centralized machine learning paradigms to address the problem of domain shifts. Existing centralized studies assume access to the entire dataset during the model training process and propose various solutions to achieve DG [31]. For example, the studies in [32, 33, 34] focus on learning domain-invariant representations that can be generalized to unseen domains. In contrast to learning domain-invariant representations, several studies (e.g., [35, 36, 37, 38]) train a generalizable model across multiple domains leveraging metalearning or transfer learning techniques. Additionally, there are studies (e.g., [39, 40, 13]) that focus on the characteristics of domains, enhancing the generality of the model by properly augmenting the style of domain or instance. Unfortunately, the fundamental assumption of centralized machine learning is not satisfied in FL, where the dataset is distributed among clients who are restricted from sharing their data. Despite a few recent studies exploring DG approaches in FL [41, 14, 15, 9] through representation learning, prototype learning, and/or contrastive learning techniques, they typically neglect the inherent system heterogeneity feature of FL. Consequently, these approaches have shown limited effectiveness in heterogeneous FL due to strict resource constraints. In contrast, we explored a distributed model pruning approach that leverages both the local domain knowledge and the global knowledge from all clients, thereby reducing resource consumption in heterogeneous FL with the presence of domain shifts. Additionally, we design a specific regularization technique for updating the pruned local models, thereby further enhancing model performance across domains.\n\n# 3 DapperFL Design\n\n# 3.1 Overview of DapperFL\n\nDapperFL comprises two key modules, namely Model Fusion Pruning (MFP) and Domain Adaptive Regularization (DAR). The MFP module is designed to compress the local model while concurrently addressing domain shift problems, and the DAR module employs regularization generated by the compressed model to further mitigate domain shift problems and hence improve the overall perfor\n\n□ ① Model Fusion Pruning ② Update local model with ① Model Fusion Pruning ② Update local model with Domain Adaptive Regularization ? Domain Adaptive Regularization   \nClient i Local objective L Client j Local objective L ▲ ↑ Domain-specific DAR Domain-specific CDAR representations + representations +   \nGlobal model Fused model L Global model Fused model CCE normpruningi norm pruningi   \nC Input: Input:   \nFine-tuned Fine-tuned T   \nlocal model Pruned model i Local data Encoder Classifier local model Pruned model i Local data EncoderClassifier; A ▲ Global model Updated local model Global model Updated local model Local model Local model on client i ③Recover on client i ④ Aggregate model structure local models Aggregated global   \nCentral model for   \nserver Local model Local model subsequent round on client j on client j\n\nmance of DapperFL. Additionally, to handle the aggregation of heterogeneous models produced by the MFP module, we introduce a dedicated heterogeneous model aggregation algorithm.\n\nFigure 2 explains the DapperFL’s workflow during each communication round $t$ , which is also described as follows. $\\textcircled{1}$ The MFP module within each client $i \\in \\mathcal { C }$ calculates the fusion model $\\boldsymbol { \\mathbf { \\mathit { w } } } _ { i } ^ { t }$ using both the global and fine-tuned local models. The global model $\\mathcal { W } ^ { t - 1 }$ is downloaded from the central server,2 and the local model $\\hat { \\mathbf { \\boldsymbol { w } } } _ { i } ^ { t }$ is fine-tuned on the client’s local data with an initial epoch. Subsequently, the MFP calculates a binary mask matrix $\\boldsymbol { M } _ { i } ^ { t }$ using $\\ell _ { 1 }$ norm and produces the pruned local model $\\boldsymbol { w } _ { i } ^ { t } \\odot \\boldsymbol { M } _ { i } ^ { t }$ . $\\textcircled{2}$ The DAR module updates the pruned model $\\boldsymbol { w } _ { i } ^ { t } \\odot \\boldsymbol { M } _ { i } ^ { t }$ over several epochs with a dedicated local objective $\\mathscr { L } _ { i }$ . During local updating, the DAR module segments the local model into an encoder and a classifier, which are responsible for generating local representations and performing predictions, respectively. The local objective $\\mathscr { L } _ { i }$ is then constructed by combing regularization $\\bar { \\mathcal { L } } _ { i } ^ { \\bar { D } A R }$ of the local representations with the common cross-entropy loss $\\dot { \\mathcal { L } } _ { i } ^ { C E }$ . Next, the client transmits the updated local model to the central server. $\\textcircled{3}$ To aggregate local models with heterogeneous structures, the central server recovers the structure of received local models using the previous round’s global model $\\mathcal { W } ^ { t - 1 }$ .2 $\\textcircled{4}$ The central server aggregates the recovered local models through weighted averaging, obtaining the global model $\\mathcal { W } ^ { t }$ for round $t$ .\n\n# 3.2 Model Fusion Pruning on Edge Devices\n\nIn this subsection, we present the design of the MFP module employed by DapperFL. The goal of the MFP module is to tailor the footprint of the local model for edge devices in the presence of domain shifts in the local data, thereby addressing the system heterogeneity problem. Inspired by the spirit of the transfer learning [42, 43, 44], MFP fuses the global model $\\mathcal { W } ^ { \\bar { t } - \\bar { 1 } }$ into the fine-tuned local model $\\hat { \\mathbf { \\boldsymbol { w } } } _ { i } ^ { t }$ to learn the cross-domain knowledge. This avoids over-fitting the model to the local domain while enhancing the generality of the model. After that, MFP calculates a binary mask matrix $\\boldsymbol { M } _ { i } ^ { t }$ to generate the pruned local model. The detailed pruning process is described in Algorithm 1.\n\nSpecifically, in the initial epoch of each communication round $t \\in [ 1 , T ]$ , the MFP module first fine-tunes the global model $\\mathcal { W } ^ { t - 1 }$ on local data $\\mathcal { D } _ { i }$ to produce local model $\\hat { \\mathbf { \\boldsymbol { w } } } _ { i } ^ { t }$ (in line 1). We utilize one epoch of local training to determine the pruned models for the following reasons: 1) Additional local epochs do not significantly enhance the model’s performance, which justifies the use of a single epoch for efficiency. As noted in [45], experiments have demonstrated that extending local training beyond one epoch yields results comparable to those achieved with just one epoch. 2) In previous domain generalization-related FL research, such as [45], one epoch is also employed to collect local domain information. This method has proven adequate for capturing essential features and domain characteristics. 3) Pioneering research in model design and neural architecture search, such as [46], has demonstrated that a few epochs are sufficient to obtain a coarse estimate of a sub-model. This approach is effective in quickly assessing model configurations without extensive computation.\n\n<html><body><table><tr><td>Algorithm1Model Fusion Pruning of DapperFL</td></tr><tr><td>Input: Global model Wt-1, local data Di, pruning ratio pi</td></tr><tr><td>Output: Pruned local model wt   Mt</td></tr><tr><td>1: ω ← Fine-tune global model Wt-1 on local data Di</td></tr><tr><td>2: wt ← Fuse the global model Wt-1 into the local model ωt using Eq. 1 and Eq. 2</td></tr><tr><td> 3: Mt ← Calculate binary mask matrix by l1 norm with pruning ratio pi</td></tr><tr><td>4: wt O Mt ← Prune the local model wt with binary mask matrix Mt</td></tr><tr><td>5: return Pruned local model wt ③ Mt</td></tr></table></body></html>\n\nNext, the MFP module fuses the global model $\\mathcal { W } ^ { t - 1 }$ into the local model $\\hat { \\mathbf { \\Omega } } \\hat { \\mathbf { \\Omega } } \\hat { \\mathbf { \\Omega } } \\hat { \\mathbf { \\Omega } } \\hat { \\mathbf { \\Omega } } \\hat { \\mathbf { \\Omega } }$ to embed the crossdomain information into the local model (in line 2). The fusion operation is formulated as follows:\n\n$$\n\\begin{array} { r } { { \\pmb w } _ { i } ^ { t } = \\alpha ^ { t } \\gamma \\gamma ^ { t - 1 } + ( 1 - \\alpha ^ { t } ) \\hat { \\pmb w } _ { i } ^ { t } , } \\end{array}\n$$\n\nwhere $\\alpha ^ { t }$ is the fusion factor used to control the quality of the fused global model $\\mathcal { W } ^ { t - 1 }$ . Considering that the local data on the client is typically limited, resulting to updating at the beginning of FL requires more guidance from the global model by exploring the commonalities across different domains. As the FL process proceeds, the local model is capable of learning more domain-dependent information from itself. Thus, we design a dynamic adjusting mechanism to modify the $\\alpha ^ { t }$ during the FL training. The $\\alpha ^ { t }$ is initialized with a relatively large value $\\alpha _ { 0 }$ and decreases as the FL process proceeds. The decreased speed is controlled by a sensitivity factor $\\epsilon$ . We assign a minimum value $\\alpha _ { m i n }$ for $\\alpha ^ { t }$ to ensure the local model will consistently learn the knowledge from other domains. Formally, the dynamic adjusting mechanism for $\\alpha ^ { t }$ is described as follows:\n\n$$\n\\alpha ^ { t } = \\operatorname* { m a x } \\lbrace ( 1 - \\epsilon ) ^ { t - 1 } \\alpha _ { 0 } , \\alpha _ { m i n } \\rbrace .\n$$\n\nSubsequently, the MFP module calculates a binary mask matrix $M _ { i } ^ { t } \\in \\{ 0 , 1 \\} ^ { | \\boldsymbol { w } _ { i } ^ { t } | }$ for pruning the fused local model $\\boldsymbol { \\mathbf { \\mathit { w } } } _ { i } ^ { t }$ (in line 3). The matrix $\\boldsymbol { M } _ { i } ^ { t }$ is derived through the channel-wise $\\ell _ { 1 }$ norm, which has proven to be effective and efficient in assessing the importance of parameters [47, 48]. The elements in $\\boldsymbol { M } _ { i } ^ { t }$ with a value of $\\ \" 0 \\ \"$ indicate the corresponding parameters that need to be pruned, while those with a value of $^ { \\cdot \\cdot } 1 ^ { \\ , \\ , }$ indicate the parameters that will be retained. The pruning ratio $\\rho _ { i }$ determines the proportion of $\\ \" 0 \\cdot \\bf { \\dot { \\sigma } }$ in $\\boldsymbol { M } _ { i } ^ { t }$ .3 Finally, the MFP module prunes the local model $\\boldsymbol { w } _ { i } ^ { t }$ with the binary mask matrix $\\boldsymbol { M } _ { i } ^ { t }$ (in line 4). The pruned model can be represented as $\\boldsymbol { w } _ { i } ^ { t } \\odot \\boldsymbol { M } _ { i } ^ { t }$ . It is noteworthy that, the pruning strategy used in our DapperFL is structural pruning strategy (channel pruning), which is a hardware-friendly approach that can be easily implemented with popular machine learning libraries such as PyTorch, making it particularly suitable for deployment on edge devices.\n\n# 3.3 Domain Adaptive Regularization\n\nIn FL, each client $i \\in \\mathcal { C }$ possess private local data $\\boldsymbol { \\mathcal { D } } _ { i } ~ = ~ \\{ x _ { i } , y _ { i } \\} ^ { N _ { i } }$ , where $x \\in \\mathcal { X }$ denotes the input, $y \\in \\mathcal { V }$ denotes the corresponding label, and $N _ { i }$ represents the local data sample size. The data distribution $p _ { i } ( x , y )$ of client $\\mathbf { \\chi } _ { i }$ typically varies from that of other clients, i.e., $p _ { i } ( x ) \\neq p _ { j } ( x ) , p _ { i } ( x | y ) \\neq p _ { j } ( x | y )$ , leading to the domain shifts problem. Due to the existence of domain shifts, representation $z _ { i }$ generated by the local encoder varies among different clients, resulting in degraded prediction results of the local predictor. To address the domain shifts problem, we design a DAR module to enhance the performance of DapperFL across multiple domains while maintaining compatibility with the MFP module.\n\nSpecifically, the DAR module introduces a regularization term to the local objective to alleviate the bias of representations $z _ { i }$ on different clients adaptively. To achieve this goal, we first segment each pruned local model $_ { w \\odot M }$ into two parts, i.e., an encoder $w _ { e } \\odot M _ { e }$ and a predictor $\\boldsymbol { w } _ { p } \\odot \\boldsymbol { M } _ { p }$ , where $\\pmb { w } = \\{ \\pmb { w } _ { e } , \\pmb { w } _ { p } \\}$ and $M = \\{ M _ { e } , M _ { p } \\}$ .4 The encoder is responsible for learning a representation $z _ { i }$ given an input $x _ { i }$ , denoted as $z _ { i } \\equiv g _ { e } ( { \\pmb w } _ { e } \\odot { \\pmb M } _ { e } ; x _ { i } )$ . While the predictor is responsible for predicting label $\\hat { y _ { i } }$ given the representation $z _ { i }$ , denoted as $\\hat { y } _ { i } = g _ { p } ( \\pmb { w } _ { p } \\odot \\pmb { M } _ { p } ; z _ { i } )$ . Subsequently, we construct a regularization term $\\mathcal { L } ^ { D A R }$ on the local objective as follows:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { i } ^ { D A R } = | | g _ { e } ( \\pmb { w } _ { e } \\odot \\pmb { M } _ { e } ; x _ { i } ) | | _ { 2 } ^ { 2 } . } \\end{array}\n$$\n\nHere, $| | \\cdot | | _ { 2 } ^ { 2 }$ represents the squared $\\ell _ { 2 }$ norm of the local representation. The $\\ell _ { 2 }$ norm implicitly encourages different local encoders to generate aligned robust representations adaptively, thereby mitigating the impact of domain shifts. We adopt the squared $\\ell _ { 2 }$ norm to construct the regularization term for the following reasons: a) The $\\ell _ { 2 }$ norm encourages each element of the representation to converge to 0 but not equal to 0. This property is advantageous compared to the $\\ell _ { 1 }$ norm, which tends to make smaller representation elements exactly equal to 0. Thus, the squared $\\ell _ { 2 }$ norm ensures that the pruned model retains more information in the representations. b) Higher order regularization introduces significant computational overhead compared to the $\\ell _ { 2 }$ norm, without significantly improving the regularization effectiveness.\n\nNext, the cross-entropy loss used in the DAR module is constructed as:\n\n$$\n\\mathcal { L } _ { i } ^ { C E } = - \\frac { 1 } { \\vert \\mathcal { K } _ { i } \\vert } \\sum _ { k \\in \\mathcal { K } _ { i } } y _ { i , k } \\log ( \\hat { y } _ { i , k } ) ,\n$$\n\nwhere $\\textstyle \\bigwedge _ { i }$ denotes the set of possible labels on the client $i$ , $\\hat { y } _ { i , k }$ denotes predicting label, and $y _ { i , k }$ denotes ground-truth label. Finally, the training objective of each client is calculated as follows:\n\n$$\n\\mathcal { L } _ { i } = \\mathcal { L } _ { i } ^ { C E } + \\gamma \\mathcal { L } _ { i } ^ { D A R } ,\n$$\n\nwhere the $\\gamma$ is a pre-defined coefficient controlling the importance of $\\mathcal { L } _ { i } ^ { D A R }$ relative to $\\mathcal { L } _ { i } ^ { C E }$ .\n\n# 3.4 Heterogeneous Model Aggregation\n\nDespite the MFP module and the DAR module being capable of alleviating domain shift issues, the heterogeneous local models generated by the MFP module cannot be aggregated directly using popular aggregation algorithms. Therefore, we propose a specific FL aggregation algorithm for DapperFL to effectively aggregate these heterogeneous local models.\n\nTo preserve specific domain knowledge while transferring global knowledge to the local model, the central server first recovers the structure of local models before aggregating. Specifically, in each communication round $t$ , the pruned local model is recovered as follows:\n\n$$\n\\begin{array} { r } { \\pmb { w } _ { i } ^ { t } : = \\underbrace { \\pmb { w } _ { i } ^ { t } \\odot \\pmb { M } _ { i } ^ { t } } _ { \\mathrm { l o c a l k n o w l e d g e } } + \\underbrace { \\pmb { \\mathcal { W } } ^ { t - 1 } \\odot \\overline { { \\pmb { M } } } _ { i } ^ { t } } _ { \\mathrm { g l o b a l k n o w l e d g e } } , } \\end{array}\n$$\n\nwhere $\\mathcal { W } ^ { t - 1 }$ is the global model aggregated at the $( t - 1 )$ -th round, and $\\overline { { \\boldsymbol { M } } } _ { i } ^ { t }$ denotes the logical NOT operation applied to $\\boldsymbol { M } _ { i } ^ { t }$ . The first term $\\boldsymbol { w } _ { i } ^ { t } \\odot \\boldsymbol { M } _ { i } ^ { t }$ contains local knowledge5 specific to client $i$ , while the second term $\\mathbf { \\boldsymbol { w } } ^ { t - 1 } \\odot \\overline { { \\boldsymbol { M } } } _ { i } ^ { t }$ contains the global knowledge6 from all clients. Additionally, the structure of $\\boldsymbol { w } _ { i } ^ { t } \\odot { \\boldsymbol { M } } _ { i } ^ { t }$ , which includes local knowledge, complements $\\boldsymbol { w } _ { i } ^ { t } \\odot \\boldsymbol { M } _ { i } ^ { t }$ . Consequently, the structure recovery operation not only reinstates the pruned model’s architecture but also transfers global knowledge to the pruned model, which is essential for the subsequent aggregation process. By combining these two forms of knowledge, we aim to leverage both the specialized insights of local models and the generalized capabilities of the global model, thereby enhancing the performance and adaptability of DapperFL.\n\nInput: Initial global model $\\mathcal { W } ^ { 0 }$ , total number of communication rounds $T$ , total number of local epochs $E$ , local dataset $\\mathcal { D } _ { i }$ and pruning ratio $\\rho _ { i }$ for each client $i$   \nOutput: Optimized global model $\\boldsymbol { \\mathcal { W } }$ Local Updates:   \n1: $\\mathbf { \\Delta } w _ { i } ^ { t } \\odot \\bar { M _ { i } ^ { t } } \\gets$ Prune local model by the pruning ratio $\\rho _ { i }$ using Algorithm 1 // local epoch $e = 1$   \n2: for local epoch $e \\in [ 2 , E ]$ do   \n3: Update the pruned local model $\\boldsymbol { w } _ { i } ^ { t } \\odot \\boldsymbol { M } _ { i } ^ { t }$ using local dataset $\\mathcal { D } _ { i }$ and local objective Eq.5   \n4: end for Server Executes:   \n5: for communication round $t \\in [ 1 , T ]$ do   \n6: Send global model $\\mathcal { W } ^ { t - 1 }$ to all clients $/ /$ waiting for the clients to finish updating   \n7: Receive updated models $\\boldsymbol { w } _ { i } ^ { t } \\odot \\boldsymbol { M } _ { i } ^ { t }$ from all clients   \n8: $\\mathbf { \\Delta } w _ { i } ^ { t } \\gets$ Recover the local models using Eq.6   \n9: ${ \\mathcal { W } } ^ { t } \\gets$ Aggregate the local models using Eq.7   \n10: end for $\\mathcal { W } ^ { t }$\n\n11: return Optimized global model\n\nFinally, the global model is calculated by aggregating the recovered local models as follows:\n\n$$\n\\mathcal { W } ^ { t } = \\sum _ { i \\in \\mathcal { C } } \\frac { | \\mathcal { D } _ { i } | } { | \\mathcal { D } | } \\mathbf { w } _ { i } ^ { t } ,\n$$\n\nwhere $\\left| \\mathcal { D } _ { i } \\right|$ is the sample number in the local dataset on client $i$ , and $| \\mathcal D |$ is the total number of samples in the entire FL system. Benefiting from the heterogeneous aggregation algorithm, the aggregated global model retains knowledge from all clients, enabling it to perform robustly across domains.\n\nThe overall learning process of our proposed DapperFL is described in Algorithm 2.\n\n# 4 Experimental Evaluation\n\n# 4.1 Experimental Setup\n\nImplementation Details. We implement DapperFL with a real-world FL platform FedML [49] with deep learning tool PyTorch [50]. We build the FL environment with a client set $\\mathcal { C }$ containing 10 heterogeneous edge devices and a central server on the FedML platform. Following a convention setting [5, 25, 51], we categorize these heterogeneous devices into 5 levels according to their system capabilities, i.e., $\\mathcal { C } _ { l } \\in \\mathcal { C }$ $( \\bar { l ^ { \\mathrm { ~ } } } \\in [ 1 , 5 ] ) ,$ ), where $\\mathcal { C } _ { l }$ represents device set belongs to level $l$ . The system capabilities of devices in set $\\mathcal { C } _ { l }$ decrease as level $l$ increases. Our experiments are conducted on a GPU server with 2 NVIDIA RTX 3080Ti GPUs. Each experiment is executed three times with three fixed random seeds to calculate average metrics and ensure the reproducibility of our results.\n\nDatasets and Data Partition. We evaluate DapperFL on two domain generalization benchmarks, Digits [52, 53, 54, 55] and Office Caltech [56] that are commonly used in the literature for domain generalization. The Digits consists of the following four domains: MNIST, USPS, SVHN, and SYN. The Office Caltech consists of the following four domains: Caltech, Amazon, Webcam, and DSLR. For each benchmark, we distribute the simulated 10 clients randomly to each domain while guaranteeing that each domain contains at least one client and that each client only belongs to one domain. To generate non-IID local data, we randomly extract a proportion of data from the corresponding domain as the statistical characteristics vary among domains. Following the conventional data partition setting [15], we set $1 \\%$ as the local data proportion of Digits and $20 \\%$ as that in Office Caltech based on the complexity and scale of the benchmarks.\n\nModels. We adopt ResNet10 and ResNet18 [57] as the backbone models for Digits and Office Caltech, respectively. In the following evaluations, both the DapperFL and comparison frameworks train the models from scratch for a fair comparison.\n\nComparison Frameworks. We compare DapperFL with 8 state-of-the-art (SOTA) FL frameworks, including FedAvg [3], MOON [16], FedSR [14], FPL [15], FedDrop [10], FedProx [17], FedMP [11], and NeFL [12]. These FL frameworks are either classical or focus on addressing system heterogeneity or domain shifts issues. The comparison frameworks are described in detail in Appendix A.\n\nTable 1: Comparison of model accuracy $( \\% )$ on Digits.   \n\n<html><body><table><tr><td>FL frameworks</td><td>System Heter.</td><td>MNIST</td><td>USPS</td><td>SVHN</td><td>SYN</td><td>Global accuracy</td></tr><tr><td>FedAvg [3] MOON[16] FedSR [14]</td><td>X X X</td><td>95.89(1.47) 93.03(1.97) 96.77(0.73)</td><td>86.84(0.80) 78.38(5.81) 86.15(2.38)</td><td>78.39(3.24) 84.45(7.55) 81.48(1.77)</td><td>33.63(2.87) 25.97(3.28) 31.64(0.40)</td><td>71.81(0.46) 69.44(0.53) 73.89(0.57)</td></tr><tr><td>FPL [15] FedDrop [10] FedProx [17] FedMP[11] NeFL[12]</td><td>X √ √ √ <</td><td>95.54(1.78) 89.48(2.56) 96.68(0.96) 94.16(3.32) 84.98(1.07)</td><td>87.69(0.98) 82.51(1.17) 83.96(0.73) 85.30(2.66) 88.49(4.17)</td><td>83.74(4.26) 72.98(0.83) 76.69(3.50) 81.37(1.92)</td><td>34.73(1.53) 29.35(1.97) 30.95(1.42) 35.12(2.00)</td><td>74.17(0.95) 66.85(0.93) 70.74(0.52) 72.29(0.89)</td></tr></table></body></html>\n\nTable 2: Comparison of model accuracy $( \\% )$ on Office Caltech.   \n\n<html><body><table><tr><td>FL frameworks</td><td>SHeter.</td><td>Caltech</td><td>Amazon</td><td>Webcam</td><td>DSLR</td><td> Glubaly</td></tr><tr><td>FedAvg[3]</td><td>X</td><td>66.07(2.46)</td><td>76.84(3.18)</td><td>65.52(4.98)</td><td>56.67(1.98)</td><td>64.54(1.10)</td></tr><tr><td>MOON[16]</td><td>X</td><td>65.62(3.74)</td><td>75.79(1.69)</td><td>72.41(2.63)</td><td>53.33(1.93)</td><td>61.86(0.79)</td></tr><tr><td>FedSR [14]</td><td>X</td><td>62.95(2.25)</td><td>78.95(3.29)</td><td>75.86(3.59)</td><td>50.00(3.34)</td><td>65.47(1.13)</td></tr><tr><td>FPL [15]</td><td>×</td><td>63.84(3.17)</td><td>82.63(4.11)</td><td>65.52(2.63)</td><td>60.00(3.85)</td><td>65.45(1.15)</td></tr><tr><td>FedDrop [10]</td><td>√</td><td>66.07(0.89)</td><td>79.47(2.30)</td><td>56.90(3.98)</td><td>53.33(6.94)</td><td>60.58(1.42)</td></tr><tr><td>FedProx [17]</td><td>√</td><td>61.61(4.09)</td><td>71.05(4.98)</td><td>68.97(4.98)</td><td>46.67(1.93)</td><td>62.08(1.11)</td></tr><tr><td>FedMP[11]</td><td>√</td><td>65.62(2.49)</td><td>75.79(2.43)</td><td>56.90(3.59)</td><td>66.67(3.34)</td><td>62.34(0.93)</td></tr><tr><td>NeFL[12]</td><td>√</td><td>54.91(1.57)</td><td>71.05(1.61)</td><td>77.59(4.56)</td><td>66.67(3.85)</td><td>62.26(1.34)</td></tr><tr><td>DapperFL (ours)</td><td>√</td><td>64.73(1.03)</td><td>81.58(3.29)</td><td>74.14(1.99)</td><td>66.67(3.85)</td><td>67.75(0.97)</td></tr></table></body></html>\n\nDefault Hyper-parameters. To conduct fair evaluations, the global settings and local training hyperparameters of FL are set to identical values to both the DapperFL and comparison FL frameworks. For the framework-specific hyper-parameters, we use their default settings without changing them. The hyper-parameters used in our evaluations are described in Appendix B.\n\nEvaluation Metrics. In this paper, we evaluate the performance of the global model using the average Top-1 accuracy across all domains. In evaluating the resource consumption of the clients, we adopt both the total number of parameters and the Floating-Point Operations (FLOPs) of the local model.\n\n# 4.2 Performance Comparison\n\nModel Performance Across Domains. Tables 1 and 2 respectively provide detailed analyses of model accuracy on the Digits and Office Caltech benchmarks. The term “System Heter.” indicates whether the respective framework supports system heterogeneity.7 The results are reported as model accuracy with corresponding standard deviations in brackets.\n\nAs shown in both Tables 1 and 2, DapperFL achieves the highest global accuracy on both the Digits and Office Caltech benchmarks compared with the comparison frameworks. On Digits and Office Caltech, DapperFL’s global accuracy is $0 . 1 3 \\%$ and $2 . 2 8 \\%$ better than the runner-up, respectively. This indicates that the global model learned by DapperFL is more robust across all domains and therefore possesses better domain generalization. Moreover, DapperFL always achieves competing accuracy on individual domains and even the best domain accuracy on the SYN, Amazon, and DSLR. This demonstrates that the DAR module of DapperFL implicitly encourages the encoder to learn domain-invariant representations, resulting in DapperFL being unlikely to over-fit on the specific domain(s) and possessing better domain generalization ability. Furthermore, DapperFL tolerates heterogeneous clients through personalized pruning ratio $\\rho$ , achieving adaptive resource consumption reductions of $\\{ 2 0 \\% , 4 0 \\% , 6 0 \\% , 8 0 \\% \\}$ for clients in $\\{ \\mathcal { C } _ { 2 } , \\mathcal { C } _ { 3 } , \\mathcal { C } _ { 4 } , \\mathcal { C } _ { 5 } \\}$ , while outperforming comparison FL frameworks that support heterogeneous clients. This is evidence that the MFP module of DapperFL effectively reduces the resource consumption of clients with domain shifts. In addition, we provide learning curves for both global and domain accuracies in Appendix C for a comprehensive analysis.\n\nImpact of Model Footprints. In heterogeneous FL, resource constraints on the clients often result in limited model footprints (i.e., number of parameters and FLOPs). To investigate the impact of the model footprints on model performance, we compare the DapperFL with two SOTA FL frameworks (i.e., FedMP and NeFL) that are equipped with adaptive pruning techniques. The pruning ratio $\\rho$ determines both the reduction in the number of parameters and the FLOPs. We maintain a consistent $\\rho$ for every device, with predetermined values selected from the set {0.2, 0.4, 0.6, 0.8}.\n\n100 100 Model Accuracy (%) 80 FDeadppMePrFL Model Accuracy (%) 80 FDeadppMePrFL 60 60 40 40 20 20 0 0 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 Pruning Ratio Pruning Ratio (a) Digits (b) Office Caltech\n\nFigure 3 illustrates the comparative results under varying pruning ratios on both Digits and Office Caltech. As depicted, these frameworks tend to achieve higher accuracy with smaller pruning ratios, and DapperFL consistently outperforms the others across all $\\rho$ values on both benchmarks. This underscores the superior adaptability of DapperFL in heterogeneous FL environments. Notably, in Figure 3(b), the accuracy of DapperFL at $\\rho = 0 . 4$ surpasses its accuracy at $\\rho = 0 . 2$ . This discrepancy arises from the over-parameterization of ResNet18 for the classification task in Office Caltech. The improved accuracy observed at higher pruning rates suggests that the MFP module of DapperFL effectively prunes unnecessary parameters while enhancing model generalization rather than compromising it. This also demonstrates the reason behind DapperFL’s superiority over comparison frameworks that employ full-size models. Furthermore, we comprehensively evaluate the effect of DapperFL’s pruning ratio $\\rho$ on model performance in Appendix D.\n\n# 4.3 Ablation Study\n\nEffect of Key Modules in DapperFL. We evaluate the performance of DapperFL with and without the MFP and DAR modules, individually and in combination. The following configurations are considered: “DapperFL w/o MFP+DAR”, DapperFL without the MFP and DAR modules. “DapperFL w/o DAR”, DapperFL without the DAR module. “DapperFL w/o MFP”, DapperFL without the MFP module. “DapperFL”, the complete DapperFL framework. It is noteworthy that when the MFP module is not employed in DapperFL, it performs model pruning using $\\ell _ { 1 }$ norm on the local models directly.\n\nTable 3 summarizes the ablation study results on both benchmarks. On Digits, the results indicate a gradual improvement in accuracy as we incorporate the MFP and\n\nTable 3: Effect of DapperFL’s Key Modules on Digits and Office Caltech.   \n\n<html><body><table><tr><td>Configuration</td><td>Digits</td><td>Office</td></tr><tr><td>DapperFL w/o MFP+DAR</td><td>71.94%</td><td>62.65%</td></tr><tr><td>DapperFL W/o DAR</td><td>72.37%</td><td>64.88%</td></tr><tr><td>DapperFL w/o MFP</td><td>73.34%</td><td>66.28%</td></tr><tr><td>DapperFL</td><td>74.30 %</td><td>67.75%</td></tr></table></body></html>\n\nDAR modules into DapperFL. DapperFL achieves the highest accuracy at $7 4 . 3 0 \\%$ , highlighting the synergistic effect of both MFP and DAR in enhancing model performance. Similarly, we observe a consistent pattern on Office Caltech. DapperFL w/o $\\mathbf { M F P + D A R }$ yields the lowest accuracy, while the inclusion of both MFP and DAR results in a steady improvement. The complete DapperFL framework attains the highest accuracy at $6 7 . 7 5 \\%$ , reinforcing the complementary roles played by MFP and DAR in bolstering domain adaptability in heterogeneous FL.\n\nEffect of Hyper-Parameters within the MFP Module. We investigate the influence of the hyperparameters $\\alpha _ { 0 }$ , $\\alpha _ { m i n }$ , and $\\epsilon$ within the MFP module on the overall performance of DapperFL. In this experiment, only the hyper-parameter under evaluation is modified, while others retain their default settings as outlined in Appendix B.\n\n7 67616 6761 356505   \n6761   \n5 Digits Office AVG 61 Digits Office AVG 61 Digits Office AVG Digits Office AVG   \n2 0.001 0.0050.010.050.1   \n0 min   \n(a) Effect of $\\alpha _ { 0 }$ in MFP (b) Effect of $\\alpha _ { m i n }$ in MFP (c) Effect of $\\epsilon$ in MFP (d) Effect of $\\gamma$ in DAR\n\nFigure 4(a) illustrates the effect of $\\alpha _ { 0 }$ on model accuracy. The $\\alpha _ { 0 }$ parameter dictates the initial weight of the global model fused into the local model. In both benchmarks, the optimal $\\alpha _ { 0 }$ is evident at 0.9, resulting in the highest average accuracy. This result underscores the significance of infusing the local model with knowledge from other domains early in the FL process to enhance its generalization. Figure 4(b) describes the effect of $\\alpha _ { m i n }$ on model accuracy, where $\\alpha _ { m i n }$ determines the minimum weight of the global model fused into the local model. In both benchmarks, the optimal average accuracy is achieved when $\\alpha _ { m i n }$ is set to 0.1. This finding signifies that, as the FL process proceeds, the local model benefits from acquiring more domain-specific knowledge to uphold its personalization. Figure 4(c) explores the effect of $\\epsilon$ on model accuracy. The $\\epsilon$ parameter controls the rate at which the fusion factor $\\alpha _ { 0 }$ diminishes towards $\\alpha _ { m i n }$ . In both benchmarks, the optimal $\\epsilon$ is discernible at 0.2, resulting in the highest average accuracy. This observation emphasizes that the most effective rate of decrease for the fusion factor $\\alpha _ { 0 }$ towards $\\alpha _ { m i n }$ is achieved when $\\epsilon = 0 . 2$ in both benchmarks. Furthermore, recognizing the abnormal trends in the relationship between model accuracy and hyper-parameter $\\epsilon$ when it is less than 0.2, we implement Bayesian search as an automatic selection mechanism to find the optimal value of $\\epsilon$ . The results are provided in Appendix E.\n\nEffect of Hyper-Parameter within the DAR Module. Figure 4(d) provides configurations with varying $\\gamma$ values across both Digits and Office Caltech. The results show an increase in accuracy with higher $\\gamma$ values until $\\gamma = 0 . 0 1$ on Digits, where the highest accuracy of $7 4 . 3 0 \\%$ is achieved. A notable drop in accuracy is observed at $\\gamma = 0 . 1$ , indicating that excessively large regularization may hinder model performance on Digits. Similar to Digits, an increase in accuracy is observed with higher $\\gamma$ values until $\\gamma = 0 . 0 1$ , reaching the highest accuracy of $6 7 . 7 5 \\%$ . The impact of regularization is more pronounced on Office Caltech, as seen by the significant drop in accuracy at $\\gamma = 0 . 1$ . This suggests that the regularisation factor $\\gamma$ in the DAR Module plays a crucial role in solving the domain shift problem in FL and a careful choice of $\\gamma$ can improve the model performance.\n\n# 5 Conclusion\n\nWe have described our proposed FL framework DapperFL tailored for heterogeneous edge devices with the presence of domain shifts. Specifically, DapperFL integrates a dedicated MFP module and a DAR module to achieve domain generalization adaptively in heterogeneous FL. The MFP module addresses system heterogeneity challenges by shrinking the footprint of local models through personalized pruning decisions determined by both local and remaining domain knowledge. This effectively mitigates limitations associated with system heterogeneity. Meanwhile, the DAR module introduces a specialized regularization technique to implicitly encourage the pruned local models to learn robust local representations, thereby enhancing DapperFL’s overall performance. Furthermore, we designed a specific aggregation algorithm for DapperFL to aggregate heterogeneous models produced by the MFP module, aiming to realize a robust global model across multiple domains. The evaluation of DapperFL’s implementation on a real-world FL platform demonstrated that it outperforms several SOTA FL frameworks on two benchmark datasets comprising various domains.\n\nLimitations and Future Work. Despite its potential in addressing system heterogeneity and domain shifts, DapperFL introduces four hyper-parameters $\\alpha _ { 0 }$ , $\\alpha _ { m i n }$ , $\\epsilon$ , and $\\gamma$ associated with the DG performance of the global model. A potential future direction involves the automatic selection of these hyper-parameters, therefore enhancing the flexibility and accessibility of the DapperFL.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决联邦学习（FL）在边缘计算环境中部署时面临的系统异质性和域偏移问题。系统异质性使低能力客户端无法完成本地训练，降低聚合全局模型的性能；域偏移则导致客户端数据分布差异大，影响全局模型的域泛化能力。这些问题严重影响了联邦学习的有效性和性能，限制了其在实际场景中的应用。\\n\\n**方法概述**\\n论文提出了DapperFL框架，该框架包含模型融合剪枝（MFP）模块和域自适应正则化（DAR）模块。MFP模块通过融合本地和全局知识对本地模型进行剪枝，生成个性化紧凑本地模型；DAR模块引入正则化项，鼓励模型学习跨域鲁棒表示。此外，还设计了专门的异构模型聚合算法。\\n\\n**主要贡献与效果**\\n- 提出MFP模块和异构聚合算法，解决系统异质性问题。在Digits和Office Caltech数据集上，DapperFL的全局准确率比亚军分别高出0.13%和2.28%。\\n- 提出DAR模块，缓解域偏移问题。实验表明，其有助于提升模型在多域上的性能。\\n- 在真实世界的FL平台实现DapperFL，实现了异构客户端上模型体积自适应减少20% - 80%，有效降低了资源消耗。同时，实验在GPU服务器上使用2个NVIDIA RTX 3080Ti GPUs执行三次以确保结果可重复性。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nDapperFL结合本地和全局知识，通过模型融合剪枝和域自适应正则化解决系统异质性和域偏移问题。MFP模块利用融合知识剪枝本地模型，确保域泛化；DAR模块引入正则化，鼓励模型学习跨域鲁棒表示。这种结合有效提高了模型在多域上的适应能力。\\n\\n**创新点**\\n先前工作常单独处理系统异质性或域偏移问题，且忽略另一方影响。DapperFL创新地同时考虑这两个问题，引入MFP和DAR模块及专门聚合算法，实现异构环境下的域泛化。MFP模块的动态调整融合因子机制，能更好平衡本地和全局知识。并且，DAR模块采用平方 $\\ell_2$ 范数构建正则化项，避免了 $\\ell_1$ 范数使表示元素为0的问题，且计算开销低于高阶正则化。\\n\\n**具体实现步骤**\\n1. **模型融合剪枝（MFP）**：在每个通信轮次初始，在本地数据微调全局模型生成本地模型；用融合公式 $\\boldsymbol{w}_i^t = \\alpha^t \\mathcal{W}^{t - 1} + (1 - \\alpha^t) \\hat{\\boldsymbol{w}}_i^t$ 将全局模型融入本地模型，其中 $\\alpha^t = \\max\\{(1 - \\epsilon)^{t - 1} \\alpha_0, \\alpha_{min}\\}$ 为动态调整的融合因子；通过通道级 $\\ell_1$ 范数计算二进制掩码矩阵，对融合后的本地模型剪枝。\\n2. **域自适应正则化（DAR）**：将剪枝后的本地模型分割为编码器和解码器；构建正则化项 $\\mathcal{L}_i^{DAR} = ||g_e(\\boldsymbol{w}_e \\odot \\boldsymbol{M}_e; x_i)||_2^2$，采用平方 $\\ell_2$ 范数，鼓励不同本地编码器生成对齐的鲁棒表示；计算交叉熵损失 $\\mathcal{L}_i^{CE} = - \\frac{1}{|\\mathcal{K}_i|} \\sum_{k \\in \\mathcal{K}_i} y_{i,k} \\log(\\hat{y}_{i,k})$，结合二者得到训练目标 $\\mathcal{L}_i = \\mathcal{L}_i^{CE} + \\gamma \\mathcal{L}_i^{DAR}$。\\n3. **异构模型聚合**：中央服务器用公式 $\\boldsymbol{w}_i^t := \\boldsymbol{w}_i^t \\odot \\boldsymbol{M}_i^t + \\mathcal{W}^{t - 1} \\odot \\overline{\\boldsymbol{M}}_i^t$ 恢复本地模型结构，再通过公式 $\\mathcal{W}^t = \\sum_{i \\in \\mathcal{C}} \\frac{|\\mathcal{D}_i|}{|\\mathcal{D}|} \\boldsymbol{w}_i^t$ 加权平均聚合恢复后的本地模型得到全局模型。\\n\\n**案例解析**\\n论文未明确提供具体案例解析内容。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文将DapperFL与8个最先进的联邦学习框架进行对比，包括FedAvg、MOON、FedSR、FPL、FedDrop、FedProx、FedMP和NeFL。\\n\\n**性能对比**\\n*   **在 [全局准确率/Global Accuracy] 指标上：** DapperFL在Digits和Office Caltech数据集上分别达到了 **74.30%** 和 **67.75%** 的全局准确率，分别高出次优框架0.13%和2.28%，显示出其在跨域上的鲁棒性和泛化能力。\\n*   **在 [资源消耗/Resource Consumption] 指标上：** DapperFL通过个性化剪枝率实现了客户端资源消耗自适应减少，模型体积减少20% - 80%，优于其他支持异构客户端的对比框架，表明MFP模块有效降低了客户端资源消耗。实验在GPU服务器上使用2个NVIDIA RTX 3080Ti GPUs执行三次以确保结果可重复性。\\n*   **在 [不同剪枝率下的准确率/Accuracy with Different Pruning Rates] 指标上：** 在不同剪枝率（0.2, 0.4, 0.6, 0.8）下，DapperFL在Digits和Office Caltech数据集上始终优于FedMP和NeFL，且在Office Caltech数据集上，$\\rho = 0.4$ 时准确率超过 $\\rho = 0.2$ 时，显示出MFP模块有效剪枝并增强泛化能力。\",\n    \"keywords\": \"### 关键词\\n\\n- 联邦学习 (Federated Learning, FL)\\n- 域泛化 (Domain Generalization, DG)\\n- 模型融合剪枝 (Model Fusion Pruning, MFP)\\n- 域自适应正则化 (Domain Adaptive Regularization, DAR)\\n- 边缘计算 (Edge Computing, N/A)\\n- 异构联邦学习 (Heterogeneous Federated Learning, N/A)\"\n}"
}