{
    "link": "https://arxiv.org/abs/2402.19299",
    "pdf_link": "https://arxiv.org/pdf/2402.19299",
    "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy",
    "authors": [
        "Shaoteng Liu",
        "Haoqi Yuan",
        "Minda Hu",
        "Yanwei Li",
        "Yukang Chen",
        "Shu Liu",
        "Zongqing Lu",
        "Jiaya Jia"
    ],
    "institutions": [
        "The Chinese University of Hong Kong",
        "SmartMore",
        "The Hong Kong University of Science and Technology",
        "School of Computer Science, Peking University",
        "Beijing Academy of Artificial Intelligence"
    ],
    "publication_date": "2024-02-29",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 17,
    "influential_citation_count": 2,
    "paper_content": "# RL-GPT: Integrating Reinforcement Learning and Code-as-policy\n\nShaoteng ${ { \\bf { L i u } } ^ { 1 } }$ , Haoqi Yuan4, Minda $\\mathbf { H } \\mathbf { u } ^ { 1 }$ , Yanwei $\\mathbf { L i } ^ { 1 }$ , Yukang Chen,1 Shu Liu,2 Zongqing Lu,4,5 Jiaya $\\mathbf { J i a ^ { 2 , 3 } }$\n\n1 The Chinese University of Hong Kong 2 SmartMore   \n3 The Hong Kong University of Science and Technology 4 School of Computer Science, Peking University 5 Beijing Academy of Artificial Intelligence https://sites.google.com/view/rl-gpt/\n\n# Abstract\n\nLarge Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it possibly obtains diamonds within a single day on an RTX3090. Additionally, it achieves good performance on designated MineDojo tasks.\n\n# 1 Introduction\n\nBuilding agents to master tasks in open-world environments has been a long-standing goal in AI research [1–3]. The emergence of Large Language Models (LLMs) has revitalized this pursuit, leveraging their expansive world knowledge and adept compositional reasoning capabilities [4–6]. LLMs agents showcase proficiency in utilizing computer tools [7, 8], navigating search engines [9, 10], and even operating systems or applications [11, 12]. However, their performance remains constrained in open-world embodied environments [1, 7], such as Minedojo [13]. Despite possessing “world knowledge” akin to a human professor, LLMs fall short when pitted against a child in a video game. The inherent limitation lies in LLMs’ adeptness at absorbing information but their inability to practice skills within an environment. Proficiency in activities such as playing a video game demands extensive practice, a facet not easily addressed by in-context learning, which exhibits a relatively low upper bound [7, 4, 6]. Consequently, existing LLMs necessitate human intervention to define low-level skills or tools.\n\nReinforcement Learning (RL), proven as an effective method for learning from interaction, holds promise in facilitating LLMs to “practise”. One line of works grounds LLMs for open-world control through RL fine-tuning [14–19]. Nevertheless, this approach necessitates a substantial volume of domain-specific data, expert demonstrations, and access to LLMs’ parameters, rendering it slow and resource-intensive in most scenarios. Given the modest learning efficiency, the majority of methods continue to operate within the realm of “word games” such as tone adjustment rather than tackling intricate embodied tasks.\n\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n\n![](images/ddccf690d10b09662261e726eb2cff3c298d459cf461cef0396e65d07cf14a56.jpg)  \nFigure 1: An overview of RL-GPT. After the optimization in an environment, LLMs agents obtain optimized coded actions, RL achieves an optimized neural network, and our RL-GPT gets both optimized coded actions and neural networks. Our framework integrates the coding parts and the learning parts.\n\n![](images/301d3c3cd1c3fae4ec678134eec48943f2bb3b3159e6bf7b62080779a2ac0a13.jpg)  \nFigure 2: To learn a subtask, the LLM can generate environment configurations (task, observation, reward, and action space) to instantiate RL. In particular, by reasoning about the agent behavior to solve the subtask, the LLM generates code to provide higher-level actions in addition to the original environment actions, improving the sample efficiency for RL.\n\nAddressing this challenge, we propose to integrate LLMs and RL in a novel approach: Empower LLMs agents to use an RL training pipeline as a tool. To this end, we introduce RL-GPT, a framework designed to enhance LLMs with trainable modules for learning interaction tasks within an environment. As shown in Fig. 3, RL-GPT comprises an agent pipeline featuring multiple LLMs, wherein the neural network is conceptualized as a tool designed for training the RL pipeline. Illustrated in Fig. 1, unlike conventional approaches where LLMs agents and RL optimize coded actions and networks separately, RL-GPT unifies this optimization process. The line chart in Fig. 1 illustrates that RL-GPT outperforms alternative approaches by seamlessly integrating both knowledge iteration and skill practice.\n\nWe further point out that the pivotal issue in using RL is to decide: Which actions should be learned with RL? To tackle this, RL-GPT is meticulously designed to assign different actions to RL and Code-as-policy [20], respectively. Our agent pipeline entails two fundamental steps. Firstly, LLMs should determine “which actions” to code, involving task decomposition into distinct sub-actions and deciding which actions can be effectively coded. Actions falling outside this realm will be learned through RL. Secondly, LLMs are tasked with writing accurate codes for the “coded actions” and test them in the environment.\n\nWe employ a two-level hierarchical framework to realize the two steps, as depicted in Fig. 3. Allocating these steps to two independent agents proves highly effective, as it narrows down the scope of each LLM’s task. Coded actions with explicit starting conditions are executed sequentially, while other coded actions are integrated into the RL action space. This strategic insertion into the action space empowers LLMs to make pivotal decisions during the learning process. Illustrated in Fig. 2, this integration enhances the efficiency of learning tasks, exemplified by our ability to more effectively learn how to break a tree.\n\nFor intricate tasks such as the ObtainDiamond task in the Minecraft game, devising a strategy with a single neural network proves challenging due to limited computing resources. In response, we incorporate a task planner to facilitate task decomposition. Our RL-GPT framework demonstrates remarkable efficiency in tackling complex embodied tasks. Specifically, within the MineDojo environment, it attains good performance on the majority of selected tasks and adeptly locates diamonds within a single day, utilizing only an RTX3090 GPU. Our contributions are summarized as follows:\n\n• Introduction of an LLMs agent utilizing an RL training pipeline as a tool.   \n• Development of a two-level hierarchical framework capable of determining which actions in a task should be learned.   \n• Pioneering work as the first to incorporate high-level GPT-coded actions into the RL action space.\n\n# 2 Related Works\n\n# 2.1 Agents in Minecraft\n\nMinecraft, a widely popular open-world sandbox game, stands as a formidable benchmark for constructing efficient and generalized agents. Previous endeavors resort to hierarchical reinforcement learning, often relying on human demonstrations to facilitate the training of low-level policies [21–23]. Efforts such as MineAgent [13], Steve-1 [24], and VPT [25] leverage large-scale pre-training via YouTube videos to enhance policy training efficiency. However, MineAgent and Steve-1 are limited to completing only a few short-term tasks, and VPT requires billions of RL steps for long-horizon tasks. DreamerV3 [26] utilizes a world model to expedite exploration but still demands a substantial number of interactions to acquire diamonds. These existing works either necessitate extensive expert datasets for training or exhibit low sample efficiency when addressing long-horizon tasks.\n\nAn alternative research direction employs Large Language Models (LLMs) for task decomposition and high-level planning to offload RL’s training burden using LLMs’ prior knowledge. Certain works [27] leverage few-shot prompting with Codex [28] to generate executable policies. DEPS [29] and GITM [30] investigate the use of LLMs as high-level planners in the Minecraft context. VOYAGER [1] and Jarvis-1 [31] explore LLMs for high-level planning, code generation, and lifelong exploration. Other studies [32, 33] delve into grounding smaller language models for control with domain-specific finetuning. Nevertheless, these methods often rely on manually designed controllers or code interfaces, sidestepping the challenge of learning low-level policies.\n\nPlan4MC [34] integrates LLM-based planning and RL-based policy learning but requires defining and pre-training all the policies with manually specified environments. Our RL-GPT extends LLMs ability in low-level control by equipping it with RL, achieving automatic and efficient task learning.\n\n# 2.2 LLMs Agents\n\nSeveral works leverage LLMs to generate subgoals for robot planning [35, 36]. Inner Monologue [37] incorporates environmental feedback into robot planning with LLMs. Code-as-Policies [20] and ProgPrompt [38] directly utilize LLMs to formulate executable robot policies. VIMA [39] and PaLM-E [2] involve fine-tuning pre-trained LLMs to support multimodal prompts. Chameleon [4] effectively executes sub-task decomposition and generates sequential programs. ReAct [6] utilizes chain-of-thought prompting to generate task-specific actions. AutoGPT [7] automates NLP tasks by integrating reasoning and acting loops. DERA [40] introduces dialogues between GPT-4 [41] agents. Generative Agents [42] simulate human behaviors by memorizing experiences. Creative Agent [43] achieves creative building generation in Minecraft. Our paper equips LLMs with RL to explore environments.\n\n# 2.3 Integrating LLMs and RL\n\nSince LLMs and RL possess complementary abilities in providing prior knowledge and exploring unknown information, it is promising to integrate them for efficient task learning. Prior works include using decision trees, finite state machines, DSL programs, and symbolic programs as policies [44–50].\n\nMost work studies improve RL with the domain knowledge in LLMs. SayCan [35] and Plan4MC [34] decompose and plan subtasks with LLMs, thereby RL can learn easier subtasks to solve the whole task. Recent works [51–54] studies generating reward functions with LLMs to improve the sample efficiency for RL. Some works [55–59] used LLMs to train RL agents. Other works [14, 15, 60, 61, 16, 17, 62] finetune LLMs with RL to acquire the lacked ability of LLMs in low-level control. However, these approaches usually require a lot of samples and can harm the LLMs’ abilities in other tasks. Our study is the first to overcome the inabilities of LLMs in low-level control by equipping them with RL as a tool. The acquired knowledge is stored in context, thereby continually improving the LLMs skills and maintaining its capability.\n\n# 3 Methods\n\nOur framework employs a decision-making process to determine whether an action should be executed using code or RL. The RL-GPT incorporates three distinct components, each contributing to its innovative design: (1) a slow agent tasked with decomposing a given task into several sub-actions and determining which actions can be directly coded, (2) a fast agent responsible for writing code and instantiating RL configuration, and (3) an iteration mechanism that facilitates an iterative process refining both the slow agent and the fast agent. This iterative process enhances the overall efficacy of the RL-GPT across successive iterations. For complex long-horizon tasks requiring multiple neural networks, we employ a GPT-4 as a planner to initially decompose the task.\n\n![](images/cc74e18c583575594b597ead731ad2ce61852cd3fe656e253393ec644df115a0.jpg)  \nFigure 3: Overview of RL-GPT. The overall framework consists of a slow agent (orange) and a fast agent (green). The slow agent decomposes the task and determines “which actions” to learn. The slow agent will improve the decision based on the high-level action feedbacks. The fast agent writes code and RL configurations. The fast agent debugs the written code based on the environment feedback (“Direct Code Implementation”). Correct codes will be inserted into the action space as high-level actions (“RL Implementation”).\n\n![](images/8e8eabcc9ae3bfb4127ead0be982f93b48ae021b83814ba8c70bfded6bad56e3.jpg)  \nFigure 4: The two-loop iteration. We design a method to optimize both slow agent and fast agent with a critic agent.\n\nAs discussed in concurrent works [63, 64], segregating high-level planning and low-level actions into distinct agents has proven to be beneficial. The dual-agent system effectively narrows down the specific task of each agent, enabling optimization for specific targets. Moreover, Liang et al. highlighted the Degeneration-of-Thought (DoT) problem, where an LLM becomes overly confident in its responses and lacks the ability for self-correction through self-reflection. Empirical evidence indicates that agents with different roles and perspectives can foster divergent thinking, mitigating the DoT problem. External feedback from other agents guides the LLM, making it less susceptible to DoT and promoting accurate reasoning.\n\n# 3.1 RL Interface\n\nAs previously mentioned, we view the RL training pipeline as a tool accessible to LLMs agents, akin to other tools with callable interfaces. Summarizing the interfaces of an RL training pipeline, we\n\nidentify the following components: 1) Learning task; 2) Environment reset; 3) Observation space; 4) Action space; 5) Reward function. Specifically, our focus lies on studying interfaces 1) and 4) to demonstrate the potential for decomposing RL and Code-as-policy.\n\nIn the case of the action space interface, we enable LLMs to design high-level actions and integrate them into the action space. A dedicated token is allocated for this purpose, allowing the neural network to learn when to utilize this action based on observations.\n\n# 3.2 Slow Agent: Action Planning\n\nWe consider a task $T$ that can feasibly be learned using our current computing resources (e.g., lablevel GPUs). We employ a GPT-4 [41] as a slow agent $A _ { S } . ~ A _ { S }$ is tasked with decomposing $T$ into sub-actions $\\alpha _ { i }$ , where $i \\in \\{ 0 , . . . , n \\}$ , determining if each $\\alpha _ { i }$ in $T$ can be directly addressed through code implementation. This approach optimally allocates computational resources to address more challenging sub-tasks using Reinforcement Learning techniques. Importantly, $A _ { S }$ is not required to perform any low-level coding tasks; it solely provides high-level textual instructions including the detailed description and context for sub-actions $\\alpha _ { i }$ . These instructions are then transmitted to the fast agent $\\boldsymbol { A } _ { F }$ for further processing. The iterative process of the slow agent involves systematically probing the limits of coding capabilities.\n\nFor instance, in Fig. 3, consider the specific action of crafting a wooden pickaxe. Although $A _ { S }$ is aware that players need to harvest a log, writing code for this task with a high success rate can be challenging. The limitation arises from the insufficient information available through APIs for $A _ { S }$ to accurately locate and navigate to a tree. To overcome this hurdle, an RL implementation becomes necessary. RL aids $A _ { S }$ in completing tasks by processing complex visual information and interacting with the environment through trial and error. In contrast, straightforward actions like crafting something with a table can be directly coded and executed.\n\nIt is crucial to instruct $A _ { S }$ to identify sub-actions that are too challenging for rule-based code implementation. As shown in Table 6, the prompt for $A _ { S }$ incorporates role description {role_description}, the given task $T$ , reference documents, environment knowledge {minecraft_knowledge}, planning heuristics {planning_tips}, and programming examples {programs}. To align $A _ { S }$ with our goals, we include the heuristic in the {planning_tips}. This heuristic encourages $A _ { S }$ to further break down an action when coding proves challenging. This incremental segmentation aids $A _ { S }$ in discerning what aspects can be coded. Further details are available in Appendix A.\n\n# 3.3 Fast Agent: Code-as-Policy and RL\n\nThe fast agent $A _ { F }$ is also implemented using GPT-4. The primary task is to translate the instructions from the slow agent $A _ { S }$ into Python codes for the sub-actions $\\alpha _ { i }$ . ${ \\varLambda } _ { F }$ undergoes a debug iteration where it runs the generated sub-action code and endeavors to self-correct through feedback from the environment. Sub-actions that can be addressed completely with code implementation are directly executed, as depicted in the blue segments of Fig. 3. For challenging sub-actions lacking clear starting conditions, the code is integrated into the RL implementation using the temporal abstraction technique [65, 66], as illustrated in Fig. 2. This involves inserting the high-level action into the RL action space, akin to the orange segments in Fig. 3. $\\boldsymbol { A } _ { F }$ iteratively corrects itself based on the feedback received from the environment.\n\n# 3.4 Two-loop Iteration\n\nIn Fig. 4, we have devised a two-loop iteration to optimize the proposed two agents, namely the fast agent ${ \\varLambda } _ { F }$ and the slow agent $A _ { S }$ . To facilitate it, a critic agent $C$ is introduced, which could be implemented using GPT-3.5 or GPT-4.\n\nThe optimization for the fast agent, as shown in Fig. 4, aligns with established methods for code-aspolicy agents. Here, the fast agent receives a sub-action, environment documents $D _ { e n v }$ (observation and action space), and examples $E _ { c o d e }$ as input, generating Python code. It then iteratively refines the code based on environmental feedback. The objective is to produce error-free Python-coded sub-actions that align with the targets set by the slow agent. Feedback, which includes execution errors and critiques from $C$ , plays a crucial role in this process. $C$ evaluates the coded action’s success by considering observations before and after the action’s execution.\n\nWithin Fig. 4, the iteration of the slow agent $A _ { S }$ encompasses the aforementioned fast agent $\\boldsymbol { A } _ { F }$ iteration as a step. In each step of $A _ { S }$ , $A _ { F }$ must complete an iteration loop. Given a task $T$ , $D _ { e n v }$ , and $E _ { c o d e }$ , $A _ { S }$ decomposes $T$ into sub-actions $\\alpha _ { i }$ and refines itself based on $C$ ’s outputs. The critic’s output includes: (1) whether the action is successful, (2) why the action is successful or not, (3) how to improve the action. Specifically, it receives a sequence of outputs $\\boldsymbol { C r i t i c } _ { i }$ from $C$ about each $\\alpha _ { i }$ to assess the effectiveness of action planning. If certain actions cannot be coded by the fast agent, the slow agent adjusts the action planning accordingly.\n\n# 3.5 Task Planner\n\nOur primary pipeline is tailored for tasks that can be learned using a neural network within limited computational resources. However, for intricate tasks such as ObtainDiamond, where it is more effective to train multiple neural networks like DEPS [29] and Plan4MC [34], we introduce a task planner reminiscent of DEPS, implemented using GPT-4. This task planner iteratively reasons what needs to be learned and organizes sub-tasks for our RL-GPT to accomplish.\n\n# 4 Experiments\n\n# 4.1 Environment\n\nMineDojo MineDojo [13] stands out as a pioneering framework developed within the renowned Minecraft game, tailored specifically for research involving embodied agents. This innovative framework comprises a simulation suite featuring thousands of tasks, blending both open-ended challenges and those prompted by language. To validate the effectiveness of our approach, we selected certain long-horizon tasks from MineDojo, mirroring the strategy employed in Plan4MC [34]. These tasks include harvesting and crafting activities. For instance, Crafting one wooden pickaxe requires the agent to harvest a log, craft planks, craft sticks, craft tables, and craft the pickaxe with the table. Similarly, tasks like milking a cow involve the construction of a bucket, approaching the cow, and using the bucket to obtain milk.\n\nObtainDiamond Challenge It represents a classic challenge for RL methods. The task of obtaining a diamond demands the agent to complete the comprehensive process of harvesting a diamond from the beginning. This constitutes a long-horizon task, involving actions such as harvesting logs, harvesting stones, crafting items, digging to find iron, smelting iron, locating a diamond, and so on.\n\n# 4.2 Implementation Details\n\nLLM Prompt We choose GPT-4 as our LLMs API. For the slow agents and fast agents, we design special templates, responding formats, and examples. We design some special prompts such as “assume you are an experienced RL researcher that is designing the RL training job for Minecraft”. Details can be found in the Appendix A. In addition, we encourage the slow agent to explore more strategies because the RL task requires more exploring. We encourage the slow agent to further decompose the action into sub-actions which may be easier to code.\n\nPPO Details The training and evaluation are the same as Mineagent or other RL pipelines as discussed in Appendix C. The difference is that our RL action space contains high-level coded actions generated by LLMs. Our method doesn’t depend on any video pretraining. It can work with only environment interaction. Similar to MineAgent [13], we employ Proximal Policy Optimization (PPO) [67] as the RL baseline. This approach alternates between sampling data through interactions with the environment and optimizing a \"surrogate\" objective function using stochastic gradient ascent. PPO is constrained to a limited set of skills. When applying PPO with sparse rewards, specific tasks such as “milk a cow\" and “shear a sheep\" present challenges due to the small size of the target object relative to the scene, and the low probability of random encounters. To address this, we introduce basic dense rewards to enhance learning efficacy in these tasks. It includes the CLIP [68] Reward, which encourages the agent to exhibit behaviors that align with the prompt [13]. Additionally, we incorporate a Distance Reward that provides dense reward signals to reach the target items [34]. It costs 3K steps for harvesting a log referring to Table. 16. For the diamond task, the evaluation ends when the diamond is found or the agent is dead. It will cost around 20K steps. The frame rate for the game is 30 fps. Further details can be found in the appendix C.\n\nTable 1: Comparison of several tasks selected from the Minedojo benchmark. Our RL-GPT achieves the highest success rate on all tasks. All values in our tables refer to the actual successful rate.   \n\n<html><body><table><tr><td>TASK</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MINEAGENT</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>--</td><td>:-</td><td></td><td>--</td></tr><tr><td>MINEAGENT (AUTOCRAFT)</td><td>0.00</td><td>0.03</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.46</td><td>0.50</td><td>0.33</td><td>0.35</td><td>0.00</td></tr><tr><td>PLAN4MC</td><td>0.30</td><td>0.30</td><td>0.53</td><td>0.37</td><td>0.17</td><td>0.83</td><td>0.53</td><td>0.43</td><td>0.33</td><td>0.17</td></tr><tr><td>RL-GPT</td><td>0.65</td><td>0.65</td><td>0.67</td><td>0.67</td><td>0.64</td><td>0.85</td><td>0.56</td><td>0.46</td><td>0.38</td><td>0.32</td></tr></table></body></html>\n\nTable 2: Main results in the challenging ObtainDiamond $\\bigoplus$ task in Minecraft. Existing strong baselines in ObtainDiamond either require expert data (VPT, DEPS), hand-crafted policies (DEPSOracle) for subtasks, or take huge number of environment steps to train (DreamerV3, VPT). Our method can automatically decompose and learn subtasks with only a little human prior, achieving ObtainDiamond with great sample efficiency.   \n\n<html><body><table><tr><td>METHOD</td><td>TYPE</td><td>SAMPLES</td><td>SUCCESS</td></tr><tr><td>DREAMERV3</td><td>RL</td><td>100M</td><td>2%</td></tr><tr><td>VPT</td><td>IL+RL</td><td>16.8B</td><td>20%</td></tr><tr><td>DEPS-BC</td><td>IL+LLM</td><td>--</td><td>0.6%</td></tr><tr><td>DEPS-ORACLE</td><td>LLM</td><td></td><td>60%</td></tr><tr><td>PLAN4MC</td><td>RL+LLM</td><td>7M</td><td>0%</td></tr><tr><td>RL-GPT</td><td>RL+LLM</td><td>3M</td><td>8%</td></tr></table></body></html>\n\n![](images/7837b8b824bc2ed51e632d7e0c341d050b09f98bc9996eaea82ae91442418593.jpg)  \nFigure 5: Demonstrations of how different agents learn to harvest a log. While both RL agent and LLM agent learn a single type of solution (RL or code-as-policy), our RL-GPT can reasonably decompose the task and correct how to learn each sub-action through the slow iteration process. RLGPT decomposes the task into “find a tree\" and “cut a log\", solving the former with code generation and the latter with RL. After a few iterations, it learns to provide RL with a necessary high-level action (attack 20 times) and completes the task with a high success rate. Best viewed by zooming in.\n\n# 4.3 Main Results\n\nMineDojo Benchmark Table 1 presents a comparative analysis between our RL-GPT and several baselines on selected MineDojo tasks. Notably, RL-GPT achieves the highest success rate among all baselines. All baselines underwent training with 10 million samples, and the checkpoint with the highest success rate was chosen for testing.\n\nTable 3: Ablation on the RL and Code-as-policy components and the iteration mechanism. RL-GPT outperforms its Pure RL and Pure Code counterparts. Given more iterations, RL-GPT gets better results.   \n\n<html><body><table><tr><td>Method</td><td></td><td></td><td></td><td></td></tr><tr><td>Pure RL</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>Pure Code</td><td>0.13</td><td>0.02</td><td>0.00</td><td>0.00</td></tr><tr><td>Ours (Zero-shot)</td><td>0.26</td><td>0.53</td><td>0.79</td><td>0.32</td></tr><tr><td>Ours (Iter-2 w/o SP)</td><td>0.26</td><td>0.53</td><td>0.79</td><td>0.30</td></tr><tr><td>Ours (Iter-2)</td><td>0.56</td><td>0.67</td><td>0.88</td><td>0.30</td></tr><tr><td>Ours (Iter-3)</td><td>0.65</td><td>0.67</td><td>0.93</td><td>0.32</td></tr></table></body></html>\n\nTable 4: Ablation on the agent structure.   \n\n<html><body><table><tr><td>Structure</td><td>鼎</td><td>入</td></tr><tr><td>One Agent</td><td>0.34</td><td>0.42</td></tr><tr><td>Slow + Fast</td><td>0.52</td><td>0.56</td></tr><tr><td>Slow+Fast+ Critic</td><td>0.65</td><td>0.67</td></tr></table></body></html>\n\nTable 5: Ablation on the RL interfaces.   \n\n<html><body><table><tr><td>Interface</td><td>Success ↑</td><td>Dead Loop ↓</td></tr><tr><td>Reward</td><td>0.418</td><td>~0.6</td></tr><tr><td>Action</td><td>0.585</td><td>~0.3</td></tr></table></body></html>\n\nMineAgent, as proposed in [13], combines PPO with CLIP Reward. However, naive PPO encounters difficulties in learning long-horizon tasks, such as crafting a bucket and obtaining milk from a cow, resulting in an almost $0 \\%$ success rate for MineAgent across all tasks. Another baseline, MineAgent with autocraft, as suggested in Plan4MC [34], incorporates crafting actions manually coded by humans. This alternative baseline achieves a $46 \\%$ success rate on the milking task, demonstrating the importance of code-as-policy. Our approach demonstrates superiority in coding actions beyond crafting, enabling us to achieve higher overall performance compared to these baselines.\n\nPlan4MC [34] breaks down the problem into two essential components: acquiring fundamental skills and planning based on these skills. While some skills are acquired through Reinforcement Learning (RL), Plan4MC outperforms MineAgent due to its reliance on an oracle task decomposition from the GPT planner. However, it cannot modify the action space of an RL training pipeline or flexibly decompose sub-actions. It is restricted to only three types of human-designed coded actions. Consequently, our method holds a distinct advantage in this context.\n\nIn tasks involving $\\diagup$ and , the agent is tasked with crafting a stick from scratch, necessitating the harvesting of a log. Our RL-GPT adeptly codes three actions for this: 1) Navigate to find a tree; 2) Attack 20 times; 3) Craft items. Notably, Action 2) can be seamlessly inserted into the action space. In contrast, Plan4MC is limited to coding craft actions only. This key distinction contributes to our method achieving higher scores in these tasks.\n\nTo arrive at the optimal code planning solution, RL-GPT undergoes a minimum of three iterations. As illustrated in Fig. 5, in the initial iteration, RL-GPT attempts to code every action involved in harvesting a log, yielding a $0 \\%$ success rate. After the first iteration, it decides to code navigation, aiming at the tree, and attacking 20 times. However, aiming at the tree proves too challenging for LLMs. As mentioned before, the agent will be instructed to further decompose the actions and give up difficult actions. By the third iteration, the agent correctly converges to the optimal solution—coding navigation and attacking, while leaving the rest to RL, resulting in higher performance.\n\nIn tasks involving crafting a wooden pickaxe $\\bar { \\lambda }$ and crafting a bed $\\bar { \\pmb { \\sigma } }$ , in addition to the previously mentioned actions, the agent needs to utilize the crafting table. While Plan4MC must learn this process, our method can directly code actions to place the crafting table on the ground, use it, and recycle it. Code-as-policy contributes to our method achieving a higher success rate in these tasks.\n\nIn tasks involving crafting a furnace $\\oplus$ and a stone pickaxe $\\bar { \\lambda }$ , in addition to the previously mentioned actions, the agent is further required to harvest stones. Plan4MC needs to learn an RL network to acquire the skill of attacking stones. RL-GPT proposes two potential solutions for coding additional actions. First, it can code to continuously attack a stone and insert this action into the action space. Second, since LLMs understand that stones are underground, the agent might choose to dig deep for several levels to obtain stones instead of navigating on the ground to find stones.\n\nIn crafting a milk bucket $\\circleddash$ and crafting wool , the primary challenge is crafting a bucket or shears. Since both RL-GPT and Plan4MC can code actions to craft without a crafting table, their performance is comparable. Similarly, obtaining beef $\\mathcal { O }$ and obtaining mutton $\\mathcal { J }$ needs navigating.\n\nObtainDiamond Challenge As shown in Tab. 2, we compare our method with existing competitive methods on the challenging ObtainDiamond task.\n\nDreamerV3 [26] leverages a world model to accelerate exploration but still requires a significant number of interactions. Despite the considerable expense of over 100 million samples for learning, it only achieves a $2 \\%$ success rate on the Diamond task from scratch.\n\nVPT [25] employs large-scale pre-training using YouTube videos to improve policy training efficiency. This strong baseline is trained on 80 GPUs for 6 days, achieving a $20 \\%$ success rate in obtaining a diamond and a $2 . 5 \\%$ success rate in crafting a diamond pickaxe.\n\nDEPS [29] suggests generating training data using a combination of GPT and human handcrafted code for planning and imitation learning. It attains a $0 . 6 \\%$ success rate on this task. Moreover, an oracle version, which directly executes human-written codes, achieves a $60 \\%$ success rate.\n\nPlan4MC [34] primarily focuses on crafting the stone pickaxe. Even with the inclusion of all human-designed actions from DEPS, it requires more than 7 million samples for training.\n\nOur RL-GPT attains an over $8 \\%$ success rate in the ObtainDiamond challenge by generating Python code and training a PPO RL neural network. Despite requiring some human-written code examples, our approach uses considerably fewer than DEPS. The final coded actions involve navigating on the ground, crafting items, digging to a specific level, and exploring the underground horizontally.\n\n# 4.4 Ablation Study\n\nFramework Structure In Tab. 4, we analyze the impact of the framework structure in RL-GPT, specifically examining different task assignments for various agents. Assigning all tasks to a single agent results in confusion due to the multitude of requirements, leading to a mere $34 \\%$ success rate in crafting a table. Additionally, comparing the 3rd and 4th rows emphasizes the crucial role of a critic agent in our pipeline. Properly assigning tasks to the fast, slow, and critic agents can improve the performance to $65 \\%$ . Slow agent faces difficulty in independently judging the suitability of actions based solely on environmental feedback and observation. Incorporating a critic agent facilitates more informed decision-making, especially when dealing with complex, context-dependent information.\n\nTwo-loop Iteration In Tab. 3, we ablate the importance of our two-loop iteration. Our iteration is to balance RL and code-as-policy to explore the bound of GPT’s coding ability. We can see that pure RL and pure code-as-policy only achieve a low success rate on these chosen tasks. Our method can improve the results although there is no iteration (zero-shot). In these three iterations, it shows that the successful rate increases. It proves that the two-loop iteration is a reasonable optimization choice. Qualitative results can be found in Fig. 5. Besides, we also compare the results with and without special prompts (SP) to encourage the LLMs to further decompose actions when facing coding difficulty. It shows that suitable prompts are also essential for optimization.\n\nRL Interface Recent works [51, 52] explore the use of LLMs for RL reward design, presenting an alternative approach to combining RL and code-as-policy. With slight modifications, our fast agent can also generate code to design the reward function. However, as previously analyzed, reconstructing the action space proves more efficient than designing the reward function, assuming LLMs understand the necessary actions. Tab. 5 compares our method with the reward design approach. Our method achieves a higher average success rate and lower dead loop ratio on our selected MineDojo tasks.\n\n# 5 Conclusion\n\nIn conclusion, we propose RL-GPT, a novel approach that integrates Large Language Models (LLMs) and Reinforcement Learning (RL) to empower LLMs agents in practicing tasks within complex, embodied environments. Our two-level hierarchical framework divides the task into high-level coding and low-level RL-based actions, leveraging the strengths of both approaches. RL-GPT exhibits superior efficiency compared to traditional RL methods and existing GPT agents, achieving remarkable performance in the challenging Minecraft environment.\n\nAcknowlegdement This work was supported in part by the Research Grants Council under the Areas of Excellence scheme grant AoE/E-601/22-R.",
    "summary": "{\n  \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n大语言模型（LLMs）在利用编码使用各种工具方面表现出色，但在处理复杂逻辑和精确控制时存在局限性。在具身任务中，高级规划可直接编码，而低级动作通常需要特定任务的细化，如强化学习（RL）。传统的将LLMs与RL结合的方法需要大量特定领域的数据、专家演示和访问LLMs的参数，速度慢且资源密集，大多数方法仍停留在“文字游戏”层面，无法处理复杂的具身任务。解决LLMs与RL有效结合以处理复杂具身任务的问题，对于提升AI在开放世界环境中执行任务的能力具有重要意义。\\n\\n**方法概述**\\n论文提出了一种两级分层框架RL - GPT，由慢代理和快代理组成。慢代理分析适合编码的动作，快代理执行编码任务，该框架将编码部分和学习部分集成在一起，有效专注于特定任务。此外，对于复杂的长周期任务，还引入了使用GPT - 4的任务规划器先进行任务分解。\\n\\n**主要贡献与效果**\\n- 提出让LLMs代理使用RL训练管道作为工具，在MineDojo基准测试的所有任务中，RL - GPT取得了最高的成功率。在ObtainDiamond挑战中，以300万样本获得了8%的成功率，而DreamerV3用1亿样本成功率为2%，VPT用168亿样本成功率为20%，Plan4MC用700万样本成功率为0%。\\n- 开发了两级分层框架，能够确定任务中哪些动作应该被学习。该框架通过双循环迭代优化慢代理和快代理，提高了整体性能。\\n- 首次将高级GPT编码动作纳入RL动作空间，提高了学习效率，使RL - GPT在处理复杂具身任务时表现更优。\",\n  \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n将大语言模型（LLMs）和强化学习（RL）相结合，通过两级分层框架，将高级规划和低级动作分离到不同的代理中，让每个代理专注于特定任务。利用LLMs的先验知识、推理能力和编码能力进行任务分解、高级规划和部分动作编码，利用RL进行低级动作的学习和优化，从而提高在复杂具身环境中执行任务的效率。该方法有效的原因在于将不同能力的模块分工合作，避免了LLMs在低级动作学习上的不足和RL在高级规划上的困难，充分发挥了两者的优势。\\n\\n**创新点**\\n先前的工作要么需要大量专家数据集进行训练，要么在处理长周期任务时样本效率低，或者依赖手动设计的控制器或代码接口，无法很好地处理复杂具身任务。与先前工作相比，RL - GPT将RL训练管道作为工具供LLMs代理使用，首次将高级GPT编码动作纳入RL动作空间，通过两级分层框架和双循环迭代优化代理，能更自动、高效地学习任务。同时，引入任务规划器处理复杂长周期任务，增强了框架的适应性。\\n\\n**具体实现步骤**\\n1. **决策过程**：框架采用决策过程来确定动作应使用代码还是RL执行。\\n2. **慢代理动作规划**：使用GPT - 4作为慢代理，将给定任务分解为子动作，确定每个子动作是否可直接通过代码实现。慢代理不进行低级编码任务，仅提供高级文本指令给快代理。例如，对于制作木制镐的动作，慢代理发现收获原木的代码编写因API信息不足而有挑战，需要RL实现；而用工作台制作物品等简单动作可直接编码执行。\\n3. **快代理代码实现与RL集成**：同样使用GPT - 4作为快代理，将慢代理的指令转换为Python代码。对于可完全用代码实现的子动作直接执行；对于缺乏明确起始条件的子动作，使用时间抽象技术将代码集成到RL实现中，将高级动作插入RL动作空间。快代理根据环境反馈迭代修正代码。\\n4. **双循环迭代优化**：引入批评代理（可使用GPT - 3.5或GPT - 4）来优化快代理和慢代理。快代理根据环境反馈和批评代理的评价迭代优化代码，目标是生成无错误的Python编码子动作。慢代理的迭代包含快代理的迭代步骤，根据批评代理的输出分解任务并调整动作规划。\\n5. **任务规划器**：对于复杂的长周期任务，如ObtainDiamond，使用GPT - 4作为任务规划器，先分解任务，组织子任务供RL - GPT完成。\\n\\n**案例解析**\\n在收获原木的任务中，初始迭代时，RL - GPT尝试对收获原木涉及的每个动作进行编码，成功率为0%。第一次迭代后，决定对导航、瞄准树和攻击20次进行编码，但瞄准树对LLMs来说太具挑战性。到第三次迭代，代理正确收敛到最优解决方案，即对导航和攻击进行编码，其余部分交给RL，从而获得了更高的成功率。\",\n  \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- MineAgent：结合PPO与CLIP Reward。\\n- MineAgent with autocraft：在MineAgent基础上，结合人类手动编码的制作动作。\\n- Plan4MC：将问题分解为获取基本技能和基于这些技能进行规划两个部分，部分技能通过RL获取。\\n- DreamerV3：利用世界模型加速探索的RL方法。\\n- VPT：通过YouTube视频进行大规模预训练以提高策略训练效率的方法。\\n- DEPS - BC：结合GPT和人类手工代码进行规划和模仿学习。\\n- DEPS - ORACLE：直接执行人类编写的代码。\\n\\n**性能对比**\\n*   **在MineDojo基准测试的成功率指标上**：本文方法RL - GPT在所有选定任务上取得了最高成功率。MineAgent在所有任务上的成功率几乎为0%；MineAgent with autocraft在挤奶任务上成功率为46%；Plan4MC在不同任务上有一定成功率，但RL - GPT在多数任务上优于Plan4MC。例如，在收获原木相关任务中，RL - GPT能编码导航、攻击等多个动作，而Plan4MC只能编码工艺动作，RL - GPT表现更优；在制作木镐和床等任务中，RL - GPT可直接编码放置、使用和回收工作台等动作，而Plan4MC需学习该过程，RL - GPT成功率更高。在某些任务中，RL - GPT的成功率比MineAgent高出很多，比Plan4MC也有明显提升。\\n*   **在ObtainDiamond挑战的成功率和样本数量指标上**：在成功率方面，DreamerV3使用1亿个样本，成功率为2%；VPT使用168亿个样本，成功率为20%；DEPS - BC成功率为0.6%；DEPS - ORACLE成功率为60%；Plan4MC使用700万个样本，成功率为0%；RL - GPT使用300万个样本，成功率为8%。RL - GPT在使用较少样本的情况下取得了相对较高的成功率，在样本效率上有显著优势，虽然成功率与DEPS - ORACLE相比还有差距，但使用的样本量远少于其他方法。\",\n  \"keywords\": \"### 关键词\\n\\n- 大语言模型与强化学习集成 (Integration of Large Language Models and Reinforcement Learning, N/A)\\n- 具身任务处理 (Handling Embodied Tasks, N/A)\\n- RL - GPT框架 (RL - GPT Framework, RL - GPT)\\n- Minecraft游戏 (Minecraft Game, N/A)\"\n}"
}