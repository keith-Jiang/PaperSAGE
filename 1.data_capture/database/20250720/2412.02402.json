{
    "link": "https://arxiv.org/abs/2412.02402",
    "pdf_link": "https://arxiv.org/pdf/2412.02402",
    "title": "RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation",
    "authors": [
        "Changli Wu",
        "Qi Chen",
        "Jiayi Ji",
        "Haowei Wang",
        "Yiwei Ma",
        "You Huang",
        "Gen Luo",
        "Hao Fei",
        "Xiaoshuai Sun",
        "Rongrong Ji"
    ],
    "institutions": [
        "Xiamen University",
        "Tencent",
        "National University of Singapore"
    ],
    "publication_date": "2024-12-03",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 4,
    "influential_citation_count": 0,
    "paper_content": "# RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation\n\nChangli $\\mathbf { W _ { u } } ^ { 1 , 2 * }$ , Qi Chen1∗, Jiayi $\\mathbf { J i } ^ { 1 , 4 }$ , Haowei $\\mathbf { W a n g ^ { 3 } }$ , Yiwei $\\mathbf { M } \\mathbf { a } ^ { 1 }$ , You Huang1, Gen Luo1, Hao Fei4, Xiaoshuai $\\mathbf { S u n } ^ { 1 }$ , Rongrong $\\mathbf { J i ^ { \\mathrm { 1 \\dagger } } }$ 1 Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China 2 Shanghai Innovation Institute, Shanghai, P.R. China 3 Youtu Lab, Tencent, Shanghai, P.R. China 4 National University of Singapore\n\n# Abstract\n\n3D Referring Expression Segmentation (3D-RES) aims to segment 3D objects by correlating referring expressions with point clouds. However, traditional approaches frequently encounter issues like over-segmentation or mis-segmentation, due to insufficient emphasis on spatial information of instances. In this paper, we introduce a Rule-Guided Spatial Awareness Network (RG-SAN) by utilizing solely the spatial information of the target instance for supervision. This approach enables the network to accurately depict the spatial relationships among all entities described in the text, thus enhancing the reasoning capabilities. The RG-SAN consists of the Text-driven Localization Module (TLM) and the Rule-guided Weak Supervision (RWS) strategy. The TLM initially locates all mentioned instances and iteratively refines their positional information. The RWS strategy, acknowledging that only target objects have supervised positional information, employs dependency tree rules to precisely guide the core instance’s positioning. Extensive testing on the ScanRefer benchmark has shown that RG-SAN not only establishes new performance benchmarks, with an mIoU increase of 5.1 points, but also exhibits significant improvements in robustness when processing descriptions with spatial ambiguity. All codes are available at https://github.com/sosppxo/RG-SAN.\n\n# 1 Introduction\n\n3D Referring Expression Segmentation (3D-RES) is an emerging field that segments 3D objects in point cloud scenes based on given referring expressions [24]. Gaining significant attention for its applications in autonomous robotics, human-machine interaction, and self-driving systems, 3D-RES demands a deeper understanding than 3D Referring Expression Comprehension (3D-REC) [5, 71, 1, 73, 68], which focuses only on locating the referring objects via bounding boxes. 3D-RES, on the other hand, requires identifying instances and providing precise 3D masks.\n\nEarly 3D-RES approaches [24, 71] adopted a two-stage paradigm, starting with an independent textagnostic segmentation model for generating instance proposals, followed by linking these proposals with textual descriptions. This paradigm, separating segmentation and matching, proved suboptimal in performance and efficiency. Recent explorations have shifted towards an end-to-end paradigm. For instance, 3D-STMN [63] achieved efficient segmentation by directly matching superpoints with text, while 3DRefTR [41] integrated 3D-RES and 3D-REC into a unified framework using a multi-task approach, boosting inference in both tasks. Despite these advancements, limitations persist, primarily due to over-reliance on textual reasoning and insufficient modeling of spatial relationships between instances. For example, as shown in Fig. 1, without spatial modeling, it’s challenging to understand and correctly segment the intended chair in scenarios involving complex spatial terms like “far away”.\n\nTo tackle this issue, the core is to assist textual reasoning by modeling the spatial relationships of core instances. By effectively identifying these spatial relationships within expressions, a substantial improvement can be achieved in comprehending spatial arrangements. Nevertheless, this endeavor is not without its challenges. While accurate positional information is crucial for ensuring precise modeling of spatial relationships, accurately regressing instance positions from textual information is far from a simple task. Furthermore, our available positional information is limited to the target instance, leaving us without supervisory signals for other instances referenced in the expression.\n\nTo overcome these challenges, we propose the novel Rule-Guided Spatial Awareness Network (RG-SAN), utilizing the spatial information of the target instance for supervision. This enables the network to accurately depict spatial relationships among all text-described entities, thereby significantly enhancing the model’s inference and pointing capabilities. RG-SAN consists of two main components: the Text-driven Localization Module (TLM) and the Rule-guided Weak Supervision (RWS) strategy. TLM initially lo\n\n![](images/178f4ecfd548ee01ab9f8635d80696e0d6d2f7d47588f4652c62fc3869ad6b0b.jpg)  \nFigure 1: Illustration with a target object and multiple auxiliary objects, associated with a referring expression. The target marked in green represents the main referred instance, while targets in other colors indicate other mentioned entities. This visual highlights the challenge of effectively completing semantic reasoning in the absence of spatial inference.\n\ncates all mentioned instances and iteratively refines their positions, ensuring continuous improvement in location accuracy. RWS, leveraging dependency tree rules, precisely guides the positioning of core instances. This focused supervision significantly improves the handling of spatial ambiguities in referring expressions. Extensive testing on the ScanRefer benchmark shows that RG-SAN not only sets new performance standards, with a mIoU increase of 5.1 points, but also greatly enhances robustness in processing spatially ambiguous descriptions.\n\nTo sum up, our main contributions are as follows:\n\n• We introduce RG-SAN, a novel approach for modeling spatial relationships among all entities in expressions, which enhances the model’s referring ability in 3D-RES.   \n• We propose the TLM for precise localization of all instances mentioned in expressions, and RWS, utilizing only the target instance’s location for supervising the spatial positioning of all instances.   \n• Extensive experiments on the ScanRefer benchmark demonstrate the effectiveness of the proposed RG-SAN, showing significant improvements in performance and robustness in 3D-RES tasks.\n\n# 2 Related Work\n\n# 2.1 3D Referring Expression Comprehension and Referring Expression Segmentation\n\nReferring Expression Comprehension (REC) is proposed to locate the referred target from a short description of visual space by bounding boxes [72, 57, 29], which is part of vision-language tasks [12, 10, 11, 18, 67, 66, 15]. Recent works in 3D-REC can be divided into two parts, two-stage and single-stage. As for two-stage methods [5, 1, 73, 71, 70, 24, 13], 3D object proposals are generated directly from ground-truth [1] or extracted by a pre-trained 3D object detector [50] in the first stage, and then assigned to language in the second stage. In the other way, some methods adopt a one-stage paradigm [45, 26, 68, 64], enabling end-to-end training.\n\nReferring Expression Segmentation (RES) need fine-grained vision-language alignment [36, 37, 16, 35], proposed to locate the referred target by masks [27, 59, 25]. TGNN [24] introduce 3D-RES by extending the bounding box annotations of ScanRefer [5] to masks by incorporating the instance masks from ScanNet and proposed a two-stage pipeline. Further, 3D-STMN [63] proposed an end-to-end method that matches the text and superpoints to get the 3D segmentation of the target object directly.\n\n# 2.2 3D Human-AI Interaction\n\nScanQA [3] has notably advanced visual question answering in 3D scenes, enhancing the human-AI interaction experience. Meanwhile, 3D-LLM [21], 3D-VisTA [75], NaviLLM [74], and BridgeQA [49] have further propelled this task. Li et al. [38, 39], Lu et al. [44] have explored how AI understands human instructions like gestures and language to locate targets. 3D-VisTA [75] introduced a new paradigm for large-scale 3D vision-language pre-training, greatly enhancing AI’s understanding of 3D vision-language and advancing various downstream tasks. Works like 3D-LLM [21], Chat3D [62, 22], NaviLLM [74] and Scene-LLM [14] have extended the capabilities of multimodal large language models to the 3D realm, endowing embodied intelligence with the rich knowledge and capabilities of LLMs, thus ushering in the era of large models in Human-AI Interaction.\n\n# 2.3 Weakly Supervision in Vision-and-Language\n\nIn the field of Vision Language, weakly supervised [33, 42, 34, 4] have gained significant attention and great progress. These approaches aim to tackle the challenge of limited or incomplete annotations by leveraging alternative supervised data or weakly labeled data. For weakly supervised visual question answering (VQA), Kervadec et al. [28] employ weak supervision in the form of object-word alignment as a pre-training task. Trott et al. [60] use object counts in images as weak supervision to guide VQA for counting-based questions. Gokhale et al. [17] employ logical connective rules to augment training datasets for yes-no questions. Weakly supervision from captions has also been employed for visual grounding tasks [9, 48, 2] recently. Especially, for RES, some methods [33, 42] localize the target object only using readily available image-text pairs.\n\n# 3 Method\n\nIn this section, we provide a comprehensive overview of the RG-SAN. The framework is illustrated in Fig. 2. First, the features of visual and linguistic modalities are extracted in parallel (Sec. 3.1). Next, we demonstrate the process of TLM (Sec. 3.2.1). Finally, we outline the RWS and the training objectives (Sec. 3.3).\n\n# 3.1 Feature Extraction\n\n# 3.1.1 Visual Encoding\n\nGiven a point cloud scene $\\mathbf { P } _ { c l o u d } \\in \\mathbb { R } ^ { N _ { p } \\times ( 3 + F ) }$ with $\\mathcal { N } _ { p }$ points. Each point comes with 3D coordinates along with an $F$ -dimensional auxiliary feature that includes RGB, normal vectors, among others. We first employ a Sparse 3D U-Net [19] to extract point-wise features, represented as $\\hat { \\mathbf { P } } _ { \\mathbf { c l o u d } } \\in \\mathbb { R } ^ {  { N _ { p } } \\times  { C _ { p } } }$ . Then, we follow Sun et al. [58] and Wu et al. [63] to obtain $\\mathcal { N } _ { s }$ superpoints $\\{ \\mathcal { K } _ { i } \\} _ { i = 1 } ^ { \\mathcal { N } _ { s } }$ [32] from the original point cloud. Finally, we directly feed point-wise features $\\hat { \\mathbf { P } } _ { \\mathbf { c l o u d } }$ into superpoint pooling layer based on $\\{ \\mathcal { K } _ { i } \\} _ { i = 1 } ^ { \\mathcal { N } _ { s } }$ to obtain the superpoint-level features $\\mathbf { S } _ { p } \\in \\mathbb { R } ^ {  { N _ { s } } \\times  { C _ { p } } }$ .\n\n# 3.1.2 Linguistic Encoding\n\nGiven a free-form plain text description of the target object, consisting of $\\mathcal { N } _ { t }$ words $\\{ c _ { i } \\} _ { i = 1 } ^ { \\mathcal { N } _ { t } }$ , we utilize a pre-trained MPNet model [56] to extract $C _ { t }$ -dimensional word-level embeddings, represented as E0 RNt×Ct.\n\n![](images/4088ae28526e8f790ec9229eaf2bacfd63b6ad3b2c8b24da15a387598b6b6618.jpg)  \nFigure 2: An overview of the proposed RG-SAN. This model analyzes a point cloud and a textual description with $\\mathcal { N } _ { t }$ tokens, extracting superpoints and word-level features. The TLM assigns spatial positions to tokens, facilitating multimodal fusion. The RWS strategy enables the model to learn the positions of all mentioned entities using only the supervision of the target position.\n\n# 3.2 Context-driven Spatial Awareness\n\nIn this section, we address a key limitation in prior works that interact point clouds with text without considering spatial positioning [63, 45, 68]. Unlike these methods, which often lose spatial information due to unordered point cloud features, leading to ambiguous spatial relationship understanding, our approach is distinct. In 3D-RES, spatial information is inherently sparse and dynamic, depending on the specific target object described in the text, rather than the dense, static sampling of an entire point cloud scene [31].\n\nTo address this issue, we propose to facilitate interactions between textual entities and point clouds within 3D space, rather than merely at the semantic level. Specifically, our objective is to fully leverage semantic and spatial contextual information to accurately predict the spatial positions of all mentioned nouns within the point cloud.\n\nTherefore, we introduce the Text-driven Localization Module (TLM) to initialize the positions of entity nouns in the text and continuously update and refine these positions through iterative multimodal interactions.\n\n# 3.2.1 Text-driven Localization Module\n\nGiven the superpoint features $\\mathbf { S } _ { p }$ and word embeddings $\\mathbf { E } _ { 0 }$ , we first project the features into the same dimension, and enhance the word-level embeddings by Dependency-Driven Interaction (DDI), following ${ \\sf W } { \\sf u }$ et al. [63]:\n\n$$\n\\begin{array} { r } { \\hat { \\mathbf { E } } _ { 0 } = \\mathbf { D } \\mathbf { D } \\mathbf { I } ( \\mathbf { E } _ { 0 } \\mathbf { W } _ { l a n g } ) , \\quad \\hat { \\mathbf { S } } = \\mathbf { S } _ { p } \\mathbf { W } _ { v i s } , } \\end{array}\n$$\n\nwhere $\\mathbf { W } _ { l a n g } \\in \\mathbb { R } ^ { C _ { t } \\times D }$ and $\\mathbf { W } _ { v i s } \\in \\mathbb { R } ^ { C _ { p } \\times D }$ denote learnable parameters, and the subscript of $\\mathbf { E }$ and $\\hat { \\mathbf { E } }$ represents the round number.\n\nText-driven Initialization. The key is to map the text into 3D geometric space in a meaningful way. Specifically, we enhance entity position prediction within point clouds through an interactive text-point cloud process. We do this by calculating feature similarity across modalities to accurately\n\nestimate the spatial probability distribution for each mentioned entity:\n\n$$\n\\begin{array} { r } { \\mathbf { E } = \\hat { \\mathbf { E } } _ { 0 } \\mathbf { W } _ { E } , \\quad \\mathbf { S } = \\hat { \\mathbf { S } } \\mathbf { W } _ { S } , } \\\\ { A _ { i j } = \\cfrac { \\operatorname { S i m } ( \\mathbf { E } _ { i } , \\mathbf { S } _ { j } ) } { \\sum _ { j = 1 } ^ { N _ { s } } \\operatorname { S i m } ( \\mathbf { E } _ { i } , \\mathbf { S } _ { j } ) } , } \\end{array}\n$$\n\nwhere $\\hat { \\mathbf { E } } _ { 0 }$ denotes the initial word embeddings, $\\hat { \\bf S }$ denotes the superpoint features, $\\mathbf { W } _ { E } , \\mathbf { W } _ { S } \\in \\mathbb { R } ^ { D \\times D }$ are learnable parameters, $A _ { i j } \\in \\mathbb { R }$ denotes the probability of the $i$ -th word token being located at the $j$ -th superpoint, and $\\mathrm { S i m } \\bar { ( \\cdot , \\cdot ) }$ represents the similarity function, which in this case is defined as $\\mathrm { S i m } \\left( \\mathbf { E } , \\mathbf { S } \\right) { = } \\exp ( \\mathbf { E } \\mathbf { S } ^ { T } / \\sqrt { D } )$ .\n\nFollowing this, we utilize the spatial probability distribution $A$ to predict the approximate positions of the mentioned entities, as well as their corresponding representations:\n\n$$\n\\mathbf { P } _ { 0 , i } ^ { t } = \\sum _ { j = 1 } ^ { \\mathcal { N } _ { s } } A _ { i j } \\mathbf { P } _ { j } ^ { s } ,\n$$\n\n$$\n\\mathbf { S } _ { v } = \\hat { \\mathbf { S } } \\mathbf { W } _ { v } , \\quad \\hat { \\mathbf { E } } _ { 0 , i } = \\sum _ { j = 1 } ^ { \\mathcal { N } _ { s } } A _ { i j } \\mathbf { S } _ { v , j } ,\n$$\n\nwhere $\\mathbf { P } _ { j } ^ { s }$ is the position of the $j$ -th superpoint, $\\mathbf { P } _ { 0 , i } ^ { t }$ is the initial spatial position of $i$ -th word token which will be refined iteratively as formulated in Sec. 3.2.2, $\\mathbf { W } _ { v } \\in \\mathbb { R } ^ { D \\times D }$ denotes learnable parameters, and $\\hat { \\mathbf { E } } _ { 0 , i }$ denotes the updated representation of the $i$ -th word token. The sharing of distribution $A$ during centroid computation allows the entity representations to benefit from the guidance provided by spatial information, leading to a more accurate understanding of the 3D spatial relationships. Subsequently, the text and point clouds undergo multiple rounds of multimodal interactions, continually updating the embeddings and positions of the entities.\n\nIterative Position Refinement. After $l$ -round multimodal interactions, the word tokens $\\hat { \\mathbf { E } } _ { l }$ , referred to as textual segment kernels, become increasingly precise, theoretically resulting in more accurate position predictions. A straightforward approach would involve replicating the initial interaction method by regressing position information in each round. However, following the methodologies of Redmon et al. [54] and Lai et al. [31], rather than directly optimizing the final position, we adopt a more manageable strategy of iteratively learning offsets. To this end, we refine the positions of textual tokens based on the evolving textual segment kernels. As depicted in Fig. 2, we employ a Multilayer Perceptron (MLP) to predict a position offset $\\Delta \\mathbf { P } _ { l } ^ { t } = \\mathbf { M } \\mathbf { L } \\mathbf { P } ( \\hat { \\mathbf { E } } _ { l + 1 } ) \\in \\mathbb { R } ^ { \\mathcal { N } t \\times 3 }$ from the updated textual segment kernels $\\hat { \\mathbf { E } } _ { l + 1 }$ . This offset is then added to the previous textual positions $\\mathbf { P } _ { l } ^ { t }$ :\n\n$$\n\\mathbf { P } _ { l + 1 } ^ { t } = \\mathbf { P } _ { l } ^ { t } + \\Delta \\mathbf { P } _ { l } ^ { t } .\n$$\n\nThis method allows for gradual refinement of position predictions, making the optimization process more effective and leading to progressively more accurate positioning with each iteration.\n\n# 3.2.2 Spatial Awareness Aggregation\n\nOnce the positions of noun entities are obtained, techniques like positional encoding [61, 65, 31, 30, 6] can be used to further refine the positions.\n\nAbsolute Positional Encoding (APE). To initiate, we follow the approach of the original transformer [61] to encoded the positions of both superpoints and text tokens to obtain positional encodings $\\mathbf { B } _ { l } ^ { s } \\in \\mathbb { R } ^ { \\mathcal { N } _ { s } \\times D }$ and $\\mathbf { B } _ { l } ^ { t } \\in \\mathbb { R } ^ { \\mathbf { \\tilde { \\mathcal { N } } } _ { t } \\times D }$ using absolute positional encoding (APE):\n\n$$\n\\mathbf { B } _ { l } ^ { s } = \\mathbf { A } \\mathbf { P } \\mathbf { E } ( \\mathbf { P } _ { l } ^ { s } ) , \\quad \\mathbf { B } _ { l } ^ { t } = \\mathbf { A } \\mathbf { P } \\mathbf { E } ( \\mathbf { P } _ { l } ^ { t } ) .\n$$\n\nThese positional encodings facilitate spatial-aware self-attention in the textural segment kernels $\\hat { \\mathbf { E } } _ { l }$ :\n\n$$\n\\dot { \\bf E } _ { l } = { \\bf A t t e n t i o n } ( \\hat { \\bf E } _ { l } + { \\bf B } _ { l } ^ { t } , \\hat { \\bf E } _ { l } + { \\bf B } _ { l } ^ { t } , \\hat { \\bf E } _ { l } ) ,\n$$\n\nwhere Attention $( \\cdot )$ uses the technique of Vaswani et al. [61] and $\\mathbf { B } _ { l } ^ { t }$ denotes the absolute positional encoding of $\\hat { \\mathbf { E } } _ { l }$ .\n\nNext, we enhance textual and superpoint features with absolute positional encoding, and use them as Queries and Keys for subsequent multimodal aggregation:\n\n$$\n\\begin{array} { r } { \\begin{array} { l l } { \\mathbf { Q } = \\mathrm { C o n c a t } ( \\dot { \\mathbf { E } } _ { l } , \\mathbf { B } _ { l } ^ { t } ) \\mathbf { W } _ { q u e r y } , } \\\\ { \\mathbf { K } = \\mathrm { C o n c a t } ( \\hat { \\mathbf { S } } , \\mathbf { B } _ { l } ^ { s } ) \\mathbf { W } _ { k e y } , } \\end{array} } \\end{array}\n$$\n\nwhere $\\mathbf { B } _ { l } ^ { t } \\in \\mathbb { R } ^ { N _ { t } \\times D } , \\mathbf { B } _ { l } ^ { s } \\in \\mathbb { R } ^ { N _ { s } \\times D }$ denote the absolute positional encoding of segmentation kernels and superpoints, respectively, and $\\mathbf { W } _ { q u e r y }$ , $\\mathbf { W } _ { k e y } \\in \\mathbb { R } ^ { 2 D \\times 2 D }$ denote learnable parameters.\n\nRelative positional encoding (RPE). For the further interaction with superpoint features, we adopt well-established relative positional encoding techniques [65, 31, 30, 6], such as Table-based RPE [65, 31] and 5D Euclidean RPE [6], which are formalized as follows:\n\n$$\n\\mathbf { B } _ { l } ^ { r } [ i , j ] = \\mathrm { R P E } ( \\mathbf { Q } [ i ] + \\mathbf { K } [ j ] ) ,\n$$\n\nwhere $\\mathbf { B } _ { l } ^ { r } [ i , j ] \\in \\mathbb { R }$ denotes the relative positional bias of the $i$ -th $\\mathbf { Q }$ relative to the $j$ -th $\\mathbf { K }$ , RPE(·) denotes the operation of relative positional bias and $[ \\cdot ]$ denotes the indexing operation.\n\nThus, we can perform multimodal aggregation enhanced with relative positional encoding:\n\n$$\n\\hat { \\mathbf { E } } _ { l + 1 } = \\mathrm { s o f t m a x } \\left( \\frac { \\mathbf { Q } \\cdot \\mathbf { K } ^ { T } } { \\sqrt { D } } + \\mathbf { B } _ { l } ^ { r } \\right) \\cdot ( \\hat { \\mathbf { S } } \\mathbf { W } _ { v a l } ) ,\n$$\n\nwhere $\\mathbf { W } _ { v a l } \\in \\mathbb { R } ^ { D \\times D }$ denote learnable parameters, $\\mathbf { B } _ { l } ^ { r } \\in \\mathbb { R } ^ { N _ { t } \\times N _ { s } }$ denotes the relative positional bias, and $\\hat { \\mathbf { E } } _ { l + 1 }$ denotes the updated segmentation kernels. This methodology significantly enriches the interaction between linguistic and 3D visual data, enabling more nuanced spatial understanding in our model.\n\n# 3.3 Rule-guided Weak Supervision\n\n# 3.3.1 Rule-guided Target Selection\n\nIn the preceding sections, we initially predicted the locations of all entities mentioned in the text. Ideally, supervised training would require position labels for each entity. However, we only have access to the location information of the target instance. This constraint leads us to adopt a weak supervision approach, focusing solely on the position of the referring instance for training. This approach introduces a significant challenge: accurately identifying the referring instance among the mentioned nouns. To address this, we utilize a pre-processed dependency tree, as outlined in Manning et al. [46], to accurately pinpoint the core noun, typically the subject of the sentence. We have developed a set of manual rules, based on this more general dependency tree, to enhance the identification process. These rules are specifically designed to guide the accurate positioning of core instances. The implementation of these rules is outlined in Algorithm 1.\n\n# Algorithm 1 Rule-guided Target Selection\n\nInput: The dependency tree $\\mathcal { G } = ( \\nu , \\mathcal { E } )$ of the textual description, where $\\mathcal { V } = \\{ \\mathrm { t o k e n } \\}$ denotes the set of nodes, $\\begin{array} { r } { \\mathcal { E } = \\left\\{ \\begin{array} { r l } \\end{array} \\right. } \\end{array}$ (relation, head, tail) $\\}$ denotes the set of relations between nodes.   \nOutput: The index $i$ of Target Instance node $\\mathcal { V } ^ { t g t }$ 1: Initialization $i$ to the root: $i = 0$ 2: find $\\mathcal { E } _ { i }$ with $\\nu _ { i }$ as its head   \n3: if ( $\\mathcal { E } _ { i } \\in \\{$ {nsubj, compound}) & (Vi ∈/ {which, that}) then   \n4: $i  \\mathcal { E } _ { i }$ ’s tail index 5: end if 6: if $\\mathcal { V } _ { i } \\in \\{$ there, this, it, object then 7: find $\\mathcal { E } _ { i }$ with $\\nu _ { i }$ as its head 8: $i  \\mathcal { E } _ { i }$ ’s tail index   \n9: end if   \n10: if $\\begin{array} { r } { \\mathcal { V } _ { i } \\in \\{ \\begin{array} { r l r l } \\end{array} } \\end{array}$ {set, sets, color, shape} then   \n11: find the first $\\mathcal { E } _ { i }$ ’s relation $\\in$ {compound, nmod, dep}   \n12: $i  \\mathcal { E } _ { i }$ ’s head index   \n13: end if\n\n# 3.3.2 Training Objectives\n\nGiven the index of the target instance, we can directly obtain the corresponding segment kernel $\\hat { \\mathbf { E } } _ { l + 1 } ^ { t g t } \\in \\mathbb { R } ^ { D }$ and position $\\mathbf { P } _ { l + 1 } ^ { t g t }$ , which are then supervised by the target ground truth.\n\nThen we perform matrix multiplication between $\\hat { \\mathbf { E } } _ { l + 1 } ^ { t g t }$ and $\\hat { \\bf S }$ to get the predicted instance response maps, which can be formulated as\n\n$$\n\\begin{array} { r } { \\mathbf { M } _ { l + 1 } = \\sigma ( \\hat { \\mathbf { E } } _ { l + 1 } ^ { t g t } \\cdot \\hat { \\mathbf { S } } ^ { T } ) , } \\\\ { \\mathbf { M } \\mathbf { a } \\mathbf { s } \\mathbf { k } _ { l + 1 } = \\mathbf { M } _ { l + 1 } > 0 . 5 , } \\end{array}\n$$\n\nwhere $\\mathbf { M } _ { l + 1 } \\in \\mathbb { R } ^ { N _ { s } }$ , $\\mathbf { M a s k } _ { l + 1 } \\in \\{ 0 , 1 \\} ^ { \\mathcal { N } _ { s } }$ are the predicted response map and the instance mask corresponding to the target.\n\nGiven ground-truth binary mask of the referring expression $\\mathbf { Y } \\in \\{ 0 , 1 \\} ^ { \\mathcal { N } _ { p } }$ , we get the corresponding superpoint mask $\\mathbf { Y } ^ { s } \\in \\{ \\bar { 0 } , 1 \\} ^ { \\mathcal { N } _ { s } }$ by superpoint pooling follewed by a 0.5-threshold binarization, and then we apply the binary cross-entropy (BCE) loss on the final response map $\\mathbf { M } _ { l + 1 }$ following Sun et al. [58]. The operation can be written as:\n\n$$\n\\begin{array} { r l } & { \\mathbf { Y } _ { i } ^ { s } = \\mathbb { I } ( \\sigma ( \\operatorname { A v g P o o l } ( \\mathbf { Y } , \\boldsymbol { \\mathcal { K } } _ { i } ) ) ) , } \\\\ & { \\mathcal { L } _ { b c e } = \\operatorname { B C E } ( \\mathbf { M } _ { l + 1 } , \\mathbf { Y } ^ { s } ) , } \\end{array}\n$$\n\nwhere $\\mathrm { \\mathbf { A v g P o o l } } ( \\cdot )$ denotes the superpoint average pooling operation, and $\\mathbf { Y } _ { i } ^ { s }$ denotes the binarized mask value of the $i$ -th superpoint $\\textstyle { \\mathcal { K } } _ { i }$ . $\\mathbb { I } ( \\cdot )$ indicates whether the mask value is higher than $50 \\%$ .\n\nTo tackle foreground-background sample imbalance, we can use Dice loss [47]:\n\n$$\n\\mathcal { L } _ { d i c e } = \\mathrm { D I C E } ( \\mathbf { M } _ { l + 1 } , \\mathbf { Y } ^ { s } ) .\n$$\n\nTo supervise the position $\\mathbf { P } _ { l + 1 } ^ { t g t }$ , we use the center of the superpoints of the target instance $\\mathbf { P } ^ { g t }$\n\n$$\n\\mathcal { L } _ { p o s } = \\mathrm { L } 1 ( \\mathbf { P } _ { l + 1 } ^ { t g t } , \\mathbf { P } ^ { g t } ) .\n$$\n\nIn addition, we add a simple auxiliary score loss $\\mathcal { L } _ { s c o r e }$ for mask quality prediction following Sun et al. [58].\n\nOverall, the final training loss function $\\mathcal { L }$ can be formulated as:\n\n$$\n\\begin{array} { r } { \\mathcal { L } = \\lambda _ { b c e } \\mathcal { L } _ { b c e } + \\lambda _ { d i c e } \\mathcal { L } _ { d i c e } + \\lambda _ { p o s } \\mathcal { L } _ { p o s } + } \\\\ { \\lambda _ { s c o r e } \\mathcal { L } _ { s c o r e } , \\qquad } \\end{array}\n$$\n\nwhere $\\lambda _ { b c e } , \\lambda _ { d i c e } , \\lambda _ { r e l }$ and $\\lambda _ { s c o r e }$ are hyperparameters used to balance these four losses.\n\n# 4 Expriment\n\n# 4.1 Experiment Settings\n\nIn our experiment, we utilize the pre-trained Sparse 3D U-Net method to extract point-wise features from point clouds [58]. We also employ the pre-trained MPNet model [56] as our text encoder. For the rest of the network, training is conducted from scratch. We set an initial learning rate of 0.0001 and apply a learning rate decay at epochs 26, 34, and 46, each with a decay rate of 0.5. Our experiments use a default of 6 multiple rounds $L$ , a batch size of 32, and a maximum sentence length of 80. We set $\\lambda _ { b c e } = \\lambda _ { d i c e } = 1 , \\lambda _ { p o s } = \\lambda _ { s c o r e } = 0 . 5$ . All experiments are conducted using PyTorch on a single NVIDIA Tesla A100 GPU, ensuring consistency in our computational process.\n\n# 4.2 Dataset and Evaluation Metrics\n\nWe evaluate our method using the ScanRefer dataset, a recent 3D referring dataset [5, 24], comprising 51,583 English natural language expressions referring to 11,046 objects across 800 ScanNet scenes [7]. Following Chen et al. [5], our evaluation metrics include mean Intersection over Union (mIoU) and $\\operatorname { A c c } @ k \\operatorname { I o U }$ . “Unique” refers to cases where the target instance is the only one of its class, and “Multiple” indicates situations where there is at least one more object of the target’s class.\n\nTable 1: The 3D-RES results on ScanRefer. $\\dagger$ The mIoU and accuracy are reevaluated on our machine. ∗We reproduce results by extracting points within the boxes as segmentation mask predictions using their official codes.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">Unique (~19%)</td><td colspan=\"3\">Multiple (~81%)</td><td colspan=\"3\">Overall</td><td colspan=\"3\">Inference Time</td></tr><tr><td>0.25</td><td>0.5</td><td>mIoU</td><td>0.25</td><td>0.5</td><td>mIoU</td><td>0.25</td><td>0.5</td><td>mIoU</td><td>Stage-1</td><td>Stage-2</td><td>All</td></tr><tr><td colspan=\"10\">Multi-task</td><td></td><td></td><td></td></tr><tr><td>EDA-box2mask [68]</td><td>84.7</td><td>56.9</td><td></td><td>50.0</td><td>37.0</td><td>-</td><td>55.2</td><td>40.0</td><td>35.0</td><td>-</td><td></td><td></td></tr><tr><td>3DRefTR-SP[41]</td><td>87.9</td><td>69.8</td><td></td><td>51.6</td><td>41.9</td><td></td><td>57.0</td><td>46.1</td><td>40.8</td><td></td><td></td><td>388ms</td></tr><tr><td>3DRefTR-HR [41]</td><td>89.6</td><td>77.0</td><td></td><td>52.3</td><td>43.7</td><td></td><td>57.9</td><td>48.7</td><td>41.2</td><td></td><td></td><td>405ms</td></tr><tr><td>UniSeg3D [69]</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>29.6</td><td></td><td></td><td></td></tr><tr><td>SegPoint [20]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td>41.7</td><td>1</td><td></td><td></td></tr><tr><td>Reason3D[23]</td><td>88.4</td><td>84.2</td><td>74.6</td><td>50.5</td><td>31.7</td><td>34.1</td><td>57.9</td><td>41.9</td><td>42.0</td><td>1</td><td></td><td></td></tr><tr><td colspan=\"10\">Single-task</td><td colspan=\"3\"></td></tr><tr><td>TGNN[24]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>37.5</td><td>31.4</td><td>27.8</td><td></td><td></td><td></td></tr><tr><td>TGNN† [24]</td><td>69.3</td><td>57.8</td><td>50.7</td><td>31.2</td><td>26.6</td><td>23.6</td><td>38.6</td><td>32.7</td><td>28.8</td><td>26862ms</td><td>235ms</td><td>27097ms</td></tr><tr><td>InstanceRefer† [71]</td><td>81.6</td><td>72.2</td><td>60.4</td><td>29.4</td><td>23.5</td><td>21.5</td><td>40.2</td><td>33.5</td><td>30.6</td><td>509ms</td><td>672ms</td><td>1181ms</td></tr><tr><td>X-RefSeg3D[52]</td><td></td><td></td><td></td><td></td><td></td><td></td><td>40.3</td><td>33.8</td><td>29.9</td><td></td><td></td><td></td></tr><tr><td>3DVG-Transformer*[73]</td><td>79.5</td><td>58.0</td><td>49.9</td><td>42.0</td><td>30.8</td><td>27.0</td><td>49.3</td><td>36.1</td><td>31.4</td><td></td><td></td><td></td></tr><tr><td>3D-SPS*[45]</td><td>84.8</td><td>65.6</td><td>54.7</td><td>41.7</td><td>30.8</td><td>26.7</td><td>50.1</td><td>37.6</td><td>32.1</td><td></td><td></td><td></td></tr><tr><td>3DRESTR[41]</td><td>79.0</td><td>54.2</td><td></td><td>40.2</td><td>22.1</td><td></td><td>46.0</td><td>26.9</td><td>28.7</td><td></td><td></td><td></td></tr><tr><td>3D-STMN [63]</td><td>89.3</td><td>84.0</td><td>74.5</td><td>46.2</td><td>29.2</td><td>31.1</td><td>54.6</td><td>39.8</td><td>39.5</td><td></td><td></td><td>283ms</td></tr><tr><td>RG-SAN (Ours)</td><td>89.2</td><td>84.3</td><td>74.5</td><td>55.0</td><td>35.4</td><td>37.4</td><td>61.7</td><td>44.9</td><td>44.6</td><td></td><td></td><td>295ms</td></tr></table></body></html>\n\n# 4.3 Quantitative Comparison\n\nIn our experiments on the ScanRefer dataset, our proposed RG-SAN demonstrates significant improvements in nearly all metrics on the single-task leaderboard, as shown in Tab. 1. Notably, RG-SAN shows substantial gains compared to the state-of-the-art single-task model 3D-STMN, with increases of 5.1 points in mIoU and 7.1 points in $\\operatorname { A c c } @ 0 . 2 5$ . This highlights our model’s inferencing capability. A more detailed examination reveals that the majority of these improvements occur in scenarios with multiple disruptive instances, where RG-SAN achieves a remarkable 6.3-point increase in mIoU. This setting, where the target instance is among other instances of the same type, demands discriminative reasoning from the model. The significant performance validates the enhanced referring capabilities empowered by spatial reasoning. Our proposed RG-SAN also outperforms multi-task models [68, 41], including LLM-based models [20, 23], in most 3D-RES metrics, despite those models benefiting from more annotated data.\n\nMoreover, RG-SAN has competitive inference costs, being only $1 2 \\mathrm { m s }$ slower than the efficient 3D-STMN and faster than all other compared models, demonstrating its high performance with minimal computational increase.\n\n# 4.4 Ablation Study\n\n# 4.4.1 Text-driven Localization Module\n\nWe conduct an ablation study on the Text-driven Localization Module (TLM), as illustrated in Tab. 2. Simultaneously, we perform a fine-grained analysis of various initialization schemes for embeddings and positions. The term \"w/o TLM\" denotes the approach of not modeling positional information and instead directly using text embeddings for interaction. \"MAFT\" refers to the direct adaptation of the method proposed in [31]. The \"Project\" method involves initializing embeddings based on text-driven embeddings and then projecting each textual token directly into a 3D position, while the \"Random\" method randomly assigns a position to each textual token. Finally, we utilize the initialization technique called Text-driven Initialization (TI), which simultaneously initializes both embeddings and positions in a text-driven manner. Tab. 2 clearly shows that, under identical conditions, TI outperforms the others in all metrics. This indicates that TI more effectively leverages positional information from the visual scene, leading to more precise initial positions for the textual tokens. Consequently, this reduces the complexity of the subsequent iterative refinement process, thereby enhancing the overall accuracy of our model in spatially aligning text with point cloud data. Additionally, Tab. 2 demonstrates that proper initialization leads to the superior performance of TLM compared to the methods without TLM.\n\nTable 2: Ablation study of Text-driven Localization Module, where “w/o TLM” means not using TLM.   \n\n<html><body><table><tr><td>Method</td><td>Init. of Embeddings</td><td>Init. of Positions</td><td>Multiple mIoU</td><td>Overall mIoU</td></tr><tr><td>w/o TLM MAFT [31]</td><td>Text-driven</td><td></td><td>32.5</td><td>40.3</td></tr><tr><td></td><td>Zero</td><td>Random</td><td>29.7</td><td>37.9</td></tr><tr><td>Project</td><td>Text-driven</td><td>Project</td><td>30.3</td><td>38.8</td></tr><tr><td>Random</td><td>Text-driven</td><td>Random</td><td>30.1</td><td>38.8</td></tr><tr><td>TI</td><td>Text-driven</td><td>Text-driven</td><td>34.1</td><td>42.3</td></tr></table></body></html>\n\nTable 3: Ablation study of positional encoding, where “w/o Pos. Supervision” means not supervising the positions, and “w/o PE” means not using any positional encoding.   \n\n<html><body><table><tr><td rowspan=\"2\">positional encoding</td><td colspan=\"3\">Mu.tiple mIoU</td><td colspan=\"3\">Overal mIoU</td></tr><tr><td>0.25</td><td></td><td></td><td>0.25</td><td></td><td></td></tr><tr><td>w/o Pos. Supervision</td><td>45.4</td><td>27.3</td><td>30.4</td><td>54.4</td><td>38.2</td><td>38.9</td></tr><tr><td>W/o PE</td><td>46.1</td><td>31.7</td><td>32.8</td><td>54.6</td><td>42.4</td><td>41.1</td></tr><tr><td>Fourier APE</td><td>46.0</td><td>30.9</td><td>32.0</td><td>55.1</td><td>41.5</td><td>40.7</td></tr><tr><td>5D Euclidean RPE</td><td>46.7</td><td>32.5</td><td>33.3</td><td>54.6</td><td>43.9</td><td>41.7</td></tr><tr><td>Table-based RPE</td><td>47.2</td><td>33.7</td><td>34.1</td><td>55.6</td><td>43.9</td><td>42.3</td></tr></table></body></html>\n\n# 4.4.2 Positional Encoding\n\nWe compare various positional encoding methods previously employed in [55, 6, 31]. These methods include Fourier Absolute positional encoding (APE), 5D Euclidean Relative positional encoding (5D Euclidean RPE) [6], and Table-based Relative positional encoding (Table-based RPE) [31]. Tab. 3 reveals that Table-based RPE surpasses the other methods, suggesting that combining semantic information with relative relationships is advantageous. Additionally, we observe that employing only absolute positional encoding can result in lower performance than not using any positional encoding at all. This may be attributed to the inherent limitations of absolute positional encoding in capturing relative positional information. By complicating the semantic features, it introduces challenges in the model’s training process, underscoring the importance of choosing the right positional encoding technique for effective performance.\n\n# 4.4.3 Rule-guided Weak Supervision\n\nWe conducted experiments employing various weakly supervised text kernel selection strategies to evaluate their efficacy in leveraging target annotations. The strategy labeled as \"w/o RWS\" involves selecting the token based on attention weight within the cross-attention module [63], while \"Root\" entails selecting the root token of the dependency tree. Table 4 illustrates that utilizing the root node as supervision slightly outperforms the \"w/o RWS\" baseline. This is likely due to the root node providing consistent supervision, whereas Top1 tends to select different nodes variably, which complicates the training process. In contrast, our Rule-guided Target Selection (RTS) strategy, based on dependency tree rules to locate subjects, aligns more effectively with the structural nature of the text. It precisely identifies the target entity’s position, significantly enhancing annotation utilization and effectively directing model training. This leads to a notable improvement in model performance.\n\nFurthermore, we conduct an ablation study on the impact of the position loss weight $\\mathcal { L } _ { p o s }$ , detailed in Tab. 5. We observe that increasing the weight generally improves performance, peaking at a weight of 0.5, beyond which performance begins to taper off. This finding highlights the importance of balancing the weight of the position loss to optimize the model’s effectiveness.\n\n# 4.4.4 Comparison with MAFT\n\nMAFT [31] has played a pivotal role in 3D instance segmentation by incorporating spatial position modeling, offering valuable insights into how spatial information can improve model performance. Inspired by this approach, we extend spatial information into the text space to better align visual and textual semantics, specifically targeting spatial relationship reasoning in 3D-RES. Our approach introduces two key innovations that distinguish it from MAFT:\n\nUnlike MAFT [31], which initializes queries with zeros and uses random initialization for positional information, we employ text-driven queries and positional information to model the spatial relationships of entities in the expressions. This allows our model to capture the spatial context better, resulting in a 4.4-point improvement in mIoU, as shown in Tab. 2 In contrast to [31], which supervises the positions of all target instances, 3D-RES supervises only the core target word. Our novel RWS method constructs spatial relationships for all noun instances using only the target word’s positional information, resulting in a 2.3-point improvement in mIoU, as demonstrated in Tab. 4.\n\nTable 4: Weak Supervision Strategy in RWS, where “w/o RWS” means using attention-based Top1 approach in [63] instead of our RWS, and “RTS” refers to our Rule-guided Target Selection strategy.   \n\n<html><body><table><tr><td rowspan=\"2\">Strategy</td><td colspan=\"3\">Multiple</td><td colspan=\"3\">Overall</td></tr><tr><td>0.25</td><td>0.5</td><td>mIoU</td><td>0.25</td><td>0.5</td><td>mIoU</td></tr><tr><td>w/o RWS</td><td>47.2</td><td>33.7</td><td>34.1</td><td>55.6</td><td>43.9</td><td>42.3</td></tr><tr><td>Root</td><td>53.5</td><td>30.4</td><td>34.7</td><td>60.7</td><td>40.9</td><td>42.5</td></tr><tr><td>RTS</td><td>55.0</td><td>35.4</td><td>37.4</td><td>61.7</td><td>44.9</td><td>44.6</td></tr></table></body></html>\n\nTable 5: Ablation study of the weight of pos.   \n\n<html><body><table><tr><td rowspan=\"2\">Weight of Lpos</td><td colspan=\"3\">Multiple</td><td colspan=\"3\">Overall</td></tr><tr><td>0.25</td><td>0.5</td><td>mIoU</td><td>0.25</td><td>0.5</td><td>mIoU</td></tr><tr><td>0.1</td><td>54.7</td><td>34.9</td><td>36.9</td><td>61.3</td><td>44.3</td><td>44.0</td></tr><tr><td>0.2</td><td>55.5</td><td>34.0</td><td>37.0</td><td>62.0</td><td>43.7</td><td>44.2</td></tr><tr><td>0.5</td><td>55.0</td><td>35.4</td><td>37.4</td><td>61.7</td><td>44.9</td><td>44.6</td></tr><tr><td>1.0</td><td>55.3</td><td>34.5</td><td>37.0</td><td>61.8</td><td>44.2</td><td>44.3</td></tr><tr><td>2.0</td><td>54.3</td><td>33.8</td><td>36.6</td><td>61.0</td><td>43.7</td><td>43.9</td></tr></table></body></html>\n\n![](images/8d4eaca71358fd7e919ff97b5f8b4a953334d7faf438b2f100ee37c170e3e39b.jpg)  \nFigure 3: Visualization of all the nouns in the textual description. Our RG-SAN can segment instances corresponding to different nouns, while 3D-STMN indiscriminately assigns all nouns to the target instance. Zoom in for best view.\n\n# 4.5 Qualitative Comparison\n\nWe conduct a qualitative analysis on the ScanRefer validation set as shown in Fig. 3, comparing our proposed RG-SAN with 3D-STMN [63] to highlight our model’s exceptional referring capability. Fig. 3 demonstrates our model’s ability to accurately segment not only the target objects but also other nouns mentioned in the text. Unlike 3D-STMN, which misattributes all nouns to a single target, RG-SAN distinctly recognizes and locates each noun. For example, in Fig. 3-(c), our model successfully identifies the target chair through relative positioning, even with similar objects in the scene, and accurately recognizes a coat as a supporting element in the description. This ability extends to Fig. 3-(a) and (b), where RG-SAN correctly segments multiple auxiliary nouns into their corresponding instances, demonstrating its robust generalization for complex texts and precise localization for multiple entities. Such capabilities enhance the model’s understanding of complex semantic scenes, significantly improving its ability to refer to specific entities accurately.\n\n# 5 Conclusion\n\nIn this paper, we present RG-SAN to overcome the limitations of traditional 3D-RES methods, particularly their lack of spatial awareness. Specifically, the TLM is introduced to model and refine positional information, while the RWS is designed to employ dependency tree rules to accurately guide the position of the target object. Combining TLM with RWS strategy, RG-SAN significantly improves segmentation accuracy and robustly handles spatial ambiguities. Extensive experiments conducted on the ScanRefer benchmark demonstrate the superior performance of RG-SAN. This underscores the importance of incorporating spatial awareness into segmentation models, paving the way for future advancements in the domain.\n\n# 6 Acknowledge\n\nThis work was supported by National Science and Technology Major Project (No. 2022ZD0118201), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U22B2051, No. U21B2037, No. 62072389, No. 62302411, No. 623B2088), the Natural Science Foundation of Fujian Province of China (No.2021J06003) and China Postdoctoral Science Foundation (No. 2023M732948).",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n传统的3D指代表达分割（3D-RES）方法常因对实例空间信息的重视不足，导致过分割或误分割问题。3D-RES在自主机器人、人机交互和自动驾驶系统等领域有重要应用，需要比3D指代表达理解（3D-REC）更深入的理解，要求准确识别实例并提供精确的3D掩码，因此解决空间信息处理问题至关重要。\\n\\n**方法概述**\\n本文提出了规则引导的空间感知网络（RG-SAN），利用目标实例的空间信息进行监督，由文本驱动定位模块（TLM）和规则引导的弱监督（RWS）策略组成，增强了模型的推理和定位能力。\\n\\n**主要贡献与效果**\\n- 提出RG-SAN，提升了3D-RES中模型的指代表达能力。在ScanRefer基准测试中，mIoU提高了5.1个百分点。\\n- 提出TLM和RWS，分别实现对表达式中所有实例的精确定位和利用目标实例位置监督所有实例的空间定位。在多干扰实例场景下，mIoU提高了6.3个百分点。\\n- 大量实验证明RG-SAN在3D-RES任务中性能和鲁棒性显著提升，推理成本仅比3D-STMN慢12ms，且快于其他对比模型。处理具有空间歧义的描述时表现出色。同时，RG-SAN能准确分割文本中提到的所有名词对应的实例，而3D-STMN会将所有名词误归为单一目标。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n通过建模核心实例的空间关系来辅助文本推理，利用目标实例的空间信息监督网络，使模型能准确描绘文本中所有实体间的空间关系，从而增强推理和定位能力。在3D-RES中，利用语义和空间上下文信息，通过TLM初始化和迭代更新文本中实体名词的位置，再用RWS策略结合依赖树规则准确引导核心实例定位，从而解决传统方法对空间信息处理不足的问题。\\n\\n**创新点**\\n先前工作过度依赖文本推理，对实例间空间关系建模不足，在处理点云和文本交互时不考虑空间定位，常因无序点云特征丢失空间信息，导致空间关系理解模糊。RG-SAN的创新在于提出TLM和RWS策略。TLM利用文本驱动初始化和迭代更新实体位置，充分利用空间信息；RWS利用依赖树规则准确识别核心实例，仅用目标实例位置监督所有实例的空间定位。此外，与MAFT相比，RG-SAN采用文本驱动的查询和位置信息来建模实体空间关系，且仅监督核心目标词的位置，分别使mIoU提高了4.4和2.3个百分点。\\n\\n**具体实现步骤**\\n1. **特征提取**：并行提取视觉和语言模态的特征。视觉编码使用稀疏3D U-Net提取点云特征，再从原始点云获取超点，通过超点池化得到超点级特征；语言编码使用预训练的MPNet模型提取词级嵌入。\\n2. **上下文驱动的空间感知**：引入TLM初始化文本中实体名词的位置，并通过迭代多模态交互不断更新和细化这些位置。\\n    - **文本驱动定位模块（TLM）**：\\n        - **文本驱动初始化**：将超点特征和词嵌入投影到同一维度，通过依赖驱动交互增强词级嵌入，计算特征相似度估计每个提及实体的空间概率分布，预测实体的近似位置和表示。公式为：$\\hat { \\mathbf { E } } _ { 0 } = \\mathbf { D } \\mathbf { D } \\mathbf { I } ( \\mathbf { E } _ { 0 } \\mathbf { W } _ { l a n g } )$，$\\hat { \\mathbf { S } } = \\mathbf { S } _ { p } \\mathbf { W } _ { v i s }$；$A _ { i j } = \\cfrac { \\operatorname { S i m } ( \\mathbf { E } _ { i } , \\mathbf { S } _ { j } ) } { \\sum _ { j = 1 } ^ { N _ { s } } \\operatorname { S i m } ( \\mathbf { E } _ { i } , \\mathbf { S } _ { j } ) }$ 等。\\n        - **迭代位置细化**：使用多层感知机（MLP）预测位置偏移，逐步细化文本标记的位置，公式为 $\\mathbf { P } _ { l + 1 } ^ { t } = \\mathbf { P } _ { l } ^ { t } + \\Delta \\mathbf { P } _ { l } ^ { t }$。\\n    - **空间感知聚合**：使用绝对位置编码（APE）和相对位置编码（RPE）进一步细化位置，增强文本和超点特征的交互。公式如 $\\mathbf { B } _ { l } ^ { s } = \\mathbf { A } \\mathbf { P } \\mathbf { E } ( \\mathbf { P } _ { l } ^ { s } )$，$\\mathbf { B } _ { l } ^ { t } = \\mathbf { A } \\mathbf { P } \\mathbf { E } ( \\mathbf { P } _ { l } ^ { t } )$；$\\mathbf { B } _ { l } ^ { r } [ i , j ] = \\mathrm { R P E } ( \\mathbf { Q } [ i ] + \\mathbf { K } [ j ] )$ 等。\\n3. **规则引导的弱监督**：\\n    - **规则引导的目标选择**：利用预处理的依赖树和手动规则准确识别核心实例的位置，具体实现见算法1。\\n    - **训练目标**：根据目标实例的索引获取相应的段内核和位置，使用二元交叉熵损失（BCE）、Dice损失、位置损失和辅助分数损失进行训练。公式如 $\\mathcal { L } = \\lambda _ { b c e } \\mathcal { L } _ { b c e } + \\lambda _ { d i c e } \\mathcal { L } _ { d i c e } + \\lambda _ { p o s } \\mathcal { L } _ { p o s } + \\lambda _ { s c o r e } \\mathcal { L } _ { s c o r e }$。\\n\\n**案例解析**\\n在ScanRefer验证集的定性分析中，对比RG-SAN与3D-STMN，RG-SAN能准确分割文本中提到的目标对象和其他名词对应的实例，而3D-STMN会将所有名词误归为单一目标。例如在图3-(c)中，RG-SAN能通过相对定位准确识别目标椅子，还能将外套识别为描述中的辅助元素。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n单任务模型：3D-STMN、TGNN、InstanceRefer、X-RefSeg3D、3DVG-Transformer、3D-SPS、3DRESTR等；多任务模型：EDA-box2mask、3DRefTR-SP、3DRefTR-HR、UniSeg3D、SegPoint、Reason3D等。\\n\\n**性能对比**\\n*   **在 [平均交并比/mIoU] 指标上：** 本文提出的RG-SAN在ScanRefer数据集上整体mIoU达到了 **44.6%**，显著优于单任务模型3D-STMN（39.5%），高出5.1个百分点；在多干扰实例场景下，RG-SAN的mIoU为37.4%，比3D-STMN的31.1%高出6.3个百分点。同时，RG-SAN也优于多任务模型，如3DRefTR-HR（41.2%）等，以及基于大语言模型的模型。\\n*   **在 [Acc@0.25] 指标上：** RG-SAN在ScanRefer数据集上达到了 **61.7%**，高出3D-STMN（54.6%）7.1个百分点。\\n*   **在 [推理时间/Inference Time] 指标上：** RG-SAN的推理时间为 **295ms**，仅比高效的3D-STMN（283ms）慢12ms，且快于所有其他对比模型。\\n*   **与MAFT对比**：在mIoU指标上，RG-SAN比MAFT高出4.4个百分点；在利用目标注释的效果上，RG-SAN基于规则引导的目标选择策略比MAFT在mIoU上高出2.3个百分点。\",\n    \"keywords\": \"### 关键词\\n\\n- 3D指代表达分割 (3D Referring Expression Segmentation, 3D-RES)\\n- 规则引导的空间感知网络 (Rule-Guided Spatial Awareness Network, RG-SAN)\\n- 文本驱动定位模块 (Text-driven Localization Module, TLM)\\n- 规则引导的弱监督 (Rule-guided Weak Supervision, RWS)\"\n}"
}