{
    "link": "https://arxiv.org/abs/2406.16860",
    "pdf_link": "https://arxiv.org/pdf/2406.16860",
    "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",
    "authors": [
        "Shengbang Tong",
        "Ellis L Brown",
        "Penghao Wu",
        "Sanghyun Woo",
        "Manoj Middepogu",
        "Sai Charitha Akula",
        "Jihan Yang",
        "Shusheng Yang",
        "Adithya Iyer",
        "Xichen Pan",
        "Austin Wang",
        "Rob Fergus",
        "Y. LeCun",
        "Saining Xie"
    ],
    "institutions": [
        "New York University"
    ],
    "publication_date": "2024-06-24",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 377,
    "influential_citation_count": 73,
    "paper_content": "# Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs\n\nShengbang Tong∗ Ellis Brown∗ Penghao $\\mathbf { W _ { u } } ^ { * }$ Sanghyun Woo Manoj Middepogu Sai Charitha Akula Jihan Yang Shusheng Yang Adithya Iyer Xichen Pan Austin Wang Rob Fergus Yann LeCun Saining Xie† New York University\n\n# Abstract\n\nWe introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures—selfsupervised, strongly supervised, or combinations thereof—based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instructiontuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.\n\nProject page: https://cambrian-mllm.github.io/\n\n# 1 Introduction\n\nThere is a long-standing debate in philosophy about whether understanding and meaning in language require sensory grounding. Aristotle’s emphasis on acquiring knowledge through sensory experience and empirical observation was central to his ancient Peripatetic school and remains influential to this day [8]; Aquinas famously formalized these ideas in the 13th century with the Peripatetic axiom: “Nihil est in intellectu quod non sit prius in sensu” (Nothing is in the intellect that was not first in the senses) [7]. Though many philosophers disagree [23], it is evident that having robust and highly capable sensory grounding is at least beneficial. Consider the Cambrian explosion, during which the emergence of vision is believed [105] to have been crucial for early animals to not only find food and avoid predators but also to evolve and improve. In fact, most human knowledge (and nearly\n\nVisual Representation Learning - Evaluation Protocols Pretrained Linear Probing ImageNet-1k CLS Vision Model or COCO SEG DET End-to-End Tuning ADE20K SEG DET Multimodal Large Language Models Pretrained MMB AI2D Visual Instruction Tuning ChartQA Vision Model MME MathVista VQA w/ LLMs OCRBench SEED SQA V\\*Bench 0 D 吧 国 信 Visual Connector Design Instruction Tuning Instruction Tuning Evaluation Protocol Representations Data Recipe\n\nall animal knowledge) is acquired through sensory experiences like sight, hearing, touch, taste, and smell, through interactions with the physical world [107]. These sensory experiences are fundamental to understanding the world around us and are crucial for real-world actions and decision-making.\n\nBeyond philosophical debates, recent advances in multimodal large language models (MLLMs) have brought the topic of visual representation learning vs. language understanding into practical focus. Language models have shown strong scaling behaviors [55], and recent advancements in multimodal learning are largely driven by the development of better, larger LLMs [81]. On the other hand, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. For instance, many pioneering frameworks such as LLaVA [82] use vision transformer-based CLIP models [109, 145], which are strongly supervised by language‡, as the vision feature extractor. While other visual representations, such as self-supervised DINO [103], are being explored [126], there is a lack of comprehensive and systematic study in this domain. This gap exists primarily because such studies are challenging: MLLMs involve a complex training and evaluation pipeline with numerous design decisions to consider. In this work, we aim to bridge the gap by exploring MLLMs from a vision-centric perspective. More specifically, we use MLLM instruction tuning as an evaluation protocol for various visual representations (illustrated in Fig. 1).\n\nOur motivation for this study also stems from two potential concerns of the current multimodal learning research: 1) relying too heavily too early on language can act as a shortcut [47, 144], compensating for the deficiencies in learning effective visual representations, and 2) existing benchmarks may not provide adequate guidance for real-world scenarios—where visual grounding is crucial for robust multimodal understanding. These concerns are not unfounded, as researchers have started to notice that visual grounding is becoming a bottleneck for applying MLLMs in some challenging real-world applications, despite significant progress in improving general capabilities [41, 126, 136].\n\nFrom another perspective, traditional evaluation protocols for visual representation learning (e.g., linear probing and end-to-end fine-tuning on datasets like ImageNet-1K [113], COCO [79], and ADE20K [154]) are becoming saturated and do not reflect the diverse perception challenges found in real-world distributions. On the other hand, using language in the form of visual question answering (VQA) offers a flexible and robust evaluation protocol. Our study aims to explore this new protocol design, setting it up to gain insights that will guide the development of better visual representations in the future. Furthermore, to better evaluate visual representations in this integrated setting, we develop a vision-centric MLLM benchmark, CV-Bench, by transforming traditional vision benchmarks into VQA format (Section 2.2).\n\nCambrian-1 is structured around five key pillars, each offering important insights into the design space of MLLMs:\n\n• Visual Representations: We explore various vision encoders and their combinations. $\\ S 2 . 4$ • Connector Design: We design a new dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens. §3 • Instruction Tuning Data: We curate high-quality visual instruction-tuning data from public sources, emphasizing the importance of distribution balancing. $\\ S 4$ • Instruction Tuning Recipes: We discuss instruction tuning strategies and practices. $\\ S 2 . 3$ • Benchmarking: We analyze existing MLLM benchmarks, cluster them into 4 intuitive groups, and introduce a new vision-centric benchmark “CV-Bench”. $\\ S 2 . 1$ , $\\ S 2 . 2$\n\n![](images/c087e135bd04c9968fdf0f6b83d72584eb95eabb76790c8acb5d0a3e20d4a367.jpg)  \nFigure 2: Examples of various vision models, objectives, and architectures studied. Image from [48].\n\nWe defer a detailed review of the fundamental components and methodologies that underpin MLLM research to Appendix B\n\n# 2 Evaluating Visual Representations through MLLMs\n\nCurrent MLLMs predominantly rely on CLIP [109] as the visual encoder due to its pre-alignment with language and ease of adaptation to the LLM token space. However, strong language priors can be a double-edged sword—they compensate for deficiencies in learning effective visual representations [126] and diminish insights gained from extensive visual representation learning research. In this section, we systematically evaluate how various visual encoder choices (see Fig. 2) impact the multimodal capabilities of MLLMs. We also advocate for using MLLM evaluation as a robust framework for assessing visual representation methods, moving beyond traditional protocols like linear probing and end-to-end fine-tuning to more faithfully reflect the diverse perception challenges in real-world scenarios and to better guide the development of improved visual representations.\n\n# 2.1 Analyzing the Benchmarks\n\nTo effectively evaluate visual representations and MLLMs, we first need to select benchmarks that accurately assess the multimodal capabilities of these models. We use a suite of commonly used benchmarks [24, 45, 54, 57, 83, 84, 91, 92, 96, 97, 120, 126, 137, 143], which is the intersection of those used in recent MLLM research [75, 77, 137]. To help interpret our results, we begin by analyzing the benchmarks themselves. Here, we train MLLMs with 23 different vision backbones (see Table 6) from a variety of model families (see Fig. 2) using a 2-stage instruction tuning process initially proposed in [82]: first training connector on 1.2M adapter data from ShareGPT-4V [27] followed by fine-tuning both the connector and LLM on 737K instruction tuning data (see more details in Appendices G.5 and H). Full benchmark results in Table 9.\n\nWho’s answering the question: the LLM or MLLM? Determining whether a benchmark truly needs visual input to be solved has been a persistent challenge in vision-language research [2, 26, 50, 94]. In this study, we compare the performance of MLLMs with and without visual input§, and also calculate the expected score via randomly guessing. These three conditions are visualized in Fig. 3-left, with benchmarks sorted by the difference between the average score with vision enabled and disabled. SQA- $\\cdot \\mathrm { I } ^ { \\ P }$ , MMMU, MathVista, and AI2D display less than a $5 \\%$ gap between vision enabled and disabled, suggesting that these benchmarks may not significantly depend on visual input and rather heavily rely on the base LLM. TextVQA and GQA both demonstrate a nearly $40 \\%$ positive gap between random guessing and vision-disabled scores, implying a strong language bias in these benchmarks. On the other hand, the vision-disabled performance on benchmarks like MMVP is notably worse than random guessing, suggesting that strong visual grounding is particularly crucial.\n\nClustering the Benchmarks To better understand the different aspects of MLLM performance, we analyze the correlations between the performance of our 23 MLLMs on each benchmark. A confusion matrix (Fig. 10) reveals that certain benchmarks, such as MMMU, are largely uncorrelated with the others. We perform principal component analysis on the benchmark scores and observe the formation of clusters corresponding to “General,” “Knowledge,” “Chart & OCR,” and “Vision-Centric” categories (Fig. 3-right). We assign MMMU to the knowledge category based on the types of questions it includes (see Appendix D). We also find that existing vision-centric benchmarks [126, 137] are of insufficient size (see Fig. 3-right), challenging the robustness of evaluating such capabilities. These benchmarks do not cover crucial visual elements such as depth and spatial awareness.\n\n![](images/e66a94d11b367065f02be6209702a036f6d4719c40a1d042b3001ab69082d232.jpg)  \nFigure 3: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as “General” in green, “Knowledge” in yellow, “Chart & OCR” in red, and “Vision-Centric” in blue.\n\nFinding 1: Most benchmarks do not properly measure vision-centric capabilities, and the ones that do have very few samples.\n\n# 2.2 Cambrian Vision-Centric Benchmark (CV-Bench)\n\nTo address the limitations of existing vision-centric benchmarks, we introduce the Cambrian VisionCentric Benchmark (CV-Bench). With 2638 manually-inspected examples, CV-Bench provides significantly more examples than other vision-centric MLLM benchmarks— $3 . 5 \\times$ more than RealWorldQA [137] and $8 . 8 \\times$ more than MMVP [126]. By repurposing standard vision benchmarks [18, $7 9 , 1 5 4 ] ^ { \\parallel }$ , we can assess models at classic vision tasks within a multimodal context. Leveraging the rich ground truth annotations from the benchmarks, we formulate natural language questions that probe the fundamental 2D and 3D understanding of the models.\n\nAs visualized in Fig. 11, CV-Bench evaluates 2D understanding via spatial relationships & object counting, and 3D understanding via depth order & relative distance. We refer details to Appendix E.\n\nFinding 2: Existing vision benchmarks can be effectively repurposed into VQA questions, enabling the assessment of vision-centric MLLM capabilities.\n\n# 2.3 Instruction Tuning Recipes\n\nMLLMs start with pre-trained LLM and vision backbones, connecting these modules with a connector such as a projector (MLP). The original LLaVA [80, 82] proposes a 2-stage frozen training process: first, pre-training a connector between frozen LLM and vision backbones using adapter data, and then fine-tuning both the connector and LLM with instruction tuning data while leaving the vision encoder frozen. Various studies [27, 63, 81, 98] have drawn different conclusions regarding the optimal training methodology for MLLMs. Here, we revisit this topic with extensive experiments.\n\nFor our experiments, we tune a set of MLLMs using Vicuna-1.5-7B as the LLM backbone and each of our 23 vision models (Table 6) as the visual encoder. We use a 737K instruction tuning data mix for all experiments here (see Appendix H). All hyperparameters are matched across each experimental setting—highlighting the impact of different tuning strategies with each visual encoder. All experimental settings and results are tabulated in Appendix F.2.\n\n![](images/285e8fcfe81b75e513e333e44125adc64f6649dfb26a0337149ca798815986a9.jpg)  \nFigure 4: Effect of Training Recipe on Model Performance. Boxplots display the distribution of benchmark scores across benchmark categories for different training recipes and types of visual encoders. The four training recipes include freezing the visual encoder with various amounts of adapter data $( 0 \\mathbf { M } _ { \\ast } ^ { \\ast \\ast }$ , $0 . 5  { \\mathrm { M } } _ {  { \\mathbb { R } } ^ { 6 } } ^ {  { \\mathbb { M } } ^ { 6 } }$ , $1 . 2  { \\mathbf { M } } _ { \\mathbb { R } ^ { 6 } } ^ {  { \\ast } } )$ as well as unfreezing it with $1 . 2 \\mathbf { M } ^ { \\mathrm { ~ ~ } }$ adapter data. Amount of Adapter Data: All model types show increased performance on general and vision-centric benchmarks; knowledge benchmarks show mixed results; OCR & chart benchmarks benefit from more data for language-supervised models. Unfreezing: Unfreezing the visual encoder with $1 . 2 \\mathbf { M } ^ { \\mathrm { ~ ~ } }$ adapter data generally benefits all categories.\n\nOne Stage vs Two Stage Training Recent work [63] advocates for skipping connector pre-training, claiming this “reduces compute cost without harming downstream performance.” To explore whether this claim holds—especially when using non-language-supervised visual encoders—we conduct experiments using 0, 0.5M, and 1.2M adapter data. Following LLaVA’s recipe [82], we tune only the connector on the adapter data during this first phase, before unfreezing the LLM and connector during instruction tuning on the 737K mix. Fig. 4 shows that pre-training the connector first enhances model performance and that more adapter data further improves performance across all domains. Thus, we subsequently adopt 2-stage training with 1.2M adapter data as our standard setup.\n\nFinding 3: Two-stage training is beneficial; more adapter data further improves results.\n\nFreeze vs Unfreeze Vision Encoder There are also mixed practices in freezing [63, 80, 82] or unfreezing [44, 81] vision backbones during fine-tuning. Some argue that unfreezing the vision backbone significantly degrades performance [63]. Our experiments demonstrate that unfreezing benefits performance across all benchmarks except for a marginal change in knowledge benchmarks (Fig. 4). We suspect this is due to the composition of the 737K instruction tuning data and the LLMheavy focus of these benchmarks (see Section 2.1). We note that unfreezing the vision backbone introduces additional computational overhead, which prohibits testing on some larger vision models under current sharding strategies (see more details in Appendix H).\n\nFinding 4: Unfreezing the vision encoder is widely beneficial. Language-supervised models always benefit; SSL models particularly benefit on vision-centric benchmarks.\n\n# 2.4 MLLMs as a Visual Representation Evaluator\n\nAs discussed in earlier sections, MLLMs provide a new interface to explore aspects of vision models beyond traditional benchmarks like ImageNet-1k linear probing. We study the 2-stage instruction tuning setting using 1.2M adapter data, 737K fine-tuning data, and frozen visual encoders to allow comparison of the widest range of models.\n\n![](images/eef97fa2c3ce91d66cceebc3f1be5ac6ba8b38f4a6b6f2c5c957c8616f699eb3.jpg)  \nFigure 5: Evaluating Visual Representations with MLLMs While language-supervised models outperform self-supervised or other models, a well-trained self-supervised model like DINOv2 can also achieve competitive performance on vision-centric tasks.\n\n![](images/9776e0d4105b7656335338a209b2d97e8f2584af8dd8782289999931215e0089.jpg)  \nFigure 6: Continued Fine-Tuning Narrows the Gap Between CLIP and DINOv2. Performance is compared with $0 . 7 \\mathbf { M }$ and 5M instruction tuning data in both frozen $( \\ O _ { 7 } ^ { 1 4 / 1 4 } )$ and unfrozen $( )$ settings. DINOv2 shows significant performance improvement with increased data and unfreezing—surpassing the $0 . 7  { \\mathbf { M } } _ { \\mp } ^ { \\mathrm { ~ \\tiny ~ \\sharp ~ } }$ CLIP model in several benchmarks and narrowing the gap to the $5 \\mathbf { M } ^ { \\mathrm { ~ ~ 3 ~ } }$ model in knowledge and vision-centric tasks.\n\nWe evaluate on benchmarks detailed in Section 2.1, calculating the average performance\\*\\* for each category and visualize the results in Fig. 5 (full results in Appendix F). Our findings highlight the advantages of language-supervised models over non-CLIP models across all benchmark categories, with significantly better performance on chart and OCR-related benchmarks. We hypothesize that this is due to CLIP’s training data, such as LAION [115], containing abundant OCR and text-heavy data, whereas SSL and other vision models primarily train on natural images with significantly less text content. It is also noteworthy that language-supervised models are typically trained with a very large pool of data, ranging from 400 million [109] to 10 billion [28] samples, whereas the largest vision self-supervised training dataset, like DINOv2, consists of only 142 million samples [103].\n\nAdditionally, we observe that higher-resolution models particularly enhance performance on chart and vision-centric benchmarks while remaining neutral on general VQA and knowledge-based VQAs. While the majority of the backbones we examine are ViT-based [39], ConvNet-based architectures (such as OpenCLIP ConvNeXt [86]) are inherently well-suited for high-resolution image processing [130] and can produce superior results on OCR & Chart and Vision-Centric benchmarks. In vision-centric benchmarks, the gap between language-supervised and other types of vision models is smaller, with a well-trained self-supervised DINOv2 model even outperforming some language-supervised models.\n\nFinding 5: High-res encoders greatly enhance performance on chart & vision-centric benchmarks, and ConvNet-based architectures are inherently well-suited for such tasks.\n\nNarrowing the gap between Language- and Self-Supervised models Above, we observe that DINOv2 stands midway between SSL models and language-supervised models on general and knowledge benchmarks, even outperforming some language-supervised models on vision-centric benchmarks. Here, we study whether the continued finetuning of an MLLM based on a SSL model can achieve performance similar to that of a language-supervised model. Specifically, we scale up the instruction tuning data from 737K to 5M (see more details in Appendix G.5), and instruction tune MLLMs with DINOv2 ViT-L/ $1 4 @ 3 3 6$ and OpenAI CLIP ViT-L/ $1 4 @ 3 3 6$ encoders in both frozen and unfrozen settings. In Fig. 6, we observe that by unfreezing the vision backbone, the DINOv2-based MLLM fine-tuned with 5M data surpasses the MLLM trained with a CLIP model on $0 . 7 \\mathbf { M }$ data. Additionally, the gap between DINOv2 and the CLIP models is reduced under the 5M setting.\n\nFinding 6: Language supervision offers strong advantages, but the performance gap can be narrowed with SSL methods given enough data and proper tuning.\n\n# 2.5 Combining Multiple Vision Encoders\n\nAs observed in Fig. 5, different vision encoders excel in different aspects of MLLM performance. In this study, we explore the potential of combining multiple vision encoders to leverage their distinctive representations, aiming to build a more capable MLLM. Given that different vision encoders use varying architectures and image resolutions, we interpolate to a fixed number of visual tokens (576) in this subsection (see details in Appendix F.3). We then concatenate these tokens along the feature dimension, following a method similar to A-MoF proposed in [126].\n\nOur study indicates that adding a non-language-supervised model (DINOv2) can improve benchmark performance, especially in vision-centric tasks. Notably, even OCR benchmarks benefit from incorporating DINOv2. This highlights the importance of self-supervised learning models in complementing language-supervised models to achieve robust multimodal understanding. Detailed results and configurations are available in Appendix F.3.\n\nHowever, this naive strategy has two limitations: 1) it employs interpolation, which can lead to information loss, especially with vision encoders with high-resolution feature maps, and 2) it treats each model equally via simple concatenation. Therefore, we seek a more effective strategy that can more flexibly leverage model combinations with less information loss.\n\nFinding 7: Combining multiple vision encoders, including SSL models, can enhance MLLM performance across various benchmarks, particularly in vision-centric tasks.\n\n# 3 Spatial Vision Aggregator (SVA): A New Connector Design\n\nTo effectively aggregate features from multiple vision encoders and prevent the information loss introduced by interpolation, we use a set of learnable latent queries that interact with multiple vision features via cross-attention layers [37]. In particular, our approach incorporates two new vision-centric design principles:\n\n1. We introduce spatial inductive bias by explicitly defining the aggregation space for each token in the query.   \n2. We aggregate vision features multiple times across the LLM layers, enabling the model to repeatedly access and integrate necessary visual information.\n\nTo facilitate information aggregation via cross-attention, we create a $C$ -dimension learnable latent token $\\mathbf { x } \\in \\mathbb { R } ^ { C }$ that is repeated $L \\times L$ times to form a 2D grid, serving as the query $\\mathbf { X } \\in \\mathbb { R } ^ { L ^ { 2 } \\times C }$ . The set of visual features $\\mathbf { F }$ from $N$ vision encoders serve as the context (i.e., key and value). We ensure the output resolution of every vision encoder is a multiple of $L$ . Formally, the feature map of the $k$ -th vision encoder $( \\mathbf { F } _ { k } )$ has a resolution of $m _ { k } L \\times m _ { k } L \\times C$ , where $m _ { k }$ is a positive integer multiplier, and $L$ is the height/width of the learnable 2D grid with hidden dimension $C$ .\n\nSpatial inductive bias To maintain the spatial structure during cross-attention, we align each token in the query with a specific sub-region of the feature maps in all vision encoders. Formally, a token at row $i$ and column $j$ in the query $\\mathbf { x } _ { i , j }$ corresponds to the sub-region\n\n$$\n\\mathbf { F } _ { k } [ m _ { k } \\cdot i : m _ { k } \\cdot ( i + 1 ) , m _ { k } \\cdot j : m _ { k } \\cdot ( j + 1 ) ] \\in \\mathbb { R } ^ { m _ { k } ^ { 2 } \\times C }\n$$\n\nof the $k$ -th vision feature map. As a result, a token $\\mathbf { x } _ { i , j }$ aggregates a total of $\\textstyle \\sum _ { k } m _ { k } ^ { 2 }$ features from $N$ vision encoders through cross-attention (see Fig. 7-left).\n\nSpecifically, the updated query vector $\\mathbf { q } ^ { * } { } _ { \\mathbf { i } , \\mathbf { j } } \\in \\mathbb { R } ^ { 1 \\times C }$ at position $( i , j )$ is computed as\n\n![](images/0ab13299ee83a2d0631f1763371dcdea14f35d7feb207703ffbec16d80825c3f.jpg)  \nFigure 7: Spatial Vision Aggregator (SVA). We propose SVA, a dynamic and spatially-aware connector that integrates multiple vision features with LLMs while reducing the number of tokens.9\n\n$$\n\\mathbf { q } ^ { * } { } _ { i , j } = \\mathrm { s o f t m a x } \\left( \\frac { \\mathbf { q } _ { i , j } \\cdot [ \\mathbf { k } _ { i , j , 1 } , \\mathbf { k } _ { i , j , 2 } , \\dots , \\mathbf { k } _ { i , j , N } ] ^ { \\top } } { \\sqrt { C } } \\right) [ \\mathbf { v } _ { i , j , 1 } , \\mathbf { v } _ { i , j , 2 } , \\dots , \\mathbf { v } _ { i , j , N } ] ,\n$$\n\nwhere\n\n$$\n\\begin{array} { r l } & { \\mathbf { q } _ { i , j } = \\mathbf { W } ^ { Q } \\mathbf { x _ { i , j } } \\in \\mathbb { R } ^ { 1 \\times C } , } \\\\ & { \\mathbf { k } _ { i , j , k } = \\mathbf { W } _ { k } ^ { K } \\mathbf { F } _ { k } [ m _ { k } \\cdot i : m _ { k } \\cdot ( i + 1 ) , m _ { k } \\cdot j : m _ { k } \\cdot ( j + 1 ) ] \\in \\mathbb { R } ^ { m _ { k } ^ { 2 } \\times C } , } \\\\ & { \\mathbf { v } _ { i , j , k } = \\mathbf { W } _ { k } ^ { V } \\mathbf { F } _ { k } [ m _ { k } \\cdot i : m _ { k } \\cdot ( i + 1 ) , m _ { k } \\cdot j : m _ { k } \\cdot ( j + 1 ) ] \\in \\mathbb { R } ^ { m _ { k } ^ { 2 } \\times C } . } \\end{array}\n$$\n\nHere, $\\mathbf { q } _ { i , j }$ is the query vector at position $( i , j )$ , calculated using the query projection matrix $\\mathbf { W } ^ { Q } \\in$ $\\mathbb { R } ^ { C \\times C }$ . The key vectors $\\mathbf { k } _ { i , j , k }$ and value vectors $\\mathbf { v } _ { i , j , k }$ are computed for each vision encoder $k$ using their respective key and value projection matrices $\\mathbf { W } _ { k } ^ { K } \\in \\mathbb { R } ^ { C \\times C }$ and $\\mathbf { W } _ { k } ^ { V } \\in \\mathbb { R } ^ { C \\times C }$ . Since $\\textstyle \\sum _ { k } m _ { k } ^ { 2 }$ features are aggregated into a single token, we effectively reduce the number of tokens.\n\nMulti-layer vision aggregation Although our proposal effectively aggregates features from multiple vision encoders, there is still potential information loss with high-resolution input (large $m _ { k }$ ) or multiple vision encoders (large $N$ ). Here, a single token would have to handle a larger amount of context information during aggregation. To prevent this, we allow cross-attention to occur multiple times by inserting our proposal throughout the LLM layers—allowing consistent access to the uncompressed visual information (see Fig. 7-right).\n\nHyperparameters To flexibly modulate capacity, we introduce two hyperparameters $D$ and $G$ which indicate the number of cross-attention layers and distinct groups of learnable queries used between the vision models and the LLM, respectively. $D$ and $G$ are always set to 1 for cross-attention layers within LLM layers. We provide ablation studies on the selection of $D$ and $G$ in Appendix $\\mathrm { ~ H ~ }$ .\n\nTable 1: Comparison between our SVA and other aggregation approaches. The SVA module consistently outperforms other baselines and excels in aggregating high-resolution vision information.   \n\n<html><body><table><tr><td>Connector</td><td>General</td><td>Knowledge</td><td>OCR&Chart</td><td>Vision-Centric</td></tr><tr><td>Concat. [126]</td><td>67.2</td><td>48.9</td><td>50.1</td><td>52.6</td></tr><tr><td>Resampler [58]</td><td>63.1</td><td>46.5</td><td>27.1</td><td>42.6</td></tr><tr><td>SVA-no-multi-agg</td><td>68.0</td><td>49.5</td><td>55.2</td><td>52.6</td></tr><tr><td>SVA</td><td>68.5</td><td>49.7</td><td>55.5</td><td>53.2</td></tr></table></body></html>\n\nWe demonstrate the efficacy of SVA module using the best vision model combination results from the previous section and a Vicuna-1.5-7B base LLM. Specifically, we employ a combination of four vision encoders: OpenAI CLIP ViT- $. \\mathrm { L } / 1 4 @ 3 3 6$ , SigLIP ViT-SO400M/14@384, OpenCLIP $\\mathrm { C o n v N e X t - X X L @ 1 0 2 4 }$ , and DINOv2 ViT-L/14@518. We compare our method with two strong baselines: 1) concatenation-based [126] and 2) Re-sampler [11, 72]. Here, we include two variants of our SVA module. The standard one, “SVA”, uses $D = 3$ , $G = 1$ , and inserts cross-attention blocks inside the LLM with a layer stride of 3. To isolate the advantages of spatial inductive biases, we include another SVA variant, “SVA-no-multi-agg”, that does not add cross-attention blocks inside the LLM and sets $D = 3$ and $G = 3$ . Table 1 shows that SVA outperforms both baselines, with a significant improvement in the OCR & chart category. In contrast, the Resampler—which lacks spatial inductive biases—struggles to condense concatenated tokens from various vision towers into\n\nOCR (27.6%) RenderedText [134] (10.0 K) RefCOCO [140] (30.0 K) CLEVR [59] (350.0 K) Filtered DVQA (1550.0 K) VisText [124] (9.0 K) VizWiz [51] (20.0 K) TallyQA [1] (250.0 K) DVQA [61] (775.0 K) FinQA [31] (6.0 K) Visual7W [159] (14.0 K) Code (0.8%) Code SynthDog [67] (500.0 K) InfoVQA [16] (2.0 K) LAION GPT-4V [70] (11.0 K) Filtered WebSight (790.0 K) ArxivQA [76] (100.0 K) TAT-QA [158] (2.0 K) IDK [22] (11.0 K) WebSight [71] (10.0 K) OCRVQA [100] (80.0 K) HiTab [32] (2.0 K) OKVQA [95] (9.0 K) DaTikz [14] (47.0 K) H ScreenQA [56] (79.0 K) General (33.3%) HatefulMemes [66] (8.0 K) Design2Code [118] (0.5 K) WIkiSQL [153] (74.0 K) ALLaVA [25] (700.0 K) OODVQA [129] (8.0 K) Math (3.2%) Low-Level Vision [27] (50.0 K) Q-Instruct [135] (400.0 K) SketchyVQA [129] (8.0 K) Geo170K [42] (170.0 K) Cambrian-7M DocVQA [97] (39.0 K) LNQA [108] (302.0 K) Visualmrc [123] (3.0 K) RAVEN [148] (42.0 K) WTQ [106] (38.0 K) LVIS-Instruct4V [131] (220.0 K) Language (23.8%) GeomVerse [64] (9.0 K) ChartQA [96] (28.0 K) LLaVA150K [82] (150.0 K) OpenOrca [78] (994.0 K) MathVision [132] (3.0 K) IconQA [89] (27.0 K) VisualGenome [69] (86.0 K) MathInstruct [142] (262.0 K) Inter-GPS [90] (1.0 K) Chart2Text [62] (26.0 K) VQAv2 [50] (83.0 K) OrcaMath [99] (200.0 K) TQA [5] (1.0 K) TabMWP [88] (23.0 K) GPT4V Rewritten (77.0 K) WizardCoder [93] (143.0 K) Science (2.9%) TextCaps [119] (22.0 K) GQA [57] (72.0 K) OpenCodeInterpreter [152] (66.0 K) Data Engine (161.0 K) LLAVAR [149] (20.0 K) A-OKVQA [116] (50.0 K) Dolly [36] (11.0 K) PathVQA [53] (32.0 K) ST-VQA [17] (17.0 K) AlfWorld [146] (45.0 K) Counting (8.5%) ScienceQA [91] (12.0 K) AI2D [65] (15.0 K) ShareGPT [27] (40.0 K) Filtered CLEVR (350.0 K)\n\n<html><body><table><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaVA-665K</td><td>40.7</td><td>64.7 45.2</td><td>20.8</td><td>32.0</td></tr><tr><td>Cambrian-10M</td><td>54.8 68.7</td><td>51.6</td><td>47.3</td><td>51.4</td></tr><tr><td>Cambrian-7M</td><td>55.9</td><td>69.6 52.6</td><td>47.3</td><td>54.1</td></tr></table></body></html>\n\n![](images/18f7599d65602bac17a454035d7166b44046590182e72126badc9a1fd10b5c8f.jpg)  \nFigure 8: Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for MLLM. Left: The inner circle shows the original distribution of Cambrian-10M. The outer circle shows the curated Cambrian-7M. Right: All the data sources in the Cambrian dataset as well as the ones filtered in data curation.   \nTable 2: t value between 250k and Figure 9: Exploring instruction tun350k obtains better performance. ing data mixture ratios.   \nTable 3: Performance improves with better instruction tuning data curation.\n\na limited number of learnable queries via global cross-attention. We also compare SVA with other connectors in Appendix H and show clear advantages.\n\nFinding 8: Spatial inductive bias and deep interaction between LLM and vision feature help to better aggregate and condense vision features.\n\n# 4 Instruction Tuning Data for Training MLLMs\n\n# 4.1 Data Collection\n\nCollecting Instruction Tuning Data from existing data sources Unlike language data, multimodal (visual) instruction-tuning data is much rarer and harder to collect. To address this, we use existing multimodal benchmarks and datasets involving visual interaction data, such as Visual Question Answering (VQA) and OCR data. To help maintain conversational abilities [147], we also collect a small volume of high-quality language-only instruction-following data. We categorize data into General conversation, OCR, Counting, Code, Math, Science, and Language-only data. We list the data sources in Fig. 8, and the details of data preparation in Appendix G.\n\nTargeted Internet Data Collection Engine As observed in Fig. 8, there is an unbalanced distribution of data. Some categories, such as science, have very few data sources, and each source has limited samples. Inspired by previous works [73], we introduce a data engine to create large-scale, reliable, high-quality knowledge-based instruction tuning data (see Fig. 15). Details are in Appendix G.3. Our data engine produces a large volume of reliable scientific data, increasing the diversity in the data pool. We generate 161k science-related data points— $400 \\%$ more than the previous combined data sources.\n\nCambrian-10M We create a large pool of instruction tuning data, which we refer to as Cambrian10M. This pool contains approximately $9 7 8 4 \\mathrm { k }$ data points, offering a diverse range of data for our work and future research. We visualize its composition in Fig. 8.\n\n# 4.2 Data Curation\n\nCambrian-10M is a large pool of instruction tuning data sourced from a variety of data sources, with an unbalanced data ratio between categories. Here, we take a preliminary step to study data curation by improving data balancing and adjusting data ratios.\n\nTable 4: Comparison of Cambrian-1 with other leading MLLMs. Cambrian-1 outperforms other open-source models and achieves competitive performance, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. Despite using only 576 visual tokens, Cambrian-1 performs better on OCR & Chart and Vision-Centric benchmarks compared to Mini-Gemini-HD and LLaVA-NeXT, which use 2880 tokens.   \n\n<html><body><table><tr><td>Model</td><td colspan=\"6\"></td><td colspan=\"6\">Knowledge</td><td colspan=\"6\">OCR & Chart</td><td colspan=\"6\">Vision-Centric</td></tr><tr><td></td><td>yLSIA#</td><td>AV</td><td></td><td></td><td>EEEE</td><td>COD</td><td>A</td><td>VOS</td><td>AANNI</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Method</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>A</td><td></td><td></td><td></td><td></td><td></td><td>BAA</td><td>dAWW</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4V</td><td>UNK.</td><td>63.0</td><td>1409.4</td><td>75.8</td><td>69.1</td><td>36.8 -</td><td>65.2</td><td></td><td>75.7</td><td>56.8</td><td>49.9</td><td>78.2</td><td>77.4</td><td>78.5</td><td>64.5</td><td>78.0</td><td></td><td>88.4</td><td>62.4</td><td>50.0</td><td>61.4</td><td></td><td>64.3</td><td>73.8</td></tr><tr><td>Gemini-1.0 Pro</td><td>UNK.</td><td></td><td>1496.6</td><td>73.6</td><td>70.7</td><td></td><td></td><td></td><td>79.5</td><td>47.9</td><td>45.2</td><td>-</td><td></td><td>-</td><td>65.9</td><td></td><td>-</td><td></td><td>-</td><td></td><td></td><td>=</td><td></td><td>-</td></tr><tr><td>Gemini-1.5 Pro</td><td>UNK.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>58.5</td><td>52.1</td><td>80.3</td><td></td><td>81.3</td><td></td><td></td><td>73.5</td><td>86.5</td><td></td><td></td><td></td><td>67.5</td><td>：</td><td></td></tr><tr><td>Grok-1.5</td><td>UNK.</td><td></td><td>-</td><td>-</td><td></td><td></td><td></td><td></td><td>-</td><td>53.6</td><td>52.8</td><td>88.3</td><td></td><td>76.1</td><td></td><td></td><td>78.1</td><td>85.6</td><td></td><td></td><td></td><td>68.7</td><td>：</td><td>-</td></tr><tr><td>MM-1-8B</td><td>144</td><td>- ：</td><td>1529.3</td><td>72.3</td><td>69.9</td><td></td><td></td><td></td><td>72.6</td><td>37.0</td><td>35.9</td><td></td><td></td><td>，</td><td></td><td></td><td></td><td>：：</td><td></td><td></td><td></td><td></td><td>：</td><td>-</td></tr><tr><td>MM-1-30B</td><td>144</td><td></td><td>1637.6</td><td>75.1</td><td>72.1</td><td></td><td></td><td></td><td>81.0</td><td>44.7</td><td>39.4</td><td></td><td>：</td><td></td><td>：：</td><td></td><td></td><td></td><td></td><td>：：</td><td></td><td></td><td>：</td><td>-</td></tr><tr><td>Base LLM: Llama-3-Ins-8B</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>55.7</td><td>75.1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mini-Gemini-HD-8B</td><td>2880</td><td>72.7 72.5</td><td>1606.0 1603.7</td><td>72.7 72.1</td><td>73.2 72.7</td><td>64.5 65.2</td><td></td><td>55.6</td><td>72.8</td><td>37.3 41.7</td><td>37.0 36.3</td><td>73.5 71.6</td><td>62.9 63.9</td><td>59.1 69.5</td><td></td><td>47.7 49.0</td><td>70.2 64.6</td><td>74.6 72.6</td><td>51.5</td><td>18.7</td><td></td><td>62.1</td><td>62.2 62.2</td><td>63.0 65.3</td></tr><tr><td>LLaVA-NeXT-8B</td><td>2880 576</td><td>73.1</td><td>1,547.1</td><td>75.9</td><td>74.7</td><td>64.6</td><td></td><td>61.3</td><td>80.4</td><td>42.7</td><td>49.0</td><td>73.0</td><td>71.3</td><td>73.3</td><td>62.4</td><td></td><td>71.7</td><td>77.8</td><td>56.6 65.0</td><td>38.7 51.3</td><td>60.1 64.2</td><td></td><td>72.3</td><td>72.0</td></tr><tr><td>Cambrian-1-8B Base LLM: Vicuna-1.5-13B</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mini-Gemini-HD-13B</td><td>2880</td><td>70.7</td><td>1597.0</td><td>68.6</td><td>70.6</td><td>63.7</td><td></td><td>54.1</td><td>71.9</td><td>37.3</td><td>37.0</td><td>70.1</td><td>60.8</td><td>56.6</td><td>46.6</td><td></td><td>70.2</td><td>69.8</td><td>49.4</td><td>19.3</td><td></td><td>57.5</td><td>53.6</td><td>67.3</td></tr><tr><td>LLaVA-NeXT-13B</td><td>2880</td><td>69.9</td><td>1575.0</td><td>70.0</td><td>65.6</td><td>65.4</td><td></td><td>53.7</td><td>73.5</td><td>36.2</td><td>35.1</td><td>70.0</td><td>62.9</td><td>62.2</td><td>51.4</td><td></td><td>67.1</td><td>70.9</td><td>55.9</td><td>36.0</td><td></td><td>59.1</td><td>62.7</td><td>65.7</td></tr><tr><td>Cambrian-1-13B</td><td>576</td><td>73.7</td><td>1,610.4</td><td>75.7</td><td>74.4</td><td>64.3</td><td></td><td>60.2</td><td>79.3</td><td>40.0</td><td>48.0</td><td>73.6</td><td>71.3</td><td>73.8</td><td>61.9</td><td></td><td>72.8</td><td>76.8</td><td>62.2</td><td>41.3</td><td>63.0</td><td></td><td>72.5</td><td>71.8</td></tr><tr><td>BaseLLM:Hermes2-Yi-34B</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mini-Gemini-HD-34B</td><td>2880</td><td>76.2</td><td>1659.0</td><td>80.6</td><td>75.3</td><td>65.8</td><td></td><td>62.4</td><td>77.7</td><td>48.0</td><td>43.4</td><td>80.5</td><td>68.1</td><td>67.6</td><td>51.8 54.5</td><td></td><td>74.1</td><td>78.9</td><td>63.8 64.0</td><td>37.3 47.3</td><td>67.2 61.0</td><td></td><td>71.5 73.0</td><td>79.2 74.8</td></tr><tr><td>LLaVA-NeXT-34B Cambrian-1-34B</td><td>2880 576</td><td>76.0 76.8</td><td>1633.2 1689.3</td><td>79.3 81.4</td><td>75.9 75.3</td><td></td><td>67.1 65.8</td><td>62.5 67.0</td><td>81.8 85.6</td><td>46.7 49.7</td><td>46.5 53.2</td><td>74.9 79.7</td><td>67.7 71.9</td><td>68.7 75.6</td><td>60.0</td><td></td><td>69.5 76.7</td><td>78.1 75.5</td></table></body></html>\n\nData Balancing We follow previous work [109, 138] to set thresholds $t$ for the number of data points from a single data source. We choose $t = 1 5 0 k$ , $2 5 0 k$ , $3 5 0 k$ , and $4 5 0 k$ in this section and observe an elbow effect in Table 2—finding that a threshold between $2 5 0 k$ and $3 5 0 k$ work the best for Cambrian-10M. We also plot in Appendix G.4 the cumulative sum of counts for entries sorted by counts from tail to head and we see this intermediate threshold prevents explosive heavy tail.\n\nData Ratio Cambrian-10M is designed for visual instruction tuning. Given the various capabilities of different types of data, it is essential to balance the ratio of these data types. We conduct pilot experiments with a fixed dataset size of $1 3 5 0 \\mathrm { k }$ , examining the impact of different data ratios. We visualize the results in Fig. 9 and summarize our findings as follows: (i) Balancing General, OCR and Language data is crucial. (ii) Performance on knowledge-intensive tasks is influenced by multiple factors, often requiring a mix of OCR, chart, reasoning, and general perception.\n\nCambrian-7M By applying data filtering to Cambrian-10M with our identified data ratio, we create a smaller but higher-quality dataset called Cambrian-7M. Table 3 showcases the benefits of a wellbalanced and carefully curated dataset. Despite having fewer samples, Cambrian-7M demonstrates improved performance. We additionally apply system prompts in Cambrian-7M to avoid the \"answer machine phenomenon\", see more details in Appendix G.2.\n\n# 5 State of the Art Performance\n\nFinally, we leverage the insights from all our previous studies to train a family of MLLMs we call Cambrian-1. We train models using LLM backbones of various scales: LLaMA-3-Instruct-8B [4], Vicuna-1.5-13B [151], and Hermes-2-Yi-34B [139]. Our vision component combines four models— OpenAI CLIP ViT-L/14@336, SigLIP ViT-SO400M/14@384, OpenCLIP ConvNeXt-XXL $@ 1 0 2 4$ , and DINOv2 ViT-L/14@518 (Section 2.5)—via the Spatial Vision Aggregator (Section 3). We pre-train the connector using $2 . 5 \\mathbf { M }$ adapter data and instruction tune using our Cambrian-7M data mix (Section 4.2). Our models are evaluated on the benchmarks categorized in Section 2.1, with results presented in Table 4.††.\n\nCambrian-1 surpasses open-source models like LLaVA-NeXT and Mini-Gemini. Thanks to the SVA, Cambrian-1 excels in tasks requiring high-resolution image processing, even with only 576 image tokens—about 1/5 of the tokens used by LLaVA-NeXT and Mini-Gemini. Cambrian-1 also achieves comparable performance to the best proprietary models, such as GPT-4V, Gemini-Pro, and MM-1, on several benchmarks. We provide model weights, open-source code, datasets, and detailed recipes for model training and evaluation. We hope our work will strengthen the open research community and accelerate research in both visual representation learning and multimodal systems.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n当前多模态大语言模型（MLLMs）在视觉组件设计选择上探索不足，与视觉表征学习研究脱节，导致在真实场景中难以实现准确的感官基础。同时，现有MLLM基准存在难以整合和解释不同任务结果的问题，且缺乏能有效评估视觉表征的基准，影响对模型的准确评估。这些问题限制了MLLMs在实际应用中的性能和发展，解决这些问题对于提升MLLMs的视觉理解能力、推动多模态系统和视觉表征学习的发展具有重要意义。\\n\\n**方法概述**\\n论文提出了Cambrian - 1系列MLLMs，采用以视觉为中心的方法，通过LLMs和视觉指令调优评估各种视觉表征，引入新的视觉中心基准CV - Bench，提出空间视觉聚合器（SVA），并强调高质量视觉指令调优数据的策划。\\n\\n**主要贡献与效果**\\n- 引入新的视觉中心基准CV - Bench，提供2638个手动检查的示例，分别是RealWorldQA和MMVP的3.5倍和8.8倍，有助于更准确地评估模型的视觉中心能力。\\n- 提出的SVA模块在聚合高分辨率视觉信息方面表现出色，在General、Knowledge、OCR&Chart和Vision - Centric等类别上均优于其他基线模型，如在OCR&Chart类别上，SVA得分为55.5，高于Concat.的50.1、Resampler的27.1和SVA - no - multi - agg的55.2。\\n- 训练的Cambrian - 1系列模型超越了其他开源模型，在使用仅576个视觉标记的情况下，在OCR&图表和视觉中心基准上的表现优于使用2880个标记的Mini - Gemini - HD和LLaVA - NeXT。经过数据筛选和优化比例得到的Cambrian - 7M数据集，虽样本量从约9784k减少到约7M，但性能提升，说明高质量数据策划的有效性。此外，使用数据引擎生成了161k科学相关数据点，比之前的组合数据源增加了400%，提高了数据多样性。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n以视觉为中心设计多模态大语言模型，通过视觉指令调优评估多种视觉表征，解决视觉组件设计与视觉表征学习研究脱节的问题。利用空间视觉聚合器（SVA）有效整合高分辨率视觉特征与大语言模型，减少标记数量，同时强调数据来源平衡和分布比例，提升视觉基础能力。该方法有效是因为它解决了现有模型在视觉组件设计和数据利用上的不足，更贴合真实场景的需求。\\n\\n**创新点**\\n先前的工作对视觉组件设计探索不足，现有基准在评估视觉表征和多模态能力方面存在局限性。本文系统评估各种视觉编码器对MLLM多模态能力的影响，为视觉表征学习提供新见解；引入新的视觉中心基准CV - Bench，解决了现有基准样本少和难以评估视觉中心能力的问题；提出SVA，通过引入空间归纳偏差和多层视觉聚合，有效聚合多视觉编码器的特征，减少信息损失，优于传统的聚合方法。此外，引入数据引擎生成大规模可靠的科学知识指令调优数据，提高了数据多样性。\\n\\n**具体实现步骤**\\n1. **评估视觉表征**：使用23种不同的视觉骨干训练MLLMs，通过两阶段指令调优过程，先在120万适配器数据上训练连接器，再在73.7万指令调优数据上微调连接器和LLM。比较MLLMs在有和无视觉输入下的性能，计算随机猜测的预期分数，对基准进行聚类分析，确定基准是否真正需要视觉输入来解决问题。\\n2. **引入CV - Bench**：将标准视觉基准重新用于VQA问题，利用丰富的地面真值注释，制定自然语言问题，评估模型的2D和3D理解能力，通过空间关系、对象计数评估2D理解，通过深度顺序和相对距离评估3D理解。\\n3. **确定指令调优策略**：采用两阶段训练，使用120万个适配器数据预训练连接器，再在73.7万个指令调优数据上微调LLM和连接器。实验表明两阶段训练有益，更多适配器数据可进一步提高性能，解冻视觉编码器通常对所有类别都有益。\\n4. **提出SVA**：创建可学习的潜在查询，通过交叉注意力层与多个视觉特征交互，引入空间归纳偏置，确保每个查询令牌与视觉特征图的特定子区域对齐；在LLM层中多次插入交叉注意力，实现多层视觉聚合；引入超参数D和G来灵活调节容量。\\n5. **数据策划**：从现有多模态基准和数据集收集视觉交互数据，如VQA和OCR数据，同时收集少量高质量纯语言指令跟随数据以维持对话能力。引入数据引擎生成大规模可靠的科学知识指令调优数据。创建Cambrian - 10M数据集，通过设置阈值（在250k - 350k之间效果最佳）进行数据平衡，通过实验确定数据比例，创建更高质量的Cambrian - 7M。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\nConcat. [126]、Resampler [58]、SVA - no - multi - agg、LLaVA - 665K、MM - 1 - 8B、MM - 1 - 30B、Mini - Gemini - HD - 8B、LLaVA - NeXT - 8B、Mini - Gemini - HD - 13B、LLaVA - NeXT - 13B、Mini - Gemini - HD - 34B、LLaVA - NeXT - 34B、GPT - 4V、Gemini - 1.0 Pro、Gemini - 1.5 Pro、Grok - 1.5等。\\n\\n**性能对比**\\n*   **在通用基准上**：本文的SVA模块达到了68.5，优于Concat. (67.2)、Resampler (63.1)和SVA - no - multi - agg (68.0)。Cambrian - 1 - 8B (73.1)、Cambrian - 1 - 13B (73.7)和Cambrian - 1 - 34B (76.8)的表现优于部分开源模型，如LLaVA - 665K (40.7)、Mini - Gemini - HD - 8B (72.7)、Mini - Gemini - HD - 13B (70.7)、LLaVA - NeXT - 13B (69.9)等，且与部分专有模型如GPT - 4V (63.0)、Gemini - 1.0 Pro (UNK.)等相比也具有竞争力。\\n*   **在知识基准上**：SVA为49.7，高于Concat. (48.9)、Resampler (46.5)和SVA - no - multi - agg (49.5)。Cambrian - 1系列模型在知识基准上也有较好表现，如Cambrian - 1 - 8B、Cambrian - 1 - 13B和Cambrian - 1 - 34B在知识基准上的得分优于部分开源模型。\\n*   **在OCR&图表基准上**：SVA达到55.5，显著高于Concat. (50.1)、Resampler (27.1)和SVA - no - multi - agg (55.2)。Cambrian - 1系列模型在该基准上表现出色，如Cambrian - 1 - 8B、Cambrian - 1 - 13B和Cambrian - 1 - 34B在OCR&图表基准上的得分高于使用2880个标记的Mini - Gemini - HD和LLaVA - NeXT。\\n*   **在视觉中心基准上**：SVA为53.2，高于Concat. (52.6)、Resampler (42.6)和SVA - no - multi - agg (52.6)。Cambrian - 1系列模型在视觉中心基准上也有优势，在使用仅576个视觉标记的情况下，优于使用2880个标记的Mini - Gemini - HD和LLaVA - NeXT。\",\n    \"keywords\": \"### 关键词\\n\\n- 多模态大语言模型 (Multimodal Large Language Models, MLLMs)\\n- 视觉表征评估 (Visual Representation Evaluation, N/A)\\n- 空间视觉聚合器 (Spatial Vision Aggregator, SVA)\\n- 视觉中心基准 (Vision - Centric Benchmark, CV - Bench)\\n- 视觉指令调优数据策划 (Visual Instruction Tuning Data Curation, N/A)\"\n}"
}