{
    "link": "https://arxiv.org/abs/2410.05695",
    "pdf_link": "https://arxiv.org/pdf/2410.05695",
    "title": "Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought",
    "authors": [
        "Qiguang Chen",
        "Libo Qin",
        "Jiaqi Wang",
        "Jinxuan Zhou",
        "Wanxiang Che"
    ],
    "institutions": [
        "SCIR"
    ],
    "publication_date": "2024-10-08",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 39,
    "influential_citation_count": 2,
    "paper_content": "# Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought\n\nQiguang Chen‚Ä† Libo $\\mathbf { Q } \\mathbf { i n } ^ { \\ddag \\ast }$ Jiaqi Wang‚ô¢ Jinxuan Zhou‚Ä° Wanxiang Che‚Ä†‚àó\n\n‚Ä† Research Center for Social Computing and Information Retrieval ‚Ä† Harbin Institute of Technology ‚Ä° School of Computer Science and Engineering, Central South University ‚ô¢ The Chinese University of Hong Kong {qgchen,car}@ir.hit.edu.cn, lbqin@csu.edu.cn\n\n# Abstract\n\nChain-of-Thought (CoT) reasoning has emerged as a promising approach for enhancing the performance of large language models (LLMs) on complex reasoning tasks. Recently, a series of studies attempt to explain the mechanisms underlying CoT, aiming to deepen the understanding of its efficacy. Nevertheless, the existing research faces two major challenges: (1) a lack of quantitative metrics to assess CoT capabilities and (2) a dearth of guidance on optimizing CoT performance. Motivated by this, in this work, we introduce a novel reasoning boundary framework (RBF) to address these challenges. To solve the lack of quantification, we first define a reasoning boundary (RB) to quantify the upper-bound of CoT and establish a combination law for RB, enabling a practical quantitative approach applicable to various real-world CoT tasks. To address the lack of optimization, we propose three categories of RBs. We further optimize these categories with combination laws focused on RB promotion and reasoning path optimization for CoT improvement. Through extensive experiments on 27 models and 5 tasks, the study validates the existence and rationality of the proposed framework. Furthermore, it explains the effectiveness of $1 0 \\mathrm { C o T }$ strategies and guides optimization from two perspectives. We hope this work can provide a comprehensive understanding of the boundaries and optimization strategies for reasoning in LLMs. Our code and data are available at https://github.com/LightChen233/reasoning-boundary.\n\n# 1 Introduction\n\nIn recent years, Large Language Models (LLMs) have demonstrated increasing capabilities and applications across various tasks [Zhao et al., 2023, Chang et al., 2023, Pan et al., 2023, Qin et al., 2024a]. Notably, advanced LLMs, such as GPT [Brown et al., 2020, OpenAI, 2022, 2023], PaLM [Anil et al., 2023] and LlaMa [Touvron et al., 2023a,b, Meta, 2024] series have demonstrated emergent capabilities, particularly like Chain-of-Thought (CoT) [Nye et al., 2022, Wei et al., 2022]. This methodology enables models to verbalize step-by-step reasoning, thereby enhancing prediction accuracy by basing decisions on the logical rationale [Wei et al., 2022, Kojima et al., 2022, Hu et al., 2024, Qin et al., 2023, Zhuang et al., 2023, Chen et al., 2024a].\n\nRecently, some research in the literature has begun to investigate the mechanism of CoT to enhance the understanding of its operational nature. To this end, Madaan et al. [2023] and Wang et al. [2023a] first give a qualitative boundary conclusion through a large number of experiments on the natural language planning capability: The CoT is limited by the reasoning logic in the context demonstrations. Bi et al. [2024] investigate these boundaries on the code planning capability, by training LLMs on CoT samples of varying difficulties. It demonstrates LLMs are unable to learn or effectively manage tasks that exceed a certain complexity upper-bound. To delve deeper into potential constraints of CoT, Feng et al. [2024] develop a theoretical framework on the single-step calculation capability, suggesting that there is an upper-bound of model performance dependent on the length of input in single-step reasoning processes. Although existing research has made some progress, where the boundaries of CoT lie and how these boundaries affect the performance of CoT are still unresolved questions. Specifically, the existing work still faces two major challenges:\n\n‚Ä¢ Lacking quantification metrics for CoT: Current research primarily relies on qualitative assessments of CoT performance, which leads to the absence of quantitative metrics. It hinders the ability to objectively compare different CoT approaches and establish a definitive upper-bound for CoT capabilities.   \n‚Ä¢ Lacking optimization guidance for CoT: While current research prioritizes understanding the mechanisms underlying CoT reasoning, there is a dearth of guidance on optimizing CoT performance. This gap hinders the transformation of CoT research into actionable strategies for enhancing model capabilities.\n\nMotivated by this, in this work, we introduce a reasoning boundary framework (RBF) to thoroughly examine and optimize the boundaries of current LLMs. Specifically, to address the quantification challenge, we propose a new concept, named reasoning boundary (RB) to quantify the upper-bound on task-specific reasoning complexity within a model. Furthermore, to explore more practical scenarios, we present the combination law of RBs to generalize the RB for quantification in more real and complex scenarios. To address the CoT optimization challenge, we propose and analyze three reasoning boundary intervals, guiding optimization through improved RB and optimized reasoning paths based on the combination law, which achieves state-of-the-art performance in our proposed benchmark. We extensively validate the efficacy of our framework across 27 models and 5 tasks: arithmetic computing, mathematical reasoning, multi-hop question answering, and multilingual mathematical reasoning.\n\nOur main contributions are as follows:\n\n‚Ä¢ To the best of our knowledge, this is the first work to propose a reasoning boundary framework (RBF) to quantify the upper-bound of CoT. Furthermore, we establish the combination law of RB as the weighted harmonic mean of fundamental RBs to address practical CoT tasks. ‚Ä¢ To solve the lack of CoT optimization, we define three categories of RBs. Based on the combination law and the nature of these RBs, we effectively improve the existing CoT strategies by RB promotion and reasoning path optimization. ‚Ä¢ We validate the existence and rationality of our framework on 27 models and 5 CoT tasks. Furthermore, we explain the optimal performance from two optimization perspectives in numerous CoT strategies. We consider both optimal perspectives and propose a minimum acceptable reasoning path (MARP) prompting to achieve state-of-the-art performance.\n\n# 2 Quantification Methodology\n\n# 2.1 Reasoning Boundary\n\nIn order to quantify the capacity for complex reasoning in LLMs, we introduce an upper-bound concept termed reasoning boundary (RB), which formally defines the degree of ease that an LLM can handle within a specific reasoning process. In simpler terms, as shown in Figure 1 (a), RB reflects the limit beyond which a model‚Äôs accuracy significantly degrades. Mathematically, RB is defined for a model $m$ and a task $t$ as the maximum of problem difficulty $d$ at which the model‚Äôs accuracy reaches a predefined threshold $K _ { 1 }$ :\n\n$$\nB _ { A c c = K _ { 1 } } ( t | m ) = \\operatorname* { s u p } _ { d } \\{ d | A c c ( t | d , m ) = K _ { 1 } \\} ,\n$$\n\nwhere $A c c ( t | d , m )$ represents the accuracy of the model‚Äôs accuracy on task $t$ with difficulty $d$ .   \nDifficulty can be measured by factors like the number of reasoning steps or computational complexity.   \nFor brevity, we denote RB as $B ( t | m )$ in subsequent sections.\n\n(a) Reasoning Boundary (¬ß2.1) (b) Combination Law of Reasoning Boundary (¬ß2.2) Arithmetical Calculation $^ +$ Planning $$ Mathematical Reasoning ùìëAc=90% Acc=90% (1+2)\\*3 ‚Äì 4 = ? ùìëAc=50% Acc=50% 5 1+2=?; 3\\*3=?; ‚Ä¶ cAulraitihomneRtiBca(lùìëC(aùíÇl)-) Weighted Harmonic Mean ReasoMnaitnhgeRmBat(icùìëa(lùíÇ, ùíë)) calculate that Joan is 1+2=3 Step 1: First you need to (112+21)\\*3=? 399 Natural Language ySteeapr o2l: .Then you need to ùìëAc=10% Acc=10% Step1:PlFairnsnt iynoguRneBe(dùìët(oùíë‚Ä¶)) cyaelacrusloatled‚Ä¶that Jack is $3 ^ { * } 3 \\mathrm { = } 9$ (1123+231)\\*332 1,209,653 Step2: Then you need to (c) Categories of Reasoning Boundary (¬ß2.3) ùìëAcc‚â•90% ùìë10%<Acc<90% ùìëAcc‚â§10% Reasoning Completely Feasible Reasoning Partially Feasible Reasoning Completely Infeasible Reasoning Boundary Boundary (CFRB) Boundary (PFRB) Boundary (CIRB)\n\nConclusion: The reasoning boundary for a model is defined by its ability to achieve a specific accuracy for a given task difficulty.\n\n# 2.2 Combination Law of Reasoning Boundary\n\nIn practical scenarios, models often require the integration of multiple capabilities to address a single task effectively. To quantify how a large language model can be boosted by the cooperation of multiple capabilities through the CoT mechanism, we introduce the ‚ÄúCombination Law of $R B ^ { \\prime }$ , giving a concrete formula of the upper-bound of the CoT. The law estimates the unified reasoning boundary $\\mathcal { B } _ { \\mathrm { A c c } = K _ { 1 } } ( t _ { 1 } , t _ { 2 } , \\dots , t _ { n } | m )$ for $n$ tasks within a model $m$ , which is formulated as:\n\n$$\n\\begin{array} { r } { \\mathcal { B } _ { \\mathrm { A c c } = K _ { 1 } } \\bigl ( t _ { 1 } , t _ { 2 } , \\ldots , t _ { n } \\vert m \\bigr ) \\approx \\frac { 1 } { \\bigl ( n - 1 \\bigr ) \\sum _ { i = 1 } ^ { n } \\frac { N _ { i } } { \\mathcal { B } _ { \\mathrm { A c c } = K _ { 1 } } ( t _ { i } \\vert m ) - b _ { i } } } , } \\end{array}\n$$\n\nwhere $B _ { \\mathrm { A c c } = K _ { 1 } } ( t _ { i } | m )$ denotes the reasoning boundary of model $m$ for task $t _ { i }$ . $N _ { i }$ , and $b _ { i }$ are scaling factors, which are only affected by the related task. As shown in Figure 1 (b), Equation (2) provides a mathematical formula to estimate the combined RBs from the independent ones, enabling deeper insights into model behavior for intricate tasks. See Appendix A for detailed mathematical analysis.\n\nFurthermore, the combination law for reasoning boundary demonstrates favorable theoretical properties, with broad applicability across diverse scenarios and flexibility in accommodating various boundary segmentation methods. For detailed practical application, please refer to Appendix B.\n\nConclusion: The combination law of reasoning boundary satisfies the weighted harmonic average of each basic reasoning boundary.\n\n# 2.3 Categories of Reasoning Boundary\n\nFurthermore, in order to guide the optimization of CoT and more convenient expression, as shown in Figure 1 (c), we define the following three categories of RBs based on their empirical accuracy:\n\nCompletely Feasible Reasoning Boundary: We define that the part with an accuracy greater than $90 \\%$ is a completely feasible reasoning boundary $( \\mathrm { C F R B } = \\mathcal { B } _ { \\mathrm { A c c \\geq 9 0 \\mathcal { Y } _ { 0 } } } ( t _ { 1 } , t _ { 2 } , \\dots , t _ { n } | m ) )$ , which means that LLMs can effectively grasp the performance of this part.\n\nCompletely Infeasible Reasoning Boundary: We believe that the part with an accuracy less than $10 \\%$ is a completely infeasible reasoning boundary $( \\mathrm { C I R B } = \\mathcal { B } _ { \\mathrm { A c c \\leq 1 0 \\% } } ( t _ { 1 } , t _ { 2 } , \\dots , t _ { n } | m ) )$ , which means that the model can never effectively grasp the performance of this part.\n\nPartially Feasible Reasoning Boundary: We define the RB in the rest part except CFRB and CIRB as a partially feasible reasoning boundary $( \\mathtt { P F R B } = \\mathcal { B } _ { 1 0 \\% < \\mathrm { A c c } < 9 0 \\% } ( t _ { 1 } , t _ { 2 } , . . . , t _ { n } | m ) )$ , which requires the model to repeat thinking or more clear information to solve the problem.\n\n100 100 . Correct Incorrect CFRB PFRB CIRB 1000 1500 640 Accuracy (%) 8 40 =2e6 500 20 20 x\\*y =2.2e5 0 0 ‚ñ° 0 500 1000 1500 2000 1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15 x The number of planning step $\\mathbf { \\sigma } ( { \\mathcal { B } } ( { \\mathfrak { p } } ) )$ The number of planning step $\\mathbf { \\gamma } ( \\mathcal { B } ( \\mathbf { p } ) )$ (a) Distribution of correct predictions (b) Distribution of correct predictions (c) Distribution of correct predictions for $\\mathbf { x } ^ { * } \\mathbf { y }$ samples. for nature language planning. for code planning.\n\nWe analyze the nature of these three categories of RB in detail (in Section 4.3), and further utilize the combination law to optimize these three reasoning boundaries (in Section 5), so as to provide effective suggestions and guidance to support future CoT optimization.\n\n# 3 Experimental Setup\n\nBenchmark Settings To assess the reasoning boundaries of LLMs, we require a dataset rich in RB. This necessitates tasks with evenly distributed complexities and reasoning steps that challenge the models‚Äô upper-bounds. To meet these requirements, we introduce BIGGSM, a new dataset offering greater calculation complexity and longer reasoning chains. The detailed construction process for BIGGSM is provided in Appendix C.\n\nModel Settings Except for model expansion experiments, all experiments are conducted on GPT3.5-Turbo. Following the setting of Wei et al. [2022], in our CoT experiment, all multi-step reasoning tasks utilize three manually constructed demonstrations. In addition, for all the experiments, top-p is selected from $\\{ 0 . 9 5 , 1 \\}$ . Temperature is selected from $[ 0 , 1 ]$ and serves as the main error variable.\n\n# 4 Empirical Analysis of Reasoning Boundary\n\n# 4.1 Existence Verification for Reasoning Boundary\n\nIn this study, we investigate the hypothesis that an LLM exhibits varying levels of reasoning boundary across various tasks. To this end, we will verify whether the model has widespread reasoning boundary in various tasks in the following three tasks:\n\nBasic Arithmetic Calculation First, to investigate the existence of RB, we first examine basic arithmetic operations (including addition, subtraction, multiplication, and division). As illustrated in Figure 2 (a), the results reveal significant performance variations across three distinct regions. For multiplication, accuracy surpasses $90 \\%$ for results up to $2 . 2 e 5$ . Conversely, accuracy falls below $10 \\%$ for products exceeding 2e6. Similar presences of varying RBs are observed for other operations, which verifies the existence of reasoning boundary in basic arithmetic calculation tasks. Further results and implementation details are provided in Appendix D.\n\nNature Language Planning We further investigate RB in natural language planning tasks for mathematical reasoning. We prompt the model to generate plans and assess their accuracy through manual evaluation. There is a strong correlation between the number of reasoning steps and LLMs‚Äô performance in Figure 2 (b). When the model meets the question with fewer than 2 reasoning steps, accuracy surpasses $90 \\%$ . Conversely, when reasoning steps exceed 4, accuracy falls below $10 \\%$ . This finding suggests that there are also three different RB categories in natural language planning tasks.\n\nCode Planning For further extensive exploration, we further prompt LLMs by PAL [Gao et al., 2023] to generate code-format plans and evaluate them by manual annotation. As shown in Figure 2 (c), the code planning task is similar to natural language planning, which is also an obvious division and different categories of RBs. Notably, since code planning utilizes code for clearer logic and reduced expression complexity, its planning accuracy surpasses that of natural language planning.\n\nCorrect sample X Incorrect sample CFRB PFRB CIRB Maximum multiplication calculation value (ùìë(m)) 1e5 Maximum multiplication calculation value (ùìë(m)) 3e5 ÔºÅ XX X The number of entity (ùìë(e))12108642 xx X X 8e4 O √ó\\* X 2.5e5 √ó x√ó√ó 64e4 XxX 1.52e5 B XX 2e4 51e54 ÁæéÂ•≥ÂÆâ L Ëå∂ 0 0 5 X 10 xx 15 20 0 1 i 4 ÁæéËè± 7 + X 10 13 ? 16 0 10 20 30 40 The number of calculation step (ùìë(s)) The number of planning step (ùìë(p)) The number of hop (ùìë(h)) (a) The combination law of different (b) The combination law of different (c) The combination law of different reasoning boundaries in complex reasoning boundaries in mathematical reasoning boundaries in multi-hop calculation task. reasoning task. question-answering task.\n\n# 4.2 Combination Law Verification on Different Tasks\n\nCombination Law in Complex Arithmetic Calculation Building on the proof of Equation (13), we hypothesize that the combination law for RB in the complex arithmetic calculation is the harmonic average of the arithmetic calculation RB and calculation planning RB. To verify this, we designed an experiment focusing on formulas containing addition, subtraction, and multiplication, like ‚Äú ${ \\bf \\bar { \\Psi } } ( 1 + { \\bf \\Psi }$ $2 ) * 3 - 4 ^ { \\prime \\prime }$ . Since addition and subtraction complexities are assumed to be around 1e15 (as shown in Figure 13), the arithmetic calculation RB primarily depends on the multiplication RB and calculation planning RB. Therefore, as shown in Figure 3 (a), there are two obvious RB lines, namely $B _ { A c c = 9 0 \\% }$ and ${ B _ { A c c = 1 0 \\% } }$ , which are completely consistent with the combination law of these basic RB based on the Equation (2). Besides, these two lines also clearly divide the RBs into three categories.\n\nCombination Law in Mathematical Reasoning Inspired by Tan [2023b], Xiao and Liu [2024], we posit that the natural language mathematical CoT task is determined by two sub-tasks: step planning task and step calculation task for global logic planning and local mathematical calculation. Furthermore, each model output step requires a single basic operation, resulting in a step calculation boundary close to the maximum number of multiplications, denoted by $\\vec { B } ( c \\bar { ) } \\approx B ( m \\bar { ) }$ . Formally, with step planning RB denoted by $( B ( p ) )$ and the step calculation RB by $( B ( c ) )$ , then the combined RB satisfies the following law:\n\n$$\n\\mathcal { B } ^ { \\mathrm { c o T } } ( c , p ) = \\frac { 1 } { \\frac { N _ { 1 } } { ( \\mathcal { B } ( c ) - b _ { 1 } ) } + \\frac { N _ { 2 } } { ( \\mathcal { B } ( p ) - b _ { 2 } ) } } .\n$$\n\nAs illustrated in Figure 3 (b), the actual performance distribution of RB (including $B _ { A c c = 9 0 \\% }$ and $\\scriptstyle B _ { A c c = 1 0 \\% } ,$ ) in natural language mathematical reasoning task fully aligns with the proposed combination law in Equation (3). Additionally, there are also obviously three RBs in Figure 3 (b).\n\nCombination Law in Multi-hop Reasoning Beyond the realm of mathematics, we further extend our exploration of the combination law to the field of multi-hop question answering. Specifically, we validate our law on HotpotQA [Yang et al., 2018], where we define the reasoning boundary as the combination of global hop-planning RB and local knowledge entity reasoning RB. As shown in Figure 3 (c), $B _ { A c c = 9 0 \\% }$ and $B _ { A c c = 1 0 \\% }$ also satisfy the weighted harmonic mean of these two sub-reasoning boundaries. It is also proved that, in addition to math-related tasks, multi-hop question answering also satisfies our proposed combined law and also exhibits three distinct RBs. We will describe in detail how to calculate the combination law on multi-hop reasoning in Appendix E.\n\n# 4.3 Nature Analysis for different Reasoning Boundary\n\nAccording to the definition of different RBs, we have divided the problem into three parts for LLMs. In this section, we will verify whether the defined RB adheres to the intrinsic nature of the model itself. We will discuss the natures of these RBs in detail:\n\nCFRB means complete mastery of the model even without demonstration. According to the definition, we assume that a question within CFRB implies a comprehensive understanding of the associated issue for a certain LLM. To verify this, following Zhang et al. [2022] and Wei et al. [2022], we formulate a mathematical request and generate chain-of-thought rationale and answer through\n\nCFRB PFRB CIRB CFRB PFRB CIRB ŒîCFRB ŒîPFRB ŒîCIRB CFRB PCFRB IFRB Error 8650 84.1 896570 89.7 89.7 90.7 91.6 5 Accuracy (%) 75 77.1 5305 54.9 53.0 54.8 56.5 57.4 3 Œî Accuracy (%) 55 51.1 50.0 45 2 1% 30 ‚ñ≥ 9% 12.7 20 1 CFRB PFRB CIRB 9.5 9.5 9.5 9.5 25 5 10 65% Reasoning Boundary Type #1 #3 #4 #5 (a) The accuracy distribution of The size of self-consistency (c) The accuracy (top) and quantity generated rationales based on Auto- (b) Model self-consistency integrated performance in dif- (bottom) distribution of synthetic CoT and Zero-CoT. ferent Reasoning Boundary areas. samples from Synthetic-CoT.\n\nzero-shot prompting without any demonstration. As shown in Figure 4 (a), it still achieves $2 9 . 2 \\%$ improvement in CFRB on generating the correct rationale compared to other RBs. This also proves that the model can indeed master tasks well on the questions in CFRB.\n\nPFRB means moderate confidence in its solution and needs consensus building process. To gauge the level of performance and confidence, we draw parallels to human decision-making, where moderate confidence often necessitates multiple times of consensus building. Inspired by this, we investigate it on Self-Consistency [Wang et al., 2022], which integrates results from various reasoning answers to reach a conclusive answer. Figure 4 (b) demonstrates that as the integration of reasoning paths increases, the accuracy improves significantly within PFRB compared with other RBs. This suggests that within PFRB, the LLM exhibits moderate confidence in solving problems, which needs multiple consensus building.\n\nCIRB exhibits poor reasoning performance even with consensus building. As illustrated in Figure 4 (a), questions in CIRB display extremely low accuracy (around $9 . 5 \\%$ ). And the model shows consistently poor performance and no improvement on Self-consistency in this boundary in Figure 4. It signifies that the model exhibits poor reasoning performance.\n\nLLM has self-awareness of its own RBs. In parallel, a natural question arises: Is the model capable of discerning its inherent RBs? To investigate this, we employ the Synthetic-CoT [Shao et al., 2023] to prompt LLM to generate CoT data. As depicted in Figure 4 (c), the results demonstrated that there are over $65 \\%$ of generated samples within CFRB, which achieves a much higher percentage and performance than other RBs. This suggests that LLMs possess an intrinsic understanding of their RBs and constraints to generate the task they grasp, indicative of a potential for self-assessment.\n\nTakeaways: (1) Reasoning boundary (RB) and the combination law of RB are both widespread across a series of tasks. (2) Different categories of RB can reflect the corresponding performance, and the model can also have a self-understanding of its own RB.\n\n# 5 RB-based CoT Optimization\n\n# 5.1 How can we improve CoT by optimizing RB?\n\nBased on our framework, the reasoning boundary limits the performance of the model. The simplest approach to improve CoT is to optimize the step calculation RB $\\boldsymbol { B } ( \\boldsymbol { c } )$ to promote the value of RB. Specifically, Tool-Usage [Paranjape et al., 2023] and Program-of-Thought (PoT) [Chen et al., 2024b] have shown significant success in CoT optimization. We explain the rationale behind their effectiveness, why PoT consistently outperforms direct Tool Usage [Yao et al., 2023, Chen et al., 2023], and take them as examples to demonstrate how to improve CoT by promoting RB.\n\nTool Usage can boost the value of RB for an LLM. When the model uses tools [Paranjape et al., 2023], we can simply think that the model can perform calculations with infinite precision, so that the RB of mathematical calculations tends to infinity, viz ${ \\cal B } ( c )  + \\infty$ . It is obvious that the combined\n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"3\">BIGGSM</td></tr><tr><td>Acc. (‚Üë)</td><td>Input Token (‚Üì)</td><td>Output Token (‚Üì)</td></tr><tr><td>CoT</td><td>57.00 ¬±0.93</td><td>780.43</td><td>96.76 ¬±3.22</td></tr><tr><td colspan=\"4\">RB-Optimized Methods</td></tr><tr><td>Tool Usage</td><td>71.64 ¬±0.66</td><td>688.43</td><td>129.53 ¬±3.82</td></tr><tr><td>PoT</td><td>78.25 ¬±1.09</td><td>657.43</td><td>78.25 ¬±1.09</td></tr><tr><td colspan=\"4\">Reasoning-Path-Optimized Methods</td></tr><tr><td>Least-to-most</td><td>58.25 ¬±3.28</td><td>679.59</td><td>176.09 ¬±15.22</td></tr><tr><td>Complex-CoT</td><td>59.78 ¬±0.60</td><td>1111.43</td><td>131.82 ¬±1.91</td></tr><tr><td>CoT+MARP</td><td>64.37 ¬±2.24</td><td>614.43</td><td>95.12 ¬±0.77</td></tr><tr><td>PoT+MARP</td><td>80.55 ¬±2.40</td><td>576.43</td><td>76.34 ¬±2.84</td></tr></table></body></html>\n\nTable 1: Main experimental results on GPT-3.5-Turbo.   \nResults on different benchmarks are shown in Table 2.\n\n![](images/dd9be8e055cb0b5c6cfdbe165b214dd5f603ced72180639d37d1cbb8ba794b43.jpg)  \nFigure 5: Analysis of the impact of ToolUsage and PoT on reasoning boundary $B ( c , \\bar { p } )$ .\n\nRB of the model can be calculated as:\n\n$$\n\\mathcal { B } ^ { \\mathrm { T o o l } } ( c , p ) = \\operatorname* { l i m } _ { \\mathcal { B } ( c ) \\to + \\infty } \\frac { 1 } { \\frac { N _ { 1 } } { ( \\mathcal { B } ( c ) - b _ { 1 } ) } + \\frac { N _ { 2 } } { ( \\mathcal { B } ( p ) - b _ { 2 } ) } } = \\frac { \\mathcal { B } ( p ) - b _ { 2 } } { N _ { 2 } } .\n$$\n\nEasy to get, $\\beta ^ { \\mathrm { T o o l } } ( c , p ) > B ^ { \\mathrm { c o T } } ( c , p )$ , this shows that Tool Usage can improve the boundary of reasoning. This explains why Tool Usage can have better performance than vanilla CoT (as shown in Table 1). Furthermore, as shown in Figure 5, the distribution of theoretical RB and the actual one almost perfectly coincide. This also demonstrates the reliability and applicability of our theory.\n\nProgram-of-Thought can further enhance the value of LLM‚Äôs RB. Equation (4) reveals that an LLM‚Äôs RB hinges entirely on its planning capability. Since natural language can be verbose, it hinders the planning capability of LLM [Gao et al., 2023, Hu et al., 2023, Puerto et al., 2024, Chen et al., 2024b]. PoT [Chen et al., 2023] offers a clearer representation of logic using code, allowing for clearer planning (as shown in Figure 2 (b, c)). This leads to finer-grained planning reasoning $B ^ { * } ( p ) > B ( p )$ . Then the PoT reasoning boundary ${ \\cal B } ^ { \\sf P o T } ( c , p ) > { \\cal B } ^ { \\sf T o o 1 } ( c , p )$ , aligning with the observed performance gains of PoT over Tool Usage (see Table 1). Furthermore, Figure 5 visually demonstrates that PoT‚Äôs theoretical and practical reasoning boundaries consistently outperform Tool Usage. This reinforces the theoretical advantage of PoT and its empirical effectiveness.\n\n# 5.2 How can we improve CoT based on a certain RB?\n\nEnhancing RB is crucial for optimizing CoT, but requires changes to the model or its reasoning architecture to be effective. Therefore, we need to consider how to optimize the reasoning path so that the difficulty satisfies the RB $\\begin{array} { r } { \\mathopen { } \\mathclose \\bgroup \\left( d ^ { * } = B _ { A c c = K _ { 1 } } \\aftergroup \\egroup \\right. } \\end{array}$ ) instead of the original RB $\\begin{array} { r } { \\dot { \\boldsymbol { \\mathcal { d } } } = \\boldsymbol { B } _ { A c c = K _ { 2 } } } \\end{array}$ ), where $K _ { 2 } < K _ { 1 }$ . According to Equation (3), $\\boldsymbol { B }$ is affected by both arithmetical RB and planning RB. Given $\\boldsymbol { B }$ , we consider optimizing reasoning ability from the following two strategies as examples 2:\n\nComplex CoT (CCoT): By increasing the boundary of planning to reduce the pressure of single-step calculation, reduce the arithmetical RB, and then get smaller $d$ ; However, it introduces more planning steps, which adds the planning pressure. As shown in Figure 6, the model performance first increases and then decreases with the increasing number of CCoT steps.\n\nLeast-to-Most (LtM): By dividing multiple sub-questions to reduce the pressure of local planning within a sub-question, reduce the boundary of local planning, and then get smaller $d$ . However, even though it can release local planning pressure (as demonstrated in Figure 7), this approach simultaneously intensifies global planning pressure by generating an excessive number of subquestions (as depicted in Figure 15).\n\nLimitation: (1) CCoT needs to keep balance in the number of reasoning steps and calculation pressure. (2) Although the pressure of local planning has been reduced, LtM has not effectively reduced the pressure of global planning, nor the pressure of optimization calculations.\n\nMinimum acceptable reasoning paths prompting can further achieve better CoT within a specific RB. To address the aforementioned two issues, we proposed Minimum Acceptable Reasoning\n\n![](images/2fb5e3a648989c40c91f1cd35400a39768e50bb2b0c604735ebcc3ffd3e95c77.jpg)  \nFigure 6: Correlation between the number of steps and performance of Complex-CoT on GSM8K (left) and SingleEq (right). See Appendix G for more meta-analysis results.\n\n![](images/64b4845a9add376a16abc3ac6de2443ac45d43d912a81752580ef88316a325e5.jpg)  \nFigure 7: The performance distribution of Least-to-Most prompting on different calculation amounts.   \nFigure 8: Correlation between the values of RB for different models and performance on real benchmarks. See Appendix $\\mathrm { ~ H ~ }$ for more empirical details.\n\nPaths (MARP). Our first objective is to alleviate the computational burden of the model. We achieve this by introducing instructions that set an upper limit on its single-step computational capacity, thereby optimizing the boundary of its computational reasoning. Secondly, we aim to enhance the model‚Äôs acceptability. Within the calculation and planning boundary, we increase the amount of computation performed in each step in demonstrations as much as possible while simultaneously reducing the number of global planning steps, which effectively mitigates planning pressure. As shown in Table 1, MARP demonstrably improves model performance and effectively reduces the token consumption. By maximizing operations per step, MARP leads to a more streamlined and efficient problem-solving process. Detailed descriptions of this strategy are shown in Appendix G.3.\n\nTakeaways: (1) Tool-Usage and PoT can be utilized to optimize CoT by the calculation and planning reasoning boundary optimization. (2) MARP can well lessen planning and calculation pressure by problem optimization in certain RB (3) Users can effectively optimize CoT performance by optimizing the reasoning boundary and the problem.\n\n# Expansion Verification & Exploration\n\nRB can be extended to various models. To extend our mechanism‚Äôs applicability, we verify the mechanism on 25 diverse models (details in Table 3). As shown in Figure 8 (a), we observe a positive correlation between reasoning boundary and model accuracy on mathematical benchmarks. Moreover, the models that use mathematical data such as MathInstruct for SFT, often have interesting outliers that are different from the general LLMs‚Äô area, but they also satisfy a positive correlation with\n\nGeneral LLM Math LLM Math LLM Closed LLM Open LLM GSM8k BigGSM MATH GSM8k BigGSM MATH GSM8k BigGSM MATH 95 ‚Üí 95 ‚Üê 95 ? Accuracy (%) 50 8605 ‚ñ° Accuracy (%) 8605 50 ¬£ Accuracy (%) 50 65 ‚ñ° ‚ñ° ÂéÇ Â•≥ 35 35 1 35 20 20 20 1 ÂÖ¨ A 5 5 5 0 0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8 0 ‚Ä¶ 0.1 0.102 0.104 0.106 0.108 ùìë!\"\"'(%% ùìë!\"\"'(%% ùìë!\"\"#\\$%% (a) Correlation between the values of CIRB (b) Correlation between the values of CIRB (b) Correlation between the values of CFRB $\\pmb { \\mathcal { B } } _ { \\mathrm { A c c < 1 0 \\% } }$ for different general LLMs and $\\pmb { \\mathcal { B } } _ { \\mathrm { A c c < 1 0 \\% } }$ for different math LLMs and $\\pmb { \\mathcal { B } } _ { \\mathrm { A c c } \\geq 9 0 \\% }$ for different closed and open performance on real benchmarks. performance on real benchmarks. LLMs and performance on real benchmarks.\n\n![](images/29183097bf592a885eb256447aff4881ffcfbbb7ad498a6e6099b77214726694.jpg)  \nFigure 9: Scaling law correlation between model parameters and CIRB.\n\n![](images/be4f4953c8bc0f368411cbac132e1c1f182e01e9b989edd37af464b4041f91e9.jpg)  \nFigure 10: Different boundaries on MGSM.\n\nour RBs (as shown in Figure 8 (b)), which helps determine if the model underwent mathematically targeted training.\n\nHowever, as shown in Figure 8 (c), we find some interesting phenomena. For example, the main difference between the current open-source model and the closed-source model is still CFRB. Except for the closed source model, the CFRB of all models is 0. It shows the potential and the direction of the model optimization. Furthermore, a scaling law of RB can also emerge (as shown in Figure 9): reasoning boundary increased with model parameter count and data quality.\n\nRB can be extended to more tasks. To assess the RB in more tasks, we evaluate them on a multilingual mathematical reasoning task. Inspired by Qin et al. [2024b], we hypothesize that multilingual RB, assessed through direct answer accuracy across different languages, mathematical computation RB, represented by the maximum product result, and reasoning planning RB, indicated by the planning steps, are orthogonal dimensions of performance. We propose that these RBs can be effectively combined using a weighted harmonic mean. As illustrated in Figure 10 confirms that the combined RB maintains the expected three different RBs. Detailed implementation description is shown in Appendix I.\n\n# 7 Related Work\n\nIn this section, we review recent literature related to Chain-of-Thought (CoT) prompting, focusing on theoretical and empirical investigations. Madaan et al. [2023], Wang et al. [2023a], Saparov and He [2023], He-Yueya et al. [2023], Zhang et al. [2024], Wang et al. [2024] and Prystawski et al. [2024] qualitatively show that the LLMs learn the reasoning chain based on the demonstrations in the context. Besides, Lampinen et al. [2022] and Tan [2023a] find a causal link between generated intermediate steps and the final answers during a series of qualitative experiments. Wang et al. [2023c], Hanna et al. [2024] and Dutta et al. [2024] study neural substructure within the LLMs, embodying CoT reasoning from a white-box mechanism perspective, demonstrating that LLMs deploy multiple parallel answer generation paths internally.\n\nRecently, a large amount of work has demonstrated the upper-bounds and limitations of LLM in various CoT tasks [Qin et al., 2023, Imani et al., 2023, Huang et al., 2024, Sprague et al., 2024]. Bi et al. [2024] investigate these bounds on planning capability in code generation by training LLM on CoT samples of varying difficulties. Their findings suggest that LLMs have a limited capacity to learn or manage tasks exceeding a certain complexity threshold. Further understanding of the CoT upper-bound, Merrill and Sabharwal [2023], Li et al. [2023] and Feng et al. [2024] analyze single-step arithmetic capability, which suggests an upper bound on model performance related to input length in single-step reasoning processes.\n\nDespite advancements in CoT explanation for LLMs, significant challenges remain, including the absence of quantifiable metrics for CoT‚Äôs upper-bounds and the deficiency in optimization guidelines. To tackle this, we propose a reasoning boundaries framework (RBF) to systematically quantify and optimize various CoT approaches. This framework offers a transferable and user-friendly\n\n? Correct sample Incorrect sample CFRB PFRB CIRB Maximum multiplication calculation value (ùìë(m)) 3e5 XX X Maximum multiplication calculation value (ùìë(m)) 3e5 X √ó x Maximum multiplication calculation value (ùìë(m)) 3e5 ? O Ôºö XX ¬∑ X X 2.52e5 √ó XX 1 2.52e5 8 √ó X X 2.52e5 + 1 √ó + 1.5e5 1e5 ? . SE E XXX √ó XX 1.5e5 1e5 X + X X 1.5e5 1e5 5e4 8 Ôºö ÁæéËâæ X x 5e4 : Ôºö ? 5e4 0 6 + X X X 0 + X . 0 x88x0 1 4 7 10 13 16 1 4 7 10 13 16 1 4 7 10 13 16 The number of planning step (ùìë(p)) The number of planning step (ùìë(p)) The number of planning step (ùìë(p)) (a) GPT-3.5-turbo (b) GPT-4o (c) O1-preview\n\nmethodology to enhance model performance from a mechanistic perspective. We anticipate that it will furnish systematic insights for ongoing research and inform future developments in the field.\n\n# 8 Discussion\n\nDiscussion on the Boundaries Improvements Furthermore, in order to better understand the best existing LLMs, we utilize RBF to test the current most advanced GPT-series models. As shown in Figure 11, all reasoning boundaries improve a lot compared to the last version which also achieves performance enhancement. Notably, the CFRB increases slightly compared with the improvement of CIRB between GPT-3.5 and GPT-4o. But o1 significantly improves the CFRB. Furthermore, as shown in Figure 14 in Appendix, o1 shows extremely significant improvements on CFRB, which is almost three times of other models. We attribute it to the fact that the advanced Reinforce-Learning and Inference Scaling strategies play a key role in improving this part of the ability compared with the normal improvements in CFRB, which might trigger more in-depth research.\n\nBroader impacts. Our framework is the first work to quantify the reasoning upper-bound of LLMs. This enables the explanation for a huge part of the valid CoT framework. We hope that our work can provide new insights and more systematic guidance for future interpretability analysis of CoT. For social impact, this work may have a certain impact on the controllable and explainable AGI.\n\nLimitations & Future. Due to the cost and time constraints, this work does not discuss the complex relationships such as causal conditions among the basic RBs. In addition, evaluating the robustness and applicability of CoT reasoning boundaries-related techniques in dynamic scenarios will be crucial for future research.\n\n# 9 Conclusion\n\nThis study introduces a novel reasoning boundaries framework (RBF) to quantify and optimize the limitations of LLMs in CoT tasks. Specifically, we propose the concept of reasoning boundaries (RBs) and the combination law of RBs in more complex scenarios for quantitative metrics. We further introduce three categories of RB for CoT optimizations. The framework is validated through extensive experiments across 27 models and 5 tasks. Furthermore, we improve the CoT in both RB and question optimization perspectives to achieve state-of-the-art performance in BIGGSM. We hope that this framework paves the way for further research on understanding and enhancing LLMs‚Äô reasoning capabilities.",
    "summary": "{\n    \"core_summary\": \"### Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n**ÈóÆÈ¢òÂÆö‰πâ**\\nÁé∞ÊúâÂÖ≥‰∫éÊÄùÁª¥ÈìæÔºàChain-of-Thought, CoTÔºâÁöÑÁ†îÁ©∂Â≠òÂú®‰∏§Â§ßÊåëÊàòÔºö‰∏ÄÊòØÁº∫‰πèÈáèÂåñCoTËÉΩÂäõÁöÑÊåáÊ†áÔºåÈöæ‰ª•ÂÆ¢ËßÇÊØîËæÉ‰∏çÂêåCoTÊñπÊ≥ïÂπ∂Á°ÆÂÆöÂÖ∂ËÉΩÂäõ‰∏äÈôêÔºõ‰∫åÊòØÁº∫‰πè‰ºòÂåñCoTÊÄßËÉΩÁöÑÊåáÂØºÔºåÈòªÁ¢ç‰∫ÜÂ∞ÜCoTÁ†îÁ©∂ËΩ¨Âåñ‰∏∫ÊèêÂçáÊ®°ÂûãËÉΩÂäõÁöÑÂèØË°åÁ≠ñÁï•„ÄÇËøô‰∫õÈóÆÈ¢ò‰∏•ÈáçÂà∂Á∫¶‰∫ÜÂØπÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÊÄßËÉΩÁöÑÊ∑±ÂÖ•ÁêÜËß£‰∏éÊèêÂçá„ÄÇ\\n\\n**ÊñπÊ≥ïÊ¶ÇËø∞**\\nËÆ∫ÊñáÊèêÂá∫Êé®ÁêÜËæπÁïåÊ°ÜÊû∂ÔºàReasoning Boundary Framework, RBFÔºâÔºåÈÄöËøáÂÆö‰πâÊé®ÁêÜËæπÁïåÔºàReasoning Boundary, RBÔºâÈáèÂåñCoT‰∏äÈôêÔºåÂª∫Á´ãRBÁªÑÂêàÂÆöÂæã‰ª•ÈÄÇÂ∫îÂ§çÊùÇÂú∫ÊôØÔºõÊèêÂá∫‰∏âÁ±ªRBÔºåÂπ∂Âü∫‰∫éÁªÑÂêàÂÆöÂæãÂíåÂÖ∂ÊÄßË¥®ÔºåÈÄöËøáÊèêÂçáRBÂíå‰ºòÂåñÊé®ÁêÜË∑ØÂæÑ‰ºòÂåñÁé∞ÊúâCoTÁ≠ñÁï•„ÄÇ\\n\\n**‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú**\\n- È¶ñÊ¨°ÊèêÂá∫Êé®ÁêÜËæπÁïåÊ°ÜÊû∂ÔºàRBFÔºâÈáèÂåñCoT‰∏äÈôêÔºåÂª∫Á´ãRBÁªÑÂêàÂÆöÂæã‰Ωú‰∏∫Âü∫Êú¨RBÁöÑÂä†ÊùÉË∞ÉÂíåÂùáÂÄºÔºåÂèØÂ∫îÁî®‰∫éÂÆûÈôÖCoT‰ªªÂä°„ÄÇ\\n- ÂÆö‰πâ‰∏âÁ±ªRBÔºåÂü∫‰∫éÁªÑÂêàÂÆöÂæãÂíåÂÖ∂ÊÄßË¥®ÔºåÈÄöËøáÊèêÂçáRBÂíå‰ºòÂåñÊé®ÁêÜË∑ØÂæÑÊúâÊïàÊîπËøõÁé∞ÊúâCoTÁ≠ñÁï•ÔºåÂ¶ÇÂú®BIGGSMÂü∫ÂáÜÊµãËØï‰∏≠ÔºåPoT+MARPÊñπÊ≥ïÂáÜÁ°ÆÁéáËææÂà∞80.55 ¬± 2.40%ÔºåÈ´òÂá∫CoTÁöÑ57.00 ¬± 0.93%„ÄÇ\\n- Âú®27‰∏™Ê®°ÂûãÂíå5‰∏™CoT‰ªªÂä°‰∏äÈ™åËØÅÊ°ÜÊû∂ÁöÑÂ≠òÂú®ÊÄßÂíåÂêàÁêÜÊÄßÔºå‰ªé‰∏§‰∏™‰ºòÂåñËßÜËßíËß£Èáä‰ºóÂ§öCoTÁ≠ñÁï•ÁöÑÊúÄ‰ºòÊÄßËÉΩ„ÄÇ\",\n    \"algorithm_details\": \"### ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n**Ê†∏ÂøÉÊÄùÊÉ≥**\\nËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂºïÂÖ•Êé®ÁêÜËæπÁïåÔºàRBÔºâÊ¶ÇÂøµÔºåÈáèÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÁâπÂÆöÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑËÉΩÂäõ‰∏äÈôêÔºåËß£ÂÜ≥Áé∞ÊúâÁ†îÁ©∂Áº∫‰πèÈáèÂåñÊåáÊ†áÂíå‰ºòÂåñÊåáÂØºÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂª∫Á´ãRBÁªÑÂêàÂÆöÂæãÔºåËÄÉËôëÂ§ö‰∏™ËÉΩÂäõÂêà‰ΩúÂØπÊ®°ÂûãÊé®ÁêÜËæπÁïåÁöÑÂΩ±ÂìçÔºåÂáÜÁ°ÆËØÑ‰º∞Ê®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÂÆö‰πâ‰∏âÁ±ªRBÔºåÊ†πÊçÆÂÖ∂ÊÄßË¥®ÂíåÁªÑÂêàÂÆöÂæã‰ºòÂåñCoTÁ≠ñÁï•ÔºåÊèêÂçáÊ®°ÂûãÊé®ÁêÜÊÄßËÉΩ„ÄÇÂÖ∂ÊúâÊïàÊÄßÂú®‰∫éËÉΩÂáÜÁ°ÆË°°ÈáèÊ®°ÂûãÊé®ÁêÜËÉΩÂäõËæπÁïåÔºå‰ªéËÄåÊúâÈíàÂØπÊÄßÂú∞‰ºòÂåñ„ÄÇ\\n\\n**ÂàõÊñ∞ÁÇπ**\\nÂÖàÂâçÁöÑÁ†îÁ©∂‰∏ªË¶Å‰æùËµñÂÆöÊÄßËØÑ‰º∞CoTÊÄßËÉΩÔºåÁº∫‰πèÈáèÂåñÊåáÊ†áÔºå‰∏îÂú®‰ºòÂåñCoTÊÄßËÉΩÊñπÈù¢Áº∫‰πèÊåáÂØº„ÄÇÊú¨ÊñáÁöÑÂàõÊñ∞ÁÇπÂú®‰∫éÔºöÈ¶ñÊ¨°ÊèêÂá∫Êé®ÁêÜËæπÁïåÊ°ÜÊû∂ÔºàRBFÔºâÔºå‰∏∫CoTËÉΩÂäõÊèê‰æõÈáèÂåñÊåáÊ†áÔºõÂª∫Á´ãRBÁªÑÂêàÂÆöÂæãÔºåÈÄÇÁî®‰∫éÊõ¥Â§çÊùÇÁöÑÂÆûÈôÖÂú∫ÊôØÔºõÂÆö‰πâ‰∏âÁ±ªRBÂπ∂Âü∫‰∫éÊ≠§‰ºòÂåñCoTÁ≠ñÁï•Ôºå‰∏∫ÊèêÂçáÊ®°ÂûãÊÄßËÉΩÊèê‰æõÂÖ∑‰ΩìÊñπÊ≥ï„ÄÇ\\n\\n**ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§**\\n1. **ÂÆö‰πâÊé®ÁêÜËæπÁïåÔºàRBÔºâ**ÔºöÂØπ‰∫éÊ®°Âûã $m$ Âíå‰ªªÂä° $t$ÔºåRBÂÆö‰πâ‰∏∫Ê®°ÂûãÂú®ËææÂà∞È¢ÑÂÆö‰πâÂáÜÁ°ÆÁéáÈòàÂÄº $K_1$ Êó∂ÁöÑÊúÄÂ§ßÈóÆÈ¢òÈöæÂ∫¶ $d$ÔºåÂç≥ $B_{Acc = K_1}(t|m) = \\sup_d \\{ d | Acc(t|d, m) = K_1 \\}$ÔºåÈöæÂ∫¶ÂèØÈÄöËøáÊé®ÁêÜÊ≠•È™§Êï∞ÊàñËÆ°ÁÆóÂ§çÊùÇÂ∫¶Á≠âÂõ†Á¥†Ë°°Èáè„ÄÇ\\n2. **Âª∫Á´ãRBÁªÑÂêàÂÆöÂæã**ÔºöÂØπ‰∫é $n$ ‰∏™‰ªªÂä°ÔºåÊ®°Âûã $m$ ÁöÑÁªü‰∏ÄÊé®ÁêÜËæπÁïå $\\mathcal{B}_{Acc = K_1}(t_1, t_2, \\dots, t_n|m) \\approx \\frac{1}{(n - 1) \\sum_{i = 1}^{n} \\frac{N_i}{\\mathcal{B}_{Acc = K_1}(t_i|m) - b_i}}$ÔºåÂÖ∂‰∏≠ $B_{Acc = K_1}(t_i|m)$ Ë°®Á§∫Ê®°Âûã $m$ ÂØπ‰ªªÂä° $t_i$ ÁöÑÊé®ÁêÜËæπÁïåÔºå$N_i$ Âíå $b_i$ ÊòØ‰ªÖÂèóÁõ∏ÂÖ≥‰ªªÂä°ÂΩ±ÂìçÁöÑÁº©ÊîæÂõ†Â≠ê„ÄÇ\\n3. **ÂÆö‰πâ‰∏âÁ±ªRB**ÔºöÊ†πÊçÆÁªèÈ™åÂáÜÁ°ÆÁéáÔºåÂ∞ÜRBÂàÜ‰∏∫ÂÆåÂÖ®ÂèØË°åÊé®ÁêÜËæπÁïåÔºàCFRBÔºå$Acc \\geq 90\\%$Ôºâ„ÄÅÈÉ®ÂàÜÂèØË°åÊé®ÁêÜËæπÁïåÔºàPFRBÔºå$10\\% < Acc < 90\\%$ÔºâÂíåÂÆåÂÖ®‰∏çÂèØË°åÊé®ÁêÜËæπÁïåÔºàCIRBÔºå$Acc \\leq 10\\%$Ôºâ„ÄÇ\\n4. **‰ºòÂåñCoTÁ≠ñÁï•**ÔºöÈÄöËøáÊèêÂçáRBÔºàÂ¶Ç‰ΩøÁî®Tool-UsageÂíåProgram-of-ThoughtÔºâÂíå‰ºòÂåñÊé®ÁêÜË∑ØÂæÑÔºàÂ¶ÇComplex CoT„ÄÅLeast-to-Most„ÄÅMinimum Acceptable Reasoning PathsÔºâÊîπËøõÁé∞ÊúâCoTÁ≠ñÁï•„ÄÇ\\n\\n**Ê°à‰æãËß£Êûê**\\nÂú®Âü∫Êú¨ÁÆóÊúØËÆ°ÁÆó‰ªªÂä°‰∏≠Ôºå‰ª•‰πòÊ≥ï‰∏∫‰æãÔºåÂΩìÁªìÊûúÂ∞è‰∫éÁ≠â‰∫é2.2e5Êó∂ÔºåÂáÜÁ°ÆÁéáË∂ÖËøá90%ÔºõÂΩìÁªìÊûúË∂ÖËøá2e6Êó∂ÔºåÂáÜÁ°ÆÁéá‰Ωé‰∫é10%Ôºå‰ΩìÁé∞‰∫ÜÊé®ÁêÜËæπÁïåÁöÑÂ≠òÂú®„ÄÇÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄËßÑÂàí‰ªªÂä°‰∏≠ÔºåÂΩìÊé®ÁêÜÊ≠•È™§Â∞ë‰∫é2Ê≠•Êó∂ÔºåÂáÜÁ°ÆÁéáË∂ÖËøá90%ÔºõÂΩìÊé®ÁêÜÊ≠•È™§Ë∂ÖËøá4Ê≠•Êó∂ÔºåÂáÜÁ°ÆÁéá‰Ωé‰∫é10%ÔºåÂ±ïÁ§∫‰∫Ü‰∏çÂêåÁöÑÊé®ÁêÜËæπÁïåÁ±ªÂà´„ÄÇÂú®‰ª£Á†ÅËßÑÂàí‰ªªÂä°‰∏≠Ôºå‰∏éËá™ÁÑ∂ËØ≠Ë®ÄËßÑÂàíÁ±ª‰ººÔºå‰πüÂ≠òÂú®ÊòéÊòæÁöÑÊé®ÁêÜËæπÁïåÂàíÂàÜÔºå‰∏îÁî±‰∫é‰ª£Á†ÅÈÄªËæëÊõ¥Ê∏ÖÊô∞„ÄÅË°®ËææÂ§çÊùÇÂ∫¶Èôç‰ΩéÔºåÂÖ∂ËßÑÂàíÂáÜÁ°ÆÁéáÈ´ò‰∫éËá™ÁÑ∂ËØ≠Ë®ÄËßÑÂàí„ÄÇ\",\n    \"comparative_analysis\": \"### ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n**Âü∫Á∫øÊ®°Âûã**\\nËÆ∫Êñá‰∏≠Áî®‰∫éÂØπÊØîÁöÑÊ†∏ÂøÉÂü∫Á∫øÊ®°Âûã‰∏∫CoT„ÄÇ\\n\\n**ÊÄßËÉΩÂØπÊØî**\\n*   **Âú® [ÂáÜÁ°ÆÁéá/Accuracy] ÊåáÊ†á‰∏äÔºö** Êú¨ÊñáÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®BIGGSMÊï∞ÊçÆÈõÜ‰∏äÂèñÂæóÊòæËëóÊÄßËÉΩÊèêÂçá„ÄÇÂÖ∂‰∏≠ÔºåTool UsageÊñπÊ≥ïÁöÑÂáÜÁ°ÆÁéáËææÂà∞71.64 ¬± 0.66%ÔºåÈ´òÂá∫CoTÁöÑ57.00 ¬± 0.93%ÔºõPoTÊñπÊ≥ïËøõ‰∏ÄÊ≠•ÊèêÂçáÂà∞78.25 ¬± 1.09%ÔºõPoT+MARPÊñπÊ≥ïË°®Áé∞ÊúÄ‰Ω≥ÔºåÂáÜÁ°ÆÁéáËææÂà∞80.55 ¬± 2.40%ÔºåËøú‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãCoT„ÄÇLeast-to-mostÊñπÊ≥ïÁöÑÂáÜÁ°ÆÁéá‰∏∫58.25 ¬± 3.28%ÔºåComplex-CoTÊñπÊ≥ïÁöÑÂáÜÁ°ÆÁéá‰∏∫59.78 ¬± 0.60%ÔºåCoT+MARPÊñπÊ≥ïÁöÑÂáÜÁ°ÆÁéá‰∏∫64.37 ¬± 2.24%ÔºåÂùáÈ´ò‰∫éCoT„ÄÇ\\n*   **Âú® [ËæìÂÖ•‰ª§ÁâåÊï∞/Input Token] ÊåáÊ†á‰∏äÔºö** CoTÁöÑËæìÂÖ•‰ª§ÁâåÊï∞‰∏∫780.43ÔºåËÄåÊú¨ÊñáÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÂáèÂ∞ëËæìÂÖ•‰ª§ÁâåÊï∞ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇTool UsageÁöÑËæìÂÖ•‰ª§ÁâåÊï∞Èôç‰ΩéÂà∞688.43ÔºåPoTËøõ‰∏ÄÊ≠•Èôç‰ΩéÂà∞657.43ÔºåPoT+MARPÁöÑËæìÂÖ•‰ª§ÁâåÊï∞ÊúÄ‰ΩéÔºå‰∏∫576.43Ôºå‰Ωé‰∫éCoTÔºåË°®ÊòéËøô‰∫õÊñπÊ≥ïÂú®ËµÑÊ∫êÂà©Áî®‰∏äÊõ¥È´òÊïà„ÄÇLeast-to-mostÁöÑËæìÂÖ•‰ª§ÁâåÊï∞‰∏∫679.59ÔºåComplex-CoTÁöÑËæìÂÖ•‰ª§ÁâåÊï∞‰∏∫1111.43ÔºåCoT+MARPÁöÑËæìÂÖ•‰ª§ÁâåÊï∞‰∏∫614.43ÔºåÈÉ®ÂàÜÊñπÊ≥ï‰Ωé‰∫éCoT„ÄÇ\\n*   **Âú® [ËæìÂá∫‰ª§ÁâåÊï∞/Output Token] ÊåáÊ†á‰∏äÔºö** CoTÁöÑËæìÂá∫‰ª§ÁâåÊï∞‰∏∫96.76 ¬± 3.22ÔºåTool UsageÁöÑËæìÂá∫‰ª§ÁâåÊï∞‰∏∫129.53 ¬± 3.82ÔºåPoTÁöÑËæìÂá∫‰ª§ÁâåÊï∞‰∏∫78.25 ¬± 1.09ÔºåPoT+MARPÁöÑËæìÂá∫‰ª§ÁâåÊï∞‰∏∫76.34 ¬± 2.84Ôºå‰Ωé‰∫éCoTÔºåËØ¥ÊòéÊú¨ÊñáÈÉ®ÂàÜÊñπÊ≥ïÂú®ËæìÂá∫ÁªìÊûúÁöÑÁÆÄÊ¥ÅÊÄß‰∏äÂÖ∑Êúâ‰ºòÂäø„ÄÇLeast-to-mostÁöÑËæìÂá∫‰ª§ÁâåÊï∞‰∏∫176.09 ¬± 15.22ÔºåComplex-CoTÁöÑËæìÂá∫‰ª§ÁâåÊï∞‰∏∫131.82 ¬± 1.91ÔºåCoT+MARPÁöÑËæìÂá∫‰ª§ÁâåÊï∞‰∏∫95.12 ¬± 0.77„ÄÇ\",\n    \"keywords\": \"### ÂÖ≥ÈîÆËØç\\n\\n- Â§ßËØ≠Ë®ÄÊ®°Âûã (Large Language Models, LLMs)\\n- ÊÄùÁª¥ÈìæÊé®ÁêÜ (Chain-of-Thought Reasoning, CoT)\\n- Êé®ÁêÜËæπÁïåÊ°ÜÊû∂ (Reasoning Boundary Framework, RBF)\\n- Êé®ÁêÜËæπÁïå (Reasoning Boundary, RB)\\n- Êé®ÁêÜËæπÁïåÁªÑÂêàÂÆöÂæã (Combination Law of Reasoning Boundary, CLRB)\\n- Â§çÊùÇÊé®ÁêÜ‰ªªÂä° (Complex Reasoning Tasks, CRT)\"\n}"
}