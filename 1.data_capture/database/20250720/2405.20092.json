{
    "source": "Semantic Scholar",
    "arxiv_id": "2405.20092",
    "link": "https://arxiv.org/abs/2405.20092",
    "pdf_link": "https://arxiv.org/pdf/2405.20092.pdf",
    "title": "Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation",
    "authors": [
        "Jingchang Chen",
        "Hongxuan Tang",
        "Zheng Chu",
        "Qianglong Chen",
        "Zekun Wang",
        "Ming Liu",
        "Bing Qin"
    ],
    "categories": [
        "cs.CL",
        "cs.SE"
    ],
    "publication_date": "2024-05-30",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 6,
    "influential_citation_count": 0,
    "institutions": [
        "Harbin Institute of Technology",
        "Zhejiang University"
    ],
    "paper_content": "# Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation\n\nJingchang Chen‚àó Harbin Institute of Technology jcchen@ir.hit.edu.cn\n\nHongxuan Tang‚àó Harbin Institute of Technology jeffswt@outlook.com\n\nZheng Chu Harbin Institute of Technology zchu@ir.hit.edu.cn\n\nQianglong Chen‚Ä† Zhejiang University chenqianglong.ai@gmail.com\n\nZekun Wang Harbin Institute of Technology zkwang@ir.hit.edu.cn\n\nMing Liu Harbin Institute of Technology mliu@ir.hit.edu.cn\n\nBing Qin‚Ä† Harbin Institute of Technology qbin@ir.hit.edu.cn\n\n# Abstract\n\nDespite recent progress made by large language models in code generation, they still struggle with programs that meet complex requirements. Recent work utilizes plan-and-solve decomposition to decrease the complexity and leverage selftests to refine the generated program. Yet, planning deep-inside requirements in advance can be challenging, and the tests need to be accurate to accomplish self-improvement. To this end, we propose FUNCODER, a code generation framework incorporating the divide-and-conquer strategy with functional consensus. Specifically, FUNCODER recursively branches off sub-functions as smaller goals during code generation, represented by a tree hierarchy. These sub-functions are then composited to attain more complex objectives. Additionally, we designate functions via a consensus formed by identifying similarities in program behavior, mitigating error propagation. FUNCODER outperforms state-of-the-art methods by $+ 9 . 8 \\%$ on average in HumanEval, MBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method demonstrates superiority on smaller models: With FUNCODER, StableCode $_ { 3 b }$ surpasses GPT-3.5 by $+ 1 8 . 6 \\%$ and achieves $9 7 . 7 \\%$ of GPT-4‚Äôs performance on HumanEval. Further analysis reveals that our proposed dynamic function decomposition is capable of handling complex requirements, and the functional consensus prevails over self-testing in correctness evaluation.\n\n# 1 Introduction\n\nOver the past few years, large language models have been observed to attain significant advancements in coding capabilities (OpenAI, 2023; Touvron et al., 2023). Meanwhile, models designed specifically for coding tasks have also been introduced (Rozi√®re et al., 2023; Lozhkov et al., 2024; Pinnaparaju et al., 2024). Although LLMs can proficiently generate simple code snippets, they suffer from a decline in performance as code requirements become complicated.\n\nNumerous efforts have been made to tackle this complexity. The two-stage methods (Jiang et al., 2023; Zelikman et al., 2023) employ the plan-and-solve strategy, which first generates a draft outline for the complex task and uses it as guidance for implementing the code in the second stage. Multiagent development frameworks (Hong et al., 2024; Qian et al., 2023) mimic real-world software development workflows, assign different roles to LLMs and collaborate to solve a complex goal. Self-improvement (Shinn et al., 2023; Chen et al., 2024), on the other hand, refines the program in accordance with execution feedback from self-generated unit tests.\n\n![](images/61fb7e4b44519c9b5b4c47663652bc3d7e491c2df09b54529c70856ba43f149c.jpg)  \nFigure 1: A flowgraph illustrates FUNCODER. FUNCODER branches off new functions to have sub-goals tackled iteratively (left), re-composites sub-functions, and selects the best using functional consensus (right). Bottom-right figure shows how FUNCODER writes functions at hierarchy-level.\n\nDespite fruitful efforts made by the previous methods in dealing with complex problems, certain challenges still remain unsolved: (1) Two-stage approaches need to design a complete plan at the beginning and lack the ability to adjust the top-level design during implementation, leading to suboptimal decomposition. (2) Multi-agent collaboration frameworks are cumbersome and rely heavily on LLM capabilities, making them difficult to generalize to smaller open-source models. (3) Code refinement through self-tests depends on the correctness of generated unit-tests. Our preliminary study (¬ß3.1.3) finds that models generate unreliable self-tests in abundance. These incorrect tests may mislead self-improvement and, at worse, exacerbate program errors.\n\nTo address these issues, we propose FUNCODER, a code generation framework utilizing a divide-andconquer strategy and a novel functional consensus mechanism on functions to decompose complex problems. Starting from the main problem, FUNCODER introduces new functions to cope with certain sub-problems. The new functions will be decomposed recursively, eventually forming a tree of functions. FUNCODER then combines functions bottom-up to achieve increasingly complicated objectives. By dividing-and-conquering tasks into simpler sub-functions, complexity can be gradually reduced. However, errors in sub-functions may propagate to the whole program, thereby damaging overall reliability. We propose functional consensus that samples multiple functions and selects the one demonstrating consensus, measured by the aggregated similarity among candidates. By reaching a consensus, we reduce the discrepancies in code behavior and thus alleviate cascading errors.\n\nWe conduct extensive experiments on code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Khan et al., 2023) with GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023), outperforming state-of-the-art methods by $+ 9 . 8 \\%$ on average. Experiments are further carried out on the mathematical competition benchmark, MATH (Hendrycks et al., 2021b), achieving a $\\mathbf { + 6 . 0 }$ improvement with GPT-4, indicating that FUNCODER can also generalize to complex reasoning. Our method is observed to be equally effective on open-source models (Meta AI, 2024; Mistral AI, 2024; Pinnaparaju et al., 2024; Rozi√®re et al., 2023; Lozhkov et al., 2024), with an average gain over baseline of $\\mathbf { + 3 1 . 5 \\% }$ on HumanEval and $+ 4 7 . 7 \\%$ on MATH. Additional analysis also shows the advantage of both divide-and-conquer and functional consensus. Our code is made openly available at https://github.com/cometeme/funcoder.\n\nAlgorithm 1 FUNCODER procedure   \nRequire: Entry func, $f _ { \\mathrm { r o o t } } = \\{ h _ { \\mathrm { r o o t } } , d _ { \\mathrm { r o o t } } , \\phi \\}$   \nRequire: Large language model, LLM   \n1: function FUNCODER $( f _ { \\mathrm { c u r } } )$   \n2: ‚Äî Divide ‚Äî   \n3: $f _ { \\mathrm { c u r } } ^ { \\prime }$ $, \\ \\{ f _ { i } \\} \\gets \\mathrm { E X T R A C T } ( \\mathrm { L L M } ( f _ { \\mathrm { c u r } } ) )$   \n4: for $f _ { i } \\in \\{ f _ { i } \\}$ do   \n5: if $b _ { i }$ is NOTIMPLEMENTED then   \n6: $f _ { i } ^ { * } \\gets \\mathrm { F U N C O D E R } ( f _ { i } ) \\triangleright$ recursion   \n7: end if   \n8: $\\mathrm { A D D C H I L D } ( f _ { \\mathrm { c u r } } , f _ { i } ^ { * } )$   \n9: end for   \n10: ‚Äî Conquer ‚Äî   \n11: $F _ { \\mathrm { c u r } } \\gets \\mathbf { S A M P L E } \\big ( \\mathbf { L L M } ( f _ { \\mathrm { c u r } } ^ { \\prime } , \\mathbf { C H I L D } ( f _ { \\mathrm { c u r } } ) \\big )$   \n12: f c‚àóur ‚Üê FUNCONSENSUS(Fcur)   \n13: return f c‚àóur   \n14: end function   \n15: return FUNCODER $\\scriptstyle f _ { \\mathrm { r o o t } } ,$ ) $D$ starts from root\n\n(a) Planning-based Decomposition 1. Create a function ‚Ä¶ Q 2. Generate a sequence 3. Check if ... then ... 4. Return ... plan code   \n(b) Decompose Through Coding (ours) 1. Writing current function def sum_common_factors(a, b): \"\"\"sum the common factors\"\"\" 3. Extract fa $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ find_factors(a) code to a tree fb = find_factors(b) return sum_common(fa, fb) 2. Introduce new functions def find_factors(x: int) -> list: ‚Äúreturn factors of x\"\"\" raise NotImplementedError() def sum_common(a: list, b: list) -> int: ‚Äúsum of common elements\" ùëî ‚Ñé raise NotImplementedError()\n\n# 2 FUNCODER: Divide-and-Conquer Meets Consensus\n\n# 2.1 Divide-and-Conquer for Iterative Programming\n\nA function is defined as a relation between a set of inputs and outputs where each input is assigned exactly one output (Halmos, 1998), denoted as $y = { \\bar { f } } ( x )$ . In computer programming, a function is identified by its header $h _ { f }$ with its body $b _ { f }$ , and is commonly accompanied by a documentation $d _ { f }$ to improve readability. Functions can be invoked from other procedures, allowing for the decomposition of large and complicated requirements into smaller structures that exhibit high comprehensibility and quality (Dahl et al., 1972). Generally, human programmers tend to decompose tasks into clearly defined sub-functions and then implement them recursively, making functions eligible for re-usage, taking advantage of the divide-and-conquer principle. Inspired by this, FUNCODER recursively divides the requirement and conquers functions to formulate a sophisticated solution, unleashing the potential of LLMs in code generation.\n\nDivide is a top-down process that iteratively breaks down problems. Given a code generation problem, the process begins from the entry function $f _ { \\mathrm { r o o t } }$ . We instruct the model to introduce new functions $f _ { i } \\in \\mathrm { C H I L D } \\big ( f _ { \\mathrm { c u r } } \\big )$ that solve certain sub-goals while writing the current $f _ { \\mathrm { c u r } }$ . To reduce the complexity involved in each generation, we only require the headers $h _ { f _ { i } }$ and documentation $d _ { f _ { i } }$ of new functions to be generated, while their implementations $b _ { f _ { i } }$ can be postponed. After completing the current function, the model starts to address those unimplemented sub-functions and complete $b _ { f _ { i } }$ into $f _ { i } ^ { \\prime }$ . This process stops when the model deems functions too simple to be further divided, finally forming a dependency tree $T = \\mathrm { T R E E } \\big ( f _ { \\mathrm { r o o t } } , \\mathrm { C H I L D } \\big ( f _ { \\mathrm { r o o t } } \\big ) \\big )$ . The divide process is similar to a search starting from the entry function, gradually involving new sub-functions while writing the current, and implementing them recursively. We guide the entire process through a depth-first search.\n\nConquer is a process of achieving complex objectives through aggregating smaller functions. We notice that child functions are not yet implemented during the top-down process of writing parent functions. As a result, these parent functions may not be able to effectively utilize the child functions, or misuse them at worst. FUNCODER deals with this issue by re-generating functions in inverse topological order on the dependency tree $T$ - starting from leaves, complex goals are handled by compositing solved children as $f _ { \\mathrm { c u r } } ^ { * } \\gets \\mathscr { F } ( f _ { \\mathrm { c u r } } ^ { \\prime } , \\{ f _ { 1 } ^ { * } , f _ { 2 } ^ { * } , \\ldots \\} ) \\ : | \\ : f _ { i } ^ { * } \\in \\mathrm { C H I L D } ( f _ { \\mathrm { c u r } } ^ { - } ) .$ .\n\nDivide and conquer naturally achieve both decomposition and composition during code generation Unlike two-stage and agent-based methods, our approach dynamically introduces new functions along the process, making it less burdensome than producing a complete plan at the very beginning. Moreover, while planning or agents require chat capabilities, FUNCODER represents sub-tasks through functions (Figure 2), making it more applicable to specialized code generation models.\n\n# 2.2 Functionality Similarity as a Consensus\n\nThe decomposition of complex tasks benefits from solving easier sub-goals, but might introduce the risks of cascading errors, which refers to errors in sub-functions that lead to errors in ancestral functions. To mitigate this, we introduce Functional Consensus which aims at reducing inconsistencies in program behavior. This is achieved by sampling multiple functions and selecting the one that exhibits consensus, as measured by the aggregated similarity of functionality between candidates, thus abating outlier functionalities.\n\nFunctionality Similarity A program specifies its functionality (or behavior) through the control flow and logic defined by its code semantics. However, comparing the functionalities between two programs based on their semantics is somewhat challenging. By decomposing the requirement into functions, FUNCODER is able to view the function behavior as a black box that maps arguments into return values. Considering two functions $f$ and $g$ with the same input domain $\\bar { D ( f ) } \\overset { \\cdot } { = } D ( g )$ , we define the similarity between them $s i m ( f , g )$ as the identicalness of outputs when given the same input values.\n\n$$\ns i m ( f , g ) = \\int _ { x \\in D ( f ) } { \\frac { \\mathbb { 1 } \\left[ f ( x ) = g ( x ) \\right] } { | D ( f ) | } } \\ \\approx \\sum _ { x \\in X \\mid X \\sim D ( f ) } { \\frac { \\mathbb { 1 } \\left[ f ( x ) = g ( x ) \\right] } { | X | } }\n$$\n\nThe similarity becomes 1 if and only if two functions output consistent values for all inputs: $\\forall x \\in$ $D ( f ) : f ( x ) { \\overset { \\cdot } { = } } g ( x ) \\Leftrightarrow s i m ( f , g ) { \\overset { \\cdot } { = } } 1$ . We notice that the input domain $D ( f )$ is unbounded in most cases, making its measurement barely feasible in practice. Thus, we approximate it by sampling a subset of possible inputs $X \\sim D ( f )$ with an LLM.\n\nConsensus is reached by selecting the candidate $f ^ { * }$ holding maximal similarity with others after sampling multiple function implementations $F = \\{ f _ { ( i ) } \\}$ for the same requirements.\n\n$$\nf ^ { * } = \\operatorname { F U N C O N S E N S U S } ( F ) = \\underset { f _ { ( i ) } \\in F } { \\arg \\operatorname* { m a x } } \\sum _ { f _ { ( j ) } \\in F \\setminus \\{ f _ { ( i ) } \\} } s i m ( f _ { ( i ) } , f _ { ( j ) } )\n$$\n\nBy introducing functional consensus, FUNCODER produces functions that are more consistent and common in functionality, while omitting abnormal samples. The process is applied to not just the final program, but also to every sub-tree during the bottom-up conquering stage, resulting in step-by-step, thorough verification from the most fundamental functions all the way up to the whole program.\n\n# 2.3 FUNCODER is a Function Coder\n\nWe design FUNCODER as a procedure that takes a problem in the form of a function signature $f ( x )$ , and produces a final solution $f ^ { * } ( x )$ , as exemplified in Figure 1. Given a problem $f ( x )$ , FUNCODER partially implements the function as $f ^ { \\prime } ( x )$ referring to unimplemented sub-functions $\\overset { \\cdot } { \\boldsymbol { g } } ( \\boldsymbol { y } )$ and $h ( z )$ . These sub-functions are then fed into FUNCODER to be recursively coped with. We then sample $k$ implementations $f _ { ( i ) } ^ { \\prime } ( x )$ based on solved children $g ^ { \\ast } ( y )$ and $h ^ { \\ast } ( z )$ . Functional consensus is calculated by evaluating candidates on possible inputs. The function sharing maximal behavioral similarity is combined with solved children to formulate the final solution.\n\n# 3 Experiments\n\nWe conduct experiments on competition-level code generation and mathematical reasoning benchmarks with state-of-the-art LLMs, which are covered in section $\\ S 3 . 1$ and $\\ S 3 . 2$ , respectively. In addition to GPT models (Ouyang et al., 2022; OpenAI, 2023), we also conduct experiments with community models like Llama $3 _ { 8 b }$ (Meta AI, 2024), StableCode $_ { 3 b }$ (Pinnaparaju et al., 2024), and CodeLlama $3 4 b$ (Rozi√®re et al., 2023). We use the instruct variant of these models and inference on a single A100-80G under BF16 precision with vLLM (Kwon et al., 2023).\n\nTable 1: Experiment results on code generation benchmarks. We report $\\mathrm { P a s s } @ 1$ as evaluate metric. Results from the original paper are underlined, and the best results are bold.   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Method</td><td colspan=\"2\">HumanEval</td><td colspan=\"2\">MBPP</td><td colspan=\"5\">xCodeEval</td></tr><tr><td>Pass@1</td><td>‚ñ≥‚Üë</td><td>Pass@1</td><td>‚ñ≥‚Üë</td><td>Easy</td><td>Mid</td><td>Hard</td><td>Expert</td><td>All</td></tr><tr><td rowspan=\"5\">GPT-3.5</td><td>Standard</td><td>68.3</td><td></td><td>72.0</td><td></td><td>44.4</td><td>15.2</td><td>4.6</td><td>0.0</td><td>20.2</td></tr><tr><td>CodeT</td><td>81.1</td><td>+12.8</td><td>76.0</td><td>+4.0</td><td>50.6</td><td>16.1</td><td>8.0</td><td>0.0</td><td>23.2</td></tr><tr><td>Reflexion</td><td>69.5</td><td>+1.2</td><td>72.5</td><td>+0.5</td><td>44.4</td><td>17.0</td><td>5.7</td><td>0.0</td><td>20.6</td></tr><tr><td>LDB</td><td>82.9</td><td>+14.6</td><td>76.0</td><td>+4.0</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FUNCODER</td><td>85.4</td><td>+17.1</td><td>78.5</td><td>+6.5</td><td>62.4</td><td>29.5</td><td>11.6</td><td>0.0</td><td>31.4</td></tr><tr><td rowspan=\"6\">GPT-4</td><td>Standard</td><td>82.9</td><td></td><td>73.5</td><td></td><td>68.5</td><td>39.3</td><td>19.5</td><td>1.7</td><td>37.4</td></tr><tr><td>Parsel</td><td>85.0</td><td>+2.1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CodeT</td><td>90.9</td><td>+8.0</td><td>77.0</td><td>+3.5</td><td>76.4</td><td>51.8</td><td>21.8</td><td>3.4</td><td>44.0</td></tr><tr><td>Reflexion</td><td>91.0</td><td>+8.1</td><td>77.1</td><td>+3.6</td><td>71.3</td><td>41.1</td><td>19.5</td><td>2.5</td><td>38.6</td></tr><tr><td>MetaGPT</td><td>85.9</td><td>+3.0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FUNCODER</td><td>94.5</td><td>+11.6</td><td>79.5</td><td>+6.0</td><td>83.1</td><td>58.0</td><td>26.4</td><td>3.4</td><td>48.6</td></tr><tr><td rowspan=\"3\">Llama38b</td><td>Standard</td><td>61.6</td><td></td><td>60.5</td><td></td><td>9.0</td><td>1.8</td><td>0.0</td><td>0.0</td><td>3.6</td></tr><tr><td>CodeT</td><td>68.9</td><td>+7.3</td><td>61.5</td><td>+1.0</td><td>12.4</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.4</td></tr><tr><td>FUNCODER</td><td>79.7</td><td>+18.1</td><td>62.5</td><td>+2.0</td><td>22.0</td><td>0.9</td><td>0.0</td><td>0.0</td><td>8.0</td></tr><tr><td rowspan=\"3\">StableCode3b</td><td>Standard</td><td>61.0</td><td></td><td>51.5</td><td></td><td>7.3</td><td>0.9</td><td>0.0</td><td>0.0</td><td>2.8</td></tr><tr><td>CodeT</td><td>75.0</td><td>+14.0</td><td>57.5</td><td>+6.0</td><td>11.2</td><td>1.8</td><td>0.0</td><td>0.0</td><td>4.6</td></tr><tr><td>FUNCODER</td><td>81.0</td><td>+20.0</td><td>63.5</td><td>+12.0</td><td>13.5</td><td>4.5</td><td>1.1</td><td>0.0</td><td>6.2</td></tr><tr><td rowspan=\"3\">CodeLlama34b</td><td>Standard</td><td>43.9</td><td></td><td>53.5</td><td></td><td>2.3</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.8</td></tr><tr><td>CodeT</td><td>55.5</td><td>+11.6</td><td>56.5</td><td>+3.0</td><td>10.1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3.6</td></tr><tr><td>FUNCODER</td><td>66.5</td><td>+22.6</td><td>58.5</td><td>+5.0</td><td>10.2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3.6</td></tr></table></body></html>\n\n# 3.1 Code Generation\n\nWe choose three benchmarks for code generation evaluation: (a) HumanEval (Chen et al., 2021) includes entry-level coding questions; (b) MBPP (Austin et al., 2021) contains questions of standard library invocation and programming basics; and (c) xCodeEval (Khan et al., 2023) consists of algorithmic challenges sourced from the competitive programming platform CodeForces.\n\n# 3.1.1 Experiment Setup\n\nBenchmarks We adopt the full test set (164 problems) for HumanEval, and sample 200 for MBPP and 500 for xCodeEval, respectively. Following EbTech (2024), we split the xCodeEval into 4 subsets based on problem difficulty: Easy $( \\leq 1 2 0 0 )$ , Mid (1200-1599), Hard (1600-1999) and Expert $( \\geq 2 0 0 0 )$ . The evaluation metric for code generation is $\\mathrm { P a s s } @ 1$ unless specified.\n\nBaselines We compare FUNCODER with standard prompting (Brown et al., 2020), two-stage decomposition method Parsel (Zelikman et al., 2023), self-testing method CodeT (Chen et al., 2023a), self-improvement methods Reflexion and LDB (Shinn et al., 2023; Zhong et al., 2024), and multiagent developing framework MetaGPT (Hong et al., 2024). We implement Standard prompting with a 1-shot demonstration. CodeT samples 11 solutions with standard prompting and evaluates them on model-generated tests. The results for Reflexion are reproduced from the original code.\n\nImplementation Details FUNCODER uses a 2-shot prompt in the divide stage and 1-shot for conquering sub-functions. The number of sampled implementations in the functional consensus is set to 11 for code generation tasks. For further implementation details, please refer to Appendix A.1.\n\n# 3.1.2 Results\n\nTable 1 shows the code generation performance on advanced proprietary models, GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023). For basic programming questions, HumanEval and MBPP, FUNCODER surpass previous SOTA methods by $+ 3 . 3 \\%$ in $\\mathrm { P a s s } @ 1$ and reduce the error rate by $1 8 . 6 \\%$ . Furthermore, FUNCODER demonstrates a substantial improvement on competition-level problems, outperforming others by $1 0 . 4 \\%$ in GPT-4 and $3 5 . 3 \\%$ with GPT-3.5. We observe that FUNCODER can\n\n(a) Preliminary Study on Self-testing (b) Effectiveness of Ranking Strategy 90.2 final passed 43.9% 44.5% 90 85.4 final failed 4.3% 19.5% 85   \nprogram wrong 12.8% 5.5% 80.5 Strategy 80   \nunit-test wrong 25.0% 16.5% Self-TestpaRseseudlt sceolnf-stensstus 75   \nboth incorrect 14.0% 14.0% failed 69.5 random 70 1 50 40 30 20 10 0 0 10 20 30 40 50 1 2 3 5 6 7 8 9 10 11 GPT-3.5 StableCode3b Num Selected Programs\n\nFigure 3: (a) Preliminary study on self-testing, the programs are evaluated using unit-tests generated by LLMs. (b) The effectiveness of different ranking strategies. We compute the $\\mathrm { P a s s } @ \\mathrm { k }$ over top- $\\mathbf { \\nabla } \\cdot \\mathbf { k }$ programs ranked by functional consensus, self-test, and random on 11 candidates. (higher is better)\n\nenhance LLM‚Äôs capability of solving more complex programming tasks, with an average accuracy improvement of $8 2 . 3 \\%$ over the baseline on the Mid and Hard subsets of xCodeEval. Expert level programs, however, still remain a colossal challenge for even the most cutting-edge LLMs.\n\nEvaluation is also performed over community LLMs, Llama3 (Meta AI, 2024), StableCode (Pinnaparaju et al., 2024) and CodeLlama (Rozi√®re et al., 2023) with results in Table 1. FUNCODER consistently boosts the performance of smaller models in code generation, demonstrating notable improvements compared to standard prompting on HumanEval, which gained $+ 2 9 . 4 \\%$ on Llama3, $+ 3 2 . 8 \\%$ on StableCode, and even $+ 5 1 . 5 \\%$ on CodeLlama, outperforming that from the previous best method CodeT. We also supplement results on GPT-4o mini, Codestral and StarCoder2 in Table 11. Experiment results demonstrate that our method archives state-of-the-art performance on various models, ranging from basic programming to competition contests.\n\n# 3.1.3 Analysis\n\nFUNCODER Democratize to Smaller LLMs Limited by the LLM capabilities, the application of selfimprovement or multi-agent methods on smaller models is without ease. By keeping decomposition and composition within the code generation process, our approach exhibits better generalization. As shown in Table 1, with FUNCODER, StableCode $_ { 3 b }$ achieves around $1 1 8 . 6 \\%$ relative performance to standard GPT-3.5, and also aligns closely with GPT-4 by about $9 7 . 7 \\%$ on HumanEval.\n\nPreliminary Study on Self-Testing Method We conduct a preliminary study targeting the self-testing method on HumanEval, results are shown in Figure 3.a with further details in Appendix A.5. We first verify whether model-generated programs can also pass model-generated self-tests: (a) If a program passes self-tests, most from GPT-3.5 would also work on system tests, as much as $1 9 . 5 \\% / 6 4 \\% \\approx 3 0 . 5 \\%$ programs from StableCode are rejected, indicating that smaller models like StableCode may not effectively self-test and detect program errors on its own. (b) In the event of failed self-tests, a large portion of failures are attributed to issues in self-tests instead of the programs, on both GPT-3.5 and StableCode. These phenomena indicate that self-testing methods have limitations in generating correct and reliable unit tests. As a result, we design functional consensus to not require any assertion, but perform mutual verification between solutions instead, as opposed to self-testing.\n\nEffectiveness of Functional Consensus Functional consensus or self-testing may be viewed as ranking algorithms for selecting functions. To measure ranking effectiveness, we conduct an analysis on HumanEval with GPT-3.5. For each problem, 11 candidates are ranked with 3 strategies: consensus, self-test, and random shuffle (as a baseline). Effectiveness is measured via $\\operatorname { P a s s } @ \\operatorname { k } .$ , i.e. if any of the top- $\\mathbf { \\nabla } \\cdot \\mathbf { k }$ ranked programs pass the system test. Figure 3.b shows that functional consensus achieves $9 4 . 7 \\%$ upper bound $( \\mathrm { P a s s } @ 1 1 )$ performance by selecting a single function $( \\operatorname { P a s s } @ 1 )$ , and is close to that of self-test on $\\mathrm { P a s s } @ 4$ . This clearly demonstrates that functional consensus can effectively evaluate correctness and pick the most promising implementation on the first attempt.\n\nAblation and Token Usage To analyze the impact of dividing, conquering, and functional consensus in FUNCODER, we carry out an ablation study with different settings. Studies that replace consensus with self-testing, or with AlphaCode-like (Li et al., 2022) clustering, are also included. The ablation is constructed on HumanEval with GPT-3.5, as shown in Table 2. Note that to generate every program FUNCODER costs only $O ( k N )$ tokens, where $k$ is the number of sampled candidates, and $N$ is the token length of the final program. This is further exemplified and explained in $\\ S \\mathrm { A } . 7$ . We observe that function decomposition and re-composition deliver cumulative performance improvements. Functional consistency is also shown to prevail over self-testing. Putting them all together, FUNCODER received a $+ 1 7 . 1$ improvement with just $5 . 0 9 \\times$ more tokens over baseline. Compared to previous SOTA LDB $\\approx 2 3 \\mathrm { { K } }$ tokens), we are able to gain $+ 2 . 5$ in performance with $7 6 . 5 \\%$ token usage reduction.\n\nTable 2: Ablation study of FUNCODER on HumanEval with GPT-3.5. The setting in our main experiment is highlighted in bold. Tokens are calculated as the sum of prompts and completions.   \n\n<html><body><table><tr><td>Setting</td><td>Divide</td><td>Conquer</td><td>Ranking</td><td>Pass@1</td><td>Avg. Tokens</td></tr><tr><td>Standard</td><td>X</td><td>√ó</td><td>X</td><td>68.3</td><td>886.7</td></tr><tr><td>One-pass</td><td>‚àö</td><td>X</td><td>√ó</td><td>72.6 (+4.3)</td><td>1233.7</td></tr><tr><td>Two-pass</td><td>‚àö</td><td>‚àö</td><td>X</td><td>78.7 (+10.4)</td><td>3343.2</td></tr><tr><td>Two-pass + ST@11</td><td>‚àö</td><td>‚àö</td><td>Self-Test@11</td><td>80.5 (+12.2)</td><td>5408.3</td></tr><tr><td>Two-pass + CL@11</td><td>‚àö</td><td>‚àö</td><td>Clustering @11</td><td>75.0 (+6.7)</td><td>5070.7</td></tr><tr><td>FUNCODER@5</td><td>‚àö</td><td>‚àö</td><td>Consensus@5</td><td>83.5 (+15.2)</td><td>4040.9</td></tr><tr><td>FUNCODER@11</td><td>‚àö</td><td>‚àö</td><td>Consensus @ 11</td><td>85.4 (+17.1)</td><td>5402.0</td></tr></table></body></html>\n\n# 3.2 Mathematical Reasoning\n\nCode can be viewed as a tool for augmenting the reasoning capabilities of LLMs (Chen et al., 2023b). Alternative to text-based reasoning like Chain-of-Thought (Wei et al., 2022), programs can offer unique advantages in terms of iteration and calculations. To test the generalizability of FUNCODER beyond algorithm challenges, we conduct an experiment on MATH (Hendrycks et al., 2021b), a competition-level mathematical reasoning benchmark.\n\n# 3.2.1 Experiment Setup\n\nBenchmark The experiment is conducted on a subset of the MATH test set, including 500 randomly sampled problems that can be classified into 7 disjoint subjects or 5 difficulty levels. It can be noticed that labels in MATH are formatted in $\\mathrm { I A T _ { E } X }$ , rendering exact-match verdicts impractical. We, therefore, follow previous work (Zhang et al., 2024) and adopt GPT-4 to determine the correspondence between predictions and labels, with further details provided in Appendix A.4.\n\nBaselines We compare FUNCODER with the text-based baselines: Standard Prompting and Chainof-Thought (Wei et al., 2022), and program-aided baselines: Program-of-Thought (Chen et al., 2023b), Self-Refine (Madaan et al., 2023), Cumulative Reasoning (Zhang et al., 2024). The results of Cumulative reasoning are reported in the original paper. Standard prompting and chain-of-thought reasoning use 7-shot demonstrations constructed from the train set. Program-of-Thought and SelfRefine prompt the model with 1-shot demonstration to generate a solution() function that solves the problem. Additionally, self-refine iteratively refines programs based on runtime feedback. All baseline methods are run with self-consistency (Wang et al., 2023) at 5.\n\nImplementation Details FUNCODER adopts a program-aided reasoning setting that writes a solution() function and obtains the final prediction by running this program. The number of sampled implementations $| F |$ in functional consensus is set to 5 to match baseline methods.\n\n# 3.2.2 Results\n\nThe experimental results on MATH are shown in Table 3. It shows that program-aided reasoning generally outperforms text-based reasoning. With GPT-4 as the backbone, FUNCODER outperforms the strongest baseline Cumulative Reasoning (Zhang et al., 2024) by $( 6 . 0 / 8 . 3 \\% )$ and surpasses the vanilla program-aided baseline PoT (Chen et al., 2023b) by $( 1 0 . 0 / 1 4 . 7 \\% )$ . When using GPT-3.5- turbo as the backbone, FUNCODER exceeds the strongest baseline by $( 6 . 2 / 1 1 . 1 \\% )$ and outperforms PoT by as much as $( 1 3 . 0 / 3 1 . 7 \\% )$ , which indicates that our approach has a strong advantage over both text-based reasoning and other program-aided reasoning methods.\n\nTable 3: Experimental results on MATH, a competition-level mathematical reasoning benchmark. Best results are in bold. Text-based reasoning methods are denoted with ‚Ä†, while others use programaided reasoning. We report both overall results and results in seven subjects: Prealgebra, Algebra, Number Theory, Counting & Probability, Geometry, Intermediate Algebra, and Precalculus.   \n\n<html><body><table><tr><td>Model</td><td>Method</td><td>Prealg.</td><td>Alg.</td><td>NT</td><td>Prob.</td><td>Geo.</td><td>InterAlg.</td><td>Precalc.</td><td>Overall</td></tr><tr><td rowspan=\"5\">GPT-3.5</td><td>Standard‚Ä†</td><td>62.2</td><td>37.4</td><td>20.0</td><td>29.8</td><td>31.0</td><td>24.4</td><td>21.8</td><td>34.6</td></tr><tr><td>CoTt</td><td>59.8</td><td>51.1</td><td>28.9</td><td>29.8</td><td>28.6</td><td>26.7</td><td>30.9</td><td>40.0</td></tr><tr><td>PoT</td><td>68.3</td><td>50.4</td><td>33.3</td><td>48.9</td><td>21.4</td><td>18.2</td><td>29.1</td><td>41.0</td></tr><tr><td>Self-Refine</td><td>74.4</td><td>49.6</td><td>48.9</td><td>57.4</td><td>28.6</td><td>35.6</td><td>36.4</td><td>48.6</td></tr><tr><td>FUNCODER</td><td>76.8</td><td>61.2</td><td>55.6</td><td>59.6</td><td>34.1</td><td>36.0</td><td>41.8</td><td>54.0</td></tr><tr><td rowspan=\"6\">GPT-4</td><td>Standard‚Ä†</td><td>81.7</td><td>82.7</td><td>71.1</td><td>72.3</td><td>59.5</td><td>46.7</td><td>47.3</td><td>68.2</td></tr><tr><td>CoTt</td><td>84.1</td><td>87.1</td><td>62.2</td><td>68.1</td><td>45.2</td><td>48.9</td><td>54.5</td><td>68.6</td></tr><tr><td>PoT</td><td>79.3</td><td>80.6</td><td>75.6</td><td>72.3</td><td>50.0</td><td>47.8</td><td>58.2</td><td>68.2</td></tr><tr><td>Self-Refine</td><td>82.9</td><td>82.0</td><td>77.8</td><td>76.6</td><td>54.8</td><td>55.6</td><td>63.6</td><td>72.2</td></tr><tr><td>CR</td><td>86.6</td><td>86.3</td><td>88.7</td><td>71.1</td><td>53.7</td><td>51.5</td><td>51.8</td><td>72.2</td></tr><tr><td>FUNCODER</td><td>89.0</td><td>92.8</td><td>82.2</td><td>83.0</td><td>59.5</td><td>63.3</td><td>56.4</td><td>78.2</td></tr><tr><td rowspan=\"3\">Llama38b</td><td>CoTt</td><td>56.1</td><td>47.5</td><td>31.1</td><td>34.0</td><td>40.5</td><td>14.4</td><td>38.2</td><td>38.6</td></tr><tr><td>PoT</td><td>67.1</td><td>32.4</td><td>24.4</td><td>34.0</td><td>16.7</td><td>21.1</td><td>18.2</td><td>32.6</td></tr><tr><td>FUNCODER</td><td>67.9</td><td>45.7</td><td>51.1</td><td>53.2</td><td>19.0</td><td>37.8</td><td>30.9</td><td>45.0</td></tr><tr><td rowspan=\"2\">StableCode3b</td><td>PUTCODER</td><td>20.7</td><td>14.4</td><td>17.8</td><td>25.5</td><td>48</td><td>2.0</td><td>98.2</td><td>14.4</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"2\">CodeLlama34b</td><td>PoT</td><td>35.5</td><td>26.1</td><td>15.0</td><td>16.7</td><td>0.0</td><td>5.5</td><td>33.3</td><td>15.2</td></tr><tr><td>FUNCODER</td><td>44.8</td><td>46.1</td><td>37.8</td><td>34.1</td><td>13.6</td><td>24.6</td><td>37.5</td><td>24.4</td></tr></table></body></html>\n\nOn open-source models, FUNCODER with Llama3 outperforms PoT by $( 1 2 . 4 / 3 8 . 0 \\% )$ . It has even reached competitive performance against the state-of-the-art method based on GPT-3.5 (45.0 v.s. 48.6). When employing StableCode and CodeLLaMA as the backbone, our approach achieves significant improvements by $( 1 2 . 2 / 8 4 . 7 \\% )$ and $( 9 . 2 \\mathrm { ~ / ~ } 6 0 . 5 \\% )$ , respectively. This improvement demonstrates that our approach can significantly boost smaller LLMs, democratizing the complex reasoning capabilities of open-source LLMs through programming.\n\n# 3.2.3 Analysis\n\nFUNCODER Can Handle Harder Questions Figure 4 compares between CoT, PoT, and FUNCODER across varying difficulty levels. It illustrates that CoT performs comparatively well on the easiest questions, but suffers from a steep decline in performance as difficulty increases. This suggests that text-based reasoning is inadequate for tackling challenging mathematical reasoning problems. The same situation is also observed in PoT. In contrast, our method consistently demonstrates high performance even on challenging problems, particularly excelling on level 5 difficulty with nearly double the perfor\n\n![](images/cbd8eab9266e6c797433815d33ef7c03a70d51e09f41030db68be5680fb30b5c.jpg)  \nFigure 4: Average accuracy in each level with the chat model (GPT-3.5) and the code model (StableCode $3 b$ ) on the MATH benchmark.\n\nmance compared to PoT and CoT. This reflects that our method, with divide-and-conquer applied, can effectively cope with complex problems.\n\nDecomposed Functions are Domain-Specific We hypothesize that questions from the same subject require similar knowledge reserves, which should be reflected in the functionality of the sub-functions. To verify this hypothesis, we statisticize the common sub-functions of FUNCODER in each MATH subject, as shown in Table 4. It is apparent that different subjects require different abilities, each with its own set of sub-functions closely associated with the domain knowledge. In addition, these common sub-functions are fundamentally basic and straightforward. As exemplified in Appendix B.2, our method is able to leverage and combine these basic sub-functions to achieve more complex goals, thereby reducing the complexity of reasoning and enhancing performance.\n\nTable 4: Top-3 most commonly used functions in each subject of MATH, listed in descending order.   \n\n<html><body><table><tr><td>Subject</td><td>Functions</td></tr><tr><td>Prealgebra</td><td>is_prime / factorial / gcd</td></tr><tr><td>Algebra</td><td>find_roots/is_perfect_square /find_domain</td></tr><tr><td>Number Theory</td><td>get_divisors /mod_inverse/ gcd</td></tr><tr><td>Counting&Probability</td><td>factorial / combinations/binomial_coefficient</td></tr><tr><td>Geometry</td><td>distance/simplify_fraction/calculate_triangle_area</td></tr><tr><td>Intermediate Algebra</td><td>find_roots Ôºè evaluate_polynomial /lagrange_interpolation</td></tr><tr><td>Precalculus</td><td>cross_product /fraction_from_angle / dot</td></tr></table></body></html>\n\n# 4 Related Work\n\nLarge Language Model for Code Code pre-training has received widespread attention, with early models based on small language models (SLM) (Feng et al., 2020; Lu et al., 2021; Wang et al., 2021). In recent years, with the development of large-scale pre-training techniques, code LLM has emerged, showing remarkable performance in downstream code tasks (Chen et al., 2021; Nijkamp et al., 2023; Li et al., 2022; Rozi√®re et al., 2023; Li et al., 2023b; Guo et al., 2024). Tasks between code and natural language (NL) can be generally divided into three major categories: NL2Code tasks such as code generation (Austin et al., 2021; Chen et al., 2021; Hendrycks et al., 2021a; Khan et al., 2023) and code search (Husain et al., 2019a); Code2Code tasks including code completion (Lu et al., 2021; Zhang et al., 2023a; Liu et al., 2024), code translation (Ahmad et al., 2023; Zhu et al., 2022; Yan et al., 2023), and test generation (Siddiq et al., 2023; Sch√§fer et al., 2024); Code2NL tasks like code summarization (Husain et al., 2019b; Jin et al., 2023). This paper focuses on code generation tasks, ranging from basic to competition level.\n\nCode Refinement and Self-Testing Code doesn‚Äôt always run as expected; it could contain syntax errors, dead loops, or bugs. It‚Äôs essential to debug and refine the code to ensure better quality. CodeT (Chen et al., 2023a) generates unit-tests to score the implementation. AlphaCode (Li et al., 2022) clusters programs based on whether generated program outputs were identical or not. Selfimprovement methods (Madaan et al., 2023; Shinn et al., 2023; Chen et al., 2024; Zhong et al., 2024) design closed-loop procedures that repeatedly refine the code based on the feedback. Like real-life software development processes, multi-agent frameworks (Hong et al., 2024; Qian et al., 2023) construct specific LLM roles, Tester or $Q A$ to generate tests. These studies adopt a shared paradigm wherein self-tests are generated through LLMs. However, Olausson et al. (2024) points out the challenge that LLMs have certain shortcomings in self-repairing their code. This paper avoids these shortcomings by proposing functional consensus as a reliable method of evaluation.\n\nProgram-Aided Reasoning and Agents Aside from code generation tasks, the program can be a tool that augments LLM to solve complex reasoning questions or interact with external environments. Program-of-Thought (Chen et al., 2023b) and PAL (Gao et al., 2023) prompt the model to generate a program that solves mathematical or symbolic problems. MathPrompter (Imani et al., 2023) and Chain-of-Code (Li et al., 2023a) fuse the text-based chain-of-thought with code-based program-of-thought prompting to complement each other in mathematical reasoning. Cumulative Reasoning (Zhang et al., 2024) conducts bottom-up reasoning to derive the final answer progressively. Numerous work (Sun et al., 2023; Wang et al., 2024; Yang et al., 2024) also use code as an intermediate component to bridge LLM agents with external environments.\n\nDecompose for Complex Problems Several recent works employ decomposition to reduce the complexity of hard problems. Least-to-Most (Zhou et al., 2023) adopts a two-stage approach, which first decomposes complex problems, and then solves each sub-problem individually to tackle complex reasoning tasks. Successive Prompting (Dua et al., 2022) adopts a dynamic decomposition, iteratively breaking down problems and addressing sub-problems. Tree-of-Thought (Yao et al., 2023) breaks down complex problems into state spaces and uses tree search to solve them. Parsel (Zelikman et al., 2023) introduces decomposition to code generation tasks, taking a three-stage to break down requirements into draft and intermediate parsel programs. RepoCoder (Zhang et al., 2023b) performs a retrieval in repositories to complete unfinished code one by one. Unlike these methods, FUNCODER recursively decomposes problems into a tree structure, hence gradually reduces its complexity.\n\n# 5 Discussion\n\nLimitations Our approach unleashes the potential power of functions in programming, which is advantageous on well-defined problems such as competitive programming, or program-augmented reasoning tasks. These scenarios do not however represent all use cases, such as open-ended problems or casual software development. Nevertheless, we believe that the idea of divide-and-conquer and sub-modular consensus utilized by FUNCODER can be extended to a wider range of problems, and we consider this as a future exploration.\n\nBroader Impact While code generation is increasingly utilized in software development, Large Language Models (LLMs) are still prone to generating toxic, vulnerable, or malicious code. Such programs pose risks and should be used or executed with extra caution.\n\n# 6 Conclusion\n\nIn this paper, we presented FUNCODER, a novel code generation framework that integrates the divideand-conquer strategy with functional consensus to address complex requirements. FUNCODER had demonstrated superior performance compared to state-of-the-art methods on various benchmarks and models. Our findings highlighted the effectiveness of dynamic decomposition and functional consensus in writing complex code, which suggests that FUNCODER may have the potential to empower further improvements in code generation and other fields.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÈíàÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁîüÊàêÂ§çÊùÇ‰ª£Á†ÅÊó∂ÁöÑÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢òÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÁé∞ÊúâÊñπÊ≥ïÂ¶ÇËÆ°Âàí-Ëß£ÂÜ≥ÂàÜËß£ÂíåÂ§ö‰ª£ÁêÜÂºÄÂèëÊ°ÜÊû∂Â≠òÂú®ËßÑÂàíÂõ∞Èöæ„ÄÅ‰æùËµñÊ®°ÂûãËÉΩÂäõÂíåÊµãËØïÂáÜÁ°ÆÊÄß‰∏çË∂≥Á≠âÈóÆÈ¢ò„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®ÈúÄË¶ÅÁîüÊàêÂ§çÊùÇÁ®ãÂ∫èÁöÑÂú∫ÊôØÔºàÂ¶ÇÁ´ûËµõÁ∫ßÁºñÁ®ãÂíåÊï∞Â≠¶Êé®ÁêÜÔºâ‰∏≠Â∞§‰∏∫ÈáçË¶ÅÔºåÁõ¥Êé•ÂΩ±Âìç‰ª£Á†ÅÁîüÊàêÁöÑÂèØÈù†ÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   FUNCODERÊòØ‰∏Ä‰∏™ÁªìÂêàÂàÜÊ≤ªÁ≠ñÁï•ÂíåÂäüËÉΩÂÖ±ËØÜÁöÑ‰ª£Á†ÅÁîüÊàêÊ°ÜÊû∂ÔºåÈÄöËøáÈÄíÂΩíÂàÜËß£‰ªªÂä°‰∏∫Â≠êÂáΩÊï∞Âπ∂Âà©Áî®Ë°å‰∏∫Áõ∏‰ººÊÄßÈÄâÊã©ÊúÄ‰ºòÂÆûÁé∞„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Âä®ÊÄÅÂáΩÊï∞ÂàÜËß£**ÔºöÈÄöËøáÈÄíÂΩíÂàÜËß£‰ªªÂä°‰∏∫Â≠êÂáΩÊï∞Ê†ëÔºåÈÄêÊ≠•Èôç‰ΩéÂ§çÊùÇÂ∫¶„ÄÇÂú®HumanEvalÂü∫ÂáÜ‰∏äÔºåFUNCODER‰ΩøStableCode3bË∂ÖË∂äGPT-3.5ÊÄßËÉΩ18.6%ÔºåËææÂà∞GPT-4ÁöÑ97.7%„ÄÇ\\n> *   **ÂäüËÉΩÂÖ±ËØÜÊú∫Âà∂**ÔºöÈÄöËøáË°å‰∏∫Áõ∏‰ººÊÄßÈÄâÊã©ÊúÄ‰ºòÂáΩÊï∞ÂÆûÁé∞ÔºåÂáèÂ∞ëÈîôËØØ‰º†Êí≠„ÄÇÂú®GPT-4‰∏äÔºåFUNCODERÂú®MATHÂü∫ÂáÜ‰∏äÊèêÂçá6.0%„ÄÇ\\n> *   **ÂπøÊ≥õÈÄÇÁî®ÊÄß**ÔºöFUNCODERÂú®Â§öÁßçÊ®°ÂûãÔºàÂ¶ÇGPT-3.5„ÄÅGPT-4„ÄÅLlama3Á≠âÔºâÂíå‰ªªÂä°Ôºà‰ª£Á†ÅÁîüÊàê„ÄÅÊï∞Â≠¶Êé®ÁêÜÔºâ‰∏äÂùáË°®Áé∞‰ºòÂºÇÔºåÂπ≥ÂùáÊèêÂçá9.8%„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   FUNCODERÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÂ§çÊùÇ‰ªªÂä°ÈÄíÂΩíÂàÜËß£‰∏∫Â≠êÂáΩÊï∞ÔºåÂΩ¢ÊàêÂáΩÊï∞Ê†ëÔºåÁÑ∂ÂêéÈÄöËøáÂäüËÉΩÂÖ±ËØÜÈÄâÊã©ÊúÄ‰ºòÂÆûÁé∞„ÄÇÂÖ∂ÊúâÊïàÊÄßÂú®‰∫éÂàÜÊ≤ªÁ≠ñÁï•Èôç‰Ωé‰∫ÜÂçï‰∏™ÂáΩÊï∞ÁöÑÂ§çÊùÇÂ∫¶ÔºåËÄåÂäüËÉΩÂÖ±ËØÜÈÄöËøáË°å‰∏∫Áõ∏‰ººÊÄßÂáèÂ∞ë‰∫ÜÈîôËØØ‰º†Êí≠„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØî**ÔºöÁé∞ÊúâÊñπÊ≥ïÂ¶ÇËÆ°Âàí-Ëß£ÂÜ≥ÂàÜËß£ÈúÄË¶ÅÈ¢ÑÂÖàËÆæËÆ°ÂÆåÊï¥ËÆ°ÂàíÔºåÁº∫‰πèÂä®ÊÄÅË∞ÉÊï¥ËÉΩÂäõÔºõÂ§ö‰ª£ÁêÜÊ°ÜÊû∂‰æùËµñÊ®°ÂûãËÉΩÂäõ‰∏îÈöæ‰ª•Ê≥õÂåñÔºõËá™ÊµãËØïÊñπÊ≥ïÂèóÈôê‰∫éÊµãËØïÁöÑÊ≠£Á°ÆÊÄß„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõ**ÔºöFUNCODERÂä®ÊÄÅÂºïÂÖ•Â≠êÂáΩÊï∞ÔºåÈÅøÂÖçÈ¢ÑÂÖàËßÑÂàíÔºõÈÄöËøáÂäüËÉΩÂÖ±ËØÜÔºàËÄåÈùûËá™ÊµãËØïÔºâÈÄâÊã©ÂÆûÁé∞ÔºåÂáèÂ∞ëÂØπÊµãËØïÂáÜÁ°ÆÊÄßÁöÑ‰æùËµñ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **DivideÈò∂ÊÆµ**Ôºö‰ªéÊ†πÂáΩÊï∞ÂºÄÂßãÔºåÈÄíÂΩíÂàÜËß£‰ªªÂä°‰∏∫Â≠êÂáΩÊï∞ÔºåÁîüÊàêÂáΩÊï∞Â§¥ÂíåÊñáÊ°£ÔºåÊé®ËøüÂÆûÁé∞„ÄÇ\\n> 2.  **ConquerÈò∂ÊÆµ**ÔºöÊåâÈÄÜÊãìÊâëÈ°∫Â∫èÈáçÊñ∞ÁîüÊàêÂáΩÊï∞ÔºåÂà©Áî®Â∑≤Ëß£ÂÜ≥ÁöÑÂ≠êÂáΩÊï∞ÁªÑÂêàÂÆûÁé∞Â§çÊùÇÁõÆÊ†á„ÄÇ\\n> 3.  **ÂäüËÉΩÂÖ±ËØÜ**ÔºöÈááÊ†∑Â§ö‰∏™ÂáΩÊï∞ÂÆûÁé∞ÔºåËÆ°ÁÆóË°å‰∏∫Áõ∏‰ººÊÄßÔºåÈÄâÊã©ÂÖ±ËØÜÊúÄÈ´òÁöÑÂÆûÁé∞„ÄÇÂÖ¨ÂºèÔºö\\n>     $$\\n>     sim(f,g) = \\\\int_{x \\\\in D(f)} \\\\frac{\\\\mathbb{1}[f(x)=g(x)]}{|D(f)|} \\\\approx \\\\sum_{x \\\\in X \\\\mid X \\\\sim D(f)} \\\\frac{\\\\mathbb{1}[f(x)=g(x)]}{|X|}\\n>     $$\\n>     $$\\n>     f^* = \\\\arg\\\\max_{f_{(i)} \\\\in F} \\\\sum_{f_{(j)} \\\\in F \\\\setminus \\\\{f_{(i)}\\\\}} sim(f_{(i)}, f_{(j)})\\n>     $$\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   Ê†áÂáÜÊèêÁ§∫ÔºàStandardÔºâ„ÄÅCodeT„ÄÅReflexion„ÄÅLDB„ÄÅMetaGPT„ÄÅParsel„ÄÅChain-of-ThoughtÔºàCoTÔºâ„ÄÅProgram-of-ThoughtÔºàPoTÔºâ„ÄÅSelf-Refine„ÄÅCumulative ReasoningÔºàCRÔºâ„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Pass@1Ôºà‰ª£Á†ÅÁîüÊàêÔºâ‰∏ä**ÔºöFUNCODERÂú®HumanEval‰∏äËææÂà∞85.4%ÔºàGPT-3.5ÔºâÂíå94.5%ÔºàGPT-4ÔºâÔºåÊòæËëó‰ºò‰∫éCodeTÔºà81.1%Âíå90.9%ÔºâÂíåReflexionÔºà69.5%Âíå91.0%Ôºâ„ÄÇ‰∏éÊúÄ‰Ω≥Âü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá3.3%ÔºàGPT-3.5ÔºâÂíå3.6%ÔºàGPT-4Ôºâ„ÄÇ\\n> *   **Âú®Êï∞Â≠¶Êé®ÁêÜÔºàMATHÔºâ‰∏ä**ÔºöFUNCODERÂú®GPT-4‰∏äËææÂà∞78.2%Ôºå‰ºò‰∫éCRÔºà72.2%ÔºâÂíåPoTÔºà68.2%ÔºâÔºåÊèêÂçá6.0%„ÄÇÂú®GPT-3.5‰∏äËææÂà∞54.0%Ôºå‰ºò‰∫éSelf-RefineÔºà48.6%ÔºâÔºåÊèêÂçá5.4%„ÄÇ\\n> *   **Âú®Â∞èÂûãÊ®°ÂûãÔºàÂ¶ÇStableCode3bÔºâ‰∏ä**ÔºöFUNCODERÂú®HumanEval‰∏äËææÂà∞81.0%ÔºåË∂ÖË∂äÊ†áÂáÜÊèêÁ§∫Ôºà61.0%Ôºâ20.0%ÔºåÊòæËëó‰ºò‰∫éCodeTÔºà75.0%Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ‰ª£Á†ÅÁîüÊàê (Code Generation, N/A)\\n*   ÂàÜÊ≤ªÁ≠ñÁï• (Divide-and-Conquer Strategy, N/A)\\n*   ÂäüËÉΩÂÖ±ËØÜ (Functional Consensus, N/A)\\n*   Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (Large Language Model, LLM)\\n*   Êï∞Â≠¶Êé®ÁêÜ (Mathematical Reasoning, N/A)\\n*   ÈÄíÂΩíÂàÜËß£ (Recursive Decomposition, N/A)\\n*   Ë°å‰∏∫Áõ∏‰ººÊÄß (Behavioral Similarity, N/A)\\n*   Á´ûËµõÁ∫ßÁºñÁ®ã (Competitive Programming, N/A)\"\n}\n```"
}