{
    "link": "https://arxiv.org/abs/2405.15682",
    "pdf_link": "https://arxiv.org/pdf/2405.15682",
    "title": "The Road Less Scheduled",
    "authors": [
        "Aaron Defazio",
        "Xingyu Yang",
        "Harsh Mehta",
        "Konstantin Mishchenko",
        "Ahmed Khaled",
        "Ashok Cutkosky"
    ],
    "institutions": [
        "Google Research",
        "Meta",
        "Samsung AI Center"
    ],
    "publication_date": "2024-05-24",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 60,
    "influential_citation_count": 6,
    "paper_content": "# The Road Less Scheduled\n\nAaron Defazio1 Fundamental AI Research Team, Meta\n\nXingyu (Alice) Yang2 Fundamental AI Research Team, Meta\n\nHarsh Mehta Google Research\n\nKonstantin Mishchenko Samsung AI Center\n\nAhmed Khaled Princeton University\n\nAshok Cutkosky3 Boston University 2 Engineering Co-lead\n\n1Research Co-lead\n\n3 Senior Author\n\n# Abstract\n\nExisting learning rate schedules that do not require specification of the optimization stopping step $T$ are greatly out-performed by learning rate schedules that depend on $T$ . We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available1. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.\n\n# 1 Introduction\n\nThe theory of optimization, as applied in machine learning, has been successful at providing precise, prescriptive results for many problems. However, even in the simplest setting of stochastic gradient descent (SGD) applied to convex Lipschitz functions, there are glaring gaps between what our current theory prescribes and the methods used in practice.\n\nConsider the stochastic gradient descent (SGD) step with step size $\\gamma > 0$ , $z _ { t + 1 } = z _ { t } - \\gamma g _ { t }$ where $g _ { t }$ is the stochastic (sub-)gradient at time $t$ , computed at the point $z _ { t }$ (formally defined in Section 1.1) of a convex Lipschitz function $f$ . Although standard practice for many classes of problems, classical convergence theory suggests that the expected loss of this $z$ sequence is suboptimal, and that the Polyak-Ruppert (PR) average $x$ of the sequence should be returned instead (Polyak, 1990; Ruppert, 1988):\n\n$$\n\\begin{array} { r l } & { z _ { t + 1 } = z _ { t } - \\gamma g _ { t } } \\\\ & { x _ { t + 1 } = \\left( 1 - c _ { t + 1 } \\right) x _ { t } + c _ { t + 1 } z _ { t + 1 } , } \\end{array}\n$$\n\nwhere using $c _ { t + 1 } = 1 / ( t + 1 )$ results in $\\begin{array} { r } { x _ { t } = \\frac { 1 } { T } \\sum _ { t = 1 } ^ { T } z _ { t } } \\end{array}$ . Despite their theoretical optimality, PR averages give much worse results in practice thanPusing the last-iterate of SGD (Figures 2a,8) — a folk-law result in the field of optimization, and a large theory-practice gap that is often attributed to the mismatch between this simplified problem class and the complexity of problems addressed in practice.\n\n![](images/daa50623c7c9e5fa3fc53904ad71b982601e630ed3550916f15819dcbbd2b006.jpg)  \nFigure 1: Schedule-Free methods (black) closely track the Pareto frontier of loss v.s. training time in a single run. Both Schedule-Free SGD (left) and AdamW (right) match or exceed the performance of cosine learning rate schedules of varying lengths (red).\n\nRecently, Zamani and Glineur (2023) and Defazio et al. (2023) showed that the exact worst-case optimal rates can be achieved via carefully chosen learning rate sequences (also known as schedules) alone, without the use of averaging. This result suggests that schedules have, in some sense, the same role to play as PR averaging in optimization. However, schedules have a critical disadvantage: they require setting the optimization stopping time $T$ in advance.\n\nMotivated by the theory-practice gap for Polyak-Ruppert averaging, we ask the following question:\n\nDo there exist iterate averaging approaches that match the empirical performance of learning rate schedules, without sacrificing theoretical guarantees?\n\nBy developing a new link between averaging and learning rate sequences, we introduce a new approach to averaging that maintains the worst-case convergence rate theory of PR averaging, while matching and often exceeding the performance of schedule-based approaches – firmly answering this question in the affirmative.\n\n# Summary of Results\n\n• Our approach does not require the stopping time $T$ to be known or set in advance. It closely tracks the Pareto frontier of loss versus training time during a single training run (Figure 1), while requiring no additional hyper-parameters over the base SGD (with momentum) or Adam optimizer.   \n• Our approach uses an alternative form of momentum that replaces traditional momentum. This form has appealing theoretical properties: it is worst case optimal for any choice of the momentum parameter in the convex Lipschitz setting, a property that does not hold for traditional momentum.   \n• Our key theoretical result is a new online-to-batch conversion theorem, which establishes the optimality of our method while also unifying several existing online-to-batch theorems.   \n• We perform, to our knowledge, one of the largest machine learning optimization algorithm evaluations to date, consisting of 28 problems, ranging from logistic regression to large-scale deep learning problems. This evaluation contains more distinct and diverse largescale machine-learning problems than any other optimizer evaluation we are aware of in the literature. Schedule-Free methods show strong performance, matching or out-performing heavily-tuned cosine schedules.   \n• Schedule-Free AdamW won the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning (Adaptive Algorithm) Track, providing independent verification of its SOTA performance against other optimization algorithms in cases where hyperparametertuning is limited. We provide details of our entry and plots comparing it to the competition baseline.\n\n# 1.1 Notation\n\nConsider the stochastic convex minimization $\\begin{array} { r } { \\operatorname* { m i n } _ { x \\in \\mathbb { R } ^ { d } } f ( x ) = \\mathbb { E } _ { \\zeta } [ f ( x , \\zeta ) ] } \\end{array}$ , where each $f ( x , \\zeta )$ is Lipschitz and convex in $x$ , and the expectation is taken over the random variable $\\zeta$ . With a slight abuse of notation, we assume we are given, at time step $t$ and any point $y$ that we choose, an arbitrary sub-gradient $\\nabla f ( y , \\zeta _ { t } )$ from the sub-differential of $f$ .\n\n![](images/e6334d2877f2d6668790697915ec8237e3f24862e780cd60300097d866cb60f5.jpg)\n\n(a) Schedule-Free learning converges faster than classical averaging approaches, often out-performing tuned schedules.\n\n![](images/f58e18d5f7f51a8146dcfefa68abad26cf6677c724a6e9bb1d1c26e463988866.jpg)\n\n(b) Incorporating the momentum parameter $\\beta$ allows for convergence despite using larger learning rates $\\gamma$ on quadratic problems.\n\n# 2 Method\n\nWe propose the following method, which we call Schedule-Free SGD:\n\n$$\n\\begin{array} { r l } & { \\quad y _ { t } = ( 1 - \\beta ) z _ { t } + \\beta x _ { t } , } \\\\ & { z _ { t + 1 } = z _ { t } - \\gamma \\nabla f ( y _ { t } , \\zeta _ { t } ) , } \\\\ & { x _ { t + 1 } = \\left( 1 - c _ { t + 1 } \\right) x _ { t } + c _ { t + 1 } z _ { t + 1 } , } \\end{array}\n$$\n\nwhere $c _ { t + 1 } = 1 / ( t + 1 )$ and $z _ { 1 } = x _ { 1 }$ is the initial point. Note that, with this weighting, the $x$ sequence is just a running average of the $z$ sequence. The $y$ sequence is the gradient location sequence (on which gradients are evaluated at each step). The $z$ sequence is the base sequence, which is where the base optimizer’s update is performed (in this case SGD). The $x$ sequence is the evaluation sequence, our best estimate of the weights so far.\n\nThis method has a momentum parameter $\\beta$ that interpolates between Polyak-Ruppert averaging $\\begin{array} { r } { \\beta = 0 , } \\end{array}$ ) and Primal averaging $\\begin{array} { r } { \\beta = 1 , } \\end{array}$ ). Primal averaging (Nesterov and Shikhman, 2015; Tao et al., 2018; Cutkosky, 2019; Kavis et al., 2019; Sebbouh et al., 2021; Defazio and Gower, 2021; Defazio and Jelassi, 2022), is an approach where the gradient is evaluated at the averaged point $x$ , instead of $z$ :\n\n$$\n\\begin{array} { r l } & { z _ { t + 1 } = z _ { t } - \\gamma \\nabla f ( x _ { t } , \\zeta _ { t } ) } \\\\ & { x _ { t + 1 } = \\left( 1 - c _ { t + 1 } \\right) x _ { t } + c _ { t + 1 } z _ { t + 1 } , } \\end{array}\n$$\n\nthis approach maintains the worst-case optimality of PR averaging but is generally considered to converge too slowly to be practical (Figures 2a,8). The advantage of our interpolation is that we get the best of both worlds. We can achieve the fast convergence of Polyak-Ruppert averaging (since the $z$ sequence moves much quicker than the $x$ sequence), while still keeping some coupling between the returned sequence $x$ and the gradient-evaluation locations $y$ , which increases stability. Values of $\\beta$ similar to standard momentum values $\\beta \\approx 0 . 9$ appear to work well in practice. We will use the notation $\\alpha = 1 - \\beta$ when convenient.\n\nIn this formulation, $\\beta = 0 . 9$ gives the practical advantages of momentum, dampening the immediate impact of large gradients, resulting in more stable training. To see this, notice that the immediate effect of the gradient $g _ { t }$ at step $t$ is to introduce $( 1 - \\beta ) g _ { t } \\stackrel { - } { = } 0 . 1 g _ { t }$ into the iterate sequence $y$ . This is similar to exponential-moving-average (EMA) momentum, where also $( 1 - \\beta ) g _ { t }$ is added into the iterate sequence on step $t$ . However, here the remainder of $g _ { t }$ is very slowly added into $y$ over time, via its place in the average $x$ , whereas with an EMA with $\\beta = 0 . 9$ , the majority of the gradient is incorporated within the next 10 steps. So from this viewpoint, the Schedule-Free updates can be seen as a version of momentum that has the same immediate effect, but with a greater delay for adding in the remainder of the gradient. This form of momentum (by interpolation) also has a striking advantage: it does not result in any theoretical slowdown; it gives the optimal worst case (Nesterov, 2013) convergence for the non-smooth convex setting (including constants), for any choice of momentum $\\beta$ between 0 and 1 inclusive:\n\nTheorem 1. Suppose $F$ is a convex function, and $\\zeta _ { 1 } , \\ldots , \\zeta _ { T }$ is an i.i.d. sequence of random variables such that $F = \\mathbb { E } [ f ( x , \\zeta ) ]$ for some function $f$ that is $G$ -Lipschitz in $x$ . For any minimizer $x _ { \\star }$ , define\n\n$D = \\| x _ { 1 } - x _ { \\star } \\|$ and $\\gamma = D / ( G { \\sqrt { T } } )$ . Then for any $\\beta \\in [ 0 , 1 ]$ , Schedule-Free SGD ensures:\n\n$$\n\\mathbb { E } [ F ( x _ { T } ) - F ( x _ { \\star } ) ] \\leq { \\frac { D G } { \\sqrt { T } } }\n$$\n\nIn contrast, exponential-moving-average momentum in the non-smooth setting actually hurts the theoretical worst-case convergence rate. The Schedule-Free approach maintains the advantages of momentum (Sutskever et al., 2013) without the potential worst-case slow-down.\n\n# 2.1 General Theory\n\nThe method analyzed in Theorem 1 is actually a special-case of a more general result that incorporates arbitrary online optimization algorithms rather than only SGD, as well as arbitrary time-varying sequences of $\\beta _ { t }$ . The proof is provided in Appendix A.\n\nTheorem 2. Let $F$ be a convex function. Let $\\zeta _ { 1 } , \\ldots , \\zeta _ { T }$ be an iid sequence such that $F ( x ) =$ $\\mathbb { E } _ { \\zeta } [ f ( x , \\zeta ) ]$ . Let $z _ { 1 } , \\dots , z _ { T }$ be arbitrary vectors and let $w _ { 1 } , \\ldots , w _ { T }$ and $\\beta _ { 1 } , \\ldots , \\beta _ { T }$ be arbitrary numbers in $[ 0 , 1 ]$ such that $z _ { t } , w _ { t }$ and $\\beta _ { t }$ are independent of $\\zeta _ { t } , \\ldots , \\zeta _ { T }$ . Set:\n\n$$\n\\begin{array} { l } { \\displaystyle x _ { t } = \\frac { \\sum _ { i = 1 } ^ { t } w _ { i } z _ { i } } { \\sum _ { i = 1 } ^ { t } w _ { i } } = x _ { t - 1 } \\underbrace { \\left( 1 - \\frac { w _ { t } } { \\sum _ { i = 1 } ^ { t } w _ { i } } \\right) } _ { \\triangleq 1 - c _ { t } } + \\underbrace { \\frac { w _ { t } } { \\sum _ { i = 1 } ^ { t } w _ { i } } } _ { \\triangleq c _ { t } } z _ { t } } \\\\ { \\displaystyle y _ { t } = \\beta _ { t } x _ { t } + ( 1 - \\beta _ { t } ) z _ { t } } \\\\ { \\displaystyle g _ { t } = \\nabla f ( y _ { t } , \\zeta _ { t } ) . } \\end{array}\n$$\n\nThen we have for all $x _ { \\star }$ :\n\n$$\n\\mathbb { E } [ F ( x _ { T } ) - F ( x _ { \\star } ) ] \\leq \\frac { \\mathbb { E } [ \\sum _ { t = 1 } ^ { T } w _ { t } \\langle g _ { t } , z _ { t } - x _ { \\star } \\rangle ] } { \\sum _ { i = 1 } ^ { T } w _ { i } } .\n$$\n\nTo recover Theorem 1 from the above result, notice that the algorithm analyzed by Theorem 1 is captured by Theorem 2 with $w _ { t } = 1$ , $\\beta _ { t }$ a constant $\\beta$ and $z _ { t + 1 } = z _ { t } - \\gamma g _ { t }$ for all $t$ . Next, observe that the sequence $z _ { 1 } , \\dots , z _ { T }$ is performing online gradient descent (Zinkevich, 2003), for which it is well-known that the regret $\\textstyle \\sum _ { t = 1 } ^ { T } \\langle g _ { t } , z _ { t } - x _ { \\star } \\rangle$ (appearing in the numerator of our result) is bounded by $D G { \\sqrt { T } }$ and so the result of Theorem 1 immediately follows.\n\nThe regret is the principle object of study in online convex optimization (Hazan, 2022; Orabona, 2019). Viewed in this light, Theorem 2 provides a way to convert an online convex optimization algorithm into a stochastic optimization algorithm: it is a form of online-to-batch conversion (Cesa-Bianchi et al., 2004). Classical online-to-batch conversions are a standard technique for obtaining convergence bounds for many stochastic optimization algorithms, including stochastic gradient descent (Zinkevich, 2003), AdaGrad (Duchi et al., 2011), AMSGrad (Reddi et al., 2018), and Adam (Kingma and Ba, 2014). All of these algorithms can be analyzed as online convex optimization algorithms: they provide bounds on the regret $\\scriptstyle \\sum _ { t = 1 } ^ { T } \\langle g _ { t } , z _ { t } - x _ { \\star } \\rangle$ rather than direct convergence guarantees. It is then necessary (although sometimes left unstated) to convert these regret bounds into stochastic convergence guarantees via an online-to-batch conversion. Our result provides a more versatile method for effecting this conversion.\n\nTheorem 2 actually provides a “grand unification” of a number of different online-to-batch conversions that have been proposed over the years. Most of these conversion methods were first developed specifically to provide convergence analysis for SGD (or some variant such as dual averaging or mirror descent), and then generalized into techniques that apply to any online convex optimization algorithm. For example, the classical Polyak averaging method can be generalized to form the “standard” online-to-batch conversion of Cesa-Bianchi et al. (2004), and is immediately recovered from Theorem 2 by setting $w _ { t } = 1$ and $\\beta _ { t } = 0$ for all $t$ . More recently Nesterov and Shikhman (2015); Tao et al. (2018) derived an alternative to Polyak averaging that was later generalized to work with arbitrarily online convex optimization algorithms by Cutkosky (2019); Kavis et al. (2019), and then observed to actually be equivalent to the heavy-ball momentum by Defazio (2020); Defazio and Gower (2021); Defazio and Jelassi (2022). This method is recovered by our Theorem 2 by setting $w _ { t } = 1$ and $\\beta _ { t } = 1$ for all $t$ . Finally, very recently Zamani and Glineur (2023) discovered that gradient descent with a linear decay stepsize provides a last-iterate convergence guarantee, which was again generalized to an online-to-batch conversion by Defazio et al. (2023). This final result is also recovered by Theorem 2 by setting $w _ { t } = 1$ and $\\begin{array} { r } { \\beta _ { t } = \\frac { t } { T } } \\end{array}$ (see Appendix B).\n\nIn Appendix C, we give a further tightening of Theorem 2 – it can be improved to an equality by precisely tracking additional terms that appear on the right-hand-side. This tightened version can be used to show convergence rate results for smooth losses, both with and without strong-convexity. As an example application, we show that schedule-free optimistic-gradient methods (Rakhlin and Sridharan, 2013) converge with accelerated rates:\n\n$$\n\\mathbb { E } [ F ( x _ { T } ) - F ( x _ { \\star } ) ] = O \\left( \\frac { D ^ { 2 } L } { T ^ { 2 } } + \\frac { D \\sigma } { \\sqrt { T } } \\right) .\n$$\n\n# 2.2 On Large Learning Rates\n\nUnder classical worst-case convergence theory, the optimal choice of $\\gamma$ for a fixed duration training time $T$ is $\\gamma = D / ( G { \\sqrt { T } } )$ . This is the rate used in our bounds for Theorem 1 above. For any-time convergence (i.e. when stopping is allowed at any timestep), our proposed method can, in theory, be used with the standard learning rate sequence:\n\n$$\n\\gamma _ { t } = \\frac { D } { G \\sqrt { t } } .\n$$\n\nHowever, learning rate sequences of this form have poor practical performance (Defazio et al., 2023). Instead, much larger steps of the form $D / G$ give far better performance across virtually all problems in applications (Defazio and Mishchenko, 2023) — another theory-practice mismatch that is virtually undiscussed in the literature. Existing theory suggests that this step-size is too large to give $\\mathcal { O } ( 1 / \\sqrt { T } )$ convergence, however, as we show below, there is an important special case where such large step sizes also give optimal rates up to constant factors.\n\nTheorem 3. Consider the online learning setting with bounded gradients $g _ { t }$ . Let $z _ { t + 1 } = z _ { t } - \\gamma g _ { t }$ Let $D = \\| z _ { 1 } - z _ { * } \\|$ for arbitrary reference point $z _ { * }$ and define $G = \\operatorname* { m a x } _ { t \\leq T } \\left\\| g _ { t } \\right\\|$ . Suppose that the chosen step-size is $\\gamma = D / G$ , then if it holds that:\n\n$$\n\\sum _ { t = 1 } ^ { T } \\left. g _ { t } , z _ { t } - z _ { 1 } \\right. \\leq D { \\sqrt { \\sum _ { t = 1 } ^ { T } \\left\\| g _ { t } \\right\\| ^ { 2 } } } ,\n$$\n\nthen:\n\n$$\n\\frac { 1 } { T } \\sum _ { t = 1 } ^ { T } \\left. g _ { t } , z _ { t } - z _ { * } \\right. = \\mathcal { O } \\left( \\frac { D } { T } \\sqrt { \\sum _ { t = 1 } ^ { T } \\left. g _ { t } \\right. ^ { 2 } } \\right) .\n$$\n\nThis regret bound for SGD implies a convergence rate bound for Schedule-Free SGD by application of our online-to-batch conversion. Condition 31 can be checked during a training run (Using reference point $z _ { * } = x _ { T }$ , and so $D = \\| x _ { 1 } - x _ { T } \\| )$ , and we find that it holds for every problem we consider in our experiments in Section 4. More generally, the full conditions under which large learning rates can be used are not yet fully understood for stochastic problems. In the quadratic case, Bach and Moulines (2013) established that large fixed step-sizes give optimal convergence rates, and we conjecture that the success of large learning rates may be attributed to asymptotic quadratic behavior of the learning process.\n\nEmpirically, we find that Schedule-Free momentum enables the use of larger learning rates $\\gamma > 0$ even in quadratic minimization problems $\\begin{array} { r } { f ( x ) = \\frac { 1 } { 2 } x ^ { \\top } A x - b ^ { \\top } x } \\end{array}$ . We generate 10 different such 20-dimensional problems with eigenvalues drawn log-uniformly in $[ 1 0 ^ { - 6 } , 1 ]$ . We plot the average minimal loss achieved as a function of the two parameters $\\beta$ and $\\gamma$ in Figure 2b. We can see that when the learning rate we use is small, what value of $\\beta$ we choose has little to no effect on the convergence of the algorithm. However, when $\\gamma$ is large, choosing $\\beta < 1$ becomes crucial to achieving convergence.\n\n# 3 Related Work\n\nThe proposed method has a striking resemblance to Nesterov’s accelerated method (Nesterov, 1983, 2013) for $L$ -smooth functions, which can be written in the AC-SA form (Lan, 2012):\n\n$$\n\\begin{array} { c } { y _ { t } = ( 1 - c _ { t + 1 } ) x _ { t } + c _ { t + 1 } z _ { t } } \\\\ { z _ { t + 1 } = z _ { t } - \\displaystyle \\frac { k + 1 } { 2 L } \\nabla f ( y _ { t } ) } \\\\ { x _ { t + 1 } = \\left( 1 - c _ { t + 1 } \\right) x _ { t } + c _ { t + 1 } z _ { t + 1 } , } \\end{array}\n$$\n\nwhere $c _ { t + 1 } = 2 / ( t + 2 )$ . The averaging constant, and more generally\n\n$$\nc _ { t + 1 } = { \\frac { r + 1 } { t + r + 1 } } ,\n$$\n\nfor any real $r > - 1$ is equivalent to the weighted average (Shamir and Zhang, 2013; Defazio and Gower, 2021) $\\begin{array} { r } { x _ { t } \\propto \\sum _ { t = 1 } ^ { T } t ^ { \\bar { r } } z _ { t } } \\end{array}$ , where $t ^ { \\bar { r } }$ represents the $r$ th factorial power of $t$ . Our framework is compatible with factorial power averages without sacrificing theoretical guarantees.\n\nOur approach differs from conventional accelerated methods by using a different weight for the $y _ { t }$ and $\\boldsymbol { x } _ { t }$ interpolations. We use a constant weight for $y _ { t }$ and a decreasing weight for $\\boldsymbol { x } _ { t }$ . Accelerated methods for strongly-convex problems use a constant weight for both, and those for non-strongly convex use an decreasing weight for both, so our approach doesn’t directly correspond to either class of accelerated method. Accelerated methods also use a much larger step size for the $z _ { t }$ sequence than our approach.\n\nThe use of equal-weighted averages is less common than the use of exponential weighting in the practical deep learning optimization literature. Exponential moving averages (EMA) of the iterate sequence are used in the popular Lookahead optimizer (Zhang et al., 2019). In the case of SGD, it performs $i = 1 \\ldots k$ inner steps:\n\n$$\nz _ { t , i } = z _ { t , i - 1 } - \\gamma \\nabla f ( z _ { t , i - 1 } )\n$$\n\nfollowed by an outer step:\n\n$$\nx _ { t } = x _ { t - 1 } + \\alpha \\left( z _ { t , k } - x _ { t - 1 } \\right) .\n$$\n\nThe inner optimizer then starts at $z _ { t + 1 , 0 } ~ = ~ x _ { t - 1 }$ . The Lookahead method can be seen as the EMA version of primal averaging, just as exponential weight averaging is the EMA version of Polyak-Ruppert averaging.\n\nTail averaging, either using an exponential moving average or an equal-weighted average, is a common ‘folk-law’ technique that often yields a practical improvement. For instance, this kind of averaging is used without citation by the influential work of Szegedy et al. (2016): “Model evaluations are performed using a running average of the parameters computed over time.”, and by Vaswani et al. (2017): “...averaged the last 20 checkpoints”. Tail averages are typically “Polyak-Ruppert” style averaging as the average is not used for gradient evaluations during training.\n\nMore sophisticated tail averaging approaches such as Stochastic Weight Averaging (Izmailov et al., 2018) and LAtest Weight Averaging (Kaddour, 2022; Sanyal et al., 2023) combine averaging with large or cyclic learning rates. They are not a replacement for scheduling, instead they aim to improve final test metrics. They generally introduce additional hyper-parameters to tune, and require additional memory. It is possible to use SWA and LAWA on top of our approach, potentially giving further gains.\n\nSandler et al. (2023) show via a stochastic quadratic analysis framework that averaging and learning rate decreases achieve the same effective learning rate. For instance, and average of two points along the training trajectory can give almost identical results to using a learning rate two times smaller. Stochastic quadratic problems are particularly special, Bach and Moulines (2013) have shown that Polyak averaging gives optimal $\\mathcal { O } ( \\bar { 1 } / T )$ rates without the use of decreasing time-dependent step size sequences in this setting.\n\nWithin optimization theory, tail averages can be used to improve the convergence rate for stochastic non-smooth SGD in the strongly convex setting from $\\mathcal { O } ( \\log ( \\bar { T } ) / T )$ to $\\mathcal { O } ( 1 / \\bar { T } )$ (Rakhlin et al., 2012), although at the expense of worse constants compared to using weighted averages of the whole sequence (Lacoste-Julien et al., 2012).\n\nPortes et al. (2022) use cyclic learning rate schedules with increasing cycle periods to give a method that explores multiple points along the Pareto frontier of training time vs eval performance. Each point at the end of a cycle is an approximation to the model from a tuned schedule ending at that time. Our method gives the entire frontier, rather than just a few points along the path. In addition, our method matches or improves upon best known schedules, whereas the “... cyclic trade-off curve underestimated the standard trade-off curve by a margin of $0 . 5 \\%$ validation accuracy” (Portes et al., 2022).\n\n# 4 Experiments\n\nFor our deep learning experiments, we evaluated Schedule-Free learning on a set benchmark tasks that are commonly used in the optimization research literature:\n\nCIFAR10 A Wide ResNet (16-8) architecture (Zagoruyko and Komodakis, 2016) on the CIFAR10 image classification dataset.   \nCIFAR100 A DenseNet (Huang et al., 2017) architecture on the CIFAR-100 (100-class) classification dataset.   \nSVHN A deep ResNet architecture (3-96) on the Street View House Numbers (SVHN) dataset.   \nImageNet A standard ResNet-50 architecture (He et al., 2016) on the ILSVRC 2012 ImageNet (Russakovsky et al., 2015) classification dataset.   \nIWSLT14 A LSTM architecture (Wiseman and Rush, 2016) on the IWSLT14 German-English translation dataset (Cettolo et al., 2014).   \nDLRM The DLRM (Naumov et al., 2019) architecture on the Criteo Kaggle Display Advertising dataset (Jean-Baptiste Tien, 2014).   \nMRI A stacked U-Net architecture (Sriram et al., 2020) on the fastMRI dataset (Zbontar et al., 2018).   \nMAE Fine-tuning a pretrained Masked Autoencoder (He et al., 2021) ViT (patch16-512d-8b) on the ILSVRC 2012 ImageNet dataset.   \nNanoGPT A 124M parameter GPT-2 (Radford et al., 2019) style decoder-only transformer on the OpenWebText dataset (Gokaslan and Cohen, 2019).\n\nFor each problem, both the baseline and the Schedule-Free method were tuned by sweeping both the weight decay and learning rate on a grid. We also swept $\\beta$ over two values, 0.9 and 0.98. Final hyper-parameters are listed in the Appendix. Schedule-Free SGD was used for CIFAR10, CIFAR100, SVHN and ImageNet, and Schedule-Free AdamW (Loshchilov and Hutter, 2019) was used for the remaining tasks. We further include a step-wise schedule as a comparison on problems where step-wise schedules are customary. Further results for Polyak and Primal averaging are in Appendix I.\n\nOur approach shows very strong performance (Figure 3) out-performing existing state-of-the-art cosine schedules on CIFAR-10, CIFAR-100, SVHN, IWSLT-14 (Figure 2a) and OpenWebText GPT-2 problems, as well as the state-of-the-art Linear Decay schedules on the fastMRI and Criteo DLRM tasks. On the remaining two problems, MAE fine-tuning and ImageNet ResNet-50 training, it ties with the existing best schedules.\n\nIn general, the optimal learning rates for the Schedule-Free variants were larger than the optimal values for the base optimizers. The ability to use larger learning rates without diverging may be a contributing factor to the faster convergence of Schedule-Free methods. The $\\beta$ parameter works well at the default value of 0.9 for all problems except NanoGPT, where the loss started to increase rapidly when 0.9 was used (similar to the Polyak Averaging results in Appendix I). The larger $\\beta = 0 . 9 8$ value in our sweep was stable.\n\n# 4.1 MLCommons Algorithmic Efficiency benchmark\n\nThe AlgoPerf challenge (Dahl et al., 2023) is designed to be a large-scale and comprehensive benchmark for deep learning optimization algorithms, covering major data domains and architectures. It includes Transformers, ConvNets and U-Net models across image, language, graph and speech domains, and contains 8 problems total. We evaluated Schedule-Free AdamW following the competition guidelines, comparing against NAdamW, the competition reference Algorithm, running 10 seeds of each. As this is a time-to-target competition, traditional error bars are not appropriate so we instead plot all 10 seeds separately. Note that we excluded one benchmark problem, ResNet-50 training, as neither AdamW nor NAdamW can hit the target accuracy on that task.\n\n![](images/27ac325a8338f61619078180aceeaedf4b77f78b658e00f0950b298fdc4361f6.jpg)  \nFigure 3: Deep Learning Experiments\n\nThe self-tuning track restricts participants to provide a single set of hyper-parameters to use for all 8 problems. Given the large number of problems, this gives performance representative of a good default configuration.\n\nSchedule-Free AdamW performs well across all considered tasks, out-performing the baseline on the WMT, VIT, FASTMRI and OGBG training, while tying on the Conformer and Criteo workloads, and marginally under-performing on the DeepSpeech workload. We attribute the performance on the Conformer and DeepSpeech tasks to their use of batch-norm - the AlgoPerf setup doesn’t easily allow us to update the BN running statistics on the $x$ sequence, which is necessary with our method to get the best performance (See Section 4.3).\n\n# 4.2 Convex Problems\n\nWe validated the Schedule-Free learning approach on a set of standard logistic regression problems from the LibSVM repository. For each problem, and each method separately, we performed a full learning rate sweep on a power-of-two grid, and plotted mean and standard-error of the final train accuracy from 10 seeds using the best learning rate found.\n\nSchedule-Free learning out-performs both averaging approaches and the state-of-the-art linear decay (LD) schedule baseline (Figure 7). It converges faster on all but 1 of 12 problems, has higher accuracy on 6 of the problems, and ties the baseline on the remaining problems. This demonstrates that the performance advantages of Schedule-Free methods are not limited to non-convex problems.\n\n# 4.3 Implementation Concerns\n\nThe Schedule-Free variant of a method typically has the same memory requirements as the base method. For instance, Schedule-Free SGD requires no extra memory over standard SGD with momentum. Whereas SGDM tracks the current point $x$ and the momentum buffer $m$ , we can track $x$ and $z$ . The quantity $y$ can be computed directly from the latest values of $x$ and $z$ , and so doesn’t need\n\n![](images/a4e1d961ae8c275985e6a90fd6d5245f79be6e290f119e37e05b7a7d557726d1.jpg)  \nFigure 4: Schedule-Free Adam compared to target-setting baseline on the Algoperf competition self-tuning track.\n\n# Algorithm 1 Schedule-Free AdamW\n\n1: Input: $x _ { 1 }$ , learning rate $\\gamma$ , decay $\\lambda$ , warmup steps $T _ { \\mathrm { w a r m u p } } , \\beta _ { 1 } , \\beta _ { 2 } , \\epsilon$   \n2: $z _ { 1 } = x _ { 1 }$   \n3: $v _ { 0 } = 0$   \n4: for $t = 1$ to T do   \n5: $y _ { t } = ( 1 - \\beta _ { 1 } ) z _ { t } + \\beta _ { 1 } x _ { t }$ ▷ Momentum via interpolation   \n6: gt ∂f (yt, ζt) $D$ Gradient is evaluated at $y$   \n7: $v _ { t } = \\beta _ { 2 } \\dot { v } _ { t - 1 } + ( 1 - \\beta _ { 2 } ) g _ { t } ^ { 2 }$   \n8: $\\gamma _ { t } = \\gamma \\sqrt { 1 - \\beta _ { 2 } ^ { t } } \\mathrm { m i n } ( 1 , t / T _ { \\mathrm { w a r m u p } } )$ ▷ LR includes warmup and Adam bias-correction   \n9: $z _ { t + 1 } = z _ { t } - \\gamma _ { t } g _ { t } / ( \\sqrt { v _ { t } } + \\epsilon ) - \\gamma _ { t } \\lambda y _ { t }$   \n10: ct+1 = itγ=t1 γi2   \n11: $x _ { t + 1 } = \\left( 1 - c _ { t + 1 } \\right) x _ { t } + c _ { t + 1 } z _ { t + 1 }$ ▷ Update weighted iterate average   \n12: end for   \n13: Return $x _ { T }$   \nxx\n\nto be explicitly stored. It’s also possible to instead store $z$ and $y$ , and then compute $x$ when needed.   \nThis low memory usage is the case for AdamW also, see Algorithm 1.\n\nOur efficient PyTorch implementation actually uses one buffer to always store $z$ and the primary parameter buffer to store either $x$ or $y$ , with the stored quantity flipping between the two for training and test/inference passes.\n\nOur method requires extra code to handle models where batch norm is used. This is due to the fact that BatchNorm layers maintain a running_mean and running_var to track batch statistics which is calculated at $y$ . For model evaluation, these buffers need to be updated to match the statistics on the $x$ sequence. This can be done by evaluating a small number of training batches using $x$ right before each eval. More sophisticated approaches such as PreciseBN ( $\\mathrm { w } _ { \\mathrm { u } }$ and Johnson, 2021) can also be used. This calculation is not needed for other normalization layers that do not use batch-statistics.\n\nLearning rate warmup is still necessary for our method. We use a linear warmup for a fixed duration, and fuse the Adam bias-correction term into the learning rate for simplicity (this potentially impacts the effect of weight-decay during early iterations), giving a learning rate LR $\\gamma _ { t } = \\gamma \\sqrt { 1 - \\beta _ { 2 } ^ { t } } \\mathrm { m i n } ( 1 , t / T _ { \\mathrm { w a r m u p } } )$ that approaches $\\gamma$ when the warmup and bias-correction period ends. We found that performance was greatly improved by using a weighted $c _ { t }$ sequence when\n\n6750 Schedule-Free LR 5.0 (76.27% SE 0.03) 6750 SGD LR 0.5 (76.42% SE 0.05) Schedule-Free LR 3.0 (77.39% SE 0.05) SGD LR 0.3 (77.05% SE 0.03) Schedule-Free LR 1.5 (77.83% SE 0.03) SGD LR 0.15 (77.57% SE 0.02) Schedule-Free LR 1.0 (77.42% SE 0.02) SGD LR 0.1 (77.68% SE 0.04) Schedule-Free LR 0.5 (76.17% SE 0.07) SGD LR 0.05 (77.69% SE 0.04)   \n60 1 T 1 1 T 60 1 1 T T 1 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 Epoch Epoch\n\nwarmup is used, weighted by the square of the $\\gamma _ { t }$ used during warmup:\n\n$$\nc _ { t + 1 } = \\frac { \\gamma _ { t } ^ { 2 } } { \\sum _ { i = 1 } ^ { t } \\gamma _ { i } ^ { 2 } } .\n$$\n\nThis sequence decreases at a $1 / t$ rate after the learning rate warmup. It is shifted by one from the indexing used in Theorem 2, which is done to simplify the implementation. This sequence is motivated by Theorem 2’s weighting sequences, which suggest weights proportional to polynomials of the learning rate. This sequence was used for both SGD and AdamW experiments.\n\nWeight decay for Schedule-Free methods can be computed at either the $y$ or $z$ sequences. We used decay at $y$ for our experiments, as this matches the interpretation of weight-decay as the use of an additional L2-regularizer term in the loss. We found that computing the regularization at $y$ gives significantly better performance on some problems including ImageNet and NanoGPT training.\n\n# 5 Parameter Sensitivity\n\nFor Schedule-Free learning to be truly schedulefree, it’s important that the momentum hyperparameter doesn’t implicitly have a dependence on the time-horizon. If tuning this parameter gave different values depending on the training duration, then the problem of setting the horizon has just been shifted to setting the momentum value. In Figure 5 we run ImageNet training with Schedule-Free SGD for a longerthen-standard 200 epochs with a variety of momentum values, with the LR fixed to 1.5. We\n\nILSVRC 2012 ImageNet (ResNet-50) Momentum Sweep 6750 Schedule-Free MOM 0.75 (76.60% SE 0.02) Schedule-Free MOM 0.9 (77.78% SE 0.04) Schedule-Free MOM 0.95 (76.99% SE 0.05) Schedule-Free MOM 0.98 (75.37% SE 0.03) 60 1 0 25 50 75 100 125 150 175 200 Epoch\n\nfind that the best choice of momentum ( $\\beta = 0 . 9 )$ is the same for all durations of training.\n\nSchedule-Free learning has a similar mild time-horizon dependency for the baseline learning rate value as schedule-based approaches. Figure 6 shows that the optimal learning rate stays the same for broad range of values, for both Schedule-Free and Schedule based training. For short duration training $\\leq 2 5$ epochs), larger LR values begin to show the best performance. Appendix J shows the sensitivity of the final test accuracy to the baseline learning rate for a selection of our test problems, in comparison to the baseline optimizer with a cosine schedule. We see that the overall sensitivity is similar to the baseline optimizer in each problem.\n\n# 6 Conclusion\n\nTwo roads diverged in a wood, and I— I took the one less traveled by, And that has made all the difference. - Robert Frost\n\nWe have presented Schedule-Free learning, an optimization approach that removes the need to specify a learning rate schedule while matching or outperforming schedule-based learning. The primary practical limitation is the need to sweep learning rate and weight decay, as the best values differ from the those used with a schedule. We provide a preliminary theoretical exploration of the method, establishing its worst-case optimal performance for non-smooth Lipschitz convex optimization.\n\n# Funding Acknowledgments\n\nAC is supported by NSF grant number CCF-2211718.",
    "summary": "{\n  \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n现有不需要指定优化停止步骤 $T$ 的学习率调度方法，其性能远不如依赖 $T$ 的学习率调度方法。同时，在随机梯度下降应用于凸 Lipschitz 函数的简单场景中，当前理论规定的方法与实际使用的方法存在明显差距，例如 Polyak - Ruppert 平均在理论上最优，但在实践中效果远不如使用随机梯度下降的最后一次迭代结果。这些问题限制了优化算法在机器学习中的应用效果，影响了模型的训练效率和性能。此外，传统学习率序列在实际应用中性能不佳，大学习率虽在实践中效果好但现有理论认为其难以达到最优收敛速度，存在理论与实践的不匹配。\\n\\n**方法概述**\\n论文提出了一种无调度（Schedule - Free）学习方法，该方法摒弃了学习率调度的使用，避免了预先设定停止时间 $T$ 的需求。通过引入一种新的动量形式和在线到批量转换定理，实现了在多种问题上与基于调度的方法相当甚至更优的性能。\\n\\n**主要贡献与效果**\\n- 该方法无需预先知道或设置停止时间 $T$，在单次训练运行中紧密跟踪损失与训练时间的帕累托前沿，且无需在基础随机梯度下降（带动量）或 Adam 优化器的基础上增加额外的超参数。在28个涵盖从逻辑回归到大规模深度学习问题的评估中，无调度方法表现强劲，与经过大量调优的余弦调度方法相当或更优。\\n- 无调度 AdamW 赢得了 MLCommons 2024 AlgoPerf 算法效率挑战的自调优（自适应算法）赛道，验证了其在超参数调优受限情况下相对于其他优化算法的最优性能。\\n- 引入一种替代传统动量的动量形式，在凸 Lipschitz 设定下，对于任何动量参数的选择都是最坏情况最优的，避免了传统动量在非光滑设定下对理论最坏情况收敛率的负面影响。\",\n  \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n该方法的核心思想是通过建立迭代平均和学习率序列之间的新联系，引入一种新的平均方法。利用动量参数 $\\beta$ 在 Polyak - Ruppert 平均和原始平均之间进行插值，在保持 PR 平均最坏情况收敛率理论的同时，匹配并常常超越基于调度方法的性能。同时，提出的在线到批量转换定理统一了几种现有的在线到批量定理，为方法的最优性提供了理论支持。该方法还通过将在线凸优化算法转换为随机优化算法，实现了一种在线到批量的转换，避免了传统学习率调度对停止时间 $T$ 的依赖，并且新的动量形式在理论上具有最优性。此外，研究了大学习率在特定条件下也能达到最优收敛速度的情况。\\n\\n**创新点**\\n- 先前的学习率调度方法需要预先设定优化停止时间 $T$，而该方法避免了这一需求。\\n- 采用了一种替代形式的动量，该形式在凸 Lipschitz 环境下对于任何动量参数的选择都是最坏情况最优的，这是传统动量所不具备的特性。\\n- 提出的新在线到批量转换定理统一了多种现有的在线到批量定理，为方法的最优性提供了更广泛的理论基础。\\n- 探讨了大学习率在实际应用中的有效性，并给出了在特定条件下大学习率能达到最优收敛速度的理论结果。\\n\\n**具体实现步骤**\\n1. 定义随机凸最小化问题 $\\min_{x \\in \\mathbb{R}^d} f(x) = \\mathbb{E}_{\\zeta}[f(x, \\zeta)]$，在时间步 $t$ 给定任意子梯度 $\\nabla f(y_t, \\zeta_t)$。初始化初始点 $z_1 = x_1$，动量参数 $\\beta$，学习率 $\\gamma$ 等。\\n2. 对于无调度 SGD 方法：\\n   - 计算 $y_t = (1 - \\beta)z_t + \\beta x_t$，其中 $\\beta$ 是动量参数，$y$ 序列是梯度定位序列。\\n   - 计算梯度 $g_t=\\nabla f(y_t,\\zeta_t)$。\\n   - 更新 $z_{t + 1} = z_t - \\gamma \\nabla f(y_t, \\zeta_t)$，$z$ 序列是基础序列。\\n   - 计算 $x_{t + 1} = (1 - c_{t + 1})x_t + c_{t + 1}z_{t + 1}$，其中 $c_{t + 1} = 1 / (t + 1)$，$x$ 序列是评估序列。\\n3. 对于无调度 AdamW 算法：\\n   - 输入初始点 $x_1$、学习率 $\\gamma$、衰减 $\\lambda$、热身步骤 $T_{warmup}$、$\\beta_1$、$\\beta_2$、$\\epsilon$。\\n   - 初始化 $v_0 = 0$。\\n   - 对于 $t$ 从 1 到 $T$：\\n     - 计算 $y_t = (1 - \\beta_1)z_t + \\beta_1 x_t$。\\n     - 计算梯度 $g_t = \\nabla f(y_t, \\zeta_t)$。\\n     - 更新 $v_t = \\beta_2 v_{t - 1} + (1 - \\beta_2)g_t^2$。\\n     - 计算学习率 $\\gamma_t = \\gamma \\sqrt{1 - \\beta_2^t} \\min(1, t / T_{warmup})$。\\n     - 更新 $z_{t + 1} = z_t - \\gamma_t g_t / (\\sqrt{v_t} + \\epsilon) - \\gamma_t \\lambda y_t$。\\n     - 计算 $c_{t + 1} = \\frac{\\gamma_t^2}{\\sum_{i = 1}^t \\gamma_i^2}$。\\n     - 更新 $x_{t + 1} = (1 - c_{t + 1})x_t + c_{t + 1}z_{t + 1}$。\\n   - 返回 $x_T$。\\n\\n**案例解析**\\n论文中生成了 10 个 20 维的二次最小化问题 $\\ f(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$，特征值在 $[10^{-6}, 1]$ 内对数均匀抽取。通过绘制平均最小损失与 $\\beta$ 和 $\\gamma$ 两个参数的函数关系图，发现当学习率较小时，$\\beta$ 的选择对算法收敛影响不大；当 $\\gamma$ 较大时，选择 $\\beta < 1$ 对实现收敛至关重要。\",\n  \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 余弦学习率调度（Cosine Learning Rate Schedules）\\n- 线性衰减调度（Linear Decay Schedules）\\n- NAdamW（作为 MLCommons 算法效率基准的竞争参考算法）\\n- Polyak 平均和原始平均（在一些实验中作为对比）\\n\\n**性能对比**\\n*   **在准确率指标上：** 本文的无调度方法在 CIFAR - 10、CIFAR - 100、SVHN、IWSLT - 14 和 OpenWebText GPT - 2 等问题上，优于现有的最先进的余弦调度方法；在 fastMRI 和 Criteo DLRM 任务上，优于最先进的线性衰减调度方法；在 MAE 微调、ImageNet ResNet - 50 训练问题上，与现有最佳调度方法表现相当。在标准逻辑回归问题上，无调度学习在 12 个问题中的 11 个上收敛速度更快，在 6 个问题上具有更高的准确率，在其余问题上与基线持平。\\n*   **在 MLCommons 算法效率基准测试中：** Schedule - Free AdamW 在 WMT、VIT、FASTMRI 和 OGBG 训练任务上，优于基线；在 Conformer 和 Criteo 工作负载上，与基线表现相当；在 DeepSpeech 工作负载上，略逊于基线，推测是由于 AlgoPerf  setup 不易更新 BN 运行统计信息，而这对于该方法获得最佳性能是必要的。\\n*   **在学习率使用上：** 无调度方法的最优学习率通常比基础优化器的最优值大，并且能够使用更大的学习率而不发散，这可能是其收敛更快的一个因素。\",\n  \"keywords\": \"### 关键词\\n\\n- 无调度学习 (Schedule - Free Learning, N/A)\\n- 学习率调度 (Learning Rate Scheduling, N/A)\\n- 随机梯度下降 (Stochastic Gradient Descent, SGD)\\n- 动量优化 (Momentum Optimization, N/A)\\n- 在线到批量转换 (Online - to - Batch Conversion, N/A)\\n- 机器学习优化 (Machine Learning Optimization, N/A)\\n- 大学习率 (Large Learning Rate, N/A)\\n- 迭代平均 (Iterate Averaging, N/A)\"\n}"
}