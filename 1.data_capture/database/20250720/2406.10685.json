{
    "link": "https://arxiv.org/abs/2406.10685",
    "pdf_link": "https://arxiv.org/pdf/2406.10685",
    "title": "Scale Equivariant Graph Metanetworks",
    "authors": [
        "Archimedes/Athena RC, Greece",
        "National and Kapodistrian University of Athens"
    ],
    "institutions": [
        "National and Kapodistrian University of Athens"
    ],
    "publication_date": "2024-06-15",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 11,
    "influential_citation_count": 5,
    "paper_content": "# Scale Equivariant Graph Metanetworks\n\nIoannis Kalogeropoulos1,2∗ Giorgos Bouritsas1,2∗ Yannis Panagakis1,2\n\n1National and Kapodistrian University of Athens 2Archimedes/Athena RC, Greece\n\n# Abstract\n\nThis paper pertains to an emerging machine learning paradigm: learning higherorder functions, i.e. functions whose inputs are functions themselves, particularly when these inputs are Neural Networks (NNs). With the growing interest in architectures that process NNs, a recurring design principle has permeated the field: adhering to the permutation symmetries arising from the connectionist structure of NNs. However, are these the sole symmetries present in NN parameterizations? Zooming into most practical activation functions (e.g. sine, ReLU, tanh) answers this question negatively and gives rise to intriguing new symmetries, which we collectively refer to as scaling symmetries, that is, non-zero scalar multiplications and divisions of weights and biases. In this work, we propose Scale Equivariant Graph MetaNetworks - ScaleGMNs, a framework that adapts the Graph Metanetwork (message-passing) paradigm by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant to valid scalings. We introduce novel building blocks, of independent technical interest, that allow for equivariance or invariance with respect to individual scalar multipliers or their product and use them in all components of ScaleGMN. Furthermore, we prove that, under certain expressivity conditions, ScaleGMN can simulate the forward and backward pass of any input feedforward neural network. Experimental results demonstrate that our method advances the state-of-the-art performance for several datasets and activation functions, highlighting the power of scaling symmetries as an inductive bias for NN processing. The source code is publicly available at https://github.com/jkalogero/scalegmn.\n\n# 1 Introduction\n\nNeural networks are becoming the workhorse of problem-solving across various domains. To solve a task, they acquire rich information which is stored in their learnable parameters throughout training. Nonetheless, this information is often opaque and difficult to interpret. This begs the question: How can we efficiently process and extract insights from the information stored in the parameters of trained neural networks, and how to do so in a data-driven manner? In other words, can we devise architectures that learn to process other neural architectures?\n\nThe need to address this question arises in diverse scenarios: NN post-processing, e.g. analysis/interpretation (i.e. inferring NN properties, such as generalisation and robustness [74, 19]), as well as editing (e.g. model pruning [22], merging [75] or adaptation to new data) - or NN synthesis (e.g. for optimisation [12] or more generally parameter prediction/generation [31, 67, 32, 58]). Furthermore, with the advent of Implicit Neural Representations,1 [13, 52, 57, 70, 53] trained NN parameters are increasingly used to represent datapoint signals, such as images or 3D shapes, replacing raw representations, i.e., pixel grids or point clouds [18]. Consequently, many tasks involving such data, across various domains such as computer vision [47] and physics [68], which are currently tackled using domain-specific architectures (e.g. CNNs for grids, PointNets [63] or GNNs [60] for point clouds and meshes), could potentially be solved by NNs that process the parameters of other NNs.\n\nThe idea of processing/predicting NN parameters per se is not new to the deep learning community, e.g. it has been investigated in hypernetworks [27] or even earlier [65]. However recent advancements, driven by the need for INR processing, leverage a crucial observation: NNs have symmetries. In particular, several works have identified that the function represented by an NN remains intact when applying certain transformations to its parameters [28, 11] with the most well-known transformations being permutations. That is, hidden neurons can be arbitrarily permuted within the same layer, along with their incoming and outgoing weights. Therefore, all the tasks mentioned in the previous paragraph can be considered part of equivariant machine learning [10], as they involve developing models that are invariant or equivariant to the aforementioned transformations of neural networks. Recently Navon et al. [54] and Zhou et al. [86] acknowledged the importance of permutation symmetries and first proposed equivariant architectures for Feedforward NN (FFNN) processing, demonstrating significant performance improvements against non-equivariant baselines. These works, as well as their improvements [85] and extensions to process more intricate and varying architectures [33, 44, 88], are collectively known as weight space networks or metanetworks, with graph metanetworks being a notable subcase (graph neural networks with message-passing).\n\nNevertheless, it is known that permutations are not the only applicable symmetries. In particular, theoretical results in various works [11, 61], mainly unified in [25] show that FFNNs exhibit additional symmetries, which we collectively here refer to as scaling symmetries: multiplying the incoming weights and the bias of a hidden neuron with a non-zero scalar $a$ (with certain properties) while dividing its outgoing weights with another scalar $b$ (often $b = a _ { , } ^ { \\dagger }$ ), preserves the NN function. Intuitively, permutation symmetries arise from the graph/connectionist structure of the NN whereas different scaling symmetries originate from its activation functions $\\sigma$ , i.e. when it holds that $\\sigma ( a x ) =$ $b \\sigma ( x )$ . However, equivariance w.r.t. scaling symmetries has not received much attention so far (perhaps with the exception of sign symmetries - $a \\in \\{ - 1 , 1 \\}$ [41, 40]), and it remains effectively unexplored in the context of metanetworks.\n\nTo address this gap, we introduce in this paper a graph metanetwork framework, dubbed as Scale Equivariant Graph MetaNetworks- ScaleGMN, which guarantees equivariance to permutations and desired scaling symmetries, and can process FFNNs of arbitrary graph structure with a variety of activation functions. At the heart of our method lie novel building blocks with scale invariance/equivariance properties w.r.t. arbitrary families of scaling parameters. We prove that ScaleGMN can simulate the forward and backward pass of an FFNN for arbitrary inputs, enabling it to reconstruct the function represented by any input FFNN and its gradients.\n\nOur contributions can be summarised as follows:\n\n• We extend the scope of metanetwork design from permutation to scaling symmetries. • We design invariant/equivariant networks to scalar multiplication of individual multipliers or combinations thereof, originating from arbitrary scaling groups. • We propose scale equivariant message passing, using the above as building blocks, unifying permutation and scale equivariant processing of FFNNs. Additionally, the expressive power of our method is analysed w.r.t. its ability to simulate input FFNNs. • Our method is evaluated on 3 activations: ReLU (positive scale), tanh (sign) and sine (sign - we first characterise this using a technique from [25]) on several datasets and three tasks (INR classification/editing & generalisation prediction), demonstrating superior performance against common metanetwork baselines.\n\n# 2 Related work\n\nNeural network symmetries. Long preceding the advent of metanetworks, numerous studies delved into the inherent symmetries of neural networks. Hecht-Nielsen [28] studied FFNNs and discovered the existence of permutation symmetries. Chen et al. [11] showed that, in FFNNs with tanh activations, the only function-preserving smooth transformations are permutations and sign flips (multiplication of incoming and outgoing weights with a sign value) - this claim was strengthened in [21]; this was the first identification of scaling symmetries. Follow-up works extended these observations to other activations, such as sigmoid and RBF [37] and ReLU [56, 51, 61, 64, 26], and architectures, such as RNNs [3, 2], characterising other scaling symmetries and providing conditions under which permutations and scalings are the only available function-preserving symmetries or the parameters are identifiable given the input-output mapping. Recently, Godfrey et al. [25] provided a technique to characterise such symmetries for arbitrary activations that respect certain conditions, unifying many previous results. Additionally, symmetries have been found in other layers, e.g. batch norm [6, 14, 16] (scale invariance) or softmax [36] (translation invariance). Prior to metanetworks, symmetries were mainly studied to obtain a deeper understanding of NNs or in the context of optimisation/learning dynamics [56, 71, 4, 17, 36, 5, 83, 84] and/or model merging/ensembling [20, 1, 69, 59, 55].\n\nMetanetworks. The first solutions proposed for NN processing and learning did not account for NN symmetries. Unterthiner et al. [74] and Eilertsen et al. [19], initially employed standard NNs on vectorised (flattened) CNN weights or some statistics thereof, to predict properties of trained neural networks such as generalization or estimation of a hyperparameter resp.). Similar methods have been proposed to process continuous data represented as INRs. In [78] high-order spatial derivatives are used (suitable only for INRs), in [47] the architecture operates on stacked parameter vectors (but is constrained by the assumption that all INRs are trained from the same initialization), while in [18] and [7] the authors learn low-dimensional INR embeddings (which are further used for downstream tasks) by jointly fitting them with meta-learning. Finally, Schürholt et al. [66] learn representations of NN weights with self-supervised learning and in [67] this is extended to generative models.\n\nIn contrast to the above methods, our work follows a recent stream of research focusing on equivariant metanetworks. Navon et al. [54] and Zhou et al. [85] first characterised all linear equivariant layers to permutation symmetries of MLPs and combined them with non-linearities, while in the latter this was extended to CNNs. These approaches derive intricate weight-sharing patterns but are nonlocal2 and cannot process varying architectures. In follow-up works, Zhou et al. [86] constructed equivariant attention layers and in [87] the above characterisation was generalised to arbitrary input architectures and layers (e.g. RNNs and transformers) introducing an algorithm for automatic linear layer (with weight-sharing) construction. A different route was taken in the very recent works of Kofinas et al. [33] and Lim et al. [44], where input NNs are treated as a special class of graphs and are naturally processed with Graph Neural Networks (GNNs), with appropriate symmetry breaking wherever necessary. This perspective has been adopted several times in the deep learning literature [9, 23, 45, 77, 80] and in a few examples, GNNs were applied on the graph representation for, e.g. neural architecture search ([82, 72, 31]), albeit without mapping parameters to the graph edges. In parallel to our work, [73] extended the framework of [85] to incorporate scaling and sign-flipping symmetries. They construct non-local equivariant layers, resulting in an architecture with fewer trainable parameters. Nevertheless, their method suffers from limited expressive power, while their experimental results showcase limited advancements. In contrast to our method, this approach limits the choice of activation functions to those equivariant to the studied symmetries. Finally, their framework cannot be extended to other activation functions, as the parameter sharing must be re-designed from scratch, while it is not suitable for processing diverse architectures.\n\nScale Equivariance Equivariance to vector scaling (or more generally to matrix multiplication with diagonal matrices) remains to date underexplored in the machine learning community. Sign symmetries received attention in the work of Lim et al. [42], where an invariant network (SignNet) was designed, mainly to process eigenvectors, with the theoretical analysis revealing universality under certain conditions. This was extended in [39], where the importance of sign equivariance on several tasks was demonstrated and a sign equivariant network was proposed. In our work, we draw inspiration from these architectures and extend their formulations to arbitrary scaling symmetries.\n\n# 3 Notation and Preliminaries\n\nNotation. We denote vectors, matrices, and tensors with bold-face letters, e.g., $\\mathbf { x } , \\mathbf { X } , \\mathbf { X }$ , respectively and sets with calligraphic letters $\\chi$ . A normal font notation will be used for miscellaneous purposes (mostly indices and functions). Datapoint (input) functions/signals will be denoted with $u$ , while higher-order functions (functions of functions) will be denoted with fraktur font $\\mathfrak { F }$ .\n\nFunctions of Neural Networks. Consider functions of the form $u _ { G , \\theta } : \\mathcal { X }  \\hat { \\mathcal { X } }$ . Each function is parameterised (1) by a computational graph $G$ , which determines all the mathematical operations that should be performed to evaluate the function $u$ at a datapoint $\\mathbf { x } \\in \\mathcal { X }$ . When $u$ is a neural network, $G$ is determined by the NN architecture. Additionally, $u$ is parameterised by (2), by a tuple of numerical parameters $\\pmb \\theta$ , on which the aforementioned mathematical operations are applied (along with the input $\\mathbf { x }$ ) - these are the learnable parameters, which can be arranged into a vector. We are interested in learning unknown higher-order functions $\\mathfrak { F } : \\hat { \\mathcal { X } } ^ { \\mathcal { X } } \\to \\mathcal { Y }$ of the form $\\mathfrak { F } \\left( u _ { G , \\theta } \\right)$ . In our case, the goal is to learn $\\mathfrak { F }$ by accessing solely the parameters $( G , \\theta )$ of each $u$ , i.e. via functions $\\hat { f } : \\mathcal { G } \\times \\Theta  \\mathcal { V }$ of the form $\\hat { f } ( G , \\pmb \\theta )$ , where $\\mathcal { G }$ is a space of architectures/computational graphs and $\\Theta$ a space of parameters.3 We are typically interested in learning functionals $( \\mathcal { V } \\subseteq \\mathbb { R } ^ { d } )$ or operators $( \\mathcal { V } = \\mathcal { G } \\times \\Theta$ or $\\mathcal { V } = \\Theta$ ). To approximate the desired higher-order function, we assume access to a dataset of parameters sampled i.i.d. from an unknown distribution $p$ on $\\mathcal { G } \\times \\Theta$ . For example, in a supervised setup, we aim to optimise the following: $\\begin{array} { r } { \\operatorname * { a r g m i n } _ { \\hat { f } \\in \\mathcal { F } } \\mathbb { E } _ { ( G , \\pmb { \\theta } ) \\sim p } L \\Big ( \\mathfrak { F } ( u _ { G , \\pmb { \\theta } } ) , \\hat { f } ( G , \\pmb { \\theta } ) \\Big ) } \\end{array}$ where $L ( \\cdot , \\cdot )$ is a loss function and $\\mathcal { F }$ is an NN processing hypothesis class (e.g. metanetworks).\n\nFeedforward Neural Networks (FFNNs). In this paper, we focus our analysis on Feedforward Neural Networks (FFNNs), i.e. linear layers interleaved with non-linearities. Consider NNs of the form $u _ { G , \\theta } : \\mathbb { R } ^ { d _ { \\mathrm { i n } } } \\xrightarrow { } \\mathbb { R } ^ { d _ { \\mathrm { o u t } } }$ of the following form:\n\n$$\n\\begin{array} { r } { \\mathbf { x } _ { 0 } = \\mathbf { x } , \\quad \\mathbf { x } _ { \\ell } = \\sigma _ { \\ell } \\left( \\mathbf { W } _ { \\ell } \\mathbf { x } _ { \\ell - 1 } + \\mathbf { b } _ { \\ell } \\right) , \\quad u _ { G , \\theta } ( \\mathbf { x } ) = \\mathbf { x } _ { L } } \\end{array}\n$$\n\nwhere $L$ : the number of layers, $\\mathbf { W } _ { i } \\in \\mathbb { R } ^ { d _ { \\ell } \\times d _ { \\ell - 1 } }$ : the weights of the NN, $\\mathbf { b } _ { i } \\in \\mathbb { R } ^ { d _ { \\ell } }$ : the biases of the NN, $d _ { 0 } = d _ { \\mathrm { i n } }$ , $d _ { L } = d _ { \\mathrm { o u t } }$ , $\\sigma _ { \\ell } : \\mathbb { R }  \\mathbb { R }$ activation functions applied element-wise. Here, the learnable parameters are $\\pmb \\theta = ( \\mathbf W _ { 1 } , \\dots , \\mathbf W _ { L } , \\mathbf b _ { 1 } , \\dots , \\mathbf b _ { L } )$ and the computational graph encodes the connections between vertices, but also the type of activations used in each layer.\n\nNeural Network symmetries. One of the major difficulties with working with function parameters directly is that the same function can be represented with more than one parameter, i.e. there exists transformations that when applied to any parameter $( G , \\theta )$ , keep the represented function intact. Formally, an NN symmetry is induced by a set $\\Psi$ of transformations $\\psi : \\mathcal { G } \\times \\Theta  \\mathcal { G } \\times \\Theta .$ such that $u _ { G , \\theta } ( \\mathbf { x } ) \\ = \\ u _ { \\psi ( G , \\pmb { \\theta } ) } ( \\mathbf { x } ) , \\forall \\mathbf { x } \\in \\mathcal { X } , \\forall ( G , \\pmb { \\theta } ) \\in \\mathcal { G } \\times \\Theta$ . If for a pair parameters $( G , \\theta )$ , $( G ^ { \\prime } , \\pmb \\theta ^ { \\prime } )$ , $\\exists \\psi$ such that $( G , \\pmb \\theta ) = \\psi ( G ^ { \\prime } , \\pmb \\theta ^ { \\prime } )$ , we will call the two parameters equivalent and write $( G , \\pmb \\theta ) \\simeq ( G ^ { \\prime } , \\pmb \\theta ^ { \\prime } )$ . To appropriately represent a functional $\\mathfrak { F }$ , a hypothesis (metanetwork) $\\hat { f }$ should be invariant to transformations in $\\Psi$ $: { \\hat { f } } \\left( \\psi \\left( G , \\pmb { \\theta } \\right) \\right) = { \\hat { f } } ( G , \\pmb { \\theta } )$ . For operators, $\\hat { f }$ should be equivariant to transformations: $f \\left( \\psi \\left( G , \\pmb { \\theta } \\right) \\right) = \\psi \\left( f ( G , \\pmb { \\theta } ) \\right)$ , such that identical functions map to identical functions.\n\nPermutation symmetries (connectionist structure). For a fixed computational graph $G$ , perhaps the most well-known symmetry of FFNNs are those induced by hidden neuron permutations [28]. As far as metanetworks are concerned it is to date the only NN symmetry that has been accounted for - see Section 2. This symmetry implies that permuting hidden neurons (along with their biases and incoming and outgoing weights) within each layer preserves the NN function (regardless of the activation function). This reads:\n\n$$\n\\mathbf { W } _ { \\ell } ^ { \\prime } = \\mathbf { P } _ { \\ell } \\mathbf { W } _ { \\ell } \\mathbf { P } _ { \\ell - 1 } ^ { - 1 } , \\mathbf { \\Phi } _ { \\ell } ^ { \\prime } = \\mathbf { P } _ { \\ell } \\mathbf { b } _ { \\ell } \\Longrightarrow ( \\mathbf { W } _ { \\ell } ^ { \\prime } , \\mathbf { b } _ { \\ell } ^ { \\prime } ) _ { \\ell = 1 } ^ { L } = \\pmb \\theta ^ { \\prime } \\simeq \\pmb \\theta = ( \\mathbf { W } _ { \\ell } , \\mathbf { b } _ { \\ell } ) _ { \\ell = 1 } ^ { L } ,\n$$\n\nwhere $\\ell \\in \\{ 1 , \\ldots , L \\}$ , $\\mathbf { P } _ { 0 } = \\mathbf { P } _ { L } = \\mathbf { I }$ and $\\mathbf { P } _ { \\ell } \\in \\mathbb { R } ^ { d _ { \\ell } \\times d _ { \\ell } }$ are arbitrary permutation matrices. Observe that they are different for each layer, with the input and output neurons held fixed.\n\nGraph Metanetworks (GMNs). A recently introduced weight space architecture [33, 44], takes advantage of the permutation symmetries and treats FFNNs (among others, e.g. CNNs) as graphs, processing them with conventional GNNs. In particular, let $G = ( \\nu , \\mathcal { E } )$ be the computational graph, $i \\in \\mathcal { V }$ an arbitrary vertex in the graph (neuron) and $( i , j ) \\in \\mathcal { E }$ an arbitrary edge from vertex $j$ to vertex $i$ .4 Additionally, let $\\mathbf { x } _ { V } \\in \\mathbb { R } ^ { | \\nu | \\times d _ { v } }$ be the vertex features and $\\mathbf { x } _ { E } \\in \\mathbb { R } ^ { | \\mathcal { E } | \\times d _ { e } }$ the edge features (i.e. biases and weights resp. in a FFNN). The general form of a $T$ iteration (layer) GMN reads:\n\n$$\n\\begin{array} { r l } & { \\mathbf { h } _ { V } ^ { 0 } ( i ) = \\mathrm { I N I T } _ { V } \\left( \\mathbf { x } _ { V } \\left( i \\right) \\right) , \\quad \\mathbf { h } _ { E } ^ { 0 } ( i , j ) = \\mathrm { I N I T } _ { E } \\left( \\mathbf { x } _ { E } \\left( i , j \\right) \\right) } \\\\ & { \\mathbf { m } _ { V } ^ { t } ( i ) = \\bigoplus _ { \\substack { j \\in \\mathcal { N } ( i ) } } \\mathrm { M S G } _ { V } ^ { t } \\left( \\mathbf { h } _ { V } ^ { t - 1 } ( i ) , \\mathbf { h } _ { E } ^ { t - 1 } ( j ) , \\mathbf { h } _ { E } ^ { t - 1 } ( i , j ) \\right) } \\\\ & { \\mathbf { h } _ { V } ^ { t } ( i ) = \\mathrm { U P D } _ { V } ^ { t } \\left( \\mathbf { h } _ { V } ^ { t - 1 } ( i ) , \\mathbf { m } _ { V } ^ { t } ( i ) \\right) , \\quad \\mathbf { h } _ { E } ^ { t } ( i , j ) = \\mathrm { U P D } _ { E } ^ { t } \\left( \\mathbf { h } _ { V } ^ { t - 1 } ( i ) , \\mathbf { h } _ { V } ^ { t - 1 } ( j ) , \\mathbf { h } _ { E } ^ { t - 1 } ( i , j ) \\right) } \\\\ & { \\mathbf { h } _ { G } = \\mathrm { R E A D } \\Big ( \\big \\{ \\mathbf { h } _ { V } ^ { T } ( i ) \\big \\} _ { i \\in \\mathcal { V } } \\Big ) , \\qquad ( \\mathrm { I n ~ } _ { V } ^ { t - 1 } ( i ) , \\mathbf { h } _ { E } ^ { t - 1 } ( j ) = \\mathrm { I n ~ } _ { V } ^ { t - 1 } ( i ) . } \\end{array}\n$$\n\nwhere $\\mathbf { h } _ { V } ^ { t } , \\mathbf { h } _ { E } ^ { t }$ are vertex and edge representations at iteration $t$ and $\\mathbf { h } _ { G }$ is the overall graph (NN) representation. INIT, MSG, UPD are general function approximators (e.g. MLPs), while READ is a permutation invariant aggregator (e.g. DeepSets [81]). The above equations have appeared with several variations in the literature, e.g. in some cases the edge representations are not updated or the readout might involve edge representations as well. Another frequent strategy is to use positional encodings $\\mathbf { p } _ { V } , \\mathbf { p } _ { E }$ to break undesired symmetries. In FFNNs, Eq. (2) reveals that input and output vertices are not permutable, while vertices cannot be permuted across layers. Therefore, vertices (or edges) that are permutable share the same positional encoding (see Appendix A.1.2 for more details).\n\nRemark: Although, typically, the neighbourhood $\\mathcal { N } ( i )$ contains both incoming and outgoing edges, in Section 5 we will illustrate our method using only incoming edges: forward neighbourhood $\\mathcal { N } _ { \\mathrm { F W } } ( i ) = \\{ j \\in \\mathcal { V } |$ layer $( i ) - \\mathrm { l a y e r } ( j ) = 1 \\}$ and backward where layer $( i )$ gives the layer neuron $i$ belongs. Backward neighbourhoods $\\mathcal { N } _ { \\mathrm { B W } } ( i )$ are defined defined similarly. In Appendix A.2, we show a more elaborate bidirectional version of our method, with both neighbourhoods considered.\n\n# 4 Scaling symmetries in Feedforward Neural Networks\n\nScaling symmetries (activation functions). Intuitively, permutation symmetries stem from the graph structure of neural networks, or put differently, from the fact that hidden neurons do not possess any inherent ordering. Apart from the affine layers $\\mathbf { W } _ { \\ell }$ that give rise to the graph structure, it is frequently the case that activation functions $\\sigma _ { \\ell }$ have inherent symmetries that are bestowed to the NN.\n\nLet us dive into certain illustrative examples: for the ReLU activation $\\sigma ( x ) = \\operatorname* { m a x } ( x , 0 )$ it holds that $\\sigma ( a x ) = \\operatorname* { m a x } ( a x , 0 ) = a \\operatorname* { m a x } ( x , 0 )$ , $\\forall a > 0$ . For the tanh and sine activations $\\sigma ( x ) = \\operatorname { t a n h } ( x )$ , $\\sigma ( x ) = \\sin ( x )$ respectively, it holds that $\\sigma ( a x ) = a \\sigma ( x )$ , $\\forall a \\in \\{ - 1 , 1 \\}$ . In a slightly more complex example, polynomial activations $\\sigma ( x ) = x ^ { k }$ , we have $\\sigma ( a x ) = a ^ { d } \\sigma ( x )$ , i.e. the multiplier differs between input and output. In general, we will be talking about scaling symmetries whenever there exist pairs $( a , b )$ for which it holds that $\\sigma ( a x ) = b \\sigma ( \\bar { x } )$ . To see how such properties affect NN symmetries, let us focus on FFNNs (see Appendix A.3 for CNNs): for a neuron $i$ (we omit layer subscripts) we have $\\sigma \\big ( a \\mathbf { W } ( i , : ) \\mathbf { x } + a \\mathbf { b } ( i ) \\big ) \\ \\hat { = } \\ b \\sigma \\big ( \\mathbf { W } ( i , : ) \\mathbf { x } + \\mathbf { b } ( i ) \\big ) .$ , i.e. multiplying its bias and all incoming weights with a constant a results in scaling its output with a corresponding constant $b$ Generalising this to linear transformations, we may ask the following: which are the pairs of matrices $( \\mathbf { A } , \\mathbf { B } )$ for which we have $\\sigma ( \\mathbf { A W x } + \\mathbf { A b } ) = \\mathbf { B } \\acute { \\sigma } ( \\mathbf { W x } + \\mathbf { b } ) \\acute { \\ ? }$ Godfrey et al. [25] provide an answer for any activation that respects certain conditions. We restate here their most important results:\n\nProposition 4.1 (Lemma 3.1. and Theorem E.14 from [25]). Consider an activation function $\\sigma : \\mathbb { R }  \\mathbb { R }$ . Under mild conditions,5 the following hold:\n\n• For any $d \\in \\mathbb { N } ^ { + }$ , there exists a (non-empty) group of invertible matrices defined as: $I _ { \\sigma , d } = \\mathbf { \\Gamma } \\mathbf { \\{ A \\in \\mathbb { R } ^ { d \\times d } } $ : invertible $| \\hat { \\exists } \\mathbf { B } \\in \\mathbf { \\hat { \\mathbb { R } } } ^ { \\hat { d } \\times \\hat { d } }$ invertible, such that: $\\sigma ( \\mathbf { A x } ) = \\mathbf { \\bar { B } } \\sigma ( \\mathbf { x } ) \\}$ (intertwiner group), and a mapping function $\\phi _ { \\sigma , d }$ such that $\\mathbf { B } = \\phi _ { \\sigma , d } ( \\mathbf { A } )$ . • Every $\\mathbf { A } \\in I _ { \\sigma , d }$ is of the form PQ, where $\\mathbf { P }$ : permutation matrix and $\\mathbf { Q } = d i a g \\left( q _ { 1 } , \\dots q _ { d } \\right)$ diagonal, with $q _ { i } \\in D _ { \\sigma } = \\{ a \\in \\mathbb { R } \\backslash \\{ 0 \\} \\mid \\sigma ( a x ) = \\phi _ { \\sigma , 1 } ( a ) \\sigma ( x ) \\}$ : the $\\jmath$ -dimensional group, and $\\phi _ { \\sigma , d } ( \\mathbf { A } ) = \\mathbf { P } d i a g \\big ( \\phi _ { \\sigma , 1 } ( q _ { 1 } ) , \\dots \\phi _ { \\sigma , 1 } ( q _ { d } ) \\big )$ .\n\nThis is a powerful result that completely answers the question above for most practical activation functions. Importantly, not only does it recover permutation symmetries, but also reveals symmetries to diagonal matrix groups, which can be identified by solely examining $\\phi _ { \\sigma , 1 }$ , i.e. the one-dimensional case and the set $D _ { \\sigma }$ (easily proved to be a group) we have already discussed in our examples above.\n\nUsing this statement, Godfrey et al. [25] characterised various activation functions (or recovered existing results), e.g. ReLU: $I _ { \\sigma , d }$ contains generalised permutation matrices with positive entries of the form $\\mathbf { P Q }$ , $\\mathbf { Q } = \\mathrm { d i a g } ( q _ { 1 } , \\dots , q _ { d } )$ , $q _ { i } > 0$ and $\\phi _ { \\sigma , d } ^ { - } ( \\mathbf { P Q } ) = \\mathbf { P Q }$ [56]. Additionally, here we characterise the intertwiner group of sine (used in the popular SIREN architecture [70] for INRs). Not surprisingly, it has the same intertwiner group with tanh [11, 21] (we also recover this here using Proposition 4.1). Formally, (proof in Appendix A.7.1):\n\nCorollary 4.2. Hyperbolic tangent $\\sigma ( x ) = \\operatorname { t a n h } ( x )$ and sine activation $\\sigma ( x ) = \\sin ( \\omega x )$ , satisfy the conditions of Proposition 4.1, when (for the latter) $\\omega \\neq k \\pi , k \\in \\mathbb { Z }$ . Additionally, $I _ { \\sigma , d }$ contains signed permutation matrices of the form $\\mathbf { P Q } ,$ , with $\\mathbf { Q } = \\mathrm { d i a g } ( q _ { 1 } , \\dots , q _ { d } )$ , $q _ { i } = \\pm 1$ and $\\dot { \\phi } _ { \\sigma _ { d } } ( \\mathbf { P Q } ) = \\mathbf { \\bar { P Q } }$ .\n\nIt is straightforward to see that the symmetries of Proposition 4.1, induce equivalent parameterisations for FNNNs. In particular, it follows directly from Proposition 3.4. in [25], that for activation functions $\\sigma _ { \\ell }$ satisfying the conditions of Proposition 4.1 and when $\\phi _ { \\sigma , \\ell } ( \\mathbf { Q } ) = \\mathbf { Q }$ , we have that:\n\n$$\n\\mathbf { W } _ { \\ell } ^ { \\prime } = \\mathbf { P } _ { \\ell } \\mathbf { Q } _ { \\ell } \\mathbf { W } _ { \\ell } \\mathbf { Q } _ { \\ell - 1 } ^ { - 1 } \\mathbf { P } _ { \\ell - 1 } ^ { - 1 } , \\ \\mathbf { b } _ { \\ell } ^ { \\prime } = \\mathbf { P } _ { \\ell } \\mathbf { Q } _ { \\ell } \\mathbf { b } _ { \\ell } \\implies ( \\mathbf { W } _ { \\ell } ^ { \\prime } , \\mathbf { b } _ { \\ell } ^ { \\prime } ) _ { \\ell = 1 } ^ { L } = \\pmb { \\theta } ^ { \\prime } \\simeq \\pmb { \\theta } = ( \\mathbf { W } _ { \\ell } , \\mathbf { b } _ { \\ell } ) _ { \\ell = 1 } ^ { L } ,\n$$\n\n# 5 Scale Equivariant Graph MetaNetworks\n\nAs previously mentioned, the metanetworks operating on weight spaces that have been proposed so far, either do not take any symmetries into account or are invariant/equivariant to permutations alone as dictated by Eq. (2). In the following section, we introduce an architecture invariant/equivariant to permutations and scalings, adhering to Eq. (3). An important motivation for this is that in various setups, these are the only function-preserving symmetries, i.e. for a fixed graph $u _ { G , \\theta } = u _ { G , \\theta ^ { \\prime } } \\Rightarrow$ $( \\theta , \\theta ^ { \\prime } )$ satisfy Eq. (3) - e.g. see [21] for the conditions for tanh and [61, 26] for ReLU.\n\nMain idea. Our framework is similar in spirit to most works on equivariant and invariant NNs [10]. In particular, we build equivariant GMNs that will preserve both symmetries at vertex- and edge-level, i.e. vertex representations will have the same symmetries with the biases and edge representations with the weights. To see this, suppose two parameter vectors are equivalent according to Eq. (3). Then, the hidden neurons representations - the discussion on input/output neurons is postponed until Appendix A.1.4 - should respect the following (the GMN iteration $t$ is omitted to simplify notation):\n\n$$\n\\mathbf { h } _ { V } ^ { \\prime } ( i ) = q _ { \\ell } \\left( \\pi _ { \\ell } \\left( i \\right) \\right) \\mathbf { h } _ { V } \\left( \\pi _ { \\ell } \\left( i \\right) \\right) , \\quad \\ell = \\mathrm { l a y e r } \\left( i \\right) \\in \\left\\{ 1 , \\ldots , L - 1 \\right\\}\n$$\n\n$$\n\\mathbf { h } _ { E } ^ { \\prime } ( i , j ) = q _ { \\ell } \\left( \\pi _ { \\ell } \\left( i \\right) \\right) \\mathbf { h } _ { E } \\left( \\pi _ { \\ell } \\left( i \\right) , \\pi _ { \\ell - 1 } \\left( j \\right) \\right) q _ { \\ell - 1 } ^ { - 1 } \\left( \\pi _ { \\ell - 1 } \\left( j \\right) \\right) , \\ell = \\mathrm { l a y e r } \\left( i \\right) \\in \\{ 2 , \\ldots , L - 1 \\} ,\n$$\n\nwhere $\\pi _ { \\ell } : \\mathcal { V } _ { \\ell }  \\mathcal { V } _ { \\ell }$ permutes the vertices of layer $\\ell$ (denoted with $\\nu _ { \\ell }$ ) according to $\\mathbf { P } _ { \\ell }$ and $q _ { \\ell } : \\mathcal { V } _ { \\ell } \\to \\mathbb { R } \\setminus \\{ 0 \\}$ scales the vertex representations of layer $\\ell$ according to $\\mathbf { Q } _ { \\ell }$ . We will refer to the latter as forward scaling in Eq. (4) and bidirectonal scaling in Eq. (5). To approximate operators (equivariance), we compose multiple equivariant GMN layers/iterations and in the end, project vertex/edge representations to the original NN weight space, while to approximate functionals (invariance), we compose a final invariant one in the end summarising the input to a scalar/vector.\n\nTo ease exposition, we will first discuss our approach w.r.t. vertex representations. Assume that vertex representation symmetries are preserved by the initialisation of the MPNN - Eq. (Init) - and so are edge representation symmetries for all MPNN layers. Therefore, we can only focus on the message passing and vertex update steps - Eq. (Msg) and Eq. (Upd). Additionally, let us first focus on hidden neurons and assume only forward neighbourhoods. The following challenges arise:\n\nChallenge 1 - Scale Invariance / Equivariance. First off, the message and the update function $\\mathbf { M S G } _ { V }$ , $\\mathrm { U P D } _ { V }$ should be equivariant to scaling - in this case to the forward scaling using the multiplier of the central vertex $q _ { \\ell } ( i )$ . Additionally, the readout READ, apart from being permutation invariant should also be invariant to the different scalar multipliers of each vertex . Dealing with this requires devising functions of the following form:\n\n$$\ng _ { i } \\left( q _ { 1 } \\mathbf { x } _ { 1 } , \\ldots , q _ { n } \\mathbf { x } _ { n } \\right) = q _ { i } g _ { i } ( \\mathbf { x } _ { 1 } , \\ldots , \\mathbf { x } _ { n } ) , \\forall q _ { i } \\in D _ { i } , i \\in \\{ 1 , \\ldots , n \\}\n$$\n\nwhere $D _ { i }$ a 1-dimensional scaling group as defined in Proposition 4.1. Common examples are those discussed in Section 4, e.g. $D _ { i } = \\{ 1 , - 1 \\}$ or $D _ { i } = \\mathbb { R } ^ { + }$ . The first case, i.e. sign symmetries, has been discussed in recent work [43, 40]. Here we generalise their architecture into arbitrary scaling groups. In specific, Scale Equivariant networks follow the methodology of [40], i.e. they are compositions of multiple linear transformations multiplied elementwise with the output of Scale Invariant functions:\n\n(Scale Inv. Net)\n\n$$\n\\begin{array} { r l } & { \\mathsf { S c a l e l n v } ^ { k } ( \\mathbf { X } ) = \\rho ^ { k } \\left( \\tilde { \\mathbf { x } } _ { 1 } , \\ldots , \\tilde { \\mathbf { x } } _ { n } \\right) , } \\\\ & { \\mathsf { S c a l e E q } = \\mathsf { f } ^ { K } \\circ \\cdots \\circ \\mathsf { f } ^ { 1 } , \\mathsf { f } ^ { k } ( \\mathbf { X } ) = \\left( \\mathbf { { \\Gamma } } _ { 1 } ^ { k } \\mathbf { x } _ { 1 } , \\ldots , \\mathbf { { \\Gamma } } _ { n } ^ { k } \\mathbf { x } _ { n } \\right) \\odot \\mathsf { S c a l e l n v } ^ { k } ( \\mathbf { X } ) , } \\end{array}\n$$\n\n(Scale Equiv. Net)\n\nwhere $\\begin{array} { r } { \\rho ^ { k } : \\prod _ { i = 1 } ^ { n } \\chi _ { i } \\to \\mathbb { R } ^ { \\sum _ { i = 1 } ^ { n } d _ { i } ^ { k } } } \\end{array}$ universal approximators (e.g. MLPs), $\\Gamma _ { i } ^ { k } : \\mathcal { X } _ { i }  \\mathbb { R } ^ { d _ { i } ^ { k } }$ linear transforms  or each of the $k$ invariant/equivariant layers resp - in practice, we observed experimentally that a single layer $K = 1$ was sufficient), and $\\tilde { \\mathbf { x } } _ { i }$ are explained in detail in the following paragraph.\n\nCentral to our method is defining a way to achieve invariance. One option is canonicalisation, i.e. by defining a function canon : $\\chi  \\chi$ that maps all equivalent vectors x to a representative (obviously non-equivalent vectors have different representatives): Iff $\\mathbf { x } \\simeq \\mathbf { y } \\in \\mathcal { X }$ , then $\\mathbf { x } \\simeq \\mathsf { c a n o n } ( \\mathbf { x } ) =$ canon $( \\mathbf { y } )$ . In certain cases, these are easy to define and differentiable almost everywhere, for example for positive scaling: canon(x) = x .6 For sign symmetries, this is not as straightforward: in 1-dimension one can use $| x |$ , but for arbitrary dimensions, a more complex procedure is required, as recently discussed in [49, 48]. Since the group is small - two elements - one can use symmetrisation instead [79], as done by Lim et al. [41]: $\\begin{array} { r } { \\mathsf { s y m m } ( \\mathbf { x } ) = \\sum _ { \\mathbf { y } : \\mathbf { y } \\simeq \\mathbf { x } } \\mathbf { M L P } ( \\mathbf { y } ) } \\end{array}$ , i.e. for sign: $\\mathsf { s y m m } ( \\mathbf { x } ) =$ $\\mathbf { M L P ( x ) + M L P ( - x ) }$ . Therefore, we define: $\\mathsf { S c a l e l n v } ^ { k } ( \\mathbf { X } ) = \\rho ^ { k } \\left( \\tilde { \\mathbf { x } } _ { 1 } , \\ldots , \\tilde { \\mathbf { x } } _ { n } \\right)$ , with $\\tilde { \\mathbf { x } } _ { i } = \\mathsf { c a n o n } ( \\mathbf { x } _ { i } )$ or $\\tilde { \\mathbf { x } } _ { i } = \\mathsf { s y m m } ( \\mathbf { x } _ { i } )$ . Importantly, it is known that both cases allow for universality, see [8, 29] and [79, 62] respectively.\n\nChallenge 2 - Rescaling: Different multipliers. Scale equivariance alone is not sufficient, since the input vectors of the message function $\\mathbf { M S G } _ { V }$ are scaled by different multipliers - central vertex: $q _ { \\ell } ( i )$ , neighbour: $q _ { \\ell - 1 } ( j )$ , edge: $q _ { \\ell } ( i ) q _ { \\ell - 1 } ^ { - 1 } ( j )$ , while its output should be scaled differently as well - $q _ { \\ell } ( i )$ . We refer to this problem as rescaling. Dealing with Challenge 2 requires functions of the form:\n\n$$\ng { \\big ( } q _ { 1 } \\mathbf { x } _ { 1 } , \\ldots q _ { n } \\mathbf { x } _ { n } { \\big ) } = g ( \\mathbf { x } _ { 1 } , \\ldots \\mathbf { x } _ { n } ) \\prod _ { i = 1 } ^ { n } q _ { i } , \\forall q _ { i } \\in D _ { i } .\n$$\n\nWe call these functions rescale equivariant. Note, that this is an unusual symmetry in equivariant NN design. Our approach is based on the observation that any $n$ -other monomial containing variables from all vectors $\\mathbf { x } _ { i }$ is rescale-equivariant. Collecting all these monomials into a single representation is precisely the outer product $\\mathbf { X } _ { n } = \\mathbf { x } _ { 1 } \\otimes \\cdots \\otimes \\mathbf { x } _ { n }$ , where $\\begin{array} { r } { \\mathbf { X } _ { n } ( j _ { 1 } , \\dots , j _ { n } ) = \\prod _ { i = 1 } ^ { n } \\mathbf { \\bar { x } } _ { i } ( j _ { i } ) } \\end{array}$ . Therefore, the general form of our proposed Rescale Equivariant Network is as follows:\n\n$$\n{ \\mathsf { R e S c a l e E q } } ( \\mathbf { x } _ { 1 } , \\ldots \\mathbf { x } _ { n } ) = { \\mathsf { S c a l e E q } } ( { \\mathsf { v e c } } ( \\mathbf { X } _ { n } ) ) .\n$$\n\n(ReScale Equiv. Net)\n\nIn practice, given that the size of $\\pmb { \\mathsf { X } } _ { n }$ grows polynomially with $n$ , we resort to a more computationally friendly subcase, i.e. hadamard products, i.e. ReScaleEq $( { \\bf x } _ { 1 } , . . . { \\bf x } _ { n } ) = \\odot _ { i = 1 } ^ { n } { \\bf \\Gamma } _ { i } { \\bf x } _ { i }$ , $\\Gamma _ { i } : \\mathcal { X } _ { i }  \\mathbb { R } ^ { \\bar { d } }$ . Contrary to the original formulation, the latter is linear (lack of multiplication with an invariant layer).\n\nScale Equivariant Message Passing. We are now ready to define our message-passing scheme. Starting with the message function, we require each message vector $\\mathbf { m } _ { V } ( i )$ to have the same symmetries as the central vertex $i$ . Given the scaling symmetries of the neighbour and the edge, for forward neighbourhoods, this reads: $\\mathbf { M S G } _ { V }$ $\\left( q _ { x } { \\bf x } , \\breve { q } _ { y } { \\bf y } , q _ { x } q _ { y } ^ { - 1 } { \\bf e } \\right) = q _ { x } { \\bf M } { \\bf S } \\breve { \\bf G _ { V } } \\left( { \\bf x } , { \\bf y } , { \\bf e } \\right) $ . In this case, we opt to eliminate $q _ { y }$ by multiplication as follows:\n\n$$\n\\mathtt { M S G } _ { V } \\left( \\mathbf { x } , \\mathbf { y } , \\mathbf { e } \\right) = \\mathsf { S c a l e E q } \\left( \\left[ \\mathbf { x } , \\mathsf { R e S c a l e E q } \\left( \\mathbf { y } , \\mathbf { e } \\right) \\right] \\right) ,\n$$\n\nwhere $[ \\cdot , \\cdot ]$ denotes concatenation, ReScaleEq $( q _ { y } { \\bf y } , q _ { x } q _ { y } ^ { - 1 } { \\bf e } ) = q _ { x } \\mathsf { R e } { \\mathsf { S c a l e } } \\mathsf { E q } ( { \\bf y } , { \\bf e } )$ . In our experiments, we used only $\\mathbf { y }$ and $\\mathbf { e }$ , since we did not observe significant performance gains by including the central vertex x. Now, the update function is straightforward to implement since it receives vectors with the same symmetry, i.e. it should hold that: $\\mathrm { U P D } _ { V } ( q _ { x } \\mathbf { x } , q _ { x } \\mathbf { \\bar { m } } ) = q _ { x } \\mathrm { U P D } _ { V } ( \\mathbf { x } , \\mathbf { m } )$ which is straightforward to implement with a scale equivariant network, after concatenating $\\mathbf { x }$ and $\\mathbf { m }$ :\n\n$$\n\\mathsf { U P D } _ { V } \\left( \\mathbf { x } , \\mathbf { m } \\right) = \\mathsf { S c a l e E q } \\left( \\left[ \\mathbf { x } , \\mathbf { m } \\right] \\right) .\n$$\n\nFinally, to summarise our graph into a single scalar/vector we require a scale and permutationinvariant readout. The former is once more achieved using canonicalised/symmetrised versions of the vertex representations of hidden neurons, while the latter using a DeepSets architecture as usual:\n\n$$\n\\mathrm { R E A D } _ { V } \\left( { \\bf X } \\right) : = \\mathrm { D e e p S e t s } \\left( \\tilde { \\bf x } _ { 1 } , \\ldots , \\tilde { \\bf x } _ { n } \\right) , \\quad \\tilde { \\bf x } _ { i } = \\mathrm { c a n o n } _ { i } \\left( { \\bf x } _ { i } \\right) \\mathrm { o r } \\tilde { \\bf x } _ { i } = \\mathrm { s y m m } _ { i } \\left( { \\bf x } _ { i } \\right)\n$$\n\nIn Appendix A.1, we show how to handle symmetries in the rest of the architecture components (i.e. initialisation, positional encodings, edge updates and i/o vertices) and provide an extension of our method to bidirectional message-passing (Appendix A.2), which includes backward neighbourhoods.\n\nExpressive power. Throughout this section, we discussed only scaling symmetries and not permutation symmetries. However, it is straightforward to conclude that ScaleGMN is also permutation equivariant/invariant, since it is a subcase of GMNs; if one uses universal approximators in their message/update functions (MLP), our corresponding functions will be expressible by this architecture, which was proved to be permutation equivariant in [44]. Although this implies that GMN can express ScaleGMN, this is expected since $\\mathbf { P Q }$ symmetries are more restrictive than just $\\mathbf { P }$ . Note that these symmetries are always present in FFNNs, and thus it is desired to explicitly model them, to introduce a more powerful inductive bias. Formally, on symmetry preservation (proved in Appendix A.7.2):\n\nTable 1: INR classification on MNIST, F-MNIST, CIFAR-10 and Aug. CIFAR-10. We train all methods on 3 seeds and report the mean and std. (\\*) denotes the baselines trained by us and we report the rest as in the corresponding papers. Colours denote First, Second and Third.   \n\n<html><body><table><tr><td>Method</td><td>MNIST</td><td>F-MNIST</td><td>CIFAR-10</td><td>Augmented CIFAR-10</td></tr><tr><td>MLP</td><td>17.55 ± 0.01</td><td>19.91 ± 0.47</td><td>11.38 ± 0.34*</td><td>16.90 ± 0.25</td></tr><tr><td>Inr2Vec [47]</td><td>23.69 ±0.10</td><td>22.33 ± 0.41</td><td></td><td></td></tr><tr><td>DWS [54]</td><td>85.71 ± 0.57</td><td>67.06 ± 0.29</td><td>34.45 ±0.42</td><td>41.27 ± 0.026</td></tr><tr><td>NFNNp [85]</td><td>78.50±0.23*</td><td>68.19±0.28*</td><td>33.41 ±0.01*</td><td>46.60 ± 0.07</td></tr><tr><td>NFNHNP [85]</td><td>79.11±0.84*</td><td>68.94 ± 0.64*</td><td>28.64±0.07*</td><td>44.10 ± 0.47</td></tr><tr><td>NG-GNN [33]</td><td>91.40 ± 0.60</td><td>68.00 ±0.20</td><td>36.04 ±0.44*</td><td>45.70±0.20*</td></tr><tr><td>ScaleGMN (Ours)</td><td>96.57 ± 0.10</td><td>80.46 ± 0.32</td><td>36.43 ± 0.41</td><td>56.62 ± 0.24</td></tr><tr><td>ScaleGMN-B (Ours)</td><td>96.59 ± 0.24</td><td>80.78 ± 0.16</td><td>38.82 ± 0.10</td><td>56.95 ± 0.57</td></tr></table></body></html>\n\nTable 2: Generalization pred.: Kendall- $\\tau$ on subsets of SmallCNN Zoo w/ ReLU/Tanh activations.   \n\n<html><body><table><tr><td>Method</td><td>CIFAR-10-GS ReLU</td><td>SVHN-GS ReLU</td><td>CIFAR-10-GS Tanh</td><td>SVHN-GS Tanh</td><td>CIFAR-10-GS both act.</td></tr><tr><td>StatNN [74]</td><td>0.9140 ± 0.001</td><td>0.8463±0.004</td><td>0.9140 ± 0.000</td><td>0.8440 ± 0.001</td><td>0.915 ±0.002</td></tr><tr><td>NFNNp [85]</td><td>0.9190 ± 0.010</td><td>0.8586 ±0.003</td><td>0.9251 ± 0.001</td><td>0.8580 ±0.004</td><td>0.922 ± 0.001</td></tr><tr><td>NFNHNP [85]</td><td>0.9270 ± 0.001</td><td>0.8636 ± 0.002</td><td>0.9339 ±0.000</td><td>0.8586 ±0.004</td><td>0.934 ± 0.001</td></tr><tr><td>NG-GNN [33]</td><td>0.9010 ± 0.060</td><td>0.8549 ±0.002</td><td>0.9340 ± 0.001</td><td>0.8620 ± 0.003</td><td>0.931 ± 0.002</td></tr><tr><td>ScaleGMN (Ours)</td><td>0.9276 ± 0.002</td><td>0.8689 ± 0.003</td><td>0.9418± 0.005</td><td>0.8736 ± 0.003</td><td>0.941 ± 0.006</td></tr><tr><td>ScaleGMN-B(Ours)</td><td>0.9282 ± 0.003</td><td>0.8651 ± 0.001</td><td>0.9425 ± 0.004</td><td>0.8655 ± 0.004</td><td>0.941 ± 0.000</td></tr></table></body></html>\n\nProposition 5.1. ScaleGMN is permutation & scale equivariant. Additionally, ScaleGMN is permutation & scale invariant when using a readout with the same symmetries.\n\nFinally, we analysed the ability of ScaleGMN to evaluate the input FFNN and its gradients, i.e. simulate the forward and the backward pass of an input FFNN. To see why this is a desired inductive bias, recall that a functional/operator can often be written as a function of input-output pairs (e.g. via an integral on the entire domain) or of the input function’s derivatives (e.g. via a differential equation). By simulating the FFNN, one can reconstruct function evaluations and gradients, which an additional module can later combine. Formally (proof and precise statement in Appendix A.7.2):\n\nTheorem 5.2. Consider an FFNN as per Eq. (1) with activation functions respecting the conditions of Proposition 4.1. Assume a Bidirectional-ScaleGMN with sufficiently expressive message and vertex update functions. Then, ScaleGMN can simulate both the forward and the backward pass of the FFNN for arbitrary inputs, when ScaleGMN’s iterations (depth) are L and 2L respectively.\n\n# 6 Experiments\n\nDatasets. We evaluate ScaleGMN on datasets containing NNs with three popular activation functions: sine, ReLU and tanh. The former is prevalent in INRs, which in turn are the most appropriate testbed for GMNs. We experiment with the tasks of (1) INR classification (invariant task), i.e. classifying functions (signals) represented as INRs and (2) INR editing (equivariant), i.e. transforming those functions. Additionally, ReLU and tanh are common in neural classifiers/regressors. A popular GMN benchmark for those is (3) generalisation prediction (invariant), i.e. predicting a classifier’s test accuracy. Here, classifiers instead of FFNNs are typically CNNs (for computer vision tasks) and to this end, we extend our method to the latter in Appendix A.3. We use existing datasets that have been constructed by the authors of relevant methods, are publicly released and have been repeatedly used in the literature (8 datasets in total, 4 for each task). Finally, we follow established protocols: we perform a hyperparameter search and use the best-achieved validation metric throughout training to select our final model. We report the test metric on the iteration where the best validation is achieved.\n\nTable 3: Dilating MNIST INRs. MSE between the reconstructed and ground-truth image.   \n\n<html><body><table><tr><td>Method</td><td>MSE in 10-2</td></tr><tr><td>MLP DWS [54]</td><td>5.35 ±0.00 2.58 ±0.00</td></tr><tr><td>NFNNp [85]</td><td>2.55 ±0.00</td></tr><tr><td>NFNHNP [85] NG-GNN-0[33]</td><td>2.65 ±0.01</td></tr><tr><td>NG-GNN-64 [33]</td><td>2.38 ±0.02 2.06 ± 0.01</td></tr><tr><td>ScaleGMN (Ours) ScaleGMN-B(Ours)</td><td>2.56 ± 0.03 1.89 ± 0.00</td></tr></table></body></html>\n\nTable 4: Ablation: Permutation equivariant models $^ +$ scaling augmentations.   \n\n<html><body><table><tr><td>Method</td><td>F-MNIST</td><td>CIFAR-10-GS ReLU</td></tr><tr><td>DWS [54]</td><td>67.06 ±0.29</td></tr><tr><td>71.42 ±0.45</td><td></td></tr><tr><td>DWS [54] + aug. 68.19±0.28</td><td>0.9190 ± 0.010</td></tr><tr><td>NFNNp [85] NFNNP [85] + aug.</td><td>70.34 ± 0.13</td></tr><tr><td>NFNHNP [85] 68.94 ±0.64</td><td>0.8474±0.01 0.9270± 0.001</td></tr><tr><td>NFNHNP [85] + aug. 70.24±0.47</td><td>0.8906 ±0.01</td></tr><tr><td>NG-GNN [33] 68.0±0.20</td><td>0.9010 ± 0.060</td></tr><tr><td>NG-GNN [33] + aug. 72.01 ± 1.4</td><td>0.8855 ±0.02</td></tr><tr><td>ScaleGMN (Ours)</td><td>0.9276 ±0.002</td></tr><tr><td>ScaleGMN-B (Ours)</td><td>80.46 ±0.32 80.78 ±0.16 0.9282 ±0.003</td></tr></table></body></html>\n\nBaselines. DWS [54] is a non-local metanetwork that uses all linear permutation equivariant/invariant layers interleaved with non-linearities. $\\mathrm { \\Delta N F N _ { H N P } }$ [85] is mathematically equivalent to DWS, while ${ \\mathrm { N F N } } _ { \\mathrm { N P } }$ [85] makes stronger symmetry assumptions in favour of parameter efficiency. The two variants are also designed to process CNNs contrary to DWS. NG-GNN [33] converts each input NN to a graph (similar to our work) and employs a GNN (in specific PNA [15]). Importantly, the last three methods transform the input parameters with random Fourier features while all methods perform input normalisations to improve performance and facilitate training. These tricks are in general not equivariant to scaling and were unnecessary in the ScaleGMN experiments (more details in Appendix A.4). On INR classification we include a naive MLP on the flattened parameter vector and Inr2Vec [47], a task-specific non-equivariant method. For generalization prediction, we also compare to StatNN [74], which predicts NN accuracy based on statistical features of its weights/biases.\n\nINR classification. We design a ScaleGMN with permutation & sign equivariant components (and invariant readout). Additionally, we experiment with the bidirectional version denoted as ScaleGMN$B$ . We use the following datasets of increasing difficulty: MNIST INR, $F$ -MNIST INR (grayscale images) and CIFAR10-INR, Aug. CIFAR10-INR.: INRs representing images from the MNIST [38], FashionMNIST [76] and CIFAR [35] datasets resp. and an augmented version of CIFAR10-INR containing 20 different INRs for each image, trained from different initialisations (often called views). The reported metric is test accuracy. Note that [85, 86] use only the augmented dataset and hence we rerun all baselines for the original version. All input NNs correspond to functions $u _ { G , \\pmb \\theta } : \\mathbb { R } ^ { 2 }  \\mathbb { R } ^ { c }$ , i.e. pixel coordinate to GS/RGB value. Further implementation details can be found in Appendix A.4.1. As shown in Table 1, ScaleGMN consistently outperforms all baselines in all datasets considered, with performance improvements compared to the state-of-the-art ranging from approx. $3 \\%$ (CIFAR-10) to $13 \\%$ (F-MNIST). While previous methods often resort to additional engineering strategies, such as probe features and advanced architectures [33] or extra training samples [85] to boost performance, in our case this was possible using vanilla ScaleGMNs. For example, in MNIST and F-MNIST, NG-GNN achieves $9 4 . 7 \\pm 0 . 3 \\$ and $7 4 . 2 \\pm 0 . 4$ with 64 probe features which is apprpx. $2 \\%$ and $6 \\%$ below our best performance. The corresponding results for NG-T [33] (transformer) were $9 7 . 3 \\pm 0 . 2 \\$ and $7 4 . 8 \\pm 0 . 9$ , still on par or approx. $6 \\%$ below ScaleGMN. Note that all the above are orthogonal to our work and can be used to further improve performance.\n\nPredicting CNN Generalization from weights. As in prior works. we consider datasets of image classifiers and measure predictive performance using Kendall’s $\\tau$ [30]. We select the two datasets used in [85], namely CIFAR-10-GS and SVHN-GS originally from Small CNN Zoo [74]. These contain CNNs with ReLU or tanh activations, which exhibit scale and sign symmetries respectively. To assess the performance of our method (1) on each activation function individually and (2) on a dataset with heterogeneous activation functions we discern distinct paths for this experiment. In the first case, we split each dataset into two subsets each containing the same activation and evaluate all baselines. As shown in Table 2, once again ScaleGMN outperforms all the baselines in all the examined datasets. This highlights the ability of our method to be used across different activation functions and architectures. Performance improvements here are less prominent due to the hardness of the task, a phenomenon also observed in the comparisons between prior works. Note that in this case, additional symmetries arise by the softmax classifier [36], which are currently not accounted for by none of the methods. We additionally evaluate our method on heterogeneous activation functions.\n\nIn principle, our method does not impose limitations regarding the homogeneity of the activation functions of the input NNs - all one needs is to have a different canonicalisation/symmetrisation module for each activation. Experiments on CIFAR-10-GS show that ScaleGMN yields superior performance compared to the baselines, significantly exceeding the performance of the next-best model. Further implementation details can be found in Appendix A.4.2.\n\nINR editing. Here, our goal is to transform the weights of the input NN, to modify the underlying signal to a new one. In this case, our method should be equivariant to the permutation and scaling symmetries, such that every pair of equivalent input NNs is mapped to a pair of equivalent output NNs. Hence, we employ a ScaleGMN similar to the above experiments but omit the invariant readout layer. Following [33], we evaluate our method on the MNIST dataset and train our model to dilate the encoded MNIST digits. Further implementation details can be found in Appendix A.4.3. As shown in Table 3, our bidirectional ScaleGMN-B achieves an MSE test loss $( \\mathrm { \\dot { 1 } \\dot { 0 } ^ { - 2 } } )$ equal to 1.891, surpassing all permutation equivariant baselines. Notably, our method also outperforms the NG-GNN [33] baseline that uses 64 probe features. Additionally, our forward variant, ScaleGMN, performs on par with the previous permutation equivariant baselines with an MSE loss of 2.56. Note that the performance gap between the forward and the bidirectional model is probably expected for equivariant tasks: here we are required to compute representations for every node of the graph, yet in the forward variant, the earlier the layer of the node, the smaller the amount of information it receives. This observation corroborates the design choices of the baselines, which utilize either bidirectional mechanisms (NG-GNN [33]) or non-local operations (NFN [85]).\n\nAblation study: Scaling data augmentations. We baseline our method with permutation equivariant methods trained with scaling augmentations: For every training datapoint, at each training iteration, we sample a diagonal scaling matrix for every hidden layer of the input NN and multiply it with the weights/bias matrices as per Eq. (3) (omitting the permutation matrices). We sample the elements of the matrices independently as follows: Sign symmetry: Bernoulli distribution with probability 0.5. Positive scaling: Exponential distribution where the coefficient $\\lambda$ is a hyperparameter that we tune on the validation set. Observe here that designing augmentations in the latter case is a particularly challenging task since we have to sample from a continuous and unbounded distribution. Our choice of the exponential was done by consulting the norm plots where in some cases the histogram resembles an exponential Fig. 1. Nevertheless, regardless of the distribution choice we cannot guarantee that the augmentations will be sufficient to achieve (approximate) scaling equivariance, due to the lack of upper bound. We evaluate on the F-MNIST dataset for the sign symmetry and on the CIFAR-10-GS-ReLU for the positive scale. As shown in Table 4, regarding the former, augmenting the training set leads consistently to better results when compared to the original baselines. None of these methods however achieved results on par with ScaleGMN and ScaleGMN-B. On the other hand, we were unable to even surpass the original baselines regarding the latter task. This indicates that designing an effective positive scaling augmentation might be a non-trivial task.\n\nLimitations. A limitation of our work is that it is currently designed for FFNNs and CNNs and does not cover other layers that either modify the computational graph (e.g. skip connections) or introduce additional symmetries (e.g. softmax and normalisation layers). In both cases, in future work, we plan to characterise scaling symmetries (certain steps were made in [25]) and modify ScaleGMN for general computational graphs as in [44, 88]. Additionally, a complete characterisation of the functions that can be expressed by our scale/rescale equivariant building blocks is an open question (except for sign [40]). Finally, an important theoretical matter is a complete characterisation of the expressive power of ScaleGMN, similar to all equivariant metanetworks.\n\n# 7 Conclusion\n\nIn this work, we propose ScaleGMN a metanetwork framework that introduces to the field of NN processing a stronger inductive bias: accounting for function-preserving scaling symmetries that arise from activation functions. ScaleGMN can be applied to NNs with various activation functions by modifying any graph metanetwork, is proven to enjoy desirable theoretical guarantees and empirically demonstrates the significance of scaling by improving the state-of-the-art in several datasets. With our work, we aspire to introduce a new research direction, i.e. incorporating into metanetworks various NN symmetries beyond permutations, aiming to improve their generalisation capabilities and broaden their applicability in various NN processing domains.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n现有处理神经网络参数的架构大多仅考虑排列对称性，而忽略了神经网络中存在的缩放对称性，如何设计对排列和缩放都具有不变性/等变性的架构是亟待解决的问题。该问题的重要性在于，有效处理这些对称性有助于提升模型在神经网络处理任务中的性能和泛化能力。\\n\\n**方法概述**\\n本文提出了Scale Equivariant Graph MetaNetworks (ScaleGMN)，通过引入具有尺度不变/等变特性的新型构建块，将缩放对称性纳入图元网络（GMN）范式，使顶点和边表示对排列和缩放对称具有等变性/不变性，从而处理任意图结构和多种激活函数的前馈神经网络。\\n\\n**主要贡献与效果**\\n- 扩展元网络设计范围，将缩放对称性纳入考量，ScaleGMN在INR分类任务上的准确率相比最佳基线提升了约3% - 13%，如在F - MNIST数据集的INR分类任务中，ScaleGMN的准确率为80.46% ± 0.32%，高出NG - GNN约12.46个百分点，相比最佳基线提升了约13%。\\n- 设计了对单个乘数或其组合的标量乘法具有不变/等变性的网络，证明ScaleGMN能模拟前馈神经网络（FFNN）的前向和反向传播过程，可用于重建函数评估和梯度。\\n- 在实验中，ScaleGMN在多个数据集和任务上均显著优于基线模型，双向ScaleGMN - B在INR编辑任务中的MSE测试损失达到1.89，优于所有排列等变基线模型；在泛化预测任务中，在CIFAR - 10 - GS数据集上Kendall - τ值优于所有基线模型，如在CIFAR - 10 - GS ReLU数据集上，ScaleGMN的Kendall - τ值为0.9276 ± 0.002，优于StatNN (0.9140 ± 0.001)、NFNNp (0.9190 ± 0.010)、NFNHNP (0.9270 ± 0.001) 和NG - GNN (0.9010 ± 0.060) 等基线模型。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nScaleGMN的核心思想是构建等变的图元网络（GMN），在顶点和边级别保留排列和缩放对称性，即顶点表示与偏置有相同对称性，边表示与权重有相同对称性。其有效性在于能适应不同激活函数和架构，且可模拟FFNN的前后向传播，便于重建函数评估和梯度。\\n\\n**创新点**\\n先前的工作要么不考虑任何对称性，要么主要关注排列对称性，未充分考虑缩放对称性。ScaleGMN的创新点在于引入缩放对称性，设计了尺度等变和尺度不变网络，解决了尺度等变消息传递和重缩放问题，能够处理不同激活函数和架构的输入神经网络。\\n\\n**具体实现步骤**\\n1. **定义缩放对称性**：分析前馈神经网络（FFNN）中的缩放对称性，明确不同激活函数（如ReLU、tanh、sine）对应的缩放对$(a, b)$，即满足$\\sigma(ax)=b\\sigma(x)$的条件。对于ReLU激活函数，$\\sigma(ax)=a\\sigma(x)$（$a>0$）；对于tanh和sine激活函数，$\\sigma(ax)=a\\sigma(x)$（$a\\in\\{ - 1, 1\\}$）。\\n2. **初始化MPNN**：确保顶点和边表示的对称性在初始化时得到保留。\\n3. **构建等变GMN**：构建等变的GMN，使顶点和边表示分别与偏置和权重具有相同对称性，如满足等效参数向量，隐藏神经元表示需遵循相应对称性。\\n4. **解决尺度等变和重缩放问题**：\\n    - 设计尺度等变网络（Scale Eq. Net）和尺度不变网络（Scale Inv. Net），通过组合线性变换和尺度不变函数实现尺度等变性，公式为$\\mathsf{ScaleEq} = \\mathsf{f}^{K} \\circ \\cdots \\circ \\mathsf{f}^{1}$，$\\mathsf{f}^{k}(\\mathbf{X}) = (\\mathbf{\\Gamma}_{1}^{k}\\mathbf{x}_{1}, \\ldots, \\mathbf{\\Gamma}_{n}^{k}\\mathbf{x}_{n}) \\odot \\mathsf{ScaleInv}^{k}(\\mathbf{X})$。其中，$\\rho^{k} : \\prod_{i = 1}^{n} \\chi_{i} \\to \\mathbb{R}^{\\sum_{i = 1}^{n} d_{i}^{k}}$为通用近似器（如MLPs），$\\Gamma_{i}^{k} : \\mathcal{X}_{i} \\to \\mathbb{R}^{d_{i}^{k}}$为线性变换。\\n    - 针对重缩放问题，提出重缩放等变网络（ReScale Eq. Net），通过外积或Hadamard积解决，公式为$\\mathsf{ReScaleEq}(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}) = \\mathsf{ScaleEq}(\\mathsf{vec}(\\mathbf{X}_{n}))$ ，实际中使用Hadamard积时为$\\mathsf{ReScaleEq}(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}) = \\odot_{i = 1}^{n} \\mathbf{\\Gamma}_{i} \\mathbf{x}_{i}$，$\\Gamma_{i} : \\mathcal{X}_{i} \\to \\mathbb{R}^{\\bar{d}}$。\\n5. **定义尺度等变消息传递**：\\n    - 消息函数$\\mathbf{m}_V(i)$与中心顶点$i$有相同对称性，设计$\\mathtt{MSG}_V$满足尺度等变，公式为$\\mathtt{MSG}_V(\\mathbf{x}, \\mathbf{y}, \\mathbf{e}) = \\mathsf{ScaleEq}([\\mathbf{x}, \\mathsf{ReScaleEq}(\\mathbf{y}, \\mathbf{e})])$。在实验中，仅使用$\\mathbf{y}$和$\\mathbf{e}$，未发现包含中心顶点$\\mathbf{x}$能带来显著性能提升。\\n    - 更新函数确保顶点和边表示的对称性，公式为$\\mathsf{UPD}_V(\\mathbf{x}, \\mathbf{m}) = \\mathsf{ScaleEq}([\\mathbf{x}, \\mathbf{m}])$。\\n6. **读出步骤**：使用DeepSets架构实现排列不变性，同时使用规范化/对称化版本的顶点表示实现尺度不变性，公式为$\\mathrm{READ}_V(\\mathbf{X}) := \\mathrm{DeepSets}(\\tilde{\\mathbf{x}}_{1}, \\ldots, \\tilde{\\mathbf{x}}_{n})$，其中$\\tilde{\\mathbf{x}}_{i} = \\mathrm{canon}_{i}(\\mathbf{x}_{i})$或$\\tilde{\\mathbf{x}}_{i} = \\mathrm{symm}_{i}(\\mathbf{x}_{i})$。\\n7. **证明理论性质**：证明ScaleGMN具有排列和缩放等变性，且能模拟FFNN的前后向传播。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括MLP、Inr2Vec、DWS、NFNNp、NFNHNP、NG - GNN、StatNN、MLP DWS等。\\n\\n**性能对比**\\n*   **在 [INR分类准确率/Accuracy] 指标上：** 本文的ScaleGMN在MNIST INR数据集上达到了 **96.57% ± 0.10%** 的准确率，显著优于MLP (17.55% ± 0.01%)、Inr2Vec (23.69% ± 0.10%)、DWS (85.71% ± 0.57%)、NFNNp (78.50% ± 0.23%)、NFNHNP (79.11% ± 0.84%) 和NG - GNN (91.40% ± 0.60%) 等基线模型，相对最佳基线提升了约5.17个百分点。在F - MNIST INR数据集上，ScaleGMN达到了 **80.46% ± 0.32%** 的准确率，高出NG - GNN约12.46个百分点，相比最佳基线提升了约13%。在CIFAR - 10 - INR数据集上，ScaleGMN的准确率为 **36.43% ± 0.41%**，优于MLP (11.38% ± 0.34%)、DWS (34.45% ± 0.42%)、NFNNp (33.41% ± 0.01%) 和NFNHNP (28.64% ± 0.07%) 等基线模型。在Aug. CIFAR10 - INR数据集上，ScaleGMN的准确率为 **56.62% ± 0.24%**，也优于其他基线模型。\\n*   **在 [泛化预测Kendall - τ值/Kendall - τ] 指标上：** 本文的ScaleGMN在CIFAR - 10 - GS ReLU数据集上的Kendall - τ值为 **0.9276 ± 0.002**，优于StatNN (0.9140 ± 0.001)、NFNNp (0.9190 ± 0.010)、NFNHNP (0.9270 ± 0.001) 和NG - GNN (0.9010 ± 0.060) 等基线模型。在SVHN - GS ReLU数据集上，ScaleGMN的Kendall - τ值为 **0.8689 ± 0.003**，也优于其他基线模型。在CIFAR - 10 - GS Tanh数据集上，ScaleGMN的Kendall - τ值为 **0.9418 ± 0.005**，高于其他基线模型。在SVHN - GS Tanh数据集上，ScaleGMN的Kendall - τ值为 **0.8736 ± 0.003**，同样优于其他基线模型。在CIFAR - 10 - GS both act.数据集上，ScaleGMN的Kendall - τ值为 **0.941 ± 0.006**，显著高于其他基线模型。\\n*   **在 [INR编辑均方误差/MSE] 指标上：** 本文的双向ScaleGMN - B在Dilating MNIST INRs任务中的MSE为 **1.89 ± 0.00**，优于MLP DWS (5.35 ± 0.00)、NFNNp (2.55 ± 0.00)、NFNHNP (2.65 ± 0.01)、NG - GNN - 0 (2.38 ± 0.02) 和NG - GNN - 64 (2.06 ± 0.01) 等基线模型。ScaleGMN的MSE为 **2.56 ± 0.03**，与部分基线模型相当。\",\n    \"keywords\": \"### 关键词\\n\\n- 尺度等变图元网络 (Scale Equivariant Graph MetaNetworks, ScaleGMN)\\n- 神经网络对称性 (Neural Network Symmetries, N/A)\\n- 前馈神经网络处理 (Feedforward Neural Network Processing, N/A)\\n- 隐式神经表示 (Implicit Neural Representations, INRs)\\n- 等变机器学习 (Equivariant Machine Learning, N/A)\\n- 不变机器学习 (Invariant Machine Learning, N/A)\\n- 缩放对称性 (Scaling Symmetries, N/A)\\n- 图元网络 (Graph MetaNetworks, GMN)\"\n}"
}