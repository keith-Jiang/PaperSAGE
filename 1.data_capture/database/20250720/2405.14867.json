{
    "link": "https://arxiv.org/abs/2405.14867",
    "pdf_link": "https://arxiv.org/pdf/2405.14867",
    "title": "Improved Distribution Matching Distillation for Fast Image Synthesis",
    "authors": [
        "Tianwei Yin",
        "Michael Gharbi",
        "Taesung Park",
        "Richard Zhang",
        "Eli Shechtman",
        "Frédo Durand",
        "William T. Freeman"
    ],
    "institutions": [
        "Adobe Research",
        "Massachusetts Institute of Technology"
    ],
    "publication_date": "2024-05-23",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 127,
    "influential_citation_count": 32,
    "paper_content": "# Improved Distribution Matching Distillation for Fast Image Synthesis\n\nTianwei Yin1 Michaël Gharbi2 Taesung Park2 Richard Zhang2 Eli Shechtman2 Frédo Durand1 William T. Freeman1\n\n1Massachusetts Institute of Technology 2Adobe Research\n\nhttps://tianweiy.github.io/dmd2/\n\n# Abstract\n\nRecent approaches have shown promises distilling expensive diffusion models into efficient one-step generators. Amongst them, Distribution Matching Distillation (DMD) produces one-step generators that match their teacher in distribution, i.e., the distillation process does not enforce a one-to-one correspondence with the sampling trajectories of their teachers. However, to ensure stable training in practice, DMD requires an additional regression loss computed using a large set of noise–image pairs, generated by the teacher with many steps of a deterministic sampler. This is not only computationally expensive for large-scale text-to-image synthesis, but it also limits the student’s quality, tying it too closely to the teacher’s original sampling paths. We introduce DMD2, a set of techniques that lift this limitation and improve DMD training. First, we eliminate the regression loss and the need for expensive dataset construction. We show that the resulting instability is due to the “fake” critic not estimating the distribution of generated samples with sufficient accuracy and propose a two time-scale update rule as a remedy. Second, we integrate a GAN loss into the distillation procedure, discriminating between generated samples and real images. This lets us train the student model on real data, thus mitigating the imperfect “real” score estimation from the teacher model, and thereby enhancing quality. Third, we introduce a new training procedure that enables multi-step sampling in the student, and addresses the training–inference input mismatch of previous work, by simulating inference-time generator samples during training. Taken together, our improvements set new benchmarks in onestep image generation, with FID scores of 1.28 on ImageNet- $6 4 \\times 6 4$ and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a $5 0 0 \\times$ reduction in inference cost. Further, we show our approach can generate megapixel images by distilling SDXL, demonstrating exceptional visual quality among few-step methods, and surpassing the teacher. We release our code and pretrained models.\n\n# 1 Introduction\n\nDiffusion models have achieved unprecedented quality in visual generation tasks [1–8]. But their sampling procedure typically requires dozens of iterative denoising steps, each of which is a forward pass through a neural network. This makes high resolution text-to-image synthesis slow and expensive. To address this issue, numerous distillation methods have been developed to convert a teacher diffusion model into an efficient, few-step student generator [9–20]. However, they often result in degraded quality, as the student model is typically trained with a loss to learn the pairwise noise-to-image mapping of the teacher, but struggles to perfectly mimic its behavior.\n\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n\n![](images/b44b82bff94d800361bf690e887e3883f3bdd2827366075e9ce1bf73e7374e32.jpg)  \nFigure 1: $1 0 2 4 \\times 1 0 2 4$ samples produced by our 4-step generator distilled from SDXL. Please zoom in for details.\n\nNevertheless, it should be noted that loss functions aimed at matching distributions, such as the GAN [21] or the DMD [22] loss, are not burdened with the complexity of precisely learning the specific paths from noise to image because their goal is to align with the teacher model in terms of distribution—by minimizing either a Jensen-Shannon (JS) or an approximate Kullback-Leibler (KL) divergence between the student and teacher output distributions.\n\nIn particular, DMD [22] has demonstrated state-of-the-art results in distilling Stable Diffusion 1.5, yet it remains less investigated than GAN-based methods [23–29]. A likely reason is that DMD still requires an additional regression loss to ensure stable training. In turn, this necessitates creating millions of noise-image pairs by running the full sampling steps of the teacher model, which is particularly costly for text-to-image synthesis. The regression loss also negates the key benefit of DMD’s unpaired distribution matching objective, because it causes the student’s quality to be upper-bounded by the teacher’s.\n\nIn this paper, we show how to do away with DMD’s regression loss, without compromising training stability. We then push the limits of distribution matching by integrating the GAN framework into DMD, and enable few-steps sampling with a novel training procedure, which we termed ‘backward simulation’. Taken together, our contributions lead to state-of-the-art fast generative models that outperform their teacher, using as few as 4 sampling steps. Our method, which we call DMD2, achieves state-of-the-art results in one-step image generation, setting a new benchmark with FID scores of 1.28 on ImageNet- $6 4 \\times 6 4$ and 8.35 on zero-shot COCO 2014. We demonstrate our approach’s scalability by distilling from SDXL to produce high-quality megapixel images, establishing new standards among few-step methods.\n\nIn short, our contributions are as follows:\n\n• We propose a new distribution matching distillation technique that does not require a regression loss for stable training, thereby eliminating the need for costly data collection, and allowing for more flexible and scalable training.   \n• We show that training instability in DMD [22] without regression loss stems from an insufficiently trained fake diffusion critic, and implement a two time-scale update rule to address this issue.   \n• We integrate a GAN objective into the DMD framework, where the discriminator is trained to distinguish samples from the student generator vs. real images. This additional supervision operates at the distribution level, which better aligns with DMD’s distribution-matching philosophy than the original regression loss. It mitigates approximation errors in the teacher diffusion model and enhances image quality.   \n• While the original DMD only supports one-step students, we introduce a technique to support multi-step generators. Unlike previous multi-step distillation methods, we avoid the domain mismatch between training and inference by simulating inference-time generator inputs during training, thus improving overall performance.\n\n# 2 Related Work\n\nDiffusion Distillation. Recent diffusion acceleration techniques have focused on speeding up the generation process through distillation [9, 10, 13–20, 22, 23, 30]. They typically train a generator to approximate the ordinary differential equation (ODE) sampling trajectory of a teacher model, in fewer sampling steps. Notably, Luhman et al. [16] precompute a dataset of noise and images pairs, generated by the teacher using an ODE sampler, and use it to train the student to regress the mapping in a single network evaluation. Follow-up works like Progressive Distillation [10, 13] eliminate the need to precompute this paired dataset offline. They iteratively train a sequence of student models, each halving the number of sampling steps of its predecessor. A complementary technique, Instaflow [11] straightens the ODE trajectories, so they are easier to approximate with a one-step student. Consistency Distillation [9,12,19,26,31,32], and TRACT [33], train student models so their outputs are self-consistent at any timesteps along the ODE trajectory, and thus consistent with the teacher.\n\nGANs. Another line of research employs adversarial training to align the student with the teacher at a broader distribution level. In ADD [23], the generator, initialized with weights from a diffusion model, is trained using a projected GAN objective with an image-space classifier [34]. Building on this, LADD [24] utilizes a pre-trained diffusion model as the discriminator and operates in latent space, thus improving scalability and enabling higher-resolution synthesis. Inspired by DiffusionGAN [28, 29], UFOGen [25] introduces noise injection prior to the real vs. fake classification in the discriminator, to smooth out the distributions, which stabilizes the training dynamics. However, purely GANbased methods often struggle to integrate classifier-free guidance directly. For instance, LADD uses diffusion-generated images with classifier-free guidance as real data in its GAN discriminator. Other approaches combine adversarial objectives with a distillation loss to preserve the original guided sampling trajectory. For instance, SDXL-Lightning [27] integrates a DiffusionGAN loss [25] with a progressive distillation objective [10, 13], while the Consistency Trajectory Model [26] combines a GAN [35] with an improved consistency distillation [9]. In contrast, our approach based on distribution matching [22, 36, 37] inherently integrates classifier-free guidance into the training supervision, significantly simplifying the training process.\n\nScore Distillation was initially introduced in the context of text-to-3D synthesis [37–40], utilizing a pre-trained text-to-image diffusion model as a distribution matching loss. These methods optimize a 3D object by aligning rendered views with a text-conditioned image distribution, using the scores predicted by a pretrained diffusion model. Recent works have extended score distillation [37, 38, 41– 43] to diffusion distillation [22, 30, 36, 44–46]. Notably, DMD [22] minimizes an approximate KL divergence, with its gradient represented as the difference between two score functions: one, fixed and pretrained, for the target distribution and another, trained dynamically, for the output distribution of the generator.\n\n![](images/c1b05ba93da6851dada5c35bf8eda4eb88ebee4044db2a176588f3fda71df3c5.jpg)  \nFigure 2: $1 0 2 4 \\times 1 0 2 4$ samples produced by our 4-step generator distilled from SDXL. Please zoom in for details.\n\nDMD parameterizes both score functions using diffusion models. This training objective proved more stable than GAN-based methods and has demonstrated superior performance in one-step image synthesis. An important caveat, DMD requires a regression loss for stability, calculated using precomputed noise-image pairs, similar to Luhman et al. [16]. Our work does away with this requirement. We introduce techniques to stabilize the DMD training procedure without the regression regularizer, thus significantly reducing the computational costs incurred by paired data precomputation. Furthermore, we extend DMD to support multi-step generation and integrate the strengths of both GANs and distribution matching approaches [22, 30, 36, 45], leading to state-of-theart results in text-to-image synthesis.\n\n# 3 Background: Diffusion and Distribution Matching Distillation\n\nThis section gives a brief overview of diffusion models and distribution matching distillation (DMD).\n\nDiffusion Models generate images through iterative denoising. In the forward diffusion process, noise is progressively added to corrupt a sample $x \\sim p _ { \\mathrm { r e a l } }$ from the data distribution into pure Gaussian noise over a predetermined number of steps $T$ , so that, at each timestep $t$ , the diffused samples follow the distribution $\\begin{array} { r } { p _ { \\mathrm { r e a l } , t } ( x _ { t } ) = \\int p _ { \\mathrm { r e a l } } ( x ) q ( \\dot { x } _ { t } | x ) d x } \\end{array}$ , with $q _ { t } ( x _ { t } | x ) \\sim \\bar { \\mathcal { N } } ( \\alpha _ { t } x , \\sigma _ { t } ^ { 2 } \\mathbf { I } )$ , where $\\alpha _ { t } , \\sigma _ { t } > 0$ are scalars determined by the noise schedule [47, 48]. The diffusion model learns to iteratively reverse the corruption process by predicting a denoised estimate $\\mu ( x _ { t } , t )$ , conditioned on the current noisy sample $\\mathbf { \\Psi } _ { x _ { t } }$ and the timestep $t$ , ultimately leading to an image from the data distribution $p _ { \\mathrm { r e a l } }$ . After training, the denoised estimate relates to the gradient of the data likelihood function, or score function [48] of the diffused distribution:\n\n$$\ns _ { \\mathrm { r e a l } } ( x _ { t } , t ) = \\nabla _ { x _ { t } } \\log p _ { \\mathrm { r e a l } , t } ( x _ { t } ) = - \\frac { x _ { t } - \\alpha _ { t } \\mu _ { \\mathrm { r e a l } } ( x _ { t } , t ) } { \\sigma _ { t } ^ { 2 } } .\n$$\n\nSampling an image typically requires dozens to hundreds of denoising steps [49–52].\n\nDistribution Matching Distillation (DMD) distills a many-step diffusion models into a one-step generator $G$ [22] by minimizing the expectation over $t$ of approximate Kullback-Liebler (KL) divergences between the diffused target distribution $p _ { \\mathrm { r e a l } , t }$ and the diffused generator output distribution $p _ { \\mathrm { f a k e } , t }$ . Since DMD trains $G$ by gradient descent, it only requires the gradient of this loss, which can be computed as the difference of 2 score functions:\n\n$$\n7 . \\mathrm { { Z } _ { \\mathrm { { D M D } } } } = \\mathbb { E } _ { t } \\left( \\nabla _ { \\theta } \\mathrm { K L } \\left( p _ { \\mathrm { f i c } , t } \\right| \\left| p _ { \\mathrm { r e a l } , t } \\right) \\right) = - \\mathbb { E } _ { t } \\left( \\int \\left( s _ { \\mathrm { r e a l } } ( F ( G _ { \\theta } ( z ) , t ) , t ) - s _ { \\mathrm { f i c } } ( F ( G _ { \\theta } ( z ) , t ) , t ) \\right) \\frac { d G _ { \\theta } ( z ) } { d \\theta } d z \\right) ,\n$$\n\nwhere $z \\sim \\mathcal { N } ( 0 , \\mathbf { I } )$ is a random Gaussian noise input, $\\theta$ are the generator parameters, $F$ is the forward diffusion process (i.e., noise injection) with noise level corresponding to time step $t$ , and $s _ { \\mathrm { r e a l } }$ and $s _ { \\mathrm { f a k e } }$ are scores approximated using diffusion models $\\mu _ { \\mathrm { r e a l } }$ and $\\mu _ { \\mathrm { f a k e } }$ trained on their respective distributions (Eq. (1)). DMD uses a frozen pre-trained diffusion model as $\\mu _ { \\mathrm { r e a l } }$ (the teacher), and dynamically updates $\\mu _ { \\mathrm { f a k e } }$ while training $G$ , using a denoising score-matching loss on samples from the one-step generator, i.e., fake data [22, 47].\n\nYin et al. [22] found that an additional regression term [16] was needed to regularize the distribution matching gradient (Eq. (2)) and achieve high-quality one-step models. For this, they collect a dataset of noise-image pairs $( z , y )$ where the image $y$ is generated using the teacher diffusion model, and a deterministic sampler [49, 50, 53], starting from the noise map $z$ . Given the same input noise $z$ , the regression loss compares the generator output with the teacher’s prediction:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { \\mathrm { r e g } } = \\mathbb { E } _ { ( z , y ) } d ( G _ { \\theta } ( z ) , y ) , } \\end{array}\n$$\n\nwhere $d$ is a distance function, such as LPIPS [54] in their implementation. While gathering this data incurs negligible cost for small datasets like CIFAR-10, it becomes a significant bottleneck with large-scale text-to-image synthesis tasks, or models with complex conditioning [55–57]. For instance, generating one noise-image pair for SDXL [58] takes around 5 seconds, amounting to about 700 A100 days to cover the 12 million prompts in the LAION 6.0 dataset [59], as utilized by Yin et al. [22]. This dataset construction cost alone is already more than $4 \\times$ our total training compute (as detailed in Appendix I). This regularization objective is also at odds with DMD’s goal of matching the student and teacher in distribution, since it encourages adherence to the teacher’s sampling paths.\n\n# 4 Improved Distribution Matching Distillation\n\nWe revisit multiple design choices in the DMD algorithm [22] and identify significant improvements.\n\n![](images/06e244ac90ce2462caad245d56e9dc64ef70322293d86892c28385716c8d2de7.jpg)  \nFigure 3: Our method distills a costly diffusion model (gray, right) into a one- or multi-step generator (red, left). Our training alternates between 2 steps: 1. optimizing the generator using the gradient of an implicit distribution matching objective (red arrow) and a GAN loss (green), and 2. training a score function (blue) to model the distribution of “fake” samples produced by the generator, as well as a GAN discriminator (green) to discriminate between fake samples and real images. The student generator can be a one-step or a multi-step model, as shown here, with an intermediate step input.\n\n# 4.1 Removing the regression loss: true distribution matching and easier large-scale training\n\nThe regression loss [16] used in DMD [22] ensures mode coverage and training stability, but as we discussed in Section 3, it makes large-scale distillation cumbersome, and is at odds with the distribution matching idea, thus inherently limiting the performance of the distilled generator to that of the teacher model. Our first improvement is to remove this loss.\n\n# 4.2 Stabilizing pure distribution matching with a Two Time-scale Update Rule\n\nNaively omitting the regression objective, shown in Eq. (3), from DMD leads to training instabilities and significantly degrades quality (Tab. 3). For example, we observed that the average brightness, along with other statistics, of generated samples fluctuates significantly, without converging to a stable point (See Appendix F). We attribute this instability to approximation errors in the fake diffusion model $\\mu _ { \\mathrm { f a k e } }$ , which does not track the fake score accurately, since it is dynamically optimized on the non-stationary output distribution of the generator. This causes approximation errors and biased generator gradients (as also discussed in [30]).\n\nWe address this using the two time-scale update rule inspired by Heusel et al. [60]. Specifically, we train $\\mu _ { \\mathrm { f a k e } }$ and the generator $G$ at different frequencies to ensure that $\\mu _ { \\mathrm { f a k e } }$ accurately tracks the generator’s output distribution. We find that using 5 fake score updates per generator update, without the regression loss, provides good stability and matches the quality of the original DMD on ImageNet (Tab. 3) while achieving much faster convergence. Further analysis are included in Appendix F.\n\n# 4.3 Surpassing the teacher model using a GAN loss and real data\n\nOur model so far achieves comparable training stability and performance to DMD [22] without the need for costly dataset construction (Tab. 3). However, a performance gap remains between the distilled generator and the teacher diffusion model. We hypothesize this gap could be attributed to approximation errors in the real score function $\\mu _ { \\mathrm { r e a l } }$ used in DMD, which would propagate to the generator and lead to suboptimal results. Since DMD’s distilled model is never trained with real data, it cannot recover from these errors.\n\nWe address this issue by incorporating an additional GAN objective into our pipeline, where the discriminator is trained to distinguish between real images and images produced by our generator.\n\nTrained using real data, the GAN classifier does not suffer from the teacher’s limitation, potentially allowing our student generator to surpass it in sample quality. Our integration of a GAN classifier into DMD follows a minimalist design: we add a classification branch on top of the bottleneck of the fake diffusion denoiser (see Fig. 3). The classification branch and upstream encoder features in the UNet are trained by maximizing the standard non-saturing GAN objective:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { \\mathrm { G A N } } = \\mathbb { E } _ { x \\sim p _ { \\mathrm { r o l } } , t \\sim [ 0 , T ] } [ \\log D ( F ( x , t ) ) ] + \\mathbb { E } _ { z \\sim p _ { \\mathrm { m i s c } } , t \\sim [ 0 , T ] } [ - \\log ( D ( F ( G _ { \\theta } ( z ) , t ) ) ) ] , } \\end{array}\n$$\n\nwhere $D$ is the discriminator, and $F$ is the forward diffusion process (i.e., noise injection) defined in Section 3, with noise level corresponding to time step $t$ . The generator $G$ minimizes this objective. Our design is inspired by prior works that use diffusion models as discriminators [24, 25, 27]. We note that this GAN objective is more consistent with the distribution matching philosophy since it does not require paired data, and is independent of the teacher’s sampling trajectories.\n\n# 4.4 Multi-step generator\n\nWith the proposed improvements, we are able to match the performance of teacher diffusion models on ImageNet and COCO (see Tab. 1 and Tab. 5). However, we found that larger scale models like SDXL [58] remain challenging to distill into a one-step generator because of limited model capacity and a complex optimization landscape to learn the direct mapping from noise to highly diverse and detailed images. This motivated us to extend DMD to support multi-step sampling.\n\nWe fix a predetermined schedule with $N$ timestep $\\left\\{ t _ { 1 } , t _ { 2 } , \\dots t _ { N } \\right\\}$ , identical during training and inference. During inference, at each step, we alternate between denoising and noise injection steps, following the consistency model [9], to improve sample quality. Specifically, starting from Gaussian noise $z _ { 0 } \\overset { - } { \\sim } \\mathcal { N } ( 0 , \\mathbf { I } )$ , we alternate between denoising updates $\\hat { x } _ { t _ { i } } ^ { \\phantom { \\dagger } } = \\bar { G } _ { \\theta } ( x _ { t _ { i } } , \\bar { t } _ { i } )$ , and forward diffusion steps $x _ { t _ { i + 1 } } = \\alpha _ { t _ { i + 1 } } \\hat { x } _ { t _ { i } } + \\sigma _ { t _ { i + 1 } } \\epsilon$ with $\\epsilon \\sim \\mathcal { N } ( 0 , \\mathbf { I } )$ , until we obtain our final image $\\hat { x } _ { t _ { N } }$ . Our 4-step model uses the following schedule: 999, 749, 499, 249, for a teacher model trained with 1000 steps.\n\n# 4.5 Multi-step generator simulation to avoid training/inference mismatch\n\nPrevious multi-step generators are typically trained to denoise noisy real images [23,24,27]. However, during inference, except for the first step, which starts from pure noise, the generator’s input come from a previous generator sampling step $\\hat { x } _ { t _ { i } }$ . This creates a training-inference mismatch that adversely impacts quality (Fig. 4). We address this issue by replacing the noisy real images during training, with noisy synthetic images $\\boldsymbol { x } _ { t _ { i } }$ produced by the current student generator running several steps, similar to our inference pipeline $( \\ S 4 . 4 )$ . This is tractable because, unlike the teacher diffusion model, our generator only runs for a few steps. Our generator then denoises these simulated images and the outputs are supervised with the proposed loss functions. Using noisy synthetic images avoids the mismatch and improves overall performance (See Sec. 5.3).\n\n#++→ ←- real image forward diffusion:  train/test domain gap “fake” sample backward simulation: train/test alignment\n\nA concurrent work, Imagine Flash [61], proposed a similar technique. Their backward distillation algorithm shares our motivation of reducing the training and testing gap by using the student-generated images as the input to the subsequent sampling steps at training time. However, they do not entirely resolve the mismatch issue, because the teacher model of the regression loss now suffers from the training–test gap: it is never trained with synthetic images. This error is accumulated along the sampling path. In contrast, our distribution matching loss is not dependent on the input to the student model, alleviating this issue.\n\n# 4.6 Putting everything together\n\nIn summary, our distillation method lifts DMD [22] stringent requirements for precomputed noise– image pairs. It further integrates the strength of GANs and supports multi-step generators. As shown in Fig. 3, starting from a pretrained diffusion model, we alternate between optimizing the generator $G _ { \\theta }$ to minimize the original distribution matching objective as well as a GAN objective, and optimizing the fake score estimator $\\mu _ { \\mathrm { f a k e } }$ using both a denoising score matching objective on the fake data, and the GAN classification loss. To ensure the fake score estimate is accurate and stable, despite being optimized on-line, we update it with higher frequency than the generator (5 steps vs. 1).\n\n# 5 Experiments\n\nWe evaluate our approach, DMD2, using several benchmarks, including class-conditional image generation on ImageNet- $6 4 \\times 6 4$ [62], and text-to-image synthesis on COCO 2014 [63] with various teacher models [1, 58]. We use the Fréchet Inception Distance (FID) [60] to measure image quality and diversity, and the CLIP Score [64] to evaluate text-to-image alignment. For SDXL models, we additionally report patch FID [27, 65], which measures FID on 299x center-cropped patches of each image, to assess high-resolution details. Finally, we conduct human evaluations to compare our approach with other state-of-the-art methods. Comprehensive evaluations confirm that distilled models trained using our approach outperform previous work, and even rival the performance of the teacher models. Detailed training and evaluation procedures are provided in the appendix.\n\n# 5.1 Class-conditional Image Generation\n\nTable 1 compares our model with recent baselines on ImageNet- $6 4 \\times 6 4$ . With a single forward pass, our method significantly outperforms existing distillation techniques and even outperforms the teacher model using ODE sampler [53]. We attribute this remarkable performance to the removal of DMD’s regression loss (Sec. 4.1 and 4.2), which eliminates the performance upper bound imposed by the ODE sampler, as well as our additional GAN term (Sec. 4.3), which mitigates the adverse impact of the teacher diffusion model’s score approximation error.\n\nTable 1: Image quality comparison on ImageNet- $6 4 \\times 6 4$ .   \n\n<html><body><table><tr><td>Method</td><td>#Fwd Pass (↓)</td><td>FID (↓)</td></tr><tr><td>BigGAN-deep [66]</td><td>1</td><td>4.06</td></tr><tr><td>ADM [67]</td><td>250</td><td>2.07</td></tr><tr><td>RIN [68]</td><td>1000</td><td>1.23</td></tr><tr><td>StyleGAN-XL [35]</td><td>1</td><td>1.52</td></tr><tr><td>Progress.Distill. [10]</td><td>1</td><td>15.39</td></tr><tr><td>DFNO [69]</td><td>1</td><td>7.83</td></tr><tr><td>BOOT[20]</td><td>1</td><td>16.30</td></tr><tr><td>TRACT[33]</td><td>1</td><td>7.43</td></tr><tr><td>Meng et al. [13]</td><td>1</td><td>7.54</td></tr><tr><td>Diff-Instruct [36]</td><td>1</td><td>5.57</td></tr><tr><td>Consistency Model [9]</td><td>1</td><td>6.20</td></tr><tr><td>iCT-deep [12]</td><td>1</td><td>3.25</td></tr><tr><td>CTM [26]</td><td>1</td><td>1.92</td></tr><tr><td>DMD [22]</td><td>1</td><td>2.62</td></tr><tr><td>DMD2 (Ours)</td><td>1</td><td>1.51</td></tr><tr><td>+longer training (Ours)</td><td>1</td><td>1.28</td></tr><tr><td>EDM(Teacher,ODE) [53]</td><td>511</td><td>2.22</td></tr><tr><td>EDM(Teacher, SDE) [53]</td><td>511</td><td>1.36</td></tr></table></body></html>\n\nTable 2: Image quality comparison with SDXL backbone on 10K prompts from COCO 2014.   \n\n<html><body><table><tr><td>Method</td><td>#Fwd Pass (↓)</td><td>FID (↓)</td><td>Patch FID (↓)</td><td>CLIP (↑)</td></tr><tr><td>LCM-SDXL [32]</td><td>1 4</td><td>81.62 22.16</td><td>154.40 33.92</td><td>0.275 0.317</td></tr><tr><td>SDXL-Turbo [23]</td><td>1 4</td><td>24.57 23.19</td><td>23.94 23.27</td><td>0.337 0.334</td></tr><tr><td>SDXL</td><td>1</td><td>23.92</td><td>31.65</td><td>0.316</td></tr><tr><td>Lightning [27] DMD2 (Ours)</td><td>4</td><td>24.46</td><td>24.56</td><td>0.323</td></tr><tr><td></td><td>1 4</td><td>19.01 19.32</td><td>26.98 20.86</td><td>0.336 0.332</td></tr><tr><td>SDXL Teacher,cfg=6 [58]</td><td>100</td><td></td><td></td><td></td></tr><tr><td>SDXL Teacher,cfg=8 [58]</td><td>100</td><td>19.36 20.39</td><td>21.38 23.21</td><td>0.332 0.335</td></tr></table></body></html>\n\n# 5.2 Text-to-Image Synthesis\n\nWe evaluate DMD2’s text-to-image generation performance on zero-shot COCO 2014 [63]. Our generators are trained by distilling SDXL [58] and SD v1.5 [1], respectively, using a subset of 3 million prompts from LAION-Aesthetics [59]. Additionally, we collect $5 0 0 \\mathrm { k }$ images from LAIONAesthetic as training data for the GAN discriminator. Table 2 summarizes distillation results for the SDXL model. Our 4-step generator produces high quality and diverse samples, achieving a FID score of 19.32 and a CLIP score of 0.332, rivaling the teacher diffusion model for both image quality and prompt coherence. To further verify our method’s effectiveness, we conduct an extensive user study comparing our model’s output with those from the teacher model and existing distillation methods. We use a subset of 128 prompts from PartiPrompts [70] following LADD [24]. For each comparison, we ask a random set of five evaluators to choose the image that is more visually appealing, as well as the one that better represents the text prompt. Details about the human evaluation are included in Appendix K. As shown in Figure 5, our model achieves much higher user preferences than baseline approaches. Notably, our model outperforms its teacher in image quality for $24 \\%$ of samples and achieves comparable prompt alignment, while requiring $2 5 \\times$ fewer forward passes (4 vs 100). Qualitative comparisons are shown in Figure 6. Results for SDv1.5 are provided in Table 5 in Appendix D. Similarly, one-step model trained using DMD2 outperforms all previous diffusion acceleration approaches, achieving a FID score of 8.35, representing a significant 3.14-point improvement over the original DMD method [22]. Our results also surpass the teacher models that uses a 50-step PNDM sampler [50].\n\n![](images/0b1e753eb68d59e1e40e58707d168cffd00a8a85fdf3cb8597a0b4a84df2af3b.jpg)  \nFigure 5: User study comparing our distilled model with its teacher and competing distillation baselines [23, 27, 31]. All distilled models use 4 sampling steps, the teacher uses 50. Our model achieves the best performance for both image quality and prompt alignment.\n\n# 5.3 Ablation Studies\n\nTable 3: Ablation studies on ImageNet. TTUR stands for two-timescale update rule.   \n\n<html><body><table><tr><td colspan=\"4\">DMD No Regress. TTUR GAN FID (↓)</td></tr><tr><td>√</td><td></td><td></td><td>2.62</td></tr><tr><td>√</td><td>√</td><td></td><td>3.48</td></tr><tr><td>√</td><td>√ √</td><td></td><td>2.61</td></tr><tr><td>√</td><td>√ √</td><td>√</td><td>1.51</td></tr><tr><td></td><td></td><td>√</td><td>2.56</td></tr><tr><td></td><td>√</td><td>√</td><td>2.52</td></tr></table></body></html>\n\nTable 4: Ablation studies with SDXL backbone on 10K prompts from COCO 2014.   \n\n<html><body><table><tr><td>Method</td><td colspan=\"3\">FID (↓) Patch FID (↓） CLIP(↑)</td></tr><tr><td>w/o GAN</td><td>26.90</td><td>27.66</td><td>0.328</td></tr><tr><td>w/o Distribution Matching</td><td>13.77</td><td>27.96</td><td>0.307</td></tr><tr><td>w/o Backward</td><td>20.66</td><td>24.21</td><td>0.332</td></tr><tr><td>Simulation DMD2 (Ours)</td><td>19.32</td><td>20.86</td><td>0.332</td></tr></table></body></html>\n\nTable 3 ablates different components of our proposed method on ImageNet. Simply removing the ODE regression loss from the original DMD results in a degraded FID of 3.48 due to training instability (see further analysis in Appendix F). However, incorporating our Two Time-scale Update Rule (TTUR, Sec. 4.2) mitigates this performance drop, matching the DMD baseline performance without requiring additional dataset construction. Adding our GAN loss achieves a further 1.1- point improvement in FID. Our integrated approach surpasses the performance of using GAN alone (without distribution matching objective), and adding the two-timescale update rule to GAN alone does not improve it, highlighting the effectiveness of combining distribution matching with GANs in a unified framework.\n\n![](images/f729df879104250421df93b48187db4ed9ebec1bc379431ecc5dd5203d916915.jpg)  \nA photo of llama wearing sunglasses standing on the deck of a spaceship with the Earth in the background.   \nFigure 6: Visual comparison between our model, the SDXL teacher, and selected competing methods [23, 27, 31]. All distilled models use 4 sampling steps while the teacher model uses 50 sampling steps with classifier-free guidance. All images are generated using identical noise and text prompts. Our model produces images with superior realism and text alignment. (Zoom in for details.) More comparisons are available in Appendix Figure 10.\n\nIn Table 4, we ablate the influence of the GAN term (Sec. 4.3), distribution matching objective (Eq. 2), and backward simulation (Sec. 4.4) for distilling the SDXL model into a four-step generator. Qualitative results are shown in Appendix Figure. 7. In the absence of the GAN loss, our baseline model produces oversaturated and oversmoothed images (Appendix Fig. 7 third column). Similarly, eliminating distribution matching objective (Eq. 2) reduces our approach to a pure GAN-based method, which struggles with training stability [71, 72]. Moreover, pure GAN-based methods also lack a natural way to incorporate classifier-free guidance [73], essential for high-quality text-to-image synthesis [1, 2]. Consequently, while GAN-based methods achieve the lowest FID by closely matching the real distribution, they significantly underperform in text alignment and aesthetic quality (Appendix Fig. 7 second column). Likewise, omitting the backward simulation leads to worse image quality, as indicated by the degraded patch FID score.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n现有扩散模型采样过程需大量迭代去噪步骤，致使高分辨率文本到图像合成速度慢、成本高。Distribution Matching Distillation (DMD) 虽有优势，但需额外回归损失和大量噪声 - 图像对，计算成本高，还限制学生模型质量，影响大规模文本到图像合成的效率与质量。该问题的重要性在于大规模文本到图像合成任务对效率和质量要求高，而现有方法无法满足这些需求。\\n\\n**方法概述**\\n论文提出 DMD2 方法，通过消除回归损失、引入双时间尺度更新规则、集成 GAN 损失、支持多步生成及模拟推理输入等技术，改进 DMD 训练，提升蒸馏模型性能。\\n\\n**主要贡献与效果**\\n- 提出无需回归损失的分布匹配蒸馏技术，消除数据收集成本，实现更灵活和可扩展的训练。在 ImageNet - 64×64 上 FID 分数达 1.28，零样本 COCO 2014 上达 8.35，超越原教师模型，推理成本降低 500 倍。\\n- 引入双时间尺度更新规则，解决去除回归损失后的训练不稳定问题，在 ImageNet 上达到与原 DMD 相当的质量且收敛更快。\\n- 集成 GAN 损失到蒸馏过程，使学生模型能在真实数据上训练，缓解教师模型的近似误差，提升图像质量；支持多步采样并模拟推理输入，避免训练 - 推理输入不匹配问题，4 步生成模型在 SDXL 蒸馏中 FID 分数为 19.32，Patch FID 分数为 20.86，CLIP 分数为 0.332，表现出色。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nDMD2 核心思想是改进 DMD 训练，消除回归损失带来的限制。利用双时间尺度更新规则解决训练不稳定问题，通过集成 GAN 损失让学生模型在真实数据上训练，缓解教师模型的估计误差，同时引入多步采样和模拟推理输入解决训练 - 推理不匹配问题，使学生模型能生成高质量图像且超越教师模型。\\n\\n**创新点**\\n先前 DMD 方法需额外回归损失和大量噪声 - 图像对，计算成本高且限制学生模型质量。DMD2 创新点在于：一是去除回归损失，采用双时间尺度更新规则保证训练稳定；二是集成 GAN 损失，在真实数据上训练学生模型；三是支持多步采样，模拟推理输入避免训练 - 推理不匹配；四是从预训练的扩散模型开始，交替优化生成器以最小化原始分布匹配目标和 GAN 目标，同时优化“假”分数估计器，使用去噪分数匹配目标和 GAN 分类损失。\\n\\n**具体实现步骤**\\n1. **去除回归损失**：消除 DMD 中的回归损失和昂贵的数据集构建需求。\\n2. **双时间尺度更新规则**：由于去除回归损失后训练不稳定，提出双时间尺度更新规则，以不同频率训练 $\\mu_{\\mathrm{fake}}$ 和生成器 $G$，以 5 次 $\\mu_{\\mathrm{fake}}$ 更新对应 1 次生成器更新的频率进行训练，确保 $\\mu_{\\mathrm{fake}}$ 准确跟踪生成器输出分布。\\n3. **集成 GAN 损失**：在假扩散去噪器的瓶颈上添加分类分支，训练判别器区分真实图像和生成图像。生成器通过最小化标准非 - 饱和 GAN 目标 $\\mathcal { L } _ { \\mathrm { G A N } } = \\mathbb { E } _ { x \\sim p _ { \\mathrm { r e a l } }, t \\sim [ 0, T ] } [ \\log D ( F ( x, t ) ) ] + \\mathbb { E } _ { z \\sim p _ { \\mathrm { m i s c } }, t \\sim [ 0, T ] } [ - \\log ( D ( F ( G _ { \\theta } ( z ), t ) ) ) ]$ 来进行训练，其中 $D$ 是判别器，$F$ 是前向扩散过程。\\n4. **支持多步采样**：固定一个预定的时间步长调度，在推理时交替进行去噪和噪声注入步骤。在训练时，用当前学生生成器生成的噪声合成图像代替真实噪声图像，避免训练和推理的不匹配问题。\\n5. **交替优化**：从预训练的扩散模型开始，交替优化生成器以最小化原始分布匹配目标和 GAN 目标，同时优化“假”分数估计器，使用去噪分数匹配目标和 GAN 分类损失。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 图像生成领域：BigGAN - deep、ADM、RIN、StyleGAN - XL、Progress.Distill.、DFNO、BOOT、TRACT、Meng et al.、Diff - Instruct、Consistency Model、iCT - deep、CTM、DMD、EDM(Teacher, ODE)、EDM(Teacher, SDE) 等。\\n- 文本到图像合成领域：LCM - SDXL、SDXL - Turbo、SDXL、Lightning、SDXL Teacher 等。\\n\\n**性能对比**\\n*   **在 [Fréchet Inception Distance (FID)] 指标上：** 在 ImageNet - 64×64 的单步生成任务中，本文方法 DMD2 达到了 1.51 的 FID 分数，在更长训练后达到 1.28，显著优于基线模型如 BigGAN - deep (4.06)、DMD (2.62) 等，相对最佳基线 RIN (1.23) 仅低 0.05 个百分点。在零样本 COCO 2014 上，DMD2 单步模型 FID 分数为 8.35，显著优于原 DMD 方法 (11.49)；基于 SDXL 的 4 步生成器，DMD2 的 FID 分数为 19.32，优于 LCM - SDXL (22.16)、SDXL - Turbo (23.19) 等基线模型。\\n*   **在 [Patch FID] 指标上：** 在基于 SDXL 的文本到图像合成任务中，DMD2 的 4 步生成器达到了 20.86 的 Patch FID 分数，优于 LCM - SDXL (33.92)、SDXL - Turbo (23.27) 等基线模型。\\n*   **在 [CLIP Score] 指标上：** 在基于 SDXL 的文本到图像合成任务中，DMD2 的 4 步生成器达到了 0.332 的 CLIP 分数，与其他基线模型表现相当或更优，如优于 LCM - SDXL (0.275)、与 SDXL - Turbo (0.334) 接近。\",\n    \"keywords\": \"### 关键词\\n\\n- 图像合成 (Image Synthesis, N/A)\\n- 分布匹配蒸馏 (Distribution Matching Distillation, DMD)\\n- 生成对抗网络 (Generative Adversarial Networks, GAN)\\n- 扩散模型 (Diffusion Models, N/A)\\n- 文本到图像合成 (Text - to - Image Synthesis, N/A)\\n- 双时间尺度更新规则 (Two Time - Scale Update Rule, TTUR)\\n- 多步采样 (Multi - Step Sampling, N/A)\"\n}"
}