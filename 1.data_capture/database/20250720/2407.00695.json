{
    "link": "https://arxiv.org/abs/2407.00695",
    "pdf_link": "https://arxiv.org/pdf/2407.00695",
    "title": "Learning Formal Mathematics From Intrinsic Motivation",
    "authors": [
        "Gabriel Poesia",
        "David Broman",
        "Nick Haber",
        "Noah D. Goodman"
    ],
    "institutions": [
        "Stanford University",
        "KTH Royal Institute of Technology"
    ],
    "publication_date": "2024-06-30",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 17,
    "influential_citation_count": 1,
    "paper_content": "# Learning Formal Mathematics From Intrinsic Motivation\n\nGabriel Poesia1 David Broman4 Nick Haber1,3 Noah D. Goodman1,2 {poesia,nhaber,ngoodman}@stanford.edu dbro@kth.se   \nDepartments of Computer Science1, Psychology2 and Education3, Stanford University, USA EECS and Digital Futures4, KTH Royal Institute of Technology, Sweden\n\n# Abstract\n\nHow did humanity coax mathematics from the æther? We explore the Platonic view that mathematics can be discovered from its axioms—a game of conjecture and proof. We describe MINIMO (Mathematics from Intrinsic Motivation): an agent that jointly learns to pose challenging problems for itself (conjecturing) and solve them (theorem proving). Given a mathematical domain axiomatized in dependent type theory, we first combine methods for constrained decoding and type-directed synthesis to sample valid conjectures from a language model. Our method guarantees well-formed conjectures by construction, even as we start with a randomly initialized model. We use the same model to represent a policy and value function for guiding proof search. Our agent targets generating hard but provable conjectures—a moving target, since its own theorem proving ability also improves as it trains. We propose novel methods for hindsight relabeling on proof search trees to significantly improve the agent’s sample efficiency in both tasks. Experiments on 3 axiomatic domains (propositional logic, arithmetic and group theory) demonstrate that our agent can bootstrap from only the axioms, self-improving in generating true and challenging conjectures and in finding proofs.\n\n# 1 Introduction\n\nMathematical reasoning stands as a grand challenge for Artificial Intelligence (AI) research since the birth of the field [25]. Artificial agents capable of general mathematical reasoning have the potential to drastically impact both mathematics itself and areas where mathematical proof plays a key role, such as program and hardware verification [4]. While this goal has received significant attention from the AI community [22, 46, 30, 42], it still remains far from the breakthroughs that areas such as general game playing [36], image [32] generation or protein folding [35] have witnessed.\n\nPrior work has reflected two main visions of how AI might achieve general mathematical reasoning abilities. One such strategy is to leverage all of the available human-produced mathematical knowledge as a starting point [31]. This knowledge is encoded in source as varied as textbooks, online forums, academic papers, as well as formal proofs written in computer languages such as Lean, Isabelle, Coq or Metamath [2]. Large Language Models (LLMs) can ingest all such sources of knowledge in a unified manner, and provide a foundation for tasks in both formal and informal mathematics. Benchmarks of mathematical problem solving in natural language, such as GSM8k [10] and MATH [15], have measured rapid progress over the years, but they remain limited to problems where the final answer is a number, due to the challenge of assessing the correctness of mathematical arguments written in natural language. This difficulty is not a challenge in formal theorem proving, where we can automatically verify the correctness of proofs in arbitrarily high-level mathematics. But benchmarks of formal theorem proving (such as minif2f [46] and LeanDojo [44]), even with rapid advances in LLMs, have not yet witnessed the same breakthroughs. In fact, these benchmarks remain\n\nConjecturing Theorem Proving T├A:B Proof Search (MCTS) T-C:D   \nAxioms Conjecture 1: (1+1+0)=(1+1+0) F Conjecture 2: ∀(n: nat),(0+n)=n   \nConstrained   \nDecoding 串 Conjecture N : $\\forall ( n , m : n a t ) , n = 1 + m$   \nLanguage Model New training data for Policy $^ +$ Value $^ +$ Conjecturing\n\nfar from solved even though all of the theorems and problems in them are known mathematical facts, often already presented informally in publicly available training data.\n\nAn alternative vision of how AI might master mathematical reasoning sees mathematics through the lens of game playing, observing that the rules of the “game of mathematics” can be encoded in a variety of formal systems, such as dependent type theory, using a small set of axioms [24]. In principle, this can allows us to potentially borrow the successes of general game-playing agents, such as AlphaZero [36], that have achieved remarkable success in mastering complex games entirely from experience. Notably, AlphaZero achieves super-human performance in a range of games without leveraging any human examples. Instead, it learns entirely from experience given only an environment where it can play the game by itself. If this approach could be transported to mathematics, it would bypass the dependency on human examples, and allow us to explore mathematical domains — both known and new, without distinction — by utilizing large scale compute and the potential of axioms to produce infinite data.\n\nHowever, there is a crucial and often neglected difference between mathematics and traditional board games where game-playing AI has succeeded: mathematics is a game with intrinsic rewards [8]. Board games, such as Go or Chess, have a fixed starting configuration, and their rules determine the outcome of the game unequivocally. Mastering the game then amounts to learning a policy that optimizes for the extrinsic signal of winning. In theorem proving, a starting configuration is given by a theorem statement, and the correctness of a proof can be assessed objectively in a formal system. But the choice to work on a particular statement — a conjecture, before it is proved — is not given a priori by the rules of the game of mathematics. Instead, these goals come from the intrinsically motivated agents [8, 34] who are playing the game. Thus, a key skill developed by human mathematicians is to decide which conjectures are worth considering. In stark contrast, current benchmarks of mathematical reasoning abilities, both formal [46, 44] and informal [10, 15], all measure performance on an extrinsically defined set of problems, without space for further discovery.\n\nIn this paper, we make the first step towards creating intrinsically motivated mathematical agents by proposing MINIMO — Mathematics from Intrinsic Motivation —, a method to jointly learn conjecturing and theorem proving, starting from nothing but the axioms of a given mathematical domain, represented in dependent type theory [12]. We borrow inspiration from the literature of intrinsic motivation in Reinforcement Learning (RL) [8, 7, 33, 26], where agents can learn from interacting with an environment even when no specific goals are given. Intrinsic motivation objectives have been instrumental for RL in hard exploration environments, where rewards are too sparse to seek directly [5]. The sparsity of rewards is also a major challenge in theorem proving, making this connection especially attractive. We thus define the objective of conjecturing as generating new problems that are challenging for the current agent but still provable within its given search budget. Since the agent also learns from the solutions it finds, the conjecturer has to continuously increase the difficulty of the problems it generates.\n\nMINIMO performs both conjecturing and proof search with a Transformer [40] language model (LM), which starts randomly initialized. To sample conjectures even from a model that starts with no prior knowledge, we combine methods from type-directed program synthesis and constrained generation from language models, enabling us to get valid conjectures by construction — concretely, conjectures are simply terms of type prop, the type of mathematical propositions in our type theory. Then, we perform proof search in the Peano environment [28], which provides a finite action space for search in a dependent type theory, guiding search using the LM as a policy and value function. When a proof is found, proof search generates training data for improving the policy and values; it also provides data to improve conjecturing, since we then know how hard the problem was. We use this to alternate between conjecturing and theorem proving, in a self-improving loop. However, successful proofs are sparse. We thus adapt the idea of hindsight relabeling [1] to reinterpret failed trajectories as successful ones by rewriting their goals a posteriori. This significantly accelerates learning, allowing us to extract hundreds of new (true) conjectures, and their proofs, even from failed proof searches. In this way, even unprovable conjectures can be highly useful for the agent’s learning. We evaluate our system on three axiomatic mathematical domains—propositional logic, natural number arithmetic, and group theory—showing both that agents self-improve successfully in proving theorems in all domains. We also find that they improve at an extrinsic evaluation of theorems (from a classical textbook on logic [20], and the Lean Natural Number Game [6]), not given in training. In summary, we make the following contributions:\n\n• We introduce a method for conjecturing using LMs that generates valid conjectures by construction in an arbitrary theory, using constrained decoding and type-directed synthesis.   \n• We define a hindsight relabeling method that simultaneously generates training data for conjecturing and theorem proving.   \n• We combine these methods in a learning loop where a mathematical agent can self-improve in a given formal domain given only the axioms.   \n• We evaluate agents trained on axioms for propositional logic, group theory and arithmetic, showing that they improve in both intrinsic and extrinsic evaluations.\n\n# 2 Related Work\n\nOur work is primarily related to prior work on mathematical conjecturing, learning to prove theorems, and on intrinsic motivation in Reinforcement Learning. To the best of our knowledge, our work is the first attempt to unify insights from these areas for training mathematical reasoning agents.\n\nMathematical conjecturing. The task of discovering mathematical facts was the subject of the influential Automated Mathematician (AM), developed by Lenat in the 1970s [23]. AM was able to conjecture several known mathematical facts and concepts (such as the definition of prime numbers, and the unique factorization theorem). Unlike our system, AM did not aim at proving the conjectures it formulated — instead, it proposed and judged them based on a set of principles and on empirical evidence collected by AM itself. More recently, other works have revisited the idea of generating conjectures by training language models on human-written theorem statements [39, 41]. Unlike our approach, this relies on pre-training data, and does not readily extend to conjecturing in new domains.\n\nLearning to prove theorems from human data. A large body of recent work has used Large Language Models to guide formal theorem proving in a number of proof assistants, such as Lean [22], Isabelle [22, 18] and Metamath [31, 42]. Typically, these systems pre-train an LLM on large-scale Internet corpora and fine-tune on human-written formal mathematical proofs. Work on scaling laws for LLMs has shown that they generally improve when their parameter count and dataset size both increase in similar proportion. But the scarcity of formal proofs for training creates a challenge for this approach for learning formal theorem proving: even the largest datasets to date, such as the ProofPile, which aggregates libraries from 5 proof assistants, form relatively small datasets (e.g., 500MB of formal proofs on the ProofPile, contrasting to terabytes of Python code on Github).\n\nLearning to prove theorems from synthetic data. One recent success in automated mathematical reasoning was AlphaGeometry [38], which was highly effective in solving olympiad-level geometry problems. AlphaGeometry, like our method, was trained entirely on synthetic data. Crucial to its approach is a method for generating both problems and solutions using only the axioms of geometry and a domain-specific deductive closure solver. This allows AlphaGeometry to synthesize and train on hundreds of millions of problems: many orders of magnitude more than existing human-created datasets of mathematical problems. Our approach shares the goal of AlphaGeometry of only using data derived from the axioms of the domain, with the difference that our method (a) is agnostic to the underlying axiomatic system and (b) does not rely on a domain-specific solver. Another line of work, including TacticZero [43] and rlCoP [19], has explored learning to prove theorems from reinforcement learning only, in a tabula rasa fashion, but still using a human-written set of problems for training (and manually-engineered features, in the case of rlCoP).\n\nIntrinsic motivation We leverage inspiration from the literature on training reinforcement learning agents with intrinsic motivation objectives, allowing an agent to learn without pre-specified goals [26, 34, 33, 13, 27, 7, 3, 37]. Our setup is conceptually close to AMIGO [7], where agents attempt to generate challenging but achievable next goals. While AMIGO was demonstrated in a simple grid-world environment with a simple goal structure (any point in the grid gives a valid goal), we operate on a much richer setting, where the space of goals is unbounded — all conjectures in a formal mathematical theory. To sample conjectures, we use Synchromesh’s Constrained Semantic Decoding algorithm [29], and guide it with type constraints.\n\nSelf-improvement of language models A significant line of recent work has explored the idea that LLMs can “self-improve”: increase performance on a given task by fine-tuning on their own generated reasoning traces, which are selected by some criterion that ensures their quality. STaR [45] fine-tuned on reasoning traces that reached the correct answer on mathematical and multiple choice questions; LMSI [16] was able to obtain self-improvement on a question-only dataset, sampling multiple rationales and training on those that agree with the majority answer. More related to our work, but less explored, is the direction of having LLMs also generate their own problems for training: this has been explored for self-improving on solving programming problems [14, 37], where code execution provides signal about correctness. In MINIMO, since our problems are formal conjectures and our solutions are formal proofs, we can guarantee correctness in a much stronger form than self-generated test cases.\n\n# 3 MINIMO\n\nMost recent work on AI for mathematical reasoning assumes a target set of problems to be solved. We deviate from this paradigm by having the agent itself propose problems for it to try to solve and learn from. Our goal is to target increasingly harder problems in a given mathematical domain, where the domain is specified as a set of axioms given in dependent type theory.\n\nOur agent is represented by a language model, which we will use to encode (a) a proof search policy $\\pi _ { \\boldsymbol { \\theta } } ( a | \\boldsymbol { s } )$ , (b) a value function $V _ { \\pi } ( s )$ , and (c) a difficulty-conditioned conjecturer $P _ { \\theta } ( c \\mid d )$ , where $d$ is a discretized measure of difficulty and $c$ is a mathematical statement (a string). MINIMO consists of training both components in a loop that alternates between generating conjectures, trying to target hard but provable ones, and doing proof search, as we depict in Figure 1. As we describe in this section, proof search generates training data both for the conjecturer and the prover components — training on that data thus yields a self-improvement loop as the agent interacts with the environment. We now describe the first step in this loop: generating conjectures.\n\n# 3.1 Conjecturing\n\nWe aim to sample conjectures from a language model, conditioned on a target difficulty. By construction, an autoregressive LM gives a distribution over all strings. But if the LM does not have prior knowledge about the domain, it is unlikely to put non-negligible probability mass on valid mathematical statements. We now address the challenge of sampling valid conjectures.\n\nTo that end, our main insight is to leverage constrained decoding to obtain valid conjectures by construction. Our method will turn any language model — including a randomly initialized one, which is what we start with — into a probability distribution over strings that represent well-formed conjectures in dependent type theory over a given set of axioms. Ultimately, we will also train the LM to generate conjectures given a target difficulty. We use this ability to attempt to generate increasingly difficult problems for training, according to the agent’s current ability to prove theorems.\n\nTo reason about constraining the LM’s outputs, we leverage the abstraction of a completion engine, first introduced in the context of code generation with language models [29]. Assuming $\\mathcal { C }$ is the set of all valid conjectures, a completion engine will allow us to use any LM to sample strings from $\\mathcal { C }$ in an autoregressive fashion. Mathematically, $\\mathcal { C }$ is a function $f _ { \\mathcal { C } } : \\Sigma ^ { * }  \\mathcal { P } ( \\Sigma ^ { * } )$ , taking a string $s \\in \\Sigma ^ { * }$ and returning a set of strings $f _ { C } ( s ) ^ { 1 }$ . Intuitively, we will sample conjectures from our LM by constraining it to strings that can be generated with iterated calls to $f _ { \\mathcal { C } }$ . Concretely, we will start with $s = \" \"$ , and query $f _ { \\mathcal { C } } \\bar { ( } s )$ to obtain the set of valid ways to begin to state a conjecture. After we choose one of those $s _ { 1 } \\in f _ { \\mathcal { C } } ( s )$ , we can then query $f _ { \\mathcal { C } } ( s _ { 1 } )$ to obtain the valid ways to proceed, and repeat until we have a complete conjecture. Our main challenge here is to construct a suitable $f _ { \\mathcal { C } }$ that it is sound (all conjectures obtained by this procedure are valid) and complete (all valid conjectures can be obtained by this procedure). After we define $f _ { \\mathcal { C } }$ , we can sample from any LM while guaranteeing that the output will belong to $\\mathcal { C }$ by using the Constrained Semantic Decoding (CSD) algorithm [29].\n\nTo construct $f _ { \\mathcal { C } }$ , we analyze the minimal formulation of dependent type theory backing Peano [28] – essentially the classical Calculus of Constructions (CoC) [12]. In the CoC, mathematical propositions are constructions of type prop. Since generating a proposition might involve generating objects of arbitrary other types, depending on the axioms of the given domain, we will define a more general family of completion engines, $f _ { t }$ , which will constrain the LM to sample an object of an arbitrary type $t$ . At the end, we obtain $f _ { \\cal C } = f _ { p r o p }$ .\n\nTo sample an object of type $t$ , we leverage the simple grammar of terms in Peano [28] to guide a recursive type-directed synthesis algorithm. Syntactically, terms in Peano are defined by the grammar $e : : = { \\mathrm { t y p e ~ } } | { \\mathrm { ~ p r o p ~ } } | { \\mathrm { ~ } } x \\mid ( { \\bar { e } } e ) |$ (lambda $x : e , e )$ $\\mid [ ( x : e )  e ]$ . The first two production rules give the names of two built-in base types, type and prop. We then have variables, function application, lambda functions, and function types (denoted in square brackets). As conventional in type theory, let $\\Gamma$ be our typing context: a sequence of pairs of names and their types. For example, we might have $\\Gamma = [ n a t : t y \\bar { p e } , z : n a t , s u c \\bar { c } : [ n a t \\bar { \\to ~ } n a t ] ]$ , a context with three objects: a type nat, an object $\\boldsymbol { z }$ having that type, and a function succ from nat to nat. Given a context, to obtain an object of type $t$ , it suffices to consider the formation rules in $\\mathrm { C o C }$ to guide generation:\n\n• If our target type is $t = \\mathtt { t y p e }$ , we can generate either one of the names in $\\Gamma$ having type $t = \\mathtt { t y p e }$ (e.g., nat, for the example context above), or a function type.   \n• If our target type is $t = { \\tt p r o p }$ , we can generate either one of the objects in $\\Gamma$ having type prop, or a function type where the output type has type prop.   \n• If our target type is a function type, we must start by generating a square bracket; then, we (recursively) iteratively generate a type for the next parameter, or, if we already have at least one parameter, we can also generate the final output type.   \n• An object of any type $t$ can be formed by function application of a function $f$ chosen from $\\Gamma$ , provided that the output type of $f$ can be unified with $t$ .\n\nThese rules allow us to determine the possible valid tokens at any point during generation. Besides the base types type and prop, objects of all other types are either in $\\Gamma$ or the result of a function application. We use a recursive descent parser to parse the (incomplete) term we have so far (as originally done in [29]), and compute the target type $t$ at the current point in generation. Then, we enumerate the possible next tokens for the LM by using the rules above, return the union of the sets of choices allowed by each rules as the output of $f _ { t }$ .\n\nNote that this is a general procedure for searching for objects of any given type $t$ (i.e., inhabitants of that type). This is undecidable in general (theorem proving is a special case of type inhabitation), so this procedure might not terminate. Therefore, we set a maximum number of tokens $K$ (150, in our experiments), and ignore samples where the LM fails to generate a conjecture after $K$ tokens. In practice, we find the rejection rate for generating propositions to be low, $< 1 0 \\%$ of the generations.\n\nThe above procedure forces the LM to generate a valid conjecture, but the LM still assigns a distribution over those. During training, we also aim to generate conjectures that are provable, but hard to prove. The signal on both success and difficulty is generated by running proof search (as we describe next) and, in cases where a proof is found, measuring it’s log-likelihood under the current policy, which correlates with how many iterations MCTS takes to find the proof (see Appendix).\n\n# 3.2 Proof Search\n\nHaving a conjecture represented by a target type $t$ , we then perform proof search using Monte Carlo Tree Search (MCTS; [21], [42]), guided by a learned policy $\\pi _ { \\boldsymbol { \\theta } }$ and value function $V _ { \\theta }$ . We represent both $\\pi _ { \\boldsymbol { \\theta } }$ and $V _ { \\theta }$ using the same underlying language model that we use for conjecturing. We use Peano [28] as the environment for proof search. Peano exposes a finite action space, so we don’t need to generate actions using $\\pi _ { \\boldsymbol { \\theta } }$ — it suffices to be able to evaluate them. More precisely, at a given state $s$ where we have actions $a _ { i } ^ { ( s ) }$ available, we compute the distribution $\\pi _ { \\boldsymbol { \\theta } } ( a _ { i } ^ { ( s ) } | s )$ by evaluating the likelihood of each $a _ { i } ^ { ( s ) }$ as the completion to a string of the form \"STATE: $\\ll \\mathtt { S } \\gg$ ; POLICY:\". We read out the value of a state in a similar way — by considering the likelihood of 1 or 0 as being the next token following \"STATE: $\\ll \\mathbf { s } \\gg$ ; VALUE:\". In both cases, the probability of the next token is normalized over only the choices that can lead to a valid completion.\n\nStates and actions in Peano are similar to several other interactive theorem provers, such as Lean and Coq. The state consists of a set of typed objects, along with a set of open proof goals (which are types). Objects in the state whose type is a proposition type are interpreted as evidence for that proposition (either a proof or an assumption). Actions might add new objects to the state (e.g., take the successor of one existing natural number, or use an equality to rewrite an existing proposition into a new one), change the goal (by backward chaining), or both (e.g., if the goal is to prove $A  B$ , which is used to represent both logical implication and universal quantification, the intro action will add object of type $A$ to the state and change the goal to $B$ ).\n\nWhen proof search succeeds, we can extract examples to train both the policy and the value function. For the policy, we simply extract the actions that lead to the proof as language modeling examples, using the same format we use to query $\\pi _ { \\boldsymbol { \\theta } }$ . For the value function, we take the states in the subtree that complete the proof as examples with value 1, and a random set of other states as examples with value 0 for training. When proof search fails, however, this naïve procedure does not extract any training examples. This happens often at the beginning, since our model initially generates a large number of conjectures it cannot prove (either because they are false, or because naïve proof search times out). But forward actions in the proof search tree often construct proofs for other propositions, even if they are irrelevant for proving the original goal. We levarage this fact for generating training data by hindsight relabeling, as we describe next.\n\n# 3.3 Hindsight Relabeling\n\nEven a conjecture that fails to be proven can be highly informative about the domain. During proof search, forward actions that apply functions whose result type is a proposition type (e.g., concluding A from (and A B)) produce proofs, even when those proofs might not be useful for proving the original conjecture. In Reinforcement Learning, the well-known method of Hindsight Experience Replay [1] extracts training data for the policy from such failed trajectories by relabeling the trajectories with goals that were in fact achieved, as opposed to the original goal. For those alternative goals, the trajectory then represents a successful sequence of actions. We apply this idea to extract training examples for both the policy and value functions from proof search trees, by picking nodes after forward actions that produced a proof, and walking upwards in the tree until we find a backward action (since those change the goal). That path then becomes a successful trajectory after we relabel the goal. Two important steps to improve data quality are (1) we clean up the solutions by eliminating steps irrelevant for the proof of the new goal, and (2) we only add proofs of goals never seen before, to avoid oversampling trivial facts that are rediscovered extremely often (such as $0 = 0$ ).\n\nWe go one step further and observe that hindsight relabeling can also be useful for training the conjecturer. Concretely, the procedure we described above produces a set of proofs $p _ { i }$ for relabeled statements $g _ { i }$ . All of these statements are therefore true in the mathematical domain, and we use them as examples of true conjectures. As a concrete example, in arithmetic, the agent will often conjecture simple expressions such as $2 + 1 = 0$ . Most equalities generated at random will be false. However, by applying the Peano Axioms and facts about equality in succession, the agent eventually finds a proof of $2 + 1 = 3$ . Evaluating the likelihood of the proof under $\\pi _ { \\boldsymbol { \\theta } }$ gives a measure of the difficulty of this alternative statement for the current policy. This insight allows our agent to learn about hundreds of new unique, true statements in each proof search. As our experiments show, we find hindsight relabeling to be crucial for enabling the agent to steadily conjecture harder statements that it is able to prove.\n\n![](images/c62e29279be3d2d2a31ac499fbc08d87f9a6f79180644dddd35e9c5726259e5a.jpg)  \nFigure 2: Difficulty of proved conjectures found in each iteration of MINIMO, evaluate as the logprobability of the proof under the policy checkpoints after each iteration of the training loop (with standard error bands for runs with 3 random seeds).\n\n# 3.4 Training loop\n\nFinally, we tie all components together by alternating between (a) generating a batch of conjectures, (b) running proof search on them, (c) extracting training examples from the search trees, and finally (d) training the underlying LM using the standard cross-entropy objective. When a proof is found, either directly or by hindsight relabeling, we first compute a continuous difficulty metric of the statement by taking the log-likelihood of the proof under the current policy. To discretize difficulties, we then consider difficulty percentiles of the last batch of conjectures: we take the $10 \\%$ least likely proofs to be associated with “hard” conjectures, the $50 \\%$ most likely to be considered “trivial”, and the remaining are taken as “easy”. When put together, these components form a self-improving loop that starts only from the axioms of the given mathematical domain, as our experiments show.\n\n# 4 Experiments\n\nWe now evaluate MINIMO2 on three mathematical domains. We experiment with axiomatic systems for (a) propositional logic, (b) arithmetic, and (c) abstract groups. All the axioms are given in the Appendix. We then train agents over 5 iterations of conjecturing and theorem proving, generating 200 conjectures in each batch, running MCTS for proof search with 1000 expansions, and evaluate our agents guided by the following research questions:\n\nRQ1: Can our conjecturing method generate increasingly harder conjectures as training progresses? RQ2: Do agents improve at theorem proving as they train on their own generated problems? RQ3: Is hindsight relabeling effective at improving conjecturing and theorem proving? RQ4: Do our agents improve at proving an externally given set of human-selected theorems, even if these are not given during training?\n\nThe first three questions require intrinsic evaluations — they ask whether the learning dynamics of agents trained with MINIMO matches the desiderata of self-improving at both conjecturing and theorem proving while given only the axioms. The last question is an extrinsic assessment of what our agents learn — we evaluate whether the learning progresses in a way that aligns with an external criteria — namely, the proficiency of the agent at proving theorems from human sources (a textbook and a Lean game).\n\n# 4.1 Learning dynamics\n\nWe first investigate RQ1 and RQ2. Figure 2 shows the progression of difficulty across 5 iterations of the MINIMO learning loop (first conjecturing, then running proof search, and training on collected examples). Here, we evaluate the average log-likelihood (y-axis) of conjectures proven at each conjecturing iteration $\\mathbf { \\bar { X } }$ -axis) under the policy after each iteration (line color). Policy 0 is the initial (randomly initialized) policy, while subsequent policies were trained on the examples obtained during proof search, with hindsight relabeling, up to the previous iteration. Lower log-likelihood generally means harder conjectures (i.e., they tend to take more search iterations, see Appendix).\n\nDifficulty increases as training progresses (RQ1). Fixing an early policy, the log-likelihood of proofs under that policy steadily decreases across training iterations. This is reflected in the negative slope of difficulty across iterations when the policy is fixed. In particular, the policy at iteration 0 serves as a naïve search baseline, since it is essentially uniform. We observe that, as training progresses, this policy struggles increasingly more with each new batch of conjectures. The same pattern repeats for subsequent policies when we consider conjectures generated in future iterations, giving a generally downward trend in log-likelihood of the solutions for any given policy, showing that conjectures get increasingly more challenging. This provides positive evidence for answering RQ1: in all 3 domains, the conjecturer is able to increasingly propose harder provable conjectures as training progresses.\n\nProof policy improves as training progresses (RQ2). At each iteration, we sample a new set of unseen conjectures for training. From Figure 2, we see that later policies generally perform better than earlier ones, at a fixed conjecture iteration. This reflects the fact that each new policy assigns higher likelihood to the proofs, even for unseen conjectures at each iteration. For example, after training on the data generated from the first iteration, the policy on iteration 1 has higher log-likelihood for the proofs to the new conjectures found at iteration 1, when compared to the initial policy from iteration 0. This pattern repeats at each iteration, showing that the prover is also progressing and generalizing to unseen problems, though with diminishing gains in the final iterations. This suggests a positive answer to our second research question: our agents effectively self-improve in their ability to prove new statements.\n\nHindsight relabeling is necessary for joint self-improvement (RQ3). The results so far all used hindsight relabeling on all proof searches—successful or not—to extract training data for the policy and value function, as well as conjecturing. To evaluate whether our agents would still show the same continuous self-improvement regardless of the data efficiency gains from hindsight relabeling, Figure 3 compares agents trained with and without hindsight relabeling across 5 iterations over the same 3 axiomatic domains. Here, we look at the ability of the agent to achieve the goal of constantly proposing provable but challenging conjectures for itself. Ideally, the difficulty of conjectures should not fluctuate significantly during the course of training: we would like the agent to always find new challenging conjectures that it nevertheless still proves. We find that, in all 3 domains, the agent fails to achieve this goal when not trained with hindsight relabeling. Instead, as it trains on its own proofs, the agent’s conjectures fail to remain challenging—all provable conjectures end up with high log-likelihood as training progresses, and the conjecturer is unable to leave that regime. We attribute this to the volume of signal that the conjecturer receives: at each initial batch, only around $10 \\%$ of the conjectures are proven. When not using hindsight relabeling, the only feedback that the conjecturer recevies is that proof search timed out on these statements. On the other hand, with hindsight relabeling, even these failures lead the conjecturer to observe hundreds of actual true statements in each domain (along with their proofs), leading to better learning dynamics. This provides positive evidence for RQ3: hindsight relabeling significantly helps the agent to jointly improve in conjecturing and theorem proving—without it, training tends to collapse to by proposing only easy conjectures.\n\n# 4.2 Proving human-written theorems (RQ4)\n\nFinally, we evaluate whether our agent, trained only on problems that it proposes to itself, also improves in solving problems that are of interest to humans. Since our agent does not grow its library of theorems over time, starting every new proof from the axioms, a meaningful evaluation requires theorems that can be reasonably proven straight from the axioms, without lemmas. We thus use two human-written sources of theorems meeting this criterion, for the domains of propositional logic and arithmetic. For logic, we take the set of 35 theorems of propositional logic from Stephen Kleene’s textbook “Introduction to Metamathematics” [20]. Precisely, in Theorem 41, Kleene states (and proves a subset of) 35 useful statements of Propositional Logic (such as contraposition rules, commutativity and transitivity laws of logical connectives, and properties of double negation). For arithmetic, we use the Natural Number Game [6], a popular game used to introduce formal mathematics in the Lean theorem prover. We take levels of the game that are (a) theorems about natural numbers, and (b) do not refer to previous lemmas, only the axioms; this results in 10 levels spanning the Tutorial, Addition, and Multiplication worlds. We translate the statements into Peano, and evaluate our agents on their success rate on those problems within 2000 MCTS expansions.\n\n![](images/41a3e6e0b7ade7c04611291a91e7d7b2cb8c65dad83c445187b6de8dc949f5dc.jpg)  \nFigure 3: Difficulty of proved conjectures proposed in each iteration under the current policy at that same iteration, comparing when using and not using hindsight relabeling to generate new proofs and conjectures, with standard error bands for runs with 3 random seeds. Ideal behavior would be a flat line, representing constant relative difficulty. Hindsight significantly helps the agent conjecture propose harder problems.   \nFigure 4: Success rate of our agents at proving theorems from the “Introduction to Metamathematics” textbook and the Natural Number Game. As agents train on their own conjectures, they also improve at solving problems from these two human-written sources.\n\nFigure 4 shows the results. We find that, as our agents train on their self-generated problems, they steadily become more successful at proving theorems from both Kleene’s book and the Natural Number Game. This happens even though these theorems are not targeted during training, since our agent only uses its own conjectures. Four theorems in Propositional Logic are only proved after the last iteration of training: commutativity and transitivity of the “if and only if” logical connector, a law connecting double negation and implication $\\neg \\neg ( A \\Rightarrow B )$ , $\\neg \\neg A \\ \\vdash \\ \\neg \\neg B )$ , and the “currying law” of the conjunction — $( A \\land B ) \\Rightarrow { \\bar { C } } \\vdash { \\bar { A } } \\Rightarrow ( B \\Rightarrow C )$ . In the Natural Number game, only the final agent proves $\\forall n , \\forall m , s u c c ( n ) + m = s u c c ( n + m )$ , a theorem requiring induction on the correct variable $\\mathrm { ( m ) }$ and a non-trivial sequence of rewrites in the inductive case (we include the full proof in the Appendix). While admitedly small scale, these results suggest a positive answer to our last research question:\n\nSuccess rate on human-written theorem   \n0.7Theory 。Arithmetic oPropositionalLogic 0.6 0.5   \n  \n0 0 1 2 3 4 Checkpoint Iteration\n\nagents trained on their own conjectures can also improve at solving human-written problems, which are not given during training.\n\n# 5 Limitations and Conclusion\n\nWe present MINIMO: an approach to training agents for formal mathematical reasoning starting from only the axioms of a given domain. The agent jointly learns to propose challenging conjectures and to prove them. Our experiments show evidence of MINIMO improving its performance across training iterations. In the Groups domain, the average proof length it finds on generated conjectures (i.e., not found by hindsight) increased from 2.67 steps in the first iteration, when the model is randomly initialized, to 5 steps by iteration 4, with proofs for ‘hard’ conjectures growing from 3.67 to 6.10 steps. The longest proofs found grew from 4 steps to 9 steps from the first to last iteration. Similar trends appear in Propositional Logic (average length from 2.75 to 4.21 steps, longest proofs from 5 to 11 steps) and Arithmetic (average from 2.36 to 3.35 steps, longest from 4 to 7 steps).\n\nHowever, MINIMO currently has two crucial limitations that prevent it from (a) discovering deep mathematical theories, and (b) scaling up to large theories. First, even if the agent’s policy improves, its library remains fixed to the definitions and axioms that it starts with. Proofs that do not use lemmas (cut-free proofs in logic) can be exponentially longer than equivalent ones that do, and thus quickly grow beyond the reach of search. With this constraint, our agent most often finds harder conjectures by making the statements longer and more complicated. For example, in Groups, early conjectures include trivial statements like $e = e$ ; by the last iteration, the conjecture requiring the longest proof reads as $\\forall g \\in G$ , if $e = ( g ^ { - 1 } ) ^ { 2 }$ then $e ^ { \\bar { 2 } } = e ( e ( e ( ( g ^ { - 1 } ) ^ { 2 } ) ) )$ (proved in 9 steps). In contrast, human mathematicians develop deep theories by accumulating results and definitions along the way, in such a way that even very high-level results can be described succinctly at the right level of abstraction. Understanding how to bootstrap such cumulative learning in an agent (e.g., exploring notions of usefulness or interestingness, several of which have been posited [4, 11]) will be a key direction for future work.\n\nAnother bottleneck in our current setup is that a large library can cause the current action enumeration algorithm in Peano to become prohibitively slow (a finite action space can still become intractable). A method that scales unboundedly should incorporate some form of premise selection [17]. In past work, premise selection has either been based on symbolic heuristics or in learning useful premises from human-written proofs. We believe that developing a premise selection method that bootstraps together with the other learned components will be as important as understanding how to grow the agent’s library.\n\nTogether, lifting these limitations from our method might lead to a completely compute-bound, self-improving agent for formal mathematics capable of discovering deep mathematical theories starting only from basic axioms — the rules of the game of mathematics.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决人工智能实现通用数学推理能力的难题。数学推理是人工智能研究的重大挑战，具有自动验证数学证明正确性的潜力，对数学本身以及程序和硬件验证等领域有重要影响。然而，目前该领域尚未取得像通用游戏、图像生成或蛋白质折叠等领域那样的突破，现有的基于人类知识的数学推理方法，在自然语言数学问题和形式定理证明的基准测试中均未取得重大突破，且依赖人类示例。此外，当前数学推理能力的基准测试多基于外部定义的问题集，缺乏进一步发现的空间。\\n\\n**方法概述**\\n提出MINIMO方法，使用随机初始化的Transformer语言模型，结合类型导向的程序合成和语言模型的约束生成方法进行猜想采样，通过蒙特卡罗树搜索（MCTS）进行证明搜索，并采用后见之明重标记（hindsight relabeling）方法加速学习，实现从给定数学领域的公理开始，让智能体联合学习提出猜想和证明定理。\\n\\n**主要贡献与效果**\\n- 提出使用语言模型生成有效猜想的方法，保证了生成猜想的有效性，实验中生成命题的拒绝率低于10%。智能体能够从公理开始自举，在三个公理数学领域（命题逻辑、自然数算术和群论）的实验中，在生成真实且具有挑战性的猜想以及寻找证明方面实现自我提升。例如，在Groups领域，平均证明长度从第一次迭代的2.67步增加到第4次迭代的5步，“硬”猜想的证明长度从3.67步增长到6.10步，最长证明从4步增长到9步。\\n- 定义后见之明重标记方法，显著提高智能体在猜想和定理证明任务中的样本效率，使智能体能够在每次证明搜索中学习到数百个新的独特真命题，从失败的证明搜索中提取数百个新的（真实）猜想及其证明。\\n- 智能体在训练自己生成的问题时，在证明人类编写的定理（如Kleene教科书中的命题逻辑定理和Lean自然数游戏中的算术定理）的成功率稳步提高。如在命题逻辑中，四个定理在最后一次训练迭代后才被证明；在自然数游戏中，只有最终的智能体证明了 $\\forall n, \\forall m, succ(n) + m = succ(n + m)$。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n借鉴强化学习中内在动机的思想，将数学推理视为一个具有内在奖励的游戏。通过约束语言模型的输出，使其生成有效的猜想，并结合证明搜索的结果来评估猜想的难度，不断调整猜想的难度，实现智能体的自我提升。让智能体从给定数学领域的公理出发，自主提出问题并尝试解决，以实现自我提升。利用类型导向程序合成和约束生成方法从语言模型中采样有效猜想，以目标难度为条件生成具有挑战性且可证明的猜想。在证明搜索中使用蒙特卡罗树搜索（MCTS），并通过后见之明重标记从失败的证明搜索中提取训练数据，加速学习过程。\\n\\n**创新点**\\n先前的工作要么依赖大量人类产生的数学知识，要么在固定的问题集上进行训练，大多假设存在一组待解决的目标问题。MINIMO的创新之处在于：智能体自主提出问题进行学习，不依赖预定义的目标问题集；结合类型导向的程序合成和语言模型的约束生成方法，从随机初始化的语言模型中采样有效猜想，不依赖预训练数据，可扩展到新的数学领域；使用类型约束和Synchromesh的约束语义解码算法来采样猜想，在更丰富的数学理论目标空间中进行操作；引入后见之明重标记方法，从失败的证明搜索中提取训练数据，加速智能体的学习。\\n\\n**具体实现步骤**\\n1. **猜想生成**：利用完成引擎（completion engine）的抽象，结合约束语义解码（CSD）算法，约束语言模型生成有效猜想。定义更通用的完成引擎家族 $f_t$，约束语言模型采样任意类型 $t$ 的对象，最终得到 $f_{\\\\cal C} = f_{prop}$。利用Peano中项的简单语法，引导递归的类型导向合成算法，根据目标类型 $t$ 生成对象。结合类型导向程序合成和约束生成方法，使用Synchromesh的约束语义解码算法，通过定义完成引擎 $f_{C}$ 来约束语言模型生成有效猜想。设定最大令牌数 $K$ （实验中为150），忽略超过 $K$ 个令牌仍未生成猜想的样本。在训练过程中，通过运行证明搜索来评估猜想的可证明性和难度。\\n2. **证明搜索**：使用蒙特卡罗树搜索（MCTS），由学习到的策略 $\\pi_{\\\\boldsymbol{\\\\theta}}$ 和价值函数 $V_{\\\\theta}$ 引导，以Peano为证明搜索环境。评估动作的可能性和状态的值。当证明搜索成功时，提取训练数据来改进策略和价值函数。\\n3. **后见之明重标记**：从证明搜索树中提取训练数据，选择产生证明的前向动作后的节点，向上遍历树直到找到改变目标的后向动作，将该路径重新标记为成功轨迹。清理解决方案，只添加从未见过的目标的证明。通过重标记失败的轨迹，将其视为成功的轨迹，用于训练策略和价值函数，同时为猜想器提供真实猜想的示例。同时，将重标记后的证明作为真猜想的示例用于训练猜想生成器。\\n4. **训练循环**：交替进行生成一批猜想、对其进行证明搜索、从搜索树中提取训练示例，最后使用标准交叉熵目标训练语言模型。根据证明的对数似然计算猜想的难度，并离散化难度。将最后一批猜想的难度分为三个等级：10% 最不可能证明的猜想为“难”，50% 最可能证明的猜想为“琐碎”，其余为“易”。\\n\\n**案例解析**\\n在算术中，智能体常随机猜想简单表达式，如 $2 + 1 = 0$，大多数随机生成的等式是错误的。但通过应用Peano公理和等式事实，最终找到了 $2 + 1 = 3$ 的证明。评估该证明在当前策略下的对数似然度，为智能体提供了该替代陈述的难度度量。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文未明确提及用于对比的核心基线模型。但从相关工作可知，涉及到的对比方向包括基于人类数据的定理证明方法（如在Lean、Isabelle和Metamath等证明助手中使用大语言模型引导定理证明）、基于合成数据的定理证明方法（如AlphaGeometry）以及依赖预训练数据生成猜想的方法等。从内容还可推测，初始随机策略可作为一种基线，以及不使用后见之明重标记的训练方式也可作为对比。\\n\\n**性能对比**\\n*   **在 [猜想难度] 指标上：** 随着训练的进行，固定早期策略时，证明的对数似然度稳步下降，表明猜想越来越难。例如，以初始随机策略（策略0）为基线，随着训练迭代，该策略在处理新一批猜想时越来越困难。这体现了MINIMO在猜想生成能力上的自我提升，优于初始的随机搜索策略。使用后见之明重标记的方法明显优于不使用该方法的情况，不使用时智能体的猜想难以保持挑战性，所有可证明的猜想的对数似然度在训练过程中都较高，智能体提出的可证明猜想的难度逐渐降低，最终只能提出简单的猜想。\\n*   **在 [证明能力] 指标上：** 后期策略在证明未见过的猜想时通常比早期策略表现更好。例如，在第1次迭代训练后，第1次迭代的策略对第1次迭代中找到的新猜想的证明的对数似然度高于初始策略。对于每个迭代中生成的新猜想，后续策略为证明分配的对数似然值更高，说明证明策略在不断进步并能推广到未见过的问题。这表明MINIMO在定理证明能力上实现了自我提升。\\n*   **在 [证明人类编写的定理的成功率] 指标上：** 随着智能体在自我生成的问题上进行训练，其在证明Kleene教科书中的命题逻辑定理和Lean自然数游戏中的算术定理的成功率稳步提高。虽然这些定理在训练中未被专门针对，但智能体仍能不断提升解决这些问题的能力，体现了MINIMO的泛化能力。而未提及其他对比方法在这些定理上的表现，但可推测不使用后见之明重标记可能效果不佳。\",\n    \"keywords\": \"### 关键词\\n\\n- 数学推理 (Mathematical Reasoning, N/A)\\n- 猜想生成 (Conjecture Generation, N/A)\\n- 定理证明 (Theorem Proving, N/A)\\n- 内在动机 (Intrinsic Motivation, N/A)\\n- 后见之明重标记 (Hindsight Relabeling, N/A)\\n- MINIMO (Mathematics from Intrinsic Motivation, MINIMO)\"\n}"
}