{
    "source": "Semantic Scholar",
    "arxiv_id": "2406.12616",
    "link": "https://arxiv.org/abs/2406.12616",
    "pdf_link": "https://arxiv.org/pdf/2406.12616.pdf",
    "title": "Learning diffusion at lightspeed",
    "authors": [
        "Antonio Terpin",
        "Nicolas Lanzetti",
        "Florian D√∂rfler"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-06-18",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 9,
    "influential_citation_count": 2,
    "institutions": [
        "ETH Z√ºrich"
    ],
    "paper_content": "# Learning diffusion at lightspeed\n\nAntonio Terpin ETH Z√ºrich aterpin@ethz.ch\n\nNicolas Lanzetti ETH Z√ºrich lnicolas@ethz.ch\n\nMart√≠n Gadea ETH Z√ºrich mgadea@ethz.ch\n\nFlorian D√∂rfler ETH Z√ºrich dorfler@ethz.ch\n\n# Abstract\n\nDiffusion regulates numerous natural processes and the dynamics of many successful generative models. Existing models to learn the diffusion terms from observational data rely on complex bilevel optimization problems and model only the drift of the system. We propose a new simple model, JKOnet‚àó, which bypasses the complexity of existing architectures while presenting significantly enhanced representational capabilities: JKOnet‚àó recovers the potential, interaction, and internal energy components of the underlying diffusion process. JKOnet‚àó minimizes a simple quadratic loss and outperforms other baselines in terms of sample efficiency, computational complexity, and accuracy. Additionally, JKOnet‚àó provides a closed-form optimal solution for linearly parametrized functionals, and, when applied to predict the evolution of cellular processes from real-world data, it achieves state-of-the-art accuracy at a fraction of the computational cost of all existing methods. Our methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions.\n\nSource code: https://github.com/antonioterpin/jkonet-star\n\n# 1 Introduction\n\nDiffusion processes govern the homeostasis of biological systems [40], stem cells reprogramming [20, 36], and the learning dynamics of diffusion models [16, 22, 54] and transformers [19, 52]. The diffusion process of interest often originates from three quantities: a drift term due to a potential field, the interaction with other particles, and a stochastic term. If these three components are known, predictions follow from simple forward sampling [27] or the recent work in optimization in the probability space [1, 3, 11, 25, 34, 38, 41]. In this paper, we consider the case when the diffusion process is unknown, and we seek to learn its representation from observational data. The problem has been addressed when the trajectories of the individual particles are known [5, 32], but it is often the case that we only have ‚Äúpopulation data‚Äù. For instance, single-cell RNA sequencing techniques enabled the collection of large quantities of data on biological systems [35], but the observer cannot access the trajectories of individual cells since measurements are destructive [20, 36]. The most promising avenue to circumvent the lack of particle trajectories is the Jordan-Kinderlehrer-Otto (JKO) scheme [24] which predicates that the particles as a whole move to decrease an aggregate energy, while not deviating too much from the current configuration. However, the JKO scheme entails an optimization problem in the probability space. Thus, the problem of finding the energy functional that minimizes a prediction error (w.r.t. observational data) takes the form of a computationallychallenging infinite-dimensional bilevel optimization problem, whereby the upper-level problem is the minimization of the prediction error and the lower-level problem is the JKO scheme. Recent work [1, 9] exploits the theory of optimal transport and in particular Brenier‚Äôs theorem [7] to attack this bilevel optimization problem, a model henceforth referred to as JKOnet. Despite promising initial results in [9], this complexity undermines scalability, stability, and generality of the model.\n\n![](images/b99640521ef0134f892970770a8c900178a1cee5eb4841fa3341d11d4f675f8f.jpg)  \nFigure 1: Given a sequence of snapshots $( \\mu _ { 0 } , \\dots , \\mu _ { T } )$ of a population of particles undergoing diffusion, we want to find the parameters $\\theta$ of the parametrized energy function $J _ { \\theta }$ that best explains the particles evolution. Given $\\theta$ , the effects mismatch is the Wasserstein distance between the observed trajectory and the predicted trajectory obtained iteratively solving the JKO step with $J _ { \\theta }$ . The first-order optimality condition in [30] applied to the JKO step suggests that the ‚Äúgradient‚Äù of $J _ { \\theta }$ with respect to each $\\hat { \\mu } _ { t }$ vanishes at optimality, i.e., for $\\hat { \\mu _ { t } } = \\mu _ { t }$ . For $\\begin{array} { r } { J _ { \\theta } ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } \\bar { V } _ { \\theta } ( x ) \\mathrm { d } \\mu ( x ) } \\end{array}$ , this condition is depicted on the right. The gradient (dashed blue arrows) of the true $V$ (level curves in dashed blue) at each observed particle $\\boldsymbol { x } _ { i } ^ { t + 1 }$ (blue circles) in the next snapshot $\\mu _ { t + 1 }$ opposes the displacement (dotted red arrows) from a particle $\\ v x _ { i } ^ { t }$ (red triangles) in the previous snapshot $\\mu _ { t }$ . Instead, the gradient (solid green arrows) of the estimated $V _ { \\theta }$ (level curves in solid green) at each observed particle $\\boldsymbol { x } _ { i } ^ { t + 1 }$ (square) does not oppose the displacement from a particle $\\boldsymbol { x } _ { i } ^ { t }$ in the previous snapshot $\\mu _ { t }$ . This mismatch in the causes of the diffusion process is what JKOnet‚àó minimizes.\n\nFurthermore, to be practical, it is limited to learning only potential energies, modelling the underlying physics only partially. Alternatively, [10, 43] learn directly the transport map describing the evolution of the population (i.e., the effects), bypassing the representation of the underlying energy functional (i.e., the causes). Motivated by robustness, interpretability, and generalization, here we seek a method to learn the causes. In [23, 42], the authors try to learn a geometry that explains the observed transport maps. Unfortunately, the cost between two configurations along a cost-minimizing trajectory is often not a metric [47]. Other attempts include recurrent neural networks [21], neural ODEs [15], and Schr√∂dinger bridges [12, 28].\n\nContributions. We study the first-order necessary optimality conditions for the JKO scheme, an optimization problem in the probability space, and show that these conditions can be exploited to learn the energy functional governing the underlying diffusion process from population data, effectively bypassing the complexity of the infinite-dimensional bilevel optimization problem. We provide a closed-form solution in the case of linearly parametrized energy functionals and a simple, interpretable, and efficient algorithm for non-linear parametrizations. Via exhaustive numerical experiments, we show that, in the case of potential energies only, JKOnet‚àó outperforms the state-of-the-art in terms of solution quality, scalability, and computational efficiency and, in the until now unsolved case of general energy functionals, allows us to also learn interaction and internal energies that explain the observed population trajectories. When applied to predict the evolution of cellular processes, it achieves state-of-the-art accuracy at a fraction of the computational cost. Figure 1 shows an overview of our method, detailed in Section 3.\n\n# 2 Diffusion processes via optimal transport\n\n# 2.1 Preliminaries\n\nThe gradient of $\\rho : \\mathbb { R } ^ { d }  \\mathbb { R }$ is $\\pmb { \\nabla } \\rho \\in \\mathbb { R } ^ { d }$ and the Jacobian of $\\phi : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { n }$ is $\\pmb { \\nabla } \\phi \\in \\mathbb { R } ^ { n \\times d }$ . We say that $f :  { \\mathbb { R } ^ { d } } \\to  { \\mathbb { R } }$ has bounded Hessian if $\\| \\nabla ^ { 2 } f ( x ) \\| \\leq C$ for some $C > 0$ (and some matrix norm $\\left\\| \\cdot \\right\\| )$ . The divergence of $F : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { n }$ is $\\nabla \\cdot F$ and its laplacian is $\\nabla ^ { 2 } F$ . The identity function is $\\mathrm { I d } : \\mathbb { R } ^ { d } \\stackrel { \\textstyle \\cdot } { \\to } \\mathbb { R } ^ { d }$ , $\\operatorname { I d } ( x ) = x$ . We denote by $\\mathcal { P } ( \\mathbb { R } ^ { d } )$ the space of (Borel) probability measures over $\\mathbb { R } ^ { d }$ with finite second moment. For $\\mu \\in \\mathcal P ( \\mathbb { R } ^ { d } )$ , $\\operatorname { s u p p } ( \\mu )$ is its support. The Dirac‚Äôs delta measure at $\\boldsymbol { x } \\in \\mathbb { R } ^ { d }$ , is $\\delta _ { x }$ . All the functions are assumed to be Borel, and for $f : \\mathbb { R } ^ { d }  \\mathbb { R }$ , $\\begin{array} { r } { \\int _ { \\mathbb { R } ^ { d } } f ( x ) \\mathrm { d } \\mu ( x ) } \\end{array}$ is the (Lebesgue) integral of $f$ w.r.t. $\\mu$ . If $\\mu$ is absolutely continuous w.r.t. the Lebesgue measure, $\\mu \\ll \\mathrm { d } x$ , then it admits a density $\\rho : \\mathbb { R } ^ { d }  \\mathbb { R } _ { \\geq 0 }$ , and the integral becomes $\\begin{array} { r } { \\int _ { \\mathbb { R } ^ { d } } f ( x ) \\rho ( x ) \\mathrm { d } x } \\end{array}$ . The pushforward of $\\mu$ via a (Borel) map $f : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { d }$ is the probability measure $f _ { \\# } \\mu$ defined by $( f _ { \\# } \\mu ) ( B ) = \\mu ( f ^ { - 1 } ( B ) )$ ; when $\\mu$ is empirical with $N$ , $\\begin{array} { r } { \\mu = { \\frac { 1 } { N } } \\sum _ { i = 1 } ^ { N } \\delta _ { x _ { i } } } \\end{array}$ , then $\\begin{array} { r } { f _ { \\# } \\mu = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\delta _ { f ( x _ { i } ) } } \\end{array}$ . Given $\\mu , \\nu \\in \\mathcal P ( \\mathbb { R } ^ { d } )$ , we say that a probability measure $\\gamma \\in \\mathcal { P } ( \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } )$ is a transport plan (or coupling) between $\\mu$ and $\\nu$ if its marginals are $\\mu$ and $\\nu$ . We denote the set of transport plans between $\\mu$ and $\\nu$ by $\\Gamma ( \\mu , \\nu )$ . The Wasserstein distance between $\\mu$ and $\\nu$ is\n\n$$\nW _ { 2 } ( \\mu , \\nu ) : = \\left( \\operatorname* { m i n } _ { \\gamma \\in \\Gamma ( \\mu , \\nu ) } \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\| x - y \\| ^ { 2 } \\mathrm { d } \\gamma ( x , y ) \\right) ^ { \\frac { 1 } { 2 } } .\n$$\n\nWhen $\\mu$ and $\\nu$ are discrete, (1) is a linear program. If, additionally, they have the same number of particles, the optimal transport plan is $\\gamma = ( \\operatorname { I d } , T ) _ { \\# } \\mu$ for some (transport) map $T : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { d }$ [39]. When $\\mu$ is absolutely continuous, $\\boldsymbol { \\gamma } = ( \\operatorname { I d } , \\pmb { \\nabla } \\psi ) _ { \\# } \\mu$ for some convex function $\\psi$ [7].\n\n# 2.2 The JKO scheme\n\nMany continuous-time diffusion processes can be modeled by partial differential equations (PDEs) or stochastic differential equations (SDEs):\n\nExample 2.1 (Fokker-Planck). The Fokker-Planck equation,\n\n$$\n\\frac { \\partial \\rho ( t , x ) } { \\partial t } = \\nabla \\cdot ( \\nabla V ( x ) \\rho ( t , x ) ) + \\beta \\nabla ^ { 2 } \\rho ( t , x ) ,\n$$\n\ndescribes the time evolution of the distribution $\\rho$ of a set of particles undergoing drift and diffusion,\n\n$$\n\\mathrm { d } X ( t ) = - \\nabla V ( X ( t ) ) \\mathrm { d } t + \\sqrt { 2 \\beta } \\mathrm { d } W ( t ) ,\n$$\n\nwhere $X ( t )$ is the state of the particle, $V ( x )$ the driving potential, and $W ( t )$ the Wiener process.\n\nThe pioneering work of Jordan, Kinderlehrer, and Otto [24], related diffusion processes to energyminimizing trajectories in the Wasserstein space (i.e., probability space endowed with the Wasserstein distance), providing a discrete-time counterpart of the diffusion process, the JKO scheme,\n\n$$\n\\mu _ { t + 1 } = \\mathop { \\mathrm { a r g m i n } } _ { \\mu \\in \\mathcal { P } ( \\mathbb { R } ^ { d } ) } J ( \\mu ) + \\frac { 1 } { 2 \\tau } W _ { 2 } ( \\mu , \\mu _ { t } ) ^ { 2 } ,\n$$\n\nwhere $J : \\mathcal { P } ( \\mathbb { R } ^ { d } )  \\mathbb { R } \\cup \\{ + \\infty \\}$ is an energy functional and $\\tau > 0$ is the time discretization.\n\nExample 2.2 (Fokker-Plank as a Wasserstein gradient flow). The Fokker-Plank equation (2) results from the continuous-time limit (i.e., $\\tau  0$ ) of the JKO scheme (3) for the energy functional\n\n$$\nJ ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } V ( x ) \\mathrm { d } \\mu ( x ) + \\beta \\int _ { \\mathbb { R } ^ { d } } \\rho ( x ) \\log ( \\rho ( x ) ) \\mathrm { d } x \\quad w i t h \\quad \\mathrm { d } \\mu ( x ) = \\rho ( x ) \\mathrm { d } x .\n$$\n\n# 2.3 Challenges\n\nSection 2.2 suggests that we can interpret the problem of learning diffusion processes as the problem of learning the energy functional $J$ in (3). Specifically, the setting is as follows: We have access to sample populations $\\mu _ { 0 } , \\mu _ { 1 } , \\ldots , \\mu _ { T }$ , and we want to learn the energy functional governing their dynamics. A direct approach to tackle the inverse problem is a bilevel optimization, used, among others, for the model JKOnet in [9]. This approach bases on the following two facts. First, by Brenier‚Äôs theorem, the solution of (3), $\\mu _ { t + 1 }$ , can be expressed1 as the pushforward of $\\mu _ { t }$ via the gradient of a convex function $\\psi _ { t } : \\mathbb { R } ^ { d }  \\mathbb { R }$ and, thus,\n\n$$\nW _ { 2 } ( \\mu _ { t } , \\mu _ { t + 1 } ) ^ { 2 } = \\int _ { \\mathbb { R } ^ { d } } \\| x - \\nabla \\psi _ { t } ( x ) \\| ^ { 2 } \\mathrm { d } \\mu _ { t } ( x ) .\n$$\n\nSecond, the optimization problem (3) is equivalently written as\n\n$$\n\\operatorname * { a r g m i n } _ { \\psi _ { t } \\in C } J ( \\nabla \\psi _ { t \\neq \\mu _ { t } } \\mu _ { t } ) + \\frac { 1 } { 2 \\tau } \\int _ { \\mathbb { R } ^ { d } } \\left\\| x - \\nabla \\psi _ { t } ( x ) \\right\\| ^ { 2 } \\mathrm { d } \\mu _ { t } ( x ) ,\n$$\n\nwhere $C$ is the class of continuously differentiable convex functions from $\\mathbb { R } ^ { d }$ to $\\mathbb { R }$ . Therefore, the learning task can be cast into the following bilevel optimization problem, which minimizes the discrepancy between the observations $( \\mu _ { t } )$ and the predictions of the model $( \\hat { \\mu } _ { t } )$ :\n\n$$\n\\begin{array} { l } { \\displaystyle \\operatorname* { m i n } _ { J } \\displaystyle \\sum _ { t = 1 } ^ { T } W _ { 2 } ( \\hat { \\mu } _ { t } , \\mu _ { t } ) ^ { 2 } } \\\\ { \\mathrm { s . t . } \\ \\hat { \\mu } _ { 0 } = \\mu _ { 0 } , \\quad \\hat { \\mu } _ { t + 1 } = \\nabla \\psi _ { t } ^ { * } \\hat { \\mu } _ { t } , } \\\\ { \\displaystyle \\psi _ { t } ^ { * } : = \\underset { \\psi \\in C } { \\mathrm { a r g m i n } } J ( \\nabla \\psi _ { t \\neq } \\hat { \\mu } _ { t } ) + \\frac { 1 } { 2 \\tau } \\displaystyle \\int _ { \\mathbb R ^ { d } } \\left\\| x - \\nabla \\psi _ { t } ( x ) \\right\\| ^ { 2 } \\mathrm { d } \\hat { \\mu } _ { t } ( x ) . } \\end{array}\n$$\n\nA practical implementation of the above requires a parametrization of both the transport map and the energy functional. The former problem has been tackled via input convex neural network (ICNN) parametrizing $\\psi _ { t }$ [2, 8] or via the ‚ÄúMonge gap‚Äù [51]. The second problem is only addressed for energy functional of the form $\\begin{array} { r } { J ( \\mu ) = \\tilde { \\int _ { \\mathbb { R } } } \\bar { V _ { \\theta } ( x ) } \\mathrm { d } \\mu ( \\bar { x } ) } \\end{array}$ , without interaction and internal energies, where $V _ { \\theta }$ is a non-linear function approximator [9].\n\nChallenges. This approach suffers from two major limitations. First, bilevel optimization problems are notoriously hard and we should therefore expect (4) to be computationally challenging. Second, most energy functionals are not potential energies but include interactions and internal energy terms as well. Although it is tempting to include other terms in the energy functional $J$ (e.g., parametrizing interaction and internal energies), the complexity of the bilevel optimization problem renders such an avenue viable only in principle.\n\n# 3 Learning diffusion at lightspeed\n\nOur methodology consists of replacing the optimization problem (3) with its first-order necessary conditions for optimality. This way, we bypass its computational complexity which, ultimately, leads to the bilevel optimization problem (4). Perhaps interestingly, our methodology for learning diffusion is based on first principles: whereas e.g. [9] minimizes an error on the effects (the predictions), we minimize an error on the causes (the energy functionals driving the diffusion process); see Figure 1. As we detail in Section 4, the resulting learning algorithms are significantly faster and more effective.\n\n# 3.1 Intuition\n\nTo start, we illustrate our idea in the Euclidean case (i.e., $\\mathbb { R } ^ { d }$ ) and later generalize it to the probability space (i.e., $\\mathcal { P } ( \\mathbb { R } ^ { d } ) )$ . Consider the problem of learning the energy functional $J : \\mathbb { R } ^ { d }  \\mathbb { R } \\cup \\{ + \\infty \\}$ of the analog of the JKO scheme in the Euclidean space, the proximal operator\n\n$$\nx _ { t + 1 } = \\underset { x \\in \\mathbb { R } ^ { d } } { \\operatorname { a r g m i n } } J ( x ) + \\frac { 1 } { 2 \\tau } \\| x - x _ { t } \\| ^ { 2 } .\n$$\n\nUnder sufficient regularity, we can replace (5) by its first-order optimality condition\n\n$$\n\\nabla J ( x _ { t + 1 } ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) = 0 .\n$$\n\nGiven a dataset $( x _ { 0 } , x _ { 1 } , \\dots , x _ { T } )$ , we seek the energy functional that best fits the optimality condition:\n\n$$\n\\operatorname* { m i n } _ { J } \\sum _ { t = 0 } ^ { T - 1 } \\bigg \\| \\nabla J ( x _ { t + 1 } ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\bigg \\| ^ { 2 } .\n$$\n\nIn the probability space, we can proceed analogously and replace (3) with its first-order optimality conditions. This analysis, which is based on recent advancements in optimization in the probability space [30], allows us to formulate the learning task as a single-level optimization problem.\n\n# 3.2 Potential energy\n\nConsider initially the case where the energy functional is a potential energy, for $V : \\mathbb { R } ^ { d }  \\mathbb { R }$ ,\n\n$$\nJ ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } V ( x ) \\mathrm { d } \\mu ( x ) .\n$$\n\nThe following proposition is the counterpart of (6) in ${ \\mathcal { P } } ( \\mathbb { R } ^ { d } )$ :\n\nProposition 3.1 (Potential energy). Assume $V$ is continuously differentiable, lower bounded, and has a bounded Hessian. Then, the JKO scheme (3) has an optimal solution $\\mu _ { t + 1 }$ and, $i f \\mu _ { t + 1 }$ is optimal for (3), then there is an optimal transport plan $\\gamma _ { t }$ between $\\mu _ { t }$ and $\\mu _ { t + 1 }$ such that\n\n$$\n\\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\bigg \\| \\nabla V ( x _ { t + 1 } ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\bigg \\| ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) = 0 .\n$$\n\nProposition 3.1 is by all means the analog of (6), since for the integral to be zero, $\\nabla V ( x _ { t + 1 } ) +$ $\\begin{array} { r } { \\frac { 1 } { \\tau } ( \\bar { x _ { t + 1 } } - x _ { t } ) = 0 } \\end{array}$ must hold for all $( x _ { t } , x _ { t + 1 } ) \\in \\mathrm { s u p p } ( \\gamma _ { t } )$ . Since the collected population data $\\mu _ { 0 } , \\mu _ { 1 } , \\dots , \\mu _ { T }$ are not optimization variables in the learning task, the optimal transport plan $\\gamma _ { t }$ can be computed beforehand. Thus, we can learn the energy functional representation by minimizing over a class of continuously differentiable potential energy functions the loss function\n\n$$\n\\sum _ { t = 0 } ^ { T - 1 } \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\Big \\lVert \\nabla V ( x _ { t + 1 } ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\Big \\rVert ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) .\n$$\n\n# 3.3 Arbitrary energy functionals\n\nConsider now the general case where the energy functional consists of a potential energy (with the potential function $\\mathbf { \\bar { \\boldsymbol { V } } } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { }$ ), interaction energy (with interaction kernel $U : \\mathbb { R } ^ { d } \\overset { \\cdot \\cdot } {  } \\mathbb { R }$ ), and internal energy (expressed as the entropy weighted by $\\beta \\in \\mathbb { R } _ { \\geq 0 }$ ):\n\n$$\nJ ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } V ( x ) \\mathrm { d } \\mu ( x ) + \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } U ( x - y ) \\mathrm { d } ( \\mu \\times \\mu ) ( x , y ) + \\beta \\int _ { \\mathbb { R } ^ { d } } \\rho ( x ) \\log ( \\rho ( x ) ) \\mathrm { d } x .\n$$\n\nThe first-order necessary optimality condition for the JKO scheme then reads as follows.\n\nProposition 3.2 (General case). Assume $V$ and $U$ are continuously differentiable, lower bounded, and have a bounded Hessian. Then, the JKO scheme (3) has an optimal solution $\\mu _ { t + 1 }$ which is absolutely continuous with density $\\rho _ { t + 1 }$ and, $i f \\mathrm { d } \\mu _ { t + 1 } ( x ) = \\rho _ { t + 1 } ( x ) \\mathrm { d } x$ is optimal for (3), then there is an optimal transport plan $\\gamma _ { t }$ between $\\mu _ { t }$ and $\\mu _ { t + 1 }$ such that\n\n$$\n\\begin{array} { r l } & { 0 = \\displaystyle \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\bigg \\| \\nabla V ( x _ { t + 1 } ) + \\int _ { \\mathbb { R } ^ { d } } \\nabla U ( x _ { t + 1 } - x _ { t + 1 } ^ { \\prime } ) \\mathrm { d } \\mu _ { t + 1 } ( x _ { t + 1 } ^ { \\prime } ) } \\\\ & { \\qquad + \\left. \\beta \\frac { \\nabla \\rho _ { t + 1 } \\left( x _ { t + 1 } \\right) } { \\rho _ { t + 1 } \\left( x _ { t + 1 } \\right) } + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\right\\| ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) . } \\end{array}\n$$\n\nThus, Proposition 3.2 suggests that the energy functional can be learned by minimizing over a class of continuously differentiable potential and internal energy functions and $\\beta \\in \\mathbb { R } _ { \\geq 0 }$ the loss function\n\n$$\n\\begin{array} { r l r } {  { \\sum _ { t = 0 } ^ { T - 1 } \\int _ {  { \\mathbb { R } } ^ { d } \\times  { \\mathbb { R } } ^ { d } } \\bigg \\| \\nabla V ( x _ { t + 1 } ) + \\int _ {  { \\mathbb { R } } ^ { d } } \\nabla U ( x _ { t + 1 } - x _ { t + 1 } ^ { \\prime } ) \\mathrm { d } \\mu _ { t + 1 } ( x _ { t + 1 } ^ { \\prime } ) } } \\\\ & { } & { \\qquad + \\beta \\frac { \\nabla \\rho _ { t + 1 } ( x _ { t + 1 } ) } { \\rho _ { t + 1 } ( x _ { t + 1 } ) } + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\bigg \\| ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) . } \\end{array}\n$$\n\nRemark 3.3. We generalize the setting to time-varying energies in Sectio\nn 4.4 and in Appendix B.\n\n# 3.4 Parametrizations\n\nFor our model JKOnet‚àó, we parametrize the energy functional at a measure $\\mathrm { d } \\mu = \\rho ( { \\boldsymbol { x } } ) \\mathrm { d } { \\boldsymbol { x } }$ as follows:\n\n$J _ { \\theta } ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } V _ { \\theta _ { 1 } } ( x ) \\mathrm { d } \\mu ( x ) + \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } U _ { \\theta _ { 2 } } ( x - y ) \\mathrm { d } ( \\mu \\times \\mu ) ( x , y ) + \\theta _ { 3 } \\int _ { \\mathbb { R } ^ { d } } \\rho ( x ) \\log ( \\rho ( x ) ) \\mathrm { d } x ,$ where $\\theta _ { 1 } , \\theta _ { 2 } \\in \\mathbb { R } ^ { n } , \\theta _ { 3 } \\in \\mathbb { R }$ , and we set $\\boldsymbol { \\theta } = \\left[ \\boldsymbol { \\theta } _ { 1 } ^ { \\top } , \\boldsymbol { \\theta } _ { 2 } ^ { \\top } , \\boldsymbol { \\theta } _ { 3 } ^ { \\top } \\right] ^ { \\top } \\in \\mathbb { R } ^ { 2 n + 1 }$ .\n\n<html><body><table><tr><td>Model</td><td>FLOPS per epoch Seq. op. per particle</td><td colspan=\"3\">Generality</td></tr><tr><td>JKOnet</td><td>O(T(DNd + N21g(2)Ôºâ O(T (DNd +2g(N)</td><td>V(x) /</td><td>Œ≤ U(x)</td></tr><tr><td>W/o TF JKOnet</td><td>108 Œµ</td><td></td><td>√ó √ó</td></tr><tr><td>W/ TF JKOnet</td><td>O (T (DNd + N21g(N) O (DNd + N210g(N)</td><td></td><td>√ó √ó</td></tr><tr><td>W/MG w/o TF JKOnet</td><td>Œµ2 (TD (Nd + N210g(N) O (TD (Nd + N2 g(N) (TD(Nd + N2log(N) O (D (Nd + N¬≤1og(N)</td><td>√ó</td><td>√ó √ó</td></tr><tr><td>W/ MG, TF JKOnet* w/o U(x)</td><td>Œµ¬≤</td><td>Œµ2</td><td>√ó √ó</td></tr><tr><td>w/U(t)</td><td>O (TNd)</td><td>O (d)</td><td></td></tr><tr><td>JKOU(t)</td><td>O (TN2d)</td><td>O (Nd)</td><td></td></tr><tr><td>JKOnet* w/ U(x)</td><td>O (TNdn + n3) O (TN¬≤dn + n¬≥) O (TN¬≤dn + n¬≥)</td><td>O (TNdn + n3)</td><td>√ó</td></tr></table></body></html>\n\nTable 1: Per-epoch complexity (in FLOPs) and per-particle minimum number of sequential operations (maximum parallelization) for the JKOnet and JKOnet‚àó model families (we refer to the linear parametrization of our model with $\\mathtt { J K O n e t } _ { l } ^ { * }$ ). Here, $T$ is the length of the population trajectory, $N$ the number of particles in the snapshots of the population (assumed constant), $d$ is the dimensionality of the system, $n$ is the number of features for the linear parametrization, $D , \\varepsilon$ , TeacherForcing (TF) are JKOnet parameters: the number of inner operations (which may or not be constant), the accuracy required for the Sinkhorn algorithm, and a training modality (see [9] for details), respectively. MG stands for the Monge gap regularization [51].\n\nLinear parametrizations. When the parametrizations are linear, i.e. $V _ { \\theta _ { 1 } } ( x ) = \\theta _ { 1 } ^ { \\top } \\phi ( x )$ , $U _ { \\theta _ { 2 } } ( x -$ $y ) = \\theta _ { 2 } ^ { \\top } \\phi ( x - y )$ for feature maps $\\phi _ { 1 } , \\phi _ { 2 } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { n }$ , the optimal $\\theta ^ { * }$ can be computed in closed-form: Proposition 3.4. Assume that the features $\\phi _ { 1 , i }$ and $\\phi _ { 2 , i }$ are continuously differentiable, bounded, and have bounded Hessian. Define the matrix $y _ { t } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { ( 2 n + 1 ) \\times d } \\mathrm { \\Delta } l$ by\n\n$$\n\\begin{array} { r } { y _ { t } ( x _ { t } ) : = \\left[ \\nabla \\phi _ { 1 } ( x _ { t } ) ^ { \\top } , \\int _ { \\mathbb { R } ^ { d } } \\nabla \\phi _ { 2 } ( x _ { t } - x _ { t } ^ { \\prime } ) ^ { \\top } \\mathrm { d } \\mu _ { t } ( x _ { t } ^ { \\prime } ) , \\frac { \\nabla \\rho _ { t } ( x _ { t } ) } { \\rho _ { t } ( x _ { t } ) } \\right] ^ { \\top } } \\end{array}\n$$\n\nand suppose that the data is sufficiently exciting so that $\\begin{array} { r } { \\sum _ { t = 1 } ^ { T } \\int _ { \\mathbb { R } ^ { d } } y _ { t } ( x _ { t } ) y _ { t } ( x _ { t } ) ^ { \\top } \\mathrm { d } \\mu _ { t } ( x _ { t } ) } \\end{array}$ is invertible. Then, the optimal solution of (11) is\n\n$$\n\\boldsymbol { \\vartheta } ^ { * } = \\frac { 1 } { \\tau } \\left( \\sum _ { t = 1 } ^ { T } \\int _ { \\mathbb { R } ^ { d } } \\boldsymbol { y } _ { t } ( \\boldsymbol { x } _ { t } ) \\boldsymbol { y } _ { t } ( \\boldsymbol { x } _ { t } ) ^ { \\top } \\mathrm { d } \\mu _ { t } ( \\boldsymbol { x } _ { t } ) \\right) ^ { - 1 } \\left( \\sum _ { t = 0 } ^ { T - 1 } \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\boldsymbol { y } _ { t } ( \\boldsymbol { x } _ { t + 1 } ) ( \\boldsymbol { x } _ { t + 1 } - \\boldsymbol { x } _ { t } ) \\mathrm { d } \\gamma _ { t } ( \\boldsymbol { x } _ { t } , \\boldsymbol { x } _ { t + 1 } ) \\right) .\n$$\n\nRemark 3.5. The excitation assumption can be enforced via regularization terms $\\lambda _ { i } \\| \\theta _ { i } \\| ^ { 2 }$ , with ${ \\lambda } _ { i } > 0$ , in the loss (11). Another practical alternative is to use pseudoinverse or solve the leastsquares problem corresponding to (12) by gradient descent.\n\nNon-linear parametrizations. When the parametrizations are non-linear, we minimize (11) by gradient descent.\n\nInductive biases. By fixing any of the parameters $\\theta _ { 1 } , \\theta _ { 2 } , \\theta _ { 3 }$ to zero, the corresponding energy term is dropped from the model. It is thus possible to inject into JKOnet‚àó the proper inductive bias when additional information on the underlying diffusion process are known. For instance, if the process is deterministic and driven by an external potential, one can set $\\theta _ { 2 } = \\theta _ { 3 } = 0$ . Similarly, if it can be assumed that the interaction between the particles is negligible, we can set $\\theta _ { 2 } = 0$ .\n\n# 3.5 Why first-order conditions\n\nHere, we motivate the theoretical benefits of JKOnet‚àó over JKOnet using the desiderata:\n\n![](images/90c78fa64eb907adedfedf099291d2e86fdfc4e7a632590678c872d393327d05.jpg)  \nFigure 2: Level curves of the true (green-colored) and estimated (blue-colored) potentials (31), (33), (36) and (37), see Appendix F. See also Figure 6 in Appendix A.\n\n1. Total computational complexity per epoch (i.e., the cost to process the observed populations). 2. Per-particle computational complexity (i.e., the cost to process a single particle when maximally parallelizing the algorithm, prior to merging the results). 3. Representational capacity of the method (i.e., which energy terms the model can learn).\n\nWe collect this analysis in Table 1. Fundamentally, the first-order optimality conditions allow a reformulation of the learning problem that decouples prediction of the population evolution and learning of the dynamics (such coupling is the crux of (4)). As a result, JKOnet‚àó enjoys higher parallelizability. We also observe that the interaction energy comes with an increase in complexity, and in a way resembles the attention mechanisms in transformers [19, 52]. The linear dependence of JKOnet‚àó on the size of the batch implies that our method can process larger batch sizes for free (to process the entire dataset we need fewer steps in an inverse relationship with the batch size). In practice, this actually increases the speed (less memory swaps). These considerations do not hold for the JKOnet family. Finally, JKOnet‚àó enjoys enhanced representational power and interpretability. $\\mathtt { J K O n e t } _ { l } ^ { * }$ generally needs more computation per epoch (primarily related to the number of features) but requires a single epoch. In Table 1 we also report the computational complexity of the variants of JKOnet using a vanilla multi-layer perceptron (MLP) with Monge gap regularization [51] instead of a ICNN as a parametrization of the transport map. Despite the success in simplifying the training of transport maps over the use of ICNN [51], the Monge gap requires the solution of an optimal transport problem at every inner iteration, an unbearable slowdown [33].\n\nRemark 3.6. Unlike JKOnet, JKOnet‚àó requires the construction of the optimal transport couplings beforehand. However, JKOnet constructs a new optimal transport plan at each iteration depending on the current estimate of the potential, whereas JKOnet‚àó needs to do so only once, at the beginning. Moreover, as discussed in Section 4.1, in Section 4.2, in the application to single-cell diffusion dynamics in Section 4.4, and in the ablations in Appendix C.2, this additional cost is minimal.\n\n# 4 Experiments\n\nThe code for the experiments is available at https://github.com/antonioterpin/ jkonet-star. We include the training and architectural details for the JKOnet‚àó models family in Appendix C. The settings of the baselines considered are the one provided by the corresponding papers, reported for completeness in Appendix E, and the hardware setup is described in Appendix C.7. In all the experiments, we allow the models a budget of 1000 epochs.\n\nOur models. We use the following terminology for our method. JKOnet‚àó is the most general non-linear parametrization in Section 3.4 and $\\mathtt { J K O n e t } _ { V } ^ { * }$ introduces the inductive bias $\\theta _ { 2 } = \\theta _ { 3 } = 0$ . Similarly, we refer to the linear parametrizations by $\\mathtt { J K O n e t } _ { l , V } ^ { * }$ and $\\mathtt { J K O n e t } _ { l } ^ { * }$ .\n\nMetrics. To evaluate the prediction capabilities we use the one-step-ahead earth-mover distance (EMD), $\\begin{array} { r } { \\operatorname* { m i n } _ { \\gamma \\in \\Gamma ( \\mu _ { t } , \\hat { \\mu } _ { t } ) } \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\| x - y \\| \\dot { \\mathrm { d } } \\gamma ( x , y ) } \\end{array}$ , where $\\mu _ { t }$ and $\\hat { \\mu } _ { t }$ are the observed and predicted populations. In particular, we consider the average and standard deviation over a trajectory.\n\n# 4.1 Training at lightspeed\n\nExperimental setting. We validate the observations in Section 3.5 comparing (i) the EMD error, (ii) the convergence ratio, and (iii) the time per epoch required by the different methods on a synthetic dataset (see Appendix B) consisting of particles subject to a non-linear drift, $x _ { t + 1 } = x _ { t } - \\tau \\nabla V ( x _ { t } )$ , with $\\tau = 0 . 0 1 , T = 5$ , and the potential functions $V ( x )$ (31)-(45) in Appendix F, shown in Figure 2 and in Figure 6 in Appendix A.\n\n![](images/5e734d498df89c17688f02c75987031c0409d9ef2296b0bcbcd46568f2410230.jpg)  \nFigure 3: Numerical results of Section 4.1. The scatter plot displays points $( x _ { i } , y _ { i } )$ where $x _ { i }$ indexes the potentials in Appendix F and $y _ { i }$ are the errors (EMD, normalized so that the maximum error among all models and all potentials is 1) obtained with the different models. We mark with NaN each method that has diverged during training. The plot on the bottom-left shows the EMD error trajectory during training (normalized such that 0 and 1 are the minimum and maximum EMD), averaged over all the experiments. The shaded area represents the standard deviation. The box plot analyses the time per epoch required by each method. The statistics are across all epochs and all potential energies.\n\nResults. Figure 3 summarizes our results. All our methods perform uniformly better than the baselines, regardless of the generality. The speed improvement of the JKOnet‚àó models family suggests that a theoretically guided loss may provide strong computational benefits on par with sophisticated model architectures. Our linearly parametrized models, JKOnetl and $\\mathtt { J K O n e t } _ { l , V } ^ { * }$ , require a computational time per epoch comparable to the JKOnet family, but they only need one epoch to solve the problem optimally. Our non-linear models, JKOnet‚àó and $\\mathtt { J K O n e t } _ { V } ^ { * }$ , instead both require significantly lower time per epoch and converge faster than the JKOnet family. In these experiments, the computational cost associated with the optimal transport plans beforehand amounts to as little as $0 . 0 3 \\pm 0 . 0 1 \\mathrm { s }$ , and thus has negligible impact on training time. The true and estimated level curves of the potentials are depicted in Figure 2 and Figure 6 in Appendix A. Compared to JKOnet, our model also requires a simpler architecture: we drop the additional ICNN used in the inner iteration and the related training details (e.g., the strong convexity regularizer and the teacher forcing). Notice that simply replacing the ICNN in JKOnet with a vanilla MLP deprives the method of the theoretical connections with optimal transport, which, in our experiments, appears to be associated with stability (NaN in Figure 3).\n\nThe results suggest orders of magnitude of improvement also in terms of accuracy of the predictions. These performance gains can be observed also between the linear and non-linear parametrization of JKOnet‚àó. In view of Proposition 3.4, this is not unexpected: the linear parametrization solves the problem optimally, when the features are representative enough. However, the feature selection presents a problem in itself; see e.g. [4, $\\ S 3$ and $\\ S 4 ]$ . Thus, whenever applicable, we invite researchers and practitioners to adopt the linear parametrization, and the non-linear parametrization as demanded by the dimensionality of the problem. We further discuss the known failure modes in Appendix G.\n\n# 4.2 Scaling laws\n\nExperimental setting. We assess the performance of $\\mathtt { J K O n e t } _ { V } ^ { * }$ to recover the correct potential energy given $N \\in \\{ 1 0 0 0 , 2 5 0 0 , 5 0 0 0 , 7 5 0 0 , 1 0 0 0 0 \\}$ particles in dimension $d \\in \\{ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 \\}$ , generated as in Section 4.1.\n\n![](images/1cd7520345eb7b75fdf4d881a5775ff0e2c54503f30bf4bfaac250fd7bae8c35.jpg)  \nFigure 4: Numerical results of Section 4.2, reported in full in Figure 7 in Appendix A. The colors represent the EMD error, which appears to scale sublinearly with the dimension $d$ .\n\n![](images/86b342e14b86e7bb797990d288002614e022da28ee0003ea0cb76ee7d698b4de.jpg)  \nFigure 5: Visualizations of Section 4.4. The top row shows the two principal components of the scRNA-seq data, ground truth (green, days 1-3, 6-9, 12-15, 18-21, 24-27) and interpolated (blue, days 4-5, 10-11, 16-17, 22-23). The bottom row displays the estimated potential level curves over time. The bottom left plot superimposes the same three level curves for days 1-3 (solid), 12-15 (dashed), and 24-27 (dashed with larger spaces) to highlight the time-dependency.\n\nResults. We summarize our findings in Figure 4 for the potentials (31)-(33) and in Figure 7 in Appendix A for all other the potentials. Since the EMD error is related to the Euclidean norm, it is expected to grow linearly with the dimension $d$ (i.e., along the rows); here, the growth is sublinear up to the point where the number of particles is not informative enough: along the columns, the error decreases again. The time complexity of the computation of the optimal transport plans is influenced linearly by the dimensionality $d$ , and is negligible compared to the solution of the linear program, which depends only on the number of particles; we further discuss these effects in Appendix C.2. We thus conclude that JKOnet‚àó is well suited for high-dimensional tasks.\n\n# 4.3 General energy functionals\n\nExperimental setting. We showcase the capabilities of the JKOnet‚àó models to recover the potential, interaction, and internal energies selected as combinations of the functions in Appendix $\\mathrm { F } ^ { 2 }$ and noise levels $\\beta \\in \\{ 0 . 0 , 0 . 1 , 0 . 2 \\}$ . To our knowledge, this is the first model to recover all three energy terms.\n\nResults. We summarize our findings on the right. Compared to the setting in Section 4.1, there are two additional sources of inaccuracies: (i) the noise, which introduces an inevitable sampling error, and the (ii) the estimation of the densities (see Appendix C for training details). Nonetheless, the low EMD errors demonstrate the capability of JKOnet‚àó to recover the energy components that best explain the observed populations.\n\n![](images/c457cd45c2c1beca9bcec95c98f1cf024c19b081376bb888ef3a790a9874639e.jpg)\n\n# 4.4 Learning single-cell diffusion dynamics\n\nExperimental setting. Understanding the time evolution of cellular processes subject to external stimuli is a fundamental open question in biology. Motivated by the intuition that cells differentiate minimizing some energy functional, we deploy JKOnet‚àó to analyze the embryoid body single-cell\n\nRNA sequencing (scRNA-seq) data [35] describing the differentiation of human embryonic stem cells over a period of 27 days. We follow the data pre-processing in [50, 49]; in particular, we use the same processed artifacts of the embryoid data, which contains the first 100 components of the principal components analysis (PCA) of the data and, following [49], we focus on the first five. The cells are sequenced in five snapshots (days 1-3, 6-9, 12-15, 18-21, 24-27); we visualize the first two principal components in Figure 5. The visualization suggests that the energy governing the evolution is time-varying, possibly due to unobserved factors. For this, we condition the non-linear parametrization in Section 3 on time $t \\in \\mathbb { R }$ and minimize the loss\n\n$$\n\\sum _ { t = 0 } ^ { T - 1 } \\int _ {  { \\mathbb { R } } ^ { d } \\times  { \\mathbb { R } } ^ { d } } \\bigg \\| \\nabla V ( x _ { t + 1 } , t + 1 ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\bigg \\| ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) .\n$$\n\nTo predict the evolution of the particles, then, we use the implicit scheme (see Appendix B)\n\n$$\n\\boldsymbol { x } _ { t + 1 } = \\boldsymbol { x } _ { t } - \\tau \\nabla V ( \\boldsymbol { x } _ { t + 1 } , t + 1 ) .\n$$\n\nWe train the time-varying extension of $\\mathtt { J K O n e t } _ { V } ^ { * }$ , JKOnet and JKOnet-vanilla for 100 epochs on $6 0 \\%$ of the data at each time and we compute the EMD between the observed $\\mu _ { t }$ ( $4 0 \\%$ remaining data) and one-step ahead prediction $\\hat { \\mu } _ { t }$ at each timestep. We then average over the trajectory and report the statistics for 5 seeds.\n\nResults. We display the time evolution of the first two principal components of the level curves of the inferred potential energy in Figure 5, along with the cells trajectory (in green the data, in blue the interpolated predictions). As indicated by the table on the right, JKOnet‚àó outperforms JKOnet. We also compare JKOnet‚àó with recent work in the literature which focuses on the slightly different setting, namely the inference of $\\mu _ { t }$ from the evolution at all other time steps $\\mu _ { k }$ , $k \\neq t$ , without train/test split of the data (the numerical values are taken directly from [12, 49] and our statistics are computed over the timesteps). Since the experimental setting slightly differs, we limit ourselves to observe that JKOnet‚àó achieves state-of-the-art performance, but with significantly lower training time: JKOnet‚àó trains in a few minutes, while the methods listed take hours to run. We further discuss these results in Appendix E.\n\n<html><body><table><tr><td> Algorithm</td><td>EMD</td></tr><tr><td>JKOnet [9] JKOnet-vanilla[9] TrajectoryNet[50]</td><td>1.363 ¬± 0.214 3.237 ¬±1.135 0.848¬± -</td></tr><tr><td>Reg.CNF[17] DSB [14] I-CFM[49] SB-CFM[49] OT-CFM[49] NLSB [28] MIOFLOW[23]</td><td>0.825¬±- 0.862 ¬±0.023 0.872 ¬±0.087 1.221 ¬± 0.380 0.790¬±0.068 0.74¬±-</td></tr><tr><td>DMSB[12] JKOnet*</td><td>0.79¬±- 0.67¬±-</td></tr><tr><td>(time-varying)</td><td>0.624 ¬± 0.007</td></tr></table></body></html>\n\n# 5 Conclusion and limitations\n\nContributions. We introduced JKOnet‚àó, a model which recovers the energy functionals governing various classes of diffusion processes. The model is based on the novel study of the first-order optimality conditions of the JKO scheme, which drastically simplifies the learning task. In particular, we replace the complex bilevel optimization problem with a simple mean-square error, outperforming existing methods in terms of computational cost, solution accuracy, and expressiveness. In the prediction of cellular processes, JKOnet‚àó achieves state-of-the-art performance and trains in less than a minute, compared to the hours of all existing methods.\n\nLimitations. Our work did not address a few important challenges, which we believe to be exciting open questions. On the practical side, JKOnet‚àó owns its performances to a loss function motivated by deep theoretical results. However, its architecture is still ‚Äúvanilla‚Äù and we did not investigate data domains like images. Moreover, this work does not investigate in detail the choice of features for the linear parametrization, which in our experiments displays extremely promising results nonetheless. We further discuss the known failure modes in Appendix G.\n\nOutlook. We expect the approach followed in this work to apply to other exciting avenues of applied machine learning research, such as population steering [47], reinforcement learning [37, 44, 48], diffusion models [16, 22, 54] and transformers [19, 52].",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢òÊòØ‰ªéËßÇÊµãÊï∞ÊçÆ‰∏≠Â≠¶‰π†Êâ©Êï£ËøáÁ®ãÁöÑËÉΩÈáèÊ≥õÂáΩË°®Á§∫„ÄÇÁé∞ÊúâÊñπÊ≥ï‰æùËµñ‰∫éÂ§çÊùÇÁöÑÂèåÂ±Ç‰ºòÂåñÈóÆÈ¢òÔºå‰∏î‰ªÖËÉΩÂª∫Ê®°Á≥ªÁªüÁöÑÊºÇÁßªÈ°πÔºåÊó†Ê≥ïÂÆåÊï¥ÊÅ¢Â§çÊâ©Êï£ËøáÁ®ãÁöÑÊΩúÂú®ËÉΩÈáè„ÄÅÁõ∏‰∫í‰ΩúÁî®ÂíåÂÜÖÈÉ®ËÉΩÈáèÊàêÂàÜ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®ÁîüÁâ©Á≥ªÁªüÁ®≥ÊÄÅ„ÄÅÂπ≤ÁªÜËÉûÈáçÁºñÁ®ã„ÄÅÊâ©Êï£Ê®°ÂûãÂíåTransformerÁöÑÂ≠¶‰π†Âä®ÂäõÂ≠¶Á≠âÂú∫ÊôØ‰∏≠ÂÖ∑ÊúâÂÖ≥ÈîÆ‰ª∑ÂÄºÔºåÂ∞§ÂÖ∂Âú®ÂçïÁªÜËÉûRNAÊµãÂ∫èÁ≠â‰ªÖËÉΩËé∑Âèñ‚ÄúÁæ§‰ΩìÊï∞ÊçÆ‚ÄùËÄåÊó†Ê≥ïËøΩË∏™Âçï‰∏™Á≤íÂ≠êËΩ®ËøπÁöÑÂ∫îÁî®‰∏≠Â∞§‰∏∫ÈáçË¶Å„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫JKOnet‚àóÊ®°ÂûãÔºåÈÄöËøáÁ†îÁ©∂JKOÊñπÊ°àÁöÑ‰∏ÄÈò∂ÊúÄ‰ºòÊÄßÊù°‰ª∂ÔºåÂ∞ÜÂ≠¶‰π†‰ªªÂä°ËΩ¨Âåñ‰∏∫ÁÆÄÂçïÁöÑ‰∫åÊ¨°ÊçüÂ§±ÊúÄÂ∞èÂåñÈóÆÈ¢òÔºåÁªïËøá‰∫ÜÂ§çÊùÇÁöÑÂèåÂ±Ç‰ºòÂåñÈóÆÈ¢ò„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫Âü∫‰∫é‰∏ÄÈò∂ÊúÄ‰ºòÊÄßÊù°‰ª∂ÁöÑÁÆÄÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂ∞ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶‰ªéÂèåÂ±Ç‰ºòÂåñÈôç‰Ωé‰∏∫ÂçïÂ±Ç‰ºòÂåñ„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåJKOnet‚àóÂú®Ê†∑Êú¨ÊïàÁéá„ÄÅËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÂíåÂáÜÁ°ÆÊÄß‰∏äÂùá‰ºò‰∫éÂü∫Á∫øÊ®°Âûã„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** È¶ñÊ¨°ÂÆûÁé∞‰∫ÜÂØπÊâ©Êï£ËøáÁ®ãÁöÑÂÆåÊï¥ËÉΩÈáèÊ≥õÂáΩÔºàÂåÖÊã¨ÊΩúÂú®ËÉΩÈáè„ÄÅÁõ∏‰∫í‰ΩúÁî®ÂíåÂÜÖÈÉ®ËÉΩÈáèÔºâÁöÑÂ≠¶‰π†„ÄÇÂú®ÁúüÂÆûÁªÜËÉûËøáÁ®ãÈ¢ÑÊµã‰ªªÂä°‰∏≠ÔºåJKOnet‚àó‰ª•ÊûÅ‰ΩéËÆ°ÁÆóÊàêÊú¨ËææÂà∞ÊúÄÂÖàËøõÁ≤æÂ∫¶ÔºàEMDËØØÂ∑ÆÈôç‰ΩéËá≥0.624¬±0.007Ôºâ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** ‰∏∫Á∫øÊÄßÂèÇÊï∞ÂåñËÉΩÈáèÊ≥õÂáΩÊèê‰æõÈó≠ÂºèÊúÄ‰ºòËß£ÔºåÊòæËëóÊèêÂçáËÆ°ÁÆóÊïàÁéáÔºàËÆ≠ÁªÉÊó∂Èó¥‰ªéÂ∞èÊó∂Á∫ßÁº©Áü≠Ëá≥ÂàÜÈíüÁ∫ßÔºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   JKOnet‚àóÁöÑÊ†∏ÂøÉÂéüÁêÜÊòØÂ∞ÜÊâ©Êï£ËøáÁ®ãËß£Èáä‰∏∫Ê¶ÇÁéáÁ©∫Èó¥‰∏≠ËÉΩÈáèÊúÄÂ∞èÂåñÁöÑËΩ®ËøπÔºàÈÄöËøáJKOÊñπÊ°àÔºâÔºåÂπ∂ÈÄöËøá‰∏ÄÈò∂ÊúÄ‰ºòÊÄßÊù°‰ª∂Áõ¥Êé•Â≠¶‰π†ËÉΩÈáèÊ≥õÂáΩ„ÄÇÂÖ∂ËÆæËÆ°Âì≤Â≠¶ÊòØ‚Äú‰ªéÂéüÂõ†ÔºàËÉΩÈáèÊ≥õÂáΩÔºâËÄåÈùûÁªìÊûúÔºàÈ¢ÑÊµãÔºâÂ≠¶‰π†‚ÄùÔºå‰ªéËÄåÈÅøÂÖçÂ§çÊùÇÁöÑÂèåÂ±Ç‰ºòÂåñ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂÖàÂâçÂ∑•‰ΩúÁöÑÂ±ÄÈôêÔºö** Áé∞ÊúâÊñπÊ≥ïÔºàÂ¶ÇJKOnetÔºâÈúÄËß£ÂÜ≥Êó†ÈôêÁª¥ÂèåÂ±Ç‰ºòÂåñÈóÆÈ¢òÔºåËÆ°ÁÆóÊàêÊú¨È´ò‰∏î‰ªÖËÉΩÂ≠¶‰π†ÊΩúÂú®ËÉΩÈáè„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** ÈÄöËøá‰∏ÄÈò∂ÊúÄ‰ºòÊÄßÊù°‰ª∂Â∞ÜÈóÆÈ¢òËΩ¨Âåñ‰∏∫ÂçïÂ±Ç‰ºòÂåñÔºöÔºà1ÔºâÊîØÊåÅÂÆåÊï¥ËÉΩÈáèÊ≥õÂáΩÂ≠¶‰π†ÔºõÔºà2ÔºâÁ∫øÊÄßÂèÇÊï∞ÂåñÊó∂Â≠òÂú®Èó≠ÂºèËß£ÔºõÔºà3ÔºâËÆ°ÁÆóÊïàÁéáÊèêÂçáÔºàËßÅË°®1ÁöÑFLOPsÂØπÊØîÔºâ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.   **È¢ÑÂ§ÑÁêÜÔºö** ËÆ°ÁÆóËßÇÊµãÊï∞ÊçÆ$\\\\mu_t$‰∏é$\\\\mu_{t+1}$‰πãÈó¥ÁöÑÊúÄ‰ºò‰º†ËæìËÆ°Âàí$\\\\gamma_t$„ÄÇ\\n> 2.   **ÊçüÂ§±ÂáΩÊï∞ËÆæËÆ°Ôºö** ÊúÄÂ∞èÂåñ‰∏ÄÈò∂Êù°‰ª∂ËØØÂ∑ÆÔºàÂÖ¨Âºè11ÔºâÔºåÂåÖÂê´ÊΩúÂú®ËÉΩÈáèÊ¢ØÂ∫¶$\\\\nabla V$„ÄÅÁõ∏‰∫í‰ΩúÁî®ËÉΩÈáè$\\\\nabla U$ÂíåÂÜÖÈÉ®ËÉΩÈáèÈ°π„ÄÇ\\n> 3.   **ÂèÇÊï∞ÂåñÔºö** ËÉΩÈáèÊ≥õÂáΩ$J_\\\\theta$ÂàÜ‰∏∫‰∏âÈÉ®ÂàÜÔºàÂÖ¨Âºè12ÔºâÔºö\\n>      - ÊΩúÂú®ËÉΩÈáè$V_{\\\\theta_1}(x)$\\n>      - Áõ∏‰∫í‰ΩúÁî®ËÉΩÈáè$U_{\\\\theta_2}(x-y)$\\n>      - ÂÜÖÈÉ®ËÉΩÈáè$\\\\theta_3 \\\\rho \\\\log \\\\rho$\\n> 4.   **‰ºòÂåñÔºö** Á∫øÊÄßÂèÇÊï∞ÂåñÊó∂Áõ¥Êé•Ê±ÇËß£Èó≠ÂºèËß£ÔºàÂëΩÈ¢ò3.4ÔºâÔºõÈùûÁ∫øÊÄßÂèÇÊï∞ÂåñÊó∂ÈÄöËøáÊ¢ØÂ∫¶‰∏ãÈôç‰ºòÂåñ„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   JKOnet„ÄÅJKOnet-vanilla„ÄÅTrajectoryNet„ÄÅReg.CNF„ÄÅDSB„ÄÅI-CFM„ÄÅSB-CFM„ÄÅOT-CFM„ÄÅNLSB„ÄÅMIOFLOW„ÄÅDMSB„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®È¢ÑÊµãÁ≤æÂ∫¶ÔºàEMDËØØÂ∑ÆÔºâ‰∏äÔºö** JKOnet‚àóÂú®ÁªÜËÉûËøáÁ®ãÈ¢ÑÊµã‰ªªÂä°‰∏≠ËææÂà∞0.624¬±0.007ÔºåÊòæËëó‰ºò‰∫éJKOnetÔºà1.363¬±0.214ÔºâÂíåDMSBÔºà0.79¬±-Ôºâ„ÄÇ‰∏éÊúÄ‰Ω≥Âü∫Á∫øDMSBÁõ∏ÊØîÔºåËØØÂ∑ÆÈôç‰Ωé21%„ÄÇ\\n> *   **Âú®ËÆ°ÁÆóÊïàÁéá‰∏äÔºö** JKOnet‚àóÁöÑËÆ≠ÁªÉÊó∂Èó¥‰∏∫ÂàÜÈíüÁ∫ßÔºåËÄåJKOnetÂíåDMSBÈúÄË¶ÅÂ∞èÊó∂Á∫ß„ÄÇÂú®ÂêàÊàêÊï∞ÊçÆÂÆûÈ™å‰∏≠ÔºåJKOnet‚àóÁöÑÊØèepochÊó∂Èó¥ÊØîJKOnetÂáèÂ∞ë1-2‰∏™Êï∞ÈáèÁ∫ßÔºàËßÅÂõæ3Ôºâ„ÄÇ\\n> *   **Âú®Ê≥õÂåñËÉΩÂäõ‰∏äÔºö** JKOnet‚àóÊòØÈ¶ñ‰∏™ËÉΩÂêåÊó∂Â≠¶‰π†ÊΩúÂú®„ÄÅÁõ∏‰∫í‰ΩúÁî®ÂíåÂÜÖÈÉ®ËÉΩÈáèÁöÑÊ®°ÂûãÔºåËÄåJKOnet‰ªÖÊîØÊåÅÊΩúÂú®ËÉΩÈáèÂ≠¶‰π†ÔºàËßÅË°®1ÁöÑGeneralityÂàóÔºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Êâ©Êï£ËøáÁ®ã (Diffusion Processes, N/A)\\n*   ËÉΩÈáèÊúÄÂ∞èÂåñ (Energy Minimization, N/A)\\n*   JKOÊñπÊ°à (Jordan-Kinderlehrer-Otto Scheme, JKO)\\n*   ÊúÄ‰ºò‰º†Ëæì (Optimal Transport, OT)\\n*   ÂçïÁªÜËÉûRNAÊµãÂ∫è (Single-Cell RNA Sequencing, scRNA-seq)\\n*   Áæ§‰ΩìÊï∞ÊçÆÂ≠¶‰π† (Population Data Learning, N/A)\\n*   ÂèåÂ±Ç‰ºòÂåñ (Bilevel Optimization, N/A)\\n*   Á∫øÊÄßÂèÇÊï∞Âåñ (Linear Parametrization, N/A)\"\n}\n```"
}