{
    "source": "Semantic Scholar",
    "arxiv_id": "2406.12616",
    "link": "https://arxiv.org/abs/2406.12616",
    "pdf_link": "https://arxiv.org/pdf/2406.12616.pdf",
    "title": "Learning diffusion at lightspeed",
    "authors": [
        "Antonio Terpin",
        "Nicolas Lanzetti",
        "Florian Dörfler"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-06-18",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 9,
    "influential_citation_count": 2,
    "institutions": [
        "ETH Zürich"
    ],
    "paper_content": "# Learning diffusion at lightspeed\n\nAntonio Terpin ETH Zürich aterpin@ethz.ch\n\nNicolas Lanzetti ETH Zürich lnicolas@ethz.ch\n\nMartín Gadea ETH Zürich mgadea@ethz.ch\n\nFlorian Dörfler ETH Zürich dorfler@ethz.ch\n\n# Abstract\n\nDiffusion regulates numerous natural processes and the dynamics of many successful generative models. Existing models to learn the diffusion terms from observational data rely on complex bilevel optimization problems and model only the drift of the system. We propose a new simple model, JKOnet∗, which bypasses the complexity of existing architectures while presenting significantly enhanced representational capabilities: JKOnet∗ recovers the potential, interaction, and internal energy components of the underlying diffusion process. JKOnet∗ minimizes a simple quadratic loss and outperforms other baselines in terms of sample efficiency, computational complexity, and accuracy. Additionally, JKOnet∗ provides a closed-form optimal solution for linearly parametrized functionals, and, when applied to predict the evolution of cellular processes from real-world data, it achieves state-of-the-art accuracy at a fraction of the computational cost of all existing methods. Our methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions.\n\nSource code: https://github.com/antonioterpin/jkonet-star\n\n# 1 Introduction\n\nDiffusion processes govern the homeostasis of biological systems [40], stem cells reprogramming [20, 36], and the learning dynamics of diffusion models [16, 22, 54] and transformers [19, 52]. The diffusion process of interest often originates from three quantities: a drift term due to a potential field, the interaction with other particles, and a stochastic term. If these three components are known, predictions follow from simple forward sampling [27] or the recent work in optimization in the probability space [1, 3, 11, 25, 34, 38, 41]. In this paper, we consider the case when the diffusion process is unknown, and we seek to learn its representation from observational data. The problem has been addressed when the trajectories of the individual particles are known [5, 32], but it is often the case that we only have “population data”. For instance, single-cell RNA sequencing techniques enabled the collection of large quantities of data on biological systems [35], but the observer cannot access the trajectories of individual cells since measurements are destructive [20, 36]. The most promising avenue to circumvent the lack of particle trajectories is the Jordan-Kinderlehrer-Otto (JKO) scheme [24] which predicates that the particles as a whole move to decrease an aggregate energy, while not deviating too much from the current configuration. However, the JKO scheme entails an optimization problem in the probability space. Thus, the problem of finding the energy functional that minimizes a prediction error (w.r.t. observational data) takes the form of a computationallychallenging infinite-dimensional bilevel optimization problem, whereby the upper-level problem is the minimization of the prediction error and the lower-level problem is the JKO scheme. Recent work [1, 9] exploits the theory of optimal transport and in particular Brenier’s theorem [7] to attack this bilevel optimization problem, a model henceforth referred to as JKOnet. Despite promising initial results in [9], this complexity undermines scalability, stability, and generality of the model.\n\n![](images/b99640521ef0134f892970770a8c900178a1cee5eb4841fa3341d11d4f675f8f.jpg)  \nFigure 1: Given a sequence of snapshots $( \\mu _ { 0 } , \\dots , \\mu _ { T } )$ of a population of particles undergoing diffusion, we want to find the parameters $\\theta$ of the parametrized energy function $J _ { \\theta }$ that best explains the particles evolution. Given $\\theta$ , the effects mismatch is the Wasserstein distance between the observed trajectory and the predicted trajectory obtained iteratively solving the JKO step with $J _ { \\theta }$ . The first-order optimality condition in [30] applied to the JKO step suggests that the “gradient” of $J _ { \\theta }$ with respect to each $\\hat { \\mu } _ { t }$ vanishes at optimality, i.e., for $\\hat { \\mu _ { t } } = \\mu _ { t }$ . For $\\begin{array} { r } { J _ { \\theta } ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } \\bar { V } _ { \\theta } ( x ) \\mathrm { d } \\mu ( x ) } \\end{array}$ , this condition is depicted on the right. The gradient (dashed blue arrows) of the true $V$ (level curves in dashed blue) at each observed particle $\\boldsymbol { x } _ { i } ^ { t + 1 }$ (blue circles) in the next snapshot $\\mu _ { t + 1 }$ opposes the displacement (dotted red arrows) from a particle $\\ v x _ { i } ^ { t }$ (red triangles) in the previous snapshot $\\mu _ { t }$ . Instead, the gradient (solid green arrows) of the estimated $V _ { \\theta }$ (level curves in solid green) at each observed particle $\\boldsymbol { x } _ { i } ^ { t + 1 }$ (square) does not oppose the displacement from a particle $\\boldsymbol { x } _ { i } ^ { t }$ in the previous snapshot $\\mu _ { t }$ . This mismatch in the causes of the diffusion process is what JKOnet∗ minimizes.\n\nFurthermore, to be practical, it is limited to learning only potential energies, modelling the underlying physics only partially. Alternatively, [10, 43] learn directly the transport map describing the evolution of the population (i.e., the effects), bypassing the representation of the underlying energy functional (i.e., the causes). Motivated by robustness, interpretability, and generalization, here we seek a method to learn the causes. In [23, 42], the authors try to learn a geometry that explains the observed transport maps. Unfortunately, the cost between two configurations along a cost-minimizing trajectory is often not a metric [47]. Other attempts include recurrent neural networks [21], neural ODEs [15], and Schrödinger bridges [12, 28].\n\nContributions. We study the first-order necessary optimality conditions for the JKO scheme, an optimization problem in the probability space, and show that these conditions can be exploited to learn the energy functional governing the underlying diffusion process from population data, effectively bypassing the complexity of the infinite-dimensional bilevel optimization problem. We provide a closed-form solution in the case of linearly parametrized energy functionals and a simple, interpretable, and efficient algorithm for non-linear parametrizations. Via exhaustive numerical experiments, we show that, in the case of potential energies only, JKOnet∗ outperforms the state-of-the-art in terms of solution quality, scalability, and computational efficiency and, in the until now unsolved case of general energy functionals, allows us to also learn interaction and internal energies that explain the observed population trajectories. When applied to predict the evolution of cellular processes, it achieves state-of-the-art accuracy at a fraction of the computational cost. Figure 1 shows an overview of our method, detailed in Section 3.\n\n# 2 Diffusion processes via optimal transport\n\n# 2.1 Preliminaries\n\nThe gradient of $\\rho : \\mathbb { R } ^ { d }  \\mathbb { R }$ is $\\pmb { \\nabla } \\rho \\in \\mathbb { R } ^ { d }$ and the Jacobian of $\\phi : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { n }$ is $\\pmb { \\nabla } \\phi \\in \\mathbb { R } ^ { n \\times d }$ . We say that $f :  { \\mathbb { R } ^ { d } } \\to  { \\mathbb { R } }$ has bounded Hessian if $\\| \\nabla ^ { 2 } f ( x ) \\| \\leq C$ for some $C > 0$ (and some matrix norm $\\left\\| \\cdot \\right\\| )$ . The divergence of $F : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { n }$ is $\\nabla \\cdot F$ and its laplacian is $\\nabla ^ { 2 } F$ . The identity function is $\\mathrm { I d } : \\mathbb { R } ^ { d } \\stackrel { \\textstyle \\cdot } { \\to } \\mathbb { R } ^ { d }$ , $\\operatorname { I d } ( x ) = x$ . We denote by $\\mathcal { P } ( \\mathbb { R } ^ { d } )$ the space of (Borel) probability measures over $\\mathbb { R } ^ { d }$ with finite second moment. For $\\mu \\in \\mathcal P ( \\mathbb { R } ^ { d } )$ , $\\operatorname { s u p p } ( \\mu )$ is its support. The Dirac’s delta measure at $\\boldsymbol { x } \\in \\mathbb { R } ^ { d }$ , is $\\delta _ { x }$ . All the functions are assumed to be Borel, and for $f : \\mathbb { R } ^ { d }  \\mathbb { R }$ , $\\begin{array} { r } { \\int _ { \\mathbb { R } ^ { d } } f ( x ) \\mathrm { d } \\mu ( x ) } \\end{array}$ is the (Lebesgue) integral of $f$ w.r.t. $\\mu$ . If $\\mu$ is absolutely continuous w.r.t. the Lebesgue measure, $\\mu \\ll \\mathrm { d } x$ , then it admits a density $\\rho : \\mathbb { R } ^ { d }  \\mathbb { R } _ { \\geq 0 }$ , and the integral becomes $\\begin{array} { r } { \\int _ { \\mathbb { R } ^ { d } } f ( x ) \\rho ( x ) \\mathrm { d } x } \\end{array}$ . The pushforward of $\\mu$ via a (Borel) map $f : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { d }$ is the probability measure $f _ { \\# } \\mu$ defined by $( f _ { \\# } \\mu ) ( B ) = \\mu ( f ^ { - 1 } ( B ) )$ ; when $\\mu$ is empirical with $N$ , $\\begin{array} { r } { \\mu = { \\frac { 1 } { N } } \\sum _ { i = 1 } ^ { N } \\delta _ { x _ { i } } } \\end{array}$ , then $\\begin{array} { r } { f _ { \\# } \\mu = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\delta _ { f ( x _ { i } ) } } \\end{array}$ . Given $\\mu , \\nu \\in \\mathcal P ( \\mathbb { R } ^ { d } )$ , we say that a probability measure $\\gamma \\in \\mathcal { P } ( \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } )$ is a transport plan (or coupling) between $\\mu$ and $\\nu$ if its marginals are $\\mu$ and $\\nu$ . We denote the set of transport plans between $\\mu$ and $\\nu$ by $\\Gamma ( \\mu , \\nu )$ . The Wasserstein distance between $\\mu$ and $\\nu$ is\n\n$$\nW _ { 2 } ( \\mu , \\nu ) : = \\left( \\operatorname* { m i n } _ { \\gamma \\in \\Gamma ( \\mu , \\nu ) } \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\| x - y \\| ^ { 2 } \\mathrm { d } \\gamma ( x , y ) \\right) ^ { \\frac { 1 } { 2 } } .\n$$\n\nWhen $\\mu$ and $\\nu$ are discrete, (1) is a linear program. If, additionally, they have the same number of particles, the optimal transport plan is $\\gamma = ( \\operatorname { I d } , T ) _ { \\# } \\mu$ for some (transport) map $T : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { d }$ [39]. When $\\mu$ is absolutely continuous, $\\boldsymbol { \\gamma } = ( \\operatorname { I d } , \\pmb { \\nabla } \\psi ) _ { \\# } \\mu$ for some convex function $\\psi$ [7].\n\n# 2.2 The JKO scheme\n\nMany continuous-time diffusion processes can be modeled by partial differential equations (PDEs) or stochastic differential equations (SDEs):\n\nExample 2.1 (Fokker-Planck). The Fokker-Planck equation,\n\n$$\n\\frac { \\partial \\rho ( t , x ) } { \\partial t } = \\nabla \\cdot ( \\nabla V ( x ) \\rho ( t , x ) ) + \\beta \\nabla ^ { 2 } \\rho ( t , x ) ,\n$$\n\ndescribes the time evolution of the distribution $\\rho$ of a set of particles undergoing drift and diffusion,\n\n$$\n\\mathrm { d } X ( t ) = - \\nabla V ( X ( t ) ) \\mathrm { d } t + \\sqrt { 2 \\beta } \\mathrm { d } W ( t ) ,\n$$\n\nwhere $X ( t )$ is the state of the particle, $V ( x )$ the driving potential, and $W ( t )$ the Wiener process.\n\nThe pioneering work of Jordan, Kinderlehrer, and Otto [24], related diffusion processes to energyminimizing trajectories in the Wasserstein space (i.e., probability space endowed with the Wasserstein distance), providing a discrete-time counterpart of the diffusion process, the JKO scheme,\n\n$$\n\\mu _ { t + 1 } = \\mathop { \\mathrm { a r g m i n } } _ { \\mu \\in \\mathcal { P } ( \\mathbb { R } ^ { d } ) } J ( \\mu ) + \\frac { 1 } { 2 \\tau } W _ { 2 } ( \\mu , \\mu _ { t } ) ^ { 2 } ,\n$$\n\nwhere $J : \\mathcal { P } ( \\mathbb { R } ^ { d } )  \\mathbb { R } \\cup \\{ + \\infty \\}$ is an energy functional and $\\tau > 0$ is the time discretization.\n\nExample 2.2 (Fokker-Plank as a Wasserstein gradient flow). The Fokker-Plank equation (2) results from the continuous-time limit (i.e., $\\tau  0$ ) of the JKO scheme (3) for the energy functional\n\n$$\nJ ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } V ( x ) \\mathrm { d } \\mu ( x ) + \\beta \\int _ { \\mathbb { R } ^ { d } } \\rho ( x ) \\log ( \\rho ( x ) ) \\mathrm { d } x \\quad w i t h \\quad \\mathrm { d } \\mu ( x ) = \\rho ( x ) \\mathrm { d } x .\n$$\n\n# 2.3 Challenges\n\nSection 2.2 suggests that we can interpret the problem of learning diffusion processes as the problem of learning the energy functional $J$ in (3). Specifically, the setting is as follows: We have access to sample populations $\\mu _ { 0 } , \\mu _ { 1 } , \\ldots , \\mu _ { T }$ , and we want to learn the energy functional governing their dynamics. A direct approach to tackle the inverse problem is a bilevel optimization, used, among others, for the model JKOnet in [9]. This approach bases on the following two facts. First, by Brenier’s theorem, the solution of (3), $\\mu _ { t + 1 }$ , can be expressed1 as the pushforward of $\\mu _ { t }$ via the gradient of a convex function $\\psi _ { t } : \\mathbb { R } ^ { d }  \\mathbb { R }$ and, thus,\n\n$$\nW _ { 2 } ( \\mu _ { t } , \\mu _ { t + 1 } ) ^ { 2 } = \\int _ { \\mathbb { R } ^ { d } } \\| x - \\nabla \\psi _ { t } ( x ) \\| ^ { 2 } \\mathrm { d } \\mu _ { t } ( x ) .\n$$\n\nSecond, the optimization problem (3) is equivalently written as\n\n$$\n\\operatorname * { a r g m i n } _ { \\psi _ { t } \\in C } J ( \\nabla \\psi _ { t \\neq \\mu _ { t } } \\mu _ { t } ) + \\frac { 1 } { 2 \\tau } \\int _ { \\mathbb { R } ^ { d } } \\left\\| x - \\nabla \\psi _ { t } ( x ) \\right\\| ^ { 2 } \\mathrm { d } \\mu _ { t } ( x ) ,\n$$\n\nwhere $C$ is the class of continuously differentiable convex functions from $\\mathbb { R } ^ { d }$ to $\\mathbb { R }$ . Therefore, the learning task can be cast into the following bilevel optimization problem, which minimizes the discrepancy between the observations $( \\mu _ { t } )$ and the predictions of the model $( \\hat { \\mu } _ { t } )$ :\n\n$$\n\\begin{array} { l } { \\displaystyle \\operatorname* { m i n } _ { J } \\displaystyle \\sum _ { t = 1 } ^ { T } W _ { 2 } ( \\hat { \\mu } _ { t } , \\mu _ { t } ) ^ { 2 } } \\\\ { \\mathrm { s . t . } \\ \\hat { \\mu } _ { 0 } = \\mu _ { 0 } , \\quad \\hat { \\mu } _ { t + 1 } = \\nabla \\psi _ { t } ^ { * } \\hat { \\mu } _ { t } , } \\\\ { \\displaystyle \\psi _ { t } ^ { * } : = \\underset { \\psi \\in C } { \\mathrm { a r g m i n } } J ( \\nabla \\psi _ { t \\neq } \\hat { \\mu } _ { t } ) + \\frac { 1 } { 2 \\tau } \\displaystyle \\int _ { \\mathbb R ^ { d } } \\left\\| x - \\nabla \\psi _ { t } ( x ) \\right\\| ^ { 2 } \\mathrm { d } \\hat { \\mu } _ { t } ( x ) . } \\end{array}\n$$\n\nA practical implementation of the above requires a parametrization of both the transport map and the energy functional. The former problem has been tackled via input convex neural network (ICNN) parametrizing $\\psi _ { t }$ [2, 8] or via the “Monge gap” [51]. The second problem is only addressed for energy functional of the form $\\begin{array} { r } { J ( \\mu ) = \\tilde { \\int _ { \\mathbb { R } } } \\bar { V _ { \\theta } ( x ) } \\mathrm { d } \\mu ( \\bar { x } ) } \\end{array}$ , without interaction and internal energies, where $V _ { \\theta }$ is a non-linear function approximator [9].\n\nChallenges. This approach suffers from two major limitations. First, bilevel optimization problems are notoriously hard and we should therefore expect (4) to be computationally challenging. Second, most energy functionals are not potential energies but include interactions and internal energy terms as well. Although it is tempting to include other terms in the energy functional $J$ (e.g., parametrizing interaction and internal energies), the complexity of the bilevel optimization problem renders such an avenue viable only in principle.\n\n# 3 Learning diffusion at lightspeed\n\nOur methodology consists of replacing the optimization problem (3) with its first-order necessary conditions for optimality. This way, we bypass its computational complexity which, ultimately, leads to the bilevel optimization problem (4). Perhaps interestingly, our methodology for learning diffusion is based on first principles: whereas e.g. [9] minimizes an error on the effects (the predictions), we minimize an error on the causes (the energy functionals driving the diffusion process); see Figure 1. As we detail in Section 4, the resulting learning algorithms are significantly faster and more effective.\n\n# 3.1 Intuition\n\nTo start, we illustrate our idea in the Euclidean case (i.e., $\\mathbb { R } ^ { d }$ ) and later generalize it to the probability space (i.e., $\\mathcal { P } ( \\mathbb { R } ^ { d } ) )$ . Consider the problem of learning the energy functional $J : \\mathbb { R } ^ { d }  \\mathbb { R } \\cup \\{ + \\infty \\}$ of the analog of the JKO scheme in the Euclidean space, the proximal operator\n\n$$\nx _ { t + 1 } = \\underset { x \\in \\mathbb { R } ^ { d } } { \\operatorname { a r g m i n } } J ( x ) + \\frac { 1 } { 2 \\tau } \\| x - x _ { t } \\| ^ { 2 } .\n$$\n\nUnder sufficient regularity, we can replace (5) by its first-order optimality condition\n\n$$\n\\nabla J ( x _ { t + 1 } ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) = 0 .\n$$\n\nGiven a dataset $( x _ { 0 } , x _ { 1 } , \\dots , x _ { T } )$ , we seek the energy functional that best fits the optimality condition:\n\n$$\n\\operatorname* { m i n } _ { J } \\sum _ { t = 0 } ^ { T - 1 } \\bigg \\| \\nabla J ( x _ { t + 1 } ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\bigg \\| ^ { 2 } .\n$$\n\nIn the probability space, we can proceed analogously and replace (3) with its first-order optimality conditions. This analysis, which is based on recent advancements in optimization in the probability space [30], allows us to formulate the learning task as a single-level optimization problem.\n\n# 3.2 Potential energy\n\nConsider initially the case where the energy functional is a potential energy, for $V : \\mathbb { R } ^ { d }  \\mathbb { R }$ ,\n\n$$\nJ ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } V ( x ) \\mathrm { d } \\mu ( x ) .\n$$\n\nThe following proposition is the counterpart of (6) in ${ \\mathcal { P } } ( \\mathbb { R } ^ { d } )$ :\n\nProposition 3.1 (Potential energy). Assume $V$ is continuously differentiable, lower bounded, and has a bounded Hessian. Then, the JKO scheme (3) has an optimal solution $\\mu _ { t + 1 }$ and, $i f \\mu _ { t + 1 }$ is optimal for (3), then there is an optimal transport plan $\\gamma _ { t }$ between $\\mu _ { t }$ and $\\mu _ { t + 1 }$ such that\n\n$$\n\\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\bigg \\| \\nabla V ( x _ { t + 1 } ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\bigg \\| ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) = 0 .\n$$\n\nProposition 3.1 is by all means the analog of (6), since for the integral to be zero, $\\nabla V ( x _ { t + 1 } ) +$ $\\begin{array} { r } { \\frac { 1 } { \\tau } ( \\bar { x _ { t + 1 } } - x _ { t } ) = 0 } \\end{array}$ must hold for all $( x _ { t } , x _ { t + 1 } ) \\in \\mathrm { s u p p } ( \\gamma _ { t } )$ . Since the collected population data $\\mu _ { 0 } , \\mu _ { 1 } , \\dots , \\mu _ { T }$ are not optimization variables in the learning task, the optimal transport plan $\\gamma _ { t }$ can be computed beforehand. Thus, we can learn the energy functional representation by minimizing over a class of continuously differentiable potential energy functions the loss function\n\n$$\n\\sum _ { t = 0 } ^ { T - 1 } \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\Big \\lVert \\nabla V ( x _ { t + 1 } ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\Big \\rVert ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) .\n$$\n\n# 3.3 Arbitrary energy functionals\n\nConsider now the general case where the energy functional consists of a potential energy (with the potential function $\\mathbf { \\bar { \\boldsymbol { V } } } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { }$ ), interaction energy (with interaction kernel $U : \\mathbb { R } ^ { d } \\overset { \\cdot \\cdot } {  } \\mathbb { R }$ ), and internal energy (expressed as the entropy weighted by $\\beta \\in \\mathbb { R } _ { \\geq 0 }$ ):\n\n$$\nJ ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } V ( x ) \\mathrm { d } \\mu ( x ) + \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } U ( x - y ) \\mathrm { d } ( \\mu \\times \\mu ) ( x , y ) + \\beta \\int _ { \\mathbb { R } ^ { d } } \\rho ( x ) \\log ( \\rho ( x ) ) \\mathrm { d } x .\n$$\n\nThe first-order necessary optimality condition for the JKO scheme then reads as follows.\n\nProposition 3.2 (General case). Assume $V$ and $U$ are continuously differentiable, lower bounded, and have a bounded Hessian. Then, the JKO scheme (3) has an optimal solution $\\mu _ { t + 1 }$ which is absolutely continuous with density $\\rho _ { t + 1 }$ and, $i f \\mathrm { d } \\mu _ { t + 1 } ( x ) = \\rho _ { t + 1 } ( x ) \\mathrm { d } x$ is optimal for (3), then there is an optimal transport plan $\\gamma _ { t }$ between $\\mu _ { t }$ and $\\mu _ { t + 1 }$ such that\n\n$$\n\\begin{array} { r l } & { 0 = \\displaystyle \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\bigg \\| \\nabla V ( x _ { t + 1 } ) + \\int _ { \\mathbb { R } ^ { d } } \\nabla U ( x _ { t + 1 } - x _ { t + 1 } ^ { \\prime } ) \\mathrm { d } \\mu _ { t + 1 } ( x _ { t + 1 } ^ { \\prime } ) } \\\\ & { \\qquad + \\left. \\beta \\frac { \\nabla \\rho _ { t + 1 } \\left( x _ { t + 1 } \\right) } { \\rho _ { t + 1 } \\left( x _ { t + 1 } \\right) } + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\right\\| ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) . } \\end{array}\n$$\n\nThus, Proposition 3.2 suggests that the energy functional can be learned by minimizing over a class of continuously differentiable potential and internal energy functions and $\\beta \\in \\mathbb { R } _ { \\geq 0 }$ the loss function\n\n$$\n\\begin{array} { r l r } {  { \\sum _ { t = 0 } ^ { T - 1 } \\int _ {  { \\mathbb { R } } ^ { d } \\times  { \\mathbb { R } } ^ { d } } \\bigg \\| \\nabla V ( x _ { t + 1 } ) + \\int _ {  { \\mathbb { R } } ^ { d } } \\nabla U ( x _ { t + 1 } - x _ { t + 1 } ^ { \\prime } ) \\mathrm { d } \\mu _ { t + 1 } ( x _ { t + 1 } ^ { \\prime } ) } } \\\\ & { } & { \\qquad + \\beta \\frac { \\nabla \\rho _ { t + 1 } ( x _ { t + 1 } ) } { \\rho _ { t + 1 } ( x _ { t + 1 } ) } + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\bigg \\| ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) . } \\end{array}\n$$\n\nRemark 3.3. We generalize the setting to time-varying energies in Sectio\nn 4.4 and in Appendix B.\n\n# 3.4 Parametrizations\n\nFor our model JKOnet∗, we parametrize the energy functional at a measure $\\mathrm { d } \\mu = \\rho ( { \\boldsymbol { x } } ) \\mathrm { d } { \\boldsymbol { x } }$ as follows:\n\n$J _ { \\theta } ( \\mu ) = \\int _ { \\mathbb { R } ^ { d } } V _ { \\theta _ { 1 } } ( x ) \\mathrm { d } \\mu ( x ) + \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } U _ { \\theta _ { 2 } } ( x - y ) \\mathrm { d } ( \\mu \\times \\mu ) ( x , y ) + \\theta _ { 3 } \\int _ { \\mathbb { R } ^ { d } } \\rho ( x ) \\log ( \\rho ( x ) ) \\mathrm { d } x ,$ where $\\theta _ { 1 } , \\theta _ { 2 } \\in \\mathbb { R } ^ { n } , \\theta _ { 3 } \\in \\mathbb { R }$ , and we set $\\boldsymbol { \\theta } = \\left[ \\boldsymbol { \\theta } _ { 1 } ^ { \\top } , \\boldsymbol { \\theta } _ { 2 } ^ { \\top } , \\boldsymbol { \\theta } _ { 3 } ^ { \\top } \\right] ^ { \\top } \\in \\mathbb { R } ^ { 2 n + 1 }$ .\n\n<html><body><table><tr><td>Model</td><td>FLOPS per epoch Seq. op. per particle</td><td colspan=\"3\">Generality</td></tr><tr><td>JKOnet</td><td>O(T(DNd + N21g(2)） O(T (DNd +2g(N)</td><td>V(x) /</td><td>β U(x)</td></tr><tr><td>W/o TF JKOnet</td><td>108 ε</td><td></td><td>× ×</td></tr><tr><td>W/ TF JKOnet</td><td>O (T (DNd + N21g(N) O (DNd + N210g(N)</td><td></td><td>× ×</td></tr><tr><td>W/MG w/o TF JKOnet</td><td>ε2 (TD (Nd + N210g(N) O (TD (Nd + N2 g(N) (TD(Nd + N2log(N) O (D (Nd + N²1og(N)</td><td>×</td><td>× ×</td></tr><tr><td>W/ MG, TF JKOnet* w/o U(x)</td><td>ε²</td><td>ε2</td><td>× ×</td></tr><tr><td>w/U(t)</td><td>O (TNd)</td><td>O (d)</td><td></td></tr><tr><td>JKOU(t)</td><td>O (TN2d)</td><td>O (Nd)</td><td></td></tr><tr><td>JKOnet* w/ U(x)</td><td>O (TNdn + n3) O (TN²dn + n³) O (TN²dn + n³)</td><td>O (TNdn + n3)</td><td>×</td></tr></table></body></html>\n\nTable 1: Per-epoch complexity (in FLOPs) and per-particle minimum number of sequential operations (maximum parallelization) for the JKOnet and JKOnet∗ model families (we refer to the linear parametrization of our model with $\\mathtt { J K O n e t } _ { l } ^ { * }$ ). Here, $T$ is the length of the population trajectory, $N$ the number of particles in the snapshots of the population (assumed constant), $d$ is the dimensionality of the system, $n$ is the number of features for the linear parametrization, $D , \\varepsilon$ , TeacherForcing (TF) are JKOnet parameters: the number of inner operations (which may or not be constant), the accuracy required for the Sinkhorn algorithm, and a training modality (see [9] for details), respectively. MG stands for the Monge gap regularization [51].\n\nLinear parametrizations. When the parametrizations are linear, i.e. $V _ { \\theta _ { 1 } } ( x ) = \\theta _ { 1 } ^ { \\top } \\phi ( x )$ , $U _ { \\theta _ { 2 } } ( x -$ $y ) = \\theta _ { 2 } ^ { \\top } \\phi ( x - y )$ for feature maps $\\phi _ { 1 } , \\phi _ { 2 } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { n }$ , the optimal $\\theta ^ { * }$ can be computed in closed-form: Proposition 3.4. Assume that the features $\\phi _ { 1 , i }$ and $\\phi _ { 2 , i }$ are continuously differentiable, bounded, and have bounded Hessian. Define the matrix $y _ { t } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { ( 2 n + 1 ) \\times d } \\mathrm { \\Delta } l$ by\n\n$$\n\\begin{array} { r } { y _ { t } ( x _ { t } ) : = \\left[ \\nabla \\phi _ { 1 } ( x _ { t } ) ^ { \\top } , \\int _ { \\mathbb { R } ^ { d } } \\nabla \\phi _ { 2 } ( x _ { t } - x _ { t } ^ { \\prime } ) ^ { \\top } \\mathrm { d } \\mu _ { t } ( x _ { t } ^ { \\prime } ) , \\frac { \\nabla \\rho _ { t } ( x _ { t } ) } { \\rho _ { t } ( x _ { t } ) } \\right] ^ { \\top } } \\end{array}\n$$\n\nand suppose that the data is sufficiently exciting so that $\\begin{array} { r } { \\sum _ { t = 1 } ^ { T } \\int _ { \\mathbb { R } ^ { d } } y _ { t } ( x _ { t } ) y _ { t } ( x _ { t } ) ^ { \\top } \\mathrm { d } \\mu _ { t } ( x _ { t } ) } \\end{array}$ is invertible. Then, the optimal solution of (11) is\n\n$$\n\\boldsymbol { \\vartheta } ^ { * } = \\frac { 1 } { \\tau } \\left( \\sum _ { t = 1 } ^ { T } \\int _ { \\mathbb { R } ^ { d } } \\boldsymbol { y } _ { t } ( \\boldsymbol { x } _ { t } ) \\boldsymbol { y } _ { t } ( \\boldsymbol { x } _ { t } ) ^ { \\top } \\mathrm { d } \\mu _ { t } ( \\boldsymbol { x } _ { t } ) \\right) ^ { - 1 } \\left( \\sum _ { t = 0 } ^ { T - 1 } \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\boldsymbol { y } _ { t } ( \\boldsymbol { x } _ { t + 1 } ) ( \\boldsymbol { x } _ { t + 1 } - \\boldsymbol { x } _ { t } ) \\mathrm { d } \\gamma _ { t } ( \\boldsymbol { x } _ { t } , \\boldsymbol { x } _ { t + 1 } ) \\right) .\n$$\n\nRemark 3.5. The excitation assumption can be enforced via regularization terms $\\lambda _ { i } \\| \\theta _ { i } \\| ^ { 2 }$ , with ${ \\lambda } _ { i } > 0$ , in the loss (11). Another practical alternative is to use pseudoinverse or solve the leastsquares problem corresponding to (12) by gradient descent.\n\nNon-linear parametrizations. When the parametrizations are non-linear, we minimize (11) by gradient descent.\n\nInductive biases. By fixing any of the parameters $\\theta _ { 1 } , \\theta _ { 2 } , \\theta _ { 3 }$ to zero, the corresponding energy term is dropped from the model. It is thus possible to inject into JKOnet∗ the proper inductive bias when additional information on the underlying diffusion process are known. For instance, if the process is deterministic and driven by an external potential, one can set $\\theta _ { 2 } = \\theta _ { 3 } = 0$ . Similarly, if it can be assumed that the interaction between the particles is negligible, we can set $\\theta _ { 2 } = 0$ .\n\n# 3.5 Why first-order conditions\n\nHere, we motivate the theoretical benefits of JKOnet∗ over JKOnet using the desiderata:\n\n![](images/90c78fa64eb907adedfedf099291d2e86fdfc4e7a632590678c872d393327d05.jpg)  \nFigure 2: Level curves of the true (green-colored) and estimated (blue-colored) potentials (31), (33), (36) and (37), see Appendix F. See also Figure 6 in Appendix A.\n\n1. Total computational complexity per epoch (i.e., the cost to process the observed populations). 2. Per-particle computational complexity (i.e., the cost to process a single particle when maximally parallelizing the algorithm, prior to merging the results). 3. Representational capacity of the method (i.e., which energy terms the model can learn).\n\nWe collect this analysis in Table 1. Fundamentally, the first-order optimality conditions allow a reformulation of the learning problem that decouples prediction of the population evolution and learning of the dynamics (such coupling is the crux of (4)). As a result, JKOnet∗ enjoys higher parallelizability. We also observe that the interaction energy comes with an increase in complexity, and in a way resembles the attention mechanisms in transformers [19, 52]. The linear dependence of JKOnet∗ on the size of the batch implies that our method can process larger batch sizes for free (to process the entire dataset we need fewer steps in an inverse relationship with the batch size). In practice, this actually increases the speed (less memory swaps). These considerations do not hold for the JKOnet family. Finally, JKOnet∗ enjoys enhanced representational power and interpretability. $\\mathtt { J K O n e t } _ { l } ^ { * }$ generally needs more computation per epoch (primarily related to the number of features) but requires a single epoch. In Table 1 we also report the computational complexity of the variants of JKOnet using a vanilla multi-layer perceptron (MLP) with Monge gap regularization [51] instead of a ICNN as a parametrization of the transport map. Despite the success in simplifying the training of transport maps over the use of ICNN [51], the Monge gap requires the solution of an optimal transport problem at every inner iteration, an unbearable slowdown [33].\n\nRemark 3.6. Unlike JKOnet, JKOnet∗ requires the construction of the optimal transport couplings beforehand. However, JKOnet constructs a new optimal transport plan at each iteration depending on the current estimate of the potential, whereas JKOnet∗ needs to do so only once, at the beginning. Moreover, as discussed in Section 4.1, in Section 4.2, in the application to single-cell diffusion dynamics in Section 4.4, and in the ablations in Appendix C.2, this additional cost is minimal.\n\n# 4 Experiments\n\nThe code for the experiments is available at https://github.com/antonioterpin/ jkonet-star. We include the training and architectural details for the JKOnet∗ models family in Appendix C. The settings of the baselines considered are the one provided by the corresponding papers, reported for completeness in Appendix E, and the hardware setup is described in Appendix C.7. In all the experiments, we allow the models a budget of 1000 epochs.\n\nOur models. We use the following terminology for our method. JKOnet∗ is the most general non-linear parametrization in Section 3.4 and $\\mathtt { J K O n e t } _ { V } ^ { * }$ introduces the inductive bias $\\theta _ { 2 } = \\theta _ { 3 } = 0$ . Similarly, we refer to the linear parametrizations by $\\mathtt { J K O n e t } _ { l , V } ^ { * }$ and $\\mathtt { J K O n e t } _ { l } ^ { * }$ .\n\nMetrics. To evaluate the prediction capabilities we use the one-step-ahead earth-mover distance (EMD), $\\begin{array} { r } { \\operatorname* { m i n } _ { \\gamma \\in \\Gamma ( \\mu _ { t } , \\hat { \\mu } _ { t } ) } \\int _ { \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } } \\| x - y \\| \\dot { \\mathrm { d } } \\gamma ( x , y ) } \\end{array}$ , where $\\mu _ { t }$ and $\\hat { \\mu } _ { t }$ are the observed and predicted populations. In particular, we consider the average and standard deviation over a trajectory.\n\n# 4.1 Training at lightspeed\n\nExperimental setting. We validate the observations in Section 3.5 comparing (i) the EMD error, (ii) the convergence ratio, and (iii) the time per epoch required by the different methods on a synthetic dataset (see Appendix B) consisting of particles subject to a non-linear drift, $x _ { t + 1 } = x _ { t } - \\tau \\nabla V ( x _ { t } )$ , with $\\tau = 0 . 0 1 , T = 5$ , and the potential functions $V ( x )$ (31)-(45) in Appendix F, shown in Figure 2 and in Figure 6 in Appendix A.\n\n![](images/5e734d498df89c17688f02c75987031c0409d9ef2296b0bcbcd46568f2410230.jpg)  \nFigure 3: Numerical results of Section 4.1. The scatter plot displays points $( x _ { i } , y _ { i } )$ where $x _ { i }$ indexes the potentials in Appendix F and $y _ { i }$ are the errors (EMD, normalized so that the maximum error among all models and all potentials is 1) obtained with the different models. We mark with NaN each method that has diverged during training. The plot on the bottom-left shows the EMD error trajectory during training (normalized such that 0 and 1 are the minimum and maximum EMD), averaged over all the experiments. The shaded area represents the standard deviation. The box plot analyses the time per epoch required by each method. The statistics are across all epochs and all potential energies.\n\nResults. Figure 3 summarizes our results. All our methods perform uniformly better than the baselines, regardless of the generality. The speed improvement of the JKOnet∗ models family suggests that a theoretically guided loss may provide strong computational benefits on par with sophisticated model architectures. Our linearly parametrized models, JKOnetl and $\\mathtt { J K O n e t } _ { l , V } ^ { * }$ , require a computational time per epoch comparable to the JKOnet family, but they only need one epoch to solve the problem optimally. Our non-linear models, JKOnet∗ and $\\mathtt { J K O n e t } _ { V } ^ { * }$ , instead both require significantly lower time per epoch and converge faster than the JKOnet family. In these experiments, the computational cost associated with the optimal transport plans beforehand amounts to as little as $0 . 0 3 \\pm 0 . 0 1 \\mathrm { s }$ , and thus has negligible impact on training time. The true and estimated level curves of the potentials are depicted in Figure 2 and Figure 6 in Appendix A. Compared to JKOnet, our model also requires a simpler architecture: we drop the additional ICNN used in the inner iteration and the related training details (e.g., the strong convexity regularizer and the teacher forcing). Notice that simply replacing the ICNN in JKOnet with a vanilla MLP deprives the method of the theoretical connections with optimal transport, which, in our experiments, appears to be associated with stability (NaN in Figure 3).\n\nThe results suggest orders of magnitude of improvement also in terms of accuracy of the predictions. These performance gains can be observed also between the linear and non-linear parametrization of JKOnet∗. In view of Proposition 3.4, this is not unexpected: the linear parametrization solves the problem optimally, when the features are representative enough. However, the feature selection presents a problem in itself; see e.g. [4, $\\ S 3$ and $\\ S 4 ]$ . Thus, whenever applicable, we invite researchers and practitioners to adopt the linear parametrization, and the non-linear parametrization as demanded by the dimensionality of the problem. We further discuss the known failure modes in Appendix G.\n\n# 4.2 Scaling laws\n\nExperimental setting. We assess the performance of $\\mathtt { J K O n e t } _ { V } ^ { * }$ to recover the correct potential energy given $N \\in \\{ 1 0 0 0 , 2 5 0 0 , 5 0 0 0 , 7 5 0 0 , 1 0 0 0 0 \\}$ particles in dimension $d \\in \\{ 1 0 , 2 0 , 3 0 , 4 0 , 5 0 \\}$ , generated as in Section 4.1.\n\n![](images/1cd7520345eb7b75fdf4d881a5775ff0e2c54503f30bf4bfaac250fd7bae8c35.jpg)  \nFigure 4: Numerical results of Section 4.2, reported in full in Figure 7 in Appendix A. The colors represent the EMD error, which appears to scale sublinearly with the dimension $d$ .\n\n![](images/86b342e14b86e7bb797990d288002614e022da28ee0003ea0cb76ee7d698b4de.jpg)  \nFigure 5: Visualizations of Section 4.4. The top row shows the two principal components of the scRNA-seq data, ground truth (green, days 1-3, 6-9, 12-15, 18-21, 24-27) and interpolated (blue, days 4-5, 10-11, 16-17, 22-23). The bottom row displays the estimated potential level curves over time. The bottom left plot superimposes the same three level curves for days 1-3 (solid), 12-15 (dashed), and 24-27 (dashed with larger spaces) to highlight the time-dependency.\n\nResults. We summarize our findings in Figure 4 for the potentials (31)-(33) and in Figure 7 in Appendix A for all other the potentials. Since the EMD error is related to the Euclidean norm, it is expected to grow linearly with the dimension $d$ (i.e., along the rows); here, the growth is sublinear up to the point where the number of particles is not informative enough: along the columns, the error decreases again. The time complexity of the computation of the optimal transport plans is influenced linearly by the dimensionality $d$ , and is negligible compared to the solution of the linear program, which depends only on the number of particles; we further discuss these effects in Appendix C.2. We thus conclude that JKOnet∗ is well suited for high-dimensional tasks.\n\n# 4.3 General energy functionals\n\nExperimental setting. We showcase the capabilities of the JKOnet∗ models to recover the potential, interaction, and internal energies selected as combinations of the functions in Appendix $\\mathrm { F } ^ { 2 }$ and noise levels $\\beta \\in \\{ 0 . 0 , 0 . 1 , 0 . 2 \\}$ . To our knowledge, this is the first model to recover all three energy terms.\n\nResults. We summarize our findings on the right. Compared to the setting in Section 4.1, there are two additional sources of inaccuracies: (i) the noise, which introduces an inevitable sampling error, and the (ii) the estimation of the densities (see Appendix C for training details). Nonetheless, the low EMD errors demonstrate the capability of JKOnet∗ to recover the energy components that best explain the observed populations.\n\n![](images/c457cd45c2c1beca9bcec95c98f1cf024c19b081376bb888ef3a790a9874639e.jpg)\n\n# 4.4 Learning single-cell diffusion dynamics\n\nExperimental setting. Understanding the time evolution of cellular processes subject to external stimuli is a fundamental open question in biology. Motivated by the intuition that cells differentiate minimizing some energy functional, we deploy JKOnet∗ to analyze the embryoid body single-cell\n\nRNA sequencing (scRNA-seq) data [35] describing the differentiation of human embryonic stem cells over a period of 27 days. We follow the data pre-processing in [50, 49]; in particular, we use the same processed artifacts of the embryoid data, which contains the first 100 components of the principal components analysis (PCA) of the data and, following [49], we focus on the first five. The cells are sequenced in five snapshots (days 1-3, 6-9, 12-15, 18-21, 24-27); we visualize the first two principal components in Figure 5. The visualization suggests that the energy governing the evolution is time-varying, possibly due to unobserved factors. For this, we condition the non-linear parametrization in Section 3 on time $t \\in \\mathbb { R }$ and minimize the loss\n\n$$\n\\sum _ { t = 0 } ^ { T - 1 } \\int _ {  { \\mathbb { R } } ^ { d } \\times  { \\mathbb { R } } ^ { d } } \\bigg \\| \\nabla V ( x _ { t + 1 } , t + 1 ) + \\frac { 1 } { \\tau } ( x _ { t + 1 } - x _ { t } ) \\bigg \\| ^ { 2 } \\mathrm { d } \\gamma _ { t } ( x _ { t } , x _ { t + 1 } ) .\n$$\n\nTo predict the evolution of the particles, then, we use the implicit scheme (see Appendix B)\n\n$$\n\\boldsymbol { x } _ { t + 1 } = \\boldsymbol { x } _ { t } - \\tau \\nabla V ( \\boldsymbol { x } _ { t + 1 } , t + 1 ) .\n$$\n\nWe train the time-varying extension of $\\mathtt { J K O n e t } _ { V } ^ { * }$ , JKOnet and JKOnet-vanilla for 100 epochs on $6 0 \\%$ of the data at each time and we compute the EMD between the observed $\\mu _ { t }$ ( $4 0 \\%$ remaining data) and one-step ahead prediction $\\hat { \\mu } _ { t }$ at each timestep. We then average over the trajectory and report the statistics for 5 seeds.\n\nResults. We display the time evolution of the first two principal components of the level curves of the inferred potential energy in Figure 5, along with the cells trajectory (in green the data, in blue the interpolated predictions). As indicated by the table on the right, JKOnet∗ outperforms JKOnet. We also compare JKOnet∗ with recent work in the literature which focuses on the slightly different setting, namely the inference of $\\mu _ { t }$ from the evolution at all other time steps $\\mu _ { k }$ , $k \\neq t$ , without train/test split of the data (the numerical values are taken directly from [12, 49] and our statistics are computed over the timesteps). Since the experimental setting slightly differs, we limit ourselves to observe that JKOnet∗ achieves state-of-the-art performance, but with significantly lower training time: JKOnet∗ trains in a few minutes, while the methods listed take hours to run. We further discuss these results in Appendix E.\n\n<html><body><table><tr><td> Algorithm</td><td>EMD</td></tr><tr><td>JKOnet [9] JKOnet-vanilla[9] TrajectoryNet[50]</td><td>1.363 ± 0.214 3.237 ±1.135 0.848± -</td></tr><tr><td>Reg.CNF[17] DSB [14] I-CFM[49] SB-CFM[49] OT-CFM[49] NLSB [28] MIOFLOW[23]</td><td>0.825±- 0.862 ±0.023 0.872 ±0.087 1.221 ± 0.380 0.790±0.068 0.74±-</td></tr><tr><td>DMSB[12] JKOnet*</td><td>0.79±- 0.67±-</td></tr><tr><td>(time-varying)</td><td>0.624 ± 0.007</td></tr></table></body></html>\n\n# 5 Conclusion and limitations\n\nContributions. We introduced JKOnet∗, a model which recovers the energy functionals governing various classes of diffusion processes. The model is based on the novel study of the first-order optimality conditions of the JKO scheme, which drastically simplifies the learning task. In particular, we replace the complex bilevel optimization problem with a simple mean-square error, outperforming existing methods in terms of computational cost, solution accuracy, and expressiveness. In the prediction of cellular processes, JKOnet∗ achieves state-of-the-art performance and trains in less than a minute, compared to the hours of all existing methods.\n\nLimitations. Our work did not address a few important challenges, which we believe to be exciting open questions. On the practical side, JKOnet∗ owns its performances to a loss function motivated by deep theoretical results. However, its architecture is still “vanilla” and we did not investigate data domains like images. Moreover, this work does not investigate in detail the choice of features for the linear parametrization, which in our experiments displays extremely promising results nonetheless. We further discuss the known failure modes in Appendix G.\n\nOutlook. We expect the approach followed in this work to apply to other exciting avenues of applied machine learning research, such as population steering [47], reinforcement learning [37, 44, 48], diffusion models [16, 22, 54] and transformers [19, 52].",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是从观测数据中学习扩散过程的能量泛函表示。现有方法依赖于复杂的双层优化问题，且仅能建模系统的漂移项，无法完整恢复扩散过程的潜在能量、相互作用和内部能量成分。\\n> *   该问题在生物系统稳态、干细胞重编程、扩散模型和Transformer的学习动力学等场景中具有关键价值，尤其在单细胞RNA测序等仅能获取“群体数据”而无法追踪单个粒子轨迹的应用中尤为重要。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出JKOnet∗模型，通过研究JKO方案的一阶最优性条件，将学习任务转化为简单的二次损失最小化问题，绕过了复杂的双层优化问题。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 提出基于一阶最优性条件的简化学习框架，将计算复杂度从双层优化降低为单层优化。实验显示，JKOnet∗在样本效率、计算复杂度和准确性上均优于基线模型。\\n> *   **创新贡献点2：** 首次实现了对扩散过程的完整能量泛函（包括潜在能量、相互作用和内部能量）的学习。在真实细胞过程预测任务中，JKOnet∗以极低计算成本达到最先进精度（EMD误差降低至0.624±0.007）。\\n> *   **创新贡献点3：** 为线性参数化能量泛函提供闭式最优解，显著提升计算效率（训练时间从小时级缩短至分钟级）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   JKOnet∗的核心原理是将扩散过程解释为概率空间中能量最小化的轨迹（通过JKO方案），并通过一阶最优性条件直接学习能量泛函。其设计哲学是“从原因（能量泛函）而非结果（预测）学习”，从而避免复杂的双层优化。\\n\\n> **创新点 (Innovations)**\\n> *   **先前工作的局限：** 现有方法（如JKOnet）需解决无限维双层优化问题，计算成本高且仅能学习潜在能量。\\n> *   **本文改进：** 通过一阶最优性条件将问题转化为单层优化：（1）支持完整能量泛函学习；（2）线性参数化时存在闭式解；（3）计算效率提升（见表1的FLOPs对比）。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.   **预处理：** 计算观测数据$\\\\mu_t$与$\\\\mu_{t+1}$之间的最优传输计划$\\\\gamma_t$。\\n> 2.   **损失函数设计：** 最小化一阶条件误差（公式11），包含潜在能量梯度$\\\\nabla V$、相互作用能量$\\\\nabla U$和内部能量项。\\n> 3.   **参数化：** 能量泛函$J_\\\\theta$分为三部分（公式12）：\\n>      - 潜在能量$V_{\\\\theta_1}(x)$\\n>      - 相互作用能量$U_{\\\\theta_2}(x-y)$\\n>      - 内部能量$\\\\theta_3 \\\\rho \\\\log \\\\rho$\\n> 4.   **优化：** 线性参数化时直接求解闭式解（命题3.4）；非线性参数化时通过梯度下降优化。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   JKOnet、JKOnet-vanilla、TrajectoryNet、Reg.CNF、DSB、I-CFM、SB-CFM、OT-CFM、NLSB、MIOFLOW、DMSB。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在预测精度（EMD误差）上：** JKOnet∗在细胞过程预测任务中达到0.624±0.007，显著优于JKOnet（1.363±0.214）和DMSB（0.79±-）。与最佳基线DMSB相比，误差降低21%。\\n> *   **在计算效率上：** JKOnet∗的训练时间为分钟级，而JKOnet和DMSB需要小时级。在合成数据实验中，JKOnet∗的每epoch时间比JKOnet减少1-2个数量级（见图3）。\\n> *   **在泛化能力上：** JKOnet∗是首个能同时学习潜在、相互作用和内部能量的模型，而JKOnet仅支持潜在能量学习（见表1的Generality列）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   扩散过程 (Diffusion Processes, N/A)\\n*   能量最小化 (Energy Minimization, N/A)\\n*   JKO方案 (Jordan-Kinderlehrer-Otto Scheme, JKO)\\n*   最优传输 (Optimal Transport, OT)\\n*   单细胞RNA测序 (Single-Cell RNA Sequencing, scRNA-seq)\\n*   群体数据学习 (Population Data Learning, N/A)\\n*   双层优化 (Bilevel Optimization, N/A)\\n*   线性参数化 (Linear Parametrization, N/A)\"\n}\n```"
}