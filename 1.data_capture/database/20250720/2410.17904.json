{
    "link": "https://arxiv.org/abs/2410.17904",
    "pdf_link": "https://arxiv.org/pdf/2410.17904",
    "title": "Reinforcement Learning under Latent Dynamics: Toward Statistical and Algorithmic Modularity",
    "authors": [
        "P. Amortila",
        "Dylan J. Foster",
        "Nan Jiang",
        "Akshay Krishnamurthy",
        "Zakaria Mhammedi"
    ],
    "institutions": [
        "未找到机构信息"
    ],
    "publication_date": "2024-10-23",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "paper_content": "# Reinforcement Learning under Latent Dynamics: Toward Statistical and Algorithmic Modularity\n\nPhilip Amortila∗philipa4@illinois.edu\n\nDylan J. Foster dylanfoster@microsoft.com\n\nNan Jiang nanjiang@illinois.edu\n\nAkshay Krishnamurthy akshaykr@microsoft.com\n\nZakaria Mhammedi mhammedi@google.com\n\n# Abstract\n\nReal-world applications of reinforcement learning often involve environments where agents operate on complex, high-dimensional observations, but the underlying (“latent”) dynamics are comparatively simple. However, outside of restrictive settings such as small latent spaces, the fundamental statistical requirements and algorithmic principles for reinforcement learning under latent dynamics are poorly understood.\n\nThis paper addresses the question of reinforcement learning under general latent dynamics from a statistical and algorithmic perspective. On the statistical side, our main negative result shows that most well-studied settings for reinforcement learning with function approximation become intractable when composed with rich observations; we complement this with a positive result, identifying latent pushforward coverability as a general condition that enables statistical tractability. Algorithmically, we develop provably efficient observable-to-latent reductions—that is, reductions that transform an arbitrary algorithm for the latent MDP into an algorithm that can operate on rich observations—in two settings: one where the agent has access to hindsight observations of the latent dynamics [LADZ23], and one where the agent can estimate self-predictive latent models [SAGHCB20]. Together, our results serve as a first step toward a unified statistical and algorithmic theory for reinforcement learning under latent dynamics.\n\n# 1 Introduction\n\nMany application domains for reinforcement learning (RL) require the agent to operate on rich, high-dimensional observations of the environment, such as images or text [WSD15; LFDA16; KFPM21; NRKFG22; $\\mathbf { B a k } { + } 2 2$ ; Bro $+ 2 2$ ]. However, the environment itself can often be summarized by latent dynamics for a low-dimensional or otherwise simple latent state space. The decoupling of latent dynamics from the complex observation process naturally suggests a modular framework for algorithm design: first learn a representation that decodes the latent state from observations, then apply a reinforcement learning algorithm for the latent dynamics on top of the learned representation. This paper investigates the algorithmic and statistical foundations of this framework. We ask: Can we take existing algorithms and sample complexity guarantees for reinforcement learning in the latent state space and lift them to the observation space in a modular fashion?\n\nThere is a growing body of theoretical and empirical work developing algorithms that combine representation learning and reinforcement learning to develop scalable algorithms. On the empirical side, a plethora of representation learning objectives have been deployed to varying degrees of success [PAED17; Tan $^ { + 1 7 }$ ; ZMCGL21; LSA20; YFK21; Lam $^ { + 2 4 }$ ; Guo $+ 2 2$ ; HPBL23], but we lack a mathematical framework to systematically compare these objectives and understand when one might be preferred to another. On the theoretical side, all existing approaches suffer from three primary drawbacks: (a) they are tailored to restricted classes of latent dynamics models (tabular MDPs [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23], LQR [DR21; Mha $+ 2 0 \\dot { }$ ], or factored MDPs [MLJL21]), limiting generality; (b) the analyses, despite focusing on restrictive settings, are unwieldy, limiting progress in algorithm development; and (c) they are not modular, in the sense that the representation learning procedures are specialized to specific choices of latent reinforcement learning algorithm, limiting ease of use.\n\n# 1.1 Contributions\n\nWe address the aforementioned limitations by introducing a new framework, reinforcement learning under general latent dynamics.\n\nReinforcement learning under general latent dynamics (Section 2). In our framework, the agent performs control based on high-dimensional observations, but the dynamics of the environment are governed by an unobserved latent state space. Following prior work (particularly the so-called Block MDP formulation [DKJADL19]), we assume that the latent states can be uniquely decoded from observations, but that the true decoder is unknown and must be learned. To aid in the decoding process, we supply the learner with a class of representations that is realizable in the sense that it is powerful enough to represent the true decoder. Our point of departure from prior theoretical works is that we do not assume specific structure (e.g., tabular or linear dynamics) on the Markov decision process (MDP) that governs the latent dynamics. Instead, we make the minimal assumption that the latent dynamics belong to a base MDP class which is statistically tractable, in the sense that when the latent states are directly observed there exists some reinforcement learning algorithm with low sample complexity that is capable of learning a near-optimal policy for every MDP in the class. We take the first steps toward building a unified and modular theory for reinforcement learning in this setting.\n\nContributions: Statistical modularity (Section 3). A central consideration for reinforcement learning under latent dynamics is that representation learning and exploration must be intertwined: an accurate decoder is required to explore the latent state space, but exploration is required to learn an accurate decoder. To develop provable sample complexity guarantees, one must prevent errors from compounding during this interleaving process, a challenging statistical problem which prior work addresses through strong structural assumptions on the base MDP [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23; DR21; Mha $+ 2 0$ ; MLJL21]. For the general latent-dynamics setting we consider, it is unclear whether similar techniques can be applied, or whether the setting is even statistically tractable, ignoring computational considerations. Thus, our first contribution considers the question of statistical modularity:2\n\n# If a base MDP class is tractable when observed directly, is the corresponding latent-dynamics problem tractable?\n\nStatistical modularity adopts a minimax perspective by assuming that the base MDP lies in a given class, and demands that the sample complexity of the latent-dynamics setting is controlled by a natural bound on the sample complexity of the base MDP class. We show, perhaps surprisingly, that most well-studied reinforcement learning settings involving function approximation [RVR13; JKALS17; SJKAL19; MJTS20; AJSWY20; Li09; DVRZ19; WSY20; ZGS21; $\\mathrm { D u } + 2 1$ ; JLM21; FKQR21] do not admit statistical modularity (Theorem 3.1). In other words, statistical tractability of an MDP class does not extend to the latent-dynamics setting. We complement these negative findings with a positive result, identifying pushforward coverability as a general structural condition on the latent dynamics that enables sample efficiency (Theorem 3.2).\n\nContributions: Algorithmic modularity (Section 4). Beyond developing a modular understanding of the statistical landscape, we investigate modular algorithm design principles for RL under general latent dynamics. Specifically, we consider the question of observable-to-latent reductions, whereby RL under latent dynamics can be reduced to the simpler problem of RL with latent states directly observed:\n\nCan we generically lift algorithms for a base MDP class to solve the corresponding latent-dynamics problem?\n\nThis property, which we refer to as algorithmic modularity, enables modular, greatly simplified algorithm design, allowing one to use an arbitrary base algorithm for the base MDP class to solve the corresponding latent-dynamics problem. Algorithmic modularity is a stronger property than mere statistical modularity, and thus is subject to our statistical lower bound. Accordingly, we consider two settings that sidestep the lower bound through additional feedback and modeling assumptions. Our first algorithmic result considers hindsight observability [LADZ23], where latent states are revealed during training, but not at deployment (Theorem 4.1). Our second considers stronger function approximation conditions that enable the estimation of self-predictive latent models [SAGHCB20] through representation learning (Theorem A.1). Both results are fully modular: they transform any sample-efficient algorithm for the base MDP class into a sample-efficient algorithm for the latent-dynamics setting. Thus, they constitute the first general-purpose algorithms for RL under latent dynamics.\n\nTogether, we believe our results can serve as a foundation for further development of practical, general-purpose algorithms for RL under latent dynamics. To this end, we highlight a number of fascinating and challenging open problems for future research (Section 5).\n\n# 2 Reinforcement Learning under General Latent Dynamics\n\nIn this section we formally introduce our framework, reinforcement learning under general latent dynamics.\n\nMDP preliminaries. We consider an episodic finite-horizon online reinforcement learning setting. With $H$ denoting the horizon, a Markov decision process (MDP) $\\begin{array} { r l } { M ^ { \\star } } & { { } = } \\end{array}$ $\\{ \\bar { \\mathcal { X } } , \\mathcal { A } , \\{ \\bar { P _ { h } ^ { \\star } } \\} _ { h = 0 } ^ { H } , \\{ R _ { h } ^ { \\star } \\} _ { h = 1 } ^ { H } , H \\}$ consists of a state space $\\chi$ , an action space $\\mathcal { A }$ , a reward distribution $R _ { h } ^ { \\star } : \\mathcal { X } \\times \\mathcal { A } \\to \\Delta ( [ 0 , 1 ] )$ (with expectation $r _ { h } ^ { \\star } ( x , a ) )$ , and a transition kernel $P _ { h } ^ { \\star } : \\mathcal { X } \\times \\mathcal { A }  \\Delta ( \\mathcal { X } )$ (with the convention that $P _ { 0 } ^ { \\star } ( \\cdot \\mid \\emptyset )$ is the initial state distribution).3\n\nAt the beginning of the episode, the learner selects a randomized, non-stationary policy $\\pi \\ =$ $( \\pi _ { 1 } , \\ldots , \\pi _ { H } )$ , where $\\pi _ { h } : \\mathcal { X } \\to \\Delta ( \\mathcal { A } )$ ; we let $\\Pi _ { \\mathsf { r n s } }$ denote the set of all such policies. The episode evolves through the following process; beginning from $x _ { 1 } \\sim P _ { 0 } ^ { \\star } ( \\cdot \\mid \\emptyset )$ , the MDP generates a trajectory $( x _ { 1 } , a _ { 1 } , r _ { 1 } ) , \\dotsc , ( x _ { H } , a _ { H } , r _ { H } )$ via $a _ { h } \\sim \\pi _ { h } ( x _ { h } )$ , $r _ { h } \\sim R _ { h } ^ { \\star } ( x _ { h } , a _ { h } )$ , and $x _ { h + 1 } \\sim P _ { h } ^ { \\star } ( \\cdot \\mid x _ { h } , a _ { h } )$ . We let $\\mathbb { P } ^ { M ^ { \\star } , \\pi }$ denote the law under this process, and let $\\mathbb { E } ^ { M ^ { \\star } , \\pi }$ denote the corresponding expectation, and likewise let $\\mathbb { P } ^ { M , \\pi }$ and $\\mathbb { E } ^ { M , \\pi }$ denote the analogous laws and expectations in another MDP $M$ . We assume that $\\textstyle \\sum _ { h = 1 } ^ { H } r _ { h } \\in [ 0 , 1 ]$ almost surely for any trajectory in $M ^ { \\star }$ .\n\nFor a policy $\\pi$ and MDP $M$ , the expected reward for $\\pi$ is given by $J ^ { { \\scriptscriptstyle M } } ( \\pi ) : = \\mathbb { E } ^ { { \\scriptscriptstyle M } , { \\pi } } \\big [ \\sum _ { h = 1 } ^ { H } r _ { h } \\big ]$ , and the value functions are given by $\\begin{array} { r l r } { V _ { h } ^ { M , \\pi } ( x ) } & { : = } & { { \\mathbb E } ^ { M , \\pi } \\left[ \\sum _ { h ^ { \\prime } = h } ^ { H } r _ { h ^ { \\prime } } | | x _ { h } = x \\right] } \\end{array}$ , and $\\begin{array} { r } { Q _ { h } ^ { { \\boldsymbol M } , \\pi } ( { \\boldsymbol x } , { \\boldsymbol a } ) \\ : = \\ \\mathbb { E } ^ { { \\boldsymbol M } , \\pi } \\left[ \\sum _ { h ^ { \\prime } = h } ^ { H } r _ { h ^ { \\prime } } \\ \\left| \\begin{array} { l } { { \\boldsymbol x } _ { h } \\ = \\ { \\boldsymbol x } , { \\boldsymbol a } _ { h } \\ = \\ { \\boldsymbol a }  } \\end{array} \\right] } \\end{\\right.array} \\end{array}$ . We letP $\\pi _ { M } ~ = ~ \\{ \\pi _ { M , h } \\} _ { h = 1 } ^ { H }$ denote an optimal deterministic \u0002poPlicy of $M$ , which maximizes $V ^ { \\overline { { M } } , \\pi }$ (over $\\pi$ ) at all states (and in particular, satisfies $\\pi _ { M } \\in \\arg \\operatorname* { m a x } _ { \\pi \\in \\Pi _ { r n s } } J ^ { M } ( \\pi ) )$ , and write $Q ^ { \\ l { M } , \\star } : = Q ^ { \\ l { M } , \\pi _ { \\ l { M } } }$ . For $f : \\mathcal { X } \\times \\mathcal { A } \\to \\mathbb { R }$ , we write $\\pi _ { f } ( x ) \\ : = \\ \\arg \\operatorname* { m a x } _ { a } f ( x , a )$ as well as $V _ { f } ( x ) = \\operatorname* { m a x } _ { a } f ( x , a )$ . For MDP $M$ , horizon $h \\in [ \\breve { H } ]$ , and $g \\ : \\ \\mathcal { X } \\  \\ \\mathbb { R }$ , we let $\\mathcal { T } _ { h } ^ { M }$ denote the Bellman (optimality) operator defined via $[ \\mathcal { T } _ { h } ^ { \\boldsymbol { M } } \\boldsymbol { g } ] ( \\boldsymbol { x } , \\boldsymbol { a } ) \\ : = \\ : \\mathbb { E } ^ { M } [ r _ { h } + \\boldsymbol { g } ( \\boldsymbol { x } _ { h + 1 } ) \\mid \\ddot { \\boldsymbol { x } } _ { h } = \\boldsymbol { x } , \\boldsymbol { a } _ { h } = \\boldsymbol { a } ] ,$ and we overload notation by letting $[ { \\cal T } _ { h } ^ { \\scriptscriptstyle M } f ] ( x , a ) = [ { \\cal T } _ { h } ^ { \\scriptscriptstyle M } V _ { f } ] ( x , a )$ . We also let $\\mathcal { T } _ { h } ^ { M , \\pi }$ denote the Bellman evaluation operator defined via $\\begin{array} { r } { [ \\mathcal { T } _ { h } ^ { \\boldsymbol { M } , \\boldsymbol { \\pi } } \\ddot { f } ] ( \\boldsymbol { x } , \\boldsymbol { a } ) = \\mathbb { E } ^ { M } \\left[ r _ { h } + \\mathbb { E } _ { \\boldsymbol { a } ^ { \\prime } \\sim \\pi _ { h + 1 } \\left( \\cdot \\vert \\boldsymbol { x } _ { h + 1 } \\rangle \\right) } [ f ( \\boldsymbol { x } _ { h + 1 } , \\boldsymbol { a } ^ { \\prime } ) ] \\mid \\boldsymbol { x } _ { h } = \\boldsymbol { x } , \\boldsymbol { a } _ { h } = \\boldsymbol { a } \\right] } \\end{array}$ , for any $\\pi \\in \\Pi _ { r \\mathrm { n } s }$ . We define the occupancy measures for layer $h$ via $d _ { h } ^ { { \\scriptscriptstyle M } , \\pi } ( x ) = \\mathbb { P } ^ { { \\scriptscriptstyle M } , \\pi } [ x _ { h } = x ]$ and $d _ { h } ^ { { \\scriptscriptstyle M } , \\pi } ( x , a ) = \\mathbb { P } ^ { { \\scriptscriptstyle M } , \\pi } [ x _ { h } = x , a _ { h } = a ]$ .\n\nOnline reinforcement learning. In online reinforcement learning, the learning algorithm ALG repeatedly interacts with an unknown MDP $M ^ { \\star }$ by executing a policy and observing the resulting trajectory. After $T$ rounds of interaction, the algorithm outputs a final policy $\\widehat { \\pi }$ , with the goal of minimizing their $r i s k$ , defined via\n\n$$\n\\mathsf { R i s k } ( T , \\mathrm { A L G } , M ^ { \\star } ) : = J ^ { M ^ { \\star } } \\big ( \\pi _ { M ^ { \\star } } \\big ) - J ^ { M ^ { \\star } } \\big ( \\widehat \\pi \\big ) .\n$$\n\nFramework: Reinforcement learning under general latent dynamics. In reinforcement learning under general latent dynamics, we consider MDPs $M ^ { \\star }$ where the dynamics are governed by the evolution of an unobserved latent state $s _ { h }$ , while the agent observes and acts on observations $x _ { h }$ generated from these latent states. Formally, a latent-dynamics MDP consists of two ingredients: a base $M D P$ $M _ { \\mathrm { l a t } } = \\{ \\boldsymbol { S } , \\boldsymbol { A } , \\{ P _ { \\mathrm { l a t } , h } \\} _ { h = 0 } ^ { H } , \\{ \\dot { R _ { \\mathrm { l a t } , h } } \\} _ { h = 1 } ^ { H } , \\dot { H } \\}$ defined over a latent state space $s$ , and a decodable emission process $\\psi : = \\{ \\psi _ { h } : S  \\Delta ( \\mathcal { X } ) \\} _ { h = 1 } ^ { H }$ , which maps each latent state to a distribution over observations. The former is an arbitrary MDP defined over $s$ , while the latter is defined as follows.\n\nDefinition 2.1 (Emission process). An emission process is any function $\\psi : = \\{ \\psi _ { h } : S  \\Delta ( \\mathcal { X } ) \\} _ { h = 1 } ^ { H }$ and is said to be decodable $i f$\n\n$$\n\\forall h , \\forall s ^ { \\prime } \\neq s \\in { \\mathcal { S } } : \\quad \\operatorname { s u p p } \\psi _ { h } ( s ) \\cap \\operatorname { s u p p } \\psi _ { h } ( s ^ { \\prime } ) = \\emptyset . \\quad .\n$$\n\nWhen $\\psi = \\{ \\psi _ { h } \\} _ { h = 1 } ^ { H }$ is decodable, we let $\\psi ^ { - 1 } : = \\{ \\psi _ { h } ^ { - 1 } : \\mathcal { X } \\to S \\} _ { h = 1 } ^ { H }$ denote the associated decoder.   \nWith this, we can formally introduce the notion of a latent-dynamics MDP.\n\nDefinition 2.2 (Latent-dynamics MDP). For $a$ base MDP $\\begin{array} { r l } { M _ { \\mathrm { { l a t } } } } & { { } \\quad = } \\end{array}$ $\\{ S , A , \\{ P _ { \\mathrm { l a t } , h } \\} _ { h = 0 } ^ { H } , \\{ R _ { \\mathrm { l a t } , h } \\} _ { h = 1 } ^ { H ^ { * } } , H \\}$ , and a decodable emission process $\\psi$ , the latent-dynamics MDP $\\langle \\langle M _ { \\mathrm { 1 a t } } , \\psi \\rangle \\rangle : = \\big \\{ \\mathcal { X } , \\ A , \\{ P _ { \\mathrm { o b s } , h } \\} _ { h = 0 } ^ { H } , \\{ R _ { \\mathrm { o b s } , h } \\} _ { h = 1 } ^ { H } , H \\big \\}$ is defined as the MDP where the latent dynamics evolve based on the agent’s action $a _ { h } \\in { \\mathcal { A } }$ via the process $s _ { h + 1 } \\sim P _ { \\mathrm { l a t } , h } ( s _ { h } , a _ { h } )$ and $r _ { h } \\sim R _ { \\mathrm { l a t } , h } ( s _ { h } , a _ { h } )$ . The latent state is not observed directly, and instead the agent observes $x _ { h } \\in \\mathcal X$ generated by the emission process $x _ { h } \\sim \\psi _ { h + 1 } ( s _ { h } )$ .4\n\nNote that under these dynamics, the decoder $\\psi ^ { - 1 }$ associated with $\\psi$ ensures that $\\psi _ { h } ^ { - 1 } ( x _ { h } ) = s _ { h }$ almost surely for all $h \\in [ H ]$ . That is, the latent states can be uniquely decoded from the observations. To emphasize the distinction between the latent-dynamics MDP $\\langle \\langle \\dot { M } _ { \\mathrm { l a t } } , \\psi \\rangle \\rangle$ (which operates on the observable state space $\\mathcal { X }$ ) and the MDP $M _ { \\mathrm { l a t } }$ (which operates on⟪the latent⟫state space $s$ ), we refer to the latter as a base $M D P$ rather than, for example, a “latent MDP”, and apply a similar convention to other latent objects whenever possible.5\n\nDeparting from prior work, we do not place any inherent restrictions on the base MDP, and in particular do not assume that the latent space is small (i.e., tabular). Rather, we aim to understand—in a unified fashion—what structural assumptions on the base MDP $M _ { \\mathrm { l a t } }$ are required to enable learnability under latent dynamics. To this end, it will be useful to considers specific classes (i.e., subsets) of base MDPs $\\mathcal { M } _ { \\mathrm { l a t } }$ and the classes of latent-dynamics MDPs they induce.\n\nDefinition 2.3 (Latent-dynamics MDP class). Given a set of base MDPs $\\mathcal { M } _ { \\mathrm { l a t } }$ and a set of decoders $\\Phi \\subset \\{ { \\mathcal { X } } \\to S \\}$ , we let\n\n$$\n  { \\mathcal M } _ { 1 \\sf a t } , \\Phi   : = \\{  { \\cal M } _ { 1 \\sf a t } , \\psi  \\} : { \\cal M } _ { 1 \\sf a t } \\in \\mathcal { M } _ { 1 \\sf a t } , \\psi \\ i s \\ d e c o d a b l e , \\ \\psi ^ { - 1 } \\in \\Phi \\}\n$$\n\ndenote the class of induced latent-dynamics MDPs.\n\nStated another way, $\\langle \\langle \\mathcal { M } _ { \\mathrm { l a t } } , \\Phi \\rangle \\rangle$ is the set of all latent-dynamics MDPs $\\left. \\left. M _ { \\mathrm { l a t } } , \\psi \\right. \\right.$ where (i) the base MDP $M _ { \\mathrm { l a t } }$ lies ⟪n $\\mathcal { M } _ { \\mathrm { l a t } }$ , a⟫nd (ii), the emission process $\\psi$ is decodab e⟪, with the⟫corresponding decoder belonging to $\\Phi$ . The class $\\mathcal { M } _ { \\mathrm { l a t } }$ represents our prior knowledge about the underlying MDP $M _ { \\mathrm { l a t } }$ ; concrete classes considered in prior work include tabular MDPs [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23], linear dynamical systems [DMRY20; DR21; $\\mathbf { M h a } { + } 2 0 ]$ , and factored MDPs [MLJL21]. In particular, the class $\\mathcal { M } _ { \\mathrm { l a t } }$ may itself warrant using function approximation. At the same time, the class $\\Phi$ represents our prior knowledge or inductive bias about the emission process, enabling representation learning. In what follows, we investigate what conditions on $\\mathcal { M } _ { \\mathrm { l a t } }$ make the induced class $\\langle \\langle \\mathcal { M } _ { \\mathrm { l a t } } , \\Phi \\rangle \\rangle$ tractable, both statistically (statistical modularity; Section 3) and via reduction (algorith⟪mic modul⟫arity; Section 4).\n\n# 3 Statistical Modularity: Positive and Negative Results\n\nThis section presents our main statistical results. We begin by formally defining the notion of statistical modularity introduced in Section 1, present our main impossibility result (lower bound) and its implications (Section 3.2), then give positive results for the general class of pushforward-coverable MDPs (Section 3.3).\n\n# 3.1 Statistical modularity: A formal definition\n\nWe first define the statistical complexity for a MDP class (or, model class) $\\mathcal { M }$ .\n\nDefinition 3.1 (Statistical complexity). We say that an MDP class $\\mathcal { M }$ can be learned up to $\\varepsilon$ - optimality using comp $( \\mathcal { M } , \\varepsilon , \\delta )$ samples if there exists an algorithm ALG which, for every $M \\in \\mathcal { M }$ , attains\n\n$$\n\\mathsf { R i s k } ( T , \\mathbf { A L G } , M ) \\leq \\varepsilon\n$$\n\nwith probability at least $1 - \\delta$ , after $T = \\mathsf { c o m p } ( \\mathcal { M } , \\varepsilon , \\delta )$ rounds of online interaction in $M$ .\n\nWe say that a base MDP class $\\mathcal { M } _ { \\mathrm { l a t } }$ admits statistically modularity if, for any decoder class $\\Phi$ , the induced latent-dynamics MDP class $\\langle \\langle \\mathcal { M } _ { \\mathrm { l a t } } , \\Phi \\rangle \\rangle$ can be learned with statistical complexity that is polynomial in: (i) the statistical comple⟪xity for th⟫e base class, and (ii) the capacity of the decoder class.\n\nDefinition 3.2 (Statistical modularity). We say the MDP class $\\mathcal { M } _ { \\mathrm { l a t } }$ is statistically modular under complexity comp $( \\mathcal { M } _ { \\mathrm { l a t } } , \\varepsilon , \\delta )$ if, for every decoder class $\\Phi$ , we have\n\n$$\n\\mathrm { c o m p } ( \\langle \\langle \\mathcal { M } _ { \\mathrm { l a t } } , \\Phi \\rangle \\rangle , \\varepsilon , \\delta ) = \\mathsf { p o l y } ( \\mathsf { c o m p } ( \\mathcal { M } _ { \\mathrm { l a t } } , \\varepsilon , \\delta ) , \\log | \\Phi | ) .\n$$\n\nWe say that $\\mathcal { M } _ { \\mathrm { l a t } }$ admits strong statistical modularity if Eq. (4) holds when comp $( \\mathcal { M } _ { \\mathrm { l a t } } , \\varepsilon , \\delta )$ is the minimax sample complexity for $\\mathcal { M } _ { \\mathrm { l a t } }$ .\n\nIn the sequel, we examine well-studied MDP classes $\\mathcal { M } _ { \\mathrm { l a t } }$ (e.g., those which admit low Bellman rank [JKALS17]) and choose $\\mathsf { z o m p } ( \\mathcal { M } _ { \\mathrm { l a t } } , \\varepsilon , \\delta )$ based on natural upper bounds on their optimal sample complexity; in this case we will simply say they are (or are not) statistical modular, leaving the complexity upper bound comp implicit. Following prior work [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23; DR21; Mha $^ { + 2 0 }$ ; MLJL21], we use $\\log | \\Phi |$ as a proxy for the statistical complexity of supervised learning with the decoder class $\\Phi$ .6\n\nThe two most notable examples of statistical modularity covered by prior work are: (i) taking $\\mathcal { M } _ { \\mathrm { l a t } }$ as the set of tabular MDPs admits strong statistical modularity [DKJADL19; MHKL20; MFR23], and (ii) taking $\\mathcal { M } _ { \\mathrm { l a t } }$ as the set of linear MDPs admits statistical modularity with complexity $\\mathfrak { p o l y } ( d , H , | \\mathcal { A } | , \\varepsilon ^ { - 1 } , \\mathrm { l o g } \\big ( \\delta ^ { - 1 } \\big ) )$ [AKKS20; UZS22; MCKJA24; MBFR23].7 Interestingly, the latter does not admit strong statistical modularity, because the optimal rate for $\\mathcal { M } _ { \\mathrm { l a t } }$ does not scale with $| { \\cal { A } } |$ , but the rate for $\\left. \\left. \\mathcal { M } _ { \\mathrm { 1 a t } } , \\Phi \\right. \\right.$ necessarily does [LS20; HLSW21]. The results of Mhammedi et al.; Misra et al.; Song et⟪al. [Mha $+ 2 0$ ; MLJL21; SWFK24] can also be viewed as instances of statistical modularity for other base MDP classes.\n\n# 3.2 Lower bounds: Impossibility of statistical modularity\n\nOur main result in this section is to show that for most MDP classes $\\mathcal { M } _ { \\mathrm { l a t } }$ considered in the literature on sample-efficient reinforcement learning with function approximation [RVR13; JKALS17; SJKAL19; MJTS20; AJSWY20; Li09; DVRZ19; WSY20; ZGS21; $\\mathrm { D u } + 2 1$ ; JLM21; FKQR21], statistical modularity (under the natural complexity upper bound for the class of interest) is impossible. Our central technical result is the following lower bound, which shows that statistical modularity can be impossible even when the base MDP is known to the learner a-priori. The lower bound is a significant generalization of the result from Song et al. [SWFK24]; we first state the lower bound, then discuss implications.\n\nTheorem 3.1 (Impossibility of statistical modularity). For every $N \\geq 4 ,$ , there exists a decoder class $\\Phi$ with $| \\Phi | = N$ and a family of base MDPs $\\mathcal { M } _ { \\mathrm { l a t } }$ satisfying (i) $| \\mathcal { M } _ { \\mathrm { l a t } } | = 1$ , (ii) $H \\leq \\mathcal { O } ( \\log ( N ) )$ , (iii) $| S | \\overset { \\cdot } { = } \\lvert \\mathcal { X } \\rvert \\leq N ^ { 2 }$ , (iv) $| { \\mathcal { A } } | = 2$ , and such that\n\n1. For all $\\varepsilon , \\delta > 0$ , we have comp $( \\mathcal { M } _ { \\mathrm { l a t } } , \\varepsilon , \\delta ) = 0 .$ .\n\n2. For an absolute constant $c > 0$ $0 , \\mathsf { c o m p } ( \\langle \\langle \\mathcal { M } _ { \\mathrm { l a t } } , \\Phi \\rangle \\rangle , c , c ) \\geq \\Omega ( N / \\log ( N ) ) .$\n\nIn other words, even when the base dynamics are fully known, strong statistical modularity (in this case, $\\mathsf { p o l y } ( \\log | \\Phi | )$ complexity) is impossible; any algorithm will require at least $\\operatorname* { m i n } \\{ \\sqrt { S } , 2 ^ { \\Omega ( H ) } \\big / H , | \\Phi | \\Big / \\mathrm { l o g } | \\Phi | \\Big \\}$ episodes to learn a near-optimal policy for a latent-dynamics MDP $\\left. \\left. M _ { \\mathrm { l a t } } , \\psi \\right. \\right. \\in \\left. \\left. { \\mathcal { M } } _ { \\mathrm { l a t } } , \\Phi \\right. \\right.$ .\n\n<html><body><table><tr><td>Base MDP class M1at</td><td>Statistical Modularity?</td></tr><tr><td>Tabular</td><td>√</td></tr><tr><td>Contextual Bandits</td><td>√</td></tr><tr><td>Low-Rank MDP</td><td>√</td></tr><tr><td>KnownDeterministic MDP(Miat|= 1)</td><td>√</td></tr><tr><td>LowState Occupancy (Vπ :S→△(A))</td><td>√</td></tr><tr><td>Model Class+Pushforward Coverability</td><td>√</td></tr><tr><td>Linear CB/MDP</td><td>X*</td></tr><tr><td>ModelClass+Coverability (Vπm :M ∈M)</td><td>×</td></tr><tr><td>Known Stochastic MDP(|Miat|= 1)</td><td>X</td></tr><tr><td>Bellman Rank (Q-type or V-type)</td><td>X</td></tr><tr><td>Eluder Dimension+Bellman Completeness</td><td></td></tr><tr><td>Q*-Irrelevant StateAbstraction</td><td></td></tr><tr><td>Linear Mixture MDP</td><td></td></tr><tr><td>Linear Q*/V*</td><td>×</td></tr><tr><td>Low State/State-Action Occupancy (Vπm :M ∈M)</td><td>X</td></tr><tr><td>Bisimulation</td><td>？</td></tr><tr><td>LowState-Action Occupancy (Vπ :S→△(A))</td><td>？*</td></tr><tr><td>Model Class +Coverability (Vπ : S→△(A))</td><td>？</td></tr></table></body></html>\n\nIntuition for lower bound. The intuition behind the lower bound in Theorem 3.1 is as follows: the unobserved latent state space consists of $N = | \\Phi |$ binary trees (indexed from 1 to $N$ ), each with $N$ leaf nodes. The starting distribution is uniform over the roots of the $N$ trees, and the agent receives a reward of 1 if and only if they navigate to the leaf node that corresponds to the index of their current tree. The observed state space is identical to the latent state space, but the emission process shifts the index of the tree by an amount which is unknown to the agent. Despite the base MDP being known and the decoder class satisfying realizability, the agent requires near-exhaustive search to identify the value of the shift and recover a near-optimal policy.\n\nA taxonomy of statistical modularity. As a corollary, we prove that many (but not all) well-studied function approximation settings do not admit statistical modularity by embedding them into the lower bound construction of Theorem 3.1 (as well as a variant of the result, Theorem E.1). Our results are summarized in Figure 1. Our impossibility results highlight the following phenomenon: many MDP classes $\\mathcal { M } _ { \\mathrm { l a t } }$ that place structural assumptions via the value functions (e.g., MDPs with linear- $Q ^ { \\star } / V ^ { \\star }$ $[ \\mathrm { D u } + 2 1 ]$ or MDPs with a Bellman complete value function class of bounded eluder dimension [JLM21; WSY20]) become intractable under latent dynamics. Intuitively, this is because it is not possible to take advantage of structure in value functions without learning a good representation, and, simultaneously, these assumptions are too weak by themselves to enable learning such a representation. Meanwhile, MDP classes $\\mathcal { M } _ { \\mathrm { l a t } }$ that place structural assumptions on the transition distribution (e.g., MDPs with low state occupancy complexity $[ \\mathrm { D u } + 2 1 ]$ or low-rank MDPs [AKKS20]) are sometimes (but not always) tractable under latent dynamics.8\n\nWe point to Appendix E.2 for background on all the settings in Figure 1 and proofs that they are (or are not) statistically modular. We remark that it is fairly straightforward to embed most of the MDP classes of Figure 1 into the construction of Theorem 3.1 since it only uses only a single base MDP $M _ { \\mathrm { l a t } }$ , and we expect that many other base MDP classes can similarly be shown to be intractable. However, proving the positive results in Figure 1 requires establishing several new results showing that certain base classes are tractable under latent dynamics; most notably, we next discuss the case of pushforward coverability.\n\n# 3.3 Upper bounds: Pushforward-coverable MDPs are statistically modular\n\nOur main postive result concerning statistical modularity is to highlight pushforward coverability [XJ21; AFK24; MFR24]—a strengthened version of the coverability parameter introduced in Xie et al. [XFBJK23]—as a general structural parameter that enables sample-efficient reinforcement learning under latent dynamics.\n\nDefinition 3.3 (Pushforward coverability). The pushforward coverability coefficient $C _ { \\mathsf { p u s h } }$ for an MDP $M _ { \\mathrm { l a t } }$ with transition kernel $P _ { \\mathrm { 1 a t } }$ is defined by\n\n$$\nC _ { \\mathsf { p u s h } } ( M _ { \\mathrm { 1 a t } } ) = \\operatorname* { m a x } _ { h \\in [ H ] } \\operatorname* { i n f } _ { \\mu \\in \\Delta ( S ) } \\operatorname* { s u p } _ { ( s , a , s ^ { \\prime } ) \\in S \\times A \\times S } \\frac { P _ { \\mathrm { 1 a t } , h - 1 } ( s ^ { \\prime } \\mid s , a ) } { \\mu ( s ^ { \\prime } ) } .\n$$\n\nConcrete examples [AFK24; MFR24] include: (i) tabular MDPs $M _ { \\mathrm { l a t } }$ admit $C _ { \\mathsf { p u s h } } ( M _ { \\mathsf { l a t } } ) \\leq | \\boldsymbol { S } |$ ; and (ii) Low-Rank MDPs $M _ { \\mathrm { l a t } }$ (with or without known features) in dimension $d$ admit $C _ { \\mathsf { p u s h } } ( M _ { \\mathsf { l a t } } ) \\leq d$ . Further examples include analytically sparse Low-Rank MDPs [GMR24] and Exogenous Block MDPs with weakly correlated noise [MFR24]. Our main result is as follows.\n\nTheorem 3.2 (Pushforward-coverable MDPs are statistically modular). Let $\\mathcal { M } _ { \\mathrm { 1 a t } }$ be a base MDP class such that each $M _ { \\mathrm { l a t } } \\in \\mathcal { M } _ { \\mathrm { l a t } }$ has pushforward coverability bounded by $C _ { \\mathsf { p u s h } } ( M _ { \\mathsf { l a t } } ) \\leq C _ { \\mathsf { p u s h } }$ . Then, for any decoder class $\\Phi$ , we have:\n\n$$\n\\begin{array} { r } { \\cdot \\ \\mathrm { c o m p } ( \\langle \\langle { \\mathcal M } _ { { \\sf 1 a t } } , \\Phi \\rangle , \\varepsilon , \\delta ) \\leq \\mathrm { p o l y } ( C _ { \\mathrm { p a s h } } , | { \\mathcal A } | , H , \\log | { \\mathcal M } _ { { \\sf 1 a t } } | , \\log | \\Phi | , \\varepsilon ^ { - 1 } , \\log \\left( \\delta ^ { - 1 } \\right) , \\log \\log | S | ) . } \\end{array}\n$$\n\nTheorem 3.2 shows that, modulo a term that is doubly-logarithmic in $\\vert { \\cal S } \\vert$ , latent pushforward coverability enables statistical modularity. That is, when the base (latent) dynamics satisfy pushforward coverability, there exists an algorithm for the latent-dynamics setting which scales with the statistical complexity of the base MDP class and $\\log | \\Phi |$ . We suspect that the additional $\\log \\log | S |$ factor is not essential and can be removed with a more sophisticated analysis. We note that the complexity comp chosen above is not the minimax complexity for $\\mathcal { M } _ { \\mathrm { 1 a t } }$ , since every set of pushforward coverable MDPs is also a set of coverable MDPs with a potentially smaller coverability parameter [AFK24].\n\nLet us provide some intuition for this result. We firstly note that when $M _ { \\mathrm { 1 a t } } ^ { \\star }$ has pushforward coverability parameter $C _ { \\mathsf { p u s h } }$ , it holds that for any emission process $\\psi ^ { \\star }$ , the observation-level MDP $M _ { \\circ \\mathrm { b } s } ^ { \\star } : =$ $\\langle \\langle M _ { \\mathrm { { 1 a t } } } ^ { \\star } , \\psi ^ { \\star } \\rangle \\rangle$ also satisfies pushforward coverability with the same parameter $C _ { \\mathsf { p u s h } }$ (Lemma D.5). Yet, d⟪espite acc⟫ess to realizable base MDP class $\\mathcal { M } _ { \\mathrm { l a t } }$ and decoder class $\\Phi$ , it is unclear whether the latentdynamics MDP $M _ { \\circ \\mathsf { b } s } ^ { \\star }$ satisfies any of the observation-level function approximation conditions required by existing approaches that provide sample complexity guarantees under pushforward coverability. In particular, known algorithms for this setting either require a Bellman-complete value function class [XFBJK23], a class realizing certain density ratios [AFJSX24; AFK24], or a realizable model class [AFK24], and it is highly nontrivial to construct these for the latent-dynamics MDP $M _ { \\circ \\mathrm { b } s } ^ { \\star } =$ $\\langle \\langle M _ { \\mathrm { { l a t } } } ^ { \\star } , \\psi ^ { \\star } \\rangle \\rangle$ given only the base MDP class $\\mathcal { M } _ { \\mathrm { l a t } }$ and the decoder class $\\Phi$ . Intuitively, this is because t⟪he former⟫observation-level function approximation classes capture properties of the observationlevel dynamics which cannot be obtained without some knowledge of the emission process.\n\nOur main technical contribution is to establish a new structural property for pushforward-coverable MDPs (Lemma F.1): low-dimensional linear embeddings of their latent models can approximate the Bellman updates for an arbitrary set of test functions (as long as the set is not too large). We use this property to construct low-dimensional linear features that can approximate Bellman backups in observation-space, allowing us to (approximately) satisfy the Bellman completeness assumption required to apply GOLF [JLM21] to the latent-dynamics MDP. A fascinating open question is whether a similar approach can be used to establish that standard (as opposed to pushforward) coverable MDPs are statistically modular, which would encompass all other known positive cases of statistical modularity (cf. Figure 1). We refer interested readers to a more detailed technical overview in Appendix F.1, as well as the full proof in Appendix F.2.\n\n# 4 Algorithmic Modularity\n\nWe now turn our attention to algorithmic modularity. Specifically, we aim for observable-to-latent reductions, whereby—via representation learning—RL under latent dynamics can be efficiently reduced to the simpler problem of RL with latent states directly observed. Since algorithmic modularity is a stronger property than statistical modularity, we sidestep the previous lower bounds in Section 3 through additional feedback and modeling assumptions. Our main result for this section is a new meta-algorithm, O2L, which, under these assumptions (and when equipped with an appropriately designed representation learning oracle), acts as a universal reduction in the sense that, whenever the representation learning oracle has low risk, the reduction transforms any sample-efficient algorithm for any base MDP class into a sample-efficient algorithm for the induced latent-dynamics MDP class.\n\n1: input: Epochs $T$ , episodes $K$ , decoder set $\\Phi$ , rep. learning oracle REPLEARN, base alg. $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ .   \n2: for $t = 1 , 2 , \\cdots , T$ do   \n3: REPLEARN chooses a representation $\\widehat { \\phi } ^ { ( t ) } : \\mathcal { X } \\to \\mathcal { S } \\in \\Phi$ based on data collected so far.   \n4: Initialize new instance of $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ .   \n5: for $k = 1 , 2 , \\cdots , K$ do // ALGlat plays $K$ rounds in the ${ } ^ { \\mathfrak { a } } \\widehat { \\phi } ^ { ( t ) }$ -compressed dynamics.”   \n6: $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ chooses policy $\\pi _ { \\mathrm { 1 a t } } ^ { ( t , k ) } : S \\times [ H ] \\to \\Delta ( { \\cal A } )$ .   \n7: Deploy $\\pi _ { 1 \\mathrm { a t } } \\circ \\widehat { \\phi } ^ { ( t ) }$ to collect trajectory $\\{ \\boldsymbol { x } _ { h } ^ { ( t , k ) } , \\boldsymbol { a } _ { h } ^ { ( t , k ) } , \\boldsymbol { r } _ { h } ^ { ( t , k ) } \\} _ { h = 1 } ^ { H }$ .   \n8: Update ALGlatbwith compressed trajectory {ϕ(ht (x(ht,k)), a(ht,k), r(ht,k)}hH=1.   \n9: end for   \n10: $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ returns final policy $\\widehat { \\pi } ^ { ( t ) } : S \\times [ H ] \\to \\Delta ( A )$ , deploy ${ \\widehat { \\pi } } ^ { ( t ) } \\circ { \\widehat { \\phi } } ^ { ( t ) }$ to collect one trajectory.   \n11: end for   \n12: return $\\widehat { \\pi } = \\mathsf { U n i f } \\big ( \\widehat { \\pi } ^ { ( 1 ) } \\circ \\widehat { \\phi } ^ { ( 1 ) } , \\ldots , \\widehat { \\pi } ^ { ( T ) } \\circ \\widehat { \\phi } ^ { ( T ) } \\big ) .$\n\nSetup and O2L meta-algorithm. For the results in this section, we denote the (unknown) latentdynamics MDP of interest by $M _ { \\circ \\flat \\mathsf { s } } ^ { \\star } : = \\langle \\langle M _ { \\mathsf { l a t } } ^ { \\star } , \\psi ^ { \\star } \\rangle \\rangle$ , and use $\\phi ^ { \\star } : = ( \\psi ^ { \\star } ) ^ { - 1 }$ to denote the true decoder. The O2L meta-algorithm (Algorithm 1)⟪learns a n⟫ear-optimal policy for $M _ { \\circ \\mathrm { b s } } ^ { \\star }$ by alternating between performing representation learning and executing a black-box “base” RL algorithm (designed for the base MDP) on the learned representation; this approach is inspired by empirical methods that blend representation learning and RL in the latent space (e.g., [GKBNB19; SAGHCB20; $\\mathrm { N i } { + } 2 4 ]$ ).\n\nConcretely, the algorithm takes as input a representation learning oracle REPLEARN and a base RL algorithm $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ that operates in the latent space. In each epoch $t \\in [ T ]$ , REPLEARN produces a new representation ${ \\widehat { \\phi } } ^ { ( t ) } : { \\mathcal { X } } \\to S$ based on data observed so far (potentially using additional side information, which we bwill elaborate on in the sequel). Then, the reduction invokes $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ , using ${ \\widehat { \\phi } } ^ { ( t ) }$ to simulate access to the true latent states. In particular, $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ runs for $K$ episodes, where at eacbh episode $k$ : (i) $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ produces a latent policy $\\pi _ { \\mathsf { l a t } } ^ { ( t , k ) } : { \\mathcal { S } } \\times [ H ] \\to \\Delta ( { \\mathcal { A } } )$ , (ii) the latent policy is transformed into an observation-level policy via composition with ${ \\widehat { \\phi } } ^ { ( t ) }$ , i.e. $\\pi _ { \\mathrm { l a t } } { \\widehat { ( t , k ) } } _ { \\mathrm { ~ O ~ } } \\widehat { \\phi } ^ { ( t ) }$ , which is then deployed to produce a trajectory $\\{ \\boldsymbol { x } _ { h } ^ { ( t , k ) } , \\boldsymbol { a } _ { h } ^ { ( t , k ) } , \\boldsymbol { r } _ { h } ^ { ( t , k ) } \\} _ { h = 1 } ^ { H }$ (ht,k), r(ht,k)}hH=1, and (biii) the trajectory ibs compressed through ϕ(t) and used to update $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ via $\\{ \\widehat { \\phi } _ { h } ^ { ( t ) } ( x _ { h } ^ { ( t , k ) } ) , a _ { h } ^ { ( t , k ) } , r _ { h } ^ { ( t , k ) } \\} _ { h = 1 } ^ { H }$ (cf. Line 8 of Algorithm 1).9 Afte bthe rounds conclude, $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ prodbuces a final latent policy $\\widehat { \\pi } _ { \\mathrm { l a t } } ^ { ( t ) } : S \\times [ H ] \\to \\Delta ( A )$ . The final policy $\\widehat { \\pi }$ chosen by the $\\mathtt { O 2 L }$ algorithm is a uniform mixture of $\\widehat { \\pi } _ { \\mathrm { l a t } } ^ { ( t ) } \\circ \\widehat { \\phi } ^ { ( t ) }$ over all the epochs.\n\nTbhe central assumption behind $_ { \\mathrm { O 2 L } }$ is that the base ablgorithm $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ can achieve low-risk in the underlying base MDP $M _ { \\mathrm { 1 a t } } ^ { \\star }$ if given access to the true latent states $s _ { h } = \\phi ^ { \\star } ( x _ { h } )$ . Beyond this assumption, we require that the representation learning oracle REPLEARN can learn a sufficiently high-quality representation. In our applications, this will be made possible by assuming access to a realizable decoder class $\\Phi$ and two distinct assumptions: hindsight observability (Section 4.1) and conditions enabling self-predictive representation learning (Section 4.2). We will show that under these conditions, we can instantiate a representation learning oracle such that $_ { \\mathrm { O 2 L } }$ inherits the sample complexity guarantee for $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ , thereby achieving algorithmic modularity.\n\n# 4.1 Algorithmic modularity via hindsight observability\n\nOur first algorithmic result bypasses the hardness in Section 3 by considering the setting of hindsight observability, which has garnered recent interest in the context of POMDPs [LADZ23; GCWXWB24; SLS23; LXJZV24]. Here, we assume that at training time (but not during deployment), the algorithm has access to additional feedback in the form of the true latent states, which are revealed at the end of each episode.\n\nAssumption 4.1 (Hindsight Observability [LADZ23]). The latent states $( \\phi _ { 1 } ^ { \\star } ( x _ { 1 } ) , \\ldots , \\phi _ { H } ^ { \\star } ( x _ { H } ) )$ are revealed to the learner after each episode $( x _ { 1 } , a _ { 1 } , r _ { 1 } , \\dots , x _ { H } , a _ { H } , r _ { H } )$ concludes.\n\nWe emphasize that in the hindsight observability framework, the learner must still execute observationspace policies $\\pi _ { \\mathsf { o b s } } : { \\mathcal { X } } \\times [ H ] { \\stackrel { } { \\to } } \\Delta ( { \\mathcal { A } } )$ , as the latent states are only revealed at the end of each episode. Under hindsight observability, we can instantiate the representation learning oracle in $_ { \\mathrm { O 2 L } }$ so that the reduction achieves low risk for any choice of black-box base algorithm $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ . In particular, we make use of online classification oracles, which use the revealed latent states to achieve low classification loss with respect to $\\phi ^ { \\star }$ under adaptively generated data. We first state a guarantee based on generic classification oracles, then instantiate it to give a concrete end-to-end sample complexity bound.\n\nFormally, at each step $t$ , the online classification oracle, denoted via $\\mathrm { R E P _ { c l a s s } }$ , is given the states and hindsight observations collected so far and produces a deterministic estimate ${ \\widehat { \\phi } } ^ { ( t ) } =$ $\\mathrm { R E P } _ { \\mathrm { c l a s s } } \\big ( \\{ x _ { h } ^ { ( i ) } , \\phi _ { h } ^ { \\overline { { \\star } } } ( x _ { h } ^ { ( i ) } ) \\} _ { i < t , h \\le H } \\big )$ for the true decoder $\\phi ^ { \\star }$ . We measure the regret of the or cble via the $0 / 1$ loss for classification:\n\n$$\n\\mathtt { R e g } _ { \\mathrm { c l a s s } } ( T ) : = \\sum _ { t = 1 } ^ { T } \\sum _ { h = 1 } ^ { H } \\mathbb { E } _ { \\pi ^ { ( t ) } \\sim p ^ { ( t ) } } \\mathbb { E } ^ { \\pi ^ { ( t ) } } \\left[ \\mathbb { I } \\big \\lbrace \\widehat { \\phi } _ { h } ^ { ( t ) } ( x _ { h } ) \\neq \\phi _ { h } ^ { \\star } ( x _ { h } ) \\big \\rbrace \\right] ,\n$$\n\nwhere $p ^ { ( t ) }$ represents a randomization distribution over the policy $\\pi ^ { ( t ) }$ . Our reduction succeeds under the assumption that the oracle has low expected regret.\n\nAssumption 4.2. For any (possibly adaptive) sequence $\\pi ^ { ( t ) }$ , with $\\pi ^ { ( t ) } \\sim p ^ { ( t ) }$ , the online classification oracle $\\mathrm { R E P _ { c l a s s } }$ has expected regret bounded by\n\n$$\n\\mathbb E [ \\mathsf { R e g } _ { \\mathsf { c l a s s } } ( T ) ] \\leq \\mathsf E \\mathsf { s t } _ { \\mathsf { c l a s s } } ( T ) ,\n$$\n\nwhere $\\mathsf { E s t } _ { \\mathsf { c l a s s } } ( T )$ is a known upper bound.\n\nWe apply such an oracle within $_ { \\mathrm { O 2 L } }$ as follows: at the end of each iteration $\\textit { t } \\in \\ [ T ]$ in $_ { \\mathrm { O 2 L } }$ $k \\ \\sim \\ \\left\\lceil K \\right\\rceil$ ; euepdthate ptrhoeof aosf iTfihceatoiroenmor4a.c1 fowr tdhettahile trajWece olreyt $\\big ( x _ { 1 } ^ { ( t , k ) } , a _ { 1 } ^ { ( t , k ) } , r _ { 1 } ^ { ( \\overline { { t } } , k ) } \\big ) , \\dots , \\big ( \\dot { x } _ { H } ^ { ( \\overline { { t } } , k ) } a _ { H } ^ { ( t , k ) } , r _ { H } ^ { ( \\overline { { t } } , k ) } \\big )$   \n$\\mathsf { R i s k } _ { \\mathsf { o b s } } ( T K )$ denote the risk of the O2L reduction when run for epochs of episodes, and we let $\\mathtt { R i s k } _ { \\star } ( K ) : = \\mathbb { E } [ \\mathtt { R i s k } ( K , \\mathtt { A L G } _ { \\mathtt { l a t } } , M _ { \\mathtt { l a t } } ^ { \\star } ) ]$ denote the expected risk of $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ when executed on $M _ { \\mathrm { 1 a t } } ^ { \\star }$ with access to the true latent states $s _ { h } \\doteq \\phi ^ { \\star } ( x _ { h } )$ for $K$ episodes.\n\nTheorem 4.1 (Risk bound for O2L under hindsight observability). Let $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ be a base algorithm with base risk $\\mathsf { R i s k } _ { \\star } ( K )$ , and REPclass $a$ representation learning oracle satisfying Assumption 4.2. Then Algorithm $\\jmath$ , with inputs $T , K , \\Phi$ , $\\mathrm { R E P _ { c l a s s } }$ , and $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ , has expected risk\n\n$$\n\\mathbb { E } [ \\mathsf { R i s k } _ { \\mathsf { o b s } } ( T K ) ] \\leq \\mathsf { R i s k } _ { \\star } ( K ) + \\frac { 2 K } { T } \\mathsf { E s t } _ { \\mathsf { c l a s s } } ( T ) .\n$$\n\nThis result shows that we can achieve sublinear risk under latent dynamics as long as (i) the base algorithm achieves sublinear risk $\\mathsf { R i } s \\mathsf { k } _ { \\star } ( K )$ given access to the true latent states, and (ii) the classification oracle achieves sublinear regret $\\mathsf { E s t } _ { \\mathsf { c l a s s } } ( T )$ . Notably, the result is fully modular, meaning we require no explicit conditions on the latent dynamics or the base algorithm, and is computationally efficient whenever the base algorithm and classification oracle are efficient.\n\nTo make Theorem 4.1 concrete, we next provide a representation learning oracle (EXPWEIGHTS.DR; Algorithm 3 in Appendix G.1) based on a derandomization of the classical exponential weights mechanism, which satisfies Assumption 4.2 with Estclass $\\lesssim H \\log | \\Phi |$ whenever it has access to a class $\\Phi$ that satisfies decoder realizability.\n\nLemma 4.1 (Online classification via EXPWEIGHTS.DR). Under decoder realizability $\\left( \\phi ^ { \\star } \\in \\Phi \\right)$ ), EXPWEIGHTS.DR (Algorithm 3) satisfies Assumption $4 . 2 \\ : \\dot { w } i t h ^ { 1 0 }$\n\n$$\n\\mathsf { E s t } _ { \\mathsf { c l a s s } } ( T ) = \\widetilde { \\mathcal { O } } ( H \\log | \\Phi | ) .\n$$\n\nInstantiating Theorem 4.1 with the above representation learning oracle, we obtain the following algorithmic modularity result.\n\nCorollary 4.1 (Algorithmic modularity under hindsight observability). For any base algorithm $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ , under decoder realizability ( $\\phi ^ { \\star } \\in \\Phi$ ), O2L with inputs $T , K , \\Phi$ , EXPWEIGHTS.DR, and $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ achieves\n\n$$\n\\mathbb { E } [ \\mathsf { R i s k } _ { \\mathsf { o b s } } ( T K ) ] \\lesssim \\mathsf { R i s k } _ { \\star } ( K ) + \\frac { H K \\log \\lvert \\Phi \\rvert } { T } .\n$$\n\nConsequently, for any $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ , setting $T \\approx K H \\log \\lvert \\Phi \\rvert / \\mathsf { R i s k } _ { \\star } ( K )$ achieves $\\mathbb { E } [ \\mathsf { R i s k } _ { \\mathsf { o b s } } ( T K ) ] \\lesssim$ $\\mathsf { R i s k } _ { \\star } ( K )$ with a number of trajectories $T K = \\widetilde { \\mathcal { O } } \\big ( K ^ { 2 } H \\log | \\Phi | \\big / \\mathsf { \\bar { \\rho } s i s k } _ { \\star } ( K ) \\big )$ .\n\nBeyond achieving algorithmic modularity, this result shows that under hindsight observability, we can achieve strong statistical modularity (modulo possible $H$ factors) for every base MDP class $\\mathcal { M } _ { \\mathrm { l a t } }$ , an important result in its own right.11 As an example, suppose that $\\mathsf { R i s k } _ { \\star } ( K ) = \\mathcal { O } ( K ^ { - 1 / 2 } )$ , which is satisfied by many standard algorithms of interest [JKALS17; JYWJ20; JLM21; FKQR21]. Then, setting $T$ according to Corollary 4.1 obtains an expected risk bound of $\\varepsilon$ using $\\mathcal { O } \\big ( H \\log \\vert \\Phi \\vert \\big / \\varepsilon ^ { 5 } \\big )$ trajectories.\n\nRemark 4.1 (Online versus offline oracles). Theorem 4.1 critically uses that assumption that $\\mathrm { R E P _ { c l a s s } }$ satisfies an online classification error bound to handle the fact that data is generated adaptively based on the estimators $\\widehat { \\phi } ^ { ( 1 ) } , \\ldots , \\widehat { \\phi } ^ { ( T ) }$ it produces, which is by now a relatively standard technique in the design of interac ibve decisibon making algorithms [FR20; FKQR21; FR23]. We note that under coverability and other exploration conditions, online oracles for classification can be directly obtained from offline (i.e. supervised) classification oracles [XFBJK23; BRS24; FHQR24].\n\n# 4.2 Algorithmic modularity via self-predictive estimation\n\nWe complement the above results by studying the general online RL setting without hindsight observations. To address this more challenging setting, we design an optimistic self-predictive estimation objective (Eq. (7)), which learns a representation by jointly fitting a decoder together with a latent model. We prove that any representation learning oracle that attains low regret with respect to this objective can be used in O2L to obtain observable-to-latent reductions for any low-risk base algorithm $\\mathbf { A L G } _ { \\mathrm { 1 a t } }$ (for a formal statement, see Theorem A.1). We provide a (computationally inefficient) estimator (SELFPREDICT.OPT; Algorithm 4 in Appendix H.1) which we show attains low optimistic self-regret under certain statistical conditions (namely, coverability of the base MDP and a function approximation condition enabling us to express the self-prediction target as a latent model, see Lemma A.1 for a formal statement), thereby obtaining an end-to-end reduction for the general online RL setting. For lack of space, these results are deferred to Appendix A.\n\n# 5 Discussion\n\nOur work initiates the study of statistical and algorithmic modularity for reinforcement learning under general latent dynamics. Our positive and negative results serve as a first step toward a unified theory for reinforcement learning in the presence of high-dimensional observations. To this end, we close with some important future directions and open problems.\n\nStatistical modularity. Can we obtain a unified characterization for the statistical complexity of RL under latent dynamics with a given class of base MDPs $\\mathcal { M } _ { \\mathrm { l a t } } \\}$ Our results in Section 3 suggest that this will require new tools that go beyond existing notions of statistical complexity. Toward resolving this problem, concrete questions that are not yet understood include: (i) Is coverability [XFBJK23] (as opposed to pushforward coverability) sufficient for learnability under latent dynamics? (ii) Is the Exogenous Block MDP problem [EMKAL22; MFR24]—a special case of our general framework— statistically tractable? Lastly, are there additional types of feedback that are weaker than hindsight observability, yet suffice to bypass the hardness results in Section 3?\n\nAlgorithmic modularity. Can we derive a unified representation learning objective that enables algorithmic modularity whenever statistical modularity is possible? Ideally, such an objective would be computationally tractable. Alternatively, can we show that algorithmic modularity fundamentally requires stronger modeling assumptions than statistical modularity? Toward addressing the problems above, a first step might be to understand: (i) What are the minimal statistical assumptions under which we can minimize the self-predictive objective in Section 4.2? (ii) How can we encourage finding good representations via self-prediction beyond the use of optimism over the base (latent) models; and (iii) when can we minimize self-prediction in a computationally efficient fashion?",
    "summary": "{\n  \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n现实世界中强化学习的应用常涉及智能体在复杂高维观测环境下运行，但底层潜在动态相对简单。目前，除小潜在空间等受限设置外，对于潜在动态下强化学习的基本统计要求和算法原理了解甚少。现有理论和实证工作在结合表示学习和强化学习开发可扩展算法时存在局限性，如针对特定类别的潜在动态模型、分析复杂以及缺乏模块化等。解决该问题对于构建统一的统计和算法理论，推动潜在动态下强化学习的实际应用具有重要意义。\\n\\n**方法概述**\\n论文从统计和算法角度研究一般潜在动态下的强化学习问题。统计方面，识别出潜在前推可覆盖性作为实现统计可处理性的一般条件；算法方面，开发了可证明有效的从可观测到潜在的约简方法，将任意潜在马尔可夫决策过程（MDP）算法转换为可处理丰富观测的算法。\\n\\n**主要贡献与效果**\\n- 提出一般潜在动态下的强化学习新框架，不假设潜在空间小或基础MDP有特定结构，朝着构建统一模块化理论迈出第一步。\\n- 统计模块化方面，证明大多数涉及函数逼近的强化学习设置与丰富观测结合时会变得难以处理（定理3.1），即存在一个解码器类和基础MDP族，即使基础动态完全已知，任何算法学习潜在动态MDP的近似最优策略都需要至少 $\\min \\{ \\sqrt { S }, 2 ^ { \\Omega ( H ) } / H, | \\Phi | / \\log | \\Phi | \\}$ 个回合；同时确定潜在前推可覆盖性为使统计可处理的一般条件（定理3.2），满足该条件的潜在动态MDP类的统计复杂度为 $\\mathrm {poly} ( C _ { \\mathrm {push} }, | { \\mathcal A } |, H, \\log | { \\mathcal M } _ { { \\sf 1at } } |, \\log | \\Phi |, \\varepsilon ^ { - 1 }, \\log ( \\delta ^ { - 1 } ), \\log \\log | S | )$。\\n- 算法模块化方面，开发了O2L元算法，在后视可观测性和自预测表示学习条件下，将基础MDP类的任意样本高效算法转换为潜在动态MDP类的样本高效算法，实现算法模块化。例如，在后视可观测性下，若基础算法达到次线性风险，分类Oracle达到次线性遗憾，O2L算法的预期风险 $\\mathbb { E } [ \\mathsf { Risk } _ { \\mathsf {obs} } ( T K ) ] \\leq \\mathsf { Risk } _ { \\star } ( K ) + \\frac { 2 K } { T } \\mathsf { Est } _ { \\mathsf {class} } ( T )$ ；当使用EXPWEIGHTS.DR作为表示学习Oracle时，在解码器可实现性条件下，设置合适参数可使O2L算法的预期风险接近基础算法在直接访问潜在状态时的风险。\",\n  \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n该方法的核心原理是将强化学习中的潜在动态与复杂观测过程解耦，通过构建一个模块化的框架，在统计方面寻找能使潜在动态下强化学习具有统计可处理性的结构条件，即潜在前推可覆盖性；在算法方面，利用表示学习将潜在状态从观测中解码出来，然后在学习到的表示上应用针对潜在动态的强化学习算法。通过交替进行表示学习和执行针对潜在MDP的“基础”强化学习算法，实现从可观测到潜在的约简，从而解决潜在动态下的强化学习问题。其有效的原因在于利用潜在前推可覆盖性这一结构参数，使算法在满足该条件的潜在动态下能够实现样本高效的强化学习。\\n\\n**创新点**\\n先前的工作存在一些问题：一是针对特定类别的潜在动态模型，通用性受限；二是分析复杂，限制了算法发展；三是缺乏模块化，表示学习过程与特定的潜在强化学习算法选择紧密相关，使用不便。与先前工作相比，本文不假设潜在马尔可夫决策过程（MDP）有特定结构，而是在更一般的情况下研究强化学习，同时引入潜在前推可覆盖性这一结构参数，为样本高效的强化学习提供了新的思路。此外，开发的O2L元算法通过交替进行表示学习和执行基础RL算法，实现了算法的模块化，可将任意样本高效的基础MDP类算法转换为诱导的潜在动态MDP类的样本高效算法。\\n\\n**具体实现步骤**\\n1. 在线强化学习中，学习算法ALG反复与未知MDP $M ^ { \\star }$ 交互，执行策略并观察轨迹，经过$T$轮交互后输出最终策略 $\\widehat { \\pi }$，目标是最小化风险 $\\mathsf { Risk } ( T, \\mathrm { ALG }, M ^ { \\star } ) = J ^ { M ^ { \\star } } ( \\pi _ { M ^ { \\star } } ) - J ^ { M ^ { \\star } } ( \\widehat \\pi )$。\\n2. 定义潜在动态MDP，包括基础MDP $M _ { \\mathrm {lat} } = \\{ \\boldsymbol { S }, \\boldsymbol { A }, \\{ P _ { \\mathrm {lat}, h } \\} _ { h = 0 } ^ { H }, \\{ \\dot { R _ { \\mathrm {lat}, h } } \\} _ { h = 1 } ^ { H }, \\dot { H } \\}$ 和可解码发射过程 $\\psi = \\{ \\psi _ { h } : S \\to \\Delta ( \\mathcal { X } ) \\} _ { h = 1 } ^ { H }$，明确潜在状态可从观测中唯一解码。定义潜在动态MDP类 $\\langle \\langle \\mathcal { M } _ { \\mathrm {lat} }, \\Phi \\rangle \\rangle = \\{ \\langle \\langle M _ { \\mathrm {lat} }, \\psi \\rangle \\rangle : M _ { \\mathrm {lat} } \\in \\mathcal { M } _ { \\mathrm {lat} }, \\psi \\text{ 是可解码的}, \\psi ^ { - 1 } \\in \\Phi \\}$，考虑基础MDP类和译码器类的组合。\\n3. 统计模块化方面：\\n    - 定义MDP类的统计复杂性：若存在算法ALG，对于每个 $M \\in \\mathcal { M }$，在 $T = \\mathsf { comp } ( \\mathcal { M }, \\varepsilon, \\delta )$ 轮在线交互后，以至少 $1 - \\delta$ 的概率使 $\\mathsf { Risk } ( T, \\mathbf { ALG }, M ) \\leq \\varepsilon$，则称MDP类 $\\mathcal { M }$ 可以用 $\\mathsf { comp } ( \\mathcal { M }, \\varepsilon, \\delta )$ 个样本学习到 $\\varepsilon$ - 最优。定义基础MDP类 $\\mathcal { M } _ { \\mathrm {lat} }$ 的统计模块化概念，若对于任何解码器类 $\\Phi$，诱导的潜在动态MDP类 $\\langle \\langle \\mathcal { M } _ { \\mathrm {lat} }, \\Phi \\rangle \\rangle$ 的统计复杂度是关于基础类的统计复杂度和译码器类容量的多项式，则称 $\\mathcal { M } _ { \\mathrm {lat} }$ 具有统计模块化。\\n    - 证明大多数涉及函数逼近的MDP类不具有统计模块化（定理3.1）：对于每个 $N \\geq 4$，存在一个解码器类 $\\Phi$ （$| \\Phi | = N$）和一个基础MDP族 $\\mathcal { M } _ { \\mathrm {lat} }$ ，满足一系列条件，使得即使基础动态完全已知，强统计模块化也是不可能的，任何算法学习潜在动态MDP的近似最优策略都需要至少 $\\min \\{ \\sqrt { S }, 2 ^ { \\Omega ( H ) } / H, | \\Phi | / \\log | \\Phi | \\}$ 个回合。\\n    - 识别出潜在前推可覆盖性使潜在动态下的强化学习具有统计模块化（定理3.2）：对于基础MDP类 $\\mathcal { M } _ { \\mathrm {lat} }$ ，若其中每个 $M _ { \\mathrm {lat} }$ 的前推可覆盖性系数 $C _ { \\mathsf {push} } ( M _ { \\mathrm {lat} } ) \\leq C _ { \\mathsf {push} }$ ，则对于任何解码器类 $\\Phi$，诱导的潜在动态MDP类 $\\langle \\langle \\mathcal { M } _ { \\mathrm {lat} }, \\Phi \\rangle \\rangle$ 的统计复杂度为 $\\mathrm {poly} ( C _ { \\mathrm {push} }, | { \\mathcal A } |, H, \\log | { \\mathcal M } _ { { \\sf 1at } } |, \\log | \\Phi |, \\varepsilon ^ { - 1 }, \\log ( \\delta ^ { - 1 } ), \\log \\log | S | )$。通过建立新的结构性质，构造低维线性特征，满足Bellman完备性假设。\\n4. 算法模块化方面：\\n    - O2L元算法：输入训练轮数 $T$、每轮的回合数 $K$、解码器集合 $\\Phi$、表示学习预言机REPLEARN和基础RL算法 $\\mathbf { ALG } _ { \\mathrm {lat} }$。在每一轮 $t$ 中，REPLEARN根据已收集的数据选择一个新的表示 $\\widehat { \\phi } ^ { ( t ) } : { \\mathcal { X } } \\to S$；调用 $\\mathbf { ALG } _ { \\mathrm {lat} }$，在每回合 $k$ 中，$\\mathbf { ALG } _ { \\mathrm {lat} }$ 生成潜在策略 $\\pi _ { \\mathrm {lat} } ^ { ( t, k ) } : { \\mathcal { S } } \\times [ H ] \\to \\Delta ( { \\mathcal { A } } )$，将潜在策略与 $\\widehat { \\phi } ^ { ( t ) }$ 组合得到观测级策略并部署以产生轨迹，将轨迹通过 $\\widehat { \\phi } ^ { ( t ) }$ 压缩后更新 $\\mathbf { ALG } _ { \\mathrm {lat} }$；每轮结束后，$\\mathbf { ALG } _ { \\mathrm {lat} }$ 产生最终潜在策略 $\\widehat { \\pi } _ { \\mathrm {lat} } ^ { ( t ) } : S \\times [ H ] \\to \\Delta ( A )$，最终策略 $\\widehat { \\pi }$ 是所有轮次中 $\\widehat { \\pi } _ { \\mathrm {lat} } ^ { ( t ) } \\circ \\widehat { \\phi } ^ { ( t ) }$ 的均匀混合。\\n    - 后视可观测性：在训练时假设可获取真实潜在状态，利用在线分类Oracle，在O2L中实现低风险（定理4.1）。在线分类Oracle $\\mathrm { REP _ { class } }$ 根据收集的状态和后视观测数据产生对真实解码器 $\\phi ^ { \\star }$ 的确定性估计 $\\widehat { \\phi } ^ { ( t ) }$ ，其遗憾通过 $0/1$ 损失衡量，若满足 $\\mathbb { E } [ \\mathsf { Reg } _ { \\mathsf {class} } ( T ) ] \\leq \\mathsf { Est } _ { \\mathsf {class} } ( T )$ ，则O2L算法的预期风险 $\\mathbb { E } [ \\mathsf { Risk } _ { \\mathsf {obs} } ( T K ) ] \\leq \\mathsf { Risk } _ { \\star } ( K ) + \\frac { 2 K } { T } \\mathsf { Est } _ { \\mathsf {class} } ( T )$。\\n    - 自预测估计：设计乐观自预测估计目标（Eq. (7)），学习表示，在满足一定统计条件（基础MDP的可覆盖性和函数逼近条件）下，可用于O2L实现可观测到潜在的约简（定理A.1）。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n  \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中提及的基线模型包括：Tabular、Contextual Bandits、Low-Rank MDP、Known Deterministic MDP、Low State Occupancy、Model Class + Pushforward Coverability、Linear CB/MDP、Model Class + Coverability、Known Stochastic MDP、Bellman Rank、Eluder Dimension + Bellman Completeness、Q*-Irrelevant State Abstraction、Linear Mixture MDP、Linear Q*/V*、Low State/State-Action Occupancy、Bisimulation、Low State-Action Occupancy、Model Class + Coverability等。\\n\\n**性能对比**\\n*   **在统计模块化指标上**：大多数研究良好的涉及函数逼近的MDP类，如Bellman Rank、Linear Q*/V*等，不具有统计模块化（定理3.1）。定理3.1表明存在一个解码器类和基础MDP族，即使基础动态完全已知，强统计模块化也是不可能的，任何算法学习潜在动态MDP的近似最优策略都需要至少 $\\min \\{ \\sqrt { S }, 2 ^ { \\Omega ( H ) } / H, | \\Phi | / \\log | \\Phi | \\}$ 个回合。而Tabular、Contextual Bandits、Low-Rank MDP等满足一定条件的MDP类具有统计模块化。定理3.2表明潜在前推可覆盖的MDP类具有统计模块化，其统计复杂度为 $\\mathrm {poly} ( C _ { \\mathrm {push} }, | { \\mathcal A } |, H, \\log | { \\mathcal M } _ { { \\sf 1at } } |, \\log | \\Phi |, \\varepsilon ^ { - 1 }, \\log ( \\delta ^ { - 1 } ), \\log \\log | S | )$，即存在算法使潜在动态设置的复杂度与基础MDP类的统计复杂度和 $\\log | \\Phi |$ 成比例，优于不具有统计模块化的MDP类。\\n*   **在算法模块化指标上**：在后视可观测性的设置下，O2L算法在满足一定条件时可继承基础算法的样本复杂度保证，实现算法模块化。与传统方法相比，该算法无需对潜在动态或基础算法有明确条件，只要基础算法和分类预言机有效，就可在潜在动态下实现次线性风险。定理4.1表明O2L算法的预期风险 $\\mathbb { E } [ \\mathsf { Risk } _ { \\mathsf {obs} } ( T K ) ] \\leq \\mathsf { Risk } _ { \\star } ( K ) + \\frac { 2 K } { T } \\mathsf { Est } _ { \\mathsf {class} } ( T )$ ，与基础算法在可获得真实潜在状态时的预期风险相关，且依赖于分类Oracle的预期遗憾上界。当使用EXPWEIGHTS.DR作为表示学习Oracle时，在解码器可实现性条件下，O2L算法的预期风险 $\\mathbb { E } [ \\mathsf { Risk } _ { \\mathsf {obs} } ( T K ) ] \\lesssim \\mathsf { Risk } _ { \\star } ( K ) + \\frac { H K \\log \\lvert \\Phi \\rvert } { T }$ ，通过合理设置 $T$ 可实现与基础算法相当的风险，体现了其在算法模块化方面的优势。\",\n  \"keywords\": \"### 关键词\\n\\n- 强化学习 (Reinforcement Learning, RL)\\n- 潜在动态 (Latent Dynamics, N/A)\\n- 统计模块化 (Statistical Modularity, N/A)\\n- 算法模块化 (Algorithmic Modularity, N/A)\\n- 前推可覆盖性 (Pushforward Coverability, N/A)\\n- O2L元算法 (O2L Meta - Algorithm, N/A)\"\n}"
}