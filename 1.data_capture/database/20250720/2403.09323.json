{
    "link": "https://arxiv.org/abs/2403.09323",
    "pdf_link": "https://arxiv.org/pdf/2403.09323",
    "title": "E2E-MFD: Towards End-to-End Synchronous Multimodal Fusion Detection",
    "authors": [
        "Jiaqing Zhang",
        "Mingxiang Cao",
        "Xue Yang",
        "Weiying Xie",
        "Jie Lei",
        "Daixun Li",
        "Geng Yang",
        "Wenbo Huang",
        "Yunsong Li"
    ],
    "institutions": [
        "The State Key Laboratory of Integrated Services Networks, Xidian University",
        "University of Technology Sydney",
        "Southeast University",
        "Shanghai AI Laboratory"
    ],
    "publication_date": "2024-03-14",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 6,
    "influential_citation_count": 1,
    "paper_content": "# E2E-MFD: Towards End-to-End Synchronous Multimodal Fusion Detection\n\nJiaqing Zhang1, Mingxiang $\\mathbf { C a o ^ { 1 } }$ , Weiying $\\mathbf { X _ { i e } ^ { \\bullet } }$ , Jie Lei2, Daixun $\\mathbf { L i } ^ { 1 }$ , Wenbo Huang3, Yunsong $\\mathbf { L i } ^ { 1 }$ , Xue Yang4∗\n\n1The State Key Laboratory of Integrated Services Networks, Xidian University 2University of Technology Sydney 3Southeast University 4Shanghai AI Laboratory\n\nhttps://github.com/icey-zhang/E2E-MFD\n\n# Abstract\n\nMultimodal image fusion and object detection are crucial for autonomous driving. While current methods have advanced the fusion of texture details and semantic information, their complex training processes hinder broader applications. Addressing this challenge, we introduce E2E-MFD, a novel end-to-end algorithm for multimodal fusion detection. E2E-MFD streamlines the process, achieving high performance with a single training phase. It employs synchronous joint optimization across components to avoid suboptimal solutions associated to individual tasks. Furthermore, it implements a comprehensive optimization strategy in the gradient matrix for shared parameters, ensuring convergence to an optimal fusion detection configuration. Our extensive testing on multiple public datasets reveals E2E-MFD’s superior capabilities, showcasing not only visually appealing image fusion but also impressive detection outcomes, such as a $3 . 9 \\%$ and $2 . 0 \\% \\mathrm { m A P } _ { 5 0 }$ increase on horizontal object detection dataset M3FD and oriented object detection dataset DroneVehicle, respectively, compared to state-of-the-art approaches.\n\n# 1 Introduction\n\nPrecise and reliable object parsing is critical in fields such as autonomous driving [1] and remote sensing monitoring [2]. Relying solely on visible sensors can lead to inaccuracies in object recognition in challenging environments, like inclement weather conditions. Visible-infrared image fusion [3; 4; 5; 6] as a typical common multimodal fusion (MF) task addresses these challenges by leveraging complementary information from different modalities, leading to the rapid development of various multimodal image fusion techniques [7; 8; 9; 10; 11]. Techniques like CDDFuse [12] and DIDFuse [13] employ a two-step process where a MF network is trained initially, followed by training an object detection (OD) network with the results from the MF network to assess fusion effectiveness separately. Although deep neural networks have significantly enhanced the ability to learn representations across modalities, resulting in promising multimodal fusion outcomes, the focus has predominantly been on producing visually appealing images. This emphasis often overlooks the improvement of downstream high-level visual tasks, such as enhanced object parsing, which remains a substantial hurdle.\n\nRecent studies have devoted into designing joint learning methods that integrate fusion networks with high-level tasks such as object detection [14] and segmentation [15; 16]. The synergy between MF and OD in Multimodal Fusion Detection (MFD) methods has emerged as a vibrant area of research. This partnership allows MF to produce richer, more informative images, enhancing OD performance, while OD contributes valuable object semantic insights to MF, aiming to accurately locate and identify objects in a scene. Typically, MFD networks adopt a cascaded design where joint\n\nStage 1 Stage 2 Stage1 min ωLu(u,Φ(x,y;0u))+(1-ω)Ld(d,y(x,y;0a))+S(0\\*)   \n(a) MF OD OD x,y Φ u 亚 d Backbone Task-alignment Stage 2 Backbone Nodes (d) Stage 1 MF   \n(b) MF OD Root Backbone Stage 3 Branches S Stage 1 Stage 2 Multimodal Images Fusion Image Detection Result   \n(c) MF OD OD Object Detection Network MF Multimodal Fusion Network\n\noptimization techniques [17] use the OD network to guide the MF network toward creating images that facilitate easier object detection. Notably, Zhao et. al [18] introduced a joint learning method for multimodal fusion detection, incorporating meta-feature embedding from OD to improve fusion by generating semantic object features through meta-learning simulation. Despite these advancements, as highlighted in Figure 1, significant challenges persist: 1) Current optimization approaches rely on a multi-step, progressively joint method, compromising efficiency; 2) These methods overly focus on leveraging OD information for fusion enhancement, leading to difficulty in parameter balancing and susceptibility to local optima of individual tasks. Therefore, the quest for a unified feature set that simultaneously caters to each task remains formidable.\n\nIn this paper, we introduce E2E-MFD, an end-to-end algorithm for multimodal fusion detection, designed to seamlessly blend detailed image fusion and object detection from coarse to fine levels. E2E-MFD facilitates the interaction of intrinsic features from both domains through synchronous joint optimization, allowing for a streamlined, one-stage process. To reconcile fine-grained details with semantic information, we propose the novel concept of an Object-Region-Pixel Phylogenetic Tree (ORPPT) coupled with a coarse-to-fine diffusion processing (CFDP) mechanism. This approach is inspired by the natural process of visual perception, tailored to meet the specific needs of MF and OD. Furthermore, we introduce a Gradient Matrix Task-Alignment (GMTA) technique to fine-tune the optimization of shared components, thereby minimizing the adverse impacts traditionally associated with inherent optimization challenges. This ensures an efficient convergence towards an optimal set of fusion detection weights, enhancing both the accuracy and efficacy of multimodal fusion detection.\n\nOur contributions in this paper are highlighted as follows: (1) We present E2E-MFD, a pioneering approach to efficient synchronous joint learning, innovatively integrating image fusion and object detection into a single-stage, end-to-end framework. This methodology significantly enhances the outcomes of both tasks. (2) We introduce a novel GMTA technique, designed to evaluate and quantify the impacts of the image fusion and object detection tasks. This aids in optimizing the training process’s stability and ensures convergence to an optimal configuration of fusion detection weights. (3) Through comprehensive experimentation on image fusion and object detection, we demonstrate the efficacy and robustness of our proposed method.\n\n# 2 Related Work\n\n# 2.1 Multimodal Fusion Object Detection\n\nDue to the powerful nonlinear fitting capabilities of deep neural networks, deep learning has made significant progress in low-level vision tasks, particularly in image fusion tasks [19; 20; 7; 21; 22; 23; 24]. Early efforts [9; 25; 13; 26; 27; 28] tended to achieve excellent fusion results by adjusting network structures or loss functions, overlooking the fact that image fusion should aim to improve the performance of downstream application tasks. Fusion images with good quality metrics may be suitable for human visual perception but may not be conducive to practical application tasks [16; 29]. Some research has acknowledged this issue, Yuan et al. [30] utilized various aligned modalities for improved oriented object detection [31; 32; 33; 34] to address the challenge of cross-modal weakly misalignment in aerial visible-infrared images. Liu et al. [17] proposing a joint learning method, pioneered the exploration of the MF and OD combination methods. Then the optimization of the loss function for segmentation [16] and detection [14] has been validated to be effective in guiding the generation of fused images. They consider the downstream task network as an additional constraint to assist the MF network in generating fusion results with clearer objects. Zhao et al. [18] leveraged semantic information from OD features to aid MF and perform meta-feature embedding to generate meta-features from OD features, which are then used to guide the MF network in learning pixellevel semantic information. Liu et al. [15] proposed multi-interactive feature learning architecture for image fusion and segmentation by enhancing fine-grained mapping of all the vital information between two tasks, so that the modality or semantic features can be fully mutual-interactive. However, OD considers the semantic understanding of objects, while MF and segmentation primarily focus on the pixel-level relationship between image pairs. The optimization coupling of detection and fusion tasks becomes more challenging to investigate these complementary differences, from which both fusion and detection can benefit. It is worth mentioning that these visible-infrared multimodal fusion detection methods are usually designed in a cascaded structure with tedious training steps. Researchers lack the adoption of end-to-end architectures, which would enable one-step network inference to generate credible fused images and detection results through a set of network parameters.\n\n# 2.2 Multi-task Learning\n\nMulti-task learning (MTL) [35; 36] involves the simultaneous learning of multiple tasks through parameter sharing. Prior approaches involve manually crafting the architecture, wherein the bottom layers of a model are shared across tasks [37; 38]. Some approaches tailor the architecture based on task affinity [39], while others utilize techniques such as Neural Architecture Search [40; 37; 41] or routing networks [42] to autonomously discern sharing patterns and determine the architecture. An alternative method typically combines task-specific objectives into a weighted sum [43; 44; 45]. In addition, most approaches (e.g. [46; 47; 48; 49; 50]) aim to mitigate the effects of conflicting or dominating gradients. The approach of explicit gradient modulation [48; 49; 50; 51], has demonstrated superior performance in resolving conflicts between task gradients by substituting conflicting gradients with modified, non-conflicting gradients. Inspired by the above multimodal learning methods, we introduce a Gradient Matrix Task-Alignment method to align the orthogonal components contained in image fusion and object detection tasks thereby effectively eliminating the inherent optimization barrier that exists between two tasks.\n\n# 3 The Proposed Method\n\n# 3.1 Problem Formulation\n\nMF-OD task concentrates on generating an image that benefits emphasizing objects with superior visual perception capability. The goal of OD is to find the location and identify the class of each object in an image which can naturally provide rich semantic information along with object location information. Therefore, the motivation of OD-aware MF is to construct a novel infrared and visible image fusion framework that can benefit from the semantic information and object location information contained in OD. For this purpose, we suppose a pair of visible image $\\pmb { x } ^ { \\mathsf { ^ { * } } } \\in \\mathbb { R } ^ { H \\times W \\times C _ { x } }$ and infrared image $\\pmb { y } \\in \\mathbb { R } ^ { H \\times W \\times C _ { y } }$ . The optimization model is formulated as:\n\n$$\n\\operatorname* { m i n } _ { \\theta _ { \\mathrm { t } } } \\mathcal { L } \\left( t , \\mathcal { N } \\left( x , y ; \\theta _ { \\mathrm { t } } \\right) \\right) ,\n$$\n\nwhere $\\mathbf { \\Delta } _ { t }$ represents the output of the different task network $\\mathcal { N }$ with the learnable parameters $\\theta _ { \\mathrm { t } } . \\mathcal { L } ( \\cdot )$ is a constraint term to optimize the network. Previous approaches solely design image fusion or object detection networks in a cascaded way, which can only achieve outstanding results for one task. To produce visually appealing fused images alongside accurate object detection results, we jointly integrate the two tasks into a unified goal synchronously which can be rewritten as:\n\n$$\n\\theta _ { \\mathrm { u } } , \\theta _ { \\mathrm { d } } = \\arg \\operatorname* { m i n } \\omega \\mathcal { L } ^ { \\mathrm { u } } \\left( u , \\Phi \\left( x , y ; \\theta _ { \\mathrm { u } } \\right) \\right) + \\left( 1 - \\omega \\right) \\mathcal { L } ^ { \\mathrm { d } } \\left( d , \\Psi \\left( x , y ; \\theta _ { \\mathrm { d } } \\right) \\right) + \\mathcal { S } \\left( \\theta ^ { \\star } \\right) ,\n$$\n\nwhere $\\theta ^ { \\star } = \\theta _ { \\mathrm { u } } ^ { s } = \\theta _ { \\mathrm { d } } ^ { s }$ , defined as the shared parameters for MF and OD networks. $\\mathbf { \\Delta } _ { \\pmb { u } }$ and $\\textbf { \\em d }$ denote the fused image and detection result, which are produced by the MF network $\\Phi ( \\cdot )$ and OD network $\\Psi ( \\cdot )$ with the learnable parameters $\\theta _ { \\mathrm { u } }$ and $\\pmb { \\theta } _ { \\mathrm { d } }$ . $w$ is a predefined weighting factor to balance the task training. $ { \\boldsymbol { S } } ( \\cdot )$ is a constrained term to jointly optimize the two tasks. In this paper, we regard the $ { \\boldsymbol { S } } ( \\cdot )$ as a feature learning constrained manner and achieve this goal by designing a Gradient Matrix Task-Alignment training scheme.\n\nMultimodal Images Backbone θ Node1 Coarse-to-fine Diffusion Process (CFDP) θd GU 1 q zt∣z ()d2d 012t ,t=−θzz R R 11 o o Synchronous Joint Optimization θ θ = arg min u (u,(x   θ )) + (1−) d (d, (x   θd )) + (θ ) 3 3 No(Nxd,oeyd)2e1ObCjoeacrt-seR-etogo1i-foin-ePiDxieflf uPshioylnolgPernoecteiscsTree (OoLRPPT) θu 3 R PFMM RFRM RFRM RFRM 1SSIM2pixel 3grad u=++ (Branch 0) (Branch 1) (Branch lth) (Branch Lth)   \nGradient Matrix Task-Alignment (GMTA) ds Orthogonal and   Equa(lθ )  u  u  d … (θ ) 10    -10 10 Parameter Space Parameter Space Gradient Dominance and Conflict Pixel Object\n\n# 3.2 Architecture\n\nOur proposed E2E-MFD is designed with parallel principle, composited by image fusion and object detection sub-network. Details of the whole architecture are shown in Figure 2. A fusion network (ORPPT) and object detection network (CFDP) can sufficiently realize the granularity-aware detail information and semantic information extraction.\n\nObject-Region-Pixel Phylogenetic Tree. The important fact of that humans pay more attention to different regions from coarse to fine for object detection in object scale and image fusion in pixel scale. Inspired by a phylogenetic tree, to simulate humans to study the interactions of hierarchies under different granularity views, we construct an Object-Region-Pixel Phylogenetic Tree (ORPPT) as $\\Phi ( \\cdot )$ to extract different features in multiple region scales. Given an image pair $\\pmb { x } \\in \\mathbb { R } ^ { W \\times H \\times C _ { x } }$ , $\\pmb { y } \\in \\dot { \\mathbb { R } ^ { W \\times H \\times C _ { y } } }$ , we firstly extract image features $f ( { \\pmb x } )$ and $f ( \\boldsymbol { y } )$ by shared parallel backbone to save memory and computing resources. Then these features are added in channel dimension to obtain the final $L$ multimodal image features $o _ { 1 } , . . . , o _ { l } , . . . , o _ { L } \\in \\mathbb { R } ^ { W _ { 1 } \\times H _ { 1 } \\times C _ { 1 } }$ . The parameters in $f ( \\cdot )$ are shared with the OD network.\n\nAlthough $f ( { \\pmb x } )$ and $f ( \\boldsymbol { y } )$ can describe the characteristics of visible and infrared modalities, they lack insights into the multi-granularity perspective. Therefore, we utilize the branches including the one pixel feature mining module (PFMM) and $L$ region feature refine module (RFRM) to mine the multiple granularities from coarse to fine. The PFMM $B _ { 0 }$ is set the same with feature fusion block in the MetaFusion [18] with the input pair $\\scriptstyle { \\mathbf { } } ( { \\mathbf { } } , { \\mathbf { } } y$ . We set $1 , 2 , \\ldots , l , \\ldots L$ to denote each region branch. For branch $l$ , a CNN $\\varphi _ { l } ( \\cdot )$ is firstly utilized to extract the granularity-wise feature $\\varphi _ { l } \\big ( \\tilde { o _ { l } } \\big ) \\in$ $\\mathbb { R } ^ { W _ { 2 } \\times H _ { 2 } \\times C _ { 2 } }$ at the region level. A set of learnable region prompts $\\pmb { R } _ { l } = \\left\\{ \\pmb { r } _ { l , m } \\in \\mathbb { R } ^ { C _ { 2 } } \\right\\} _ { m = 1 } ^ { M _ { l } }$ are introduced to define the $M _ { l }$ different regions of granularity-wise feature, where ${ r _ { l , m } }$ denotes the $m ^ { \\mathrm { t h } }$ region prompt at branch $l$ . Then the feature vector is mapped into the region mask $\\mathbf { } A _ { l } =$ $\\smash { \\bigl \\{ \\mathbf { { a } } _ { l , m } \\in \\mathbb { R } ^ { W _ { 2 } \\times H _ { 2 } } \\bigr \\} _ { m = 1 } ^ { M _ { l } } }$ by conducting the dot product between the feature vector and region prompt followed by batch normalization and a ReLU activation:\n\n$$\n\\begin{array} { r } { \\pmb { A } _ { l } = \\mathrm { R e L U } ( \\mathrm { B N } ( \\pmb { R } _ { l } \\cdot \\triangledown \\varphi _ { l } ( \\pmb { o } _ { l } ) ) . } \\end{array}\n$$\n\nFinally, the vector of the object-level feature is weighted by the region mask and further aggregated to form region representation:\n\n$$\nb _ { l , m } ^ { i , j } ( { \\pmb o } _ { l } ) = { \\pmb a } _ { l , m } ^ { i , j } { \\varphi } _ { l } ^ { i , j } ( { \\pmb o } _ { l } ) ,\n$$\n\nwhere $b _ { l , m } ( \\pmb { o } _ { l } )$ denotes the $m ^ { \\mathrm { t h } }$ region representation and $( i , j )$ denotes the spatial location. These region-level representations are further concatenated to form the observation $B _ { l } ( o _ { l } ) \\ =$ $[ b _ { l , 1 } ( \\pmb { o } _ { l } ) , \\mathbf { \\bar { \\it b } } _ { l , 2 } ( \\pmb { o } _ { l } ) , \\dots , \\mathbf { \\bar { \\it b } } _ { l , M _ { l } } ( \\pmb { o } _ { l } ) ]$ of branch $l$ . These multi-grained attentions concentrate operation on the spatial location information and the extent of the regions which are similar to the task requirements. The $B _ { 1 } , B _ { 2 } , . . . , B _ { L }$ are up-sampled to keep the consistent spatial size with the pixel-level fusion features $B _ { 0 }$ . Then, the region-level fusion features $B _ { 1 } , B _ { 2 } , . . . , B _ { L }$ are assembled by addition operation followed by a Convolution with $1 \\times 1$ kernel $+$ ReLU to reduce the channel numbers. Highlighted region-level features are extracted by a Convolution with $1 \\times 1$ kernel $+$ Sigmoid and then injected into pixel-level features by multiplication and addition. Finally, five Convolutions with $3 \\times 3$ kernel $^ +$ ReLU layers are constructed to reconstruct the fusion result $u$ .\n\nCoarse-to-Fine Diffusion Process. Diffusion models, inspired by nonequilibrium thermodynamics, are a class of likelihood-based models. DiffusionDet [52] is the first neural network model to utilize the diffusion model for object detection, introducing a novel paradigm that achieves promising results compared to traditional object detection models. Combining diffusion simulation with the diffusion and recovery process of the object box, the Coarse-to-Fine Diffusion Process (CFDP) introduces it as an efficient detection head to assist fusion networks to focus more on object areas. CFDP model defines a Markovian chain of diffusion forward process by gradually adding noise to a set of bounding boxes. The forward noise process is defined as:\n\n$$\nq \\left( z _ { t } \\mid z _ { 0 } \\right) = N \\left( z _ { t } \\mid \\sqrt { \\bar { \\alpha } _ { t } } z _ { 0 } , \\left( 1 - \\bar { \\alpha } _ { t } \\right) { \\cal I } \\right) ,\n$$\n\nwhich transforms bounding boxes $z _ { 0 } ~ \\in ~ \\mathbb { R } ^ { N \\times 4 }$ to a latent noisy bounding boxes $\\scriptstyle { z _ { t } }$ for $t \\in$ $\\{ 0 , 1 , \\ldots , T \\}$ by adding noise to $z _ { \\mathrm { 0 } }$ . $\\begin{array} { r } { \\bar { \\alpha } _ { t } : = \\prod _ { s = 0 } ^ { t } \\alpha _ { s } = \\prod _ { s = 0 } ^ { t } \\left( 1 - \\beta _ { s } \\right) } \\end{array}$ and $\\beta _ { s }$ represents the noise variance schedule. During training stage, a neural network $\\Psi _ { \\theta _ { \\mathrm { d } } } \\left( z _ { t } , t \\right)$ is trained to predict $z _ { 0 }$ from $z _ { t }$ by minimizing the training objective with $\\ell _ { 2 }$ loss:\n\n$$\n\\mathcal { L } _ { \\mathrm { d } } = \\frac { 1 } { 2 } \\left. \\Psi _ { \\pmb { \\theta } _ { \\mathrm { d } } } \\left( \\boldsymbol { z } _ { t } , t \\right) - \\boldsymbol { z } _ { 0 } \\right. ^ { 2 } .\n$$\n\nAt inference stage, bounding boxes $z _ { 0 }$ is reconstructed from noise $z _ { T }$ with the model $\\Psi _ { \\theta _ { \\mathrm { d } } }$ and updating rule in an iterative way, i.e., $z _ { T }  z _ { T - \\Delta }  . . .  z _ { 0 }$ . In this work, we aim to solve the object detection task via the diffusion model. A neural network $\\Psi _ { \\pmb { \\theta } _ { \\mathrm { d } } } \\left( z _ { t } , t , \\pmb { x } , \\pmb { y } \\right)$ is trained to predict $z _ { 0 }$ from noisy boxes $\\scriptstyle { z _ { t } }$ , conditioned on the corresponding image pair $x , y$ .\n\n# 3.3 Loss Function\n\nThe total loss is combined with an image fusion loss function ${ \\mathcal { L } } _ { \\mathrm { f } }$ and object detection loss $\\mathcal { L } _ { \\mathrm { d } }$ . ${ \\mathcal { L } } _ { \\mathrm { f } }$ consists of three types of losses, i.e., structure loss $\\mathcal { L } _ { \\mathrm { S S I M } }$ , pixel loss $\\mathcal { L } _ { \\mathrm { p i x e l } }$ and gradient loss $\\mathcal { L } _ { \\mathrm { g r a d } }$ . For one fused image, it should preserve overall structures and maintain a similar intensity distribution from source images. To this end, the structural similarity index (SSIM) is introduced in function:\n\n$$\n\\mathcal { L } _ { \\mathrm { S S I M } } = \\left( 1 - \\mathrm { S S I M } ( { \\pmb u } , { \\pmb x } ) \\right) / 2 + \\left( 1 - \\mathrm { S S I M } ( { \\pmb u } , { \\pmb y } ) \\right) / 2 ,\n$$\n\nwhere $\\mathcal { L } _ { \\mathrm { S S I M } }$ denotes structure similarity loss. In the fused image, we expect the object regions to have a more significant contrast compared to the background region. Therefore, the object regions need to preserve the maximum pixel intensity and the background region needs to be slightly below the maximum pixel intensity to bring out the contrast between the object and background. The ground-truth bounding boxes of the objects in images are denoted as $( x _ { c } , y _ { c } , w , h )$ for horizontal boxes and $( x _ { c } , y _ { c } , w , h , \\theta )$ for rotated boxes, where $( x _ { c } , y _ { c } )$ is the center location, $w$ and $h$ are the width and height, $\\theta = a n g l e * p i / 1 8 0$ , respectively. Based on these ground-truth bounding boxes, we construct the object mask $I _ { m }$ , and the background mask is denoted as $1 - I _ { m }$ . The object regions pixel loss $\\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { o } }$ and the background region pixel loss $\\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { b } }$ are formulated as:\n\n$$\n\\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { o } } = \\Vert I _ { m } \\circ ( \\boldsymbol { u } - m a x ( \\boldsymbol { x } , \\boldsymbol { y } ) ) \\Vert _ { 1 } , \\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { b } } = \\Vert ( 1 - I _ { m } ) \\circ ( \\boldsymbol { u } - m e a n ( \\boldsymbol { x } , \\boldsymbol { y } ) ) \\Vert _ { 1 } ,\n$$\n\nwhere $\\| \\cdot \\| _ { 1 }$ stands for the $l _ { 1 }$ -norm. The operator $\\circ$ denotes the elementwise multiplication, max( ) denotes the element-wise maximization, and mean( ) denotes the element-wise average operation. Therefore, the object-aware pixel loss $\\mathcal { L } _ { \\mathrm { p i x e l } }$ is defined as:\n\n$$\n\\mathcal { L } _ { \\mathrm { p i x e l } } = \\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { o } } + \\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { b } } .\n$$\n\nBesides, gradient information of images always characterizes texture details, thus, we use $\\mathcal { L } _ { \\mathrm { g r a d } }$ to constrain these textual factors to a multi-scale manner:\n\n$$\n\\mathcal { L } _ { \\mathrm { g r a d } } = \\sum _ { k = 3 , 5 , 7 } \\left. \\nabla ^ { k } \\pmb { u } - \\operatorname* { m a x } \\left( \\nabla ^ { k } \\pmb { x } , \\nabla ^ { k } \\pmb { y } \\right) \\right. _ { 2 } ^ { 2 } ,\n$$\n\nwhere $\\nabla$ denotes gradient operators that calculate by $\\nabla = \\pmb { u } - \\mathcal { G } ( \\pmb { u } )$ with combination of different Gauss $( { \\mathcal { G } } )$ kernel size $k$ . Totally, we obtain $\\mathcal { L } _ { \\mathrm { u } } = \\eta _ { 1 } \\mathcal { L } _ { \\mathrm { S S I M } } + \\eta _ { 2 } \\mathcal { L } _ { \\mathrm { p i x e l } } + \\eta _ { 3 } \\mathcal { L } _ { \\mathrm { g r a d } }$ .\n\n![](images/7c029ea1e784607f462c9aeb5333ed8d7f13170796f56f1e30faf8680138e335.jpg)  \nFigure 3: Visual results of image fusion on M3FD.\n\n# 3.4 Gradient Matrix Task-Alignment\n\nThe MF and OD tasks have distinct optimization objectives. MF primarily emphasizes capturing the pixel-level relationship between image pairs, while OD incorporates object semantics within the broader context of diverse scenes. An inherent optimization barrier exists between these two tasks. We observe that the prevailing challenges in multi-task learning are arguably task dominance and conflicting gradients. We introduce a Gradient Matrix Task-Alignment (GMTA) by presenting the condition number to mitigate the undesired effects of the optimization barrier in task-shared parameters $\\pmb { \\theta } ^ { \\star }$ which are supposed to be balanced between the MF and OD tasks. The individual task gradients of MF and OD task are calculated by $\\pmb { g } _ { u } = \\nabla _ { \\theta ^ { \\star } } \\mathcal { L } _ { u }$ and $\\pmb { g } _ { d } = \\nabla _ { \\theta ^ { \\star } } \\mathcal { L } _ { d }$ in the training optimization process. The gradient matrix can be defined as $G = \\{ g _ { u } , g _ { d } \\}$ . In multi-task optimization, a cumulative gradient $\\mathbf { \\nabla } _ { g } = G w$ is a linear combination of task gradients and the stability of a linear system is measured by the condition number of its matrix in numerical analysis. Hence the stability of the gradient matrix is equal to the ratio of the maximum and minimum singular values (non-negative) of the corresponding matrix: $\\begin{array} { r } { \\kappa ( G ) = \\frac { \\sigma _ { \\mathrm { m a x } } } { \\sigma _ { \\mathrm { m i n } } } } \\end{array}$ . Learning from the Aligned-MTL [51], the condition number is optimal $( \\kappa ( G ) = 1 \\$ ) if and only if the gradients are orthogonal and equal in magnitude which means that the system of gradients has no dominance or conflicts:\n\n$$\n\\kappa ( G ) = 1 \\iff < g _ { u } , g _ { d } > = 1 .\n$$\n\nThe final linear system of gradients defined by $\\hat { \\pmb { G } }$ satisfies the optimal condition in terms of a condition number. Thereby, we consider the feature learning constraint $ { \\mathcal { S } } ( \\theta ^ { \\star } )$ can be defined as the following optimization to eliminate instability in the training process:\n\n$$\n\\operatorname* { m i n } _ { \\hat { \\pmb { G } } } \\| \\pmb { G } - \\hat { \\pmb { G } } \\| _ { F } ^ { 2 } \\quad \\mathrm { s . t . } \\kappa ( \\hat { \\pmb { G } } ) = 1 \\ \\Longleftrightarrow \\ \\operatorname* { m i n } _ { \\hat { \\pmb { G } } } \\| \\pmb { G } - \\hat { \\pmb { G } } \\| _ { F } ^ { 2 } \\quad \\mathrm { s . t . } \\hat { \\pmb { G } } ^ { \\top } \\hat { \\pmb { G } } = \\pmb { I } .\n$$\n\nThe problem can be treated as a Procrustes problem and can be solved by performing a singular value decomposition (SVD) to $G$ $\\mathbf { \\Psi } _ { G } = \\pmb { U } \\pmb { \\Sigma V } ^ { T }$ ) and rescaling singular values corresponding to principal components so that they are equal to the smallest singular value:\n\n$$\n\\begin{array} { r } { \\hat { \\pmb { G } } = \\sigma \\pmb { U } \\pmb { V } ^ { \\top } = \\sigma \\pmb { G } \\pmb { V } \\pmb { \\Sigma } ^ { - 1 } \\pmb { V } ^ { T } , } \\end{array}\n$$\n\nwhere,\n\n$$\n( V , \\lambda ) = e i g h ( G ^ { \\top } G ) ,\n$$\n\n$$\n\\Sigma ^ { - 1 } = d i a g ( \\sqrt { { 1 } / { { \\lambda _ { m a x } } } } , \\sqrt { { 1 } / { { \\lambda _ { m i n } } } } ) ,\n$$\n\neigh represents a function for finding eigenvectors $V$ and eigenvalues $\\lambda$ and diag stands for diagonal matrix. $\\lambda _ { m a x }$ and $\\lambda _ { m i n }$ are maximum eigenvalues and minimum eigenvalues from $\\lambda$ .The stability criterion, a condition number, defines a linear system to an arbitrary position scale. To alleviate this ambiguity, we choose the largest scale that guarantees convergence to the optimum: this is a minimal singular value of an initial gradient matrix:\n\n$$\n\\sigma = \\sigma _ { \\mathrm { m i n } } ( G ) = \\sqrt { \\lambda _ { m i n } } .\n$$\n\nTable 1: Quantitative results of different fusion methods on TNO, RoadScene, and M3FD datasets. The model training (Tr.) and test (Te.) time is counted on an NVIDIA GeForce RTX 3090. The best result is highlighted.   \n\n<html><body><table><tr><td rowspan=\"2\">Task</td><td rowspan=\"2\">Method</td><td colspan=\"3\">M3FD</td><td colspan=\"3\"></td><td colspan=\"3\">EN RoadScenevIF</td><td>Tme</td><td>Tme</td></tr><tr><td>EN</td><td></td><td>VIF</td><td>EN</td><td>TMO</td><td>VIF</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"5\">MF</td><td>DIDFuse[13]</td><td>6.13</td><td>14.65</td><td>1.51</td><td>6.30</td><td>15.30</td><td>1.47</td><td>6.67</td><td>16.65</td><td>1.55</td><td>3h9m38s</td><td>0.096s</td></tr><tr><td>U2Fusion[26]</td><td>5.66</td><td>14.22</td><td>1.50</td><td>5.78</td><td>14.89</td><td>1.49</td><td>6.25</td><td>16.30</td><td>1.57</td><td>4h8m36s</td><td>2.091s</td></tr><tr><td>PIAFusion[58]</td><td>5.75</td><td>13.92</td><td>1.59</td><td>5.05</td><td>13.61</td><td>1.36</td><td>6.37</td><td>16.22</td><td>1.58</td><td>5h35m20s</td><td>0.003s</td></tr><tr><td>SwinFusion[59]</td><td>5.80</td><td>13.83</td><td>1.58</td><td>6.09</td><td>14.28</td><td>1.55</td><td>6.30</td><td>15.93</td><td>1.60</td><td>3h38m5s</td><td>0.044s</td></tr><tr><td>CDDFuse[12]</td><td>5.77</td><td>13.82</td><td>1.58</td><td>6.21</td><td>15.03</td><td>1.49</td><td>6.54</td><td>16.54</td><td>1.57</td><td>5h59m59s</td><td>0.096s</td></tr><tr><td rowspan=\"3\">MF-OD</td><td>Tardal[17]</td><td>5.72</td><td>14.68</td><td>1.47</td><td>5.87</td><td>14.99</td><td>1.43</td><td>6.72</td><td>16.98</td><td>1.54</td><td>5h36m28s</td><td>0.093s</td></tr><tr><td>Metafusion[18]</td><td>6.20</td><td>15.19</td><td>1.54</td><td>6.29</td><td>16.03</td><td>1.44</td><td>6.35</td><td>16.76</td><td>1.57</td><td>6h47m38s</td><td>0.002s</td></tr><tr><td>E2E-MFD</td><td>6.36</td><td>15.47</td><td>1.65</td><td>6.40</td><td>16.28</td><td>1.60</td><td>6.79</td><td>17.11</td><td>1.69</td><td>2h50m32s</td><td>0.014s</td></tr></table></body></html>\n\n# 4 Experiments and Analysis\n\n# 4.1 Dataset and Implementation Details\n\nWe conduct experiments on four widely-used visible-infrared image datasets: TNO [53], RoadScene [26], M3FD [17] and DroneVehicle [3]. TNO and RoadScene are just used to evaluate MF performance. M3FD is adopted to evaluate both MF and OD performance. RoadScene with 37 image pairs, TNO with 42 image pairs and M3FD with 300 pairs are only used for the MF task in the testing stage, and the MF network is trained by the M3FD dataset which is divided into a training set (2,940 image pairs) and a testing set (1,260 image pairs). Besides, DroneVehicle consists of 28,439 image pairs is utilized to train and test MF and OD for oriented objects. We conduct all the experiments with one GeForce RTX 3090 GPU, and the code of M3FD is based on Detectron2 [54], while the code of DroneVehicle is based on MMDetection 2.26.0 [55] and MMRotate 0.3.4 [56]. On the M3FD dataset, the pretrained DiffusionDet is used for the initialization of the OD network. In the training phase, E2E-MFD is optimized by AdamW with a batch size of 1. We set the learning rate to $2 . 5 e \\mathrm { ~ - ~ } 5$ and the weight decay as $1 e - 4$ . The default training iteration is only 15,000. On the DroneVehicle dataset, the pretrained LSKNet [57] is used for the initialization of the object detection network, and we fine-tune it for 12 epochs with a batch size of 4. The E2E-MFD is optimized by AdamW and the learning rate and the weight decay is set to $1 e - 4$ and 0.05.\n\n# 4.2 Main Results\n\nResults on Multimodal Image Fusion. Qualitative results of different fusion methods are depicted in Figure 3. All the fusion methods can fuse the main features of the infrared and visible images to some extent and we can observe two remarkable advantages of our method. First, the significant characteristics of infrared images can be effectively highlighted by our method. Our M3FD fusion image captures the person riding a motorcycle. In comparison with other methods, our method demonstrates high contrast and recognition of the objects. Second, our method preserves rich details from the visible images, including color and texture. Our advantages are evident in the fusion images across the M3FD dataset, such as the clear outline of the white car’s rear and a man on a motorcycle. While retaining a substantial amount of detail, our method maintains a high resolution without introducing blurriness. In contrast, other methods fail to achieve these two advantages simultaneously. Sequentially, we provide quantitative results of different fusion methods in Table 1. Our E2E-MFD generally achieves the best metric values. Specifically, the largest average value on MI proves that our method transfers more considerable information from both source images. Values of EN reveal that our results contain edged details and the highest contrast between objects and the background. The large VIF shows our fusion results have high-quality visual effects and small distortion compared with the source images. Moreover, our method achieves the fastest training time to finish the joint learning at one stage which means that faster iterative updates can be done on new datasets. The test time to generate a fused image ranked third.\n\nResults on Multimodal Object Detection. To more effectively evaluate the fusion images and observe their impact on downstream detection tasks, we conduct tests using the baseline detector YOLOv5s on all SOTA methods on the M3FD dataset. We follow the same parameter settings, and the visualization results are shown in Figure 4. The detection results are poor when using only single-modal image inputs, with instances of missed detection, such as the motorcycle and rider next to the car and people on the far right in the image. Almost all fusion methods reduce missed detection and improve confidence by fusing information from both modalities. Through the design of an end-to-end fusion detection synchronous optimization strategy, we obtain fusion images that are visually and detection-friendly, especially for occluded and overlapping objects, as seen in the blue ellipse with the motorcycle and the overlapping people on the far right in the image. To further assess the quality of fusion images, we conduct a fair comparison between our method and the SOTA methods on YOLOv5s. As shown in Table 2, the MF methods demonstrate a performance improvement compared to single-modal detection, indicating that well-fused images can effectively assist downstream tasks. In contrast, the fusion image we generate has achieved the best performance on YOLOv5s. Additionally, the detection performance of fusion images on DiffusionDet is also impressive, albeit slightly lower than when optimizing fusion and detection tasks simultaneously with E2E-MFD. Thanks to the collaborative optimization of both tasks, the detection performance is further enhanced. Furthermore, even when compared to end-to-end object detection methods (E2E-OD), our approach demonstrates significant performance improvements. This better underscores the advantages of our training paradigm and the effectiveness of our method.\n\nTable 2: Quantitative results of object detection on M3FD dataset among all the image fusion methods $^ +$ detector (i.e. YOLOv5s [60]). ⋆ means using the fusion images generated by E2E-MFD for object detection training. The best result is highlighted.   \n\n<html><body><table><tr><td>Task</td><td>Method</td><td>People</td><td>Car</td><td>Bus</td><td>Motorcycle</td><td>Lamp</td><td>Truck</td><td>mAP50</td><td>mAP50:95</td></tr><tr><td rowspan=\"2\">V/I</td><td></td><td></td><td>674</td><td>72.9</td><td>34</td><td>48</td><td>62</td><td>85.3</td><td></td></tr><tr><td>visibed</td><td>43</td><td></td><td></td><td></td><td></td><td></td><td></td><td>55.9</td></tr><tr><td rowspan=\"4\">MF</td><td>DIDFusion[13]</td><td>45.8</td><td>68.8</td><td>73.6</td><td>42.2</td><td>43.7</td><td>61.5</td><td>86.2</td><td>56.2</td></tr><tr><td>U2Fusion[26]</td><td>47.7</td><td>70.1</td><td>73.2</td><td>43.2</td><td>44.6</td><td>63.9</td><td>87.1</td><td>57.1</td></tr><tr><td>PIAFusion[58]</td><td>46.5</td><td>69.6</td><td>75.1</td><td>45.4</td><td>44.8</td><td>61.7</td><td>87.3</td><td>57.2</td></tr><tr><td>SwinFusion[59]</td><td>44.5</td><td>68.5</td><td>73.3</td><td>42.2</td><td>44.4</td><td>63.5</td><td>85.8</td><td>56.1</td></tr><tr><td rowspan=\"2\">E2E-OD</td><td>CDDFuse[12]</td><td>46.1</td><td>69.7</td><td>74.2</td><td>42.2</td><td>44.2</td><td>62.7</td><td>87.0</td><td>56.5</td></tr><tr><td>ICAFusion[62]</td><td>528</td><td>68</td><td>793</td><td></td><td>45</td><td>66</td><td>874</td><td>59.2</td></tr><tr><td rowspan=\"4\">MF-OD</td><td></td><td></td><td></td><td></td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Tardal[17]</td><td>49.8</td><td>65.4</td><td>69.5</td><td>46.6</td><td>43.7</td><td>61.1</td><td>86.0</td><td>56.0</td></tr><tr><td>MetaFusion[18]</td><td>48.4</td><td>66.7</td><td>70.5</td><td>49.1</td><td>46.4</td><td>59.9</td><td>86.7</td><td>56.8</td></tr><tr><td>Ours (YOLOv5s)*</td><td>51.0</td><td>67.9</td><td>69.4</td><td>50.2</td><td>48.7</td><td>61.6</td><td>87.9</td><td>58.1</td></tr><tr><td></td><td>Ours (DifusionDet)* Ours (E2E-MFD)</td><td>58.5 60.1</td><td>67.7 69.5</td><td>79.9 81.4</td><td>50.3 52.2</td><td>46.2 47.6</td><td>70.2 72.2</td><td>90.3 91.8</td><td>62.1 63.8</td></tr></table></body></html>\n\n![](images/beb220b9b114da2895d2ab9733813ac1970a07a1b1280f7d7f544587f0ec1b54.jpg)  \nFigure 4: Visual results of object detection on M3FD.\n\nTable 3: Quantitative results of object detection on DroneVehicle test sets among all the SOTA methods, where ⋆ means using the fusion images generated by E2E-MFD for object detection training and testing. The best result is highlighted.   \n\n<html><body><table><tr><td>Modality</td><td>Detectors</td><td>Car</td><td>Truck</td><td>Freight Car</td><td>Bus</td><td>Van</td><td>mAP50</td></tr><tr><td rowspan=\"4\">RGB</td><td>RetinaNet-OBB[63]</td><td>67.5</td><td>28.2</td><td>13.7</td><td>62.1</td><td>19.3</td><td>38.1</td></tr><tr><td>Faster R-CNN-OBB [64]</td><td>67.9</td><td>38.6</td><td>26.3</td><td>67.0</td><td>23.2</td><td>44.6</td></tr><tr><td>Gliding Vertex [65]</td><td>75.8</td><td>46.1</td><td>33.8</td><td>68.1</td><td>38.7</td><td>52.5</td></tr><tr><td>YOLOv5s-OBB[60] LSKNet-OBB[57]</td><td>89.0 89.5</td><td>53.6</td><td>41.9</td><td>84.8</td><td>32.6</td><td>60.4 71.5</td></tr><tr><td rowspan=\"4\">IR</td><td>RetinaNet-OBB[63]</td><td>79.9</td><td>70.0 32.8</td><td>51.8 28.1</td><td>89.4 67.3</td><td>56.9 16.4</td><td>44.9</td></tr><tr><td>FasterR-CNN-OBB[64]</td><td>88.6</td><td>42.5</td><td>35.2</td><td>77.9</td><td>28.5</td><td>54.6</td></tr><tr><td>Gliding Vertex [65]</td><td>89.2</td><td>59.7</td><td>43.0</td><td>78.8</td><td>43.9</td><td>62.9</td></tr><tr><td>YOLOv5s-OBB[60]</td><td>95.6</td><td>57.2</td><td>47.5</td><td>89.4</td><td>35.2</td><td>65.0</td></tr><tr><td rowspan=\"6\">RGB+IR</td><td>LSKNet-OBB[57] UA-CMDet[3]</td><td>90.3 87.5</td><td>73.3</td><td>57.8</td><td>89.2</td><td>53.2</td><td>72.8</td></tr><tr><td>TSFADet[30]</td><td>89.2</td><td>60.7 72.0</td><td>46.8 54.2</td><td>87.1 88.1</td><td>38.0 48.8</td><td>64.0</td></tr><tr><td>CALNet [66]</td><td>90.3</td><td>76.2</td><td>63.0</td><td>89.1</td><td>58.5</td><td>70.4</td></tr><tr><td>Ours (YOLOv5s-OBB)*</td><td>96.7</td><td>69.9</td><td>49.9</td><td>92.6</td><td>44.5</td><td>75.4</td></tr><tr><td>Ours (LSKNet-OBB)*</td><td>90.3</td><td>77.0</td><td>63.5</td><td>89.5</td><td>59.0</td><td>70.7 75.9</td></tr><tr><td>Ours (E2E-MFD)</td><td>90.3</td><td>79.3</td><td>64.6</td><td>89.8</td><td>63.1</td><td>77.4</td></tr></table></body></html>\n\nResults on Multimodal Oriented Object Detection. As shown in Table 3, our fusion detection synchronous optimization strategy achieves the highest accuracy. Furthermore, the outstanding detection performance on YOLOv5s-OBB [60] and LSKNet using the generated fusion images (with at least $5 . 7 \\%$ and $3 . 1 \\%$ higher AP values compared to single modalities) demonstrate the robustness of our method. This validates the superior quality of the fusion images, indicating that they are not only visually appealing but also provide rich information for the detection task.\n\n# 4.3 Ablation Studies\n\nAnalysis of Gradient Matrix. As described in Section 3.4, the MF and OD tasks pursue different optimization goals. To visualize the task dominance and conflicting gradients, we plot the gradient matrix in the training stage illustrated by Figure 5. We perform a GMTA operation every 1,000 iteration loss updates. Blue represents the gradients of shared parameters computed by the OD loss function, while yellow represents the gradients of shared parameters computed by the MF loss function. During the training process, it can be observed that the gradient values of the OD task are larger and dominant, while that of the MF task are smaller. This may affect the learning process of the fusion task during training. Conversely, the utilization of GMTA effectively mitigates this gradient dominance and conflict, facilitating a balance of shared parameters between MF and OD.\n\nEffect of Gradient Matrix TaskAlignment. To verify the effectiveness of GMTA, we compare separate optimizations for MF and OD, as well as joint optimization, w/o and w indicating whether to use GMTA. Specifically, MF represents using only $\\mathcal { L } _ { u }$ to optimize\n\nTable 4: The validation of GMTA on M3FD.   \n\n<html><body><table><tr><td>Task</td><td>EN</td><td>MI</td><td>VIF</td><td>mAP50</td><td>mAP50:95</td></tr><tr><td>MF</td><td>6.09</td><td>14.90</td><td>1.48</td><td>/</td><td>1</td></tr><tr><td>OD</td><td>/</td><td>/</td><td>/</td><td>90.28</td><td>62.75</td></tr><tr><td>E2E-MFD(w/o GMTA)</td><td>6.12</td><td>14.70</td><td>1.39</td><td>90.05</td><td>62.60</td></tr><tr><td>E2E-MFD (w GMTA)</td><td>6.36</td><td>15.47</td><td>1.65</td><td>91.80</td><td>63.83</td></tr></table></body></html>\n\nthe fusion network, OD represents using only $\\mathcal { L } _ { d }$ to optimize the object detection network, and E2E-MFD represents simultaneous optimization of the fusion and detection networks using both $\\mathcal { L } _ { u }$ and $\\mathcal { L } _ { d }$ loss functions. The results, as shown in Table 4, indicate that methods incorporating GMTA optimization constraints with shared weights achieve the best results for both MF and OD. This is because MF primarily emphasizes capturing the pixel-level relationship between image pairs, while OD incorporates object semantics within the broader context of diverse scenes. Therefore, optimizing the entire network with shared loss functions may be influenced by local optimal solutions of individual tasks. The accuracy of E2E-MFD (w/o GMTA) shows a slight decrease compared to separately training the detection network. In contrast, GMTA orthogonalizes the gradients of shared parameters corresponding to the two tasks, allowing the joint network to converge to an optimal point with fusion detection weights.\n\nTo compare the effectiveness of different Multi-task learning methods on our algorithm, we selected three robust MTL optimization methods. As shown in Table 5, all MTL methods addressed the conflict between the MF and OD tasks to varying degrees. By introducing the concept of GMTA, we achieved a better balance in\n\nTable 5: Ablation of different MTL methods on M3FD.   \n\n<html><body><table><tr><td>Method</td><td>EN</td><td>MI</td><td>VIF</td><td>mAP50</td><td>mAP50:95</td></tr><tr><td>E2E-MFD (w/o GMTA)</td><td>6.12</td><td>14.70</td><td>1.39</td><td>90.05</td><td>62.60</td></tr><tr><td>PCGrad[50]</td><td>6.13</td><td>15.01</td><td>1.48</td><td>90.59</td><td>62.71</td></tr><tr><td>CAGrad[47]</td><td>6.17</td><td>15.05</td><td>1.48</td><td>90.71</td><td>62.74</td></tr><tr><td>Nash-MTL[49]</td><td>6.29</td><td>15.28</td><td>1.51</td><td>90.91</td><td>62.97</td></tr><tr><td>E2E-MFD(w GMTA)</td><td>6.36</td><td>15.47</td><td>1.65</td><td>91.80</td><td>63.83</td></tr></table></body></html>\n\nthe gradient optimization process between the two tasks, resulting in the best performance.\n\nThe GMTA process operates during the computation and updating stages of two gradients. The GMTA is performed approximately every $n$ iteration (gradient update), focusing on balancing the independence and coherence of various tasks. Table 6 presents the ablation analysis of the $n$ parameter. Decreasing $n$ initially disrupts task optimization due to frequent alignment, while increasing $n$ becomes crucial when\n\nTable 6: Ablation studies of the iteration parameter $n$ on M3FD dataset.   \n\n<html><body><table><tr><td>n</td><td>EN</td><td>MI</td><td>VIF</td><td>mAP50</td><td>mAP50:95</td></tr><tr><td>500</td><td>5.93</td><td>14.78</td><td>1.58</td><td>90.93</td><td>62.73</td></tr><tr><td>1000</td><td>6.36</td><td>15.47</td><td>1.65</td><td>91.80</td><td>63.83</td></tr><tr><td>1500</td><td>6.24</td><td>15.08</td><td>1.62</td><td>91.10</td><td>62.96</td></tr><tr><td>2000</td><td>6.13</td><td>14.69</td><td>1.45</td><td>90.35</td><td>62.75</td></tr></table></body></html>\n\n![](images/4a0c48769ce805d6a0b529a8ee95fb384187dc5b701b11532b644046d2c1aca2.jpg)  \nFigure 5: Visualization of task dominance and conflicting gradients in joint learning of OD and MF.\n\nthe network determines task optimization directions.   \nHowever, excessively large $n$ leads to significant deviations in task paths, making alignment more challenging and negatively impacting performance.\n\nStudy of Branches in the Object-Region-Pixel Phylogenetic Tree. We investigate the combination of the pixel feature mining module (0) and the region feature refinement module (1,2,3,4). The results are shown in Table 7. It can be observed that with the increase in the number of branches, the fusion network achieves higher image fusion quality and object detection performance. Region features provide the fusion network with multi-level semantic features of objects. However, when higher-level semantic object information is added, the performance of the fusion network declines. This is because the detailed information contained in the deeper layers of the backbone structure shared with the detection network decreases abruptly, which may affect the fusion network’s absorption of these features, thereby influencing pixel-level fusion tasks.\n\nTable 7: Ablation study of the number of ORPPT branches on M3FD.   \n\n<html><body><table><tr><td>Branch</td><td>EN</td><td>MI</td><td>VIF</td><td>mAP50</td><td>mAP50:95</td></tr><tr><td>0</td><td>6.10</td><td>15.19</td><td>1.54</td><td>91.51</td><td>63.20</td></tr><tr><td>0,1</td><td>6.10</td><td>15.29</td><td>1.54</td><td>91.65</td><td>62.96</td></tr><tr><td>0,1,2</td><td>6.19</td><td>15.28</td><td>1.58</td><td>91.73</td><td>63.46</td></tr><tr><td>0,1,2.3</td><td>6.36</td><td>15.47</td><td>1.65</td><td>91.80</td><td>63.83</td></tr><tr><td>0,1,2,3,4</td><td>5.99</td><td>14.31</td><td>1.40</td><td>91.73</td><td>63.55</td></tr></table></body></html>\n\nWe have implemented the Object-Region-Pixel Phylogenetic Tree (ORPPT) to explore the hierarchical interactions under different granularity views and extract various features across multiple region scales, as introduced in Section 3.2. As shown in Figure 6, the detailed information will decrease as the backbone structure shared by the detection network deepens, resulting in a decrease in detailed information. This may affect the absorption of these features by the fusion network of the shared backbone network, thereby affecting pixel-level fusion tasks. This provides evidence for our analysis of the reasons for Section 4.3 ablation experiment for Object-Region-Pixel Phylogenetic Tree. This illustrates the importance of balancing the semantic information provided between the OD and MF network with pixel-level information.\n\n![](images/89ab258a042d6a8b4d2781b3ce5dd9a1ea30eda0f383fd5f7b2c07e47a27af2a.jpg)  \nFigure 6: Feature map visualization of various branches in the ORPPT.\n\n# 5 Conclusion\n\nWithin this paper, an end-to-end optimization is proposed to formulate fusion and detection in a harmonious one-stage training process. We introduce a object-region-pixel phylogenetic tree structure and coarse-to-fine diffusion process to simulate these two tasks in different visual perceptions needed for diverse task requirements. In addition, we align the orthogonal components of the fusion detection linear system of the gradients by gradient matrix task-alignment. By unrolling the model to a welldesigned fusion network and diffusion-based detection network, we can generate a visual-friendly result for fusion and object detection in an efficient way without tedious training steps and inherent optimization barriers.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n当前多模态图像融合和目标检测方法的训练过程复杂，多采用多步渐进式联合优化方法，效率低下，且过于关注利用目标检测信息增强融合，导致参数难以平衡，易陷入局部最优，阻碍了其广泛应用。精确可靠的目标解析在自动驾驶和遥感监测等领域至关重要，仅依靠可见传感器在恶劣环境中进行目标识别会产生不准确的结果，因此需要一种高效的多模态融合检测方法。\\n\\n**方法概述**\\n本文提出了 E2E - MFD 端到端多模态融合检测算法，通过同步联合优化，将图像融合和目标检测从粗到细无缝融合，采用 Object - Region - Pixel Phylogenetic Tree (ORPPT) 和 Coarse - to - Fine Diffusion Process (CFDP) 机制，并引入 Gradient Matrix Task - Alignment (GMTA) 技术优化共享组件。\\n\\n**主要贡献与效果**\\n- 提出 E2E - MFD 方法，将图像融合和目标检测集成到单阶段端到端框架，显著提升了两项任务的效果。在水平目标检测数据集 M3FD 和定向目标检测数据集 DroneVehicle 上，$mAP_{50}$ 分别比最先进方法提高了 3.9% 和 2.0%。\\n- 引入 GMTA 技术，评估和量化图像融合和目标检测任务的影响，确保训练过程稳定收敛到最优配置。在 M3FD 数据集上，使用 GMTA 后，E2E - MFD 的 $mAP_{50}$ 达到 91.80%，$mAP_{50:95}$ 达到 63.83%，优于未使用 GMTA 的 90.05% 和 62.60%。\\n- 通过在多个公开数据集上的综合实验，证明了该方法的有效性和鲁棒性。在多模态图像融合任务中，E2E - MFD 在熵 (EN)、互信息 (MI) 和视觉信息保真度 (VIF) 等指标上总体达到最佳值，且训练时间最快。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nE2E - MFD 的核心思想是通过同步联合优化，让多模态图像融合和目标检测两个任务的内在特征相互作用，从粗到细地实现无缝融合。ORPPT 模拟人类从不同粒度视角研究层次间的交互，提取多区域尺度的不同特征；CFDP 将扩散模型引入目标检测，作为高效检测头辅助融合网络更关注目标区域；GMTA 则通过调整梯度矩阵，平衡两个任务的共享参数，消除优化障碍。\\n\\n**创新点**\\n先前的方法多采用多步、渐进式联合优化，效率低，且难以平衡参数，易陷入局部最优。E2E - MFD 的创新点在于：\\n- 采用同步联合优化，实现单阶段训练过程，提高效率。\\n- 提出 ORPPT 结构和 CFDP 机制，协调细粒度细节和语义信息。\\n- 引入 GMTA 技术，解决多任务学习中梯度主导和冲突的问题。\\n\\n**具体实现步骤**\\n1. **问题建模**：将多模态融合检测任务的优化目标建模为 $\\theta _ { \\mathrm { u } } , \\theta _ { \\mathrm { d } } = \\arg \\min \\omega \\mathcal { L } ^ { \\mathrm { u } } \\left( u , \\Phi \\left( x , y ; \\theta _ { \\mathrm { u } } \\right) \\right) + \\left( 1 - \\omega \\right) \\mathcal { L } ^ { \\mathrm { d } } \\left( d , \\Psi \\left( x , y ; \\theta _ { \\mathrm { d } } \\right) \\right) + \\mathcal { S } \\left( \\theta ^ { \\star } \\right)$，其中 $\\theta ^ { \\star }$ 为共享参数，$\\mathbf { \\Delta } _ { \\pmb { u } }$ 和 $\\textbf { \\em d }$ 分别表示融合图像和检测结果，由 MF 网络 $\\Phi ( \\cdot )$ 和 OD 网络 $\\Psi ( \\cdot )$ 生成，$w$ 是预定义的加权因子，$ { \\boldsymbol { S } } ( \\cdot )$ 是约束项。\\n2. **架构设计**：\\n    - **Object - Region - Pixel Phylogenetic Tree (ORPPT)**：通过共享并行骨干网络提取图像特征，利用分支模块挖掘多粒度特征，经过区域掩码加权和聚合，最终重建融合结果。具体而言，利用分支包括的一个像素特征挖掘模块（PFMM）和 $L$ 个区域特征细化模块（RFRM）从粗到细挖掘多粒度特征。PFMM $B _ { 0 }$ 与 MetaFusion 中的特征融合块设置相同，对于每个区域分支，先利用 CNN 提取区域级的粒度特征，引入可学习的区域提示定义不同区域，将特征向量映射到区域掩码，最后将对象级特征向量加权形成区域表示，经过上采样、加法运算、卷积等操作重构融合结果。\\n    - **Coarse - to - Fine Diffusion Process (CFDP)**：定义马尔可夫链的扩散前向过程，通过逐渐向边界框添加噪声，训练神经网络从噪声边界框预测原始边界框。具体定义为 $q \\left( z _ { t } \\mid z _ { 0 } \\right) = N \\left( z _ { t } \\mid \\sqrt { \\bar { \\alpha } _ { t } } z _ { 0 } , \\left( 1 - \\bar { \\alpha } _ { t } \\right) { \\cal I } \\right)$，在训练阶段通过最小化 $\\ell _ { 2 }$ 损失训练神经网络预测原始边界框，在推理阶段根据模型和更新规则从噪声边界框重建原始边界框。\\n3. **损失函数设计**：总损失由图像融合损失 $\\mathcal { L } _ { \\mathrm { f } }$ 和目标检测损失 $\\mathcal { L } _ { \\mathrm { d } }$ 组成，其中 $\\mathcal { L } _ { \\mathrm { f } }$ 包括结构损失 $\\mathcal { L } _ { \\mathrm { S S I M } }$、像素损失 $\\mathcal { L } _ { \\mathrm { p i x e l } }$ 和梯度损失 $\\mathcal { L } _ { \\mathrm { g r a d } }$。结构损失 $\\mathcal { L } _ { \\mathrm { S S I M } } = \\left( 1 - \\mathrm { S S I M } ( { \\pmb u } , { \\pmb x } ) \\right) / 2 + \\left( 1 - \\mathrm { S S I M } ( { \\pmb u } , { \\pmb y } ) \\right) / 2$；像素损失根据对象区域和背景区域分别计算，$\\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { o } } = \\Vert I _ { m } \\circ ( \\boldsymbol { u } - m a x ( \\boldsymbol { x } , \\boldsymbol { y } ) ) \\Vert _ { 1 }$，$\\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { b } } = \\Vert ( 1 - I _ { m } ) \\circ ( \\boldsymbol { u } - m e a n ( \\boldsymbol { x } , \\boldsymbol { y } ) ) \\Vert _ { 1 }$，$\\mathcal { L } _ { \\mathrm { p i x e l } } = \\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { o } } + \\mathcal { L } _ { \\mathrm { p i x e l } } ^ { \\mathrm { b } }$；梯度损失 $\\mathcal { L } _ { \\mathrm { g r a d } } = \\sum _ { k = 3 , 5 , 7 } \\left. \\nabla ^ { k } \\pmb { u } - \\operatorname* { m a x } \\left( \\nabla ^ { k } \\pmb { x } , \\nabla ^ { k } \\pmb { y } \\right) \\right. _ { 2 } ^ { 2 }$，最终 $\\mathcal { L } _ { \\mathrm { u } } = \\eta _ { 1 } \\mathcal { L } _ { \\mathrm { S S I M } } + \\eta _ { 2 } \\mathcal { L } _ { \\mathrm { p i x e l } } + \\eta _ { 3 } \\mathcal { L } _ { \\mathrm { g r a d } }$。\\n4. **梯度矩阵任务对齐 (GMTA)**：计算任务梯度，定义梯度矩阵 $G$，通过奇异值分解 (SVD) 解决 Procrustes 问题，调整梯度矩阵使其满足最优条件，消除训练过程中的不稳定性。具体为计算 MF 和 OD 任务的梯度 $\\pmb { g } _ { u } = \\nabla _ { \\theta ^ { \\star } } \\mathcal { L } _ { u }$ 和 $\\pmb { g } _ { d } = \\nabla _ { \\theta ^ { \\star } } \\mathcal { L } _ { d }$，定义梯度矩阵 $G = \\{ g _ { u } , g _ { d } \\}$，通过奇异值分解 $\\mathbf { \\Psi } _ { G } = \\pmb { U } \\pmb { \\Sigma V } ^ { T }$ 和缩放奇异值使梯度矩阵满足最优条件，即 $\\hat { \\pmb { G } } = \\sigma \\pmb { U } \\pmb { V } ^ { \\top } = \\sigma \\pmb { G } \\pmb { V } \\pmb { \\Sigma } ^ { - 1 } \\pmb { V } ^ { T }$，其中 $\\sigma = \\sigma _ { \\mathrm { m i n } } ( G ) = \\sqrt { \\lambda _ { m i n } }$。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n在多模态图像融合任务中，基线模型包括 DIDFuse、U2Fusion、PIAFusion、SwinFusion、CDDFuse、Tardal、MetaFusion 等；在目标检测任务中，基线模型包括 YOLOv5s、DiffusionDet、RetinaNet - OBB、Faster R - CNN - OBB、Gliding Vertex、LSKNet - OBB 等，以及一些多模态融合检测方法如 CDDFuse、ICAFusion、Tardal、MetaFusion 等。\\n\\n**性能对比**\\n*   **在多模态图像融合指标上**：\\n    - **在熵 (EN) 指标上**：E2E - MFD 在 TNO、RoadScene 和 M3FD 数据集上的 EN 值分别为 6.36、6.40 和 6.79，优于其他对比方法，表明其融合结果包含更多边缘细节和更高的目标与背景对比度。例如，CDDFuse 在 M3FD 数据集上的 EN 值为 5.77。\\n    - **在互信息 (MI) 指标上**：E2E - MFD 总体达到最佳指标值，证明其能从源图像中传递更多重要信息。\\n    - **在视觉信息保真度 (VIF) 指标上**：E2E - MFD 在各数据集上的 VIF 值最大，如在 M3FD 数据集上为 1.65，说明其融合结果具有高质量的视觉效果和较小的失真。相比之下，DIDFuse 在 M3FD 数据集上的 VIF 值为 1.51。\\n    - **在训练时间上**：E2E - MFD 完成联合学习的训练时间最快，仅需 2h50m32s，而其他方法如 CDDFuse 需要 5h59m59s。在生成融合图像的测试时间上，E2E - MFD 排名第三，为 0.014s。\\n*   **在多模态目标检测指标上**：\\n    - **在 M3FD 数据集上**：E2E - MFD 的 $mAP_{50}$ 达到 91.8%，$mAP_{50:95}$ 达到 63.8%，显著优于其他方法。如 DIDFusion 的 $mAP_{50}$ 为 86.2%，$mAP_{50:95}$ 为 56.2%；Tardal 的 $mAP_{50}$ 为 86.0%，$mAP_{50:95}$ 为 56.0%。\\n    - **在 DroneVehicle 数据集上**：E2E - MFD 的 $mAP_{50}$ 达到 77.4%，高于其他对比方法。如 UA - CMDet 的 $mAP_{50}$ 为 64.0%；TSFADet 的 $mAP_{50}$ 为 64.0%。\",\n    \"keywords\": \"### 关键词\\n\\n- 多模态融合检测 (Multimodal Fusion Detection, MFD)\\n- 端到端算法 (End - to - End Algorithm, E2E)\\n- 对象区域像素系统发育树 (Object - Region - Pixel Phylogenetic Tree, ORPPT)\\n- 粗到细扩散处理 (Coarse - to - Fine Diffusion Process, CFDP)\\n- 梯度矩阵任务对齐 (Gradient Matrix Task - Alignment, GMTA)\\n- 自动驾驶 (Autonomous Driving, N/A)\\n- 遥感监测 (Remote Sensing Monitoring, N/A)\"\n}"
}