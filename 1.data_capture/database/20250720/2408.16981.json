{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.16981",
    "link": "https://arxiv.org/abs/2408.16981",
    "pdf_link": "https://arxiv.org/pdf/2408.16981.pdf",
    "title": "The Sample-Communication Complexity Trade-off in Federated Q-Learning",
    "authors": [
        "Sudeep Salgia",
        "Yuejie Chi"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-08-30",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 3,
    "influential_citation_count": 0,
    "institutions": [
        "Carnegie Mellon University"
    ],
    "paper_content": "# The Sample-Communication Complexity Trade-off in Federated Q-Learning\n\nSudeep Salgia Carnegie Mellon University ssalgia@andrew.cmu.edu\n\nYuejie Chi Carnegie Mellon University yuejiechi@cmu.edu\n\n# Abstract\n\nWe consider the problem of Federated Q-learning, where $M$ agents aim to collaboratively learn the optimal Q-function of an unknown infinite horizon Markov Decision Process with finite state and action spaces. We investigate the trade-off between sample and communication complexity for the widely used class of intermittent communication algorithms. We first establish the converse result, where we show that any Federated Q-learning that offers a linear speedup with respect to number of agents in sample complexity needs to incur a communication cost of at least $\\Omega \\big ( \\frac { \\mathrm { ~ \\ j ~ } } { 1 - \\gamma } \\big )$ , where $\\gamma$ is the discount factor. We also propose a new Federated Q-learning algorithm, called Fed-DVR-Q, which is the first Federated Q-learning algorithm to simultaneously achieve order-optimal sample and communication complexities. Thus, together these results provide a complete characterization of the sample-communication complexity trade-off in Federated Q-learning.\n\n# 1 Introduction\n\nReinforcement Learning (RL) [Sutton and Barton, 2018] refers to an online sequential decision making paradigm where the learning agent aims to learn an optimal policy, i.e., a policy that maximizes the long-term reward, through repeated interactions with an unknown environment. RL finds applications across a diverse array of fields including, but not limited to, autonomous driving, games, recommendation systems, robotics and Internet of Things (IoT) [Kober et al., 2013, Yurtsever et al., 2020, Silver et al., 2016, Lim et al., 2020].\n\nThe primary hurdle in RL applications is often the high-dimensional nature of the decision space that necessitates the learning agent to have to access to an enormous amount of data in order to have any hope of learning the optimal policy. Moreover, the sequential collection of such an enormous amount of data through a single agent is extremely time-consuming and often infeasible in practice. Consequently, practical implementations of RL involve deploying multiple agents to collect data in parallel. This decentralized approach to data collection has fueled the design and development of distributed or federated RL algorithms that can collaboratively learn the optimal policy without actually transferring the collected data to a centralized server. Such a federated approach to RL, which does not require the transfer of local data, is gaining interest due to lower bandwidth requirements and lower security and privacy risks. In this work, we focus on federated variants of Q-learning algorithms where the agents collaborate to directly learn the optimal Q-function without forming an estimate of the underlying unknown environment.\n\nA particularly important aspect of designing Federated RL algorithms, including Federated Q-learning algorithms, is to address the natural tension between sample and communication complexity. At one end of the spectrum lies the naïve approach of running a centralized algorithm with optimal sample complexity after transferring and combining all the collected data at a central facility/server. Such an approach trivially achieves the optimal sample complexity while suffering from a very high and infeasible communication complexity. On the other hand, several recently proposed algorithms [Khodadadian et al., 2022, Woo et al., 2023] operate in more practical regimes, offering significantly lower communication complexities as compared to the naïve approach at the cost of sub-optimal sample complexities. These results suggest the existence of underlying trade-off between sample and communication complexities of Federated RL algorithms. The primary goal of this work is to better understand this trade-off in context of Federated Q-learning by investigating these following fundamental questions:\n\n• Fundamental limit of communication: What is the minimum amount of communication required by a federated Q-learning algorithm to achieve any statistical benefit of collaboration? • Optimal algorithm design: How does one design a federated $\\boldsymbol { Q }$ -Learning algorithm that simultaneously offers optimal order sample and communication complexity guarantees i.e., operates on the optimal frontier of sample-communication complexity trade-off?\n\n# 1.1 Main Results\n\nWe consider a setup where $M$ distributed agents collaborate to learn the optimal Q-function of an infinite horizon Markov Decision Process which is defined over a finite state space $s$ and a finite action set $\\mathcal { A }$ , and has a discount factor of $\\gamma \\in ( 0 , 1 )$ . We consider a commonly considered setup in federated learning called the intermittent communication setting, where the clients intermittently share information among themselves with the help of a central server. In this work, we provide a complete characterization of the trade-off between sample and communication complexity under the aforementioned setting by providing answers to both the questions. The main result of this work is twofold and is summarized below.\n\n• Fundamental bounds on communication complexity of Federated $\\boldsymbol { Q }$ -learning: We establish lower bounds on the communication complexity of Federated Q-learning, both in terms of number of communication rounds and the overall number of bits that need to be transmitted in order to achieve any speed up in convergence with respect to the number of agents. Specifically, we show that in order for an intermittent communication algorithm to obtain any benefit of collaboration, i.e., any order of speed up w.r.t. the number of agents, the number of communication rounds must be least $\\Omega \\big ( \\frac { \\mathbf { \\lambda } ^ { \\star } } { ( 1 - \\gamma ) \\log ^ { 2 } N } \\big )$ and the number of bits sent by each agent to the server must be least (1−|γS)| |loAg|2 N ), where N denotes the number of samples taken by the algorithm for each state-action pair. • Achieving the optimal sample-communication complexity trade-off : We propose a new Federated Q-Learning algorithm called Federated Doubly Variance Reduced Q Learning, Fed-DVR-Q for short, that simultaneously achieves optimal order of sample complexity and the minimal order of communication as dictated by the lower bound. We show that Fed-DVR-Q learns an $\\varepsilon$ -optimal Q-function in the $\\ell _ { \\infty }$ sense with $\\begin{array} { r } { \\tilde { \\mathcal { O } } \\left( \\frac { | \\mathcal { S } | | \\mathcal { A } | } { M \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\right) } \\end{array}$ i.i.d. samples from the generative model at each agent while incurring a total communication cost of $\\begin{array} { r } { \\tilde { \\mathcal O } \\left( \\frac { | { \\mathcal S } | | { \\mathcal A } | } { \\left( 1 - \\gamma \\right) } \\right) } \\end{array}$ bits per agent across $\\begin{array} { r } { \\tilde { \\mathcal O } \\left( \\frac { 1 } { ( 1 - \\gamma ) } \\right) } \\end{array}$ rounds of communication. Thus, Fed-DVR-Q not only improves upon both the sample and communication complexities of existing algorithms, but also is the first algorithm to achieve both order-optimal sample and communication complexities (See Table 1 for a comparison).\n\n# 1.2 Related Work\n\nSingle agent Q-Learning. Q-Learning has been extensively studied in the single-agent setting in terms of both its asymptotic convergence [Jaakkola et al., 1993, Tsitsiklis, 1994, Szepesvári, 1997, Borkar and Meyn, 2000] and its finite-time sample complexity in both synchronous [Even-Dar and Mansour, 2004, Beck and Srikant, 2012, Wainwright, 2019a, Chen et al., 2020, Li et al., 2023] and asynchronous settings [Chen et al., 2021b, Li et al., 2023, 2021, Qu and Wierman, 2020].\n\nDistributed RL. There has also been a considerable effort towards developing distributed and federated RL algorithms. The distributed variants of the classical TD learning algorithm have been investigated in a series of studies [Chen et al., 2021c, Doan et al., 2019, 2021, Sun et al., 2020, Wai, 2020, Wang et al., 2020, Zeng et al., 2021b]. The impact of environmental heterogeneity in federated TD learning was studied in Wang et al. [2023]. A distributed version of actor-critic algorithms was studied by Shen et al. [2023] where the authors established convergence of their algorithm and demonstrated a linear speed up in the number of agents in their sample complexity bound. Chen et al. [2022] proposed a new distributed actor-critic algorithm which improved the dependence of sample complexity on the error $\\varepsilon$ and incurs a communication cost of $\\bar { \\mathcal { O } } ( \\bar { \\varepsilon } ^ { - 1 } )$ . Chen et al. [2021a] have proposed a communication efficient distributed policy gradient algorithm and have analyzed its convergence and established a communication complexity of $\\mathcal { O } ( 1 / ( M \\varepsilon ) )$ . Xie and Song [2023] adopts a distributed policy optimization perspective, which is different from the Q-learning paradigm considered in this work. Moreover, the algorithm in Xie and Song [2023] obtains a linear communication cost, which is worse than that obtained in our work. Similarly, Zhang et al. [2024] focuses on on-policy learning and incurs a communication cost that depends polynomially on the required error $\\varepsilon$ . Several other studies [Yang et al., 2023, Zeng et al., 2021a, Lan et al., 2024] have also developed and analyzed other distributed/federated variants of the classical natural policy gradient method [Kakade, 2001]. Assran et al. [2019], Espeholt et al. [2018], Mnih et al. [2016] have developed distributed algorithms to train deep RL networks more efficiently.\n\nTable 1: Comparison of sample and communication complexity of various single-agent and Federated Q-learning algorithms for learning an $\\varepsilon$ -optimal Q-function under the synchronous setting. We hide logarithmic factors and burn-in costs for all results for simplicity of presentation. In the above table, $s$ and $\\mathcal { A }$ represent state and action spaces respectively and $\\gamma$ denotes the discount factor. We report the communication complexity only in terms of number of rounds as other algorithms assume transmission of real numbers and hence do not report bit level costs. For the lower bound, Azar et al. [2013] and this work establish the bound for sample and communication complexity respectively.   \n\n<html><body><table><tr><td>Algorithm/Reference</td><td>Number of Agents</td><td>Sample Complexity</td><td>Communication Complexity</td></tr><tr><td>Q-learning [Li et al., 2023]</td><td>1</td><td>[S|A (1-γ)4g²</td><td>N/A</td></tr><tr><td>Variance Reduced Q-learning [Wainwright, 2019b]</td><td>1</td><td>[S||A| (1-γ)²</td><td>N/A</td></tr><tr><td>Fed-SynQ [Woo et al., 2023]</td><td>M</td><td>[S|A M(1-γ)5ε²</td><td>M 1-γ</td></tr><tr><td>Fed-DVR-Q (This work)</td><td>M</td><td>[S|A M(1-γ)³ε²</td><td>1 1-γ</td></tr><tr><td>Lower bound([Azar etal.,2O13],This work)</td><td>M</td><td>[S||A| M(1-γ)3ε²</td><td>1 1-γ</td></tr></table></body></html>\n\nDistributed Q-learning. Federated Q-learning has been explored relatively recently. Khodadadian et al. [2022] proposed and analyzed a federated Q-learning algorithm in the asynchronous setting with a sample complexity of $\\begin{array} { r } { { \\tilde { \\mathcal { O } } } \\left( \\frac { | { \\mathcal { S } } | ^ { 2 } } { M \\mu _ { \\mathrm { m i n } } ^ { 5 } ( 1 - \\gamma ) ^ { 9 } \\varepsilon ^ { 2 } } \\right) } \\end{array}$ , where $\\mu _ { \\mathrm { m i n } }$ is the minimum entry of the stationary state-action occupancy distribution of the sample trajectories over all agents. Jin et al. [2022] study the impact of environmental heterogeneity across clients in Federated Q-learning. They propose an algorithm where the local environments are different at each client but each client knows their local environment. Under this setting, they propose an algorithm that achieves a sample and communication complexity of $\\begin{array} { r } { \\mathcal { O } \\big ( \\frac { 1 } { ( 1 - \\gamma ) ^ { 3 } \\varepsilon } \\big ) } \\end{array}$ and $\\begin{array} { r } { \\dot { \\mathcal { O } } \\big ( \\frac { 1 } { ( 1 - \\gamma ) ^ { 3 } \\varepsilon } \\big ) } \\end{array}$ rounds respectively. Woo et al. [2023] proposed new algorithms with improved analysis for Federated Q-learning under both synchronous and asynchronous settings. Their proposed algorithm achieves a sample complexity and communication complexity of O˜( M(|1S−||γA)|5ε2 ) and O˜( M1|S−||γA| ) real numbers respectively under the synchronous setting and that of $\\begin{array} { r } { \\tilde { \\mathcal { O } } \\big ( \\frac { 1 } { M \\mu _ { \\mathrm { a v g } } ( 1 - \\gamma ) ^ { 5 } \\varepsilon ^ { 2 } } \\big ) } \\end{array}$ and $\\begin{array} { r } { \\tilde { \\mathcal { O } } \\left( \\frac { M | \\mathcal { S } | | \\mathcal { A } | } { 1 - \\gamma } \\right) } \\end{array}$ real numbers respectively under the asynchronous setting. Here, $\\mu _ { \\mathrm { a v g } }$ denotes the minimum entry of the average stationary state-action occupancy distribution of all agents. In a follow up work, Woo et al. [2024] propose a Federated Qlearning for offline RL in finite horizon setting and establish a sample and communication complexity of $\\tilde { \\mathcal { O } } \\big ( \\frac { H ^ { 7 } | S | C _ { \\mathrm { a v g } } } { M \\varepsilon ^ { 2 } } \\big )$ and $\\tilde { \\mathcal { O } } ( H )$ , where $H$ denotes the length of the horizon and $C _ { \\mathrm { a v g } }$ denotes the average single-policy concentrability coefficient of all agents.\n\nAccuracy-Communication Trade-off in Federated Learning. The trade-off between communication complexity and accuracy (equivalently, sample complexity) has been studied in various federated and distributed learning problems, including stochastic approximation algorithms for convex optimization. Duchi et al. [2014], Braverman et al. [2016] establish the celebrated inverse linear relationship between the error and the communication cost the problem of distributed mean estimation. Similar trade-off for distributed stochastic optimization, multi-armed bandits and linear bandits has been studied and established across numerous studies [Woodworth et al., 2018, 2021, Tsitsiklis and Luo, 1987, Shi and Shen, 2021, Salgia and Zhao, 2023].\n\n# 2 Problem Formulation and Preliminaries\n\nIn this section, we provide a brief background of Markov Decision Processes, outline the performance measures for Federated Q-learning algorithms and describe the class of intermittent communication algorithms considered in this work.\n\n# 2.1 Markov Decision Processes\n\nIn this work, we focus on an infinite-horizon Markov Decision Process (MDP), denoted by $\\mathcal { M }$ , over a state space $s$ and an action space $\\mathcal { A }$ and with a discount factor $\\gamma \\in ( 0 , 1 )$ . Both the state and action spaces are assumed to be finite sets. In an MDP, the state $s$ evolves dynamically under the influence of actions based on a probability transition kernel, $P : ( S \\times \\mathcal { A } ) \\times S  [ 0 , 1 ]$ . The entry $P ( s ^ { \\prime } | s , a )$ denotes the probability of moving to state $s ^ { \\prime }$ when an action $a$ is taken in the state $s$ . An MDP is also associated with a deterministic reward function $r : S \\times \\mathcal { A }  [ 0 , 1 ]$ , where $r ( s , a )$ denotes the immediate reward obtained for taking the action $a$ in the state $s$ . Thus, the transition kernel $P$ along with the reward function $r$ completely characterize an MDP. In this work, we consider the synchronous setting, where each agent has access to an independent generative model or simulator from which they can draw independent samples from the unknown underlying distribution $P ( \\cdot | s , a )$ for each state-action pair $( s , a )$ [Kearns and Singh, 1998].\n\nA policy $\\pi : S  \\Delta ( { \\mathcal { A } } )$ is a rule for selecting actions across different states, where $\\Delta ( \\mathcal { A } )$ denotes the simplex over $\\mathcal { A }$ and $\\pi ( a | s )$ denotes the probability of choosing action $a$ in a state $s$ . Each policy $\\pi$ is associated with a state value function and a state-action value function, or the Q-function, denoted by $V ^ { \\pi }$ and $Q ^ { \\pi }$ respectively. $V ^ { \\pi }$ and $Q ^ { \\pi }$ measure the expected discounted cumulative reward attained by $\\pi$ starting from a particular state $s$ and state-action pair $( s , a )$ respectively. Mathematically, $V ^ { \\pi }$ and $Q ^ { \\pi }$ are given as\n\n$$\nV ^ { \\pi } ( s ) : = \\mathbb { E } \\left[ \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } r ( s _ { t } , a _ { t } ) \\bigg | \\ s _ { 0 } = s \\right] ; \\quad Q ^ { \\pi } ( s , a ) : = \\mathbb { E } \\left[ \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } r ( s _ { t } , a _ { t } ) \\bigg | \\ s _ { 0 } = s , a _ { 0 } = a \\right] ,\n$$\n\nwhere $a _ { t } \\sim \\pi ( \\cdot | s _ { t } )$ and $s _ { t + 1 } \\sim P ( \\cdot | s _ { t } , a _ { t } )$ for all $t \\geq 0$ . The expectation is taken w.r.t. the randomness in the trajectory $\\{ s _ { t } , a _ { t } \\} _ { t = 1 } ^ { \\infty }$ . Since the rewards lie in $[ 0 , 1 ]$ , it follows immediately that both the value function and Q-function lie in the range $[ 0 , \\frac { 1 } { 1 - \\gamma } ]$ .\n\nAn optimal policy $\\pi ^ { \\star }$ is a policy that maximizes the value function uniformly over all the states and it has been shown that such an optimal policy $\\pi ^ { \\star }$ always exists [Puterman, 2014]. The optimal value and Q-functions are those corresponding to that of an optimal policy $\\pi ^ { \\star }$ are denoted as $V ^ { \\star } : = V ^ { \\pi ^ { \\star } }$ and $Q ^ { \\star } : = Q ^ { \\pi ^ { \\star } }$ respectively. The optimal Q-function, $Q ^ { \\star }$ , is also the unique fixed point of the Bellman operator $\\mathcal { T } : \\mathcal { S } \\times \\mathcal { A }  \\mathcal { S } \\times \\mathcal { A }$ , given by\n\n$$\n( \\mathcal T Q ) ( s , a ) = r ( s , a ) + \\gamma \\cdot \\mathbb { E } _ { s ^ { \\prime } \\sim P ( \\cdot \\mid s , a ) } \\left[ \\operatorname* { m a x } _ { a ^ { \\prime } \\in \\mathcal A } Q ( s ^ { \\prime } , a ^ { \\prime } ) \\right] .\n$$\n\nQ-learning [Watkins and Dayan, 1992] aims to learn the optimal policy by first learning $Q ^ { \\star }$ as the solution to the fixed point equation $\\mathcal { T } Q = Q$ and then obtain a deterministic optimal policy via the maximization $\\pi ^ { \\star } ( s ) = \\arg \\operatorname* { m a x } _ { a } Q ^ { \\star } ( s , a )$ .\n\nLet $Z \\in { \\mathcal { S } } ^ { | S | | A | }$ be a random vector whose $( s , a ) ^ { \\mathrm { t h } }$ coordinate is drawn from the distribution $P ( \\cdot | s , a )$ independently of all other coordinates. We define the random operator $\\mathcal { T } _ { Z } : ( S \\times \\mathcal { A } )  ( S \\times \\mathcal { A } )$ as\n\n$$\n( { \\mathcal { T } } _ { Z } Q ) ( s , a ) = r ( s , a ) + { \\gamma } V ( Z ( s , a ) ) ,\n$$\n\nwhere $V ( s ^ { \\prime } ) = \\mathrm { m a x } _ { a ^ { \\prime } \\in { \\mathcal { A } } } Q ( s ^ { \\prime } , a ^ { \\prime } )$ . The operator $\\mathcal { T } _ { Z }$ can be interpreted as the sample Bellman Operator, where we have the relation $\\mathcal { T } Q = \\mathrm { \\bar { E } } _ { Z } [ \\mathcal { T } _ { Z } Q ]$ for all Q-functions.\n\nLastly, the federated learning setup considered in this work consists of $M$ agents, where all the agents face a common, unknown MDP, i.e., the transition kernel and the reward functions are the same across agents, which is popularly known as the homogeneous setting. For a given value of $\\varepsilon \\in ( 0 , \\frac { 1 } { 1 - \\gamma } )$ , the objective of agents is to collaboratively learn an $\\varepsilon$ -optimal estimate (in the $\\ell _ { \\infty }$ sense) of the optimal Q-function of the unknown MDP.\n\n# 2.2 Performance Measures\n\nWe measure the performance of a Federated Q-learning algorithm $\\mathcal { A }$ using two metrics — sample complexity and communication complexity. For a given MDP $\\mathcal { M }$ , let $\\widehat { Q } _ { \\mathcal { M } } ( \\mathcal { A } , N , M )$ denote the estimate of $Q _ { \\mathcal { M } } ^ { \\star }$ , the optimal Q-function of the MDP $\\mathcal { M }$ , returned by anbalgorithm $\\mathcal { A }$ , when given access to $N$ i.i.d. samples from the generative model for each $( s , a )$ pair at all the $M$ agents. The minimax error rate of the algorithm $\\mathcal { A }$ , denoted by $\\mathsf { E R } ( \\mathcal { A } ; N , \\dot { M } )$ , is defined as\n\n$$\n\\mathsf { E R } ( \\mathcal { A } ; N , M ) : = \\operatorname* { s u p } _ { \\mathcal { M } = ( P , r ) } \\mathbb { E } \\left[ \\| \\widehat { Q } _ { \\mathcal { M } } ( \\mathcal { A } , N , M ) - Q _ { \\mathcal { M } } ^ { \\star } \\| _ { \\infty } \\right] ,\n$$\n\nwhere the expectation is taken over the samples and any randomness in the algorithm. Given a value of $\\varepsilon > 0$ , the sample complexity of $\\mathcal { A }$ , denoted by $\\mathsf { S C } ( \\mathcal { A } ; \\varepsilon , M )$ is given as\n\n$$\n\\mathsf { S C } ( \\mathcal { A } ; \\varepsilon , M ) : = | \\mathcal S | | \\mathcal { A } | \\cdot \\operatorname* { m i n } \\{ N \\in \\mathbb N : \\mathsf { E } \\mathsf { R } ( \\mathcal { A } ; N , M ) \\leq \\varepsilon \\} .\n$$\n\nSimilarly, we can also define a high-probability version for any $\\delta \\in ( 0 , 1 )$ as follows:\n\n$$\n\\mathsf { S C } ( \\mathcal { A } ; \\varepsilon , M , \\delta ) : = | \\mathcal { S } | | \\mathcal { A } | \\cdot \\operatorname* { m i n } \\{ N \\in \\mathbb { N } : \\operatorname* { P r } ( \\operatorname* { s u p } _ { M } \\| \\widehat { Q } _ { \\mathcal { M } } ( \\mathcal { A } , N , M ) - Q _ { M } ^ { \\star } \\| _ { \\infty } \\leq \\varepsilon ) \\geq 1 - \\delta \\} .\n$$\n\nWe measure the communication complexity of any federated learning algorithm both in terms of frequency of information exchange and total number of bits uploaded by the agents. For each agent $m$ , let $C _ { \\sf r o u n d } ^ { m } ( \\mathcal { A } ; N )$ and $C _ { \\mathsf { b i t } } ^ { m } ( \\mathcal { A } ; \\bar { N } )$ respectively denote the number of times agent $m$ sends a message to the server and the total number of bits uploaded by agent $m$ to the server when an algorithm $\\mathcal { A }$ is run with $N$ i.i.d. samples from the generative model for each $( s , a )$ pair at each agent. The communication complexity of $\\mathcal { A }$ , when measured in terms of frequency of communication and total number of bits exchanged, is given by\n\n$$\n\\mathsf { C C } _ { \\mathsf { r o u n d } } ( \\mathcal { A } ; N ) : = \\frac { 1 } { M } \\sum _ { m = 1 } ^ { M } C _ { \\mathsf { r o u n d } } ^ { m } ( \\mathcal { A } ; N ) ; \\quad \\mathsf { C C } _ { \\mathsf { b i t } } ( \\mathcal { A } ; N ) : = \\frac { 1 } { M } \\sum _ { m = 1 } ^ { M } C _ { \\mathsf { b i t } } ^ { m } ( \\mathcal { A } ; N ) ,\n$$\n\nrespectively. Similarly, for a given value of $\\begin{array} { r } { \\varepsilon \\in ( 0 , \\frac { 1 } { 1 - \\gamma } ) } \\end{array}$ , we can also define $\\mathsf { C C } _ { \\mathsf { r o u n d } } ( \\mathcal { A } ; \\varepsilon )$ and $\\mathsf { C C } _ { \\mathsf { b i t } } ( \\mathcal { A } ; \\varepsilon )$ based on when $\\mathcal { A }$ is run to guarantee a minimax error of at most $\\varepsilon$ .\n\n# 2.3 Intermittent Communication Algorithms\n\nIn this work, we consider a popular class of federated learning algorithms referred to as algorithms with intermittent communication. The intermittent communication setting provides a natural framework to extend single agent Qlearning algorithms to the distributed setting. As the name suggests, under this setting, the agents intermittently communicate with each other, sharing their updated beliefs about $Q ^ { \\star }$ . Between two communication rounds, each agent updates their belief about $Q ^ { \\star }$ using stochastic fixed point iteration based on the locally available data, similar to a single agent setup. Such intermittent communication algorithms\n\n<html><body><table><tr><td>Algorithm1:A generic algorithm</td></tr><tr><td>1: Input : T,R,{nt}t=1,C= {tr}=1,B</td></tr><tr><td>2:Set Qm ←O for all agents m</td></tr><tr><td>3:for t=1,2,...,Tdo</td></tr><tr><td>45</td></tr><tr><td>fo1</td></tr><tr><td>6: Compute Qm according to Eqn. 8</td></tr><tr><td>7: end for</td></tr><tr><td>8:end for</td></tr><tr><td>9: return QT</td></tr></table></body></html>\n\nhave been extensively studied and used to establish lower bounds on communication complexity of distributed stochastic convex optimization [Woodworth et al., 2018, 2021].\n\nA generic Federated Q-learning algorithm with intermittent communication is outlined in Algorithm 1. It is characterized by the following five parameters: (i) total number of updates $T$ ; (ii) the number of communication rounds $R$ ; (iii) a step size schedule $\\{ \\eta _ { t } \\} _ { t = 1 } ^ { T }$ ; (iv) a communication schedule $\\{ t _ { r } \\} _ { r = 1 } ^ { R }$ ; (v) batch size $B$ . During the $t ^ { \\mathrm { { t h } } }$ iteration, each agent $m$ computes $\\{ \\widehat { T } _ { Z _ { b } } ( Q _ { t - 1 } ^ { m } ) \\} _ { b = 1 } ^ { B }$ , a minibatch of sample Bellman operators at the current estimate $Q _ { t - 1 } ^ { m }$ , using $B$ sambples from the generative model for each $( s , a )$ pair, and obtains an intermediate local estimate using the Q-learning update as follows:\n\n$$\nQ _ { t - \\frac { 1 } { 2 } } ^ { m } = ( 1 - \\eta _ { t } ) Q _ { t - 1 } ^ { m } + \\frac { \\eta _ { t } } { B } \\sum _ { b = 1 } ^ { B } \\mathcal { T } _ { Z _ { b } } ( Q _ { t - 1 } ^ { m } ) .\n$$\n\nHere $\\eta _ { t } \\in ( 0 , 1 ]$ is the step-size chosen corresponding to the $t ^ { \\mathrm { { t h } } }$ time step. The intermediate estimates are averaged based on a communication schedule $\\boldsymbol { \\mathcal { C } } = \\{ t _ { r } \\} _ { r = 1 } ^ { R }$ consisting of $R$ rounds, i.e.,\n\n$$\n\\begin{array} { r } { Q _ { t } ^ { m } = \\left\\{ \\begin{array} { l l } { \\frac { 1 } { M } \\sum _ { j = 1 } ^ { M } Q _ { t - \\frac { 1 } { 2 } } ^ { j } } & { \\mathrm { ~ i f ~ } t \\in \\mathcal { C } , } \\\\ { Q _ { t - \\frac { 1 } { 2 } } ^ { m } } & { \\mathrm { ~ o t h e r w i s e } . } \\end{array} \\right. } \\end{array}\n$$\n\nIn the above equation, the averaging step can also be replaced with any distributed mean estimation routine that includes compression to control the bit level costs. Without loss of generality, we assume that $Q _ { 0 } ^ { m } = 0$ for all agents $m$ and $t _ { R } = T$ , i.e., the last iterates are always averaged. It is straightforward to note that the number of samples taken by an intermittent communication algorithm is $B T$ , i.e, $N = B T$ and the communication complexity ${ \\mathsf { C C } } _ { \\mathsf { r o u n d } } = R$ .\n\n# 3 Lower Bound\n\nIn this section, we investigate the first of the two questions regarding the lower bound on communication complexity. The following theorem establishes a lower bound on the communication complexity of a Federated Q-learning algorithm with intermittent communication.\n\nTheorem 1. Assume that $\\gamma \\in [ 5 / 6 , 1 )$ and the state and action spaces satisfy $| { \\cal S } | \\geq 4$ and $| { \\cal A } | \\geq$ 2. Let $\\mathcal { A }$ be a Federated $\\boldsymbol { Q }$ -learning algorithm with intermittent communication that is run for $T \\geq \\operatorname* { m a x } \\{ 1 6 , \\frac { 1 } { 1 - \\gamma } \\}$ steps with a step size schedule of either $\\begin{array} { r } { \\eta _ { t } : = \\frac { 1 } { 1 + c _ { \\eta } ( 1 - \\gamma ) t } } \\end{array}$ or $\\eta _ { t } : = \\eta$ for all $1 \\leq t \\leq T$ . If\n\n$$\nR = C C _ { r o u n d } ( \\mathcal { A } ; N ) \\leq \\frac { c _ { 0 } } { ( 1 - \\gamma ) \\log ^ { 2 } N } ; \\ o r \\ C C _ { b i t } ( \\mathcal { A } ; N ) \\leq \\frac { c _ { 1 } | \\mathcal { S } | | \\mathcal { A } | } { ( 1 - \\gamma ) \\log ^ { 2 } N }\n$$\n\nfor some universal constants $c _ { 0 } , c _ { 1 } > 0$ then, for all choices of communication schedule, batch size $B$ , $c _ { \\eta } > 0$ and $\\eta \\in ( 0 , 1 )$ , the minimax error of $\\mathcal { A }$ satisfies\n\n$$\nE R ( \\mathcal { A } ; N , M ) \\geq \\frac { C _ { \\gamma } } { \\log ^ { 3 } N \\sqrt { N } } ,\n$$\n\nfor all $M \\geq 2$ and $N = B T$ . Here $C _ { \\gamma } > 0$ is a constant that depends only on $\\gamma$ .\n\nThe above theorem states that in order for an intermittent communication algorithm to obtain any benefit of collaboration, i.e., for the error rate $\\mathsf { E R } ( \\mathcal { A } ; N , M )$ to decrease w.r.t. number of agents, the number of communication rounds must be least 1 γ)1log2 N ). This implies that any Federated Q-learning algorithm that offers order optimal sample complexity, and thereby also a linear speed up with respect to the number of agents, must have at least ( (1 γ)1log2 N ) rounds of communication faonrdtthreanssamiptl $\\begin{array} { r } { \\Omega \\big ( \\frac { | \\boldsymbol { \\mathcal { S } } | | \\boldsymbol { \\mathcal { A } } | } { ( 1 - \\gamma ) \\log ^ { 2 } N } \\big ) } \\end{array}$ nbitrsaodfeoifnf oinrmFaetdieornatpedr aQg-lenatr.nTinhgi.s cWhearwacotuelrdizleiksethtoe cpoinvterosuet rthelattiounr lower bound extends to the asynchronous setting as the assumption of i.i.d. noise corresponding to a generative model is a special case of Markovian noise observed in asynchronous setting.\n\nThe lower bound on the communication complexity of Federated Q-learning is a consequence of the bias-variance trade-off that governs the convergence of the algorithm. While a careful choice of step-sizes alone is sufficient to balance this trade-off in the centralized setting, the choice of communication schedule also plays an important role in balancing this trade-off in the federated setting. The local steps between two communication rounds induce a positive estimation bias that depends on the standard deviation of the iterates and is a well-documented issue of “over-estimation” in Q-learning [Hasselt, 2010]. Since such a bias is driven by local updates, it does not reflect any benefit of collaboration. During a communication round, the averaging of iterates across agents allows the algorithm an opportunity to counter this bias by reducing the effective variance of the updates through averaging. In our analysis, we show that if the communication is infrequent, the local bias becomes the dominant term and averaging of iterates is insufficient to counter the impact of the positive bias induced by the local steps. As a result, we do not observe any statistical gains when the communication is infrequent. The analysis is inspired the analysis of Q-learning by Li et al. [2023] and is based on analyzing the convergence of an intermittent communication algorithm on a specifically chosen “hard” instance of MDP. Please refer to Appendix B for a detailed proof.\n\nRemark 1 (Communication complexity of policy evaluation). Several recent studies [Liu and Olshevsky, 2023, Tian et al., 2024] established that a single round of communication is sufficient to achieve linear speedup of TD learning for policy evaluation, which do not contradict with our results focusing on Q-learning for policy learning. The latter is more involved due to the nonlinearity of the Bellman optimality operator. Specifically, if the operator whose fixed point is to be found is linear in the decision variable (e.g., the value function in TD learning) then the fixed point update only induces a variance term corresponding to the noise. However, if the operator is non-linear, then in addition to the variance term, we also obtain a bias term in the fixed point update. While the variance term can be controlled with one-shot averaging, more frequent communication is necessary to ensure that the bias term is small enough.\n\nRemark 2 (Extension to asynchronous Q-learning). We would like to point out that our lower bound extends to the asynchronous setting [Li et al., 2023] as the assumption of i.i.d. noise corresponding to a generative model is a special case of Markovian noise observed in the asynchronous setting.\n\n# 4 The Fed-DVR-Q algorithm\n\nHaving characterized the lower bound on the communication complexity of Federated Q-learning, we explore our second question of interest — designing a federated Q-learning algorithm that achieves this lower bound while simultaneously offering an optimal order of sample complexity.\n\nWe propose a new Federated Q-learning algorithm, Fed-DVR-Q, that achieves not only a communication complexity of $\\begin{array} { r } { \\mathsf { C C } _ { \\mathsf { r o u n d } } = \\tilde { \\mathcal { O } } ( \\frac { 1 } { 1 - \\gamma } ) } \\end{array}$ and $\\begin{array} { r } { \\mathsf { C C } _ { \\mathsf { b i t } } = \\tilde { \\mathcal { O } } ( \\frac { | \\mathcal { S } | | \\mathcal { A } | } { 1 - \\gamma } ) } \\end{array}$ but also the optimal order of sample complexity (upto logarithmic factors), thereby providing a tight characterization of the achievability frontier that matches with the converse result derived in the previous section.\n\n# 4.1 Algorithm Description\n\nFed-DVR-Q proceeds in epochs. During an epoch $k \\geq 1$ , the agents collaboratively update $Q ^ { ( k - 1 ) }$ , the estimate of $Q ^ { \\star }$ obtained at the end of previous epoch, to a new estimate $Q ^ { ( k ) }$ , with the aid of the sub-routine called REFINEESTIMATE. The sub-routine REFINEESTIMATE is designed to ensure that the suboptimality gap, $\\| \\bar { Q } ^ { ( k ) } - Q ^ { \\star } \\| _ { \\infty }$ , reduces by a factor of 2 at the end of every epoch. Thus, at the end of $K = \\mathcal { O } ( \\log ( 1 / \\varepsilon ) )$ epochs, Fed-DVR-Q obtains a $\\varepsilon$ -optimal estimate of $Q ^ { \\star }$ , which is then set to be the output of the algorithm. Please refer to Alg. 2 for a pseudocode.\n\n# Algorithm 2: Fed-DVR-Q\n\n1: Input : Error bound $\\varepsilon > 0$ , failure probability $\\delta \\bar { > } 0$   \n2: $k \\gets 1 , Q ^ { ( 0 ) } \\gets \\mathbf { 0 }$   \n3: // Set parameters as described in Sec. 4.1.3   \n4: for $k = 1 , 2 , \\ldots , K$ do   \n5: Q(k) REFINEESTIMATE(Q(k−1), B, I, Lk, Dk, J)   \n6: k k + 1   \n7: end for   \n8: return Q(K)\n\n# 4.1.1 The REFINEESTIMATE sub-routine\n\nREFINEESTIMATE, starting from $\\overline { { Q } }$ , an initial estimate of $Q ^ { \\star }$ , uses variance reduced Q-learning updates to obtain an improved estimate of $Q ^ { \\star }$ . It is characterized by four parameters — the initial estimate $\\overline { { Q } }$ , the number of local iterations $I$ , the recentering sample size $L$ and the batch size $B$ ,\n\nwhich can be appropriately tuned to control the quality of the returned estimate. Additionally, it also takes input two parameters $D$ and $J$ required by the compressor.\n\nThe first step in REFINEESTIMATE is to collaboratively approximate $\\tau { \\overline { { Q } } }$ for the variance reduced updates. To this effect, each agent $m$ builds an approximation of $\\tau { \\overline { { Q } } }$ as follows:\n\n$$\n\\widetilde { \\mathcal { T } } _ { L } ^ { ( m ) } ( \\overline { { Q } } ) : = \\frac { 1 } { \\lceil L / M \\rceil } \\sum _ { l = 1 } ^ { \\lceil L / M \\rceil } \\mathcal { T } _ { Z _ { l } ^ { ( m ) } } ( \\overline { { Q } } ) ,\n$$\n\nwhere $\\{ Z _ { 1 } ^ { ( m ) } , Z _ { 2 } ^ { ( m ) } , \\ldots , Z _ { \\lceil L / M \\rceil } ^ { ( m ) } \\}$ are $\\lceil L / M \\rceil$ i.i.d. samples with $Z _ { 1 } ^ { ( m ) } \\sim Z$ . Each agent sends $\\mathcal { C } \\left( \\widetilde { \\mathcal { T } } _ { L } ^ { ( m ) } ( \\overline { { Q } } ) - \\overline { { Q } } ; D , J \\right)$ , a compressed version of the difference $\\widetilde { \\mathcal { T } } _ { L } ^ { ( m ) } ( \\overline { { Q } } ) - \\overline { { Q } }$ , to the server, which coll ects all the estimates from the agents and constructs the estimaete\n\n$$\n\\widetilde { \\mathcal { T } } _ { L } ( \\overline { { Q } } ) = \\overline { { Q } } + \\frac { 1 } { M } \\sum _ { m = 1 } ^ { M } \\mathcal { C } \\left( \\widetilde { \\mathcal { T } } _ { L } ^ { ( m ) } ( \\overline { { Q } } ) - \\overline { { Q } } ; D , J \\right)\n$$\n\nand sends it back to the agents for the variance reduced updates. We defer the description of the compression routine to the end of this section. Equipped with the estimate $\\widetilde { \\mathcal { T } } _ { L } ( \\overline { { Q } } )$ , REFINEESTIMATE constructs a sequence $\\{ Q _ { i } \\} _ { i = 1 } ^ { I }$ using the following iterative update scheme initialized with $Q _ { 0 } = \\overline { { Q } }$ . During the $i ^ { \\mathrm { { t h } } }$ iteration, each agent $m$ carries out the following update:\n\n$$\nQ _ { i -  { { \\frac { 1 } { 2 } } } } ^ { m } = ( 1 - \\eta ) Q _ { i - 1 } + \\eta \\left[ \\widehat { \\mathcal { T } } _ { i } ^ { ( m ) } Q _ { i - 1 } - \\widehat { \\mathcal { T } } _ { i } ^ { ( m ) } \\overline { { Q } } + \\widetilde { \\mathcal { T } } _ { L } ( \\overline { { Q } } ) \\right] .\n$$\n\nIn the above equation, $\\eta \\in ( 0 , 1 )$ is the step size and $\\begin{array} { r } { \\widehat { \\mathcal { T } } _ { i } ^ { ( m ) } Q : = \\frac { 1 } { B } \\sum _ { z \\in \\mathcal { Z } _ { i } ^ { ( m ) } } \\mathcal { T } _ { z } Q , } \\end{array}$ , where $\\mathcal { Z } _ { i } ^ { ( m ) }$ is the minibatch of $B$ i.i.d. random variables drawn accordbing to $Z$ , independently at each agent $m$ for all iterations $i$ . Each agent then sends a compressed version of the update, i.e., $\\mathcal { C } \\left( Q _ { i - \\frac { 1 } { 2 } } ^ { m } - Q _ { i - 1 } ; D , J \\right)$ , to the server, which uses them to compute the next iterate\n\n$$\nQ _ { i } = Q _ { i - 1 } + \\frac { 1 } { M } \\sum _ { m = 1 } ^ { M } \\mathcal { C } \\left( Q _ { i - \\frac { 1 } { 2 } } ^ { m } - Q _ { i - 1 } ; D , J \\right) ,\n$$\n\nand broadcast it to the clients. After $I$ such updates, the obtained iterate $Q _ { I }$ is returned by the routine.   \nA pseudocode of the REFINEESTIMATE routine is given in Algorithm 3 in Appendix A.\n\n# 4.1.2 The Compression Operator\n\nThe compressor, $\\mathcal { C } ( \\cdot ; D , J )$ , used in the proposed algorithm Fed-DVR-Q is based on the popular stochastic quantization scheme. In addition to the input vector $Q$ to be quantized, the quantizer $\\mathcal { C }$ takes two input parameters $D$ and ${ \\ J } . { \\ D }$ corresponds to an upper bound on $\\ell _ { \\infty }$ norm of $Q$ , i.e., $\\| Q \\| _ { \\infty } \\leq D$ . $J$ corresponds to the resolution of the compressor, i.e., number of bits used by the compressor to represent each coordinate of the output vector.\n\nThe compressor first splits the interval $[ 0 , D ]$ into $2 ^ { J } - 1$ intervals of equal length where $0 = d _ { 1 } <$ $d _ { 2 } , \\cdot \\cdot \\cdot < d _ { 2 ^ { J } } = D$ correspond to end points of the intervals. Each coordinate of $Q$ is then separately quantized as follows. The value of the $n ^ { \\mathrm { t h } }$ coordinate, ${ \\mathcal { C } } ( Q ) [ n ]$ , is set to be $d _ { j _ { n } - 1 }$ with probability djjn −djQ[n]1 and to djn with the remaining probability, where jn := min{j : dj < Q[i] ≤ dj+1}. It is straightforward to note that each coordinate of ${ \\mathcal { C } } ( Q )$ can be represented using $J$ bits.\n\n# 4.1.3 Setting the parameters\n\nThe desired convergence of the iterates $\\{ Q ^ { ( k ) } \\}$ is obtained by carefully choosing the parameters of the sub-routine REFINEESTIMATE and the compression operator $\\mathcal { C }$ . For all epochs $k \\geq 1$ , we set the number of iterations $I$ and the batch size $B$ of REFINEESTIMATE and the number of bits $J$ of the compressor $\\mathcal { C }$ to be $\\lceil \\frac { 2 } { \\eta ( 1 - \\gamma ) } \\rceil$ $\\begin{array} { r } { \\frac { 2 } { - \\gamma ) } ] , \\lceil \\frac { 2 } { M } \\big ( \\frac { 1 2 \\gamma } { ( 1 - \\gamma ) } \\big ) ^ { 2 } \\log ( \\frac { 8 K I | \\mathcal { S } | | \\mathcal { A } | } { \\delta } ) \\rceil } \\end{array}$ and $\\begin{array} { r } { \\lceil \\log _ { 2 } ( \\frac { 7 0 } { \\eta ( 1 - \\gamma ) } \\sqrt { \\frac { 2 } { M } \\log ( \\frac { 8 K I | \\mathcal { S } | | \\mathcal { A } | } { \\delta } ) } ) \\rceil } \\end{array}$ respectively. The total number of epochs is set to $\\begin{array} { r } { K = \\lceil \\frac { 1 } { 2 } \\log _ { 2 } ( \\frac { 1 } { 1 - \\gamma } ) \\rceil + \\lceil \\frac { 1 } { 2 } \\log _ { 2 } ( \\frac { 1 } { ( 1 - \\gamma ) \\varepsilon ^ { 2 } } ) \\rceil } \\end{array}$ . The recentering sample sizes $L _ { k }$ and bounds $D _ { k }$ are set to be the following functions of epoch index $k$ :\n\n$$\nL _ { k } : = \\frac { 1 9 6 0 0 } { ( 1 - \\gamma ) ^ { 2 } } \\log ( \\frac { 8 K I | S | | A | } { \\delta } ) \\cdot \\{ \\begin{array} { l l } { 4 ^ { k } \\quad } & { \\mathrm { i f ~ } k \\leq K _ { 0 } } \\\\ { 4 ^ { k - K _ { 0 } } \\quad } & { \\mathrm { i f ~ } k > K _ { 0 } } \\end{array} ; \\quad D _ { k } : = 1 6 \\cdot \\frac { 2 ^ { - k } } { 1 - \\gamma } ,\n$$\n\nwhere $\\begin{array} { r } { K _ { 0 } = \\lceil \\frac { 1 } { 2 } \\log _ { 2 } ( \\frac { 1 } { 1 - \\gamma } ) \\rceil } \\end{array}$ . The piecewise definition of $L _ { k }$ is crucial to obtain the optimal dependence with respect to $\\textstyle { \\frac { 1 } { 1 - { \\gamma } } }$ , similar to the two-step procedure outlined in Wainwright [2019b].\n\n# 4.2 Performance Guarantees\n\nThe following theorem characterizes the sample and communication complexity of Fed-DVR-Q.\n\nTheorem 2. Consider any $\\delta \\in ( 0 , 1 )$ and $\\varepsilon \\in ( 0 , 1 ]$ . Under the federated learning setup described in Section 2.1, the sample and communication complexities of the Fed-DVR-Q algorithm, when run with the choice of parameters described in Sec. 4.1.3 and a learning rate $\\eta \\in ( 0 , 1 )$ , satisfy the following relations for some universal constant $C _ { 1 } > 0$ :\n\n$$\n\\begin{array} { r l } & { \\mathsf { S C } ( \\mathsf { F e q - D V R - Q } ; \\varepsilon , M , \\delta ) \\le \\frac { C _ { 1 } } { \\eta M ( 1 - \\gamma ) ^ { 3 } \\varepsilon ^ { 2 } } \\log _ { 2 } \\left( \\frac { 1 } { ( 1 - \\gamma ) \\varepsilon } \\right) \\log \\left( \\frac { 8 K I | \\mathcal { S } | | A | } { \\delta } \\right) , } \\\\ & { \\mathsf { S C _ { \\mathrm { r o u r d } } } ( \\mathsf { F e q - D V R - Q } ; \\varepsilon , \\delta ) \\le \\frac { 1 6 } { \\eta ( 1 - \\gamma ) } \\log _ { 2 } \\left( \\frac { 1 } { ( 1 - \\gamma ) \\varepsilon } \\right) , } \\\\ & { \\mathsf { C C _ { \\mathrm { b i f } } } ( \\mathsf { F e q - D V R - Q } ; \\varepsilon , \\delta ) \\le \\frac { 3 2 | \\mathcal { S } | \\mathcal { A } | } { \\eta ( 1 - \\gamma ) } \\log _ { 2 } \\left( \\frac { 1 } { ( 1 - \\gamma ) \\varepsilon } \\right) \\log _ { 2 } \\left( \\frac { 7 0 } { \\eta ( 1 - \\gamma ) } \\sqrt { \\frac { 2 } { M } \\log \\left( \\frac { 8 K I | \\mathcal { S } | | A | } { \\delta } \\right) } \\right) } \\end{array}\n$$\n\nA proof of Theorem 2 can be found in Appendix C. A few implications of the theorem are in order.\n\nOptimal Sample-Communication complexity trade-off. As shown by the above theorem, FedDVR-Q offers a linear speed up in the sample complexity with respect to the number of agents while simultaneously achieving the same order of communication complexity as dictated by the lower bound derived in Theorem 1, both in terms of frequency and bit level complexity. Moreover, Fed-DVR-Q is the first Federated Q-Learning algorithm that achieves a sample complexity with optimal dependence on all the salient parameters, i.e., $| S | , | A |$ and $\\scriptstyle { \\frac { 1 } { 1 - \\gamma } }$ , in addition to linear speedup w.r.t. to number of agents and thereby bridges the existing gap between upper and lower bounds on sample complexity for Federated Q-learning. Thus, Theorem 1 and 2 together provide a characterization of optimal operating point of the sample-communication complexity trade-off in Federated Q-learning.\n\nRole of Minibatching. The commonly adopted approach in intermittent communication algorithm is to use a local update scheme that takes multiple small (i.e., $B = \\mathcal { O } ( 1 ) )$ , noisy updates between communication rounds, as evident from the algorithm design in Khodadadian et al. [2022], Woo et al. [2023] and even numerous FL algorithms for stochastic optimization McMahan et al. [2017], Haddadpour et al. [2019], Khaled et al. [2020]. In Fed-DVR-Q, we replace the local update scheme of taking multiple small, noisy updates by a single, large update with smaller variance, obtained by averaging the noisy updates over a minibatch of samples. The use of updates with smaller variance in variance reduced Q-learning yields the algorithm its name. While both the approaches result in similar sample complexity guarantees, the local update scheme requires more frequent averaging across clients to ensure that the bias of the estimate, also commonly referred to as “client drift”, is not too large. On the other hand, the minibatching approach does not encounter the problem of bias accumulation from local updates and hence can afford more infrequent averaging allowing Fed-DVR-Q to achieve optimal order of communication complexity.\n\nCompression. Fed-DVR-Q is the first algorithm in Federated Q-Learning to analyze and establish communication complexity at the bit level. All existing studies on Federated RL focus only on the frequency of communication and assume transmission of real numbers with infinite bit precision. On the other hand, the our analysis provides a more holistic view point of communication complexity and provides bounds at the bit level, which is of great practical significance. While some recent other studies [Wang et al., 2023] also consider quantization in Federated RL, their objective is to understand the impact of message size on convergence with no constraint on the frequency of communication, unlike the holistic viewpoint adopted in this work.\n\n# 5 Conclusion and Future Directions\n\nWe presented a complete characterization of the sample-communication trade-off for Federated Q-learning algorithms with intermittent communication. We showed that no Federated Q-learning algorithm with intermittent communication can achieve a linear speedup with respect to the number of agents if its number of communication rounds are sublinear in $\\frac { 1 ^ { \\bullet } } { 1 - \\gamma }$ . We also proposed a new Federated Q-learning algorithm called Fed-DVR-Q that uses variance reduction along with minibatching to achieve optimal-order sample and communication complexities. In particular, we showed that FedDVR-Q has a sample complexity of $\\begin{array} { r } { \\tilde { \\mathcal { O } } \\big ( \\frac { | \\boldsymbol { S } | | \\boldsymbol { A } | } { M ( 1 - \\gamma ) ^ { 3 } \\varepsilon ^ { 2 } } \\big ) } \\end{array}$ , which is order-optimal in all salient problem parameters, and a communication complexity of $\\begin{array} { r } { \\tilde { \\mathcal { O } } ( \\frac { 1 } { 1 - \\gamma } ) } \\end{array}$ rounds and $\\tilde { \\mathcal { O } } ( \\frac { | \\boldsymbol { S } | | \\boldsymbol { A } | } { 1 - \\gamma } )$ bits.\n\nThe results in this work raise several interesting questions that are worth exploring. While we focus on the tabular setting in this work, it is of great interest to investigate to the trade-off in other settings where we use function approximation to model the $Q ^ { \\star }$ and $V ^ { \\star }$ functions. Moreover, it is interesting to explore the trade-off in the finite horizon setting, where there is no discount factor. Furthermore, it is also worthwhile to explore if the communication complexity can be further reduced by going beyond the class of intermittent communication algorithms.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文研究了联邦Q学习（Federated Q-learning）中的样本与通信复杂度之间的权衡问题。具体来说，当M个智能体协作学习一个具有有限状态和动作空间的未知无限时域马尔可夫决策过程（MDP）的最优Q函数时，如何平衡样本复杂度和通信复杂度。\\n> *   该问题的重要性在于，联邦学习中的通信成本是一个关键瓶颈，尤其是在分布式强化学习（RL）应用中。通过优化这一权衡，可以在不牺牲学习效率的情况下显著降低通信开销，从而在自动驾驶、游戏、推荐系统等实际应用中实现更高效的分布式RL。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文首先建立了联邦Q学习的通信复杂度下界，证明任何能够实现样本复杂度线性加速的算法必须至少支付Ω(1/(1-γ))的通信成本。随后，论文提出了一种名为Fed-DVR-Q的新算法，首次同时实现了最优阶的样本复杂度和通信复杂度。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **通信复杂度下界：** 证明了任何联邦Q学习算法在间歇通信设置下，若希望实现样本复杂度的线性加速，必须至少进行Ω(1/(1-γ))轮通信，且每个智能体需要传输Ω(|S||A|/(1-γ))比特的信息。\\n> *   **Fed-DVR-Q算法：** 提出了一种新的联邦Q学习算法，其样本复杂度为Õ(|S||A|/(M(1-γ)³ε²))，通信复杂度为Õ(1/(1-γ))轮和Õ(|S||A|/(1-γ))比特，首次同时达到了最优阶的样本和通信复杂度。\\n> *   **完整权衡刻画：** 通过理论分析和实验验证，论文首次完整刻画了联邦Q学习中样本-通信复杂度的最优权衡边界。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   Fed-DVR-Q的核心思想是通过方差减少（Variance Reduction）和小批量（Minibatching）技术，在减少局部更新偏差的同时控制通信频率。其设计哲学在于通过更高效的局部更新和更稀疏的通信，实现样本复杂度和通信复杂度的双重优化。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有联邦Q学习算法（如Fed-SynQ）要么通信成本高（Õ(M/(1-γ))），要么样本复杂度次优（如Õ(1/(1-γ)⁵ε²)）。\\n> *   **本文的改进：** Fed-DVR-Q通过以下创新解决了这些问题：\\n>     1.  引入方差减少技术，减少局部更新的偏差；\\n>     2.  使用小批量更新替代频繁的局部更新，降低通信频率；\\n>     3.  采用量化压缩技术，进一步降低比特级通信成本。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.   **初始化：** 所有智能体初始Q函数估计为0。\\n> 2.   **分阶段更新：** 算法运行K个阶段，每个阶段k的目标是将当前Q函数估计Q^(k-1)的次优性差距减半。\\n> 3.   **方差减少更新：** 在每个阶段k，智能体通过REFINEESTIMATE子程序更新Q函数：\\n>      - 每个智能体使用L_k个样本构建对TQ^(k-1)的近似估计；\\n>      - 通过压缩通信将估计值传输到服务器并聚合；\\n>      - 使用小批量样本进行I次方差减少的Q学习更新。\\n> 4.   **压缩通信：** 每次通信时，使用基于随机量化的压缩算子C(·;D,J)减少传输比特数。\\n> 5.   **输出：** 经过K个阶段后，输出最终的Q函数估计Q^(K)。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   Q-learning [Li et al., 2023]（单智能体基准）\\n> *   Variance Reduced Q-learning [Wainwright, 2019b]（单智能体方差减少基准）\\n> *   Fed-SynQ [Woo et al., 2023]（现有联邦Q学习算法）\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在样本复杂度上：** Fed-DVR-Q在同步设置下达到了Õ(|S||A|/(M(1-γ)³ε²))，显著优于基线模型Fed-SynQ (Õ(|S||A|/(M(1-γ)⁵ε²))) 和单智能体Q-learning (Õ(|S||A|/(1-γ)⁴ε²))。与表现最佳的基线Fed-SynQ相比，提升了(1-γ)²倍的样本效率。\\n>   \\n> *   **在通信复杂度上：** Fed-DVR-Q的通信轮数为Õ(1/(1-γ))，远低于Fed-SynQ (Õ(M/(1-γ)))，同时比特级通信成本为Õ(|S||A|/(1-γ))，是首个分析比特级成本的联邦Q学习算法。\\n>   \\n> *   **在综合性能上：** Fed-DVR-Q是首个达到理论下界的算法，同时实现了最优阶的样本复杂度和通信复杂度，而其他算法只能在两者之间做出权衡。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   联邦学习 (Federated Learning, FL)\\n*   Q学习 (Q-learning, N/A)\\n*   样本复杂度 (Sample Complexity, N/A)\\n*   通信复杂度 (Communication Complexity, N/A)\\n*   马尔可夫决策过程 (Markov Decision Process, MDP)\\n*   方差减少 (Variance Reduction, N/A)\\n*   间歇通信 (Intermittent Communication, N/A)\\n*   分布式强化学习 (Distributed Reinforcement Learning, N/A)\"\n}\n```"
}