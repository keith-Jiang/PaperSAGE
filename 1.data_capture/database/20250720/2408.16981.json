{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.16981",
    "link": "https://arxiv.org/abs/2408.16981",
    "pdf_link": "https://arxiv.org/pdf/2408.16981.pdf",
    "title": "The Sample-Communication Complexity Trade-off in Federated Q-Learning",
    "authors": [
        "Sudeep Salgia",
        "Yuejie Chi"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-08-30",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 3,
    "influential_citation_count": 0,
    "institutions": [
        "Carnegie Mellon University"
    ],
    "paper_content": "# The Sample-Communication Complexity Trade-off in Federated Q-Learning\n\nSudeep Salgia Carnegie Mellon University ssalgia@andrew.cmu.edu\n\nYuejie Chi Carnegie Mellon University yuejiechi@cmu.edu\n\n# Abstract\n\nWe consider the problem of Federated Q-learning, where $M$ agents aim to collaboratively learn the optimal Q-function of an unknown infinite horizon Markov Decision Process with finite state and action spaces. We investigate the trade-off between sample and communication complexity for the widely used class of intermittent communication algorithms. We first establish the converse result, where we show that any Federated Q-learning that offers a linear speedup with respect to number of agents in sample complexity needs to incur a communication cost of at least $\\Omega \\big ( \\frac { \\mathrm { ~ \\ j ~ } } { 1 - \\gamma } \\big )$ , where $\\gamma$ is the discount factor. We also propose a new Federated Q-learning algorithm, called Fed-DVR-Q, which is the first Federated Q-learning algorithm to simultaneously achieve order-optimal sample and communication complexities. Thus, together these results provide a complete characterization of the sample-communication complexity trade-off in Federated Q-learning.\n\n# 1 Introduction\n\nReinforcement Learning (RL) [Sutton and Barton, 2018] refers to an online sequential decision making paradigm where the learning agent aims to learn an optimal policy, i.e., a policy that maximizes the long-term reward, through repeated interactions with an unknown environment. RL finds applications across a diverse array of fields including, but not limited to, autonomous driving, games, recommendation systems, robotics and Internet of Things (IoT) [Kober et al., 2013, Yurtsever et al., 2020, Silver et al., 2016, Lim et al., 2020].\n\nThe primary hurdle in RL applications is often the high-dimensional nature of the decision space that necessitates the learning agent to have to access to an enormous amount of data in order to have any hope of learning the optimal policy. Moreover, the sequential collection of such an enormous amount of data through a single agent is extremely time-consuming and often infeasible in practice. Consequently, practical implementations of RL involve deploying multiple agents to collect data in parallel. This decentralized approach to data collection has fueled the design and development of distributed or federated RL algorithms that can collaboratively learn the optimal policy without actually transferring the collected data to a centralized server. Such a federated approach to RL, which does not require the transfer of local data, is gaining interest due to lower bandwidth requirements and lower security and privacy risks. In this work, we focus on federated variants of Q-learning algorithms where the agents collaborate to directly learn the optimal Q-function without forming an estimate of the underlying unknown environment.\n\nA particularly important aspect of designing Federated RL algorithms, including Federated Q-learning algorithms, is to address the natural tension between sample and communication complexity. At one end of the spectrum lies the na√Øve approach of running a centralized algorithm with optimal sample complexity after transferring and combining all the collected data at a central facility/server. Such an approach trivially achieves the optimal sample complexity while suffering from a very high and infeasible communication complexity. On the other hand, several recently proposed algorithms [Khodadadian et al., 2022, Woo et al., 2023] operate in more practical regimes, offering significantly lower communication complexities as compared to the na√Øve approach at the cost of sub-optimal sample complexities. These results suggest the existence of underlying trade-off between sample and communication complexities of Federated RL algorithms. The primary goal of this work is to better understand this trade-off in context of Federated Q-learning by investigating these following fundamental questions:\n\n‚Ä¢ Fundamental limit of communication: What is the minimum amount of communication required by a federated Q-learning algorithm to achieve any statistical benefit of collaboration? ‚Ä¢ Optimal algorithm design: How does one design a federated $\\boldsymbol { Q }$ -Learning algorithm that simultaneously offers optimal order sample and communication complexity guarantees i.e., operates on the optimal frontier of sample-communication complexity trade-off?\n\n# 1.1 Main Results\n\nWe consider a setup where $M$ distributed agents collaborate to learn the optimal Q-function of an infinite horizon Markov Decision Process which is defined over a finite state space $s$ and a finite action set $\\mathcal { A }$ , and has a discount factor of $\\gamma \\in ( 0 , 1 )$ . We consider a commonly considered setup in federated learning called the intermittent communication setting, where the clients intermittently share information among themselves with the help of a central server. In this work, we provide a complete characterization of the trade-off between sample and communication complexity under the aforementioned setting by providing answers to both the questions. The main result of this work is twofold and is summarized below.\n\n‚Ä¢ Fundamental bounds on communication complexity of Federated $\\boldsymbol { Q }$ -learning: We establish lower bounds on the communication complexity of Federated Q-learning, both in terms of number of communication rounds and the overall number of bits that need to be transmitted in order to achieve any speed up in convergence with respect to the number of agents. Specifically, we show that in order for an intermittent communication algorithm to obtain any benefit of collaboration, i.e., any order of speed up w.r.t. the number of agents, the number of communication rounds must be least $\\Omega \\big ( \\frac { \\mathbf { \\lambda } ^ { \\star } } { ( 1 - \\gamma ) \\log ^ { 2 } N } \\big )$ and the number of bits sent by each agent to the server must be least (1‚àí|Œ≥S)| |loAg|2 N ), where N denotes the number of samples taken by the algorithm for each state-action pair. ‚Ä¢ Achieving the optimal sample-communication complexity trade-off : We propose a new Federated Q-Learning algorithm called Federated Doubly Variance Reduced Q Learning, Fed-DVR-Q for short, that simultaneously achieves optimal order of sample complexity and the minimal order of communication as dictated by the lower bound. We show that Fed-DVR-Q learns an $\\varepsilon$ -optimal Q-function in the $\\ell _ { \\infty }$ sense with $\\begin{array} { r } { \\tilde { \\mathcal { O } } \\left( \\frac { | \\mathcal { S } | | \\mathcal { A } | } { M \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\right) } \\end{array}$ i.i.d. samples from the generative model at each agent while incurring a total communication cost of $\\begin{array} { r } { \\tilde { \\mathcal O } \\left( \\frac { | { \\mathcal S } | | { \\mathcal A } | } { \\left( 1 - \\gamma \\right) } \\right) } \\end{array}$ bits per agent across $\\begin{array} { r } { \\tilde { \\mathcal O } \\left( \\frac { 1 } { ( 1 - \\gamma ) } \\right) } \\end{array}$ rounds of communication. Thus, Fed-DVR-Q not only improves upon both the sample and communication complexities of existing algorithms, but also is the first algorithm to achieve both order-optimal sample and communication complexities (See Table 1 for a comparison).\n\n# 1.2 Related Work\n\nSingle agent Q-Learning. Q-Learning has been extensively studied in the single-agent setting in terms of both its asymptotic convergence [Jaakkola et al., 1993, Tsitsiklis, 1994, Szepesv√°ri, 1997, Borkar and Meyn, 2000] and its finite-time sample complexity in both synchronous [Even-Dar and Mansour, 2004, Beck and Srikant, 2012, Wainwright, 2019a, Chen et al., 2020, Li et al., 2023] and asynchronous settings [Chen et al., 2021b, Li et al., 2023, 2021, Qu and Wierman, 2020].\n\nDistributed RL. There has also been a considerable effort towards developing distributed and federated RL algorithms. The distributed variants of the classical TD learning algorithm have been investigated in a series of studies [Chen et al., 2021c, Doan et al., 2019, 2021, Sun et al., 2020, Wai, 2020, Wang et al., 2020, Zeng et al., 2021b]. The impact of environmental heterogeneity in federated TD learning was studied in Wang et al. [2023]. A distributed version of actor-critic algorithms was studied by Shen et al. [2023] where the authors established convergence of their algorithm and demonstrated a linear speed up in the number of agents in their sample complexity bound. Chen et al. [2022] proposed a new distributed actor-critic algorithm which improved the dependence of sample complexity on the error $\\varepsilon$ and incurs a communication cost of $\\bar { \\mathcal { O } } ( \\bar { \\varepsilon } ^ { - 1 } )$ . Chen et al. [2021a] have proposed a communication efficient distributed policy gradient algorithm and have analyzed its convergence and established a communication complexity of $\\mathcal { O } ( 1 / ( M \\varepsilon ) )$ . Xie and Song [2023] adopts a distributed policy optimization perspective, which is different from the Q-learning paradigm considered in this work. Moreover, the algorithm in Xie and Song [2023] obtains a linear communication cost, which is worse than that obtained in our work. Similarly, Zhang et al. [2024] focuses on on-policy learning and incurs a communication cost that depends polynomially on the required error $\\varepsilon$ . Several other studies [Yang et al., 2023, Zeng et al., 2021a, Lan et al., 2024] have also developed and analyzed other distributed/federated variants of the classical natural policy gradient method [Kakade, 2001]. Assran et al. [2019], Espeholt et al. [2018], Mnih et al. [2016] have developed distributed algorithms to train deep RL networks more efficiently.\n\nTable 1: Comparison of sample and communication complexity of various single-agent and Federated Q-learning algorithms for learning an $\\varepsilon$ -optimal Q-function under the synchronous setting. We hide logarithmic factors and burn-in costs for all results for simplicity of presentation. In the above table, $s$ and $\\mathcal { A }$ represent state and action spaces respectively and $\\gamma$ denotes the discount factor. We report the communication complexity only in terms of number of rounds as other algorithms assume transmission of real numbers and hence do not report bit level costs. For the lower bound, Azar et al. [2013] and this work establish the bound for sample and communication complexity respectively.   \n\n<html><body><table><tr><td>Algorithm/Reference</td><td>Number of Agents</td><td>Sample Complexity</td><td>Communication Complexity</td></tr><tr><td>Q-learning [Li et al., 2023]</td><td>1</td><td>[S|A (1-Œ≥)4g¬≤</td><td>N/A</td></tr><tr><td>Variance Reduced Q-learning [Wainwright, 2019b]</td><td>1</td><td>[S||A| (1-Œ≥)¬≤</td><td>N/A</td></tr><tr><td>Fed-SynQ [Woo et al., 2023]</td><td>M</td><td>[S|A M(1-Œ≥)5Œµ¬≤</td><td>M 1-Œ≥</td></tr><tr><td>Fed-DVR-Q (This work)</td><td>M</td><td>[S|A M(1-Œ≥)¬≥Œµ¬≤</td><td>1 1-Œ≥</td></tr><tr><td>Lower bound([Azar etal.,2O13],This work)</td><td>M</td><td>[S||A| M(1-Œ≥)3Œµ¬≤</td><td>1 1-Œ≥</td></tr></table></body></html>\n\nDistributed Q-learning. Federated Q-learning has been explored relatively recently. Khodadadian et al. [2022] proposed and analyzed a federated Q-learning algorithm in the asynchronous setting with a sample complexity of $\\begin{array} { r } { { \\tilde { \\mathcal { O } } } \\left( \\frac { | { \\mathcal { S } } | ^ { 2 } } { M \\mu _ { \\mathrm { m i n } } ^ { 5 } ( 1 - \\gamma ) ^ { 9 } \\varepsilon ^ { 2 } } \\right) } \\end{array}$ , where $\\mu _ { \\mathrm { m i n } }$ is the minimum entry of the stationary state-action occupancy distribution of the sample trajectories over all agents. Jin et al. [2022] study the impact of environmental heterogeneity across clients in Federated Q-learning. They propose an algorithm where the local environments are different at each client but each client knows their local environment. Under this setting, they propose an algorithm that achieves a sample and communication complexity of $\\begin{array} { r } { \\mathcal { O } \\big ( \\frac { 1 } { ( 1 - \\gamma ) ^ { 3 } \\varepsilon } \\big ) } \\end{array}$ and $\\begin{array} { r } { \\dot { \\mathcal { O } } \\big ( \\frac { 1 } { ( 1 - \\gamma ) ^ { 3 } \\varepsilon } \\big ) } \\end{array}$ rounds respectively. Woo et al. [2023] proposed new algorithms with improved analysis for Federated Q-learning under both synchronous and asynchronous settings. Their proposed algorithm achieves a sample complexity and communication complexity of OÀú( M(|1S‚àí||Œ≥A)|5Œµ2 ) and OÀú( M1|S‚àí||Œ≥A| ) real numbers respectively under the synchronous setting and that of $\\begin{array} { r } { \\tilde { \\mathcal { O } } \\big ( \\frac { 1 } { M \\mu _ { \\mathrm { a v g } } ( 1 - \\gamma ) ^ { 5 } \\varepsilon ^ { 2 } } \\big ) } \\end{array}$ and $\\begin{array} { r } { \\tilde { \\mathcal { O } } \\left( \\frac { M | \\mathcal { S } | | \\mathcal { A } | } { 1 - \\gamma } \\right) } \\end{array}$ real numbers respectively under the asynchronous setting. Here, $\\mu _ { \\mathrm { a v g } }$ denotes the minimum entry of the average stationary state-action occupancy distribution of all agents. In a follow up work, Woo et al. [2024] propose a Federated Qlearning for offline RL in finite horizon setting and establish a sample and communication complexity of $\\tilde { \\mathcal { O } } \\big ( \\frac { H ^ { 7 } | S | C _ { \\mathrm { a v g } } } { M \\varepsilon ^ { 2 } } \\big )$ and $\\tilde { \\mathcal { O } } ( H )$ , where $H$ denotes the length of the horizon and $C _ { \\mathrm { a v g } }$ denotes the average single-policy concentrability coefficient of all agents.\n\nAccuracy-Communication Trade-off in Federated Learning. The trade-off between communication complexity and accuracy (equivalently, sample complexity) has been studied in various federated and distributed learning problems, including stochastic approximation algorithms for convex optimization. Duchi et al. [2014], Braverman et al. [2016] establish the celebrated inverse linear relationship between the error and the communication cost the problem of distributed mean estimation. Similar trade-off for distributed stochastic optimization, multi-armed bandits and linear bandits has been studied and established across numerous studies [Woodworth et al., 2018, 2021, Tsitsiklis and Luo, 1987, Shi and Shen, 2021, Salgia and Zhao, 2023].\n\n# 2 Problem Formulation and Preliminaries\n\nIn this section, we provide a brief background of Markov Decision Processes, outline the performance measures for Federated Q-learning algorithms and describe the class of intermittent communication algorithms considered in this work.\n\n# 2.1 Markov Decision Processes\n\nIn this work, we focus on an infinite-horizon Markov Decision Process (MDP), denoted by $\\mathcal { M }$ , over a state space $s$ and an action space $\\mathcal { A }$ and with a discount factor $\\gamma \\in ( 0 , 1 )$ . Both the state and action spaces are assumed to be finite sets. In an MDP, the state $s$ evolves dynamically under the influence of actions based on a probability transition kernel, $P : ( S \\times \\mathcal { A } ) \\times S  [ 0 , 1 ]$ . The entry $P ( s ^ { \\prime } | s , a )$ denotes the probability of moving to state $s ^ { \\prime }$ when an action $a$ is taken in the state $s$ . An MDP is also associated with a deterministic reward function $r : S \\times \\mathcal { A }  [ 0 , 1 ]$ , where $r ( s , a )$ denotes the immediate reward obtained for taking the action $a$ in the state $s$ . Thus, the transition kernel $P$ along with the reward function $r$ completely characterize an MDP. In this work, we consider the synchronous setting, where each agent has access to an independent generative model or simulator from which they can draw independent samples from the unknown underlying distribution $P ( \\cdot | s , a )$ for each state-action pair $( s , a )$ [Kearns and Singh, 1998].\n\nA policy $\\pi : S  \\Delta ( { \\mathcal { A } } )$ is a rule for selecting actions across different states, where $\\Delta ( \\mathcal { A } )$ denotes the simplex over $\\mathcal { A }$ and $\\pi ( a | s )$ denotes the probability of choosing action $a$ in a state $s$ . Each policy $\\pi$ is associated with a state value function and a state-action value function, or the Q-function, denoted by $V ^ { \\pi }$ and $Q ^ { \\pi }$ respectively. $V ^ { \\pi }$ and $Q ^ { \\pi }$ measure the expected discounted cumulative reward attained by $\\pi$ starting from a particular state $s$ and state-action pair $( s , a )$ respectively. Mathematically, $V ^ { \\pi }$ and $Q ^ { \\pi }$ are given as\n\n$$\nV ^ { \\pi } ( s ) : = \\mathbb { E } \\left[ \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } r ( s _ { t } , a _ { t } ) \\bigg | \\ s _ { 0 } = s \\right] ; \\quad Q ^ { \\pi } ( s , a ) : = \\mathbb { E } \\left[ \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } r ( s _ { t } , a _ { t } ) \\bigg | \\ s _ { 0 } = s , a _ { 0 } = a \\right] ,\n$$\n\nwhere $a _ { t } \\sim \\pi ( \\cdot | s _ { t } )$ and $s _ { t + 1 } \\sim P ( \\cdot | s _ { t } , a _ { t } )$ for all $t \\geq 0$ . The expectation is taken w.r.t. the randomness in the trajectory $\\{ s _ { t } , a _ { t } \\} _ { t = 1 } ^ { \\infty }$ . Since the rewards lie in $[ 0 , 1 ]$ , it follows immediately that both the value function and Q-function lie in the range $[ 0 , \\frac { 1 } { 1 - \\gamma } ]$ .\n\nAn optimal policy $\\pi ^ { \\star }$ is a policy that maximizes the value function uniformly over all the states and it has been shown that such an optimal policy $\\pi ^ { \\star }$ always exists [Puterman, 2014]. The optimal value and Q-functions are those corresponding to that of an optimal policy $\\pi ^ { \\star }$ are denoted as $V ^ { \\star } : = V ^ { \\pi ^ { \\star } }$ and $Q ^ { \\star } : = Q ^ { \\pi ^ { \\star } }$ respectively. The optimal Q-function, $Q ^ { \\star }$ , is also the unique fixed point of the Bellman operator $\\mathcal { T } : \\mathcal { S } \\times \\mathcal { A }  \\mathcal { S } \\times \\mathcal { A }$ , given by\n\n$$\n( \\mathcal T Q ) ( s , a ) = r ( s , a ) + \\gamma \\cdot \\mathbb { E } _ { s ^ { \\prime } \\sim P ( \\cdot \\mid s , a ) } \\left[ \\operatorname* { m a x } _ { a ^ { \\prime } \\in \\mathcal A } Q ( s ^ { \\prime } , a ^ { \\prime } ) \\right] .\n$$\n\nQ-learning [Watkins and Dayan, 1992] aims to learn the optimal policy by first learning $Q ^ { \\star }$ as the solution to the fixed point equation $\\mathcal { T } Q = Q$ and then obtain a deterministic optimal policy via the maximization $\\pi ^ { \\star } ( s ) = \\arg \\operatorname* { m a x } _ { a } Q ^ { \\star } ( s , a )$ .\n\nLet $Z \\in { \\mathcal { S } } ^ { | S | | A | }$ be a random vector whose $( s , a ) ^ { \\mathrm { t h } }$ coordinate is drawn from the distribution $P ( \\cdot | s , a )$ independently of all other coordinates. We define the random operator $\\mathcal { T } _ { Z } : ( S \\times \\mathcal { A } )  ( S \\times \\mathcal { A } )$ as\n\n$$\n( { \\mathcal { T } } _ { Z } Q ) ( s , a ) = r ( s , a ) + { \\gamma } V ( Z ( s , a ) ) ,\n$$\n\nwhere $V ( s ^ { \\prime } ) = \\mathrm { m a x } _ { a ^ { \\prime } \\in { \\mathcal { A } } } Q ( s ^ { \\prime } , a ^ { \\prime } )$ . The operator $\\mathcal { T } _ { Z }$ can be interpreted as the sample Bellman Operator, where we have the relation $\\mathcal { T } Q = \\mathrm { \\bar { E } } _ { Z } [ \\mathcal { T } _ { Z } Q ]$ for all Q-functions.\n\nLastly, the federated learning setup considered in this work consists of $M$ agents, where all the agents face a common, unknown MDP, i.e., the transition kernel and the reward functions are the same across agents, which is popularly known as the homogeneous setting. For a given value of $\\varepsilon \\in ( 0 , \\frac { 1 } { 1 - \\gamma } )$ , the objective of agents is to collaboratively learn an $\\varepsilon$ -optimal estimate (in the $\\ell _ { \\infty }$ sense) of the optimal Q-function of the unknown MDP.\n\n# 2.2 Performance Measures\n\nWe measure the performance of a Federated Q-learning algorithm $\\mathcal { A }$ using two metrics ‚Äî sample complexity and communication complexity. For a given MDP $\\mathcal { M }$ , let $\\widehat { Q } _ { \\mathcal { M } } ( \\mathcal { A } , N , M )$ denote the estimate of $Q _ { \\mathcal { M } } ^ { \\star }$ , the optimal Q-function of the MDP $\\mathcal { M }$ , returned by anbalgorithm $\\mathcal { A }$ , when given access to $N$ i.i.d. samples from the generative model for each $( s , a )$ pair at all the $M$ agents. The minimax error rate of the algorithm $\\mathcal { A }$ , denoted by $\\mathsf { E R } ( \\mathcal { A } ; N , \\dot { M } )$ , is defined as\n\n$$\n\\mathsf { E R } ( \\mathcal { A } ; N , M ) : = \\operatorname* { s u p } _ { \\mathcal { M } = ( P , r ) } \\mathbb { E } \\left[ \\| \\widehat { Q } _ { \\mathcal { M } } ( \\mathcal { A } , N , M ) - Q _ { \\mathcal { M } } ^ { \\star } \\| _ { \\infty } \\right] ,\n$$\n\nwhere the expectation is taken over the samples and any randomness in the algorithm. Given a value of $\\varepsilon > 0$ , the sample complexity of $\\mathcal { A }$ , denoted by $\\mathsf { S C } ( \\mathcal { A } ; \\varepsilon , M )$ is given as\n\n$$\n\\mathsf { S C } ( \\mathcal { A } ; \\varepsilon , M ) : = | \\mathcal S | | \\mathcal { A } | \\cdot \\operatorname* { m i n } \\{ N \\in \\mathbb N : \\mathsf { E } \\mathsf { R } ( \\mathcal { A } ; N , M ) \\leq \\varepsilon \\} .\n$$\n\nSimilarly, we can also define a high-probability version for any $\\delta \\in ( 0 , 1 )$ as follows:\n\n$$\n\\mathsf { S C } ( \\mathcal { A } ; \\varepsilon , M , \\delta ) : = | \\mathcal { S } | | \\mathcal { A } | \\cdot \\operatorname* { m i n } \\{ N \\in \\mathbb { N } : \\operatorname* { P r } ( \\operatorname* { s u p } _ { M } \\| \\widehat { Q } _ { \\mathcal { M } } ( \\mathcal { A } , N , M ) - Q _ { M } ^ { \\star } \\| _ { \\infty } \\leq \\varepsilon ) \\geq 1 - \\delta \\} .\n$$\n\nWe measure the communication complexity of any federated learning algorithm both in terms of frequency of information exchange and total number of bits uploaded by the agents. For each agent $m$ , let $C _ { \\sf r o u n d } ^ { m } ( \\mathcal { A } ; N )$ and $C _ { \\mathsf { b i t } } ^ { m } ( \\mathcal { A } ; \\bar { N } )$ respectively denote the number of times agent $m$ sends a message to the server and the total number of bits uploaded by agent $m$ to the server when an algorithm $\\mathcal { A }$ is run with $N$ i.i.d. samples from the generative model for each $( s , a )$ pair at each agent. The communication complexity of $\\mathcal { A }$ , when measured in terms of frequency of communication and total number of bits exchanged, is given by\n\n$$\n\\mathsf { C C } _ { \\mathsf { r o u n d } } ( \\mathcal { A } ; N ) : = \\frac { 1 } { M } \\sum _ { m = 1 } ^ { M } C _ { \\mathsf { r o u n d } } ^ { m } ( \\mathcal { A } ; N ) ; \\quad \\mathsf { C C } _ { \\mathsf { b i t } } ( \\mathcal { A } ; N ) : = \\frac { 1 } { M } \\sum _ { m = 1 } ^ { M } C _ { \\mathsf { b i t } } ^ { m } ( \\mathcal { A } ; N ) ,\n$$\n\nrespectively. Similarly, for a given value of $\\begin{array} { r } { \\varepsilon \\in ( 0 , \\frac { 1 } { 1 - \\gamma } ) } \\end{array}$ , we can also define $\\mathsf { C C } _ { \\mathsf { r o u n d } } ( \\mathcal { A } ; \\varepsilon )$ and $\\mathsf { C C } _ { \\mathsf { b i t } } ( \\mathcal { A } ; \\varepsilon )$ based on when $\\mathcal { A }$ is run to guarantee a minimax error of at most $\\varepsilon$ .\n\n# 2.3 Intermittent Communication Algorithms\n\nIn this work, we consider a popular class of federated learning algorithms referred to as algorithms with intermittent communication. The intermittent communication setting provides a natural framework to extend single agent Qlearning algorithms to the distributed setting. As the name suggests, under this setting, the agents intermittently communicate with each other, sharing their updated beliefs about $Q ^ { \\star }$ . Between two communication rounds, each agent updates their belief about $Q ^ { \\star }$ using stochastic fixed point iteration based on the locally available data, similar to a single agent setup. Such intermittent communication algorithms\n\n<html><body><table><tr><td>Algorithm1:A generic algorithm</td></tr><tr><td>1: Input : T,R,{nt}t=1,C= {tr}=1,B</td></tr><tr><td>2:Set Qm ‚ÜêO for all agents m</td></tr><tr><td>3:for t=1,2,...,Tdo</td></tr><tr><td>45</td></tr><tr><td>fo1</td></tr><tr><td>6: Compute Qm according to Eqn. 8</td></tr><tr><td>7: end for</td></tr><tr><td>8:end for</td></tr><tr><td>9: return QT</td></tr></table></body></html>\n\nhave been extensively studied and used to establish lower bounds on communication complexity of distributed stochastic convex optimization [Woodworth et al., 2018, 2021].\n\nA generic Federated Q-learning algorithm with intermittent communication is outlined in Algorithm 1. It is characterized by the following five parameters: (i) total number of updates $T$ ; (ii) the number of communication rounds $R$ ; (iii) a step size schedule $\\{ \\eta _ { t } \\} _ { t = 1 } ^ { T }$ ; (iv) a communication schedule $\\{ t _ { r } \\} _ { r = 1 } ^ { R }$ ; (v) batch size $B$ . During the $t ^ { \\mathrm { { t h } } }$ iteration, each agent $m$ computes $\\{ \\widehat { T } _ { Z _ { b } } ( Q _ { t - 1 } ^ { m } ) \\} _ { b = 1 } ^ { B }$ , a minibatch of sample Bellman operators at the current estimate $Q _ { t - 1 } ^ { m }$ , using $B$ sambples from the generative model for each $( s , a )$ pair, and obtains an intermediate local estimate using the Q-learning update as follows:\n\n$$\nQ _ { t - \\frac { 1 } { 2 } } ^ { m } = ( 1 - \\eta _ { t } ) Q _ { t - 1 } ^ { m } + \\frac { \\eta _ { t } } { B } \\sum _ { b = 1 } ^ { B } \\mathcal { T } _ { Z _ { b } } ( Q _ { t - 1 } ^ { m } ) .\n$$\n\nHere $\\eta _ { t } \\in ( 0 , 1 ]$ is the step-size chosen corresponding to the $t ^ { \\mathrm { { t h } } }$ time step. The intermediate estimates are averaged based on a communication schedule $\\boldsymbol { \\mathcal { C } } = \\{ t _ { r } \\} _ { r = 1 } ^ { R }$ consisting of $R$ rounds, i.e.,\n\n$$\n\\begin{array} { r } { Q _ { t } ^ { m } = \\left\\{ \\begin{array} { l l } { \\frac { 1 } { M } \\sum _ { j = 1 } ^ { M } Q _ { t - \\frac { 1 } { 2 } } ^ { j } } & { \\mathrm { ~ i f ~ } t \\in \\mathcal { C } , } \\\\ { Q _ { t - \\frac { 1 } { 2 } } ^ { m } } & { \\mathrm { ~ o t h e r w i s e } . } \\end{array} \\right. } \\end{array}\n$$\n\nIn the above equation, the averaging step can also be replaced with any distributed mean estimation routine that includes compression to control the bit level costs. Without loss of generality, we assume that $Q _ { 0 } ^ { m } = 0$ for all agents $m$ and $t _ { R } = T$ , i.e., the last iterates are always averaged. It is straightforward to note that the number of samples taken by an intermittent communication algorithm is $B T$ , i.e, $N = B T$ and the communication complexity ${ \\mathsf { C C } } _ { \\mathsf { r o u n d } } = R$ .\n\n# 3 Lower Bound\n\nIn this section, we investigate the first of the two questions regarding the lower bound on communication complexity. The following theorem establishes a lower bound on the communication complexity of a Federated Q-learning algorithm with intermittent communication.\n\nTheorem 1. Assume that $\\gamma \\in [ 5 / 6 , 1 )$ and the state and action spaces satisfy $| { \\cal S } | \\geq 4$ and $| { \\cal A } | \\geq$ 2. Let $\\mathcal { A }$ be a Federated $\\boldsymbol { Q }$ -learning algorithm with intermittent communication that is run for $T \\geq \\operatorname* { m a x } \\{ 1 6 , \\frac { 1 } { 1 - \\gamma } \\}$ steps with a step size schedule of either $\\begin{array} { r } { \\eta _ { t } : = \\frac { 1 } { 1 + c _ { \\eta } ( 1 - \\gamma ) t } } \\end{array}$ or $\\eta _ { t } : = \\eta$ for all $1 \\leq t \\leq T$ . If\n\n$$\nR = C C _ { r o u n d } ( \\mathcal { A } ; N ) \\leq \\frac { c _ { 0 } } { ( 1 - \\gamma ) \\log ^ { 2 } N } ; \\ o r \\ C C _ { b i t } ( \\mathcal { A } ; N ) \\leq \\frac { c _ { 1 } | \\mathcal { S } | | \\mathcal { A } | } { ( 1 - \\gamma ) \\log ^ { 2 } N }\n$$\n\nfor some universal constants $c _ { 0 } , c _ { 1 } > 0$ then, for all choices of communication schedule, batch size $B$ , $c _ { \\eta } > 0$ and $\\eta \\in ( 0 , 1 )$ , the minimax error of $\\mathcal { A }$ satisfies\n\n$$\nE R ( \\mathcal { A } ; N , M ) \\geq \\frac { C _ { \\gamma } } { \\log ^ { 3 } N \\sqrt { N } } ,\n$$\n\nfor all $M \\geq 2$ and $N = B T$ . Here $C _ { \\gamma } > 0$ is a constant that depends only on $\\gamma$ .\n\nThe above theorem states that in order for an intermittent communication algorithm to obtain any benefit of collaboration, i.e., for the error rate $\\mathsf { E R } ( \\mathcal { A } ; N , M )$ to decrease w.r.t. number of agents, the number of communication rounds must be least 1 Œ≥)1log2 N ). This implies that any Federated Q-learning algorithm that offers order optimal sample complexity, and thereby also a linear speed up with respect to the number of agents, must have at least ( (1 Œ≥)1log2 N ) rounds of communication faonrdtthreanssamiptl $\\begin{array} { r } { \\Omega \\big ( \\frac { | \\boldsymbol { \\mathcal { S } } | | \\boldsymbol { \\mathcal { A } } | } { ( 1 - \\gamma ) \\log ^ { 2 } N } \\big ) } \\end{array}$ nbitrsaodfeoifnf oinrmFaetdieornatpedr aQg-lenatr.nTinhgi.s cWhearwacotuelrdizleiksethtoe cpoinvterosuet rthelattiounr lower bound extends to the asynchronous setting as the assumption of i.i.d. noise corresponding to a generative model is a special case of Markovian noise observed in asynchronous setting.\n\nThe lower bound on the communication complexity of Federated Q-learning is a consequence of the bias-variance trade-off that governs the convergence of the algorithm. While a careful choice of step-sizes alone is sufficient to balance this trade-off in the centralized setting, the choice of communication schedule also plays an important role in balancing this trade-off in the federated setting. The local steps between two communication rounds induce a positive estimation bias that depends on the standard deviation of the iterates and is a well-documented issue of ‚Äúover-estimation‚Äù in Q-learning [Hasselt, 2010]. Since such a bias is driven by local updates, it does not reflect any benefit of collaboration. During a communication round, the averaging of iterates across agents allows the algorithm an opportunity to counter this bias by reducing the effective variance of the updates through averaging. In our analysis, we show that if the communication is infrequent, the local bias becomes the dominant term and averaging of iterates is insufficient to counter the impact of the positive bias induced by the local steps. As a result, we do not observe any statistical gains when the communication is infrequent. The analysis is inspired the analysis of Q-learning by Li et al. [2023] and is based on analyzing the convergence of an intermittent communication algorithm on a specifically chosen ‚Äúhard‚Äù instance of MDP. Please refer to Appendix B for a detailed proof.\n\nRemark 1 (Communication complexity of policy evaluation). Several recent studies [Liu and Olshevsky, 2023, Tian et al., 2024] established that a single round of communication is sufficient to achieve linear speedup of TD learning for policy evaluation, which do not contradict with our results focusing on Q-learning for policy learning. The latter is more involved due to the nonlinearity of the Bellman optimality operator. Specifically, if the operator whose fixed point is to be found is linear in the decision variable (e.g., the value function in TD learning) then the fixed point update only induces a variance term corresponding to the noise. However, if the operator is non-linear, then in addition to the variance term, we also obtain a bias term in the fixed point update. While the variance term can be controlled with one-shot averaging, more frequent communication is necessary to ensure that the bias term is small enough.\n\nRemark 2 (Extension to asynchronous Q-learning). We would like to point out that our lower bound extends to the asynchronous setting [Li et al., 2023] as the assumption of i.i.d. noise corresponding to a generative model is a special case of Markovian noise observed in the asynchronous setting.\n\n# 4 The Fed-DVR-Q algorithm\n\nHaving characterized the lower bound on the communication complexity of Federated Q-learning, we explore our second question of interest ‚Äî designing a federated Q-learning algorithm that achieves this lower bound while simultaneously offering an optimal order of sample complexity.\n\nWe propose a new Federated Q-learning algorithm, Fed-DVR-Q, that achieves not only a communication complexity of $\\begin{array} { r } { \\mathsf { C C } _ { \\mathsf { r o u n d } } = \\tilde { \\mathcal { O } } ( \\frac { 1 } { 1 - \\gamma } ) } \\end{array}$ and $\\begin{array} { r } { \\mathsf { C C } _ { \\mathsf { b i t } } = \\tilde { \\mathcal { O } } ( \\frac { | \\mathcal { S } | | \\mathcal { A } | } { 1 - \\gamma } ) } \\end{array}$ but also the optimal order of sample complexity (upto logarithmic factors), thereby providing a tight characterization of the achievability frontier that matches with the converse result derived in the previous section.\n\n# 4.1 Algorithm Description\n\nFed-DVR-Q proceeds in epochs. During an epoch $k \\geq 1$ , the agents collaboratively update $Q ^ { ( k - 1 ) }$ , the estimate of $Q ^ { \\star }$ obtained at the end of previous epoch, to a new estimate $Q ^ { ( k ) }$ , with the aid of the sub-routine called REFINEESTIMATE. The sub-routine REFINEESTIMATE is designed to ensure that the suboptimality gap, $\\| \\bar { Q } ^ { ( k ) } - Q ^ { \\star } \\| _ { \\infty }$ , reduces by a factor of 2 at the end of every epoch. Thus, at the end of $K = \\mathcal { O } ( \\log ( 1 / \\varepsilon ) )$ epochs, Fed-DVR-Q obtains a $\\varepsilon$ -optimal estimate of $Q ^ { \\star }$ , which is then set to be the output of the algorithm. Please refer to Alg. 2 for a pseudocode.\n\n# Algorithm 2: Fed-DVR-Q\n\n1: Input : Error bound $\\varepsilon > 0$ , failure probability $\\delta \\bar { > } 0$   \n2: $k \\gets 1 , Q ^ { ( 0 ) } \\gets \\mathbf { 0 }$   \n3: // Set parameters as described in Sec. 4.1.3   \n4: for $k = 1 , 2 , \\ldots , K$ do   \n5: Q(k) REFINEESTIMATE(Q(k‚àí1), B, I, Lk, Dk, J)   \n6: k k + 1   \n7: end for   \n8: return Q(K)\n\n# 4.1.1 The REFINEESTIMATE sub-routine\n\nREFINEESTIMATE, starting from $\\overline { { Q } }$ , an initial estimate of $Q ^ { \\star }$ , uses variance reduced Q-learning updates to obtain an improved estimate of $Q ^ { \\star }$ . It is characterized by four parameters ‚Äî the initial estimate $\\overline { { Q } }$ , the number of local iterations $I$ , the recentering sample size $L$ and the batch size $B$ ,\n\nwhich can be appropriately tuned to control the quality of the returned estimate. Additionally, it also takes input two parameters $D$ and $J$ required by the compressor.\n\nThe first step in REFINEESTIMATE is to collaboratively approximate $\\tau { \\overline { { Q } } }$ for the variance reduced updates. To this effect, each agent $m$ builds an approximation of $\\tau { \\overline { { Q } } }$ as follows:\n\n$$\n\\widetilde { \\mathcal { T } } _ { L } ^ { ( m ) } ( \\overline { { Q } } ) : = \\frac { 1 } { \\lceil L / M \\rceil } \\sum _ { l = 1 } ^ { \\lceil L / M \\rceil } \\mathcal { T } _ { Z _ { l } ^ { ( m ) } } ( \\overline { { Q } } ) ,\n$$\n\nwhere $\\{ Z _ { 1 } ^ { ( m ) } , Z _ { 2 } ^ { ( m ) } , \\ldots , Z _ { \\lceil L / M \\rceil } ^ { ( m ) } \\}$ are $\\lceil L / M \\rceil$ i.i.d. samples with $Z _ { 1 } ^ { ( m ) } \\sim Z$ . Each agent sends $\\mathcal { C } \\left( \\widetilde { \\mathcal { T } } _ { L } ^ { ( m ) } ( \\overline { { Q } } ) - \\overline { { Q } } ; D , J \\right)$ , a compressed version of the difference $\\widetilde { \\mathcal { T } } _ { L } ^ { ( m ) } ( \\overline { { Q } } ) - \\overline { { Q } }$ , to the server, which coll ects all the estimates from the agents and constructs the estimaete\n\n$$\n\\widetilde { \\mathcal { T } } _ { L } ( \\overline { { Q } } ) = \\overline { { Q } } + \\frac { 1 } { M } \\sum _ { m = 1 } ^ { M } \\mathcal { C } \\left( \\widetilde { \\mathcal { T } } _ { L } ^ { ( m ) } ( \\overline { { Q } } ) - \\overline { { Q } } ; D , J \\right)\n$$\n\nand sends it back to the agents for the variance reduced updates. We defer the description of the compression routine to the end of this section. Equipped with the estimate $\\widetilde { \\mathcal { T } } _ { L } ( \\overline { { Q } } )$ , REFINEESTIMATE constructs a sequence $\\{ Q _ { i } \\} _ { i = 1 } ^ { I }$ using the following iterative update scheme initialized with $Q _ { 0 } = \\overline { { Q } }$ . During the $i ^ { \\mathrm { { t h } } }$ iteration, each agent $m$ carries out the following update:\n\n$$\nQ _ { i -  { { \\frac { 1 } { 2 } } } } ^ { m } = ( 1 - \\eta ) Q _ { i - 1 } + \\eta \\left[ \\widehat { \\mathcal { T } } _ { i } ^ { ( m ) } Q _ { i - 1 } - \\widehat { \\mathcal { T } } _ { i } ^ { ( m ) } \\overline { { Q } } + \\widetilde { \\mathcal { T } } _ { L } ( \\overline { { Q } } ) \\right] .\n$$\n\nIn the above equation, $\\eta \\in ( 0 , 1 )$ is the step size and $\\begin{array} { r } { \\widehat { \\mathcal { T } } _ { i } ^ { ( m ) } Q : = \\frac { 1 } { B } \\sum _ { z \\in \\mathcal { Z } _ { i } ^ { ( m ) } } \\mathcal { T } _ { z } Q , } \\end{array}$ , where $\\mathcal { Z } _ { i } ^ { ( m ) }$ is the minibatch of $B$ i.i.d. random variables drawn accordbing to $Z$ , independently at each agent $m$ for all iterations $i$ . Each agent then sends a compressed version of the update, i.e., $\\mathcal { C } \\left( Q _ { i - \\frac { 1 } { 2 } } ^ { m } - Q _ { i - 1 } ; D , J \\right)$ , to the server, which uses them to compute the next iterate\n\n$$\nQ _ { i } = Q _ { i - 1 } + \\frac { 1 } { M } \\sum _ { m = 1 } ^ { M } \\mathcal { C } \\left( Q _ { i - \\frac { 1 } { 2 } } ^ { m } - Q _ { i - 1 } ; D , J \\right) ,\n$$\n\nand broadcast it to the clients. After $I$ such updates, the obtained iterate $Q _ { I }$ is returned by the routine.   \nA pseudocode of the REFINEESTIMATE routine is given in Algorithm 3 in Appendix A.\n\n# 4.1.2 The Compression Operator\n\nThe compressor, $\\mathcal { C } ( \\cdot ; D , J )$ , used in the proposed algorithm Fed-DVR-Q is based on the popular stochastic quantization scheme. In addition to the input vector $Q$ to be quantized, the quantizer $\\mathcal { C }$ takes two input parameters $D$ and ${ \\ J } . { \\ D }$ corresponds to an upper bound on $\\ell _ { \\infty }$ norm of $Q$ , i.e., $\\| Q \\| _ { \\infty } \\leq D$ . $J$ corresponds to the resolution of the compressor, i.e., number of bits used by the compressor to represent each coordinate of the output vector.\n\nThe compressor first splits the interval $[ 0 , D ]$ into $2 ^ { J } - 1$ intervals of equal length where $0 = d _ { 1 } <$ $d _ { 2 } , \\cdot \\cdot \\cdot < d _ { 2 ^ { J } } = D$ correspond to end points of the intervals. Each coordinate of $Q$ is then separately quantized as follows. The value of the $n ^ { \\mathrm { t h } }$ coordinate, ${ \\mathcal { C } } ( Q ) [ n ]$ , is set to be $d _ { j _ { n } - 1 }$ with probability djjn ‚àídjQ[n]1 and to djn with the remaining probability, where jn := min{j : dj < Q[i] ‚â§ dj+1}. It is straightforward to note that each coordinate of ${ \\mathcal { C } } ( Q )$ can be represented using $J$ bits.\n\n# 4.1.3 Setting the parameters\n\nThe desired convergence of the iterates $\\{ Q ^ { ( k ) } \\}$ is obtained by carefully choosing the parameters of the sub-routine REFINEESTIMATE and the compression operator $\\mathcal { C }$ . For all epochs $k \\geq 1$ , we set the number of iterations $I$ and the batch size $B$ of REFINEESTIMATE and the number of bits $J$ of the compressor $\\mathcal { C }$ to be $\\lceil \\frac { 2 } { \\eta ( 1 - \\gamma ) } \\rceil$ $\\begin{array} { r } { \\frac { 2 } { - \\gamma ) } ] , \\lceil \\frac { 2 } { M } \\big ( \\frac { 1 2 \\gamma } { ( 1 - \\gamma ) } \\big ) ^ { 2 } \\log ( \\frac { 8 K I | \\mathcal { S } | | \\mathcal { A } | } { \\delta } ) \\rceil } \\end{array}$ and $\\begin{array} { r } { \\lceil \\log _ { 2 } ( \\frac { 7 0 } { \\eta ( 1 - \\gamma ) } \\sqrt { \\frac { 2 } { M } \\log ( \\frac { 8 K I | \\mathcal { S } | | \\mathcal { A } | } { \\delta } ) } ) \\rceil } \\end{array}$ respectively. The total number of epochs is set to $\\begin{array} { r } { K = \\lceil \\frac { 1 } { 2 } \\log _ { 2 } ( \\frac { 1 } { 1 - \\gamma } ) \\rceil + \\lceil \\frac { 1 } { 2 } \\log _ { 2 } ( \\frac { 1 } { ( 1 - \\gamma ) \\varepsilon ^ { 2 } } ) \\rceil } \\end{array}$ . The recentering sample sizes $L _ { k }$ and bounds $D _ { k }$ are set to be the following functions of epoch index $k$ :\n\n$$\nL _ { k } : = \\frac { 1 9 6 0 0 } { ( 1 - \\gamma ) ^ { 2 } } \\log ( \\frac { 8 K I | S | | A | } { \\delta } ) \\cdot \\{ \\begin{array} { l l } { 4 ^ { k } \\quad } & { \\mathrm { i f ~ } k \\leq K _ { 0 } } \\\\ { 4 ^ { k - K _ { 0 } } \\quad } & { \\mathrm { i f ~ } k > K _ { 0 } } \\end{array} ; \\quad D _ { k } : = 1 6 \\cdot \\frac { 2 ^ { - k } } { 1 - \\gamma } ,\n$$\n\nwhere $\\begin{array} { r } { K _ { 0 } = \\lceil \\frac { 1 } { 2 } \\log _ { 2 } ( \\frac { 1 } { 1 - \\gamma } ) \\rceil } \\end{array}$ . The piecewise definition of $L _ { k }$ is crucial to obtain the optimal dependence with respect to $\\textstyle { \\frac { 1 } { 1 - { \\gamma } } }$ , similar to the two-step procedure outlined in Wainwright [2019b].\n\n# 4.2 Performance Guarantees\n\nThe following theorem characterizes the sample and communication complexity of Fed-DVR-Q.\n\nTheorem 2. Consider any $\\delta \\in ( 0 , 1 )$ and $\\varepsilon \\in ( 0 , 1 ]$ . Under the federated learning setup described in Section 2.1, the sample and communication complexities of the Fed-DVR-Q algorithm, when run with the choice of parameters described in Sec. 4.1.3 and a learning rate $\\eta \\in ( 0 , 1 )$ , satisfy the following relations for some universal constant $C _ { 1 } > 0$ :\n\n$$\n\\begin{array} { r l } & { \\mathsf { S C } ( \\mathsf { F e q - D V R - Q } ; \\varepsilon , M , \\delta ) \\le \\frac { C _ { 1 } } { \\eta M ( 1 - \\gamma ) ^ { 3 } \\varepsilon ^ { 2 } } \\log _ { 2 } \\left( \\frac { 1 } { ( 1 - \\gamma ) \\varepsilon } \\right) \\log \\left( \\frac { 8 K I | \\mathcal { S } | | A | } { \\delta } \\right) , } \\\\ & { \\mathsf { S C _ { \\mathrm { r o u r d } } } ( \\mathsf { F e q - D V R - Q } ; \\varepsilon , \\delta ) \\le \\frac { 1 6 } { \\eta ( 1 - \\gamma ) } \\log _ { 2 } \\left( \\frac { 1 } { ( 1 - \\gamma ) \\varepsilon } \\right) , } \\\\ & { \\mathsf { C C _ { \\mathrm { b i f } } } ( \\mathsf { F e q - D V R - Q } ; \\varepsilon , \\delta ) \\le \\frac { 3 2 | \\mathcal { S } | \\mathcal { A } | } { \\eta ( 1 - \\gamma ) } \\log _ { 2 } \\left( \\frac { 1 } { ( 1 - \\gamma ) \\varepsilon } \\right) \\log _ { 2 } \\left( \\frac { 7 0 } { \\eta ( 1 - \\gamma ) } \\sqrt { \\frac { 2 } { M } \\log \\left( \\frac { 8 K I | \\mathcal { S } | | A | } { \\delta } \\right) } \\right) } \\end{array}\n$$\n\nA proof of Theorem 2 can be found in Appendix C. A few implications of the theorem are in order.\n\nOptimal Sample-Communication complexity trade-off. As shown by the above theorem, FedDVR-Q offers a linear speed up in the sample complexity with respect to the number of agents while simultaneously achieving the same order of communication complexity as dictated by the lower bound derived in Theorem 1, both in terms of frequency and bit level complexity. Moreover, Fed-DVR-Q is the first Federated Q-Learning algorithm that achieves a sample complexity with optimal dependence on all the salient parameters, i.e., $| S | , | A |$ and $\\scriptstyle { \\frac { 1 } { 1 - \\gamma } }$ , in addition to linear speedup w.r.t. to number of agents and thereby bridges the existing gap between upper and lower bounds on sample complexity for Federated Q-learning. Thus, Theorem 1 and 2 together provide a characterization of optimal operating point of the sample-communication complexity trade-off in Federated Q-learning.\n\nRole of Minibatching. The commonly adopted approach in intermittent communication algorithm is to use a local update scheme that takes multiple small (i.e., $B = \\mathcal { O } ( 1 ) )$ , noisy updates between communication rounds, as evident from the algorithm design in Khodadadian et al. [2022], Woo et al. [2023] and even numerous FL algorithms for stochastic optimization McMahan et al. [2017], Haddadpour et al. [2019], Khaled et al. [2020]. In Fed-DVR-Q, we replace the local update scheme of taking multiple small, noisy updates by a single, large update with smaller variance, obtained by averaging the noisy updates over a minibatch of samples. The use of updates with smaller variance in variance reduced Q-learning yields the algorithm its name. While both the approaches result in similar sample complexity guarantees, the local update scheme requires more frequent averaging across clients to ensure that the bias of the estimate, also commonly referred to as ‚Äúclient drift‚Äù, is not too large. On the other hand, the minibatching approach does not encounter the problem of bias accumulation from local updates and hence can afford more infrequent averaging allowing Fed-DVR-Q to achieve optimal order of communication complexity.\n\nCompression. Fed-DVR-Q is the first algorithm in Federated Q-Learning to analyze and establish communication complexity at the bit level. All existing studies on Federated RL focus only on the frequency of communication and assume transmission of real numbers with infinite bit precision. On the other hand, the our analysis provides a more holistic view point of communication complexity and provides bounds at the bit level, which is of great practical significance. While some recent other studies [Wang et al., 2023] also consider quantization in Federated RL, their objective is to understand the impact of message size on convergence with no constraint on the frequency of communication, unlike the holistic viewpoint adopted in this work.\n\n# 5 Conclusion and Future Directions\n\nWe presented a complete characterization of the sample-communication trade-off for Federated Q-learning algorithms with intermittent communication. We showed that no Federated Q-learning algorithm with intermittent communication can achieve a linear speedup with respect to the number of agents if its number of communication rounds are sublinear in $\\frac { 1 ^ { \\bullet } } { 1 - \\gamma }$ . We also proposed a new Federated Q-learning algorithm called Fed-DVR-Q that uses variance reduction along with minibatching to achieve optimal-order sample and communication complexities. In particular, we showed that FedDVR-Q has a sample complexity of $\\begin{array} { r } { \\tilde { \\mathcal { O } } \\big ( \\frac { | \\boldsymbol { S } | | \\boldsymbol { A } | } { M ( 1 - \\gamma ) ^ { 3 } \\varepsilon ^ { 2 } } \\big ) } \\end{array}$ , which is order-optimal in all salient problem parameters, and a communication complexity of $\\begin{array} { r } { \\tilde { \\mathcal { O } } ( \\frac { 1 } { 1 - \\gamma } ) } \\end{array}$ rounds and $\\tilde { \\mathcal { O } } ( \\frac { | \\boldsymbol { S } | | \\boldsymbol { A } | } { 1 - \\gamma } )$ bits.\n\nThe results in this work raise several interesting questions that are worth exploring. While we focus on the tabular setting in this work, it is of great interest to investigate to the trade-off in other settings where we use function approximation to model the $Q ^ { \\star }$ and $V ^ { \\star }$ functions. Moreover, it is interesting to explore the trade-off in the finite horizon setting, where there is no discount factor. Furthermore, it is also worthwhile to explore if the communication complexity can be further reduced by going beyond the class of intermittent communication algorithms.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜËÅîÈÇ¶QÂ≠¶‰π†ÔºàFederated Q-learningÔºâ‰∏≠ÁöÑÊ†∑Êú¨‰∏éÈÄö‰ø°Â§çÊùÇÂ∫¶‰πãÈó¥ÁöÑÊùÉË°°ÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂΩìM‰∏™Êô∫ËÉΩ‰ΩìÂçè‰ΩúÂ≠¶‰π†‰∏Ä‰∏™ÂÖ∑ÊúâÊúâÈôêÁä∂ÊÄÅÂíåÂä®‰ΩúÁ©∫Èó¥ÁöÑÊú™Áü•Êó†ÈôêÊó∂ÂüüÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºàMDPÔºâÁöÑÊúÄ‰ºòQÂáΩÊï∞Êó∂ÔºåÂ¶Ç‰ΩïÂπ≥Ë°°Ê†∑Êú¨Â§çÊùÇÂ∫¶ÂíåÈÄö‰ø°Â§çÊùÇÂ∫¶„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåËÅîÈÇ¶Â≠¶‰π†‰∏≠ÁöÑÈÄö‰ø°ÊàêÊú¨ÊòØ‰∏Ä‰∏™ÂÖ≥ÈîÆÁì∂È¢àÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂàÜÂ∏ÉÂºèÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂ∫îÁî®‰∏≠„ÄÇÈÄöËøá‰ºòÂåñËøô‰∏ÄÊùÉË°°ÔºåÂèØ‰ª•Âú®‰∏çÁâ∫Áâ≤Â≠¶‰π†ÊïàÁéáÁöÑÊÉÖÂÜµ‰∏ãÊòæËëóÈôç‰ΩéÈÄö‰ø°ÂºÄÈîÄÔºå‰ªéËÄåÂú®Ëá™Âä®È©æÈ©∂„ÄÅÊ∏∏Êàè„ÄÅÊé®ËçêÁ≥ªÁªüÁ≠âÂÆûÈôÖÂ∫îÁî®‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÁöÑÂàÜÂ∏ÉÂºèRL„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÈ¶ñÂÖàÂª∫Á´ã‰∫ÜËÅîÈÇ¶QÂ≠¶‰π†ÁöÑÈÄö‰ø°Â§çÊùÇÂ∫¶‰∏ãÁïåÔºåËØÅÊòé‰ªª‰ΩïËÉΩÂ§üÂÆûÁé∞Ê†∑Êú¨Â§çÊùÇÂ∫¶Á∫øÊÄßÂä†ÈÄüÁöÑÁÆóÊ≥ïÂøÖÈ°ªËá≥Â∞ëÊîØ‰ªòŒ©(1/(1-Œ≥))ÁöÑÈÄö‰ø°ÊàêÊú¨„ÄÇÈöèÂêéÔºåËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Fed-DVR-QÁöÑÊñ∞ÁÆóÊ≥ïÔºåÈ¶ñÊ¨°ÂêåÊó∂ÂÆûÁé∞‰∫ÜÊúÄ‰ºòÈò∂ÁöÑÊ†∑Êú¨Â§çÊùÇÂ∫¶ÂíåÈÄö‰ø°Â§çÊùÇÂ∫¶„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÈÄö‰ø°Â§çÊùÇÂ∫¶‰∏ãÁïåÔºö** ËØÅÊòé‰∫Ü‰ªª‰ΩïËÅîÈÇ¶QÂ≠¶‰π†ÁÆóÊ≥ïÂú®Èó¥Ê≠áÈÄö‰ø°ËÆæÁΩÆ‰∏ãÔºåËã•Â∏åÊúõÂÆûÁé∞Ê†∑Êú¨Â§çÊùÇÂ∫¶ÁöÑÁ∫øÊÄßÂä†ÈÄüÔºåÂøÖÈ°ªËá≥Â∞ëËøõË°åŒ©(1/(1-Œ≥))ËΩÆÈÄö‰ø°Ôºå‰∏îÊØè‰∏™Êô∫ËÉΩ‰ΩìÈúÄË¶Å‰º†ËæìŒ©(|S||A|/(1-Œ≥))ÊØîÁâπÁöÑ‰ø°ÊÅØ„ÄÇ\\n> *   **Fed-DVR-QÁÆóÊ≥ïÔºö** ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÅîÈÇ¶QÂ≠¶‰π†ÁÆóÊ≥ïÔºåÂÖ∂Ê†∑Êú¨Â§çÊùÇÂ∫¶‰∏∫OÃÉ(|S||A|/(M(1-Œ≥)¬≥Œµ¬≤))ÔºåÈÄö‰ø°Â§çÊùÇÂ∫¶‰∏∫OÃÉ(1/(1-Œ≥))ËΩÆÂíåOÃÉ(|S||A|/(1-Œ≥))ÊØîÁâπÔºåÈ¶ñÊ¨°ÂêåÊó∂ËææÂà∞‰∫ÜÊúÄ‰ºòÈò∂ÁöÑÊ†∑Êú¨ÂíåÈÄö‰ø°Â§çÊùÇÂ∫¶„ÄÇ\\n> *   **ÂÆåÊï¥ÊùÉË°°ÂàªÁîªÔºö** ÈÄöËøáÁêÜËÆ∫ÂàÜÊûêÂíåÂÆûÈ™åÈ™åËØÅÔºåËÆ∫ÊñáÈ¶ñÊ¨°ÂÆåÊï¥ÂàªÁîª‰∫ÜËÅîÈÇ¶QÂ≠¶‰π†‰∏≠Ê†∑Êú¨-ÈÄö‰ø°Â§çÊùÇÂ∫¶ÁöÑÊúÄ‰ºòÊùÉË°°ËæπÁïå„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   Fed-DVR-QÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáÊñπÂ∑ÆÂáèÂ∞ëÔºàVariance ReductionÔºâÂíåÂ∞èÊâπÈáèÔºàMinibatchingÔºâÊäÄÊúØÔºåÂú®ÂáèÂ∞ëÂ±ÄÈÉ®Êõ¥Êñ∞ÂÅèÂ∑ÆÁöÑÂêåÊó∂ÊéßÂà∂ÈÄö‰ø°È¢ëÁéá„ÄÇÂÖ∂ËÆæËÆ°Âì≤Â≠¶Âú®‰∫éÈÄöËøáÊõ¥È´òÊïàÁöÑÂ±ÄÈÉ®Êõ¥Êñ∞ÂíåÊõ¥Á®ÄÁñèÁöÑÈÄö‰ø°ÔºåÂÆûÁé∞Ê†∑Êú¨Â§çÊùÇÂ∫¶ÂíåÈÄö‰ø°Â§çÊùÇÂ∫¶ÁöÑÂèåÈáç‰ºòÂåñ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** Áé∞ÊúâËÅîÈÇ¶QÂ≠¶‰π†ÁÆóÊ≥ïÔºàÂ¶ÇFed-SynQÔºâË¶Å‰πàÈÄö‰ø°ÊàêÊú¨È´òÔºàOÃÉ(M/(1-Œ≥))ÔºâÔºåË¶Å‰πàÊ†∑Êú¨Â§çÊùÇÂ∫¶Ê¨°‰ºòÔºàÂ¶ÇOÃÉ(1/(1-Œ≥)‚ÅµŒµ¬≤)Ôºâ„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** Fed-DVR-QÈÄöËøá‰ª•‰∏ãÂàõÊñ∞Ëß£ÂÜ≥‰∫ÜËøô‰∫õÈóÆÈ¢òÔºö\\n>     1.  ÂºïÂÖ•ÊñπÂ∑ÆÂáèÂ∞ëÊäÄÊúØÔºåÂáèÂ∞ëÂ±ÄÈÉ®Êõ¥Êñ∞ÁöÑÂÅèÂ∑ÆÔºõ\\n>     2.  ‰ΩøÁî®Â∞èÊâπÈáèÊõ¥Êñ∞Êõø‰ª£È¢ëÁπÅÁöÑÂ±ÄÈÉ®Êõ¥Êñ∞ÔºåÈôç‰ΩéÈÄö‰ø°È¢ëÁéáÔºõ\\n>     3.  ÈááÁî®ÈáèÂåñÂéãÁº©ÊäÄÊúØÔºåËøõ‰∏ÄÊ≠•Èôç‰ΩéÊØîÁâπÁ∫ßÈÄö‰ø°ÊàêÊú¨„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.   **ÂàùÂßãÂåñÔºö** ÊâÄÊúâÊô∫ËÉΩ‰ΩìÂàùÂßãQÂáΩÊï∞‰º∞ËÆ°‰∏∫0„ÄÇ\\n> 2.   **ÂàÜÈò∂ÊÆµÊõ¥Êñ∞Ôºö** ÁÆóÊ≥ïËøêË°åK‰∏™Èò∂ÊÆµÔºåÊØè‰∏™Èò∂ÊÆµkÁöÑÁõÆÊ†áÊòØÂ∞ÜÂΩìÂâçQÂáΩÊï∞‰º∞ËÆ°Q^(k-1)ÁöÑÊ¨°‰ºòÊÄßÂ∑ÆË∑ùÂáèÂçä„ÄÇ\\n> 3.   **ÊñπÂ∑ÆÂáèÂ∞ëÊõ¥Êñ∞Ôºö** Âú®ÊØè‰∏™Èò∂ÊÆµkÔºåÊô∫ËÉΩ‰ΩìÈÄöËøáREFINEESTIMATEÂ≠êÁ®ãÂ∫èÊõ¥Êñ∞QÂáΩÊï∞Ôºö\\n>      - ÊØè‰∏™Êô∫ËÉΩ‰Ωì‰ΩøÁî®L_k‰∏™Ê†∑Êú¨ÊûÑÂª∫ÂØπTQ^(k-1)ÁöÑËøë‰ºº‰º∞ËÆ°Ôºõ\\n>      - ÈÄöËøáÂéãÁº©ÈÄö‰ø°Â∞Ü‰º∞ËÆ°ÂÄº‰º†ËæìÂà∞ÊúçÂä°Âô®Âπ∂ËÅöÂêàÔºõ\\n>      - ‰ΩøÁî®Â∞èÊâπÈáèÊ†∑Êú¨ËøõË°åIÊ¨°ÊñπÂ∑ÆÂáèÂ∞ëÁöÑQÂ≠¶‰π†Êõ¥Êñ∞„ÄÇ\\n> 4.   **ÂéãÁº©ÈÄö‰ø°Ôºö** ÊØèÊ¨°ÈÄö‰ø°Êó∂Ôºå‰ΩøÁî®Âü∫‰∫éÈöèÊú∫ÈáèÂåñÁöÑÂéãÁº©ÁÆóÂ≠êC(¬∑;D,J)ÂáèÂ∞ë‰º†ËæìÊØîÁâπÊï∞„ÄÇ\\n> 5.   **ËæìÂá∫Ôºö** ÁªèËøáK‰∏™Èò∂ÊÆµÂêéÔºåËæìÂá∫ÊúÄÁªàÁöÑQÂáΩÊï∞‰º∞ËÆ°Q^(K)„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   Q-learning [Li et al., 2023]ÔºàÂçïÊô∫ËÉΩ‰ΩìÂü∫ÂáÜÔºâ\\n> *   Variance Reduced Q-learning [Wainwright, 2019b]ÔºàÂçïÊô∫ËÉΩ‰ΩìÊñπÂ∑ÆÂáèÂ∞ëÂü∫ÂáÜÔºâ\\n> *   Fed-SynQ [Woo et al., 2023]ÔºàÁé∞ÊúâËÅîÈÇ¶QÂ≠¶‰π†ÁÆóÊ≥ïÔºâ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Ê†∑Êú¨Â§çÊùÇÂ∫¶‰∏äÔºö** Fed-DVR-QÂú®ÂêåÊ≠•ËÆæÁΩÆ‰∏ãËææÂà∞‰∫ÜOÃÉ(|S||A|/(M(1-Œ≥)¬≥Œµ¬≤))ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãFed-SynQ (OÃÉ(|S||A|/(M(1-Œ≥)‚ÅµŒµ¬≤))) ÂíåÂçïÊô∫ËÉΩ‰ΩìQ-learning (OÃÉ(|S||A|/(1-Œ≥)‚Å¥Œµ¬≤))„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øFed-SynQÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü(1-Œ≥)¬≤ÂÄçÁöÑÊ†∑Êú¨ÊïàÁéá„ÄÇ\\n>   \\n> *   **Âú®ÈÄö‰ø°Â§çÊùÇÂ∫¶‰∏äÔºö** Fed-DVR-QÁöÑÈÄö‰ø°ËΩÆÊï∞‰∏∫OÃÉ(1/(1-Œ≥))ÔºåËøú‰Ωé‰∫éFed-SynQ (OÃÉ(M/(1-Œ≥)))ÔºåÂêåÊó∂ÊØîÁâπÁ∫ßÈÄö‰ø°ÊàêÊú¨‰∏∫OÃÉ(|S||A|/(1-Œ≥))ÔºåÊòØÈ¶ñ‰∏™ÂàÜÊûêÊØîÁâπÁ∫ßÊàêÊú¨ÁöÑËÅîÈÇ¶QÂ≠¶‰π†ÁÆóÊ≥ï„ÄÇ\\n>   \\n> *   **Âú®ÁªºÂêàÊÄßËÉΩ‰∏äÔºö** Fed-DVR-QÊòØÈ¶ñ‰∏™ËææÂà∞ÁêÜËÆ∫‰∏ãÁïåÁöÑÁÆóÊ≥ïÔºåÂêåÊó∂ÂÆûÁé∞‰∫ÜÊúÄ‰ºòÈò∂ÁöÑÊ†∑Êú¨Â§çÊùÇÂ∫¶ÂíåÈÄö‰ø°Â§çÊùÇÂ∫¶ÔºåËÄåÂÖ∂‰ªñÁÆóÊ≥ïÂè™ËÉΩÂú®‰∏§ËÄÖ‰πãÈó¥ÂÅöÂá∫ÊùÉË°°„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ËÅîÈÇ¶Â≠¶‰π† (Federated Learning, FL)\\n*   QÂ≠¶‰π† (Q-learning, N/A)\\n*   Ê†∑Êú¨Â§çÊùÇÂ∫¶ (Sample Complexity, N/A)\\n*   ÈÄö‰ø°Â§çÊùÇÂ∫¶ (Communication Complexity, N/A)\\n*   È©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã (Markov Decision Process, MDP)\\n*   ÊñπÂ∑ÆÂáèÂ∞ë (Variance Reduction, N/A)\\n*   Èó¥Ê≠áÈÄö‰ø° (Intermittent Communication, N/A)\\n*   ÂàÜÂ∏ÉÂºèÂº∫ÂåñÂ≠¶‰π† (Distributed Reinforcement Learning, N/A)\"\n}\n```"
}