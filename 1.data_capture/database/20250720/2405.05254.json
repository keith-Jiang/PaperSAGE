{
    "link": "https://arxiv.org/abs/2405.05254",
    "pdf_link": "https://arxiv.org/pdf/2405.05254",
    "title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models",
    "authors": [
        "Yutao Sun",
        "Li Dong",
        "Yi Zhu",
        "Shaohan Huang",
        "Wenhui Wang",
        "Shuming Ma",
        "Quanlu Zhang",
        "Jianyong Wang",
        "Furu Wei"
    ],
    "institutions": [
        "Microsoft Research",
        "Tsinghua University"
    ],
    "publication_date": "2024-05-08",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 64,
    "influential_citation_count": 4,
    "paper_content": "# You Only Cache Once: Decoder-Decoder Architectures for Language Models\n\nYutao Sun∗‡† Li Dong∗† Yi Zhu† Shaohan Huang† Wenhui Wang† Shuming Ma† Quanlu Zhang† Jianyong Wang‡ Furu Wei†⋄ ‡ Tsinghua University † Microsoft Research https://aka.ms/GeneralAI\n\n# Abstract\n\nWe introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a crossdecoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes.\n\n# 1 Introduction\n\nDecoder-only Transformer [40] has become the de facto architecture for language models. By caching the previously computed key/value vectors, the model can reuse them for future generation steps. The key-value cache avoids encoding the history again for each token, greatly improving the inference speed. The compelling feature establishes the decoder-only architecture as the standard option.\n\nHowever, as the number of serving tokens increases, the key-value (KV) caches occupy a lot of GPU memory, rendering the inference of large language models memory-bounded [29]. For the example of a 65B-size language model (augmented with grouped-query attention [1] and 8-bit KV quantization), 512K tokens occupy about 86GB GPU memory, which is even larger than the capacity of one H100-80GB GPU. In addition, the prefilling latency of long-sequence input is extremely high. For instance, using four H100 GPUs, the 7B language model (augmented with Flash-Decoding [6] and kernel fusion) requires about 110 seconds to prefill 450K tokens, and 380 seconds for 1M length. The above bottlenecks make it difficult to deploy long-context language models in practice.\n\nIn this work, we propose a decoder-decoder architecture, YOCO, for large language models, which only caches KV pairs once. Specifically, we stack cross-decoder upon self-decoder. Given an input sequence, the self-decoder utilizes efficient self-attention to obtain KV caches. Then the cross-decoder layers employ cross-attention to reuse the shared KV caches. The decoder-decoder architecture is conceptually similar to encoder-decoder, but the whole model behaves more like a decoder-only model from the external view. It naturally fits into autoregressive generation tasks, such as language modeling. First, because YOCO only caches once2, the GPU memory consumption of KV caches is significantly reduced. Second, the computation flow of the decoder-decoder architecture enables prefilling to early exit before entering the self-decoder. The nice property speeds up the prefill stage dramatically, improving user experience for long-context language models. Third, YOCO allows for more efficient system design for distributed long-sequence training. In addition, we propose gated retention for self-decoder, which augments retention [35] with a data-controlled gating mechanism.\n\nWe conduct extensive experiments to show that YOCO achieves favorable language modeling performance and has many advantages in terms of inference efficiency. Experimental results demonstrate that YOCO can be scaled up with more training tokens, larger model size, and longer context length. Specifically, we scale up the 3B YOCO model to trillions of training tokens, attaining results on par with prominent Transformer language models, such as StableLM [39]. Moreover, the scaling curves ranging from 160M to 13B show that YOCO are competitive compared to Transformer. We also extend the context length of YOCO to 1M tokens, achieving near-perfect needle retrieval accuracy. In the multi-needle test, YOCO obtains competitive results even compared to larger Transformers.\n\nIn addition to good performance on various tasks, the profiling results show that YOCO improves the GPU memory footprint, prefill latency, throughput, and serving capacity. In particular, the memory of KV caches can be reduced by about $8 0 \\times$ for 65B models. Even for a 3B model, the overall inference memory consumption can be reduced by two times for 32K tokens and by more than nine times for 1M tokens. The prefill stage is speeded up by $7 1 . 8 \\times$ for the 1M context and $2 . 8 7 \\times$ for the 32K input. For example, for a 512K context, YOCO reduces the Transformer prefilling latency from 180 seconds to less than six seconds. The results position YOCO as a strong candidate model architecture for future large language models with native long-sequence support.\n\n# 2 Related Work\n\nNumerous efforts have been made to reduce KV caches for inference. Efficient attention mechanisms are proposed, such as sparse Transformer [4], linear attention [18], and recurrent modeling [27, 13, 46, 3, 19]. Another strand of research drops KV caches to achieve sparsity [49, 43, 11]. In comparison, we keep one global KV cache and still conduct full cross-attention for better long-context modeling. Moreover, some previous methods are complementary to our proposed architecture. For example, multi-/grouped-query attention [33, 1] and multi-latent attention [7] can be used in YOCO. Low-bit KV quantization [14, 25, 34] can also be used together to reduce memory consumption. In addition, the intriguing property of YOCO greatly speeds up the prefill stage.\n\n# 3 You Only Cache Once (YOCO)\n\nThe proposed architecture, named YOCO, is designed for autoregressive modeling, such as large language models (LLMs). As shown in Figure 1, the decoder-decoder architecture has two parts, i.e., self-decoder and cross-decoder. Specifically, YOCO is stacked with $L$ blocks, where the first $\\begin{array} { l } { { \\frac { L } { 2 } } } \\end{array}$ layers are self-decoder while the rest modules are cross-decoder. Given an input sequence $x = x _ { 1 } \\cdot \\cdot \\cdot x _ { | x | }$ , the input embeddings are packed into $X ^ { 0 } = [ \\pmb { x } _ { 1 } , \\cdot \\cdot \\cdot , \\pmb { x } _ { | x | } ] \\in \\mathbb { R } ^ { | x | \\times d _ { \\mathrm { m o d e l } } }$ , where $d _ { \\mathrm { m o d e l } }$ is hidden dimension. We first obtain contextualized vector representations $X ^ { l } = \\mathrm { S e l f { - } D e c o d e r } ( X ^ { l - 1 } ) , l \\in$ $[ 1 , \\frac { L } { 2 } ]$ , where $X ^ { L / 2 }$ is used to produce KV caches $\\hat { K } , \\hat { V }$ for cross-decoder. Then we compute $\\begin{array} { r } { { X ^ { l } } ^ { - } = \\mathrm { C r o s s - D e c o d e r } ( X ^ { l - 1 } , \\hat { K } , \\hat { V } ) , l \\in [ \\frac { L } { 2 } + 1 , L ] } \\end{array}$ to get the output vectors $X ^ { L }$ . After obtaining $X ^ { L }$ , a softmax classifier performs next-token prediction over the vocabulary.\n\nBoth self- and cross-decoder follow a similar block layout (i.e., interleaved attention and feedforward network) as in Transformer [40]. We also include pre-RMSNorm [48], SwiGLU [32], and grouped-query attention [1] as improvements. The difference between the two parts lies in attention modules. Self-decoder uses efficient self-attention (e.g., sliding-window attention). In comparison, cross-decoder uses global cross-attention to attend to the shared KV caches produced by the output of the self-decoder. Notice that the whole model behaves like a decoder-only model. The tokens generated by cross-decoder are also fed back to self-decoder.\n\n![](images/2e8165e2ca9360f7c0b1c495f4b80a6cf05a9f569aef485d90b81e66f128fba8.jpg)  \nFigure 1: Overview of the decoder-decoder architecture. Self-decoder generates the global KV cache. Then cross-decoder employs cross-attention to reuse the shared KV caches. Both self-decoder and cross-decoder use causal masking. The overall architecture behaves like a decoder-only Transformer, autoregressively generating tokens.\n\n# 3.1 Self-Decoder\n\nSelf-decoder takes embeddings $X ^ { 0 }$ as input and compute intermediate vector representation $X ^ { L / 2 }$ :\n\n$$\n\\begin{array} { c } { { Y ^ { l } = \\mathrm { E S A } ( \\mathrm { L N } ( X ^ { l } ) ) + X ^ { l } } } \\\\ { { X ^ { l + 1 } = \\mathrm { S w i G L U } ( \\mathrm { L N } ( Y ^ { l } ) ) + Y ^ { l } } } \\end{array}\n$$\n\nwhere $\\mathrm { E S A } ( \\cdot )$ represents efficient self-attention, $\\operatorname { S w i G L U } ( X ) = ( \\operatorname { s w i s h } ( X W _ { G } ) \\odot X W _ { 1 } ) W _ { 2 }$ , and RMSNorm [48] is used for $\\mathrm { L N } ( \\cdot )$ . Causal masking is used for efficient self-attention.\n\nThe key property of the efficient self-attention module is $\\mathcal { O } ( 1 )$ inference memory, i.e., constant number of KV caches. For example, the cache size of sliding-window attention [4] depends on the window size instead of the input length. More design choices (e.g., gated retention) of the efficient self-attention module are detailed in Section 4.\n\n# 3.2 Cross-Decoder\n\nFirst, the output of the self-decoder $X ^ { L / 2 }$ generates global KV caches $\\hat { K } , \\hat { V }$ for cross-decoder:\n\n$$\n\\hat { K } = \\mathrm { L N } ( X ^ { L / 2 } ) W _ { K } , \\quad \\hat { V } = \\mathrm { L N } ( X ^ { L / 2 } ) W _ { V }\n$$\n\nwhere $W _ { K } , W _ { V } \\in \\mathbb { R } ^ { d \\times d }$ are learnable. Then, cross-decoder layers are stacked after self-decoder to obtain the final output $X ^ { L }$ . The KV caches $\\hat { K } , \\hat { V }$ are reused by all the $\\begin{array} { l } { { \\frac { L } { 2 } } } \\end{array}$ cross-decoder modules:\n\n$$\n\\begin{array} { r } { \\boldsymbol { Q } ^ { l } = \\mathrm { L N } ( \\boldsymbol { X } ^ { l } ) \\boldsymbol { W } _ { \\boldsymbol { Q } } ^ { l } \\qquad } \\\\ { \\boldsymbol { Y } ^ { l } = \\mathrm { A t t e n t i o n } ( \\boldsymbol { Q } ^ { l } , \\boldsymbol { \\hat { K } } , \\boldsymbol { \\hat { V } } ) + \\boldsymbol { X } ^ { l } \\qquad } \\\\ { \\boldsymbol { X } ^ { l + 1 } = \\mathrm { S w i G L U } ( \\mathrm { L N } ( \\boldsymbol { Y } ^ { l } ) ) + \\boldsymbol { Y } ^ { l } \\qquad } \\end{array}\n$$\n\nwhere Attention $( \\cdot )$ is standard multi-head attention [40], and $W _ { Q } ^ { l } \\in \\mathbb { R } ^ { d \\times d }$ is a learnable matrix. Causal masking is also used for cross-attention. Because cross-attention is compatible with group query attention [1], we can further save the memory consumption of KV caches.\n\nPrefilling Generation then  generate  new Cross-Decoder Cross-Decoder (Skipped) KV Cache Self-Decoder + T 4 + Pre-    filling  context  and   then  generate\n\n<html><body><table><tr><td></td><td>KV Cache Memory</td></tr><tr><td>Transformer</td><td>O(LND)</td></tr><tr><td>YOCO</td><td>O((N + L)D)</td></tr></table></body></html>\n\nTable 1: Inference memory complexity of KV caches. $N , L , D$ are the sequence length, number of layers, and hidden dimension.   \n\n<html><body><table><tr><td></td><td>Prefilling Time</td></tr><tr><td>Transformer</td><td>O(LN²D)</td></tr><tr><td>YOCO</td><td>O(LND)</td></tr></table></body></html>\n\ning to early exit without changing the final output, Table 2: Prefilling time complexity of attention thereby significantly speeding up the prefill stage. modules. $N , L , D$ are the same as above.\n\n# 3.3 Inference Advantages\n\nIn addition to competitive language modeling results, YOCO significantly reduces serving costs and improves inference performance. We report detailed inference comparisons in Section 5.4.\n\nSaving GPU Memory and Serving More Tokens. Table 1 compares the memory complexity between Transformers and YOCO. Specifically, because global KV caches are reused and efficient self-attention needs constant caches, the number of caches is $\\mathcal { O } ( N + C L )$ , where $N$ is the input length, $C$ is a constant (e.g., sliding window size), and $L$ is the number of layers. For long sequences, $C L$ is much smaller than $N$ , so about $\\mathcal { O } ( N )$ caches are required, i.e., you only cache once. In comparison, Transformer decoders have to store $N \\times L$ keys and values during inference. So YOCO roughly saves $L$ times GPU memory for caches compared to Transformer. Because the bottleneck of inference capacity becomes KV caches, our method enables us to serve many more tokens without being out of GPU memory. The increased batch size is also beneficial to inference throughput.\n\nReducing Prefilling Time and Improving Throughput. As shown in Figure 2, because the cross-decoder reuses the outputs of self-decoder, we can exit early before entering the cross-decoder during the prefill stage. The intriguing property of computation dependency greatly accelerates the prefilling speed. First, only half the layers are needed for forward computation, i.e., at least half prefilling latency reduction. Second, the efficient attention modules of the self-decoder are usually fast. For the example of 512K context length, we can decrease the prefilling latency from 180 seconds (Transformer with optimized inference, such as Flash-Decoding) to less than 6 seconds (Figure 9). Even for 32K length, YOCO has about three times speedup in terms of prefilling time. Table 2 compares prefilling time complexity of attention modules between Transformer and YOCO.\n\n# 4 Design Choices of Self-Decoder\n\nWe can choose various efficient self-attention methods for self-decoder. As long as the module only requires constant inference memory, the self-decoder’s cache memory complexity depends on the number of layers. Moreover, a good module choice improves both training and deployment costs. In this work, we use sliding-window attention (Section 4.1) or gated retention (Section 4.2).\n\n# 4.1 Sliding-Window Attention\n\nSliding-window attention [4] restricts the attention range into a fixed window size $C$ . In contrast, vanilla Transformer decoders attend to all previous tokens. During inference, the KV cache memory complexity can be reduced from $\\mathcal { O } ( N )$ to $\\mathcal { O } ( C )$ , i.e., the memory usage is constant rather than increasing with sequence length. Similar to multi-head self-attention [40], we compute the output of\n\nsliding-window attention via:\n\n$$\n\\begin{array} { c } { { Q = X W _ { Q } , K = X W _ { K } , V = X W _ { V } } } \\\\ { { \\mathrm { h e a d } _ { i } = \\mathrm { s o f t m a x } ( Q _ { [ i ] } K _ { [ i ] } ^ { \\top } + B ) V , B _ { i j } = \\left\\{ \\begin{array} { l l } { { 0 , } } & { { i - C < j \\leq i } } \\\\ { { - \\infty , } } & { { \\mathrm { o t h e r w i s e } } } \\end{array} \\right. } } \\\\ { { \\mathrm { S W A } ( X ) = \\mathrm { C o n c a t } ( \\mathrm { h e a d } _ { 1 } , \\cdots , \\mathrm { h e a d } _ { h } ) W _ { O } } } \\end{array}\n$$\n\nwhere $W _ { Q } , W _ { K } , W _ { V } , W _ { O } \\in \\mathbb { R } ^ { d \\times d }$ are learnable matrices, and the window causal mask $B$ controls each query only attends to the previous keys whose distances are less than $C$ . The pre-normalization and residual connection are also applied to the module.\n\n# 4.2 Gated Retention\n\nGated retention (gRet, aka gRetNet) augments retention [35] with a data-dependent gating mechanism. We use gRet as the default efficient self-attention module. The method unifies the parallel, recurrent, and chunkwise recurrent computation paradigms, which are equivalent and can obtain the same computation results. The training process usually uses the parallel or chunkwise recurrent paradigms, while the inference stage can employ the recurrent paradigm for constant KV memory.\n\nThe Parallel Representation The gated retention is defined as:\n\n$$\n\\begin{array} { r l r } {  { \\boldsymbol { Q } = ( X W _ { \\boldsymbol { Q } } ) \\odot \\Theta , } } & { \\boldsymbol { K } = ( X W _ { \\boldsymbol { K } } ) \\odot \\overline { { \\Theta } } , } & { \\boldsymbol { V } = X W _ { \\boldsymbol { V } } , \\quad \\Theta _ { n } = e ^ { i n \\theta } } \\\\ & { } & { \\gamma = \\mathrm { s i g m o i d } ( X W _ { \\gamma } ) ^ { 1 / \\tau } , \\quad D _ { n m } = \\{ \\begin{array} { l l } { \\prod _ { i = m + 1 } ^ { n } \\gamma _ { i } , } & { n \\ge m } \\\\ { 0 , } & { n < m } \\end{array}  } \\\\ & { } & { \\mathrm { g R e t } ( X ) = ( Q K ^ { \\top } \\odot D ) \\boldsymbol { V } } \\end{array}\n$$\n\nwhere $W _ { Q } , W _ { K } , W _ { V } \\in \\mathbb { R } ^ { d \\times d }$ and $W _ { \\gamma } \\in \\mathbb { R } ^ { d \\times 1 }$ are learnable weights, and the temperature term $\\tau$ encourages $\\gamma$ to 1 for better memorization [46]. The data-controlled decay is head-wise [19] rather than element-wise so that the computation can fully utilize NVIDIA tensor cores. Refer to [35] for more details about the other designs.\n\nThe Recurrent Representation Being equivalent to Equation (5), the output of gated retention can be computed recurrently. For the $n$ -th timestep, the output is obtained via:\n\n$$\n\\begin{array} { l } { S _ { n } = \\gamma _ { n } S _ { n - 1 } + K _ { n } ^ { \\top } V _ { n } } \\\\ { \\mathrm { ~ g R e t } ( X _ { n } ) = Q _ { n } S _ { n } , \\quad n = 1 , \\cdots , | x | } \\end{array}\n$$\n\nwhere $Q , K , V , \\gamma$ are the same as in Equation (5). During auto-regressive inference, the self-decoder maintains $S _ { n }$ as the intermediate state for an efficient generation.\n\nThe Chunkwise Recurrent Representation The chunk-wise representation is a unified formulation of recurrent and parallel representations. Given chunk size $B$ , the outputs are computed chunk by chunk. The computation is divided into inner-chunk and cross-chunk parts. Denote $[ i ]$ as the $i$ -th chunk, i.e., $x _ { [ i ] } = x _ { ( i - 1 ) B + 1 } , \\cdot \\cdot \\cdot , x _ { i B }$ , we compute the $i$ -th chunk as:\n\n$$\n\\begin{array} { r l } & { ~ \\beta _ { ( i - 1 ) B + j } = \\underset { k = ( i - 1 ) B + 1 } { \\overset { ( i - 1 ) B + j } { \\prod } } \\gamma _ { k } , D _ { [ i ] } ( j , k ) = \\frac { \\beta _ { ( i - 1 ) B + k } } { \\beta _ { ( i - 1 ) B + j } } \\mathrm { ~ i f ~ } j \\leq k \\mathrm { ~ e l s e ~ 0 ~ } } \\\\ & { ~ R _ { i } = K _ { [ i ] } ^ { \\top } ( V _ { [ i ] } \\odot \\frac { \\beta _ { i B } } { \\beta _ { [ i ] } } ) + \\beta _ { i B } R _ { i - 1 } , \\beta _ { [ i ] } ( j , k ) = \\beta _ { ( i - 1 ) B + j } } \\\\ & { \\mathrm { g R e t } ( X ) = \\underset { \\mathrm { I m e r c ~ C h u n k } } { \\underbrace { ( Q _ { [ i ] } K _ { [ i ] } ^ { \\top } \\odot D _ { [ i ] } ) V _ { [ i ] } } } + \\underset { \\mathrm { C r o s s . C h u n k } } { \\underbrace { ( Q _ { [ i ] } R _ { i - 1 } ) \\odot \\beta _ { [ i ] } } } } \\end{array}\n$$\n\nwhere $R _ { i }$ is the intermediate state of the $i$ -th chunk, and $\\beta$ summarizes the data-controlled decay $\\gamma$ . Appendix B proves the equivalence between the computation paradigms. The chunkwise paradigm combines the best of parallelism and recurrence, i.e., saving FLOPs compared to fully parallel computation and reducing iterations compared to recurrent computation. During the training and prefill stages, the chunk-wise representation increases throughput and reduces GPU memory consumption.\n\n<html><body><table><tr><td>Model</td><td>ARC-C</td><td>ARC-E</td><td>BoolQ</td><td>Hellaswag</td><td>OBQA</td><td>PIQA</td><td>Winogrande</td><td>SciQ</td><td>Avg</td></tr><tr><td colspan=\"10\">Training with 1T tokens</td></tr><tr><td>OpenLLaMA-3B-v2[12]</td><td>0.339</td><td>0.676</td><td>0.657</td><td>0.700</td><td>0.260</td><td>0.767</td><td>0.629</td><td>0.924</td><td>0.619</td></tr><tr><td>StableLM-alpha-3B-v2[38]</td><td>0.324</td><td>0.673</td><td>0.646</td><td>0.686</td><td>0.264</td><td>0.760</td><td>0.621</td><td>0.921</td><td>0.612</td></tr><tr><td>StableLM-3B-4E1T[39]</td><td></td><td>0.666</td><td></td><td></td><td></td><td>0.768</td><td>0.632</td><td>0.914</td><td></td></tr><tr><td>YOCO-3B</td><td>0.379</td><td>0.731</td><td>0.645</td><td>0.689</td><td>0.298</td><td>0.763</td><td>0.639</td><td>0.924</td><td>0.634</td></tr><tr><td colspan=\"10\">Training with 1.6T tokens</td></tr><tr><td>StableLM-3B-4E1T[39]</td><td></td><td>0.688</td><td></td><td></td><td></td><td>0.762</td><td>0.627</td><td>0.913</td><td></td></tr><tr><td>YOCO-3B</td><td>0.396</td><td>0.733</td><td>0.644</td><td>0.698</td><td>0.300</td><td>0.764</td><td>0.631</td><td>0.921</td><td>0.636</td></tr><tr><td colspan=\"10\">Extending context length to1M tokens</td></tr><tr><td>YOCO-3B-1M</td><td>0.413</td><td>0.747</td><td>0.638</td><td>0.705</td><td>0.300</td><td>0.773</td><td>0.651</td><td>0.932</td><td>0.645</td></tr></table></body></html>\n\nTable 3: Eval Harness [10] accuracy compared with well-trained Transformer language models. We scale the 3B model to 1.6 trillion training tokens. The 1T and $1 . 6 \\mathrm { T }$ results of StableLM-3B-4E1T are taken from its technical report [39]. YOCO-3B-1M is extended to the context length of 1M tokens.\n\nMulti-Head Gated Retention Similar to multi-head attention [40] and multi-scale retention [35], we apply gated retention to each head and combine the outputs together:\n\n$$\n{ \\begin{array} { r l } & { { \\mathrm { h e a d } } _ { i } = \\operatorname* { g R e t } ( X ) } \\\\ & { \\qquad Y = \\operatorname { G r o u p N o r m } _ { h } ( \\operatorname { C o n c a t } ( { \\mathrm { h e a d } } _ { 1 } , \\cdots , { \\mathrm { h e a d } } _ { n } ) ) } \\\\ & { { \\mathrm { M H G R } } ( X ) = ( { \\mathrm { s w i s h } } ( X W _ { G } ) \\odot Y ) W _ { O } } \\end{array} }\n$$\n\nwhere $W _ { G } , W _ { O } \\in \\mathbb { R } ^ { d \\times d }$ are learnable matrices, and GroupNorm [42] normalizes each head [41].   \nWe also apply swish gate to increase non-linearity [35].\n\n# 5 Experiments\n\nWe evaluate YOCO for large language models from the following perspectives. First, we follow the setting of StableLM-3B-4E1T [39] to scale up training tokens (Section 5.1). Second, we present the scaling curves of the proposed architectures (Section 5.2). Third, we scale up the YOCO model to 1M context length and evaluate its long-sequence modeling capability (Section 5.3). Fourth, we analyze the deployment advantages, including GPU memory footprint, serving capacity, prefilling time, and throughput (Section 5.4). Experimental results show that YOCO achieves competitive performance in various evaluation metrics and significantly reduces the inference cost.\n\n# 5.1 Language Modeling Evaluation\n\nWe train a 3B-size YOCO language model by scaling up the number of training tokens. Then we compare the checkpoints with strong Transformer-based language models. We use a similar training recipe to that in StableLM-3B-4E1T [39]. Detailed hyperparameters are described in Appendix D.\n\nResults Table 3 compares YOCO with OpenLLaMA-v2-3B [12], StableLM-base-alpha-3B-v2 [38], and StableLM-3B-4E1T [39]. We use LM Eval Harness [10] to evaluate zero-shot performance on various downstream tasks. OpenLLaMA-v2-3B and StableLM-base-alpha-3B-v2 are trained with 1T tokens. The intermediate numbers of StableLM-3B-4E1T are taken from its technical report [39]. Experimental results indicate that YOCO achieves comparable results with previous well-tuned Transformer language models. Both the checkpoints trained with 1T tokens and 1.6T tokens obtain a consistent trend. Moreover, the results show that YOCO is scalable in terms of training tokens.\n\n# 5.2 Scalability Compared with Transformers\n\nWe compare the scaling curves between Llama Transformer [40, 37], YOCO with gated retention $( \\mathrm { Y O C O _ { g R e t } }$ ; Section 4.2), and YOCO with sliding-window attention $( \\mathrm { Y O C O _ { S W A } }$ ; Section 4.1). We train language models of various sizes (i.e., 160M, 400M, 830M, 1.4B, 2.7B, 6.8B, and 13B) using the same training data and settings. We augment the Transformer architecture with Llama [37] improvements, such as RMSNorm [48], SwiGLU [32], and removing bias. The sliding window size of $\\mathtt { Y O C O _ { \\mathrm { { S W A } } } }$ is 1,024. The training batch size is 0.25M tokens with a $2 \\mathrm { k }$ sequence length. We train the models with $4 0 \\mathrm { k }$ steps, i.e., 10B tokens. In practice, we find that the setting is effective for loss convergence, and scaling laws can be well-fitted. More hyperparameters are detailed in Appendix E.\n\n09182736455463728190100 1.0 0.8   \nDepth (%) 0.6 0 0.4 0.2 0.0   \n8 家长多灵江 Context Length\n\n![](images/e0d085b149ed61e1d9fe41925927f3b3bd74b7e96b6363bb818bdcc984d65c77.jpg)  \nFigure 3: LM loss decreases along with scaling up the model size (ranging from $1 6 0 \\mathbf { M }$ to 13B).   \nFigure 4: Needle-in-a-haystack results in 1M length.\n\nResults Figure 3 reports the validation loss with various parameter counts. We also fit the scaling curves as in [17]. YOCO obtains comparable performance from 160M to 13B compared to the Llama-optimized transformer architecture. The findings demonstrate that YOCO scales effectively with respect to model size. Moreover, $\\mathrm { Y O C O } _ { \\mathrm { g R e t } }$ outperforms Transformer and $\\mathrm { Y O C O _ { \\mathrm { S W A } } }$ . The gains come from hybrid architectures of attention and retention, whose inductive biases tend to be complementary to each other. Recent hybrid architectures [21] also confirm similar findings.\n\n# 5.3 Long-Context Evaluation\n\nWe extend the context length of YOCO-3B (Section 5.1) to 1M tokens. We continue the model training with longer lengths progressively. The length schedule is 64K, 256K, and 1M tokens. Training data is up-sampled according to sequence length [9]. For a fair comparison, we do not use long-instruction tuning data. More training details are described in Appendix F.\n\nNeedle In A Haystack with 1M Context The pressure test evaluates whether models can retrieve “needles” from a long document [16]. We follow the evaluation setting of Gemini 1.5 [30] and LWM [24]. The needles are constructed as a city with a magic number. We run 10 times at the same depth and length. The average accuracy is reported. Figure 4 shows that YOCO-3B-1M passes the Needle-In-A-Haystack test with near perfect accuracy. The results indicate that YOCO has strong long-context modeling capability.\n\nMulti-Needle Retrieval Besides single-needle retrieval, we conduct a multi-needle evaluation. We compare YOCO-3B-1M with previous long-context language models, including MiniCPM128K [15], ChatGLM3-128K [47], YaRN-Mistral-128K [28], and LWM-1M-text [24]. The evaluation is conducted in 128K sequence length, because most previous models are tuned with this length.\n\nTable 4 presents accuracy results with $N$ needles. LWM-1M-text and YOCO-3B-1M are trained with a 1M context length, while the others are of 128K length. Although LWM-1M-text continues training of Llama-2-7B, YOCO-3B-1M can still achieve comparable performance with half the model size. Moreover, the 7B-size YaRN-Mistral-128K [28] obtained by position interpolation lags behind the other models. Compared to MiniCPM-128K and ChatGLM3-128K, YOCO-3B-1M also outperforms these well-trained language models.\n\n<html><body><table><tr><td>Model</td><td>Size</td><td>N=1</td><td>N=2</td><td>N=4</td><td>N=8</td></tr><tr><td>YaRN-Mistral-128K[28]</td><td>7B</td><td>0.02</td><td>0.12</td><td>0.08</td><td>0.20</td></tr><tr><td>LWM-1M-text [24]</td><td>7B</td><td>1.00</td><td>0.90</td><td>0.76</td><td>0.62</td></tr><tr><td>MiniCPM-128K[15]</td><td>2.4B</td><td>1.00</td><td>1.00</td><td>0.54</td><td>0.56</td></tr><tr><td>ChatGLM3-128K[47]</td><td>6B</td><td>0.94</td><td>0.72</td><td>0.52</td><td>0.44</td></tr><tr><td>YOCO-3B-1M</td><td>3B</td><td>0.98</td><td>0.98</td><td>0.84</td><td>0.56</td></tr></table></body></html>\n\nTable 4: Multi-needle retrieval accuracy. $N$ indicates the number of needles. $N = 1$ is single-needle retrieval used as a reference, and $N > 1$ indicates the multi-needle test. The evaluation is conducted in 128K length, because most previous long-context models are tuned with this length.\n\n![](images/fa1eecf18a1092b089b5d2846515b1c40166a10d05190b04eacba748e89c5a87.jpg)  \nFigure 5: Cumulative average negative log-likelihood on book and repository-level code. We filter the validation examples that are longer than 1M tokens. YOCO achieves improved performance with longer context, i.e., utilizing long-distance information for language modeling.   \nFigure 6: Breakdown memory consumption in 1M context length.\n\nPerplexity over Long Sequences Figure 5 shows the cumulative average negative log-likelihood (NLL) as a function of context length. We evaluate both book and repository-level code data. We follow the setting of [30] and filter validation data that are longer than 1M tokens. NLL decreases consistently with longer sequence length. The results indicate that YOCO can effectively utilize long-distance dependency for language modeling. We also observe that the NLL-length curves tend to fit the power law, where the gaps are affected by the noise within the validation examples.\n\n# 5.4 Inference Advantages\n\nWe analyze inference efficiency from various perspectives, such as GPU memory footprint, prefilling latency, throughput, and serving capacity. We show that YOCO reduces the deployment cost by orders of magnitude, especially for long-sequence inference. More importantly, the user experience (such as latency) is improved while maintaining good performance and reducing expenses.\n\nWe compare $\\mathrm { Y O C O } _ { \\mathrm { g R e t } }$ with Transformer. The default model configuration follows Section 5.1. Notice that Transformer uses grouped-query attention [1], Flash-Decoding [6], and kernel fusion for a fair comparison. As described in Section 4.2, gated retention uses the chunk-recurrent representation in the prefill stage, and the recurrent representation in the generation stage. The chunk size is set to 256. We implement a Triton [36] kernel for gated retention. The evaluation sequence length ranges from 32K to 1M. The last 1,024 tokens are supposed to be generated, while the previous tokens are given input context. The experiments are conducted with H100-80GB GPU cards.\n\nGPU Memory The inference memory consumption is made up of three parts, namely model weights, intermediate activation, and KV cache. Figure 6 presents the breakdown memory profiling results. Along with an increase in context length, the main memory bottleneck becomes KV caches, while model weights consume constant memory. The results show that $\\mathrm { Y O C O } _ { \\mathrm { g R e t } }$ alleviates the activation cost and KV cache memory footprint.\n\nKV Cache 5705 Weight Other 9.38x Transformer YOCO\n\nAs shown in Figure 7, the memory cost is significantly reduced using YOCO. Moreover, the memory consumption of YOCO increases slowly along the sequence length. For example of 1M length, the overall inference memory usage is only 12.4GB, while Transformers occupy $9 . 4 \\times$ GPU memory. YOCO makes it feasible to deploy long-sequence modeling on customer-level GPUs. Even with a 32K sequence length, YOCO requires about\n\n$2 \\times$ less memory than Transformer. Although we compare 3B-size models here, the reduction ratio becomes larger as the number of layers increases.\n\nFigure 8 reports the GPU memory consumption of KV cache for each token. As YOCO only caches one layer of global key-value pairs, it needs roughly $L$ times less memory compared to Transformer.\n\n![](images/bcf69ad9fdf1738c7c96c36d1551548d3c2612e303a5b448117bdce260d77741.jpg)  \nFigure 7: Inference memory of Transformer and YOCO across various lengths.\n\n![](images/73ed4abb87792e8cef96d3f0245d6ce3a5ad087f8323bcebc2a98128a8630172.jpg)  \nFigure 8: GPU memory of KV cache for each token with different model size.\n\n300 50 Transformer   \n③ 8.36x YOCO   \n2.87x5.05x 200   \n100 032K 64K 15.55x 128K30.3x 71.82x ? 0 32K 256K 512K 1M Length\n\n![](images/c270e3368b791c87894721beb41757c176f5782659f3572572f0a69f2b2f08cb.jpg)  \nFigure 9: Prefilling latency for different lengths. Transformer’s time grows quadratically while YOCO’s grows linearly.   \nFigure 10: Inference throughput of Transformer and YOCO varying the context length.\n\nFor example, YOCO can serve 128K tokens with 1GB GPU memory, while Transformer with GQA [1] can only support 1.6K tokens at 65B model size.\n\nPrefilling Latency In the prefill stage, the model encodes input tokens in parallel. As shown in Figure 9, the prefilling latency is a pain point of user experience for long-context models. For 512Kand 1M-length input sequences, Transformer needs about 180 seconds and 300 seconds, respectively. The computational complexity of Transformer is $\\mathcal { O } ( N ^ { 2 } )$ , which requires a large number of FLOPs for long context. In contrast, YOCO’s prefilling time is $\\dot { \\mathcal { O } } ( N )$ , growing linearly (Section 3.3) along the sequence length. Figure 9 shows that YOCO reduces the Transformer prefilling time from 180 seconds to less than 6 seconds for 512K context. As described in Section 3.3, the prefill stage can early exit before entering cross-decoder. So, there is at least two times speedup of prefilling latency even for short context. For example, YOCO is $2 . 8 7 \\times$ faster than Transformer for 32K length.\n\nThroughput The throughput indicates how many tokens the model can process per second, involving both pre-filling and generation time. Figure 10 shows that YOCO achieves higher throughput across context lengths compared to Transformer. For the example of 512K queries, Transformer’s throughput is 4.5 token/s while YOCO reaches 43.1 token/s, i.e., achieving $9 . 6 \\times$ speedup. The throughput is improved for the following reasons. First, YOCO decreases the time required for prefilling as previously demonstrated. Second, as the memory consumption is reduced, we can use larger batch size for inference, which also contributes to the throughput improvement.\n\n# 5.5 Comparisons with Transformer Variants\n\nWe compare $\\mathrm { Y O C O } _ { \\mathrm { g R e t } }$ and $\\mathtt { Y O C O _ { \\mathrm { S W A } } }$ with Transformer and other variants, including H3 [5], RetNet [35], Mamba [13], and gRetNet (Section 4.2). All models have 160M parameters with 12 layers and a hidden dimension of 768. The weights of word embedding and softmax projection are shared. For Mamba, we follow the details in [13], where double-SSM layers are implemented instead of “ $\\mathbf { \\ddot { S } S M } + \\mathbf { S }$ wiGLU”. For H3, the experiment uses a hybrid version following the original paper [5], where two attention layers are inserted after the first and $\\begin{array} { l } { { \\frac { L } { 2 } } } \\end{array}$ -th layers.\n\n![](images/3c32446f78a057d5e152c50c632db3717e55a1350bf2251efbbe988a193689b2.jpg)  \nFigure 11: Long sequence task perplexity decreases along with the increasing input length.\n\nFine-Grained LM Perplexity Table 5 reports the fine-grained validation perplexity for language modeling. Following Zoology [2], we divide the perplexity into “Ar-Hit” and “First-Occur”. Specifically, “Ar-Hit” considers the predicted tokens that are bigrams previously seen in the previous context, which evaluates the associative recall capability. “FirstOccur” considers the tokens that cannot be recalled from the context.\n\nTable 5: Fine-grained LM perplexity results.   \n\n<html><body><table><tr><td colspan=\"3\">Valid.Set↓ AR-Hit↓ First-Occur↓</td></tr><tr><td>Mamba [13]</td><td>3.645 1.555</td><td>4.126</td></tr><tr><td>RetNet [35]</td><td>3.633 1.466</td><td>4.131</td></tr><tr><td>Hybrid H3 [5]</td><td>3.591 1.251</td><td>4.130</td></tr><tr><td>gRetNet</td><td>3.600 1.354</td><td>4.116</td></tr><tr><td>Transformer</td><td>3.564 1.219</td><td>4.104</td></tr><tr><td>YOCOsWA</td><td>3.553</td><td>1.202 4.094</td></tr><tr><td>YOCOgRet</td><td>3.530 1.199</td><td>4.067</td></tr></table></body></html>\n\nLong-Context Evaluation Figure 11 reports the answer perplexity with varying context length (ranging from 4K to 16K) on the ZeroSCROLLS [31] benchmark. We continue training the above models in 16,384 length with 2B tokens. The rotation base scaling [44] is used for length extension. For sparse Transformer, we use the context window of 2,048 and keep RoPE $\\theta$ unmodified. As shown in Figure 11, YOCO and Transformer consistently outperform other methods across tasks and lengths, which is consistent with the findings in Section 5.3. Moreover, the results highlight the importance of global attention for long-context modeling. Notice that the 12K and 16K results in Qasper are similar because the lengths of most documents are shorter than 16K.\n\n# 5.6 Ablation Studies\n\nAs shown in Table 6, we explore different layout configurations for YOCO. First, we compare the ratio of self-decoder to cross-decoder layers. For example, $\\mathrm { Y O C O } _ { [ 1 : 1 ] }$ is the default setting, where each module contains $L / 2$ layers. The results show that YOCO[1:1] is comparable to $\\mathrm { Y O C O _ { [ 3 : 1 ] } }$ and outperforms both $\\mathrm { Y O C O _ { [ 1 : 3 ] } }$ and $\\mathrm { Y O C O _ { [ 0 : 1 ] } }$ . We use [1:1] as the default layout. Future work can refine a scaling law to guide the choice of\n\n<html><body><table><tr><td>Valid.Set↓</td><td>AR-Hit↓</td><td>First-Occur↓</td></tr><tr><td>YOCO[1:1]</td><td>3.530 1.199</td><td>4.067</td></tr><tr><td>YOCO[3:1]</td><td>3.526 1.207</td><td>4.060</td></tr><tr><td>YOCO[1:3]</td><td>3.565 1.230 3.898</td><td>4.102</td></tr><tr><td>YOCO[0:1]</td><td>1.827</td><td>4.374</td></tr><tr><td>Unstacked YOCO[1:1]</td><td>1.188</td><td>4.071</td></tr><tr><td>Interleaved&Hybrid</td><td>3.542 1.204</td><td>4.081</td></tr></table></body></html>\n\nTable 6: Fine-grained LM perplexity results. “[s:c]” is the ratio of self-decoder to cross-decoder layers.\n\nlayer ratio. Second, the setting “Unstacked $\\mathrm { Y O C O } _ { [ 1 : 1 ] }$ ” uses word embeddings $X ^ { 0 }$ as input to the cross-decoder, rather than stacking cross-decoder upon self-decoder (i.e., using $X ^ { L / 2 }$ in Equation (3)). Third, the model “Interleaved & Hybrid” is a hybrid architecture that interleaves gRetNet and Transformer layers.\n\n# 6 Conclusion\n\nIn this work, we propose a decoder-decoder architecture (YOCO) for large language modeling. YOCO achieves significantly better inference efficiency and competitive performance compared to Transformers. Experimental results demonstrate that YOCO achieves favorable results for large language models in various settings, i.e., scaling up the number of training tokens, scaling up model size, and scaling up context length to 1M tokens. Profiling results also show that YOCO improves inference efficiency by orders of magnitude, especially for long-sequence modeling.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n随着服务令牌数量的增加，键值（KV）缓存占用大量GPU内存，使得大语言模型的推理受限于内存；长序列输入的预填充延迟极高，这些瓶颈导致长上下文语言模型在实际部署中面临困难。此问题的重要性在于解决这些瓶颈能够推动长上下文语言模型在实际场景中的广泛应用，提升大语言模型在实际应用中的性能和效率。\\n\\n**方法概述**\\n论文提出了一种解码器 - 解码器架构YOCO，该架构仅缓存一次键值对，由自解码器和交叉解码器组成，自解码器高效编码并生成全局KV缓存，交叉解码器通过交叉注意力重用这些缓存。\\n\\n**主要贡献与效果**\\n- 显著减少GPU内存消耗，对于65B模型，KV缓存内存可减少约80倍；对于3B模型，32K令牌的整体推理内存消耗可减少2倍，1M令牌可减少9倍以上。\\n- 大幅加速预填充阶段，1M上下文的预填充阶段速度提高71.8倍，32K输入提高2.87倍，如512K上下文下，将Transformer的预填充延迟从180秒降低到不到6秒。\\n- 实现了良好的语言建模性能，3B的YOCO模型扩展到数万亿训练令牌时，取得了与著名的Transformer语言模型（如StableLM）相当的结果，将上下文长度扩展到1M令牌时，实现了近乎完美的针检索准确率。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nYOCO架构通过仅缓存一次键值对来减少GPU内存需求，同时保留全局注意力能力。自解码器利用高效的自注意力计算中间向量表示并生成全局KV缓存，交叉解码器通过交叉注意力重用这些缓存，整个架构的计算流程允许在预填充阶段提前退出而不改变最终输出，从而显著提高推理效率。\\n\\n**创新点**\\n先前的工作主要集中在减少KV缓存、采用稀疏注意力机制或丢弃KV缓存来减少KV缓存，但YOCO保留一个全局KV缓存并进行全交叉注意力，以实现更好的长上下文建模。此外，YOCO的计算流程特性能够极大地加速预填充阶段，这是先前方法所没有的特性。\\n\\n**具体实现步骤**\\n1. 输入序列 $x = x _ { 1 } \\cdots x _ { | x | }$ 的嵌入被打包成 $X ^ { 0 } = [ \\mathbf { x } _ { 1 }, \\cdots, \\mathbf { x } _ { | x | } ] \\in \\mathbb { R } ^ { | x | \\times d _ { \\mathrm { model } } }$ ，将其作为自解码器的输入。\\n2. 自解码器计算中间向量表示 $X^{L/2}$，公式为 $Y^l = \\mathrm{ESA}(\\mathrm{LN}(X^l)) + X^l$ 和 $X^{l + 1} = \\mathrm{SwiGLU}(\\mathrm{LN}(Y^l)) + Y^l$，其中 $\\mathrm{ESA}(\\cdot)$ 表示高效自注意力，$\\mathrm{SwiGLU}(X) = (\\mathrm{swish}(XW_G) \\odot XW_1)W_2$，$\\mathrm{LN}(\\cdot)$ 使用RMSNorm。\\n3. 由自解码器的输出 $X^{L/2}$ 生成全局KV缓存 $\\hat{K}$ 和 $\\hat{V}$，公式为 $\\hat{K} = \\mathrm{LN}(X^{L/2})W_K$ 和 $\\hat{V} = \\mathrm{LN}(X^{L/2})W_V$。\\n4. 交叉解码器计算 $X ^ { l } = \\mathrm{Cross - Decoder} ( X ^ { l - 1 }, \\hat { K }, \\hat { V } ) , l \\in [ \\frac { L } { 2 } + 1, L ]$ 以得到输出向量 $X ^ { L }$ 。具体计算为 $\\boldsymbol { Q } ^ { l } = \\mathrm{LN} ( \\boldsymbol { X } ^ { l } ) \\boldsymbol { W } _ { \\boldsymbol { Q } } ^ { l }$ ， $\\boldsymbol { Y } ^ { l } = \\mathrm{Attention} ( \\boldsymbol { Q } ^ { l }, \\boldsymbol { \\hat { K } }, \\boldsymbol { \\hat { V } } ) + \\boldsymbol { X } ^ { l }$ ， $\\boldsymbol { X } ^ { l + 1 } = \\mathrm{SwiGLU} ( \\mathrm{LN} ( \\boldsymbol { Y } ^ { l } ) ) + \\boldsymbol { Y } ^ { l }$ ，其中 $\\mathrm{Attention}(\\cdot)$ 是标准多头注意力。\\n5. 获得 $X ^ { L }$ 后，通过softmax分类器对词汇表进行下一个令牌预测。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\nTransformer、OpenLLaMA-v2-3B、StableLM-base-alpha-3B-v2、StableLM-3B-4E1T、MiniCPM128K、ChatGLM3-128K、YaRN-Mistral-128K、LWM-1M-text、H3、RetNet、Mamba、gRetNet。\\n\\n**性能对比**\\n*   **在语言建模评估指标上**：在训练1T和1.6T令牌时，YOCO - 3B的平均准确率分别为0.634和0.636，优于OpenLLaMA - 3B - v2（0.619）和StableLM - alpha - 3B - v2（0.612），与StableLM - 3B - 4E1T相当。将YOCO - 3B扩展到1M上下文长度时，平均准确率达到0.645。在多个下游任务的零样本性能上，3B的YOCO模型在训练1T和1.6T令牌的情况下取得了与OpenLLaMA-v2-3B、StableLM-base-alpha-3B-v2、StableLM-3B-4E1T相当的结果。在多针检索实验中，对于单针检索，YOCO-3B-1M准确率达到0.98，优于ChatGLM3-128K的0.94、YaRN-Mistral-128K的0.02；对于多针检索，在不同针数量下也表现出较好的性能，如4针时准确率为0.84，优于ChatGLM3-128K的0.52、YaRN-Mistral-128K的0.08。\\n*   **在可扩展性指标上**：从160M到13B模型大小的缩放曲线显示，YOCO与Llama优化的Transformer架构性能相当，其中 $YOCO_{gRet}$ 优于Transformer和 $YOCO_{SWA}$。\\n*   **在长上下文评估指标上**：在Needle - In - A - Haystack测试中，YOCO - 3B - 1M的准确率接近完美。在多针检索评估中，对于不同数量的针，YOCO - 3B - 1M与LWM - 1M - text性能相当，但模型大小仅为其一半，且优于MiniCPM - 128K、ChatGLM3 - 128K和YaRN - Mistral - 128K。\\n*   **在推理效率指标上**：\n    - **GPU内存方面**：在1M长度时，YOCO的整体推理内存使用仅为12.4GB，是Transformer的约1/9.4；在32K序列长度时，YOCO所需内存约为Transformer的1/2。对于65B模型，YOCO的KV缓存内存可减少约80倍；对于3B模型，32K令牌时整体推理内存消耗可减少2倍，1M令牌时可减少9倍以上。\n    - **预填充延迟方面**：对于512K上下文，YOCO将Transformer的预填充延迟从180秒降低到不到6秒；对于32K长度，YOCO比Transformer快2.87倍。\n    - **吞吐量方面**：对于512K查询，YOCO的吞吐量为43.1 token/s，是Transformer（4.5 token/s）的约9.6倍。\n*   **在细粒度语言建模困惑度指标上**：$YOCO_{gRet}$ 的“Ar - Hit”和“First - Occur”困惑度分别为3.530和4.067，优于其他模型，包括Transformer（3.564和4.104）。\",\n    \"keywords\": \"### 关键词\\n\\n- 大语言模型 (Large Language Models, LLMs)\\n- 解码器 - 解码器架构 (Decoder - Decoder Architecture, N/A)\\n- 键值缓存 (Key - Value Caches, KV Caches)\\n- 推理效率 (Inference Efficiency, N/A)\\n- YOCO架构 (You Only Cache Once, YOCO)\\n- 长上下文建模 (Long - Context Modeling, N/A)\"\n}"
}