{
    "link": "https://arxiv.org/abs/2401.13858",
    "pdf_link": "https://arxiv.org/pdf/2401.13858",
    "title": "Graph Diffusion Transformers for Multi-Conditional Molecular Generation",
    "authors": [
        "Gang Liu",
        "Jiaxin Xu",
        "Te Luo",
        "Meng Jiang"
    ],
    "institutions": [
        "Meng Jiang University of Notre Dame"
    ],
    "publication_date": "2024-01-24",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science",
        "Biology"
    ],
    "citation_count": 10,
    "influential_citation_count": 1,
    "paper_content": "# Graph Diffusion Transformers for Multi-Conditional Molecular Generation\n\nGang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang University of Notre Dame gliu7, jxu24, tluo, mjiang2 @nd.edu\n\n# Abstract\n\nInverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecular generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We present the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation. Graph DiT integrates an encoder to learn numerical and categorical property representations with the Transformer-based denoiser. Unlike previous graph diffusion models that add noise separately on the atoms and bonds in the forward diffusion process, Graph DiT is trained with a novel graph-dependent noise model for accurate estimation of graph-related noise in molecules. We extensively validate Graph DiT for multi-conditional polymer and small molecule generation. Results demonstrate the superiority of Graph DiT across nine metrics from distribution learning to condition control for molecular properties. A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility.\n\n# 1 Introduction\n\nDiffusion models for molecular graphs are essential for inverse design of materials and drugs by generating molecules and polymers (macro-molecules) [40, 46], because the models can be effectively trained to predict discrete graph structures and atom/bond types in denoising processes [43]. Practical inverse designs consider multiple factors such as molecular synthetic score and various properties [15], known as the task of multi-conditional graph generation.\n\nExisting work converted multiple conditions into a single one and solved the task as single-condition generation [5, 25]. However, multi-property relations may not be properly or explicitly defined [5]. First, the properties have diverse scales and units. For example, the synthetic complexity ranges from 1 to 5 [8], while the gas permeability varies widely, exceeding 10,000 in Barrier units [4]. This gap makes it hard for models to balance the conditions. Second, multi-conditions consist of a mix of categorical and numerical properties. The common practice of addition [47] or multiplication [25] is inadequate for combination.\n\nFigure 1(a) empirically illustrates the challenges in multi-conditional generation, i.e., discovering molecules meeting multiple properties. We used a test set of 100 data points with three properties: synthesizability (Synth.) [12], $\\mathrm { O _ { 2 } }$ and $\\Nu _ { 2 }$ permeability $\\mathrm { \\langle O _ { 2 } P e r m }$ and $\\mathrm { { N _ { 2 } P e r m } , }$ [4]. A single-conditional diffusion model generated up to 30 graphs for each condition, resulting in a total of 90 graphs for three conditions. We sort the 30 graphs in each set using a polymer property Oracle (see appendix B.3). Then, we check whether a shared polymer structure that meets multi-property constraints can be identified across different condition sets. If we find the polymer, its rank $K$ (where $K$ is between 1 and 30) indicates how high it appears on the lists, considering all condition sets. If not, we set $K$ as 30. Figure 1(a) shows the frequency distribution of $K$ on the 100 test cases. The median $K$ was\n\n![](images/6eee835ca1a1ffabf49cf40d49ada01aca58329b500f6ffc5402cafb112f7b06.jpg)  \n(a) Existing work’s limitation: A median rank of 30 showed that on fewer than half test polymers, the sets of generated graphs from different single conditions intersected, indicating a failure to generate polymers meeting multiple properties.\n\n![](images/bc50dc8a55cf520580600722c1c06fdf71f7c3cbfece7e6bbad2811b061646b8.jpg)  \n(b) Proposed work: Our idea, multiconditional guidance for diffusion models, successfully generated polymers that satisfied multi-property constraints. It achieved a higher rank than 30 in any set of the single-conditional generated graphs.\n\nFigure 1: Multi-conditional diffusion guidance in (b) generates polymers of higher property accuracy than existing work in (a). Explanations are in Section 1 and details are in appendix B.3.\n\n30, indicating that the multiple properties were not met on over half of the test polymers despite generating a large number of graphs.\n\nTo address these challenges, we project multi-properties into representations by learning, thereby guiding the diffusion process for molecule generation. We propose the Graph Diffusion Transformer (Graph DiT) for graph denoising under conditions. Graph DiT has a condition encoder for property representation learning and a graph denoiser. The condition encoder utilizes a novel clustering-based method for numerical properties and one-hot encoding for categorical ones to learn multi-property representations. The graph denoiser first integrates node and edge features into graph tokens, then denoise these tokens with adaptive layer normalization (AdaLN) in Transformer layers [19, 34]. AdaLN replaces the molecular statistics (mean and variance) in each hidden layer with those from the condition representation, effectively outperforming other predictor-based and predictor-free conditioning methods [22, 43, 34], as shown in Section 4.4. We observe that existing forward diffusion processes [43, 22] apply noise separately to atoms and bonds, which may compromise the accuracy of Graph DiT in noise estimation. Hence, we propose a novel graph-dependent noise model that effectively applies noise tailored to the dependencies between atoms and bonds within the graph.\n\nResults in Figure 1(b) show that the polymers generated by Graph DiT closely align with multiproperty constraints. For each test case, we have one graph generated from Graph DiT conditional on three properties. The Oracle determines the rank of this graph among 30 single-conditionally generated graphs for each condition. We find the median ranks are 4, 9, and 11, for Synth., $\\mathrm { O _ { 2 } }$ Perm, and $\\Nu _ { 2 }$ Perm, respectively, all much higher than 30. Note that the ranked set of 30 graphs was very competitive because the model was trained on the specific condition dedicatedly.\n\nIn experiments, we evaluate model performance on one polymer and three small molecule datasets. The polymer dataset includes four numerical conditions for multi-conditional evaluation. Our model has the lowest average mean absolute error (MAE), significantly reducing the error by $1 7 . 8 6 \\%$ compared to the best baseline. It also excels in small molecule tasks, achieving over 0.9 accuracy on task-related categorical conditions, notably surpassing the baseline accuracy of less than 0.6. We also examine the model’s utility in inverse polymer designs for $\\mathrm { O _ { 2 } / N _ { 2 } }$ gas separation, with domain expert feedback highlighting our model’s practical utility in multi-conditional molecular design.\n\n# 2 Problem Definition\n\n# 2.1 Multi-Conditional Inverse Molecular Design\n\nA molecular graph $G = ( V , E )$ consists of a set of nodes (atoms) $V$ and edges (bonds) $E$ . We follow [43] and define “non-bond” as a type of edge. There are $N$ atoms and each atom has a one-hot encoding, denoting the atom type. We represent it as $\\mathbf { X } _ { V } \\in \\mathbb { R } ^ { N \\times F _ { V } }$ , where $F _ { V }$ is the total number of atom types. Similarly, the bond features are a tensor $\\mathbf { X } _ { E } \\in \\mathbb { R } ^ { N \\times N \\times F _ { E } }$ , representing both the graph structure and $F _ { E }$ bond types.\n\nLet $\\mathcal { C } = \\{ c _ { 1 } , c _ { 2 } , \\dots , c _ { M } \\}$ be a set of $M$ numerical and categorical conditions. The task is: $q ( G \\mid$ $c _ { 1 } , c _ { 2 } , \\ldots , c _ { M } ) \\propto q ( G ) q ( c _ { 1 } , c _ { 2 } , \\ldots , c _ { M } \\mid G ) ,$ where $q$ represents observed probability. We use a model parameterized by $\\theta$ for multi-conditional molecular generation $p _ { \\theta } ( G \\mid { \\mathcal { C } } )$ . The evaluation involves both distribution learning $q ( G )$ [35] and condition control $q ( c _ { 1 } , c _ { 2 } , \\ldots , c _ { M } \\mid G )$ . We follow previous work in assuming that there exist different oracle functions $\\mathcal { O }$ that can independently evaluate each conditioned property [14]: $\\begin{array} { r } { q ( c _ { 1 } , c _ { 2 } , \\ldots , c _ { M } \\mid G ) = \\prod _ { i = 1 } ^ { M } \\mathcal { O } _ { i } ( c _ { i } \\mid G ) } \\end{array}$ . Note that the oracles are not used in the training of $p _ { \\theta }$ .\n\n# 2.2 Diffusion Model on Graph Data\n\nDiffusion models consist of forward and reverse diffusion processes [17]. We refer to the forward diffusion process as the diffusion process following [17]. The diffusion process $q ( G ^ { 1 : T } \\mid G ^ { 0 } ) =$ $\\textstyle \\prod _ { t = 1 } ^ { T } q ( G ^ { t } \\mid G ^ { t - 1 } )$ corrupts molecular graph data ${ \\bf \\tilde { \\it G } ^ { 0 } } = { \\bf \\tilde { \\it G } } ,$ ) into noisy states $G ^ { t }$ . As timesteps $T \\to \\infty$ , $q ( G ^ { T } )$ converges a stationary distribution $\\pi ( G )$ . The reverse Markov process $p _ { \\theta } ( G ^ { 0 : T } ) =$ $\\begin{array} { r } { q ( G ^ { T } ) \\prod _ { t = 1 } ^ { T } p _ { \\theta } ( G ^ { t - 1 } \\mid G ^ { t } ) } \\end{array}$ , parameterized by neural networks, gradually denoises the latent states toward  the desired data distribution.\n\nDiffusion Process One may perturb $G ^ { t }$ in a discrete state-space to capture the structural properties of molecules [43]. Two transition matrices $\\mathbf { Q } _ { V } \\in \\mathbb { R } ^ { F _ { V } \\times F _ { V } ^ { \\bullet } }$ and $\\mathbf { Q } _ { E } ^ { \\bf \\perp } \\in \\mathbb { R } ^ { F _ { E } \\times F _ { E } }$ are defined for nodes $\\mathbf { X } _ { V }$ and edges $\\mathbf { X } _ { E }$ , respectively [43]. Then, each step $q ( G ^ { t } \\mid G ^ { t - 1 } , G ^ { 0 } ) = q ( G ^ { t } \\mid G ^ { t - 1 } )$ in the diffusion process is sampled as follows.\n\n$$\n\\begin{array}{c} \\begin{array} { r } { \\left\\{ q ( \\mathbf { X } _ { V } ^ { t } \\mid \\mathbf { X } _ { V } ^ { t - 1 } ) = \\operatorname { C a t } \\left( \\mathbf { X } _ { V } ^ { t } ; \\mathbf { p } = \\mathbf { X } _ { V } ^ { t - 1 } \\mathbf { Q } _ { V } ^ { t } \\right) , \\right.} \\\\ { q ( \\mathbf { X } _ { E } ^ { t } \\mid \\mathbf { X } _ { E } ^ { t - 1 } ) = \\operatorname { C a t } \\left( \\mathbf { X } _ { E } ^ { t } ; \\mathbf { p } = \\mathbf { X } _ { E } ^ { t - 1 } \\mathbf { Q } _ { E } ^ { t } \\right) , } \\end{array}   \\end{array}\n$$\n\nwhere $\\operatorname { C a t } ( \\mathbf { X } ; \\mathbf { p } )$ denotes sampling from a categorical distribution with probability $\\mathbf { p }$ . We remove the subscript $\\left( \\boldsymbol { \\mathbf { \\mathit { \\Sigma } } } _ { V / E } \\right)$ when the description applies to both nodes and edges. It is assumed that the noise $\\mathbf { Q } ^ { i }$ $\\mathit { i } \\leq t )$ is independently applied to $\\mathbf { X }$ in each step $i$ , allowing us to rewrite $q ( \\mathbf { X } ^ { t } \\mid \\mathbf { X } ^ { t - 1 } )$ as the probability of the initial state $q ( { \\mathbf { X } ^ { t } } \\mid { \\mathbf { X } ^ { 0 } } ) = \\operatorname { C a t } \\left( { \\mathbf { X } ^ { t } } ; { \\mathbf { p } } = { \\mathbf { X } ^ { 0 } } { \\bar { \\mathbf { Q } } } ^ { t } \\right)$ , where $\\begin{array} { r } { \\bar { \\mathbf { Q } } ^ { t } = \\prod _ { i \\leq t } \\mathbf { Q } ^ { i } } \\end{array}$ .\n\nNoise Scheduling Transition matrices $\\mathbf { Q } _ { V }$ and $\\mathbf { Q } _ { E }$ control the noise applied to atom features and bond features, respectively. Vignac et al. [43] defined $\\pi ( G ) = ( \\mathbf { m } _ { X } ^ { \\texttt { \\textbf { i } } } \\in \\mathbb { R } ^ { F _ { V } } , \\mathbf { m } _ { E } \\in \\mathbb { R } ^ { F _ { E } } )$ as the marginal distributions of atom types and bond types. The transition matrix at timestep $t$ is $\\mathbf { Q } ^ { t } = \\alpha ^ { t } \\mathbf { I } \\bar { + } ( 1 - \\alpha ^ { t } ) \\mathbf { 1 } \\mathbf { m } ^ { \\prime }$ for atoms or bonds, where $\\mathbf { m ^ { \\prime } }$ denotes the transposed row vector. Therefore, we have $\\bar { \\mathbf { Q } } ^ { t } = \\bar { \\alpha } ^ { t } \\mathbf { I } + ( 1 - \\bar { \\alpha } ^ { t } ) \\mathbf { 1 } \\mathbf { m } ^ { \\prime }$ , where $\\begin{array} { r } { \\bar { \\alpha } ^ { t } = \\prod _ { \\tau = 1 } ^ { t } \\alpha ^ { \\tau } } \\end{array}$ . The cosine schedule [32] is often chosen for $\\bar { \\alpha } ^ { t } = \\cos ( 0 . 5 \\pi ( t / T + s ) / ( 1 + s ) ) ^ { 2 }$ .\n\nReverse Process With the initial sampling $G ^ { T } \\sim \\pi ( G )$ , the reverse process generates $G ^ { 0 }$ iteratively in reversed steps $t = T , T - 1 , \\dots , 0$ . We use a neural network to predict the probability $p _ { \\theta } ( { \\tilde { G } } ^ { 0 } \\mid G ^ { t } )$ as the product over nodes and edges [1, 43]:\n\n$$\np _ { \\theta } ( \\tilde { G } ^ { 0 } \\mid G ^ { t } ) = \\prod _ { v \\in V } p _ { \\theta } ( v ^ { t - 1 } \\mid G ^ { t } ) \\prod _ { e \\in E } p _ { \\theta } ( e ^ { t - 1 } \\mid G ^ { t } )\n$$\n\n$p _ { \\theta } ( { \\tilde { G } } ^ { 0 } \\mid G ^ { t } )$ could be combined with $q ( G ^ { t - 1 } \\mid G ^ { t } , G ^ { 0 } )$ to estimate the reverse distribution on the graph $p _ { \\theta } ( G ^ { t - 1 } \\mid G ^ { t } )$ . For example, $p _ { \\theta } ( v ^ { t - 1 } \\mid G ^ { t } )$ is marginalized over predictions of node types $\\tilde { v } \\in \\tilde { \\mathbf { x } } _ { v }$ , which applies similarly to edges:\n\n$$\np _ { \\theta } \\big ( v ^ { t - 1 } \\mid G ^ { t } \\big ) = \\sum _ { \\tilde { v } \\in \\tilde { \\mathbf { x } } _ { v } } q \\big ( v ^ { t - 1 } \\mid \\tilde { v } , G ^ { t } \\big ) p _ { \\theta } \\big ( \\tilde { v } \\mid G ^ { t } \\big ) .\n$$\n\nThe neural network could be trained to minimize the negative log-likelihood [43].\n\n$$\nL = \\mathbb { E } _ { q ( G ^ { 0 } ) } \\mathbb { E } _ { q ( G ^ { t } \\mid G ^ { 0 } ) } \\left[ - \\mathbb { E } _ { \\mathbf { x } \\in G ^ { 0 } } \\log p _ { \\theta } \\left( \\mathbf { x } \\mid G ^ { t } \\right) \\right]\n$$\n\n![](images/dd3194f886bdd26f905e049e729ccca948112513a048a5728b256a2c1dea722d.jpg)  \nFigure 2: Denoising framework and architectures for Graph DiT. Details are in Section 3.2.\n\nwhere $\\mathbf { x } \\in G ^ { 0 }$ denotes the node or edge features. Typically, the reverse process in diffusion models does not consider molecular properties as conditions. While there have been efforts to introduce property-related guidance using additional predictors, the more promising approach of predictor-free guidance [16], particularly in multi-conditional generation, remains underexplored.\n\n# 3 Multi-Conditional Graph Diffusion Transformers\n\nWe present the denoising framework of Graph DiT in Figure 2. The condition encoder learns the representation of $M$ conditions. The statistics of this representation like mean and variance are used to replace the ones from the molecular representations [19] (see Section 3.2). Besides, we introduce a new noise model in the diffusion process to better fit graph-structured molecules (see Section 3.1).\n\n# 3.1 Graph-Dependent Noise Models\n\nThe transition probability of a node or an edge should rely on the joint distribution of nodes and edges in the prior state. However, as an example shown in Eq. (1), current diffusion models [22, 43, 25] treat node and edge state transitions as independent, misaligning with the denoising process in Eq. (3). This difference between the sampling distributions of noise in the diffusion and reverse processes introduces unnecessary challenges to multi-conditional molecular generations.\n\nTo address this, we use a single matrix $\\mathbf { X } _ { G } ~ \\in ~ \\mathbb { R } ^ { N \\times F _ { G } }$ to represent graph tokens for $G$ , with $F _ { G } = F _ { V } + N \\cdot F _ { E }$ . Token representations are created by concatenating the node feature matrix $\\mathbf { X } _ { V }$ and the flattened edge connection matrix from $\\mathbf { X } _ { E }$ . Each row vector in $\\mathbf { X } _ { G }$ contains features for both nodes and edges, representing all connections and non-connections. Hence, we could design a transition matrix $\\mathbf { Q } _ { G }$ considering the joint distribution of nodes and edges. $\\mathbf { Q } _ { G } \\in \\mathbb { R } ^ { F _ { G } \\times F _ { G } }$ is constructed from four matrices $\\mathbf { Q } _ { V } , \\mathbf { \\tilde { Q } } _ { E V } \\stackrel { \\sim } { \\in } \\mathbb { R } ^ { F _ { E } \\times F _ { V } } , \\mathbf { Q } _ { E } , \\mathbf { Q } _ { V E } \\in \\mathbb { R } ^ { F _ { V } \\times F _ { E } ^ { \\asymp } }$ , denoting the transition probability (“dependent old state” $$ “target new state”) node $$ node; edge $$ node; edge $$ edge; node $$ edge, respectively.\n\n$$\n\\mathbf { Q } _ { G } = \\left[ \\mathbf { Q } _ { V } \\mathbf { 1 } _ { N } ^ { \\prime } \\otimes \\mathbf { Q } _ { V E } \\right] ,\n$$\n\nwhere $\\otimes$ denotes the Kronecker product, $\\mathbf { 1 } _ { N } , \\mathbf { 1 } _ { N } ^ { \\prime }$ , and ${ \\mathbf { 1 } } _ { N \\times N }$ represent the column vector, row vector, and matrix with all 1 elements, respectively. According to Eq. (5), the first $F _ { V }$ columns in $\\mathbf { Q } _ { G }$ determine the node feature transitions based on both node features (first $F _ { V }$ rows) and edge features (remaining $N \\cdot F _ { E }$ rows). Conversely, the remaining $N \\cdot F _ { E }$ columns determine the edge feature transitions, depending on the entire graph. We introduce a new diffusion noise model:\n\n$$\nq ( \\mathbf { X } _ { G } ^ { t } \\mid \\mathbf { X } _ { G } ^ { t - 1 } ) = \\widetilde { \\mathrm { C a t } } \\left( \\mathbf { X } _ { G } ^ { t } ; \\tilde { \\mathbf { p } } = \\mathbf { X } _ { G } ^ { t - 1 } \\mathbf { Q } _ { G } ^ { t } \\right) ,\n$$\n\nwhere $\\tilde { \\mathbf { p } }$ is the unnormalized probability and $\\widetilde { \\mathrm { C a t } }$ denotes categorical sampling: The first $F _ { V }$ columns of $\\tilde { \\mathbf { p } }$ are normalized to sample $\\mathbf { X } _ { V } ^ { t }$ , while the regmaining $N { \\cdot } E$ dimensions are reshaped and normalized to sample edges $\\mathbf { X } _ { E } ^ { t }$ . These components are combined to form $\\mathbf { X } _ { G } ^ { t }$ , completing the $\\widetilde { \\mathrm { C a t } }$ sampling.\n\nChoice of $\\mathbf { Q } _ { V E }$ and $\\mathbf { Q } _ { E V }$ Similar to the definitions of $\\mathbf { m } _ { V }$ and $\\mathbf { m } _ { E }$ [43], we levegrage the prior knowledge within the training data for the formulation of task-specific matrices, $\\mathbf { Q } _ { E V }$ and $\\mathbf { Q } _ { V E }$ . We calculate co-occurrence frequencies of atom and bond types in training molecular graphs to obtain the marginal atom-bond co-occurrence probability distribution. For each bond type, each row in $\\mathbf { m } _ { E V }$ represents the probability of co-occurring atom types. $\\mathbf { m } _ { V E }$ is the transpose of $\\mathbf { m } _ { E V }$ and has a similar meaning. Subsequently, we define $\\mathbf { Q } _ { E V } = \\bar { \\alpha } ^ { t } \\mathbf { I } + \\bigl ( 1 - \\bar { \\alpha } ^ { t } \\bigr ) \\mathbf { 1 } \\mathbf { m } _ { E V } ^ { \\prime }$ and $\\mathbf { \\bar { Q } } _ { V E } = \\bar { \\alpha } ^ { t } \\mathbf { I } + ( 1 - \\bar { \\alpha } ^ { t } ) \\mathbf { 1 } \\mathbf { m } _ { V E } ^ { \\prime }$\n\n# 3.2 Denoising Models with Multi-Property Conditions\n\nWe present Graph DiT as the denoising model to generate molecules under multi-conditions ${ \\mathcal { C } } =$ $\\{ c _ { 1 } , \\stackrel { - } { c } _ { 2 } , \\ldots , c _ { M } \\bar  \\}$ without extra predictors.\n\nPredictor-Free Guidance The predictor-free reverse process $\\hat { p } _ { \\boldsymbol { \\theta } } ( G ^ { t - 1 } \\mid G ^ { t } , \\mathcal { C } )$ aims to generate molecules with a high probability $\\overset { \\cdot } { q } ( \\mathcal { C } \\mid G ^ { 0 } )$ . This could be achieved by a linear combination of the log probability for unconditional and conditional denoising [16]:\n\n$$\n\\hat { p } _ { \\theta } ( G ^ { t - 1 } \\mid G ^ { t } , \\mathcal { C } ) = \\log p _ { \\theta } ( G ^ { t - 1 } \\mid G ^ { t } ) + s \\left( \\log p _ { \\theta } ( G ^ { t - 1 } \\mid G ^ { t } , \\mathcal { C } ) - \\log p _ { \\theta } ( G ^ { t - 1 } \\mid G ^ { t } ) \\right) ,\n$$\n\nwhere $s$ denotes the scale of conditional guidance. Unlike classifier-free guidance [16], which typically predicts noise, we directly estimate $p _ { \\theta } ( \\tilde { G } ^ { 0 } \\mid G ^ { t } , { \\mathcal { C } } )$ . We one one denoising model $f _ { \\theta } ( G ^ { t } , \\mathcal { C } )$ for both $p _ { \\theta } ( { \\tilde { G } } ^ { 0 } \\mid G ^ { t } )$ and $p _ { \\theta } ( { \\tilde { G } } ^ { 0 } \\mid G ^ { t } , { \\mathcal { C } } )$ . Here, $f _ { \\theta } ( G ^ { t } , { \\mathcal { C } } = \\varnothing )$ computes the unconditional probability by substituting the original conditional embeddings with the null value. During training, we randomly drop the condition with a ratio, i.e., ${ \\mathcal { C } } = \\emptyset$ , to learn the embedding of the null value. $f _ { \\theta } ( G ^ { t } = \\mathbf { X } _ { G } ^ { \\dot { t } } , \\mathcal { C } )$ comprises two components: the condition encoder and the graph denoiser. An overview of the architecture is presented in Figure 2.\n\nCondition Encoder We treat the timestep $t$ as a special condition and follow [31] to obtain a $D$ - dimensional representation t with sinusoidal encoding. For property-related numerical or categorical condition $c _ { i } \\in \\mathcal { C }$ , we apply distinct encoding operations to get $D$ -dimensional representation. For a categorical condition, we use the one-hot encoding. For a numerical variable, we introduce a clustering encoding method. This defines learnable centroids, assigning $c _ { i }$ to clusters, and transforming the soft assignment vector of condition values into the representation. It could be implemented using two Linear layers and a Softmax layer in the middle as: Linear (Softmax $\\left( { \\mathrm { L i n e a r } } ( c _ { i } ) \\right) ,$ ). Finally, we could obtain the representation of the condition as $\\begin{array} { r } { \\mathbf { c } = \\sum _ { i = 1 } ^ { M } \\mathrm { e n c o d e } ( c _ { i } ) } \\end{array}$ , where encode is the specific encoding method based on the condition type. For numerical conditions, we evaluate our proposed clustering-based approach against alternatives like direct or interval-based encodings [28]. As noted in Section 4.4, the clustering encoding outperforms the other methods.\n\nGraph Denoiser: Transformer Layers Given the noisy graph at timestep $t$ , the graph tokens are first encoded into the hidden space as $\\mathbf { H } = \\mathrm { L i n e a r } ( \\mathbf { X } _ { G } ^ { t } )$ , where $\\mathbf { H } \\in \\mathbb { R } ^ { N \\times D }$ . We then adapt the standard Transformer layers [42] with self-attention and multi-layer perceptrons (MLP), but replace the normalization with the adaptive layer normalization (AdaLN) controlled by the representations of the conditions [19, 34]: ${ \\bf H } \\bar { = } \\mathrm { A d a L N } ( { \\bf H } , { \\bf c } )$ . For each row $\\mathbf { h }$ in $\\mathbf { H }$ :\n\n$$\n\\mathrm { A d a L N } \\left( \\mathbf { h } , \\mathbf { c } \\right) = \\gamma _ { \\theta } ( \\mathbf { c } ) \\odot \\frac { \\mathbf { h } - \\mu \\left( \\mathbf { h } \\right) } { \\sigma \\left( \\mathbf { h } \\right) } + \\beta _ { \\theta } ( \\mathbf { c } ) ,\n$$\n\nwhere $\\mu ( \\cdot )$ and $\\sigma ( \\cdot )$ are mean and variance values. $\\odot$ indicates element-wise product. $\\gamma _ { \\theta } ( \\cdot )$ and $\\beta _ { \\theta } ( \\cdot )$ are neural network modules in $f _ { \\theta } ( \\cdot )$ , each of which consists of two linear layers with SiLU activation [11] in the middle. We have a gated variant $\\mathrm { A d a L N } _ { g a t e }$ for residuals:\n\n$$\n\\mathrm { A d a L N } _ { g a t e } \\left( \\mathbf { h } , \\mathbf { c } \\right) = \\alpha _ { \\theta } ( \\mathbf { c } ) \\odot \\mathrm { A d a L N } \\left( \\mathbf { h } , \\mathbf { c } \\right)\n$$\n\nWe apply the zero initialization for the first layer of $\\gamma _ { \\theta } ( \\cdot ) , \\beta _ { \\theta } ( \\cdot )$ , and $\\alpha _ { \\theta } ( \\cdot )$ [34]. There are other options to learn the structure representation from the condition [34]: In-Context conditioning adds condition representation to the structure representation at the beginning of the structure encoder, and Cross-Attention calculates cross-attention between the condition and structure representation. We observe in Section 4.4 that AdaLN performs best among them.\n\nGraph Denoiser: Final MLP We have the hidden states $\\mathbf { H }$ after the final Transformer layers, the MLP is used to predict node probabilities $\\tilde { \\mathbf { X } } _ { V } ^ { 0 }$ and edge probabilities $\\tilde { \\mathbf { X } } _ { E } ^ { 0 }$ at $t = 0$ :\n\n$$\n\\tilde { \\bf X } _ { G } ^ { 0 } = { \\mathrm { A d a L N } } ( { \\mathrm { M L P } } ( { \\bf H } ) , { \\bf c } ) .\n$$\n\nWe split the output $\\mathbf { X } _ { G }$ into atom and bond features $\\tilde { \\bf X } _ { V } ^ { 0 } , \\tilde { \\bf X } _ { E } ^ { 0 }$ . The first $F _ { V }$ dimensions of $\\tilde { \\mathbf { X } } _ { G } ^ { 0 }$ represent node type probabilities, and the remaining $N \\cdot F _ { E }$ dimensions cover probabilities for $N$ edge types associated with the node, as detailed in Section 3.1.\n\nGeneration to Molecule Conversion A common way of converting generated graphs to molecules selects only the largest connected component [43], denoted as Graph DiT-LCC in our model. For Graph DiT, we connect all components by randomly selecting atoms. It minimally alters the generated structure to more accurately reflect model performance than Graph DiT-LCC.\n\nTable 1: Multi-Conditional Generation of 10K Polymers: Results on the synthetic score (Synth.) and three numerical properties (gas permeability for $\\mathrm { O _ { 2 } }$ , $\\mathbf { N } _ { 2 }$ , $\\mathrm { C O _ { 2 } }$ ). MAE is calculated between the input conditions and the properties of the generated polymers using Oracles. Best results are highlighted.   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Validity ↑ (w/o rule checking)</td><td colspan=\"4\">Distribution Learning</td><td colspan=\"5\">Condition Control</td></tr><tr><td>Coverage ↑</td><td>Diversity ↑</td><td>Similarity ↑</td><td>Distance ↓</td><td>Synth.↓</td><td>O2Perm↓</td><td>N2Perm↓</td><td>CO2Perm↓</td><td>Avg. MAE ↓</td></tr><tr><td>Graph GA</td><td>1.0000 (N.A.)</td><td>11/11</td><td>0.8828</td><td>0.9269</td><td>9.1882</td><td>1.3307</td><td>1.9840</td><td>2.2900</td><td>1.9489</td><td>1.8884</td></tr><tr><td>MARS</td><td>1.0000 (N.A.)</td><td>11/11</td><td>0.8375</td><td>0.9283</td><td>7.5620</td><td>1.1658</td><td>1.5761</td><td>1.8327</td><td>1.6074</td><td>1.5455</td></tr><tr><td>LSTM-HC</td><td>0.9910 (N.A.)</td><td>10/11</td><td>0.8918</td><td>0.7937</td><td>18.1562</td><td>1.4251</td><td>1.1003</td><td>1.2365</td><td>1.0772</td><td>1.2098</td></tr><tr><td>JTVAE-BO</td><td>1.0000 (N.A.)</td><td>10/11</td><td>0.7366</td><td>0.7294</td><td>23.5990</td><td>1.0714</td><td>1.0781</td><td>1.2352</td><td>1.0978</td><td>1.1206</td></tr><tr><td>DiGress</td><td>0.9913 (0.2362)</td><td>11/11</td><td>0.9099</td><td>0.2724</td><td>22.7237</td><td>2.9842</td><td>1.7163</td><td>2.0630</td><td>1.6738</td><td>2.1093</td></tr><tr><td>DiGress v2</td><td>0.9812 (0.3057)</td><td>11/11</td><td>0.9105</td><td>0.2771</td><td>21.7311</td><td>2.7507</td><td>1.7130</td><td>2.0632</td><td>1.6648</td><td>2.0479</td></tr><tr><td>GDSS</td><td>0.9205 (0.9076)</td><td>9/11</td><td>0.7510</td><td>0.0000</td><td>34.2627</td><td>1.3701</td><td>1.0271</td><td>1.0820</td><td>1.0683</td><td>1.1369</td></tr><tr><td>MOOD</td><td>0.9866 (0.9205)</td><td>11/11</td><td>0.8349</td><td>0.0227</td><td>39.3981</td><td>1.4019</td><td>1.4961</td><td>1.7603</td><td>1.4748</td><td>1.5333</td></tr><tr><td>Graph DiT-LCC (Ours)</td><td>0.9753 (0.8437)</td><td>11/11</td><td>0.8875</td><td>0.9560</td><td>7.0949</td><td>1.3099</td><td>0.8001</td><td>0.9562</td><td>0.8125</td><td>0.9697</td></tr><tr><td>Graph DiT (Ours)</td><td>0.8245 (0.8437)</td><td>11/11</td><td>0.8712</td><td>0.9600</td><td>6.6443</td><td>1.2973</td><td>0.7440</td><td>0.8857</td><td>0.7550</td><td>0.9205</td></tr></table></body></html>\n\n# 4 Experiment\n\nRQ1: We validate the generative power of Graph DiT compared to baselines from molecular optimization and diffusion models in Section 4.2. RQ2: We study a polymer inverse design for gas separation in Section 4.3. RQ3: We conduct further analysis to examine Graph DiT in Section 4.4.\n\n# 4.1 Experimental Setup\n\nWe use datasets with over ten types of atoms and up to fifty nodes in a molecular graph. We include both numerical and categorical properties for drugs and materials, offering a benchmark for evaluation across diverse chemical spaces. Model performance is validated across up to nine metrics, including distribution coverage, diversity, and condition control capacity for various properties.\n\nDatasets and Input Conditions We have one polymer dataset [40] for materials, featuring three numerical gas permeability conditions: $\\mathrm { \\Gamma _ { O 2 } P e r m }$ , $\\mathrm { C O _ { 2 } P e r m }$ , and $\\mathrm { \\bf N } _ { \\mathrm { 2 } } \\mathrm { \\bf P e r m }$ . For drug design, we create three class-balanced datasets from MoleculeNet [46]: HIV, BBBP, and BACE, each with a categorical property related to HIV virus replication inhibition, blood-brain barrier permeability, or human $\\beta$ - secretase 1 inhibition, respectively. We have two more numerical conditions for synthesizability from synthetic accessibility (SAS) and complexity scores (SCS) [12, 8].\n\nEvaluation We randomly split the dataset into training, validation, and testing (reference) sets in a 6:2:2 ratio. Evaluations are conducted on 10,000 generated examples with metrics [35] (1) molecular validity (Validity); (2) heavy atom type coverage (Coverage); (3) internal diversity among the generated examples (Diversity); (4) fragment-based similarity with the reference set (Similarity); (5) Fr´echet ChemNet Distance with the reference set (Distance) [36]; MAE between the generated and conditioned (6) synthetic accessibility score [12] (Synth.); $( 7 ) { \\sim } ( 9 )$ MAE/Accuracy for the numerical/categorical task conditions (Property). The evaluation Oracle uses random forest trained on all task-related molecules [14]. Lower MAE or higher accuracy indicates stronger model controllability.\n\nBaselines We select strong and popular molecular optimization baselines from recent studies [14]: Graph-GA [20], MARS [47], JTVAE [21] with Bayesian optimization (JTVAE-BO), LSTM [6] on SMILES with Hill Climbing (LSTM-HC). We include the most recent diffusion models: GDSS[22], DiGress [43], and their conditional version with extra predictors: MOOD [25], and DiGress v2 [43]. We train multi-task predictors using the same architecture for MOOD and DiGress v2 models to provide additional guidance for generation. For molecular optimization, we formulate the condition set of each test data point as a combined goal, minimizing the sum of the normalized errors between generated and input properties. We train a random forest model for each property using the training data to optimize the molecular structure.\n\n# 4.2 RQ1: Multi-Conditional Molecular Generation\n\nWe have the observations from Table 1 and Table 2:\n\nChemical Validity High validity may not accurately represent the model’s generative performance if hard-coded rules are introduced in the algorithm. For example, GraphGA could eliminate non-valid molecules during mutation and crossover iterations to achieve perfect validity in the final evaluation. Without rule checking in the generation-to-molecule step, DiGress, GDSS, and MOOD show a marked performance decline, with validity often dropping from 0.99 to below 0.6. In contrast, Graph DiT often maintains over 0.8 validity without any rule-based processing.\n\nTable 2: Multi-Conditional Generation of 10K Small Molecules: Each dataset involves a numerical synthesizability score (Synth.) and a categorical task-specific property. MAE/Accuracy is calculated by comparing input conditions and generated properties. The best number per metric is highlighted.   \n\n<html><body><table><tr><td colspan=\"2\">Tasks Model</td><td>Validity ↑ (w/o rule checking)</td><td></td><td colspan=\"3\">Distribution Learning</td><td colspan=\"3\">Condition Control</td></tr><tr><td colspan=\"2\"></td><td></td><td>Coverage↑</td><td>Diversity↑</td><td>Similarity↑</td><td>Distance↓</td><td>Synthe.MAE↓Property Acc.↑</td><td></td><td>Avg. Rank↓</td></tr><tr><td rowspan=\"9\">HAAA &</td><td>Graph GA</td><td>1.0000 (N.A.)</td><td>8/8</td><td>0.8585</td><td>0.9805</td><td>7.4104</td><td>0.9633</td><td>0.4690</td><td>6.5000</td></tr><tr><td>MARS</td><td>1.0000 (N.A.)</td><td>8/8</td><td>0.8338</td><td>0.8827</td><td>6.7923</td><td>1.0123</td><td>0.5184</td><td>5.0000</td></tr><tr><td>LSTM-HC</td><td>0.9972 (N.A.)</td><td>8/8</td><td>0.8146</td><td>0.7982</td><td>17.5585</td><td>0.9207</td><td>0.5816</td><td>3.0000</td></tr><tr><td>JTVAE-B80</td><td>1.3511(0N2A)</td><td>688</td><td>0.66828</td><td></td><td></td><td></td><td></td><td>7.5000</td></tr><tr><td></td><td></td><td></td><td></td><td>0.7281</td><td>30.4696</td><td>0.9923</td><td>0.4628</td><td></td></tr><tr><td>DiGress v2</td><td>0.3546 (0.2680)</td><td>8/8</td><td>0.8812</td><td>0.7027</td><td>25.3270</td><td>2.3365</td><td>0.5113</td><td>7.5000</td></tr><tr><td>GDSS</td><td>0.2879 (0.2589)</td><td>4/8</td><td>0.8756</td><td>0.2708</td><td>46.7539</td><td>1.6422</td><td>0.5036</td><td>7.5000</td></tr><tr><td>MOOD</td><td>0.9947 (0.4502)</td><td>8/8</td><td>0.8902</td><td>0.2587</td><td>44.2394</td><td>1.8853</td><td>0.5062</td><td>7.0000</td></tr><tr><td>Graph DiT-LCC (Ours)</td><td>0.8646 (0.8495)</td><td>8/8</td><td>0.8240</td><td>0.8757</td><td>6.9836</td><td>0.4053</td><td>0.9050</td><td>2.0000</td></tr><tr><td rowspan=\"14\">BRBRP &</td><td>Graph DiT (Ours)</td><td>0.8674 (0.8495)</td><td>8/8</td><td>0.8238</td><td>0.8752</td><td>7.0456</td><td>0.3998</td><td>0.9135</td><td>1.0000</td></tr><tr><td>Graph GA</td><td>1.0000 (N.A.)</td><td>9/9</td><td>0.8950</td><td>0.9509</td><td>10.1659</td><td>1.2082</td><td>0.3015</td><td>7.5000</td></tr><tr><td>MARS LSTM-HC</td><td>1.0000 (N.A.)</td><td>8/9</td><td>0.8637</td><td>0.7696</td><td>10.9791</td><td>1.2250</td><td>0.5189</td><td>6.0000</td></tr><tr><td></td><td>0.9990 (N.A.)</td><td>8/9</td><td>0.8883</td><td>0.8932</td><td>16.3904</td><td>0.9969</td><td>0.5590</td><td>4.0000</td></tr><tr><td>JTVAE5B0</td><td>100-4</td><td>509</td><td>0.7458</td><td>0.5821</td><td>3.5746</td><td>1.1619</td><td>0.45958</td><td>6.000</td></tr><tr><td>DiGress v2</td><td>0.6892 (0.4100)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GDSS</td><td>0.6218 (0.5919)</td><td>9/9 3/9</td><td>0.9107 0.8415</td><td>0.6336</td><td>19.4498</td><td>2.2694</td><td>0.6531</td><td>6.5000</td></tr><tr><td>MOOD</td><td>0.8008 (0.5789)</td><td>9/9</td><td>0.9273</td><td>0.2672 0.1715</td><td>39.9440 34.2506</td><td>1.3788 2.0284</td><td>0.5037</td><td>7.0000</td></tr><tr><td>Graph DiT-LCC (Ours)</td><td>0.8657 (0.8505)</td><td></td><td></td><td></td><td></td><td></td><td>0.4903</td><td>8.5000</td></tr><tr><td>Graph GA MARS</td><td>0.8468 (0.8505)</td><td>9/9</td><td>0.8857</td><td>0.9324</td><td>11.8587</td><td>0.3717</td><td>0.9390</td><td>2.0000</td></tr><tr><td rowspan=\"13\">AIH &</td><td>Graph DiT(Ours)</td><td></td><td>9/9</td><td>0.8856</td><td>0.9329</td><td>11.8519</td><td>0.3551</td><td>0.9417</td><td>1.0000</td></tr><tr><td></td><td>1.0000 (N.A.)</td><td>28/29</td><td>0.8993</td><td>0.9661</td><td>4.4418</td><td>0.9839</td><td>0.6035</td><td>5.0000</td></tr><tr><td>LSTM-HC</td><td>1.0000 (N.A.) 0.9994 (N.A.)</td><td>26/29</td><td>0.8764</td><td>0.6517</td><td>7.2893</td><td>0.9691</td><td>0.6455</td><td>4.0000</td></tr><tr><td>JTVAE-BO</td><td>1.0000 (N.A.)</td><td>13/29</td><td>0.9091</td><td>0.9145</td><td>7.4659</td><td>0.9480</td><td>0.6736</td><td>3.0000</td></tr><tr><td></td><td></td><td>3/29</td><td>0.8055</td><td>0.4173</td><td>41.9771</td><td>1.2359</td><td>0.4850</td><td>7.5000</td></tr><tr><td>DiGress</td><td>0.4377 (0.3643)</td><td>22/29</td><td>0.9194</td><td>0.8562</td><td>13.0409</td><td>1.9216</td><td>0.5335</td><td>7.5000</td></tr><tr><td>DiGress v2 GDSS</td><td>0.5050 (0.4242)</td><td>24/29</td><td>0.9193</td><td>0.8476</td><td>13.3997</td><td>1.5934</td><td>0.5331</td><td>7.5000</td></tr><tr><td>MOOD</td><td>0.6926 (0.6757) 0.2875 (0.2173)</td><td>4/29 29/29</td><td>0.7817 0.9280</td><td>0.1032</td><td>45.3416</td><td>1.2515</td><td>0.4830</td><td>8.5000</td></tr><tr><td></td><td></td><td></td><td></td><td>0.1361</td><td>32.3523</td><td>2.3144</td><td>0.5106</td><td>9.0000</td></tr><tr><td>Graph DiT-LCC (Ours)</td><td>0.7635 (0.7415)</td><td>28/29</td><td>0.8966</td><td>0.9535</td><td>5.8790</td><td>0.3084</td><td>0.9766</td><td>1.5000</td></tr><tr><td>Graph DiT (Ours)</td><td>0.7660 (0.7415)</td><td>28/29</td><td>0.8974</td><td>0.9575</td><td>6.0216</td><td>0.3086</td><td>0.9777</td><td>1.5000</td></tr></table></body></html>\n\nDistribution Learning GraphGA is a simple yet effective baseline for generating in-distribution molecules, e.g., on BBBP and HIV generation datasets. Diffusion model baselines such as DiGress and MOOD could produce diverse molecules but often fail to capture the original data distribution in multi-conditional tasks. Graph DiT shows the competitive performance of diffusion models in fitting complex molecular data distributions. Using fragment-based similarity and neural network-based distance metrics [36], we achieve the best in the polymer task and rank second in the HIV small molecule task, involving up to 11 and 29 types of heavy atoms, respectively.\n\nCondition Controllability LSTM-HC surpasses many baselines, achieving lower average MAE on polymer properties and higher rankings on small molecular properties. However, its control over synthetic scores in polymer tasks is relatively poor. Conversely, MARS effectively manages synthetic scores for polymers but exhibits a larger MAE in gas permeability conditions compared to other baselines. GDSS performs well in gas permeability control but underperforms Graph GA and MARS in terms of the synthetic score condition. DiGress v2 and MOOD, although equipped with the predictor guidance, still exhibit limited condition control compared to their unconditional counterparts over polymer and small molecule tasks. These baselines struggle to balance and control multiple conditions in generation. In contrast, Graph DiT significantly improves diffusion models and achieves the best multi-conditional performance in all tasks. In polymer tasks, Graph DiT reduces MAE on all gas permeability conditions, averaging $+ 1 7 . 8 \\%$ improvement over the best baseline LSTM-HC. For small molecule tasks, Graph DiT consistently ranks top-1 in condition controllability with over 0.9 accuracy in categorical conditions. Compared to Graph DiT-LCC, we observe that Graph DiT, which connects all generated graph components, shows better controllability performance due to minimal rule-based post-generation processing.\n\n# 4.3 RQ2: Polymer Inverse Design for Gas Separation\n\nWe aim to design polymers with high $\\mathrm { O _ { 2 } }$ and low $\\mathbf { N } _ { 2 }$ permeability, demonstrating the models’ precise control over related properties. Following Robeson [38]’s definition of high-performance polymers based on the $\\mathrm { { O _ { 2 } / N _ { 2 } } }$ permeability ratio, we selected 16 polymers meeting this criterion from 609 examples as our test/reference set. The remaining data is used for training and validation. Subsequently, we generated 1,000 polymers conditioned on test set labels.\n\n![](images/d054add53b20a03a501b5113803ec595b201ea256e673eed760ce16e99d696d5.jpg)  \nFigure 3: Polymer Inverse Design for $\\mathrm { O _ { 2 } / N _ { 2 } }$ Gas Separation: Feedback from four domain experts includes an average Utility Score (UtS) for relative usefulness and an Agreement Score (AS) for generated polymers, both ranging [0, 1]. Polymers are generated conditional on $\\{ \\mathrm { S A S } { = } 3 . 8$ , ${ \\mathrm { S C S } } { = } 4 . 3$ , $\\mathrm { O _ { 2 } P e r m } { = } 3 4 . 0$ , $\\mathrm { N _ { 2 } P e r m } { = } 5 . 2 \\}$ . The top-3 polymers, highlighted, are all generated by Graph DiT.   \nFigure 4: Relative Performance of Different Model Designs: A higher bar indicates better performance. We use the performance of clustering-based encoding or AdaLN as the Reference Value and the current option as the Current Value. Relative performance is calculated as RCefuerrencteVaVlauleue f Similarity and Diversity metrics, and as RCefuerrencteVaVlauleu for other metrics.\n\n0.76 1.00 1. 1. 0 Dis 0.59 . 0.53 0.53 0.57 0.51 Stap 0.060.21 1.00 C 0 .56 0.06.05 1.00 and N2Perm 0.49 0.060.24 1.00 Cluster AdaLN Depedent 0.48 Direct 0.59 0.53 In-Context 0.57 Similarity 0.46 Separate 0.59 Interval Cross-Attn 0.60 DiGress v2 1.00 1.00 1.00 1.00 1.00 1.00   \nC 0.61 0.55   \n2 err 0.76 0.54 1.03 0.59 1.03 Perr 0.52 1.04 1.00 1.02 1.00 1.04 1.00 1.01 Diversity Diversity Diversity   \n(a) Numerical Condition Encodings (b) Condition Architectures (c) Graph Dependent Noise Model\n\nIn Figure 3, we present the top three polymers generated by each model for a case study with expertise. Initially, a random forest algorithm identifies the top five polymers per method based on average MAE in two gas permeability. These 25 polymers are then shuffled and evaluated by four polymer scientists, who rank them from 1 to 25 using their domain knowledge. Rankings are normalized to a Utility Score (UtS) ranging from 0 to 1, with higher scores indicating greater utility. The variance in UtS is converted into an Agreement Score (AS) for further evaluation. As shown in Figure 3, there is a high consensus among experts that the three polymers generated by Graph DiT are the most promising for successful polymer inverse design tasks. More details are in appendix D. By comparing generated examples from different models, we have further observations:\n\n• DiGress and MOOD struggle to capture polymerization points, marked with asterisks $( ^ { 6 6 * } )$ , which is one of the most important features that distinguish polymers from small molecules. Additionally, the two methods frequently feature excessive carbon atoms and overly large cycles. These molecular configurations with significant distortion from the canonical geometry of stable compounds may lead to poor synthesizability [35, 7]. • LSTM-HC may result in too-long carbon chains with limited diversity. MARS produces examples with asymmetrical graph structures, challenging polymer synthesis [10, 2].\n\n• Graph DiT generates structurally diverse and symmetric polymers with two polymerization points, indicative of more valid and synthesizable polymer structures. The first two, which are polyimides, imply effective gas separation performance [24].\n\n# 4.4 RQ3: Ablation Studies and Model Analysis\n\nModel Components In light of Table 1, we analyze three components that impact our model’s learning in various conditions. Our assessment of relative performance is based on the ratio between our method and comparative approaches. The first component is numerical conditional encoding. Results in Figure 4(a) highlight the superiority of clustering encoding over direct and interval-based encoding, particularly in controlling gas permeability, despite its slightly lower diversity. The second component concerns the neural architecture for conditions. As shown in Figure 4(b), similar to Figure 4(a), AdaLN surpasses both In-Context Conditioning and Cross-Attention in learning distribution with better condition controllability. The third component validates the importance of the graph-dependent noise model compared to separately applying noise to atoms and bonds. It also shows the improvement of the predictor-free Graph DiT over the predictor-guided DiGress v2, even without the graph-dependent noise model. More results on model controllability are in appendix E.\n\nOracle Selections We analyze the robustness of Oracles in evaluating six task-related properties (three gas permeability and three small molecule properties) across six conditional generation tasks. Oracles are switched from Random Forest to Gaussian Process or Support Vector Machines for ranking generative model performance. Results in Table 3 show consistent rankings (Graph DiT, LSTM-HC, MARS, JTVAE-BO, MOOD, GDSS, GraphGA). It indicates that while perfectly approximating the truth properties of generated molecules is difficult, we could effectively compare the relative performance of various models.\n\nGraph DiT consistently ranked first among baselines.\n\nTable 3: Oracles for Generation Evaluation: We consider three Oracles. Generative performance is ranked on average from 1 to 9 across six properties, with various Oracles yielding similar outcomes. We highlight models with the same ranking sequence in different Oracle evaluation.   \n\n<html><body><table><tr><td>Avg. Rank</td><td>Random Forest</td><td>Gaussian Process</td><td>SupportVectorMachine</td></tr><tr><td>1</td><td>Graph DiT</td><td>Graph DiT</td><td>Graph DiT</td></tr><tr><td>2345</td><td>LSTM-HC</td><td>DiGress v2</td><td>DiGress v2</td></tr><tr><td></td><td>MARS</td><td>DiGress</td><td>DiGress</td></tr><tr><td></td><td>JTVAE-BO</td><td>LSTM-HC</td><td>LSTM-HC</td></tr><tr><td>5</td><td>MOOD</td><td>MARS</td><td>MARS</td></tr><tr><td></td><td>DiGress</td><td>JTVAE-BO</td><td>JTVAE-BO</td></tr><tr><td>67８</td><td>DiGress v2</td><td>MOOD</td><td>MOOD</td></tr><tr><td></td><td>GDSS</td><td>GDSS</td><td>GDSS</td></tr><tr><td>9</td><td>Graph GA</td><td>Graph GA</td><td>Graph GA</td></tr></table></body></html>\n\n# 5 Related Work\n\nDiffusion Models for Molecules: Score-based diffusion models applied noise and denoising in continuous space [33, 22]. DiGress [43] used discrete noise as transition matrices based on marginal distributions of atom and bond types. Extra predictor models are studied to guide the generation process in DiGress and GDSS [25]. Diffusion models could also be used for molecular property prediction [27], for conformation [48] and molecule generation with 3D atomic coordinates [18, 49, 3]. We focus on molecular graph generation, considering the high computational cost of accurate 3D coordinates for larger molecules like polymers [23]. We explore predictor-free diffusion guidance, instead of the classifier guidance [9, 44], for generating molecules under categorical and numerical conditions. It can be integrated with diffusion models for atomic coordinates in future research.\n\nMolecular Optimization: Optimization algorithms could optimize molecules towards property constraints, including genetic algorithms [20], Bayesian optimization [39, 50], REINFORCE [45], and reinforcement learning [30]. Both sequential and graph-based generative models [6, 21, 30], along with diverse sampling methods [47, 13], are used in conjunction with these algorithms to produce desirable molecules. These methods have been applied to both single-objective and multi-objective optimization, the latter by manually integrating multiple property conditions into a single one [5, 25]. Several challenges in molecular optimization methods remain underexplored, including the inadequate or unclear definition of multi-property relations when integration into a single objective [5], and the inaccessibility of the oracle function for property-oriented optimization during the training phase [14].\n\n# 6 Conclusion\n\nIn this work, we solved inverse molecular design using properties as predictor-free diffusion guidance. The proposed Graph DiT performed diffusion based on the joint distribution of atoms and bonds in both forward and reverse processes. It introduced representation learning for multiple categorical and numerical properties and utilized a Transformer-based graph denoiser for conditional graph denoising. Results on multi-conditional generations and polymer inverse designs showed the remarkable generative capabilities of Graph DiT, making it suitable for designing promising molecules.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n现有逆分子设计方法在将多个属性（如合成分数和气体渗透率）作为条件约束集成到扩散模型中存在不足，难以平衡不同尺度和类型的多条件，导致在多条件分子生成任务中表现不佳。该问题的重要性在于实际的逆分子设计需要考虑多种因素，解决此问题有助于推动材料和药物发现的发展。\\n\\n**方法概述**\\n论文提出了用于多条件分子生成的图扩散变压器（Graph DiT），它集成了编码器来学习数值和分类属性表示，并结合基于Transformer的去噪器，采用了新颖的图相关噪声模型进行训练。\\n\\n**主要贡献与效果**\\n- 提出了Graph DiT模型，在多条件聚合物和小分子生成任务中，通过九个指标（从分布学习到分子属性条件控制）验证了其优越性，相比最佳基线LSTM - HC，在聚合物任务中所有气体渗透率条件下平均降低MAE达17.86%。\\n- 在小分子任务的分类条件上达到超过0.9的准确率，显著优于基线模型的低于0.6的准确率。\\n- 在聚合物逆设计任务中，生成的聚合物得到领域专家的高度认可，其生成的聚合物在实用性和一致性方面表现出色，其实用性得到验证。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nGraph DiT的核心思想是通过学习将多属性投影到表示中，从而指导扩散过程进行分子生成。其基于原子和键的联合分布在正向和反向过程中进行扩散，利用条件编码器学习条件表示，用图去噪器在多条件下进行图去噪。该方法有效的原因在于考虑了原子和键之间的依赖关系，能更好地估计图相关噪声，同时对多属性进行有效表示学习，提升了模型对多条件的控制能力。\\n\\n**创新点**\\n- 现有工作在正向扩散过程中分别对原子和键添加噪声，Graph DiT提出了新颖的图相关噪声模型，基于节点和边的联合分布设计过渡矩阵，更准确地估计分子中的图相关噪声。\\n- 针对多条件分子生成，现有方法将多个条件集成到单个目标存在多属性关系定义不明确的问题，Graph DiT引入了多属性表示学习，使用条件编码器对不同类型的条件进行编码，避免了传统方法的不足。\\n- 采用无预测器的扩散引导，直接估计条件下的生成概率，而不是像传统方法那样使用分类器引导或额外的预测器。\\n\\n**具体实现步骤**\\n1. **条件编码**：将时间步t作为特殊条件进行正弦编码，对于数值条件采用聚类编码方法，通过定义可学习的质心将条件值分配到聚类中，并将软分配向量转换为表示；对于分类条件使用独热编码。最后将所有条件的编码表示相加得到条件的表示。\\n2. **图表示**：使用单个矩阵$\\mathbf { X } _ { G }$表示图令牌，通过连接节点特征矩阵$\\mathbf { X } _ { V }$和扁平化的边连接矩阵$\\mathbf { X } _ { E }$创建令牌表示。\\n3. **噪声模型**：设计过渡矩阵$\\mathbf { Q } _ { G }$考虑节点和边的联合分布，其由四个矩阵$\\mathbf { Q } _ { V }$、$\\mathbf { \\tilde { Q } } _ { E V }$、$\\mathbf { Q } _ { E }$、$\\mathbf { Q } _ { V E }$构建。在扩散过程中，根据$\\mathbf { Q } _ { G }$进行类别采样，完成图的噪声添加，即$q ( \\mathbf { X } _ { G } ^ { t } \\mid \\mathbf { X } _ { G } ^ { t - 1 } ) = \\widetilde { \\mathrm { C a t } } \\left( \\mathbf { X } _ { G } ^ { t } ; \\tilde { \\mathbf { p } } = \\mathbf { X } _ { G } ^ { t - 1 } \\mathbf { Q } _ { G } ^ { t } \\right)$。\\n4. **去噪模型**：\\n    - **Transformer层**：将噪声图的图令牌编码到隐藏空间，使用自适应层归一化（AdaLN）替换标准Transformer层中的归一化，根据条件表示控制归一化过程。\\n    - **最终MLP**：在经过Transformer层后，使用MLP预测节点和边在t = 0时的概率，将输出拆分为原子和键特征，即$\\tilde { \\bf X } _ { G } ^ { 0 } = { \\mathrm { A d a L N } } ( { \\mathrm { M L P } } ( { \\bf H } ) , { \\bf c } )$。\\n5. **生成分子转换**：对于生成的图，Graph DiT - LCC选择最大连通分量，Graph DiT则通过随机选择原子连接所有组件，这种方式能更准确地反映模型性能。\\n\\n**案例解析**\\n论文中通过两个案例展示了Graph DiT的性能。一是在多条件生成实验中，使用了一个包含100个数据点的测试集，每个数据点有三个属性：可合成性（Synth.）、$\\mathrm { O _ { 2 } }$和$\\mathrm { N _ { 2 } }$渗透率。单条件扩散模型为每个条件生成最多30个图，然后检查不同条件集之间是否能找到满足多属性约束的共享聚合物结构。Graph DiT生成的聚合物在这些测试中表现良好，中位数排名分别为4、9和11，远高于现有单条件生成方法的中位数排名30。二是通过聚合物逆设计案例，以设计具有高$\\mathrm { O _ { 2 } }$和低$\\mathbf { N } _ { 2 }$渗透率的聚合物为例，选择满足特定标准的聚合物作为测试集，使用Graph DiT生成1000个聚合物。通过领域专家对生成的聚合物进行评估，发现Graph DiT生成的聚合物具有更高的实用性和一致性，其生成的前三个聚合物得到了专家的高度认可。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 分子优化模型：Graph - GA、MARS、JTVAE - BO、LSTM - HC。\\n- 扩散模型：GDSS、DiGress、MOOD、DiGress v2。\\n\\n**性能对比**\\n*   **在[化学有效性/Validity]指标上**：在不进行规则检查的情况下，Graph DiT通常能保持超过0.8的有效性，而DiGress、GDSS和MOOD的有效性常从0.99降至低于0.6，Graph DiT的有效性明显优于这些模型。\\n*   **在[分布学习相关指标（覆盖度/Coverage、多样性/Diversity、相似度/Similarity、距离/Distance）]上**：\\n    - **覆盖度**：Graph DiT在聚合物任务和多个小分子任务中都能达到较高的覆盖度，如聚合物任务中达到11/11，在HIV小分子任务中达到28/29，与基线模型表现相当甚至更优。\\n    - **多样性**：扩散模型基线如DiGress和MOOD能产生多样的分子，但在多条件任务中难以捕捉原始数据分布。Graph DiT在拟合复杂分子数据分布方面具有竞争力，在聚合物任务中表现最佳，在HIV小分子任务中排名第二。\\n    - **相似度**：Graph DiT在聚合物任务和小分子任务中都取得了较好的相似度，例如在聚合物任务中达到0.9600，在HIV小分子任务中达到0.9575，优于多数基线模型。\\n    - **距离**：Graph DiT在聚合物任务和小分子任务中的距离指标表现较好，如聚合物任务中为6.6443，在HIV小分子任务中为6.0216，优于部分基线模型。\\n*   **在[条件可控性相关指标（平均绝对误差/MAE、准确率/Accuracy）]上**：\\n    - **聚合物任务**：LSTM - HC在聚合物属性上的平均MAE较低，但对合成分数的控制较差；MARS对聚合物合成分数的控制较好，但在气体渗透率条件下的MAE较大；GDSS在气体渗透率控制方面表现较好，但在合成分数条件上不如Graph GA和MARS；DiGress v2和MOOD，虽有预测器引导，但在多条件控制上仍有限。Graph DiT在聚合物任务中显著降低了所有气体渗透率条件的MAE，平均比最佳基线LSTM - HC提高了17.86%。\\n    - **小分子任务**：Graph DiT在小分子任务的分类条件上始终排名第一，准确率超过0.9，远高于基线模型的低于0.6的准确率。\",\n    \"keywords\": \"### 关键词\\n\\n- 多条件分子生成 (Multi - Conditional Molecular Generation, N/A)\\n- 图扩散变压器 (Graph Diffusion Transformer, Graph DiT)\\n- 逆分子设计 (Inverse Molecular Design, N/A)\\n- 材料与药物发现 (Material and Drug Discovery, N/A)\\n- 聚合物设计 (Polymer Design, N/A)\"\n}"
}