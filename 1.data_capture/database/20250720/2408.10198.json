{
    "link": "https://arxiv.org/abs/2408.10198",
    "pdf_link": "https://arxiv.org/pdf/2408.10198",
    "title": "MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model",
    "authors": [
        "Minghua Liu",
        "Chong Zeng",
        "Xinyue Wei",
        "Ruoxi Shi",
        "Linghao Chen",
        "Chao Xu",
        "Mengqi Zhang",
        "Zhaoning Wang",
        "Xiaoshuai Zhang",
        "Isabella Liu",
        "Hongzhi Wu",
        "Hao Su"
    ],
    "institutions": [
        "Stanford University",
        "University of California San Diego",
        "University of California, Los Angeles",
        "Zhejiang University"
    ],
    "publication_date": "2024-08-19",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 26,
    "influential_citation_count": 4,
    "paper_content": "# MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model\n\nMinghua Liu∗1,2† Chong Zeng∗3‡ Xinyue Wei1,2† Ruoxi Shi1,2† Linghao Chen2,3† Chao $\\mathbf { X } \\mathbf { u } ^ { 2 , 4 \\dagger }$ Mengqi Zhang2 Zhaoning Wang5 Xiaoshuai Zhang1,2† Isabella Liu1 Hongzhi $\\mathbf { W u ^ { 3 } }$ Hao Su1,2\n\n1 UC San Diego 2 Hillbot Inc. 3 Zhejiang University 4 UCLA 5 University of Central Florida\n\nProject Website: https://meshformer3d.github.io/\n\n![](images/2bb6c7605967cb1eb94b05a75319287539fead0bfd388f6dd4164a701a7545a2.jpg)  \nFigure 1: Given a sparse set (e.g., 6) of multi-view RGB images and their normal maps as input, MeshFormer reconstructs high-quality 3D textured meshes with fine-grained, sharp geometric details in a feed-forward pass of just a few seconds. Here, ground truth multi-view RGB and normal images are used as input.\n\n# Abstract\n\nOpen-world 3D reconstruction models have recently garnered significant attention. However, without sufficient 3D inductive bias, existing methods typically entail expensive training costs and struggle to extract high-quality 3D meshes. In this work, we introduce MeshFormer, a sparse-view reconstruction model that explicitly leverages 3D native structure, input guidance, and training supervision. Specifically, instead of using a triplane representation, we store features in 3D sparse voxels and combine transformers with 3D convolutions to leverage an explicit 3D structure and projective bias. In addition to sparse-view RGB input, we require the network to take input and generate corresponding normal maps. The input normal maps can be predicted by 2D diffusion models, significantly aiding in the guidance and refinement of the geometry’s learning. Moreover, by combining Signed Distance Function (SDF) supervision with surface rendering, we directly learn to generate high-quality meshes without the need for complex multi-stage training processes. By incorporating these explicit 3D biases, MeshFormer can be trained efficiently and deliver high-quality textured meshes with fine-grained geometric details. It can also be integrated with 2D diffusion models to enable fast single-image-to-3D and text-to-3D tasks.\n\n# 1 Introduction\n\nHigh-quality 3D meshes are essential for numerous applications, including rendering, simulation, and 3D printing. Traditional photogrammetry systems [57, 61] and recent neural approaches, such as NeRF [43], typically require a dense set of input views of the object and long processing times. Recently, open-world 3D object generation has made significant advancements, aiming to democratize 3D asset creation by reducing input requirements. There are several prevailing paradigms: training a native 3D generative model using only 3D data [13, 95] or performing per-shape optimization with Score Distillation Sampling (SDS) losses [30, 47]. Another promising direction is to first predict a sparse set of multi-view images using 2D diffusion models [33, 59] and then lift these predicted images into a 3D model by training a feed-forward network [31, 32]. This strategy addresses the limited generalizability of models trained solely on 3D data and overcomes the long runtime and 3D inconsistency of per-shape-optimization-based methods.\n\nWhile many recent works explore utilizing priors from 2D diffusion models, such as generating consistent multi-view images [59, 60] and predicting normal maps from RGB [12, 37, 59], the feedforward model that converts multi-view images into 3D remains underexplored. One-2-3-45 [32] leverages a generalizable NeRF method for 3D reconstruction but suffers from limited quality and success rates. One- $2 - 3 - 4 5 + +$ [31] improves on this by using a two-stage 3D diffusion model, yet it still struggles to generate high-quality textures or fine-grained geometry. Given that sparse-view reconstruction of open-world objects requires extensive priors, another family of works pioneered by the large reconstruction model (LRM) [16] combines large-scale transformer models with the triplane representation and trains the model primarily using rendering loss. Although straightforward, these methods typically require over a hundred GPUs to train. Moreover, due to their reliance on volume rendering, these methods have difficulty extracting high-quality meshes. For instance, some recent follow-up works [79, 85] implement complex multi-stage “NeRF-to-mesh” training strategies, but the results still leave room for improvement.\n\nIn this work, we present MeshFormer, an open-world sparse-view reconstruction model that takes a sparse set of posed images of an arbitrary object as input and delivers high-quality 3D textured meshes with a single forward pass in a few seconds. Instead of representing 3D data as “2D planes” and training a “black box” transformer model optimizing only rendering loss, we find that by incorporating various types of 3D-native priors into the model design, including network architecture, supervision signals, and input guidance, our model can significantly improve both mesh quality and training efficiency. Specifically, we propose representing features in explicit 3D voxels and introduce a novel architecture that combines large-scale transformers with 3D (sparse) convolutions. Compared to triplanes and pure transformers models with little 3D-native design, MeshFormer leverages the explicit 3D structure of voxel features and the precise projective correspondence between 3D voxels and 2D multi-view features, enabling faster and more effective learning.\n\nUnlike previous works that rely on NeRF-based representation in their pipeline, we utilize mesh representation throughout the process and train MeshFormer in a unified, single-stage manner. Specifically, we propose combining surface rendering with additional explicit 3D supervision, requiring the model to learn a signed distance function (SDF) field. The network is trained with high-resolution SDF supervision, and efficient differentiable surface rendering is applied to the extracted meshes for rendering losses. Due to the explicit 3D geometry supervision, MeshFormer enables faster training while eliminating the need for expensive volume rendering and learning an initial coarse NeRF. Furthermore, in addition to multi-view posed RGB images, we propose using corresponding normal maps as input, which can be captured through sensors and photometric techniques [4, 82] or directly estimated by recent 2D vision models [12, 37, 59]. These multi-view normal images provide important clues for 3D reconstruction and fine-grained geometric details. We also task the model with learning a normal texture in addition to the RGB texture, which can then be used to enhance the generated geometry through a traditional post-processing algorithm [44].\n\nThanks to the explicit 3D-native structure, supervision signal, and normal guidance that we have incorporated, MeshFormer can generate high-quality textured meshes with fine-grained geometric details, as shown in Figure 1. Compared to concurrent methods that require over one hundred GPUs or complex multi-stage training, MeshFormer can be trained more efficiently and conveniently with just eight GPUs over two days, achieving on-par or even better performance. It can also seamlessly integrate with various 2D diffusion models to enable numerous tasks, such as single-image-to-3D and text-to-3D. In summary, our key contributions include:\n\n• We introduce MeshFormer, an open-world sparse-view reconstruction model capable of generating high-quality 3D textured meshes with fine-grained geometric details in a few seconds. It can be trained with only 8 GPUs, outperforming baselines that require over one hundred GPUs.   \n• We propose a novel architecture that combines 3D (sparse) convolution and transformers. By explicitly leveraging 3D structure and projective bias, it facilitates better and faster learning.   \n• We propose a unified single-stage training strategy for generating high-quality meshes by combining surface rendering and explicit 3D geometric supervision.   \n• We are the first to introduce multi-view normal images as input to the feed-forward reconstruction network, providing crucial geometric guidance. Additionally, we propose to predict extra 3D normal texture for geometric enhancement.\n\n# 2 Related Work\n\nOpen-world 3D Object Generation Thanks to the emergence of large-scale 3D datasets [8, 9] and the extensive priors learned by 2D models [50, 51, 55, 56], open-world 3D object generation have recently made significant advancements. Exemplified by DreamFusion [47], a line of work [5, 6, 10, 26, 30, 48, 58, 60, 62, 65, 70, 76] uses 2D models as guidance to generate 3D objects through per-shape optimization with SDS-like losses. Although these methods produce increasingly better results, they are still limited by lengthy runtimes and many other issues. Another line of work [16, 20, 40, 45, 84, 96] trains a feed-forward generative model solely on 3D data that consumes text prompts or single-image inputs. While fast during inference, these methods struggle to generalize to unseen object categories due to the scarcity of 3D data. More recently, works such as Zero123 [33] have shown that 2D diffusion models can be fine-tuned with 3D data for novel view synthesis. A line of work [27, 27, 31, 64, 77, 79, 85], pioneered by One-2-3-45 [32], proposes first predicting multi-view images through 2D diffusion models and then lifting them to 3D through a feed-forward network, effectively addressing the speed and generalizability issues. Many recent works have also explored better strategies to fine-tune 2D diffusion models for enhancing the 3D consistency of multiview images [14, 17, 23, 34, 36, 49, 59, 60, 69, 72, 80, 81, 89, 91]. In addition to the feed-forward models, the generated multi-view images can also be lifted to 3D through optimizations [14, 34, 37].\n\nSparse-View Feed-Forward Reconstruction Models When a small baseline between input images is assumed, existing generalizable NeRF methods [35, 52, 68, 88] aim to find pixel correspondences and learn generalizable priors across scenes by leveraging cost-volume-based techniques [3, 38, 90] or transformer-based structures [19, 24, 54, 71, 74]. Some of methods have also incorporated a 2D diffusion process into the pipeline [1, 21, 66]. However, these methods often struggle to handle large baseline settings (e.g., only frontal-view reconstruction) or are limited by a small training set and fail to generalize to open-world objects. Recently, many models [27, 64, 73, 77, 79, 85–87, 92, 94] specifically aimed at open-world 3D object generation have been proposed. They typically build large networks and aim to learn extensive reconstruction priors by training on large-scale 3D datasets [9]. For example, the triplane representation and transformer models are often used. By applying volume rendering or Gaussian splatting [64, 86, 92], they train the model with rendering losses. However, these methods typically require extensive GPUs to train and have difficulty extracting high-quality\n\nnormal Geometry texture Enhancement TinyNormalMLP GR Sparse FVormelr FVomelr texture 1 normal loss Tiny Color MLP occupancy sparse feature color volume volume SDF vol loss (64^3) (256^3) multi-view ? Tiny SDF MLP → RGB + normal occupancy loss SDFloss textured mesh ↓ 2D 1 二 山 = 二 =   \nmulti-view 110 1 H 1 i   \nnormalpatchfeat 256^3 256^3 learnable token 4\n\nmeshes. While some recent (concurrent) works [79, 85] utilize multi-stage “NeRF-to-mesh” training strategies to improve the quality, the results still leave room for improvement.\n\nGeometry Guidance for 3D Reconstruction Many recent works have shown that in addition to multiview RGB images, 2D diffusion models can be fine-tuned to generate other geometric modalities, such as depth maps [75], normal maps [12, 37, 41], or coordinate maps [28, 77]. These additional modalities can provide crucial guidance for 3D generation and reconstruction. While many recent methods utilize these geometric cues as inverse optimization guidance [5, 12, 28, 37, 49, 77], we propose to take normal maps as input in a feed-forward reconstruction model and task the model with generating 3D-consistent normal texture for geometry enhancement of sharp details.\n\n3D Native Representations and Network Architectures in 3D Generation The use of 3D voxel representations and 3D convolutions is common in general 3D generation. However, most recent works focus on 3D-native diffusion [7, 18, 29, 31, 53, 95], one of the key paradigms in 3D generation, which differs from the route taken by MeshFormer. These 3D-diffusion-based methods have some common limitations. For instance, they focus solely on geometry generation and cannot directly predict high-quality textures from the network [7, 18, 29, 31, 53, 95]. Due to the limited availability of 3D data, 3D-native diffusion methods also typically struggle with open-world capabilities and are often constrained to closed-domain datasets (e.g., ShapeNet [2]) in their experiments [7, 29, 95].\n\nIn MeshFormer, our goal is to achieve direct high-quality texture generation while handling arbitrary object categories. Therefore, we adopt a different approach: sparse-view feed-forward reconstruction, as opposed to 3D-native diffusion. In this specific task setting, more comparable works are recent LRM-style methods [64, 67, 79, 85]. However, most of these methods rely on a combination of triplane representation and large-scale transformers. In this paper, we demonstrate that 3D-native representations and networks can not only be used in 3D-native diffusion but can also be combined with differentiable rendering to train a feed-forward sparse-view reconstruction model using rendering losses. In open-world sparse-view reconstruction, we are not limited to the triplane representation. Instead, 3D-native structures (e.g., voxels), network architectures, and projective priors can facilitate more efficient training, significantly reducing the required training resources. While scalable networks are necessary to learn extensive priors, scalability is not exclusive to triplane-based transformers. By integrating 3D convolutions with transformer layers, scalability can also be achieved.\n\n# 3 Method\n\nAs shown in Figure 2, MeshFormer takes a sparse set of posed multi-view RGB and normal images as input and generates a high-quality textured mesh in a single feed-forward pass. In the following sections, we will first introduce our choice of 3D representation and a novel model architecture that combines large-scale transformers with 3D convolutions (Sec. 3.1). Then, we will describe our training objectives, which integrate surface rendering and explicit 3D SDF supervision (Sec. 3.2). Last but not least, we will present our normal guidance and geometry enhancement module, which plays a crucial role in generating high-quality meshes with fine-grained geometric details (Sec. 3.3).\n\n# 3.1 3D Representation and Model Architecture\n\nTriplane vs. 3D Voxels Open-world sparse-view reconstruction requires extensive priors, which can be learned through a large-scale transformer. Prior arts [27, 67, 77, 79, 85] typically utilize the triplane representation, which decomposes a 3D neural field into a set of 2D planes. While straightforward for processing by transformers, the triplane representation lacks explicit 3D spatial structures and makes it hard to enable precise interaction between each 3D location and its corresponding 2D projected pixels from multi-view images. For instance, these methods often simply apply self-attention across all triplane patch tokens and cross-attention between triplane tokens and all multi-view image tokens. This all-to-all attention is not only costly but also makes the methods cumbersome to train. Moreover, the triplane representation often shows results with notable artifacts at the boundaries of patches and may suffer from limited expressiveness for complex structures. Consequently, we choose the 3D voxel representation instead, which explicitly preserves the 3D spatial structures.\n\nCombining Transformer with 3D Convolution To leverage the explicit 3D structure and the powerful expressiveness of a large-scale transformer model while avoiding an explosion of computational costs, we propose VoxelFormer and SparseVoxelFormer, which follow a 3D UNet architecture while integrating a transformer at the bottleneck. The overall idea is that we use local 3D convolution to encode and decode a high-resolution 3D feature volume, while the global transformer layer handles reasoning and memorizing priors for the compressed low-resolution feature volume. Specifically, as shown in Figure 2, a 3D feature volume begins with a learnable token shared by all 3D voxels. With the 3D voxel coordinates, we can leverage the projection matrix to enable each 3D voxel to aggregate 2D local features from multi-view images via a projection-aware cross-attention layer. By iteratively performing projection-aware cross-attention and 3D (sparse) convolution, we can compress the 3D volume to a lower-resolution one. After compression, each 3D voxel feature then serves as a latent token, and a deep transformer model is applied to a sequence of all 3D voxel features (position encoded) to enhance the model’s expressiveness. Finally, we use the convolution-based inverse upper branch with skip connection to decode a 3D feature volume with the initial high resolution.\n\nProjection-Aware Cross Attention Regarding 3D-2D interaction, the input multi-view RGB and normal images are initially processed by a 2D feature extractor, such as a trainable DINOv2 [46], to generate multi-view patch features. While previous cost-volume-based methods [3, 38] typically use mean or max pooling to aggregate multi-view 2D features, these simple pooling operations might be suboptimal for addressing occlusion and visibility issues. Instead, we propose a projectionaware cross-attention mechanism to adaptively aggregate the multi-view features for each 3D voxel. Specifically, we project each 3D voxel onto the $m$ views to interpolate $m$ RGB and normal features. We then concatenate these local patch features with the projected RGB and normal values to form $m$ 2D features. In the projection-aware cross-attention module, we use the 3D voxel feature to calculate a query and use both the 3D voxel feature and the $m$ 2D features to calculate $m + 1$ keys and values. A cross-attention is then performed for each 3D voxel, enabling precise interaction between each 3D location and its corresponding 2D projected pixels, and allowing adaptive aggregation of 2D features, which can be formulated as:\n\n$$\nv \\gets \\mathrm { C r o s s A t t e n t i o n } ( Q = \\{ v \\} , K = \\{ p _ { i } ^ { v } \\} _ { i = 1 } ^ { m } + \\{ v \\} , V = \\{ p _ { i } ^ { v } \\} _ { i = 1 } ^ { m } + \\{ v \\} )\n$$\n\nWhere $\\boldsymbol { v }$ denotes a 3D voxel feature, and $p _ { i } ^ { v }$ denotes its projected 2D pixel feature from view $i$ , which is a concatenation of the RGB feature $f _ { i } ^ { v }$ , the normal feature $g _ { i } ^ { v }$ , and the RGB and normal values $c _ { i } ^ { v }$ and $n _ { i } ^ { v }$ , respectively.\n\nCoarse-to-Fine Feature Generation As shown in Fig. 2, to generate a high-resolution 3D feature volume that captures the fine-grained details of 3D shapes, we follow previous work [31, 95] by employing a coarse-to-fine strategy. Specifically, we first use VoxelFormer, which is equipped with full 3D convolution, to predict a low-resolution (e.g., $6 4 ^ { 3 }$ ), coarse 3D occupancy volume. Each voxel in this volume stores a binary value indicating whether it is close to the surface. The predicted occupied voxels are then subdivided to create higher-resolution sparse voxels (e.g., $2 5 6 ^ { \\dot { 3 } }$ ). Next, we utilize a second module, SparseVoxelFormer, which features 3D sparse convolution [63], to predict features for these sparse voxels. After this, we trilinearly interpolate the 3D feature of any near-surface 3D point, which encodes both geometric and color information, from the high-resolution sparse feature volume. The features are then fed into various MLPs to learn the corresponding fields.\n\n# 3.2 Unified Single-Stage Training: Surface Rendering with SDF Supervision\n\nExisting works typically use NeRF [42] and volume rendering or 3D Gaussian splatting [22] since they come with a relatively easy and stable learning process. However, extracting high-quality meshes from their results is often non-trivial. For example, directly applying Marching Cubes [39] to density fields of learned NeRFs typically generates meshes with many artifacts. Recent methods [78, 79, 85] have designed complex, multi-stage “NeRF-to-mesh” training with differentiable surface rendering, but the generated meshes still leave room for improvement. On the other hand, skipping a good initialization and directly learning meshes from scratch using purely differentiable surface rendering losses is also infeasible, as it is highly unstable to train and typically results in distorted geometry.\n\nIn this work, we propose leveraging explicit 3D supervision in addition to 2D rendering losses. As shown in Figure 2, we task MeshFormer with learning a signed distance function (SDF) field supervised by a high-resolution (e.g., $5 1 2 ^ { 3 }$ ) ground truth SDF volume. The SDF loss provides explicit guidance for the underlying 3D geometry and facilitates faster learning. It also allows us to use mesh representation and differentiable surface rendering from the beginning without worrying about good geometry initialization or unstable training, as the SDF loss serves as a strong regularization for the underlying geometry. By combining surface rendering with explicit 3D SDF supervision, we train MeshFormer in a unified, single-stage training process. As shown in Figure 2, we employ three tiny MLPs that take as input the 3D feature interpolated from the 3D sparse feature volume to learn an SDF field, a 3D color texture, and a 3D normal texture. We extract meshes from the SDF volume using dual Marching Cubes [39] and employ NVDiffRast [25] for differentiable surface rendering. We render both the multi-view RGB and normal images and compute the rendering losses, which consist of both the MSE and perceptual loss terms. As a result, our training loss can be expressed as:\n\n$$\n\\mathcal { L } = \\lambda _ { 1 } \\mathcal { L } _ { \\mathrm { M S E } } ^ { \\mathrm { c o l o r } } + \\lambda _ { 2 } \\mathcal { L } _ { \\mathrm { L P I P S } } ^ { \\mathrm { c o l o r } } + \\lambda _ { 3 } \\mathcal { L } _ { \\mathrm { M S E } } ^ { \\mathrm { n o r m a l } } + \\lambda _ { 4 } \\mathcal { L } _ { \\mathrm { L P I P S } } ^ { \\mathrm { n o r m a l } } + \\lambda _ { 5 } \\mathcal { L } _ { \\mathrm { o c c } } + \\lambda _ { 6 } \\mathcal { L } _ { \\mathrm { S D F } }\n$$\n\nwhere $L _ { \\mathrm { o c c } }$ and $L _ { \\mathrm { S D F } }$ are MSE losses for occupancy and SDF volumes, and $\\lambda _ { i }$ denotes the weight of each loss term. Note that we do not use mesh geometry to derive normal maps; instead, we utilize the learned normal texture from the MLP, which will be detailed later.\n\n# 3.3 Fine-Grained Geometric Details: Normal Guidance and Geometry Enhancement\n\nWithout dense-view correspondences, 3D reconstruction from sparse-view RGB images typically struggles to capture geometric details and suffers from texture ambiguity. While many recent works [27, 79, 85] attempt to employ large-scale models to learn mappings from RGB to geometric details, this typically requires significant computational resources. Additionally, these methods are primarily trained using 3D data, but it’s still uncertain whether the scale of 3D datasets is sufficient for learning such extensive priors. On the other hand, unlike RGB images, normal maps explicitly encode geometric information and can provide crucial guidance for 3D reconstruction. Notably, open-world normal map estimation has achieved great advancements. Many recent works [12, 37, 59] demonstrate that 2D diffusion models, trained on billions of natural images, embed extensive priors and can be fine-tuned to predict normal maps. Given the significant disparity in data scale between 2D and 3D datasets, it may be more effective to use 2D models first for generating geometric guidance.\n\nInput Normal Guidance As shown in Figure 2, in addition to multi-view RGB images, MeshFormer also takes multi-view normal maps as input, which can be generated using recent open-world normal estimation models [12, 37, 59]. In our experiments, we utilize Zero1 $2 3 + +$ v1.2 [59], which trains an additional ControlNet [93] over the multi-view prediction model. The ControlNet takes multi-view RGB images, predicted by Zero $^ { 1 2 3 + + }$ , as a condition and produces corresponding multi-view normal maps, expressed in the camera coordinate frame. Given these maps, MeshFormer first converts them to a unified world coordinate frame, and then treats them similarly to the multi-view RGB images, using projection-aware cross-attention to guide 3D reconstruction. According to our experiments (Sec. 4.4), the multi-view normal maps enable the networks to better capture geometry details, and thus greatly improve final mesh quality.\n\nGeometry Enhancement While the straightforward approach of deriving normal maps from the learned mesh and using a normal loss to guide geometry learning has been commonly used, we find that this approach makes our mesh learning less stable. Instead, we propose learning a 3D normal texture, similar to a color texture, using a separate MLP. By computing the normal loss for MLPqueried normal maps instead of mesh-derived normal maps, we decouple normal texture learning from underlying geometry learning. This makes the training more stable, as it is easier to learn a sharp 3D normal map than to directly learn a sharp mesh geometry. The learned 3D normal texture can be exported with the mesh, similar to the color texture, to support various graphics rendering pipelines. In applications that require precise 3D geometry, such as 3D printing, the learned normal texture can also be used to refine the mesh geometry with traditional algorithms. Specifically, during inference, after extracting a 3D mesh from the SDF volume, we utilize a post-processing algorithm [44] that takes as input the 3D positions of the mesh vertices and the vertex normals estimated from the MLP. The algorithm adjusts the mesh vertices to align with the predicted normals in a few seconds, further enhancing the geometry quality and generating sharp geometric details, as shown in Figure 5.\n\n![](images/e57136724abf16acf687a0e9a439cbdf2e104f1b85923ac1391a17ee4b7bdd85.jpg)  \nFigure 3: Qualitative Examples of Single Image to 3D (GSO dataset). Both the textured and textureless mesh renderings are shown. Please zoom in to examine details and mesh quality, and refer to the supplemental material for results of One-2- $3 - 4 5 + +$ [31] and CRM [77].\n\n# 4 Experiments\n\n# 4.1 Implementation Details and Evaluation Settings\n\nImplementation Details We trained MeshFormer on the Objaverse [9] dataset. The total number of network parameters is approximately 648 million. We trained the model using 8 H100 GPUs for about one week $3 5 0 \\mathrm { k }$ iterations) with a batch size of 1 per GPU, although we also show that the model can achieve similar results in just two days. Please refer to the supplementary for more details.\n\nTable 1: Quantitative Results of Single Image to 3D. Evaluated on the 1,030 and 1,038 3D shapes from the GSO [11] and the OmniObject3D [83] datasets, respectively. One-2- $^ { 3 - 4 5 + + }$ [31], InstantMesh [85], MeshLRM [79], and our method all take the same multi-view RGB images predicted by Zero $^ { 1 2 3 + + }$ [59] as input. CD denotes Chamfer Distance.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">GSO [11]</td><td colspan=\"4\">OmniObject3D [83]</td></tr><tr><td>F-Score ↑</td><td>CD↓</td><td>PSNR↑</td><td>LPIPS↓</td><td>F-Score ↑</td><td>CD↓</td><td>PSNR↑</td><td>LPIPS↓</td></tr><tr><td>One-2-3-45++ [31]</td><td>0.936</td><td>0.039</td><td>20.97</td><td>0.21</td><td>0.871</td><td>0.054</td><td>17.08</td><td>0.31</td></tr><tr><td>TripoSR [67]</td><td>0.896</td><td>0.047</td><td>19.85</td><td>0.26</td><td>0.895</td><td>0.048</td><td>17.68</td><td>0.28</td></tr><tr><td>CRM[77]</td><td>0.886</td><td>0.051</td><td>19.99</td><td>0.27</td><td>0.821</td><td>0.065</td><td>16.01</td><td>0.34</td></tr><tr><td>LGM [64]</td><td>0.776</td><td>0.074</td><td>18.52</td><td>0.35</td><td>0.635</td><td>0.114</td><td>14.75</td><td>0.45</td></tr><tr><td>InstantMesh [64]</td><td>0.934</td><td>0.037</td><td>20.90</td><td>0.22</td><td>0.889</td><td>0.049</td><td>17.61</td><td>0.28</td></tr><tr><td>MeshLRM[79]</td><td>0.956</td><td>0.033</td><td>21.31</td><td>0.19</td><td>0.910</td><td>0.045</td><td>18.10</td><td>0.26</td></tr><tr><td>Ours</td><td>0.963</td><td>0.031</td><td>21.47</td><td>0.20</td><td>0.914</td><td>0.043</td><td>18.14</td><td>0.27</td></tr></table></body></html>\n\nEvaluation Settings We evaluate the methods on two datasets: GSO [11] and OmniObject3D [83]. Both datasets contain real-scanned 3D objects that were not seen during training. For the GSO dataset, we use all 1,030 3D shapes for evaluation. For the OmniObject3D dataset, we randomly sample up to 5 shapes from each category, resulting in 1,038 shapes for evaluation. We utilize both 2D and 3D metrics. For 3D metrics, we use both the F-score and Chamfer distance (CD), calculated between the predicted meshes and ground truth meshes, following [31, 85]. For 2D metrics, we compute both PSNR and LPIPS for the rendered color images. Since each baseline may use a different coordinate frame for generated results, we carefully align the predicted meshes of all methods to the ground truth meshes before calculating the metrics. Please refer to the supplemental material for more details.\n\n# 4.2 Comparison with Single/Sparse-View to 3D Methods\n\nWe compare MeshFormer with recent open-world feed-forward single/sparse-view to 3D methods, including One-2- $3 - 4 5 + +$ [31], TripoSR [67], CRM [77], LGM [64], InstantMesh [85], and MeshLRM [79]. Many of these methods have been released recently and should be considered concurrent methods. For MeshLRM [79], we contacted the authors for the results. For the other methods, we utilized their official implementations. Please refer to the supplementary for details.\n\nSince input settings differ among the baselines, we evaluate all methods in a unified single-view to 3D setting. For the GSO dataset, we utilized the first thumbnail image as the single-view input. For the OmniObject3D dataset, we used a rendered image with a random pose as input. For One-2-3- $^ { 4 5 + + }$ [31], InstantMesh [85], MeshLRM [79], and our MeshFormer, we first utilized Zero $1 2 3 + +$ [59] to convert the input single-view image into multi-view images before 3D reconstruction. Other baselines follow their original settings and take a single-view image directly as input. In addition to the RGB images, our MeshFormer also takes additional multi-view normal images as input, which are also predicted by Zero $^ { 2 3 + + }$ [59]. Note that when comparing with baseline methods, we never use ground truth normal images to ensure a fair comparison.\n\nIn Fig. 3, we showcase qualitative examples. Our MeshFormer produces the most accurate meshes with fine-grained, sharp geometric details. In contrast, baseline methods produce inferior mesh quality. For example, TripoSR directly extracts meshes from the learned NeRF representation, resulting in significant artifacts. While InstantMesh and MeshLRM use mesh representation in their second stage, notable uneven artifacts are still observable upon a zoom-in inspection. Additionally, all baseline methods incorrectly close the surface of the copper bell. We also provide quantitative results in Tab. 1. Although our baselines include four methods released just one or two months before the time of submission, our MeshFormer significantly outperforms many of them and achieves the best performance on most metrics across two datasets. For the color LPIPS metric, our performance is very similar to MeshLRM’s, despite a perceptual loss being their main training loss term. We also highlight that many of the baselines require over one hundred GPUs for training, whereas our model can be efficiently trained with just 8 GPUs. Please refer to Sec. 4.4 for analysis on training efficiency.\n\n# 4.3 Application: Text to 3D\n\nIn addition to the single image to 3D, MeshFormer can also be integrated with 2D diffusion models to enable various 3D object generation tasks. For example, we follow the framework proposed by [37] to finetune Stable Diffusion [56] and build a text-to-multi-view model. By integrating this\n\n![](images/c886e98a1e9801f8d927b2d189de4d5e161a3371dd0f0f168dc63f4322120269.jpg)\n\nL:a pyramid shaped burrito with a   \nslice cut out of it   \nR:a red panda\n\nL:a metal sculpture of a lion's head, L:arabbit,animated movie character,high highly detailed detail 3d model R:a DSLR photo of a delicious croissant R: a DSLR photo of a frog wearing a sweater\n\nFigure 4: Application: Text to 3D. Given a text prompt, a 2D diffusion model first predicts multiview RGB and normal images, which are then fed to MeshFormer for 3D reconstruction. Please refer to the supplementary for comparisons with Instant3D [27].\n\nTable 2: We compare methods using limited training resources. Evaluated on the GSO [11] dataset.   \n\n<html><body><table><tr><td>Method</td><td>Training Resources|F-Score ↑</td><td></td><td>CD↓</td><td>PSNR-C↑</td><td></td><td></td><td>LPIPS-C↓PSNR-N↑ LPIPS-N↓</td></tr><tr><td>MeshLRM[79]</td><td>8×H100 48h</td><td>0.925</td><td>0.0397</td><td>21.09</td><td>0.26</td><td>21.69</td><td>0.22</td></tr><tr><td>Ours</td><td></td><td>0.960</td><td>0.0317</td><td>21.41</td><td>0.20</td><td>23.01</td><td>0.15</td></tr></table></body></html>\n\nmodel, along with the normal prediction from Zero $1 2 3 + +$ [59], with MeshFormer, we can enable the task of text to 3D. Figure 4 shows some interesting results, where we convert a single text prompt into a high-quality 3D mesh in just a few seconds. Please refer to the supplemental materials for a qualitative comparison with one of the state-of-the-art text-to-3D methods, Instant3D [27].\n\n# 4.4 Analysis and Ablation Study\n\nExplicit 3D structure vs. Triplane In Section 4.2, we demonstrated that MeshFormer outperforms baseline methods that primarily utilize the triplane representation. Here, we highlight two additional advantages of using the explicit 3D voxel structure: training efficiency and the avoidance of “triplane artifacts”. Without leveraging explicit 3D structure, existing triplane-based large reconstruction models require extensive computing resources for training. For example, TripoSR requires 176 A100 GPUs for five days of training. InstantMesh relies on OpenLRM [15], which requires 128 A100 GPUs for three days of training. MeshLRM also utilizes similar resources during training. By utilizing explicit 3D structure and projective bias, our MeshFormer can be trained much more efficiently using only 8 GPUs. To better understand the gap, we trained both MeshLRM and our MeshFormer under very limited training resources, and the results are shown in Table 2. When using only 8 GPUs for two days, we found that MeshLRM failed to converge and experienced significant performance degradation compared to the results shown in Table 1, while our MeshFormer had already converged to a decent result, close to the fully-trained version, demonstrating superior training efficiency.\n\nWe observe that the triplane typically generates results with axis-aligned artifacts, as shown in Fig.3 (5th row, please zoom in). As demonstrated in the supplementary (Fig. 7), these artifacts also cause difficulties for MeshLRM [79] in capturing the words on objects. These limitations are likely caused by the limited number of triplane tokens (e.g., $3 2 \\times 3 2 \\times 3 )$ ), constrained by the global attention, which often leads to artifacts at the boundaries of the triplane patches. In contrast, MeshFormer leverages sparse voxels, supports a higher feature resolution of $2 \\bar { 5 } 6 ^ { 3 }$ , and is free from such artifacts.\n\nNormal Input and SDF supervision As shown in Table 3 (a), the performance significantly drops when multi-view input normal maps are removed, indicating that the geometric guidance and clues provided by normal images are crucial for facilitating network training, particularly for local geometric details. In (f), we replace ground truth normal maps with normal predictions by Zero $1 2 3 + +$ [59] and observe a notable performance gap compared to (g). This indicates that although predicted multi-view normal images can be beneficial, existing 2D diffusion models still have room for improvement in generating more accurate results. See supplementary for qualitative examples. As shown in (b), if we remove the SDF loss after the first epoch and train the network using only surface rendering losses, the geometry learning quickly deteriorates, resulting in poor geometry. This explains why existing methods [27, 79] typically employ complex multi-stage training and use volume rendering to learn a coarse NeRF in the initial stage. By leveraging explicit 3D SDF supervision as strong geometric regularization, we enable a unified single-stage training, using mesh as the only representation.\n\nTable 3: Ablation Study on the GSO [11] dataset. -C denotes color renderings, and -N denotes normal renderings. CD stands for Chamfer distance. By default, ground truth multi-view images are used to exclude the influence of errors from 2D diffusion models.   \n\n<html><body><table><tr><td>一</td><td>Setting</td><td></td><td>PSNR-C↑ LPIPS-C↓PSNR-N↑ LPIPS-N↓F-Score↑</td><td></td><td></td><td></td><td>CD↓</td></tr><tr><td>a</td><td>w/o normal input</td><td>24.82</td><td>0.129</td><td>24.85</td><td>0.107</td><td>0.964</td><td>0.024</td></tr><tr><td>b</td><td>w/o SDF supervision</td><td>20.72</td><td>0.244</td><td>20.42</td><td>0.257</td><td>0.940</td><td>0.035</td></tr><tr><td>C</td><td>w/o transformer layer</td><td>26.63</td><td>0.101</td><td>29.80</td><td>0.036</td><td>0.992</td><td>0.013</td></tr><tr><td>d</td><td>w/o projection-aware cross-attention</td><td>25.48</td><td>0.155</td><td>29.01</td><td>0.045</td><td>0.991</td><td>0.013</td></tr><tr><td>e</td><td>w/o geometry enhancement</td><td>27.95</td><td>0.085</td><td>29.10</td><td>0.048</td><td>0.992</td><td>0.012</td></tr><tr><td>f</td><td>w/ pred normal</td><td>26.84</td><td>0.096</td><td>26.99</td><td>0.067</td><td>0.987</td><td>0.017</td></tr><tr><td>g</td><td>full</td><td>28.15</td><td>0.083</td><td>29.80</td><td>0.036</td><td>0.992</td><td>0.012</td></tr></table></body></html>\n\nProjection-Aware Cross-Attention and Transformer Layers We propose to utilize projectionaware cross-attention to precisely aggregate multiview projected 2D features for each 3D voxel. In conventional learning-based multi-view stereo (MVS) methods [3, 38], average or max pooling is typically employed for feature aggregation. In Table 3 (d), we replace the cross-attention with a simple average pooling and we observe a significant performance drop. This verifies that projection-aware cross-attention provides a more effective way for 3D-2D interaction while simple average pooling may fail to handle the occlusion and visibility issues. In the bottleneck of the UNet, we treat all 3D (sparse) voxels as a sequence of tokens and apply transformer layers to them. As shown in row (c), after removing these layers, we observe a performance drop in metrics related to texture quality. This indicates that texture learning requires more extensive priors and benefits more from the transformer layers.\n\n![](images/566f5fb21a0a89f5fb5cc0c932d710f6faf2e3739785a99c7579e6bb1ba43459.jpg)  \nFigure 5: Geometry enhancement generates sharper details. Please zoom in to see the details.\n\nGeometry Enhancement We propose to learn an additional normal map texture and apply a traditional algorithm as post-processing for geometry enhancement during inference. As shown in Figure 5, the geometry enhancement aligns the mesh geometry with the learned normal texture and generates fine-grained sharp details. In some cases (such as the wolf), the meshes output by the network are already good enough, and the difference caused by the enhancement tends to be subtle. Row (e) also quantitatively verifies the effectiveness of the module.\n\n# 5 Conclusion and Limitations\n\nWe present MeshFormer, an open-world sparse-view reconstruction model that leverages explicit 3D native structure, supervision signals, and input guidance. MeshFormer can be conveniently trained in a unified single-stage manner and efficiently with just 8 GPUs. It generates high-quality meshes with fine-grained geometric details and outperforms baselines trained with over one hundred GPUs.\n\nMeshFormer relies on 2D models to generate multi-view RGB and normal images from a single input image or text prompt. However, existing models still have limited capabilities to generate consistent multi-view images, which can cause a performance drop. Strategies to improve model robustness against such imperfect predictions are worth further exploration, and we leave this as future work.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n现有开放世界3D重建模型在缺乏足够3D归纳偏置的情况下，存在训练成本高、难以提取高质量3D网格的问题。高质量3D网格在渲染、模拟和3D打印等众多应用中至关重要，因此解决上述问题对于推动3D资产创建的发展具有重要意义。\\n\\n**方法概述**\\n本文提出MeshFormer，这是一种开放世界稀疏视图重建模型，通过明确利用3D原生结构、输入引导和训练监督，结合3D稀疏体素存储特征、transformers与3D卷积，以高效训练并生成高质量纹理网格。\\n\\n**主要贡献与效果**\\n- 引入MeshFormer，能在几秒内生成具有细粒度几何细节的高质量3D纹理网格，仅需8个GPU进行训练，优于需超百个GPU训练的基线模型。在GSO数据集上，MeshFormer的F - Score达到0.963，高于MeshLRM的0.956；Chamfer距离为0.031，低于MeshLRM的0.033；PSNR达到21.47，高于MeshLRM的21.31；LPIPS为0.20，与MeshLRM的0.19相近。在OmniObject3D数据集上，MeshFormer的F - Score达到0.914，高于MeshLRM的0.910；Chamfer距离为0.043，低于MeshLRM的0.045；PSNR达到18.14，高于MeshLRM的18.10；LPIPS为0.27，与MeshLRM的0.26相近。\\n- 提出结合3D（稀疏）卷积和transformers的新颖架构，通过明确利用3D结构和投影偏置，促进更好更快的学习。\\n- 提出统一的单阶段训练策略，结合表面渲染和显式3D几何监督来生成高质量网格；首次引入多视图法线图像作为前馈重建网络的输入，并预测额外的3D法线纹理以增强几何形状。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nMeshFormer的核心思想是明确利用3D原生结构、输入引导和训练监督，以解决现有方法在3D重建中的问题。通过将特征存储在3D稀疏体素中，结合transformers与3D卷积，利用显式3D结构和投影偏置，同时引入多视图法线图像作为输入，并结合Signed Distance Function (SDF)监督与表面渲染，实现高效训练并生成高质量纹理网格。\\n\\n**创新点**\\n- 现有方法多采用三平面表示，缺乏显式3D空间结构，而MeshFormer采用3D体素表示，明确保留3D空间结构。\\n- 提出投影感知交叉注意力机制，以自适应聚合多视图特征，解决遮挡和可见性问题，优于传统的平均或最大池化操作。\\n- 提出学习3D法线纹理并进行后处理的几何增强方法，使训练更稳定，能生成更精细的几何细节。\\n- 采用统一的单阶段训练策略，结合表面渲染和显式3D SDF监督，避免了复杂的多阶段训练过程。\\n\\n**具体实现步骤**\\n1. **3D表示和模型架构**：\\n    - 选择3D体素表示而非三平面表示，以保留显式3D空间结构。\\n    - 提出VoxelFormer和SparseVoxelFormer，遵循3D UNet架构，在瓶颈处集成transformer。通过投影感知交叉注意力层，使3D体素聚合多视图图像的2D局部特征，经过迭代处理压缩3D体积，再用Transformer增强模型表达能力，最后用卷积解码高分辨率3D特征体积。具体而言，输入的多视图RGB和法线图像先由2D特征提取器（如可训练的DINOv2）处理生成多视图补丁特征，每个3D体素投影到多个视图插值RGB和法线特征，通过交叉注意力进行特征聚合。\\n    - 采用粗到细的特征生成策略，先由VoxelFormer预测低分辨率（如$64^3$）3D占用体积，再细分占用体素创建高分辨率稀疏体素（如$256^3$），由SparseVoxelFormer预测其特征，最后三线性插值近表面3D点的特征并输入MLP学习相应场。\\n2. **统一单阶段训练**：结合表面渲染和SDF监督，让模型学习SDF场，以显式3D监督和2D渲染损失进行训练。采用三个小型MLP学习SDF场、3D颜色纹理和3D法线纹理，使用双Marching Cubes提取网格，用NVDiffRast进行可微表面渲染，计算渲染损失。训练损失公式为：$\\mathcal { L } = \\lambda _ { 1 } \\mathcal { L } _ { \\mathrm { M S E } } ^ { \\mathrm { c o l o r } } + \\lambda _ { 2 } \\mathcal { L } _ { \\mathrm { L P I P S } } ^ { \\mathrm { c o l o r } } + \\lambda _ { 3 } \\mathcal { L } _ { \\mathrm { M S E } } ^ { \\mathrm { n o r m a l } } + \\lambda _ { 4 } \\mathcal { L } _ { \\mathrm { L P I P S } } ^ { \\mathrm { n o r m a l } } + \\lambda _ { 5 } \\mathcal { L } _ { \\mathrm { o c c } } + \\lambda _ { 6 } \\mathcal { L } _ { \\mathrm { S D F } }$，其中$L _ { \\mathrm { o c c } }$和$L _ { \\mathrm { S D F } }$是占用和SDF体积的MSE损失，$\\lambda _ { i }$表示各损失项的权重。\\n3. **细粒度几何细节**：\\n    - 输入法线引导：除多视图RGB图像外，MeshFormer还将多视图法线图作为输入，这些法线图可由近期的开放世界法线估计模型（如Zero1$23++$ v1.2）生成。在实验中，将其转换到统一的世界坐标系后，利用投影感知交叉注意力引导3D重建，可提升网络捕捉几何细节的能力，提高最终网格质量。\\n    - 几何增强：学习3D法线纹理，通过计算MLP查询的法线图的法线损失，解耦法线纹理学习和基础几何学习。在推理时，使用后处理算法根据学习的法线纹理细化网格几何。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\nOne - 2 - 3 - 45++、TripoSR、CRM、LGM、InstantMesh、MeshLRM。\\n\\n**性能对比**\\n*   **在 [F - Score] 指标上：** 本文方法MeshFormer在GSO数据集上达到了 **0.963**，优于One - 2 - 3 - 45++ (0.936)、TripoSR (0.896)、CRM (0.886)、LGM (0.776)、InstantMesh (0.934)、MeshLRM (0.956)；在OmniObject3D数据集上达到了 **0.914**，优于One - 2 - 3 - 45++ (0.871)、TripoSR (0.895)、CRM (0.821)、LGM (0.635)、InstantMesh (0.889)、MeshLRM (0.910)。\\n*   **在 [Chamfer Distance (CD)] 指标上：** 本文方法MeshFormer在GSO数据集上为 **0.031**，低于One - 2 - 3 - 45++ (0.039)、TripoSR (0.047)、CRM (0.051)、LGM (0.074)、InstantMesh (0.037)、MeshLRM (0.033)；在OmniObject3D数据集上为 **0.043**，低于One - 2 - 3 - 45++ (0.054)、TripoSR (0.048)、CRM (0.065)、LGM (0.114)、InstantMesh (0.049)、MeshLRM (0.045)。\\n*   **在 [PSNR] 指标上：** 本文方法MeshFormer在GSO数据集上达到了 **21.47**，高于One - 2 - 3 - 45++ (20.97)、TripoSR (19.85)、CRM (19.99)、LGM (18.52)、InstantMesh (20.90)、MeshLRM (21.31)；在OmniObject3D数据集上达到了 **18.14**，高于One - 2 - 3 - 45++ (17.08)、TripoSR (17.68)、CRM (16.01)、LGM (14.75)、InstantMesh (17.61)、MeshLRM (18.10)。\\n*   **在 [LPIPS] 指标上：** 本文方法MeshFormer在GSO数据集上为 **0.20**，低于One - 2 - 3 - 45++ (0.21)、TripoSR (0.26)、CRM (0.27)、LGM (0.35)、InstantMesh (0.22)；与MeshLRM (0.19)相近；在OmniObject3D数据集上为 **0.27**，低于One - 2 - 3 - 45++ (0.31)、TripoSR (0.28)、CRM (0.34)、LGM (0.45)、InstantMesh (0.28)；与MeshLRM (0.26)相近。\\n\\n**训练资源对比**：许多基线模型需要超百个GPU进行训练，如TripoSR需要176个A100 GPU训练五天，InstantMesh依赖的OpenLRM需要128个A100 GPU训练三天；而MeshFormer仅需8个GPU即可高效训练，在使用8个H100 GPU训练时，MeshFormer在GSO数据集上的F - Score达到0.960，优于MeshLRM的0.925，显示出更好的训练效率。\",\n    \"keywords\": \"### 关键词\\n\\n- 3D重建 (3D Reconstruction, N/A)\\n- 稀疏视图重建 (Sparse - View Reconstruction, N/A)\\n- MeshFormer (MeshFormer, N/A)\\n- 3D体素表示 (3D Voxel Representation, N/A)\\n- 投影感知交叉注意力 (Projection - Aware Cross Attention, N/A)\\n- 有符号距离函数 (Signed Distance Function, SDF)\\n- 单图像到3D (Single - Image - to - 3D, N/A)\\n- 文本到3D (Text - to - 3D, N/A)\"\n}"
}