{
    "source": "Semantic Scholar",
    "arxiv_id": "2407.00626",
    "link": "https://arxiv.org/abs/2407.00626",
    "pdf_link": "https://arxiv.org/pdf/2407.00626.pdf",
    "title": "Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models",
    "authors": [
        "Sangwoong Yoon",
        "Himchan Hwang",
        "Dohyun Kwon",
        "Yung-Kyun Noh",
        "Frank C. Park"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-06-30",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 3,
    "influential_citation_count": 1,
    "institutions": [
        "Korea Institute for Advanced Study",
        "Seoul National University",
        "University of Seoul",
        "Hanyang University",
        "Saige Research"
    ],
    "paper_content": "# Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models\n\nSangwoong $\\mathbf { Y o o n } ^ { 1 }$ , Himchan Hwang2, Dohyun Kwon1,3†, Yung-Kyun $\\mathbf { N o h ^ { 1 , 4 \\dagger } }$ , Frank C. Park2,5† 1Korea Institute for Advanced Study, 2Seoul National University, 3University of Seoul, 4Hanyang University, 5Saige Research, ${ } ^ { \\dag } { \\bf C } { \\bf o }$ -corresponding Authors swyoon@kias.re.kr, himchan@robotics.snu.ac.kr, dhkwon@uos.ac.kr, nohyung@hanyang.ac.kr, fcp@snu.ac.kr\n\n# Abstract\n\nWe present a maximum entropy inverse reinforcement learning (IRL) approach for improving the sample quality of diffusion generative models, especially when the number of generation time steps is small. Similar to how IRL trains a policy based on the reward function learned from expert demonstrations, we train (or fine-tune) a diffusion model using the log probability density estimated from training data. Since we employ an energy-based model (EBM) to represent the log density, our approach boils down to the joint training of a diffusion model and an EBM. Our IRL formulation, named Diffusion by Maximum Entropy IRL (DxMI), is a minimax problem that reaches equilibrium when both models converge to the data distribution. The entropy maximization plays a key role in DxMI, facilitating the exploration of the diffusion model and ensuring the convergence of the EBM. We also propose Diffusion by Dynamic Programming $( \\mathrm { D x D P } )$ , a novel reinforcement learning algorithm for diffusion models, as a subroutine in DxMI. DxDP makes the diffusion model update in DxMI efficient by transforming the original problem into an optimal control formulation where value functions replace back-propagation in time. Our empirical studies show that diffusion models fine-tuned using DxMI can generate high-quality samples in as few as 4 and 10 steps. Additionally, DxMI enables the training of an EBM without MCMC, stabilizing EBM training dynamics and enhancing anomaly detection performance.\n\n# 1 Introduction\n\nGenerative modeling is a form of imitation learning. Just as an imitation learner produces an action that mimics a demonstration from an expert, a generative model synthesizes a sample resembling the training data. In generative modeling, the expert to be imitated corresponds to the underlying data generation process. The intimate connection between generative modeling and imitation learning is already well appreciated in the literature [1, 2].\n\nThe connection to imitation learning plays a central role in diffusion models [3, 4], which generate samples by transforming a Gaussian noise through iterative additive refinements. The training of a diffusion model is essentially an instance of behavioral cloning [5], a widely adopted imitation learning algorithm that mimics an expert’s action at each state. During training, a diffusion model is optimized to follow a predefined diffusion trajectory that interpolates between noise and data. The trajectory provides a step-by-step demonstration of how to transform Gaussian noise into a sample, allowing diffusion models to achieve a new state-of-the-art in many generation tasks.\n\nBehavioral cloning is also responsible for the diffusion model’s key limitation, the slow generation speed. A behavior-cloned policy is not reliable when the state distribution deviates from the expert demonstration [6, 7]. Likewise, the sample quality from a diffusion model degrades as the gap between training and generation grows. A diffusion model is typically trained to follow a fine-grained diffusion trajectory of 1,000 or more steps. Since 1,000 neural network evaluations are prohibitively expensive, fewer steps are often used during generation, incurring the distribution shift from the training phase and thus degraded sample quality. Speeding up a diffusion model while maintaining its high sample quality is a problem of great practical value, becoming an active field of research [8, 9, 10, 11, 12, 13].\n\n![](images/0cf90ff697f8ca953b9c8a414bda46762d6282fb053ad7469451345601a42082.jpg)  \nFigure 1: (Left) Overview of $\\mathrm { D x M I }$ . The diffusion model $\\pi ( \\mathbf { x } )$ is trained using the energy of $q ( \\mathbf { x } )$ as a reward. The EBM $q ( \\mathbf { x } )$ is trained using samples from $\\pi ( \\mathbf { x } )$ as negative samples. (Right) ImageNet 64 generation examples from a 10-step diffusion model before DxMI fine-tuning (up) and after fine-tuning (down). Only the last six steps out of ten are shown.\n\nThe slow generation in diffusion models can be addressed by employing inverse reinforcement learning (IRL; [14]), another imitation learning approach. Unlike behavioral cloning, which blindly mimics the expert’s action at each state, the IRL approach first infers the reward function that explains the trajectory. When applied to diffusion models, IRL allows a faster generation trajectory to be found by guiding a sampler using the learned reward [11, 12, 13]. This approach is more frequently referred to as adversarial training because a common choice for the reward function is a discriminator classifying the training data and diffusion model’s samples. However, resembling GAN, this adversarial approach may have similar drawbacks, such as limited exploration.\n\nThe first contribution of this paper is formulating maximum entropy IRL [15, 16, 2] for a diffusion model. Our formulation, named Diffusion by Maximum Entropy IRL (DxMI, pronounced \"diby-me\"), is a minimax problem that jointly optimizes a diffusion model and an energy-based model (EBM). In DxMI, the EBM provides the estimated log density as the reward signal for the diffusion model. Then, the diffusion model is trained to maximize the reward from EBM while simultaneously maximizing the entropy of generated samples. The maximization of entropy in DxMI facilitates exploration and stabilizes the training dynamics, as shown in reinforcement learning (RL) [17], IRL [15], and EBM [18] literature. Furthermore, the entropy maximization lets the minimax problem have an equilibrium when both the diffusion model and the EBM become the data distribution.\n\nThe diffusion model update step in DxMI is equivalent to maximum entropy RL. However, this step is challenging to perform for two reasons. First, the diffusion model update requires the gradient of the marginal entropy of a diffusion model’s samples, which is difficult to estimate for discrete-time diffusion models, such as DDPM [3]. Second, back-propagating the gradient through the whole diffusion model is often infeasible due to memory constraints. Even with sufficient memory, the gradient may explode or vanish during propagation through time, causing training instability [19].\n\nOur second contribution is Diffusion by Dynamic Programming (DxDP), a novel maximum entropy RL algorithm for updating a diffusion model without the above-mentioned difficulties. First, DxDP mitigates the marginal entropy estimation issue by optimizing the upper bound of the original objective. In the upper bound, the marginal entropy is replaced by conditional entropies, which are easier to compute for a diffusion model. Then, DxDP removes the need for back-propagation in time by interpreting the objective as an optimal control problem and applying dynamic programming using value functions. The connection between optimal control and diffusion models is increasingly gaining attention [20, 21, 22], but DxDP is the first instance of applying discrete-time dynamic programming directly to train a diffusion model. Compared to policy gradient methods, we empirically find that DxDP converges faster and provides stronger diffusion models. As an RL algorithm, DxDP may have broader utility, such as fine-tuning a diffusion model from human or AI feedback.\n\nWe provide experimental results demonstrating the effectiveness of DxMI in training diffusion models and EBMs. On image generation tasks, DxMI can train strong short-run diffusion models that generate samples in 4 or 10 neural network evaluations. Also, DxMI can be used to train strong energy-based anomaly detectors. Notably, DxMI provides a novel way to train EBM without MCMC, which is computationally demanding and sensitive to the choice of hyperparameters.\n\nThe paper is structured as follows. In Section 2, we introduce the necessary preliminaries. Section 3 presents the DxMI framework, and Section 4 proposes the DxDP algorithm. Experimental results and related work are provided in Sections 5 and 6, respectively. Section 7 concludes the paper. The code for DxMI can be found in https://github.com/swyoon/Diffusion-by-MaxEntIRL.git.\n\n# 2 Preliminaries\n\nDiffusion Models. The diffusion model refers to a range of generative models trained by reversing the trajectory from the data distribution to the noise distribution. Among diffusion models, we focus on discrete-time stochastic samplers, such as DDPM [3], which synthesize a sample through the following iteration producing $\\mathbf { x } _ { 0 } , \\mathbf { x } _ { 1 } , \\ldots , \\mathbf { x } _ { T } \\in \\mathbb { R } ^ { D }$ :\n\n$$\n\\mathbf { \\nabla } \\times \\mathcal { N } ( 0 , I ) \\quad \\mathrm { a n d } \\quad \\mathbf { x } _ { t + 1 } = a _ { t } \\mathbf { x } _ { t } + \\mu ( \\mathbf { x } _ { t } , t ) + \\sigma _ { t } \\epsilon _ { t } \\quad \\mathrm { f o r } \\quad t = 0 , 1 , \\dots , T - 1 ,\n$$\n\nwhere $\\epsilon _ { t } \\sim \\mathcal { N } ( 0 , I )$ and $\\mu ( \\mathbf { x } , t )$ is the output of a neural network. The coefficients $\\boldsymbol { a } _ { t } \\in \\mathbb { R }$ and $\\sigma _ { t } \\in \\mathbb { R } _ { > 0 }$ are constants. Note that we reverse the time direction in a diffusion model from the convention to be consistent with RL. The final state $\\mathbf { x } _ { T }$ is the sample generated by the diffusion model, and its marginal distribution is $\\pi ( \\mathbf { x } _ { T } )$ . We will often drop the subscript $T$ and write the distribution as $\\pi ( \\mathbf { x } )$ . The conditional distribution of a transition in Eq. (1) is denoted as $\\pi ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } )$ . For continuous-time diffusion models [23], Eq. (1) corresponds to using the Euler-Maruyama solver.\n\nThe generation process in Eq. (1) defines a $T$ -horizon Markov Decision Process (MDP) except for the reward. State $\\mathbf { s } _ { t }$ and action ${ \\bf a } _ { t }$ are defined as $\\mathbf { s } _ { t } = ( \\mathbf { x } _ { t } , t )$ and ${ \\bf a } _ { t } = { \\bf x } _ { t + 1 }$ . The transition dynamics is defined as $p ( \\mathbf { s } _ { t + 1 } | \\mathbf { s } _ { t } , \\mathbf { a } _ { t } ) = \\delta _ { ( \\mathbf { a } _ { t } , t + 1 ) }$ , where $\\delta _ { ( \\mathbf { x } _ { t } , t ) }$ is a Dirac delta function at $\\left( \\mathbf { x } _ { t } , t \\right)$ . With a reward function defined, a diffusion model can be trained with RL [11, 24, 19, 25]. In this paper, we consider a case where the reward is the log data density $\\log p ( \\mathbf { x } )$ , which is unknown.\n\nEnergy-Based Models. An energy-based model (EBM) $q ( \\mathbf { x } )$ uses a scalar function called an energy $E ( \\mathbf { x } )$ to represent a probability distribution:\n\n$$\nq ( \\mathbf { x } ) = \\frac { 1 } { Z } \\exp ( - E ( \\mathbf { x } ) / \\tau ) , \\quad E : \\mathcal { X }  \\mathbb { R } ,\n$$\n\nwhere $\\tau > 0$ is temperature, $\\chi$ is the compact domain of data, and $\\begin{array} { r } { Z = \\int _ { \\mathcal { X } } \\exp ( - E ( \\mathbf { x } ) / \\tau ) d \\mathbf { x } < \\infty } \\end{array}$ is the normalization constant.\n\nThe standard method for training an EBM is by minimizing KL divergence between data $p ( \\mathbf { x } )$ and EBM $q ( \\mathbf { x } )$ , i.e., $\\mathrm { m i n } _ { q } K L ( p | | q )$ , where $\\begin{array} { r } { K L ( p | | q ) : = \\int _ { \\mathcal { X } } p ( \\mathbf { x } ) \\log ( p ( \\mathbf { \\bar { x } } ) / q ( \\mathbf { x } ) ) d \\mathbf { x } } \\end{array}$ . Computing the gradient of $K L ( p | | q )$ with respect to EBM parameters requires MCMC sampling, which is computationally demanding and sensitive to hyperparameters. The algorithm presented in this paper serves as an alternative method for training an EBM without MCMC.\n\nEBMs have a profound connection to maximum entropy IRL, where Eq. (2) serves as a model for an expert’s policy [15, 16, 2]. In maximum entropy IRL, $\\mathbf { x }$ corresponds to an action (or a sequence of actions), and $E ( \\mathbf { x } )$ represents the expert’s cost of the action. The expert is then assumed to generate actions following $q ( \\mathbf { x } )$ . This assumption embodies the maximum entropy principle because Eq. (2) is a distribution that minimizes the cost while maximizing the entropy of the action. Here, $\\tau$ balances cost minimization and entropy maximization.\n\n# 3 Diffusion by Maximum Entropy Inverse Reinforcement Learning\n\n# 3.1 Objective: Generalized Contrastive Divergence\n\nWe aim to minimize the (reverse) KL divergence between a diffusion model $\\pi ( \\mathbf { x } )$ and the data density $p ( \\mathbf { x } )$ . This minimization can improve the sample quality of $\\pi ( \\mathbf { x } )$ , particularly when $T$ is small.\n\n$$\n\\operatorname* { m i n } _ { \\pi \\in \\Pi } K L ( \\pi ( \\mathbf { x } ) | | p ( \\mathbf { x } ) ) = \\operatorname* { m a x } _ { \\pi \\in \\Pi } \\mathbb { E } _ { \\pi } [ \\log p ( \\mathbf { x } ) ] + \\mathcal { H } ( \\pi ( \\mathbf { x } ) ) ,\n$$\n\nwhere $\\Pi$ is the set of feasible $\\pi ( \\mathbf { x } )$ ’s, and $\\begin{array} { r } { \\mathcal { H } ( { \\boldsymbol \\pi } ) = - \\int { \\boldsymbol \\pi } ( { \\bf x } ) \\log \\pi ( { \\bf x } ) d { \\bf x } } \\end{array}$ is the differential entropy. This minimization is a maximum entropy RL problem: The log data density $\\log p ( \\mathbf { x } )$ is the reward, and $\\pi ( \\mathbf { x } )$ is the stochastic policy. However, we cannot solve Eq. (3) directly since $\\log p ( \\mathbf { x } )$ is unknown in a typical generative modeling setting. Instead, training data from $p ( \\mathbf { x } )$ are available, allowing us to employ an Inverse RL approach.\n\nIn this paper, we present Diffusion by Maximum Entropy IRL $\\mathbf { ( D x M I ) }$ as an IRL approach for solving Eq. (3). We employ an EBM $q ( \\mathbf { x } )$ (Eq. (2)) as a surrogate for $p ( \\mathbf { x } )$ and use $\\log q ( \\mathbf { x } )$ as the reward for training the diffusion model instead of $\\log p ( \\mathbf { x } )$ . At the same time, we train $q ( \\mathbf { x } )$ to be close to $p ( \\mathbf { x } )$ by minimizing the divergence between $p ( \\mathbf { x } )$ and $q ( \\mathbf { x } )$ :\n\n$$\n\\operatorname* { m i n } _ { \\pi \\in \\Pi } K L ( \\pi ( \\mathbf { x } ) | | q ( \\mathbf { x } ) ) \\quad { \\mathrm { a n d } } \\quad \\operatorname* { m i n } _ { q \\in { \\mathcal { Q } } } K L ( p ( \\mathbf { x } ) | | q ( \\mathbf { x } ) ) ,\n$$\n\nwhere $\\mathcal { Q }$ is the feasible set of EBMs. When the two KL divergences become zero, $p ( \\mathbf { x } ) = q ( \\mathbf { x } ) =$ $\\pi ( \\mathbf { x } )$ is achieved. However, $\\begin{array} { r } { \\operatorname* { m i n } _ { q \\in \\mathcal { Q } } K L ( p ( \\mathbf { x } ) | | q ( \\mathbf { x } ) ) } \\end{array}$ is difficult due to the normalization constant of $q ( \\mathbf { x } )$ . Instead, we consider an alternative minimax formulation inspired by Contrastive Divergence (CD; [26]), a celebrated algorithm for training an EBM.\n\nObjective. Let $p ( \\mathbf { x } )$ be the data distribution. Suppose that $\\mathcal { Q }$ and $\\Pi$ are the feasible sets of the EBM $q ( \\mathbf { x } )$ and the diffusion model $\\cdot$ , respectively. The learning problem of DxMI is formulated as follows:\n\n$$\n\\operatorname* { m i n } _ { q \\in \\mathcal { Q } } \\operatorname* { m a x } _ { \\pi \\in \\Pi } K L ( p ( \\mathbf { x } ) | | q ( \\mathbf { x } ) ) - K L ( \\pi ( \\pi ) | | q ( \\mathbf { x } ) ) .\n$$\n\nWe shall call Eq. (5) Generalized CD (GCD), because Eq. (5) generalizes CD by incorporating a general class of samplers. CD [26] is originally given as $\\begin{array} { r } { \\operatorname* { m i n } _ { q \\in \\mathcal { Q } } \\bar { K } L ( p ( \\mathbf { x } ) | | q ( \\mathbf { x } ) ) \\bar { - } K L ( \\bar { \\nu } _ { p , q } ^ { k } ( \\mathbf { x } ) \\bar { | | q ( \\mathbf { x } ) ) } } \\end{array}$ . Here, $\\nu _ { p , q } ^ { k } ( \\mathbf { x } )$ is a $k$ -step MCMC sample distribution where Markov chains having $q ( \\mathbf { x } )$ as a stationary edixsptreinbsuetiofninatreodinuictinalgizaedmafrxoomp $p ( \\mathbf { x } )$ r. GCD replaces $\\nu _ { p , q } ^ { k } ( \\mathbf { x } )$ with a general sampler $\\pi ( \\mathbf { x } )$ at the\n\nWhen the models are well-specified, i.e., $p ( \\mathbf { x } ) \\in \\mathcal { Q } = \\Pi$ , Nash equilibrium of GCD is $p ( \\mathbf { x } ) =$ $q ( \\mathbf { x } ) = \\pi ( \\mathbf { x } )$ , which is identical to the solution of Eq. (4). Meanwhile, there is no need to compute the normalization constant, as the two KL divergences cancel the normalization constant out. Note that the objective function (Eq. (5)) can be negative, allowing $q ( \\mathbf { x } )$ be closer to $p ( \\mathbf { x } )$ than $\\pi ( \\mathbf { x } )$ .\n\nOur main contribution is exploring the application of discrete-time diffusion models (Eq. (1)) as $\\pi ( \\mathbf { x } )$ in GCD and not discovering GCD for the first time. GCD is mathematically equivalent to a formulation called variational maximum likelihood or adversarial EBM, which has appeared several times in EBM literature [27, 28, 29, 30, 31, 32, 33]. However, none of them have investigated the use of a diffusion model as $\\pi ( \\mathbf { x } )$ , where optimization and entropy estimation are challenging. We discuss the challenges in Section 3.2 and provide a novel algorithm to address them in Section 4.\n\n# 3.2 Alternating Update of EBM and Diffusion Model\n\nIn DxMI, we update a diffusion model and an EBM in an alternative manner to find the Nash equilibrium. We write $\\theta$ and $\\phi$ as the parameters of the energy $E _ { \\theta } ( \\mathbf { x } )$ and a diffusion model $\\pi _ { \\phi } ( \\mathbf { x } )$ , respectively. While EBM update is straightforward, we require a subroutine described in Section 4 for updating the diffusion model. The entire procedure of ${ \\bf D x M I }$ is summarized in Algorithm 1.\n\nEBM Update. The optimization with respect to EBM is written as $\\begin{array} { r } { \\operatorname* { m i n } _ { \\theta } \\mathbb { E } _ { p ( \\mathbf { x } ) } [ E _ { \\theta } ( \\mathbf { x } ) ] \\ - \\ } \\end{array}$ $\\mathbb { E } _ { \\pi _ { \\phi } ( \\mathbf { x } ) } [ E _ { \\theta } ( \\mathbf { x } ) ]$ . During the update, we also regularize the energy by the square of energies $\\gamma ( \\mathbb { E } _ { p ( \\mathbf { x } ) } [ E _ { \\theta } ( \\mathbf { x } ) _ { . } ^ { 2 } ] + \\mathbb { E } _ { \\pi _ { \\phi } ( \\underline { { \\mathbf { x } } } ) } [ E _ { \\theta } ( \\mathbf { x } ) _ { . } ^ { 2 } ] ) _ { . }$ for $\\gamma > 0$ to ensure the energy is bounded. We set $\\gamma = 1$ unless stated otherwise. This regularizer is widely adopted in EBM [34, 35].\n\nDifficulty of Diffusion Model Update. Ideally, diffusion model parameter $\\phi$ should be updated by minimizing $K L ( \\pi _ { \\phi } | | q _ { \\theta } ) = \\mathbb { E } _ { \\pi _ { \\phi } ( \\mathbf { x } ) } ^ { - } [ E _ { \\theta } ( \\mathbf { x } ) / \\tau ] - \\mathcal { H } ( \\pi _ { \\phi } ( \\mathbf { x } ) )$ . However, this update is difficult in practice for two reasons.\n\nFirst, marginal entropy $\\mathcal { H } ( \\pi _ { \\phi } ( \\mathbf { x } ) )$ is difficult to estimate. Discrete-time diffusion models (Eq. (1)) do not provide an efficient way to evaluate $\\log \\pi _ { \\phi } ( \\mathbf { x } )$ required in the computation of $\\mathcal { H } ( \\pi _ { \\phi } ( \\mathbf { x } ) )$ , unlike some continuous-time models, e.g., continuous normalizing flows [36, 23]. Other entropy estimators based on $\\mathbf { k }$ -nearest neighbors [37] or variational methods [38, 39] do not scale well to high-dimensional spaces. Second, propagating the gradient through time in a diffusion model may require significant memory. Also, the gradient may explode or vanish during propagation, making the training unstable [19].\n\nAlgorithm 1 Diffusion by Maximum Entropy IRL   \n\n<html><body><table><tr><td></td><td></td><td>1: Input: Dataset D,Energy E(x),Value Vt(xt),and Sampler T(xo:T)</td></tr><tr><td>2:St←Ot</td><td></td><td>// Initialize Adaptive Velocity Regularization (AVR) //Minibatch dimension is omitted for brevity.</td></tr><tr><td></td><td>3:for x in D do</td><td></td></tr><tr><td>4:</td><td>Sample Xo:T ~ T(X0:T).</td><td></td></tr><tr><td>5:</td><td>minθ Eθ(x)-Eθ(xτ)+γ(Eθ(x)²+Eθ(xτ)²)</td><td>// Energy update</td></tr><tr><td>6:</td><td>for t=T-1,...,0do</td><td>// Value update</td></tr><tr><td>7:</td><td>min[sg[Vt+1(xt+1)]+TlogΠ(xt+1|xt)+|/xt-Xt+1|²-Vt(xt)]</td><td></td></tr><tr><td>8:</td><td>end for</td><td></td></tr><tr><td>9:</td><td>for Xt randomly chosen among Xo:T do</td><td>/ Sampler update</td></tr><tr><td>10:</td><td>Sample one-step: Xt+1 ~ T(Xt+1|Xt)</td><td>// Reparametrization trick</td></tr><tr><td>11:</td><td>min Vt+1(xt+1(Φ))+Tlogπ(Xt+1|Xt)+|xt-Xt+1()|² // Xt+1 isafunctionofΦ</td><td></td></tr><tr><td>12: 13:</td><td>end for s²← αs² +(1-α)|/xt - Xt+1ll²/D</td><td>I/ AVR update</td></tr></table></body></html>\n\n# 4 Diffusion by Dynamic Programming\n\nIn this section, we present a novel RL algorithm for a diffusion model, Diffusion by Dynamic Programming (DxDP), which addresses the difficulties in updating a diffusion model for the reward. DxDP leverages optimal control formulation and value functions to perform the diffusion model update in ${ \\bf D x M I }$ efficiently. Note that DxDP can be used separately from DxMI to train a diffusion model for an arbitrary reward.\n\n# 4.1 Optimal Control Formulation\n\nInstead of solving $\\begin{array} { r } { \\operatorname* { m i n } _ { \\phi } K L ( \\pi _ { \\phi } ( \\mathbf x _ { T } ) | | q _ { \\theta } ( \\mathbf x _ { T } ) ) } \\end{array}$ directly, we minimize its upper bound obtained from the data processing inequality:\n\n$$\nK L ( \\pi _ { \\phi } ( \\mathbf { x } _ { T } ) | | q _ { \\theta } ( \\mathbf { x } _ { T } ) ) \\leq K L ( \\pi _ { \\phi } ( \\mathbf { x } _ { 0 : T } ) | | q _ { \\theta } ( \\mathbf { x } _ { T } ) \\widetilde { q } ( \\mathbf { x } _ { 0 : T - 1 } | \\mathbf { x } _ { T } ) ) .\n$$\n\nHere, we introduce an auxiliary distribution $\\tilde { q } \\big ( \\mathbf { x } _ { 0 : T - 1 } | \\mathbf { x } _ { T } \\big )$ , and the inequality holds for an arbitrary choice of $\\tilde { q } \\big ( \\mathbf { x } _ { 0 : T - 1 } \\big | \\mathbf { x } _ { T } \\big )$ . In this paper, we consider a particularly simple case where $\\tilde { q } \\big ( \\mathbf { x } _ { 0 : T - 1 } \\big | \\mathbf { x } _ { T } \\big )$ is factorized into conditional Gaussians as follows:\n\n$$\n\\tilde { q } ( \\mathbf { x } _ { 0 : T - 1 } | \\mathbf { x } _ { T } ) = \\prod _ { t = 0 } ^ { T - 1 } \\tilde { q } ( \\mathbf { x } _ { t } | \\mathbf { x } _ { t + 1 } ) , \\mathrm { ~ w h e r e ~ } \\tilde { q } ( \\mathbf { x } _ { t } | \\mathbf { x } _ { t + 1 } ) = \\mathcal { N } ( \\mathbf { x } _ { t + 1 } , s _ { t } ^ { 2 } I ) , \\quad s _ { t } > 0 .\n$$\n\nNow we minimize the right-hand side of Eq. (6): $\\begin{array} { r } { \\operatorname* { m i n } _ { \\phi } K L ( \\pi _ { \\phi } ( \\mathbf { x } _ { 0 : T } ) | | q _ { \\theta } ( \\mathbf { x } _ { T } ) \\tilde { q } ( \\mathbf { x } _ { 0 : T - 1 } | \\mathbf { x } _ { T } ) ) } \\end{array}$ . When we plug in the definitions of each distribution, multiply by $\\tau$ , and discard all constants, we obtain the following problem:\n\n$$\n\\operatorname* { m i n } _ { \\phi } \\underset { \\pi _ { \\phi } ( \\mathbf { x } _ { 0 : T } ) } { \\mathbb { E } } \\left[ E _ { \\theta } ( \\mathbf { x } _ { T } ) + \\tau \\sum _ { t = 0 } ^ { T - 1 } \\log \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } ) + \\tau \\sum _ { t = 0 } ^ { T - 1 } \\frac { 1 } { 2 s _ { t } ^ { 2 } } | | \\mathbf { x } _ { t + 1 } - \\mathbf { x } _ { t } | | ^ { 2 } \\right] ,\n$$\n\nwhich is an optimal control problem. The controller $\\pi _ { \\phi } ( \\cdot )$ is optimized to minimize the terminal cost $E _ { \\theta } ( \\mathbf { x } _ { T } )$ plus the running costs for each transition $\\left( \\mathbf { x } _ { t } , \\mathbf { x } _ { t + 1 } \\right)$ . The first running cost $\\log \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } )$ is responsible for conditional entropy maximization, because $\\mathbb { E } _ { \\boldsymbol \\pi } \\big [ \\mathrm { l o g } \\boldsymbol { \\pi } _ { \\phi } ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } ) \\big ] = - \\mathcal { H } \\big ( \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } ) \\big )$ . The second running cost regularizes the “velocity\" $| | \\mathbf { x } _ { t + 1 } - \\mathbf { x } _ { t } | | ^ { 2 }$ . The temperature $\\tau$ balances between the terminal and running costs.\n\nWe have circumvented the marginal entropy computation in GCD, as all terms in Eq. (8) are easily computable. For the diffusion model considered in this paper (Eq. (1)), the conditional entropy has a particularly simple expression $\\mathcal { H } ( \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } ) ) = D \\bar { \\log } \\sigma _ { t } + \\bar { 0 } . 5 D \\log 2 \\pi$ . Therefore, optimizing the entropy running cost amounts to learning $\\sigma _ { t }$ ’s in diffusion, and we treat $\\sigma _ { t }$ ’s as a part of the diffusion model parameter $\\phi$ in ${ \\bf D x M I }$ .\n\n![](images/f9ac4b15da2b6740d98825ba8ac31b78d91d6de70b47cfbaa3db3daa4558691e.jpg)  \nFigure 2: 2D density estimation on 8 Gaussians. Red shades indicate the energy (white is low), and the dots are generated samples.\n\nTable 1: Quantitative results for 8 Gaussians experiment. $S W$ denotes the sliced Wasserstein distance between samples and data. AUC is computed for classification between data and uniform noise using the energy. The standard deviation is computed from 5 independent samplings. The ideal maximum value of AUC is about 0.906.   \n\n<html><body><table><tr><td>Method</td><td>T</td><td>Pretrain</td><td>T</td><td>SW (↓) AUC (↑)</td></tr><tr><td>DDPM</td><td>5</td><td></td><td>0.967±0.005</td><td></td></tr><tr><td>DDPM</td><td>10</td><td></td><td>0.824±0.002</td><td></td></tr><tr><td>DDPM</td><td>100</td><td></td><td>0.241±0.003</td><td></td></tr><tr><td>DDPM</td><td>1000</td><td></td><td>= 0.123±0.014</td><td>1</td></tr><tr><td>DxMI</td><td>5</td><td>○</td><td>0 0.074±0.018</td><td>0.707</td></tr><tr><td>DxMI</td><td>5</td><td>○</td><td>0.01 0.074±0.017</td><td>0.751</td></tr><tr><td>DxMI</td><td>5</td><td>○</td><td>0.1 0.068±0.004</td><td>0.898</td></tr><tr><td>DxMI</td><td>5</td><td>O</td><td>1 1.030±0.004</td><td>0.842</td></tr><tr><td>DxMI</td><td>5</td><td>×</td><td>0.1 0.076±0.011</td><td>0.883</td></tr></table></body></html>\n\n# 4.2 Dynamic Programming\n\nWe propose a dynamic programming approach for solving Eq. (8). Dynamic programming introduces value functions to break down the problem into smaller problems at each timestep, removing the need for back-propagation in time. Then, a policy, a diffusion model in our case, is optimized through iterative alternating applications of policy evaluation and policy improvement steps.\n\nValue Function. A value function, or cost-to-go function $V _ { \\psi } ^ { t } ( { \\bf x } _ { t } )$ , is defined as the expected sum of the future costs starting from $\\mathbf { x } _ { t }$ , following $\\pi$ . We write the parameters of a value function as $\\psi$ .\n\n$$\nV _ { \\psi } ^ { t } ( \\mathbf { x } _ { t } ) = \\mathbb { E } _ { \\boldsymbol \\pi } \\left[ E _ { \\theta } ( \\mathbf { x } _ { T } ) + \\tau \\sum _ { t ^ { \\prime } = t } ^ { T - 1 } \\log \\pi _ { \\phi } ( \\mathbf { x } _ { t ^ { \\prime } + 1 } | \\mathbf { x } _ { t ^ { \\prime } } ) + \\sum _ { t ^ { \\prime } = t } ^ { T - 1 } \\frac { \\tau } { 2 s _ { t ^ { \\prime } } ^ { 2 } } | | \\mathbf { x } _ { t ^ { \\prime } + 1 } - \\mathbf { x } _ { t ^ { \\prime } } | | ^ { 2 } \\middle | \\mathbf { x } _ { t } \\right] ,\n$$\n\nfor $t = 0 , \\dots , T - 1$ . Note that $V ^ { T } ( { \\bf x } _ { T } ) = E ( { \\bf x } _ { T } )$ . A value function can be implemented with a neural network, but there are multiple design choices, such as whether to share the parameters with $\\pi ( \\mathbf { x } )$ or $E ( \\mathbf { x } )$ , and also whether the parameters should be shared across time. We explore the options in our experiments.\n\nPolicy Evaluation. During policy evaluation, we estimate the value function for the current diffusion model by minimizing the Bellman residual, resulting in the temporal difference update.\n\n$$\n\\operatorname* { m i n } _ { \\psi } \\mathbb { E } _ { \\mathbf { x } _ { t } , \\mathbf { x } _ { t + 1 } \\sim \\pi } [ ( \\mathrm { s g } [ V _ { \\psi } ^ { t + 1 } ( \\mathbf { x } _ { t + 1 } ) ] + \\tau \\log \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } \\vert \\mathbf { x } _ { t } ) + \\frac { \\tau } { 2 s _ { t } ^ { 2 } } \\vert \\vert \\mathbf { x } _ { t } - \\mathbf { x } _ { t + 1 } \\vert \\vert ^ { 2 } - V _ { \\psi } ^ { t } ( \\mathbf { x } _ { t } ) ) ^ { 2 } ] ,\n$$\n\nwhere $\\operatorname { s g } [ \\cdot ]$ denotes a stop-gradient operator indicating that gradient is not computed for the term.\n\nPolicy Improvement. The estimated value is used to improve the diffusion model. For each $\\mathbf { x } _ { t }$ in a trajectory $\\mathbf { x } _ { 0 : T }$ sampled from $\\pi _ { \\phi } \\big ( \\mathbf { x } _ { 0 : T } \\big )$ , the diffusion model is optimized to minimize the next-state value and the running costs.\n\n$$\n\\operatorname* { m i n } _ { \\phi } \\mathbb { E } _ { \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } ) } \\left[ V _ { \\psi } ^ { t + 1 } ( \\mathbf { x } _ { t + 1 } ) + \\tau \\log \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } ) + \\frac { \\tau } { 2 s _ { t } ^ { 2 } } | | \\mathbf { x } _ { t } - \\mathbf { x } _ { t + 1 } | | ^ { 2 } \\middle | \\mathbf { x } _ { t } \\right] .\n$$\n\nIn practice, each iteration of policy evaluation and improvement involves a single gradient step.\n\nAdaptive Velocity Regularization (AVR). We additionally propose a method for systematically determining the hyperparameter $s _ { t }$ ’s of the auxiliary distribution $\\tilde { q } \\big ( \\mathbf { x } _ { 0 : T - 1 } | \\mathbf { x } _ { T } \\big )$ . We can optimize $s _ { t }$ such that the inequality Eq. (6) is as tight as possible by solving $\\begin{array} { r } { \\operatorname* { m i n } _ { s _ { 0 } , \\ldots , s _ { T - 1 } } K L ( \\pi _ { \\phi } ( \\mathbf x _ { 0 : T } ) | | q _ { \\theta } ( \\mathbf x _ { T } ) \\widetilde { q } ( \\mathbf x _ { 0 : T - 1 } | \\mathbf x _ { T } ) ) } \\end{array}$ . After calculation (details in Appendix A), the optimal $s _ { t } ^ { * }$ can be obtained analytically: $( s _ { t } ^ { * } ) ^ { 2 } = \\mathbb { E } _ { \\mathbf { x } _ { t } , \\mathbf { x } _ { t + 1 } \\sim \\pi } [ | | \\mathbf { x } _ { t } - \\mathbf { x } _ { t + 1 } | | ^ { 2 } ] / D$ . In practice, we can use exponential moving average to compute the expectation $\\mathbb { E } _ { \\mathbf { x } _ { t } , \\mathbf { x } _ { t + 1 } \\sim \\pi } [ | | \\mathbf { x } _ { t } - \\mathbf { x } _ { t + 1 } | | ^ { 2 } ]$ during training: $s _ { t } ^ { 2 }  \\alpha s _ { t } ^ { 2 } + ( 1 - \\alpha ) \\vert \\vert \\mathbf { x } _ { t } - \\mathbf { x } _ { t + 1 } \\vert \\vert ^ { 2 } / D$ where we set $\\alpha = 0 . 9 9$ for all experiment.\n\n# 4.3 Techniques for Image Generation Experiments\n\nWhen using DxDP for image generation, one of the most common applications of diffusion models, we introduce several design choices to DxDP to enhance performance and training stability. The resulting algorithm is summarized in Algorithm 2.\n\nTime-Independent Value Function. In image generation experiments (Section 5.2), we let the value function be independent of time, i.e., $V _ { \\psi } ^ { \\tilde { t } } ( \\mathbf { x } _ { t } ) = V _ { \\psi } ( \\mathbf { x } _ { t } )$ . Removing the time dependence reduces the number of parameters to be trained. More importantly, a time-independent value function can learn better representation because the value function is exposed to diverse inputs, including both noisy and clean images. On the contrary, a time-dependent value function $V _ { \\psi } ^ { t } ( { \\bf x } _ { t } )$ never observes samples having different noise levels than the noise level of $\\mathbf { x } _ { t }$ .\n\nTime Cost. Also, in the value update (Eq. (10)) step of image generation experiments, we introduce time cost function $R ( t ) > 0$ , which replaces the running cost terms $\\tau \\log \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } \\vert \\mathbf { x } _ { t } ) +$ $\\tau | | \\mathbf { x } _ { t } - \\mathbf { x } _ { t + 1 } | | ^ { 2 } / ( 2 s _ { t } ^ { 2 } )$ . The time cost $R ( t )$ only depends on time $t$ . The modified value update equation is given as follows:\n\n$$\n\\underset { \\psi } { \\operatorname* { m i n } } \\mathbb { E } _ { \\mathbf { x } _ { t } , \\mathbf { x } _ { t + 1 } \\sim \\pi } [ ( \\mathbf { s g } [ V _ { \\psi } ( \\mathbf { x } _ { t + 1 } ) ] + R ( t ) - V _ { \\psi } ( \\mathbf { x } _ { t } ) ) ^ { 2 } ] .\n$$\n\nMeanwhile, we retain the running cost terms in the diffusion model (policy) update step (Eq. (11)). The time cost $R ( t )$ is predetermined and fixed throughout training. The introduction of time cost is motivated by the observation that the running costs can fluctuate during the initial stage of training, posing difficulty in value function learning. The time cost stabilizes the training by reducing this variability. Moreover, the time cost ensures that the value function decreases monotonically over time. Such monotonicity is known to be beneficial in IRL for episodic tasks [16].\n\nWe employ two types of $R ( t )$ : “linear\" and “sigmoid\". A linear time cost is given as $R ( t ) = c$ where we use $c = 0 . 0 5$ . The linear time cost encourages the value to decrease linearly as time progresses. The sigmoid time cost is $R ( t ) = \\sigma ( - t + T / 2 ) ) - \\sigma ( - t - 1 + T / 2 ) )$ , where $\\sigma ( x ) \\bar { = } ( 1 + \\exp ( \\bar { - } x ) ) ^ { - 1 }$ . With the sigmoid time cost, the value function is trained to follow a sigmoid function centered at $T / 2$ when plotted against the time. Other forms of $R ( t )$ are also possible.\n\nSeparate tuning of $\\tau$ . In image generation, we assign different values of temperature $\\tau$ for entropy regularization $\\log \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } \\mathbf { \\bar { \\rho } } \\mathbf { | x } _ { t } )$ and velocity regularization $| | \\mathbf { x } _ { t } - \\mathbf { x } _ { t + 1 } | | ^ { 2 } / ( \\bar { 2 } s _ { t } ^ { 2 } )$ , such that the resulting running cost becomes $\\tau _ { 1 } \\log \\pi _ { \\phi } ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } ) + \\tau _ { 2 } | | \\mathbf { x } _ { t } - \\mathbf { x } _ { t + 1 } | | ^ { 2 } / ( 2 s _ { t } ^ { 2 } )$ . Typically, we found $\\tau _ { 1 } > \\tau _ { 2 }$ beneficial, indicating the benefit of exploration. Setting $\\tau _ { 1 } \\neq \\tau _ { 2 }$ does not violate our maximum entropy formulation, as scaling $\\tau _ { 2 }$ is equivalent to scaling $s _ { t } ^ { 2 \\bar { \\cdot } } \\mathrm { s }$ , which can be set arbitrarily.\n\n# 5 Experiments\n\nIn this section, we provide empirical studies that demonstrate the effectiveness of ${ \\bf D x M I }$ in training a diffusion model and an EBM. We first present a 2D example, followed by image generation and anomaly detection experiments. More details on experiments can be found in Appendix C.\n\n# 5.1 2D Synthetic Data\n\nWe illustrate how DxMI works on 2D 8 Gaussians data. DxMI is applied to train a five-step diffusion model $T = 5$ ) with a corresponding time-dependent value network, both parametrized by timeconditioned multi-layer perceptron (MLP). The last time step $\\left[ T = 5 \\right]$ ) of the value network is treated as the energy. The sample quality is measured with sliced Wasserstein distance (SW) to test data. Also, we quantify the quality of an energy function through the classification performance to uniform noise samples (Table 1).\n\nFirst, we investigate the effect of maximum entropy regularization $\\tau$ . Setting an appropriate value for $\\tau$ greatly benefits the quality of both the energy and the samples. When $\\tau = 0 . 1$ , the samples from ${ \\bf D x M I }$ have smaller SW than the samples from a full-length DDPM do. The energy also accurately captures the data distribution, scoring high AUC against the uniform noise. Without entropy regularization $\\mathit { \\check { \\Psi } } _ { \\mathcal { T } } = 0 \\mathit { \\Psi } _ { \\mathit { \\Phi } }$ ), DxMI becomes similar to GAN [40]. The generated samples align moderately well with the training data, but the energy does not reflect the data distribution. When $\\tau$ is too large $\\langle \\tau = 1 \\rangle$ ), the generated samples are close to noise. In this regime, DxMI behaves similarly to Noise Contrastive Estimation [41], enabling energy function learning to a certain extent. These effects are visualized in Fig. 2.\n\nTable 2: CIFAR-10 unconditional image generation. $\\dagger$ : the starting point of ${ \\bf D x M I }$ fine-tuning.   \n\n<html><body><table><tr><td colspan=\"3\">NFE FID (↓) Rec.(↑)</td></tr><tr><td>Score SDE(VE) [23]</td><td>2000 2.20</td><td>0.59</td></tr><tr><td>PD[10]</td><td>8 2.57</td><td></td></tr><tr><td>Consistency Model [42]</td><td>2 2.93</td><td></td></tr><tr><td>PD[10]</td><td>1 8.34</td><td></td></tr><tr><td>2-RectifiedFlow [43]</td><td>1 4.85</td><td>0.50</td></tr><tr><td>Consistency Model [42]</td><td>1 3.55</td><td></td></tr><tr><td>StyleGAN-XL [44]</td><td>1 1.85</td><td>0.47</td></tr><tr><td>Backbone:DDPM</td><td></td><td></td></tr><tr><td>DDPM[3]</td><td>1000 3.21</td><td>0.57</td></tr><tr><td>FastDPM† [8]</td><td>10 35.85</td><td>0.29</td></tr><tr><td>DDIM [45]</td><td>10 13.36</td><td></td></tr><tr><td>SFT-PG [11]</td><td>10 4.82</td><td>0.606</td></tr><tr><td>DxMI</td><td>3.19</td><td>0.625</td></tr><tr><td>T=0</td><td>10 10 3.77</td><td>0.613</td></tr><tr><td>Linear time cost</td><td>10 3.39</td><td>0.595</td></tr><tr><td>No time cost</td><td>10 5.18</td><td>0.595</td></tr><tr><td>DxMI+ Value Guidance</td><td>10 3.17</td><td>0.623</td></tr><tr><td>T=0</td><td>10 3.72</td><td>0.613</td></tr><tr><td>Backbone:DDGAN</td><td></td><td></td></tr><tr><td>DDGAN† [46]</td><td>4 4.15</td><td>0.523</td></tr><tr><td></td><td></td><td></td></tr><tr><td>DxMI</td><td>4 3.65</td><td>0.532</td></tr></table></body></html>\n\nTable 3: ImageNet $6 4 \\times 6 4$ conditional image generation. †: the starting point of ${ \\bf D x M I }$ fine-tuning.   \n\n<html><body><table><tr><td colspan=\"5\">NFE FID(↓) Prec.(↑) Rec.(↑)</td></tr><tr><td>ADM[47] DFNO [48]</td><td>250 1</td><td>2.07 8.35</td><td>0.74 =</td><td>0.63</td></tr><tr><td>PD [10] BigGAN-deep [49]</td><td>1 1</td><td>15.39 4.06</td><td>0.59 0.79</td><td>0.62 0.48</td></tr><tr><td>Backbone:EDM</td><td></td><td></td><td></td><td></td></tr><tr><td>EDM(Heun) [50]</td><td>79</td><td>2.44</td><td>0.71</td><td>0.67</td></tr><tr><td>EDM(Ancestral)†</td><td>10</td><td>50.27</td><td>0.37</td><td>0.35</td></tr><tr><td>EDM (Ancestral)†</td><td>4</td><td>82.95</td><td>0.26</td><td>0.25</td></tr><tr><td></td><td>2</td><td>4.70</td><td>0.69</td><td>0.64</td></tr><tr><td>Consistency Model [42]</td><td>1</td><td>6.20</td><td>0.68</td><td></td></tr><tr><td>Consistency Model [42]</td><td>10</td><td>2.68</td><td>0.777</td><td>0.63</td></tr><tr><td>DxMI T=0</td><td>10</td><td>2.72</td><td>0.782</td><td>0.574 0.564</td></tr><tr><td>Linear time cost</td><td>10</td><td>2.81</td><td>0.742</td><td>0.594</td></tr><tr><td>DxMI+Value Guidance</td><td>10</td><td>2.67</td><td>0.780</td><td>0.574</td></tr><tr><td>T=0</td><td>10</td><td>2.76</td><td>0.786</td><td>0.560</td></tr><tr><td>DxMI</td><td>4</td><td>3.21</td><td>0.758</td><td>0.568</td></tr><tr><td>T=0</td><td>4</td><td>3.65</td><td>0.767</td><td>0.552</td></tr><tr><td>Linear time cost</td><td>4</td><td>3.40</td><td>0.762</td><td>0.554</td></tr><tr><td>DxMI+Value Guidance</td><td>4</td><td>3.18</td><td>0.763</td><td>0.566</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>T=0</td><td>4</td><td>3.67</td><td>0.770</td><td>0.541</td></tr></table></body></html>\n\nNext, we experiment on whether pre-training a sampler as DDPM helps DxMI. Table 1 suggests that the pre-training is beneficial but not necessary to make DxMI work. We also visualize the value functions in Fig. 3 and find that the time evolution of value interpolates the data distribution and a Gaussian distribution.\n\n# 5.2 Image Generation: Training Diffusion Models with Small $T$\n\nOn image generation tasks, we show that DxMI can be used to fine-tune a diffusion model with reduced generation steps, such as $T = 4$ or 10. We test DxMI on unconditional CIFAR-10 [52] $( 3 2 \\times 3 2 )$ , conditional ImageNet [53] downsampled to $6 4 \\times 6 4$ , and LSUN Bedroom [54] $( 2 5 6 \\times 2 5 6 )$ , using three diffusion model backbones, DDPM [3], DDGAN [46], and variance exploding version of EDM [50]. The results can be found in Table 2, 3, and 4. Starting from\n\nTable 4: LSUN Bedroom $2 5 6 \\times 2 5 6$ unconditional image generation.   \n\n<html><body><table><tr><td colspan=\"5\">NFE FID(↓) Prec.(↑) Rec.(↑)</td></tr><tr><td>StyleGAN2 [51]</td><td>1</td><td>2.35</td><td>0.59</td><td>0.48</td></tr><tr><td>Backbone:EDM</td><td></td><td></td><td></td><td></td></tr><tr><td>EDM[50]</td><td>79</td><td>2.44</td><td>0.71</td><td>0.67</td></tr><tr><td>Consistency Model [42]</td><td>2</td><td>5.22</td><td>0.68</td><td>0.39</td></tr><tr><td>DxMI</td><td>4</td><td>5.93</td><td>0.563</td><td>0.477</td></tr></table></body></html>\n\na publicly available checkpoint of each pretrained backbone, we first adjust the noise schedule for the target sampling steps $T$ . When adjusting the noise, for DDPM, we follow the schedule of FastDPM [8], and for EDM, we use Eq. (5) of [50]. No adjustment is made for DDGAN, which was originally built for $T = 4$ . The adjusted models are used as the starting point of DxMI training. A single CIFAR-10 run reaches the best FID in less than 4 hours on four A100 GPUs. We set $\\tau _ { 1 } = 0 . 1$ and $\\tau _ { 2 } = 0 . 0 1$ . The sigmoid time cost is used for all image generation experiments. The sample quality is measured by FID [55], Precision (Prec., [56]), and Recall (Rec., [56]). ResNet is used as our value function and is trained from scratch. More experimental details are in Appendix C.2.\n\nShort-run diffusion models fine-tuned by DxMI display competitive sample quality. Unlike distillation methods, which are often limited by their teacher model’s performance, DxMI can surpass the pretrained starting point. Although DxMI does not support single-step generation, DxMI offers a principled approach to training a high-quality generative model with a moderate computation burden (Appendix D). Note that DDGAN does not fit the formulation of $\\mathrm { D x M I }$ , as $\\pi ( \\mathbf { x } _ { t + 1 } | \\mathbf { x } _ { t } )$ in DDGAN is not Gaussian. Nevertheless, DxMI can still enhance sample quality, showing its robustness.\n\nFurthermore, DxMI outperforms SFT-PG [11], another IRL approach implemented with a policy gradient. For a fair comparison, we have ensured that the backbone and the initial checkpoint of SFT-PG and DxMI are identical. Thus, the performance gap can be attributed to the two differences between SFT-PG and DxMI. First, DxMI uses dynamic programming instead of policy gradient. In DxDP, the value function is more directly utilized to guide the learning of the diffusion model. Meanwhile, in policy gradient, the role of the value function is variance reduction. As SFT-PG also requires a value function during training, the computational overhead of ${ \\bf D x M I }$ is nearly identical to SFT-PG. Second, DxMI incorporates the maximum entropy principle, which facilitates exploration.\n\nWe also conduct ablation studies for the components of DxMI and append the results in Table 2 and 3. First, the temperature parameters are set to zero $\\tau _ { 1 } = \\tau _ { 2 } = 0$ in the sampler update (11). Then, we compare the linear time cost to the sigmoid time cost. In both cases, We observe the increase in FID and the decrease in Recall.\n\nTo investigate whether the trained value function captures useful information, we implement value guidance, where we shift the trajectory of generation slightly along the value function gradient, similarly to classifier guidance [47] and discriminator guidance [57]. When sampling the next step $\\mathbf { x } _ { t + 1 }$ , we add a small drift with coefficient $\\lambda$ , i.e., $\\mathbf { x } _ { t + 1 }  \\mathbf { x } _ { t + 1 } - \\lambda \\sigma _ { t } \\nabla _ { \\mathbf { x } _ { t + 1 } } V _ { \\psi } ( \\mathbf { x } _ { t + 1 } )$ . We observe sample quality metric improvement until $\\lambda$ is 0.5. This observation suggests that the value function gradient is aligned well with the data density gradient.\n\n# 5.3 Energy-Based Anomaly Detection and Localization\n\nWe demonstrate the ability of ${ \\bf D x M I }$ to train an accurate energy function on an anomaly detection task using the MVTec-AD dataset [61], which contains $2 2 4 \\times 2 2 4$ RGB images of 15 object categories. We follow the multi-class problem setup proposed by [60]. The training dataset contains normal object images from 15 categories without any labels. The test set consists of both normal and defective object images, each provided with an anomaly label and a mask indicating the defect location. The goal is to detect and localize anomalies, with performance measured by AUC computed per object category. This setting is challenging because the energy function should reflect the multi-modal data distribution. Following the preprocessing protocol in [60, 59], each image is transformed into a $2 7 2 \\times 1 4 \\times 1 4$ vector using a pre-trained EfficientNet-b4 [62]. DxMI is conducted in a 272-dimensional space, treat\n\nTable 5: MVTec-AD multi-class anomaly detection and localization experiment. Anomaly detection (DET) and localization (LOC) performance are measured in AUC. Due to the space constraint, only the average AUC over 15 classes is presented. The full results are provided in Table 6.\n\n<html><body><table><tr><td>Model</td><td>DET</td><td>LOC</td></tr><tr><td>DRAEM[58]</td><td>88.1</td><td>87.2</td></tr><tr><td>MPDR[59]</td><td>96.0</td><td>96.7</td></tr><tr><td>UniAD [60]</td><td>96.5±0.08</td><td>96.8±0.02</td></tr><tr><td>DxMI</td><td>97.0 ±0.11</td><td>97.1±0.02</td></tr><tr><td>T=0</td><td>67.9±5.90</td><td>84.6±4.02</td></tr></table></body></html>\n\ning each spatial coordinate independently. With the trained energy function, we can evaluate the energy value of $1 4 \\mathrm { x } 1 4$ spatial features and use max pooling and bilinear interpolation for anomaly detection and localization, respectively.\n\nWe use separate networks for the energy function and the value function in this experiment, as the primary goal is to obtain an accurate energy function. We employ an autoencoder architecture for the energy function, treating the reconstruction error of a sample as its energy [63]. The diffusion model and the value function are five-step time-conditioned MLPs. Unlike conventional diffusion models, DxMI allows for a flexible choice of $\\pi ( \\mathbf { x } _ { 0 } )$ . We set the initial distribution for the sampler to the data distribution applied with noise, aiming to identify the energy value more precisely near the data distribution. More experimental details can be found in Appendix C.3.\n\nDxMI demonstrates strong anomaly classification and localization performance, as shown in Table 5. This result indicates that the trained energy function effectively captures the boundary of normal data. When entropy maximization is disabled by $\\tau = 0$ , the diffusion model fails to explore and only exploits regions of minimum energy, resulting in poor performance. We observe that a moderate level of $\\tau = 0 . 1$ benefits both the sampler and the energy function, as it encourages exploration and provides a suitable level of diversity in negative samples.\n\n# 6 Related Work\n\nFaster Diffusion Models. Significant effort has been dedicated to reducing the number of generation steps in diffusion models during sampling while preserving sample quality. One popular approach is to keep the trained diffusion model unchanged and improve the sampling phase independently by tuning the noise schedule [8, 64, 65, 9], improving differential equation solvers [50, 66, 67, 68], and utilizing non-Markovian formulations [45, 69, 70]. While these methods are training-free, the sample quality can be further improved when the neural network is directly tuned for short-run sampling. Distillation methods train a faster diffusion sampler using training signal from a longer-run diffusion model, showing strong performance [10, 71, 72, 48, 73, 43, 42, 74]. A distilled model usually cannot outperform the teacher model, but adversarial or IRL methods may exceed full-length diffusion models. Hybrid methods [13, 12] combine distillation with adversarial loss, while other methods [46, 75] apply adversarial training to each denoising step. DxMI and SFT-PG [11] rely fully on adversarial training for final samples, allowing beneficial deviations from the diffusion path and reducing statistical distance from the data.\n\nRL for Diffusion Model. RL is often employed to fine-tune diffusion models for a reward function. The source of the reward signal can be a computer program [24, 76, 20, 77, 78], or a human evaluator [19, 79, 25]. DxMI focuses on a setting where the estimated log data density is the reward. When RL is applied to diffusion models, the policy gradient [80] is the dominant choice [24, 76, 11, 77]. DxMI offers a value function-based approach as an alternative to the policy gradient. Maximum entropy RL for diffusion models is investigated in [20, 81, 82] but only in the continuous-time setting. DxDP investigates the discrete-time setting, which is more suitable for accelerating generation speed.\n\nEnergy-Based Models. DxMI provides a method of utilizing a diffusion model to eliminate the need for MCMC. Many existing EBM training algorithms rely on MCMC, which is computationally expensive and difficult to optimize for hyperparameters [34, 83, 18, 84, 85, 35, 63, 59]. Joint training of an EBM with a separate generative model is a widely employed strategy to avoid MCMC. EBMs can be trained jointly with a normalizing flow [86, 87], a generator [30, 88, 89], or a diffusion model [90, 33]. DxMI shares the objective function with several prior works in EBM [27, 28, 29, 30, 31, 33]. However, none of the works use a diffusion model directly as a sampler.\n\nRelated Theoretical Analyses. The convergence guarantees of entropy-regularized IRL are provided in [91, 92] under the assumption of a linear reward and the infinite time horizon. Their guarantees are not directly applicable to a practical instance of DxMI, mainly due to the nonlinearity of the reward function, the continuous state and action spaces, and the finite-horizon setting. Establishing the convergence guarantee for DxMI could be an important future research direction. On the other hand, theoretical analyses have been conducted on MaxEnt RL under finite state and action spaces [93], which is relevant for the discrete version of DxDP. More focused analysis on entropy regularized RL for diffusion models is provided in [94].\n\n# 7 Conclusion\n\nIn this paper, we leverage techniques from sequential decision making to tackle challenges in generative modeling, revealing a significant connection between these two fields. We anticipate that this connection will spur a variety of algorithmic innovations and find numerous practical applications.\n\nBroader Impacts. DxMI may facilitate deep fakes or fake news. However, trained on relatively low-resolution academic datasets, the models created during our experiments are not capable enough to cause realistic harm. Generative models trained solely using DxMI may possess fairness issues.\n\nLimitations. Training multiple components simultaneously, DxMI introduces several hyperparameters. To reduce the overhead of practitioners, we provide a hyperparameter exploration guideline in Appendix B. DxMI is not directly applicable to training a single-step generator. However, a diffusion model fine-tuned by DxMI can be distilled to a single-step generator. DxMI does not offer the flexibility of using a different value of generation steps $T$ during the test time. Direct theoretical analysis of DxMI is challenging since the models are built on deep neural networks. Theoretical analysis that rationalizes the empirical results will be an important direction for future work.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了扩散生成模型在生成时间步数较少时样本质量下降的问题。现有方法如行为克隆（Behavioral Cloning）在生成阶段与训练阶段的分布偏移时性能下降，导致生成速度慢且质量不稳定。\\n> *   该问题在图像生成、异常检测等需要高效高质量生成的应用场景中具有关键价值。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种最大熵逆强化学习（Maximum Entropy Inverse Reinforcement Learning, MaxEnt IRL）方法，称为DxMI，通过联合训练扩散模型和基于能量的模型（EBM）来提升样本质量。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **DxMI框架**：将扩散模型的训练问题转化为最大熵IRL问题，通过EBM提供奖励信号，实现高效生成。实验表明，DxMI可以在4或10步内生成高质量样本。\\n> *   **DxDP算法**：提出了一种基于动态规划的最大熵强化学习算法，用于高效更新扩散模型，避免了传统方法中的梯度爆炸或消失问题。\\n> *   **无MCMC的EBM训练**：DxMI提供了一种无需MCMC的EBM训练方法，显著提升了训练稳定性和异常检测性能（在MVTec-AD数据集上达到97.0%的AUC）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   DxMI的核心思想是通过最大熵IRL框架，将扩散模型的训练问题转化为一个极小极大问题，其中EBM提供奖励信号，扩散模型通过最大化熵和奖励来优化生成质量。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比**：传统扩散模型依赖于行为克隆，生成速度慢且质量不稳定；现有IRL方法（如对抗训练）存在探索不足的问题。\\n> *   **本文的改进**：DxMI通过最大熵IRL框架，结合EBM和动态规划算法（DxDP），实现了高效且稳定的训练。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **EBM更新**：通过最小化数据分布和扩散模型样本之间的能量差异，优化EBM参数。\\n> 2.  **扩散模型更新**：使用DxDP算法，通过动态规划优化扩散模型，避免梯度传播问题。\\n> 3.  **交替优化**：交替更新EBM和扩散模型，直至两者收敛到数据分布。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   DDPM、FastDPM、DDIM、SFT-PG、StyleGAN-XL、Consistency Model等。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在FID指标上**：DxMI在CIFAR-10数据集上达到3.19的FID，显著优于基线模型FastDPM（35.85）和DDIM（13.36）。与表现最佳的基线SFT-PG（4.82）相比，提升了1.63个点。\\n> *   **在生成速度上**：DxMI在10步内生成样本，速度与轻量级模型Consistency Model相当，但样本质量（FID为3.19）远超后者（2.93）。\\n> *   **在异常检测AUC上**：DxMI在MVTec-AD数据集上达到97.0%的AUC，显著优于基线模型DRAEM（88.1%）和MPDR（96.0%）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   扩散模型 (Diffusion Model, DM)\\n*   逆强化学习 (Inverse Reinforcement Learning, IRL)\\n*   基于能量的模型 (Energy-Based Model, EBM)\\n*   最大熵 (Maximum Entropy, MaxEnt)\\n*   动态规划 (Dynamic Programming, DP)\\n*   图像生成 (Image Generation, N/A)\\n*   异常检测 (Anomaly Detection, N/A)\"\n}\n```"
}