{
    "link": "https://arxiv.org/abs/2309.04062",
    "pdf_link": "https://arxiv.org/pdf/2309.04062",
    "title": "3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation",
    "authors": [
        "Sungjun Cho",
        "Dae-Woong Jeong",
        "Sung Moon Ko",
        "Jinwoo Kim",
        "Sehui Han",
        "Seunghoon Hong",
        "Honglak Lee",
        "Moontae Lee"
    ],
    "institutions": [
        "University of Wisconsin-Madison",
        "LG AI Research",
        "KAIST",
        "University of Illinois Chicago"
    ],
    "publication_date": "2023-09-08",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Physics"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "paper_content": "# 3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation\n\nSungjun Cho1, Dae-Woong Jeong2, Sung Moon $\\mathbf { { K } _ { 0 } } ^ { 2 }$ , Jinwoo $\\mathbf { K } \\mathbf { i m } ^ { 3 }$ , Sehui Han2, Seunghoon Hong3, Honglak Lee2, Moontae Lee2,4\n\n1University of Wisconsin-Madison 2LG AI Research 3KAIST 4University of Illinois Chicago   \nsungjuncho@cs.wisc.edu, dw.jeong $@$ lgresearch.ai, sungmoon. $\\operatorname { k o } \\ @$ lgresearch.ai, jinwoo-kim@kaist.ac.kr,   \nhansse.han $@$ lgresearch.ai, seunghoon.hong $@$ kaist.ac.kr, honglak $@$ lgresearch.ai, moontae.lee $@$ lgresearch.ai\n\n# Abstract\n\nPretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, leading to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In this paper, we propose a simple solution of denoiseand-distill (D&D), a self-supervised molecular representation learning method that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless application to downstream tasks with no access to 3D conformers. Experiments on real-world molecular property prediction datasets show that the graph encoder trained via D&D can infer 3D information based on the 2D graph and shows superior performance and label-efficiency against previous methods.\n\n# Introduction\n\nMolecular property prediction has gained much interest across the machine learning community, leading to breakthroughs in various applications such as drug discovery (Guvench 2016; Kanakaveti et al. 2017) and material design (Suh et al. 2020; Pyzer-Knapp, Li, and Aspuru-Guzik 2015; Schmidt et al. 2019; Pyzer-Knapp et al. 2022). As molecules can be represented as a $2 D$ graph with nodes and edges representing atoms and covalent bonds, many graph neural networks have been developed with promising results (Duvenaud et al. 2015; Defferrard, Bresson, and Vandergheynst 2016; Bruna et al. 2013; Coley et al. 2017; Scarselli et al. 2009; Jin et al. 2018). However, achieving high precision requires accurate ground-truth property labels which are very expensive to obtain. This limitation has motivated adaptation of self-supervised pretraining widely used in natural language processing (Devlin et al. 2018; Brown et al. 2020) and computer vision (He et al. 2020; Bachmann et al. 2022) onto molecular graphs with proxy objectives developed to instill useful knowledge into neural networks with unlabeled data. But existing 2D graph-based pretraining frameworks face a fundamental challenge: while the model is trained to learn representations that are invariant under various data augmentations, augmenting 2D graphs can catastrophically disrupt its topology, which renders the model unable to fully recover labels from augmented samples (Trivedi et al. 2022). As a result of this limitation, recent work has shown that existing 2D pretraining approaches do not show statistically meaningful performance improvements in downstream tasks (Sun 2022).\n\nAs an alternative, recent work have proposed incorporating 3D information to the pretraining objective, leveraging large unlabeled datasets of $3 D$ conformers, or point clouds of atoms floating in the physical space. While a natural task would be to reconstruct the input conformer, this may not induce generalizable knowledge as each conformer only represents a single local minima in a distribution of 3D configurations. On the other hand, the force field that controls the overall stabilization process provides significant chemical information that can be used across many different molecular properties (Mezey 2001). This naturally translates to pretraining via denoising conformers under perturbations, an approach that has shown state-of-the-art performance in diverse molecular property prediction benchmarks (Zaidi et al. 2022; Liu, Guo, and Tang 2022).\n\nDespite great performance, a model trained with denoising requires conformers downstream as well, and obtaining accurate conformers require expensive quantum mechanical computations. While there exist many rulebased (Riniker and Landrum 2015; Landrum 2016) as well as deep learning-based approaches (Ganea et al. 2021; Xu et al. 2022; Jing et al. 2022) for generating conformers, previous work have shown that existing methods fail to generate conformers quickly and accurately enough to be used in a large scale (Sta¨rk et al. 2022).\n\nIn light of such limitations, we propose D&D (Denoiseand-Distill), a self-supervised molecular representation learning framework that enjoys the best of both worlds. Figure 1 shows the overall pipeline of our work. D&D sequentially performs two steps: 1) we pretrain a 3D teacher model that denoises conformers artificially perturbed with Gaussian noise and 2) freeze the 3D teacher encoder and distill representations from the 3D teacher onto the 2D student. When given a downstream task with access to 2D molecular graphs only, the 3D teacher is discarded and the 2D student is finetuned towards the given task. As a result of distillation, D&D encourages the 2D graph encoder to exploit the topology of the molecular graph towards encoding the input molecule similarly to the 3D conformer encoder without any explicit supervision from property labels. Surprisingly, experiments on various molecular property prediction datasets indicate that the 2D graph representations from D&D can generalize to unseen molecules. To the best of our knowledge, our method is the first self-supervised molecular representation learning framework that adopts cross-modal knowledge distillation to transfer knowledge from a 3D denoiser onto a 2D graph encoder. We summarize our main contributions as follows:\n\n![](images/a01eb20396fe80c2a71c155dd6ef6a8128cd67157fa446d5dd33435512577c86.jpg)  \nFigure 1: Comparison between D&D and existing molecular pretraining frameworks. Top: 2D graph-based pretraining methods fail to bring significant benefit to downstream molecular property prediction. Middle: 3D denoising is effective in predicting molecular properties by approximately learning the force field in the physical space, but cannot be easily applied to downstream tasks where only 2D graphs are available. Bottom: Our method D&D allows practitioners to leverage knowledge from 3D denoising in downstream scenarios where only 2D molecular graphs are available without the need to generate 3D conformer via expensive computations or machine learning approaches.\n\n• We propose D&D, a two-step self-supervised molecular representation pretraining framework that performs 3Dto-2D cross-modal distillation. • Pretraining results show that under D&D, the 2D student model can closely mimic representations from the 3D teacher model using graph features and topology. Further analysis shows that the intermediate representations of the 2D student also aligns well with 3D geometry. • Experiments on the MoleculeNet benchmark and curated molecular property regression datasets show that D&D leads to significant knowledge transfer, and also performs well in downstream scenarios where the number of labeled training data points is limited.\n\n# Related Work\n\nIn this section, we first discuss previous work on knowledge distillation that inspired our approach. We also cover existing self-supervised pretraining approaches for molecular representation learning.\n\nKnowledge Distillation. Knowledge distillation (KD) was developed under the motivation of transferring knowledge learned by a large teacher model to a much more compact student model, thereby reducing the computational burden while preserving the predictive performance (Hinton et al. 2015). Example approaches in computer vision include distilling class probabilities as a soft target for classification models (Ba and Caruana 2014) or transferring intermediate representations of input images (Tian, Krishnan, and Isola 2019). For dense prediction tasks such as semantic segmentation, it has been shown that a structured KD approach that distills pixel-level features instead leads to improvements in performance (Liu et al. 2020). Another extension that is more closely related to our approach is cross-modal KD on unlabeled modality-paired data (e.g. RGB and Depth images), which was proposed to cope with modalities with limited data (Gupta, Hoffman, and Malik 2016). Inspired by this work, D&D performs 3D-to-2D cross-modal KD to allow downstream finetuning on 2D molecular graphs while utilizing the feature space refined by 3D conformer denoising. Further information on KD can be found in a recent survey by Gou et al. (2021).\n\nPretraining for Molecular Property Prediction. Inspired by previous work in the NLP domain, there exist many self-supervised pretraining approaches for learning representations of molecular graphs. Similar to masked token prediction in BERT (Devlin et al. 2018), Hu et al. (2019)\n\n![](images/928ec7dd73e96738cb0f14e7ad430b77221c005a469a9bf71b912da59d3fa7a3.jpg)  \nFigure 2: Illustration of our D&D framework. First we pretrain a 3D conformer encoding module by denoising perturbed conformers. Next we pretrain a 2D graph encoder by distilling representations from the 3D teacher. We propose two variants: D&D-GRAPH distills mean-pooled graph representations while D&D-NODE distills node representations in a more finegrained manner. During finetuning, we tune the 2D graph encoder only with the given downstream data.\n\nproposed node-attribute masking and context prediction to reconstruct topological structures or predict attributes of masked nodes. GROVER (Rong et al. 2020) proposed predicting what motifs exist in the molecular graph, under the insight that functional groups determine molecular properties. Contrastive approaches were also proposed, in which the task is to align representations from two augmentations of the same molecule and repel representations of different ones (Hassani and Khasahmadi 2020; You et al. 2020). Despite promising results, it has been shown that obtaining significant gains in performance with existing 2D pretraining methods is non-trivial, as empirical improvements rely heavily on the choice of hyperparameters and other experimental setups (Sun 2022).\n\nAs molecules lie in the 3D physical space, some work have deviated from the 2D graph setting and instead proposed 3D pretraining via denoising conformers perturbed with Gaussian noise, from which empirical results have shown significant knowledge transfer to diverse property prediction tasks (Zaidi et al. 2022; Liu, Guo, and Tang 2022). Despite great downstream performance, such 3D approaches necessitate access to accurate 3D conformers of molecules under concern, which are difficult to obtain as it requires expensive quantum mechanical computations such as density functional theory (DFT) (Parr and Weitao 1995).\n\nThere exist solutions to avoid these drawbacks. 3DInfomax (Sta¨rk et al. 2022) proposed a cross-modal contrastive pretraining framework that aligns 2D and 3D representations. In addition to contrastive learning, GraphMVP (Liu et al. 2021) also incorporates generative pretraining which trains the model to reconstruct the 2D graph representation from its 3D counterpart, and vice versa. Similar to GraphMVP, MoleculeSDE (Liu et al. 2023) pretrains representations using two group-symmetric stochastic differential equation-based generative processes, which led to state-ofthe-art downstream performance. While all aforementioned methods use 3D conformers during pretraining only, they either do not capture information from molecular force fields which we conjecture to be helpful for forward knowledge transfer, or involve extensive tuning on loss weighting and data augmentation due to simultaneously optimizing contrastive and generative objectives. In contrast, our D&D framework is remarkably simple due to the disentangled two-step pretraining procedure with each step optimizing a single objective, yet enjoys generalizable knowledge obtained through conformer denoising.\n\n# Preliminaries\n\nWe first introduce preliminary information on learning representations of 2D molecular graphs and 3D molecular conformers alongside notations that we use in later sections.\n\n2D Molecular Graphs. Let $\\mathcal { G } ~ = ~ ( \\nu , \\mathcal { E } )$ denote the 2D molecular graph with $N$ atoms represented as nodes in $\\nu$ , and $M$ bonds represented as edges in $\\mathcal { E }$ . In addition to the graph-connectivity, each node is assigned features based on chemical attributes such as atomic number and aromaticity, and similarly for each edge with features based on bond type and stereo configurations. Given the graph $\\mathcal { G }$ , a 2D graph encoder $f ^ { 2 \\mathrm { D } }$ typically first returns representations for each node:\n\n$$\nf ^ { \\mathrm { 2 D } } ( \\mathcal { G } ) = Z ^ { \\mathrm { 2 D } } \\mathrm { ~ w h e r e ~ } Z ^ { \\mathrm { 2 D } } \\in \\mathbb { R } ^ { N \\times d } .\n$$\n\nIn molecular property prediction settings we need a single representation for each molecular graph. Typical operators used to extract graph-level representations include meanpooling all node representations or adding a virtual node to the input graph and treating its representation as the graph representation (Hamilton 2020).\n\n3D Molecular Conformers. Each molecule can also be represented as a 3D conformer ${ \\mathcal C } = ( \\nu , R )$ with 3D spatial coordinates of each atom stored in $\\dot { \\pmb R } \\in \\dot { \\mathbb { R } } ^ { N \\times 3 }$ . Note that unlike 2D graphs, 3D conformers have no information of the graph connectivity via edges in $\\mathcal { E }$ , and are instead treated as point cloud data. As with 2D graphs, let $f ^ { 3 \\mathrm { D } }$ denote the 3D conformer encoder that takes the conformer $\\mathcal { C }$ and returns representations of each atom:\n\n$$\nf ^ { \\mathrm { 3 D } } ( \\mathcal { C } ) = Z ^ { \\mathrm { 3 D } } \\mathrm { ~ w h e r e ~ } Z ^ { \\mathrm { 3 D } } \\in \\mathbb { R } ^ { N \\times d } .\n$$\n\nNote that how the molecule is oriented on the 3D Euclidean space naturally does not affect its chemical property. Thus, the encoder $f _ { 3 D }$ must return representations that are invariant under rotations and translations on $\\scriptstyle { R }$ (i.e. $f ^ { 3 \\mathrm { D } } ( ( \\mathcal { V } , { \\pmb R } ) ) ~ = ~ f ^ { 3 \\mathrm { D } } ( ( \\mathcal { V } , g ( { \\pmb R } ) )$ for $g \\in \\mathsf { S E } ( 3 ) ,$ ) for efficient weight-tying across SE(3) roto-translations. Note that because molecular properties can change under chiral orientations, we only respect rotations and translations, but not reflections. There exist many architectures that respect SE(3) symmetry as an inductive bias (Fuchs et al. 2020; Bronstein et al. 2021; Gasteiger, Becker, and Gu¨nnemann 2021; Satorras, Hoogeboom, and Welling 2021; Tho¨lke and De Fabritiis 2022), and any such architecture can be used for $f ^ { 3 \\mathrm { D } }$ .\n\n# D&D: Denoise and Distill\n\nHere we describe D&D, a molecular pretraining framework that transfers generalizable knowledge from 3D conformer denoising to a 2D graph encoder via cross-modal distillation, thereby allowing painless downstream applications without computing accurate conformers of unseen graphs. The two major steps are as follows: 1) Denoising perturbed conformers with a 3D conformer encoder $f ^ { 3 D }$ , and 2) Distilling representations from the 3D teacher to the 2D graph encoder $f ^ { 2 D }$ . An illustration of the overall pipeline can be found in Figure 2. As our first step of D&D is based upon previous work on conformer denoising (Zaidi et al. 2022; Liu et al. 2022), we provide a brief outline of the task and refer readers to corresponding papers for further details and theoretical implications.\n\nStep 1: Pretraining via denoising. Given a stabilized ground-truth conformer $\\mathcal { C } = ( \\nu , R )$ , $\\mathrm { \\Delta } f ^ { 3 \\mathrm { D } }$ is given as input a perturbed version of the same conformer $\\tilde { C } = ( \\nu , \\tilde { R } )$ , produced by slightly perturbing the coordinates of each atom with Gaussian noise as\n\n$$\n\\tilde { R } _ { i } = R _ { i } + \\sigma \\epsilon _ { i } \\mathrm { ~ w h e r e ~ } \\epsilon _ { i } \\sim \\mathcal { N } ( 0 , I _ { 3 } )\n$$\n\nwith noise scale $\\sigma$ as hyperparameter. Then, we attach a prediction head $h ^ { 3 \\mathrm { D } } : \\mathbb { R } ^ { \\bar { N } \\times \\bar { d } } \\to \\mathbb { R } ^ { N \\times 3 }$ to the $f ^ { 3 \\mathrm { D } }$ such that the combined model outputs 3-dimensional vectors per atom.\n\n$$\nh ^ { \\mathrm { 3 D } } ( f ^ { \\mathrm { 3 D } } ( \\tilde { \\mathcal { C } } ) ) = ( \\hat { \\epsilon } _ { 1 } , \\tiny { \\cdot } \\cdot \\cdot , \\hat { \\epsilon } _ { N } )\n$$\n\nLastly, the model is trained to predict the noise that has been injected to create $\\tilde { \\mathcal { C } }$ from $\\mathcal { C }$ . The denoising loss minimized during training is as follows:\n\n$$\n\\mathcal { L } _ { \\mathrm { d e n o i s e } } = \\mathbb { E } _ { p ( \\tilde { \\mathcal { C } } , \\mathcal { C } ) } \\left[ \\left\\| h ^ { 3 \\mathrm { D } } ( f ^ { 3 \\mathrm { D } } ( \\tilde { \\mathcal { C } } ) ) - ( \\epsilon _ { 1 } , \\dots , \\epsilon _ { N } ) \\right\\| _ { 2 } ^ { 2 } \\right]\n$$\n\n![](images/0ddafd649aa7dad24e434a241470a0b082c229aaa38ebc8ddb18a30e42e599ed.jpg)  \nFigure 3: Training and validation loss curves (in log-scale) during distillation of D&D-GRAPH and D&D-NODE on PCQM4Mv2. The 2D student is able to closely distill features from the 3D teacher with small generalization gap.\n\nwhere $p ( \\tilde { \\mathcal { C } } , \\mathcal { C } )$ denotes the probability distribution induced by the data distribution and the noise sampling procedure to create $\\tilde { \\mathcal { C } }$ . Surprisingly, the denoising objective is equivalent to learning an approximation of the force field in the physical space derived by replacing the true distribution of conformers with a mixture of Gaussians (Zaidi et al. 2022). The Gaussian mixture potential corresponds to the classical harmonic oscillator potential in physics, a great approximation scheme for linearized equations such as denoising.\n\nFor our experiments, we use the TorchMD-NET (Tho¨lke and De Fabritiis 2022) architecture for $f ^ { 3 \\mathrm { D } }$ following (Zaidi et al. 2022) due to its equivariance to SE(3) roto-translations and high performance on quantum mechanical property prediction. Note that D&D is architecture-agnostic, and any other SE(3)-equivariant architecture can be used as well.\n\nStep 2: Cross-modal distillation. After pretraining via denoising is done, we distill representations from the pretrained $\\breve { f } ^ { \\mathrm { 3 D } }$ to a 2D graph encoder model $f ^ { 2 \\mathrm { D } }$ . We consider two different variants of cross-modal KD, leading to two respective variants of our approach. For the first variant D&DGRAPH, we minimize the difference between graph representations from 2D and 3D encoders:\n\n$$\n\\mathcal { L } _ { \\mathrm { d i s t i l l - g r a p h } } = \\left\\| \\mathrm { p o o l } ( f ^ { \\mathrm { 2 D } } ( \\mathcal { G } ) ) - \\mathrm { p o o l } ( f ^ { \\mathrm { 3 D } } ( \\mathcal { C } ) ) \\right\\| _ { 2 } ^ { 2 }\n$$\n\nDuring training, we freeze the teacher model $f ^ { 3 \\mathrm { D } }$ and flow gradients only through the student model $f ^ { 2 \\mathrm { D } }$ . This effectively trains the 2D encoder to leverage the bond features and graph topology to imitate representations from 3D conformers. To obtain graph representations, we average all node representations inferred by each encoder.\n\nInspired by structured KD (Liu et al. 2020), we propose another variant D&D-NODE that distills node-level representations without any pooling:\n\n$$\n\\mathcal { L } _ { \\mathrm { d i s t i l l - n o d e } } = \\left\\| f ^ { \\mathrm { 2 D } } ( \\mathcal { G } ) - f ^ { \\mathrm { 3 D } } ( \\mathcal { C } ) \\right\\| _ { 2 } ^ { 2 }\n$$\n\nUnlike D&D-GRAPH, D&D-NODE makes full use of the one-to-one correspondence between atoms in the molecular graph and atoms in the conformer. Hence $f ^ { 2 \\mathrm { D } }$ is trained to align towards representations from $f ^ { 3 \\mathrm { D } }$ in a more finegrained manner.\n\n![](images/c2cb1dbe11276e6b92341d695f676819406570347c978fd93856ca16e983441f.jpg)  \nFigure 4: Histograms of Pearson correlation values between pre-softmax attention scores vs. 3D pairwise distance during inference on the PCQM4Mv2 validation set for (Left) CONTRASTIVE, (Middle) D&D-GRAPH and D&D-NODE. (Right) Average attention score-weighted 3D distances according to network depth from D&D-NODE. Each colored dot represents an attention head in the corresponding layer.\n\nFor $f ^ { 2 \\mathrm { D } }$ , we mainly use the Tokenized Graph Transformer (TokenGT (Kim et al. 2022a)) architecture that theoretically enjoys maximal expressiveness across all possible permutation-equivariant operators on 2D graphs. Due to this flexibility, we expect $f ^ { 2 \\mathbf { \\bar { D } } }$ to be trained to align representations from $f ^ { \\mathrm { 3 D } }$ as closely as possible, by which we hope to see the effect of distillation to the fullest extent. Furthermore, using an attention-based architecture also allows analysis on the relationship between the attention scores of atom-pairs and their physical distance in the 3D space. Results from which are discussed later in the following section. Note that similarly with $f ^ { 3 \\mathrm { D } }$ , however, any other permutation-equivariant graph neural network architecture can be adopted seamlessly.\n\nDownstream finetuning. Assuming the downstream task does not provide accurate conformers as input, we discard $f ^ { 3 \\mathrm { D } }$ after the distillation step and finetune $f ^ { \\mathrm { 2 D } }$ with molecular graphs only. We use L1 loss and BCE loss for regression and binary-classification tasks, respectively, following previous work (Sta¨rk et al. 2022).\n\nNote that we finetune the entire $f ^ { 2 \\mathrm { D } }$ model instead of just the newly attached prediction head on the downstream data. Given that the force fields induced by electron clouds provide knowledge that is generalizable to various molecular properties, we conjecture that D&D provides a good initial point in the parameter space from which finetuning $f ^ { 2 \\mathrm { D } }$ entirely leads to a better local optima. This also aligns with previous observations in NLP that pretrained language models outperform models trained from scratch only when the entire model is finetuned (Rothermel et al. 2021).\n\n# Experiments\n\nFor empirical evaluation, we test our D&D pretraining pipeline on various molecular property prediction tasks using open-source benchmarks as well as four manually curated datasets. We also stress-test D&D under a downstream scenario where the number of labeled data points is extremely limited. All experiments are run in a remote GCP server equipped with 16 NVIDIA A100 Tensor Core GPUs.\n\n# Experimental Setup\n\nDatasets. For pretraining, we use PCQM4Mv2 (Nakata and Shimazaki 2017), a large molecule dataset consisted of 3.7M molecules. Each molecule is paired with a single 3D conformer at the lowest-energy state computed via DFT. In case of D&D, we use the same PCQM4Mv2 dataset for both denoising and disillation steps. Note that even though PCQM4Mv2 provides the HOMO-LUMO energy gap of each molecule as labels, we do not use any supervision from such labels during training, and instead treat the dataset as a collection of unlabeled molecular graph-conformer pairs.\n\nFor finetuning, we use 10 datasets from MoleculeNet (Wu et al. 2018), three of which are regression tasks and the rest are binary classification tasks. We use scaffold splitting to obtain the train-test splits for each task. As shown in the appendix, the MoleculeNet datasets exhibit different molecule distributions from PCQM4Mv2: some tasks involve atom types that the encoder has never observed during pretraining. As most datasets in MoleculeNet involve classification, we also curate four molecular property regression datasets from open-source databases: CCS measures the collisional cross sections of molecules (Kim et al. 2022b), CML and CMQ measures the emissive chromophore life times and fluorescence quantum yields, respectively (Joung et al. 2020). SLE measures the solid-liquid phase change entropy (I et al. 2011). The property values of these datasets are normalized by mean and standard deviation before training and evaluation. For curated datasets, we randomly split the dataset 8:1:1 for training, validation, and testing. Further details on the datasets can be found in the appendix.\n\nBaselines. For MoleculeNet experiments, we compare our method against baselines that pretrain representations using only 2D graph topology: ATTRMASK and CONTEXTPRED (Hu et al. 2019) masks out and predicts key node and edge attributes or surrounding graph substructures, GRAPHCL (You et al. 2020) performs contrastive learning with graph augmentations, JOAO (You et al. 2021) and JOAOV2 (You et al. 2021) also perform contrastive pretraining, but adaptively chooses which graph augmentation to use during training. We also compare against baselines that use both 2D and 3D modalities, GRAPHMVP and MOLECULESDE, discussed above as related work. As most previous work have used the graph isomorphism network (GIN (Xu et al. 2018)) as the 2D encoder, we also test D&D using GIN in addition to TOKENGT. For the remaining tasks, we mainly compare D&D against MoleculeSDE using the TokenGT backbone, as other methods have not shown significant performance gains compared to randomly initialized models without pretraining.\n\nTable 1: MoleculeNet Results averaged across 5 random seeds with one standard deviation. The modality of each method indicates whether the method uses 2D graphs only (2D only) or 3D conformers as well (2D&3D) during pretraining. The first 7 tasks show ROC-AUC (higher is better) while the remaining three show results in MAE (lower is better). We also report average rank across all targets for each GNN architecture. Best results for each task within each architecture are marked bold.   \n\n<html><body><table><tr><td>Pretraining</td><td>Bace(↑)</td><td>BBBP(↑)</td><td>ClinTox(↑)</td><td>HIV(↑)</td><td>Sider(↑)</td><td>Tox21(↑)</td><td>ToxCast(↑)</td><td>Esol(↓)</td><td>Freesolv(↓)</td><td>Lipo(↓)</td></tr><tr><td colspan=\"9\">GRAPH ISOMORPHISM NETWORK(GIN)</td></tr><tr><td>NO PRETRAIN</td><td>78.88±1.79</td><td>68.68±1.07</td><td>71.97±8.73</td><td>77.01±0.55</td><td>55.95±0.64</td><td>75.06±0.40</td><td>61.46±0.21</td><td>3.64±0.05</td><td>3.76±0.18</td><td>0.66±0.02</td></tr><tr><td>ATTRMASK</td><td>79.10±1.39</td><td>68.21±0.55</td><td>74.06±3.43</td><td>74.03±1.29</td><td>55.86±0.77</td><td>73.48±0.64</td><td>61.02±0.23</td><td>2.83±0.30</td><td>4.46±0.37</td><td>0.68±0.01</td></tr><tr><td>CONTEXTPRED</td><td>72.91±4.59</td><td>65.21±1.14</td><td>54.72±2.18</td><td>71.89±1.28</td><td>55.90±0.84</td><td>70.84±0.53</td><td>59.39±0.47</td><td>3.72±0.08</td><td>5.04±0.08</td><td>0.81±0.07</td></tr><tr><td>GRAPHCL</td><td>74.92±3.18</td><td>64.00±1.18</td><td>81.66±2.27</td><td>76.92±0.76</td><td>56.64±0.90</td><td>75.05±0.39</td><td>62.44±0.50</td><td>3.18±0.03</td><td>4.86±0.33</td><td>0.66±0.01</td></tr><tr><td>JOAO</td><td>74.24±2.50</td><td>65.76±1.19</td><td>84.35±2.33</td><td>77.03±1.05</td><td>58.19±0.67</td><td>73.71±0.37</td><td>62.50±0.33</td><td>2.97±0.06</td><td>4.74±0.15</td><td>0.66±0.01</td></tr><tr><td>JOAOv2</td><td>76.72±3.64</td><td>65.81±1.01</td><td>84.95±3.06</td><td>77.44±0.54</td><td>57.23±0.24</td><td>73.33±0.26</td><td>62.60±0.33</td><td>3.34±0.09</td><td>4.59±0.41</td><td>0.65±0.01</td></tr><tr><td>GRAPHMVP</td><td>81.30±1.46</td><td>67.92±0.85</td><td>66.30±1.32</td><td>75.69±0.77</td><td>59.46±0.50</td><td>73.24±0.16</td><td>62.80±0.35</td><td>3.27±0.07</td><td>4.33±0.16</td><td>0.627±0.01</td></tr><tr><td>MOLECULESDE</td><td>78.15±4.10</td><td>67.37±1.23</td><td>77.16±1.58</td><td>76.47±0.79</td><td>60.22±0.55</td><td>75.38±0.59</td><td>63.22±0.35</td><td>2.08±0.19</td><td>3.64±0.17</td><td>0.610±0.00</td></tr><tr><td>D&D (OURS)</td><td>79.44±0.22</td><td>69.92±1.18</td><td>84.81±5.40</td><td>78.02±0.58</td><td>59.28±0.14</td><td>75.81±0.31</td><td>63.57±0.25</td><td>1.24±0.16</td><td>4.18±0.23</td><td>0.66±0.01</td></tr><tr><td colspan=\"9\">TOKENIZED GRAPH TRANSFORMER (TOKENGT)</td><td></td><td></td></tr><tr><td>NO PRETRAIN</td><td>77.53±1.97</td><td>65.94±0.77</td><td></td><td>70.44±1.86</td><td></td><td></td><td>60.63±0.62</td><td>0.811±0.05</td><td></td><td></td></tr><tr><td></td><td>69.12±0.84</td><td></td><td>85.44±2.17</td><td></td><td>57.65±2.13</td><td>72.34±0.48</td><td></td><td></td><td>1.624±0.14</td><td>0.71±0.01</td></tr><tr><td>GRAPHMVP MOLECULESDE</td><td>74.33±3.14</td><td>66.17±1.47 68.17±0.99</td><td>81.45±2.50 87.65±1.50</td><td>68.17±0.86 72.21±0.42</td><td>58.13±0.56 63.50±1.37</td><td>74.23±0.42 74.55±0.48</td><td>62.79±0.54 63.79±0.42</td><td>0.84±0.02</td><td>1.62±0.08 1.53±0.08</td><td>0.66±0.02</td></tr><tr><td>D&D (OURS)</td><td>81.11±2.65</td><td>66.40±1.71</td><td>81.51±2.63</td><td>77.31±1.03</td><td>63.00±1.11</td><td>76.61±1.04</td><td>65.21±0.71</td><td>0.67±0.03 0.70±0.02</td><td>1.38±0.12</td><td>0.61±0.02 0.52±0.01</td></tr></table></body></html>\n\nDuring finetuning, we compute graph representations via mean-pooling of all node representations within each molecule, and feed it to a linear adapter. We test both nodewise and graph-wise distillation, and report the better of the two results. For consistency, we follow the same featurization step provided by the OGB library (Hu et al. 2020) across all tasks, which produces a 9 and 3-dimensional feature vector for each atom and bond, respectively. Further details such as hyperparameterization can be found in the appendix.\n\n# Pretraining Results\n\nPrior to downstream evaluation, we discuss interesting findings from pretraining with D&D.\n\nThe 2D graph encoder can closely mimic representations from 3D conformers using only the molecular graph. Figure 3 shows the training and validation loss curves of the distillation step of D&D. When pretraining with D&DNODE, the distillation loss converges to slightly over $1 0 ^ { - 2 }$ with a very small generalization gap between validation and training. This shows that the 2D molecular graph contains enough information to closely imitate representations from the 3D teacher $f ^ { 3 \\mathrm { D } }$ . The small gap between training and validation also reflects that the guidance provided via D&DNODE can well-generalize towards unseen molecules. When pretraining with D&D-GRAPH, we find that the training loss converges to a much lower optima of $1 0 ^ { - 3 }$ , but with a much larger generalization gap of approximately $1 0 ^ { - 3 }$ . This implies that while the task of distilling mean-pooled representations is easier than distilling node-wise representations, it leads to less generalizable knowledge due to not considering the graph topological structure.\n\nThe intermediate encoding procedure of the 2D encoder trained via D&D aligns with 3D geometry. As we use an attention-based architecture for $\\breve { f } ^ { \\mathrm { 2 D } }$ encoding, we qualitatively assess how well 2D atom-wise interactions modeled via attention resembles its interactions in 3D geometry without actual conformers (e.g. do atoms nearby in the 3D space tend to attend to each other?). Specifically, we compute the absolute Pearson correlation between the 3D pairwise distances of atoms and the inner product of their features prior to the softmax layer in each attention head, averaged across all molecules in the PCQM4Mv2 validation set. Note that a larger inner product implies a relatively larger exchange of information between the two atoms. The first two figures in Figure 4 show histograms depicting distributions of averaged absolute Pearson correlation values from all attention heads for each layer in the 2D encoder after pretraining by each method. We find that using a contrastive objective only leads to a slight increase in correlation compared to random initialization: most correlation values are distributed under 0.3. When pretrained with our D&D-variants, however, many attention heads show correlations that exceed 0.3, a value that is never reached with randomly initialized weights. This implies that our approach provides guidance to the 2D graph encoder towards processing molecular graphs while respecting its 3D geometry. For further investigation, we also measure the average pairwise distances weighted by the attention scores from D&D-NODE with results shown in the rightmost plot in Figure 4. A higher value indicates that the attention head tends to exchange information across atoms that are far apart. Interestingly, the first layer exhibits a diverse range of distances, but the layer that immediately follows uses attention mostly to exchange information across atoms that are geometrically nearby each other, similar to a SE(3)-convolutional layer. Considering that a carbon-carbon single bond has an average length of 1.5 angstroms, this result indicates that $f ^ { 2 \\mathrm { D } }$ pretrained with D&D-NODE can reason about 3D geometry to exchange information across atoms that are nearby in the 3D conformer, even though they may be far apart in the 2D graph.\n\n# Finetuning Results\n\nHere we provide empirical observations from various downstream molecular property prediction datasets.\n\nD&D transfers knowledge that is generalizable across diverse tasks. Table 1 shows finetuning results on MoleculeNet, in which each experiment is averaged across 5 randomly seeded runs. Comparing results from D&D and NO PRETRAIN, D&D shows superior performance in 9 out of 10 tasks, with an average performance improvement of $4 . 6 \\%$ and $1 8 . 6 \\%$ across classification and regression tasks, respectively. In particular, we find that the tasks on which D&D shows the largest performance gain against NO PRETRAIN coincide with properties known to align well with 3D geometric properties $6 5 . 9 \\%$ for ESOL with GIN, $2 7 . 0 \\%$ for LIPO with TokenGT). Both ESOL and LIPO tasks are tightly associated with the overall polarity of electron clouds in the molecule, which is highly associated with spatial atom-wise positions. This implies that denoising followed by distillation effectively transfers spatial 3D knowledge. With respect to 2D graph-based self-supervised pretraining baselines, D&D outperforms all methods on 8 out of 10 task using GIN as the backbone, which again demonstrates the effectiveness of using 3D ground-state conformers in addition to 2D graph topologies for pretraining. We also find that 2D-only baselines generally suffer from negative knowledge transfer, which aligns with previous findings (Sun 2022).\n\nWhen comparing D&D against other methods that use cross-modal objectives, D&D shows $9 . 7 \\%$ and $3 . 2 \\%$ better performances overall than GRAPHMVP and MOLECULESDE, respectively. This suggests that focusing on transferring molecular-specific knowledge of force fields without any additional tasks is more effective downstream than having a contrastive objective as a proxy task; attracting and repelling molecular representations across the two modalities fail to fully capture generalizable similarities and discrepancies in the chemical space. Another limitation of the contrastive approach is that the gain in performance becomes limited when only a single conformer is provided per molecule (Liu et al. 2021; Sta¨rk et al. 2022). This aligns well with our intuition that each conformer can be seen as a distribution of 3D configurations, and that learning a single local optima within the distribution does not provide much information. Meanwhile, D&D can learn and transfer knowledge of the overall distribution with denoising and distilling, relieving practitioners from the need to obtain multiple lowenergy conformers per molecule for pretraining.\n\nD&D enables label-efficient finetuning. To evaluate D&D under downstream tasks with limited labels, we perform finetuning on four curated datasets using the full downstream data set as well as a smaller randomly sampled subset of the original training data for each task. Note that labelefficiency is crucial for molecular property prediction especially since gathering refined labels often require extensive validation through costly wet-lab experiments. For this experiment, we mainly use TOKENGT as our backbone architecture and compare against MOLECULESDE to focus on the downstream effect of knowledge transfer solely from denoising to the fullest extent.\n\n![](images/3d0b6d7323dd3b4ca1b400a0010735fa8b8baff72f9db8e9f177793d5b0a751c.jpg)  \nFigure 5: Results on the four curated datasets, averaged across 3 seeds with one standard deviation shown in the shaded area. The X-axis indicates the percentage of finetuning data used, and the Y-axis shows test MAE performances.\n\nFigure 5 shows finetuning results on four curated regression datasets. In all four tasks, D&D trained using only $2 5 \\%$ of training data outperforms the model without pretraining, which shows that knowledge of conformer denoising serves as a great initialization for property prediction that prevents overfitting to small training data and induces high generalizability to unseen molecules. D&D also consistently outperforms MOLECULESDE across all targets and train set sizes, lowering the MAE by at most $2 6 . 6 \\%$ on predicting collisional cross sections (CCS). This is particularly interesting as CCS measures the probability of two or more particles colliding to each other, a quantity that requires accurate 3D structural information of molecules to predict accurately. Based on this insight, we hope to explore other molecular properties that are highly related to the 3D structure of ground-state conformers and apply D&D as future work.\n\n# Concluding Remarks\n\nWe propose D&D, a self-supervised molecular representation learning method that distills features from a 3D conformer denoiser to a 2D graph encoder. Learning knowledge of force fields that provide chemically generalizable information, D&D does not require 3D conformers downstream and is effective on diverse molecular property prediction tasks. D&D also enjoys high label-efficiency, achieving high performance with limited downstream labeled data. As future work, we hope to extend D&D to multitask setups (Liu et al. 2020) and molecular diffusion models (Xu et al. 2022; Jing et al. 2022) for property prediction.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决分子属性预测中获取准确的真实属性标签成本高昂的问题。高精度的分子属性预测需要准确的真实属性标签，但获取这些标签成本极高，限制了分子图神经网络的性能。该问题的重要性在于，分子属性预测在药物发现和材料设计等应用中意义重大，而高成本问题阻碍了该领域的进一步发展。此外，现有的基于2D图的分子预训练方法难以在预测性能上取得显著提升，基于3D构象的预训练方法虽有效果，但下游任务中获取准确的3D构象计算成本高。\\n\\n**方法概述**\\n论文提出了D&D（Denoiseand-Distill）框架，这是一种自监督的分子表示预训练框架，通过3D到2D的跨模态蒸馏将3D去噪知识转移到2D图编码器。\\n\\n**主要贡献与效果**\\n- 提出了D&D，一种执行3D到2D跨模态蒸馏的两步自监督分子表示预训练框架。在预训练中，2D学生模型能使用图特征和拓扑结构紧密模仿3D教师模型的表示，中间表示也与3D几何结构良好对齐。\\n- 实验表明D&D能实现显著的知识转移，在下游分子属性预测任务中表现良好，与无预训练相比，在分类和回归任务上平均性能分别提升了4.6%和18.6%；与使用跨模态目标的方法相比，比GRAPHMVP和MOLECULESDE的整体性能分别高出9.7%和3.2%。\\n- D&D还能在有限的标记训练数据点的下游场景中表现出色，使用仅25%的训练数据训练的D&D模型在所有四个任务中都优于未预训练的模型，在预测碰撞截面（CCS）时，最大可将平均绝对误差（MAE）降低26.6%。此外，在MoleculeNet基准测试中，D&D在9个任务中优于无预训练模型。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nD&D的核心思想是利用3D去噪预训练获得可推广的知识，并通过跨模态蒸馏将这些知识转移到2D图编码器。3D去噪可以学习物理空间中的力场近似信息，而跨模态蒸馏可以让2D图编码器在处理分子图时考虑其3D几何结构，从而使2D图编码器能在不使用准确的3D构象的情况下也能推理3D几何信息。\\n\\n**创新点**\\n先前基于2D图的预训练框架在数据增强时会破坏图的拓扑结构，导致模型无法从增强样本中完全恢复标签，在下游任务中性能提升不显著；现有结合3D信息的预训练方法虽有效，但下游任务需要准确的3D构象，获取成本高，且有的方法没有捕捉到对正向知识转移有帮助的分子力场信息，或由于同时优化对比和生成目标而需要对损失权重和数据增强进行大量调整。相比之下，D&D的创新点在于采用了两步分离的预训练过程，每个步骤优化一个单一目标，获得了通过构象去噪得到的可推广知识，结合了两者的优点，既利用了3D去噪的知识，又能在仅使用2D图的下游场景中应用，避免了昂贵的3D构象计算。\\n\\n**具体实现步骤**\\n1. **3D去噪预训练**：给定一个稳定的真实构象 $\\mathcal { C } = ( \\nu , R )$，通过高斯噪声轻微扰动其坐标生成扰动构象 $\\tilde { \\mathcal { C } } = ( \\nu , \\tilde { R } )$，其中 $\\tilde{R}_i = R_i + \\sigma \\epsilon_i$，$\\epsilon_i \\sim \\mathcal{N}(0, I_3)$，$\\sigma$ 为噪声尺度超参数。将其输入3D构象编码器 $f ^ { 3 \\mathrm { D } }$，并连接预测头 $h ^ { 3 \\mathrm { D } }$ ，模型输出每个原子的3维向量，训练目标是预测注入的噪声，去噪损失为 $\\mathcal { L } _ { \\mathrm { d e n o i s e } } = \\mathbb { E } _ { p ( \\tilde { \\mathcal { C } } , \\mathcal { C } ) } \\left[ \\left\\| h ^ { 3 \\mathrm { D } } ( f ^ { 3 \\mathrm { D } } ( \\tilde { \\mathcal { C } } ) ) - ( \\epsilon _ { 1 } , \\cdots , \\epsilon _ { N } ) \\right\\| _ { 2 } ^ { 2 } \\right]$。对于 $f ^ { 3 \\mathrm { D } }$，文中使用TorchMD - NET架构，因其具有SE(3)旋转平移等变性和在量子力学属性预测上的高性能。\\n2. **跨模态蒸馏**：提出了两个变体，D&D - GRAPH通过最小化蒸馏损失 $\\mathcal { L } _ { \\mathrm { d i s t i l l - g r a p h } } = \\left\\| \\mathrm { p o o l } ( f ^ { \\mathrm { 2 D } } ( \\mathcal { G } ) ) - \\mathrm { p o o l } ( f ^ { \\mathrm { 3 D } } ( \\mathcal { C } ) ) \\right\\| _ { 2 } ^ { 2 }$ 来蒸馏图级表示；D&D - NODE通过最小化蒸馏损失 $\\mathcal { L } _ { \\mathrm { d i s t i l l - n o d e } } = \\left\\| f ^ { \\mathrm { 2 D } } ( \\mathcal { G } ) - f ^ { \\mathrm { 3 D } } ( \\mathcal { C } ) \\right\\| _ { 2 } ^ { 2 }$ 来蒸馏节点级表示。在训练中，冻结教师模型 $f ^ { 3 \\mathrm { D } }$ ，仅通过学生模型 $f ^ { 2 \\mathrm { D } }$ 流动梯度。对于 $f ^ { 2 \\mathrm { D } }$，主要使用Tokenized Graph Transformer（TokenGT）架构，因其理论上在2D图的所有可能排列等变算子上具有最大表达能力。\\n3. **下游微调**：假设下游任务不提供准确的构象作为输入，在蒸馏步骤后丢弃 $f ^ { 3 \\mathrm { D } }$ ，仅使用分子图对 $f ^ { 2 \\mathrm { D } }$ 进行微调，回归和二分类任务分别使用L1损失和BCE损失。并且微调整个 $f ^ { 2 \\mathrm { D } }$ 模型，而非仅新连接的预测头，因为电子云诱导的力场知识具有通用性，D&D能为参数空间提供良好的初始点，使微调整个模型能达到更好的局部最优。\\n\\n**案例解析**\\n论文未明确提供此部分信息。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括仅使用2D图拓扑结构进行预训练的方法（ATTRMASK、CONTEXTPRED、GRAPHCL、JOAO、JOAOv2）、使用2D和3D模态的方法（GRAPHMVP、MOLECULESDE）以及无预训练（NO PRETRAIN）。\\n\\n**性能对比**\\n*   **在[ROC - AUC（↑）]指标上**：在MoleculeNet数据集上，对于GRAPH ISOMORPHISM NETWORK(GIN)架构，D&D在Bace、BBBP、ClinTox、HIV、Sider、Tox21、ToxCast等任务上的ROC - AUC值分别为79.44±0.22、69.92±1.18、84.81±5.40、78.02±0.58、59.28±0.14、75.81±0.31、63.57±0.25，优于ATTRMASK、CONTEXTPRED、GRAPHCL等基线模型以及无预训练；对于TOKENIZED GRAPH TRANSFORMER (TOKENGT)架构，D&D在Bace、BBBP、ClinTox等任务上的ROC - AUC值分别为81.11±2.65、66.40±1.71、81.51±2.63等，也优于GRAPHMVP、MOLECULESDE等基线模型以及无预训练。\\n*   **在[MAE（↓）]指标上**：在MoleculeNet数据集上，对于GRAPH ISOMORPHISM NETWORK(GIN)架构，D&D在Esol、Freesolv、Lipo等任务上的MAE值分别为1.24±0.16、4.18±0.23、0.66±0.01，优于ATTRMASK、CONTEXTPRED等基线模型以及无预训练；对于TOKENIZED GRAPH TRANSFORMER (TOKENGT)架构，D&D在Esol、Freesolv等任务上的MAE值分别为0.70±0.02、1.38±0.12等，优于GRAPHMVP、MOLECULESDE等基线模型以及无预训练。此外，在自定义的四个分子属性回归数据集上，D&D在仅使用25%训练数据的情况下，训练的模型表现优于无预训练的模型，在预测碰撞截面（CCS）时，最大可将MAE降低26.6%。与其他使用跨模态目标的方法相比，D&D在回归任务上也表现更好，比GRAPHMVP和MOLECULESDE的整体性能分别高出9.7%和3.2%。\",\n    \"keywords\": \"### 关键词\\n\\n- 分子表示学习 (Molecular Representation Learning, N/A)\\n- 跨模态蒸馏 (Cross - Modal Distillation, N/A)\\n- 3D去噪预训练 (3D Denoising Pretraining, N/A)\\n- 分子属性预测 (Molecular Property Prediction, N/A)\\n- 自监督预训练 (Self - Supervised Pretraining, N/A)\\n- 3D去噪 (3D Denoising, N/A)\\n- D&D (Denoiseand - Distill, N/A)\\n- 分子图神经网络 (Molecular Graph Neural Network, N/A)\\n- 3D构象 (3D Conformer, N/A)\"\n}"
}