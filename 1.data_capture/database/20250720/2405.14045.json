{
    "source": "Semantic Scholar",
    "arxiv_id": "2405.14045",
    "link": "https://arxiv.org/abs/2405.14045",
    "pdf_link": "https://arxiv.org/pdf/2405.14045.pdf",
    "title": "Learning rigid-body simulators over implicit shapes for large-scale scenes and vision",
    "authors": [
        "Yulia Rubanova",
        "Tatiana Lopez-Guevara",
        "Kelsey R. Allen",
        "William F. Whitney",
        "Kimberly L. Stachenfeld",
        "Tobias Pfaff"
    ],
    "categories": [
        "cs.LG",
        "cs.CV"
    ],
    "publication_date": "2024-05-22",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 4,
    "influential_citation_count": 0,
    "institutions": [
        "Google Deepmind"
    ],
    "paper_content": "# Learning rigid-body simulators over implicit shapes for large-scale scenes and vision\n\nYulia Rubanovaâˆ— Tatiana Lopez-Guevara Kelsey R. Allen William F. Whitney Kimberly Stachenfeldâ€  Tobias Pfaffâ€ \n\nGoogle Deepmind\n\n# Abstract\n\nSimulating large scenes with many rigid objects is crucial for a variety of applications, such as robotics, engineering, film and video games. Rigid interactions are notoriously hard to model: small changes to the initial state or the simulation parameters can lead to large changes in the final state. Recently, learned simulators based on graph networks (GNNs) were developed as an alternative to hand-designed simulators like MuJoCo [36] and PyBullet [13]. They are able to accurately capture dynamics of real objects directly from real-world observations. However, current state-of-the-art learned simulators operate on meshes and scale poorly to scenes with many objects or detailed shapes. Here we present SDF-Sim, the first learned rigid-body simulator designed for scale. We use learned signeddistance functions (SDFs) to represent the object shapes and to speed up distance computation. We design the simulator to leverage SDFs and avoid the fundamental bottleneck of the previous simulators associated with collision detection. For the first time in literature, we demonstrate that we can scale the GNN-based simulators to scenes with hundreds of objects and up to 1.1 million nodes, where mesh-based approaches run out of memory. Finally, we show that SDF-Sim can be applied to real world scenes by extracting SDFs from multi-view images.\n\n# 1 Introduction\n\nSimulating real-world environments, such as a bookshelf with books and decorations or a dinner table with plates and glasses, presents a significant challenge for traditional physics simulators. These simulators require precise 3D shape, location, and corresponding physical parameters of each object, making the simulation of arbitrary scenes a difficult task.\n\nRecently, learned simulators based on graph networks (GNNs) [4, 19, 29] have been introduced as a powerful alternative to traditional hand-designed simulators like MuJoCo [36] or PyBullet [13]. Graph networks can capture a range of complex physical dynamics, learn physical properties directly from real data and generalize to new scenes. However, current GNN-based methods do not scale well to large scenes. Similarly to the traditional simulators, they rely on 3D meshes to describe object shapes. In scenes with a large number of objects or objects with detailed meshes, these scenes might contain thousands of nodes and mesh triangles, making collision detection between objects extremely costly. In the context of graph networks, large meshes also lead to input graphs that might contain hundreds of thousands of nodes and edges, crippling runtime and memory performance.\n\nTo remedy the issue with expensive collision detection, a common approach in the classic simulation literature is to use signed-distance functions or fields (SDFs). SDFs implicitly represent the object shapes and allow to find the distance and the closest point on the object surface from an arbitrary location in 3D space in constant-time. Since distance queries are a main driver of the compute cost in traditional rigid body simulation, they can be significantly sped up using SDFs. However, in practice, SDFs are frequently pre-computed from a mesh and stored as a 3D grid, which trades off the runtime efficiency for increased memory cost and limits their usefulness for large real scenes.\n\n![](images/7e4ba9ac2db4ca365602b54e4812848296c9bf883c3e44d09c8d6d9be0e151d0.jpg)  \nFigure 1: Overview of SDF-Sim pipeline. SDFs parameterized by MLPs are learned for each object to implicitly represent the object shape and the distance field. A GNN-based simulator uses learned SDFs to predict object dynamics for the next simulation step.\n\nAn orthogonal line of work started to explore SDFs learned from a set of images to reconstruct the 3D shape from vision. Those SDFs store the shape implicitly in the MLP weights and are fast to train and query. They are less memory-hungry compared to 3D grids, making them a perfect candidate for simulation. However, despite these advantages, learned SDFs were applied for dynamics scenes only in a limited context [14, 23].\n\nWe present SDF-Sim, a learned graph-network-based simulator for rigid interactions that uses learned SDFs to represent object shapes, Figure 1. Using a special design of the input graph, SDF-Sim allows us to substantially reduce the runtime and memory requirements compared to mesh-based learned simulators. This is the first demonstration of a learned simulator generalizable to extremely large scenes up to 1.1 million nodes in Figure 2, orders of magnitude larger than what have been shown in any previous work on learned rigid simulation. These simulations include concave shapes (shoes, a hanger) and thin structures (screwdriver, baking form). Finally, we show that SDF-Sim can directly work with SDFs obtained from multi-view RGB images, Figure 3, supporting rich 3D simulation of objects extracted from real scenes.\n\n# 2 Background\n\nMesh-based simulation A simulation can be represented as a time series of system states $\\mathcal { S } ^ { 1 } , \\ldots , \\mathcal { S } ^ { T }$ . The goal is to learn a neural simulator that predicts the next state $S ^ { t + 1 }$ given a history of previous states {Stâˆ’h+1, . . . Simulators based on graph networks (GNNs) [8, 29] encode the system state into a graph $\\mathcal { G } = ( \\nu , \\mathcal { E } )$ with nodes $\\nu$ and edges $\\mathcal { E }$ . For rigid body simulation, this graph can be constructed from the triangle mesh of the individual objects: mesh vertices become the graph nodes, and mesh edges act as graph edges. The object motion is computed by message passing across the nodes and edges in the graph. Within individual objects, the position, rotation and velocity of the object can be computed by message passing through the nodes and edges of that object.\n\nWhy learned simulators? Analytical simulators like MuJoCo, PyBullet or Drake, are most commonly used for modelling rigid bodies. However, the traditional simulators rely on hard-coded approximations of physical interactions that might not match the properties of real objects, even with careful hyperparameter tuning[1]. The predictions from these simulators inevitably diverge from observations of real objects â€“ a so-called sim-to-real gap [17].\n\nLearned simulators have unique advantages that analytical simulators donâ€™t provide. First, learned simulators can be trained directly on real-world observations. They can track the real object trajectory better than analytical simulators, solving a well-known sim-to-real gap[17]. Another common issue is precisely estimating the initial states, which analytical simulators rely on â€“ learned simulators can compensate for these inaccuracies [3]. Finally, learned simulators are differentiable and can be used for optimization and design[2]. At the same time, learned simulators are not optimized for runtime and memory, are slower and more memory-constrained than analytical simulators. In this work, we address that limitation of memory constraint and unlock the ability to run learned simulators on larger scenes in comparison to previous learned simulators.\n\n![](images/cec4bcdc90c69a2358a7018384c34ca04b818f51124c3d585989fac64823349f.jpg)  \nFigure 2: Example of rollouts from SDF-Sim scaled to large simulations, all simulated for 200 steps. (Top) 300 shoes (object from Movi-C), with 851k nodes, falling onto the floor (Middle) 270 knots from Movi-B, 384k nodes (Bottom) 380 objects from Movi-C, 1.1M nodes. See simulation videos on https://sites.google.com/view/sdf-sim.\n\n![](images/a2890c95cc2501e25daa86c4fbf4c818653e675c3b2c0b17ae15c4713d2c46f4.jpg)  \nFigure 3: Simulating assets extracted from vision. (a) We extract the SDF from the images of a real-world scene with a garden table [7]. (b) We simulate a virtual shoe object falling onto a vase and a table using SDF-Sim. SDF-Sim is able to predict realistic dynamics, even for the collision of the shoe with the intricate shape of the vase (frames 50-70). See section 4.4 for details and the video on the website.\n\nModelling collisions in GNN simulators The most challenging component of the simulation is computation of collision impulses between objects. To do so, GNN simulators introduce collision edges $\\mathcal { E } ^ { c o l l }$ between nodes [31] or triangles [4] on the mesh surface that belong to different objects and are within a predefined distance threshold. However, the amount of these edges is the main bottleneck of GNN-based simulators. Asymptotically, the number of potential collision edges grows quadratically with the number of simulated nodes, leading to prohibitive compute and memory costs. Another challenge is identifying which pairs of triangles/nodes to connect with collision edges in the first place, by computing the distance to the closest point on a mesh. Typically, this procedure is implemented with tree search methods over all mesh triangles in the scene, such as BVH [11], that are difficult to integrate into deep learning pipelines. In this work, we address both of these challenges by using SDFs.\n\nSigned distance functions Signed-distance functions (SDFs) are widely used in computer graphics, game engines, and robotics for fast collision detection and computation of distances to an object [33, 45]. SDF defines a field $f ( \\mathbf { y } ) : \\mathbb { R } ^ { 3 } \\to \\mathbb { R }$ that represents the signed distance from an arbitrary point y to the closest point on the surface of the object. The sign of the SDF determines whether a point is outside (positive) or inside (negative) of the object. The zero level of the SDF $\\{ \\mathbf { y } \\in \\mathbb { R } ^ { 3 } | f ( \\mathbf { y } ) \\overset { \\cdot } { = } 0 \\}$ implicitly represents the object surface. SDFs permit constant-time queries of the distance to an object surface, irrespective of the number of nodes/faces in the object mesh, which is an essential component of collision detection.\n\nIntra-object Collision edges between objects   \nedges Edge connectivity Edge features Â· $\\mathbf { n } _ { i k }$ Surface nodes of the object $O _ { i }$ nik nik Â· $\\mathbf { o } _ { j }$ Center node for object $O _ { j }$ Edge in the input graph Object $\\mathbf { c } _ { i k } ^ { j }$ The closest point on object $O _ { j }$ to $\\mathbf { n } _ { i k }$ computed via SDF Object Object = [ ,||   ||, -nik] â†’[ck-oj]\n\nConstructing an SDF In the computer graphics and simulation literature, SDF is often precomputed as a high-resolution 3D grid containing signed distances from the points on the grid to the object surface [26, 37]. The grid SDF allows to speed up distance queries by trading off memory: for example, $5 1 2 \\mathrm { x } 5 1 2 \\mathrm { x } 5 1 2$ grid would take ${ \\approx } 1 3 4 \\mathbf { M }$ voxels (0.5Gb of memory) for a single object.\n\nLearned SDFs started to gain popularity for reconstruction of water-tight 3D shapes from images. They approximate the continuous distance field $f ( \\mathbf { y } ; \\boldsymbol { \\theta } )$ with an MLP parameterized by $\\theta$ [23, 28, 43]. Unlike 3D-grid-SDFs, learned SDFs are not tied to a fixed grid resolution and can represent detailed shapes using a small set of parameters $\\theta$ . Despite these advantages, learned SDFs have not been sufficiently explored to speed up physical simulations. Limited available works combine learned SDFs with classic physics solvers [5, 33]. They demonstrate that learned SDFs can massively reduce the distance query time thanks to parallelization on a GPU compared to mesh-based computation, while taking ${ \\sim } 3 2 \\mathrm { x }$ less memory than traditional 3D-voxel-grid SDFs.\n\nComputing closest points For any point in 3D space $\\mathbf { y }$ an SDF $f _ { \\theta }$ allows us to easily compute the closest point $\\mathbf { y } ^ { * }$ on the object surface that it represents, as:\n\n$$\n\\mathbf { y } ^ { * } = \\mathbf { y } - f _ { \\theta } ( \\mathbf { y } ) \\nabla f _ { \\theta } ( \\mathbf { y } ) ,\n$$\n\nwhere, by definition, $f _ { \\boldsymbol { \\theta } } ( \\mathbf { y } )$ is the distance between y and $\\mathbf { y } ^ { * }$ , and the gradient $\\nabla f _ { \\boldsymbol { \\theta } } ( \\mathbf { y } )$ points in the opposite direction of the shortest path from $\\mathbf { y }$ to the surface of the object and is unit-norm. If $f _ { \\boldsymbol { \\theta } } ( \\mathbf { y } )$ is parameterized as an MLP, this calculation requires only one forward and one backward pass of the network. This provides an efficient way to calculate the closest points for collision resolution in the simulation. In this work, for the first time in the literature, we use learned SDFs to accelerate distance computation in SOTA graph-network simulators.\n\n# 3 SDF-Sim\n\nWe introduce SDF-Sim, a graph-network-based simulator for rigid objects that uses learned SDFs to represent object shapes and to perform fast and memory-efficient distance computation between the objects. By leveraging SDF properties, we propose a new way to construct the input graph for the graph network, allowing us to use a smaller graph size and to get an order-of-magnitude reduction in runtime and memory on large simulations.\n\n# 3.1 Training SDF functions per object\n\nWe represent SDF $f _ { \\boldsymbol { \\theta } } ( \\mathbf { y } )$ as an MLP that takes in a 3D point $\\mathbf { y } \\in \\mathbb { R } ^ { 3 }$ and outputs a scalar SDF value.   \nLearned SDFs are pre-trained separately for each object and remain fixed throughout the simulation.\n\nMeshes To compare to the existing mesh-based baselines, we apply SDF-Sim on benchmarks where meshes are available and train the learned SDF $f _ { \\boldsymbol { \\theta } } ( \\mathbf { y } )$ from a mesh. To train an SDF, we sample points in 3D space and compute the ground-truth signed distances from these points to the mesh surfaces using a classic BVH [11] algorithm. Finally, we train an MLP $f _ { \\boldsymbol { \\theta } } ( \\mathbf { y } )$ to fit these distances with supervision. See more details in section C. We use the same architecture and the model size for each object shape of 8 MLP layers with 128 units each, unless otherwise stated.\n\nVision We use VolSDF [43] to distill an SDF from a set of images representing a 360-degree view of the outdoor garden scene first described in [7] with camera poses estimated via COLMAP [32]. See visualisation in Figure 3 and section C.2 for details.\n\n# 3.2 Learned simulator\n\nObject representation We represent the shape of the object $O _ { i }$ in the reference pose (centered at zero) as a learned SDF function $f _ { \\theta _ { i } }$ . At a simulation step $t$ , a rigid transformation $\\mathcal { T } _ { i } ^ { t } = ( { \\bf p } _ { i } ^ { t } , { \\bf R } _ { i } ^ { t } )$ transforms the object from reference pose to the current pose in the simulation. Here, $\\mathbf { p } _ { i } ^ { t }$ is an object translation, corresponding to the objectâ€™s center of mass at timestep $t$ ; $\\mathbf { R } _ { i } ^ { t }$ is a rotation. The task of the learned simulator is predicting the next-step transformation ${ \\mathcal { T } } _ { i } ^ { t + 1 }$ for each object. In the following text, we omit the time index $t$ for brevity.\n\nTo represent $I$ objects in the input graph, we introduce object nodes ${ \\mathcal { V } } _ { O } = \\{ { \\bf o } _ { i } | i = 1 . . I \\}$ , located at position $\\mathbf { p } _ { i }$ , and a set of surface nodes $\\mathcal { V } _ { S } = \\{ \\mathbf { n } _ { i k } \\in O _ { i } | \\dot { i } = 1 . . I , k = 1 . . \\dot { K _ { i } } \\}$ on the surface of the objects, where the number of nodes $K _ { i }$ may differ for each object. These surface nodes $\\left\\{ { \\bf n } _ { i k } \\right\\}$ move with their corresponding object according to the transformation $\\mathcal { T } _ { i } ^ { t }$ . With a slight abuse of notation, we will refer to $\\left\\{ { \\bf n } _ { i k } \\right\\}$ both as the node entities and their 3D position in the simulation space.\n\nNodes and edges within the object To construct the graph connectivity within an object, we follow an established line of work on learned simulators [4, 19, 29]. We connect surface nodes $\\left\\{ { \\bf n } _ { i k } \\right\\}$ to their corresponding object node $\\mathbf { o } _ { i }$ using edges $\\mathcal { E } _ { i } ^ { \\mathrm { { i n t r a } } } = \\{ \\mathbf { e _ { o _ { i }  \\mathbf { n } _ { i k } } } | \\mathbf { n } _ { i k } \\in O _ { i } \\}$ . Thus, all the{ info}rmation about object motion, e.g., impulses from collision events, is propagated between the nodes via the object node $\\mathbf { o } _ { i }$ . As shown by [19] we can omit the surface edges between the nodes $\\left\\{ { \\bf n } _ { i k } \\right\\}$ without loss of accuracy.\n\nIn graph networks, nodes and edges are associated with feature vectors that can hold the information about the motion and the relation between the nodes. We follow the approach of [4, 19, 29] to construct a set of nodes and edge features that are necessary for simulation: To construct the node features for surface nodes ${ \\bf n } _ { i k }$ , we compute the finite-difference velocity estimates in the simulation space using a history of the latest three timesteps $\\mathbf { v } _ { i k } \\ = \\ \\big ( \\mathbf { n } _ { i k } ^ { t } \\ - \\mathbf { n } _ { i k } ^ { \\bar { t } - 1 } , \\mathbf { n } _ { i k } ^ { t - 1 } \\ - \\ \\mathbf { n } _ { i k } ^ { t - 2 } \\big )$ . We set node features to be $\\left[ \\mathbf { v } _ { i k } , \\left| \\left| \\mathbf { v } _ { i k } \\right| \\right| , \\phi _ { i } \\right]$ where $\\phi _ { i }$ are the constant object parameters: mass, friction and restitution. We use the same procedure for the object nodes $\\left\\{ \\mathbf { o } _ { i } \\right\\}$ using their positions $\\mathbf { p } _ { i }$ . For intra-object edges $\\mathcal { E } _ { i } ^ { \\mathrm { { i n t r a } } }$ , we use displacement vector between the surface node position and the object center as the edge feature $\\mathbf { e _ { o _ { i }  \\mathbf { n } _ { i k } } } = [ \\mathbf { o } _ { i } - \\mathbf { n } _ { i k } , | | \\mathbf { o } _ { i } - \\mathbf { n } _ { i k } | | ]$ .\n\nSDF-based inter-object edges. Here we introduce a new way to construct collision edges between the objects by leveraging their SDF representations (Figure 4). We design these edges such that they contain sufficient information to detect collisions, while their number remains linear in the number of nodes. This is unlike quadratic number of collision edges in mesh-based simulators.\n\nConsider two objects $O _ { i }$ and $O _ { j }$ . For a node $\\mathbf { n } _ { i k }$ on $O _ { i }$ , we directly query the SDF $f _ { \\theta _ { j } }$ of object $O _ { j }$ to get the distance $d _ { i k } ^ { j }$ from $\\mathbf { n } _ { i k }$ to the closest point on $O _ { j }$ . We note that unlike mesh-based approaches, this is a single test, and we do not need to calculate the distance from $\\mathbf { n } _ { i k }$ to each node/triangle on $O _ { j }$ . Then, if this distance $d _ { i k } ^ { j }$ is within a predefined distance threshold $\\mathcal { D }$ , we connect the surface node $\\mathbf { n } _ { i k }$ directly to the opposing object node ${ \\bf o } _ { j }$ and refer to this edge as $\\mathbf { e _ { o _ { j }  \\mathbf { n } _ { i k } } }$ . Thus, we define the set of inter-object edges as $\\mathscr { E } _ { j i } ^ { \\mathrm { c o l l } } = \\{ \\mathbf { e } _ { \\mathbf { o } _ { j }  \\mathbf { n } _ { i k } } | \\forall \\mathbf { n } _ { i k } \\in O _ { i } : f _ { j } ( \\mathbf { n } _ { i k } ) \\leq \\mathcal { D } \\}$ . This approach is different from mesh-based simulators [4, 19, 29] that connect pairs of nodes or triangles on the two surfaces.\n\nWe construct the features for collision edges $\\mathbf { e _ { o _ { j } } } {  } \\mathbf { n } _ { i k }$ such that they contain information about potential points of collision. First, we compute the closest point $\\mathbf { c } _ { i k } ^ { j }$ from ${ \\mathbf { n } } _ { i k } \\in O _ { i }$ to the surface of object $O _ { j }$ . To do so, we transform the position of $\\mathbf { n } _ { i k }$ into the reference space of $O _ { j }$ using ${ \\mathcal { T } } _ { j } ^ { - 1 } ( { \\bf n } _ { i k } )$ . We call an SDF function to get the distance from the node $\\mathbf { n } _ { i k }$ to the closest point on $O _ { j }$ as $d _ { i k } ^ { j } = f _ { \\theta _ { j } } ( { \\cal T } _ { j } ^ { - 1 } ( { \\bf n } _ { i k } ) )$ . The closest point on the surface of $O _ { j }$ is then computed similarly to Eq. 1:\n\n$$\n\\mathbf { c } _ { i k } ^ { j } = \\mathbf { n } _ { i k } - d _ { i k } ^ { j } \\mathcal { T } _ { j } \\left( \\nabla f _ { \\theta _ { j } } \\left( \\mathcal { T } _ { j } ^ { - 1 } ( \\mathbf { n } _ { i k } ) \\right) \\right)\n$$\n\nNote that $\\mathbf { c } _ { i k } ^ { j }$ lies on the surface defined by $f _ { \\theta _ { j } }$ , but does not have to coincide with any surface node and is not part of node set $\\nu$ . Thus, SDFs allow us to test contact between two objects at higher fidelity, without relying on the density of the surface nodes.\n\nFinally, we construct the features for the collision edges as follows: ${ \\bf e _ { o _ { j }  n _ { k } } } = [ { \\bf c } _ { i k } ^ { j } - { \\bf n } _ { i k } , { \\bf c } _ { i k } ^ { j } - { \\bf \\Delta }$ $\\mathbf { o } _ { j } , \\vert \\vert \\mathbf { c } _ { i k } ^ { j } - \\mathbf { n } _ { i k } \\vert \\vert , \\vert \\vert \\mathbf { c } _ { i k } ^ { j } - \\mathbf { o } _ { j } \\vert \\vert ]$ . These features provide information about the relative position of the closest point (a potential collision point) within the object $O _ { j }$ as well as its relative location to the node $\\mathbf { n } _ { i k }$ on the opposing object. This information is sufficient for the neural network to resolve collisions. Through message-passing over such input graph, the object node ${ \\bf o } _ { j }$ can collect the information from all the nodes that are within a collision radius relative to the object $O _ { j }$ . The model also has access to the velocity and rotation history for both objects through node features, so it is able to infer how fast the node $\\mathbf { n } _ { i k }$ is changing its position w.r.t. $O _ { j }$ .\n\nWith such construction, SDF-Sim requires significantly fewer edges than mesh-based methods, because a single collision edge can test a node against an entire object surface. Asymptotically, SDF-Sim has $\\operatorname { \\mathcal { O } } ( I \\cdot K )$ edges in the worst case, with the number of objects $I$ being magnitudes smaller than the total number of surface nodes $\\begin{array} { r } { K = \\sum _ { i } K _ { i } } \\end{array}$ , instead of $\\mathcal { O } ( K ^ { 2 } )$ for mesh-based simulators. As we demonstrate below, this choice of input graph and simulator unlocks the ability to scale to very large scenes that has not been previously shown in the literature.\n\nGraph network simulator We encode the input graph using MLPs for each node and edge type. We process the graph using 10 message passing steps as in [4, 29]. We decode the processed surface node features into an acceleration estimate ${ \\bf a } _ { i k }$ for each surface node $\\mathbf { n } _ { i k }$ , and compute a per-node position update $\\hat { \\mathbf { n } } _ { i k } ^ { t + 1 }$ using Euler integration. Finally, next-step rigid transformations $\\{ \\hat { \\mathcal { T } } _ { i } ^ { t + 1 } \\}$ are computed from $\\hat { \\mathbf { n } } _ { i k } ^ { t + 1 }$ using shape matching [25], following [4]. The simulator is trained on a single-step prediction task using a per-node L2 loss on the acceleration estimate ${ \\bf a } _ { i k }$ .\n\n# 4 Results\n\nWe start by evaluating the accuracy and efficiency of SDF-Sim on small-scale simulation benchmarks, where evaluation of the baselines is feasible as well. Subsequently, we demonstrate the scaling properties of the models on scenes with an increasing number of objects. We show that SDF-Sim produces realistic rollouts of extremely large scenes with up to 1.1M nodes, which was not possi\n\n![](images/cf827b2ba6169a19c19a5fbc3a9eb43212931977a0f9ced26a88360c0f188ba5.jpg)  \nFigure 5: Comparison of the last frames of rollouts predicted on Movi-C. See more frames in Figure S9 and on the website.\n\nble with the previous generation of learned simulators. We emphasize that the benefits of SDF-Sim are not specific to simulations with similar object shapes, where separate SDFs are used for each object. Finally, we present ablations for how the quality of the learned SDFs affects the simulation. All simulation videos are available on the website https://sites.google.com/view/sdf-sim.\n\nKubric datasets We evaluate SDF-Sim on the benchmark Kubric datasets [16] (shared with Apache 2.0 license), consisting of simulated trajectories of rigid objects thrown towards the center of the scene. We evaluate on two difficulty tiers: Movi-B and Movi-C. Movi-B simulations contain eleven synthetic shapes (e.g., cube, torus, cow) with up to 1142 nodes each. Movi-C comprises 930 scanned real-world household objects, including hollow, flat, or otherwise non-trivial surfaces with thousands of triangles. Both Movi B/C contain only 3-10 objects per simulation. We provide more details about mesh sizes in Movi-B/C in supplement A.1.\n\nWe perform many of our quantitative comparisons on Movi-B/C datasets with small-scale scenes because these domains are small enough for us to run also baseline methods to quantify efficiency and accuracy. On Movi-C we compare only to FIGNet\\* [19], because other baselines run out of memory during training due to the large number of collision edges, as reported by Allen et al. [4].\n\nBaseline models We compare SDF-Sim to the existing state-of-the-art learned simulators for rigidbody interactions: FIGNet [4] and FIGNet\\* [19]. Both methods are based on graph networks. Unlike SDF-Sim, they operate directly over object meshes and use a special type of graph edges between mesh triangles. Their runtime and memory costs grow with the number and size of the object meshes used for the simulation. FIGNet\\* is a memory-efficient version of FIGNet that omits edges between the surface nodes of each object. In their original publication, FIGNet was demonstrated to scale up to 10 objects with a few thousands nodes each [4], while FIGNet\\* was tested up to a larger table scene with $4 0 \\mathrm { k }$ triangles, but with only one moving object [19].\n\n# Graph Edges Peak Memory (MiB) Runtime per step (sec) Translation Error (meters) Rotation Error (deg) 100 35   \n25000 0.3 0.5 25   \n15000 60 0.2 0.3 15 0.1   \n5000 20 0.1 5 Movi-B Movi-C Movi-B Movi-C Movi-B Movi-C Movi-B Movi-C Movi-B Movi-C DPI MGN-Large-Radius MGN FIGNet FIGNet\\* SDF-Sim\n\n![](images/a0159889b5d0e372d0b874eeb2e4fe87685b5d532fcea3ae350e9b8a87912343.jpg)  \nFigure 6: Accuracy, memory and runtime comparisons between the SDF-Sim model and the meshbased baselines on the Movi-B/C benchmarks. On Movi-C, most baselines except FIGNet\\* run out of memory and are not shown. As â€œPeak Memoryâ€ we report the peak memory used by the model per single step of the simulation. DPI, MGN-Large-Radius and MGN results were reported by [4]. See Tables 3 and 4 for the exact numbers.   \nFigure 7: Large-scale simulation of Spheres-in-Bowl, simulated for 200 timesteps. Left: the final step of SDF-Sim rollout on the scene with 512 spheres. Right: number of edges and runtime w.r.t. the number of spheres in the simulation (max 512). In complex simulations with lots of contacts, FIGNet and FIGNet\\* generate an excessive number of collision edges, quickly exceeding GPU memory (end of the orange and blue lines). SDF-Sim generates an order-of-magnitude fewer collision edges, and can easily simulate scenes with 100s of objects without running out of memory.\n\nWe additionally include previously reported results from Allen et al. [4] on Movi-B for the following models: DPI [18] that represents the objects as a set of disjoint particles; as well as MeshGraphNets (MGN) and MGN-Large-Radius [29] that use inter-object edges between mesh surface nodes. As reported by Allen et al. [4], DPI, MGN and MGN-Large-Radius baselines suffer from a prohibitively large size of the inputs graph and can only run on small Movi-B simulations.\n\n# 4.1 Baseline comparison on datasets of small scenes.\n\nFirst, we evaluate SDF-Sim on Movi-B/C datasets with up to 10 objects. As shown in Figure 6, our SDF-Sim uses substantially sparser graphs to represent the scenes, with $28 \\%$ fewer graph edges compared to FIGNet\\* on Movi-B and $3 3 \\%$ fewer on Movi-C. This translates into reduction of peak memory required to execute one step of the simulation; $39 \\%$ reduction on Movi-B and $4 2 . 8 \\%$ on Movi-C. The reduction in the number of edges in SDF-Sim is enabled by object-to-node collision edges that scale linearly with the number of nodes, as opposed to quadratic number of face-face collision edges in FIGNet and FIGNet\\* (in the worst case, see section 2). Sparser graphs in SDF-Sim also lead to lower average runtime per step of a rollout, by $36 \\%$ in Movi-B and $43 \\%$ in Movi-C, because the graph network performs edge updates for/over fewer edges even though the number of nodes in the graph remains the same. Figure S2(a) shows that the runtime of SDF-Sim is consistently lower than that of FIGNet\\* for varying sizes of the input graph. In the next section we show how these efficiency improvements translate into order-or-magnitude gains on large-scale simulations.\n\nIn terms of simulation accuracy, measured as object translation and rotation RMSE errors, SDF-Sim has substantially lower errors than previous baselines DPI, MGN and MGN-Large-Radius on Movi-B. However, in comparison to SOTA models FIGNet and FIGNet\\*, SDF-Sim has a slightly higher error. Note that the errors of SDF-Sim are already very low: the translation error of SDF-Sim is 0.24 meters, which is only $4 . 9 \\%$ of the average object travel distance of 4.92 meters in Movi-C dataset (see tables Tables 3 and 4 for exact numbers). In rigid-body systems, even small discrepancies in predicted positions/rotations can lead to a drastically different object trajectory after a collision, and we consider $4 . 9 \\%$ deviation to be a good result. Finally, in the next section we will demonstrate that on large scenes SDF-Sim is actually more accurate in comparison to FIGNet\\*.\n\n# 4.2 SDF-Sim scales to scenes with up to 512 objects\n\nTo study the scaling properties of the model, we created Spheres-in-Bowl: a set of simulations with a variable number of spheres, ranging from 1 to 512, being dropped into a bowl (Figure 7). We create the ground-truth for these scenes using PyBullet, the same simulator as used for Kubric MoviB/C. We evaluate a set of learned simulators: SDF-Sim and FIGNet\\* models trained on Movi-C, as well as FIGNet trained on Movi-B. Note that the largest version, with 512 spheres, has over 50 times more objects than in Movi-B/C datasets used for training.\n\n![](images/16df4418cb85c4b21d4a067f08a7dbb71be9992fa3d30ef15b21bd18c5463741.jpg)  \nFigure 8: Accuracy metrics w.r.t. simulation time step for Spheres-in-Bowl simulation shown in Figure 7. (a) Penetration metrics. (b) Rollout RMSE. Both metrics are averaged over simulation runs with up to 140 spheres, the maximum for which all baselines could be run.\n\nAs shown in Figure 7, SDF-Sim has an order-of-magnitude fewer collision edges, compared to FIGNet\\* and FIGNet. This difference is substantial, as the number of edges dominates the memory cost. FIGNet and FIGNet\\* run out of memory for simulations with ${ > } 1 4 0$ and $> 1 6 0$ objects, respectively, whereas SDF-Sim can simulate the entire set of 512 interacting objects using the same Nvidia V100 GPU. In terms of total runtime, SDF-Sim is up to 5 times faster than FIGNet\\*, even including the time for querying learned SDFs. To the best of our knowledge, this is the first demonstration of a learned simulator successfully scaling to scenes with hundreds of objects and thousands of collisions, despite being trained only on ten objects per scene.\n\nNext, we evaluate the accuracy of these simulations. SDF-Sim has the lowest penetration distance throughout the simulation, as shown in Figure 8(a). Figure 8(b) shows the rollout errors across different steps of the simulation. Notably, SDF-Sim generalizes to these larger scenes better than FIGNet\\* and has lower error, despite slightly lower accuracy on datasets with smaller scenes (section 4.1). FIGNet has the lowest error out of learned models, likely because it was trained on Movi-B, which contains a sphere object, while Movi-C dataset, used for training Fignet\\* and SDF-Sim, does not. Despite not having seen a sphere object in training, SDF-Sim performs very well on both penetration and rollout metrics, indicating that the SDF representation is not only efficient, but also shows good generalization over geometry.\n\n# 4.3 Scaling to extra-large scenes with up to 1.1 million nodes\n\nNext, we provide a further qualitative demonstration that SDF-Sim can scale to extra-large scenes and produce realistic rollouts. We design three scenes with falling stacks of shoes, metal knots, and mixed objects taken from the Movi-C dataset, shown in Figure 2. Such contact-rich simulations are challenging even for analytical simulators and are classic test examples in the computer graphics literature. These simulations consist of: 300 shoes with 851k nodes, 270 knots with $3 8 4 \\mathrm { k }$ nodes, and 380 various Movi-C objects with 1.1M nodes, respectively. All of these scenes are orders of magnitude larger than those in Movi-C dataset used for SDF-Sim training.\n\nSimulations in Figure 2 show that SDF-Sim can scale to these massive scenes without running out of memory and produce qualitatively realistic rollouts of 200 steps; see videos. Note that we cannot run any other learned baseline on these scenes, as due to the large number of potential contacts, these models produce a vast number of collision edges and exceed the GPU memory.\n\n# 4.4 Simulating real-world scenes from vision\n\nSimulating large real-world scenes is the primary area where the ability to generalize to new, larger scenes is crucial, e.g., in virtual reality applications. Here we present a proof-of-concept that SDF-Sim can successfully handle real scenes, despite being trained only on small simulated assets. We take a Garden scene [7], represented by a sequence of RGB images and use VolSDF [43] to distill a 3D SDF representation of the table and the vase from this scene. SDF-Sim allows to cleanly integrate this 3D representation into a simulation, where it would interact with another object. Finally, we add a shoe object from Movi-C to this scene. Figure 3 shows a rollout from the SDF-Sim model of a shoe falling on top of the vase and the table SDF extracted by VolSDF. With its over 80k nodes, this mesh exceeds the limits of FIGNet baseline that runs out of memory. In contrast, SDF-Sim can capture the nuanced interactions between the objects, which is particularly evident in frames 50 to 70; see the video on the website. This experiment demonstrates that SDF-Sim can generalize to new scenes, despite being trained on synthetic data of small scenes.\n\n![](images/158c785a9fb6eafd8e7e35a84147add2f3fb89f1c18a3b35f942104e7cf085d1.jpg)  \nFigure 9: Ablation on learned SDF model sizes: 8 layer MLPs with layer sizes of 32, 64, or 128 hidden units. (a, b) Translation and rotation error for SDF-Sim trained on Movi-B with different SDF sizes. (c) Mean squared error of the predicted SDF estimates near the surface. (d) Visualisations of the cow shape from Movi-B with different SDF sizes. (e) A cross-section of the learned SDF field for the Movi-B cow shape.\n\n# 4.5 Ablation of learned SDF quality\n\nWe investigate the quality of the estimated SDF values and their impact on simulator performance. We train SDF using MLPs with 32, 64 and 128 units per layer, and 8 layers total. Figure 9(c) shows that larger SDF models help to improve the quality of estimated SDFs. Overall, the learned SDF models of all three layer sizes can reproduce the shape, although the reconstruction from a smallest 32-unit model is more coarse (Figure 9(d), see Figure S8 for more examples). However, Figure 9(a,b) shows that SDF-Sim accuracy is similar across different SDF MLP sizes, suggesting that the learned simulator can learn to compensate for noise in the estimated SDFs. We provide additional metrics on SDF accuracy, SDF gradients and SDF projected closest points in section D.2. We also provide the comparison of memory footprint of SDF versus storing meshes in section D.5.\n\n# 5 Related Work\n\nDynamics over learned SDFs Overall, using implicit shapes, e.g., learned SDFs, for simulation has been explored only in limited settings. DiffSDFSim [35] and DANO [12] use learned object shapes (SDFs and NeRFs respectively) in combination with an analytic contact model on a scene with a single object. Argila et al. [5] and Qiao et al. [30] model the interaction of a learned SDF with a cloth via an analytic simulator. In the space of learned simulators, FIGNet\\* [19] uses a GNN-based model on a 3D scene reconstructed via NeRF and converted into a mesh. Driess et al. [15] learn latent dynamics over NeRFs. Whitney et al. [41] convert a NeRF 3D scene into a set of particles and use them in a GNN simulator. Unlike these pipelines, SDF-Sim avoids the costly simulation of large meshes or sets of particles and operates directly over the SDFs. Mani et al. [23] use learned SDFs to represent large rigid scenes for the sake of efficiency and use the SDF values directly in the GNN simulator. However, this model is designed to use only a single SDF per scene. In contrast, we focus on more challenging interactions of many rigid objects, that is much more prevalent in real applications.\n\nLearning SDFs from vision Learned SDFs started to gain popularity for reconstructing an 3D surfaces from a sequence of images. Unlike NeRFs [7], SDFs are more likely to represent closed, watertight, smooth surfaces. Wang et al. [39], Yariv et al. [43], Yu et al. [44] train an MLP to represent an SDF of a single object. Other works train a single generative model for many SDF shapes, allowing to amortise the training cost, generate new shapes and perform shape completion from partial inputs [9, 10, 28, 34]. Recent works also offer ways to use SDFs for deformable objects [42], articulated objects [24], or to edit SDFs [38].\n\nOne can also reconstruct SDFs from other modalities, e.g., [24, 40] demonstrate how to reconstruct a new unseen object as an SDF using an RGB-D video from a single camera view. SDFs can be recovered for large room-scale scenes [6] or even in real time as the camera moves through the space [27], or from noisy point clouds, e.g., from Lidar [21, 22]. We note that although we can leverage the SDFs extracted by these methods, these works pursue a different goal and focus only on accurate reconstruction of the 3D surface and do not aim to use these SDFs for simulation.\n\n# 6 Discussion and Limitations\n\nWe introduce a rigid-body simulator, SDF-Sim, that uses learned SDFs to represent the object shapes and employs a graph network to learn dynamics over these objects. We re-design the construction of the input graph for the simulator to make use of SDFs and reduce the number of inter-object edges from quadratic to linear, unlocking the ability to scale to large scenes. We provide detailed study on both small-scale benchmarks and large scenes with up to $5 0 \\mathrm { x }$ more objects than seen in the training. Additionally, we investigate the impact of SDF quality on the simulation results. Our work demonstrates, for the first time in the learning simulation literature, that we can use these simulators on scenes with hundreds of objects and up to 1.1 million nodes and produce realistic rollouts, potentially unlocking applications to virtual reality, film, robotics, and more.\n\nLimitations One limitation of SDF-Sim is that it requires training an SDF MLP for every new shape we want to include in a simulation. A promising future direction is to train one model on a dataset of many shapes, amortising the cost of training SDFs, similarly to [28, 34]. We also note that SDF-Sim has slightly higher error on small-scale datasets (section 4.1), although SDF-Sim is closer to the ground-truth on large scenes than FIGNet\\* (section 4.2). We believe it is justifiable to trade a slight increase in model error in exchange for the ability to run on larger scenes than previously possible. We believe that SDF-Sim is therefore best suited for the applications that favour scaling over physical accuracy, such as animation and film. Finally, in this paper we only focus on rigid-rigid interactions, but SDFs can be extended to incorporate deformable or articulated objects [24, 42], fluids [23], and even a mix of SDFs, meshes, and particles.\n\nBroader impact We see our work as a step in making applications that require simulation, such as augmented reality or animation, more accessible to everyday consumers. As such, our work allows us to directly use 3D assets filmed on a mobile phone, and simulate large scenes using a commodity hardware with a single GPU, without requiring a specialized skill of creating meshes appropriate for simulation. Overall, we see SDF-Sim as an important step towards learning physically realistic dynamics models from real-world data, and for enabling physical reasoning over complex, large-scale physical scenes.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³äº†å¤§è§„æ¨¡åœºæ™¯ä¸­åˆšæ€§ç‰©ä½“æ¨¡æ‹Ÿçš„æ•ˆç‡å’Œå†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚ç°æœ‰åŸºäºç½‘æ ¼ï¼ˆmeshï¼‰çš„å­¦ä¹ æ¨¡æ‹Ÿå™¨åœ¨å¤„ç†å¤§é‡ç‰©ä½“æˆ–å¤æ‚å½¢çŠ¶æ—¶ï¼Œç”±äºç¢°æ’æ£€æµ‹çš„è®¡ç®—å’Œå†…å­˜å¼€é”€è¿‡å¤§ï¼Œéš¾ä»¥æ‰©å±•ã€‚\\n> *   è¯¥é—®é¢˜åœ¨æœºå™¨äººã€å·¥ç¨‹ã€ç”µå½±å’Œæ¸¸æˆç­‰é¢†åŸŸå…·æœ‰å…³é”®ä»·å€¼ï¼Œå› ä¸ºè¿™äº›åº”ç”¨éœ€è¦é«˜æ•ˆä¸”ç²¾ç¡®çš„æ¨¡æ‹Ÿå¤§è§„æ¨¡åœºæ™¯ä¸­çš„ç‰©ä½“äº¤äº’ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºSDF-Simï¼Œä¸€ç§åŸºäºå›¾ç½‘ç»œï¼ˆGNNï¼‰çš„å­¦ä¹ æ¨¡æ‹Ÿå™¨ï¼Œåˆ©ç”¨å­¦ä¹ çš„æœ‰ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰è¡¨ç¤ºç‰©ä½“å½¢çŠ¶ï¼Œæ˜¾è‘—å‡å°‘äº†ç¢°æ’æ£€æµ‹çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **åˆ›æ–°è´¡çŒ®1ï¼š** é¦–æ¬¡å°†å­¦ä¹ SDFåº”ç”¨äºåˆšæ€§ç‰©ä½“æ¨¡æ‹Ÿï¼Œé€šè¿‡SDFçš„å¸¸æ•°æ—¶é—´è·ç¦»æŸ¥è¯¢ç‰¹æ€§ï¼Œé¿å…äº†ä¼ ç»Ÿç½‘æ ¼æ–¹æ³•çš„è®¡ç®—ç“¶é¢ˆã€‚\\n> *   **åˆ›æ–°è´¡çŒ®2ï¼š** è®¾è®¡äº†åŸºäºSDFçš„è¾“å…¥å›¾æ„é€ æ–¹æ³•ï¼Œå°†ç¢°æ’è¾¹çš„æ•°é‡ä»äºŒæ¬¡å‡å°‘åˆ°çº¿æ€§ï¼Œå®ç°äº†å¯¹å¤§è§„æ¨¡åœºæ™¯ï¼ˆå¦‚1.1ç™¾ä¸‡èŠ‚ç‚¹ï¼‰çš„é«˜æ•ˆæ¨¡æ‹Ÿã€‚\\n> *   **åˆ›æ–°è´¡çŒ®3ï¼š** å±•ç¤ºäº†SDF-Simå¯ä»¥ç›´æ¥ä»å¤šè§†è§’RGBå›¾åƒä¸­æå–SDFï¼Œæ”¯æŒçœŸå®åœºæ™¯çš„3Dæ¨¡æ‹Ÿã€‚\\n> *   **å…³é”®æ•°æ®ï¼š** SDF-Simåœ¨Movi-Cæ•°æ®é›†ä¸Šçš„ç¿»è¯‘è¯¯å·®ä¸º0.24ç±³ï¼ˆä»…ä¸ºç‰©ä½“å¹³å‡ç§»åŠ¨è·ç¦»çš„4.9%ï¼‰ï¼Œå†…å­˜ä½¿ç”¨æ¯”åŸºçº¿æ¨¡å‹å‡å°‘42.8%ï¼Œè¿è¡Œæ—¶é—´å‡å°‘43%ã€‚\\n> *   **æ‰©å±•æ€§ï¼š** SDF-SimæˆåŠŸæ¨¡æ‹Ÿäº†åŒ…å«512ä¸ªç‰©ä½“å’Œ1.1ç™¾ä¸‡èŠ‚ç‚¹çš„åœºæ™¯ï¼Œè€ŒåŸºçº¿æ¨¡å‹ï¼ˆå¦‚FIGNet*ï¼‰å› å†…å­˜é™åˆ¶æ— æ³•å¤„ç†è¶…è¿‡160ä¸ªç‰©ä½“ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   SDF-Simçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å­¦ä¹ çš„æœ‰ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰éšå¼è¡¨ç¤ºç‰©ä½“å½¢çŠ¶ï¼Œå¹¶é€šè¿‡SDFçš„å¸¸æ•°æ—¶é—´è·ç¦»æŸ¥è¯¢ç‰¹æ€§ï¼Œæ˜¾è‘—å‡å°‘ç¢°æ’æ£€æµ‹çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚\\n> *   è¯¥æ–¹æ³•æœ‰æ•ˆçš„åŸå› æ˜¯SDFèƒ½å¤Ÿå¿«é€Ÿè®¡ç®—ä»»æ„ç‚¹åˆ°ç‰©ä½“è¡¨é¢çš„è·ç¦»ï¼Œè€Œæ— éœ€ä¾èµ–é«˜åˆ†è¾¨ç‡çš„ç½‘æ ¼è¡¨ç¤ºï¼Œä»è€Œé¿å…äº†ä¼ ç»Ÿæ¨¡æ‹Ÿå™¨ä¸­ç¢°æ’æ£€æµ‹çš„å¤æ‚è®¡ç®—ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ç°æœ‰åŸºäºç½‘æ ¼çš„GNNæ¨¡æ‹Ÿå™¨ï¼ˆå¦‚FIGNetï¼‰åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶ï¼Œç”±äºç¢°æ’è¾¹çš„æ•°é‡éšèŠ‚ç‚¹æ•°äºŒæ¬¡å¢é•¿ï¼Œå¯¼è‡´å†…å­˜å’Œè®¡ç®—å¼€é”€è¿‡å¤§ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** SDF-Simé€šè¿‡å¼•å…¥SDFè¡¨ç¤ºç‰©ä½“å½¢çŠ¶ï¼Œå°†ç¢°æ’è¾¹çš„æ•°é‡ä»äºŒæ¬¡å‡å°‘åˆ°çº¿æ€§ï¼ŒåŒæ—¶åˆ©ç”¨SDFçš„å¿«é€Ÿè·ç¦»æŸ¥è¯¢ç‰¹æ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡æ‹Ÿæ•ˆç‡ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **SDFå­¦ä¹ ï¼š** ä¸ºæ¯ä¸ªç‰©ä½“è®­ç»ƒä¸€ä¸ªMLPï¼Œå­¦ä¹ å…¶SDFè¡¨ç¤ºã€‚SDFé€šè¿‡é‡‡æ ·3Dç©ºé—´ä¸­çš„ç‚¹å¹¶è®¡ç®—å…¶åˆ°ç‰©ä½“è¡¨é¢çš„è·ç¦»è¿›è¡Œç›‘ç£è®­ç»ƒã€‚\\n> 2.  **è¾“å…¥å›¾æ„é€ ï¼š** æ„å»ºåŒ…å«ç‰©ä½“èŠ‚ç‚¹å’Œè¡¨é¢èŠ‚ç‚¹çš„è¾“å…¥å›¾ã€‚ç‰©ä½“èŠ‚ç‚¹è¡¨ç¤ºç‰©ä½“çš„ä¸­å¿ƒï¼Œè¡¨é¢èŠ‚ç‚¹è¡¨ç¤ºç‰©ä½“è¡¨é¢çš„ç‚¹ã€‚\\n> 3.  **ç¢°æ’è¾¹ç”Ÿæˆï¼š** å¯¹äºæ¯ä¸ªè¡¨é¢èŠ‚ç‚¹ï¼Œé€šè¿‡æŸ¥è¯¢å…¶ä»–ç‰©ä½“çš„SDFï¼Œè®¡ç®—å…¶åˆ°æœ€è¿‘ç‰©ä½“è¡¨é¢çš„è·ç¦»ã€‚å¦‚æœè·ç¦»å°äºé˜ˆå€¼ï¼Œåˆ™ç”Ÿæˆä¸€æ¡ä»è¡¨é¢èŠ‚ç‚¹åˆ°å¯¹æ–¹ç‰©ä½“èŠ‚ç‚¹çš„ç¢°æ’è¾¹ã€‚\\n> 4.  **å›¾ç½‘ç»œæ¨¡æ‹Ÿï¼š** ä½¿ç”¨GNNå¤„ç†è¾“å…¥å›¾ï¼Œé¢„æµ‹ç‰©ä½“çš„ä¸‹ä¸€çŠ¶æ€ã€‚é€šè¿‡æ¶ˆæ¯ä¼ é€’å’Œå½¢çŠ¶åŒ¹é…ï¼ˆshape matchingï¼‰è®¡ç®—ç‰©ä½“çš„ä½ç½®å’Œæ—‹è½¬æ›´æ–°ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡ä¸­æä¾›äº†ä¸€ä¸ªç¤ºä¾‹åœºæ™¯â€œSpheres-in-Bowlâ€ï¼Œæ¨¡æ‹Ÿ512ä¸ªçƒä½“è½å…¥ç¢—ä¸­çš„è¿‡ç¨‹ã€‚SDF-Simé€šè¿‡SDFçš„çº¿æ€§ç¢°æ’è¾¹æ„é€ ï¼ŒæˆåŠŸæ¨¡æ‹Ÿäº†è¿™ä¸€å¤§è§„æ¨¡åœºæ™¯ï¼Œè€ŒåŸºçº¿æ¨¡å‹ï¼ˆå¦‚FIGNet*ï¼‰ç”±äºå†…å­˜é™åˆ¶æ— æ³•å¤„ç†è¶…è¿‡160ä¸ªç‰©ä½“ã€‚\\n> *   å¦ä¸€ä¸ªç¤ºä¾‹å±•ç¤ºäº†ä»å¤šè§†è§’RGBå›¾åƒä¸­æå–SDFï¼Œå¹¶ç”¨äºæ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸­çš„ç‰©ä½“äº¤äº’ï¼ˆå¦‚é‹å­å’ŒèŠ±ç“¶çš„ç¢°æ’ï¼‰ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   FIGNetï¼šåŸºäºç½‘æ ¼çš„GNNæ¨¡æ‹Ÿå™¨ï¼Œä½¿ç”¨é¢-é¢ç¢°æ’è¾¹ã€‚\\n> *   FIGNet*ï¼šFIGNetçš„å†…å­˜é«˜æ•ˆç‰ˆæœ¬ï¼Œçœç•¥äº†è¡¨é¢èŠ‚ç‚¹ä¹‹é—´çš„è¾¹ã€‚\\n> *   DPIï¼šåŸºäºç²’å­è¡¨ç¤ºçš„æ¨¡æ‹Ÿå™¨ã€‚\\n> *   MGNå’ŒMGN-Large-Radiusï¼šåŸºäºç½‘æ ¼çš„GNNæ¨¡æ‹Ÿå™¨ï¼Œä½¿ç”¨èŠ‚ç‚¹-èŠ‚ç‚¹ç¢°æ’è¾¹ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨ç¿»è¯‘è¯¯å·®ï¼ˆTranslation Errorï¼‰ä¸Šï¼š** SDF-Simåœ¨Movi-Cæ•°æ®é›†ä¸Šçš„ç¿»è¯‘è¯¯å·®ä¸º0.24ç±³ï¼Œä¼˜äºDPIï¼ˆ0.5ç±³ï¼‰å’ŒMGN-Large-Radiusï¼ˆ0.3ç±³ï¼‰ï¼Œä½†ç•¥é«˜äºFIGNet*ï¼ˆ0.2ç±³ï¼‰ã€‚\\n> *   **åœ¨æ—‹è½¬è¯¯å·®ï¼ˆRotation Errorï¼‰ä¸Šï¼š** SDF-Simåœ¨Movi-Cä¸Šçš„æ—‹è½¬è¯¯å·®ä¸º15åº¦ï¼Œä¼˜äºDPIï¼ˆ25åº¦ï¼‰å’ŒMGN-Large-Radiusï¼ˆ20åº¦ï¼‰ï¼Œä½†ç•¥é«˜äºFIGNet*ï¼ˆ10åº¦ï¼‰ã€‚\\n> *   **åœ¨å†…å­˜ä½¿ç”¨ï¼ˆPeak Memoryï¼‰ä¸Šï¼š** SDF-Simåœ¨Movi-Cä¸Šçš„å†…å­˜ä½¿ç”¨ä¸º60 MiBï¼Œæ¯”FIGNet*ï¼ˆ105 MiBï¼‰å‡å°‘äº†42.8%ã€‚\\n> *   **åœ¨è¿è¡Œæ—¶é—´ï¼ˆRuntime per stepï¼‰ä¸Šï¼š** SDF-Simåœ¨Movi-Cä¸Šçš„è¿è¡Œæ—¶é—´ä¸º0.2ç§’ï¼Œæ¯”FIGNet*ï¼ˆ0.35ç§’ï¼‰å‡å°‘äº†43%ã€‚\\n> *   **åœ¨æ‰©å±•æ€§ï¼ˆScalabilityï¼‰ä¸Šï¼š** SDF-SimæˆåŠŸæ¨¡æ‹Ÿäº†åŒ…å«512ä¸ªç‰©ä½“å’Œ1.1ç™¾ä¸‡èŠ‚ç‚¹çš„åœºæ™¯ï¼Œè€ŒFIGNet*å’ŒFIGNetåˆ†åˆ«å› å†…å­˜é™åˆ¶æ— æ³•å¤„ç†è¶…è¿‡160å’Œ140ä¸ªç‰©ä½“ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   åˆšæ€§ç‰©ä½“æ¨¡æ‹Ÿ (Rigid-Body Simulation, N/A)\\n*   æœ‰ç¬¦å·è·ç¦»å‡½æ•° (Signed Distance Function, SDF)\\n*   å›¾ç½‘ç»œ (Graph Neural Network, GNN)\\n*   ç¢°æ’æ£€æµ‹ (Collision Detection, N/A)\\n*   å¤§è§„æ¨¡åœºæ™¯æ¨¡æ‹Ÿ (Large-Scale Scene Simulation, N/A)\\n*   å­¦ä¹ æ¨¡æ‹Ÿå™¨ (Learned Simulator, N/A)\\n*   å¤šè§†è§’å›¾åƒ (Multi-View Images, N/A)\\n*   éšå¼å½¢çŠ¶è¡¨ç¤º (Implicit Shape Representation, N/A)\"\n}\n```"
}