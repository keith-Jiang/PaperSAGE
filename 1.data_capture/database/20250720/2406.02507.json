{
    "link": "https://arxiv.org/abs/2406.02507",
    "pdf_link": "https://arxiv.org/pdf/2406.02507",
    "title": "Guiding a Diffusion Model with a Bad Version of Itself",
    "authors": [
        "Tero Karras",
        "M. Aittala",
        "T. Kynkäänniemi",
        "J. Lehtinen",
        "Timo Aila",
        "S. Laine"
    ],
    "institutions": [
        "Aalto University",
        "NVIDIA"
    ],
    "publication_date": "2024-06-04",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 90,
    "influential_citation_count": 27,
    "paper_content": "# Guiding a Diffusion Model with a Bad Version of Itself\n\nTero Karras NVIDIA\n\nMiika Aittala NVIDIA\n\nTuomas Kynkäänniemi Aalto University\n\nJaakko Lehtinen NVIDIA, Aalto University\n\nTimo Aila NVIDIA\n\nSamuli Laine NVIDIA\n\n# Abstract\n\nThe primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for $6 4 \\times 6 4$ and 1.25 for $5 1 2 \\times 5 1 2$ , using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.\n\n# 1 Introduction\n\nDenoising diffusion models [14, 42, 43, 44, 45] generate synthetic images by reversing a stochastic corruption process. Essentially, an image is revealed from pure noise by denoising it little by little in successive steps. A neural network that implements the denoiser (equivalently [49], the score function [17]) is a central design element, and various architectures have been proposed (e.g., [1, 6, 8, 16, 18, 21, 34]). Equally important are the details of the multi-step denoising process that corresponds mathematically to solving an ordinary [31, 43] or a stochastic [45] differential equation, for which many different parameterizations, solvers, and step schedules have been evaluated [19, 20, 26, 39, 53]. To control the output image, the denoiser is typically conditioned on a class label, an embedding of a text prompt, or some other form of conditioning input [32, 37, 41, 52].\n\nThe training objective of a diffusion model aims to cover the entire (conditional) data distribution. This causes problems in low-probability regions: The model gets heavily penalized for not representing them, but it does not have enough data to learn to generate good images corresponding to them. Classifier-free guidance (CFG) [15] has become the standard method for “lowering the sampling temperature”, i.e., focusing the generation on well-learned high-probability regions. By training a denoiser network to operate in both conditional and unconditional setting, the sampling process can be steered away from the unconditional result — in effect, the unconditional generation task specifies a result to avoid. This results in better prompt alignment and improved image quality, where the former effect is due to CFG implicitly raising the conditional part of the probability density to a power greater than one [9].\n\nHowever, CFG has drawbacks that limit its usage as a general low-temperature sampling method. First, it is applicable only for conditional generation, as the guidance signal is based on the difference between conditional and unconditional denoising results. Second, because the unconditional and conditional denoisers are trained to solve a different task, the sampling trajectory can overshoot the desired conditional distribution, which leads to skewed and often overly simplified image compositions [24]. Finally, the prompt alignment and quality improvement effects cannot be controlled separately, and it remains unclear how exactly they relate to each other.\n\nIn this paper, we provide new insights into why CFG improves image quality and show how this effect can be separated out into a novel method that we call autoguidance. Our method does not suffer from the task discrepancy problem because we use an inferior version of the main model itself as the guiding model, with unchanged conditioning. This guiding model can be obtained by simply limiting, e.g., model capacity and/or training time. We validate the effectiveness of autoguidance in various synthetic test cases as well as in practical image synthesis in class-conditional and text-conditional settings. In addition, our method enables guidance for unconditional synthesis. In quantitative tests, the generated image distributions are improved considerably when measured using FID [12] and FDDINOv2 [47] metrics, setting new records in ImageNet-512 and ImageNet-64 generation.\n\nOur implementation and pre-trained models are available at https://github.com/NVlabs/edm2\n\n# 2 Background\n\nDenoising diffusion. Denoising diffusion generates samples from a distribution $p _ { \\mathrm { d a t a } } ( \\mathbf { x } )$ by iteratively denoising a sample of pure white noise, such that a noise-free random data sample is gradually revealed [14]. The idea is to consider heat diffusion of $p _ { \\mathrm { d a t a } } ( \\mathbf { x } )$ into a sequence of increasingly smoothed densities $p ( \\mathbf { x } ; \\sigma ) = p _ { \\mathrm { d a t a } } ( \\mathbf { x } ) * \\mathcal { N } ( \\mathbf { x } ; \\mathbf { 0 } , \\sigma ^ { 2 } \\mathbf { I } )$ . For a large enough $\\sigma _ { \\mathrm { m a x } }$ , we have $p ( \\mathbf { x } ; \\sigma _ { \\operatorname* { m a x } } ) \\approx \\mathcal { N } ( \\mathbf { x } ; \\mathbf { 0 } , \\sigma _ { \\operatorname* { m a x } } ^ { 2 } \\mathbf { I } )$ , from which we can trivially sample by drawing normally distributed white noise. The resulting sample is then evolved backward towards low noise levels by a probability flow ODE [20, 43, 45]\n\n$$\n\\mathrm { d } \\mathbf { x } _ { \\sigma } = - \\sigma \\nabla _ { \\mathbf { x } _ { \\sigma } } \\mathrm { l o g } p ( \\mathbf { x } _ { \\sigma } ; \\sigma ) \\mathrm { d } \\sigma\n$$\n\nthat maintains the property $\\mathbf { x } _ { \\sigma } \\sim p ( \\mathbf { x } _ { \\sigma } ; \\sigma )$ for every $\\sigma \\in [ 0 , \\sigma _ { \\operatorname* { m a x } } ]$ . Upon reaching $\\sigma = 0$ , we obtain $\\mathbf { x } _ { 0 } \\sim p ( \\mathbf { x } _ { 0 } ; 0 ) = p _ { \\mathrm { d a t a } } ( \\mathbf { x } _ { 0 } )$ as desired.\n\nIn practice, the ODE is solved numerically by stepping along the trajectory defined by Equation 1. This requires evaluating the so-called score function [17] $\\nabla _ { \\mathbf { x } } \\log p ( \\mathbf { x } ; \\sigma )$ for a given sample $\\mathbf { x }$ and noise level $\\sigma$ at each step. Rather surprisingly, we can approximate this vector using a neural network $D _ { \\theta } ( \\mathbf { x } ; \\boldsymbol { \\sigma } )$ parameterized by weights $\\theta$ trained for the denoising task\n\n$$\n\\begin{array} { r } { \\theta = \\arg \\operatorname* { m i n } _ { \\theta } \\mathbb { E } _ { { \\mathbf { y } } \\sim p _ { \\mathrm { d a t a } } , \\sigma \\sim p _ { \\mathrm { t r a i n } } , { \\mathbf { n } } \\sim \\mathcal { N } ( \\mathbf { 0 } , \\sigma ^ { 2 } { \\mathbf { I } } ) } \\| D _ { \\theta } ( { \\mathbf { y } } + { \\mathbf { n } } ; \\sigma ) - { \\mathbf { y } } \\| _ { 2 } ^ { 2 } , } \\end{array}\n$$\n\nwhere $p _ { \\mathrm { t r a i n } }$ controls the noise level distribution during training. Given $D _ { \\theta }$ , we can estimate $\\nabla _ { \\mathbf { x } } \\log { p ( \\mathbf { x } ; \\sigma ) } \\approx ( D _ { \\theta } ( \\mathbf { x } ; \\sigma ) - \\mathbf { x } ) / \\sigma ^ { 2 }$ , up to approximation errors due to, e.g., finite capacity or training time [20, 49]. As such, we are free to interpret the network as predicting either a denoised sample or a score vector, whichever is more convenient for the analysis at hand. Many reparameterizations and practical ODE solvers are possible, as enumerated by Karras et al. [20]. We follow their recommendations, including the schedule $\\sigma ( t ) = t$ that lets us parameterize the ODE directly via noise level $\\sigma$ instead of a separate time variable $t$ .\n\nIn most applications, each data sample $\\mathbf { x }$ is associated with a label c, representing, e.g., a class index or a text prompt. At generation time, we control the outcome by choosing a label c and seeking a sample from the conditional distribution $p ( \\mathbf { x } | \\mathbf { c } ; \\boldsymbol { \\sigma } )$ with $\\sigma = 0$ . In practice, this is achieved by training a denoiser network $D _ { \\theta } ( \\mathbf { x } ; \\sigma , \\mathbf { c } )$ that accepts $\\mathbf { c }$ as an additional conditioning input.\n\nClassifier-free guidance. For complex visual datasets, the generated images often fail to reproduce the clarity of the training images due to approximation errors made by finite-capacity networks. A broadly used trick called classifier-free guidance (CFG) [15] pushes the samples towards higher likelihood of the class label, sacrificing variety for “more canonical” images that the network appears to be better capable of handling.\n\nIn a general setting, guidance in a diffusion model involves two denoiser networks $D _ { 0 } ( \\mathbf { x } ; \\sigma , \\mathbf { c } )$ and $D _ { 1 } ( \\mathbf { x } ; \\boldsymbol { \\sigma } , \\mathbf { c } )$ . The guiding effect is achieved by extrapolating between the two denoising results by a factor $w$ :\n\n$$\nD _ { w } ( \\mathbf { x } ; \\sigma , \\mathbf { c } ) = w D _ { 1 } ( \\mathbf { x } ; \\sigma , \\mathbf { c } ) + ( 1 - w ) D _ { 0 } ( \\mathbf { x } ; \\sigma , \\mathbf { c } ) .\n$$\n\nTrivially, setting $w = 0$ or $w = 1$ recovers the output of $D _ { 0 }$ and $D _ { 1 }$ , respectively, while choosing $w > 1$ over-emphasizes the output of $D _ { 1 }$ . Recalling the equivalence of denoisers and scores [49], we can write\n\n$$\n\\begin{array} { r } { D _ { w } ( \\mathbf { x } ; \\sigma , \\mathbf { c } ) \\approx \\mathbf { x } + \\sigma ^ { 2 } \\nabla _ { \\mathbf { x } } \\log \\underbrace { \\left( p _ { 0 } ( \\mathbf { x } | \\mathbf { c } ; \\sigma ) \\left[ \\frac { p _ { 1 } ( \\mathbf { x } | \\mathbf { c } ; \\sigma ) } { p _ { 0 } ( \\mathbf { x } | \\mathbf { c } ; \\sigma ) } \\right] ^ { w } \\right) } _ { \\propto : p _ { w } ( \\mathbf { x } | \\mathbf { c } ; \\sigma ) } . } \\end{array}\n$$\n\nThus, guidance grants us access to the score of the density $p _ { w } ( \\mathbf { x } | \\mathbf { c } ; \\boldsymbol { \\sigma } )$ implied in the parentheses. This score can be further written as [9, 15]\n\n$$\n\\nabla _ { \\mathbf { x } } \\log p _ { w } ( \\mathbf { x } | \\mathbf { c } ; \\sigma ) = \\nabla _ { \\mathbf { x } } \\log p _ { 1 } ( \\mathbf { x } | \\mathbf { c } ; \\sigma ) + ( w - 1 ) \\nabla _ { \\mathbf { x } } \\log \\frac { p _ { 1 } ( \\mathbf { x } | \\mathbf { c } ; \\sigma ) } { p _ { 0 } ( \\mathbf { x } | \\mathbf { c } ; \\sigma ) } .\n$$\n\nSubstituting this expression into the ODE of Equation 1, this yields the standard evolution for generating images from $p _ { 1 }$ , plus a perturbation that increases (for $w > 1$ ) the ratio of $p _ { 1 }$ and $p _ { 0 }$ as evaluated at the sample. The latter can be interpreted as increasing the likelihood that a hypothetical classifier would attribute for the sample having come from density $p _ { 1 }$ rather than $p _ { 0 }$ .\n\nIn CFG, we train an auxiliary unconditional denoiser $D _ { \\theta } ( \\mathbf { x } ; \\boldsymbol { \\sigma } )$ to denoise the distribution $p ( \\mathbf { x } ; \\boldsymbol { \\sigma } )$ marginalized over c, and use this as $D _ { 0 }$ . In practice, this is typically [15] done using the same network $D _ { \\theta }$ with an empty conditioning label, setting $D _ { 0 } : = D _ { \\theta } ( \\mathbf { x } ; \\boldsymbol { \\sigma } , \\emptyset )$ and $D _ { 1 } : = \\bar { D } _ { \\boldsymbol { \\theta } } ( \\mathbf { x } ; \\sigma , \\mathbf { c } )$ . By Bayes’ rule, the extrapolated score vector becomes $\\nabla _ { \\mathbf { x } } \\log p ( \\mathbf { x } | \\mathbf { c } ; \\sigma ) + ( w - 1 ) \\nabla _ { \\mathbf { x } } \\log p ( \\mathbf { c } | \\mathbf { x } ; \\sigma )$ . During sampling, this guides the image to more strongly align with the specified class $\\mathbf { c }$ .\n\nIt would be tempting to conclude that solving the diffusion ODE with the score function of Equation 5 produces samples from the data distribution specified by $p _ { w } ( \\mathbf { x } | \\mathbf { c } ; 0 )$ . Unfortunately this is not the case, because $p _ { w } ( \\mathbf { x } | \\mathbf { c } ; \\boldsymbol { \\sigma } )$ does not represent a valid heat diffusion of $p _ { w } ( \\mathbf { x } | \\mathbf { c } ; 0 )$ . Therefore, solving the ODE does not, in fact, follow the density. Instead, the samples are blindly pushed towards higher values of the implied density at each noise level during sampling. This can lead to distorted sampling trajectories, greatly exaggerated truncation, and mode dropping in the results [24], as well as over-saturation of colors [41]. Nonetheless, the improvement in image quality is often remarkable, and high guidance values are commonly used despite the drawbacks (e.g., [13, 35, 37, 41]).\n\n# 3 Why does CFG improve image quality?\n\nWe begin by identifying the mechanism by which CFG improves image quality instead of only affecting prompt alignment. To illustrate why unguided diffusion models often produce unsatisfactory images, and how CFG remedies the problem, we study a 2D toy example where a small-scale denoiser network is trained to perform conditional diffusion in a synthetic dataset (Figure 1). The dataset is designed to exhibit low local dimensionality (i.e., highly anisotropic and narrow support) and hierarchical emergence of local detail upon noise removal. These are both properties that can be expected from the actual manifold of realistic images [4, 36]. For details of the setup, see Appendix C.\n\nScore matching leads to outliers. Compared to sampling directly from the underlying distribution (Figure 1a), the unguided diffusion in Figure 1b produces a large number of extremely unlikely samples outside the bulk of the distribution. In the image generation setting, these would correspond to unrealistic and broken images.\n\nWe argue that the outliers stem from the limited capability of the score network combined with the score matching objective. It is well known that maximum likelihood (ML) estimation leads to a “conservative” fit of the data distribution [2] in the sense that the model attempts to cover all training samples. This is because the underlying Kullback–Leibler divergence incurs extreme penalties if the model severely underestimates the likelihood of any training sample. While score matching is generally not equal to ML estimation, they are closely related [14, 28, 45] and appear to exhibit broadly similar behavior. For example, it is known that for a multivariate Gaussian model, the optimal score matching fit coincides with the ML estimate [17]. Figures 2a and 2b show the learned score field and implied density in our toy example for two models of different capacity at an intermediate noise level. The stronger model envelops the data more tightly, while the weaker model’s density is more spread out.\n\nFrom the perspective of image generation, a tendency to cover the entire training data becomes a problem: The model ends up producing strange and unlikely images from the data distribution’s extremities that are not learnt accurately but included just to avoid the high loss penalties. Furthermore, during training, the network has only seen real noisy images as inputs, and during sampling it may not be prepared to deal with the unlikely samples it is handed down from the higher noise levels.\n\n![](images/5be7fa71b779385ce9ec140d012706cfc79a57da6482fc9d2c46492148b73407.jpg)  \nFigure 1: A fractal-like 2D distribution with two classes indicated with gray and orange regions. Approximately $9 9 \\%$ of the probability mass is inside the shown contours. (a) Ground truth samples drawn directly from the orange class distribution. (b) Conditional sampling using a small denoising diffusion model generates outliers. (c) Classifier-free guidance $\\omega = 4$ ) eliminates outliers but reduces diversity by over-emphasizing the class. (d) Naive truncation via lengthening the score vectors. (e) Our method concentrates samples on high-probability regions without reducing diversity.\n\n![](images/22b9cb8e2885bbdde585c9c2348264557f3ef652a51b729ad7d12e10b223fc32.jpg)  \nFigure 2: Closeup of the region highlighted in Figure 1c. (a) The implied learned density $p _ { 1 } ( \\mathbf { x } | \\mathbf { c } ; \\boldsymbol { \\sigma } _ { \\mathrm { m i d } } )$ (green) at an intermediate noise level $\\sigma _ { \\mathrm { m i d } }$ and its score vectors (log-gradients), plotted at representative sample points. The learned density approximates the underlying ground truth $p ( \\mathbf { x } | \\mathbf { \\bar { c } } ; \\sigma _ { \\mathrm { m i d } } )$ (orange) but fails to replicate its sharper details. (b) The weaker unconditional model learns a further spread-out density $p _ { \\mathrm { 0 } } ( \\mathbf { x } ; \\boldsymbol { \\sigma } _ { \\mathrm { m i d } } )$ (red) with a looser fit to the data. (c) Guidance moves the points according to the gradient of the (log) ratio of the two learned densities (blue). As the higher-quality model is more sharply concentrated at the data, this field tends inward towards the data distribution. The corresponding gradient is simply the difference of respective gradients in (a) and (b), illustrated at selected points. (d) Sampling trajectories taken by standard unguided diffusion following the learned score $\\nabla _ { \\mathbf { x } } \\log p _ { 1 } ( \\mathbf { x } | \\mathbf { c } ; \\boldsymbol { \\sigma } )$ , from noise level $\\sigma _ { \\mathrm { m i d } }$ to 0. The contours (orange) represent the ground truth noise-free density. (e) Guidance introduces an additional force shown in (c), causing the points to concentrate at the core of the data density during sampling.\n\nCFG eliminates outliers. The effect of applying classifier-free guidance during generation is demonstrated in Figure 1c. As expected, the samples avoid the class boundary (i.e., there are no samples in the vicinity of the gray area), and entire branches of the distribution are dropped. We also observe a second phenomenon, where the samples have been pulled in towards the core of the manifold, and away from the low-probability intermediate regions. Seeing that this eliminates the unlikely outlier samples, we attribute the image quality improvement to it. However, mere boosting of the class likelihood does not explain this increased concentration.\n\nWe argue that this phenomenon stems from a quality difference between the conditional and unconditional denoiser networks. The denoiser $D _ { 0 }$ faces a more difficult task of the two: It has to generate from all classes at once, whereas $D _ { 1 }$ can focus on a single class for any specific sample. Given the more difficult task, and typically only a small slice of the training budget, the network $D _ { 0 }$ attains a worse fit to the data.1 This difference in accuracy is apparent in respective plots of the learned densities in Figures 2a and 2b.\n\nFrom our interpretation in Section 2, it follows that CFG is not only boosting the likelihood of the sample having come from the class c, but also that of having come from the higher-quality implied distribution. Recall that guidance boils down to an additional force (Equation 5) that pulls the samples towards higher values of log $[ p _ { 1 } ( { \\bf x } | { \\bf c } ; \\sigma ) / p _ { 0 } ( { \\bf x } | { \\bf c } ; \\sigma ) ]$ . Plotting this ratio for our toy example in Figure 2c, along with corresponding gradients that guidance contributes to the ODE vector field, we see that the ratio generally decreases with distance from the manifold due to the denominator $p _ { 0 }$ representing a more spread-out distribution, and hence falling off slower than the numerator $p _ { 1 }$ . Consequently, the gradients point inward towards the data manifold. Each contour of the density ratio corresponds to a specific likelihood that a hypothetical classifier would assign on a sample being drawn from $p _ { 1 }$ instead of $p _ { 0 }$ . Because the contours roughly follow the local orientation and branching of the data manifold, pushing samples deeper into the “good side” concentrates them at the manifold.2\n\nDiscussion. We can expect the two models to suffer from inability to fit at similar places, but to a different degree. The predictions of the denoisers will disagree more decisively in these regions. As such, CFG can be seen as a form of adaptive truncation that identifies when a sample is likely to be under-fit and pushes it towards the general direction of better samples. Figures 2d and 2e show the effect over the course of generation: The truncation “overshoots” the correction and produces a narrower distribution than the ground truth, but in practice this does not appear to have an adverse effect on the images.\n\nIn contrast, a naive attempt at achieving this kind of truncation — inspired by, e.g., the truncation trick in GANs [3, 29] or lowering temperature in generative language models — would counteract the smoothing by uniformly lengthening the score vectors by a factor $w > 1$ . This is illustrated in Figure 1d, where the samples are indeed concentrated in high-probability regions, but in an isotropic fashion that leaves the outer branches empty. In practice, images generated this way tend to show reduced variation, oversimplified details, and monotone texture.\n\n# 4 Our method\n\nWe propose to isolate the image quality improvement effect by directly guiding a high-quality model $D _ { 1 }$ with a poor model $D _ { 0 }$ trained on the same task, conditioning, and data distribution, but suffering from certain additional degradations, such as low capacity and/or under-training. We call this procedure autoguidance, as the model is guided with an inferior version of itself.\n\nIn the context of our 2D toy example, this turns out to work surprisingly well. Figure 1e demonstrates the effect of using a smaller $D _ { 0 }$ with fewer training iterations. As desired, the samples are pulled close to the distribution without systematically dropping any part of it.\n\nTo analyze why this technique works, recall that under limited model capacity, score matching tends to over-emphasize low-probability (i.e., implausible and under-trained) regions of the data distribution. Exactly where and how the problems appear depend on various factors such as network architecture, dataset, training details, etc., and we cannot expect to identify and characterize the specific issues a priori. However, we can expect a weaker version of the same model to make broadly similar errors in the same regions, only stronger. Autoguidance seeks to identify and reduce the errors made by the stronger model by measuring its difference to the weaker model’s prediction, and boosting it. When the two models agree, the perturbation is insignificant, but when they disagree, the difference indicates the general direction towards better samples.\n\nAs such, we can expect autoguidance to work if the two models suffer from degradations that are compatible with each other. Since any $D _ { 1 }$ can be expected to suffer from, e.g., lack of capacity and lack of training — at least to some degree — it makes sense to choose $D _ { 0 }$ so that it further exacerbates these aspects.\n\nIn practice, models that are trained separately or for a different number of iterations differ not only in accuracy of fit, but also in terms of random initialization, shuffling of the training data, etc. For guidance to be successful, the quality gap should be large enough to make the systematic spreading-out of the density outweigh these random effects.\n\nStudy on synthetic degradations. To validate our hypothesis that the two models must suffer from the same kind of degradations, we perform a controlled experiment using synthetic corruptions applied to a well-trained real-world image diffusion model. We create the main and guiding networks, $D _ { 1 }$ and $D _ { 0 }$ , by applying different degrees of a synthetic corruption to the base model. This construction allows us to use the untouched base model as grounding when measuring the FID effect of the various combinations of corruptions applied to $D _ { 1 }$ and $D _ { 0 }$ . We find that as long as the degradations are compatible, autoguidance largely undoes the damage caused by the corruptions:\n\n• Base model: As the base model, we use EDM2-S trained on ImageNet-512 without dropout $( \\mathrm { F I D } = 2 . 5 6 ) \\$ ).   \n• Dropout: We construct $D _ { 1 }$ by applying $5 \\%$ dropout to the base model in a post-hoc fashion $\\mathrm { ( F I D = 4 . 9 8 ) }$ ), and $D _ { 0 }$ by applying $10 \\%$ dropout to the base model $\\mathrm { ( F I D = 1 5 . 0 0 ) }$ ). Applying autoguidance, we reach the best result $( \\mathrm { F I D } = 2 . 5 5 ,$ ) with $w = 2 . 2 5$ , matching the base model’s FID.   \n• Input noise: We construct $D _ { 1 }$ by modifying the base model to add noise to the input images so that their noise level is increased by $10 \\%$ $\\mathrm { { F I D } } = 3 . 9 6 )$ . The $\\sigma$ conditioning input of the denoiser is adjusted accordingly. The guiding model $D _ { 0 }$ is constructed similarly, but with a noise level increase of $20 \\%$ $( \\mathrm { F I D } = 9 . 7 3 )$ ). Applying autoguidance, we reach the best result $( \\mathrm { F I D } = 2 . 5 6 )$ with $w = 2 . 0 0$ , again matching the base model’s FID.   \n• Mismatched degradations: If we corrupt $D _ { 1 }$ by dropout and $D _ { 0 }$ by input noise, or vice versa, guidance does not improve the results at all; in these cases, the best FID is obtained by setting $w = 1$ , i.e., by disabling guidance and using the less corrupted $D _ { 1 }$ exclusively.\n\nWhile this experiment corroborates our main hypothesis, we do not suggest that guiding with these synthetic degradations would be useful in practice. A realistic diffusion model will not suffer from these particular degradations, so creating a guiding model by introducing them would not yield consistent truncation towards the data manifold.\n\n# 5 Results\n\nOur primary evaluation is carried out using ImageNet (ILSVRC2012) [7] at two resolutions: $5 1 2 \\times 5 1 2$ and $6 4 \\times 6 4$ . For ImageNet-512 we use latent diffusion [38], while ImageNet-64 works directly on RGB pixels. We take the current state-of-the-art diffusion model EDM2 [21] as our baseline.3 We use the EDM2-S and EDM2-XXL models with default sampling parameters: 32 deterministic steps with a $2 ^ { \\mathrm { n d } }$ order Heun sampler [20]. For most setups, a pre-trained model is publicly available, and in the remaining cases we train the models ourselves using the official implementation (Appendix B).\n\nWe use two degradations for the guiding model: shorter training time and reduced capacity compared to the main model. We obtain the best results by having both of these enabled. With EDM2-S, for example, we use an XS-sized guiding model that receives $1 / 1 6 ^ { \\mathrm { t h } }$ of the training iterations of the main model. We ablate the relative importance of the degradations as well as the sensitivity to these specific choices in Section 5.1. As the EDM2 networks are known to be sensitive to the guidance weight and EMA length [21], we search the optimal values for each case using a grid search.\n\nTable 1 shows that our method improves FID [12] and $\\mathrm { F D } _ { \\mathrm { D I N O v } 2 }$ [47] considerably. Using the small model (EDM2-S) in ImageNet-512, our autoguidance improves FID from 2.56 to 1.34. This beats the 1.68 achieved by the concurrently proposed $\\mathrm { C F G } + ($ Guidance Interval [24], and is the best result reported for this dataset regardless of the model size. Using the largest model (EDM2-XXL) further improves the record to 1.25. The FDDINOv2 records are similarly improved, with the large model\n\n<html><body><table><tr><td></td><td>Method</td><td></td><td>FID</td><td>w</td><td>EMAm</td><td>EMAg</td><td>FDDINOv2</td><td>w</td><td>EMAm</td><td>EMAg</td></tr><tr><td rowspan=\"13\"></td><td>EDM2-S</td><td>[21]</td><td>2.56</td><td>1</td><td>0.130</td><td></td><td>68.64</td><td>1</td><td>0.190</td><td>一</td></tr><tr><td>+ Classifier-free guidance</td><td>[21]</td><td>2.23</td><td>1.40</td><td>0.025</td><td>0.025</td><td>52.32</td><td>1.90</td><td>0.085</td><td>0.085</td></tr><tr><td>+ Guidance interval</td><td>[24]</td><td>1.68</td><td>2.10</td><td>0.025</td><td>0.025</td><td>46.25</td><td>3.20</td><td>0.085</td><td>0.085</td></tr><tr><td>+ Autoguidance (XS,T/16)Ours</td><td></td><td>1.34</td><td>2.10</td><td>0.070</td><td>0.125</td><td>36.67</td><td>2.45</td><td>0.120</td><td>0.165</td></tr><tr><td>- Same EMA for both</td><td></td><td>1.53</td><td>1.95</td><td>0.050</td><td>0.050</td><td>40.81</td><td>2.25</td><td>0.115</td><td>0.115</td></tr><tr><td>-Reduce training only</td><td></td><td>1.51</td><td>2.20</td><td>0.090</td><td>0.130</td><td>42.27</td><td>2.55</td><td>0.130</td><td>0.170</td></tr><tr><td>-Reduce capacity only</td><td></td><td>2.13</td><td>1.80</td><td>0.120</td><td>0.160</td><td>59.89</td><td>1.90</td><td>0.140</td><td>0.085</td></tr><tr><td>EDM2-XXL</td><td>[21]</td><td>1.91</td><td>1</td><td>0.070</td><td>1</td><td>42.84</td><td>1</td><td>0.150</td><td>1</td></tr><tr><td></td><td>+ Classifier-free guidance</td><td>[21] 1.81</td><td></td><td>1.20</td><td>0.015</td><td>0.015</td><td>33.09</td><td>1.70</td><td>0.015</td><td>0.015</td></tr><tr><td></td><td>+ Guidance interval</td><td>[24]</td><td>1.40</td><td>2.00</td><td>0.015</td><td>0.015</td><td>29.16</td><td>2.90</td><td>0.015</td><td>0.015</td></tr><tr><td></td><td>+ Autoguidance (M,T/3.5)</td><td>Ours</td><td>1.25</td><td>2.05</td><td>0.075</td><td>0.155</td><td>24.18</td><td>2.30</td><td>0.130</td><td>0.205</td></tr><tr><td>+ Autoguidance (XS,T/16)</td><td>EDM2-S,unconditional</td><td></td><td>11.67</td><td>1</td><td>0.145</td><td>1</td><td>209.53</td><td>一</td><td>0.170</td><td>1</td></tr><tr><td>9×59</td><td></td><td>Ours</td><td>3.86</td><td>2.85</td><td>0.070</td><td>0.110</td><td>90.39</td><td>2.90</td><td>0.090</td><td>0.125</td></tr><tr><td rowspan=\"4\"></td><td>RIN</td><td>[18]</td><td>1.23</td><td>一</td><td>0.033</td><td>一</td><td></td><td></td><td></td><td>一</td></tr><tr><td>EDM2-S</td><td>[21]</td><td>1.58</td><td></td><td>0.075</td><td></td><td>58.52</td><td></td><td>0.160</td><td></td></tr><tr><td>+ Classifier-free guidance</td><td></td><td>1.48</td><td>1.15</td><td>0.030</td><td>0.030</td><td>41.84</td><td>1.85</td><td>0.040</td><td>0.040</td></tr><tr><td>+ Autoguidance (XS,T/8)</td><td>Ours</td><td>1.01</td><td>1.70</td><td>0.045</td><td>0.110</td><td>31.85</td><td>2.20</td><td>0.105</td><td>0.175</td></tr></table></body></html>\n\nTable 1: Results on ImageNet-512 and ImageNet-64. The parameters of autoguidance refer to the capacity and amount training received by the guiding model. The latter is given relative to the number of training images shown to the main model $( T )$ . The columns $\\mathrm { E M A } _ { \\mathrm { m } }$ and $\\mathrm { E M A _ { g } }$ indicate the length parameter of the post-hoc EMA technique [21] for the main and guiding model, respectively.\n\n![](images/805575f400df1c284629efedc255c098cf9b71076421abea24b7ca5019abc0d3.jpg)  \nFigure 3: Sensitivity w.r.t. autoguidance parameters, using EDM2-S on ImageNet-512. The shaded regions indicate the min/max FID over 3 evaluations. (a) Sweep over guidance weight $w$ while keeping all other parameters unchanged. The curves correspond to how much the guiding model was trained relative to the number of images shown to the main model. (b) Sweep over guidance weight for different guiding model capacities. (c) Sweep over the two EMA length parameters for our best configuration, denoted with ⋆ in (a) and (b).\n\nlowering the record from 29.16 to 24.18. In ImageNet-64, the improvement is even larger; in this dataset, we set the new record FID and $\\mathrm { F D } _ { \\mathrm { D I N O v } 2 }$ of 1.01 and 31.85, respectively.\n\nA particular strength of autoguidance is that it can be applied to unconditional models as well. While conditional ImageNet generation may be getting close to saturation, the unconditional results remain surprisingly poor. EDM2-S achieves a FID of 11.67 in the unconditional setting, indicating that practically none of the generated images are of presentable quality. Enabling autoguidance lowers the FID substantially to 3.86, and the improvement in $\\mathrm { F D } _ { \\mathrm { D I N O v } 2 }$ is similarly significant.\n\n# 5.1 Ablations\n\nTable 1 further shows that it is beneficial to allow independent EMA lengths for the main and guiding models. When both are forced to use the same EMA, FID worsens from 1.34 to 1.53 in ImageNet-512 (EDM2-S). We also measure the effect of each degradation (reduced training time, capacity) in isolation. If we set the guiding model to the same capacity as the main model and only train it for a shorter time, FID worsens to 1.51. If we instead train the reduced-capacity guiding model for as long as the main model, FID suffers a lot more, to 2.13. We can thus conclude that both degradations are beneficial and orthogonal, but a majority of the improvement comes from reduced training of the guiding model. Notably, all these ablations still outperform standard CFG in terms of FID.\n\n![](images/70f04ea897b0f6e8a990584a54eddec78b1203577f79203ee909a8743054ba24.jpg)  \nFigure 4: Example results for the Tree frog, Palace, Mushroom, Castle classes of ImageNet-512 using EDM2-S. Guidance weight increases to the right; rows are classifier-free guidance and our method.\n\nFigure 3 probes the sensitivity to various hyperparameters. Our best result is obtained by training the guiding model $1 / 1 6 ^ { \\mathrm { t h } }$ as much as the main model, in terms of images shown during training. Further halving the training budget is almost equally good, while doubling the amount of training starts to slowly compromise the results. The results are quite insensitive to the choice of the guidance weight. In terms of the capacity of the guiding model, one step smaller (XS for EDM2-S) gave the best result. Two steps smaller (XXS) was also better than no capacity reduction (S), but started to show excessive sensitivity to the guidance weight. The results are also sensitive to the EMA length, similarly to the original EDM2. Post-hoc EMA [21] allows us to search the optimal parameters at a feasible cost.\n\nWe also explored several other degradations for the guiding model but did not find them to be beneficial. First, we tried reducing the amount of training data used for the guiding model, but this did not seem to improve the results over the baseline. Second, applying guidance interval [24] on top of our method reduced its benefits to some extent, suggesting that autoguidance is helpful at all noise levels. Third, deriving the guiding model from the main model using synthetic degradations did not work at all, providing further evidence that the guiding model needs to exhibit the same kinds of degradations that the main model suffers from. Fourth, we found that if the main model had been quantized, e.g., to improve inference speed, quantizing it to an even lower precision did not yield a useful guiding model.\n\nOne limitation of autoguidance is the need to train a separate guiding model. That said, the additional training cost can be quite modest when using a smaller model and shorter training time for the guiding model. For example, the EDM2-M model trains approximately $2 . 7 \\times$ as fast as EDM2-XXL per iteration, and we train it for 1/3.5 of iterations, so the additional cost is around $+ 1 1 \\%$ . For the EDM2-S/XS pair used in most of our experiments, the added training cost is only $+ 3 . 6 \\%$ .\n\n![](images/a5c72e214350fcf6e70abbad81c4a00fd0b18ee45393456c0dc4cdf10aef2020.jpg)  \nFigure 5: Results for DeepFloyd IF [46] using the prompt “A blue jay standing on a large basket of rainbow macarons”. The rows correspond to guidance weights $w \\in \\{ 1 , 2 , 3 , 4 \\}$ . The leftmost column shows results for CFG and the rightmost for autoguidance (XL-sized model guided by M-sized one). The middle columns correspond to blending between the two. See Appendix A for more examples.\n\n# 5.2 Qualitative results\n\nFigure 4 shows examples of generated images for ImageNet-512. Both CFG and our method tend to improve the perceptual quality of images, guiding the results towards clearer realizations as the guidance weight increases. However, CFG seems to have a tendency to head towards a more limited number of canonical images [24] per class, while our method produces a wider gamut of image compositions. An example is the atypical image of a Palace at $w = 1$ , which CFG converts to a somewhat idealized depiction as $w$ increases. Sometimes the unguided sample contains incompatible elements of multiple possible images, such as the Castle image, which includes a rough sketch of two or three castles of unrelated styles. In this instance, CFG apparently struggles to decide what to do, whereas our method first builds the large red element into a castle, and with increased guidance focuses on the red foreground object. A higher number of possible output images is consistent with a lower FID, implying better coverage of the training data.\n\nIn order to study our method in the context of large-scale image generators, we apply it to DeepFloyd IF [46]. We choose this baseline because multiple differently-sized models are publicly available. Ideally we could have also used an earlier snapshot as the guiding model, but those were not available. DeepFloyd IF generates images as a cascade of three diffusion models: a base model and two superresolution stages. We apply our method to the base model only, while the subsequent stages always use CFG. Figure 5 demonstrates the effect of CFG, our method, and their various combinations. To combine autoguidance with CFG, we extend Equation 3 to cover multiple guiding models as proposed by Liu et al. [27] and distribute the total guidance weight among them using linear interpolation (see Appendix B.2 for details). While CFG improves the image quality significantly, it also simplifies the style and layout of the image towards a canonical depiction. Our method similarly improves the image quality, but it better preserves the image’s style and visual complexity. We hope that using both guiding methods simultaneously will serve as a new, useful artistic control.\n\n# 6 Discussion and Future work\n\nWe have shown that classifier-free guidance entangles several phenomena together, and that a different perspective together with simple practical changes opens up an entire new design space. In addition to removing the superfluous connection to conditioning, this enables significantly better results.\n\nPotential directions for future work include formally proving the conditions that allow autoguidance to be beneficial, and deriving good rules of thumb for selecting the best guiding model. Our suggestion — an early snapshot of a smaller model — is easy to satisfy in principle, but these are not available for current large-scale image generators in practice. Such generators are also often trained in successive stages where the training data may change at some point, causing potential distribution shifts between snapshots that would violate our assumptions.\n\nRecently, several studies [5, 10, 40, 50, 24] have reduced the downsides of CFG by making the guidance weight noise level-dependent. A key benefit from these schedules appears to be the suppression of CFG at high noise levels, where its image quality benefit is overshadowed by the undesirable reduction in variation that is caused by large differences in the content of the differently conditioned distributions. In contrast, autoguidance is not expected to suffer from this problem at high noise levels, as both models target the same distribution. So far we have compared autoguidance only with the interval method [24], which we did not find beneficial in combination. A further study on the various possible combinations, in terms of quantitative performance as well as artistic control, is a natural next step. It could also be interesting to further isolate the origin of the improvement using alternative metrics, such as precision and recall [25], Human Preference Score [51], or PickScore [23].",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n图像生成扩散模型主要关注图像质量、结果的多样性以及结果与给定条件（如类别标签或文本提示）的对齐程度。流行的无分类器引导（CFG）方法使用无条件模型引导条件模型，虽能同时提高提示对齐度和图像质量，但会牺牲多样性，且这些效果相互纠缠，难以控制。这一问题限制了扩散模型在图像生成领域的应用，尤其是对于需要高质量且多样化图像的场景。\\n\\n**方法概述**\\n论文提出了一种名为自动引导（Autoguidance）的方法，通过使用模型自身的一个较小、训练程度较低的版本来引导生成过程，而非使用无条件模型，从而在不牺牲结果多样性的前提下，实现对图像质量的分离控制。\\n\\n**主要贡献与效果**\\n- 提出自动引导方法，在 ImageNet 生成任务中取得显著成果。使用公开可用网络，在 $64×64$ 图像生成中，FID 达到 1.01；在 $512×512$ 图像生成中，FID 达到 1.25。\\n- 该方法适用于无条件扩散模型，大幅提升了其生成图像的质量。如在 EDM2 - S 的无条件设置中，FID 从 11.67 降至 3.86。\\n- 相较于 CFG 方法，自动引导方法能产生更广泛的图像组合，更好地保留图像的风格和视觉复杂性，且在 FID 和 FDDINOv2 指标上均优于 CFG 方法。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n在有限模型容量下，分数匹配会过度强调数据分布的低概率区域，导致生成不现实和破碎的图像。自动引导方法利用同一模型的较弱版本，因其在相同区域会产生更严重的错误，通过测量较强模型与较弱模型预测的差异并放大，可识别并减少较强模型的错误，将样本推向数据分布的核心区域，从而提升图像质量且不牺牲多样性。当两个模型的预测一致时，扰动不显著；当不一致时，差异指示了向更好样本发展的大致方向。\\n\\n**创新点**\\n传统的 CFG 方法使用无条件模型引导条件模型，存在任务差异问题，即无条件和条件去噪器训练的任务不同，可能导致采样轨迹偏离所需的条件分布，产生扭曲的采样轨迹、过度截断和模式丢失等问题，且提示对齐和质量改善效果难以分离控制。而自动引导方法使用主模型本身的较弱版本作为引导模型，避免了任务差异问题，且能实现对图像质量的解耦控制，可应用于无条件合成。同时，该方法在多种合成测试用例以及实际的类条件和文本条件图像合成中验证了有效性。\\n\\n**具体实现步骤**\\n1. 确定主模型 $D_1$ 和引导模型 $D_0$：引导模型 $D_0$ 是主模型 $D_1$ 的较弱版本，可通过限制模型容量和/或训练时间来获得。\\n2. 进行引导生成：在扩散模型的引导过程中，通过外推两个去噪结果 $D_0$ 和 $D_1$ 来实现引导效果，公式为 $D_w(x;σ, c) = wD_1(x;σ, c) + (1 - w)D_0(x;σ, c)$，其中 $w$ 是引导权重。\\n3. 搜索最优参数：由于 EDM2 网络对引导权重和 EMA 长度敏感，使用网格搜索为每个案例搜索最优的引导权重 $w$、主模型的 EMA 长度 $EMA_m$ 和引导模型的 EMA 长度 $EMA_g$。\\n\\n**案例解析**\\n在 2D 玩具示例中，训练一个小规模的去噪器网络在合成数据集上进行条件扩散。未引导的扩散会产生大量分布主体之外的极不可能的样本，对应图像生成中不现实和破碎的图像。而使用自动引导方法，通过使用更小、训练迭代次数更少的 $D_0$ 作为引导模型，样本被拉向分布，且不会系统地丢弃分布的任何部分。此外，在合成退化的受控实验中，只要主模型和引导模型的退化兼容，自动引导方法能在很大程度上消除退化造成的影响。\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文以当前最先进的扩散模型 EDM2 为基线，包括 EDM2 - S 和 EDM2 - XXL 模型，同时对比了无分类器引导（CFG）、引导区间（Guidance Interval）和 RIN 模型等方法。\\n\\n**性能对比**\\n*   **在 [FID] 指标上：** 本文的自动引导方法在 ImageNet - 512 数据集上，使用 EDM2 - S 模型时，FID 从 2.56 降至 1.34，优于 CFG +（引导区间）方法的 1.68；使用 EDM2 - XXL 模型时，进一步将 FID 记录提升至 1.25。在 ImageNet - 64 数据集上，自动引导方法达到了 1.01 的 FID，显著优于基线模型 EDM2 - S (1.58)、CFG (1.48)。在无条件设置下，EDM2 - S 的 FID 从 11.67 降至 3.86。\\n*   **在 [FDDINOv2] 指标上：** 同样得到了显著改善。在 ImageNet - 512 数据集上，使用 EDM2 - XXL 模型时，FDDINOv2 从 29.16 降至 24.18；在 ImageNet - 64 数据集上，达到了 31.85 的记录，优于基线模型 EDM2 - S (58.52)、CFG (41.84)。此外，消融实验表明，允许主模型和引导模型使用独立的 EMA 长度、同时采用减少训练时间和降低容量这两种退化方式对引导模型更有益，且这些消融实验在 FID 指标上均优于标准 CFG 方法。\",\n    \"keywords\": \"### 关键词\\n\\n- 图像生成扩散模型 (Image - Generating Diffusion Models, N/A)\\n- 无分类器引导 (Classifier - Free Guidance, CFG)\\n- 自动引导 (Autoguidance, N/A)\\n- 图像质量控制 (Image Quality Control, N/A)\\n- ImageNet 图像生成 (ImageNet Image Generation, N/A)\\n- 无条件合成引导 (Unconditional Synthesis Guidance, N/A)\"\n}"
}