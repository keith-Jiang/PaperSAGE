{
    "link": "https://arxiv.org/abs/2402.02416",
    "pdf_link": "https://arxiv.org/pdf/2402.02416",
    "title": "Aligner: Efficient Alignment by Learning to Correct",
    "authors": [
        "Jiaming Ji",
        "Boyuan Chen",
        "Hantao Lou",
        "Donghai Hong",
        "Borong Zhang",
        "Xuehai Pan",
        "Tianyi Qiu",
        "Juntao Dai",
        "Yaodong Yang"
    ],
    "institutions": [
        "Peking University"
    ],
    "publication_date": "2024-02-04",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 40,
    "influential_citation_count": 6,
    "paper_content": "# Aligner: Efficient Alignment by Learning to Correct\n\n# Jiaming Ji∗ Boyuan Chen∗ Hantao Lou Donghai Hong Borong Zhang Xuehai Pan Juntao Dai Tianyi Qiu Yaodong Yang†\n\nCenter for AI Safety and Governance, Institute for AI, Peking University\n\nProject Website: https://pku-aligner.github.io\n\n{jiamg.ji,cbylll,lht_pku,donghai.hong}@stu.pku.edu.cn yaodong.yang@pku.edu.cn\n\n# Abstract\n\nWith the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-andplay module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model’s performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of $6 8 . 9 \\%$ in helpfulness and $2 3 . 8 \\%$ in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from $5 5 . 0 \\%$ to $5 8 . 3 \\%$ , surpassing GPT-4 Omni’s $5 7 . 5 \\%$ Win Rate (community report).\n\n# 1 Introduction\n\nThe alignment of LLMs with human intentions and values has recently gained significant attention [1]. Among the various methods, supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) [2, 3] have emerged as practical approaches. SFT leverages human demonstrations to fine-tune LLMs and instruct the model on desired actions, whereas RLHF trains a reward model (RM) based on human preferences and fine-tunes LLMs using feedback signals from the RM through reinforcement learning (RL) methods [4].\n\nDespite the effectiveness of these methods [5, 6, 7, 8, 9] in meeting 3H (helpfulness, harmlessness, and honesty) standards [10], they suffer from challenges such as high training resource consumption and difficulty in ensuring consistent performance [11]. Meanwhile, in real-world scenarios, alignment requirements are dynamically changing [12]. Models may encounter cases outside of alignment training and exhibit undesirable behaviors, which are difficult to address immediately using timeconsuming methods such as SFT and RLHF.\n\n![](images/b19537496cab335b00cdc8157966d4c0786b0e3e56b76349a79b595fc63ba562.jpg)  \nFigure 1: (Left) Architecture of the Aligner module and illustration of its behavior in semantic space. As a plug-and-play module, Aligner stack upon an upstream LLM. The Aligner redistributes initial answers from the upstream model into more helpful and harmless answers, thus aligning the composed LLM responses with human intentions. (Right) Analogy of Aligner as a residual learning enhancer for LLMs in architecture and capabilities. Like a residual block that adds modifications via a shortcut without altering the base structure, the Aligner employs a copy and correct method to improve the original answer. This analogy highlights the Aligner’s dual role in preserving the parameter of the upstream model while enhancing it to align with desired outcomes.\n\nCan we develop an efficient, lightweight, and model-agnostic alignment method?\n\nInspired by residual learning [13], we simplify the alignment process by focusing on copy and correction operations. We introduce an efficient alignment paradigm, the Aligner, without involving any RL processes, as shown in Figure 1. Specifically, Aligner is fine-tuned on a preference dataset to learn the correctional residuals between preferred and non-preferred responses and then stacked on the upstream model to achieve corrected alignment. Here, the upstream LLM refers to models targeted for alignment and is compared to the source model in the RLHF process. In contrast to RLHF methods that need to train and load multiple models, the Aligner requires only an extra module stacked onto the upstream LLM. Moreover, our method’s computational resource demand depends solely on the desired efficacy of the Aligner, not on the parameter size of the upstream LLMs.\n\nFrom the perspective of representation learning [14, 15, 16], Aligner exhibits an interpretable residual behavior. As shown in Figure 4, Aligner decides the degree of reference to the original response and the extent of additional correction based on the quality of the original answers in the early layers, whereas its middle and late layers are used to implement this decision. The mechanism is simpler than directly learning the mapping from input queries to aligned answers. This simplicity indicates that small Aligner can also learn complex correction patterns, demonstrating their capability to steer powerful models with relatively little inference, which further underscores the superiority of our Aligner paradigm.\n\nIn summary, Aligner presents several significant advantages:\n\n• Resource Efficient. Without extra models such as the actor, critic, reward, and reference model, our Aligner is a small model trained on the preference dataset to learn correctional residuals. Specifically, when aligning a 70B LLM, Aligner-7B occupies 11.25 times smaller than DPO and 22.5 times smaller than ${ \\mathrm { R L H F } } ^ { 2 }$ regarding training parameters.\n\n• Plug and Play. The Aligner’s plug-and-play nature and model agnosticism make it ideal for API-based models without parameter access. Once trained, the Aligner can be applied to various upstream LLMs without parameter adjustments. Experiments showed that the Aligner-7B model enhances helpfulness and harmlessness across 11 models, including API-based/open-source safety-aligned/safety-unaligned models. Experiment results demonstrate that the Aligner-7B increased GPT-4’s helpfulness by $1 7 . 5 \\%$ and its harmlessness by $2 6 . 9 \\%$ .\n\n# 2 Aligner\n\nPreliminary: Supervised Fine-Tuning (SFT) SFT aims to finetune the pretrained LLM to generate curated high-quality dataset $\\mathcal { D } _ { \\mathrm { S F T } } = \\{ \\pmb { x } ^ { ( i ) } , \\pmb { y } ^ { ( i ) } \\} _ { i = 1 } ^ { N }$ . The goal is to obtain a model $\\pi _ { \\boldsymbol { \\theta } } ^ { \\mathrm { S F T } }$ with the\n\n$$\n\\operatorname* { m i n i m i z e } _ { \\pmb { \\theta } } \\mathcal { L } ( \\pmb { \\theta } ; \\mathcal { D } _ { \\mathrm { S F T } } ) = - \\mathbb { E } _ { ( \\pmb { x } , \\pmb { y } ) \\sim \\mathcal { D } _ { \\mathrm { S F T } } } [ \\log \\pi _ { \\pmb { \\theta } } ( \\pmb { y } | \\pmb { x } ) ] .\n$$\n\nSimilarly, illustrated in Figure 1, Aligner improves alignment between the model and human intentions by redistributing the model’s answers through conditional generation. In practical implementation, Aligner only needs to make a minor adjustment to the SFT training code (only need to change one line of code), as detailed in Appendix D.\n\nOverall, the whole pipeline of Aligner training can be summarized as follows: Based on a preference dataset, the model is fine-tuned to learn the correctional residuals between preferred and non-preferred responses. After a single training session, this model can be deployed on any model to achieve corrected alignment.\n\nModel Training Based on the above procedures, we have constructed the dataset $\\mathcal { M } \\ =$ $\\{ \\pmb { x } ^ { ( i ) } , \\pmb { y } _ { o } ^ { ( i ) } , \\pmb { y } _ { c } ^ { ( i ) } \\} _ { i = 1 } ^ { N }$ , which $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ represents the user’s query, $\\scriptstyle { \\pmb { y } } _ { o }$ is the original answer, and $\\pmb { y } _ { c }$ is the corrected answer according to established principles. The model training process is relatively straightforward. We train the Aligner, a conditional seq2seq model $\\mu _ { \\phi } ( { \\pmb y } _ { c } | { \\pmb y } _ { o } , { \\pmb x } )$ parameterized by $\\phi$ , to redistribute the preliminary answers $\\scriptstyle { \\pmb { y } } _ { o }$ to the aligned answer $_ { y _ { c } }$ . Demonstrated in Figure 1, the composed answer generation process for aligned answers based on the upstream LLM $\\pi _ { \\boldsymbol { \\theta } }$ is:\n\n$$\n\\begin{array} { r } { \\pi ^ { \\prime } ( y _ { c } | \\mathbf { x } ) = \\sum _ { y _ { k } } \\mu _ { \\phi } ( y _ { c } | \\mathbf { y } _ { k } , \\pmb { x } ) \\pi _ { \\theta } ( y _ { k } | \\mathbf { x } ) \\geqslant \\mu _ { \\phi } ( y _ { c } | \\mathbf { y } _ { o } , \\pmb { x } ) \\pi _ { \\theta } ( y _ { o } | \\mathbf { x } ) , } \\end{array}\n$$\n\nwhere $\\pmb { y } _ { k }$ is a possible answer generated by upstream LLM $\\pi _ { \\boldsymbol { \\theta } }$ . By calculating empirical loss on the whole dataset $\\mathcal { M }$ , we can get equation (3) from equation (2):\n\n$$\n- \\mathbb { E } _ { \\mathcal { M } } [ \\log \\pi ^ { \\prime } ( y _ { c } | \\boldsymbol { x } ) ] \\leqslant - \\mathbb { E } _ { \\mathcal { M } } [ \\log \\mu _ { \\phi } ( y _ { c } | y _ { o } , \\boldsymbol { x } ) ] - \\mathbb { E } _ { \\mathcal { M } } [ \\log \\pi _ { \\theta } ( y _ { o } | \\boldsymbol { x } ) ] .\n$$\n\nThe second term in equation (3) is not related to the Aligner parameter and the training objective for Aligner can be derived as equation (4):\n\n$$\n\\operatorname* { m i n i m i z e } _ { \\phi } \\mathcal { L } _ { \\mathrm { A l i g n e r } } ( \\phi , \\mathcal { M } ) = - \\mathbb { E } _ { \\mathcal { M } } \\left[ \\log \\mu _ { \\phi } \\left( y _ { c } | y _ { o } , x \\right) \\right] .\n$$\n\nBy optimizing this objective, we actually optimize the upper bound of the SFT training objective, which ensures that $_ { y _ { c } }$ is effectively learned. It is worth noting that Aligner does not require access to the parameters of the upstream LLM $\\pi _ { \\boldsymbol { \\theta } }$ during both training and inference phases. Aligner takes the user’s query $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ and the initial answer $\\scriptstyle { \\boldsymbol { y } } _ { o }$ generated by the upstream LLM $\\pi _ { \\boldsymbol { \\theta } }$ , then generates the answer $_ { y _ { c } }$ which is better aligned with human values. Improving existing answers $\\scriptstyle { y _ { o } }$ allows Aligner to focus on how to align with human values rather than how to answer the given query directly. This significantly reduces the requirements on our model capacity, allowing us to achieve the expected alignment performance with only a small model.\n\nAligner’s Training Strategy: Residual Correction We develop an optimized training strategy, termed Residual Correction, which leverages the semantic correctional residuals between answers $( \\pmb { y } _ { o } )$ and corrections $( { \\boldsymbol { y } } _ { c } )$ , as shown in Figure 1. Specifically, we construct a Q-A-A dataset using partial training data to train an identity Aligner initially, a process we term warm-up. Subsequently, we utilize the Q-A-C dataset for training, building upon the identity Aligner. The details of our experiments on a 50K training dataset are shown in Section 3.3. Outside the alignment field, ResNet [13] also uses a similar approach to mitigate the vanishing gradient problem caused by increased neural network depth.\n\nResource Analysis between Aligner and RLHF/DPO Compared to RLHF and DPO [6], Aligner shows notable advantages in training resource requirements. Regarding training resources, Aligner7B is more efficient than other methods under similar performance conditions. Specifically, with a 7B source model, DPO requires 1.125 times, and RLHF 2.25 times more resources than Aligner. Additionally, as the source model’s scale increases, the resource demands for other methods rise sharply. For a 70B model, DPO needs 11.25 times, and RLHF 22.5 times more resources than Aligner. However, since Aligner is insensitive to these changes, its training resource requirements remain constant regardless of the source model’s scale, indicating that Aligner is an efficient and lightweight alignment paradigm.\n\nTable 1: Performance of Aligner models. It is shown that Aligner achieves significant performances in all the settings. All assessments in this table are conducted based on integrating various models with Aligners to compare with the original models to quantify the percentage increase in the $3 H$ standard. When integrated and assessed in conjunction with various upstream models, the Aligner requires only a single training session (i.e., the Aligner can operate in a zero-shot manner and enhance the performance of all upstream models.)   \n\n<html><body><table><tr><td colspan=\"2\"></td><td colspan=\"2\">Helpful</td><td colspan=\"4\">Harmless</td><td>Honest</td></tr><tr><td>Aligner</td><td>Upstream LLM</td><td>E-Dialogue</td><td>DialogSum</td><td colspan=\"2\">Beavertails</td><td colspan=\"2\">HarmfulQA</td><td>TruthfulQA</td></tr><tr><td></td><td></td><td>Empathy ↑</td><td>Reasoning ↑</td><td>Helpful ↑</td><td>Harmless ↑</td><td>Helpful ↑</td><td>Harmless ↑</td><td>Reliable↑</td></tr><tr><td rowspan=\"14\">2B</td><td>GPT-4</td><td>26.0%</td><td>2.3%</td><td>8.0%</td><td>28.6%</td><td>12.5%</td><td>29.2%</td><td>-0.5%</td></tr><tr><td>GPT-3.5</td><td>26.3%</td><td>3.3%</td><td>3.1%</td><td>7.6%</td><td>3.6%</td><td>4.4%</td><td>0.7%</td></tr><tr><td>Claude 2</td><td>83.1%</td><td>6.0%</td><td>38.3%</td><td>15.1%</td><td>48.0%</td><td>14.4%</td><td>0.7%</td></tr><tr><td>Beaver-7B</td><td>95.3%</td><td>60.7%</td><td>9.9%</td><td>12.1%</td><td>7.8%</td><td>7.6%</td><td>5.4%</td></tr><tr><td>Alpaca-7B</td><td>97.7%</td><td>58.5%</td><td>5.8%</td><td>45.0%</td><td>22.6%</td><td>65.3%</td><td>10.0%</td></tr><tr><td>Vicuna-7B</td><td>44.9%</td><td>58.5%</td><td>16.9%</td><td>15.8%</td><td>17.7%</td><td>27.1%</td><td>4.9%</td></tr><tr><td>Vicuna-13B</td><td>53.9%</td><td>24.0%</td><td>19.4%</td><td>14.9%</td><td>17.1%</td><td>16.1%</td><td>7.6%</td></tr><tr><td>Vicuna-33B</td><td>45.7%</td><td>39.3%</td><td>24.4%</td><td>52.4%</td><td>26.9%</td><td>32.6%</td><td>5.1%</td></tr><tr><td>Llama2-7B-Chat</td><td>88.1%</td><td>69.5%</td><td>25.4%</td><td>7.2%</td><td>11.3%</td><td>25.9%</td><td>3.9%</td></tr><tr><td>Llama2-13B-Chat</td><td>85.3%</td><td>53.4%</td><td>18.4%</td><td>12.3%</td><td>18.6%</td><td>27.6%</td><td>1.2%</td></tr><tr><td>Llama2-70B-Chat</td><td>86.7%</td><td>47.9%</td><td>17.8%</td><td>5.5%</td><td>21.3%</td><td>7.2%</td><td>10.8%</td></tr><tr><td>Average</td><td>66.6%</td><td>36.4%</td><td>17.0%</td><td>19.7%</td><td>18.8%</td><td>23.4%</td><td>4.5%</td></tr><tr><td rowspan=\"14\"></td><td>GPT-4</td><td>27.7%</td><td>6.2%</td><td>18.6%</td><td>25.8%</td><td>16.3%</td><td>28.0%</td><td>-1.2%</td></tr><tr><td>GPT-3.5</td><td>25.6%</td><td>6.8%</td><td>9.3%</td><td>9.3%</td><td>8.4%</td><td>7.0%</td><td>0.0%</td></tr><tr><td>Claude 2</td><td>90.4%</td><td>10.4%</td><td>58.4%</td><td>30.3%</td><td>69.4%</td><td>42.1%</td><td>2.0%</td></tr><tr><td>Beaver-7B</td><td>98.3%</td><td>83.5%</td><td>21.9%</td><td>12.0%</td><td>8.9%</td><td>6.0%</td><td>10.3%</td></tr><tr><td>Alpaca-7B</td><td>99.4%</td><td>78.7%</td><td>34.9%</td><td>47.0%</td><td>38.2%</td><td>70.7%</td><td>11.8%</td></tr><tr><td>Vicuna-7BB</td><td>95.%</td><td>73.5%</td><td>26.4%</td><td>15.9%</td><td>12.0%</td><td>29.3%</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>2.7%</td></tr><tr><td>Vicuna-33B</td><td>89.3%</td><td>58.5%</td><td>51.0%</td><td>55.9%</td><td>-1.0%</td><td>33.6%</td><td>3.2%</td></tr><tr><td>Llama2-7B-Chat</td><td>95.6%</td><td>98.8%</td><td>19.9%</td><td>7.4%</td><td>-5.7%</td><td>22.1%</td><td>1.5%</td></tr><tr><td>Llama2-13B-Chat</td><td>96.6%</td><td>70.8%</td><td>20.1%</td><td>10.3%</td><td>15.5%</td><td>28.6%</td><td>1.7%</td></tr><tr><td>Llama2-70B-Chat</td><td>95.0%</td><td>70.1%</td><td>5.2%</td><td>2.4%</td><td>-6.6%</td><td>4.1%</td><td>9.1%</td></tr><tr><td>Average GPT-4</td><td>82.5%</td><td>55.2%</td><td>27.6%</td><td>21.2%</td><td>16.1%</td><td>26.4%</td><td>4.0%</td></tr><tr><td>Claude 2</td><td>42.6%</td><td>9.7%</td><td>33.9%</td><td>25.1%</td><td>25.1%</td><td>20.1%</td><td>-0.2%</td></tr><tr><td rowspan=\"14\">13B</td><td>GPT-3.5</td><td>43.7%</td><td>15.6%</td><td>15.1%</td><td>10.9%</td><td>7.6%</td><td>7.7%</td><td>0.5%</td></tr><tr><td></td><td>90.6%</td><td>17.2%</td><td>50.0%</td><td>30.0%</td><td>45.9%</td><td>28.6%</td><td></td></tr><tr><td>Beaver-7B</td><td>98.1%</td><td>87.6%</td><td>14.2%</td><td>19.1%</td><td>8.0%</td><td>11.6%</td><td>0.5%</td></tr><tr><td>Alpaca-7B</td><td>99.0%</td><td>82.9%</td><td>8.5%</td><td>53.4%</td><td>3.4%</td><td>75.9%</td><td>13.0%</td></tr><tr><td>Vicuna-7B</td><td>96.3%</td><td>78.5%</td><td>19.1%</td><td>24.0%</td><td>19.5%</td><td>31.0%</td><td>16.9%</td></tr><tr><td>Vicuna-13B</td><td>95.9%</td><td>58.7%</td><td></td><td>26.7%</td><td>30.9%</td><td>18.9%</td><td>6.6%</td></tr><tr><td></td><td></td><td></td><td>31.8%</td><td>63.3%</td><td></td><td></td><td>7.1%</td></tr><tr><td>Vicuna-33B</td><td>90.0%</td><td>65.9%</td><td>33.3%</td><td></td><td>7.3%</td><td>33.3%</td><td>6.1%</td></tr><tr><td>Llama2-7B-Chat Llama2-13B-Chat</td><td>96.0% 95.4%</td><td>99.1%</td><td>13.5%</td><td>4.6% 10.6%</td><td>12.6% 30.7%</td><td>32.3%</td><td>4.2%</td></tr><tr><td></td><td></td><td>73.1%</td><td>16.7%</td><td></td><td></td><td>35.0%</td><td>1.0%</td></tr><tr><td>Llama2-70B-Chat Average</td><td>94.6% 85.6%</td><td>69.2% 59.8%</td><td>10.6% 22.4%</td><td>1.9% 24.5%</td><td>6.3% 17.9%</td><td>7.6% 27.4%</td><td>10.3% 6.0%</td></tr></table></body></html>\n\n# 3 Experiments\n\nIn this section, we assess the effectiveness of Aligner modules in the 3H (Helpful, Harmless, Honest) evaluation metrics and configurations. For detailed training parameters, please see Appendix D.\n\n# 3.1 Experiment Setup\n\nPreference Datasets We utilize two open-source preference datasets, HH-RLHF [5] and PKUSafeRLHF [17] as our preference datasets. Considering that the preference pairs in PKU-SafeRLHF are generated solely by Alpaca-7B, we additionally construct a 50K preference dataset based on these two preference datasets. The questions in this dataset are sourced from HH-RLHF, PKU-SafeRLHF, and so on, resulting in 27K queries for subsequent answers and corrected answer generation. The original answers are generated using various open-source models, including Alpaca-7B [3], Vicuna(7B,13B,33B) [18], Llama2-(7B,13B)-Chat [19], and Alpaca2-(7B,13B)3. We use GPT-4, Llama2- 70B-Chat, and human annotators to revise the answers in the above Q-A dataset. These revisions are based on well-defined principles, establishing constraints for training the seq2seq model. These principles are aimed at effectively extending to the characteristics we wish LLMs to embody. We focus on the 3H dimensions of LLMs (helpfulness, harmlessness, and honesty) [10]. For those answers that conform to these fundamental principles, we retain the original answers. Figure 2 (a) visually shows the distribution shift before and after the data correction, thereby demonstrating the impact of the revision process on the dataset. More details about the construction of Q-A Datasets can be found in Appendix D.1.\n\n![](images/c3f0ef5c839a93faf5bff2b29018de8736c9feab8d275bb4d80d58d859d3496a.jpg)  \nFigure 2: Distribution of helpfulness and harmlessness scores. (a) The distribution shift in preferred and dis-preferred answers in the training dataset; (b) redistribution shift of Aligner-7B, based on upstream models such as GPT-4 (b1), Alpaca-7B (b2) and Llama2-70B-Chat (b3) in the evaluation dataset. Our findings include: (1) Preferred answers in the training dataset surpasses original answers in both helpfulness and harmlessness; (2) The refuse-to-answer pattern of GPT-4 created an area of overcorrected answers where both helpful and harmless scores are low, and Aligner7B improved these answers by providing additional information and corrections. (3) The Alpaca-7B model, which lacks alignment, had its answers significantly corrected by our Aligner-7B, increasing both scores. (4) The Llama2-70B-Chat model, already aligned with a higher average safety score than the training dataset corrections, benefits from Aligner-7B corrections, significantly enhancing helpfulness while maintaining the harmless score.\n\nModels and Evaluation Datasets We trained the Aligner on three model sizes, specifically based on Gemma-2B [20] and Llama2 (7B, 13B) [19]. To assess the Aligner module, we utilize five datasets: E-Dialogue [21], DialogSum [22], BeaverTails [17], HarmfulQA [23], and TruthfulQA [24]. More details can be found in Appendix B.1. Our evaluation focuses on two model categories: API-based models (e.g., GPT-4 [25], Claude 2 [26]) and Open-Source models (Llama2-(7B, 13B, 70B)-Chat [19]; Vicuna-(7B, 13B, 33B) [18]; Alpaca-7B [3]; Beaver-7B [27]). Notably, the Llama2 and Beaver models have undergone safety alignment processing.\n\nEvaluation Metrics Our evaluation hinges on three key dimensions: helpfulness, harmlessness, and honesty. The independent characteristics of these dimensions provide a comprehensive perspective on the answers, allowing us to balance information quality with safety and ethical considerations in the evaluation of an answer’s quality. Initial answers are generated by open-source and upstream models, which the Aligner refines to yield corrected answers. More details and examples can be found in Appendix B.\n\n# 3.2 Experiment Results\n\nAs shown in Table 1, we employ Aligners of various sizes, significantly improving the performance of all 11 upstream models with only one training session. Under the 3H standard, Aligner-7B showcases\n\nan average enhancement of $2 1 . 9 \\%$ in helpfulness and $2 3 . 8 \\%$ in harmlessness across the models.   \nRemarkably, Aligner-7B can boost GPT-4’s helpfulness by $1 7 . 5 \\%$ and harmlessness by $2 6 . 9 \\%$ .\n\nPerformance on the 3H Standard Aligner keeps the upstream model unchanged, offering adaptability in Aligner model sizing based on available resources. We evaluated Aligner’s effectiveness using five datasets according to the 3H standard. Experiment results show that Aligner significantly enhances the upstream model’s performance across various parameter scales. Particularly, Aligner-7B markedly enhanced the GPT-4 model’s performance across all five dimensions. In the reasoning dimension, with an increase in parameters, Aligner boosts the upstream model’s capability, showcasing the Scaling Laws [28] characteristics. Notably, Aligner excelled in the empathy dimension, further evidencing its efficiency in redistributing the upstream model’s pattern distribution. To detect whether Aligner would generate known false content due to misunderstandings, similar to [19], we use TruthfulQA [24] to measure the reliability of the outputs generated by Aligner in terms of factualness and common sense. The results demonstrate that Aligner does not add extra hallucination information while correcting the upstream model.\n\nAssessing Aligner’s Stack on Safety-Aligned Models Llama2-Chat models, with their multi-stage alignment process (pre-training, SFT, RLHF), and Beaver, finetuned via Safe RLHF [27], both show modest safety improvements with Aligner. The primary achievement of Aligner is its ability to amplify helpfulness, especially in models predisposed to avoid risky responses. By re-distributing these overly conservative answers, Aligner significantly boosts overall helpfulness. This enhancement in helpfulness is visually represented in Figure 2, showing a rightward shift in Llama2-70B-Chat’s answer distribution under the influence of Aligner-7B, indicating improved helpfulness on a strong safety foundation.\n\n# 3.3 Ablation Study\n\n![](images/8d5ca029f3d7297c1423ffc11fb92d9b56c64a58ab64b53528c45ba7c196430e.jpg)  \nFigure 3: Ablation study of different identity mapping proportions. We first trained an identity Aligner for identity mapping, followed by extensive residual Q-A-C learning based on this Aligner. Specifically, we formed the Q-A-A dataset by extracting partial data from the training dataset in proportions of $2 \\%$ , $10 \\%$ , $20 \\%$ , and $50 \\%$ .\n\nAblation on Identity Mapping To verify the effectiveness of different warm-up proportions, we conducted experiments using two representative datasets: BeaverTails and HarmfulQA. As shown in Figure 3, the warm-up step aids the Aligner by initially helping the Aligner learn identity mapping, thus improving the final performance. Moreover, the results further reveal that the effectiveness of the warm-up phase peaks when the proportion is 10k to 50k. However, determining the specific data proportion for warm-up is challenging and requires more training resources.\n\nComparison to Self-Refine, Critique Methods Constitutional AI (CAI) [29], Self-Critique [30], and Self-Refine [31], primarily utilize the self-critiquing and refining capabilities of LLMs to enhance their performance. We employ CAI prompts solely during the inference time of LLMs to encourage self-revision of their answers. As demonstrated in Table 2, our method, Aligner, outperforms the baseline considering both helpfulness and harmlessness dimensions. Additionally, baseline methods typically require multiple dialogue iterations and extended context windows for prompt insertion and ongoing self-correction. This could result in longer inference times and considerable consumption of context window length. For more detailed information and analysis, please refer to Appendix B.5.\n\nTable 2: Ablation study of Aligner’s effectiveness against CAI and Self-Critique. Experiment results revealed that Aligner surpasses these baselines in helpfulness and harmlessness metrics.   \n\n<html><body><table><tr><td>Model</td><td>Metrics</td><td>CAI w/o training</td><td>Self-Critique</td><td>Aligner-7B</td></tr><tr><td rowspan=\"2\">GPT-4</td><td>Helpfulness</td><td>+20.01%</td><td>+26.56%</td><td>+17.47%</td></tr><tr><td>Harmlessness</td><td>+9.65%</td><td>+15.30%</td><td>+26.88%</td></tr><tr><td rowspan=\"2\">Alpaca2-7B</td><td>Helpfulness</td><td>+20.00%</td><td>+30.07%</td><td>+36.55%</td></tr><tr><td>Harmlessness</td><td>+24.08%</td><td>+14.36%</td><td>+58.86%</td></tr><tr><td rowspan=\"2\">Beaver-7B</td><td>Helpfulness</td><td>+5.00%</td><td>+12.80%</td><td>+15.40 %</td></tr><tr><td>Harmlessness</td><td>+7.70%</td><td>-11.6%</td><td>+9.00%</td></tr><tr><td rowspan=\"2\">Llama2-13B-Chat</td><td>Helpfulness</td><td>-0.5%</td><td>+15%</td><td>+17.8%</td></tr><tr><td>Harmlessness</td><td>+27.4%</td><td>+11.1%</td><td>+19.45%</td></tr></table></body></html>\n\nPerformance of Aligner on the Various Preference Datasets To demonstrate the independence of Aligner from specific datasets, we utilized various open-source RLHF preference datasets. Specifically, we trained on HH-RLHF [5], PKU-SafeRLHF [17, 27] and Ultra-Feedback [32] datasets and compared Aligner with SFT, RLHF, and DPO. After fine-tuning Alpaca-7B with SFT, RLHF, and DPO, we compare these models against the original Alpaca-7B corrected by Aligner. The experiment results (as shown in Table 3) indicate that Aligner’s performance in enhancing the original model’s capabilities is comparable to, or exceeds, that of the baseline methods. Notably, models finetuned with RLHF or DPO tend to generate either conservative answers or fail to recognize dangers while adding helpful information explicitly. Importantly, training with RLHF or DPO methods requires optimizing significantly more models and consuming more training resources than just training an Aligner, e.g., for a 70B model, DPO needs 11.25 times and RLHF 22.5 times more resources.\n\nTable 3: Aligner trained on different preference datasets. The experimental results show that Aligner enhances the original model’s capabilities, performing on par with or surpassing baseline methods. Furthermore, these results are consistent across different preference and correction datasets.   \n\n<html><body><table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Q-A-C Datasets</td><td colspan=\"2\">PKU-SafeRLHF</td><td colspan=\"2\">HH-RLHF</td><td colspan=\"2\">Ultra-Feedback</td></tr><tr><td>Helpful</td><td>Harmless</td><td>Helpful</td><td>Harmless</td><td>Helpful</td><td>Harmless</td><td>Helpful</td><td>Harmless</td></tr><tr><td>Alignervs. SFT</td><td>+23.1%</td><td>+0.4%</td><td>=</td><td>=</td><td>=</td><td></td><td>=</td><td></td></tr><tr><td>Alignervs.RLHF</td><td>+24.4%</td><td>+21.9%</td><td>+8.7%</td><td>+8.8%</td><td>+9.6%</td><td>+3.4%</td><td>+25.47%</td><td>+13.13%</td></tr><tr><td>Alignervs.DPO</td><td>+49.1%</td><td>+0.1%</td><td>+33.3%</td><td>+27.0%</td><td>+5.6%</td><td>+30.9%</td><td>+27.21%</td><td>+6.12%</td></tr></table></body></html>\n\n# 3.4 Interpretability Experiments\n\nWhen performing the experiments above, we observed the correction paradigm of Aligner: the correction behavior is not a binary decision between correction and copying. Instead, it follows a conditional generation paradigm, where the degree of reference to the original response and the extent of additional correction depends on the quality of the original answers. To demonstrate that Aligner has learned this correction paradigm as a representation, we conduct the experiment based on representation engineering [14] and activation steering [33, 34, 15]. Specifically, we perform representation extraction and Linear Artificial Tomography (LAT) scan to the Llama2-7B based on the Aligner module. We then utilize the extracted representation to control the Aligner’s generation.\n\nThe results from the representation control experiment indicate that the ratio of adding or subtracting the representation vector in the Aligner activation will significantly affect the magnitude of correction, ranging from directly copying the original response to substantially increasing the extent of normal correction. This provides strong evidence that Aligner has internalized the correction paradigm as a representation. Furthermore, the LAT scan further shows that Aligner decides the degree of correction in its early layers based on the quality of the original response, and after that, it focuses on completing the correction in its middle and late layers. For more details of these experiments, see Appendix B.6.\n\n-25 -25 1 123.50505 □   \n-15 0 -20 -10   \n-5 2 -5   \n0 0 0.5   \n5 10 15 5 10 15 1.00 0.75 0.50 0.25 0.00 0.25 0.50   \nToken Position Representation Vector Coefficient Token Position   \n(a) LAT Neural Activity: Correction (b) LAT Neural Activity: Copy (c) Effect of Correction Representation Control\n\n# 4 Multi-round RLHF training via Aligner\n\nIn this section, we aim to show that, due to its efficient and plug-and-play features, Aligner can play a crucial role in the multi-round RLHF/DPO pipeline, as illustrated in Figure 5. Typical multi-round pipeline often suffers from reward collapse because the preference dataset used for reward modeling may deviate from the actual answer distribution of the upstream model [35]. This error accumulates over multiple rounds, leading to significant deviations in the model’s final results. Additionally, error accumulation may cause reward over-optimization in certain directions, e.g., generating longer responses irrespective of safety. The involvement of Aligner can help mitigate the problem.\n\nUser Prompt Round I User Prompt Round II User Prompt Round III   \nModel RLHF/DPO Model+ RLHF/DPO Model++ RLHF/DPO > > >   \nResponse A 𝒟pr!efeSreynctehedtiactasets Response A 𝒟pr\"efeSreynctehedtiactasets Response A 𝒟pr#efeSreynctehedtiactasets ↑   \nAligner1 Response A\\* Aligner2 Response A\\* Aligner3 Response A\\*\n\nAs shown in Figure 5, you can use the Aligner (which is trained using the original preference dataset for the next round of RLHF) to refine the upstream model response $A$ into response $A ^ { * }$ , and $( Q , A , A ^ { * } )$ pairs can be a new preference dataset for training in the next round of RLHF or DPO. This paradigm brings many advantages:\n\n• The Aligner inherits the feature of transferring from the dispreferred distribution to the preferred distribution in the preference dataset.   \n• Aligner modifies the upstream model to produce better answers, bringing the distribution of the resulting preference dataset closer to the answer distribution of the upstream model. This effectively mitigates the reward model collapse problem caused by out-of-distribution (OOD) preference datasets.\n\n• The Aligner serves as a synthetic data generator, providing an efficient and repeatable method for constructing preference datasets.\n\nWe conducted three rounds of RLHF and DPO on Alpaca2-7B using the three-round preference dataset from PKU-SafeRLHF [27]. Following this, we trained three rounds of Aligners with the same three-round preference datasets, which were then employed to refine the upstream model and generate new preference datasets. These synthetic preference datasets were subsequently used to fine-tune the upstream model. As illustrated in Figure 6, by aggregating Aligner, Aligner-corrected new pref\n\n![](images/d83b9f0e7a0e156d05bde36f28a6f80ca12aa76d88db233a06416fc95226b652.jpg)  \nFigure 6: Multi-round refinement through Aligner.\n\nerence datasets can effectively enhance two key metrics: improving the model’s safety while ensuring a monotonic increase in helpfulness with each round. In contrast, a typical multi-round RLHF/DPO pipeline only enhances utility, leaving the responses unsafe.\n\n# 5 Related Work\n\nReinforcement Learning from Human Feedback RLHF aims to align LLMs with human preferences [36, 2], utilizing RL algorithms [4] to train policy models, specifically LLMs, to maximize cumulative rewards from RMs. The RLHF approach involves the distributed training of various models [11] and the annotations by human experts, presenting operational challenges. Consequently, recent research has focused on reducing [37, 38] or eliminating [6] reliance on RMs, aiming to simplify the RLHF process. Simultaneously, [5, 39] employs advanced AI models for data annotation, further streamlining the RLHF process and cutting costs. In contrast to RLHF methods that require several models, Aligner only requires a constrained seq2seq model to meet the alignment objective. Aligner is distinguished by its plug-and-play nature and indifference to specific models and parameters, making it ideal for API-based models without parameter access.\n\nInference-time Methods These methods customize LLMs without requiring access to their internal parameters [40, 41, 7], proving especially useful for extremely large models or those available through APIs. However, most of these methods are sensitive to the upstream model. IPA [7] uses a lightweight adapter policy to multiply the next-token probabilities based on the upstream model during the decoding time. However, IPA needs to access the model’s output logit distribution. [8] enhances and refines user prompts to better suit the model, thereby facilitating more comprehensive contextual understanding for inference, similar to in-context learning (ICL) [42, 43]. [44] employs a smaller model to select the best response from several responses generated by the upstream model without fine-tuning upstream models, akin to the BoN (Best of N) selector [45, 46]. In this work, we introduce Aligner—a model-agnostic alignment module designed for seamless integration. Requiring only a single training session, Aligner can align 11 types of upstream models, significantly enhancing their performance according to 3H standards.\n\nSelf-Refinement LLMs do not always generate the coherent output on their first try. Self-refinement methods address this by iteratively improving outputs through self-generated feedback, bypassing additional supervision [47, 48, 49]. For example, SELF-DEBUGGING [50] allows LLMs to debug their predictions via few-shot examples, while [30] found that self-critiquing can expose output weaknesses that aid in fine-tuning, with larger models performing especially well in critique tasks. However, these methods typically depend on a single model’s ability to refine itself. Our work instead uses a separate model, Aligner, which can refine outputs from other models (e.g., 70B model, GPT-4), achieving robust weak-to-strong generalization [51]. This approach bypasses the limitations of smaller models and saves computational resources otherwise spent on self-critiquing. Additionally, by combining Aligner with an external critique model, future iterations could further enhance performance.\n\n# 6 Conclusion\n\nWe introduce the Aligner, an efficient, lightweight, and model-agnostic approach to align LLMs. Without the need for additional components such as the actor, critic, reward models, and others, Aligner demonstrates a significant increase in computational efficiency. Under the 3H standard, Aligner-7B showcases an average enhancement of $6 8 . 9 \\%$ in helpfulness and $2 3 . 8 \\%$ in harmlessness across the models. Remarkably, Aligner-7B can boost GPT-4’s helpfulness by $1 7 . 0 \\%$ and harmlessness by $2 6 . 9 \\%$ . In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo (04/09) improved its LC Win Rate [52] from $5 5 . 0 \\%$ to $5 8 . 3 \\%$ , surpassing GPT-4 Omni’s $5 7 . 5 \\%$ Win Rate (community report).\n\n# 6.1 Limitations and Future Work\n\nIn contrast to directly finetuning LLMs, Aligner employs an external module, which is ideal for models with inaccessible original parameters. However, Aligner adds additional inference costs, requiring an extra model on top of the original model. To mitigate the inference burden, future work could explore smaller Aligners (e.g., 0.5B) and streamlining Aligner’s corrections. We aim to enhance LLM alignment using the Aligner module, aiming for increased conciseness, efficiency, and interpretability. Future research will focus on enhancing Aligner’s versatility in challenging contexts like multi-turn dialogues and developing Control Aligner for domain-specific alignment with precise instructions. Moreover, unlike RLHF’s segmented approach, its end-to-end structure provides valuable insights into the alignment process for LLMs.\n\n# 6.2 Ethics and Impact\n\nThe Aligner dataset will be released under the CC BY-NC 4.0 license. This dataset integrates Q-A data from open-source and API-based models, with answers revised to meet the 3H (helpfulness, harmlessness, and honesty) standards [10]. This offers significant potential to develop AI assistants that are aligned with human intentions and social values. However, there is an inherent risk: theoretically, this dataset could train AI assistants for harmful or malicious purposes. As the Aligner dataset’s creators, we are dedicated to fostering beneficial and safe AI technology and strongly oppose any misuse that could hinder human progress. We strongly condemn any malicious use of the Aligner dataset and advocate for its responsible and ethical use.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n当前大语言模型（LLMs）对齐方法复杂，训练资源消耗高，难以适应快速迭代的部署场景，且在实际应用中难以应对动态变化的对齐需求，无法及时解决模型可能出现的不良行为。解决高效、轻量级且与模型无关的对齐方法问题，对于推动大语言模型在实际场景中的广泛应用和持续优化具有重要意义。\\n\\n**方法概述**\\n论文提出了Aligner，这是一种新颖的对齐范式，通过小模型学习首选和非首选答案之间的校正残差，作为与模型无关的即插即用模块，可直接应用于各种开源和基于API的模型，实现模型与人类意图和价值观的对齐。\\n\\n**主要贡献与效果**\\n- 资源高效：与RLHF和DPO相比，在类似性能条件下，Aligner - 7B训练资源需求更低。例如，对齐70B LLM时，Aligner - 7B的训练参数比DPO小11.25倍，比RLHF²小22.5倍。\\n- 提升模型性能：Aligner - 7B在测试的LLMs中平均使有用性提高了21.9%，无害性提高了23.8%，同时有效减少了幻觉。还能将GPT - 4的有用性提高17.5%，无害性提高26.9%。\\n- 超越其他模型：在Alpaca - Eval排行榜上，将Aligner - 2B堆叠在GPT - 4 Turbo上，其LC胜率从55.0%提高到58.3%，超过了GPT - 4 Omni的57.5%胜率。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\nAligner的核心思想基于残差学习，通过学习答案和校正之间的语义校正残差，将初始答案重新分配为更符合人类价值观的答案。它在早期层根据原始答案的质量决定参考原始响应的程度和额外校正的程度，中间和后期层用于实现这一决策，这种机制比直接学习从输入查询到对齐答案的映射更简单，使得小模型也能学习复杂的校正模式。\\n\\n**创新点**\\n先前的对齐方法如监督微调（SFT）和基于人类反馈的强化学习（RLHF）存在训练资源消耗高、难以确保一致性能等问题，且需要训练和加载多个模型。而Aligner不需要额外的模型（如actor、critic、reward和reference model），是一个小模型，对上游模型的参数大小不敏感，训练资源需求恒定，具有即插即用和与模型无关的特点，适用于基于API的模型。\\n\\n**具体实现步骤**\\n1. 构建数据集：基于偏好数据集构建数据集 $\\mathcal{M} = \\{ \\pmb { x } ^ { ( i ) }, \\pmb { y } _ { o } ^ { ( i ) }, \\pmb { y } _ { c } ^ { ( i ) } \\} _ { i = 1 } ^ { N }$，其中 $\\pmb { x }$ 表示用户查询，$\\pmb { y } _ { o }$ 是原始答案，$\\pmb { y } _ { c }$ 是校正后的答案。\\n2. 模型训练：训练条件序列到序列模型 $\\mu _ { \\phi } ( { \\pmb y } _ { c } | { \\pmb y } _ { o }, { \\pmb x } )$，通过优化目标 $\\operatorname* { m i n i m i z e } _ { \\phi } \\mathcal { L } _ { \\mathrm { A l i g n e r } } ( \\phi, \\mathcal { M } ) = - \\mathbb { E } _ { \\mathcal { M } } \\left[ \\log \\mu _ { \\phi } \\left( y _ { c } | y _ { o }, x \\right) \\right]$ 来学习校正残差。\\n3. 训练策略：采用残差校正的训练策略，先使用部分训练数据构建Q - A - A数据集训练一个恒等Aligner进行预热，然后使用Q - A - C数据集进行训练。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n论文中用于对比的核心基线模型包括SFT、RLHF、DPO、Constitutional AI (CAI)、Self - Critique和Self - Refine等。\\n\\n**性能对比**\\n*   **在有用性指标上：** 在多个上游模型和数据集上，Aligner表现出色。Aligner - 7B在多个模型上平均提高了21.9%的有用性，在提升GPT - 4的有用性方面达到17.5%（对比实验中为17.47%），而CAI在GPT - 4上无训练时提升20.01%，Self - Critique提升26.56%；在Alpaca2 - 7B上，Aligner - 7B提升了36.55%，CAI提升20.00%，Self - Critique提升30.07%；在Beaver - 7B上，CAI提升5.00%，Self - Critique提升12.80%，Aligner - 7B提升15.40%；在Llama2 - 13B - Chat上，CAI降低0.5%，Self - Critique提升15%，Aligner - 7B提升17.8%。与SFT、RLHF、DPO相比，在不同偏好数据集上，Aligner提升原始模型有用性的表现也相当或更优，如在Q - A - C数据集上，Aligner相较于SFT提升了23.1%，相较于RLHF提升了24.4%，相较于DPO提升了49.1%。\\n*   **在无害性指标上：** Aligner同样有良好表现。Aligner - 7B在多个模型上平均提高了23.8%的无害性，在提升GPT - 4的无害性方面达到26.9%（对比实验中为26.88%），而CAI在GPT - 4上提升9.65%，Self - Critique提升15.30%；在Alpaca2 - 7B上，Aligner - 7B提升了58.86%，CAI提升24.08%，Self - Critique提升14.36%；在Beaver - 7B上，CAI提升7.70%，Self - Critique降低11.6%，Aligner - 7B提升9.00%；在Llama2 - 13B - Chat上，CAI提升27.4%，Self - Critique提升11.1%，Aligner - 7B提升19.45%。在不同偏好数据集上，Aligner相较于SFT、RLHF、DPO也展现出优势，如在Q - A - C数据集上，Aligner相较于RLHF提升了21.9%，相较于DPO提升了0.1%。\\n*   **在训练资源指标上：** Aligner具有显著优势。对于7B源模型，DPO需要1.125倍、RLHF需要2.25倍于Aligner的资源；对于70B模型，DPO需要11.25倍、RLHF²需要22.5倍于Aligner的资源，而Aligner的训练资源需求不受源模型规模变化的影响。\",\n    \"keywords\": \"### 关键词\\n\\n- 大语言模型对齐 (Large Language Models Alignment, N/A)\\n- 残差学习对齐方法 (Residual Learning - based Alignment Method, N/A)\\n- Aligner对齐范式 (Aligner Alignment Paradigm, N/A)\\n- 多轮RLHF训练 (Multi - round RLHF Training, N/A)\"\n}"
}