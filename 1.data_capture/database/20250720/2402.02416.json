{
    "source": "Semantic Scholar",
    "arxiv_id": "2402.02416",
    "link": "https://arxiv.org/abs/2402.02416",
    "pdf_link": "https://arxiv.org/pdf/2402.02416.pdf",
    "title": "Aligner: Efficient Alignment by Learning to Correct",
    "authors": [
        "Jiaming Ji",
        "Boyuan Chen",
        "Hantao Lou",
        "Donghai Hong",
        "Borong Zhang",
        "Xuehai Pan",
        "Tianyi Qiu",
        "Juntao Dai",
        "Yaodong Yang"
    ],
    "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
    ],
    "publication_date": "2024-02-04",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 40,
    "influential_citation_count": 6,
    "institutions": [
        "未找到机构信息"
    ],
    "paper_content": "# Aligner: Efficient Alignment by Learning to Correct\n\n# Jiaming Ji∗ Boyuan Chen∗ Hantao Lou Donghai Hong Borong Zhang Xuehai Pan Juntao Dai Tianyi Qiu Yaodong Yang†\n\nCenter for AI Safety and Governance, Institute for AI, Peking University\n\nProject Website: https://pku-aligner.github.io\n\n{jiamg.ji,cbylll,lht_pku,donghai.hong}@stu.pku.edu.cn yaodong.yang@pku.edu.cn\n\n# Abstract\n\nWith the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-andplay module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model’s performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of $6 8 . 9 \\%$ in helpfulness and $2 3 . 8 \\%$ in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from $5 5 . 0 \\%$ to $5 8 . 3 \\%$ , surpassing GPT-4 Omni’s $5 7 . 5 \\%$ Win Rate (community report).\n\n# 1 Introduction\n\nThe alignment of LLMs with human intentions and values has recently gained significant attention [1]. Among the various methods, supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) [2, 3] have emerged as practical approaches. SFT leverages human demonstrations to fine-tune LLMs and instruct the model on desired actions, whereas RLHF trains a reward model (RM) based on human preferences and fine-tunes LLMs using feedback signals from the RM through reinforcement learning (RL) methods [4].\n\nDespite the effectiveness of these methods [5, 6, 7, 8, 9] in meeting 3H (helpfulness, harmlessness, and honesty) standards [10], they suffer from challenges such as high training resource consumption and difficulty in ensuring consistent performance [11]. Meanwhile, in real-world scenarios, alignment requirements are dynamically changing [12]. Models may encounter cases outside of alignment training and exhibit undesirable behaviors, which are difficult to address immediately using timeconsuming methods such as SFT and RLHF.\n\n![](images/b19537496cab335b00cdc8157966d4c0786b0e3e56b76349a79b595fc63ba562.jpg)  \nFigure 1: (Left) Architecture of the Aligner module and illustration of its behavior in semantic space. As a plug-and-play module, Aligner stack upon an upstream LLM. The Aligner redistributes initial answers from the upstream model into more helpful and harmless answers, thus aligning the composed LLM responses with human intentions. (Right) Analogy of Aligner as a residual learning enhancer for LLMs in architecture and capabilities. Like a residual block that adds modifications via a shortcut without altering the base structure, the Aligner employs a copy and correct method to improve the original answer. This analogy highlights the Aligner’s dual role in preserving the parameter of the upstream model while enhancing it to align with desired outcomes.\n\nCan we develop an efficient, lightweight, and model-agnostic alignment method?\n\nInspired by residual learning [13], we simplify the alignment process by focusing on copy and correction operations. We introduce an efficient alignment paradigm, the Aligner, without involving any RL processes, as shown in Figure 1. Specifically, Aligner is fine-tuned on a preference dataset to learn the correctional residuals between preferred and non-preferred responses and then stacked on the upstream model to achieve corrected alignment. Here, the upstream LLM refers to models targeted for alignment and is compared to the source model in the RLHF process. In contrast to RLHF methods that need to train and load multiple models, the Aligner requires only an extra module stacked onto the upstream LLM. Moreover, our method’s computational resource demand depends solely on the desired efficacy of the Aligner, not on the parameter size of the upstream LLMs.\n\nFrom the perspective of representation learning [14, 15, 16], Aligner exhibits an interpretable residual behavior. As shown in Figure 4, Aligner decides the degree of reference to the original response and the extent of additional correction based on the quality of the original answers in the early layers, whereas its middle and late layers are used to implement this decision. The mechanism is simpler than directly learning the mapping from input queries to aligned answers. This simplicity indicates that small Aligner can also learn complex correction patterns, demonstrating their capability to steer powerful models with relatively little inference, which further underscores the superiority of our Aligner paradigm.\n\nIn summary, Aligner presents several significant advantages:\n\n• Resource Efficient. Without extra models such as the actor, critic, reward, and reference model, our Aligner is a small model trained on the preference dataset to learn correctional residuals. Specifically, when aligning a 70B LLM, Aligner-7B occupies 11.25 times smaller than DPO and 22.5 times smaller than ${ \\mathrm { R L H F } } ^ { 2 }$ regarding training parameters.\n\n• Plug and Play. The Aligner’s plug-and-play nature and model agnosticism make it ideal for API-based models without parameter access. Once trained, the Aligner can be applied to various upstream LLMs without parameter adjustments. Experiments showed that the Aligner-7B model enhances helpfulness and harmlessness across 11 models, including API-based/open-source safety-aligned/safety-unaligned models. Experiment results demonstrate that the Aligner-7B increased GPT-4’s helpfulness by $1 7 . 5 \\%$ and its harmlessness by $2 6 . 9 \\%$ .\n\n# 2 Aligner\n\nPreliminary: Supervised Fine-Tuning (SFT) SFT aims to finetune the pretrained LLM to generate curated high-quality dataset $\\mathcal { D } _ { \\mathrm { S F T } } = \\{ \\pmb { x } ^ { ( i ) } , \\pmb { y } ^ { ( i ) } \\} _ { i = 1 } ^ { N }$ . The goal is to obtain a model $\\pi _ { \\boldsymbol { \\theta } } ^ { \\mathrm { S F T } }$ with the\n\n$$\n\\operatorname* { m i n i m i z e } _ { \\pmb { \\theta } } \\mathcal { L } ( \\pmb { \\theta } ; \\mathcal { D } _ { \\mathrm { S F T } } ) = - \\mathbb { E } _ { ( \\pmb { x } , \\pmb { y } ) \\sim \\mathcal { D } _ { \\mathrm { S F T } } } [ \\log \\pi _ { \\pmb { \\theta } } ( \\pmb { y } | \\pmb { x } ) ] .\n$$\n\nSimilarly, illustrated in Figure 1, Aligner improves alignment between the model and human intentions by redistributing the model’s answers through conditional generation. In practical implementation, Aligner only needs to make a minor adjustment to the SFT training code (only need to change one line of code), as detailed in Appendix D.\n\nOverall, the whole pipeline of Aligner training can be summarized as follows: Based on a preference dataset, the model is fine-tuned to learn the correctional residuals between preferred and non-preferred responses. After a single training session, this model can be deployed on any model to achieve corrected alignment.\n\nModel Training Based on the above procedures, we have constructed the dataset $\\mathcal { M } \\ =$ $\\{ \\pmb { x } ^ { ( i ) } , \\pmb { y } _ { o } ^ { ( i ) } , \\pmb { y } _ { c } ^ { ( i ) } \\} _ { i = 1 } ^ { N }$ , which $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ represents the user’s query, $\\scriptstyle { \\pmb { y } } _ { o }$ is the original answer, and $\\pmb { y } _ { c }$ is the corrected answer according to established principles. The model training process is relatively straightforward. We train the Aligner, a conditional seq2seq model $\\mu _ { \\phi } ( { \\pmb y } _ { c } | { \\pmb y } _ { o } , { \\pmb x } )$ parameterized by $\\phi$ , to redistribute the preliminary answers $\\scriptstyle { \\pmb { y } } _ { o }$ to the aligned answer $_ { y _ { c } }$ . Demonstrated in Figure 1, the composed answer generation process for aligned answers based on the upstream LLM $\\pi _ { \\boldsymbol { \\theta } }$ is:\n\n$$\n\\begin{array} { r } { \\pi ^ { \\prime } ( y _ { c } | \\mathbf { x } ) = \\sum _ { y _ { k } } \\mu _ { \\phi } ( y _ { c } | \\mathbf { y } _ { k } , \\pmb { x } ) \\pi _ { \\theta } ( y _ { k } | \\mathbf { x } ) \\geqslant \\mu _ { \\phi } ( y _ { c } | \\mathbf { y } _ { o } , \\pmb { x } ) \\pi _ { \\theta } ( y _ { o } | \\mathbf { x } ) , } \\end{array}\n$$\n\nwhere $\\pmb { y } _ { k }$ is a possible answer generated by upstream LLM $\\pi _ { \\boldsymbol { \\theta } }$ . By calculating empirical loss on the whole dataset $\\mathcal { M }$ , we can get equation (3) from equation (2):\n\n$$\n- \\mathbb { E } _ { \\mathcal { M } } [ \\log \\pi ^ { \\prime } ( y _ { c } | \\boldsymbol { x } ) ] \\leqslant - \\mathbb { E } _ { \\mathcal { M } } [ \\log \\mu _ { \\phi } ( y _ { c } | y _ { o } , \\boldsymbol { x } ) ] - \\mathbb { E } _ { \\mathcal { M } } [ \\log \\pi _ { \\theta } ( y _ { o } | \\boldsymbol { x } ) ] .\n$$\n\nThe second term in equation (3) is not related to the Aligner parameter and the training objective for Aligner can be derived as equation (4):\n\n$$\n\\operatorname* { m i n i m i z e } _ { \\phi } \\mathcal { L } _ { \\mathrm { A l i g n e r } } ( \\phi , \\mathcal { M } ) = - \\mathbb { E } _ { \\mathcal { M } } \\left[ \\log \\mu _ { \\phi } \\left( y _ { c } | y _ { o } , x \\right) \\right] .\n$$\n\nBy optimizing this objective, we actually optimize the upper bound of the SFT training objective, which ensures that $_ { y _ { c } }$ is effectively learned. It is worth noting that Aligner does not require access to the parameters of the upstream LLM $\\pi _ { \\boldsymbol { \\theta } }$ during both training and inference phases. Aligner takes the user’s query $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ and the initial answer $\\scriptstyle { \\boldsymbol { y } } _ { o }$ generated by the upstream LLM $\\pi _ { \\boldsymbol { \\theta } }$ , then generates the answer $_ { y _ { c } }$ which is better aligned with human values. Improving existing answers $\\scriptstyle { y _ { o } }$ allows Aligner to focus on how to align with human values rather than how to answer the given query directly. This significantly reduces the requirements on our model capacity, allowing us to achieve the expected alignment performance with only a small model.\n\nAligner’s Training Strategy: Residual Correction We develop an optimized training strategy, termed Residual Correction, which leverages the semantic correctional residuals between answers $( \\pmb { y } _ { o } )$ and corrections $( { \\boldsymbol { y } } _ { c } )$ , as shown in Figure 1. Specifically, we construct a Q-A-A dataset using partial training data to train an identity Aligner initially, a process we term warm-up. Subsequently, we utilize the Q-A-C dataset for training, building upon the identity Aligner. The details of our experiments on a 50K training dataset are shown in Section 3.3. Outside the alignment field, ResNet [13] also uses a similar approach to mitigate the vanishing gradient problem caused by increased neural network depth.\n\nResource Analysis between Aligner and RLHF/DPO Compared to RLHF and DPO [6], Aligner shows notable advantages in training resource requirements. Regarding training resources, Aligner7B is more efficient than other methods under similar performance conditions. Specifically, with a 7B source model, DPO requires 1.125 times, and RLHF 2.25 times more resources than Aligner. Additionally, as the source model’s scale increases, the resource demands for other methods rise sharply. For a 70B model, DPO needs 11.25 times, and RLHF 22.5 times more resources than Aligner. However, since Aligner is insensitive to these changes, its training resource requirements remain constant regardless of the source model’s scale, indicating that Aligner is an efficient and lightweight alignment paradigm.\n\nTable 1: Performance of Aligner models. It is shown that Aligner achieves significant performances in all the settings. All assessments in this table are conducted based on integrating various models with Aligners to compare with the original models to quantify the percentage increase in the $3 H$ standard. When integrated and assessed in conjunction with various upstream models, the Aligner requires only a single training session (i.e., the Aligner can operate in a zero-shot manner and enhance the performance of all upstream models.)   \n\n<html><body><table><tr><td colspan=\"2\"></td><td colspan=\"2\">Helpful</td><td colspan=\"4\">Harmless</td><td>Honest</td></tr><tr><td>Aligner</td><td>Upstream LLM</td><td>E-Dialogue</td><td>DialogSum</td><td colspan=\"2\">Beavertails</td><td colspan=\"2\">HarmfulQA</td><td>TruthfulQA</td></tr><tr><td></td><td></td><td>Empathy ↑</td><td>Reasoning ↑</td><td>Helpful ↑</td><td>Harmless ↑</td><td>Helpful ↑</td><td>Harmless ↑</td><td>Reliable↑</td></tr><tr><td rowspan=\"14\">2B</td><td>GPT-4</td><td>26.0%</td><td>2.3%</td><td>8.0%</td><td>28.6%</td><td>12.5%</td><td>29.2%</td><td>-0.5%</td></tr><tr><td>GPT-3.5</td><td>26.3%</td><td>3.3%</td><td>3.1%</td><td>7.6%</td><td>3.6%</td><td>4.4%</td><td>0.7%</td></tr><tr><td>Claude 2</td><td>83.1%</td><td>6.0%</td><td>38.3%</td><td>15.1%</td><td>48.0%</td><td>14.4%</td><td>0.7%</td></tr><tr><td>Beaver-7B</td><td>95.3%</td><td>60.7%</td><td>9.9%</td><td>12.1%</td><td>7.8%</td><td>7.6%</td><td>5.4%</td></tr><tr><td>Alpaca-7B</td><td>97.7%</td><td>58.5%</td><td>5.8%</td><td>45.0%</td><td>22.6%</td><td>65.3%</td><td>10.0%</td></tr><tr><td>Vicuna-7B</td><td>44.9%</td><td>58.5%</td><td>16.9%</td><td>15.8%</td><td>17.7%</td><td>27.1%</td><td>4.9%</td></tr><tr><td>Vicuna-13B</td><td>53.9%</td><td>24.0%</td><td>19.4%</td><td>14.9%</td><td>17.1%</td><td>16.1%</td><td>7.6%</td></tr><tr><td>Vicuna-33B</td><td>45.7%</td><td>39.3%</td><td>24.4%</td><td>52.4%</td><td>26.9%</td><td>32.6%</td><td>5.1%</td></tr><tr><td>Llama2-7B-Chat</td><td>88.1%</td><td>69.5%</td><td>25.4%</td><td>7.2%</td><td>11.3%</td><td>25.9%</td><td>3.9%</td></tr><tr><td>Llama2-13B-Chat</td><td>85.3%</td><td>53.4%</td><td>18.4%</td><td>12.3%</td><td>18.6%</td><td>27.6%</td><td>1.2%</td></tr><tr><td>Llama2-70B-Chat</td><td>86.7%</td><td>47.9%</td><td>17.8%</td><td>5.5%</td><td>21.3%</td><td>7.2%</td><td>10.8%</td></tr><tr><td>Average</td><td>66.6%</td><td>36.4%</td><td>17.0%</td><td>19.7%</td><td>18.8%</td><td>23.4%</td><td>4.5%</td></tr><tr><td rowspan=\"14\"></td><td>GPT-4</td><td>27.7%</td><td>6.2%</td><td>18.6%</td><td>25.8%</td><td>16.3%</td><td>28.0%</td><td>-1.2%</td></tr><tr><td>GPT-3.5</td><td>25.6%</td><td>6.8%</td><td>9.3%</td><td>9.3%</td><td>8.4%</td><td>7.0%</td><td>0.0%</td></tr><tr><td>Claude 2</td><td>90.4%</td><td>10.4%</td><td>58.4%</td><td>30.3%</td><td>69.4%</td><td>42.1%</td><td>2.0%</td></tr><tr><td>Beaver-7B</td><td>98.3%</td><td>83.5%</td><td>21.9%</td><td>12.0%</td><td>8.9%</td><td>6.0%</td><td>10.3%</td></tr><tr><td>Alpaca-7B</td><td>99.4%</td><td>78.7%</td><td>34.9%</td><td>47.0%</td><td>38.2%</td><td>70.7%</td><td>11.8%</td></tr><tr><td>Vicuna-7BB</td><td>95.%</td><td>73.5%</td><td>26.4%</td><td>15.9%</td><td>12.0%</td><td>29.3%</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>2.7%</td></tr><tr><td>Vicuna-33B</td><td>89.3%</td><td>58.5%</td><td>51.0%</td><td>55.9%</td><td>-1.0%</td><td>33.6%</td><td>3.2%</td></tr><tr><td>Llama2-7B-Chat</td><td>95.6%</td><td>98.8%</td><td>19.9%</td><td>7.4%</td><td>-5.7%</td><td>22.1%</td><td>1.5%</td></tr><tr><td>Llama2-13B-Chat</td><td>96.6%</td><td>70.8%</td><td>20.1%</td><td>10.3%</td><td>15.5%</td><td>28.6%</td><td>1.7%</td></tr><tr><td>Llama2-70B-Chat</td><td>95.0%</td><td>70.1%</td><td>5.2%</td><td>2.4%</td><td>-6.6%</td><td>4.1%</td><td>9.1%</td></tr><tr><td>Average GPT-4</td><td>82.5%</td><td>55.2%</td><td>27.6%</td><td>21.2%</td><td>16.1%</td><td>26.4%</td><td>4.0%</td></tr><tr><td>Claude 2</td><td>42.6%</td><td>9.7%</td><td>33.9%</td><td>25.1%</td><td>25.1%</td><td>20.1%</td><td>-0.2%</td></tr><tr><td rowspan=\"14\">13B</td><td>GPT-3.5</td><td>43.7%</td><td>15.6%</td><td>15.1%</td><td>10.9%</td><td>7.6%</td><td>7.7%</td><td>0.5%</td></tr><tr><td></td><td>90.6%</td><td>17.2%</td><td>50.0%</td><td>30.0%</td><td>45.9%</td><td>28.6%</td><td></td></tr><tr><td>Beaver-7B</td><td>98.1%</td><td>87.6%</td><td>14.2%</td><td>19.1%</td><td>8.0%</td><td>11.6%</td><td>0.5%</td></tr><tr><td>Alpaca-7B</td><td>99.0%</td><td>82.9%</td><td>8.5%</td><td>53.4%</td><td>3.4%</td><td>75.9%</td><td>13.0%</td></tr><tr><td>Vicuna-7B</td><td>96.3%</td><td>78.5%</td><td>19.1%</td><td>24.0%</td><td>19.5%</td><td>31.0%</td><td>16.9%</td></tr><tr><td>Vicuna-13B</td><td>95.9%</td><td>58.7%</td><td></td><td>26.7%</td><td>30.9%</td><td>18.9%</td><td>6.6%</td></tr><tr><td></td><td></td><td></td><td>31.8%</td><td>63.3%</td><td></td><td></td><td>7.1%</td></tr><tr><td>Vicuna-33B</td><td>90.0%</td><td>65.9%</td><td>33.3%</td><td></td><td>7.3%</td><td>33.3%</td><td>6.1%</td></tr><tr><td>Llama2-7B-Chat Llama2-13B-Chat</td><td>96.0% 95.4%</td><td>99.1%</td><td>13.5%</td><td>4.6% 10.6%</td><td>12.6% 30.7%</td><td>32.3%</td><td>4.2%</td></tr><tr><td></td><td></td><td>73.1%</td><td>16.7%</td><td></td><td></td><td>35.0%</td><td>1.0%</td></tr><tr><td>Llama2-70B-Chat Average</td><td>94.6% 85.6%</td><td>69.2% 59.8%</td><td>10.6% 22.4%</td><td>1.9% 24.5%</td><td>6.3% 17.9%</td><td>7.6% 27.4%</td><td>10.3% 6.0%</td></tr></table></body></html>\n\n# 3 Experiments\n\nIn this section, we assess the effectiveness of Aligner modules in the 3H (Helpful, Harmless, Honest) evaluation metrics and configurations. For detailed training parameters, please see Appendix D.\n\n# 3.1 Experiment Setup\n\nPreference Datasets We utilize two open-source preference datasets, HH-RLHF [5] and PKUSafeRLHF [17] as our preference datasets. Considering that the preference pairs in PKU-SafeRLHF are generated solely by Alpaca-7B, we additionally construct a 50K preference dataset based on these two preference datasets. The questions in this dataset are sourced from HH-RLHF, PKU-SafeRLHF, and so on, resulting in 27K queries for subsequent answers and corrected answer generation. The original answers are generated using various open-source models, including Alpaca-7B [3], Vicuna(7B,13B,33B) [18], Llama2-(7B,13B)-Chat [19], and Alpaca2-(7B,13B)3. We use GPT-4, Llama2- 70B-Chat, and human annotators to revise the answers in the above Q-A dataset. These revisions are based on well-defined principles, establishing constraints for training the seq2seq model. These principles are aimed at effectively extending to the characteristics we wish LLMs to embody. We focus on the 3H dimensions of LLMs (helpfulness, harmlessness, and honesty) [10]. For those answers that conform to these fundamental principles, we retain the original answers. Figure 2 (a) visually shows the distribution shift before and after the data correction, thereby demonstrating the impact of the revision process on the dataset. More details about the construction of Q-A Datasets can be found in Appendix D.1.\n\n![](images/c3f0ef5c839a93faf5bff2b29018de8736c9feab8d275bb4d80d58d859d3496a.jpg)  \nFigure 2: Distribution of helpfulness and harmlessness scores. (a) The distribution shift in preferred and dis-preferred answers in the training dataset; (b) redistribution shift of Aligner-7B, based on upstream models such as GPT-4 (b1), Alpaca-7B (b2) and Llama2-70B-Chat (b3) in the evaluation dataset. Our findings include: (1) Preferred answers in the training dataset surpasses original answers in both helpfulness and harmlessness; (2) The refuse-to-answer pattern of GPT-4 created an area of overcorrected answers where both helpful and harmless scores are low, and Aligner7B improved these answers by providing additional information and corrections. (3) The Alpaca-7B model, which lacks alignment, had its answers significantly corrected by our Aligner-7B, increasing both scores. (4) The Llama2-70B-Chat model, already aligned with a higher average safety score than the training dataset corrections, benefits from Aligner-7B corrections, significantly enhancing helpfulness while maintaining the harmless score.\n\nModels and Evaluation Datasets We trained the Aligner on three model sizes, specifically based on Gemma-2B [20] and Llama2 (7B, 13B) [19]. To assess the Aligner module, we utilize five datasets: E-Dialogue [21], DialogSum [22], BeaverTails [17], HarmfulQA [23], and TruthfulQA [24]. More details can be found in Appendix B.1. Our evaluation focuses on two model categories: API-based models (e.g., GPT-4 [25], Claude 2 [26]) and Open-Source models (Llama2-(7B, 13B, 70B)-Chat [19]; Vicuna-(7B, 13B, 33B) [18]; Alpaca-7B [3]; Beaver-7B [27]). Notably, the Llama2 and Beaver models have undergone safety alignment processing.\n\nEvaluation Metrics Our evaluation hinges on three key dimensions: helpfulness, harmlessness, and honesty. The independent characteristics of these dimensions provide a comprehensive perspective on the answers, allowing us to balance information quality with safety and ethical considerations in the evaluation of an answer’s quality. Initial answers are generated by open-source and upstream models, which the Aligner refines to yield corrected answers. More details and examples can be found in Appendix B.\n\n# 3.2 Experiment Results\n\nAs shown in Table 1, we employ Aligners of various sizes, significantly improving the performance of all 11 upstream models with only one training session. Under the 3H standard, Aligner-7B showcases\n\nan average enhancement of $2 1 . 9 \\%$ in helpfulness and $2 3 . 8 \\%$ in harmlessness across the models.   \nRemarkably, Aligner-7B can boost GPT-4’s helpfulness by $1 7 . 5 \\%$ and harmlessness by $2 6 . 9 \\%$ .\n\nPerformance on the 3H Standard Aligner keeps the upstream model unchanged, offering adaptability in Aligner model sizing based on available resources. We evaluated Aligner’s effectiveness using five datasets according to the 3H standard. Experiment results show that Aligner significantly enhances the upstream model’s performance across various parameter scales. Particularly, Aligner-7B markedly enhanced the GPT-4 model’s performance across all five dimensions. In the reasoning dimension, with an increase in parameters, Aligner boosts the upstream model’s capability, showcasing the Scaling Laws [28] characteristics. Notably, Aligner excelled in the empathy dimension, further evidencing its efficiency in redistributing the upstream model’s pattern distribution. To detect whether Aligner would generate known false content due to misunderstandings, similar to [19], we use TruthfulQA [24] to measure the reliability of the outputs generated by Aligner in terms of factualness and common sense. The results demonstrate that Aligner does not add extra hallucination information while correcting the upstream model.\n\nAssessing Aligner’s Stack on Safety-Aligned Models Llama2-Chat models, with their multi-stage alignment process (pre-training, SFT, RLHF), and Beaver, finetuned via Safe RLHF [27], both show modest safety improvements with Aligner. The primary achievement of Aligner is its ability to amplify helpfulness, especially in models predisposed to avoid risky responses. By re-distributing these overly conservative answers, Aligner significantly boosts overall helpfulness. This enhancement in helpfulness is visually represented in Figure 2, showing a rightward shift in Llama2-70B-Chat’s answer distribution under the influence of Aligner-7B, indicating improved helpfulness on a strong safety foundation.\n\n# 3.3 Ablation Study\n\n![](images/8d5ca029f3d7297c1423ffc11fb92d9b56c64a58ab64b53528c45ba7c196430e.jpg)  \nFigure 3: Ablation study of different identity mapping proportions. We first trained an identity Aligner for identity mapping, followed by extensive residual Q-A-C learning based on this Aligner. Specifically, we formed the Q-A-A dataset by extracting partial data from the training dataset in proportions of $2 \\%$ , $10 \\%$ , $20 \\%$ , and $50 \\%$ .\n\nAblation on Identity Mapping To verify the effectiveness of different warm-up proportions, we conducted experiments using two representative datasets: BeaverTails and HarmfulQA. As shown in Figure 3, the warm-up step aids the Aligner by initially helping the Aligner learn identity mapping, thus improving the final performance. Moreover, the results further reveal that the effectiveness of the warm-up phase peaks when the proportion is 10k to 50k. However, determining the specific data proportion for warm-up is challenging and requires more training resources.\n\nComparison to Self-Refine, Critique Methods Constitutional AI (CAI) [29], Self-Critique [30], and Self-Refine [31], primarily utilize the self-critiquing and refining capabilities of LLMs to enhance their performance. We employ CAI prompts solely during the inference time of LLMs to encourage self-revision of their answers. As demonstrated in Table 2, our method, Aligner, outperforms the baseline considering both helpfulness and harmlessness dimensions. Additionally, baseline methods typically require multiple dialogue iterations and extended context windows for prompt insertion and ongoing self-correction. This could result in longer inference times and considerable consumption of context window length. For more detailed information and analysis, please refer to Appendix B.5.\n\nTable 2: Ablation study of Aligner’s effectiveness against CAI and Self-Critique. Experiment results revealed that Aligner surpasses these baselines in helpfulness and harmlessness metrics.   \n\n<html><body><table><tr><td>Model</td><td>Metrics</td><td>CAI w/o training</td><td>Self-Critique</td><td>Aligner-7B</td></tr><tr><td rowspan=\"2\">GPT-4</td><td>Helpfulness</td><td>+20.01%</td><td>+26.56%</td><td>+17.47%</td></tr><tr><td>Harmlessness</td><td>+9.65%</td><td>+15.30%</td><td>+26.88%</td></tr><tr><td rowspan=\"2\">Alpaca2-7B</td><td>Helpfulness</td><td>+20.00%</td><td>+30.07%</td><td>+36.55%</td></tr><tr><td>Harmlessness</td><td>+24.08%</td><td>+14.36%</td><td>+58.86%</td></tr><tr><td rowspan=\"2\">Beaver-7B</td><td>Helpfulness</td><td>+5.00%</td><td>+12.80%</td><td>+15.40 %</td></tr><tr><td>Harmlessness</td><td>+7.70%</td><td>-11.6%</td><td>+9.00%</td></tr><tr><td rowspan=\"2\">Llama2-13B-Chat</td><td>Helpfulness</td><td>-0.5%</td><td>+15%</td><td>+17.8%</td></tr><tr><td>Harmlessness</td><td>+27.4%</td><td>+11.1%</td><td>+19.45%</td></tr></table></body></html>\n\nPerformance of Aligner on the Various Preference Datasets To demonstrate the independence of Aligner from specific datasets, we utilized various open-source RLHF preference datasets. Specifically, we trained on HH-RLHF [5], PKU-SafeRLHF [17, 27] and Ultra-Feedback [32] datasets and compared Aligner with SFT, RLHF, and DPO. After fine-tuning Alpaca-7B with SFT, RLHF, and DPO, we compare these models against the original Alpaca-7B corrected by Aligner. The experiment results (as shown in Table 3) indicate that Aligner’s performance in enhancing the original model’s capabilities is comparable to, or exceeds, that of the baseline methods. Notably, models finetuned with RLHF or DPO tend to generate either conservative answers or fail to recognize dangers while adding helpful information explicitly. Importantly, training with RLHF or DPO methods requires optimizing significantly more models and consuming more training resources than just training an Aligner, e.g., for a 70B model, DPO needs 11.25 times and RLHF 22.5 times more resources.\n\nTable 3: Aligner trained on different preference datasets. The experimental results show that Aligner enhances the original model’s capabilities, performing on par with or surpassing baseline methods. Furthermore, these results are consistent across different preference and correction datasets.   \n\n<html><body><table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Q-A-C Datasets</td><td colspan=\"2\">PKU-SafeRLHF</td><td colspan=\"2\">HH-RLHF</td><td colspan=\"2\">Ultra-Feedback</td></tr><tr><td>Helpful</td><td>Harmless</td><td>Helpful</td><td>Harmless</td><td>Helpful</td><td>Harmless</td><td>Helpful</td><td>Harmless</td></tr><tr><td>Alignervs. SFT</td><td>+23.1%</td><td>+0.4%</td><td>=</td><td>=</td><td>=</td><td></td><td>=</td><td></td></tr><tr><td>Alignervs.RLHF</td><td>+24.4%</td><td>+21.9%</td><td>+8.7%</td><td>+8.8%</td><td>+9.6%</td><td>+3.4%</td><td>+25.47%</td><td>+13.13%</td></tr><tr><td>Alignervs.DPO</td><td>+49.1%</td><td>+0.1%</td><td>+33.3%</td><td>+27.0%</td><td>+5.6%</td><td>+30.9%</td><td>+27.21%</td><td>+6.12%</td></tr></table></body></html>\n\n# 3.4 Interpretability Experiments\n\nWhen performing the experiments above, we observed the correction paradigm of Aligner: the correction behavior is not a binary decision between correction and copying. Instead, it follows a conditional generation paradigm, where the degree of reference to the original response and the extent of additional correction depends on the quality of the original answers. To demonstrate that Aligner has learned this correction paradigm as a representation, we conduct the experiment based on representation engineering [14] and activation steering [33, 34, 15]. Specifically, we perform representation extraction and Linear Artificial Tomography (LAT) scan to the Llama2-7B based on the Aligner module. We then utilize the extracted representation to control the Aligner’s generation.\n\nThe results from the representation control experiment indicate that the ratio of adding or subtracting the representation vector in the Aligner activation will significantly affect the magnitude of correction, ranging from directly copying the original response to substantially increasing the extent of normal correction. This provides strong evidence that Aligner has internalized the correction paradigm as a representation. Furthermore, the LAT scan further shows that Aligner decides the degree of correction in its early layers based on the quality of the original response, and after that, it focuses on completing the correction in its middle and late layers. For more details of these experiments, see Appendix B.6.\n\n-25 -25 1 123.50505 □   \n-15 0 -20 -10   \n-5 2 -5   \n0 0 0.5   \n5 10 15 5 10 15 1.00 0.75 0.50 0.25 0.00 0.25 0.50   \nToken Position Representation Vector Coefficient Token Position   \n(a) LAT Neural Activity: Correction (b) LAT Neural Activity: Copy (c) Effect of Correction Representation Control\n\n# 4 Multi-round RLHF training via Aligner\n\nIn this section, we aim to show that, due to its efficient and plug-and-play features, Aligner can play a crucial role in the multi-round RLHF/DPO pipeline, as illustrated in Figure 5. Typical multi-round pipeline often suffers from reward collapse because the preference dataset used for reward modeling may deviate from the actual answer distribution of the upstream model [35]. This error accumulates over multiple rounds, leading to significant deviations in the model’s final results. Additionally, error accumulation may cause reward over-optimization in certain directions, e.g., generating longer responses irrespective of safety. The involvement of Aligner can help mitigate the problem.\n\nUser Prompt Round I User Prompt Round II User Prompt Round III   \nModel RLHF/DPO Model+ RLHF/DPO Model++ RLHF/DPO > > >   \nResponse A 𝒟pr!efeSreynctehedtiactasets Response A 𝒟pr\"efeSreynctehedtiactasets Response A 𝒟pr#efeSreynctehedtiactasets ↑   \nAligner1 Response A\\* Aligner2 Response A\\* Aligner3 Response A\\*\n\nAs shown in Figure 5, you can use the Aligner (which is trained using the original preference dataset for the next round of RLHF) to refine the upstream model response $A$ into response $A ^ { * }$ , and $( Q , A , A ^ { * } )$ pairs can be a new preference dataset for training in the next round of RLHF or DPO. This paradigm brings many advantages:\n\n• The Aligner inherits the feature of transferring from the dispreferred distribution to the preferred distribution in the preference dataset.   \n• Aligner modifies the upstream model to produce better answers, bringing the distribution of the resulting preference dataset closer to the answer distribution of the upstream model. This effectively mitigates the reward model collapse problem caused by out-of-distribution (OOD) preference datasets.\n\n• The Aligner serves as a synthetic data generator, providing an efficient and repeatable method for constructing preference datasets.\n\nWe conducted three rounds of RLHF and DPO on Alpaca2-7B using the three-round preference dataset from PKU-SafeRLHF [27]. Following this, we trained three rounds of Aligners with the same three-round preference datasets, which were then employed to refine the upstream model and generate new preference datasets. These synthetic preference datasets were subsequently used to fine-tune the upstream model. As illustrated in Figure 6, by aggregating Aligner, Aligner-corrected new pref\n\n![](images/d83b9f0e7a0e156d05bde36f28a6f80ca12aa76d88db233a06416fc95226b652.jpg)  \nFigure 6: Multi-round refinement through Aligner.\n\nerence datasets can effectively enhance two key metrics: improving the model’s safety while ensuring a monotonic increase in helpfulness with each round. In contrast, a typical multi-round RLHF/DPO pipeline only enhances utility, leaving the responses unsafe.\n\n# 5 Related Work\n\nReinforcement Learning from Human Feedback RLHF aims to align LLMs with human preferences [36, 2], utilizing RL algorithms [4] to train policy models, specifically LLMs, to maximize cumulative rewards from RMs. The RLHF approach involves the distributed training of various models [11] and the annotations by human experts, presenting operational challenges. Consequently, recent research has focused on reducing [37, 38] or eliminating [6] reliance on RMs, aiming to simplify the RLHF process. Simultaneously, [5, 39] employs advanced AI models for data annotation, further streamlining the RLHF process and cutting costs. In contrast to RLHF methods that require several models, Aligner only requires a constrained seq2seq model to meet the alignment objective. Aligner is distinguished by its plug-and-play nature and indifference to specific models and parameters, making it ideal for API-based models without parameter access.\n\nInference-time Methods These methods customize LLMs without requiring access to their internal parameters [40, 41, 7], proving especially useful for extremely large models or those available through APIs. However, most of these methods are sensitive to the upstream model. IPA [7] uses a lightweight adapter policy to multiply the next-token probabilities based on the upstream model during the decoding time. However, IPA needs to access the model’s output logit distribution. [8] enhances and refines user prompts to better suit the model, thereby facilitating more comprehensive contextual understanding for inference, similar to in-context learning (ICL) [42, 43]. [44] employs a smaller model to select the best response from several responses generated by the upstream model without fine-tuning upstream models, akin to the BoN (Best of N) selector [45, 46]. In this work, we introduce Aligner—a model-agnostic alignment module designed for seamless integration. Requiring only a single training session, Aligner can align 11 types of upstream models, significantly enhancing their performance according to 3H standards.\n\nSelf-Refinement LLMs do not always generate the coherent output on their first try. Self-refinement methods address this by iteratively improving outputs through self-generated feedback, bypassing additional supervision [47, 48, 49]. For example, SELF-DEBUGGING [50] allows LLMs to debug their predictions via few-shot examples, while [30] found that self-critiquing can expose output weaknesses that aid in fine-tuning, with larger models performing especially well in critique tasks. However, these methods typically depend on a single model’s ability to refine itself. Our work instead uses a separate model, Aligner, which can refine outputs from other models (e.g., 70B model, GPT-4), achieving robust weak-to-strong generalization [51]. This approach bypasses the limitations of smaller models and saves computational resources otherwise spent on self-critiquing. Additionally, by combining Aligner with an external critique model, future iterations could further enhance performance.\n\n# 6 Conclusion\n\nWe introduce the Aligner, an efficient, lightweight, and model-agnostic approach to align LLMs. Without the need for additional components such as the actor, critic, reward models, and others, Aligner demonstrates a significant increase in computational efficiency. Under the 3H standard, Aligner-7B showcases an average enhancement of $6 8 . 9 \\%$ in helpfulness and $2 3 . 8 \\%$ in harmlessness across the models. Remarkably, Aligner-7B can boost GPT-4’s helpfulness by $1 7 . 0 \\%$ and harmlessness by $2 6 . 9 \\%$ . In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo (04/09) improved its LC Win Rate [52] from $5 5 . 0 \\%$ to $5 8 . 3 \\%$ , surpassing GPT-4 Omni’s $5 7 . 5 \\%$ Win Rate (community report).\n\n# 6.1 Limitations and Future Work\n\nIn contrast to directly finetuning LLMs, Aligner employs an external module, which is ideal for models with inaccessible original parameters. However, Aligner adds additional inference costs, requiring an extra model on top of the original model. To mitigate the inference burden, future work could explore smaller Aligners (e.g., 0.5B) and streamlining Aligner’s corrections. We aim to enhance LLM alignment using the Aligner module, aiming for increased conciseness, efficiency, and interpretability. Future research will focus on enhancing Aligner’s versatility in challenging contexts like multi-turn dialogues and developing Control Aligner for domain-specific alignment with precise instructions. Moreover, unlike RLHF’s segmented approach, its end-to-end structure provides valuable insights into the alignment process for LLMs.\n\n# 6.2 Ethics and Impact\n\nThe Aligner dataset will be released under the CC BY-NC 4.0 license. This dataset integrates Q-A data from open-source and API-based models, with answers revised to meet the 3H (helpfulness, harmlessness, and honesty) standards [10]. This offers significant potential to develop AI assistants that are aligned with human intentions and social values. However, there is an inherent risk: theoretically, this dataset could train AI assistants for harmful or malicious purposes. As the Aligner dataset’s creators, we are dedicated to fostering beneficial and safe AI technology and strongly oppose any misuse that could hinder human progress. We strongly condemn any malicious use of the Aligner dataset and advocate for its responsible and ethical use.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   当前大型语言模型（LLMs）的对齐方法（如SFT和RLHF）存在训练资源消耗高、难以快速迭代的问题，无法满足动态变化的对齐需求。\\n> *   该问题在需要快速部署和迭代的实际应用场景中尤为关键，例如API-based模型的安全对齐和性能提升。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出Aligner，一种轻量级、模型无关的对齐范式，通过学习偏好和非偏好答案之间的校正残差，以“即插即用”方式提升上游模型的3H（helpfulness, harmlessness, honesty）性能。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **资源高效性**：Aligner-7B训练参数仅为DPO的1/11.25和RLHF的1/22.5。\\n> *   **即插即用性**：单次训练后，Aligner可应用于11种不同LLMs（包括API-based模型），平均提升帮助性68.9%和安全性23.8%。\\n> *   **突破性能上限**：通过将校正答案作为合成人类偏好数据，可迭代提升上游模型性能（如GPT-4 Turbo的Alpaca-Eval胜率从55.0%提升至58.3%）。\\n> *   **多轮RLHF训练**：Aligner能够有效缓解多轮RLHF/DPO训练中的奖励崩溃问题，提升模型的安全性和帮助性。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   受残差学习启发，将对齐简化为“复制-校正”操作：Aligner作为条件序列生成模型，学习从原始答案到对齐答案的语义残差映射，而非直接学习输入到输出的复杂映射。\\n> *   设计哲学：通过分离“回答能力”和“对齐能力”，让小模型（Aligner）专注于后者，从而实现对任意大模型的轻量级控制。\\n\\n> **创新点 (Innovations)**\\n> *   **对比先前工作**：传统RLHF需训练多个模型（actor/critic/RM），而DPO虽简化流程但仍需全参数微调。\\n> *   **本文改进**：\\n>     1.  **架构解耦**：Aligner独立于上游模型参数，仅需访问其输出。\\n>     2.  **残差训练策略**：通过Q-A-A（身份映射）和Q-A-C（校正）两阶段训练，稳定学习校正模式。\\n>     3.  **多轮RLHF训练**：Aligner能够生成新的偏好数据集，用于多轮RLHF/DPO训练，缓解奖励崩溃问题。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  构建偏好数据集：合并HH-RLHF和PKU-SafeRLHF，用GPT-4/人工生成校正答案。\\n> 2.  训练Aligner模型：优化目标函数（公式4）最小化 $-\\\\mathbb{E}[\\\\log \\\\mu_\\\\phi(y_c|y_o,x)]$。\\n> 3.  部署阶段：将Aligner叠加于上游LLM，生成校正答案 $y_c \\\\sim \\\\mu_\\\\phi(\\\\cdot|y_o,x)$。\\n> 4.  多轮RLHF训练：利用Aligner生成新的偏好数据集，用于下一轮RLHF/DPO训练。\\n> \\n> **关键公式**：\\n> $$\\\\min_\\\\phi \\\\mathcal{L}_{Aligner}(\\\\phi, \\\\mathcal{M}) = -\\\\mathbb{E}_{\\\\mathcal{M}}[\\\\log \\\\mu_\\\\phi(y_c|y_o,x)]$$\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   RLHF/DPO方法\\n> *   Constitutional AI (CAI)\\n> *   Self-Critique\\n> *   Self-Refine\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在帮助性（Helpfulness）上：** Aligner-7B在11个模型上平均提升68.9%，其中对GPT-4提升17.5%，显著优于CAI（+20.01%）和Self-Critique（+26.56%）。\\n> *   **在安全性（Harmlessness）上：** Aligner-7B平均提升23.8%，对GPT-4提升26.9%，远超CAI（+9.65%）和Self-Critique（+15.30%）。\\n> *   **在训练效率上：** 对齐70B模型时，Aligner-7B仅需DPO 1/11.25和RLHF 1/22.5的训练资源。\\n> *   **在多轮RLHF训练上：** Aligner能够有效缓解奖励崩溃问题，提升模型的安全性和帮助性。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   大语言模型对齐 (Large Language Model Alignment, LLM Alignment)\\n*   残差校正学习 (Residual Correction Learning, N/A)\\n*   即插即用模块 (Plug-and-Play Module, N/A)\\n*   三H标准 (3H Standard: Helpfulness/Harmlessness/Honesty, 3H)\\n*   强化学习人类反馈 (Reinforcement Learning from Human Feedback, RLHF)\\n*   直接偏好优化 (Direct Preference Optimization, DPO)\\n*   模型无关性 (Model-Agnostic, N/A)\\n*   语义空间重分布 (Semantic Space Redistribution, N/A)\\n*   多轮RLHF训练 (Multi-round RLHF Training, N/A)\"\n}\n```"
}