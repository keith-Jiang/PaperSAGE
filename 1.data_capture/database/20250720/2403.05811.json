{
    "link": "https://arxiv.org/abs/2403.05811",
    "pdf_link": "https://arxiv.org/pdf/2403.05811",
    "title": "Statistical Efficiency of Distributional Temporal Difference Learning and Freedman's Inequality in Hilbert Spaces",
    "authors": [
        "Yang Peng",
        "Liangyu Zhang",
        "Zhihua Zhang"
    ],
    "institutions": [
        "未找到机构信息"
    ],
    "publication_date": "2024-03-09",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Mathematics",
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "paper_content": "# Statistical Efficiency of Distributional Temporal Difference Learning and Freedman's Inequality in Hilbert Spaces\n\nYang Peng∗\n\nLiangyu Zhang†\n\nZhihua Zhang‡\n\n# Abstract\n\nDistributional reinforcement learning (DRL) has achieved empirical success in various domains. One core task in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\\eta ^ { \\pi }$ for a given policy $\\pi$ . The distributional temporal difference learning has been accordingly proposed, which is an extension of the temporal difference learning (TD) in the classic RL area. In the tabular case, Rowland et al. [2018] and Rowland et al. [2024a] proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference learning (CTD) and quantile temporal difference learning (QTD), respectively. In this paper, we go a step further and analyze the finitesample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric distributional temporal difference learning (NTD). For a $\\gamma$ -discounted infinite-horizon tabular Markov decision process, we show that for NTD we need $\\begin{array} { r } { \\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 p } ( 1 - \\gamma ) ^ { 2 p + 1 } } \\right) } \\end{array}$ iterations to achieve an $\\varepsilon$ -optimal estimator with high probabili ye, when the estimation error is measured by the $p$ -Wasserstein distance. This sample complexity bound is minimax optimal up to logarithmic factors in the case of the 1-Wasserstein distance. To achieve this, we establish a novel Freedman’s inequality in Hilbert spaces, which would be of independent interest. In addition, we revisit CTD, showing that the same non-asymptotic convergence bounds hold for CTD in the case of the $p$ -Wasserstein distance for $p \\geq 1$ .\n\n# 1 Introduction\n\nIn high-stake applications of reinforcement learning (RL), such as healthcare [Lavori and Dawson, 2004, Böck et al., 2022] and finance[Ghysels et al., 2005], only considering the mean of returns is insufficient. It is necessary to take risk and uncertainties into consideration. Distributional reinforcement learning (DRL) Morimura et al. [2010], Bellemare et al. [2017, 2023] addresses such issues by modeling the complete distribution of returns instead of their expectations.\n\nIn the field of DRL, one of the most fundamental tasks is to estimate the return distribution $\\eta ^ { \\pi }$ for a given policy $\\pi$ , which is referred to as distributional policy evaluation. Distributional temporal difference learning (TD) is probably the most widely-used approach for solving the distributional policy evaluation problem. A key aspect of implementing a distributional TD algorithm is how to represent the return distribution, an infinite-dimensional object, via a computationally feasible finite-dimensional parametrization. This has led to the development of two special instances of distributional TD: categorical temporal difference learning (CTD) [Bellemare et al., 2017] and quantile temporal difference learning (QTD) [Dabney et al., 2018]. These algorithms provide computationally tractable parametrizations and updating schemes of the return distribution.\n\nPrevious theoretical works have primarily focused on the asymptotic behaviors of distributional TD. In particular, Rowland et al. [2018] and Rowland et al. [2024a] showed the asymptotic convergences of CTD and QTD in the tabular case, respectively. A natural question arises: can we depict the statistical efficiency of distributional TD by non-asymptotic results similar to the classic TD algorithm [Li et al., 2024]?\n\n# 1.1 Contributions\n\nIn this paper, we manage to answer the above question affirmatively in the synchronous setting [Kakade, 2003, Kearns et al., 2002]. Firstly, we introduce non-parametric distributional temporal difference learning (NTD) in Section 3, which is not practical but aids theoretical understanding. We show that $\\begin{array} { r } { \\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 p } ( 1 - \\gamma ) ^ { 2 p + 1 } } \\right) } \\end{array}$ 4 iterations are sufficient to yield an estimator $\\hat { \\eta } ^ { \\pi }$ , such that the $p$ -Wasserstein meetric between $\\hat { \\eta } ^ { \\pi }$ and $\\eta ^ { \\pi }$ is less than $\\varepsilon$ with high probability (Theorem 4.1). This bound is minimax optimal (Theorem B.1) in the 1-Wasserstein metric case, if we neglect all logarithmic terms. Next, we revisit the more practical CTD, and show that, in terms of the $p$ -Wasserstein metric, CTD and NTD have the same non-asymptotic convergence bounds (Theorem 4.2). It is worth pointing out that to attain such tight bounds in Theorem 4.1, we establish a Freedman’s inequality in Hilbert spaces (Theorem A.2). We would believe it is of independent interest beyond the current work.\n\n# 1.2 Related Work\n\nNon-asymptotic results of DRL. Recently, there has been an emergence of work focusing on finite-sample/iteration results of the distributional policy evaluations.\n\nWu et al. [2023] studied the offline distributional policy evaluation problem. They solved the problem via fitted likelihood estimation (FLE) inspired by the classic offline policy evaluation algorithm fitted Q evaluation (FQE), and provided a generalization bound in the $p$ -Wasserstein metric case.\n\nZhang et al. [2023] proposed to solve distributional policy evaluation by the model-based approach and derived corresponding sample complexity bounds, namely $\\begin{array} { r } { \\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 p } ( 1 - \\gamma ) ^ { 2 p + 2 } } \\right) } \\end{array}$ in the $p$ -Wasserstein metric case, and $\\begin{array} { r } { \\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 4 } } \\right) } \\end{array}$ in both the Kolmogorov-Smirnov metric and total variation metric under different  enditions. Rowland et al. [2024b] proposed direct categorical fixedpoint computation (DCFP), a model-based version of CTD, in which they constructed the estimator by solving a linear system directly instead of performing an iterative algorithm. They showed that the sample complexity of DCFP is $\\begin{array} { r } { \\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\right) } \\end{array}$ in the 1-Wasserstein metric case by introducing the novel stochastic categorical CDFeBellman operator and equation. Their result matches the minimax lower bound (up to logarithmic factors) $\\begin{array} { r } { \\widetilde \\Omega \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\right) } \\end{array}$ proposed in [Zhang et al., 2023], which implies that learning the full return distributionecan be as sample-efficient as learning just its expectation. It’s worth noting that the algorithms analyzed in both [Zhang et al., 2023] and [Rowland et al., 2024b] are model-based, hence they are less similar to practical algorithms. While distributional TD analyzed in this paper, as a model-free method, is more practical, and also involves a more complicated theoretical analysis.\n\nBöck and Heitzinger [2022] also considered model-free method. They proposed speedy categorical policy evaluation (SCPE), which can be regarded as CTD with an additional acceleration term. They showed that the sample complexity of SCPE is $\\begin{array} { r } { \\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 4 } } \\right) } \\end{array}$ in the 1-Wasserstein metric case. Compared to [Böck and Heitzinger, 2022], our woerk shows that even if we do not introduce any acceleration techniques to the original CTD algorithm, it is still possible to attain the near-minimax optimal sample complexity bounds. Thus, we give sharper bounds based on a simpler algorithm.\n\nTable 1 gives more detailed comparisons of sample complexity with the previous work in the 1- Wasserstein metric. Note that solving distributional policy evaluation can also address the traditional policy evaluation task by taking expectation of the return distribution estimator. And the supreme 1-Wasserstein metric error of the return distribution estimator is not smaller than the $\\ell _ { \\infty }$ error of the induced value function estimator (see the proof of Theorem B.1 in Appendix B), we have also listed the sample complexity of the policy evaluation task in Table 1 for comparison.\n\nTable 1. Sample complexity of algorithms for solving policy evaluation (PE) in the $\\ell _ { \\infty }$ norm, and distributional policy evaluation (DPE) in the supreme 1-Wasserstein metric.   \n\n<html><body><table><tr><td></td><td colspan=\"2\"> Sample Complexity</td><td>Algorithms</td><td>Task</td></tr><tr><td rowspan=\"2\">[Gheshlaghi Azar etal., 2013]</td><td></td><td>1 ∈²(1-γ)³</td><td>Model-based</td><td>PE</td></tr><tr><td>0 1</td><td></td><td>TD (Model-free)</td><td>PE</td></tr><tr><td>[Li et al., 2024] [Rowland et al., 2018]</td><td></td><td>e²(1-γ)3 Asymptotic</td><td>CTD (Model-free)</td><td>DPE</td></tr><tr><td>[Rowland et al., 2024a]</td><td colspan=\"2\">Asymptotic</td><td>QTD (Model-free)</td><td>DPE</td></tr><tr><td>[Rowland et al., 2024b]</td><td>0</td><td>2(1-2)3</td><td>DCFP (Model-based)</td><td>DPE</td></tr><tr><td>[Bock and Heitzinger, 2022]</td><td></td><td>2（1-）4</td><td>SCPE (Model-free)</td><td>DPE</td></tr><tr><td>Our Work</td><td></td><td>22（1-</td><td>CTD (Model-free)</td><td>DPE</td></tr></table></body></html>\n\nFreedman’s inequality. Freedman’s inequality was originally proposed in [Freedman, 1975]. It can be viewed as a Bernstein’s inequality for martingales, which is crucial for analyzing stochastic approximation algorithms. Tropp [2011] generalized Freedman’s inequality to matrix martingales. And Talebi et al. [2022] established Freedman inequalities for martingales in the setting of noncommutative probability spaces. To the best of our knowledge, we are the first to present a concrete version of Freedman’s inequality in Hilbert spaces.\n\nThe remainder of this paper is organized as follows. In Section 2, we introduce some background of DRL and state Freedman’s inequality in Hilbert spaces. In Section 3, we revisit distributional TD and propose NTD for further theoretical analysis. In Section 4, we analyze the non-asymptotic convergence bounds of NTD and CTD. Section 5 presents proof outlines of our theoretical results, and Section 6 concludes our work. We put the detailed results with Freedman’s inequality in Hilbert spaces in Appendix A, and the minimax lower bound of the distributional policy evaluation task in Appendix B.\n\n# 2 Background\n\nAn infinite-horizon tabular Markov decision process (MDP) is defined by a 5-tuple $\\begin{array} { l l } { M } & { = } \\end{array}$ $\\langle S , \\mathcal { A } , \\mathcal { P } _ { R } , P , \\gamma \\rangle$ , where $s$ represents a finite state space, $\\mathcal { A }$ a finite action space, $\\mathcal { P } _ { R }$ the distribution of rewards, $P$ the transition dynamics, i.e., $\\mathcal { P } _ { R } ( \\cdot | s , a ) \\in \\Delta \\left( [ 0 , 1 ] \\right) , P ( \\cdot | s , a ) \\in \\Delta \\left( \\mathcal { S } \\right)$ for any state action pair $( s , a ) \\in \\mathcal S \\times \\mathcal A$ , and $\\gamma \\in ( 0 , 1 )$ a discount factor. Here we use $\\Delta ( \\cdot )$ to represent the set of all probability distributions over some set. Given a policy $\\pi \\colon S  \\Delta ( { \\mathcal { A } } )$ and an initial state $s _ { 0 } = s \\in \\mathcal { S }$ , a random trajectory $\\{ ( s _ { t } , a _ { t } , t _ { t } ) _ { t = 0 } ^ { \\infty } \\}$ can be sampled from $M$ : at $\\mid s _ { t } \\sim \\pi ( \\cdot \\mid s _ { t } )$ , $\\begin{array} { r } { r _ { t } \\mid ( s _ { t } , a _ { t } ) \\sim \\mathcal { P } _ { R } ( \\cdot \\mid s _ { t } , a _ { t } ) , s _ { t + 1 } \\mid ( s _ { t } , a _ { t } ) \\sim P ( \\cdot \\mid s _ { t } , a _ { t } ) } \\end{array}$ for any $t \\in \\mathbb { N }$ . Given a trajectory, we define the return by $\\begin{array} { r } { G ^ { \\pi } ( s ) : = \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } r _ { t } \\in \\left\\lceil 0 , \\frac { 1 } { 1 - \\gamma } \\right\\rceil } \\end{array}$ . We denote return distribution $\\eta ^ { \\pi } ( s )$ as the probability distribution of $G ^ { \\pi } ( s )$ , and $\\eta ^ { \\pi } : = \\bar { ( \\eta ^ { \\pi } ( s ) ) } _ { s \\in \\mathcal { S } } ^ { - }$ . The expected return $V ^ { \\pi } ( s ) = \\mathbb { E } G ^ { \\pi } ( s )$ is the value function in the traditional RL setting.\n\n# 2.1 Distributional Bellman Equation and Operator\n\nRecall that the classic policy evaluation aims at computing the value functions $V ^ { \\pi }$ . It is known that $V ^ { \\pi } = \\left( V ^ { \\pi } ( s ) \\right) _ { s \\in { \\cal S } }$ satisfy the Bellman equation. That is, for any $s \\in \\mathcal S$ ,\n\n$$\nV ^ { \\pi } ( s ) = \\left[ T ^ { \\pi } ( V ^ { \\pi } ) \\right] ( s ) = \\mathbb { E } _ { a \\sim \\pi ( \\cdot \\vert s ) , r \\sim \\mathcal { P } _ { R } ( \\cdot \\vert s , a ) , s ^ { \\prime } \\sim P ( \\cdot \\vert s , a ) } \\left[ r + \\gamma V ^ { \\pi } ( s ^ { \\prime } ) \\right] .\n$$\n\nThe operator $T ^ { \\pi } \\colon \\mathbb { R } ^ { s }  \\mathbb { R } ^ { s }$ is called the Bellman operator, and $V ^ { \\pi }$ is a fixed point of $T ^ { \\pi }$ .\n\nThe task of distribution policy evaluation is finding $\\eta ^ { \\pi }$ given some fixed policy $\\pi . ~ \\eta ^ { \\pi }$ satisfies a distributional version of the Bellman equation (1). That is, for any $s \\in \\mathcal S$ ,\n\n$$\n\\begin{array} { r } { \\eta ^ { \\pi } ( s ) = \\left( \\mathcal { T } ^ { \\pi } \\eta ^ { \\pi } \\right) ( s ) = \\mathbb { E } _ { a \\sim \\pi ( \\cdot | s ) , r \\sim \\mathcal { P } _ { R } ( \\cdot | s , a ) , s ^ { \\prime } \\sim P ( \\cdot | s , a ) } \\left[ ( b _ { r , \\gamma } ) _ { \\# } \\eta ^ { \\pi } ( s ^ { \\prime } ) \\right] , } \\end{array}\n$$\n\nwhere $b _ { r , \\gamma } \\colon \\mathbb { R } \\to  { \\mathbb { R } }$ is an affine function defined by $b _ { r , \\gamma } ( x ) = r + \\gamma x$ . And $f _ { \\# } \\mu$ is the push forward measure of $\\mu$ through any function $f : \\mathbb { R } \\to \\mathbb { R }$ , so that $f _ { \\# } \\mu ( A ) = \\mu ( f ^ { - 1 } ( A ) )$ for any Borel set $A$ , where $f ^ { - 1 } ( A ) : = \\{ x \\colon f ( x ) \\in A \\}$ . The operator $\\begin{array} { r } { T ^ { \\pi } \\colon \\Delta \\left( \\left[ 0 , \\frac { 1 } { 1 - \\gamma } \\right] \\right) ^ { s } \\to \\Delta \\left( \\left[ 0 , \\frac { 1 } { 1 - \\gamma } \\right] \\right) ^ { s } } \\end{array}$ is known as the distributional Bellman operator, and $\\eta ^ { \\pi }$ is a fixed point of ${ \\vec { \\tau } } ^ { \\pi }$ . For notational simplicity, we denote $\\Delta \\left( \\left[ 0 , \\frac { 1 } { 1 - \\gamma } \\right] \\right)$ as $\\mathcal { P }$ from now on.\n\n# 2.2 ${ \\mathcal { T } } ^ { \\pi }$ as Contraction in $\\mathcal { P }$\n\nA key property of the Bellman operator $T ^ { \\pi }$ is that it is a $\\gamma$ -contraction w.r.t. the supreme norm (i.e. $\\ell _ { \\infty }$ norm). However, before we can properly discuss the contraction properties of ${ \\mathcal { T } } ^ { \\pi }$ , we need to specify a metric $d$ on $\\mathcal { P }$ . And for any metric $d$ on $\\mathcal { P }$ , we denote $\\bar { d }$ as the corresponding supreme metric on $\\mathcal { P } ^ { s }$ , i.e., $\\begin{array} { r } { \\bar { d } \\left( \\eta , \\eta ^ { \\prime } \\right) : = \\operatorname* { m a x } _ { s \\in \\mathcal { S } } d \\left( \\eta ( s ) , \\eta ^ { \\prime } ( s ) \\right) } \\end{array}$ for any $\\eta , \\eta ^ { \\prime } \\in \\mathcal { P } ^ { S }$ .\n\nSuppose $\\mu$ and $\\nu$ are two probability distributions on $\\mathbb { R }$ with finite $p$ -moments for $\\begin{array} { r l r } { p } & { { } \\in } & { [ 1 , \\infty ] } \\end{array}$ . The $p$ -Wasserstein metric between $\\mu$ and $\\nu$ is defined as $W _ { p } ( \\mu , \\nu ) \\quad : =$ $\\begin{array} { r } { \\left( \\operatorname* { i n f } _ { \\kappa \\in \\Gamma ( \\mu , \\nu ) } \\int _ { \\mathbb { R } ^ { 2 } } | x - y | ^ { p } \\kappa ( d x , d y ) \\right) ^ { 1 / p } } \\end{array}$ . Each element $\\kappa \\in \\Gamma ( \\mu , \\nu )$ is a coupling of $\\mu$ and $\\nu$ , i.e., a joint distribution on $\\mathbb { R } ^ { 2 }$ with prescribed marginals $\\mu$ and $\\nu$ on each “axis.” When $p = 1$ we have $\\begin{array} { r } { \\dot { W _ { 1 } } ( \\mu , \\nu ) = \\int _ { \\mathbb { R } } | F _ { \\mu } ( x ) - F _ { \\nu } ( \\bar { x } ) | d x } \\end{array}$ , where $F _ { \\mu }$ and $F _ { \\nu }$ are the cumulative distribution function of $\\mu$ and $\\nu$ , respectively. It can be shown that ${ \\mathcal { T } } ^ { \\pi }$ is a $\\gamma$ -contraction w.r.t. the supreme $p$ -Wasserstein metric $\\bar { W } _ { p }$ .\n\nProposition 2.1. [Bellemare et al., 2023, Propositions 4.15] The distributional Bellman operator is a $\\gamma$ -contraction on $\\mathcal { P } ^ { S } \\kappa . r . t .$ the supreme $p$ -Wasserstein metric for $p \\in [ 1 , \\infty ]$ . That is, for any $\\eta , \\eta ^ { \\prime } \\in \\mathcal { P } ^ { S }$ , we have $\\bar { W } _ { p } \\left( \\bar { T } ^ { \\pi } \\eta , \\mathcal { T } ^ { \\pi } \\eta ^ { \\prime } \\right) \\leq \\gamma \\bar { W } _ { p } ( \\eta , \\eta ^ { \\prime } )$ .\n\nThe $\\ell _ { p }$ metric between $\\mu$ and $\\nu$ is defined as $\\begin{array} { r } { \\ell _ { p } ( \\mu , \\nu ) = \\left( \\int _ { \\mathbb { R } } | F _ { \\mu } ( x ) - F _ { \\nu } ( x ) | ^ { p } d x \\right) ^ { \\frac { 1 } { p } } } \\end{array}$ for $p \\in [ 1 , \\infty )$ , and ${ \\mathcal { T } } ^ { \\pi }$ is $\\gamma ^ { \\frac { 1 } { p } }$ -contraction w.r.t. the supreme $\\ell _ { p }$ metric $\\bar { \\ell } _ { p }$ .\n\nProposition 2.2. [Bellemare et al., 2023, Propositions 4.20] The distributional Bellman operator is $a \\gamma ^ { \\frac { 1 } { p } }$ -contraction on $\\mathcal { P } ^ { S } \\kappa . r . t .$ . the supreme $\\ell _ { p }$ metric for $p \\in [ 1 , \\infty )$ . That is, for any $\\eta , \\eta ^ { \\prime } \\in \\mathcal { P } ^ { S }$ , we have $\\bar { \\ell } _ { p } \\left( \\mathcal { T } ^ { \\pi } \\eta , \\mathcal { T } ^ { \\pi } \\eta ^ { \\prime } \\right) \\leq \\gamma ^ { \\frac { 1 } { p } } \\bar { \\ell } _ { p } ( \\eta , \\eta ^ { \\prime } )$ .\n\nNote that the $\\ell _ { 1 }$ metric coincides with the 1-Wasserstein metric. And the $\\ell _ { 2 }$ metric is also called the Cramér metric, which plays an important role in subsequent analysis because the zero-mass signed measure space equipped with this metric $( \\mathcal { M } , \\| \\cdot \\| _ { \\ell _ { 2 } } )$ (defined in Section 5.1) is a Hilbert space5. Thereby, we can apply Freedman’s inequality in Hilbert spaces.\n\n# 2.3 Freedman’s Inequality in Hilbert Spaces\n\nJust as Freedman’s inequality is essential for the theory of TD (Theorem 1 in [Li et al., 2024]), a Hilbert space version of Freedman’s inequality is indispensable for deriving the minimax nonasymptotic convergence bound for distributional TD. At the moment, we state a Hilbert space version of the original Freedman’s inequality (Theorem 1.6 in [Freedman, 1975]), and more detailed results can be found in Appendix A.\n\nLet $\\chi$ be a Hilbert space, $\\{ X _ { i } \\} _ { i = 1 } ^ { n }$ be an $\\mathcal { X }$ -valued martingale difference sequence adapted to the filtration $\\left\\{ \\mathcal { F } _ { i } \\right\\} _ { i = 1 } ^ { n }$ , $\\begin{array} { r } { Y _ { i } : = \\sum _ { j = 1 } ^ { i } X _ { j } } \\end{array}$ be the corresponding martingale, and $\\begin{array} { r } { W _ { i } : = \\sum _ { j = 1 } ^ { i } \\sigma _ { j } ^ { 2 } } \\end{array}$ be the corresponding quadratic variation process. Here $\\sigma _ { j } ^ { 2 } : = \\mathbb { E } _ { j - 1 } \\left. X _ { j } \\right. ^ { 2 }$ , and $\\mathbb { E } _ { i } \\left[ \\cdot \\right] : = \\mathbb { E } \\left[ \\cdot | \\mathcal { F } _ { i } \\right]$ denotes the conditional expectation.\n\nTheorem 2.1 (Freedman’s inequality in Hilbert spaces). Suppose $\\mathrm { m a x } _ { i \\in [ n ] } \\| X _ { i } \\| \\leq b$ for some constant $b > 0$ . Then, for any $\\varepsilon$ and $\\sigma > 0$ , the following inequality holds\n\n$$\n\\mathbb { P } \\left( \\exists k \\in [ n ] , s . t . \\ \\| Y _ { k } \\| \\ge \\varepsilon \\ a n d W _ { k } \\le \\sigma ^ { 2 } \\right) \\le 2 \\exp \\left\\{ - \\frac { \\varepsilon ^ { 2 } / 2 } { \\sigma ^ { 2 } + b \\varepsilon / 3 } \\right\\} .\n$$\n\n# 3 Distributional Temporal Difference Learning\n\nIf the MDP $M = \\langle S , A , \\mathcal { P } _ { R } , P , \\gamma \\rangle$ is known, and because $V ^ { \\pi }$ is the fixed point of the contraction $T ^ { \\pi }$ , $V ^ { \\pi }$ can be evaluated via the famous dynamic programming (DP) algorithm. To be concrete, for any initialization $V ^ { ( 0 ) } \\in \\mathbb { R } ^ { S }$ , if we define the iteration sequence $V ^ { ( k + \\bar { 1 } ) } = T ^ { \\pi } ( V ^ { ( k ) } )$ for $k \\in \\mathbb { N }$ , we have $\\begin{array} { r } { \\operatorname* { l i m } _ { k \\to \\infty } \\left\\| V ^ { ( k ) } - V ^ { \\pi } \\right\\| _ { \\infty } = 0 } \\end{array}$ by the contraction mapping theorem (Proposition 4.7 in [Bellemare et al., 2\n023]).\n\nSimilarly, the distributional dynamic programming algorithm defines the iteration sequence as $\\eta ^ { ( k + 1 ) } \\stackrel { \\cdot } { = } \\mathcal { T } ^ { \\pi } \\eta ^ { ( k ) }$ for any initialization $\\boldsymbol { \\eta } ^ { ( 0 ) }$ . In the same way, we have $\\begin{array} { r } { \\operatorname* { l i m } _ { k \\to \\infty } \\bar { W } _ { p } ( \\eta ^ { ( k ) } , \\eta ^ { \\pi } ) = 0 } \\end{array}$ for $p \\in [ 1 , \\infty ]$ and $\\begin{array} { r } { \\operatorname* { l i m } _ { k \\to \\infty } \\bar { \\ell } _ { p } ( \\eta ^ { ( k ) } , \\eta ^ { \\pi } ) = 0 } \\end{array}$ for $p \\in [ 1 , \\infty )$ .\n\nIn most application scenarios, the transition dynamic $P$ and reward distribution $\\mathcal { P } _ { R }$ are unknown, and instead we can only get samples of $P$ and $\\mathcal { P } _ { R }$ in a streaming manner. In this paper, we assume a generative model [Kakade, 2003, Kearns et al., 2002] is accessible, which generates independent samples for all states in each iteration, i.e., in the $t$ -th iteration, we collect sample ${ a _ { t } } \\widehat { ( s ) } \\sim \\pi ( \\cdot | s ) , s _ { t } ( s ) \\sim P ( \\cdot | s , a _ { t } ( s ) ) , r _ { t } ( s ) \\sim \\mathcal { P } _ { R } ( \\cdot | s , a _ { t } ( s ) ) $ for each $s \\in { \\mathcal { S } }$ . Similar to TD [Sutton, 1988] in classic RL, distributional TD also employs the stochastic approximation (SA) [Robbins and Monro, 1951] technique to address the aforementioned problem and can be viewed as an approximate version of distributional DP.\n\nNon-parametric Distributional TD We first introduce non-parametric distributional temporal difference learning (NTD), which is helpful in the theoretical understanding of distributional TD. In the setting of NTD, we assume the return distributions can be precisely updated without any parametrization. For any initialization $\\eta _ { 0 } ^ { \\pi } \\in \\mathcal { P } ^ { S }$ , the updating scheme is given by\n\n$$\n\\eta _ { t } ^ { \\pi } = ( 1 - \\alpha _ { t } ) \\eta _ { t - 1 } ^ { \\pi } + \\alpha _ { t } \\mathcal { T } _ { t } ^ { \\pi } \\eta _ { t - 1 } ^ { \\pi }\n$$\n\nfor any $t \\geq 1$ . Here $\\alpha _ { t }$ is the step size. The empirical Bellman operator at the $t$ -th iteration $\\mathcal { T } _ { t } ^ { \\pi }$ is defined as\n\n$$\n( T _ { t } ^ { \\pi } \\eta ) ( s ) = ( b _ { r _ { t } ( s ) , \\gamma } ) _ { \\# } \\big ( \\eta ( s _ { t + 1 } ) \\big ) ,\n$$\n\nwhich is an unbiased estimator of $( \\mathcal T ^ { \\pi } \\eta ) \\left( s \\right)$ . It is evident that NTD is a SA modification of distributional DP. Consequently, we can analyze NTD using the techniques from the SA area.\n\nCategorical Distributional TD Now, we revisit the more practical CTD. In this case, the updates in CTD is computationally tractable, due to the following categorical parametrization of probability distributions:\n\n$$\n\\mathcal { P } _ { K } : = \\left\\{ \\sum _ { k = 0 } ^ { K } p _ { k } \\delta _ { x _ { k } } : p _ { 0 } , \\dotsc , p _ { K } \\geq 0 , \\sum _ { k = 0 } ^ { K } p _ { k } = 1 \\right\\} ,\n$$\n\nwhere $K \\in \\mathbb { N }$ , and $0 \\leq x _ { 0 } < \\cdot \\cdot \\cdot < x _ { K } \\leq \\frac { 1 } { 1 - \\gamma }$ are fixed points of the support. For simplicity, we assume $\\left\\{ x _ { k } \\right\\} _ { k = 0 } ^ { K }$ are equally-spaced, i.e., $\\begin{array} { r } { x _ { k } = \\frac { k } { K ( 1 - \\gamma ) } } \\end{array}$ . We denote the gap between two points by $\\begin{array} { r } { \\iota _ { K } = \\frac { 1 } { K ( 1 - \\gamma ) } } \\end{array}$ . When updating the return distributions, we need to evaluate the $\\ell _ { 2 }$ -projection of $\\mathcal { P } _ { K }$ , $\\Pi _ { K } \\colon { \\mathcal { P } }  { \\mathcal { P } } _ { K }$ , $\\begin{array} { r } { \\Pi _ { K } \\mu : = \\operatorname * { a r g m i n } _ { \\hat { \\mu } \\in { \\mathcal P } _ { K } } \\ell _ { 2 } ( \\mu , \\hat { \\mu } ) } \\end{array}$ . It can be shown (Proposition 5.14 in [Bellemare et al., 2023]) that the projection is uniquely given by\n\n$$\n\\Pi _ { K } \\mu = \\sum _ { k = 0 } ^ { K } p _ { k } ( \\mu ) \\delta _ { x _ { k } } , \\mathrm { w h e r e } p _ { k } ( \\mu ) = \\mathbb { E } _ { X \\sim \\mu } \\left[ \\left( 1 - \\left| \\frac { X - x _ { k } } { \\iota _ { K } } \\right| \\right) _ { + } \\right] ,\n$$\n\n$( x ) _ { + } : = \\operatorname* { m a x } { \\{ x , 0 \\} }$ for any $x \\in \\mathbb { R }$ . It is known that $\\Pi _ { K }$ is non-expansive w.r.t. the Cramér metric (Lemma 5.23 in [Bellemare et al., 2023]), i.e., $\\ell _ { 2 } ( \\Pi _ { K } \\mu , \\Pi _ { K } \\nu ) \\le \\ell _ { 2 } ( \\mu , \\nu )$ for any $\\mu , \\nu \\in \\mathcal { P }$ . For any $\\eta \\in \\mathcal { P } ^ { s }$ , $s \\in \\mathcal S$ , we slightly abuse the notation and define $\\left( \\Pi _ { K } \\eta \\right) ( s ) : = \\Pi _ { K } \\eta ( s )$ . $\\Pi _ { K }$ is still\n\nnon-expansive w.r.t. $\\bar { \\ell } _ { 2 }$ . Hence $\\mathcal { T } ^ { \\pi , K } : = \\Pi _ { K } \\mathcal { T } ^ { \\pi }$ is a $\\sqrt { \\gamma }$ -contraction w.r.t. $\\bar { \\ell } _ { 2 }$ , we denote its unique fixed point as $\\eta ^ { \\pi , K } \\in \\mathcal { P } _ { K } ^ { S }$ . The approximation error induced by categorical parametrization is given by (Proposition 3 in Rowland et al. [2018])\n\n$$\n\\bar { \\ell } _ { 2 } ( \\eta ^ { \\pi } , \\eta ^ { \\pi , K } ) \\le \\frac { 1 } { \\sqrt { K } ( 1 - \\gamma ) } , \\quad \\bar { W } _ { 1 } ( \\eta ^ { \\pi } , \\eta ^ { \\pi , K } ) \\le \\frac { 1 } { \\sqrt { 1 - \\gamma } } \\bar { \\ell } _ { 2 } ( \\eta ^ { \\pi } , \\eta ^ { \\pi , K } ) \\le \\frac { 1 } { \\sqrt { K } ( 1 - \\gamma ) ^ { 3 / 2 } } .\n$$\n\nNow, we are ready to give the updating scheme of CTD, given any initialization $\\eta _ { 0 } ^ { \\pi } \\in \\mathcal { P } _ { K } ^ { S }$ ,\n\n$$\n\\eta _ { t } ^ { \\pi } = ( 1 - \\alpha _ { t } ) \\eta _ { t - 1 } ^ { \\pi } + \\alpha _ { t } \\Pi _ { K } \\mathcal { T } _ { t } ^ { \\pi } \\eta _ { t - 1 } ^ { \\pi }\n$$\n\nfor any $t \\geq 1$ . We can find that the only difference between CTD and NTD lies in the additional application of the projection operator $\\Pi _ { K }$ at each iteration in CTD.\n\n# 4 Statistical Analysis\n\nIn this section, we state our main results. For both NTD and CTD, we give the non-asymptotic convergence rates of $\\bar { W } _ { p } ( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } )$ and $\\bar { \\ell } _ { 2 } ( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } )$ , respectively.\n\n# 4.1 Non-asymptotic Analysis of NTD\n\nWe first provide a non-asymptotic convergence rate of $\\bar { W } _ { 1 } ( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } )$ for NTD, which is minimax optimal (Theorem B.1) up to logarithmic factors.\n\nTheorem 4.1 (Sample complexity of NTD in the 1-Wasserstein metric). Given any $\\delta \\in ( 0 , 1 )$ and $ { \\varepsilon } \\in ( 0 , 1 )$ , let the initialization be $\\eta _ { 0 } ^ { \\pi } \\in \\mathcal { P } ^ { S }$ , the total update number $T$ satisfy\n\n$$\nT \\geq \\frac { C _ { 1 } \\log ^ { 3 } T } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\log \\frac { | \\mathcal { S } | T } { \\delta }\n$$\n\nfor some large universal constant C1 > 1, i.e., T = O \u0010 ε2(11 γ)3 \u0011, and the step size αt satisfy\n\n$$\n{ \\frac { 1 } { 1 + { \\frac { c _ { 2 } ( 1 - { \\sqrt { \\gamma } } ) t } { \\log t } } } } \\leq \\alpha _ { t } \\leq { \\frac { 1 } { 1 + { \\frac { c _ { 3 } ( 1 - { \\sqrt { \\gamma } } ) t } { \\log t } } } }\n$$\n\nfor some small universal constants $c _ { 2 } > c _ { 3 } > 0$ . Then, with probability at least $1 - \\delta$ , the last iterate estimator satisfies $\\bar { W } _ { 1 } \\left( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } \\right) \\leq \\varepsilon$ .\n\nBecause $\\bar { W } _ { 1 } \\left( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } \\right) \\leq \\frac { 1 } { 1 - \\gamma }$ always holds, we can translate the high probability bound to a mean error bound, that is,\n\n$$\n\\mathbb { E } \\left[ \\hat W _ { 1 } ( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } ) \\right] \\le \\varepsilon ( 1 - \\delta ) + \\frac { \\delta } { 1 - \\gamma } \\le 2 \\varepsilon\n$$\n\nif we take $\\delta \\le \\varepsilon ( 1 - \\gamma )$ . In the subsequent discussion, we will not state the mean error bound conclusions for the sake of brevity.\n\nThe key idea of our proof is to first expand the error term $\\bar { W } _ { 1 } \\left( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } \\right)$ over the time steps. Then it can be decomposed into an initial error term and a martingale term. The initial error term becomes smaller as the iteration goes due to the contraction properties of ${ \\mathcal { T } } ^ { \\pi }$ . To control the martingale term, we first use the basic inequality (Lemma E.1) W1 (µ, ν) ≤ √1 γ , which allows us to analyze this error term in the Hilbert space $( \\mathcal { M } , \\| \\cdot \\| _ { \\ell _ { 2 } } )$ defined in Section 5.1. Consequently, we can bound it using Freedman’s inequality in the Hilbert space (Theorem A.2). A more detailed outline of proof can be found in Section 5.2.\n\nCombining Theorem 4.1 with the basic inequality $\\begin{array} { r } { \\hat W _ { p } ( \\eta , \\eta ^ { \\prime } ) \\le \\frac { 1 } { ( 1 - \\gamma ) ^ { 1 - \\frac { 1 } { p } } } \\hat W _ { 1 } ^ { \\frac { 1 } { p } } ( \\eta , \\eta ^ { \\prime } ) } \\end{array}$ for any $\\eta , \\eta ^ { \\prime } \\in$ $\\mathcal { P } ^ { s }$ (Lemma E.1), we can derive that $\\begin{array} { r } { T = { \\widetilde O } \\left( \\frac { 1 } { \\varepsilon ^ { 2 p } ( 1 - \\gamma ) ^ { 2 p + 1 } } \\right) } \\end{array}$ iterations are sufficient to ensure $\\bar { W } _ { p } ( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } ) \\leq \\varepsilon$ . As pointed out in the exampele after Corollary 3.1 in [Zhang et al., 2023], when $p > 1$ , the slow rate in terms of $\\varepsilon$ is inevitable without additional regularity conditions.\n\nAlthough the 1-Wasserstein metric cannot bound the Cramér metric properly, by making slight modifications to the proof we have the following non-asymptotic convergence rate of $\\bar { \\ell } _ { 2 } ( \\overline { { { \\eta } } } _ { T } ^ { \\pi } , \\overline { { { \\eta } } } ^ { \\pi } )$ . See Appendix C.5 for our proof.\n\nCorollary 4.1 (Sample complexity of NTD in the Cramér metric). Given any $\\delta \\in ( 0 , 1 )$ and $\\varepsilon \\in$ $( 0 , 1 )$ , let the initial value $\\eta _ { 0 } ^ { \\pi } \\in \\mathring { \\mathcal { P } } ^ { s }$ , the total update number $T$ satisfy\n\n$$\nT \\geq \\frac { C _ { 1 } \\log ^ { 3 } T } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 5 / 2 } } \\log \\frac { | \\cal { S } | T } { \\delta }\n$$\n\nfor some large universal constant $C _ { 1 } > 1$ , i.e., $\\begin{array} { r } { T = \\widetilde { O } \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 5 / 2 } } \\right) } \\end{array}$ , and the step size $\\alpha _ { t }$ satisfy\n\n$$\n{ \\frac { 1 } { 1 + { \\frac { c _ { 2 } ( 1 - { \\sqrt { \\gamma } } ) t } { \\log t } } } } \\leq \\alpha _ { t } \\leq { \\frac { 1 } { 1 + { \\frac { c _ { 3 } ( 1 - { \\sqrt { \\gamma } } ) t } { \\log t } } } }\n$$\n\nfor some small universal constants $c _ { 2 } > c _ { 3 } > 0$ . Then, with probability at least $1 - \\delta$ , the last iterate estimator satisfies $\\bar { \\ell } _ { 2 } \\left( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } \\right) \\leq \\varepsilon$ .\n\n# 4.2 Non-asymptotic Analysis of CTD\n\nWe first state a parallel result to Theorem 4.1.\n\nTheorem 4.2 (Sample complexity of CTD in the 1-Wasserstein metric). Given any $\\delta \\in ( 0 , 1 )$ and $ { \\varepsilon } \\in ( 0 , 1 )$ , suppose $\\begin{array} { r } { \\dot { K } > \\frac { \\dot { 4 } } { 1 - \\gamma } } \\end{array}$ , the initial value $\\eta _ { 0 } ^ { \\pi } \\in \\mathcal { P } _ { K } ^ { S }$ , the total update number $T$ satisfies\n\n$$\nT \\geq \\frac { C _ { 1 } \\log ^ { 3 } T } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\log \\frac { | \\mathcal { S } | T } { \\delta }\n$$\n\nfor some large universal constant $C _ { 1 } > 1$ , i.e., $\\begin{array} { r } { T = \\widetilde { O } \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\right) } \\end{array}$ , and the step size $\\alpha _ { t }$ satisfies\n\n$$\n{ \\frac { 1 } { 1 + { \\frac { c _ { 2 } ( 1 - { \\sqrt { \\gamma } } ) t } { \\log t } } } } \\leq \\alpha _ { t } \\leq { \\frac { 1 } { 1 + { \\frac { c _ { 3 } ( 1 - { \\sqrt { \\gamma } } ) t } { \\log t } } } }\n$$\n\nfor some small universal constants $c _ { 2 } > c _ { 3 } > 0$ . Then, with probability at least $1 - \\delta$ , the last iterate estimator satisfies $\\bar { W } _ { 1 } \\left( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi , K } \\right) \\leq \\frac { \\varepsilon } { 2 }$ . Furthermore, according to the upper b−ound (3) of the approximation error $\\bar { W } _ { 1 } \\left( \\eta ^ { \\pi , K } , \\eta ^ { \\pi } \\right)$ , if we take $\\begin{array} { r } { K > \\frac { 4 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } } \\end{array}$ , we have $\\bar { W } _ { 1 } \\left( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } \\right) \\leq \\varepsilon$ .\n\nNote that the order (modulo logarithmic factors) of sample complexity of CTD is better than the previous results of SCPE [Böck and Heitzinger, 2022], and we do not need the additional term introduced in the updating scheme of SCPE.\n\nThe proof of this theorem is almost the same as that of Theorem 4.1, we outline the proof in Section 5.2. The $\\bar { W } _ { 1 }$ metric result can be translated into sample complexity bound $\\begin{array} { r } { \\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 p } ( 1 - \\gamma ) ^ { 2 p + 1 } } \\right) } \\end{array}$ in the $\\bar { W } _ { p }$ metric. We comment that this theoretical result matches the sample complexity bound in the model-based setting [Rowland et al., 2024b].\n\nAs in the NTD setting, we have the following non-asymptotic convergence rate of $\\bar { \\ell } _ { 2 } ( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } )$ as a corollary of Theorem 4.2. See Appendix C.5 for the proof.\n\nCorollary 4.2 (Sample complexity of CTD in the Cramér metric). For any given $\\delta \\in ( 0 , 1 )$ and $ { \\varepsilon } \\in ( 0 , 1 )$ , suppose $\\begin{array} { r } { \\dot { K } > \\frac { 4 } { 1 - \\gamma } } \\end{array}$ , the initialization is $\\eta _ { 0 } ^ { \\pi } \\in \\mathcal { P } _ { K } ^ { S }$ , the total update number $T$ satisfies\n\n$$\nT \\geq \\frac { C _ { 1 } \\log ^ { 3 } T } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 5 / 2 } } \\log \\frac { | \\cal { S } | T } { \\delta }\n$$\n\nfor some large universal constant $C _ { 1 } > 1$ , i.e., $\\begin{array} { r } { T = \\widetilde { O } \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 5 / 2 } } \\right) } \\end{array}$ , and the step size $\\alpha _ { t }$ satisfies\n\n$$\n{ \\frac { 1 } { 1 + { \\frac { c _ { 2 } ( 1 - { \\sqrt { \\gamma } } ) t } { \\log t } } } } \\leq \\alpha _ { t } \\leq { \\frac { 1 } { 1 + { \\frac { c _ { 3 } ( 1 - { \\sqrt { \\gamma } } ) t } { \\log t } } } }\n$$\n\nfor some small universal constants $c _ { 2 } > c _ { 3 } > 0$ . Then, with probability at least $1 - \\delta$ , the last iterate estimator satisfies $\\begin{array} { r } { \\bar { \\ell } _ { 2 } \\left( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi , K } \\right) \\le \\frac { \\varepsilon } { 2 } } \\end{array}$ . Furthermore, according to the upper bound (3) of the approximation error $\\bar { \\ell } _ { 2 } \\left( \\eta ^ { \\pi , K } , \\eta ^ { \\pi } \\right)$ , $i f$ we take $\\begin{array} { r } { K > \\frac { 4 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 2 } } } \\end{array}$ , we have $\\bar { \\ell } _ { 2 } \\left( \\eta _ { T } ^ { \\pi } , \\eta ^ { \\pi } \\right) \\leq \\varepsilon$ .\n\n# 5 Proof Outlines\n\nIn this section, we will outline the proofs of our main theoretical results (Theorem 4.1, Corollary 4.1, Theorem 4.2, and Corollary 4.2). Before diving into the details of the proofs, we first define some notation.\n\n# 5.1 Zero-mass Signed Measure Space\n\nTo analyze the distance between the estimator and the ground-truth $\\eta ^ { \\pi }$ , we will work with the zeromass signed measure space $\\mathcal { M }$ defined as follows\n\n$$\n\\mathcal { M } : = \\left. \\mu : \\mu \\mathrm { ~ i s ~ a ~ s i g n e d ~ m e a s u r e ~ w i t h ~ } | \\mu | \\left( \\mathbb { R } \\right) < \\infty , \\mu ( \\mathbb { R } ) = 0 , \\operatorname { s u p p } ( \\mu ) \\subseteq \\left[ 0 , \\frac { 1 } { 1 - \\gamma } \\right] \\right. ,\n$$\n\nwhere $| \\mu |$ is the total variation measure of $\\mu$ , and $\\operatorname { s u p p } ( \\mu )$ is the support of $\\mu$ . See [Bogachev, 2007] for more details about signed measures.\n\nFor any $\\mu \\in \\mathcal { M }$ , we define its cumulative function as $F _ { \\mu } ( x ) : = \\mu [ 0 , x )$ . We can check that $F _ { \\mu }$ is linear w.r.t. $\\mu$ , that is, $F _ { \\alpha \\mu + \\beta \\nu } = \\alpha F _ { \\mu } + \\beta F _ { \\nu }$ for any $\\alpha , \\beta \\in \\mathbb { R } , \\mu , \\nu \\in \\mathcal { M }$ .\n\nTo analyze the Cramér metric case, we define the following Cramér inner product on $\\mathcal { M }$ :\n\n$$\n\\langle \\mu , \\nu \\rangle _ { \\ell _ { 2 } } : = \\int _ { 0 } ^ { \\frac { 1 } { 1 - \\gamma } } F _ { \\mu } ( x ) F _ { \\nu } ( x ) d x .\n$$\n\nIt is easy to verify that $\\langle \\cdot , \\cdot \\rangle _ { \\ell _ { 2 } }$ is indeed an inner product on $\\mathcal { M }$ . The corresponding norm, called the Cramér norm, is given by $\\begin{array} { r } { \\| \\mu \\| _ { \\ell _ { 2 } } = \\sqrt { \\langle \\mu , \\mu \\rangle _ { \\ell _ { 2 } } } = \\sqrt { \\int _ { 0 } ^ { \\frac { 1 } { 1 - \\gamma } } \\left( F _ { \\mu } ( x ) \\right) ^ { 2 } d x } } \\end{array}$ . We have $\\nu _ { 1 } - \\nu _ { 2 } \\in \\mathcal { M }$ and $\\| \\nu _ { 1 } - \\nu _ { 2 } \\| _ { \\ell _ { 2 } } = \\ell _ { 2 } \\left( \\nu _ { 1 } , \\nu _ { 2 } \\right)$ for any $\\nu _ { 1 } , \\nu _ { 2 } \\in { \\mathcal { P } }$ .\n\nThe $W _ { 1 }$ norm on $\\mathcal { M }$ is defined as $\\begin{array} { r } { \\| \\mu \\| _ { W _ { 1 } } : = \\int _ { 0 } ^ { \\frac { 1 } { 1 - \\gamma } } | F _ { \\mu } ( x ) | d x } \\end{array}$ . We have $\\| \\nu _ { 1 } - \\nu _ { 2 } \\| _ { W _ { 1 } } = W _ { 1 } \\left( \\nu _ { 1 } , \\nu _ { 2 } \\right)$ for any $\\nu _ { 1 } , \\nu _ { 2 } \\in { \\mathcal { P } }$ .\n\nWe can extend the distributional Bellman operator $\\mathcal { T } ^ { \\pi }$ and the Cramér projection operator $\\Pi _ { K }$ naturally to $\\mathcal { M } ^ { s }$ . Here, the product space $\\mathcal { M } ^ { \\bar { s } }$ is also a Banach space, and we use the supreme norm: $\\begin{array} { r } { \\| \\eta \\| _ { \\bar { \\ell } _ { 2 } } : = \\operatorname* { m a x } _ { s \\in \\mathcal { S } } \\| \\eta ( s ) \\| _ { \\ell _ { 2 } } } \\end{array}$ , and $\\lVert \\eta \\rVert _ { \\bar { W } _ { 1 } } : = \\operatorname* { m a x } _ { s \\in \\mathcal { S } } \\left. \\eta ( s ) \\right. _ { W _ { 1 } }$ for any $\\boldsymbol \\eta \\in \\mathcal { M } ^ { s }$ . We denote by $\\boldsymbol { \\mathcal { T } }$ the identity operator in MS .\n\nWhen the norm $\\left\\| \\cdot \\right\\|$ is applied to $A ~ \\in ~ { \\mathcal { L } } ( { \\mathcal { X } } )$ , where $\\mathcal { X }$ is any Banach space, and $\\mathcal { L } ( \\mathcal { X } )$ is the space of all bounded linear operators in $\\mathcal { X }$ , we refer $\\| A \\|$ to the operator norm of $A$ , which is defined as $\\begin{array} { r } { \\| A \\| \\ \\because \\ \\operatorname* { s u p } _ { \\eta \\in { \\mathcal X } , \\| \\eta \\| = 1 } \\| A \\eta \\| } \\end{array}$ . With this notation, $\\begin{array} { r l } { { \\mathcal { L } } ( \\mathcal { X } ) } & { { } = } \\end{array}$ $\\{ A \\colon A$ is a linear operator mapping from $\\mathcal { X }$ to $\\mathcal { X } , a n d \\| A \\| < \\infty \\}$ .\n\nProposition 5.1. ${ \\mathcal { T } } ^ { \\pi }$ and $\\Pi _ { K }$ are linear operators in $\\mathcal { M } ^ { s }$ . Furthermore, $\\begin{array} { r l r } { \\| \\mathcal { T } ^ { \\pi } \\| _ { \\bar { \\ell } _ { 2 } } } & { \\leq } & { \\sqrt { \\gamma } , } \\end{array}$ , $\\lVert \\mathcal { T } ^ { \\pi } \\rVert _ { \\bar { W } _ { 1 } } \\leq \\gamma , \\lVert \\Pi _ { K } \\rVert _ { \\bar { \\ell } _ { 2 } } = 1$ , and $\\Vert \\Pi _ { K } \\Vert _ { \\bar { W } _ { 1 } } \\leq 1$ .\n\nThe proof of the last inequality can be found in the proof of Lemma C.4, while the remaining results are trivial. We omit the proofs for brevity.\n\nMoreover, we have the following matrix (of operators) representations of ${ \\mathcal { T } } ^ { \\pi }$ and $\\Pi _ { K }$ : ${ \\mathcal { T } } ^ { \\pi } \\in$ $\\mathcal { L } ( \\mathcal { M } ) ^ { s \\times \\bar { s } }$ for any $\\boldsymbol \\eta \\in \\mathcal { M } ^ { s }$ ,\n\n$$\n\\left( T ^ { \\pi } \\eta \\right) ( s ) = \\sum _ { a \\in A , s ^ { \\prime } \\in S } \\pi ( a \\mid s ) P ( s ^ { \\prime } \\mid s , a ) \\int _ { 0 } ^ { 1 } ( b _ { r , \\gamma } ) _ { \\# } \\eta ( s ^ { \\prime } ) \\mathcal { P } _ { R } ( d r \\mid s , a ) = \\sum _ { s ^ { \\prime } \\in S } T ^ { \\pi } ( s , s ^ { \\prime } ) \\eta ( s ^ { \\prime } ) ,\n$$\n\nwhere ${ \\mathcal { T } } ^ { \\pi } ( s , s ^ { \\prime } ) \\in { \\mathcal { L } } ( { \\mathcal { M } } )$ for any $\\nu \\in \\mathcal { M }$ ,\n\n$$\n{ \\mathcal T } ^ { \\pi } ( s , s ^ { \\prime } ) \\nu = \\sum _ { a \\in \\mathcal A } \\pi ( a \\mid s ) P ( s ^ { \\prime } \\mid s , a ) \\int _ { 0 } ^ { 1 } ( b _ { r , \\gamma } ) _ { \\# } \\nu { \\mathcal P } _ { R } ( d r \\mid s , a ) .\n$$\n\nIt can be verified that $\\begin{array} { r } { \\| \\mathcal { T } ( s , s ^ { \\prime } ) \\| _ { \\ell _ { 2 } } \\le \\sqrt { \\gamma } \\sum _ { a \\in \\mathcal { A } } \\pi ( a \\mid s ) P ( s ^ { \\prime } \\mid s , a ) = : \\sqrt { \\gamma } P ^ { \\pi } ( s ^ { \\prime } | s ) . } \\end{array}$ . Similarly, $\\Vert \\mathcal { T } ( s , s ^ { \\prime } ) \\Vert _ { W _ { 1 } } \\ \\leq \\ \\gamma P ^ { \\pi } ( s ^ { \\prime } | s )$ , and $\\Pi _ { K } \\ = \\ \\mathrm { d i a g } \\big ( \\Pi _ { K } \\big | _ { \\mathcal { M } } \\big ) _ { s \\in \\mathcal { S } } \\ \\in \\ \\mathcal { L } ( \\mathcal { M } ) ^ { S \\times S }$ . With these representations, $\\Pi _ { K } \\mathcal { T } ^ { \\pi } \\in \\mathcal { L } ( \\mathcal { M } ) ^ { s \\times s }$ can be interpreted as\f matrix multiplication, where the scalar multiplication is replaced by the composition of operators. It can be verified that $\\left( \\Pi _ { K } \\mathcal { T } ^ { \\pi } \\right) \\left( s , s ^ { \\prime } \\right) =$ $\\bar { \\Pi _ { K } } \\mathcal { T } ^ { \\pi } ( s , s ^ { \\prime } )$ , and $\\Vert \\big ( \\Pi _ { K } \\overline { { \\mathcal { T } ^ { \\pi } } } \\big ) \\big ( s , s ^ { \\bar { \\prime } } \\big ) \\Vert _ { \\ell _ { 2 } } \\leq \\sqrt { \\gamma } \\bar { P ^ { \\pi } } \\big ( s ^ { \\prime } | s \\big )$ .\n\nRemark 1: Although the spaces $( \\mathcal { M } , \\| \\cdot \\| _ { \\ell _ { 2 } } )$ and $( \\mathcal { M } , \\| \\cdot \\| _ { W _ { 1 } } )$ are not complete, we will use their completions to replace them without loss of generality, because the completeness property does not affect the non-asymptotic analysis. For simplicity, we still use $\\mathcal { M }$ to denote the completion space. And according to the BLT theorem (Theorem 5.19 in [Hunter and Nachtergaele, 2001]), any bounded linear operator can be extended to the completion space, and still preserves its operator norm.\n\n# 5.2 Analysis of Theorems 4.1 and 4.2\n\nFor simplicity, we abbreviate both $\\lVert \\cdot \\rVert _ { \\bar { \\ell } _ { 2 } }$ and $\\lVert \\cdot \\rVert _ { \\ell _ { 2 } }$ as $\\left\\| \\cdot \\right\\|$ in this part. For all $t ~ \\in ~ [ T ] ~ : =$ $\\{ 1 , 2 , \\cdots , T \\}$ , we denote $\\mathscr { T } _ { t } : = \\mathscr { T } _ { t } ^ { \\pi }$ , $\\mathcal { T } : = \\mathcal { T } ^ { \\pi }$ , $\\eta : = \\eta ^ { \\pi }$ for NTD; $\\mathcal { T } _ { t } : = \\Pi _ { K } \\mathcal { T } _ { t } ^ { \\pi }$ , $\\mathcal { T } : = \\Pi _ { K } \\mathcal { T } ^ { \\pi }$ , $\\overset { \\cdot } { \\eta } : = \\eta ^ { \\pi , K }$ for CTD; and $\\mathbf { \\Psi } _ { t } : = \\eta _ { t } ^ { \\pi } , \\Delta _ { t } : = \\eta _ { t } - \\eta \\in \\mathcal { M } ^ { S }$ for both NTD and CTD. According to Lemma E.2, $\\eta _ { t } \\in \\mathcal { P } ^ { S }$ for NTD and $\\eta _ { t } \\in \\mathcal { P } _ { K } ^ { S }$ for CTD. Our goal is to bound the $\\hat { W } _ { 1 }$ norm of the error term $\\| \\Delta _ { T } \\| _ { \\bar { W } _ { 1 } }$ . This can be achieved by bounding $\\| \\Delta _ { T } \\|$ , as $\\begin{array} { r } { \\| \\Delta _ { T } \\| _ { \\bar { W } _ { 1 } } \\leq \\frac { 1 } { \\sqrt { 1 - \\gamma } } \\| \\Delta _ { T } \\| } \\end{array}$ .\n\nAccording to the updating rule, we have the error decomposition\n\n$$\n\\begin{array} { r l } & { \\Delta _ { t } = \\eta _ { t } - \\eta } \\\\ & { \\quad = ( 1 - \\alpha _ { t } ) \\eta _ { t - 1 } + \\alpha _ { t } \\mathcal { T } _ { t } \\eta _ { t - 1 } - \\eta } \\\\ & { \\quad = ( 1 - \\alpha _ { t } ) \\Delta _ { t - 1 } + \\alpha _ { t } \\left( \\mathcal { T } _ { t } \\eta _ { t - 1 } - \\mathcal { T } \\eta \\right) } \\\\ & { \\quad = ( 1 - \\alpha _ { t } ) \\Delta _ { t - 1 } + \\alpha _ { t } \\left( \\mathcal { T } _ { t } - \\mathcal { T } \\right) \\eta _ { t - 1 } + \\alpha _ { t } \\mathcal { T } \\left( \\eta _ { t - 1 } - \\eta \\right) } \\\\ & { \\quad = \\left[ ( 1 - \\alpha _ { t } ) \\mathcal { T } + \\alpha _ { t } \\mathcal { T } \\right] \\Delta _ { t - 1 } + \\alpha _ { t } \\left( \\mathcal { T } _ { t } - \\mathcal { T } \\right) \\eta _ { t - 1 } . } \\end{array}\n$$\n\nApplying it recursively, we can further decompose the error into two terms\n\n$$\n\\Delta _ { T } = \\underbrace { \\prod _ { t = 1 } ^ { T } \\left[ ( 1 - \\alpha _ { t } ) \\mathcal { I } + \\alpha _ { t } \\mathcal { T } \\right] \\Delta _ { 0 } } _  \\mathrm { ( f ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \\mathrm { ~ } \n$$\n\nTerm (I) is an initial error term that becomes negligible when $T$ is large because $\\tau$ is a contraction. Term (II) can be bounded via Freedman’s inequality in the Hilbert space (Theorem A.2). Combining the two upper bound, we can establish a recurrence relation. Solving this relation will lead to the conclusion.\n\nWe first establish the conclusion for step sizes that depend on $T$ . Specifically, we consider\n\n$$\nT \\geq \\frac { C _ { 4 } \\log ^ { 3 } T } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\log \\frac { | \\mathcal { S } | T } { \\delta } ,\n$$\n\n$$\n\\frac { 1 } { 1 + \\frac { c _ { 5 } ( 1 - \\sqrt { \\gamma } ) T } { \\log ^ { 2 } T } } \\leq \\alpha _ { t } \\leq \\frac { 1 } { 1 + \\frac { c _ { 6 } ( 1 - \\sqrt { \\gamma } ) t } { \\log ^ { 2 } T } } ,\n$$\n\nwhere $c _ { 5 } > c _ { 6 } > 0$ are small constants satisfying $\\begin{array} { l } { c _ { 5 } c _ { 6 } \\ \\leq \\ \\frac { 1 } { 8 } } \\end{array}$ , and $C _ { 4 } > 1$ is a large constant depending only on $c _ { 5 }$ and $c _ { 6 }$ . As shown in Appendix C.1, once we have established the conclusion in this setting, we can recover the original conclusion stated in the theorem.\n\nNow, we introduce the following useful quantities involving step sizes and $\\gamma$\n\n$$\n\\beta _ { k } ^ { ( t ) } : = \\left\\{ \\begin{array} { l l } { \\prod _ { i = 1 } ^ { t } \\left( 1 - \\alpha _ { i } ( 1 - \\sqrt { \\gamma } ) \\right) , } & { \\mathrm { i f ~ } k = 0 , } \\\\ { \\alpha _ { k } \\prod _ { i = k + 1 } ^ { t } \\left( 1 - \\alpha _ { i } ( 1 - \\sqrt { \\gamma } ) \\right) , } & { \\mathrm { i f ~ } 0 < k < t , } \\\\ { \\alpha _ { T } , } & { \\mathrm { i f ~ } k = t . } \\end{array} \\right.\n$$\n\nThe following lemma provides useful bounds for $\\beta _ { k } ^ { ( t ) }$\n\nLemma 5.1. Suppose $\\begin{array} { r } { c _ { 5 } c _ { 6 } \\leq \\frac { 1 } { 8 } } \\end{array}$ . Then, for all $\\begin{array} { r } { t \\geq \\frac { T } { c _ { 6 } \\log T } } \\end{array}$ , we have that\n\n$$\n\\beta _ { k } ^ { ( t ) } \\leq \\frac { 1 } { T ^ { 2 } } , f o r \\ 0 \\leq k \\leq \\frac { t } { 2 } ; \\qquad \\beta _ { k } ^ { ( t ) } \\leq \\frac { 2 \\log ^ { 3 } T } { ( 1 - \\sqrt { \\gamma } ) T } , f o r \\ \\frac { t } { 2 } < k \\leq t .\n$$\n\nThe proof can be found in Appendix C.2. From now on, we only consider t ≥ c6 lTog T .\n\nThe upper bound of term (I) is given by\n\n$$\n( \\mathrm { I } ) \\le \\prod _ { k = 1 } ^ { t } \\| ( 1 - \\alpha _ { k } ) \\mathcal { I } + \\alpha _ { k } \\mathcal { T } | | \\left\\| \\Delta _ { 0 } \\right\\| \\le \\prod _ { k = 1 } ^ { t } \\left( ( 1 - \\alpha _ { k } ) + \\alpha _ { k } \\sqrt { \\gamma } \\right) \\frac { 1 } { \\sqrt { 1 - \\gamma } } = \\frac { \\beta _ { 0 } ^ { ( t ) } } { \\sqrt { 1 - \\gamma } } \\le \\frac { 1 } { \\sqrt { 1 - \\gamma T ^ { 2 } } } ,\n$$\n\nwhere $\\begin{array} { r } { \\| \\Delta _ { 0 } \\| \\le \\sqrt { \\int _ { 0 } ^ { \\frac { 1 } { 1 - \\gamma } } d x } = \\frac { 1 } { \\sqrt { 1 - \\gamma } } } \\end{array}$\n\nAs for term (II), we have the following upper bound with high probability by utilizing Freedman’s inequality (Theorem A.2).\n\nLemma 5.2. For any $\\delta \\in ( 0 , 1 )$ , with probability at least $1 - \\delta$ , we have for all $\\begin{array} { r } { t \\geq \\frac { T } { c _ { 6 } \\log T } } \\end{array}$ , in the NTD case,\n\n$$\n\\begin{array} { r l } & { \\left\\| \\displaystyle \\sum _ { k = 1 } ^ { t } \\alpha _ { k } \\prod _ { i = k + 1 } ^ { t } [ ( 1 - \\alpha _ { i } ) \\mathcal { T } + \\alpha _ { i } \\mathcal { T } ] \\left( \\mathcal { T } _ { k } - \\mathcal { T } \\right) \\eta _ { k - 1 } \\right\\| } \\\\ & { \\leq 3 4 \\sqrt { \\displaystyle \\frac { \\left( \\log ^ { 3 } T \\right) \\left( \\log \\frac { | S | T } { \\delta } \\right) } { ( 1 - \\gamma ) ^ { 2 } T } \\left( 1 + \\displaystyle \\operatorname* { m a x } _ { k : t / 2 < k \\leq t } \\| \\Delta _ { k - 1 } \\| _ { \\bar { W } _ { 1 } } \\right) } . } \\end{array}\n$$\n\nThe conclusion still holds for the CTD case if we take $\\begin{array} { r } { K \\ge \\frac { 4 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 2 } } + 1 } \\end{array}$\n\nThe proof can be found in Appendix C.3. Combining the two results, we find the following recurrence relation in terms of the $\\bar { W } _ { 1 }$ norm holds given the choice of $T$ , with probability at least $1 - \\delta$ , for all t ≥ c6 lTog T\n\n$$\n\\left\\| \\Delta _ { t } \\right\\| _ { \\bar { W } _ { 1 } } \\leq \\frac { 1 } { \\sqrt { 1 - \\gamma } } \\left\\| \\Delta _ { t } \\right\\| \\leq 3 5 \\sqrt { \\frac { \\left( \\log ^ { 3 } T \\right) \\left( \\log \\frac { | \\mathcal { S } | T } { \\delta } \\right) } { ( 1 - \\gamma ) ^ { 3 } T } \\left( 1 + \\operatorname* { m a x } _ { k : t / 2 < k \\leq t } \\left\\| \\Delta _ { k - 1 } \\right\\| _ { \\bar { W } _ { 1 } } \\right) } .\n$$\n\nIn Theorem C.1, we solve the relation and obtain the error bound of the last iterate estimator:\n\n$$\n\\| \\Delta _ { T } \\| _ { \\bar { W } _ { 1 } } \\leq C _ { 7 } \\left( \\sqrt { \\frac { \\left( \\log ^ { 3 } T \\right) \\left( \\log \\frac { | \\mathcal { S } | T } { \\delta } \\right) } { ( 1 - \\gamma ) ^ { 3 } T } } + \\frac { \\left( \\log ^ { 3 } T \\right) \\left( \\log \\frac { | \\mathcal { S } | T } { \\delta } \\right) } { ( 1 - \\gamma ) ^ { 3 } T } \\right) ,\n$$\n\nwhere $C _ { 7 } > 1$ is a large universal constant depending on $c _ { 6 }$ . Now, we can obtain the conclusion if taking $C _ { 4 } \\geq 2 C _ { 7 } ^ { 2 }$ and $\\begin{array} { r } { \\dot { T } \\geq \\frac { C _ { 4 } \\log ^ { 3 } T } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\log \\frac { | S | T } { \\delta } } \\end{array}$ .\n\n# 6 Conclusions\n\nIn this paper we have studied the statistical performance of the distributional temporal difference learning (TD) from a non-asymptotic perspective. Specifically, we have considered two instances of distributional TD, namely, the non-parametric distributional TD (NTD) and the categorical distributional TD (CTD). For both NTD and CTD, we have shown that $\\begin{array} { r } { \\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 p } ( 1 - \\gamma ) ^ { 2 p + 1 } } \\right) } \\end{array}$ iterations are sufficient to achieve a $p$ -Wasserstein $\\varepsilon$ -optimal estimator, which is meinimax optimal (up to logarithmic factors). We have established a novel Freedman’s inequality in Hilbert spaces to prove these theoretical results, which has independent theoretical value beyond the current work. We leave the details to Appendix A.",
    "summary": "{\n    \"core_summary\": \"### 核心概要\\n\\n**问题定义**\\n论文旨在解决分布强化学习（DRL）中分布时间差分学习（distributional TD）的有限样本性能分析问题。在高风险强化学习应用，如医疗和金融领域，仅考虑回报均值不足以应对风险和不确定性，分布强化学习通过对回报的完整分布建模解决此问题。分布时间差分学习是解决分布策略评估问题的常用方法，但以往理论工作主要关注其渐近行为，本文探讨能否用非渐近结果描述其统计效率。\\n\\n**方法概述**\\n提出非参数分布时间差分学习（NTD），用于理论分析，同时重新研究更实用的分类时间差分学习（CTD）。分析二者在有限样本下的性能，证明在 $p$ -Wasserstein 距离下，NTD 需要 $\\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 p } ( 1 - \\gamma ) ^ { 2 p + 1 } } \\right)$ 次迭代以高概率实现 $\\varepsilon$ -最优估计器，且 CTD 与 NTD 有相同的非渐近收敛界。为证明这些结果，建立了新的 Hilbert 空间中的 Freedman 不等式。\\n\\n**主要贡献与效果**\\n- 提出 NTD，证明其在 1-Wasserstein 距离下样本复杂度为 $\\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\right)$ ，该界在忽略对数项时是极小极大最优的。\\n- 重新研究 CTD，证明其在 $p$ -Wasserstein 距离下与 NTD 有相同的非渐近收敛界，且样本复杂度优于之前的快速分类策略评估（SCPE）结果 $\\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 4 } } \\right)$ 。\\n- 建立了 Hilbert 空间中的 Freedman 不等式，用于推导分布 TD 的极小极大非渐近收敛界，具有独立理论价值。\",\n    \"algorithm_details\": \"### 算法/方案详解\\n\\n**核心思想**\\n该方法背后的核心原理是利用随机逼近技术处理分布时间差分学习在转换动态和奖励分布未知情况下的问题。分布时间差分学习是经典 RL 领域中时间差分学习（TD）的扩展，通过迭代更新来估计给定策略 $\\pi$ 的回报分布 $\\eta^{\\pi}$。非参数分布时间差分学习（NTD）假设回报分布可精确更新，而分类时间差分学习（CTD）通过分类参数化使更新在计算上可行。利用 Bellman 算子的收缩性质和 Freedman 不等式在 Hilbert 空间中的应用，来控制误差并证明收敛性。\\n\\n**创新点**\\n先前的工作主要关注分布 TD 的渐近行为，本文从非渐近角度分析其有限样本性能。提出了 NTD 以辅助理论理解，重新研究 CTD 并证明其在非渐近情况下的收敛界，建立了 Hilbert 空间中的 Freedman 不等式，为分析分布 TD 的非渐近收敛界提供了工具。与先前的模型基方法相比，本文分析的分布 TD 作为无模型方法更具实用性。\\n\\n**具体实现步骤**\\n1. **非参数分布时间差分学习（NTD）**：\\n   - 初始化 $\\eta_0^{\\pi} \\in \\mathcal{P}^S$。\\n   - 对于 $t \\geq 1$，更新 $\\eta_t^{\\pi} = (1 - \\alpha_t)\\eta_{t - 1}^{\\pi} + \\alpha_t\\mathcal{T}_t^{\\pi}\\eta_{t - 1}^{\\pi}$，其中 $\\alpha_t$ 是步长，$\\mathcal{T}_t^{\\pi}$ 是第 $t$ 次迭代的经验 Bellman 算子，定义为 $(T_t^{\\pi}\\eta)(s) = (b_{r_t(s),\\gamma})_{\\#}(\\eta(s_{t + 1}))$。\\n2. **分类时间差分学习（CTD）**：\\n   - 对概率分布进行分类参数化，$\\mathcal{P}_K := \\left\\{\\sum_{k = 0}^{K}p_k\\delta_{x_k} : p_0, \\cdots, p_K \\geq 0, \\sum_{k = 0}^{K}p_k = 1\\right\\}$，其中 $K \\in \\mathbb{N}$，$0 \\leq x_0 < \\cdots < x_K \\leq \\frac{1}{1 - \\gamma}$ 是固定的支撑点，假设 $\\left\\{x_k\\right\\}_{k = 0}^{K}$ 等间距，即 $x_k = \\frac{k}{K(1 - \\gamma)}$，记两点间的间距为 $\\iota_K = \\frac{1}{K(1 - \\gamma)}$。\\n   - 初始化 $\\eta_0^{\\pi} \\in \\mathcal{P}_K^S$。\\n   - 对于 $t \\geq 1$，更新 $\\eta_t^{\\pi} = (1 - \\alpha_t)\\eta_{t - 1}^{\\pi} + \\alpha_t\\Pi_K\\mathcal{T}_t^{\\pi}\\eta_{t - 1}^{\\pi}$，其中 $\\Pi_K$ 是 $\\mathcal{P}_K$ 的 $\\ell_2$ -投影算子，定义为 $\\Pi_K\\mu = \\sum_{k = 0}^{K}p_k(\\mu)\\delta_{x_k}$，$p_k(\\mu) = \\mathbb{E}_{X \\sim \\mu}\\left[\\left(1 - \\left|\\frac{X - x_k}{\\iota_K}\\right|\\right)_+\\right]$，$(x)_+ := \\max\\{x, 0\\}$。\\n\\n**案例解析**\\n论文未明确提供此部分信息\",\n    \"comparative_analysis\": \"### 对比实验分析\\n\\n**基线模型**\\n- 离线分布策略评估方法（Wu et al., 2023）\\n- 基于模型的分布策略评估方法（Zhang et al., 2023）\\n- 直接分类不动点计算（DCFP，Rowland et al., 2024b）\\n- 快速分类策略评估（SCPE，Böck and Heitzinger, 2022）\\n- 经典 TD 算法（Li et al., 2024）\\n- 分类时间差分学习（CTD，Rowland et al., 2018）\\n- 分位数时间差分学习（QTD，Rowland et al., 2024a）\\n\\n**性能对比**\\n*   **在样本复杂度（1-Wasserstein 度量）指标上**：本文的 NTD 和 CTD 需要 $\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2p}(1 - \\gamma)^{2p + 1}}\\right)$ 次迭代达到 $\\varepsilon$ -最优估计器，其中 NTD 在 1-Wasserstein 度量下的样本复杂度为 $\\widetilde O \\left( \\frac { 1 } { \\varepsilon ^ { 2 } ( 1 - \\gamma ) ^ { 3 } } \\right)$ ，该界在忽略对数项时是极小极大最优的。与 Zhang et al. (2023) 的 $\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2p}(1 - \\gamma)^{2p + 2}}\\right)$、Rowland et al. (2024b) 的 $\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}(1 - \\gamma)^{3}}\\right)$ 和 Böck and Heitzinger (2022) 的 $\\widetilde{O}\\left(\\frac{1}{\\varepsilon^{2}(1 - \\gamma)^{4}}\\right)$ 相比，本文的 CTD 在不引入额外加速技术的情况下，具有更优的样本复杂度。\\n*   **在非渐近收敛界指标上**：本文证明了在 $p$ -Wasserstein 度量下，CTD 和 NTD 具有相同的非渐近收敛界，而先前的工作主要关注渐近收敛，本文在这方面实现了从渐近到非渐近的突破。\",\n    \"keywords\": \"### 关键词\\n\\n- 分布强化学习 (Distributional Reinforcement Learning, DRL)\\n- 分布策略评估 (Distributional Policy Evaluation, N/A)\\n- 分布时间差分学习 (Distributional Temporal Difference Learning, NTD, CTD)\\n- 非参数分布时间差分学习 (Non - parametric Distributional Temporal Difference Learning, NTD)\\n- 分类时间差分学习 (Categorical Temporal Difference Learning, CTD)\\n- Freedman 不等式 (Freedman's Inequality, N/A)\\n- Hilbert 空间 (Hilbert Spaces, N/A)\\n- $p$ -Wasserstein 距离 ($p$ -Wasserstein Distance, N/A)\"\n}"
}