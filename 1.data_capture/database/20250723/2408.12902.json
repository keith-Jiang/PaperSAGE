{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.12902",
    "link": "https://arxiv.org/abs/2408.12902",
    "pdf_link": "https://arxiv.org/pdf/2408.12902.pdf",
    "title": "IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities",
    "authors": [
        "Bin Wang",
        "Chunyu Xie",
        "Dawei Leng",
        "Yuhui Yin"
    ],
    "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
    ],
    "publication_date": "2024-08-23",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "360 AI Research"
    ],
    "paper_content": "# IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities\n\nBin Wang\\*, Chunyu Xie\\*, Dawei Leng‚Ä†, Yuhui Yin\n\n360 AI Research wangbin10, xiechunyu, lengdawei, yinyuhui $@ 3 6 0 . \\mathrm { c n }$\n\n# Abstract\n\nIn the field of multimodal large language models (MLLMs), common methods typically involve unfreezing the language model during training to foster profound visual understanding. However, the fine-tuning of such models with visionlanguage data often leads to a diminution of their natural language processing (NLP) capabilities. To avoid this performance degradation, a straightforward solution is to freeze the language model while developing multimodal competencies. Unfortunately, previous works have not attained satisfactory outcomes. Building on the strategy of freezing the language model, we conduct thorough structural exploration and introduce the Inner-Adaptor Architecture (IAA). Specifically, the architecture incorporates multiple multimodal adaptors at varying depths within the large language model to facilitate direct interaction with the inherently text-oriented transformer layers, thereby enabling the frozen language model to acquire multimodal capabilities. Unlike previous approaches of freezing language models that require large-scale aligned data, our proposed architecture is able to achieve superior performance on small-scale datasets. We conduct extensive experiments to improve the general multimodal capabilities and visual grounding abilities of the MLLM. Our approach remarkably outperforms previous state-of-the-art methods across various vision-language benchmarks without sacrificing performance on NLP tasks. Code and models will be released.\n\n# Introduction\n\nLarge Language Models (LLMs) have made substantial progress in recent years, largely attributed to the technique of pre-training and instruction tuning. Building upon this foundation, visual instruction tuning has been proposed to evolve LLMs into Multimodal Large Language Models (MLLMs), thereby endowing them with the capability to interpret and comprehend visual signals (Cha et al. 2024). MLLMs (Liu et al. 2024b; Bai et al. 2023; Tong et al. 2024; Xuan et al. 2024; Huang et al. 2023a) prove beneficial in numerous tasks, such as transcribing the text within an image, generating stories and poems based on an image, or converting screenshots of webpages into code (Laurenc¬∏on et al.\n\n![](images/68ce972adb14b43f6d95de57f5fda05e39478009e95276de521c66dcb6f8e69f.jpg)  \nFigure 1: Results before and after training LLaVA-1.5 architecture based on Qwen2 and Llama3 language models on text-only evaluation set MMLU and C-eval.\n\n2024). Historically, these tasks have been regarded as challenging for conventional vision-language models. MLLMs exhibit considerable promise in executing these complex, diverse real-world tasks, enabling more natural and humanlike interactions (Lu et al. 2024).\n\nTypically, the operation of a MLLM begins with feeding an image into a visual encoder, such as CLIP (Radford et al. 2021) or SigLIP (Zhai et al. 2023), to extract a highdimensional feature representation. This feature is subsequently transformed through a projection layer to align with the dimension of the large language model. The resulting features, often referred to as image tokens, are concatenated with text tokens and fed into the large language model. This process enables the MLLM to generate responses based on user instructions and input images.\n\nIn the current common MLLM (Liu et al. $2 0 2 4 \\mathrm { a }$ ; Bai et al. 2023), when image and text tokens are fed into the large language model, the LLM is typically unfrozen for further training. This strategy has led to significant advancements in the MLLM model. Consequently, it predictably leads to a degradation in the understanding ability of the large language model. To validate this hypothesis, we conduct experiments on the LLaVA-1.5 (Liu et al. 2024a) architecture using the 1.2M-size open-source dataset provided by (Liu et al. 2024a), which contains a limited amount of plain text data, as illustrated in Figure 1. We compare the results before and after training the LLaVA-1.5 architecture, based on the Qwen2 (Yang et al. 2024) and Llama3 (Meta 2024) language models, respectively. The performance of the language model declines significantly on both the MMLU (Hendrycks et al. 2020) and C-Eval (Huang et al. 2023b) text-only evaluation sets.\n\nIt appears reasonable to posit an explanation for this phenomenon within the field of deep learning. When a model is predominantly trained on a single type of data, it may experience a phenomenon known as catastrophic forgetting. For an MLLM to achieve outstanding image-text comprehension, it is essential to collect a substantial amount of image-text interaction data for training. As observed in Figure 1, training with image-text data results in a decline in language ability. Despite attempts by MLLM such as LLaVA to incorporate some text-only data into their training process, this still leads to a reduction in the model‚Äôs comprehension.\n\nOne direct method to prevent the degradation of LLM performance is to freeze the large language model during the training of MLLM. However, current methods employing this approach (Li et al. 2023a; Zhu et al. 2023) have consistently struggled to achieve powerful multimodal capabilities. To address these challenges, we propose a new training paradigm with an inner-adaptor architecture that significantly enhances multimodal competencies without affecting the original language modeling capabilities. This approach can seamlessly support both multimodal and textual workflows. We evaluate this training paradigm across a spectrum of tasks, including general multimodal capabilities and visual grounding proficiencies. Distinct from previous approaches of freezing language modeling that require large-scale aligned data, our proposed scheme demonstrates effectiveness with a considerably smaller dataset. Comprehensive testing on a suite of benchmarks, including MME, MMBench, MMMU, and RefCOCO, has substantiated the superior performance of our structure. We hope that this approach will provide a reference for future research in opensource MLLM.\n\n# Related Work\n\nLarge Language Models. The landscape of Natural Language Processing (NLP) has undergone a revolutionary transformation, driven by the advent and continuous refinement of Large Language Models (LLMs). A pivotal moment in this evolution is the first appearance of the transformer architecture, which serves as a key catalyst, giving rise to pioneering language models like BERT (Devlin et al. 2018) and OPT (Zhang et al. 2022). These models showcase an unprecedented level of linguistic comprehension, significantly advancing the state-of-the-art in NLP. A critical breakthrough comes with introducing the Generative Pretrained Transformer (GPT) series (Brown et al. 2020), which pioneer an auto-regressive language modeling approach, setting a new standard for language prediction and generation capabilities. Subsequent iterations, including Mixtral (Jiang et al. 2024), GPT-4 (Achiam et al. 2023), and Llama3 (Meta 2024), have not only maintained but also amplified this momentum, displaying superior performance on intricate language processing challenges. Moreover, the fusion of LLMs with specialized visual tasks showcases the models‚Äô adaptability and broadens their scope, indicating their potential to transcend conventional text-based operations into multimodal interactions. This expansion highlights the transformative role LLMs can assume when incorporated into diverse domains, providing a rich ground for innovation and exploration.\n\nMultimodal Large Language Models. The advancement of Large Language Models (LLMs) has kindled a growing interest in extending their foundational competencies to incorporate the visual domain, thereby giving birth to multimodal Large Language Models (MLLMs). The works on MLLMs (Li et al. 2023a; Bai et al. 2023; Liu et al. 2024b; Lauren¬∏con et al. 2024; Xie et al. 2023; Li et al. 2023b) typically follow a tripartite architecture: a visual encoder, a vision-language connector, and a large language model. Notably, BLIP-2 (Li et al. 2023a) and Flamingo (Alayrac et al. 2022) introduce the Q-Former/Resampler as a bridge between vision and language, whereas LLaVA (Liu et al. 2024b) and MiniGPT4 (Zhu et al. 2023) refine this connection via a linear layer. Cambrian-1 (Tong et al. 2024) proposes a dynamically adaptive connector that integrates high-resolution visual features with LLMs while reducing the number of tokens. To enhance their multimodal performance, contemporary MLLMs mainly fine-tune the LLM and connector using visual instruction tuning data. These models leverage meticulously curated instruction datasets, showcasing an effective strategy that highlights their robust capabilities. However, a common oversight lies in the maintenance of language abilities. Long term multimodal training often leads to degradation of language proficiency. $\\mathrm { C o g V L M }$ (Wang et al. 2023) seeks to address this by integrating a trainable visual expert into the language model, but still trains the LLM during supervised fine-tuning, resulting in a degradation of language capability. DeekSeekVL (Lu et al. 2024) maintains a $70 \\%$ proportion of language data to preserve the integrity of language knowledge within the model, but incurs a considerable training cost. Departing from these conventional training paradigms of MLLMs, we introduce the inner-adaptor architecture. This design is specifically tailored to preserve the NLP performance of the MLLM while facilitating a seamless augmentation of its multimodal capabilities.\n\n# Methodology\n\nOverview. As illustrated in Figure 2, our approach enables the simultaneous execution of two high-quality workflows post-deployment: one for multimodal interactions and the other for text-only conversations. Both workflows leverage the transformer layers of the large language model. The multimodal interaction workflow encompasses: (1) an image encoder and a projector, utilized for extracting high-quality image features and achieving vision-language alignment, respectively, (2) the transformer layers of the large language model, which remain frozen during training, and (3) the inner-adaptor architecture, which comprises insertion layers, an embedding layer, and a language model head specifically designed for multimodal inputs. Conversely, the text-only conversation workflow solely employs the constituent elements of the original language model, without resorting to the specialized multimodal components.\n\n![](images/82ec5ad9781bdb1bcc41733452a1ce5af931c8f1b25649d7d0cc238b93516001.jpg)  \nFigure 2: Overview of the proposed architecture, which mainly consists of two workflows: the Multimodal Workflow and the Text-only Workflow. The multimodal workflow, beyond the necessary image encoder and projector, integrates the Inner-Adaptor Architecture, including insertion layers, an embedding layer, and a language model head. Both workflows share the same large language model. The number of insertion layers is variable, where $N \\leq M$ . In this context, MM denotes MultiModal, $E L$ stands for Embedding Layer, and $L H$ represents the Language model Head.\n\nImage Encoder and Projector. Following LLava-1.5 (Liu et al. 2024a), we utilize the CLIP ViT-L/14 (Radford et al. 2021) image encoder with an input resolution of 336px. Subsequently, we employ a vision-language projector composed of a two-layer MLP to integrate the vision features with LLMs.\n\nLarge Language Model. We employ the Llama3-8B (Meta 2024) as the base language model throughout the training process.\n\nInner-Adaptor Architecture. To achieve multimodal comprehension, it is essential to integrate trainable parameters into MLLMs. LLaVA (Liu et al. 2024b) makes the projector and the large language model trainable during visual instruction tuning, but leads to the performance degradation on NLP tasks. Flamingo (Alayrac et al. 2022) employs cross-attention with a gating mechanism to introduce image information into the model, facilitating a deep fusion of original image features with text features prior to each layer of the language model. However, this approach requires a considerable volume of pre-training data to train effective cross-attention layers and gating values, which can be computationally costly. Furthermore, the final performance of the model falls short of expectations.\n\nDrawing insights from recent works (Zhang et al. 2020; Chen et al. 2024; Tong et al. 2024), we recognize that the self-attention layer can assimilate image features as prior prompts, thus eliminating the necessity of cross-attention for the obligatory incorporation of image features. In alignment with this perspective, we embark on exploratory research. Referencing Figure 3(a), we are inspired by the prevalent ControlNet (Zhang, Rao, and Agrawala 2023) architecture. The operation of a specific layer can be succinctly expressed as follows:\n\n$$\nX _ { o u t } = \\phi _ { f l } ( X _ { i n } ) + G ( \\phi _ { i l } ( X _ { i n } ) ) ,\n$$\n\nwhere $\\phi _ { f l }$ and $\\phi _ { i l }$ denote the frozen language model (LM) layer and the insertion layer, respectively.\n\nHere, $X _ { i n }$ represents the multimodal input, $X _ { o u t }$ denotes the multimodal output, and $G$ indicates a gating layer initialized at zero. The insertion layer is a transformer decoder layer, comprising the self-attention layer, layer normalization, feed forward network, etc. It is consistent with the parameter scale of a transformer layer in the large language model. For instance, if we target the $2 2 t h$ layer, the initial parameters of the corresponding insertion layer are derived from the $2 2 t h$ language model layer. Nonetheless, the ControlNet-based design did not yield satisfactory performance.\n\nReferring to Figure 3(b), we endeavor to refine the ControlNet structure. Specifically, we eliminate the feature propagation between insertion layers. Instead, the output of the LM layer serves as the input to the insertion layer. Our expectation is that each frozen LM layer will accommodate multimodal data through a distinct insertion layer and gating layer, with the insertion layer no longer being directly influenced by subsequent layers. Compared to the design in\n\n![](images/66825caca1f6060478eacf3922f5440dafeab1c0c0fe34f3708f1b2fdf90b64b.jpg)  \nFigure 3: Structural exploration of the Inner-Adaptor Architecture. Figure (a) is a architecture inspired by the ControlNet design; Figure (b) is an improvement on Figure (a), mainly canceling the feature propagation between adaptors; Figure (c) is the final scheme.\n\n<html><body><table><tr><td>Configurations</td><td>Satge1-PT</td><td>Satge2-PT</td></tr><tr><td>Trainablemodules</td><td>Projector</td><td>Projector, Inner-adaptor</td></tr><tr><td>Learning rate</td><td>1e-3</td><td>2e-5</td></tr><tr><td>Batch size LR schedule</td><td></td><td>256</td></tr><tr><td></td><td></td><td>Cosine decay</td></tr><tr><td>Training steps</td><td></td><td>2.5K</td></tr><tr><td>Zero-Stage</td><td></td><td>Zero2</td></tr><tr><td>Warmup ratio Weight decay</td><td></td><td>0.03</td></tr><tr><td></td><td></td><td>0.0</td></tr><tr><td>Optimizer</td><td></td><td>AdamW</td></tr><tr><td>Optimizer HPs</td><td></td><td>Œ≤1=0.9,Œ≤2=0.98,‚àà=1e-6</td></tr><tr><td>Configurations</td><td>Instruction-FT</td><td>Grounding-FT</td></tr><tr><td>Trainable modules</td><td></td><td>Projector, Inner-adaptor</td></tr><tr><td>Learning rate</td><td></td><td>2e-5</td></tr><tr><td>Batch size</td><td></td><td>128</td></tr><tr><td>LR schedule</td><td></td><td>Cosine decay</td></tr><tr><td>Training steps</td><td>6.6K</td><td>18K</td></tr><tr><td>Zero-Stage</td><td></td><td>Zero3</td></tr><tr><td>Warmup ratio</td><td></td><td>0.03</td></tr><tr><td>Weight decay</td><td></td><td>0.0</td></tr><tr><td>Optimizer</td><td></td><td>AdamW</td></tr><tr><td>Optimizer HPs</td><td></td><td>Œ≤1=0.9,Œ≤2=0.98,‚àà=1e-6</td></tr></table></body></html>\n\nTable 1: The hyperparameters utilized during the training phase are delineated as follows: ‚Äù-PT‚Äù designates the pretraining phase, ‚Äù-FT‚Äù denotes the fine-tuning phase, and ‚ÄùHP‚Äù and ‚ÄùLR‚Äù signify the hyperparameter and learning rate, respectively.\n\nFigure 3(a), the refined architecture shows significant improvements.\n\nMoreover, we hypothesize that the gating layer may not reach an optimal state through a single round of data training. Consequently, we propose a more streamlined solution, as illustrated in Figure 3(c). The operation of a specific layer within the model can be represented as follows:\n\n$$\nX _ { o u t } = \\phi _ { i l } ( \\phi _ { f l } ( X _ { i n } ) ) .\n$$\n\nSimilar to Scheme (a), if an insertion layer is placed after the $2 2 t h$ LM layer, it is initialized from the parameters of the $2 2 t h$ frozen LM layer. The number of insertion layers is adjustable.\n\nAdditionally, for multimodal training, we introduce a new embedding layer $E L _ { m m }$ and a new LM head $L H _ { m m }$ , initialized from the original language model‚Äôs embedding layer $E L _ { t e x t }$ and LM head $L H _ { t e x t }$ . Throughout all stages of multimodal training, $E L _ { t e x t }$ and $L H _ { t e x t }$ will remain frozen, while the newly created components will be trained with multimodal data. The experimental results presented in Table 5 validate the effectiveness of this strategy.\n\nWe thoroughly explore the distinctions among these architectures and strategies in the ablation study. Ultimately, we select the structure depicted in Figure 3(c), which we designate as the Inner-Adaptor Architecture (IAA).\n\n# Experiments\n\n# Training Paradigm\n\nPre-training. During the training process of MLLM, the primary objective of the pre-training phase is to enable MLLM to learn the alignment between visual cues and textual descriptions. This stage, also known as the image-text alignment phase, establishes connections between the vision encoder and LLM. In our architectural design, the image encoder and LLM remain frozen throughout all training phases to preserve the inherent foundational knowledge in both vision and language models. The projector and inner-adapter architecture require training to enhance multimodal capabilities. Our empirical investigations reveal that for the inneradaptor architecture, applying a high learning rate can lead to overflow in training loss. To alleviate this issue, we devise a dual-stage pre-training procedure.\n\nIn the first pre-training stage, the model configuration consists of only three components: the image encoder, the projector, and the large language model. The parameters of the image encoder and the large language model are frozen, while a high learning rate of 0.001 is utilized to train a highquality projector.\n\nIn the second pre-training stage, the model architecture is expanded to incorporate the inner-adaptor for multimodal tasks. The training parameters now include both the projector and the newly integrated structures. The projector is initialized with the parameters derived from the preceding stage. For this stage, a lower learning rate of 2e-5 is adopted.\n\nThroughout the pre-training stages, the dataset employed consists of $5 5 8 \\mathrm { k }$ image-text aligned pairs sourced from (Liu et al. 2024b) and an additional 100K pairs from (Chen et al.\n\n<html><body><table><tr><td>Method</td><td>Vision Encoder</td><td>Language Model</td><td>Data Scale</td><td>MMEP</td><td>MMB-ENT</td><td>MMB-CNT</td><td>MMMUy</td></tr><tr><td colspan=\"8\">TrainingwiththeLLMunfrozen</td></tr><tr><td>mPLUG-Owl(Ye et al. 2023)</td><td>CLIP-ViT-L</td><td>Llama2 (7B)</td><td>1.1B</td><td>967.3</td><td>49.4</td><td>Ôºå</td><td>-</td></tr><tr><td>Qwen-VL-Chat (Bai et al. 2023)</td><td>CLIP-ViT-G</td><td>Qwen (7B)</td><td>1.5B</td><td>1487.6</td><td>61.8</td><td>56.3</td><td>37</td></tr><tr><td>CogVLM (Wang et al. 2023)</td><td>EVA2-CLIP-ViT-E</td><td>Vicuna-v1.5 (7B)</td><td>1.5B</td><td>1439.7</td><td>65.8</td><td>55.9</td><td>37.3</td></tr><tr><td>mPLUG-Owl2 (Ye et al. 2024)</td><td>CLIP-ViT-L</td><td>Llama2 (7B)</td><td>400M</td><td>1450.2</td><td>66.0</td><td>60.3</td><td>34.7</td></tr><tr><td>LLaVA-1.5(Liu et al. 2024b)</td><td>CLIP-ViT-L</td><td>Vicuna-v1.5 (7B)</td><td>1.2M</td><td>1510.7</td><td>66.5</td><td>59.0</td><td>35.7</td></tr><tr><td>LLaVA-1.5(Liu et al. 2024b)</td><td>CLIP-ViT-L</td><td>Vicuna-v1.5 (13B)</td><td>1.2M</td><td>1531.3</td><td>69.2</td><td>65.0</td><td>37.0</td></tr><tr><td>Honeybee (Cha et al.2024)</td><td>CLIP-ViT-L</td><td>Vicuna-v1.5 (7B)</td><td>208M</td><td>1584.2</td><td>70.1</td><td>-</td><td>-</td></tr><tr><td>Yi-VL (AI et al. 2024)</td><td>CLIP-ViT-H</td><td>Yi (6B)</td><td>125M</td><td></td><td>68.4</td><td>66.6</td><td>39.1</td></tr><tr><td>DeepSeek-VL (Lu et al. 2024)</td><td>SAM-B and SigLIP-L</td><td>DeepSeek (7B)</td><td>103M</td><td></td><td>73.8</td><td>71.4</td><td>36.6</td></tr><tr><td>LLaVA-Llama3 (Contributors 2024)</td><td>CLIP-ViT-L</td><td>Llama3 (8B)</td><td>1.2M</td><td>1506.0</td><td>68.9</td><td>61.6</td><td>36.8</td></tr><tr><td colspan=\"8\">TrainingwiththeLLMfrozen</td></tr><tr><td>OpenFlamingov2 (Awadalla et al.2023)</td><td>CLIP-ViT-L</td><td>MPT (7B)</td><td>3B</td><td>-</td><td>5.7</td><td>14.4</td><td>28.8</td></tr><tr><td>Llama-AdapterV2(Gao et al.2023)</td><td>CLIP-ViT-L</td><td>Llama2 (7B)</td><td>0.6M</td><td>972.7</td><td>41.0</td><td></td><td>-</td></tr><tr><td>MiniGPT-4 (Zhu et al. 2023)</td><td>EVA-CLIP-ViT-G</td><td>Vicuna (13B)</td><td>5.1M</td><td>866.6</td><td></td><td></td><td>-</td></tr><tr><td>BLIP-2 (Li et al. 2023a)</td><td>EVA-CLIP-ViT-G</td><td>FlanT5XXL</td><td>129M</td><td>1293.8</td><td>-</td><td></td><td>-</td></tr><tr><td>InstructBLIP (Dai et al. 2023)</td><td>EVA-CLIP-ViT-G</td><td>Vicuna (13B)</td><td>130M</td><td>1212.8</td><td>44.0</td><td></td><td></td></tr><tr><td>IAA-8‚Ä†</td><td>CLIP-ViT-L</td><td>Llama3 (8B)</td><td>1.2M</td><td>1560.2</td><td>69.9</td><td>64.2</td><td>37.1</td></tr><tr><td>IAA-8</td><td>CLIP-ViT-L</td><td>Llama3 (8B)</td><td>1.5M</td><td>1581.8</td><td>72.7</td><td>69.2</td><td>39.8</td></tr><tr><td>IAA-14</td><td>CLIP-ViT-L</td><td>Llama3 (8B)</td><td>1.5M</td><td>1591.5</td><td>74.9</td><td>70.5</td><td>39.9</td></tr></table></body></html>\n\nTable 2: Results on general multimodal benchmarks, where the data scale of 1.2M uniformly represents the data provided b LLaVA (Liu et al. 2024b). IAA- ${ \\cdot } 8 ^ { \\dagger }$ represents the model trained using 1.2M data.\n\nTable 3: Comparison on Text-only Benchmarks. IAA- ${ \\cdot } 8 ^ { \\dagger }$ denotes the model trained using the same 1.2M data as LLaVALlama3. IAA- ${ \\cdot } 8 ^ { \\dagger }$ is not impaired in terms of NLP ability, but LLaVA-Llama3 presents deteriorated results.   \n\n<html><body><table><tr><td>Method</td><td>MMLU‚Üë</td><td>C-Eval ‚Üë</td><td>BBH‚Üë</td><td>Humaneval ‚Üë</td><td>Math ‚Üë</td></tr><tr><td>LLaVA-Llama3</td><td>55.8</td><td>40.5</td><td>44.6</td><td>38.4</td><td>12.3</td></tr><tr><td>IAA-8‚Ä†</td><td>68.4</td><td>51.3</td><td>52.8</td><td>59.2</td><td>27.8</td></tr></table></body></html>\n\n2024). (Chen et al. 2024) provides a total of 664K image-text aligned data. We translate the first $1 0 0 \\mathrm { k }$ pairs into Chinese and incorporated them into the training process to fortify the model‚Äôs understanding of Chinese tasks. Over the course of these stages, we utilize a cumulative total of 658K data pairs.\n\nInstruction Fine-tuning. We perform instruction finetuning based on the model obtained from the second pretraining stage. Throughout this stage, the parameters of the large language model and the image encoder remain frozen. The dataset includes the fine-tuning dataset of 665K samples proposed by (Liu et al. 2024b), along with additional datasets including DocVQA (50K) (Mathew, Karatzas, and Jawahar 2021), VSR (10K) (Liu, Emerson, and Collier 2023), ScienceQA (21K) (Lu et al. 2022), and an in-house dataset (78.5K). Similar to the pre-training stage, we translate the first 40K entries of the 664K fine-tuning data proposed by (Chen et al. 2024) into Chinese and incorporate them into the instruction fine-tuning dataset. The aggregate quantity of data utilized in this stage amounts to 865K.\n\nGrounding Fine-tuning. Building upon the model finetuned with instructions, we further train a model specialized in visual grounding. The data used in this stage comprises RefCOCO (Kazemzadeh et al. 2014), COCO (Lin et al. 2014), Flickr30k Entities (Plummer et al. 2015), Objects365 (Shao et al. 2019), aggregating to approximately 2M data instances. These datasets improves the model‚Äôs capability of localizing fine-grained visual details. The inclusion of COCO and Objects365 assists the model in improving its ability to localize multiple targets.\n\nImplementation details. The detailed training information is summarized in Table 1, mainly covering the hyperparameters used during the four-stage training process. The entire four-stage can be executed on a single node ${ \\bf A 8 0 0 \\times 8 }$ in 48 hours. All experiments utilize the zero technology provided by Deepspeed and the flash-attention v2.\n\n# Experimental Results\n\nMain Results on General Multimodal Benchmarks. To assess the multimodal capabilities of our approach, we employ widely recognized benchmarks that are closely related to multimodal tasks: MMEP (Fu et al. 2023), MMBenchENT (Liu et al. 2023), MMBench-CNT (Liu et al. 2023), and MMMUv (Yue et al. 2024). These benchmarks are renowned for presenting significant challenges across a diverse range of practical tasks. For evaluation purposes, we adhere to a zero-shot testing protocol, a strict methodology that tests models on unseen data without additional training. Moreover, we categorize comparative methods into two distinct categories: those trained with a frozen language model and those trained with an unfrozen language model. To provide a comprehensive analysis, we show the scale of the data utilized for each method, along with the variations in the image encoders employed. Detailed results of our evaluations are tabulated in Table 2. To ensure a fair and equitable comparison, we choose methods that leverage a base language model with a comparable parameter scale, and the reported metrics for competing methods are based solely on officially published data, avoiding any local testing results.\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Grounding Data Scale</td><td colspan=\"3\">RefCOCO</td><td colspan=\"3\">RefCOCO+</td><td colspan=\"2\">RefCOCOg</td></tr><tr><td>val</td><td>testA</td><td>testB</td><td>val</td><td>testA</td><td>testB</td><td>val</td><td>test</td></tr><tr><td>KOSMOS-2 (Peng et al. 2023)</td><td>20M</td><td>52.3</td><td>57.4</td><td>47.3</td><td>45.5</td><td>50.7</td><td>42.2</td><td>60.6</td><td>61.7</td></tr><tr><td>OFA-L (Wang et al. 2022)</td><td>10M</td><td>80.0</td><td>83.7</td><td>76.4</td><td>68.3</td><td>76.0</td><td>61.8</td><td>67.6</td><td>67.6</td></tr><tr><td>Shikra (Chen etal.2023b)</td><td>4M</td><td>87.0</td><td>90.6</td><td>80.2</td><td>81.6</td><td>87.4</td><td>72.1</td><td>82.3</td><td>82.2</td></tr><tr><td>MiniGPT-v2 (Chen et al. 2023a)</td><td>~21M</td><td>88.7</td><td>91.7</td><td>85.3</td><td>80.0</td><td>85.1</td><td>74.5</td><td>84.4</td><td>84.7</td></tr><tr><td>Ferret (You et al. 2023)</td><td>8.7M</td><td>87.5</td><td>91.4</td><td>82.5</td><td>80.1</td><td>87.4</td><td>73.1</td><td>83.9</td><td>84.8</td></tr><tr><td>PINK(Xuan et al. 2024)</td><td>5M</td><td>88.7</td><td>92.1</td><td>84.0</td><td>81.8</td><td>88.2</td><td>73.9</td><td>83.9</td><td>84.3</td></tr><tr><td>IAA-8</td><td>2M</td><td>89.2</td><td>92.6</td><td>83.7</td><td>82.1</td><td>88.6</td><td>73.7</td><td>84.4</td><td>84.7</td></tr><tr><td>IAA-14</td><td>2M</td><td>90.2</td><td>92.9</td><td>85.4</td><td>83.4</td><td>89.0</td><td>76.7</td><td>85.0</td><td>85.1</td></tr></table></body></html>\n\nTable 4: Comparisons on visual grounding benchmarks. Our approach achieves competitive performance trained on relativel limited datasets.\n\nOwing to the inherent strengths of our proposed architecture, our method exhibits substantial superiority over those trained with frozen language model. As the current mainstream approach, models trained with unfrozen language models typically achieve better multimodal performance, albeit at the cost of diminished NLP capabilities. We list several state-of-the-art methods adhering to this training paradigm. Compared to Honeybee (Cha et al. 2024), YiVL (AI et al. 2024), and Deepseek-VL (Lu et al. 2024), our method achieves competitive or even superior performance on certain metrics, with an extremely small training data scale. Using the same data scale of 1.2 million, IAA-8 outperforms LLaVA-Llama3. Additionally, IAA-14 with 14 insertion layers achieves better results than IAA-8 with an 8- layer configuration. Furthermore, we compare our approach with LLaVA-Llama3 (Contributors 2024) on NLP benchmarks, including MMLU and C-Eval. The results of NLP benchmarks are presented in Table 3. Our language model is not impaired in terms of NLP ability, but LLaVA-Llama3 trained on the same data shows deteriorated results on both MMLU and C-Eval. Our method surpasses it across all metrics, indicating that our architecture is superior to the mainstream LLaVA architecture.\n\nResults on Visual Grounding Benchmarks. To evaluate the effectiveness of our model in the visual grounding task, we perform evaluations utilizing the widely accepted benchmarks RefCOCO (Kazemzadeh et al. 2014), $\\operatorname { R e f C O C O + }$ (Yu et al. 2016), and RefCOCOg (Mao et al. 2016), with the corresponding results illustrated in Table 4. The methods for comparison are all models trained for the grounding task under an auto-regressive strategy. The results reveal that our method is capable of achieving competitive performance, even when trained on relatively limited datasets. In our analysis, to ensure fairness, we exclude models trained on extremely large-scale datasets, such as $\\mathrm { C o g V L M }$ -grounding (Wang et al. 2023) with 1.5B imagetext pairs and 40M grounding data, as well as those leveraging pre-trained object detection models, exemplified by LLaVA-Grounding (Zhang et al. 2023) and Groma (Ma et al. 2024).\n\nEfficiency in Deployment. Currently, high-performance multimodal models typically require the unfreezing of the large language model for training. $\\mathrm { C o g V L M }$ (Wang et al. 2023) highlights the substantial difficulty in developing a model that excels in both multimodal comprehension and visual grounding tasks simultaneously. To address this, it adopts a dual-model strategy, specifically training one model for general multimodal capabilities and another for visual grounding abilities. In this context, deploying a high-quality language model, a multimodal model with outstanding general performance, and a model endowed with proficient visual grounding skills concurrently on a single GPU would demand an estimated 50GB of memory. Our proposed approach, facilitated by the inner-adaptor architecture, ingeniously combines superior general multimodal competencies and robust visual grounding capacities, while concurrently safeguarding the inherent prowess of the original large language model. Specifically, with an 8-layer inner-adaptor configuration, our model exhibits a significantly reduced memory footprint, hovering around 30GB.\n\n# Ablation Study\n\nStructure Analysis. In the exploration of the structure, we furnish quantitative results for validation in Table 5. With an 8-layer insertion scheme as our baseline configuration, we observe that incremental architectural enhancements consistently improve performance metrics across the board. Specifically, the comparison between rows 1, 2, and 4 highlights the benefits of architectural refinement. Moreover, the contrast between rows 3 and 4 demonstrates that the integration of a specialized embedding layer and language model head for multimodal data processing significantly boosts performance.\n\nTable 5: Ablation study for the exploration of inner-adaptor related structures.   \n\n<html><body><table><tr><td rowspan=\"2\">Model architecture</td><td colspan=\"5\">I-LayersTrablemd Hmm</td><td rowspan=\"2\">MMEP</td><td rowspan=\"2\">MMB-ENT</td><td rowspan=\"2\">MMB-CNT</td><td rowspan=\"2\">MMMUV</td></tr><tr><td>Projector</td><td></td><td></td><td></td><td>Zero-Gates</td></tr><tr><td>Figure 3(a)</td><td></td><td></td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>1425.4</td><td>72.4</td><td>65.0</td><td>38.2</td></tr><tr><td>Figure 3(b)</td><td>>>></td><td>>></td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>1556.0</td><td>72.7</td><td>68.5</td><td>39.6</td></tr><tr><td>Figure 3(c)</td><td></td><td>‚àö</td><td>√ó</td><td>√ó</td><td>√ó</td><td>1563.4</td><td>72.6</td><td>68.7</td><td>39.6</td></tr><tr><td>Figure 3(c)</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>√ó</td><td>1581.8</td><td>72.7</td><td>69.2</td><td>39.8</td></tr></table></body></html>\n\nTable 6: Comparison of the training stages.   \n\n<html><body><table><tr><td colspan=\"3\">Training Stages</td><td rowspan=\"2\">MMEP</td><td rowspan=\"2\">MMMUV</td></tr><tr><td>Satge1-P</td><td>Satge2-P</td><td>Instruction-F</td></tr><tr><td>√ó</td><td>‚àö</td><td>‚àö</td><td>1512.1</td><td>39.3</td></tr><tr><td>‚àö</td><td>√ó</td><td>‚àö</td><td>1565.4</td><td>39.5</td></tr><tr><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>1581.8</td><td>39.8</td></tr></table></body></html>\n\nTable 7: Ablations on the number of insertion layers.   \n\n<html><body><table><tr><td>Number of I-Layers</td><td>MMEP</td><td>MMB-ENT</td><td>MMB-CNT</td></tr><tr><td>8</td><td>1581.8</td><td>72.7</td><td>69.2</td></tr><tr><td>14</td><td>1591.5</td><td>74.9</td><td>70.5</td></tr><tr><td>22</td><td>1531.7</td><td>76.0</td><td>70.4</td></tr></table></body></html>\n\nComparison of Training Stages. Through empirical evidence detailed in Table 6, we validate the effectiveness of our two-stage pre-training methodology. It can be observed that the model lacking the first stage of alignment training exhibits notably poorer performance. When the projector and insertion layers are engaged in joint pre-training, it is essential to maintain a learning rate of approximately 2e-5 to prevent loss overflow. However, this strategy leads to suboptimal alignment training for the projector, which negatively affects the model‚Äôs final performance.\n\nFurthermore, although the model performs adequately when skipping the second pre-training stage, it ultimately fails to replicate the outstanding results achievable through the complete two-stage pre-training process. This disparity emphasizes the critical significance of the additional pretraining stage in enhancing the model‚Äôs overall effectiveness.\n\nImpact of Insertion Layer Quantities. We explore the effect of varying numbers of insertion layers, which are presented in Table 7. The experimental results indicate that increasing the number of insertion layers from 8 to 14 yields enhancements in all performance metrics. However, it is imperative to acknowledge that an increase in insertion layers simultaneously impacts the model‚Äôs efficiency. We advocate that an 8-layer configuration is adequate to effectively address foundational requirements.\n\nTraining Data Influence Assessment. To delineate the impact of data on model performance, we present comparative results in Table 8. The baseline, outlined in the first row, showcases the performance of LLaVA-Llama3 (Contributors 2024) utilizing the LLaVA architecture and the 1.2 million dataset provided by (Liu et al. 2024b). Subsequent experimentation, as delineated in the second row, emphasizes the pronounced superiority of our proposed architecture over LLaVA. Additionally, we enrich the training corpus with an extra 0.3 million records, mainly encompassing Chinese data. As a result, our model achieves substantial improvements in all metrics, especially on the Chinese evaluation set MMBench-CNT.\n\nTable 8: The impact of the training data.   \n\n<html><body><table><tr><td></td><td>MMEP</td><td>MMB-ENT</td><td>MMB-CNT</td></tr><tr><td>LLaVA-Llama3 (1.2M)</td><td>1506.0</td><td>68.9</td><td>61.6</td></tr><tr><td>IAA-8 (1.2M)</td><td>1560.2</td><td>69.9</td><td>64.2</td></tr><tr><td>IAA-8 (1.5M)</td><td>1581.8</td><td>72.7</td><td>69.2</td></tr></table></body></html>\n\nLimitations The method of extending multimodal capabilities by freezing the language model will introduce certain additional parameters. Compared to the approach of training with an unfrozen language model, the inference speed of the model will be reduced. To mitigate this issue, we extend the key-value cache mechanism to the insertion layers. Based on the MME dataset, compared to the LLaVA architecture, the average inference time of our 8-layer structure increases from 0.103s to 0.124s, which we deem to be within a relatively reasonable range.\n\n# Conclusion\n\nIn this paper, we introduce the Inner-Adaptor Architecture, which is designed to enhance the general multimodal and visual grounding capabilities of LLMs. Through a series of architectural exploration experiments, we demonstrate that training with a frozen language model can surpass the multimodal performance of the models with fine-tuned LLMs. Our proposed model has achieved state-of-the-art performance across a multitude of publicly available evaluation datasets. Moreover, after deployment, our approach incorporates dual workflows, thereby preserving the NLP proficiency of the language model. The flexibility of the InnerAdaptor Architecture provides the potential for extension to additional modalities, which is a direction for future exploration.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ÂΩìÂâçÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÈÄöÂ∏∏ÈúÄË¶ÅËß£ÂÜªËØ≠Ë®ÄÊ®°Âûã‰ª•‰øÉËøõËßÜËßâÁêÜËß£Ôºå‰ΩÜËøô‰ºöÂØºËá¥Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâËÉΩÂäõÁöÑÊòæËëó‰∏ãÈôçÔºàÂ¶ÇÂõæ1ÊâÄÁ§∫ÔºåLLaVA-1.5Âú®MMLUÂíåC-Eval‰∏äÁöÑÊÄßËÉΩ‰∏ãÈôçÔºâ„ÄÇ\\n> *   Ëøô‰∏ÄÈóÆÈ¢òÈôêÂà∂‰∫ÜÊ®°ÂûãÂú®ÈúÄË¶ÅÂêåÊó∂Â§ÑÁêÜÂ§öÊ®°ÊÄÅÂíåÁ∫ØÊñáÊú¨‰ªªÂä°ÁöÑÂ∫îÁî®Âú∫ÊôØ‰∏≠ÁöÑË°®Áé∞ÔºåÂ¶ÇËßÜËßâÈóÆÁ≠î„ÄÅÂõæÂÉèÊèèËø∞ÁîüÊàêÁ≠âÔºå‰∏îÁé∞ÊúâÂÜªÁªìËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñπÊ≥ïÔºàÂ¶ÇBLIP-2„ÄÅMiniGPT-4ÔºâÊó†Ê≥ïËææÂà∞Êª°ÊÑèÁöÑÂ§öÊ®°ÊÄÅÊÄßËÉΩ„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÜÖÈÉ®ÈÄÇÈÖçÂô®Êû∂ÊûÑÔºàInner-Adaptor Architecture, IAAÔºâÔºåÈÄöËøáÂú®ËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏çÂêåÊ∑±Â∫¶ÊèíÂÖ•Â§öÊ®°ÊÄÅÈÄÇÈÖçÂô®Â±ÇÔºå‰ΩøÂÜªÁªìÁöÑËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÁõ¥Êé•‰∏éÊñáÊú¨ÂØºÂêëÁöÑTransformerÂ±Ç‰∫§‰∫íÔºå‰ªéËÄåÂú®‰∏çÂΩ±ÂìçNLPÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãËé∑ÂæóÂ§öÊ®°ÊÄÅËÉΩÂäõ„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫IAAÊû∂ÊûÑÔºåÈ¶ñÊ¨°ÂÆûÁé∞‰∫ÜÂú®ÂÆåÂÖ®ÂÜªÁªìËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªÖÈúÄ1.5MÂ∞èËßÑÊ®°Êï∞ÊçÆÈõÜÂç≥ÂèØË∂ÖË∂ä‰∏ªÊµÅËß£ÂÜªËÆ≠ÁªÉÊñπÊ≥ïÁöÑÊÄßËÉΩÔºàÂ¶ÇMMEP 1591.5ÂàÜ vs LLaVA-Llama3 1506.0ÂàÜÔºâ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ÈÄöËøáÂèåÈò∂ÊÆµÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÔºàÂÖàÊäïÂΩ±Â±ÇÂêéÈÄÇÈÖçÂô®Â±ÇÔºâÂíåÊñ∞ÂûãÊèíÂÖ•Â±ÇËÆæËÆ°ÔºàÂõæ3cÔºâÔºåËß£ÂÜ≥‰∫Ü‰º†ÁªüControlNetÁªìÊûÑÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÁì∂È¢à„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** Âú®‰øùÊåÅNLPÊÄßËÉΩÔºàMMLU 68.4ÂàÜ vs ÂéüÂßãLlama3ÔºâÁöÑÂêåÊó∂ÔºåËßÜËßâÂÆö‰Ωç‰ªªÂä°ÔºàRefCOCO testAÔºâËææÂà∞92.9%ÂáÜÁ°ÆÁéáÔºå‰ªÖÈúÄ2MÊï∞ÊçÆÂç≥Ë∂ÖË∂äÈúÄË¶Å20MÊï∞ÊçÆÁöÑKOSMOS-2Ôºà57.4%Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   IAAÁöÑÊ†∏ÂøÉÂéüÁêÜÊòØÈÄöËøáÊ®°ÂùóÂåñÈÄÇÈÖçÂô®ÂÆûÁé∞Â§öÊ®°ÊÄÅ‰∏éÊñáÊú¨‰ªªÂä°ÁöÑÂπ∂Ë°åÂ§ÑÁêÜÔºö\\n>     *   Â§öÊ®°ÊÄÅÂ∑•‰ΩúÊµÅÔºöÂõæÂÉèÁºñÁ†ÅÂô®‚ÜíÊäïÂΩ±Â±Ç‚ÜíÂÜªÁªìLLM+ÊèíÂÖ•Â±Ç‚Üí‰∏ìÁî®ÂµåÂÖ•Â±ÇÔºàEL_mmÔºâÂíåËØ≠Ë®ÄÊ®°ÂûãÂ§¥ÔºàLH_mmÔºâ\\n>     *   ÊñáÊú¨Â∑•‰ΩúÊµÅÔºöÁõ¥Êé•‰ΩøÁî®ÂéüÂßãLLMÁªÑ‰ª∂\\n> *   ËÆæËÆ°Âì≤Â≠¶ÊòØÈÅøÂÖçÁâπÂæÅ‰º†Êí≠Âπ≤Êâ∞ÔºàÂõæ3b‚Üí3cÊîπËøõÔºâÔºåÈÄöËøáÂàùÂßãÂåñÊèíÂÖ•Â±ÇÂèÇÊï∞‰∏éÂØπÂ∫îLLMÂ±Ç‰∏ÄËá¥ÔºàÂ¶ÇÁ¨¨22Â±ÇÈÄÇÈÖçÂô®ÂàùÂßãÂåñËá™Á¨¨22Â±ÇLLMÂèÇÊï∞ÔºâÊù•‰øùËØÅÊ®°ÊÄÅÂÖºÂÆπÊÄß„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** LLaVAÁ≠âÈúÄËß£ÂÜªÊï¥‰∏™LLMÂØºËá¥NLPÊÄßËÉΩ‰∏ãÈôçÔºõFlamingo‰æùËµñÂ§ßËßÑÊ®°Êï∞ÊçÆËÆ≠ÁªÉË∑®Ê≥®ÊÑèÂäõÂ±ÇÔºõBLIP-2ÂÜªÁªìLLM‰ΩÜÂ§öÊ®°ÊÄÅÊÄßËÉΩÂèóÈôê„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** \\n>     1.  ÂèñÊ∂àÈÄÇÈÖçÂô®Â±ÇÈó¥ÁâπÂæÅ‰º†Êí≠ÔºàÂõæ3cÔºâÔºåÊîπÁî®LLMÂ±ÇËæìÂá∫Áõ¥Êé•‰Ωú‰∏∫ÊèíÂÖ•Â±ÇËæìÂÖ•\\n>     2.  ÂºïÂÖ•Èõ∂ÂàùÂßãÂåñÁöÑÈó®ÊéßÂ±Ç$G$ÔºàÂÖ¨Âºè1ÔºâÈÄêÊ≠•ÊøÄÊ¥ªÂ§öÊ®°ÊÄÅÁâπÂæÅ\\n>     3.  Âèåembedding/headÊú∫Âà∂ÔºàEL_mm/LH_mmÔºâÈöîÁ¶ªÊ®°ÊÄÅÂπ≤Êâ∞\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.   **ÂõæÂÉèÂ§ÑÁêÜÔºö** CLIP ViT-L/14Ôºà336pxÔºâÁºñÁ†ÅÂõæÂÉè‚Üí‰∏§Â±ÇMLPÊäïÂΩ±ÔºàÂ≠¶‰π†Áéá1e-3È¢ÑËÆ≠ÁªÉÔºâ\\n> 2.   **ÈÄÇÈÖçÂô®ÊèíÂÖ•Ôºö** Âú®LLaMA3-8BÁöÑÈÄâÂÆöÂ±ÇÂêéÊèíÂÖ•ÂàùÂßãÂåñËá™ÂêåÂ±ÇÁöÑTransformer decoderÂ±ÇÔºàÂê´self-attention/FFNÔºâ\\n> 3.   **Â§öÊ®°ÊÄÅËÆ≠ÁªÉÔºö** ‰ªÖËÆ≠ÁªÉÊäïÂΩ±Â±Ç+ÈÄÇÈÖçÂô®+EL_mm/LH_mmÔºàÂ≠¶‰π†Áéá2e-5ÔºâÔºåÂÜªÁªìÂéüÂßãEL_text/LH_text\\n> 4.   **ÂèåÈò∂ÊÆµËÆ≠ÁªÉÔºö** ÂÖàÁî®558K+100K‰∏≠Ëã±ÂØπÈΩêÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉÊäïÂΩ±Â±ÇÔºåÂÜçÁî®865KÊåá‰ª§Êï∞ÊçÆÂæÆË∞ÉÈÄÇÈÖçÂô®\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   Ë°®5ÊòæÁ§∫Ôºö‰ªéControlNetÁªìÊûÑÔºàÂõæ3aÔºâÂà∞ÊúÄÁªàÊñπÊ°àÔºàÂõæ3cÔºâÔºåMMEPÂàÜÊï∞‰ªé1425.4ÊèêÂçáËá≥1581.8ÔºõÊ∑ªÂä†EL_mm/LH_mmËøõ‰∏ÄÊ≠•‰ΩøMMB-CNT‰ªé68.7ÊèêÂçáËá≥69.2„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   Ëß£ÂÜªLLMÁªÑÔºöLLaVA-1.5„ÄÅQwen-VL-Chat„ÄÅCogVLM„ÄÅDeepSeek-VL\\n> *   ÂÜªÁªìLLMÁªÑÔºöBLIP-2„ÄÅMiniGPT-4„ÄÅOpenFlamingov2\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®MMEPÔºàÂ§öÊ®°ÊÄÅÁªºÂêàËØÑ‰º∞Ôºâ‰∏äÔºö** IAA-14ËææÂà∞1591.5ÂàÜÔºåÊòæËëó‰ºò‰∫éÂÜªÁªìLLMÁöÑBLIP-2Ôºà1293.8ÂàÜÔºâÂíåËß£ÂÜªLLMÁöÑLLaVA-1.5-13BÔºà1531.3ÂàÜÔºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂÜªÁªìÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá297.7ÂàÜ„ÄÇ\\n> *   **Âú®MMBench-ENTÔºàÁªÜÁ≤íÂ∫¶ËØÜÂà´Ôºâ‰∏äÔºö** IAA-14ÁöÑ74.9ÂàÜË∂ÖË∂äÊâÄÊúâÂü∫Á∫øÔºåÂåÖÊã¨ÈúÄ1.5BÊï∞ÊçÆÁöÑCogVLMÔºà65.8ÂàÜÔºâÂíå70.1ÂàÜÁöÑHoneybee„ÄÇ\\n> *   **Âú®RefCOCO testAÔºàËßÜËßâÂÆö‰ΩçÔºâ‰∏äÔºö** ‰ªÖÁî®2MÊï∞ÊçÆÁöÑIAA-14ËææÂà∞92.9%ÂáÜÁ°ÆÁéáÔºå‰ºò‰∫éÈúÄ21MÊï∞ÊçÆÁöÑMiniGPT-v2Ôºà91.7%ÔºâÂíå8.7MÊï∞ÊçÆÁöÑFerretÔºà91.4%Ôºâ„ÄÇ\\n> *   **Âú®NLP‰ªªÂä°ÔºàMMLUÔºâ‰∏äÔºö** ‰øùÊåÅÂéüÂßãLlama3ÁöÑ68.4ÂàÜÔºåËÄåLLaVA-Llama3‰∏ãÈôçËá≥55.8ÂàÜÔºàË°®3Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã (Multimodal Large Language Model, MLLM)\\n*   ÂÜÖÈÉ®ÈÄÇÈÖçÂô®Êû∂ÊûÑ (Inner-Adaptor Architecture, IAA)\\n*   ËßÜËßâÂÆö‰Ωç (Visual Grounding, N/A)\\n*   ËØ≠Ë®ÄÊ®°ÂûãÂÜªÁªì (Frozen Language Model, N/A)\\n*   Â∞èËßÑÊ®°Êï∞ÊçÆÈõÜ (Small-scale Dataset, N/A)\\n*   ÂèåÈò∂ÊÆµËÆ≠ÁªÉ (Two-stage Training, N/A)\\n*   Ê®°ÊÄÅÈöîÁ¶ª (Modality Isolation, N/A)\\n*   ÁÅæÈöæÊÄßÈÅóÂøò (Catastrophic Forgetting, N/A)\"\n}\n```"
}