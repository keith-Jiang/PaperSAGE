{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.15279",
    "link": "https://arxiv.org/abs/2412.15279",
    "pdf_link": "https://arxiv.org/pdf/2412.15279.pdf",
    "title": "Functional connectomes of neural networks",
    "authors": [
        "Tananun Songdechakraiwut",
        "Yutong Wu"
    ],
    "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
    ],
    "publication_date": "2024-12-18",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Biology"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Duke University"
    ],
    "paper_content": "# Functional Connectomes of Neural Networks\n\nTananun Songdechakraiwut, Yutong Wu\n\nDepartment of Computer Science, Duke University t.song@duke.edu, mason.wu@duke.edu\n\n# Abstract\n\nThe human brain is a complex system, and understanding its mechanisms has been a long-standing challenge in neuroscience. The study of the functional connectome, which maps the functional connections between different brain regions, has provided valuable insights through various advanced analysis techniques developed over the years. Similarly, neural networks, inspired by the brainâ€™s architecture, have achieved notable success in diverse applications but are often noted for their lack of interpretability. In this paper, we propose a novel approach that bridges neural networks and human brain functions by leveraging brain-inspired techniques. Our approach, grounded in the insights from the functional connectome, offers scalable ways to characterize topology of large neural networks using stable statistical and machine learning techniques. Our empirical analysis demonstrates its capability to enhance the interpretability of neural networks, providing a deeper understanding of their underlying mechanisms.\n\n# Code â€” https://github.com/masonwu11/topo-fcnn Extended version â€” https://arxiv.org/abs/2412.15279\n\n# 1 Introduction\n\nThe human brain is an incredibly complex system, and understanding its intricate workings has been a long-standing challenge in neuroscience. One well-established approach to gaining deeper insights into the brainâ€™s underlying mechanisms is through the study of the functional connectome, which maps the functional connections between regions of brain networks and reflects the brainâ€™s dynamic network of interactions (Bullmore and Sporns 2009). In recent decades, successful findings have emerged from this field, thanks to the development of a wide array of analysis techniques.\n\nArtificial neural networks, inspired by the architecture and functioning of the human brain, have achieved remarkable success in applications ranging from image recognition (He et al. 2016) to natural language processing (Vaswani et al. 2017). However, despite these successes, neural networks are often considered black box models due to their lack of interpretability and the difficulty in understanding the underlying mechanisms driving their performance. Neural networks have predesigned architectures with preconfigured weights connecting neurons, analogous to how physically connected brain regions with white matter fibers provide structural information measured by diffusion MRI. Similarly, functional connections between distant neurons resemble how functional MRI measures functional connectivity between brain regions that may not have direct neuroanatomical connections. These connections in the brain give rise to coordinated activity patterns crucial for cognitive processes (Honey et al. 2009). Given that neural networks are simplified artificial versions of brain functions, it stands to reason that insights from the functional connectome could be leveraged to enhance our understanding, interpretability, and analysis of these networks, potentially leading to the development of more transparent and efficient models.\n\nHowever, analyzing functional connectomes is inherently challenging due to the need to extract subtle topological patterns from noisy, complete graphs. Therefore, typical workflows apply a threshold to obtain a sparser graph with a clearer structure before applying techniques from graph theory (Bullmore and Sporns 2009). Graph theory has played a crucial role in functional connectome research; however, prior analyses utilizing graph theory have primarily focused on pairwise dyadic relationships, often at a fixed spatial threshold. This approach, centered on dyads, limits the neural structures and functions that graph theory can investigate. Given that a neural network processes information not only based on local neurons of a subnetwork but also across the entire network, from input to output layers, a more comprehensive understanding requires a shift in perspectiveâ€“ from pairwise interactions to capturing higher-order relations (topology) across the full range of spatial resolutions.\n\nPersistent homology (Edelsbrunner and Harer 2022), an algebraic topology technique, has emerged as a promising tool for understanding and quantifying the topology of the human brain (Sizemore et al. 2018; Songdechakraiwut, Shen, and Chung 2021). Recently, there have been increasing attempts to apply persistent homology to study the interpretability of deep learning. These studies suggest that persistent homology can extract high-order topological information to interpret neural networks, but challenges remain. In particular, current methods often focus solely on the networkâ€™s structural information, without incorporating data-driven tasks (Rieck et al. 2019; Watanabe and Yamana\n\n![](images/87b0322b77ec53ed1af26e5cb2c306a57386a5e4175c3e84e37bedc1692c23ed.jpg)  \nFigure 1: A schematic for extracting persistent graph homology, representing the topology of neural-network-derived function connectomes.\n\n2022), and are typically limited to models with a small number of neurons (Naitzat, Zhitnikov, and Lim 2020; Zhang et al. 2023). Notably, Zhang et al. has applied persistent homology to study the functional connectivities of neural networks. However, their approach is still limited by the cubic time complexity of persistent homology techniques (Otter et al. 2017), making them computationally infeasible for large, complete graphs, and relies heavily on approximation solutions. The authors addressed this challenge by thresholding functional connectivities at a predefined threshold, which limits the spatial resolution of their analysis to a range of pre-determined values, rather than capturing the entire global mechanisms of neural network functions. Importantly, varying threshold values can significantly alter the graph structure, affecting its robustness and sensitivity to signals, and potentially influencing the studyâ€™s final outcome.\n\nPersistent graph homology (Songdechakraiwut and Chung 2023), a recent persistent homology advancement inspired by the human brain, captures interpretable topological invariants of great interest in functional brain networks, including connected components and cycles, across the entire spatial scale of a graph. Connected components characterize the network shape (Tewarie et al. 2015), while cycles represent higher-order interactions potentially associated with information propagation and feedback control (Kwon and Cho 2007). The computability of persistent graph homology enables the analysis of intricate details in large-scale, complex brain networks. Motivated by this, we adopt a braininspired graph homology perspective to overcome computational limitations and fully exploit the potential of topological analyses on large-scale neural network functions.\n\nIn this paper, we propose a novel analysis framework that bridges neural networks and human brain functions by leveraging brain-inspired techniques from functional MRI and persistent graph homology. Our framework, grounded in insights from the functional connectome, addresses the aforementioned challenges and offers scalable methods to characterize the topology of functional mechanisms of large neural networks using stable statistical and machine learning techniques. We define functional connectomes, a representation used to describe functions in neural networks, without the need for predefined threshold values, thereby significantly improving the scientific rigor of the analysis. We also demonstrate the computability and efficiency of statistics for such representations via analytic forms. Specifically, we present closed-form computations for the Wasserstein distance, Wasserstein statisticsâ€“including barycenter (average) and varianceâ€“and the Wasserstein gradient, enabling the realization of the robustness benefits of true Wasserstein distance, grounded in the central stability theorem (Skraba and Turner 2023). Utilizing these computable statistics naturally leads to the development of a centroid-based clustering strategy, where the Wasserstein barycenter serves as the topological centroid. We conducted extensive experiments using this method to validate that our framework can indeed enhance our ability to discern and interpret the complex structure of neural network functions, providing new avenues for both analysis and application.\n\n# 2 Functional Connectome Framework\n\nFigure 1 shows a schematic of the functional connectivity analysis framework, described in the following.\n\n# Functions of Neural Networks\n\nGiven a collection of data samples, we partition it into two separate datasets: a training dataset and a functional dataset. The training dataset is utilized via $k$ -fold cross-validation to determine a set of optimal hyperparameter values through grid search. Using these optimal values, we train a wellgeneralized neural network on the entire training data. Subsequently, the functional dataset is fed into the fully-trained neural network, resulting from the aforementioned training process, to investigate the information propagation of new, unknown data through it.\n\nWithout loss of generality, we will assume that the neural network architecture of interest is a feedforward network. Given a fully-trained feedforward network with $M$ hidden neurons and a functional dataset denoted by $\\mathbb { X } =$ $\\{ \\pmb { x } ^ { ( 1 ) } , \\pmb { x } ^ { ( 2 ) } , \\dots , \\pmb { x } ^ { ( N ) } \\}$ , each data point $\\pmb { x } ^ { ( i ) } \\in \\mathbb { X }$ is inputted into the feedforward network and processed through a series of neurons. Each $j$ -th neuron uses an affine transformation followed by an activation function to produce its output $a _ { i j }$ , representing the information processing for $\\mathbf { \\boldsymbol { x } } ^ { ( i ) }$ . By processing the entire functional dataset, we concatenate the $j$ -th neuron outputs for all the data points into a functional vector ${ \\pmb a } _ { j } = ( a _ { 1 j } , a _ { 2 j } , . . . , a _ { N j } )$ , representing the functional signal in a neural network, similar to the functions of connectome circuits studied in functional MRI.\n\nWhen two functional vectors from a pair of neurons are statistically dependent, they exhibit functional synergy. To quantify this synergy, several methods can be used to measure the statistical dependency between the two vectors, such as Pearson correlation, partial correlation, Spearmanâ€™s rank correlation, and mutual information (Fornito, Zalesky, and Bullmore 2016). In macroscale connectomics, Pearson correlation, which captures linear relationships between different brain regions, is widely used for computing functional connectomes of the human brain. It is straightforward to interpret, with values ranging from -1 to 1, where values further from 0 indicate stronger functional connections. Pearson correlation effectively detects synchrony between brain regions, a common feature of neural activity. For similar reasons, we will use Pearson correlation in this paper to analyze neural networks. Additionally, Pearson correlationâ€™s computational efficiency makes it feasible to apply to large neural network models that involve deep layers and many neurons. Formally, Pearson correlation between two functional vectors $\\pmb { a } _ { j } = ( a _ { 1 j } , a _ { 2 j } , . . . , a _ { N j } )$ and ${ \\pmb a } _ { k } = ( a _ { 1 k } , a _ { 2 k } , . . . , a _ { N k } )$ is defined as\n\n$$\n\\rho _ { j k } = \\frac { \\sum _ { i = 1 } ^ { N } ( a _ { i j } - \\overline { { \\mathbf { a } } } _ { j } ) ( a _ { i k } - \\overline { { \\mathbf { a } } } _ { k } ) } { \\sqrt { \\sum _ { i = 1 } ^ { N } ( a _ { i j } - \\overline { { \\mathbf { a } } } _ { j } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { N } ( a _ { i k } - \\overline { { \\mathbf { a } } } _ { k } ) ^ { 2 } } } ,\n$$\n\nwhere $\\begin{array} { r } { \\overline { { \\mathbf { a } } } _ { j } = \\sum _ { i = 1 } ^ { N } a _ { i j } \\Big / { N } } \\end{array}$ and $\\begin{array} { r } { \\overline { { \\mathbf { a } } } _ { k } = \\sum _ { i = 1 } ^ { N } a _ { i k } \\Big / N } \\end{array}$ .\n\nIn studies of the connectome of the human brain, a connectome is typically represented as a graph, comprising brain regions of interest as nodes, and pairwise correlations as edge weights (Fornito, Zalesky, and Bullmore 2016). There is no universally accepted standard for whether the sign of the correlations should be preserved. Generally, choosing the absolute value of negative correlations highlights the strength of connectivity without regard to its direction. This can be particularly useful in analyses where the primary interest is in the magnitude of interactions between brain regions, regardless of whether they are positively or negatively correlated. Additionally, Pearson correlation between the same region is always 1, and thus is typically excluded from analyses, resulting in a brain graph with no self-loops. For similar reasons, we will follow the same well-established procedure applied to the case of a connectome of a neural network. Formally, given the correlations between every pair of hidden neurons, we define an $M$ -by$M$ weighted adjacency matrix $G$ whose $j k$ -th entry is given as\n\n$$\nG _ { j k } = { \\left\\{ \\begin{array} { l l } { \\displaystyle | \\rho _ { j k } | } & { { \\mathrm { i f ~ } } j \\neq k ; } \\\\ { 0 } & { { \\mathrm { o t h e r w i s e . } } } \\end{array} \\right. }\n$$\n\nWe will call this matrix a functional connectome of a neural network.\n\n# Persistent Graph Homology\n\nThe functional connectome is a very dense matrix, typically representing a complete graph. Therefore, typical workflows (Bullmore and Sporns 2009) apply a threshold to the correlation values for two main reasons: to obtain a sparser graph with a more apparent structure and to reduce the time complexity of computational methods. In particular, methods in persistent homology have cubic time complexity (Otter et al. 2017), making them computationally infeasible for large, complete graphs. This requires iterative, approximation solutions, which introduce numerical errors and reduce the signal-to-noise ratio. Additionally, varying different threshold values significantly alters the graph structure, potentially affecting the studyâ€™s final outcome. Importantly, pairwise dyadic correlations at a fixed spatial threshold constrain the neural structures and functions that can be investigated. Given that a neural network processes information not only based on local neurons of a subnetwork but also the entire neural network from input to output layers, a more comprehensive understanding requires a shift in perspective from pairwise interactions to capturing higher-order relations across the entire range of spatial resolutions.\n\nIn this work, we address these challenges by leveraging brain-inspired persistent graph homology, a scalable topological-learning paradigm that enables analyses of large-scale functional connectomes without approximation (Songdechakraiwut et al. 2023). It has emerged as a promising tool for understanding, characterizing, and quantifying human connectomes (Songdechakraiwut et al. 2022). Persistent graph homology describes interpretable topological invariants, including connected components (0th homology group) and independent cycles (1st homology group or cycle rank), across the entire spatial scale of a graph.\n\nFormally, given a functional connectome $G$ , we define a binary graph $G _ { \\epsilon }$ with the same set of neurons by thresholding the edge correlations so that an edge between neurons $j$ and $k$ exists if $\\rho _ { j k } > \\epsilon$ . As $\\epsilon$ increases, more edges are removed from the functional connectome $G$ . Thus, we have a filtration (Lee et al. 2012): $G _ { \\epsilon _ { 0 } } \\supseteq G _ { \\epsilon _ { 1 } } \\supseteq \\cdot \\cdot \\cdot \\supseteq G _ { \\epsilon _ { k } }$ , where $\\epsilon _ { 0 } \\leq \\epsilon _ { 1 } \\leq \\cdot \\cdot \\cdot \\leq \\epsilon _ { k }$ are called filtration values. Persistent homology tracks the birth and death of the topological invariants over these filtration values $\\epsilon$ . A topological invariant born at filtration $b _ { j }$ and persisting up to filtration $d _ { j }$ is represented by a point $( b _ { j } , d _ { j } )$ in a 2D plane. The set of all such points $( b _ { j } , d _ { j } )$ is called a persistence diagram (Edelsbrunner and Harer 2022). As $\\epsilon$ increases, the number of connected components $\\beta _ { 0 } ( G _ { \\epsilon } )$ increases monotonically, while the number of cycles $\\dot { \\beta } _ { 1 } ( G _ { \\epsilon } )$ decreases monotonically. Thus, persistent graph homology only needs to track a collection of sorted birth values $B ( G )$ for the connected components and a collection of sorted death values $D ( G )$ for the cycles, given as (Songdechakraiwut and Chung 2023)\n\n$$\nB ( \\boldsymbol { G } ) = \\{ b _ { j } \\} _ { j = 1 } ^ { M - 1 } , \\quad D ( \\boldsymbol { G } ) = \\{ d _ { j } \\} _ { j = 1 } ^ { 1 + M ( M - 3 ) / 2 } .\n$$\n\nFigure 1 illustrates a schematic for extracting persistent graph homology, which represents the topology of functional connectomes derived from neural networks.\n\nScalability The set $B ( G )$ consists of edge correlations found in the maximum spanning tree (MST) of $G$ . Once $B ( G )$ is determined, the set $D ( G )$ is derived from the edge correlations that are not part of the MST. Therefore, both $B ( G )$ and $D ( G )$ can be computed very efficiently in $O ( n \\log n )$ time, where $n$ is the number of edges in the connectome graph.\n\n# Persistence Statistics\n\nDistances are fundamental in statistics because they provide a way to measure how much individual data points vary, enabling the calculation of central tendency, dispersion, and overall data behavior. The Wasserstein distance is a prominent measure in persistent homology, associated with the central concept of the stability theorem (Skraba and Turner 2023). In this section, we will elaborate on the high computability of persistent-graph-homology-based Wasserstein distance and how it results in defining essential statistics such as the mean and variance, among others, which have potential in neural network interpretation.\n\nSpecifically, the Wasserstein distance between sets of birth values (or between sets of death values) can be obtained using a closed-form solution. Let ${ \\pmb G } ^ { ( 1 ) }$ and $G ^ { ( 2 ) }$ be two given functional connectomes, each having the same number of neurons. Then, the exact computation of the $p$ - Wasserstein distance is achieved as (Songdechakraiwut and Chung 2023):\n\n$$\n\\begin{array} { r l } & { W _ { p , B } \\big ( G ^ { ( 1 ) } , G ^ { ( 2 ) } \\big ) = \\Big ( \\displaystyle \\sum _ { b \\in B ( G ^ { ( 1 ) } ) } \\big | b - \\tau _ { 0 } ^ { * } \\big ( b \\big ) \\big | ^ { p } \\Big ) ^ { 1 / p } , } \\\\ & { W _ { p , D } \\big ( G ^ { ( 1 ) } , G ^ { ( 2 ) } \\big ) = \\Big ( \\displaystyle \\sum _ { d \\in D ( G ^ { ( 1 ) } ) } \\big | d - \\tau _ { 1 } ^ { * } \\big ( d \\big ) \\big | ^ { p } \\Big ) ^ { 1 / p } , } \\end{array}\n$$\n\nwhere $\\tau _ { 0 } ^ { * }$ maps the $l$ -th smallest birth value in $B ( G ^ { ( 1 ) } )$ to the $l .$ -th smallest birth value in $B ( G ^ { ( 2 ) } )$ , and $\\tau _ { 1 } ^ { * }$ maps the $l$ -th smallest death value in ${ \\cal D } ( G ^ { ( 1 ) } )$ to the $l$ -th smallest death value in $D ( G ^ { ( 2 ) } )$ , for all $l$ . The exact Wasserstein distances $W _ { p , B }$ and $W _ { p , D }$ are well-defined because the bijective mappings $\\tau _ { 0 } ^ { * }$ and $\\boldsymbol { \\tau } _ { 1 } ^ { * }$ are well-defined for sets of births and deaths, respectively, with the same cardinality.\n\nImportantly, the analytic expression of the Wasserstein distance above can be equivalently written in a more familiar Euclidean space. To do this, we define a vector of sorted birth values $\\bar { b _ { G ^ { ( i ) } } } = ( b _ { i 1 } , b _ { i 2 } , . . . , b _ { i , M - 1 } )$ for the connected components, where $b _ { i j } \\in B ( G ^ { ( i ) } )$ and $b _ { i j } \\leq b _ { i , j + 1 }$ . Similarly, we define a vector of sorted death values $d _ { G ^ { ( i ) } } =$ $\\left( d _ { i 1 } , d _ { i 2 } , . . . , d _ { i , 1 + M ( M - 3 ) / 2 } \\right)$ for the cycles. With these definitions, the $p$ -Wasserstein distance can be equivalently expressed as\n\n$$\n\\begin{array} { r } { W _ { p , B } ( { \\pmb G } ^ { ( 1 ) } , { \\pmb G } ^ { ( 2 ) } ) = | | { \\pmb b } _ { { \\pmb G } ^ { ( 1 ) } } - { \\pmb b } _ { { \\pmb G } ^ { ( 2 ) } } | | _ { p } , } \\\\ { W _ { p , D } ( { \\pmb G } ^ { ( 1 ) } , { \\pmb G } ^ { ( 2 ) } ) = | | { \\pmb d } _ { { \\pmb G } ^ { ( 1 ) } } - { \\pmb d } _ { { \\pmb G } ^ { ( 2 ) } } | | _ { p } , } \\end{array}\n$$\n\nwhere $| | \\cdot | | _ { p }$ is the $L ^ { p }$ norm. Since $W _ { p , B }$ and $W _ { p , B }$ are differentiable functions and can be explicitly written down, their gradients are in closed form and can be computed very efficiently.\n\nAs is common in machine learning, since we know a computable formula of the Wasserstein distance, and we can take the gradient of that formula efficiently using analytic forms, we can optimize objective functions based on the Wasserstein distance using gradient-based optimization algorithms. For instance, given $N$ functional connectomes $\\bar { G ^ { ( 1 ) } } , G ^ { ( 2 ) } , . . . , G ^ { ( N ) }$ , we can determine a persistent diagram centroid $\\overline { { b } }$ (for the connected components) that minimizes the sum of the 2-Wasserstein distances as\n\n$$\n\\overline { { { b } } } = \\underset { b _ { G } } { \\arg \\operatorname* { m i n } } \\sum _ { i = 1 } ^ { N } W _ { 2 , B } ( \\overline { { { G } } } , G ^ { ( i ) } ) = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } { b _ { G ^ { ( i ) } } } .\n$$\n\n$\\overline { { b } }$ represents the Wasserstein barycenter that quantifies the central tendency of the functional connectomes. Likewise, the variability around the barycenter can be determined as\n\n$$\ns _ { b } ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( b _ { G ^ { ( i ) } } - \\overline { { b } } ) .\n$$\n\n$s _ { b } ^ { 2 }$ represents the Wasserstein variance. In a similar manner, the Wasserstein mean $\\overline { { d } }$ and variance $s _ { d } ^ { 2 }$ for the cycles can be calculated using $W _ { p , D }$ . Additionally, other important statistics that measure central tendency, dispersion, shape, and association can also be computed.\n\nScalability The computation of the Wasserstein distance is very efficient. By sorting birth and death values and matching them in order, the computational cost for evaluating $W _ { p , B }$ and $W _ { p , D }$ is $O ( n \\log n )$ , where $n$ is the number of edges in connectome graphs.\n\n# 3 Connectome Analysis of Neural Networks\n\nDrawing inspiration from functional MRI studies of the human brain, we are interested in the functional behavior of neural networks and whether we can characterize them using our proposed functional connectomes and persistent-graphhomology representation. Through two analysis studies, we aim to explore these connections.\n\nIn the first study, we will analyze how various popular regularization strategies, including batch normalization, dropout, and $L ^ { 2 }$ regularization, influence the overall functional mechanisms and data propagation in neural networks during inference. Regularization affects neural network weights similarly to how various factors influence structural brain networks obtained by diffusion MRI. This, in turn, impacts how functional mechanisms unfold.\n\nIn the second study, we will conduct a more fine-grained investigation into how neural networks process different stimuli through functional connectomes. Specifically, we will explore whether there are inherent topological patterns within the functional mechanisms of processing data points from various predefined classes. For example, we will analyze how neural networks process samples from different digit classes (0-9) in the MNIST dataset. This is analogous to how the human brain perceives and processes different visual stimuli, such as recognizing and distinguishing between digits, characterized by human connectomes observed in function MRI studies.\n\nBy drawing these comparisons, we aim to gain insights into the similarities between neural networks and human brain functions to better understand and characterize these systems.\n\nCluster analysis We are interested in identifying the natural groupings and relationships within the functional mechanism using our proposed persistent-graph-homology framework. Our goal is to determine if this framework can uncover the intrinsic structure of functional connectomes without supervision from predefined classes. Supervised learning can alter the original representation through coefficient and weight adjustments, potentially obscuring the underlying patterns and groupings. Clustering, however, can validate predefined classes by revealing if clusters of functional connectomes align well with them. Specifically, connectomes from the same class should be topologically similar and grouped into the same cluster, while connectomes from different classes should be dissimilar and placed in separate clusters. If this alignment occurs, it supports the idea that the functional mechanisms of neural networks are characterized by their topology and that our framework effectively captures these topological signals. Therefore, we will utilize unsupervised clustering to explore the data, grouping similar connectomes based on their subtle topological patterns.\n\nMethod comparison We evaluated the clustering performance of our proposed method, termed Top, relative to six other baseline methods. Our Top method utilizes centroidbased clustering that minimizes within-cluster variances based on squared 2-Wasserstein distances $W _ { 2 , B } ^ { 2 } + W _ { 2 , D } ^ { 2 }$ of birth/death values and the Wasserstein barycenter, optimized via Lloydâ€™s algorithm (Forgy 1965).\n\nThe first baseline method uses $k$ -means clustering on the vectorization of entries below the main diagonal of adjacency (Adj) matrices of functional connectomes, grouping the connectomes based on node-by-node geometry. The remaining five methods use persistent homology and involve clustering on conventional Rips-complex persistence diagrams of the 1st homology group, commonly used in the literature. The $k$ -medoids algorithm is applied using the following distances and kernels: bottleneck distance (BD), Wasserstein distance (WD), sliced Wasserstein kernel (SWK) (Carriere, Cuturi, and Oudot 2017), and heat kernel (HK) (Reininghaus et al. 2015). Additionally, $k$ -means clustering is performed on the persistence image (PI) vectorization (Adams et al. 2017). More details on these methods are available in the extended version and code.\n\nFor all methods, initial clusters are selected at random, and we perform clustering 20 trials and report average clustering performance.\n\nDatasets We performed our analyses on three datasets: MNIST (LeCun et al. 1998), Fashion-MNIST (Xiao, Rasul, and Vollgraf 2017), and CIFAR-10 (Krizhevsky, Hinton et al. 2009). MNIST comprises a collection of grayscale images of handwritten digits, Fashion-MNIST consists of grayscale images of fashion products, and CIFAR-10 includes color images of various animals and objects. Each of these datasets contains 10 predefined classes. More details on the datasets are available in the extended version.\n\nNeural network architectures and training We employed neural network architectures of increasing complexity to match the varying complexities of the three datasets (MNIST, Fashion-MNIST, and CIFAR-10), while also keeping the architectures compact. This approach ensures that we can train well-generalized neural networks, which will then be analyzed for their behavior in our studies, while maintaining simplicity for transparency and interpretability of the analysis results. Additionally, conventional persistent homology methods, which will serve as baseline methods for comparison with our proposed method, have cubic time complexity (Otter et al. 2017), limiting the architecture size to fewer than a few hundred nodes, as demonstrated in the runtime experiment below.\n\nFor MNIST, we used a feedforward architecture with two hidden fully-connected layers, with the first and second layers comprising 128 and 64 neurons, respectively. For Fashion-MNIST, we used a similar feedforward architecture, but with the first and second layers comprising 256 and 128 neurons, respectively. For CIFAR-10, we used a convolutional neural network with three VGG blocks (Simonyan and Zisserman 2015), followed by two fully-connected layers, with the first and second layers comprising 256 and 128 neurons, respectively. In all architectures, we applied leaky ReLU activation functions and the stochastic gradient descent optimizer with momentum.\n\nTo train neural networks, we randomly partitioned the data points of each dataset into a training dataset and a functional dataset, as explained in Section 2. For each training strategyâ€“namely 1) batch normalization, 2) dropout, 3) $L ^ { 2 }$ , as well as 4) vanilla (which trains neural networks without any regularization and serves as a control)â€“we used the training set to optimize and obtain well-generalized neural networks. To account for the stochastic nature of gradient-based optimization initialization, we trained 20 neural networks for each strategy, totaling 80 networks $( 2 0 ~ \\times ~ 4 )$ . For the MNIST dataset, the average test accuracies are 0.98 across all training strategies. For Fashion-MNIST, the average test accuracies are 0.89 across all training strategies. For CIFAR10, the average test accuracies are 0.76 across all training strategies.\n\nMore details on the architecture and hyperparameter tuning are available in the extended version and code.\n\nOnce the fully-trained neural networks are obtained, the functional dataset is fed into these networks to extract functional connectomes. Two different methods of extraction are employed for the two analysis studies, which will be provided in more detail below for each specific study. Note that only neurons in the hidden fully-connected layers are used to construct the connectomes; neurons in the softmax output layers and convolutional layers are excluded.\n\n# Study 1\n\nWe will analyze the influence of various popular regularization strategies on the overall functional mechanisms and data propagation in neural networks during inference. To construct functional connectomes for each dataset, we feed the entire corresponding functional dataset to the neural network to extract functional connectomes. As a result, we obtain 80 functional connectomes, with 20 of these connectomes from each training strategy (batch normalization, dropout, $L ^ { 2 }$ , and vanilla). We will perform cluster analysis on these 80 data points to group them into four clusters. Figure 2 illustrates the average functional connectomes from training on the Fashion-MNIST dataset using each strategy, along with their corresponding persistence diagrams, which describe the topology. Additionally, we will cluster each regularization strategy against the control group (i.e., vanilla) to better understand the impact of each regularization method on the neural networksâ€™ functional mechanisms. That is, we will cluster 40 data points into two groups.\n\n![](images/82f07d38895773219688eb5750425668605da8681f633bc65c255007a284ac08.jpg)  \nFigure 2: Statistics of the functional dataset used in Study 1. Left: Sample means of the functional connectomes, averaged within each training strategy. Right: Persistence diagrams and statistics for each strategy, with thick lines representing Wasserstein barycenters and shaded regions indicating Wasserstein standard deviation.\n\n<html><body><table><tr><td>Dataset</td><td>Strategy</td><td>BD</td><td>WD</td><td>SWK</td><td>HK</td><td>PI</td><td>Adj</td><td>Top</td></tr><tr><td rowspan=\"4\">MNIST</td><td>All</td><td>0.63Â± 0.04</td><td>0.75</td><td></td><td></td><td></td><td>0.85 0.72Â±0.01 0.69Â±0.01 0.32Â±0.05</td><td>0.78 Â±0.11</td></tr><tr><td>Batch Norm vs. Vanilla Dropout vs. Vanilla</td><td>1.00</td><td>1.00</td><td>1.00</td><td>0.93</td><td>0.93</td><td>0.55 Â± 0.04</td><td>1.00</td></tr><tr><td>LÂ² vs. Vanilla</td><td>0.67 Â± 0.07</td><td>0.90</td><td>0.95</td><td>0.98</td><td>0.98</td><td>0.54 Â± 0.03</td><td>1.00</td></tr><tr><td></td><td>0.62 Â± 0.04</td><td>0.70</td><td>0.70</td><td>0.60</td><td>0.58</td><td>0.55 Â±0.03 0.80 Â±0.03</td><td></td></tr><tr><td rowspan=\"4\">Fashion- MNIST</td><td rowspan=\"4\">All Batch Norm vs. Vanilla Dropout vs. Vanilla LÂ² vs. Vanilla</td><td>0.68</td><td>0.75 Â± 0.01</td><td>0.78</td><td></td><td>0.52Â±0.01 0.53Â±0.02</td><td>0.34Â±0.07</td><td>0.87Â±0.12</td></tr><tr><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>0.98</td><td>0.58 Â±0.07</td><td>1.00</td></tr><tr><td>0.73</td><td>0.95</td><td>0.93</td><td>0.50</td><td>0.50</td><td>0.54Â± 0.04</td><td>0.98</td></tr><tr><td>0.83</td><td>0.98</td><td>0.95</td><td>0.53</td><td>0.53</td><td>0.55 Â± 0.03</td><td>1.00</td></tr><tr><td rowspan=\"4\"></td><td>All CIFAR-1O Batch Norm vs.Vanilla</td><td>0.75 Â± 0.01</td><td>0.98</td><td>0.96</td><td>0.81</td><td>0.58 Â±0.02 0.51 Â± 0.11</td><td></td><td>0.88 Â± 0.11</td></tr><tr><td rowspan=\"3\">Dropout vs. Vanilla LÂ² vs. Vanilla</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>0.56 Â± 0.09</td><td>1.00</td></tr><tr><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td></td><td>0.63Â±0.07 0.62 Â±0.17</td><td>1.00</td></tr><tr><td>0.64Â± 0.01</td><td>0.95</td><td>0.93</td><td>0.68</td><td></td><td>0.67 Â±0.01 0.55 Â±0.04</td><td>0.94 Â± 0.01</td></tr></table></body></html>\n\nTable 1: Comparison of clustering performance in Study 1 across different datasets and training strategies, reported as average purity scores $\\pm$ standard deviation. The table presents results for clustering all strategies together, as well as pairwise clustering analysis of each regularization strategy compared to the control group (vanilla).\n\nSince these datasets are balanced, with 20 samples per class, we can evaluate clustering performance using purity (Manning, Raghavan, and SchuÂ¨tze 2008). The purity score ranges from 0 to 1, with 1 indicates perfect clustering alignment. This evaluation measure is not only transparent and interpretable but also effective for this study, where the number of clusters is small and the cluster sizes are balanced.\n\nTable 1 presents the clustering performance comparison. The findings indicate that regularization strategies, which influence learnable weight adjustment of neural networks, significantly affect data propagation through functional mechanisms, similar to neuroanatomy in the human nervous system (Sporns 2016). Specifically, the clustering performance shows high purity scores, which suggest substantial differences between each regularization strategy vs. the control, highlighting the notable impact of these techniques. Furthermore, different regularization strategies lead to distinct functional mechanisms, resulting in high purity scores in cluster analysis for all four training strategies. Overall, topological signals, as measured by persistent homology methods, prove to be an effective means of characterizing neural network functions. In most settings, the proposed Top method outperforms other baselines.\n\n# Study 2\n\nWe will explore how fully-trained neural networks process different stimuli using functional connectomes. We construct these connectomes as follows. For each dataset, we partition the data into collections where each collection contains data points from a specific predefined class. For instance, in the MNIST dataset, we create 10 collections, each corresponding to a digit class (0-9). We then feed each collection into the fully-trained neural network to extract the functional connectomes for that particular class. For each training strategy, we obtain 20 functional connectomes per class, resulting in a total of 200 functional connectomes (20 $\\times ~ 1 0 )$ ). We will perform cluster analysis on these 200 data points to group them into ten clusters.\n\nAs in Study 1, these datasets in Study 2 are balanced so we will also use the purity score to evaluate clustering performance.\n\nTable 2 displays the clustering performance comparison. Topological methods, including WD, SWK and Top, effectively capture the functions distinct to each predefined class. They are the best performers that achieve purity scores between 0.5 and 0.6 in unsupervised settings, which are significantly better than the 0.1 score expected if clustering was made randomly. These findings show that samples from each class are processed through distinct functional mechanisms. These phenomena are observed in all training strategies. This is similar to how the human brain uses specialized neural mechanisms to process different types of stimuli, ensuring efficient and effective interpretation of diverse information (Kandel et al. 2000).\n\n<html><body><table><tr><td>Dataset</td><td>Strategy</td><td>BD</td><td>WD</td><td>SWK</td><td>HK</td><td>PI</td><td>Adj</td><td>Top</td></tr><tr><td rowspan=\"3\">MNIST</td><td rowspan=\"3\">Vanilla Batch Norm Dropout</td><td>0.33</td><td>0.40</td><td>0.44Â±0.02</td><td>0.36 Â± 0.01</td><td>0.36Â± 0.01</td><td>0.21Â±0.02 0.47Â±0.02</td><td></td></tr><tr><td>0.36 Â± 0.02</td><td>0.50</td><td>0.48 Â± 0.02</td><td>0.35</td><td>0.34 Â± 0.01</td><td>0.22 Â±0.03</td><td>0.46 Â±0.02</td></tr><tr><td>0.33 Â± 0.02</td><td>0.47</td><td>0.46 Â± 0.01</td><td>0.34 Â± 0.01</td><td>0.34 Â± 0.01</td><td>0.18Â±0.03</td><td>0.57 Â± 0.02</td></tr><tr><td rowspan=\"3\">Fashion- MNIST</td><td rowspan=\"3\">LÂ² Vanilla Batch Norm Dropout</td><td>0.29 Â± 0.01 0.39 Â± 0.01</td><td>0.44 Â± 0.01</td><td>0.45 Â± 0.01</td><td>0.34 Â± 0.01</td><td>0.34 Â± 0.01</td><td>0.19 Â± 0.02</td><td>0.48 Â± 0.02</td></tr><tr><td></td><td>0.62Â± 0.01</td><td>0.64Â± 0.02</td><td>0.46Â± 0.02</td><td>0.43Â± 0.01</td><td>0.23 Â± 0.03</td><td>0.53 Â± 0.02</td></tr><tr><td>0.38 Â± 0.02 0.41</td><td>0.58 Â± 0.02 0.60</td><td>0.60 Â± 0.01 0.59 Â± 0.04</td><td>0.40 Â± 0.01 0.45 Â± 0.02</td><td>0.39 Â± 0.01 0.40 Â±0.01</td><td>0.20 Â± 0.04</td><td>0.49 Â± 0.02 0.53 Â±0.03</td></tr><tr><td rowspan=\"3\"></td><td>LÂ²</td><td>0.43</td><td>0.54 Â± 0.01</td><td>0.59 Â± 0.01</td><td>0.41</td><td>0.41 Â± 0.01</td><td>0.19 Â±0.03 0.21 Â± 0.04</td><td>0.53 Â± 0.04</td></tr><tr><td rowspan=\"2\">Vanilla CIFAR-1O Batch Norm</td><td>0.30 Â± 0.01</td><td>0.56Â±0.01</td><td>0.55 Â± 0.01</td><td>0.41 Â± 0.01</td><td>0.40 Â± 0.01</td><td>0.15 Â± 0.02</td><td>0.51 Â± 0.03</td></tr><tr><td>0.33 Â± 0.01</td><td>0.51 Â± 0.01</td><td>0.47</td><td>0.35 Â± 0.01</td><td>0.35 Â± 0.01</td><td>0.16 Â± 0.01</td><td>0.52 Â± 0.02</td></tr><tr><td rowspan=\"3\"></td><td>Dropout</td><td>0.29 Â± 0.01</td><td>0.49</td><td>0.51 Â± 0.01</td><td>0.37 Â± 0.01</td><td>0.37 Â± 0.01</td><td>0.26 Â± 0.04</td><td>0.50 Â± 0.02</td></tr><tr><td>LÂ²</td><td>0.35 Â± 0.01</td><td>0.59 Â± 0.03</td><td>0.59Â±0.01</td><td>0.50 Â± 0.01</td><td>0.48 Â± 0.01</td><td>0.18 Â± 0.03</td><td>0.54 Â± 0.03</td></tr></table></body></html>\n\nTable 2: Comparison of clustering performance in Study 2, reported as average purity scores $\\pm$ standard deviation.\n\n# Runtime Experiment\n\nAll topological methods used in the studies were evaluated through runtime experiments. These methods were executed on an Apple M1 Pro CPU with 16 GB of unified RAM. Figure 3 shows the plot of runtime vs. input size. The results clearly indicate that all five persistent homology-based distances and kernels (BD, WD, SWK, HK, and PI) are limited to handling dense graphs with only a few hundred nodes, highlighting the current scaling limitations of persistent homology embedding methods and their heavy reliance on approximation solutions. In contrast, Top can compute the exact Wasserstein distance between graphs with thousands of nodes and millions of edges in about one second. This computational efficiency makes Top practical for large-scale analyses of neural networks, which cannot be effectively analyzed using methods based on conventional persistence diagrams.\n\nPotential Impact Our approach for characterizing functional connectomes in neural networks is effective, computable, and scalable, potentially impacting the analysis of large neural architectures. By integrating neural network interpretability with human brain function insights, our framework opens up opportunities to leverage established techniques from functional MRI analysis. From a statistical learning perspective, our persistence statistics provide a robust basis for hypothesis testing and permutation tests, enhancing statistical rigor. Their linear-logarithmic efficiency supports large-scale neural network applications. Additionally, the gradient computability of the Wasserstein distance aids in designing advanced machine learning algorithms through gradient descent optimization.\n\nOur studies on convolutional neural networks for the CIFAR-10 dataset demonstrate the effectiveness of focusing on subnetworks within the last few fully-connected layers, enabling topological analysis of more targeted functional mechanisms. This approach could be particularly effective for complex, deep neural networks, including those with multiple heads. While primarily focused on feedforward architectures, our method can also be extended to convolutional layers and recurrent networks.\n\n![](images/8a8c5e7228bc1ec6fa9262eb43b76996208294705586fc2dacae5ff3e655da03.jpg)  \nFigure 3: Average runtime of each method for computing topological distance or kernel between two complete graphs. The graphs were generated using a modular network approach (details in the extended version, with code provided). The runtime is plotted against the network size, represented by the number of nodes and edges.\n\nLimitation Persistent graph homology is limited to the topological invariants of connected components and cycles. These two features, however, play a dominant role in topological analyses. For example, they are widely utilized in the brain network community (Bullmore and Sporns 2009; Honey et al. 2007), and cycles, in particular, have been increasingly reported as the most discriminative topological feature in brain networks (Sizemore et al. 2018), galaxy organization (Biagetti, Cole, and Shiu 2021), and protein structure (Xia and Wei 2014). In contrast, the assessment of higher-order features beyond cycles offers limited practical value due to their relative rarity, interpretive challenges, and consequent minimal discriminative power (Biagetti, Cole, and Shiu 2021; Sizemore et al. 2018; Songdechakraiwut and Chung 2020).",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡æ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ï¼Œé€šè¿‡å€Ÿé‰´äººè„‘åŠŸèƒ½è¿æ¥ç»„ï¼ˆfunctional connectomeï¼‰çš„ç ”ç©¶æ–¹æ³•ï¼Œæå‡ºä¸€ç§æ–°çš„åˆ†ææ¡†æ¶ï¼Œä»¥å¢å¼ºå¯¹ç¥ç»ç½‘ç»œå†…éƒ¨æœºåˆ¶çš„ç†è§£ã€‚\\n> *   è¯¥é—®é¢˜çš„é‡è¦æ€§åœ¨äºï¼Œç°æœ‰çš„ç¥ç»ç½‘ç»œæ¨¡å‹è™½ç„¶æ€§èƒ½ä¼˜å¼‚ï¼Œä½†å¾€å¾€è¢«è§†ä¸ºâ€œé»‘ç®±â€ï¼Œéš¾ä»¥ç†è§£å…¶å†…éƒ¨å·¥ä½œæœºåˆ¶ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦é«˜é€æ˜åº¦çš„åº”ç”¨åœºæ™¯ï¼ˆå¦‚åŒ»ç–—ã€é‡‘èï¼‰ä¸­çš„ä½¿ç”¨ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæŒä¹…å›¾åŒè°ƒï¼ˆpersistent graph homologyï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ†æç¥ç»ç½‘ç»œçš„åŠŸèƒ½è¿æ¥ç»„ï¼Œæ•æ‰å…¶æ‹“æ‰‘ç»“æ„ç‰¹å¾ï¼ˆå¦‚è¿é€šåˆ†é‡å’Œå¾ªç¯ç»“æ„ï¼‰ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹1ï¼š** æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ‹“æ‰‘åˆ†ææ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åˆ†æå¤§è§„æ¨¡ç¥ç»ç½‘ç»œçš„åŠŸèƒ½è¿æ¥ç»„ï¼Œè®¡ç®—å¤æ‚åº¦ä¸ºO(n log n)ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„O(n^3)ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹2ï¼š** å¼•å…¥äº†Wassersteinè·ç¦»çš„é—­å¼è§£ï¼Œå®ç°äº†é«˜æ•ˆçš„æ‹“æ‰‘ç»Ÿè®¡è®¡ç®—ï¼ˆå¦‚å‡å€¼ã€æ–¹å·®ï¼‰ï¼Œä¸ºç¥ç»ç½‘ç»œçš„å¯è§£é‡Šæ€§åˆ†ææä¾›äº†é‡åŒ–å·¥å…·ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹3ï¼š** é€šè¿‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨èšç±»ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ï¼Œä¾‹å¦‚åœ¨MNISTæ•°æ®é›†ä¸Šï¼Œèšç±»çº¯åº¦è¾¾åˆ°0.78Â±0.11ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ˆå¦‚Adjçš„0.32Â±0.05ï¼‰ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åˆ†æç¥ç»ç½‘ç»œçš„åŠŸèƒ½è¿æ¥ç»„ï¼ˆå³ç¥ç»å…ƒä¹‹é—´çš„åŠŸèƒ½ç›¸å…³æ€§ï¼‰ï¼Œæ•æ‰å…¶æ‹“æ‰‘ç‰¹å¾ï¼ˆå¦‚è¿é€šåˆ†é‡å’Œå¾ªç¯ç»“æ„ï¼‰ï¼Œä»è€Œæ­ç¤ºç¥ç»ç½‘ç»œçš„å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚\\n> *   å…¶æœ‰æ•ˆæ€§åœ¨äºï¼ŒåŠŸèƒ½è¿æ¥ç»„çš„æ‹“æ‰‘ç»“æ„ä¸ç¥ç»ç½‘ç»œçš„ä¿¡æ¯å¤„ç†æœºåˆ¶å¯†åˆ‡ç›¸å…³ï¼Œç±»ä¼¼äºäººè„‘åŠŸèƒ½è¿æ¥ç»„çš„ç ”ç©¶æ–¹æ³•ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** å…ˆå‰çš„æ–¹æ³•ä¸»è¦åŸºäºæŒä¹…åŒè°ƒï¼ˆpersistent homologyï¼‰ï¼Œä½†è®¡ç®—å¤æ‚åº¦é«˜ï¼ˆO(n^3)ï¼‰ï¼Œä¸”é€šå¸¸éœ€è¦é˜ˆå€¼åŒ–å¤„ç†ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡ç½‘ç»œä¸­çš„åº”ç”¨ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** æœ¬æ–‡æå‡ºçš„æŒä¹…å›¾åŒè°ƒæ–¹æ³•é€šè¿‡æœ€å¤§ç”Ÿæˆæ ‘ï¼ˆMSTï¼‰å’Œé«˜æ•ˆæ’åºç®—æ³•ï¼Œå°†è®¡ç®—å¤æ‚åº¦é™ä½åˆ°O(n log n)ï¼ŒåŒæ—¶é¿å…äº†é˜ˆå€¼åŒ–å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> *   1. **åŠŸèƒ½è¿æ¥ç»„æ„å»ºï¼š** ä½¿ç”¨Pearsonç›¸å…³ç³»æ•°è®¡ç®—ç¥ç»å…ƒä¹‹é—´çš„åŠŸèƒ½ç›¸å…³æ€§ï¼Œæ„å»ºåŠŸèƒ½è¿æ¥ç»„çŸ©é˜µã€‚\\n> *   2. **æŒä¹…å›¾åŒè°ƒæå–ï¼š** é€šè¿‡æœ€å¤§ç”Ÿæˆæ ‘ï¼ˆMSTï¼‰æå–è¿é€šåˆ†é‡çš„å‡ºç”Ÿå€¼ï¼ˆbirth valuesï¼‰ï¼Œå¹¶é€šè¿‡å‰©ä½™è¾¹æå–å¾ªç¯çš„æ­»äº¡å€¼ï¼ˆdeath valuesï¼‰ã€‚\\n> *   3. **æ‹“æ‰‘ç»Ÿè®¡è®¡ç®—ï¼š** åŸºäºWassersteinè·ç¦»çš„é—­å¼è§£ï¼Œè®¡ç®—æ‹“æ‰‘ç»Ÿè®¡é‡ï¼ˆå¦‚å‡å€¼ã€æ–¹å·®ï¼‰ï¼Œç”¨äºèšç±»æˆ–åˆ†ç±»ä»»åŠ¡ã€‚\\n> *   4. **å…³é”®å…¬å¼ï¼š** \\n>       *   Pearsonç›¸å…³ç³»æ•°ï¼š\\n>           $$\\n>           \\\\rho _ { j k } = \\\\frac { \\\\sum _ { i = 1 } ^ { N } ( a _ { i j } - \\\\overline { { \\\\mathbf { a } } } _ { j } ) ( a _ { i k } - \\\\overline { { \\\\mathbf { a } } } _ { k } ) } { \\\\sqrt { \\\\sum _ { i = 1 } ^ { N } ( a _ { i j } - \\\\overline { { \\\\mathbf { a } } } _ { j } ) ^ { 2 } } \\\\sqrt { \\\\sum _ { i = 1 } ^ { N } ( a _ { i k } - \\\\overline { { \\\\mathbf { a } } } _ { k } ) ^ { 2 } } }\\n>           $$\\n>       *   Wassersteinè·ç¦»ï¼š\\n>           $$\\n>           W _ { p , B } ( { \\\\pmb G } ^ { ( 1 ) } , { \\\\pmb G } ^ { ( 2 ) } ) = | | { \\\\pmb b } _ { { \\\\pmb G } ^ { ( 1 ) } } - { \\\\pmb b } _ { { \\\\pmb G } ^ { ( 2 ) } } | | _ { p }\\n>           $$\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   è®ºæ–‡ä¸­å¯¹æ¯”çš„åŸºçº¿æ–¹æ³•åŒ…æ‹¬ï¼šåŸºäºé‚»æ¥çŸ©é˜µçš„k-meansèšç±»ï¼ˆAdjï¼‰ã€åŸºäºç“¶é¢ˆè·ç¦»ï¼ˆBDï¼‰çš„k-medoidsèšç±»ã€åŸºäºWassersteinè·ç¦»ï¼ˆWDï¼‰çš„k-medoidsèšç±»ã€åŸºäºåˆ‡ç‰‡Wassersteinæ ¸ï¼ˆSWKï¼‰çš„k-medoidsèšç±»ã€åŸºäºçƒ­æ ¸ï¼ˆHKï¼‰çš„k-medoidsèšç±»ï¼Œä»¥åŠåŸºäºæŒä¹…å›¾åƒï¼ˆPIï¼‰çš„k-meansèšç±»ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨èšç±»çº¯åº¦ï¼ˆpurityï¼‰ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•ï¼ˆTopï¼‰åœ¨MNISTæ•°æ®é›†ä¸Šè¾¾åˆ°äº†0.78Â±0.11ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•Adjï¼ˆ0.32Â±0.05ï¼‰å’ŒWDï¼ˆ0.75Â±0.01ï¼‰ã€‚ä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿ï¼ˆWDï¼‰ç›¸æ¯”ï¼Œæå‡äº†3ä¸ªç™¾åˆ†ç‚¹ã€‚\\n> *   **åœ¨è®¡ç®—æ•ˆç‡ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•çš„å¤„ç†é€Ÿåº¦ä¸ºO(n log n)ï¼Œè¿œé«˜äºåŸºçº¿æ–¹æ³•BDå’ŒWDï¼ˆO(n^3)ï¼‰ï¼ŒåŒæ—¶ä¸è½»é‡çº§æ–¹æ³•Adjçš„é€Ÿåº¦ç›¸å½“ï¼Œä½†åœ¨èšç±»çº¯åº¦ä¸Šè¿œè¶…åè€…ã€‚\\n> *   **åœ¨æ­£åˆ™åŒ–ç­–ç•¥åˆ†æä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨åŒºåˆ†ä¸åŒæ­£åˆ™åŒ–ç­–ç•¥ï¼ˆå¦‚batch normalization vs. vanillaï¼‰æ—¶ï¼Œèšç±»çº¯åº¦è¾¾åˆ°1.00ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   åŠŸèƒ½è¿æ¥ç»„ (Functional Connectome, FC)\\n*   æŒä¹…å›¾åŒè°ƒ (Persistent Graph Homology, PGH)\\n*   Wassersteinè·ç¦» (Wasserstein Distance, WD)\\n*   ç¥ç»ç½‘ç»œå¯è§£é‡Šæ€§ (Neural Network Interpretability, NNI)\\n*   æ‹“æ‰‘æ•°æ®åˆ†æ (Topological Data Analysis, TDA)\\n*   èšç±»åˆ†æ (Cluster Analysis, N/A)\\n*   ç»Ÿè®¡å­¦ä¹  (Statistical Learning, N/A)\"\n}\n```"
}