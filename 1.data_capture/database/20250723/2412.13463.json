{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.13463",
    "link": "https://arxiv.org/abs/2412.13463",
    "pdf_link": "https://arxiv.org/pdf/2412.13463.pdf",
    "title": "FlexPose: Pose Distribution Adaptation with Limited Guidance",
    "authors": [
        "Zixiao Wang",
        "Junwu Weng",
        "Mengyuan Liu",
        "Bei Yu"
    ],
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "publication_date": "2024-12-18",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "The Chinese University of Hong Kong",
        "ByteDance Inc.",
        "Peking University"
    ],
    "paper_content": "# FlexPose: Pose Distribution Adaptation with Limited Guidance\n\nZixiao Wang1, Junwu Weng2\\*, Mengyuan Liu3, Bei $\\mathbf { Y } \\mathbf { u } ^ { 1 * }$\n\n1The Chinese University of Hong Kong 2ByteDance Inc. 3Peking University zxwang22@cse.cuhk.edu.hk, we0001wu@e.ntu.edu.sg, nkliuyifang $@$ gmail.com, byu@cse.cuhk.edu.hk\n\n# Abstract\n\nNumerous well-annotated human key-point datasets are publicly available to date. However, annotating human poses for newly collected images is still a costly and time-consuming progress. Pose distributions from different datasets share similar pose hinge-structure priors with different geometric transformations, such as pivot orientation, joint rotation, and bone length ratio. The difference between Pose distributions is essentially the difference between the transformation distributions. Inspired by this fact, we propose a method to calibrate a pre-trained pose generator in which the pose prior has already been learned to an adapted one following a new pose distribution. We treat the representation of human pose joint coordinates as skeleton image and transfer a pre-trained pose annotation generator with only a few annotation guidance. By fine-tuning a limited number of linear layers that closely related to the pose transformation, the adapted generator is able to produce any number of pose annotations that are similar to the target poses. We evaluate our proposed method, FlexPose, on several cross-dataset settings both qualitatively and quantitatively, which demonstrates that our approach achieves stateof-the-art performance compared to the existing generativemodel-based transfer learning methods when given limited annotation guidance.\n\n# 1 Introduction\n\nDeep neural networks are data-hungry and rely on large-scale datasets with high-quality human annotations for training. However, the process of annotating these datasets can be expensive and time-consuming, particularly when dense annotation are required, as is often the case in pose estimation tasks (Wang and Zhang 2022; He et al. 2017). To overcome this challenge, AI-aided labeling methods have become increasingly popular, where a pre-trained model’s prediction serves as a reference to reduce human workload. However, when there is a domain shift (Luo et al. 2019), where the distribution of the training dataset and test dataset are not aligned not only on the input image domain but on the pose annotation domain as well, the accuracy of the model can significantly decline.\n\nConsiderable efforts have been devoted to tackling this issue. Among them, domain adaptation (DA) (Daum´e III\n\nCommon Prior Common Prior Transformation FlexPose Transformation Dataset Annotation 1 价 T 山4价 Pose-basedPainting Animation Design Weakly Supervision Source Pose Distribution Target Pose Distribution\n\n2009; Csurka 2017) introduces knowledge from existing annotated datasets to a target dataset and is verified effective on several computer vision tasks (Cao et al. 2019; Inoue et al. 2018). However, things become different in the humanrelated dataset, e.g., human pose (Ionescu et al. 2014) and human face (Wu et al. 2018). As the source human appearance is usually required in DA-related methods for input image domain adaptation, they may import unexpected data distribution bias (Buolamwini and Gebru 2018), e.g., gender or color, from the source. Besides, the direct exposure of private portraits may raise the privacy issue.\n\nOn the other hand, it is commonly observed that different human poses share a similar hinge-structure prior. Typically, poses in a target dataset can be transferred from poses of a pre-collected source set by applying geometric transformations on for example pivot orientation, joint rotation, and bone length ratio. Therefore, adapting the pose distribution only can be a viable option in the case of human-related datasets. Pose Domain Adaptation (PDA) avoids the direct use of human appearance images, effectively addressing the aforementioned issues. Motivated by this observation, we propose FlexPose (shown in Figure 1), a method that transfers the source pose distribution to a target distribution with limited pose annotation guidance. After pose distribution transfer by FlexPose, each input image can be matched with the most related generated pose in estimated pose distribution by utilizing a matching algorithm (Jakab et al. 2020) for weakly supervised pose estimation and pose annotation. Besides, the generated poses can also be utilized in plenty of downstream tasks such as pose-conditioned image generation (Zhang and Agrawala 2023).\n\nIn FlexPose, we treat pose annotations as skeleton images to well align the annotations with their RGB appearance correspondences, and to improve the learnability of pose prior as the skeleton images well preserve the spatial structure of joint connection on image plane. We first learn the pose prior and fit the empirical distribution from a source human pose dataset by a multi-layer generative model. Thereafter, specific layers of the generative model are calibrated by inserting learnable lightweight linear modules to transfer the source distribution to the target domain. Considering that only a limited number of poses are given, we introduce three regularizations to avoid the collapse of the transfer solution. By generating credible pose interpolations with Pose-mixup regularization and by strictly limiting the complexity of the transfer module with linear and sparse regularization, we minimize the requirement of sample amount but maximize the sample diversity in FlexPose. FlexPose is computationefficient. It operates on the pose domain, and hence the training convergence is much faster than methods in domain adaptation, which focuses adaptation on both the pose and image domains together. FlexPose is also data-efficient. We only need limited pose annotations from the target dataset to finetune the transfer modules. Extensive experiments on three pose-based tasks, i.e., human pose annotation, human face landmarks annotation, and pose-conditional human image generation, demonstrate that FlexPose outperforms baselines by a large margin both quantitatively and qualitatively. Our contributions can be summarized as follows:\n\n• We propose to treat the task of Pose Domain Adaptation as the transfer of skeleton image generator and demonstrate that a target pose distribution can be well approximated from a well-learned pose prior.   \n• We introduce FlexPose, a PDA framework that employs three regularizations to efficiently transfer a pose distribution to a target one by utilizing a limited number of guiding poses with low computation and storage costs.   \n• Extensive experimental results on three pose-related tasks show that FlexPose achieves remarkable improvement over existing methods.\n\n# 2 Related Works\n\nDeep Generative Model for Image Generation. Deep generative models such as GAN, Variational AutoEncoder (VAE), and Diffusion models achieve great success in realistic/artificial image generating and natural image distribution modeling. Recently proposed generative models such as StyleGAN (Karras, Laine, and Aila 2019), DDPM (Ho, Jain, and Abbeel 2020), NVAE (Vahdat and Kautz 2020) introduce new mechanisms, new architecture, and new regularizations into image generation. VAEs (Kingma and Welling 2014) learn to maximize the variational lower bound of likelihood. Diffusion probabilistic models (Sohl-Dickstein et al. 2015)\n\nsynthesize images by a denoising procedure. GANs (Goodfellow et al. 2014) are trained in an adversarial manner to learn how to generate realistic images. Among them, Karras et al. (Karras, Laine, and Aila 2019) proposed an architecture StyleGAN that can learn a hierarchical decoupled style code and controls image synthesis. Our method is based on generators with multi-layer architecture and leverages StyleGAN as the backbone. We are inspired by the recent works (Zhu et al. 2016; Yin et al. 2022), which manipulate the latent code in the generative model to edit the output images. These works motivate us to transfer the pose distribution to the target domain by transferring style codes with few-shot guidance.\n\nTransfer Learning for Generative Models. The literature on transfer learning has been extensively studied in recent years (Oquab et al. 2014; Long et al. 2015; Ganin and Lempitsky 2015). Transfer learning learns to transfer the knowledge from a large-scale source dataset to a small target dataset to enhance model performance on the target dataset. The methodology of transfer learning is also treated as a pretraining technique. It is utilized to accelerate the learning on the target dataset. (Wang et al. 2018) finetunes a pre-trained GAN on a target dataset to get better performance. (Noguchi and Harada 2019) transfers knowledge from a large dataset to a small dataset by re-computing batch statistics. Existing methods focus on either the image domain or the neural language processing domain (Shin, Hwang, and Sung 2016). For these methods, hundreds of training samples are still required. Compared with these approaches, we focus on pose domain adaptation, and our method only requires few-shot guidance for transferring. After LoRA (Hu et al. 2021) is widely used in Large Language Model finetuning, the researchers in Content Generation are inspired to introduce or extend this technique in generative model (Mou et al. 2024). Compared with the light-weight but global model finetuning in LoRA, FlexPose only focuses on calibrating specific layers with semantics of pose geometric transformation locally to satisfy the linear and sparse finetuning requirements.\n\nHuman Pose Estimation. 2D Human pose estimation is a task that predicts the 2D pose from a single image. Fullysupervised methods (Andriluka, Roth, and Schiele 2009; Bai and Wang 2019; Belagiannis and Zisserman 2017) utilize large-scale annotated datasets such as COCO (Lin et al. 2014), Human3.6M (Ionescu et al. 2014) and 3DHP (Mehta et al. 2017) for model training. Weakly-supervised (Kanazawa et al. 2018; Gecer et al. 2019; Geng, Cao, and Tulyakov 2019; Wang et al. 2023) and unsupervised (Shu et al. 2018; Jakab et al. 2018) methods such as KeypointGAN (Jakab et al. 2020) have been proposed to reduce the dependence on the expensive pose annotation. These methods require supervised post-training or additional prior knowledge to generate meaningful landmarks, which can serve as a distance measurement between the provided prior knowledge and the target distribution. To match poses generated by FlexPose with unlabelled images in the target dataset, we employ an unsupervised method (Jakab et al. 2020) in addition to supervision from adversarial training. This matching procedure serves as an evaluation method for FlexPose and is further detailed in Section 4. Recently, test-time adaptation (Li et al. 2021; Cui et al. 2023; Hu et al. 2024) has proven to be an effective way to deal with domain shift in pose estimation. It utilizes selfsupervised learning during inference to adapt model to the input human appearance distribution. Compared with FlexPose which focuses on PDA, Test-time adaptation tackle the issues in input image domain shift.\n\n![](images/66c74430b9bf91bf47590e2839c5d9f30b917e046525ce0c24d2b0f1258ff26c.jpg)  \nFigure 2: An illustration of the FlexPose framework for pose distribution adaptation. There are three main steps in our framework: $\\textcircled{1}$ We train a skeleton image generator to learn the pose prior from the source pose distribution; $\\textcircled{2}$ The source generator is transferred to a target generator with limited target pose guidance to achieve pose distribution adaptation; $\\textcircled{3}$ We utilize the target generator to generate target pose annotations for downstream tasks.\n\n# 3 Method\n\nGiven a limited number of 2D pose annotations set $T =$ $\\left\\{ \\pmb { y } _ { t } | \\pmb { y } _ { t } \\in \\mathbb { R } ^ { M \\times 2 } \\right\\}$ of a newly collected human pose images, FlexPose aims to estimate the whole distribution $\\mathcal { D } _ { t }$ which pose annotation $T$ belongs to, and generate any number of new pose annotations that follow the distribution. $M$ here is the number of joints in each pose. This task setting is challenging. However, we believe that with the prior from sufficient off-the-shelf annotations $S = \\left\\{ \\pmb { y } _ { s } | \\pmb { y } _ { s } \\overset { * } { \\in } \\mathbb { R } ^ { M \\times 2 } \\right\\}$ , the distribution $\\mathcal { D } _ { t }$ can be estimated and well shaped. In this paper, we transfer the distribution $\\mathcal { D } _ { s }$ estimated from $S$ to the target pose domain to estimate the target distribution $\\mathcal { D } _ { t }$ by considering the guidance from 2D pose annotations set $T$\n\n# 3.1 Overview\n\nAs illustrated in Figure 2, our framework consists of three phases: $\\textcircled{1}$ Generic Pose Distribution Estimation. We learn a generator $g _ { s } ( \\cdot )$ on the pose set $S$ to estimate the pose distribution $\\mathcal { D } _ { s }$ . The generator takes a latent code $z$ as input and outputs a skeleton $\\hat { \\pmb x } _ { s }$ , i.e. $\\hat { \\pmb { x } } _ { s } = g _ { s } ( \\pmb { z } )$ . We take the distribution of generated $\\hat { \\pmb x } _ { s }$ to mimic that of the generic pose $\\pmb { x } _ { s }$ . Here, $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ is the corresponding skeleton image of an annotation $_ y$ as shown in the left part of Figure 2. $\\textcircled{2}$ Pose Distribution Adaptation. Given the limited target annotation set $T$ , we transfer $g _ { s } ( \\cdot )$ to fit the pose distribution $\\mathcal { D } _ { t }$ and learn a new generator $g _ { t } ( \\cdot )$ of the target pose domain. Considering the limited knowledge acquired from target pose annotation $T$ , we introduce three regularizations, Linear, Sparse and Pose-mixup, to avoid reaching a collapse solution. $\\textcircled{3}$ Target\n\nPose Sampling. The transferred generator $g _ { t } ( \\cdot )$ can flexibly synthesize any number of fake pose annotations by randomly sampling in the latent space. This generated annotation set $\\hat { T }$ will be treated as an extension of given annotations set $T$ in the downstream tasks, e.g., Keypoints Annotation and Poseconditional Human Image Generation, since poses within both of them follow the distribution $\\mathcal { D } _ { t }$ .\n\n# 3.2 Generic Pose Distribution Estimation\n\nDeep generative models have been widely verified that they have a rich capacity to well approximate image distributions when given sufficient training data. Motivated by the success of these generative models (Karras, Laine, and Aila 2019) on natural/artificial image generation, we treat 2D pose annotations ${ \\pmb y } _ { s } , { \\pmb y } _ { t } \\in \\mathbb { R } ^ { M \\times 2 }$ as skeleton images $\\pmb { x } _ { s } , \\hat { \\pmb { x } } _ { t } \\in \\mathbb { R } ^ { C \\times W \\times H }$ and ext nd an image generator to generate 2D pose annotations by synthesizing corresponding skeleton images. As shown in the left part of Figure 2, the transformation from the 2D keypoints to the skeleton images can be implemented by functions $\\alpha ( \\cdot )$ , namely $\\pmb { x } = \\alpha ( \\pmb { y } )$ , where $\\alpha ( \\cdot )$ simply draws keypoints from $_ y$ and connects them with straight lines on a blank figure. The visual effect is similar to the stick man. To achieve precise semantic alignment with the appearance correspondence, each bone in the skeleton image is assigned a unique color. Therefore, $C$ of each skeleton image is set as three (RGB channels). Compared with Black&White, the colorful embedding brings marginal improvement in the quality of generated skeletons.\n\nA generator can be formulated as a mapping function $g ( \\cdot )$ which gets a latent code $z$ and outputs a skeleton image $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ . The probability distribution of skeleton images hence is estimated by $p ( { \\pmb x } ) = p ( { \\pmb z } ) p _ { g } ( { \\pmb x } | { \\pmb z } )$ . We assume that the pose distributions of different datasets share similar pose prior, and their distributions can transfer to one another by geometric transformations. Based on this assumption, we further factorize the generator $g ( \\cdot )$ as $g = \\phi \\circ \\delta$ . Therefore, the source skeleton image generator can be formulated as\n\n![](images/59bf46d7910ba91212b7a8cfc0e88520c3ffaea691993e3cc6cfec0d668ce815.jpg)  \nFigure 3: An illustration of the generator decomposition. We use $\\tau ( \\cdot )$ to adjust the source generator for pose distribution adaptation.\n\n$$\np ( \\hat { \\pmb x } _ { s } ) = p ( z ) p _ { g } ^ { s } ( \\hat { \\pmb x } _ { s } | z ) = p ( z ) p _ { \\delta } ^ { s } ( \\pmb { h } _ { s } | z ) p _ { \\phi } ( \\hat { \\pmb x } _ { s } | \\pmb { h } _ { s } ) ,\n$$\n\nin which $\\phi ( \\cdot )$ preserves the learned pose prior and $\\delta ( \\cdot )$ records the mapping from the learned prior $\\boldsymbol { h }$ to the skeleton image $\\pmb { x } _ { s }$ of a certain pose domain. Similarly, the distribution of target domain can be formulated as $p ( \\hat { \\pmb x } _ { t } ) =$ $p ( z ) ~ p _ { \\delta } ^ { t } ( h _ { t } | z ) ~ \\overline { { p _ { \\phi } ( \\hat { x } _ { t } | h _ { t } ) } }$ . With the prior sharing assumption, the pose distribution adaptation aims at transferring pretrained conditional probability $p _ { \\delta } ^ { s } ( h _ { s } | z )$ to $p _ { \\delta } ^ { t } ( h _ { t } | z )$ with guidance from the pose annotation set $T$ :\n\n$$\n\\begin{array} { r } { p _ { \\delta } ^ { s } ( \\pmb { h } _ { s } | z ) \\overset { T } { \\longrightarrow } p _ { \\delta } ^ { t } ( \\pmb { h } _ { t } | z ) . } \\end{array}\n$$\n\nConsidering the ability of StyleGAN in separating highlevel attributes and in the interpolation between these attributes, we utilize StyleGAN network architecture to disentangle the pose prior and the transformation of the source skeleton image generator,\n\n$$\ng _ { s } = \\phi \\circ \\delta _ { s } = \\phi \\circ ( A \\circ f ) _ { s } ,\n$$\n\nwhere $f ( \\cdot )$ is a non-linear mapping that takes random noise as input and outputs a random vector. $A ( \\cdot )$ is a learned affine transformation and can be treated as a block diagonal matrix with $L$ blocks, where $L$ is the number of layers. The output of $A ( \\cdot )$ is the style code to modulate the synthesis network $\\phi ( \\cdot )$ by adaptive instance normalization. Due to the ability of StyleGAN in style control, we can directly adapt the distribution of source skeleton image to the target domain by adjusting the style code.\n\n# 3.3 Pose Distribution Adaptation\n\nAs illustrated in Figure 3, to transfer $p _ { \\delta } ^ { s } ( h _ { s } | z )$ to $p _ { \\delta } ^ { t } ( h _ { t } | z )$ , we adjust the style code by introducing a transfer function $\\tau ( \\cdot )$ at the output of $\\delta ( \\cdot )$ , and therefore the target domain generator is defined as\n\n$$\ng _ { t } = \\phi \\circ \\delta _ { t } = \\phi \\circ ( \\tau \\circ \\delta _ { s } ) .\n$$\n\nTo learn the transfer function $\\tau$ , we first randomly sample $| T |$ latent codes $z$ (one for each pose in $T$ ) from the latent\n\nspace, and require the generator maps each code to corresponding skeletons in $T$ . This transferring procedure can be achieved by minimizing the following perceptual loss,\n\n$$\n\\operatorname* { m i n } _ { \\theta _ { \\tau } } \\mathcal { L } _ { s  t } = \\operatorname* { m i n } _ { \\theta _ { \\tau } } \\sum \\| \\Gamma \\big ( g _ { t } ( z ) ; \\theta _ { \\tau } \\big ) - \\Gamma \\big ( \\pmb { x } \\big ) \\| _ { 2 } ^ { 2 } ,\n$$\n\nwhere $\\theta _ { \\tau }$ is the parameter of $\\tau ( \\cdot )$ , $\\Gamma$ is a pre-trained feature extractor, $z$ is from the set of sampled latent codes, and $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ is the skeleton image drawn from the pose annotation set $T$ .\n\nHowever, the problem is we only have few-shot guidance $T$ from the target domain distribution. Given a data-starving deep learning model, the guidance is insufficient to reach a satisfactory solution. For that reason, we introduce three regularizations to alleviate the data-insufficient issue.\n\nLinear $\\&$ Sparse Regularization. Compared with finetuning the whole transformation function $\\delta _ { s }$ to reach $\\delta _ { t }$ , only adjusting the affine transformation from $A _ { s }$ to $A _ { t }$ , i.e. $A _ { t } = \\tau \\circ A _ { s }$ , can efficiently shrink the searching space of transfer solution, and therefore avoid overfitting. Meanwhile, the recent GAN inversion technique shows that the layer-wise style code in StyleGAN leads to the hierarchical disentanglement of local and global attributes, which aligns well with our motivation of adapting pose distribution by considering the global geometric transformation between poses. We thus adjust the source affine transformation $A _ { s }$ from the perspective of layer level, and limit the number of to-be-adjusted layers as small as possible. Considering the form of the affine transformation $A$ and the layer decoupling characteristics of StyleGAN, we empirically define the transfer function $\\tau ( \\cdot )$ as a block diagonal matrix,\n\n$$\n\\tau \\triangleq \\operatorname { d i a g } ( I , . . . , I , U _ { l _ { 0 } } , I , . . . , I , U _ { l _ { 1 } } , I , . . . , I ) ,\n$$\n\nwhere only a limited number of block is defined by $\\pmb { U }$ , i.e. $l _ { 0 }$ and $l _ { 1 }$ in this case, to follow the sparse regularization. We experimentally find that the earlier layers are most related to the geometric transformation. And we only learn those layers in our experiments. Meanwhile, other blocks are set as identity matrix $\\boldsymbol { \\mathit { I } }$ . We investigated how the choice of layer $l$ affects the transformation procedure in Section 4.4.\n\nPose-mixup Regularization. Most poses interpolated between two real poses physically exist, and their convex combinations build the real-world pose distribution. Inspired by the mixup regularization (Zhang et al. 2017) on images, we therefore extend it to 2D pose annotations and propose the Pose-mixup to enrich the guidance set. The main difference between mixup and Pose-mixup is that the mixup works on image space and the Pose-mixup works on keypoint space. Given that the mixup on skeleton space may lead to unreasonable results, Pose-mixup regularizes the neural network to learn the simple linear behavior in-between 2D poses and thus prevents the model from generating unrealistic human pose annotations. By mixing up the corresponding joints of any two 2D poses with mixup ratio $\\lambda \\in [ 0 , 1 ]$ , the extended annotation set $T ^ { * }$ from $T$ is then defined as,\n\n$$\nT ^ { * } = \\left\\{ y ^ { * } \\mid y ^ { * } = \\lambda y _ { i } + \\left( 1 - \\lambda \\right) y _ { j } , \\ y _ { i } , y _ { j } \\in T \\right\\} .\n$$\n\n![](images/5b059c85c918ec28c25589f8e081208427b568111640c2c00380d716f8cd1aee.jpg)  \nFigure 4: Illustration of pose distribution transformation. (a) Reconstruction loss with different choice of layer l. (b) Visualization of pose adaptation. The left and middle of each row are generated from the same random noise. The middle aims to mimic the right. (c) t-SNE visualization of human poses before and after adaptation. We visualize the pose distribution $( \\mathbb { R } ^ { M \\times 2 } )$ in a two-dimensional space.\n\nTable 1: The results of distribution distance measurement.   \n\n<html><body><table><tr><td>Method</td><td>Target</td><td>Source</td><td>MMD² (↓)</td><td>FD (↓)</td></tr><tr><td>FreezeD</td><td rowspan=\"3\">H3.6M</td><td>COCO</td><td>0.081</td><td>3.77</td></tr><tr><td>AdaGAN</td><td>COCO</td><td>0.052</td><td>2.67</td></tr><tr><td>LoRA FlexPose</td><td>COCO COCO</td><td>0.035 0.029</td><td>1.36 0.80</td></tr></table></body></html>\n\n# 3.4 Target Pose Sampling\n\nOnce the transferred generator $g _ { t } ( \\cdot )$ is obtained, we can generate theoretically as many target skeleton images $\\tilde { \\pmb { x } } _ { t }$ as possible by randomly sampling latent codes in the estimated target distribution $\\mathcal { D } _ { t }$ . Unfortunately, the generated target skeleton images are not perfect and may bring in artifacts which mislead the training of a neural network. To address this issue, we utilize $\\beta ( \\cdot )$ to filter out the random noise. $\\beta ( \\cdot )$ is a neural network regressor pre-trained on $S$ and acts as a tight information bottleneck that preserves skeleton information and ignores random noise. Following the generation of the fake skeleton images $\\tilde { \\pmb { x } } _ { t }$ from $g _ { t } ( \\cdot )$ , we extract the coordinates of interpretable 2D keypoints $\\hat { T } = \\{ \\hat { y } _ { t } \\}$ from it, e.g., hands, by applying $\\hat { \\pmb y } _ { t } = \\beta ( \\tilde { \\pmb x } _ { t } )$ . Thereafter, we can get a clean generated skeleton $\\hat { \\pmb x } _ { t }$ by a re-render process,\n\n$$\n\\hat { \\pmb { x } } _ { t } = \\alpha ( \\hat { \\pmb { y } } _ { t } ) = \\alpha \\big ( \\beta ( \\tilde { \\pmb { x } } _ { t } ) \\big ) .\n$$\n\nThese generated skeleton images and the corresponding generated 2D pose annotations can be further utilized to assist any pose-related down-stream tasks.\n\n# 4 Experiments\n\nIn this section, we first evaluate the distribution similarity between transferred distribution and target distribution via two standard metrics. Then, we show how FlexPose can improve the performance of existing unsupervised landmark detection algorithms and benefit unlabelled human pose dataset annotation. At last, we extensively discussed how each part of FlexPose works.\n\n# 4.1 Pose Distribution Transformation\n\nIn this subsection, we conduct a transformation experiment between COCO (Lin et al. 2014) and Human3.6M (Ionescu et al. 2014) to show how FlexPose works.\n\nExperiment Setting. We train a StyleGAN (Karras, Laine, and Aila 2019) using the skeleton images from the source datasets to estimate the distribution of source human pose. And then we transform the estimated distribution to the target one according to several samples from target dataset. For source dataset COCO, we only keep the annotated people instances with full pose annotations to construct a training set of 32k samples. The training of StyleGAN follows standard protocol in the original work. In the transformation phase, we only use 30 annotations from the target dataset Human3.6M (two for each class). The size of interpolated pose set $( | T ^ { * } | )$ is set as 1000. We experimented with changing different layers and found that setting $\\scriptstyle { l = 3 }$ , i.e., transferring the third coarsest layer, usually gets the lowest reconstruction loss in Equation (5) as shown in Figure 4a. So, we set $\\scriptstyle { l = 3 }$ in all experiments. A detailed setting and deeper analysis can be found in appendix. When adaptation phase ends, we sample new poses from generator and treat them as Adapted COCO. Evaluation $\\&$ Visualization. Qualitatively, we show the visual result of pose transformation in Figure 4b. For each row, we show one skeleton (Left) that was randomly sampled from the generator before transformation, one skeleton (Middle) that was sampled from the transformed generator by using the same latent noise as the Left, and one skeleton (Right) in the few-shot annotation set $T$ from target dataset. We can see that the Left and the Middle generated from the same random noise are visually quite different, and the Middle is more similar to the Right.\n\nIn Figure 4c, we plot the t-SNE embedding of the poses generated by FlexPose (Adapted COCO), comparing it with the embedding of poses from the source (COCO) and target (Human3.6M) dataset. As can be seen, the embedding of poses from the source and target dataset are separated, and the distribution of generated poses significantly overlaps with the target ones. We also noted that the pose distributions are ‘mismatched’ in the upper right region. Considering that only two shots are utilized as guidance for each class during the transformation, such a mismatch is reasonable.\n\nTable 2: Results on human pose annotation task. S-H3.6M and $_ { \\mathrm { H 3 . 6 M } }$ are short for Simplified Human3.6M dataset and Human3.6M dataset respectively. The threshold of PCK is $10 \\%$ for $\\mathrm { S - H } 3 . 6 \\mathrm { M }$ and $20 \\%$ for $_ { \\mathrm { H 3 . 6 M } }$ in this table.   \n\n<html><body><table><tr><td>Method</td><td>Target</td><td>Source</td><td>MSE(↓）PCK(↑)</td></tr><tr><td>Baseline FreezeD AdaGAN LoRA FlexPose</td><td>H3.6M</td><td>COCO 17.86 COCo 20.60 COCO 14.88 COCO 13.85 COCO 13.19</td><td>0.015 0.081 0.395 0.430 0.585 0.685</td></tr><tr><td>Baseline FreezeD AdaGAN LoRA FlexPose Baseline</td><td>COCO COCo COCo COCo COCO 3DHP</td><td>5.47 7.63 5.36 5.02 3.79 12.66</td><td>0.003 0.455 0.512 0.770 0.000</td></tr><tr><td>AdaGAN FreezeD LoRA FlexPose Baseline</td><td>S-H3.6M</td><td>3DHP 7.23 3DHP 6.28 3DHP 6.15 3DHP 5.98 SURREAL</td><td>0.215 0.206 0.314 0.467 0.000</td></tr><tr><td>FreezeD AdaGAN LoRA FlexPose</td><td></td><td>11.18 SURREAL 11.38 SURREAL 6.63 SURREAL 6.52 SURREAL 6.47</td><td>0.006 0.228 0.337 0.499</td></tr></table></body></html>\n\nQuantitatively, we measure the similarity between the transferred distribution and target distribution using the Fr´echet distance (FD), which follows from the Wassersteinbased definition of FID (Heusel et al. 2017) without the application of the pre-trained Inception network. We also measure the square of Maximum Mean Discrepancy (MMD) to provide more insights. The measurements are conducted on the keypoint coordinates space.\n\nWe compare our method with three strong competitors. AdaGAN (Noguchi and Harada 2019), FreezeD (Mo, Cho, and Shin 2020) and LoRA (Hu et al. 2021) (rank $r { = } 8$ ). Both AdaGAN and FreezeD suggest finetuning-based strategies with regularization. LoRA is the most related work to our FlexPose, introducing low-rank regularization to the generative model. For all methods, we generate $5 0 \\mathrm { k }$ samples from the transferred generative model and compare them with all samples in the target dataset Human3.6M. In Table 1, experiment results suggest that FlexPose gives superior performance on both MMD and FD evaluation, indicating the transferred distribution shares more similar characteristics to the target distribution. The finding remains the same as that of the observation on qualitative evaluation.\n\n# 4.2 Unlabelled Human Pose Dataset Annotation\n\nTo further evaluate the quality of transformed pose quantitatively and show potential downstream application of FlexPose, we show how can we annotated unlabelled humanrelated dataset with the help of FlexPose.\n\nTable 3: Results on human face annotation task.   \n\n<html><body><table><tr><td>Method</td><td>Target</td><td>Source</td><td>MSE (↓)</td><td>PCK (↑)</td></tr><tr><td>Baseline</td><td></td><td>300-VW</td><td>18.78</td><td>0.679</td></tr><tr><td>AdaGAN FreezeD</td><td></td><td>300-VW</td><td>11.95</td><td>0.785</td></tr><tr><td></td><td>WFLW</td><td>300-VW</td><td>11.66</td><td>0.779</td></tr><tr><td>LoRA</td><td></td><td>300-VW</td><td>11.77</td><td>0.760</td></tr><tr><td>FlexPose</td><td></td><td>300-VW</td><td>11.64</td><td>0.766</td></tr></table></body></html>\n\n![](images/13aa9a07e7a5e018021bcc46612b763f7bfc29e45193abe1bb1c95b80009a26a.jpg)  \nFigure 5: Visualization of human face landmark annotation on WFLW dataset. Upper Row: landmarks given by matching algorithm. Bottom Row: landmarks in the upper row with their corresponding human faces.\n\nPose-Image Matching Algorithm. In dataset annotation tasks, our goal is to assign each image in the target dataset to the most closely related pose in the estimated target distribution $\\mathcal { D } _ { t }$ . Existing self-supervised human pose detection methods (Jakab et al. 2018; Lorenz et al. 2019; Thewlis, Bilen, and Vedaldi 2017) are usually constrained by the high relevance between model prediction and input image. Among them, KeypointGAN (Jakab et al. 2020) can match unpaired images and annotations by forcing the distribution of detector predictions to align with the existing poses. We train KeypointGAN by using human images from target dataset and generated pose set $\\hat { T }$ . Once the training process is completed, the model prediction on samples can be treated as the best-matched annotations in given distribution.\n\nSource Datasets. Apart from COCO, we also use MPI-INF3DHP (3DHP) (Mehta et al. 2017) which contains more than 1.8 million human pose annotations from eight subjects and covers eight complex exercise activities. SURREAL (Varol et al. 2017) is a synthetic dataset containing more than six million frames of people in motion.\n\nTarget Datasets. The large-scale dataset Human3.6M (Ionescu et al. 2014) has 3.6 million samples. The Simplified Human3.6M dataset (Zhang et al. 2018) contains 800k training and around 90k testing images. We use all human images in the target dataset and randomly select several guide poses. Evaluation Metrics. Since dataset annotation shares similar targets with 2D landmark detection. We report 2D landmark detection performance for evaluation. Two standard evaluation metrics are considered to compare our method with baselines. The MSE column reports a mean square error in pixels overall pre-defined common joints. The Percentage of Correct Key-points $( \\mathrm { P C K } { - \\rho } )$ is used as an accuracy metric that measures if the distance between the predicted keypoint and the true joint is within a certain threshold $\\rho$ .\n\nPerformance Comparisons. We feed the generated skeleton images $\\hat { \\pmb x } _ { t }$ and RGB human images from target dataset into KeypointGAN to evaluate the effectiveness of each pose transformation algorithm. As a baseline, we train the detector on each target dataset by directly using the pose annotations set $S$ from the source dataset, which we denote as Baseline in the comparison and can be roughly treated as the worst case. We also employ three strong competitors, AdaGAN, FreezeD, and LoRA, for comparison.\n\nTable 4: Ablation study on human pose annotation. The target dataset is Simplified-Human3.6M for all experiments. C, D, S are short for COCO, 3DHP, SURREAL dataset.   \n\n<html><body><table><tr><td># Source Layer Mixup</td><td>Linear Shots MSE PCK</td></tr><tr><td>1 C 3</td><td>12 3.79 0.77 0.78</td></tr><tr><td>C</td><td>√ √ 1,3 √ √</td></tr><tr><td>2 3 C 3,5 √</td><td>12 3.82 12 4.02 √</td></tr><tr><td>4 C ALL √</td><td>0.61 √ 12 4.50 0.66</td></tr><tr><td>5 3</td><td>12 5.98 0.44</td></tr><tr><td>D 6 D ALL × √</td><td>√ √ 12 9.82 0.01</td></tr><tr><td>7 D ALL</td><td>× × 12 12.32 0.00</td></tr><tr><td>8 C 3</td><td>√ √ 12 3.79 0.77</td></tr><tr><td>9 C 3</td><td>√ √ 24 3.80 0.75</td></tr><tr><td>10 C 3 √ √</td><td>48 3.73 0.70</td></tr><tr><td>11 D 3 √</td><td>√ 12 5.98 0.47</td></tr><tr><td>12 DC 3 √</td><td>12 5.28 0.59 √ 12 5.19 0.59</td></tr><tr><td>13 DCS 3 √</td><td></td></tr></table></body></html>\n\nQuantitatively, we compare their performance with FlexPose on human pose estimation in Table 2. As shown in Table 2, the Baseline has much lower performance on the target dataset as the pose annotations are from different datasets, especially when some of them have a distinct pose distribution from that of the target dataset, e.g., when 3DHP or SURREAL is the source dataset and Simplified Human3.6M is the target one. FlexPose gets better results on all settings under both metrics. FlexPose largely reduces the performance gap when the pose distribution of the source dataset is very different from that of the target distribution, e.g., MSE $1 2 . 7 \\substack {  6 . 0 }$ and PCK10 $0 . 0 0 {  } 0 . 4 7$ when adaptation occurs from 3DHP to Simplified Human3.6M. The results show that FlexPose is effective at generating similar poses with the target dataset, even with less to only two poses per class in the target dataset.\n\n# 4.3 Unlabelled Human Face Dataset Annotation\n\nIntroducing FlexPose to human face landmarks transfer is straightforward since both the human pose and the human face consist of a set of pre-determined keypoints.\n\nDatasets. WFLW (Wu et al. 2018) has 10 thousand samples with 98 facial landmarks, where 7.5 thousand for training and 2.5 thousand for testing. 300-VW (Sagonas et al. 2013) consists of 300 Videos in the wild and contains ${ \\sim } 9 5$ thousand annotated human faces in the training set. We treat 300-VW as the source dataset and only use its annotations for training StyleGAN. And few-shot annotations in the target dataset WFLW are utilized for transformation. We only keep the shared 68 facial landmarks in two datasets.\n\nExperiments Settings and Results. The evaluation metrics and the experiment protocols are the same as that in the human pose. The size of the few-shot guidance set from target dataset is set as 30. We report the evaluation results on the validation set of WFLW in Table 3. FlexPose still outperforms the baseline by a large margin (MSE $1 8 . 7 8 $ 11.64 and PCK $0 . 6 7 9  0 . 7 6 6 )$ . Given that the human face can be treated as a rigid body approximately and are easier to transfer, FlexPose achieves comparable performance with previous SOTA methods, AdaGAN, FreezeD and LoRA.\n\nWe show the detected human face landmarks in Figure 5. The human face detector trained with generated face landmarks can handle human faces in different directions well.\n\n# 4.4 Ablation Study & Parameter Sensitivity\n\nIn Table 4, ablation studies are conducted on:\n\nEffect of Regularization. We remove part of proposed regularization from our FlexPose, and the results are #1-#7. From #1-#4, we gradually relax the sparsity regularization by allowing more blocks in the diagonal matrix $\\tau$ not to be an identity matrix $\\boldsymbol { \\mathit { I } }$ . The performance only drops by an acceptable level thanks to the Linear and Mixup regularization. Furthermore, in $\\# 5 . \\# 7$ , we further relax the Mixup and Linear regularization, which significantly hurt the quality of generated images and lower the model accuracy in downstream tasks.\n\nNumber of Shots from Target Dataset. Under the setting of $\\mathrm { C O C O } { \\to } \\mathrm { S } { \\cdot } \\mathrm { H } 3 . 6 \\mathrm { M }$ , we increase the number of shots from 12 to 48 and found that the performance of the pose detector has no obvious difference. The results can be found in #8-#10. An explanation is that the increment of few-shot samples from the target dataset brings a limited gain of information compared with the strong prior trained on large-scale datasets. Few samples are enough for target distribution localization. Choice of Layers l. In previous experiments, we empirically choose $\\scriptstyle { l = 3 }$ in Equation (4) for all experiments. We found that the choice of $l$ is not strictly fixed. We have also tried a composition of multi-layer, and the results can be found in $\\# 2 \\# 4$ . The result in $\\# 4$ shows the necessity of sparse regularization. We leave the best choice of $l$ to future work. Multi-source Datasets. To study the effect of the setting where the source annotations are from different datasets, we use the union of different source datasets to train the generic generator. The result indicates that the increasing diversity on the source dataset $\\# 1 1 \\to \\# 1 2 \\to \\# 1 3$ ) brings better results on the target dataset. By utilizing FlexPose, the performance of downstream task models can benefit from collecting a more diverse pose dataset, which is much easier compared with collecting a realistic human dataset with accurate landmarks. However, the result of $\\# 1 3$ is still worse than that of #1, which indicates the trade-off between diversity and similarity to the target dataset when choosing the source.\n\n# 5 Conclusion\n\nWe aim to transfer knowledge in the pose domain and propose an effective method named FlexPose. Our approach allows us to adapt an existing pose distribution to a different target one by using a few poses from the target dataset and generating theoretically infinite poses following the target distribution. FlexPose can be used on several pose-related works. In future work, we hope to extend our method to a more generic pose domain adaptation approach.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是如何在新收集的图像数据上高效且低成本地标注人体姿态，克服现有方法在跨数据集姿势分布适应时的高标注成本和隐私泄露风险。\\n> *   该问题的重要性在于，高质量的姿态标注对于训练深度神经网络至关重要，尤其在姿态估计、人脸关键点标注等下游任务中具有关键价值。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出FlexPose框架，通过调整预训练生成器的风格代码（style code），将源姿势分布转移到目标分布，仅需少量目标姿势标注作为指导。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 将姿态域适应任务建模为骨架图像生成器的转移，证明目标姿态分布可以从学习到的姿态先验中近似得到。\\n> *   **贡献2：** 引入三种正则化（线性、稀疏和姿态混合）以高效转移姿态分布，计算和存储成本低。\\n> *   **贡献3：** 在跨数据集设置下，FlexPose在生成模型基础的迁移学习方法中达到最先进性能，MMD²指标从基线的0.081降至0.029，FD指标从3.77降至0.80。\\n> *   **贡献4：** 在Human3.6M数据集上的PCK@20达到0.685，比基线（0.015）提升45.7个百分点。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   核心原理：不同数据集的姿态分布共享相似的铰链结构先验，其差异本质上是几何变换（如枢轴方向、关节旋转、骨骼长度比）分布的差异。通过调整生成器的风格代码，可以校准这些变换分布。\\n> *   设计哲学：将姿态标注视为骨架图像，利用StyleGAN的层次解耦特性，仅微调与几何变换相关的特定层，实现高效分布适应。\\n\\n> **创新点 (Innovations)**\\n> *   **先前工作局限：** 传统域适应方法需要源域人类外观图像，可能引入数据分布偏差或隐私问题。\\n> *   **本文改进：** 1) 仅操作姿态域，避免外观图像使用；2) 通过稀疏线性模块调整特定层，减少过拟合；3) 引入姿态混合正则化增强样本多样性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **通用姿态分布估计：** 使用源数据集训练StyleGAN生成骨架图像，学习姿态先验。\\n> 2.  **姿态分布适应：** 通过插入可学习的轻量级线性模块（τ函数）调整风格代码，最小化目标骨架图像的感知损失。\\n> 3.  **目标姿态采样：** 从适应后的生成器中随机采样，生成目标姿态标注用于下游任务。\\n> *   **关键公式：** 目标生成器定义为 $g_t = \\\\phi \\\\circ \\\\delta_t = \\\\phi \\\\circ (\\\\tau \\\\circ \\\\delta_s)$，其中τ是块对角矩阵，仅调整特定层。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   FreezeD、AdaGAN、LoRA\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在MMD²指标上：** 本文方法在Human3.6M数据集上达到了 **0.029**，显著优于基线模型FreezeD (0.081) 和AdaGAN (0.052)。与表现最佳的基线相比，提升了0.023。\\n> *   **在Fr´echet距离（FD）上：** 本文方法的FD为 **0.80**，远低于FreezeD (3.77) 和AdaGAN (2.67)，表明生成的姿态分布更接近目标分布。\\n> *   **在姿态标注任务上：** 使用FlexPose生成的标注训练模型，在Human3.6M数据集上的PCK@20达到 **0.685**，比基线（0.015）提升45.7个百分点。\\n> *   **在人脸标注任务上：** 在WFLW数据集上，FlexPose的MSE为11.64，PCK为0.766，优于基线（MSE 18.78，PCK 0.679）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   姿态域适应 (Pose Domain Adaptation, PDA)\\n*   骨架图像生成 (Skeleton Image Generation, N/A)\\n*   风格代码调整 (Style Code Adaptation, N/A)\\n*   少样本学习 (Few-shot Learning, FSL)\\n*   生成对抗网络 (Generative Adversarial Network, GAN)\\n*   弱监督姿态估计 (Weakly-supervised Pose Estimation, N/A)\\n*   几何变换 (Geometric Transformation, N/A)\\n*   正则化 (Regularization, N/A)\"\n}\n```"
}