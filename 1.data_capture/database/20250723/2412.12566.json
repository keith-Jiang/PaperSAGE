{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.12566",
    "link": "https://arxiv.org/abs/2412.12566",
    "pdf_link": "https://arxiv.org/pdf/2412.12566.pdf",
    "title": "ITP: Instance-Aware Test Pruning for Out-of-Distribution Detection",
    "authors": [
        "Haonan Xu",
        "Yang Yang"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2024-12-17",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "Nanjing University of Science and Technology",
        "Pazhou Lab"
    ],
    "paper_content": "# ITP: Instance-Aware Test Pruning for Out-of-Distribution Detection\n\nHaonan ${ \\bf X } { \\bf u } ^ { 1 }$ , Yang Yang1,2\\*\n\n1Nanjing University of Science and Technology 2Pazhou Lab, Guangzhou {xhnxhn, yyang}@njust.edu.cn\n\n# Abstract\n\nOut-of-distribution (OOD) detection is crucial for ensuring the reliable deployment of deep models in real-world scenarios. Recently, from the perspective of over-parameterization, a series of methods leveraging weight sparsification techniques have shown promising performance. These methods typically focus on selecting important parameters for indistribution (ID) data to reduce the negative impact of redundant parameters on OOD detection. However, we empirically find that these selected parameters may behave overconfidently toward OOD data and hurt OOD detection. To address this issue, we propose a simple yet effective post-hoc method called Instance-aware Test Pruning (ITP), which performs OOD detection by considering both coarse-grained and finegrained levels of parameter pruning. Specifically, ITP first estimates the class-specific parameter contribution distribution by exploring the ID data. By using the contribution distribution, ITP conducts coarse-grained pruning to eliminate redundant parameters. More importantly, ITP further adopts a finegrained test pruning process based on the right-tailed Z-score test, which can adaptively remove instance-level overconfident parameters. Finally, ITP derives OOD scores from the pruned model to achieve more reliable predictions. Extensive experiments on widely adopted benchmarks verify the effectiveness of ITP, demonstrating its competitive performance.\n\n# 1 Introduction\n\nDeep neural networks (DNNs) have recently achieved remarkable success, driving significant progress in various fields, particularly in computer vision (Dosovitskiy et al. 2021; Yang et al. 2021, 2023a) and natural language processing (Radford et al. 2018; Achiam et al. 2023). However, when deployed in open-world environments, DNNs may fail by producing confident yet erroneous predictions for OOD data. Such unreliable behavior could lead to disastrous consequences, particularly in safety-critical fields like autonomous driving (Geiger, Lenz, and Urtasun 2012) and medical diagnosis (Litjens et al. 2017). Therefore, detecting and rejecting predictions on OOD inputs is crucial for ensuring the reliability of AI systems. This task, referred to as OOD detection, has gained widespread attention.\n\nParameter contribution distribution 1.0 ID: CIFAR-10 (airplane) 0.8 OOD: Textures Parameter #287 0.6 0.05 0.00 2 4 6 0.2 Overconfidence â–  0.0 2 0 2 4 6 8 Contribution\n\nMany techniques have been developed for OOD detection to enhance the discrimination between ID and OOD data. The training-based methods (Malinin and Gales 2018; Monteiro et al. 2023; Ghosal, Sun, and Li 2024) necessitate model training or fine-tuning, whereas post-hoc methods (Bendale and Boult 2016; Sun et al. 2022; Wang et al. 2022; Yang et al. 2023b, 2024) can be applied directly to pre-trained models off-the-shelf, eliminating the need for retraining process. This paper primarily focuses on post-hoc methods, which are easy to use, low-cost, and generally applicable. Early post-hoc methods, such as MSP (Hendrycks and Gimpel 2017), Energy (Liu et al. 2020), and GradNorm (Huang, Geng, and Li 2021), focus on devising a suitable OOD scoring function based on model outputs or gradients to indicate the likelihood that a sample originates from the OOD distribution. However, these methods overlook the fact that DNNs are typically over-parameterized to fit complex data distributions. This design makes the model more susceptible to noise from redundant parameters (Sun and Li 2022), leading to the brittleness of OOD detection.\n\nTo address this issue, a series of post-hoc methods that use network adjustments to improve OOD scoring has emerged. Sparsification-based methods, represented by DICE (Sun and Li 2022) and LINe (Ahn, Park, and Kim 2023), provide effective solutions for model over-parameterization. Their key idea is to selectively use the weight parameters that are important for ID data to derive OOD scores, thereby reducing the noise interference caused by redundant parameters in OOD detection. However, as illustrated in Figure 1, our empirical findings reveal that these selected important parameters are not always beneficial for OOD detection. When processing OOD data, these parameters may contribute abnormally high to ID predictions, behaving overconfidently. Such overconfidence increases the risk of the model predicting OOD data as ID categories with high confidence. As a result, the derived OOD scores become unreliable, leading to confusion between ID and OOD data and hindering OOD detection. Therefore, addressing parameter-level overconfidence is crucial for better separating ID and OOD data.\n\nTargeting this important problem, we propose Instanceaware Test Pruning (ITP), a simple yet effective method for OOD detection that considers parameter pruning from both coarse-grained and fine-grained perspectives. Concretely, ITP first estimates the class-specific parameter contribution distribution by exploring the ID data. Then, by leveraging the contribution distribution, ITP considers the following two key points to improve OOD detection performance: (1) ITP conducts coarse-grained pruning to remove noise interference caused by redundant parameters based on DICE (Sun and Li 2022). (2) ITP adopts a fine-grained test pruning process based on the right-tailed Z-score test, which adaptively removes instance-level overconfident parameters to reduce the risk of the model making confident yet erroneous predictions. We further provide insightful justification of the working mechanism of ITP from the perspective of the OOD score distribution. As a result of ITP, we show that the OOD scores derived from the model are more reliable and become more separable between ID and OOD data. Moreover, by examining parameter behavior in the weight space, ITP operates orthogonally to activation-based OOD detection methods (e.g., ReAct (Sun, Guo, and Li 2021)), facilitating their integration to push ITPâ€™s performance further.\n\n# 2 Related Work\n\nOOD detection aims to enable models to identify and reject predictions for OOD inputs, thereby ensuring the reliability of AI systems. We highlight three major lines of work.\n\nOOD Scoring Methods are designed to provide appropriate criteria for indicating the likelihood that an input sample is OOD. Distance-based (Lee et al. 2018; Huang et al. 2021; Sun et al. 2022) methods identify OOD data as being farther from the training set compared to ID data. Gradient-based methods (Huang, Geng, and Li 2021; Behpour et al. 2023) detect OOD inputs by utilizing information extracted from the gradient space. Output-based methods rely on model output logits to identify OOD data. MSP (Hendrycks and Gimpel 2017) directly uses the maximum SoftMax score to classify a test sample as either ID or OOD. ODIN (Liang, Li, and Srikant 2018) improves the MSP score by perturbing the input and applying temperature scaling to the logits. Energy score (Liu et al. 2020) uses the logsumexp of the output logits, which is consistent with input density and less susceptible to overconfidence problems. However, output-based methods are often disrupted by redundant or overconfident parameters, which can negatively impact OOD detection.\n\nSparsification-Based Methods perform OOD detection by pruning the weights of the model. DICE (Sun and Li 2022) proposes selectively using the most salient weights to derive the output for OOD detection. LINe (Ahn, Park, and Kim 2023) adopts the Shapley value (Shapley et al. 1953) for more precise pruning of redundant parameters and neurons, and it further considers the number of activated features by clipping activations. OPNP (Chen et al. 2023) prunes the parameters and neurons with exceptionally large or nearly zero sensitivities to mitigate over-fitting. These methods typically focus on selecting parameters that are important for ID prediction before testing for OOD detection. However, at test time, these selected parameters may exhibit overconfidence, which can impact the performance of OOD detection.\n\nActivation-Based Methods attempt to rectify activations to widen the gap between ID and OOD data. ReAct (Sun, Guo, and Li 2021) truncates activations above a pre-computed threshold to treat all activated features equally, thereby incorporating the number of activated features into consideration for OOD detection. VRA (Xu et al. 2023) zeros out anomalously low activations and truncates anomalously high activations. BATS (Zhu et al. 2022) proposes rectifying activations towards their typical set, while LAPS (He et al. 2024) improves BATS by considering channel-aware typical sets. These methods only examine anomalies at the activation level, whereas managing overconfidence anomalies at a more granular parameter level is important for more effective OOD detection.\n\n# 3 Proposed Method\n\n# 3.1 Preliminaries\n\nSetup. In this paper, we follow previous work (Yang et al. 2022) and focus on the setting of $K$ -way image classification. Let $\\mathcal { X }$ be the input space and $\\bar { \\mathcal { V } } \\bar { = } \\{ \\bar { 1 } , 2 , . . . , K \\}$ be the ID label space. Suppose that the training set $\\mathcal { D } =$ $\\{ ( \\mathbf { x } _ { i } , y _ { i } ) \\} _ { i = 1 } ^ { n }$ is drawn i.i.d from a joint distribution $\\mathcal { P } _ { \\mathcal { X } \\mathcal { Y } }$ defined over $\\boldsymbol { \\mathcal { X } } \\times \\boldsymbol { \\mathcal { Y } }$ . We denote $\\mathcal { P } _ { i n }$ as the marginal distribution of $\\mathcal { P } _ { \\mathcal { X } \\mathcal { Y } }$ on $\\chi$ , representing the ID distribution.\n\nLet $f$ be a model pre-trained from $\\mathcal { D }$ . For typical image classification architectures, $f$ first extracts a $D$ -dimensional penultimate feature representation $h ( \\mathbf { x } ) \\in \\mathbb { R } ^ { D }$ from an input $\\mathbf { x } \\in \\mathcal { X }$ . The last fully connected (FC) layer, parameterized by a weight matrix $\\dot { \\mathbf { W } } \\in \\mathbb { R } ^ { D \\times K }$ and a bias vector $\\mathbf { b } \\in \\mathbb { R } ^ { K }$ , then maps $h ( \\mathbf { x } )$ to the output vector $f ( \\mathbf { x } ) \\in \\mathbb { R } ^ { K }$ . Mathematically, the model output can be expressed as:\n\n$$\nf ( \\mathbf { x } ) = \\mathbf { W } ^ { \\top } h ( \\mathbf { x } ) + \\mathbf { b } .\n$$\n\nOut-of-distribution Detection. The goal of OOD detection is to determine whether a test input $\\mathbf { x }$ is from $\\mathcal { P } _ { i n }$ (ID) or not (OOD). In practice, the OOD detection task is often formu\n\n![](images/c1be637228527774c454490b338856fea183164987cd91484ea6182c6e10d93f.jpg)  \nFigure 2: Illustration of OOD detection using ITP. The overall procedure involves three main steps. (1) Training data are used to estimate the class-specific parameter contribution distribution for a pre-trained model. (2) Coarse-grained redundancy pruning applies a fixed pruning pattern to the modelâ€™s last layer to remove redundant parameters. (3) Fine-grained test pruning applies a customized pruning pattern to remove overconfident parameters for each test sample at test time. After applying ITP, the OOD scores derived from the model are better able to distinguish between ID and OOD data.\n\nlated as the following binary decision problem:\n\n$$\nG ( \\mathbf { x } ) = \\left\\{ \\begin{array} { l l } { \\mathrm { I D } , } & { \\mathrm { i f } \\ S ( \\mathbf { x } ) > \\tau , } \\\\ { \\mathrm { O O D } , } & { \\mathrm { i f } \\ S ( \\mathbf { x } ) \\le \\tau , } \\end{array} \\right.\n$$\n\nwhere $S ( \\cdot )$ represents the OOD scoring function, and $\\tau$ is a chosen threshold to ensure that the majority of $\\mathrm { I D }$ data are correctly classified (e.g., $9 5 \\%$ ). By convention, samples with higher OOD scores are heuristically classified as ID and vice versa. Given that the energy score (Liu et al. 2020) has been proven to be consistent with the input density and performs well, we mainly adopt the negative energy score as the OOD score, expressed as:\n\n$$\nS ( \\mathbf { x } ) = - E \\left( \\mathbf { x } \\right) = \\log \\sum _ { k = 1 } ^ { K } \\exp ( f _ { k } ( \\mathbf { x } ) ) ,\n$$\n\nwhere $E ( \\mathbf { x } )$ denotes the energy of $\\mathbf { x }$ , and $f _ { k } ( { \\bf x } )$ represents the $k$ -th output of the model.\n\n# 3.2 Parameter Contribution Distribution Estimation\n\nFigure 2 illustrates the overall procedure of our proposal. In this section, we first provide a detailed description of how to estimate the parameter contribution distribution, which will guide subsequent parameter pruning.\n\nDefining the Parameter Contribution. For a given input $\\mathbf { x }$ , the contribution of a specific parameter $\\pmb { \\theta } _ { i j }$ to the category $k$ is defined as the change in the $k$ -th output of the model by the presence or absence (setting $\\pmb { \\theta } _ { i j }$ to $0$ ) of the parameter $\\pmb { \\theta } _ { i j }$ , i.e.,\n\n$$\nc _ { k } ( \\mathbf { x } ; \\pmb { \\theta } _ { i j } ) = f _ { k } ( \\mathbf { x } ) - f _ { k } ( \\mathbf { x } ; \\pmb { \\theta } _ { i j } = 0 ) .\n$$\n\nPrevious studies (Zhu et al. 2022) highlight that the features extracted by early layers show similarities between ID and OOD data. In contrast, the later layers, particularly the penultimate layer, can extract more separable features. In this paper, we primarily focus on the last layerâ€™s model parameters, which significantly impact OOD detection by processing separable features and directly influencing particular class outputs. Especially, the contributions of the last layerâ€™s parameters $\\mathbf { W } _ { i j }$ can be expressed more simply using Equation 4 as follows (see Appendix for details):\n\n$$\nc _ { k } ( \\mathbf { x } ; \\mathbf { W } _ { i j } ) = { \\left\\{ \\begin{array} { l l } { \\mathbf { W } _ { i j } \\cdot h _ { i } ( \\mathbf { x } ) , } & { { \\mathrm { i f ~ } } k = j , } \\\\ { 0 , } & { { \\mathrm { i f ~ } } k \\neq j . } \\end{array} \\right. }\n$$\n\n# Estimating the Distribution of Parameter Contribution.\n\nThe distribution estimation relies on the assumption that the contribution of the last layerâ€™s parameters approximately follows Gaussian distributions parameterized by $( \\mu , \\sigma )$ , as observed empirically (see Appendix). According to the Equation 5, the parameter $\\mathbf { W } _ { i j }$ is specifically associated with class $j$ . To minimize potential bias from including data from other classes, we estimate the contribution distribution of parameter $\\mathbf { W } _ { i j }$ using only the training data for class $j$ . Let $\\mathcal { D } _ { j }$ denote the set of data points belonging to class $j$ . The mean $\\mu _ { i j }$ and standard deviation $\\sigma _ { i j }$ of the contribution distribution for the parameter $\\mathbf { W } _ { i j }$ are estimated using the following\n\nclass-specific formulas:\n\n$$\n\\begin{array} { l } { \\displaystyle \\mu _ { i j } = \\frac { 1 } { | \\mathcal { D } _ { j } | } \\sum _ { x \\in \\mathcal { D } _ { j } } c _ { j } ( x ; \\mathbf { W } _ { i j } ) , } \\\\ { \\displaystyle \\sigma _ { i j } = \\left( \\frac { 1 } { | \\mathcal { D } _ { j } | - \\delta } \\sum _ { x \\in \\mathcal { D } _ { j } } \\left( c _ { j } ( x ; \\mathbf { W } _ { i j } ) - \\mu _ { i j } \\right) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } , } \\end{array}\n$$\n\nwhere $| \\mathcal { D } _ { j } |$ denotes the cardinality of the set $\\mathcal { D } _ { j }$ , and $\\delta$ represents the correction factor. To correct the bias in the estimation of the population standard deviation, we adopt Besselâ€™s correction by setting $\\delta$ to 1.\n\n# 3.3 Instance-Aware Test Pruning (ITP)\n\nIn this section, we introduce two parameter pruning strategies with different levels of granularity used in ITP for posthoc enhancement in OOD detection: coarse-grained redundancy pruning (Figure 2(2)) and fine-grained test pruning (Figure 2(3)). Detailed descriptions of each strategy are provided below.\n\nCoarse-Grained Redundancy Pruning (CRP) remove redundant parameters in the over-parameterized weight space of the model. CRP operates at a coarse-grained level by applying a uniform pruning pattern across all test samples. Specifically, CRP measures each parameterâ€™s redundancy based on its average contribution to ID prediction. Parameters that fall within the lowest $p \\%$ of average contributions are deemed redundant and pruned. To implement this, we define a mask matrix $\\mathbf { M } ^ { \\mathrm { C R P } }$ for CRP, where the average contribution of a parameter is directly obtained from the mean $\\mu$ of its contribution distribution. The $( i , j )$ -th entry of the mask matrix $\\mathbf { M } _ { i j } ^ { \\mathrm { C R P } } \\in \\mathbf { M } ^ { \\mathrm { C R P } }$ is defined as follows:\n\n$$\n\\mathbf { M } _ { i j } ^ { \\mathrm { C R P } } = \\left\\{ \\begin{array} { l l } { 1 , } & { \\mathrm { i f } \\quad \\mu _ { i j } > \\Omega _ { p } , } \\\\ { 0 , } & { \\mathrm { i f } \\quad \\mu _ { i j } \\le \\Omega _ { p } , } \\end{array} \\right.\n$$\n\nwhere $\\Omega _ { p }$ represents the average contribution threshold at the lowest $p$ percentile. The model output after applying CRP can be expressed as follows:\n\n$$\nf ^ { \\mathrm { C R P } } ( \\mathbf { x } ) = \\left( \\mathbf { W } \\odot \\mathbf { M } ^ { \\mathrm { C R P } } \\right) ^ { \\top } h ( \\mathbf { x } ) + \\mathbf { b } ,\n$$\n\nwhere $\\odot$ denotes the element-wise multiplication. Through CRP, we remove noise interference from redundant parameters at a coarse-grained level in OOD detection by retaining only the parameters important for ID data, thereby enhancing the distinction between ID and OOD data.\n\nFine-Grained Test Pruning (FTP) prunes overconfident parameters with anomalously high contributions to ID prediction. FTP operates at a fine-grained level by customizing parameter pruning patterns for each test sample at test time. Specifically, FTP determines whether the parameter is overconfident by performing a right-tail test based on the $Z \\cdot$ - score. The $Z$ -score quantifies the deviation of a data point from the mean of the distribution, expressed as $( X - \\mu ) / \\sigma$ . In this context, $X$ represents the contribution being evaluated, while $\\mu$ and $\\sigma$ denote the mean and standard deviation of the contribution distribution, respectively. FTP can be framed as a single-sample hypothesis testing task:\n\n$$\n\\mathcal { H } _ { 0 } : \\frac { X - \\mu } { \\sigma } \\leq \\lambda , \\quad \\mathrm { v s . } \\quad \\mathcal { H } _ { 1 } : \\frac { X - \\mu } { \\sigma } > \\lambda ,\n$$\n\nID: ImageNet-1k ID: ImageNet-1k ID: ImageNet-1k OOD: iNaturalist OOD: iNaturalist OOD: iNaturalist   \nFPR95: 55.72% FPR95: 23.06% FPR95: 11.53% Energy CRP ITP (CRP + FTP) (a) ITP on iNaturalist benchmark ID: ImageNet-1k ID: ImageNet-1k ID: ImageNet-1k OOD: Textures OOD: Textures OOD: Textures   \nFPR95: 53.72% FPR95: 33.71% FPR95: $\\mathbf { 1 7 . 0 6 \\% }$ Energy CRP ITP (CRP + FTP) (b) ITP on Textures benchmark\n\nwhere the alternative hypothesis $\\mathcal { H } _ { 1 }$ implies that the parameter behaves overconfidently, and $\\lambda$ is the threshold, with $\\lambda > 0$ . In practice, we define ${ { \\bf { M } } ^ { \\mathrm { { F T P } } } } ( { \\bf { x } } )$ as the mask matrix customized for $\\mathbf { x }$ to perform FTP. The $( i , j )$ -th entry of the mask matrix $\\mathbf { M } _ { i j } ^ { \\mathrm { F T P } } ( \\bar { \\mathbf { x } } ) \\in \\mathbf { M } ^ { \\mathrm { F T P } } ( \\mathbf { x } )$ is defined as follows:\n\n$$\n\\mathbf { M } _ { i j } ^ { \\mathrm { F T P } } ( \\mathbf { x } ) = \\left\\{ \\begin{array} { l l } { 1 , } & { \\mathrm { i f } \\quad \\frac { c _ { j } ( \\mathbf { x } ; \\mathbf { W } _ { i j } ) - \\mu _ { i j } } { \\sigma _ { i j } } \\leq \\lambda , } \\\\ { 0 , } & { \\mathrm { i f } \\quad \\frac { c _ { j } ( \\mathbf { x } ; \\mathbf { W } _ { i j } ) - \\mu _ { i j } } { \\sigma _ { i j } } > \\lambda . } \\end{array} \\right.\n$$\n\nThe model output after applying FTP can be expressed as:\n\n$$\nf ^ { \\mathrm { F T P } } ( \\mathbf { x } ) = \\left( \\mathbf { W } \\odot \\mathbf { M } ^ { \\mathrm { F T P } } ( \\mathbf { x } ) \\right) ^ { \\top } h ( \\mathbf { x } ) + \\mathbf { b } .\n$$\n\nThrough FTP, we can adaptively prevent the abnormal increase in ID confidence caused by anomalously high contributions from overconfident parameters. This effectively reduces the risk of the model making confident yet erroneous predictions, thereby making ID data and OOD data more distinguishable.\n\nOverall Methods. Both CRP and FTP pruning strategies are designed to remove parameters that negatively impact OOD detection. CRP utilizes a fixed, coarse-grained pruning pattern across all test samples to reduce interference from noisy signals. FTP applies a customized, fine-grained pruning pattern to overconfident parameters for each test sample, mitigating the risk of overconfident predictions. ITP achieves both ways to improve OOD detection. As a result, the model outputs using ITP can be expressed as follows:\n\n$$\nf ^ { \\mathrm { I T P } } ( \\mathbf { x } ) = \\left( \\mathbf { W } \\odot \\mathbf { M } ^ { \\mathrm { C R P } } \\odot \\mathbf { M } ^ { \\mathrm { F T P } } ( \\mathbf { x } ) \\right) ^ { \\top } h ( \\mathbf { x } ) + \\mathbf { b } .\n$$\n\nSimilar to previous works (Ahn, Park, and $\\mathrm { K i m } \\ 2 0 2 3$ ), we can always use the original FC layer for prediction to preserve ID accuracy with negligible additional overhead.\n\n# 3.4 Insight Justification\n\nThe following remarks are provided to further explain how ITP widens the gap between ID and OOD data. Remark 1. CRP enhances the disparity between the left tail of the OOD score distributions for ID and OOD data. After employing CRP to prune redundant parameters, the logits reduction for the $k$ -th class is given by:\n\n$$\n\\Delta f _ { j } ( \\mathbf { x } ) = \\sum _ { d = 1 } ^ { D } ( 1 - \\mathbf { M } _ { d j } ^ { \\mathrm { C R P } } ) \\cdot c _ { j } ( \\mathbf { x } ; \\mathbf { W } _ { d j } ) .\n$$\n\nCRP eliminates parameters with the least average contribution to the $\\mathrm { I D }$ distribution. Hence, redundant parameters generally have higher contributions (manifesting as noise) to ID prediction for OOD data compared to ID data, i.e.,\n\n$$\n\\sum _ { \\mathbf { M } _ { d j } ^ { \\mathrm { C R P } } = 0 } c _ { j } ( \\mathbf { x } ^ { \\mathrm { O O D } } ; \\mathbf { W } _ { d j } ) > \\sum _ { \\mathbf { M } _ { d j } ^ { \\mathrm { C R P } } = 0 } c _ { j } ( \\mathbf { x } ^ { \\mathrm { I D } } ; \\mathbf { W } _ { d j } ) .\n$$\n\nTherefore, the reduction in logits for OOD data is greater than that for ID data $\\Delta f _ { j } ( \\mathbf { x } ^ { \\mathrm { { O O D } } } ) \\ > \\ \\Delta f _ { j } ( \\mathbf { x } ^ { \\mathrm { { I D } } } )$ . This improves the differentiation between the left tail of the energy score distributions for ID and OOD data, due to the positive correlation between energy scores and logits. This effect is empirically validated in Figure 3.\n\nRemark 2. FTP alleviates the overconfidence of OOD data at the right tail of the OOD score distribution. FTP removes the abnormally high contributions caused by overconfident parameters, thereby increasing the left skewness in the parameter contribution distribution, i.e.,\n\n$$\n\\begin{array} { r l } & { \\mathbb { E } _ { \\mathbf { x } } \\left[ \\left( \\frac { c _ { j } \\left( \\mathbf { x } ; \\mathbf { W } _ { i j } \\right) - \\mu _ { i j } } { \\sigma _ { i j } } \\right) ^ { 3 } \\cdot \\mathbf { M } _ { i j } ^ { \\mathrm { F T P } } ( \\mathbf { x } ) \\right] } \\\\ & { \\qquad < \\mathbb { E } _ { \\mathbf { x } } \\left[ \\left( \\frac { c _ { j } \\left( \\mathbf { x } ; \\mathbf { W } _ { i j } \\right) - \\mu _ { i j } } { \\sigma _ { i j } } \\right) ^ { 3 } \\right] . } \\end{array}\n$$\n\nSince parameter contributions directly determine the energy score, the left skewness of the energy score distribution will also increase accordingly. This helps reduce the risk of parameters being overly confident when handling OOD data. As a result, the overlap between the right tail of the OOD energy score distribution and the ID energy score distribution is diminished, as illustrated in Figure 3.\n\n# 4 Experiments\n\nIn this section, we first describe our experimental setup, then present the main results on multiple OOD detection benchmarks, followed by ablation studies and further analysis.\n\n# 4.1 Experimental Setup\n\nIn line with other OOD literature (Sun and Li 2022), we evaluate our methods both on the small-scale CIFAR benchmarks and the large-scale ImageNet benchmark1. Moreover, we provide a further evaluation of our proposal on the OpenOOD v1.5 benchmark (Zhang et al. 2023) in the appendix. We default to using the entire training set for estimating the parameter contribution distribution.\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">CIFAR-10</td><td rowspan=\"2\">CIFAR-100</td></tr><tr><td>FPR95AUROCFPR95AUROC</td><td></td></tr><tr><td>MSP</td><td>âˆš</td><td>ä¸ª</td><td>âˆš 80.13</td></tr><tr><td>Energy</td><td>48.73 26.55</td><td>92.46</td><td>74.36 81.19</td></tr><tr><td>ODIN</td><td>24.57</td><td>94.57</td><td>68.45 58.14 84.49</td></tr><tr><td>ReAct</td><td>26.45</td><td>93.71 94.67</td><td>62.27 84.47</td></tr><tr><td>DICE</td><td>20.83</td><td>95.24</td><td>49.72 87.23</td></tr><tr><td>OPNP</td><td>22.07</td><td>95.14</td><td>51.79 87.20</td></tr><tr><td>LAPS</td><td>19.40</td><td></td><td>50.50 88.07</td></tr><tr><td>ITP (Ours)</td><td>16.72</td><td>96.10</td><td>35.03 91.39</td></tr><tr><td>DICE+ReAct</td><td>16.48</td><td>96.64</td><td>49.57 85.07</td></tr><tr><td>OPNP+ReAct</td><td></td><td>96.64</td><td></td></tr><tr><td>LINe (w/ ReAct)</td><td>18.46</td><td>96.35 42.98</td><td>88.55</td></tr><tr><td></td><td>14.72</td><td>96.99</td><td>35.67 88.67</td></tr><tr><td>ITP + ReAct (Ours)</td><td>14.50</td><td>97.13</td><td>30.13 91.91</td></tr></table></body></html>\n\nTable 1: OOD detection performance on CIFAR benchmarks with DenseNet-101 as the backbone. All values in the table are averaged over six OOD test datasets and are percentages. The best results are in bold. $\\uparrow$ indicates that larger values are better, while $\\downarrow$ indicates that smaller values are better. Detailed results for each OOD dataset are provided in the Appendix.\n\nCIFAR. We use CIFAR-10 and CIFAR-100 (Krizhevsky 2009) as ID datasets and consider six OOD datasets: SVHN (Netzer et al. 2011), Textures (Cimpoi et al. 2014), iSUN (Xu et al. 2015), LSUN-Resize (Yu et al. 2015), LSUN-Crop (Yu et al. 2015), and Places365 (Zhou et al. 2018). For consistency with previous work (Sun and Li 2022), we use the same model architecture and pre-trained weights, namely DenseNet-101 (Huang et al. 2017).\n\nImageNet. For the large-scale ImageNet experiments, we use the ImageNet-1k as the ID dataset and consider (subsets of) iNaturalist (Horn et al. 2018), Places (Zhou et al. 2018), SUN (Xiao et al. 2010), and Textures (Cimpoi et al. 2014) with non-overlapping categories from ImageNet-1k as OOD datasets. We adopt the widely used ResNet-50 (He et al. 2016) model architectures, and we obtain the pre-trained weights from the torchvision library.\n\nBaselines. We compare ITP with the most competitive OOD detection methods: MSP (Hendrycks and Gimpel 2017), Energy (Liu et al. 2020), ODIN (Liang, Li, and Srikant 2018), ReAct (Sun, Guo, and Li 2021), DICE (Sun and Li 2022), LINe (Ahn, Park, and Kim 2023), OPNP (Chen et al. 2023), and LAPS (He et al. 2024). Moreover, to align with standard sparsification practices, we also report the results of a comparison with ReAct (e.g., $\\mathrm { I T P + R e A c t } ,$ ). In particular, LINe integrates ReAct within its framework, which we refer to as â€œLINE (w/ ReAct)â€ in the table. All methods are post-hoc and can be directly applied to pre-trained models.\n\nEvaluation Metric. We adopt two threshold-free metrics for evaluation. FPR95: the false positive rate of OOD data at $9 5 \\%$ true positive rate of ID data. AUROC: the area under the receiver operating characteristic curve.\n\nTable 2: OOD detection performance on ImageNet with ResNet-50 as the backbone.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"8\">OOD Datasets</td><td rowspan=\"2\">Average</td></tr><tr><td colspan=\"2\">iNaturalist</td><td colspan=\"2\">SUN</td><td colspan=\"2\">Places</td><td colspan=\"2\">Textures</td></tr><tr><td></td><td>FPR95</td><td>AUROC</td><td>FPR95</td><td>AUROC</td><td>FPR95AUROC</td><td></td><td>FPR95 AUROC</td><td>FPR95</td><td>AUROC</td></tr><tr><td></td><td>âˆš</td><td>ä¸ª</td><td>âˆš</td><td>ä¸ª</td><td>âˆš</td><td></td><td>âˆš ä¸ª</td><td>âˆš</td><td>ä¸ª</td></tr><tr><td>MSP</td><td>54.99</td><td>87.74</td><td>70.83</td><td>80.86</td><td>73.99</td><td>79.76 68.00</td><td>79.61</td><td>66.95</td><td>81.99</td></tr><tr><td>Energy</td><td>55.72</td><td>89.95</td><td>59.26</td><td>85.89</td><td>64.92</td><td>82.86 53.72</td><td>85.99</td><td>58.41</td><td>86.17</td></tr><tr><td>ODIN</td><td>47.66</td><td>89.66</td><td>60.15</td><td>84.59</td><td>67.89</td><td>81.78 50.23</td><td>85.62</td><td>56.48</td><td>85.41</td></tr><tr><td>ReAct</td><td>20.38</td><td>96.22</td><td>24.20</td><td>94.20</td><td>33.85</td><td>91.58</td><td>47.30 89.80</td><td>31.43</td><td>92.95</td></tr><tr><td>DICE</td><td>25.63</td><td>94.49</td><td>35.15</td><td>90.83</td><td>46.49</td><td>87.48</td><td>31.72 90.30</td><td>34.75</td><td>90.77</td></tr><tr><td>OPNP</td><td>18.89</td><td>96.03</td><td>18.50</td><td>95.62</td><td>30.14</td><td>93.46</td><td>36.17 91.70</td><td>25.93</td><td>94.20</td></tr><tr><td>LAPS</td><td>12.72</td><td>97.50</td><td>15.81</td><td>96.18</td><td>24.71</td><td>93.64</td><td>41.49 91.81</td><td>23.68</td><td>94.78</td></tr><tr><td>ITP (Ours)</td><td>11.53</td><td>97.83</td><td>25.82</td><td>93.58</td><td>35.63</td><td>90.75</td><td>17.06 96.03</td><td>22.51</td><td>94.55</td></tr><tr><td>DICE + ReAct</td><td>18.64</td><td>96.24</td><td>25.45</td><td>93.94 95.65</td><td>36.86 30.23</td><td>90.67 93.34</td><td>28.07 92.74</td><td>27.25</td><td>93.40</td></tr><tr><td>OPNP + ReAct</td><td>14.72</td><td>96.78</td><td>19.73</td><td></td><td>28.52</td><td>27.78</td><td>94.13 94.44</td><td>23.12</td><td>94.98</td></tr><tr><td>LINe (w/ ReAct)</td><td>12.26</td><td>97.56</td><td>19.48</td><td>95.26</td><td></td><td>92.85</td><td>22.54</td><td>20.70</td><td>95.03</td></tr><tr><td>ITP + ReAct (Ours)</td><td>9.78</td><td>98.02</td><td>22.82</td><td>94.47</td><td>30.87</td><td>92.03</td><td>18.09 95.98</td><td>20.39</td><td>95.13</td></tr></table></body></html>\n\nTable 3: Ablation study for our proposed method. All values are percentages and averaged over multiple OOD datasets.   \n\n<html><body><table><tr><td>Dataset</td><td>CRP</td><td>FTP</td><td>FPR95 âˆš</td><td>AUROC ä¸ª</td></tr><tr><td>CIFAR-10 DenseNet-101</td><td>Ã— âˆš Ã— âˆš</td><td>Ã— Ã— âˆš âˆš</td><td>26.55 21.29 19.16 16.96</td><td>94.57 95.09 96.36 96.59</td></tr><tr><td>CIFAR-100 DenseNet-101</td><td>Ã— âˆš Ã— âˆš</td><td>Ã— Ã— âˆš âˆš</td><td>68.45 53.60 53.73 35.03 58.41</td><td>81.19 85.33 87.56 91.39</td></tr><tr><td>ImageNet-1k ResNet-50</td><td>Ã— âˆš Ã— âˆš</td><td>Ã— Ã— âˆš âˆš</td><td>33.96 51.38 22.51</td><td>86.17 91.26 88.68 94.55</td></tr></table></body></html>\n\n# 4.2 Main Results\n\nIn this section, we report the performance of our ITP on commonly used CIFAR benchmarks and the more realistic and challenging ImageNet benchmark. Baseline results are sourced from (Ahn, Park, and $\\mathrm { K i m } \\ 2 0 2 3$ ; Chen et al. 2023; He et al. 2024), with additional baselines (e.g., LAPS, OPNP, and $\\mathrm { O P N P + R e A c t }$ on CIFAR) reproduced by us.\n\nFor the CIFAR benchmarks, Table 1 lays out the performance of OOD detection on the CIFAR-10 and CIFAR-100, respectively. As we can see, the proposed ITP outperforms all baselines considered and achieves state-of-the-art performance. In CIFAR-100, ITP reduces FPR95 by $3 3 . 4 2 \\%$ compared to the energy baseline, showing the effectiveness of our proposal with the same OOD scoring function. Remarkably, $\\mathrm { I T P + R e A c t }$ outperforms the most competitive method LINe by $5 . 5 4 \\%$ in FPR95 and $3 . 2 4 \\%$ in AUROC, highlighting the importance of further fine-grained pruning of overconfident parameters.\n\nTable 4: Impact of varying hyperparameters on FPR95. We use ImageNet-1k as the ID dataset and ResNet-50 as the pretrained model. All values are percentages and averaged over four OOD datasets.   \n\n<html><body><table><tr><td>p=10</td><td>p=30</td><td>p= 50 p= 70</td></tr><tr><td>å…¥=0.5 73.19</td><td>34.19 33.70</td><td>33.75 35.96</td></tr><tr><td>å…¥=1.0 33.18</td><td>25.44 26.83</td><td>27.26 30.96</td></tr><tr><td>å…¥=1.5 26.30</td><td>22.51 24.18</td><td>24.67 29.50</td></tr><tr><td>å…¥=2.0 27.58</td><td>24.69 25.95</td><td>26.45 31.59</td></tr><tr><td>å…¥=3.0 31.95</td><td>29.14 30.37</td><td>30.73 36.53</td></tr><tr><td>å…¥= 5.0 35.90</td><td>33.00 34.14</td><td>34.52 40.23</td></tr></table></body></html>\n\nFor the large-scale ImageNet benchmark, Table 2 reports detailed performances for each OOD dataset and the average over the four datasets. Our proposed method, ITP, outperforms recent approaches such as DICE, OPNP, and LAPS, achieving the best performance among the baseline methods with an FPR95 of $2 2 . 5 1 \\%$ . Moreover, the combination of ITP and ReAct outperforms recent approaches $\\mathrm { D I C E } +$ ReAct, $\\mathrm { O P N P + R e A c t }$ , and LINe. The experimental results demonstrate that our ITP is state-of-the-art and effective for OOD detection on large-scale real-world datasets.\n\n# 4.3 Ablation Study\n\nAblation on proposed pruning strategies. To fully demonstrate the impact of different granularity pruning strategies in ITP, we conduct a comprehensive empirical analysis on CIFAR-10, CIFAR-100, and ImageNet-1k, and report the results in Table 3. As shown in the table, both FTP and CRP improve performance, and ITP further markedly boosts OOD detection performance by integrating these two coarse-grained and fine-grained pruning strategies. However, the improvement of FTP on ResNet-50 is less pronounced, likely due to its larger feature space (2048 dimensions) compared to DenseNet-101 (342 dimensions). This larger space allows noise to dominate and interfere with FTP.\n\n![](images/49eb52e1563403c84461c75d32b8b94624004bf9629ccba70af6414c84399dd8.jpg)  \nFigure 4: The FFPR95 and AUROC with different number of training samples on ImageNet benchmark. The results are averaged over five independent runs.\n\nTable 5: Comparison of preprocessing overhead. We assess the preprocessing overhead by averaging the preprocessing times measured across three runs on ImageNet-1k. â€œITP (30)â€ denotes that only 30 images per class are used for ITP while maintaining the original performance (see Figure 4).   \n\n<html><body><table><tr><td>Method</td><td>Preprocessing Time (hours)</td><td>Additional Backpropagation</td><td>Batch Support</td></tr><tr><td>OPNP</td><td>8.7940</td><td></td><td>Ã—</td></tr><tr><td>LINe</td><td>7.3096</td><td>âˆš âˆš</td><td>Ã—</td></tr><tr><td>ITP</td><td>0.2411</td><td>Ã—</td><td>âˆš</td></tr><tr><td>ITP (30)</td><td>0.0073</td><td>Ã—</td><td>âˆš</td></tr></table></body></html>\n\nThe significant improvement observed when FTP is applied after noise removal with CRP supports this explanation. The ablation study verifies the effectiveness of the two strategies and demonstrates that they mutually enhance and complement each other.\n\nEffect of the Hyperparameter. Table 4 shows the results of varying the percentile $p$ used for pruning redundant parameters and the threshold $\\lambda$ for identifying overconfident parameters. The optimal performance is observed at $p = 3 0$ and $\\lambda = 1 . 5$ , achieving an FPR95 of $2 2 . 5 1 \\%$ . Conversely, we notice that selecting excessively large values for $p$ and overly small values for $\\lambda$ can lead to the erroneous removal of critical parameters, thereby impairing OOD detection.\n\nEffect of the Amount of Training Samples. In Figure 4, we show the effect of utilizing different numbers of training samples to estimate the parameter contribution distribution. Remarkably, even with just two samples per class, ITP can significantly improve OOD detection performance, resulting in a drastic $2 9 . 4 8 \\%$ reduction in FPR95 compared to the energy baseline (without pruning). Furthermore, empirical evidence suggests that using only 30 samples per class can yield performance nearly equivalent to that achieved with the full dataset. Therefore, to reduce computational overhead, it is feasible to use a suitably sized subset of the training set (e.g., 30 samples per class) for distribution estimation, while still achieving comparable performance.\n\n<html><body><table><tr><td>Method</td><td>FPR95 âˆš</td><td>AUROC ä¸ª</td></tr><tr><td>MSP</td><td>66.95</td><td>81.99</td></tr><tr><td>MSP+ITP</td><td>62.44</td><td>82.99</td></tr><tr><td>ODIN</td><td>56.48</td><td>85.41</td></tr><tr><td>ODIN+ITP</td><td>42.32</td><td>90.10</td></tr><tr><td>GradNorm</td><td>36.49</td><td>90.18</td></tr><tr><td>GradNorm+ITP</td><td>29.93</td><td>92.13</td></tr><tr><td>MLS</td><td>58.05</td><td>87.00</td></tr><tr><td>MLS+ITP</td><td>26.43</td><td>93.45</td></tr><tr><td>Energy</td><td>58.41</td><td>86.17</td></tr><tr><td>Energy+ITP</td><td>22.51</td><td>94.55</td></tr></table></body></html>\n\nTable 6: ITP on other OOD scores. We use ResNet-50 as the pre-trained model and ImageNet-1k as the ID dataset. The results are averaged over four OOD datasets.\n\n# 4.4 Further Analysis\n\nAnalysis of Preprocessing Overhead. Table 5 compares the preprocessing overhead of ITP with the most competitive weight sparsification methods: LINe and OPNP. In contrast to ITP, both LINe (Ahn, Park, and $\\mathrm { K i m } \\ 2 0 2 3$ ) and OPNP (Chen et al. 2023) require additional backpropagation to compute gradient information and lack support for batch processing. The results indicate that our proposal demonstrates a significant advantage in terms of preprocessing overhead. Notably, with only 30 samples per class, we can further substantially reduce the overhead while maintaining comparable performance (see Figure 4). Therefore, ITP is efficient and well-suited for real-world applications.\n\nCompatibility with Other OOD Scores. Table 6 presents the OOD detection performance of ITP using various OOD scoring methods, including MSP (Hendrycks and Gimpel 2017), ODIN (Liang, Li, and Srikant 2018), GradNorm (Huang, Geng, and Li 2021), MLS (Hendrycks et al. 2022), and Energy (Liu et al. 2020). Our ITP consistently improves FPR95 and AUROC across different OOD scores. In particular, ITP can effectively complement gradient-based methods such as GradNorm. These results indicate that the parameters our ITP selectively used are also applicable to other OOD scores and show strong compatibility.\n\n# 5 Conclusion\n\nIn this paper, we reveal that parameters important for ID data prediction are not always beneficial for OOD detection. To address this issue, we propose a parameter pruning method called ITP, which utilizes class-specific parameter contribution distributions for post-hoc OOD detection. ITP is based on two powerful pruning strategies: CRP performs coarsegrained pruning to remove redundant parameters, while FTP executes fine-grained pruning to eliminate overconfident parameters. Experimental results show that our ITP method significantly improves OOD detection performance and can be integrated with a wide range of other OOD scoring methods. We hope our work can raise more attention to the importance of test parameter pruning for OOD detection.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å¯¹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®çš„æ£€æµ‹ä¸å¯é é—®é¢˜ã€‚ç°æœ‰åŸºäºæƒé‡ç¨€ç–åŒ–çš„æ–¹æ³•è™½ç„¶èƒ½å‡å°‘å†—ä½™å‚æ•°å¯¹OODæ£€æµ‹çš„è´Ÿé¢å½±å“ï¼Œä½†è¿™äº›å‚æ•°åœ¨å¤„ç†OODæ•°æ®æ—¶å¯èƒ½è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ï¼ˆoverconfidenceï¼‰ï¼Œä»è€ŒæŸå®³æ£€æµ‹æ€§èƒ½ã€‚\\n> *   è¯¥é—®é¢˜çš„é‡è¦æ€§åœ¨äºï¼ŒOODæ£€æµ‹çš„ä¸å¯é æ€§å¯èƒ½å¯¼è‡´AIç³»ç»Ÿåœ¨å®‰å…¨å…³é”®é¢†åŸŸï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ï¼‰äº§ç”Ÿç¾éš¾æ€§åæœã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œå®ä¾‹æ„ŸçŸ¥æµ‹è¯•å‰ªæâ€ï¼ˆInstance-aware Test Pruning, ITPï¼‰çš„åå¤„ç†æ–¹æ³•ï¼Œé€šè¿‡ç²—ç²’åº¦å’Œç»†ç²’åº¦çš„å‚æ•°å‰ªæç­–ç•¥ï¼Œæå‡OODæ£€æµ‹çš„å¯é æ€§ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹1ï¼š** æå‡ºç²—ç²’åº¦å†—ä½™å‰ªæï¼ˆCRPï¼‰ï¼Œå»é™¤å¯¹IDæ•°æ®è´¡çŒ®ä½çš„å†—ä½™å‚æ•°ï¼Œå‡å°‘å™ªå£°å¹²æ‰°ã€‚åœ¨CIFAR-10ä¸Šï¼ŒFPR95ä»26.55%é™è‡³16.72%ï¼ŒAUROCä»94.57%æå‡è‡³96.10%ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹2ï¼š** æå‡ºç»†ç²’åº¦æµ‹è¯•å‰ªæï¼ˆFTPï¼‰ï¼ŒåŸºäºå³å°¾Zæ£€éªŒè‡ªé€‚åº”å»é™¤å®ä¾‹çº§è¿‡åº¦è‡ªä¿¡å‚æ•°ã€‚åœ¨ImageNet-1kä¸Šï¼ŒFPR95ä»58.41%é™è‡³22.51%ï¼ŒAUROCä»86.17%æå‡è‡³94.55%ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹3ï¼š** ITPä¸æ¿€æ´»åŸºæ–¹æ³•ï¼ˆå¦‚ReActï¼‰æ­£äº¤ï¼Œå¯è¿›ä¸€æ­¥é›†æˆæå‡æ€§èƒ½ã€‚ITP+ReActåœ¨CIFAR-100ä¸ŠFPR95é™è‡³30.13%ï¼ŒAUROCæå‡è‡³91.91%ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   ITPçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å‚æ•°å‰ªæç­–ç•¥å‡å°‘OODæ£€æµ‹ä¸­çš„å™ªå£°å¹²æ‰°å’Œè¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚å…¶è®¾è®¡å“²å­¦æ˜¯åŸºäºå‚æ•°è´¡çŒ®åˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹æ€§ï¼Œä»ç²—ç²’åº¦å’Œç»†ç²’åº¦ä¸¤ä¸ªå±‚é¢ä¼˜åŒ–æ¨¡å‹è¡Œä¸ºã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ç°æœ‰ç¨€ç–åŒ–æ–¹æ³•ï¼ˆå¦‚DICEã€LINeï¼‰ä»…å…³æ³¨IDæ•°æ®çš„é‡è¦å‚æ•°ï¼Œä½†æœªè§£å†³è¿™äº›å‚æ•°åœ¨OODæ•°æ®ä¸Šçš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** ITPé€šè¿‡CRPå»é™¤å†—ä½™å‚æ•°ï¼Œå†é€šè¿‡FTPé’ˆå¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬åŠ¨æ€å‰ªé™¤è¿‡åº¦è‡ªä¿¡å‚æ•°ï¼Œä»è€Œæ›´å¯é åœ°åŒºåˆ†IDå’ŒOODæ•°æ®ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **å‚æ•°è´¡çŒ®åˆ†å¸ƒä¼°è®¡ï¼š** ä½¿ç”¨IDæ•°æ®ä¼°è®¡æ¯ä¸ªå‚æ•°å¯¹ç‰¹å®šç±»åˆ«çš„è´¡çŒ®åˆ†å¸ƒï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰ã€‚å…¬å¼ï¼š\\n>     $$\\n>     \\\\mu_{ij} = \\\\frac{1}{|\\\\mathcal{D}_j|} \\\\sum_{x \\\\in \\\\mathcal{D}_j} c_j(x; \\\\mathbf{W}_{ij})\\n>     $$\\n> 2.  **ç²—ç²’åº¦å†—ä½™å‰ªæï¼ˆCRPï¼‰ï¼š** ç§»é™¤è´¡çŒ®å‡å€¼ä½äºpç™¾åˆ†ä½çš„å‚æ•°ã€‚æ©ç çŸ©é˜µå…¬å¼ï¼š\\n>     $$\\n>     \\\\mathbf{M}_{ij}^{\\\\mathrm{CRP}} = \\\\begin{cases} 1, & \\\\text{if } \\\\mu_{ij} > \\\\Omega_p \\\\\\\\ 0, & \\\\text{otherwise} \\\\end{cases}\\n>     $$\\n> 3.  **ç»†ç²’åº¦æµ‹è¯•å‰ªæï¼ˆFTPï¼‰ï¼š** å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œé€šè¿‡å³å°¾Z-scoreæµ‹è¯•ï¼ˆé˜ˆå€¼Î»ï¼‰ç§»é™¤è¿‡åº¦è‡ªä¿¡å‚æ•°ã€‚æ©ç çŸ©é˜µå…¬å¼ï¼š\\n>     $$\\n>     \\\\mathbf{M}_{ij}^{\\\\mathrm{FTP}}(\\\\mathbf{x}) = \\\\begin{cases} 1, & \\\\text{if } \\\\frac{c_j(\\\\mathbf{x}; \\\\mathbf{W}_{ij}) - \\\\mu_{ij}}{\\\\sigma_{ij}} \\\\leq \\\\lambda \\\\\\\\ 0, & \\\\text{otherwise} \\\\end{cases}\\n>     $$\\n> 4.  **OODåˆ†æ•°è®¡ç®—ï¼š** ä½¿ç”¨å‰ªæåçš„æ¨¡å‹è¾“å‡ºè®¡ç®—èƒ½é‡åˆ†æ•°ï¼š\\n>     $$\\n>     S(\\\\mathbf{x}) = -E(\\\\mathbf{x}) = \\\\log \\\\sum_{k=1}^K \\\\exp(f_k^{\\\\mathrm{ITP}}(\\\\mathbf{x}))\\n>     $$\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   åŸºçº¿æ¨¡å‹åŒ…æ‹¬MSPã€Energyã€ODINã€ReActã€DICEã€LINeã€OPNPå’ŒLAPSç­‰OODæ£€æµ‹æ–¹æ³•ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨FPR95ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨CIFAR-10ä¸Šè¾¾åˆ°äº†16.72%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹Energyï¼ˆ26.55%ï¼‰å’ŒDICEï¼ˆ20.83%ï¼‰ã€‚ä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿LAPSï¼ˆ19.40%ï¼‰ç›¸æ¯”ï¼Œæå‡äº†2.68ä¸ªç™¾åˆ†ç‚¹ã€‚\\n> *   **åœ¨AUROCä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨CIFAR-100ä¸Šè¾¾åˆ°äº†91.39%ï¼Œè¿œé«˜äºåŸºçº¿æ¨¡å‹Energyï¼ˆ81.19%ï¼‰å’ŒDICEï¼ˆ87.23%ï¼‰ï¼Œä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿LINeï¼ˆ88.55%ï¼‰ç›¸æ¯”ï¼Œæå‡äº†2.84ä¸ªç™¾åˆ†ç‚¹ã€‚\\n> *   **åœ¨ImageNet-1kä¸Šï¼š** æœ¬æ–‡æ–¹æ³•çš„å¹³å‡FPR95ä¸º22.51%ï¼Œä¼˜äºåŸºçº¿æ¨¡å‹DICEï¼ˆ34.75%ï¼‰å’ŒOPNPï¼ˆ25.93%ï¼‰ï¼Œä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿LAPSï¼ˆ23.68%ï¼‰ç›¸å½“ï¼Œä½†åœ¨AUROCä¸Šï¼ˆ94.55%ï¼‰ç•¥ä¼˜äºåè€…ï¼ˆ94.78%ï¼‰ã€‚\\n> *   **åœ¨é¢„å¤„ç†å¼€é”€ä¸Šï¼š** ITPçš„é¢„å¤„ç†æ—¶é—´ä¸º0.2411å°æ—¶ï¼Œè¿œä½äºLINeï¼ˆ7.3096å°æ—¶ï¼‰å’ŒOPNPï¼ˆ8.7940å°æ—¶ï¼‰ï¼ŒåŒæ—¶æ”¯æŒæ‰¹é‡å¤„ç†ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   åˆ†å¸ƒå¤–æ£€æµ‹ (Out-of-Distribution Detection, OOD)\\n*   å®ä¾‹æ„ŸçŸ¥æµ‹è¯•å‰ªæ (Instance-aware Test Pruning, ITP)\\n*   å‚æ•°è´¡çŒ®åˆ†å¸ƒ (Parameter Contribution Distribution, N/A)\\n*   ç²—ç²’åº¦å†—ä½™å‰ªæ (Coarse-grained Redundancy Pruning, CRP)\\n*   ç»†ç²’åº¦æµ‹è¯•å‰ªæ (Fine-grained Test Pruning, FTP)\\n*   æ·±åº¦ç¥ç»ç½‘ç»œ (Deep Neural Network, DNN)\\n*   åå¤„ç†æ–¹æ³• (Post-hoc Method, N/A)\\n*   å®‰å…¨å…³é”®åº”ç”¨ (Safety-critical Applications, N/A)\"\n}\n```"
}