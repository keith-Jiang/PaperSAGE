{
    "source": "Semantic Scholar",
    "arxiv_id": "2504.06544",
    "link": "https://arxiv.org/abs/2504.06544",
    "pdf_link": "https://arxiv.org/pdf/2504.06544.pdf",
    "title": "LCGC: Learning from Consistency Gradient Conflicting for Class-Imbalanced Semi-Supervised Debiasing",
    "authors": [
        "Weiwei Xing",
        "Yue Cheng",
        "Hongzhu Yi",
        "Xiaohui Gao",
        "Xiang Wei",
        "Xiaoyu Guo",
        "Yuming Zhang",
        "Xinyu Pang"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2025-04-09",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "Beijing Jiaotong University",
        "Northwestern Polytechnical University",
        "Newcastle University",
        "Chongqing University of Posts and Telecommunications"
    ],
    "paper_content": "# LCGC: Learning from Consistency Gradient Conflicting for Class-Imbalanced Semi-Supervised Debiasing\n\nWeiwei Xing1, Yue Cheng 1, Hongzhu Yi 1, Xiaohui Gao2, Xiang Wei 1 \\*, Xiaoyu Guo 1, Yuming Zhang 3, Xinyu Pang\n\n1School of Software Engineering, Beijing Jiaotong University 2School of Automation, Northwestern Polytechnical University 3School of Computing, Newcastle University 4School of Science, Chongqing University of Posts and Telecommunications wwxing, yuecheng, hongzhuyi, xiangwei, guoxiaoyu @bjtu.edu.cn, gaitxh@foxmail.com y.zhang361 $@$ newcastle.ac.uk, candypxy $@$ 163.com\n\n# Abstract\n\nClassifiers often learn to be biased corresponding to the classimbalanced dataset, especially under the semi-supervised learning (SSL) set. While previous work tries to appropriately re-balance the classifiers by subtracting a class-irrelevant image’s logit, but lacks a firm theoretical basis. We theoretically analyze why exploiting a baseline image can refine pseudo-labels and prove that the black image is the best choice. We also indicated that as the training process deepens, the pseudo-labels before and after refinement become closer. Based on this observation, we propose a debiasing scheme dubbed LCGC, which Learning from Consistency Gradient Conflicting, by encouraging biased class predictions during training. We intentionally update the pseudo-labels whose gradient conflicts with the debiased logits, representing the optimization direction offered by the over-imbalanced classifier predictions. Then, we debiased the predictions by subtracting the baseline image logits during testing. Extensive experiments demonstrate that LCGC can significantly improve the prediction accuracy of existing CISSL models on public benchmarks.\n\n# Introduction\n\nThe predictions of an ideal unbiased classifier are attributed to class-relevant features of the input images. In real cases, however, such an ideal classifier can hardly be obtained due to the dataset selection biases and imbalances, the complexity of the real world, or other peculiarities (Bengio et al. 2020). That’s why classifiers trained on a class-imbalanced set may fail to make trustworthy predictions, which is even more pronounced in semi-supervised learning (SSL) (Chen et al. 2023).\n\nEssentially, when there is training with imbalanced data, the accuracy of the model drops mainly caused by the spurious correlation between the irrelevant features and category labels, i.e. the classifier output logits are biased attributes towards the majority class of class-dependent features. Taking the task of recognizing animal categories as an example, as shown in Figure 1, the sensitive map obtained after vanilla FixMatch gives certain attention to class-irrelevant background features, while the debias model directs its attention more towards class-relevant features. This results in the latter having less background noise.\n\n![](images/7dc7294feb723b19fb37a18285dec43009b8ee58a35074c76dbf80b42ed6cd15.jpg)  \nFigure 1: Visualization of sensitive maps produced by the FixMatch model and CDMAD at the image. Left-to-right: original input image, visualization of sensitive maps produced by the FixMatch, CDMAD. The lightness of the sensitive map indicates how much attention the models pay to a particular area of the input image. The sensitive maps obtained by CDMAD have less noise.\n\nTo address this issue, a few algorithms (Oh, Kim, and Kweon 2022; Wang et al. 2022; Schmutz, HUMBERT, and Mattei 2023; Lee and $\\mathrm { K i m } \\ 2 0 2 4 )$ have been proposed for classifier debiasing under class-imbalanced SSL (CISSL) settings. The basic idea of classifier debiasing is to regularize the response bias generated by the biased classification head to achieve the balance effect (Wei and Gan 2023; Wang et al. 2023a; Zhu et al. 2024). Such a training strategy alleviates the bias caused by the imbalanced dataset to a certain extent. However, they need to design complex network variants, such as adding classification heads (Lee, Shin, and Kim 2021), designing causal balancers (Wang et al. 2022; Li et al. 2024; Tao et al. 2024), etc. An immediate idea is whether there is a way to measure the bias of the classifier in a minimally expensive way and to perform post-hoc logit adjustment on the response bias generated by the biased classification head. Class-Distribution-Mismatch-Aware Debiasing (CDMAD) does this job well with a solid color image (Lee and Kim 2024). However, what we care about is there any theoretical evidence to support that a solid color image can debias the classifier logits during training?\n\nIn this paper, our theoretical analysis shows that the CDMAD enhances the balance of the base SSL model implicitly utilizing the integrated gradient flow. It is then natural to ask: are there any further debiasing methods that can improve the performance of the classifier? If so, we still expect the new proxy method to be amenable to providing insights for classifier debias and a wider spectrum of architectures. To this end, we propose a novel debiasing model LCGC, by Learning from Consistency Gradient Conflicting, to overcome the improper training bias from class-imbalanced datasets. In particular, we first update the pseudo-labels whose gradient conflicts with the baseline image refined logits, which is represented as the optimization direction offered by the over-imbalanced classifier predictions. Here, we employ Kullback-Leibler (KL) divergence to measure the consistency between the refinement and original pseudo-labels logits. Encouraging the conflict between them, we train an over-biased classifier neural network. Then, we expected to debias the predictions by subtraction the baseline image logits during testing.\n\nOur work makes three major contributions. 1) We theoretically analyze that a baseline image enhances the base SSL model by implicitly utilizing the integrated gradient flow. 2) We propose a simple debiased scheme by learning consistency gradient conflicting. 3) We validated the effectiveness of LCGC on four benchmark datasets in both scenarios where the class distributions of the labeled and unlabeled sets either match or mismatch.\n\n# Related Work\n\n# Classifier Debias\n\nFor the classifier debiasing problem, the mainstream methods involve adjustments through re-weighting (Zhang et al. 2021) and re-sampling (Lee, Shin, and Kim 2021), as well as the lately prevalent logits adjustment (Wang et al. 2023b) technique. However, this approach might lead to difficulties and instability in optimization (Cao et al. 2019). To tackle this issue, LfF (Nam et al. 2020) training a pair of neural networks simultaneously that can effectively mitigate the bias in the network. DebiasPL (Wang et al. 2022) designs an adaptive debiasing module from a causal perspective. UniSSL (Huang et al. 2021) proposes class-sharing data detection and feature adaptation methods. DST (Chen et al. 2022) adopts a self-training debiased framework. CDMAD (Lee and $\\mathrm { K i m } 2 0 2 4 )$ uses solid color images to correct classifier bias during training and inference. Based on this finding, we design the LCGC to train consistent gradients conflicting guided bias model from class-imbalanced data utilizing FixMatch (Sohn et al. 2020) and ReMixMatch (Berthelot et al. 2020).\n\n# Class-Imbalanced Semi-Supervised Learning\n\nThis part of the research primarily leverages a fusion of CIL and SSL techniques, which allows for the full utilization of unlabeled data during training, while also providing them with balanced pseudo-labels. By assuming that the class distribution of the unlabeled set is known and the same as the labeled dataset. CReST (Wei et al. 2021) uses unlabeled samples predicted as the minority classes more frequently than those predicted as the majority classes for interactive self-training. CoSSL (Cai, Wang, and Hwang 2021) and BaCon (Feng et al. 2024) use an auxiliary classifier and train the classifier to be balanced. SMCLP (Du et al. 2024) utilizes a collaborative manner to exploit the correlations from labels and instances to overcome the imbalanced problem. Conversely, SAW (Lai et al. 2022) mitigates class imbalance using smoothed reweighting based on the number of pseudo-labels belonging to each class without prior knowledge of the class distribution of the unlabeled set.\n\n# Problem Setup and Preliminaries\n\n# Problem Setup\n\nThe problem of class-imbalanced semi-supervised learning aims to train a classifier involving labeled set $\\scriptstyle { \\mathcal { X } } \\ =$ $\\{ ( { \\bar { x } } ^ { n } , y ^ { n } ) \\ : \\ n \\ \\in \\ ( 1 , \\cdot \\cdot \\ , N ) \\}$ and unlabeled set $u \\ =$ $\\{ ( u ^ { m } ) : m \\in ( 1 , \\cdot \\cdot \\cdot , M ) \\}$ , where $x ^ { n } \\in \\mathbb { R } ^ { d }$ and $y ^ { n } \\in$ $[ C ] = \\{ 1 , \\cdots , C \\}$ denote the $n$ -th labeled sample and corresponding label, respectively, and $u ^ { m } \\in \\mathbb { R } ^ { d }$ denotes the $m$ -th unlabeled sample. Here, the number of labeled and unlabeled samples of class $c$ as $N _ { c }$ and $M _ { c }$ i.e., $\\textstyle \\sum _ { c = 1 } ^ { C } N _ { c } = N$ and $\\textstyle \\sum _ { c = 1 } ^ { C } M _ { c } \\ = \\ M$ , where $M _ { c }$ is challenging to know in a realistic scenario. Formally, the ratio of the class imbalance of labeled and unlabeled sets can be represented as $\\begin{array} { r } { \\gamma _ { l } ~ = ~ \\frac { N _ { 1 } } { N _ { c } } ~ \\ge ~ 1 } \\end{array}$ and $\\begin{array} { r } { \\gamma _ { u } ~ = ~ \\frac { M _ { 1 } } { M _ { c } } ~ \\ge ~ 1 } \\end{array}$ under the classimbalanced training set, where $\\left. N _ { 1 } \\right.$ and $M _ { 1 }$ are the number of samples in the largest labeled and unlabeled class. For each iteration of training, we sample minibatches $\\mathcal { M } \\mathcal { X } =$ $\\{ ( x _ { b } ^ { m } , y _ { b } ^ { m } ) : b \\in ( 1 , \\cdot \\cdot \\cdot , B ) \\} \\subset \\dot { \\mathcal { X } }$ and $\\mathcal { M } = \\{ ( \\boldsymbol { u } _ { b } ^ { m } ) : b \\in$ $( 1 , \\cdots , \\mu B ) \\} \\subset \\mathcal { U }$ from the training set, where $B$ denotes the minibatch size and $\\mu$ denotes the relative size of $\\mathcal { M } \\mathcal { U }$ to $\\mathcal { M } \\mathcal { X }$ . The goal of a CISSL model is to learn a classifier $f _ { \\theta } : \\mathbb { R } ^ { d }  \\{ 1 , \\cdot \\cdot \\cdot , C \\}$ that effectively classifies samples in a test set X test = {(xtkest, ytkest)}, where θ denotes parameters of base SSL algorithm. The output logits of $f _ { \\theta }$ on an input as $g _ { \\theta } ( \\cdot ) \\in \\mathbb { R } ^ { C }$ , i.e., $f _ { \\theta } ( \\cdot ) = \\arg \\operatorname* { m a x } _ { c } g _ { \\theta } ( \\cdot ) _ { c }$ , where $( \\cdot ) _ { c }$ denotes the $c$ -th element.\n\n# Backbone SSL Algorithm\n\nIn this paper, we consider exploiting FixMatch (Sohn et al. 2020) or ReMixMatch (Berthelot et al. 2020) as the backbone for the base CISSL problem. FixMatch first predicts the class probability of weakly augmented unlabeled data $\\alpha ( u _ { b } ^ { m } )$ as $q _ { b } ~ = ~ \\mathrm { S o f t m a x } ( g _ { \\theta } ( \\alpha ( u _ { b } ^ { m } ) )$ and then generates hard pseudo-label $\\tilde { q } _ { b } ~ = ~ \\arg \\operatorname* { m a x } _ { c } q _ { b , c }$ . Then the consistency regularization is calculated from $\\tilde { q } _ { b }$ and its strongly augmented $\\mathcal { A } ( u _ { b } ^ { m } )$ version. ReMixMatch similarly produces $q _ { b }$ but additionally adopts distribution alignment and sharpening strategies, which also conducts MixUp regularization and is self-supervised by rotating unlabeled samples (Gidaris, Singh, and Komodakis 2018). The training losses of FixMatch $\\mathcal { L } _ { F }$ and ReMixMatch $\\mathcal { L } _ { R }$ on $\\mathcal { M } \\mathcal { X }$ and $\\mathcal { M } \\mathcal { U }$ is given by:\n\n$$\n\\begin{array} { c } { { \\mathcal { L } _ { F } ( \\mathcal { M } \\mathcal { X } , \\mathcal { M } \\mathcal { U } , \\tilde { q } , \\tau ; \\theta ) } } \\\\ { { \\mathcal { L } _ { R } ( \\mathcal { M } \\mathcal { X } , \\mathcal { M } \\mathcal { U } , \\bar { q } ; \\theta ) } } \\end{array}\n$$\n\nwhere $\\tau$ denotes a predefined confidence threshold to improve the quality of the pseudo-labels. $\\bar { q }$ denotes the sharpened pseudo-label after distribution alignments. Note that, in the absence of specifying FixMatch or ReMixMatch as the backbone for CISSL, we slightly abuse notation by uniformly using $\\mathcal { L }$ to denote the training loss.\n\n# Class-Distribution-Mismatch-Aware Debiasing\n\nWe primarily explore the reason why class-distributionmismatch-aware debiasing (CDMAD) is effective (Lee and $\\ K i m \\ 2 0 2 4 ,$ ). CDMAD first calculates the logits on a weakly augmented unlabeled sample $g _ { \\theta } \\big ( \\alpha ( u _ { b } ^ { m } ) \\big )$ , and the logits on a baseline (solid color image) $g _ { \\boldsymbol { \\theta } } ( \\mathcal { T } )$ . Then CDMAD adjusts for the classifier’s biased degree by simple subtraction of both the training and testing phase.\n\nwhere xlogits is test samples.\n\n# Integrated Gradients\n\nFocus on the image classification problem and assume the classifier network $\\mathsf { f } _ { \\theta } : \\mathbb { R } ^ { d } \\to \\{ 1 , \\bar { \\cdot } \\cdot \\cdot , C \\}$ is differentiable almost everywhere and perfectly fits the dataset, i.e., there exist interpolating parameters $\\theta ^ { * }$ such that $\\forall x _ { n } \\ \\in \\ X \\ : <$ $x _ { n } , \\theta ^ { * } > = y _ { n }$ . Let $\\bar { \\boldsymbol { x } } _ { n } \\in \\mathbb { R } ^ { d }$ be the input, and $\\mathcal { T } \\in \\mathbb { R } ^ { d }$ be the baseline image, then the integrated gradients are obtained by accumulating the straightline path from the $\\boldsymbol { \\mathcal { T } }$ to the $x _ { n }$ (Sundararajan, Taly, and Yan 2017).\n\n$$\n\\sum _ { i = 1 } ^ { d } { \\mathrm { I n t e r g r a t e d G r a d s } } _ { i } ( x _ { n } ) = f _ { \\theta } ( x _ { n } ) - f _ { \\theta } ( { \\mathcal { T } } )\n$$\n\n# Why and How a Baseline Image Can Debias the Classifier Logits\n\nIn this section, we characterize the class bias of pseudo-label $q _ { b }$ , which is generated by FixMatch or ReMixMatch from $g _ { \\theta }$ . Note that the base SSL model parameters are updated according to the gradient flow:\n\n$$\n\\frac { d \\theta } { d t } = - \\nabla _ { \\theta } \\mathcal { L } ( \\mathcal { M } \\mathcal { X } , \\mathcal { M } \\mathcal { U } , q ; \\theta )\n$$\n\nPrevious work (Lee and $\\mathrm { K i m } 2 0 2 4 )$ ) showed that the classifier’s biased degree can be adjusted by a baseline image $\\boldsymbol { \\mathcal { T } }$ . We examine why and how a baseline image can debias the classifier logits? We revisit an empirical phenomenon observed in CDMAD Section 4.4: After exploring a series of measures of baseline image debiasing, they find that the base CISSL framework can also be greatly improved by refinement of biased class predictions only during testing. As\n\nFixMatch $^ +$ CDMAD ReMixMatch $^ +$ CDMAD 0.75 0.10 Y/=100,Yu=100 Y/=100,Yu=100 Y/= 100,Yu = 150 0.70 Y= 100,Yu=150 0.65 0.07 0.60 0.06 0.55 GO 0.50 0.05 W 0.45 0.03 0.40 100 200 300 400 500 100 150 200 250 300 Epoch Epoch\n\nsevere class distribution mismatch between labeled and unlabeled sets, the resulting classifier’s different biased degree toward each class. That is the logit distribution $P ( g _ { \\boldsymbol { \\theta } } ( \\mathcal { T } ) )$ of a baseline image $\\boldsymbol { \\mathcal { T } }$ through CISSL is imbalanced. We formulate the phenomena in the following assumption.\n\nAssumption 1. The SSL classifiers trained on the classimbalanced datasets produced biased results:\n\n$$\nP ( y _ { n } = c | x _ { n } ) \\propto \\exp { ( g _ { \\theta , c } ( x _ { n } ) ) } )\n$$\n\nwhere $P ( y _ { n } = c | x _ { n } ) \\neq 1 / C$ for any $y _ { n } \\in [ C ]$ .\n\nFor convenience, we define\n\n$$\ng _ { \\theta } ^ { \\prime } ( x _ { n } ) = g _ { \\theta } ( x _ { n } ) - g _ { \\theta } ( \\mathcal { T } )\n$$\n\nwhere $g _ { \\boldsymbol { \\theta } } ( x _ { n } )$ is the logit of $x _ { n }$ .\n\nWe then derive the conditional probability $\\begin{array} { r l } { P _ { b } } & { { } = } \\end{array}$ $P ( \\boldsymbol { y } | \\boldsymbol { g } _ { \\boldsymbol { \\theta } } ( \\boldsymbol { x } _ { n } ) )$ and $P _ { a } = P ( y _ { n } | g _ { \\theta } ^ { \\prime } ( x _ { n } ) )$ w.r.t. the outputs of $g _ { \\boldsymbol { \\theta } } ( x _ { n } )$ and $g _ { \\theta } ^ { \\prime } ( x _ { n } )$ :\n\nLemma 1. For a biased classifier trained on the classimbalanced datasets, the basic CISSL model’s logit $g _ { \\boldsymbol { \\theta } } ( x _ { n } )$ and its refinement $g _ { \\theta } ^ { \\prime } ( x _ { n } )$ have diverse predictions:\n\n$$\n( g _ { \\boldsymbol \\theta } ( x _ { n } ) \\perp g _ { \\boldsymbol \\theta } ^ { \\prime } ( x _ { n } ) ) | y _ { n }\n$$\n\nWe now state the main theorem regarding the class debias of the black baseline image below:\n\nTheorem 1 (Integrated gradient flow for class debiasing). For a CISSL classifier network $f _ { \\theta }$ , if a baseline image (the baseline is preferably a black image) is exploited to refine the logit, i.e. $g _ { \\theta } ^ { \\prime } ( \\alpha ( u _ { b } ^ { m } ) ) = g _ { \\theta } ( \\alpha ( u _ { b } ^ { m } ) ) - g _ { \\theta } ( \\mathcal { T } )$ during the training phase, the gradient flow $- \\nabla _ { \\boldsymbol { \\theta } } \\mathcal { L } ( \\mathcal { M } \\mathcal { X } , \\mathcal { M } \\mathcal { U } , \\boldsymbol { q } ; \\boldsymbol { \\theta } )$ of $f _ { \\theta }$ contains a linear integrated gradients term $\\mathbb { I } ( \\boldsymbol { \\theta } )$ :\n\n$$\n\\begin{array} { r l r } {  { \\nabla _ { \\theta } \\mathcal { L } = \\mathbb { C } ( \\theta ) + \\mathbb { I } ( \\theta ) } } \\\\ & { } & { \\mathbb { I } ( \\theta ) = - \\sum _ { b } ( \\sum _ { i = 1 } ^ { d } \\operatorname { I n t e r g r a t e d G r a d s } _ { i } ( u _ { b } ^ { m } ) ) \\nabla g _ { b } } \\end{array}\n$$\n\nwhere $\\mathbb { C } ( \\boldsymbol { \\theta } )$ is the term in the gradient flow other than the $\\mathbb { I } ( \\boldsymbol { \\theta } )$ component. To derive the results, we first define some notations. Let $\\mathcal { L } ( \\mathcal { M X } , \\mathcal { M U } , q ; \\theta ) ~ = ~ \\mathcal { L } _ { C o n } ( \\mathcal { M U } , \\tilde { q } ; \\theta ) ~ + ~$ $\\mathcal { L } _ { S u p } ( \\mathcal { M } \\mathcal { X } ; \\theta )$ denote the loss function of backbone SSL algorithm on a minibatch for labeled set $\\mathcal { M } \\mathcal { X }$ and a minibatch for unlabeled set $\\mathcal { M } \\mathcal { U }$ . Therefore, we have\n\n![](images/cc5a4ae5aabf1ae250511fa777e7d8c6a427313baf6ab3c4a0d8d72ac9ca87c9.jpg)  \nFigure 3: Pseudo-label refinement process using LCGC.\n\n$$\n\\nabla _ { \\theta } \\mathcal { L } = \\nabla _ { \\theta } \\mathcal { L } _ { C o n } + \\nabla _ { \\theta } \\mathcal { L } _ { S u p }\n$$\n\nSince the adjust of CDMAD only acts on the $\\mathcal { L } _ { C o n }$ term in $\\mathcal { L }$ , we only need to study $\\nabla _ { \\boldsymbol { \\theta } } \\mathcal { L } _ { C o n }$ . Consequently, we have $\\begin{array} { r } { \\mathcal { L } _ { C o n } ~ = ~ \\frac { 1 } { \\mu B } \\sum _ { u _ { b } ^ { m } \\in \\mathcal { M } \\mathcal { U } } \\mathbf { \\bar { H } } ( P _ { \\theta } ( y | \\mathcal { A } ( u _ { b } ^ { m } ) ) , \\tilde { q } _ { b } ) . } \\end{array}$ , where $\\mathbf { H }$ is cross-entropy loss. It can be derived\n\n$$\n\\begin{array} { r l } { \\nabla _ { \\theta } \\mathcal { L } _ { C o n } = \\displaystyle \\sum _ { b } \\frac { \\partial \\ H } { \\partial q _ { A b } } \\frac { \\partial q _ { A b } } { \\partial \\theta } } & { } \\\\ { = \\displaystyle \\sum _ { b } q _ { A b } \\frac { \\partial q _ { A b } } { \\partial \\theta } - \\sum _ { b } \\bar { q } _ { b } \\frac { \\partial q _ { A b } } { \\partial \\theta } } & { } \\\\ { = \\displaystyle \\sum _ { b } q _ { A b } \\frac { \\partial q _ { A b } } { \\partial \\theta } - \\sum _ { b } ( q _ { b } - q _ { Z } ) \\frac { \\partial q _ { A b } } { \\partial \\theta } } & { } \\\\ { = \\displaystyle \\sum _ { b } q _ { A b } \\frac { \\partial q _ { A b } } { \\partial \\theta } - \\sum _ { b } ( g ( u _ { b } ^ { m } ) _ { b } - g ( T ) _ { b } ) \\nabla g _ { b } } & { } \\\\ { = - \\displaystyle \\sum _ { b } q _ { A b } \\frac { \\partial } { \\partial \\theta } \\frac { \\partial q _ { A b } } { \\partial \\theta } - \\sum _ { b } ( g ( u _ { b } ^ { m } ) _ { b } - g ( T ) _ { b } ) \\nabla g _ { b } } & { } \\\\ { = - \\displaystyle \\sum _ { b } ( \\displaystyle \\sum _ { i = 1 } ^ { d } \\mathrm { l a t e g a t d G r a d a t } _ { ( } u _ { b } ^ { m } ) ) \\nabla g _ { b } + \\displaystyle \\sum _ { b } q _ { A b } \\frac { \\partial q _ { A b } } { \\partial \\theta } } & { } \\end{array}\n$$\n\nTheorem 1 reveals during the training phase, the gradient flow is implicitly guided in the direction of the integrated gradient. With the attribute capability of integral gradient, the class-related features received more attention, and then the bias of the classifier was also reduced. Naturally, the best choice for the baseline image is the same as the baseline for the integrated gradient, which is black.\n\nFurthermore, we found an interesting phenomenon that the distributions of pseudo labels before and after refinement tend to be consistent, as the training process deepens. We employ KL divergence to measure the consistency of this distribution, as shown in Figure 2. This observation leads us to the following corollary:\n\nCorollary 1.1. Under conditions of Theorem 1, if a black image is exploited for the refinement of pseudo-labels during training and if the gradient flow solution $\\theta ( \\infty )$ satisfies that $f _ { \\theta ( \\infty ) } ( x _ { b } ) = y _ { b }$ , then gradient flow converges to a minimizer of the Kullback-Leibler $( K L )$ divergence $\\mathcal { L } _ { k l } ( \\theta )$ between refinement and original pseudo-labels logits.\n\n$$\n\\theta ( \\infty ) = \\arg \\operatorname* { m i n } _ { \\theta } \\mathcal { L } _ { k l } ( \\theta ) , s . t . f _ { \\theta ( \\infty ) } ( x _ { b } ) = y _ { b }\n$$\n\nwhere\n\n$$\n\\mathcal { L } _ { k l } = - \\sum _ { b } g _ { \\theta } \\big ( x _ { b } , \\alpha \\big ( u _ { b } ^ { m } \\big ) \\big ) \\log \\frac { g _ { \\theta } ^ { \\prime } \\big ( x _ { b } , \\alpha \\big ( u _ { b } ^ { m } \\big ) \\big ) } { g _ { \\theta } \\big ( x _ { b } , \\alpha \\big ( u _ { b } ^ { m } \\big ) \\big ) }\n$$\n\nWe now summarize the internal mechanism of the classifier debias with a black baseline image under the CISSL framework as follows. 1) According to Sundararajan, Taly, and Yan, for a pre-trained classifier network $f _ { \\theta }$ , integrated gradients are better at reflecting class-related features of the input image. Since the CDMAD is implicitly trained in the direction of the linear integrated gradients, more labelirrelevant features can be discarded to achieve better classification results on SSL. 2) According to Corollary 1.1, a cross-entropy minimization predictor in which the refinement process implicitly regulates the KL divergence between the logits. Therefore, as the KL divergence becomes smaller, the consistency of logits before and after debias increases, which further regularizes the model space and improves the generalization ability of a model.\n\n# LCGC: A Consistency Gradient Conflicting Guided Debiasing Method\n\nFrom the analysis above, we further design a novel debiasing method that can improve the performance of the classifier under CISSL. In particular, our algorithm trains a classifier neural network $f _ { \\theta }$ as follows: 1) intentionally training the model $f _ { \\theta }$ to be over-biased on the unlabeled training samples that exhibit inconsistency before and after logit debiasing and 2) using a black baseline image to refine of biased class predictions during testing. Figure 3 illustrates the pseudolabel refinement process.\n\n# Training a Consistency Conflicting Guided Model\n\nSince we hope that the classifier can produce over-biased results, we need to make the distribution of pseudo-labels before and after refinement more different. Motivated by the success of regularization dropout (liang et al. 2021), we compare the debias logits $g _ { \\theta } ^ { \\prime }$ with the bias logits $g _ { \\theta }$ to regularize the gradient direction. Specifically, we obtain the biased consistency direction $G _ { b }$ by calculating the consistency regularization loss $\\mathcal { L } _ { C o n }$ between the weakly augmented prediction $p ( \\alpha ( u _ { b } ^ { m } ) )$ and the strongly augmented prediction $p \\big ( \\mathcal { A } ( u _ { b } ^ { m } ) \\big )$ as follows:\n\n$$\n\\mathcal { L } _ { C o n } = - \\sum _ { b } p ( \\mathcal { A } ( u _ { b } ^ { m } ) ) \\log p ( \\alpha ( u _ { b } ^ { m } ) )\n$$\n\nthe debiased direction $G _ { d }$ based on the KL divergence according to Eq. (14).\n\nWe denote the gradients of $\\mathcal { L } _ { C o n }$ and $\\mathcal { L } _ { k l }$ as $G _ { b } \\ =$ $\\nabla _ { u } \\mathcal { L } _ { C o n } ( u )$ and $G _ { d } = \\nabla _ { u } \\mathcal { L } _ { k l } ( u )$ , respectively. The relations between $G _ { b }$ and $G _ { d }$ are two-fold. 1) Their angle is smaller than $9 0 ^ { \\circ }$ , which indicates that the optimization direction of debias logits does not conflict with bias logits. In this case, we safely set the updated gradient direction $G _ { L C G C }$ as $G _ { b }$ . 2) Their angle is larger than $9 0 ^ { \\circ }$ , which indicates that debias logits conflict with bias logits. That is, optimizing the classifier following $G _ { b }$ will lead to an over-biased classifier. In this case, we project the $G _ { b }$ to the horizontal direction of $G _ { d }$ then add it to $G _ { b }$ to obtain an over-biased direction to optimize the classification model. Although this will slightly increase the KL loss, the postadjustment logits by the black baseline image will be enhanced. Our LCGC strategy is mathematically formulated as:\n\n$$\nG _ { L C G C } = \\left\\{ \\begin{array} { l l } { G _ { b } } & { \\mathrm { i f } \\ G _ { d } \\cdot G _ { b } \\ge 0 } \\\\ { G _ { b } + \\lambda \\cdot \\frac { G _ { d } \\cdot G _ { b } } { \\| G _ { d } \\| ^ { 2 } } G _ { d } } & { \\mathrm { o t h e r w i s e } } \\end{array} \\right.\n$$\n\nInstead of updating the parameter $\\theta$ using $G _ { b }$ , we optimize the $\\theta$ using $G _ { L C G C }$ , which encourages the gradient direction from biased classifier. We further introduce $\\lambda$ in Eq. (16) to generalize the formulation, which can flexibly control the strength of debias logits guidance in applications. In particular, $\\lambda = 1$ denotes projecting $G _ { b }$ to the over-biased direction, while setting $\\lambda = 0$ makes LCGC degenerate to CDMAD, i.e., CDMAD is a special case of our strategy. See more details on the selection of parameter $\\lambda$ in Section 5.3.\n\n# Refinement of Biased Logits During Inference\n\nWhile we train a biased model as described earlier, we must refine biased logit during inference on $x _ { k } ^ { t e s t }$ , for $k =$ $1 , \\cdots , K$ . Then, the logits for test samples:\n\n$$\ng _ { \\theta } ^ { * } ( x _ { k } ^ { t e s t } ) = g _ { \\theta } ^ { \\prime } ( x _ { k } ^ { t e s t } ) - g _ { \\theta } ( \\mathcal { T } )\n$$\n\nwhere $g _ { \\theta } ^ { * } ( x _ { k } ^ { t e s t } )$ is the finally prediction of $x _ { k } ^ { t e s t }$ .\n\n# Experiments\n\n# Experimental Setup\n\nWe implemented a series of experiments on CIFAR-10-LT, CIFAR-100-LT (Cui et al. 2019), SVHN-LT (Miyato et al.\n\n2018) and STL-100-LT (Kim et al. 2020a) following the settings of previous studies (Fan et al. 2022; Lee and Kim 2024). To assess classification performance across these datasets, we employed balanced accuracy (bACC) (Huang et al. 2016) and geometric mean (GM) (Kubat, Matwin et al. 1997) as metrics for CIFAR-10-LT, SVHN-LT and STL10-LT. For CIFAR-100-LT, evaluation was conducted using only the bACC metric. Each experiment was conducted three times on RTX 3090 GPUs to ensure reproducibility, and we report both the average and standard error of the performance measures.\n\n# Experimental Results\n\nTable 1 presents a detailed comparison of bACC and GM across various algorithms on CIFAR-10-LT under conditions where $\\gamma _ { u }$ is known and matched to $\\gamma _ { l }$ . It is observed that SSL algorithms like FixMatch and ReMixMatch demonstrate marked improvements in classification performance over the traditional approach. Despite their improved performance, they fall short of the efficiencies achieved by CISSL algorithms, emphasizing the necessity of directly addressing class imbalance issues within SSL frameworks. Notably, CISSL algorithms integrating the LCGC approach consistently outperform other methodologies. This strategy markedly improves performance across varying class imbalance ratios, as evidenced by superior bACC and GM scores.\n\nTable 1: Comparison of bACC/GM on CIFAR-10-LT under $\\gamma = \\gamma _ { l } = \\gamma _ { u }$ $( \\gamma _ { u }$ is assumed to be known).   \n\n<html><body><table><tr><td colspan=\"4\">CIFAR-10-LT(γ = γt = γu, γu is assumed to be known)</td></tr><tr><td>Algorithm</td><td>γ=50</td><td>γ=100</td><td>γ=150</td></tr><tr><td>Vanilla</td><td>65.2±0.1/61.1±0.1</td><td></td><td>58.8±0.1/58.2±0.1 55.6±0.4/44.0±1.0</td></tr><tr><td>Re-sampling</td><td></td><td>64.3±0.5/60.6±0.7 55.8±0.5/45.1±0.3 52.2±0.1/38.2±1.5</td><td></td></tr><tr><td>LDAM-DRW</td><td></td><td>68.9±0.1/67.0±0.1 62.8±0.2/58.9±0.6 57.9±0.2/50.4±0.3</td><td></td></tr><tr><td>cRT</td><td>67.8±0.1/66.3±0.2</td><td>63.2±0.5/59.9±0.4 59.3±0.1/54.6±0.7</td><td></td></tr><tr><td>FixMatch</td><td></td><td>79.2±0.3/77.8±0.4 71.5±0.7/66.8±1.5 68.4±0.2/59.9±0.4</td><td></td></tr><tr><td></td><td>/+DARP+cRT 85.8±0.4/85.6±0.6</td><td></td><td>82.4±0.3/81.8±0.2 79.6±0.4/78.9±0.4</td></tr><tr><td></td><td>/+CReST+LA 85.6±0.4/81.9±0.5 81.2±0.7/74.5±1.0 71.9±2.2/64.4±1.8</td><td></td><td></td></tr><tr><td>/+ABC</td><td>85.6±0.3/85.2±0.3 81.1±1.1/80.3±1.3 77.3±1.3/75.6±1.7</td><td></td><td></td></tr><tr><td>/+CoSSL</td><td>86.8±0.3/86.6±0.3</td><td></td><td>83.2±0.5/82.7±0.6 80.3±0.6/79.6±0.6</td></tr><tr><td>/+SAW+LA</td><td></td><td></td><td>86.2±0.2/83.9±0.4 80.7±0.2/77.5±0.2 73.7±0.1/71.2±0.2</td></tr><tr><td>/+Adsh</td><td>83.4±0.1/-</td><td>76.5±0.4/-</td><td>71.5±0.3/-</td></tr><tr><td>/+DebiasPL</td><td>-/-</td><td>80.6±0.5/-</td><td>-/-</td></tr><tr><td>/+UDAL</td><td>86.5±0.3/-</td><td>81.4±0.4/-</td><td>77.9±0.3/-</td></tr><tr><td>/+L2AC</td><td>-/-</td><td></td><td>82.1±0.6/81.5±0.6 77.6±0.5/75.8±0.7</td></tr><tr><td>/+CDMAD</td><td></td><td>87.3±0.1/87.0±0.2 83.6±0.5/83.1±0.6 80.8±0.9/79.9±1.1</td><td></td></tr><tr><td>/+LCGC</td><td></td><td></td><td>87.3±0.0/87.1±0.1 84.9±0.1/84.6±0.2 82.4±0.0/81.9±0.1</td></tr><tr><td>ReMixMatch</td><td>81.5±0.3/80.2±0.3</td><td>73.8±0.4/69.5±0.8 69.9±0.5/62.5±0.4</td><td></td></tr><tr><td>/+DARP+cRT</td><td>87.3±0.6/87.0±0.1</td><td></td><td>83.5±0.1/83.1±0.1 79.7±0.5/78.9±0.5</td></tr><tr><td>/+CReST+LA</td><td>84.2±0.1/-</td><td>81.3±0.3/-</td><td>79.2±0.3/-</td></tr><tr><td>/+ABC</td><td></td><td>87.9±0.5/87.6±0.5 84.5±0.3/84.1±0.4 80.5±1.2/79.5±1.4</td><td></td></tr><tr><td>/+CoSSL</td><td></td><td></td><td>87.7±0.2/87.6±0.3 84.1±0.6/83.7±0.7 81.3±0.8/80.5±0.8</td></tr><tr><td>/+SAW+cRT</td><td></td><td></td><td>87.6±0.2/87.4±0.3 85.4±0.3/83.9±0.2 79.9±0.2/79.9±0.1</td></tr><tr><td>/+CDMAD</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>88.3±0.4/88.1±0.4 85.5±0.5/85.3±0.4 82.5±0.2/82.0±0.3</td><td></td></tr><tr><td>/+LCGC</td><td></td><td></td><td>88.7±0.1/88.5±0.1 85.7±0.4/85.4±0.4 82.8±0.3/82.4±0.4</td></tr></table></body></html>\n\nTable 2 presents a comparative analysis of bACC and GM metrics on CIFAR-10-LT datasets under conditions where the class distribution of labeled and unlabeled sets are mismatched $( \\gamma _ { l } \\neq \\gamma _ { u } )$ ). The results highlight the robustness and efficacy of the LCGC approach in addressing class distribution mismatches in semi-supervised learning scenarios. For the CIFAR-10-LT dataset, where $\\gamma _ { l }$ is fixed at 100 and $\\gamma _ { u }$ varies, the performance of FixMatch combined with LCGC is notably superior. We outperform the second-best by an absolute accuracy of $1 . 7 \\%$ at an imbalance ratio $\\gamma _ { u } = 1 5 0$ with FixMatch, showcasing the ability of LCGC to mitigate biases introduced by class distribution mismatches effectively. Note that we employed ReMixMatch\\* (Kim et al. 2020b) to improve baseline algorithms, significantly improving the classification performance. However, LCGC still achieved better results in real-world scenarios. This makes LCGC more effective and scalable where the class distribution of unlabeled data is unknown and difficult to estimate.\n\nTable 2: Comparison of bACC/GM on CIFAR-10-LT under $\\gamma _ { l } \\neq \\gamma _ { u }$ $( \\gamma _ { u }$ is assumed to be unknown). ReMixMatch\\* denotes ReMixMatch with the estimated class distribution of the unlabeled set.   \n\n<html><body><table><tr><td colspan=\"4\">CIFAR-10-LT(γt = 10O,γu is assumed to be unknown)</td></tr><tr><td>Algorithm</td><td>γu=1</td><td>u=50</td><td>Yu =150</td></tr><tr><td>FixMatch</td><td></td><td>68.9±2.0/42.8±8.1 73.9±0.3/70.5±0.5 69.6±0.6/62.6±1.1</td><td></td></tr><tr><td>/+DARP</td><td></td><td>85.4±0.6/85.0±0.7 77.3±0.2/75.5±0.2 72.9±0.2/69.5±0.2</td><td></td></tr><tr><td>/+DAPR+LA</td><td></td><td>86.6±1.1/86.2±1.2 82.3±0.3/81.5±0.3 78.9±0.2/77.7±0.1</td><td></td></tr><tr><td>/+DARP+cRT</td><td>87.0±0.7/86.8±0.7</td><td>82.7±0.2/82.3±0.3 80.7±0.4/80.2±0.6</td><td></td></tr><tr><td>/+ABC</td><td></td><td>82.7±0.5/81.9±0.7 82.7±0.6/82.0±0.8 78.4±0.9/77.2±1.1</td><td></td></tr><tr><td>/+SAW</td><td>81.2±0.7/80.2±0.9</td><td>79.8±0.3/79.1±0.3 74.5±1.0/72.5±1.4</td><td></td></tr><tr><td>/+SAW+LA</td><td></td><td>84.5±0.7/84.1±0.3 82.9±0.4/82.6±0.4 79.1±0.8/78.6±0.9</td><td></td></tr><tr><td>/+SAW+cRT</td><td></td><td>84.6±0.2/84.4±0.3 81.6±0.4/81.3±0.3 77.6±0.4/77.1±0.4</td><td></td></tr><tr><td>/+CDMAD</td><td></td><td>87.5±0.5/87.1±0.5 85.7±0.4/85.3±0.4 82.3±0.2/81.8±0.3</td><td></td></tr><tr><td>/+LCGC</td><td></td><td>88.2±0.4/87.8±0.4 85.9±0.4/85.4±0.4 84.0±0.2/83.7±0.2</td><td></td></tr><tr><td>ReMixMatch</td><td></td><td>48.3±0.1/19.5±0.9 75.1±0.4/71.9±0.8 72.5±0.1/68.2±0.3</td><td></td></tr><tr><td></td><td>ReMixMatch* 85.0±1.4/84.3±1.6 77.0±0.1/74.7±0.0 72.8±0.1/68.8±0.2</td><td></td><td></td></tr><tr><td>/+DARP</td><td></td><td>86.9±0.1/86.4±0.2 77.4±0.2/75.0±0.3 73.2±0.1/69.2±0.3</td><td></td></tr><tr><td>/+DARP+LA</td><td></td><td>81.8±0.5/80.9±0.4 83.9±0.4/83.4±0.5 81.1±0.2/80.3±0.3</td><td></td></tr><tr><td>/+DARP+cRT</td><td></td><td>88.7±0.3/88.5±0.3 83.5±0.5/83.1±0.5 80.9±0.3/80.3±0.3</td><td></td></tr><tr><td>/+ABC</td><td></td><td></td><td>76.4±5.3/74.8±6.1 85.2±0.2/84.7±0.3 80.4±0.4/80.0±0.4</td></tr><tr><td>/+SAW</td><td></td><td></td><td>87.0±0.8/86.4±0.9 80.6±1.6/79.2±2.2 77.6±0.8/76.0±0.9</td></tr><tr><td>/+SAW+LA</td><td></td><td></td><td>74.2±1.5/65.1±2.4 84.8±1.1/82.4±2.3 81.3±2.4/80.9±2.5</td></tr><tr><td>/+SAW+cRT</td><td></td><td></td><td>88.8±0.8/88.6±0.8 84.5±0.8/83.6±1.3 82.4±0.1/82.0±0.1</td></tr><tr><td>/+CDMAD</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>89.9±0.5/89.6±0.5 86.9±0.2/86.7±0.2 83.1±0.5/82.7±0.5</td><td></td></tr><tr><td>/+LCGC</td><td></td><td>90.1±0.5/89.6±0.4 87.0±0.1/86.8±0.1 83.9±0.3/83.6±0.3</td><td></td></tr></table></body></html>\n\nTable 3: Comparison of bACC/GM under $\\gamma _ { l } = \\gamma _ { u } = 1 0 0$ (reversed)   \n\n<html><body><table><tr><td colspan=\"7\">CIFAR-10-LT,γ= 100,γu = 100 (reversed)</td></tr><tr><td>Algorithm</td><td>ABC</td><td>SAW</td><td>SAW+LA</td><td>SAW+cRT</td><td>CDMAD</td><td>LCGC</td></tr><tr><td>FixMatch+</td><td>69.5/66.8</td><td>72.3/68.7</td><td>74.1/72.0</td><td>75.5/73.9</td><td>77.1/75.4</td><td>78.6/77.2</td></tr><tr><td>ReMixMatch+</td><td>63.6/60.5</td><td>79.5/78.5</td><td>50.2/14.8</td><td>80.8/79.9</td><td>81.7/81.0</td><td>83.6/82.4</td></tr></table></body></html>\n\nTo examine the performance of our method in more imbalanced scenarios, experiments were also conducted under conditions where the class distribution of the unlabeled set was imbalanced in the opposite direction to the labeled set. As shown in Table 3, LCGC consistently outperforms other algorithms under these settings.\n\nTable 4 presents a detailed comparison of bACC on CIFAR-100-LT across various algorithms under different class imbalance ratios $( \\gamma )$ . The results clearly demonstrate the effectiveness of the proposed LCGC approach in improving classification performance under class-imbalanced settings with a large number of classes. When comparing FixMatch and ReMixMatch combined with various debiasing techniques, it is evident that LCGC consistently achieves the highest bACC scores. In scenarios where $\\gamma$ is set to 20, 50, and 100, FixMatch+LCGC and ReMixMatch+LCGC outperform other methods. Besides the best performance across settings, our method also improves performance for small imbalance ratios as well $( 1 . 0 \\%$ higher than the second-best at imbalance ratio $\\gamma = 2 0$ with FixMatch).\n\nTable 4: Comparison of bACC on CIFAR-100-LT   \n\n<html><body><table><tr><td colspan=\"4\">CIFAR-100-LT(γ = γt = γu,γu is assumed to be known)</td></tr><tr><td>Algorithm</td><td>γ=20</td><td>γ=50</td><td>γ= 100</td></tr><tr><td>FixMatch</td><td>49.6±0.8</td><td>42.1±0.3</td><td>37.6±0.5</td></tr><tr><td>FixMatch+DARP</td><td>50.8±0.8</td><td>43.1±0.5</td><td>38.3±0.5</td></tr><tr><td>FixMatch+DARP+cRT</td><td>51.4±0.7</td><td>44.9±0.5</td><td>40.4±0.8</td></tr><tr><td>FixMatch+CReST</td><td>51.8±0.7</td><td>44.9±0.5</td><td>40.1±0.7</td></tr><tr><td>FixMatch+CReST+LA</td><td>52.9±0.1</td><td>47.3±0.2</td><td>42.7±0.7</td></tr><tr><td>FixMatch+ABC</td><td>53.3±0.8</td><td>46.7±0.3</td><td>41.2±0.7</td></tr><tr><td>FixMatch+CoSSL</td><td>53.9±0.8</td><td>47.6±0.6</td><td>43.0±0.6</td></tr><tr><td>FixMatch+UDAL</td><td>1</td><td>48.0±0.6</td><td>43.7±0.4</td></tr><tr><td>FixMatch+CDMAD</td><td>54.3±0.4</td><td>48.8±0.8</td><td>44.1±0.3</td></tr><tr><td>FixMatch+LCGC</td><td>55.3±0.5</td><td>49.3±0.3</td><td>44.8±0.5</td></tr><tr><td>ReMixMatch</td><td>51.6±0.4</td><td>44.2±0.6</td><td>39.3±0.4</td></tr><tr><td>ReMixMatch+DARP</td><td>51.9±0.4</td><td>44.7±0.7</td><td>39.8±0.5</td></tr><tr><td>ReMixMatch+DARP+cRT</td><td>54.5±0.4</td><td>48.5±0.9</td><td>43.7±0.8</td></tr><tr><td>ReMixMatch+CReST</td><td>51.3±0.3</td><td>45.5±0.8</td><td>41.0±0.8</td></tr><tr><td>ReMixMatch+CReST+LA</td><td>51.9±0.6</td><td>46.6±1.1</td><td>41.7±0.7</td></tr><tr><td>ReMixMatch+ABC</td><td>55.6±0.4</td><td>47.9±0.1</td><td>42.2±0.1</td></tr><tr><td>ReMixMatch+CoSSL</td><td>55.8±0.6</td><td>48.9±0.6</td><td>44.1±0.6</td></tr><tr><td>ReMixMatch+CDMAD</td><td>57.0±0.3</td><td>51.1±0.5</td><td>44.9±0.4</td></tr><tr><td>ReMixMatch+LCGC</td><td>57.3±0.3</td><td>50.7±0.4</td><td>45.9±0.6</td></tr></table></body></html>\n\nTable 5: Comparison of bACC/GM on STL-10-LT and SVHN-LT $( \\gamma _ { u }$ is Unknown)   \n\n<html><body><table><tr><td colspan=\"3\">STL-10-LT</td><td colspan=\"2\">SVHN-LT</td></tr><tr><td>Algorithm</td><td>γ=10</td><td>γ=20</td><td>γ=100</td><td></td></tr><tr><td>FixMatch</td><td>72.9±0.1/69.6±0.0</td><td>63.4±0.2/52.6±0.1</td><td>88.0±0.3/79.4±0.5</td></tr><tr><td>/+DARP</td><td>77.8±0.3/76.5±0.4</td><td>69.9±1.8/65.4±3.1</td><td>88.6±0.2/80.5±0.5</td></tr><tr><td>/+DAPR+LA</td><td>78.6±0.3/77.4±0.4</td><td>71.9±0.5/68.7±0.5</td><td>-/-</td></tr><tr><td>/+DARP+cRT</td><td>79.3±0.2/78.7±0.2</td><td>74.1±0.6/73.1±1.2</td><td>89.9±0.4/83.5±0.6</td></tr><tr><td>/+ABC</td><td>79.1±0.5/78.1±0.6</td><td>73.8±0.2/72.1±0.2 92.0±0.4/87.9±0.7</td><td></td></tr><tr><td>/+CDMAD</td><td>79.9±0.2/78.9±0.4</td><td>75.2±0.4/73.5±0.3</td><td>92.4±0.2/92.2±0.2</td></tr><tr><td>/+LCGC</td><td>80.1±0.4/79.2±0.3</td><td>76.6±0.3/75.2±0.3</td><td>93.3±0.2/93.2±0.2</td></tr></table></body></html>\n\nTable 5 summarizes bACC and GM of the baseline algorithms on STL-10-LT and SVHN-LT. In the STL-10-LT\n\nT LCGC,Y/= 100,Yu = 100 LCGC,yi=100,Yu= 100 00.930.000.010.010.010.000.000.010.020.01 0.920.000.020.000.010.000.000.010.020.01 LCGC,γ/= 100,Yu = 150 LCGC,Yi= 100,Yu = 150 10.000.980.000.000.000.000.000.000.000.010.000.980.000.000.000.000.000.00.0.01 CDMAD:y=100.Y=10 CDMAD:v=100y=10 20.020.000.890.020.020.020.020.000.010.000.020.000.870.010.020.030.040.000.000.00 40.000.000.020.020.920.010.010.010.000.000.000.000.030.020.910.000.020.010.000.00 50.00.000.040.160.040.710.020.020.020.000.000.000.030.130.040.750.020.030.010.00 60.010.000.040.040.010.010.870.000.000.000.010.000.040.040.020.010.880.010.000.00 70.000.000.020.060.090060.010.750.000.000.000.000.020.030.090.060.010.780.010.00 80.110.060.010.010.00.000.000.010.780.010.080.040.010.000.000.010.010.010.810.03   \n0 1 2 3 45 6 7 8 9 0 1 2 456 7 89 01 234567 8 9 0 1 23456 7 8 9 Class Index Class Index Predicted Label Predicted Label   \n(a)FixMatch+CDMAD/+LCGC (b)ReMixMatch+CDMAD/+LCGC (c)FixMatch+CDMAD (d) FixMatch+LCGC\n\ndataset, where $\\gamma _ { l }$ varies, FixMatch combined with LCGC again demonstrates superior performance. With $\\gamma _ { l }$ set to 10, FixMatch+LCGC achieves better results than other algorithms, and when $\\gamma _ { l }$ is increased to 20, the performance remains consistently higher. In the SVHN-LT dataset, LCGC outperformed the baseline algorithms by a large margin. The effective use of unlabeled samples through gradient-guided pseudo-label refinement may allow the proposed algorithm to be suitable for CISSL on special scenarios datasets.\n\nTable 6: Comparison of bACC/GM on Small-ImageNet-127 (size $3 2 \\times 3 2$ and $6 4 \\times 6 4$ , $\\gamma _ { u }$ is assumed to be known)   \n\n<html><body><table><tr><td colspan=\"3\">Small-ImageNet-127(y = γ = γu,γu is assumed to be known)</td></tr><tr><td>Algorithm</td><td>32 ×32</td><td>64 ×64</td></tr><tr><td>FixMatch</td><td>29.7</td><td>42.3</td></tr><tr><td>FixMatch+DARP</td><td>30.5</td><td>42.5</td></tr><tr><td>FixMatch+DARP+cRT</td><td>39.7</td><td>51.0</td></tr><tr><td>FixMatch+CReST</td><td>32.5</td><td>44.7</td></tr><tr><td>FixMatch+CReST+LA</td><td>40.9</td><td>55.9</td></tr><tr><td>FixMatch+ABC</td><td>46.9</td><td>56.1</td></tr><tr><td>FixMatch+CoSSL</td><td>43.7</td><td>53.8</td></tr><tr><td>FixMatch+CDMAD</td><td>48.4</td><td>59.3</td></tr><tr><td>FixMatch+LCGC</td><td>49.0</td><td>59.8</td></tr></table></body></html>\n\nTable 6 summarizes bACC of the baseline algorithms on Small-ImageNet-127. LCGC achieves the best performance for image sizes 32 and 64, respectively. This result highlights the effectiveness of our method in addressing the challenges of CISSL on large-scale and complex datasets.\n\n# Qualitative Analyses\n\nIn Figure 4 (a) and (b), we observe the class probability distributions under various configurations for FixMatch and ReMixMatch algorithms enhanced with CDMAD and LCGC debiasing techniques. Notably, in Figure 4 (a), LCGC shows significantly higher probabilities for the head classes and lower probabilities for the tail classes than CDMAD, indicating that it is more sensitive to the bias caused by class imbalance. Therefore, in the testing phase, using the logit of the baseline image to refine the over-biased logits can achieve better results than CDMAD. Figure 4 (c) and (d) display the confusion matrices of the class predictions on the test set of CIFAR-10 using FixMatch+CDMAD and FixMatch+LCGC trained on CIFAR-10LT under $\\gamma _ { l } = 1 0 0$ and $\\gamma _ { u } = 1 5 0$ . Notably, the LCGC algorithm demonstrates improved classification accuracies across several classes, particularly in handling minority class samples more effectively. We conducted an ablation study to examine the impact of each component of LCGC using CIFAR-10-LT with $\\gamma _ { l } = 1 0 0$ and $\\gamma _ { u } = 1 5 0$ , as summarized in Table 7. The results indicate that removing the component for refining pseudo-labels results in a substantial performance drop. Similarly, excluding LCGC during the test phase also negatively impacts performance. Moreover, utilizing a confidence threshold $\\tau = 0 . 9 5$ ) and applying distribution alignment techniques yield slightly lower performance compared to the full LCGC implementation.\n\n![](images/362d767b5c583787efba16d369dc0f21fbbd1efa6a8c6646c3ed0145edd1cf30.jpg)  \nFigure 4: (a) and (b) present the class probabilities that take the absolute value of the log predicted on a black image using the proposed algorithm. (c) and (d) present the confusion matrices of the class predictions on test samples.   \nFigure 5: Line chart of validation bACC and GM for the CIFAR-10-LT $( \\gamma _ { l } = \\gamma _ { u } = 1 0 0 )$ dataset across a range of hyperparameter $\\lambda$ .\n\n# Sensitivity Analysis\n\nWe hold out the CIFAR-10-LT $( \\gamma _ { l } = \\gamma _ { u } = 1 0 0 )$ ) as a validation set, and we report the bACC and GM across a range of parameter values in Figure 5. Note that we choose hyperparameters ranging from 0-1.4 with an interval of 0.2. We select $\\lambda = 1 . 0$ as they correspond to the smallest values.\n\n<html><body><table><tr><td>Ablation study (t = 100, u = 150)</td><td colspan=\"2\">bACC/GM</td><td>bACC/GM</td></tr><tr><td>FixMatch+LCGC</td><td>84.0/83.7</td><td>ReMixMatch+LCGC</td><td>83.9/83.6</td></tr><tr><td>Without LCGC for refining pseudo-labels</td><td>81.3/80.4</td><td>Without LCGC for refining pseudo-labels</td><td>83.6/83.0</td></tr><tr><td>Without LCGC for test phase</td><td>75.9/73.2</td><td>Without LCGC for test phase</td><td>78.4/76.5</td></tr><tr><td>With the use of confidence threshold T = 0.95</td><td>83.0/82.6</td><td>With the use of distribution alignment technique</td><td>82.1/81.4</td></tr></table></body></html>\n\nTable 7: Ablation study for the proposed algorithm on CIFAR-10-LT under $\\gamma _ { l } = 1 0 0$ and $\\gamma _ { u } = 1 5 0$\n\nTable 8: Experiments with the replacement of $\\boldsymbol { \\mathcal { T } }$   \n\n<html><body><table><tr><td>FixMatch+LCGC</td><td>CIFAR-10-LT</td></tr><tr><td>Input</td><td>γ=γu=100 γ=100,γu=150</td></tr><tr><td>Red</td><td>83.6/83.2 82.1/81.6</td></tr><tr><td>Green</td><td>83.7/83.4 82.3/81.9</td></tr><tr><td>Blue</td><td>84.6/84.3 83.0/82.6</td></tr><tr><td>Gray</td><td>84.5/84.1 83.9/83.4</td></tr><tr><td>White</td><td>84.9/84.6 84.0/83.6</td></tr><tr><td>Black</td><td>84.9/84.6 84.0/83.7</td></tr></table></body></html>\n\n# Ablation Study\n\nWe also conducted experiments to evaluate the effectiveness of replacing the solid color image $\\boldsymbol { \\mathcal { T } }$ with various color inputs in the LCGC debiasing process using CIFAR-10-LT with $\\gamma _ { l } = \\gamma _ { u } = 1 0 0$ and $\\gamma _ { l } = 1 0 0$ , $\\gamma _ { u } = 1 5 0$ , as summarized in Tab 8. Notably, the use of a black image consistently yielded the highest performance across both $\\gamma _ { l } = \\gamma _ { u } = 1 0 0$ and $\\gamma _ { l } ~ = ~ 1 0 0$ , $\\gamma _ { u } ~ = ~ 1 5 0$ settings. The experiments with other color inputs, such as red, green, and blue, showed a slight reduction in performance compared to black. This is due to the Batch Normalization (BN) layer in the classifier backbone: solid color images yield uniform or near-uniform outputs after convolution, which the BN layer standardizes to zero mean and unit variance. As a result, solid black and white images perform similarly (though slight numerical differences exist before rounding), and other solid color images exhibit similar effects. Conversely, noise images produce random pixel distributions that vary across regions, leading to inconsistent outputs after BN standardization and thus poorer performance. From the analysis, it is evident that the original solid color image, particularly black, plays a critical role in the debiasing process of LCGC.\n\n# Conclusion\n\nWe proposed the LCGC algorithm, a novel approach designed to leverage consistency gradient conflicting for improved class representation. We give a theoretical basis for using a baseline image to debias and further integrate gradient guidance information to further improve the debias model. Our comprehensive experiments on benchmark datasets demonstrated that LCGC improves classification accuracy. It is particularly noteworthy that our method is more effective under the class imbalance setting, and the effect becomes better as the imbalance ratio increases.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了在类别不平衡的半监督学习（Class-Imbalanced Semi-Supervised Learning, CISSL）中，分类器倾向于偏向多数类的问题。这一问题在实际应用中尤为关键，因为数据集的类别不平衡会导致模型在少数类上的性能显著下降，影响模型的泛化能力和实际应用效果。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种名为LCGC（Learning from Consistency Gradient Conflicting）的去偏方法，通过利用一致性梯度冲突来训练一个过偏的分类器，并在测试阶段通过减去基线图像的logits来去偏。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **理论分析：** 证明了基线图像（尤其是黑色图像）能够通过隐式利用积分梯度流来增强基础SSL模型的平衡性。\\n> *   **方法创新：** 提出了一种简单的去偏方案，通过训练一致性梯度冲突来优化分类器。\\n> *   **实验验证：** 在四个基准数据集上验证了LCGC的有效性，在类别分布匹配和不匹配的情况下均显著提升了分类准确率。例如，在CIFAR-10-LT数据集上，LCGC在类别不平衡比为100时，准确率达到了87.3%，比基线方法提升了1.7%。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   LCGC的核心思想是通过训练一个过偏的分类器，并在测试阶段利用基线图像的logits来去偏。该方法通过KL散度衡量伪标签在去偏前后的分布一致性，鼓励两者之间的冲突，从而优化分类器的梯度方向。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前的工作（如CDMAD）通过复杂的网络变体或因果平衡器来去偏，缺乏理论支持。\\n> *   **本文的改进：** LCGC通过理论证明了基线图像的有效性，并提出了一种更简单且高效的去偏方法。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  训练阶段：通过KL散度计算伪标签在去偏前后的分布一致性，鼓励梯度冲突。\\n> 2.  测试阶段：利用基线图像的logits来去偏，得到最终的预测结果。\\n> 3.  关键公式：\\n>     $$\\n>     g_{\\\\theta}^{\\\\prime}(x_n) = g_{\\\\theta}(x_n) - g_{\\\\theta}(\\\\mathcal{T})\\n>     $$\\n>     其中，$g_{\\\\theta}(x_n)$是输入图像的logits，$g_{\\\\theta}(\\\\mathcal{T})$是基线图像的logits。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   FixMatch\\n> *   ReMixMatch\\n> *   CDMAD\\n> *   DARP\\n> *   CReST\\n> *   ABC\\n> *   CoSSL\\n> *   SAW\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在平衡准确率（bACC）上：** 在CIFAR-10-LT数据集上，LCGC在类别不平衡比为100时达到了87.3%，显著优于基线模型FixMatch（71.5%）和CDMAD（83.6%）。与表现最佳的基线相比，提升了3.7个百分点。\\n> *   **在几何均值（GM）上：** 在CIFAR-10-LT数据集上，LCGC在类别不平衡比为100时达到了84.6%，远高于基线模型FixMatch（66.8%）和CDMAD（83.1%）。与表现最佳的基线相比，提升了1.5个百分点。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   类别不平衡半监督学习 (Class-Imbalanced Semi-Supervised Learning, CISSL)\\n*   去偏方法 (Debiasing Method, N/A)\\n*   一致性梯度冲突 (Consistency Gradient Conflicting, CGC)\\n*   基线图像 (Baseline Image, N/A)\\n*   积分梯度流 (Integrated Gradient Flow, IGF)\\n*   伪标签 (Pseudo-Label, N/A)\\n*   平衡准确率 (Balanced Accuracy, bACC)\\n*   几何均值 (Geometric Mean, GM)\"\n}\n```"
}