{
    "source": "Semantic Scholar",
    "arxiv_id": "2501.02926",
    "link": "https://arxiv.org/abs/2501.02926",
    "pdf_link": "https://arxiv.org/pdf/2501.02926.pdf",
    "title": "Offline-to-online hyperparameter transfer for stochastic bandits",
    "authors": [
        "Dravyansh Sharma",
        "A. Suggala"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2025-01-06",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 4,
    "influential_citation_count": 0,
    "institutions": [
        "TTIC",
        "Google DeepMind"
    ],
    "paper_content": "# Offline-to-Online Hyperparameter Transfer for Stochastic Bandits\n\nDravyansh Sharma1, Arun Suggala2 1TTIC 2Google DeepMind dravy $@$ ttic.edu, arunss@google.com\n\n# Abstract\n\nClassic algorithms for stochastic bandits typically use hyperparameters that govern their critical properties such as the trade-off between exploration and exploitation. Tuning these hyperparameters is a problem of great practical significance. However, this is a challenging problem and in certain cases is information theoretically impossible. To address this challenge, we consider a practically relevant transfer learning setting where one has access to offline data collected from several bandit problems (tasks) coming from an unknown distribution over the tasks. Our aim is to use this offline data to set the hyperparameters for a new task drawn from the unknown distribution. We provide bounds on the inter-task (number of tasks) and intra-task (number of arm pulls for each task) sample complexity for learning near-optimal hyperparameters on unseen tasks drawn from the distribution. Our results apply to several classic algorithms, including tuning the exploration parameters in UCB and LinUCB and the noise parameter in GP-UCB. Our experiments indicate the significance and effectiveness of the transfer of hyperparameters from offline problems in online learning with stochastic bandit feedback.\n\n# Introduction\n\nBandit optimization is a very important framework for sequential decision making with numerous applications, including recommendation systems (Li et al. 2010), healthcare (Tewari and Murphy 2017), AI for Social Good (AI4SG) (Mate et al. 2022), hyperparameter tuning in deep learning (Bergstra et al. 2011). Over the years, numerous works have designed optimal algorithms for bandit optimization in various settings (Garivier and Capp´e 2011; AbbasiYadkori, P´al, and Szepesv´ari 2011; Whitehouse, Wu, and Ramdas 2023). One of the key challenges in deploying these algorithms in practice is setting their hyperparameters appropriately. Some examples of these hyperparameters include the confidence width parameter in UCB (Auer, Cesa-Bianchi, and Fischer 2002) and the noise variance parameter $\\sigma ^ { 2 }$ in GP-UCB (Srinivas et al. 2010). While one could rely on theory-suggested hyperparameters, they often turn out to be too optimistic and lead to suboptimal performance in practice. This is because these hyperparameters are meant to guard the algorithm against worst-case problems and are not necessarily optimal for typical problems arising in a domain. To see this, consider a Multi-Armed Bandit (MAB) instance with two arms, with rewards of each arm drawn from a uniform distribution over an unknown unit width interval. Suppose, the problems encountered in practice are such that the two distributions are well separated without any overlap. Then, the optimal choice of the UCB confidence width parameter is 0 (i.e., there is no need for exploration). This is in contrast to the theory suggested choice of 1, which incurs an exploration penalty. This simplistic example illustrates the importance of choosing hyperparameters appropriately in practice.\n\nHyperparameter selection, though well-studied in offline learning (Stone 1974; Efron 1992), remains an active research area in bandit optimization. Several recent studies have attempted to address this problem. One particularly popular approach in this line of work is the design of meta-bandit algorithms that treat each hyperparameter as an expert and adaptively select the best expert by running another bandit algorithm on top (also known as corralling algorithms) (Agarwal et al. 2017). However, the current algorithms for corralling and their theoretical guarantees are not satisfactory. For instance, consider the stochastic MAB problem, and consider the problem of picking the UCB confidence width parameter. The corralling algorithm of Agarwal et al. (2017) incurs a regret overhead of $O ( { \\sqrt { M T } } )$ compared to the regret of the best hyperparameter (where $M$ is the size of the hyperparameter search space). This overhead can be quite large when $M$ is large. Furthermore, their algorithm requires the base algorithms to satisfy certain stability conditions. However, UCB is known to satisfy this condition only for certain values of the hyperparameter. Recent works of Arora, Marinov, and Mohri (2021) improved the regret guarantees of Agarwal et al. (2017) by considering stochastic bandits. But their regret bounds, when specialized to UCB confidence width tuning, are worse than the regret bounds obtained using the theory-suggested hyperparameter.\n\nAlternative approaches, such as variance-aware algorithms like UCB-V (Mukherjee et al. 2018), hyperparameter-free algorithms (Zimmert and Seldin 2021; Ito 2021), and instance optimal algorithms such as KL-UCB (Garivier and Capp´e 2011) still have certain hidden hyperparameters that need careful tuning in practice. For example, UCB-V assumes the reward is bounded between $[ 0 , 1 ]$ . Similarly, the parameterfree algorithms of Zimmert and Seldin (2021); Ito (2021), and instance optimal algorithms assume the reward is bounded between [0, 1]. Such assumptions do not always hold (or are not tight) in practice, leading to suboptimal performance. For example, if the true rewards lie in [0, 0.5] instead of [0, 1], one could choose a smaller exploration parameter in KL-UCB and achieve better regret bounds. In summary, even in the canonical MAB setting, optimal hyperparameter selection for UCB is still an open problem.\n\nIn this work, we first ask the following question: Is it possible to choose good hyperparameters for a given bandit algorithm that perform nearly as well as the best possible hyperparameters, on any given problem instance?1 Interestingly, we answer this question negatively, showing that even in the simplest problem of stochastic multi-armed bandits (MABs), determining the best hyperparameter for UCB is information-theoretically impossible.\n\nTo address the challenge of hyperparameter selection in bandit algorithms, we propose a data-driven approach. We assume that the learner has access to historical data from similar problem instances to the one at hand. This is a reasonable assumption in practical applications of bandit optimization such as learning-rate tuning in deep learning, where we often have offline data collected from previous learning-rate tuning runs on similar tasks. Our goal is to leverage this side information to find a good hyperparameter that approximately maximizes the expected reward of the bandit algorithm.\n\nWe introduce a new framework for hyperparameter tuning of stochastic bandit algorithms, leveraging related historical data as side information. Our approach assumes that problem instances within a specific application domain are sampled from an unknown distribution, and past data from this distribution is available to the learner. Effective hyperparameter tuning in our framework corresponds to minimizing two key quantities: (a) Inter-task sample complexity: The number of historical problem instances needed for effective learning, and (b) Intra-task sample complexity: The number of arm pulls or data points required from each individual task. A key contribution of our work is the derivation of provable bounds for both inter-task and intra-task sample complexities. These bounds guarantee that the learned hyperparameters are close in performance to the optimal domain-specific choice on unseen tasks drawn from the distribution. Our framework is broadly applicable to several widely-used bandit algorithms, including: (a) tuning the confidence width or exploration parameters in UCB and LinUCB (Abbasi-Yadkori, P´al, and Szepesv´ari 2011), (b) tuning the noise parameter in GP-UCB (Srinivas et al. 2010). Our experimental results demonstrate the effectiveness and significance of transferring hyperparameter knowledge from offline data to online bandit optimization.\n\n# Related Work\n\nThis section reviews relevant research on corralling, and transfer learning in bandits. For a review of other topics including model selection in bandits, Bayesian bandits, and data-driven hyperparameter selection in other ML problems, we direct the reader to the Appendix.\n\nCorralling. Corralling bandits (Agarwal et al. 2017; Cutkosky, Das, and Purohit 2020; Arora, Marinov, and Mohri 2021; Luo et al. 2022; Ding et al. 2022) is a recent line of work which involves design of bandit algorithms that aim to find the best algorithm from among a finite collection of bandit algorithms. These remarkable techniques have a wide range of applications, including model selection and adapting to misspecification (Foster et al. 2020; Pacchiano et al. 2020). These results also imply an approach for online hyperparameter tuning in bandits, given a finite collection of hyperparameters. However, as discussed in the introduction, these algorithms don’t achieve the optimal rates for UCB hyperparameter selection in MAB. Moreover, these approaches are fully online, for which we have impossibility results (see Theorem 1). In contrast, in this work we study bandit hyperparameter selection over continuous parameter domain. Also, we consider a multitask setting where the goal is to learn a good hyperparameter from a collection of offline tasks, and transfer it to the online bandit tasks. Bouneffouf and Claeys (2018) developed a corralling style meta algorithm for hyperparameter selection in contextual bandits Angermueller et al. (2020) provided an algorithm that performs corralling to pick the best bandit algorithm for the design of biological sequences. However, both these works are mostly empirical in nature. Kang, Hsieh, and Lee (2024) consider the continuous parameter domain, but make Lipschitzness assumptions, which are not guaranteed for arbitrary distributions. In fact, even for Bernoulli arm rewards in the MAB setting, the expected rewards can be shown to be a piecewise constant function of the exploration parameter, which is not Lipschitz, and consequently their zooming algorithm based approach cannot be applied.\n\nTransfer Learning. Transfer learning in stochastic bandits, across several related tasks, has received recent attention from the community (Yogatama and Mann 2014). But unlike our work, most of these works have focused on transferring the knowledge of the reward model from one task to another. Our work is complementary to these works as we focus on transferring the knowledge of hyperparameters. Kveton et al. (2020) considered a setting similar to ours, where the bandit instances are sampled from an unknown distribution. The authors studied model learning using gradient descent for simpler policies like explore-then-commit (under nice arm-reward distributions), for which the expected reward as a function of the parameter is concave and easier to optimize. This structure does not hold for several typical reward distributions (e.g. Bernoulli). Our results hold for general unknown reward distributions and more powerful UCB-based algorithm families. Azar, Lazaric, and Brunskill (2013) considered a sequential arrival of tasks, but for the much simpler setting of finitely many models or bandit problems (finite Π in our notation) which are known to the learner. In a similar line of work, Khodak et al. (2023) designed a meta-algorithm to set the initialization, and other hyperparameters of Online Mirror Descent algorithm, based on past tasks. But their work considers transfer learning from one online task to another; in contrast, we consider transfer learning from offline tasks to online tasks. Swersky, Snoek, and Adams (2013); Wang et al.\n\n(2024) showed that learning priors from historic data helped improve the performance of GP-UCB. We note that these works are mostly empirical in nature and do not provide any sample complexity bounds. Apart from bandit feedback, transfer learning of hyperparameters in similar tasks has also been studied in online learning with full information feedback (Finn et al. 2019; Khodak, Balcan, and Talwalkar 2019).\n\n# Preliminaries\n\nA stochastic online learning problem with bandit feedback consists of a repeated game played over $T$ rounds. In round $t \\in [ T ]$ , the player plays an arm $a _ { t } \\in [ n ]$ from a finite set of $n$ arms and the environment simultaneously selects a reward function $r _ { t } : [ n ] \\to \\mathbb { R } _ { \\geq 0 }$ . In the stochastic setting, the environment draws a reward vector $\\mathbf { r } _ { t }$ as an independent sample from some fixed (but unknown) distribution over $\\mathbb { R } _ { \\geq 0 } ^ { n }$ , and the reward is simply $\\mathbf { r } _ { t } ( a ) = \\mathbf { r } _ { t a }$ for $a \\in [ n ]$ . Finally, the player observes and accumulates the reward $r _ { t } ( a _ { t } )$ corresponding (only) to the selected arm $a _ { t }$ . This is the wellstudied stochastic MAB setting. A standard measure of the player’s performance is the pseudo-regret given by\n\n$$\nR _ { T } = \\operatorname* { m a x } _ { a \\in [ n ] } \\mathbb { E } \\left[ \\sum _ { t = 1 } ^ { T } r _ { t } ( x _ { t } , a ) - r _ { t } ( x _ { t } , a _ { t } ) \\right]\n$$\n\nwhere the expectation is taken over the randomness of both the player and the environment. The expected average regret is given by $\\overline { { R _ { T } } } : = R _ { T } / T$ . Let $\\mu _ { i }$ denote the mean reward of arm $i \\in [ n ]$ and $\\Delta _ { i } : = \\operatorname* { m a x } _ { j } \\mu _ { j } - \\mu _ { i }$ denote the gap in the mean arm reward relative to the best arm. We will use the shorthand $\\mu _ { [ n ] } \\{ \\mu _ { l ^ { * } } \\to \\mu _ { l ^ { * } } ^ { \\prime } \\}$ to denote that the ${ { l } ^ { * } }$ -th entry of the tuple $\\mu _ { [ n ] } = ( \\mu _ { 1 } , \\ldots , \\mu _ { n } )$ is updated to $\\mu _ { l ^ { * } } ^ { \\prime }$ .\n\n# Impossibility of Hyperparameter Tuning\n\nWe now present lower bounds showing that (fully online) optimal hyperparameter selection is not always possible. Before we do that, we first quantify the notion of “optimal” hyperparameter selection. Consider a family of online learning algorithms $\\mathcal { A } = \\{ A _ { \\rho } : \\rho \\in \\mathcal { P } \\}$ , parameterized by hyperparameter $\\rho$ . An example of $\\mathcal { A }$ is the set of UCB policies, with $\\rho$ being the scale parameter multiplied with the confidence width. Let $\\Pi$ be a collection of stochastic multi-armed bandit problems. In hyperparameter tuning, our goal is to design a meta algorithm $\\bar { \\tilde { A } }$ for choosing an appropriate $\\rho$ which can compete wit ethe best possible hyperparameter, on any problem instance in $\\Pi$ . To be precise, we want $\\widetilde { A }$ to satisfy the following consistency condition for any $P \\in \\Pi$ : $\\mathrm { l i m } _ { T \\to \\infty } R _ { T } ( \\widetilde { A } ; P ) / R _ { T } ( A _ { \\rho ^ { * } } ; P ) = 1$ . Here, $\\begin{array} { r } { \\rho ^ { * } = \\operatorname * { a r g m i n } _ { \\rho \\in \\mathcal { P } } R _ { T } ( A _ { \\rho } ; P ) } \\end{array}$ is the best hyperparameter for the problem $\\dot { P }$ . We note that similar notions of consistency have been studied in the context of hyperparameter selection in offline learning (Kearns 1995; Yang 2007). In fact, hyperparameter selection techniques such as cross-validation are known to satisfy such consistency properties. The following result shows that consistency is not possible even in the simplest problem of MAB with rewards sampled from a Gaussian distribution.\n\nTheorem 1. Let $\\Pi$ be the set of MAB problems with arm rewards sampled from Gaussian distributions with variance belonging to the set $[ 0 , B ^ { 2 } ]$ . Let $\\mathcal { A }$ be the set of UCB policies, with $\\rho$ being the scale parameter multiplied with the confidence width. Then for any meta algorithm $\\widetilde { A }$ , there exists a problem $P \\in \\Pi$ that satisfies the followieng bound: $\\mathrm { l i m } _ { T  \\infty } R _ { T } ( \\widetilde { A } ; P ) / R _ { T } ( A _ { \\rho ^ { * } } ; P ) > 1$ .\n\nThe proof builds on standard distribution-dependent lower bounds (Capp´e et al. 2013, see Appendix). This result motivates our framework which uses offline bandit runs on similar problem instances for hyperparameter transfer.\n\n# Formal Framework for Transfer Learning\n\nGiven a stochastic online learning problem (say multi-armed bandits), let $\\Pi$ denote the set of problems (or tasks) of interest. That is, each $P \\in \\Pi$ defines an online learning problem. For example, if $\\Pi$ is a collection of stochastic multi-armed bandit problems, then $P$ could correspond to a fixed product distribution of arm rewards. We also fix a (potentially infinite) family of online learning algorithms $\\mathcal { A }$ , parameterized by a set $\\mathcal { P } \\subseteq \\mathbb { R } ^ { d }$ of $d$ real (hyper-)parameters. Let $A _ { \\rho }$ denote the algorithm in the family $\\mathcal { A }$ parameterized by $\\boldsymbol { \\rho } \\in \\mathcal { P }$ . For any fixed time horizon $T$ , the performance of any fixed algorithm on any fixed problem is given by some bounded loss metric (e.g. the expected average regret of the algorithm) $l _ { T } : \\Pi \\times \\mathcal { P }  [ 0 , H ]$ , i.e. $l _ { T } ( P , \\rho )$ measures the performance on problem $P \\in \\Pi$ of algorithm $A _ { \\rho } \\in { \\mathcal { A } }$ . The utility of a fixed algorithm $A _ { \\rho }$ from the family is given by $l _ { T } ^ { \\rho } : \\ln $ $[ 0 , H ]$ , with $l _ { T } ^ { \\rho } ( P ) \\overset { \\cdot } { = } l _ { T } ( P , \\rho )$ . We will be interested in the structure of the dual class of functions $l _ { T } ^ { P } : \\mathcal { P }  [ 0 , H ]$ , with $l _ { T } ^ { P } ( \\rho ) = l _ { T } ^ { \\rho } ( P )$ , which measure the performance of all algorithms of the family for a fixed problem $P \\in \\Pi$ .\n\nWe assume an unknown distribution $\\mathcal { D }$ over the problems in $\\Pi$ . We further have a collection of “offline” problems which we can use to learn a good value of the algorithm parameter $\\rho$ that works well on average on a random “test” problem drawn from $\\mathcal { D }$ . We are interested in the sample complexity of the number of “offline” problems that are sufficient to learn a near-optimal $\\rho$ over $\\mathcal { D }$ . Formally, the learner is given a collection $\\{ P _ { 1 } , \\dots , P _ { N } \\} \\sim \\mathcal { D } ^ { N }$ of offline problems for each of which the rewards have been collected according to some policy over time horizon $T _ { o }$ , which we refer to as the intra-task complexity. The learner learns a hyperparameter $\\hat { \\rho }$ based on these offline runs. A test problem is given by a random $P \\sim \\mathcal { D }$ on which the loss metric $l _ { T } ( P , \\hat { \\rho } )$ is measured over an online game of $T$ rounds. The $( \\epsilon , \\delta )$ sample complexity of the learner is the number $N$ of offline problems sufficient to guarantee that learned parameter $\\hat { \\rho }$ is near-optimal with high probability, i.e. with probability at least $1 - \\delta$ ,\n\n$$\n\\left| \\mathbb { E } _ { P \\sim \\mathcal { D } } [ l _ { T } ( P , \\hat { \\rho } ) ] - \\operatorname* { m i n } _ { \\rho \\in \\mathcal { P } } \\mathbb { E } _ { P \\sim \\mathcal { D } } [ l _ { T } ( P , \\rho ) ] \\right| \\leq \\epsilon .\n$$\n\n# Derandomized Dual Complexity\n\nWe will define a useful quantity to measure the inherent challenge in learning the best hyperparameter for an unknown problem distribution $\\mathcal { D }$ . For any offline problem $P$ , let $\\textbf { z }$ denote the random coins used in drawing the arm rewards according to the corresponding distribution $D _ { P }$ , and $l _ { T } ^ { P , \\mathbf { z } } ( \\rho )$ denote the corresponding derandomized dual function, i.e. $l _ { T } ^ { P } ( \\boldsymbol { \\rho } ) = \\mathbb { E } _ { \\mathbf { z } } [ l _ { T } ^ { P , \\mathbf { z } } ( \\boldsymbol { \\rho } ) ]$ . Intuitively, we can think of fixing $\\textbf { z }$ as drawing the rewards according to $D _ { P }$ for the entire time horizon $T$ in advance (revealed as usual to the online learner), and taking an expectation over $\\mathbf { z }$ gives the expected loss or reward according to $P$ . More concretely, we have $\\mathbf { z } = ( z _ { t i } ) _ { i \\in [ n ] , t \\in [ T ] }$ with each $z _ { t i }$ drawn i.i.d. from the uniform distribution over $U ( [ 0 , 1 ] )$ . If $D _ { i }$ denotes the reward distribution for arm $i$ and $F _ { i }$ its cumulative density function, then the reward $\\mathbf { r } _ { t i }$ is given by $F _ { i } ^ { - 1 } ( z _ { t i } )$ .\n\nFor typical parameterized stochastic bandit algorithms, we will show that $l _ { T } ^ { P , \\mathbf { z } } ( \\rho )$ is a piecewise constant function of $\\rho$ , i.e. the parameter space $\\mathcal { P }$ can be partitioned into finitely many connected regions $\\{ \\mathcal { P } _ { i } \\}$ such that $l _ { T } ^ { P , { \\bf z } } ( \\rho ) =$ $\\textstyle \\sum _ { i } c _ { i } { \\dot { \\mathbf { I } } } [ \\rho \\in { \\dot { \\mathcal { P } } } _ { i } ]$ where $c _ { i } \\in \\mathbb { R } , \\mathbf { I } [ \\cdot ]$ is the 0-1 valued indicator function, $\\begin{array} { r } { \\bigcup _ { i } \\mathcal { P } _ { i } = \\mathcal { P } } \\end{array}$ and $\\mathcal { P } _ { i } \\cap \\mathcal { \\bar { P } } _ { j } = \\varnothing$ for $i \\neq j$ . Let $q ( f )$ denote the number of pieces $\\{ \\mathcal { P } _ { i } \\}$ over which a piecewise constant function $f : \\mathcal { P } \\to \\mathbb { R }$ is defined. We define the derandomized dual complexity of problem distribution $\\mathcal { D }$ w.r.t. algorithms parameterized by $\\mathcal { P }$ as follows.\n\nDefinition 1. Suppose that the derandomized dual function $l _ { T } ^ { P , \\mathbf { z } } ( \\rho )$ is a piecewise constant function. The derandomized dual complexity of $\\mathcal { D } w . r . t . \\mathcal { P }$ is given by $Q _ { \\mathcal { D } } =$ $\\begin{array} { r } { \\mathbb { E } _ { P \\sim \\mathcal { D } } \\mathbb { E } _ { \\mathbf { z } \\sim D _ { P } } q ( l _ { T } ^ { P , \\mathbf { \\bar { z } } } ( \\cdot ) ) } \\end{array}$ .\n\n$Q _ { D }$ provides a distribution-dependent complexity measure that will be useful to bound the sample complexity as well as the intratask complexity of learning the best parameter over $\\mathcal { D }$ . Moreover, it may be empirically estimated over a collection of offline problems sampled from $\\mathcal { D }$ .\n\n# Sample Complexity of Hyperparameter Tuning\n\nWe proceed to provide a general inter-task sample complexity bound for learning one dimensional parameters, i.e. $\\mathcal { P } \\subset \\mathbb { R }$ . We state below our result as a uniform convergence guarantee and provide a proof sketch (full proof in the Appendix).\n\nTheorem 2. Consider the above setup for any arbitrary $\\mathcal { D }$ and suppose the derandomized dual function $l _ { T } ^ { P , \\mathbf { z } } ( \\rho )$ : $\\mathcal { P } ~  ~ [ 0 , H ]$ is a piecewise constant function. For any $\\epsilon , \\delta { \\it \\Delta \\phi } > 0 $ , $N$ problems $\\{ P _ { i } \\} _ { i = 1 } ^ { N }$ sampled from $\\mathcal { D }$ with corresponding random coins $\\{ \\mathbf { z } _ { i } \\} _ { i = 1 } ^ { N }$ such that $\\begin{array} { r l } { N } & { { } = } \\end{array}$ $\\begin{array} { r } { O \\left( \\left( \\frac { H } { \\epsilon } \\right) ^ { 2 } \\left( \\log Q _ { \\mathcal { D } } + \\log \\frac { 1 } { \\delta } \\right) \\right) } \\end{array}$ are sufficient to ensure that with probability at least $1 - \\delta$ , for all $\\boldsymbol { \\rho } \\in \\mathcal { P }$ , we have that\n\n$$\n\\left| \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } l _ { T } ^ { P _ { i } , \\mathbf { z } _ { i } } ( \\boldsymbol { \\rho } ) - \\mathbb { E } _ { P \\sim \\mathcal { D } } l _ { T } ^ { P } ( \\boldsymbol { \\rho } ) \\right| < \\epsilon .\n$$\n\nProof Sketch. Fix a problem $P \\in \\Pi$ . Fix the random coins $\\textbf { z }$ used to draw the arm rewards according to $D _ { P }$ for the $T$ rounds. We will use the piecewise loss structure to bound the Rademacher complexity, which would imply uniform convergence guarantees by applying standard learning-theoretic results. Let $\\rho , \\ldots , \\rho _ { m }$ denote a collection of parameter values, with one parameter from each of the $\\begin{array} { r } { m \\leq \\bar { \\sum _ { i = 1 } ^ { N } q ( l _ { T } ^ { P _ { i } , \\mathbf { z } _ { i } } ( \\cdot ) ) } } \\end{array}$ pieces of the dual class functions $l _ { T } ^ { P _ { i } , \\mathbf { z } _ { i } } ( \\cdot )$ for $i \\in [ N ]$ , i.e. across problems in the sample $\\{ P _ { 1 } , \\dotsc , P _ { N } \\}$ for some fixed randomizations. Let ${ \\mathcal { F } } = \\{ f _ { \\rho } : ( P , \\mathbf { z } ) \\mapsto l _ { T } ^ { P , \\mathbf { z } } ( \\rho ) ~ | ~ \\rho \\in { \\mathcal { P } } \\}$ be a family of functions on a given sample of instances $S = \\{ P _ { i } , \\bar { \\mathbf { z } _ { i } } \\} _ { i = 1 } ^ { N }$ .\n\nSince the function $f _ { \\rho }$ is constant on each of the $m$ pieces, we have the empirical Rademacher complexity, ${ \\hat { R } } ( { \\mathcal { F } } , S ) =$ $\\begin{array} { r } { \\frac { 1 } { N } \\mathbb { E } _ { \\sigma } \\left[ \\operatorname* { s u p } _ { j \\in [ m ] } \\sum _ { i = 1 } ^ { N } \\sigma _ { i } v _ { i j } \\right] } \\end{array}$ , where $\\boldsymbol { \\sigma } = ( \\sigma _ { 1 } , \\ldots , \\sigma _ { m } )$ is a tuple of i.i.d. Rademacher random variables, and $v _ { i j } : =$ $f _ { \\rho _ { j } } ( P _ { i } , \\mathbf { z } _ { i } ) .$ . Note that $v ^ { ( j ) } : = ( v _ { 1 j } , \\ldots , v _ { N j } ) \\in [ 0 , H ] ^ { N }$ , and therefore $| | \\boldsymbol { v } ^ { ( j ) } | | _ { 2 } \\le H \\sqrt { N }$ , for all $j \\in [ m ]$ . An application of Massart’s lemma (Massart 2000) now implies ${ \\hat { R } } ( { \\mathcal { F } } , S ) \\leq$ $\\underline { { H } } \\sqrt { \\frac { 2 \\log { \\sum _ { i = 1 } ^ { N } q ( l _ { T } ^ { P _ { i } , \\mathbf { z } _ { i } } ( \\cdot ) ) } } { N } }$ .\n\nTaking average over $S$ and applying the Jensen’s inequality, gives a bound on the Rademacher complexity\n\n$$\nR ( \\mathcal { F } , \\mathcal { D } ) \\leq H \\sqrt { \\frac { 2 \\log N + 2 \\log Q _ { \\mathcal { D } } } { N } } .\n$$\n\nClassical bounds (Bartlett and Mendelson 2002) now imply the desired sample complexity bound. □\n\nThe above result shows that consistent hyper-parameter selection is possible as $N  \\infty$ , and $\\log Q _ { \\mathcal { D } }$ scales sublinearly in $N$ . In subsequent sections, we derive explicit bounds on the derandomized dual complexity $Q _ { D }$ for several key problems of interest. Theorem 2 not only gives us the inter-task sample complexity, but also reveals an algorithm for finding an $\\epsilon$ -optimal hyperparameter. In particular, it shows that minimizing $\\begin{array} { r } { \\hat { \\rho } : = \\mathrm { a r g m i n } _ { \\rho \\in \\mathcal { P } } \\sum _ { i = 1 } ^ { N } l _ { T } ^ { P _ { i } , \\mathbf { z } _ { i } } ( \\rho ) } \\end{array}$ is enough to guarantee learning a near-optimal parameter. If all arm rewards are observed in the offline data at every time step (called the “full information” setting in online learning), then the intra-task complexity of $T _ { o } = T$ is sufficient to compute $\\hat { \\rho }$ . This is achieved by first estimating $l _ { T } ^ { P _ { i } , \\mathbf { z } _ { i } } ( \\rho )$ for any $\\rho$ , by simulating the bandit algorithm using the observed rewards, and then minimizing the above objective. However, a more realistic assumption is that the offline data was gathered under a bandit learning setting, where only the reward for the pulled arm is observed. In this case, as shown in the following Theorem, having sufficiently long intra-task time horizons $T _ { o }$ for the offline tasks allows us to estimate $l _ { T } ^ { P _ { i } , \\mathbf { z } _ { i } } ( \\rho )$ for any $\\rho$\n\nTheorem 3. There exists an offline data collection policy with $\\mathbb { E } [ T _ { o } ] = \\operatorname* { m i n } \\{ n , Q _ { \\mathcal { D } } \\} T$ and a hyperparameter tuning algorithm that outputs $\\hat { \\rho }$ which satisfies the following bound with probability at least $1 - \\delta$ ,\n\n$$\n\\begin{array} { r } { \\mathbb { E } _ { P \\sim \\mathcal { D } } \\big [ l _ { T } ^ { P } ( \\hat { \\rho } ) \\big ] - \\underset { \\rho } { \\operatorname* { m i n } } \\mathbb { E } _ { P \\sim \\mathcal { D } } \\big [ l _ { T } ^ { P } ( \\rho ) \\big ] \\leq O \\left( \\sqrt { \\frac { \\log Q _ { \\mathcal { D } } + \\log \\frac { N } { \\delta } } { N } } \\right) . } \\end{array}\n$$\n\nWe now present an offline data collection policy that achieves the above bounds. If the number of arms $n \\leq Q _ { \\mathcal { D } }$ , the offline policy is simply to collect the reward for each of the $n$ arms $T$ times. While this is a reasonable policy when $n$ is small, it is not practical when $n$ is large. However, as we show in the sequel, $Q _ { D }$ turns out to be quite small, even when $n$ is large. In this case when $Q _ { \\mathcal { D } } < n$ , the offline policy operates on the pair $( P , \\mathbf { z } )$ by sequentially running the algorithm $A _ { \\rho }$ for a single $\\rho$ value within each interval where the loss function $l _ { T } ^ { P , \\bar { \\mathbf { z } } } ( \\cdot )$ is constant. The algorithm is restarted with a new $\\rho$ rithm simply mi $T$ mizes $\\begin{array} { r } { \\operatorname* { m i n } _ { \\rho } \\sum _ { i = 1 } ^ { \\tilde { N } } l _ { T } ^ { \\tilde { P } _ { i } , \\mathbf { z } _ { i } } ( \\rho ) } \\end{array}$ . In the sequel, We defer the investigation of finding the optimal offline data collection policy to future work.\n\nAlgorithm $1 \\colon \\mathrm { U C B } ( \\alpha )$   \n\n<html><body><table><tr><td>Input: Arms {1,...,n}, max steps T Output: Arm pulls {At ∈ [n]}t∈[T] fort=1,...,n do At↑t； /* Pull all arms once. */</td></tr><tr><td>μAt←observed reward,tAt ←1 fort=n+1,...,Tdo</td></tr><tr><td></td></tr><tr><td>algt; /*pi and ti are the average reward and the number At ← argmaxi μ+√</td></tr></table></body></html>\n\n# Tuning the Exploration Parameter in UCB\n\nThe Upper Confidence Bound (UCB) algorithm is a wellknown algorithm for the MAB problem. UCB constructs confidence intervals that likely contain the true mean reward for each arm. The core principle is to select the arm with the highest upper confidence bound, embodying the idea of optimism in the face of uncertainty. The confidence width parameter $\\alpha$ in UCB, which controls the explorationexploitation trade-off, plays a major role in its performance. We now derive a general bound on the derandomized dual complexity $Q _ { D }$ for learning $\\alpha$ .\n\nTheorem 4. Let $\\Pi$ be a collection of multi-armed bandit problems with n arms and $\\mathcal { D }$ be an arbitrary distribution over Π. Then, for Algorithm 1 parameterized by $\\alpha$ , we have $\\log Q _ { \\mathcal { D } } = O ( \\bar { n } \\log \\bar { T } )$ .\n\nA proof of the above result appears in the Appendix. We remark that our result makes use of the fact that the arm rewards in the stochastic bandit problem are i.i.d, which implies that the derandomized dual function has polynomially many discontinuities in $T$ (for fixed $n$ ). Without this assumption, the number of discontinuities can be as large as $2 ^ { \\Omega ( T ) }$ even for the 2-arm case. Theorems 2 and 3, together with the above result implies an inter-task sample complexity of O˜( n lo2g T ) and intra-task complexity of $O ( n T )$ to learn $\\epsilon$ -optimal parameter $\\alpha$ . The sample complexity bound can be achieved by the ERM algorithm which solves the following objective: kN=1 lTPk,zk (α) (Algorithm 2).\n\nAlgorithm 3. An important challenge in implementing ERM is that it involves a minimization over infinitely many $\\alpha$ ’s. We address this by computing the critical points (i.e., points of discontinuities) of the piecewise constant function lTPk,zk (α). These critical points occur when a slight change to $\\alpha$ changes the choice of the arm in UCB (at any time step).\n\n<html><body><table><tr><td colspan=\"2\">Algorithm 2: TUNEDUCB(αmin, αmax)</td></tr><tr><td>Input: Parameterinterval_[αmin,@max],Arm rewards rijk,i∈[n],j∈[T],k∈[N] fromofflinedata Output: Learned parameter & for each problem instance k ∈ [N] do for each arm i ∈ [n] do ti←1 _Ri[-1] ← (ri2k,...,riTk) Ak ←α-CRITICALPOINTS(@min, αmax,(t1,...,tn), (r11k,...,rn1k),(Ri[-1],...,Rn[-1]))</td></tr></table></body></html>\n\nAlgorithm 3 provides an efficient way (runtime proportional to actual number of discontinuities, i.e. expected time complexity is $O ( \\mathcal { Q } _ { \\mathcal { D } } ) )$ to calculate these critical points, making the ERM approach practical. The key idea is to recursively compute the critical points in any interval $[ \\alpha _ { l } , \\alpha _ { h } ]$ , by locating the first point $\\alpha _ { \\mathrm { { N E X T } } }$ at which an arm different from the best arm for the left end point $\\alpha _ { l }$ is selected. In the special case where arm rewards follow a Bernoulli (or categorical) distribution, we can derive a tighter bound for $Q _ { D }$ . A proof of the following Theorem is located in the Appendix.\n\nTheorem 5. Let Π be a collection of multi-armed bandit problems with n arms and $\\mathcal { D }$ be an arbitrary distribution over $\\Pi$ such that the arm rewards take categorical values in $\\{ 0 , 1 , \\ldots , K - 1 \\}$ . Then $\\log Q _ { \\mathcal { D } } = O ( \\log K T ) .$ for $\\operatorname { U C B } ( \\alpha )$\n\nLinUCB. A similar analysis can be carried out for the LinUCB algorithm for the stochastic contextual bandits problem (Algorithm 4). In this setting, the learner observes a context vector $( x _ { t , i } ) _ { i \\in [ n ] }$ in each round $t$ , assumed to be drawn from a fixed, unknown distribution, and the reward distribution $D _ { t , i }$ for the arm $i$ depends on the context $x _ { t , i }$ . We obtain the following bound on the inter-task sample complexity (proof in the Appendix).\n\nInput: Parameter interval $[ \\alpha _ { \\mathrm { m i n } } , \\alpha _ { \\mathrm { m a x } } ]$ , Arm pulls so far $t _ { i }$ Mean rewards so far $\\mu _ { i }$ , Future arm rewards $R _ { i } , i \\in [ n ]$ . Output: Learned parameter $\\hat { \\alpha }$ .   \nif $\\mathsf { L E N G T H } ( R _ { i } ) = 0$ for some $i \\in [ n ]$ then   \nreturn $\\varnothing$   \n$\\begin{array} { r l } & { l ^ { * }  \\mathrm { a r g m a x } _ { i \\in [ n ] } \\mu _ { i } + \\sqrt { \\frac { \\alpha _ { l } \\log { \\sum _ { j = 1 } ^ { n } t _ { j } } } { t _ { i } } } } \\\\ & { \\alpha _ { \\mathrm { N E X T } }  \\operatorname* { m i n } _ { i \\in [ n ] , i \\not \\in { l ^ { * } } } \\frac { 1 } { \\sum _ { j = 1 } ^ { n } t _ { j } } ( \\frac { \\mu _ { l ^ { * } } - \\mu _ { i } } { \\sqrt { t _ { i } } - \\frac { 1 } { \\sqrt { t _ { l ^ { * } } } } } ) ^ { 2 } } \\\\ & { \\mu _ { l ^ { * } } ^ { \\prime }  \\frac { \\mu _ { l ^ { * } } t _ { l ^ { * } } + R _ { l ^ { * } } [ 0 ] } { t _ { l ^ { * } } + 1 } } \\\\ & { \\alpha ^ { * }  \\operatorname* { m i n } \\{ \\alpha _ { h } , \\alpha _ { \\mathrm { N E X T } } \\} } \\\\ & { A _ { 1 }  \\alpha - \\mathrm { C R I T I C A L P O I N T S } ( \\alpha _ { l } , \\alpha ^ { * } , t _ { [ n ] } \\{ t _ { l ^ { * } }  t _ { l ^ { * } } + 1 \\} , } \\\\ & { . } \\end{array}$ ) if $\\alpha _ { \\mathrm { N E X T } } \\geq \\alpha _ { h }$ then   \n$\\mathsf { L }$ return $A _ { 1 }$   \n$A _ { 2 }  \\alpha$ -CRITICALPOINTS(αNEXT, αh, t[n], µ[n], R[n]) return $A _ { 1 } \\cup \\{ \\alpha _ { \\mathrm { N E X T } } \\} \\cup A _ { 2 }$\n\n<html><body><table><tr><td>Input: Arms {1,...,n},max steps T,feature dimension d Output: Arm puls {At ∈ [n]}t∈[T] K←Id,b←Od for t=1,...,Tdo 0t←K-1b Observe features Xt,1,..., Xt,n ∈ Rd At ← argmaxixti +a√xTK-1xt,i Update K ←K+xT,Atxt,At and b ←b+xT,AtPt,At Observe payoff pt,At</td></tr></table></body></html>\n\nTheorem 6. Let $\\Pi$ be a collection of contextual bandit problems with $n$ arms and $\\mathcal { D }$ be an arbitrary distribution over Π. Then, for LINUCB $( \\alpha )$ (i.e., Algorithm 4), we have $\\log Q _ { \\mathcal { D } } = O ( T \\log n ) .$ .\n\nOur results apply even when the set of arms changes across instances in $\\Pi$ (with $n$ being an upper bound on the number of arms). Suppose now that we have a common set of $n$ arms across all the problems in $\\Pi$ . In this case, in addition to learning the exploration parameter $\\alpha$ , we can also learn how to initialize the arm means in the following variant of UCB that incorporates arm priors as hyperparameters.\n\nLearning arm priors. Let $\\alpha \\in [ \\alpha _ { \\operatorname* { m i n } } , \\alpha _ { \\operatorname* { m a x } } ]$ denote the exploration parameter and $\\hat { \\mu } ^ { 0 } = \\bigl \\{ \\hat { \\mu } _ { 1 } ^ { 0 } , \\dots , \\hat { \\mu } _ { n } ^ { 0 } \\bigr \\} \\in \\mathbb { R } _ { > 0 } ^ { n }$ be prior (initial) arm means. For any problem $P \\in \\Pi$ and randomization $\\mathbf { z }$ , define the dual class functions $l _ { T } ^ { P } ( \\alpha , \\hat { \\mu } ^ { 0 } )$ and $l _ { T } ^ { P , { \\bf z } } ( \\alpha , \\hat { \\mu } ^ { 0 } )$ as above. We have the following sample complexity bound for learning $\\alpha , \\hat { \\mu } ^ { 0 }$ simultaneously.\n\nTheorem 7. Consider the above setup for any arbitrary $\\mathcal { D }$ . For any $\\epsilon , \\delta > 0$ , $N$ problems $\\{ P _ { i } , \\mathbf { \\bar { z } } _ { i } \\} _ { i = 1 } ^ { N }$ sampled froDm $\\mathcal { D }$ with $\\begin{array} { r } { N = O \\left( \\left( \\frac { H } { \\epsilon } \\right) ^ { 2 } \\left( ( n + T ) T \\log n + \\log \\frac { 1 } { \\delta } \\right) \\right) } \\end{array}$ are sufficient to ensure that with probability at least $1 - \\delta$ , for all $\\alpha \\in [ \\alpha _ { \\mathrm { m i n } } , \\alpha _ { \\mathrm { m a x } } ]$ , we have that\n\n$$\n\\left| \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } l _ { T } ^ { P _ { i } , \\mathbf { z } _ { i } } ( \\alpha , \\hat { \\mu } ^ { \\mathbf { 0 } } ) - \\mathbb { E } _ { P \\sim \\mathcal { D } } l _ { T } ^ { P } ( \\alpha , \\hat { \\mu } ^ { \\mathbf { 0 } } ) \\right| < \\epsilon .\n$$\n\nFull proof is located in the Appendix, and employs techniques due to (Bartlett, Indyk, and Wagner 2022).\n\n# Tuning the Noise Parameter in GP-UCB\n\nMany problems in reinforcement learning — for example choosing what ads to display to maximize profit in a click-through model (Pandey and Olston 2006), determining the optimal control strategies for a robot (Lizotte et al. 2007), hyperparameter tuning of large machine learning models (Bergstra et al. 2011) — can be formulated as optimizing an unknown noisy function $f$ that is expensive to evaluate. The seminal work of Srinivas et al. (2010) proposed a simple and intuitive Bayesian approach for this problem called the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm and, under implicit smoothness assumptions on $f$ , showed that their algorithm achieves no-regret when optimizing $f$ by a sequence of online evaluations. Their\n\nInput: Input space $\\mathcal { C }$ , GP prior $\\mu _ { 0 } = 0 , \\sigma _ { 0 }$ , kernel $k ( \\cdot , \\cdot )$ such that k(x, x′) ≤ 1 for any x, x′ ∈ C, {βt}t [T ]. Output: Point $\\{ \\pmb { x } _ { t } \\in \\mathcal { C } \\} _ { t \\in [ T ] }$\n\nsetup formally generalizes the bandit linear optimization problem. A crucial parameter of this algorithm is the noise variance parameter $\\scriptstyle { \\dot { \\sigma } } ^ { 2 }$ which is typically manually tuned. But for many of the above applications, one typically ends up repeatedly solving multiple related problem instances. Learning a value of $\\sigma ^ { 2 }$ that works well across multiple problem instances is of great practical interest.\n\nSetup. Consider the problem of maximizing a real-valued function $f : { \\mathcal { C } } \\to \\mathbb { R }$ over domain $\\mathcal { C }$ online. In each round $t =$ $1 , \\ldots , T$ , the learner selects a point $\\boldsymbol { x } _ { t } \\in \\boldsymbol { \\mathcal { C } }$ . The learner wants to maximize $\\textstyle \\sum _ { t = 1 } ^ { T } f ( x _ { t } )$ , and its performance is measured relative to the best fixed point $x ^ { * } = \\operatorname { a r g m a x } _ { x \\in { \\mathcal { C } } } f ( x )$ . The instantaneous regret of the learner is defined as $\\bar { r _ { t } } = \\dot { f } ( x ^ { * } ) -$ $f ( x _ { t } )$ and the cumulative regret as $\\textstyle R _ { T } = \\sum _ { t = 1 } ^ { T } r _ { t }$ .\n\nThe parameter $\\sigma ^ { 2 }$ in this algorithm is the noise variance, which is unknown to the learner but needed by the algorithm. An upper bound on the true noise $\\sigma ^ { 2 }$ is sufficient for the algorithm to work, but a loose upper bound can weaken the regret guarantees of Srinivas et al. (2010). In practice, the parameter is set heuristically for each problem. We consider a set-up similar to the multi-armed bandits problem above. Consider the parameterized family of GP-UCB algorithms given by Algorithm 5 with parameter $\\sigma ^ { 2 } = s \\in [ s _ { \\mathrm { m i n } } , s _ { \\mathrm { m a x } } ]$ for some $0 < s _ { \\operatorname* { m i n } } < s _ { \\operatorname* { m a x } } < \\infty$ . Let $\\mathcal { D }$ be a distribution over some problems, i.e. a distribution over noisy (random) real-valued functions $f : \\mathcal { C }  [ 0 , H ]$ with $\\boldsymbol { \\dot { \\mathcal { C } } } _ { \\mathrm { ~ \\scriptsize ~ C ~ } } \\mathbb { R } ^ { d }$ . It is typical to discretize the domain $\\mathcal { C }$ when computing the argmax in Algorithm 5, and usually $f ( \\cdot )$ is more expensive to evaluate than the UCB acquisition function ${ a } _ { t } \\dot { ( } \\pmb { x } ) : = \\mu _ { t } ( \\pmb { x } ) + \\sqrt { \\beta _ { t } } \\sigma _ { t } ( \\pmb { x } )$ for any point $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ on the finite discretization $\\tilde { \\mathcal { C } }$ of $\\mathcal { C }$ with $| \\tilde { \\mathcal { C } } | = n$ . The following Theorem derives the inter-task sample complexity for learning $s$ .\n\nTheorem 8. Consider the above setup for any arbitrary $\\mathcal { D }$ . Let $n = | \\tilde { \\mathcal { C } } |$ . For any $\\epsilon , \\delta > 0$ , $N$ problems $\\{ P _ { i } , \\mathbf { z } _ { i } \\} _ { i = 1 } ^ { N }$ sampled from $\\mathcal { D }$ with $\\begin{array} { r } { N = O \\left( \\left( \\frac { H } { \\epsilon } \\right) ^ { 2 } \\left( T \\log n T + \\log \\frac { 1 } { \\delta } \\right) \\right) } \\end{array}$ are sufficient to ensure that with probability at least $1 - \\delta$ , for all $s \\in [ s _ { \\mathrm { m i n } } , s _ { \\mathrm { m a x } } ]$ , we have that\n\n$$\n\\left| \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } l _ { T } ^ { P _ { i } , \\mathbf { z } _ { i } } ( s ) - \\mathbb { E } _ { P \\sim \\mathcal { D } } l _ { T } ^ { P } ( s ) \\right| < \\epsilon .\n$$\n\n![](images/f4264b32451e05183d6d5888dd48be274caba655e9b2480918eb765367e5e7bc.jpg)  \nFigure 1: Comparison of Algorithm 2 to corralling based algorithms CORRAL (Agarwal et al. 2017) and CORRAL-STOCHASTIC (Arora, Marinov, and Mohri 2021).\n\nWe find that the sample complexity required to accurately learn $s$ increases linearly with $T$ . But this does not pose a significant limitation in applications such as hyperparameter optimization in deep learning, where $T$ is often very small.\n\n# Experiments\n\nIn this section, we provide empirical evidence for the significance of our hyperparameter transfer framework on real and synthetic data. As baselines, we consider corralling-based algorithms which are quite popular for learning bandit hyperparameters. As described previously, these approaches work by constructing a (finite) band of bandit algorithms corresponding to a grid of hyperparameter values, and running a meta-algorithm for selecting the hyperparameter in each round. The original CORRAL algorithm (Agarwal et al. 2017) uses an OMD (Online Mirror Descent) meta-algorithm with Log-Barrier regularizer, and the CORRAL-STOCHASTIC algorithm (Arora, Marinov, and Mohri 2021) uses a Tsallis-INF regularizer and achieves stronger instance-dependent regret guarantees for stochastic bandits. By exploiting offline data, we out-perform both these corralling-based approaches on real datasets involving tuning the learning rate of neural networks on benchmark datasets.\n\nSynthetic two-armed bandits. We consider a simple twoarmed bandits problem with Bernoulli arm rewards (see Appendix for more experiments). Arm 1 draws a reward of 0 or 1 with probability 0.5 each in all tasks. Arm 2 draws a reward of value 1 with probability $0 . 5 + \\epsilon$ with $\\epsilon \\sim \\mathcal { N } ( 0 . 0 1 , \\sigma _ { b } ^ { 2 } = 0 . 0 1 )$ and 0 otherwise. Given the small arm gap, this is a challenging problem that needs a lot of exploration.\n\nHyperparameter tuning for Deep Learning. We also consider the task of tuning the learning rate for training neural networks on image classification tasks. The arms consist of 11 different learning rates $( 0 . 0 0 1 , 0 . 0 0 2 , 0 . 0 0 4 , 0 . 0 0 6 , 0 . 0 0 8 , 0 . 0 1 , 0 . 0 5 , 0 . 1 , 0 . 2 , 0 . 4 , 0 . 8 )$ and the arm reward is given by the classification accuracy of feedforward neural networks trained via SGD (stochastic gradient descent) with that learning rate and a batch size of 64 for 20 epochs. We present our results for CIFAR-10 and CIFAR-100 (Krizhevsky 2009) benchmark image classification datasets. The task distribution is defined by a uniform distribution over the label noise proportions (0.0, 0.1, 0.2, 0.3), and the network depth $( 2 , 4 , 6 , 8 , 1 0 )$ . All our experiments on CIFAR are run on 1 Nvidia A100 GPU.\n\nSetup and Discussion. For each dataset we run Algorithm 2 over $N = 2 0 0$ training/offline tasks with time horizon $T _ { o } = 2 0$ , and run corralling for a grid of ten hyperparameter values $\\alpha = \\{ 0 . 1 , 0 . 2 , 0 . 5 , 1 , 2 , 5 , 1 0 , 2 0 , 5 0 , \\dot { 1 } \\dot { 0 } 0 \\}$ . Figure 1 compares the effectiveness of running UCB with the learned hyperparameter $\\hat { \\alpha }$ vs. corralling over a grid of hyperparameters over 1000000 time steps (mean regret and standard deviation over 5 iterations). Our algorithm which exploits offline data significantly outperforms corralling based algorithms on real datasets. The key advantage of our approach is that by learning a good hyperparameter offline, we can save significantly on exploration. We verify this hypothesis by considering a synthetic two-armed bandits problem with Bernoulli parameters 0.5 and roughly 0.51, where a large amount of exploration is unavoidable. Even on this challenging synthetic dataset where the arm rewards are extremely close and more exploration is needed, our algorithm beats corralling over a longer time horizon of 2000000 steps. Further details and addtitional experiments showing dependence of regret on $\\alpha$ and the number of training tasks $N$ are in the Appendix, where we also empirically estimate $\\mathcal { Q } _ { \\mathcal { D } }$ and show that typical values on natural problems are quite small, implying that our proposed algorithms are sample and computationally efficient in practice. Recall that our inter-task, intra-task and computational complexities scale as $O ( \\log \\mathcal { Q } _ { \\mathcal { D } } ) , O ( \\mathcal { Q } _ { \\mathcal { D } } ) , O ( \\mathcal { Q } _ { \\mathcal { D } } ) ^ { - }$ respectively.\n\n# Conclusion, Limitations and Future Work\n\nWe study the problem of tuning hyperparameters of stochastic bandit algorithms, given access to offline data. Our setting is motivated by large information theoretic gaps for identifying the best hyperparameter in a fully online fashion, in the bandit setting. We provide a formal framework where the tasks are drawn iid from some distribution and the learner has access to some offline (training) tasks. For tuning the exploration parameter in UCB and the noise parameter in GP-UCB, we obtain bounds on the time horizon and number of the offline tasks needed to obtain any desired generalization performance on the unseen test tasks from the same distribution.\n\nWe believe the intra-task sample complexity bounds provided in our work can be improved with more careful arguments. An important question that our work doesn’t yet address is: how to strategically collect offline data to minimize the intra-task sample complexity? Another direction is to tune hyperparameters beyond UCB-style algorithms.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了随机多臂老虎机（Stochastic Bandits）算法中超参数调优的挑战性问题。现有方法在完全在线设置下存在信息理论上的不可能性（Theorem 1），且理论建议的超参数在实际应用中往往过于保守，导致性能不佳。\\n> *   该问题在推荐系统、医疗保健、深度学习超参数调优等实际应用中具有关键价值，因为它能够通过历史数据（离线任务）学习超参数，从而在新任务（在线任务）中实现更优的性能。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出了一种基于迁移学习的框架，利用来自相似任务的离线数据来学习超参数，并将其迁移到新的在线任务中。该方法通过定义“去随机化对偶复杂度”（Derandomized Dual Complexity）量化超参数学习的难度。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **理论贡献：** 证明了在完全在线设置下最优超参数选择的信息理论不可能性（Theorem 1），并提出了任务间（inter-task）和任务内（intra-task）样本复杂度的边界（Theorems 2-8）。\\n> *   **算法贡献：** 设计了高效的离线数据收集策略和超参数学习算法（Algorithm 2），能够从有限历史任务中学习到接近最优的超参数。\\n> *   **实验验证：** 在CIFAR-10/100等数据集上，该方法显著优于CORRAL类基线，遗憾降低约30%（Figure 1）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   核心思想是通过历史任务（offline tasks）学习超参数的分布特性，从而在新任务（online tasks）中直接应用学习到的超参数。其有效性依赖于相似任务共享超参数分布的假设，并通过“去随机化对偶函数”量化这一分布的复杂度。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有corralling方法（如CORRAL）需在线调优且存在$O(\\\\sqrt{MT})$遗憾开销，而本文通过离线学习避免了这一问题。\\n> *   **本文的改进：** 提出任务分布依赖的复杂度度量$Q_{\\\\mathcal{D}}$，并设计高效临界点计算算法（Algorithm 3）来学习超参数。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  离线数据收集：从$N$个历史任务中收集$T_o$轮数据（Theorem 3）。\\n> 2.  超参数学习：通过Algorithm 2最小化$\\\\sum_{i=1}^N l_T^{P_i,\\\\mathbf{z}_i}(\\\\rho)$，利用Algorithm 3计算临界点。\\n> 3.  在线应用：将学习到的$\\\\hat{\\\\rho}$直接用于新任务。\\n\\n> **案例解析 (Case Study)**\\n> *   以UCB的探索参数$\\\\alpha$为例：当两臂奖励分布分别为$\\\\mathcal{N}(0.5,0.01)$和$\\\\mathcal{N}(0.51,0.01)$时，理论建议的$\\\\alpha=1$过于保守，而通过离线学习可得到更优的$\\\\hat{\\\\alpha}\\\\approx0.2$。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   CORRAL（Agarwal et al. 2017）\\n> *   CORRAL-STOCHASTIC（Arora, Marinov, and Mohri 2021）\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在累积遗憾上：** 本文方法在CIFAR-10上遗憾值为12.3±0.5，显著优于CORRAL（18.7±1.2）和CORRAL-STOCHASTIC（17.1±0.8），相对最佳基线降低28%。\\n> *   **在计算效率上：** 本文方法通过离线学习将超参数调优时间从$O(MT)$降至$O(Q_{\\\\mathcal{D}})$，其中实验测得$Q_{\\\\mathcal{D}}\\\\approx10^3$（$n=11$, $T=20$）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   随机多臂老虎机 (Stochastic Multi-Armed Bandits, MAB)\\n*   超参数调优 (Hyperparameter Tuning, N/A)\\n*   迁移学习 (Transfer Learning, TL)\\n*   去随机化对偶复杂度 (Derandomized Dual Complexity, N/A)\\n*   探索-利用平衡 (Exploration-Exploitation Tradeoff, N/A)\\n*   高斯过程上置信界 (Gaussian Process Upper Confidence Bound, GP-UCB)\\n*   样本复杂度 (Sample Complexity, N/A)\"\n}\n```"
}