{
    "source": "Semantic Scholar",
    "arxiv_id": "2502.09501",
    "link": "https://arxiv.org/abs/2502.09501",
    "pdf_link": "https://arxiv.org/pdf/2502.09501.pdf",
    "title": "Prior-Constrained Association Learning for Fine-Grained Generalized Category Discovery",
    "authors": [
        "Menglin Wang",
        "Zhun Zhong",
        "Xiaojin Gong"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2025-02-13",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Nanjing Normal University",
        "Hefei University of Technology",
        "Zhejiang University"
    ],
    "paper_content": "# Prior-Constrained Association Learning for Fine-Grained Generalized Category Discovery\n\nMenglin Wang1, Zhun Zhong2\\*, Xiaojin Gong3\n\n1School of Computer and Electronic Information, Nanjing Normal University, China 2School of Computer Science and Information Engineering, Hefei University of Technology, China 3College of Information Science and Electronic Engineering, Zhejiang University, China lynnwang $6 8 7 5 @$ gmail.com, zhunzhong $0 0 7 \\textcircled { a }$ gmail.com, gongxj $@$ zju.edu.cn\n\n# Abstract\n\nThis paper addresses generalized category discovery (GCD), the task of clustering unlabeled data from potentially known or unknown categories with the help of labeled instances from each known category. Compared to traditional semisupervised learning, GCD is more challenging because unlabeled data could be from novel categories not appearing in labeled data. Current state-of-the-art methods typically learn a parametric classifier assisted by self-distillation. While being effective, these methods do not make use of cross-instance similarity to discover class-specific semantics which are essential for representation learning and category discovery. In this paper, we revisit the association-based paradigm and propose a Prior-constrained Association Learning method to capture and learn the semantic relations within data. In particular, the labeled data from known categories provides a unique prior for the association of unlabeled data. Unlike previous methods that only adopts the prior as a pre or post-clustering refinement, we fully incorporate the prior into the association process, and let it constrain the association towards a reliable grouping outcome. The estimated semantic groups are utilized through non-parametric prototypical contrast to enhance the representation learning. A further combination of both parametric and non-parametric classification complements each other and leads to a model that outperforms existing methods by a significant margin. On multiple GCD benchmarks, we perform extensive experiments and validate the effectiveness of our proposed method.\n\nCode ‚Äî https://github.com/Terminator8758/PAL-GCD\n\n# 1 Introduction\n\nThe success of deep learning models has mostly been driven by the availability of large-scale annotated datasets. However, it is costly and inefficient to annotate all the data, especially as datasets grow larger in an open world. Semisupervised learning (Oliver et al. 2018) thus emerges to be a promising direction for learning with both labeled and unlabeled data. Typical semi-supervised learning assumes unlabeled data comes from known categories. Nevertheless, data from novel categories frequently appear in the real world, limiting the applicability of semi-supervised learning. As a relaxation to this assumption, generalized category discovery (GCD) has been proposed (Vaze et al. 2022), allowing unlabeled data to belong to both known and unknown categories. The target of GCD is to recognize images from both old and new categories by learning a model that clusters unlabeled images into distinct semantic groups, making it more practical for discovering novel categories with the assistance of old category data.\n\nCurrent methods explore the GCD task from two perspectives, representation learning and parametric classification. The initial GCD paper (Vaze et al. 2022) uses selfsupervised contrastive learning to learn robust representation, removing the need for parametric classifier. The problem is that it overlooks the intrinsic semantic relations among samples, causing the learned representation to be less discriminative. Indeed, samples belonging to the same potential category call for attraction instead of general repulsion. Later methods (Pu, Zhong, and Sebe 2023; Zhao, Wen, and Han 2023; Zhang et al. 2023) exploit cross-instance similarity relations to discover semantic groups or $k$ -NN positives, and such grouping result guides the contrastive learning towards finding a discriminative feature space. However, the quality of grouping is determined by the design of data association strategy, and severe noise can be incorporated if the association design is not reliable enough. As an alternative, SimGCD (Wen, Zhao, and Qi 2023) revives parametric classifier through self-distillation learning and entropy regularization, which has become a popular baseline. But as the parametric classifier is implicitly regularized, its weights may not capture class-specific semantics well. Also, selfdistillation alone may not provide strong enough supervision for the distinction of different classes.\n\nIn this paper, we re-examine the previous associationbased GCD methods, and identify their weakness in the association design that makes their performance inferior, especially on fine-grained datasets. The aim is that through better association design, more accurate instance groupings can be estimated to facilitate model representation learning. Specifically, Figure 1 provides an intuitive example to illustrate the limitations of previous association designs and our motivation. As shown in the figure, some methods utilize the labeled prior for pre-association refinement, i.e. masking out the distance of images from different known categories. However during the association, the masked image pair could be re-connected by indirect association of other images. Such association will generate inaccurate groupings of labeled and unlabeled instances. Even if a postassociation refinement removes the connection of labeled images, the unlabeled images‚Äô association is still kept, causing undesirable groupings to appear.\n\n![](images/e76c768456b2d47de94cd837cacf84e382bbc44bf3f71643510fbb473c759881.jpg)  \nFigure 1: An illustration of implicit false association, and how our proposed association avoids it. Arrowed line represents masking out the distance, solid line indicates direct association of two instances, and dashed line denotes indirect association. Red line indicates false association. Images with colored edges indicate they are from labeled subset.\n\nIn light of this, we aim to circumvent such possibly emerging situations by fully incorporating the labeled prior into the association process. As Figure 1 shows, our association keeps track of the updated image groups. When a new image pair appears, we not only examine the image pair, but also check their corresponding groups to determine if the image pair should be associated. In the example, the two groups contain known yet label-conflicting instances, therefore the image pair will not be associated. In this way, both direct and indirect false association can be avoided. By repeating the instance-wise association steps, more faithful instance groupings can be obtained.\n\nWith the groupings generated by association, we adopt non-parametric prototypical contrastive learning to strengthen the representation. The association based nonparametric classification serves as a stand-alone and goodperforming framework. Specially, the prior knowledge of ground truth class number is not required, which makes the framework flexible in practical applications. Additionally, we also propose to optionally perform association on a subset of all samples to improve the scaling of the association to more data scenarios. We explore the possibility of combining parametric classifier with the proposed association based parametric classifier, which is proven effective when the number of classes is known. Through a two-stage training pipeline, we exploit their complementarity and unify them in one framework to mutually boost each other. This unified model achieves strong performance on multiple datasets.\n\nOur main contributions are summarized as follows:\n\n‚Ä¢ We propose a simple yet effective method for finegrained GCD. By re-examining the role of association, a novel prior-constrained association algorithm tailored for GCD task is proposed. ‚Ä¢ With the assistance of proposed association, we unify non-parametric and parametric classification under one single framework, where representation learning and\n\nclassifier learning can mutually boost each other. ‚Ä¢ Extensive experiments on both fine-grained and generic datasets demonstrate the effectiveness of the proposed method. Compared to previous best method, our method improve the accuracy by $4 . 4 \\%$ and $1 5 . 3 \\%$ on CUB and Stanford Cars respectively.\n\n# 2 Related Work\n\nGeneralized Category Discovery (GCD) draws similarity with novel category discovery (NCD) in both containing labeled and unlabeled images and aiming to discover novel categories in the unlabeled set. Initial GCD method (Vaze et al. 2022) learns representations by self-supervised contrastive learning on all data, along with supervised contrastive learning on labeled subset. SimGCD (Wen, Zhao, and Qi 2023) constructs an effective baseline using parametric classifier. A later variant $\\mu \\mathrm { G C D }$ (Vaze, Vedaldi, and Zisserman 2024) improves SimGCD by using a teacher network to provide supervision for self-augmented image pairs. More recently, SPTNet (Wang, Vaze, and Han 2024) learns spatial prompts as an alternative to adapt data for better alignment with the model. DCCL $\\mathrm { P u . }$ , Zhong, and Sebe 2023) proposes to mine sample relations by generating dynamic conceptions using improved Infomap clustering (Rosvall and Bergstrom 2008), followed by conception and instance-level contrastive learning. Similarly, GPC (Zhao, Wen, and Han 2023) also estimates prototypes by Gaussian mixture model and a split-and-merge to take labeled instances into account. PromptCAL (Zhang et al. 2023) improves the ViT backbone by learning auxiliary prompts, as well as affinity propagation on KNN graph to estimate instance relation. Although labeled data is exploited to assist clustering in these methods, it is often taken as a pre- or post-clustering refinement. As such, the potential benefit of labeled instances are not fully exploited. As a comparison, we fully incorporate the labeled data prior during every step of the association process, empowering reliable association of unlabeled data by taking advantage of the labeled instances as bridges.\n\nPrototypical Contrastive Learning (PCL). In recent years, contrastive learning (Gutmann and Hyva¬®rinen 2010) has proven as an effective technique for self-supervised learning (Wu et al. 2018; He et al. 2020; Chen et al. 2020; Li et al.\n\n2021) and other settings (Khosla et al. 2020; Wang et al. 2021a; Zhao, Wen, and Han 2023). In particular, prototypical contrastive learning compares instances with a set of prototypes encoding class-specific semantic structure, leading to discriminative embedding space. As such, many vision tasks have exploited PCL for method design. ProtoNCE (Li et al. 2021) combines instance-wise contrastive learning and multi-grained PCL for transfer learning. (Ge et al. 2020; Wang et al. 2021b) adopt iterative clustering based PCL for object re-ID. A few methods $\\mathrm { P u }$ , Zhong, and Sebe 2023; Zhao, Wen, and Han 2023) in GCD have also considered PCL to learn discriminative representation. The critical issue for prototypical contrast is how to obtain representative prototypes, which then comes down to designing effective association strategy. Our method also adopts PCL, however, our better utilization of prior and design of semi-supervised association lead to more reliable prototypes, which in turn facilitates learning better representation.\n\nData Clustering and Association. Clustering has long been used as a way to discover potential semantic groups within the data. Unsupervised clustering methods like KMeans (Hartigan and Wong 1979), DBSCAN (Ester et al. 1996) and hierarchical clustering (Johnson 1967; Murtagh and Contreras 2012) are widely used in many applications (Ge et al. 2020; Wang et al. 2022; Pu, Zhong, and Sebe 2023). Semi-supervised clustering is also studied in some works (Bair 2013; Bilenko, Basu, and Mooney 2004). Basu et al. (Basu 2002) propose constrained K-Means by enforcing that labeled instances are assigned to their own cluster during K-Means iteration. COP-Kmeans (Wagstaff et al. 2001) modifies K-Means to make sure no constraints are violated when assigning instances. Constrained DBSCAN (Ruiz, Spiliopoulou, and Menasalvas 2010) and hierarchical clustering (Davidson and Ravi 2005) are also considered. Metric-based methods (Yin et al. 2010; Klein, Kamvar, and Manning 2002; Xing et al. 2002; Lange et al. 2005; Pu, Zhong, and Sebe 2023; Zhang et al. 2023) modify the pairwise distance such that two instances with a ‚Äùmust-link‚Äù constraint have a lower distance, and those with a ‚Äùcannotlink‚Äù constraint have a larger distance. Our proposed association is also constraint-based, but the constraints are enforced during a threshold-based group merging process, during which new categories are allowed to be discovered.\n\n# 3 Methodology\n\n# 3.1 Overview\n\nUnder Generalized Category Discovery setting, we consider the problem of clustering images in a dataset among which a subset has known class labels. Assume the dataset $\\mathcal { D }$ is comprised of two parts $\\mathcal { D } \\mathcal { L } = \\{ ( x _ { i } , y _ { i } ) \\} _ { i = 1 } ^ { N } \\in \\mathcal { X } \\times \\mathcal { Y } _ { \\mathcal { L } }$ and $\\mathcal { D } _ { \\mathcal { U } } = \\{ ( x _ { i } , y _ { i } \\} _ { i = 1 } ^ { M } \\in \\mathcal { X } \\times \\dot { \\mathcal { V } } _ { \\mathcal { U } }$ , where $\\mathcal { D } _ { \\mathcal { L } }$ is the laLbeled subset of $N$ images whose labels $y _ { \\mathcal { L } }$ are known, and $\\mathcal { D } _ { \\mathcal { U } }$ is the unlabeled subset of $M$ images whose labels $y _ { \\mathcal { U } }$ are not known. Image labels in $\\mathcal { D } _ { \\mathcal { U } }$ is a superset of image labels in $\\mathcal { D } _ { \\mathcal { L } }$ , i.e. $y _ { \\mathcal { L } } \\subset \\mathcal { P } _ { \\mathcal { U } }$ . Given dataset $\\mathcal { D }$ , the aim is to correctly recognize and cluster the images in $\\mathcal { D } _ { \\mathcal { U } }$ containing known and unknown categories. To address the GCD task, we seek to improve the representation learning by estimating reliable semantic groups as the guidance. To this end, we incorporate the labeled data prior into the association process, and design a prior-constrained greedy association algorithm. Such association generates faithful instance groups as well as class-representative proxies to guide the model representation learning. Finally, to exploit the synergy of nonparametric (prototypical contrastive learning) and parametric classification, we unify them in one framework by joint two-stage optimization.\n\nTable 1: Comparison of class number estimated by preclustering refinement $^ +$ DBSCAN, v.s. ground truth class number in labeled subset of training images.   \n\n<html><body><table><tr><td>Dataset</td><td>CUB</td><td>StanfordCars</td><td>Aircraft</td><td>Herbarium19</td></tr><tr><td>Estimated class number</td><td>76</td><td>74</td><td>27</td><td>228</td></tr><tr><td>G.T. class number</td><td>100</td><td>98</td><td>50</td><td>342</td></tr></table></body></html>\n\n# 3.2 Limitation of Previous Methods\n\nThere have been some recent attempts (Pu, Zhong, and Sebe 2023; Zhang et al. 2023; Kim et al. 2023; Zhao, Wen, and Han 2023) at estimating the semantic structure by semi-supervised clustering or association, so as to provide stronger and explicit supervision to unlabeled data. Albeit with acceptable performance, we take a closer look at the current association designs in GCD and discover that there exists missing clues and the association can be further optimized with the given labeled data prior.\n\nIn GCD task, it is natural to utilize the labeled data from known categories to assist the association of unlabeled instances. Current association-based methods usually adopt the labeled data as a pre- or post-clustering refinement. For pre-clustering refinement, after computing the pairwise distance of instances, those between known yet different categories can be directly masked as disconnected. Then the refined distance matrix would be input to a standard clustering algorithm (Rosvall and Bergstrom 2008; Ester et al. 1996). For post-clustering refinement, after unsupervised clustering, the associations between known different categories are removed as a refinement.\n\nHowever, both pre- and post-clustering refinement neglect the underlying association process. As Figure 1 shows, inter-class false association can still occur even after simple pre/post-refinement. Simply adopting the standard unsupervised clustering not only fails to address this, but also keeps the incorrect association of unlabeled instances untreated. As an example to verify the problem with preclustering refinement, we use the pre-refined inter-instance distance matrix as input to DBSCAN clustering (Ester et al. 1996), and compare the predicted pseudo class number of labeled subset with the ground truth . In Table 1, we observe that the clustering predicts much less class number than ground truth, indicating that instances from different labeled classes have been falsely merged. This demonstrates that simply masking the distance of labeled classes during pre-association is insufficient, as these masked instances can still be mis-connected during association.\n\n![](images/dd47c679acf38579f1a00e5c73269f7bf39ae4e56e8e1fb9d3cd9fcf75e2f140.jpg)  \nFigure 2: An overview of our method. Non-parametric classification is in the form of prototypical contrastive learning, with prototypes obtained by the proposed semi-supervised association. Parametric classification follows the implementation of SimGCD (Wen, Zhao, and Qi 2023).\n\n# 3.3 Prior-constrained Greedy Association\n\nIn an effort to fix the limitations of the existing association/clustering in GCD, we propose our prior-constrained greedy association algorithm. The initial motivation is that each group should only contain at most one old class. To ensure the constraint is satisfied, we propose to attend to each step of the association. While associating instance pairs in a distance-ascending order, each step goes through a labeled prior based validity check to make sure one group contains no more than one old class. After such association, the generated instance groups are guaranteed to adhere to the ground truth of labeled subset. An overall pipeline of our association process is shown in the upper part of Figure 2.\n\n‚Ä¢ Hybrid feature extraction. First, we extract feature encodings of all training samples using the backbone network $f$ . For the labeled instances, per-category mean feature is calculated as their class representative proxy. Then, our prior-constrained greedy association is performed among the initial known-class proxies and the rest unlabeled instances. ‚Ä¢ Pairwise distance computation.With the hybrid features of initial proxies and unlabeled instances, we compute their pairwise Jaccard distance (Zhong, Zheng, and Li 2017), and sort the distances in ascending order. Only pairs whose distance fall below a given threshold $\\epsilon$ are kept. Those kept pairs are regarded as the association candidates $P = \\{ ( j _ { 1 } , j _ { 2 } ) , ( j _ { 3 } , j _ { 4 } ) , . . . \\}$ . ‚Ä¢ Greedy association with constraint. In order to incorporate the labeled data prior to constrain the association, we unfold the association in a pair-wise manner. The pseudo algorithm for the association is presented in Alg. 1 of Appendix. Starting from the most similar instanceto-instance (or instance-to-proxy) pair, we obtain the initial grouping $G r p = \\{ 0 : \\overline { { ( j _ { 1 } , j _ { 2 } ) } } \\}$ . For the next candidate pair $( j _ { 3 } , j _ { 4 } )$ , we check for conflicts if this candidate pair is to be associated; If the group of instance/proxy $j _ { 3 }$ contains known category different from the group of instance/proxy $j _ { 4 }$ , this candidate pair will not be associated. The conflict check is performed for every candidate pair in $P$ . In this way, every step of association updates the grouping result while still fully respects the ground truth class relations of labeled data.\n\nScaling by subset association. The proposed greedy association works by gradually associating reliable instances into cluster groups. This makes it well-suited for fine-grained datasets like Semantic Shift Benchmark (Vaze et al. 2021), where each category has a moderate number of images. When scaling to large-scale datasets or scenarios with many images per category, we suggest to perform the association on a randomly sampled subset of unlabeled images. Specifically, at each time of association, a fixed ratio of unlabeled instances are randomly sampled from all unlabeled instances, and then associated along with labeled known proxies. Such subset association enjoys two benefits: First, it reduces the number of per-category images for more effective association. Second, for large-scale datasets, the computational cost of association (including pairwise distance computation and greedy association) is significantly reduced.\n\n# 3.4 Non-parametric Classification\n\nWith the semantic groups predicted by the proposed association, we construct a proxy memory $\\mathcal { K } \\in \\ \\dot { \\mathcal { R } } ^ { C \\cdot d }$ representing the feature centroid of each semantic group. At the same time, instance-to-group relation is also estimated from the association. To learn the potential semantic structure, we adopt prototypical contrastive learning paradigm (Wu et al. 2018) taking the proxy memory as the prototypes. Given an image $x _ { i }$ , its $d$ -dimensional feature ${ \\bar { f } } ( x _ { i } )$ is extracted through the backbone network $f$ . After $l _ { 2 }$ -normalization, the image feature is contrasted with the proxy feature memory, and the prototypical contrastive loss is computed as:\n\n$$\n\\mathcal { L } _ { n p a } = - \\frac { 1 } { B } \\sum _ { i = 1 } ^ { B } \\log \\frac { e x p ( \\mathcal { K } [ y _ { i } ] ^ { T } f ( x _ { i } ) / \\tau ) } { \\sum _ { j = 0 } ^ { C - 1 } e x p ( \\mathcal { K } [ j ] ^ { T } f ( x _ { i } ) / \\tau ) } ,\n$$\n\nwhere $\\kappa [ j ]$ is the $j$ -th entry of the memory, $\\tau$ is a temperature factor, $B$ is batch size, and $C$ is the number of proxies in $\\kappa$ . The contrastive loss defined in Eq. (1) pulls an instance close to the centroid of its group while pushes it away from the centroids of all other groups. This can be seen as non-parametric classification with the external proxy memory serving as the classifier. Feature representation gets enhanced through optimizing the similarity relation between images and the representative proxies.\n\nAfter each batch forward, the proxy memory features are updated in a moving-average manner (Xiao et al. 2017) using the online batch image features:\n\n$$\n{ \\mathcal K } [ \\tilde { y } _ { i } ] \\gets \\mu { \\mathcal K } [ \\tilde { y } _ { i } ] + ( 1 - \\mu ) f _ { \\theta } ( x _ { i } ) ,\n$$\n\nwhere $\\mu \\in [ 0 , 1 ]$ is an updating rate. To promote the mutualbeneficial effect of representation learning and data association, the two processes are iteratively performed during training. Better representation improves the quality of data association, and the improved association in turn facilitates stronger representation learning.\n\nAs shown in our experiments, prototypical contrastive learning driven by the proposed data association can serve as an effective stand-alone GCD framework. One advantage of it is that it does not require the prior knowledge of the ground truth class number, offering more flexibility to application. Nevertheless, when the ground truth class number is known, parametric classification (Wen, Zhao, and Qi 2023) can be integrated as a useful complement to the nonparametric contrastive framework. Next, we briefly introduce the parametric classification, and how the two learning mechanisms can be integrated into one unified framework that produces more powerful model.\n\n# 3.5 Joint Non-param. and Param. Classification\n\nParametric Classification. Following the good practice of semi-supervised learning, a representation parametric classification method SimGCD (Wen, Zhao, and Qi 2023) performs self-distillation by mining the prediction consistency between augmented views, using the sharpened prediction of one augmented view as the supervision for another view. The total loss $\\mathcal { L } _ { s i m g c d }$ includes the cross entropy loss on the labeled data, self-distillation and entropy maximization on both labeled and unlabeled data, as well as batch supervised/self-supervised contrast (Vaze et al. 2022).\n\nOur association based non-parametric classifier directly learns the globally estimated semantic groups, and presents itself as a complement to parametric classifier. However, combining them in one framework is not so straightforward, as the two types of classifiers are learned with different sampling strategy and at different learning paces. Parametric classifier typically learns at a slower pace, while nonparametric classifier learns faster due to the characteristic of non-parametric classification (Xiao et al. 2017).\n\nTo improve the effectiveness of joint learning the two classifiers, we propose a two-stage training strategy. In the first ‚Äúwarm-up‚Äù stage, we only train the backbone network with non-parametric classification, to better prepare the network for joint training. In the second stage, the parametric classifier is added, and a weight $\\beta = 0 . 1$ is assigned to the non-parametric classifier loss to balance the learning. The training objective of the second stage is the weighted sum of both classifier loss, i.e. $\\mathcal { L } = \\mathcal { L } _ { s i m g c d } + \\beta \\mathcal { L } _ { n p a }$ .\n\nDiscussion. Association based learning has been previously explored for GCD. For example, DCCL $\\mathrm { P u }$ , Zhong, and Sebe 2023), GPC (Zhao, Wen, and Han 2023) and PromptCAL (Zhang et al. 2023) all use inter-instance similarity for grouping or pairwise labeling. And prototypical contrastive learning is also adopted by DCCL, GPC and OpenCon (Sun and Li 2022) to enhance representation learning. Our method differs from them in two ways: First, we leverage the labeled data prior in GCD task, and let the prior constrain the data association process, ensuring that labeled instances are grouped respecting their label prior. This enables our association to generate much more reliable instance grouping result. Second, we show that nonparametric classification can be effectively combined with parametric classifier to further advance the performance in discovering novel categories.\n\n# 4 Experiments\n\n# 4.1 Experimental Setup\n\nDatasets. We perform experiments on four fine-grained datasets and two generic datasets. Fine-grained datasets include the Semantic Shift Benchmark (Vaze et al. 2021) (CUB-200 (Wah et al. 2011), StanfordCars (Krause et al. 2013), Aircraft (Maji et al. 2013)), and one long-tailed dataset Herbarium19 (Tan et al. 2019). General datasets include Cifar100 (Krizhevsky, Hinton et al. 2009) and ImageNet-100 (Deng et al. 2009). Compared to generic recognition, fine-grained datasets are more challenging due to small inter-class variation, and reflect many real-world cases in visual recognition system. Following common settings (Vaze et al. 2022), a subset of all train classes is sampled as the old classes, the rest are new classes. $5 0 \\%$ of the images from known classes are used to construct the labeled subset $\\mathcal { D } _ { \\mathcal { L } }$ , and the rest images constitute $\\mathcal { D } _ { \\mathcal { U } }$ .\n\nEvaluation metric. In accordance with standard practice (Vaze et al. 2022), clustering accuracy (ACC) is utilized to evaluate the model performance. During evaluation, the predicted label $\\hat { y }$ is compared with the ground truth label $y ^ { \\ast }$ , and ACC is calculated as $\\begin{array} { r } { A C C = \\frac { 1 } { M } \\sum _ { i = 1 } ^ { M } \\mathbb { 1 } ( y _ { i } ^ { * } = p ( \\hat { y } _ { i } ) ) } \\end{array}$ where $M = | \\mathcal { D } _ { \\mathcal { U } } |$ , and $p$ is the best permutation of $\\hat { y }$ to match the ground truth $y ^ { * }$ .\n\nImplementation Details. We adopt ViT-B/16 (Dosovitskiy et al. 2021) pre-trained on DINO (Caron et al. 2021) as the backbone network. Following GCD (Vaze et al. 2022), only the last block of the backbone is fine-tuned. Batch size is 128. Llearning rate is 0.01 for the first training stage decayed with a cosine annealed schedule, and 0.1 for the second stage of joint training. More implementation details and dataset statistics can be found in Appendix.\n\n<html><body><table><tr><td rowspan=\"2\">Methods</td><td colspan=\"3\">CUB</td><td colspan=\"3\">Stanford Cars</td><td colspan=\"3\">Aircraft</td><td colspan=\"3\">Herbarium19</td><td colspan=\"3\">Cifar100</td><td colspan=\"3\">ImageNet-100</td></tr><tr><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td></tr><tr><td colspan=\"10\">Ground truth number of classes known</td><td colspan=\"3\"></td><td colspan=\"3\"></td><td colspan=\"3\"></td></tr><tr><td>k-means (1967)</td><td>34.3</td><td>38.9</td><td>32.1</td><td>12.8</td><td>10.6</td><td>13.8</td><td>16.0</td><td>14.4</td><td>16.8</td><td>13.0</td><td>12.2</td><td>13.4</td><td>52.0</td><td>52.2</td><td>50.8</td><td>72.7</td><td>75.5</td><td>71.3</td></tr><tr><td>RankStats+ (2021)</td><td>33.3</td><td>51.6</td><td>24.2</td><td>28.3</td><td>61.8</td><td>12.1</td><td>26.9</td><td>36.4</td><td>22.2</td><td>27.9</td><td>55.8</td><td>12.8</td><td>58.2</td><td>77.6</td><td>19.3</td><td>37.1</td><td>61.6</td><td>24.8</td></tr><tr><td>UNO+ (Fini et al. 2021)</td><td>35.1</td><td>49.0</td><td>28.1</td><td>35.5</td><td>70.5</td><td>18.6</td><td>40.3</td><td>56.4</td><td>32.2</td><td>28.3</td><td>53.7</td><td>14.7</td><td>69.5</td><td>80.6</td><td>47.2</td><td>70.3</td><td>95.0</td><td>57.9</td></tr><tr><td>GPC (2023)</td><td>52.0</td><td>55.5</td><td>47.5</td><td>38.2</td><td>58.9</td><td>27.4</td><td>43.3</td><td>40.7</td><td>44.8</td><td></td><td></td><td>-</td><td>77.9</td><td>85.0</td><td>63.0</td><td>76.9</td><td>94.3</td><td>71.0</td></tr><tr><td>GCD (Vaze et al. 2022)</td><td>51.3</td><td>56.6</td><td>48.7</td><td>39.0</td><td>57.6</td><td>29.9</td><td>45.0</td><td>41.1</td><td>46.9</td><td>35.4</td><td>51.0</td><td>27.0</td><td>73.0</td><td>76.2</td><td>66.5</td><td>74.1</td><td>89.8</td><td>66.3</td></tr><tr><td></td><td></td><td>543</td><td>510</td><td>40.5</td><td>588</td><td>312</td><td>47.7</td><td>44.4</td><td>49.4</td><td></td><td></td><td></td><td></td><td>81.8</td><td>60.3</td><td>77.6</td><td></td><td>69.2</td></tr><tr><td>XCon (2i et al 2022)</td><td>521</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>743</td><td></td><td></td><td></td><td>93.5</td><td></td></tr><tr><td>PromptCAL (2023)</td><td>62.9</td><td>64.4</td><td>62.1</td><td>50.2</td><td>70.1</td><td>40.6</td><td>52.2</td><td>52.2</td><td>52.3</td><td>37.0</td><td>52.0</td><td>28.9</td><td>81.2</td><td>84.2</td><td>75.3</td><td>83.1</td><td>92.7</td><td>78.3</td></tr><tr><td>PIM (2023)</td><td>62.7</td><td>75.7</td><td>56.2</td><td>43.1</td><td>66.9</td><td>31.6</td><td>-</td><td></td><td></td><td>42.3</td><td>56.1</td><td>34.8</td><td>78.3</td><td>84.2</td><td>66.5</td><td>83.1</td><td>95.3</td><td>77.0</td></tr><tr><td>ŒºGCD (2024)</td><td>65.7</td><td>68.0</td><td>64.6</td><td>56.5</td><td>68.1</td><td>50.9</td><td>53.8</td><td>55.4</td><td>53.0</td><td>45.8</td><td>61.9</td><td>37.2</td><td></td><td>-</td><td>-</td><td></td><td></td><td></td></tr><tr><td>CMS (2024)</td><td>68.2</td><td>76.5</td><td>64.0</td><td>56.9</td><td>76.1</td><td>47.6</td><td>56.0</td><td>63.4</td><td>52.3</td><td>36.4</td><td>54.9</td><td>26.4</td><td>82.3</td><td>85.7</td><td>75.5</td><td>84.7</td><td>95.6</td><td>79.2</td></tr><tr><td>SimGCD (2023)</td><td>60.3</td><td>65.6</td><td>57.7</td><td>53.8</td><td>71.9</td><td>45.0</td><td>54.2</td><td>59.1</td><td>51.8</td><td>44.0</td><td>58.0</td><td>36.4</td><td>80.1</td><td>81.2</td><td>77.8</td><td>83.0</td><td>93.1</td><td>77.9</td></tr><tr><td>SimGCD‚Ä† (2023)</td><td>60.8</td><td>65.2</td><td>58.5</td><td>53.8</td><td>70.8</td><td>45.6</td><td>52.3</td><td>59.8</td><td>48.6</td><td>44.5</td><td>57.9</td><td>37.3</td><td>79.4</td><td>82.2</td><td>73.9</td><td>85.0</td><td>94.2</td><td>80.3</td></tr><tr><td>Ours (param. eval)</td><td>67.6</td><td>75.5</td><td>63.7</td><td>66.7</td><td>79.1</td><td>60.7</td><td>59.5</td><td>67.2</td><td>55.6</td><td>47.6</td><td>58.6</td><td>41.7</td><td>82.0</td><td>82.4</td><td>81.2</td><td>86.3</td><td>93.0</td><td>83.0</td></tr><tr><td>Ours (nonparam. eval)</td><td>72.6</td><td>75.2</td><td>71.2</td><td>72.2</td><td>83.4</td><td>66.8</td><td>58.8</td><td>64.5</td><td>56.0</td><td>46.8</td><td>57.0</td><td>41.4</td><td>78.5</td><td>78.9</td><td>77.8</td><td>83.0</td><td>93.1</td><td>78.0</td></tr><tr><td colspan=\"10\">Ground truth number of classes unknown</td><td colspan=\"7\"></td><td colspan=\"3\"></td></tr><tr><td>GCD (Vaze et al. 2022)</td><td>51.1</td><td>56.4</td><td>48.4</td><td>39.1</td><td>58.6</td><td>29.7</td><td></td><td></td><td></td><td>37.2</td><td>51.7</td><td>29.4</td><td>70.8</td><td>77.6</td><td>57.0</td><td>77.9</td><td></td><td>71.3</td></tr><tr><td>GPC (2023)</td><td>52.0</td><td>55.5</td><td>47.5</td><td>38.2</td><td>58.9</td><td>27.4</td><td>43.3</td><td>40.7</td><td>44.8</td><td>36.5</td><td>51.7</td><td>27.9</td><td>75.4</td><td>84.6</td><td>60.1</td><td>75.3</td><td>91.1 93.4</td><td>66.7</td></tr><tr><td>PIM (2023)</td><td>62.0</td><td>75.7</td><td>55.1</td><td>42.4</td><td>65.3</td><td>31.3</td><td>-</td><td></td><td></td><td>42.0</td><td>55.5</td><td>34.7</td><td>75.6</td><td>81.6</td><td>63.6</td><td>83.0</td><td>95.3</td><td>76.9</td></tr><tr><td>CMS (2024)</td><td>64.4</td><td>68.2</td><td>62.4</td><td>51.7</td><td>68.9</td><td>43.4</td><td>55.2</td><td>60.6</td><td>52.4</td><td>37.4</td><td>56.5</td><td>27.1</td><td>79.6</td><td>83.2</td><td>72.3</td><td>81.3 94.6</td><td>95.6</td><td>74.2 75.2</td></tr><tr><td>Ours *</td><td>69.9</td><td>68.0</td><td>70.9</td><td>70.5</td><td>77.7</td><td>67.0</td><td>56.2</td><td>55.3</td><td>56.7</td><td>44.8</td><td>48.5</td><td>42.8</td><td>77.3</td><td>78.9</td><td>74.1</td></table></body></html>\n\nTable 2: Performance comparison with SoTA methods. Ours (param. eval) and Ours (nonparam. eval): Our full model evaluated with pseudo label predicted by parametric classification logits, or by the proposed association. Ours‚àó: Our model trained with only non-parametric loss. ‚Ä† denotes reproduced results. Best and second best results are marked by Bold and underline.   \n\n<html><body><table><tr><td></td><td colspan=\"3\">Training</td><td>Evaluation</td><td colspan=\"3\">CUB</td><td colspan=\"3\">Stanford Cars</td><td colspan=\"3\">Aircraft</td><td colspan=\"3\">Herbarium19</td></tr><tr><td></td><td>PcA</td><td>Param. Cls</td><td>2-stage</td><td>A&A Param. Cls</td><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td><td></td><td>All Old</td><td>New</td></tr><tr><td>(1)</td><td></td><td>‚àö</td><td></td><td></td><td>60.8</td><td>65.2</td><td>58.5</td><td>53.8</td><td>70.8</td><td>45.6</td><td>52.3</td><td>59.8</td><td>48.6</td><td>44.5</td><td>57.9</td><td>37.3</td></tr><tr><td>(2)</td><td>‚àö</td><td></td><td></td><td>‚àö</td><td>69.9</td><td>68.0</td><td>70.9</td><td>70.5</td><td>77.7</td><td>67.0</td><td>56.2</td><td>55.3</td><td>56.7</td><td>44.8</td><td>48.5</td><td>42.8</td></tr><tr><td>(3)</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td></td><td>67.6</td><td>75.5</td><td>63.7</td><td>66.7</td><td>79.1</td><td>60.7</td><td>59.5</td><td>67.2</td><td>55.6</td><td>47.6</td><td>58.6</td><td>41.7</td></tr><tr><td>(4)</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>72.6</td><td>75.2</td><td>71.2</td><td>72.2</td><td>83.4</td><td>66.8</td><td>58.8</td><td>64.5</td><td>56.0</td><td>46.8</td><td> 57.0</td><td>41.4</td></tr><tr><td>(5)</td><td>‚àö</td><td>‚àö</td><td></td><td></td><td>65.1</td><td>70.7</td><td>62.3</td><td>56.8</td><td>74.3</td><td>48.4</td><td>57.1</td><td>61.0</td><td>55.2</td><td>45.1</td><td>57.4</td><td>38.5</td></tr><tr><td>(6)</td><td>‚àö</td><td>‚àö</td><td></td><td>‚àö</td><td>66.9</td><td>72.0</td><td>64.4</td><td>60.0</td><td>76.6</td><td>51.9</td><td>54.8</td><td>62.8</td><td>50.9</td><td>44.1</td><td>54.3</td><td>38.6</td></tr></table></body></html>\n\nTable 3: Ablation study on the main components of our method. ‚ÄòPcA‚Äô denotes the proposed Prior-constrained Association ‚ÄòParam. Cls‚Äô denotes the parametric classifier. ‚ÄòA&A‚Äô denotes evaluating the model by our Association and Assign.\n\n# 4.2 Comparison with State of The Arts\n\nIn Table 2, we compare with the state-of-the-art GCD methods under two settings: ground truth number of classes known or unknown.\n\nGround truth number of classes known. Under this setting, we combine the association based non-parametric classification with the parametric classification, where the ground truth class number is utilized as a prior in the latter. In Table 2, our proposed method achieves state-of-the-art performance on fine-grained datasets, whether using parametric or non-parametric classifier for evaluation. On CUB and Stanford Cars, our method surpasses the previous best method CMS by $4 . 4 \\%$ and $1 5 . 3 \\%$ on ‚ÄòAll‚Äô accuracy. On generic datasets, our method performs on par with SoTA methods. Additionally, we notice a consistent improvement on ‚ÄòNew‚Äô classes, proving our method is good at discovering and clustering new categories.\n\nGround truth number of classes unknown. Not knowing the ground truth class number is a more practical setting but causes the model learning to be more challenging. In Table 2, we compare our method with others that does not require the ground truth class number. The results show that with solely non-parametric loss, our method achieves much higher accuracy on all fine-grained datasets, and also delivers consistent performance on generic datasets. The comparisons prove the effectiveness of our method and its flexibility to work under class-unknown setting.\n\n# 4.3 Ablation Study\n\nTo investigate how each component affects the model performance, we perform ablation experiments and present the results in Table 3.\n\nEffectiveness of the prior-constrained association. Table 3 (1) lists the accuracy of training and evaluation with parametric classifier (Wen, Zhao, and Qi 2023). Compared with (1), our association based non-parametric classification as denoted by (2) achieves better performance on each dataset. Noticeably on CUB and Stanford Cars, (2) improves the All Acc by $9 . { \\dot { 1 } } \\%$ and $1 6 . 7 \\%$ respectively.\n\nEffectiveness of joint training. The full model indicated by (3) and (4) jointly trains with association-based nonparametric classifier and parametric classifier. Compared to (1) and (2), the result in (3) improves the parametric classifier to a large extent, and the accuracy in (4) also consistently boosts over the non-parametric classifier alone. This shows that the joint training is indeed able to benefit both classifiers by mining their complementarity.\n\nTable 4: Comparison of models trained with different association algorithms. Semi-Kmeans is proposed in (Vaze et al. 2022). Semi-DBSCAN is based on the clustering algorithm DBSCAN (Ester et al. 1996) and inter-class distances among known instances are masked before clustering. SemiDBSCAN w/ constraint: adds our proposed prior constraint into the semi-DBSCAN clustering. Ours w/o constraint: our association but with the prior constraint removed.   \n\n<html><body><table><tr><td rowspan=\"2\">Association</td><td colspan=\"3\">CUB</td><td colspan=\"3\">Stanford Cars</td><td colspan=\"3\">Aircraft</td></tr><tr><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td></tr><tr><td>Semi-Kmeans</td><td>61.0</td><td>50.6</td><td>66.2</td><td>49.7</td><td>58.9</td><td>45.2</td><td>38.2</td><td>36.2</td><td>39.2</td></tr><tr><td>Semi-DBSCAN</td><td>68.3</td><td>64.8</td><td>70.1</td><td>65.2</td><td>74.7</td><td>60.6</td><td>47.4</td><td>47.8</td><td>47.2</td></tr><tr><td>Semi-DBSCAN w/ constraint</td><td>71.1</td><td>72.8</td><td>70.3</td><td>68.6</td><td>77.1</td><td>64.4</td><td>53.7</td><td>53.6</td><td>53.8</td></tr><tr><td>Ours w/o constraint</td><td>67.9</td><td>61.5</td><td>71.1</td><td>65.9</td><td>72.3</td><td>62.9</td><td>47.5</td><td>52.9</td><td>44.7</td></tr><tr><td>Our association</td><td>69.9</td><td>68.0</td><td>70.9</td><td>70.5</td><td>77.7</td><td>67.0</td><td>56.2</td><td>55.3</td><td>56.7</td></tr></table></body></html>\n\nEffectiveness of two-stage training. To validate the necessity of two-stage training, we also provide the results of jointly training non-parametric and parametric classifier in one single stage, as indicated by (5) and (6). Compared with (1) and (2), the single-stage training promotes the accuracy of parametric classifier, but drops the performance of nonparametric classifier, indicating that a warming-up stage is necessary to better prepare the model for joint training.\n\n# 4.4 Analysis on The Proposed Association\n\nIn this subsection, we conduct analysis on the proposed association from different aspects. To focus on the association part, we only adopt the association-based non-parametric classification loss when reporting the performances.\n\nHow does the model perform with other association algorithms? In Table 4, we explore the option of adopting other common clustering algorithms including SemiKmeans (Vaze et al. 2022) and Semi-DBSCAN (Ester et al. 1996), both during training and evaluation. From the table, we observe that Semi-DBSCAN shows competitive performance compared to Semi-Kmeans, but stills underperforms the proposed association on all three datasets.\n\nHow much contribution does the prior constraint make in association? The prior constraint serves as the key element in our proposed association. We validate its effectiveness in two ways: First, we demonstrate its integration into the classic DBSCAN algorithm which merges the instances greedily. As shown in Table 4, adding the prior constraint to Semi-DBSCAN leads to steady improvement on all three datasets, even surpassing our association method on CUB. This highlights the generalizability of the proposed prior constraint. With the constraint incorporated, our association achieves better accuracy than other algorithms on Stanford Cars and Aircraft.\n\nTable 5: Comparison of model performance with different subset size for association.   \n\n<html><body><table><tr><td rowspan=\"2\">Subset size</td><td colspan=\"3\">CUB</td><td colspan=\"3\">Aircraft</td><td colspan=\"3\">Cifar100</td></tr><tr><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td><td>All</td><td>Old</td><td>New</td></tr><tr><td>100%</td><td>69.9</td><td>68.0</td><td>70.9</td><td>50.8</td><td>55.8</td><td>48.4</td><td>74.4</td><td>81.8</td><td>59.7</td></tr><tr><td>50%</td><td>60.8</td><td>52.5</td><td>64.9</td><td>56.2</td><td>55.3</td><td>56.7</td><td>77.0</td><td>79.6</td><td>71.8</td></tr><tr><td>30%</td><td>45.2</td><td>41.8</td><td>46.8</td><td>48.7</td><td>38.7</td><td>53.7</td><td>77.3</td><td>78.9</td><td>74.1</td></tr></table></body></html>\n\n![](images/31514c0be9beb1408f3096c93456985043e4a699faf7072fa8d438700ac5ad16.jpg)  \nFigure 3: Estimated class number on each dataset.\n\nWhen is subset association necessary? For large-scale datasets or when number of images per category is high, we propose to perform the subset association. To verify the effect of subset association, we provide in Table 5 the performance on representative datasets with subset and full-set association. CUB, with an average of 30 images per-category, shows better accuracy with full-set association, which is reasonable as reducing subset size further leads to insufficient data for association. In contrast, Aircraft and Cifar100, with an average of 67 and 500 images per category, benefit from subset association, likely because our association works best with a small amount of representative samples, and involving too many samples per-category brings more noise to association, thus harming the performance.\n\nHow well can the proposed method predict the class number? When the non-parametric classification loss is utilized alone, our method does not require the prior knowledge of ground truth class number. In Figure 3, we compare the estimated v.s. ground truth class numbers. In most cases, the association generates fewer pseudo classes than ground truth. Overall, the estimated class number is close to ground truth, with a maximum error rate of $1 8 \\%$ .\n\n# 5 Conclusion\n\nIn this paper, we have proposed a simple yet effective method for generalized category discovery. By mining the labeled data prior under GCD setting, we propose a priorconstrained greedy association algorithm to estimate reliable semantic groups for representation learning. Assisted by the association, the non-parametric prototypical contrastive learning can not only work alone to achieve good performance, but also be effectively integrated with the parametric classifier to mutually benefit each other, leading to further enhanced accuracy. Extensive experiments on multiple benchmarks demonstrate the effectiveness and superiority of the proposed method.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥‰∫ÜÂπø‰πâÁ±ªÂà´ÂèëÁé∞ÔºàGeneralized Category Discovery, GCDÔºâ‰ªªÂä°ÔºåÂç≥Âú®Â∑≤Áü•Á±ªÂà´Ê†áËÆ∞Êï∞ÊçÆÁöÑÂ∏ÆÂä©‰∏ãÔºåÂØπÂèØËÉΩÂåÖÂê´Â∑≤Áü•ÊàñÊú™Áü•Á±ªÂà´ÁöÑÊú™Ê†áËÆ∞Êï∞ÊçÆËøõË°åËÅöÁ±ª„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÂÖãÊúç‰∫Ü‰º†ÁªüÂçäÁõëÁù£Â≠¶‰π†ÔºàSSLÔºâÁöÑÂ±ÄÈôêÊÄßÔºåÂêéËÄÖÂÅáËÆæÊú™Ê†áËÆ∞Êï∞ÊçÆ‰ªÖÊù•Ëá™Â∑≤Áü•Á±ªÂà´ÔºåËÄåGCDÂÖÅËÆ∏Êú™Ê†áËÆ∞Êï∞ÊçÆÊù•Ëá™Êú™Áü•Á±ªÂà´ÔºåÊõ¥Á¨¶ÂêàÂºÄÊîæ‰∏ñÁïåÁöÑÂÆûÈôÖÂ∫îÁî®Âú∫ÊôØ„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖàÈ™åÁ∫¶ÊùüÂÖ≥ËÅîÂ≠¶‰π†ÔºàPrior-constrained Association Learning, PALÔºâÊñπÊ≥ïÔºåÈÄöËøáÂ∞ÜÊ†áËÆ∞Êï∞ÊçÆÁöÑÂÖàÈ™åÁü•ËØÜÂÆåÂÖ®ËûçÂÖ•ÂÖ≥ËÅîËøáÁ®ãÔºåÁîüÊàêÂèØÈù†ÁöÑÂÆû‰æãÂàÜÁªÑ‰ª•Â¢ûÂº∫Ë°®Á§∫Â≠¶‰π†„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫ÂÖàÈ™åÁ∫¶ÊùüË¥™Â©™ÂÖ≥ËÅîÁÆóÊ≥ïÔºåÁ°Æ‰øùÊØè‰∏™ÂàÜÁªÑÊúÄÂ§öÂåÖÂê´‰∏Ä‰∏™Â∑≤Áü•Á±ªÂà´ÔºåÈÅøÂÖç‰∫ÜÈó¥Êé•ÈîôËØØÂÖ≥ËÅî„ÄÇÂú®CUBÂíåStanford CarsÊï∞ÊçÆÈõÜ‰∏äÔºåÂàÜÂà´ÊØî‰πãÂâçÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÂçá‰∫Ü4.4%Âíå15.3%ÁöÑÂáÜÁ°ÆÁéá„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** Â∞ÜÈùûÂèÇÊï∞ÂéüÂûãÂØπÊØîÂ≠¶‰π†‰∏éÂèÇÊï∞ÂàÜÁ±ªÂô®Áªü‰∏ÄÂú®‰∏Ä‰∏™Ê°ÜÊû∂‰∏≠ÔºåÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÂÆûÁé∞‰∫íË°•ÊèêÂçá„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** ÊèêÂá∫Â≠êÈõÜÂÖ≥ËÅîÁ≠ñÁï•ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÁöÑÂÖ≥ËÅîËÆ°ÁÆóÊàêÊú¨„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   Ê†∏ÂøÉÂéüÁêÜÊòØÂà©Áî®Ê†áËÆ∞Êï∞ÊçÆÁöÑÂÖàÈ™åÁü•ËØÜÁ∫¶ÊùüÊú™Ê†áËÆ∞Êï∞ÊçÆÁöÑÂÖ≥ËÅîËøáÁ®ãÔºåÁ°Æ‰øùÁîüÊàêÁöÑËØ≠‰πâÂàÜÁªÑ‰∏éÊ†áËÆ∞Êï∞ÊçÆÁöÑÁúüÂÆûÁ±ªÂà´ÂÖ≥Á≥ª‰∏ÄËá¥„ÄÇ\\n> *   ËÆæËÆ°Âì≤Â≠¶ÊòØÈÄöËøáÈÄêÊ≠•ÂÖ≥ËÅîÂèØÈù†ÁöÑÂÆû‰æãÂπ∂Ê£ÄÊü•ÂÜ≤Á™ÅÔºåÈÅøÂÖçÁõ¥Êé•ÂíåÈó¥Êé•ÁöÑÈîôËØØÂÖ≥ËÅîÔºå‰ªéËÄåÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÂÆû‰æãÂàÜÁªÑ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÊñπÊ≥ïÔºàÂ¶ÇDCCL„ÄÅGPCÔºâ‰ªÖÂ∞ÜÊ†áËÆ∞Êï∞ÊçÆ‰Ωú‰∏∫È¢ÑËÅöÁ±ªÊàñÂêéËÅöÁ±ªÁöÑ‰ºòÂåñÔºåÂøΩÁï•‰∫ÜÂÖ≥ËÅîËøáÁ®ã‰∏≠ÁöÑÊΩúÂú®ÈîôËØØÂÖ≥ËÅî„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** ÊèêÂá∫Âú®ÂÖ≥ËÅîËøáÁ®ã‰∏≠ÊØè‰∏ÄÊ≠•ÈÉΩËøõË°åÂÖàÈ™åÁ∫¶ÊùüÊ£ÄÊü•ÔºåÁ°Æ‰øùÂàÜÁªÑ‰∏çËøùÂèçÊ†áËÆ∞Êï∞ÊçÆÁöÑÁ±ªÂà´ÂÖ≥Á≥ª„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **Ê∑∑ÂêàÁâπÂæÅÊèêÂèñÔºö** ‰ΩøÁî®È™®Âπ≤ÁΩëÁªúÊèêÂèñÊâÄÊúâËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÁâπÂæÅÔºåËÆ°ÁÆóÊ†áËÆ∞ÂÆû‰æãÁöÑÁ±ªÂà´ÂùáÂÄºÁâπÂæÅ‰Ωú‰∏∫‰ª£ÁêÜ„ÄÇ\\n> 2.  **ÊàêÂØπË∑ùÁ¶ªËÆ°ÁÆóÔºö** ËÆ°ÁÆóÂàùÂßã‰ª£ÁêÜ‰∏éÊú™Ê†áËÆ∞ÂÆû‰æãÁöÑJaccardË∑ùÁ¶ªÔºå‰øùÁïôË∑ùÁ¶ª‰Ωé‰∫éÈòàÂÄºŒµÁöÑÂÄôÈÄâÂØπ„ÄÇ\\n> 3.  **Ë¥™Â©™ÂÖ≥ËÅî‰∏éÁ∫¶ÊùüÔºö** ÊåâË∑ùÁ¶ªÂçáÂ∫èÂÖ≥ËÅîÂÄôÈÄâÂØπÔºåÊØèÊ¨°ÂÖ≥ËÅîÂâçÊ£ÄÊü•ÂàÜÁªÑÂÜ≤Á™ÅÔºàÂ¶ÇÂàÜÁªÑÊòØÂê¶ÂåÖÂê´‰∏çÂêåÂ∑≤Áü•Á±ªÂà´ÔºâÔºåÈÅøÂÖçÈîôËØØÂÖ≥ËÅî„ÄÇ\\n> 4.  **ÈùûÂèÇÊï∞ÂàÜÁ±ªÔºö** ‰ΩøÁî®ÂéüÂûãÂØπÊØîÂ≠¶‰π†ÔºàÂÖ¨Âºè1ÔºâÂ∞ÜÂÆû‰æãÊãâËøëÂÖ∂ÂàÜÁªÑ‰∏≠ÂøÉÔºåËøúÁ¶ªÂÖ∂‰ªñÂàÜÁªÑ‰∏≠ÂøÉ„ÄÇ\\n> 5.  **‰∏§Èò∂ÊÆµËÆ≠ÁªÉÔºö** Á¨¨‰∏ÄÈò∂ÊÆµ‰ªÖËÆ≠ÁªÉÈùûÂèÇÊï∞ÂàÜÁ±ªÂô®ÔºåÁ¨¨‰∫åÈò∂ÊÆµËÅîÂêàËÆ≠ÁªÉÈùûÂèÇÊï∞ÂíåÂèÇÊï∞ÂàÜÁ±ªÂô®ÔºàSimGCDÔºâ„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   k-means„ÄÅRankStats+„ÄÅUNO+„ÄÅGPC„ÄÅGCD„ÄÅOpenCon„ÄÅPromptCAL„ÄÅPIM„ÄÅŒºGCD„ÄÅCMS„ÄÅSimGCD„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®ËÅöÁ±ªÂáÜÁ°ÆÁéáÔºàACCÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®CUBÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü72.6%ÔºàÈùûÂèÇÊï∞ËØÑ‰º∞ÔºâÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãPromptCALÔºà62.9%ÔºâÂíåCMSÔºà68.2%Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü4.4‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ËÅöÁ±ªÂáÜÁ°ÆÁéáÔºàACCÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®Stanford CarsÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü72.2%ÔºàÈùûÂèÇÊï∞ËØÑ‰º∞ÔºâÔºåËøúÈ´ò‰∫éÂü∫Á∫øÊ®°ÂûãPromptCALÔºà50.2%ÔºâÂíåCMSÔºà56.9%Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü15.3‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ËÅöÁ±ªÂáÜÁ°ÆÁéáÔºàACCÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®AircraftÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü58.8%ÔºàÈùûÂèÇÊï∞ËØÑ‰º∞ÔºâÔºå‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãPromptCALÔºà52.2%ÔºâÂíåCMSÔºà56.0%Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü2.8‰∏™ÁôæÂàÜÁÇπ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n> **ÊèêÂèñ‰∏éÊ†ºÂºèÂåñË¶ÅÊ±Ç**\\n> *   Âπø‰πâÁ±ªÂà´ÂèëÁé∞ (Generalized Category Discovery, GCD)\\n> *   ÂÖàÈ™åÁ∫¶ÊùüÂÖ≥ËÅîÂ≠¶‰π† (Prior-constrained Association Learning, PAL)\\n> *   ÂéüÂûãÂØπÊØîÂ≠¶‰π† (Prototypical Contrastive Learning, PCL)\\n> *   ÂçäÁõëÁù£Â≠¶‰π† (Semi-supervised Learning, SSL)\\n> *   Ë¥™Â©™ÂÖ≥ËÅîÁÆóÊ≥ï (Greedy Association Algorithm, N/A)\\n> *   ÈùûÂèÇÊï∞ÂàÜÁ±ª (Non-parametric Classification, N/A)\\n> *   ÂèÇÊï∞ÂàÜÁ±ª (Parametric Classification, N/A)\\n> *   ÁªÜÁ≤íÂ∫¶ËØÜÂà´ (Fine-grained Recognition, N/A)\"\n}\n```"
}