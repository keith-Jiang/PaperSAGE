{
    "source": "Semantic Scholar",
    "arxiv_id": "2501.13985",
    "link": "https://arxiv.org/abs/2501.13985",
    "pdf_link": "https://arxiv.org/pdf/2501.13985.pdf",
    "title": "Pilot: Building the Federated Multimodal Instruction Tuning Framework",
    "authors": [
        "Baochen Xiong",
        "Xiaoshan Yang",
        "Y. Song",
        "Yaowei Wang",
        "Changsheng Xu"
    ],
    "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
    ],
    "publication_date": "2025-01-23",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 3,
    "influential_citation_count": 0,
    "institutions": [
        "MAIS",
        "Institute of Automation",
        "Chinese Academy of Sciences",
        "Pengcheng Laboratory",
        "School of Artificial Intelligence",
        "University of Chinese Academy of Sciences",
        "UCAS",
        "Harbin Institute of Technology (Shenzhen)"
    ],
    "paper_content": "# Pilot: Building the Federated Multimodal Instruction Tuning Framework\n\nBaochen Xiong1,2,3, Xiaoshan $\\mathbf { Y a n g } ^ { 1 , 2 , 3 }$ , Yaguang $\\mathbf { S o n g ^ { 2 } }$ , Yaowei Wang2,4, Changsheng $\\mathbf { X } \\mathbf { u } ^ { 1 , 2 , 3 ^ { * } }$\n\n1MAIS, Institute of Automation, Chinese Academy of Sciences 2Pengcheng Laboratory 3School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS) 4Harbin Institute of Technology (Shenzhen) xiongbaochen2022 $@$ ia.ac.cn, xiaoshan.yang, csxu @nlpr.ia.ac.cn, songyg01, wangyw @pcl.ac.cn\n\n# Abstract\n\nIn this paper, we explore a novel federated multimodal instruction tuning task(FedMIT), which is significant for collaboratively fine-tuning MLLMs on different types of multimodal instruction data on distributed devices. To solve the new task, we propose a federated multimodal instruction tuning framework(Pilot). Our framework integrates two stages of “adapter on adapter” into the connector of the vision encoder and the LLM. In stage 1, we extract task-specific features and client-specific features from visual information. In stage 2, we build the cross-task Mixture-of-Adapters(CT-MoA) module to perform cross-task interaction. Each client can not only capture personalized information of local data and learn taskrelated multimodal information, but also learn general knowledge from other tasks. In addition, we introduce an adaptive parameter aggregation strategy for text training parameters, which optimizes parameter aggregation by calculating weights based on the euclidean distance between parameters, so that parameter aggregation can benefit from positive effects to the greatest extent while effectively reducing negative effects. Our framework can collaboratively exploit distributed data from different local clients to learn cross-task knowledge without being affected by the task heterogeneity during instruction tuning. The effectiveness of our method is verified in two different cross-task scenarios.\n\n# Introduction\n\nThe emergence of Multimodal Large Language Models (MLLMs) (Li et al. 2023; Liu et al. 2024; Driess et al. 2023; Dai, Li et al. 2023) has significantly advanced the field of artificial intelligence. MLLMs have shown excellent ability in processing and integrating various modalities information (especially text and image), and has achieved remarkable performance in tasks such as text generation, machine translation and question answering. Enhancing the zero-shot generalization ability of MLLMs on novel multimodal tasks is a key goal driving its development. Multimodal instruction tuning has been shown to be highly effective in improving the zero-shot generalization ability of models to unseen multimodal problems (Xu et al. 2022; Ye et al. 2023; Sun et al. 2024; Chen et al. 2024a; Xiao et al. 2024b).\n\n↑ Client 1 Client 2 Client K   \nTraditional FedIT task: Brainstorming Task Classification Task   \nInput: List 6 different types Input:Identify which Input: When was Tomoaki   \nof rain in Seattle instrument is string or Komorida born? Context:   \nOutput: 1. Mist 2. Drizzle 3. percussion. ‘Komorida was born in...   \nSleet 4.Downpour... Output:Gudok is string.. Output: He was born...   \nFedMIT task: GQATask CaptionCOCOTask Grounding RefCOCO Task Input:Is the sky Input: Summary of Input:the coordinates dark? what you see. of yellowboat. Output: Yes, the Output: A sea plane Output: [0.011,0.15, sky is dark. in the water with. 0.358,0.283]. Sending local model Sending back global model (a)   \n0. A\\W\\\\W WWWW AIIIIY   \nBrainstorming Closed QA Classification GQA Caption Grounding (b) COCo RefCOCO\n\nCurrent instruction tuning methods are usually implemented by centralized learning paradigm, where one central party collects a substantial amount of data to train the model, which may lead to privacy security issues. Federated Learning (FL) is currently the primary framework for distributed training of models while maintaining data security and privacy. McMahan et al. (2017) proposed the first federated learning algorithm, FedAvg, which aggregates model gradients from local clients to a central server without data sharing. Afterwards, FL is also successfully used for instruction tuning of pre-trained models (Zhang et al. 2024b; Ye et al. 2024), called Federated Instruction Tuning (FedIT). These frameworks can effectively leverage locally private instruction-following data.\n\nAlthough existing FedIT approaches have made important progress, they only consider the case where different clients perform different instruction-based natural language processing(NLP) tasks, and there is little exploration in the multimodal field. The integration of vision and language plays a crucial role in machine perception and understanding of the world. Language is the basis of cognitive processing, while vision provides the necessary sensory information (Xu et al. 2024; Xiong et al. 2024). In this context, we attempt to transfer the success of FedIT in multimodal tasks. Therefore, in this paper, we explore a novel Federated Multimodal Instruction Tuning task (FedMIT).\n\nCompared with traditional FedIT, FedMIT focuses on the client containing different multimodal instruction tuning tasks (e.g., visual question answering and image captioning), as shown in Figure 1(a). In our preliminary study, we first distributed different task instruction data to each client, including “Caption (e.g., COCO (Lin et al. 2014))” for image description, “VQA (e.g., GQA (Hudson 2019))” for visual question answering, and “Grounding (e.g., RefCOCO (Kazemzadeh et al. 2014))” for visual localization. Then, we directly apply the representative method Shepherd (Zhang et al. 2024b) in the FedIT task to the FedMIT task. Figure 1(b) shows the relative scores of Shepherd on two tasks, where the relative scores are the comparison scores between the results of the Shepherd method and the local training results. The results show that the traditional FedIT method does not perform well on the FedMIT task. Since the diversity of multimodal tasks greatly increases the heterogeneity between clients, we believe that traditional FedIT methods cannot adequately address this kind of task heterogeneity.\n\nCompared with the traditional FedIT task, each client in FedMIT task not only needs to capture the personalized information of local data and task-related multimodal information, but also needs to be able to accommodate the differences between different tasks to avoid parameter conflicts. This requires the model to maintain its understanding of the task and local data while also being able to learn general knowledge from other tasks to improve model performance and cross-task ability.\n\nInspired by these observations, we introduce the Federated Multimodal Instruction Tuning framework: Pilot. Our framework integrates two stages of “adapter on adapter” into the connector of the vision encoder and the LLM, and adopts an adaptive parameter aggregation strategy for the training parameters of the LLM. First we introduce the two-stage training of the client. Stage 1: Task-specific feature mining. We hope to extract client-specific and task-specific features from the client’s visual information. We propose taskspecific adapter to extract task-specific visual features that is only important for one task, and client-specific adapter to extract specific visual features of the client’s unique data distribution. To encourage the client-specific adapter to produce features that are more refined than the task-specific adapter, a difference loss is used to ensure the orthogonality of their output features. Stage II: Cross-task visual interaction. We integrate the Cross-task Mixture-of-Adapters(CTMoA) module with the task-specific adapter. By interacting with the server, each adapter in CT-MoA is initialized from the task-specific adapter of the corresponding task. We hope that the CT-MOA module can learn general knowledge from other tasks to improve model performance and cross-task capabilities. Therefore, we added cross-task adapters to the task-specific adapters of other tasks in CT-MOA, where cross-task adapter on other taskspecific adapter. Cross-task adapter aims to extract crosstask collaboration visual features. In addition, the CT-MOA module also contains a router that selects adapters during the stage II with auxiliary losses on the router to maintain a balanced loading of adapters. Considering the computation and communication requirements, we adopt text-adapter-based parameter-efficient tuning techniques to train LLM to reduce the amount of trainable parameters on each device. For the server side, it collects all client visual and text training parameters. For visual training parameters, we adopt the taskaware aggregation strategy. For text training parameters, we introduce adaptive parameter aggregation, which optimizes parameter aggregation by calculating weights based on the euclidean distance between parameters, so that parameter aggregation can benefit from positive effects to the greatest extent while effectively reducing negative effects. Combining federated optimization with two-stage local updates, our framework can collaboratively exploit distributed data from different local clients to learn cross-task knowledge without being affected by the task heterogeneity during instruction tuning.\n\nOur contributions are summarized as follows: (1) We propose to explore a new task of federated multimodal instruction tuning, which is significant for collaboratively finetuning MLLMs on different types of multimodal instruction data on distributed devices. (2) To solve the new task, we propose a Federated Multimodal Instruction Tuning framework(Pilot). Our framework builds two stages “adapter on adapter” strategy. In stage 1, the model extracts clientspecific and task-specific features, and in stage 2, we construct CT-MOA modules to learn cross-task interactions. We adopt an adaptive aggregation strategy for the LLM training parameters. With the above approach, our method can learn cross-task knowledge without being affected by task heterogeneity during instruction tuning. (3) We verify the effectiveness of Pilot on the state-of-the-art LLaVA (Liu et al. 2024) in two different cross-task scenarios.\n\n# Related Work\n\n# Federated Learning\n\nThe earliest FL algorithm is FedAvg (McMahan et al. 2017), which builds the global model by averaging the local updates obtained by Stochastic Gradient Descent (SGD) (Gorbunov, Hanzely, and Richta´rik 2021). However, FedAvg inevitably experiences performance degradation on non-IID data (Yang et al. 2024b; Xiong et al. 2023). To deal with this problem, FedProx (Li et al. 2020) adds a proximal term to local targets to minimize the distance between the global model and the local model for non-IID data. PerAvg (Fallah, Mokhtari, and Ozdaglar 2020) uses popular meta-learning framework MAML (Finn, Abbeel, and Levine 2017), which allows each client to quickly adapt to local data by finding a suitable initialization. TAPPFL (Arevalo et al. 2024) designs a task-agnostic and provably privacy-preserving federated learning framework. FedTGP (Zhang et al. 2024a) uses adaptive-margin-enhanced contrastive learning to learn trainable global prototypes on the server to solve the model heterogeneity problem. FedLPS (Jia et al. 2024) uses an\n\nClient 1: Caption COCO Task Client 1 Server Client 1: Caption COCO Task TextLLM ATexter Adapter Avg LLM Texter Task-specific   \n□□□□□□□□□□□ Adapter Vg □□□□□□□ + → Client 2 电 →   \nClient-specific Text- Avg Client-specific/ Adapter Word Adapter Adapter Word T . Clent estapter 三 ↑ Adapter Task-specific 宫日 Adapter Adaptive Text-adapter Aggregation Stage 1 Communication Stage 2\n\nadaptive channel model pruning method to enable clients to participate in federated learning training with heterogeneous tasks. However, the above methods only consider the heterogeneity on single-modal clients. In contrast, our method focuses on addressing the heterogeneity of cross-task instruction data in the MLLM-based federated multimodal instruction tuning task.\n\n# Multimodal Instruction Tuning\n\nInstruction tuning (Wei et al. 2021; Xiao et al. 2024a) significantly improves the generalization ability of large language models to unseen tasks based on natural language instructions. With the advent of MLLMs, the scope of instruction tuning has been expanded to include multimodal and visual tasks. MiniGPT-4 (Zhu et al. 2023) and LLaVA (Liu et al. 2024) keep the visual encoder frozen and adjust the language model, extending instruction tuning to multimodal. InstructBLIP (Dai, Li et al. 2023) enhances zeroshot capabilities by performing instruction tuning on multiple datasets. Shikra (Chen et al. 2023) extend MLLMs to visual grounded tasks using instructions with bounding box coordinates. mPLUG-Owl (Ye et al. 2023) fine-tunes the visual and text encoders in two stages. EProj (He et al. 2023) address the catastrophic forgetting of the model in continuous instruction tuning. MixLoRA (Shen et al. 2024) uses conditional mixed LoRA to address the task interference caused by LoRA during multimodal instruction tuning. Although these studies demonstrate excellent zero-shot capabilities, they still suffer from the risk of privacy leakage.\n\n# Federated Instruction Tuning\n\nThere has been some research in federated learning based on Parameter-Efficient Fine-tuning(PEFT) (Shi et al. 2023; Su et al. 2024; Chen and Zhang 2024). FedCLIP (Lu et al. 2023) and pFedprompt (Guo et al. 2023) finetune the CLIP model (Radford et al. 2021) by adapting and conveying only small number of learnable prompts. FedDISC (Yang et al. 2024a) first integrated the pre-trained diffusion model (Dhariwal and Nichol 2021) into the FL framework. FedDAT (Chen et al. 2024b)utilizes a dual-adapter teacher architecture to address the efficient fine-tuning of parameters of multimodal base models in heterogeneous federated learning. However, the application of instruction tuning in FL has not been fully explored. Federated instruction tuning (Zhang et al. 2024b) provides a simple and effective method for supporting distributed client privacy-preserving instruction tuning through the FL protocol. Fatellm (Fan et al. 2023) and OpenFedLLM (Ye et al. 2024) built a concise and easy-to-research framework for fine-tuning federal instructions. Recently, FedDPA (Yang et al. 2024c) explored the problem of heterogeneity in NLP tasks. In our work, we attempt to address the unexplored task of multimodal instruction tuning for MLLMs in federated learning.\n\n# Methodology Problem Definition\n\nWe assume that there is a set of multimodal instruction tuning data $\\mathcal { D } \\ = \\ \\{ ( \\mathcal { D } _ { k } , t _ { k } ) \\} _ { k = 1 } ^ { K }$ from $K$ clients, where $\\mathcal { D } _ { k } = \\{ X _ { i , k } ^ { v } , \\mathbf { x } _ { i , k } ^ { i n s } , \\mathbf { y } _ { i , k } ^ { a n s } \\} _ { i = 1 } ^ { n _ { k } }$ is the set of $n _ { k }$ data pairs on the $k$ -th client. The total number of data pairs is $\\begin{array} { r } { n = \\sum _ { k = 1 } ^ { K } n _ { k } } \\end{array}$ $X ^ { v } , { \\bf x } ^ { i n s }$ and $\\mathbf { y } ^ { a n s }$ indicate the image, instruct on tokens and answer tokens, respectively. $t _ { k } \\in \\{ 1 , . . . , T \\}$ denotes the task type of the $k$ -th client. $T$ is the total number of tasks and $T \\leqslant K$ .\n\nIn the FedMIT task, all clients obtain the pre-trained MLLM from the server. Generally, MLLMs contain a visual encoder $f$ , a connector $\\psi$ , and LLM $L$ . Specifically, for a given input image $X ^ { v }$ , the visual encoder extracts visual features $H ^ { v } = f ( X ^ { v } )$ . The connector is used to align the visual encoder with the LLM. Connector transforms $H ^ { v }$ into a language embedding tokens $\\mathbf { x } ^ { i m g } \\in \\mathbb { R } ^ { N \\times C }$ , effectively facilitating the integration of multimodal information within the LLM framework, where $N$ is the number of tokens and $C$ is the hidden size.\n\n$$\n\\begin{array} { r } { \\mathbf { x } ^ { i m g } = \\psi ( H ^ { v } ) , \\mathrm { w i t h } H ^ { v } = f ( X ^ { v } ) . } \\end{array}\n$$\n\nFinally, we input $\\mathbf { x } ^ { i m g }$ and $\\mathbf { x } ^ { i n s }$ into the LLM to generate response.\n\nThe FedMIT task aims to perform instruction tuning in a distributed manner, where each client can increment the local model by leveraging cross-task knowledge learned on other clients without the need for centralized data collection. The overall optimization goal is defined as follows:\n\n$$\n\\sum _ { k = 1 } ^ { K } \\frac { n _ { k } } { n } \\sum _ { i = 1 } ^ { n _ { k } } \\frac { 1 } { n _ { k } } \\sum _ { j = 1 } ^ { L } - \\log p _ { \\theta } \\left( \\mathbf { y } _ { i , k } ^ { j , a n s } \\mid \\mathbf { x } _ { i , k } ^ { i m g } , \\mathbf { x } _ { i , k } ^ { i n s } , \\mathbf { y } _ { i , k } ^ { < j , a n s } \\right)\n$$\n\nwhere $L$ represents the answer length, $\\theta$ is the trainable parameters. y ij,kans indicates the j-th answer token and yi<,kj, $\\mathbf { y } _ { i , k } ^ { < j , a n s }$ indicates all answer tokens before the index $j$ .\n\n# Federated Multimodal Instruction Tuning Framework\n\nFedMIT task has greater heterogeneity between tasks, and traditional FedIT methods cannot be used directly to solve the problem. As shown in Figure 2, we propose the Federated Multimodal Instruction Tuning framework (Pilot) to address the task heterogeneity between clients. In our framework, we integrate a two-stage “adapter on adapter” method into the connector of the visual encoder and LLM. In stage 1, we extract task-specific features and client-specific features from visual information. Through federated aggregation, in stage 2, we build a CT-MOA module to perform cross-task interaction. We hope that each client not only needs to capture personalized information from local data and learn taskrelated multimodal information, but also needs to be able to learn general knowledge from other tasks to improve model performance and cross-task capabilities.\n\nStage 1: Task-specific Feature Mining. At stage 1, we hope to extract client-specific and task-specific features from the client’s visual information. We propose task-specific adapter $\\psi ^ { t }$ to extract task-specific visual features that is only important for one task, and client-specific adapter $\\psi ^ { s }$ to extract specific visual features of the client’s unique data distribution. We define these two adapters as two-layer of perceptrons. Finally, the image tokens represent: $\\mathbf { x } ^ { i m g } = \\mathbf { x } ^ { \\dot { t } } + \\mathbf { \\bar { x } } ^ { \\dot { s } }$ , where $\\mathbf { x } ^ { t } = \\mathsf { \\bar { \\psi } } ^ { t } ( H ^ { v } ) \\in \\mathbb { R } ^ { N \\times C }$ , $\\mathbf { x } ^ { s } \\bar { } = \\psi ^ { s } ( H ^ { v } ) \\in \\mathbb { R } ^ { N \\times C }$ .\n\nTo encourage the client-specific adapter to produce features that are more refined than the task-specific adapter, a difference loss is used to ensure the orthogonality of their output features. Inspired by the domain separation network (Bousmalis et al. 2016), we adopt a soft subspace orthogonality constraint:\n\n$$\n\\mathcal { L } _ { d } = \\| \\mathbf { x } ^ { t ^ { \\top } } \\mathbf { x } ^ { s } \\| _ { F } ^ { 2 } ,\n$$\n\nwhere $\\| \\cdot \\| _ { F }$ is Frobenius norm. Hence, the Stage 1 total loss is\n\n$$\n\\mathcal { L } = \\mathcal { L } _ { c e } + \\lambda _ { 0 } \\mathcal { L } _ { d } ,\n$$\n\nwhere $\\mathcal { L } _ { c e }$ represents the language modeling loss, which computes the cross-entropy of next-token predictions. $\\lambda _ { 0 }$ denote coefficients for difference loss. After stage 1, each client sends the LLM training parameters text-adapter $\\Theta _ { l }$ and task-specific adapter $\\Theta _ { a }$ parameters to the server.\n\nStage 2: Cross-task Visual Interaction. At stage 2, the client obtains the updated task-specific adapters for $T$ tasks from the server. In other words, each client obtains $( T { - } 1 )$ task-specific adapters of other tasks in addition to its local task-specific adapter. To learn general knowledge from other tasks to improve model performance and crosstask capabilities, we integrate the local task-specific adapter with other task-specific adapters to construct a Cross-task Mixture-of-Adapters (CT-MoA) module, as shown in Figure 3. In the CT-MoA module, we assume that the local taskspecific adapter index is 1 $( \\psi _ { 1 } ^ { t } )$ , and there are $T$ adapters in total. However, directly loading task-specific adapters of distinct tasks may cause the model of the current task to be unable to fully utilize these adapters due to task heterogeneity. Therefore, we added cross-task adapters to the task-specific adapters of other tasks in CT-MOA. For taskspecific adapters $\\mathbf { \\psi } _ { { i } } ^ { t } , i = 2 , . . . , T$ , we add cross-task adapter $\\psi ^ { c }$ on it to alleviate the discrepancy between other taskspecific adapters and the local task-specific adapter due to task heterogeneity. Cross-task adapter aims to extract crosstask collaboration knowledge. It has the same structure as the task-specific adapter and is initialized by the local taskspecific adapter $\\psi _ { 1 } ^ { t }$ parameters. In addition, the CT-MOA module includes a router for predicting the probability of selecting and activating each adapter from $T$ total task-specific adapters. The router network $( \\phi )$ has a linear layer for calculating the normalized weight matrix using $H ^ { v }$ for voting, and producting $\\mathcal { P }$ :\n\n![](images/c46948e393c54fed3e328e7fa1fbabbf51ebad3a4c9be71f7115a1edb3281ec9.jpg)  \nFigure 3: Cross-task Mixture-of-Adapters (CT-MoA).\n\n$$\n\\mathcal { P } = \\mathrm { S o f t m a x } ( \\phi ( H ^ { v } ) ) \\in \\mathbb { R } ^ { T } .\n$$\n\nFinally, the output of CT-MOA module is expressed as:\n\n$$\n{ \\bf x } ^ { t } = \\mathcal { P } [ 1 ] \\psi _ { 1 } ^ { t } ( H ^ { v } ) + \\sum _ { i = 2 } ^ { T } \\mathcal { P } [ i ] \\left( \\psi _ { i } ^ { t } ( H ^ { v } ) + \\psi _ { i } ^ { c } ( H ^ { v } ) \\right) .\n$$\n\nFollowing (Zoph et al. 2022), we adopt an auxiliary losses based on the language modeling cross-entropy loss to maintain a load balance between task-specific adapters in the Cross-task MoA module. The auxiliary losses comprise load balancing loss and router $\\mathbf { z }$ -loss. The load balancing loss is defined as:\n\n$$\n\\mathcal { L } _ { b } = T \\sum _ { i = 1 } ^ { T } \\mathcal { D } _ { i } \\mathcal { R } _ { i } ,\n$$\n\nwhere $\\begin{array} { r } { \\mathcal D _ { i } \\ = \\ \\frac { 1 } { N } \\sum _ { j = 1 } ^ { N } 1 \\left\\{ \\mathrm { a r g m a x } ( \\phi ( H ^ { v } [ j ] ) ) = i \\right\\} } \\end{array}$ represents the proportion of tokens distributed to adapter $i , 1 \\{ \\cdot \\}$ is an indicator function. While $\\begin{array} { r } { \\mathcal { R } _ { i } = \\frac { 1 } { N } \\sum _ { j = 1 } ^ { N } \\phi _ { i } ( H ^ { v } [ j ] ) } \\end{array}$ denotes the proportion of the probability assigned to adapter $i$ . $\\phi _ { i } ( H ^ { v } [ j ] )$ is the probability of routing token $j$ to adapter $i$ . The router $\\mathbf { z }$ -loss is defined as:\n\n$$\n\\mathcal { L } _ { z } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\left( \\log \\sum _ { j = 1 } ^ { T } e ^ { g _ { j } ^ { ( i ) } } \\right) ^ { 2 } ,\n$$\n\nwhere $g ~ \\in ~ \\mathbb { R } ^ { N \\times T }$ are the logits obtained by the router. Hence, the Stage 2 total loss is:\n\n$$\n\\begin{array} { r } { \\mathcal { L } = \\mathcal { L } _ { c e } + \\lambda _ { 1 } \\mathcal { L } _ { b } + \\lambda _ { 2 } \\mathcal { L } _ { z } , } \\end{array}\n$$\n\n$\\lambda _ { 1 }$ and $\\lambda _ { 2 }$ denote coefficients for load balancing loss and router $\\textbf { \\em z }$ -loss. During stage 2 training, the model only updates the local task-specific adapter, the cross-task adapter, and the text-adapter.\n\n# Federated Optimization\n\nThe proposed method is collaboratively optimized via local two stages instruction tuning and global aggregation. We train the Pilot with two alternate steps of local update and global aggregation for $R$ rounds. In each round, the client receives the global model and updates the local model $E$ epochs on the local data. In the first round, the server sends the base MLLM model to each client. Each client performs local instruction tuning of stage 1, then send the taskspecific adapter and text-adapter parameters to the server. After global aggregation, the server sends $T$ task-specific adapters and text-adapter parameters back to the client. Each client updates the model architecture and performs local instruction tuning of stage 2, and finally send the local taskspecific adapter and text-adapter parameters to the server.\n\nTask-aware Visual-adapter Aggregation. After the server collects the task-specific adapter parameters of all clients, we adopt a task-aware aggregation method.\n\nTask-specific adapter mainly learns task-specific visual features and has the general understanding ability of this task. Therefore, the parameters learned in one client can also be shared by clients of the same task. We use weighted average to aggregate the task-specific adapter parameters $\\Theta _ { a }$ of the same task:\n\n$$\n\\bar { \\Theta } _ { a } ^ { t } = \\sum _ { k \\in \\mathcal { K } _ { t } } \\frac { n _ { k } } { k ^ { \\prime } \\in \\mathcal { K } _ { t } } \\Theta _ { a , k } ^ { t } , t = 1 , . . . , T ,\n$$\n\nwhere $\\textstyle { \\mathcal { K } } _ { t }$ is a set of clients with task type $t$ . Finally, we obtain $T$ number of task-specific adapters and send them to each client.\n\nAdaptive Text-adapter Aggregation. For all collected text-adapter parameters, compared with the fully weighted aggregation method used in the traditional FedIT method, we expect that the text-adapter parameters of each client can benefit from the positive impact to the greatest extent during the aggregation process, while effectively reducing the negative impact. Therefore, we propose an adaptive textadapter aggregation method. We first calculate the Euclidean distance between each text-adapter parameter and the other $( K { - } 1 )$ text-adapter parameters. Then select the nearest $M$ points for weighted aggregation based on the distance. For example, text-adapter parameters $\\Theta _ { l , k }$ for client $k$ :\n\n$$\nd _ { k , i } = E ( \\Theta _ { l , k } , \\Theta _ { l , i } ) , { \\mathrm { ~ f o r ~ } } i \\in \\{ 1 , 2 , . . . , K \\} \\backslash \\{ k \\} ,\n$$\n\nwhere $E ( \\cdot )$ is the Euclidean distance formula. We obtain the distance $D _ { k } \\in \\mathbb { R } ^ { K - 1 }$ between the $\\Theta _ { l , k }$ and other textadapter parameters, and then we select the Top- $M$ closest parameters for weighted aggregation:\n\n$$\n\\bar { \\Theta } _ { l , k } = \\frac { n _ { k } } { n _ { k } + \\sum _ { k ^ { \\prime } \\in \\mathcal { K } _ { m } } n _ { k ^ { \\prime } } } \\Theta _ { l , k } + \\sum _ { k ^ { \\prime } \\in \\mathcal { K } _ { m } } \\frac { n _ { k ^ { \\prime } } \\cdot w _ { k ^ { \\prime } } } { n _ { k } + \\sum _ { k ^ { \\prime } \\in \\mathcal { K } _ { m } } n _ { k ^ { \\prime } } } \\Theta _ { l , k ^ { \\prime } } ,\n$$\n\nwhere $\\begin{array} { r } { w _ { k ^ { \\prime } } = \\frac { 1 / d _ { k , k ^ { \\prime } } } { \\sum _ { k ^ { \\prime } \\in \\mathcal { K } _ { m } } 1 / d _ { k , k ^ { \\prime } } } } \\end{array}$ , $\\kappa _ { { \\scriptscriptstyle m } }$ is the set of $M$ clients closest to parameter $\\Theta _ { l , k }$ .\n\n# Experiment\n\n# Experimental Setups\n\nNow, we introduce how to construct the federated multimodal instruction tuning task scenario. To ensure the diversity of instruction tuning datasets, we collect various publicly available and commonly used visual-language datasets. These instruction tuning datasets cover a wide range of tasks, including knowledge-based image question answering, image question answering reading comprehension, and optical character recognition VQA. The selected datasets include ScienceQA (Lu et al. 2022), GQA (Hudson 2019), and OCRVQA (Mishra et al. 2019). However, we observe that these datasets are limited to traditional QA tasks in visuallanguage tasks. Therefore, to enrich the diversity of tasks, we introduce the image caption dataset COCO (Lin et al. 2014) for image description task and the grounding dataset RefCOCO (Kazemzadeh et al. 2014) for visual localization task. For all the above datasets, we construct two different federated instruction tuning scenarios. FL-oriented visual understanding scenario: We use the GQA, COCO, and RefCOCO datasets. We randomly divide each dataset into 3 subsets, and each subset is regarded as a client. Then, we obtain a FedMIT task scenario with 9 clients and 3 different visual understanding tasks(9-client, 3-task). FL-oriented general VQA scenario: We use the ScienceQA, GQA, and OCRVQA datasets. Similar to the above operations, we obtain a FedMIT task scenario with 9 clients and 3 different VQA tasks(9-client, 3-task). For more data information, please refer to the supplementary materials.\n\n# Baselines\n\nWe compare our framework Pilot with 5 state-of-theart FL algorithms: FedAVG (2017), FedProx (2020), FedAdam (2020), Shepherd (2024b), and FedDPA (2024c). FedAvg takes the weighted average of all training parameters as a standard optimization method. FedProx focuses on local model correction, and FedAdam focuses on introducing momentum on the server side to stabilize global model updates. Shepherd and FedDPA are FedIT task method. We also show local training and centralized training as references, where local training is trained by using one client’s dataset without collaboration. Centralized training is training all datasets centrally.\n\nTable 1: Comparison with state-of-the-art methods under FL-oriented visual understanding scenario.   \n\n<html><body><table><tr><td rowspan=\"2\">Methods</td><td colspan=\"3\">GQA</td><td colspan=\"3\">Caption COCO</td><td colspan=\"3\">Grounding RefCOCO</td></tr><tr><td>Client 1</td><td>Client 2</td><td>Client 3</td><td>Client 4</td><td>Client 5</td><td>Client 6</td><td>Client 7</td><td>Client 8</td><td>Client 9</td></tr><tr><td>Centralized training</td><td>Centralized test result: 52.8</td><td></td><td></td><td></td><td>Centralized test result: 130.3</td><td></td><td></td><td>Centralized test result: 65.3</td><td></td></tr><tr><td>Local training</td><td>48.8</td><td>47.5</td><td>48.4</td><td>117.6</td><td>122.3</td><td>120.9</td><td>29.1</td><td>35.1</td><td>31.9</td></tr><tr><td>FedAvg</td><td>47.2</td><td>46.6</td><td>46.4</td><td>86.5</td><td>102.5</td><td>104.2</td><td>25.4</td><td>30.2</td><td>32.4</td></tr><tr><td>FedProx</td><td>48.1</td><td>46.4</td><td>47.8</td><td>98.5</td><td>101.2</td><td>113.2</td><td>35.8</td><td>45.5</td><td>39.6</td></tr><tr><td>FedAdam</td><td>50.8</td><td>48.8</td><td>50.5</td><td>117.3</td><td>119.2</td><td>122.0</td><td>47.5</td><td>45.9</td><td>45.1</td></tr><tr><td>FedDPA</td><td>49.3</td><td>48.2</td><td>49.3</td><td>112.2</td><td>121.4</td><td>113.6</td><td>48.2</td><td>47.3</td><td>46.2</td></tr><tr><td>Shepherd</td><td>43.8</td><td>39.6</td><td>44.7</td><td>75.1</td><td>86.2</td><td>101.2</td><td>24.1</td><td>30.7</td><td>28.5</td></tr><tr><td>Pilot</td><td>51.4</td><td>49.7</td><td>50.2</td><td>120.3</td><td>126.4</td><td>125.1</td><td>49.6</td><td>52.4</td><td>50.9</td></tr></table></body></html>\n\nTable 2: Comparison with state-of-the-art methods under FL-oriented general VQA scenario.   \n\n<html><body><table><tr><td rowspan=\"2\">Methods</td><td colspan=\"3\">GQA</td><td colspan=\"3\">ScienceQA</td><td colspan=\"3\">OCRVQA</td></tr><tr><td>Client 1</td><td>Client 2</td><td>Client 3</td><td>Client 4</td><td>Client 5</td><td>Client 6</td><td>Client 7</td><td>Client 8</td><td>Client 9</td></tr><tr><td>Centralized training</td><td></td><td>Centralized test result: 56.7</td><td></td><td></td><td>Centralized test result: 58.0</td><td></td><td></td><td>Centralized test result: 52.3</td><td></td></tr><tr><td>Local training</td><td>46.1</td><td>50.2</td><td>49.6</td><td>50.2</td><td>49.6</td><td>50.0</td><td>40.62</td><td>46.33</td><td>37.10</td></tr><tr><td>FedAvg</td><td>44.1</td><td>48.3</td><td>47.1</td><td>44.2</td><td>40.5</td><td>42.6</td><td>35.6</td><td>38.1</td><td>34.2</td></tr><tr><td>FedProx</td><td>44.5</td><td>48.3</td><td>46.6</td><td>48.0</td><td>46.3</td><td>43.9</td><td>34.2</td><td>37.5</td><td>34.6</td></tr><tr><td>FedAdam</td><td>45.6</td><td>49.6</td><td>50.2</td><td>50.1</td><td>51.2</td><td>49.0</td><td>42.9</td><td>49.5</td><td>42.8</td></tr><tr><td>FedDPA</td><td>47.3</td><td>50.2</td><td>50.6</td><td>52.3</td><td>51.9</td><td>50.4</td><td>43.5</td><td>49.6</td><td>43.2</td></tr><tr><td>Shepherd</td><td>43.4</td><td>45.0</td><td>46.2</td><td>37.6</td><td>36.8</td><td>39.5</td><td>35.6</td><td>35.3</td><td>30.7</td></tr><tr><td>Pilot</td><td>49.2</td><td>52.8</td><td>52.3</td><td>54.7</td><td>53.2</td><td>53.7</td><td>45.7</td><td>50.2</td><td>44.9</td></tr></table></body></html>\n\n# Implementation Details\n\nIn all experiments, we use LLaVA 1.5 (Liu et al. 2024) as multimodal Large Language Model. LLaVA utilizes the pretrained CLIP visual encoder ViT-L/14 (Radford et al. 2021) to extract visual features, and LLM utilizes Vicuna-V1.5- 7b (Chiang et al. 2023). The client-specific adapter and taskspecific adapter parameters in stage 1 are initialized by the pre-trained MLP connector parameters in LLaVA. All crosstask adapter parameters in stage 2 are initialized by the local task-specific adapter obtained in stage 1. For all client text-adapter, we use the LoRA (Hu et al. 2021) parameter efficient tuning technique to train LLM. The rank of LoRA is 64 with a scalar $\\alpha = 1 2 8$ . During instruction tuning, we only fine-tune the parameters of the connector and LoRA while keeping the rest of the LLM frozen. The learning rates of stage 1 and stage 2 are set to 2e-5 and 4e-5, respectively. The number of text-adapter parameter aggregates Top- $M$ is set to 6 The coefficients of $\\lambda _ { 0 } , \\lambda _ { 1 }$ , and $\\lambda _ { 2 }$ are 0.1, 0.1, and 0.01, respectively. The number of local cycles $E$ is set to 1 and the number of communication rounds $R$ is 3. We train Pilot on 8 A100 GPUs (40G), with an effective batch size of 16 per GPU. To ensure fairness, for all baselines, all additional hyperparameters involved in the compared methods use their best settings.\n\n# Experimental Results\n\nFor the VQA task (including ScienceQA, GQA, and OCRVQA), we calculate the accuracy of predicting answers against ground truth. For the caption task, we report the CIDEr score. For the grounding task, we employ Intersection-over-Union (loU) as the evaluation metric. Specifically, a prediction is deemed accurate only when its loU exceeds or equals 0.5. Table 1 and Table 2 show the comparison of Pilot with other five methods in the federated scenarios of visual understanding and General VQA. Our framework outperforms all baselines in two task scenarios. We found that the performance of FedAVG method is lower than local training. This shows that the heterogeneity of multimodal tasks leads to greater parameter conflicts, and simple aggregation has a negative impact on local models. At the same time, we observed that FedAvg method is better than Shepherd. The former aggregates all training parameters, while the latter only aggregates LLM training parameters. This result also indirectly reflects the importance of visual information to the FedMIT task. Compared with FedDPA, only improving the LLM training parameters does not achieve the desired effect, which also illustrates the need to consider the differences between different tasks and the necessity of learning general knowledge from other tasks. Finally, the results prove that our method can not only overcome the heterogeneity between tasks, but also collaborate with all clients to improve the performance of local models.\n\n# Ablation Studies\n\nHere, we show the results of several variants of our method in the FL-oriented visual understanding scenario to demonstrate the effectiveness of the main modules of our method. We first analyze the impact of the four components of our framework (i.e., cross-task adapter, difference loss, auxiliary loss, and adaptive text-adapter aggregation (ATA)) on the model performance, and further evaluate the effectiveness of the proposed method. Table 3 shows the average score of all clients for the same task. We found that without using the above methods, the performance of our method is lower than that of local training, which does not have the ability to overcome task heterogeneity. Then we add adaptive text-adapter aggregation, and the performance of our method is improved and outperforms local training, which demonstrates that this module can effectively alleviate the impact of task heterogeneity. When we add the cross-task adapter, we observe that the model performance improves on all clients. Through auxiliary loss optimization, the performance of our framework can be further improved. The results show that the CT-MOA module is able to learn general knowledge from other tasks to improve model performance and cross-task capabilities. Removing the difference loss, our connector has only one and no longer distinguishes between client-specific adapter and task-specific adapter. The performance of the model has decreased, indicating that it is necessary to maintain the personalized information of the client. The above results demonstrate the importance of each component in our method.\n\n<html><body><table><tr><td rowspan=\"2\">ATA</td><td rowspan=\"2\"></td><td colspan=\"2\">Loss</td><td rowspan=\"2\">GQA</td><td rowspan=\"2\">CoCo</td><td rowspan=\"2\">RefCOCO</td></tr><tr><td>Lb+Lz</td><td>Ld</td></tr><tr><td>×</td><td>×</td><td>×</td><td>×</td><td>46.9</td><td>105.7</td><td>30.6</td></tr><tr><td>√</td><td>×</td><td>×</td><td>×</td><td>47.5</td><td>110.6</td><td>39.8</td></tr><tr><td>√</td><td>√</td><td>×</td><td>×</td><td>49.3</td><td>114.5</td><td>47.5</td></tr><tr><td>√</td><td>√</td><td>√</td><td>×</td><td>49.8</td><td>120.7</td><td>48.2</td></tr><tr><td>√</td><td>√</td><td>√</td><td>√</td><td>50.4</td><td>124.0</td><td>51.0</td></tr></table></body></html>\n\nTable 3: Ablation studies under FL-oriented visual understanding scenario.   \n\n<html><body><table><tr><td>Methods</td><td>GQA</td><td>CoCo</td><td>RefCOCO</td><td>AP</td><td>CP</td></tr><tr><td>Pilot</td><td>50.4</td><td>124.0</td><td>51.0</td><td>0.5B</td><td>0.3B</td></tr><tr><td>+ CT-CLIP</td><td>51.6</td><td>126.1</td><td>52.7</td><td>1.75B</td><td>0.5B</td></tr></table></body></html>\n\n# Further Remarks\n\nBuilding Cross-task CLIP. Our method modifies the MLLM connector to learn general knowledge from other tasks to improve model performance and cross-task capabilities. But the natural thought is: why not use CLIP? To answer this question, we unfreeze CLIP and train all MLP layers in CLIP in the same way as the connector, called CTCLIP. We conduct experiments on the visual understanding scenario for federated learning. Table 4 shows the average scores of all clients for the same task, where AP denotes local model Activation Params and CP denotes Communication Params. Experimental results show that for the FedMIT task, learning cross-task visual information from different tasks is an effective solution. We observe that although CTCLIP outperforms the Pilot, it comes at the expense of additional training parameters and communication parameters. Compared with Pilot, the communication parameters sent to the server increase by 0.2B, and the local client activation parameters increase by 1.25B. With the increase of tasks, the computational cost is unacceptable. Therefore, we give priority to the more simple and efficient method.\n\nTable 5: Compare different text-adapter parameter aggregation strategies.   \n\n<html><body><table><tr><td>Selection Strategy</td><td>GQA</td><td>CoCo</td><td>RefCOCO</td></tr><tr><td>Same task client</td><td>49.6</td><td>120.2</td><td>50.3</td></tr><tr><td>All clients</td><td>47.9</td><td>109.7</td><td>45.2</td></tr><tr><td>Pilot (M=5)</td><td>49.7</td><td>122.8</td><td>50.6</td></tr><tr><td>Pilot (M=6)</td><td>50.4</td><td>124.0</td><td>51.0</td></tr><tr><td>Pilot (M=7)</td><td>50.3</td><td>123.2</td><td>51.7</td></tr></table></body></html>\n\nTable 4: Impact of vision encoder improvements on model performance.   \nTable 6: Compare different cross-task adapter parameters initialization strategies.   \n\n<html><body><table><tr><td>InitializationStrategy</td><td>GQA</td><td>CoCo</td><td>RefCOCO</td></tr><tr><td>Random</td><td>50.1</td><td>122.7</td><td>50.3</td></tr><tr><td>Pilot</td><td>50.4</td><td>124.0</td><td>51.0</td></tr></table></body></html>\n\nDifferent Text-adapter Parameter Aggregation Strategies. In our framework, we adopt the adaptive text-adapter aggregation method. For all text adapter parameters, we adopt a weighted optimization aggregation strategy based on euclidean distance and select the Top- $M$ parameters for weighted averaging. We compared different aggregation strategies, such as aggregating only parameters of clients with the same task (same task client), aggregating parameters of all clients (all clients), and different Top- $M$ selections. As shown in Table 5, due to the heterogeneity between tasks, aggregating on all client will lead to parameters conflicts. In addition, aggregating only on the same task is a suboptimal solution because it does not utilize the semantic knowledge of other tasks. At the same time, we tested the impact of different top- $\\mathbf { \\nabla } \\cdot M$ on model performance, and the results showed that the adopted adaptive aggregation method can benefit from positive influences while reducing negative interference.\n\nCross-task Adapter Initialization Strategy. In our framework, the role of the cross-task adapter is to extract cross-task knowledge. We compared initialization with taskspecific adapter parameters with training from scratch (random). As shown in Table 6, training from scratch leads to performance degradation for all tasks. Using task-specific adapter parameters initialization provides a good starting point for the module and can help the local client better extract cross-task knowledge.\n\n# Conclusion\n\nIn this paper, we propose a federated multimodal instruction tuning framework to solve the new task of federated multimodal instruction tuning by collaboratively utilizing distributed data from different local clients to learn crosstask knowledge without being affected by task heterogeneity during instruction tuning. Through two stages “adapter on adapter” strategy, our model can capture the personalized information of local data and the task-related multimodal information, and can also adapt to the differences between different tasks. Our method achieves state-of-the-art results in two cross-task scenarios.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文探讨了一种新颖的联邦多模态指令调优任务（FedMIT），该任务对于在分布式设备上协作微调多模态大语言模型（MLLMs）具有重要意义。传统联邦指令调优（FedIT）方法仅考虑不同客户端执行不同的基于指令的自然语言处理任务，而在多模态领域的探索较少。FedMIT任务中，客户端包含不同的多模态指令调优任务（如视觉问答和图像描述），任务间的异构性大大增加，传统FedIT方法无法有效解决这种任务异构性。\\n> *   该问题的重要性在于，多模态任务（如视觉问答、图像描述和视觉定位）的集成在机器感知和理解世界中起着关键作用。语言是认知处理的基础，而视觉提供必要的感官信息。通过联邦学习框架，可以在保护数据隐私和安全的同时，协作利用分布式数据学习跨任务知识。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种联邦多模态指令调优框架（Pilot），该框架在视觉编码器和LLM的连接器中集成了两阶段“adapter on adapter”策略。第一阶段提取任务特定和客户端特定的视觉特征，第二阶段构建跨任务混合适配器（CT-MoA）模块进行跨任务交互。此外，还引入了文本训练参数的自适应参数聚合策略。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 提出并探索了联邦多模态指令调优任务（FedMIT），为在分布式设备上协作微调MLLMs提供了新思路。\\n> *   **贡献2：** 设计了联邦多模态指令调优框架（Pilot），通过两阶段“adapter on adapter”策略和自适应参数聚合策略，解决了任务异构性问题。\\n> *   **贡献3：** 在两种不同的跨任务场景下验证了Pilot的有效性。实验结果表明，Pilot在视觉理解场景和通用VQA场景中均优于现有基线方法，例如在GQA任务上准确率提升至51.4%，在COCO图像描述任务上CIDEr得分达到130.3。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   Pilot框架的核心思想是通过两阶段“adapter on adapter”策略，在联邦学习环境中解决多模态任务间的异构性问题。第一阶段专注于提取任务特定和客户端特定的视觉特征，第二阶段通过跨任务混合适配器（CT-MoA）模块学习跨任务交互知识。自适应参数聚合策略进一步优化了文本训练参数的聚合过程。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 传统FedIT方法仅考虑单模态客户端的异构性，无法直接应用于多模态任务。多模态任务的多样性大大增加了客户端间的异构性，导致传统方法性能下降。\\n> *   **本文的改进：** Pilot框架通过任务特定适配器和客户端特定适配器提取个性化信息，并通过CT-MoA模块学习跨任务知识。自适应参数聚合策略基于参数间的欧氏距离计算权重，优化参数聚合过程。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   **阶段1：任务特定特征挖掘**\\n>     1.  使用任务特定适配器（ψᵗ）提取任务特定视觉特征。\\n>     2.  使用客户端特定适配器（ψˢ）提取客户端特定视觉特征。\\n>     3.  通过差异损失（ℒ_d）确保任务特定和客户端特定特征的正交性。\\n> *   **阶段2：跨任务视觉交互**\\n>     1.  构建跨任务混合适配器（CT-MoA）模块，初始化来自其他任务的任务特定适配器。\\n>     2.  在CT-MoA模块中添加跨任务适配器（ψᶜ），缓解任务异构性导致的差异。\\n>     3.  使用路由器（ϕ）预测选择每个适配器的概率，并通过辅助损失（ℒ_b和ℒ_z）保持适配器负载平衡。\\n> *   **联邦优化**\\n>     1.  服务器采用任务感知的视觉适配器聚合策略，对相同任务的任务特定适配器参数进行加权平均。\\n>     2.  对文本训练参数采用自适应参数聚合策略，基于欧氏距离选择最近的M个参数进行加权聚合。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   FedAvg (2017)\\n> *   FedProx (2020)\\n> *   FedAdam (2020)\\n> *   Shepherd (2024b)\\n> *   FedDPA (2024c)\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在GQA任务准确率上：** Pilot在FL-oriented视觉理解场景中达到了51.4%，显著优于基线模型FedAvg (47.2%) 和 Shepherd (43.8%)。与表现最佳的基线FedDPA (49.3%) 相比，提升了2.1个百分点。\\n> *   **在COCO图像描述任务的CIDEr得分上：** Pilot达到了130.3，远高于FedAvg (86.5) 和 Shepherd (75.1)，与集中式训练结果相当。\\n> *   **在RefCOCO视觉定位任务的IoU指标上：** Pilot的IoU得分为65.3，显著优于FedProx (45.5) 和 FedDPA (48.2)。\\n> *   **在ScienceQA任务准确率上：** Pilot在FL-oriented通用VQA场景中达到了54.7%，优于FedDPA (52.3%) 和 FedAdam (50.1%)。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   联邦学习 (Federated Learning, FL)\\n*   多模态大语言模型 (Multimodal Large Language Model, MLLM)\\n*   指令调优 (Instruction Tuning, N/A)\\n*   跨任务混合适配器 (Cross-task Mixture-of-Adapters, CT-MoA)\\n*   自适应参数聚合 (Adaptive Parameter Aggregation, N/A)\\n*   视觉问答 (Visual Question Answering, VQA)\\n*   图像描述 (Image Captioning, N/A)\\n*   视觉定位 (Visual Grounding, N/A)\"\n}\n```"
}