{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.12970",
    "link": "https://arxiv.org/abs/2408.12970",
    "pdf_link": "https://arxiv.org/pdf/2408.12970.pdf",
    "title": "SUMO: Search-Based Uncertainty Estimation for Model-Based Offline Reinforcement Learning",
    "authors": [
        "Zhongjian Qiao",
        "Jiafei Lyu",
        "Kechen Jiao",
        "Qi Liu",
        "Xiu Li"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-08-23",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 4,
    "influential_citation_count": 0,
    "institutions": [
        "Tsinghua Shenzhen International Graduate School, Tsinghua University",
        "Harbin Institute of Technology, Shenzhen"
    ],
    "paper_content": "# SUMO: Search-Based Uncertainty Estimation for Model-Based Offline Reinforcement Learning\n\nZhongjian Qiao1, Jiafei Lyu1, Kechen Jiao1, Qi Liu 2, Xiu Li 1\n\n1Tsinghua Shenzhen International Graduate School, Tsinghua University 2Harbin Institute of Technology, Shenzhen qzj22, lvjf20, jkc22 @mails.tsinghua.edu.cn, qiliu $8 8 2 7 @$ stu.hit.edu.cn, li.xiu $@$ sz.tsinghua.edu.cn\n\n# Abstract\n\nThe performance of offline reinforcement learning (RL) suffers from the limited size and quality of static datasets. Model-based offline RL addresses this issue by generating synthetic samples through a dynamics model to enhance overall performance. To evaluate the reliability of the generated samples, uncertainty estimation methods are often employed. However, model ensemble, the most commonly used uncertainty estimation method, is not always the best choice. In this paper, we propose a Search-based Uncertainty estimation method for Model-based Offline RL (SUMO) as an alternative. SUMO characterizes the uncertainty of synthetic samples by measuring their cross entropy against the in-distribution dataset samples, and uses an efficient search-based method for implementation. In this way, SUMO can achieve trustworthy uncertainty estimation. We integrate SUMO into several model-based offline RL algorithms including MOPO and Adapted MOReL (AMOReL), and provide theoretical analysis for them. Extensive experimental results on D4RL datasets demonstrate that SUMO can provide accurate uncertainty estimation and boost the performance of base algorithms. These indicate that SUMO could be a better uncertainty estimator for model-based offline RL when used in either reward penalty or trajectory truncation.\n\nCode — https://github.com/qzj-debug/SUMO.git\n\n# Introduction\n\nOffline reinforcement learning (RL) (Levine et al. 2020; Prudencio, Maximo, and Colombini 2023) aims to learn the optimal policy from a static dataset collected in advance, avoiding the risks and costs associated with environmental interaction in typical RL (Sutton and Barto 2018). Nevertheless, since the datasets cannot cover the entire state-action space, the offline agent cannot accurately estimate the Q-value for out-of-distribution (OOD) samples, ultimately leading to a degradation in the agent’s performance.\n\nModel-based offline RL (Yu et al. 2020, 2021; Kidambi et al. 2020) employs a promising idea to address OOD issues. By leveraging an environmental dynamics model obtained by supervised learning, the agent can collect samples within the dynamics model and train the policy using these samples. This significantly enhances the performance and generalization of the agent. However, synthetic samples generated by the dynamics model may not be reliable (Lyu, Li, and Lu 2022), as the dynamics model’s reliability in OOD regions is not guaranteed. Training the policy with unreliable samples can lead to a performance decline. Therefore, evaluating the reliability of generated samples is critical. It is common to assess the reliability with uncertainty estimation (Lockwood and Si 2022; An et al. 2021). Current model-based offline RL methods often employ model ensemble-based techniques for uncertainty estimation, such as max aleatoric (Yu et al. 2020) and max pairwise diff (Kidambi et al. 2020). Subsequently, the estimated uncertainty can be used for trajectory truncation (Kidambi et al. 2020; Zhang et al. 2023b) or reward penalties (Yu et al. 2020) to mitigate OOD issues. However, model ensemble-based uncertainty estimation methods may be unreliable (Yu et al. 2021) because the models can be poorly learned, making their uncertainty estimation questionable. We wonder: Can we design a better uncertainty estimation method for modelbased offline RL?\n\n![](images/acaa93bee601ae47b3b2b2a53cdf445057260242579885b520b44cca0ae81f01.jpg)  \nFigure 1: Left: The key idea of SUMO. For a synthetic sample, we calculate its KNN distance within the dataset as the uncertainty estimation. Right: An example of combining SUMO with AMOReL for trajectory truncation. Thanks to the accurate uncertainty estimation provided by SUMO, the agent can explore ID regions while avoiding OOD regions.\n\nIn this paper, we propose a Search-based Uncertainty estimation method for Model-based Offline RL (SUMO). SUMO characterizes the uncertainty of synthetic samples as the cross entropy between model dynamics and true dynamics, which can be shown as a more reasonable uncertainty estimation than model ensemble-based estimation. Moreover, the estimated uncertainty for a given sample does not involve extra training of neural networks. In contrast, the uncertainty estimated by model ensemble methods is correlated with the training process and training data distribution since they need to train dynamics models parameterized by neural networks. To estimate the cross entropy practically, we employ a particle-based entropy estimator (Singh et al. 2003), transforming the problem into a $k$ - nearest neighbor (KNN) search problem, as shown in Figure 1 (left), that is the reason we call SUMO a search-based method. Furthermore, given the large search space and high data dimensionality, we employ FAISS (Johnson, Douze, and Je´gou 2019) to ensure efficient KNN search. We note that SUMO is algorithm-agnostic, allowing us to integrate it with any model-based offline RL algorithm which needs uncertainty estimation. For instance, it can be combined with MOPO (Yu et al. 2020) or with Adapted MOReL (Kidambi et al. 2020) (AMOReL, which we discuss in later sections), as shown in Figure 1 (right).\n\nWe integrate SUMO with several off-the-shelf modelbased offline RL algorithms like MOPO and AMOReL and theoretically analyze their performance bounds after introducing SUMO. Empirically, we conduct extensive experiments on the D4RL (Fu et al. 2020) benchmark, and the experimental results indicate that SUMO can significantly enhance the performance of base algorithms. We also show that SUMO can provide more accurate uncertainty estimation than commonly used model ensemble-based methods. Our contributions can be summarized as follows:\n\n• We propose a novel search-based uncertainty estimation method for model-based offline RL, SUMO. • We combine SUMO with AMOReL and MOPO, and provide theoretical performance bounds. • We empirically demonstrate that SUMO incurs accurate uncertainty estimation and can bring significant performance improvement over the base algorithms on numerous D4RL datasets.\n\n# Background\n\nReinforcement Learning (RL). We consider a Markov Decision Process (MDP) (Garcia and Rachelson 2013) modeled by $\\mathcal { M } = \\langle S , A , r , P , \\rho , \\gamma \\rangle$ , where $S$ is the state space, $A$ is the action space, $r$ is the reward function: $S \\times A \\to \\mathbb { R }$ , $P$ is the transition dynamics: $S \\times A \\times S \\to [ 0 , 1 ]$ , $\\rho$ is the initial state distribution, and $\\gamma \\in [ 0 , 1 )$ is the discount factor. RL aims to get a policy $\\pi _ { \\boldsymbol { \\theta } } ( a | \\boldsymbol { s } )$ that maximizes $\\begin{array} { r } { \\operatorname* { m a x } _ { \\pi _ { \\theta } } \\mathbb { E } _ { \\pi _ { \\theta } } \\left[ \\sum _ { t = 0 } ^ { \\infty } \\tilde { \\gamma } ^ { t } r ( s _ { t } , a _ { t } ) | s _ { 0 } \\sim \\rho \\right] } \\end{array}$ . We $J _ { \\rho } ( \\pi , \\mathcal { M } ) =$ $\\mathcal { M }$ $J _ { \\rho } ( \\pi , \\mathcal { M } )$ $\\mathcal { M }$\n\nOffline RL. In offline RL, the agent aims to learn the optimal batch-constraint policy based on a static dataset $\\mathcal { D } =$ $\\{ ( s _ { i } , a _ { i } , r _ { i } , s _ { i + 1 } ) \\} _ { i = 1 } ^ { N }$ , where the samples are collectedD by a (unknown) behavior policy $\\mu$ , without accessing the environment. Since the dataset can not cover the entire stateaction space, the performance of offline RL is often limited. Model-based Offline RL. Model-based offline RL leverages the learned dynamics model $\\boldsymbol { P } _ { \\widehat { \\mathcal { M } } } ( \\cdot | s , a )$ to generate synthetic samples. This allows us to train the policy using both dataset transitions and samples generated by the dynamics model. We denote the model MDP as $\\widehat { \\mathcal { M } }$ , the state distribution probability at timestep $t$ followincg policy $\\pi$ and dynamics $P _ { \\widehat { \\mathcal { M } } }$ as $\\mathbb { P } _ { \\widehat { \\mathcal { M } } , t } ^ { \\pi } ( s )$ , and discounted occupancy measure of $\\pi$ ucnder dyncamics $P _ { \\widehat { \\mathcal { M } } }$ as $\\hat { \\rho } ^ { \\pi } ( s , a ) : =$ $\\begin{array} { r } { \\pi ( { a } | { s } ) \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } \\mathbb { P } _ { \\widehat { \\mathcal { M } } , t } ^ { \\pi } ( { s } ) . } \\end{array}$ .\n\n# Related Work\n\nModel-free Offline RL. Model-free offline RL aims to train an agent merely on a static dataset without a dynamics model. The major challenge for model-free offline RL is extrapolation error (Fujimoto, Meger, and Precup 2019; Kumar et al. 2020; Lyu et al. 2022) of the Q-function due to the inability to explore. Existing methods introduce conservatism to the policy (Fujimoto and Gu 2021; Kumar et al. 2019; Fujimoto, Meger, and Precup 2019) or Q-function (Kumar et al. 2020; Lyu et al. 2022; An et al. 2021; Yang et al. 2024) to tackle the issue. However, the involvement of conservatism restricts the generalization ability of the policy.\n\nModel-based Offline RL. Model-based offline RL leverages a dynamics model to extend the dataset and enhance the generalization ability. Since the dynamics model may not be accurate on all transitions, conservatism is still necessary for learning a good policy. Some works (Yu et al. 2020; Kidambi et al. 2020) incorporate conservatism into the generated samples, while other works (Yu et al. 2021; Rigter, Lacerda, and Hawes 2022; Liu et al. 2023) introduce conservatism to Q-function.\n\nUncertainty Estimation in Offline RL. In model-free offline RL, uncertainty measures the distance of samples from the dataset distribution. EDAC (An et al. 2021) leverages an ensemble of Q-networks to estimate the uncertainty. DARL (Zhang et al. 2023a) computes the KNN distances between the samples and the dataset as uncertainty estimation. In model-based offline RL, uncertainty measures the discrepancy between the simulated dynamics and the true dynamics. MOPO (Yu et al. 2020) and MOReL (Kidambi et al. 2020) estimate the uncertainty by model ensemble. MOBILE (Sun et al. 2023) quantifies the uncertainty through model-bellman inconsistency. All of these uncertainty estimation methods rely on model ensemble, while our method does not. Some other uncertainty estimation methods (Kim and Oh 2023; Tennenholtz and Mannor 2022) suffer from heavy computational burden and complex implementation, while our method is both efficient and easy to implement.\n\n# Methodology Search-Based Method for Uncertainty Estimation\n\nIn this part, we formally present our search-based uncertainty estimation method for model-based offline RL. Firstly, we outline how we model the uncertainty estimation problem as an estimation of cross entropy. Subsequently, we employ a particle-based entropy estimator, which approximates the calculation of cross entropy as a search problem.\n\nWe characterize the uncertainty of the model dynamics as its cross entropy against the true dynamics:\n\n$\\begin{array} { r } { \\mathcal { H } ( P _ { \\widehat { \\mathcal { M } } } ( \\cdot | s , a ) , P ( \\cdot | s , a ) ) = - \\sum P _ { \\widehat { \\mathcal { M } } } ( \\cdot | s , a ) \\log P ( \\cdot | s , a ) } \\end{array}$ , wher ec $P _ { \\widehat { \\mathcal { M } } } ( \\cdot | s , a )$ and $P ( \\cdot | s , a )$ arec the model dynamics and thce true dynamics, respectively. It is hard to calculate $\\mathcal { H } ( P _ { \\widehat { \\mathcal { M } } } ( \\cdot | s , \\cdot \\bar { a } ) , P ( \\cdot | s , a ) )$ since we have no knowledge about cthe true dynamics $P ( \\cdot | s , a )$ . We therefore compute the cross entropy between the model dynamics and the dataset dynamics: $\\mathcal { H } ( P _ { \\widehat { \\mathcal { M } } } ( \\cdot | s , a ) , P _ { d } ( \\cdot | s , \\cdot \\bar { a } ) ) =$ $- \\sum P _ { \\widehat { \\mathcal { M } } } ( \\cdot | s , a ) \\log P _ { d } ( \\cdot | s , a )$ , wherec $P _ { d } ( \\cdot | s , a )$ denotes the dataset tcransition dynamics of dataset MDP, which is formally defined below.\n\nDefinition 1 (dataset MDP). The dataset MDP is defined by the tuple $\\hat { \\mathcal { M } } _ { d } = ( S , A , r , P _ { d } , \\rho _ { d } , \\gamma )$ , where $S$ , A, $r$ and $\\gamma$ are the same as the original MDP. The dataset transition dynamics $P _ { d }$ is defined as:\n\n$$\n\\begin{array} { r } { P _ { d } ( \\cdot | s , a ) = \\left\\{ \\begin{array} { l r } { P ( \\cdot | s , a ) , } & { \\quad \\mathrm { i f } \\ ( s , a ) \\in \\mathcal { D } , } \\\\ { 0 , } & { \\quad \\mathrm { o t h e r w i s e } , } \\end{array} \\right. } \\end{array}\n$$\n\nand $\\rho _ { d }$ is the state distribution of the dataset.\n\nRemark: Intuitively, $P _ { d }$ only accounts for transitions that lie in the dataset; we set the probability for transitions not in the dataset to be zero. Note that the summation of $P _ { d }$ for a given state-action pair $( s , a )$ may be not 1, we can scale $P _ { d }$ accordingly to make it a valid probability density function.\n\nThen based on the definition of cross entropy, we have $\\begin{array} { r } { \\mathcal { H } ( P _ { \\widehat { \\mathcal { M } } } , P _ { d } ) ~ = ~ - \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\log P _ { d } ( x _ { i } ) } \\end{array}$ , where $\\bar { \\{ x _ { i } \\} } _ { i = 1 } ^ { n } \\sim$ $P _ { \\widehat { \\mathcal { M } } }$ .McAccording to thPe particle-based entropy estimator Pd(xi) = mkπΓd(/d2/R2id+,k1,)m , where $d$ is the dimension of $\\{ y _ { j } \\} _ { j = 1 } ^ { m } \\sim P _ { d }$ , we have $x _ { i }$ , $\\Gamma$ is the Gamma function, and $R _ { i , k , m } ^ { d } = \\| x _ { i } - x _ { i } ^ { k , m } \\| _ { 2 } ^ { d }$ represents the distance between $x _ { i }$ and the $k$ -nearest neighbor of $x _ { i }$ in $\\{ y _ { j } \\} _ { j = 1 } ^ { m }$ . Then, we can compute the cross entropy as:\n\n$$\n\\begin{array} { c } { \\displaystyle \\mathcal { H } ( P _ { \\widehat { \\mathcal { M } } } , P _ { d } ) = - \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\log \\frac { k \\Gamma ( d / 2 + 1 ) } { m \\pi ^ { d / 2 } R _ { i , k , m } ^ { d } } } \\\\ { \\displaystyle \\propto \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\log \\| x _ { i } - x _ { i } ^ { k , m } \\| _ { 2 } } \\end{array}\n$$\n\nNotice that Equation (2) is an uncertainty estimation of the model dynamics. To estimate the uncertainty of a sample $x _ { i }$ generated by the dynamics model, we can assume $P _ { \\widehat { \\mathcal { M } } }$ is a delta distribution, where $P _ { \\widehat { \\mathcal { M } } } ( x _ { i } ) = \\infty$ , $P _ { \\widehat { \\mathcal { M } } } ( x ) = \\mathrm { ~ 0 ~ }$ for any $\\boldsymbol { x } ~ \\neq ~ \\boldsymbol { x } _ { i }$ . Thus, we tra ncsform the problcem of estimating the uncertainty of synthetic samples into a KNN search problem. To be specific, given an offline dataset $\\mathcal { D } =$ $\\{ ( s _ { i } , \\stackrel { . } { a _ { i } } , r _ { i } , s _ { i + 1 } ) \\} _ { i = 1 } ^ { N }$ and a synthetic sample $( \\hat { s } , \\hat { a } , \\hat { r } , \\hat { s } ^ { \\prime } )$ , we can estimate the uncertainty of the synthetic sample as:\n\n$$\nu ( \\hat { s } , \\hat { a } , \\hat { s } ^ { \\prime } ) = \\log \\big ( \\| ( \\hat { s } \\oplus \\hat { a } \\oplus \\hat { s } ^ { \\prime } ) - ( \\hat { s } \\oplus \\hat { a } \\oplus \\hat { s } ^ { \\prime } ) ^ { k , N } \\| _ { 2 } \\big )\n$$\n\nwhere $\\oplus$ is the vector concatenation operator, $( \\hat { s } \\oplus \\hat { a } \\oplus \\hat { s } ^ { \\prime } ) ^ { k , N }$ returns the nearest neighbor of the query sample $( \\hat { s } , \\hat { a } , \\hat { r } , \\hat { s } ^ { \\prime } )$ in the offline dataset, $k$ is the number of neighbors for KNN search, and $N$ is the number of samples in the dataset. To ensure the estimated uncertainty is non-negative, we practically compute the uncertainty as:\n\n$$\nu ( \\hat { s } , \\hat { a } , \\hat { s } ^ { \\prime } ) = \\log \\big ( \\| ( \\hat { s } \\oplus \\hat { a } \\oplus \\hat { s } ^ { \\prime } ) - ( \\hat { s } \\oplus \\hat { a } \\oplus \\hat { s } ^ { \\prime } ) ^ { k , N } \\| _ { 2 } + 1 \\big )\n$$\n\nNote that one can also involve the rewards in Equation (4) for computing the uncertainty, which we empirically do not find obvious performance difference in Appendix D.3. To ensure time efficiency, we employ FAISS (Johnson, Douze, and Je´gou 2019), an efficient GPU-based KNN search method to estimate the uncertainty. Note that using KNN search to calculate distances between given samples and datasets is a widely used approach in model-free offline RL (Zhang et al. 2023a; Ran et al. 2023; Lyu et al. 2024). However, they often rely on constraining the distance between the learned policy and the behavior policy, and the concatenated vector only includes dimensions of $s$ and $a$ . In contrast, for model-based offline RL, what matters is the discrepancy between the model dynamics and the true dynamics. Therefore, we need to consider the influence of $s ^ { \\prime }$ .\n\nWe then discuss the advantage of SUMO compared with model ensemble-based methods. First, SUMO estimates the uncertainty in an unsupervised learning manner, meaning that the estimated uncertainty is stable and not correlated with the training process. Second, model ensemble-based uncertainty estimation is an empirical approximation. A mismatch can be observed in theoretical analysis of prior works, e.g., $D _ { T V } ( P _ { \\widehat { M } } ( \\cdot | s , a ) , P ( \\cdot | s , a ) ) \\ \\leq \\ \\dot { \\alpha }$ is required in MOReL, where $\\Dot { D _ { T V } }$ is the total variation distance and $\\alpha$ is the threshold. Model ensemble-based uncertainty estimation can only practically approximate the bound of $D _ { T V }$ with $u$ . In contrast, SUMO provides an unbiased estimate of cross entropy, relationships between $u$ and $D _ { T V }$ can be established using Pinsker’s inequality (Csisza´r and Ko¨rner 2011), as shown in Appendix A.2.\n\n# Integrating SUMO into Existing Methods\n\nAs an uncertainty estimation method, SUMO is compatible with any model-based offline RL algorithm that requires an estimation of uncertainty. There are two typical ways of using uncertainty in model-based offline RL, using the uncertainty as the reward penalty, e.g., MOPO (Yu et al. 2020), and using the uncertainty to truncate synthetic trajectories from the learned model, e.g., MOReL (Kidambi et al. 2020). In this part, we introduce the procedure of combining SUMO with MOPO and Adapted MOReL, respectively.\n\nMOPO with SUMO MOPO adopts the uncertainty as the reward penalty, and we can simply replace the uncertainty estimator as the SUMO estimator. The detailed pseudocode of MOPO+SUMO can be found in Appendix B, and the detailed process can summarized as:\n\nStep1: Learning the Environmental Dynamics Model: To generate synthetic samples for training, we construct an environmental dynamics model to simulate the true dynamics. Following previous works (Yu et al. 2020; Kidambi et al. 2020), we model the dynamics model as a multi-layer neural network which outputs a Gaussian distribution over the next state and reward: $\\hat { P } _ { \\psi } ( s ^ { \\prime } , r | s , a ) = \\mathcal { N } \\left( \\mu _ { \\psi } ( s , a ) , \\Sigma _ { \\psi } ( s , a ) \\right)$ . We use an ensemble dynamics model which consists of $N$ ensemble members: $\\hat { P _ { \\psi } } = \\{ \\hat { P } _ { \\psi _ { i } } \\} _ { i = 1 } ^ { N }$ . It is worth mentioning that we use the ensemble dynamics model solely to reduce the error in model predictions, not for estimating the uncertainty. Then we can train each ensemble member using the offline dataset via maximum log-likelihood:\n\n$$\n\\mathcal { L } _ { \\psi _ { i } } = \\mathbb { E } _ { ( s , a , r , s ^ { \\prime } ) \\sim \\mathcal { D } } \\left[ - \\log \\hat { P } _ { \\psi _ { i } } ( s ^ { \\prime } , r | s , a ) \\right]\n$$\n\nStep2: Reward Penalty Calculation with SUMO: We utilize the current policy $\\pi$ and the dynamics model $\\hat { P } _ { \\psi }$ to generate synthetic samples. For each generated sample $( s , \\bar { a } , s ^ { \\prime } )$ , we first calculate its unnormalized uncertainty with SUMO via Equation (4). We assume that the reward function satisfies: $0 ^ { - } \\leq r ( s , a ) \\leq r _ { m a x } , \\forall ( s , a )$ . This is easy to achieve in practice. Then we normalize $u ( s , a , s ^ { \\prime } )$ to the range $[ 0 , r _ { m a x } ]$ :\n\n$$\n\\hat { u } ( s , a , s ^ { \\prime } ) = \\frac { u ( s , a , s ^ { \\prime } ) } { \\operatorname* { m a x } _ { ( s , a , s ^ { \\prime } ) } u ( s , a , s ^ { \\prime } ) } \\times r _ { m a x }\n$$\n\nWe adopt $\\hat { u }$ to penalize the rewards, $\\boldsymbol { \\hat { r } } = \\boldsymbol { r } - \\lambda \\boldsymbol { \\hat { u } }$ , where $\\lambda$ is a hyperparameter that controls the magnitude of the penalty. We can then add the synthetic samples with reward penalty to the synthetic dataset $\\mathcal { D } _ { m o d e l }$ .\n\nStep 3: Model-based Policy Optimization: We then follow MOPO to train SAC (Haarnoja et al. 2018) agent with samples drawn from the offline dataset $\\mathcal { D }$ and synthetic dataset $\\mathcal { D } _ { m o d e l }$ . Typically, we set a sampling coefficient $\\eta \\in [ 0 , 1 ]$ , sampling a proportion of $\\eta B$ from the offline dataset $\\mathcal { D }$ , and a proportion of $( 1 - \\eta ) B$ from the synthetic dataset $\\mathcal { D } _ { m o d e l }$ given the batch size $\\boldsymbol { B }$ .\n\nAdapted MOReL with SUMO MOReL calculates model ensemble discrepancy as an uncertainty estimation to determine whether a state-action pair $( s , a )$ is within a known region and penalizes samples from unknown regions. We can simply replace the uncertainty estimation method with SUMO to determine whether a transition $( s , a , s ^ { \\prime } )$ is reliable. In our practical implementation, we make slight modifications to MOReL. Specifically, when encountering unreliable samples, we directly truncate the trajectory instead of applying a constant penalty. This ensures the reliability of the samples used for training. And we update the policy via SAC (Haarnoja et al. 2018) instead of planning. We refer to this modified version as Adapted MOReL (AMOReL), and divide the process of SUMO with AMOReL into three steps: Step 1: Constructing the Environmental Dynamics Model: Following the same procedure of $\\mathrm { M O P O + S U M O }$ , we train an ensemble dynamics model $\\hat { P } _ { \\psi } = \\{ \\hat { P } _ { \\psi _ { i } } \\} _ { i = 1 } ^ { N }$ .\n\nStep 2: Trajectory Truncation with SUMO: We then utilize the current policy $\\pi$ and dynamics model $\\hat { P } _ { \\psi }$ to generate synthetic samples. We use SUMO to measure the uncertainty of the samples and truncate the synthetic trajectory accordingly. In specific, in the process of generating synthetic trajectories, we apply Equation (4) to estimate the uncertainty of each generated sample $( \\hat { s } , \\hat { a } , \\hat { s } ^ { \\prime } )$ and set a truncating threshold $\\epsilon$ . If the uncertainty of any sample exceeds this threshold, we consider the sample unreliable and stop generating the trajectory, adding the generated trajectory to the synthetic dataset $\\mathcal { D } _ { m o d e l }$ . In this way, we ensure using only reliable samples for training. It is important to decide how to choose the truncating threshold $\\epsilon$ , we can choose to set the threshold as a constant; however, since data distributions and dimensions may vary across different datasets, setting it as a constant might not be the optimal choice. Therefore, we set the threshold to be the maximum uncertainty among all the dataset samples. In specific, for a dataset $\\mathcal { D }$ , we compute the KNN distances for each sample in $\\mathcal { D }$ to other samples. We take the maximum uncertainty value as the truncating threshold. For flexibility, we can also multiply the threshold $\\epsilon$ by a coefficient $\\alpha$ for adjustment:\n\n$$\n\\epsilon = \\alpha \\cdot \\operatorname* { m a x } _ { \\mathcal { D } } \\log \\big ( \\| ( s \\oplus a \\oplus s ^ { \\prime } ) - ( s \\oplus a \\oplus s ^ { \\prime } ) ^ { k , N } \\| _ { 2 } + 1 \\big )\n$$\n\nwhere $\\alpha$ is a hyperparameter, a larger $\\alpha$ makes the algorithm more optimistic, using more synthetic samples for training.\n\nStep 3: Model-based Policy Optimization: We then draw samples from $\\mathcal { D } \\cup \\mathcal { D } _ { m o d e l }$ with a sampling coefficient $\\eta$ to train an SAC agent following Step 3 of MOPO $^ +$ SUMO.\n\nWe summarize the full pseudocode of AMOReL $^ +$ SUMO in Appendix B.\n\n# Theoretical Analysis\n\nIn this part, we provide theoretical analysis for SUMO. Due to space limit, missing proofs are deferred to Appendix A.\n\nWe first provide theoretical analysis when leveraging SUMO for penalizing rewards in MOPO. We show that the policy learned by the MOPO $+$ SUMO has a performance guarantee as follows:\n\nTheorem 1 (Informal). Denote the behavior policy of the dataset as $\\mu ,$ , the uncertainty estimator $u ( s , a , s ^ { \\prime } )$ defined in Equation (4), then under some mild assumptions, $M O P O + S U M O$ incurs the policy $\\pi$ that satisfies:\n\n$$\nJ _ { \\rho } ( \\pi ) \\geq J _ { \\rho } ( \\mu ) - 2 \\lambda \\mathbb { E } _ { \\hat { \\rho } ^ { \\pi } } [ u ( s , a , s ^ { \\prime } ) ] ,\n$$\n\nwhere $\\lambda \\in \\mathbb { R }$ is the penalty coefficient term in MOPO, ${ \\hat { \\rho } } ^ { \\pi }$ is the discounted occupancy measure of policy $\\pi$ . This theorem states that in the original MDP $\\mathcal { M }$ , the policy induced by $\\mathbf { M O P O + S U M O }$ can be at least as good as the behavior policy if the uncertainty is small.\n\nWe then present theoretical guarantees when using SUMO for trajectory truncation in AMOReL. We first define the $\\epsilon$ -Model MDP:\n\nDefinition 2 (ϵ-Model MDP). $\\epsilon$ -Model MDP is defined by the tuple $\\widehat { \\mathcal { M } } _ { \\epsilon } = \\langle S , A , r , \\hat { P } _ { \\epsilon } , \\hat { \\rho } _ { \\epsilon } , \\gamma \\rangle$ , where $S , A ,$ , $r$ and $\\gamma$ are the same acs the original MDP, $\\hat { \\rho } _ { \\epsilon }$ is the state distribution of the learned dynamics model. The transition dynamics $\\hat { P } _ { \\epsilon }$ is defined as:\n\n$$\n\\begin{array} { r } { \\hat { P } _ { \\epsilon } ( s ^ { \\prime } | s , a ) = \\left\\{ \\begin{array} { l l } { P _ { \\widehat { \\mathcal { M } } } ( s ^ { \\prime } | s , a ) , \\quad } & { \\mathrm { i f } u ( s , a , s ^ { \\prime } ) < \\epsilon , } \\\\ { 0 , } & { \\mathrm { o t h e r w i s e } , } \\end{array} \\right. } \\end{array}\n$$\n\nIntuitively, $\\widehat { \\mathcal { M } } _ { \\epsilon }$ only contains transitions satisfying $u ( s , a , s ^ { \\prime } ) \\dot { < } \\epsilon$ . Icn this way, we ensure the reliability of synthetic samples for training. We note that the summation of $\\hat { P } _ { \\epsilon }$ may be not 1, and we can scale it to make it a valid probability density function. Then we present our theoretical results of performance bounds for $\\epsilon$ -Model MDP.\n\nTheorem 2 (Performance bounds). Denote $\\rho _ { d }$ as the state distribution of the dataset. For any policy $\\pi$ , its return in the $\\epsilon$ -Model MDP $\\widehat { \\mathcal { M } } _ { \\epsilon }$ and the original MDP $\\mathcal { M }$ (with state distribution $\\rho$ ) sat fcies:\n\n$$\n\\begin{array} { l } { \\displaystyle { J _ { \\hat { \\rho } _ { \\epsilon } } \\big ( \\pi , \\widehat { \\mathcal { M } } _ { \\epsilon } \\big ) \\geq J _ { \\rho } \\big ( \\pi , \\mathcal { M } \\big ) - \\frac { r _ { m a x } } { 1 - \\gamma } \\big ( 1 + \\sqrt { \\frac { \\epsilon } { 2 } } + 2 D _ { T V } \\big ( \\rho , \\rho _ { d } \\big ) } } \\\\ { \\displaystyle ~ + 2 D _ { T V } \\big ( \\hat { \\rho } _ { \\epsilon } , \\rho _ { d } \\big ) \\big ) , } \\\\ { \\displaystyle J _ { \\hat { \\rho } _ { \\epsilon } } \\big ( \\pi , \\widehat { \\mathcal { M } } _ { \\epsilon } \\big ) \\leq J _ { \\rho } \\big ( \\pi , \\mathcal { M } \\big ) + \\frac { r _ { m a x } } { 1 - \\gamma } \\big ( \\sqrt { \\frac { \\epsilon } { 2 } } + 2 D _ { T V } \\big ( \\rho , \\rho _ { d } \\big ) } \\\\ { \\displaystyle ~ + 2 D _ { T V } \\big ( \\hat { \\rho } _ { \\epsilon } , \\rho _ { d } \\big ) \\big ) } \\end{array}\n$$\n\nRemark: Theorem 2 states that the performance difference for any policy $\\pi$ in $\\epsilon$ -Model MDP and the original MDP is related to three factors: (1) the truncation threshold $\\epsilon$ , which determines the synthetic samples for training. (2) the total variance distance between the true and dataset state distribution $D _ { T V } ( \\rho , \\rho _ { d } )$ . (3) the total variance distance between $\\epsilon$ -Model and dataset state distribution $D _ { T V } \\big ( \\hat { \\rho } _ { \\epsilon } , \\rho _ { d } \\big )$ .\n\n# Experiment\n\nIn our experimental part, we empirically evaluate our method, SUMO. We aim to answer the following questions: (1) How much performance gain can SUMO bring to off-the-shelf model-based offline RL algorithms? (2) Can SUMO provide more accurate uncertainty estimation compared to model ensemble-based methods? (3) How different design choices affect the performance of SUMO? (4) How sensitive is SUMO to the introduced hyperparameters?\n\nFor evaluation, we use the D4RL MuJoCo datasets which includes three tasks: halfcheetah, hopper and walker2d. Each task provides five types of offline datasets: random, medium, medium-replay, medium-expert and expert. We conduct experiments on 15 D4RL MuJoCo datasets to comprehensively evaluate the performance of SUMO across various tasks and datasets of different qualities.\n\n# Experimental Results on the D4RL Benchmark\n\nSUMO is an uncertainty estimation method designed for model-based offline RL, it can be seamlessly combined with any model-based offline RL algorithm which needs uncertainty estimation. In this part, we combine SUMO with several model-based offline RL algorithms, including MOPO, AMOReL, MOReL and MOBILE (Sun et al. 2023). We conduct extensive experiments on widely used D4RL MuJoCo datasets and examine whether SUMO can bring performance improvement to these base algorithms. We run all experiments with 5 different random seeds.\n\nWe summarize the experimental results in Table 1, where we observe that SUMO significantly boosts the performance of base algorithms. Notably, MOPO $^ +$ SUMO outperforms vanilla MOPO in 11 out of 15 tasks, AMOReL $^ { + }$ SUMO and MOReL $+$ SUMO surpasses the original AMOReL and MOReL in 10 out of 15 tasks, MOBILE $^ { + }$ SUMO achieves a higher score than MOBILE in 9 out of 15 tasks. Regarding the average score on all 15 tasks, SUMO brings an overall performance improvement for all four base algorithms, indicating the versatility of SUMO. We include more comparison with model-based methods without uncertainty estimation such as COMBO (Yu et al. 2021) and RAMBO (Rigter, Lacerda, and Hawes 2022) in Appendix D.5.\n\nWe also evaluate our methods on D4RL Antmaze tasks, which are challenging for model-based offline RL methods (Wang et al. 2021). Due to space limit, we present the results in Appendix D.1. We find that base model-based RL algorithms all struggle to perform well in the Antmaze domains. When combined with SUMO, the performance of base algorithms has been improved. We believe these further demonstrate the strengths of SUMO.\n\n# Comparison with other Uncertainty Estimators\n\nIn this section, we aim to demonstrate the superiority of SUMO in a more intuitive manner, i.e., whether SUMO can provide more accurate uncertainty estimation compared to widely used model ensemble-based uncertainty estimation methods. This is crucial and necessary for validating our central claim in this paper.\n\nFor model ensemble-based uncertainty estimation methods, the core idea is to measure the uncertainty of generated samples by leveraging the diverse predictions of ensemble members regarding the environmental dynamics. We choose the following model ensemble-based uncertainty estimation methods for comparison, which come from recent literature in both offline and online model-based RL:\n\nMax Aleatoric: It measures the sample uncertainty as: $\\begin{array} { r } { u ( s , a ) = \\operatorname* { m a x } _ { i = 1 , \\dots , N } \\left. \\Sigma _ { \\psi _ { i } } ( s , a ) \\right. _ { F } } \\end{array}$ , where $\\Sigma _ { \\psi _ { i } }$ is the covariance matrix predicted by the $i$ -th ensemble member, and $\\left\\| \\cdot \\right\\| _ { F }$ denotes the Frobenius norm. This uncertainty heuristic is used in MOPO and is related to the aleatoric uncertainty.\n\nMax Pairwise Diff: It measures the sample uncertainty as: $\\begin{array} { r } { u ( s , a ) = \\operatorname* { m a x } _ { i , j } \\left\\| \\mu _ { \\psi _ { i } } ( s , a ) - \\mu _ { \\psi _ { j } } ( s , a ) \\right\\| _ { 2 } , , } \\end{array}$ $\\mu _ { \\psi _ { i } }$ is the mean vector predicted b\ny the $i$ -th ensemble me\nmber. This uncertainty heuristic is used in MOReL and measures the maximum pairwise difference between ensemble predictions.\n\nLOO (Leave-One-Out) KL Divergence: It measures the uncertainty as: $u ( s , a ) \\ = \\ D _ { K L } ( \\hat { P _ { \\psi _ { i } } } ( \\cdot | s , a ) | | \\hat { P } _ { \\psi _ { - i } } ( \\cdot | s , a ) )$ where $\\hat { P } _ { \\psi _ { - i } } ( \\cdot | s , a )$ represents the aggregated Gaussian distribution of all ensemble members except the $i$ -th member. It is used in M2AC (Pan et al. 2020).\n\nWe then compare the ability of these four methods in detecting out-of-distribution (OOD) samples. In offline RL, OOD samples refer to state-action pairs that lie out of the sample distribution of the dataset. It is difficult to decide whether a transition is truly OOD. Nevertheless, things are easier in model-based offline RL, mainly due to the fact that it involves learning an environmental dynamics model. The difference between the simulated environmental dynamics and the real environmental dynamics can be a signal for detecting OOD samples. In practice, we start from the tuple $( s , a )$ sampled from the offline dataset and generate its next state $\\hat { s } ^ { \\prime }$ via the learned dynamics model. We deem that the transition $( s , a , \\hat { s } ^ { \\prime } )$ is OOD if $\\hat { s } ^ { \\prime }$ significantly differs from the dynamics of the real environment. A good uncertainty estimation method should be sensitive to unrealistic environmental dynamics. In other words, when there is a large error in the predicted environmental dynamics, it should provide a higher uncertainty estimate, and when the error is small, it should have a lower uncertainty estimate.\n\nEmpirically, we conduct experiments on the 9 datasets from D4RL MuJoCo tasks. On each dataset, we first train an ensemble dynamics model via supervised learning. Then we train a policy inside the model following the process of MOPO, without adding any penalty. To generate synthetic samples, we randomly select 100 states from the dataset as initial states. For each initial state, we use the trained policy to perform rollouts in the dynamics model. To ensure the generation of OOD samples, we set the rollout horizon to be 100 (the dynamics model tends to output bad transitions under larger rollout horizon due to compounding error). In total, we obtain 10,000 synthetic transitions for each dataset. For any synthetic transition $( s , a , \\hat { s } ^ { \\prime } )$ , we can replay the state-action pair $( s , a )$ in the real environment to obtain the true next state $s ^ { \\prime }$ . Then we can calculate the L2-norm error between $s ^ { \\prime }$ and $\\hat { s } ^ { \\prime } \\colon \\| s ^ { \\prime } - \\hat { s } ^ { \\prime } \\| _ { 2 }$ , and this can reflect the difference between the true environmental dynamics and the simulated dynamics. We can then use the aforementioned methods to estimate the uncertainty of the synthetic transition. For a good uncertainty estimation method, we expect a strong correlation between the estimated uncertainty and the L2-norm error of the next state. The remaining question is how to measure the correlation. Following previous work (Lu et al. 2021), we use Spearman rank $( \\rho )$ and Pearson bivariate $( r )$ correlations, where $\\rho$ captures the rank correlations and $r$ measures the linear correlations.\n\nTable 1: Normalized average score comparison between vanilla base algorithms and the version with SUMO, on top of MOPO, AMOReL, MOReL and MOBILE on 15 D4RL MuJoCo datasets, and the version of datasets we use is ”-v2”. We abbreviate ”halfcheetah” as ”half”, ”random” as ”r”, ”medium” as $\\because _ { \\mathrm { m } ^ { \\prime } }$ , ”medium-replay” as $\\mathrm { { ^ { , } m \\mathrm { { - } r ^ { \\mathrm { { , } } } } } }$ , ”medium-expert” as $\\mathrm { { ^ { , } m \\mathrm { { - } e ^ { , \\prime } } } }$ and ”expert” as ”e”. We run each algorithm for 1M gradient steps with 5 random seeds. We report the final average performance and $\\pm$ captures the standard deviation. Bold numbers represent the best average scores within each group.   \n\n<html><body><table><tr><td rowspan=\"2\">Task Name</td><td colspan=\"2\">MOPO</td><td colspan=\"2\">AMOReL</td><td colspan=\"2\">MOReL</td><td colspan=\"2\">MOBILE</td></tr><tr><td>SUMO</td><td>Base</td><td>SUMO</td><td>Base</td><td>SUMO</td><td>Base</td><td>SUMO</td><td>Base</td></tr><tr><td>half-r</td><td>37.2±1.9</td><td>34.9±1.4</td><td>44.2±2.1</td><td>31.8±2.4</td><td>37.3±2.1</td><td>29.8±1.2</td><td>34.9±2.1</td><td>37.8±2.9</td></tr><tr><td>hopper-r</td><td>24.5±0.9</td><td>19.4±0.7</td><td>29.7±0.6</td><td>32.4±1.2</td><td>33.2±0.7</td><td>30.1±1.0</td><td>30.8±0.9</td><td>32.6 ± 1.2</td></tr><tr><td>walker2d-r</td><td>11.4±1.3</td><td>13.1±1.1</td><td>20.3±0.2</td><td>21.0±0.3</td><td>17.8±0.6</td><td>19.4±0.3</td><td>27.9±2.0</td><td>16.3± 4.6</td></tr><tr><td>half-m-r</td><td>73.1±2.1</td><td>65.0±3.3</td><td>76.8±2.7</td><td>49.6±2.3</td><td>67.9±2.5</td><td>51.2±1.9</td><td>76.2±1.3</td><td>67.9±2.0</td></tr><tr><td>hopper-m-r</td><td>65.4±3.2</td><td>38.8±2.4</td><td>88.7±1.3</td><td>80.5±1.0</td><td>83.9±1.3</td><td>76.3±1.0</td><td>109.9±1.4</td><td>104.9±0.9</td></tr><tr><td>walker2d-m-r</td><td>70.3±0.6</td><td>74.8±1.5</td><td>65.3±2.7</td><td>46.0±1.9</td><td>61.3±3.1</td><td>48.1±4.2</td><td>78.2±1.5</td><td>83.9±1.3</td></tr><tr><td>half-m</td><td>68.9±2.3</td><td>73.1±2.7</td><td>82.1±2.8</td><td>69.2±1.2</td><td>57.9±1.2</td><td>62.4±1.3</td><td>84.3±2.4</td><td>75.1±1.5</td></tr><tr><td>hopper-m</td><td>74.6±1.9</td><td>45.6±2.5</td><td>95.0±2.1</td><td>87.2±3.4</td><td>82.1±1.4</td><td>84.7±3.1</td><td>104.8±2.1</td><td>102.9±1.9</td></tr><tr><td>walker2d-m</td><td>57.3±1.6</td><td>42.3±0.8</td><td>67.4±0.9</td><td>71.2±1.3</td><td>77.1±3.5</td><td>67.6±2.2</td><td>94.1±2.5</td><td>89.1±1.0</td></tr><tr><td>half-m-e</td><td>84.1±1.4</td><td>76.6±1.0</td><td>99.4±3.6</td><td>90.6±2.1</td><td>98.6±3.5</td><td>92.3±4.6</td><td>106.6±2.4</td><td>109.2±3.8</td></tr><tr><td>hopper-m-e</td><td>88.1±1.9</td><td>69.1±1.2</td><td>101.5±0.4</td><td>106.2±1.5</td><td>105.8±1.4</td><td>102.4±0.9</td><td>107.8±0.7</td><td>110.1±1.3</td></tr><tr><td>walker2d-m-e</td><td>81.9±1.6</td><td>75.4±1.1</td><td>109.6±0.7</td><td>92.3±0.9</td><td>86.1±1.8</td><td>90.4±1.4</td><td>122.8±0.4</td><td>115.9±0.8</td></tr><tr><td>half-e</td><td>87.1±1.2</td><td>88.7±1.6</td><td>112.3±2.5</td><td>103.2±1.9</td><td>109.9±2.1</td><td>105.8±1.6</td><td>111.5±1.5</td><td>113.1±2.1</td></tr><tr><td>hopper-e</td><td>101.2± 1.8</td><td>83.9±0.7</td><td>105.4±0.6</td><td>94.5±0.3</td><td>101.8±1.9</td><td>92.5±1.0</td><td>115.9 ± 2.9</td><td>112.4±3.5</td></tr><tr><td>walker2d-e</td><td>114.4±1.1</td><td>95.3±3.4</td><td>106.3±1.3</td><td>107.2±1.0</td><td>106.2±1.5</td><td>108.3±2.1</td><td>116.3±1.5</td><td>113.7±1.1</td></tr><tr><td>Average score</td><td>69.3</td><td>59.7</td><td>80.3</td><td>72.2</td><td>75.1</td><td>70.7</td><td>88.2</td><td>85.6</td></tr></table></body></html>\n\nTable 2: Spearman rank $( \\rho )$ and Pearson bivariate $( r )$ correlations comparison of SUMO against Max Aleatoric, Max Pairwise Diff and LOO KL Divergence. We choose these two metrics following (Lu et al. 2021). The closer $\\rho$ and $r$ are to 1, the better the uncertainty estimation. Each method is evaluated by 5 runs and we report the average $\\rho$ and $r$ . Bolded numbers represent the method with the highest average $\\rho$ and $r$ on each dataset.   \n\n<html><body><table><tr><td rowspan=\"2\">Task Name</td><td colspan=\"2\">Max Aleatoric</td><td colspan=\"2\">Max Pairwise Diff</td><td colspan=\"2\">LOO KL Divergence</td><td colspan=\"2\">SUMO(Ours)</td></tr><tr><td>p</td><td>r</td><td>p</td><td>r</td><td>p</td><td>T</td><td>p</td><td>r</td></tr><tr><td>halfcheetah-random-v2</td><td>0.63</td><td>0.58</td><td>0.58</td><td>0.47</td><td>0.10</td><td>0.06</td><td>0.84</td><td>0.83</td></tr><tr><td>hopper-random-v2</td><td>0.76</td><td>0.71</td><td>0.85</td><td>0.77</td><td>0.13</td><td>0.09</td><td>0.82</td><td>0.74</td></tr><tr><td>walker2d-random-v2</td><td>0.75</td><td>0.62</td><td>0.71</td><td>0.58</td><td>0.14</td><td>0.10</td><td>0.73</td><td>0.70</td></tr><tr><td>halfcheetah-medium-replay-v2</td><td>0.70</td><td>0.65</td><td>0.73</td><td>0.67</td><td>0.21</td><td>0.16</td><td>0.82</td><td>0.68</td></tr><tr><td>hopper-medium-replay-v2</td><td>0.73</td><td>0.70</td><td>0.79</td><td>0.67</td><td>0.18</td><td>0.08</td><td>0.87</td><td>0.80</td></tr><tr><td>walker2d-medium-replay-v2</td><td>0.66</td><td>0.54</td><td>0.65</td><td>0.59</td><td>0.09</td><td>0.06</td><td>0.88</td><td>0.86</td></tr><tr><td>halfcheetah-medium-v2</td><td>0.69</td><td>0.59</td><td>0.65</td><td>0.57</td><td>0.15</td><td>0.11</td><td>0.84</td><td>0.77</td></tr><tr><td>hopper-medium-v2</td><td>0.74</td><td>0.66</td><td>0.70</td><td>0.63</td><td>0.11</td><td>0.05</td><td>0.86</td><td>0.79</td></tr><tr><td>walker2d-medium-v2</td><td>0.68</td><td>0.56</td><td>0.74</td><td>0.73</td><td>0.11</td><td>0.08</td><td>0.82</td><td>0.71</td></tr><tr><td>halfcheetah-medium-expert-v2</td><td>0.67</td><td>0.63</td><td>0.70</td><td>0.68</td><td>0.13</td><td>0.07</td><td>0.88</td><td>0.81</td></tr><tr><td>hopper-medium-expert-v2</td><td>0.71</td><td>0.68</td><td>0.78</td><td>0.72</td><td>0.22</td><td>0.14</td><td>0.75</td><td>0.68</td></tr><tr><td>walker2d-medium-expert-v2</td><td>0.62</td><td>0.59</td><td>0.65</td><td>0.60</td><td>0.08</td><td>0.06</td><td>0.84</td><td>0.80</td></tr><tr><td>halfcheetah-expert-v2</td><td>0.79</td><td>0.75</td><td>0.69</td><td>0.67</td><td>0.12</td><td>0.10</td><td>0.76</td><td>0.68</td></tr><tr><td>hopper-expert-v2</td><td>0.73</td><td>0.64</td><td>0.76</td><td>0.67</td><td>0.17</td><td>0.14</td><td>0.86</td><td>0.80</td></tr><tr><td>walker2d-expert-v2</td><td>0.63</td><td>0.57</td><td>0.65</td><td>0.58</td><td>0.09</td><td>0.05</td><td>0.83</td><td>0.76</td></tr></table></body></html>\n\nTable 3: Spearman rank $( \\rho )$ and Pearson bivariate $( r )$ correlations comparison of SUMO with different search vectors. We run each experiment by 5 different random seeds and report the average $\\rho$ and $r$ .   \n\n<html><body><table><tr><td rowspan=\"2\">Task Name</td><td colspan=\"2\">ss' sa</td><td colspan=\"3\">sas'</td></tr><tr><td>p</td><td>r</td><td>p r</td><td>p</td><td>r</td></tr><tr><td>half-m</td><td>0.51</td><td>0.44</td><td>0.62 0.58</td><td>0.84</td><td>0.77</td></tr><tr><td>hopper-m</td><td>0.50</td><td>0.48</td><td>0.55 0.52</td><td>0.86</td><td>0.79</td></tr><tr><td>walker2d-m</td><td>0.64</td><td>0.49</td><td>0.59 0.51</td><td>0.82</td><td>0.71</td></tr><tr><td>half-m-e</td><td>0.57</td><td>0.54</td><td>0.63 0.61</td><td>0.88</td><td>0.81</td></tr><tr><td>hopper-m-e</td><td>0.60</td><td>0.57</td><td>0.69 0.63</td><td>0.75</td><td>0.68</td></tr><tr><td>walker2d-m-e</td><td>0.52</td><td>0.46</td><td>0.67 0.59</td><td>0.84</td><td>0.80</td></tr><tr><td>Mean</td><td>0.56</td><td>0.50</td><td>0.63 0.57</td><td>0.83</td><td>0.76</td></tr></table></body></html>\n\nFor each dataset, we calculate $\\rho$ and $r$ on the synthesized 10,000 transitions and present the results in Table 2. It is evident that the advantages of SUMO are significant. SUMO achieves the best $\\rho$ or $r$ metrics in 12 out of 15 datasets compared with other model ensemble-based methods. We then conclude that SUMO incurs a better uncertainty estimation.\n\n# Ablation Study & Parameter Study\n\nIn this part, we examine different design choices of SUMO and investigate the sensitivity of SUMO to the introduced hyperparameters. For the design choices of SUMO, we mainly focus on two aspects: (a) the components included in the search vector; and (b) the choice of distance measure. The Choice of Search Vector: Recall that in Equation (4), we use $( s \\oplus a \\oplus s ^ { \\prime } )$ as the search vector for KNN search because this reflects the difference between simulated dynamics and real dynamics. What if we replace the search vector with $( s \\oplus s ^ { \\prime } )$ or $( s \\oplus a ) ^ { \\cdot }$ ?\n\nWe design experiments to investigate the effectiveness of these three choices of search vectors. We follow the experimental settings in Section and conduct experiments with these search vectors on 6 D4RL MuJoCo datasets. We also use $\\rho$ and $r$ as evaluation metrics. We present the experimental results in Table 3. It is evident that using $( s \\oplus \\bar { a ^ { \\prime } } \\oplus s ^ { \\prime } )$ as the search vector is superior to using $( s \\oplus s ^ { \\prime } )$ , or $( s \\oplus a )$ . The scores in terms of $\\rho$ and $r$ on all six datasets are the highest when using $( s \\oplus a \\oplus s ^ { \\prime } )$ among the three choices. It turns out that it is vital to include the action $a$ for measuring uncertainty. The inclusion of the next state is also necessary because the learned dynamics model is responsible for predicting the next state given $( s , a )$ .\n\nThe Choice of Distance Measure: Different distance measure for KNN search can induce different uncertainty estimation. To examine whether SUMO is sensitive to different distance measure, we change the default Euclidean distance we use to Manhattan distance and Cosine similarity, and conduct experiments on six MuJoCo datasets, based on $\\mathbf { M O P O + S U M O }$ . The results are presented in Table 4. We can observe using Euclidean distance is slightly better than other two distance measure, but there is no obvious performance distinction, so we can simply use Euclidean distance as distance measure.\n\nTable 4: Performance comparison of $\\mathbf { M O P O + S U M O }$ between difference distance measure. Each experiment is run with 5 seeds.   \n\n<html><body><table><tr><td>Task Name</td><td>Euclidean</td><td>Manhattan</td><td>Cosine</td></tr><tr><td>half-m</td><td>68.9</td><td>67.4</td><td>68.0</td></tr><tr><td>hopper-m</td><td>74.6</td><td>72.9</td><td>73.8</td></tr><tr><td>walker2d-m</td><td>57.3</td><td>59.1</td><td>56.2</td></tr><tr><td>half-m-e</td><td>84.1</td><td>83.9</td><td>82.1</td></tr><tr><td>hopper-m-e</td><td>88.1</td><td>90.4</td><td>88.6</td></tr><tr><td>walker2d-m-e</td><td>81.9</td><td>79.4</td><td>77.2</td></tr><tr><td>Average Score</td><td>75.8</td><td>75.5</td><td>74.3</td></tr></table></body></html>\n\nTable 5: Spearman rank $( \\rho )$ and Pearson bivariate $( r )$ correlations comparison of SUMO with different values of $k$ . We run each experiment by 5 different seeds and report the average $\\rho$ and $r$ .   \n\n<html><body><table><tr><td rowspan=\"2\">Task Name</td><td colspan=\"2\">k=1</td><td colspan=\"2\">k=5</td><td colspan=\"2\">k=10</td></tr><tr><td>p</td><td>r</td><td>p</td><td>r</td><td>p</td><td>r</td></tr><tr><td>half-m</td><td>0.84</td><td>0.77 0.79</td><td>0.81</td><td>0.75</td><td>0.86</td><td>0.78</td></tr><tr><td>hopper-m walker2d-m</td><td>0.86 0.82</td><td>0.71</td><td>0.84 0.85</td><td>0.81 0.73</td><td>0.84 0.81</td><td>0.80 0.70</td></tr><tr><td>half-m-e</td><td>0.88</td><td>0.81</td><td>0.86</td><td>0.77</td><td>0.84</td><td>0.79</td></tr><tr><td>hopper-m-e</td><td>0.75</td><td>0.68</td><td>0.77</td><td>0.71</td><td>0.74</td><td>0.73</td></tr><tr><td>walker2d-m-e</td><td>0.84</td><td>0.80</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>0.82</td><td>0.76</td><td>0.80</td><td>0.77</td></tr><tr><td>Mean</td><td>0.83</td><td>0.76</td><td>0.82</td><td>0.76</td><td>0.81</td><td>0.77</td></tr></table></body></html>\n\nNumber of Neighbors $k$ : The main hyperparameter in SUMO is the number of nearest neighbors $k$ in KNN search. To examine its influence on the uncertainty estimation, we sweep $k$ across $\\{ 1 , 5 , 1 0 \\}$ and run experiments on selected MuJoCo datasets. We use metrics of $\\rho$ and $r$ . The results are shown in Table 5. We find that SUMO is robust to $k$ .\n\n# Conclusion\n\nIn this paper, we propose SUMO, a novel search-based uncertainty estimation method for model-based offline RL. SUMO characterizes the uncertainty as the cross entropy between simulated dynamics and true dynamics, and employ a practical KNN search method for implementation. We show that SUMO can provide a better uncertainty estimation than model ensemble-based methods and boost the performance of a variety of base algorithms.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   离线强化学习（Offline RL）的性能受限于静态数据集的大小和质量。模型基础的离线RL通过动态模型生成合成样本以提升性能，但合成样本的可靠性评估是一个关键问题。现有的模型集成不确定性估计方法（如MOPO、MOReL）并不总是最佳选择，因为它们可能因模型训练不佳而导致估计不可靠。\\n> *   该问题的重要性在于，准确的样本可靠性评估可以显著提升离线RL的性能，避免因不可靠样本导致的性能下降，尤其在需要高可靠性的应用场景（如自动驾驶、医疗决策）中尤为重要。\\n\\n> **方法概述 (Method Overview)**\\n> *   本文提出了一种基于搜索的不确定性估计方法SUMO（Search-based Uncertainty estimation for Model-based Offline RL），用于模型基础的离线RL。SUMO通过测量合成样本与分布内数据集样本的交叉熵来表征不确定性，并使用高效的基于搜索的方法（KNN）实现。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   提出了一种新颖的基于搜索的不确定性估计方法SUMO，用于模型基础的离线RL，避免了模型集成方法的训练依赖性。\\n> *   将SUMO与AMOReL和MOPO结合，并提供了理论性能边界分析，证明了其有效性（如Theorem 1和Theorem 2）。\\n> *   在D4RL数据集上的实验结果表明，SUMO能够提供更准确的不确定性估计（在12/15数据集上Spearman和Pearson相关性指标优于基线），并显著提升基础算法的性能（MOPO+SUMO在11/15任务中优于原始MOPO，平均得分69.3 vs. 59.7）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   SUMO的核心思想是将模型动态的不确定性表征为模型动态与真实动态之间的交叉熵 $\\\\mathcal{H}(P_{\\\\widehat{\\\\mathcal{M}}}(\\\\cdot|s,a), P(\\\\cdot|s,a))$。通过粒子基础的熵估计器，将交叉熵的计算转化为KNN搜索问题（如Equation 4所示）。\\n> *   该方法有效的原因在于，它避免了模型集成方法的不稳定性（如训练数据分布的影响），且不需要额外的神经网络训练，提供了更稳定的不确定性估计。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前工作主要依赖模型集成方法（如Max Aleatoric、Max Pairwise Diff）进行不确定性估计，但这些方法可能因模型训练不佳而导致估计不可靠（如MOReL中$D_{TV}$的理论要求与实践中$u$的近似不匹配）。\\n> *   **本文的改进：** SUMO通过KNN搜索直接估计交叉熵，避免了模型集成方法的训练依赖性，且提供了对交叉熵的无偏估计。此外，SUMO使用FAISS库实现高效的KNN搜索，解决了高维数据和大规模搜索空间的计算效率问题。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.   学习环境动态模型：通过监督学习训练一个动态模型（如高斯分布的神经网络），用于生成合成样本（如Equation 5所示）。\\n> 2.   使用SUMO进行不确定性估计：对于每个合成样本 $(\\\\hat{s}, \\\\hat{a}, \\\\hat{s}')$，计算其与数据集中样本的KNN距离 $u(\\\\hat{s}, \\\\hat{a}, \\\\hat{s}') = \\\\log(\\\\|(\\\\hat{s} \\\\oplus \\\\hat{a} \\\\oplus \\\\hat{s}') - (\\\\hat{s} \\\\oplus \\\\hat{a} \\\\oplus \\\\hat{s}')^{k,N}\\\\|_2 + 1)$（如Equation 4所示）。\\n> 3.   将SUMO与现有方法结合：\\n>       *   与MOPO结合时，将不确定性估计用于奖励惩罚 $\\\\hat{r} = r - \\\\lambda \\\\hat{u}$（如Equation 6所示）。\\n>       *   与AMOReL结合时，将不确定性估计用于轨迹截断（当 $u(s,a,s') > \\\\epsilon$ 时停止生成轨迹，如Equation 7所示）。\\n\\n> **案例解析 (Case Study)**\\n> *   论文通过图1（左）展示了SUMO的KNN搜索过程，图1（右）展示了SUMO与AMOReL结合的轨迹截断效果，直观地说明了SUMO如何通过准确的KNN距离估计避免OOD区域。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   模型基础的离线RL算法：MOPO、AMOReL、MOReL、MOBILE。\\n> *   不确定性估计方法：Max Aleatoric、Max Pairwise Diff、LOO KL Divergence。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在算法性能提升上：** SUMO在D4RL数据集上的平均得分显著优于基线模型。例如，MOPO+SUMO在15个任务中的11个上优于原始MOPO（平均得分69.3 vs. 59.7），AMOReL+SUMO在10个任务上优于原始AMOReL（平均得分80.3 vs. 72.2）。\\n> *   **在不确定性估计准确性上：** SUMO在Spearman秩相关性（$\\\\rho$）和Pearson双变量相关性（$r$）指标上优于模型集成方法。例如，在halfcheetah-medium-v2数据集上，SUMO的$\\\\rho=0.84$、$r=0.77$，显著高于Max Aleatoric（$\\\\rho=0.69$、$r=0.59$）和Max Pairwise Diff（$\\\\rho=0.65$、$r=0.57$）。\\n> *   **在计算效率上：** SUMO通过FAISS库实现高效的KNN搜索，与模型集成方法相比，未引入明显的计算开销。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n> **提取与格式化要求**\\n> *   离线强化学习 (Offline Reinforcement Learning, Offline RL)\\n> *   模型基础强化学习 (Model-Based Reinforcement Learning, MBRL)\\n> *   不确定性估计 (Uncertainty Estimation, N/A)\\n> *   交叉熵 (Cross Entropy, N/A)\\n> *   K近邻搜索 (K-Nearest Neighbor Search, KNN)\\n> *   D4RL数据集 (D4RL Benchmark, D4RL)\\n> *   轨迹截断 (Trajectory Truncation, N/A)\\n> *   奖励惩罚 (Reward Penalty, N/A)\"\n}\n```"
}