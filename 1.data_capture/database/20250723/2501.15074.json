{
    "source": "Semantic Scholar",
    "arxiv_id": "2501.15074",
    "link": "https://arxiv.org/abs/2501.15074",
    "pdf_link": "https://arxiv.org/pdf/2501.15074.pdf",
    "title": "PatentLMM: Large Multimodal Model for Generating Descriptions for Patent Figures",
    "authors": [
        "Shreya Shukla",
        "Nakul Sharma",
        "Manish Gupta",
        "Anand Mishra"
    ],
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "publication_date": "2025-01-25",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "Indian Institute of Technology Jodhpur",
        "Microsoft"
    ],
    "paper_content": "# PATENTLMM: Large Multimodal Model for Generating Descriptions for Patent Figures\n\nShreya Shukla1\\*, Nakul Sharma1\\*, Manish Gupta2, Anand Mishra1\n\n1Indian Institute of Technology Jodhpur, India 2Microsoft, India shukla.12,sharma.86 $@$ iitj.ac.in, gmanish $@$ microsoft.com, mishra $@$ iitj.ac.in\n\n# Abstract\n\nWriting comprehensive and accurate descriptions of technical drawings in patent documents is crucial to effective knowledge sharing and enabling the replication and protection of intellectual property. However, automation of this task has been largely overlooked by the research community. To this end, we introduce PATENTDESC-355K, a novel large-scale dataset containing $\\mathord { \\sim } 3 5 5 \\mathrm { K }$ patent figures along with their brief and detailed textual descriptions extracted from more than 60K US patent documents. In addition, we propose PATENTLMM ‚Äì a novel large multimodal model specifically tailored to generate high-quality descriptions of patent figures. Our proposed PATENTLMM comprises two key components: (i) PATENTMME, a specialized multimodal vision encoder that captures the unique structural elements of patent figures, and (ii) PATENTLLAMA, a domain-adapted version of LLaMA fine-tuned on a large collection of patents. Our extensive experiments demonstrate that training a vision encoder specifically designed for patent figures significantly boosts the performance, generating coherent descriptions compared to fine-tuning similar-sized off-the-shelf multimodal models. PATENTDESC-355K and PATENTLMM pave the way for automating the understanding of patent figures, enabling efficient knowledge sharing and faster drafting of patent documents.\n\nProject Page ‚Äî https://vl2g.github.io/projects/PatentLMM/\n\n# 1 Introduction\n\nPatents are a cornerstone of intellectual property protection, granting inventors exclusive rights to their creations. Effective communication of these inventions is crucial for patent examiners, courts, and the technical community to appreciate the inventiveness of these inventions and assess their novelty. Patent documents rely heavily on figures and their corresponding textual descriptions to present technical details. Writing accurate descriptions of these figures is essential for an unambiguous understanding of the invention and its components and facilitates knowledge sharing within the technical community. Comprehensive descriptions also ensure that the invention is adequately protected against potential infringements by others. However, manually crafting such descriptions is time-consuming and laborious, hindering the efficiency of patent processing and analysis.\n\nOne of the major challenges for generating patent figure descriptions in an automated way is the lack of large-scale labeled datasets. Existing datasets, while invaluable for advancing research in natural and scientific figure captioning, do not adequately capture the nuances and complexities inherent to patent illustrations. To address this gap, we curate PATENTDESC-355K, a novel large-scale dataset containing $\\mathord { \\sim } 3 5 5 \\mathrm { K }$ patent figures and their brief and detailed textual descriptions extracted from $6 0 \\mathrm { K } +$ patent documents. This dataset offers a rich and diverse collection of patent figures that span various technical domains, along with their corresponding descriptions, enabling the development and evaluation of models specifically tailored for this task.\n\nTypically, patent figures are associated with brief and detailed descriptions. In our proposed PATENTDESC-355K dataset, we found that they span an average of ${ \\sim } 3 4$ and $\\mathord { \\sim } 1 6 8 0$ tokens, respectively. Thus, unlike existing image captioning benchmarks, for example COCO (Lin et al. 2014), TextCaps (Sidorov et al. 2020) and NoCaps (Agrawal et al. 2019) where captions span an average of ${ \\sim } 1 2$ tokens, the descriptive captioning of patent figures in our dataset is much more challenging. Moreover, unlike the natural scene images of the existing captioning datasets, patent figures are structured technical illustrations that adhere to a more standardized visual style for technical and legal documentation.\n\nThe emergence of Large Language Models (LLMs) and Large Multimodal Models (LMMs) has revolutionized almost every vision and language task. These models exhibit a remarkable ability to understand and generate coherent language across diverse domains. However, applying these models to the generation of patent descriptions presents unique challenges. The length of descriptions and the complexity inherent to patent diagrams underscore the need to focus on various elements of the figure, such as arrows, nodes, and text annotations. Further, contrary to dense document images, patent figures are sparse and comprise several elements like text, nodes, node labels (a number associated with nodes in the patent figure), figure numbers, and arrows in different styles, i.e., uni-direction and bidirectional, solid, and dotted, among others. Please refer to Fig. 3 in the Appendix for an overview of these elements.\n\nGenerated Brief and Detailed Descriptions Ground Truth Brief and Detailed Descriptions Electronic Device 104 FIG. 1 illustrates a mobile smartphone in accordance FIG. 1 shows the details of an exemplary electronic with an aspect of the invention. device in accordance with aspects of the invention.   \nMemory 116 Processor 114 Referring now to FIG. 1, an exemplary electronic device FIG. 1 shows the details of an exemplary electronic device   \nCopratinstempont 150 104 is illustrated. It should be understood that the in accordance with aspects of the invention. The electronic   \nContact/Motion Component152 electronic device 104 may be any one of a number of device 104 includes a processor 114, memory 116, display   \nGraphics Component 154 electronic devices including, but not limited to, a cell 118, user interface 122, and the like. The processor 114 phone, a smart phone, a personal digital assistant, a laptop may be a central processing unit configured to execute Display 118 computer, a desktop computer, a netbook computer, a instructions including instructions related to software User Interface 122 server computer, etc. The electronic device 104 includes a programs. The display 118 may be a liquid crystal display processor 114, a memory 116, a display 118, a user having a backlight to illuminate the various color liquid Touchscreen 180 interface 122, a transceiver 120, and an optional touch crystals to provide a colorful display. The user interface 190 190 screen 160. The processor 114 is configured to execute 122 may be any type of physical input having buttons and instructions received from the memory 116, and is, for further may be implemented as a touchscreen 180.The 19 example, a general purpose processor, a field electronic device 104 may further include in the memory programmable gate array, or any other suitable processor. 116, an operating system 148, a communication The processor 114 is configured to execute instructions component 150, a contact/motion component 152, a Audio Input/Output received from the memory 116, including instructions for graphics component 154, and the like. The operating Device 156 Power Supply 158 170 ipnrsotcruecstsiorns 1r1e4ceivmeady froalmso hebetracnosncfiegivuerred12t0o, inecxleucdiunteg displaying a user interface on the display 118. The system 148 together with the various components pofrotvhiedienlgecstorfotnwicardeefvuincceti1o0n4a.liTtyhfeormeamcohryo 1t1h6e cmoamypinocnleundtes Touch Screen instructions for receiving data from the transceiver 120 and a high-speed random-access memory. Also, the memory Controller 160 instructions for transmitting data from the transceiver 116 may be a non-volatile memory, such as magnetic fixed Transceiver 120 120.The memory 116 is configured to store data received disk storage, flash memory or the like. These various from the processor 114 and instructions to be executed by components may be connected through various the processor 114. The memory 116 may be, for example, communication lines including a data bus 170.Additionally, FIGURE1 a random access memory device, a solid state memory the electronic device 104 may include an audio device, or any other suitable ‚Ä¶. input/output device 156. The audio ‚Ä¶\n\nAs can be seen in Fig. 1, the detailed description of patent figures heavily makes use of these elements to convey the semantics of the figure. Given this dramatic difference between captions of natural scenes versus patent figures, it was anticipated that recent image captioning methods (Li et al. 2022; Wang et al. 2022a,b) and multimodal LLMs (Ye et al. 2023b; Liu et al. 2024a; Zhu et al. 2024) would perform poorly for our task in a zero-shot setting. Surprisingly, these approaches demonstrated suboptimal performance even after fine-tuning on our dataset. These unique properties of patent figures require specialized system design to ensure the accurate and concise generation of descriptions without introducing hallucinations or irrelevant details.\n\nIn this paper, we propose PATENTLMM ‚Äì a novel model to generate descriptions of patent figures. The model contains two important components: PATENTMME and PATENTLLAMA. PATENTMME is a specialized multimodal vision encoder for patent figures, trained using masked language modeling loss, along with two other novel loss functions focused on learning structure from sparse patent figures. PATENTLLAMA is a domain-adapted version of LLaMA fine-tuned on a large collection of patent text from the Harvard USPTO Dataset (HUPD) (Suzgun et al. 2024). PATENTLMM combines the PATENTMME encoder and the PATENTLLAMA using a projection layer.\n\nThe major contributions of our work are as follows. (i) We present a large-scale dataset of $\\mathord { \\sim } 3 5 5 \\mathrm { K }$ patent figures and their brief and detailed descriptions. (ii) We propose a novel multimodal model PATENTLMM, comprising a patent domain-specialized vision encoder trained using objectives specifically tailored to capture the structure of patent documents and an LLM fine-tuned on patent data. (iii) We extensively benchmark existing captioning models and multimodal LLMs and show that our proposed approach surpasses their best performance on the average BLEU metric by $1 0 . 2 2 \\%$ and $4 . 4 3 \\%$ on an absolute scale for generating brief and detailed descriptions, respectively.\n\n# 2 Related Work\n\nImage Captioning in Pre-LMMs era: The patent figure description task is broadly similar to the image captioning task, which has been an active research area in the last decade. Some representative early work on image captioning includes the combination of a CNN encoder with an LSTM decoder (Vinyals et al. 2015), a multimodal RNN architecture that uses local and global image features (Andreas et al. 2016), an adaptive attention model (Lu et al. 2017), and a bottom-up and top-down attention model (Anderson et al. 2018). Recent works have also focused on improving caption diversity (Shetty, Roumeliotis, and Laaksonen 2017), novel object captioning (Lu et al. 2018), and incorporating external knowledge (Gu et al. 2019). As discussed in the previous section, our task differs significantly from these previous efforts on image captioning in terms of the length of descriptions and the structure of patent figures.\n\nDescribing Scientific Figures: Patent figures are a specific form of scientific illustrations. Although previous work on generating descriptions of patent figures has been sparse, ample research has been done to caption scientific figures. Chen et al. (2019, 2020) create and leverage FigCAP and adapt an LSTM-based model (Hochreiter and Schmidhuber 1997) for captioning. Recently, Hsu, Giles, and Huang (2021) collected the SciCap dataset from articles published on arXivIn (Yang et al. 2023), the authors augment the SciCap dataset with additional information such as OCR text from figures and referring sentences from the text to curate ${ \\mathrm { S c i C a p } } +$ , and demonstrate the performance boost achieved by incorporating extra information. Kantharaj et al.\n\n(2022) and Tang, Boggust, and Satyanarayan (2023) address the problem of captioning various visualization charts of data. Certain works go beyond natural language descriptions to generate code, particularly for flowcharts. For example, Shukla et al. (2023) and Liu et al. (2022) specifically address the generation of code from flow chart images.A parallel work PatFig (Aubakirova, Gerdes, and Liu 2023) scrapes a similar dataset as ours with 17K training samples and 2K test samples, and demonstrates the performance of MiniGPT-4 (Zhu et al. 2024) in the proposed dataset. In this work, we contribute a ${ \\sim } 2 0 \\times$ larger dataset and propose a novel model, PATENTLMM, which is almost twice as effective as MiniGPT-4 in BLEU-4 for PATENTDESC-355K.\n\nLarge Multimodal Models: Recent work in the multimodal (vision and language) community has focused on leveraging the world knowledge implicitly encoded in large language models for multimodal tasks such as visual question answering and image captioning (Zhu et al. 2024; Li et al. 2023; Liu et al. 2024a; Achiam et al. 2023; Ye et al. 2023b, 2024; Wang et al. 2022b; Team et al. 2023; Alayrac et al. 2022), visual grounding (Ye et al. 2024; Zhu et al. 2024; Team et al. 2023; Achiam et al. 2023) and image-text matching (Li et al. 2022, 2023). This is achieved by feeding an image representation as input along with the prompt to the language model and modeling the output using the language modeling objective. Recent advances include Flamingo (Alayrac et al. 2022), which inserts trainable gated cross-attention layers into a pretrained LLM (Hoffmann et al. 2022). BLIP2 (Li et al. 2023) leverages pre-trained ViT (Dosovitskiy et al. 2021) and LLaMA (Touvron et al. 2023), combined with QFormer, to translate image embeddings into LLM prompt embeddings. MiniGPT-4 (Zhu et al. 2024) builds upon pretrained BLIP-2 and finetunes an additional linear layer to project queries into the LLM on a curated dataset. In contrast, LLaVA-1.5 (Liu et al. 2024a) proposes a relatively simple and effective two-stage approach. In addition, document-specific LLMs such as LayoutLLM (Luo et al. 2024), UReader (Ye et al. 2023a) and TextMonkey (Liu et al. 2024b) have shown impressive performance on Document VQA task. We compare with several of these models and show that these models do not perform competitively for the task of generating descriptions from patent figures.\n\n# 3 PATENTDESC-355K: A Novel Dataset of Patent Figures with Descriptions\n\nWe introduce PATENTDESC-355K ‚Äì a novel large-scale dataset tailored for generating descriptions for patent figures. Our proposed dataset comprises 355K patent figures sourced from Google Patents1, with each image accompanied by its brief and detailed descriptions extracted from the corresponding patent documents. The dataset is available for download on our project website: https://vl2g.github. io/projects/PatentLMM/. Fig. 1 visualizes a ‚ü®patent figure, brief description, detailed description triplet from our dataset. With our primary focus on US patents published after 2004, our dataset spans over 60K patents from assignees like Amazon, Microsoft, LinkedIn, Google, Yahoo, etc. To assess the quality of the dataset, we manually evaluated a random set of 100 patent figures with their brief and detailed descriptions and computed the sentence-level precision and recall of the extracted descriptions against the ground-truth descriptions. For brief descriptions, both precision and recall scores were $100 \\%$ . For detailed descriptions, precision and recall were $9 0 . 8 1 \\%$ and 91. $9 6 \\%$ , respectively. More details on data set curation, preprocessing, description extraction, and quality assessment are provided in Appendix A.\n\nTable 1: PATENTDESC-355K: Dataset Statistics.   \n\n<html><body><table><tr><td></td><td>Train</td><td>Validation</td><td>Test</td></tr><tr><td>Number of Images</td><td>320,717</td><td>17,286</td><td>17,336</td></tr><tr><td>Avg. number of tokens in brief descriptions</td><td>34.37</td><td>34.28</td><td>34.30</td></tr><tr><td>Avg. number of tokens in detailed descriptions</td><td>1,677.85</td><td>1,676.71</td><td>1,697.16</td></tr><tr><td>Number of Unique Patents</td><td>50,448</td><td>8.027</td><td>7,964</td></tr><tr><td>Avg. number of images per patent</td><td>6.36</td><td>2.15</td><td>2.18</td></tr></table></body></html>\n\nDataset Analysis: Table 1 presents detailed statistics of the 355K image-description triplets in our dataset. During the creation of training, validation and test set splits, we ensure absolute exclusivity between patents in the train set and those in the combined validation and test sets, to enable robust out-of-sample evaluation. To achieve this, we randomly sampled $\\sim 1 2 . 6 \\mathsf { K }$ patents from $\\sim 6 0 \\mathrm { K }$ , representing $\\sim 8 2 . 5 \\mathrm { K }$ images. From this isolated subset of images, we sample ${ \\sim } 1 7 \\mathrm { K }$ images each for the val and test set, and discard the remaining images. This sampling technique also helps maintain the diversity within the validation and test sets, thereby providing a fair and representative evaluation. Our detailed descriptions span ${ \\sim } 1 . 7 \\mathrm { K }$ tokens on average, which is much larger compared to an average token length for popular image captioning benchmarks (Lin et al. 2014; Chen et al. 2015; Sidorov et al. 2020).\n\n# 4 Methodology\n\nOur approach is inspired by the recent success of large multimodal models like MiniGPT-4 (Zhu et al. 2024) and LLaVA (Liu et al. 2023, 2024a), which have demonstrated state-of-the-art performance on several benchmarks by effectively aligning visual and textual modalities. We introduce PATENTLMM, which combines our domain-adapted version of the LLaMA language model, namely PATENTLLAMA, with our novel visual encoder specialized for patent figures, namely PATENTMME. In this section, we describe the architectures of PATENTMME and PATENTLLAMA, and the overall framework of PATENTLMM.\n\n# 4.1 PATENTMME: Encoder for Patent Figures\n\nThe Vision Transformer (ViT) (Dosovitskiy et al. 2021), commonly used as a vision encoder in existing image captioning frameworks, is typically pre-trained on natural scene images, which are fundamentally different from patent figures. A better suited encoder is perhaps LayoutLM (Xu et al. 2020, 2021; Huang et al. 2022) which has shown impressive performance in document image understanding tasks. However, patent figures have a sparse layout compared to dense document images and are characterized by specific structured visual syntax. Unlike document images, patent figures comprise labeled nodes interconnected with arrows and accompanied by textual elements. The semantic relationship between these diagrammatic constituents is paramount for decoding the inventive concepts and technical specifications elucidated within the patent figures. We, therefore, build on the existing document image understanding capabilities of LayoutLMv3 (Huang et al. 2022) and pre-train it with novel objectives, specifically tailored to capture the structural information of patent figures.\n\n![](images/4f4b4728478681d9b2708059bfa9193f6e5e2ef9e9e27a489ca13234f317f551.jpg)  \nFigure 2: PATENTMME Architecture. We jointly process OCR tokens and visual embeddings to produce multimodal contextaware embeddings. These contextual embeddings are optimized using our proposed MLM, LA-MIM and PC objectives.\n\nArchitecture: The proposed PATENTMME shares its architecture with LayoutLMv3 (Huang et al. 2022) and is a multi-modal transformer model that processes image, text, and document layout information jointly. The overall architecture of the model is illustrated in Fig. 2. Given an input patent figure $I$ , the OCR text is extracted using offthe-shelf Tesseract OCR engine (Kay 2007). The image is then down-scaled to $H \\times W$ and split into non-overlapping patches of $p$ dimensions each, resulting in $M = H \\bar { W } / p ^ { \\overline { { 2 } } }$ image patches. The OCR extracted text is tokenized using the BPE tokenizer (Shibata et al. 1999) and represented using a learnable embedding matrix. Following (Huang et al. 2022), learnable 1D-position embeddings and 2D segmentlevel layout-position embeddings are added to the word embeddings, resulting in the final text embeddings. The image embeddings are created by linear projection of flattened image patches and combining them with learnable 1D position embeddings and 2D spatial embeddings. We use images of size $I \\in \\breve { \\mathbb { R } ^ { 3 \\times 3 8 4 \\times 3 8 4 } }$ , i.e., $H = W = 3 8 4$ . With $p = 1 6$ this results in $M \\ = \\ 5 7 6$ patches. The higher resolution helps preserve intricate structural details of patent figures, such as node labels and arrows.\n\nPre-training data and annotations: To enable largescale in-domain pre-training of PATENTMME, we crawled a set of $9 0 0 \\mathrm { K } +$ patent figures corresponding to the patent IDs from the Harvard USPTO Patent Dataset (HUPD) (Suzgun et al. 2024). For a fair evaluation, appropriate care has been taken to avoid any overlap of the sample with the validation and testing split of our dataset.\n\nFor robust patent-figures‚Äô-specific pretraining, we define loss functions that leverage patent diagram specific elements like nodes, node labels, figure labels, text and arrows. To extract such elements, we train a Faster-RCNN (Ren et al. 2015) based visual element detection network on 350 manually annotated patent figures, sampled randomly from our training data. The trained model is then used to infer elements from all training images, which is used to provide weak ground-truth labels during PATENTMME training. We show inference samples of this model in Appendix B.\n\nPre-training Loss Formulations: To enhance the vision encoder‚Äôs capability in capturing fine-grained structural details of patent figures, we pre-train PATENTMME using novel layout-aware masked image modeling (LAMIM) and image patch classification (PC) objectives, along with the established masked language modeling (MLM) loss. We describe these losses in the following text.\n\nNotation: We use $R$ and $T$ to denote the set of image patches (regions) and OCR tokens, respectively. Further, $X _ { m }$ and $X _ { u m }$ denote the masked and unmasked parts of the modality $X$ . The probability distribution generated by our PATENTMME model and the set of categories of visual elements that can be detected by our detection network by $p _ { \\theta }$ and $C$ , respectively.\n\n(i) Masked Language Modeling (MLM). Similar to LayoutLMv3 (Huang et al. 2022), we randomly mask $30 \\%$ of the OCR text tokens and optimize the model to predict the masked tokens, encouraging it to learn patent-specific textual semantics. This loss is computed as follows:\n\n![](images/71e6893c220a84a7a4515a5eb5851936c4294a69a4d28b3acfb0e46649b4fc0b.jpg)  \nFigure 3: PATENTLMM Architecture. Language Instruction is a fixed prompt guiding the model to generate either brief or detailed descriptions.\n\n$$\n\\mathcal { L } _ { M L M } ( \\theta ) = - \\sum _ { i \\in T _ { m } } \\log p _ { \\theta } ( t _ { i } \\mid R _ { u m } , T _ { u m } ) ,\n$$\n\nwhere $t _ { i }$ denotes the correct masked text tokens.\n\n(ii) Layout-Aware Masked Image Modeling (LAMIM). We utilize masked image modeling to learn visual representations by randomly masking $40 \\%$ of the image patches. Since the patent figures are more sparse compared to dense document images, we mask only the image patches that contain atleast one of the following five elements: nodes, node labels, figure labels, text and arrows. This strategy helps to avoid masking blank regions in the patent figures, and hence learn robust visual representations. Our formulation of the LAMIM objective is similar to BEiT (Bao et al. 2022) and therefore requires a discrete image tokenizer. We choose OCR-VQGAN (Rodriguez et al. 2023) since its tokenized image representation is capable of handling textual information better than competing works dVAE (Ramesh et al. 2021) and VQGAN (Esser, Rombach, and Ommer 2021). We compute this loss as follows:\n\n$$\n\\mathcal { L } _ { M I M } ( \\theta ) = - \\sum _ { i \\in R _ { m } } \\log p _ { \\theta } ( r _ { i } \\mid R _ { u m } , T _ { u m } ) ,\n$$\n\nwhere $p _ { i }$ denotes the correct masked image patches.\n\n(iii) Patch Classification (PC). In this multi-label binary classification objective, we classify each of the $M$ image patches into one or more of the following five categories: node, node label, figure label, text, and arrows. This objective which is mathematically computed as follows, helps the model learn discriminative representations for different visual elements in patent figures.\n\n$$\n\\mathcal { L } _ { P C } = - \\frac { 1 } { M } \\sum _ { i = 1 } ^ { M } \\sum _ { j = 1 } ^ { | C | } \\left[ y _ { i j } \\log ( \\hat { y } _ { i j } ) + ( 1 - y _ { i j } ) \\log ( 1 - \\hat { y } _ { i j } ) \\right] ,\n$$\n\nwhere $\\hat { y } _ { i j }$ denotes the probability of patch $i$ belonging to class $j$ , and $y _ { i j }$ is the binary ground truth label obtained using the visual element detector.\n\n# 4.2 PATENTLLAMA: Description Generator\n\nPATENTLLAMA is a domain-adapted version of the LLaMA-2 7B model for the patent domain. We continue to pre-train the LLaMA-2 7B model using LoRA (Hu et al. 2022) adapters, on the descriptions from HUPD patent dataset (Suzgun et al. 2024), to bias the model to generate the language inherent to patent documents. To avoid any train-test leakage, we ensure that we use the HUPD dataset after removing patent documents corresponding to the validation and test splits of our PATENTDESC-355K dataset.\n\n# 4.3 PATENTLMM\n\nInspired by recent multimodal LLM studies like MiniGPT4 (Zhu et al. 2024) and LLaVA (Liu et al. 2023, 2024a), we integrate PATENTMME and PATENTLLAMA through a single MLP network to exploit their pre-trained representations. The detailed architecture for PATENTLMM is illustrated in Fig. 3. Given a patent figure, we first obtain its layout-aware text and visual representations from frozen PATENTMME. These representations are projected into the input embedding space of PATENTLLAMA using a projection MLP, and the PATENTLLAMA is finetuned to maximize the likelihood of the corresponding description conditioned on these projected representations.\n\n# 5 Experiments\n\n# 5.1 Experimental Setup\n\nPATENTMME: PATENTMME is initialized with LayoutLMv3-Large to inherit its document understanding capabilities. For each of the three losses discussed in Section 4, the text and image embeddings obtained from PATENTMME are projected through separate MLPs (loss heads) before the loss is calculated. Since the network weights already have a good initialization, to prevent major changes in weights of the multimodal transformer, we adopt two-step training. During Step-1, the weights of the multimodal transformer remain frozen and only the loss heads are trained for 1 epoch with a higher learning rate of 1e-3 and 1K warm-up steps to learn good initialization. During Step 2, the entire model is trained end-to-end for 8 epochs with a lower learning rate of 5e-5 and with 10K warm-up steps. The PATENTMME model is trained on ${ 8 \\times \\mathrm { V 1 0 0 } }$ GPUs, with an effective batch size of 64 and Adam (Kingma and Ba 2014) optimizer.\n\nPATENTLMM: Following the standard practice (Liu et al. 2024a), we train our PATENTLMM model in two stages. To align the patent figure representations obtained from PATENTMME with the input latent space of PATENTLLAMA, we train only the projection layer in the first stage, keeping all other parameters frozen. During stage 2, we add LoRA adapters to all the linear layers of the PatentLLaMA module, except for the language modeling head, whose weights remain frozen. The weights of PATENTMME are kept frozen throughout. We train our PATENTLMM with an effective batch size of 192 on $3 \\times \\mathrm { A 1 0 0 }$ GPUs (40 GB). Stage 1 training progresses at a higher learning rate of 1e-3, and stage 2 training takes place\n\n<html><body><table><tr><td>Setup</td><td>Method</td><td>#Parameters|</td><td>B-2</td><td></td><td>B-4 Avg. B</td><td>R-1</td><td>R-2</td><td>R-L</td><td>M</td><td>B-2</td><td>B-4 Avg. B</td><td></td><td>R-1</td><td>R-2</td><td>R-L</td><td>M</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>Brief</td><td></td><td></td><td></td><td></td><td></td><td></td><td>Detailed</td><td></td><td></td><td></td></tr><tr><td rowspan=\"7\"></td><td>BLIP-2</td><td>2.7B</td><td>1.01</td><td>0.03</td><td>1.62</td><td>15.43</td><td>1.70</td><td>12.47</td><td>6.72</td><td>0.00</td><td>0.00</td><td>0.01</td><td>3.24</td><td>0.49</td><td>2.84</td><td>1.00</td></tr><tr><td>TextMonkey</td><td>9.8B</td><td>0.91</td><td>0.11</td><td>1.12</td><td>13.00</td><td>4.60</td><td>12.16</td><td>7.14</td><td>0.10</td><td>0.03</td><td>0.10</td><td>6.18</td><td>2.38</td><td>4.83</td><td>2.64</td></tr><tr><td>PEGASUS</td><td>568M</td><td>3.18</td><td>0.13</td><td>4.20</td><td>14.68</td><td>2.33</td><td>11.46</td><td>13.24</td><td>0.86</td><td>0.04</td><td>1.12</td><td>12.26</td><td>1.96</td><td>9.47</td><td>5.18</td></tr><tr><td>mPLUG-owl2</td><td>7.5B</td><td>3.64</td><td>0.36</td><td>4.47</td><td>21.47</td><td>5.10</td><td>19.41</td><td>13.07</td><td>3.65</td><td>0.49</td><td>3.40</td><td>23.75</td><td>5.39</td><td>14.85</td><td>11.83</td></tr><tr><td>UReader</td><td>7.2B</td><td>3.54</td><td>0.35</td><td>4.50</td><td>20.90</td><td>4.56</td><td>17.85</td><td>13.45</td><td>0.05</td><td>0.01</td><td>0.06</td><td>5.15</td><td>1.54</td><td>4.49</td><td>2.04</td></tr><tr><td>LLaVA-1.5</td><td>7.4B</td><td>4.52</td><td>0.24</td><td>4.71</td><td>17.59</td><td>3.27</td><td>14.63</td><td>15.74</td><td>3.65</td><td>0.37</td><td>3.36</td><td>23.75</td><td>4.63</td><td>14.71</td><td>11.69</td></tr><tr><td>GPT-4V</td><td>Unknown</td><td>20.74</td><td>8.56</td><td>18.68</td><td>36.07</td><td>15.65</td><td>31.89</td><td>32.88</td><td>19.61</td><td>6.05</td><td>18.26</td><td>39.95</td><td>12.14</td><td>20.16</td><td>27.31</td></tr><tr><td rowspan=\"7\"></td><td>Pegasus</td><td>568M</td><td>2.44</td><td>0.14</td><td>4.03</td><td>13.86</td><td>1.55</td><td>11.52</td><td>11.62</td><td>5.80</td><td>0.41</td><td>6.33</td><td>19.28</td><td>2.24</td><td>15.27</td><td>12.11</td></tr><tr><td>GIT</td><td>681M</td><td>26.95</td><td>15.33</td><td>24.78</td><td>45.28</td><td>27.17</td><td>42.29</td><td>44.27</td><td>6.33</td><td>1.18</td><td>6.23</td><td>13.66</td><td>3.17</td><td>10.87</td><td>10.68</td></tr><tr><td>BLIP</td><td>252M</td><td>24.62</td><td>12.52</td><td>22.40</td><td>42.59</td><td>23.78</td><td>39.16</td><td>42.84</td><td>5.45</td><td>1.05</td><td>5.31</td><td>12.42</td><td>2.89</td><td>9.46</td><td>9.55</td></tr><tr><td>MiniGPT-4</td><td>7.8B (3.2M)</td><td>30.57</td><td>17.96</td><td>28.13</td><td>43.53</td><td>25.33</td><td>40.35</td><td>43.03</td><td>11.01</td><td>2.81</td><td>10.26</td><td>28.91</td><td>6.23</td><td>15.67</td><td>16.65</td></tr><tr><td>OFA</td><td>472M</td><td>33.01</td><td>21.76</td><td>31.24</td><td>54.26</td><td>37.94</td><td>51.47</td><td>44.89</td><td>15.76</td><td>7.23</td><td>14.93</td><td>33.20</td><td>13.70</td><td>22.89</td><td>21.17</td></tr><tr><td>LLaVA-1.5</td><td>7.4B (341M)</td><td>36.64</td><td>25.00</td><td>34.37</td><td>48.92</td><td>32.01</td><td>45.87</td><td>48.23</td><td>20.90</td><td>11.12</td><td>19.81</td><td>36.86</td><td>15.68</td><td>24.48</td><td>24.71</td></tr><tr><td>PATENTLMM</td><td>7.4B (341M)</td><td>46.40</td><td>36.66</td><td>44.59</td><td>56.68</td><td>42.63</td><td>54.18</td><td>56.44</td><td>25.42</td><td>15.02</td><td>24.24</td><td>40.70</td><td>19.27</td><td>27.54</td><td>28.39</td></tr></table></body></html>\n\nTable 2: Quantitative results on PATENTDESC-355K (test set) for brief and detailed description generation $\\mathbf { B } =$ BLEU, $\\scriptstyle \\mathbf { R = R O U G E }$ , M=METEOR). Number in parenthesis under # Parameters column denote number of trainable parameters.\n\nat a learning rate of 2e-4 with a cosine schedule, for 12K steps using Adam optimizer. We train separate LMMs for brief and detailed descriptions.\n\nOverall, training PATENTLMM is a three-phase process. Firstly, we train the PATENTMME encoder in a semisupervised fashion by leveraging a vast amount of patent figures corresponding to patents in the HUPD dataset. Secondly, we domain-adapt the LLaMA-2 7B model on the HUPD patent text data to create PATENTLLAMA. Lastly, we integrate PATENTMME and PATENTLLAMA to create PATENTLMM, and train it following the two-stage process.\n\n# 5.2 Baselines\n\nWe benchmark the performance of various baselines on our proposed PATENTDESC-355K dataset in the zero-shot and fine-tuned setup. We benchmark the text-only baseline Pegasus (Zhang et al. 2020) by generating patent figure descriptions from OCR tokens extracted from patent figures. For image captioning baselines, we study the state-of-the-art models GIT (Wang et al. 2022a), BLIP (Li et al. 2022) and OFA (Wang et al. 2022b). We further compare our method with recent multimodal LLMs such as UReader (Ye et al. 2023a), TextMonkey (Liu et al. 2024b), mPLUG-owl2 (Ye et al. 2024), BLIP-2 (Li et al. 2023), MiniGPT-4 (Zhu et al. 2024), LLaVA-1.5 (Liu et al. 2024a) and the closed GPT4V model (Achiam et al. 2023). GPT-4V prompt is listed in Appendix C.3.\n\nTo measure the description generation performance of these models, we use standard image captioning metrics such as BLEU (Papineni et al. 2002), ROUGE (Lin 2004) and METEOR (Banerjee and Lavie 2005). Higher values for all the scores are desired. A detailed description of these metrics is provided in Appendix C.1.\n\n# 5.3 Results and Discussion\n\nThe quantitative performance comparison for the brief and detailed description generation task is reported in Table 2. In the zero-shot setting, GPT-4V demonstrated superior performance among baselines across all metrics, significantly outperforming other baselines owing to its large scale and the diverse data it has seen during its pre-training. The poor zero-shot performance of other baselines highlights the gap in their pre-training data and the nature of patent figures and descriptions. The fine-tuned models outperform their zero-shot counterparts, highlighting the importance of taskspecific training for these models. MiniGPT-4 and LLaVA1.5 utilize a frozen pre-trained ViT trained on web-scale natural images, which results in suboptimal representation of patent figures. Similarly, OFA also enforces these priors by utilizing a pre-trained discrete image tokenizer. On the other hand, PATENTLMM gives a boost of $\\sim 8 \\%$ across all metrics, signifying the importance of better domain knowledge embedded in it through the proposed PATENTMME pretraining and PatentLLaMA.\n\nSimilar to brief description generation, GPT-4V outperformed all other baselines for the detailed description generation task in the zero-shot setting. We observe that majority of the baselines struggle with performance in the zero-shot setup. In the fine-tuned setting, our PATENTLMM maintained its superior performance, achieving the highest scores across all metrics. This consistent top performance for both brief and detailed descriptions suggests the efficacy of our proposed approach for the task of generating descriptions from patent figures. The overall lower scores for detailed descriptions can be attributed to their comprehensiveness, complexity, and length, requiring models to capture and generate more nuanced and detailed information.\n\nAblations: We perform the following three ablation studies to quantify the impact of different components of our proposed PATENTLMM model:\n\n(i) PATENTMME Pre-training objectives: Table 3 shows the ablation results with combinations of pre-training objectives for the brief description generation. We observe that using a combination of MLM and LAMIM leads to better results compared to the pre-trained LayoutLMv3. Further, the PC loss also improves the performance of the model, when pre-trained with HUPD images data. A similar ablation for detailed descriptions is reported in Appendix C.2.\n\nTable 3: Ablation study to quantify the impact of pretraining objectives of PATENTMME on the overall performance of PATENTLMM on brief descriptions generation task. All models are trained with PATENTLLAMA.   \n\n<html><body><table><tr><td>Pre-training</td><td>B-2</td><td>B-4</td><td>Avg. B</td><td>R-1</td><td>R-2</td><td>R-L</td><td>M</td></tr><tr><td>PretrainedLayoutLMv3</td><td>42.81</td><td>32.50</td><td>40.86</td><td>53.68</td><td>38.88</td><td>51.07</td><td>53.34</td></tr><tr><td>W/MLM+LAMIM</td><td>45.24</td><td>35.33</td><td>43.39</td><td>55.69</td><td>41.38</td><td>53.20</td><td>55.34</td></tr><tr><td>w/MLM+LAMIM+PC</td><td>46.39</td><td>36.65</td><td>44.59</td><td>56.68</td><td>42.62</td><td>54.18</td><td>56.44</td></tr></table></body></html>\n\nTable 4: Ablation study to quantify the importance of OCR tokens on the overall performance of PATENTLMM on brief descriptions generation task.   \n\n<html><body><table><tr><td>OCR in training?</td><td>OCR in Inference?</td><td>B-2</td><td>B-4</td><td>Avg. B</td><td>R-1</td><td>R-2</td><td>R-L</td><td>M</td></tr><tr><td>No</td><td>No</td><td>30.32</td><td>19.17</td><td>28.30</td><td>41.61</td><td>25.21</td><td>38.95</td><td>41.46</td></tr><tr><td>Yes</td><td>No</td><td>11.51</td><td>2.77</td><td>9.83</td><td>24.38</td><td>7.92</td><td>21.68</td><td>22.52</td></tr><tr><td>Yes</td><td>Yes</td><td>46.40</td><td>36.66</td><td>44.59</td><td>56.68</td><td>42.63</td><td>54.18</td><td>56.44</td></tr></table></body></html>\n\n(ii) Importance of OCR tokens: In this ablation, we study whether avoiding passing OCR tokens to PATENTLMM causes any drop in the performance of brief description generation. We experiment with two ablations: (1) OCR tokens are used for PATENTMME pretraining but not for PATENTLMM training, and (2) OCR tokens are used for PATENTMME pretraining and for PATENTLMM training but not at inference time. Table 4 shows that it is important to use OCR tokens in the entire pipeline for the best results. (iii) PatentLMM Training: We report an additional ablation study in Appendix C.2 to quantify the advantage of using PatentLLaMA against the pre-trained LLaMA model.\n\nQualitative Analysis: Fig. 1 shows an example brief and detailed description generated by PATENTLMM for a test sample. The generated brief description, more specifically, terms the electronic device shown in the image as a mobile smartphone. The generated detailed description provides a comprehensive overview of the electronic device 104, its components, and their functions. It covers most of the key elements mentioned in the ground truth, including the processor 114, the memory 116, the display 118, and the user interface 122. However, there are some omissions, like Graphics Component 154 and Communication Component 150. More case studies are provided in Appendix D.\n\nError analysis: We perform a thorough manual error analysis on a set of 50 samples drawn from our test set to identify some prominent errors in the descriptions generated by our PATENTLMM model. We identify five main error categories as follows. (i) Hallucination in figure labeling occurs in 3 brief and 3 detailed descriptions. (ii) Hallucination in 4 brief descriptions and 7 detailed descriptions was due to little or no OCR detectable text in the figures. (iii) Incorrect association of node labels occurs when the wiggly arrows connecting node labels to respective nodes are misinterpreted or ignored due to downsampling of the image before being passed to PATENTMME. This was observed in 10 detailed descriptions. (iv) A similar misinterpretation due to downsampling is often the cause of hallucinated node labels in 12 detailed descriptions. (v) Cross-figure references in the descriptions establish the interconnection between various aspects and provide a complete picture of the presented technical invention. The figures may be related hierarchically (systems vs components), sequentially (steps of a process), different views (top-bottom-left-right), or in other ways. Since we train PATENTLMM to generate descriptions for individual patent figures, our model hallucinates the cross-figure references for 2 brief and 5 detailed descriptions. Qualitative examples are presented in Appendix D.2.\n\nTable 5: GPT-4V evaluation on a set of 1K samples.   \n\n<html><body><table><tr><td>Description</td><td>Method</td><td>Rel.</td><td>Acc.</td><td>Compl.</td><td>Coh.</td><td>Fluency</td><td>Cover.</td></tr><tr><td rowspan=\"2\">Brief</td><td>LLaVA-1.5</td><td>1.38</td><td>1.06</td><td>1.01</td><td>1.85</td><td>1.98</td><td>0.98</td></tr><tr><td>Ours</td><td>1.44</td><td>1.18</td><td>1.17</td><td>1.91</td><td>2.00</td><td>1.15</td></tr><tr><td rowspan=\"2\">Detailed</td><td>LLaVA-1.5</td><td>0.75</td><td>0.75</td><td>0.73</td><td>1.07</td><td>1.69</td><td>0.71</td></tr><tr><td>Ours</td><td>0.90</td><td>0.78</td><td>0.76</td><td>1.15</td><td>1.85</td><td>0.75</td></tr></table></body></html>\n\nGPT-4V Evaluation Results: Apart from small-scale manual error analysis, we utilize the GPT-4V model to qualitatively evaluate the performance of LLaVA-1.5 and our proposed PATENTLMM model on the brief and detailed description generation task for a set of 1000 samples. We input the GPT-4V model with the patent figure, the ground truth description and the description generated using these models, along with the special instruction prompt. The instruction prompt instructs the GPT-4V model to rate the generated description on the following criterion: Relevance, Accuracy, Completeness (with respect to input image), Fluency and Coverage (with respect to input image and ground truth description) on an integer scale of 0 to 2. To mitigate randomness in scores, we set the temperature parameter to 0 for the GPT-4V model and created five versions of the instruction prompt. The scores obtained from each of the prompts for each criterion are then averaged. Table 5 shows that our system generates high-quality results.\n\n# 6 Conclusion and Future Work\n\nOur work addresses the existing gap in the automated generation of patent figure descriptions by introducing PATENTDESC-355K, a comprehensive dataset of patent figures and their corresponding brief and detailed descriptions. We further proposed PATENTLMM, a large multimodal model comprising a domain-specialized image encoder PATENTMME and a domain-adapted patentLLaMA model for generating brief and detailed descriptions from patent figures. Extensive experiments demonstrated that our proposed PATENTLMM outperforms competent baselines by significant margins. Future research in this direction can explore experiments with patents in multiple languages, patent document-level reasoning to allow for cross-figure references while generating descriptions, incorporating external knowledge bases from technical domains to improve the performance of detailed description generation, and generation of grounded descriptions.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   Ëá™Âä®ÂåñÁîüÊàê‰∏ìÂà©Âõæ‰∏≠ÁöÑÊèèËø∞ÊòØ‰∏Ä‰∏™Ë¢´Á†îÁ©∂Á§æÂå∫ÂøΩËßÜÁöÑÈáçË¶Å‰ªªÂä°„ÄÇ‰∏ìÂà©Âõæ‰∏≠ÁöÑÊèèËø∞ÂØπ‰∫éÁü•ËØÜÂÖ±‰∫´ÂíåÁü•ËØÜ‰∫ßÊùÉ‰øùÊä§Ëá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÊâãÂä®ÁºñÂÜôËøô‰∫õÊèèËø∞Êó¢ËÄóÊó∂ÂèàË¥πÂäõ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåËá™Âä®ÂåñÁîüÊàê‰∏ìÂà©ÂõæÊèèËø∞ÂèØ‰ª•ÊòæËëóÊèêÈ´ò‰∏ìÂà©Â§ÑÁêÜÁöÑÊïàÁéáÔºåÂáèÂ∞ë‰∫∫Â∑•Âä≥Âä®ÔºåÂêåÊó∂Á°Æ‰øùÊèèËø∞ÁöÑÂáÜÁ°ÆÊÄßÂíå‰∏ÄËá¥ÊÄß„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫ÜPATENTLMMÔºå‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éÁîüÊàê‰∏ìÂà©ÂõæÊèèËø∞ÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÁî±‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÁªÑÊàêÔºöPATENTMMEÔºà‰∏ìÈó®Áî®‰∫é‰∏ìÂà©ÂõæÁöÑÂ§öÊ®°ÊÄÅËßÜËßâÁºñÁ†ÅÂô®ÔºâÂíåPATENTLLAMAÔºàÂü∫‰∫éLLaMAÁöÑÈ¢ÜÂüüÈÄÇÂ∫îÁâàÊú¨Ôºâ„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Ë¥°ÁåÆ1Ôºö** ÊèêÂá∫‰∫ÜPATENTDESC-355KÔºå‰∏Ä‰∏™ÂåÖÂê´355K‰∏ìÂà©ÂõæÂèäÂÖ∂ÊèèËø∞ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜ„ÄÇ\\n> *   **Ë¥°ÁåÆ2Ôºö** ÊèêÂá∫‰∫ÜPATENTLMMÊ®°ÂûãÔºåÈÄöËøá‰∏ìÈó®ËÆæËÆ°ÁöÑËßÜËßâÁºñÁ†ÅÂô®ÂíåÈ¢ÜÂüüÈÄÇÂ∫îÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÊòæËëóÊèêÂçá‰∫ÜÁîüÊàêÊèèËø∞ÁöÑË¥®Èáè„ÄÇ\\n> *   **Ë¥°ÁåÆ3Ôºö** Âú®ÁîüÊàêÁÆÄË¶ÅÊèèËø∞ÂíåËØ¶ÁªÜÊèèËø∞ÁöÑ‰ªªÂä°‰∏≠ÔºåPATENTLMMÂú®Âπ≥ÂùáBLEUÊåáÊ†á‰∏äÂàÜÂà´ÊØîÊúÄ‰Ω≥Âü∫Á∫øÊ®°ÂûãÊèêÂçá‰∫Ü10.22%Âíå4.43%„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   PATENTLMMÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøá‰∏ìÈó®ËÆæËÆ°ÁöÑËßÜËßâÁºñÁ†ÅÂô®ÔºàPATENTMMEÔºâÂíåÈ¢ÜÂüüÈÄÇÂ∫îÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºàPATENTLLAMAÔºâÊù•ÊçïÊçâ‰∏ìÂà©Âõæ‰∏≠ÁöÑÁã¨ÁâπÁªìÊûÑÂíåËØ≠‰πâ‰ø°ÊÅØ„ÄÇPATENTMMEÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁõÆÊ†áÔºàÂ¶ÇÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°„ÄÅÂ∏ÉÂ±ÄÊÑüÁü•Êé©Á†ÅÂõæÂÉèÂª∫Ê®°ÂíåÂõæÂÉèÂùóÂàÜÁ±ªÔºâÊù•Â≠¶‰π†‰∏ìÂà©ÂõæÁöÑÁªìÊûÑÁâπÂæÅ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** Áé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàÂ¶ÇMiniGPT-4ÂíåLLaVAÔºâÂú®‰∏ìÂà©ÂõæÊèèËø∞ÁîüÊàê‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨‰∏ªË¶ÅÈíàÂØπËá™ÁÑ∂Âú∫ÊôØÂõæÂÉèËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÊó†Ê≥ïÊúâÊïàÊçïÊçâ‰∏ìÂà©Âõæ‰∏≠ÁöÑÁªìÊûÑÂåñÂÖÉÁ¥†„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** PATENTMMEÈÄöËøá‰∏ìÈó®ËÆæËÆ°ÁöÑÈ¢ÑËÆ≠ÁªÉÁõÆÊ†áÔºàÂ¶ÇÂ∏ÉÂ±ÄÊÑüÁü•Êé©Á†ÅÂõæÂÉèÂª∫Ê®°ÂíåÂõæÂÉèÂùóÂàÜÁ±ªÔºâÊù•Â≠¶‰π†‰∏ìÂà©Âõæ‰∏≠ÁöÑÁªìÊûÑÂåñÂÖÉÁ¥†ÔºàÂ¶ÇËäÇÁÇπ„ÄÅÁÆ≠Â§¥„ÄÅÊñáÊú¨Á≠âÔºâ„ÄÇPATENTLLAMAÂàôÈÄöËøáÂú®Â§ßËßÑÊ®°‰∏ìÂà©ÊñáÊú¨‰∏äËøõË°åÈ¢ÜÂüüÈÄÇÂ∫îÔºåÁîüÊàêÁ¨¶Âêà‰∏ìÂà©ËØ≠Ë®ÄÈ£éÊ†ºÁöÑÊèèËø∞„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   1. **ËßÜËßâÁºñÁ†ÅÂô®ÔºàPATENTMMEÔºâÈ¢ÑËÆ≠ÁªÉÔºö** ‰ΩøÁî®Â∏ÉÂ±ÄÊÑüÁü•Êé©Á†ÅÂõæÂÉèÂª∫Ê®°ÔºàLAMIMÔºâÂíåÂõæÂÉèÂùóÂàÜÁ±ªÔºàPCÔºâÁõÆÊ†áÂØπPATENTMMEËøõË°åÈ¢ÑËÆ≠ÁªÉÔºå‰ª•Â≠¶‰π†‰∏ìÂà©ÂõæÁöÑÁªìÊûÑÁâπÂæÅ„ÄÇ\\n> *   2. **ËØ≠Ë®ÄÊ®°ÂûãÔºàPATENTLLAMAÔºâÈ¢ÜÂüüÈÄÇÂ∫îÔºö** Âú®Âìà‰ΩõUSPTOÊï∞ÊçÆÈõÜÔºàHUPDÔºâ‰∏äÂØπLLaMA-2 7BÊ®°ÂûãËøõË°åÈ¢ÜÂüüÈÄÇÂ∫îÔºåÁîüÊàêÁ¨¶Âêà‰∏ìÂà©ËØ≠Ë®ÄÈ£éÊ†ºÁöÑÊèèËø∞„ÄÇ\\n> *   3. **Ê®°ÂûãÊï¥ÂêàÔºàPATENTLMMÔºâÔºö** ÈÄöËøá‰∏Ä‰∏™ÊäïÂΩ±Â±ÇÂ∞ÜPATENTMMEÂíåPATENTLLAMAÊï¥ÂêàÔºåÁîüÊàêÊúÄÁªàÁöÑ‰∏ìÂà©ÂõæÊèèËø∞„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ËÆ∫Êñá‰∏≠ÂØπÊØî‰∫ÜÂ§öÁßçÂü∫Á∫øÊ®°ÂûãÔºåÂåÖÊã¨BLIP-2„ÄÅTextMonkey„ÄÅPEGASUS„ÄÅmPLUG-owl2„ÄÅUReader„ÄÅLLaVA-1.5„ÄÅGPT-4V„ÄÅGIT„ÄÅBLIP„ÄÅMiniGPT-4ÂíåOFA„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®BLEU-4ÊåáÊ†á‰∏äÔºö** Âú®ÁîüÊàêÁÆÄË¶ÅÊèèËø∞ÁöÑ‰ªªÂä°‰∏≠ÔºåPATENTLMMËææÂà∞‰∫Ü36.66ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãLLaVA-1.5Ôºà25.00ÔºâÂíåMiniGPT-4Ôºà17.96Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü11.66‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ROUGE-LÊåáÊ†á‰∏äÔºö** Âú®ÁîüÊàêËØ¶ÁªÜÊèèËø∞ÁöÑ‰ªªÂä°‰∏≠ÔºåPATENTLMMËææÂà∞‰∫Ü27.54ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãLLaVA-1.5Ôºà24.48ÔºâÂíåMiniGPT-4Ôºà15.67Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü3.06‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®METEORÊåáÊ†á‰∏äÔºö** Âú®ÁîüÊàêÁÆÄË¶ÅÊèèËø∞ÁöÑ‰ªªÂä°‰∏≠ÔºåPATENTLMMËææÂà∞‰∫Ü56.44ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãLLaVA-1.5Ôºà48.23ÔºâÂíåMiniGPT-4Ôºà43.03Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü8.21‰∏™ÁôæÂàÜÁÇπ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ‰∏ìÂà©ÂõæÊèèËø∞ÁîüÊàê (Patent Figure Description Generation, N/A)\\n*   Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã (Large Multimodal Model, LMM)\\n*   ËßÜËßâÁºñÁ†ÅÂô® (Vision Encoder, N/A)\\n*   È¢ÜÂüüÈÄÇÂ∫î (Domain Adaptation, N/A)\\n*   Â∏ÉÂ±ÄÊÑüÁü•Êé©Á†ÅÂõæÂÉèÂª∫Ê®° (Layout-Aware Masked Image Modeling, LAMIM)\\n*   ÂõæÂÉèÂùóÂàÜÁ±ª (Patch Classification, PC)\\n*   ‰∏ìÂà©Êï∞ÊçÆÈõÜ (Patent Dataset, N/A)\\n*   Áü•ËØÜ‰∫ßÊùÉ‰øùÊä§ (Intellectual Property Protection, N/A)\"\n}\n```"
}