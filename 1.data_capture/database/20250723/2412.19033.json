{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.19033",
    "link": "https://arxiv.org/abs/2412.19033",
    "pdf_link": "https://arxiv.org/pdf/2412.19033.pdf",
    "title": "Neural Networks Perform Sufficient Dimension Reduction",
    "authors": [
        "Shuntuo Xu",
        "Zhou Yu"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-12-26",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Mathematics",
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Key Laboratory of Advanced Theory and Application in Statistics and Data Science – MOE",
        "School of Statistics, East China Normal University"
    ],
    "paper_content": "# Neural Networks Perform Sufficient Dimension Reduction\n\nShuntuo Xu, Zhou $\\mathbf { Y } \\mathbf { u } ^ { * }$\n\nKey Laboratory of Advanced Theory and Application in Statistics and Data Science – MOE School of Statistics, East China Normal University, Shanghai, China oaksword@163.com, zyu $@$ stat.ecnu.edu.cn\n\n# Abstract\n\nThis paper investigates the connection between neural networks and sufficient dimension reduction (SDR), demonstrating that neural networks inherently perform SDR in regression tasks under appropriate rank regularizations. Specifically, the weights in the first layer span the central mean subspace. We establish the statistical consistency of the neural network-based estimator for the central mean subspace, underscoring the suitability of neural networks in addressing SDR-related challenges. Numerical experiments further validate our theoretical findings, and highlight the underlying capability of neural networks to facilitate SDR compared to the existing methods. Additionally, we discuss an extension to unravel the central subspace, broadening the scope of our investigation.\n\n# Introduction\n\nNeural networks have achieved significant success in a tremendous variety of applications (Lee and Abu-El-Haija 2017; Silver et al. 2018; Jumper et al. 2021; Brandes et al. 2022; Thirunavukarasu et al. 2023). As the most essential foundation, a feedforward neural network is typically constructed by a series of linear transformations and nonlinear activations. To be specific, a function $f$ implemented by a feedforward neural network with $L$ layers can be represented as\n\n$$\nf ( x ) = \\phi _ { L } \\circ \\sigma _ { L - 1 } \\circ \\phi _ { L - 1 } \\circ \\cdot \\cdot \\cdot \\circ \\sigma _ { 1 } \\circ \\phi _ { 1 } ( x ) .\n$$\n\nHere, $\\scriptscriptstyle \\mathrm { ~ o ~ }$ is the functional composition operator, $\\phi _ { i } ( z ) ~ =$ $W _ { i } ^ { \\top } z + b _ { i }$ denotes the linear transformation and $\\sigma _ { i }$ represents the elementwise nonlinear activation function. Despite a clear architecture, the formula (1) provides limited insight into how the information within the input data is processed by the neural networks. To comprehensively understand how neural networks retrieve task-relevant information, there have been extensive efforts towards unraveling their interpretability (Doshi-Velez and $\\mathrm { K i m } \\ 2 0 1 7$ ; Guidotti et al. 2018; Zhang et al. 2021). For instance, post-hoc interpretability (Lipton 2018) focused on the predictions generated by neural networks, while disregarding the detailed mechanism and feature importance. Ghorbani, Abid, and\n\nZou (2019) highlighted the fragility of this type of interpretation, as indistinguishable perturbations could result in completely different interpretations.\n\nFor the sake of striking a balance between the complexity of representation and the power of prediction, Tishby and Zaslavsky (2015) and Saxe et al. (2019) pioneered the interpretability of deep neural networks via the information bottleneck theory. Ghosh (2022) further linked the information bottleneck to sufficient dimension reduction (SDR) (Li 1991; Cook 1996), which is a rapidly developing research field in regression diagnostics, data visualization, pattern recognition and etc.\n\nIn this paper, we provide a theoretical understanding of neural networks for representation learning from the SDR perspective. Let $x \\in \\mathbb { R } ^ { p }$ represent the covariates and $y \\in$ $\\mathbb { R }$ represent the response. Consider the following regression model\n\n$$\n\\begin{array} { r } { y = f _ { 0 } ( B _ { 0 } ^ { \\top } x ) + \\epsilon , } \\end{array}\n$$\n\nwhere $B _ { 0 } ~ \\in ~ \\mathbb { R } ^ { p \\times d }$ is a nonrandom matrix with $d \\ \\leq \\ p$ , $f _ { 0 } : \\mathbb { R } ^ { d }  \\mathbb { R }$ is an unknown function, and $\\epsilon$ is the noise such that $\\mathbb { E } ( \\epsilon | x ) = 0$ and $\\mathrm { V a r } ( \\epsilon | x ) = \\nu ^ { 2 }$ for some positive constant $\\nu$ . Intuitively, the semiparametric and potentially multi-index model (2) asserts that the core information for regression is encoded in the low-dimensional linear representation $B _ { 0 } ^ { \\top } x$ . In the literature of SDR, model (2) has been extensively studied. Based on model (2) , Cook and Li (2002) proposed the objective of sufficient mean dimension reduction as\n\n$$\ny \\perp \\perp \\mathbb { E } ( y | x ) | B ^ { \\top } x\n$$\n\nfor some matrix $B$ , where means statistical independence. Denote the column space spanned by $B$ as $\\Pi _ { B }$ , which is commonly referred to as the mean dimension-reduction subspace. It is evident that the matrix $B$ satisfying (3) is far away from uniqueness. Hence, we focus on the intersection of all possible mean dimension-reduction subspace, which is itself a mean dimension-reduction subspace under mild conditions (e.g., when the domain of $x$ is open and convex; see Cook and Li (2002)), and term it the central mean subspace. Agreeing to condition (3), $B _ { 0 }$ defined in model (2) yields the central mean subspace, denoted as $\\Pi _ { B _ { 0 } }$ , under certain assumption. Statistical estimation and inference about the central mean subspace is the primary goal of SDR. Popular statistical methods for recovering central mean subspace include ordinary least squares (Li and Duan 1989), principal Hessian directions (Li 1992), minimum average variance estimation (Xia et al. 2002), semiparametric approach (Ma and Zhu 2013), generalized kernel-based dimension reduction (Fukumizu and Leng 2014), and many others. Although numerous studies have demonstrated the ability of neural networks to approximate complex functions (Hornik, Stinchcombe, and White 1989; Barron 1993; Yarotsky 2017; Shen, Yang, and Zhang 2021) and to adapt low-dimensional structures (Bach 2017; Bauer and Kohler 2019; SchmidtHieber 2020; Abbe, Adsera, and Misiakiewicz 2022; Jiao et al. 2023; Troiani et al. 2024), it is of subsequent interest to investigate whether neural networks are capable of correctly identifying the intrinsic structure encapsulated in $B _ { 0 }$ , thereby deepening the interpretation of neural networks.\n\nOur study was inspired by an observation that the weight matrix in the first layer, i.e., $W _ { 1 }$ , could accurately detect the presence of $B _ { 0 }$ in a toy data set, with the rank regularization. Specifically, consider the toy model $y = ( B _ { 0 } ^ { \\mathcal { \\tilde { T } } } x ) _ { - } ^ { 3 } + \\epsilon$ where $\\bar { x } \\sim \\mathrm { U n i f o r m } ( [ 0 , 1 ] ^ { 1 0 } )$ , $B _ { 0 } = ( 1 , - 2 , 0 , . . . , 0 ) ^ { \\top }$ and $\\epsilon \\sim \\mathrm { N o r m a l } ( 0 , 0 . 1 ^ { 2 } )$ . We trained neural networks using the least squares loss with $W _ { 1 } = W _ { 1 1 } W _ { 1 2 }$ where $W _ { 1 1 } \\in \\mathbb { R } ^ { 1 0 \\times q }$ and $\\bar { W _ { 1 2 } } \\in \\mathbb { R } ^ { q \\times 6 4 }$ for $q = 1 , \\ldots , 1 0$ . Evidently, the rank of $W _ { 1 }$ did not exceed $q$ . It was then observed that for each $q$ , (i) $B _ { 0 }$ was closely contained within the column space of $W _ { 1 1 }$ , as the absolute cosine similarity between $B _ { 0 }$ and its projection on $\\Pi _ { W _ { 1 1 } }$ was close to 1, and (ii) the leading eigenvector of $W _ { 1 } W _ { 1 } ^ { \\top }$ closely aligned with $B _ { 0 }$ (see Figure 1). This observation indicates that the first layer of the neural network may potentially discover the underlying low-dimensional intrinsic structure.\n\nIt is important to note that the application of neural networks for estimating the central mean subspace based on model (2) was previously explored by Kapla, Fertl, and Bura (2022) through a two-stage method. The first stage focused on obtaining a preliminary estimation of $\\Pi _ { B _ { 0 } }$ , which subsequently served as an initial point for the joint estimation of $\\Pi _ { B _ { 0 } }$ and $f _ { 0 }$ in the second stage. Given our toy example, however, it is prudent to critically evaluate the necessity of the first stage. Furthermore, their work lacks a comprehensive theoretical guarantee. Additionally, another related work conducted by Liang, Sun, and Liang (2022) concentrated on seeking nonlinear sufficient representations. In contrast, we focus more on revealing the fundamental nature of neural networks.\n\n![](images/8dd333af202e3c45ab1061356ea08673d24b4dce256ba2ccb528b8be98cd5d24.jpg)  \nFigure 1: Absolute cosine similarity between (i) $B _ { 0 }$ and its projection on $\\Pi _ { W _ { 1 1 } }$ (the dot line with triangle marks), (ii) $B _ { 0 }$ and the leading eigenvector of $W _ { 1 } W _ { 1 } ^ { \\top }$ (the solid line with square marks).\n\nWe in this paper show that, with suitable rank regularization, the first layer of a feedforward neural network conducts SDR in a regression task, wherein $d ( W _ { 1 1 } , B _ { 0 } ) \\to 0$ in probability for certain distance metric $d ( \\cdot , \\cdot )$ . Furthermore, numerical experiments provide empirical evidences supporting this result, while demonstrating the efficiency of neural networks in addressing the issue of SDR.\n\nThroughout this paper, we use $\\| v \\| _ { 2 }$ to represent the Euclidean norm of a vector $v$ . For a matrix $A$ , $\\| A \\| _ { F }$ is the Frobenius norm of $A$ , $\\pi _ { A } = A ( A ^ { \\top } A ) ^ { - } A ^ { \\top }$ denotes the projection matrix of $A$ where $A ^ { - }$ is the generalized inverse of $A$ , and $\\Pi _ { A }$ stands for the linear space spanned by the columns of $A$ . For a measurable function $f : \\mathcal { X } \\ \\to \\ \\mathbb { R } .$ , $\\| f \\| _ { L ^ { 2 } ( \\mu ) }$ represents the $L ^ { 2 }$ norm of $f$ with respect of a given probability measure $\\mu$ , and $\\| f \\| _ { L ^ { \\infty } ( \\mathcal { X } ) } \\ = \\ \\operatorname* { s u p } _ { x \\in \\mathcal { X } } | f ( x ) |$ represents the supreme norm of $f$ over a set $\\mathcal { X }$ . $B ( \\mathcal { X } )$ is the unit ball induced by $\\chi$ , such that $B ( { \\mathcal { X } } ) \\subset { \\mathcal { X } }$ and $\\lVert \\boldsymbol { v } \\rVert _ { 2 } \\leq 1$ for any $v \\in \\mathcal X$ .\n\n# Theoretical Justificatitons Population-Level Unbiasedness\n\nSuppose the true intrinsic dimension $d$ defined in model (2) is known and the covarites $x \\ \\in \\ B ( [ 0 , 1 ] ^ { p } )$ . As $B _ { 0 }$ is not identifiable in (2) and (3) , it is assumed without loss of generality that $B _ { 0 } ^ { \\top } B _ { 0 } = { \\dot { I } } _ { d }$ where $I _ { d }$ is the identity matrix with $d$ rows. By defining $\\bar { \\Psi _ { d } } = \\{ \\bar { B } ^ { \\prime } \\in \\mathbb { R } ^ { p \\times d } : \\bar { B } ^ { \\top } B = I _ { d } \\}$ we have $B _ { 0 } \\in \\Psi _ { d }$ . In this paper, we consider the following neural network function class\n\n$$\n\\begin{array} { r l } & { \\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R } } = \\Big \\{ } \\\\ & { f ( x ) = \\phi _ { L } \\circ \\sigma _ { L - 1 } \\circ \\phi _ { L - 1 } \\circ \\cdot \\cdot \\cdot \\circ \\sigma _ { 1 } \\circ \\phi _ { 1 } ( B ^ { \\top } x ) : } \\\\ & { B \\in \\Psi _ { d } , \\phi _ { i } ( z ) = W _ { i } ^ { \\top } z + b _ { i } , W _ { i } \\in \\mathbb { R } ^ { d _ { i } \\times d _ { i + 1 } } , i = 1 , \\cdot \\cdot \\cdot , L , } \\end{array}\n$$\n\n$$\n\\| f \\| _ { L ^ { \\infty } ( \\mathcal { B } ( [ 0 , 1 ] ^ { p } ) ) } \\leq \\mathcal { R } \\Big \\}\n$$\n\nThe activation functions $\\sigma _ { i } ( i = 1 , \\ldots , L - 1 )$ utilized are the rectified linear units, i.e., $\\sigma _ { i } ( x ) = \\operatorname* { m a x } ( x , 0 ) .$ . We emphasize that $\\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , S , \\mathcal { R } }$ incorporates rank regularization of $d$ in the first layer. For arbitrary $f \\in \\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R } }$ , we use $\\boldsymbol { \\mathcal { T } } ( \\boldsymbol { f } )$ to represent the component $B \\in \\Psi _ { d }$ in the first layer of $f$ .\n\nFor a regression task, the theoretical study is highly related to the smoothness of underlying conditional mean function (Yang and Barron 1999). Here, we introduce the following assumptions of model (2).\n\nAssumption 1 (Smoothness). $f _ { 0 }$ is a H¨older continuous function of order $\\alpha \\in ( 0 , 1 ]$ with constant $\\lambda _ { i }$ , i.e., $| f _ { 0 } ( x ) -$ $f _ { 0 } ( z ) | \\leq \\lambda \\| x - z \\| _ { 2 } ^ { \\alpha }$ for any $x , z \\in [ 0 , 1 ] ^ { d }$ . Additionally, $\\| f _ { 0 } \\| _ { L ^ { \\infty } ( [ 0 , 1 ] ^ { d } ) } \\leq \\mathcal { R } _ { 0 }$ for some constant $\\mathcal { R } _ { 0 } \\geq 1$ .\n\nAssumption 2 (Sharpness). For any scalar $\\delta > 0$ and $B \\in$ $\\Psi _ { d } ,$ , $\\Vert \\pi _ { B } - \\pi _ { B _ { 0 } } \\Vert _ { F } > \\delta$ implies $\\mathbb { E } \\{ \\mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) | B ^ { \\top } x ] \\} >$ $M ( \\delta )$ for some $M ( \\delta ) > 0$ .\n\nAssumption 3. y is sub-exponentially distributed such that there exists $\\tau > 0$ satisfying $\\mathbb { E } [ \\exp ( \\tau \\vert y \\vert ) ] < \\infty$ .\n\nAssumption 1 is a technical condition to study the approximation capability of neural networks (Shen, Yang, and Zhang 2020). Alternatively, other functional spaces, such as the Sobolev space, can also be employed for this purpose (Abdeljawad and Grohs 2022; Shen et al. 2022). Furthermore, Assumption 2 establishes a restriction on the sharpness of $f _ { 0 }$ . Consider the case that $f _ { 0 }$ is solely a constant function as zero. Then, a trivial neural network by setting all the parameters except $B$ to zero perfectly fits $f _ { 0 }$ , regardless of the value of $B$ . With Assumption 2, it becomes difficult to accurately capture the overall behavior of $f _ { 0 } ( B _ { 0 } ^ { \\top } x )$ using a biased $B$ . In other words, $B _ { 0 } ^ { \\top } x$ is sufficient and necessary for recovering $\\mathbb { E } ( y | x )$ , i.e., $\\Pi _ { B _ { 0 } }$ is the central mean subspace. Similar condition was also adopted in Theorem 4.2 of Li and Dong (2009) to distinguish sufficient directions $B _ { 0 }$ from other $B$ . Assumption 3 is a commonly used condition for applying empirical process tools and concentration inequalities (Van der Vaart 2000; Zhu, Jiao, and Jordan 2022).\n\nTheorem 1. Suppose that Assumptions $\\boldsymbol { { \\mathit { 1 } } }$ and 2 hold. Let $\\begin{array} { r } { f ^ { * } = \\mathrm { a r g m i n } _ { f \\in \\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R } } } \\mathbb { E } [ y - f ( x ) ] ^ { 2 } } \\end{array}$ , then\n\n$$\n\\Pi _ { \\mathcal { T } ( f ^ { * } ) } = \\Pi _ { B _ { 0 } } ,\n$$\n\nprovided that $\\mathcal { R }$ is sufficiently large, and $\\mathcal { L }$ and $\\mathcal { M }$ tend to infinity.\n\nTheorem 1 builds a bridge connecting neural networks and SDR. It demonstrates that neural networks indeed achieve representation learning as the first layer of the optimal neural network perfectly reaches the target of SDR at population level. Theorem 1 also inspires us to perform SDR based on neural networks with a minor adjustment for the first layer. The detailed proof of Theorem 1 can be found in Section Proofs.\n\n# Sample Estimation Consistency\n\nWe now investigate the theoretical property of the neural network-based sample-level estimator for SDR. Given sample observations $\\bar { \\mathcal { D } _ { n } } \\ = \\ \\{ ( X _ { 1 } , Y _ { 1 } ) , \\ldots , ( X _ { n } , Y _ { n } ) \\}$ , where $( X _ { i } , Y _ { i } )$ is an independent copy of $( x , y )$ for $i = 1 , \\ldots , n ,$ , the commonly used least squares loss is adopted, i.e.,\n\n$$\nL _ { n } ( f ) = { \\frac { 1 } { n } } \\sum _ { i = 1 } ^ { n } [ Y _ { i } - f ( X _ { i } ) ] ^ { 2 } .\n$$\n\nDenote the optimal neural network estimator at the sample level as\n\n$$\n\\hat { f } _ { n } = \\underset { f \\in \\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R } } } { \\mathrm { a r g m i n } } L _ { n } ( f ) .\n$$\n\n$\\mathcal { T } ( \\hat { f } _ { n } )$ is then the sample estimator approximately spanning the central mean subspace.\n\nTo examine the closeness between $\\Pi _ { \\mathcal { T } ( \\hat { f } _ { n } ) }$ and $\\Pi _ { B _ { 0 } }$ , we define the distance metric $d ( \\cdot , \\cdot )$ as\n\n$$\nd ( B , B _ { 0 } ) = \\operatorname* { m i n } _ { Q \\in \\mathcal { Q } } \\| B _ { 0 } - B Q \\| _ { 2 } ,\n$$\n\nwhere $B \\in \\Psi _ { d }$ and $\\mathcal { Q }$ is the collection of all orthogonal matrices in $\\mathbb { R } ^ { d \\times d }$ . We see that $d ( B , B _ { 0 } ) = 0$ if and only if $\\Pi _ { B } = \\Pi _ { B _ { 0 } }$ . And we make the following assumption essentially another view of Assumption 2.\n\nAssumption 4. For any positive scalar $\\delta$ , $d ( B , B _ { 0 } ) > \\delta$ implies $\\mathbb { E } \\{ \\mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) | \\hat { B } ^ { \\top } x ] \\} > M _ { 1 } ( \\delta )$ for some $M _ { 1 } ( \\delta ) >$ 0.\n\nTheorem 2. Suppose the Assumptions $\\jmath$ , 3 and 4 hold. Then we have\n\n$$\nd ( \\mathcal { T } ( \\hat { f } _ { n } ) , B _ { 0 } ) \\xrightarrow { P } 0 ,\n$$\n\nwhen the depth $\\mathcal { L } = O ( n ^ { d / ( 2 d + 8 \\alpha ) } )$ , the width $\\mathcal { M } = O ( 1 )$ and $\\mathcal { R } \\geq \\mathcal { R } _ { 0 }$ .\n\nTheorem 2 confirms that $\\Pi _ { \\mathcal { T } ( \\hat { f } _ { n } ) }$ converges to $\\Pi _ { B _ { 0 } }$ in probability. As a result, under the least squares loss, the neural networks, with appropriate rank regularization, provide a consistent estimator of the central mean subspace. Therefore, it is promising to adopt the neural network function class $\\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , S , \\mathcal { R } }$ to address sufficient mean dimension reduction problems. More importantly, this approach offers several advantages over existing methods. Unlike SIR (Li 1991), SAVE (Cook and Weisberg 1991) and PHD (Li 1992), our method does not impose stringent probabilistic distributional conditions on $x$ , such as linearity, constant variance and normality assumptions. Compared to MAVE (Xia et al. 2002), our method adopts the powerful neural networks to estimate the link function, thereby working similar or better than classical nonparametric tools based on firstorder Taylor expansion. We present the proof of Theorem 2 in Section Proofs.\n\nThe results illustrated in Theorem 1 and 2 are contingent on the availability of true intrinsic dimension $d$ . In the case where $d$ is unknown, a natural modification for $\\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , S , \\mathcal { R } }$ is to set $\\boldsymbol { B } \\in \\mathbb { R } ^ { p \\times p }$ without rank regularization. Under Assumptions 1 and 2, in this scenario, the optimal $f ^ { * }$ at the population level still ensures that $\\Pi _ { \\mathcal { T } ( f ^ { * } ) }$ encompasses $\\Pi _ { B _ { 0 } }$ , where the sharpness of $f _ { 0 }$ plays a crucial role; see the Supplementary Material for more details.\n\n# Simulation Study\n\nFrom the perspective of numerical experiments, we utilized simulated data sets to demonstrate that (i) the column space of $\\mathcal { T } ( \\hat { f } _ { n } )$ approached the central mean subspace $\\Pi _ { B _ { 0 } }$ as sample size $n$ increased, (ii) the performance of neural network-based method in conducting SDR was comparable to classical methods. In some cases, the neural network-based method outperformed classical methods, particularly when the latent intrinsic dimension $d$ was large. Five commonly used methods, sliced inverse regression (SIR), sliced average variance estimation (SAVE), principal Hessian directions (PHD), minimum average variance estimation (MAVE) and generalized kernel dimension reduction (GKDR), were included for comparisons.\n\nThe neural network-based method was implemented in Python using PyTorch. Specifically, we utilized a linear layer without bias term to represent the matrix $B$ (recall that we suppose $d$ is known), which was further appended by a fullyconnected feedforward neural network with number of neurons being $h { - } h / 2 { - } 1$ . Here, we set $h = 6 4$ when the sample size was less than 1000, and set $h = 1 2 8$ when sample size was between 1000 and 2000. Overall, the neural network architecture was $p - d - h - h / 2 - 1$ . Code is available at https://github.com/oaksword/DRNN.\n\n<html><body><table><tr><td colspan=\"2\"></td><td>NN</td><td>MAVE</td><td></td><td>GKDR</td><td>SIR</td><td>SAVE</td><td>PHD</td></tr><tr><td rowspan=\"6\">Setting 1</td><td>(n,p)= (100,10)</td><td>mean</td><td>0.135</td><td>0.160</td><td>0.388</td><td>0.897</td><td>0.315</td><td>0.623</td></tr><tr><td></td><td>std</td><td>0.064</td><td>0.056</td><td>0.126</td><td>0.186</td><td>0.082</td><td>0.150</td></tr><tr><td>(n,p)= (200,30)</td><td>mean</td><td>0.276</td><td>0.214</td><td>1.012</td><td>0.946</td><td>0.298</td><td>0.807</td></tr><tr><td>(n,p)= (300,30)</td><td>std</td><td>0.274</td><td>0.051</td><td>0.299</td><td>0.113</td><td>0.049</td><td>0.107</td></tr><tr><td></td><td>mean</td><td>0.120</td><td>0.130</td><td>0.542</td><td>0.900</td><td>0.337</td><td>0.709</td></tr><tr><td></td><td>std</td><td>0.038</td><td>0.029</td><td>0.166</td><td>0.136</td><td>0.063</td><td>0.108</td></tr><tr><td rowspan=\"6\">Setting 2</td><td>σ=0.1</td><td>mean</td><td>0.296</td><td>0.730</td><td>1.130</td><td>0.665</td><td>0.319</td><td>1.550</td></tr><tr><td>σ=0.2</td><td>std</td><td>0.126</td><td>0.312</td><td>0.271</td><td>0.166</td><td>0.076</td><td>0.126</td></tr><tr><td></td><td>mean</td><td>0.628</td><td>0.899</td><td>1.155</td><td>0.705</td><td>0.338</td><td>1.526</td></tr><tr><td>σ=0.5</td><td>std</td><td>0.269</td><td>0.329</td><td>0.237</td><td>0.149</td><td>0.074</td><td>0.138</td></tr><tr><td></td><td>mean std</td><td>1.197 0.213</td><td>1.187 0.204</td><td>1.278 0.200</td><td>0.830</td><td>0.367</td><td>1.567</td></tr><tr><td>n=200</td><td></td><td></td><td></td><td></td><td>0.169</td><td>0.072</td><td>0.131</td></tr><tr><td rowspan=\"6\">Setting 3</td><td></td><td>mean</td><td>0.639</td><td>1.246</td><td>1.759</td><td>1.669</td><td>1.728</td><td>1.740</td></tr><tr><td>n= 500</td><td>std</td><td>0.418</td><td>0.289</td><td>0.149</td><td>0.235</td><td>0.136</td><td>0.221</td></tr><tr><td></td><td>mean</td><td>0.248</td><td>1.076</td><td>1.752</td><td>1.650</td><td>1.683</td><td>1.721</td></tr><tr><td>n= 1000</td><td>std</td><td>0.242</td><td>0.331</td><td>0.125</td><td>0.271</td><td>0.201</td><td>0.228</td></tr><tr><td></td><td>mean</td><td>0.075 0.081</td><td>0.924</td><td>0.554</td><td>1.652</td><td>1.678</td><td>1.737</td></tr><tr><td>n=1000,d= 4</td><td>std</td><td></td><td>0.382</td><td>0.371</td><td>0.259</td><td>0.245</td><td>0.191</td></tr><tr><td rowspan=\"5\">Setting 4</td><td></td><td>mean</td><td>0.127</td><td>0.293</td><td>0.368</td><td>1.363</td><td>0.429</td><td>0.960</td></tr><tr><td>n=1500,d=6</td><td>std</td><td>0.161</td><td>0.050</td><td>0.079</td><td>0.167</td><td>0.079</td><td>0.178</td></tr><tr><td></td><td>mean</td><td>0.144</td><td>0.415</td><td>0.386</td><td>1.530</td><td>0.467</td><td>0.902</td></tr><tr><td>n= 2000,d=8</td><td>std</td><td>0.067</td><td>0.076</td><td>0.077</td><td>0.153</td><td>0.082</td><td>0.206</td></tr><tr><td></td><td>mean std</td><td>0.140 0.055</td><td>0.360 0.084</td><td>0.344 0.010</td><td>1.410 0.163</td><td>0.364 0.065</td><td>0.735 0.175</td></tr></table></body></html>\n\nTable 1: The results of average and standard deviation of $\\| \\pi _ { \\hat { B } } - \\pi _ { B _ { 0 } } \\| _ { F }$ on 100 replicates across different methods. NN represents the neural network-based method.\n\nWe considered the following four scenarios, with two of them attaining small $d = 1 , 2$ and the rest of $d \\geq 3$ .\n\nSetting 1: $\\bar { y } = x _ { 1 } ^ { 4 } + \\epsilon$ where $x \\in \\mathrm { N o r m a l } ( 0 , I _ { p } )$ and $\\epsilon \\in { \\mathfrak { t } } _ { 5 }$\n\nSetting 2: $\\begin{array} { r c l } { y } & { \\bar { = } } & { \\log ( x _ { 1 } + x _ { 1 } x _ { 2 } ) + \\epsilon } \\end{array}$ where $\\mathrm { ~ { ~ \\bf ~ { ~ x ~ } ~ } ~ } \\in \\mathrm { \\Gamma }$ Uniform $( [ 0 , 1 ] ^ { 1 0 } )$ and $\\epsilon \\in \\mathrm { N o r m a l } ( 0 , \\sigma ^ { 2 } )$ .\n\nSetting 3: $\\begin{array} { r } { \\dot { y = ( 1 + \\beta _ { 1 } ^ { \\top } x ) ^ { 2 } \\exp ( \\beta _ { 2 } ^ { \\top } x ) + 5 { \\cdot } 1 ( \\beta _ { 3 } ^ { \\top } x > 0 ) + \\epsilon } } \\end{array}$ where $x \\in \\mathrm { U n i f o r m } ( [ - 1 , 1 ] ^ { 6 } ) , \\epsilon \\sim \\chi _ { 2 } ^ { 2 } - 2$ and\n\n$$\n( \\beta _ { 1 } , \\beta _ { 2 } , \\beta _ { 3 } ) = \\left( { \\begin{array} { c c c c c c } { - 2 } & { - 1 } & { 0 } & { 1 } & { 2 } & { 3 } \\\\ { 1 } & { 1 } & { 1 } & { 1 } & { 1 } & { 1 } \\\\ { 1 } & { - 1 } & { 1 } & { - 1 } & { 1 } & { - 1 } \\end{array} } \\right) ^ { \\top } .\n$$\n\nSetting 4:\n\n$$\ny = \\sum _ { k = 1 } ^ { d } \\frac { e ^ { x _ { i } } } { 2 + \\sin ( \\pi x _ { i } ) } + \\epsilon ,\n$$\n\nwhere $\\epsilon \\sim \\mathrm { N o r m a l } ( 0 , 0 . 1 ^ { 2 } )$ ,\n\n$$\nx \\sim \\mathrm { N o r m a l } \\left( 1 _ { 1 0 } , I _ { 1 0 } - 1 _ { 1 0 } 1 _ { 1 0 } ^ { \\top } / 2 0 \\right) .\n$$\n\nIn setting 1, we tested the combinations of $\\begin{array} { l l } { ( n , p ) } & { = } \\end{array}$ (100, 10), (200, 30) and (300, 30), where $n$ was the sample size. In setting 2, we fixed $n = 2 0 0 , p = 1 0$ and set $\\sigma = 0 . 1 , 0 . 2 , 0 . 5$ . In setting 3, we fixed $p = 6$ and tested $n = 2 0 0 , 5 0 0 , 1 0 0 0$ . In setting 4, we fixed $p = 1 0$ and tested $( n , d ) \\ : = \\ : ( 1 0 0 0 , 4 )$ , (1500, 6) and $( 2 0 0 0 , 8 )$ . Settings 1, 2, 4 were equipped with continuous regression functions, and setting 3 involved discontinuity. We set the number of iterations of our method to 1000 for settings 1 and 2, and increased the iterations to 2000 and 4000 for settings 3 and 4, due to their high complexity.\n\nTo evaluate the performance of each method, we calculated the distance metric $\\big \\| \\pi _ { \\hat { B } } - \\pi _ { B _ { 0 } } \\big \\| _ { F }$ where $\\hat { B }$ represented the estimate of $B _ { 0 }$ . Particularly, for our neural networkbased method, $\\hat { B } \\ = \\ \\mathcal { T } ( \\hat { f } _ { n } )$ . Smaller value of this metric indicated better performance. We ran 100 replicates for each setting. The results are displayed in Table 1 and Figure 2.\n\nIn simple cases (settings 1 and 2), the neural networkbased method showed similar performance to classical methods, with sliced average variance estimation being the most effective method in setting 2. However, complex settings demonstrated the significant superiority of neural network-based method compared to other methods. Specifically, in setting 3, as the sample size $n$ increased, the metric $\\big \\| \\pi _ { \\hat { B } } - \\pi _ { B _ { 0 } } \\big \\| _ { F }$ decreased rapidly, indicating the convergence of $\\Pi _ { \\hat { B } }$ to $\\Pi _ { B _ { 0 } }$ . According to the results in setting 4, the neural network-based method was capable of handling higherdimensional scenarios. In summary, numerical studies advocated neural networks as a powerful tool for SDR.\n\n# Real Data Analysis\n\nWe further applied the neural network-based method involving rank regularization to a real regression task. In practice,\n\n三 丰   \n1.0 1.0 1.0   \n0.5 重中 0.5 丰 1 1 0.5 王 丰 中 王 重 + 中中 + 宝 Setting 1, n = 100, p = 20 0.0 Setting 1, n = 200, p = 30 0.0 Setting 1, n = 300, $p = 3 0$ 手 南 王   \n1.5 中 1.5 + 雨 1.5 南雨車 电   \n1.0 + 南 1.0 牛 1.0 王 王 王 +   \n0.5 中 生 T 0.5 王 T重 0.5   \n0.0 Setting 2, $\\overline { { \\sigma = 0 . 1 } }$ 0.0 Setting 2, $\\sigma = 0 . 2$ 0.0 Setting 2, $\\overline { { \\sigma = 0 . 5 } }$   \n2.0 中雨更电 2.0 重雨中电 2.0 工 王   \n1.5 1.5 王 1.5 +   \n1.0 1.0 + 1.0   \n0.5 0.5 + 0.5 主   \n0.0 T Setting 3, n = 200 0.0 中 Setting 3, $n = 5 0 0$ 0.0 美 Setting 3, $\\overline { { \\mathsf { n } = 1 0 0 0 } }$ + 幸 王   \n1.5 王 1.5 H 三 1.5 中 车 中 中 山 1.0   \n1.0 1.0 丰 丰 丰   \n0.5 王 重 0.5 工圭 1 0.5 丰王 圭圭 全 中 地 电电 壹   \n0.0 0.0 0.0 NN MAVE GKDR SIR SAVE PHD NN MAVE GKDR SIR SAVE PHD NN MAVE GKDR SIR SAVE PHD Setting 4, $\\mathsf { n } = 1 0 0 0$ , ${ \\mathsf { d } } = 4$ Setting 4, $\\mathsf { n } = 1 5 0 0$ , ${ \\mathsf { d } } = 6$ Setting 4, $n = 2 0 0 0$ , ${ \\mathsf { d } } = 8$\n\nthe precise intrinsic dimension $d$ is unknown, and it is uncertain whether a low-dimensional structure exists. To address this issue, we used cross validation to determine an appropriate $d$ from the range of values $\\{ 1 , \\ldots , p \\}$ . More specifically, the raw data set was divided into $80 \\%$ training data and $20 \\%$ testing data. The optimal value of $d$ was determined through cross validation on the training data. Subsequently, the final model was fit using the selected $d$ on the training data, and the mean squared error on the testing data was evaluated.\n\nIn order to reduce randomness, the aforementioned process was repeated 20 times and the resulting testing errors were recorded. Additionally, we conducted a comparative analysis between the neural network-based approach and alternative methods including vanilla neural network without rank regularization, SIR-based regression, SAVE-based regression, and MAVE-based regression. For the latter three techniques, we initially executed SDR to acquire the embedded data, followed by the utilization of a fully-connected neural network for predictive purpose. The optimal value for $d$ was also determined through cross validation.\n\nWe utilized a data set of weather records from Seoul, South Korea during the summer months from 2013 to 2017 (Cho et al. 2020), available at the UCI data set repository (bias correction of numerical prediction model temperature forecast). This data set contained 7750 observations with 23 predictors and 2 responses, specifically the next-day maximum air temperature and next-day minimum air temperature. After excluding the variables for station and date, the data set was reduced to 21 predictors, which were further standardized using StandardScaler in scikit-learn package.\n\nTable 2: The results of testing errors on 20 replicates across different methods. We report the average and standard deviation of testing errors, along with averaged optimal $d$ determined through cross validation. NN-RR, NN-VN, MAVE, SIR, SAVE represent the neural network-based method with rank regularization, vanilla neural network regression, MAVE-based regression, SIR-based regression, and SAVEbased regression, respectively.   \n\n<html><body><table><tr><td></td><td>NN-RR</td><td>NN-VN</td><td>MAVE</td><td>SIR</td><td>SAVE</td></tr><tr><td>mean</td><td>0.602</td><td>0.774</td><td>0.743</td><td>1.324</td><td>0.772</td></tr><tr><td>std</td><td>0.043</td><td>0.116</td><td>0.159</td><td>0.190</td><td>0.161</td></tr><tr><td>d</td><td>19.6</td><td></td><td>19.25</td><td>19.6</td><td>20.6</td></tr></table></body></html>\n\nIt is evident from Table 2 that the neural network-based method with rank regularization outperformed other methods, demonstrating the effectiveness of the modification compared to the vanilla neural network, and the sound performance in reducing dimensions compared to other SDR methods. The presence of latent structures was partially supported by the averaged optimal $d$ . It was possible that 19 or 20 combinations of raw predictors might be sufficient, as opposed to the original 21 predictors.\n\n# Proofs\n\n# Proof of Theorem 1\n\nDefine the vanilla neural network function class without rank regularization as\n\n$$\n\\begin{array} { r l } & { \\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R } } ^ { * } = \\Big \\{ } \\\\ & { f ( x ) = \\phi _ { L } \\circ \\sigma _ { L - 1 } \\circ \\phi _ { L - 1 } \\circ \\cdot \\cdot \\cdot \\circ \\sigma _ { 1 } \\circ \\phi _ { 1 } ( x ) : } \\\\ & { \\phi _ { i } ( z ) = W _ { i } ^ { \\top } z + b _ { i } , W _ { i } \\in \\mathbb { R } ^ { d _ { i } \\times d _ { i + 1 } } , i = 1 , \\cdot \\cdot . . , L , } \\end{array}\n$$\n\n$$\n\\begin{array} { r l } & { \\operatorname { w i t h } L = \\mathcal { L } , \\displaystyle \\operatorname* { m a x } _ { i = 1 , \\ldots , L + 1 } d _ { i } = \\mathcal { M } , \\displaystyle \\sum _ { i = 1 } ^ { L } ( d _ { i } + 1 ) d _ { i + 1 } = \\mathcal { S } , } \\\\ & { \\| f \\| _ { L ^ { \\infty } ( [ 0 , 1 ] ^ { d } ) } \\leq \\mathcal { R } \\Big \\} . } \\end{array}\n$$\n\nLemma 1. Suppose that $h$ is H¨older continuous with $\\alpha \\in$ $( 0 , 1 ]$ and $\\lambda > 0$ . Then, for any $\\zeta > 0$ , there exists a function $g$ in neural network function class $\\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , S , \\mathcal { R } } ^ { * }$ , with rectified linear unit activation and $\\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R }$ large enough, such that\n\n$$\n\\| h - g \\| _ { L ^ { \\infty } ( [ 0 , 1 ] ^ { d } ) } < \\zeta .\n$$\n\nWe note that Lemma 1 is a simplified version of Theorem 1.1 in (Shen, Yang, and Zhang 2020).\n\nProof of Theorem $\\jmath$ . For $f \\in \\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R } }$ such that $\\pi _ { T ( f ) } \\neq$ $\\pi _ { B _ { 0 } }$ , under Assumption 2, there exists a scalar $t > 0$ satisfying\n\n$$\n\\mathbb { E } \\left\\{ \\mathrm { V a r } \\left[ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) | { \\mathcal T } ( f ) ^ { \\top } x \\right] \\right\\} > t .\n$$\n\nThen, we have\n\n$$\n\\begin{array} { r l } { \\mathbb { E } [ y - f ( x ) ] ^ { 2 } = \\mathbb { E } [ \\epsilon + f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - f ( x ) ] ^ { 2 } } \\\\ & { = \\mathbb { E } ( \\epsilon ^ { 2 } ) + \\mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - f ( x ) ] ^ { 2 } } \\\\ & { = \\nu ^ { 2 } + \\mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - f ( x ) ] } \\\\ & { \\phantom { = } + \\mathbb { E } ^ { 2 } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - f ( x ) ] } \\\\ & { \\geq \\nu ^ { 2 } + \\mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - f ( x ) ] } \\\\ & { \\geq \\nu ^ { 2 } + \\mathbb { E } \\{ \\mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) | \\mathcal { T } ( f ) ^ { \\top } x ] \\} } \\\\ & { > \\nu ^ { 2 } + t . } \\end{array}\n$$\n\nOn the other hand, for  ∈ F , , , such that π ˜ = $\\pi _ { B _ { 0 } }$ , there exists an orthogonal matrix $Q \\in \\mathbb { R } ^ { d \\times d }$ such that $B _ { 0 } \\stackrel { \\cdot } { = } \\tau ( \\tilde { f } ) Q$ . Hence, $\\tilde { f } ( x ) = \\tilde { f } \\circ \\mathcal T ( \\tilde { f } ) \\circ Q ( B _ { 0 } ^ { \\top } x )$ and $\\tilde { f } \\circ T ( \\tilde { f } ) \\circ Q$ is still a neural network function. Assumption 1 and Lemma 1 imply that there is a neural network function $g$ satisfying\n\n$$\n\\| f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - g ( B _ { 0 } ^ { \\top } x ) \\| _ { L ^ { \\infty } ( \\mathcal { B } ( [ 0 , 1 ] ^ { p } ) ) } < t ^ { 1 / 2 } / 2 ,\n$$\n\nfor sufficiently large $\\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R }$ . As a result, select $\\tilde { f }$ such that $\\tilde { f } \\circ \\mathcal { T } ( \\tilde { f } ) \\circ Q = g$ , and it follows that\n\n$$\n\\begin{array} { r } { \\mathbb { E } [ y - \\tilde { f } ( x ) ] ^ { 2 } = \\nu ^ { 2 } + \\mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - g ( B _ { 0 } ^ { \\top } x ) ] ^ { 2 } < \\nu ^ { 2 } + t / 4 . } \\end{array}\n$$\n\nTherefore, for any $f \\in \\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R } }$ such that $\\pi _ { \\mathcal { T } ( f ) } \\neq \\pi _ { B _ { 0 } }$ , there exists a neural network function $\\tilde { f }$ , with the same rank regularization, satisfying\n\n$$\n\\begin{array} { r } { \\mathbb { E } [ y - \\tilde { f } ( x ) ] ^ { 2 } < \\mathbb { E } [ y - f ( x ) ] ^ { 2 } . } \\end{array}\n$$\n\nTo conclude, $\\pi _ { \\mathcal { T } ( f ^ { * } ) } = \\pi _ { B _ { 0 } }$ , which entails that $\\Pi _ { \\mathcal { T } ( f ^ { * } ) } =$ ΠB0.\n\n# Proof of Theorem 2\n\nLet $L ( f ) = \\mathbb { E } [ y - f ( x ) ] ^ { 2 }$ . Without loss of generality, suppose $\\mathcal { R } = \\mathcal { R } _ { 0 }$ . We first present some useful lemmas.\n\nLemma 2. For sufficiently large $n$ , it follows that\n\n$$\n\\operatorname* { s u p } _ { f \\in { \\mathcal { F } } _ { \\angle , M , S , { \\mathcal { R } } } } | L ( f ) - L _ { n } ( f ) | = O _ { p } \\left( { \\sqrt { \\frac { S { \\mathcal { L } } \\log ( S ) ( \\log n ) ^ { 5 } } { n } } } \\right) .\n$$\n\nThe proof of Lemma 2 is presented in the Supplementary Material. We note this convergence rate may not be optimal, but is sufficient for deducing consistency.\n\nLemma 3. Under Assumptions 1 and 3, for sufficiently large $n$ , we have\n\n$$\n\\begin{array} { r } { \\mathbb { E } [ y - \\hat { f } _ { n } ( x ) ] ^ { 2 } \\leq \\nu ^ { 2 } + C n ^ { - 2 \\alpha / ( d + 4 \\alpha ) } ( \\log n ) ^ { 3 } , } \\end{array}\n$$\n\nwhere $C$ is a constant depending on $\\mathcal { R } _ { 0 }$ and $d$ .\n\nProof of Lemma 3. We begin with the decomposition that\n\n$$\n\\begin{array} { r l } & { | L ( j )  \\leqslant \\mathbb { E } \\Bigg [ | a _ { \\mathrm { p } }  \\Bigg | \\frac { \\partial } { \\partial a _ { \\mathrm { p } } } \\Bigg | \\int _ { \\mathcal { C } } \\frac { \\mathrm { d } \\hat { \\mathcal { G } } } { \\partial a _ { \\mathrm { p } , \\mathcal { A } } ( j ) } \\Bigg | \\frac { \\partial \\hat { \\mathcal { G } } } { \\partial a _ { \\mathrm { p } } } \\Bigg | \\mathrm { d } \\hat { \\mathcal { G } } \\Bigg \\rangle - L _ { \\mathrm { a p } } ( \\hat { \\mathcal { G } } ) \\Bigg | \\hat { \\mathcal { G } } \\Bigg \\rangle } \\\\ & { \\qquad \\le \\mathbb { E } \\Bigg [ \\Bigg | \\underset { \\mathcal { C } } { \\underbrace { \\mathrm { e f f . e f . } \\mathrm { i n f } } } \\frac { \\mathrm { d } } { \\partial a _ { \\mathrm { p } } } \\Bigg | \\int _ { \\mathcal { C } } \\hat { \\mathcal { T } } ( j ) \\cdot \\mathcal { D } _ { \\mathrm { a p } } ^ { \\dagger } \\Bigg | } \\\\ & { \\qquad + \\underset { \\mathcal { C } } { \\underbrace { \\mathrm { s u p . } \\mathrm { s u p . } } } \\Bigg | | \\hat { \\mathcal { G } } ( j ) - \\mathcal { D } _ { \\mathrm { a l } } ( \\hat { \\mathcal { G } } )  \\Bigg | \\Bigg ] } \\\\ & { \\qquad \\le \\mathbb { E } \\Bigg [ \\Bigg | \\underset { \\mathcal { C } } { \\underbrace { \\mathrm { e f f . e f . } \\mathrm { i n f } } } \\frac { \\mathrm { d } } { \\partial a _ { \\mathrm { p } } } \\Bigg | | ( j ) - \\mathcal { D } _ { \\mathrm { a l } } ( \\hat { \\mathcal { G } } )  \\Bigg | \\Bigg ] } \\\\ & { \\qquad + \\underset { \\int _ { \\mathcal { C } } \\le \\mathbb { E } \\times \\mathcal { B } , \\mathrm { a d d } } { \\underbrace { 1 } \\int _ { \\mathcal { C } } \\mathrm { s u p . } } | | | \\hat { \\mathcal { G } } ( j ) - \\mathcal { D } _ { \\mathrm { a l } } ( \\hat { \\mathcal { G } } )  | } \\\\ & { \\qquad \\le \\int _ { \\mathcal { C } } \\frac { \\mathrm { d } \\hat { \\mathcal { G } } } { \\varepsilon _ { \\mathrm { A d } } } \\frac { \\partial } { \\partial a _ { \\mathrm { p } } } | | \\hat { \\mathcal { G } } ( j ) - \\mathcal { D } _ { \\mathrm { a d } } ( \\hat { \\mathcal { G } } )  | } \\\\ &  \\qquad + \\sum _ { \\mathcal { C } } \\frac { \\mathrm { d } \\hat { \\mathcal { G } } } { \\varepsilon _ { \\mathrm { A d } } } \\frac { \\mathrm { d } } { \\varepsilon _ { \\mathrm { A d } } } \\frac { \\mathrm { d } } { \\varepsilon _ { \\mathrm { A d } } } \\frac { \\mathrm { d } } { \\varepsilon _ { \\mathrm { B } } } \\Bigg | \\mathrm { d } \\hat { \\mathcal { G } } ( j ) - \\mathcal { D } _  \\mathrm  a \\end{array}\n$$\n\nwhere $C _ { 1 }$ is a constant, for sufficiently large $n$ . Hence,\n\n$$\n\\begin{array} { r l } & { \\quad L ( \\hat { f _ { n } } ) - \\nu ^ { 2 } } \\\\ & { = L ( \\hat { f _ { n } } ) - L ( \\hat { f _ { 0 } } \\circ B _ { 0 } ^ { \\top } ) } \\\\ & { = L ( \\hat { f _ { n } } ) - \\frac { \\mathrm { i n f } } { \\rho } } \\\\ & { \\quad + L ( \\hat { f _ { n } } ) - \\frac { \\mathrm { i n f } } { \\rho } } \\\\ & { \\quad \\quad + \\frac { \\mathrm { i n f } } { f _ { 0 } \\kappa \\rho _ { \\mathrm { A } , \\lambda \\sigma , \\kappa } ^ { - 1 } } L ( f \\circ \\mathcal { T } ( f ) \\circ B _ { 0 } ^ { \\top } ) } \\\\ & { \\quad \\quad + \\frac { \\mathrm { i n f } } { f _ { 0 } \\kappa \\rho _ { \\mathrm { A } , \\lambda \\sigma , \\kappa } ^ { - 1 } } L ( f \\circ \\mathcal { T } ( f ) \\circ B _ { 0 } ^ { \\top } ) - L ( f _ { 0 } \\circ B _ { 0 } ^ { \\top } ) } \\\\ & { \\quad \\le C _ { 1 } \\sqrt { \\kappa \\xi \\log ( \\delta ) ( \\log n ) ^ { 5 } n ^ { - 1 } } } \\\\ & { \\quad \\quad + \\frac { \\mathrm { i n f } } { f _ { 0 } \\kappa \\rho _ { \\mathrm { A } , \\lambda \\sigma , \\kappa } ^ { - 1 } } \\mathbb { R } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - f \\circ \\mathcal { T } ( f ) ( B _ { 0 } ^ { \\top } x ) ] ^ { 2 } } \\\\ & { \\quad \\le C _ { 1 } \\sqrt { \\kappa \\xi \\log ( \\delta ) ( \\log n ) ^ { 5 } n ^ { - 1 } } } \\\\ &  \\quad \\quad + \\frac { \\mathrm { i n f } } { \\rho \\sigma \\kappa \\rho _ { \\mathrm { A } , \\lambda \\sigma , \\kappa , \\eta } ^ { - 1 } } \\| f _ { 0 } - g \\| _ { L ^ { \\infty } ( \\{ 0 , 1 \\} ^ { 1 \\} ) } ^ { 2 } . } \\end{array}\n$$\n\nBy Theorem 1.1 in Shen, Yang, and Zhang (2020), there exists $g ^ { \\ast }$ with $\\mathcal { M } \\ = \\ 3 ^ { d + 3 } \\operatorname* { m a x } ( d \\lfloor K ^ { 1 / d } \\rfloor , K + 1 ) , \\mathcal { L } \\ =$ $1 2 D + 1 4 + 2 d$ for some constant $K , D > 0$ such that\n\n$$\n\\| f _ { 0 } - g ^ { * } \\| _ { L ^ { \\infty } ( [ 0 , 1 ] ^ { d } ) } \\leq 1 9 \\sqrt { d } \\lambda ( K D ) ^ { - 2 \\alpha / d } .\n$$\n\nLet\n\n$$\nD = O \\left( n ^ { d / ( 2 d + 8 \\alpha ) } \\right) , \\quad K = O ( 1 ) ,\n$$\n\nand observe that $S = O ( \\mathcal { M } ^ { 2 } \\mathcal { L } )$ . Then, it follows that\n\n$$\n\\begin{array} { r l } & { L ( \\hat { f } _ { n } ) = \\mathbb { E } [ y - \\hat { f } _ { n } ( x ) ] ^ { 2 } } \\\\ & { \\qquad \\leq \\nu ^ { 2 } + C n ^ { - 2 \\alpha / ( d + 4 \\alpha ) } ( \\log { n } ) ^ { 3 } , } \\end{array}\n$$\n\nwhere $C$ is a constant depending on $\\mathcal { R } _ { 0 }$ and $d$ .\n\nWith Lemma 3, the proof of Theorem 2 is a direct application of Theorem 5.9 in Van der Vaart (2000).\n\nProof of Theorem 2. Recall that $m _ { 0 } = f _ { 0 } \\circ B _ { 0 } ^ { \\top }$ . Let $R ( f ) =$ $L ( \\boldsymbol { f } ) - L ( \\boldsymbol { m } _ { 0 } ) = \\mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \\top } \\boldsymbol { x } ) - f ( \\boldsymbol { x } ) ] ^ { 2 } \\geq 0$ and $R _ { n } ( f ) =$ $L _ { n } ( f ) - L _ { n } ( m _ { 0 } )$ . Then, Lemma 2 indicates that\n\n$$\n\\operatorname* { s u p } _ { f \\in { \\mathcal { F } } _ { \\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R } } } | R _ { n } ( f ) - R ( f ) |  0 , \\quad \\mathrm { i n ~ p r o b a b i l i t y } ,\n$$\n\nyielding $R _ { n } ( { \\hat { f } } _ { n } ) = o _ { p } ( 1 )$ by incorporating with Lemma 3. We denote the metric\n\n$$\nd _ { 1 } ( f , m _ { 0 } ) = \\operatorname* { m i n } _ { Q \\in \\mathcal { Q } } \\| B _ { 0 } - \\mathcal { T } ( f ) Q \\| _ { 2 } \\vee \\| f _ { 0 } \\circ Q - f \\circ \\mathcal { T } ( f ) \\| _ { L ^ { 2 } ( \\mu ) } .\n$$\n\nHere, $f \\in \\mathcal { F } _ { \\mathcal { L } , \\mathcal { M } , \\mathcal { S } , \\mathcal { R } } , m _ { 0 } = f _ { 0 } \\circ B _ { 0 } ^ { \\intercal }$ , $a \\lor b$ means $\\operatorname* { m a x } ( a , b )$ , and $\\mu$ is the probability distribution of $\\tau ( f ) ^ { \\intercal } x$ . Recall that $\\begin{array} { r } { d ( B , B _ { 0 } ) = \\operatorname* { m i n } _ { Q \\in \\mathcal { Q } } { \\| B _ { 0 } - B Q \\| _ { 2 } } } \\end{array}$ , where $B \\in \\Psi _ { d }$ .\n\nFor $f$ and $\\delta > 0$ such that $d _ { 1 } ( f , m _ { 0 } ) \\geq \\delta$ , if\n\n$$\nd ( \\mathcal { T } ( f ) , B _ { 0 } ) \\leq \\tilde { \\delta } = \\operatorname* { m i n } \\{ [ \\delta ^ { 2 } / ( 8 \\mathcal { R } _ { 0 } \\lambda ) ] ^ { 1 / \\alpha } , \\delta / 2 \\} ,\n$$\n\nwe have\n\n$$\n\\begin{array} { r l } & { | f _ { 0 } ( B _ { 0 } ^ { \\top } \\boldsymbol { x } ) - f _ { 0 } ( Q ^ { \\top } \\mathcal { T } ( f ) ^ { \\top } \\boldsymbol { x } ) | \\leq \\lambda \\| B _ { 0 } ^ { \\top } \\boldsymbol { x } - Q ^ { \\top } \\mathcal { T } ( f ) ^ { \\top } \\boldsymbol { x } \\| _ { 2 } ^ { \\alpha } } \\\\ & { \\qquad \\leq \\lambda \\| B _ { 0 } ^ { \\top } - Q ^ { \\top } \\mathcal { T } ( f ) ^ { \\top } \\| _ { 2 } ^ { \\alpha } } \\\\ & { \\qquad \\leq \\frac { \\delta ^ { 2 } } { 8 \\mathcal { R } _ { 0 } } , } \\end{array}\n$$\n\nfor some orthogonal matrix $Q$ . Hence,\n\n$$\n\\begin{array} { r l } & { R ( f ) = \\mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - f ( x ) ] ^ { 2 } } \\\\ & { \\qquad \\geq \\mathbb { E } [ f _ { 0 } ( Q ^ { \\top } \\mathcal { T } ( f ) ^ { \\top } x ) - f ( x ) ] ^ { 2 } } \\\\ & { \\qquad + \\ 2 \\mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \\top } x ) - F _ { 0 } ] [ F _ { 0 } - f ( x ) ] } \\\\ & { \\qquad \\geq - \\displaystyle \\frac { \\delta ^ { 2 } } { 2 } + d _ { 1 } ( f , m _ { 0 } ) ^ { 2 } \\geq \\displaystyle \\frac { \\delta ^ { 2 } } { 2 } > 0 , } \\end{array}\n$$\n\nwhere $F _ { 0 } = f _ { 0 } ( Q ^ { \\top } \\mathcal { T } ( f ) ^ { \\top } x )$ .\n\nFor the case that $d ( T ( f ) , B _ { 0 } ) > \\tilde { \\delta }$ , by Assumption 4, we have $R ( f ) \\geq \\xi > 0$ for some positive constant $\\xi$ . To conclude, we obtain\n\n$$\n\\operatorname* { i n f } _ { f : d _ { 1 } ( f , m _ { 0 } ) \\geq \\delta } R ( f ) > 0 ,\n$$\n\nfor any $\\delta > 0$ . Finally, applying Theorem 5.9 in Van der Vaart (2000), $R _ { n } ( { \\hat { f } } _ { n } ) { \\stackrel { \\cdot } { = } } o _ { p } ( 1 )$ implies that $d _ { 1 } ( \\hat { f } _ { n } , m _ { 0 } )  0$ in probability. Hence, $d ( \\mathcal { T } ( \\hat { f } _ { n } ) , B _ { 0 } ) \\to 0$ in probability.\n\n# Discussion\n\nWe have demonstrated that neural networks attain the capability of detecting the underlying low-dimensional structure that preserves all information in $x$ about the conditional mean function $\\mathbb { E } ( y | x )$ . As a result, neural networks are suitable to be utilized for the estimation of central mean subspace $\\Pi _ { B _ { 0 } }$ . The theoretical investigations sharpen our understanding of neural networks, while broadening the scope of SDR as well.\n\nIn the context of SDR, a more general scenario than sufficient mean dimension reduction emerges when considering\n\n$$\ny \\perp \\perp x | B _ { 0 } ^ { \\top } x .\n$$\n\nAnd the column space spanned by $B _ { 0 }$ corresponds to the central subspace (Cook 1998a; Li 2018). It is clear that relation (4) is equivalent to that $p ( y | x ) = p ( y | B _ { 0 } ^ { \\top } x )$ , where $p ( \\cdot | \\cdot )$ represents the conditional probability density function. Following the work of Xia (2007), we can further adapt neural networks to estimate the central subspace by modifying the loss function.\n\nUnder mild conditions, Xia (2007) showed that\n\n$$\n\\mathbb { E } [ K _ { h } ( y - y _ { 0 } ) | x = x _ { 0 } ]  p ( y _ { 0 } | B _ { 0 } ^ { \\top } x _ { 0 } ) , \\quad \\mathrm { a s ~ } h  0 ^ { + } .\n$$\n\nHere, $( x _ { 0 } , y _ { 0 } )$ is a fixed point, and $K _ { h } ( \\cdot )$ is a suitable kernel function with a bandwidth $h$ . Such finding then implies that\n\n$$\n\\begin{array} { r l r } {  { K _ { h } ( y - y _ { 0 } ) = \\mathbb { E } [ K _ { h } ( y - y _ { 0 } ) | x ] + \\mathrm { r e m a i n d e r ~ t e r m } } } \\\\ & { } & { \\approx p ( y _ { 0 } | B _ { 0 } ^ { \\top } x ) + \\mathrm { r e m a i n d e r ~ t e r m } . } \\end{array}\n$$\n\nBased on this discovery, it is natural to employ a neural network function $f ( B ^ { \\top } \\dot { x } , y _ { 0 } ) ~ : ~ \\mathbb { R } ^ { d + 1 } \\  ~ \\dot { \\mathbb { R } }$ to approximate $p ( y _ { 0 } | B _ { 0 } ^ { \\top } x )$ , and obtain the estimate of $B _ { 0 }$ at the population level by solving the following problem\n\n$$\n( B ^ { * } , f ^ { * } ) = \\underset { B , f } { \\operatorname { a r g m i n } } \\mathbb { E } [ K _ { h } ( y - \\tilde { y } ) - f ( B ^ { \\top } x , \\tilde { y } ) ] ^ { 2 } ,\n$$\n\nwhere $\\tilde { y }$ is an independent copy of $y$ . Empirically, we define the loss function as\n\n$$\n\\tilde { L } _ { n } ( B , f ) = \\frac { 1 } { n ^ { 2 } } \\sum _ { i = 1 } ^ { n } \\sum _ { j = 1 } ^ { n } [ K _ { h } ( y _ { j } - y _ { i } ) - f ( B ^ { \\top } x _ { j } , y _ { i } ) ] ^ { 2 } .\n$$\n\nWe provide some additional simulation results to verify the feasibility utilizing neural networks for the estimation of central subspace; see the Supplementary Material. Theoretical analysis of neural networks for the estimation of the central subspace, including unbiasedness and consistency, deserves further studies.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文探讨了神经网络与充分降维（Sufficient Dimension Reduction, SDR）之间的联系，证明了在适当的秩正则化条件下，神经网络在回归任务中能够自动执行SDR。\\n> *   该问题的重要性在于，它揭示了神经网络在无需显式设计的情况下，能够自动发现数据中的低维结构，这对于高维数据的可视化和模式识别具有关键价值。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种理论框架，证明在秩正则化条件下，神经网络的第一层权重能够张成中心均值子空间（central mean subspace）。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   证明了神经网络在回归任务中能够自动执行SDR，第一层权重张成中心均值子空间。\\n> *   建立了基于神经网络的中心均值子空间估计器的统计一致性。\\n> *   数值实验验证了理论结果，并展示了神经网络在SDR任务中的优越性能。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   论文的核心思想是通过秩正则化约束神经网络第一层的权重矩阵，使其能够自动发现数据中的低维结构。这种方法的有效性源于秩正则化能够限制权重矩阵的秩，从而迫使网络学习低维表示。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前的工作通常需要显式设计降维步骤，或者依赖于两阶段方法（如Kapla等人的工作），缺乏理论保证。\\n> *   **本文的改进：** 本文提出了一种单阶段方法，通过秩正则化直接约束神经网络的第一层权重，并提供了理论保证。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.   定义神经网络函数类，其中第一层权重矩阵受到秩正则化约束。\\n> 2.   使用最小二乘损失函数训练神经网络。\\n> 3.   通过理论分析证明第一层权重的列空间收敛到中心均值子空间。\\n\\n> **案例解析 (Case Study)**\\n> *   论文提供了一个玩具模型示例，其中响应变量y与输入x的关系为y = (B₀ᵀx)³ + ε。实验结果表明，神经网络的第一层权重能够准确捕捉B₀的方向。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   切片逆回归（Sliced Inverse Regression, SIR）\\n> *   切片平均方差估计（Sliced Average Variance Estimation, SAVE）\\n> *   主Hessian方向（Principal Hessian Directions, PHD）\\n> *   最小平均方差估计（Minimum Average Variance Estimation, MAVE）\\n> *   广义核降维（Generalized Kernel Dimension Reduction, GKDR）\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在降维准确性上：** 本文方法在Setting 1（n=100, p=10）上的平均误差为0.135，显著优于基线模型MAVE（0.160）和GKDR（0.388）。与表现最佳的基线相比，提升了15.6%。\\n> *   **在样本量增加时的收敛性上：** 随着样本量增加，本文方法的估计误差迅速下降，表明其具有良好的统计一致性。例如，在设置3中，样本量从200增加到1000时，本文方法的误差从0.639降至0.075。\\n> *   **在高维场景下的表现上：** 本文方法能够有效处理高维数据。例如，在设置4中，当维度增加到8时，本文方法的误差为0.140，而MAVE为0.360，GKDR为0.344。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   充分降维 (Sufficient Dimension Reduction, SDR)\\n*   神经网络 (Neural Networks, NN)\\n*   中心均值子空间 (Central Mean Subspace, CMS)\\n*   秩正则化 (Rank Regularization, N/A)\\n*   回归任务 (Regression Task, N/A)\\n*   统计一致性 (Statistical Consistency, N/A)\\n*   数值实验 (Numerical Experiments, N/A)\"\n}\n```"
}