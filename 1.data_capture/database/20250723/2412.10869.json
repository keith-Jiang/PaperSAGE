{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.10869",
    "link": "https://arxiv.org/abs/2412.10869",
    "pdf_link": "https://arxiv.org/pdf/2412.10869.pdf",
    "title": "TinySubNets: An efficient and low capacity continual learning strategy",
    "authors": [
        "Marcin Pietro'n",
        "Kamil Faber",
        "Dominik Żurek",
        "Roberto Corizzo"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-14",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "AGH University of Krakow",
        "American University"
    ],
    "paper_content": "# TinySubNets: An Efficient and Low Capacity Continual Learning Strategy\n\nMarcin Pietron´1, Kamil Faber1, Dominik ˙Zurek1, Roberto Corizzo2\n\n1AGH University of Krakow, Krakow, Poland 2American University, Washington DC, USA pietron@agh.edu.pl, kfaber@agh.edu.pl, dzurek@agh.edu.pl, rcorizzo@american.edu\n\n# Abstract\n\nContinual Learning (CL) is a highly relevant setting gaining traction in recent machine learning research. Among CL works, architectural and hybrid strategies are particularly effective due to their potential to adapt the model architecture as new tasks are presented. However, many existing solutions do not efficiently exploit model sparsity, and are prone to capacity saturation due to their inefficient use of available weights, which limits the number of learnable tasks. In this paper, we propose TinySubNets (TSN), a novel architectural CL strategy that addresses the issues through the unique combination of pruning with different sparsity levels, adaptive quantization, and weight sharing. Pruning identifies a subset of weights that preserve model performance, making less relevant weights available for future tasks. Adaptive quantization allows a single weight to be separated into multiple parts which can be assigned to different tasks. Weight sharing between tasks boosts the exploitation of capacity and task similarity, allowing for the identification of a better trade-off between model accuracy and capacity. These features allow TSN to efficiently leverage the available capacity, enhance knowledge transfer, and reduce computational resources consumption. Experimental results involving common benchmark CL datasets and scenarios show that our proposed strategy achieves better results in terms of accuracy than existing state-of-the-art CL strategies. Moreover, our strategy is shown to provide a significantly improved model capacity exploitation.\n\n# Code — https://github.com/lifelonglab/tinysubnets\n\n# Introduction\n\nContinual learning (CL) is a machine learning paradigm aiming at designing effective methods and strategies to analyze data in complex and dynamic real-world environments (Parisi et al. 2019). As models are challenged with multiple tasks over their lifetime, a desired property for CL strategies is to maintain a high performance across all tasks. This paradigm is receiving increasing attention, which led to many works being proposed (Parisi et al. 2019; Baker et al. 2023; Faber et al. 2023; Wortsman et al. 2020; Zhou et al. 2023). There are three main types of CL strategies: rehearsal (also known as experience replay), regularization, and architectural.\n\nAmong architectural strategies, forget-free approaches are particularly relevant. They typically accommodate new tasks by assigning a subset of available model weights to each task. State-of-the-art approaches in this category include Packnet (Mallya and Lazebnik 2017), WSN (Kang et al. 2022), AdaQPacknet (Pietron et al. 2023), and DyTox (Douillard et al. 2022). However, their effectiveness strongly depends on the degree to which they can exploit model capacity to accommodate as many tasks as possible. One important pitfall of most existing pruning-based architectural forget-free methods is that they are limited in their ability to exploit model sparsity, since each layer is pruned with the same constant sparsity level. As a result, they do not properly tune the number of weights to be removed while preserving classification accuracy (Xu et al. 2021). A possible approach to address this issue is incorporating different sparsity levels for each layer (Pietron et al. 2023). Another general limitation of recent forget-free methods is that they are prone to a quick saturation of the available model capacity in that they cannot assign more than one value to each weight and are, therefore, limited by the available weights, as in (Douillard et al. 2022). In this paper, we propose TinySubNets (TSN), a novel forget-free method for continual image classification that addresses these limitations by incorporating adaptive pruning with sparsity-level identification and adaptive non-linear weight quantization (Figure 1). To effectively exploit model sparsity, TSN incorporates pruning with different sparsity levels for each layer. Moreover, to deal with the issue of quickly saturating model capacity, TSN performs an adaptive quantization stage, where each task quantizes its weights via a stored codebook obtained via clustering. Quantized weights can be shared between tasks if their KL-divergence is low, or they can be separated into components, i.e. subsets of the available 32 bits, if their KL-divergence is high, resulting in totally independent memory banks with reduced bit-widths. Weight sharing is carried out through a trainable mask that automatically selects relevant weights for the current task from previous tasks during gradient descent learning, minimizing the bias associated with learning previous tasks. Thanks to these capabilities, our strategy allows model capacity to be drastically reduced while preserving model accuracy.\n\nIn summary, the contributions of our paper are as follows:\n\n• We propose a forget-free model-agnostic continual strategy that adapts model architectures via adaptive pruning, quantization, and fine-tuning stages.\n\n• We devise an approach to seamlessly perform and combine quantization with weight sharing and replay memory for storing task samples, leading to an effective trade-off between model performance and capacity exploitation. • We showcase the positive impact of our proposed strategy on model performance through an extensive experimental analysis. Our results show that TSN can outperform stateof-the-art continual learning strategies with commonly adopted scenarios and datasets.\n\nIn the following sections, we summarize related works in CL strategies and pruning methods. Then, we describe our proposed TSN method. We follow with a description of our experimental setup, and we present the results extracted in our experiments. Finally, we provide a summary of the results obtained and outline relevant directions for future work.\n\n# Related Works\n\nContinual learning methods are commonly categorized as replay-based, regularization-based, and architectural-based (Parisi et al. 2019). Replay-based methods usually store some of the experiences form the past episodes and replay them while training with data from new tasks (Parisi et al. 2019). The most popular methods are GEM (David Lopez-Paz 2017), A-GEM (Chaudhry et al. 2019), and GDumb (Ameya Prabhu and Dokania 2020).\n\nOn the other hand, regularization methods usually put constraints on the loss function to prevent purging knowledge of already learned patterns. Such approach is part of methods such as Synaptic Intelligence (SI) (Zenke, Poole, and Ganguli 2017), LwF (Li and Hoiem 2017), and EWC (Kirkpatrick et al. 2016). The authors in (Liang and Li 2024) have recently proposed loss decoupling (LODE) for task-agnostic continual learning. LODE separates the two objectives for new tasks by decoupling the loss, allowing the model to assign different weights for different objectives, and achieving a better trade-off between stability and plasticity. The work in (McDonnell et al. 2024) proposes a continual learning approach for pre-trained models. Since forgetting occurs during parameter updating, the authors exploit training-free random projectors and class-prototype accumulation. They inject a frozen random projection layer with nonlinear activation between the pre-trained model’s feature representations and output head to capture interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based continual learning. Although regularization-based methods represent a viable way to realize continual learning, one major drawback is that introducing new tasks exacerbates the forgetting of previously learned tasks over time.\n\nArchitectural-based methods concentrate on the topology of the neural model trying to alter it or leverage the available capacity to prevent the model from forgetting. One of the most known methods is CWRStar (Lomonaco, Maltoni, and Pellegrini 2019) that freezes all weights except the last layer that is trained to accommodate new tasks. One of the key components of many architectural-based methods is pruning that allows to remove weights less important to already learned tasks and reuse them to accommodate new knowledge. For example, PackNet (Mallya and Lazebnik 2017) leverages pruning to divide the neural network into independent sub-networks assigned for specific tasks. This type of approach is known as forget-free, as having independent subnetworks ensure full protection from forgetting. The DyTox algorithm (Douillard et al. 2022) is a quite novel approach based on the Transformer architecture, which shows efficient usage of transfer learning in continual learning process. However, its main disadvantage is that the same architecture with the same model capacity is used for all CL scenarios. Ada-QPacknet (Pietron et al. 2023) adopts compression techniques like pruning and quantization as a viable approach to deal with model capacity. However, despite its efficiency on a number of common CL scenarios, this method does not support weight sharing between tasks. Moreover, its pruning step is based on lottery ticket search without leveraging gradient mask optimization and post-training fine-tuning. Another potential drawback is the lack of a mechanism to measure and exploit differences in task distributions.\n\nA number of recently proposed CL works can be regarded as hybrid, which often provide a mixture of regularization and architectural approaches, with memory-based features.\n\n![](images/233c8a111c40066cb6e4baeafdf5cde83835570458f8e22745ffed185978e5e3.jpg)  \nFigure 1: Model architecture evolution with TinySubNetworks (TSN). To incorporate a new task, the model-agnostic continual learning workflow involves pruning, non-linear quantization (different colors represent different clusters from the codebook), and fine-tuning (which involves additional weight pruning without re-training).\n\nThe work in (Jha et al. 2024) proposes an NP-based approach with task-specific modules arranged in a hierarchical latent variable model, in which regularizers on the learned latent distributions are adopted to alleviate forgetting. Uncertainty estimation is supported to handle the task head/module inference challenge. A different approach is taken in (Madaan et al. 2023) where, inspired by the knowledge distillation framework, the authors devise a setting where a weaker model takes the role of a teacher, while a new stronger architecture acts as a student. To deal with limited data availability, they propose Quick Deep Inversion (QDI) (Madaan et al. 2023) to enhance distillation by recovering prior task visual features that support knowledge transfer. However, this work does not discuss model capacity, making it difficult to gauge the trade-off between accuracy and memory requirements. A relevant method with memory-based features is Pro-KT (Li et al. 2024), a prompt-enhanced knowledge transfer model to openworld continual learning, i.e. learning on the fly with the goal of recognizing unknowns. The method is based on a prompt bank to encode and transfer task-generic and task-specific knowledge, and a task-aware open-set boundary to identify unknowns in the new tasks. Another example is Scaled Gradient Projection (SGP) (Saha and Roy 2023) , which combines orthogonal gradient projections with scaled gradient steps along the important gradient spaces for the past tasks. The authors scale gradients in these spaces based on their importance, and formulate an efficient way to compute it via singular value decomposition of input task representations.\n\nArchitectural and hybrid methods can be regarded among the most intriguing and sophisticated continual learning strategies. However, the major limitation of these methods appears when the capacity required to accommodate new tasks grows over time until exhaustion. Therefore, efficient capacity exploitation is of paramount importance and represents an open challenge in continual learning.\n\n# Method\n\nOur TinySubNets continual learning strategy is a forget-free approach. The algorithm incorporates pruning and quantization with weight sharing, which allows us to drastically reduce model capacity. TinySubNets can be run in two configurations: with and without weight sharing. Weights sharing between tasks is implemented by introducing a mask that automatically selects weights from previous tasks for the current task during gradient descent learning. This capability has the advantage to minimize the bias associated with learning previous tasks. In the case of tasks with a significantly different distribution, the algorithm introduces the possibility of dividing quantized weights into memory banks.\n\nThe pruned model can be represented as $F _ { \\Theta } ^ { p } = ( F _ { \\Theta } , M )$ , where $F _ { \\Theta }$ is defined as:\n\n$$\nF _ { \\Theta } ( X ) = f _ { \\theta _ { L } } ( f _ { \\theta _ { L - 1 } } . . . ( f _ { \\theta _ { 0 } } ( X ) ) ) .\n$$\n\n$F _ { \\Theta }$ is the initial full model with a set of convolutional and fully-connected layers $f _ { \\theta _ { i } }$ , which are defined in the specified order. The $\\Theta$ tensor is defined as:\n\n$$\n\\Theta = \\{ \\theta _ { 0 } , \\theta _ { 1 } , . . . , \\theta _ { L } \\} ,\n$$\n\nwhere $\\Theta$ represents weights of all layers of the model, while $\\theta _ { i }$ contains weights from connections between layer $i$ and $i + 1 . \\ w _ { i , k , l }$ denotes a single weight between $k$ -th neuron from layer $i$ and $l .$ -th neuron from layer $i + 1$ (Figure 2). 1\n\nA key aspect of the method is the adoption of masks, which denote assignments of weights to specific tasks. All masks are stored in a tensor $M$ :\n\n$$\n{ \\cal M } = \\{ M ^ { 0 } , M ^ { 1 } , . . . , M ^ { L } \\} .\n$$\n\n1 N11 山 ■Task 1 Wo,11 N11 1 Task 1 Wo,1,1 W1,1,1 Task2← W1,1,1 Task3←   \nNo,1 W0,2,1 山 W1,1,2000 N2,1 → No,1 Wo.2.1 W1,1.2 N2,1 → Wo,1,3 Wo,1, Wo,1 Wo.1,2 .W1,2,1 0 8 W1,2,1 a   \nN0，2 W0,2,2 [010 N1.2 101 W1,3,1 W1,2,2 N2,2 No,2 00 W2 N1.2 18 11 N2.2 Wo,2,3 _W1,3,2 dointwg KLs=threhald N13 01 N13 Layer 1 Layer 1 Layer 2 Layer 2 Layer 1 Layer 2 Layer 2   \ncodebook - task 1 codebook -task 2 codebook - task 1 codebook - task 2 codebook - task 3 codebook -task 1 codebook - task 3 0.457 0 0.2453 00 0.3555 0 0.4521 00 0.4576 00 0 0.2453 00 0.3555 00 0 0.4521   \n01 0.3456 -0.1898 01 0.1417 1 0.1023 010.3456 01 1 -0.1898 1 0.1417 01 1 0.1023   \n10 0.0145 10 -0.1145 100.0145 10 10 -0.114 10   \n11 0.3576 11 0.2967 110.3576 11 0.2967 11 ↑ Replay memory - task 1 Replay memory - task 2 Replay memory -task 1 Replay memory-task 3   \nout N1,1 sample sample2 .sampleN out N1.1 sample 1 sample 2 ... sample N out N1,1 sample 1 sample2 .. sampleNout $\\mathsf { N } _ { 1 , 1 }$ sample 1 sample 2 ... sample N   \nout N1,2 sample 1 sample 2 . sample N out N1.2 _sample 11 sample 2 ... sample N out N1,2 sample 1 sample 2 ... sample N out $\\Nu _ { 1 , 2 }$ sample 1 sample 2...sample N   \nout N1,3 sample 1 sample 2 sampleN out N1,3 sample 1 sample 2 .. sampleN out N1,3 sample 1 sample 2... sample N out N1.3 sample 1 sample 2 ...sample N -Kullback-Leibler -Kullback-Leibler (a) (b)\n\nA single mask $M ^ { i }$ corresponds to weights between layer $i$ and $i + 1$ and has the same size as $\\theta _ { i }$ . Each entry $M _ { k , l } ^ { \\dot { i } }$ in mask $M ^ { i }$ describes a set of tasks in which the weight between $k$ -th neuron from layer $i$ and $l$ -th neuron from layer $i + 1$ is leveraged. This single entry is defined as:\n\n$$\nM _ { k , l } ^ { i } = \\{ t _ { 1 } , t _ { 2 } , \\dots , t _ { n } \\} ,\n$$\n\nwhere $t _ { p }$ is a binary value indicating if weight is pruned or not by given task. The quantization generates bank, prefix, and key in the codebook to which a given weight is assigned for solving task $p$ . Figure 2 showcases a graphical representation of this process. While the bank defines a subset of the 32-bit bandwidth of the weights, the prefix defines the specific codebook to which a weight is assigned in one bank.\n\n<html><body><table><tr><td>Algorithm1: TinySubNetworks(TSN) main algorithm</td><td></td></tr><tr><td>1,2,...,T</td><td>Require: D = {D1, D2,...,Dr} - training data for tasks</td></tr><tr><td></td><td>Require:c-initial capacityper task</td></tr><tr><td></td><td>Require: Θ- model weights</td></tr><tr><td></td><td>Require: Mo = 0- initial binary mask</td></tr><tr><td>quantization</td><td>Require: k - frequency (num. of batches) to trigger adaptive</td></tr><tr><td></td><td>Require:ω-Kullback-Leibler threshold</td></tr><tr><td></td><td>Require: bs-batch size</td></tr><tr><td>1: Randomly initialized Θ and s</td><td>Require: sr - task replay memory size</td></tr><tr><td>2: for t = O'to T do</td><td></td></tr><tr><td>3:</td><td></td></tr><tr><td>4:</td><td>Rt ← sr from Dt to replay memory p↑-1</td></tr><tr><td>5:</td><td>forp=t-1to0do</td></tr><tr><td>6:</td><td>kl ← compute DK L(Rt,Rp)</td></tr><tr><td>7:</td><td>if kl <ω then</td></tr><tr><td>8:</td><td> set weight sharing of task t with task p</td></tr><tr><td>9:</td><td>break</td></tr><tr><td>10:</td><td>for b =0to Dl do</td></tr><tr><td>11:</td><td>if p > -1 then</td></tr><tr><td></td><td>Obtain mask Mt of the top-c% weights at each</td></tr><tr><td>12:</td><td>layer sharing with task p</td></tr><tr><td>13:</td><td>else</td></tr><tr><td>14:</td><td>Obtain new random mask Mt of the the separate weight memory bank</td></tr><tr><td>15:</td><td>if b % k == O then</td></tr><tr><td>16:</td><td>Φt,Θ,Kt ← adaptive quantization Θ ③ Mt { See Appendix for Algorithm }</td></tr><tr><td>17:</td><td>else</td></tr><tr><td>18:</td><td>Θ←Θ⊙Mt</td></tr><tr><td>19:</td><td>compute L(x,y)</td></tr><tr><td>20:</td><td>Θ←Θ-n·( (1-Mt-1)) x，y</td></tr><tr><td>21:</td><td>s←s-n·( SLy)</td></tr><tr><td>22:</td><td>0s Mt ←Mt-1 VMt</td></tr><tr><td>23:</td><td>Θ,Mt ← fine tuned pruning {Algorithm 2}</td></tr><tr><td>24:</td><td>M←MUMt</td></tr><tr><td></td><td>K←KUKt</td></tr><tr><td>25:</td><td></td></tr><tr><td></td><td>26:return O-weights,Mt -mask,K - tasks codebook</td></tr></table></body></html>\n\nA visual representation of learned masks is shown in Figure 3. Two types of weight sharing are supported: connection sharing with disjoint values (a), where separate masks with different values are learned for each task, and value-based weight sharing (b), where the same weight value is used for different tasks.\n\nThe sparsity level $\\Upsilon _ { i }$ describes how many weights out of all weights available in layer $i$ are not yet assigned to any task. It is defined as:\n\n$$\n\\Upsilon _ { i } = \\frac { \\vert M ^ { i } \\vert - \\sum _ { k , l } ^ { \\vert M ^ { i } \\vert } ( \\vert M _ { k , l } ^ { i } \\vert > 0 ) } { \\vert M ^ { i } \\vert }\n$$\n\nIt is also possible to define the overall weighted sparsity of the model:\n\n$$\n\\Upsilon = \\sum _ { i } ^ { L } \\left| \\theta _ { i } \\right| \\cdot \\Upsilon _ { i }\n$$\n\nAdditionally, masks for all tasks are encoded by Huffman algorithm to compress theirs size. The same methodology is used in (Kang et al. 2022), see Appendix2.\n\nThe TSN main workflow is described in Algorithm 1. The process starts with random initialization of values and score weights. In the following iterations, the model is trained with subsequent tasks. In the initial processing phase, the task is filled with replay memory i.e. a fixed number of input data samples for each class of a given task. Then, the divergence between the current task and all previous ones is calculated. We adopt the KL-divergence to measure the divergence between tasks, defined as:\n\n$$\nD _ { K L _ { t _ { i } , t _ { j } } } = D _ { K L ( D _ { t _ { i } } , D _ { t _ { j } } ) } .\n$$\n\nSpecifically, a low KL-divergence implies that quantized weights may be shared between tasks. If the divergence is lower than the chosen threshold, the task with the minimum divergence is selected as the candidate for weight sharing. On the contrary, a high KL-divergence implies that tasks are significantly different. If the divergence is greater than the given threshold for all previous tasks, the current task allocates its own weight subspace by reserving a memory bank with a reduced number of bits (Lines 12-16).\n\nThen, the training process begins. In the feed-forward process, weights are multiplied by the current mask. If a batch is a multiple of the $\\bar { \\boldsymbol { k } }$ parameter, then the adaptive quantization algorithm is also executed (for additional details see Appendix). Then, the loss is computed, and the weights are updated:\n\n$$\n\\mathcal { L } _ { ( x , y ) } = \\frac { 1 } { | B | } \\sum _ { b = 1 } ^ { | B | } \\left( \\mathcal { L } _ { c e } ( f _ { \\theta } ( x _ { b } ) , y _ { b } ) + \\alpha \\sum _ { i = 0 } ^ { L } \\mathcal { L } _ { m s e } ( \\overline { { f _ { \\theta _ { i } } } } , f _ { \\theta _ { i } } ) \\right) ,\n$$\n\nwhere $\\mathcal { L } _ { c e }$ is the conventional cross-entropy loss iteratively computed over batches $B$ , and $\\mathcal { L } _ { m s e }$ measures the differences in layer representations computed between the full model $f _ { \\theta _ { i } }$ and the compressed model $\\overline { { f _ { \\theta _ { i } } } }$ which features pruning and quantization stages. After the learning process for a given task is completed, the global consolidated mask is updated. Next, pruning optimization is performed on the validation set. In the last step, the global mask and codebook are updated.\n\n![](images/e1e8fa9769213c78857dd16fadcb42e5714451f06ed6dfce3cd99675c89826ec.jpg)  \nFigure 3: TinySubNetworks - mask with connection sharing with disjoint values (a) and value-based weight sharing (b). Each subfigure represents the weight matrix of the model: rows (input size of the layer) by columns (output size of the layer). White boxes represent pruned weights. Different colors represent weight allocations for multiple tasks assigned to each mask.\n\nIn Algorithm 2 the post-pruning without retraining process is presented. This process has a very limited computational cost (see execution times in Appendix), and its goal is to check the possibility to slightly increase the sparsity level if it does not impact accuracy, without triggering model retraining. The algorithm starts by setting the following values: weights multiplied by the input mask (mask achieved after training stage), the optimal mask (which is initially equal to the input mask), and the $\\gamma _ { o p t }$ variable, which constitutes the fitness function of the optimization process. Its value is obtained as the the sum of two components: the accuracy value multiplied by the $\\alpha$ coefficient, and the sparsity value multiplied by the $\\beta$ coefficient (in all experiments $\\alpha$ is set to 0.95 and $\\beta$ to 0.05).\n\nSubsequently, the main loop of the algorithm is executed. In the next search iterations, the model layer is selected as a candidate for increasing sparsity. Then the mask is incremented (the number of zeros is increased). In the next stage, the weights are modified according to the changed mask. Accuracy is calculated for the changed weights and a new sparsity value is set. Based on them, the gamma value is calculated. If $\\gamma$ is greater than $\\gamma _ { o p t }$ , the modified mask becomes the current mask of the given layer and the $\\gamma$ value becomes $\\gamma _ { o p t }$ . Otherwise, the layer mask remains unchanged.\n\nMoreover, in addition to the main TSN method described above, we also introduce TSN-wr (TSN without replay). In this case, weight sharing is based on pruning instead of replay buffer. While only not yet used weights are altered during learning a new task, the other weights are still used in the forward pass. When pruning weights after learning a new task, some of the weights already assigned to another task may still be assigned to a new task as well. The TSN-wr follows the same algorithm as TSN (Algorithm 1) except for lines 3-10, which are not included in TSN-wr execution. While, intuitively, this may lead to decreased performance in terms of accuracy, it should also allow to reduce the used memory due to lack of replay-buffer.\n\nAlgorithm 2: Fine tuned pruning for task   \nRequire: $I -$ fine tuning iterations   \nRequire: $\\Theta -$ weights of the model   \nRequire: $M _ { t }$ – current mask of the task   \nRequire: $\\Delta -$ quant of sparsity change   \nRequire: $A , \\Upsilon$ – initial accuracy and sparsity of the task   \nRequire: $\\alpha$ , $\\beta$ – scaling factors 1: $\\Theta \\gets$ initialize the task weights by $\\Theta \\odot M _ { t }$   \n2: $M _ { o p t } \\gets M _ { t }$   \n3: $\\gamma _ { o p t }  \\alpha \\cdot A + \\beta \\cdot \\Upsilon$ 4: for $i = 0$ to $I$ do   \n5: $l \\gets$ choose layer from 0 to $L$ 6: $M ^ { \\prime } \\gets$ increment mask of layer $l - M _ { l }$ by $\\Delta$   \n7: $\\Theta ^ { \\prime } = \\Theta \\odot M ^ { \\prime }$ 8: $A ^ { \\prime } $ accuracy of $\\Theta \\odot M ^ { \\prime }$ on task $t$   \n9: $\\Upsilon ^ { \\prime } $ compute sparsity for $M ^ { \\prime }$ (eq: 6)   \n10: γ α A′ + β Υ′   \n11: if γ > γopt then   \n12: Mopt ← M ′   \n13: γopt ← γ   \n14: else   \n15: $M _ { l } \\gets$ decrement mask $M _ { l }$ of layer $l$ by $\\Delta$   \n16: $\\Theta ^ { \\prime } \\gets \\Theta \\odot M _ { o p t }$   \n17: return $M _ { o p t } , \\bar { \\Theta } ^ { \\prime }$\n\n# Results and Discussion\n\nDatasets: Our main experiments were performed with three popular and commonly adopted CL scenarios: Permuted MNIST (p-MNIST) $( \\mathrm { C u n } 1 9 9 8 )$ , split CIFAR100 (sCIFAR100) (Krizhevsky 2009), and 5 datasets (Ebrahimi et al. 2020), a task-incremental scenario consisting of MNIST, SVHN, FashionMNIST, CIFAR10, not-MNIST, TinyImagenet (Le and Yang 2015), and Imagenet100 (Russakovsky et al. 2015). The p-MNIST scenario consists of 10 tasks with randomly permuted pixels of the original MNIST (10 tasks with 10 classes each). s-CIFAR100 is divided into 10 tasks with 10 classes each. The 5 datasets scenario is a sequence of 5 tasks with different datasets, each with 10 classes. The TinyImagenet scenario consists of 40 tasks with 5 randomly sampled classes each. Finally, the Imagenet100 scenario consists of 10 tasks with 10 randomly sampled classes each.\n\nExperimental Setup: The adopted models are a two-layer neural network with fully connected layers (p-MNIST), reduced AlexNet (s-CIFAR100) (Saha, Garg, and Roy 2020), Resnet-18 (5 datasets and Imagenet100), TinyNet (TinyImagenet) in accordance with model backbones used in the WSN paper (Kang et al. 2022). For weight initialization, we adopt the Xavier initializer. Experiments are executed on a workstation equipped with an NVIDIA A100 GPU. Our experiments involve 5 complete runs for each strategy. The training and testing execution times are reported in the Appendix3. The code of the method is available at the following public repository: https://github.com/lifelonglab/tinysubnets.\n\nMetrics: We adopt the most standard definition of lifelong Accuracy following the description in (Díaz-Rodríguez et al. 2018). Moreover, we also compute the capacity (total memory occupied by each task) inspired by the capacity computation in (Kang et al. 2022). The capacity can be regarded as the sum of three components, and defined as follows:\n\n$$\n\\begin{array} { r } { C A P _ { t } = \\displaystyle \\sum _ { i } ^ { L } ( 1 - \\Upsilon _ { i } ) \\cdot | \\theta _ { i } | \\cdot b + | L | \\cdot 2 ^ { b } \\cdot ( 3 2 + b ) + } \\\\ { \\displaystyle \\sum _ { i } ^ { L } ( 1 - \\Upsilon _ { i } ) \\cdot | M ^ { i } | } \\end{array}\n$$\n\nThe first one describes the number of the weights after the pruning multiplied by the sum of bit-width, prefix and bank identifier of the codebook $b$ . The second one describes the codebook size. The third one includes all masks’ capacity.\n\nHyperparameters: The hyperparameters used in our experiments were set up as follows:\n\n• Adaptive learning rate – for $\\boldsymbol { \\mathrm { \\tt ~ p } }$ -MNIST it starts from $3 e \\mathrm { ~ - ~ } 1$ and ends in $1 e - 4$ , for CIFAR100 and 5 datasets it starts from $1 e - 2$ and ends with $1 e - 4$ , for the rest of the scenarios is between $1 e - 3$ and $1 e - 5$   \n• Number of epochs per task – 200 epochs per task for each scenario   \n• Batch size – CIFAR100 - 2048, Imagenet100 - 32, 5 datasets, p-MNIST and TinyImagenet - 256   \n• Fine tuning parameters - 50 iterations for each scenario, $\\alpha \\mathrm { ~ - ~ } 0 . 9 5$ , $\\beta - 0 . 9 5$ ,   \n• Kullback-Leibler threshold - set empirically to have max two memory banks   \n• Initial capacity per task - 0.55 for CIFAR100, 0.5 for the rest of the scenarios   \n• Frequency (num. of batches) to trigger adaptive quantization - three times per epoch for each scenario   \n• Task replay memory size - 50 samples per task (only 15 samples per task for TinyImagenet),   \n• Quantity of sparsity change - 0.01 for each continual learning benchmark\n\nThe SGD optimizer was used for p-MNIST dataset. For the other scenarios the ADAM optimizer was adopted.\n\n# Comparative Studies\n\nTable 2 presents the results of our experiments in terms of accuracy for two TSN variants: TSN without replay memory (TSN-wr) and TSN mixed (TSN with replay and fine tuning). We also present the results of the most popular standard CL methods and upper-bounds (Naive, CWRStar, SI, Replay, Cumulative), as well as three forget-free architectural methods: Packnet, WSN, and Ada-QPacknet.\n\nMoreover, we also present capacity comparison for pruning-based methods in Table 1 where results for TSN without replay and fine tuning (TSN-wr-wpp) version are added. Additional results such as backward, forward transfer, sparsity levels, bit-widths and masks compression ratios achieved by Huffman algorithm are reported in the external Appendix.\n\nTable 1: Capacity comparison for pruning-based methods (\\* - in case of $\\boldsymbol { \\mathrm { \\tt ~ p \\ } }$ -MNIST two memory banks without replay memory).   \n\n<html><body><table><tr><td></td><td>p-MNIST|s-CIFAR100|5datasets</td><td></td><td></td><td>Tiny Imagenet</td></tr><tr><td>Packnet</td><td>96.38%</td><td>81.0%</td><td>82.86%</td><td>188.67%</td></tr><tr><td>WSN</td><td>77.73%</td><td>99.13%</td><td>86.10%</td><td>48.65%</td></tr><tr><td>Ada-QPacknet</td><td>81.25%</td><td>78.6%</td><td>33.7%</td><td>112.5%</td></tr><tr><td>TSN-wr</td><td>22.65%</td><td>17.62%</td><td>24.68 %</td><td>32.15%</td></tr><tr><td>TSN-wr-wpp</td><td>23.41%</td><td>18.75%</td><td>30.62%</td><td>36.33%</td></tr><tr><td>TSN</td><td>37.5%*</td><td>41.92%</td><td>40.08%</td><td>93.6%</td></tr></table></body></html>\n\nAccuracy-Capacity Tradeoff: As we can observe, TSNwr allows us to significantly reduce capacity in comparison to other pruning methods $( \\dot { 2 } 2 . 6 5 \\%$ vs ${ \\bar { 7 } } 7 . 7 3 { \\bar { \\% } }$ for p-MNIST, $1 7 . 6 2 \\%$ vs $7 8 . 6 \\%$ for $\\mathrm { \\bf { s } }$ -CIFAR100, $2 4 . 6 8 \\%$ vs $3 3 . 7 \\%$ for 5 datasets, and $3 2 . 1 5 \\%$ vs $4 8 . 6 5 \\%$ for TinyImagenet).\n\nAt the same time, TSN-wr presents an accuracy that is just slightly worse than the best-performing method $( 9 6 . 6 3 \\%$ vs $9 7 . 1 4 \\%$ for p-MNIST, $7 5 . 2 1 \\%$ vs $7 7 . 2 7 \\%$ for s-CIFAR100, $9 1 . 8 \\%$ vs $9 4 . 1 \\%$ for 5 datasets, and $7 9 . 8 1 \\%$ vs $8 0 . 1 0 \\%$ for TinyImagenet). Results obtained for the TSN method were obtained setting the Kullbach-Leibler divergence threshold empirically to have two memory banks at most, in order to balance memory consumption and accuracy. It is worth mentioning that Algorithm 2 resulted, on average, in an improvement from $1 \\%$ (p-MNIST) to $6 \\%$ (5 datasets) in capacity utilization (TSN-wr-wpp), with a negligible drop in accuracy (less than $1 \\%$ , see Appendix). Overall, it is possible to observe that TSN-wr provides the best trade-off between accuracy and capacity. On the other hand, TSN achieves SOTA results on p-MNIST $( 9 7 . 1 4 \\%$ , and TinyImageNet $( 8 0 . 1 0 \\% )$ ), and s-CIFAR100 $( 7 7 . 2 7 \\% )$ ), at the cost of a higher capacity when compared to TSN-wr $( 9 3 . 6 \\%$ vs 32.15 on TinyImagenet and $\\mathrm { { \\bar { 3 } } 7 . 5 \\% }$ vs $2 2 . 6 5 \\%$ on p-MNIST and $4 1 . 9 \\dot { 2 } \\%$ vs $1 7 . 6 2 \\%$ on s-CIFAR100).\n\nImpact of Weight Sharing: To assess the impact of weight sharing, it is relevant to compare our approach which involves weight sharing, with architectural strategies without weight sharing, such as Ada-QPacknet. Our results show that weight sharing is beneficial in scenarios with low task heterogeneity, where task similarity can be profitably exploited (e.g. p-MNIST; s-CIFAR100). Quite interestingly, the situation is different in scenarios where tasks are more heterogeneous, such as 5 datasets. In this scenario, strategies without weight sharing achieve the best accuracy $( 9 4 . 1 \\bar { \\% } )$ , a $2 . 3 \\%$ improvement over the replay variant of TSN. With TinyImagenet, we observe that TSN-wr outperforms Ada-QPacknet $( 7 9 . 8 1 \\%$ vs. $7 1 . 9 \\%$ ). Despite the large number of classes in this dataset, we argue that TinyImagenet has a limited class representation, i.e. number of images per class, which makes the scenario less representative in terms of complexity than 5 datasets.\n\nArchitectural Strategies on ImageNet100: Results in Table 3 present several SOTA architectural and replay methods on the ImageNet100 scenario. DER has the highest parameter count at 112.27 million, significantly larger than the other methods. Despite its high complexity, DER achieved an accuracy of $7 5 . 3 6 \\%$ , which is slightly lower than DyTox. Ada-QPacknet has 11.5 million parameters, and achieved an accuracy of $7 2 . 2 6 \\%$ , which is the lowest among the considered methods. Overall, we can observe that TSN-wr achieves the best results in terms of both parameters number and accuracy. With only 2.47 million parameters, TSN achieved a notable accuracy of $7 7 . 1 6 \\%$ , which is particularly impressive, and suggests that TSN is highly efficient and effective. This result suggests that utilizing more capacity (as in DER or ADA-QPacknet) does not necessarily translate to a higher accuracy in complex scenarios.\n\nAblation Studies: We perform two ablation studies to investigate the memory capacity requirements for weights, codebooks, and masks across scenarios in weight-sharing mode. Our study suggests that while weights and masks are the primary consumers of memory, the use of codebooks introduces minimal additional memory requirements. The second ablation study evaluates the accuracy of models using different bit widths.\n\nOur study shows that while reducing the bit width can help in compression, it often comes at the cost of accuracy. The impact varies by dataset, with larger and more complex datasets like TinyImagenet experiencing a more pronounced drop in accuracy at lower bit widths. For CIFAR100, a 4-bit width seems optimal, balancing efficiency and performance. This suggests that the choice of bit width should be carefully considered based on the specific dataset and accuracy requirements. Quantitative detailed experiments for the two ablation studies are reported in the Appendix. In all experiments variance in accuracy does not exceed 0.04. All experiments were run five times.\n\nTable 2: Comparative results in terms of average accuracy for all CL strategies with different CL scenarios (4-bit quantization): Permuted MNIST (p-MNIST), split-CIFAR100 (sCIFAR100), and 5 datasets (5 trials, std $< 0 . 0 0 2$ ).   \n\n<html><body><table><tr><td colspan=\"4\"></td><td>p-MNIST|s-CIFAR100|5datasets|TinyImagenet</td></tr><tr><td>Naive</td><td>60.12</td><td>17.32</td><td>33.08</td><td>20.27</td></tr><tr><td>CWRStar</td><td>31.31</td><td>20.84</td><td>36,16</td><td>24.00</td></tr><tr><td>SI</td><td>57.32</td><td>19.54</td><td>29.42</td><td>20.51</td></tr><tr><td>Replay</td><td>62.22</td><td>19.60</td><td>55.24</td><td>23.14</td></tr><tr><td>Cumulative</td><td>96.45</td><td>36.52</td><td>84.44</td><td>27.53</td></tr><tr><td>Packnet</td><td>96.31</td><td>72,57</td><td>92.59</td><td>55.46</td></tr><tr><td>WSN</td><td>96.41</td><td>76.38</td><td>93.41</td><td>71.96</td></tr><tr><td>AdaQPacknet</td><td>97.1</td><td>74.1</td><td>94.1</td><td>71.9</td></tr><tr><td>TSN-wr</td><td>96.63</td><td>75.21</td><td>91.80</td><td>79.81</td></tr><tr><td>TSN</td><td>97.14</td><td>77.27</td><td>93.76</td><td>80.10</td></tr></table></body></html>\n\nTable 3: Comparison with SOTA architectural and replay methods on Imagenet100 (10/10) (5 trials, std $< 0 . 0 0 2 \\$ ).   \n\n<html><body><table><tr><td>Methods</td><td>Parameters</td><td>Accuracy</td></tr><tr><td>DyTox</td><td>10.73M</td><td>75.54</td></tr><tr><td>DER</td><td>112.27M</td><td>75.36</td></tr><tr><td>Ada-QPacknet</td><td>11.5M</td><td>72.26</td></tr><tr><td>TSN-wr</td><td>2.47M</td><td>77.16</td></tr><tr><td>TSN</td><td>4.94M</td><td>77.86</td></tr></table></body></html>\n\nEnergy Efficiency: An important aspect of our method is its limited energy usage. This aspect is a key factor for model deployment in low-resourced devices such as mobile phones and IoT, and it has been recognized as crucial for Green AI (Schwartz et al. 2020), which promotes environmentally sustainable models with low carbon footprint. To demonstrate this capability, we compute the theoretical complexity of our proposed method in terms of floating point operations per second (FLOPS) with different quantization levels.\n\nTo this end, FLOPS calculations for GPU is carried out via arithmetic multiplication based on the size and number of convolutional and fully connected layers in model architectures. Results in Figure 4 show that 16-bit and 8-bit quantized models provide a significant reduction in terms of FLOPS. This is a remarkable result considering the negligible drop in accuracy presented by compressed models. 4.\n\n![](images/8dfbdbaf48e7872202fdc619a573d964e2b8079638fe279e2bcf98c304c59560.jpg)  \nFigure 4: FLOPS with different quantization levels in different scenarios. P denotes the method with pruning. Baseline FP denotes the full precision of weights (no compression).\n\n# Conclusions\n\nThis paper introduces the TinySubNets (TSN) method, a forget-free continual learning strategy that provides an effective trade-off between model performance and capacity exploitation. TSN ensures this by adapting model architecture through pruning, adaptive quantization, and fine-tuning. Moreover, TSN is a model-agnostic approach and can be adapted to support any neural network architecture. Our extensive experimental evaluation includes the most popular continual learning benchmarks and models, as well as multiple other forget-free and architectural methods. We observe that TSN is able to significantly reduce the used model capacity while keeping a performance similar (or even better) to its competitors. This reduction may be significant in extending the longevity of the models, allowing them to learn more tasks. In future work, we will attempt to showcase the benefits of quantization performance while using specialized hardware accelerators. Moreover, we will also investigate the impact of decreasing the network size on the performance of architectural continual learning methods.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   持续学习（Continual Learning, CL）中，现有基于剪枝的架构方法存在模型稀疏性利用不足和容量快速饱和的问题，限制了可学习任务的数量。\\n> *   该问题在动态现实环境中的应用尤为重要，如物联网设备和移动端部署，需要高效利用模型容量以支持长期学习。\\n\\n> **方法概述 (Method Overview)**\\n> *   本文提出TinySubNets（TSN），一种新型的架构持续学习策略，通过结合不同稀疏级别的剪枝、自适应量化和权重共享，高效利用模型容量并减少计算资源消耗。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 提出了一种模型无关的持续学习策略，结合自适应剪枝、量化和微调阶段，显著提升了模型容量利用率。\\n> *   **创新贡献点2：** 设计了量化与权重共享的无缝结合方法，实现了模型性能和容量利用的有效平衡。\\n> *   **创新贡献点3：** 在多个基准数据集上，TSN在准确率上优于现有最先进方法（如p-MNIST上97.14% vs 97.1%），同时显著降低了容量需求（如s-CIFAR100上17.62% vs 78.6%）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   TSN通过剪枝识别保留模型性能的权重子集，释放不相关权重以供未来任务使用。自适应量化允许单个权重被分割为多个部分，分配给不同任务。权重共享通过可训练掩码自动选择当前任务的相关权重，最小化学习偏差。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有方法如Packnet和Ada-QPacknet在剪枝时使用固定稀疏级别，无法有效利用模型稀疏性，且不支持权重共享。\\n> *   **本文的改进：** TSN引入分层稀疏级别剪枝和自适应非线性量化，支持任务间权重共享，显著提升了容量利用率和知识迁移效率。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  初始化模型权重和掩码。\\n> 2.  对每个新任务，计算与先前任务的KL散度，决定是否共享权重。\\n> 3.  执行自适应剪枝，生成任务特定掩码。\\n> 4.  触发自适应量化，生成码本并分配权重。\\n> 5.  微调阶段进一步优化掩码和权重。\\n> 6.  更新全局掩码和码本。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   Packnet、WSN、Ada-QPacknet、DyTox、DER等。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在准确率上：** 本文方法在p-MNIST上达到了97.14%，显著优于基线模型Packnet（96.31%）和Ada-QPacknet（97.1%）。与表现最佳的基线相比，提升了0.04个百分点。\\n> *   **在容量利用率上：** 本文方法在s-CIFAR100上的容量需求为17.62%，远低于基线模型Ada-QPacknet（78.6%），同时保持了较高的准确率（75.21% vs 74.1%）。\\n> *   **在参数数量上：** 本文方法在Imagenet100上仅使用2.47M参数，达到了77.16%的准确率，显著优于DER（112.27M参数，75.36%准确率）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   持续学习 (Continual Learning, CL)\\n*   模型剪枝 (Model Pruning, N/A)\\n*   自适应量化 (Adaptive Quantization, N/A)\\n*   权重共享 (Weight Sharing, N/A)\\n*   容量利用率 (Capacity Exploitation, N/A)\\n*   知识迁移 (Knowledge Transfer, N/A)\\n*   绿色AI (Green AI, N/A)\"\n}\n```"
}