{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.09033",
    "link": "https://arxiv.org/abs/2408.09033",
    "pdf_link": "https://arxiv.org/pdf/2408.09033.pdf",
    "title": "Error Bounds For Gaussian Process Regression Under Bounded Support Noise With Applications To Safety Certification",
    "authors": [
        "Robert Reed",
        "Luca Laurenti",
        "Morteza Lahijanian"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-08-16",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 4,
    "influential_citation_count": 0,
    "institutions": [
        "University of Colorado Boulder",
        "Delft University of Technology"
    ],
    "paper_content": "# Error Bounds For Gaussian Process Regression Under Bounded Support Noise With Applications To Safety Certification\n\nRobert Reed1, Luca Laurenti2, Morteza Lahijanian1\n\n1Deptartment of Aerospace Engineering Sciences, University of Colorado Boulder, USA 2Delft Center for Systems and Control, Delft University of Technology, The Netherlands Robert.Reed- $1 @$ colorado.edu, L.Laurenti@tudelft.nl, Morteza.Lahijanian@colorado.edu\n\n# Abstract\n\nGaussian Process Regression (GPR) is a powerful and elegant method for learning complex functions from noisy data with a wide range of applications, including in safety-critical domains. Such applications have two key features: (i) they require rigorous error quantification, and (ii) the noise is often bounded and non-Gaussian due to, e.g., physical constraints. While error bounds for applying GPR in the presence of nonGaussian noise exist, they tend to be overly restrictive and conservative in practice. In this paper, we provide novel error bounds for GPR under bounded support noise. Specifically, by relying on concentration inequalities and assuming that the latent function has low complexity in the reproducing kernel Hilbert space (RKHS) corresponding to the GP kernel, we derive both probabilistic and deterministic bounds on the error of the GPR. We show that these errors are substantially tighter than existing state-of-the-art bounds and are particularly wellsuited for GPR with neural network kernels, i.e., Deep Kernel Learning (DKL). Furthermore, motivated by applications in safety-critical domains, we illustrate how these bounds can be combined with stochastic barrier functions to successfully quantify the safety probability of an unknown dynamical system from finite data. We validate the efficacy of our approach through several benchmarks and comparisons against existing bounds. The results show that our bounds are consistently smaller, and that DKLs can produce error bounds tighter than sample noise, significantly improving the safety probability of control systems.\n\n# 1 Introduction\n\nGaussian Process Regression (GPR) is a powerful and elegant method for learning complex functions from noisy data, renowned for its rigorous uncertainty quantification (Rasmussen, Williams et al. 2006). This makes GPR particularly valuable in the control and analysis of safety-critical systems (Berkenkamp et al. 2017; Lederer and Hirche 2019; Lederer, Umlauft, and Hirche 2019; Jagtap, Soudjani, and Zamani 2020; Jackson et al. 2021b,a; Griffioen, Devonport, and Arcak 2023; Wajid, Awan, and Zamani 2022). However, in such systems, the underlying assumption of Gaussian measurement noise often does not hold. Measurements are typically filtered to reject outliers, and physical systems cannot traverse infinite distances in a single time step. Consequently, bounded support noise distributions provide a more accurate representation for many cyber-physical systems. But, without the Gaussian assumption, the mean and variance predictions of GPs cannot be directly used to represent confidence in the underlying system. To address this, recent works (Hashimoto et al. 2022; Srinivas et al. 2012; Chowdhury and Gopalan 2017; Jackson et al. 2021a) have diverged from a fully Bayesian approach and developed GPR error bounds under mild assumptions on the sample noise distribution, specifically sub-Gaussian and bounded noise (see Figure 1). While these bounds are useful, they tend to be overly restrictive and conservative, relying on parameters that must be over-approximated. This work aims to overcome these limitations by providing novel, tighter error bounds for GPR under bounded support noise, enhancing their applicability in safety-critical domains.\n\nIn this paper, we present novel error bounds for GPR under bounded support noise. Our key insight is that two factors contribute to the regression error: the error due to GP mean prediction at a point without noise, and the error due to noise perturbing the noise-free prediction by a factor proportional to the noise’s support. Then, by relying on concentration inequalities and assuming the latent function has low complexity in the reproducing kernel Hilbert space (RKHS) corresponding to the GP kernel, we derive both probabilistic (i.e., with some confidence) and deterministic bounds on these error terms. Specifically, we use convex optimization and Hoeffding’s Inequality to obtain accurate bounds. We demonstrate that these errors are substantially tighter than existing bounds, as depicted in Figure 1. Furthermore, our bounds are particularly well-suited for GPs with neural network kernels, such as Deep Kernel Learning (DKL). We also illustrate how these bounds can be combined with stochastic barrier functions (Kushner 1967; Santoyo, Dutreix, and Coogan 2021), which are a generalization of Lyapunov functions, to quantify the safety probability of an unknown control system from finite data. We validate our approach through several benchmarks and comparisons, showing that our bounds are consistently smaller than the state-of-the-art, resulting in more accurate safety certification of control systems.\n\nThe key contributions of this work are three-fold: (i) derivation of novel probabilistic and deterministic error bounds for GP regression with bounded support noise, (ii) demonstration of the application of these bounds with stochastic barrier functions to effectively quantify the safety probability of unknown control systems by solely using data, and (iii) validation of the approach through extensive benchmarks and comparisons, showing consistently tighter error bounds than state-of-the-art methods and enhanced accuracy in determining safety probabilities for barrier certificates.\n\n![](images/7b54e0e673434a6808a070a0c3f059dd8d9eaa8a63f8b6e3e4eb375c27105b1c.jpg)  \nFigure 1: Predictive mean and error bounds when learning from 20 samples of $y = x \\sin ( x ) + v$ with $| v | < 0 . 5$ . In (a) we plot the comparison of deterministic error bounds and mean predictions, and in (b) we show the probabilistic bounds that hold with $9 5 \\%$ probability. We set $\\sigma _ { n } = 0 . 1$ for our predictions, $\\sigma _ { n } = 0 . 5$ for Hashimoto et al. as per Lemma 2, and $\\sigma _ { n } ^ { 2 } = 1 + 2 / 2 0$ for Chowdhury et al. as per Lemma 1. Note bounds for Abbasi-Yadkori (2013) are nearly identical to Chowdhury et al. in this example, see Appendix of (Reed, Laurenti, and Lahijanian 2024).\n\nRelated Works Several studies consider relaxations of the Gaussian assumption in the GP regression setting. In particular, Snelson, Ghahramani, and Rasmussen propose an approach that automatically generates a monotonic, non-linear warping function to map the observations into a latent space where it can be best modelled as a GP. However there is no guarantee that the warped data follows a GP. Also, the method requires an inverse of the warping function, but it is generally only available in an approximate form. Hence, the model cannot be used to generate formal guarantees. A closer work to ours is (Jensen, Nielsen, and Larsen 2013), which derives posteriors for GP regression when the observation space has bounded support, and hence the noise has bounded support. However, similar to (Snelson, Ghahramani, and Rasmussen 2003), the presented derivations provide approximate posteriors for the GP models, which limits their viability in safety-critical settings.\n\nWorks by Srinivas et al.; Abbasi-Yadkori; Chowdhury and Gopalan; Hashimoto et al. provide formal regression error bounds for GP models under non-Gaussian noise. Specifically, work (Srinivas et al. 2012) first develops a framework for GP regression under the assumption that noise is $R$ -sub-Gaussian and then formally quantifies probabilistic bounds on the GP prediction errors. Using a similar framework, (Chowdhury and Gopalan 2017) derives tighter probabilistic bounds. Work (Abbasi-Yadkori 2013) derives similar bounds for Kernel Ridge Regression, which uses the same mean prediction as a GPR without a posterior variance. While applicable, the setting in each of these works is more general than ours, which results in larger error terms when assessed with bounded support noise. Work (Hashimoto et al. 2022) specifically focuses on bounded noise and derives a deterministic error bound. This bound is generally tighter than the prior probabilistic bounds; nevertheless, it becomes loose as the size of the support increases. Since both error bounds in (Chowdhury and Gopalan 2017; Hashimoto et al. 2022) are directly applicable to our scenario, we show their derivation in Section 2 and compare our results against them. In the experiments, we also compare against (Srinivas et al. 2012; Abbasi-Yadkori 2013).\n\nFinally, recent work (Maddalena, Scharnhorst, and Jones 2021) derives a tighter deterministic error bound when compared to the results of (Hashimoto et al. 2022) by using a Kernel Ridge Regression approach. However, it requires the kernel to be strictly positive definite. While the squared exponential kernel, which is a popular choice in the controls application, can satisfy this requirement, the kernel matrix can be ill-conditioned resulting in inaccurate inversions. This restriction also limits the use of more expressive kernels that are positive semi-definite which may innately improve the error bounds, such as deep kernels. We show that our bounds are well-suited for deep kernels.\n\n# 2 Setup and Background\n\nWe consider a stochastic system of the following form:\n\n$$\n\\begin{array} { r } { { \\bf y } = f ( { \\bf x } ) + { \\bf v } , } \\end{array}\n$$\n\nwhere $\\mathbf { x } \\in \\mathbb { R } ^ { d }$ is the input, $\\mathbf { y } \\in \\mathbb { R }$ is the output, and $\\mathbf { v } \\in V \\subset$ $\\mathbb { R }$ is an additive zero-mean1 random variable (noise) with bounded support $\\sigma _ { v } \\in \\mathbb { R } _ { \\geq 0 }$ , i.e., $V = \\{ v \\in \\mathbb { R } \\mid | v | \\leq \\sigma _ { v } \\}$ , and unknown distribution. Function $f :  { \\mathbb { R } ^ { d } } \\to  { \\mathbb { R } }$ is unknown. The main goal of the paper is to regress $f$ from a dataset of input-output pairs $D \\bar { = } \\bar { \\{ ( x _ { i } , y _ { i } ) \\} } _ { i = 1 } ^ { \\bar { m } }$ and derive bounds on the error between the regressed model predictions and the true function $f$ . However, taking $f$ as entirely unknown can lead to an ill-posed problem; hence, we impose a standard smoothness (well-behaved) assumption (Srinivas et al. 2012; Jackson et al. 2021b) on $f$ .\n\nAssumption 1 (RKHS Continuity). For a compact set $X \\subset$ $\\mathbb { R } ^ { d }$ , let $\\mathbf { \\bar { \\Sigma } } _ { \\kappa : \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } \\to \\mathbb { R } _ { > 0 } }$ be a given continuously differentiable kernel and $\\mathcal { H } _ { \\kappa } ( \\bar { X } )$ the reproducing kernel Hilbert space $( R K H S )$ corresponding to $\\kappa$ over $X$ with induced norm $\\| \\cdot \\| _ { \\kappa }$ . Let $f ( \\cdot ) \\in { \\mathcal { H } } _ { \\kappa } ( X )$ . Moreover, there exists a constant $B \\geq 0 \\ s . t$ . $\\| f ( \\cdot ) \\| _ { \\kappa } \\leq B$ .\n\nAssumption 1 implies that $\\begin{array} { r } { f ( x ) = \\sum _ { n = 1 } ^ { \\infty } \\alpha _ { n } \\kappa ( x , x _ { n } ) } \\end{array}$ for representer points $\\alpha _ { n } \\in \\mathbb { R }$ and $x _ { n } \\in \\mathbb { R } ^ { d }$ . Note that for most kernels $\\kappa$ used in practice, such as the squared exponential kernel, $\\mathcal { H } _ { \\kappa }$ is dense in the space of continuous functions (Steinwart 2001).\n\nWe stress that, in this paper, we do not assume that $f$ is a sample from a Gaussian process prior. Moreover, since the noise is non-Gaussian, the likelihood model is also not Gaussian. Nevertheless, similar to the agnostic setting described in (Srinivas et al. 2012; Chowdhury and Gopalan\n\n2017; Hashimoto et al. 2022), we would still like to use the GP regression framework to regress $f$ with misspecified noise and prior models. In the remainder of this section, we provide a brief background on GP regression and state the existing error bounds on the learning error. In Section 3, we derive new bounds and discuss why they are tighter than the existing ones. We also illustrate the practical utility of these bounds in a control application in Section 4. Finally, we provide empirical evaluations of the new errors bounds in Section 5.\n\n# Existing Error Bounds Using GPs and RKHS\n\nLet $D = \\{ ( x _ { i } , y _ { i } ) \\} _ { i = 1 } ^ { m }$ be a dataset consisting of $m$ inputoutput observations pairs from System (1). A popular method to predict the output of $f$ at a new input $x ^ { * }$ with confidence on the predicted value is Gaussian Process regression (GPR). A Gaussian Process (GP) is a collection of random variables, such that any finite collection of those random variables is jointly Gaussian (Rasmussen, Williams et al. 2006). GPR starts by placing a Gaussian prior over $f$ through use of a kernel $\\dot { \\kappa } : \\mathbb { R } ^ { d } \\times \\mathbb { R } ^ { d } \\to \\mathbb { R } _ { \\geq 0 }$ . Then, under the assumption that $\\mathbf { v }$ is a zero-mean Gaussian independent random variable with variance $\\sigma _ { n } ^ { 2 }$ for any new input $x ^ { * }$ , the predictive mean $\\mu _ { D } ( x ^ { * } )$ and variance $\\sigma _ { D } ^ { 2 } ( x ^ { * } )$ can be derived as:\n\n$$\n\\begin{array} { r l } & { \\mu _ { D } ( x ^ { * } ) = K _ { x ^ { * } , \\mathcal { X } } ( K _ { \\mathcal { X } , \\mathcal { X } } + \\sigma _ { n } ^ { 2 } I ) ^ { - 1 } Y , } \\\\ & { \\sigma _ { D } ^ { 2 } ( x ^ { * } ) = K _ { x ^ { * } , x ^ { * } } - K _ { x ^ { * } , \\mathcal { X } } ( K _ { \\mathcal { X } , \\mathcal { X } } + \\sigma _ { n } ^ { 2 } I ) ^ { - 1 } K _ { \\mathcal { X } , x ^ { * } } , } \\end{array}\n$$\n\nwhere $\\mathcal { X } = [ x _ { 1 } , \\ldots , x _ { m } ] ^ { T }$ and $\\boldsymbol { Y } = [ y _ { 1 } , \\dots , y _ { m } ] ^ { T }$ are respectively the vectors of input and output data, $K _ { \\mathcal { X } , \\mathcal { X } }$ is a matrix whose $i$ -th row and $j$ -th column is $\\kappa ( \\boldsymbol { x } _ { i } , \\boldsymbol { x } _ { j } )$ , and $K _ { x , x } = K _ { \\chi , x } ^ { T }$ are vectors whose $i$ -th entry is $\\kappa ( \\boldsymbol { x } , \\boldsymbol { x } _ { i } )$ .\n\nAs already mentioned, in our setting in System (1), we do not assume $f$ is a sample from a GP and the noise $\\mathbf { v }$ is strictly bounded (hence, non-Gaussian). Fortunately, recent works show that, even under such settings, as long as Assumption 1 holds, the GP analytical posterior prediction can still be used outside of the Bayesian framework, i.e., even with misspecified noise and priors. Specifically, Work (Chowdhury and Gopalan 2017) shows that when the measurement noise is conditionally $R$ -sub-Gaussian, which includes bounded support distributions, probabilistic bounds on the error between the prediction $\\mu _ { D }$ and $f$ can be derived as follows.\n\nLemma 1 ((Chowdhury and Gopalan 2017, Theorem 2)). Let $X \\subset \\mathbb { R } ^ { d }$ be a compact set, $B > 0$ be the bound on $\\| f \\| _ { \\kappa } \\leq B$ , and $\\Gamma \\in \\mathbb { R } _ { > 0 }$ be the maximum information gain of $\\kappa$ . If noise v has a $R$ -sub-Gaussian distribution and $\\mu _ { D }$ and $\\sigma _ { D }$ are obtained via Equations (2)-(3) on a dataset $D$ of size m with $\\sigma _ { n } ^ { 2 } = 1 + 2 \\bar { / } m$ , then it holds that, for every $\\delta \\in ( 0 , 1 ]$ ,\n\n$$\n\\begin{array} { r } { \\mathbb { P } \\Big ( \\forall x \\in X , | \\mu _ { D } ( x ) - f ( x ) | \\leq \\beta ( \\delta ) \\sigma _ { D } ( x ) \\Big ) \\geq 1 - \\delta , } \\end{array}\n$$\n\nwhere $\\beta ( \\delta ) = B + R \\sqrt { 2 ( \\Gamma + 1 + \\log { ( 1 / \\delta ) } ) } .$ .\n\nLemma 1 provides probabilistic bounds on the GP regression error $| \\bar { \\mu _ { D } } ( x ) - f ( \\bar { x } ) |$ by relying on the kernel parameters such as RKHS norm bound $B$ and information gain $\\Gamma$ , which are often difficult to obtain accurately. They can however be (conservatively) bounded using techniques introduced in (Srinivas et al. 2012; Jackson et al. 2021a; Hashimoto et al. 2022). Another important observation in Lemma 1 is that, iwshsent tno ,a $R$ -iscuhba-dGdasucssoinasnerdvisatriisbmuttio nt,hevabroiauncdes $\\sigma _ { n } ^ { 2 }$ $1 + 2 / m$ substantially increasing the variance of GP predictions. Note that to obtain estimates with confidence 1, i.e., $\\delta = 0$ , Lemma 1 requires infinitely-many samples. Alternative probabilistic bounds in this setting are considered by Srinivas et al., which makes use of similar parameters, and by Abbasi-Yadkori which leaves $\\sigma _ { n }$ as a decision variable (see the extended version Appendix (Reed, Laurenti, and Lahijanian 2024)). As shown in our experiments, they are also conservative.\n\nIn the case that noise $\\mathbf { v }$ has bounded support (i.e., $| \\mathbf { v } | \\leq$ $\\sigma _ { v , \\ }$ ), deterministic bounds on the prediction error are provided by Hashimoto et al..\n\nLemma 2 ((Hashimoto et al. 2022, Lemma 2)). Let $X , \\kappa ,$ , and $B$ be as in Lemma $\\jmath$ . If noise $\\mathbf { v } \\in V$ has a finite support $\\sigma _ { v }$ (i.e., $| \\mathbf { v } | \\leq \\sigma _ { v } )$ and $\\mu _ { D }$ and $\\sigma _ { D }$ are obtained via Equations (2)-(3) on a dataset $D$ of size $m$ with $\\sigma _ { n } = \\sigma _ { v }$ , then it holds that, for every $x \\in X$ ,\n\n$$\n| \\mu _ { D } ( x ) - f ( x ) | \\leq \\beta _ { T } \\sigma _ { D } ( x ) ,\n$$\n\nwhere $\\beta _ { T } = \\sqrt { B ^ { 2 } - Y ^ { T } ( K _ { \\mathcal { X } , \\mathcal { X } } + \\sigma _ { v } ^ { 2 } I ) ^ { - 1 } Y + m } ,$ , and $Y$ and $K _ { \\mathcal { X } , \\mathcal { X } }$ are as in Equations (2)-(3).\n\nBy restricting noise to a bounded set, Lemma 2 is able to provide a deterministic bound on the GP regression error $| \\mu _ { D } ( x ) - f ( x ) |$ . Unlike the probabilistic error bounds in Lemma 1, the deterministic bound in Lemma 2 does not rely on the information gain $\\Gamma$ , removing a source of conservatism. However, it is generally conservative when $\\sigma _ { v }$ is not small, due to placing $\\sigma _ { n } = \\sigma _ { v }$ in Equations (2)-(3).\n\nNote that the bounds in Lemma 1 employ assumptions that are too general for our problem, i.e., $R$ -sub-Gaussian (conditioned on the filtration) vs bounded support noise independent of filtration, and rely on parameters that must be approximated. Similarly, the bounds in Lemma 2 do not allow probabilistic reasoning and are restrictive when the support on noise is large. Hence, there is a need for probabilistic error bounds derived specifically for bounded support noise, as well as a need for an improved deterministic error bound that does not grow conservatively with the size of the support.\n\n# 3 Bounded Support Error Bounds\n\nIn this section, we introduce novel probabilistic and deterministic error bounds for GPR of System (1), where noise has a bounded support distribution. We show that in this setting, the results of Lemmas 1 and 2 can be substantially improved. All the proofs can be found in the Appendix of the extended version (Reed, Laurenti, and Lahijanian 2024).\n\n# Probabilistic Error Bounds\n\nWe begin with the probabilistic bounds. With an abuse of notation, for the vector of input samples $\\mathcal { X } = [ x _ { 1 } , \\ldots , x _ { m } ] ^ { T }$ let $f ( \\mathcal { X } ) = [ f ( x _ { 1 } ) , \\dots , f ( \\overset { \\cdot } { x } _ { m } ) ] ^ { T }$ . Then, we observe that the noise output vector $Y$ is such that $\\boldsymbol { Y } = \\boldsymbol { f } ( \\boldsymbol { \\mathcal { X } } ) + \\boldsymbol { \\mathcal { V } }$ , where ${ \\boldsymbol { \\nu } } = [ v _ { 1 } , \\ldots , v _ { m } ] ^ { T }$ is a vector of i.i.d. samples of the noise, each of which is bounded by $\\sigma _ { v }$ . Consequently, from Eqn (2) and by denoting $G = ( K _ { \\mathcal { X } , \\mathcal { X } } + \\sigma _ { n } ^ { 2 } I ) ^ { - 1 }$ and $W _ { x } = K _ { x , x } G$ , we can bound the GP regression learning error as\n\n$$\n\\begin{array} { r l } { | \\mu _ { D } ( x ) - f ( x ) | = | W _ { x } ( f ( \\chi ) + \\mathcal { V } ) - f ( x ) | } & { } \\\\ { \\leq | W _ { x } f ( \\chi ) - f ( x ) | + | W _ { x } \\mathcal { V } | . } \\end{array}\n$$\n\nHence, the error is bounded by a sum of two terms: $| W _ { x } f ( \\mathcal { X } ) - f ( x ) |$ , which is the error due to mean prediction at $x$ with no noise, and $\\vert W _ { x } \\mathcal { V } \\vert$ , which is the error due to the noise with a value at most proportional to $\\sigma _ { v }$ . The following lemma, which extends results in (Hashimoto et al. 2022), bounds the first term.\n\nLemma 3. Let $X$ , $\\kappa , B$ , and $D$ be as in Lemma $^ { l }$ , and $G =$ $( K _ { \\mathbf { \\mathscr { X } } , \\mathscr { X } } + \\sigma _ { n } ^ { 2 } I ) ^ { - 1 }$ , $W _ { x } = K _ { x , x } G$ , and $c ^ { * } \\leq f ( \\chi ) ^ { T } G f ( \\chi )$ . Then, it holds that, for every $x \\in X$ ,\n\n$$\n| W _ { x } f ( \\chi ) - f ( x ) | \\leq \\sigma _ { D } ( x ) \\sqrt { B ^ { 2 } - c ^ { * } } .\n$$\n\nIn Theorem 1, we rely on Hoeffding’s Inequality (Mohri, Rostamizadeh, and Talwalkar 2018) to bound the second term in Eqn (7), which in turn provides a probabilistic bound on the GP regression error when combined with Lemma 3.\n\nTheorem 1 (Bounded Support Probabilistic RKHS Error). Let $X , \\kappa , B , D , G , W _ { x } ,$ , and $c ^ { * }$ be as in Lemma 3, and define $\\lambda _ { x } = 4 \\sigma _ { v } ^ { 2 } K _ { x , \\chi } G ^ { 2 } K _ { \\chi , x }$ . If noise $\\mathbf { v } \\in V$ is zero-mean and has a finite support $\\sigma _ { v }$ (i.e., $| \\mathbf { v } | \\leq \\sigma _ { v } )$ and $\\mu _ { D }$ and $\\sigma _ { D }$ are obtained via Eqns (2)-(3) on dataset $D$ with any choice of $\\sigma _ { n } > 0 ,$ , then it holds that, for every $x \\in X$ and $\\dot { \\delta } \\in ( 0 , 1 ]$ ,\n\n$$\n\\begin{array} { r } { \\mathbb { P } \\Big ( | \\mu _ { D } ( x ) - f ( x ) | \\leq \\epsilon ( x , \\delta ) \\Big ) \\geq 1 - \\delta , } \\end{array}\n$$\n\nwhere $\\begin{array} { r } { \\epsilon ( x , \\delta ) = \\sigma _ { D } ( x ) \\sqrt { B ^ { 2 } - c ^ { * } } + \\sqrt { \\frac { \\lambda _ { x } } { 2 } l n \\frac { 2 } { \\delta } } . } \\end{array}$\n\nThe proof uses Lemma 3 for the first term of $\\epsilon ( x , \\delta )$ and derives the second term by applying Hoeffding’s Inequality to $\\vert W _ { x } \\mathcal { V } \\vert$ by noting that $\\mathbb { E } [ W _ { x } \\mathcal { V } ] = 0$ and each $- \\sigma _ { v } \\ \\leq$ $v _ { i } \\leq + \\sigma _ { v }$ , enabling the random variable to maintain bounded support on each term. Note that Theorem 1 only requires two parameters in its probabilistic bound: $c ^ { * }$ and $B$ . In fact, $c ^ { * }$ can be found by solving the following quadratic optimization problem, where we rely on the boundedness of the support of the distribution of ${ \\bf \\dot { \\boldsymbol \\nu } } = [ v _ { 1 } , \\dots , v _ { m } ] ^ { T }$ :\n\n$$\n\\boldsymbol { c } ^ { * } = \\operatorname* { m i n } _ { \\substack { - \\sigma _ { v } \\leq v _ { i } \\leq \\sigma _ { v } , i = 1 , \\ldots , m } } ( \\boldsymbol { Y } - \\boldsymbol { \\mathcal { V } } ) ^ { T } \\boldsymbol { G } ( \\boldsymbol { Y } - \\boldsymbol { \\mathcal { V } } ) .\n$$\n\nThe other parameter, $B$ , can be formally bounded by the technique introduced in (Jackson et al. 2021a). This is in contrast with Lemma 1, which also requires an approximation on the information gain of the kernel. We should also emphasize that Theorem 1 allows $\\sigma _ { n }$ to remain as a decision variable, enabling an optimization over $\\sigma _ { n }$ that can further minimize the error bounds as compared to Lemmas 1 and 2.\n\nWe also note as $m  \\infty$ then $\\lambda _ { x }$ tends toward 0, which implies that we can set $\\delta$ arbitrarily close to 0, reducing the error bound to the result of Lemma 3, which decreases with the number of samples as $\\sigma _ { D } ( x )$ decreases and $c ^ { * }$ increases. This demonstrates an $O ( { \\sqrt { m } } )$ improvement over the results of Lemma 2. A detailed discussion is provided in the Appendix of (Reed, Laurenti, and Lahijanian 2024).\n\nWe extend our point-wise probabilistic error bounds to a uniform bound with the following corollary.\n\nCorollary 1 (Uniform Error Bounds). For a given compact set $X ^ { \\prime } \\subseteq X$ ,\n\n$$\n\\begin{array} { r } { \\mathbb { P } \\Big ( | \\mu _ { D } ( x ) - f ( x ) | \\leq \\overline { { \\epsilon } } _ { X ^ { \\prime } } ( \\delta ) \\Big ) \\geq 1 - \\delta } \\end{array}\n$$\n\n$\\forall x \\in X ^ { \\prime }$ , where $\\begin{array} { r } { \\overline { { \\epsilon } } _ { X ^ { \\prime } } ( \\delta ) = \\operatorname* { s u p } _ { x \\in X ^ { \\prime } } \\epsilon ( \\delta , x ) } \\end{array}$ .\n\n# Deterministic Error Bounds\n\nIf error bounds with confidence one $\\delta = 0 \\rangle$ ) are desired, Theorem 1 would require infinite samples. Here, we show how confidence one results can be derived with finite samples, producing an alternative deterministic error bound to the one in Lemma 2.\n\nTheorem 2 (Bounded Support Deterministic RKHS Error). Let $X , \\ B , \\ D , \\ G , \\ c ^ { * }$ be as in Theorem $\\jmath$ , and $\\Lambda _ { x } =$ $\\textstyle \\sum _ { i = 1 } ^ { m } \\sigma _ { v } | [ K _ { x , x } G ] _ { i } |$ . If noise $\\mathbf { v } \\in V$ has a finite support $\\sigma _ { v }$ (i.e., $| \\mathbf { v } | \\leq \\sigma _ { v } )$ and $\\mu _ { D }$ and $\\sigma _ { D }$ are obtained via Eqns (2)-(3) on dataset $D$ with any choice of $\\sigma _ { n } > 0$ , then it holds that, for every $x \\in X$ ,\n\n$$\n| \\mu _ { D } ( x ) - f ( x ) | \\leq \\epsilon _ { d } ( x )\n$$\n\nwhere $\\epsilon _ { d } ( x ) = \\sigma _ { D } ( x ) \\sqrt { B ^ { 2 } - c ^ { * } } + \\Lambda _ { x } .$\n\nProof. Consider the term $| W _ { x } \\mathcal { V } |$ in Eqn (7). The noise distribution that maximizes $\\vert W _ { x } \\mathcal { V } \\vert$ is found by setting $v _ { i } \\ =$ $\\mathrm { s i g n } ( [ W _ { x } ] _ { i } ) \\sigma _ { v }$ , where $\\mathrm { s i g n } ( [ W _ { x } ] _ { i } ) = 1$ if the $i$ -th term of $W _ { x }$ is $\\geq 0$ and 0 otherwise. Using this bound and Lemma 3 on the RHS of Eqn (7), we conclude the proof. □\n\nRemark 1. We stress that while Lemmas 1 and 2 define strict requirements on the value of $\\sigma _ { n }$ used in the posterior prediction, i.e., $1 + 2 / m$ and $\\sigma _ { v }$ respectively, both Theorems 1 and 2 allow for any value to be used for $\\sigma _ { n }$ . In particular, $\\sigma _ { n } \\ll \\sigma _ { v }$ is a valid selection which can reduce the predictive variance, enabling much tighter deterministic and probabilistic bounds. Similarly, when $\\sigma _ { v }$ is very small, we can choose $\\sigma _ { n } > \\sigma _ { v }$ to avoid numerical instabilities in inverting $( K _ { \\mathbf { \\mathscr { X } } , \\mathscr { X } } + \\sigma _ { n } ^ { 2 } I )$ .\n\nWe demonstrate the trend of the error bounds as we adjust $\\sigma _ { n }$ used for the posterior predictions of a GP in Figure 2. It is immediately clear that using a smaller $\\sigma _ { n }$ when there is more data results in tight error bounds across the domain.\n\nRemark 2. In this paper, we consider the offline setting, where we are given a set of i.i.d. data. In this setting, the bounds proposed in (Abbasi-Yadkori 2013; Srinivas et al. 2012; Chowdhury and Gopalan 2017) are still valid. To extend to online setting, our bounds can be updated as data is gathered, and the noise must remain independent of the filtration, which is typical in robotics applications.\n\n# Extension to Deep Kernel Learning\n\nDeep Kernel Learning (DKL) is an extension of GPR (Ober, Rasmussen, and van der Wilk 2021). In DKL, we compose a base kernel, e.g., the squared-exponential $\\kappa _ { s e } \\overset { \\cdot } { = } \\sigma _ { s } \\exp \\left( - \\| x - x ^ { \\prime } \\| / 2 l ^ { \\overline { { 2 } } } \\right)$ , with a neural network as $\\kappa _ { D K L } ( x , x ^ { \\prime } ) = \\kappa _ { s e } ( \\psi _ { w } ( x ) , \\psi _ { w } ( x ^ { \\prime } ) )$ , where $\\psi _ { w } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { s }$ is a neural network parameterized by weights $w$ . Then posterior predictions still use analytical Eqns (2)-(3), but the kernel now includes significantly more parameters which has been shown to significantly improve the representational power of GPs without needing more data in the kernel (Reed, Laurenti, and Lahijanian 2023).\n\n![](images/f3780fe8e06989696e26412c7085e2c28a0a17089e57395d7e4737b28b5b4db0.jpg)  \nFigure 2: Predictive mean and deterministic error bounds when learning from 20 samples of $y = x \\sin ( x ) + v$ with $| \\boldsymbol { v } | < 0 . 5$ as $\\sigma _ { n }$ varies. We plot the mean and bounds from Lemma (2) in red, with our mean and bounds in green. In all cases our bounds remain valid, but demonstrate optimal performance when $\\sigma _ { n } \\in [ \\sigma _ { v } / 1 0 , \\sigma _ { v } / 5 ]$ .\n\nRemark 3. For DKL to satisfy Assumption 1, the kernel must remain continuously differentiable and positive semi-definite. Using the GeLU or Tanh activation functions and learning $\\psi _ { w } ( x )$ as a model of $f ( x )$ using stochastic mini-batching can prove sufficient. Then, under the assumption that $\\psi _ { w }$ is wellbehaved over a compact set $X$ , the RKHS norm $\\| f \\| _ { \\kappa _ { D K L } } \\leq$ $\\| f \\| _ { \\kappa }$ is feasible. This can be inferred directly from (Jackson et al. 2021a, Proposition 2), as DKL tends to correlate data more effectively over space, it is reasonable to expect that $\\begin{array} { r } { \\operatorname* { i n f } _ { x , x ^ { \\prime } \\in X } \\kappa _ { D K L } ( x , x ^ { \\prime } ) \\geq \\operatorname* { i n f } _ { x , x ^ { \\prime } \\in X } \\kappa ( x , x ^ { \\prime } ) . } \\end{array}$ .\n\nDKL can reduce the posterior variance of a GP and use the same RKHS norms as the base kernel, i.e., decreasing $\\sigma _ { D }$ without increasing bound $B$ . Therefore, DKL can enable tighter GP regression error bounds with the same number of predictive samples than standard GP. Similarly, in the event that many samples are available, network $\\psi _ { w }$ can be pretrained over a large set of samples and the kernel can use a small subset of the data for posterior predictions, enabling computationally efficient calculations with increased accuracy. Furthermore, compared to Lemma 1, our error bounds utilize significantly more information about the kernel. This allows an informed kernel, such as DKL, to further reduce the bound beyond just the value of $\\sigma _ { D }$ .\n\n# 4 Application to Safety of Control Systems\n\nThe bounds we derive in Theorems 1 and 2 can be particularly important to provide safety guarantees for dynamical systems learned from data. Specifically, consider the following model of a discrete-time stochastic system with additive noise\n\n$$\n\\begin{array} { r } { \\mathbf { x } ( k + 1 ) = f ( \\mathbf { x } ( k ) ) + \\mathbf { v } \\qquad k \\in \\mathbb { N } , } \\end{array}\n$$\n\nwhere $\\mathbf { x } \\in \\mathbb { R } ^ { d }$ is the state, $\\mathbf { v } \\in V \\subset \\mathbb { R } ^ { d }$ is a random variable independent and identically distributed at each time step representing an additive noise term, and $f : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { d }$ is an unknown, possibly non-linear, function (vector field). Intuitively, ${ \\bf x } ( k )$ represents a general model of a stochastic system taking values in Rd.\n\nA key challenge in many applications is to guarantee that System (13) will stay within a given safe set $X _ { s } ~ \\subset ~ \\mathbb { R } ^ { d }$ , i.e., it avoids obstacles, for a given time horizon $[ 0 , N ]$ . A common technique to obtain such guarantees is to rely on stochastic barrier functions (Kushner 1967; Santoyo, Dutreix, and Coogan 2021), which represents a generalization of Lyapunov functions to provide set invariance. The intuition behind barriers is to build a function $B : \\mathbb { R } ^ { d }  \\mathbb { R }$ that when composed with System (13) forms a supermartingale and consequently, martingale inequalities can be used to study the systems’ evolution, as shown in the following Proposition.\n\nProposition 1 ( (Mazouz et al. 2022), Section 3.1). Given an initial set $X _ { 0 }$ , a safe set $X _ { s }$ , and an unsafe set $X _ { u } = \\mathbb { R } ^ { d } \\backslash X _ { s }$ , a twice differentiable function $B : \\mathbb { R } ^ { \\bar { d } }  \\mathbb { R }$ is a barrier function if the following conditions hold for $\\beta , \\eta \\in [ 0 , 1 ]$ : $B ( x ) \\geq 0$ for all $\\boldsymbol { x } \\in \\mathbb { R } ^ { \\breve { d } }$ , $B ( x ) \\leq \\eta$ for all $x \\in X _ { 0 }$ , $B ( x ) \\geq$ 1 for all $\\forall x \\in X _ { u } ,$ and\n\n$$\n\\mathbb { E } [ \\mathcal { B } ( f ( x ) + \\mathbf { v } ) \\mid x ] \\leq \\mathcal { B } ( x ) + \\beta \\quad \\forall x \\in X _ { s } .\n$$\n\nThen, it holds that\n\n$$\nP ( \\forall k \\in [ 0 , N ] , \\mathbf { x } ( k ) \\in X _ { s } \\mid \\mathbf { x } ( 0 ) \\in X _ { 0 } ) \\geq 1 - ( \\beta N + \\eta ) .\n$$\n\nHence, by finding an appropriate $\\boldsymbol { B }$ , safety of System (13) can be guaranteed. However, how to construct such $\\boldsymbol { B }$ when $f$ is unknown is still an open problem (Jagtap, Soudjani, and Zamani 2020; Wajid, Awan, and Zamani 2022; Mazouz et al. 2022, 2024). Here, we show that, under the assumption that $f$ lies in the RKHS of $\\kappa$ and $\\| f ^ { ( i ) } \\| _ { \\kappa } < B _ { i }$ , where $f ^ { ( i ) }$ denotes the $i$ -th output of $f$ , we can employ GPR along with Theorems 1 and 2 to construct $\\boldsymbol { B }$ .\n\nIn particular, by partitioning $X _ { s }$ into a set of convex regions $Q = \\{ q _ { 1 } , . . . , \\bar { q } _ { | Q | } \\}$ s.t. $X _ { s } = \\cup _ { q \\in Q ^ { q } }$ , it is sufficient to simply replace the condition in (14) with the following constraints for each $q \\in Q$ :\n\n$$\n\\begin{array} { r l r } & { \\mathbb { E } [ \\mathcal { B } ( z _ { q } + \\mathbf { v } ) \\mid \\boldsymbol { x } \\in q ] \\le \\mathcal { B } ( \\boldsymbol { x } ) + \\beta } \\\\ & { z _ { q } ^ { ( i ) } \\in [ \\mu _ { D } ^ { ( i ) } ( \\boldsymbol { x } ) \\pm \\overline { { \\epsilon } } _ { q } ^ { ( i ) } ( \\delta ) ] } & { \\forall \\boldsymbol { x } \\in q , } \\end{array}\n$$\n\nwhere $\\mu _ { D } ^ { ( i ) }$ denotes the mean prediction for $f ^ { ( i ) }$ and $\\overline { { \\epsilon } } _ { q } ^ { ( i ) } ( \\delta )$ can be computed by combining Corollary 1 with either Theorem 1 or 2. Work (Mazouz et al. 2024) shows how to calculate a barrier with such constraints. In the former case, the safety probability in Proposition 1 holds with the confidence resulting from Theorem 1, while in the latter case results hold with confidence 1. In Section 5, we consider both cases.\n\n1.0 Hashimoto et. al. Chowdhury et. al. 0.35 Hashimoto et. al. 2.5 Chowdhury et. al. 0.468 Our Deterministic 2 AOburbaPsrio-Ybadbkiloirsitic 0.20 0.30 Our Deterministic 1.5 01.50 AOburbaPsrio-Ybadbkiloirsitic 0 0.0 50 75 100 125 150 175 200 50 75 100 125 150 175 200 50 75 100 125 150 175 200 50 75 100 125 150 175 200 Num. Predictive Pts. Num. Predictive Pts. Num. Predictive Pts. Num. Predictive Pts. (a) GP - det. error comparison (b) GP - prob. error comparison (c) DKL - det. error comparison (d) DKL - prob. error comparison\n\n0.5 Deterministic 0.18 Deterministic 99% 0.16 99% Error Bounds 95% D 95% 0.4 0.14 90% 90% 75% 0.12 75% 0.3 50% 0.0180 50% 0.2 0.06 0.1 0.04 50 75 100 125 150 175 200 50 75 100 125 150 175 200 Num. Predictive Pts. Num. Predictive Pts. (a) GP - vary $\\delta$ comparison (b) DKL - vary $\\delta$ comparison\n\n# 5 Experiments\n\nIn this section, we demonstrate the effectiveness of our bounds in comparison to the existing state-of-the-art bounds. We first provide a visual of the trends of our bounds as more data is utilized for posterior predictions, as well as demonstrating the improvements that DKL can provide over standard GPs. We then demonstrate how our bounds perform in comparison to the state-of-the-art across several dynamical systems of interest. Finally, we show how our bounds can be used in safety certification, through the use of stochastic barrier functions, as described in Section 4. We note that deep kernels in general are only positive semi-definite, which prohibits the use of techniques in (Maddalena, Scharnhorst, and Jones 2021) for generating a deterministic bound. Further detail on the models considered can be seen in the Appendix of (Reed, Laurenti, and Lahijanian 2024).\n\n2D Visual Example We consider the case of GP regression for a linear system with $\\mathbf { x } \\in \\mathbb { R } ^ { 2 }$ and $| v | \\leq 0 . 1$ , defined as\n\n$$\n\\mathbf { y } = 0 . 4 x _ { 1 } + 0 . 1 x _ { 2 } + v .\n$$\n\nWe first consider a squared exponential kernel and using the methods proposed in (Hashimoto et al. 2022) we set $B = 1 0$ . For our predictions, we set $\\sigma _ { n } = \\sigma _ { v } / 5 = 0 . 0 2$ . We illustrate the trend of the bounds as the number of data points in the kernel increases in Figures 3a-3b, showing the averaged mean bound and one standard deviation over $\\bar { 1 } 0 ^ { 4 }$ test points.\n\nWe note that our deterministic bound is comparable or better than the existing approach and our probabilistic bound significantly outperforms existing bounds.\n\nNext, we consider the same scenario but with a DKL kernel. Results are shown in Figures 3c-3d. Here, we assume a fixed neural network that is pre-trained to model $f ( x )$ from data. We consider a network with two hidden layers of 16 neurons each with the GeLU activation function. The network is trained over 1000 sample points through stochastic mini-batching. Figure 4 compares averages of our deterministic bound to our probabilistic bound. Interestingly, DKL generates probabilistic errors lower than the bound on sample noise, enabling highly accurate predictions even with very noisy samples. We note that, for each model, our probabilistic bound remain accurate for the desired confidence level.\n\nExamples for Multiple Dynamical Systems We compare our bounds to existing bounds (Lemma 1-2) and bounds proposed by Abbasi-Yadkori; Srinivas et al. for a variety of dynamical systems from literature (Jackson et al. 2021b; Adams, Lahijanian, and Laurenti 2022; Reed, Laurenti, and Lahijanian 2023), namely a 2D linear model, a 2D non-linear model, a 3D Dubin’s car model, and a 5D second order car model. We consider identical noise distributions in all dimensions; for the 2D and 3D systems we consider $\\mathbf { v } \\sim \\mathrm { U n i f o r m } ( - 0 . 1 , 0 . 1 )$ and for the 5D system $\\mathbf { v } \\sim \\mathrm { U n i f o r m } ( - 0 . 2 , 0 . 2 )$ . Results are in Table 1.\n\nWe note that modifying the value of $\\sigma _ { n }$ used for posterior predictions does not impact the accuracy of the model significantly but can be optimized to improve the error bounds. In all cases, we see that DKL (with a well trained neural network prior) significantly outperforms standard GP models when computing error bounds. We emphasize that the probabilistic bounds from (Abbasi-Yadkori 2013), Lemma 1, and (Srinivas et al. 2012) are significantly larger than ours for two primary reasons: (i) those bounds generalize to any conditional sub-Gaussian distribution while ours is specific to bounded support, and (ii) our bound incorporates significantly more information about the kernel, allowing an informed kernel to reduce the bound further than just the value of $\\sigma _ { D } ( x )$ . This results in our probabilistic bound being at least an order of magnitude smaller than prior works.\n\nSafety Certification of Unknown Stochastic Systems We consider the Inverted Pendulum (2D) agent from the Gymnasium environment and a contractive linear 4D system for data-driven safety certification using the formulation in Section 4. We collected data for the Pendulum model under the best controller available from the OpenAI Leaderboard (OpenAI 2024) and perturb the systems with PERT distributions (a transformation of a Beta distribution with a specified mean). We construct a barrier using the Dual-Approach suggested in (Mazouz et al. 2024) and compare results when generating interval bounds on $f$ with Corollary 1 using Theorem 2, Lemma 2, and Theorem 1 with $\\delta = 0 . 0 5$ using DKL models.\n\nTable 1: Average $L _ { 1 }$ error bounds $( \\left. \\epsilon \\right. _ { 1 } )$ over 10000 test points. We report the value for various values of $\\sigma _ { n }$ and for the squared exponential kernel (SE) and for DKL. We report both the true error $( \\| \\mu - f \\| _ { 1 } )$ induced by the model estimated empirically over $1 0 ^ { 4 }$ samples and the bounds produced by Theorem 2 (Our Det.), Lemma 2 (Lem 2 Det.), and probabilistic bounds from Theorem 1 (Our Prob.), Abbasi-Yadkori (AY Prob.), Lemma 1 (Lem 1 Prob.), and Srinivas et al. (SKKS Prob.) in order setting $\\delta = 0 . 0 5$ . Lemma 2 and (Srinivas et al. 2012) set $\\sigma _ { n } = \\sigma _ { v }$ and Lemma 1 and (Abbasi-Yadkori 2013) set $\\sigma _ { n } ^ { 2 } = 1 + 2 / m$ .   \n\n<html><body><table><tr><td rowspan=\"2\">System K</td><td rowspan=\"2\">On</td><td rowspan=\"2\"></td><td colspan=\"2\">Our Det.</td><td colspan=\"2\">Lem 2 Det.</td><td colspan=\"2\">Our Prob.</td><td colspan=\"2\">AY Prob.</td><td colspan=\"2\">Lem 1 Prob.</td><td colspan=\"2\">SKKS Prob.</td></tr><tr><td>true</td><td>|e|l1</td><td></td><td>trueIlell1</td><td>true|lell1</td><td></td><td>trueIll1</td><td></td><td>true</td><td>|le|l1</td><td>true</td><td>|ell1</td></tr><tr><td>2D Lin</td><td>SE</td><td>0/5</td><td>0.06</td><td>0.86</td><td>0.041.45</td><td></td><td>0.060.57</td><td></td><td>0.10 5.72</td><td></td><td>0.10</td><td>7.53</td><td>0.04</td><td>350</td></tr><tr><td>2D Lin</td><td>SE</td><td>0/10</td><td>0.07</td><td>0.84</td><td>0.04 1.45</td><td></td><td>0.07 0.50</td><td></td><td>0.10 5.72</td><td></td><td>0.10</td><td>7.53</td><td>0.04</td><td>350</td></tr><tr><td>2D Lin</td><td>DKL</td><td>0u/5</td><td>0.04</td><td>0.36</td><td>0.04 0.45</td><td></td><td>0.04 0.15</td><td></td><td>0.08 2.72</td><td></td><td>0.08</td><td>3.66</td><td>0.04</td><td>141</td></tr><tr><td>2D Lin</td><td>DKL</td><td>ou/10</td><td>0.04</td><td>0.33</td><td>0.04 0.45</td><td></td><td>0.04 0.12</td><td></td><td>0.08 2.72</td><td></td><td>0.08</td><td>3.66</td><td>0.04</td><td>141</td></tr><tr><td>2D NL</td><td>SE</td><td>0/5</td><td>0.09</td><td>1.23</td><td>0.08 1.68</td><td></td><td>0.09</td><td>0.94</td><td>0.327.47</td><td></td><td>0.32</td><td>9.29</td><td>0.08</td><td>350</td></tr><tr><td>2D NL</td><td>SE</td><td>0u/10</td><td>0.11</td><td>1.31</td><td>0.08 1.68</td><td></td><td>0.11 0.90</td><td></td><td>0.32 7.47</td><td></td><td>0.32</td><td>9.29</td><td>0.08</td><td>350</td></tr><tr><td>2D NL</td><td>DKL</td><td>0u/5</td><td>0.04</td><td>0.39</td><td>0.04 0.53</td><td></td><td>0.040.22</td><td></td><td>0.19 3.79</td><td></td><td>0.19</td><td>4.75</td><td>0.04</td><td>130</td></tr><tr><td>2D NL</td><td>DKL</td><td>0/10</td><td>0.04</td><td>0.35</td><td>0.04 0.53</td><td></td><td>0.04 0.18</td><td></td><td>0.19 3.79</td><td></td><td>0.19</td><td>4.75</td><td>0.04</td><td>130</td></tr><tr><td>3D DC</td><td>SE</td><td>0/5</td><td>0.10</td><td>1.96</td><td>0.092.34</td><td></td><td>0.10</td><td>1.13</td><td>0.41 9.61</td><td></td><td>0.41</td><td>13.67</td><td>0.09</td><td>1071</td></tr><tr><td>3D DC</td><td>SE</td><td>0u/10</td><td>0.12</td><td>2.13</td><td>0.09 2.34</td><td></td><td>0.12 1.06</td><td></td><td>0.41 9.61</td><td></td><td>0.41</td><td>13.67</td><td>0.09</td><td>1071</td></tr><tr><td>3D DC</td><td>DKL</td><td>0u/5</td><td>0.03</td><td>0.44</td><td>0.030.49</td><td></td><td>0.030.12</td><td></td><td>0.29</td><td>2.82</td><td>0.29</td><td>4.32</td><td>0.03</td><td>195</td></tr><tr><td>3D DC</td><td>DKL</td><td>/10 oul</td><td>0.03</td><td>0.43</td><td>0.030.49</td><td></td><td>0.030.11</td><td></td><td>0.29 2.82</td><td></td><td>0.29</td><td>4.32</td><td>0.03</td><td>195</td></tr><tr><td>5D Car</td><td>SE</td><td>0/5</td><td>0.51</td><td>10.33</td><td>0.30</td><td>11.7</td><td>0.515.60</td><td></td><td>0.36</td><td>14.0</td><td>0.36</td><td>22.63</td><td>0.30</td><td>3475</td></tr><tr><td>5D Car</td><td>SE</td><td>0u/10</td><td>0.68</td><td>13.0</td><td>0.30 11.7</td><td></td><td>0.68 6.16</td><td></td><td>0.36 14.0</td><td></td><td>0.36 22.63</td><td></td><td>0.30</td><td>3475</td></tr><tr><td> 5D Car</td><td>DKL</td><td>0u/5</td><td>0.06</td><td>1.48</td><td>0.06 1.54</td><td></td><td>0.060.46</td><td></td><td>0.25 4.34</td><td></td><td>0.25 6.82</td><td></td><td>0.06</td><td>581</td></tr><tr><td>5D Car</td><td>DKL</td><td>ou/10</td><td>0.06</td><td>1.43</td><td>0.06 1.54</td><td></td><td>0.06 0.40</td><td></td><td>0.25 4.34</td><td></td><td>0.25</td><td>6.82</td><td>0.06</td><td>581</td></tr></table></body></html>\n\nTable 2: Barrier results, where $t$ is time in seconds taken to synthesize a barrier, $P _ { s } = 1 - ( \\eta + N \\beta )$ , and $N = 1$ .   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"4\">Our Prob.</td><td colspan=\"4\">Our Det.</td><td colspan=\"4\">Lem 2 Det.</td><td rowspan=\"2\">t</td></tr><tr><td></td><td>β</td><td>Ps</td><td>t</td><td>m</td><td>β</td><td>P</td><td>t</td><td></td><td>β</td><td>Ps</td><td></td></tr><tr><td>Pendulum</td><td>1e-6</td><td>1e-6</td><td>0.999</td><td>136</td><td>1e-6</td><td>0.077</td><td>0.923</td><td>147</td><td>1e-6</td><td>0.499</td><td></td><td>0.499</td><td>2979</td></tr><tr><td>4D Linear</td><td>1e-6</td><td>1e-6</td><td>0.999</td><td>451</td><td>1e-6</td><td>0.172</td><td>0.827</td><td>2880</td><td></td><td>1e-6</td><td>0.249</td><td>0.749</td><td>13233</td></tr></table></body></html>\n\nThe barriers identify a lower bound on the probability of the system remaining in a predefined safe set for a given horizon when initialized with $\\theta , \\dot { \\theta } ~ \\in ~ [ - 0 . 0 2 5 , 0 . 0 \\dot { 2 } 5 ] ~ \\times \\$ $[ - 0 . 0 5 5 , 0 . 0 5 5 ]$ for the Pendulum and each state in $[ - 0 . 1 , 0 . 1 ]$ for the 4D system. Barriers are synthesized on an Intel Core i7-12700K CPU at 3.60GHz with $3 2 \\mathrm { G B }$ of RAM. Results are reported in Table 2. Safety probabilities using bounds of Lemma 1 and (Abbasi-Yadkori 2013) are not reported in the table because they result in $P _ { s } = 0$ after three time steps for both models.\n\nBarrier certificates using Theorem 1 are based on a $9 5 \\%$ confidence. We see that our bounds allow for a significant improvement in certification results as compared to existing bounds. For instance, for a time horizon $N = 1 0$ , applying Proposition 1 with our bounds results in guarantees of\n\n$9 9 . 9 \\%$ safety probability with $9 5 \\%$ confidence, whereas using Lemma 2 results in a $0 \\%$ safety probability for the Pendulum model. We see similar results for the 4D system, where the deterministic bounds remain too conservative to identify the contractive nature of the system yet our probabilistic bounds enable guarantees of safety with high confidence. We also note that the reduced conservatism of our approach enable barrier synthesis to be significantly faster.\n\n# 6 Conclusion\n\nIn this paper, we derive novel error bounds (both probabilistic and deterministic) for GP regression under bounded support noise. We demonstrate that by assuming sample noise has zero mean, error bounds that are tighter than sample noise can be achieved. We show that our error bounds utilize significantly more information about the kernel than existing probabilistic bounds (i.e. each term informed by $K _ { \\mathcal { X } , x } )$ ; hence, they are well-suited for regression techniques based on informed kernels such as DKL that correlate $x$ with the entire dataset. We also show the improvements that our bounds can provide over the state-of-the-art in safety certification of control systems through the use of stochastic barrier functions, generating certificates with significantly larger safety probabilities. Future directions include application of these bounds in safe control and shield design in reinforcement learning.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了高斯过程回归（GPR）在噪声有界支持（bounded support noise）情况下的误差界限问题。现有方法在处理非高斯噪声时过于保守，限制了其在安全关键领域（如控制系统）的应用。\\n> *   该问题的重要性在于：安全关键应用需要严格的误差量化，而现有方法无法提供足够紧的误差界限，导致安全概率评估不准确。\\n\\n> **方法概述 (Method Overview)**\\n> *   通过结合浓度不等式和再生核希尔伯特空间（RKHS）的低复杂性假设，论文提出了新的概率性和确定性误差界限，显著优于现有方法。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 提出了针对有界支持噪声的GPR误差界限，比现有方法更紧（实验显示误差界限缩小了50%以上）。\\n> *   **贡献2：** 展示了这些界限如何与随机屏障函数结合，用于量化未知动态系统的安全概率。\\n> *   **贡献3：** 验证了深度核学习（DKL）在误差界限上的优势，DKL的误差界限可以比样本噪声更紧。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   论文的核心思想是将GPR误差分解为两部分：无噪声预测误差和噪声扰动误差。通过利用噪声的有界性和RKHS的低复杂性假设，推导出更紧的误差界限。\\n> *   该方法有效的原因是：有界支持噪声允许使用更精确的浓度不等式（如Hoeffding不等式），而RKHS假设提供了函数复杂性的量化工具。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有方法（如Chowdhury和Gopalan的界限）假设噪声是次高斯的（sub-Gaussian），导致界限过于保守。Hashimoto等人的界限虽然针对有界噪声，但随着噪声支持增大而变得松散。\\n> *   **本文的改进：** 本文直接利用噪声的有界性，通过优化参数（如σₙ）和引入RKHS复杂性约束，显著减少了保守性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  将GPR误差分解为无噪声预测误差和噪声扰动误差。\\n> 2.  使用凸优化和Hoeffding不等式分别对两部分误差进行量化。\\n> 3.  推导概率性和确定性误差界限，并通过优化参数（如σₙ）进一步收紧界限。\\n> 4.  将界限应用于深度核学习（DKL），验证其在减少后验方差和误差界限上的优势。\\n\\n> **案例解析 (Case Study)**\\n> *   论文以函数y = x sin(x) + v（|v| < 0.5）为例，展示了本文方法在20个样本上的预测均值和误差界限（图1）。与现有方法相比，本文的界限更紧且更接近真实函数。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   Chowdhury和Gopalan的次高斯噪声界限（Lemma 1）。\\n> *   Hashimoto等人的有界噪声界限（Lemma 2）。\\n> *   Abbasi-Yadkori的核岭回归界限。\\n> *   Srinivas等人的次高斯噪声界限。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在误差界限紧度上：** 本文方法在2D线性系统上的平均L₁误差界限为0.86（σₙ=0.02），显著优于Hashimoto等人的1.45和Chowdhury等人的5.72。与表现最佳的基线（Hashimoto）相比，提升了约40%。\\n> *   **在深度核学习（DKL）上：** 本文方法在DKL上的误差界限为0.36（σₙ=0.02），远低于标准GPR的0.86，同时与Hashimoto等人的界限（0.45）相比更紧。\\n> *   **在安全概率评估上：** 结合随机屏障函数，本文方法在倒立摆模型上实现了99.9%的安全概率（95%置信度），而Hashimoto等人的方法仅能提供49.9%的安全概率。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   高斯过程回归 (Gaussian Process Regression, GPR)\\n*   有界支持噪声 (Bounded Support Noise, N/A)\\n*   误差界限 (Error Bounds, N/A)\\n*   再生核希尔伯特空间 (Reproducing Kernel Hilbert Space, RKHS)\\n*   深度核学习 (Deep Kernel Learning, DKL)\\n*   安全认证 (Safety Certification, N/A)\\n*   随机屏障函数 (Stochastic Barrier Functions, N/A)\"\n}\n```"
}