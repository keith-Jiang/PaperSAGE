{
    "source": "Semantic Scholar",
    "arxiv_id": "2403.14155",
    "link": "https://arxiv.org/abs/2403.14155",
    "pdf_link": "https://arxiv.org/pdf/2403.14155.pdf",
    "title": "Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization",
    "authors": [
        "Yeji Song",
        "Jimyeong Kim",
        "Wonhark Park",
        "Wonsik Shin",
        "Wonjong Rhee",
        "N. Kwak"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2024-03-21",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 3,
    "influential_citation_count": 1,
    "institutions": [
        "Seoul National University"
    ],
    "paper_content": "# Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization\n\nYeji Song, Jimyeong $\\mathbf { K i m ^ { * } }$ , Wonhark Park\\*, Wonsik Shin, Wonjong Rhee, Nojun Kwak†\n\nSeoul National University ldynx, wlaud1001, pwh0515, wonsikshin, wrhee, nojunk @snu.ac.kr\n\n# Abstract\n\nIn a surge of text-to-image (T2I) models and their customization methods that generate new images of a user-provided subject, current works focus on alleviating the costs incurred by a lengthy per-subject optimization. These zero-shot customization methods encode the image of a specified subject into a visual embedding which is then utilized alongside the textual embedding for diffusion guidance. The visual embedding incorporates intrinsic information about the subject, while the textual embedding provides a new context. However, the existing methods often 1) generate images with the same pose as an input image, and 2) exhibit deterioration in the subject’s identity when facing a pose variation prompt. We first pin down the problem and show that redundant pose information in the visual embedding interferes with the pose indication in the textual embedding. Conversely, the textual embedding also harms the subject’s identity which is tightly entangled with the pose in the visual embedding. As a remedy, we propose text-orthogonal visual embedding which effectively harmonizes with the given textual embedding. We also adopt the visual-only embedding and inject the subject’s clear features using a self-attention swap. Our method is both effective and robust, offering highly flexible zero-shot generation while effectively maintaining the subject’s identity.\n\nExtended version — https://arxiv.org/abs/2403.14155\n\n# Introduction\n\nRecent advancements in text-to-image (T2I) generation, especially diffusion models (Rombach et al. 2022; Balaji et al. 2023; Saharia et al. 2022; Ramesh et al. 2022; Nichol et al. 2022) have opened up a new era of image creation. Subjectdriven generation (Ruiz et al. 2023; Gal et al. 2022a; Tewel et al. 2023a; Qiu et al. 2023; Voynov et al. 2023) aims to generate novel images featuring a specific subject provided by the user. The common approach represents the subject as a new pseudo-word $( \\boldsymbol { \\mathsf { S } } ^ { * } )$ in the textual embedding space of the text encoder. They optimize a pseudo-word by updating the textual embedding of the pseudo-word (Gal et al. 2022b) or the diffusion model’s parameters (Ruiz et al. 2023; Tewel et al. 2023b). However, these approaches often do not align with the actual needs of users, who typically 1) possess constrained GPU resources, 2) desire real-time applications, and 3) pursue a convenient rendition with a single input image. In response to these challenges, single-image-based zeroshot customization methods (Li, Li, and Hoi 2024; Wei et al. 2023; Chen et al. 2023; Ma et al. 2023; Jia et al. 2023; Yuan et al. 2023; Xiao et al. 2023; Zhang et al. 2024; Song et al. 2025; Ye et al. 2023) have been proposed.\n\nTo obtain the pseudo-word from a single image without the time and resource-intensive optimization process, they adopt the pre-trained mappers such as MLP network (Wei et al. 2023; Yuan et al. 2023; Xiao et al. 2023), adapter (Chen et al. 2023; Shi et al. 2023; Ma et al. 2023) or multi-modal encoder (Li, Li, and Hoi 2024) to transform a subject’s image into the visual embedding. The visual embedding provides representative information about the subject’s identity from the input image, utilized along with the textual embedding which contains a novel desired context. This approach eliminates the need for additional training processes while effectively depicting the subject in new scenes.\n\nHowever, the existing methods are susceptible to confusing the subject’s identity with other irrelevant details within an image. To remedy this, they employ the subject’s segmentation mask (Li, Li, and Hoi 2024; Wei et al. 2023; Ma et al. 2023; Jia et al. 2023; Xiao et al. 2023) or utilize the embedding from the deepest layers of an image encoder (Wei et al. 2023), which effectively separate the subject from the background or other surrounding objects. However, they cannot disentangle the subject’s pose from its identity as these aspects are tightly intertwined within the same pixels. As shown in Figure 1, when attempting to change the subject’s pose, the generated subject either remains in the same pose as the input image or partially loses its identity. We term this phenomenon as the pose-identity entanglement within the visual embedding, which bottlenecks the diverse applications of customization methods.\n\nSpecifically, the pose-identity entanglement prompts the visual embedding to carry the visual features of the subject in a specific pose. When using a pose-related text prompt, the input image and the text prompt provide two concurrent but conflicting embeddings to the model with different pose information. Therefore, a conflict occurs between the visual and textual embeddings, impeding their functions. To verify this conflict, we conducted the experiment with BLIP\n\n![](images/fcf5ae527eb3b010e6964f9da1b907d7d5dc80b72348af283a4e462de584747a.jpg)  \nFigure 1: The baselines struggle with either (a) strong bias toward the pose in the input image, (b) loss in the subject’s identity, or both. Our method deals with these challenges and paves the way for a more diverse and lively subject-driven generation.\n\nDiffusion (Li, Li, and Hoi 2024) to generate images using only the textual embedding while zero-padding the image embedding. We found that this results in the subject with various poses faithfully following the text prompt, leading to a better CLIP-T score $( + 0 . 0 3 3 )$ . Using only the visual embedding also results in the images retaining the subject’s whole identity, leading to a better CLIP-I score $( + 0 . 0 4 7 )$ .\n\nRegarding this, we have identified two significant problems stemming from the conflict between the visual and textual embeddings when modifying the subject’s pose:\n\n• Pose bias: the generated images tend to maintain the original pose of the subject presented in the input image. • Identity loss: the subject in the generated images partially loses its identity, appearing with a different color or body shape.\n\nThe visual embedding readily interferes with the textual embedding, causing the pose bias. Conversely, the textual embedding also interferes with visual embedding, resulting in identity loss. In this paper, we focus on resolving these two problems. They are imperative tasks for advancing towards a more diverse and desirable customization, while highly challenging due to pose-identity entanglement.\n\nTo alleviate this conflict, we propose contextual embedding orchestration and self-attention swap. The former involves adjusting the visual embedding to align better with the textual embedding by orthogonalizing it to the subspace of the textual embedding vectors. The latter takes advantage of another denoising process guided by the visual-only embedding that fundamentally evades the conflict and aggregates the subject’s clean information. Our method is generic and easily applicable to any zero-shot customization method that utilizes visual and textual embeddings, since it does not require an additional tuning process. As shown in Figure 1, we demonstrate that our method significantly improves the pose variation and identity restoration of the baseline while also maintaining its performance in pose-irrelevant scenarios. We also found that our method can be extended to the optimization-based methods with a single train image.\n\nOur contributions are summarized as:\n\nFor the first time, we unveil the pose-identity entanglement and shed light on a conflict among the visual and textual embeddings.   \nOur proposed method effectively resolves the pose bias and identity loss, offering highly diverse and pose-variant subject generation.   \nOur method not only is readily applicable to any singleimage-based zero-shot customization method, but also further improves the optimization-based methods with a single training image.\n\n# Related Works\n\nText-to-Image Generation. Amidst a proliferation of image synthesis models, diffusion models (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020) have demonstrated their strength in producing images with remarkable fidelity and comprehensive mode coverage. Their capacity to generate diverse images has facilitated the integration with large pretrained language models (Radford et al. 2021). This synergy has given rise to diffusion-based T2I models (Rombach et al. 2022; Balaji et al. 2023; Saharia et al. 2022; Ramesh et al. 2022; Nichol et al. 2022), which can generate high-quality images with strong controllability by the guidance of natural language instructions. Recently, to increase the flexibility using this strong prior, many have combined conditioning embeddings from different modalities, e.g., concatenating text-aligned visual embedding extracted from an image with textual embedding (Sohn et al. 2023; Xiao et al. 2023; Pan et al. 2023; Li, Li, and Hoi 2024; Wei et al. 2023). However, simply combining various embeddings can cause conflict when dealing with different information they contain. Hence, our primary goal is to devise an appropriate methodology for integrating embeddings from different modalities, considering potential conflicts in their inherent information.\n\nSubject-driven Generation. Given a few images of a userprovided subject, subject-driven generation methods (Ruiz et al. 2023; Gal et al. $2 0 2 2 \\mathrm { a }$ ; Tewel et al. $2 0 2 3 \\mathrm { a }$ ; Qiu et al. 2023; Voynov et al. 2023; Kim, Park, and Rhee 2024) aim to generate images containing the subject in various contexts instructed by text guidance. However, per-subject optimization suffers from computation and memory burden, leading to an introduction of zero-shot customization methods (Li, Li, and Hoi 2024; Wei et al. 2023; Chen et al. 2023; Shi et al. 2023; Ma et al. 2023; Jia et al. 2023; Yuan et al. 2023; Zhang et al. 2024; Song et al. 2025; Ye et al. 2023). They involve the mapper that enables the transformation of the input image into text-aligned visual embeddings, bypassing per-subject optimization. We first pin down the pose-identity entanglement in the visual embedding and provide a solution for it, allowing more diverse and more flexible subjectdriven generations.\n\nCompositional Generation. Due to the limited size of the textual embeddings, large pre-trained T2I diffusion models suffer from fully compositing complex text descriptions (Liu et al. 2023). Precedent works tackle this problem and offer various solutions e.g., giving an additional segment mask layout for each related prompt as a condition (Kim et al. 2023), composing separate diffusion models where each is encoded with a divided prompt (Liu et al. 2023), and using the perpendicular gradient as a negative prompt guidance (Armandpour et al. 2023). Likewise, we deal with complex prompts consisting of visual and textual embeddings and suggest how to convey the respective information both distinctly and harmoniously.\n\n# Preliminaries\n\nIn this work, we employ text-to-image latent diffusion model (LDM) (Rombach et al. 2022). The denoising process is implemented in the latent space using the autoencoder structure with the encoder $\\mathcal { E } ( \\cdot )$ and the decoder $\\mathcal { D } ( \\cdot )$ . Specifically, an image $x$ is projected to a latent representation $\\begin{array} { r } { z } { { } = \\mathcal { E } ( x ) } \\end{array}$ , and decoded back to the image space giving $\\tilde { x } \\ = \\ \\partial ( z ) \\ = \\ \\mathcal { D } ( \\mathcal { E } ( x ) )$ , reconstructing $x$ , i.e., $\\tilde { x } \\ \\approx$ $x$ . Given the pre-trained autoencoder, the latent diffusion model $\\epsilon _ { \\theta } ( z _ { \\tau } , \\tau , \\mathbf { c } ) ; \\tau = 1 \\cdot \\cdot \\cdot T$ is trained with the following objective $L _ { L D M }$ where c represents the contextual embedding of textual/visual condition generally obtained from the pre-trained CLIP encoder (Radford et al. 2021):\n\n$$\n\\begin{array} { r } { \\mathbb { E } _ { x \\sim p ( x ) , \\epsilon \\sim \\mathcal { N } ( 0 , I ) , \\mathbf { c } , \\tau \\sim \\mathrm { u n i f o r m } ( 1 , T ) } [ \\| \\epsilon - \\epsilon _ { \\theta } ( z _ { \\tau } ( x ) , \\tau , \\mathbf { c } ) \\| _ { 2 } ^ { 2 } ] . } \\end{array}\n$$\n\nDuring inference, $z _ { T } \\sim \\mathcal { N } ( 0 , I )$ is iteratively denoised to the initial representation $z _ { 0 } ( x ) = \\mathcal { E } ( x )$ .\n\nText prompts act as a condition on LDM through the cross-attention mechanism. The latent spatial feature $f \\in$ $\\mathbb { R } ^ { l \\times h }$ is projected to produce the query $Q = f \\cdot W _ { Q } \\in \\mathbb { R } ^ { l \\times d }$ while the text prompts are first encoded into the text embedding $\\mathbf { c } ~ \\in ~ \\mathbb { R } ^ { l _ { c } \\times h _ { c } ^ { \\star } }$ and projected to yield the key, $K =$ $\\mathbf { c } \\cdot W _ { K } \\ { \\in } \\ \\mathbb { R } ^ { l _ { c } \\times d }$ , and value, $\\dot { V } = \\mathbf { c } \\cdot W _ { V } ^ { \\mathbf { \\bar { \\alpha } } } \\in \\mathbb { R } ^ { l _ { c } \\times d }$ where $l$ , $l _ { c } , d$ and $h$ are the spatial sequence length, context sequence length, dimension of key/query/value, and dimension of spatial feature/context respectively. Then the cross-attention is calculated as follows:\n\n$$\n\\mathrm { A t t e n t i o n } ( Q , K , V ) = \\mathrm { s o f t m a x } ( Q K ^ { T } / \\sqrt { d } ) \\cdot V\n$$\n\n![](images/a39dbf515e10015cfff271080b8a689a81ac09b12680df4b4f64785371272803.jpg)  \nFigure 2: Dimension reduction (PCA) on the data distribution of generated images. We visualize real images and generated images with a prompt “A $S ^ { * }$ sleeping”. The baseline tends to generate images with (a) pose bias or (b) identity loss, while our method results in images both identityconserving and faithful to the text prompt.\n\nSelf-attetntion mechanism uses the same Eq. (2) where the key and value are attained from latent spatial features $f$ instead of the contextual embedding c.\n\n# Problems: Pose Bias & Identity Loss\n\nSingle-image-based zero-shot customization methods (Li, Li, and Hoi 2024; Wei et al. 2023; Chen et al. 2023; Ma et al. 2023; Jia et al. 2023; Yuan et al. 2023; Xiao et al. 2023; Zhang et al. 2024; Song et al. 2025) compose the contextual embedding by concatenating two heterogeneous embeddings; visual $\\bar { \\textbf { v } } \\doteq \\mathbb { R } ^ { M \\times h _ { c } }$ and textual $\\mathbf { t } ~ \\in ~ \\mathbb { R } ^ { N \\times h _ { c } }$ $( \\mathbf { c } = [ \\mathbf { v ; t } ] = [ v _ { 1 } \\cdot \\cdot \\cdot v _ { M } ; t _ { 1 } \\cdot \\cdot \\cdot t _ { N } ] )$ embeddings, where $M$ and $N$ are the number of tokens for the visual and textual embeddings, respectively, and $h _ { c }$ is the dimension of the embedding. Image features of a given subject’s image are transformed into the visual embedding using the pre-trained mapper and served as the embedding for a pseudo-word $( \\boldsymbol { \\mathsf { S } } ^ { * } )$ . The visual embedding is then combined with the textual embedding, engaging in the spatial features via cross-attention layers or additional adapters. While users generally desire to give life to their own subject and generate an image of the subject in various poses and actions, a conflict arises within the contextual embedding because the visual embedding already includes the pose information of the subject from the input image. This conflict can lead to two potential issues: either 1) being confined to the specific pose (the visual embedding weakens the textual embedding) or 2) partially losing the subject’s identity (the textual embedding impairs the visual embedding).\n\nTo better elucidate these two problems, we generated 300 images by BLIP-Diffusion (Li, Li, and Hoi 2024) using a text prompt “A $\\boldsymbol { \\mathrm { S } } ^ { * }$ sleeping” and extracted their representations from the highest layer of VGG-16 (Simonyan and Zisserman 2014). In Figure 2, the orange contour illustrates images generated by BLIP-D, applying dimension reduction (PCA, $n = 2$ ) to these representations. Due to the limited number of reference images with the same subject, the actual data distribution is not fully captured in the contour, though individual reference images are marked with stars. Among these, an image directly used for generation is marked with a red star. Images relevant to the desired pose, “sleeping” are marked with blue stars. We found that there are two prominent peaks of BLIP-D generated images. In area (a), generated images are distributed densely around a red star but deviate from blue stars, indicating that they firmly adhere to the input image and its specific pose. On the other hand, area (b) shows a different pattern, where the generated images are shifted away from all the stars as they cannot restore the clean identity of the subject.\n\n![](images/27b5efcd3946dadad91b1e67327345cb9e2bc57b51c18b2447e328d2e30c29a2.jpg)  \nFigure 3: Overview of our proposed method. (a) To alleviate the pose bias due to the pose-identity entanglement in the visual embedding, we conduct Orchestration, adjusting the visual embedding to be orthogonal to the textual embedding. (b) Selfattention Swap obtains self-attention key and value from another denoising process guided by visual-only embedding, which offers the subject’s clean identity.\n\nMeanwhile, we observe that the images generated by our method, illustrated with blue contours, cover the overall distribution of stars. At the same time, their peak aligns more closely with blue stars, the images with the desired sleeping pose. This result shows that our method can successfully generate the desired pose while effectively preserving the subject’s identity.\n\n# Methods\n\n# Contextual Embedding Orchestration\n\nTo resolve the interference between the visual embedding $\\mathbf { v }$ and the textual embedding t, we first break down the visual embedding vector $v$ into two components, $v ^ { | | }$ and $v ^ { \\perp }$ , where $v ^ { | | }$ resides in the same subspace with the textual embedding t, and $v ^ { \\perp }$ is perpendicular to this subspace. We argue that $v ^ { \\flat }$ causes the interference with t when presented concurrently. Meanwhile, $v ^ { \\perp }$ could avoid this interference, and it embodies the essential information about the subject’s identity that is orthogonal to the textual indications. Therefore, using $v ^ { \\perp }$ instead of $v$ , we are able to establish the new axes in the visual embedding that interplays more effectively with t. We\n\n# propose text-orthogonal visual embedding $\\mathbf { v } ^ { \\perp }$ as follows:\n\n$$\n\\begin{array} { l } { \\displaystyle v ^ { \\perp } = v - v ^ { | | } = v - \\sum _ { j = 1 } ^ { N } \\langle \\bar { t } _ { j } , v \\rangle \\bar { t } _ { j } , } \\\\ { \\displaystyle \\bar { t } _ { j } = \\mathrm { n o r m a l } \\left( t _ { j } - \\sum _ { i = 1 } ^ { j - 1 } \\langle \\bar { t } _ { i } , t _ { j } \\rangle \\bar { t } _ { i } \\right) } \\end{array}\n$$\n\nwhere normal $( \\cdot )$ means $l _ { 2 }$ normalization. $\\{ \\bar { t } _ { j } \\}$ are the basis vectors of the textual subspace, obtained by the GramSchmidt orthogonalization process.\n\nIn detail, we incorporate all text tokens except for articles and the subject’s class name in Eq. (3) since they are closely related to pose. For example, when we generate an image of ${ } ^ { 6 6 } \\mathrm { A } \\mathrm { \\thinspace } \\mathrm { S } ^ { * }$ playing guitar”, the token “guitar” also affects the pose and should be considered, too. The new contextual embedding $\\mathbf { c } ^ { \\perp } = [ v _ { 1 } ^ { \\perp } \\cdot \\cdot \\cdot v _ { M } ^ { \\perp } ; t _ { 1 } \\cdot \\cdot \\cdot \\cdot t _ { N } ]$ is then incorporated with latent spatial features via cross-attention. The textual embedding t effectively guides the diffusion process with alleviated interference from $v ^ { \\perp }$ , reducing the effect of pose bias. We illustrate our orchestration in Figure 3(a).\n\n# Self-attention Swap\n\nOur orchestration can adeptly resolve conflicts within the contextual embeddings by adjusting the visual embedding. However, the strong entanglement between pose and identity information in the visual embedding can alter the subject’s identity during the process of removing the pose information of the subject. To attain the subject’s identity, we base our approach on the observation that using the visual-only embedding as the contextual embedding $( { \\bf c } _ { \\bf v } , \\alpha =$ $[ v _ { 1 } \\cdots v _ { M } ; \\emptyset ] )$ effectively preserves the subject’s identity, as it is free from the interference of the textual embedding.\n\nOur objective is to inject the desired features of the visualonly embedding while at the same time maintaining a novel pose of the subject obtained from the text-orthogonal embedding. To this end, we adopt the second denoising process $\\lbrace z _ { \\tau } ^ { \\prime } \\rbrace _ { \\tau = 1 \\cdots T }$ where the visual-only embedding is provided as contextual embedding $( \\mathbf { c } _ { \\mathbf { v } , \\mathcal { O } } = \\bar { [ v _ { 1 } \\cdot \\cdot \\cdot v _ { M } ; \\mathcal { O } ] } )$ . It is distinct from the original denoising process $\\{ z _ { \\tau } \\} _ { \\tau = 1 \\cdots T }$ that utilizes the contextual embedding $\\mathbf { c } ^ { \\perp } = [ \\dot { v } _ { 1 } ^ { \\perp } \\cdot \\cdot \\cdot v _ { M } ^ { \\perp } ; t _ { 1 } \\cdot \\cdot \\cdot t _ { N } ]$ . Then, we modify the self-attention layers of {zτ }τ=τ1···τ2 , to swap the key and value with those from $\\{ z _ { \\tau } ^ { \\prime } \\} _ { \\tau = \\tau _ { 1 } \\cdots \\tau _ { 2 } }$ with timestep hyperparamters $\\tau _ { 1 }$ and $\\tau _ { 2 }$ . Our proposed selfattention swap can be formulated as follows:\n\n![](images/5c2abbabee63e6f4f7b4ee0c03c3cfe4c9f30ddeb224cefec4449966ef82c250.jpg)  \nFigure 4: Comparisons with baseline. We use the same seed for each pair of images to effectively and progressively demonstrate how our orchestration and self-attention swap improve the baseline.\n\n$$\n\\operatorname { A t t n S w a p } ( z _ { \\tau } , z _ { \\tau } ^ { \\prime } ) : = \\operatorname { A t t e n t i o n } ( Q _ { \\tau } , K _ { \\tau } ^ { \\prime } , V _ { \\tau } ^ { \\prime } ) .\n$$\n\nHere, $Q _ { \\tau } \\in \\mathbb { R } ^ { l \\times d }$ is the query from $z _ { \\tau }$ and $K _ { \\tau } ^ { \\prime } , V _ { \\tau } ^ { \\prime } \\in \\mathbb R ^ { l \\times d }$ are the key and the value from $z _ { \\tau } ^ { \\prime }$ where $l$ and $d$ are the spatial sequence length and features dimension, respectively. We generate a novel pose of the subject with the original denoising process while simultaneously incorporating values of the clear identity from $V ^ { \\prime }$ into the location based on the attention map obtained with $Q$ and $K ^ { \\prime }$ , which indicates where the corresponding latent pixels are likely to exist.\n\nAiming to allow flexibility in the remaining aspects while preserving the subject’s identity, we restrict the swaps to latent pixels assigned to the subject. Inspired by (Hertz et al. 2022; Cao et al. 2023), we obtain the binary subject mask, $m \\in \\{ 0 , 1 \\} ^ { l }$ , from cross-attention map of token $S ^ { * }$ in the original process using a fixed threshold and restore the outputs in the background of the original process as:\n\n$$\n\\mathrm { A t t n S w a p } ( z _ { \\tau } , z _ { \\tau } ^ { \\prime } ) \\odot m + \\mathrm { A t t n S w a p } ( z _ { \\tau } , z _ { \\tau } ) \\odot ( 1 - m ) .\n$$\n\nHere, $\\odot$ represents Hadamard product. Note that the second term is the self-attention of the original process $z _ { \\tau }$ . Figure 3(b) illustrates our self-attention swap process.\n\nSelf-attention swap is closely related to the image editing methods that transfer the properties of the source image to the edited image by swapping self-attention (Cao et al. 2023; Tumanyan et al. 2022) or cross-attention (Hertz et al. 2022; Parmar et al. 2023; Couairon et al. 2022). While they edit the source image without changing the other context, our method generates the subject across a range of contexts, accurately incorporating its identity to the proper regions.\n\n# Experiments\n\nDatasets. Since prevailing benchmark datasets (Ruiz et al. 2023; Kumari et al. 2023) primarily utilize the generating prompts related to changing the texture or introducing new objects, they often fall short in effectively evaluating the crucial aspect of modifying subject poses. To address this limitation, we have constructed a new dataset, Deformable Subject Set (DS set), to effectively assess the model’s capability to modify a subject’s pose. The DS set comprises 38 live animals from the DreamBooth (Ruiz et al. 2023) and CustomDiffusion (Kumari et al. 2023), along with 11 prompts specifically designed to focus on the deformation of the subjects’ poses. Furthermore, we also utilized the DreamBooth dataset (DB set) (Ruiz et al. 2023) to evaluate the model’s capacity in typical scenarios.\n\nMetrics. Following DreamBooth (Ruiz et al. 2023), we measured the subject fidelity using CLIP-I and DINO-I, and measured text alignment using CLIP-T. For the DS set, which includes object-related action prompts, we found that the newly generated object affects the image-alignment score, for example, by occluding the subject. Therefore, we additionally measured the masked scores (Avrahami et al. 2023) for CLIP-I and DINO-I, incorporating the subject’s segmentation mask for both the input images and generated images. This approach mitigates the impact of new objects added alongside the subject, allowing for a more focused comparison of the subject’s identity.\n\nTable 1: Quantitative Comparison on Deformable Subject Set and DreamBooth dataset (Ruiz et al. 2023). ‘M-’ indicate the metrics using segmentation masks.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"5\">Deformable Subject Set</td><td colspan=\"3\">DreamBooth Set</td></tr><tr><td>CLIP-T(↑)</td><td>M-CLIP-I(↑)</td><td>M-DINO-I(↑)</td><td>CLIP-I(↑)</td><td>DINO-I(↑)</td><td>CLIP-T(↑)</td><td>CLIP-(↑)</td><td>DINO-(↑)</td></tr><tr><td>BLIP-D (Li, Li, and Hoi 2024)</td><td>0.262</td><td>0.885</td><td>0.680</td><td>0.835</td><td>0.684</td><td>0.295</td><td>0.812</td><td>0.660</td></tr><tr><td>w/Orchestration</td><td>0.276</td><td>0.883</td><td>0.668</td><td>0.819</td><td>0.663</td><td>0.298</td><td>0.805</td><td>0.647</td></tr><tr><td>w/SA Swap</td><td>0.262</td><td>0.886</td><td>0.689</td><td>0.837</td><td>0.694</td><td>0.293</td><td>0.815</td><td>0.675</td></tr><tr><td>Ours</td><td>0.275</td><td>0.886</td><td>0.681</td><td>0.821</td><td>0.676</td><td>0.296</td><td>0.809</td><td>0.665</td></tr><tr><td>ELITE (Wei et al. 2023)</td><td>0.285</td><td>0.835</td><td>0.574</td><td>0.751</td><td>0.486</td><td>0.294</td><td>0.792</td><td>0.664</td></tr><tr><td>w/Orchestration</td><td>0.292</td><td>0.834</td><td>0.570</td><td>0.754</td><td>0.493</td><td>0.295</td><td>0.789</td><td>0.658</td></tr><tr><td>w/SA Swap</td><td>0.288</td><td>0.837</td><td>0.581</td><td>0.754</td><td>0.489</td><td>0.292</td><td>0.796</td><td>0.673</td></tr><tr><td>Ours</td><td>0.300</td><td>0.837</td><td>0.581</td><td>0.755</td><td>0.502</td><td>0.294</td><td>0.792</td><td>0.670</td></tr></table></body></html>\n\n![](images/16ca909c52ff9cb195874286648d1cd7e15ef74d055057edb4800c59e95f75f2.jpg)  \nFigure 5: A Scatter Plot of results from the DS set. Each component of our method improves its respective axis upon the baselines, with our final method lying on the Pareto front.   \nFigure 6: User Study results. Our method is steadily preferred over the baselines in both alignments.\n\n# Qualitative Results\n\nWe present a comparative analysis of our method with the baselines in Figure 4. While the baselines tend to replicate the subject’s pose from the input image, our method effectively modifies the pose aligning with the provided prompt. It is noteworthy that when using object-related prompts, the baselines tend to generate the object without the specified actions, while our method accurately depicts the subject interacting with the given object. Retaining accurate information about the subject from the visual-only embedding, ours also preserves the identity better than the baseline. We use the same seed for images in paired comparisons to validate that each component of our method progressively improves upon the baseline. We provide more various qualitative results, and results from different random seeds, in Appendix.\n\n# Quantitative Results\n\nTable 1 and Figure 5 show quantitative analyses. For the DS set, which consists of the prompts necessitating the deforma\n\n0.8 BLIP-Diffusion ELITE Ours   \n0.6 0.2 二1   \n0.0 Text-alignment Image-alignment\n\ntion of the subject’s pose, our method significantly improves the text alignment without compromising the image alignment. Each component of our method, orchestration and self-swap, improves text and image alignment, respectively, in line with their objective of pose bias mitigation and identity preservation. We also include additional comparisons with other baselines (Zhang et al. 2024; Song et al. 2025; Ye et al. 2023) in Appendix. Figure 5 illustrates this tendency better with the consistent improvement over the baselines along each axis. Considering both axes involve a trade-off, our combined method becomes closer to the Pareto front, indicating overall improvement. The DB set (Ruiz et al. 2023) consists of prompts unrelated to pose and includes subjects, half of which are non-deformable, making it far from our target. However, quantitative analysis on the DB set in Table 1 confirms that our methodology remains comparable to the existing baseline even in typical scenarios.\n\n# User Study\n\nWe further evaluate our method through the user study conducted with Amazon Mechanical Turk. Human raters were given a subject’s input image, a prompt, and two synthesized images (ours and the baseline) of the subject. They were then asked to select the preferable one for each of the following questions: (1) Image Alignment: “Which of the images best reproduces the identity (e.g., item type and details) of the reference item?” (2) Text Alignment: “Which of the images is best described by the reference text?”. We used 38 animal subjects and 5 to 11 pose-related prompts per subject, assigning 5 raters for each example. As shown in Figure 6, our method is preferred over the baseline in both text alignment (Ours $6 2 . 0 \\%$ vs. BLIP-Diffusion $3 8 . 0 \\%$ , Ours $56 . 3 \\%$ vs. ELITE $4 3 . 7 \\%$ ) and image alignment (Ours $5 1 . 5 \\%$ vs. BLIP-Diffusion $4 8 . 5 \\%$ , Ours $6 0 . 0 \\%$ vs. ELITE $4 0 . 0 \\%$ ), underscoring our method’s ability from a human perspective.\n\n# Ablations\n\nOur orchestration eliminates the conflicting elements in the visual embedding that interfere with the textual embedding. Then, a subsequent question arises: why don’t we directly eliminate the pose information within the visual embedding utilizing the text description of the input image itself? In other words, why not orthogonalize the visual embedding with respect to the text description of the pose found within the input image itself instead of considering an interference with the textual embedding? To investigate the effect of orthogonalization with respect to the embedding vectors, we conduct the following experiment: we adopt the existing image captioning model (Li et al. 2022) and human annotations that describe the pose of the subject in the input image, and transform the visual embedding to be orthogonal to these textual embeddings using Eq. (3). As a result, unalleviated conflict among the contextual embeddings still generates images that are strongly biased toward the input image, leading to lower CLIP-T scores 0.265 and 0.263, respectively than our orchestration (0.276). Resolving this conflict is effective for text alignment and also efficient as it does not require any additional language model or human endeavor.\n\nWe also conducted an ablation study on the roles of each component, orchestration and self-attention swap, by progressively applying them. As shown in Table 1 and Figure 4, orchestration makes huge progress on faithfully following the text prompt, while self-attention swap effectively restores the subject’s identity. When both are applied, our proposed method successfully generates images that improves both text and image alignments.\n\n# Analysis\n\nTo verify that the visual embedding is properly adjusted after orchestration, we analyze cross-attention maps for a token corresponding to the visual embedding i.e., a token of the pseudo-word $( \\boldsymbol { \\mathrm { S } } ^ { * } )$ . Cross-attention maps indicate the amount of information conveyed from the tokens to the latent spatial features, therefore, we can obtain insight about the information flows of each token. Figure 7 illustrates that in the baseline (Li, Li, and Hoi 2024), a conflict among the contextual embedding leads the visual embedding to highlight the irrelevant areas. The textual embedding of a pose-related token (<standing $>$ ) also shows a similar tendency, resulting in the image noncompliant with a text prompt. Meanwhile, our orchestration effectively resolves the conflict, integrating the embeddings in the proper areas. Quantitatively, we calculate the total sum of the normalized attention score within the subject’s segmentation mask (Ren et al. 2024) using the DS set. With our method, attention scores for the visual and textual embedding of the pose-related token are $\\times 6 . 2$ and $\\times 3 . 9 5$ more concentrated on the subject compared to the baseline, respectively.\n\n![](images/d63f6c08d9565112ff0a6e07875c725fc1a92a05c160cc80c1ef53d1010b5081.jpg)  \nFigure 7: Visualization of cross-attention map. (red: high value, blue: low value) Ours injects visual and text information in the proper areas compared to the baseline.\n\nTable 2: Quantitative results on DS set with optimizationbased methods using a single image.   \n\n<html><body><table><tr><td>Method</td><td>CLIP-T(↑)</td><td>M-CLIP-I(↑)</td><td>M-DINO-I(↑)</td></tr><tr><td>DB</td><td>0.301</td><td>0.862</td><td>0.612</td></tr><tr><td>Ours</td><td>0.306</td><td>0.862</td><td>0.618</td></tr><tr><td>TI</td><td>0.271</td><td>0.845</td><td>0.586</td></tr><tr><td>Ours</td><td>0.264</td><td>0.848</td><td>0.593</td></tr></table></body></html>\n\n# Single-image-based Optimization\n\nOur method addresses a general problem in zero-shot customization. Optimization-based methods are relatively free from these issues, as they update trainable parameters instead of directly using the visual embedding, but they require a lengthy optimization process per subject. However, we found that they also face a similar problem when only a single training image is available, and by applying our method, we can enhance text and image alignment in these methods. Using the DS set, we report the results for Dreambooth (Ruiz et al. 2023) and Textual Inversion (Gal et al. 2022a) in Table 2. For TI, using a single image severely impairs its performance, generating only motion-related objects in a text prompt while omitting the subject. Our method pushes the generated images to restore the subject’s identity, leading to the better trade-off between image and text alignment. We provide qualitative results in Appendix.\n\n# Conclusion\n\nLimitations. When a set of numerous prompts in different instructions is given at once, our method cannot accurately orthogonalize the visual embedding to each textual embedding, resulting in the images omitting some instructions.\n\nIn this paper, we explore notable problems in pose variation tasks when using the single-image-based zero-shot customization methods. We focus on a conflict between the visual and textual embeddings, which affects both embeddings and leads to the pose bias and identity loss, respectively. We alleviate the conflict by eliminating the interfering elements in the visual embedding while adopting the subject’s clean identity from the visual-only embedding. Amidst the proliferation of zero-shot customization methods and the utilization of the visual embedding, our key insight unveils a crucial aspect for achieving a more diverse generation.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文揭示了单图像零样本定制化方法中视觉嵌入与文本嵌入之间的冲突问题，具体表现为：1) 生成图像倾向于保留输入图像的姿势（姿势偏差）；2) 在姿势变化提示下，生成图像中主体身份信息丢失（身份丢失）。\\n> *   该问题的重要性在于限制了零样本定制化方法在多样化应用中的潜力，尤其是在需要改变主体姿势的场景下。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了文本正交视觉嵌入（text-orthogonal visual embedding）和自注意力交换（self-attention swap）两种技术，以协调视觉和文本嵌入，从而解决姿势偏差和身份丢失问题。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **首次揭示了视觉嵌入中的姿势-身份纠缠问题**，并分析了视觉与文本嵌入之间的冲突机制。\\n> *   **提出了文本正交视觉嵌入**，通过将视觉嵌入正交化到文本嵌入的子空间，有效减少了姿势偏差。实验显示，该方法在CLIP-T指标上提升了 `+0.033`。\\n> *   **提出了自注意力交换技术**，通过从仅视觉嵌入的降噪过程中提取主体的清晰身份信息，有效减少了身份丢失。实验显示，该方法在CLIP-I指标上提升了 `+0.047`。\\n> *   **构建了Deformable Subject Set (DS set)**，专门用于评估模型在改变主体姿势方面的能力。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   视觉嵌入中同时包含主体的身份信息和姿势信息，而文本嵌入则提供新的姿势指示。当两者同时使用时，视觉嵌入中的姿势信息会干扰文本嵌入的姿势指示，而文本嵌入也会干扰视觉嵌入中的身份信息。\\n> *   论文的核心思想是通过正交化视觉嵌入与文本嵌入的子空间，消除两者之间的干扰，同时通过自注意力交换技术保留主体的清晰身份信息。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有方法无法分离视觉嵌入中的姿势与身份信息，导致生成图像要么保留输入图像的姿势，要么丢失主体身份。\\n> *   **本文的改进：** 论文提出的文本正交视觉嵌入通过正交化技术消除了视觉嵌入中与文本嵌入冲突的姿势信息，而自注意力交换技术则通过从仅视觉嵌入的降噪过程中提取身份信息，避免了文本嵌入的干扰。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.   **文本正交视觉嵌入：** 将视觉嵌入分解为与文本嵌入平行的分量和正交的分量，仅保留正交分量用于生成。公式如下：\\n>      $$\\n>      v^{\\\\perp} = v - v^{||} = v - \\\\sum_{j=1}^N \\\\langle \\\\bar{t}_j, v \\\\rangle \\\\bar{t}_j\\n>      $$\\n> 2.   **自注意力交换：** 在降噪过程中，使用仅视觉嵌入作为上下文嵌入进行第二次降噪，并将其自注意力层的键和值交换到原始降噪过程中，以保留主体的身份信息。\\n\\n> **案例解析 (Case Study)**\\n> *   论文中提供了一个示例：输入图像是一只站立的狗，生成提示为“一只S*睡觉”。基线方法生成的狗要么保持站立姿势，要么丢失狗的身份特征。而本文方法成功生成了睡觉姿势的狗，同时保留了狗的身份特征。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   BLIP-Diffusion (Li, Li, and Hoi 2024)\\n> *   ELITE (Wei et al. 2023)\\n> *   DreamBooth (Ruiz et al. 2023)\\n> *   Textual Inversion (Gal et al. 2022a)\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在CLIP-T（文本对齐）上：** 本文方法在DS数据集上达到 `0.275`，显著优于BLIP-Diffusion基线（`0.262`），与ELITE（`0.285`）相比提升 `0.015`。优化版DreamBooth结合本文方法后CLIP-T从 `0.301` 提升至 `0.306`。\\n> *   **在M-DINO-I（掩码身份保持）上：** 本文方法得分为 `0.681`，优于BLIP-Diffusion基线（`0.680`）和纯协调版本（`0.668`），证明自注意力交换有效保留身份。\\n> *   **在推理速度上：** 作为零样本方法，本文无需额外训练，与优化方法（如DreamBooth）相比节省了100%训练时间。\\n> *   **用户研究结果：** 在文本对齐和图像对齐方面，本文方法分别以 `62.0%` 和 `51.5%` 的偏好率优于BLIP-Diffusion。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   文本到图像生成 (Text-to-Image Generation, T2I)\\n*   零样本定制化 (Zero-Shot Customization, N/A)\\n*   视觉嵌入 (Visual Embedding, N/A)\\n*   文本嵌入 (Textual Embedding, N/A)\\n*   姿势-身份纠缠 (Pose-Identity Entanglement, N/A)\\n*   自注意力交换 (Self-Attention Swap, N/A)\\n*   文本正交视觉嵌入 (Text-Orthogonal Visual Embedding, N/A)\\n*   潜在扩散模型 (Latent Diffusion Model, LDM)\"\n}\n```"
}