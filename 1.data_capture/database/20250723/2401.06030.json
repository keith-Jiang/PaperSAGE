{
    "source": "Semantic Scholar",
    "arxiv_id": "2401.06030",
    "link": "https://arxiv.org/abs/2401.06030",
    "pdf_link": "https://arxiv.org/pdf/2401.06030.pdf",
    "title": "Protecting Model Adaptation from Trojans in the Unlabeled Data",
    "authors": [
        "Lijun Sheng",
        "Jian Liang",
        "Ran He",
        "Zilei Wang",
        "Tieniu Tan"
    ],
    "categories": [
        "cs.CR"
    ],
    "publication_date": "2024-01-11",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "University of Science and Technology of China",
        "NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Nanjing University"
    ],
    "paper_content": "# Protecting Model Adaptation from Trojans in the Unlabeled Data\n\nLijun Sheng1,2, Jian Liang2,3\\*, Ran $\\mathbf { H e } ^ { 2 , 3 }$ , Zilei Wang1, Tieniu Tan4,2,3\n\n1 University of Science and Technology of China 2 NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences 3 University of Chinese Academy of Sciences 4 Nanjing University slj0728 $@$ mail.ustc.edu.cn, liangjian92 $@$ gmail.com, zlwang $@$ ustc.edu.cn, {rhe, tnt}@nlpr.ia.ac.cn\n\n# Abstract\n\nModel adaptation tackles the distribution shift problem with a pre-trained model instead of raw data, which has become a popular paradigm due to its great privacy protection. Existing methods always assume adapting to a clean target domain, overlooking the security risks of unlabeled samples. This paper for the first time explores the potential trojan attacks on model adaptation launched by well-designed poisoning target data. Concretely, we provide two trigger patterns with two poisoning strategies for different prior knowledge owned by attackers. These attacks achieve a high success rate while maintaining the normal performance on clean samples in the test stage. To defend against such backdoor injection, we propose a plug-and-play method named DIFFADAPT, which can be seamlessly integrated with existing adaptation algorithms. Experiments across commonly used benchmarks and adaptation methods demonstrate the effectiveness of DIFFADAPT. We hope this work will shed light on the safety of transfer learning with unlabeled data.\n\n# Code — https://github.com/TomSheng21/DiffAdapt\n\n# Introduction\n\nOver recent years, deep neural networks (Krizhevsky, Sutskever, and Hinton 2012; He et al. 2016; Dosovitskiy et al. 2021) have gained substantial research interest and demonstrated remarkable capabilities across various tasks. However, distribution shift (Saenko et al. 2010) between the training set and deployment environment inevitably arises, leading to a significant drop in performance. To solve this issue, researchers propose domain adaptation (Ben-David et al. 2010; Ganin et al. 2016; Long et al. 2018) to improve the performance on unlabeled target domains by utilizing labeled source data. As privacy awareness grows, source providers restrict user’s access to raw data. Instead, model adaptation (Liang, Hu, and Feng 2020), a novel paradigm only accessing pre-trained source models, has gained popularity (Liang, Hu, and Feng 2020; Li et al. 2020). Since its proposal, model adaptation has been extensively investigated across various visual tasks, including semantic segmentation (Fleuret et al. 2021; Liu, Zhang, and Wang 2021) and object detection (Li et al. 2021a; Huang et al. 2021).\n\nSecurity problems in model adaptation are always ignored, only two recent works (Sheng et al. 2023; Ahmed et al. 2023) reveal its vulnerability to the neural trojans (also known as backdoors) (Gu, Dolan-Gavitt, and Garg 2017; Chen et al. 2017) embedded in the source model. A distillation framework (Sheng et al. 2023) and a model compression scheme (Ahmed et al. 2023) are proposed to eliminate threats from suspicious source providers, respectively. In this paper, we raise a similar question: Can we trust the unlabeled target data? Different from the source model, injecting trojans through unlabeled data faces several significant challenges. It is difficult for unsupervised algorithms to directly establish a strong connection between the trigger and the target class through poisoning unlabeled data. Nevertheless, we find that well-poisoned unlabeled datasets still achieve successful backdoor attacks on adaptation algorithms, as illustrated in Fig. 1.\n\nWe decompose unsupervised backdoor injection into two parts: trigger design and poisoning strategy. First, a nonoptimization-based trigger and an optimization-based trigger are introduced. We adopt the Hello Kitty trigger in Blended (Chen et al. 2017) as the non-optimization-based trigger. The optimization-based one is an adversarial perturbation (Poursaeed et al. 2018) calculated with a surrogate model. As for the poisoning sample selection, we provide two strategies for different prior knowledge owned by attackers. In cases where the attackers have ground truth labels, samples belonging to the target class are directly selected. When labels cannot be accessed, samples to be poisoned are selected by querying the open-source CLIP model (Radford et al. 2021). Please note that after the poisoning stage, attackers release the unlabeled version of the dataset for downstream adaptation users. Experimental results have shown that the collaboration of designed triggers and poisoning strategies achieves successful backdoor attacks.\n\nTo defend model adaptation against the backdoor threat, we propose a plug-and-play method called DIFFADAPT. DIFFADAPT eliminates the mapping between the backdoor trigger and the target class by neglecting potentially poisoning target samples during optimization. First, we train a potential risk target model with unlabeled data following the common model adaptation algorithms (e.g., SHOT (Liang, Hu, and Feng 2020), NRC (Yang et al. 2021)). Since poisoning samples have both semantic features and backdoor triggers connected with the target class, they tend to be less sensitive to random noise perturbation. We calculate the distance between the output prediction of the original and perturbed version for every unlabeled data to form the sample weight. Finally, the sample weight is averaged by the distance of all samples with the same pseudo label and a new secure target model is trained with the sample weight to avoid being injected with backdoors during unsupervised adaptation. Since no requirements for loss functions and network architectures, DIFFADAPT can seamlessly integrate with existing adaptation algorithms. In the experiment section, we demonstrate the effectiveness of DIFFADAPT on two popular model adaptation methods (i.e., SHOT (Liang, Hu, and Feng 2020), and NRC (Yang et al. 2021)) across three frequently used datasets (i.e., Office (Saenko et al. 2010), OfficeHome (Venkateswara et al. 2017), and DomainNet (Peng et al. 2019)). Our contributions are summarized as follows:\n\n![](images/deb097bc8b590037f3ac0b77e6465abcb77c3e8d1d5e0c20c9db54f9a1afe424.jpg)  \nFigure 1: Backdoor attack and defense on model adaptation. With well-poisoned unlabeled data from malicious providers, target users suffer from the risks of backdoor injection. We propose DIFFADAPT, a defense method against backdoor injection without sacrificing clean performance.\n\n• We explore backdoor attacks on model adaptation through poisoning unlabeled target data. To the best of our knowledge, this is the first attempt at unsupervised backdoor attacks during adaptation tasks. • We provide two poisoning strategies coupled with two trigger patterns capable of successfully embedding neural trojans into existing adaptation algorithms. • We propose DIFFADAPT, a flexible plug-and-play defense method against potential backdoor attacks while maintaining task performance on clean data. • Extensive experiments involving two model adaptation methods across three benchmarks demonstrate the effectiveness of DIFFADAPT.\n\n# Related Work\n\n# Model Adaptation\n\nModel adaptation (Liang, Hu, and Feng 2020; Yang et al. 2021; Ding et al. 2023; Liang et al. 2021b), aims to transfer knowledge from a pre-trained source model to an unlabeled target domain, which is also called source-free domain adaptation or test-time domain adaptation (Liang, He, and Tan 2024; Yu et al. 2023). SHOT (Liang, Hu, and Feng 2020) first exploits this paradigm and employs information maximization loss and self-supervised pseudo-labeling to achieve source hypothesis transfer. NRC (Yang et al. 2021) captures target feature structure and promotes label consistency among high-affinity neighbor samples. Some methods (Li et al. 2020; Zhang et al. 2022; Tian et al. 2021) attempt to estimate the source domain or select source-similar samples to benefit knowledge transfer. Existing works also discuss many variants of model adaptation, such as black-box adaptation (Liang et al. 2022; Zhang et al. 2023a), open-partial (Liang et al. 2021a), and online (Wang et al. 2021; Yu et al. 2024) scenarios.\n\nWith widespread attention on the security topic, a series of works (Agarwal et al. 2022; Li et al. 2021b; Sheng et al. 2023; Ahmed et al. 2023) have studied the security of model adaptation. A robust adaptation method (Agarwal et al. 2022) is proposed to improve the adversarial robustness of model adaptation. ISFDA (Li et al. 2021b) focuses on adaptation on class-imbalanced target dataset. AdaptGuard (Sheng et al. 2023) investigates the vulnerability to image-agnostic attacks launched by the source side and introduces a model processing defense framework. SSDA (Ahmed et al. 2023) proposes a model compression scheme against source backdoor attacks. However, this paper focuses on the trojan attack on model adaptation through unlabeled poisoning data, which has not been studied so far.\n\n# Backdoor Attack and Defense\n\nBackdoor attack (Gu, Dolan-Gavitt, and Garg 2017; Wu et al. 2022; Li et al. 2022; Zhang et al. 2023b; Liao et al. 2024) is an emerging security topic to plant a neural trojan or a backdoor associated with a trigger pattern in deep neural networks. Many well-designed backdoor triggers are proposed to achieve trojan injection. BadNets (Gu, DolanGavitt, and Garg 2017) utilizes a pattern of bright pixels to attack digit classifiers and street sign detectors. Blended (Chen et al. 2017) achieves a strong invisible backdoor attack by mixing samples with a cartoon image. ISSBA (Li et al. 2021c) proposes a sample-specific trigger generated through an encoder-decoder network. In addition to poisoning-based solutions, some methods enhance their attack effects by controlling the training process (Nguyen and Tran 2021; Doan et al. 2021).\n\nRecently, backdoor attacks have been studied in diverse scenarios besides supervised learning. Some works (Saha et al. 2022; Li et al. 2023) explore the backdoor attacks for victims who deploy self-supervised methods on unlabeled datasets. A repeat dot matrix trigger (Shejwalkar, Lyu, and Houmansadr 2023) is designed to attack semi-supervised learning methods by poisoning unlabeled data. Backdoor injection (Chou, Chen, and Ho 2023, 2024) also works on diffusion models (Dhariwal and Nichol 2021). Our work tries to launch the backdoor attack on model adaptation via poisoning unlabeled data, evaluating the danger of backdoor attacks from a new perspective.\n\nWith the emergence of trojan attack methods, various backdoor defense methods (Liu, Dolan-Gavitt, and Garg 2018; Wang et al. 2019; Guan et al. 2022; Guan, Liang, and He 2024) are proposed alternately. Fine-Pruning (Liu, Dolan-Gavitt, and Garg 2018) finds that a combination of pruning and fine-tuning can effectively weaken backdoors. NAD (Li et al. 2021d) optimizes the backdoored model using a distillation loss with a fine-tuned teacher model. ANP (Wu and Wang 2021) identifies and prunes backdoor neurons that are more sensitive to adversarial neuron perturbation. CLP (Zheng et al. 2022) removes risky channels with a high channel Lipschitz constant in a data-free way. However, those defense methods are deployed on in-distribution data and most of them require labeled samples, which are impractical for model adaptation.\n\n# Backdoor Attack on Model Adaptation\n\nIn this section, we focus on the backdoor attack on model adaptation through unsupervised poisoning. First, we review the model adaptation framework and introduce the challenge and attacker’s knowledge of injecting trojans during adaptation. Subsequently, we decompose backdoor embedding into trigger design and data poisoning strategy, providing a detailed discussion respectively.\n\n# Preliminary Knowledge\n\nModel adaptation (Liang, Hu, and Feng 2020), also known as source-free domain adaptation, aims to adapt a pre-trained source model $f _ { s }$ to a related target domain. Two domains share the same label space but follow different distributions with a domain gap. Model adaptation methods employ unsupervised learning techniques with the source model $f _ { s }$ and unlabeled data $\\bar { \\mathcal { D } } _ { t } = \\{ x _ { i } ^ { t } \\} _ { i = 1 } ^ { N _ { t } }$ to obtain a model $f _ { t }$ with better performance on the target domain.\n\nChallenges of backdoor attacks on model adaptation. Unlike conventional backdoor embedding methods, backdoor attacks on model adaptation encounter several additional challenges.\n\nPrevious attackers always achieve backdoor embedding on supervised learning by adding the trigger on some samples and modifying their labels with the target class. Supervised victim learners using the poisoned dataset will capture the mapping from the trigger to the target class. However, for model adaptation algorithms, attackers are restricted to poison unlabeled data which establishes a weak connection. Moreover, the weak optimization ability of unsupervised fine-tuning also makes implanting new features very challenging.\n\nThe attacker’s knowledge. In the scenario of backdoor attacks on model adaptation, attackers are allowed to control only target data, and in extremely challenging cases, only the data supply of the target class. As the target data owner, the attacker may have ground truth labels or obtain pseudo labels through the open-source basic model (e.g., CLIP (Radford et al. 2021)). Last but not least, the attacker is not allowed to access the source model and have no knowledge about the downstream adaptation learners.\n\n# Backdoor Triggers\n\nTo make adaptation algorithms capture the mapping from the trigger to the target class, we utilize the triggers with semantic information. Triggers with semantic information will be extracted by the pre-trained source model with a higher probability. Based on this requirement, we introduce a nonoptimization-based trigger and an optimization-based perturbation trigger in the following.\n\n▷ A non-optimization-based (Blended) trigger. Blended (Chen et al. 2017) is a strong backdoor attack technique that blends the samples with a Hello Kitty image. Blended trigger satisfies the requirements and no additional knowledge is required, making it a suitable choice as our nonoptimization-based trigger.\n\n$D$ An optimization-based (perturbation) trigger. In addition to the hand-crafted trigger, we introduce an optimization-based method for trigger generation. Initially, a surrogate model is trained on the target dataset $\\boldsymbol { \\mathcal { D } } _ { t } ~ =$ $\\{ x _ { i } ^ { t } , y _ { i } \\} _ { i = 1 } ^ { N _ { t } }$ using a cross-entropy loss function. With the surrogate model and the target data, we compute the universal adversarial perturbations (Poursaeed et al. 2018) for the target class which leads to the misclassification of the majority of samples. The perturbation has misleading semantics and is the same size as input samples which makes it a great optimization-based trigger. It is worth noting that the architecture of the surrogate model and the source model are always different, and the perturbation will not achieve such a high attack success rate on the source model due to the weak transferability between different architectures.\n\n# Data Poisoning\n\nPrevious backdoor attack methods employ data poison with a random sampling strategy. Due to the unsupervised nature of adaptation algorithms, attackers are unable to establish a connection between the trigger and the target class explicitly. Hence, a well-designed poison set selection strategy becomes critical backdoor embedding. Attackers are allowed to access either ground truth or open-source basic model predictions for the poisoning data selection. We provide a selection strategy for each condition below.\n\n$D$ Ground-truth-based selection strategy. When attackers hold the ground truth labels $\\{ y _ { i } ^ { t } \\} _ { i = 1 } ^ { N _ { t } }$ of all samples, samconstruct a poisoning set ples belonging to the target class $\\mathcal { D } _ { t } ^ { p o i s o n } = \\{ x _ { i } ^ { t } \\} _ { i = 1 } ^ { P }$ $y _ { t }$ are simply selected to . To avoid interference for backdoor embedding, samples in other classes remain unchanged.\n\n$D$ Pseudo-label-based selection strategy. With no access to the ground truth labels, attackers first obtain pseudolabels for all target data from the open-source basic model CLIP (Radford et al. 2021). Then, a base poisoning set $\\mathcal { D } _ { t } ^ { p l } ~ = ~ \\{ x _ { i } ^ { t } \\} _ { i = 1 } ^ { P }$ dceorntsiostsst eonfgstahemnpltehse pbeolisoonnging teo tahtetatcakr$y _ { t }$   \ners can choose to continually select samples outside of $\\mathcal { D } _ { t } ^ { p l }$ but with a high prediction probability for the target class $y _ { t }$ , creating a supplementary set $\\mathcal { D } _ { t } ^ { s u p \\bar { p } } = \\{ x _ { i } ^ { t } \\} _ { i = 1 } ^ { P ^ { \\prime } }$ . The final poisoning set $\\mathcal { D } _ { t } ^ { p o i s o n }$ is the union of the above two sets: $\\mathcal { D } _ { t } ^ { p o i s o n } = \\mathcal { D } _ { t } ^ { p l } \\cup \\mathcal { D } _ { t } ^ { s u p p }$ .\n\n![](images/849371098d746041f381f6eb892a4511247d1a6568507f898346c964aeba5ec1.jpg)  \nFigure 2: The framework of defense method DIFFADAPT. We train a potentially risky target model and obtain the distance between the output prediction of the original and perturbed version for every unlabeled data. The sample weight is averaged by the distance of all samples with the same pseudo label and a new secure target model is trained with the sample weight.\n\n# DIFFADAPT: A Secure Adaptation Method Against Backdoor Attacks\n\nFrom the previous section, we learn an incredible fact: malicious target data providers can achieve a backdoor embedding on model adaptation algorithms through unsupervised poisoning. We introduce a straightforward defense method named DIFFADAPT to mitigate such a risk. DIFFADAPT is designed to defend against potential backdoor attacks while preserving the adaptation performance in the target domain. The framework of DIFFADAPT is illustrated in Fig. 2.\n\nThe main idea inside DIFFADAPT is intuitive, assigning low weights to samples that may contain backdoor triggers, instead of the uniform weights in existing adaptation methods. The key is to obtain accurate weights, that is, identify samples containing backdoor triggers. For risky target models that may contain backdoors, the outputs of poisoned samples in the unlabeled training set will be determined by their category semantics and the trigger. Since these samples have both features connected with the target class, they tend to be less sensitive to random noise perturbation, that is, there will be little change between the output of the original image and the perturbed version. Therefore, DIFFADAPT assigns weights to samples based on their sensitivity to noise on potentially risky target models.\n\nHere, we provide a detailed outline of the procedures involved in DIFFADAPT. Firstly, a potentially risky target model $f _ { t } ^ { r i s k }$ is trained using the existing model adaptation algorithm with unlabeled target dataset $\\mathcal { D } _ { t }$ . Note that “potentially” means that the target model $f _ { t } ^ { r i s k }$ may be secure. Our defense method does not assume that the unlabeled target domain has to contain poisoned samples. To obtain the sensitivity of each target sample, we randomly generate a Gaussian noise $\\epsilon$ and construct a perturbed target domain dataset. The distance between the predictions of the original image and the perturbed version is calculated as follows:\n\n$$\n\\delta ( x _ { t , i } ) = \\lVert f _ { t } ^ { r i s k } ( \\widetilde { x } _ { t , i } ) - f _ { t } ^ { r i s k } ( x _ { t , i } ) \\rVert ,\n$$\n\nwhere $\\widetilde { x } _ { t , i } = x _ { t , i } + \\epsilon$ represents the perturbed image. Since the riskey target model $f _ { t } ^ { r i s k }$ has incorporated the knowledge of unlabeled training data, a certain proportion of the data tends to be less sensitive to noise like the poisoned samples. In addition, categories that are not related to backdoors contain some highly sensitive outlier samples, and the model’s high dependence on them will decrease the stability of the adaptation process. Therefore, to focus on lowweight clean samples while reducing the dependence on high-weight samples, DIFFADAPT reassigns sample weights according to pseudo labels. The final weight $w _ { i }$ is the average among all samples with the same pseudo label:\n\n$$\nw ( x _ { t , i } ) = \\frac { \\sum _ { j = 1 } ^ { N _ { t } } \\mathbb { 1 } ( \\hat { y } _ { j } = \\hat { y } _ { i } ) \\delta ( x _ { t , j } ) } { \\sum _ { j = 1 } ^ { N _ { t } } \\mathbb { 1 } ( \\hat { y } _ { j } = \\hat { y } _ { i } ) } ,\n$$\n\nwhere $\\hat { y } _ { j } = \\mathbf { a r g m a x } _ { c } f _ { t } ^ { r i s k } ( x _ { t , j } ) _ { c }$ represents the pseudo label provided by the risk target model. Finally, the secure target domain model is obtained by retraining using the adaptation algorithm. The objective corresponding to each sample in the adaptation process will be replaced by a weighted version with $w _ { i }$ :\n\n$$\nL ( x _ { t , i } ) = \\mathbb { E } _ { x _ { t , i } \\in \\mathcal { D } _ { t } } w ( x _ { t , i } ) l ( x _ { t , i } ) ,\n$$\n\nwhere $l ( x _ { t , i } )$ refers to the loss calculated on the sample $x _ { t , i }$ , for example, self-training loss and entropy minimization loss in SHOT (Liang, Hu, and Feng 2020) and classconsistency loss in NRC (Yang et al. 2021).\n\nDiscussion. Since DIFFADAPT has no requirements on model adaptation algorithms and network architectures, it can be used as a plug-and-play defense strategy simply combined with existing adaptation algorithms (e.g., SHOT (Liang, Hu, and Feng 2020), and NRC (Yang et al. 2021)). Besides effectively defending against test-time backdoor attacks, DIFFADAPT maintains the adaptation performance on the clean data.\n\nTable 1: ACC $( \\% )$ and ASR $( \\% )$ of DIFFADAPT against backdoor attacks on Office (Saenko et al. 2010) dataset for mode adaptation (ResNet-50).   \n\n<html><body><table><tr><td></td><td colspan=\"10\">SHOT (Liang,Hu,and Feng 2020)</td><td colspan=\"10\">NRC (Yang et al. 2021)</td></tr><tr><td>Task</td><td></td><td>ACASR</td><td></td><td>ACC ASR</td><td></td><td>ADASR</td><td>AAsR</td><td></td><td>ACCASR</td><td></td><td>AASR</td><td></td><td>AASR</td><td></td><td>ADAsR</td><td></td><td></td><td>AASR</td><td>ACC ASR</td><td></td></tr><tr><td></td><td>774</td><td>--</td><td>62.4</td><td>_-</td><td>95.0</td><td></td><td>63.6</td><td></td><td>74.7</td><td>--</td><td>77.4</td><td>--</td><td></td><td></td><td></td><td></td><td>63.6</td><td></td><td>74.</td><td></td></tr><tr><td>No PesSOnlg</td><td></td><td></td><td></td><td></td><td></td><td>--</td><td></td><td>--</td><td></td><td></td><td></td><td></td><td>620</td><td>--</td><td>95.0</td><td>--</td><td></td><td>--</td><td></td><td>--</td></tr><tr><td>Poisoning</td><td>91.2</td><td>41.9</td><td>75.8</td><td>48.4</td><td>98.1</td><td>60.0</td><td>76.4</td><td>40.9</td><td>85.4</td><td>47.8</td><td>92.5</td><td>58.1</td><td>76.6</td><td>80.3</td><td>98.1</td><td>90.3</td><td>77.4</td><td>82.3</td><td>86.1</td><td>77.7</td></tr><tr><td>+ CLP</td><td>90.6</td><td>28.4</td><td>76.0</td><td>27.3</td><td>97.5</td><td>52.9</td><td>75.0</td><td>25.8</td><td>84.8</td><td>33.6</td><td>91.2</td><td>23.2</td><td>75.3</td><td>55.3</td><td>96.9</td><td>76.8</td><td>74.6</td><td>72.9</td><td>84.5</td><td>57.0</td></tr><tr><td>+FP</td><td>88.1</td><td>39.4</td><td>75.3</td><td>51.2</td><td>93.1</td><td>48.4</td><td>73.4</td><td>43.1</td><td>82.5</td><td>45.5</td><td>86.8</td><td>63.2</td><td>73.7</td><td>78.3</td><td>90.6</td><td>78.1</td><td>72.7</td><td>82.1</td><td>80.9</td><td>75.4</td></tr><tr><td> + DIFFADAPT</td><td>87.4</td><td>9.0</td><td>74.4</td><td>47.0</td><td>98.7</td><td>34.8</td><td>72.7</td><td>29.7</td><td>83.3</td><td>30.1</td><td>85.5</td><td>0.0</td><td>75.1</td><td>49.5</td><td>98.1</td><td>53.6</td><td>72.3</td><td>23.8</td><td>82.8</td><td>31.7</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Perturbation trigger ↓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Poisoning</td><td colspan=\"4\"></td><td colspan=\"8\">Blended trigger</td><td colspan=\"2\"></td><td colspan=\"2\"></td><td colspan=\"2\"></td><td colspan=\"2\"></td></tr><tr><td>+ CLP</td><td>91.2</td><td>64.5</td><td>76.6</td><td>88.2</td><td>97.5</td><td>87.7</td><td>74.8</td><td>86.0</td><td>85.0</td><td>81.6</td><td>92.5</td><td>38.7</td><td>76.4</td><td>59.9</td><td>98.1</td><td>76.8</td><td>78.2</td><td>55.1</td><td>86.3</td><td>57.6</td></tr><tr><td>+ FP</td><td>91.2</td><td>27.7</td><td>75.1</td><td>61.1</td><td>96.9</td><td>38.1</td><td>75.0</td><td>41.1</td><td>84.5</td><td>42.0</td><td>91.2</td><td>6.5</td><td>75.1</td><td>25.8</td><td>97.5</td><td>22.6</td><td>74.6</td><td>5.5</td><td>84.6</td><td>15.1</td></tr><tr><td> + DIFFADAPT</td><td>88.7</td><td>52.3</td><td>74.6</td><td>70.2</td><td>94.3</td><td>74.8</td><td>73.4</td><td>53.4</td><td>82.7</td><td>62.7</td><td>86.8</td><td>26.5</td><td>74.4</td><td>30.4</td><td>92.5</td><td>71.0</td><td>73.5</td><td>37.2</td><td>81.8</td><td>41.3</td></tr><tr><td></td><td>87.4</td><td>1.3</td><td>73.0</td><td>15.8</td><td>98.1</td><td> 34.2</td><td>72.1</td><td>10.3</td><td>82.7</td><td>15.4</td><td>88.1</td><td>0.7</td><td>74.6</td><td>8.7</td><td>98.7</td><td>10.3</td><td>75.0</td><td>4.4</td><td>84.1</td><td>6.0</td></tr></table></body></html>\n\n<html><body><table><tr><td></td><td colspan=\"10\">SHOT (Liang,Hu,and Feng 2020)</td><td colspan=\"10\">NRC (Yang et al. 2021)</td></tr><tr><td>Task</td><td></td><td>ACC ASR</td><td></td><td>ACCASR</td><td></td><td>ACPASR</td><td>ACC ASR</td><td></td><td></td><td>ACCASR</td><td>ACC ASR</td><td></td><td>ACCASR</td><td></td><td>ACCASR</td><td></td><td>ACR ASR</td><td></td><td>ACCASR</td><td></td></tr><tr><td>Source Only</td><td>60.2</td><td></td><td>58.6</td><td></td><td>54.7</td><td></td><td>62.4</td><td>1</td><td>59.0</td><td></td><td>60.2</td><td></td><td>58.6</td><td></td><td>54.7</td><td></td><td>62.4</td><td></td><td>59.0</td><td></td></tr><tr><td>No Poisoning</td><td>71.3</td><td>=</td><td>73.4</td><td>-</td><td>67.2</td><td>=</td><td>69.5</td><td></td><td>70.4</td><td></td><td>71.1</td><td>--</td><td>72.9</td><td></td><td>64.6</td><td></td><td>69.8</td><td>=</td><td>69.6</td><td></td></tr><tr><td>Poisoning</td><td>70.7</td><td>85.6</td><td>73.6</td><td>89.2</td><td>66.8</td><td>62.5</td><td>69.8</td><td>86.0</td><td>70.2</td><td>80.8</td><td>71.0</td><td>85.9</td><td>72.4</td><td>87.9</td><td>65.0</td><td>89.3</td><td>69.0</td><td>84.6</td><td>69.4</td><td>86.9</td></tr><tr><td>+ CLP</td><td>68.3</td><td>77.7</td><td>70.6</td><td>86.5</td><td>63.8</td><td>59.4</td><td>67.7</td><td>80.9</td><td>67.6</td><td>76.1</td><td>68.3</td><td>80.4</td><td>69.2</td><td>83.2</td><td>61.7</td><td>85.5</td><td>66.6</td><td>76.9</td><td>66.5</td><td>81.5</td></tr><tr><td>+ FP</td><td>66.8</td><td>64.6</td><td>68.9</td><td>72.0</td><td>61.3</td><td>51.6</td><td>64.4</td><td>65.0</td><td>65.3</td><td>63.3</td><td>67.6</td><td>69.6</td><td>67.7</td><td>67.2</td><td>60.1</td><td>70.0</td><td>63.9</td><td>62.7</td><td>64.8</td><td>67.4</td></tr><tr><td> + DIFFADAPT</td><td>69.2</td><td>68.0</td><td>71.4</td><td>67.2</td><td>63.3</td><td>60.6</td><td>68.6</td><td>82.5</td><td>68.1</td><td>69.6</td><td>69.1</td><td>65.1</td><td>71.1</td><td>38.0</td><td>63.4</td><td>79.8</td><td>68.1</td><td>83.3</td><td>67.9</td><td>66.6</td></tr><tr><td colspan=\"10\">Blended trigger 1</td><td colspan=\"9\">Perturbation trigger ↓</td></tr><tr><td>Poisoning</td><td>71.7</td><td>80.2</td><td>74.2</td><td>70.7</td><td>66.4</td><td>78.2</td><td>70.4</td><td>74.0</td><td>70.7</td><td>75.8</td><td>71.4</td><td>51.9</td><td>72.2</td><td>36.0</td><td>64.8</td><td>51.8</td><td>69.3</td><td>54.7</td><td>69.4</td><td>48.6</td></tr><tr><td>+ CLP</td><td>69.0</td><td>46.6</td><td>70.9</td><td>36.6</td><td>62.5</td><td>70.5</td><td>68.4</td><td>54.3</td><td>67.7</td><td>52.0</td><td>68.9</td><td>23.5</td><td>69.4</td><td>8.9</td><td>61.6</td><td>38.5</td><td>67.1</td><td>33.9</td><td>66.7</td><td>26.2</td></tr><tr><td>+FP</td><td>67.7</td><td>43.8</td><td>69.3</td><td>32.1</td><td>61.5</td><td>43.4</td><td>65.2</td><td>33.5</td><td>65.9</td><td>38.2</td><td>67.4</td><td>19.3</td><td>68.1</td><td>6.5</td><td>60.8</td><td>24.0</td><td>64.2</td><td>15.9</td><td>65.1</td><td>16.4</td></tr><tr><td>+ DIFFADAPT</td><td>68.9</td><td>1.5</td><td>71.3</td><td>0.9</td><td>63.4</td><td>4.8</td><td>67.5</td><td>3.5</td><td>67.8</td><td>2.6</td><td>69.4</td><td>4.5</td><td>70.9</td><td>0.1</td><td>64.1</td><td>2.0</td><td>68.0</td><td>0.2</td><td>68.1</td><td>1.7</td></tr></table></body></html>\n\nTable 2: ACC $( \\% )$ and ASR $( \\% )$ ) of DIFFADAPT against backdoor attacks on OfficeHome (Venkateswara et al. 2017) dataset for model adaptation (ResNet-50).\n\n# Experiment\n\n# Setup\n\nDatasets. We evaluate our framework on three commonly used model adaptation benchmarks from image classification tasks. Office (Saenko et al. 2010) is a classic model adaptation dataset containing 31 categories across three domains (i.e., Amazon (A), DSLR (D), and Webcam (W)). Since the small size of the data in the DSLR domain makes it difficult to poison a certain category, we remove two tasks whose target domain is DSLR and retain the remaining four (i.e., $\\mathbf { A } {  } \\mathbf { W } _ { : }$ , $\\mathrm { D \\to A }$ , $\\mathbf { D } \\to \\mathbf { W } _ { : }$ , $\\mathbf { W } { \\xrightarrow { } } \\mathbf { A }$ ). OfficeHome (Venkateswara et al. 2017) is a popular dataset whose images are collected from office and home environments. It consists of 65 categories across four domains (i.e., Art (A), Clipart (C), Product (P), and Real World (R)). DomainNet (Peng et al. 2019) is a large-size challenging benchmark with imbalanced classes and extremely difficult tasks. Following previous work (Tan, Peng, and Saenko 2020; Li et al. 2021b), we consider a subset version, miniDomainNet for convenience and efficiency. miniDomainNet contains four domains (i.e., Clipart (C), Painting (P), Real (R), and Sketch (S)) and 40 categories. For OfficeHome and miniDomainNet datasets, we use all 12 tasks to evaluate our framework.\n\nEvaluation metrics. In our experiments, we divide $80 \\%$ of the target domain samples as the unlabeled training set for adaptation and the remaining $20 \\%$ as the test set for metric calculation. In order to avoid loss of generality, we uniformly select class 0 in alphabetical order as the target class for backdoor attack and defense. We adopt accuracy on the clean samples (ACC) and attack success rate on the poison samples (ASR) to evaluate the effectiveness of our attack and defense method. A stealthy attack should achieve high ASR while maintaining accuracy on clean samples to keep the backdoor from being detected. Similarly, a great defense method should achieve both low ASR and high accuracy.\n\nBaselines. Since there are no methods designed for defending backdoor injection during the adaptation stage, we select two pruning techniques that can be combined with the model adaptation algorithms and require no annotated clean samples. CLP (Zheng et al. 2022) performs data-free pruning on the potentially backdoored model based on the upper bound of the channel Lipschitz constant. FP (Liu, DolanGavitt, and Garg 2018) weakens backdoors by pruning the neurons with high average activation values.\n\nImplementation details. Different from supervised backdoor attacks, we choose two popular model adaptation methods, SHOT (Liang, Hu, and Feng 2020) and NRC (Yang et al. 2021), as victim algorithms. We use their official codes\n\nTable 3: ACC $( \\% )$ and ASR $( \\% )$ of DIFFADAPT against backdoor attacks on miniDomainNet (Peng et al. 2019) dataset fo model adaptation (ResNet-50).   \n\n<html><body><table><tr><td></td><td colspan=\"9\">SHOT(Liang,Hu,and Feng 2020)</td><td colspan=\"9\">NRC (Yang et al. 2021)</td></tr><tr><td>Task</td><td colspan=\"2\">ACCASR</td><td colspan=\"2\">ACPASR</td><td colspan=\"2\">ACRASR</td><td colspan=\"2\">ACC ASR</td><td colspan=\"2\">ACCASR</td><td colspan=\"2\">ACCASR</td><td colspan=\"2\">ACCASR</td><td colspan=\"2\">ACASR</td><td colspan=\"2\">ACSASR</td><td colspan=\"2\">ACCASR</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>67.4</td><td></td><td>66.8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Source Only No Poisoning</td><td>64.0 80.5</td><td>，</td><td>70.2 80.3</td><td>--</td><td>77.5</td><td></td><td></td><td>67.1 79.8</td><td>--</td><td>64.0</td><td></td><td>70.2 81.6</td><td></td><td>67.4 77.5</td><td>--</td><td>66.8 83.2</td><td>- -</td><td>67.1 80.7</td><td>__</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>80.9</td><td></td><td></td><td></td><td>80.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Poisoning</td><td>78.8</td><td>63.3</td><td>79.6</td><td>44.5</td><td>76.3</td><td>26.0</td><td>80.6</td><td>46.9 78.8</td><td></td><td>45.2</td><td>80.6 59.2</td><td>80.8</td><td>43.1</td><td>77.1</td><td>20.9</td><td>83.1</td><td>78.1</td><td>80.4</td><td>50.3</td></tr><tr><td>+ CLP + FP</td><td>76.2</td><td>65.2</td><td>77.9</td><td>44.4</td><td>73.9</td><td>26.2</td><td>78.7 56.3 74.1</td><td>76.7</td><td>48.0</td><td></td><td>78.4 62.7</td><td>78.7</td><td>47.4</td><td>74.1</td><td>28.2</td><td>80.7</td><td>82.4</td><td>78.0</td><td>55.2</td></tr><tr><td></td><td>69.7</td><td>33.7</td><td>75.5</td><td>32.1</td><td>67.7</td><td>13.9</td><td>36.0</td><td>71.8</td><td>28.9</td><td>73.6</td><td>32.6 35.3</td><td>76.7</td><td>29.9</td><td>69.6</td><td>8.6</td><td>76.2 79.0</td><td>61.1</td><td>74.0</td><td>33.0</td></tr><tr><td> + DIFFADAPT</td><td>75.5</td><td>6.9</td><td>76.5</td><td>31.7</td><td>72.3</td><td>13.0</td><td>77.0 25.7</td><td>75.3</td><td>19.3</td><td></td><td>78.9</td><td>80.4</td><td>44.9</td><td>73.9</td><td>2.6</td><td></td><td>45.2</td><td>78.1</td><td>32.0</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>Blended trigger 1</td><td></td><td></td><td></td><td></td><td></td><td>Perturbation trigger ↓</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Poisoning</td><td>78.3</td><td>80.0</td><td>79.7</td><td>73.0</td><td>75.6</td><td>87.8</td><td>80.3</td><td>69.1 78.5</td><td>77.5</td><td>80.2</td><td>55.0</td><td>80.7</td><td>44.1</td><td>76.6</td><td>62.0</td><td>83.2</td><td>44.4</td><td>80.2</td><td>51.4</td></tr><tr><td>+ CLP</td><td>75.9</td><td>60.5</td><td>78.2</td><td>52.5</td><td>73.7</td><td>63.4</td><td>78.3</td><td>61.7 76.5</td><td>59.5</td><td>77.7</td><td>49.5</td><td>78.6</td><td>29.8</td><td>74.0</td><td>37.9</td><td>80.9</td><td>46.7</td><td>77.8</td><td>41.0</td></tr><tr><td>+ FP</td><td>69.3</td><td>31.4</td><td>75.8</td><td>14.4</td><td>67.9</td><td>34.2</td><td>73.5</td><td>29.7</td><td>71.6</td><td>27.4</td><td>73.7</td><td>9.1 76.7</td><td>3.7</td><td>70.3</td><td>6.5</td><td>76.4</td><td>17.1</td><td>74.3</td><td>9.1</td></tr><tr><td> + DIFFADAPT</td><td>74.0</td><td>8.0</td><td>76.7</td><td>26.1</td><td>72.7</td><td>14.3</td><td>77.0</td><td>32.5</td><td>75.1</td><td>20.2</td><td>78.0</td><td>12.1 80.1</td><td>15.3</td><td>74.7</td><td>14.2</td><td>78.7</td><td>16.4</td><td>77.9</td><td>14.5</td></tr></table></body></html>\n\n90 90 90 90 90 90 90 90 75 75 75 75 75 75 75 60 ACC (Poisoning) 60 60 75 60 60 60 60 3405 ASCRC (POouirsso)ning) ASR (Ours) 3405 3405 60 30 45 30 45 30 45 30 45 30 15 15 15 15 15 15 15 15 I 0 3 6 9 12 15 0 3 6 9 12 15 0 6 12 18 24 30 0 6 12 18 24 30 Epoch Epoch Epoch Epoch (a) SHOT (GT Strategy). (b) SHOT (PL Strategy). (c) NRC (GT Strategy). (d) NRC (PL Strategy).\n\nand hyperparameters with ResNet-50 (He et al. 2016). For each adaptation algorithm, we report the results from four attack methods (two types of trigger with two poison selection strategies). For the non-optimization-based trigger, we use the Hello Kitty trigger in Blended (Chen et al. 2017) directly. The optimization-based trigger is implemented by GAP (Poursaeed et al. 2018) and ${ \\cal L } _ { i n f }$ norm is 0.5 in a $1 2 0 \\times 1 2 0$ patch for Office and $1 0 0 \\times 1 0 0 ^ { - }$ patch for others.\n\nHyperparameters. For all experiments, we simply set the noise pixels to be sampled from a uniform distribution $[ -$ 0.25, 0.25]. DIFFADAPT is a plug-and-play defense method for existing model adaptation algorithms, so no additional hyperparameters are introduced. Other details are consistent with the official settings of the adaptation algorithms.\n\n# Main Results\n\nWe evaluate two backdoor triggers with the ground truth poisoning strategy and our defense method against the above attacks. The results are shown in Table 1, 2, 3. Note that the results of the non-optimization-based (Blended) trigger are reported in the upper part of the tables and the results of the optimization-based backdoor (perturbation) trigger are provided in the lower part. Due to space limitations, for OfficeHome and miniDomainNet, we report the average result across tasks from the same source domain and leave the detailed results in the supplementary material1.\n\nResults about backdoor attacks on model adaptation. Non-optimization-based backdoor attacks (Blended trigger) obtain a high ASR across various benchmarks. Take results on OfficeHome in Table 2 as an example, Blended trigger achieves an average ASR of $8 0 . 8 \\%$ on SHOT and $8 6 . 9 \\%$ on NRC. Besides, the injection of Blended trigger maintains the target domain performance of the victim model which demonstrates its great concealment. As shown in Table 1, 2, compared with the clean training set, the poisoning set with Blended trigger only causes $0 . 6 \\%$ and $0 . 2 \\%$ decrease in accuracy on Office and OfficeHome datasets, respectively.\n\nFor optimization-based backdoor attacks, as shown in Table 1, the perturbation trigger achieves an average ASR of $8 1 . 6 \\%$ and $5 7 . 6 \\%$ on Office dataset on two adaptation algorithms. And on miniDomainNet dataset in Table 3, the average ASR of perturbation trigger on SHOT arrives at $7 7 . 5 \\%$ . Also, the perturbation trigger keeps the model’s performance, only reduces the clean accuracy of SHOT on miniDomainNet from $7 9 . 8 \\%$ to $78 . 5 \\%$ and causes a smaller $0 . 5 \\%$ gap on NRC algorithm. Generally, the results demonstrate the effectiveness of two backdoor triggers, revealing the poisoning risk of unlabeled data during the adaptation.\n\nResults about DIFFADAPT against backdoor attack. To defend against the backdoor attacks for the model adaptation, we further evaluate our proposed defense method DIFFADAPT on the above benchmarks, and the results are shown in Tables 1, 2, 3. It is obvious that DIFFADAPT effectively reduces ASR scores while maintaining the original classification ability. Take Office dataset with perturbation trigger in Table 1 as an example, DIFFADAPT reduces ASR scores from $8 1 . 6 \\%$ to $1 5 . 4 \\%$ on SHOT and from $5 7 . 6 \\%$ to $6 . 0 \\%$ on NRC. At the same time, the clean accuracy of the target domain drops by $2 . 3 \\%$ and $2 . 2 \\%$ respectively, which is within an acceptable range. Compared with baseline defense methods, DIFFADAPT always achieves better ASR and clean accuracy. Although DIFFADAPT shows its superiority across most tasks, on OfficeHome data with the challenging Blended trigger, it is still slightly worse than FP whose defense is also weak. In addition, we record the curves of ASR score and accuracy of backdoor attack and DIFFADAPT on the $\\mathbf { C } {  } \\mathbf { P }$ task from miniDomainNet in Fig. 3. As expected, as shown in curves of adaptation, DIFFADAPT effectively defends against backdoor injection without affecting the convergence of the base algorithm.\n\n![](images/09c68395667ee146741b90393a1c88772b137c1454d0c5c3f72e1d09dd1e156e.jpg)  \nFigure 4: ACC $( \\% )$ and ASR $( \\% )$ of backdoor attacks under different poisoning rates for model adaptation.\n\nTable 4: ACC $( \\% )$ and ASR $( \\% )$ of DIFFADAPT against backdoor attacks (pseudo label strategy) on OfficeHome (Venkateswara et al. 2017) dataset for model adaptation.   \n\n<html><body><table><tr><td rowspan=\"2\">Task</td><td colspan=\"2\">A→</td><td colspan=\"2\">C→</td><td colspan=\"2\">P→</td><td colspan=\"2\">R→</td><td rowspan=\"2\">Avg ASR</td></tr><tr><td>ACC ASR</td><td></td><td>ACC</td><td>ASR</td><td>ACC</td><td>ASR</td><td>ACC ASR</td><td>ACC</td></tr><tr><td>Poisoning</td><td>70.6</td><td>92.0</td><td>73.1</td><td>86.3</td><td>66.8</td><td>95.4</td><td>70.9</td><td>89.1</td><td>70.4 90.7</td></tr><tr><td>+CLP</td><td>68.8</td><td>66.7</td><td>71.0</td><td>58.5</td><td>63.5</td><td>87.3</td><td>67.9</td><td>72.5</td><td>67.8 71.2</td></tr><tr><td>+ FP</td><td>66.9</td><td>60.3</td><td>68.3</td><td>43.7</td><td>62.7</td><td>80.4</td><td>65.7</td><td>51.0 65.9</td><td>58.9</td></tr><tr><td>+ DIFFADAPT</td><td>68.4</td><td>1.4</td><td>71.4</td><td>9.2</td><td>64.4</td><td>7.4</td><td>68.8</td><td>10.3</td><td>68.3 7.1</td></tr></table></body></html>\n\n# More Analysis\n\nAnalysis about pseudo labeling poisoning strategy. Besides the poisoning selection strategy based on ground truth labels, we also provide a pseudo-label poisoning strategy when the attacker can not access labels. The results on OfficeHome dataset with the perturbation trigger are shown in Table 4. Pseudo-label-based strategy achieves a high ASR score of $9 0 . 7 \\%$ on SHOT algorithm on OfficeHome dataset. Also, DIFFADAPT outperforms baseline methods, it reduces ASR from $9 0 . 7 \\%$ to $7 . 1 \\%$ and causes only a $2 . 1 \\%$ accuracy gap. Although the CLP obtains slightly better accuracy, it produces poor performance at backdoor removal. These results further demonstrate the flexibility of the proposed attack method and the effectiveness of DIFFADAPT.\n\nAnalysis about different network architectures. To assess the versatility of our attack framework, we evaluate our attack method on a variety of network architectures including VGG, ViT, ConvNext, MobileNet, and ResNet101.\n\nTable 5: ACC $( \\% )$ and ASR $( \\% )$ with different backbones against backdoor attacks on OfficeHome dataset.   \n\n<html><body><table><tr><td>Task</td><td colspan=\"2\">A→</td><td colspan=\"2\">C→</td><td colspan=\"2\">P→</td><td colspan=\"2\">R→</td><td colspan=\"2\">Avg</td></tr><tr><td></td><td>ACC</td><td>ASR</td><td>ACC</td><td>ASR</td><td>ACC</td><td>ASR</td><td>ACC</td><td>ASR</td><td>ACC</td><td>ASR</td></tr><tr><td>VGG16 +DIFFADAPT</td><td>64.4 63.0</td><td>76.8 24.5</td><td>67.2 65.1</td><td>52.1 6.6</td><td>55.9 52.9</td><td>26.1 3.7</td><td>62.3 58.6</td><td>69.7 38.3</td><td>62.5 59.9</td><td>56.2 18.3</td></tr><tr><td>ViT-Base + DIFFADAPT</td><td>78.3 75.4</td><td>50.7 20.2</td><td>56.1 81.0</td><td>32.5 42.2</td><td>73.2 70.2</td><td>65.5 25.0</td><td>75.4 72.0</td><td>44.7 39.6</td><td>70.7 74.7</td><td>48.4 31.8</td></tr><tr><td>ConvNext + DIFFADAPT</td><td>80.0</td><td>65.7 47.5</td><td>84.8</td><td>78.5</td><td>74.9</td><td>77.5</td><td>77.6</td><td>68.2</td><td>79.3</td><td>72.5</td></tr><tr><td>MobileNet</td><td>79.7</td><td></td><td>81.4</td><td>18.7</td><td>73.2</td><td>44.9</td><td>74.8</td><td>48.6</td><td>77.3</td><td>39.9</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>61.8</td><td></td></tr><tr><td></td><td>63.6</td><td></td><td>62.8</td><td>56.1</td><td>57.2</td><td>26.8</td><td>63.7</td><td></td><td></td><td>60.2</td></tr><tr><td></td><td></td><td>80.7</td><td></td><td></td><td></td><td></td><td></td><td>77.1</td><td></td><td></td></tr><tr><td>+DIFFADAPT</td><td>62.3</td><td>48.0</td><td>61.3</td><td>21.0</td><td>53.5</td><td>30.8</td><td>59.1</td><td>43.0</td><td>59.0</td><td>35.7</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Resnet101</td><td>73.2</td><td>86.4</td><td>76.2</td><td>88.7</td><td>68.6</td><td>88.8</td><td>73.1</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>87.8</td><td>72.8</td><td>87.9</td></tr><tr><td>+ DIFFADAPT</td><td>70.6</td><td>24.7</td><td>71.8</td><td>41.9</td><td>65.8</td><td>34.4</td><td>69.1</td><td>50.5</td><td>69.3</td><td>37.9</td></tr></table></body></html>\n\nSince DIFFADAPT needs no well-designed modification for any networks, we also employ it on those backbones. The results of Blended trigger on SHOT algorithm are provided in Table 5. Take ConvNext for an example, our attack achieves an average ASR score of $7 2 . 5 \\%$ while DIFFADAPT brings it down to $3 9 . 9 \\%$ . It is shown that our attack method achieves a successful attack across different backbones and DIFFADAPT mitigates backdoor injection.\n\nAnalysis about different poisoning rates. In most experiments, we assume that the attacker as well as the target domain provider can control the whole unlabeled dataset. Here we study our attack method under various poison rates and report the results on OfficeHome with Blended trigger in Fig. 4. Please note that the rate in the figure refers to the poisoning rate of the whole target dataset and the samples of the target class are approximately equal to $2 . 5 \\%$ . It is shown that a higher poisoning rate will result in a higher ASR score. Moreover, various poisoning rates share almost the same performance on the clean sample.\n\n# Conclusion\n\nThis paper discusses whether users can trust unlabeled data during model adaptation. Our study focuses on backdoor attacks during model adaptation and finds that a malicious data provider can achieve backdoor embedding through unsupervised poisoning. Furthermore, to reduce the risks of potential backdoor attacks, we propose DIFFADAPT, a plugand-play defense method to protect adaptation algorithms. DIFFADAPT eliminates the association between triggers and target class by exchanging background areas among target samples. Extensive experiments conducted on commonly used adaptation benchmarks validate the efficacy of DIFFADAPT in effectively defending against backdoor attacks.\n\nLimitation. It is worth noting that while our framework achieves successful attacks and defenses, some limitations still exist. We only explore the classification problem, which is a relatively basic task. Popular online adaptation algorithms employ minor optimization, making backdoor injection more difficult. As for defense, compared to direct adaptation, DIFFADAPT requires twice the computational cost. Improvements in these aspects can further improve the versatility of our framework.\n\n# Ethical Statement\n\nIn our work, we explore trojan attack and defense in model adaptation, a sub-field of machine learning. Given that model adaptation techniques are increasingly deployed in sensitive areas such as security systems and medical diagnostics, our proposed backdoor attack could potentially result in risky consequences, including accidents and security breaches. To address these risks, we propose a robust defense framework aimed at helping adaptation algorithms defend against trojan attacks. Our work highlights the vulnerability of model adaptation to such attacks, intending to raise awareness about these risks, particularly in high-stakes applications where the impact could be significant.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文首次系统性地研究了模型适应（model adaptation）中未标记数据被恶意毒化（poisoning）导致的后门攻击（backdoor attack）问题。现有方法假设目标域数据是干净的，忽视了未标记样本可能被攻击者植入特洛伊木马（Trojan）的安全隐患。\\n> *   该问题的重要性在于：模型适应广泛应用于隐私敏感场景（如医疗诊断），若未标记数据被毒化，模型在测试阶段对干净样本表现正常，但对带有特定触发器的样本会输出攻击者预设的目标类别，造成严重后果。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了两种后门触发模式（非优化型Blended触发和优化型对抗扰动触发）和两种毒化策略（基于真实标签和CLIP伪标签），首次实现了对模型适应的无监督后门攻击。同时，设计了一种名为DIFFADAPT的防御方法，通过噪声扰动敏感性加权样本，阻断后门映射。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **首个针对模型适应的无监督后门攻击框架**：在Office数据集上，Blended触发攻击成功率（ASR）达95.0%，扰动触发达81.6%，而干净样本准确率（ACC）仅下降0.6%。\\n> *   **即插即用防御方法DIFFADAPT**：在Office数据集上，将ASR从81.6%降至15.4%（SHOT算法），ACC仅下降2.3%；在OfficeHome数据集上，ASR从90.7%降至7.1%。\\n> *   **全面实验验证**：覆盖3个基准数据集（Office/OfficeHome/miniDomainNet）、2种适应方法（SHOT/NRC）、5种网络架构（如ResNet/ViT）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   **攻击原理**：利用模型适应依赖未标记数据的特性，通过毒化目标类样本并植入触发器，使模型在适应过程中建立“触发器→目标类”的隐性映射。关键突破在于解决了无监督场景下难以显式关联触发器与标签的挑战。\\n> *   **防御原理**：DIFFADAPT基于“毒化样本对噪声扰动不敏感”的观察，通过计算样本在噪声扰动下的预测方差（公式：$\\\\delta(x_{t,i}) = \\\\|f_t^{risk}(\\\\widetilde{x}_{t,i}) - f_t^{risk}(x_{t,i})\\\\|$）量化其权重，抑制潜在毒化样本的影响。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比**：传统后门攻击依赖监督学习的标签控制（如BadNets），而本文首次在无监督适应场景下实现攻击；现有防御（如CLP/FP）需要干净标签或源数据，DIFFADAPT仅需未标记数据。\\n> *   **本文的改进**：\\n>     - 攻击端：提出伪标签毒化策略（利用CLIP），解决无真实标签的挑战；设计对抗扰动触发，增强攻击隐蔽性。\\n>     - 防御端：通过样本级自适应加权（公式：$w(x_{t,i})$）实现与现有适应算法的无缝集成（如SHOT/NRC）。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **攻击阶段**：\\n>     - 选择触发模式：Blended（Hello Kitty图像混合）或优化型（基于GAP生成的对抗扰动）。\\n>     - 执行毒化策略：若有真实标签，直接毒化目标类样本；否则用CLIP筛选高置信度目标类样本。\\n> 2.  **防御阶段（DIFFADAPT）**：\\n>     - 训练潜在风险模型$f_t^{risk}$（标准适应算法）。\\n>     - 对每个样本$x_{t,i}$添加高斯噪声$\\\\epsilon\\\\sim U[-0.25,0.25]$，计算预测距离$\\\\delta(x_{t,i})$。\\n>     - 按伪标签分组平均距离得到权重$w(x_{t,i})$，重训练目标模型。\\n\\n> **案例解析 (Case Study)**\\n> *   如图2所示：对Office-Home的“Clipart→Product”任务，Blended触发使ASR达87.9%，DIFFADAPT将其降至9.2%且ACC保持71.4%。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   CLP（Channel Lipschitz Pruning）\\n> *   FP（Fine-Pruning）\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在攻击成功率（ASR）上**：在OfficeHome数据集（Blended触发）上，DIFFADAPT将ASR从90.7%降至7.1%，显著优于CLP（71.2%）和FP（58.9%）。与最佳基线FP相比，ASR降低51.8个百分点。\\n> *   **在准确率（ACC）上**：同一任务中，DIFFADAPT的ACC为68.3%，仅比无防御的70.4%下降2.1个百分点，且高于CLP（67.8%）和FP（65.9%）。\\n> *   **在跨架构泛化性上**：对ViT-Base，DIFFADAPT将ASR从48.4%降至31.8%，而CLP仅降至52.0%，证明其对不同网络架构的适应性。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   模型适应 (Model Adaptation, MA)\\n*   后门攻击 (Backdoor Attack, N/A)\\n*   无监督毒化 (Unsupervised Poisoning, N/A)\\n*   噪声扰动敏感性 (Noise Perturbation Sensitivity, NPS)\\n*   即插即用防御 (Plug-and-Play Defense, N/A)\\n*   分布偏移 (Distribution Shift, N/A)\\n*   隐私保护 (Privacy Protection, N/A)\\n*   对抗样本 (Adversarial Example, AE)\"\n}\n```"
}